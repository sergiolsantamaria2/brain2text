output_dir: ${oc.env:SLURM_TMPDIR,/tmp}/brain2text/trained_models/backup_cosineDecay30k_120k/gru_cosineDecay30k_120k_seed10_wer1024
checkpoint_dir: ${output_dir}/checkpoint

num_training_batches: 120000
lr_scheduler_type: cosine

# Key idea: LR reaches lr_min by 30k, then stays near lr_min for refinement
lr_decay_steps: 30000
lr_decay_steps_day: 30000

lr_max: 0.003
lr_min: 0.0001
lr_max_day: 0.001
lr_min_day: 0.0001

lr_warmup_steps: 500
lr_warmup_steps_day: 500

batches_per_val_step: 5000

save_val_logits: false
save_all_val_steps: false
save_best_checkpoint: false
save_final_model: false

seed: 10
dataset:
  seed: 10

wandb:
  enabled: true
  project: brain2text
  run_name: gru_cosineDecay30k_120k_seed10_wer1024
  tags: ["backup", "gru", "cosine", "decay30k", "warmup500", "seed10", "120k", "wer1024"]

eval:
  compute_wer: true
  wer_max_trials: 1024
  wer_every_val_steps: 2
  wer_tag: "1gram"
  lm_dir: "assets/lm/openwebtext_1gram_lm_sil"
  acoustic_scale: 0.35
  blank_penalty: 90.0
  max_active: 7000
  min_active: 200
  beam: 15.0
  lattice_beam: 8.0
  ctc_blank_skip_threshold: 0.95
  length_penalty: 0.0
  nbest: 50
