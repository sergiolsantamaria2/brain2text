# xLSTM sweep: number of blocks
num_training_batches: 20000

lr_scheduler_type: cosine
lr_decay_steps: ${num_training_batches}
lr_decay_steps_day: ${num_training_batches}

model:
  decoder_type: xlstm

  # Sweep variable
  xlstm_num_blocks: 1

  # Fixed xLSTM knobs for this sweep
  xlstm_num_heads: 4
  xlstm_conv1d_kernel_size: 4
  xlstm_dropout: 0.2

output_dir: trained_models/xlstm/num_blocks/b1
checkpoint_dir: ${output_dir}/checkpoint

save_all_val_steps: false
save_val_logits: false
save_final_model: false
save_best_checkpoint: true

wandb:
  enabled: true
  group: xlstm_num_blocks
  run_name: b1
  tags: ["xlstm", "num_blocks", "b1", "h4", "k4", "T=20k"]
