# xLSTM sanity: train-only, very short (no val, no WER)
num_training_batches: 20
batches_per_train_log: 1
batches_per_val_step: 9999999

# Ensure no AMP + no compile (you already implemented compile gate in trainer)
use_amp: false
torch_compile: false

# Keep optimizer schedule simple
lr_scheduler_type: cosine
lr_decay_steps: ${num_training_batches}
lr_decay_steps_day: ${num_training_batches}

# Disable expensive eval for this sanity check
eval:
  compute_wer: false

# Disable saving to reduce I/O noise
save_all_val_steps: false
save_val_logits: false
save_val_metrics: false
save_final_model: false
save_best_checkpoint: false

model:
  decoder_type: xlstm
  xlstm_num_blocks: 1
  xlstm_num_heads: 4
  xlstm_conv1d_kernel_size: 3
  xlstm_dropout: 0.2

  # Keep these off for the first sanity run (you can re-enable later)
  head_type: "none"
  head_num_blocks: 0
  input_speckle_p: 0.0

output_dir: trained_models/xlstm/sanity_trainonly/b1
checkpoint_dir: ${output_dir}/checkpoint

wandb:
  enabled: false
