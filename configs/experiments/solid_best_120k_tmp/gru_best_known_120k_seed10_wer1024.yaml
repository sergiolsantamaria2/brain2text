output_dir: ${oc.env:TMPDIR}/brain2text/trained_models/solid_best_120k/gru_best_known_120k_seed10_wer1024
checkpoint_dir: ${output_dir}/checkpoint

num_training_batches: 120000
lr_scheduler_type: cosine
lr_decay_steps: ${num_training_batches}
lr_decay_steps_day: ${num_training_batches}

lr_max: 0.003
lr_min: 0.0001
lr_warmup_steps: 500
lr_warmup_steps_day: 500

# Para 120k: no hagas val cada 500 batches (sería demasiado overhead)
batches_per_val_step: 10000

# Evitar escritura pesada (quota)
save_val_logits: false
save_all_val_steps: false
save_best_checkpoint: false
save_final_model: false

# Patching: deja esto igual que tu baseline ahora.
# Cuando acabe el patch sweep, cambia aquí a la mejor combinación.
model:
  patch_size: 14
  patch_stride: 4

seed: 10
dataset:
  seed: 10

wandb:
  enabled: true
  project: brain2text
  run_name: gru_best_known_120k_seed10_wer1024
  tags: ["solid", "gru", "cosine", "warmup500", "seed10", "120k", "wer1024", "tmpdir"]

eval:
  compute_wer: true
  wer_max_trials: 1024
  wer_every_val_steps: 1
  wer_tag: "1gram"
  lm_dir: "assets/lm/openwebtext_1gram_lm_sil"
  acoustic_scale: 0.35
  blank_penalty: 90.0
  max_active: 7000
  min_active: 200
  beam: 15.0
  lattice_beam: 8.0
  ctc_blank_skip_threshold: 0.95
  length_penalty: 0.0
  nbest: 50
