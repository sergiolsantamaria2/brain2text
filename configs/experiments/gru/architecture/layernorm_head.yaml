# Experiment: LayerNorm Head
num_training_batches: 20000

lr_scheduler_type: cosine
lr_max: 0.0040
lr_min: 0.0001333
lr_decay_steps: ${num_training_batches}
lr_warmup_steps: 500

lr_max_day: 0.00133
lr_min_day: 0.0001333
lr_decay_steps_day: ${num_training_batches}
lr_warmup_steps_day: 500

weight_decay: 0.00001

# Architecture Change: Head enabled
model:
  head_type: "resffn"
  head_num_blocks: 1
  head_norm: "layernorm"
  head_dropout: 0.2
  head_activation: "gelu"

output_dir: trained_models/gru/architecture/layernorm_head
checkpoint_dir: ${output_dir}/checkpoint

save_all_val_steps: false
save_val_logits: false
save_final_model: false
save_best_checkpoint: true

wandb:
  enabled: true
  group: gru_arch_comparison
  run_name: layernorm_head
  tags: ["gru", "layernorm", "arch_exp"]
