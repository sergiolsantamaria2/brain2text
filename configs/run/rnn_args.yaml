model:
  n_input_features: 512 # number of input features in the neural data. (2 features per electrode, 256 electrodes)
  n_units: 768 # number of units per GRU layer
  rnn_dropout: 0.2 # <-- OPTIMAL from our sweeps
  rnn_trainable: true # whether the GRU layers are trainable
  n_layers: 5 # number of GRU layers
  decoder_type: gru

  patch_size: 14 # size of the input patches (14 time steps)
  patch_stride: 4 # stride for the input patches (4 time steps)

  # Input network (day-specific)
  input_network:
    n_input_layers: 1 # number of input layers per network (one network for each day)
    input_layer_sizes:
    - 512 # size of the input layer (number of input features)
    input_trainable: true # whether the input layer is trainable
    input_layer_dropout: 0.2 # dropout rate for the input layer

  # --- Explicit "new" model params (to avoid relying on constructor defaults) ---
  # Optional head (kept OFF in baseline)
  head_type: "resffn"
  head_num_blocks: 1
  head_norm: "layernorm"
  head_dropout: 0.1
  head_activation: "gelu"

  # Speckled masking (coordinated dropout) - OPTIMAL setting you reported
  input_speckle_p: 0.10
  input_speckle_mode: "feature"

  # If present in your GRU signature (otherwise harmless if ignored by OmegaConf in your setup)
  d_model: 0

  # ResLSTM knobs (unused when decoder_type=gru, kept for compatibility)
  reslstm_num_blocks: 1
  reslstm_sublayers_per_block: 2
  reslstm_lstm_layers: 2
  reslstm_lstm_dropout: 0.1
  reslstm_norm: bn            # bn | layernorm | rmsnorm | none
  reslstm_pre_norm: false
  reslstm_residual_dropout: 0.0

gpu_number: '0' # GPU number to use for training, formatted as a string (e.g., '0', '1', etc.)
mode: train
use_amp: true # whether to use automatic mixed precision (AMP) for training

output_dir: trained_models/dev_run # directory to save the trained model and logs
checkpoint_dir: ${output_dir}/checkpoint # directory to save checkpoints during training
init_from_checkpoint: false # whether to initialize the model from a checkpoint
init_checkpoint_path: None # path of the checkpoint to initialize the model from, if any

# Keep disk usage conservative by default
save_best_checkpoint: false
save_all_val_steps: true
save_final_model: false
save_val_metrics: true
early_stopping: false
early_stopping_val_steps: 20

# --- OPTIMAL LR regime from sweeps ---
num_training_batches: 100000
lr_scheduler_type: cosine
lr_stepdrop_step: 18000
lr_stepdrop_factor: 0.1
lr_max: 0.0035
lr_min: 0.00001
lr_decay_steps: ${num_training_batches}
lr_warmup_steps: 500

lr_max_day: 0.00133
lr_min_day: 0.0001333
lr_decay_steps_day: ${num_training_batches}
lr_warmup_steps_day: 500

beta0: 0.9
beta1: 0.999
epsilon: 0.1
weight_decay: 0.001
weight_decay_day: 0
seed: 10
grad_norm_clip_value: 5

batches_per_train_log: 200
batches_per_val_step: 500

batches_per_save: 0
log_individual_day_val_PER: true
log_val_skip_logs: false
save_val_logits: true
save_val_data: false




wandb:
  group: run
  run_name: run

