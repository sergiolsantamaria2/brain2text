# Force 5-gram as the *main* LM used for decoding
eval:
  compute_wer: true
  lm_dir: data/lm/5gram/languageModel
  lm_dir_5gram: null
  wer_tag: "5gram"

  # Reduce cost (you can raise later)
  wer_max_trials: 128
  wer_every_val_steps: 1

# Keep artifacts on a persistent path (NOT /tmp)
output_dir: trained_models/winner_5gram/norm10_head_ln_1blk_seed10_10k
checkpoint_dir: ${output_dir}/checkpoint

# Avoid huge disk usage
save_val_logits: false
save_val_data: false
save_best_checkpoint: true
save_all_val_steps: false

wandb:
  enabled: true
  group: winner_5gram
  run_name: norm10_head_ln_1blk_seed10_10k_5gram
  tags: ["winner", "5gram"]
