torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  3 19:37 /tmp/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
PYTHONFAULTHANDLER=1
MAX_JOBS=8
TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_348216
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_348216/torch_extensions
TORCH_CUDA_ARCH_LIST=8.0
WANDB_DIR=/tmp/e12511253_b2t_348216/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib64:/tmp/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  3 19:37 /tmp/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
PYTHONFAULTHANDLER=1
MAX_JOBS=8
TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_348216
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_348216/torch_extensions
TORCH_CUDA_ARCH_LIST=8.0
WANDB_DIR=/tmp/e12511253_b2t_348216/wandb
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/pkgs/cuda-toolkit/lib64/libcudart.so.11.7.60
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/pkgs/cuda-toolkit/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
trained_models -> /tmp/e12511253_b2t_348216/trained_models
==============================================
Job: b2t_exp  ID: 348216
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/gru/rnn_dropout/lr40
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/gru/rnn_dropout/lr40 ==========
Num configs: 5

=== RUN base.yaml ===
2026-01-03 19:37:43,918: Using device: cuda:0
2026-01-03 19:37:45,483: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-03 19:37:45,507: Using 45 sessions after filtering (from 45).
2026-01-03 19:37:45,936: Using torch.compile (if available)
2026-01-03 19:37:45,937: torch.compile not available (torch<2.0). Skipping.
2026-01-03 19:37:45,937: Initialized RNN decoding model
2026-01-03 19:37:45,937: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 19:37:45,937: Model has 44,907,305 parameters
2026-01-03 19:37:45,937: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 19:37:47,205: Successfully initialized datasets
2026-01-03 19:37:47,206: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 19:37:48,215: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.179
2026-01-03 19:37:48,216: Running test after training batch: 0
2026-01-03 19:37:48,326: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:37:53,803: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-03 19:37:54,507: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-03 19:38:27,665: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 39.449
2026-01-03 19:38:27,668: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-03 19:38:27,668: t15.2023.08.13 val PER: 1.3056
2026-01-03 19:38:27,668: t15.2023.08.18 val PER: 1.4208
2026-01-03 19:38:27,669: t15.2023.08.20 val PER: 1.3002
2026-01-03 19:38:27,669: t15.2023.08.25 val PER: 1.3389
2026-01-03 19:38:27,669: t15.2023.08.27 val PER: 1.2460
2026-01-03 19:38:27,669: t15.2023.09.01 val PER: 1.4537
2026-01-03 19:38:27,669: t15.2023.09.03 val PER: 1.3171
2026-01-03 19:38:27,669: t15.2023.09.24 val PER: 1.5461
2026-01-03 19:38:27,669: t15.2023.09.29 val PER: 1.4671
2026-01-03 19:38:27,669: t15.2023.10.01 val PER: 1.2147
2026-01-03 19:38:27,669: t15.2023.10.06 val PER: 1.4876
2026-01-03 19:38:27,669: t15.2023.10.08 val PER: 1.1827
2026-01-03 19:38:27,669: t15.2023.10.13 val PER: 1.3964
2026-01-03 19:38:27,669: t15.2023.10.15 val PER: 1.3889
2026-01-03 19:38:27,670: t15.2023.10.20 val PER: 1.4866
2026-01-03 19:38:27,670: t15.2023.10.22 val PER: 1.3942
2026-01-03 19:38:27,670: t15.2023.11.03 val PER: 1.5923
2026-01-03 19:38:27,670: t15.2023.11.04 val PER: 2.0171
2026-01-03 19:38:27,670: t15.2023.11.17 val PER: 1.9518
2026-01-03 19:38:27,670: t15.2023.11.19 val PER: 1.6707
2026-01-03 19:38:27,670: t15.2023.11.26 val PER: 1.5413
2026-01-03 19:38:27,670: t15.2023.12.03 val PER: 1.4254
2026-01-03 19:38:27,670: t15.2023.12.08 val PER: 1.4487
2026-01-03 19:38:27,670: t15.2023.12.10 val PER: 1.6899
2026-01-03 19:38:27,670: t15.2023.12.17 val PER: 1.3077
2026-01-03 19:38:27,670: t15.2023.12.29 val PER: 1.4063
2026-01-03 19:38:27,670: t15.2024.02.25 val PER: 1.4228
2026-01-03 19:38:27,670: t15.2024.03.08 val PER: 1.3257
2026-01-03 19:38:27,670: t15.2024.03.15 val PER: 1.3196
2026-01-03 19:38:27,671: t15.2024.03.17 val PER: 1.4052
2026-01-03 19:38:27,671: t15.2024.05.10 val PER: 1.3224
2026-01-03 19:38:27,671: t15.2024.06.14 val PER: 1.5315
2026-01-03 19:38:27,671: t15.2024.07.19 val PER: 1.0817
2026-01-03 19:38:27,671: t15.2024.07.21 val PER: 1.6290
2026-01-03 19:38:27,671: t15.2024.07.28 val PER: 1.6588
2026-01-03 19:38:27,671: t15.2025.01.10 val PER: 1.0923
2026-01-03 19:38:27,671: t15.2025.01.12 val PER: 1.7629
2026-01-03 19:38:27,671: t15.2025.03.14 val PER: 1.0414
2026-01-03 19:38:27,671: t15.2025.03.16 val PER: 1.6257
2026-01-03 19:38:27,671: t15.2025.03.30 val PER: 1.2874
2026-01-03 19:38:27,671: t15.2025.04.13 val PER: 1.5949
2026-01-03 19:38:27,673: New best val WER(1gram) inf% --> 100.00%
2026-01-03 19:38:27,673: Checkpointing model
2026-01-03 19:38:27,911: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 19:38:28,156: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_0
2026-01-03 19:38:45,923: Train batch 200: loss: 77.59 grad norm: 106.06 time: 0.053
2026-01-03 19:39:03,086: Train batch 400: loss: 53.81 grad norm: 86.62 time: 0.062
2026-01-03 19:39:11,668: Running test after training batch: 500
2026-01-03 19:39:11,800: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:39:16,636: WER debug example
  GT : you can see the code at this point as well
  PR : yule and ease thus uhde at this ide is aisle
2026-01-03 19:39:16,670: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as tides
2026-01-03 19:39:18,937: Val batch 500: PER (avg): 0.5200 CTC Loss (avg): 55.3283 WER(1gram): 87.82% (n=64) time: 7.269
2026-01-03 19:39:18,937: WER lens: avg_true_words=6.16 avg_pred_words=5.61 max_pred_words=12
2026-01-03 19:39:18,938: t15.2023.08.13 val PER: 0.4615
2026-01-03 19:39:18,938: t15.2023.08.18 val PER: 0.4443
2026-01-03 19:39:18,938: t15.2023.08.20 val PER: 0.4464
2026-01-03 19:39:18,938: t15.2023.08.25 val PER: 0.4337
2026-01-03 19:39:18,938: t15.2023.08.27 val PER: 0.5177
2026-01-03 19:39:18,938: t15.2023.09.01 val PER: 0.4205
2026-01-03 19:39:18,938: t15.2023.09.03 val PER: 0.5083
2026-01-03 19:39:18,938: t15.2023.09.24 val PER: 0.4260
2026-01-03 19:39:18,938: t15.2023.09.29 val PER: 0.4690
2026-01-03 19:39:18,938: t15.2023.10.01 val PER: 0.5218
2026-01-03 19:39:18,938: t15.2023.10.06 val PER: 0.4252
2026-01-03 19:39:18,938: t15.2023.10.08 val PER: 0.5386
2026-01-03 19:39:18,938: t15.2023.10.13 val PER: 0.5811
2026-01-03 19:39:18,939: t15.2023.10.15 val PER: 0.5016
2026-01-03 19:39:18,939: t15.2023.10.20 val PER: 0.4530
2026-01-03 19:39:18,939: t15.2023.10.22 val PER: 0.4454
2026-01-03 19:39:18,939: t15.2023.11.03 val PER: 0.5095
2026-01-03 19:39:18,939: t15.2023.11.04 val PER: 0.2662
2026-01-03 19:39:18,939: t15.2023.11.17 val PER: 0.3577
2026-01-03 19:39:18,939: t15.2023.11.19 val PER: 0.3393
2026-01-03 19:39:18,939: t15.2023.11.26 val PER: 0.5580
2026-01-03 19:39:18,939: t15.2023.12.03 val PER: 0.5137
2026-01-03 19:39:18,939: t15.2023.12.08 val PER: 0.5260
2026-01-03 19:39:18,939: t15.2023.12.10 val PER: 0.4442
2026-01-03 19:39:18,939: t15.2023.12.17 val PER: 0.5676
2026-01-03 19:39:18,939: t15.2023.12.29 val PER: 0.5402
2026-01-03 19:39:18,939: t15.2024.02.25 val PER: 0.4789
2026-01-03 19:39:18,939: t15.2024.03.08 val PER: 0.6102
2026-01-03 19:39:18,939: t15.2024.03.15 val PER: 0.5547
2026-01-03 19:39:18,939: t15.2024.03.17 val PER: 0.5077
2026-01-03 19:39:18,940: t15.2024.05.10 val PER: 0.5513
2026-01-03 19:39:18,940: t15.2024.06.14 val PER: 0.5095
2026-01-03 19:39:18,940: t15.2024.07.19 val PER: 0.6724
2026-01-03 19:39:18,940: t15.2024.07.21 val PER: 0.4703
2026-01-03 19:39:18,940: t15.2024.07.28 val PER: 0.5081
2026-01-03 19:39:18,940: t15.2025.01.10 val PER: 0.7548
2026-01-03 19:39:18,940: t15.2025.01.12 val PER: 0.5627
2026-01-03 19:39:18,940: t15.2025.03.14 val PER: 0.7707
2026-01-03 19:39:18,940: t15.2025.03.16 val PER: 0.6060
2026-01-03 19:39:18,940: t15.2025.03.30 val PER: 0.7276
2026-01-03 19:39:18,941: t15.2025.04.13 val PER: 0.5706
2026-01-03 19:39:18,941: New best val WER(1gram) 100.00% --> 87.82%
2026-01-03 19:39:18,941: Checkpointing model
2026-01-03 19:39:19,552: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 19:39:19,804: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_500
2026-01-03 19:39:28,553: Train batch 600: loss: 48.60 grad norm: 86.52 time: 0.078
2026-01-03 19:39:45,690: Train batch 800: loss: 40.93 grad norm: 83.40 time: 0.057
2026-01-03 19:40:02,987: Train batch 1000: loss: 42.64 grad norm: 76.71 time: 0.066
2026-01-03 19:40:02,988: Running test after training batch: 1000
2026-01-03 19:40:03,116: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:40:07,899: WER debug example
  GT : you can see the code at this point as well
  PR : used ent ease utt owed at this royd is while
2026-01-03 19:40:07,932: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke that wass it
2026-01-03 19:40:09,804: Val batch 1000: PER (avg): 0.4076 CTC Loss (avg): 42.3267 WER(1gram): 81.73% (n=64) time: 6.816
2026-01-03 19:40:09,805: WER lens: avg_true_words=6.16 avg_pred_words=5.50 max_pred_words=12
2026-01-03 19:40:09,805: t15.2023.08.13 val PER: 0.3867
2026-01-03 19:40:09,805: t15.2023.08.18 val PER: 0.3395
2026-01-03 19:40:09,805: t15.2023.08.20 val PER: 0.3503
2026-01-03 19:40:09,805: t15.2023.08.25 val PER: 0.2922
2026-01-03 19:40:09,805: t15.2023.08.27 val PER: 0.4260
2026-01-03 19:40:09,805: t15.2023.09.01 val PER: 0.3076
2026-01-03 19:40:09,806: t15.2023.09.03 val PER: 0.3884
2026-01-03 19:40:09,806: t15.2023.09.24 val PER: 0.3350
2026-01-03 19:40:09,806: t15.2023.09.29 val PER: 0.3618
2026-01-03 19:40:09,806: t15.2023.10.01 val PER: 0.4029
2026-01-03 19:40:09,806: t15.2023.10.06 val PER: 0.3175
2026-01-03 19:40:09,806: t15.2023.10.08 val PER: 0.4493
2026-01-03 19:40:09,806: t15.2023.10.13 val PER: 0.4631
2026-01-03 19:40:09,807: t15.2023.10.15 val PER: 0.3784
2026-01-03 19:40:09,807: t15.2023.10.20 val PER: 0.3624
2026-01-03 19:40:09,807: t15.2023.10.22 val PER: 0.3575
2026-01-03 19:40:09,807: t15.2023.11.03 val PER: 0.3989
2026-01-03 19:40:09,807: t15.2023.11.04 val PER: 0.1604
2026-01-03 19:40:09,807: t15.2023.11.17 val PER: 0.2582
2026-01-03 19:40:09,807: t15.2023.11.19 val PER: 0.2096
2026-01-03 19:40:09,807: t15.2023.11.26 val PER: 0.4384
2026-01-03 19:40:09,807: t15.2023.12.03 val PER: 0.4076
2026-01-03 19:40:09,807: t15.2023.12.08 val PER: 0.4121
2026-01-03 19:40:09,808: t15.2023.12.10 val PER: 0.3587
2026-01-03 19:40:09,808: t15.2023.12.17 val PER: 0.4075
2026-01-03 19:40:09,808: t15.2023.12.29 val PER: 0.4049
2026-01-03 19:40:09,808: t15.2024.02.25 val PER: 0.3539
2026-01-03 19:40:09,808: t15.2024.03.08 val PER: 0.4950
2026-01-03 19:40:09,808: t15.2024.03.15 val PER: 0.4315
2026-01-03 19:40:09,808: t15.2024.03.17 val PER: 0.4059
2026-01-03 19:40:09,808: t15.2024.05.10 val PER: 0.4235
2026-01-03 19:40:09,808: t15.2024.06.14 val PER: 0.4101
2026-01-03 19:40:09,808: t15.2024.07.19 val PER: 0.5300
2026-01-03 19:40:09,808: t15.2024.07.21 val PER: 0.3710
2026-01-03 19:40:09,809: t15.2024.07.28 val PER: 0.4147
2026-01-03 19:40:09,809: t15.2025.01.10 val PER: 0.6074
2026-01-03 19:40:09,809: t15.2025.01.12 val PER: 0.4503
2026-01-03 19:40:09,809: t15.2025.03.14 val PER: 0.6317
2026-01-03 19:40:09,809: t15.2025.03.16 val PER: 0.4751
2026-01-03 19:40:09,809: t15.2025.03.30 val PER: 0.6425
2026-01-03 19:40:09,810: t15.2025.04.13 val PER: 0.4836
2026-01-03 19:40:09,810: New best val WER(1gram) 87.82% --> 81.73%
2026-01-03 19:40:09,810: Checkpointing model
2026-01-03 19:40:10,404: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 19:40:10,651: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_1000
2026-01-03 19:40:27,944: Train batch 1200: loss: 32.70 grad norm: 74.71 time: 0.067
2026-01-03 19:40:45,547: Train batch 1400: loss: 36.36 grad norm: 78.21 time: 0.059
2026-01-03 19:40:54,333: Running test after training batch: 1500
2026-01-03 19:40:54,492: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:40:59,275: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt e the good at this boyde is will
2026-01-03 19:40:59,308: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that os
2026-01-03 19:41:00,859: Val batch 1500: PER (avg): 0.3809 CTC Loss (avg): 37.2872 WER(1gram): 76.14% (n=64) time: 6.525
2026-01-03 19:41:00,859: WER lens: avg_true_words=6.16 avg_pred_words=5.03 max_pred_words=11
2026-01-03 19:41:00,860: t15.2023.08.13 val PER: 0.3420
2026-01-03 19:41:00,860: t15.2023.08.18 val PER: 0.3143
2026-01-03 19:41:00,860: t15.2023.08.20 val PER: 0.3042
2026-01-03 19:41:00,860: t15.2023.08.25 val PER: 0.2605
2026-01-03 19:41:00,860: t15.2023.08.27 val PER: 0.3923
2026-01-03 19:41:00,860: t15.2023.09.01 val PER: 0.2800
2026-01-03 19:41:00,860: t15.2023.09.03 val PER: 0.3789
2026-01-03 19:41:00,860: t15.2023.09.24 val PER: 0.3083
2026-01-03 19:41:00,860: t15.2023.09.29 val PER: 0.3395
2026-01-03 19:41:00,860: t15.2023.10.01 val PER: 0.3923
2026-01-03 19:41:00,861: t15.2023.10.06 val PER: 0.2853
2026-01-03 19:41:00,861: t15.2023.10.08 val PER: 0.4235
2026-01-03 19:41:00,861: t15.2023.10.13 val PER: 0.4492
2026-01-03 19:41:00,861: t15.2023.10.15 val PER: 0.3606
2026-01-03 19:41:00,861: t15.2023.10.20 val PER: 0.3389
2026-01-03 19:41:00,861: t15.2023.10.22 val PER: 0.3185
2026-01-03 19:41:00,861: t15.2023.11.03 val PER: 0.3677
2026-01-03 19:41:00,861: t15.2023.11.04 val PER: 0.1297
2026-01-03 19:41:00,861: t15.2023.11.17 val PER: 0.2240
2026-01-03 19:41:00,861: t15.2023.11.19 val PER: 0.1896
2026-01-03 19:41:00,861: t15.2023.11.26 val PER: 0.4123
2026-01-03 19:41:00,861: t15.2023.12.03 val PER: 0.3739
2026-01-03 19:41:00,861: t15.2023.12.08 val PER: 0.3529
2026-01-03 19:41:00,861: t15.2023.12.10 val PER: 0.3035
2026-01-03 19:41:00,861: t15.2023.12.17 val PER: 0.3742
2026-01-03 19:41:00,861: t15.2023.12.29 val PER: 0.3761
2026-01-03 19:41:00,861: t15.2024.02.25 val PER: 0.3188
2026-01-03 19:41:00,862: t15.2024.03.08 val PER: 0.4623
2026-01-03 19:41:00,862: t15.2024.03.15 val PER: 0.4196
2026-01-03 19:41:00,862: t15.2024.03.17 val PER: 0.3815
2026-01-03 19:41:00,862: t15.2024.05.10 val PER: 0.3759
2026-01-03 19:41:00,862: t15.2024.06.14 val PER: 0.3880
2026-01-03 19:41:00,862: t15.2024.07.19 val PER: 0.5359
2026-01-03 19:41:00,862: t15.2024.07.21 val PER: 0.3421
2026-01-03 19:41:00,862: t15.2024.07.28 val PER: 0.3713
2026-01-03 19:41:00,862: t15.2025.01.10 val PER: 0.6295
2026-01-03 19:41:00,862: t15.2025.01.12 val PER: 0.4280
2026-01-03 19:41:00,862: t15.2025.03.14 val PER: 0.5991
2026-01-03 19:41:00,862: t15.2025.03.16 val PER: 0.4450
2026-01-03 19:41:00,862: t15.2025.03.30 val PER: 0.6287
2026-01-03 19:41:00,862: t15.2025.04.13 val PER: 0.4679
2026-01-03 19:41:00,864: New best val WER(1gram) 81.73% --> 76.14%
2026-01-03 19:41:00,864: Checkpointing model
2026-01-03 19:41:01,479: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 19:41:01,730: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_1500
2026-01-03 19:41:10,358: Train batch 1600: loss: 36.28 grad norm: 76.82 time: 0.064
2026-01-03 19:41:27,670: Train batch 1800: loss: 35.49 grad norm: 72.93 time: 0.088
2026-01-03 19:41:44,991: Train batch 2000: loss: 33.73 grad norm: 70.75 time: 0.066
2026-01-03 19:41:44,992: Running test after training batch: 2000
2026-01-03 19:41:45,126: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:41:49,900: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt e the code at this bonde is will
2026-01-03 19:41:49,930: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap thus cus it
2026-01-03 19:41:51,478: Val batch 2000: PER (avg): 0.3300 CTC Loss (avg): 32.8695 WER(1gram): 69.29% (n=64) time: 6.486
2026-01-03 19:41:51,478: WER lens: avg_true_words=6.16 avg_pred_words=5.50 max_pred_words=11
2026-01-03 19:41:51,478: t15.2023.08.13 val PER: 0.3025
2026-01-03 19:41:51,478: t15.2023.08.18 val PER: 0.2565
2026-01-03 19:41:51,478: t15.2023.08.20 val PER: 0.2581
2026-01-03 19:41:51,479: t15.2023.08.25 val PER: 0.2259
2026-01-03 19:41:51,479: t15.2023.08.27 val PER: 0.3424
2026-01-03 19:41:51,479: t15.2023.09.01 val PER: 0.2346
2026-01-03 19:41:51,479: t15.2023.09.03 val PER: 0.3349
2026-01-03 19:41:51,479: t15.2023.09.24 val PER: 0.2609
2026-01-03 19:41:51,479: t15.2023.09.29 val PER: 0.2763
2026-01-03 19:41:51,479: t15.2023.10.01 val PER: 0.3236
2026-01-03 19:41:51,479: t15.2023.10.06 val PER: 0.2357
2026-01-03 19:41:51,479: t15.2023.10.08 val PER: 0.4019
2026-01-03 19:41:51,480: t15.2023.10.13 val PER: 0.3863
2026-01-03 19:41:51,480: t15.2023.10.15 val PER: 0.2980
2026-01-03 19:41:51,480: t15.2023.10.20 val PER: 0.2819
2026-01-03 19:41:51,480: t15.2023.10.22 val PER: 0.2550
2026-01-03 19:41:51,481: t15.2023.11.03 val PER: 0.3256
2026-01-03 19:41:51,481: t15.2023.11.04 val PER: 0.1024
2026-01-03 19:41:51,481: t15.2023.11.17 val PER: 0.1742
2026-01-03 19:41:51,481: t15.2023.11.19 val PER: 0.1337
2026-01-03 19:41:51,481: t15.2023.11.26 val PER: 0.3696
2026-01-03 19:41:51,482: t15.2023.12.03 val PER: 0.3277
2026-01-03 19:41:51,482: t15.2023.12.08 val PER: 0.3189
2026-01-03 19:41:51,482: t15.2023.12.10 val PER: 0.2510
2026-01-03 19:41:51,482: t15.2023.12.17 val PER: 0.3150
2026-01-03 19:41:51,482: t15.2023.12.29 val PER: 0.3219
2026-01-03 19:41:51,482: t15.2024.02.25 val PER: 0.2767
2026-01-03 19:41:51,482: t15.2024.03.08 val PER: 0.4026
2026-01-03 19:41:51,482: t15.2024.03.15 val PER: 0.3646
2026-01-03 19:41:51,482: t15.2024.03.17 val PER: 0.3424
2026-01-03 19:41:51,482: t15.2024.05.10 val PER: 0.3447
2026-01-03 19:41:51,482: t15.2024.06.14 val PER: 0.3486
2026-01-03 19:41:51,483: t15.2024.07.19 val PER: 0.4680
2026-01-03 19:41:51,483: t15.2024.07.21 val PER: 0.3048
2026-01-03 19:41:51,483: t15.2024.07.28 val PER: 0.3228
2026-01-03 19:41:51,483: t15.2025.01.10 val PER: 0.5399
2026-01-03 19:41:51,483: t15.2025.01.12 val PER: 0.3849
2026-01-03 19:41:51,483: t15.2025.03.14 val PER: 0.5296
2026-01-03 19:41:51,483: t15.2025.03.16 val PER: 0.4110
2026-01-03 19:41:51,483: t15.2025.03.30 val PER: 0.5609
2026-01-03 19:41:51,483: t15.2025.04.13 val PER: 0.4051
2026-01-03 19:41:51,484: New best val WER(1gram) 76.14% --> 69.29%
2026-01-03 19:41:51,484: Checkpointing model
2026-01-03 19:41:52,088: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 19:41:52,336: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_2000
2026-01-03 19:42:09,735: Train batch 2200: loss: 28.96 grad norm: 73.71 time: 0.060
2026-01-03 19:42:27,128: Train batch 2400: loss: 28.88 grad norm: 60.85 time: 0.052
2026-01-03 19:42:36,065: Running test after training batch: 2500
2026-01-03 19:42:36,180: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:42:41,394: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-03 19:42:41,423: WER debug example
  GT : how does it keep the cost down
  PR : houde does it keep the cost it
2026-01-03 19:42:43,024: Val batch 2500: PER (avg): 0.3042 CTC Loss (avg): 30.2167 WER(1gram): 67.01% (n=64) time: 6.958
2026-01-03 19:42:43,025: WER lens: avg_true_words=6.16 avg_pred_words=5.52 max_pred_words=10
2026-01-03 19:42:43,025: t15.2023.08.13 val PER: 0.2817
2026-01-03 19:42:43,025: t15.2023.08.18 val PER: 0.2389
2026-01-03 19:42:43,025: t15.2023.08.20 val PER: 0.2407
2026-01-03 19:42:43,025: t15.2023.08.25 val PER: 0.2169
2026-01-03 19:42:43,025: t15.2023.08.27 val PER: 0.3232
2026-01-03 19:42:43,025: t15.2023.09.01 val PER: 0.1997
2026-01-03 19:42:43,025: t15.2023.09.03 val PER: 0.2957
2026-01-03 19:42:43,025: t15.2023.09.24 val PER: 0.2245
2026-01-03 19:42:43,025: t15.2023.09.29 val PER: 0.2655
2026-01-03 19:42:43,025: t15.2023.10.01 val PER: 0.3085
2026-01-03 19:42:43,025: t15.2023.10.06 val PER: 0.2164
2026-01-03 19:42:43,026: t15.2023.10.08 val PER: 0.3857
2026-01-03 19:42:43,026: t15.2023.10.13 val PER: 0.3553
2026-01-03 19:42:43,026: t15.2023.10.15 val PER: 0.2894
2026-01-03 19:42:43,026: t15.2023.10.20 val PER: 0.2617
2026-01-03 19:42:43,026: t15.2023.10.22 val PER: 0.2372
2026-01-03 19:42:43,026: t15.2023.11.03 val PER: 0.2965
2026-01-03 19:42:43,026: t15.2023.11.04 val PER: 0.0819
2026-01-03 19:42:43,026: t15.2023.11.17 val PER: 0.1493
2026-01-03 19:42:43,026: t15.2023.11.19 val PER: 0.1257
2026-01-03 19:42:43,026: t15.2023.11.26 val PER: 0.3449
2026-01-03 19:42:43,026: t15.2023.12.03 val PER: 0.3004
2026-01-03 19:42:43,026: t15.2023.12.08 val PER: 0.2723
2026-01-03 19:42:43,026: t15.2023.12.10 val PER: 0.2392
2026-01-03 19:42:43,027: t15.2023.12.17 val PER: 0.2973
2026-01-03 19:42:43,027: t15.2023.12.29 val PER: 0.3089
2026-01-03 19:42:43,027: t15.2024.02.25 val PER: 0.2472
2026-01-03 19:42:43,027: t15.2024.03.08 val PER: 0.3755
2026-01-03 19:42:43,027: t15.2024.03.15 val PER: 0.3471
2026-01-03 19:42:43,027: t15.2024.03.17 val PER: 0.3006
2026-01-03 19:42:43,027: t15.2024.05.10 val PER: 0.3195
2026-01-03 19:42:43,027: t15.2024.06.14 val PER: 0.3170
2026-01-03 19:42:43,027: t15.2024.07.19 val PER: 0.4430
2026-01-03 19:42:43,027: t15.2024.07.21 val PER: 0.2634
2026-01-03 19:42:43,028: t15.2024.07.28 val PER: 0.2978
2026-01-03 19:42:43,028: t15.2025.01.10 val PER: 0.4972
2026-01-03 19:42:43,028: t15.2025.01.12 val PER: 0.3626
2026-01-03 19:42:43,028: t15.2025.03.14 val PER: 0.4956
2026-01-03 19:42:43,028: t15.2025.03.16 val PER: 0.3377
2026-01-03 19:42:43,028: t15.2025.03.30 val PER: 0.4977
2026-01-03 19:42:43,028: t15.2025.04.13 val PER: 0.4009
2026-01-03 19:42:43,029: New best val WER(1gram) 69.29% --> 67.01%
2026-01-03 19:42:43,029: Checkpointing model
2026-01-03 19:42:43,651: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 19:42:43,901: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_2500
2026-01-03 19:42:52,580: Train batch 2600: loss: 35.16 grad norm: 85.06 time: 0.055
2026-01-03 19:43:09,941: Train batch 2800: loss: 26.22 grad norm: 72.13 time: 0.081
2026-01-03 19:43:27,559: Train batch 3000: loss: 30.87 grad norm: 70.03 time: 0.083
2026-01-03 19:43:27,559: Running test after training batch: 3000
2026-01-03 19:43:27,659: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:43:32,415: WER debug example
  GT : you can see the code at this point as well
  PR : yule end sze the code at this point is will
2026-01-03 19:43:32,444: WER debug example
  GT : how does it keep the cost down
  PR : houde des it hipp the cost get
2026-01-03 19:43:34,068: Val batch 3000: PER (avg): 0.2799 CTC Loss (avg): 27.6809 WER(1gram): 64.47% (n=64) time: 6.508
2026-01-03 19:43:34,068: WER lens: avg_true_words=6.16 avg_pred_words=5.67 max_pred_words=11
2026-01-03 19:43:34,069: t15.2023.08.13 val PER: 0.2640
2026-01-03 19:43:34,069: t15.2023.08.18 val PER: 0.2188
2026-01-03 19:43:34,069: t15.2023.08.20 val PER: 0.2184
2026-01-03 19:43:34,069: t15.2023.08.25 val PER: 0.1958
2026-01-03 19:43:34,069: t15.2023.08.27 val PER: 0.2942
2026-01-03 19:43:34,069: t15.2023.09.01 val PER: 0.1875
2026-01-03 19:43:34,069: t15.2023.09.03 val PER: 0.2945
2026-01-03 19:43:34,069: t15.2023.09.24 val PER: 0.2184
2026-01-03 19:43:34,069: t15.2023.09.29 val PER: 0.2336
2026-01-03 19:43:34,069: t15.2023.10.01 val PER: 0.2860
2026-01-03 19:43:34,069: t15.2023.10.06 val PER: 0.2013
2026-01-03 19:43:34,069: t15.2023.10.08 val PER: 0.3505
2026-01-03 19:43:34,069: t15.2023.10.13 val PER: 0.3382
2026-01-03 19:43:34,070: t15.2023.10.15 val PER: 0.2709
2026-01-03 19:43:34,070: t15.2023.10.20 val PER: 0.2651
2026-01-03 19:43:34,070: t15.2023.10.22 val PER: 0.2027
2026-01-03 19:43:34,070: t15.2023.11.03 val PER: 0.2653
2026-01-03 19:43:34,070: t15.2023.11.04 val PER: 0.0853
2026-01-03 19:43:34,070: t15.2023.11.17 val PER: 0.1306
2026-01-03 19:43:34,070: t15.2023.11.19 val PER: 0.1118
2026-01-03 19:43:34,070: t15.2023.11.26 val PER: 0.2971
2026-01-03 19:43:34,070: t15.2023.12.03 val PER: 0.2679
2026-01-03 19:43:34,070: t15.2023.12.08 val PER: 0.2477
2026-01-03 19:43:34,071: t15.2023.12.10 val PER: 0.1997
2026-01-03 19:43:34,071: t15.2023.12.17 val PER: 0.2661
2026-01-03 19:43:34,071: t15.2023.12.29 val PER: 0.2773
2026-01-03 19:43:34,071: t15.2024.02.25 val PER: 0.2346
2026-01-03 19:43:34,071: t15.2024.03.08 val PER: 0.3528
2026-01-03 19:43:34,071: t15.2024.03.15 val PER: 0.3308
2026-01-03 19:43:34,071: t15.2024.03.17 val PER: 0.2908
2026-01-03 19:43:34,071: t15.2024.05.10 val PER: 0.2957
2026-01-03 19:43:34,071: t15.2024.06.14 val PER: 0.2855
2026-01-03 19:43:34,071: t15.2024.07.19 val PER: 0.4061
2026-01-03 19:43:34,071: t15.2024.07.21 val PER: 0.2345
2026-01-03 19:43:34,071: t15.2024.07.28 val PER: 0.2691
2026-01-03 19:43:34,071: t15.2025.01.10 val PER: 0.4862
2026-01-03 19:43:34,071: t15.2025.01.12 val PER: 0.3172
2026-01-03 19:43:34,071: t15.2025.03.14 val PER: 0.4645
2026-01-03 19:43:34,071: t15.2025.03.16 val PER: 0.3233
2026-01-03 19:43:34,072: t15.2025.03.30 val PER: 0.4977
2026-01-03 19:43:34,072: t15.2025.04.13 val PER: 0.3452
2026-01-03 19:43:34,072: New best val WER(1gram) 67.01% --> 64.47%
2026-01-03 19:43:34,072: Checkpointing model
2026-01-03 19:43:34,676: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 19:43:34,927: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_3000
2026-01-03 19:43:52,218: Train batch 3200: loss: 26.49 grad norm: 69.38 time: 0.076
2026-01-03 19:44:09,496: Train batch 3400: loss: 18.11 grad norm: 54.85 time: 0.049
2026-01-03 19:44:18,385: Running test after training batch: 3500
2026-01-03 19:44:18,513: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:44:23,627: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point will
2026-01-03 19:44:23,655: WER debug example
  GT : how does it keep the cost down
  PR : houde des it kipp thus cussed get
2026-01-03 19:44:25,207: Val batch 3500: PER (avg): 0.2668 CTC Loss (avg): 26.3718 WER(1gram): 67.01% (n=64) time: 6.822
2026-01-03 19:44:25,207: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=11
2026-01-03 19:44:25,207: t15.2023.08.13 val PER: 0.2401
2026-01-03 19:44:25,207: t15.2023.08.18 val PER: 0.2154
2026-01-03 19:44:25,208: t15.2023.08.20 val PER: 0.2137
2026-01-03 19:44:25,208: t15.2023.08.25 val PER: 0.1762
2026-01-03 19:44:25,208: t15.2023.08.27 val PER: 0.2637
2026-01-03 19:44:25,208: t15.2023.09.01 val PER: 0.1786
2026-01-03 19:44:25,208: t15.2023.09.03 val PER: 0.2518
2026-01-03 19:44:25,208: t15.2023.09.24 val PER: 0.2087
2026-01-03 19:44:25,208: t15.2023.09.29 val PER: 0.2202
2026-01-03 19:44:25,208: t15.2023.10.01 val PER: 0.2827
2026-01-03 19:44:25,208: t15.2023.10.06 val PER: 0.1873
2026-01-03 19:44:25,208: t15.2023.10.08 val PER: 0.3315
2026-01-03 19:44:25,208: t15.2023.10.13 val PER: 0.3134
2026-01-03 19:44:25,209: t15.2023.10.15 val PER: 0.2558
2026-01-03 19:44:25,209: t15.2023.10.20 val PER: 0.2483
2026-01-03 19:44:25,209: t15.2023.10.22 val PER: 0.2127
2026-01-03 19:44:25,209: t15.2023.11.03 val PER: 0.2626
2026-01-03 19:44:25,209: t15.2023.11.04 val PER: 0.0614
2026-01-03 19:44:25,209: t15.2023.11.17 val PER: 0.1182
2026-01-03 19:44:25,209: t15.2023.11.19 val PER: 0.1018
2026-01-03 19:44:25,209: t15.2023.11.26 val PER: 0.2819
2026-01-03 19:44:25,209: t15.2023.12.03 val PER: 0.2300
2026-01-03 19:44:25,209: t15.2023.12.08 val PER: 0.2503
2026-01-03 19:44:25,209: t15.2023.12.10 val PER: 0.2024
2026-01-03 19:44:25,209: t15.2023.12.17 val PER: 0.2599
2026-01-03 19:44:25,209: t15.2023.12.29 val PER: 0.2601
2026-01-03 19:44:25,209: t15.2024.02.25 val PER: 0.2149
2026-01-03 19:44:25,209: t15.2024.03.08 val PER: 0.3471
2026-01-03 19:44:25,210: t15.2024.03.15 val PER: 0.3171
2026-01-03 19:44:25,210: t15.2024.03.17 val PER: 0.2817
2026-01-03 19:44:25,210: t15.2024.05.10 val PER: 0.2719
2026-01-03 19:44:25,210: t15.2024.06.14 val PER: 0.2681
2026-01-03 19:44:25,210: t15.2024.07.19 val PER: 0.3935
2026-01-03 19:44:25,210: t15.2024.07.21 val PER: 0.2241
2026-01-03 19:44:25,210: t15.2024.07.28 val PER: 0.2706
2026-01-03 19:44:25,210: t15.2025.01.10 val PER: 0.4601
2026-01-03 19:44:25,210: t15.2025.01.12 val PER: 0.3002
2026-01-03 19:44:25,210: t15.2025.03.14 val PER: 0.4482
2026-01-03 19:44:25,210: t15.2025.03.16 val PER: 0.3312
2026-01-03 19:44:25,210: t15.2025.03.30 val PER: 0.4506
2026-01-03 19:44:25,210: t15.2025.04.13 val PER: 0.3310
2026-01-03 19:44:25,450: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_3500
2026-01-03 19:44:34,066: Train batch 3600: loss: 22.50 grad norm: 59.10 time: 0.066
2026-01-03 19:44:51,295: Train batch 3800: loss: 25.63 grad norm: 69.14 time: 0.066
2026-01-03 19:45:08,827: Train batch 4000: loss: 19.75 grad norm: 55.87 time: 0.056
2026-01-03 19:45:08,828: Running test after training batch: 4000
2026-01-03 19:45:08,980: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:45:13,746: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point will
2026-01-03 19:45:13,774: WER debug example
  GT : how does it keep the cost down
  PR : aue des it keep the us it
2026-01-03 19:45:15,349: Val batch 4000: PER (avg): 0.2487 CTC Loss (avg): 24.3624 WER(1gram): 62.94% (n=64) time: 6.521
2026-01-03 19:45:15,349: WER lens: avg_true_words=6.16 avg_pred_words=5.86 max_pred_words=11
2026-01-03 19:45:15,350: t15.2023.08.13 val PER: 0.2287
2026-01-03 19:45:15,350: t15.2023.08.18 val PER: 0.1995
2026-01-03 19:45:15,350: t15.2023.08.20 val PER: 0.2017
2026-01-03 19:45:15,350: t15.2023.08.25 val PER: 0.1566
2026-01-03 19:45:15,350: t15.2023.08.27 val PER: 0.2862
2026-01-03 19:45:15,350: t15.2023.09.01 val PER: 0.1567
2026-01-03 19:45:15,350: t15.2023.09.03 val PER: 0.2387
2026-01-03 19:45:15,350: t15.2023.09.24 val PER: 0.1857
2026-01-03 19:45:15,350: t15.2023.09.29 val PER: 0.2042
2026-01-03 19:45:15,350: t15.2023.10.01 val PER: 0.2655
2026-01-03 19:45:15,350: t15.2023.10.06 val PER: 0.1625
2026-01-03 19:45:15,350: t15.2023.10.08 val PER: 0.3126
2026-01-03 19:45:15,350: t15.2023.10.13 val PER: 0.3041
2026-01-03 19:45:15,351: t15.2023.10.15 val PER: 0.2281
2026-01-03 19:45:15,351: t15.2023.10.20 val PER: 0.2383
2026-01-03 19:45:15,351: t15.2023.10.22 val PER: 0.1882
2026-01-03 19:45:15,351: t15.2023.11.03 val PER: 0.2334
2026-01-03 19:45:15,351: t15.2023.11.04 val PER: 0.0614
2026-01-03 19:45:15,351: t15.2023.11.17 val PER: 0.1058
2026-01-03 19:45:15,351: t15.2023.11.19 val PER: 0.0938
2026-01-03 19:45:15,351: t15.2023.11.26 val PER: 0.2688
2026-01-03 19:45:15,351: t15.2023.12.03 val PER: 0.2342
2026-01-03 19:45:15,351: t15.2023.12.08 val PER: 0.2210
2026-01-03 19:45:15,351: t15.2023.12.10 val PER: 0.1892
2026-01-03 19:45:15,351: t15.2023.12.17 val PER: 0.2370
2026-01-03 19:45:15,352: t15.2023.12.29 val PER: 0.2608
2026-01-03 19:45:15,352: t15.2024.02.25 val PER: 0.2149
2026-01-03 19:45:15,352: t15.2024.03.08 val PER: 0.3457
2026-01-03 19:45:15,352: t15.2024.03.15 val PER: 0.3102
2026-01-03 19:45:15,352: t15.2024.03.17 val PER: 0.2524
2026-01-03 19:45:15,352: t15.2024.05.10 val PER: 0.2660
2026-01-03 19:45:15,352: t15.2024.06.14 val PER: 0.2729
2026-01-03 19:45:15,352: t15.2024.07.19 val PER: 0.3612
2026-01-03 19:45:15,352: t15.2024.07.21 val PER: 0.1931
2026-01-03 19:45:15,352: t15.2024.07.28 val PER: 0.2294
2026-01-03 19:45:15,352: t15.2025.01.10 val PER: 0.4242
2026-01-03 19:45:15,352: t15.2025.01.12 val PER: 0.2779
2026-01-03 19:45:15,352: t15.2025.03.14 val PER: 0.4098
2026-01-03 19:45:15,353: t15.2025.03.16 val PER: 0.3154
2026-01-03 19:45:15,353: t15.2025.03.30 val PER: 0.4069
2026-01-03 19:45:15,353: t15.2025.04.13 val PER: 0.3252
2026-01-03 19:45:15,354: New best val WER(1gram) 64.47% --> 62.94%
2026-01-03 19:45:15,354: Checkpointing model
2026-01-03 19:45:15,942: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 19:45:16,189: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_4000
2026-01-03 19:45:33,792: Train batch 4200: loss: 22.03 grad norm: 62.12 time: 0.079
2026-01-03 19:45:51,238: Train batch 4400: loss: 17.27 grad norm: 54.64 time: 0.066
2026-01-03 19:45:59,938: Running test after training batch: 4500
2026-01-03 19:46:00,068: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:46:04,866: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-03 19:46:04,894: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it hipp the cussed get
2026-01-03 19:46:06,442: Val batch 4500: PER (avg): 0.2382 CTC Loss (avg): 23.1771 WER(1gram): 59.39% (n=64) time: 6.503
2026-01-03 19:46:06,442: WER lens: avg_true_words=6.16 avg_pred_words=6.00 max_pred_words=11
2026-01-03 19:46:06,442: t15.2023.08.13 val PER: 0.2079
2026-01-03 19:46:06,443: t15.2023.08.18 val PER: 0.1894
2026-01-03 19:46:06,443: t15.2023.08.20 val PER: 0.1827
2026-01-03 19:46:06,443: t15.2023.08.25 val PER: 0.1370
2026-01-03 19:46:06,443: t15.2023.08.27 val PER: 0.2572
2026-01-03 19:46:06,443: t15.2023.09.01 val PER: 0.1502
2026-01-03 19:46:06,443: t15.2023.09.03 val PER: 0.2435
2026-01-03 19:46:06,443: t15.2023.09.24 val PER: 0.1796
2026-01-03 19:46:06,443: t15.2023.09.29 val PER: 0.2036
2026-01-03 19:46:06,443: t15.2023.10.01 val PER: 0.2675
2026-01-03 19:46:06,443: t15.2023.10.06 val PER: 0.1529
2026-01-03 19:46:06,443: t15.2023.10.08 val PER: 0.3166
2026-01-03 19:46:06,444: t15.2023.10.13 val PER: 0.2987
2026-01-03 19:46:06,444: t15.2023.10.15 val PER: 0.2353
2026-01-03 19:46:06,444: t15.2023.10.20 val PER: 0.2315
2026-01-03 19:46:06,444: t15.2023.10.22 val PER: 0.1882
2026-01-03 19:46:06,444: t15.2023.11.03 val PER: 0.2429
2026-01-03 19:46:06,444: t15.2023.11.04 val PER: 0.0717
2026-01-03 19:46:06,444: t15.2023.11.17 val PER: 0.0964
2026-01-03 19:46:06,444: t15.2023.11.19 val PER: 0.0878
2026-01-03 19:46:06,444: t15.2023.11.26 val PER: 0.2623
2026-01-03 19:46:06,444: t15.2023.12.03 val PER: 0.2111
2026-01-03 19:46:06,444: t15.2023.12.08 val PER: 0.2157
2026-01-03 19:46:06,444: t15.2023.12.10 val PER: 0.1787
2026-01-03 19:46:06,444: t15.2023.12.17 val PER: 0.2277
2026-01-03 19:46:06,444: t15.2023.12.29 val PER: 0.2416
2026-01-03 19:46:06,444: t15.2024.02.25 val PER: 0.1994
2026-01-03 19:46:06,445: t15.2024.03.08 val PER: 0.3058
2026-01-03 19:46:06,445: t15.2024.03.15 val PER: 0.2833
2026-01-03 19:46:06,445: t15.2024.03.17 val PER: 0.2476
2026-01-03 19:46:06,445: t15.2024.05.10 val PER: 0.2556
2026-01-03 19:46:06,445: t15.2024.06.14 val PER: 0.2413
2026-01-03 19:46:06,445: t15.2024.07.19 val PER: 0.3421
2026-01-03 19:46:06,446: t15.2024.07.21 val PER: 0.1786
2026-01-03 19:46:06,446: t15.2024.07.28 val PER: 0.2206
2026-01-03 19:46:06,446: t15.2025.01.10 val PER: 0.4091
2026-01-03 19:46:06,446: t15.2025.01.12 val PER: 0.2579
2026-01-03 19:46:06,446: t15.2025.03.14 val PER: 0.4127
2026-01-03 19:46:06,447: t15.2025.03.16 val PER: 0.2906
2026-01-03 19:46:06,447: t15.2025.03.30 val PER: 0.3966
2026-01-03 19:46:06,447: t15.2025.04.13 val PER: 0.2981
2026-01-03 19:46:06,447: New best val WER(1gram) 62.94% --> 59.39%
2026-01-03 19:46:06,448: Checkpointing model
2026-01-03 19:46:07,067: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 19:46:07,316: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_4500
2026-01-03 19:46:15,929: Train batch 4600: loss: 20.42 grad norm: 61.76 time: 0.062
2026-01-03 19:46:34,329: Train batch 4800: loss: 13.68 grad norm: 51.42 time: 0.063
2026-01-03 19:46:52,139: Train batch 5000: loss: 31.60 grad norm: 82.83 time: 0.064
2026-01-03 19:46:52,140: Running test after training batch: 5000
2026-01-03 19:46:52,254: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:46:57,041: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point as wheel
2026-01-03 19:46:57,071: WER debug example
  GT : how does it keep the cost down
  PR : how just it yip the cussed get
2026-01-03 19:46:58,664: Val batch 5000: PER (avg): 0.2276 CTC Loss (avg): 22.0464 WER(1gram): 60.15% (n=64) time: 6.524
2026-01-03 19:46:58,664: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=11
2026-01-03 19:46:58,664: t15.2023.08.13 val PER: 0.1975
2026-01-03 19:46:58,665: t15.2023.08.18 val PER: 0.1735
2026-01-03 19:46:58,665: t15.2023.08.20 val PER: 0.1779
2026-01-03 19:46:58,665: t15.2023.08.25 val PER: 0.1401
2026-01-03 19:46:58,665: t15.2023.08.27 val PER: 0.2363
2026-01-03 19:46:58,665: t15.2023.09.01 val PER: 0.1420
2026-01-03 19:46:58,665: t15.2023.09.03 val PER: 0.2340
2026-01-03 19:46:58,665: t15.2023.09.24 val PER: 0.1820
2026-01-03 19:46:58,665: t15.2023.09.29 val PER: 0.1800
2026-01-03 19:46:58,665: t15.2023.10.01 val PER: 0.2384
2026-01-03 19:46:58,665: t15.2023.10.06 val PER: 0.1507
2026-01-03 19:46:58,665: t15.2023.10.08 val PER: 0.3018
2026-01-03 19:46:58,666: t15.2023.10.13 val PER: 0.2847
2026-01-03 19:46:58,666: t15.2023.10.15 val PER: 0.2274
2026-01-03 19:46:58,666: t15.2023.10.20 val PER: 0.2282
2026-01-03 19:46:58,666: t15.2023.10.22 val PER: 0.1648
2026-01-03 19:46:58,666: t15.2023.11.03 val PER: 0.2212
2026-01-03 19:46:58,666: t15.2023.11.04 val PER: 0.0410
2026-01-03 19:46:58,666: t15.2023.11.17 val PER: 0.0886
2026-01-03 19:46:58,666: t15.2023.11.19 val PER: 0.0758
2026-01-03 19:46:58,666: t15.2023.11.26 val PER: 0.2428
2026-01-03 19:46:58,666: t15.2023.12.03 val PER: 0.2027
2026-01-03 19:46:58,666: t15.2023.12.08 val PER: 0.1984
2026-01-03 19:46:58,666: t15.2023.12.10 val PER: 0.1564
2026-01-03 19:46:58,666: t15.2023.12.17 val PER: 0.2235
2026-01-03 19:46:58,666: t15.2023.12.29 val PER: 0.2237
2026-01-03 19:46:58,666: t15.2024.02.25 val PER: 0.1812
2026-01-03 19:46:58,666: t15.2024.03.08 val PER: 0.3058
2026-01-03 19:46:58,667: t15.2024.03.15 val PER: 0.2814
2026-01-03 19:46:58,667: t15.2024.03.17 val PER: 0.2406
2026-01-03 19:46:58,667: t15.2024.05.10 val PER: 0.2437
2026-01-03 19:46:58,667: t15.2024.06.14 val PER: 0.2461
2026-01-03 19:46:58,667: t15.2024.07.19 val PER: 0.3329
2026-01-03 19:46:58,667: t15.2024.07.21 val PER: 0.1876
2026-01-03 19:46:58,667: t15.2024.07.28 val PER: 0.2169
2026-01-03 19:46:58,667: t15.2025.01.10 val PER: 0.3967
2026-01-03 19:46:58,667: t15.2025.01.12 val PER: 0.2463
2026-01-03 19:46:58,667: t15.2025.03.14 val PER: 0.3964
2026-01-03 19:46:58,667: t15.2025.03.16 val PER: 0.2840
2026-01-03 19:46:58,667: t15.2025.03.30 val PER: 0.3943
2026-01-03 19:46:58,667: t15.2025.04.13 val PER: 0.3081
2026-01-03 19:46:58,907: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_5000
2026-01-03 19:47:16,441: Train batch 5200: loss: 16.12 grad norm: 54.99 time: 0.052
2026-01-03 19:47:33,732: Train batch 5400: loss: 17.97 grad norm: 61.49 time: 0.068
2026-01-03 19:47:42,445: Running test after training batch: 5500
2026-01-03 19:47:42,653: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:47:47,402: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 19:47:47,430: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the rust get
2026-01-03 19:47:49,003: Val batch 5500: PER (avg): 0.2168 CTC Loss (avg): 21.3155 WER(1gram): 55.58% (n=64) time: 6.558
2026-01-03 19:47:49,004: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-03 19:47:49,004: t15.2023.08.13 val PER: 0.1809
2026-01-03 19:47:49,004: t15.2023.08.18 val PER: 0.1609
2026-01-03 19:47:49,004: t15.2023.08.20 val PER: 0.1628
2026-01-03 19:47:49,004: t15.2023.08.25 val PER: 0.1220
2026-01-03 19:47:49,004: t15.2023.08.27 val PER: 0.2508
2026-01-03 19:47:49,004: t15.2023.09.01 val PER: 0.1412
2026-01-03 19:47:49,004: t15.2023.09.03 val PER: 0.2185
2026-01-03 19:47:49,005: t15.2023.09.24 val PER: 0.1602
2026-01-03 19:47:49,005: t15.2023.09.29 val PER: 0.1761
2026-01-03 19:47:49,005: t15.2023.10.01 val PER: 0.2332
2026-01-03 19:47:49,005: t15.2023.10.06 val PER: 0.1410
2026-01-03 19:47:49,005: t15.2023.10.08 val PER: 0.2950
2026-01-03 19:47:49,005: t15.2023.10.13 val PER: 0.2715
2026-01-03 19:47:49,005: t15.2023.10.15 val PER: 0.2136
2026-01-03 19:47:49,005: t15.2023.10.20 val PER: 0.2282
2026-01-03 19:47:49,005: t15.2023.10.22 val PER: 0.1548
2026-01-03 19:47:49,005: t15.2023.11.03 val PER: 0.2232
2026-01-03 19:47:49,005: t15.2023.11.04 val PER: 0.0648
2026-01-03 19:47:49,005: t15.2023.11.17 val PER: 0.0840
2026-01-03 19:47:49,005: t15.2023.11.19 val PER: 0.0758
2026-01-03 19:47:49,006: t15.2023.11.26 val PER: 0.2152
2026-01-03 19:47:49,006: t15.2023.12.03 val PER: 0.1870
2026-01-03 19:47:49,006: t15.2023.12.08 val PER: 0.1924
2026-01-03 19:47:49,006: t15.2023.12.10 val PER: 0.1590
2026-01-03 19:47:49,006: t15.2023.12.17 val PER: 0.2256
2026-01-03 19:47:49,006: t15.2023.12.29 val PER: 0.2135
2026-01-03 19:47:49,006: t15.2024.02.25 val PER: 0.1742
2026-01-03 19:47:49,006: t15.2024.03.08 val PER: 0.2873
2026-01-03 19:47:49,006: t15.2024.03.15 val PER: 0.2589
2026-01-03 19:47:49,006: t15.2024.03.17 val PER: 0.2183
2026-01-03 19:47:49,006: t15.2024.05.10 val PER: 0.2348
2026-01-03 19:47:49,006: t15.2024.06.14 val PER: 0.2397
2026-01-03 19:47:49,006: t15.2024.07.19 val PER: 0.3164
2026-01-03 19:47:49,006: t15.2024.07.21 val PER: 0.1738
2026-01-03 19:47:49,006: t15.2024.07.28 val PER: 0.2184
2026-01-03 19:47:49,007: t15.2025.01.10 val PER: 0.3733
2026-01-03 19:47:49,007: t15.2025.01.12 val PER: 0.2502
2026-01-03 19:47:49,007: t15.2025.03.14 val PER: 0.3713
2026-01-03 19:47:49,007: t15.2025.03.16 val PER: 0.2709
2026-01-03 19:47:49,007: t15.2025.03.30 val PER: 0.3667
2026-01-03 19:47:49,007: t15.2025.04.13 val PER: 0.2825
2026-01-03 19:47:49,008: New best val WER(1gram) 59.39% --> 55.58%
2026-01-03 19:47:49,009: Checkpointing model
2026-01-03 19:47:49,617: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 19:47:49,866: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_5500
2026-01-03 19:47:58,588: Train batch 5600: loss: 19.74 grad norm: 70.20 time: 0.061
2026-01-03 19:48:15,948: Train batch 5800: loss: 13.84 grad norm: 60.43 time: 0.081
2026-01-03 19:48:33,554: Train batch 6000: loss: 14.48 grad norm: 58.14 time: 0.049
2026-01-03 19:48:33,555: Running test after training batch: 6000
2026-01-03 19:48:33,664: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:48:38,557: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 19:48:38,586: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nett
2026-01-03 19:48:40,220: Val batch 6000: PER (avg): 0.2112 CTC Loss (avg): 20.9751 WER(1gram): 58.12% (n=64) time: 6.665
2026-01-03 19:48:40,221: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 19:48:40,221: t15.2023.08.13 val PER: 0.1736
2026-01-03 19:48:40,221: t15.2023.08.18 val PER: 0.1660
2026-01-03 19:48:40,221: t15.2023.08.20 val PER: 0.1652
2026-01-03 19:48:40,221: t15.2023.08.25 val PER: 0.1175
2026-01-03 19:48:40,221: t15.2023.08.27 val PER: 0.2524
2026-01-03 19:48:40,222: t15.2023.09.01 val PER: 0.1323
2026-01-03 19:48:40,222: t15.2023.09.03 val PER: 0.2138
2026-01-03 19:48:40,222: t15.2023.09.24 val PER: 0.1602
2026-01-03 19:48:40,222: t15.2023.09.29 val PER: 0.1780
2026-01-03 19:48:40,222: t15.2023.10.01 val PER: 0.2266
2026-01-03 19:48:40,222: t15.2023.10.06 val PER: 0.1346
2026-01-03 19:48:40,222: t15.2023.10.08 val PER: 0.2950
2026-01-03 19:48:40,222: t15.2023.10.13 val PER: 0.2684
2026-01-03 19:48:40,222: t15.2023.10.15 val PER: 0.2103
2026-01-03 19:48:40,222: t15.2023.10.20 val PER: 0.2148
2026-01-03 19:48:40,222: t15.2023.10.22 val PER: 0.1715
2026-01-03 19:48:40,222: t15.2023.11.03 val PER: 0.2205
2026-01-03 19:48:40,223: t15.2023.11.04 val PER: 0.0648
2026-01-03 19:48:40,223: t15.2023.11.17 val PER: 0.0747
2026-01-03 19:48:40,223: t15.2023.11.19 val PER: 0.0719
2026-01-03 19:48:40,223: t15.2023.11.26 val PER: 0.2152
2026-01-03 19:48:40,223: t15.2023.12.03 val PER: 0.1838
2026-01-03 19:48:40,223: t15.2023.12.08 val PER: 0.1731
2026-01-03 19:48:40,223: t15.2023.12.10 val PER: 0.1472
2026-01-03 19:48:40,223: t15.2023.12.17 val PER: 0.1923
2026-01-03 19:48:40,223: t15.2023.12.29 val PER: 0.2141
2026-01-03 19:48:40,224: t15.2024.02.25 val PER: 0.1629
2026-01-03 19:48:40,224: t15.2024.03.08 val PER: 0.2945
2026-01-03 19:48:40,224: t15.2024.03.15 val PER: 0.2664
2026-01-03 19:48:40,224: t15.2024.03.17 val PER: 0.2064
2026-01-03 19:48:40,224: t15.2024.05.10 val PER: 0.2259
2026-01-03 19:48:40,224: t15.2024.06.14 val PER: 0.2145
2026-01-03 19:48:40,224: t15.2024.07.19 val PER: 0.3059
2026-01-03 19:48:40,224: t15.2024.07.21 val PER: 0.1648
2026-01-03 19:48:40,224: t15.2024.07.28 val PER: 0.2051
2026-01-03 19:48:40,225: t15.2025.01.10 val PER: 0.3595
2026-01-03 19:48:40,225: t15.2025.01.12 val PER: 0.2248
2026-01-03 19:48:40,225: t15.2025.03.14 val PER: 0.3683
2026-01-03 19:48:40,225: t15.2025.03.16 val PER: 0.2736
2026-01-03 19:48:40,225: t15.2025.03.30 val PER: 0.3690
2026-01-03 19:48:40,225: t15.2025.04.13 val PER: 0.2753
2026-01-03 19:48:40,463: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_6000
2026-01-03 19:48:58,242: Train batch 6200: loss: 16.41 grad norm: 61.26 time: 0.070
2026-01-03 19:49:15,530: Train batch 6400: loss: 18.19 grad norm: 64.55 time: 0.063
2026-01-03 19:49:24,059: Running test after training batch: 6500
2026-01-03 19:49:24,200: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:49:28,995: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point as will
2026-01-03 19:49:29,024: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-03 19:49:30,617: Val batch 6500: PER (avg): 0.2034 CTC Loss (avg): 20.0040 WER(1gram): 52.54% (n=64) time: 6.557
2026-01-03 19:49:30,617: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 19:49:30,617: t15.2023.08.13 val PER: 0.1642
2026-01-03 19:49:30,618: t15.2023.08.18 val PER: 0.1484
2026-01-03 19:49:30,618: t15.2023.08.20 val PER: 0.1541
2026-01-03 19:49:30,618: t15.2023.08.25 val PER: 0.1130
2026-01-03 19:49:30,618: t15.2023.08.27 val PER: 0.2395
2026-01-03 19:49:30,618: t15.2023.09.01 val PER: 0.1226
2026-01-03 19:49:30,618: t15.2023.09.03 val PER: 0.2078
2026-01-03 19:49:30,618: t15.2023.09.24 val PER: 0.1590
2026-01-03 19:49:30,618: t15.2023.09.29 val PER: 0.1736
2026-01-03 19:49:30,618: t15.2023.10.01 val PER: 0.2147
2026-01-03 19:49:30,618: t15.2023.10.06 val PER: 0.1206
2026-01-03 19:49:30,618: t15.2023.10.08 val PER: 0.2936
2026-01-03 19:49:30,618: t15.2023.10.13 val PER: 0.2676
2026-01-03 19:49:30,618: t15.2023.10.15 val PER: 0.2109
2026-01-03 19:49:30,618: t15.2023.10.20 val PER: 0.2081
2026-01-03 19:49:30,619: t15.2023.10.22 val PER: 0.1492
2026-01-03 19:49:30,619: t15.2023.11.03 val PER: 0.2083
2026-01-03 19:49:30,619: t15.2023.11.04 val PER: 0.0444
2026-01-03 19:49:30,619: t15.2023.11.17 val PER: 0.0544
2026-01-03 19:49:30,619: t15.2023.11.19 val PER: 0.0699
2026-01-03 19:49:30,619: t15.2023.11.26 val PER: 0.2145
2026-01-03 19:49:30,619: t15.2023.12.03 val PER: 0.1786
2026-01-03 19:49:30,619: t15.2023.12.08 val PER: 0.1678
2026-01-03 19:49:30,619: t15.2023.12.10 val PER: 0.1445
2026-01-03 19:49:30,619: t15.2023.12.17 val PER: 0.1819
2026-01-03 19:49:30,619: t15.2023.12.29 val PER: 0.1970
2026-01-03 19:49:30,619: t15.2024.02.25 val PER: 0.1657
2026-01-03 19:49:30,619: t15.2024.03.08 val PER: 0.2916
2026-01-03 19:49:30,620: t15.2024.03.15 val PER: 0.2652
2026-01-03 19:49:30,620: t15.2024.03.17 val PER: 0.2064
2026-01-03 19:49:30,620: t15.2024.05.10 val PER: 0.2229
2026-01-03 19:49:30,620: t15.2024.06.14 val PER: 0.2098
2026-01-03 19:49:30,620: t15.2024.07.19 val PER: 0.3131
2026-01-03 19:49:30,620: t15.2024.07.21 val PER: 0.1517
2026-01-03 19:49:30,620: t15.2024.07.28 val PER: 0.1824
2026-01-03 19:49:30,620: t15.2025.01.10 val PER: 0.3554
2026-01-03 19:49:30,620: t15.2025.01.12 val PER: 0.2125
2026-01-03 19:49:30,620: t15.2025.03.14 val PER: 0.3876
2026-01-03 19:49:30,620: t15.2025.03.16 val PER: 0.2448
2026-01-03 19:49:30,620: t15.2025.03.30 val PER: 0.3517
2026-01-03 19:49:30,620: t15.2025.04.13 val PER: 0.2611
2026-01-03 19:49:30,622: New best val WER(1gram) 55.58% --> 52.54%
2026-01-03 19:49:30,622: Checkpointing model
2026-01-03 19:49:31,209: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 19:49:31,457: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_6500
2026-01-03 19:49:39,911: Train batch 6600: loss: 12.78 grad norm: 57.33 time: 0.045
2026-01-03 19:49:57,441: Train batch 6800: loss: 15.38 grad norm: 57.32 time: 0.048
2026-01-03 19:50:14,992: Train batch 7000: loss: 17.59 grad norm: 66.33 time: 0.060
2026-01-03 19:50:14,993: Running test after training batch: 7000
2026-01-03 19:50:15,144: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:50:19,924: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 19:50:19,953: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-03 19:50:21,587: Val batch 7000: PER (avg): 0.1929 CTC Loss (avg): 19.2142 WER(1gram): 51.78% (n=64) time: 6.594
2026-01-03 19:50:21,587: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-03 19:50:21,588: t15.2023.08.13 val PER: 0.1435
2026-01-03 19:50:21,588: t15.2023.08.18 val PER: 0.1358
2026-01-03 19:50:21,588: t15.2023.08.20 val PER: 0.1430
2026-01-03 19:50:21,588: t15.2023.08.25 val PER: 0.1039
2026-01-03 19:50:21,588: t15.2023.08.27 val PER: 0.2219
2026-01-03 19:50:21,588: t15.2023.09.01 val PER: 0.1112
2026-01-03 19:50:21,588: t15.2023.09.03 val PER: 0.1888
2026-01-03 19:50:21,588: t15.2023.09.24 val PER: 0.1578
2026-01-03 19:50:21,588: t15.2023.09.29 val PER: 0.1672
2026-01-03 19:50:21,588: t15.2023.10.01 val PER: 0.2127
2026-01-03 19:50:21,588: t15.2023.10.06 val PER: 0.1173
2026-01-03 19:50:21,588: t15.2023.10.08 val PER: 0.2760
2026-01-03 19:50:21,589: t15.2023.10.13 val PER: 0.2514
2026-01-03 19:50:21,589: t15.2023.10.15 val PER: 0.1984
2026-01-03 19:50:21,589: t15.2023.10.20 val PER: 0.2148
2026-01-03 19:50:21,589: t15.2023.10.22 val PER: 0.1403
2026-01-03 19:50:21,589: t15.2023.11.03 val PER: 0.1974
2026-01-03 19:50:21,589: t15.2023.11.04 val PER: 0.0478
2026-01-03 19:50:21,589: t15.2023.11.17 val PER: 0.0638
2026-01-03 19:50:21,589: t15.2023.11.19 val PER: 0.0559
2026-01-03 19:50:21,589: t15.2023.11.26 val PER: 0.1942
2026-01-03 19:50:21,589: t15.2023.12.03 val PER: 0.1607
2026-01-03 19:50:21,589: t15.2023.12.08 val PER: 0.1585
2026-01-03 19:50:21,589: t15.2023.12.10 val PER: 0.1498
2026-01-03 19:50:21,589: t15.2023.12.17 val PER: 0.1778
2026-01-03 19:50:21,589: t15.2023.12.29 val PER: 0.1990
2026-01-03 19:50:21,589: t15.2024.02.25 val PER: 0.1559
2026-01-03 19:50:21,590: t15.2024.03.08 val PER: 0.2760
2026-01-03 19:50:21,590: t15.2024.03.15 val PER: 0.2433
2026-01-03 19:50:21,590: t15.2024.03.17 val PER: 0.1897
2026-01-03 19:50:21,590: t15.2024.05.10 val PER: 0.1887
2026-01-03 19:50:21,590: t15.2024.06.14 val PER: 0.2161
2026-01-03 19:50:21,590: t15.2024.07.19 val PER: 0.2966
2026-01-03 19:50:21,590: t15.2024.07.21 val PER: 0.1338
2026-01-03 19:50:21,590: t15.2024.07.28 val PER: 0.1794
2026-01-03 19:50:21,590: t15.2025.01.10 val PER: 0.3554
2026-01-03 19:50:21,590: t15.2025.01.12 val PER: 0.2071
2026-01-03 19:50:21,590: t15.2025.03.14 val PER: 0.3447
2026-01-03 19:50:21,590: t15.2025.03.16 val PER: 0.2382
2026-01-03 19:50:21,590: t15.2025.03.30 val PER: 0.3529
2026-01-03 19:50:21,590: t15.2025.04.13 val PER: 0.2653
2026-01-03 19:50:21,591: New best val WER(1gram) 52.54% --> 51.78%
2026-01-03 19:50:21,591: Checkpointing model
2026-01-03 19:50:22,201: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 19:50:22,447: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_7000
2026-01-03 19:50:39,955: Train batch 7200: loss: 14.39 grad norm: 58.70 time: 0.078
2026-01-03 19:50:57,282: Train batch 7400: loss: 13.50 grad norm: 54.60 time: 0.076
2026-01-03 19:51:05,830: Running test after training batch: 7500
2026-01-03 19:51:05,934: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:51:10,861: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point us will
2026-01-03 19:51:10,890: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost nett
2026-01-03 19:51:12,538: Val batch 7500: PER (avg): 0.1903 CTC Loss (avg): 18.7859 WER(1gram): 54.57% (n=64) time: 6.708
2026-01-03 19:51:12,538: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-03 19:51:12,538: t15.2023.08.13 val PER: 0.1466
2026-01-03 19:51:12,539: t15.2023.08.18 val PER: 0.1375
2026-01-03 19:51:12,539: t15.2023.08.20 val PER: 0.1461
2026-01-03 19:51:12,539: t15.2023.08.25 val PER: 0.1069
2026-01-03 19:51:12,539: t15.2023.08.27 val PER: 0.2074
2026-01-03 19:51:12,539: t15.2023.09.01 val PER: 0.1177
2026-01-03 19:51:12,539: t15.2023.09.03 val PER: 0.1853
2026-01-03 19:51:12,539: t15.2023.09.24 val PER: 0.1505
2026-01-03 19:51:12,539: t15.2023.09.29 val PER: 0.1570
2026-01-03 19:51:12,539: t15.2023.10.01 val PER: 0.1982
2026-01-03 19:51:12,540: t15.2023.10.06 val PER: 0.1173
2026-01-03 19:51:12,540: t15.2023.10.08 val PER: 0.2639
2026-01-03 19:51:12,540: t15.2023.10.13 val PER: 0.2583
2026-01-03 19:51:12,540: t15.2023.10.15 val PER: 0.1925
2026-01-03 19:51:12,540: t15.2023.10.20 val PER: 0.1879
2026-01-03 19:51:12,540: t15.2023.10.22 val PER: 0.1370
2026-01-03 19:51:12,540: t15.2023.11.03 val PER: 0.2001
2026-01-03 19:51:12,540: t15.2023.11.04 val PER: 0.0512
2026-01-03 19:51:12,540: t15.2023.11.17 val PER: 0.0715
2026-01-03 19:51:12,540: t15.2023.11.19 val PER: 0.0599
2026-01-03 19:51:12,541: t15.2023.11.26 val PER: 0.1833
2026-01-03 19:51:12,541: t15.2023.12.03 val PER: 0.1618
2026-01-03 19:51:12,541: t15.2023.12.08 val PER: 0.1505
2026-01-03 19:51:12,541: t15.2023.12.10 val PER: 0.1406
2026-01-03 19:51:12,541: t15.2023.12.17 val PER: 0.1757
2026-01-03 19:51:12,541: t15.2023.12.29 val PER: 0.1867
2026-01-03 19:51:12,541: t15.2024.02.25 val PER: 0.1475
2026-01-03 19:51:12,541: t15.2024.03.08 val PER: 0.2788
2026-01-03 19:51:12,541: t15.2024.03.15 val PER: 0.2495
2026-01-03 19:51:12,542: t15.2024.03.17 val PER: 0.1799
2026-01-03 19:51:12,542: t15.2024.05.10 val PER: 0.2065
2026-01-03 19:51:12,542: t15.2024.06.14 val PER: 0.1893
2026-01-03 19:51:12,542: t15.2024.07.19 val PER: 0.2960
2026-01-03 19:51:12,542: t15.2024.07.21 val PER: 0.1352
2026-01-03 19:51:12,542: t15.2024.07.28 val PER: 0.1816
2026-01-03 19:51:12,542: t15.2025.01.10 val PER: 0.3471
2026-01-03 19:51:12,542: t15.2025.01.12 val PER: 0.2032
2026-01-03 19:51:12,542: t15.2025.03.14 val PER: 0.3609
2026-01-03 19:51:12,543: t15.2025.03.16 val PER: 0.2565
2026-01-03 19:51:12,543: t15.2025.03.30 val PER: 0.3391
2026-01-03 19:51:12,543: t15.2025.04.13 val PER: 0.2568
2026-01-03 19:51:12,783: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_7500
2026-01-03 19:51:21,346: Train batch 7600: loss: 15.66 grad norm: 58.08 time: 0.068
2026-01-03 19:51:38,834: Train batch 7800: loss: 14.64 grad norm: 57.45 time: 0.056
2026-01-03 19:51:56,309: Train batch 8000: loss: 11.25 grad norm: 51.04 time: 0.071
2026-01-03 19:51:56,309: Running test after training batch: 8000
2026-01-03 19:51:56,406: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:52:01,135: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-03 19:52:01,165: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-03 19:52:02,819: Val batch 8000: PER (avg): 0.1862 CTC Loss (avg): 18.2655 WER(1gram): 53.30% (n=64) time: 6.510
2026-01-03 19:52:02,820: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-03 19:52:02,820: t15.2023.08.13 val PER: 0.1507
2026-01-03 19:52:02,820: t15.2023.08.18 val PER: 0.1249
2026-01-03 19:52:02,820: t15.2023.08.20 val PER: 0.1334
2026-01-03 19:52:02,820: t15.2023.08.25 val PER: 0.1099
2026-01-03 19:52:02,820: t15.2023.08.27 val PER: 0.2154
2026-01-03 19:52:02,820: t15.2023.09.01 val PER: 0.1128
2026-01-03 19:52:02,820: t15.2023.09.03 val PER: 0.1924
2026-01-03 19:52:02,821: t15.2023.09.24 val PER: 0.1517
2026-01-03 19:52:02,821: t15.2023.09.29 val PER: 0.1525
2026-01-03 19:52:02,821: t15.2023.10.01 val PER: 0.2074
2026-01-03 19:52:02,821: t15.2023.10.06 val PER: 0.1044
2026-01-03 19:52:02,821: t15.2023.10.08 val PER: 0.2815
2026-01-03 19:52:02,821: t15.2023.10.13 val PER: 0.2452
2026-01-03 19:52:02,821: t15.2023.10.15 val PER: 0.1991
2026-01-03 19:52:02,821: t15.2023.10.20 val PER: 0.1913
2026-01-03 19:52:02,822: t15.2023.10.22 val PER: 0.1414
2026-01-03 19:52:02,822: t15.2023.11.03 val PER: 0.2008
2026-01-03 19:52:02,822: t15.2023.11.04 val PER: 0.0410
2026-01-03 19:52:02,822: t15.2023.11.17 val PER: 0.0591
2026-01-03 19:52:02,822: t15.2023.11.19 val PER: 0.0579
2026-01-03 19:52:02,822: t15.2023.11.26 val PER: 0.1833
2026-01-03 19:52:02,822: t15.2023.12.03 val PER: 0.1597
2026-01-03 19:52:02,822: t15.2023.12.08 val PER: 0.1411
2026-01-03 19:52:02,822: t15.2023.12.10 val PER: 0.1288
2026-01-03 19:52:02,822: t15.2023.12.17 val PER: 0.1705
2026-01-03 19:52:02,823: t15.2023.12.29 val PER: 0.1778
2026-01-03 19:52:02,823: t15.2024.02.25 val PER: 0.1433
2026-01-03 19:52:02,823: t15.2024.03.08 val PER: 0.2774
2026-01-03 19:52:02,823: t15.2024.03.15 val PER: 0.2458
2026-01-03 19:52:02,823: t15.2024.03.17 val PER: 0.1736
2026-01-03 19:52:02,823: t15.2024.05.10 val PER: 0.2006
2026-01-03 19:52:02,823: t15.2024.06.14 val PER: 0.2003
2026-01-03 19:52:02,823: t15.2024.07.19 val PER: 0.2953
2026-01-03 19:52:02,823: t15.2024.07.21 val PER: 0.1262
2026-01-03 19:52:02,823: t15.2024.07.28 val PER: 0.1574
2026-01-03 19:52:02,823: t15.2025.01.10 val PER: 0.3306
2026-01-03 19:52:02,823: t15.2025.01.12 val PER: 0.1932
2026-01-03 19:52:02,823: t15.2025.03.14 val PER: 0.3713
2026-01-03 19:52:02,824: t15.2025.03.16 val PER: 0.2317
2026-01-03 19:52:02,824: t15.2025.03.30 val PER: 0.3425
2026-01-03 19:52:02,824: t15.2025.04.13 val PER: 0.2525
2026-01-03 19:52:03,060: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_8000
2026-01-03 19:52:20,527: Train batch 8200: loss: 10.21 grad norm: 51.64 time: 0.054
2026-01-03 19:52:37,977: Train batch 8400: loss: 9.75 grad norm: 44.05 time: 0.063
2026-01-03 19:52:46,752: Running test after training batch: 8500
2026-01-03 19:52:46,845: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:52:51,869: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 19:52:51,899: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost ent
2026-01-03 19:52:53,586: Val batch 8500: PER (avg): 0.1807 CTC Loss (avg): 17.8603 WER(1gram): 50.51% (n=64) time: 6.834
2026-01-03 19:52:53,587: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 19:52:53,587: t15.2023.08.13 val PER: 0.1351
2026-01-03 19:52:53,587: t15.2023.08.18 val PER: 0.1333
2026-01-03 19:52:53,587: t15.2023.08.20 val PER: 0.1350
2026-01-03 19:52:53,587: t15.2023.08.25 val PER: 0.1175
2026-01-03 19:52:53,588: t15.2023.08.27 val PER: 0.2090
2026-01-03 19:52:53,588: t15.2023.09.01 val PER: 0.1080
2026-01-03 19:52:53,588: t15.2023.09.03 val PER: 0.2031
2026-01-03 19:52:53,588: t15.2023.09.24 val PER: 0.1420
2026-01-03 19:52:53,588: t15.2023.09.29 val PER: 0.1538
2026-01-03 19:52:53,588: t15.2023.10.01 val PER: 0.1889
2026-01-03 19:52:53,588: t15.2023.10.06 val PER: 0.1033
2026-01-03 19:52:53,588: t15.2023.10.08 val PER: 0.2639
2026-01-03 19:52:53,588: t15.2023.10.13 val PER: 0.2382
2026-01-03 19:52:53,588: t15.2023.10.15 val PER: 0.1826
2026-01-03 19:52:53,589: t15.2023.10.20 val PER: 0.2013
2026-01-03 19:52:53,589: t15.2023.10.22 val PER: 0.1414
2026-01-03 19:52:53,589: t15.2023.11.03 val PER: 0.2015
2026-01-03 19:52:53,589: t15.2023.11.04 val PER: 0.0546
2026-01-03 19:52:53,589: t15.2023.11.17 val PER: 0.0638
2026-01-03 19:52:53,589: t15.2023.11.19 val PER: 0.0559
2026-01-03 19:52:53,589: t15.2023.11.26 val PER: 0.1775
2026-01-03 19:52:53,589: t15.2023.12.03 val PER: 0.1492
2026-01-03 19:52:53,589: t15.2023.12.08 val PER: 0.1391
2026-01-03 19:52:53,590: t15.2023.12.10 val PER: 0.1222
2026-01-03 19:52:53,590: t15.2023.12.17 val PER: 0.1622
2026-01-03 19:52:53,590: t15.2023.12.29 val PER: 0.1709
2026-01-03 19:52:53,590: t15.2024.02.25 val PER: 0.1334
2026-01-03 19:52:53,590: t15.2024.03.08 val PER: 0.2660
2026-01-03 19:52:53,590: t15.2024.03.15 val PER: 0.2339
2026-01-03 19:52:53,590: t15.2024.03.17 val PER: 0.1660
2026-01-03 19:52:53,590: t15.2024.05.10 val PER: 0.2036
2026-01-03 19:52:53,590: t15.2024.06.14 val PER: 0.1909
2026-01-03 19:52:53,590: t15.2024.07.19 val PER: 0.2795
2026-01-03 19:52:53,591: t15.2024.07.21 val PER: 0.1207
2026-01-03 19:52:53,591: t15.2024.07.28 val PER: 0.1721
2026-01-03 19:52:53,591: t15.2025.01.10 val PER: 0.3375
2026-01-03 19:52:53,591: t15.2025.01.12 val PER: 0.1886
2026-01-03 19:52:53,591: t15.2025.03.14 val PER: 0.3609
2026-01-03 19:52:53,591: t15.2025.03.16 val PER: 0.2120
2026-01-03 19:52:53,591: t15.2025.03.30 val PER: 0.3207
2026-01-03 19:52:53,591: t15.2025.04.13 val PER: 0.2368
2026-01-03 19:52:53,592: New best val WER(1gram) 51.78% --> 50.51%
2026-01-03 19:52:53,592: Checkpointing model
2026-01-03 19:52:54,207: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 19:52:54,454: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_8500
2026-01-03 19:53:03,124: Train batch 8600: loss: 15.96 grad norm: 57.97 time: 0.054
2026-01-03 19:53:20,364: Train batch 8800: loss: 15.11 grad norm: 57.71 time: 0.060
2026-01-03 19:53:37,552: Train batch 9000: loss: 16.23 grad norm: 63.83 time: 0.072
2026-01-03 19:53:37,552: Running test after training batch: 9000
2026-01-03 19:53:37,655: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:53:42,410: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 19:53:42,442: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 19:53:44,143: Val batch 9000: PER (avg): 0.1758 CTC Loss (avg): 17.2954 WER(1gram): 51.02% (n=64) time: 6.590
2026-01-03 19:53:44,143: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 19:53:44,143: t15.2023.08.13 val PER: 0.1383
2026-01-03 19:53:44,144: t15.2023.08.18 val PER: 0.1215
2026-01-03 19:53:44,144: t15.2023.08.20 val PER: 0.1326
2026-01-03 19:53:44,144: t15.2023.08.25 val PER: 0.1069
2026-01-03 19:53:44,144: t15.2023.08.27 val PER: 0.2074
2026-01-03 19:53:44,144: t15.2023.09.01 val PER: 0.0990
2026-01-03 19:53:44,144: t15.2023.09.03 val PER: 0.1865
2026-01-03 19:53:44,144: t15.2023.09.24 val PER: 0.1493
2026-01-03 19:53:44,144: t15.2023.09.29 val PER: 0.1500
2026-01-03 19:53:44,144: t15.2023.10.01 val PER: 0.1962
2026-01-03 19:53:44,144: t15.2023.10.06 val PER: 0.1012
2026-01-03 19:53:44,144: t15.2023.10.08 val PER: 0.2652
2026-01-03 19:53:44,144: t15.2023.10.13 val PER: 0.2374
2026-01-03 19:53:44,144: t15.2023.10.15 val PER: 0.1846
2026-01-03 19:53:44,144: t15.2023.10.20 val PER: 0.1779
2026-01-03 19:53:44,145: t15.2023.10.22 val PER: 0.1303
2026-01-03 19:53:44,145: t15.2023.11.03 val PER: 0.2069
2026-01-03 19:53:44,145: t15.2023.11.04 val PER: 0.0410
2026-01-03 19:53:44,145: t15.2023.11.17 val PER: 0.0544
2026-01-03 19:53:44,145: t15.2023.11.19 val PER: 0.0539
2026-01-03 19:53:44,145: t15.2023.11.26 val PER: 0.1732
2026-01-03 19:53:44,145: t15.2023.12.03 val PER: 0.1408
2026-01-03 19:53:44,145: t15.2023.12.08 val PER: 0.1345
2026-01-03 19:53:44,145: t15.2023.12.10 val PER: 0.1025
2026-01-03 19:53:44,145: t15.2023.12.17 val PER: 0.1653
2026-01-03 19:53:44,145: t15.2023.12.29 val PER: 0.1613
2026-01-03 19:53:44,145: t15.2024.02.25 val PER: 0.1419
2026-01-03 19:53:44,145: t15.2024.03.08 val PER: 0.2518
2026-01-03 19:53:44,145: t15.2024.03.15 val PER: 0.2251
2026-01-03 19:53:44,145: t15.2024.03.17 val PER: 0.1681
2026-01-03 19:53:44,145: t15.2024.05.10 val PER: 0.1902
2026-01-03 19:53:44,145: t15.2024.06.14 val PER: 0.1782
2026-01-03 19:53:44,146: t15.2024.07.19 val PER: 0.2709
2026-01-03 19:53:44,146: t15.2024.07.21 val PER: 0.1138
2026-01-03 19:53:44,146: t15.2024.07.28 val PER: 0.1500
2026-01-03 19:53:44,146: t15.2025.01.10 val PER: 0.3099
2026-01-03 19:53:44,146: t15.2025.01.12 val PER: 0.1778
2026-01-03 19:53:44,146: t15.2025.03.14 val PER: 0.3595
2026-01-03 19:53:44,146: t15.2025.03.16 val PER: 0.2160
2026-01-03 19:53:44,147: t15.2025.03.30 val PER: 0.3322
2026-01-03 19:53:44,147: t15.2025.04.13 val PER: 0.2454
[1;34mwandb[0m: 
[1;34mwandb[0m:  View run [33mbase[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../tmp/e12511253_b2t_348216/wandb/wandb/run-20260103_193744-djby5vt4/logs[0m
