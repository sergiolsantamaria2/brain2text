TMPDIR=/home/e12511253/tmp
JOB_TMP=/home/e12511253/tmp/e12511253_b2t_351125
TORCH_EXTENSIONS_DIR=/home/e12511253/tmp/e12511253_b2t_351125/torch_extensions
WANDB_DIR=/home/e12511253/tmp/e12511253_b2t_351125/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/home/e12511253/tmp/e12511253_b2t_351125/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  8 15:21 /home/e12511253/tmp/e12511253_b2t_351125/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
OUT_ROOT=/home/e12511253/Brain2Text/brain2text/trained_models
==============================================
Job: b2t5g  ID: 351125
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_5gram_only.yaml
Folders: configs/experiments/gru/combined_improvements
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/gru/combined_improvements ==========
Num configs: 1

=== RUN combined_improvements.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/combined_improvements/combined_improvements
2026-01-08 15:21:37,429: Using device: cuda:0
2026-01-08 15:25:26,955: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-08 15:25:26,978: Using 45 sessions after filtering (from 45).
2026-01-08 15:25:27,467: Using torch.compile (if available)
2026-01-08 15:25:27,467: torch.compile not available (torch<2.0). Skipping.
2026-01-08 15:25:27,468: Initialized RNN decoding model
2026-01-08 15:25:27,468: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(10240, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
    (1): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-08 15:25:27,468: Model has 52,577,321 parameters
2026-01-08 15:25:27,468: Model has 11,819,520 day-specific parameters | 22.48% of total parameters
2026-01-08 15:25:28,726: Successfully initialized datasets
2026-01-08 15:25:28,727: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-08 15:25:30,274: Train batch 0: loss: 556.69 grad norm: 2282.59 time: 0.186
2026-01-08 15:25:30,275: Running test after training batch: 0
2026-01-08 15:25:30,387: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:25:35,642: WER debug example
  GT : you can see the code at this point as well
  PR : hobart
2026-01-08 15:25:35,991: WER debug example
  GT : how does it keep the cost down
  PR : talk to to to to
2026-01-08 15:26:53,612: Val batch 0: PER (avg): 1.1978 CTC Loss (avg): 576.8645 WER(5gram): 99.02% (n=256) time: 83.337
2026-01-08 15:26:53,612: WER lens: avg_true_words=5.99 avg_pred_words=3.36 max_pred_words=9
2026-01-08 15:26:53,613: t15.2023.08.13 val PER: 1.1123
2026-01-08 15:26:53,613: t15.2023.08.18 val PER: 1.2104
2026-01-08 15:26:53,613: t15.2023.08.20 val PER: 1.2010
2026-01-08 15:26:53,613: t15.2023.08.25 val PER: 1.1777
2026-01-08 15:26:53,613: t15.2023.08.27 val PER: 1.0772
2026-01-08 15:26:53,613: t15.2023.09.01 val PER: 1.2102
2026-01-08 15:26:53,613: t15.2023.09.03 val PER: 1.1425
2026-01-08 15:26:53,613: t15.2023.09.24 val PER: 1.2670
2026-01-08 15:26:53,613: t15.2023.09.29 val PER: 1.2393
2026-01-08 15:26:53,614: t15.2023.10.01 val PER: 1.0350
2026-01-08 15:26:53,614: t15.2023.10.06 val PER: 1.2013
2026-01-08 15:26:53,614: t15.2023.10.08 val PER: 1.0054
2026-01-08 15:26:53,614: t15.2023.10.13 val PER: 1.0970
2026-01-08 15:26:53,614: t15.2023.10.15 val PER: 1.1457
2026-01-08 15:26:53,614: t15.2023.10.20 val PER: 1.1846
2026-01-08 15:26:53,614: t15.2023.10.22 val PER: 1.1793
2026-01-08 15:26:53,614: t15.2023.11.03 val PER: 1.3236
2026-01-08 15:26:53,614: t15.2023.11.04 val PER: 1.5085
2026-01-08 15:26:53,614: t15.2023.11.17 val PER: 1.6159
2026-01-08 15:26:53,614: t15.2023.11.19 val PER: 1.3553
2026-01-08 15:26:53,614: t15.2023.11.26 val PER: 1.2797
2026-01-08 15:26:53,614: t15.2023.12.03 val PER: 1.1996
2026-01-08 15:26:53,614: t15.2023.12.08 val PER: 1.2197
2026-01-08 15:26:53,614: t15.2023.12.10 val PER: 1.3075
2026-01-08 15:26:53,614: t15.2023.12.17 val PER: 1.0114
2026-01-08 15:26:53,614: t15.2023.12.29 val PER: 1.1146
2026-01-08 15:26:53,614: t15.2024.02.25 val PER: 1.2219
2026-01-08 15:26:53,615: t15.2024.03.08 val PER: 1.0669
2026-01-08 15:26:53,615: t15.2024.03.15 val PER: 1.1151
2026-01-08 15:26:53,615: t15.2024.03.17 val PER: 1.1332
2026-01-08 15:26:53,615: t15.2024.05.10 val PER: 1.1634
2026-01-08 15:26:53,615: t15.2024.06.14 val PER: 1.3249
2026-01-08 15:26:53,615: t15.2024.07.19 val PER: 0.9927
2026-01-08 15:26:53,615: t15.2024.07.21 val PER: 1.3938
2026-01-08 15:26:53,615: t15.2024.07.28 val PER: 1.4051
2026-01-08 15:26:53,615: t15.2025.01.10 val PER: 1.0014
2026-01-08 15:26:53,615: t15.2025.01.12 val PER: 1.4203
2026-01-08 15:26:53,615: t15.2025.03.14 val PER: 1.0030
2026-01-08 15:26:53,615: t15.2025.03.16 val PER: 1.4188
2026-01-08 15:26:53,615: t15.2025.03.30 val PER: 1.0805
2026-01-08 15:26:53,615: t15.2025.04.13 val PER: 1.2368
2026-01-08 15:26:53,617: New best val WER(5gram) inf% --> 99.02%
2026-01-08 15:26:53,811: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/combined_improvements/combined_improvements/checkpoint/checkpoint_batch_0
2026-01-08 15:27:10,626: Train batch 200: loss: 82.69 grad norm: 210.95 time: 0.063
2026-01-08 15:27:28,083: Train batch 400: loss: 54.99 grad norm: 95.48 time: 0.073
2026-01-08 15:27:36,428: Running test after training batch: 500
2026-01-08 15:27:36,536: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:27:41,766: WER debug example
  GT : you can see the code at this point as well
  PR : l z z z z z z z z
2026-01-08 15:27:41,903: WER debug example
  GT : how does it keep the cost down
  PR : z z z z z z z
