TMPDIR=/home/e12511253/tmp
JOB_TMP=/home/e12511253/tmp/e12511253_b2t_354080
TORCH_EXTENSIONS_DIR=/home/e12511253/tmp/e12511253_b2t_354080/torch_extensions
WANDB_DIR=/home/e12511253/tmp/e12511253_b2t_354080/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/home/e12511253/tmp/e12511253_b2t_354080/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan 11 17:08 /home/e12511253/tmp/e12511253_b2t_354080/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
OUT_ROOT=/home/e12511253/Brain2Text/brain2text/trained_models
==============================================
Job: b2t5g  ID: 354080
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_5gram_only.yaml
Folders: configs/experiments/diphones
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/diphones ==========
Num configs: 5

=== RUN diphone_100k.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k
2026-01-11 17:08:47,621: Using device: cuda:0
2026-01-11 17:13:26,929: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-11 17:13:26,932: Diphone mode ENABLED: n_classes changed from 41 to 1601
2026-01-11 17:13:26,961: Using 45 sessions after filtering (from 45).
2026-01-11 17:13:27,489: Using torch.compile (if available)
2026-01-11 17:13:27,489: torch.compile not available (torch<2.0). Skipping.
2026-01-11 17:13:27,489: Initialized RNN decoding model
2026-01-11 17:13:27,489: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=1601, bias=True)
)
2026-01-11 17:13:27,490: Model has 46,106,945 parameters
2026-01-11 17:13:27,490: Model has 11,819,520 day-specific parameters | 25.64% of total parameters
2026-01-11 17:13:32,406: Successfully initialized datasets
2026-01-11 17:13:32,407: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-11 17:13:34,125: Train batch 0: loss: 1341.10 grad norm: 955.07 time: 0.228
2026-01-11 17:13:34,125: Running test after training batch: 0
2026-01-11 17:13:34,254: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:13:42,083: WER debug example
  GT : you can see the code at this point as well
  PR : as
2026-01-11 17:13:44,131: WER debug example
  GT : how does it keep the cost down
  PR : here's
2026-01-11 17:19:40,641: Val batch 0: PER (avg): 1.5858 CTC Loss (avg): 1498.2145 WER(5gram): 99.87% (n=256) time: 366.516
2026-01-11 17:19:40,647: WER lens: avg_true_words=5.99 avg_pred_words=0.20 max_pred_words=1
2026-01-11 17:19:40,652: t15.2023.08.13 val PER: 1.4324
2026-01-11 17:19:40,657: t15.2023.08.18 val PER: 1.5105
2026-01-11 17:19:40,658: t15.2023.08.20 val PER: 1.4234
2026-01-11 17:19:40,658: t15.2023.08.25 val PER: 1.5919
2026-01-11 17:19:40,658: t15.2023.08.27 val PER: 1.4180
2026-01-11 17:19:40,658: t15.2023.09.01 val PER: 1.5544
2026-01-11 17:19:40,658: t15.2023.09.03 val PER: 1.4976
2026-01-11 17:19:40,659: t15.2023.09.24 val PER: 1.7269
2026-01-11 17:19:40,659: t15.2023.09.29 val PER: 1.6943
2026-01-11 17:19:40,659: t15.2023.10.01 val PER: 1.3032
2026-01-11 17:19:40,659: t15.2023.10.06 val PER: 1.6136
2026-01-11 17:19:40,659: t15.2023.10.08 val PER: 1.1894
2026-01-11 17:19:40,659: t15.2023.10.13 val PER: 1.4810
2026-01-11 17:19:40,660: t15.2023.10.15 val PER: 1.5313
2026-01-11 17:19:40,660: t15.2023.10.20 val PER: 1.6879
2026-01-11 17:19:40,660: t15.2023.10.22 val PER: 1.7227
2026-01-11 17:19:40,660: t15.2023.11.03 val PER: 1.7795
2026-01-11 17:19:40,660: t15.2023.11.04 val PER: 2.0478
2026-01-11 17:19:40,660: t15.2023.11.17 val PER: 2.1820
2026-01-11 17:19:40,660: t15.2023.11.19 val PER: 1.9242
2026-01-11 17:19:40,660: t15.2023.11.26 val PER: 1.7457
2026-01-11 17:19:40,660: t15.2023.12.03 val PER: 1.5557
2026-01-11 17:19:40,660: t15.2023.12.08 val PER: 1.6884
2026-01-11 17:19:40,661: t15.2023.12.10 val PER: 1.8817
2026-01-11 17:19:40,661: t15.2023.12.17 val PER: 1.4168
2026-01-11 17:19:40,661: t15.2023.12.29 val PER: 1.5278
2026-01-11 17:19:40,661: t15.2024.02.25 val PER: 1.5899
2026-01-11 17:19:40,661: t15.2024.03.08 val PER: 1.6117
2026-01-11 17:19:40,661: t15.2024.03.15 val PER: 1.4878
2026-01-11 17:19:40,661: t15.2024.03.17 val PER: 1.5167
2026-01-11 17:19:40,661: t15.2024.05.10 val PER: 1.4978
2026-01-11 17:19:40,661: t15.2024.06.14 val PER: 1.7035
2026-01-11 17:19:40,662: t15.2024.07.19 val PER: 1.1780
2026-01-11 17:19:40,662: t15.2024.07.21 val PER: 1.7931
2026-01-11 17:19:40,662: t15.2024.07.28 val PER: 1.7868
2026-01-11 17:19:40,663: t15.2025.01.10 val PER: 1.2617
2026-01-11 17:19:40,663: t15.2025.01.12 val PER: 1.8961
2026-01-11 17:19:40,663: t15.2025.03.14 val PER: 1.1612
2026-01-11 17:19:40,663: t15.2025.03.16 val PER: 1.8521
2026-01-11 17:19:40,663: t15.2025.03.30 val PER: 1.4839
2026-01-11 17:19:40,664: t15.2025.04.13 val PER: 1.7247
2026-01-11 17:19:40,665: New best val WER(5gram) inf% --> 99.87%
2026-01-11 17:19:40,836: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_0
2026-01-11 17:20:01,855: Train batch 200: loss: 151.61 grad norm: 54.06 time: 0.064
2026-01-11 17:20:22,549: Train batch 400: loss: 110.74 grad norm: 89.81 time: 0.074
2026-01-11 17:20:33,102: Running test after training batch: 500
2026-01-11 17:20:33,246: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:20:39,978: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-11 17:20:40,010: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-11 17:20:46,961: Val batch 500: PER (avg): 0.8950 CTC Loss (avg): 116.2796 WER(5gram): 99.74% (n=256) time: 13.858
2026-01-11 17:20:46,962: WER lens: avg_true_words=5.99 avg_pred_words=0.02 max_pred_words=1
2026-01-11 17:20:46,962: t15.2023.08.13 val PER: 0.8929
2026-01-11 17:20:46,962: t15.2023.08.18 val PER: 0.8759
2026-01-11 17:20:46,963: t15.2023.08.20 val PER: 0.8689
2026-01-11 17:20:46,963: t15.2023.08.25 val PER: 0.8720
2026-01-11 17:20:46,963: t15.2023.08.27 val PER: 0.9180
2026-01-11 17:20:46,964: t15.2023.09.01 val PER: 0.8515
2026-01-11 17:20:46,964: t15.2023.09.03 val PER: 0.9145
2026-01-11 17:20:46,964: t15.2023.09.24 val PER: 0.8799
2026-01-11 17:20:46,964: t15.2023.09.29 val PER: 0.8424
2026-01-11 17:20:46,964: t15.2023.10.01 val PER: 0.8831
2026-01-11 17:20:46,964: t15.2023.10.06 val PER: 0.8493
2026-01-11 17:20:46,964: t15.2023.10.08 val PER: 0.8890
2026-01-11 17:20:46,964: t15.2023.10.13 val PER: 0.8860
2026-01-11 17:20:46,965: t15.2023.10.15 val PER: 0.8556
2026-01-11 17:20:46,965: t15.2023.10.20 val PER: 0.8322
2026-01-11 17:20:46,965: t15.2023.10.22 val PER: 0.8497
2026-01-11 17:20:46,965: t15.2023.11.03 val PER: 0.8677
2026-01-11 17:20:46,965: t15.2023.11.04 val PER: 0.8567
2026-01-11 17:20:46,965: t15.2023.11.17 val PER: 0.8569
2026-01-11 17:20:46,965: t15.2023.11.19 val PER: 0.8743
2026-01-11 17:20:46,965: t15.2023.11.26 val PER: 0.8928
2026-01-11 17:20:46,966: t15.2023.12.03 val PER: 0.8834
2026-01-11 17:20:46,966: t15.2023.12.08 val PER: 0.8875
2026-01-11 17:20:46,966: t15.2023.12.10 val PER: 0.8581
2026-01-11 17:20:46,966: t15.2023.12.17 val PER: 0.9501
2026-01-11 17:20:46,966: t15.2023.12.29 val PER: 0.9286
2026-01-11 17:20:46,966: t15.2024.02.25 val PER: 0.8610
2026-01-11 17:20:46,966: t15.2024.03.08 val PER: 0.9559
2026-01-11 17:20:46,967: t15.2024.03.15 val PER: 0.9174
2026-01-11 17:20:46,967: t15.2024.03.17 val PER: 0.8982
2026-01-11 17:20:46,967: t15.2024.05.10 val PER: 0.9004
2026-01-11 17:20:46,967: t15.2024.06.14 val PER: 0.9164
2026-01-11 17:20:46,967: t15.2024.07.19 val PER: 0.9730
2026-01-11 17:20:46,967: t15.2024.07.21 val PER: 0.8910
2026-01-11 17:20:46,967: t15.2024.07.28 val PER: 0.8926
2026-01-11 17:20:46,967: t15.2025.01.10 val PER: 0.9656
2026-01-11 17:20:46,968: t15.2025.01.12 val PER: 0.9145
2026-01-11 17:20:46,968: t15.2025.03.14 val PER: 0.9852
2026-01-11 17:20:46,968: t15.2025.03.16 val PER: 0.9332
2026-01-11 17:20:46,968: t15.2025.03.30 val PER: 0.9667
2026-01-11 17:20:46,968: t15.2025.04.13 val PER: 0.9101
2026-01-11 17:20:46,968: New best val WER(5gram) 99.87% --> 99.74%
2026-01-11 17:20:47,147: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_500
2026-01-11 17:20:57,530: Train batch 600: loss: 97.92 grad norm: 88.80 time: 0.101
2026-01-11 17:21:18,352: Train batch 800: loss: 77.49 grad norm: 101.22 time: 0.068
2026-01-11 17:21:39,324: Train batch 1000: loss: 77.70 grad norm: 102.85 time: 0.076
2026-01-11 17:21:39,324: Running test after training batch: 1000
2026-01-11 17:21:39,541: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:21:45,874: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the
2026-01-11 17:21:46,050: WER debug example
  GT : how does it keep the cost down
  PR : it is the
2026-01-11 17:22:36,215: Val batch 1000: PER (avg): 0.4561 CTC Loss (avg): 80.0290 WER(5gram): 79.60% (n=256) time: 56.890
2026-01-11 17:22:36,216: WER lens: avg_true_words=5.99 avg_pred_words=3.14 max_pred_words=10
2026-01-11 17:22:36,216: t15.2023.08.13 val PER: 0.4335
2026-01-11 17:22:36,216: t15.2023.08.18 val PER: 0.3973
2026-01-11 17:22:36,216: t15.2023.08.20 val PER: 0.3892
2026-01-11 17:22:36,216: t15.2023.08.25 val PER: 0.3690
2026-01-11 17:22:36,217: t15.2023.08.27 val PER: 0.4920
2026-01-11 17:22:36,217: t15.2023.09.01 val PER: 0.3628
2026-01-11 17:22:36,217: t15.2023.09.03 val PER: 0.4596
2026-01-11 17:22:36,217: t15.2023.09.24 val PER: 0.4053
2026-01-11 17:22:36,217: t15.2023.09.29 val PER: 0.4320
2026-01-11 17:22:36,217: t15.2023.10.01 val PER: 0.4736
2026-01-11 17:22:36,217: t15.2023.10.06 val PER: 0.3994
2026-01-11 17:22:36,217: t15.2023.10.08 val PER: 0.4912
2026-01-11 17:22:36,218: t15.2023.10.13 val PER: 0.5283
2026-01-11 17:22:36,218: t15.2023.10.15 val PER: 0.4436
2026-01-11 17:22:36,218: t15.2023.10.20 val PER: 0.4295
2026-01-11 17:22:36,218: t15.2023.10.22 val PER: 0.4310
2026-01-11 17:22:36,218: t15.2023.11.03 val PER: 0.4450
2026-01-11 17:22:36,218: t15.2023.11.04 val PER: 0.2423
2026-01-11 17:22:36,218: t15.2023.11.17 val PER: 0.3219
2026-01-11 17:22:36,218: t15.2023.11.19 val PER: 0.2695
2026-01-11 17:22:36,218: t15.2023.11.26 val PER: 0.4920
2026-01-11 17:22:36,218: t15.2023.12.03 val PER: 0.4380
2026-01-11 17:22:36,218: t15.2023.12.08 val PER: 0.4454
2026-01-11 17:22:36,219: t15.2023.12.10 val PER: 0.4126
2026-01-11 17:22:36,219: t15.2023.12.17 val PER: 0.4491
2026-01-11 17:22:36,219: t15.2023.12.29 val PER: 0.4550
2026-01-11 17:22:36,219: t15.2024.02.25 val PER: 0.3806
2026-01-11 17:22:36,219: t15.2024.03.08 val PER: 0.5235
2026-01-11 17:22:36,219: t15.2024.03.15 val PER: 0.4903
2026-01-11 17:22:36,219: t15.2024.03.17 val PER: 0.4491
2026-01-11 17:22:36,219: t15.2024.05.10 val PER: 0.4294
2026-01-11 17:22:36,219: t15.2024.06.14 val PER: 0.4511
2026-01-11 17:22:36,219: t15.2024.07.19 val PER: 0.5564
2026-01-11 17:22:36,219: t15.2024.07.21 val PER: 0.4179
2026-01-11 17:22:36,220: t15.2024.07.28 val PER: 0.4581
2026-01-11 17:22:36,220: t15.2025.01.10 val PER: 0.6267
2026-01-11 17:22:36,220: t15.2025.01.12 val PER: 0.4750
2026-01-11 17:22:36,220: t15.2025.03.14 val PER: 0.6272
2026-01-11 17:22:36,220: t15.2025.03.16 val PER: 0.5288
2026-01-11 17:22:36,220: t15.2025.03.30 val PER: 0.6437
2026-01-11 17:22:36,220: t15.2025.04.13 val PER: 0.5178
2026-01-11 17:22:36,221: New best val WER(5gram) 99.74% --> 79.60%
2026-01-11 17:22:36,500: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_1000
2026-01-11 17:22:57,547: Train batch 1200: loss: 62.66 grad norm: 110.01 time: 0.080
2026-01-11 17:23:18,927: Train batch 1400: loss: 63.50 grad norm: 126.73 time: 0.072
2026-01-11 17:23:29,483: Running test after training batch: 1500
2026-01-11 17:23:29,699: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:23:36,014: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the way it is
2026-01-11 17:23:36,196: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the sa
2026-01-11 17:24:18,729: Val batch 1500: PER (avg): 0.3976 CTC Loss (avg): 68.5312 WER(5gram): 70.14% (n=256) time: 49.245
2026-01-11 17:24:18,730: WER lens: avg_true_words=5.99 avg_pred_words=4.98 max_pred_words=12
2026-01-11 17:24:18,730: t15.2023.08.13 val PER: 0.3638
2026-01-11 17:24:18,730: t15.2023.08.18 val PER: 0.3353
2026-01-11 17:24:18,731: t15.2023.08.20 val PER: 0.3312
2026-01-11 17:24:18,731: t15.2023.08.25 val PER: 0.3238
2026-01-11 17:24:18,731: t15.2023.08.27 val PER: 0.4277
2026-01-11 17:24:18,731: t15.2023.09.01 val PER: 0.2938
2026-01-11 17:24:18,731: t15.2023.09.03 val PER: 0.3943
2026-01-11 17:24:18,731: t15.2023.09.24 val PER: 0.3350
2026-01-11 17:24:18,731: t15.2023.09.29 val PER: 0.3618
2026-01-11 17:24:18,731: t15.2023.10.01 val PER: 0.4141
2026-01-11 17:24:18,731: t15.2023.10.06 val PER: 0.3186
2026-01-11 17:24:18,732: t15.2023.10.08 val PER: 0.4560
2026-01-11 17:24:18,732: t15.2023.10.13 val PER: 0.4779
2026-01-11 17:24:18,732: t15.2023.10.15 val PER: 0.3869
2026-01-11 17:24:18,732: t15.2023.10.20 val PER: 0.3758
2026-01-11 17:24:18,732: t15.2023.10.22 val PER: 0.3330
2026-01-11 17:24:18,732: t15.2023.11.03 val PER: 0.3758
2026-01-11 17:24:18,732: t15.2023.11.04 val PER: 0.1502
2026-01-11 17:24:18,732: t15.2023.11.17 val PER: 0.2504
2026-01-11 17:24:18,732: t15.2023.11.19 val PER: 0.2176
2026-01-11 17:24:18,733: t15.2023.11.26 val PER: 0.4478
2026-01-11 17:24:18,733: t15.2023.12.03 val PER: 0.3718
2026-01-11 17:24:18,733: t15.2023.12.08 val PER: 0.3955
2026-01-11 17:24:18,733: t15.2023.12.10 val PER: 0.3601
2026-01-11 17:24:18,733: t15.2023.12.17 val PER: 0.3960
2026-01-11 17:24:18,733: t15.2023.12.29 val PER: 0.4043
2026-01-11 17:24:18,733: t15.2024.02.25 val PER: 0.3216
2026-01-11 17:24:18,733: t15.2024.03.08 val PER: 0.4467
2026-01-11 17:24:18,733: t15.2024.03.15 val PER: 0.4271
2026-01-11 17:24:18,733: t15.2024.03.17 val PER: 0.4038
2026-01-11 17:24:18,734: t15.2024.05.10 val PER: 0.3908
2026-01-11 17:24:18,734: t15.2024.06.14 val PER: 0.4054
2026-01-11 17:24:18,734: t15.2024.07.19 val PER: 0.5214
2026-01-11 17:24:18,734: t15.2024.07.21 val PER: 0.3634
2026-01-11 17:24:18,734: t15.2024.07.28 val PER: 0.3875
2026-01-11 17:24:18,734: t15.2025.01.10 val PER: 0.5606
2026-01-11 17:24:18,734: t15.2025.01.12 val PER: 0.4103
2026-01-11 17:24:18,734: t15.2025.03.14 val PER: 0.5710
2026-01-11 17:24:18,734: t15.2025.03.16 val PER: 0.4764
2026-01-11 17:24:18,734: t15.2025.03.30 val PER: 0.6103
2026-01-11 17:24:18,735: t15.2025.04.13 val PER: 0.4693
2026-01-11 17:24:18,735: New best val WER(5gram) 79.60% --> 70.14%
2026-01-11 17:24:18,900: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_1500
2026-01-11 17:24:29,234: Train batch 1600: loss: 64.08 grad norm: 102.47 time: 0.075
2026-01-11 17:24:50,800: Train batch 1800: loss: 62.55 grad norm: 122.53 time: 0.106
2026-01-11 17:25:12,157: Train batch 2000: loss: 60.60 grad norm: 108.64 time: 0.078
2026-01-11 17:25:12,158: Running test after training batch: 2000
2026-01-11 17:25:12,453: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:25:18,769: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the way this is why
2026-01-11 17:25:18,927: WER debug example
  GT : how does it keep the cost down
  PR : to see it in the sa
2026-01-11 17:25:51,861: Val batch 2000: PER (avg): 0.3644 CTC Loss (avg): 60.5822 WER(5gram): 54.95% (n=256) time: 39.703
2026-01-11 17:25:51,862: WER lens: avg_true_words=5.99 avg_pred_words=5.31 max_pred_words=12
2026-01-11 17:25:51,863: t15.2023.08.13 val PER: 0.3441
2026-01-11 17:25:51,863: t15.2023.08.18 val PER: 0.3101
2026-01-11 17:25:51,863: t15.2023.08.20 val PER: 0.2939
2026-01-11 17:25:51,863: t15.2023.08.25 val PER: 0.2816
2026-01-11 17:25:51,863: t15.2023.08.27 val PER: 0.3923
2026-01-11 17:25:51,863: t15.2023.09.01 val PER: 0.2630
2026-01-11 17:25:51,863: t15.2023.09.03 val PER: 0.3872
2026-01-11 17:25:51,864: t15.2023.09.24 val PER: 0.2925
2026-01-11 17:25:51,864: t15.2023.09.29 val PER: 0.3204
2026-01-11 17:25:51,864: t15.2023.10.01 val PER: 0.3838
2026-01-11 17:25:51,864: t15.2023.10.06 val PER: 0.2863
2026-01-11 17:25:51,864: t15.2023.10.08 val PER: 0.4344
2026-01-11 17:25:51,864: t15.2023.10.13 val PER: 0.4500
2026-01-11 17:25:51,864: t15.2023.10.15 val PER: 0.3481
2026-01-11 17:25:51,864: t15.2023.10.20 val PER: 0.3523
2026-01-11 17:25:51,865: t15.2023.10.22 val PER: 0.3196
2026-01-11 17:25:51,865: t15.2023.11.03 val PER: 0.3507
2026-01-11 17:25:51,865: t15.2023.11.04 val PER: 0.1195
2026-01-11 17:25:51,865: t15.2023.11.17 val PER: 0.1897
2026-01-11 17:25:51,866: t15.2023.11.19 val PER: 0.1577
2026-01-11 17:25:51,866: t15.2023.11.26 val PER: 0.4210
2026-01-11 17:25:51,866: t15.2023.12.03 val PER: 0.3508
2026-01-11 17:25:51,866: t15.2023.12.08 val PER: 0.3628
2026-01-11 17:25:51,866: t15.2023.12.10 val PER: 0.3114
2026-01-11 17:25:51,866: t15.2023.12.17 val PER: 0.3316
2026-01-11 17:25:51,866: t15.2023.12.29 val PER: 0.3651
2026-01-11 17:25:51,866: t15.2024.02.25 val PER: 0.2978
2026-01-11 17:25:51,866: t15.2024.03.08 val PER: 0.4395
2026-01-11 17:25:51,867: t15.2024.03.15 val PER: 0.3952
2026-01-11 17:25:51,867: t15.2024.03.17 val PER: 0.3640
2026-01-11 17:25:51,867: t15.2024.05.10 val PER: 0.3774
2026-01-11 17:25:51,867: t15.2024.06.14 val PER: 0.3644
2026-01-11 17:25:51,867: t15.2024.07.19 val PER: 0.4799
2026-01-11 17:25:51,867: t15.2024.07.21 val PER: 0.3352
2026-01-11 17:25:51,867: t15.2024.07.28 val PER: 0.3537
2026-01-11 17:25:51,867: t15.2025.01.10 val PER: 0.5193
2026-01-11 17:25:51,867: t15.2025.01.12 val PER: 0.3911
2026-01-11 17:25:51,867: t15.2025.03.14 val PER: 0.5311
2026-01-11 17:25:51,867: t15.2025.03.16 val PER: 0.4398
2026-01-11 17:25:51,867: t15.2025.03.30 val PER: 0.5598
2026-01-11 17:25:51,868: t15.2025.04.13 val PER: 0.4251
2026-01-11 17:25:51,868: New best val WER(5gram) 70.14% --> 54.95%
2026-01-11 17:25:52,039: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_2000
2026-01-11 17:26:12,942: Train batch 2200: loss: 52.82 grad norm: 93.96 time: 0.071
2026-01-11 17:26:34,029: Train batch 2400: loss: 51.06 grad norm: 96.60 time: 0.062
2026-01-11 17:26:44,380: Running test after training batch: 2500
2026-01-11 17:26:44,562: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:26:51,030: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 17:26:51,187: WER debug example
  GT : how does it keep the cost down
  PR : to see it in the sa new
2026-01-11 17:27:22,996: Val batch 2500: PER (avg): 0.3341 CTC Loss (avg): 55.4002 WER(5gram): 54.11% (n=256) time: 38.616
2026-01-11 17:27:22,997: WER lens: avg_true_words=5.99 avg_pred_words=5.83 max_pred_words=13
2026-01-11 17:27:22,998: t15.2023.08.13 val PER: 0.3067
2026-01-11 17:27:22,998: t15.2023.08.18 val PER: 0.2791
2026-01-11 17:27:22,999: t15.2023.08.20 val PER: 0.2716
2026-01-11 17:27:22,999: t15.2023.08.25 val PER: 0.2334
2026-01-11 17:27:22,999: t15.2023.08.27 val PER: 0.3682
2026-01-11 17:27:22,999: t15.2023.09.01 val PER: 0.2443
2026-01-11 17:27:22,999: t15.2023.09.03 val PER: 0.3337
2026-01-11 17:27:22,999: t15.2023.09.24 val PER: 0.2791
2026-01-11 17:27:22,999: t15.2023.09.29 val PER: 0.3063
2026-01-11 17:27:23,000: t15.2023.10.01 val PER: 0.3540
2026-01-11 17:27:23,000: t15.2023.10.06 val PER: 0.2573
2026-01-11 17:27:23,000: t15.2023.10.08 val PER: 0.4127
2026-01-11 17:27:23,000: t15.2023.10.13 val PER: 0.4189
2026-01-11 17:27:23,000: t15.2023.10.15 val PER: 0.3197
2026-01-11 17:27:23,000: t15.2023.10.20 val PER: 0.3188
2026-01-11 17:27:23,000: t15.2023.10.22 val PER: 0.2940
2026-01-11 17:27:23,000: t15.2023.11.03 val PER: 0.3141
2026-01-11 17:27:23,000: t15.2023.11.04 val PER: 0.0853
2026-01-11 17:27:23,000: t15.2023.11.17 val PER: 0.1602
2026-01-11 17:27:23,000: t15.2023.11.19 val PER: 0.1457
2026-01-11 17:27:23,000: t15.2023.11.26 val PER: 0.3739
2026-01-11 17:27:23,001: t15.2023.12.03 val PER: 0.3193
2026-01-11 17:27:23,001: t15.2023.12.08 val PER: 0.3342
2026-01-11 17:27:23,001: t15.2023.12.10 val PER: 0.2891
2026-01-11 17:27:23,001: t15.2023.12.17 val PER: 0.3306
2026-01-11 17:27:23,001: t15.2023.12.29 val PER: 0.3432
2026-01-11 17:27:23,001: t15.2024.02.25 val PER: 0.2781
2026-01-11 17:27:23,001: t15.2024.03.08 val PER: 0.3940
2026-01-11 17:27:23,001: t15.2024.03.15 val PER: 0.3665
2026-01-11 17:27:23,001: t15.2024.03.17 val PER: 0.3354
2026-01-11 17:27:23,002: t15.2024.05.10 val PER: 0.3447
2026-01-11 17:27:23,002: t15.2024.06.14 val PER: 0.3249
2026-01-11 17:27:23,002: t15.2024.07.19 val PER: 0.4509
2026-01-11 17:27:23,002: t15.2024.07.21 val PER: 0.2807
2026-01-11 17:27:23,003: t15.2024.07.28 val PER: 0.3132
2026-01-11 17:27:23,003: t15.2025.01.10 val PER: 0.4752
2026-01-11 17:27:23,003: t15.2025.01.12 val PER: 0.3503
2026-01-11 17:27:23,003: t15.2025.03.14 val PER: 0.5015
2026-01-11 17:27:23,003: t15.2025.03.16 val PER: 0.3822
2026-01-11 17:27:23,003: t15.2025.03.30 val PER: 0.5552
2026-01-11 17:27:23,003: t15.2025.04.13 val PER: 0.4051
2026-01-11 17:27:23,005: New best val WER(5gram) 54.95% --> 54.11%
2026-01-11 17:27:23,216: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_2500
2026-01-11 17:27:33,853: Train batch 2600: loss: 57.59 grad norm: 117.79 time: 0.068
2026-01-11 17:27:55,304: Train batch 2800: loss: 45.17 grad norm: 101.60 time: 0.093
2026-01-11 17:28:17,221: Train batch 3000: loss: 56.23 grad norm: 115.37 time: 0.094
2026-01-11 17:28:17,221: Running test after training batch: 3000
2026-01-11 17:28:17,364: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:28:24,109: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good this is why
2026-01-11 17:28:24,235: WER debug example
  GT : how does it keep the cost down
  PR : to see it in the cost
2026-01-11 17:28:45,651: Val batch 3000: PER (avg): 0.3217 CTC Loss (avg): 50.6576 WER(5gram): 55.67% (n=256) time: 28.430
2026-01-11 17:28:45,652: WER lens: avg_true_words=5.99 avg_pred_words=5.64 max_pred_words=13
2026-01-11 17:28:45,653: t15.2023.08.13 val PER: 0.2838
2026-01-11 17:28:45,653: t15.2023.08.18 val PER: 0.2825
2026-01-11 17:28:45,653: t15.2023.08.20 val PER: 0.2661
2026-01-11 17:28:45,653: t15.2023.08.25 val PER: 0.2349
2026-01-11 17:28:45,653: t15.2023.08.27 val PER: 0.3424
2026-01-11 17:28:45,653: t15.2023.09.01 val PER: 0.2200
2026-01-11 17:28:45,653: t15.2023.09.03 val PER: 0.3290
2026-01-11 17:28:45,653: t15.2023.09.24 val PER: 0.2670
2026-01-11 17:28:45,653: t15.2023.09.29 val PER: 0.2846
2026-01-11 17:28:45,653: t15.2023.10.01 val PER: 0.3461
2026-01-11 17:28:45,653: t15.2023.10.06 val PER: 0.2400
2026-01-11 17:28:45,654: t15.2023.10.08 val PER: 0.3978
2026-01-11 17:28:45,654: t15.2023.10.13 val PER: 0.4205
2026-01-11 17:28:45,654: t15.2023.10.15 val PER: 0.3131
2026-01-11 17:28:45,654: t15.2023.10.20 val PER: 0.3087
2026-01-11 17:28:45,654: t15.2023.10.22 val PER: 0.2784
2026-01-11 17:28:45,655: t15.2023.11.03 val PER: 0.3161
2026-01-11 17:28:45,655: t15.2023.11.04 val PER: 0.0819
2026-01-11 17:28:45,655: t15.2023.11.17 val PER: 0.1555
2026-01-11 17:28:45,655: t15.2023.11.19 val PER: 0.1457
2026-01-11 17:28:45,655: t15.2023.11.26 val PER: 0.3659
2026-01-11 17:28:45,655: t15.2023.12.03 val PER: 0.3036
2026-01-11 17:28:45,656: t15.2023.12.08 val PER: 0.3136
2026-01-11 17:28:45,656: t15.2023.12.10 val PER: 0.2628
2026-01-11 17:28:45,656: t15.2023.12.17 val PER: 0.2963
2026-01-11 17:28:45,656: t15.2023.12.29 val PER: 0.3178
2026-01-11 17:28:45,656: t15.2024.02.25 val PER: 0.2823
2026-01-11 17:28:45,656: t15.2024.03.08 val PER: 0.3855
2026-01-11 17:28:45,656: t15.2024.03.15 val PER: 0.3546
2026-01-11 17:28:45,656: t15.2024.03.17 val PER: 0.3152
2026-01-11 17:28:45,657: t15.2024.05.10 val PER: 0.3284
2026-01-11 17:28:45,657: t15.2024.06.14 val PER: 0.2997
2026-01-11 17:28:45,657: t15.2024.07.19 val PER: 0.4199
2026-01-11 17:28:45,657: t15.2024.07.21 val PER: 0.2745
2026-01-11 17:28:45,657: t15.2024.07.28 val PER: 0.3184
2026-01-11 17:28:45,657: t15.2025.01.10 val PER: 0.4614
2026-01-11 17:28:45,657: t15.2025.01.12 val PER: 0.3480
2026-01-11 17:28:45,657: t15.2025.03.14 val PER: 0.5030
2026-01-11 17:28:45,657: t15.2025.03.16 val PER: 0.3809
2026-01-11 17:28:45,657: t15.2025.03.30 val PER: 0.5253
2026-01-11 17:28:45,658: t15.2025.04.13 val PER: 0.3795
2026-01-11 17:28:45,829: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_3000
2026-01-11 17:29:06,940: Train batch 3200: loss: 43.85 grad norm: 91.30 time: 0.088
2026-01-11 17:29:27,670: Train batch 3400: loss: 33.59 grad norm: 83.67 time: 0.059
2026-01-11 17:29:38,208: Running test after training batch: 3500
2026-01-11 17:29:38,342: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:29:44,805: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point we
2026-01-11 17:29:44,887: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 17:30:03,912: Val batch 3500: PER (avg): 0.3048 CTC Loss (avg): 47.6970 WER(5gram): 40.16% (n=256) time: 25.703
2026-01-11 17:30:03,913: WER lens: avg_true_words=5.99 avg_pred_words=5.78 max_pred_words=12
2026-01-11 17:30:03,913: t15.2023.08.13 val PER: 0.2744
2026-01-11 17:30:03,914: t15.2023.08.18 val PER: 0.2582
2026-01-11 17:30:03,914: t15.2023.08.20 val PER: 0.2542
2026-01-11 17:30:03,914: t15.2023.08.25 val PER: 0.2108
2026-01-11 17:30:03,914: t15.2023.08.27 val PER: 0.3215
2026-01-11 17:30:03,914: t15.2023.09.01 val PER: 0.2078
2026-01-11 17:30:03,914: t15.2023.09.03 val PER: 0.3088
2026-01-11 17:30:03,914: t15.2023.09.24 val PER: 0.2354
2026-01-11 17:30:03,914: t15.2023.09.29 val PER: 0.2527
2026-01-11 17:30:03,914: t15.2023.10.01 val PER: 0.3322
2026-01-11 17:30:03,914: t15.2023.10.06 val PER: 0.2174
2026-01-11 17:30:03,914: t15.2023.10.08 val PER: 0.4046
2026-01-11 17:30:03,914: t15.2023.10.13 val PER: 0.4019
2026-01-11 17:30:03,915: t15.2023.10.15 val PER: 0.3125
2026-01-11 17:30:03,915: t15.2023.10.20 val PER: 0.3087
2026-01-11 17:30:03,915: t15.2023.10.22 val PER: 0.2506
2026-01-11 17:30:03,915: t15.2023.11.03 val PER: 0.3026
2026-01-11 17:30:03,915: t15.2023.11.04 val PER: 0.0751
2026-01-11 17:30:03,915: t15.2023.11.17 val PER: 0.1291
2026-01-11 17:30:03,916: t15.2023.11.19 val PER: 0.1257
2026-01-11 17:30:03,916: t15.2023.11.26 val PER: 0.3413
2026-01-11 17:30:03,916: t15.2023.12.03 val PER: 0.2794
2026-01-11 17:30:03,916: t15.2023.12.08 val PER: 0.3036
2026-01-11 17:30:03,916: t15.2023.12.10 val PER: 0.2484
2026-01-11 17:30:03,916: t15.2023.12.17 val PER: 0.2713
2026-01-11 17:30:03,916: t15.2023.12.29 val PER: 0.2910
2026-01-11 17:30:03,916: t15.2024.02.25 val PER: 0.2612
2026-01-11 17:30:03,916: t15.2024.03.08 val PER: 0.3713
2026-01-11 17:30:03,917: t15.2024.03.15 val PER: 0.3408
2026-01-11 17:30:03,917: t15.2024.03.17 val PER: 0.3013
2026-01-11 17:30:03,917: t15.2024.05.10 val PER: 0.3165
2026-01-11 17:30:03,917: t15.2024.06.14 val PER: 0.3091
2026-01-11 17:30:03,917: t15.2024.07.19 val PER: 0.4100
2026-01-11 17:30:03,917: t15.2024.07.21 val PER: 0.2572
2026-01-11 17:30:03,917: t15.2024.07.28 val PER: 0.2971
2026-01-11 17:30:03,917: t15.2025.01.10 val PER: 0.4559
2026-01-11 17:30:03,917: t15.2025.01.12 val PER: 0.3418
2026-01-11 17:30:03,918: t15.2025.03.14 val PER: 0.4852
2026-01-11 17:30:03,918: t15.2025.03.16 val PER: 0.3639
2026-01-11 17:30:03,918: t15.2025.03.30 val PER: 0.4805
2026-01-11 17:30:03,918: t15.2025.04.13 val PER: 0.3638
2026-01-11 17:30:03,918: New best val WER(5gram) 54.11% --> 40.16%
2026-01-11 17:30:04,107: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_3500
2026-01-11 17:30:14,774: Train batch 3600: loss: 38.80 grad norm: 91.35 time: 0.078
2026-01-11 17:30:35,472: Train batch 3800: loss: 43.06 grad norm: 93.10 time: 0.079
2026-01-11 17:30:56,684: Train batch 4000: loss: 37.19 grad norm: 82.76 time: 0.066
2026-01-11 17:30:56,684: Running test after training batch: 4000
2026-01-11 17:30:56,821: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:31:03,157: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point will
2026-01-11 17:31:03,243: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost of
2026-01-11 17:31:22,233: Val batch 4000: PER (avg): 0.2830 CTC Loss (avg): 44.2644 WER(5gram): 34.09% (n=256) time: 25.549
2026-01-11 17:31:22,234: WER lens: avg_true_words=5.99 avg_pred_words=5.71 max_pred_words=12
2026-01-11 17:31:22,234: t15.2023.08.13 val PER: 0.2380
2026-01-11 17:31:22,234: t15.2023.08.18 val PER: 0.2364
2026-01-11 17:31:22,234: t15.2023.08.20 val PER: 0.2280
2026-01-11 17:31:22,234: t15.2023.08.25 val PER: 0.1837
2026-01-11 17:31:22,234: t15.2023.08.27 val PER: 0.3119
2026-01-11 17:31:22,234: t15.2023.09.01 val PER: 0.1940
2026-01-11 17:31:22,235: t15.2023.09.03 val PER: 0.3005
2026-01-11 17:31:22,235: t15.2023.09.24 val PER: 0.2233
2026-01-11 17:31:22,235: t15.2023.09.29 val PER: 0.2451
2026-01-11 17:31:22,235: t15.2023.10.01 val PER: 0.3111
2026-01-11 17:31:22,235: t15.2023.10.06 val PER: 0.2110
2026-01-11 17:31:22,236: t15.2023.10.08 val PER: 0.3735
2026-01-11 17:31:22,236: t15.2023.10.13 val PER: 0.3739
2026-01-11 17:31:22,236: t15.2023.10.15 val PER: 0.2894
2026-01-11 17:31:22,236: t15.2023.10.20 val PER: 0.2919
2026-01-11 17:31:22,236: t15.2023.10.22 val PER: 0.2383
2026-01-11 17:31:22,236: t15.2023.11.03 val PER: 0.2802
2026-01-11 17:31:22,236: t15.2023.11.04 val PER: 0.0717
2026-01-11 17:31:22,236: t15.2023.11.17 val PER: 0.1229
2026-01-11 17:31:22,237: t15.2023.11.19 val PER: 0.1018
2026-01-11 17:31:22,237: t15.2023.11.26 val PER: 0.3145
2026-01-11 17:31:22,237: t15.2023.12.03 val PER: 0.2626
2026-01-11 17:31:22,237: t15.2023.12.08 val PER: 0.2776
2026-01-11 17:31:22,237: t15.2023.12.10 val PER: 0.2168
2026-01-11 17:31:22,237: t15.2023.12.17 val PER: 0.2370
2026-01-11 17:31:22,238: t15.2023.12.29 val PER: 0.2903
2026-01-11 17:31:22,238: t15.2024.02.25 val PER: 0.2598
2026-01-11 17:31:22,238: t15.2024.03.08 val PER: 0.3442
2026-01-11 17:31:22,238: t15.2024.03.15 val PER: 0.3158
2026-01-11 17:31:22,238: t15.2024.03.17 val PER: 0.2789
2026-01-11 17:31:22,238: t15.2024.05.10 val PER: 0.2912
2026-01-11 17:31:22,239: t15.2024.06.14 val PER: 0.2792
2026-01-11 17:31:22,239: t15.2024.07.19 val PER: 0.3902
2026-01-11 17:31:22,239: t15.2024.07.21 val PER: 0.2152
2026-01-11 17:31:22,239: t15.2024.07.28 val PER: 0.2713
2026-01-11 17:31:22,239: t15.2025.01.10 val PER: 0.4242
2026-01-11 17:31:22,239: t15.2025.01.12 val PER: 0.3110
2026-01-11 17:31:22,239: t15.2025.03.14 val PER: 0.4601
2026-01-11 17:31:22,239: t15.2025.03.16 val PER: 0.3390
2026-01-11 17:31:22,239: t15.2025.03.30 val PER: 0.4471
2026-01-11 17:31:22,239: t15.2025.04.13 val PER: 0.3481
2026-01-11 17:31:22,240: New best val WER(5gram) 40.16% --> 34.09%
2026-01-11 17:31:22,431: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_4000
2026-01-11 17:31:44,239: Train batch 4200: loss: 38.60 grad norm: 92.48 time: 0.090
2026-01-11 17:32:05,394: Train batch 4400: loss: 28.05 grad norm: 76.24 time: 0.077
2026-01-11 17:32:15,594: Running test after training batch: 4500
2026-01-11 17:32:15,888: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:32:22,758: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point as well
2026-01-11 17:32:22,827: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 17:32:40,622: Val batch 4500: PER (avg): 0.2665 CTC Loss (avg): 41.8550 WER(5gram): 28.62% (n=256) time: 25.027
2026-01-11 17:32:40,623: WER lens: avg_true_words=5.99 avg_pred_words=5.91 max_pred_words=11
2026-01-11 17:32:40,623: t15.2023.08.13 val PER: 0.2339
2026-01-11 17:32:40,623: t15.2023.08.18 val PER: 0.2263
2026-01-11 17:32:40,624: t15.2023.08.20 val PER: 0.2121
2026-01-11 17:32:40,624: t15.2023.08.25 val PER: 0.1792
2026-01-11 17:32:40,624: t15.2023.08.27 val PER: 0.2797
2026-01-11 17:32:40,624: t15.2023.09.01 val PER: 0.1818
2026-01-11 17:32:40,624: t15.2023.09.03 val PER: 0.2755
2026-01-11 17:32:40,625: t15.2023.09.24 val PER: 0.2051
2026-01-11 17:32:40,625: t15.2023.09.29 val PER: 0.2304
2026-01-11 17:32:40,625: t15.2023.10.01 val PER: 0.2814
2026-01-11 17:32:40,625: t15.2023.10.06 val PER: 0.1808
2026-01-11 17:32:40,625: t15.2023.10.08 val PER: 0.3640
2026-01-11 17:32:40,625: t15.2023.10.13 val PER: 0.3685
2026-01-11 17:32:40,625: t15.2023.10.15 val PER: 0.2716
2026-01-11 17:32:40,626: t15.2023.10.20 val PER: 0.2953
2026-01-11 17:32:40,626: t15.2023.10.22 val PER: 0.2294
2026-01-11 17:32:40,626: t15.2023.11.03 val PER: 0.2707
2026-01-11 17:32:40,626: t15.2023.11.04 val PER: 0.0648
2026-01-11 17:32:40,626: t15.2023.11.17 val PER: 0.0824
2026-01-11 17:32:40,626: t15.2023.11.19 val PER: 0.0998
2026-01-11 17:32:40,627: t15.2023.11.26 val PER: 0.2891
2026-01-11 17:32:40,627: t15.2023.12.03 val PER: 0.2342
2026-01-11 17:32:40,627: t15.2023.12.08 val PER: 0.2663
2026-01-11 17:32:40,627: t15.2023.12.10 val PER: 0.1958
2026-01-11 17:32:40,627: t15.2023.12.17 val PER: 0.2464
2026-01-11 17:32:40,627: t15.2023.12.29 val PER: 0.2629
2026-01-11 17:32:40,628: t15.2024.02.25 val PER: 0.2430
2026-01-11 17:32:40,628: t15.2024.03.08 val PER: 0.3215
2026-01-11 17:32:40,628: t15.2024.03.15 val PER: 0.3114
2026-01-11 17:32:40,628: t15.2024.03.17 val PER: 0.2538
2026-01-11 17:32:40,628: t15.2024.05.10 val PER: 0.2823
2026-01-11 17:32:40,628: t15.2024.06.14 val PER: 0.2571
2026-01-11 17:32:40,628: t15.2024.07.19 val PER: 0.3566
2026-01-11 17:32:40,628: t15.2024.07.21 val PER: 0.2048
2026-01-11 17:32:40,628: t15.2024.07.28 val PER: 0.2544
2026-01-11 17:32:40,628: t15.2025.01.10 val PER: 0.4118
2026-01-11 17:32:40,628: t15.2025.01.12 val PER: 0.2948
2026-01-11 17:32:40,629: t15.2025.03.14 val PER: 0.4423
2026-01-11 17:32:40,629: t15.2025.03.16 val PER: 0.3285
2026-01-11 17:32:40,629: t15.2025.03.30 val PER: 0.4471
2026-01-11 17:32:40,629: t15.2025.04.13 val PER: 0.3096
2026-01-11 17:32:40,631: New best val WER(5gram) 34.09% --> 28.62%
2026-01-11 17:32:40,853: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_4500
2026-01-11 17:32:51,528: Train batch 4600: loss: 32.70 grad norm: 88.79 time: 0.077
2026-01-11 17:33:12,766: Train batch 4800: loss: 24.42 grad norm: 79.82 time: 0.073
2026-01-11 17:33:33,819: Train batch 5000: loss: 53.70 grad norm: 123.17 time: 0.077
2026-01-11 17:33:33,820: Running test after training batch: 5000
2026-01-11 17:33:33,952: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:33:40,309: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 17:33:40,366: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost of
2026-01-11 17:33:57,622: Val batch 5000: PER (avg): 0.2480 CTC Loss (avg): 39.9235 WER(5gram): 24.32% (n=256) time: 23.802
2026-01-11 17:33:57,623: WER lens: avg_true_words=5.99 avg_pred_words=5.94 max_pred_words=12
2026-01-11 17:33:57,624: t15.2023.08.13 val PER: 0.2256
2026-01-11 17:33:57,624: t15.2023.08.18 val PER: 0.2079
2026-01-11 17:33:57,624: t15.2023.08.20 val PER: 0.2057
2026-01-11 17:33:57,624: t15.2023.08.25 val PER: 0.1687
2026-01-11 17:33:57,624: t15.2023.08.27 val PER: 0.2637
2026-01-11 17:33:57,624: t15.2023.09.01 val PER: 0.1648
2026-01-11 17:33:57,624: t15.2023.09.03 val PER: 0.2732
2026-01-11 17:33:57,624: t15.2023.09.24 val PER: 0.1869
2026-01-11 17:33:57,624: t15.2023.09.29 val PER: 0.2163
2026-01-11 17:33:57,624: t15.2023.10.01 val PER: 0.2748
2026-01-11 17:33:57,624: t15.2023.10.06 val PER: 0.1733
2026-01-11 17:33:57,625: t15.2023.10.08 val PER: 0.3505
2026-01-11 17:33:57,625: t15.2023.10.13 val PER: 0.3320
2026-01-11 17:33:57,625: t15.2023.10.15 val PER: 0.2558
2026-01-11 17:33:57,625: t15.2023.10.20 val PER: 0.2919
2026-01-11 17:33:57,625: t15.2023.10.22 val PER: 0.2038
2026-01-11 17:33:57,625: t15.2023.11.03 val PER: 0.2531
2026-01-11 17:33:57,625: t15.2023.11.04 val PER: 0.0478
2026-01-11 17:33:57,625: t15.2023.11.17 val PER: 0.0871
2026-01-11 17:33:57,625: t15.2023.11.19 val PER: 0.0798
2026-01-11 17:33:57,625: t15.2023.11.26 val PER: 0.2587
2026-01-11 17:33:57,625: t15.2023.12.03 val PER: 0.2342
2026-01-11 17:33:57,625: t15.2023.12.08 val PER: 0.2310
2026-01-11 17:33:57,625: t15.2023.12.10 val PER: 0.1919
2026-01-11 17:33:57,626: t15.2023.12.17 val PER: 0.1985
2026-01-11 17:33:57,626: t15.2023.12.29 val PER: 0.2457
2026-01-11 17:33:57,626: t15.2024.02.25 val PER: 0.2331
2026-01-11 17:33:57,626: t15.2024.03.08 val PER: 0.3158
2026-01-11 17:33:57,626: t15.2024.03.15 val PER: 0.2927
2026-01-11 17:33:57,626: t15.2024.03.17 val PER: 0.2490
2026-01-11 17:33:57,626: t15.2024.05.10 val PER: 0.2585
2026-01-11 17:33:57,626: t15.2024.06.14 val PER: 0.2744
2026-01-11 17:33:57,626: t15.2024.07.19 val PER: 0.3237
2026-01-11 17:33:57,626: t15.2024.07.21 val PER: 0.1945
2026-01-11 17:33:57,626: t15.2024.07.28 val PER: 0.2257
2026-01-11 17:33:57,627: t15.2025.01.10 val PER: 0.3774
2026-01-11 17:33:57,627: t15.2025.01.12 val PER: 0.2633
2026-01-11 17:33:57,627: t15.2025.03.14 val PER: 0.4157
2026-01-11 17:33:57,627: t15.2025.03.16 val PER: 0.2971
2026-01-11 17:33:57,627: t15.2025.03.30 val PER: 0.3828
2026-01-11 17:33:57,627: t15.2025.04.13 val PER: 0.3067
2026-01-11 17:33:57,630: New best val WER(5gram) 28.62% --> 24.32%
2026-01-11 17:33:57,825: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_5000
2026-01-11 17:34:18,643: Train batch 5200: loss: 25.92 grad norm: 83.53 time: 0.062
2026-01-11 17:34:39,797: Train batch 5400: loss: 32.37 grad norm: 94.60 time: 0.080
2026-01-11 17:34:50,257: Running test after training batch: 5500
2026-01-11 17:34:50,452: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:34:57,059: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 17:34:57,108: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost of
2026-01-11 17:35:11,377: Val batch 5500: PER (avg): 0.2334 CTC Loss (avg): 37.6955 WER(5gram): 23.08% (n=256) time: 21.119
2026-01-11 17:35:11,378: WER lens: avg_true_words=5.99 avg_pred_words=6.01 max_pred_words=11
2026-01-11 17:35:11,378: t15.2023.08.13 val PER: 0.1965
2026-01-11 17:35:11,378: t15.2023.08.18 val PER: 0.1928
2026-01-11 17:35:11,378: t15.2023.08.20 val PER: 0.1835
2026-01-11 17:35:11,378: t15.2023.08.25 val PER: 0.1672
2026-01-11 17:35:11,379: t15.2023.08.27 val PER: 0.2492
2026-01-11 17:35:11,379: t15.2023.09.01 val PER: 0.1477
2026-01-11 17:35:11,379: t15.2023.09.03 val PER: 0.2411
2026-01-11 17:35:11,379: t15.2023.09.24 val PER: 0.1833
2026-01-11 17:35:11,379: t15.2023.09.29 val PER: 0.2017
2026-01-11 17:35:11,379: t15.2023.10.01 val PER: 0.2675
2026-01-11 17:35:11,379: t15.2023.10.06 val PER: 0.1529
2026-01-11 17:35:11,380: t15.2023.10.08 val PER: 0.3302
2026-01-11 17:35:11,380: t15.2023.10.13 val PER: 0.3313
2026-01-11 17:35:11,380: t15.2023.10.15 val PER: 0.2426
2026-01-11 17:35:11,380: t15.2023.10.20 val PER: 0.2886
2026-01-11 17:35:11,380: t15.2023.10.22 val PER: 0.1938
2026-01-11 17:35:11,380: t15.2023.11.03 val PER: 0.2456
2026-01-11 17:35:11,381: t15.2023.11.04 val PER: 0.0478
2026-01-11 17:35:11,381: t15.2023.11.17 val PER: 0.0809
2026-01-11 17:35:11,381: t15.2023.11.19 val PER: 0.0778
2026-01-11 17:35:11,381: t15.2023.11.26 val PER: 0.2420
2026-01-11 17:35:11,381: t15.2023.12.03 val PER: 0.1996
2026-01-11 17:35:11,381: t15.2023.12.08 val PER: 0.2077
2026-01-11 17:35:11,381: t15.2023.12.10 val PER: 0.1669
2026-01-11 17:35:11,381: t15.2023.12.17 val PER: 0.1965
2026-01-11 17:35:11,382: t15.2023.12.29 val PER: 0.2327
2026-01-11 17:35:11,382: t15.2024.02.25 val PER: 0.2022
2026-01-11 17:35:11,382: t15.2024.03.08 val PER: 0.2930
2026-01-11 17:35:11,382: t15.2024.03.15 val PER: 0.2871
2026-01-11 17:35:11,382: t15.2024.03.17 val PER: 0.2280
2026-01-11 17:35:11,382: t15.2024.05.10 val PER: 0.2496
2026-01-11 17:35:11,382: t15.2024.06.14 val PER: 0.2366
2026-01-11 17:35:11,382: t15.2024.07.19 val PER: 0.3217
2026-01-11 17:35:11,383: t15.2024.07.21 val PER: 0.1697
2026-01-11 17:35:11,383: t15.2024.07.28 val PER: 0.2309
2026-01-11 17:35:11,383: t15.2025.01.10 val PER: 0.3705
2026-01-11 17:35:11,383: t15.2025.01.12 val PER: 0.2456
2026-01-11 17:35:11,383: t15.2025.03.14 val PER: 0.3920
2026-01-11 17:35:11,383: t15.2025.03.16 val PER: 0.2801
2026-01-11 17:35:11,383: t15.2025.03.30 val PER: 0.3667
2026-01-11 17:35:11,383: t15.2025.04.13 val PER: 0.2853
2026-01-11 17:35:11,384: New best val WER(5gram) 24.32% --> 23.08%
2026-01-11 17:35:11,551: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_5500
2026-01-11 17:35:22,245: Train batch 5600: loss: 31.40 grad norm: 89.82 time: 0.073
2026-01-11 17:35:43,964: Train batch 5800: loss: 22.77 grad norm: 83.31 time: 0.095
2026-01-11 17:36:05,106: Train batch 6000: loss: 21.27 grad norm: 73.95 time: 0.059
2026-01-11 17:36:05,106: Running test after training batch: 6000
2026-01-11 17:36:05,247: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:36:11,957: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point as well
2026-01-11 17:36:12,018: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost of
2026-01-11 17:36:27,425: Val batch 6000: PER (avg): 0.2301 CTC Loss (avg): 36.8122 WER(5gram): 22.03% (n=256) time: 22.318
2026-01-11 17:36:27,426: WER lens: avg_true_words=5.99 avg_pred_words=5.89 max_pred_words=12
2026-01-11 17:36:27,426: t15.2023.08.13 val PER: 0.2058
2026-01-11 17:36:27,426: t15.2023.08.18 val PER: 0.1961
2026-01-11 17:36:27,426: t15.2023.08.20 val PER: 0.1811
2026-01-11 17:36:27,426: t15.2023.08.25 val PER: 0.1611
2026-01-11 17:36:27,426: t15.2023.08.27 val PER: 0.2556
2026-01-11 17:36:27,426: t15.2023.09.01 val PER: 0.1526
2026-01-11 17:36:27,426: t15.2023.09.03 val PER: 0.2411
2026-01-11 17:36:27,426: t15.2023.09.24 val PER: 0.1808
2026-01-11 17:36:27,427: t15.2023.09.29 val PER: 0.2049
2026-01-11 17:36:27,427: t15.2023.10.01 val PER: 0.2675
2026-01-11 17:36:27,427: t15.2023.10.06 val PER: 0.1679
2026-01-11 17:36:27,427: t15.2023.10.08 val PER: 0.3369
2026-01-11 17:36:27,427: t15.2023.10.13 val PER: 0.3157
2026-01-11 17:36:27,427: t15.2023.10.15 val PER: 0.2439
2026-01-11 17:36:27,427: t15.2023.10.20 val PER: 0.2450
2026-01-11 17:36:27,428: t15.2023.10.22 val PER: 0.1960
2026-01-11 17:36:27,428: t15.2023.11.03 val PER: 0.2381
2026-01-11 17:36:27,428: t15.2023.11.04 val PER: 0.0341
2026-01-11 17:36:27,428: t15.2023.11.17 val PER: 0.0684
2026-01-11 17:36:27,428: t15.2023.11.19 val PER: 0.0679
2026-01-11 17:36:27,428: t15.2023.11.26 val PER: 0.2319
2026-01-11 17:36:27,428: t15.2023.12.03 val PER: 0.1922
2026-01-11 17:36:27,429: t15.2023.12.08 val PER: 0.2004
2026-01-11 17:36:27,429: t15.2023.12.10 val PER: 0.1761
2026-01-11 17:36:27,429: t15.2023.12.17 val PER: 0.1871
2026-01-11 17:36:27,429: t15.2023.12.29 val PER: 0.2258
2026-01-11 17:36:27,429: t15.2024.02.25 val PER: 0.2107
2026-01-11 17:36:27,429: t15.2024.03.08 val PER: 0.2973
2026-01-11 17:36:27,429: t15.2024.03.15 val PER: 0.2821
2026-01-11 17:36:27,430: t15.2024.03.17 val PER: 0.2211
2026-01-11 17:36:27,430: t15.2024.05.10 val PER: 0.2392
2026-01-11 17:36:27,430: t15.2024.06.14 val PER: 0.2461
2026-01-11 17:36:27,430: t15.2024.07.19 val PER: 0.3072
2026-01-11 17:36:27,430: t15.2024.07.21 val PER: 0.1731
2026-01-11 17:36:27,430: t15.2024.07.28 val PER: 0.2228
2026-01-11 17:36:27,430: t15.2025.01.10 val PER: 0.3623
2026-01-11 17:36:27,431: t15.2025.01.12 val PER: 0.2525
2026-01-11 17:36:27,431: t15.2025.03.14 val PER: 0.3772
2026-01-11 17:36:27,431: t15.2025.03.16 val PER: 0.2618
2026-01-11 17:36:27,431: t15.2025.03.30 val PER: 0.3678
2026-01-11 17:36:27,431: t15.2025.04.13 val PER: 0.2653
2026-01-11 17:36:27,432: New best val WER(5gram) 23.08% --> 22.03%
2026-01-11 17:36:27,630: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_6000
2026-01-11 17:36:48,415: Train batch 6200: loss: 25.06 grad norm: 81.30 time: 0.082
2026-01-11 17:37:09,004: Train batch 6400: loss: 34.32 grad norm: 97.90 time: 0.074
2026-01-11 17:37:19,057: Running test after training batch: 6500
2026-01-11 17:37:19,228: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:37:26,292: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point as well
2026-01-11 17:37:26,338: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 17:37:41,154: Val batch 6500: PER (avg): 0.2120 CTC Loss (avg): 35.4157 WER(5gram): 18.77% (n=256) time: 22.096
2026-01-11 17:37:41,155: WER lens: avg_true_words=5.99 avg_pred_words=5.97 max_pred_words=12
2026-01-11 17:37:41,155: t15.2023.08.13 val PER: 0.1830
2026-01-11 17:37:41,155: t15.2023.08.18 val PER: 0.1651
2026-01-11 17:37:41,156: t15.2023.08.20 val PER: 0.1636
2026-01-11 17:37:41,156: t15.2023.08.25 val PER: 0.1416
2026-01-11 17:37:41,156: t15.2023.08.27 val PER: 0.2363
2026-01-11 17:37:41,156: t15.2023.09.01 val PER: 0.1234
2026-01-11 17:37:41,156: t15.2023.09.03 val PER: 0.2221
2026-01-11 17:37:41,156: t15.2023.09.24 val PER: 0.1675
2026-01-11 17:37:41,156: t15.2023.09.29 val PER: 0.1902
2026-01-11 17:37:41,156: t15.2023.10.01 val PER: 0.2450
2026-01-11 17:37:41,156: t15.2023.10.06 val PER: 0.1475
2026-01-11 17:37:41,156: t15.2023.10.08 val PER: 0.3424
2026-01-11 17:37:41,156: t15.2023.10.13 val PER: 0.3041
2026-01-11 17:37:41,157: t15.2023.10.15 val PER: 0.2189
2026-01-11 17:37:41,157: t15.2023.10.20 val PER: 0.2450
2026-01-11 17:37:41,157: t15.2023.10.22 val PER: 0.1893
2026-01-11 17:37:41,157: t15.2023.11.03 val PER: 0.2280
2026-01-11 17:37:41,157: t15.2023.11.04 val PER: 0.0341
2026-01-11 17:37:41,157: t15.2023.11.17 val PER: 0.0653
2026-01-11 17:37:41,157: t15.2023.11.19 val PER: 0.0619
2026-01-11 17:37:41,157: t15.2023.11.26 val PER: 0.2051
2026-01-11 17:37:41,157: t15.2023.12.03 val PER: 0.1733
2026-01-11 17:37:41,157: t15.2023.12.08 val PER: 0.1711
2026-01-11 17:37:41,157: t15.2023.12.10 val PER: 0.1577
2026-01-11 17:37:41,157: t15.2023.12.17 val PER: 0.1736
2026-01-11 17:37:41,158: t15.2023.12.29 val PER: 0.2073
2026-01-11 17:37:41,158: t15.2024.02.25 val PER: 0.1868
2026-01-11 17:37:41,158: t15.2024.03.08 val PER: 0.3044
2026-01-11 17:37:41,158: t15.2024.03.15 val PER: 0.2483
2026-01-11 17:37:41,158: t15.2024.03.17 val PER: 0.2197
2026-01-11 17:37:41,158: t15.2024.05.10 val PER: 0.2288
2026-01-11 17:37:41,158: t15.2024.06.14 val PER: 0.2098
2026-01-11 17:37:41,158: t15.2024.07.19 val PER: 0.2894
2026-01-11 17:37:41,158: t15.2024.07.21 val PER: 0.1476
2026-01-11 17:37:41,158: t15.2024.07.28 val PER: 0.1904
2026-01-11 17:37:41,158: t15.2025.01.10 val PER: 0.3581
2026-01-11 17:37:41,158: t15.2025.01.12 val PER: 0.2240
2026-01-11 17:37:41,159: t15.2025.03.14 val PER: 0.3802
2026-01-11 17:37:41,159: t15.2025.03.16 val PER: 0.2421
2026-01-11 17:37:41,159: t15.2025.03.30 val PER: 0.3448
2026-01-11 17:37:41,159: t15.2025.04.13 val PER: 0.2668
2026-01-11 17:37:41,161: New best val WER(5gram) 22.03% --> 18.77%
2026-01-11 17:37:41,370: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_6500
2026-01-11 17:37:51,856: Train batch 6600: loss: 20.12 grad norm: 66.87 time: 0.053
2026-01-11 17:38:12,785: Train batch 6800: loss: 27.48 grad norm: 81.37 time: 0.058
2026-01-11 17:38:34,588: Train batch 7000: loss: 26.40 grad norm: 90.39 time: 0.074
2026-01-11 17:38:34,589: Running test after training batch: 7000
2026-01-11 17:38:35,008: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:38:41,747: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point as well
2026-01-11 17:38:41,797: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 17:38:56,331: Val batch 7000: PER (avg): 0.2071 CTC Loss (avg): 33.7173 WER(5gram): 20.34% (n=256) time: 21.742
2026-01-11 17:38:56,332: WER lens: avg_true_words=5.99 avg_pred_words=5.95 max_pred_words=11
2026-01-11 17:38:56,332: t15.2023.08.13 val PER: 0.1819
2026-01-11 17:38:56,332: t15.2023.08.18 val PER: 0.1760
2026-01-11 17:38:56,332: t15.2023.08.20 val PER: 0.1477
2026-01-11 17:38:56,332: t15.2023.08.25 val PER: 0.1370
2026-01-11 17:38:56,332: t15.2023.08.27 val PER: 0.2379
2026-01-11 17:38:56,333: t15.2023.09.01 val PER: 0.1250
2026-01-11 17:38:56,333: t15.2023.09.03 val PER: 0.2328
2026-01-11 17:38:56,333: t15.2023.09.24 val PER: 0.1626
2026-01-11 17:38:56,333: t15.2023.09.29 val PER: 0.1806
2026-01-11 17:38:56,333: t15.2023.10.01 val PER: 0.2483
2026-01-11 17:38:56,333: t15.2023.10.06 val PER: 0.1259
2026-01-11 17:38:56,333: t15.2023.10.08 val PER: 0.3099
2026-01-11 17:38:56,334: t15.2023.10.13 val PER: 0.2909
2026-01-11 17:38:56,334: t15.2023.10.15 val PER: 0.2090
2026-01-11 17:38:56,334: t15.2023.10.20 val PER: 0.2450
2026-01-11 17:38:56,334: t15.2023.10.22 val PER: 0.1771
2026-01-11 17:38:56,334: t15.2023.11.03 val PER: 0.2266
2026-01-11 17:38:56,334: t15.2023.11.04 val PER: 0.0410
2026-01-11 17:38:56,334: t15.2023.11.17 val PER: 0.0684
2026-01-11 17:38:56,334: t15.2023.11.19 val PER: 0.0459
2026-01-11 17:38:56,334: t15.2023.11.26 val PER: 0.1971
2026-01-11 17:38:56,334: t15.2023.12.03 val PER: 0.1586
2026-01-11 17:38:56,334: t15.2023.12.08 val PER: 0.1611
2026-01-11 17:38:56,335: t15.2023.12.10 val PER: 0.1327
2026-01-11 17:38:56,335: t15.2023.12.17 val PER: 0.1642
2026-01-11 17:38:56,335: t15.2023.12.29 val PER: 0.1997
2026-01-11 17:38:56,335: t15.2024.02.25 val PER: 0.1966
2026-01-11 17:38:56,335: t15.2024.03.08 val PER: 0.2760
2026-01-11 17:38:56,335: t15.2024.03.15 val PER: 0.2552
2026-01-11 17:38:56,335: t15.2024.03.17 val PER: 0.2050
2026-01-11 17:38:56,335: t15.2024.05.10 val PER: 0.2110
2026-01-11 17:38:56,335: t15.2024.06.14 val PER: 0.1909
2026-01-11 17:38:56,335: t15.2024.07.19 val PER: 0.3052
2026-01-11 17:38:56,336: t15.2024.07.21 val PER: 0.1428
2026-01-11 17:38:56,336: t15.2024.07.28 val PER: 0.2022
2026-01-11 17:38:56,336: t15.2025.01.10 val PER: 0.3664
2026-01-11 17:38:56,336: t15.2025.01.12 val PER: 0.2225
2026-01-11 17:38:56,336: t15.2025.03.14 val PER: 0.3772
2026-01-11 17:38:56,336: t15.2025.03.16 val PER: 0.2291
2026-01-11 17:38:56,336: t15.2025.03.30 val PER: 0.3494
2026-01-11 17:38:56,336: t15.2025.04.13 val PER: 0.2668
2026-01-11 17:38:56,496: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_7000
2026-01-11 17:39:17,703: Train batch 7200: loss: 22.44 grad norm: 90.35 time: 0.090
2026-01-11 17:39:39,128: Train batch 7400: loss: 23.78 grad norm: 79.88 time: 0.088
2026-01-11 17:39:49,517: Running test after training batch: 7500
2026-01-11 17:39:49,763: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:39:56,187: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 17:39:56,262: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 17:40:10,706: Val batch 7500: PER (avg): 0.1975 CTC Loss (avg): 32.3631 WER(5gram): 18.12% (n=256) time: 21.189
2026-01-11 17:40:10,707: WER lens: avg_true_words=5.99 avg_pred_words=6.04 max_pred_words=12
2026-01-11 17:40:10,707: t15.2023.08.13 val PER: 0.1694
2026-01-11 17:40:10,707: t15.2023.08.18 val PER: 0.1567
2026-01-11 17:40:10,707: t15.2023.08.20 val PER: 0.1438
2026-01-11 17:40:10,707: t15.2023.08.25 val PER: 0.1220
2026-01-11 17:40:10,707: t15.2023.08.27 val PER: 0.2058
2026-01-11 17:40:10,707: t15.2023.09.01 val PER: 0.1128
2026-01-11 17:40:10,707: t15.2023.09.03 val PER: 0.2150
2026-01-11 17:40:10,708: t15.2023.09.24 val PER: 0.1505
2026-01-11 17:40:10,708: t15.2023.09.29 val PER: 0.1691
2026-01-11 17:40:10,708: t15.2023.10.01 val PER: 0.2305
2026-01-11 17:40:10,708: t15.2023.10.06 val PER: 0.1259
2026-01-11 17:40:10,708: t15.2023.10.08 val PER: 0.2991
2026-01-11 17:40:10,708: t15.2023.10.13 val PER: 0.2801
2026-01-11 17:40:10,708: t15.2023.10.15 val PER: 0.2090
2026-01-11 17:40:10,709: t15.2023.10.20 val PER: 0.2248
2026-01-11 17:40:10,709: t15.2023.10.22 val PER: 0.1715
2026-01-11 17:40:10,709: t15.2023.11.03 val PER: 0.2103
2026-01-11 17:40:10,709: t15.2023.11.04 val PER: 0.0341
2026-01-11 17:40:10,709: t15.2023.11.17 val PER: 0.0544
2026-01-11 17:40:10,709: t15.2023.11.19 val PER: 0.0439
2026-01-11 17:40:10,709: t15.2023.11.26 val PER: 0.1899
2026-01-11 17:40:10,709: t15.2023.12.03 val PER: 0.1597
2026-01-11 17:40:10,710: t15.2023.12.08 val PER: 0.1598
2026-01-11 17:40:10,710: t15.2023.12.10 val PER: 0.1327
2026-01-11 17:40:10,710: t15.2023.12.17 val PER: 0.1601
2026-01-11 17:40:10,710: t15.2023.12.29 val PER: 0.1771
2026-01-11 17:40:10,710: t15.2024.02.25 val PER: 0.1868
2026-01-11 17:40:10,710: t15.2024.03.08 val PER: 0.2717
2026-01-11 17:40:10,710: t15.2024.03.15 val PER: 0.2514
2026-01-11 17:40:10,710: t15.2024.03.17 val PER: 0.1960
2026-01-11 17:40:10,711: t15.2024.05.10 val PER: 0.2155
2026-01-11 17:40:10,711: t15.2024.06.14 val PER: 0.2129
2026-01-11 17:40:10,711: t15.2024.07.19 val PER: 0.2821
2026-01-11 17:40:10,711: t15.2024.07.21 val PER: 0.1414
2026-01-11 17:40:10,711: t15.2024.07.28 val PER: 0.1926
2026-01-11 17:40:10,711: t15.2025.01.10 val PER: 0.3457
2026-01-11 17:40:10,711: t15.2025.01.12 val PER: 0.2063
2026-01-11 17:40:10,711: t15.2025.03.14 val PER: 0.3491
2026-01-11 17:40:10,711: t15.2025.03.16 val PER: 0.2356
2026-01-11 17:40:10,711: t15.2025.03.30 val PER: 0.3322
2026-01-11 17:40:10,711: t15.2025.04.13 val PER: 0.2725
2026-01-11 17:40:10,712: New best val WER(5gram) 18.77% --> 18.12%
2026-01-11 17:40:10,880: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_7500
2026-01-11 17:40:21,214: Train batch 7600: loss: 24.13 grad norm: 85.19 time: 0.081
2026-01-11 17:40:41,932: Train batch 7800: loss: 21.95 grad norm: 82.33 time: 0.065
2026-01-11 17:41:02,859: Train batch 8000: loss: 17.35 grad norm: 70.99 time: 0.084
2026-01-11 17:41:02,860: Running test after training batch: 8000
2026-01-11 17:41:02,974: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:41:09,846: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 17:41:09,916: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 17:41:23,649: Val batch 8000: PER (avg): 0.1877 CTC Loss (avg): 31.2783 WER(5gram): 16.43% (n=256) time: 20.789
2026-01-11 17:41:23,650: WER lens: avg_true_words=5.99 avg_pred_words=5.96 max_pred_words=11
2026-01-11 17:41:23,650: t15.2023.08.13 val PER: 0.1622
2026-01-11 17:41:23,650: t15.2023.08.18 val PER: 0.1542
2026-01-11 17:41:23,650: t15.2023.08.20 val PER: 0.1374
2026-01-11 17:41:23,651: t15.2023.08.25 val PER: 0.1325
2026-01-11 17:41:23,651: t15.2023.08.27 val PER: 0.2010
2026-01-11 17:41:23,651: t15.2023.09.01 val PER: 0.1006
2026-01-11 17:41:23,651: t15.2023.09.03 val PER: 0.1960
2026-01-11 17:41:23,651: t15.2023.09.24 val PER: 0.1420
2026-01-11 17:41:23,651: t15.2023.09.29 val PER: 0.1595
2026-01-11 17:41:23,651: t15.2023.10.01 val PER: 0.2226
2026-01-11 17:41:23,651: t15.2023.10.06 val PER: 0.1152
2026-01-11 17:41:23,651: t15.2023.10.08 val PER: 0.2950
2026-01-11 17:41:23,651: t15.2023.10.13 val PER: 0.2808
2026-01-11 17:41:23,651: t15.2023.10.15 val PER: 0.1971
2026-01-11 17:41:23,651: t15.2023.10.20 val PER: 0.2483
2026-01-11 17:41:23,652: t15.2023.10.22 val PER: 0.1759
2026-01-11 17:41:23,652: t15.2023.11.03 val PER: 0.2198
2026-01-11 17:41:23,652: t15.2023.11.04 val PER: 0.0341
2026-01-11 17:41:23,652: t15.2023.11.17 val PER: 0.0529
2026-01-11 17:41:23,652: t15.2023.11.19 val PER: 0.0559
2026-01-11 17:41:23,652: t15.2023.11.26 val PER: 0.1659
2026-01-11 17:41:23,652: t15.2023.12.03 val PER: 0.1555
2026-01-11 17:41:23,652: t15.2023.12.08 val PER: 0.1391
2026-01-11 17:41:23,653: t15.2023.12.10 val PER: 0.1327
2026-01-11 17:41:23,653: t15.2023.12.17 val PER: 0.1414
2026-01-11 17:41:23,653: t15.2023.12.29 val PER: 0.1613
2026-01-11 17:41:23,653: t15.2024.02.25 val PER: 0.1517
2026-01-11 17:41:23,653: t15.2024.03.08 val PER: 0.2560
2026-01-11 17:41:23,653: t15.2024.03.15 val PER: 0.2433
2026-01-11 17:41:23,653: t15.2024.03.17 val PER: 0.1841
2026-01-11 17:41:23,653: t15.2024.05.10 val PER: 0.2065
2026-01-11 17:41:23,653: t15.2024.06.14 val PER: 0.2066
2026-01-11 17:41:23,653: t15.2024.07.19 val PER: 0.2663
2026-01-11 17:41:23,653: t15.2024.07.21 val PER: 0.1283
2026-01-11 17:41:23,653: t15.2024.07.28 val PER: 0.1750
2026-01-11 17:41:23,653: t15.2025.01.10 val PER: 0.3251
2026-01-11 17:41:23,654: t15.2025.01.12 val PER: 0.1917
2026-01-11 17:41:23,654: t15.2025.03.14 val PER: 0.3698
2026-01-11 17:41:23,654: t15.2025.03.16 val PER: 0.2120
2026-01-11 17:41:23,654: t15.2025.03.30 val PER: 0.3034
2026-01-11 17:41:23,654: t15.2025.04.13 val PER: 0.2525
2026-01-11 17:41:23,655: New best val WER(5gram) 18.12% --> 16.43%
2026-01-11 17:41:23,831: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_8000
2026-01-11 17:41:45,099: Train batch 8200: loss: 15.05 grad norm: 66.03 time: 0.064
2026-01-11 17:42:06,308: Train batch 8400: loss: 16.44 grad norm: 70.58 time: 0.074
2026-01-11 17:42:17,027: Running test after training batch: 8500
2026-01-11 17:42:17,149: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:42:23,815: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 17:42:23,871: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 17:42:36,933: Val batch 8500: PER (avg): 0.1805 CTC Loss (avg): 30.1717 WER(5gram): 17.01% (n=256) time: 19.905
2026-01-11 17:42:36,934: WER lens: avg_true_words=5.99 avg_pred_words=6.01 max_pred_words=11
2026-01-11 17:42:36,934: t15.2023.08.13 val PER: 0.1528
2026-01-11 17:42:36,934: t15.2023.08.18 val PER: 0.1551
2026-01-11 17:42:36,934: t15.2023.08.20 val PER: 0.1247
2026-01-11 17:42:36,934: t15.2023.08.25 val PER: 0.1205
2026-01-11 17:42:36,934: t15.2023.08.27 val PER: 0.2138
2026-01-11 17:42:36,935: t15.2023.09.01 val PER: 0.0982
2026-01-11 17:42:36,935: t15.2023.09.03 val PER: 0.1876
2026-01-11 17:42:36,935: t15.2023.09.24 val PER: 0.1347
2026-01-11 17:42:36,935: t15.2023.09.29 val PER: 0.1698
2026-01-11 17:42:36,935: t15.2023.10.01 val PER: 0.2180
2026-01-11 17:42:36,935: t15.2023.10.06 val PER: 0.1109
2026-01-11 17:42:36,936: t15.2023.10.08 val PER: 0.3112
2026-01-11 17:42:36,936: t15.2023.10.13 val PER: 0.2715
2026-01-11 17:42:36,936: t15.2023.10.15 val PER: 0.1879
2026-01-11 17:42:36,936: t15.2023.10.20 val PER: 0.2416
2026-01-11 17:42:36,936: t15.2023.10.22 val PER: 0.1581
2026-01-11 17:42:36,936: t15.2023.11.03 val PER: 0.2049
2026-01-11 17:42:36,936: t15.2023.11.04 val PER: 0.0273
2026-01-11 17:42:36,937: t15.2023.11.17 val PER: 0.0435
2026-01-11 17:42:36,937: t15.2023.11.19 val PER: 0.0319
2026-01-11 17:42:36,937: t15.2023.11.26 val PER: 0.1486
2026-01-11 17:42:36,937: t15.2023.12.03 val PER: 0.1324
2026-01-11 17:42:36,937: t15.2023.12.08 val PER: 0.1312
2026-01-11 17:42:36,937: t15.2023.12.10 val PER: 0.1209
2026-01-11 17:42:36,937: t15.2023.12.17 val PER: 0.1445
2026-01-11 17:42:36,937: t15.2023.12.29 val PER: 0.1668
2026-01-11 17:42:36,938: t15.2024.02.25 val PER: 0.1559
2026-01-11 17:42:36,938: t15.2024.03.08 val PER: 0.2461
2026-01-11 17:42:36,938: t15.2024.03.15 val PER: 0.2264
2026-01-11 17:42:36,938: t15.2024.03.17 val PER: 0.1688
2026-01-11 17:42:36,938: t15.2024.05.10 val PER: 0.2021
2026-01-11 17:42:36,938: t15.2024.06.14 val PER: 0.1845
2026-01-11 17:42:36,938: t15.2024.07.19 val PER: 0.2597
2026-01-11 17:42:36,939: t15.2024.07.21 val PER: 0.1297
2026-01-11 17:42:36,939: t15.2024.07.28 val PER: 0.1684
2026-01-11 17:42:36,939: t15.2025.01.10 val PER: 0.3278
2026-01-11 17:42:36,939: t15.2025.01.12 val PER: 0.1794
2026-01-11 17:42:36,939: t15.2025.03.14 val PER: 0.3565
2026-01-11 17:42:36,939: t15.2025.03.16 val PER: 0.2016
2026-01-11 17:42:36,939: t15.2025.03.30 val PER: 0.3000
2026-01-11 17:42:36,939: t15.2025.04.13 val PER: 0.2397
2026-01-11 17:42:37,089: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_8500
2026-01-11 17:42:47,472: Train batch 8600: loss: 22.83 grad norm: 78.59 time: 0.066
2026-01-11 17:43:08,377: Train batch 8800: loss: 23.29 grad norm: 77.57 time: 0.071
2026-01-11 17:43:29,281: Train batch 9000: loss: 24.96 grad norm: 93.92 time: 0.084
2026-01-11 17:43:29,282: Running test after training batch: 9000
2026-01-11 17:43:29,408: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:43:35,751: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 17:43:35,805: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 17:43:48,620: Val batch 9000: PER (avg): 0.1757 CTC Loss (avg): 29.5211 WER(5gram): 15.91% (n=256) time: 19.338
2026-01-11 17:43:48,621: WER lens: avg_true_words=5.99 avg_pred_words=5.95 max_pred_words=11
2026-01-11 17:43:48,621: t15.2023.08.13 val PER: 0.1476
2026-01-11 17:43:48,621: t15.2023.08.18 val PER: 0.1408
2026-01-11 17:43:48,621: t15.2023.08.20 val PER: 0.1183
2026-01-11 17:43:48,621: t15.2023.08.25 val PER: 0.1084
2026-01-11 17:43:48,622: t15.2023.08.27 val PER: 0.1945
2026-01-11 17:43:48,622: t15.2023.09.01 val PER: 0.0998
2026-01-11 17:43:48,622: t15.2023.09.03 val PER: 0.1888
2026-01-11 17:43:48,622: t15.2023.09.24 val PER: 0.1335
2026-01-11 17:43:48,622: t15.2023.09.29 val PER: 0.1551
2026-01-11 17:43:48,622: t15.2023.10.01 val PER: 0.2160
2026-01-11 17:43:48,622: t15.2023.10.06 val PER: 0.1076
2026-01-11 17:43:48,622: t15.2023.10.08 val PER: 0.2788
2026-01-11 17:43:48,623: t15.2023.10.13 val PER: 0.2645
2026-01-11 17:43:48,623: t15.2023.10.15 val PER: 0.1898
2026-01-11 17:43:48,623: t15.2023.10.20 val PER: 0.2248
2026-01-11 17:43:48,623: t15.2023.10.22 val PER: 0.1470
2026-01-11 17:43:48,623: t15.2023.11.03 val PER: 0.2008
2026-01-11 17:43:48,623: t15.2023.11.04 val PER: 0.0341
2026-01-11 17:43:48,623: t15.2023.11.17 val PER: 0.0513
2026-01-11 17:43:48,623: t15.2023.11.19 val PER: 0.0479
2026-01-11 17:43:48,623: t15.2023.11.26 val PER: 0.1594
2026-01-11 17:43:48,623: t15.2023.12.03 val PER: 0.1282
2026-01-11 17:43:48,624: t15.2023.12.08 val PER: 0.1278
2026-01-11 17:43:48,624: t15.2023.12.10 val PER: 0.1104
2026-01-11 17:43:48,624: t15.2023.12.17 val PER: 0.1403
2026-01-11 17:43:48,624: t15.2023.12.29 val PER: 0.1579
2026-01-11 17:43:48,624: t15.2024.02.25 val PER: 0.1489
2026-01-11 17:43:48,624: t15.2024.03.08 val PER: 0.2461
2026-01-11 17:43:48,624: t15.2024.03.15 val PER: 0.2195
2026-01-11 17:43:48,624: t15.2024.03.17 val PER: 0.1743
2026-01-11 17:43:48,624: t15.2024.05.10 val PER: 0.1753
2026-01-11 17:43:48,624: t15.2024.06.14 val PER: 0.1798
2026-01-11 17:43:48,624: t15.2024.07.19 val PER: 0.2597
2026-01-11 17:43:48,625: t15.2024.07.21 val PER: 0.1234
2026-01-11 17:43:48,625: t15.2024.07.28 val PER: 0.1640
2026-01-11 17:43:48,625: t15.2025.01.10 val PER: 0.3154
2026-01-11 17:43:48,625: t15.2025.01.12 val PER: 0.1824
2026-01-11 17:43:48,625: t15.2025.03.14 val PER: 0.3269
2026-01-11 17:43:48,625: t15.2025.03.16 val PER: 0.2016
2026-01-11 17:43:48,625: t15.2025.03.30 val PER: 0.2851
2026-01-11 17:43:48,625: t15.2025.04.13 val PER: 0.2568
2026-01-11 17:43:48,626: New best val WER(5gram) 16.43% --> 15.91%
2026-01-11 17:43:48,789: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_9000
2026-01-11 17:44:09,543: Train batch 9200: loss: 15.61 grad norm: 67.37 time: 0.068
2026-01-11 17:44:30,330: Train batch 9400: loss: 9.49 grad norm: 58.49 time: 0.080
2026-01-11 17:44:40,599: Running test after training batch: 9500
2026-01-11 17:44:40,717: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:44:46,912: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 17:44:46,965: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 17:45:01,532: Val batch 9500: PER (avg): 0.1725 CTC Loss (avg): 29.1708 WER(5gram): 14.60% (n=256) time: 20.933
2026-01-11 17:45:01,533: WER lens: avg_true_words=5.99 avg_pred_words=6.04 max_pred_words=11
2026-01-11 17:45:01,533: t15.2023.08.13 val PER: 0.1414
2026-01-11 17:45:01,533: t15.2023.08.18 val PER: 0.1425
2026-01-11 17:45:01,533: t15.2023.08.20 val PER: 0.1176
2026-01-11 17:45:01,533: t15.2023.08.25 val PER: 0.1024
2026-01-11 17:45:01,534: t15.2023.08.27 val PER: 0.2010
2026-01-11 17:45:01,534: t15.2023.09.01 val PER: 0.0990
2026-01-11 17:45:01,534: t15.2023.09.03 val PER: 0.1817
2026-01-11 17:45:01,534: t15.2023.09.24 val PER: 0.1274
2026-01-11 17:45:01,534: t15.2023.09.29 val PER: 0.1583
2026-01-11 17:45:01,534: t15.2023.10.01 val PER: 0.2114
2026-01-11 17:45:01,534: t15.2023.10.06 val PER: 0.1087
2026-01-11 17:45:01,534: t15.2023.10.08 val PER: 0.2909
2026-01-11 17:45:01,535: t15.2023.10.13 val PER: 0.2521
2026-01-11 17:45:01,535: t15.2023.10.15 val PER: 0.1833
2026-01-11 17:45:01,535: t15.2023.10.20 val PER: 0.2181
2026-01-11 17:45:01,535: t15.2023.10.22 val PER: 0.1559
2026-01-11 17:45:01,535: t15.2023.11.03 val PER: 0.1920
2026-01-11 17:45:01,535: t15.2023.11.04 val PER: 0.0410
2026-01-11 17:45:01,536: t15.2023.11.17 val PER: 0.0560
2026-01-11 17:45:01,536: t15.2023.11.19 val PER: 0.0499
2026-01-11 17:45:01,536: t15.2023.11.26 val PER: 0.1341
2026-01-11 17:45:01,536: t15.2023.12.03 val PER: 0.1261
2026-01-11 17:45:01,536: t15.2023.12.08 val PER: 0.1218
2026-01-11 17:45:01,536: t15.2023.12.10 val PER: 0.1156
2026-01-11 17:45:01,536: t15.2023.12.17 val PER: 0.1341
2026-01-11 17:45:01,536: t15.2023.12.29 val PER: 0.1469
2026-01-11 17:45:01,537: t15.2024.02.25 val PER: 0.1601
2026-01-11 17:45:01,537: t15.2024.03.08 val PER: 0.2518
2026-01-11 17:45:01,537: t15.2024.03.15 val PER: 0.2164
2026-01-11 17:45:01,537: t15.2024.03.17 val PER: 0.1639
2026-01-11 17:45:01,537: t15.2024.05.10 val PER: 0.1768
2026-01-11 17:45:01,537: t15.2024.06.14 val PER: 0.1735
2026-01-11 17:45:01,537: t15.2024.07.19 val PER: 0.2518
2026-01-11 17:45:01,537: t15.2024.07.21 val PER: 0.1297
2026-01-11 17:45:01,538: t15.2024.07.28 val PER: 0.1603
2026-01-11 17:45:01,538: t15.2025.01.10 val PER: 0.3209
2026-01-11 17:45:01,538: t15.2025.01.12 val PER: 0.1740
2026-01-11 17:45:01,538: t15.2025.03.14 val PER: 0.3373
2026-01-11 17:45:01,538: t15.2025.03.16 val PER: 0.2016
2026-01-11 17:45:01,538: t15.2025.03.30 val PER: 0.2862
2026-01-11 17:45:01,538: t15.2025.04.13 val PER: 0.2382
2026-01-11 17:45:01,539: New best val WER(5gram) 15.91% --> 14.60%
2026-01-11 17:45:01,712: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_9500
2026-01-11 17:45:12,149: Train batch 9600: loss: 13.09 grad norm: 70.53 time: 0.084
2026-01-11 17:45:32,930: Train batch 9800: loss: 18.07 grad norm: 85.79 time: 0.075
2026-01-11 17:45:54,179: Train batch 10000: loss: 6.81 grad norm: 45.32 time: 0.072
2026-01-11 17:45:54,179: Running test after training batch: 10000
2026-01-11 17:45:54,337: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:46:00,557: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 17:46:00,608: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 17:46:12,991: Val batch 10000: PER (avg): 0.1717 CTC Loss (avg): 28.3865 WER(5gram): 16.04% (n=256) time: 18.812
2026-01-11 17:46:12,992: WER lens: avg_true_words=5.99 avg_pred_words=5.98 max_pred_words=12
2026-01-11 17:46:12,992: t15.2023.08.13 val PER: 0.1383
2026-01-11 17:46:12,993: t15.2023.08.18 val PER: 0.1316
2026-01-11 17:46:12,993: t15.2023.08.20 val PER: 0.1255
2026-01-11 17:46:12,993: t15.2023.08.25 val PER: 0.1145
2026-01-11 17:46:12,993: t15.2023.08.27 val PER: 0.1994
2026-01-11 17:46:12,993: t15.2023.09.01 val PER: 0.0942
2026-01-11 17:46:12,993: t15.2023.09.03 val PER: 0.1888
2026-01-11 17:46:12,993: t15.2023.09.24 val PER: 0.1323
2026-01-11 17:46:12,993: t15.2023.09.29 val PER: 0.1563
2026-01-11 17:46:12,993: t15.2023.10.01 val PER: 0.2180
2026-01-11 17:46:12,993: t15.2023.10.06 val PER: 0.1130
2026-01-11 17:46:12,993: t15.2023.10.08 val PER: 0.2950
2026-01-11 17:46:12,993: t15.2023.10.13 val PER: 0.2475
2026-01-11 17:46:12,993: t15.2023.10.15 val PER: 0.1852
2026-01-11 17:46:12,993: t15.2023.10.20 val PER: 0.2215
2026-01-11 17:46:12,994: t15.2023.10.22 val PER: 0.1459
2026-01-11 17:46:12,994: t15.2023.11.03 val PER: 0.1940
2026-01-11 17:46:12,994: t15.2023.11.04 val PER: 0.0239
2026-01-11 17:46:12,994: t15.2023.11.17 val PER: 0.0420
2026-01-11 17:46:12,994: t15.2023.11.19 val PER: 0.0399
2026-01-11 17:46:12,994: t15.2023.11.26 val PER: 0.1283
2026-01-11 17:46:12,995: t15.2023.12.03 val PER: 0.1113
2026-01-11 17:46:12,995: t15.2023.12.08 val PER: 0.1105
2026-01-11 17:46:12,995: t15.2023.12.10 val PER: 0.1170
2026-01-11 17:46:12,995: t15.2023.12.17 val PER: 0.1455
2026-01-11 17:46:12,995: t15.2023.12.29 val PER: 0.1393
2026-01-11 17:46:12,995: t15.2024.02.25 val PER: 0.1629
2026-01-11 17:46:12,995: t15.2024.03.08 val PER: 0.2333
2026-01-11 17:46:12,995: t15.2024.03.15 val PER: 0.2208
2026-01-11 17:46:12,995: t15.2024.03.17 val PER: 0.1681
2026-01-11 17:46:12,995: t15.2024.05.10 val PER: 0.1872
2026-01-11 17:46:12,995: t15.2024.06.14 val PER: 0.1719
2026-01-11 17:46:12,996: t15.2024.07.19 val PER: 0.2505
2026-01-11 17:46:12,996: t15.2024.07.21 val PER: 0.1234
2026-01-11 17:46:12,996: t15.2024.07.28 val PER: 0.1632
2026-01-11 17:46:12,996: t15.2025.01.10 val PER: 0.3196
2026-01-11 17:46:12,996: t15.2025.01.12 val PER: 0.1755
2026-01-11 17:46:12,996: t15.2025.03.14 val PER: 0.3491
2026-01-11 17:46:12,996: t15.2025.03.16 val PER: 0.2055
2026-01-11 17:46:12,996: t15.2025.03.30 val PER: 0.2851
2026-01-11 17:46:12,996: t15.2025.04.13 val PER: 0.2382
2026-01-11 17:46:13,151: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_10000
2026-01-11 17:46:34,179: Train batch 10200: loss: 10.82 grad norm: 59.93 time: 0.058
2026-01-11 17:46:55,304: Train batch 10400: loss: 11.91 grad norm: 61.07 time: 0.084
2026-01-11 17:47:05,761: Running test after training batch: 10500
2026-01-11 17:47:06,008: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:47:12,328: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point as well
2026-01-11 17:47:12,390: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 17:47:24,786: Val batch 10500: PER (avg): 0.1638 CTC Loss (avg): 28.1812 WER(5gram): 13.75% (n=256) time: 19.024
2026-01-11 17:47:24,787: WER lens: avg_true_words=5.99 avg_pred_words=6.00 max_pred_words=12
2026-01-11 17:47:24,787: t15.2023.08.13 val PER: 0.1268
2026-01-11 17:47:24,787: t15.2023.08.18 val PER: 0.1324
2026-01-11 17:47:24,787: t15.2023.08.20 val PER: 0.1136
2026-01-11 17:47:24,787: t15.2023.08.25 val PER: 0.1114
2026-01-11 17:47:24,787: t15.2023.08.27 val PER: 0.1881
2026-01-11 17:47:24,788: t15.2023.09.01 val PER: 0.0974
2026-01-11 17:47:24,788: t15.2023.09.03 val PER: 0.1841
2026-01-11 17:47:24,788: t15.2023.09.24 val PER: 0.1335
2026-01-11 17:47:24,788: t15.2023.09.29 val PER: 0.1493
2026-01-11 17:47:24,788: t15.2023.10.01 val PER: 0.2054
2026-01-11 17:47:24,788: t15.2023.10.06 val PER: 0.1023
2026-01-11 17:47:24,788: t15.2023.10.08 val PER: 0.2923
2026-01-11 17:47:24,788: t15.2023.10.13 val PER: 0.2149
2026-01-11 17:47:24,789: t15.2023.10.15 val PER: 0.1826
2026-01-11 17:47:24,789: t15.2023.10.20 val PER: 0.2215
2026-01-11 17:47:24,789: t15.2023.10.22 val PER: 0.1303
2026-01-11 17:47:24,789: t15.2023.11.03 val PER: 0.1859
2026-01-11 17:47:24,789: t15.2023.11.04 val PER: 0.0307
2026-01-11 17:47:24,789: t15.2023.11.17 val PER: 0.0467
2026-01-11 17:47:24,789: t15.2023.11.19 val PER: 0.0499
2026-01-11 17:47:24,789: t15.2023.11.26 val PER: 0.1116
2026-01-11 17:47:24,789: t15.2023.12.03 val PER: 0.1197
2026-01-11 17:47:24,789: t15.2023.12.08 val PER: 0.1112
2026-01-11 17:47:24,789: t15.2023.12.10 val PER: 0.0907
2026-01-11 17:47:24,789: t15.2023.12.17 val PER: 0.1279
2026-01-11 17:47:24,790: t15.2023.12.29 val PER: 0.1386
2026-01-11 17:47:24,790: t15.2024.02.25 val PER: 0.1320
2026-01-11 17:47:24,790: t15.2024.03.08 val PER: 0.2432
2026-01-11 17:47:24,790: t15.2024.03.15 val PER: 0.2195
2026-01-11 17:47:24,790: t15.2024.03.17 val PER: 0.1583
2026-01-11 17:47:24,790: t15.2024.05.10 val PER: 0.1738
2026-01-11 17:47:24,790: t15.2024.06.14 val PER: 0.1735
2026-01-11 17:47:24,790: t15.2024.07.19 val PER: 0.2492
2026-01-11 17:47:24,790: t15.2024.07.21 val PER: 0.1062
2026-01-11 17:47:24,790: t15.2024.07.28 val PER: 0.1478
2026-01-11 17:47:24,791: t15.2025.01.10 val PER: 0.3140
2026-01-11 17:47:24,791: t15.2025.01.12 val PER: 0.1663
2026-01-11 17:47:24,791: t15.2025.03.14 val PER: 0.3314
2026-01-11 17:47:24,791: t15.2025.03.16 val PER: 0.1741
2026-01-11 17:47:24,791: t15.2025.03.30 val PER: 0.2828
2026-01-11 17:47:24,791: t15.2025.04.13 val PER: 0.2454
2026-01-11 17:47:24,791: New best val WER(5gram) 14.60% --> 13.75%
2026-01-11 17:47:24,957: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_10500
2026-01-11 17:47:35,884: Train batch 10600: loss: 12.67 grad norm: 72.58 time: 0.084
2026-01-11 17:47:56,835: Train batch 10800: loss: 21.99 grad norm: 88.40 time: 0.076
2026-01-11 17:48:17,820: Train batch 11000: loss: 18.64 grad norm: 79.65 time: 0.068
2026-01-11 17:48:17,821: Running test after training batch: 11000
2026-01-11 17:48:18,018: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:48:24,802: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point as well
2026-01-11 17:48:24,858: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 17:48:37,385: Val batch 11000: PER (avg): 0.1582 CTC Loss (avg): 27.4813 WER(5gram): 13.62% (n=256) time: 19.564
2026-01-11 17:48:37,386: WER lens: avg_true_words=5.99 avg_pred_words=6.02 max_pred_words=12
2026-01-11 17:48:37,386: t15.2023.08.13 val PER: 0.1227
2026-01-11 17:48:37,386: t15.2023.08.18 val PER: 0.1350
2026-01-11 17:48:37,387: t15.2023.08.20 val PER: 0.1048
2026-01-11 17:48:37,387: t15.2023.08.25 val PER: 0.0934
2026-01-11 17:48:37,387: t15.2023.08.27 val PER: 0.1945
2026-01-11 17:48:37,387: t15.2023.09.01 val PER: 0.0869
2026-01-11 17:48:37,387: t15.2023.09.03 val PER: 0.1627
2026-01-11 17:48:37,387: t15.2023.09.24 val PER: 0.1274
2026-01-11 17:48:37,387: t15.2023.09.29 val PER: 0.1468
2026-01-11 17:48:37,387: t15.2023.10.01 val PER: 0.1948
2026-01-11 17:48:37,387: t15.2023.10.06 val PER: 0.0904
2026-01-11 17:48:37,388: t15.2023.10.08 val PER: 0.2639
2026-01-11 17:48:37,388: t15.2023.10.13 val PER: 0.2258
2026-01-11 17:48:37,388: t15.2023.10.15 val PER: 0.1681
2026-01-11 17:48:37,388: t15.2023.10.20 val PER: 0.1980
2026-01-11 17:48:37,388: t15.2023.10.22 val PER: 0.1370
2026-01-11 17:48:37,388: t15.2023.11.03 val PER: 0.1927
2026-01-11 17:48:37,388: t15.2023.11.04 val PER: 0.0307
2026-01-11 17:48:37,388: t15.2023.11.17 val PER: 0.0435
2026-01-11 17:48:37,388: t15.2023.11.19 val PER: 0.0299
2026-01-11 17:48:37,389: t15.2023.11.26 val PER: 0.1123
2026-01-11 17:48:37,389: t15.2023.12.03 val PER: 0.1092
2026-01-11 17:48:37,389: t15.2023.12.08 val PER: 0.1065
2026-01-11 17:48:37,389: t15.2023.12.10 val PER: 0.0894
2026-01-11 17:48:37,389: t15.2023.12.17 val PER: 0.1112
2026-01-11 17:48:37,389: t15.2023.12.29 val PER: 0.1325
2026-01-11 17:48:37,389: t15.2024.02.25 val PER: 0.1292
2026-01-11 17:48:37,389: t15.2024.03.08 val PER: 0.2361
2026-01-11 17:48:37,389: t15.2024.03.15 val PER: 0.2101
2026-01-11 17:48:37,390: t15.2024.03.17 val PER: 0.1541
2026-01-11 17:48:37,390: t15.2024.05.10 val PER: 0.1620
2026-01-11 17:48:37,390: t15.2024.06.14 val PER: 0.1672
2026-01-11 17:48:37,390: t15.2024.07.19 val PER: 0.2360
2026-01-11 17:48:37,390: t15.2024.07.21 val PER: 0.1103
2026-01-11 17:48:37,390: t15.2024.07.28 val PER: 0.1471
2026-01-11 17:48:37,390: t15.2025.01.10 val PER: 0.2948
2026-01-11 17:48:37,390: t15.2025.01.12 val PER: 0.1563
2026-01-11 17:48:37,390: t15.2025.03.14 val PER: 0.3343
2026-01-11 17:48:37,391: t15.2025.03.16 val PER: 0.1976
2026-01-11 17:48:37,391: t15.2025.03.30 val PER: 0.2885
2026-01-11 17:48:37,391: t15.2025.04.13 val PER: 0.2254
2026-01-11 17:48:37,391: New best val WER(5gram) 13.75% --> 13.62%
2026-01-11 17:48:37,549: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_11000
2026-01-11 17:48:58,340: Train batch 11200: loss: 16.83 grad norm: 80.88 time: 0.083
2026-01-11 17:49:19,383: Train batch 11400: loss: 14.59 grad norm: 71.42 time: 0.067
2026-01-11 17:49:30,459: Running test after training batch: 11500
2026-01-11 17:49:30,580: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:49:37,668: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 17:49:37,724: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 17:49:49,837: Val batch 11500: PER (avg): 0.1568 CTC Loss (avg): 27.5007 WER(5gram): 15.06% (n=256) time: 19.377
2026-01-11 17:49:49,838: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=11
2026-01-11 17:49:49,839: t15.2023.08.13 val PER: 0.1185
2026-01-11 17:49:49,839: t15.2023.08.18 val PER: 0.1324
2026-01-11 17:49:49,839: t15.2023.08.20 val PER: 0.1080
2026-01-11 17:49:49,839: t15.2023.08.25 val PER: 0.0994
2026-01-11 17:49:49,839: t15.2023.08.27 val PER: 0.1913
2026-01-11 17:49:49,839: t15.2023.09.01 val PER: 0.0869
2026-01-11 17:49:49,839: t15.2023.09.03 val PER: 0.1663
2026-01-11 17:49:49,840: t15.2023.09.24 val PER: 0.1177
2026-01-11 17:49:49,840: t15.2023.09.29 val PER: 0.1461
2026-01-11 17:49:49,840: t15.2023.10.01 val PER: 0.1869
2026-01-11 17:49:49,840: t15.2023.10.06 val PER: 0.0893
2026-01-11 17:49:49,840: t15.2023.10.08 val PER: 0.2747
2026-01-11 17:49:49,840: t15.2023.10.13 val PER: 0.2211
2026-01-11 17:49:49,840: t15.2023.10.15 val PER: 0.1569
2026-01-11 17:49:49,840: t15.2023.10.20 val PER: 0.2181
2026-01-11 17:49:49,840: t15.2023.10.22 val PER: 0.1314
2026-01-11 17:49:49,841: t15.2023.11.03 val PER: 0.1859
2026-01-11 17:49:49,841: t15.2023.11.04 val PER: 0.0307
2026-01-11 17:49:49,841: t15.2023.11.17 val PER: 0.0404
2026-01-11 17:49:49,841: t15.2023.11.19 val PER: 0.0220
2026-01-11 17:49:49,841: t15.2023.11.26 val PER: 0.1036
2026-01-11 17:49:49,841: t15.2023.12.03 val PER: 0.0987
2026-01-11 17:49:49,841: t15.2023.12.08 val PER: 0.1052
2026-01-11 17:49:49,841: t15.2023.12.10 val PER: 0.0894
2026-01-11 17:49:49,841: t15.2023.12.17 val PER: 0.1279
2026-01-11 17:49:49,842: t15.2023.12.29 val PER: 0.1283
2026-01-11 17:49:49,842: t15.2024.02.25 val PER: 0.1292
2026-01-11 17:49:49,842: t15.2024.03.08 val PER: 0.2333
2026-01-11 17:49:49,842: t15.2024.03.15 val PER: 0.2170
2026-01-11 17:49:49,842: t15.2024.03.17 val PER: 0.1555
2026-01-11 17:49:49,842: t15.2024.05.10 val PER: 0.1590
2026-01-11 17:49:49,842: t15.2024.06.14 val PER: 0.1640
2026-01-11 17:49:49,842: t15.2024.07.19 val PER: 0.2393
2026-01-11 17:49:49,843: t15.2024.07.21 val PER: 0.1083
2026-01-11 17:49:49,843: t15.2024.07.28 val PER: 0.1493
2026-01-11 17:49:49,843: t15.2025.01.10 val PER: 0.3072
2026-01-11 17:49:49,843: t15.2025.01.12 val PER: 0.1694
2026-01-11 17:49:49,843: t15.2025.03.14 val PER: 0.3121
2026-01-11 17:49:49,843: t15.2025.03.16 val PER: 0.1859
2026-01-11 17:49:49,843: t15.2025.03.30 val PER: 0.2759
2026-01-11 17:49:49,843: t15.2025.04.13 val PER: 0.2268
2026-01-11 17:49:50,026: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_11500
2026-01-11 17:50:00,252: Train batch 11600: loss: 14.56 grad norm: 68.99 time: 0.077
2026-01-11 17:50:20,605: Train batch 11800: loss: 8.99 grad norm: 59.46 time: 0.052
2026-01-11 17:50:41,165: Train batch 12000: loss: 18.68 grad norm: 78.35 time: 0.082
2026-01-11 17:50:41,166: Running test after training batch: 12000
2026-01-11 17:50:41,280: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:50:47,628: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 17:50:47,698: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost now
2026-01-11 17:51:02,211: Val batch 12000: PER (avg): 0.1518 CTC Loss (avg): 26.8584 WER(5gram): 12.65% (n=256) time: 21.045
2026-01-11 17:51:02,213: WER lens: avg_true_words=5.99 avg_pred_words=6.06 max_pred_words=12
2026-01-11 17:51:02,213: t15.2023.08.13 val PER: 0.1279
2026-01-11 17:51:02,213: t15.2023.08.18 val PER: 0.1132
2026-01-11 17:51:02,213: t15.2023.08.20 val PER: 0.0985
2026-01-11 17:51:02,214: t15.2023.08.25 val PER: 0.0979
2026-01-11 17:51:02,214: t15.2023.08.27 val PER: 0.1785
2026-01-11 17:51:02,214: t15.2023.09.01 val PER: 0.0852
2026-01-11 17:51:02,214: t15.2023.09.03 val PER: 0.1532
2026-01-11 17:51:02,214: t15.2023.09.24 val PER: 0.1153
2026-01-11 17:51:02,214: t15.2023.09.29 val PER: 0.1532
2026-01-11 17:51:02,214: t15.2023.10.01 val PER: 0.1790
2026-01-11 17:51:02,214: t15.2023.10.06 val PER: 0.0807
2026-01-11 17:51:02,214: t15.2023.10.08 val PER: 0.2612
2026-01-11 17:51:02,214: t15.2023.10.13 val PER: 0.2296
2026-01-11 17:51:02,214: t15.2023.10.15 val PER: 0.1589
2026-01-11 17:51:02,215: t15.2023.10.20 val PER: 0.2181
2026-01-11 17:51:02,215: t15.2023.10.22 val PER: 0.1292
2026-01-11 17:51:02,215: t15.2023.11.03 val PER: 0.1832
2026-01-11 17:51:02,215: t15.2023.11.04 val PER: 0.0307
2026-01-11 17:51:02,215: t15.2023.11.17 val PER: 0.0327
2026-01-11 17:51:02,215: t15.2023.11.19 val PER: 0.0299
2026-01-11 17:51:02,215: t15.2023.11.26 val PER: 0.1000
2026-01-11 17:51:02,216: t15.2023.12.03 val PER: 0.0914
2026-01-11 17:51:02,216: t15.2023.12.08 val PER: 0.0999
2026-01-11 17:51:02,216: t15.2023.12.10 val PER: 0.0762
2026-01-11 17:51:02,217: t15.2023.12.17 val PER: 0.1206
2026-01-11 17:51:02,217: t15.2023.12.29 val PER: 0.1229
2026-01-11 17:51:02,217: t15.2024.02.25 val PER: 0.1362
2026-01-11 17:51:02,217: t15.2024.03.08 val PER: 0.2191
2026-01-11 17:51:02,217: t15.2024.03.15 val PER: 0.2058
2026-01-11 17:51:02,217: t15.2024.03.17 val PER: 0.1464
2026-01-11 17:51:02,217: t15.2024.05.10 val PER: 0.1575
2026-01-11 17:51:02,218: t15.2024.06.14 val PER: 0.1625
2026-01-11 17:51:02,218: t15.2024.07.19 val PER: 0.2307
2026-01-11 17:51:02,218: t15.2024.07.21 val PER: 0.1090
2026-01-11 17:51:02,218: t15.2024.07.28 val PER: 0.1456
2026-01-11 17:51:02,218: t15.2025.01.10 val PER: 0.2851
2026-01-11 17:51:02,218: t15.2025.01.12 val PER: 0.1540
2026-01-11 17:51:02,219: t15.2025.03.14 val PER: 0.3195
2026-01-11 17:51:02,219: t15.2025.03.16 val PER: 0.1741
2026-01-11 17:51:02,219: t15.2025.03.30 val PER: 0.2805
2026-01-11 17:51:02,219: t15.2025.04.13 val PER: 0.2154
2026-01-11 17:51:02,220: New best val WER(5gram) 13.62% --> 12.65%
2026-01-11 17:51:02,413: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_12000
2026-01-11 17:51:23,171: Train batch 12200: loss: 6.32 grad norm: 50.41 time: 0.076
2026-01-11 17:51:43,666: Train batch 12400: loss: 6.90 grad norm: 47.31 time: 0.048
2026-01-11 17:51:54,235: Running test after training batch: 12500
2026-01-11 17:51:54,693: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:52:01,327: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point as well
2026-01-11 17:52:01,389: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 17:52:15,657: Val batch 12500: PER (avg): 0.1499 CTC Loss (avg): 26.8806 WER(5gram): 13.30% (n=256) time: 21.421
2026-01-11 17:52:15,658: WER lens: avg_true_words=5.99 avg_pred_words=6.04 max_pred_words=12
2026-01-11 17:52:15,658: t15.2023.08.13 val PER: 0.1227
2026-01-11 17:52:15,658: t15.2023.08.18 val PER: 0.1123
2026-01-11 17:52:15,659: t15.2023.08.20 val PER: 0.1001
2026-01-11 17:52:15,659: t15.2023.08.25 val PER: 0.0964
2026-01-11 17:52:15,659: t15.2023.08.27 val PER: 0.1833
2026-01-11 17:52:15,659: t15.2023.09.01 val PER: 0.0722
2026-01-11 17:52:15,660: t15.2023.09.03 val PER: 0.1544
2026-01-11 17:52:15,660: t15.2023.09.24 val PER: 0.1129
2026-01-11 17:52:15,661: t15.2023.09.29 val PER: 0.1436
2026-01-11 17:52:15,661: t15.2023.10.01 val PER: 0.1810
2026-01-11 17:52:15,661: t15.2023.10.06 val PER: 0.0829
2026-01-11 17:52:15,661: t15.2023.10.08 val PER: 0.2517
2026-01-11 17:52:15,661: t15.2023.10.13 val PER: 0.2242
2026-01-11 17:52:15,661: t15.2023.10.15 val PER: 0.1477
2026-01-11 17:52:15,661: t15.2023.10.20 val PER: 0.2248
2026-01-11 17:52:15,661: t15.2023.10.22 val PER: 0.1292
2026-01-11 17:52:15,661: t15.2023.11.03 val PER: 0.1777
2026-01-11 17:52:15,661: t15.2023.11.04 val PER: 0.0239
2026-01-11 17:52:15,662: t15.2023.11.17 val PER: 0.0404
2026-01-11 17:52:15,662: t15.2023.11.19 val PER: 0.0319
2026-01-11 17:52:15,662: t15.2023.11.26 val PER: 0.1000
2026-01-11 17:52:15,662: t15.2023.12.03 val PER: 0.1061
2026-01-11 17:52:15,662: t15.2023.12.08 val PER: 0.0879
2026-01-11 17:52:15,662: t15.2023.12.10 val PER: 0.0775
2026-01-11 17:52:15,662: t15.2023.12.17 val PER: 0.1154
2026-01-11 17:52:15,663: t15.2023.12.29 val PER: 0.1277
2026-01-11 17:52:15,663: t15.2024.02.25 val PER: 0.1264
2026-01-11 17:52:15,663: t15.2024.03.08 val PER: 0.2262
2026-01-11 17:52:15,663: t15.2024.03.15 val PER: 0.2033
2026-01-11 17:52:15,664: t15.2024.03.17 val PER: 0.1471
2026-01-11 17:52:15,664: t15.2024.05.10 val PER: 0.1694
2026-01-11 17:52:15,664: t15.2024.06.14 val PER: 0.1672
2026-01-11 17:52:15,664: t15.2024.07.19 val PER: 0.2294
2026-01-11 17:52:15,664: t15.2024.07.21 val PER: 0.0966
2026-01-11 17:52:15,664: t15.2024.07.28 val PER: 0.1360
2026-01-11 17:52:15,664: t15.2025.01.10 val PER: 0.2920
2026-01-11 17:52:15,664: t15.2025.01.12 val PER: 0.1470
2026-01-11 17:52:15,664: t15.2025.03.14 val PER: 0.3299
2026-01-11 17:52:15,664: t15.2025.03.16 val PER: 0.1715
2026-01-11 17:52:15,664: t15.2025.03.30 val PER: 0.2782
2026-01-11 17:52:15,665: t15.2025.04.13 val PER: 0.2340
2026-01-11 17:52:15,832: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_12500
2026-01-11 17:52:26,141: Train batch 12600: loss: 11.12 grad norm: 57.56 time: 0.067
2026-01-11 17:52:47,158: Train batch 12800: loss: 6.12 grad norm: 45.16 time: 0.066
2026-01-11 17:53:08,630: Train batch 13000: loss: 7.14 grad norm: 55.45 time: 0.078
2026-01-11 17:53:08,631: Running test after training batch: 13000
2026-01-11 17:53:08,777: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:53:15,132: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 17:53:15,200: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 17:53:27,750: Val batch 13000: PER (avg): 0.1446 CTC Loss (avg): 26.5053 WER(5gram): 12.65% (n=256) time: 19.119
2026-01-11 17:53:27,751: WER lens: avg_true_words=5.99 avg_pred_words=6.06 max_pred_words=12
2026-01-11 17:53:27,751: t15.2023.08.13 val PER: 0.1164
2026-01-11 17:53:27,751: t15.2023.08.18 val PER: 0.1140
2026-01-11 17:53:27,751: t15.2023.08.20 val PER: 0.0985
2026-01-11 17:53:27,751: t15.2023.08.25 val PER: 0.0904
2026-01-11 17:53:27,752: t15.2023.08.27 val PER: 0.1913
2026-01-11 17:53:27,752: t15.2023.09.01 val PER: 0.0763
2026-01-11 17:53:27,752: t15.2023.09.03 val PER: 0.1615
2026-01-11 17:53:27,752: t15.2023.09.24 val PER: 0.1117
2026-01-11 17:53:27,752: t15.2023.09.29 val PER: 0.1283
2026-01-11 17:53:27,752: t15.2023.10.01 val PER: 0.1777
2026-01-11 17:53:27,752: t15.2023.10.06 val PER: 0.0829
2026-01-11 17:53:27,752: t15.2023.10.08 val PER: 0.2490
2026-01-11 17:53:27,752: t15.2023.10.13 val PER: 0.2110
2026-01-11 17:53:27,753: t15.2023.10.15 val PER: 0.1589
2026-01-11 17:53:27,753: t15.2023.10.20 val PER: 0.2148
2026-01-11 17:53:27,753: t15.2023.10.22 val PER: 0.1281
2026-01-11 17:53:27,753: t15.2023.11.03 val PER: 0.1784
2026-01-11 17:53:27,753: t15.2023.11.04 val PER: 0.0307
2026-01-11 17:53:27,753: t15.2023.11.17 val PER: 0.0311
2026-01-11 17:53:27,753: t15.2023.11.19 val PER: 0.0379
2026-01-11 17:53:27,753: t15.2023.11.26 val PER: 0.0862
2026-01-11 17:53:27,754: t15.2023.12.03 val PER: 0.0893
2026-01-11 17:53:27,754: t15.2023.12.08 val PER: 0.0885
2026-01-11 17:53:27,754: t15.2023.12.10 val PER: 0.0710
2026-01-11 17:53:27,754: t15.2023.12.17 val PER: 0.1123
2026-01-11 17:53:27,754: t15.2023.12.29 val PER: 0.1174
2026-01-11 17:53:27,754: t15.2024.02.25 val PER: 0.1264
2026-01-11 17:53:27,754: t15.2024.03.08 val PER: 0.2447
2026-01-11 17:53:27,754: t15.2024.03.15 val PER: 0.2008
2026-01-11 17:53:27,755: t15.2024.03.17 val PER: 0.1318
2026-01-11 17:53:27,755: t15.2024.05.10 val PER: 0.1426
2026-01-11 17:53:27,755: t15.2024.06.14 val PER: 0.1483
2026-01-11 17:53:27,755: t15.2024.07.19 val PER: 0.2189
2026-01-11 17:53:27,755: t15.2024.07.21 val PER: 0.0890
2026-01-11 17:53:27,755: t15.2024.07.28 val PER: 0.1368
2026-01-11 17:53:27,755: t15.2025.01.10 val PER: 0.2769
2026-01-11 17:53:27,755: t15.2025.01.12 val PER: 0.1370
2026-01-11 17:53:27,755: t15.2025.03.14 val PER: 0.3047
2026-01-11 17:53:27,756: t15.2025.03.16 val PER: 0.1688
2026-01-11 17:53:27,756: t15.2025.03.30 val PER: 0.2632
2026-01-11 17:53:27,756: t15.2025.04.13 val PER: 0.2240
2026-01-11 17:53:27,917: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_13000
2026-01-11 17:53:48,904: Train batch 13200: loss: 14.29 grad norm: 80.22 time: 0.065
2026-01-11 17:54:09,954: Train batch 13400: loss: 9.75 grad norm: 68.04 time: 0.074
2026-01-11 17:54:20,628: Running test after training batch: 13500
2026-01-11 17:54:20,945: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:54:27,501: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 17:54:27,568: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost now
2026-01-11 17:54:40,033: Val batch 13500: PER (avg): 0.1406 CTC Loss (avg): 25.9444 WER(5gram): 10.37% (n=256) time: 19.404
2026-01-11 17:54:40,034: WER lens: avg_true_words=5.99 avg_pred_words=6.02 max_pred_words=12
2026-01-11 17:54:40,034: t15.2023.08.13 val PER: 0.1154
2026-01-11 17:54:40,034: t15.2023.08.18 val PER: 0.0956
2026-01-11 17:54:40,034: t15.2023.08.20 val PER: 0.0961
2026-01-11 17:54:40,034: t15.2023.08.25 val PER: 0.1039
2026-01-11 17:54:40,035: t15.2023.08.27 val PER: 0.1768
2026-01-11 17:54:40,035: t15.2023.09.01 val PER: 0.0698
2026-01-11 17:54:40,035: t15.2023.09.03 val PER: 0.1556
2026-01-11 17:54:40,035: t15.2023.09.24 val PER: 0.1165
2026-01-11 17:54:40,035: t15.2023.09.29 val PER: 0.1340
2026-01-11 17:54:40,035: t15.2023.10.01 val PER: 0.1816
2026-01-11 17:54:40,036: t15.2023.10.06 val PER: 0.0840
2026-01-11 17:54:40,036: t15.2023.10.08 val PER: 0.2517
2026-01-11 17:54:40,036: t15.2023.10.13 val PER: 0.2025
2026-01-11 17:54:40,036: t15.2023.10.15 val PER: 0.1496
2026-01-11 17:54:40,037: t15.2023.10.20 val PER: 0.1946
2026-01-11 17:54:40,037: t15.2023.10.22 val PER: 0.1125
2026-01-11 17:54:40,037: t15.2023.11.03 val PER: 0.1716
2026-01-11 17:54:40,037: t15.2023.11.04 val PER: 0.0375
2026-01-11 17:54:40,037: t15.2023.11.17 val PER: 0.0342
2026-01-11 17:54:40,037: t15.2023.11.19 val PER: 0.0220
2026-01-11 17:54:40,037: t15.2023.11.26 val PER: 0.0848
2026-01-11 17:54:40,037: t15.2023.12.03 val PER: 0.0882
2026-01-11 17:54:40,038: t15.2023.12.08 val PER: 0.0839
2026-01-11 17:54:40,038: t15.2023.12.10 val PER: 0.0696
2026-01-11 17:54:40,038: t15.2023.12.17 val PER: 0.1060
2026-01-11 17:54:40,038: t15.2023.12.29 val PER: 0.1174
2026-01-11 17:54:40,038: t15.2024.02.25 val PER: 0.1222
2026-01-11 17:54:40,038: t15.2024.03.08 val PER: 0.2191
2026-01-11 17:54:40,038: t15.2024.03.15 val PER: 0.1914
2026-01-11 17:54:40,038: t15.2024.03.17 val PER: 0.1276
2026-01-11 17:54:40,038: t15.2024.05.10 val PER: 0.1293
2026-01-11 17:54:40,038: t15.2024.06.14 val PER: 0.1562
2026-01-11 17:54:40,039: t15.2024.07.19 val PER: 0.2136
2026-01-11 17:54:40,039: t15.2024.07.21 val PER: 0.0876
2026-01-11 17:54:40,040: t15.2024.07.28 val PER: 0.1353
2026-01-11 17:54:40,040: t15.2025.01.10 val PER: 0.2617
2026-01-11 17:54:40,040: t15.2025.01.12 val PER: 0.1386
2026-01-11 17:54:40,040: t15.2025.03.14 val PER: 0.3166
2026-01-11 17:54:40,040: t15.2025.03.16 val PER: 0.1623
2026-01-11 17:54:40,041: t15.2025.03.30 val PER: 0.2644
2026-01-11 17:54:40,041: t15.2025.04.13 val PER: 0.2083
2026-01-11 17:54:40,042: New best val WER(5gram) 12.65% --> 10.37%
2026-01-11 17:54:40,214: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_13500
2026-01-11 17:54:50,455: Train batch 13600: loss: 15.46 grad norm: 83.58 time: 0.078
2026-01-11 17:55:11,071: Train batch 13800: loss: 9.59 grad norm: 75.23 time: 0.067
2026-01-11 17:55:32,160: Train batch 14000: loss: 15.71 grad norm: 82.76 time: 0.061
2026-01-11 17:55:32,161: Running test after training batch: 14000
2026-01-11 17:55:32,354: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:55:39,613: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 17:55:39,674: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost now
2026-01-11 17:55:52,444: Val batch 14000: PER (avg): 0.1418 CTC Loss (avg): 25.3291 WER(5gram): 12.32% (n=256) time: 20.283
2026-01-11 17:55:52,445: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-11 17:55:52,445: t15.2023.08.13 val PER: 0.1081
2026-01-11 17:55:52,445: t15.2023.08.18 val PER: 0.0972
2026-01-11 17:55:52,445: t15.2023.08.20 val PER: 0.1001
2026-01-11 17:55:52,446: t15.2023.08.25 val PER: 0.0919
2026-01-11 17:55:52,446: t15.2023.08.27 val PER: 0.1656
2026-01-11 17:55:52,446: t15.2023.09.01 val PER: 0.0698
2026-01-11 17:55:52,446: t15.2023.09.03 val PER: 0.1591
2026-01-11 17:55:52,446: t15.2023.09.24 val PER: 0.1068
2026-01-11 17:55:52,446: t15.2023.09.29 val PER: 0.1308
2026-01-11 17:55:52,446: t15.2023.10.01 val PER: 0.1836
2026-01-11 17:55:52,446: t15.2023.10.06 val PER: 0.0861
2026-01-11 17:55:52,447: t15.2023.10.08 val PER: 0.2436
2026-01-11 17:55:52,447: t15.2023.10.13 val PER: 0.2071
2026-01-11 17:55:52,447: t15.2023.10.15 val PER: 0.1569
2026-01-11 17:55:52,447: t15.2023.10.20 val PER: 0.2315
2026-01-11 17:55:52,447: t15.2023.10.22 val PER: 0.1169
2026-01-11 17:55:52,447: t15.2023.11.03 val PER: 0.1784
2026-01-11 17:55:52,447: t15.2023.11.04 val PER: 0.0307
2026-01-11 17:55:52,447: t15.2023.11.17 val PER: 0.0311
2026-01-11 17:55:52,448: t15.2023.11.19 val PER: 0.0339
2026-01-11 17:55:52,448: t15.2023.11.26 val PER: 0.0855
2026-01-11 17:55:52,448: t15.2023.12.03 val PER: 0.0914
2026-01-11 17:55:52,448: t15.2023.12.08 val PER: 0.0859
2026-01-11 17:55:52,448: t15.2023.12.10 val PER: 0.0683
2026-01-11 17:55:52,448: t15.2023.12.17 val PER: 0.1008
2026-01-11 17:55:52,448: t15.2023.12.29 val PER: 0.1098
2026-01-11 17:55:52,448: t15.2024.02.25 val PER: 0.1236
2026-01-11 17:55:52,449: t15.2024.03.08 val PER: 0.2262
2026-01-11 17:55:52,449: t15.2024.03.15 val PER: 0.2001
2026-01-11 17:55:52,449: t15.2024.03.17 val PER: 0.1248
2026-01-11 17:55:52,449: t15.2024.05.10 val PER: 0.1530
2026-01-11 17:55:52,449: t15.2024.06.14 val PER: 0.1372
2026-01-11 17:55:52,449: t15.2024.07.19 val PER: 0.2149
2026-01-11 17:55:52,449: t15.2024.07.21 val PER: 0.0897
2026-01-11 17:55:52,449: t15.2024.07.28 val PER: 0.1309
2026-01-11 17:55:52,449: t15.2025.01.10 val PER: 0.2617
2026-01-11 17:55:52,450: t15.2025.01.12 val PER: 0.1332
2026-01-11 17:55:52,450: t15.2025.03.14 val PER: 0.3358
2026-01-11 17:55:52,450: t15.2025.03.16 val PER: 0.1584
2026-01-11 17:55:52,450: t15.2025.03.30 val PER: 0.2747
2026-01-11 17:55:52,450: t15.2025.04.13 val PER: 0.2254
2026-01-11 17:55:52,605: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_14000
2026-01-11 17:56:13,375: Train batch 14200: loss: 10.38 grad norm: 63.55 time: 0.066
2026-01-11 17:56:34,079: Train batch 14400: loss: 8.03 grad norm: 52.57 time: 0.079
2026-01-11 17:56:44,925: Running test after training batch: 14500
2026-01-11 17:56:45,044: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:56:52,069: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 17:56:52,137: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost did
2026-01-11 17:57:05,549: Val batch 14500: PER (avg): 0.1394 CTC Loss (avg): 25.7097 WER(5gram): 12.32% (n=256) time: 20.624
2026-01-11 17:57:05,551: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-11 17:57:05,551: t15.2023.08.13 val PER: 0.1133
2026-01-11 17:57:05,551: t15.2023.08.18 val PER: 0.1048
2026-01-11 17:57:05,552: t15.2023.08.20 val PER: 0.0929
2026-01-11 17:57:05,552: t15.2023.08.25 val PER: 0.0843
2026-01-11 17:57:05,552: t15.2023.08.27 val PER: 0.1785
2026-01-11 17:57:05,552: t15.2023.09.01 val PER: 0.0666
2026-01-11 17:57:05,552: t15.2023.09.03 val PER: 0.1461
2026-01-11 17:57:05,552: t15.2023.09.24 val PER: 0.1141
2026-01-11 17:57:05,552: t15.2023.09.29 val PER: 0.1283
2026-01-11 17:57:05,552: t15.2023.10.01 val PER: 0.1744
2026-01-11 17:57:05,552: t15.2023.10.06 val PER: 0.0872
2026-01-11 17:57:05,553: t15.2023.10.08 val PER: 0.2436
2026-01-11 17:57:05,553: t15.2023.10.13 val PER: 0.2095
2026-01-11 17:57:05,553: t15.2023.10.15 val PER: 0.1477
2026-01-11 17:57:05,553: t15.2023.10.20 val PER: 0.2081
2026-01-11 17:57:05,553: t15.2023.10.22 val PER: 0.1247
2026-01-11 17:57:05,553: t15.2023.11.03 val PER: 0.1744
2026-01-11 17:57:05,553: t15.2023.11.04 val PER: 0.0307
2026-01-11 17:57:05,554: t15.2023.11.17 val PER: 0.0373
2026-01-11 17:57:05,554: t15.2023.11.19 val PER: 0.0279
2026-01-11 17:57:05,554: t15.2023.11.26 val PER: 0.0797
2026-01-11 17:57:05,554: t15.2023.12.03 val PER: 0.0830
2026-01-11 17:57:05,554: t15.2023.12.08 val PER: 0.0772
2026-01-11 17:57:05,554: t15.2023.12.10 val PER: 0.0631
2026-01-11 17:57:05,554: t15.2023.12.17 val PER: 0.1102
2026-01-11 17:57:05,555: t15.2023.12.29 val PER: 0.1112
2026-01-11 17:57:05,555: t15.2024.02.25 val PER: 0.1039
2026-01-11 17:57:05,555: t15.2024.03.08 val PER: 0.2319
2026-01-11 17:57:05,555: t15.2024.03.15 val PER: 0.1976
2026-01-11 17:57:05,555: t15.2024.03.17 val PER: 0.1304
2026-01-11 17:57:05,555: t15.2024.05.10 val PER: 0.1486
2026-01-11 17:57:05,555: t15.2024.06.14 val PER: 0.1404
2026-01-11 17:57:05,555: t15.2024.07.19 val PER: 0.2123
2026-01-11 17:57:05,555: t15.2024.07.21 val PER: 0.0952
2026-01-11 17:57:05,555: t15.2024.07.28 val PER: 0.1272
2026-01-11 17:57:05,556: t15.2025.01.10 val PER: 0.2713
2026-01-11 17:57:05,556: t15.2025.01.12 val PER: 0.1416
2026-01-11 17:57:05,556: t15.2025.03.14 val PER: 0.3092
2026-01-11 17:57:05,556: t15.2025.03.16 val PER: 0.1649
2026-01-11 17:57:05,556: t15.2025.03.30 val PER: 0.2414
2026-01-11 17:57:05,556: t15.2025.04.13 val PER: 0.2083
2026-01-11 17:57:05,745: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_14500
2026-01-11 17:57:16,206: Train batch 14600: loss: 13.63 grad norm: 75.11 time: 0.070
2026-01-11 17:57:36,941: Train batch 14800: loss: 6.88 grad norm: 52.90 time: 0.059
2026-01-11 17:57:57,480: Train batch 15000: loss: 9.81 grad norm: 55.17 time: 0.061
2026-01-11 17:57:57,480: Running test after training batch: 15000
2026-01-11 17:57:57,805: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:58:03,922: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 17:58:03,979: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost now
2026-01-11 17:58:18,777: Val batch 15000: PER (avg): 0.1366 CTC Loss (avg): 25.3999 WER(5gram): 11.80% (n=256) time: 21.297
2026-01-11 17:58:18,779: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-11 17:58:18,779: t15.2023.08.13 val PER: 0.1102
2026-01-11 17:58:18,779: t15.2023.08.18 val PER: 0.0922
2026-01-11 17:58:18,780: t15.2023.08.20 val PER: 0.0826
2026-01-11 17:58:18,780: t15.2023.08.25 val PER: 0.0934
2026-01-11 17:58:18,780: t15.2023.08.27 val PER: 0.1656
2026-01-11 17:58:18,780: t15.2023.09.01 val PER: 0.0601
2026-01-11 17:58:18,780: t15.2023.09.03 val PER: 0.1580
2026-01-11 17:58:18,780: t15.2023.09.24 val PER: 0.1177
2026-01-11 17:58:18,780: t15.2023.09.29 val PER: 0.1238
2026-01-11 17:58:18,780: t15.2023.10.01 val PER: 0.1764
2026-01-11 17:58:18,780: t15.2023.10.06 val PER: 0.0786
2026-01-11 17:58:18,781: t15.2023.10.08 val PER: 0.2476
2026-01-11 17:58:18,781: t15.2023.10.13 val PER: 0.1963
2026-01-11 17:58:18,781: t15.2023.10.15 val PER: 0.1477
2026-01-11 17:58:18,781: t15.2023.10.20 val PER: 0.1846
2026-01-11 17:58:18,781: t15.2023.10.22 val PER: 0.1136
2026-01-11 17:58:18,781: t15.2023.11.03 val PER: 0.1716
2026-01-11 17:58:18,781: t15.2023.11.04 val PER: 0.0171
2026-01-11 17:58:18,781: t15.2023.11.17 val PER: 0.0311
2026-01-11 17:58:18,781: t15.2023.11.19 val PER: 0.0200
2026-01-11 17:58:18,781: t15.2023.11.26 val PER: 0.0826
2026-01-11 17:58:18,781: t15.2023.12.03 val PER: 0.0840
2026-01-11 17:58:18,781: t15.2023.12.08 val PER: 0.0706
2026-01-11 17:58:18,781: t15.2023.12.10 val PER: 0.0631
2026-01-11 17:58:18,782: t15.2023.12.17 val PER: 0.1154
2026-01-11 17:58:18,782: t15.2023.12.29 val PER: 0.1126
2026-01-11 17:58:18,782: t15.2024.02.25 val PER: 0.1166
2026-01-11 17:58:18,782: t15.2024.03.08 val PER: 0.2162
2026-01-11 17:58:18,782: t15.2024.03.15 val PER: 0.1982
2026-01-11 17:58:18,782: t15.2024.03.17 val PER: 0.1227
2026-01-11 17:58:18,782: t15.2024.05.10 val PER: 0.1412
2026-01-11 17:58:18,782: t15.2024.06.14 val PER: 0.1341
2026-01-11 17:58:18,782: t15.2024.07.19 val PER: 0.2156
2026-01-11 17:58:18,782: t15.2024.07.21 val PER: 0.0814
2026-01-11 17:58:18,782: t15.2024.07.28 val PER: 0.1213
2026-01-11 17:58:18,782: t15.2025.01.10 val PER: 0.2741
2026-01-11 17:58:18,783: t15.2025.01.12 val PER: 0.1347
2026-01-11 17:58:18,783: t15.2025.03.14 val PER: 0.3328
2026-01-11 17:58:18,783: t15.2025.03.16 val PER: 0.1479
2026-01-11 17:58:18,783: t15.2025.03.30 val PER: 0.2540
2026-01-11 17:58:18,783: t15.2025.04.13 val PER: 0.2197
2026-01-11 17:58:18,951: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_15000
2026-01-11 17:58:40,574: Train batch 15200: loss: 5.36 grad norm: 45.99 time: 0.066
2026-01-11 17:59:01,403: Train batch 15400: loss: 13.48 grad norm: 84.81 time: 0.059
2026-01-11 17:59:12,117: Running test after training batch: 15500
2026-01-11 17:59:12,351: WER debug GT example: You can see the code at this point as well.
2026-01-11 17:59:18,552: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 17:59:18,610: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost now
2026-01-11 17:59:32,383: Val batch 15500: PER (avg): 0.1334 CTC Loss (avg): 25.1727 WER(5gram): 12.39% (n=256) time: 20.266
2026-01-11 17:59:32,384: WER lens: avg_true_words=5.99 avg_pred_words=6.06 max_pred_words=12
2026-01-11 17:59:32,384: t15.2023.08.13 val PER: 0.1019
2026-01-11 17:59:32,384: t15.2023.08.18 val PER: 0.0905
2026-01-11 17:59:32,385: t15.2023.08.20 val PER: 0.0921
2026-01-11 17:59:32,385: t15.2023.08.25 val PER: 0.0753
2026-01-11 17:59:32,385: t15.2023.08.27 val PER: 0.1720
2026-01-11 17:59:32,385: t15.2023.09.01 val PER: 0.0649
2026-01-11 17:59:32,386: t15.2023.09.03 val PER: 0.1496
2026-01-11 17:59:32,386: t15.2023.09.24 val PER: 0.1092
2026-01-11 17:59:32,386: t15.2023.09.29 val PER: 0.1232
2026-01-11 17:59:32,386: t15.2023.10.01 val PER: 0.1717
2026-01-11 17:59:32,386: t15.2023.10.06 val PER: 0.0700
2026-01-11 17:59:32,386: t15.2023.10.08 val PER: 0.2436
2026-01-11 17:59:32,386: t15.2023.10.13 val PER: 0.2087
2026-01-11 17:59:32,386: t15.2023.10.15 val PER: 0.1450
2026-01-11 17:59:32,386: t15.2023.10.20 val PER: 0.2013
2026-01-11 17:59:32,386: t15.2023.10.22 val PER: 0.1281
2026-01-11 17:59:32,386: t15.2023.11.03 val PER: 0.1771
2026-01-11 17:59:32,386: t15.2023.11.04 val PER: 0.0307
2026-01-11 17:59:32,387: t15.2023.11.17 val PER: 0.0249
2026-01-11 17:59:32,387: t15.2023.11.19 val PER: 0.0240
2026-01-11 17:59:32,387: t15.2023.11.26 val PER: 0.0775
2026-01-11 17:59:32,387: t15.2023.12.03 val PER: 0.0714
2026-01-11 17:59:32,387: t15.2023.12.08 val PER: 0.0639
2026-01-11 17:59:32,387: t15.2023.12.10 val PER: 0.0552
2026-01-11 17:59:32,387: t15.2023.12.17 val PER: 0.1040
2026-01-11 17:59:32,387: t15.2023.12.29 val PER: 0.1126
2026-01-11 17:59:32,387: t15.2024.02.25 val PER: 0.1011
2026-01-11 17:59:32,387: t15.2024.03.08 val PER: 0.2148
2026-01-11 17:59:32,387: t15.2024.03.15 val PER: 0.1882
2026-01-11 17:59:32,387: t15.2024.03.17 val PER: 0.1185
2026-01-11 17:59:32,387: t15.2024.05.10 val PER: 0.1545
2026-01-11 17:59:32,388: t15.2024.06.14 val PER: 0.1293
2026-01-11 17:59:32,388: t15.2024.07.19 val PER: 0.2011
2026-01-11 17:59:32,388: t15.2024.07.21 val PER: 0.0848
2026-01-11 17:59:32,388: t15.2024.07.28 val PER: 0.1243
2026-01-11 17:59:32,388: t15.2025.01.10 val PER: 0.2521
2026-01-11 17:59:32,388: t15.2025.01.12 val PER: 0.1232
2026-01-11 17:59:32,388: t15.2025.03.14 val PER: 0.3018
2026-01-11 17:59:32,388: t15.2025.03.16 val PER: 0.1518
2026-01-11 17:59:32,388: t15.2025.03.30 val PER: 0.2506
2026-01-11 17:59:32,388: t15.2025.04.13 val PER: 0.2140
2026-01-11 17:59:32,549: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_15500
2026-01-11 17:59:43,081: Train batch 15600: loss: 11.47 grad norm: 77.10 time: 0.074
2026-01-11 18:00:04,190: Train batch 15800: loss: 14.02 grad norm: 82.87 time: 0.080
2026-01-11 18:00:25,526: Train batch 16000: loss: 8.58 grad norm: 78.70 time: 0.066
2026-01-11 18:00:25,527: Running test after training batch: 16000
2026-01-11 18:00:25,651: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:00:31,836: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:00:31,897: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:00:44,904: Val batch 16000: PER (avg): 0.1362 CTC Loss (avg): 25.1078 WER(5gram): 12.78% (n=256) time: 19.377
2026-01-11 18:00:44,904: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-11 18:00:44,905: t15.2023.08.13 val PER: 0.1008
2026-01-11 18:00:44,905: t15.2023.08.18 val PER: 0.1014
2026-01-11 18:00:44,905: t15.2023.08.20 val PER: 0.0850
2026-01-11 18:00:44,905: t15.2023.08.25 val PER: 0.0843
2026-01-11 18:00:44,905: t15.2023.08.27 val PER: 0.1720
2026-01-11 18:00:44,905: t15.2023.09.01 val PER: 0.0657
2026-01-11 18:00:44,905: t15.2023.09.03 val PER: 0.1508
2026-01-11 18:00:44,905: t15.2023.09.24 val PER: 0.1104
2026-01-11 18:00:44,905: t15.2023.09.29 val PER: 0.1264
2026-01-11 18:00:44,905: t15.2023.10.01 val PER: 0.1618
2026-01-11 18:00:44,905: t15.2023.10.06 val PER: 0.0850
2026-01-11 18:00:44,905: t15.2023.10.08 val PER: 0.2287
2026-01-11 18:00:44,905: t15.2023.10.13 val PER: 0.1986
2026-01-11 18:00:44,906: t15.2023.10.15 val PER: 0.1404
2026-01-11 18:00:44,906: t15.2023.10.20 val PER: 0.2081
2026-01-11 18:00:44,906: t15.2023.10.22 val PER: 0.1214
2026-01-11 18:00:44,906: t15.2023.11.03 val PER: 0.1628
2026-01-11 18:00:44,906: t15.2023.11.04 val PER: 0.0341
2026-01-11 18:00:44,906: t15.2023.11.17 val PER: 0.0358
2026-01-11 18:00:44,907: t15.2023.11.19 val PER: 0.0339
2026-01-11 18:00:44,907: t15.2023.11.26 val PER: 0.0833
2026-01-11 18:00:44,907: t15.2023.12.03 val PER: 0.0809
2026-01-11 18:00:44,907: t15.2023.12.08 val PER: 0.0632
2026-01-11 18:00:44,907: t15.2023.12.10 val PER: 0.0644
2026-01-11 18:00:44,907: t15.2023.12.17 val PER: 0.1185
2026-01-11 18:00:44,907: t15.2023.12.29 val PER: 0.1002
2026-01-11 18:00:44,907: t15.2024.02.25 val PER: 0.1110
2026-01-11 18:00:44,907: t15.2024.03.08 val PER: 0.2219
2026-01-11 18:00:44,907: t15.2024.03.15 val PER: 0.1920
2026-01-11 18:00:44,907: t15.2024.03.17 val PER: 0.1241
2026-01-11 18:00:44,907: t15.2024.05.10 val PER: 0.1590
2026-01-11 18:00:44,908: t15.2024.06.14 val PER: 0.1514
2026-01-11 18:00:44,908: t15.2024.07.19 val PER: 0.2208
2026-01-11 18:00:44,908: t15.2024.07.21 val PER: 0.0890
2026-01-11 18:00:44,908: t15.2024.07.28 val PER: 0.1309
2026-01-11 18:00:44,908: t15.2025.01.10 val PER: 0.2769
2026-01-11 18:00:44,908: t15.2025.01.12 val PER: 0.1293
2026-01-11 18:00:44,908: t15.2025.03.14 val PER: 0.3180
2026-01-11 18:00:44,908: t15.2025.03.16 val PER: 0.1610
2026-01-11 18:00:44,908: t15.2025.03.30 val PER: 0.2460
2026-01-11 18:00:44,908: t15.2025.04.13 val PER: 0.2097
2026-01-11 18:00:45,083: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_16000
2026-01-11 18:01:06,067: Train batch 16200: loss: 6.70 grad norm: 62.61 time: 0.065
2026-01-11 18:01:26,897: Train batch 16400: loss: 13.17 grad norm: 80.67 time: 0.067
2026-01-11 18:01:37,128: Running test after training batch: 16500
2026-01-11 18:01:37,272: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:01:43,466: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:01:43,533: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:01:58,044: Val batch 16500: PER (avg): 0.1314 CTC Loss (avg): 24.7899 WER(5gram): 10.10% (n=256) time: 20.916
2026-01-11 18:01:58,045: WER lens: avg_true_words=5.99 avg_pred_words=6.01 max_pred_words=12
2026-01-11 18:01:58,045: t15.2023.08.13 val PER: 0.0915
2026-01-11 18:01:58,046: t15.2023.08.18 val PER: 0.0897
2026-01-11 18:01:58,061: t15.2023.08.20 val PER: 0.0921
2026-01-11 18:01:58,061: t15.2023.08.25 val PER: 0.0813
2026-01-11 18:01:58,062: t15.2023.08.27 val PER: 0.1833
2026-01-11 18:01:58,062: t15.2023.09.01 val PER: 0.0698
2026-01-11 18:01:58,062: t15.2023.09.03 val PER: 0.1378
2026-01-11 18:01:58,062: t15.2023.09.24 val PER: 0.1153
2026-01-11 18:01:58,062: t15.2023.09.29 val PER: 0.1232
2026-01-11 18:01:58,062: t15.2023.10.01 val PER: 0.1684
2026-01-11 18:01:58,062: t15.2023.10.06 val PER: 0.0797
2026-01-11 18:01:58,063: t15.2023.10.08 val PER: 0.2206
2026-01-11 18:01:58,063: t15.2023.10.13 val PER: 0.1939
2026-01-11 18:01:58,063: t15.2023.10.15 val PER: 0.1371
2026-01-11 18:01:58,063: t15.2023.10.20 val PER: 0.2047
2026-01-11 18:01:58,063: t15.2023.10.22 val PER: 0.1080
2026-01-11 18:01:58,063: t15.2023.11.03 val PER: 0.1655
2026-01-11 18:01:58,063: t15.2023.11.04 val PER: 0.0205
2026-01-11 18:01:58,063: t15.2023.11.17 val PER: 0.0327
2026-01-11 18:01:58,064: t15.2023.11.19 val PER: 0.0319
2026-01-11 18:01:58,064: t15.2023.11.26 val PER: 0.0645
2026-01-11 18:01:58,064: t15.2023.12.03 val PER: 0.0767
2026-01-11 18:01:58,064: t15.2023.12.08 val PER: 0.0632
2026-01-11 18:01:58,064: t15.2023.12.10 val PER: 0.0591
2026-01-11 18:01:58,064: t15.2023.12.17 val PER: 0.1091
2026-01-11 18:01:58,064: t15.2023.12.29 val PER: 0.0988
2026-01-11 18:01:58,064: t15.2024.02.25 val PER: 0.1138
2026-01-11 18:01:58,064: t15.2024.03.08 val PER: 0.1906
2026-01-11 18:01:58,065: t15.2024.03.15 val PER: 0.1807
2026-01-11 18:01:58,065: t15.2024.03.17 val PER: 0.1172
2026-01-11 18:01:58,065: t15.2024.05.10 val PER: 0.1426
2026-01-11 18:01:58,065: t15.2024.06.14 val PER: 0.1293
2026-01-11 18:01:58,065: t15.2024.07.19 val PER: 0.2070
2026-01-11 18:01:58,065: t15.2024.07.21 val PER: 0.0821
2026-01-11 18:01:58,065: t15.2024.07.28 val PER: 0.1191
2026-01-11 18:01:58,065: t15.2025.01.10 val PER: 0.2713
2026-01-11 18:01:58,066: t15.2025.01.12 val PER: 0.1247
2026-01-11 18:01:58,066: t15.2025.03.14 val PER: 0.3284
2026-01-11 18:01:58,066: t15.2025.03.16 val PER: 0.1662
2026-01-11 18:01:58,066: t15.2025.03.30 val PER: 0.2494
2026-01-11 18:01:58,066: t15.2025.04.13 val PER: 0.2168
2026-01-11 18:01:58,067: New best val WER(5gram) 10.37% --> 10.10%
2026-01-11 18:01:58,252: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_16500
2026-01-11 18:02:09,454: Train batch 16600: loss: 9.32 grad norm: 60.27 time: 0.062
2026-01-11 18:02:30,670: Train batch 16800: loss: 16.17 grad norm: 92.63 time: 0.074
2026-01-11 18:02:52,039: Train batch 17000: loss: 8.08 grad norm: 58.83 time: 0.093
2026-01-11 18:02:52,039: Running test after training batch: 17000
2026-01-11 18:02:52,150: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:02:58,240: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:02:58,303: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:03:12,235: Val batch 17000: PER (avg): 0.1330 CTC Loss (avg): 25.1381 WER(5gram): 12.78% (n=256) time: 20.196
2026-01-11 18:03:12,236: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-11 18:03:12,236: t15.2023.08.13 val PER: 0.1019
2026-01-11 18:03:12,236: t15.2023.08.18 val PER: 0.0989
2026-01-11 18:03:12,236: t15.2023.08.20 val PER: 0.0858
2026-01-11 18:03:12,236: t15.2023.08.25 val PER: 0.0919
2026-01-11 18:03:12,236: t15.2023.08.27 val PER: 0.1785
2026-01-11 18:03:12,237: t15.2023.09.01 val PER: 0.0568
2026-01-11 18:03:12,237: t15.2023.09.03 val PER: 0.1473
2026-01-11 18:03:12,237: t15.2023.09.24 val PER: 0.0971
2026-01-11 18:03:12,237: t15.2023.09.29 val PER: 0.1302
2026-01-11 18:03:12,237: t15.2023.10.01 val PER: 0.1790
2026-01-11 18:03:12,237: t15.2023.10.06 val PER: 0.0764
2026-01-11 18:03:12,238: t15.2023.10.08 val PER: 0.2327
2026-01-11 18:03:12,238: t15.2023.10.13 val PER: 0.1932
2026-01-11 18:03:12,238: t15.2023.10.15 val PER: 0.1345
2026-01-11 18:03:12,238: t15.2023.10.20 val PER: 0.2081
2026-01-11 18:03:12,238: t15.2023.10.22 val PER: 0.1114
2026-01-11 18:03:12,238: t15.2023.11.03 val PER: 0.1744
2026-01-11 18:03:12,238: t15.2023.11.04 val PER: 0.0273
2026-01-11 18:03:12,239: t15.2023.11.17 val PER: 0.0218
2026-01-11 18:03:12,239: t15.2023.11.19 val PER: 0.0220
2026-01-11 18:03:12,239: t15.2023.11.26 val PER: 0.0710
2026-01-11 18:03:12,239: t15.2023.12.03 val PER: 0.0777
2026-01-11 18:03:12,239: t15.2023.12.08 val PER: 0.0666
2026-01-11 18:03:12,239: t15.2023.12.10 val PER: 0.0591
2026-01-11 18:03:12,239: t15.2023.12.17 val PER: 0.1164
2026-01-11 18:03:12,240: t15.2023.12.29 val PER: 0.0961
2026-01-11 18:03:12,240: t15.2024.02.25 val PER: 0.1166
2026-01-11 18:03:12,240: t15.2024.03.08 val PER: 0.2091
2026-01-11 18:03:12,240: t15.2024.03.15 val PER: 0.1870
2026-01-11 18:03:12,240: t15.2024.03.17 val PER: 0.1192
2026-01-11 18:03:12,240: t15.2024.05.10 val PER: 0.1605
2026-01-11 18:03:12,240: t15.2024.06.14 val PER: 0.1325
2026-01-11 18:03:12,240: t15.2024.07.19 val PER: 0.2004
2026-01-11 18:03:12,241: t15.2024.07.21 val PER: 0.0876
2026-01-11 18:03:12,241: t15.2024.07.28 val PER: 0.1191
2026-01-11 18:03:12,241: t15.2025.01.10 val PER: 0.2796
2026-01-11 18:03:12,241: t15.2025.01.12 val PER: 0.1263
2026-01-11 18:03:12,241: t15.2025.03.14 val PER: 0.3299
2026-01-11 18:03:12,241: t15.2025.03.16 val PER: 0.1453
2026-01-11 18:03:12,241: t15.2025.03.30 val PER: 0.2414
2026-01-11 18:03:12,241: t15.2025.04.13 val PER: 0.2126
2026-01-11 18:03:12,407: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_17000
2026-01-11 18:03:33,059: Train batch 17200: loss: 8.71 grad norm: 60.09 time: 0.097
2026-01-11 18:03:53,895: Train batch 17400: loss: 10.67 grad norm: 70.21 time: 0.102
2026-01-11 18:04:03,790: Running test after training batch: 17500
2026-01-11 18:04:04,008: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:04:10,710: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:04:10,771: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:04:26,124: Val batch 17500: PER (avg): 0.1299 CTC Loss (avg): 25.2584 WER(5gram): 10.95% (n=256) time: 22.334
2026-01-11 18:04:26,125: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-11 18:04:26,125: t15.2023.08.13 val PER: 0.1008
2026-01-11 18:04:26,126: t15.2023.08.18 val PER: 0.0847
2026-01-11 18:04:26,126: t15.2023.08.20 val PER: 0.0858
2026-01-11 18:04:26,126: t15.2023.08.25 val PER: 0.0813
2026-01-11 18:04:26,126: t15.2023.08.27 val PER: 0.1720
2026-01-11 18:04:26,126: t15.2023.09.01 val PER: 0.0593
2026-01-11 18:04:26,126: t15.2023.09.03 val PER: 0.1485
2026-01-11 18:04:26,126: t15.2023.09.24 val PER: 0.1056
2026-01-11 18:04:26,126: t15.2023.09.29 val PER: 0.1219
2026-01-11 18:04:26,127: t15.2023.10.01 val PER: 0.1612
2026-01-11 18:04:26,127: t15.2023.10.06 val PER: 0.0678
2026-01-11 18:04:26,127: t15.2023.10.08 val PER: 0.2368
2026-01-11 18:04:26,127: t15.2023.10.13 val PER: 0.2009
2026-01-11 18:04:26,127: t15.2023.10.15 val PER: 0.1233
2026-01-11 18:04:26,127: t15.2023.10.20 val PER: 0.1812
2026-01-11 18:04:26,127: t15.2023.10.22 val PER: 0.1058
2026-01-11 18:04:26,128: t15.2023.11.03 val PER: 0.1635
2026-01-11 18:04:26,128: t15.2023.11.04 val PER: 0.0341
2026-01-11 18:04:26,128: t15.2023.11.17 val PER: 0.0327
2026-01-11 18:04:26,128: t15.2023.11.19 val PER: 0.0140
2026-01-11 18:04:26,128: t15.2023.11.26 val PER: 0.0652
2026-01-11 18:04:26,128: t15.2023.12.03 val PER: 0.0746
2026-01-11 18:04:26,128: t15.2023.12.08 val PER: 0.0613
2026-01-11 18:04:26,128: t15.2023.12.10 val PER: 0.0552
2026-01-11 18:04:26,129: t15.2023.12.17 val PER: 0.1008
2026-01-11 18:04:26,129: t15.2023.12.29 val PER: 0.1057
2026-01-11 18:04:26,129: t15.2024.02.25 val PER: 0.1067
2026-01-11 18:04:26,129: t15.2024.03.08 val PER: 0.2248
2026-01-11 18:04:26,129: t15.2024.03.15 val PER: 0.1851
2026-01-11 18:04:26,129: t15.2024.03.17 val PER: 0.1158
2026-01-11 18:04:26,129: t15.2024.05.10 val PER: 0.1471
2026-01-11 18:04:26,129: t15.2024.06.14 val PER: 0.1262
2026-01-11 18:04:26,129: t15.2024.07.19 val PER: 0.2129
2026-01-11 18:04:26,130: t15.2024.07.21 val PER: 0.0903
2026-01-11 18:04:26,130: t15.2024.07.28 val PER: 0.1265
2026-01-11 18:04:26,130: t15.2025.01.10 val PER: 0.2727
2026-01-11 18:04:26,130: t15.2025.01.12 val PER: 0.1186
2026-01-11 18:04:26,130: t15.2025.03.14 val PER: 0.3033
2026-01-11 18:04:26,130: t15.2025.03.16 val PER: 0.1505
2026-01-11 18:04:26,130: t15.2025.03.30 val PER: 0.2379
2026-01-11 18:04:26,130: t15.2025.04.13 val PER: 0.2183
2026-01-11 18:04:26,289: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_17500
2026-01-11 18:04:36,551: Train batch 17600: loss: 7.01 grad norm: 58.21 time: 0.063
2026-01-11 18:04:57,113: Train batch 17800: loss: 6.11 grad norm: 59.47 time: 0.060
2026-01-11 18:05:17,657: Train batch 18000: loss: 8.01 grad norm: 70.12 time: 0.077
2026-01-11 18:05:17,658: Running test after training batch: 18000
2026-01-11 18:05:17,781: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:05:23,961: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:05:24,026: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:05:38,189: Val batch 18000: PER (avg): 0.1269 CTC Loss (avg): 24.8470 WER(5gram): 11.67% (n=256) time: 20.531
2026-01-11 18:05:38,190: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-11 18:05:38,191: t15.2023.08.13 val PER: 0.1029
2026-01-11 18:05:38,191: t15.2023.08.18 val PER: 0.0813
2026-01-11 18:05:38,191: t15.2023.08.20 val PER: 0.0794
2026-01-11 18:05:38,191: t15.2023.08.25 val PER: 0.0828
2026-01-11 18:05:38,191: t15.2023.08.27 val PER: 0.1624
2026-01-11 18:05:38,191: t15.2023.09.01 val PER: 0.0601
2026-01-11 18:05:38,191: t15.2023.09.03 val PER: 0.1390
2026-01-11 18:05:38,191: t15.2023.09.24 val PER: 0.1019
2026-01-11 18:05:38,192: t15.2023.09.29 val PER: 0.1238
2026-01-11 18:05:38,192: t15.2023.10.01 val PER: 0.1684
2026-01-11 18:05:38,192: t15.2023.10.06 val PER: 0.0635
2026-01-11 18:05:38,192: t15.2023.10.08 val PER: 0.2327
2026-01-11 18:05:38,192: t15.2023.10.13 val PER: 0.1908
2026-01-11 18:05:38,192: t15.2023.10.15 val PER: 0.1345
2026-01-11 18:05:38,192: t15.2023.10.20 val PER: 0.1846
2026-01-11 18:05:38,192: t15.2023.10.22 val PER: 0.1024
2026-01-11 18:05:38,193: t15.2023.11.03 val PER: 0.1696
2026-01-11 18:05:38,193: t15.2023.11.04 val PER: 0.0273
2026-01-11 18:05:38,193: t15.2023.11.17 val PER: 0.0264
2026-01-11 18:05:38,193: t15.2023.11.19 val PER: 0.0160
2026-01-11 18:05:38,193: t15.2023.11.26 val PER: 0.0659
2026-01-11 18:05:38,193: t15.2023.12.03 val PER: 0.0756
2026-01-11 18:05:38,193: t15.2023.12.08 val PER: 0.0566
2026-01-11 18:05:38,193: t15.2023.12.10 val PER: 0.0512
2026-01-11 18:05:38,193: t15.2023.12.17 val PER: 0.0967
2026-01-11 18:05:38,194: t15.2023.12.29 val PER: 0.1043
2026-01-11 18:05:38,194: t15.2024.02.25 val PER: 0.1096
2026-01-11 18:05:38,194: t15.2024.03.08 val PER: 0.2091
2026-01-11 18:05:38,194: t15.2024.03.15 val PER: 0.1770
2026-01-11 18:05:38,194: t15.2024.03.17 val PER: 0.1025
2026-01-11 18:05:38,194: t15.2024.05.10 val PER: 0.1382
2026-01-11 18:05:38,194: t15.2024.06.14 val PER: 0.1388
2026-01-11 18:05:38,194: t15.2024.07.19 val PER: 0.1892
2026-01-11 18:05:38,195: t15.2024.07.21 val PER: 0.0834
2026-01-11 18:05:38,195: t15.2024.07.28 val PER: 0.1235
2026-01-11 18:05:38,195: t15.2025.01.10 val PER: 0.2576
2026-01-11 18:05:38,195: t15.2025.01.12 val PER: 0.1186
2026-01-11 18:05:38,195: t15.2025.03.14 val PER: 0.3121
2026-01-11 18:05:38,195: t15.2025.03.16 val PER: 0.1414
2026-01-11 18:05:38,195: t15.2025.03.30 val PER: 0.2506
2026-01-11 18:05:38,195: t15.2025.04.13 val PER: 0.2054
2026-01-11 18:05:38,355: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_18000
2026-01-11 18:05:59,609: Train batch 18200: loss: 7.61 grad norm: 58.49 time: 0.090
2026-01-11 18:06:20,231: Train batch 18400: loss: 3.02 grad norm: 42.25 time: 0.067
2026-01-11 18:06:30,768: Running test after training batch: 18500
2026-01-11 18:06:30,947: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:06:37,280: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:06:37,342: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:06:51,140: Val batch 18500: PER (avg): 0.1263 CTC Loss (avg): 25.2880 WER(5gram): 12.32% (n=256) time: 20.371
2026-01-11 18:06:51,140: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 18:06:51,140: t15.2023.08.13 val PER: 0.1060
2026-01-11 18:06:51,141: t15.2023.08.18 val PER: 0.0796
2026-01-11 18:06:51,141: t15.2023.08.20 val PER: 0.0770
2026-01-11 18:06:51,141: t15.2023.08.25 val PER: 0.0768
2026-01-11 18:06:51,141: t15.2023.08.27 val PER: 0.1704
2026-01-11 18:06:51,141: t15.2023.09.01 val PER: 0.0617
2026-01-11 18:06:51,141: t15.2023.09.03 val PER: 0.1401
2026-01-11 18:06:51,141: t15.2023.09.24 val PER: 0.0886
2026-01-11 18:06:51,141: t15.2023.09.29 val PER: 0.1276
2026-01-11 18:06:51,141: t15.2023.10.01 val PER: 0.1691
2026-01-11 18:06:51,141: t15.2023.10.06 val PER: 0.0732
2026-01-11 18:06:51,142: t15.2023.10.08 val PER: 0.2422
2026-01-11 18:06:51,142: t15.2023.10.13 val PER: 0.1916
2026-01-11 18:06:51,142: t15.2023.10.15 val PER: 0.1371
2026-01-11 18:06:51,142: t15.2023.10.20 val PER: 0.2013
2026-01-11 18:06:51,142: t15.2023.10.22 val PER: 0.1158
2026-01-11 18:06:51,142: t15.2023.11.03 val PER: 0.1723
2026-01-11 18:06:51,142: t15.2023.11.04 val PER: 0.0239
2026-01-11 18:06:51,142: t15.2023.11.17 val PER: 0.0264
2026-01-11 18:06:51,142: t15.2023.11.19 val PER: 0.0259
2026-01-11 18:06:51,143: t15.2023.11.26 val PER: 0.0674
2026-01-11 18:06:51,143: t15.2023.12.03 val PER: 0.0714
2026-01-11 18:06:51,143: t15.2023.12.08 val PER: 0.0539
2026-01-11 18:06:51,143: t15.2023.12.10 val PER: 0.0473
2026-01-11 18:06:51,143: t15.2023.12.17 val PER: 0.0977
2026-01-11 18:06:51,143: t15.2023.12.29 val PER: 0.0975
2026-01-11 18:06:51,143: t15.2024.02.25 val PER: 0.0885
2026-01-11 18:06:51,143: t15.2024.03.08 val PER: 0.1878
2026-01-11 18:06:51,143: t15.2024.03.15 val PER: 0.1795
2026-01-11 18:06:51,143: t15.2024.03.17 val PER: 0.1032
2026-01-11 18:06:51,143: t15.2024.05.10 val PER: 0.1322
2026-01-11 18:06:51,143: t15.2024.06.14 val PER: 0.1262
2026-01-11 18:06:51,143: t15.2024.07.19 val PER: 0.1951
2026-01-11 18:06:51,144: t15.2024.07.21 val PER: 0.0759
2026-01-11 18:06:51,144: t15.2024.07.28 val PER: 0.1199
2026-01-11 18:06:51,144: t15.2025.01.10 val PER: 0.2617
2026-01-11 18:06:51,144: t15.2025.01.12 val PER: 0.1201
2026-01-11 18:06:51,144: t15.2025.03.14 val PER: 0.2988
2026-01-11 18:06:51,144: t15.2025.03.16 val PER: 0.1479
2026-01-11 18:06:51,144: t15.2025.03.30 val PER: 0.2483
2026-01-11 18:06:51,144: t15.2025.04.13 val PER: 0.1983
2026-01-11 18:06:51,305: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_18500
2026-01-11 18:07:01,295: Train batch 18600: loss: 9.70 grad norm: 72.15 time: 0.079
2026-01-11 18:07:23,218: Train batch 18800: loss: 5.46 grad norm: 51.45 time: 0.077
2026-01-11 18:07:44,737: Train batch 19000: loss: 7.45 grad norm: 60.46 time: 0.075
2026-01-11 18:07:44,737: Running test after training batch: 19000
2026-01-11 18:07:44,890: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:07:51,873: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:07:51,955: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:08:06,506: Val batch 19000: PER (avg): 0.1257 CTC Loss (avg): 24.8029 WER(5gram): 11.93% (n=256) time: 21.768
2026-01-11 18:08:06,506: WER lens: avg_true_words=5.99 avg_pred_words=6.06 max_pred_words=12
2026-01-11 18:08:06,506: t15.2023.08.13 val PER: 0.0946
2026-01-11 18:08:06,507: t15.2023.08.18 val PER: 0.0863
2026-01-11 18:08:06,507: t15.2023.08.20 val PER: 0.0763
2026-01-11 18:08:06,507: t15.2023.08.25 val PER: 0.0783
2026-01-11 18:08:06,507: t15.2023.08.27 val PER: 0.1640
2026-01-11 18:08:06,507: t15.2023.09.01 val PER: 0.0641
2026-01-11 18:08:06,507: t15.2023.09.03 val PER: 0.1413
2026-01-11 18:08:06,507: t15.2023.09.24 val PER: 0.1007
2026-01-11 18:08:06,507: t15.2023.09.29 val PER: 0.1244
2026-01-11 18:08:06,507: t15.2023.10.01 val PER: 0.1645
2026-01-11 18:08:06,507: t15.2023.10.06 val PER: 0.0710
2026-01-11 18:08:06,507: t15.2023.10.08 val PER: 0.2300
2026-01-11 18:08:06,507: t15.2023.10.13 val PER: 0.1901
2026-01-11 18:08:06,507: t15.2023.10.15 val PER: 0.1338
2026-01-11 18:08:06,507: t15.2023.10.20 val PER: 0.1980
2026-01-11 18:08:06,508: t15.2023.10.22 val PER: 0.1080
2026-01-11 18:08:06,508: t15.2023.11.03 val PER: 0.1649
2026-01-11 18:08:06,508: t15.2023.11.04 val PER: 0.0239
2026-01-11 18:08:06,508: t15.2023.11.17 val PER: 0.0280
2026-01-11 18:08:06,508: t15.2023.11.19 val PER: 0.0140
2026-01-11 18:08:06,508: t15.2023.11.26 val PER: 0.0623
2026-01-11 18:08:06,508: t15.2023.12.03 val PER: 0.0725
2026-01-11 18:08:06,508: t15.2023.12.08 val PER: 0.0593
2026-01-11 18:08:06,508: t15.2023.12.10 val PER: 0.0447
2026-01-11 18:08:06,509: t15.2023.12.17 val PER: 0.0967
2026-01-11 18:08:06,509: t15.2023.12.29 val PER: 0.0988
2026-01-11 18:08:06,509: t15.2024.02.25 val PER: 0.0997
2026-01-11 18:08:06,509: t15.2024.03.08 val PER: 0.1991
2026-01-11 18:08:06,509: t15.2024.03.15 val PER: 0.1814
2026-01-11 18:08:06,509: t15.2024.03.17 val PER: 0.1053
2026-01-11 18:08:06,509: t15.2024.05.10 val PER: 0.1352
2026-01-11 18:08:06,509: t15.2024.06.14 val PER: 0.1388
2026-01-11 18:08:06,509: t15.2024.07.19 val PER: 0.1918
2026-01-11 18:08:06,509: t15.2024.07.21 val PER: 0.0807
2026-01-11 18:08:06,509: t15.2024.07.28 val PER: 0.1088
2026-01-11 18:08:06,509: t15.2025.01.10 val PER: 0.2755
2026-01-11 18:08:06,509: t15.2025.01.12 val PER: 0.1178
2026-01-11 18:08:06,509: t15.2025.03.14 val PER: 0.3047
2026-01-11 18:08:06,510: t15.2025.03.16 val PER: 0.1414
2026-01-11 18:08:06,510: t15.2025.03.30 val PER: 0.2356
2026-01-11 18:08:06,510: t15.2025.04.13 val PER: 0.2068
2026-01-11 18:08:06,710: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_19000
2026-01-11 18:08:26,755: Train batch 19200: loss: 4.35 grad norm: 49.14 time: 0.073
2026-01-11 18:08:47,700: Train batch 19400: loss: 2.76 grad norm: 41.37 time: 0.062
2026-01-11 18:08:57,939: Running test after training batch: 19500
2026-01-11 18:08:58,235: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:09:04,387: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:09:04,457: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cent
2026-01-11 18:09:18,421: Val batch 19500: PER (avg): 0.1245 CTC Loss (avg): 25.1800 WER(5gram): 11.93% (n=256) time: 20.481
2026-01-11 18:09:18,422: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 18:09:18,422: t15.2023.08.13 val PER: 0.0988
2026-01-11 18:09:18,422: t15.2023.08.18 val PER: 0.0872
2026-01-11 18:09:18,422: t15.2023.08.20 val PER: 0.0818
2026-01-11 18:09:18,422: t15.2023.08.25 val PER: 0.0708
2026-01-11 18:09:18,422: t15.2023.08.27 val PER: 0.1495
2026-01-11 18:09:18,423: t15.2023.09.01 val PER: 0.0609
2026-01-11 18:09:18,423: t15.2023.09.03 val PER: 0.1318
2026-01-11 18:09:18,423: t15.2023.09.24 val PER: 0.0910
2026-01-11 18:09:18,423: t15.2023.09.29 val PER: 0.1257
2026-01-11 18:09:18,423: t15.2023.10.01 val PER: 0.1678
2026-01-11 18:09:18,423: t15.2023.10.06 val PER: 0.0624
2026-01-11 18:09:18,423: t15.2023.10.08 val PER: 0.2300
2026-01-11 18:09:18,423: t15.2023.10.13 val PER: 0.1870
2026-01-11 18:09:18,423: t15.2023.10.15 val PER: 0.1358
2026-01-11 18:09:18,424: t15.2023.10.20 val PER: 0.1980
2026-01-11 18:09:18,424: t15.2023.10.22 val PER: 0.1080
2026-01-11 18:09:18,424: t15.2023.11.03 val PER: 0.1615
2026-01-11 18:09:18,424: t15.2023.11.04 val PER: 0.0307
2026-01-11 18:09:18,424: t15.2023.11.17 val PER: 0.0295
2026-01-11 18:09:18,424: t15.2023.11.19 val PER: 0.0140
2026-01-11 18:09:18,424: t15.2023.11.26 val PER: 0.0623
2026-01-11 18:09:18,424: t15.2023.12.03 val PER: 0.0767
2026-01-11 18:09:18,424: t15.2023.12.08 val PER: 0.0593
2026-01-11 18:09:18,425: t15.2023.12.10 val PER: 0.0512
2026-01-11 18:09:18,425: t15.2023.12.17 val PER: 0.1019
2026-01-11 18:09:18,425: t15.2023.12.29 val PER: 0.1030
2026-01-11 18:09:18,425: t15.2024.02.25 val PER: 0.0927
2026-01-11 18:09:18,425: t15.2024.03.08 val PER: 0.1963
2026-01-11 18:09:18,425: t15.2024.03.15 val PER: 0.1689
2026-01-11 18:09:18,425: t15.2024.03.17 val PER: 0.1060
2026-01-11 18:09:18,425: t15.2024.05.10 val PER: 0.1308
2026-01-11 18:09:18,425: t15.2024.06.14 val PER: 0.1215
2026-01-11 18:09:18,426: t15.2024.07.19 val PER: 0.2037
2026-01-11 18:09:18,426: t15.2024.07.21 val PER: 0.0759
2026-01-11 18:09:18,426: t15.2024.07.28 val PER: 0.1118
2026-01-11 18:09:18,426: t15.2025.01.10 val PER: 0.2424
2026-01-11 18:09:18,426: t15.2025.01.12 val PER: 0.1216
2026-01-11 18:09:18,426: t15.2025.03.14 val PER: 0.2973
2026-01-11 18:09:18,426: t15.2025.03.16 val PER: 0.1401
2026-01-11 18:09:18,426: t15.2025.03.30 val PER: 0.2483
2026-01-11 18:09:18,426: t15.2025.04.13 val PER: 0.2011
2026-01-11 18:09:18,587: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_19500
2026-01-11 18:09:29,545: Train batch 19600: loss: 6.02 grad norm: 56.96 time: 0.067
2026-01-11 18:09:50,494: Train batch 19800: loss: 4.46 grad norm: 48.07 time: 0.065
2026-01-11 18:10:11,652: Train batch 20000: loss: 4.02 grad norm: 44.86 time: 0.078
2026-01-11 18:10:11,653: Running test after training batch: 20000
2026-01-11 18:10:11,844: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:10:17,979: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:10:18,051: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:10:31,876: Val batch 20000: PER (avg): 0.1237 CTC Loss (avg): 25.7308 WER(5gram): 10.95% (n=256) time: 20.223
2026-01-11 18:10:31,877: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 18:10:31,877: t15.2023.08.13 val PER: 0.0904
2026-01-11 18:10:31,877: t15.2023.08.18 val PER: 0.0796
2026-01-11 18:10:31,877: t15.2023.08.20 val PER: 0.0755
2026-01-11 18:10:31,877: t15.2023.08.25 val PER: 0.0843
2026-01-11 18:10:31,877: t15.2023.08.27 val PER: 0.1543
2026-01-11 18:10:31,877: t15.2023.09.01 val PER: 0.0568
2026-01-11 18:10:31,877: t15.2023.09.03 val PER: 0.1401
2026-01-11 18:10:31,878: t15.2023.09.24 val PER: 0.1080
2026-01-11 18:10:31,878: t15.2023.09.29 val PER: 0.1187
2026-01-11 18:10:31,878: t15.2023.10.01 val PER: 0.1631
2026-01-11 18:10:31,878: t15.2023.10.06 val PER: 0.0764
2026-01-11 18:10:31,878: t15.2023.10.08 val PER: 0.2314
2026-01-11 18:10:31,878: t15.2023.10.13 val PER: 0.1877
2026-01-11 18:10:31,879: t15.2023.10.15 val PER: 0.1338
2026-01-11 18:10:31,879: t15.2023.10.20 val PER: 0.2081
2026-01-11 18:10:31,879: t15.2023.10.22 val PER: 0.1136
2026-01-11 18:10:31,879: t15.2023.11.03 val PER: 0.1635
2026-01-11 18:10:31,879: t15.2023.11.04 val PER: 0.0239
2026-01-11 18:10:31,879: t15.2023.11.17 val PER: 0.0342
2026-01-11 18:10:31,879: t15.2023.11.19 val PER: 0.0140
2026-01-11 18:10:31,879: t15.2023.11.26 val PER: 0.0674
2026-01-11 18:10:31,879: t15.2023.12.03 val PER: 0.0735
2026-01-11 18:10:31,879: t15.2023.12.08 val PER: 0.0553
2026-01-11 18:10:31,880: t15.2023.12.10 val PER: 0.0499
2026-01-11 18:10:31,880: t15.2023.12.17 val PER: 0.0936
2026-01-11 18:10:31,880: t15.2023.12.29 val PER: 0.0913
2026-01-11 18:10:31,881: t15.2024.02.25 val PER: 0.0857
2026-01-11 18:10:31,881: t15.2024.03.08 val PER: 0.2063
2026-01-11 18:10:31,881: t15.2024.03.15 val PER: 0.1839
2026-01-11 18:10:31,881: t15.2024.03.17 val PER: 0.1032
2026-01-11 18:10:31,881: t15.2024.05.10 val PER: 0.1456
2026-01-11 18:10:31,881: t15.2024.06.14 val PER: 0.1309
2026-01-11 18:10:31,881: t15.2024.07.19 val PER: 0.1866
2026-01-11 18:10:31,881: t15.2024.07.21 val PER: 0.0717
2026-01-11 18:10:31,882: t15.2024.07.28 val PER: 0.1147
2026-01-11 18:10:31,882: t15.2025.01.10 val PER: 0.2548
2026-01-11 18:10:31,882: t15.2025.01.12 val PER: 0.1032
2026-01-11 18:10:31,882: t15.2025.03.14 val PER: 0.3092
2026-01-11 18:10:31,882: t15.2025.03.16 val PER: 0.1531
2026-01-11 18:10:31,882: t15.2025.03.30 val PER: 0.2241
2026-01-11 18:10:31,882: t15.2025.04.13 val PER: 0.1940
2026-01-11 18:10:32,055: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_20000
2026-01-11 18:10:52,162: Train batch 20200: loss: 3.10 grad norm: 45.99 time: 0.070
2026-01-11 18:11:13,452: Train batch 20400: loss: 3.39 grad norm: 44.39 time: 0.074
2026-01-11 18:11:23,949: Running test after training batch: 20500
2026-01-11 18:11:24,068: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:11:30,288: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:11:30,351: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:11:44,307: Val batch 20500: PER (avg): 0.1243 CTC Loss (avg): 25.0756 WER(5gram): 10.69% (n=256) time: 20.358
2026-01-11 18:11:44,308: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-11 18:11:44,308: t15.2023.08.13 val PER: 0.0998
2026-01-11 18:11:44,308: t15.2023.08.18 val PER: 0.0830
2026-01-11 18:11:44,309: t15.2023.08.20 val PER: 0.0794
2026-01-11 18:11:44,309: t15.2023.08.25 val PER: 0.0693
2026-01-11 18:11:44,309: t15.2023.08.27 val PER: 0.1656
2026-01-11 18:11:44,309: t15.2023.09.01 val PER: 0.0657
2026-01-11 18:11:44,309: t15.2023.09.03 val PER: 0.1508
2026-01-11 18:11:44,309: t15.2023.09.24 val PER: 0.1056
2026-01-11 18:11:44,310: t15.2023.09.29 val PER: 0.1213
2026-01-11 18:11:44,310: t15.2023.10.01 val PER: 0.1618
2026-01-11 18:11:44,310: t15.2023.10.06 val PER: 0.0743
2026-01-11 18:11:44,310: t15.2023.10.08 val PER: 0.2382
2026-01-11 18:11:44,310: t15.2023.10.13 val PER: 0.1808
2026-01-11 18:11:44,310: t15.2023.10.15 val PER: 0.1365
2026-01-11 18:11:44,310: t15.2023.10.20 val PER: 0.1779
2026-01-11 18:11:44,310: t15.2023.10.22 val PER: 0.1069
2026-01-11 18:11:44,311: t15.2023.11.03 val PER: 0.1689
2026-01-11 18:11:44,311: t15.2023.11.04 val PER: 0.0273
2026-01-11 18:11:44,311: t15.2023.11.17 val PER: 0.0358
2026-01-11 18:11:44,311: t15.2023.11.19 val PER: 0.0180
2026-01-11 18:11:44,311: t15.2023.11.26 val PER: 0.0601
2026-01-11 18:11:44,311: t15.2023.12.03 val PER: 0.0641
2026-01-11 18:11:44,311: t15.2023.12.08 val PER: 0.0459
2026-01-11 18:11:44,311: t15.2023.12.10 val PER: 0.0407
2026-01-11 18:11:44,311: t15.2023.12.17 val PER: 0.1102
2026-01-11 18:11:44,311: t15.2023.12.29 val PER: 0.0906
2026-01-11 18:11:44,311: t15.2024.02.25 val PER: 0.0899
2026-01-11 18:11:44,312: t15.2024.03.08 val PER: 0.1935
2026-01-11 18:11:44,312: t15.2024.03.15 val PER: 0.1732
2026-01-11 18:11:44,312: t15.2024.03.17 val PER: 0.1081
2026-01-11 18:11:44,312: t15.2024.05.10 val PER: 0.1248
2026-01-11 18:11:44,312: t15.2024.06.14 val PER: 0.1341
2026-01-11 18:11:44,312: t15.2024.07.19 val PER: 0.1971
2026-01-11 18:11:44,313: t15.2024.07.21 val PER: 0.0793
2026-01-11 18:11:44,313: t15.2024.07.28 val PER: 0.1118
2026-01-11 18:11:44,313: t15.2025.01.10 val PER: 0.2507
2026-01-11 18:11:44,313: t15.2025.01.12 val PER: 0.1109
2026-01-11 18:11:44,313: t15.2025.03.14 val PER: 0.2973
2026-01-11 18:11:44,314: t15.2025.03.16 val PER: 0.1479
2026-01-11 18:11:44,314: t15.2025.03.30 val PER: 0.2460
2026-01-11 18:11:44,314: t15.2025.04.13 val PER: 0.2040
2026-01-11 18:11:44,488: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_20500
2026-01-11 18:11:54,916: Train batch 20600: loss: 4.02 grad norm: 52.69 time: 0.067
2026-01-11 18:12:15,709: Train batch 20800: loss: 5.06 grad norm: 62.44 time: 0.064
2026-01-11 18:12:37,063: Train batch 21000: loss: 3.65 grad norm: 45.57 time: 0.062
2026-01-11 18:12:37,063: Running test after training batch: 21000
2026-01-11 18:12:37,250: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:12:43,313: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:12:43,373: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost said
2026-01-11 18:12:58,333: Val batch 21000: PER (avg): 0.1222 CTC Loss (avg): 25.1008 WER(5gram): 11.54% (n=256) time: 21.269
2026-01-11 18:12:58,334: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 18:12:58,334: t15.2023.08.13 val PER: 0.0884
2026-01-11 18:12:58,334: t15.2023.08.18 val PER: 0.0729
2026-01-11 18:12:58,335: t15.2023.08.20 val PER: 0.0778
2026-01-11 18:12:58,335: t15.2023.08.25 val PER: 0.0678
2026-01-11 18:12:58,335: t15.2023.08.27 val PER: 0.1672
2026-01-11 18:12:58,335: t15.2023.09.01 val PER: 0.0576
2026-01-11 18:12:58,335: t15.2023.09.03 val PER: 0.1378
2026-01-11 18:12:58,335: t15.2023.09.24 val PER: 0.0995
2026-01-11 18:12:58,335: t15.2023.09.29 val PER: 0.1251
2026-01-11 18:12:58,335: t15.2023.10.01 val PER: 0.1612
2026-01-11 18:12:58,335: t15.2023.10.06 val PER: 0.0797
2026-01-11 18:12:58,336: t15.2023.10.08 val PER: 0.2395
2026-01-11 18:12:58,336: t15.2023.10.13 val PER: 0.1839
2026-01-11 18:12:58,336: t15.2023.10.15 val PER: 0.1252
2026-01-11 18:12:58,336: t15.2023.10.20 val PER: 0.1946
2026-01-11 18:12:58,336: t15.2023.10.22 val PER: 0.0991
2026-01-11 18:12:58,336: t15.2023.11.03 val PER: 0.1649
2026-01-11 18:12:58,337: t15.2023.11.04 val PER: 0.0239
2026-01-11 18:12:58,337: t15.2023.11.17 val PER: 0.0295
2026-01-11 18:12:58,337: t15.2023.11.19 val PER: 0.0200
2026-01-11 18:12:58,337: t15.2023.11.26 val PER: 0.0594
2026-01-11 18:12:58,337: t15.2023.12.03 val PER: 0.0662
2026-01-11 18:12:58,337: t15.2023.12.08 val PER: 0.0519
2026-01-11 18:12:58,338: t15.2023.12.10 val PER: 0.0434
2026-01-11 18:12:58,338: t15.2023.12.17 val PER: 0.0967
2026-01-11 18:12:58,338: t15.2023.12.29 val PER: 0.0995
2026-01-11 18:12:58,338: t15.2024.02.25 val PER: 0.0941
2026-01-11 18:12:58,338: t15.2024.03.08 val PER: 0.1920
2026-01-11 18:12:58,338: t15.2024.03.15 val PER: 0.1782
2026-01-11 18:12:58,338: t15.2024.03.17 val PER: 0.1018
2026-01-11 18:12:58,338: t15.2024.05.10 val PER: 0.1204
2026-01-11 18:12:58,338: t15.2024.06.14 val PER: 0.1278
2026-01-11 18:12:58,339: t15.2024.07.19 val PER: 0.1978
2026-01-11 18:12:58,339: t15.2024.07.21 val PER: 0.0793
2026-01-11 18:12:58,339: t15.2024.07.28 val PER: 0.1044
2026-01-11 18:12:58,339: t15.2025.01.10 val PER: 0.2534
2026-01-11 18:12:58,339: t15.2025.01.12 val PER: 0.1062
2026-01-11 18:12:58,339: t15.2025.03.14 val PER: 0.3107
2026-01-11 18:12:58,340: t15.2025.03.16 val PER: 0.1466
2026-01-11 18:12:58,340: t15.2025.03.30 val PER: 0.2276
2026-01-11 18:12:58,340: t15.2025.04.13 val PER: 0.1969
2026-01-11 18:12:58,539: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_21000
2026-01-11 18:13:19,219: Train batch 21200: loss: 3.88 grad norm: 40.85 time: 0.090
2026-01-11 18:13:40,382: Train batch 21400: loss: 6.37 grad norm: 56.57 time: 0.071
2026-01-11 18:13:50,577: Running test after training batch: 21500
2026-01-11 18:13:50,813: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:13:57,699: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:13:57,778: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cent
2026-01-11 18:14:12,799: Val batch 21500: PER (avg): 0.1217 CTC Loss (avg): 25.3395 WER(5gram): 11.21% (n=256) time: 22.221
2026-01-11 18:14:12,799: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-11 18:14:12,800: t15.2023.08.13 val PER: 0.1081
2026-01-11 18:14:12,800: t15.2023.08.18 val PER: 0.0796
2026-01-11 18:14:12,800: t15.2023.08.20 val PER: 0.0691
2026-01-11 18:14:12,800: t15.2023.08.25 val PER: 0.0723
2026-01-11 18:14:12,800: t15.2023.08.27 val PER: 0.1688
2026-01-11 18:14:12,800: t15.2023.09.01 val PER: 0.0552
2026-01-11 18:14:12,800: t15.2023.09.03 val PER: 0.1390
2026-01-11 18:14:12,800: t15.2023.09.24 val PER: 0.0995
2026-01-11 18:14:12,801: t15.2023.09.29 val PER: 0.1206
2026-01-11 18:14:12,801: t15.2023.10.01 val PER: 0.1664
2026-01-11 18:14:12,801: t15.2023.10.06 val PER: 0.0689
2026-01-11 18:14:12,801: t15.2023.10.08 val PER: 0.2219
2026-01-11 18:14:12,801: t15.2023.10.13 val PER: 0.1870
2026-01-11 18:14:12,801: t15.2023.10.15 val PER: 0.1246
2026-01-11 18:14:12,801: t15.2023.10.20 val PER: 0.2013
2026-01-11 18:14:12,801: t15.2023.10.22 val PER: 0.0947
2026-01-11 18:14:12,801: t15.2023.11.03 val PER: 0.1716
2026-01-11 18:14:12,802: t15.2023.11.04 val PER: 0.0375
2026-01-11 18:14:12,802: t15.2023.11.17 val PER: 0.0342
2026-01-11 18:14:12,802: t15.2023.11.19 val PER: 0.0240
2026-01-11 18:14:12,802: t15.2023.11.26 val PER: 0.0616
2026-01-11 18:14:12,802: t15.2023.12.03 val PER: 0.0672
2026-01-11 18:14:12,802: t15.2023.12.08 val PER: 0.0459
2026-01-11 18:14:12,802: t15.2023.12.10 val PER: 0.0499
2026-01-11 18:14:12,802: t15.2023.12.17 val PER: 0.0925
2026-01-11 18:14:12,803: t15.2023.12.29 val PER: 0.0940
2026-01-11 18:14:12,803: t15.2024.02.25 val PER: 0.0969
2026-01-11 18:14:12,803: t15.2024.03.08 val PER: 0.1863
2026-01-11 18:14:12,803: t15.2024.03.15 val PER: 0.1757
2026-01-11 18:14:12,803: t15.2024.03.17 val PER: 0.1004
2026-01-11 18:14:12,803: t15.2024.05.10 val PER: 0.1263
2026-01-11 18:14:12,803: t15.2024.06.14 val PER: 0.1293
2026-01-11 18:14:12,803: t15.2024.07.19 val PER: 0.1905
2026-01-11 18:14:12,803: t15.2024.07.21 val PER: 0.0731
2026-01-11 18:14:12,803: t15.2024.07.28 val PER: 0.1029
2026-01-11 18:14:12,803: t15.2025.01.10 val PER: 0.2534
2026-01-11 18:14:12,803: t15.2025.01.12 val PER: 0.0978
2026-01-11 18:14:12,804: t15.2025.03.14 val PER: 0.3240
2026-01-11 18:14:12,804: t15.2025.03.16 val PER: 0.1322
2026-01-11 18:14:12,804: t15.2025.03.30 val PER: 0.2368
2026-01-11 18:14:12,804: t15.2025.04.13 val PER: 0.2026
2026-01-11 18:14:12,982: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_21500
2026-01-11 18:14:23,489: Train batch 21600: loss: 7.36 grad norm: 76.38 time: 0.092
2026-01-11 18:14:43,940: Train batch 21800: loss: 3.83 grad norm: 50.84 time: 0.095
2026-01-11 18:15:04,678: Train batch 22000: loss: 7.64 grad norm: 60.34 time: 0.062
2026-01-11 18:15:04,678: Running test after training batch: 22000
2026-01-11 18:15:04,983: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:15:11,100: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point as well
2026-01-11 18:15:11,163: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:15:25,633: Val batch 22000: PER (avg): 0.1207 CTC Loss (avg): 25.3306 WER(5gram): 11.02% (n=256) time: 20.954
2026-01-11 18:15:25,633: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-11 18:15:25,633: t15.2023.08.13 val PER: 0.0915
2026-01-11 18:15:25,634: t15.2023.08.18 val PER: 0.0796
2026-01-11 18:15:25,634: t15.2023.08.20 val PER: 0.0818
2026-01-11 18:15:25,634: t15.2023.08.25 val PER: 0.0693
2026-01-11 18:15:25,634: t15.2023.08.27 val PER: 0.1688
2026-01-11 18:15:25,634: t15.2023.09.01 val PER: 0.0503
2026-01-11 18:15:25,634: t15.2023.09.03 val PER: 0.1425
2026-01-11 18:15:25,634: t15.2023.09.24 val PER: 0.0922
2026-01-11 18:15:25,634: t15.2023.09.29 val PER: 0.1181
2026-01-11 18:15:25,635: t15.2023.10.01 val PER: 0.1466
2026-01-11 18:15:25,635: t15.2023.10.06 val PER: 0.0667
2026-01-11 18:15:25,635: t15.2023.10.08 val PER: 0.2233
2026-01-11 18:15:25,635: t15.2023.10.13 val PER: 0.1823
2026-01-11 18:15:25,635: t15.2023.10.15 val PER: 0.1285
2026-01-11 18:15:25,635: t15.2023.10.20 val PER: 0.1812
2026-01-11 18:15:25,635: t15.2023.10.22 val PER: 0.1013
2026-01-11 18:15:25,635: t15.2023.11.03 val PER: 0.1669
2026-01-11 18:15:25,635: t15.2023.11.04 val PER: 0.0171
2026-01-11 18:15:25,635: t15.2023.11.17 val PER: 0.0264
2026-01-11 18:15:25,636: t15.2023.11.19 val PER: 0.0220
2026-01-11 18:15:25,636: t15.2023.11.26 val PER: 0.0630
2026-01-11 18:15:25,636: t15.2023.12.03 val PER: 0.0651
2026-01-11 18:15:25,636: t15.2023.12.08 val PER: 0.0526
2026-01-11 18:15:25,636: t15.2023.12.10 val PER: 0.0394
2026-01-11 18:15:25,636: t15.2023.12.17 val PER: 0.0925
2026-01-11 18:15:25,636: t15.2023.12.29 val PER: 0.0879
2026-01-11 18:15:25,636: t15.2024.02.25 val PER: 0.0899
2026-01-11 18:15:25,636: t15.2024.03.08 val PER: 0.1849
2026-01-11 18:15:25,636: t15.2024.03.15 val PER: 0.1801
2026-01-11 18:15:25,637: t15.2024.03.17 val PER: 0.1046
2026-01-11 18:15:25,637: t15.2024.05.10 val PER: 0.1278
2026-01-11 18:15:25,637: t15.2024.06.14 val PER: 0.1420
2026-01-11 18:15:25,637: t15.2024.07.19 val PER: 0.1905
2026-01-11 18:15:25,637: t15.2024.07.21 val PER: 0.0807
2026-01-11 18:15:25,637: t15.2024.07.28 val PER: 0.1088
2026-01-11 18:15:25,637: t15.2025.01.10 val PER: 0.2562
2026-01-11 18:15:25,637: t15.2025.01.12 val PER: 0.1008
2026-01-11 18:15:25,637: t15.2025.03.14 val PER: 0.3107
2026-01-11 18:15:25,637: t15.2025.03.16 val PER: 0.1387
2026-01-11 18:15:25,638: t15.2025.03.30 val PER: 0.2471
2026-01-11 18:15:25,638: t15.2025.04.13 val PER: 0.1812
2026-01-11 18:15:25,796: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_22000
2026-01-11 18:15:47,201: Train batch 22200: loss: 3.88 grad norm: 48.20 time: 0.070
2026-01-11 18:16:07,858: Train batch 22400: loss: 3.21 grad norm: 44.98 time: 0.063
2026-01-11 18:16:18,039: Running test after training batch: 22500
2026-01-11 18:16:18,189: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:16:25,314: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:16:25,382: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:16:41,415: Val batch 22500: PER (avg): 0.1183 CTC Loss (avg): 25.3035 WER(5gram): 10.63% (n=256) time: 23.375
2026-01-11 18:16:41,416: WER lens: avg_true_words=5.99 avg_pred_words=6.06 max_pred_words=12
2026-01-11 18:16:41,416: t15.2023.08.13 val PER: 0.0936
2026-01-11 18:16:41,416: t15.2023.08.18 val PER: 0.0780
2026-01-11 18:16:41,416: t15.2023.08.20 val PER: 0.0786
2026-01-11 18:16:41,416: t15.2023.08.25 val PER: 0.0663
2026-01-11 18:16:41,416: t15.2023.08.27 val PER: 0.1592
2026-01-11 18:16:41,417: t15.2023.09.01 val PER: 0.0584
2026-01-11 18:16:41,417: t15.2023.09.03 val PER: 0.1295
2026-01-11 18:16:41,417: t15.2023.09.24 val PER: 0.0934
2026-01-11 18:16:41,417: t15.2023.09.29 val PER: 0.1123
2026-01-11 18:16:41,417: t15.2023.10.01 val PER: 0.1565
2026-01-11 18:16:41,417: t15.2023.10.06 val PER: 0.0603
2026-01-11 18:16:41,418: t15.2023.10.08 val PER: 0.2260
2026-01-11 18:16:41,418: t15.2023.10.13 val PER: 0.1753
2026-01-11 18:16:41,418: t15.2023.10.15 val PER: 0.1279
2026-01-11 18:16:41,418: t15.2023.10.20 val PER: 0.1812
2026-01-11 18:16:41,418: t15.2023.10.22 val PER: 0.1080
2026-01-11 18:16:41,418: t15.2023.11.03 val PER: 0.1601
2026-01-11 18:16:41,418: t15.2023.11.04 val PER: 0.0171
2026-01-11 18:16:41,418: t15.2023.11.17 val PER: 0.0280
2026-01-11 18:16:41,419: t15.2023.11.19 val PER: 0.0140
2026-01-11 18:16:41,419: t15.2023.11.26 val PER: 0.0543
2026-01-11 18:16:41,419: t15.2023.12.03 val PER: 0.0599
2026-01-11 18:16:41,419: t15.2023.12.08 val PER: 0.0459
2026-01-11 18:16:41,419: t15.2023.12.10 val PER: 0.0368
2026-01-11 18:16:41,419: t15.2023.12.17 val PER: 0.0863
2026-01-11 18:16:41,419: t15.2023.12.29 val PER: 0.0851
2026-01-11 18:16:41,420: t15.2024.02.25 val PER: 0.0941
2026-01-11 18:16:41,420: t15.2024.03.08 val PER: 0.1878
2026-01-11 18:16:41,420: t15.2024.03.15 val PER: 0.1701
2026-01-11 18:16:41,420: t15.2024.03.17 val PER: 0.0990
2026-01-11 18:16:41,420: t15.2024.05.10 val PER: 0.1293
2026-01-11 18:16:41,420: t15.2024.06.14 val PER: 0.1309
2026-01-11 18:16:41,420: t15.2024.07.19 val PER: 0.1885
2026-01-11 18:16:41,421: t15.2024.07.21 val PER: 0.0793
2026-01-11 18:16:41,421: t15.2024.07.28 val PER: 0.1059
2026-01-11 18:16:41,421: t15.2025.01.10 val PER: 0.2590
2026-01-11 18:16:41,421: t15.2025.01.12 val PER: 0.0962
2026-01-11 18:16:41,421: t15.2025.03.14 val PER: 0.3092
2026-01-11 18:16:41,421: t15.2025.03.16 val PER: 0.1427
2026-01-11 18:16:41,421: t15.2025.03.30 val PER: 0.2310
2026-01-11 18:16:41,421: t15.2025.04.13 val PER: 0.2040
2026-01-11 18:16:41,605: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_22500
2026-01-11 18:16:52,166: Train batch 22600: loss: 5.63 grad norm: 55.00 time: 0.072
2026-01-11 18:17:13,218: Train batch 22800: loss: 4.91 grad norm: 55.52 time: 0.068
2026-01-11 18:17:34,762: Train batch 23000: loss: 4.25 grad norm: 51.23 time: 0.074
2026-01-11 18:17:34,762: Running test after training batch: 23000
2026-01-11 18:17:35,224: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:17:42,194: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:17:42,257: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:17:56,613: Val batch 23000: PER (avg): 0.1190 CTC Loss (avg): 25.2793 WER(5gram): 11.67% (n=256) time: 21.851
2026-01-11 18:17:56,614: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 18:17:56,614: t15.2023.08.13 val PER: 0.0832
2026-01-11 18:17:56,615: t15.2023.08.18 val PER: 0.0780
2026-01-11 18:17:56,615: t15.2023.08.20 val PER: 0.0763
2026-01-11 18:17:56,615: t15.2023.08.25 val PER: 0.0663
2026-01-11 18:17:56,615: t15.2023.08.27 val PER: 0.1656
2026-01-11 18:17:56,615: t15.2023.09.01 val PER: 0.0609
2026-01-11 18:17:56,615: t15.2023.09.03 val PER: 0.1318
2026-01-11 18:17:56,615: t15.2023.09.24 val PER: 0.0959
2026-01-11 18:17:56,616: t15.2023.09.29 val PER: 0.1181
2026-01-11 18:17:56,616: t15.2023.10.01 val PER: 0.1592
2026-01-11 18:17:56,616: t15.2023.10.06 val PER: 0.0678
2026-01-11 18:17:56,616: t15.2023.10.08 val PER: 0.2233
2026-01-11 18:17:56,616: t15.2023.10.13 val PER: 0.1823
2026-01-11 18:17:56,616: t15.2023.10.15 val PER: 0.1226
2026-01-11 18:17:56,616: t15.2023.10.20 val PER: 0.2047
2026-01-11 18:17:56,616: t15.2023.10.22 val PER: 0.1013
2026-01-11 18:17:56,616: t15.2023.11.03 val PER: 0.1581
2026-01-11 18:17:56,616: t15.2023.11.04 val PER: 0.0205
2026-01-11 18:17:56,616: t15.2023.11.17 val PER: 0.0249
2026-01-11 18:17:56,616: t15.2023.11.19 val PER: 0.0200
2026-01-11 18:17:56,616: t15.2023.11.26 val PER: 0.0609
2026-01-11 18:17:56,617: t15.2023.12.03 val PER: 0.0609
2026-01-11 18:17:56,617: t15.2023.12.08 val PER: 0.0513
2026-01-11 18:17:56,617: t15.2023.12.10 val PER: 0.0381
2026-01-11 18:17:56,617: t15.2023.12.17 val PER: 0.0863
2026-01-11 18:17:56,617: t15.2023.12.29 val PER: 0.0933
2026-01-11 18:17:56,617: t15.2024.02.25 val PER: 0.0927
2026-01-11 18:17:56,617: t15.2024.03.08 val PER: 0.1920
2026-01-11 18:17:56,617: t15.2024.03.15 val PER: 0.1682
2026-01-11 18:17:56,617: t15.2024.03.17 val PER: 0.0976
2026-01-11 18:17:56,617: t15.2024.05.10 val PER: 0.1263
2026-01-11 18:17:56,618: t15.2024.06.14 val PER: 0.1199
2026-01-11 18:17:56,618: t15.2024.07.19 val PER: 0.1866
2026-01-11 18:17:56,618: t15.2024.07.21 val PER: 0.0759
2026-01-11 18:17:56,618: t15.2024.07.28 val PER: 0.0993
2026-01-11 18:17:56,618: t15.2025.01.10 val PER: 0.2438
2026-01-11 18:17:56,618: t15.2025.01.12 val PER: 0.1016
2026-01-11 18:17:56,618: t15.2025.03.14 val PER: 0.3121
2026-01-11 18:17:56,618: t15.2025.03.16 val PER: 0.1322
2026-01-11 18:17:56,618: t15.2025.03.30 val PER: 0.2494
2026-01-11 18:17:56,618: t15.2025.04.13 val PER: 0.2097
2026-01-11 18:17:56,775: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_23000
2026-01-11 18:18:18,335: Train batch 23200: loss: 3.89 grad norm: 52.57 time: 0.073
2026-01-11 18:18:39,671: Train batch 23400: loss: 1.88 grad norm: 28.13 time: 0.082
2026-01-11 18:18:50,088: Running test after training batch: 23500
2026-01-11 18:18:50,459: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:18:56,872: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:18:56,940: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:19:11,289: Val batch 23500: PER (avg): 0.1194 CTC Loss (avg): 25.7088 WER(5gram): 11.41% (n=256) time: 21.200
2026-01-11 18:19:11,289: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 18:19:11,289: t15.2023.08.13 val PER: 0.0925
2026-01-11 18:19:11,290: t15.2023.08.18 val PER: 0.0796
2026-01-11 18:19:11,290: t15.2023.08.20 val PER: 0.0786
2026-01-11 18:19:11,290: t15.2023.08.25 val PER: 0.0738
2026-01-11 18:19:11,290: t15.2023.08.27 val PER: 0.1592
2026-01-11 18:19:11,290: t15.2023.09.01 val PER: 0.0568
2026-01-11 18:19:11,291: t15.2023.09.03 val PER: 0.1271
2026-01-11 18:19:11,291: t15.2023.09.24 val PER: 0.0910
2026-01-11 18:19:11,291: t15.2023.09.29 val PER: 0.1181
2026-01-11 18:19:11,291: t15.2023.10.01 val PER: 0.1519
2026-01-11 18:19:11,291: t15.2023.10.06 val PER: 0.0667
2026-01-11 18:19:11,291: t15.2023.10.08 val PER: 0.2233
2026-01-11 18:19:11,291: t15.2023.10.13 val PER: 0.1777
2026-01-11 18:19:11,291: t15.2023.10.15 val PER: 0.1246
2026-01-11 18:19:11,291: t15.2023.10.20 val PER: 0.1846
2026-01-11 18:19:11,291: t15.2023.10.22 val PER: 0.1036
2026-01-11 18:19:11,292: t15.2023.11.03 val PER: 0.1642
2026-01-11 18:19:11,292: t15.2023.11.04 val PER: 0.0239
2026-01-11 18:19:11,292: t15.2023.11.17 val PER: 0.0311
2026-01-11 18:19:11,292: t15.2023.11.19 val PER: 0.0200
2026-01-11 18:19:11,292: t15.2023.11.26 val PER: 0.0572
2026-01-11 18:19:11,292: t15.2023.12.03 val PER: 0.0641
2026-01-11 18:19:11,292: t15.2023.12.08 val PER: 0.0566
2026-01-11 18:19:11,293: t15.2023.12.10 val PER: 0.0434
2026-01-11 18:19:11,293: t15.2023.12.17 val PER: 0.0946
2026-01-11 18:19:11,293: t15.2023.12.29 val PER: 0.0879
2026-01-11 18:19:11,293: t15.2024.02.25 val PER: 0.0913
2026-01-11 18:19:11,293: t15.2024.03.08 val PER: 0.1807
2026-01-11 18:19:11,293: t15.2024.03.15 val PER: 0.1645
2026-01-11 18:19:11,293: t15.2024.03.17 val PER: 0.1004
2026-01-11 18:19:11,293: t15.2024.05.10 val PER: 0.1263
2026-01-11 18:19:11,293: t15.2024.06.14 val PER: 0.1309
2026-01-11 18:19:11,294: t15.2024.07.19 val PER: 0.1872
2026-01-11 18:19:11,294: t15.2024.07.21 val PER: 0.0800
2026-01-11 18:19:11,294: t15.2024.07.28 val PER: 0.1110
2026-01-11 18:19:11,294: t15.2025.01.10 val PER: 0.2534
2026-01-11 18:19:11,294: t15.2025.01.12 val PER: 0.1024
2026-01-11 18:19:11,294: t15.2025.03.14 val PER: 0.3077
2026-01-11 18:19:11,294: t15.2025.03.16 val PER: 0.1361
2026-01-11 18:19:11,294: t15.2025.03.30 val PER: 0.2437
2026-01-11 18:19:11,294: t15.2025.04.13 val PER: 0.1926
2026-01-11 18:19:11,451: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_23500
2026-01-11 18:19:21,840: Train batch 23600: loss: 1.23 grad norm: 21.82 time: 0.067
2026-01-11 18:19:42,694: Train batch 23800: loss: 3.93 grad norm: 62.67 time: 0.066
2026-01-11 18:20:04,229: Train batch 24000: loss: 3.12 grad norm: 44.47 time: 0.092
2026-01-11 18:20:04,229: Running test after training batch: 24000
2026-01-11 18:20:04,362: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:20:11,058: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:20:11,121: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:20:26,613: Val batch 24000: PER (avg): 0.1173 CTC Loss (avg): 25.6425 WER(5gram): 12.06% (n=256) time: 22.383
2026-01-11 18:20:26,614: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-11 18:20:26,614: t15.2023.08.13 val PER: 0.0852
2026-01-11 18:20:26,614: t15.2023.08.18 val PER: 0.0780
2026-01-11 18:20:26,614: t15.2023.08.20 val PER: 0.0770
2026-01-11 18:20:26,615: t15.2023.08.25 val PER: 0.0753
2026-01-11 18:20:26,615: t15.2023.08.27 val PER: 0.1576
2026-01-11 18:20:26,615: t15.2023.09.01 val PER: 0.0495
2026-01-11 18:20:26,615: t15.2023.09.03 val PER: 0.1271
2026-01-11 18:20:26,615: t15.2023.09.24 val PER: 0.0922
2026-01-11 18:20:26,615: t15.2023.09.29 val PER: 0.1104
2026-01-11 18:20:26,615: t15.2023.10.01 val PER: 0.1499
2026-01-11 18:20:26,615: t15.2023.10.06 val PER: 0.0646
2026-01-11 18:20:26,615: t15.2023.10.08 val PER: 0.2219
2026-01-11 18:20:26,615: t15.2023.10.13 val PER: 0.1893
2026-01-11 18:20:26,616: t15.2023.10.15 val PER: 0.1173
2026-01-11 18:20:26,616: t15.2023.10.20 val PER: 0.1879
2026-01-11 18:20:26,616: t15.2023.10.22 val PER: 0.1002
2026-01-11 18:20:26,616: t15.2023.11.03 val PER: 0.1608
2026-01-11 18:20:26,616: t15.2023.11.04 val PER: 0.0171
2026-01-11 18:20:26,616: t15.2023.11.17 val PER: 0.0295
2026-01-11 18:20:26,616: t15.2023.11.19 val PER: 0.0120
2026-01-11 18:20:26,616: t15.2023.11.26 val PER: 0.0587
2026-01-11 18:20:26,616: t15.2023.12.03 val PER: 0.0588
2026-01-11 18:20:26,617: t15.2023.12.08 val PER: 0.0433
2026-01-11 18:20:26,617: t15.2023.12.10 val PER: 0.0407
2026-01-11 18:20:26,617: t15.2023.12.17 val PER: 0.0904
2026-01-11 18:20:26,617: t15.2023.12.29 val PER: 0.0865
2026-01-11 18:20:26,617: t15.2024.02.25 val PER: 0.0815
2026-01-11 18:20:26,617: t15.2024.03.08 val PER: 0.1892
2026-01-11 18:20:26,617: t15.2024.03.15 val PER: 0.1689
2026-01-11 18:20:26,617: t15.2024.03.17 val PER: 0.1004
2026-01-11 18:20:26,617: t15.2024.05.10 val PER: 0.1248
2026-01-11 18:20:26,617: t15.2024.06.14 val PER: 0.1388
2026-01-11 18:20:26,618: t15.2024.07.19 val PER: 0.1813
2026-01-11 18:20:26,618: t15.2024.07.21 val PER: 0.0779
2026-01-11 18:20:26,618: t15.2024.07.28 val PER: 0.1081
2026-01-11 18:20:26,618: t15.2025.01.10 val PER: 0.2466
2026-01-11 18:20:26,618: t15.2025.01.12 val PER: 0.1039
2026-01-11 18:20:26,618: t15.2025.03.14 val PER: 0.2959
2026-01-11 18:20:26,618: t15.2025.03.16 val PER: 0.1374
2026-01-11 18:20:26,619: t15.2025.03.30 val PER: 0.2483
2026-01-11 18:20:26,619: t15.2025.04.13 val PER: 0.2011
2026-01-11 18:20:26,780: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_24000
2026-01-11 18:20:48,414: Train batch 24200: loss: 3.10 grad norm: 53.98 time: 0.066
2026-01-11 18:21:09,119: Train batch 24400: loss: 5.61 grad norm: 58.72 time: 0.065
2026-01-11 18:21:19,225: Running test after training batch: 24500
2026-01-11 18:21:19,345: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:21:26,466: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:21:26,576: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:21:41,399: Val batch 24500: PER (avg): 0.1164 CTC Loss (avg): 25.5141 WER(5gram): 12.39% (n=256) time: 22.173
2026-01-11 18:21:41,399: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-11 18:21:41,400: t15.2023.08.13 val PER: 0.0780
2026-01-11 18:21:41,400: t15.2023.08.18 val PER: 0.0821
2026-01-11 18:21:41,400: t15.2023.08.20 val PER: 0.0786
2026-01-11 18:21:41,400: t15.2023.08.25 val PER: 0.0648
2026-01-11 18:21:41,400: t15.2023.08.27 val PER: 0.1672
2026-01-11 18:21:41,400: t15.2023.09.01 val PER: 0.0455
2026-01-11 18:21:41,400: t15.2023.09.03 val PER: 0.1378
2026-01-11 18:21:41,400: t15.2023.09.24 val PER: 0.0959
2026-01-11 18:21:41,400: t15.2023.09.29 val PER: 0.1181
2026-01-11 18:21:41,400: t15.2023.10.01 val PER: 0.1513
2026-01-11 18:21:41,401: t15.2023.10.06 val PER: 0.0592
2026-01-11 18:21:41,401: t15.2023.10.08 val PER: 0.2219
2026-01-11 18:21:41,401: t15.2023.10.13 val PER: 0.1753
2026-01-11 18:21:41,401: t15.2023.10.15 val PER: 0.1206
2026-01-11 18:21:41,401: t15.2023.10.20 val PER: 0.1846
2026-01-11 18:21:41,401: t15.2023.10.22 val PER: 0.0924
2026-01-11 18:21:41,402: t15.2023.11.03 val PER: 0.1601
2026-01-11 18:21:41,402: t15.2023.11.04 val PER: 0.0205
2026-01-11 18:21:41,402: t15.2023.11.17 val PER: 0.0233
2026-01-11 18:21:41,402: t15.2023.11.19 val PER: 0.0220
2026-01-11 18:21:41,402: t15.2023.11.26 val PER: 0.0594
2026-01-11 18:21:41,402: t15.2023.12.03 val PER: 0.0546
2026-01-11 18:21:41,403: t15.2023.12.08 val PER: 0.0466
2026-01-11 18:21:41,403: t15.2023.12.10 val PER: 0.0447
2026-01-11 18:21:41,403: t15.2023.12.17 val PER: 0.0967
2026-01-11 18:21:41,403: t15.2023.12.29 val PER: 0.0906
2026-01-11 18:21:41,403: t15.2024.02.25 val PER: 0.0941
2026-01-11 18:21:41,403: t15.2024.03.08 val PER: 0.1963
2026-01-11 18:21:41,403: t15.2024.03.15 val PER: 0.1732
2026-01-11 18:21:41,403: t15.2024.03.17 val PER: 0.0879
2026-01-11 18:21:41,403: t15.2024.05.10 val PER: 0.1263
2026-01-11 18:21:41,404: t15.2024.06.14 val PER: 0.1278
2026-01-11 18:21:41,404: t15.2024.07.19 val PER: 0.1892
2026-01-11 18:21:41,404: t15.2024.07.21 val PER: 0.0683
2026-01-11 18:21:41,404: t15.2024.07.28 val PER: 0.0993
2026-01-11 18:21:41,404: t15.2025.01.10 val PER: 0.2452
2026-01-11 18:21:41,404: t15.2025.01.12 val PER: 0.1055
2026-01-11 18:21:41,404: t15.2025.03.14 val PER: 0.2870
2026-01-11 18:21:41,404: t15.2025.03.16 val PER: 0.1283
2026-01-11 18:21:41,404: t15.2025.03.30 val PER: 0.2276
2026-01-11 18:21:41,405: t15.2025.04.13 val PER: 0.1926
2026-01-11 18:21:41,564: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_24500
2026-01-11 18:21:52,013: Train batch 24600: loss: 3.39 grad norm: 48.13 time: 0.069
2026-01-11 18:22:12,885: Train batch 24800: loss: 3.95 grad norm: 53.25 time: 0.091
2026-01-11 18:22:33,419: Train batch 25000: loss: 3.50 grad norm: 45.91 time: 0.078
2026-01-11 18:22:33,420: Running test after training batch: 25000
2026-01-11 18:22:33,733: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:22:40,354: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:22:40,418: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:22:55,131: Val batch 25000: PER (avg): 0.1170 CTC Loss (avg): 26.3296 WER(5gram): 13.10% (n=256) time: 21.712
2026-01-11 18:22:55,132: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-11 18:22:55,132: t15.2023.08.13 val PER: 0.0925
2026-01-11 18:22:55,132: t15.2023.08.18 val PER: 0.0704
2026-01-11 18:22:55,132: t15.2023.08.20 val PER: 0.0699
2026-01-11 18:22:55,133: t15.2023.08.25 val PER: 0.0753
2026-01-11 18:22:55,133: t15.2023.08.27 val PER: 0.1704
2026-01-11 18:22:55,133: t15.2023.09.01 val PER: 0.0584
2026-01-11 18:22:55,133: t15.2023.09.03 val PER: 0.1188
2026-01-11 18:22:55,133: t15.2023.09.24 val PER: 0.0886
2026-01-11 18:22:55,133: t15.2023.09.29 val PER: 0.1130
2026-01-11 18:22:55,133: t15.2023.10.01 val PER: 0.1453
2026-01-11 18:22:55,133: t15.2023.10.06 val PER: 0.0635
2026-01-11 18:22:55,133: t15.2023.10.08 val PER: 0.2111
2026-01-11 18:22:55,133: t15.2023.10.13 val PER: 0.1761
2026-01-11 18:22:55,133: t15.2023.10.15 val PER: 0.1200
2026-01-11 18:22:55,134: t15.2023.10.20 val PER: 0.1779
2026-01-11 18:22:55,134: t15.2023.10.22 val PER: 0.0958
2026-01-11 18:22:55,134: t15.2023.11.03 val PER: 0.1615
2026-01-11 18:22:55,134: t15.2023.11.04 val PER: 0.0171
2026-01-11 18:22:55,134: t15.2023.11.17 val PER: 0.0264
2026-01-11 18:22:55,134: t15.2023.11.19 val PER: 0.0120
2026-01-11 18:22:55,134: t15.2023.11.26 val PER: 0.0601
2026-01-11 18:22:55,135: t15.2023.12.03 val PER: 0.0620
2026-01-11 18:22:55,135: t15.2023.12.08 val PER: 0.0466
2026-01-11 18:22:55,135: t15.2023.12.10 val PER: 0.0473
2026-01-11 18:22:55,135: t15.2023.12.17 val PER: 0.0873
2026-01-11 18:22:55,135: t15.2023.12.29 val PER: 0.0920
2026-01-11 18:22:55,135: t15.2024.02.25 val PER: 0.0801
2026-01-11 18:22:55,135: t15.2024.03.08 val PER: 0.1764
2026-01-11 18:22:55,135: t15.2024.03.15 val PER: 0.1701
2026-01-11 18:22:55,136: t15.2024.03.17 val PER: 0.1018
2026-01-11 18:22:55,136: t15.2024.05.10 val PER: 0.1322
2026-01-11 18:22:55,136: t15.2024.06.14 val PER: 0.1309
2026-01-11 18:22:55,136: t15.2024.07.19 val PER: 0.1839
2026-01-11 18:22:55,136: t15.2024.07.21 val PER: 0.0655
2026-01-11 18:22:55,136: t15.2024.07.28 val PER: 0.1037
2026-01-11 18:22:55,136: t15.2025.01.10 val PER: 0.2645
2026-01-11 18:22:55,136: t15.2025.01.12 val PER: 0.1178
2026-01-11 18:22:55,137: t15.2025.03.14 val PER: 0.3077
2026-01-11 18:22:55,137: t15.2025.03.16 val PER: 0.1348
2026-01-11 18:22:55,137: t15.2025.03.30 val PER: 0.2414
2026-01-11 18:22:55,137: t15.2025.04.13 val PER: 0.1940
2026-01-11 18:22:55,298: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_25000
2026-01-11 18:23:15,895: Train batch 25200: loss: 2.23 grad norm: 41.80 time: 0.068
2026-01-11 18:23:36,281: Train batch 25400: loss: 2.64 grad norm: 55.03 time: 0.067
2026-01-11 18:23:46,994: Running test after training batch: 25500
2026-01-11 18:23:47,109: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:23:53,536: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:23:53,609: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:24:08,648: Val batch 25500: PER (avg): 0.1163 CTC Loss (avg): 26.4397 WER(5gram): 10.23% (n=256) time: 21.653
2026-01-11 18:24:08,649: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-11 18:24:08,649: t15.2023.08.13 val PER: 0.0790
2026-01-11 18:24:08,649: t15.2023.08.18 val PER: 0.0754
2026-01-11 18:24:08,649: t15.2023.08.20 val PER: 0.0683
2026-01-11 18:24:08,650: t15.2023.08.25 val PER: 0.0768
2026-01-11 18:24:08,650: t15.2023.08.27 val PER: 0.1543
2026-01-11 18:24:08,650: t15.2023.09.01 val PER: 0.0487
2026-01-11 18:24:08,650: t15.2023.09.03 val PER: 0.1401
2026-01-11 18:24:08,650: t15.2023.09.24 val PER: 0.0898
2026-01-11 18:24:08,650: t15.2023.09.29 val PER: 0.1117
2026-01-11 18:24:08,650: t15.2023.10.01 val PER: 0.1460
2026-01-11 18:24:08,650: t15.2023.10.06 val PER: 0.0689
2026-01-11 18:24:08,650: t15.2023.10.08 val PER: 0.2124
2026-01-11 18:24:08,651: t15.2023.10.13 val PER: 0.1823
2026-01-11 18:24:08,651: t15.2023.10.15 val PER: 0.1187
2026-01-11 18:24:08,651: t15.2023.10.20 val PER: 0.1779
2026-01-11 18:24:08,651: t15.2023.10.22 val PER: 0.0947
2026-01-11 18:24:08,651: t15.2023.11.03 val PER: 0.1703
2026-01-11 18:24:08,651: t15.2023.11.04 val PER: 0.0239
2026-01-11 18:24:08,651: t15.2023.11.17 val PER: 0.0218
2026-01-11 18:24:08,652: t15.2023.11.19 val PER: 0.0180
2026-01-11 18:24:08,652: t15.2023.11.26 val PER: 0.0558
2026-01-11 18:24:08,652: t15.2023.12.03 val PER: 0.0672
2026-01-11 18:24:08,652: t15.2023.12.08 val PER: 0.0506
2026-01-11 18:24:08,652: t15.2023.12.10 val PER: 0.0434
2026-01-11 18:24:08,652: t15.2023.12.17 val PER: 0.0977
2026-01-11 18:24:08,652: t15.2023.12.29 val PER: 0.0885
2026-01-11 18:24:08,653: t15.2024.02.25 val PER: 0.0801
2026-01-11 18:24:08,653: t15.2024.03.08 val PER: 0.1764
2026-01-11 18:24:08,653: t15.2024.03.15 val PER: 0.1676
2026-01-11 18:24:08,653: t15.2024.03.17 val PER: 0.0969
2026-01-11 18:24:08,653: t15.2024.05.10 val PER: 0.1293
2026-01-11 18:24:08,653: t15.2024.06.14 val PER: 0.1388
2026-01-11 18:24:08,653: t15.2024.07.19 val PER: 0.1846
2026-01-11 18:24:08,653: t15.2024.07.21 val PER: 0.0662
2026-01-11 18:24:08,654: t15.2024.07.28 val PER: 0.1066
2026-01-11 18:24:08,654: t15.2025.01.10 val PER: 0.2493
2026-01-11 18:24:08,654: t15.2025.01.12 val PER: 0.0985
2026-01-11 18:24:08,654: t15.2025.03.14 val PER: 0.3092
2026-01-11 18:24:08,654: t15.2025.03.16 val PER: 0.1374
2026-01-11 18:24:08,654: t15.2025.03.30 val PER: 0.2264
2026-01-11 18:24:08,654: t15.2025.04.13 val PER: 0.1940
2026-01-11 18:24:08,813: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_25500
2026-01-11 18:24:19,125: Train batch 25600: loss: 2.88 grad norm: 46.46 time: 0.069
2026-01-11 18:24:40,051: Train batch 25800: loss: 1.98 grad norm: 34.76 time: 0.073
2026-01-11 18:25:01,267: Train batch 26000: loss: 1.86 grad norm: 36.74 time: 0.073
2026-01-11 18:25:01,267: Running test after training batch: 26000
2026-01-11 18:25:01,438: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:25:08,447: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:25:08,526: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:25:24,453: Val batch 26000: PER (avg): 0.1183 CTC Loss (avg): 26.5825 WER(5gram): 11.54% (n=256) time: 23.185
2026-01-11 18:25:24,454: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 18:25:24,454: t15.2023.08.13 val PER: 0.0936
2026-01-11 18:25:24,454: t15.2023.08.18 val PER: 0.0796
2026-01-11 18:25:24,454: t15.2023.08.20 val PER: 0.0707
2026-01-11 18:25:24,454: t15.2023.08.25 val PER: 0.0693
2026-01-11 18:25:24,454: t15.2023.08.27 val PER: 0.1559
2026-01-11 18:25:24,454: t15.2023.09.01 val PER: 0.0495
2026-01-11 18:25:24,454: t15.2023.09.03 val PER: 0.1223
2026-01-11 18:25:24,455: t15.2023.09.24 val PER: 0.0886
2026-01-11 18:25:24,455: t15.2023.09.29 val PER: 0.1136
2026-01-11 18:25:24,455: t15.2023.10.01 val PER: 0.1519
2026-01-11 18:25:24,455: t15.2023.10.06 val PER: 0.0614
2026-01-11 18:25:24,455: t15.2023.10.08 val PER: 0.2273
2026-01-11 18:25:24,455: t15.2023.10.13 val PER: 0.1854
2026-01-11 18:25:24,455: t15.2023.10.15 val PER: 0.1187
2026-01-11 18:25:24,456: t15.2023.10.20 val PER: 0.1879
2026-01-11 18:25:24,456: t15.2023.10.22 val PER: 0.1036
2026-01-11 18:25:24,456: t15.2023.11.03 val PER: 0.1608
2026-01-11 18:25:24,456: t15.2023.11.04 val PER: 0.0273
2026-01-11 18:25:24,456: t15.2023.11.17 val PER: 0.0218
2026-01-11 18:25:24,456: t15.2023.11.19 val PER: 0.0200
2026-01-11 18:25:24,456: t15.2023.11.26 val PER: 0.0543
2026-01-11 18:25:24,456: t15.2023.12.03 val PER: 0.0567
2026-01-11 18:25:24,457: t15.2023.12.08 val PER: 0.0479
2026-01-11 18:25:24,457: t15.2023.12.10 val PER: 0.0394
2026-01-11 18:25:24,457: t15.2023.12.17 val PER: 0.0977
2026-01-11 18:25:24,457: t15.2023.12.29 val PER: 0.0789
2026-01-11 18:25:24,457: t15.2024.02.25 val PER: 0.0857
2026-01-11 18:25:24,457: t15.2024.03.08 val PER: 0.1949
2026-01-11 18:25:24,457: t15.2024.03.15 val PER: 0.1707
2026-01-11 18:25:24,457: t15.2024.03.17 val PER: 0.0990
2026-01-11 18:25:24,457: t15.2024.05.10 val PER: 0.1322
2026-01-11 18:25:24,458: t15.2024.06.14 val PER: 0.1215
2026-01-11 18:25:24,458: t15.2024.07.19 val PER: 0.1931
2026-01-11 18:25:24,458: t15.2024.07.21 val PER: 0.0772
2026-01-11 18:25:24,458: t15.2024.07.28 val PER: 0.1059
2026-01-11 18:25:24,458: t15.2025.01.10 val PER: 0.2686
2026-01-11 18:25:24,458: t15.2025.01.12 val PER: 0.1109
2026-01-11 18:25:24,458: t15.2025.03.14 val PER: 0.3107
2026-01-11 18:25:24,458: t15.2025.03.16 val PER: 0.1401
2026-01-11 18:25:24,459: t15.2025.03.30 val PER: 0.2345
2026-01-11 18:25:24,459: t15.2025.04.13 val PER: 0.2040
2026-01-11 18:25:24,616: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_26000
2026-01-11 18:25:45,611: Train batch 26200: loss: 1.27 grad norm: 30.03 time: 0.073
2026-01-11 18:26:07,126: Train batch 26400: loss: 3.74 grad norm: 48.74 time: 0.090
2026-01-11 18:26:17,646: Running test after training batch: 26500
2026-01-11 18:26:17,764: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:26:25,386: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:26:25,452: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cent
2026-01-11 18:26:40,990: Val batch 26500: PER (avg): 0.1164 CTC Loss (avg): 26.0655 WER(5gram): 10.63% (n=256) time: 23.343
2026-01-11 18:26:40,990: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-11 18:26:40,991: t15.2023.08.13 val PER: 0.0884
2026-01-11 18:26:40,991: t15.2023.08.18 val PER: 0.0704
2026-01-11 18:26:40,991: t15.2023.08.20 val PER: 0.0691
2026-01-11 18:26:40,991: t15.2023.08.25 val PER: 0.0813
2026-01-11 18:26:40,991: t15.2023.08.27 val PER: 0.1511
2026-01-11 18:26:40,992: t15.2023.09.01 val PER: 0.0487
2026-01-11 18:26:40,992: t15.2023.09.03 val PER: 0.1235
2026-01-11 18:26:40,992: t15.2023.09.24 val PER: 0.0910
2026-01-11 18:26:40,992: t15.2023.09.29 val PER: 0.1059
2026-01-11 18:26:40,992: t15.2023.10.01 val PER: 0.1513
2026-01-11 18:26:40,992: t15.2023.10.06 val PER: 0.0721
2026-01-11 18:26:40,992: t15.2023.10.08 val PER: 0.2219
2026-01-11 18:26:40,992: t15.2023.10.13 val PER: 0.1769
2026-01-11 18:26:40,993: t15.2023.10.15 val PER: 0.1167
2026-01-11 18:26:40,993: t15.2023.10.20 val PER: 0.1846
2026-01-11 18:26:40,993: t15.2023.10.22 val PER: 0.0991
2026-01-11 18:26:40,993: t15.2023.11.03 val PER: 0.1588
2026-01-11 18:26:40,993: t15.2023.11.04 val PER: 0.0273
2026-01-11 18:26:40,993: t15.2023.11.17 val PER: 0.0311
2026-01-11 18:26:40,993: t15.2023.11.19 val PER: 0.0200
2026-01-11 18:26:40,994: t15.2023.11.26 val PER: 0.0558
2026-01-11 18:26:40,994: t15.2023.12.03 val PER: 0.0651
2026-01-11 18:26:40,994: t15.2023.12.08 val PER: 0.0513
2026-01-11 18:26:40,994: t15.2023.12.10 val PER: 0.0460
2026-01-11 18:26:40,994: t15.2023.12.17 val PER: 0.0956
2026-01-11 18:26:40,994: t15.2023.12.29 val PER: 0.0830
2026-01-11 18:26:40,994: t15.2024.02.25 val PER: 0.0801
2026-01-11 18:26:40,994: t15.2024.03.08 val PER: 0.1892
2026-01-11 18:26:40,994: t15.2024.03.15 val PER: 0.1701
2026-01-11 18:26:40,995: t15.2024.03.17 val PER: 0.0969
2026-01-11 18:26:40,995: t15.2024.05.10 val PER: 0.1308
2026-01-11 18:26:40,995: t15.2024.06.14 val PER: 0.1356
2026-01-11 18:26:40,995: t15.2024.07.19 val PER: 0.1773
2026-01-11 18:26:40,995: t15.2024.07.21 val PER: 0.0662
2026-01-11 18:26:40,995: t15.2024.07.28 val PER: 0.1103
2026-01-11 18:26:40,995: t15.2025.01.10 val PER: 0.2617
2026-01-11 18:26:40,995: t15.2025.01.12 val PER: 0.1039
2026-01-11 18:26:40,996: t15.2025.03.14 val PER: 0.3092
2026-01-11 18:26:40,996: t15.2025.03.16 val PER: 0.1427
2026-01-11 18:26:40,996: t15.2025.03.30 val PER: 0.2253
2026-01-11 18:26:40,996: t15.2025.04.13 val PER: 0.1897
2026-01-11 18:26:41,158: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_26500
2026-01-11 18:26:51,186: Train batch 26600: loss: 1.30 grad norm: 31.30 time: 0.069
2026-01-11 18:27:11,693: Train batch 26800: loss: 4.51 grad norm: 56.62 time: 0.092
2026-01-11 18:27:32,508: Train batch 27000: loss: 3.54 grad norm: 49.00 time: 0.074
2026-01-11 18:27:32,508: Running test after training batch: 27000
2026-01-11 18:27:32,690: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:27:38,881: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:27:38,953: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:27:53,654: Val batch 27000: PER (avg): 0.1147 CTC Loss (avg): 26.5367 WER(5gram): 11.34% (n=256) time: 21.145
2026-01-11 18:27:53,654: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-11 18:27:53,655: t15.2023.08.13 val PER: 0.0842
2026-01-11 18:27:53,655: t15.2023.08.18 val PER: 0.0738
2026-01-11 18:27:53,655: t15.2023.08.20 val PER: 0.0699
2026-01-11 18:27:53,655: t15.2023.08.25 val PER: 0.0723
2026-01-11 18:27:53,655: t15.2023.08.27 val PER: 0.1479
2026-01-11 18:27:53,655: t15.2023.09.01 val PER: 0.0511
2026-01-11 18:27:53,655: t15.2023.09.03 val PER: 0.1259
2026-01-11 18:27:53,655: t15.2023.09.24 val PER: 0.0874
2026-01-11 18:27:53,656: t15.2023.09.29 val PER: 0.1098
2026-01-11 18:27:53,656: t15.2023.10.01 val PER: 0.1559
2026-01-11 18:27:53,656: t15.2023.10.06 val PER: 0.0592
2026-01-11 18:27:53,656: t15.2023.10.08 val PER: 0.2152
2026-01-11 18:27:53,656: t15.2023.10.13 val PER: 0.1753
2026-01-11 18:27:53,656: t15.2023.10.15 val PER: 0.1173
2026-01-11 18:27:53,656: t15.2023.10.20 val PER: 0.1779
2026-01-11 18:27:53,657: t15.2023.10.22 val PER: 0.0935
2026-01-11 18:27:53,657: t15.2023.11.03 val PER: 0.1533
2026-01-11 18:27:53,657: t15.2023.11.04 val PER: 0.0205
2026-01-11 18:27:53,657: t15.2023.11.17 val PER: 0.0233
2026-01-11 18:27:53,657: t15.2023.11.19 val PER: 0.0120
2026-01-11 18:27:53,657: t15.2023.11.26 val PER: 0.0572
2026-01-11 18:27:53,657: t15.2023.12.03 val PER: 0.0641
2026-01-11 18:27:53,657: t15.2023.12.08 val PER: 0.0419
2026-01-11 18:27:53,657: t15.2023.12.10 val PER: 0.0460
2026-01-11 18:27:53,658: t15.2023.12.17 val PER: 0.0915
2026-01-11 18:27:53,658: t15.2023.12.29 val PER: 0.0879
2026-01-11 18:27:53,658: t15.2024.02.25 val PER: 0.0927
2026-01-11 18:27:53,658: t15.2024.03.08 val PER: 0.1991
2026-01-11 18:27:53,658: t15.2024.03.15 val PER: 0.1670
2026-01-11 18:27:53,658: t15.2024.03.17 val PER: 0.0976
2026-01-11 18:27:53,658: t15.2024.05.10 val PER: 0.1322
2026-01-11 18:27:53,659: t15.2024.06.14 val PER: 0.1230
2026-01-11 18:27:53,659: t15.2024.07.19 val PER: 0.1740
2026-01-11 18:27:53,659: t15.2024.07.21 val PER: 0.0697
2026-01-11 18:27:53,659: t15.2024.07.28 val PER: 0.0971
2026-01-11 18:27:53,659: t15.2025.01.10 val PER: 0.2672
2026-01-11 18:27:53,659: t15.2025.01.12 val PER: 0.0901
2026-01-11 18:27:53,659: t15.2025.03.14 val PER: 0.3107
2026-01-11 18:27:53,659: t15.2025.03.16 val PER: 0.1414
2026-01-11 18:27:53,659: t15.2025.03.30 val PER: 0.2241
2026-01-11 18:27:53,659: t15.2025.04.13 val PER: 0.1969
2026-01-11 18:27:53,816: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_27000
2026-01-11 18:28:14,552: Train batch 27200: loss: 3.61 grad norm: 53.03 time: 0.065
2026-01-11 18:28:35,972: Train batch 27400: loss: 3.89 grad norm: 49.41 time: 0.067
2026-01-11 18:28:47,003: Running test after training batch: 27500
2026-01-11 18:28:47,229: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:28:53,943: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:28:54,023: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:29:08,633: Val batch 27500: PER (avg): 0.1160 CTC Loss (avg): 27.0191 WER(5gram): 11.08% (n=256) time: 21.630
2026-01-11 18:29:08,633: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 18:29:08,634: t15.2023.08.13 val PER: 0.0946
2026-01-11 18:29:08,634: t15.2023.08.18 val PER: 0.0821
2026-01-11 18:29:08,634: t15.2023.08.20 val PER: 0.0786
2026-01-11 18:29:08,634: t15.2023.08.25 val PER: 0.0753
2026-01-11 18:29:08,634: t15.2023.08.27 val PER: 0.1881
2026-01-11 18:29:08,634: t15.2023.09.01 val PER: 0.0414
2026-01-11 18:29:08,634: t15.2023.09.03 val PER: 0.1259
2026-01-11 18:29:08,634: t15.2023.09.24 val PER: 0.0898
2026-01-11 18:29:08,635: t15.2023.09.29 val PER: 0.1110
2026-01-11 18:29:08,635: t15.2023.10.01 val PER: 0.1565
2026-01-11 18:29:08,635: t15.2023.10.06 val PER: 0.0603
2026-01-11 18:29:08,635: t15.2023.10.08 val PER: 0.2206
2026-01-11 18:29:08,635: t15.2023.10.13 val PER: 0.1722
2026-01-11 18:29:08,635: t15.2023.10.15 val PER: 0.1154
2026-01-11 18:29:08,635: t15.2023.10.20 val PER: 0.1812
2026-01-11 18:29:08,635: t15.2023.10.22 val PER: 0.1002
2026-01-11 18:29:08,635: t15.2023.11.03 val PER: 0.1588
2026-01-11 18:29:08,636: t15.2023.11.04 val PER: 0.0239
2026-01-11 18:29:08,636: t15.2023.11.17 val PER: 0.0264
2026-01-11 18:29:08,636: t15.2023.11.19 val PER: 0.0100
2026-01-11 18:29:08,636: t15.2023.11.26 val PER: 0.0536
2026-01-11 18:29:08,636: t15.2023.12.03 val PER: 0.0588
2026-01-11 18:29:08,636: t15.2023.12.08 val PER: 0.0473
2026-01-11 18:29:08,636: t15.2023.12.10 val PER: 0.0368
2026-01-11 18:29:08,636: t15.2023.12.17 val PER: 0.0946
2026-01-11 18:29:08,636: t15.2023.12.29 val PER: 0.0920
2026-01-11 18:29:08,636: t15.2024.02.25 val PER: 0.0815
2026-01-11 18:29:08,636: t15.2024.03.08 val PER: 0.1707
2026-01-11 18:29:08,637: t15.2024.03.15 val PER: 0.1701
2026-01-11 18:29:08,637: t15.2024.03.17 val PER: 0.0969
2026-01-11 18:29:08,637: t15.2024.05.10 val PER: 0.1189
2026-01-11 18:29:08,637: t15.2024.06.14 val PER: 0.1293
2026-01-11 18:29:08,637: t15.2024.07.19 val PER: 0.1839
2026-01-11 18:29:08,637: t15.2024.07.21 val PER: 0.0662
2026-01-11 18:29:08,637: t15.2024.07.28 val PER: 0.1088
2026-01-11 18:29:08,637: t15.2025.01.10 val PER: 0.2534
2026-01-11 18:29:08,638: t15.2025.01.12 val PER: 0.0947
2026-01-11 18:29:08,638: t15.2025.03.14 val PER: 0.3240
2026-01-11 18:29:08,638: t15.2025.03.16 val PER: 0.1401
2026-01-11 18:29:08,638: t15.2025.03.30 val PER: 0.2172
2026-01-11 18:29:08,638: t15.2025.04.13 val PER: 0.1854
2026-01-11 18:29:08,798: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_27500
2026-01-11 18:29:18,977: Train batch 27600: loss: 6.88 grad norm: 53.54 time: 0.074
2026-01-11 18:29:39,985: Train batch 27800: loss: 3.55 grad norm: 50.62 time: 0.050
2026-01-11 18:30:00,790: Train batch 28000: loss: 2.32 grad norm: 57.48 time: 0.064
2026-01-11 18:30:00,790: Running test after training batch: 28000
2026-01-11 18:30:00,992: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:30:07,839: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:30:07,942: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost said
2026-01-11 18:30:23,682: Val batch 28000: PER (avg): 0.1155 CTC Loss (avg): 27.1661 WER(5gram): 12.84% (n=256) time: 22.891
2026-01-11 18:30:23,683: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-11 18:30:23,683: t15.2023.08.13 val PER: 0.0873
2026-01-11 18:30:23,683: t15.2023.08.18 val PER: 0.0771
2026-01-11 18:30:23,683: t15.2023.08.20 val PER: 0.0715
2026-01-11 18:30:23,683: t15.2023.08.25 val PER: 0.0768
2026-01-11 18:30:23,684: t15.2023.08.27 val PER: 0.1801
2026-01-11 18:30:23,684: t15.2023.09.01 val PER: 0.0495
2026-01-11 18:30:23,684: t15.2023.09.03 val PER: 0.1247
2026-01-11 18:30:23,684: t15.2023.09.24 val PER: 0.0934
2026-01-11 18:30:23,684: t15.2023.09.29 val PER: 0.1130
2026-01-11 18:30:23,684: t15.2023.10.01 val PER: 0.1466
2026-01-11 18:30:23,684: t15.2023.10.06 val PER: 0.0614
2026-01-11 18:30:23,684: t15.2023.10.08 val PER: 0.2233
2026-01-11 18:30:23,684: t15.2023.10.13 val PER: 0.1815
2026-01-11 18:30:23,685: t15.2023.10.15 val PER: 0.1180
2026-01-11 18:30:23,685: t15.2023.10.20 val PER: 0.1812
2026-01-11 18:30:23,685: t15.2023.10.22 val PER: 0.1047
2026-01-11 18:30:23,685: t15.2023.11.03 val PER: 0.1554
2026-01-11 18:30:23,686: t15.2023.11.04 val PER: 0.0205
2026-01-11 18:30:23,686: t15.2023.11.17 val PER: 0.0249
2026-01-11 18:30:23,687: t15.2023.11.19 val PER: 0.0120
2026-01-11 18:30:23,687: t15.2023.11.26 val PER: 0.0536
2026-01-11 18:30:23,687: t15.2023.12.03 val PER: 0.0578
2026-01-11 18:30:23,687: t15.2023.12.08 val PER: 0.0526
2026-01-11 18:30:23,688: t15.2023.12.10 val PER: 0.0342
2026-01-11 18:30:23,688: t15.2023.12.17 val PER: 0.1008
2026-01-11 18:30:23,688: t15.2023.12.29 val PER: 0.0789
2026-01-11 18:30:23,688: t15.2024.02.25 val PER: 0.0758
2026-01-11 18:30:23,688: t15.2024.03.08 val PER: 0.1849
2026-01-11 18:30:23,688: t15.2024.03.15 val PER: 0.1632
2026-01-11 18:30:23,688: t15.2024.03.17 val PER: 0.0962
2026-01-11 18:30:23,688: t15.2024.05.10 val PER: 0.1382
2026-01-11 18:30:23,688: t15.2024.06.14 val PER: 0.1293
2026-01-11 18:30:23,688: t15.2024.07.19 val PER: 0.1826
2026-01-11 18:30:23,689: t15.2024.07.21 val PER: 0.0614
2026-01-11 18:30:23,689: t15.2024.07.28 val PER: 0.1022
2026-01-11 18:30:23,689: t15.2025.01.10 val PER: 0.2603
2026-01-11 18:30:23,689: t15.2025.01.12 val PER: 0.0993
2026-01-11 18:30:23,689: t15.2025.03.14 val PER: 0.2914
2026-01-11 18:30:23,690: t15.2025.03.16 val PER: 0.1427
2026-01-11 18:30:23,690: t15.2025.03.30 val PER: 0.2253
2026-01-11 18:30:23,690: t15.2025.04.13 val PER: 0.1997
2026-01-11 18:30:23,868: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_28000
2026-01-11 18:30:44,563: Train batch 28200: loss: 1.39 grad norm: 27.81 time: 0.073
2026-01-11 18:31:06,271: Train batch 28400: loss: 2.77 grad norm: 41.43 time: 0.072
2026-01-11 18:31:16,757: Running test after training batch: 28500
2026-01-11 18:31:16,906: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:31:24,154: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:31:24,234: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:31:39,787: Val batch 28500: PER (avg): 0.1133 CTC Loss (avg): 27.4037 WER(5gram): 11.08% (n=256) time: 23.029
2026-01-11 18:31:39,788: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 18:31:39,788: t15.2023.08.13 val PER: 0.0811
2026-01-11 18:31:39,789: t15.2023.08.18 val PER: 0.0780
2026-01-11 18:31:39,789: t15.2023.08.20 val PER: 0.0715
2026-01-11 18:31:39,789: t15.2023.08.25 val PER: 0.0648
2026-01-11 18:31:39,789: t15.2023.08.27 val PER: 0.1640
2026-01-11 18:31:39,789: t15.2023.09.01 val PER: 0.0455
2026-01-11 18:31:39,790: t15.2023.09.03 val PER: 0.1295
2026-01-11 18:31:39,790: t15.2023.09.24 val PER: 0.0850
2026-01-11 18:31:39,790: t15.2023.09.29 val PER: 0.1072
2026-01-11 18:31:39,790: t15.2023.10.01 val PER: 0.1466
2026-01-11 18:31:39,790: t15.2023.10.06 val PER: 0.0571
2026-01-11 18:31:39,790: t15.2023.10.08 val PER: 0.2192
2026-01-11 18:31:39,790: t15.2023.10.13 val PER: 0.1800
2026-01-11 18:31:39,790: t15.2023.10.15 val PER: 0.1206
2026-01-11 18:31:39,790: t15.2023.10.20 val PER: 0.1745
2026-01-11 18:31:39,791: t15.2023.10.22 val PER: 0.0958
2026-01-11 18:31:39,791: t15.2023.11.03 val PER: 0.1526
2026-01-11 18:31:39,791: t15.2023.11.04 val PER: 0.0171
2026-01-11 18:31:39,791: t15.2023.11.17 val PER: 0.0295
2026-01-11 18:31:39,791: t15.2023.11.19 val PER: 0.0140
2026-01-11 18:31:39,791: t15.2023.11.26 val PER: 0.0507
2026-01-11 18:31:39,791: t15.2023.12.03 val PER: 0.0578
2026-01-11 18:31:39,791: t15.2023.12.08 val PER: 0.0466
2026-01-11 18:31:39,792: t15.2023.12.10 val PER: 0.0381
2026-01-11 18:31:39,792: t15.2023.12.17 val PER: 0.0904
2026-01-11 18:31:39,792: t15.2023.12.29 val PER: 0.0700
2026-01-11 18:31:39,792: t15.2024.02.25 val PER: 0.0815
2026-01-11 18:31:39,792: t15.2024.03.08 val PER: 0.1735
2026-01-11 18:31:39,792: t15.2024.03.15 val PER: 0.1582
2026-01-11 18:31:39,792: t15.2024.03.17 val PER: 0.0948
2026-01-11 18:31:39,792: t15.2024.05.10 val PER: 0.1263
2026-01-11 18:31:39,793: t15.2024.06.14 val PER: 0.1293
2026-01-11 18:31:39,793: t15.2024.07.19 val PER: 0.1885
2026-01-11 18:31:39,793: t15.2024.07.21 val PER: 0.0655
2026-01-11 18:31:39,793: t15.2024.07.28 val PER: 0.1000
2026-01-11 18:31:39,793: t15.2025.01.10 val PER: 0.2590
2026-01-11 18:31:39,793: t15.2025.01.12 val PER: 0.0985
2026-01-11 18:31:39,793: t15.2025.03.14 val PER: 0.3033
2026-01-11 18:31:39,793: t15.2025.03.16 val PER: 0.1270
2026-01-11 18:31:39,794: t15.2025.03.30 val PER: 0.2425
2026-01-11 18:31:39,794: t15.2025.04.13 val PER: 0.1983
2026-01-11 18:31:39,956: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_28500
2026-01-11 18:31:50,340: Train batch 28600: loss: 2.76 grad norm: 41.28 time: 0.072
2026-01-11 18:32:11,186: Train batch 28800: loss: 2.09 grad norm: 61.25 time: 0.057
2026-01-11 18:32:32,234: Train batch 29000: loss: 2.23 grad norm: 50.10 time: 0.060
2026-01-11 18:32:32,234: Running test after training batch: 29000
2026-01-11 18:32:32,362: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:32:39,189: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:32:39,257: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost said
2026-01-11 18:32:54,151: Val batch 29000: PER (avg): 0.1127 CTC Loss (avg): 27.0468 WER(5gram): 12.78% (n=256) time: 21.916
2026-01-11 18:32:54,151: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-11 18:32:54,152: t15.2023.08.13 val PER: 0.0936
2026-01-11 18:32:54,152: t15.2023.08.18 val PER: 0.0754
2026-01-11 18:32:54,153: t15.2023.08.20 val PER: 0.0723
2026-01-11 18:32:54,153: t15.2023.08.25 val PER: 0.0753
2026-01-11 18:32:54,153: t15.2023.08.27 val PER: 0.1431
2026-01-11 18:32:54,153: t15.2023.09.01 val PER: 0.0446
2026-01-11 18:32:54,153: t15.2023.09.03 val PER: 0.1223
2026-01-11 18:32:54,153: t15.2023.09.24 val PER: 0.0922
2026-01-11 18:32:54,153: t15.2023.09.29 val PER: 0.1034
2026-01-11 18:32:54,153: t15.2023.10.01 val PER: 0.1493
2026-01-11 18:32:54,154: t15.2023.10.06 val PER: 0.0614
2026-01-11 18:32:54,154: t15.2023.10.08 val PER: 0.2084
2026-01-11 18:32:54,154: t15.2023.10.13 val PER: 0.1777
2026-01-11 18:32:54,154: t15.2023.10.15 val PER: 0.1206
2026-01-11 18:32:54,154: t15.2023.10.20 val PER: 0.1711
2026-01-11 18:32:54,154: t15.2023.10.22 val PER: 0.0924
2026-01-11 18:32:54,154: t15.2023.11.03 val PER: 0.1547
2026-01-11 18:32:54,154: t15.2023.11.04 val PER: 0.0171
2026-01-11 18:32:54,154: t15.2023.11.17 val PER: 0.0218
2026-01-11 18:32:54,154: t15.2023.11.19 val PER: 0.0120
2026-01-11 18:32:54,155: t15.2023.11.26 val PER: 0.0558
2026-01-11 18:32:54,155: t15.2023.12.03 val PER: 0.0578
2026-01-11 18:32:54,155: t15.2023.12.08 val PER: 0.0446
2026-01-11 18:32:54,155: t15.2023.12.10 val PER: 0.0263
2026-01-11 18:32:54,155: t15.2023.12.17 val PER: 0.0925
2026-01-11 18:32:54,155: t15.2023.12.29 val PER: 0.0851
2026-01-11 18:32:54,155: t15.2024.02.25 val PER: 0.0843
2026-01-11 18:32:54,155: t15.2024.03.08 val PER: 0.1835
2026-01-11 18:32:54,155: t15.2024.03.15 val PER: 0.1676
2026-01-11 18:32:54,155: t15.2024.03.17 val PER: 0.0934
2026-01-11 18:32:54,155: t15.2024.05.10 val PER: 0.1189
2026-01-11 18:32:54,156: t15.2024.06.14 val PER: 0.1278
2026-01-11 18:32:54,156: t15.2024.07.19 val PER: 0.1701
2026-01-11 18:32:54,156: t15.2024.07.21 val PER: 0.0655
2026-01-11 18:32:54,156: t15.2024.07.28 val PER: 0.1037
2026-01-11 18:32:54,156: t15.2025.01.10 val PER: 0.2534
2026-01-11 18:32:54,156: t15.2025.01.12 val PER: 0.0993
2026-01-11 18:32:54,156: t15.2025.03.14 val PER: 0.2959
2026-01-11 18:32:54,156: t15.2025.03.16 val PER: 0.1204
2026-01-11 18:32:54,156: t15.2025.03.30 val PER: 0.2195
2026-01-11 18:32:54,156: t15.2025.04.13 val PER: 0.2111
2026-01-11 18:32:54,312: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_29000
2026-01-11 18:33:15,256: Train batch 29200: loss: 2.74 grad norm: 42.64 time: 0.078
2026-01-11 18:33:36,985: Train batch 29400: loss: 1.37 grad norm: 39.75 time: 0.071
2026-01-11 18:33:47,500: Running test after training batch: 29500
2026-01-11 18:33:47,731: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:33:54,096: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:33:54,160: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:34:09,326: Val batch 29500: PER (avg): 0.1144 CTC Loss (avg): 26.9523 WER(5gram): 12.13% (n=256) time: 21.826
2026-01-11 18:34:09,327: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-11 18:34:09,327: t15.2023.08.13 val PER: 0.0800
2026-01-11 18:34:09,327: t15.2023.08.18 val PER: 0.0780
2026-01-11 18:34:09,328: t15.2023.08.20 val PER: 0.0763
2026-01-11 18:34:09,328: t15.2023.08.25 val PER: 0.0768
2026-01-11 18:34:09,328: t15.2023.08.27 val PER: 0.1543
2026-01-11 18:34:09,328: t15.2023.09.01 val PER: 0.0503
2026-01-11 18:34:09,328: t15.2023.09.03 val PER: 0.1259
2026-01-11 18:34:09,328: t15.2023.09.24 val PER: 0.1044
2026-01-11 18:34:09,328: t15.2023.09.29 val PER: 0.1136
2026-01-11 18:34:09,328: t15.2023.10.01 val PER: 0.1499
2026-01-11 18:34:09,328: t15.2023.10.06 val PER: 0.0646
2026-01-11 18:34:09,329: t15.2023.10.08 val PER: 0.2152
2026-01-11 18:34:09,329: t15.2023.10.13 val PER: 0.1761
2026-01-11 18:34:09,329: t15.2023.10.15 val PER: 0.1180
2026-01-11 18:34:09,329: t15.2023.10.20 val PER: 0.1812
2026-01-11 18:34:09,329: t15.2023.10.22 val PER: 0.1002
2026-01-11 18:34:09,329: t15.2023.11.03 val PER: 0.1520
2026-01-11 18:34:09,329: t15.2023.11.04 val PER: 0.0239
2026-01-11 18:34:09,330: t15.2023.11.17 val PER: 0.0249
2026-01-11 18:34:09,330: t15.2023.11.19 val PER: 0.0140
2026-01-11 18:34:09,330: t15.2023.11.26 val PER: 0.0522
2026-01-11 18:34:09,330: t15.2023.12.03 val PER: 0.0578
2026-01-11 18:34:09,330: t15.2023.12.08 val PER: 0.0419
2026-01-11 18:34:09,330: t15.2023.12.10 val PER: 0.0315
2026-01-11 18:34:09,330: t15.2023.12.17 val PER: 0.0956
2026-01-11 18:34:09,330: t15.2023.12.29 val PER: 0.0803
2026-01-11 18:34:09,330: t15.2024.02.25 val PER: 0.0843
2026-01-11 18:34:09,330: t15.2024.03.08 val PER: 0.1735
2026-01-11 18:34:09,331: t15.2024.03.15 val PER: 0.1701
2026-01-11 18:34:09,331: t15.2024.03.17 val PER: 0.0955
2026-01-11 18:34:09,331: t15.2024.05.10 val PER: 0.1218
2026-01-11 18:34:09,331: t15.2024.06.14 val PER: 0.1356
2026-01-11 18:34:09,331: t15.2024.07.19 val PER: 0.1806
2026-01-11 18:34:09,331: t15.2024.07.21 val PER: 0.0710
2026-01-11 18:34:09,331: t15.2024.07.28 val PER: 0.0978
2026-01-11 18:34:09,331: t15.2025.01.10 val PER: 0.2700
2026-01-11 18:34:09,331: t15.2025.01.12 val PER: 0.0955
2026-01-11 18:34:09,331: t15.2025.03.14 val PER: 0.2870
2026-01-11 18:34:09,331: t15.2025.03.16 val PER: 0.1257
2026-01-11 18:34:09,332: t15.2025.03.30 val PER: 0.2322
2026-01-11 18:34:09,332: t15.2025.04.13 val PER: 0.1940
2026-01-11 18:34:09,489: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_29500
2026-01-11 18:34:20,083: Train batch 29600: loss: 2.17 grad norm: 52.00 time: 0.061
2026-01-11 18:34:40,341: Train batch 29800: loss: 1.89 grad norm: 37.02 time: 0.094
2026-01-11 18:35:00,374: Train batch 30000: loss: 2.30 grad norm: 43.71 time: 0.079
2026-01-11 18:35:00,375: Running test after training batch: 30000
2026-01-11 18:35:00,544: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:35:07,438: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:35:07,514: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:35:25,523: Val batch 30000: PER (avg): 0.1131 CTC Loss (avg): 27.0748 WER(5gram): 11.15% (n=256) time: 25.148
2026-01-11 18:35:25,525: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 18:35:25,525: t15.2023.08.13 val PER: 0.0956
2026-01-11 18:35:25,525: t15.2023.08.18 val PER: 0.0729
2026-01-11 18:35:25,525: t15.2023.08.20 val PER: 0.0620
2026-01-11 18:35:25,525: t15.2023.08.25 val PER: 0.0738
2026-01-11 18:35:25,525: t15.2023.08.27 val PER: 0.1576
2026-01-11 18:35:25,526: t15.2023.09.01 val PER: 0.0487
2026-01-11 18:35:25,526: t15.2023.09.03 val PER: 0.1200
2026-01-11 18:35:25,526: t15.2023.09.24 val PER: 0.0825
2026-01-11 18:35:25,526: t15.2023.09.29 val PER: 0.1117
2026-01-11 18:35:25,526: t15.2023.10.01 val PER: 0.1526
2026-01-11 18:35:25,526: t15.2023.10.06 val PER: 0.0646
2026-01-11 18:35:25,526: t15.2023.10.08 val PER: 0.2097
2026-01-11 18:35:25,527: t15.2023.10.13 val PER: 0.1691
2026-01-11 18:35:25,527: t15.2023.10.15 val PER: 0.1220
2026-01-11 18:35:25,528: t15.2023.10.20 val PER: 0.1477
2026-01-11 18:35:25,528: t15.2023.10.22 val PER: 0.0969
2026-01-11 18:35:25,528: t15.2023.11.03 val PER: 0.1499
2026-01-11 18:35:25,528: t15.2023.11.04 val PER: 0.0205
2026-01-11 18:35:25,528: t15.2023.11.17 val PER: 0.0233
2026-01-11 18:35:25,529: t15.2023.11.19 val PER: 0.0120
2026-01-11 18:35:25,529: t15.2023.11.26 val PER: 0.0558
2026-01-11 18:35:25,529: t15.2023.12.03 val PER: 0.0515
2026-01-11 18:35:25,529: t15.2023.12.08 val PER: 0.0426
2026-01-11 18:35:25,529: t15.2023.12.10 val PER: 0.0315
2026-01-11 18:35:25,530: t15.2023.12.17 val PER: 0.0946
2026-01-11 18:35:25,530: t15.2023.12.29 val PER: 0.0782
2026-01-11 18:35:25,530: t15.2024.02.25 val PER: 0.0843
2026-01-11 18:35:25,530: t15.2024.03.08 val PER: 0.1892
2026-01-11 18:35:25,530: t15.2024.03.15 val PER: 0.1664
2026-01-11 18:35:25,530: t15.2024.03.17 val PER: 0.0879
2026-01-11 18:35:25,530: t15.2024.05.10 val PER: 0.1337
2026-01-11 18:35:25,530: t15.2024.06.14 val PER: 0.1341
2026-01-11 18:35:25,531: t15.2024.07.19 val PER: 0.1786
2026-01-11 18:35:25,531: t15.2024.07.21 val PER: 0.0683
2026-01-11 18:35:25,531: t15.2024.07.28 val PER: 0.1007
2026-01-11 18:35:25,531: t15.2025.01.10 val PER: 0.2645
2026-01-11 18:35:25,531: t15.2025.01.12 val PER: 0.0893
2026-01-11 18:35:25,531: t15.2025.03.14 val PER: 0.3151
2026-01-11 18:35:25,531: t15.2025.03.16 val PER: 0.1322
2026-01-11 18:35:25,531: t15.2025.03.30 val PER: 0.2264
2026-01-11 18:35:25,532: t15.2025.04.13 val PER: 0.2011
2026-01-11 18:35:25,703: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_30000
2026-01-11 18:35:46,732: Train batch 30200: loss: 3.46 grad norm: 57.65 time: 0.090
2026-01-11 18:36:07,537: Train batch 30400: loss: 1.76 grad norm: 42.09 time: 0.071
2026-01-11 18:36:17,936: Running test after training batch: 30500
2026-01-11 18:36:18,236: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:36:24,988: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:36:25,055: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:36:40,553: Val batch 30500: PER (avg): 0.1141 CTC Loss (avg): 27.8741 WER(5gram): 11.41% (n=256) time: 22.617
2026-01-11 18:36:40,554: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 18:36:40,554: t15.2023.08.13 val PER: 0.0925
2026-01-11 18:36:40,554: t15.2023.08.18 val PER: 0.0687
2026-01-11 18:36:40,554: t15.2023.08.20 val PER: 0.0659
2026-01-11 18:36:40,555: t15.2023.08.25 val PER: 0.0663
2026-01-11 18:36:40,555: t15.2023.08.27 val PER: 0.1704
2026-01-11 18:36:40,555: t15.2023.09.01 val PER: 0.0463
2026-01-11 18:36:40,555: t15.2023.09.03 val PER: 0.1223
2026-01-11 18:36:40,555: t15.2023.09.24 val PER: 0.0862
2026-01-11 18:36:40,555: t15.2023.09.29 val PER: 0.1225
2026-01-11 18:36:40,555: t15.2023.10.01 val PER: 0.1579
2026-01-11 18:36:40,556: t15.2023.10.06 val PER: 0.0743
2026-01-11 18:36:40,556: t15.2023.10.08 val PER: 0.2070
2026-01-11 18:36:40,556: t15.2023.10.13 val PER: 0.1738
2026-01-11 18:36:40,556: t15.2023.10.15 val PER: 0.1233
2026-01-11 18:36:40,556: t15.2023.10.20 val PER: 0.1611
2026-01-11 18:36:40,556: t15.2023.10.22 val PER: 0.0991
2026-01-11 18:36:40,556: t15.2023.11.03 val PER: 0.1574
2026-01-11 18:36:40,556: t15.2023.11.04 val PER: 0.0239
2026-01-11 18:36:40,557: t15.2023.11.17 val PER: 0.0202
2026-01-11 18:36:40,557: t15.2023.11.19 val PER: 0.0080
2026-01-11 18:36:40,557: t15.2023.11.26 val PER: 0.0514
2026-01-11 18:36:40,557: t15.2023.12.03 val PER: 0.0536
2026-01-11 18:36:40,557: t15.2023.12.08 val PER: 0.0493
2026-01-11 18:36:40,557: t15.2023.12.10 val PER: 0.0302
2026-01-11 18:36:40,557: t15.2023.12.17 val PER: 0.0852
2026-01-11 18:36:40,557: t15.2023.12.29 val PER: 0.0734
2026-01-11 18:36:40,558: t15.2024.02.25 val PER: 0.0899
2026-01-11 18:36:40,558: t15.2024.03.08 val PER: 0.1963
2026-01-11 18:36:40,558: t15.2024.03.15 val PER: 0.1670
2026-01-11 18:36:40,558: t15.2024.03.17 val PER: 0.0879
2026-01-11 18:36:40,558: t15.2024.05.10 val PER: 0.1144
2026-01-11 18:36:40,558: t15.2024.06.14 val PER: 0.1483
2026-01-11 18:36:40,558: t15.2024.07.19 val PER: 0.1753
2026-01-11 18:36:40,558: t15.2024.07.21 val PER: 0.0669
2026-01-11 18:36:40,558: t15.2024.07.28 val PER: 0.1037
2026-01-11 18:36:40,558: t15.2025.01.10 val PER: 0.2603
2026-01-11 18:36:40,559: t15.2025.01.12 val PER: 0.0901
2026-01-11 18:36:40,559: t15.2025.03.14 val PER: 0.2929
2026-01-11 18:36:40,559: t15.2025.03.16 val PER: 0.1387
2026-01-11 18:36:40,559: t15.2025.03.30 val PER: 0.2287
2026-01-11 18:36:40,559: t15.2025.04.13 val PER: 0.2026
2026-01-11 18:36:40,734: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_30500
2026-01-11 18:36:51,242: Train batch 30600: loss: 2.50 grad norm: 55.83 time: 0.079
2026-01-11 18:37:11,892: Train batch 30800: loss: 0.80 grad norm: 24.21 time: 0.072
2026-01-11 18:37:32,533: Train batch 31000: loss: 2.30 grad norm: 51.57 time: 0.075
2026-01-11 18:37:32,533: Running test after training batch: 31000
2026-01-11 18:37:32,741: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:37:39,558: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:37:39,630: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:37:54,436: Val batch 31000: PER (avg): 0.1121 CTC Loss (avg): 27.5677 WER(5gram): 10.50% (n=256) time: 21.902
2026-01-11 18:37:54,437: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-11 18:37:54,437: t15.2023.08.13 val PER: 0.0832
2026-01-11 18:37:54,437: t15.2023.08.18 val PER: 0.0712
2026-01-11 18:37:54,437: t15.2023.08.20 val PER: 0.0731
2026-01-11 18:37:54,437: t15.2023.08.25 val PER: 0.0693
2026-01-11 18:37:54,437: t15.2023.08.27 val PER: 0.1576
2026-01-11 18:37:54,437: t15.2023.09.01 val PER: 0.0455
2026-01-11 18:37:54,438: t15.2023.09.03 val PER: 0.1259
2026-01-11 18:37:54,438: t15.2023.09.24 val PER: 0.0874
2026-01-11 18:37:54,438: t15.2023.09.29 val PER: 0.1123
2026-01-11 18:37:54,438: t15.2023.10.01 val PER: 0.1506
2026-01-11 18:37:54,438: t15.2023.10.06 val PER: 0.0667
2026-01-11 18:37:54,438: t15.2023.10.08 val PER: 0.2179
2026-01-11 18:37:54,438: t15.2023.10.13 val PER: 0.1668
2026-01-11 18:37:54,438: t15.2023.10.15 val PER: 0.1259
2026-01-11 18:37:54,438: t15.2023.10.20 val PER: 0.1577
2026-01-11 18:37:54,438: t15.2023.10.22 val PER: 0.1002
2026-01-11 18:37:54,438: t15.2023.11.03 val PER: 0.1554
2026-01-11 18:37:54,439: t15.2023.11.04 val PER: 0.0137
2026-01-11 18:37:54,439: t15.2023.11.17 val PER: 0.0233
2026-01-11 18:37:54,439: t15.2023.11.19 val PER: 0.0140
2026-01-11 18:37:54,439: t15.2023.11.26 val PER: 0.0478
2026-01-11 18:37:54,439: t15.2023.12.03 val PER: 0.0525
2026-01-11 18:37:54,439: t15.2023.12.08 val PER: 0.0473
2026-01-11 18:37:54,439: t15.2023.12.10 val PER: 0.0394
2026-01-11 18:37:54,439: t15.2023.12.17 val PER: 0.0894
2026-01-11 18:37:54,439: t15.2023.12.29 val PER: 0.0782
2026-01-11 18:37:54,439: t15.2024.02.25 val PER: 0.0815
2026-01-11 18:37:54,439: t15.2024.03.08 val PER: 0.1693
2026-01-11 18:37:54,440: t15.2024.03.15 val PER: 0.1620
2026-01-11 18:37:54,440: t15.2024.03.17 val PER: 0.0969
2026-01-11 18:37:54,440: t15.2024.05.10 val PER: 0.1218
2026-01-11 18:37:54,440: t15.2024.06.14 val PER: 0.1278
2026-01-11 18:37:54,440: t15.2024.07.19 val PER: 0.1707
2026-01-11 18:37:54,440: t15.2024.07.21 val PER: 0.0676
2026-01-11 18:37:54,440: t15.2024.07.28 val PER: 0.0985
2026-01-11 18:37:54,440: t15.2025.01.10 val PER: 0.2617
2026-01-11 18:37:54,441: t15.2025.01.12 val PER: 0.0970
2026-01-11 18:37:54,441: t15.2025.03.14 val PER: 0.3062
2026-01-11 18:37:54,441: t15.2025.03.16 val PER: 0.1165
2026-01-11 18:37:54,441: t15.2025.03.30 val PER: 0.2207
2026-01-11 18:37:54,441: t15.2025.04.13 val PER: 0.1883
2026-01-11 18:37:54,602: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_31000
2026-01-11 18:38:15,224: Train batch 31200: loss: 1.45 grad norm: 29.24 time: 0.079
2026-01-11 18:38:36,846: Train batch 31400: loss: 2.96 grad norm: 52.29 time: 0.069
2026-01-11 18:38:47,082: Running test after training batch: 31500
2026-01-11 18:38:47,195: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:38:53,468: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:38:53,539: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:39:08,363: Val batch 31500: PER (avg): 0.1129 CTC Loss (avg): 27.8144 WER(5gram): 11.41% (n=256) time: 21.280
2026-01-11 18:39:08,363: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 18:39:08,364: t15.2023.08.13 val PER: 0.0863
2026-01-11 18:39:08,364: t15.2023.08.18 val PER: 0.0654
2026-01-11 18:39:08,364: t15.2023.08.20 val PER: 0.0723
2026-01-11 18:39:08,364: t15.2023.08.25 val PER: 0.0723
2026-01-11 18:39:08,364: t15.2023.08.27 val PER: 0.1479
2026-01-11 18:39:08,364: t15.2023.09.01 val PER: 0.0471
2026-01-11 18:39:08,364: t15.2023.09.03 val PER: 0.1283
2026-01-11 18:39:08,364: t15.2023.09.24 val PER: 0.0886
2026-01-11 18:39:08,364: t15.2023.09.29 val PER: 0.1085
2026-01-11 18:39:08,364: t15.2023.10.01 val PER: 0.1506
2026-01-11 18:39:08,364: t15.2023.10.06 val PER: 0.0646
2026-01-11 18:39:08,365: t15.2023.10.08 val PER: 0.2192
2026-01-11 18:39:08,365: t15.2023.10.13 val PER: 0.1792
2026-01-11 18:39:08,365: t15.2023.10.15 val PER: 0.1206
2026-01-11 18:39:08,365: t15.2023.10.20 val PER: 0.1812
2026-01-11 18:39:08,366: t15.2023.10.22 val PER: 0.0902
2026-01-11 18:39:08,366: t15.2023.11.03 val PER: 0.1574
2026-01-11 18:39:08,366: t15.2023.11.04 val PER: 0.0171
2026-01-11 18:39:08,366: t15.2023.11.17 val PER: 0.0264
2026-01-11 18:39:08,366: t15.2023.11.19 val PER: 0.0100
2026-01-11 18:39:08,366: t15.2023.11.26 val PER: 0.0522
2026-01-11 18:39:08,367: t15.2023.12.03 val PER: 0.0557
2026-01-11 18:39:08,367: t15.2023.12.08 val PER: 0.0393
2026-01-11 18:39:08,367: t15.2023.12.10 val PER: 0.0355
2026-01-11 18:39:08,367: t15.2023.12.17 val PER: 0.0863
2026-01-11 18:39:08,367: t15.2023.12.29 val PER: 0.0837
2026-01-11 18:39:08,367: t15.2024.02.25 val PER: 0.0772
2026-01-11 18:39:08,367: t15.2024.03.08 val PER: 0.1835
2026-01-11 18:39:08,367: t15.2024.03.15 val PER: 0.1645
2026-01-11 18:39:08,368: t15.2024.03.17 val PER: 0.0872
2026-01-11 18:39:08,368: t15.2024.05.10 val PER: 0.1218
2026-01-11 18:39:08,368: t15.2024.06.14 val PER: 0.1341
2026-01-11 18:39:08,368: t15.2024.07.19 val PER: 0.1879
2026-01-11 18:39:08,368: t15.2024.07.21 val PER: 0.0634
2026-01-11 18:39:08,368: t15.2024.07.28 val PER: 0.0956
2026-01-11 18:39:08,368: t15.2025.01.10 val PER: 0.2507
2026-01-11 18:39:08,368: t15.2025.01.12 val PER: 0.1032
2026-01-11 18:39:08,368: t15.2025.03.14 val PER: 0.3033
2026-01-11 18:39:08,368: t15.2025.03.16 val PER: 0.1322
2026-01-11 18:39:08,368: t15.2025.03.30 val PER: 0.2115
2026-01-11 18:39:08,369: t15.2025.04.13 val PER: 0.2111
2026-01-11 18:39:08,526: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_31500
2026-01-11 18:39:18,785: Train batch 31600: loss: 1.40 grad norm: 30.81 time: 0.080
2026-01-11 18:39:39,115: Train batch 31800: loss: 2.66 grad norm: 43.90 time: 0.063
2026-01-11 18:40:00,080: Train batch 32000: loss: 2.29 grad norm: 39.44 time: 0.084
2026-01-11 18:40:00,080: Running test after training batch: 32000
2026-01-11 18:40:00,218: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:40:06,683: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:40:06,751: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:40:21,060: Val batch 32000: PER (avg): 0.1121 CTC Loss (avg): 27.5631 WER(5gram): 12.91% (n=256) time: 20.979
2026-01-11 18:40:21,060: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 18:40:21,061: t15.2023.08.13 val PER: 0.0821
2026-01-11 18:40:21,061: t15.2023.08.18 val PER: 0.0620
2026-01-11 18:40:21,061: t15.2023.08.20 val PER: 0.0778
2026-01-11 18:40:21,061: t15.2023.08.25 val PER: 0.0678
2026-01-11 18:40:21,062: t15.2023.08.27 val PER: 0.1640
2026-01-11 18:40:21,062: t15.2023.09.01 val PER: 0.0536
2026-01-11 18:40:21,062: t15.2023.09.03 val PER: 0.1259
2026-01-11 18:40:21,062: t15.2023.09.24 val PER: 0.0862
2026-01-11 18:40:21,062: t15.2023.09.29 val PER: 0.1098
2026-01-11 18:40:21,062: t15.2023.10.01 val PER: 0.1565
2026-01-11 18:40:21,062: t15.2023.10.06 val PER: 0.0635
2026-01-11 18:40:21,062: t15.2023.10.08 val PER: 0.2179
2026-01-11 18:40:21,062: t15.2023.10.13 val PER: 0.1691
2026-01-11 18:40:21,063: t15.2023.10.15 val PER: 0.1140
2026-01-11 18:40:21,063: t15.2023.10.20 val PER: 0.1611
2026-01-11 18:40:21,063: t15.2023.10.22 val PER: 0.0991
2026-01-11 18:40:21,063: t15.2023.11.03 val PER: 0.1621
2026-01-11 18:40:21,063: t15.2023.11.04 val PER: 0.0205
2026-01-11 18:40:21,063: t15.2023.11.17 val PER: 0.0249
2026-01-11 18:40:21,063: t15.2023.11.19 val PER: 0.0140
2026-01-11 18:40:21,064: t15.2023.11.26 val PER: 0.0536
2026-01-11 18:40:21,064: t15.2023.12.03 val PER: 0.0620
2026-01-11 18:40:21,064: t15.2023.12.08 val PER: 0.0453
2026-01-11 18:40:21,064: t15.2023.12.10 val PER: 0.0342
2026-01-11 18:40:21,064: t15.2023.12.17 val PER: 0.0894
2026-01-11 18:40:21,064: t15.2023.12.29 val PER: 0.0782
2026-01-11 18:40:21,064: t15.2024.02.25 val PER: 0.0772
2026-01-11 18:40:21,064: t15.2024.03.08 val PER: 0.1636
2026-01-11 18:40:21,064: t15.2024.03.15 val PER: 0.1689
2026-01-11 18:40:21,065: t15.2024.03.17 val PER: 0.0976
2026-01-11 18:40:21,065: t15.2024.05.10 val PER: 0.1040
2026-01-11 18:40:21,065: t15.2024.06.14 val PER: 0.1199
2026-01-11 18:40:21,065: t15.2024.07.19 val PER: 0.1707
2026-01-11 18:40:21,065: t15.2024.07.21 val PER: 0.0655
2026-01-11 18:40:21,065: t15.2024.07.28 val PER: 0.0904
2026-01-11 18:40:21,065: t15.2025.01.10 val PER: 0.2562
2026-01-11 18:40:21,065: t15.2025.01.12 val PER: 0.1024
2026-01-11 18:40:21,065: t15.2025.03.14 val PER: 0.2944
2026-01-11 18:40:21,066: t15.2025.03.16 val PER: 0.1191
2026-01-11 18:40:21,066: t15.2025.03.30 val PER: 0.2264
2026-01-11 18:40:21,066: t15.2025.04.13 val PER: 0.1954
2026-01-11 18:40:21,228: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_32000
2026-01-11 18:40:41,753: Train batch 32200: loss: 1.49 grad norm: 30.72 time: 0.098
2026-01-11 18:41:02,444: Train batch 32400: loss: 7.48 grad norm: 51.85 time: 0.075
2026-01-11 18:41:13,249: Running test after training batch: 32500
2026-01-11 18:41:13,405: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:41:19,557: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:41:19,624: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:41:33,797: Val batch 32500: PER (avg): 0.1113 CTC Loss (avg): 27.5923 WER(5gram): 12.52% (n=256) time: 20.548
2026-01-11 18:41:33,798: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 18:41:33,799: t15.2023.08.13 val PER: 0.0863
2026-01-11 18:41:33,799: t15.2023.08.18 val PER: 0.0721
2026-01-11 18:41:33,799: t15.2023.08.20 val PER: 0.0763
2026-01-11 18:41:33,799: t15.2023.08.25 val PER: 0.0617
2026-01-11 18:41:33,799: t15.2023.08.27 val PER: 0.1527
2026-01-11 18:41:33,799: t15.2023.09.01 val PER: 0.0455
2026-01-11 18:41:33,799: t15.2023.09.03 val PER: 0.1176
2026-01-11 18:41:33,800: t15.2023.09.24 val PER: 0.1032
2026-01-11 18:41:33,800: t15.2023.09.29 val PER: 0.1066
2026-01-11 18:41:33,800: t15.2023.10.01 val PER: 0.1473
2026-01-11 18:41:33,800: t15.2023.10.06 val PER: 0.0646
2026-01-11 18:41:33,800: t15.2023.10.08 val PER: 0.2152
2026-01-11 18:41:33,800: t15.2023.10.13 val PER: 0.1707
2026-01-11 18:41:33,800: t15.2023.10.15 val PER: 0.1193
2026-01-11 18:41:33,800: t15.2023.10.20 val PER: 0.1611
2026-01-11 18:41:33,800: t15.2023.10.22 val PER: 0.0835
2026-01-11 18:41:33,800: t15.2023.11.03 val PER: 0.1689
2026-01-11 18:41:33,800: t15.2023.11.04 val PER: 0.0205
2026-01-11 18:41:33,801: t15.2023.11.17 val PER: 0.0249
2026-01-11 18:41:33,801: t15.2023.11.19 val PER: 0.0120
2026-01-11 18:41:33,801: t15.2023.11.26 val PER: 0.0500
2026-01-11 18:41:33,801: t15.2023.12.03 val PER: 0.0567
2026-01-11 18:41:33,801: t15.2023.12.08 val PER: 0.0459
2026-01-11 18:41:33,801: t15.2023.12.10 val PER: 0.0302
2026-01-11 18:41:33,801: t15.2023.12.17 val PER: 0.0780
2026-01-11 18:41:33,801: t15.2023.12.29 val PER: 0.0796
2026-01-11 18:41:33,801: t15.2024.02.25 val PER: 0.0730
2026-01-11 18:41:33,801: t15.2024.03.08 val PER: 0.1679
2026-01-11 18:41:33,801: t15.2024.03.15 val PER: 0.1601
2026-01-11 18:41:33,802: t15.2024.03.17 val PER: 0.0927
2026-01-11 18:41:33,802: t15.2024.05.10 val PER: 0.1159
2026-01-11 18:41:33,802: t15.2024.06.14 val PER: 0.1325
2026-01-11 18:41:33,802: t15.2024.07.19 val PER: 0.1885
2026-01-11 18:41:33,802: t15.2024.07.21 val PER: 0.0648
2026-01-11 18:41:33,802: t15.2024.07.28 val PER: 0.0860
2026-01-11 18:41:33,802: t15.2025.01.10 val PER: 0.2603
2026-01-11 18:41:33,802: t15.2025.01.12 val PER: 0.0962
2026-01-11 18:41:33,802: t15.2025.03.14 val PER: 0.3047
2026-01-11 18:41:33,802: t15.2025.03.16 val PER: 0.1270
2026-01-11 18:41:33,802: t15.2025.03.30 val PER: 0.2069
2026-01-11 18:41:33,802: t15.2025.04.13 val PER: 0.1897
2026-01-11 18:41:33,964: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_32500
2026-01-11 18:41:44,585: Train batch 32600: loss: 1.55 grad norm: 28.54 time: 0.100
2026-01-11 18:42:05,367: Train batch 32800: loss: 1.65 grad norm: 50.48 time: 0.071
2026-01-11 18:42:25,462: Train batch 33000: loss: 1.07 grad norm: 30.56 time: 0.077
2026-01-11 18:42:25,462: Running test after training batch: 33000
2026-01-11 18:42:25,581: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:42:31,731: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:42:31,793: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:42:46,476: Val batch 33000: PER (avg): 0.1123 CTC Loss (avg): 27.9051 WER(5gram): 12.39% (n=256) time: 21.013
2026-01-11 18:42:46,477: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 18:42:46,477: t15.2023.08.13 val PER: 0.0852
2026-01-11 18:42:46,477: t15.2023.08.18 val PER: 0.0654
2026-01-11 18:42:46,477: t15.2023.08.20 val PER: 0.0818
2026-01-11 18:42:46,477: t15.2023.08.25 val PER: 0.0648
2026-01-11 18:42:46,477: t15.2023.08.27 val PER: 0.1656
2026-01-11 18:42:46,478: t15.2023.09.01 val PER: 0.0455
2026-01-11 18:42:46,478: t15.2023.09.03 val PER: 0.1211
2026-01-11 18:42:46,478: t15.2023.09.24 val PER: 0.1044
2026-01-11 18:42:46,478: t15.2023.09.29 val PER: 0.1123
2026-01-11 18:42:46,478: t15.2023.10.01 val PER: 0.1480
2026-01-11 18:42:46,478: t15.2023.10.06 val PER: 0.0624
2026-01-11 18:42:46,478: t15.2023.10.08 val PER: 0.2057
2026-01-11 18:42:46,478: t15.2023.10.13 val PER: 0.1738
2026-01-11 18:42:46,479: t15.2023.10.15 val PER: 0.1167
2026-01-11 18:42:46,479: t15.2023.10.20 val PER: 0.1577
2026-01-11 18:42:46,479: t15.2023.10.22 val PER: 0.0924
2026-01-11 18:42:46,479: t15.2023.11.03 val PER: 0.1554
2026-01-11 18:42:46,479: t15.2023.11.04 val PER: 0.0102
2026-01-11 18:42:46,479: t15.2023.11.17 val PER: 0.0233
2026-01-11 18:42:46,479: t15.2023.11.19 val PER: 0.0120
2026-01-11 18:42:46,480: t15.2023.11.26 val PER: 0.0507
2026-01-11 18:42:46,480: t15.2023.12.03 val PER: 0.0599
2026-01-11 18:42:46,480: t15.2023.12.08 val PER: 0.0439
2026-01-11 18:42:46,480: t15.2023.12.10 val PER: 0.0381
2026-01-11 18:42:46,480: t15.2023.12.17 val PER: 0.0873
2026-01-11 18:42:46,480: t15.2023.12.29 val PER: 0.0693
2026-01-11 18:42:46,480: t15.2024.02.25 val PER: 0.0801
2026-01-11 18:42:46,480: t15.2024.03.08 val PER: 0.1935
2026-01-11 18:42:46,480: t15.2024.03.15 val PER: 0.1632
2026-01-11 18:42:46,480: t15.2024.03.17 val PER: 0.0844
2026-01-11 18:42:46,481: t15.2024.05.10 val PER: 0.1233
2026-01-11 18:42:46,481: t15.2024.06.14 val PER: 0.1167
2026-01-11 18:42:46,481: t15.2024.07.19 val PER: 0.1839
2026-01-11 18:42:46,481: t15.2024.07.21 val PER: 0.0579
2026-01-11 18:42:46,481: t15.2024.07.28 val PER: 0.0993
2026-01-11 18:42:46,481: t15.2025.01.10 val PER: 0.2603
2026-01-11 18:42:46,481: t15.2025.01.12 val PER: 0.1055
2026-01-11 18:42:46,481: t15.2025.03.14 val PER: 0.3062
2026-01-11 18:42:46,482: t15.2025.03.16 val PER: 0.1296
2026-01-11 18:42:46,482: t15.2025.03.30 val PER: 0.2126
2026-01-11 18:42:46,482: t15.2025.04.13 val PER: 0.2126
2026-01-11 18:42:46,641: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_33000
2026-01-11 18:43:07,768: Train batch 33200: loss: 2.55 grad norm: 44.99 time: 0.075
2026-01-11 18:43:28,759: Train batch 33400: loss: 0.22 grad norm: 13.29 time: 0.064
2026-01-11 18:43:38,711: Running test after training batch: 33500
2026-01-11 18:43:38,894: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:43:45,032: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:43:45,101: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:44:00,377: Val batch 33500: PER (avg): 0.1113 CTC Loss (avg): 28.3490 WER(5gram): 12.26% (n=256) time: 21.665
2026-01-11 18:44:00,377: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 18:44:00,377: t15.2023.08.13 val PER: 0.0780
2026-01-11 18:44:00,378: t15.2023.08.18 val PER: 0.0687
2026-01-11 18:44:00,378: t15.2023.08.20 val PER: 0.0778
2026-01-11 18:44:00,378: t15.2023.08.25 val PER: 0.0708
2026-01-11 18:44:00,378: t15.2023.08.27 val PER: 0.1640
2026-01-11 18:44:00,378: t15.2023.09.01 val PER: 0.0495
2026-01-11 18:44:00,378: t15.2023.09.03 val PER: 0.1140
2026-01-11 18:44:00,378: t15.2023.09.24 val PER: 0.0825
2026-01-11 18:44:00,378: t15.2023.09.29 val PER: 0.0996
2026-01-11 18:44:00,379: t15.2023.10.01 val PER: 0.1513
2026-01-11 18:44:00,379: t15.2023.10.06 val PER: 0.0678
2026-01-11 18:44:00,379: t15.2023.10.08 val PER: 0.2138
2026-01-11 18:44:00,379: t15.2023.10.13 val PER: 0.1621
2026-01-11 18:44:00,379: t15.2023.10.15 val PER: 0.1154
2026-01-11 18:44:00,379: t15.2023.10.20 val PER: 0.1544
2026-01-11 18:44:00,379: t15.2023.10.22 val PER: 0.1013
2026-01-11 18:44:00,379: t15.2023.11.03 val PER: 0.1567
2026-01-11 18:44:00,379: t15.2023.11.04 val PER: 0.0102
2026-01-11 18:44:00,380: t15.2023.11.17 val PER: 0.0311
2026-01-11 18:44:00,380: t15.2023.11.19 val PER: 0.0140
2026-01-11 18:44:00,380: t15.2023.11.26 val PER: 0.0522
2026-01-11 18:44:00,380: t15.2023.12.03 val PER: 0.0630
2026-01-11 18:44:00,380: t15.2023.12.08 val PER: 0.0399
2026-01-11 18:44:00,380: t15.2023.12.10 val PER: 0.0368
2026-01-11 18:44:00,380: t15.2023.12.17 val PER: 0.0842
2026-01-11 18:44:00,381: t15.2023.12.29 val PER: 0.0741
2026-01-11 18:44:00,381: t15.2024.02.25 val PER: 0.0815
2026-01-11 18:44:00,381: t15.2024.03.08 val PER: 0.1707
2026-01-11 18:44:00,381: t15.2024.03.15 val PER: 0.1645
2026-01-11 18:44:00,381: t15.2024.03.17 val PER: 0.0851
2026-01-11 18:44:00,381: t15.2024.05.10 val PER: 0.1174
2026-01-11 18:44:00,381: t15.2024.06.14 val PER: 0.1325
2026-01-11 18:44:00,381: t15.2024.07.19 val PER: 0.1753
2026-01-11 18:44:00,381: t15.2024.07.21 val PER: 0.0683
2026-01-11 18:44:00,382: t15.2024.07.28 val PER: 0.0993
2026-01-11 18:44:00,382: t15.2025.01.10 val PER: 0.2534
2026-01-11 18:44:00,382: t15.2025.01.12 val PER: 0.1032
2026-01-11 18:44:00,382: t15.2025.03.14 val PER: 0.3062
2026-01-11 18:44:00,382: t15.2025.03.16 val PER: 0.1335
2026-01-11 18:44:00,382: t15.2025.03.30 val PER: 0.2172
2026-01-11 18:44:00,382: t15.2025.04.13 val PER: 0.1997
2026-01-11 18:44:00,539: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_33500
2026-01-11 18:44:11,440: Train batch 33600: loss: 1.47 grad norm: 31.15 time: 0.066
2026-01-11 18:44:32,111: Train batch 33800: loss: 0.92 grad norm: 27.45 time: 0.080
2026-01-11 18:44:53,342: Train batch 34000: loss: 2.19 grad norm: 52.13 time: 0.088
2026-01-11 18:44:53,343: Running test after training batch: 34000
2026-01-11 18:44:53,481: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:45:00,133: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:45:00,209: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:45:17,343: Val batch 34000: PER (avg): 0.1097 CTC Loss (avg): 27.7547 WER(5gram): 12.26% (n=256) time: 24.000
2026-01-11 18:45:17,344: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 18:45:17,344: t15.2023.08.13 val PER: 0.0821
2026-01-11 18:45:17,345: t15.2023.08.18 val PER: 0.0679
2026-01-11 18:45:17,345: t15.2023.08.20 val PER: 0.0763
2026-01-11 18:45:17,345: t15.2023.08.25 val PER: 0.0708
2026-01-11 18:45:17,345: t15.2023.08.27 val PER: 0.1640
2026-01-11 18:45:17,346: t15.2023.09.01 val PER: 0.0487
2026-01-11 18:45:17,346: t15.2023.09.03 val PER: 0.1152
2026-01-11 18:45:17,346: t15.2023.09.24 val PER: 0.0910
2026-01-11 18:45:17,346: t15.2023.09.29 val PER: 0.1091
2026-01-11 18:45:17,346: t15.2023.10.01 val PER: 0.1486
2026-01-11 18:45:17,346: t15.2023.10.06 val PER: 0.0635
2026-01-11 18:45:17,347: t15.2023.10.08 val PER: 0.2030
2026-01-11 18:45:17,347: t15.2023.10.13 val PER: 0.1668
2026-01-11 18:45:17,347: t15.2023.10.15 val PER: 0.1074
2026-01-11 18:45:17,347: t15.2023.10.20 val PER: 0.1745
2026-01-11 18:45:17,347: t15.2023.10.22 val PER: 0.0913
2026-01-11 18:45:17,348: t15.2023.11.03 val PER: 0.1560
2026-01-11 18:45:17,348: t15.2023.11.04 val PER: 0.0102
2026-01-11 18:45:17,348: t15.2023.11.17 val PER: 0.0264
2026-01-11 18:45:17,348: t15.2023.11.19 val PER: 0.0140
2026-01-11 18:45:17,348: t15.2023.11.26 val PER: 0.0435
2026-01-11 18:45:17,348: t15.2023.12.03 val PER: 0.0557
2026-01-11 18:45:17,348: t15.2023.12.08 val PER: 0.0406
2026-01-11 18:45:17,349: t15.2023.12.10 val PER: 0.0302
2026-01-11 18:45:17,349: t15.2023.12.17 val PER: 0.0904
2026-01-11 18:45:17,349: t15.2023.12.29 val PER: 0.0762
2026-01-11 18:45:17,349: t15.2024.02.25 val PER: 0.0758
2026-01-11 18:45:17,349: t15.2024.03.08 val PER: 0.1650
2026-01-11 18:45:17,349: t15.2024.03.15 val PER: 0.1639
2026-01-11 18:45:17,349: t15.2024.03.17 val PER: 0.0788
2026-01-11 18:45:17,349: t15.2024.05.10 val PER: 0.1278
2026-01-11 18:45:17,349: t15.2024.06.14 val PER: 0.1215
2026-01-11 18:45:17,349: t15.2024.07.19 val PER: 0.1806
2026-01-11 18:45:17,350: t15.2024.07.21 val PER: 0.0648
2026-01-11 18:45:17,350: t15.2024.07.28 val PER: 0.0897
2026-01-11 18:45:17,350: t15.2025.01.10 val PER: 0.2479
2026-01-11 18:45:17,350: t15.2025.01.12 val PER: 0.0924
2026-01-11 18:45:17,350: t15.2025.03.14 val PER: 0.2899
2026-01-11 18:45:17,350: t15.2025.03.16 val PER: 0.1361
2026-01-11 18:45:17,350: t15.2025.03.30 val PER: 0.2207
2026-01-11 18:45:17,350: t15.2025.04.13 val PER: 0.2040
2026-01-11 18:45:17,520: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_34000
2026-01-11 18:45:38,525: Train batch 34200: loss: 1.17 grad norm: 31.38 time: 0.054
2026-01-11 18:45:59,274: Train batch 34400: loss: 1.19 grad norm: 29.64 time: 0.070
2026-01-11 18:46:09,916: Running test after training batch: 34500
2026-01-11 18:46:10,072: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:46:16,296: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:46:16,365: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:46:32,031: Val batch 34500: PER (avg): 0.1106 CTC Loss (avg): 28.6991 WER(5gram): 11.60% (n=256) time: 22.114
2026-01-11 18:46:32,031: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 18:46:32,032: t15.2023.08.13 val PER: 0.0842
2026-01-11 18:46:32,032: t15.2023.08.18 val PER: 0.0645
2026-01-11 18:46:32,032: t15.2023.08.20 val PER: 0.0739
2026-01-11 18:46:32,032: t15.2023.08.25 val PER: 0.0783
2026-01-11 18:46:32,032: t15.2023.08.27 val PER: 0.1688
2026-01-11 18:46:32,032: t15.2023.09.01 val PER: 0.0479
2026-01-11 18:46:32,032: t15.2023.09.03 val PER: 0.1200
2026-01-11 18:46:32,032: t15.2023.09.24 val PER: 0.0898
2026-01-11 18:46:32,033: t15.2023.09.29 val PER: 0.1123
2026-01-11 18:46:32,033: t15.2023.10.01 val PER: 0.1420
2026-01-11 18:46:32,033: t15.2023.10.06 val PER: 0.0603
2026-01-11 18:46:32,033: t15.2023.10.08 val PER: 0.2124
2026-01-11 18:46:32,033: t15.2023.10.13 val PER: 0.1683
2026-01-11 18:46:32,033: t15.2023.10.15 val PER: 0.1173
2026-01-11 18:46:32,034: t15.2023.10.20 val PER: 0.1577
2026-01-11 18:46:32,034: t15.2023.10.22 val PER: 0.0891
2026-01-11 18:46:32,034: t15.2023.11.03 val PER: 0.1567
2026-01-11 18:46:32,034: t15.2023.11.04 val PER: 0.0102
2026-01-11 18:46:32,034: t15.2023.11.17 val PER: 0.0295
2026-01-11 18:46:32,034: t15.2023.11.19 val PER: 0.0080
2026-01-11 18:46:32,034: t15.2023.11.26 val PER: 0.0457
2026-01-11 18:46:32,034: t15.2023.12.03 val PER: 0.0546
2026-01-11 18:46:32,034: t15.2023.12.08 val PER: 0.0413
2026-01-11 18:46:32,035: t15.2023.12.10 val PER: 0.0342
2026-01-11 18:46:32,035: t15.2023.12.17 val PER: 0.0884
2026-01-11 18:46:32,035: t15.2023.12.29 val PER: 0.0741
2026-01-11 18:46:32,035: t15.2024.02.25 val PER: 0.0843
2026-01-11 18:46:32,035: t15.2024.03.08 val PER: 0.1764
2026-01-11 18:46:32,035: t15.2024.03.15 val PER: 0.1670
2026-01-11 18:46:32,035: t15.2024.03.17 val PER: 0.0921
2026-01-11 18:46:32,035: t15.2024.05.10 val PER: 0.1174
2026-01-11 18:46:32,035: t15.2024.06.14 val PER: 0.1278
2026-01-11 18:46:32,036: t15.2024.07.19 val PER: 0.1885
2026-01-11 18:46:32,036: t15.2024.07.21 val PER: 0.0634
2026-01-11 18:46:32,036: t15.2024.07.28 val PER: 0.0868
2026-01-11 18:46:32,036: t15.2025.01.10 val PER: 0.2603
2026-01-11 18:46:32,036: t15.2025.01.12 val PER: 0.0862
2026-01-11 18:46:32,036: t15.2025.03.14 val PER: 0.2944
2026-01-11 18:46:32,036: t15.2025.03.16 val PER: 0.1191
2026-01-11 18:46:32,036: t15.2025.03.30 val PER: 0.2253
2026-01-11 18:46:32,036: t15.2025.04.13 val PER: 0.1854
2026-01-11 18:46:32,201: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_34500
2026-01-11 18:46:42,419: Train batch 34600: loss: 1.39 grad norm: 34.00 time: 0.072
2026-01-11 18:47:03,325: Train batch 34800: loss: 0.76 grad norm: 22.96 time: 0.065
2026-01-11 18:47:23,757: Train batch 35000: loss: 1.80 grad norm: 37.64 time: 0.067
2026-01-11 18:47:23,757: Running test after training batch: 35000
2026-01-11 18:47:23,928: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:47:30,079: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:47:30,155: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:47:45,701: Val batch 35000: PER (avg): 0.1106 CTC Loss (avg): 28.8015 WER(5gram): 10.82% (n=256) time: 21.943
2026-01-11 18:47:45,701: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 18:47:45,701: t15.2023.08.13 val PER: 0.0884
2026-01-11 18:47:45,702: t15.2023.08.18 val PER: 0.0637
2026-01-11 18:47:45,702: t15.2023.08.20 val PER: 0.0667
2026-01-11 18:47:45,702: t15.2023.08.25 val PER: 0.0783
2026-01-11 18:47:45,702: t15.2023.08.27 val PER: 0.1624
2026-01-11 18:47:45,702: t15.2023.09.01 val PER: 0.0406
2026-01-11 18:47:45,702: t15.2023.09.03 val PER: 0.1152
2026-01-11 18:47:45,702: t15.2023.09.24 val PER: 0.0837
2026-01-11 18:47:45,702: t15.2023.09.29 val PER: 0.1110
2026-01-11 18:47:45,702: t15.2023.10.01 val PER: 0.1546
2026-01-11 18:47:45,702: t15.2023.10.06 val PER: 0.0624
2026-01-11 18:47:45,703: t15.2023.10.08 val PER: 0.2097
2026-01-11 18:47:45,703: t15.2023.10.13 val PER: 0.1668
2026-01-11 18:47:45,703: t15.2023.10.15 val PER: 0.1127
2026-01-11 18:47:45,703: t15.2023.10.20 val PER: 0.1577
2026-01-11 18:47:45,703: t15.2023.10.22 val PER: 0.1024
2026-01-11 18:47:45,703: t15.2023.11.03 val PER: 0.1554
2026-01-11 18:47:45,703: t15.2023.11.04 val PER: 0.0171
2026-01-11 18:47:45,703: t15.2023.11.17 val PER: 0.0249
2026-01-11 18:47:45,703: t15.2023.11.19 val PER: 0.0120
2026-01-11 18:47:45,703: t15.2023.11.26 val PER: 0.0464
2026-01-11 18:47:45,703: t15.2023.12.03 val PER: 0.0557
2026-01-11 18:47:45,704: t15.2023.12.08 val PER: 0.0399
2026-01-11 18:47:45,704: t15.2023.12.10 val PER: 0.0342
2026-01-11 18:47:45,704: t15.2023.12.17 val PER: 0.0915
2026-01-11 18:47:45,704: t15.2023.12.29 val PER: 0.0679
2026-01-11 18:47:45,704: t15.2024.02.25 val PER: 0.0787
2026-01-11 18:47:45,704: t15.2024.03.08 val PER: 0.1792
2026-01-11 18:47:45,704: t15.2024.03.15 val PER: 0.1695
2026-01-11 18:47:45,704: t15.2024.03.17 val PER: 0.0830
2026-01-11 18:47:45,704: t15.2024.05.10 val PER: 0.1248
2026-01-11 18:47:45,704: t15.2024.06.14 val PER: 0.1183
2026-01-11 18:47:45,704: t15.2024.07.19 val PER: 0.1740
2026-01-11 18:47:45,704: t15.2024.07.21 val PER: 0.0614
2026-01-11 18:47:45,705: t15.2024.07.28 val PER: 0.1015
2026-01-11 18:47:45,705: t15.2025.01.10 val PER: 0.2466
2026-01-11 18:47:45,705: t15.2025.01.12 val PER: 0.1016
2026-01-11 18:47:45,705: t15.2025.03.14 val PER: 0.3136
2026-01-11 18:47:45,705: t15.2025.03.16 val PER: 0.1440
2026-01-11 18:47:45,705: t15.2025.03.30 val PER: 0.2218
2026-01-11 18:47:45,705: t15.2025.04.13 val PER: 0.1797
2026-01-11 18:47:45,863: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_35000
2026-01-11 18:48:06,902: Train batch 35200: loss: 1.17 grad norm: 29.06 time: 0.076
2026-01-11 18:48:27,630: Train batch 35400: loss: 1.42 grad norm: 41.35 time: 0.095
2026-01-11 18:48:38,170: Running test after training batch: 35500
2026-01-11 18:48:38,307: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:48:44,556: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:48:44,631: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:48:59,874: Val batch 35500: PER (avg): 0.1088 CTC Loss (avg): 28.2650 WER(5gram): 14.02% (n=256) time: 21.703
2026-01-11 18:48:59,875: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-11 18:48:59,875: t15.2023.08.13 val PER: 0.0863
2026-01-11 18:48:59,875: t15.2023.08.18 val PER: 0.0712
2026-01-11 18:48:59,875: t15.2023.08.20 val PER: 0.0683
2026-01-11 18:48:59,875: t15.2023.08.25 val PER: 0.0648
2026-01-11 18:48:59,875: t15.2023.08.27 val PER: 0.1720
2026-01-11 18:48:59,876: t15.2023.09.01 val PER: 0.0430
2026-01-11 18:48:59,876: t15.2023.09.03 val PER: 0.1211
2026-01-11 18:48:59,876: t15.2023.09.24 val PER: 0.0934
2026-01-11 18:48:59,876: t15.2023.09.29 val PER: 0.1040
2026-01-11 18:48:59,876: t15.2023.10.01 val PER: 0.1513
2026-01-11 18:48:59,876: t15.2023.10.06 val PER: 0.0624
2026-01-11 18:48:59,876: t15.2023.10.08 val PER: 0.2206
2026-01-11 18:48:59,876: t15.2023.10.13 val PER: 0.1645
2026-01-11 18:48:59,877: t15.2023.10.15 val PER: 0.1094
2026-01-11 18:48:59,877: t15.2023.10.20 val PER: 0.1611
2026-01-11 18:48:59,877: t15.2023.10.22 val PER: 0.0947
2026-01-11 18:48:59,877: t15.2023.11.03 val PER: 0.1499
2026-01-11 18:48:59,877: t15.2023.11.04 val PER: 0.0137
2026-01-11 18:48:59,877: t15.2023.11.17 val PER: 0.0264
2026-01-11 18:48:59,877: t15.2023.11.19 val PER: 0.0220
2026-01-11 18:48:59,877: t15.2023.11.26 val PER: 0.0493
2026-01-11 18:48:59,877: t15.2023.12.03 val PER: 0.0525
2026-01-11 18:48:59,878: t15.2023.12.08 val PER: 0.0393
2026-01-11 18:48:59,878: t15.2023.12.10 val PER: 0.0342
2026-01-11 18:48:59,878: t15.2023.12.17 val PER: 0.0769
2026-01-11 18:48:59,878: t15.2023.12.29 val PER: 0.0762
2026-01-11 18:48:59,878: t15.2024.02.25 val PER: 0.0716
2026-01-11 18:48:59,878: t15.2024.03.08 val PER: 0.1607
2026-01-11 18:48:59,878: t15.2024.03.15 val PER: 0.1632
2026-01-11 18:48:59,878: t15.2024.03.17 val PER: 0.0865
2026-01-11 18:48:59,879: t15.2024.05.10 val PER: 0.1204
2026-01-11 18:48:59,879: t15.2024.06.14 val PER: 0.1183
2026-01-11 18:48:59,879: t15.2024.07.19 val PER: 0.1688
2026-01-11 18:48:59,879: t15.2024.07.21 val PER: 0.0655
2026-01-11 18:48:59,879: t15.2024.07.28 val PER: 0.0934
2026-01-11 18:48:59,879: t15.2025.01.10 val PER: 0.2534
2026-01-11 18:48:59,879: t15.2025.01.12 val PER: 0.0885
2026-01-11 18:48:59,879: t15.2025.03.14 val PER: 0.2781
2026-01-11 18:48:59,880: t15.2025.03.16 val PER: 0.1361
2026-01-11 18:48:59,880: t15.2025.03.30 val PER: 0.2172
2026-01-11 18:48:59,880: t15.2025.04.13 val PER: 0.1969
2026-01-11 18:49:00,033: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_35500
2026-01-11 18:49:10,777: Train batch 35600: loss: 1.04 grad norm: 30.31 time: 0.093
2026-01-11 18:49:32,650: Train batch 35800: loss: 0.38 grad norm: 13.85 time: 0.080
2026-01-11 18:49:53,391: Train batch 36000: loss: 1.28 grad norm: 27.34 time: 0.067
2026-01-11 18:49:53,392: Running test after training batch: 36000
2026-01-11 18:49:53,560: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:50:00,229: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:50:00,317: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:50:16,071: Val batch 36000: PER (avg): 0.1106 CTC Loss (avg): 28.8097 WER(5gram): 12.06% (n=256) time: 22.679
2026-01-11 18:50:16,072: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-11 18:50:16,072: t15.2023.08.13 val PER: 0.0852
2026-01-11 18:50:16,072: t15.2023.08.18 val PER: 0.0629
2026-01-11 18:50:16,072: t15.2023.08.20 val PER: 0.0707
2026-01-11 18:50:16,072: t15.2023.08.25 val PER: 0.0693
2026-01-11 18:50:16,073: t15.2023.08.27 val PER: 0.1576
2026-01-11 18:50:16,073: t15.2023.09.01 val PER: 0.0365
2026-01-11 18:50:16,073: t15.2023.09.03 val PER: 0.1259
2026-01-11 18:50:16,073: t15.2023.09.24 val PER: 0.0934
2026-01-11 18:50:16,073: t15.2023.09.29 val PER: 0.1078
2026-01-11 18:50:16,073: t15.2023.10.01 val PER: 0.1433
2026-01-11 18:50:16,073: t15.2023.10.06 val PER: 0.0721
2026-01-11 18:50:16,073: t15.2023.10.08 val PER: 0.2179
2026-01-11 18:50:16,073: t15.2023.10.13 val PER: 0.1683
2026-01-11 18:50:16,074: t15.2023.10.15 val PER: 0.1107
2026-01-11 18:50:16,074: t15.2023.10.20 val PER: 0.1678
2026-01-11 18:50:16,074: t15.2023.10.22 val PER: 0.0947
2026-01-11 18:50:16,075: t15.2023.11.03 val PER: 0.1642
2026-01-11 18:50:16,075: t15.2023.11.04 val PER: 0.0068
2026-01-11 18:50:16,075: t15.2023.11.17 val PER: 0.0264
2026-01-11 18:50:16,075: t15.2023.11.19 val PER: 0.0160
2026-01-11 18:50:16,075: t15.2023.11.26 val PER: 0.0493
2026-01-11 18:50:16,075: t15.2023.12.03 val PER: 0.0620
2026-01-11 18:50:16,075: t15.2023.12.08 val PER: 0.0373
2026-01-11 18:50:16,075: t15.2023.12.10 val PER: 0.0355
2026-01-11 18:50:16,075: t15.2023.12.17 val PER: 0.0832
2026-01-11 18:50:16,076: t15.2023.12.29 val PER: 0.0714
2026-01-11 18:50:16,076: t15.2024.02.25 val PER: 0.0702
2026-01-11 18:50:16,076: t15.2024.03.08 val PER: 0.1721
2026-01-11 18:50:16,076: t15.2024.03.15 val PER: 0.1720
2026-01-11 18:50:16,076: t15.2024.03.17 val PER: 0.0927
2026-01-11 18:50:16,076: t15.2024.05.10 val PER: 0.1233
2026-01-11 18:50:16,076: t15.2024.06.14 val PER: 0.1136
2026-01-11 18:50:16,077: t15.2024.07.19 val PER: 0.1767
2026-01-11 18:50:16,077: t15.2024.07.21 val PER: 0.0572
2026-01-11 18:50:16,077: t15.2024.07.28 val PER: 0.0941
2026-01-11 18:50:16,077: t15.2025.01.10 val PER: 0.2645
2026-01-11 18:50:16,077: t15.2025.01.12 val PER: 0.0962
2026-01-11 18:50:16,077: t15.2025.03.14 val PER: 0.3033
2026-01-11 18:50:16,077: t15.2025.03.16 val PER: 0.1296
2026-01-11 18:50:16,078: t15.2025.03.30 val PER: 0.2172
2026-01-11 18:50:16,078: t15.2025.04.13 val PER: 0.1983
2026-01-11 18:50:16,254: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_36000
2026-01-11 18:50:37,295: Train batch 36200: loss: 1.32 grad norm: 35.31 time: 0.082
2026-01-11 18:50:58,283: Train batch 36400: loss: 0.90 grad norm: 34.16 time: 0.101
2026-01-11 18:51:08,438: Running test after training batch: 36500
2026-01-11 18:51:08,717: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:51:14,825: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:51:14,898: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:51:30,165: Val batch 36500: PER (avg): 0.1109 CTC Loss (avg): 28.7443 WER(5gram): 12.13% (n=256) time: 21.727
2026-01-11 18:51:30,166: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-11 18:51:30,166: t15.2023.08.13 val PER: 0.0852
2026-01-11 18:51:30,167: t15.2023.08.18 val PER: 0.0696
2026-01-11 18:51:30,167: t15.2023.08.20 val PER: 0.0707
2026-01-11 18:51:30,167: t15.2023.08.25 val PER: 0.0768
2026-01-11 18:51:30,167: t15.2023.08.27 val PER: 0.1656
2026-01-11 18:51:30,167: t15.2023.09.01 val PER: 0.0422
2026-01-11 18:51:30,167: t15.2023.09.03 val PER: 0.1223
2026-01-11 18:51:30,167: t15.2023.09.24 val PER: 0.0716
2026-01-11 18:51:30,167: t15.2023.09.29 val PER: 0.1078
2026-01-11 18:51:30,168: t15.2023.10.01 val PER: 0.1499
2026-01-11 18:51:30,168: t15.2023.10.06 val PER: 0.0732
2026-01-11 18:51:30,168: t15.2023.10.08 val PER: 0.2165
2026-01-11 18:51:30,168: t15.2023.10.13 val PER: 0.1691
2026-01-11 18:51:30,168: t15.2023.10.15 val PER: 0.1094
2026-01-11 18:51:30,168: t15.2023.10.20 val PER: 0.1644
2026-01-11 18:51:30,168: t15.2023.10.22 val PER: 0.0935
2026-01-11 18:51:30,168: t15.2023.11.03 val PER: 0.1554
2026-01-11 18:51:30,169: t15.2023.11.04 val PER: 0.0171
2026-01-11 18:51:30,169: t15.2023.11.17 val PER: 0.0218
2026-01-11 18:51:30,169: t15.2023.11.19 val PER: 0.0180
2026-01-11 18:51:30,169: t15.2023.11.26 val PER: 0.0442
2026-01-11 18:51:30,169: t15.2023.12.03 val PER: 0.0567
2026-01-11 18:51:30,169: t15.2023.12.08 val PER: 0.0406
2026-01-11 18:51:30,169: t15.2023.12.10 val PER: 0.0368
2026-01-11 18:51:30,169: t15.2023.12.17 val PER: 0.0832
2026-01-11 18:51:30,169: t15.2023.12.29 val PER: 0.0782
2026-01-11 18:51:30,170: t15.2024.02.25 val PER: 0.0702
2026-01-11 18:51:30,170: t15.2024.03.08 val PER: 0.1764
2026-01-11 18:51:30,170: t15.2024.03.15 val PER: 0.1689
2026-01-11 18:51:30,170: t15.2024.03.17 val PER: 0.0934
2026-01-11 18:51:30,170: t15.2024.05.10 val PER: 0.1233
2026-01-11 18:51:30,170: t15.2024.06.14 val PER: 0.1278
2026-01-11 18:51:30,170: t15.2024.07.19 val PER: 0.1753
2026-01-11 18:51:30,170: t15.2024.07.21 val PER: 0.0641
2026-01-11 18:51:30,170: t15.2024.07.28 val PER: 0.0993
2026-01-11 18:51:30,171: t15.2025.01.10 val PER: 0.2548
2026-01-11 18:51:30,171: t15.2025.01.12 val PER: 0.1016
2026-01-11 18:51:30,171: t15.2025.03.14 val PER: 0.3003
2026-01-11 18:51:30,171: t15.2025.03.16 val PER: 0.1243
2026-01-11 18:51:30,171: t15.2025.03.30 val PER: 0.2287
2026-01-11 18:51:30,171: t15.2025.04.13 val PER: 0.1769
2026-01-11 18:51:30,324: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_36500
2026-01-11 18:51:40,892: Train batch 36600: loss: 1.47 grad norm: 41.63 time: 0.090
2026-01-11 18:52:01,808: Train batch 36800: loss: 1.90 grad norm: 35.20 time: 0.065
2026-01-11 18:52:23,692: Train batch 37000: loss: 0.83 grad norm: 27.33 time: 0.079
2026-01-11 18:52:23,693: Running test after training batch: 37000
2026-01-11 18:52:23,809: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:52:29,906: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:52:29,982: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:52:45,322: Val batch 37000: PER (avg): 0.1106 CTC Loss (avg): 29.6120 WER(5gram): 11.08% (n=256) time: 21.629
2026-01-11 18:52:45,322: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 18:52:45,323: t15.2023.08.13 val PER: 0.0884
2026-01-11 18:52:45,323: t15.2023.08.18 val PER: 0.0629
2026-01-11 18:52:45,323: t15.2023.08.20 val PER: 0.0580
2026-01-11 18:52:45,324: t15.2023.08.25 val PER: 0.0828
2026-01-11 18:52:45,324: t15.2023.08.27 val PER: 0.1559
2026-01-11 18:52:45,324: t15.2023.09.01 val PER: 0.0422
2026-01-11 18:52:45,324: t15.2023.09.03 val PER: 0.1235
2026-01-11 18:52:45,324: t15.2023.09.24 val PER: 0.0910
2026-01-11 18:52:45,324: t15.2023.09.29 val PER: 0.1027
2026-01-11 18:52:45,324: t15.2023.10.01 val PER: 0.1446
2026-01-11 18:52:45,324: t15.2023.10.06 val PER: 0.0624
2026-01-11 18:52:45,325: t15.2023.10.08 val PER: 0.2165
2026-01-11 18:52:45,325: t15.2023.10.13 val PER: 0.1645
2026-01-11 18:52:45,325: t15.2023.10.15 val PER: 0.1140
2026-01-11 18:52:45,325: t15.2023.10.20 val PER: 0.1879
2026-01-11 18:52:45,325: t15.2023.10.22 val PER: 0.0969
2026-01-11 18:52:45,325: t15.2023.11.03 val PER: 0.1547
2026-01-11 18:52:45,325: t15.2023.11.04 val PER: 0.0137
2026-01-11 18:52:45,325: t15.2023.11.17 val PER: 0.0264
2026-01-11 18:52:45,325: t15.2023.11.19 val PER: 0.0100
2026-01-11 18:52:45,325: t15.2023.11.26 val PER: 0.0514
2026-01-11 18:52:45,326: t15.2023.12.03 val PER: 0.0651
2026-01-11 18:52:45,326: t15.2023.12.08 val PER: 0.0433
2026-01-11 18:52:45,326: t15.2023.12.10 val PER: 0.0460
2026-01-11 18:52:45,326: t15.2023.12.17 val PER: 0.0780
2026-01-11 18:52:45,326: t15.2023.12.29 val PER: 0.0707
2026-01-11 18:52:45,326: t15.2024.02.25 val PER: 0.0758
2026-01-11 18:52:45,326: t15.2024.03.08 val PER: 0.1707
2026-01-11 18:52:45,326: t15.2024.03.15 val PER: 0.1770
2026-01-11 18:52:45,326: t15.2024.03.17 val PER: 0.0788
2026-01-11 18:52:45,327: t15.2024.05.10 val PER: 0.1218
2026-01-11 18:52:45,327: t15.2024.06.14 val PER: 0.1104
2026-01-11 18:52:45,327: t15.2024.07.19 val PER: 0.1806
2026-01-11 18:52:45,327: t15.2024.07.21 val PER: 0.0628
2026-01-11 18:52:45,327: t15.2024.07.28 val PER: 0.0941
2026-01-11 18:52:45,327: t15.2025.01.10 val PER: 0.2507
2026-01-11 18:52:45,327: t15.2025.01.12 val PER: 0.1001
2026-01-11 18:52:45,327: t15.2025.03.14 val PER: 0.3062
2026-01-11 18:52:45,328: t15.2025.03.16 val PER: 0.1414
2026-01-11 18:52:45,328: t15.2025.03.30 val PER: 0.2207
2026-01-11 18:52:45,328: t15.2025.04.13 val PER: 0.1926
2026-01-11 18:52:45,481: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_37000
2026-01-11 18:53:06,828: Train batch 37200: loss: 0.85 grad norm: 25.60 time: 0.076
2026-01-11 18:53:27,831: Train batch 37400: loss: 1.29 grad norm: 32.40 time: 0.094
2026-01-11 18:53:38,031: Running test after training batch: 37500
2026-01-11 18:53:38,206: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:53:44,311: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:53:44,378: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:53:59,262: Val batch 37500: PER (avg): 0.1096 CTC Loss (avg): 28.9457 WER(5gram): 11.67% (n=256) time: 21.230
2026-01-11 18:53:59,263: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 18:53:59,263: t15.2023.08.13 val PER: 0.0863
2026-01-11 18:53:59,263: t15.2023.08.18 val PER: 0.0645
2026-01-11 18:53:59,264: t15.2023.08.20 val PER: 0.0651
2026-01-11 18:53:59,264: t15.2023.08.25 val PER: 0.0783
2026-01-11 18:53:59,264: t15.2023.08.27 val PER: 0.1608
2026-01-11 18:53:59,264: t15.2023.09.01 val PER: 0.0398
2026-01-11 18:53:59,264: t15.2023.09.03 val PER: 0.1223
2026-01-11 18:53:59,264: t15.2023.09.24 val PER: 0.0777
2026-01-11 18:53:59,264: t15.2023.09.29 val PER: 0.1040
2026-01-11 18:53:59,264: t15.2023.10.01 val PER: 0.1420
2026-01-11 18:53:59,264: t15.2023.10.06 val PER: 0.0592
2026-01-11 18:53:59,264: t15.2023.10.08 val PER: 0.2084
2026-01-11 18:53:59,265: t15.2023.10.13 val PER: 0.1730
2026-01-11 18:53:59,265: t15.2023.10.15 val PER: 0.1094
2026-01-11 18:53:59,265: t15.2023.10.20 val PER: 0.1846
2026-01-11 18:53:59,265: t15.2023.10.22 val PER: 0.0935
2026-01-11 18:53:59,265: t15.2023.11.03 val PER: 0.1452
2026-01-11 18:53:59,265: t15.2023.11.04 val PER: 0.0068
2026-01-11 18:53:59,265: t15.2023.11.17 val PER: 0.0233
2026-01-11 18:53:59,266: t15.2023.11.19 val PER: 0.0160
2026-01-11 18:53:59,266: t15.2023.11.26 val PER: 0.0449
2026-01-11 18:53:59,266: t15.2023.12.03 val PER: 0.0515
2026-01-11 18:53:59,266: t15.2023.12.08 val PER: 0.0399
2026-01-11 18:53:59,266: t15.2023.12.10 val PER: 0.0342
2026-01-11 18:53:59,266: t15.2023.12.17 val PER: 0.0915
2026-01-11 18:53:59,266: t15.2023.12.29 val PER: 0.0693
2026-01-11 18:53:59,266: t15.2024.02.25 val PER: 0.0787
2026-01-11 18:53:59,266: t15.2024.03.08 val PER: 0.1750
2026-01-11 18:53:59,267: t15.2024.03.15 val PER: 0.1714
2026-01-11 18:53:59,267: t15.2024.03.17 val PER: 0.0858
2026-01-11 18:53:59,267: t15.2024.05.10 val PER: 0.1159
2026-01-11 18:53:59,267: t15.2024.06.14 val PER: 0.1199
2026-01-11 18:53:59,267: t15.2024.07.19 val PER: 0.1793
2026-01-11 18:53:59,267: t15.2024.07.21 val PER: 0.0648
2026-01-11 18:53:59,267: t15.2024.07.28 val PER: 0.0971
2026-01-11 18:53:59,267: t15.2025.01.10 val PER: 0.2603
2026-01-11 18:53:59,267: t15.2025.01.12 val PER: 0.0978
2026-01-11 18:53:59,267: t15.2025.03.14 val PER: 0.3136
2026-01-11 18:53:59,268: t15.2025.03.16 val PER: 0.1309
2026-01-11 18:53:59,268: t15.2025.03.30 val PER: 0.2207
2026-01-11 18:53:59,268: t15.2025.04.13 val PER: 0.1997
2026-01-11 18:53:59,423: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_37500
2026-01-11 18:54:10,006: Train batch 37600: loss: 0.73 grad norm: 30.61 time: 0.065
2026-01-11 18:54:31,321: Train batch 37800: loss: 1.82 grad norm: 40.16 time: 0.107
2026-01-11 18:54:52,628: Train batch 38000: loss: 2.01 grad norm: 49.86 time: 0.093
2026-01-11 18:54:52,628: Running test after training batch: 38000
2026-01-11 18:54:53,007: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:54:59,124: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:54:59,202: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:55:15,562: Val batch 38000: PER (avg): 0.1097 CTC Loss (avg): 29.2444 WER(5gram): 10.63% (n=256) time: 22.934
2026-01-11 18:55:15,563: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 18:55:15,563: t15.2023.08.13 val PER: 0.0884
2026-01-11 18:55:15,563: t15.2023.08.18 val PER: 0.0645
2026-01-11 18:55:15,564: t15.2023.08.20 val PER: 0.0707
2026-01-11 18:55:15,564: t15.2023.08.25 val PER: 0.0678
2026-01-11 18:55:15,564: t15.2023.08.27 val PER: 0.1640
2026-01-11 18:55:15,564: t15.2023.09.01 val PER: 0.0463
2026-01-11 18:55:15,564: t15.2023.09.03 val PER: 0.1128
2026-01-11 18:55:15,564: t15.2023.09.24 val PER: 0.0850
2026-01-11 18:55:15,565: t15.2023.09.29 val PER: 0.1053
2026-01-11 18:55:15,565: t15.2023.10.01 val PER: 0.1446
2026-01-11 18:55:15,565: t15.2023.10.06 val PER: 0.0571
2026-01-11 18:55:15,565: t15.2023.10.08 val PER: 0.2097
2026-01-11 18:55:15,565: t15.2023.10.13 val PER: 0.1668
2026-01-11 18:55:15,565: t15.2023.10.15 val PER: 0.1074
2026-01-11 18:55:15,565: t15.2023.10.20 val PER: 0.1711
2026-01-11 18:55:15,566: t15.2023.10.22 val PER: 0.0869
2026-01-11 18:55:15,566: t15.2023.11.03 val PER: 0.1581
2026-01-11 18:55:15,566: t15.2023.11.04 val PER: 0.0068
2026-01-11 18:55:15,566: t15.2023.11.17 val PER: 0.0218
2026-01-11 18:55:15,566: t15.2023.11.19 val PER: 0.0120
2026-01-11 18:55:15,566: t15.2023.11.26 val PER: 0.0500
2026-01-11 18:55:15,566: t15.2023.12.03 val PER: 0.0578
2026-01-11 18:55:15,566: t15.2023.12.08 val PER: 0.0373
2026-01-11 18:55:15,567: t15.2023.12.10 val PER: 0.0315
2026-01-11 18:55:15,567: t15.2023.12.17 val PER: 0.0925
2026-01-11 18:55:15,567: t15.2023.12.29 val PER: 0.0796
2026-01-11 18:55:15,567: t15.2024.02.25 val PER: 0.0787
2026-01-11 18:55:15,567: t15.2024.03.08 val PER: 0.1807
2026-01-11 18:55:15,567: t15.2024.03.15 val PER: 0.1776
2026-01-11 18:55:15,567: t15.2024.03.17 val PER: 0.0872
2026-01-11 18:55:15,567: t15.2024.05.10 val PER: 0.1278
2026-01-11 18:55:15,567: t15.2024.06.14 val PER: 0.1215
2026-01-11 18:55:15,568: t15.2024.07.19 val PER: 0.1740
2026-01-11 18:55:15,568: t15.2024.07.21 val PER: 0.0586
2026-01-11 18:55:15,568: t15.2024.07.28 val PER: 0.0919
2026-01-11 18:55:15,568: t15.2025.01.10 val PER: 0.2590
2026-01-11 18:55:15,568: t15.2025.01.12 val PER: 0.0939
2026-01-11 18:55:15,568: t15.2025.03.14 val PER: 0.3003
2026-01-11 18:55:15,568: t15.2025.03.16 val PER: 0.1178
2026-01-11 18:55:15,568: t15.2025.03.30 val PER: 0.2230
2026-01-11 18:55:15,569: t15.2025.04.13 val PER: 0.1883
2026-01-11 18:55:15,716: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_38000
2026-01-11 18:55:36,563: Train batch 38200: loss: 1.70 grad norm: 38.89 time: 0.108
2026-01-11 18:55:57,131: Train batch 38400: loss: 3.80 grad norm: 40.99 time: 0.089
2026-01-11 18:56:07,620: Running test after training batch: 38500
2026-01-11 18:56:07,779: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:56:13,892: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:56:13,959: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cent
2026-01-11 18:56:29,422: Val batch 38500: PER (avg): 0.1108 CTC Loss (avg): 29.4242 WER(5gram): 12.65% (n=256) time: 21.801
2026-01-11 18:56:29,423: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 18:56:29,423: t15.2023.08.13 val PER: 0.0925
2026-01-11 18:56:29,423: t15.2023.08.18 val PER: 0.0729
2026-01-11 18:56:29,423: t15.2023.08.20 val PER: 0.0691
2026-01-11 18:56:29,424: t15.2023.08.25 val PER: 0.0738
2026-01-11 18:56:29,424: t15.2023.08.27 val PER: 0.1608
2026-01-11 18:56:29,424: t15.2023.09.01 val PER: 0.0446
2026-01-11 18:56:29,424: t15.2023.09.03 val PER: 0.1306
2026-01-11 18:56:29,424: t15.2023.09.24 val PER: 0.0850
2026-01-11 18:56:29,424: t15.2023.09.29 val PER: 0.1053
2026-01-11 18:56:29,424: t15.2023.10.01 val PER: 0.1374
2026-01-11 18:56:29,424: t15.2023.10.06 val PER: 0.0614
2026-01-11 18:56:29,424: t15.2023.10.08 val PER: 0.2260
2026-01-11 18:56:29,425: t15.2023.10.13 val PER: 0.1676
2026-01-11 18:56:29,425: t15.2023.10.15 val PER: 0.1140
2026-01-11 18:56:29,425: t15.2023.10.20 val PER: 0.1745
2026-01-11 18:56:29,425: t15.2023.10.22 val PER: 0.0924
2026-01-11 18:56:29,425: t15.2023.11.03 val PER: 0.1608
2026-01-11 18:56:29,425: t15.2023.11.04 val PER: 0.0068
2026-01-11 18:56:29,425: t15.2023.11.17 val PER: 0.0218
2026-01-11 18:56:29,426: t15.2023.11.19 val PER: 0.0140
2026-01-11 18:56:29,426: t15.2023.11.26 val PER: 0.0457
2026-01-11 18:56:29,426: t15.2023.12.03 val PER: 0.0536
2026-01-11 18:56:29,426: t15.2023.12.08 val PER: 0.0373
2026-01-11 18:56:29,426: t15.2023.12.10 val PER: 0.0237
2026-01-11 18:56:29,426: t15.2023.12.17 val PER: 0.0821
2026-01-11 18:56:29,426: t15.2023.12.29 val PER: 0.0776
2026-01-11 18:56:29,426: t15.2024.02.25 val PER: 0.0716
2026-01-11 18:56:29,426: t15.2024.03.08 val PER: 0.1792
2026-01-11 18:56:29,427: t15.2024.03.15 val PER: 0.1764
2026-01-11 18:56:29,427: t15.2024.03.17 val PER: 0.0830
2026-01-11 18:56:29,427: t15.2024.05.10 val PER: 0.1308
2026-01-11 18:56:29,427: t15.2024.06.14 val PER: 0.1372
2026-01-11 18:56:29,427: t15.2024.07.19 val PER: 0.1819
2026-01-11 18:56:29,427: t15.2024.07.21 val PER: 0.0531
2026-01-11 18:56:29,427: t15.2024.07.28 val PER: 0.0949
2026-01-11 18:56:29,427: t15.2025.01.10 val PER: 0.2590
2026-01-11 18:56:29,428: t15.2025.01.12 val PER: 0.0931
2026-01-11 18:56:29,428: t15.2025.03.14 val PER: 0.2973
2026-01-11 18:56:29,428: t15.2025.03.16 val PER: 0.1230
2026-01-11 18:56:29,428: t15.2025.03.30 val PER: 0.2333
2026-01-11 18:56:29,428: t15.2025.04.13 val PER: 0.1969
2026-01-11 18:56:29,575: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_38500
2026-01-11 18:56:39,992: Train batch 38600: loss: 1.48 grad norm: 29.59 time: 0.081
2026-01-11 18:57:00,825: Train batch 38800: loss: 1.31 grad norm: 36.61 time: 0.068
2026-01-11 18:57:22,009: Train batch 39000: loss: 0.86 grad norm: 37.83 time: 0.096
2026-01-11 18:57:22,010: Running test after training batch: 39000
2026-01-11 18:57:22,159: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:57:28,240: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:57:28,313: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:57:43,551: Val batch 39000: PER (avg): 0.1082 CTC Loss (avg): 29.3524 WER(5gram): 10.82% (n=256) time: 21.541
2026-01-11 18:57:43,551: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=13
2026-01-11 18:57:43,551: t15.2023.08.13 val PER: 0.0811
2026-01-11 18:57:43,552: t15.2023.08.18 val PER: 0.0654
2026-01-11 18:57:43,552: t15.2023.08.20 val PER: 0.0659
2026-01-11 18:57:43,552: t15.2023.08.25 val PER: 0.0648
2026-01-11 18:57:43,552: t15.2023.08.27 val PER: 0.1640
2026-01-11 18:57:43,552: t15.2023.09.01 val PER: 0.0422
2026-01-11 18:57:43,552: t15.2023.09.03 val PER: 0.1093
2026-01-11 18:57:43,552: t15.2023.09.24 val PER: 0.0862
2026-01-11 18:57:43,553: t15.2023.09.29 val PER: 0.1059
2026-01-11 18:57:43,553: t15.2023.10.01 val PER: 0.1427
2026-01-11 18:57:43,553: t15.2023.10.06 val PER: 0.0571
2026-01-11 18:57:43,553: t15.2023.10.08 val PER: 0.2206
2026-01-11 18:57:43,553: t15.2023.10.13 val PER: 0.1683
2026-01-11 18:57:43,553: t15.2023.10.15 val PER: 0.1147
2026-01-11 18:57:43,553: t15.2023.10.20 val PER: 0.1544
2026-01-11 18:57:43,554: t15.2023.10.22 val PER: 0.0935
2026-01-11 18:57:43,554: t15.2023.11.03 val PER: 0.1621
2026-01-11 18:57:43,554: t15.2023.11.04 val PER: 0.0068
2026-01-11 18:57:43,554: t15.2023.11.17 val PER: 0.0187
2026-01-11 18:57:43,554: t15.2023.11.19 val PER: 0.0080
2026-01-11 18:57:43,554: t15.2023.11.26 val PER: 0.0514
2026-01-11 18:57:43,554: t15.2023.12.03 val PER: 0.0546
2026-01-11 18:57:43,554: t15.2023.12.08 val PER: 0.0406
2026-01-11 18:57:43,554: t15.2023.12.10 val PER: 0.0315
2026-01-11 18:57:43,555: t15.2023.12.17 val PER: 0.0894
2026-01-11 18:57:43,555: t15.2023.12.29 val PER: 0.0700
2026-01-11 18:57:43,555: t15.2024.02.25 val PER: 0.0730
2026-01-11 18:57:43,555: t15.2024.03.08 val PER: 0.1863
2026-01-11 18:57:43,555: t15.2024.03.15 val PER: 0.1657
2026-01-11 18:57:43,555: t15.2024.03.17 val PER: 0.0879
2026-01-11 18:57:43,555: t15.2024.05.10 val PER: 0.1278
2026-01-11 18:57:43,555: t15.2024.06.14 val PER: 0.1120
2026-01-11 18:57:43,555: t15.2024.07.19 val PER: 0.1661
2026-01-11 18:57:43,556: t15.2024.07.21 val PER: 0.0607
2026-01-11 18:57:43,556: t15.2024.07.28 val PER: 0.0868
2026-01-11 18:57:43,556: t15.2025.01.10 val PER: 0.2645
2026-01-11 18:57:43,556: t15.2025.01.12 val PER: 0.0931
2026-01-11 18:57:43,556: t15.2025.03.14 val PER: 0.2840
2026-01-11 18:57:43,556: t15.2025.03.16 val PER: 0.1217
2026-01-11 18:57:43,556: t15.2025.03.30 val PER: 0.2218
2026-01-11 18:57:43,556: t15.2025.04.13 val PER: 0.1812
2026-01-11 18:57:43,705: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_39000
2026-01-11 18:58:04,800: Train batch 39200: loss: 0.99 grad norm: 24.14 time: 0.073
2026-01-11 18:58:26,386: Train batch 39400: loss: 0.97 grad norm: 26.32 time: 0.074
2026-01-11 18:58:37,271: Running test after training batch: 39500
2026-01-11 18:58:37,401: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:58:43,530: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:58:43,591: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 18:58:59,413: Val batch 39500: PER (avg): 0.1085 CTC Loss (avg): 29.1719 WER(5gram): 11.41% (n=256) time: 22.141
2026-01-11 18:58:59,414: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 18:58:59,414: t15.2023.08.13 val PER: 0.0852
2026-01-11 18:58:59,414: t15.2023.08.18 val PER: 0.0604
2026-01-11 18:58:59,414: t15.2023.08.20 val PER: 0.0675
2026-01-11 18:58:59,414: t15.2023.08.25 val PER: 0.0693
2026-01-11 18:58:59,414: t15.2023.08.27 val PER: 0.1592
2026-01-11 18:58:59,415: t15.2023.09.01 val PER: 0.0438
2026-01-11 18:58:59,415: t15.2023.09.03 val PER: 0.1235
2026-01-11 18:58:59,415: t15.2023.09.24 val PER: 0.0801
2026-01-11 18:58:59,415: t15.2023.09.29 val PER: 0.1078
2026-01-11 18:58:59,415: t15.2023.10.01 val PER: 0.1453
2026-01-11 18:58:59,415: t15.2023.10.06 val PER: 0.0560
2026-01-11 18:58:59,415: t15.2023.10.08 val PER: 0.2219
2026-01-11 18:58:59,415: t15.2023.10.13 val PER: 0.1621
2026-01-11 18:58:59,415: t15.2023.10.15 val PER: 0.1035
2026-01-11 18:58:59,416: t15.2023.10.20 val PER: 0.1611
2026-01-11 18:58:59,416: t15.2023.10.22 val PER: 0.0947
2026-01-11 18:58:59,416: t15.2023.11.03 val PER: 0.1615
2026-01-11 18:58:59,416: t15.2023.11.04 val PER: 0.0102
2026-01-11 18:58:59,416: t15.2023.11.17 val PER: 0.0202
2026-01-11 18:58:59,416: t15.2023.11.19 val PER: 0.0120
2026-01-11 18:58:59,416: t15.2023.11.26 val PER: 0.0493
2026-01-11 18:58:59,416: t15.2023.12.03 val PER: 0.0567
2026-01-11 18:58:59,416: t15.2023.12.08 val PER: 0.0360
2026-01-11 18:58:59,416: t15.2023.12.10 val PER: 0.0342
2026-01-11 18:58:59,417: t15.2023.12.17 val PER: 0.0800
2026-01-11 18:58:59,417: t15.2023.12.29 val PER: 0.0755
2026-01-11 18:58:59,417: t15.2024.02.25 val PER: 0.0730
2026-01-11 18:58:59,417: t15.2024.03.08 val PER: 0.1721
2026-01-11 18:58:59,417: t15.2024.03.15 val PER: 0.1676
2026-01-11 18:58:59,417: t15.2024.03.17 val PER: 0.0858
2026-01-11 18:58:59,417: t15.2024.05.10 val PER: 0.1263
2026-01-11 18:58:59,417: t15.2024.06.14 val PER: 0.1246
2026-01-11 18:58:59,417: t15.2024.07.19 val PER: 0.1648
2026-01-11 18:58:59,417: t15.2024.07.21 val PER: 0.0655
2026-01-11 18:58:59,418: t15.2024.07.28 val PER: 0.0934
2026-01-11 18:58:59,418: t15.2025.01.10 val PER: 0.2479
2026-01-11 18:58:59,418: t15.2025.01.12 val PER: 0.0870
2026-01-11 18:58:59,418: t15.2025.03.14 val PER: 0.2914
2026-01-11 18:58:59,418: t15.2025.03.16 val PER: 0.1401
2026-01-11 18:58:59,418: t15.2025.03.30 val PER: 0.2345
2026-01-11 18:58:59,418: t15.2025.04.13 val PER: 0.1854
2026-01-11 18:58:59,572: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_39500
2026-01-11 18:59:09,714: Train batch 39600: loss: 0.83 grad norm: 23.98 time: 0.072
2026-01-11 18:59:30,469: Train batch 39800: loss: 0.97 grad norm: 27.60 time: 0.061
2026-01-11 18:59:51,561: Train batch 40000: loss: 0.53 grad norm: 23.98 time: 0.077
2026-01-11 18:59:51,561: Running test after training batch: 40000
2026-01-11 18:59:51,727: WER debug GT example: You can see the code at this point as well.
2026-01-11 18:59:57,805: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 18:59:57,876: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:00:13,994: Val batch 40000: PER (avg): 0.1094 CTC Loss (avg): 29.6902 WER(5gram): 11.34% (n=256) time: 22.432
2026-01-11 19:00:13,995: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 19:00:13,995: t15.2023.08.13 val PER: 0.0832
2026-01-11 19:00:13,995: t15.2023.08.18 val PER: 0.0637
2026-01-11 19:00:13,995: t15.2023.08.20 val PER: 0.0675
2026-01-11 19:00:13,995: t15.2023.08.25 val PER: 0.0648
2026-01-11 19:00:13,995: t15.2023.08.27 val PER: 0.1624
2026-01-11 19:00:13,996: t15.2023.09.01 val PER: 0.0446
2026-01-11 19:00:13,996: t15.2023.09.03 val PER: 0.1176
2026-01-11 19:00:13,996: t15.2023.09.24 val PER: 0.0910
2026-01-11 19:00:13,996: t15.2023.09.29 val PER: 0.1040
2026-01-11 19:00:13,996: t15.2023.10.01 val PER: 0.1427
2026-01-11 19:00:13,997: t15.2023.10.06 val PER: 0.0635
2026-01-11 19:00:13,997: t15.2023.10.08 val PER: 0.2206
2026-01-11 19:00:13,997: t15.2023.10.13 val PER: 0.1769
2026-01-11 19:00:13,997: t15.2023.10.15 val PER: 0.1088
2026-01-11 19:00:13,998: t15.2023.10.20 val PER: 0.1678
2026-01-11 19:00:13,998: t15.2023.10.22 val PER: 0.0980
2026-01-11 19:00:13,998: t15.2023.11.03 val PER: 0.1493
2026-01-11 19:00:13,998: t15.2023.11.04 val PER: 0.0171
2026-01-11 19:00:13,999: t15.2023.11.17 val PER: 0.0202
2026-01-11 19:00:13,999: t15.2023.11.19 val PER: 0.0080
2026-01-11 19:00:13,999: t15.2023.11.26 val PER: 0.0471
2026-01-11 19:00:13,999: t15.2023.12.03 val PER: 0.0651
2026-01-11 19:00:13,999: t15.2023.12.08 val PER: 0.0366
2026-01-11 19:00:13,999: t15.2023.12.10 val PER: 0.0342
2026-01-11 19:00:13,999: t15.2023.12.17 val PER: 0.0915
2026-01-11 19:00:13,999: t15.2023.12.29 val PER: 0.0755
2026-01-11 19:00:13,999: t15.2024.02.25 val PER: 0.0829
2026-01-11 19:00:14,000: t15.2024.03.08 val PER: 0.1821
2026-01-11 19:00:14,000: t15.2024.03.15 val PER: 0.1632
2026-01-11 19:00:14,000: t15.2024.03.17 val PER: 0.0844
2026-01-11 19:00:14,000: t15.2024.05.10 val PER: 0.1218
2026-01-11 19:00:14,000: t15.2024.06.14 val PER: 0.1151
2026-01-11 19:00:14,001: t15.2024.07.19 val PER: 0.1655
2026-01-11 19:00:14,001: t15.2024.07.21 val PER: 0.0628
2026-01-11 19:00:14,001: t15.2024.07.28 val PER: 0.0912
2026-01-11 19:00:14,001: t15.2025.01.10 val PER: 0.2548
2026-01-11 19:00:14,001: t15.2025.01.12 val PER: 0.0901
2026-01-11 19:00:14,001: t15.2025.03.14 val PER: 0.3018
2026-01-11 19:00:14,002: t15.2025.03.16 val PER: 0.1309
2026-01-11 19:00:14,002: t15.2025.03.30 val PER: 0.2253
2026-01-11 19:00:14,002: t15.2025.04.13 val PER: 0.2026
2026-01-11 19:00:14,154: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_40000
2026-01-11 19:00:35,005: Train batch 40200: loss: 0.43 grad norm: 19.54 time: 0.075
2026-01-11 19:00:56,463: Train batch 40400: loss: 1.50 grad norm: 45.06 time: 0.068
2026-01-11 19:01:07,148: Running test after training batch: 40500
2026-01-11 19:01:07,262: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:01:13,375: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:01:13,450: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:01:30,424: Val batch 40500: PER (avg): 0.1094 CTC Loss (avg): 29.4505 WER(5gram): 11.80% (n=256) time: 23.276
2026-01-11 19:01:30,426: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 19:01:30,426: t15.2023.08.13 val PER: 0.0832
2026-01-11 19:01:30,426: t15.2023.08.18 val PER: 0.0629
2026-01-11 19:01:30,427: t15.2023.08.20 val PER: 0.0651
2026-01-11 19:01:30,427: t15.2023.08.25 val PER: 0.0813
2026-01-11 19:01:30,427: t15.2023.08.27 val PER: 0.1624
2026-01-11 19:01:30,427: t15.2023.09.01 val PER: 0.0455
2026-01-11 19:01:30,427: t15.2023.09.03 val PER: 0.1152
2026-01-11 19:01:30,427: t15.2023.09.24 val PER: 0.0898
2026-01-11 19:01:30,427: t15.2023.09.29 val PER: 0.1066
2026-01-11 19:01:30,427: t15.2023.10.01 val PER: 0.1440
2026-01-11 19:01:30,427: t15.2023.10.06 val PER: 0.0657
2026-01-11 19:01:30,427: t15.2023.10.08 val PER: 0.2138
2026-01-11 19:01:30,427: t15.2023.10.13 val PER: 0.1683
2026-01-11 19:01:30,427: t15.2023.10.15 val PER: 0.1068
2026-01-11 19:01:30,428: t15.2023.10.20 val PER: 0.1678
2026-01-11 19:01:30,428: t15.2023.10.22 val PER: 0.0891
2026-01-11 19:01:30,428: t15.2023.11.03 val PER: 0.1547
2026-01-11 19:01:30,428: t15.2023.11.04 val PER: 0.0102
2026-01-11 19:01:30,428: t15.2023.11.17 val PER: 0.0295
2026-01-11 19:01:30,428: t15.2023.11.19 val PER: 0.0080
2026-01-11 19:01:30,429: t15.2023.11.26 val PER: 0.0543
2026-01-11 19:01:30,429: t15.2023.12.03 val PER: 0.0578
2026-01-11 19:01:30,429: t15.2023.12.08 val PER: 0.0353
2026-01-11 19:01:30,429: t15.2023.12.10 val PER: 0.0329
2026-01-11 19:01:30,430: t15.2023.12.17 val PER: 0.0904
2026-01-11 19:01:30,430: t15.2023.12.29 val PER: 0.0700
2026-01-11 19:01:30,430: t15.2024.02.25 val PER: 0.0787
2026-01-11 19:01:30,430: t15.2024.03.08 val PER: 0.1949
2026-01-11 19:01:30,430: t15.2024.03.15 val PER: 0.1676
2026-01-11 19:01:30,430: t15.2024.03.17 val PER: 0.0921
2026-01-11 19:01:30,431: t15.2024.05.10 val PER: 0.1233
2026-01-11 19:01:30,431: t15.2024.06.14 val PER: 0.1246
2026-01-11 19:01:30,431: t15.2024.07.19 val PER: 0.1707
2026-01-11 19:01:30,431: t15.2024.07.21 val PER: 0.0607
2026-01-11 19:01:30,431: t15.2024.07.28 val PER: 0.0971
2026-01-11 19:01:30,431: t15.2025.01.10 val PER: 0.2383
2026-01-11 19:01:30,431: t15.2025.01.12 val PER: 0.0901
2026-01-11 19:01:30,431: t15.2025.03.14 val PER: 0.2855
2026-01-11 19:01:30,431: t15.2025.03.16 val PER: 0.1283
2026-01-11 19:01:30,431: t15.2025.03.30 val PER: 0.2264
2026-01-11 19:01:30,431: t15.2025.04.13 val PER: 0.1897
2026-01-11 19:01:30,620: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_40500
2026-01-11 19:01:40,895: Train batch 40600: loss: 1.81 grad norm: 48.74 time: 0.070
2026-01-11 19:02:01,497: Train batch 40800: loss: 0.79 grad norm: 25.03 time: 0.069
2026-01-11 19:02:23,181: Train batch 41000: loss: 1.04 grad norm: 32.09 time: 0.073
2026-01-11 19:02:23,181: Running test after training batch: 41000
2026-01-11 19:02:23,351: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:02:30,080: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:02:30,182: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 19:02:47,818: Val batch 41000: PER (avg): 0.1070 CTC Loss (avg): 29.7646 WER(5gram): 11.60% (n=256) time: 24.636
2026-01-11 19:02:47,819: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 19:02:47,819: t15.2023.08.13 val PER: 0.0759
2026-01-11 19:02:47,820: t15.2023.08.18 val PER: 0.0645
2026-01-11 19:02:47,820: t15.2023.08.20 val PER: 0.0588
2026-01-11 19:02:47,820: t15.2023.08.25 val PER: 0.0633
2026-01-11 19:02:47,820: t15.2023.08.27 val PER: 0.1704
2026-01-11 19:02:47,820: t15.2023.09.01 val PER: 0.0406
2026-01-11 19:02:47,820: t15.2023.09.03 val PER: 0.1235
2026-01-11 19:02:47,821: t15.2023.09.24 val PER: 0.0777
2026-01-11 19:02:47,821: t15.2023.09.29 val PER: 0.0989
2026-01-11 19:02:47,821: t15.2023.10.01 val PER: 0.1400
2026-01-11 19:02:47,821: t15.2023.10.06 val PER: 0.0549
2026-01-11 19:02:47,821: t15.2023.10.08 val PER: 0.2179
2026-01-11 19:02:47,821: t15.2023.10.13 val PER: 0.1761
2026-01-11 19:02:47,821: t15.2023.10.15 val PER: 0.1088
2026-01-11 19:02:47,821: t15.2023.10.20 val PER: 0.1644
2026-01-11 19:02:47,821: t15.2023.10.22 val PER: 0.0924
2026-01-11 19:02:47,821: t15.2023.11.03 val PER: 0.1574
2026-01-11 19:02:47,822: t15.2023.11.04 val PER: 0.0068
2026-01-11 19:02:47,822: t15.2023.11.17 val PER: 0.0171
2026-01-11 19:02:47,822: t15.2023.11.19 val PER: 0.0080
2026-01-11 19:02:47,822: t15.2023.11.26 val PER: 0.0413
2026-01-11 19:02:47,822: t15.2023.12.03 val PER: 0.0557
2026-01-11 19:02:47,822: t15.2023.12.08 val PER: 0.0326
2026-01-11 19:02:47,822: t15.2023.12.10 val PER: 0.0302
2026-01-11 19:02:47,822: t15.2023.12.17 val PER: 0.0790
2026-01-11 19:02:47,822: t15.2023.12.29 val PER: 0.0734
2026-01-11 19:02:47,823: t15.2024.02.25 val PER: 0.0801
2026-01-11 19:02:47,823: t15.2024.03.08 val PER: 0.1792
2026-01-11 19:02:47,823: t15.2024.03.15 val PER: 0.1626
2026-01-11 19:02:47,823: t15.2024.03.17 val PER: 0.0809
2026-01-11 19:02:47,823: t15.2024.05.10 val PER: 0.1248
2026-01-11 19:02:47,823: t15.2024.06.14 val PER: 0.1199
2026-01-11 19:02:47,823: t15.2024.07.19 val PER: 0.1701
2026-01-11 19:02:47,823: t15.2024.07.21 val PER: 0.0703
2026-01-11 19:02:47,823: t15.2024.07.28 val PER: 0.0875
2026-01-11 19:02:47,823: t15.2025.01.10 val PER: 0.2397
2026-01-11 19:02:47,824: t15.2025.01.12 val PER: 0.0978
2026-01-11 19:02:47,824: t15.2025.03.14 val PER: 0.3003
2026-01-11 19:02:47,824: t15.2025.03.16 val PER: 0.1270
2026-01-11 19:02:47,824: t15.2025.03.30 val PER: 0.2253
2026-01-11 19:02:47,824: t15.2025.04.13 val PER: 0.1797
2026-01-11 19:02:47,999: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_41000
2026-01-11 19:03:09,304: Train batch 41200: loss: 0.96 grad norm: 26.08 time: 0.088
2026-01-11 19:03:29,524: Train batch 41400: loss: 0.92 grad norm: 30.46 time: 0.073
2026-01-11 19:03:39,734: Running test after training batch: 41500
2026-01-11 19:03:39,936: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:03:46,050: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:03:46,118: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:04:02,719: Val batch 41500: PER (avg): 0.1073 CTC Loss (avg): 30.2791 WER(5gram): 10.43% (n=256) time: 22.985
2026-01-11 19:04:02,720: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 19:04:02,720: t15.2023.08.13 val PER: 0.0738
2026-01-11 19:04:02,720: t15.2023.08.18 val PER: 0.0612
2026-01-11 19:04:02,720: t15.2023.08.20 val PER: 0.0596
2026-01-11 19:04:02,720: t15.2023.08.25 val PER: 0.0723
2026-01-11 19:04:02,721: t15.2023.08.27 val PER: 0.1608
2026-01-11 19:04:02,721: t15.2023.09.01 val PER: 0.0446
2026-01-11 19:04:02,721: t15.2023.09.03 val PER: 0.1093
2026-01-11 19:04:02,721: t15.2023.09.24 val PER: 0.0862
2026-01-11 19:04:02,721: t15.2023.09.29 val PER: 0.1034
2026-01-11 19:04:02,721: t15.2023.10.01 val PER: 0.1301
2026-01-11 19:04:02,721: t15.2023.10.06 val PER: 0.0614
2026-01-11 19:04:02,721: t15.2023.10.08 val PER: 0.2138
2026-01-11 19:04:02,722: t15.2023.10.13 val PER: 0.1598
2026-01-11 19:04:02,722: t15.2023.10.15 val PER: 0.1154
2026-01-11 19:04:02,722: t15.2023.10.20 val PER: 0.1711
2026-01-11 19:04:02,722: t15.2023.10.22 val PER: 0.0891
2026-01-11 19:04:02,722: t15.2023.11.03 val PER: 0.1621
2026-01-11 19:04:02,722: t15.2023.11.04 val PER: 0.0137
2026-01-11 19:04:02,722: t15.2023.11.17 val PER: 0.0280
2026-01-11 19:04:02,722: t15.2023.11.19 val PER: 0.0100
2026-01-11 19:04:02,722: t15.2023.11.26 val PER: 0.0493
2026-01-11 19:04:02,723: t15.2023.12.03 val PER: 0.0641
2026-01-11 19:04:02,723: t15.2023.12.08 val PER: 0.0353
2026-01-11 19:04:02,723: t15.2023.12.10 val PER: 0.0329
2026-01-11 19:04:02,723: t15.2023.12.17 val PER: 0.0790
2026-01-11 19:04:02,723: t15.2023.12.29 val PER: 0.0721
2026-01-11 19:04:02,723: t15.2024.02.25 val PER: 0.0801
2026-01-11 19:04:02,723: t15.2024.03.08 val PER: 0.1821
2026-01-11 19:04:02,723: t15.2024.03.15 val PER: 0.1695
2026-01-11 19:04:02,723: t15.2024.03.17 val PER: 0.0907
2026-01-11 19:04:02,724: t15.2024.05.10 val PER: 0.1189
2026-01-11 19:04:02,724: t15.2024.06.14 val PER: 0.1167
2026-01-11 19:04:02,724: t15.2024.07.19 val PER: 0.1641
2026-01-11 19:04:02,724: t15.2024.07.21 val PER: 0.0662
2026-01-11 19:04:02,724: t15.2024.07.28 val PER: 0.0897
2026-01-11 19:04:02,724: t15.2025.01.10 val PER: 0.2424
2026-01-11 19:04:02,724: t15.2025.01.12 val PER: 0.0924
2026-01-11 19:04:02,724: t15.2025.03.14 val PER: 0.2885
2026-01-11 19:04:02,724: t15.2025.03.16 val PER: 0.1204
2026-01-11 19:04:02,724: t15.2025.03.30 val PER: 0.2241
2026-01-11 19:04:02,724: t15.2025.04.13 val PER: 0.1897
2026-01-11 19:04:02,879: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_41500
2026-01-11 19:04:13,523: Train batch 41600: loss: 1.11 grad norm: 24.25 time: 0.078
2026-01-11 19:04:34,859: Train batch 41800: loss: 1.59 grad norm: 39.43 time: 0.085
2026-01-11 19:04:56,057: Train batch 42000: loss: 0.79 grad norm: 27.82 time: 0.076
2026-01-11 19:04:56,057: Running test after training batch: 42000
2026-01-11 19:04:56,180: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:05:02,424: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:05:02,517: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:05:18,446: Val batch 42000: PER (avg): 0.1079 CTC Loss (avg): 30.6177 WER(5gram): 10.82% (n=256) time: 22.389
2026-01-11 19:05:18,448: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-11 19:05:18,448: t15.2023.08.13 val PER: 0.0780
2026-01-11 19:05:18,448: t15.2023.08.18 val PER: 0.0637
2026-01-11 19:05:18,448: t15.2023.08.20 val PER: 0.0667
2026-01-11 19:05:18,448: t15.2023.08.25 val PER: 0.0768
2026-01-11 19:05:18,448: t15.2023.08.27 val PER: 0.1624
2026-01-11 19:05:18,449: t15.2023.09.01 val PER: 0.0430
2026-01-11 19:05:18,449: t15.2023.09.03 val PER: 0.1152
2026-01-11 19:05:18,449: t15.2023.09.24 val PER: 0.0837
2026-01-11 19:05:18,449: t15.2023.09.29 val PER: 0.1072
2026-01-11 19:05:18,449: t15.2023.10.01 val PER: 0.1499
2026-01-11 19:05:18,449: t15.2023.10.06 val PER: 0.0614
2026-01-11 19:05:18,449: t15.2023.10.08 val PER: 0.2179
2026-01-11 19:05:18,449: t15.2023.10.13 val PER: 0.1621
2026-01-11 19:05:18,449: t15.2023.10.15 val PER: 0.1088
2026-01-11 19:05:18,450: t15.2023.10.20 val PER: 0.1611
2026-01-11 19:05:18,450: t15.2023.10.22 val PER: 0.0935
2026-01-11 19:05:18,450: t15.2023.11.03 val PER: 0.1581
2026-01-11 19:05:18,450: t15.2023.11.04 val PER: 0.0102
2026-01-11 19:05:18,450: t15.2023.11.17 val PER: 0.0171
2026-01-11 19:05:18,450: t15.2023.11.19 val PER: 0.0140
2026-01-11 19:05:18,450: t15.2023.11.26 val PER: 0.0428
2026-01-11 19:05:18,450: t15.2023.12.03 val PER: 0.0536
2026-01-11 19:05:18,450: t15.2023.12.08 val PER: 0.0346
2026-01-11 19:05:18,450: t15.2023.12.10 val PER: 0.0302
2026-01-11 19:05:18,451: t15.2023.12.17 val PER: 0.0832
2026-01-11 19:05:18,451: t15.2023.12.29 val PER: 0.0762
2026-01-11 19:05:18,451: t15.2024.02.25 val PER: 0.0744
2026-01-11 19:05:18,451: t15.2024.03.08 val PER: 0.1750
2026-01-11 19:05:18,451: t15.2024.03.15 val PER: 0.1682
2026-01-11 19:05:18,451: t15.2024.03.17 val PER: 0.0767
2026-01-11 19:05:18,451: t15.2024.05.10 val PER: 0.1263
2026-01-11 19:05:18,451: t15.2024.06.14 val PER: 0.1215
2026-01-11 19:05:18,451: t15.2024.07.19 val PER: 0.1740
2026-01-11 19:05:18,452: t15.2024.07.21 val PER: 0.0634
2026-01-11 19:05:18,452: t15.2024.07.28 val PER: 0.0897
2026-01-11 19:05:18,452: t15.2025.01.10 val PER: 0.2466
2026-01-11 19:05:18,452: t15.2025.01.12 val PER: 0.0978
2026-01-11 19:05:18,452: t15.2025.03.14 val PER: 0.2781
2026-01-11 19:05:18,452: t15.2025.03.16 val PER: 0.1335
2026-01-11 19:05:18,452: t15.2025.03.30 val PER: 0.2092
2026-01-11 19:05:18,452: t15.2025.04.13 val PER: 0.1969
2026-01-11 19:05:18,605: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_42000
2026-01-11 19:05:39,266: Train batch 42200: loss: 1.36 grad norm: 33.29 time: 0.074
2026-01-11 19:06:00,342: Train batch 42400: loss: 1.52 grad norm: 47.73 time: 0.073
2026-01-11 19:06:10,934: Running test after training batch: 42500
2026-01-11 19:06:11,186: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:06:17,394: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:06:17,471: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:06:33,332: Val batch 42500: PER (avg): 0.1083 CTC Loss (avg): 30.5104 WER(5gram): 11.47% (n=256) time: 22.398
2026-01-11 19:06:33,333: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 19:06:33,334: t15.2023.08.13 val PER: 0.0738
2026-01-11 19:06:33,334: t15.2023.08.18 val PER: 0.0629
2026-01-11 19:06:33,334: t15.2023.08.20 val PER: 0.0627
2026-01-11 19:06:33,334: t15.2023.08.25 val PER: 0.0768
2026-01-11 19:06:33,334: t15.2023.08.27 val PER: 0.1704
2026-01-11 19:06:33,334: t15.2023.09.01 val PER: 0.0422
2026-01-11 19:06:33,334: t15.2023.09.03 val PER: 0.1176
2026-01-11 19:06:33,335: t15.2023.09.24 val PER: 0.0874
2026-01-11 19:06:33,335: t15.2023.09.29 val PER: 0.1021
2026-01-11 19:06:33,335: t15.2023.10.01 val PER: 0.1420
2026-01-11 19:06:33,335: t15.2023.10.06 val PER: 0.0624
2026-01-11 19:06:33,335: t15.2023.10.08 val PER: 0.2233
2026-01-11 19:06:33,335: t15.2023.10.13 val PER: 0.1629
2026-01-11 19:06:33,335: t15.2023.10.15 val PER: 0.1088
2026-01-11 19:06:33,335: t15.2023.10.20 val PER: 0.1779
2026-01-11 19:06:33,335: t15.2023.10.22 val PER: 0.0891
2026-01-11 19:06:33,336: t15.2023.11.03 val PER: 0.1588
2026-01-11 19:06:33,336: t15.2023.11.04 val PER: 0.0068
2026-01-11 19:06:33,336: t15.2023.11.17 val PER: 0.0171
2026-01-11 19:06:33,336: t15.2023.11.19 val PER: 0.0120
2026-01-11 19:06:33,336: t15.2023.11.26 val PER: 0.0493
2026-01-11 19:06:33,336: t15.2023.12.03 val PER: 0.0504
2026-01-11 19:06:33,336: t15.2023.12.08 val PER: 0.0333
2026-01-11 19:06:33,336: t15.2023.12.10 val PER: 0.0302
2026-01-11 19:06:33,337: t15.2023.12.17 val PER: 0.0842
2026-01-11 19:06:33,337: t15.2023.12.29 val PER: 0.0755
2026-01-11 19:06:33,337: t15.2024.02.25 val PER: 0.0787
2026-01-11 19:06:33,337: t15.2024.03.08 val PER: 0.1679
2026-01-11 19:06:33,337: t15.2024.03.15 val PER: 0.1639
2026-01-11 19:06:33,337: t15.2024.03.17 val PER: 0.0886
2026-01-11 19:06:33,337: t15.2024.05.10 val PER: 0.1218
2026-01-11 19:06:33,337: t15.2024.06.14 val PER: 0.1325
2026-01-11 19:06:33,337: t15.2024.07.19 val PER: 0.1780
2026-01-11 19:06:33,337: t15.2024.07.21 val PER: 0.0655
2026-01-11 19:06:33,338: t15.2024.07.28 val PER: 0.0853
2026-01-11 19:06:33,338: t15.2025.01.10 val PER: 0.2534
2026-01-11 19:06:33,338: t15.2025.01.12 val PER: 0.0916
2026-01-11 19:06:33,338: t15.2025.03.14 val PER: 0.2929
2026-01-11 19:06:33,338: t15.2025.03.16 val PER: 0.1257
2026-01-11 19:06:33,338: t15.2025.03.30 val PER: 0.2253
2026-01-11 19:06:33,338: t15.2025.04.13 val PER: 0.1940
2026-01-11 19:06:33,494: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_42500
2026-01-11 19:06:44,350: Train batch 42600: loss: 0.58 grad norm: 26.34 time: 0.065
2026-01-11 19:07:05,083: Train batch 42800: loss: 1.19 grad norm: 30.08 time: 0.066
2026-01-11 19:07:26,722: Train batch 43000: loss: 0.69 grad norm: 19.26 time: 0.077
2026-01-11 19:07:26,722: Running test after training batch: 43000
2026-01-11 19:07:26,887: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:07:33,005: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:07:33,086: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:07:49,462: Val batch 43000: PER (avg): 0.1069 CTC Loss (avg): 31.1585 WER(5gram): 11.21% (n=256) time: 22.739
2026-01-11 19:07:49,462: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-11 19:07:49,463: t15.2023.08.13 val PER: 0.0707
2026-01-11 19:07:49,463: t15.2023.08.18 val PER: 0.0679
2026-01-11 19:07:49,463: t15.2023.08.20 val PER: 0.0691
2026-01-11 19:07:49,463: t15.2023.08.25 val PER: 0.0723
2026-01-11 19:07:49,463: t15.2023.08.27 val PER: 0.1704
2026-01-11 19:07:49,463: t15.2023.09.01 val PER: 0.0430
2026-01-11 19:07:49,464: t15.2023.09.03 val PER: 0.1164
2026-01-11 19:07:49,464: t15.2023.09.24 val PER: 0.0862
2026-01-11 19:07:49,464: t15.2023.09.29 val PER: 0.1034
2026-01-11 19:07:49,464: t15.2023.10.01 val PER: 0.1473
2026-01-11 19:07:49,464: t15.2023.10.06 val PER: 0.0549
2026-01-11 19:07:49,464: t15.2023.10.08 val PER: 0.2165
2026-01-11 19:07:49,464: t15.2023.10.13 val PER: 0.1621
2026-01-11 19:07:49,464: t15.2023.10.15 val PER: 0.1107
2026-01-11 19:07:49,465: t15.2023.10.20 val PER: 0.1644
2026-01-11 19:07:49,465: t15.2023.10.22 val PER: 0.0857
2026-01-11 19:07:49,465: t15.2023.11.03 val PER: 0.1588
2026-01-11 19:07:49,465: t15.2023.11.04 val PER: 0.0102
2026-01-11 19:07:49,465: t15.2023.11.17 val PER: 0.0187
2026-01-11 19:07:49,465: t15.2023.11.19 val PER: 0.0100
2026-01-11 19:07:49,465: t15.2023.11.26 val PER: 0.0442
2026-01-11 19:07:49,466: t15.2023.12.03 val PER: 0.0483
2026-01-11 19:07:49,466: t15.2023.12.08 val PER: 0.0406
2026-01-11 19:07:49,466: t15.2023.12.10 val PER: 0.0329
2026-01-11 19:07:49,466: t15.2023.12.17 val PER: 0.0811
2026-01-11 19:07:49,466: t15.2023.12.29 val PER: 0.0714
2026-01-11 19:07:49,466: t15.2024.02.25 val PER: 0.0801
2026-01-11 19:07:49,466: t15.2024.03.08 val PER: 0.1792
2026-01-11 19:07:49,467: t15.2024.03.15 val PER: 0.1601
2026-01-11 19:07:49,467: t15.2024.03.17 val PER: 0.0816
2026-01-11 19:07:49,467: t15.2024.05.10 val PER: 0.1129
2026-01-11 19:07:49,467: t15.2024.06.14 val PER: 0.1262
2026-01-11 19:07:49,467: t15.2024.07.19 val PER: 0.1688
2026-01-11 19:07:49,467: t15.2024.07.21 val PER: 0.0634
2026-01-11 19:07:49,467: t15.2024.07.28 val PER: 0.0816
2026-01-11 19:07:49,467: t15.2025.01.10 val PER: 0.2452
2026-01-11 19:07:49,467: t15.2025.01.12 val PER: 0.0962
2026-01-11 19:07:49,468: t15.2025.03.14 val PER: 0.2914
2026-01-11 19:07:49,468: t15.2025.03.16 val PER: 0.1322
2026-01-11 19:07:49,468: t15.2025.03.30 val PER: 0.2138
2026-01-11 19:07:49,468: t15.2025.04.13 val PER: 0.1812
2026-01-11 19:07:49,620: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_43000
2026-01-11 19:08:10,635: Train batch 43200: loss: 1.97 grad norm: 49.00 time: 0.075
2026-01-11 19:08:31,279: Train batch 43400: loss: 0.66 grad norm: 21.66 time: 0.083
2026-01-11 19:08:42,073: Running test after training batch: 43500
2026-01-11 19:08:42,249: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:08:48,403: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:08:48,475: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:09:04,304: Val batch 43500: PER (avg): 0.1071 CTC Loss (avg): 30.5279 WER(5gram): 12.78% (n=256) time: 22.231
2026-01-11 19:09:04,305: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-11 19:09:04,306: t15.2023.08.13 val PER: 0.0738
2026-01-11 19:09:04,306: t15.2023.08.18 val PER: 0.0654
2026-01-11 19:09:04,306: t15.2023.08.20 val PER: 0.0635
2026-01-11 19:09:04,306: t15.2023.08.25 val PER: 0.0693
2026-01-11 19:09:04,306: t15.2023.08.27 val PER: 0.1736
2026-01-11 19:09:04,306: t15.2023.09.01 val PER: 0.0414
2026-01-11 19:09:04,307: t15.2023.09.03 val PER: 0.1235
2026-01-11 19:09:04,307: t15.2023.09.24 val PER: 0.0837
2026-01-11 19:09:04,307: t15.2023.09.29 val PER: 0.1021
2026-01-11 19:09:04,307: t15.2023.10.01 val PER: 0.1486
2026-01-11 19:09:04,307: t15.2023.10.06 val PER: 0.0538
2026-01-11 19:09:04,307: t15.2023.10.08 val PER: 0.2138
2026-01-11 19:09:04,307: t15.2023.10.13 val PER: 0.1583
2026-01-11 19:09:04,307: t15.2023.10.15 val PER: 0.1147
2026-01-11 19:09:04,308: t15.2023.10.20 val PER: 0.1745
2026-01-11 19:09:04,308: t15.2023.10.22 val PER: 0.0869
2026-01-11 19:09:04,308: t15.2023.11.03 val PER: 0.1506
2026-01-11 19:09:04,308: t15.2023.11.04 val PER: 0.0205
2026-01-11 19:09:04,308: t15.2023.11.17 val PER: 0.0156
2026-01-11 19:09:04,308: t15.2023.11.19 val PER: 0.0120
2026-01-11 19:09:04,308: t15.2023.11.26 val PER: 0.0435
2026-01-11 19:09:04,308: t15.2023.12.03 val PER: 0.0536
2026-01-11 19:09:04,308: t15.2023.12.08 val PER: 0.0353
2026-01-11 19:09:04,308: t15.2023.12.10 val PER: 0.0223
2026-01-11 19:09:04,309: t15.2023.12.17 val PER: 0.0915
2026-01-11 19:09:04,309: t15.2023.12.29 val PER: 0.0659
2026-01-11 19:09:04,309: t15.2024.02.25 val PER: 0.0801
2026-01-11 19:09:04,309: t15.2024.03.08 val PER: 0.1693
2026-01-11 19:09:04,309: t15.2024.03.15 val PER: 0.1707
2026-01-11 19:09:04,309: t15.2024.03.17 val PER: 0.0879
2026-01-11 19:09:04,309: t15.2024.05.10 val PER: 0.1204
2026-01-11 19:09:04,309: t15.2024.06.14 val PER: 0.1183
2026-01-11 19:09:04,309: t15.2024.07.19 val PER: 0.1641
2026-01-11 19:09:04,310: t15.2024.07.21 val PER: 0.0559
2026-01-11 19:09:04,310: t15.2024.07.28 val PER: 0.0926
2026-01-11 19:09:04,310: t15.2025.01.10 val PER: 0.2534
2026-01-11 19:09:04,310: t15.2025.01.12 val PER: 0.0931
2026-01-11 19:09:04,310: t15.2025.03.14 val PER: 0.3047
2026-01-11 19:09:04,310: t15.2025.03.16 val PER: 0.1217
2026-01-11 19:09:04,310: t15.2025.03.30 val PER: 0.2207
2026-01-11 19:09:04,310: t15.2025.04.13 val PER: 0.1926
2026-01-11 19:09:04,464: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_43500
2026-01-11 19:09:14,942: Train batch 43600: loss: 1.10 grad norm: 30.66 time: 0.057
2026-01-11 19:09:35,517: Train batch 43800: loss: 0.90 grad norm: 32.38 time: 0.077
2026-01-11 19:09:56,280: Train batch 44000: loss: 1.11 grad norm: 32.85 time: 0.070
2026-01-11 19:09:56,281: Running test after training batch: 44000
2026-01-11 19:09:56,517: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:10:02,713: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 19:10:02,787: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:10:21,026: Val batch 44000: PER (avg): 0.1081 CTC Loss (avg): 30.2807 WER(5gram): 12.91% (n=256) time: 24.745
2026-01-11 19:10:21,027: WER lens: avg_true_words=5.99 avg_pred_words=6.15 max_pred_words=12
2026-01-11 19:10:21,027: t15.2023.08.13 val PER: 0.0759
2026-01-11 19:10:21,027: t15.2023.08.18 val PER: 0.0654
2026-01-11 19:10:21,028: t15.2023.08.20 val PER: 0.0683
2026-01-11 19:10:21,028: t15.2023.08.25 val PER: 0.0678
2026-01-11 19:10:21,028: t15.2023.08.27 val PER: 0.1576
2026-01-11 19:10:21,028: t15.2023.09.01 val PER: 0.0495
2026-01-11 19:10:21,028: t15.2023.09.03 val PER: 0.1093
2026-01-11 19:10:21,028: t15.2023.09.24 val PER: 0.0874
2026-01-11 19:10:21,028: t15.2023.09.29 val PER: 0.1110
2026-01-11 19:10:21,029: t15.2023.10.01 val PER: 0.1413
2026-01-11 19:10:21,029: t15.2023.10.06 val PER: 0.0646
2026-01-11 19:10:21,029: t15.2023.10.08 val PER: 0.2179
2026-01-11 19:10:21,029: t15.2023.10.13 val PER: 0.1621
2026-01-11 19:10:21,029: t15.2023.10.15 val PER: 0.1147
2026-01-11 19:10:21,029: t15.2023.10.20 val PER: 0.1711
2026-01-11 19:10:21,029: t15.2023.10.22 val PER: 0.0924
2026-01-11 19:10:21,029: t15.2023.11.03 val PER: 0.1574
2026-01-11 19:10:21,030: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:10:21,030: t15.2023.11.17 val PER: 0.0171
2026-01-11 19:10:21,030: t15.2023.11.19 val PER: 0.0180
2026-01-11 19:10:21,030: t15.2023.11.26 val PER: 0.0507
2026-01-11 19:10:21,030: t15.2023.12.03 val PER: 0.0567
2026-01-11 19:10:21,030: t15.2023.12.08 val PER: 0.0419
2026-01-11 19:10:21,030: t15.2023.12.10 val PER: 0.0342
2026-01-11 19:10:21,030: t15.2023.12.17 val PER: 0.0863
2026-01-11 19:10:21,030: t15.2023.12.29 val PER: 0.0734
2026-01-11 19:10:21,030: t15.2024.02.25 val PER: 0.0716
2026-01-11 19:10:21,030: t15.2024.03.08 val PER: 0.1622
2026-01-11 19:10:21,031: t15.2024.03.15 val PER: 0.1601
2026-01-11 19:10:21,031: t15.2024.03.17 val PER: 0.0872
2026-01-11 19:10:21,031: t15.2024.05.10 val PER: 0.1204
2026-01-11 19:10:21,031: t15.2024.06.14 val PER: 0.1246
2026-01-11 19:10:21,031: t15.2024.07.19 val PER: 0.1595
2026-01-11 19:10:21,031: t15.2024.07.21 val PER: 0.0655
2026-01-11 19:10:21,031: t15.2024.07.28 val PER: 0.0860
2026-01-11 19:10:21,032: t15.2025.01.10 val PER: 0.2479
2026-01-11 19:10:21,032: t15.2025.01.12 val PER: 0.0901
2026-01-11 19:10:21,032: t15.2025.03.14 val PER: 0.3121
2026-01-11 19:10:21,032: t15.2025.03.16 val PER: 0.1217
2026-01-11 19:10:21,032: t15.2025.03.30 val PER: 0.2184
2026-01-11 19:10:21,032: t15.2025.04.13 val PER: 0.1926
2026-01-11 19:10:21,186: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_44000
2026-01-11 19:10:43,081: Train batch 44200: loss: 0.76 grad norm: 22.13 time: 0.069
2026-01-11 19:11:04,181: Train batch 44400: loss: 0.18 grad norm: 5.43 time: 0.066
2026-01-11 19:11:14,517: Running test after training batch: 44500
2026-01-11 19:11:14,629: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:11:21,133: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:11:21,211: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:11:37,661: Val batch 44500: PER (avg): 0.1096 CTC Loss (avg): 30.3546 WER(5gram): 12.13% (n=256) time: 23.143
2026-01-11 19:11:37,662: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 19:11:37,663: t15.2023.08.13 val PER: 0.0821
2026-01-11 19:11:37,663: t15.2023.08.18 val PER: 0.0662
2026-01-11 19:11:37,663: t15.2023.08.20 val PER: 0.0707
2026-01-11 19:11:37,663: t15.2023.08.25 val PER: 0.0708
2026-01-11 19:11:37,663: t15.2023.08.27 val PER: 0.1559
2026-01-11 19:11:37,663: t15.2023.09.01 val PER: 0.0430
2026-01-11 19:11:37,663: t15.2023.09.03 val PER: 0.1152
2026-01-11 19:11:37,664: t15.2023.09.24 val PER: 0.0971
2026-01-11 19:11:37,664: t15.2023.09.29 val PER: 0.1059
2026-01-11 19:11:37,664: t15.2023.10.01 val PER: 0.1427
2026-01-11 19:11:37,664: t15.2023.10.06 val PER: 0.0538
2026-01-11 19:11:37,664: t15.2023.10.08 val PER: 0.2273
2026-01-11 19:11:37,664: t15.2023.10.13 val PER: 0.1583
2026-01-11 19:11:37,664: t15.2023.10.15 val PER: 0.1187
2026-01-11 19:11:37,664: t15.2023.10.20 val PER: 0.1711
2026-01-11 19:11:37,664: t15.2023.10.22 val PER: 0.0924
2026-01-11 19:11:37,664: t15.2023.11.03 val PER: 0.1581
2026-01-11 19:11:37,664: t15.2023.11.04 val PER: 0.0102
2026-01-11 19:11:37,665: t15.2023.11.17 val PER: 0.0202
2026-01-11 19:11:37,665: t15.2023.11.19 val PER: 0.0120
2026-01-11 19:11:37,665: t15.2023.11.26 val PER: 0.0428
2026-01-11 19:11:37,665: t15.2023.12.03 val PER: 0.0588
2026-01-11 19:11:37,665: t15.2023.12.08 val PER: 0.0353
2026-01-11 19:11:37,665: t15.2023.12.10 val PER: 0.0342
2026-01-11 19:11:37,665: t15.2023.12.17 val PER: 0.0811
2026-01-11 19:11:37,665: t15.2023.12.29 val PER: 0.0789
2026-01-11 19:11:37,665: t15.2024.02.25 val PER: 0.0857
2026-01-11 19:11:37,666: t15.2024.03.08 val PER: 0.1721
2026-01-11 19:11:37,666: t15.2024.03.15 val PER: 0.1739
2026-01-11 19:11:37,666: t15.2024.03.17 val PER: 0.0837
2026-01-11 19:11:37,666: t15.2024.05.10 val PER: 0.1263
2026-01-11 19:11:37,667: t15.2024.06.14 val PER: 0.1262
2026-01-11 19:11:37,667: t15.2024.07.19 val PER: 0.1668
2026-01-11 19:11:37,667: t15.2024.07.21 val PER: 0.0676
2026-01-11 19:11:37,667: t15.2024.07.28 val PER: 0.0897
2026-01-11 19:11:37,668: t15.2025.01.10 val PER: 0.2548
2026-01-11 19:11:37,668: t15.2025.01.12 val PER: 0.1024
2026-01-11 19:11:37,668: t15.2025.03.14 val PER: 0.2973
2026-01-11 19:11:37,668: t15.2025.03.16 val PER: 0.1296
2026-01-11 19:11:37,668: t15.2025.03.30 val PER: 0.2218
2026-01-11 19:11:37,668: t15.2025.04.13 val PER: 0.1826
2026-01-11 19:11:37,854: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_44500
2026-01-11 19:11:48,502: Train batch 44600: loss: 0.26 grad norm: 14.82 time: 0.064
2026-01-11 19:12:10,174: Train batch 44800: loss: 1.24 grad norm: 44.89 time: 0.058
2026-01-11 19:12:31,012: Train batch 45000: loss: 0.54 grad norm: 19.37 time: 0.081
2026-01-11 19:12:31,013: Running test after training batch: 45000
2026-01-11 19:12:31,154: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:12:38,123: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 19:12:38,192: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:12:53,787: Val batch 45000: PER (avg): 0.1070 CTC Loss (avg): 30.6191 WER(5gram): 12.91% (n=256) time: 22.774
2026-01-11 19:12:53,788: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 19:12:53,789: t15.2023.08.13 val PER: 0.0738
2026-01-11 19:12:53,789: t15.2023.08.18 val PER: 0.0687
2026-01-11 19:12:53,789: t15.2023.08.20 val PER: 0.0667
2026-01-11 19:12:53,789: t15.2023.08.25 val PER: 0.0723
2026-01-11 19:12:53,789: t15.2023.08.27 val PER: 0.1592
2026-01-11 19:12:53,789: t15.2023.09.01 val PER: 0.0438
2026-01-11 19:12:53,789: t15.2023.09.03 val PER: 0.1128
2026-01-11 19:12:53,789: t15.2023.09.24 val PER: 0.0765
2026-01-11 19:12:53,790: t15.2023.09.29 val PER: 0.1098
2026-01-11 19:12:53,790: t15.2023.10.01 val PER: 0.1387
2026-01-11 19:12:53,790: t15.2023.10.06 val PER: 0.0581
2026-01-11 19:12:53,790: t15.2023.10.08 val PER: 0.2179
2026-01-11 19:12:53,790: t15.2023.10.13 val PER: 0.1715
2026-01-11 19:12:53,790: t15.2023.10.15 val PER: 0.1009
2026-01-11 19:12:53,790: t15.2023.10.20 val PER: 0.1644
2026-01-11 19:12:53,790: t15.2023.10.22 val PER: 0.0913
2026-01-11 19:12:53,790: t15.2023.11.03 val PER: 0.1554
2026-01-11 19:12:53,791: t15.2023.11.04 val PER: 0.0068
2026-01-11 19:12:53,791: t15.2023.11.17 val PER: 0.0140
2026-01-11 19:12:53,791: t15.2023.11.19 val PER: 0.0200
2026-01-11 19:12:53,791: t15.2023.11.26 val PER: 0.0420
2026-01-11 19:12:53,791: t15.2023.12.03 val PER: 0.0473
2026-01-11 19:12:53,791: t15.2023.12.08 val PER: 0.0373
2026-01-11 19:12:53,791: t15.2023.12.10 val PER: 0.0355
2026-01-11 19:12:53,791: t15.2023.12.17 val PER: 0.0790
2026-01-11 19:12:53,791: t15.2023.12.29 val PER: 0.0748
2026-01-11 19:12:53,792: t15.2024.02.25 val PER: 0.0801
2026-01-11 19:12:53,792: t15.2024.03.08 val PER: 0.1636
2026-01-11 19:12:53,792: t15.2024.03.15 val PER: 0.1607
2026-01-11 19:12:53,792: t15.2024.03.17 val PER: 0.0872
2026-01-11 19:12:53,792: t15.2024.05.10 val PER: 0.1204
2026-01-11 19:12:53,792: t15.2024.06.14 val PER: 0.1230
2026-01-11 19:12:53,792: t15.2024.07.19 val PER: 0.1793
2026-01-11 19:12:53,792: t15.2024.07.21 val PER: 0.0593
2026-01-11 19:12:53,792: t15.2024.07.28 val PER: 0.0875
2026-01-11 19:12:53,792: t15.2025.01.10 val PER: 0.2603
2026-01-11 19:12:53,793: t15.2025.01.12 val PER: 0.0924
2026-01-11 19:12:53,793: t15.2025.03.14 val PER: 0.2944
2026-01-11 19:12:53,793: t15.2025.03.16 val PER: 0.1335
2026-01-11 19:12:53,793: t15.2025.03.30 val PER: 0.2126
2026-01-11 19:12:53,793: t15.2025.04.13 val PER: 0.1783
2026-01-11 19:12:53,951: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_45000
2026-01-11 19:13:14,587: Train batch 45200: loss: 1.44 grad norm: 52.75 time: 0.072
2026-01-11 19:13:35,268: Train batch 45400: loss: 0.61 grad norm: 22.70 time: 0.066
2026-01-11 19:13:46,130: Running test after training batch: 45500
2026-01-11 19:13:46,247: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:13:52,611: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:13:52,692: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:14:11,293: Val batch 45500: PER (avg): 0.1066 CTC Loss (avg): 30.8829 WER(5gram): 11.08% (n=256) time: 25.163
2026-01-11 19:14:11,294: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 19:14:11,294: t15.2023.08.13 val PER: 0.0748
2026-01-11 19:14:11,295: t15.2023.08.18 val PER: 0.0612
2026-01-11 19:14:11,295: t15.2023.08.20 val PER: 0.0691
2026-01-11 19:14:11,295: t15.2023.08.25 val PER: 0.0783
2026-01-11 19:14:11,295: t15.2023.08.27 val PER: 0.1576
2026-01-11 19:14:11,295: t15.2023.09.01 val PER: 0.0381
2026-01-11 19:14:11,295: t15.2023.09.03 val PER: 0.1057
2026-01-11 19:14:11,295: t15.2023.09.24 val PER: 0.0837
2026-01-11 19:14:11,295: t15.2023.09.29 val PER: 0.1059
2026-01-11 19:14:11,295: t15.2023.10.01 val PER: 0.1413
2026-01-11 19:14:11,295: t15.2023.10.06 val PER: 0.0603
2026-01-11 19:14:11,296: t15.2023.10.08 val PER: 0.2206
2026-01-11 19:14:11,296: t15.2023.10.13 val PER: 0.1606
2026-01-11 19:14:11,296: t15.2023.10.15 val PER: 0.1061
2026-01-11 19:14:11,296: t15.2023.10.20 val PER: 0.1711
2026-01-11 19:14:11,296: t15.2023.10.22 val PER: 0.0880
2026-01-11 19:14:11,296: t15.2023.11.03 val PER: 0.1601
2026-01-11 19:14:11,296: t15.2023.11.04 val PER: 0.0068
2026-01-11 19:14:11,297: t15.2023.11.17 val PER: 0.0202
2026-01-11 19:14:11,297: t15.2023.11.19 val PER: 0.0140
2026-01-11 19:14:11,297: t15.2023.11.26 val PER: 0.0435
2026-01-11 19:14:11,297: t15.2023.12.03 val PER: 0.0599
2026-01-11 19:14:11,298: t15.2023.12.08 val PER: 0.0346
2026-01-11 19:14:11,298: t15.2023.12.10 val PER: 0.0302
2026-01-11 19:14:11,298: t15.2023.12.17 val PER: 0.0790
2026-01-11 19:14:11,298: t15.2023.12.29 val PER: 0.0666
2026-01-11 19:14:11,298: t15.2024.02.25 val PER: 0.0787
2026-01-11 19:14:11,298: t15.2024.03.08 val PER: 0.1707
2026-01-11 19:14:11,299: t15.2024.03.15 val PER: 0.1595
2026-01-11 19:14:11,299: t15.2024.03.17 val PER: 0.0788
2026-01-11 19:14:11,299: t15.2024.05.10 val PER: 0.1293
2026-01-11 19:14:11,299: t15.2024.06.14 val PER: 0.1262
2026-01-11 19:14:11,299: t15.2024.07.19 val PER: 0.1674
2026-01-11 19:14:11,299: t15.2024.07.21 val PER: 0.0655
2026-01-11 19:14:11,299: t15.2024.07.28 val PER: 0.0882
2026-01-11 19:14:11,299: t15.2025.01.10 val PER: 0.2631
2026-01-11 19:14:11,299: t15.2025.01.12 val PER: 0.0878
2026-01-11 19:14:11,299: t15.2025.03.14 val PER: 0.3003
2026-01-11 19:14:11,300: t15.2025.03.16 val PER: 0.1309
2026-01-11 19:14:11,300: t15.2025.03.30 val PER: 0.2264
2026-01-11 19:14:11,300: t15.2025.04.13 val PER: 0.1712
2026-01-11 19:14:11,474: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_45500
2026-01-11 19:14:22,299: Train batch 45600: loss: 0.81 grad norm: 35.30 time: 0.059
2026-01-11 19:14:42,633: Train batch 45800: loss: 0.51 grad norm: 15.33 time: 0.071
2026-01-11 19:15:03,696: Train batch 46000: loss: 0.47 grad norm: 12.93 time: 0.068
2026-01-11 19:15:03,696: Running test after training batch: 46000
2026-01-11 19:15:03,866: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:15:10,877: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 19:15:10,979: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:15:28,666: Val batch 46000: PER (avg): 0.1069 CTC Loss (avg): 30.3301 WER(5gram): 12.39% (n=256) time: 24.970
2026-01-11 19:15:28,668: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-11 19:15:28,668: t15.2023.08.13 val PER: 0.0738
2026-01-11 19:15:28,668: t15.2023.08.18 val PER: 0.0671
2026-01-11 19:15:28,668: t15.2023.08.20 val PER: 0.0659
2026-01-11 19:15:28,668: t15.2023.08.25 val PER: 0.0783
2026-01-11 19:15:28,668: t15.2023.08.27 val PER: 0.1383
2026-01-11 19:15:28,669: t15.2023.09.01 val PER: 0.0390
2026-01-11 19:15:28,669: t15.2023.09.03 val PER: 0.1140
2026-01-11 19:15:28,669: t15.2023.09.24 val PER: 0.0862
2026-01-11 19:15:28,669: t15.2023.09.29 val PER: 0.1059
2026-01-11 19:15:28,669: t15.2023.10.01 val PER: 0.1446
2026-01-11 19:15:28,669: t15.2023.10.06 val PER: 0.0592
2026-01-11 19:15:28,669: t15.2023.10.08 val PER: 0.2233
2026-01-11 19:15:28,669: t15.2023.10.13 val PER: 0.1590
2026-01-11 19:15:28,669: t15.2023.10.15 val PER: 0.1081
2026-01-11 19:15:28,670: t15.2023.10.20 val PER: 0.1611
2026-01-11 19:15:28,670: t15.2023.10.22 val PER: 0.0913
2026-01-11 19:15:28,670: t15.2023.11.03 val PER: 0.1594
2026-01-11 19:15:28,670: t15.2023.11.04 val PER: 0.0068
2026-01-11 19:15:28,670: t15.2023.11.17 val PER: 0.0202
2026-01-11 19:15:28,670: t15.2023.11.19 val PER: 0.0140
2026-01-11 19:15:28,670: t15.2023.11.26 val PER: 0.0399
2026-01-11 19:15:28,670: t15.2023.12.03 val PER: 0.0504
2026-01-11 19:15:28,670: t15.2023.12.08 val PER: 0.0313
2026-01-11 19:15:28,670: t15.2023.12.10 val PER: 0.0263
2026-01-11 19:15:28,671: t15.2023.12.17 val PER: 0.0863
2026-01-11 19:15:28,671: t15.2023.12.29 val PER: 0.0762
2026-01-11 19:15:28,671: t15.2024.02.25 val PER: 0.0787
2026-01-11 19:15:28,671: t15.2024.03.08 val PER: 0.1821
2026-01-11 19:15:28,671: t15.2024.03.15 val PER: 0.1570
2026-01-11 19:15:28,671: t15.2024.03.17 val PER: 0.0837
2026-01-11 19:15:28,671: t15.2024.05.10 val PER: 0.1248
2026-01-11 19:15:28,671: t15.2024.06.14 val PER: 0.1167
2026-01-11 19:15:28,672: t15.2024.07.19 val PER: 0.1701
2026-01-11 19:15:28,672: t15.2024.07.21 val PER: 0.0600
2026-01-11 19:15:28,672: t15.2024.07.28 val PER: 0.0890
2026-01-11 19:15:28,672: t15.2025.01.10 val PER: 0.2617
2026-01-11 19:15:28,672: t15.2025.01.12 val PER: 0.0978
2026-01-11 19:15:28,672: t15.2025.03.14 val PER: 0.2899
2026-01-11 19:15:28,672: t15.2025.03.16 val PER: 0.1270
2026-01-11 19:15:28,672: t15.2025.03.30 val PER: 0.2207
2026-01-11 19:15:28,672: t15.2025.04.13 val PER: 0.1854
2026-01-11 19:15:28,850: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_46000
2026-01-11 19:15:49,287: Train batch 46200: loss: 0.49 grad norm: 26.76 time: 0.063
2026-01-11 19:16:11,259: Train batch 46400: loss: 1.19 grad norm: 26.71 time: 0.066
2026-01-11 19:16:21,554: Running test after training batch: 46500
2026-01-11 19:16:21,663: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:16:27,810: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:16:27,886: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:16:44,744: Val batch 46500: PER (avg): 0.1066 CTC Loss (avg): 30.5768 WER(5gram): 11.47% (n=256) time: 23.190
2026-01-11 19:16:44,744: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 19:16:44,745: t15.2023.08.13 val PER: 0.0800
2026-01-11 19:16:44,745: t15.2023.08.18 val PER: 0.0696
2026-01-11 19:16:44,745: t15.2023.08.20 val PER: 0.0659
2026-01-11 19:16:44,745: t15.2023.08.25 val PER: 0.0663
2026-01-11 19:16:44,745: t15.2023.08.27 val PER: 0.1463
2026-01-11 19:16:44,745: t15.2023.09.01 val PER: 0.0455
2026-01-11 19:16:44,746: t15.2023.09.03 val PER: 0.1045
2026-01-11 19:16:44,746: t15.2023.09.24 val PER: 0.0850
2026-01-11 19:16:44,746: t15.2023.09.29 val PER: 0.1053
2026-01-11 19:16:44,746: t15.2023.10.01 val PER: 0.1433
2026-01-11 19:16:44,746: t15.2023.10.06 val PER: 0.0603
2026-01-11 19:16:44,746: t15.2023.10.08 val PER: 0.2016
2026-01-11 19:16:44,746: t15.2023.10.13 val PER: 0.1660
2026-01-11 19:16:44,746: t15.2023.10.15 val PER: 0.1114
2026-01-11 19:16:44,746: t15.2023.10.20 val PER: 0.1577
2026-01-11 19:16:44,746: t15.2023.10.22 val PER: 0.0969
2026-01-11 19:16:44,747: t15.2023.11.03 val PER: 0.1547
2026-01-11 19:16:44,747: t15.2023.11.04 val PER: 0.0205
2026-01-11 19:16:44,747: t15.2023.11.17 val PER: 0.0233
2026-01-11 19:16:44,747: t15.2023.11.19 val PER: 0.0120
2026-01-11 19:16:44,747: t15.2023.11.26 val PER: 0.0435
2026-01-11 19:16:44,748: t15.2023.12.03 val PER: 0.0483
2026-01-11 19:16:44,748: t15.2023.12.08 val PER: 0.0326
2026-01-11 19:16:44,748: t15.2023.12.10 val PER: 0.0302
2026-01-11 19:16:44,748: t15.2023.12.17 val PER: 0.0790
2026-01-11 19:16:44,748: t15.2023.12.29 val PER: 0.0700
2026-01-11 19:16:44,748: t15.2024.02.25 val PER: 0.0758
2026-01-11 19:16:44,748: t15.2024.03.08 val PER: 0.1707
2026-01-11 19:16:44,749: t15.2024.03.15 val PER: 0.1714
2026-01-11 19:16:44,749: t15.2024.03.17 val PER: 0.0872
2026-01-11 19:16:44,749: t15.2024.05.10 val PER: 0.1263
2026-01-11 19:16:44,749: t15.2024.06.14 val PER: 0.1183
2026-01-11 19:16:44,749: t15.2024.07.19 val PER: 0.1701
2026-01-11 19:16:44,749: t15.2024.07.21 val PER: 0.0593
2026-01-11 19:16:44,749: t15.2024.07.28 val PER: 0.0801
2026-01-11 19:16:44,749: t15.2025.01.10 val PER: 0.2631
2026-01-11 19:16:44,750: t15.2025.01.12 val PER: 0.0893
2026-01-11 19:16:44,750: t15.2025.03.14 val PER: 0.2840
2026-01-11 19:16:44,750: t15.2025.03.16 val PER: 0.1270
2026-01-11 19:16:44,750: t15.2025.03.30 val PER: 0.2241
2026-01-11 19:16:44,750: t15.2025.04.13 val PER: 0.1854
2026-01-11 19:16:44,909: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_46500
2026-01-11 19:16:55,102: Train batch 46600: loss: 1.23 grad norm: 27.22 time: 0.097
2026-01-11 19:17:16,467: Train batch 46800: loss: 0.87 grad norm: 29.33 time: 0.072
2026-01-11 19:17:38,480: Train batch 47000: loss: 0.52 grad norm: 19.78 time: 0.057
2026-01-11 19:17:38,481: Running test after training batch: 47000
2026-01-11 19:17:38,688: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:17:44,794: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:17:44,892: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 19:18:02,173: Val batch 47000: PER (avg): 0.1075 CTC Loss (avg): 31.0449 WER(5gram): 11.67% (n=256) time: 23.692
2026-01-11 19:18:02,174: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 19:18:02,174: t15.2023.08.13 val PER: 0.0800
2026-01-11 19:18:02,174: t15.2023.08.18 val PER: 0.0696
2026-01-11 19:18:02,174: t15.2023.08.20 val PER: 0.0635
2026-01-11 19:18:02,174: t15.2023.08.25 val PER: 0.0678
2026-01-11 19:18:02,175: t15.2023.08.27 val PER: 0.1640
2026-01-11 19:18:02,175: t15.2023.09.01 val PER: 0.0406
2026-01-11 19:18:02,175: t15.2023.09.03 val PER: 0.1093
2026-01-11 19:18:02,175: t15.2023.09.24 val PER: 0.0837
2026-01-11 19:18:02,175: t15.2023.09.29 val PER: 0.1002
2026-01-11 19:18:02,175: t15.2023.10.01 val PER: 0.1400
2026-01-11 19:18:02,175: t15.2023.10.06 val PER: 0.0571
2026-01-11 19:18:02,175: t15.2023.10.08 val PER: 0.2192
2026-01-11 19:18:02,175: t15.2023.10.13 val PER: 0.1715
2026-01-11 19:18:02,175: t15.2023.10.15 val PER: 0.1134
2026-01-11 19:18:02,175: t15.2023.10.20 val PER: 0.1544
2026-01-11 19:18:02,176: t15.2023.10.22 val PER: 0.0947
2026-01-11 19:18:02,176: t15.2023.11.03 val PER: 0.1520
2026-01-11 19:18:02,176: t15.2023.11.04 val PER: 0.0102
2026-01-11 19:18:02,176: t15.2023.11.17 val PER: 0.0218
2026-01-11 19:18:02,176: t15.2023.11.19 val PER: 0.0080
2026-01-11 19:18:02,176: t15.2023.11.26 val PER: 0.0464
2026-01-11 19:18:02,176: t15.2023.12.03 val PER: 0.0515
2026-01-11 19:18:02,176: t15.2023.12.08 val PER: 0.0360
2026-01-11 19:18:02,176: t15.2023.12.10 val PER: 0.0302
2026-01-11 19:18:02,177: t15.2023.12.17 val PER: 0.0852
2026-01-11 19:18:02,177: t15.2023.12.29 val PER: 0.0728
2026-01-11 19:18:02,177: t15.2024.02.25 val PER: 0.0829
2026-01-11 19:18:02,177: t15.2024.03.08 val PER: 0.1721
2026-01-11 19:18:02,177: t15.2024.03.15 val PER: 0.1689
2026-01-11 19:18:02,177: t15.2024.03.17 val PER: 0.0858
2026-01-11 19:18:02,177: t15.2024.05.10 val PER: 0.1278
2026-01-11 19:18:02,177: t15.2024.06.14 val PER: 0.1151
2026-01-11 19:18:02,178: t15.2024.07.19 val PER: 0.1701
2026-01-11 19:18:02,178: t15.2024.07.21 val PER: 0.0614
2026-01-11 19:18:02,178: t15.2024.07.28 val PER: 0.0882
2026-01-11 19:18:02,178: t15.2025.01.10 val PER: 0.2410
2026-01-11 19:18:02,178: t15.2025.01.12 val PER: 0.0901
2026-01-11 19:18:02,178: t15.2025.03.14 val PER: 0.3062
2026-01-11 19:18:02,178: t15.2025.03.16 val PER: 0.1204
2026-01-11 19:18:02,178: t15.2025.03.30 val PER: 0.2218
2026-01-11 19:18:02,178: t15.2025.04.13 val PER: 0.1940
2026-01-11 19:18:02,335: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_47000
2026-01-11 19:18:23,152: Train batch 47200: loss: 0.51 grad norm: 24.51 time: 0.059
2026-01-11 19:18:44,578: Train batch 47400: loss: 0.41 grad norm: 24.94 time: 0.079
2026-01-11 19:18:54,741: Running test after training batch: 47500
2026-01-11 19:18:54,921: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:19:01,016: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:19:01,099: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 19:19:17,803: Val batch 47500: PER (avg): 0.1065 CTC Loss (avg): 30.7464 WER(5gram): 11.73% (n=256) time: 23.062
2026-01-11 19:19:17,804: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 19:19:17,804: t15.2023.08.13 val PER: 0.0780
2026-01-11 19:19:17,804: t15.2023.08.18 val PER: 0.0729
2026-01-11 19:19:17,805: t15.2023.08.20 val PER: 0.0715
2026-01-11 19:19:17,805: t15.2023.08.25 val PER: 0.0708
2026-01-11 19:19:17,805: t15.2023.08.27 val PER: 0.1543
2026-01-11 19:19:17,805: t15.2023.09.01 val PER: 0.0446
2026-01-11 19:19:17,805: t15.2023.09.03 val PER: 0.1116
2026-01-11 19:19:17,805: t15.2023.09.24 val PER: 0.0862
2026-01-11 19:19:17,805: t15.2023.09.29 val PER: 0.1015
2026-01-11 19:19:17,806: t15.2023.10.01 val PER: 0.1466
2026-01-11 19:19:17,806: t15.2023.10.06 val PER: 0.0506
2026-01-11 19:19:17,806: t15.2023.10.08 val PER: 0.1908
2026-01-11 19:19:17,806: t15.2023.10.13 val PER: 0.1621
2026-01-11 19:19:17,806: t15.2023.10.15 val PER: 0.1094
2026-01-11 19:19:17,806: t15.2023.10.20 val PER: 0.1745
2026-01-11 19:19:17,806: t15.2023.10.22 val PER: 0.0969
2026-01-11 19:19:17,806: t15.2023.11.03 val PER: 0.1540
2026-01-11 19:19:17,806: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:19:17,806: t15.2023.11.17 val PER: 0.0280
2026-01-11 19:19:17,806: t15.2023.11.19 val PER: 0.0120
2026-01-11 19:19:17,806: t15.2023.11.26 val PER: 0.0428
2026-01-11 19:19:17,807: t15.2023.12.03 val PER: 0.0494
2026-01-11 19:19:17,807: t15.2023.12.08 val PER: 0.0413
2026-01-11 19:19:17,808: t15.2023.12.10 val PER: 0.0289
2026-01-11 19:19:17,808: t15.2023.12.17 val PER: 0.0873
2026-01-11 19:19:17,808: t15.2023.12.29 val PER: 0.0679
2026-01-11 19:19:17,808: t15.2024.02.25 val PER: 0.0941
2026-01-11 19:19:17,808: t15.2024.03.08 val PER: 0.1707
2026-01-11 19:19:17,808: t15.2024.03.15 val PER: 0.1639
2026-01-11 19:19:17,808: t15.2024.03.17 val PER: 0.0746
2026-01-11 19:19:17,808: t15.2024.05.10 val PER: 0.1233
2026-01-11 19:19:17,808: t15.2024.06.14 val PER: 0.1183
2026-01-11 19:19:17,808: t15.2024.07.19 val PER: 0.1648
2026-01-11 19:19:17,809: t15.2024.07.21 val PER: 0.0614
2026-01-11 19:19:17,809: t15.2024.07.28 val PER: 0.0860
2026-01-11 19:19:17,809: t15.2025.01.10 val PER: 0.2548
2026-01-11 19:19:17,809: t15.2025.01.12 val PER: 0.0862
2026-01-11 19:19:17,809: t15.2025.03.14 val PER: 0.2914
2026-01-11 19:19:17,809: t15.2025.03.16 val PER: 0.1191
2026-01-11 19:19:17,809: t15.2025.03.30 val PER: 0.2276
2026-01-11 19:19:17,809: t15.2025.04.13 val PER: 0.1883
2026-01-11 19:19:17,971: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_47500
2026-01-11 19:19:28,415: Train batch 47600: loss: 0.81 grad norm: 22.66 time: 0.093
2026-01-11 19:19:48,629: Train batch 47800: loss: 0.69 grad norm: 25.20 time: 0.066
2026-01-11 19:20:09,630: Train batch 48000: loss: 0.54 grad norm: 19.84 time: 0.057
2026-01-11 19:20:09,630: Running test after training batch: 48000
2026-01-11 19:20:09,794: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:20:15,894: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 19:20:15,984: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:20:32,467: Val batch 48000: PER (avg): 0.1064 CTC Loss (avg): 30.7110 WER(5gram): 10.95% (n=256) time: 22.836
2026-01-11 19:20:32,468: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 19:20:32,468: t15.2023.08.13 val PER: 0.0842
2026-01-11 19:20:32,469: t15.2023.08.18 val PER: 0.0645
2026-01-11 19:20:32,469: t15.2023.08.20 val PER: 0.0691
2026-01-11 19:20:32,469: t15.2023.08.25 val PER: 0.0708
2026-01-11 19:20:32,469: t15.2023.08.27 val PER: 0.1543
2026-01-11 19:20:32,469: t15.2023.09.01 val PER: 0.0414
2026-01-11 19:20:32,469: t15.2023.09.03 val PER: 0.1128
2026-01-11 19:20:32,469: t15.2023.09.24 val PER: 0.0813
2026-01-11 19:20:32,469: t15.2023.09.29 val PER: 0.1059
2026-01-11 19:20:32,470: t15.2023.10.01 val PER: 0.1427
2026-01-11 19:20:32,470: t15.2023.10.06 val PER: 0.0581
2026-01-11 19:20:32,470: t15.2023.10.08 val PER: 0.2233
2026-01-11 19:20:32,470: t15.2023.10.13 val PER: 0.1699
2026-01-11 19:20:32,470: t15.2023.10.15 val PER: 0.1061
2026-01-11 19:20:32,470: t15.2023.10.20 val PER: 0.1577
2026-01-11 19:20:32,470: t15.2023.10.22 val PER: 0.1013
2026-01-11 19:20:32,470: t15.2023.11.03 val PER: 0.1581
2026-01-11 19:20:32,471: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:20:32,471: t15.2023.11.17 val PER: 0.0218
2026-01-11 19:20:32,471: t15.2023.11.19 val PER: 0.0140
2026-01-11 19:20:32,471: t15.2023.11.26 val PER: 0.0471
2026-01-11 19:20:32,471: t15.2023.12.03 val PER: 0.0494
2026-01-11 19:20:32,471: t15.2023.12.08 val PER: 0.0399
2026-01-11 19:20:32,471: t15.2023.12.10 val PER: 0.0276
2026-01-11 19:20:32,471: t15.2023.12.17 val PER: 0.0821
2026-01-11 19:20:32,472: t15.2023.12.29 val PER: 0.0707
2026-01-11 19:20:32,472: t15.2024.02.25 val PER: 0.0674
2026-01-11 19:20:32,472: t15.2024.03.08 val PER: 0.1650
2026-01-11 19:20:32,472: t15.2024.03.15 val PER: 0.1626
2026-01-11 19:20:32,472: t15.2024.03.17 val PER: 0.0802
2026-01-11 19:20:32,472: t15.2024.05.10 val PER: 0.1263
2026-01-11 19:20:32,472: t15.2024.06.14 val PER: 0.1120
2026-01-11 19:20:32,472: t15.2024.07.19 val PER: 0.1688
2026-01-11 19:20:32,473: t15.2024.07.21 val PER: 0.0586
2026-01-11 19:20:32,473: t15.2024.07.28 val PER: 0.0897
2026-01-11 19:20:32,473: t15.2025.01.10 val PER: 0.2328
2026-01-11 19:20:32,473: t15.2025.01.12 val PER: 0.0793
2026-01-11 19:20:32,473: t15.2025.03.14 val PER: 0.2840
2026-01-11 19:20:32,473: t15.2025.03.16 val PER: 0.1361
2026-01-11 19:20:32,473: t15.2025.03.30 val PER: 0.2195
2026-01-11 19:20:32,473: t15.2025.04.13 val PER: 0.1969
2026-01-11 19:20:32,627: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_48000
2026-01-11 19:20:53,538: Train batch 48200: loss: 0.30 grad norm: 14.80 time: 0.055
2026-01-11 19:21:14,897: Train batch 48400: loss: 0.95 grad norm: 32.12 time: 0.089
2026-01-11 19:21:25,557: Running test after training batch: 48500
2026-01-11 19:21:25,689: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:21:31,846: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 19:21:31,930: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:21:52,689: Val batch 48500: PER (avg): 0.1053 CTC Loss (avg): 30.8222 WER(5gram): 11.80% (n=256) time: 27.131
2026-01-11 19:21:52,690: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 19:21:52,690: t15.2023.08.13 val PER: 0.0769
2026-01-11 19:21:52,690: t15.2023.08.18 val PER: 0.0679
2026-01-11 19:21:52,690: t15.2023.08.20 val PER: 0.0635
2026-01-11 19:21:52,690: t15.2023.08.25 val PER: 0.0708
2026-01-11 19:21:52,690: t15.2023.08.27 val PER: 0.1543
2026-01-11 19:21:52,691: t15.2023.09.01 val PER: 0.0390
2026-01-11 19:21:52,691: t15.2023.09.03 val PER: 0.1116
2026-01-11 19:21:52,691: t15.2023.09.24 val PER: 0.0825
2026-01-11 19:21:52,691: t15.2023.09.29 val PER: 0.1072
2026-01-11 19:21:52,691: t15.2023.10.01 val PER: 0.1347
2026-01-11 19:21:52,691: t15.2023.10.06 val PER: 0.0581
2026-01-11 19:21:52,691: t15.2023.10.08 val PER: 0.2003
2026-01-11 19:21:52,692: t15.2023.10.13 val PER: 0.1683
2026-01-11 19:21:52,692: t15.2023.10.15 val PER: 0.1081
2026-01-11 19:21:52,692: t15.2023.10.20 val PER: 0.1644
2026-01-11 19:21:52,692: t15.2023.10.22 val PER: 0.0969
2026-01-11 19:21:52,692: t15.2023.11.03 val PER: 0.1520
2026-01-11 19:21:52,692: t15.2023.11.04 val PER: 0.0068
2026-01-11 19:21:52,692: t15.2023.11.17 val PER: 0.0249
2026-01-11 19:21:52,692: t15.2023.11.19 val PER: 0.0120
2026-01-11 19:21:52,693: t15.2023.11.26 val PER: 0.0464
2026-01-11 19:21:52,693: t15.2023.12.03 val PER: 0.0609
2026-01-11 19:21:52,693: t15.2023.12.08 val PER: 0.0340
2026-01-11 19:21:52,693: t15.2023.12.10 val PER: 0.0210
2026-01-11 19:21:52,693: t15.2023.12.17 val PER: 0.0800
2026-01-11 19:21:52,693: t15.2023.12.29 val PER: 0.0686
2026-01-11 19:21:52,693: t15.2024.02.25 val PER: 0.0899
2026-01-11 19:21:52,693: t15.2024.03.08 val PER: 0.1735
2026-01-11 19:21:52,693: t15.2024.03.15 val PER: 0.1657
2026-01-11 19:21:52,694: t15.2024.03.17 val PER: 0.0795
2026-01-11 19:21:52,694: t15.2024.05.10 val PER: 0.1159
2026-01-11 19:21:52,694: t15.2024.06.14 val PER: 0.1230
2026-01-11 19:21:52,694: t15.2024.07.19 val PER: 0.1582
2026-01-11 19:21:52,694: t15.2024.07.21 val PER: 0.0579
2026-01-11 19:21:52,694: t15.2024.07.28 val PER: 0.0801
2026-01-11 19:21:52,694: t15.2025.01.10 val PER: 0.2548
2026-01-11 19:21:52,694: t15.2025.01.12 val PER: 0.0754
2026-01-11 19:21:52,694: t15.2025.03.14 val PER: 0.2781
2026-01-11 19:21:52,694: t15.2025.03.16 val PER: 0.1335
2026-01-11 19:21:52,695: t15.2025.03.30 val PER: 0.2287
2026-01-11 19:21:52,695: t15.2025.04.13 val PER: 0.1983
2026-01-11 19:21:52,857: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_48500
2026-01-11 19:22:03,337: Train batch 48600: loss: 1.03 grad norm: 32.48 time: 0.093
2026-01-11 19:22:24,791: Train batch 48800: loss: 0.64 grad norm: 18.91 time: 0.073
2026-01-11 19:22:46,116: Train batch 49000: loss: 0.67 grad norm: 24.77 time: 0.074
2026-01-11 19:22:46,116: Running test after training batch: 49000
2026-01-11 19:22:46,588: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:22:53,654: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:22:53,757: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 19:23:12,617: Val batch 49000: PER (avg): 0.1050 CTC Loss (avg): 31.2568 WER(5gram): 11.47% (n=256) time: 26.500
2026-01-11 19:23:12,618: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 19:23:12,618: t15.2023.08.13 val PER: 0.0811
2026-01-11 19:23:12,619: t15.2023.08.18 val PER: 0.0687
2026-01-11 19:23:12,619: t15.2023.08.20 val PER: 0.0763
2026-01-11 19:23:12,619: t15.2023.08.25 val PER: 0.0693
2026-01-11 19:23:12,619: t15.2023.08.27 val PER: 0.1511
2026-01-11 19:23:12,619: t15.2023.09.01 val PER: 0.0373
2026-01-11 19:23:12,619: t15.2023.09.03 val PER: 0.1081
2026-01-11 19:23:12,619: t15.2023.09.24 val PER: 0.0789
2026-01-11 19:23:12,619: t15.2023.09.29 val PER: 0.1053
2026-01-11 19:23:12,620: t15.2023.10.01 val PER: 0.1367
2026-01-11 19:23:12,620: t15.2023.10.06 val PER: 0.0603
2026-01-11 19:23:12,620: t15.2023.10.08 val PER: 0.2016
2026-01-11 19:23:12,620: t15.2023.10.13 val PER: 0.1629
2026-01-11 19:23:12,620: t15.2023.10.15 val PER: 0.1061
2026-01-11 19:23:12,620: t15.2023.10.20 val PER: 0.1678
2026-01-11 19:23:12,620: t15.2023.10.22 val PER: 0.0813
2026-01-11 19:23:12,621: t15.2023.11.03 val PER: 0.1526
2026-01-11 19:23:12,621: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:23:12,621: t15.2023.11.17 val PER: 0.0218
2026-01-11 19:23:12,621: t15.2023.11.19 val PER: 0.0100
2026-01-11 19:23:12,621: t15.2023.11.26 val PER: 0.0428
2026-01-11 19:23:12,621: t15.2023.12.03 val PER: 0.0473
2026-01-11 19:23:12,621: t15.2023.12.08 val PER: 0.0379
2026-01-11 19:23:12,622: t15.2023.12.10 val PER: 0.0302
2026-01-11 19:23:12,622: t15.2023.12.17 val PER: 0.0832
2026-01-11 19:23:12,622: t15.2023.12.29 val PER: 0.0645
2026-01-11 19:23:12,622: t15.2024.02.25 val PER: 0.0758
2026-01-11 19:23:12,622: t15.2024.03.08 val PER: 0.1693
2026-01-11 19:23:12,622: t15.2024.03.15 val PER: 0.1563
2026-01-11 19:23:12,622: t15.2024.03.17 val PER: 0.0865
2026-01-11 19:23:12,622: t15.2024.05.10 val PER: 0.1233
2026-01-11 19:23:12,623: t15.2024.06.14 val PER: 0.1183
2026-01-11 19:23:12,623: t15.2024.07.19 val PER: 0.1681
2026-01-11 19:23:12,623: t15.2024.07.21 val PER: 0.0648
2026-01-11 19:23:12,623: t15.2024.07.28 val PER: 0.0868
2026-01-11 19:23:12,623: t15.2025.01.10 val PER: 0.2534
2026-01-11 19:23:12,623: t15.2025.01.12 val PER: 0.0862
2026-01-11 19:23:12,623: t15.2025.03.14 val PER: 0.2929
2026-01-11 19:23:12,623: t15.2025.03.16 val PER: 0.1086
2026-01-11 19:23:12,624: t15.2025.03.30 val PER: 0.2184
2026-01-11 19:23:12,624: t15.2025.04.13 val PER: 0.1926
2026-01-11 19:23:12,785: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_49000
2026-01-11 19:23:33,626: Train batch 49200: loss: 0.13 grad norm: 5.41 time: 0.064
2026-01-11 19:23:54,377: Train batch 49400: loss: 0.69 grad norm: 27.07 time: 0.074
2026-01-11 19:24:04,594: Running test after training batch: 49500
2026-01-11 19:24:04,704: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:24:11,518: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:24:11,592: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:24:27,390: Val batch 49500: PER (avg): 0.1053 CTC Loss (avg): 31.2942 WER(5gram): 11.54% (n=256) time: 22.796
2026-01-11 19:24:27,391: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-11 19:24:27,392: t15.2023.08.13 val PER: 0.0728
2026-01-11 19:24:27,392: t15.2023.08.18 val PER: 0.0637
2026-01-11 19:24:27,392: t15.2023.08.20 val PER: 0.0683
2026-01-11 19:24:27,392: t15.2023.08.25 val PER: 0.0633
2026-01-11 19:24:27,392: t15.2023.08.27 val PER: 0.1431
2026-01-11 19:24:27,392: t15.2023.09.01 val PER: 0.0414
2026-01-11 19:24:27,392: t15.2023.09.03 val PER: 0.1140
2026-01-11 19:24:27,392: t15.2023.09.24 val PER: 0.0825
2026-01-11 19:24:27,393: t15.2023.09.29 val PER: 0.1104
2026-01-11 19:24:27,393: t15.2023.10.01 val PER: 0.1400
2026-01-11 19:24:27,393: t15.2023.10.06 val PER: 0.0517
2026-01-11 19:24:27,393: t15.2023.10.08 val PER: 0.2070
2026-01-11 19:24:27,393: t15.2023.10.13 val PER: 0.1645
2026-01-11 19:24:27,393: t15.2023.10.15 val PER: 0.1042
2026-01-11 19:24:27,393: t15.2023.10.20 val PER: 0.1577
2026-01-11 19:24:27,394: t15.2023.10.22 val PER: 0.0913
2026-01-11 19:24:27,394: t15.2023.11.03 val PER: 0.1540
2026-01-11 19:24:27,394: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:24:27,394: t15.2023.11.17 val PER: 0.0218
2026-01-11 19:24:27,394: t15.2023.11.19 val PER: 0.0140
2026-01-11 19:24:27,394: t15.2023.11.26 val PER: 0.0406
2026-01-11 19:24:27,394: t15.2023.12.03 val PER: 0.0536
2026-01-11 19:24:27,394: t15.2023.12.08 val PER: 0.0360
2026-01-11 19:24:27,394: t15.2023.12.10 val PER: 0.0263
2026-01-11 19:24:27,395: t15.2023.12.17 val PER: 0.0759
2026-01-11 19:24:27,395: t15.2023.12.29 val PER: 0.0686
2026-01-11 19:24:27,395: t15.2024.02.25 val PER: 0.0871
2026-01-11 19:24:27,395: t15.2024.03.08 val PER: 0.1835
2026-01-11 19:24:27,395: t15.2024.03.15 val PER: 0.1614
2026-01-11 19:24:27,395: t15.2024.03.17 val PER: 0.0753
2026-01-11 19:24:27,395: t15.2024.05.10 val PER: 0.1129
2026-01-11 19:24:27,395: t15.2024.06.14 val PER: 0.1167
2026-01-11 19:24:27,395: t15.2024.07.19 val PER: 0.1608
2026-01-11 19:24:27,396: t15.2024.07.21 val PER: 0.0655
2026-01-11 19:24:27,396: t15.2024.07.28 val PER: 0.0904
2026-01-11 19:24:27,396: t15.2025.01.10 val PER: 0.2521
2026-01-11 19:24:27,396: t15.2025.01.12 val PER: 0.0924
2026-01-11 19:24:27,396: t15.2025.03.14 val PER: 0.2840
2026-01-11 19:24:27,396: t15.2025.03.16 val PER: 0.1165
2026-01-11 19:24:27,396: t15.2025.03.30 val PER: 0.2241
2026-01-11 19:24:27,396: t15.2025.04.13 val PER: 0.2040
2026-01-11 19:24:27,551: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_49500
2026-01-11 19:24:38,432: Train batch 49600: loss: 0.18 grad norm: 12.71 time: 0.077
2026-01-11 19:24:59,315: Train batch 49800: loss: 1.08 grad norm: 33.08 time: 0.059
2026-01-11 19:25:20,594: Train batch 50000: loss: 0.26 grad norm: 12.25 time: 0.070
2026-01-11 19:25:20,595: Running test after training batch: 50000
2026-01-11 19:25:21,305: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:25:27,626: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:25:27,703: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost dead
2026-01-11 19:25:43,918: Val batch 50000: PER (avg): 0.1056 CTC Loss (avg): 31.4428 WER(5gram): 11.41% (n=256) time: 23.323
2026-01-11 19:25:43,919: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 19:25:43,920: t15.2023.08.13 val PER: 0.0769
2026-01-11 19:25:43,920: t15.2023.08.18 val PER: 0.0629
2026-01-11 19:25:43,920: t15.2023.08.20 val PER: 0.0627
2026-01-11 19:25:43,920: t15.2023.08.25 val PER: 0.0648
2026-01-11 19:25:43,920: t15.2023.08.27 val PER: 0.1543
2026-01-11 19:25:43,921: t15.2023.09.01 val PER: 0.0398
2026-01-11 19:25:43,921: t15.2023.09.03 val PER: 0.1235
2026-01-11 19:25:43,921: t15.2023.09.24 val PER: 0.0825
2026-01-11 19:25:43,921: t15.2023.09.29 val PER: 0.0976
2026-01-11 19:25:43,921: t15.2023.10.01 val PER: 0.1440
2026-01-11 19:25:43,921: t15.2023.10.06 val PER: 0.0581
2026-01-11 19:25:43,921: t15.2023.10.08 val PER: 0.2097
2026-01-11 19:25:43,921: t15.2023.10.13 val PER: 0.1598
2026-01-11 19:25:43,922: t15.2023.10.15 val PER: 0.1088
2026-01-11 19:25:43,922: t15.2023.10.20 val PER: 0.1611
2026-01-11 19:25:43,922: t15.2023.10.22 val PER: 0.0958
2026-01-11 19:25:43,922: t15.2023.11.03 val PER: 0.1547
2026-01-11 19:25:43,922: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:25:43,922: t15.2023.11.17 val PER: 0.0187
2026-01-11 19:25:43,922: t15.2023.11.19 val PER: 0.0160
2026-01-11 19:25:43,922: t15.2023.11.26 val PER: 0.0471
2026-01-11 19:25:43,922: t15.2023.12.03 val PER: 0.0515
2026-01-11 19:25:43,923: t15.2023.12.08 val PER: 0.0393
2026-01-11 19:25:43,923: t15.2023.12.10 val PER: 0.0263
2026-01-11 19:25:43,923: t15.2023.12.17 val PER: 0.0686
2026-01-11 19:25:43,923: t15.2023.12.29 val PER: 0.0693
2026-01-11 19:25:43,923: t15.2024.02.25 val PER: 0.0815
2026-01-11 19:25:43,923: t15.2024.03.08 val PER: 0.1778
2026-01-11 19:25:43,923: t15.2024.03.15 val PER: 0.1682
2026-01-11 19:25:43,923: t15.2024.03.17 val PER: 0.0858
2026-01-11 19:25:43,923: t15.2024.05.10 val PER: 0.1144
2026-01-11 19:25:43,924: t15.2024.06.14 val PER: 0.1151
2026-01-11 19:25:43,924: t15.2024.07.19 val PER: 0.1721
2026-01-11 19:25:43,924: t15.2024.07.21 val PER: 0.0628
2026-01-11 19:25:43,924: t15.2024.07.28 val PER: 0.0860
2026-01-11 19:25:43,924: t15.2025.01.10 val PER: 0.2452
2026-01-11 19:25:43,924: t15.2025.01.12 val PER: 0.0824
2026-01-11 19:25:43,924: t15.2025.03.14 val PER: 0.2796
2026-01-11 19:25:43,924: t15.2025.03.16 val PER: 0.1283
2026-01-11 19:25:43,925: t15.2025.03.30 val PER: 0.2230
2026-01-11 19:25:43,925: t15.2025.04.13 val PER: 0.1826
2026-01-11 19:25:44,084: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_50000
2026-01-11 19:26:04,505: Train batch 50200: loss: 0.70 grad norm: 26.00 time: 0.075
2026-01-11 19:26:26,384: Train batch 50400: loss: 0.32 grad norm: 20.83 time: 0.067
2026-01-11 19:26:36,824: Running test after training batch: 50500
2026-01-11 19:26:37,005: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:26:43,138: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:26:43,218: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:27:00,630: Val batch 50500: PER (avg): 0.1058 CTC Loss (avg): 31.9634 WER(5gram): 10.89% (n=256) time: 23.805
2026-01-11 19:27:00,631: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 19:27:00,631: t15.2023.08.13 val PER: 0.0832
2026-01-11 19:27:00,631: t15.2023.08.18 val PER: 0.0629
2026-01-11 19:27:00,631: t15.2023.08.20 val PER: 0.0715
2026-01-11 19:27:00,632: t15.2023.08.25 val PER: 0.0678
2026-01-11 19:27:00,632: t15.2023.08.27 val PER: 0.1543
2026-01-11 19:27:00,632: t15.2023.09.01 val PER: 0.0373
2026-01-11 19:27:00,632: t15.2023.09.03 val PER: 0.1152
2026-01-11 19:27:00,632: t15.2023.09.24 val PER: 0.0813
2026-01-11 19:27:00,632: t15.2023.09.29 val PER: 0.1072
2026-01-11 19:27:00,632: t15.2023.10.01 val PER: 0.1387
2026-01-11 19:27:00,632: t15.2023.10.06 val PER: 0.0571
2026-01-11 19:27:00,632: t15.2023.10.08 val PER: 0.2070
2026-01-11 19:27:00,632: t15.2023.10.13 val PER: 0.1583
2026-01-11 19:27:00,632: t15.2023.10.15 val PER: 0.1074
2026-01-11 19:27:00,632: t15.2023.10.20 val PER: 0.1611
2026-01-11 19:27:00,633: t15.2023.10.22 val PER: 0.0980
2026-01-11 19:27:00,633: t15.2023.11.03 val PER: 0.1547
2026-01-11 19:27:00,633: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:27:00,633: t15.2023.11.17 val PER: 0.0264
2026-01-11 19:27:00,633: t15.2023.11.19 val PER: 0.0080
2026-01-11 19:27:00,633: t15.2023.11.26 val PER: 0.0449
2026-01-11 19:27:00,633: t15.2023.12.03 val PER: 0.0557
2026-01-11 19:27:00,633: t15.2023.12.08 val PER: 0.0453
2026-01-11 19:27:00,633: t15.2023.12.10 val PER: 0.0250
2026-01-11 19:27:00,633: t15.2023.12.17 val PER: 0.0644
2026-01-11 19:27:00,634: t15.2023.12.29 val PER: 0.0714
2026-01-11 19:27:00,635: t15.2024.02.25 val PER: 0.0843
2026-01-11 19:27:00,635: t15.2024.03.08 val PER: 0.1664
2026-01-11 19:27:00,635: t15.2024.03.15 val PER: 0.1620
2026-01-11 19:27:00,635: t15.2024.03.17 val PER: 0.0858
2026-01-11 19:27:00,635: t15.2024.05.10 val PER: 0.1144
2026-01-11 19:27:00,635: t15.2024.06.14 val PER: 0.1199
2026-01-11 19:27:00,635: t15.2024.07.19 val PER: 0.1595
2026-01-11 19:27:00,635: t15.2024.07.21 val PER: 0.0621
2026-01-11 19:27:00,635: t15.2024.07.28 val PER: 0.0897
2026-01-11 19:27:00,635: t15.2025.01.10 val PER: 0.2438
2026-01-11 19:27:00,635: t15.2025.01.12 val PER: 0.0924
2026-01-11 19:27:00,635: t15.2025.03.14 val PER: 0.2973
2026-01-11 19:27:00,636: t15.2025.03.16 val PER: 0.1204
2026-01-11 19:27:00,636: t15.2025.03.30 val PER: 0.2195
2026-01-11 19:27:00,636: t15.2025.04.13 val PER: 0.1869
2026-01-11 19:27:00,791: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_50500
2026-01-11 19:27:10,700: Train batch 50600: loss: 1.12 grad norm: 36.53 time: 0.067
2026-01-11 19:27:31,494: Train batch 50800: loss: 0.53 grad norm: 25.10 time: 0.091
2026-01-11 19:27:52,367: Train batch 51000: loss: 2.04 grad norm: 46.51 time: 0.075
2026-01-11 19:27:52,367: Running test after training batch: 51000
2026-01-11 19:27:52,543: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:27:58,945: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:27:59,072: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:28:16,544: Val batch 51000: PER (avg): 0.1064 CTC Loss (avg): 31.4940 WER(5gram): 11.15% (n=256) time: 24.176
2026-01-11 19:28:16,545: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 19:28:16,545: t15.2023.08.13 val PER: 0.0780
2026-01-11 19:28:16,545: t15.2023.08.18 val PER: 0.0637
2026-01-11 19:28:16,546: t15.2023.08.20 val PER: 0.0747
2026-01-11 19:28:16,546: t15.2023.08.25 val PER: 0.0708
2026-01-11 19:28:16,546: t15.2023.08.27 val PER: 0.1527
2026-01-11 19:28:16,546: t15.2023.09.01 val PER: 0.0438
2026-01-11 19:28:16,546: t15.2023.09.03 val PER: 0.1093
2026-01-11 19:28:16,546: t15.2023.09.24 val PER: 0.0825
2026-01-11 19:28:16,546: t15.2023.09.29 val PER: 0.1021
2026-01-11 19:28:16,547: t15.2023.10.01 val PER: 0.1433
2026-01-11 19:28:16,547: t15.2023.10.06 val PER: 0.0635
2026-01-11 19:28:16,547: t15.2023.10.08 val PER: 0.2179
2026-01-11 19:28:16,547: t15.2023.10.13 val PER: 0.1606
2026-01-11 19:28:16,547: t15.2023.10.15 val PER: 0.1134
2026-01-11 19:28:16,547: t15.2023.10.20 val PER: 0.1544
2026-01-11 19:28:16,547: t15.2023.10.22 val PER: 0.0947
2026-01-11 19:28:16,547: t15.2023.11.03 val PER: 0.1554
2026-01-11 19:28:16,547: t15.2023.11.04 val PER: 0.0102
2026-01-11 19:28:16,547: t15.2023.11.17 val PER: 0.0187
2026-01-11 19:28:16,547: t15.2023.11.19 val PER: 0.0060
2026-01-11 19:28:16,548: t15.2023.11.26 val PER: 0.0370
2026-01-11 19:28:16,548: t15.2023.12.03 val PER: 0.0504
2026-01-11 19:28:16,548: t15.2023.12.08 val PER: 0.0386
2026-01-11 19:28:16,548: t15.2023.12.10 val PER: 0.0329
2026-01-11 19:28:16,548: t15.2023.12.17 val PER: 0.0748
2026-01-11 19:28:16,548: t15.2023.12.29 val PER: 0.0659
2026-01-11 19:28:16,548: t15.2024.02.25 val PER: 0.0744
2026-01-11 19:28:16,548: t15.2024.03.08 val PER: 0.1835
2026-01-11 19:28:16,549: t15.2024.03.15 val PER: 0.1576
2026-01-11 19:28:16,549: t15.2024.03.17 val PER: 0.0802
2026-01-11 19:28:16,549: t15.2024.05.10 val PER: 0.1218
2026-01-11 19:28:16,549: t15.2024.06.14 val PER: 0.1293
2026-01-11 19:28:16,549: t15.2024.07.19 val PER: 0.1688
2026-01-11 19:28:16,549: t15.2024.07.21 val PER: 0.0593
2026-01-11 19:28:16,549: t15.2024.07.28 val PER: 0.0956
2026-01-11 19:28:16,549: t15.2025.01.10 val PER: 0.2479
2026-01-11 19:28:16,549: t15.2025.01.12 val PER: 0.0885
2026-01-11 19:28:16,550: t15.2025.03.14 val PER: 0.2870
2026-01-11 19:28:16,550: t15.2025.03.16 val PER: 0.1283
2026-01-11 19:28:16,550: t15.2025.03.30 val PER: 0.2230
2026-01-11 19:28:16,550: t15.2025.04.13 val PER: 0.1926
2026-01-11 19:28:16,709: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_51000
2026-01-11 19:28:37,954: Train batch 51200: loss: 0.39 grad norm: 18.63 time: 0.097
2026-01-11 19:28:59,289: Train batch 51400: loss: 0.85 grad norm: 27.26 time: 0.064
2026-01-11 19:29:10,556: Running test after training batch: 51500
2026-01-11 19:29:10,669: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:29:17,189: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:29:17,272: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:29:38,447: Val batch 51500: PER (avg): 0.1040 CTC Loss (avg): 31.2114 WER(5gram): 11.67% (n=256) time: 27.890
2026-01-11 19:29:38,448: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-11 19:29:38,448: t15.2023.08.13 val PER: 0.0842
2026-01-11 19:29:38,448: t15.2023.08.18 val PER: 0.0629
2026-01-11 19:29:38,448: t15.2023.08.20 val PER: 0.0643
2026-01-11 19:29:38,449: t15.2023.08.25 val PER: 0.0723
2026-01-11 19:29:38,449: t15.2023.08.27 val PER: 0.1543
2026-01-11 19:29:38,449: t15.2023.09.01 val PER: 0.0398
2026-01-11 19:29:38,449: t15.2023.09.03 val PER: 0.1188
2026-01-11 19:29:38,449: t15.2023.09.24 val PER: 0.0813
2026-01-11 19:29:38,449: t15.2023.09.29 val PER: 0.1015
2026-01-11 19:29:38,449: t15.2023.10.01 val PER: 0.1328
2026-01-11 19:29:38,450: t15.2023.10.06 val PER: 0.0571
2026-01-11 19:29:38,450: t15.2023.10.08 val PER: 0.2043
2026-01-11 19:29:38,450: t15.2023.10.13 val PER: 0.1536
2026-01-11 19:29:38,450: t15.2023.10.15 val PER: 0.1035
2026-01-11 19:29:38,450: t15.2023.10.20 val PER: 0.1611
2026-01-11 19:29:38,450: t15.2023.10.22 val PER: 0.0846
2026-01-11 19:29:38,450: t15.2023.11.03 val PER: 0.1513
2026-01-11 19:29:38,450: t15.2023.11.04 val PER: 0.0102
2026-01-11 19:29:38,450: t15.2023.11.17 val PER: 0.0156
2026-01-11 19:29:38,451: t15.2023.11.19 val PER: 0.0140
2026-01-11 19:29:38,451: t15.2023.11.26 val PER: 0.0406
2026-01-11 19:29:38,451: t15.2023.12.03 val PER: 0.0494
2026-01-11 19:29:38,451: t15.2023.12.08 val PER: 0.0353
2026-01-11 19:29:38,451: t15.2023.12.10 val PER: 0.0329
2026-01-11 19:29:38,451: t15.2023.12.17 val PER: 0.0832
2026-01-11 19:29:38,451: t15.2023.12.29 val PER: 0.0686
2026-01-11 19:29:38,451: t15.2024.02.25 val PER: 0.0758
2026-01-11 19:29:38,451: t15.2024.03.08 val PER: 0.1807
2026-01-11 19:29:38,452: t15.2024.03.15 val PER: 0.1557
2026-01-11 19:29:38,452: t15.2024.03.17 val PER: 0.0760
2026-01-11 19:29:38,452: t15.2024.05.10 val PER: 0.1189
2026-01-11 19:29:38,452: t15.2024.06.14 val PER: 0.1230
2026-01-11 19:29:38,452: t15.2024.07.19 val PER: 0.1668
2026-01-11 19:29:38,452: t15.2024.07.21 val PER: 0.0655
2026-01-11 19:29:38,452: t15.2024.07.28 val PER: 0.0868
2026-01-11 19:29:38,452: t15.2025.01.10 val PER: 0.2438
2026-01-11 19:29:38,452: t15.2025.01.12 val PER: 0.0878
2026-01-11 19:29:38,452: t15.2025.03.14 val PER: 0.2973
2026-01-11 19:29:38,453: t15.2025.03.16 val PER: 0.1257
2026-01-11 19:29:38,453: t15.2025.03.30 val PER: 0.2103
2026-01-11 19:29:38,453: t15.2025.04.13 val PER: 0.1826
2026-01-11 19:29:38,633: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_51500
2026-01-11 19:29:49,062: Train batch 51600: loss: 0.36 grad norm: 18.83 time: 0.097
2026-01-11 19:30:09,710: Train batch 51800: loss: 0.64 grad norm: 28.13 time: 0.073
2026-01-11 19:30:30,861: Train batch 52000: loss: 0.65 grad norm: 26.05 time: 0.080
2026-01-11 19:30:30,862: Running test after training batch: 52000
2026-01-11 19:30:31,074: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:30:37,400: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:30:37,494: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:30:54,467: Val batch 52000: PER (avg): 0.1058 CTC Loss (avg): 31.4666 WER(5gram): 11.08% (n=256) time: 23.605
2026-01-11 19:30:54,468: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 19:30:54,468: t15.2023.08.13 val PER: 0.0884
2026-01-11 19:30:54,468: t15.2023.08.18 val PER: 0.0629
2026-01-11 19:30:54,468: t15.2023.08.20 val PER: 0.0635
2026-01-11 19:30:54,468: t15.2023.08.25 val PER: 0.0648
2026-01-11 19:30:54,468: t15.2023.08.27 val PER: 0.1624
2026-01-11 19:30:54,468: t15.2023.09.01 val PER: 0.0406
2026-01-11 19:30:54,469: t15.2023.09.03 val PER: 0.1176
2026-01-11 19:30:54,469: t15.2023.09.24 val PER: 0.0850
2026-01-11 19:30:54,469: t15.2023.09.29 val PER: 0.1015
2026-01-11 19:30:54,469: t15.2023.10.01 val PER: 0.1400
2026-01-11 19:30:54,469: t15.2023.10.06 val PER: 0.0603
2026-01-11 19:30:54,469: t15.2023.10.08 val PER: 0.2233
2026-01-11 19:30:54,469: t15.2023.10.13 val PER: 0.1536
2026-01-11 19:30:54,469: t15.2023.10.15 val PER: 0.1088
2026-01-11 19:30:54,469: t15.2023.10.20 val PER: 0.1644
2026-01-11 19:30:54,470: t15.2023.10.22 val PER: 0.0913
2026-01-11 19:30:54,470: t15.2023.11.03 val PER: 0.1479
2026-01-11 19:30:54,470: t15.2023.11.04 val PER: 0.0102
2026-01-11 19:30:54,470: t15.2023.11.17 val PER: 0.0187
2026-01-11 19:30:54,470: t15.2023.11.19 val PER: 0.0100
2026-01-11 19:30:54,470: t15.2023.11.26 val PER: 0.0435
2026-01-11 19:30:54,470: t15.2023.12.03 val PER: 0.0504
2026-01-11 19:30:54,471: t15.2023.12.08 val PER: 0.0353
2026-01-11 19:30:54,471: t15.2023.12.10 val PER: 0.0276
2026-01-11 19:30:54,471: t15.2023.12.17 val PER: 0.0769
2026-01-11 19:30:54,471: t15.2023.12.29 val PER: 0.0652
2026-01-11 19:30:54,471: t15.2024.02.25 val PER: 0.0702
2026-01-11 19:30:54,471: t15.2024.03.08 val PER: 0.1721
2026-01-11 19:30:54,471: t15.2024.03.15 val PER: 0.1588
2026-01-11 19:30:54,471: t15.2024.03.17 val PER: 0.0858
2026-01-11 19:30:54,471: t15.2024.05.10 val PER: 0.1233
2026-01-11 19:30:54,472: t15.2024.06.14 val PER: 0.1073
2026-01-11 19:30:54,472: t15.2024.07.19 val PER: 0.1688
2026-01-11 19:30:54,472: t15.2024.07.21 val PER: 0.0607
2026-01-11 19:30:54,472: t15.2024.07.28 val PER: 0.0875
2026-01-11 19:30:54,472: t15.2025.01.10 val PER: 0.2534
2026-01-11 19:30:54,472: t15.2025.01.12 val PER: 0.0855
2026-01-11 19:30:54,472: t15.2025.03.14 val PER: 0.2811
2026-01-11 19:30:54,472: t15.2025.03.16 val PER: 0.1387
2026-01-11 19:30:54,472: t15.2025.03.30 val PER: 0.2356
2026-01-11 19:30:54,473: t15.2025.04.13 val PER: 0.2026
2026-01-11 19:30:54,645: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_52000
2026-01-11 19:31:15,681: Train batch 52200: loss: 1.15 grad norm: 39.16 time: 0.087
2026-01-11 19:31:36,596: Train batch 52400: loss: 0.54 grad norm: 23.50 time: 0.065
2026-01-11 19:31:46,939: Running test after training batch: 52500
2026-01-11 19:31:47,055: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:31:53,558: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 19:31:53,631: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:32:10,652: Val batch 52500: PER (avg): 0.1047 CTC Loss (avg): 31.8505 WER(5gram): 11.93% (n=256) time: 23.712
2026-01-11 19:32:10,654: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 19:32:10,654: t15.2023.08.13 val PER: 0.0800
2026-01-11 19:32:10,654: t15.2023.08.18 val PER: 0.0604
2026-01-11 19:32:10,654: t15.2023.08.20 val PER: 0.0627
2026-01-11 19:32:10,654: t15.2023.08.25 val PER: 0.0648
2026-01-11 19:32:10,655: t15.2023.08.27 val PER: 0.1495
2026-01-11 19:32:10,655: t15.2023.09.01 val PER: 0.0406
2026-01-11 19:32:10,655: t15.2023.09.03 val PER: 0.1081
2026-01-11 19:32:10,655: t15.2023.09.24 val PER: 0.0801
2026-01-11 19:32:10,655: t15.2023.09.29 val PER: 0.1015
2026-01-11 19:32:10,655: t15.2023.10.01 val PER: 0.1460
2026-01-11 19:32:10,656: t15.2023.10.06 val PER: 0.0581
2026-01-11 19:32:10,656: t15.2023.10.08 val PER: 0.2057
2026-01-11 19:32:10,656: t15.2023.10.13 val PER: 0.1637
2026-01-11 19:32:10,656: t15.2023.10.15 val PER: 0.1127
2026-01-11 19:32:10,656: t15.2023.10.20 val PER: 0.1678
2026-01-11 19:32:10,656: t15.2023.10.22 val PER: 0.0924
2026-01-11 19:32:10,656: t15.2023.11.03 val PER: 0.1669
2026-01-11 19:32:10,656: t15.2023.11.04 val PER: 0.0102
2026-01-11 19:32:10,656: t15.2023.11.17 val PER: 0.0233
2026-01-11 19:32:10,657: t15.2023.11.19 val PER: 0.0120
2026-01-11 19:32:10,657: t15.2023.11.26 val PER: 0.0428
2026-01-11 19:32:10,657: t15.2023.12.03 val PER: 0.0504
2026-01-11 19:32:10,657: t15.2023.12.08 val PER: 0.0373
2026-01-11 19:32:10,657: t15.2023.12.10 val PER: 0.0250
2026-01-11 19:32:10,657: t15.2023.12.17 val PER: 0.0748
2026-01-11 19:32:10,657: t15.2023.12.29 val PER: 0.0734
2026-01-11 19:32:10,657: t15.2024.02.25 val PER: 0.0688
2026-01-11 19:32:10,657: t15.2024.03.08 val PER: 0.1906
2026-01-11 19:32:10,657: t15.2024.03.15 val PER: 0.1570
2026-01-11 19:32:10,658: t15.2024.03.17 val PER: 0.0802
2026-01-11 19:32:10,658: t15.2024.05.10 val PER: 0.1114
2026-01-11 19:32:10,658: t15.2024.06.14 val PER: 0.1215
2026-01-11 19:32:10,658: t15.2024.07.19 val PER: 0.1562
2026-01-11 19:32:10,658: t15.2024.07.21 val PER: 0.0566
2026-01-11 19:32:10,658: t15.2024.07.28 val PER: 0.0831
2026-01-11 19:32:10,658: t15.2025.01.10 val PER: 0.2479
2026-01-11 19:32:10,658: t15.2025.01.12 val PER: 0.0878
2026-01-11 19:32:10,658: t15.2025.03.14 val PER: 0.2825
2026-01-11 19:32:10,659: t15.2025.03.16 val PER: 0.1374
2026-01-11 19:32:10,659: t15.2025.03.30 val PER: 0.2138
2026-01-11 19:32:10,659: t15.2025.04.13 val PER: 0.1712
2026-01-11 19:32:10,815: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_52500
2026-01-11 19:32:21,190: Train batch 52600: loss: 0.95 grad norm: 33.95 time: 0.066
2026-01-11 19:32:42,397: Train batch 52800: loss: 0.64 grad norm: 27.24 time: 0.082
2026-01-11 19:33:02,763: Train batch 53000: loss: 0.80 grad norm: 33.23 time: 0.068
2026-01-11 19:33:02,763: Running test after training batch: 53000
2026-01-11 19:33:02,953: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:33:09,626: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:33:09,702: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:33:27,732: Val batch 53000: PER (avg): 0.1040 CTC Loss (avg): 31.6148 WER(5gram): 11.47% (n=256) time: 24.969
2026-01-11 19:33:27,733: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 19:33:27,733: t15.2023.08.13 val PER: 0.0728
2026-01-11 19:33:27,734: t15.2023.08.18 val PER: 0.0704
2026-01-11 19:33:27,734: t15.2023.08.20 val PER: 0.0667
2026-01-11 19:33:27,734: t15.2023.08.25 val PER: 0.0723
2026-01-11 19:33:27,734: t15.2023.08.27 val PER: 0.1592
2026-01-11 19:33:27,734: t15.2023.09.01 val PER: 0.0446
2026-01-11 19:33:27,734: t15.2023.09.03 val PER: 0.1140
2026-01-11 19:33:27,734: t15.2023.09.24 val PER: 0.0740
2026-01-11 19:33:27,734: t15.2023.09.29 val PER: 0.1021
2026-01-11 19:33:27,735: t15.2023.10.01 val PER: 0.1347
2026-01-11 19:33:27,735: t15.2023.10.06 val PER: 0.0603
2026-01-11 19:33:27,735: t15.2023.10.08 val PER: 0.2057
2026-01-11 19:33:27,735: t15.2023.10.13 val PER: 0.1575
2026-01-11 19:33:27,735: t15.2023.10.15 val PER: 0.1101
2026-01-11 19:33:27,735: t15.2023.10.20 val PER: 0.1711
2026-01-11 19:33:27,735: t15.2023.10.22 val PER: 0.0835
2026-01-11 19:33:27,735: t15.2023.11.03 val PER: 0.1533
2026-01-11 19:33:27,736: t15.2023.11.04 val PER: 0.0137
2026-01-11 19:33:27,736: t15.2023.11.17 val PER: 0.0187
2026-01-11 19:33:27,736: t15.2023.11.19 val PER: 0.0100
2026-01-11 19:33:27,736: t15.2023.11.26 val PER: 0.0457
2026-01-11 19:33:27,736: t15.2023.12.03 val PER: 0.0462
2026-01-11 19:33:27,736: t15.2023.12.08 val PER: 0.0360
2026-01-11 19:33:27,736: t15.2023.12.10 val PER: 0.0302
2026-01-11 19:33:27,736: t15.2023.12.17 val PER: 0.0780
2026-01-11 19:33:27,736: t15.2023.12.29 val PER: 0.0721
2026-01-11 19:33:27,737: t15.2024.02.25 val PER: 0.0716
2026-01-11 19:33:27,737: t15.2024.03.08 val PER: 0.1835
2026-01-11 19:33:27,737: t15.2024.03.15 val PER: 0.1639
2026-01-11 19:33:27,737: t15.2024.03.17 val PER: 0.0795
2026-01-11 19:33:27,737: t15.2024.05.10 val PER: 0.1114
2026-01-11 19:33:27,737: t15.2024.06.14 val PER: 0.1041
2026-01-11 19:33:27,737: t15.2024.07.19 val PER: 0.1575
2026-01-11 19:33:27,737: t15.2024.07.21 val PER: 0.0545
2026-01-11 19:33:27,737: t15.2024.07.28 val PER: 0.0875
2026-01-11 19:33:27,738: t15.2025.01.10 val PER: 0.2479
2026-01-11 19:33:27,738: t15.2025.01.12 val PER: 0.0862
2026-01-11 19:33:27,738: t15.2025.03.14 val PER: 0.2929
2026-01-11 19:33:27,738: t15.2025.03.16 val PER: 0.1178
2026-01-11 19:33:27,738: t15.2025.03.30 val PER: 0.2161
2026-01-11 19:33:27,738: t15.2025.04.13 val PER: 0.1812
2026-01-11 19:33:27,895: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_53000
2026-01-11 19:33:48,911: Train batch 53200: loss: 0.57 grad norm: 28.40 time: 0.077
2026-01-11 19:34:09,589: Train batch 53400: loss: 0.29 grad norm: 26.10 time: 0.069
2026-01-11 19:34:19,679: Running test after training batch: 53500
2026-01-11 19:34:19,861: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:34:26,706: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:34:26,810: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:34:45,596: Val batch 53500: PER (avg): 0.1055 CTC Loss (avg): 32.1248 WER(5gram): 12.26% (n=256) time: 25.916
2026-01-11 19:34:45,597: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-11 19:34:45,597: t15.2023.08.13 val PER: 0.0780
2026-01-11 19:34:45,597: t15.2023.08.18 val PER: 0.0662
2026-01-11 19:34:45,597: t15.2023.08.20 val PER: 0.0627
2026-01-11 19:34:45,598: t15.2023.08.25 val PER: 0.0678
2026-01-11 19:34:45,598: t15.2023.08.27 val PER: 0.1640
2026-01-11 19:34:45,598: t15.2023.09.01 val PER: 0.0406
2026-01-11 19:34:45,598: t15.2023.09.03 val PER: 0.1081
2026-01-11 19:34:45,598: t15.2023.09.24 val PER: 0.0740
2026-01-11 19:34:45,599: t15.2023.09.29 val PER: 0.1008
2026-01-11 19:34:45,599: t15.2023.10.01 val PER: 0.1361
2026-01-11 19:34:45,599: t15.2023.10.06 val PER: 0.0624
2026-01-11 19:34:45,599: t15.2023.10.08 val PER: 0.2097
2026-01-11 19:34:45,599: t15.2023.10.13 val PER: 0.1629
2026-01-11 19:34:45,599: t15.2023.10.15 val PER: 0.1114
2026-01-11 19:34:45,599: t15.2023.10.20 val PER: 0.1644
2026-01-11 19:34:45,599: t15.2023.10.22 val PER: 0.0891
2026-01-11 19:34:45,599: t15.2023.11.03 val PER: 0.1588
2026-01-11 19:34:45,600: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:34:45,600: t15.2023.11.17 val PER: 0.0202
2026-01-11 19:34:45,600: t15.2023.11.19 val PER: 0.0120
2026-01-11 19:34:45,600: t15.2023.11.26 val PER: 0.0442
2026-01-11 19:34:45,600: t15.2023.12.03 val PER: 0.0494
2026-01-11 19:34:45,600: t15.2023.12.08 val PER: 0.0340
2026-01-11 19:34:45,600: t15.2023.12.10 val PER: 0.0342
2026-01-11 19:34:45,600: t15.2023.12.17 val PER: 0.0769
2026-01-11 19:34:45,600: t15.2023.12.29 val PER: 0.0762
2026-01-11 19:34:45,600: t15.2024.02.25 val PER: 0.0730
2026-01-11 19:34:45,601: t15.2024.03.08 val PER: 0.1622
2026-01-11 19:34:45,601: t15.2024.03.15 val PER: 0.1595
2026-01-11 19:34:45,601: t15.2024.03.17 val PER: 0.0858
2026-01-11 19:34:45,601: t15.2024.05.10 val PER: 0.1204
2026-01-11 19:34:45,601: t15.2024.06.14 val PER: 0.1246
2026-01-11 19:34:45,601: t15.2024.07.19 val PER: 0.1562
2026-01-11 19:34:45,601: t15.2024.07.21 val PER: 0.0579
2026-01-11 19:34:45,601: t15.2024.07.28 val PER: 0.0838
2026-01-11 19:34:45,601: t15.2025.01.10 val PER: 0.2521
2026-01-11 19:34:45,602: t15.2025.01.12 val PER: 0.0970
2026-01-11 19:34:45,602: t15.2025.03.14 val PER: 0.2973
2026-01-11 19:34:45,602: t15.2025.03.16 val PER: 0.1374
2026-01-11 19:34:45,602: t15.2025.03.30 val PER: 0.2230
2026-01-11 19:34:45,602: t15.2025.04.13 val PER: 0.1840
2026-01-11 19:34:45,765: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_53500
2026-01-11 19:34:56,437: Train batch 53600: loss: 0.72 grad norm: 23.32 time: 0.074
2026-01-11 19:35:16,784: Train batch 53800: loss: 0.65 grad norm: 23.20 time: 0.080
2026-01-11 19:35:37,507: Train batch 54000: loss: 0.80 grad norm: 23.38 time: 0.077
2026-01-11 19:35:37,508: Running test after training batch: 54000
2026-01-11 19:35:37,668: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:35:43,794: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:35:43,875: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:36:00,979: Val batch 54000: PER (avg): 0.1035 CTC Loss (avg): 32.1809 WER(5gram): 11.99% (n=256) time: 23.471
2026-01-11 19:36:00,981: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 19:36:00,981: t15.2023.08.13 val PER: 0.0748
2026-01-11 19:36:00,981: t15.2023.08.18 val PER: 0.0629
2026-01-11 19:36:00,982: t15.2023.08.20 val PER: 0.0651
2026-01-11 19:36:00,982: t15.2023.08.25 val PER: 0.0617
2026-01-11 19:36:00,982: t15.2023.08.27 val PER: 0.1592
2026-01-11 19:36:00,982: t15.2023.09.01 val PER: 0.0357
2026-01-11 19:36:00,982: t15.2023.09.03 val PER: 0.1105
2026-01-11 19:36:00,982: t15.2023.09.24 val PER: 0.0752
2026-01-11 19:36:00,982: t15.2023.09.29 val PER: 0.0989
2026-01-11 19:36:00,982: t15.2023.10.01 val PER: 0.1361
2026-01-11 19:36:00,982: t15.2023.10.06 val PER: 0.0538
2026-01-11 19:36:00,982: t15.2023.10.08 val PER: 0.2179
2026-01-11 19:36:00,982: t15.2023.10.13 val PER: 0.1699
2026-01-11 19:36:00,983: t15.2023.10.15 val PER: 0.0949
2026-01-11 19:36:00,983: t15.2023.10.20 val PER: 0.1477
2026-01-11 19:36:00,983: t15.2023.10.22 val PER: 0.0935
2026-01-11 19:36:00,983: t15.2023.11.03 val PER: 0.1628
2026-01-11 19:36:00,983: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:36:00,983: t15.2023.11.17 val PER: 0.0187
2026-01-11 19:36:00,983: t15.2023.11.19 val PER: 0.0160
2026-01-11 19:36:00,983: t15.2023.11.26 val PER: 0.0420
2026-01-11 19:36:00,983: t15.2023.12.03 val PER: 0.0557
2026-01-11 19:36:00,983: t15.2023.12.08 val PER: 0.0353
2026-01-11 19:36:00,984: t15.2023.12.10 val PER: 0.0329
2026-01-11 19:36:00,984: t15.2023.12.17 val PER: 0.0696
2026-01-11 19:36:00,984: t15.2023.12.29 val PER: 0.0762
2026-01-11 19:36:00,984: t15.2024.02.25 val PER: 0.0646
2026-01-11 19:36:00,984: t15.2024.03.08 val PER: 0.1508
2026-01-11 19:36:00,984: t15.2024.03.15 val PER: 0.1570
2026-01-11 19:36:00,984: t15.2024.03.17 val PER: 0.0753
2026-01-11 19:36:00,984: t15.2024.05.10 val PER: 0.1189
2026-01-11 19:36:00,984: t15.2024.06.14 val PER: 0.1278
2026-01-11 19:36:00,984: t15.2024.07.19 val PER: 0.1681
2026-01-11 19:36:00,984: t15.2024.07.21 val PER: 0.0600
2026-01-11 19:36:00,985: t15.2024.07.28 val PER: 0.0824
2026-01-11 19:36:00,985: t15.2025.01.10 val PER: 0.2397
2026-01-11 19:36:00,985: t15.2025.01.12 val PER: 0.0808
2026-01-11 19:36:00,985: t15.2025.03.14 val PER: 0.2959
2026-01-11 19:36:00,985: t15.2025.03.16 val PER: 0.1296
2026-01-11 19:36:00,985: t15.2025.03.30 val PER: 0.2253
2026-01-11 19:36:00,985: t15.2025.04.13 val PER: 0.1769
2026-01-11 19:36:01,139: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_54000
2026-01-11 19:36:21,704: Train batch 54200: loss: 0.52 grad norm: 22.26 time: 0.100
2026-01-11 19:36:42,842: Train batch 54400: loss: 0.32 grad norm: 23.66 time: 0.083
2026-01-11 19:36:53,772: Running test after training batch: 54500
2026-01-11 19:36:54,024: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:37:00,126: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:37:00,211: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:37:16,758: Val batch 54500: PER (avg): 0.1045 CTC Loss (avg): 31.6454 WER(5gram): 11.15% (n=256) time: 22.986
2026-01-11 19:37:16,759: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 19:37:16,759: t15.2023.08.13 val PER: 0.0811
2026-01-11 19:37:16,759: t15.2023.08.18 val PER: 0.0645
2026-01-11 19:37:16,760: t15.2023.08.20 val PER: 0.0635
2026-01-11 19:37:16,760: t15.2023.08.25 val PER: 0.0753
2026-01-11 19:37:16,760: t15.2023.08.27 val PER: 0.1543
2026-01-11 19:37:16,760: t15.2023.09.01 val PER: 0.0341
2026-01-11 19:37:16,760: t15.2023.09.03 val PER: 0.1105
2026-01-11 19:37:16,760: t15.2023.09.24 val PER: 0.0777
2026-01-11 19:37:16,761: t15.2023.09.29 val PER: 0.1002
2026-01-11 19:37:16,761: t15.2023.10.01 val PER: 0.1427
2026-01-11 19:37:16,761: t15.2023.10.06 val PER: 0.0527
2026-01-11 19:37:16,761: t15.2023.10.08 val PER: 0.2111
2026-01-11 19:37:16,761: t15.2023.10.13 val PER: 0.1559
2026-01-11 19:37:16,761: t15.2023.10.15 val PER: 0.1121
2026-01-11 19:37:16,761: t15.2023.10.20 val PER: 0.1611
2026-01-11 19:37:16,761: t15.2023.10.22 val PER: 0.0924
2026-01-11 19:37:16,761: t15.2023.11.03 val PER: 0.1588
2026-01-11 19:37:16,762: t15.2023.11.04 val PER: 0.0102
2026-01-11 19:37:16,762: t15.2023.11.17 val PER: 0.0202
2026-01-11 19:37:16,762: t15.2023.11.19 val PER: 0.0100
2026-01-11 19:37:16,762: t15.2023.11.26 val PER: 0.0449
2026-01-11 19:37:16,762: t15.2023.12.03 val PER: 0.0536
2026-01-11 19:37:16,762: t15.2023.12.08 val PER: 0.0320
2026-01-11 19:37:16,762: t15.2023.12.10 val PER: 0.0276
2026-01-11 19:37:16,763: t15.2023.12.17 val PER: 0.0676
2026-01-11 19:37:16,763: t15.2023.12.29 val PER: 0.0734
2026-01-11 19:37:16,763: t15.2024.02.25 val PER: 0.0772
2026-01-11 19:37:16,763: t15.2024.03.08 val PER: 0.1679
2026-01-11 19:37:16,763: t15.2024.03.15 val PER: 0.1632
2026-01-11 19:37:16,763: t15.2024.03.17 val PER: 0.0774
2026-01-11 19:37:16,763: t15.2024.05.10 val PER: 0.1218
2026-01-11 19:37:16,763: t15.2024.06.14 val PER: 0.1215
2026-01-11 19:37:16,764: t15.2024.07.19 val PER: 0.1668
2026-01-11 19:37:16,764: t15.2024.07.21 val PER: 0.0586
2026-01-11 19:37:16,764: t15.2024.07.28 val PER: 0.0831
2026-01-11 19:37:16,764: t15.2025.01.10 val PER: 0.2355
2026-01-11 19:37:16,764: t15.2025.01.12 val PER: 0.0847
2026-01-11 19:37:16,764: t15.2025.03.14 val PER: 0.2944
2026-01-11 19:37:16,764: t15.2025.03.16 val PER: 0.1270
2026-01-11 19:37:16,764: t15.2025.03.30 val PER: 0.2310
2026-01-11 19:37:16,765: t15.2025.04.13 val PER: 0.1783
2026-01-11 19:37:16,921: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_54500
2026-01-11 19:37:27,415: Train batch 54600: loss: 0.25 grad norm: 12.63 time: 0.063
2026-01-11 19:37:48,365: Train batch 54800: loss: 0.43 grad norm: 30.21 time: 0.083
2026-01-11 19:38:09,433: Train batch 55000: loss: 0.05 grad norm: 3.04 time: 0.074
2026-01-11 19:38:09,433: Running test after training batch: 55000
2026-01-11 19:38:09,708: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:38:15,836: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:38:15,921: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:38:32,991: Val batch 55000: PER (avg): 0.1057 CTC Loss (avg): 32.5258 WER(5gram): 11.47% (n=256) time: 23.558
2026-01-11 19:38:32,992: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 19:38:32,992: t15.2023.08.13 val PER: 0.0748
2026-01-11 19:38:32,992: t15.2023.08.18 val PER: 0.0654
2026-01-11 19:38:32,992: t15.2023.08.20 val PER: 0.0683
2026-01-11 19:38:32,993: t15.2023.08.25 val PER: 0.0723
2026-01-11 19:38:32,993: t15.2023.08.27 val PER: 0.1624
2026-01-11 19:38:32,993: t15.2023.09.01 val PER: 0.0357
2026-01-11 19:38:32,993: t15.2023.09.03 val PER: 0.1152
2026-01-11 19:38:32,993: t15.2023.09.24 val PER: 0.0825
2026-01-11 19:38:32,993: t15.2023.09.29 val PER: 0.1072
2026-01-11 19:38:32,993: t15.2023.10.01 val PER: 0.1427
2026-01-11 19:38:32,993: t15.2023.10.06 val PER: 0.0678
2026-01-11 19:38:32,994: t15.2023.10.08 val PER: 0.2138
2026-01-11 19:38:32,994: t15.2023.10.13 val PER: 0.1552
2026-01-11 19:38:32,994: t15.2023.10.15 val PER: 0.1074
2026-01-11 19:38:32,994: t15.2023.10.20 val PER: 0.1544
2026-01-11 19:38:32,994: t15.2023.10.22 val PER: 0.0969
2026-01-11 19:38:32,994: t15.2023.11.03 val PER: 0.1540
2026-01-11 19:38:32,994: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:38:32,994: t15.2023.11.17 val PER: 0.0187
2026-01-11 19:38:32,994: t15.2023.11.19 val PER: 0.0080
2026-01-11 19:38:32,994: t15.2023.11.26 val PER: 0.0449
2026-01-11 19:38:32,995: t15.2023.12.03 val PER: 0.0525
2026-01-11 19:38:32,995: t15.2023.12.08 val PER: 0.0446
2026-01-11 19:38:32,995: t15.2023.12.10 val PER: 0.0302
2026-01-11 19:38:32,995: t15.2023.12.17 val PER: 0.0832
2026-01-11 19:38:32,995: t15.2023.12.29 val PER: 0.0679
2026-01-11 19:38:32,995: t15.2024.02.25 val PER: 0.0801
2026-01-11 19:38:32,996: t15.2024.03.08 val PER: 0.1679
2026-01-11 19:38:32,996: t15.2024.03.15 val PER: 0.1570
2026-01-11 19:38:32,996: t15.2024.03.17 val PER: 0.0830
2026-01-11 19:38:32,996: t15.2024.05.10 val PER: 0.1233
2026-01-11 19:38:32,996: t15.2024.06.14 val PER: 0.1246
2026-01-11 19:38:32,996: t15.2024.07.19 val PER: 0.1608
2026-01-11 19:38:32,996: t15.2024.07.21 val PER: 0.0566
2026-01-11 19:38:32,996: t15.2024.07.28 val PER: 0.0949
2026-01-11 19:38:32,997: t15.2025.01.10 val PER: 0.2534
2026-01-11 19:38:32,997: t15.2025.01.12 val PER: 0.0808
2026-01-11 19:38:32,997: t15.2025.03.14 val PER: 0.2870
2026-01-11 19:38:32,997: t15.2025.03.16 val PER: 0.1230
2026-01-11 19:38:32,997: t15.2025.03.30 val PER: 0.2184
2026-01-11 19:38:32,997: t15.2025.04.13 val PER: 0.1812
2026-01-11 19:38:33,154: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_55000
2026-01-11 19:38:54,322: Train batch 55200: loss: 0.45 grad norm: 19.18 time: 0.079
2026-01-11 19:39:14,905: Train batch 55400: loss: 0.77 grad norm: 48.28 time: 0.064
2026-01-11 19:39:25,264: Running test after training batch: 55500
2026-01-11 19:39:25,580: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:39:31,768: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:39:31,847: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:39:48,966: Val batch 55500: PER (avg): 0.1034 CTC Loss (avg): 33.0042 WER(5gram): 9.71% (n=256) time: 23.702
2026-01-11 19:39:48,967: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-11 19:39:48,967: t15.2023.08.13 val PER: 0.0748
2026-01-11 19:39:48,968: t15.2023.08.18 val PER: 0.0662
2026-01-11 19:39:48,968: t15.2023.08.20 val PER: 0.0635
2026-01-11 19:39:48,968: t15.2023.08.25 val PER: 0.0723
2026-01-11 19:39:48,968: t15.2023.08.27 val PER: 0.1608
2026-01-11 19:39:48,968: t15.2023.09.01 val PER: 0.0365
2026-01-11 19:39:48,968: t15.2023.09.03 val PER: 0.1021
2026-01-11 19:39:48,969: t15.2023.09.24 val PER: 0.0789
2026-01-11 19:39:48,969: t15.2023.09.29 val PER: 0.1078
2026-01-11 19:39:48,969: t15.2023.10.01 val PER: 0.1281
2026-01-11 19:39:48,969: t15.2023.10.06 val PER: 0.0635
2026-01-11 19:39:48,969: t15.2023.10.08 val PER: 0.2070
2026-01-11 19:39:48,969: t15.2023.10.13 val PER: 0.1528
2026-01-11 19:39:48,969: t15.2023.10.15 val PER: 0.1081
2026-01-11 19:39:48,969: t15.2023.10.20 val PER: 0.1644
2026-01-11 19:39:48,969: t15.2023.10.22 val PER: 0.0913
2026-01-11 19:39:48,970: t15.2023.11.03 val PER: 0.1560
2026-01-11 19:39:48,970: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:39:48,970: t15.2023.11.17 val PER: 0.0171
2026-01-11 19:39:48,970: t15.2023.11.19 val PER: 0.0100
2026-01-11 19:39:48,970: t15.2023.11.26 val PER: 0.0464
2026-01-11 19:39:48,970: t15.2023.12.03 val PER: 0.0473
2026-01-11 19:39:48,970: t15.2023.12.08 val PER: 0.0406
2026-01-11 19:39:48,970: t15.2023.12.10 val PER: 0.0263
2026-01-11 19:39:48,971: t15.2023.12.17 val PER: 0.0738
2026-01-11 19:39:48,971: t15.2023.12.29 val PER: 0.0604
2026-01-11 19:39:48,971: t15.2024.02.25 val PER: 0.0815
2026-01-11 19:39:48,971: t15.2024.03.08 val PER: 0.1735
2026-01-11 19:39:48,971: t15.2024.03.15 val PER: 0.1607
2026-01-11 19:39:48,971: t15.2024.03.17 val PER: 0.0767
2026-01-11 19:39:48,971: t15.2024.05.10 val PER: 0.1189
2026-01-11 19:39:48,971: t15.2024.06.14 val PER: 0.1230
2026-01-11 19:39:48,971: t15.2024.07.19 val PER: 0.1674
2026-01-11 19:39:48,972: t15.2024.07.21 val PER: 0.0572
2026-01-11 19:39:48,972: t15.2024.07.28 val PER: 0.0853
2026-01-11 19:39:48,972: t15.2025.01.10 val PER: 0.2342
2026-01-11 19:39:48,972: t15.2025.01.12 val PER: 0.0801
2026-01-11 19:39:48,972: t15.2025.03.14 val PER: 0.2959
2026-01-11 19:39:48,972: t15.2025.03.16 val PER: 0.1165
2026-01-11 19:39:48,972: t15.2025.03.30 val PER: 0.2184
2026-01-11 19:39:48,972: t15.2025.04.13 val PER: 0.1883
2026-01-11 19:39:48,973: New best val WER(5gram) 10.10% --> 9.71%
2026-01-11 19:39:49,139: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_55500
2026-01-11 19:39:59,744: Train batch 55600: loss: 1.03 grad norm: 64.20 time: 0.065
2026-01-11 19:40:20,882: Train batch 55800: loss: 0.21 grad norm: 11.59 time: 0.069
2026-01-11 19:40:42,271: Train batch 56000: loss: 0.16 grad norm: 12.39 time: 0.062
2026-01-11 19:40:42,272: Running test after training batch: 56000
2026-01-11 19:40:42,500: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:40:48,599: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:40:48,680: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:41:11,739: Val batch 56000: PER (avg): 0.1042 CTC Loss (avg): 32.6586 WER(5gram): 12.52% (n=256) time: 29.467
2026-01-11 19:41:11,739: WER lens: avg_true_words=5.99 avg_pred_words=6.15 max_pred_words=13
2026-01-11 19:41:11,740: t15.2023.08.13 val PER: 0.0759
2026-01-11 19:41:11,740: t15.2023.08.18 val PER: 0.0629
2026-01-11 19:41:11,740: t15.2023.08.20 val PER: 0.0651
2026-01-11 19:41:11,740: t15.2023.08.25 val PER: 0.0678
2026-01-11 19:41:11,741: t15.2023.08.27 val PER: 0.1608
2026-01-11 19:41:11,741: t15.2023.09.01 val PER: 0.0398
2026-01-11 19:41:11,741: t15.2023.09.03 val PER: 0.1105
2026-01-11 19:41:11,741: t15.2023.09.24 val PER: 0.0862
2026-01-11 19:41:11,741: t15.2023.09.29 val PER: 0.1047
2026-01-11 19:41:11,741: t15.2023.10.01 val PER: 0.1453
2026-01-11 19:41:11,741: t15.2023.10.06 val PER: 0.0581
2026-01-11 19:41:11,741: t15.2023.10.08 val PER: 0.2057
2026-01-11 19:41:11,741: t15.2023.10.13 val PER: 0.1505
2026-01-11 19:41:11,741: t15.2023.10.15 val PER: 0.1022
2026-01-11 19:41:11,742: t15.2023.10.20 val PER: 0.1477
2026-01-11 19:41:11,742: t15.2023.10.22 val PER: 0.0902
2026-01-11 19:41:11,742: t15.2023.11.03 val PER: 0.1567
2026-01-11 19:41:11,742: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:41:11,742: t15.2023.11.17 val PER: 0.0218
2026-01-11 19:41:11,742: t15.2023.11.19 val PER: 0.0080
2026-01-11 19:41:11,742: t15.2023.11.26 val PER: 0.0406
2026-01-11 19:41:11,743: t15.2023.12.03 val PER: 0.0525
2026-01-11 19:41:11,743: t15.2023.12.08 val PER: 0.0353
2026-01-11 19:41:11,743: t15.2023.12.10 val PER: 0.0289
2026-01-11 19:41:11,743: t15.2023.12.17 val PER: 0.0780
2026-01-11 19:41:11,743: t15.2023.12.29 val PER: 0.0741
2026-01-11 19:41:11,743: t15.2024.02.25 val PER: 0.0716
2026-01-11 19:41:11,743: t15.2024.03.08 val PER: 0.1778
2026-01-11 19:41:11,743: t15.2024.03.15 val PER: 0.1614
2026-01-11 19:41:11,743: t15.2024.03.17 val PER: 0.0746
2026-01-11 19:41:11,743: t15.2024.05.10 val PER: 0.1114
2026-01-11 19:41:11,743: t15.2024.06.14 val PER: 0.1309
2026-01-11 19:41:11,744: t15.2024.07.19 val PER: 0.1635
2026-01-11 19:41:11,744: t15.2024.07.21 val PER: 0.0621
2026-01-11 19:41:11,744: t15.2024.07.28 val PER: 0.0801
2026-01-11 19:41:11,744: t15.2025.01.10 val PER: 0.2424
2026-01-11 19:41:11,744: t15.2025.01.12 val PER: 0.0793
2026-01-11 19:41:11,744: t15.2025.03.14 val PER: 0.2914
2026-01-11 19:41:11,744: t15.2025.03.16 val PER: 0.1283
2026-01-11 19:41:11,744: t15.2025.03.30 val PER: 0.2287
2026-01-11 19:41:11,744: t15.2025.04.13 val PER: 0.1812
2026-01-11 19:41:11,908: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_56000
2026-01-11 19:41:33,715: Train batch 56200: loss: 0.38 grad norm: 23.62 time: 0.084
2026-01-11 19:41:54,902: Train batch 56400: loss: 0.54 grad norm: 20.83 time: 0.067
2026-01-11 19:42:05,809: Running test after training batch: 56500
2026-01-11 19:42:06,338: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:42:12,514: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:42:12,595: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 19:42:29,779: Val batch 56500: PER (avg): 0.1052 CTC Loss (avg): 32.8341 WER(5gram): 11.34% (n=256) time: 23.970
2026-01-11 19:42:29,781: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 19:42:29,781: t15.2023.08.13 val PER: 0.0821
2026-01-11 19:42:29,781: t15.2023.08.18 val PER: 0.0595
2026-01-11 19:42:29,781: t15.2023.08.20 val PER: 0.0588
2026-01-11 19:42:29,782: t15.2023.08.25 val PER: 0.0663
2026-01-11 19:42:29,782: t15.2023.08.27 val PER: 0.1543
2026-01-11 19:42:29,782: t15.2023.09.01 val PER: 0.0422
2026-01-11 19:42:29,782: t15.2023.09.03 val PER: 0.1116
2026-01-11 19:42:29,782: t15.2023.09.24 val PER: 0.0898
2026-01-11 19:42:29,782: t15.2023.09.29 val PER: 0.1091
2026-01-11 19:42:29,782: t15.2023.10.01 val PER: 0.1387
2026-01-11 19:42:29,783: t15.2023.10.06 val PER: 0.0624
2026-01-11 19:42:29,783: t15.2023.10.08 val PER: 0.2152
2026-01-11 19:42:29,783: t15.2023.10.13 val PER: 0.1645
2026-01-11 19:42:29,783: t15.2023.10.15 val PER: 0.1061
2026-01-11 19:42:29,783: t15.2023.10.20 val PER: 0.1711
2026-01-11 19:42:29,783: t15.2023.10.22 val PER: 0.0991
2026-01-11 19:42:29,783: t15.2023.11.03 val PER: 0.1547
2026-01-11 19:42:29,783: t15.2023.11.04 val PER: 0.0068
2026-01-11 19:42:29,784: t15.2023.11.17 val PER: 0.0218
2026-01-11 19:42:29,784: t15.2023.11.19 val PER: 0.0140
2026-01-11 19:42:29,784: t15.2023.11.26 val PER: 0.0471
2026-01-11 19:42:29,784: t15.2023.12.03 val PER: 0.0525
2026-01-11 19:42:29,784: t15.2023.12.08 val PER: 0.0353
2026-01-11 19:42:29,784: t15.2023.12.10 val PER: 0.0276
2026-01-11 19:42:29,784: t15.2023.12.17 val PER: 0.0800
2026-01-11 19:42:29,784: t15.2023.12.29 val PER: 0.0645
2026-01-11 19:42:29,785: t15.2024.02.25 val PER: 0.0744
2026-01-11 19:42:29,785: t15.2024.03.08 val PER: 0.1835
2026-01-11 19:42:29,785: t15.2024.03.15 val PER: 0.1607
2026-01-11 19:42:29,785: t15.2024.03.17 val PER: 0.0844
2026-01-11 19:42:29,785: t15.2024.05.10 val PER: 0.1070
2026-01-11 19:42:29,785: t15.2024.06.14 val PER: 0.1151
2026-01-11 19:42:29,785: t15.2024.07.19 val PER: 0.1635
2026-01-11 19:42:29,785: t15.2024.07.21 val PER: 0.0607
2026-01-11 19:42:29,786: t15.2024.07.28 val PER: 0.0838
2026-01-11 19:42:29,786: t15.2025.01.10 val PER: 0.2493
2026-01-11 19:42:29,786: t15.2025.01.12 val PER: 0.0816
2026-01-11 19:42:29,786: t15.2025.03.14 val PER: 0.2870
2026-01-11 19:42:29,786: t15.2025.03.16 val PER: 0.1178
2026-01-11 19:42:29,786: t15.2025.03.30 val PER: 0.2172
2026-01-11 19:42:29,786: t15.2025.04.13 val PER: 0.1926
2026-01-11 19:42:29,946: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_56500
2026-01-11 19:42:40,558: Train batch 56600: loss: 0.26 grad norm: 18.61 time: 0.061
2026-01-11 19:43:01,726: Train batch 56800: loss: 0.20 grad norm: 12.90 time: 0.055
2026-01-11 19:43:23,112: Train batch 57000: loss: 0.12 grad norm: 5.01 time: 0.079
2026-01-11 19:43:23,113: Running test after training batch: 57000
2026-01-11 19:43:23,251: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:43:29,395: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:43:29,491: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:43:45,890: Val batch 57000: PER (avg): 0.1036 CTC Loss (avg): 32.1321 WER(5gram): 11.67% (n=256) time: 22.777
2026-01-11 19:43:45,891: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 19:43:45,891: t15.2023.08.13 val PER: 0.0769
2026-01-11 19:43:45,891: t15.2023.08.18 val PER: 0.0612
2026-01-11 19:43:45,891: t15.2023.08.20 val PER: 0.0667
2026-01-11 19:43:45,892: t15.2023.08.25 val PER: 0.0678
2026-01-11 19:43:45,892: t15.2023.08.27 val PER: 0.1592
2026-01-11 19:43:45,892: t15.2023.09.01 val PER: 0.0381
2026-01-11 19:43:45,892: t15.2023.09.03 val PER: 0.1116
2026-01-11 19:43:45,892: t15.2023.09.24 val PER: 0.0825
2026-01-11 19:43:45,892: t15.2023.09.29 val PER: 0.1059
2026-01-11 19:43:45,892: t15.2023.10.01 val PER: 0.1407
2026-01-11 19:43:45,892: t15.2023.10.06 val PER: 0.0538
2026-01-11 19:43:45,892: t15.2023.10.08 val PER: 0.2043
2026-01-11 19:43:45,892: t15.2023.10.13 val PER: 0.1567
2026-01-11 19:43:45,893: t15.2023.10.15 val PER: 0.1035
2026-01-11 19:43:45,893: t15.2023.10.20 val PER: 0.1510
2026-01-11 19:43:45,893: t15.2023.10.22 val PER: 0.0947
2026-01-11 19:43:45,893: t15.2023.11.03 val PER: 0.1540
2026-01-11 19:43:45,893: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:43:45,893: t15.2023.11.17 val PER: 0.0202
2026-01-11 19:43:45,893: t15.2023.11.19 val PER: 0.0060
2026-01-11 19:43:45,894: t15.2023.11.26 val PER: 0.0406
2026-01-11 19:43:45,894: t15.2023.12.03 val PER: 0.0504
2026-01-11 19:43:45,894: t15.2023.12.08 val PER: 0.0320
2026-01-11 19:43:45,894: t15.2023.12.10 val PER: 0.0263
2026-01-11 19:43:45,894: t15.2023.12.17 val PER: 0.0873
2026-01-11 19:43:45,894: t15.2023.12.29 val PER: 0.0638
2026-01-11 19:43:45,895: t15.2024.02.25 val PER: 0.0716
2026-01-11 19:43:45,895: t15.2024.03.08 val PER: 0.1920
2026-01-11 19:43:45,895: t15.2024.03.15 val PER: 0.1639
2026-01-11 19:43:45,895: t15.2024.03.17 val PER: 0.0802
2026-01-11 19:43:45,895: t15.2024.05.10 val PER: 0.1248
2026-01-11 19:43:45,895: t15.2024.06.14 val PER: 0.1278
2026-01-11 19:43:45,895: t15.2024.07.19 val PER: 0.1536
2026-01-11 19:43:45,895: t15.2024.07.21 val PER: 0.0634
2026-01-11 19:43:45,896: t15.2024.07.28 val PER: 0.0853
2026-01-11 19:43:45,896: t15.2025.01.10 val PER: 0.2342
2026-01-11 19:43:45,896: t15.2025.01.12 val PER: 0.0770
2026-01-11 19:43:45,896: t15.2025.03.14 val PER: 0.2855
2026-01-11 19:43:45,896: t15.2025.03.16 val PER: 0.1283
2026-01-11 19:43:45,896: t15.2025.03.30 val PER: 0.2103
2026-01-11 19:43:45,896: t15.2025.04.13 val PER: 0.1869
2026-01-11 19:43:46,057: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_57000
2026-01-11 19:44:06,426: Train batch 57200: loss: 1.54 grad norm: 57.65 time: 0.072
2026-01-11 19:44:26,575: Train batch 57400: loss: 0.74 grad norm: 34.30 time: 0.081
2026-01-11 19:44:36,934: Running test after training batch: 57500
2026-01-11 19:44:37,076: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:44:43,167: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:44:43,246: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:44:59,744: Val batch 57500: PER (avg): 0.1048 CTC Loss (avg): 32.4599 WER(5gram): 11.47% (n=256) time: 22.809
2026-01-11 19:44:59,745: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 19:44:59,745: t15.2023.08.13 val PER: 0.0738
2026-01-11 19:44:59,745: t15.2023.08.18 val PER: 0.0595
2026-01-11 19:44:59,745: t15.2023.08.20 val PER: 0.0659
2026-01-11 19:44:59,746: t15.2023.08.25 val PER: 0.0633
2026-01-11 19:44:59,746: t15.2023.08.27 val PER: 0.1559
2026-01-11 19:44:59,746: t15.2023.09.01 val PER: 0.0398
2026-01-11 19:44:59,746: t15.2023.09.03 val PER: 0.1116
2026-01-11 19:44:59,746: t15.2023.09.24 val PER: 0.0801
2026-01-11 19:44:59,746: t15.2023.09.29 val PER: 0.1053
2026-01-11 19:44:59,746: t15.2023.10.01 val PER: 0.1460
2026-01-11 19:44:59,746: t15.2023.10.06 val PER: 0.0560
2026-01-11 19:44:59,746: t15.2023.10.08 val PER: 0.2152
2026-01-11 19:44:59,747: t15.2023.10.13 val PER: 0.1575
2026-01-11 19:44:59,747: t15.2023.10.15 val PER: 0.1068
2026-01-11 19:44:59,747: t15.2023.10.20 val PER: 0.1577
2026-01-11 19:44:59,747: t15.2023.10.22 val PER: 0.0924
2026-01-11 19:44:59,747: t15.2023.11.03 val PER: 0.1588
2026-01-11 19:44:59,747: t15.2023.11.04 val PER: 0.0068
2026-01-11 19:44:59,747: t15.2023.11.17 val PER: 0.0187
2026-01-11 19:44:59,747: t15.2023.11.19 val PER: 0.0160
2026-01-11 19:44:59,747: t15.2023.11.26 val PER: 0.0435
2026-01-11 19:44:59,747: t15.2023.12.03 val PER: 0.0525
2026-01-11 19:44:59,747: t15.2023.12.08 val PER: 0.0333
2026-01-11 19:44:59,747: t15.2023.12.10 val PER: 0.0276
2026-01-11 19:44:59,748: t15.2023.12.17 val PER: 0.0769
2026-01-11 19:44:59,748: t15.2023.12.29 val PER: 0.0693
2026-01-11 19:44:59,748: t15.2024.02.25 val PER: 0.0758
2026-01-11 19:44:59,748: t15.2024.03.08 val PER: 0.1849
2026-01-11 19:44:59,748: t15.2024.03.15 val PER: 0.1645
2026-01-11 19:44:59,748: t15.2024.03.17 val PER: 0.0767
2026-01-11 19:44:59,748: t15.2024.05.10 val PER: 0.1144
2026-01-11 19:44:59,748: t15.2024.06.14 val PER: 0.1246
2026-01-11 19:44:59,748: t15.2024.07.19 val PER: 0.1615
2026-01-11 19:44:59,748: t15.2024.07.21 val PER: 0.0600
2026-01-11 19:44:59,749: t15.2024.07.28 val PER: 0.0897
2026-01-11 19:44:59,749: t15.2025.01.10 val PER: 0.2507
2026-01-11 19:44:59,749: t15.2025.01.12 val PER: 0.0739
2026-01-11 19:44:59,749: t15.2025.03.14 val PER: 0.2781
2026-01-11 19:44:59,749: t15.2025.03.16 val PER: 0.1296
2026-01-11 19:44:59,749: t15.2025.03.30 val PER: 0.2287
2026-01-11 19:44:59,749: t15.2025.04.13 val PER: 0.1854
2026-01-11 19:44:59,905: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_57500
2026-01-11 19:45:10,393: Train batch 57600: loss: 0.28 grad norm: 14.93 time: 0.066
2026-01-11 19:45:31,377: Train batch 57800: loss: 0.61 grad norm: 27.36 time: 0.063
2026-01-11 19:45:52,454: Train batch 58000: loss: 0.33 grad norm: 16.03 time: 0.077
2026-01-11 19:45:52,454: Running test after training batch: 58000
2026-01-11 19:45:52,741: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:45:58,850: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:45:58,939: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 19:46:15,740: Val batch 58000: PER (avg): 0.1049 CTC Loss (avg): 32.4976 WER(5gram): 10.76% (n=256) time: 23.285
2026-01-11 19:46:15,741: WER lens: avg_true_words=5.99 avg_pred_words=6.06 max_pred_words=12
2026-01-11 19:46:15,742: t15.2023.08.13 val PER: 0.0769
2026-01-11 19:46:15,742: t15.2023.08.18 val PER: 0.0578
2026-01-11 19:46:15,742: t15.2023.08.20 val PER: 0.0675
2026-01-11 19:46:15,742: t15.2023.08.25 val PER: 0.0678
2026-01-11 19:46:15,742: t15.2023.08.27 val PER: 0.1688
2026-01-11 19:46:15,743: t15.2023.09.01 val PER: 0.0414
2026-01-11 19:46:15,743: t15.2023.09.03 val PER: 0.1057
2026-01-11 19:46:15,743: t15.2023.09.24 val PER: 0.0765
2026-01-11 19:46:15,743: t15.2023.09.29 val PER: 0.1047
2026-01-11 19:46:15,743: t15.2023.10.01 val PER: 0.1374
2026-01-11 19:46:15,743: t15.2023.10.06 val PER: 0.0603
2026-01-11 19:46:15,743: t15.2023.10.08 val PER: 0.2124
2026-01-11 19:46:15,744: t15.2023.10.13 val PER: 0.1590
2026-01-11 19:46:15,744: t15.2023.10.15 val PER: 0.1114
2026-01-11 19:46:15,744: t15.2023.10.20 val PER: 0.1577
2026-01-11 19:46:15,744: t15.2023.10.22 val PER: 0.0958
2026-01-11 19:46:15,744: t15.2023.11.03 val PER: 0.1472
2026-01-11 19:46:15,744: t15.2023.11.04 val PER: 0.0068
2026-01-11 19:46:15,744: t15.2023.11.17 val PER: 0.0218
2026-01-11 19:46:15,744: t15.2023.11.19 val PER: 0.0160
2026-01-11 19:46:15,745: t15.2023.11.26 val PER: 0.0399
2026-01-11 19:46:15,745: t15.2023.12.03 val PER: 0.0494
2026-01-11 19:46:15,745: t15.2023.12.08 val PER: 0.0280
2026-01-11 19:46:15,745: t15.2023.12.10 val PER: 0.0302
2026-01-11 19:46:15,745: t15.2023.12.17 val PER: 0.0759
2026-01-11 19:46:15,745: t15.2023.12.29 val PER: 0.0631
2026-01-11 19:46:15,745: t15.2024.02.25 val PER: 0.0758
2026-01-11 19:46:15,745: t15.2024.03.08 val PER: 0.1707
2026-01-11 19:46:15,745: t15.2024.03.15 val PER: 0.1732
2026-01-11 19:46:15,746: t15.2024.03.17 val PER: 0.0823
2026-01-11 19:46:15,746: t15.2024.05.10 val PER: 0.1144
2026-01-11 19:46:15,746: t15.2024.06.14 val PER: 0.1230
2026-01-11 19:46:15,746: t15.2024.07.19 val PER: 0.1707
2026-01-11 19:46:15,746: t15.2024.07.21 val PER: 0.0628
2026-01-11 19:46:15,746: t15.2024.07.28 val PER: 0.0882
2026-01-11 19:46:15,746: t15.2025.01.10 val PER: 0.2355
2026-01-11 19:46:15,746: t15.2025.01.12 val PER: 0.0831
2026-01-11 19:46:15,746: t15.2025.03.14 val PER: 0.2988
2026-01-11 19:46:15,747: t15.2025.03.16 val PER: 0.1309
2026-01-11 19:46:15,747: t15.2025.03.30 val PER: 0.2287
2026-01-11 19:46:15,747: t15.2025.04.13 val PER: 0.1755
2026-01-11 19:46:15,907: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_58000
2026-01-11 19:46:36,287: Train batch 58200: loss: 0.66 grad norm: 27.69 time: 0.083
2026-01-11 19:46:56,464: Train batch 58400: loss: 0.16 grad norm: 11.20 time: 0.070
2026-01-11 19:47:07,478: Running test after training batch: 58500
2026-01-11 19:47:07,618: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:47:13,810: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:47:13,896: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 19:47:30,738: Val batch 58500: PER (avg): 0.1045 CTC Loss (avg): 33.1130 WER(5gram): 11.54% (n=256) time: 23.259
2026-01-11 19:47:30,740: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 19:47:30,740: t15.2023.08.13 val PER: 0.0759
2026-01-11 19:47:30,740: t15.2023.08.18 val PER: 0.0637
2026-01-11 19:47:30,740: t15.2023.08.20 val PER: 0.0651
2026-01-11 19:47:30,740: t15.2023.08.25 val PER: 0.0678
2026-01-11 19:47:30,740: t15.2023.08.27 val PER: 0.1720
2026-01-11 19:47:30,741: t15.2023.09.01 val PER: 0.0390
2026-01-11 19:47:30,741: t15.2023.09.03 val PER: 0.1128
2026-01-11 19:47:30,741: t15.2023.09.24 val PER: 0.0765
2026-01-11 19:47:30,741: t15.2023.09.29 val PER: 0.1040
2026-01-11 19:47:30,741: t15.2023.10.01 val PER: 0.1387
2026-01-11 19:47:30,741: t15.2023.10.06 val PER: 0.0635
2026-01-11 19:47:30,741: t15.2023.10.08 val PER: 0.2084
2026-01-11 19:47:30,741: t15.2023.10.13 val PER: 0.1575
2026-01-11 19:47:30,742: t15.2023.10.15 val PER: 0.1088
2026-01-11 19:47:30,742: t15.2023.10.20 val PER: 0.1678
2026-01-11 19:47:30,742: t15.2023.10.22 val PER: 0.0891
2026-01-11 19:47:30,742: t15.2023.11.03 val PER: 0.1574
2026-01-11 19:47:30,742: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:47:30,742: t15.2023.11.17 val PER: 0.0171
2026-01-11 19:47:30,742: t15.2023.11.19 val PER: 0.0120
2026-01-11 19:47:30,743: t15.2023.11.26 val PER: 0.0428
2026-01-11 19:47:30,743: t15.2023.12.03 val PER: 0.0515
2026-01-11 19:47:30,743: t15.2023.12.08 val PER: 0.0360
2026-01-11 19:47:30,743: t15.2023.12.10 val PER: 0.0315
2026-01-11 19:47:30,743: t15.2023.12.17 val PER: 0.0790
2026-01-11 19:47:30,743: t15.2023.12.29 val PER: 0.0679
2026-01-11 19:47:30,743: t15.2024.02.25 val PER: 0.0744
2026-01-11 19:47:30,743: t15.2024.03.08 val PER: 0.1764
2026-01-11 19:47:30,743: t15.2024.03.15 val PER: 0.1620
2026-01-11 19:47:30,744: t15.2024.03.17 val PER: 0.0788
2026-01-11 19:47:30,744: t15.2024.05.10 val PER: 0.1159
2026-01-11 19:47:30,744: t15.2024.06.14 val PER: 0.1246
2026-01-11 19:47:30,744: t15.2024.07.19 val PER: 0.1595
2026-01-11 19:47:30,744: t15.2024.07.21 val PER: 0.0545
2026-01-11 19:47:30,744: t15.2024.07.28 val PER: 0.0846
2026-01-11 19:47:30,744: t15.2025.01.10 val PER: 0.2466
2026-01-11 19:47:30,744: t15.2025.01.12 val PER: 0.0839
2026-01-11 19:47:30,744: t15.2025.03.14 val PER: 0.2840
2026-01-11 19:47:30,745: t15.2025.03.16 val PER: 0.1401
2026-01-11 19:47:30,745: t15.2025.03.30 val PER: 0.2149
2026-01-11 19:47:30,745: t15.2025.04.13 val PER: 0.1797
2026-01-11 19:47:30,904: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_58500
2026-01-11 19:47:41,296: Train batch 58600: loss: 0.72 grad norm: 20.81 time: 0.076
2026-01-11 19:48:02,215: Train batch 58800: loss: 0.57 grad norm: 26.00 time: 0.073
2026-01-11 19:48:23,081: Train batch 59000: loss: 0.53 grad norm: 22.45 time: 0.063
2026-01-11 19:48:23,082: Running test after training batch: 59000
2026-01-11 19:48:23,277: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:48:29,817: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:48:29,906: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:48:47,110: Val batch 59000: PER (avg): 0.1046 CTC Loss (avg): 32.8144 WER(5gram): 11.08% (n=256) time: 24.028
2026-01-11 19:48:47,113: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 19:48:47,113: t15.2023.08.13 val PER: 0.0780
2026-01-11 19:48:47,114: t15.2023.08.18 val PER: 0.0637
2026-01-11 19:48:47,114: t15.2023.08.20 val PER: 0.0643
2026-01-11 19:48:47,114: t15.2023.08.25 val PER: 0.0738
2026-01-11 19:48:47,114: t15.2023.08.27 val PER: 0.1592
2026-01-11 19:48:47,114: t15.2023.09.01 val PER: 0.0406
2026-01-11 19:48:47,114: t15.2023.09.03 val PER: 0.1152
2026-01-11 19:48:47,115: t15.2023.09.24 val PER: 0.0837
2026-01-11 19:48:47,115: t15.2023.09.29 val PER: 0.1034
2026-01-11 19:48:47,115: t15.2023.10.01 val PER: 0.1394
2026-01-11 19:48:47,115: t15.2023.10.06 val PER: 0.0614
2026-01-11 19:48:47,115: t15.2023.10.08 val PER: 0.2124
2026-01-11 19:48:47,115: t15.2023.10.13 val PER: 0.1458
2026-01-11 19:48:47,115: t15.2023.10.15 val PER: 0.1121
2026-01-11 19:48:47,116: t15.2023.10.20 val PER: 0.1644
2026-01-11 19:48:47,116: t15.2023.10.22 val PER: 0.0913
2026-01-11 19:48:47,116: t15.2023.11.03 val PER: 0.1506
2026-01-11 19:48:47,116: t15.2023.11.04 val PER: 0.0068
2026-01-11 19:48:47,116: t15.2023.11.17 val PER: 0.0202
2026-01-11 19:48:47,116: t15.2023.11.19 val PER: 0.0100
2026-01-11 19:48:47,116: t15.2023.11.26 val PER: 0.0413
2026-01-11 19:48:47,117: t15.2023.12.03 val PER: 0.0504
2026-01-11 19:48:47,117: t15.2023.12.08 val PER: 0.0333
2026-01-11 19:48:47,117: t15.2023.12.10 val PER: 0.0302
2026-01-11 19:48:47,117: t15.2023.12.17 val PER: 0.0769
2026-01-11 19:48:47,117: t15.2023.12.29 val PER: 0.0679
2026-01-11 19:48:47,117: t15.2024.02.25 val PER: 0.0730
2026-01-11 19:48:47,117: t15.2024.03.08 val PER: 0.1764
2026-01-11 19:48:47,117: t15.2024.03.15 val PER: 0.1639
2026-01-11 19:48:47,118: t15.2024.03.17 val PER: 0.0781
2026-01-11 19:48:47,118: t15.2024.05.10 val PER: 0.1114
2026-01-11 19:48:47,118: t15.2024.06.14 val PER: 0.1325
2026-01-11 19:48:47,118: t15.2024.07.19 val PER: 0.1615
2026-01-11 19:48:47,118: t15.2024.07.21 val PER: 0.0552
2026-01-11 19:48:47,118: t15.2024.07.28 val PER: 0.0949
2026-01-11 19:48:47,118: t15.2025.01.10 val PER: 0.2397
2026-01-11 19:48:47,118: t15.2025.01.12 val PER: 0.0816
2026-01-11 19:48:47,119: t15.2025.03.14 val PER: 0.3033
2026-01-11 19:48:47,119: t15.2025.03.16 val PER: 0.1217
2026-01-11 19:48:47,119: t15.2025.03.30 val PER: 0.2241
2026-01-11 19:48:47,119: t15.2025.04.13 val PER: 0.1840
2026-01-11 19:48:47,277: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_59000
2026-01-11 19:49:08,301: Train batch 59200: loss: 1.45 grad norm: 56.01 time: 0.101
2026-01-11 19:49:28,716: Train batch 59400: loss: 0.43 grad norm: 17.59 time: 0.095
2026-01-11 19:49:39,679: Running test after training batch: 59500
2026-01-11 19:49:39,874: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:49:46,483: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:49:46,590: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:50:03,600: Val batch 59500: PER (avg): 0.1045 CTC Loss (avg): 32.6428 WER(5gram): 11.02% (n=256) time: 23.921
2026-01-11 19:50:03,602: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 19:50:03,602: t15.2023.08.13 val PER: 0.0842
2026-01-11 19:50:03,602: t15.2023.08.18 val PER: 0.0671
2026-01-11 19:50:03,602: t15.2023.08.20 val PER: 0.0604
2026-01-11 19:50:03,602: t15.2023.08.25 val PER: 0.0693
2026-01-11 19:50:03,603: t15.2023.08.27 val PER: 0.1608
2026-01-11 19:50:03,603: t15.2023.09.01 val PER: 0.0398
2026-01-11 19:50:03,603: t15.2023.09.03 val PER: 0.1105
2026-01-11 19:50:03,603: t15.2023.09.24 val PER: 0.0789
2026-01-11 19:50:03,603: t15.2023.09.29 val PER: 0.1040
2026-01-11 19:50:03,603: t15.2023.10.01 val PER: 0.1387
2026-01-11 19:50:03,603: t15.2023.10.06 val PER: 0.0635
2026-01-11 19:50:03,603: t15.2023.10.08 val PER: 0.2192
2026-01-11 19:50:03,603: t15.2023.10.13 val PER: 0.1606
2026-01-11 19:50:03,604: t15.2023.10.15 val PER: 0.1081
2026-01-11 19:50:03,604: t15.2023.10.20 val PER: 0.1577
2026-01-11 19:50:03,604: t15.2023.10.22 val PER: 0.0913
2026-01-11 19:50:03,604: t15.2023.11.03 val PER: 0.1479
2026-01-11 19:50:03,604: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:50:03,604: t15.2023.11.17 val PER: 0.0171
2026-01-11 19:50:03,604: t15.2023.11.19 val PER: 0.0140
2026-01-11 19:50:03,604: t15.2023.11.26 val PER: 0.0406
2026-01-11 19:50:03,604: t15.2023.12.03 val PER: 0.0494
2026-01-11 19:50:03,604: t15.2023.12.08 val PER: 0.0306
2026-01-11 19:50:03,604: t15.2023.12.10 val PER: 0.0289
2026-01-11 19:50:03,604: t15.2023.12.17 val PER: 0.0790
2026-01-11 19:50:03,605: t15.2023.12.29 val PER: 0.0645
2026-01-11 19:50:03,605: t15.2024.02.25 val PER: 0.0702
2026-01-11 19:50:03,605: t15.2024.03.08 val PER: 0.1750
2026-01-11 19:50:03,605: t15.2024.03.15 val PER: 0.1582
2026-01-11 19:50:03,605: t15.2024.03.17 val PER: 0.0739
2026-01-11 19:50:03,605: t15.2024.05.10 val PER: 0.1189
2026-01-11 19:50:03,605: t15.2024.06.14 val PER: 0.1136
2026-01-11 19:50:03,605: t15.2024.07.19 val PER: 0.1688
2026-01-11 19:50:03,605: t15.2024.07.21 val PER: 0.0628
2026-01-11 19:50:03,605: t15.2024.07.28 val PER: 0.0919
2026-01-11 19:50:03,605: t15.2025.01.10 val PER: 0.2369
2026-01-11 19:50:03,605: t15.2025.01.12 val PER: 0.0893
2026-01-11 19:50:03,606: t15.2025.03.14 val PER: 0.3003
2026-01-11 19:50:03,606: t15.2025.03.16 val PER: 0.1230
2026-01-11 19:50:03,606: t15.2025.03.30 val PER: 0.2264
2026-01-11 19:50:03,606: t15.2025.04.13 val PER: 0.1812
2026-01-11 19:50:03,765: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_59500
2026-01-11 19:50:14,259: Train batch 59600: loss: 0.30 grad norm: 11.92 time: 0.058
2026-01-11 19:50:35,411: Train batch 59800: loss: 0.55 grad norm: 23.46 time: 0.074
2026-01-11 19:50:56,517: Train batch 60000: loss: 0.42 grad norm: 18.60 time: 0.112
2026-01-11 19:50:56,518: Running test after training batch: 60000
2026-01-11 19:50:56,653: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:51:03,232: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:51:03,315: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:51:20,390: Val batch 60000: PER (avg): 0.1039 CTC Loss (avg): 32.5127 WER(5gram): 10.89% (n=256) time: 23.871
2026-01-11 19:51:20,391: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 19:51:20,391: t15.2023.08.13 val PER: 0.0790
2026-01-11 19:51:20,392: t15.2023.08.18 val PER: 0.0637
2026-01-11 19:51:20,392: t15.2023.08.20 val PER: 0.0612
2026-01-11 19:51:20,392: t15.2023.08.25 val PER: 0.0602
2026-01-11 19:51:20,392: t15.2023.08.27 val PER: 0.1543
2026-01-11 19:51:20,392: t15.2023.09.01 val PER: 0.0430
2026-01-11 19:51:20,392: t15.2023.09.03 val PER: 0.1093
2026-01-11 19:51:20,392: t15.2023.09.24 val PER: 0.0813
2026-01-11 19:51:20,392: t15.2023.09.29 val PER: 0.1034
2026-01-11 19:51:20,392: t15.2023.10.01 val PER: 0.1387
2026-01-11 19:51:20,393: t15.2023.10.06 val PER: 0.0581
2026-01-11 19:51:20,393: t15.2023.10.08 val PER: 0.2043
2026-01-11 19:51:20,393: t15.2023.10.13 val PER: 0.1536
2026-01-11 19:51:20,393: t15.2023.10.15 val PER: 0.1088
2026-01-11 19:51:20,393: t15.2023.10.20 val PER: 0.1544
2026-01-11 19:51:20,393: t15.2023.10.22 val PER: 0.0902
2026-01-11 19:51:20,393: t15.2023.11.03 val PER: 0.1526
2026-01-11 19:51:20,393: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:51:20,393: t15.2023.11.17 val PER: 0.0218
2026-01-11 19:51:20,394: t15.2023.11.19 val PER: 0.0120
2026-01-11 19:51:20,394: t15.2023.11.26 val PER: 0.0370
2026-01-11 19:51:20,394: t15.2023.12.03 val PER: 0.0557
2026-01-11 19:51:20,394: t15.2023.12.08 val PER: 0.0320
2026-01-11 19:51:20,394: t15.2023.12.10 val PER: 0.0329
2026-01-11 19:51:20,394: t15.2023.12.17 val PER: 0.0738
2026-01-11 19:51:20,394: t15.2023.12.29 val PER: 0.0666
2026-01-11 19:51:20,394: t15.2024.02.25 val PER: 0.0688
2026-01-11 19:51:20,394: t15.2024.03.08 val PER: 0.1693
2026-01-11 19:51:20,394: t15.2024.03.15 val PER: 0.1664
2026-01-11 19:51:20,394: t15.2024.03.17 val PER: 0.0739
2026-01-11 19:51:20,395: t15.2024.05.10 val PER: 0.1204
2026-01-11 19:51:20,395: t15.2024.06.14 val PER: 0.1246
2026-01-11 19:51:20,395: t15.2024.07.19 val PER: 0.1648
2026-01-11 19:51:20,395: t15.2024.07.21 val PER: 0.0566
2026-01-11 19:51:20,395: t15.2024.07.28 val PER: 0.0875
2026-01-11 19:51:20,395: t15.2025.01.10 val PER: 0.2424
2026-01-11 19:51:20,395: t15.2025.01.12 val PER: 0.0831
2026-01-11 19:51:20,395: t15.2025.03.14 val PER: 0.2914
2026-01-11 19:51:20,395: t15.2025.03.16 val PER: 0.1230
2026-01-11 19:51:20,395: t15.2025.03.30 val PER: 0.2264
2026-01-11 19:51:20,395: t15.2025.04.13 val PER: 0.2040
2026-01-11 19:51:20,559: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_60000
2026-01-11 19:51:40,984: Train batch 60200: loss: 0.17 grad norm: 10.46 time: 0.086
2026-01-11 19:52:01,288: Train batch 60400: loss: 0.42 grad norm: 17.57 time: 0.064
2026-01-11 19:52:11,685: Running test after training batch: 60500
2026-01-11 19:52:11,851: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:52:18,008: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:52:18,092: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:52:34,099: Val batch 60500: PER (avg): 0.1046 CTC Loss (avg): 32.7214 WER(5gram): 11.15% (n=256) time: 22.413
2026-01-11 19:52:34,101: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-11 19:52:34,101: t15.2023.08.13 val PER: 0.0873
2026-01-11 19:52:34,101: t15.2023.08.18 val PER: 0.0679
2026-01-11 19:52:34,101: t15.2023.08.20 val PER: 0.0675
2026-01-11 19:52:34,101: t15.2023.08.25 val PER: 0.0693
2026-01-11 19:52:34,102: t15.2023.08.27 val PER: 0.1559
2026-01-11 19:52:34,102: t15.2023.09.01 val PER: 0.0414
2026-01-11 19:52:34,102: t15.2023.09.03 val PER: 0.1152
2026-01-11 19:52:34,102: t15.2023.09.24 val PER: 0.0777
2026-01-11 19:52:34,102: t15.2023.09.29 val PER: 0.1015
2026-01-11 19:52:34,102: t15.2023.10.01 val PER: 0.1328
2026-01-11 19:52:34,102: t15.2023.10.06 val PER: 0.0603
2026-01-11 19:52:34,103: t15.2023.10.08 val PER: 0.2165
2026-01-11 19:52:34,103: t15.2023.10.13 val PER: 0.1583
2026-01-11 19:52:34,103: t15.2023.10.15 val PER: 0.1101
2026-01-11 19:52:34,103: t15.2023.10.20 val PER: 0.1577
2026-01-11 19:52:34,103: t15.2023.10.22 val PER: 0.0913
2026-01-11 19:52:34,103: t15.2023.11.03 val PER: 0.1615
2026-01-11 19:52:34,103: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:52:34,103: t15.2023.11.17 val PER: 0.0187
2026-01-11 19:52:34,104: t15.2023.11.19 val PER: 0.0160
2026-01-11 19:52:34,104: t15.2023.11.26 val PER: 0.0420
2026-01-11 19:52:34,104: t15.2023.12.03 val PER: 0.0578
2026-01-11 19:52:34,104: t15.2023.12.08 val PER: 0.0313
2026-01-11 19:52:34,104: t15.2023.12.10 val PER: 0.0315
2026-01-11 19:52:34,104: t15.2023.12.17 val PER: 0.0738
2026-01-11 19:52:34,104: t15.2023.12.29 val PER: 0.0625
2026-01-11 19:52:34,104: t15.2024.02.25 val PER: 0.0716
2026-01-11 19:52:34,104: t15.2024.03.08 val PER: 0.1750
2026-01-11 19:52:34,105: t15.2024.03.15 val PER: 0.1601
2026-01-11 19:52:34,105: t15.2024.03.17 val PER: 0.0823
2026-01-11 19:52:34,105: t15.2024.05.10 val PER: 0.1114
2026-01-11 19:52:34,105: t15.2024.06.14 val PER: 0.1215
2026-01-11 19:52:34,105: t15.2024.07.19 val PER: 0.1575
2026-01-11 19:52:34,105: t15.2024.07.21 val PER: 0.0648
2026-01-11 19:52:34,105: t15.2024.07.28 val PER: 0.0816
2026-01-11 19:52:34,105: t15.2025.01.10 val PER: 0.2383
2026-01-11 19:52:34,105: t15.2025.01.12 val PER: 0.0847
2026-01-11 19:52:34,106: t15.2025.03.14 val PER: 0.2988
2026-01-11 19:52:34,106: t15.2025.03.16 val PER: 0.1230
2026-01-11 19:52:34,106: t15.2025.03.30 val PER: 0.2184
2026-01-11 19:52:34,106: t15.2025.04.13 val PER: 0.1869
2026-01-11 19:52:34,261: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_60500
2026-01-11 19:52:44,832: Train batch 60600: loss: 0.95 grad norm: 50.69 time: 0.103
2026-01-11 19:53:05,312: Train batch 60800: loss: 0.16 grad norm: 7.87 time: 0.093
2026-01-11 19:53:26,150: Train batch 61000: loss: 0.18 grad norm: 12.98 time: 0.082
2026-01-11 19:53:26,151: Running test after training batch: 61000
2026-01-11 19:53:26,284: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:53:32,415: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:53:32,491: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:53:50,810: Val batch 61000: PER (avg): 0.1041 CTC Loss (avg): 33.2111 WER(5gram): 11.54% (n=256) time: 24.659
2026-01-11 19:53:50,811: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 19:53:50,811: t15.2023.08.13 val PER: 0.0748
2026-01-11 19:53:50,811: t15.2023.08.18 val PER: 0.0612
2026-01-11 19:53:50,811: t15.2023.08.20 val PER: 0.0683
2026-01-11 19:53:50,812: t15.2023.08.25 val PER: 0.0648
2026-01-11 19:53:50,812: t15.2023.08.27 val PER: 0.1511
2026-01-11 19:53:50,812: t15.2023.09.01 val PER: 0.0390
2026-01-11 19:53:50,812: t15.2023.09.03 val PER: 0.1105
2026-01-11 19:53:50,812: t15.2023.09.24 val PER: 0.0789
2026-01-11 19:53:50,812: t15.2023.09.29 val PER: 0.1091
2026-01-11 19:53:50,812: t15.2023.10.01 val PER: 0.1308
2026-01-11 19:53:50,813: t15.2023.10.06 val PER: 0.0635
2026-01-11 19:53:50,813: t15.2023.10.08 val PER: 0.2003
2026-01-11 19:53:50,813: t15.2023.10.13 val PER: 0.1583
2026-01-11 19:53:50,813: t15.2023.10.15 val PER: 0.1114
2026-01-11 19:53:50,813: t15.2023.10.20 val PER: 0.1644
2026-01-11 19:53:50,813: t15.2023.10.22 val PER: 0.0846
2026-01-11 19:53:50,813: t15.2023.11.03 val PER: 0.1554
2026-01-11 19:53:50,813: t15.2023.11.04 val PER: 0.0068
2026-01-11 19:53:50,814: t15.2023.11.17 val PER: 0.0202
2026-01-11 19:53:50,814: t15.2023.11.19 val PER: 0.0140
2026-01-11 19:53:50,814: t15.2023.11.26 val PER: 0.0399
2026-01-11 19:53:50,814: t15.2023.12.03 val PER: 0.0504
2026-01-11 19:53:50,814: t15.2023.12.08 val PER: 0.0373
2026-01-11 19:53:50,814: t15.2023.12.10 val PER: 0.0315
2026-01-11 19:53:50,814: t15.2023.12.17 val PER: 0.0769
2026-01-11 19:53:50,814: t15.2023.12.29 val PER: 0.0707
2026-01-11 19:53:50,814: t15.2024.02.25 val PER: 0.0772
2026-01-11 19:53:50,815: t15.2024.03.08 val PER: 0.1835
2026-01-11 19:53:50,815: t15.2024.03.15 val PER: 0.1576
2026-01-11 19:53:50,815: t15.2024.03.17 val PER: 0.0830
2026-01-11 19:53:50,815: t15.2024.05.10 val PER: 0.1248
2026-01-11 19:53:50,815: t15.2024.06.14 val PER: 0.1309
2026-01-11 19:53:50,815: t15.2024.07.19 val PER: 0.1622
2026-01-11 19:53:50,815: t15.2024.07.21 val PER: 0.0634
2026-01-11 19:53:50,815: t15.2024.07.28 val PER: 0.0846
2026-01-11 19:53:50,815: t15.2025.01.10 val PER: 0.2204
2026-01-11 19:53:50,815: t15.2025.01.12 val PER: 0.0870
2026-01-11 19:53:50,815: t15.2025.03.14 val PER: 0.2811
2026-01-11 19:53:50,816: t15.2025.03.16 val PER: 0.1178
2026-01-11 19:53:50,816: t15.2025.03.30 val PER: 0.2218
2026-01-11 19:53:50,816: t15.2025.04.13 val PER: 0.1840
2026-01-11 19:53:50,970: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_61000
2026-01-11 19:54:11,357: Train batch 61200: loss: 0.23 grad norm: 12.65 time: 0.071
2026-01-11 19:54:32,565: Train batch 61400: loss: 0.10 grad norm: 5.74 time: 0.077
2026-01-11 19:54:43,053: Running test after training batch: 61500
2026-01-11 19:54:43,219: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:54:49,348: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:54:49,424: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:55:05,936: Val batch 61500: PER (avg): 0.1016 CTC Loss (avg): 32.4664 WER(5gram): 10.89% (n=256) time: 22.883
2026-01-11 19:55:05,938: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 19:55:05,939: t15.2023.08.13 val PER: 0.0748
2026-01-11 19:55:05,939: t15.2023.08.18 val PER: 0.0654
2026-01-11 19:55:05,939: t15.2023.08.20 val PER: 0.0675
2026-01-11 19:55:05,939: t15.2023.08.25 val PER: 0.0633
2026-01-11 19:55:05,939: t15.2023.08.27 val PER: 0.1463
2026-01-11 19:55:05,940: t15.2023.09.01 val PER: 0.0430
2026-01-11 19:55:05,940: t15.2023.09.03 val PER: 0.1140
2026-01-11 19:55:05,940: t15.2023.09.24 val PER: 0.0777
2026-01-11 19:55:05,940: t15.2023.09.29 val PER: 0.1034
2026-01-11 19:55:05,940: t15.2023.10.01 val PER: 0.1314
2026-01-11 19:55:05,940: t15.2023.10.06 val PER: 0.0527
2026-01-11 19:55:05,940: t15.2023.10.08 val PER: 0.2043
2026-01-11 19:55:05,940: t15.2023.10.13 val PER: 0.1606
2026-01-11 19:55:05,940: t15.2023.10.15 val PER: 0.1101
2026-01-11 19:55:05,941: t15.2023.10.20 val PER: 0.1611
2026-01-11 19:55:05,941: t15.2023.10.22 val PER: 0.0802
2026-01-11 19:55:05,941: t15.2023.11.03 val PER: 0.1547
2026-01-11 19:55:05,941: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:55:05,941: t15.2023.11.17 val PER: 0.0202
2026-01-11 19:55:05,941: t15.2023.11.19 val PER: 0.0120
2026-01-11 19:55:05,941: t15.2023.11.26 val PER: 0.0457
2026-01-11 19:55:05,941: t15.2023.12.03 val PER: 0.0536
2026-01-11 19:55:05,941: t15.2023.12.08 val PER: 0.0346
2026-01-11 19:55:05,941: t15.2023.12.10 val PER: 0.0302
2026-01-11 19:55:05,941: t15.2023.12.17 val PER: 0.0707
2026-01-11 19:55:05,942: t15.2023.12.29 val PER: 0.0618
2026-01-11 19:55:05,942: t15.2024.02.25 val PER: 0.0772
2026-01-11 19:55:05,942: t15.2024.03.08 val PER: 0.1764
2026-01-11 19:55:05,942: t15.2024.03.15 val PER: 0.1507
2026-01-11 19:55:05,942: t15.2024.03.17 val PER: 0.0739
2026-01-11 19:55:05,942: t15.2024.05.10 val PER: 0.1070
2026-01-11 19:55:05,942: t15.2024.06.14 val PER: 0.1151
2026-01-11 19:55:05,942: t15.2024.07.19 val PER: 0.1575
2026-01-11 19:55:05,942: t15.2024.07.21 val PER: 0.0559
2026-01-11 19:55:05,943: t15.2024.07.28 val PER: 0.0816
2026-01-11 19:55:05,943: t15.2025.01.10 val PER: 0.2314
2026-01-11 19:55:05,943: t15.2025.01.12 val PER: 0.0847
2026-01-11 19:55:05,943: t15.2025.03.14 val PER: 0.2855
2026-01-11 19:55:05,943: t15.2025.03.16 val PER: 0.1126
2026-01-11 19:55:05,943: t15.2025.03.30 val PER: 0.2195
2026-01-11 19:55:05,943: t15.2025.04.13 val PER: 0.1797
2026-01-11 19:55:06,103: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_61500
2026-01-11 19:55:16,208: Train batch 61600: loss: 0.28 grad norm: 15.36 time: 0.072
2026-01-11 19:55:37,373: Train batch 61800: loss: 0.20 grad norm: 9.66 time: 0.091
2026-01-11 19:55:58,463: Train batch 62000: loss: 0.38 grad norm: 22.38 time: 0.077
2026-01-11 19:55:58,464: Running test after training batch: 62000
2026-01-11 19:55:58,634: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:56:04,769: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:56:04,913: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 19:56:22,532: Val batch 62000: PER (avg): 0.1035 CTC Loss (avg): 33.0546 WER(5gram): 10.69% (n=256) time: 24.068
2026-01-11 19:56:22,533: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 19:56:22,533: t15.2023.08.13 val PER: 0.0769
2026-01-11 19:56:22,533: t15.2023.08.18 val PER: 0.0696
2026-01-11 19:56:22,533: t15.2023.08.20 val PER: 0.0683
2026-01-11 19:56:22,534: t15.2023.08.25 val PER: 0.0633
2026-01-11 19:56:22,534: t15.2023.08.27 val PER: 0.1543
2026-01-11 19:56:22,534: t15.2023.09.01 val PER: 0.0349
2026-01-11 19:56:22,534: t15.2023.09.03 val PER: 0.1164
2026-01-11 19:56:22,534: t15.2023.09.24 val PER: 0.0777
2026-01-11 19:56:22,534: t15.2023.09.29 val PER: 0.1098
2026-01-11 19:56:22,534: t15.2023.10.01 val PER: 0.1354
2026-01-11 19:56:22,534: t15.2023.10.06 val PER: 0.0517
2026-01-11 19:56:22,534: t15.2023.10.08 val PER: 0.1949
2026-01-11 19:56:22,535: t15.2023.10.13 val PER: 0.1559
2026-01-11 19:56:22,535: t15.2023.10.15 val PER: 0.1061
2026-01-11 19:56:22,535: t15.2023.10.20 val PER: 0.1678
2026-01-11 19:56:22,535: t15.2023.10.22 val PER: 0.0835
2026-01-11 19:56:22,535: t15.2023.11.03 val PER: 0.1499
2026-01-11 19:56:22,535: t15.2023.11.04 val PER: 0.0034
2026-01-11 19:56:22,535: t15.2023.11.17 val PER: 0.0187
2026-01-11 19:56:22,536: t15.2023.11.19 val PER: 0.0140
2026-01-11 19:56:22,536: t15.2023.11.26 val PER: 0.0406
2026-01-11 19:56:22,536: t15.2023.12.03 val PER: 0.0494
2026-01-11 19:56:22,536: t15.2023.12.08 val PER: 0.0346
2026-01-11 19:56:22,536: t15.2023.12.10 val PER: 0.0250
2026-01-11 19:56:22,536: t15.2023.12.17 val PER: 0.0811
2026-01-11 19:56:22,536: t15.2023.12.29 val PER: 0.0659
2026-01-11 19:56:22,536: t15.2024.02.25 val PER: 0.0758
2026-01-11 19:56:22,537: t15.2024.03.08 val PER: 0.1764
2026-01-11 19:56:22,537: t15.2024.03.15 val PER: 0.1582
2026-01-11 19:56:22,537: t15.2024.03.17 val PER: 0.0746
2026-01-11 19:56:22,537: t15.2024.05.10 val PER: 0.1144
2026-01-11 19:56:22,537: t15.2024.06.14 val PER: 0.1183
2026-01-11 19:56:22,537: t15.2024.07.19 val PER: 0.1628
2026-01-11 19:56:22,537: t15.2024.07.21 val PER: 0.0648
2026-01-11 19:56:22,537: t15.2024.07.28 val PER: 0.0846
2026-01-11 19:56:22,537: t15.2025.01.10 val PER: 0.2493
2026-01-11 19:56:22,537: t15.2025.01.12 val PER: 0.0901
2026-01-11 19:56:22,538: t15.2025.03.14 val PER: 0.2899
2026-01-11 19:56:22,538: t15.2025.03.16 val PER: 0.1152
2026-01-11 19:56:22,538: t15.2025.03.30 val PER: 0.2230
2026-01-11 19:56:22,538: t15.2025.04.13 val PER: 0.1897
2026-01-11 19:56:22,693: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_62000
2026-01-11 19:56:43,985: Train batch 62200: loss: 0.26 grad norm: 19.04 time: 0.076
2026-01-11 19:57:04,862: Train batch 62400: loss: 0.33 grad norm: 16.07 time: 0.080
2026-01-11 19:57:15,561: Running test after training batch: 62500
2026-01-11 19:57:15,794: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:57:22,680: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:57:22,772: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:57:40,943: Val batch 62500: PER (avg): 0.1044 CTC Loss (avg): 33.1858 WER(5gram): 10.56% (n=256) time: 25.381
2026-01-11 19:57:40,944: WER lens: avg_true_words=5.99 avg_pred_words=6.06 max_pred_words=12
2026-01-11 19:57:40,944: t15.2023.08.13 val PER: 0.0738
2026-01-11 19:57:40,944: t15.2023.08.18 val PER: 0.0712
2026-01-11 19:57:40,944: t15.2023.08.20 val PER: 0.0715
2026-01-11 19:57:40,944: t15.2023.08.25 val PER: 0.0663
2026-01-11 19:57:40,945: t15.2023.08.27 val PER: 0.1624
2026-01-11 19:57:40,945: t15.2023.09.01 val PER: 0.0430
2026-01-11 19:57:40,945: t15.2023.09.03 val PER: 0.1116
2026-01-11 19:57:40,946: t15.2023.09.24 val PER: 0.0874
2026-01-11 19:57:40,946: t15.2023.09.29 val PER: 0.1117
2026-01-11 19:57:40,946: t15.2023.10.01 val PER: 0.1374
2026-01-11 19:57:40,946: t15.2023.10.06 val PER: 0.0571
2026-01-11 19:57:40,946: t15.2023.10.08 val PER: 0.2043
2026-01-11 19:57:40,946: t15.2023.10.13 val PER: 0.1606
2026-01-11 19:57:40,946: t15.2023.10.15 val PER: 0.1015
2026-01-11 19:57:40,947: t15.2023.10.20 val PER: 0.1812
2026-01-11 19:57:40,947: t15.2023.10.22 val PER: 0.0835
2026-01-11 19:57:40,947: t15.2023.11.03 val PER: 0.1520
2026-01-11 19:57:40,947: t15.2023.11.04 val PER: 0.0102
2026-01-11 19:57:40,947: t15.2023.11.17 val PER: 0.0202
2026-01-11 19:57:40,947: t15.2023.11.19 val PER: 0.0120
2026-01-11 19:57:40,947: t15.2023.11.26 val PER: 0.0442
2026-01-11 19:57:40,947: t15.2023.12.03 val PER: 0.0557
2026-01-11 19:57:40,947: t15.2023.12.08 val PER: 0.0340
2026-01-11 19:57:40,947: t15.2023.12.10 val PER: 0.0263
2026-01-11 19:57:40,947: t15.2023.12.17 val PER: 0.0811
2026-01-11 19:57:40,947: t15.2023.12.29 val PER: 0.0700
2026-01-11 19:57:40,948: t15.2024.02.25 val PER: 0.0674
2026-01-11 19:57:40,948: t15.2024.03.08 val PER: 0.1792
2026-01-11 19:57:40,948: t15.2024.03.15 val PER: 0.1588
2026-01-11 19:57:40,948: t15.2024.03.17 val PER: 0.0753
2026-01-11 19:57:40,948: t15.2024.05.10 val PER: 0.1233
2026-01-11 19:57:40,948: t15.2024.06.14 val PER: 0.1167
2026-01-11 19:57:40,948: t15.2024.07.19 val PER: 0.1569
2026-01-11 19:57:40,948: t15.2024.07.21 val PER: 0.0579
2026-01-11 19:57:40,948: t15.2024.07.28 val PER: 0.0853
2026-01-11 19:57:40,948: t15.2025.01.10 val PER: 0.2273
2026-01-11 19:57:40,949: t15.2025.01.12 val PER: 0.0839
2026-01-11 19:57:40,949: t15.2025.03.14 val PER: 0.3077
2026-01-11 19:57:40,949: t15.2025.03.16 val PER: 0.1178
2026-01-11 19:57:40,949: t15.2025.03.30 val PER: 0.2253
2026-01-11 19:57:40,949: t15.2025.04.13 val PER: 0.1797
2026-01-11 19:57:41,136: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_62500
2026-01-11 19:57:51,365: Train batch 62600: loss: 0.66 grad norm: 29.00 time: 0.058
2026-01-11 19:58:12,596: Train batch 62800: loss: 0.83 grad norm: 49.94 time: 0.090
2026-01-11 19:58:33,985: Train batch 63000: loss: 0.35 grad norm: 14.93 time: 0.067
2026-01-11 19:58:33,985: Running test after training batch: 63000
2026-01-11 19:58:34,233: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:58:41,388: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:58:41,467: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 19:58:59,058: Val batch 63000: PER (avg): 0.1032 CTC Loss (avg): 33.2085 WER(5gram): 10.69% (n=256) time: 25.072
2026-01-11 19:58:59,059: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 19:58:59,060: t15.2023.08.13 val PER: 0.0748
2026-01-11 19:58:59,060: t15.2023.08.18 val PER: 0.0612
2026-01-11 19:58:59,060: t15.2023.08.20 val PER: 0.0643
2026-01-11 19:58:59,060: t15.2023.08.25 val PER: 0.0648
2026-01-11 19:58:59,060: t15.2023.08.27 val PER: 0.1543
2026-01-11 19:58:59,060: t15.2023.09.01 val PER: 0.0446
2026-01-11 19:58:59,060: t15.2023.09.03 val PER: 0.1081
2026-01-11 19:58:59,061: t15.2023.09.24 val PER: 0.0813
2026-01-11 19:58:59,061: t15.2023.09.29 val PER: 0.1047
2026-01-11 19:58:59,061: t15.2023.10.01 val PER: 0.1367
2026-01-11 19:58:59,061: t15.2023.10.06 val PER: 0.0635
2026-01-11 19:58:59,061: t15.2023.10.08 val PER: 0.2057
2026-01-11 19:58:59,061: t15.2023.10.13 val PER: 0.1552
2026-01-11 19:58:59,061: t15.2023.10.15 val PER: 0.1081
2026-01-11 19:58:59,061: t15.2023.10.20 val PER: 0.1577
2026-01-11 19:58:59,062: t15.2023.10.22 val PER: 0.0846
2026-01-11 19:58:59,062: t15.2023.11.03 val PER: 0.1581
2026-01-11 19:58:59,062: t15.2023.11.04 val PER: 0.0068
2026-01-11 19:58:59,062: t15.2023.11.17 val PER: 0.0187
2026-01-11 19:58:59,062: t15.2023.11.19 val PER: 0.0140
2026-01-11 19:58:59,062: t15.2023.11.26 val PER: 0.0435
2026-01-11 19:58:59,062: t15.2023.12.03 val PER: 0.0546
2026-01-11 19:58:59,062: t15.2023.12.08 val PER: 0.0346
2026-01-11 19:58:59,062: t15.2023.12.10 val PER: 0.0315
2026-01-11 19:58:59,063: t15.2023.12.17 val PER: 0.0728
2026-01-11 19:58:59,063: t15.2023.12.29 val PER: 0.0652
2026-01-11 19:58:59,063: t15.2024.02.25 val PER: 0.0716
2026-01-11 19:58:59,063: t15.2024.03.08 val PER: 0.1764
2026-01-11 19:58:59,063: t15.2024.03.15 val PER: 0.1570
2026-01-11 19:58:59,063: t15.2024.03.17 val PER: 0.0732
2026-01-11 19:58:59,063: t15.2024.05.10 val PER: 0.1174
2026-01-11 19:58:59,063: t15.2024.06.14 val PER: 0.1199
2026-01-11 19:58:59,063: t15.2024.07.19 val PER: 0.1615
2026-01-11 19:58:59,064: t15.2024.07.21 val PER: 0.0628
2026-01-11 19:58:59,064: t15.2024.07.28 val PER: 0.0919
2026-01-11 19:58:59,064: t15.2025.01.10 val PER: 0.2314
2026-01-11 19:58:59,064: t15.2025.01.12 val PER: 0.0808
2026-01-11 19:58:59,064: t15.2025.03.14 val PER: 0.2825
2026-01-11 19:58:59,064: t15.2025.03.16 val PER: 0.1086
2026-01-11 19:58:59,064: t15.2025.03.30 val PER: 0.2287
2026-01-11 19:58:59,064: t15.2025.04.13 val PER: 0.1797
2026-01-11 19:58:59,220: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_63000
2026-01-11 19:59:20,351: Train batch 63200: loss: 0.26 grad norm: 11.87 time: 0.072
2026-01-11 19:59:42,230: Train batch 63400: loss: 0.14 grad norm: 8.68 time: 0.081
2026-01-11 19:59:53,491: Running test after training batch: 63500
2026-01-11 19:59:53,672: WER debug GT example: You can see the code at this point as well.
2026-01-11 19:59:59,757: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 19:59:59,835: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:00:18,946: Val batch 63500: PER (avg): 0.1038 CTC Loss (avg): 33.8374 WER(5gram): 11.21% (n=256) time: 25.454
2026-01-11 20:00:18,948: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 20:00:18,950: t15.2023.08.13 val PER: 0.0780
2026-01-11 20:00:18,950: t15.2023.08.18 val PER: 0.0629
2026-01-11 20:00:18,950: t15.2023.08.20 val PER: 0.0635
2026-01-11 20:00:18,951: t15.2023.08.25 val PER: 0.0648
2026-01-11 20:00:18,951: t15.2023.08.27 val PER: 0.1608
2026-01-11 20:00:18,951: t15.2023.09.01 val PER: 0.0430
2026-01-11 20:00:18,951: t15.2023.09.03 val PER: 0.1116
2026-01-11 20:00:18,951: t15.2023.09.24 val PER: 0.0813
2026-01-11 20:00:18,951: t15.2023.09.29 val PER: 0.1021
2026-01-11 20:00:18,951: t15.2023.10.01 val PER: 0.1367
2026-01-11 20:00:18,951: t15.2023.10.06 val PER: 0.0581
2026-01-11 20:00:18,951: t15.2023.10.08 val PER: 0.2084
2026-01-11 20:00:18,951: t15.2023.10.13 val PER: 0.1544
2026-01-11 20:00:18,951: t15.2023.10.15 val PER: 0.1147
2026-01-11 20:00:18,951: t15.2023.10.20 val PER: 0.1611
2026-01-11 20:00:18,952: t15.2023.10.22 val PER: 0.0869
2026-01-11 20:00:18,952: t15.2023.11.03 val PER: 0.1479
2026-01-11 20:00:18,952: t15.2023.11.04 val PER: 0.0068
2026-01-11 20:00:18,952: t15.2023.11.17 val PER: 0.0202
2026-01-11 20:00:18,952: t15.2023.11.19 val PER: 0.0100
2026-01-11 20:00:18,952: t15.2023.11.26 val PER: 0.0457
2026-01-11 20:00:18,952: t15.2023.12.03 val PER: 0.0473
2026-01-11 20:00:18,952: t15.2023.12.08 val PER: 0.0326
2026-01-11 20:00:18,952: t15.2023.12.10 val PER: 0.0315
2026-01-11 20:00:18,952: t15.2023.12.17 val PER: 0.0915
2026-01-11 20:00:18,952: t15.2023.12.29 val PER: 0.0631
2026-01-11 20:00:18,952: t15.2024.02.25 val PER: 0.0702
2026-01-11 20:00:18,953: t15.2024.03.08 val PER: 0.1792
2026-01-11 20:00:18,953: t15.2024.03.15 val PER: 0.1576
2026-01-11 20:00:18,953: t15.2024.03.17 val PER: 0.0795
2026-01-11 20:00:18,953: t15.2024.05.10 val PER: 0.1055
2026-01-11 20:00:18,953: t15.2024.06.14 val PER: 0.1183
2026-01-11 20:00:18,953: t15.2024.07.19 val PER: 0.1615
2026-01-11 20:00:18,954: t15.2024.07.21 val PER: 0.0586
2026-01-11 20:00:18,954: t15.2024.07.28 val PER: 0.0941
2026-01-11 20:00:18,954: t15.2025.01.10 val PER: 0.2410
2026-01-11 20:00:18,954: t15.2025.01.12 val PER: 0.0847
2026-01-11 20:00:18,954: t15.2025.03.14 val PER: 0.2811
2026-01-11 20:00:18,954: t15.2025.03.16 val PER: 0.1257
2026-01-11 20:00:18,954: t15.2025.03.30 val PER: 0.2218
2026-01-11 20:00:18,954: t15.2025.04.13 val PER: 0.1812
2026-01-11 20:00:19,121: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_63500
2026-01-11 20:00:29,441: Train batch 63600: loss: 0.13 grad norm: 9.79 time: 0.081
2026-01-11 20:00:50,659: Train batch 63800: loss: 0.63 grad norm: 26.92 time: 0.075
2026-01-11 20:01:11,800: Train batch 64000: loss: 0.55 grad norm: 22.40 time: 0.079
2026-01-11 20:01:11,800: Running test after training batch: 64000
2026-01-11 20:01:11,954: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:01:18,333: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:01:18,412: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:01:35,761: Val batch 64000: PER (avg): 0.1042 CTC Loss (avg): 34.0823 WER(5gram): 10.95% (n=256) time: 23.961
2026-01-11 20:01:35,763: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-11 20:01:35,764: t15.2023.08.13 val PER: 0.0748
2026-01-11 20:01:35,764: t15.2023.08.18 val PER: 0.0645
2026-01-11 20:01:35,764: t15.2023.08.20 val PER: 0.0620
2026-01-11 20:01:35,765: t15.2023.08.25 val PER: 0.0648
2026-01-11 20:01:35,765: t15.2023.08.27 val PER: 0.1543
2026-01-11 20:01:35,765: t15.2023.09.01 val PER: 0.0422
2026-01-11 20:01:35,765: t15.2023.09.03 val PER: 0.1116
2026-01-11 20:01:35,765: t15.2023.09.24 val PER: 0.0801
2026-01-11 20:01:35,765: t15.2023.09.29 val PER: 0.1053
2026-01-11 20:01:35,766: t15.2023.10.01 val PER: 0.1354
2026-01-11 20:01:35,766: t15.2023.10.06 val PER: 0.0603
2026-01-11 20:01:35,766: t15.2023.10.08 val PER: 0.2124
2026-01-11 20:01:35,766: t15.2023.10.13 val PER: 0.1567
2026-01-11 20:01:35,766: t15.2023.10.15 val PER: 0.1081
2026-01-11 20:01:35,766: t15.2023.10.20 val PER: 0.1745
2026-01-11 20:01:35,766: t15.2023.10.22 val PER: 0.0924
2026-01-11 20:01:35,767: t15.2023.11.03 val PER: 0.1438
2026-01-11 20:01:35,767: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:01:35,767: t15.2023.11.17 val PER: 0.0171
2026-01-11 20:01:35,767: t15.2023.11.19 val PER: 0.0100
2026-01-11 20:01:35,767: t15.2023.11.26 val PER: 0.0428
2026-01-11 20:01:35,767: t15.2023.12.03 val PER: 0.0473
2026-01-11 20:01:35,767: t15.2023.12.08 val PER: 0.0313
2026-01-11 20:01:35,767: t15.2023.12.10 val PER: 0.0329
2026-01-11 20:01:35,767: t15.2023.12.17 val PER: 0.0790
2026-01-11 20:01:35,768: t15.2023.12.29 val PER: 0.0741
2026-01-11 20:01:35,768: t15.2024.02.25 val PER: 0.0702
2026-01-11 20:01:35,768: t15.2024.03.08 val PER: 0.1735
2026-01-11 20:01:35,768: t15.2024.03.15 val PER: 0.1645
2026-01-11 20:01:35,768: t15.2024.03.17 val PER: 0.0767
2026-01-11 20:01:35,768: t15.2024.05.10 val PER: 0.1308
2026-01-11 20:01:35,768: t15.2024.06.14 val PER: 0.1262
2026-01-11 20:01:35,768: t15.2024.07.19 val PER: 0.1674
2026-01-11 20:01:35,769: t15.2024.07.21 val PER: 0.0634
2026-01-11 20:01:35,769: t15.2024.07.28 val PER: 0.0882
2026-01-11 20:01:35,769: t15.2025.01.10 val PER: 0.2410
2026-01-11 20:01:35,769: t15.2025.01.12 val PER: 0.0839
2026-01-11 20:01:35,769: t15.2025.03.14 val PER: 0.2796
2026-01-11 20:01:35,769: t15.2025.03.16 val PER: 0.1191
2026-01-11 20:01:35,769: t15.2025.03.30 val PER: 0.2207
2026-01-11 20:01:35,769: t15.2025.04.13 val PER: 0.1812
2026-01-11 20:01:35,924: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_64000
2026-01-11 20:01:56,815: Train batch 64200: loss: 0.22 grad norm: 38.01 time: 0.082
2026-01-11 20:02:17,592: Train batch 64400: loss: 0.28 grad norm: 13.33 time: 0.067
2026-01-11 20:02:28,182: Running test after training batch: 64500
2026-01-11 20:02:28,611: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:02:34,707: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:02:34,793: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 20:02:53,969: Val batch 64500: PER (avg): 0.1042 CTC Loss (avg): 34.1501 WER(5gram): 10.82% (n=256) time: 25.786
2026-01-11 20:02:53,971: WER lens: avg_true_words=5.99 avg_pred_words=6.06 max_pred_words=12
2026-01-11 20:02:53,971: t15.2023.08.13 val PER: 0.0748
2026-01-11 20:02:53,972: t15.2023.08.18 val PER: 0.0662
2026-01-11 20:02:53,972: t15.2023.08.20 val PER: 0.0635
2026-01-11 20:02:53,972: t15.2023.08.25 val PER: 0.0678
2026-01-11 20:02:53,972: t15.2023.08.27 val PER: 0.1511
2026-01-11 20:02:53,972: t15.2023.09.01 val PER: 0.0446
2026-01-11 20:02:53,972: t15.2023.09.03 val PER: 0.1188
2026-01-11 20:02:53,972: t15.2023.09.24 val PER: 0.0777
2026-01-11 20:02:53,972: t15.2023.09.29 val PER: 0.1059
2026-01-11 20:02:53,972: t15.2023.10.01 val PER: 0.1354
2026-01-11 20:02:53,972: t15.2023.10.06 val PER: 0.0592
2026-01-11 20:02:53,972: t15.2023.10.08 val PER: 0.2070
2026-01-11 20:02:53,973: t15.2023.10.13 val PER: 0.1590
2026-01-11 20:02:53,973: t15.2023.10.15 val PER: 0.1101
2026-01-11 20:02:53,973: t15.2023.10.20 val PER: 0.1577
2026-01-11 20:02:53,974: t15.2023.10.22 val PER: 0.0924
2026-01-11 20:02:53,974: t15.2023.11.03 val PER: 0.1520
2026-01-11 20:02:53,974: t15.2023.11.04 val PER: 0.0068
2026-01-11 20:02:53,974: t15.2023.11.17 val PER: 0.0171
2026-01-11 20:02:53,974: t15.2023.11.19 val PER: 0.0100
2026-01-11 20:02:53,974: t15.2023.11.26 val PER: 0.0428
2026-01-11 20:02:53,974: t15.2023.12.03 val PER: 0.0494
2026-01-11 20:02:53,975: t15.2023.12.08 val PER: 0.0313
2026-01-11 20:02:53,975: t15.2023.12.10 val PER: 0.0329
2026-01-11 20:02:53,975: t15.2023.12.17 val PER: 0.0769
2026-01-11 20:02:53,975: t15.2023.12.29 val PER: 0.0679
2026-01-11 20:02:53,975: t15.2024.02.25 val PER: 0.0758
2026-01-11 20:02:53,975: t15.2024.03.08 val PER: 0.1721
2026-01-11 20:02:53,975: t15.2024.03.15 val PER: 0.1588
2026-01-11 20:02:53,976: t15.2024.03.17 val PER: 0.0788
2026-01-11 20:02:53,976: t15.2024.05.10 val PER: 0.1233
2026-01-11 20:02:53,976: t15.2024.06.14 val PER: 0.1167
2026-01-11 20:02:53,976: t15.2024.07.19 val PER: 0.1694
2026-01-11 20:02:53,976: t15.2024.07.21 val PER: 0.0614
2026-01-11 20:02:53,976: t15.2024.07.28 val PER: 0.0941
2026-01-11 20:02:53,976: t15.2025.01.10 val PER: 0.2479
2026-01-11 20:02:53,977: t15.2025.01.12 val PER: 0.0739
2026-01-11 20:02:53,977: t15.2025.03.14 val PER: 0.2707
2026-01-11 20:02:53,977: t15.2025.03.16 val PER: 0.1126
2026-01-11 20:02:53,977: t15.2025.03.30 val PER: 0.2264
2026-01-11 20:02:53,977: t15.2025.04.13 val PER: 0.1912
2026-01-11 20:02:54,177: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_64500
2026-01-11 20:03:04,829: Train batch 64600: loss: 0.29 grad norm: 14.60 time: 0.086
2026-01-11 20:03:25,734: Train batch 64800: loss: 0.76 grad norm: 30.85 time: 0.069
2026-01-11 20:03:46,843: Train batch 65000: loss: 0.22 grad norm: 16.66 time: 0.065
2026-01-11 20:03:46,844: Running test after training batch: 65000
2026-01-11 20:03:47,010: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:03:53,637: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:03:53,715: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 20:04:10,655: Val batch 65000: PER (avg): 0.1039 CTC Loss (avg): 33.7381 WER(5gram): 11.54% (n=256) time: 23.811
2026-01-11 20:04:10,657: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 20:04:10,657: t15.2023.08.13 val PER: 0.0811
2026-01-11 20:04:10,657: t15.2023.08.18 val PER: 0.0620
2026-01-11 20:04:10,657: t15.2023.08.20 val PER: 0.0667
2026-01-11 20:04:10,657: t15.2023.08.25 val PER: 0.0678
2026-01-11 20:04:10,658: t15.2023.08.27 val PER: 0.1672
2026-01-11 20:04:10,658: t15.2023.09.01 val PER: 0.0422
2026-01-11 20:04:10,658: t15.2023.09.03 val PER: 0.1200
2026-01-11 20:04:10,658: t15.2023.09.24 val PER: 0.0777
2026-01-11 20:04:10,658: t15.2023.09.29 val PER: 0.1053
2026-01-11 20:04:10,658: t15.2023.10.01 val PER: 0.1354
2026-01-11 20:04:10,658: t15.2023.10.06 val PER: 0.0560
2026-01-11 20:04:10,658: t15.2023.10.08 val PER: 0.2152
2026-01-11 20:04:10,658: t15.2023.10.13 val PER: 0.1544
2026-01-11 20:04:10,658: t15.2023.10.15 val PER: 0.1081
2026-01-11 20:04:10,659: t15.2023.10.20 val PER: 0.1745
2026-01-11 20:04:10,659: t15.2023.10.22 val PER: 0.0913
2026-01-11 20:04:10,659: t15.2023.11.03 val PER: 0.1493
2026-01-11 20:04:10,659: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:04:10,659: t15.2023.11.17 val PER: 0.0202
2026-01-11 20:04:10,660: t15.2023.11.19 val PER: 0.0100
2026-01-11 20:04:10,660: t15.2023.11.26 val PER: 0.0428
2026-01-11 20:04:10,660: t15.2023.12.03 val PER: 0.0473
2026-01-11 20:04:10,660: t15.2023.12.08 val PER: 0.0273
2026-01-11 20:04:10,660: t15.2023.12.10 val PER: 0.0302
2026-01-11 20:04:10,660: t15.2023.12.17 val PER: 0.0821
2026-01-11 20:04:10,660: t15.2023.12.29 val PER: 0.0659
2026-01-11 20:04:10,660: t15.2024.02.25 val PER: 0.0674
2026-01-11 20:04:10,660: t15.2024.03.08 val PER: 0.1735
2026-01-11 20:04:10,660: t15.2024.03.15 val PER: 0.1614
2026-01-11 20:04:10,660: t15.2024.03.17 val PER: 0.0760
2026-01-11 20:04:10,661: t15.2024.05.10 val PER: 0.1174
2026-01-11 20:04:10,661: t15.2024.06.14 val PER: 0.1230
2026-01-11 20:04:10,661: t15.2024.07.19 val PER: 0.1661
2026-01-11 20:04:10,661: t15.2024.07.21 val PER: 0.0648
2026-01-11 20:04:10,661: t15.2024.07.28 val PER: 0.0860
2026-01-11 20:04:10,661: t15.2025.01.10 val PER: 0.2466
2026-01-11 20:04:10,661: t15.2025.01.12 val PER: 0.0855
2026-01-11 20:04:10,662: t15.2025.03.14 val PER: 0.2751
2026-01-11 20:04:10,662: t15.2025.03.16 val PER: 0.1204
2026-01-11 20:04:10,662: t15.2025.03.30 val PER: 0.2161
2026-01-11 20:04:10,662: t15.2025.04.13 val PER: 0.1783
2026-01-11 20:04:10,816: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_65000
2026-01-11 20:04:31,630: Train batch 65200: loss: 0.39 grad norm: 20.98 time: 0.073
2026-01-11 20:04:52,843: Train batch 65400: loss: 0.37 grad norm: 22.98 time: 0.076
2026-01-11 20:05:03,497: Running test after training batch: 65500
2026-01-11 20:05:03,685: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:05:10,149: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:05:10,233: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:05:27,320: Val batch 65500: PER (avg): 0.1018 CTC Loss (avg): 33.2887 WER(5gram): 10.76% (n=256) time: 23.822
2026-01-11 20:05:27,320: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 20:05:27,321: t15.2023.08.13 val PER: 0.0748
2026-01-11 20:05:27,321: t15.2023.08.18 val PER: 0.0604
2026-01-11 20:05:27,321: t15.2023.08.20 val PER: 0.0564
2026-01-11 20:05:27,321: t15.2023.08.25 val PER: 0.0587
2026-01-11 20:05:27,321: t15.2023.08.27 val PER: 0.1511
2026-01-11 20:05:27,322: t15.2023.09.01 val PER: 0.0422
2026-01-11 20:05:27,322: t15.2023.09.03 val PER: 0.1105
2026-01-11 20:05:27,322: t15.2023.09.24 val PER: 0.0789
2026-01-11 20:05:27,322: t15.2023.09.29 val PER: 0.1034
2026-01-11 20:05:27,322: t15.2023.10.01 val PER: 0.1394
2026-01-11 20:05:27,322: t15.2023.10.06 val PER: 0.0549
2026-01-11 20:05:27,322: t15.2023.10.08 val PER: 0.2165
2026-01-11 20:05:27,322: t15.2023.10.13 val PER: 0.1575
2026-01-11 20:05:27,323: t15.2023.10.15 val PER: 0.1022
2026-01-11 20:05:27,323: t15.2023.10.20 val PER: 0.1544
2026-01-11 20:05:27,323: t15.2023.10.22 val PER: 0.0846
2026-01-11 20:05:27,323: t15.2023.11.03 val PER: 0.1479
2026-01-11 20:05:27,323: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:05:27,323: t15.2023.11.17 val PER: 0.0124
2026-01-11 20:05:27,323: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:05:27,323: t15.2023.11.26 val PER: 0.0435
2026-01-11 20:05:27,323: t15.2023.12.03 val PER: 0.0536
2026-01-11 20:05:27,323: t15.2023.12.08 val PER: 0.0286
2026-01-11 20:05:27,324: t15.2023.12.10 val PER: 0.0355
2026-01-11 20:05:27,324: t15.2023.12.17 val PER: 0.0790
2026-01-11 20:05:27,324: t15.2023.12.29 val PER: 0.0590
2026-01-11 20:05:27,324: t15.2024.02.25 val PER: 0.0702
2026-01-11 20:05:27,324: t15.2024.03.08 val PER: 0.1679
2026-01-11 20:05:27,324: t15.2024.03.15 val PER: 0.1645
2026-01-11 20:05:27,325: t15.2024.03.17 val PER: 0.0753
2026-01-11 20:05:27,325: t15.2024.05.10 val PER: 0.1204
2026-01-11 20:05:27,325: t15.2024.06.14 val PER: 0.1278
2026-01-11 20:05:27,325: t15.2024.07.19 val PER: 0.1661
2026-01-11 20:05:27,325: t15.2024.07.21 val PER: 0.0579
2026-01-11 20:05:27,325: t15.2024.07.28 val PER: 0.0875
2026-01-11 20:05:27,325: t15.2025.01.10 val PER: 0.2410
2026-01-11 20:05:27,325: t15.2025.01.12 val PER: 0.0762
2026-01-11 20:05:27,325: t15.2025.03.14 val PER: 0.2840
2026-01-11 20:05:27,325: t15.2025.03.16 val PER: 0.1178
2026-01-11 20:05:27,326: t15.2025.03.30 val PER: 0.2126
2026-01-11 20:05:27,326: t15.2025.04.13 val PER: 0.1683
2026-01-11 20:05:27,478: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_65500
2026-01-11 20:05:38,188: Train batch 65600: loss: 0.39 grad norm: 24.43 time: 0.121
2026-01-11 20:05:59,321: Train batch 65800: loss: 0.61 grad norm: 36.91 time: 0.087
2026-01-11 20:06:20,717: Train batch 66000: loss: 0.19 grad norm: 12.15 time: 0.084
2026-01-11 20:06:20,717: Running test after training batch: 66000
2026-01-11 20:06:21,042: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:06:27,694: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:06:27,784: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 20:06:45,298: Val batch 66000: PER (avg): 0.1027 CTC Loss (avg): 33.8919 WER(5gram): 10.95% (n=256) time: 24.580
2026-01-11 20:06:45,299: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-11 20:06:45,300: t15.2023.08.13 val PER: 0.0707
2026-01-11 20:06:45,300: t15.2023.08.18 val PER: 0.0654
2026-01-11 20:06:45,300: t15.2023.08.20 val PER: 0.0596
2026-01-11 20:06:45,300: t15.2023.08.25 val PER: 0.0633
2026-01-11 20:06:45,300: t15.2023.08.27 val PER: 0.1511
2026-01-11 20:06:45,300: t15.2023.09.01 val PER: 0.0390
2026-01-11 20:06:45,301: t15.2023.09.03 val PER: 0.1128
2026-01-11 20:06:45,301: t15.2023.09.24 val PER: 0.0850
2026-01-11 20:06:45,301: t15.2023.09.29 val PER: 0.1059
2026-01-11 20:06:45,301: t15.2023.10.01 val PER: 0.1374
2026-01-11 20:06:45,301: t15.2023.10.06 val PER: 0.0527
2026-01-11 20:06:45,301: t15.2023.10.08 val PER: 0.2124
2026-01-11 20:06:45,301: t15.2023.10.13 val PER: 0.1575
2026-01-11 20:06:45,301: t15.2023.10.15 val PER: 0.1035
2026-01-11 20:06:45,301: t15.2023.10.20 val PER: 0.1510
2026-01-11 20:06:45,302: t15.2023.10.22 val PER: 0.0846
2026-01-11 20:06:45,302: t15.2023.11.03 val PER: 0.1526
2026-01-11 20:06:45,302: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:06:45,302: t15.2023.11.17 val PER: 0.0156
2026-01-11 20:06:45,302: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:06:45,302: t15.2023.11.26 val PER: 0.0406
2026-01-11 20:06:45,302: t15.2023.12.03 val PER: 0.0494
2026-01-11 20:06:45,302: t15.2023.12.08 val PER: 0.0326
2026-01-11 20:06:45,302: t15.2023.12.10 val PER: 0.0315
2026-01-11 20:06:45,303: t15.2023.12.17 val PER: 0.0863
2026-01-11 20:06:45,303: t15.2023.12.29 val PER: 0.0700
2026-01-11 20:06:45,303: t15.2024.02.25 val PER: 0.0716
2026-01-11 20:06:45,303: t15.2024.03.08 val PER: 0.1636
2026-01-11 20:06:45,303: t15.2024.03.15 val PER: 0.1614
2026-01-11 20:06:45,303: t15.2024.03.17 val PER: 0.0809
2026-01-11 20:06:45,303: t15.2024.05.10 val PER: 0.1055
2026-01-11 20:06:45,303: t15.2024.06.14 val PER: 0.1199
2026-01-11 20:06:45,304: t15.2024.07.19 val PER: 0.1681
2026-01-11 20:06:45,304: t15.2024.07.21 val PER: 0.0586
2026-01-11 20:06:45,304: t15.2024.07.28 val PER: 0.0926
2026-01-11 20:06:45,304: t15.2025.01.10 val PER: 0.2287
2026-01-11 20:06:45,304: t15.2025.01.12 val PER: 0.0847
2026-01-11 20:06:45,304: t15.2025.03.14 val PER: 0.2766
2026-01-11 20:06:45,304: t15.2025.03.16 val PER: 0.1191
2026-01-11 20:06:45,304: t15.2025.03.30 val PER: 0.2069
2026-01-11 20:06:45,304: t15.2025.04.13 val PER: 0.1840
2026-01-11 20:06:45,463: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_66000
2026-01-11 20:07:07,013: Train batch 66200: loss: 0.36 grad norm: 27.40 time: 0.074
2026-01-11 20:07:27,692: Train batch 66400: loss: 0.21 grad norm: 11.85 time: 0.071
2026-01-11 20:07:38,282: Running test after training batch: 66500
2026-01-11 20:07:38,735: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:07:45,559: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:07:45,685: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 20:08:04,152: Val batch 66500: PER (avg): 0.1029 CTC Loss (avg): 33.6251 WER(5gram): 11.15% (n=256) time: 25.869
2026-01-11 20:08:04,154: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 20:08:04,154: t15.2023.08.13 val PER: 0.0748
2026-01-11 20:08:04,154: t15.2023.08.18 val PER: 0.0629
2026-01-11 20:08:04,155: t15.2023.08.20 val PER: 0.0540
2026-01-11 20:08:04,155: t15.2023.08.25 val PER: 0.0617
2026-01-11 20:08:04,155: t15.2023.08.27 val PER: 0.1511
2026-01-11 20:08:04,155: t15.2023.09.01 val PER: 0.0414
2026-01-11 20:08:04,155: t15.2023.09.03 val PER: 0.1200
2026-01-11 20:08:04,155: t15.2023.09.24 val PER: 0.0825
2026-01-11 20:08:04,155: t15.2023.09.29 val PER: 0.1066
2026-01-11 20:08:04,156: t15.2023.10.01 val PER: 0.1367
2026-01-11 20:08:04,156: t15.2023.10.06 val PER: 0.0549
2026-01-11 20:08:04,156: t15.2023.10.08 val PER: 0.2165
2026-01-11 20:08:04,156: t15.2023.10.13 val PER: 0.1552
2026-01-11 20:08:04,156: t15.2023.10.15 val PER: 0.1028
2026-01-11 20:08:04,156: t15.2023.10.20 val PER: 0.1477
2026-01-11 20:08:04,156: t15.2023.10.22 val PER: 0.0913
2026-01-11 20:08:04,156: t15.2023.11.03 val PER: 0.1493
2026-01-11 20:08:04,156: t15.2023.11.04 val PER: 0.0102
2026-01-11 20:08:04,157: t15.2023.11.17 val PER: 0.0187
2026-01-11 20:08:04,157: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:08:04,157: t15.2023.11.26 val PER: 0.0413
2026-01-11 20:08:04,157: t15.2023.12.03 val PER: 0.0515
2026-01-11 20:08:04,157: t15.2023.12.08 val PER: 0.0333
2026-01-11 20:08:04,157: t15.2023.12.10 val PER: 0.0329
2026-01-11 20:08:04,157: t15.2023.12.17 val PER: 0.0769
2026-01-11 20:08:04,157: t15.2023.12.29 val PER: 0.0659
2026-01-11 20:08:04,158: t15.2024.02.25 val PER: 0.0716
2026-01-11 20:08:04,158: t15.2024.03.08 val PER: 0.1735
2026-01-11 20:08:04,158: t15.2024.03.15 val PER: 0.1632
2026-01-11 20:08:04,158: t15.2024.03.17 val PER: 0.0809
2026-01-11 20:08:04,158: t15.2024.05.10 val PER: 0.1174
2026-01-11 20:08:04,158: t15.2024.06.14 val PER: 0.1199
2026-01-11 20:08:04,158: t15.2024.07.19 val PER: 0.1661
2026-01-11 20:08:04,158: t15.2024.07.21 val PER: 0.0600
2026-01-11 20:08:04,158: t15.2024.07.28 val PER: 0.0897
2026-01-11 20:08:04,158: t15.2025.01.10 val PER: 0.2190
2026-01-11 20:08:04,158: t15.2025.01.12 val PER: 0.0816
2026-01-11 20:08:04,159: t15.2025.03.14 val PER: 0.2914
2026-01-11 20:08:04,159: t15.2025.03.16 val PER: 0.1165
2026-01-11 20:08:04,159: t15.2025.03.30 val PER: 0.2092
2026-01-11 20:08:04,159: t15.2025.04.13 val PER: 0.1854
2026-01-11 20:08:04,319: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_66500
2026-01-11 20:08:14,784: Train batch 66600: loss: 0.20 grad norm: 15.44 time: 0.089
2026-01-11 20:08:35,829: Train batch 66800: loss: 0.40 grad norm: 18.38 time: 0.062
2026-01-11 20:08:56,776: Train batch 67000: loss: 0.17 grad norm: 10.49 time: 0.092
2026-01-11 20:08:56,776: Running test after training batch: 67000
2026-01-11 20:08:56,949: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:09:03,066: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 20:09:03,141: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:09:20,906: Val batch 67000: PER (avg): 0.1019 CTC Loss (avg): 33.7892 WER(5gram): 12.06% (n=256) time: 24.129
2026-01-11 20:09:20,907: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 20:09:20,908: t15.2023.08.13 val PER: 0.0769
2026-01-11 20:09:20,908: t15.2023.08.18 val PER: 0.0645
2026-01-11 20:09:20,908: t15.2023.08.20 val PER: 0.0635
2026-01-11 20:09:20,908: t15.2023.08.25 val PER: 0.0633
2026-01-11 20:09:20,909: t15.2023.08.27 val PER: 0.1527
2026-01-11 20:09:20,909: t15.2023.09.01 val PER: 0.0390
2026-01-11 20:09:20,909: t15.2023.09.03 val PER: 0.1105
2026-01-11 20:09:20,909: t15.2023.09.24 val PER: 0.0789
2026-01-11 20:09:20,909: t15.2023.09.29 val PER: 0.1066
2026-01-11 20:09:20,909: t15.2023.10.01 val PER: 0.1288
2026-01-11 20:09:20,909: t15.2023.10.06 val PER: 0.0581
2026-01-11 20:09:20,910: t15.2023.10.08 val PER: 0.2070
2026-01-11 20:09:20,910: t15.2023.10.13 val PER: 0.1521
2026-01-11 20:09:20,910: t15.2023.10.15 val PER: 0.1081
2026-01-11 20:09:20,910: t15.2023.10.20 val PER: 0.1611
2026-01-11 20:09:20,910: t15.2023.10.22 val PER: 0.0857
2026-01-11 20:09:20,910: t15.2023.11.03 val PER: 0.1513
2026-01-11 20:09:20,910: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:09:20,910: t15.2023.11.17 val PER: 0.0171
2026-01-11 20:09:20,910: t15.2023.11.19 val PER: 0.0180
2026-01-11 20:09:20,911: t15.2023.11.26 val PER: 0.0399
2026-01-11 20:09:20,911: t15.2023.12.03 val PER: 0.0504
2026-01-11 20:09:20,911: t15.2023.12.08 val PER: 0.0346
2026-01-11 20:09:20,911: t15.2023.12.10 val PER: 0.0276
2026-01-11 20:09:20,911: t15.2023.12.17 val PER: 0.0832
2026-01-11 20:09:20,911: t15.2023.12.29 val PER: 0.0625
2026-01-11 20:09:20,911: t15.2024.02.25 val PER: 0.0660
2026-01-11 20:09:20,912: t15.2024.03.08 val PER: 0.1636
2026-01-11 20:09:20,912: t15.2024.03.15 val PER: 0.1614
2026-01-11 20:09:20,912: t15.2024.03.17 val PER: 0.0774
2026-01-11 20:09:20,912: t15.2024.05.10 val PER: 0.1218
2026-01-11 20:09:20,912: t15.2024.06.14 val PER: 0.1057
2026-01-11 20:09:20,912: t15.2024.07.19 val PER: 0.1641
2026-01-11 20:09:20,912: t15.2024.07.21 val PER: 0.0614
2026-01-11 20:09:20,912: t15.2024.07.28 val PER: 0.0919
2026-01-11 20:09:20,913: t15.2025.01.10 val PER: 0.2300
2026-01-11 20:09:20,913: t15.2025.01.12 val PER: 0.0831
2026-01-11 20:09:20,913: t15.2025.03.14 val PER: 0.2663
2026-01-11 20:09:20,913: t15.2025.03.16 val PER: 0.1178
2026-01-11 20:09:20,913: t15.2025.03.30 val PER: 0.2126
2026-01-11 20:09:20,913: t15.2025.04.13 val PER: 0.1769
2026-01-11 20:09:21,085: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_67000
2026-01-11 20:09:41,778: Train batch 67200: loss: 1.05 grad norm: 47.99 time: 0.097
2026-01-11 20:10:02,771: Train batch 67400: loss: 0.16 grad norm: 11.48 time: 0.083
2026-01-11 20:10:13,367: Running test after training batch: 67500
2026-01-11 20:10:13,549: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:10:19,700: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:10:19,807: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 20:10:38,109: Val batch 67500: PER (avg): 0.1025 CTC Loss (avg): 33.5006 WER(5gram): 11.54% (n=256) time: 24.742
2026-01-11 20:10:38,112: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 20:10:38,112: t15.2023.08.13 val PER: 0.0728
2026-01-11 20:10:38,113: t15.2023.08.18 val PER: 0.0612
2026-01-11 20:10:38,113: t15.2023.08.20 val PER: 0.0612
2026-01-11 20:10:38,113: t15.2023.08.25 val PER: 0.0633
2026-01-11 20:10:38,113: t15.2023.08.27 val PER: 0.1543
2026-01-11 20:10:38,113: t15.2023.09.01 val PER: 0.0381
2026-01-11 20:10:38,113: t15.2023.09.03 val PER: 0.1105
2026-01-11 20:10:38,113: t15.2023.09.24 val PER: 0.0777
2026-01-11 20:10:38,114: t15.2023.09.29 val PER: 0.1015
2026-01-11 20:10:38,114: t15.2023.10.01 val PER: 0.1341
2026-01-11 20:10:38,114: t15.2023.10.06 val PER: 0.0538
2026-01-11 20:10:38,114: t15.2023.10.08 val PER: 0.2152
2026-01-11 20:10:38,114: t15.2023.10.13 val PER: 0.1583
2026-01-11 20:10:38,114: t15.2023.10.15 val PER: 0.1042
2026-01-11 20:10:38,114: t15.2023.10.20 val PER: 0.1544
2026-01-11 20:10:38,114: t15.2023.10.22 val PER: 0.0869
2026-01-11 20:10:38,114: t15.2023.11.03 val PER: 0.1560
2026-01-11 20:10:38,114: t15.2023.11.04 val PER: 0.0068
2026-01-11 20:10:38,114: t15.2023.11.17 val PER: 0.0202
2026-01-11 20:10:38,114: t15.2023.11.19 val PER: 0.0080
2026-01-11 20:10:38,115: t15.2023.11.26 val PER: 0.0391
2026-01-11 20:10:38,115: t15.2023.12.03 val PER: 0.0504
2026-01-11 20:10:38,115: t15.2023.12.08 val PER: 0.0313
2026-01-11 20:10:38,115: t15.2023.12.10 val PER: 0.0302
2026-01-11 20:10:38,115: t15.2023.12.17 val PER: 0.0769
2026-01-11 20:10:38,115: t15.2023.12.29 val PER: 0.0659
2026-01-11 20:10:38,115: t15.2024.02.25 val PER: 0.0702
2026-01-11 20:10:38,116: t15.2024.03.08 val PER: 0.1664
2026-01-11 20:10:38,116: t15.2024.03.15 val PER: 0.1595
2026-01-11 20:10:38,116: t15.2024.03.17 val PER: 0.0816
2026-01-11 20:10:38,116: t15.2024.05.10 val PER: 0.1174
2026-01-11 20:10:38,116: t15.2024.06.14 val PER: 0.1325
2026-01-11 20:10:38,116: t15.2024.07.19 val PER: 0.1608
2026-01-11 20:10:38,116: t15.2024.07.21 val PER: 0.0621
2026-01-11 20:10:38,116: t15.2024.07.28 val PER: 0.0860
2026-01-11 20:10:38,117: t15.2025.01.10 val PER: 0.2314
2026-01-11 20:10:38,117: t15.2025.01.12 val PER: 0.0855
2026-01-11 20:10:38,117: t15.2025.03.14 val PER: 0.2766
2026-01-11 20:10:38,117: t15.2025.03.16 val PER: 0.1270
2026-01-11 20:10:38,117: t15.2025.03.30 val PER: 0.2184
2026-01-11 20:10:38,118: t15.2025.04.13 val PER: 0.1783
2026-01-11 20:10:38,275: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_67500
2026-01-11 20:10:48,724: Train batch 67600: loss: 0.26 grad norm: 33.19 time: 0.104
2026-01-11 20:11:09,272: Train batch 67800: loss: 0.30 grad norm: 32.16 time: 0.076
2026-01-11 20:11:30,668: Train batch 68000: loss: 0.35 grad norm: 26.54 time: 0.077
2026-01-11 20:11:30,668: Running test after training batch: 68000
2026-01-11 20:11:30,837: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:11:37,468: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:11:37,544: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:11:54,900: Val batch 68000: PER (avg): 0.1019 CTC Loss (avg): 33.5140 WER(5gram): 11.21% (n=256) time: 24.232
2026-01-11 20:11:54,903: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=13
2026-01-11 20:11:54,903: t15.2023.08.13 val PER: 0.0759
2026-01-11 20:11:54,903: t15.2023.08.18 val PER: 0.0620
2026-01-11 20:11:54,903: t15.2023.08.20 val PER: 0.0580
2026-01-11 20:11:54,903: t15.2023.08.25 val PER: 0.0648
2026-01-11 20:11:54,904: t15.2023.08.27 val PER: 0.1576
2026-01-11 20:11:54,904: t15.2023.09.01 val PER: 0.0381
2026-01-11 20:11:54,904: t15.2023.09.03 val PER: 0.1081
2026-01-11 20:11:54,904: t15.2023.09.24 val PER: 0.0789
2026-01-11 20:11:54,904: t15.2023.09.29 val PER: 0.1040
2026-01-11 20:11:54,904: t15.2023.10.01 val PER: 0.1328
2026-01-11 20:11:54,904: t15.2023.10.06 val PER: 0.0495
2026-01-11 20:11:54,904: t15.2023.10.08 val PER: 0.2016
2026-01-11 20:11:54,904: t15.2023.10.13 val PER: 0.1645
2026-01-11 20:11:54,905: t15.2023.10.15 val PER: 0.0995
2026-01-11 20:11:54,905: t15.2023.10.20 val PER: 0.1510
2026-01-11 20:11:54,905: t15.2023.10.22 val PER: 0.0824
2026-01-11 20:11:54,905: t15.2023.11.03 val PER: 0.1526
2026-01-11 20:11:54,905: t15.2023.11.04 val PER: 0.0068
2026-01-11 20:11:54,905: t15.2023.11.17 val PER: 0.0171
2026-01-11 20:11:54,905: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:11:54,905: t15.2023.11.26 val PER: 0.0377
2026-01-11 20:11:54,905: t15.2023.12.03 val PER: 0.0462
2026-01-11 20:11:54,905: t15.2023.12.08 val PER: 0.0326
2026-01-11 20:11:54,905: t15.2023.12.10 val PER: 0.0276
2026-01-11 20:11:54,906: t15.2023.12.17 val PER: 0.0852
2026-01-11 20:11:54,906: t15.2023.12.29 val PER: 0.0652
2026-01-11 20:11:54,906: t15.2024.02.25 val PER: 0.0674
2026-01-11 20:11:54,906: t15.2024.03.08 val PER: 0.1707
2026-01-11 20:11:54,906: t15.2024.03.15 val PER: 0.1588
2026-01-11 20:11:54,906: t15.2024.03.17 val PER: 0.0753
2026-01-11 20:11:54,906: t15.2024.05.10 val PER: 0.1144
2026-01-11 20:11:54,906: t15.2024.06.14 val PER: 0.1215
2026-01-11 20:11:54,906: t15.2024.07.19 val PER: 0.1569
2026-01-11 20:11:54,906: t15.2024.07.21 val PER: 0.0607
2026-01-11 20:11:54,907: t15.2024.07.28 val PER: 0.0875
2026-01-11 20:11:54,907: t15.2025.01.10 val PER: 0.2397
2026-01-11 20:11:54,907: t15.2025.01.12 val PER: 0.0847
2026-01-11 20:11:54,907: t15.2025.03.14 val PER: 0.2929
2026-01-11 20:11:54,907: t15.2025.03.16 val PER: 0.1296
2026-01-11 20:11:54,907: t15.2025.03.30 val PER: 0.2149
2026-01-11 20:11:54,907: t15.2025.04.13 val PER: 0.1840
2026-01-11 20:11:55,063: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_68000
2026-01-11 20:12:16,220: Train batch 68200: loss: 0.11 grad norm: 6.24 time: 0.066
2026-01-11 20:12:37,474: Train batch 68400: loss: 0.20 grad norm: 12.94 time: 0.065
2026-01-11 20:12:47,954: Running test after training batch: 68500
2026-01-11 20:12:48,095: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:12:55,154: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:12:55,276: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 20:13:17,411: Val batch 68500: PER (avg): 0.1021 CTC Loss (avg): 33.8755 WER(5gram): 10.89% (n=256) time: 29.456
2026-01-11 20:13:17,415: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 20:13:17,417: t15.2023.08.13 val PER: 0.0686
2026-01-11 20:13:17,417: t15.2023.08.18 val PER: 0.0620
2026-01-11 20:13:17,417: t15.2023.08.20 val PER: 0.0635
2026-01-11 20:13:17,417: t15.2023.08.25 val PER: 0.0617
2026-01-11 20:13:17,417: t15.2023.08.27 val PER: 0.1543
2026-01-11 20:13:17,417: t15.2023.09.01 val PER: 0.0398
2026-01-11 20:13:17,417: t15.2023.09.03 val PER: 0.1140
2026-01-11 20:13:17,418: t15.2023.09.24 val PER: 0.0740
2026-01-11 20:13:17,418: t15.2023.09.29 val PER: 0.1034
2026-01-11 20:13:17,418: t15.2023.10.01 val PER: 0.1394
2026-01-11 20:13:17,418: t15.2023.10.06 val PER: 0.0549
2026-01-11 20:13:17,418: t15.2023.10.08 val PER: 0.1962
2026-01-11 20:13:17,418: t15.2023.10.13 val PER: 0.1513
2026-01-11 20:13:17,418: t15.2023.10.15 val PER: 0.1061
2026-01-11 20:13:17,418: t15.2023.10.20 val PER: 0.1611
2026-01-11 20:13:17,418: t15.2023.10.22 val PER: 0.0835
2026-01-11 20:13:17,419: t15.2023.11.03 val PER: 0.1472
2026-01-11 20:13:17,419: t15.2023.11.04 val PER: 0.0102
2026-01-11 20:13:17,419: t15.2023.11.17 val PER: 0.0218
2026-01-11 20:13:17,419: t15.2023.11.19 val PER: 0.0100
2026-01-11 20:13:17,419: t15.2023.11.26 val PER: 0.0413
2026-01-11 20:13:17,419: t15.2023.12.03 val PER: 0.0452
2026-01-11 20:13:17,419: t15.2023.12.08 val PER: 0.0366
2026-01-11 20:13:17,419: t15.2023.12.10 val PER: 0.0250
2026-01-11 20:13:17,419: t15.2023.12.17 val PER: 0.0790
2026-01-11 20:13:17,420: t15.2023.12.29 val PER: 0.0625
2026-01-11 20:13:17,420: t15.2024.02.25 val PER: 0.0702
2026-01-11 20:13:17,420: t15.2024.03.08 val PER: 0.1721
2026-01-11 20:13:17,420: t15.2024.03.15 val PER: 0.1576
2026-01-11 20:13:17,420: t15.2024.03.17 val PER: 0.0844
2026-01-11 20:13:17,420: t15.2024.05.10 val PER: 0.1233
2026-01-11 20:13:17,420: t15.2024.06.14 val PER: 0.1151
2026-01-11 20:13:17,420: t15.2024.07.19 val PER: 0.1556
2026-01-11 20:13:17,420: t15.2024.07.21 val PER: 0.0621
2026-01-11 20:13:17,420: t15.2024.07.28 val PER: 0.0860
2026-01-11 20:13:17,421: t15.2025.01.10 val PER: 0.2342
2026-01-11 20:13:17,421: t15.2025.01.12 val PER: 0.0808
2026-01-11 20:13:17,421: t15.2025.03.14 val PER: 0.2944
2026-01-11 20:13:17,421: t15.2025.03.16 val PER: 0.1270
2026-01-11 20:13:17,421: t15.2025.03.30 val PER: 0.2184
2026-01-11 20:13:17,421: t15.2025.04.13 val PER: 0.1854
2026-01-11 20:13:17,584: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_68500
2026-01-11 20:13:27,839: Train batch 68600: loss: 0.29 grad norm: 20.16 time: 0.057
2026-01-11 20:13:49,222: Train batch 68800: loss: 0.33 grad norm: 19.55 time: 0.099
2026-01-11 20:14:10,552: Train batch 69000: loss: 0.41 grad norm: 26.22 time: 0.071
2026-01-11 20:14:10,552: Running test after training batch: 69000
2026-01-11 20:14:10,681: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:14:17,172: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:14:17,288: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 20:14:34,215: Val batch 69000: PER (avg): 0.1025 CTC Loss (avg): 34.2592 WER(5gram): 10.95% (n=256) time: 23.663
2026-01-11 20:14:34,217: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 20:14:34,219: t15.2023.08.13 val PER: 0.0707
2026-01-11 20:14:34,219: t15.2023.08.18 val PER: 0.0654
2026-01-11 20:14:34,219: t15.2023.08.20 val PER: 0.0596
2026-01-11 20:14:34,219: t15.2023.08.25 val PER: 0.0602
2026-01-11 20:14:34,219: t15.2023.08.27 val PER: 0.1592
2026-01-11 20:14:34,219: t15.2023.09.01 val PER: 0.0381
2026-01-11 20:14:34,219: t15.2023.09.03 val PER: 0.1164
2026-01-11 20:14:34,219: t15.2023.09.24 val PER: 0.0716
2026-01-11 20:14:34,219: t15.2023.09.29 val PER: 0.1008
2026-01-11 20:14:34,220: t15.2023.10.01 val PER: 0.1387
2026-01-11 20:14:34,220: t15.2023.10.06 val PER: 0.0549
2026-01-11 20:14:34,220: t15.2023.10.08 val PER: 0.2084
2026-01-11 20:14:34,220: t15.2023.10.13 val PER: 0.1552
2026-01-11 20:14:34,220: t15.2023.10.15 val PER: 0.1074
2026-01-11 20:14:34,220: t15.2023.10.20 val PER: 0.1510
2026-01-11 20:14:34,220: t15.2023.10.22 val PER: 0.0913
2026-01-11 20:14:34,220: t15.2023.11.03 val PER: 0.1540
2026-01-11 20:14:34,220: t15.2023.11.04 val PER: 0.0102
2026-01-11 20:14:34,220: t15.2023.11.17 val PER: 0.0202
2026-01-11 20:14:34,220: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:14:34,220: t15.2023.11.26 val PER: 0.0377
2026-01-11 20:14:34,221: t15.2023.12.03 val PER: 0.0473
2026-01-11 20:14:34,221: t15.2023.12.08 val PER: 0.0333
2026-01-11 20:14:34,221: t15.2023.12.10 val PER: 0.0237
2026-01-11 20:14:34,221: t15.2023.12.17 val PER: 0.0863
2026-01-11 20:14:34,221: t15.2023.12.29 val PER: 0.0659
2026-01-11 20:14:34,221: t15.2024.02.25 val PER: 0.0758
2026-01-11 20:14:34,221: t15.2024.03.08 val PER: 0.1664
2026-01-11 20:14:34,221: t15.2024.03.15 val PER: 0.1588
2026-01-11 20:14:34,221: t15.2024.03.17 val PER: 0.0795
2026-01-11 20:14:34,221: t15.2024.05.10 val PER: 0.1293
2026-01-11 20:14:34,221: t15.2024.06.14 val PER: 0.1136
2026-01-11 20:14:34,222: t15.2024.07.19 val PER: 0.1575
2026-01-11 20:14:34,222: t15.2024.07.21 val PER: 0.0593
2026-01-11 20:14:34,222: t15.2024.07.28 val PER: 0.0890
2026-01-11 20:14:34,222: t15.2025.01.10 val PER: 0.2424
2026-01-11 20:14:34,222: t15.2025.01.12 val PER: 0.0793
2026-01-11 20:14:34,222: t15.2025.03.14 val PER: 0.2825
2026-01-11 20:14:34,222: t15.2025.03.16 val PER: 0.1178
2026-01-11 20:14:34,222: t15.2025.03.30 val PER: 0.2161
2026-01-11 20:14:34,222: t15.2025.04.13 val PER: 0.1869
2026-01-11 20:14:34,381: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_69000
2026-01-11 20:14:54,906: Train batch 69200: loss: 0.45 grad norm: 25.80 time: 0.082
2026-01-11 20:15:15,316: Train batch 69400: loss: 0.12 grad norm: 14.98 time: 0.062
2026-01-11 20:15:25,849: Running test after training batch: 69500
2026-01-11 20:15:26,025: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:15:32,100: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:15:32,174: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:15:48,651: Val batch 69500: PER (avg): 0.1015 CTC Loss (avg): 33.9205 WER(5gram): 11.21% (n=256) time: 22.802
2026-01-11 20:15:48,653: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 20:15:48,654: t15.2023.08.13 val PER: 0.0676
2026-01-11 20:15:48,654: t15.2023.08.18 val PER: 0.0645
2026-01-11 20:15:48,654: t15.2023.08.20 val PER: 0.0612
2026-01-11 20:15:48,654: t15.2023.08.25 val PER: 0.0648
2026-01-11 20:15:48,654: t15.2023.08.27 val PER: 0.1495
2026-01-11 20:15:48,654: t15.2023.09.01 val PER: 0.0357
2026-01-11 20:15:48,654: t15.2023.09.03 val PER: 0.1057
2026-01-11 20:15:48,654: t15.2023.09.24 val PER: 0.0813
2026-01-11 20:15:48,654: t15.2023.09.29 val PER: 0.0996
2026-01-11 20:15:48,654: t15.2023.10.01 val PER: 0.1400
2026-01-11 20:15:48,655: t15.2023.10.06 val PER: 0.0538
2026-01-11 20:15:48,655: t15.2023.10.08 val PER: 0.1989
2026-01-11 20:15:48,655: t15.2023.10.13 val PER: 0.1559
2026-01-11 20:15:48,655: t15.2023.10.15 val PER: 0.1022
2026-01-11 20:15:48,655: t15.2023.10.20 val PER: 0.1577
2026-01-11 20:15:48,655: t15.2023.10.22 val PER: 0.0869
2026-01-11 20:15:48,655: t15.2023.11.03 val PER: 0.1513
2026-01-11 20:15:48,655: t15.2023.11.04 val PER: 0.0068
2026-01-11 20:15:48,655: t15.2023.11.17 val PER: 0.0233
2026-01-11 20:15:48,656: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:15:48,656: t15.2023.11.26 val PER: 0.0413
2026-01-11 20:15:48,656: t15.2023.12.03 val PER: 0.0494
2026-01-11 20:15:48,656: t15.2023.12.08 val PER: 0.0340
2026-01-11 20:15:48,656: t15.2023.12.10 val PER: 0.0263
2026-01-11 20:15:48,656: t15.2023.12.17 val PER: 0.0832
2026-01-11 20:15:48,656: t15.2023.12.29 val PER: 0.0638
2026-01-11 20:15:48,656: t15.2024.02.25 val PER: 0.0702
2026-01-11 20:15:48,656: t15.2024.03.08 val PER: 0.1735
2026-01-11 20:15:48,656: t15.2024.03.15 val PER: 0.1632
2026-01-11 20:15:48,656: t15.2024.03.17 val PER: 0.0816
2026-01-11 20:15:48,657: t15.2024.05.10 val PER: 0.1129
2026-01-11 20:15:48,657: t15.2024.06.14 val PER: 0.1215
2026-01-11 20:15:48,657: t15.2024.07.19 val PER: 0.1575
2026-01-11 20:15:48,657: t15.2024.07.21 val PER: 0.0559
2026-01-11 20:15:48,657: t15.2024.07.28 val PER: 0.0846
2026-01-11 20:15:48,657: t15.2025.01.10 val PER: 0.2410
2026-01-11 20:15:48,657: t15.2025.01.12 val PER: 0.0808
2026-01-11 20:15:48,657: t15.2025.03.14 val PER: 0.2855
2026-01-11 20:15:48,657: t15.2025.03.16 val PER: 0.1204
2026-01-11 20:15:48,657: t15.2025.03.30 val PER: 0.2115
2026-01-11 20:15:48,657: t15.2025.04.13 val PER: 0.1683
2026-01-11 20:15:48,814: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_69500
2026-01-11 20:15:59,365: Train batch 69600: loss: 0.21 grad norm: 15.57 time: 0.079
2026-01-11 20:16:21,102: Train batch 69800: loss: 0.25 grad norm: 14.59 time: 0.102
2026-01-11 20:16:42,829: Train batch 70000: loss: 0.19 grad norm: 10.10 time: 0.065
2026-01-11 20:16:42,829: Running test after training batch: 70000
2026-01-11 20:16:42,986: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:16:49,138: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:16:49,222: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:17:06,822: Val batch 70000: PER (avg): 0.1025 CTC Loss (avg): 33.6493 WER(5gram): 11.34% (n=256) time: 23.992
2026-01-11 20:17:06,824: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 20:17:06,824: t15.2023.08.13 val PER: 0.0759
2026-01-11 20:17:06,824: t15.2023.08.18 val PER: 0.0629
2026-01-11 20:17:06,825: t15.2023.08.20 val PER: 0.0620
2026-01-11 20:17:06,825: t15.2023.08.25 val PER: 0.0572
2026-01-11 20:17:06,825: t15.2023.08.27 val PER: 0.1463
2026-01-11 20:17:06,825: t15.2023.09.01 val PER: 0.0333
2026-01-11 20:17:06,825: t15.2023.09.03 val PER: 0.1105
2026-01-11 20:17:06,825: t15.2023.09.24 val PER: 0.0789
2026-01-11 20:17:06,826: t15.2023.09.29 val PER: 0.1066
2026-01-11 20:17:06,826: t15.2023.10.01 val PER: 0.1374
2026-01-11 20:17:06,826: t15.2023.10.06 val PER: 0.0581
2026-01-11 20:17:06,826: t15.2023.10.08 val PER: 0.2003
2026-01-11 20:17:06,826: t15.2023.10.13 val PER: 0.1528
2026-01-11 20:17:06,826: t15.2023.10.15 val PER: 0.1035
2026-01-11 20:17:06,826: t15.2023.10.20 val PER: 0.1644
2026-01-11 20:17:06,827: t15.2023.10.22 val PER: 0.0891
2026-01-11 20:17:06,827: t15.2023.11.03 val PER: 0.1574
2026-01-11 20:17:06,827: t15.2023.11.04 val PER: 0.0068
2026-01-11 20:17:06,827: t15.2023.11.17 val PER: 0.0202
2026-01-11 20:17:06,827: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:17:06,827: t15.2023.11.26 val PER: 0.0377
2026-01-11 20:17:06,827: t15.2023.12.03 val PER: 0.0462
2026-01-11 20:17:06,827: t15.2023.12.08 val PER: 0.0326
2026-01-11 20:17:06,827: t15.2023.12.10 val PER: 0.0302
2026-01-11 20:17:06,828: t15.2023.12.17 val PER: 0.0811
2026-01-11 20:17:06,828: t15.2023.12.29 val PER: 0.0597
2026-01-11 20:17:06,828: t15.2024.02.25 val PER: 0.0730
2026-01-11 20:17:06,828: t15.2024.03.08 val PER: 0.1750
2026-01-11 20:17:06,828: t15.2024.03.15 val PER: 0.1645
2026-01-11 20:17:06,828: t15.2024.03.17 val PER: 0.0809
2026-01-11 20:17:06,828: t15.2024.05.10 val PER: 0.1144
2026-01-11 20:17:06,829: t15.2024.06.14 val PER: 0.1104
2026-01-11 20:17:06,829: t15.2024.07.19 val PER: 0.1668
2026-01-11 20:17:06,829: t15.2024.07.21 val PER: 0.0579
2026-01-11 20:17:06,829: t15.2024.07.28 val PER: 0.0882
2026-01-11 20:17:06,829: t15.2025.01.10 val PER: 0.2397
2026-01-11 20:17:06,829: t15.2025.01.12 val PER: 0.0831
2026-01-11 20:17:06,829: t15.2025.03.14 val PER: 0.3033
2026-01-11 20:17:06,829: t15.2025.03.16 val PER: 0.1178
2026-01-11 20:17:06,829: t15.2025.03.30 val PER: 0.2080
2026-01-11 20:17:06,830: t15.2025.04.13 val PER: 0.1826
2026-01-11 20:17:06,987: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_70000
2026-01-11 20:17:27,608: Train batch 70200: loss: 0.09 grad norm: 6.18 time: 0.091
2026-01-11 20:17:48,488: Train batch 70400: loss: 0.57 grad norm: 22.91 time: 0.071
2026-01-11 20:17:58,791: Running test after training batch: 70500
2026-01-11 20:17:58,904: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:18:05,077: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 20:18:05,155: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:18:21,388: Val batch 70500: PER (avg): 0.1023 CTC Loss (avg): 34.3448 WER(5gram): 10.50% (n=256) time: 22.597
2026-01-11 20:18:21,390: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-11 20:18:21,390: t15.2023.08.13 val PER: 0.0728
2026-01-11 20:18:21,390: t15.2023.08.18 val PER: 0.0679
2026-01-11 20:18:21,391: t15.2023.08.20 val PER: 0.0651
2026-01-11 20:18:21,391: t15.2023.08.25 val PER: 0.0648
2026-01-11 20:18:21,391: t15.2023.08.27 val PER: 0.1543
2026-01-11 20:18:21,391: t15.2023.09.01 val PER: 0.0381
2026-01-11 20:18:21,391: t15.2023.09.03 val PER: 0.1081
2026-01-11 20:18:21,391: t15.2023.09.24 val PER: 0.0740
2026-01-11 20:18:21,391: t15.2023.09.29 val PER: 0.1034
2026-01-11 20:18:21,392: t15.2023.10.01 val PER: 0.1380
2026-01-11 20:18:21,392: t15.2023.10.06 val PER: 0.0592
2026-01-11 20:18:21,392: t15.2023.10.08 val PER: 0.2003
2026-01-11 20:18:21,392: t15.2023.10.13 val PER: 0.1552
2026-01-11 20:18:21,392: t15.2023.10.15 val PER: 0.1035
2026-01-11 20:18:21,392: t15.2023.10.20 val PER: 0.1611
2026-01-11 20:18:21,392: t15.2023.10.22 val PER: 0.0857
2026-01-11 20:18:21,392: t15.2023.11.03 val PER: 0.1526
2026-01-11 20:18:21,393: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:18:21,393: t15.2023.11.17 val PER: 0.0218
2026-01-11 20:18:21,393: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:18:21,393: t15.2023.11.26 val PER: 0.0384
2026-01-11 20:18:21,393: t15.2023.12.03 val PER: 0.0473
2026-01-11 20:18:21,393: t15.2023.12.08 val PER: 0.0306
2026-01-11 20:18:21,393: t15.2023.12.10 val PER: 0.0263
2026-01-11 20:18:21,394: t15.2023.12.17 val PER: 0.0821
2026-01-11 20:18:21,394: t15.2023.12.29 val PER: 0.0652
2026-01-11 20:18:21,394: t15.2024.02.25 val PER: 0.0702
2026-01-11 20:18:21,394: t15.2024.03.08 val PER: 0.1650
2026-01-11 20:18:21,394: t15.2024.03.15 val PER: 0.1620
2026-01-11 20:18:21,394: t15.2024.03.17 val PER: 0.0865
2026-01-11 20:18:21,394: t15.2024.05.10 val PER: 0.1114
2026-01-11 20:18:21,394: t15.2024.06.14 val PER: 0.1199
2026-01-11 20:18:21,395: t15.2024.07.19 val PER: 0.1648
2026-01-11 20:18:21,395: t15.2024.07.21 val PER: 0.0593
2026-01-11 20:18:21,395: t15.2024.07.28 val PER: 0.0816
2026-01-11 20:18:21,395: t15.2025.01.10 val PER: 0.2369
2026-01-11 20:18:21,395: t15.2025.01.12 val PER: 0.0808
2026-01-11 20:18:21,395: t15.2025.03.14 val PER: 0.2870
2026-01-11 20:18:21,395: t15.2025.03.16 val PER: 0.1204
2026-01-11 20:18:21,395: t15.2025.03.30 val PER: 0.2115
2026-01-11 20:18:21,395: t15.2025.04.13 val PER: 0.1840
2026-01-11 20:18:21,551: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_70500
2026-01-11 20:18:32,078: Train batch 70600: loss: 0.41 grad norm: 36.96 time: 0.078
2026-01-11 20:18:52,914: Train batch 70800: loss: 0.36 grad norm: 16.01 time: 0.074
2026-01-11 20:19:13,948: Train batch 71000: loss: 0.35 grad norm: 24.34 time: 0.071
2026-01-11 20:19:13,948: Running test after training batch: 71000
2026-01-11 20:19:14,073: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:19:20,250: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 20:19:20,321: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:19:37,627: Val batch 71000: PER (avg): 0.1017 CTC Loss (avg): 33.9881 WER(5gram): 10.76% (n=256) time: 23.678
2026-01-11 20:19:37,628: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-11 20:19:37,629: t15.2023.08.13 val PER: 0.0717
2026-01-11 20:19:37,629: t15.2023.08.18 val PER: 0.0612
2026-01-11 20:19:37,629: t15.2023.08.20 val PER: 0.0627
2026-01-11 20:19:37,629: t15.2023.08.25 val PER: 0.0633
2026-01-11 20:19:37,629: t15.2023.08.27 val PER: 0.1592
2026-01-11 20:19:37,630: t15.2023.09.01 val PER: 0.0365
2026-01-11 20:19:37,630: t15.2023.09.03 val PER: 0.1057
2026-01-11 20:19:37,630: t15.2023.09.24 val PER: 0.0837
2026-01-11 20:19:37,630: t15.2023.09.29 val PER: 0.1053
2026-01-11 20:19:37,630: t15.2023.10.01 val PER: 0.1361
2026-01-11 20:19:37,630: t15.2023.10.06 val PER: 0.0592
2026-01-11 20:19:37,630: t15.2023.10.08 val PER: 0.2043
2026-01-11 20:19:37,630: t15.2023.10.13 val PER: 0.1559
2026-01-11 20:19:37,631: t15.2023.10.15 val PER: 0.1042
2026-01-11 20:19:37,631: t15.2023.10.20 val PER: 0.1611
2026-01-11 20:19:37,631: t15.2023.10.22 val PER: 0.0835
2026-01-11 20:19:37,631: t15.2023.11.03 val PER: 0.1560
2026-01-11 20:19:37,631: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:19:37,631: t15.2023.11.17 val PER: 0.0202
2026-01-11 20:19:37,631: t15.2023.11.19 val PER: 0.0140
2026-01-11 20:19:37,631: t15.2023.11.26 val PER: 0.0406
2026-01-11 20:19:37,632: t15.2023.12.03 val PER: 0.0462
2026-01-11 20:19:37,632: t15.2023.12.08 val PER: 0.0313
2026-01-11 20:19:37,632: t15.2023.12.10 val PER: 0.0263
2026-01-11 20:19:37,632: t15.2023.12.17 val PER: 0.0748
2026-01-11 20:19:37,632: t15.2023.12.29 val PER: 0.0597
2026-01-11 20:19:37,632: t15.2024.02.25 val PER: 0.0688
2026-01-11 20:19:37,632: t15.2024.03.08 val PER: 0.1721
2026-01-11 20:19:37,632: t15.2024.03.15 val PER: 0.1595
2026-01-11 20:19:37,632: t15.2024.03.17 val PER: 0.0823
2026-01-11 20:19:37,632: t15.2024.05.10 val PER: 0.1114
2026-01-11 20:19:37,633: t15.2024.06.14 val PER: 0.1183
2026-01-11 20:19:37,633: t15.2024.07.19 val PER: 0.1615
2026-01-11 20:19:37,633: t15.2024.07.21 val PER: 0.0586
2026-01-11 20:19:37,633: t15.2024.07.28 val PER: 0.0890
2026-01-11 20:19:37,633: t15.2025.01.10 val PER: 0.2273
2026-01-11 20:19:37,633: t15.2025.01.12 val PER: 0.0839
2026-01-11 20:19:37,633: t15.2025.03.14 val PER: 0.2751
2026-01-11 20:19:37,633: t15.2025.03.16 val PER: 0.1217
2026-01-11 20:19:37,633: t15.2025.03.30 val PER: 0.2046
2026-01-11 20:19:37,634: t15.2025.04.13 val PER: 0.1883
2026-01-11 20:19:37,789: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_71000
2026-01-11 20:19:58,828: Train batch 71200: loss: 0.31 grad norm: 16.27 time: 0.086
2026-01-11 20:20:20,039: Train batch 71400: loss: 0.12 grad norm: 7.99 time: 0.090
2026-01-11 20:20:30,244: Running test after training batch: 71500
2026-01-11 20:20:30,398: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:20:37,044: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:20:37,122: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 20:20:53,302: Val batch 71500: PER (avg): 0.1033 CTC Loss (avg): 34.3762 WER(5gram): 11.15% (n=256) time: 23.057
2026-01-11 20:20:53,303: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 20:20:53,304: t15.2023.08.13 val PER: 0.0759
2026-01-11 20:20:53,304: t15.2023.08.18 val PER: 0.0662
2026-01-11 20:20:53,304: t15.2023.08.20 val PER: 0.0604
2026-01-11 20:20:53,304: t15.2023.08.25 val PER: 0.0693
2026-01-11 20:20:53,304: t15.2023.08.27 val PER: 0.1511
2026-01-11 20:20:53,305: t15.2023.09.01 val PER: 0.0422
2026-01-11 20:20:53,305: t15.2023.09.03 val PER: 0.1176
2026-01-11 20:20:53,305: t15.2023.09.24 val PER: 0.0813
2026-01-11 20:20:53,305: t15.2023.09.29 val PER: 0.1021
2026-01-11 20:20:53,305: t15.2023.10.01 val PER: 0.1427
2026-01-11 20:20:53,305: t15.2023.10.06 val PER: 0.0560
2026-01-11 20:20:53,305: t15.2023.10.08 val PER: 0.2192
2026-01-11 20:20:53,305: t15.2023.10.13 val PER: 0.1575
2026-01-11 20:20:53,306: t15.2023.10.15 val PER: 0.1061
2026-01-11 20:20:53,306: t15.2023.10.20 val PER: 0.1678
2026-01-11 20:20:53,306: t15.2023.10.22 val PER: 0.0880
2026-01-11 20:20:53,306: t15.2023.11.03 val PER: 0.1479
2026-01-11 20:20:53,306: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:20:53,306: t15.2023.11.17 val PER: 0.0187
2026-01-11 20:20:53,306: t15.2023.11.19 val PER: 0.0140
2026-01-11 20:20:53,306: t15.2023.11.26 val PER: 0.0435
2026-01-11 20:20:53,306: t15.2023.12.03 val PER: 0.0452
2026-01-11 20:20:53,307: t15.2023.12.08 val PER: 0.0333
2026-01-11 20:20:53,307: t15.2023.12.10 val PER: 0.0263
2026-01-11 20:20:53,307: t15.2023.12.17 val PER: 0.0748
2026-01-11 20:20:53,307: t15.2023.12.29 val PER: 0.0645
2026-01-11 20:20:53,307: t15.2024.02.25 val PER: 0.0730
2026-01-11 20:20:53,307: t15.2024.03.08 val PER: 0.1607
2026-01-11 20:20:53,307: t15.2024.03.15 val PER: 0.1576
2026-01-11 20:20:53,307: t15.2024.03.17 val PER: 0.0802
2026-01-11 20:20:53,307: t15.2024.05.10 val PER: 0.1204
2026-01-11 20:20:53,307: t15.2024.06.14 val PER: 0.1136
2026-01-11 20:20:53,307: t15.2024.07.19 val PER: 0.1714
2026-01-11 20:20:53,307: t15.2024.07.21 val PER: 0.0600
2026-01-11 20:20:53,307: t15.2024.07.28 val PER: 0.0801
2026-01-11 20:20:53,308: t15.2025.01.10 val PER: 0.2342
2026-01-11 20:20:53,308: t15.2025.01.12 val PER: 0.0855
2026-01-11 20:20:53,308: t15.2025.03.14 val PER: 0.2959
2026-01-11 20:20:53,308: t15.2025.03.16 val PER: 0.1204
2026-01-11 20:20:53,308: t15.2025.03.30 val PER: 0.2138
2026-01-11 20:20:53,308: t15.2025.04.13 val PER: 0.1854
2026-01-11 20:20:53,469: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_71500
2026-01-11 20:21:03,825: Train batch 71600: loss: 0.35 grad norm: 24.02 time: 0.074
2026-01-11 20:21:24,686: Train batch 71800: loss: 0.56 grad norm: 25.14 time: 0.080
2026-01-11 20:21:45,172: Train batch 72000: loss: 0.78 grad norm: 37.56 time: 0.066
2026-01-11 20:21:45,172: Running test after training batch: 72000
2026-01-11 20:21:45,291: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:21:51,659: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:21:51,753: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 20:22:08,262: Val batch 72000: PER (avg): 0.1027 CTC Loss (avg): 34.1934 WER(5gram): 11.15% (n=256) time: 23.090
2026-01-11 20:22:08,264: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 20:22:08,265: t15.2023.08.13 val PER: 0.0665
2026-01-11 20:22:08,265: t15.2023.08.18 val PER: 0.0620
2026-01-11 20:22:08,265: t15.2023.08.20 val PER: 0.0627
2026-01-11 20:22:08,265: t15.2023.08.25 val PER: 0.0708
2026-01-11 20:22:08,265: t15.2023.08.27 val PER: 0.1527
2026-01-11 20:22:08,265: t15.2023.09.01 val PER: 0.0390
2026-01-11 20:22:08,265: t15.2023.09.03 val PER: 0.1116
2026-01-11 20:22:08,266: t15.2023.09.24 val PER: 0.0789
2026-01-11 20:22:08,266: t15.2023.09.29 val PER: 0.1066
2026-01-11 20:22:08,266: t15.2023.10.01 val PER: 0.1374
2026-01-11 20:22:08,266: t15.2023.10.06 val PER: 0.0614
2026-01-11 20:22:08,266: t15.2023.10.08 val PER: 0.2097
2026-01-11 20:22:08,266: t15.2023.10.13 val PER: 0.1552
2026-01-11 20:22:08,266: t15.2023.10.15 val PER: 0.1094
2026-01-11 20:22:08,266: t15.2023.10.20 val PER: 0.1678
2026-01-11 20:22:08,266: t15.2023.10.22 val PER: 0.0891
2026-01-11 20:22:08,267: t15.2023.11.03 val PER: 0.1493
2026-01-11 20:22:08,267: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:22:08,267: t15.2023.11.17 val PER: 0.0233
2026-01-11 20:22:08,267: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:22:08,267: t15.2023.11.26 val PER: 0.0399
2026-01-11 20:22:08,267: t15.2023.12.03 val PER: 0.0410
2026-01-11 20:22:08,267: t15.2023.12.08 val PER: 0.0326
2026-01-11 20:22:08,267: t15.2023.12.10 val PER: 0.0276
2026-01-11 20:22:08,267: t15.2023.12.17 val PER: 0.0780
2026-01-11 20:22:08,267: t15.2023.12.29 val PER: 0.0583
2026-01-11 20:22:08,268: t15.2024.02.25 val PER: 0.0660
2026-01-11 20:22:08,268: t15.2024.03.08 val PER: 0.1764
2026-01-11 20:22:08,268: t15.2024.03.15 val PER: 0.1595
2026-01-11 20:22:08,268: t15.2024.03.17 val PER: 0.0774
2026-01-11 20:22:08,268: t15.2024.05.10 val PER: 0.1159
2026-01-11 20:22:08,268: t15.2024.06.14 val PER: 0.1215
2026-01-11 20:22:08,268: t15.2024.07.19 val PER: 0.1674
2026-01-11 20:22:08,268: t15.2024.07.21 val PER: 0.0566
2026-01-11 20:22:08,268: t15.2024.07.28 val PER: 0.0816
2026-01-11 20:22:08,269: t15.2025.01.10 val PER: 0.2369
2026-01-11 20:22:08,269: t15.2025.01.12 val PER: 0.0816
2026-01-11 20:22:08,269: t15.2025.03.14 val PER: 0.2914
2026-01-11 20:22:08,269: t15.2025.03.16 val PER: 0.1270
2026-01-11 20:22:08,269: t15.2025.03.30 val PER: 0.2218
2026-01-11 20:22:08,269: t15.2025.04.13 val PER: 0.1869
2026-01-11 20:22:08,427: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_72000
2026-01-11 20:22:29,085: Train batch 72200: loss: 0.17 grad norm: 7.95 time: 0.075
2026-01-11 20:22:50,242: Train batch 72400: loss: 0.21 grad norm: 11.78 time: 0.062
2026-01-11 20:23:00,765: Running test after training batch: 72500
2026-01-11 20:23:00,977: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:23:07,562: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:23:07,639: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:23:24,732: Val batch 72500: PER (avg): 0.1033 CTC Loss (avg): 34.0824 WER(5gram): 10.43% (n=256) time: 23.967
2026-01-11 20:23:24,735: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-11 20:23:24,735: t15.2023.08.13 val PER: 0.0759
2026-01-11 20:23:24,735: t15.2023.08.18 val PER: 0.0612
2026-01-11 20:23:24,735: t15.2023.08.20 val PER: 0.0659
2026-01-11 20:23:24,735: t15.2023.08.25 val PER: 0.0663
2026-01-11 20:23:24,736: t15.2023.08.27 val PER: 0.1624
2026-01-11 20:23:24,736: t15.2023.09.01 val PER: 0.0398
2026-01-11 20:23:24,736: t15.2023.09.03 val PER: 0.1093
2026-01-11 20:23:24,736: t15.2023.09.24 val PER: 0.0789
2026-01-11 20:23:24,736: t15.2023.09.29 val PER: 0.1002
2026-01-11 20:23:24,736: t15.2023.10.01 val PER: 0.1361
2026-01-11 20:23:24,737: t15.2023.10.06 val PER: 0.0624
2026-01-11 20:23:24,737: t15.2023.10.08 val PER: 0.2124
2026-01-11 20:23:24,737: t15.2023.10.13 val PER: 0.1528
2026-01-11 20:23:24,737: t15.2023.10.15 val PER: 0.1068
2026-01-11 20:23:24,737: t15.2023.10.20 val PER: 0.1577
2026-01-11 20:23:24,737: t15.2023.10.22 val PER: 0.0857
2026-01-11 20:23:24,738: t15.2023.11.03 val PER: 0.1601
2026-01-11 20:23:24,738: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:23:24,738: t15.2023.11.17 val PER: 0.0187
2026-01-11 20:23:24,738: t15.2023.11.19 val PER: 0.0160
2026-01-11 20:23:24,738: t15.2023.11.26 val PER: 0.0413
2026-01-11 20:23:24,738: t15.2023.12.03 val PER: 0.0483
2026-01-11 20:23:24,738: t15.2023.12.08 val PER: 0.0333
2026-01-11 20:23:24,738: t15.2023.12.10 val PER: 0.0302
2026-01-11 20:23:24,738: t15.2023.12.17 val PER: 0.0811
2026-01-11 20:23:24,739: t15.2023.12.29 val PER: 0.0638
2026-01-11 20:23:24,739: t15.2024.02.25 val PER: 0.0688
2026-01-11 20:23:24,739: t15.2024.03.08 val PER: 0.1721
2026-01-11 20:23:24,739: t15.2024.03.15 val PER: 0.1551
2026-01-11 20:23:24,739: t15.2024.03.17 val PER: 0.0830
2026-01-11 20:23:24,739: t15.2024.05.10 val PER: 0.1218
2026-01-11 20:23:24,739: t15.2024.06.14 val PER: 0.1215
2026-01-11 20:23:24,739: t15.2024.07.19 val PER: 0.1688
2026-01-11 20:23:24,739: t15.2024.07.21 val PER: 0.0593
2026-01-11 20:23:24,740: t15.2024.07.28 val PER: 0.0846
2026-01-11 20:23:24,740: t15.2025.01.10 val PER: 0.2369
2026-01-11 20:23:24,740: t15.2025.01.12 val PER: 0.0801
2026-01-11 20:23:24,740: t15.2025.03.14 val PER: 0.2855
2026-01-11 20:23:24,740: t15.2025.03.16 val PER: 0.1126
2026-01-11 20:23:24,740: t15.2025.03.30 val PER: 0.2207
2026-01-11 20:23:24,740: t15.2025.04.13 val PER: 0.1840
2026-01-11 20:23:24,896: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_72500
2026-01-11 20:23:35,414: Train batch 72600: loss: 0.22 grad norm: 15.30 time: 0.073
2026-01-11 20:23:56,496: Train batch 72800: loss: 0.39 grad norm: 24.65 time: 0.094
2026-01-11 20:24:17,309: Train batch 73000: loss: 0.08 grad norm: 6.80 time: 0.097
2026-01-11 20:24:17,310: Running test after training batch: 73000
2026-01-11 20:24:17,424: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:24:24,222: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:24:24,302: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:24:42,885: Val batch 73000: PER (avg): 0.1023 CTC Loss (avg): 34.3520 WER(5gram): 10.56% (n=256) time: 25.575
2026-01-11 20:24:42,888: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 20:24:42,890: t15.2023.08.13 val PER: 0.0707
2026-01-11 20:24:42,890: t15.2023.08.18 val PER: 0.0637
2026-01-11 20:24:42,891: t15.2023.08.20 val PER: 0.0604
2026-01-11 20:24:42,891: t15.2023.08.25 val PER: 0.0617
2026-01-11 20:24:42,891: t15.2023.08.27 val PER: 0.1592
2026-01-11 20:24:42,891: t15.2023.09.01 val PER: 0.0381
2026-01-11 20:24:42,891: t15.2023.09.03 val PER: 0.1045
2026-01-11 20:24:42,891: t15.2023.09.24 val PER: 0.0850
2026-01-11 20:24:42,891: t15.2023.09.29 val PER: 0.1053
2026-01-11 20:24:42,892: t15.2023.10.01 val PER: 0.1413
2026-01-11 20:24:42,892: t15.2023.10.06 val PER: 0.0624
2026-01-11 20:24:42,892: t15.2023.10.08 val PER: 0.2084
2026-01-11 20:24:42,892: t15.2023.10.13 val PER: 0.1528
2026-01-11 20:24:42,892: t15.2023.10.15 val PER: 0.1055
2026-01-11 20:24:42,892: t15.2023.10.20 val PER: 0.1644
2026-01-11 20:24:42,892: t15.2023.10.22 val PER: 0.0869
2026-01-11 20:24:42,893: t15.2023.11.03 val PER: 0.1615
2026-01-11 20:24:42,893: t15.2023.11.04 val PER: 0.0102
2026-01-11 20:24:42,894: t15.2023.11.17 val PER: 0.0156
2026-01-11 20:24:42,894: t15.2023.11.19 val PER: 0.0140
2026-01-11 20:24:42,894: t15.2023.11.26 val PER: 0.0420
2026-01-11 20:24:42,894: t15.2023.12.03 val PER: 0.0504
2026-01-11 20:24:42,894: t15.2023.12.08 val PER: 0.0300
2026-01-11 20:24:42,894: t15.2023.12.10 val PER: 0.0263
2026-01-11 20:24:42,894: t15.2023.12.17 val PER: 0.0800
2026-01-11 20:24:42,894: t15.2023.12.29 val PER: 0.0570
2026-01-11 20:24:42,894: t15.2024.02.25 val PER: 0.0674
2026-01-11 20:24:42,894: t15.2024.03.08 val PER: 0.1593
2026-01-11 20:24:42,894: t15.2024.03.15 val PER: 0.1588
2026-01-11 20:24:42,894: t15.2024.03.17 val PER: 0.0774
2026-01-11 20:24:42,894: t15.2024.05.10 val PER: 0.1278
2026-01-11 20:24:42,895: t15.2024.06.14 val PER: 0.1151
2026-01-11 20:24:42,895: t15.2024.07.19 val PER: 0.1622
2026-01-11 20:24:42,895: t15.2024.07.21 val PER: 0.0586
2026-01-11 20:24:42,895: t15.2024.07.28 val PER: 0.0897
2026-01-11 20:24:42,896: t15.2025.01.10 val PER: 0.2300
2026-01-11 20:24:42,896: t15.2025.01.12 val PER: 0.0839
2026-01-11 20:24:42,896: t15.2025.03.14 val PER: 0.2811
2026-01-11 20:24:42,896: t15.2025.03.16 val PER: 0.1204
2026-01-11 20:24:42,896: t15.2025.03.30 val PER: 0.2126
2026-01-11 20:24:42,896: t15.2025.04.13 val PER: 0.1769
2026-01-11 20:24:43,081: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_73000
2026-01-11 20:25:03,954: Train batch 73200: loss: 0.27 grad norm: 16.83 time: 0.083
2026-01-11 20:25:25,702: Train batch 73400: loss: 0.03 grad norm: 1.44 time: 0.059
2026-01-11 20:25:36,262: Running test after training batch: 73500
2026-01-11 20:25:36,423: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:25:43,250: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:25:43,344: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:26:02,634: Val batch 73500: PER (avg): 0.1026 CTC Loss (avg): 34.4372 WER(5gram): 11.67% (n=256) time: 26.372
2026-01-11 20:26:02,636: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 20:26:02,637: t15.2023.08.13 val PER: 0.0717
2026-01-11 20:26:02,637: t15.2023.08.18 val PER: 0.0604
2026-01-11 20:26:02,637: t15.2023.08.20 val PER: 0.0627
2026-01-11 20:26:02,637: t15.2023.08.25 val PER: 0.0633
2026-01-11 20:26:02,637: t15.2023.08.27 val PER: 0.1576
2026-01-11 20:26:02,638: t15.2023.09.01 val PER: 0.0414
2026-01-11 20:26:02,638: t15.2023.09.03 val PER: 0.1057
2026-01-11 20:26:02,638: t15.2023.09.24 val PER: 0.0777
2026-01-11 20:26:02,638: t15.2023.09.29 val PER: 0.1021
2026-01-11 20:26:02,638: t15.2023.10.01 val PER: 0.1361
2026-01-11 20:26:02,639: t15.2023.10.06 val PER: 0.0603
2026-01-11 20:26:02,639: t15.2023.10.08 val PER: 0.2124
2026-01-11 20:26:02,639: t15.2023.10.13 val PER: 0.1629
2026-01-11 20:26:02,639: t15.2023.10.15 val PER: 0.1074
2026-01-11 20:26:02,639: t15.2023.10.20 val PER: 0.1611
2026-01-11 20:26:02,639: t15.2023.10.22 val PER: 0.0869
2026-01-11 20:26:02,639: t15.2023.11.03 val PER: 0.1540
2026-01-11 20:26:02,639: t15.2023.11.04 val PER: 0.0068
2026-01-11 20:26:02,640: t15.2023.11.17 val PER: 0.0187
2026-01-11 20:26:02,640: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:26:02,640: t15.2023.11.26 val PER: 0.0391
2026-01-11 20:26:02,640: t15.2023.12.03 val PER: 0.0494
2026-01-11 20:26:02,640: t15.2023.12.08 val PER: 0.0340
2026-01-11 20:26:02,640: t15.2023.12.10 val PER: 0.0237
2026-01-11 20:26:02,640: t15.2023.12.17 val PER: 0.0800
2026-01-11 20:26:02,640: t15.2023.12.29 val PER: 0.0625
2026-01-11 20:26:02,641: t15.2024.02.25 val PER: 0.0702
2026-01-11 20:26:02,641: t15.2024.03.08 val PER: 0.1664
2026-01-11 20:26:02,641: t15.2024.03.15 val PER: 0.1595
2026-01-11 20:26:02,641: t15.2024.03.17 val PER: 0.0865
2026-01-11 20:26:02,641: t15.2024.05.10 val PER: 0.1189
2026-01-11 20:26:02,641: t15.2024.06.14 val PER: 0.1151
2026-01-11 20:26:02,641: t15.2024.07.19 val PER: 0.1622
2026-01-11 20:26:02,641: t15.2024.07.21 val PER: 0.0572
2026-01-11 20:26:02,641: t15.2024.07.28 val PER: 0.0831
2026-01-11 20:26:02,642: t15.2025.01.10 val PER: 0.2300
2026-01-11 20:26:02,642: t15.2025.01.12 val PER: 0.0801
2026-01-11 20:26:02,642: t15.2025.03.14 val PER: 0.2885
2026-01-11 20:26:02,642: t15.2025.03.16 val PER: 0.1257
2026-01-11 20:26:02,642: t15.2025.03.30 val PER: 0.2092
2026-01-11 20:26:02,642: t15.2025.04.13 val PER: 0.1897
2026-01-11 20:26:02,798: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_73500
2026-01-11 20:26:13,064: Train batch 73600: loss: 0.16 grad norm: 12.01 time: 0.068
2026-01-11 20:26:33,949: Train batch 73800: loss: 0.70 grad norm: 40.65 time: 0.086
2026-01-11 20:26:54,689: Train batch 74000: loss: 0.03 grad norm: 3.07 time: 0.068
2026-01-11 20:26:54,689: Running test after training batch: 74000
2026-01-11 20:26:54,816: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:27:00,941: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:27:01,042: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 20:27:18,630: Val batch 74000: PER (avg): 0.1010 CTC Loss (avg): 34.1295 WER(5gram): 11.34% (n=256) time: 23.940
2026-01-11 20:27:18,631: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 20:27:18,633: t15.2023.08.13 val PER: 0.0676
2026-01-11 20:27:18,633: t15.2023.08.18 val PER: 0.0629
2026-01-11 20:27:18,633: t15.2023.08.20 val PER: 0.0596
2026-01-11 20:27:18,633: t15.2023.08.25 val PER: 0.0693
2026-01-11 20:27:18,633: t15.2023.08.27 val PER: 0.1527
2026-01-11 20:27:18,633: t15.2023.09.01 val PER: 0.0406
2026-01-11 20:27:18,634: t15.2023.09.03 val PER: 0.1081
2026-01-11 20:27:18,634: t15.2023.09.24 val PER: 0.0801
2026-01-11 20:27:18,634: t15.2023.09.29 val PER: 0.0996
2026-01-11 20:27:18,634: t15.2023.10.01 val PER: 0.1321
2026-01-11 20:27:18,634: t15.2023.10.06 val PER: 0.0592
2026-01-11 20:27:18,634: t15.2023.10.08 val PER: 0.1962
2026-01-11 20:27:18,634: t15.2023.10.13 val PER: 0.1559
2026-01-11 20:27:18,634: t15.2023.10.15 val PER: 0.1055
2026-01-11 20:27:18,635: t15.2023.10.20 val PER: 0.1577
2026-01-11 20:27:18,635: t15.2023.10.22 val PER: 0.0857
2026-01-11 20:27:18,635: t15.2023.11.03 val PER: 0.1533
2026-01-11 20:27:18,635: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:27:18,635: t15.2023.11.17 val PER: 0.0233
2026-01-11 20:27:18,635: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:27:18,635: t15.2023.11.26 val PER: 0.0384
2026-01-11 20:27:18,635: t15.2023.12.03 val PER: 0.0431
2026-01-11 20:27:18,636: t15.2023.12.08 val PER: 0.0353
2026-01-11 20:27:18,636: t15.2023.12.10 val PER: 0.0250
2026-01-11 20:27:18,636: t15.2023.12.17 val PER: 0.0707
2026-01-11 20:27:18,636: t15.2023.12.29 val PER: 0.0583
2026-01-11 20:27:18,636: t15.2024.02.25 val PER: 0.0646
2026-01-11 20:27:18,636: t15.2024.03.08 val PER: 0.1622
2026-01-11 20:27:18,636: t15.2024.03.15 val PER: 0.1576
2026-01-11 20:27:18,636: t15.2024.03.17 val PER: 0.0858
2026-01-11 20:27:18,637: t15.2024.05.10 val PER: 0.1233
2026-01-11 20:27:18,637: t15.2024.06.14 val PER: 0.1151
2026-01-11 20:27:18,637: t15.2024.07.19 val PER: 0.1648
2026-01-11 20:27:18,637: t15.2024.07.21 val PER: 0.0600
2026-01-11 20:27:18,637: t15.2024.07.28 val PER: 0.0853
2026-01-11 20:27:18,637: t15.2025.01.10 val PER: 0.2314
2026-01-11 20:27:18,637: t15.2025.01.12 val PER: 0.0862
2026-01-11 20:27:18,638: t15.2025.03.14 val PER: 0.2914
2026-01-11 20:27:18,638: t15.2025.03.16 val PER: 0.1139
2026-01-11 20:27:18,638: t15.2025.03.30 val PER: 0.2023
2026-01-11 20:27:18,638: t15.2025.04.13 val PER: 0.1812
2026-01-11 20:27:18,801: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_74000
2026-01-11 20:27:39,665: Train batch 74200: loss: 0.14 grad norm: 8.98 time: 0.061
2026-01-11 20:27:59,810: Train batch 74400: loss: 0.48 grad norm: 29.19 time: 0.099
2026-01-11 20:28:10,331: Running test after training batch: 74500
2026-01-11 20:28:10,442: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:28:16,594: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:28:16,671: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 20:28:35,240: Val batch 74500: PER (avg): 0.1025 CTC Loss (avg): 34.5859 WER(5gram): 11.54% (n=256) time: 24.909
2026-01-11 20:28:35,241: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 20:28:35,242: t15.2023.08.13 val PER: 0.0717
2026-01-11 20:28:35,242: t15.2023.08.18 val PER: 0.0637
2026-01-11 20:28:35,242: t15.2023.08.20 val PER: 0.0643
2026-01-11 20:28:35,242: t15.2023.08.25 val PER: 0.0633
2026-01-11 20:28:35,243: t15.2023.08.27 val PER: 0.1576
2026-01-11 20:28:35,243: t15.2023.09.01 val PER: 0.0390
2026-01-11 20:28:35,243: t15.2023.09.03 val PER: 0.1105
2026-01-11 20:28:35,243: t15.2023.09.24 val PER: 0.0789
2026-01-11 20:28:35,244: t15.2023.09.29 val PER: 0.0996
2026-01-11 20:28:35,244: t15.2023.10.01 val PER: 0.1341
2026-01-11 20:28:35,244: t15.2023.10.06 val PER: 0.0571
2026-01-11 20:28:35,244: t15.2023.10.08 val PER: 0.2097
2026-01-11 20:28:35,244: t15.2023.10.13 val PER: 0.1552
2026-01-11 20:28:35,244: t15.2023.10.15 val PER: 0.1068
2026-01-11 20:28:35,244: t15.2023.10.20 val PER: 0.1544
2026-01-11 20:28:35,244: t15.2023.10.22 val PER: 0.0902
2026-01-11 20:28:35,245: t15.2023.11.03 val PER: 0.1533
2026-01-11 20:28:35,245: t15.2023.11.04 val PER: 0.0068
2026-01-11 20:28:35,245: t15.2023.11.17 val PER: 0.0156
2026-01-11 20:28:35,246: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:28:35,246: t15.2023.11.26 val PER: 0.0406
2026-01-11 20:28:35,246: t15.2023.12.03 val PER: 0.0515
2026-01-11 20:28:35,246: t15.2023.12.08 val PER: 0.0360
2026-01-11 20:28:35,247: t15.2023.12.10 val PER: 0.0276
2026-01-11 20:28:35,247: t15.2023.12.17 val PER: 0.0863
2026-01-11 20:28:35,247: t15.2023.12.29 val PER: 0.0618
2026-01-11 20:28:35,247: t15.2024.02.25 val PER: 0.0702
2026-01-11 20:28:35,247: t15.2024.03.08 val PER: 0.1863
2026-01-11 20:28:35,247: t15.2024.03.15 val PER: 0.1639
2026-01-11 20:28:35,247: t15.2024.03.17 val PER: 0.0823
2026-01-11 20:28:35,247: t15.2024.05.10 val PER: 0.1114
2026-01-11 20:28:35,247: t15.2024.06.14 val PER: 0.1167
2026-01-11 20:28:35,248: t15.2024.07.19 val PER: 0.1602
2026-01-11 20:28:35,248: t15.2024.07.21 val PER: 0.0538
2026-01-11 20:28:35,248: t15.2024.07.28 val PER: 0.0816
2026-01-11 20:28:35,248: t15.2025.01.10 val PER: 0.2287
2026-01-11 20:28:35,248: t15.2025.01.12 val PER: 0.0793
2026-01-11 20:28:35,248: t15.2025.03.14 val PER: 0.2959
2026-01-11 20:28:35,248: t15.2025.03.16 val PER: 0.1230
2026-01-11 20:28:35,249: t15.2025.03.30 val PER: 0.2126
2026-01-11 20:28:35,249: t15.2025.04.13 val PER: 0.1797
2026-01-11 20:28:35,418: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_74500
2026-01-11 20:28:45,751: Train batch 74600: loss: 0.14 grad norm: 8.22 time: 0.096
2026-01-11 20:29:06,837: Train batch 74800: loss: 0.07 grad norm: 3.96 time: 0.074
2026-01-11 20:29:27,179: Train batch 75000: loss: 0.04 grad norm: 3.70 time: 0.075
2026-01-11 20:29:27,180: Running test after training batch: 75000
2026-01-11 20:29:27,292: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:29:33,462: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:29:33,547: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:29:52,804: Val batch 75000: PER (avg): 0.1019 CTC Loss (avg): 34.0801 WER(5gram): 10.69% (n=256) time: 25.623
2026-01-11 20:29:52,806: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 20:29:52,807: t15.2023.08.13 val PER: 0.0686
2026-01-11 20:29:52,807: t15.2023.08.18 val PER: 0.0604
2026-01-11 20:29:52,807: t15.2023.08.20 val PER: 0.0635
2026-01-11 20:29:52,807: t15.2023.08.25 val PER: 0.0663
2026-01-11 20:29:52,808: t15.2023.08.27 val PER: 0.1511
2026-01-11 20:29:52,808: t15.2023.09.01 val PER: 0.0357
2026-01-11 20:29:52,808: t15.2023.09.03 val PER: 0.1069
2026-01-11 20:29:52,808: t15.2023.09.24 val PER: 0.0789
2026-01-11 20:29:52,808: t15.2023.09.29 val PER: 0.1027
2026-01-11 20:29:52,808: t15.2023.10.01 val PER: 0.1295
2026-01-11 20:29:52,809: t15.2023.10.06 val PER: 0.0571
2026-01-11 20:29:52,809: t15.2023.10.08 val PER: 0.2070
2026-01-11 20:29:52,809: t15.2023.10.13 val PER: 0.1598
2026-01-11 20:29:52,809: t15.2023.10.15 val PER: 0.1048
2026-01-11 20:29:52,809: t15.2023.10.20 val PER: 0.1544
2026-01-11 20:29:52,809: t15.2023.10.22 val PER: 0.0891
2026-01-11 20:29:52,809: t15.2023.11.03 val PER: 0.1540
2026-01-11 20:29:52,809: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:29:52,809: t15.2023.11.17 val PER: 0.0171
2026-01-11 20:29:52,810: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:29:52,810: t15.2023.11.26 val PER: 0.0413
2026-01-11 20:29:52,810: t15.2023.12.03 val PER: 0.0473
2026-01-11 20:29:52,810: t15.2023.12.08 val PER: 0.0313
2026-01-11 20:29:52,810: t15.2023.12.10 val PER: 0.0263
2026-01-11 20:29:52,810: t15.2023.12.17 val PER: 0.0832
2026-01-11 20:29:52,810: t15.2023.12.29 val PER: 0.0659
2026-01-11 20:29:52,810: t15.2024.02.25 val PER: 0.0730
2026-01-11 20:29:52,811: t15.2024.03.08 val PER: 0.1650
2026-01-11 20:29:52,811: t15.2024.03.15 val PER: 0.1588
2026-01-11 20:29:52,811: t15.2024.03.17 val PER: 0.0844
2026-01-11 20:29:52,811: t15.2024.05.10 val PER: 0.1174
2026-01-11 20:29:52,811: t15.2024.06.14 val PER: 0.1167
2026-01-11 20:29:52,811: t15.2024.07.19 val PER: 0.1661
2026-01-11 20:29:52,812: t15.2024.07.21 val PER: 0.0566
2026-01-11 20:29:52,812: t15.2024.07.28 val PER: 0.0846
2026-01-11 20:29:52,812: t15.2025.01.10 val PER: 0.2369
2026-01-11 20:29:52,812: t15.2025.01.12 val PER: 0.0839
2026-01-11 20:29:52,812: t15.2025.03.14 val PER: 0.2929
2026-01-11 20:29:52,812: t15.2025.03.16 val PER: 0.1086
2026-01-11 20:29:52,812: t15.2025.03.30 val PER: 0.2138
2026-01-11 20:29:52,812: t15.2025.04.13 val PER: 0.1812
2026-01-11 20:29:52,969: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_75000
2026-01-11 20:30:14,103: Train batch 75200: loss: 0.54 grad norm: 23.50 time: 0.067
2026-01-11 20:30:35,642: Train batch 75400: loss: 0.26 grad norm: 13.73 time: 0.060
2026-01-11 20:30:46,141: Running test after training batch: 75500
2026-01-11 20:30:46,256: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:30:52,426: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:30:52,523: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:31:09,805: Val batch 75500: PER (avg): 0.1025 CTC Loss (avg): 34.6721 WER(5gram): 11.47% (n=256) time: 23.663
2026-01-11 20:31:09,806: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 20:31:09,807: t15.2023.08.13 val PER: 0.0717
2026-01-11 20:31:09,807: t15.2023.08.18 val PER: 0.0645
2026-01-11 20:31:09,807: t15.2023.08.20 val PER: 0.0675
2026-01-11 20:31:09,807: t15.2023.08.25 val PER: 0.0617
2026-01-11 20:31:09,808: t15.2023.08.27 val PER: 0.1511
2026-01-11 20:31:09,808: t15.2023.09.01 val PER: 0.0381
2026-01-11 20:31:09,808: t15.2023.09.03 val PER: 0.1045
2026-01-11 20:31:09,808: t15.2023.09.24 val PER: 0.0765
2026-01-11 20:31:09,808: t15.2023.09.29 val PER: 0.1027
2026-01-11 20:31:09,808: t15.2023.10.01 val PER: 0.1295
2026-01-11 20:31:09,808: t15.2023.10.06 val PER: 0.0549
2026-01-11 20:31:09,808: t15.2023.10.08 val PER: 0.2030
2026-01-11 20:31:09,809: t15.2023.10.13 val PER: 0.1598
2026-01-11 20:31:09,809: t15.2023.10.15 val PER: 0.1074
2026-01-11 20:31:09,809: t15.2023.10.20 val PER: 0.1678
2026-01-11 20:31:09,809: t15.2023.10.22 val PER: 0.0869
2026-01-11 20:31:09,809: t15.2023.11.03 val PER: 0.1588
2026-01-11 20:31:09,809: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:31:09,809: t15.2023.11.17 val PER: 0.0171
2026-01-11 20:31:09,809: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:31:09,809: t15.2023.11.26 val PER: 0.0384
2026-01-11 20:31:09,809: t15.2023.12.03 val PER: 0.0473
2026-01-11 20:31:09,810: t15.2023.12.08 val PER: 0.0320
2026-01-11 20:31:09,810: t15.2023.12.10 val PER: 0.0250
2026-01-11 20:31:09,811: t15.2023.12.17 val PER: 0.0873
2026-01-11 20:31:09,811: t15.2023.12.29 val PER: 0.0645
2026-01-11 20:31:09,811: t15.2024.02.25 val PER: 0.0815
2026-01-11 20:31:09,811: t15.2024.03.08 val PER: 0.1579
2026-01-11 20:31:09,811: t15.2024.03.15 val PER: 0.1576
2026-01-11 20:31:09,812: t15.2024.03.17 val PER: 0.0844
2026-01-11 20:31:09,812: t15.2024.05.10 val PER: 0.1218
2026-01-11 20:31:09,812: t15.2024.06.14 val PER: 0.1151
2026-01-11 20:31:09,812: t15.2024.07.19 val PER: 0.1694
2026-01-11 20:31:09,812: t15.2024.07.21 val PER: 0.0517
2026-01-11 20:31:09,812: t15.2024.07.28 val PER: 0.0926
2026-01-11 20:31:09,812: t15.2025.01.10 val PER: 0.2383
2026-01-11 20:31:09,813: t15.2025.01.12 val PER: 0.0816
2026-01-11 20:31:09,813: t15.2025.03.14 val PER: 0.2840
2026-01-11 20:31:09,813: t15.2025.03.16 val PER: 0.1204
2026-01-11 20:31:09,813: t15.2025.03.30 val PER: 0.2092
2026-01-11 20:31:09,813: t15.2025.04.13 val PER: 0.1826
2026-01-11 20:31:09,969: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_75500
2026-01-11 20:31:20,121: Train batch 75600: loss: 0.17 grad norm: 16.30 time: 0.082
2026-01-11 20:31:41,168: Train batch 75800: loss: 0.08 grad norm: 7.18 time: 0.071
2026-01-11 20:32:02,477: Train batch 76000: loss: 0.21 grad norm: 33.64 time: 0.102
2026-01-11 20:32:02,477: Running test after training batch: 76000
2026-01-11 20:32:02,618: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:32:09,570: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 20:32:09,652: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:32:26,508: Val batch 76000: PER (avg): 0.1012 CTC Loss (avg): 34.3779 WER(5gram): 11.28% (n=256) time: 24.031
2026-01-11 20:32:26,511: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 20:32:26,512: t15.2023.08.13 val PER: 0.0738
2026-01-11 20:32:26,512: t15.2023.08.18 val PER: 0.0687
2026-01-11 20:32:26,512: t15.2023.08.20 val PER: 0.0596
2026-01-11 20:32:26,512: t15.2023.08.25 val PER: 0.0693
2026-01-11 20:32:26,512: t15.2023.08.27 val PER: 0.1511
2026-01-11 20:32:26,512: t15.2023.09.01 val PER: 0.0381
2026-01-11 20:32:26,513: t15.2023.09.03 val PER: 0.1021
2026-01-11 20:32:26,513: t15.2023.09.24 val PER: 0.0777
2026-01-11 20:32:26,513: t15.2023.09.29 val PER: 0.1015
2026-01-11 20:32:26,513: t15.2023.10.01 val PER: 0.1341
2026-01-11 20:32:26,513: t15.2023.10.06 val PER: 0.0635
2026-01-11 20:32:26,513: t15.2023.10.08 val PER: 0.1935
2026-01-11 20:32:26,513: t15.2023.10.13 val PER: 0.1513
2026-01-11 20:32:26,513: t15.2023.10.15 val PER: 0.1022
2026-01-11 20:32:26,513: t15.2023.10.20 val PER: 0.1544
2026-01-11 20:32:26,514: t15.2023.10.22 val PER: 0.0880
2026-01-11 20:32:26,514: t15.2023.11.03 val PER: 0.1540
2026-01-11 20:32:26,514: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:32:26,514: t15.2023.11.17 val PER: 0.0156
2026-01-11 20:32:26,514: t15.2023.11.19 val PER: 0.0080
2026-01-11 20:32:26,514: t15.2023.11.26 val PER: 0.0391
2026-01-11 20:32:26,514: t15.2023.12.03 val PER: 0.0515
2026-01-11 20:32:26,514: t15.2023.12.08 val PER: 0.0326
2026-01-11 20:32:26,515: t15.2023.12.10 val PER: 0.0237
2026-01-11 20:32:26,515: t15.2023.12.17 val PER: 0.0759
2026-01-11 20:32:26,515: t15.2023.12.29 val PER: 0.0659
2026-01-11 20:32:26,515: t15.2024.02.25 val PER: 0.0702
2026-01-11 20:32:26,515: t15.2024.03.08 val PER: 0.1522
2026-01-11 20:32:26,515: t15.2024.03.15 val PER: 0.1607
2026-01-11 20:32:26,515: t15.2024.03.17 val PER: 0.0886
2026-01-11 20:32:26,516: t15.2024.05.10 val PER: 0.1174
2026-01-11 20:32:26,516: t15.2024.06.14 val PER: 0.1215
2026-01-11 20:32:26,516: t15.2024.07.19 val PER: 0.1608
2026-01-11 20:32:26,516: t15.2024.07.21 val PER: 0.0517
2026-01-11 20:32:26,516: t15.2024.07.28 val PER: 0.0838
2026-01-11 20:32:26,516: t15.2025.01.10 val PER: 0.2410
2026-01-11 20:32:26,516: t15.2025.01.12 val PER: 0.0824
2026-01-11 20:32:26,516: t15.2025.03.14 val PER: 0.2973
2026-01-11 20:32:26,516: t15.2025.03.16 val PER: 0.1217
2026-01-11 20:32:26,516: t15.2025.03.30 val PER: 0.1989
2026-01-11 20:32:26,517: t15.2025.04.13 val PER: 0.1797
2026-01-11 20:32:26,676: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_76000
2026-01-11 20:32:47,399: Train batch 76200: loss: 0.50 grad norm: 22.60 time: 0.067
2026-01-11 20:33:08,667: Train batch 76400: loss: 0.20 grad norm: 12.28 time: 0.064
2026-01-11 20:33:19,306: Running test after training batch: 76500
2026-01-11 20:33:19,436: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:33:25,932: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 20:33:26,031: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 20:33:43,146: Val batch 76500: PER (avg): 0.1008 CTC Loss (avg): 34.2014 WER(5gram): 11.15% (n=256) time: 23.839
2026-01-11 20:33:43,147: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 20:33:43,147: t15.2023.08.13 val PER: 0.0707
2026-01-11 20:33:43,147: t15.2023.08.18 val PER: 0.0662
2026-01-11 20:33:43,148: t15.2023.08.20 val PER: 0.0620
2026-01-11 20:33:43,148: t15.2023.08.25 val PER: 0.0693
2026-01-11 20:33:43,148: t15.2023.08.27 val PER: 0.1527
2026-01-11 20:33:43,148: t15.2023.09.01 val PER: 0.0390
2026-01-11 20:33:43,148: t15.2023.09.03 val PER: 0.0998
2026-01-11 20:33:43,148: t15.2023.09.24 val PER: 0.0728
2026-01-11 20:33:43,148: t15.2023.09.29 val PER: 0.1027
2026-01-11 20:33:43,148: t15.2023.10.01 val PER: 0.1321
2026-01-11 20:33:43,149: t15.2023.10.06 val PER: 0.0549
2026-01-11 20:33:43,149: t15.2023.10.08 val PER: 0.1949
2026-01-11 20:33:43,149: t15.2023.10.13 val PER: 0.1590
2026-01-11 20:33:43,149: t15.2023.10.15 val PER: 0.1009
2026-01-11 20:33:43,149: t15.2023.10.20 val PER: 0.1544
2026-01-11 20:33:43,149: t15.2023.10.22 val PER: 0.0824
2026-01-11 20:33:43,149: t15.2023.11.03 val PER: 0.1513
2026-01-11 20:33:43,149: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:33:43,149: t15.2023.11.17 val PER: 0.0171
2026-01-11 20:33:43,150: t15.2023.11.19 val PER: 0.0100
2026-01-11 20:33:43,150: t15.2023.11.26 val PER: 0.0413
2026-01-11 20:33:43,150: t15.2023.12.03 val PER: 0.0483
2026-01-11 20:33:43,150: t15.2023.12.08 val PER: 0.0293
2026-01-11 20:33:43,150: t15.2023.12.10 val PER: 0.0197
2026-01-11 20:33:43,150: t15.2023.12.17 val PER: 0.0748
2026-01-11 20:33:43,150: t15.2023.12.29 val PER: 0.0618
2026-01-11 20:33:43,150: t15.2024.02.25 val PER: 0.0716
2026-01-11 20:33:43,150: t15.2024.03.08 val PER: 0.1579
2026-01-11 20:33:43,150: t15.2024.03.15 val PER: 0.1601
2026-01-11 20:33:43,151: t15.2024.03.17 val PER: 0.0851
2026-01-11 20:33:43,151: t15.2024.05.10 val PER: 0.1174
2026-01-11 20:33:43,151: t15.2024.06.14 val PER: 0.1136
2026-01-11 20:33:43,151: t15.2024.07.19 val PER: 0.1648
2026-01-11 20:33:43,151: t15.2024.07.21 val PER: 0.0531
2026-01-11 20:33:43,151: t15.2024.07.28 val PER: 0.0824
2026-01-11 20:33:43,151: t15.2025.01.10 val PER: 0.2424
2026-01-11 20:33:43,151: t15.2025.01.12 val PER: 0.0901
2026-01-11 20:33:43,151: t15.2025.03.14 val PER: 0.2973
2026-01-11 20:33:43,152: t15.2025.03.16 val PER: 0.1152
2026-01-11 20:33:43,152: t15.2025.03.30 val PER: 0.2057
2026-01-11 20:33:43,152: t15.2025.04.13 val PER: 0.1783
2026-01-11 20:33:43,309: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_76500
2026-01-11 20:33:53,809: Train batch 76600: loss: 0.13 grad norm: 9.80 time: 0.067
2026-01-11 20:34:14,791: Train batch 76800: loss: 0.19 grad norm: 17.25 time: 0.064
2026-01-11 20:34:36,375: Train batch 77000: loss: 0.44 grad norm: 19.02 time: 0.072
2026-01-11 20:34:36,375: Running test after training batch: 77000
2026-01-11 20:34:36,672: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:34:42,739: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:34:42,822: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:34:59,583: Val batch 77000: PER (avg): 0.1021 CTC Loss (avg): 34.5019 WER(5gram): 11.08% (n=256) time: 23.208
2026-01-11 20:34:59,585: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 20:34:59,585: t15.2023.08.13 val PER: 0.0707
2026-01-11 20:34:59,585: t15.2023.08.18 val PER: 0.0654
2026-01-11 20:34:59,586: t15.2023.08.20 val PER: 0.0604
2026-01-11 20:34:59,586: t15.2023.08.25 val PER: 0.0633
2026-01-11 20:34:59,586: t15.2023.08.27 val PER: 0.1559
2026-01-11 20:34:59,586: t15.2023.09.01 val PER: 0.0373
2026-01-11 20:34:59,586: t15.2023.09.03 val PER: 0.1093
2026-01-11 20:34:59,586: t15.2023.09.24 val PER: 0.0728
2026-01-11 20:34:59,587: t15.2023.09.29 val PER: 0.1040
2026-01-11 20:34:59,587: t15.2023.10.01 val PER: 0.1387
2026-01-11 20:34:59,587: t15.2023.10.06 val PER: 0.0560
2026-01-11 20:34:59,587: t15.2023.10.08 val PER: 0.1989
2026-01-11 20:34:59,587: t15.2023.10.13 val PER: 0.1598
2026-01-11 20:34:59,587: t15.2023.10.15 val PER: 0.1009
2026-01-11 20:34:59,587: t15.2023.10.20 val PER: 0.1678
2026-01-11 20:34:59,587: t15.2023.10.22 val PER: 0.0902
2026-01-11 20:34:59,588: t15.2023.11.03 val PER: 0.1588
2026-01-11 20:34:59,588: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:34:59,588: t15.2023.11.17 val PER: 0.0171
2026-01-11 20:34:59,588: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:34:59,588: t15.2023.11.26 val PER: 0.0406
2026-01-11 20:34:59,588: t15.2023.12.03 val PER: 0.0515
2026-01-11 20:34:59,588: t15.2023.12.08 val PER: 0.0306
2026-01-11 20:34:59,588: t15.2023.12.10 val PER: 0.0250
2026-01-11 20:34:59,588: t15.2023.12.17 val PER: 0.0821
2026-01-11 20:34:59,588: t15.2023.12.29 val PER: 0.0645
2026-01-11 20:34:59,589: t15.2024.02.25 val PER: 0.0716
2026-01-11 20:34:59,589: t15.2024.03.08 val PER: 0.1565
2026-01-11 20:34:59,589: t15.2024.03.15 val PER: 0.1582
2026-01-11 20:34:59,589: t15.2024.03.17 val PER: 0.0816
2026-01-11 20:34:59,589: t15.2024.05.10 val PER: 0.1189
2026-01-11 20:34:59,589: t15.2024.06.14 val PER: 0.1136
2026-01-11 20:34:59,589: t15.2024.07.19 val PER: 0.1655
2026-01-11 20:34:59,589: t15.2024.07.21 val PER: 0.0600
2026-01-11 20:34:59,589: t15.2024.07.28 val PER: 0.0838
2026-01-11 20:34:59,589: t15.2025.01.10 val PER: 0.2383
2026-01-11 20:34:59,590: t15.2025.01.12 val PER: 0.0855
2026-01-11 20:34:59,590: t15.2025.03.14 val PER: 0.2899
2026-01-11 20:34:59,590: t15.2025.03.16 val PER: 0.1178
2026-01-11 20:34:59,590: t15.2025.03.30 val PER: 0.2080
2026-01-11 20:34:59,590: t15.2025.04.13 val PER: 0.1769
2026-01-11 20:34:59,745: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_77000
2026-01-11 20:35:20,582: Train batch 77200: loss: 0.12 grad norm: 13.13 time: 0.069
2026-01-11 20:35:41,452: Train batch 77400: loss: 0.21 grad norm: 17.40 time: 0.070
2026-01-11 20:35:51,913: Running test after training batch: 77500
2026-01-11 20:35:52,036: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:35:58,204: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:35:58,284: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:36:15,348: Val batch 77500: PER (avg): 0.1026 CTC Loss (avg): 34.5648 WER(5gram): 11.34% (n=256) time: 23.435
2026-01-11 20:36:15,350: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 20:36:15,350: t15.2023.08.13 val PER: 0.0811
2026-01-11 20:36:15,350: t15.2023.08.18 val PER: 0.0629
2026-01-11 20:36:15,350: t15.2023.08.20 val PER: 0.0620
2026-01-11 20:36:15,351: t15.2023.08.25 val PER: 0.0602
2026-01-11 20:36:15,351: t15.2023.08.27 val PER: 0.1543
2026-01-11 20:36:15,351: t15.2023.09.01 val PER: 0.0406
2026-01-11 20:36:15,351: t15.2023.09.03 val PER: 0.1045
2026-01-11 20:36:15,351: t15.2023.09.24 val PER: 0.0765
2026-01-11 20:36:15,351: t15.2023.09.29 val PER: 0.1021
2026-01-11 20:36:15,351: t15.2023.10.01 val PER: 0.1394
2026-01-11 20:36:15,352: t15.2023.10.06 val PER: 0.0624
2026-01-11 20:36:15,352: t15.2023.10.08 val PER: 0.2070
2026-01-11 20:36:15,352: t15.2023.10.13 val PER: 0.1598
2026-01-11 20:36:15,352: t15.2023.10.15 val PER: 0.1035
2026-01-11 20:36:15,352: t15.2023.10.20 val PER: 0.1611
2026-01-11 20:36:15,352: t15.2023.10.22 val PER: 0.0913
2026-01-11 20:36:15,352: t15.2023.11.03 val PER: 0.1540
2026-01-11 20:36:15,352: t15.2023.11.04 val PER: 0.0068
2026-01-11 20:36:15,352: t15.2023.11.17 val PER: 0.0171
2026-01-11 20:36:15,353: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:36:15,353: t15.2023.11.26 val PER: 0.0413
2026-01-11 20:36:15,353: t15.2023.12.03 val PER: 0.0483
2026-01-11 20:36:15,353: t15.2023.12.08 val PER: 0.0353
2026-01-11 20:36:15,353: t15.2023.12.10 val PER: 0.0263
2026-01-11 20:36:15,353: t15.2023.12.17 val PER: 0.0811
2026-01-11 20:36:15,353: t15.2023.12.29 val PER: 0.0652
2026-01-11 20:36:15,353: t15.2024.02.25 val PER: 0.0674
2026-01-11 20:36:15,354: t15.2024.03.08 val PER: 0.1593
2026-01-11 20:36:15,354: t15.2024.03.15 val PER: 0.1551
2026-01-11 20:36:15,354: t15.2024.03.17 val PER: 0.0837
2026-01-11 20:36:15,354: t15.2024.05.10 val PER: 0.1144
2026-01-11 20:36:15,354: t15.2024.06.14 val PER: 0.1183
2026-01-11 20:36:15,354: t15.2024.07.19 val PER: 0.1668
2026-01-11 20:36:15,354: t15.2024.07.21 val PER: 0.0566
2026-01-11 20:36:15,354: t15.2024.07.28 val PER: 0.0875
2026-01-11 20:36:15,354: t15.2025.01.10 val PER: 0.2300
2026-01-11 20:36:15,355: t15.2025.01.12 val PER: 0.0839
2026-01-11 20:36:15,355: t15.2025.03.14 val PER: 0.2825
2026-01-11 20:36:15,355: t15.2025.03.16 val PER: 0.1204
2026-01-11 20:36:15,355: t15.2025.03.30 val PER: 0.2184
2026-01-11 20:36:15,355: t15.2025.04.13 val PER: 0.1783
2026-01-11 20:36:15,511: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_77500
2026-01-11 20:36:25,771: Train batch 77600: loss: 0.20 grad norm: 17.26 time: 0.080
2026-01-11 20:36:46,233: Train batch 77800: loss: 0.33 grad norm: 16.21 time: 0.064
2026-01-11 20:37:07,361: Train batch 78000: loss: 0.10 grad norm: 9.90 time: 0.083
2026-01-11 20:37:07,361: Running test after training batch: 78000
2026-01-11 20:37:07,524: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:37:13,652: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:37:13,734: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:37:30,806: Val batch 78000: PER (avg): 0.1012 CTC Loss (avg): 34.1028 WER(5gram): 12.39% (n=256) time: 23.445
2026-01-11 20:37:30,808: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-11 20:37:30,809: t15.2023.08.13 val PER: 0.0728
2026-01-11 20:37:30,809: t15.2023.08.18 val PER: 0.0671
2026-01-11 20:37:30,809: t15.2023.08.20 val PER: 0.0596
2026-01-11 20:37:30,809: t15.2023.08.25 val PER: 0.0663
2026-01-11 20:37:30,809: t15.2023.08.27 val PER: 0.1543
2026-01-11 20:37:30,809: t15.2023.09.01 val PER: 0.0390
2026-01-11 20:37:30,809: t15.2023.09.03 val PER: 0.1069
2026-01-11 20:37:30,810: t15.2023.09.24 val PER: 0.0801
2026-01-11 20:37:30,810: t15.2023.09.29 val PER: 0.1015
2026-01-11 20:37:30,810: t15.2023.10.01 val PER: 0.1308
2026-01-11 20:37:30,810: t15.2023.10.06 val PER: 0.0603
2026-01-11 20:37:30,811: t15.2023.10.08 val PER: 0.1962
2026-01-11 20:37:30,811: t15.2023.10.13 val PER: 0.1614
2026-01-11 20:37:30,811: t15.2023.10.15 val PER: 0.1042
2026-01-11 20:37:30,811: t15.2023.10.20 val PER: 0.1544
2026-01-11 20:37:30,811: t15.2023.10.22 val PER: 0.0869
2026-01-11 20:37:30,811: t15.2023.11.03 val PER: 0.1499
2026-01-11 20:37:30,811: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:37:30,811: t15.2023.11.17 val PER: 0.0156
2026-01-11 20:37:30,811: t15.2023.11.19 val PER: 0.0100
2026-01-11 20:37:30,811: t15.2023.11.26 val PER: 0.0399
2026-01-11 20:37:30,812: t15.2023.12.03 val PER: 0.0494
2026-01-11 20:37:30,812: t15.2023.12.08 val PER: 0.0373
2026-01-11 20:37:30,812: t15.2023.12.10 val PER: 0.0263
2026-01-11 20:37:30,812: t15.2023.12.17 val PER: 0.0842
2026-01-11 20:37:30,812: t15.2023.12.29 val PER: 0.0659
2026-01-11 20:37:30,812: t15.2024.02.25 val PER: 0.0632
2026-01-11 20:37:30,812: t15.2024.03.08 val PER: 0.1579
2026-01-11 20:37:30,812: t15.2024.03.15 val PER: 0.1557
2026-01-11 20:37:30,812: t15.2024.03.17 val PER: 0.0788
2026-01-11 20:37:30,812: t15.2024.05.10 val PER: 0.1189
2026-01-11 20:37:30,813: t15.2024.06.14 val PER: 0.1183
2026-01-11 20:37:30,813: t15.2024.07.19 val PER: 0.1668
2026-01-11 20:37:30,813: t15.2024.07.21 val PER: 0.0552
2026-01-11 20:37:30,813: t15.2024.07.28 val PER: 0.0882
2026-01-11 20:37:30,813: t15.2025.01.10 val PER: 0.2273
2026-01-11 20:37:30,813: t15.2025.01.12 val PER: 0.0808
2026-01-11 20:37:30,813: t15.2025.03.14 val PER: 0.2811
2026-01-11 20:37:30,813: t15.2025.03.16 val PER: 0.1086
2026-01-11 20:37:30,814: t15.2025.03.30 val PER: 0.2092
2026-01-11 20:37:30,814: t15.2025.04.13 val PER: 0.1826
2026-01-11 20:37:30,974: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_78000
2026-01-11 20:37:52,260: Train batch 78200: loss: 0.20 grad norm: 12.51 time: 0.067
2026-01-11 20:38:14,102: Train batch 78400: loss: 0.06 grad norm: 3.50 time: 0.072
2026-01-11 20:38:24,773: Running test after training batch: 78500
2026-01-11 20:38:24,907: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:38:31,671: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 20:38:31,768: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:38:51,014: Val batch 78500: PER (avg): 0.1011 CTC Loss (avg): 34.2562 WER(5gram): 11.34% (n=256) time: 26.240
2026-01-11 20:38:51,018: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 20:38:51,020: t15.2023.08.13 val PER: 0.0717
2026-01-11 20:38:51,020: t15.2023.08.18 val PER: 0.0671
2026-01-11 20:38:51,020: t15.2023.08.20 val PER: 0.0612
2026-01-11 20:38:51,020: t15.2023.08.25 val PER: 0.0678
2026-01-11 20:38:51,020: t15.2023.08.27 val PER: 0.1479
2026-01-11 20:38:51,021: t15.2023.09.01 val PER: 0.0341
2026-01-11 20:38:51,021: t15.2023.09.03 val PER: 0.1069
2026-01-11 20:38:51,021: t15.2023.09.24 val PER: 0.0789
2026-01-11 20:38:51,021: t15.2023.09.29 val PER: 0.0983
2026-01-11 20:38:51,021: t15.2023.10.01 val PER: 0.1341
2026-01-11 20:38:51,021: t15.2023.10.06 val PER: 0.0581
2026-01-11 20:38:51,021: t15.2023.10.08 val PER: 0.1976
2026-01-11 20:38:51,021: t15.2023.10.13 val PER: 0.1521
2026-01-11 20:38:51,021: t15.2023.10.15 val PER: 0.1022
2026-01-11 20:38:51,021: t15.2023.10.20 val PER: 0.1577
2026-01-11 20:38:51,021: t15.2023.10.22 val PER: 0.0824
2026-01-11 20:38:51,021: t15.2023.11.03 val PER: 0.1499
2026-01-11 20:38:51,021: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:38:51,022: t15.2023.11.17 val PER: 0.0171
2026-01-11 20:38:51,022: t15.2023.11.19 val PER: 0.0080
2026-01-11 20:38:51,022: t15.2023.11.26 val PER: 0.0384
2026-01-11 20:38:51,022: t15.2023.12.03 val PER: 0.0473
2026-01-11 20:38:51,022: t15.2023.12.08 val PER: 0.0320
2026-01-11 20:38:51,022: t15.2023.12.10 val PER: 0.0250
2026-01-11 20:38:51,022: t15.2023.12.17 val PER: 0.0832
2026-01-11 20:38:51,022: t15.2023.12.29 val PER: 0.0645
2026-01-11 20:38:51,022: t15.2024.02.25 val PER: 0.0744
2026-01-11 20:38:51,022: t15.2024.03.08 val PER: 0.1636
2026-01-11 20:38:51,022: t15.2024.03.15 val PER: 0.1601
2026-01-11 20:38:51,022: t15.2024.03.17 val PER: 0.0837
2026-01-11 20:38:51,023: t15.2024.05.10 val PER: 0.1189
2026-01-11 20:38:51,023: t15.2024.06.14 val PER: 0.1183
2026-01-11 20:38:51,023: t15.2024.07.19 val PER: 0.1641
2026-01-11 20:38:51,023: t15.2024.07.21 val PER: 0.0531
2026-01-11 20:38:51,023: t15.2024.07.28 val PER: 0.0868
2026-01-11 20:38:51,023: t15.2025.01.10 val PER: 0.2259
2026-01-11 20:38:51,023: t15.2025.01.12 val PER: 0.0885
2026-01-11 20:38:51,023: t15.2025.03.14 val PER: 0.2914
2026-01-11 20:38:51,023: t15.2025.03.16 val PER: 0.1178
2026-01-11 20:38:51,023: t15.2025.03.30 val PER: 0.2057
2026-01-11 20:38:51,023: t15.2025.04.13 val PER: 0.1854
2026-01-11 20:38:51,195: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_78500
2026-01-11 20:39:01,425: Train batch 78600: loss: 0.13 grad norm: 9.54 time: 0.065
2026-01-11 20:39:22,280: Train batch 78800: loss: 0.20 grad norm: 26.18 time: 0.091
2026-01-11 20:39:43,349: Train batch 79000: loss: 0.08 grad norm: 4.86 time: 0.060
2026-01-11 20:39:43,350: Running test after training batch: 79000
2026-01-11 20:39:43,572: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:39:49,641: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 20:39:49,724: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-11 20:40:11,341: Val batch 79000: PER (avg): 0.1009 CTC Loss (avg): 34.1086 WER(5gram): 11.80% (n=256) time: 27.991
2026-01-11 20:40:11,343: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-11 20:40:11,343: t15.2023.08.13 val PER: 0.0696
2026-01-11 20:40:11,343: t15.2023.08.18 val PER: 0.0645
2026-01-11 20:40:11,343: t15.2023.08.20 val PER: 0.0620
2026-01-11 20:40:11,344: t15.2023.08.25 val PER: 0.0663
2026-01-11 20:40:11,344: t15.2023.08.27 val PER: 0.1463
2026-01-11 20:40:11,344: t15.2023.09.01 val PER: 0.0390
2026-01-11 20:40:11,344: t15.2023.09.03 val PER: 0.1057
2026-01-11 20:40:11,344: t15.2023.09.24 val PER: 0.0789
2026-01-11 20:40:11,344: t15.2023.09.29 val PER: 0.1034
2026-01-11 20:40:11,344: t15.2023.10.01 val PER: 0.1308
2026-01-11 20:40:11,344: t15.2023.10.06 val PER: 0.0614
2026-01-11 20:40:11,344: t15.2023.10.08 val PER: 0.2030
2026-01-11 20:40:11,345: t15.2023.10.13 val PER: 0.1590
2026-01-11 20:40:11,345: t15.2023.10.15 val PER: 0.1002
2026-01-11 20:40:11,345: t15.2023.10.20 val PER: 0.1577
2026-01-11 20:40:11,345: t15.2023.10.22 val PER: 0.0857
2026-01-11 20:40:11,345: t15.2023.11.03 val PER: 0.1486
2026-01-11 20:40:11,345: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:40:11,345: t15.2023.11.17 val PER: 0.0156
2026-01-11 20:40:11,345: t15.2023.11.19 val PER: 0.0100
2026-01-11 20:40:11,345: t15.2023.11.26 val PER: 0.0391
2026-01-11 20:40:11,345: t15.2023.12.03 val PER: 0.0473
2026-01-11 20:40:11,346: t15.2023.12.08 val PER: 0.0286
2026-01-11 20:40:11,346: t15.2023.12.10 val PER: 0.0250
2026-01-11 20:40:11,346: t15.2023.12.17 val PER: 0.0780
2026-01-11 20:40:11,346: t15.2023.12.29 val PER: 0.0611
2026-01-11 20:40:11,346: t15.2024.02.25 val PER: 0.0716
2026-01-11 20:40:11,346: t15.2024.03.08 val PER: 0.1636
2026-01-11 20:40:11,346: t15.2024.03.15 val PER: 0.1620
2026-01-11 20:40:11,346: t15.2024.03.17 val PER: 0.0802
2026-01-11 20:40:11,346: t15.2024.05.10 val PER: 0.1159
2026-01-11 20:40:11,346: t15.2024.06.14 val PER: 0.1215
2026-01-11 20:40:11,346: t15.2024.07.19 val PER: 0.1661
2026-01-11 20:40:11,346: t15.2024.07.21 val PER: 0.0552
2026-01-11 20:40:11,347: t15.2024.07.28 val PER: 0.0890
2026-01-11 20:40:11,347: t15.2025.01.10 val PER: 0.2328
2026-01-11 20:40:11,347: t15.2025.01.12 val PER: 0.0808
2026-01-11 20:40:11,347: t15.2025.03.14 val PER: 0.2840
2026-01-11 20:40:11,347: t15.2025.03.16 val PER: 0.1152
2026-01-11 20:40:11,347: t15.2025.03.30 val PER: 0.2057
2026-01-11 20:40:11,347: t15.2025.04.13 val PER: 0.1826
2026-01-11 20:40:11,507: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_79000
2026-01-11 20:40:32,225: Train batch 79200: loss: 0.19 grad norm: 11.60 time: 0.074
2026-01-11 20:40:53,790: Train batch 79400: loss: 0.04 grad norm: 4.49 time: 0.076
2026-01-11 20:41:04,513: Running test after training batch: 79500
2026-01-11 20:41:04,659: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:41:11,331: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:41:11,419: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:41:33,428: Val batch 79500: PER (avg): 0.1020 CTC Loss (avg): 34.4641 WER(5gram): 10.63% (n=256) time: 28.914
2026-01-11 20:41:33,431: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 20:41:33,432: t15.2023.08.13 val PER: 0.0707
2026-01-11 20:41:33,432: t15.2023.08.18 val PER: 0.0629
2026-01-11 20:41:33,433: t15.2023.08.20 val PER: 0.0588
2026-01-11 20:41:33,433: t15.2023.08.25 val PER: 0.0648
2026-01-11 20:41:33,433: t15.2023.08.27 val PER: 0.1527
2026-01-11 20:41:33,433: t15.2023.09.01 val PER: 0.0381
2026-01-11 20:41:33,433: t15.2023.09.03 val PER: 0.1140
2026-01-11 20:41:33,433: t15.2023.09.24 val PER: 0.0777
2026-01-11 20:41:33,433: t15.2023.09.29 val PER: 0.1053
2026-01-11 20:41:33,433: t15.2023.10.01 val PER: 0.1295
2026-01-11 20:41:33,434: t15.2023.10.06 val PER: 0.0517
2026-01-11 20:41:33,434: t15.2023.10.08 val PER: 0.2124
2026-01-11 20:41:33,434: t15.2023.10.13 val PER: 0.1606
2026-01-11 20:41:33,434: t15.2023.10.15 val PER: 0.1015
2026-01-11 20:41:33,434: t15.2023.10.20 val PER: 0.1510
2026-01-11 20:41:33,434: t15.2023.10.22 val PER: 0.0935
2026-01-11 20:41:33,434: t15.2023.11.03 val PER: 0.1526
2026-01-11 20:41:33,435: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:41:33,435: t15.2023.11.17 val PER: 0.0171
2026-01-11 20:41:33,435: t15.2023.11.19 val PER: 0.0100
2026-01-11 20:41:33,435: t15.2023.11.26 val PER: 0.0377
2026-01-11 20:41:33,435: t15.2023.12.03 val PER: 0.0431
2026-01-11 20:41:33,435: t15.2023.12.08 val PER: 0.0326
2026-01-11 20:41:33,435: t15.2023.12.10 val PER: 0.0250
2026-01-11 20:41:33,435: t15.2023.12.17 val PER: 0.0800
2026-01-11 20:41:33,436: t15.2023.12.29 val PER: 0.0611
2026-01-11 20:41:33,436: t15.2024.02.25 val PER: 0.0744
2026-01-11 20:41:33,436: t15.2024.03.08 val PER: 0.1622
2026-01-11 20:41:33,436: t15.2024.03.15 val PER: 0.1588
2026-01-11 20:41:33,436: t15.2024.03.17 val PER: 0.0837
2026-01-11 20:41:33,436: t15.2024.05.10 val PER: 0.1159
2026-01-11 20:41:33,436: t15.2024.06.14 val PER: 0.1262
2026-01-11 20:41:33,436: t15.2024.07.19 val PER: 0.1681
2026-01-11 20:41:33,437: t15.2024.07.21 val PER: 0.0621
2026-01-11 20:41:33,437: t15.2024.07.28 val PER: 0.0868
2026-01-11 20:41:33,437: t15.2025.01.10 val PER: 0.2369
2026-01-11 20:41:33,437: t15.2025.01.12 val PER: 0.0870
2026-01-11 20:41:33,437: t15.2025.03.14 val PER: 0.2885
2026-01-11 20:41:33,437: t15.2025.03.16 val PER: 0.1126
2026-01-11 20:41:33,437: t15.2025.03.30 val PER: 0.2092
2026-01-11 20:41:33,437: t15.2025.04.13 val PER: 0.1797
2026-01-11 20:41:33,606: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_79500
2026-01-11 20:41:43,948: Train batch 79600: loss: 0.21 grad norm: 14.29 time: 0.089
2026-01-11 20:42:04,658: Train batch 79800: loss: 0.07 grad norm: 5.35 time: 0.066
2026-01-11 20:42:26,409: Train batch 80000: loss: 0.15 grad norm: 9.89 time: 0.077
2026-01-11 20:42:26,410: Running test after training batch: 80000
2026-01-11 20:42:26,520: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:42:32,702: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 20:42:32,785: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:42:49,096: Val batch 80000: PER (avg): 0.1016 CTC Loss (avg): 33.9788 WER(5gram): 11.67% (n=256) time: 22.686
2026-01-11 20:42:49,099: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 20:42:49,099: t15.2023.08.13 val PER: 0.0717
2026-01-11 20:42:49,099: t15.2023.08.18 val PER: 0.0620
2026-01-11 20:42:49,099: t15.2023.08.20 val PER: 0.0643
2026-01-11 20:42:49,099: t15.2023.08.25 val PER: 0.0633
2026-01-11 20:42:49,100: t15.2023.08.27 val PER: 0.1479
2026-01-11 20:42:49,100: t15.2023.09.01 val PER: 0.0390
2026-01-11 20:42:49,100: t15.2023.09.03 val PER: 0.1093
2026-01-11 20:42:49,100: t15.2023.09.24 val PER: 0.0813
2026-01-11 20:42:49,100: t15.2023.09.29 val PER: 0.1027
2026-01-11 20:42:49,100: t15.2023.10.01 val PER: 0.1295
2026-01-11 20:42:49,100: t15.2023.10.06 val PER: 0.0560
2026-01-11 20:42:49,100: t15.2023.10.08 val PER: 0.2124
2026-01-11 20:42:49,101: t15.2023.10.13 val PER: 0.1567
2026-01-11 20:42:49,101: t15.2023.10.15 val PER: 0.1042
2026-01-11 20:42:49,101: t15.2023.10.20 val PER: 0.1644
2026-01-11 20:42:49,101: t15.2023.10.22 val PER: 0.0857
2026-01-11 20:42:49,101: t15.2023.11.03 val PER: 0.1493
2026-01-11 20:42:49,101: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:42:49,101: t15.2023.11.17 val PER: 0.0202
2026-01-11 20:42:49,101: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:42:49,101: t15.2023.11.26 val PER: 0.0391
2026-01-11 20:42:49,101: t15.2023.12.03 val PER: 0.0410
2026-01-11 20:42:49,102: t15.2023.12.08 val PER: 0.0300
2026-01-11 20:42:49,102: t15.2023.12.10 val PER: 0.0237
2026-01-11 20:42:49,102: t15.2023.12.17 val PER: 0.0790
2026-01-11 20:42:49,102: t15.2023.12.29 val PER: 0.0638
2026-01-11 20:42:49,102: t15.2024.02.25 val PER: 0.0688
2026-01-11 20:42:49,102: t15.2024.03.08 val PER: 0.1650
2026-01-11 20:42:49,102: t15.2024.03.15 val PER: 0.1582
2026-01-11 20:42:49,103: t15.2024.03.17 val PER: 0.0816
2026-01-11 20:42:49,103: t15.2024.05.10 val PER: 0.1189
2026-01-11 20:42:49,103: t15.2024.06.14 val PER: 0.1278
2026-01-11 20:42:49,103: t15.2024.07.19 val PER: 0.1681
2026-01-11 20:42:49,103: t15.2024.07.21 val PER: 0.0614
2026-01-11 20:42:49,103: t15.2024.07.28 val PER: 0.0846
2026-01-11 20:42:49,103: t15.2025.01.10 val PER: 0.2300
2026-01-11 20:42:49,104: t15.2025.01.12 val PER: 0.0847
2026-01-11 20:42:49,104: t15.2025.03.14 val PER: 0.2944
2026-01-11 20:42:49,104: t15.2025.03.16 val PER: 0.1165
2026-01-11 20:42:49,104: t15.2025.03.30 val PER: 0.2023
2026-01-11 20:42:49,104: t15.2025.04.13 val PER: 0.1797
2026-01-11 20:42:49,266: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_80000
2026-01-11 20:43:10,736: Train batch 80200: loss: 0.19 grad norm: 10.56 time: 0.082
2026-01-11 20:43:31,742: Train batch 80400: loss: 0.13 grad norm: 12.05 time: 0.072
2026-01-11 20:43:42,135: Running test after training batch: 80500
2026-01-11 20:43:42,271: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:43:49,013: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 20:43:49,142: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:44:09,195: Val batch 80500: PER (avg): 0.1013 CTC Loss (avg): 34.3025 WER(5gram): 11.15% (n=256) time: 27.059
2026-01-11 20:44:09,198: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 20:44:09,198: t15.2023.08.13 val PER: 0.0686
2026-01-11 20:44:09,198: t15.2023.08.18 val PER: 0.0645
2026-01-11 20:44:09,198: t15.2023.08.20 val PER: 0.0620
2026-01-11 20:44:09,198: t15.2023.08.25 val PER: 0.0663
2026-01-11 20:44:09,198: t15.2023.08.27 val PER: 0.1479
2026-01-11 20:44:09,198: t15.2023.09.01 val PER: 0.0381
2026-01-11 20:44:09,198: t15.2023.09.03 val PER: 0.1057
2026-01-11 20:44:09,199: t15.2023.09.24 val PER: 0.0765
2026-01-11 20:44:09,199: t15.2023.09.29 val PER: 0.1015
2026-01-11 20:44:09,199: t15.2023.10.01 val PER: 0.1314
2026-01-11 20:44:09,199: t15.2023.10.06 val PER: 0.0560
2026-01-11 20:44:09,199: t15.2023.10.08 val PER: 0.2124
2026-01-11 20:44:09,199: t15.2023.10.13 val PER: 0.1544
2026-01-11 20:44:09,200: t15.2023.10.15 val PER: 0.1009
2026-01-11 20:44:09,200: t15.2023.10.20 val PER: 0.1611
2026-01-11 20:44:09,200: t15.2023.10.22 val PER: 0.0857
2026-01-11 20:44:09,200: t15.2023.11.03 val PER: 0.1547
2026-01-11 20:44:09,200: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:44:09,200: t15.2023.11.17 val PER: 0.0202
2026-01-11 20:44:09,200: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:44:09,200: t15.2023.11.26 val PER: 0.0435
2026-01-11 20:44:09,201: t15.2023.12.03 val PER: 0.0420
2026-01-11 20:44:09,201: t15.2023.12.08 val PER: 0.0353
2026-01-11 20:44:09,201: t15.2023.12.10 val PER: 0.0237
2026-01-11 20:44:09,201: t15.2023.12.17 val PER: 0.0811
2026-01-11 20:44:09,201: t15.2023.12.29 val PER: 0.0645
2026-01-11 20:44:09,202: t15.2024.02.25 val PER: 0.0702
2026-01-11 20:44:09,202: t15.2024.03.08 val PER: 0.1679
2026-01-11 20:44:09,203: t15.2024.03.15 val PER: 0.1563
2026-01-11 20:44:09,203: t15.2024.03.17 val PER: 0.0830
2026-01-11 20:44:09,203: t15.2024.05.10 val PER: 0.1114
2026-01-11 20:44:09,203: t15.2024.06.14 val PER: 0.1167
2026-01-11 20:44:09,203: t15.2024.07.19 val PER: 0.1655
2026-01-11 20:44:09,203: t15.2024.07.21 val PER: 0.0579
2026-01-11 20:44:09,203: t15.2024.07.28 val PER: 0.0875
2026-01-11 20:44:09,204: t15.2025.01.10 val PER: 0.2300
2026-01-11 20:44:09,204: t15.2025.01.12 val PER: 0.0808
2026-01-11 20:44:09,204: t15.2025.03.14 val PER: 0.2796
2026-01-11 20:44:09,204: t15.2025.03.16 val PER: 0.1152
2026-01-11 20:44:09,204: t15.2025.03.30 val PER: 0.2161
2026-01-11 20:44:09,204: t15.2025.04.13 val PER: 0.1797
2026-01-11 20:44:09,375: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_80500
2026-01-11 20:44:20,005: Train batch 80600: loss: 0.19 grad norm: 9.64 time: 0.066
2026-01-11 20:44:42,256: Train batch 80800: loss: 0.33 grad norm: 16.75 time: 0.065
2026-01-11 20:45:02,700: Train batch 81000: loss: 0.11 grad norm: 9.58 time: 0.068
2026-01-11 20:45:02,701: Running test after training batch: 81000
2026-01-11 20:45:02,866: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:45:09,593: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:45:09,682: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:45:26,912: Val batch 81000: PER (avg): 0.1008 CTC Loss (avg): 34.1266 WER(5gram): 11.80% (n=256) time: 24.211
2026-01-11 20:45:26,914: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 20:45:26,914: t15.2023.08.13 val PER: 0.0696
2026-01-11 20:45:26,915: t15.2023.08.18 val PER: 0.0654
2026-01-11 20:45:26,915: t15.2023.08.20 val PER: 0.0627
2026-01-11 20:45:26,915: t15.2023.08.25 val PER: 0.0617
2026-01-11 20:45:26,915: t15.2023.08.27 val PER: 0.1447
2026-01-11 20:45:26,915: t15.2023.09.01 val PER: 0.0398
2026-01-11 20:45:26,915: t15.2023.09.03 val PER: 0.1057
2026-01-11 20:45:26,915: t15.2023.09.24 val PER: 0.0777
2026-01-11 20:45:26,915: t15.2023.09.29 val PER: 0.1015
2026-01-11 20:45:26,916: t15.2023.10.01 val PER: 0.1314
2026-01-11 20:45:26,916: t15.2023.10.06 val PER: 0.0538
2026-01-11 20:45:26,916: t15.2023.10.08 val PER: 0.2057
2026-01-11 20:45:26,916: t15.2023.10.13 val PER: 0.1552
2026-01-11 20:45:26,916: t15.2023.10.15 val PER: 0.0995
2026-01-11 20:45:26,916: t15.2023.10.20 val PER: 0.1544
2026-01-11 20:45:26,916: t15.2023.10.22 val PER: 0.0891
2026-01-11 20:45:26,916: t15.2023.11.03 val PER: 0.1554
2026-01-11 20:45:26,916: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:45:26,916: t15.2023.11.17 val PER: 0.0202
2026-01-11 20:45:26,917: t15.2023.11.19 val PER: 0.0100
2026-01-11 20:45:26,917: t15.2023.11.26 val PER: 0.0377
2026-01-11 20:45:26,917: t15.2023.12.03 val PER: 0.0473
2026-01-11 20:45:26,917: t15.2023.12.08 val PER: 0.0286
2026-01-11 20:45:26,917: t15.2023.12.10 val PER: 0.0237
2026-01-11 20:45:26,917: t15.2023.12.17 val PER: 0.0780
2026-01-11 20:45:26,917: t15.2023.12.29 val PER: 0.0652
2026-01-11 20:45:26,917: t15.2024.02.25 val PER: 0.0787
2026-01-11 20:45:26,917: t15.2024.03.08 val PER: 0.1679
2026-01-11 20:45:26,918: t15.2024.03.15 val PER: 0.1563
2026-01-11 20:45:26,918: t15.2024.03.17 val PER: 0.0802
2026-01-11 20:45:26,918: t15.2024.05.10 val PER: 0.1144
2026-01-11 20:45:26,918: t15.2024.06.14 val PER: 0.1167
2026-01-11 20:45:26,918: t15.2024.07.19 val PER: 0.1694
2026-01-11 20:45:26,918: t15.2024.07.21 val PER: 0.0538
2026-01-11 20:45:26,918: t15.2024.07.28 val PER: 0.0838
2026-01-11 20:45:26,918: t15.2025.01.10 val PER: 0.2190
2026-01-11 20:45:26,918: t15.2025.01.12 val PER: 0.0831
2026-01-11 20:45:26,918: t15.2025.03.14 val PER: 0.2766
2026-01-11 20:45:26,918: t15.2025.03.16 val PER: 0.1217
2026-01-11 20:45:26,919: t15.2025.03.30 val PER: 0.2195
2026-01-11 20:45:26,919: t15.2025.04.13 val PER: 0.1783
2026-01-11 20:45:27,072: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_81000
2026-01-11 20:45:47,683: Train batch 81200: loss: 0.17 grad norm: 9.48 time: 0.071
2026-01-11 20:46:08,946: Train batch 81400: loss: 0.06 grad norm: 5.08 time: 0.074
2026-01-11 20:46:19,516: Running test after training batch: 81500
2026-01-11 20:46:19,658: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:46:26,716: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:46:26,849: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:46:45,451: Val batch 81500: PER (avg): 0.1016 CTC Loss (avg): 34.6361 WER(5gram): 11.28% (n=256) time: 25.934
2026-01-11 20:46:45,453: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 20:46:45,453: t15.2023.08.13 val PER: 0.0717
2026-01-11 20:46:45,453: t15.2023.08.18 val PER: 0.0637
2026-01-11 20:46:45,454: t15.2023.08.20 val PER: 0.0667
2026-01-11 20:46:45,454: t15.2023.08.25 val PER: 0.0663
2026-01-11 20:46:45,454: t15.2023.08.27 val PER: 0.1463
2026-01-11 20:46:45,454: t15.2023.09.01 val PER: 0.0381
2026-01-11 20:46:45,454: t15.2023.09.03 val PER: 0.1069
2026-01-11 20:46:45,454: t15.2023.09.24 val PER: 0.0752
2026-01-11 20:46:45,454: t15.2023.09.29 val PER: 0.1021
2026-01-11 20:46:45,454: t15.2023.10.01 val PER: 0.1301
2026-01-11 20:46:45,455: t15.2023.10.06 val PER: 0.0538
2026-01-11 20:46:45,455: t15.2023.10.08 val PER: 0.2057
2026-01-11 20:46:45,455: t15.2023.10.13 val PER: 0.1590
2026-01-11 20:46:45,455: t15.2023.10.15 val PER: 0.1061
2026-01-11 20:46:45,455: t15.2023.10.20 val PER: 0.1544
2026-01-11 20:46:45,455: t15.2023.10.22 val PER: 0.0869
2026-01-11 20:46:45,455: t15.2023.11.03 val PER: 0.1493
2026-01-11 20:46:45,455: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:46:45,455: t15.2023.11.17 val PER: 0.0202
2026-01-11 20:46:45,455: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:46:45,456: t15.2023.11.26 val PER: 0.0428
2026-01-11 20:46:45,456: t15.2023.12.03 val PER: 0.0441
2026-01-11 20:46:45,456: t15.2023.12.08 val PER: 0.0320
2026-01-11 20:46:45,456: t15.2023.12.10 val PER: 0.0237
2026-01-11 20:46:45,456: t15.2023.12.17 val PER: 0.0821
2026-01-11 20:46:45,456: t15.2023.12.29 val PER: 0.0645
2026-01-11 20:46:45,457: t15.2024.02.25 val PER: 0.0744
2026-01-11 20:46:45,457: t15.2024.03.08 val PER: 0.1607
2026-01-11 20:46:45,457: t15.2024.03.15 val PER: 0.1626
2026-01-11 20:46:45,457: t15.2024.03.17 val PER: 0.0830
2026-01-11 20:46:45,457: t15.2024.05.10 val PER: 0.1189
2026-01-11 20:46:45,457: t15.2024.06.14 val PER: 0.1183
2026-01-11 20:46:45,457: t15.2024.07.19 val PER: 0.1661
2026-01-11 20:46:45,457: t15.2024.07.21 val PER: 0.0531
2026-01-11 20:46:45,457: t15.2024.07.28 val PER: 0.0838
2026-01-11 20:46:45,458: t15.2025.01.10 val PER: 0.2273
2026-01-11 20:46:45,458: t15.2025.01.12 val PER: 0.0855
2026-01-11 20:46:45,458: t15.2025.03.14 val PER: 0.2840
2026-01-11 20:46:45,458: t15.2025.03.16 val PER: 0.1191
2026-01-11 20:46:45,458: t15.2025.03.30 val PER: 0.2149
2026-01-11 20:46:45,458: t15.2025.04.13 val PER: 0.1769
2026-01-11 20:46:45,614: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_81500
2026-01-11 20:46:55,641: Train batch 81600: loss: 0.38 grad norm: 15.87 time: 0.082
2026-01-11 20:47:16,267: Train batch 81800: loss: 0.19 grad norm: 12.69 time: 0.065
2026-01-11 20:47:36,696: Train batch 82000: loss: 0.07 grad norm: 5.07 time: 0.074
2026-01-11 20:47:36,697: Running test after training batch: 82000
2026-01-11 20:47:36,902: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:47:43,033: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:47:43,125: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:48:00,042: Val batch 82000: PER (avg): 0.1014 CTC Loss (avg): 34.4214 WER(5gram): 11.21% (n=256) time: 23.345
2026-01-11 20:48:00,044: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 20:48:00,045: t15.2023.08.13 val PER: 0.0728
2026-01-11 20:48:00,045: t15.2023.08.18 val PER: 0.0587
2026-01-11 20:48:00,045: t15.2023.08.20 val PER: 0.0635
2026-01-11 20:48:00,045: t15.2023.08.25 val PER: 0.0663
2026-01-11 20:48:00,045: t15.2023.08.27 val PER: 0.1559
2026-01-11 20:48:00,045: t15.2023.09.01 val PER: 0.0373
2026-01-11 20:48:00,046: t15.2023.09.03 val PER: 0.1033
2026-01-11 20:48:00,046: t15.2023.09.24 val PER: 0.0765
2026-01-11 20:48:00,046: t15.2023.09.29 val PER: 0.1002
2026-01-11 20:48:00,046: t15.2023.10.01 val PER: 0.1394
2026-01-11 20:48:00,047: t15.2023.10.06 val PER: 0.0517
2026-01-11 20:48:00,047: t15.2023.10.08 val PER: 0.2070
2026-01-11 20:48:00,047: t15.2023.10.13 val PER: 0.1598
2026-01-11 20:48:00,047: t15.2023.10.15 val PER: 0.1028
2026-01-11 20:48:00,047: t15.2023.10.20 val PER: 0.1611
2026-01-11 20:48:00,047: t15.2023.10.22 val PER: 0.0913
2026-01-11 20:48:00,047: t15.2023.11.03 val PER: 0.1520
2026-01-11 20:48:00,047: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:48:00,048: t15.2023.11.17 val PER: 0.0202
2026-01-11 20:48:00,048: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:48:00,048: t15.2023.11.26 val PER: 0.0435
2026-01-11 20:48:00,048: t15.2023.12.03 val PER: 0.0483
2026-01-11 20:48:00,048: t15.2023.12.08 val PER: 0.0320
2026-01-11 20:48:00,048: t15.2023.12.10 val PER: 0.0250
2026-01-11 20:48:00,048: t15.2023.12.17 val PER: 0.0821
2026-01-11 20:48:00,048: t15.2023.12.29 val PER: 0.0638
2026-01-11 20:48:00,049: t15.2024.02.25 val PER: 0.0730
2026-01-11 20:48:00,049: t15.2024.03.08 val PER: 0.1650
2026-01-11 20:48:00,049: t15.2024.03.15 val PER: 0.1582
2026-01-11 20:48:00,049: t15.2024.03.17 val PER: 0.0816
2026-01-11 20:48:00,049: t15.2024.05.10 val PER: 0.1159
2026-01-11 20:48:00,049: t15.2024.06.14 val PER: 0.1183
2026-01-11 20:48:00,049: t15.2024.07.19 val PER: 0.1661
2026-01-11 20:48:00,049: t15.2024.07.21 val PER: 0.0524
2026-01-11 20:48:00,049: t15.2024.07.28 val PER: 0.0838
2026-01-11 20:48:00,050: t15.2025.01.10 val PER: 0.2300
2026-01-11 20:48:00,050: t15.2025.01.12 val PER: 0.0839
2026-01-11 20:48:00,050: t15.2025.03.14 val PER: 0.2870
2026-01-11 20:48:00,050: t15.2025.03.16 val PER: 0.1165
2026-01-11 20:48:00,050: t15.2025.03.30 val PER: 0.2057
2026-01-11 20:48:00,050: t15.2025.04.13 val PER: 0.1726
2026-01-11 20:48:00,210: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_82000
2026-01-11 20:48:21,675: Train batch 82200: loss: 0.48 grad norm: 24.81 time: 0.098
2026-01-11 20:48:43,107: Train batch 82400: loss: 0.37 grad norm: 14.57 time: 0.099
2026-01-11 20:48:53,535: Running test after training batch: 82500
2026-01-11 20:48:53,704: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:49:00,191: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:49:00,299: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:49:17,975: Val batch 82500: PER (avg): 0.1011 CTC Loss (avg): 34.5415 WER(5gram): 11.34% (n=256) time: 24.439
2026-01-11 20:49:17,977: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 20:49:17,978: t15.2023.08.13 val PER: 0.0738
2026-01-11 20:49:17,978: t15.2023.08.18 val PER: 0.0620
2026-01-11 20:49:17,978: t15.2023.08.20 val PER: 0.0659
2026-01-11 20:49:17,979: t15.2023.08.25 val PER: 0.0648
2026-01-11 20:49:17,979: t15.2023.08.27 val PER: 0.1527
2026-01-11 20:49:17,979: t15.2023.09.01 val PER: 0.0390
2026-01-11 20:49:17,979: t15.2023.09.03 val PER: 0.1057
2026-01-11 20:49:17,979: t15.2023.09.24 val PER: 0.0752
2026-01-11 20:49:17,979: t15.2023.09.29 val PER: 0.1002
2026-01-11 20:49:17,979: t15.2023.10.01 val PER: 0.1321
2026-01-11 20:49:17,979: t15.2023.10.06 val PER: 0.0560
2026-01-11 20:49:17,979: t15.2023.10.08 val PER: 0.2097
2026-01-11 20:49:17,979: t15.2023.10.13 val PER: 0.1513
2026-01-11 20:49:17,980: t15.2023.10.15 val PER: 0.0995
2026-01-11 20:49:17,980: t15.2023.10.20 val PER: 0.1577
2026-01-11 20:49:17,980: t15.2023.10.22 val PER: 0.0935
2026-01-11 20:49:17,980: t15.2023.11.03 val PER: 0.1526
2026-01-11 20:49:17,980: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:49:17,980: t15.2023.11.17 val PER: 0.0187
2026-01-11 20:49:17,980: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:49:17,980: t15.2023.11.26 val PER: 0.0391
2026-01-11 20:49:17,980: t15.2023.12.03 val PER: 0.0462
2026-01-11 20:49:17,980: t15.2023.12.08 val PER: 0.0326
2026-01-11 20:49:17,980: t15.2023.12.10 val PER: 0.0276
2026-01-11 20:49:17,980: t15.2023.12.17 val PER: 0.0738
2026-01-11 20:49:17,981: t15.2023.12.29 val PER: 0.0583
2026-01-11 20:49:17,981: t15.2024.02.25 val PER: 0.0730
2026-01-11 20:49:17,981: t15.2024.03.08 val PER: 0.1622
2026-01-11 20:49:17,981: t15.2024.03.15 val PER: 0.1582
2026-01-11 20:49:17,981: t15.2024.03.17 val PER: 0.0816
2026-01-11 20:49:17,981: t15.2024.05.10 val PER: 0.1233
2026-01-11 20:49:17,981: t15.2024.06.14 val PER: 0.1215
2026-01-11 20:49:17,981: t15.2024.07.19 val PER: 0.1688
2026-01-11 20:49:17,981: t15.2024.07.21 val PER: 0.0566
2026-01-11 20:49:17,981: t15.2024.07.28 val PER: 0.0831
2026-01-11 20:49:17,981: t15.2025.01.10 val PER: 0.2273
2026-01-11 20:49:17,982: t15.2025.01.12 val PER: 0.0839
2026-01-11 20:49:17,982: t15.2025.03.14 val PER: 0.2899
2026-01-11 20:49:17,982: t15.2025.03.16 val PER: 0.1191
2026-01-11 20:49:17,982: t15.2025.03.30 val PER: 0.2115
2026-01-11 20:49:17,982: t15.2025.04.13 val PER: 0.1712
2026-01-11 20:49:18,163: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_82500
2026-01-11 20:49:28,530: Train batch 82600: loss: 0.05 grad norm: 2.84 time: 0.093
2026-01-11 20:49:49,417: Train batch 82800: loss: 0.15 grad norm: 9.33 time: 0.079
2026-01-11 20:50:10,696: Train batch 83000: loss: 0.10 grad norm: 9.04 time: 0.059
2026-01-11 20:50:10,697: Running test after training batch: 83000
2026-01-11 20:50:10,869: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:50:17,311: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:50:17,392: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:50:35,585: Val batch 83000: PER (avg): 0.1015 CTC Loss (avg): 34.4332 WER(5gram): 11.08% (n=256) time: 24.888
2026-01-11 20:50:35,588: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 20:50:35,589: t15.2023.08.13 val PER: 0.0676
2026-01-11 20:50:35,589: t15.2023.08.18 val PER: 0.0654
2026-01-11 20:50:35,589: t15.2023.08.20 val PER: 0.0635
2026-01-11 20:50:35,589: t15.2023.08.25 val PER: 0.0663
2026-01-11 20:50:35,589: t15.2023.08.27 val PER: 0.1608
2026-01-11 20:50:35,589: t15.2023.09.01 val PER: 0.0373
2026-01-11 20:50:35,590: t15.2023.09.03 val PER: 0.1081
2026-01-11 20:50:35,590: t15.2023.09.24 val PER: 0.0765
2026-01-11 20:50:35,590: t15.2023.09.29 val PER: 0.1002
2026-01-11 20:50:35,590: t15.2023.10.01 val PER: 0.1328
2026-01-11 20:50:35,590: t15.2023.10.06 val PER: 0.0517
2026-01-11 20:50:35,590: t15.2023.10.08 val PER: 0.2111
2026-01-11 20:50:35,590: t15.2023.10.13 val PER: 0.1583
2026-01-11 20:50:35,590: t15.2023.10.15 val PER: 0.1035
2026-01-11 20:50:35,590: t15.2023.10.20 val PER: 0.1544
2026-01-11 20:50:35,590: t15.2023.10.22 val PER: 0.0913
2026-01-11 20:50:35,591: t15.2023.11.03 val PER: 0.1513
2026-01-11 20:50:35,591: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:50:35,591: t15.2023.11.17 val PER: 0.0202
2026-01-11 20:50:35,591: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:50:35,591: t15.2023.11.26 val PER: 0.0413
2026-01-11 20:50:35,591: t15.2023.12.03 val PER: 0.0452
2026-01-11 20:50:35,591: t15.2023.12.08 val PER: 0.0313
2026-01-11 20:50:35,591: t15.2023.12.10 val PER: 0.0250
2026-01-11 20:50:35,591: t15.2023.12.17 val PER: 0.0780
2026-01-11 20:50:35,591: t15.2023.12.29 val PER: 0.0631
2026-01-11 20:50:35,592: t15.2024.02.25 val PER: 0.0702
2026-01-11 20:50:35,592: t15.2024.03.08 val PER: 0.1664
2026-01-11 20:50:35,592: t15.2024.03.15 val PER: 0.1588
2026-01-11 20:50:35,592: t15.2024.03.17 val PER: 0.0865
2026-01-11 20:50:35,592: t15.2024.05.10 val PER: 0.1174
2026-01-11 20:50:35,592: t15.2024.06.14 val PER: 0.1183
2026-01-11 20:50:35,592: t15.2024.07.19 val PER: 0.1622
2026-01-11 20:50:35,592: t15.2024.07.21 val PER: 0.0552
2026-01-11 20:50:35,592: t15.2024.07.28 val PER: 0.0846
2026-01-11 20:50:35,592: t15.2025.01.10 val PER: 0.2314
2026-01-11 20:50:35,593: t15.2025.01.12 val PER: 0.0862
2026-01-11 20:50:35,593: t15.2025.03.14 val PER: 0.2796
2026-01-11 20:50:35,593: t15.2025.03.16 val PER: 0.1152
2026-01-11 20:50:35,593: t15.2025.03.30 val PER: 0.2149
2026-01-11 20:50:35,593: t15.2025.04.13 val PER: 0.1783
2026-01-11 20:50:35,765: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_83000
2026-01-11 20:50:56,099: Train batch 83200: loss: 0.25 grad norm: 17.18 time: 0.082
2026-01-11 20:51:17,012: Train batch 83400: loss: 0.11 grad norm: 9.38 time: 0.056
2026-01-11 20:51:27,541: Running test after training batch: 83500
2026-01-11 20:51:27,723: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:51:34,095: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:51:34,175: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:51:52,565: Val batch 83500: PER (avg): 0.1004 CTC Loss (avg): 34.8274 WER(5gram): 11.08% (n=256) time: 25.023
2026-01-11 20:51:52,567: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 20:51:52,567: t15.2023.08.13 val PER: 0.0717
2026-01-11 20:51:52,567: t15.2023.08.18 val PER: 0.0629
2026-01-11 20:51:52,567: t15.2023.08.20 val PER: 0.0643
2026-01-11 20:51:52,568: t15.2023.08.25 val PER: 0.0648
2026-01-11 20:51:52,568: t15.2023.08.27 val PER: 0.1511
2026-01-11 20:51:52,568: t15.2023.09.01 val PER: 0.0390
2026-01-11 20:51:52,568: t15.2023.09.03 val PER: 0.1081
2026-01-11 20:51:52,568: t15.2023.09.24 val PER: 0.0801
2026-01-11 20:51:52,568: t15.2023.09.29 val PER: 0.0983
2026-01-11 20:51:52,568: t15.2023.10.01 val PER: 0.1347
2026-01-11 20:51:52,569: t15.2023.10.06 val PER: 0.0517
2026-01-11 20:51:52,569: t15.2023.10.08 val PER: 0.2070
2026-01-11 20:51:52,569: t15.2023.10.13 val PER: 0.1521
2026-01-11 20:51:52,569: t15.2023.10.15 val PER: 0.1015
2026-01-11 20:51:52,569: t15.2023.10.20 val PER: 0.1544
2026-01-11 20:51:52,569: t15.2023.10.22 val PER: 0.0913
2026-01-11 20:51:52,569: t15.2023.11.03 val PER: 0.1486
2026-01-11 20:51:52,569: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:51:52,569: t15.2023.11.17 val PER: 0.0202
2026-01-11 20:51:52,569: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:51:52,569: t15.2023.11.26 val PER: 0.0420
2026-01-11 20:51:52,570: t15.2023.12.03 val PER: 0.0462
2026-01-11 20:51:52,570: t15.2023.12.08 val PER: 0.0293
2026-01-11 20:51:52,570: t15.2023.12.10 val PER: 0.0223
2026-01-11 20:51:52,570: t15.2023.12.17 val PER: 0.0800
2026-01-11 20:51:52,570: t15.2023.12.29 val PER: 0.0645
2026-01-11 20:51:52,570: t15.2024.02.25 val PER: 0.0730
2026-01-11 20:51:52,570: t15.2024.03.08 val PER: 0.1593
2026-01-11 20:51:52,570: t15.2024.03.15 val PER: 0.1588
2026-01-11 20:51:52,570: t15.2024.03.17 val PER: 0.0837
2026-01-11 20:51:52,571: t15.2024.05.10 val PER: 0.1085
2026-01-11 20:51:52,571: t15.2024.06.14 val PER: 0.1151
2026-01-11 20:51:52,571: t15.2024.07.19 val PER: 0.1635
2026-01-11 20:51:52,571: t15.2024.07.21 val PER: 0.0545
2026-01-11 20:51:52,571: t15.2024.07.28 val PER: 0.0868
2026-01-11 20:51:52,571: t15.2025.01.10 val PER: 0.2218
2026-01-11 20:51:52,571: t15.2025.01.12 val PER: 0.0793
2026-01-11 20:51:52,571: t15.2025.03.14 val PER: 0.2737
2026-01-11 20:51:52,571: t15.2025.03.16 val PER: 0.1113
2026-01-11 20:51:52,571: t15.2025.03.30 val PER: 0.2115
2026-01-11 20:51:52,571: t15.2025.04.13 val PER: 0.1854
2026-01-11 20:51:52,748: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_83500
2026-01-11 20:52:03,215: Train batch 83600: loss: 0.41 grad norm: 24.73 time: 0.074
2026-01-11 20:52:24,418: Train batch 83800: loss: 0.10 grad norm: 7.36 time: 0.086
2026-01-11 20:52:45,361: Train batch 84000: loss: 0.23 grad norm: 15.94 time: 0.077
2026-01-11 20:52:45,362: Running test after training batch: 84000
2026-01-11 20:52:45,614: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:52:52,123: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:52:52,251: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:53:09,633: Val batch 84000: PER (avg): 0.1012 CTC Loss (avg): 34.6587 WER(5gram): 10.95% (n=256) time: 24.270
2026-01-11 20:53:09,634: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 20:53:09,634: t15.2023.08.13 val PER: 0.0696
2026-01-11 20:53:09,635: t15.2023.08.18 val PER: 0.0637
2026-01-11 20:53:09,635: t15.2023.08.20 val PER: 0.0651
2026-01-11 20:53:09,635: t15.2023.08.25 val PER: 0.0633
2026-01-11 20:53:09,635: t15.2023.08.27 val PER: 0.1511
2026-01-11 20:53:09,635: t15.2023.09.01 val PER: 0.0365
2026-01-11 20:53:09,635: t15.2023.09.03 val PER: 0.1093
2026-01-11 20:53:09,635: t15.2023.09.24 val PER: 0.0777
2026-01-11 20:53:09,635: t15.2023.09.29 val PER: 0.0970
2026-01-11 20:53:09,635: t15.2023.10.01 val PER: 0.1367
2026-01-11 20:53:09,635: t15.2023.10.06 val PER: 0.0517
2026-01-11 20:53:09,636: t15.2023.10.08 val PER: 0.2138
2026-01-11 20:53:09,636: t15.2023.10.13 val PER: 0.1575
2026-01-11 20:53:09,636: t15.2023.10.15 val PER: 0.1074
2026-01-11 20:53:09,636: t15.2023.10.20 val PER: 0.1510
2026-01-11 20:53:09,636: t15.2023.10.22 val PER: 0.0891
2026-01-11 20:53:09,636: t15.2023.11.03 val PER: 0.1506
2026-01-11 20:53:09,636: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:53:09,636: t15.2023.11.17 val PER: 0.0187
2026-01-11 20:53:09,637: t15.2023.11.19 val PER: 0.0100
2026-01-11 20:53:09,637: t15.2023.11.26 val PER: 0.0399
2026-01-11 20:53:09,637: t15.2023.12.03 val PER: 0.0441
2026-01-11 20:53:09,637: t15.2023.12.08 val PER: 0.0326
2026-01-11 20:53:09,637: t15.2023.12.10 val PER: 0.0237
2026-01-11 20:53:09,637: t15.2023.12.17 val PER: 0.0769
2026-01-11 20:53:09,637: t15.2023.12.29 val PER: 0.0652
2026-01-11 20:53:09,637: t15.2024.02.25 val PER: 0.0716
2026-01-11 20:53:09,637: t15.2024.03.08 val PER: 0.1593
2026-01-11 20:53:09,637: t15.2024.03.15 val PER: 0.1595
2026-01-11 20:53:09,638: t15.2024.03.17 val PER: 0.0844
2026-01-11 20:53:09,638: t15.2024.05.10 val PER: 0.1174
2026-01-11 20:53:09,638: t15.2024.06.14 val PER: 0.1167
2026-01-11 20:53:09,638: t15.2024.07.19 val PER: 0.1628
2026-01-11 20:53:09,638: t15.2024.07.21 val PER: 0.0497
2026-01-11 20:53:09,638: t15.2024.07.28 val PER: 0.0860
2026-01-11 20:53:09,638: t15.2025.01.10 val PER: 0.2328
2026-01-11 20:53:09,638: t15.2025.01.12 val PER: 0.0847
2026-01-11 20:53:09,638: t15.2025.03.14 val PER: 0.2796
2026-01-11 20:53:09,638: t15.2025.03.16 val PER: 0.1113
2026-01-11 20:53:09,638: t15.2025.03.30 val PER: 0.2207
2026-01-11 20:53:09,639: t15.2025.04.13 val PER: 0.1755
2026-01-11 20:53:09,808: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_84000
2026-01-11 20:53:30,312: Train batch 84200: loss: 0.14 grad norm: 15.87 time: 0.088
2026-01-11 20:53:51,175: Train batch 84400: loss: 0.14 grad norm: 9.80 time: 0.083
2026-01-11 20:54:01,531: Running test after training batch: 84500
2026-01-11 20:54:01,660: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:54:08,321: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:54:08,430: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:54:26,540: Val batch 84500: PER (avg): 0.1004 CTC Loss (avg): 34.3004 WER(5gram): 11.41% (n=256) time: 25.008
2026-01-11 20:54:26,541: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 20:54:26,542: t15.2023.08.13 val PER: 0.0717
2026-01-11 20:54:26,543: t15.2023.08.18 val PER: 0.0612
2026-01-11 20:54:26,543: t15.2023.08.20 val PER: 0.0643
2026-01-11 20:54:26,543: t15.2023.08.25 val PER: 0.0678
2026-01-11 20:54:26,543: t15.2023.08.27 val PER: 0.1447
2026-01-11 20:54:26,543: t15.2023.09.01 val PER: 0.0381
2026-01-11 20:54:26,543: t15.2023.09.03 val PER: 0.1081
2026-01-11 20:54:26,543: t15.2023.09.24 val PER: 0.0728
2026-01-11 20:54:26,543: t15.2023.09.29 val PER: 0.1002
2026-01-11 20:54:26,544: t15.2023.10.01 val PER: 0.1321
2026-01-11 20:54:26,544: t15.2023.10.06 val PER: 0.0571
2026-01-11 20:54:26,544: t15.2023.10.08 val PER: 0.2016
2026-01-11 20:54:26,544: t15.2023.10.13 val PER: 0.1544
2026-01-11 20:54:26,544: t15.2023.10.15 val PER: 0.1028
2026-01-11 20:54:26,544: t15.2023.10.20 val PER: 0.1443
2026-01-11 20:54:26,544: t15.2023.10.22 val PER: 0.0924
2026-01-11 20:54:26,544: t15.2023.11.03 val PER: 0.1459
2026-01-11 20:54:26,544: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:54:26,545: t15.2023.11.17 val PER: 0.0171
2026-01-11 20:54:26,545: t15.2023.11.19 val PER: 0.0100
2026-01-11 20:54:26,545: t15.2023.11.26 val PER: 0.0406
2026-01-11 20:54:26,545: t15.2023.12.03 val PER: 0.0452
2026-01-11 20:54:26,545: t15.2023.12.08 val PER: 0.0320
2026-01-11 20:54:26,545: t15.2023.12.10 val PER: 0.0263
2026-01-11 20:54:26,545: t15.2023.12.17 val PER: 0.0842
2026-01-11 20:54:26,545: t15.2023.12.29 val PER: 0.0638
2026-01-11 20:54:26,545: t15.2024.02.25 val PER: 0.0730
2026-01-11 20:54:26,545: t15.2024.03.08 val PER: 0.1636
2026-01-11 20:54:26,546: t15.2024.03.15 val PER: 0.1601
2026-01-11 20:54:26,546: t15.2024.03.17 val PER: 0.0788
2026-01-11 20:54:26,546: t15.2024.05.10 val PER: 0.1159
2026-01-11 20:54:26,546: t15.2024.06.14 val PER: 0.1104
2026-01-11 20:54:26,546: t15.2024.07.19 val PER: 0.1681
2026-01-11 20:54:26,546: t15.2024.07.21 val PER: 0.0510
2026-01-11 20:54:26,546: t15.2024.07.28 val PER: 0.0846
2026-01-11 20:54:26,546: t15.2025.01.10 val PER: 0.2204
2026-01-11 20:54:26,546: t15.2025.01.12 val PER: 0.0855
2026-01-11 20:54:26,547: t15.2025.03.14 val PER: 0.2796
2026-01-11 20:54:26,547: t15.2025.03.16 val PER: 0.1126
2026-01-11 20:54:26,547: t15.2025.03.30 val PER: 0.2172
2026-01-11 20:54:26,547: t15.2025.04.13 val PER: 0.1755
2026-01-11 20:54:26,725: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_84500
2026-01-11 20:54:36,989: Train batch 84600: loss: 0.14 grad norm: 7.52 time: 0.082
2026-01-11 20:54:57,958: Train batch 84800: loss: 0.20 grad norm: 12.33 time: 0.095
2026-01-11 20:55:18,691: Train batch 85000: loss: 0.02 grad norm: 4.05 time: 0.064
2026-01-11 20:55:18,691: Running test after training batch: 85000
2026-01-11 20:55:18,819: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:55:26,068: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 20:55:26,159: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:55:43,287: Val batch 85000: PER (avg): 0.1015 CTC Loss (avg): 34.5421 WER(5gram): 11.15% (n=256) time: 24.596
2026-01-11 20:55:43,290: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 20:55:43,291: t15.2023.08.13 val PER: 0.0696
2026-01-11 20:55:43,291: t15.2023.08.18 val PER: 0.0629
2026-01-11 20:55:43,291: t15.2023.08.20 val PER: 0.0683
2026-01-11 20:55:43,291: t15.2023.08.25 val PER: 0.0663
2026-01-11 20:55:43,291: t15.2023.08.27 val PER: 0.1479
2026-01-11 20:55:43,291: t15.2023.09.01 val PER: 0.0373
2026-01-11 20:55:43,291: t15.2023.09.03 val PER: 0.1057
2026-01-11 20:55:43,291: t15.2023.09.24 val PER: 0.0728
2026-01-11 20:55:43,292: t15.2023.09.29 val PER: 0.0989
2026-01-11 20:55:43,292: t15.2023.10.01 val PER: 0.1347
2026-01-11 20:55:43,292: t15.2023.10.06 val PER: 0.0549
2026-01-11 20:55:43,292: t15.2023.10.08 val PER: 0.2057
2026-01-11 20:55:43,292: t15.2023.10.13 val PER: 0.1590
2026-01-11 20:55:43,292: t15.2023.10.15 val PER: 0.1035
2026-01-11 20:55:43,292: t15.2023.10.20 val PER: 0.1443
2026-01-11 20:55:43,292: t15.2023.10.22 val PER: 0.0913
2026-01-11 20:55:43,292: t15.2023.11.03 val PER: 0.1513
2026-01-11 20:55:43,292: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:55:43,292: t15.2023.11.17 val PER: 0.0202
2026-01-11 20:55:43,292: t15.2023.11.19 val PER: 0.0100
2026-01-11 20:55:43,292: t15.2023.11.26 val PER: 0.0384
2026-01-11 20:55:43,293: t15.2023.12.03 val PER: 0.0452
2026-01-11 20:55:43,293: t15.2023.12.08 val PER: 0.0313
2026-01-11 20:55:43,293: t15.2023.12.10 val PER: 0.0263
2026-01-11 20:55:43,293: t15.2023.12.17 val PER: 0.0832
2026-01-11 20:55:43,293: t15.2023.12.29 val PER: 0.0659
2026-01-11 20:55:43,293: t15.2024.02.25 val PER: 0.0730
2026-01-11 20:55:43,293: t15.2024.03.08 val PER: 0.1679
2026-01-11 20:55:43,293: t15.2024.03.15 val PER: 0.1601
2026-01-11 20:55:43,293: t15.2024.03.17 val PER: 0.0830
2026-01-11 20:55:43,293: t15.2024.05.10 val PER: 0.1218
2026-01-11 20:55:43,293: t15.2024.06.14 val PER: 0.1167
2026-01-11 20:55:43,293: t15.2024.07.19 val PER: 0.1655
2026-01-11 20:55:43,293: t15.2024.07.21 val PER: 0.0538
2026-01-11 20:55:43,294: t15.2024.07.28 val PER: 0.0816
2026-01-11 20:55:43,294: t15.2025.01.10 val PER: 0.2259
2026-01-11 20:55:43,294: t15.2025.01.12 val PER: 0.0885
2026-01-11 20:55:43,294: t15.2025.03.14 val PER: 0.2766
2026-01-11 20:55:43,294: t15.2025.03.16 val PER: 0.1178
2026-01-11 20:55:43,294: t15.2025.03.30 val PER: 0.2126
2026-01-11 20:55:43,294: t15.2025.04.13 val PER: 0.1854
2026-01-11 20:55:43,458: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_85000
2026-01-11 20:56:03,934: Train batch 85200: loss: 0.20 grad norm: 12.68 time: 0.070
2026-01-11 20:56:24,608: Train batch 85400: loss: 0.39 grad norm: 22.33 time: 0.070
2026-01-11 20:56:35,147: Running test after training batch: 85500
2026-01-11 20:56:35,464: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:56:41,965: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:56:42,057: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:56:59,443: Val batch 85500: PER (avg): 0.1006 CTC Loss (avg): 34.2249 WER(5gram): 11.47% (n=256) time: 24.295
2026-01-11 20:56:59,445: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 20:56:59,446: t15.2023.08.13 val PER: 0.0707
2026-01-11 20:56:59,446: t15.2023.08.18 val PER: 0.0629
2026-01-11 20:56:59,446: t15.2023.08.20 val PER: 0.0659
2026-01-11 20:56:59,446: t15.2023.08.25 val PER: 0.0633
2026-01-11 20:56:59,446: t15.2023.08.27 val PER: 0.1511
2026-01-11 20:56:59,446: t15.2023.09.01 val PER: 0.0373
2026-01-11 20:56:59,447: t15.2023.09.03 val PER: 0.1069
2026-01-11 20:56:59,447: t15.2023.09.24 val PER: 0.0765
2026-01-11 20:56:59,447: t15.2023.09.29 val PER: 0.0976
2026-01-11 20:56:59,447: t15.2023.10.01 val PER: 0.1354
2026-01-11 20:56:59,447: t15.2023.10.06 val PER: 0.0581
2026-01-11 20:56:59,447: t15.2023.10.08 val PER: 0.2111
2026-01-11 20:56:59,447: t15.2023.10.13 val PER: 0.1536
2026-01-11 20:56:59,447: t15.2023.10.15 val PER: 0.1022
2026-01-11 20:56:59,447: t15.2023.10.20 val PER: 0.1443
2026-01-11 20:56:59,447: t15.2023.10.22 val PER: 0.0924
2026-01-11 20:56:59,447: t15.2023.11.03 val PER: 0.1560
2026-01-11 20:56:59,447: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:56:59,448: t15.2023.11.17 val PER: 0.0187
2026-01-11 20:56:59,448: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:56:59,448: t15.2023.11.26 val PER: 0.0362
2026-01-11 20:56:59,448: t15.2023.12.03 val PER: 0.0473
2026-01-11 20:56:59,448: t15.2023.12.08 val PER: 0.0313
2026-01-11 20:56:59,448: t15.2023.12.10 val PER: 0.0223
2026-01-11 20:56:59,448: t15.2023.12.17 val PER: 0.0821
2026-01-11 20:56:59,448: t15.2023.12.29 val PER: 0.0638
2026-01-11 20:56:59,448: t15.2024.02.25 val PER: 0.0744
2026-01-11 20:56:59,448: t15.2024.03.08 val PER: 0.1664
2026-01-11 20:56:59,448: t15.2024.03.15 val PER: 0.1595
2026-01-11 20:56:59,448: t15.2024.03.17 val PER: 0.0809
2026-01-11 20:56:59,448: t15.2024.05.10 val PER: 0.1248
2026-01-11 20:56:59,449: t15.2024.06.14 val PER: 0.1104
2026-01-11 20:56:59,449: t15.2024.07.19 val PER: 0.1595
2026-01-11 20:56:59,449: t15.2024.07.21 val PER: 0.0497
2026-01-11 20:56:59,449: t15.2024.07.28 val PER: 0.0794
2026-01-11 20:56:59,449: t15.2025.01.10 val PER: 0.2245
2026-01-11 20:56:59,449: t15.2025.01.12 val PER: 0.0870
2026-01-11 20:56:59,449: t15.2025.03.14 val PER: 0.2811
2026-01-11 20:56:59,449: t15.2025.03.16 val PER: 0.1165
2026-01-11 20:56:59,449: t15.2025.03.30 val PER: 0.2126
2026-01-11 20:56:59,449: t15.2025.04.13 val PER: 0.1726
2026-01-11 20:56:59,614: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_85500
2026-01-11 20:57:09,904: Train batch 85600: loss: 0.56 grad norm: 27.07 time: 0.079
2026-01-11 20:57:30,553: Train batch 85800: loss: 0.59 grad norm: 28.94 time: 0.076
2026-01-11 20:57:51,315: Train batch 86000: loss: 0.22 grad norm: 18.53 time: 0.073
2026-01-11 20:57:51,315: Running test after training batch: 86000
2026-01-11 20:57:51,544: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:57:58,231: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 20:57:58,356: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:58:15,310: Val batch 86000: PER (avg): 0.1009 CTC Loss (avg): 34.4706 WER(5gram): 10.89% (n=256) time: 23.995
2026-01-11 20:58:15,312: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 20:58:15,313: t15.2023.08.13 val PER: 0.0707
2026-01-11 20:58:15,313: t15.2023.08.18 val PER: 0.0620
2026-01-11 20:58:15,313: t15.2023.08.20 val PER: 0.0588
2026-01-11 20:58:15,313: t15.2023.08.25 val PER: 0.0633
2026-01-11 20:58:15,313: t15.2023.08.27 val PER: 0.1543
2026-01-11 20:58:15,313: t15.2023.09.01 val PER: 0.0357
2026-01-11 20:58:15,313: t15.2023.09.03 val PER: 0.1069
2026-01-11 20:58:15,313: t15.2023.09.24 val PER: 0.0813
2026-01-11 20:58:15,313: t15.2023.09.29 val PER: 0.1015
2026-01-11 20:58:15,314: t15.2023.10.01 val PER: 0.1361
2026-01-11 20:58:15,314: t15.2023.10.06 val PER: 0.0517
2026-01-11 20:58:15,314: t15.2023.10.08 val PER: 0.2003
2026-01-11 20:58:15,314: t15.2023.10.13 val PER: 0.1521
2026-01-11 20:58:15,314: t15.2023.10.15 val PER: 0.1074
2026-01-11 20:58:15,314: t15.2023.10.20 val PER: 0.1443
2026-01-11 20:58:15,314: t15.2023.10.22 val PER: 0.0980
2026-01-11 20:58:15,314: t15.2023.11.03 val PER: 0.1533
2026-01-11 20:58:15,314: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:58:15,314: t15.2023.11.17 val PER: 0.0187
2026-01-11 20:58:15,314: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:58:15,314: t15.2023.11.26 val PER: 0.0384
2026-01-11 20:58:15,314: t15.2023.12.03 val PER: 0.0473
2026-01-11 20:58:15,315: t15.2023.12.08 val PER: 0.0320
2026-01-11 20:58:15,315: t15.2023.12.10 val PER: 0.0237
2026-01-11 20:58:15,315: t15.2023.12.17 val PER: 0.0759
2026-01-11 20:58:15,315: t15.2023.12.29 val PER: 0.0638
2026-01-11 20:58:15,315: t15.2024.02.25 val PER: 0.0787
2026-01-11 20:58:15,315: t15.2024.03.08 val PER: 0.1650
2026-01-11 20:58:15,315: t15.2024.03.15 val PER: 0.1588
2026-01-11 20:58:15,315: t15.2024.03.17 val PER: 0.0830
2026-01-11 20:58:15,315: t15.2024.05.10 val PER: 0.1129
2026-01-11 20:58:15,315: t15.2024.06.14 val PER: 0.1230
2026-01-11 20:58:15,315: t15.2024.07.19 val PER: 0.1641
2026-01-11 20:58:15,315: t15.2024.07.21 val PER: 0.0510
2026-01-11 20:58:15,316: t15.2024.07.28 val PER: 0.0757
2026-01-11 20:58:15,316: t15.2025.01.10 val PER: 0.2273
2026-01-11 20:58:15,316: t15.2025.01.12 val PER: 0.0947
2026-01-11 20:58:15,316: t15.2025.03.14 val PER: 0.2781
2026-01-11 20:58:15,316: t15.2025.03.16 val PER: 0.1099
2026-01-11 20:58:15,316: t15.2025.03.30 val PER: 0.2103
2026-01-11 20:58:15,316: t15.2025.04.13 val PER: 0.1840
2026-01-11 20:58:15,479: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_86000
2026-01-11 20:58:35,794: Train batch 86200: loss: 0.06 grad norm: 3.50 time: 0.089
2026-01-11 20:58:56,228: Train batch 86400: loss: 0.12 grad norm: 11.32 time: 0.083
2026-01-11 20:59:06,416: Running test after training batch: 86500
2026-01-11 20:59:06,649: WER debug GT example: You can see the code at this point as well.
2026-01-11 20:59:12,971: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 20:59:13,072: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 20:59:30,662: Val batch 86500: PER (avg): 0.1007 CTC Loss (avg): 34.7056 WER(5gram): 11.80% (n=256) time: 24.246
2026-01-11 20:59:30,664: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 20:59:30,664: t15.2023.08.13 val PER: 0.0738
2026-01-11 20:59:30,664: t15.2023.08.18 val PER: 0.0662
2026-01-11 20:59:30,664: t15.2023.08.20 val PER: 0.0651
2026-01-11 20:59:30,664: t15.2023.08.25 val PER: 0.0648
2026-01-11 20:59:30,665: t15.2023.08.27 val PER: 0.1511
2026-01-11 20:59:30,665: t15.2023.09.01 val PER: 0.0357
2026-01-11 20:59:30,665: t15.2023.09.03 val PER: 0.1069
2026-01-11 20:59:30,665: t15.2023.09.24 val PER: 0.0752
2026-01-11 20:59:30,665: t15.2023.09.29 val PER: 0.1015
2026-01-11 20:59:30,665: t15.2023.10.01 val PER: 0.1354
2026-01-11 20:59:30,665: t15.2023.10.06 val PER: 0.0527
2026-01-11 20:59:30,665: t15.2023.10.08 val PER: 0.2016
2026-01-11 20:59:30,665: t15.2023.10.13 val PER: 0.1474
2026-01-11 20:59:30,665: t15.2023.10.15 val PER: 0.1015
2026-01-11 20:59:30,665: t15.2023.10.20 val PER: 0.1477
2026-01-11 20:59:30,666: t15.2023.10.22 val PER: 0.0958
2026-01-11 20:59:30,666: t15.2023.11.03 val PER: 0.1520
2026-01-11 20:59:30,666: t15.2023.11.04 val PER: 0.0034
2026-01-11 20:59:30,666: t15.2023.11.17 val PER: 0.0156
2026-01-11 20:59:30,666: t15.2023.11.19 val PER: 0.0120
2026-01-11 20:59:30,666: t15.2023.11.26 val PER: 0.0406
2026-01-11 20:59:30,666: t15.2023.12.03 val PER: 0.0462
2026-01-11 20:59:30,666: t15.2023.12.08 val PER: 0.0320
2026-01-11 20:59:30,666: t15.2023.12.10 val PER: 0.0250
2026-01-11 20:59:30,666: t15.2023.12.17 val PER: 0.0717
2026-01-11 20:59:30,666: t15.2023.12.29 val PER: 0.0679
2026-01-11 20:59:30,666: t15.2024.02.25 val PER: 0.0716
2026-01-11 20:59:30,666: t15.2024.03.08 val PER: 0.1679
2026-01-11 20:59:30,667: t15.2024.03.15 val PER: 0.1582
2026-01-11 20:59:30,667: t15.2024.03.17 val PER: 0.0837
2026-01-11 20:59:30,667: t15.2024.05.10 val PER: 0.1174
2026-01-11 20:59:30,667: t15.2024.06.14 val PER: 0.1057
2026-01-11 20:59:30,667: t15.2024.07.19 val PER: 0.1635
2026-01-11 20:59:30,667: t15.2024.07.21 val PER: 0.0510
2026-01-11 20:59:30,667: t15.2024.07.28 val PER: 0.0831
2026-01-11 20:59:30,667: t15.2025.01.10 val PER: 0.2287
2026-01-11 20:59:30,667: t15.2025.01.12 val PER: 0.0885
2026-01-11 20:59:30,667: t15.2025.03.14 val PER: 0.2825
2026-01-11 20:59:30,667: t15.2025.03.16 val PER: 0.1178
2026-01-11 20:59:30,667: t15.2025.03.30 val PER: 0.2057
2026-01-11 20:59:30,667: t15.2025.04.13 val PER: 0.1840
2026-01-11 20:59:30,831: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_86500
2026-01-11 20:59:40,829: Train batch 86600: loss: 0.11 grad norm: 5.87 time: 0.086
2026-01-11 21:00:01,746: Train batch 86800: loss: 0.10 grad norm: 15.13 time: 0.080
2026-01-11 21:00:22,424: Train batch 87000: loss: 0.09 grad norm: 8.83 time: 0.087
2026-01-11 21:00:22,425: Running test after training batch: 87000
2026-01-11 21:00:22,539: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:00:28,802: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 21:00:28,890: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:00:46,252: Val batch 87000: PER (avg): 0.1010 CTC Loss (avg): 34.7161 WER(5gram): 10.95% (n=256) time: 23.827
2026-01-11 21:00:46,254: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 21:00:46,255: t15.2023.08.13 val PER: 0.0738
2026-01-11 21:00:46,255: t15.2023.08.18 val PER: 0.0629
2026-01-11 21:00:46,255: t15.2023.08.20 val PER: 0.0651
2026-01-11 21:00:46,255: t15.2023.08.25 val PER: 0.0648
2026-01-11 21:00:46,255: t15.2023.08.27 val PER: 0.1592
2026-01-11 21:00:46,255: t15.2023.09.01 val PER: 0.0390
2026-01-11 21:00:46,255: t15.2023.09.03 val PER: 0.1057
2026-01-11 21:00:46,255: t15.2023.09.24 val PER: 0.0777
2026-01-11 21:00:46,255: t15.2023.09.29 val PER: 0.1047
2026-01-11 21:00:46,255: t15.2023.10.01 val PER: 0.1341
2026-01-11 21:00:46,256: t15.2023.10.06 val PER: 0.0527
2026-01-11 21:00:46,256: t15.2023.10.08 val PER: 0.1989
2026-01-11 21:00:46,256: t15.2023.10.13 val PER: 0.1497
2026-01-11 21:00:46,256: t15.2023.10.15 val PER: 0.1028
2026-01-11 21:00:46,256: t15.2023.10.20 val PER: 0.1477
2026-01-11 21:00:46,256: t15.2023.10.22 val PER: 0.0902
2026-01-11 21:00:46,256: t15.2023.11.03 val PER: 0.1520
2026-01-11 21:00:46,256: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:00:46,256: t15.2023.11.17 val PER: 0.0156
2026-01-11 21:00:46,256: t15.2023.11.19 val PER: 0.0120
2026-01-11 21:00:46,256: t15.2023.11.26 val PER: 0.0413
2026-01-11 21:00:46,256: t15.2023.12.03 val PER: 0.0452
2026-01-11 21:00:46,257: t15.2023.12.08 val PER: 0.0320
2026-01-11 21:00:46,257: t15.2023.12.10 val PER: 0.0250
2026-01-11 21:00:46,257: t15.2023.12.17 val PER: 0.0748
2026-01-11 21:00:46,257: t15.2023.12.29 val PER: 0.0645
2026-01-11 21:00:46,257: t15.2024.02.25 val PER: 0.0772
2026-01-11 21:00:46,257: t15.2024.03.08 val PER: 0.1607
2026-01-11 21:00:46,257: t15.2024.03.15 val PER: 0.1582
2026-01-11 21:00:46,257: t15.2024.03.17 val PER: 0.0844
2026-01-11 21:00:46,257: t15.2024.05.10 val PER: 0.1144
2026-01-11 21:00:46,257: t15.2024.06.14 val PER: 0.1183
2026-01-11 21:00:46,257: t15.2024.07.19 val PER: 0.1622
2026-01-11 21:00:46,257: t15.2024.07.21 val PER: 0.0517
2026-01-11 21:00:46,257: t15.2024.07.28 val PER: 0.0794
2026-01-11 21:00:46,258: t15.2025.01.10 val PER: 0.2259
2026-01-11 21:00:46,258: t15.2025.01.12 val PER: 0.0885
2026-01-11 21:00:46,258: t15.2025.03.14 val PER: 0.2811
2026-01-11 21:00:46,258: t15.2025.03.16 val PER: 0.1217
2026-01-11 21:00:46,258: t15.2025.03.30 val PER: 0.2046
2026-01-11 21:00:46,258: t15.2025.04.13 val PER: 0.1926
2026-01-11 21:00:46,418: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_87000
2026-01-11 21:01:06,704: Train batch 87200: loss: 0.14 grad norm: 8.14 time: 0.088
2026-01-11 21:01:27,159: Train batch 87400: loss: 0.05 grad norm: 3.04 time: 0.075
2026-01-11 21:01:37,300: Running test after training batch: 87500
2026-01-11 21:01:37,433: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:01:44,619: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 21:01:44,716: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:02:06,388: Val batch 87500: PER (avg): 0.0995 CTC Loss (avg): 34.3644 WER(5gram): 10.76% (n=256) time: 29.087
2026-01-11 21:02:06,390: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 21:02:06,392: t15.2023.08.13 val PER: 0.0686
2026-01-11 21:02:06,392: t15.2023.08.18 val PER: 0.0645
2026-01-11 21:02:06,392: t15.2023.08.20 val PER: 0.0596
2026-01-11 21:02:06,392: t15.2023.08.25 val PER: 0.0633
2026-01-11 21:02:06,392: t15.2023.08.27 val PER: 0.1543
2026-01-11 21:02:06,392: t15.2023.09.01 val PER: 0.0357
2026-01-11 21:02:06,392: t15.2023.09.03 val PER: 0.1081
2026-01-11 21:02:06,393: t15.2023.09.24 val PER: 0.0740
2026-01-11 21:02:06,393: t15.2023.09.29 val PER: 0.0996
2026-01-11 21:02:06,393: t15.2023.10.01 val PER: 0.1361
2026-01-11 21:02:06,393: t15.2023.10.06 val PER: 0.0517
2026-01-11 21:02:06,393: t15.2023.10.08 val PER: 0.1976
2026-01-11 21:02:06,393: t15.2023.10.13 val PER: 0.1513
2026-01-11 21:02:06,394: t15.2023.10.15 val PER: 0.0989
2026-01-11 21:02:06,394: t15.2023.10.20 val PER: 0.1477
2026-01-11 21:02:06,394: t15.2023.10.22 val PER: 0.0935
2026-01-11 21:02:06,394: t15.2023.11.03 val PER: 0.1506
2026-01-11 21:02:06,394: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:02:06,394: t15.2023.11.17 val PER: 0.0171
2026-01-11 21:02:06,395: t15.2023.11.19 val PER: 0.0140
2026-01-11 21:02:06,395: t15.2023.11.26 val PER: 0.0355
2026-01-11 21:02:06,395: t15.2023.12.03 val PER: 0.0462
2026-01-11 21:02:06,395: t15.2023.12.08 val PER: 0.0346
2026-01-11 21:02:06,395: t15.2023.12.10 val PER: 0.0250
2026-01-11 21:02:06,395: t15.2023.12.17 val PER: 0.0738
2026-01-11 21:02:06,395: t15.2023.12.29 val PER: 0.0645
2026-01-11 21:02:06,395: t15.2024.02.25 val PER: 0.0674
2026-01-11 21:02:06,395: t15.2024.03.08 val PER: 0.1593
2026-01-11 21:02:06,396: t15.2024.03.15 val PER: 0.1588
2026-01-11 21:02:06,396: t15.2024.03.17 val PER: 0.0774
2026-01-11 21:02:06,396: t15.2024.05.10 val PER: 0.1129
2026-01-11 21:02:06,396: t15.2024.06.14 val PER: 0.1183
2026-01-11 21:02:06,396: t15.2024.07.19 val PER: 0.1628
2026-01-11 21:02:06,396: t15.2024.07.21 val PER: 0.0517
2026-01-11 21:02:06,396: t15.2024.07.28 val PER: 0.0824
2026-01-11 21:02:06,397: t15.2025.01.10 val PER: 0.2328
2026-01-11 21:02:06,397: t15.2025.01.12 val PER: 0.0847
2026-01-11 21:02:06,397: t15.2025.03.14 val PER: 0.2707
2026-01-11 21:02:06,397: t15.2025.03.16 val PER: 0.1113
2026-01-11 21:02:06,397: t15.2025.03.30 val PER: 0.2080
2026-01-11 21:02:06,397: t15.2025.04.13 val PER: 0.1854
2026-01-11 21:02:06,578: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_87500
2026-01-11 21:02:16,642: Train batch 87600: loss: 0.37 grad norm: 33.18 time: 0.091
2026-01-11 21:02:37,361: Train batch 87800: loss: 0.20 grad norm: 22.38 time: 0.085
2026-01-11 21:02:57,997: Train batch 88000: loss: 0.04 grad norm: 3.76 time: 0.082
2026-01-11 21:02:57,998: Running test after training batch: 88000
2026-01-11 21:02:58,246: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:03:04,650: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 21:03:04,746: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:03:23,301: Val batch 88000: PER (avg): 0.1000 CTC Loss (avg): 34.4845 WER(5gram): 11.02% (n=256) time: 25.303
2026-01-11 21:03:23,303: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 21:03:23,304: t15.2023.08.13 val PER: 0.0717
2026-01-11 21:03:23,304: t15.2023.08.18 val PER: 0.0645
2026-01-11 21:03:23,304: t15.2023.08.20 val PER: 0.0635
2026-01-11 21:03:23,304: t15.2023.08.25 val PER: 0.0663
2026-01-11 21:03:23,304: t15.2023.08.27 val PER: 0.1511
2026-01-11 21:03:23,304: t15.2023.09.01 val PER: 0.0373
2026-01-11 21:03:23,304: t15.2023.09.03 val PER: 0.1069
2026-01-11 21:03:23,304: t15.2023.09.24 val PER: 0.0752
2026-01-11 21:03:23,305: t15.2023.09.29 val PER: 0.1015
2026-01-11 21:03:23,305: t15.2023.10.01 val PER: 0.1288
2026-01-11 21:03:23,305: t15.2023.10.06 val PER: 0.0527
2026-01-11 21:03:23,305: t15.2023.10.08 val PER: 0.1976
2026-01-11 21:03:23,305: t15.2023.10.13 val PER: 0.1505
2026-01-11 21:03:23,305: t15.2023.10.15 val PER: 0.1048
2026-01-11 21:03:23,305: t15.2023.10.20 val PER: 0.1477
2026-01-11 21:03:23,305: t15.2023.10.22 val PER: 0.0869
2026-01-11 21:03:23,305: t15.2023.11.03 val PER: 0.1486
2026-01-11 21:03:23,305: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:03:23,306: t15.2023.11.17 val PER: 0.0187
2026-01-11 21:03:23,306: t15.2023.11.19 val PER: 0.0120
2026-01-11 21:03:23,306: t15.2023.11.26 val PER: 0.0384
2026-01-11 21:03:23,306: t15.2023.12.03 val PER: 0.0452
2026-01-11 21:03:23,306: t15.2023.12.08 val PER: 0.0326
2026-01-11 21:03:23,306: t15.2023.12.10 val PER: 0.0250
2026-01-11 21:03:23,306: t15.2023.12.17 val PER: 0.0728
2026-01-11 21:03:23,306: t15.2023.12.29 val PER: 0.0693
2026-01-11 21:03:23,306: t15.2024.02.25 val PER: 0.0744
2026-01-11 21:03:23,306: t15.2024.03.08 val PER: 0.1607
2026-01-11 21:03:23,306: t15.2024.03.15 val PER: 0.1626
2026-01-11 21:03:23,306: t15.2024.03.17 val PER: 0.0788
2026-01-11 21:03:23,307: t15.2024.05.10 val PER: 0.1144
2026-01-11 21:03:23,307: t15.2024.06.14 val PER: 0.1183
2026-01-11 21:03:23,307: t15.2024.07.19 val PER: 0.1635
2026-01-11 21:03:23,307: t15.2024.07.21 val PER: 0.0524
2026-01-11 21:03:23,307: t15.2024.07.28 val PER: 0.0816
2026-01-11 21:03:23,307: t15.2025.01.10 val PER: 0.2342
2026-01-11 21:03:23,307: t15.2025.01.12 val PER: 0.0855
2026-01-11 21:03:23,308: t15.2025.03.14 val PER: 0.2633
2026-01-11 21:03:23,308: t15.2025.03.16 val PER: 0.1099
2026-01-11 21:03:23,308: t15.2025.03.30 val PER: 0.2080
2026-01-11 21:03:23,308: t15.2025.04.13 val PER: 0.1840
2026-01-11 21:03:23,472: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_88000
2026-01-11 21:03:43,862: Train batch 88200: loss: 0.21 grad norm: 20.03 time: 0.086
2026-01-11 21:04:04,225: Train batch 88400: loss: 0.13 grad norm: 11.13 time: 0.095
2026-01-11 21:04:14,415: Running test after training batch: 88500
2026-01-11 21:04:14,564: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:04:21,979: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 21:04:22,071: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:04:39,875: Val batch 88500: PER (avg): 0.1005 CTC Loss (avg): 34.4383 WER(5gram): 10.82% (n=256) time: 25.459
2026-01-11 21:04:39,877: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 21:04:39,878: t15.2023.08.13 val PER: 0.0696
2026-01-11 21:04:39,879: t15.2023.08.18 val PER: 0.0637
2026-01-11 21:04:39,879: t15.2023.08.20 val PER: 0.0667
2026-01-11 21:04:39,879: t15.2023.08.25 val PER: 0.0648
2026-01-11 21:04:39,879: t15.2023.08.27 val PER: 0.1527
2026-01-11 21:04:39,879: t15.2023.09.01 val PER: 0.0357
2026-01-11 21:04:39,879: t15.2023.09.03 val PER: 0.1069
2026-01-11 21:04:39,879: t15.2023.09.24 val PER: 0.0752
2026-01-11 21:04:39,879: t15.2023.09.29 val PER: 0.0996
2026-01-11 21:04:39,879: t15.2023.10.01 val PER: 0.1367
2026-01-11 21:04:39,879: t15.2023.10.06 val PER: 0.0517
2026-01-11 21:04:39,880: t15.2023.10.08 val PER: 0.1962
2026-01-11 21:04:39,880: t15.2023.10.13 val PER: 0.1536
2026-01-11 21:04:39,880: t15.2023.10.15 val PER: 0.1048
2026-01-11 21:04:39,880: t15.2023.10.20 val PER: 0.1409
2026-01-11 21:04:39,880: t15.2023.10.22 val PER: 0.0891
2026-01-11 21:04:39,880: t15.2023.11.03 val PER: 0.1533
2026-01-11 21:04:39,880: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:04:39,880: t15.2023.11.17 val PER: 0.0202
2026-01-11 21:04:39,880: t15.2023.11.19 val PER: 0.0120
2026-01-11 21:04:39,880: t15.2023.11.26 val PER: 0.0391
2026-01-11 21:04:39,880: t15.2023.12.03 val PER: 0.0473
2026-01-11 21:04:39,880: t15.2023.12.08 val PER: 0.0326
2026-01-11 21:04:39,881: t15.2023.12.10 val PER: 0.0250
2026-01-11 21:04:39,881: t15.2023.12.17 val PER: 0.0748
2026-01-11 21:04:39,881: t15.2023.12.29 val PER: 0.0631
2026-01-11 21:04:39,881: t15.2024.02.25 val PER: 0.0688
2026-01-11 21:04:39,881: t15.2024.03.08 val PER: 0.1679
2026-01-11 21:04:39,881: t15.2024.03.15 val PER: 0.1601
2026-01-11 21:04:39,881: t15.2024.03.17 val PER: 0.0823
2026-01-11 21:04:39,881: t15.2024.05.10 val PER: 0.1189
2026-01-11 21:04:39,881: t15.2024.06.14 val PER: 0.1167
2026-01-11 21:04:39,881: t15.2024.07.19 val PER: 0.1628
2026-01-11 21:04:39,882: t15.2024.07.21 val PER: 0.0517
2026-01-11 21:04:39,882: t15.2024.07.28 val PER: 0.0816
2026-01-11 21:04:39,882: t15.2025.01.10 val PER: 0.2300
2026-01-11 21:04:39,882: t15.2025.01.12 val PER: 0.0870
2026-01-11 21:04:39,882: t15.2025.03.14 val PER: 0.2722
2026-01-11 21:04:39,882: t15.2025.03.16 val PER: 0.1178
2026-01-11 21:04:39,882: t15.2025.03.30 val PER: 0.2000
2026-01-11 21:04:39,882: t15.2025.04.13 val PER: 0.1840
2026-01-11 21:04:40,066: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_88500
2026-01-11 21:04:50,199: Train batch 88600: loss: 0.51 grad norm: 34.51 time: 0.090
2026-01-11 21:05:10,353: Train batch 88800: loss: 0.10 grad norm: 8.15 time: 0.082
2026-01-11 21:05:30,838: Train batch 89000: loss: 0.30 grad norm: 19.25 time: 0.074
2026-01-11 21:05:30,839: Running test after training batch: 89000
2026-01-11 21:05:31,015: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:05:37,240: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 21:05:37,320: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:05:54,463: Val batch 89000: PER (avg): 0.0999 CTC Loss (avg): 34.4825 WER(5gram): 11.15% (n=256) time: 23.624
2026-01-11 21:05:54,465: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 21:05:54,465: t15.2023.08.13 val PER: 0.0707
2026-01-11 21:05:54,465: t15.2023.08.18 val PER: 0.0629
2026-01-11 21:05:54,465: t15.2023.08.20 val PER: 0.0627
2026-01-11 21:05:54,465: t15.2023.08.25 val PER: 0.0648
2026-01-11 21:05:54,465: t15.2023.08.27 val PER: 0.1479
2026-01-11 21:05:54,465: t15.2023.09.01 val PER: 0.0365
2026-01-11 21:05:54,465: t15.2023.09.03 val PER: 0.1033
2026-01-11 21:05:54,466: t15.2023.09.24 val PER: 0.0752
2026-01-11 21:05:54,466: t15.2023.09.29 val PER: 0.0970
2026-01-11 21:05:54,466: t15.2023.10.01 val PER: 0.1354
2026-01-11 21:05:54,466: t15.2023.10.06 val PER: 0.0506
2026-01-11 21:05:54,466: t15.2023.10.08 val PER: 0.1976
2026-01-11 21:05:54,466: t15.2023.10.13 val PER: 0.1505
2026-01-11 21:05:54,466: t15.2023.10.15 val PER: 0.1028
2026-01-11 21:05:54,466: t15.2023.10.20 val PER: 0.1409
2026-01-11 21:05:54,466: t15.2023.10.22 val PER: 0.0869
2026-01-11 21:05:54,466: t15.2023.11.03 val PER: 0.1526
2026-01-11 21:05:54,466: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:05:54,466: t15.2023.11.17 val PER: 0.0202
2026-01-11 21:05:54,467: t15.2023.11.19 val PER: 0.0120
2026-01-11 21:05:54,467: t15.2023.11.26 val PER: 0.0406
2026-01-11 21:05:54,467: t15.2023.12.03 val PER: 0.0452
2026-01-11 21:05:54,467: t15.2023.12.08 val PER: 0.0340
2026-01-11 21:05:54,467: t15.2023.12.10 val PER: 0.0210
2026-01-11 21:05:54,467: t15.2023.12.17 val PER: 0.0748
2026-01-11 21:05:54,467: t15.2023.12.29 val PER: 0.0645
2026-01-11 21:05:54,467: t15.2024.02.25 val PER: 0.0772
2026-01-11 21:05:54,467: t15.2024.03.08 val PER: 0.1650
2026-01-11 21:05:54,467: t15.2024.03.15 val PER: 0.1620
2026-01-11 21:05:54,467: t15.2024.03.17 val PER: 0.0802
2026-01-11 21:05:54,467: t15.2024.05.10 val PER: 0.1174
2026-01-11 21:05:54,468: t15.2024.06.14 val PER: 0.1136
2026-01-11 21:05:54,468: t15.2024.07.19 val PER: 0.1648
2026-01-11 21:05:54,468: t15.2024.07.21 val PER: 0.0483
2026-01-11 21:05:54,468: t15.2024.07.28 val PER: 0.0809
2026-01-11 21:05:54,468: t15.2025.01.10 val PER: 0.2163
2026-01-11 21:05:54,468: t15.2025.01.12 val PER: 0.0885
2026-01-11 21:05:54,468: t15.2025.03.14 val PER: 0.2751
2026-01-11 21:05:54,468: t15.2025.03.16 val PER: 0.1126
2026-01-11 21:05:54,468: t15.2025.03.30 val PER: 0.2092
2026-01-11 21:05:54,468: t15.2025.04.13 val PER: 0.1897
2026-01-11 21:05:54,629: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_89000
2026-01-11 21:06:14,966: Train batch 89200: loss: 0.11 grad norm: 8.02 time: 0.079
2026-01-11 21:06:35,670: Train batch 89400: loss: 0.21 grad norm: 18.69 time: 0.102
2026-01-11 21:06:45,884: Running test after training batch: 89500
2026-01-11 21:06:45,999: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:06:52,281: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 21:06:52,380: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:07:12,055: Val batch 89500: PER (avg): 0.1002 CTC Loss (avg): 34.5850 WER(5gram): 10.82% (n=256) time: 26.170
2026-01-11 21:07:12,057: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 21:07:12,057: t15.2023.08.13 val PER: 0.0738
2026-01-11 21:07:12,057: t15.2023.08.18 val PER: 0.0629
2026-01-11 21:07:12,057: t15.2023.08.20 val PER: 0.0659
2026-01-11 21:07:12,057: t15.2023.08.25 val PER: 0.0648
2026-01-11 21:07:12,057: t15.2023.08.27 val PER: 0.1527
2026-01-11 21:07:12,058: t15.2023.09.01 val PER: 0.0349
2026-01-11 21:07:12,058: t15.2023.09.03 val PER: 0.0986
2026-01-11 21:07:12,058: t15.2023.09.24 val PER: 0.0777
2026-01-11 21:07:12,058: t15.2023.09.29 val PER: 0.0989
2026-01-11 21:07:12,058: t15.2023.10.01 val PER: 0.1341
2026-01-11 21:07:12,058: t15.2023.10.06 val PER: 0.0538
2026-01-11 21:07:12,058: t15.2023.10.08 val PER: 0.1976
2026-01-11 21:07:12,058: t15.2023.10.13 val PER: 0.1513
2026-01-11 21:07:12,058: t15.2023.10.15 val PER: 0.1035
2026-01-11 21:07:12,058: t15.2023.10.20 val PER: 0.1510
2026-01-11 21:07:12,059: t15.2023.10.22 val PER: 0.0835
2026-01-11 21:07:12,059: t15.2023.11.03 val PER: 0.1520
2026-01-11 21:07:12,059: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:07:12,059: t15.2023.11.17 val PER: 0.0187
2026-01-11 21:07:12,059: t15.2023.11.19 val PER: 0.0120
2026-01-11 21:07:12,059: t15.2023.11.26 val PER: 0.0399
2026-01-11 21:07:12,059: t15.2023.12.03 val PER: 0.0441
2026-01-11 21:07:12,059: t15.2023.12.08 val PER: 0.0333
2026-01-11 21:07:12,059: t15.2023.12.10 val PER: 0.0223
2026-01-11 21:07:12,060: t15.2023.12.17 val PER: 0.0800
2026-01-11 21:07:12,060: t15.2023.12.29 val PER: 0.0618
2026-01-11 21:07:12,060: t15.2024.02.25 val PER: 0.0702
2026-01-11 21:07:12,060: t15.2024.03.08 val PER: 0.1622
2026-01-11 21:07:12,060: t15.2024.03.15 val PER: 0.1632
2026-01-11 21:07:12,060: t15.2024.03.17 val PER: 0.0823
2026-01-11 21:07:12,060: t15.2024.05.10 val PER: 0.1144
2026-01-11 21:07:12,060: t15.2024.06.14 val PER: 0.1136
2026-01-11 21:07:12,060: t15.2024.07.19 val PER: 0.1608
2026-01-11 21:07:12,060: t15.2024.07.21 val PER: 0.0545
2026-01-11 21:07:12,060: t15.2024.07.28 val PER: 0.0809
2026-01-11 21:07:12,060: t15.2025.01.10 val PER: 0.2300
2026-01-11 21:07:12,060: t15.2025.01.12 val PER: 0.0839
2026-01-11 21:07:12,061: t15.2025.03.14 val PER: 0.2811
2026-01-11 21:07:12,061: t15.2025.03.16 val PER: 0.1178
2026-01-11 21:07:12,061: t15.2025.03.30 val PER: 0.2069
2026-01-11 21:07:12,061: t15.2025.04.13 val PER: 0.1840
2026-01-11 21:07:12,220: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_89500
2026-01-11 21:07:22,222: Train batch 89600: loss: 0.44 grad norm: 23.96 time: 0.086
2026-01-11 21:07:42,144: Train batch 89800: loss: 0.27 grad norm: 19.18 time: 0.091
2026-01-11 21:08:02,491: Train batch 90000: loss: 0.30 grad norm: 26.47 time: 0.075
2026-01-11 21:08:02,491: Running test after training batch: 90000
2026-01-11 21:08:02,863: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:08:11,771: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 21:08:11,916: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:08:28,957: Val batch 90000: PER (avg): 0.1006 CTC Loss (avg): 34.7846 WER(5gram): 11.34% (n=256) time: 26.465
2026-01-11 21:08:28,959: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 21:08:28,960: t15.2023.08.13 val PER: 0.0696
2026-01-11 21:08:28,960: t15.2023.08.18 val PER: 0.0645
2026-01-11 21:08:28,960: t15.2023.08.20 val PER: 0.0643
2026-01-11 21:08:28,960: t15.2023.08.25 val PER: 0.0663
2026-01-11 21:08:28,960: t15.2023.08.27 val PER: 0.1543
2026-01-11 21:08:28,960: t15.2023.09.01 val PER: 0.0357
2026-01-11 21:08:28,961: t15.2023.09.03 val PER: 0.1081
2026-01-11 21:08:28,961: t15.2023.09.24 val PER: 0.0777
2026-01-11 21:08:28,961: t15.2023.09.29 val PER: 0.1008
2026-01-11 21:08:28,961: t15.2023.10.01 val PER: 0.1347
2026-01-11 21:08:28,961: t15.2023.10.06 val PER: 0.0527
2026-01-11 21:08:28,961: t15.2023.10.08 val PER: 0.2003
2026-01-11 21:08:28,961: t15.2023.10.13 val PER: 0.1521
2026-01-11 21:08:28,961: t15.2023.10.15 val PER: 0.1009
2026-01-11 21:08:28,961: t15.2023.10.20 val PER: 0.1477
2026-01-11 21:08:28,961: t15.2023.10.22 val PER: 0.0869
2026-01-11 21:08:28,961: t15.2023.11.03 val PER: 0.1513
2026-01-11 21:08:28,962: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:08:28,962: t15.2023.11.17 val PER: 0.0187
2026-01-11 21:08:28,962: t15.2023.11.19 val PER: 0.0100
2026-01-11 21:08:28,962: t15.2023.11.26 val PER: 0.0399
2026-01-11 21:08:28,962: t15.2023.12.03 val PER: 0.0462
2026-01-11 21:08:28,962: t15.2023.12.08 val PER: 0.0300
2026-01-11 21:08:28,962: t15.2023.12.10 val PER: 0.0263
2026-01-11 21:08:28,962: t15.2023.12.17 val PER: 0.0790
2026-01-11 21:08:28,962: t15.2023.12.29 val PER: 0.0625
2026-01-11 21:08:28,962: t15.2024.02.25 val PER: 0.0716
2026-01-11 21:08:28,962: t15.2024.03.08 val PER: 0.1664
2026-01-11 21:08:28,963: t15.2024.03.15 val PER: 0.1607
2026-01-11 21:08:28,963: t15.2024.03.17 val PER: 0.0858
2026-01-11 21:08:28,963: t15.2024.05.10 val PER: 0.1174
2026-01-11 21:08:28,963: t15.2024.06.14 val PER: 0.1199
2026-01-11 21:08:28,963: t15.2024.07.19 val PER: 0.1595
2026-01-11 21:08:28,963: t15.2024.07.21 val PER: 0.0517
2026-01-11 21:08:28,963: t15.2024.07.28 val PER: 0.0868
2026-01-11 21:08:28,963: t15.2025.01.10 val PER: 0.2328
2026-01-11 21:08:28,963: t15.2025.01.12 val PER: 0.0847
2026-01-11 21:08:28,963: t15.2025.03.14 val PER: 0.2692
2026-01-11 21:08:28,963: t15.2025.03.16 val PER: 0.1152
2026-01-11 21:08:28,963: t15.2025.03.30 val PER: 0.2092
2026-01-11 21:08:28,963: t15.2025.04.13 val PER: 0.1812
2026-01-11 21:08:29,125: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_90000
2026-01-11 21:08:49,153: Train batch 90200: loss: 0.26 grad norm: 16.06 time: 0.069
2026-01-11 21:09:09,521: Train batch 90400: loss: 0.07 grad norm: 5.15 time: 0.081
2026-01-11 21:09:19,645: Running test after training batch: 90500
2026-01-11 21:09:19,807: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:09:26,325: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 21:09:26,409: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:09:43,258: Val batch 90500: PER (avg): 0.1005 CTC Loss (avg): 34.7087 WER(5gram): 11.73% (n=256) time: 23.612
2026-01-11 21:09:43,260: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 21:09:43,260: t15.2023.08.13 val PER: 0.0748
2026-01-11 21:09:43,261: t15.2023.08.18 val PER: 0.0654
2026-01-11 21:09:43,261: t15.2023.08.20 val PER: 0.0683
2026-01-11 21:09:43,261: t15.2023.08.25 val PER: 0.0663
2026-01-11 21:09:43,261: t15.2023.08.27 val PER: 0.1576
2026-01-11 21:09:43,261: t15.2023.09.01 val PER: 0.0365
2026-01-11 21:09:43,261: t15.2023.09.03 val PER: 0.1081
2026-01-11 21:09:43,261: t15.2023.09.24 val PER: 0.0777
2026-01-11 21:09:43,261: t15.2023.09.29 val PER: 0.0989
2026-01-11 21:09:43,261: t15.2023.10.01 val PER: 0.1347
2026-01-11 21:09:43,261: t15.2023.10.06 val PER: 0.0517
2026-01-11 21:09:43,261: t15.2023.10.08 val PER: 0.1989
2026-01-11 21:09:43,262: t15.2023.10.13 val PER: 0.1505
2026-01-11 21:09:43,262: t15.2023.10.15 val PER: 0.1022
2026-01-11 21:09:43,262: t15.2023.10.20 val PER: 0.1544
2026-01-11 21:09:43,262: t15.2023.10.22 val PER: 0.0846
2026-01-11 21:09:43,262: t15.2023.11.03 val PER: 0.1465
2026-01-11 21:09:43,262: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:09:43,262: t15.2023.11.17 val PER: 0.0171
2026-01-11 21:09:43,262: t15.2023.11.19 val PER: 0.0100
2026-01-11 21:09:43,263: t15.2023.11.26 val PER: 0.0413
2026-01-11 21:09:43,263: t15.2023.12.03 val PER: 0.0452
2026-01-11 21:09:43,263: t15.2023.12.08 val PER: 0.0320
2026-01-11 21:09:43,263: t15.2023.12.10 val PER: 0.0223
2026-01-11 21:09:43,263: t15.2023.12.17 val PER: 0.0790
2026-01-11 21:09:43,263: t15.2023.12.29 val PER: 0.0631
2026-01-11 21:09:43,263: t15.2024.02.25 val PER: 0.0688
2026-01-11 21:09:43,263: t15.2024.03.08 val PER: 0.1636
2026-01-11 21:09:43,263: t15.2024.03.15 val PER: 0.1601
2026-01-11 21:09:43,263: t15.2024.03.17 val PER: 0.0844
2026-01-11 21:09:43,263: t15.2024.05.10 val PER: 0.1218
2026-01-11 21:09:43,263: t15.2024.06.14 val PER: 0.1215
2026-01-11 21:09:43,263: t15.2024.07.19 val PER: 0.1608
2026-01-11 21:09:43,263: t15.2024.07.21 val PER: 0.0497
2026-01-11 21:09:43,264: t15.2024.07.28 val PER: 0.0831
2026-01-11 21:09:43,264: t15.2025.01.10 val PER: 0.2355
2026-01-11 21:09:43,264: t15.2025.01.12 val PER: 0.0855
2026-01-11 21:09:43,264: t15.2025.03.14 val PER: 0.2678
2026-01-11 21:09:43,264: t15.2025.03.16 val PER: 0.1191
2026-01-11 21:09:43,264: t15.2025.03.30 val PER: 0.2069
2026-01-11 21:09:43,264: t15.2025.04.13 val PER: 0.1812
2026-01-11 21:09:43,422: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_90500
2026-01-11 21:09:53,598: Train batch 90600: loss: 0.05 grad norm: 5.82 time: 0.098
2026-01-11 21:10:13,669: Train batch 90800: loss: 0.11 grad norm: 7.08 time: 0.065
2026-01-11 21:10:33,988: Train batch 91000: loss: 0.08 grad norm: 7.23 time: 0.077
2026-01-11 21:10:33,989: Running test after training batch: 91000
2026-01-11 21:10:34,109: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:10:40,434: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 21:10:40,521: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:10:57,718: Val batch 91000: PER (avg): 0.1005 CTC Loss (avg): 34.6518 WER(5gram): 11.41% (n=256) time: 23.728
2026-01-11 21:10:57,719: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 21:10:57,720: t15.2023.08.13 val PER: 0.0696
2026-01-11 21:10:57,720: t15.2023.08.18 val PER: 0.0637
2026-01-11 21:10:57,720: t15.2023.08.20 val PER: 0.0651
2026-01-11 21:10:57,720: t15.2023.08.25 val PER: 0.0663
2026-01-11 21:10:57,720: t15.2023.08.27 val PER: 0.1624
2026-01-11 21:10:57,720: t15.2023.09.01 val PER: 0.0365
2026-01-11 21:10:57,721: t15.2023.09.03 val PER: 0.1045
2026-01-11 21:10:57,721: t15.2023.09.24 val PER: 0.0789
2026-01-11 21:10:57,721: t15.2023.09.29 val PER: 0.0983
2026-01-11 21:10:57,721: t15.2023.10.01 val PER: 0.1308
2026-01-11 21:10:57,721: t15.2023.10.06 val PER: 0.0560
2026-01-11 21:10:57,721: t15.2023.10.08 val PER: 0.2030
2026-01-11 21:10:57,721: t15.2023.10.13 val PER: 0.1528
2026-01-11 21:10:57,721: t15.2023.10.15 val PER: 0.1042
2026-01-11 21:10:57,721: t15.2023.10.20 val PER: 0.1443
2026-01-11 21:10:57,721: t15.2023.10.22 val PER: 0.0857
2026-01-11 21:10:57,722: t15.2023.11.03 val PER: 0.1479
2026-01-11 21:10:57,722: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:10:57,722: t15.2023.11.17 val PER: 0.0187
2026-01-11 21:10:57,722: t15.2023.11.19 val PER: 0.0100
2026-01-11 21:10:57,722: t15.2023.11.26 val PER: 0.0413
2026-01-11 21:10:57,722: t15.2023.12.03 val PER: 0.0441
2026-01-11 21:10:57,722: t15.2023.12.08 val PER: 0.0300
2026-01-11 21:10:57,722: t15.2023.12.10 val PER: 0.0223
2026-01-11 21:10:57,722: t15.2023.12.17 val PER: 0.0811
2026-01-11 21:10:57,722: t15.2023.12.29 val PER: 0.0590
2026-01-11 21:10:57,722: t15.2024.02.25 val PER: 0.0716
2026-01-11 21:10:57,723: t15.2024.03.08 val PER: 0.1622
2026-01-11 21:10:57,723: t15.2024.03.15 val PER: 0.1582
2026-01-11 21:10:57,723: t15.2024.03.17 val PER: 0.0830
2026-01-11 21:10:57,723: t15.2024.05.10 val PER: 0.1218
2026-01-11 21:10:57,723: t15.2024.06.14 val PER: 0.1230
2026-01-11 21:10:57,723: t15.2024.07.19 val PER: 0.1707
2026-01-11 21:10:57,723: t15.2024.07.21 val PER: 0.0517
2026-01-11 21:10:57,723: t15.2024.07.28 val PER: 0.0831
2026-01-11 21:10:57,723: t15.2025.01.10 val PER: 0.2231
2026-01-11 21:10:57,723: t15.2025.01.12 val PER: 0.0831
2026-01-11 21:10:57,724: t15.2025.03.14 val PER: 0.2781
2026-01-11 21:10:57,724: t15.2025.03.16 val PER: 0.1217
2026-01-11 21:10:57,724: t15.2025.03.30 val PER: 0.2057
2026-01-11 21:10:57,724: t15.2025.04.13 val PER: 0.1826
2026-01-11 21:10:57,885: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_91000
2026-01-11 21:11:18,091: Train batch 91200: loss: 0.12 grad norm: 11.53 time: 0.084
2026-01-11 21:11:38,417: Train batch 91400: loss: 0.18 grad norm: 11.03 time: 0.117
2026-01-11 21:11:48,509: Running test after training batch: 91500
2026-01-11 21:11:48,659: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:11:55,018: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 21:11:55,119: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:12:12,029: Val batch 91500: PER (avg): 0.1000 CTC Loss (avg): 34.5726 WER(5gram): 11.54% (n=256) time: 23.519
2026-01-11 21:12:12,031: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 21:12:12,031: t15.2023.08.13 val PER: 0.0707
2026-01-11 21:12:12,031: t15.2023.08.18 val PER: 0.0645
2026-01-11 21:12:12,031: t15.2023.08.20 val PER: 0.0635
2026-01-11 21:12:12,031: t15.2023.08.25 val PER: 0.0678
2026-01-11 21:12:12,032: t15.2023.08.27 val PER: 0.1559
2026-01-11 21:12:12,032: t15.2023.09.01 val PER: 0.0349
2026-01-11 21:12:12,032: t15.2023.09.03 val PER: 0.1105
2026-01-11 21:12:12,032: t15.2023.09.24 val PER: 0.0765
2026-01-11 21:12:12,032: t15.2023.09.29 val PER: 0.0996
2026-01-11 21:12:12,032: t15.2023.10.01 val PER: 0.1301
2026-01-11 21:12:12,032: t15.2023.10.06 val PER: 0.0538
2026-01-11 21:12:12,032: t15.2023.10.08 val PER: 0.2016
2026-01-11 21:12:12,032: t15.2023.10.13 val PER: 0.1490
2026-01-11 21:12:12,032: t15.2023.10.15 val PER: 0.1042
2026-01-11 21:12:12,032: t15.2023.10.20 val PER: 0.1477
2026-01-11 21:12:12,032: t15.2023.10.22 val PER: 0.0846
2026-01-11 21:12:12,033: t15.2023.11.03 val PER: 0.1493
2026-01-11 21:12:12,033: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:12:12,033: t15.2023.11.17 val PER: 0.0202
2026-01-11 21:12:12,033: t15.2023.11.19 val PER: 0.0120
2026-01-11 21:12:12,033: t15.2023.11.26 val PER: 0.0399
2026-01-11 21:12:12,033: t15.2023.12.03 val PER: 0.0452
2026-01-11 21:12:12,033: t15.2023.12.08 val PER: 0.0306
2026-01-11 21:12:12,033: t15.2023.12.10 val PER: 0.0237
2026-01-11 21:12:12,033: t15.2023.12.17 val PER: 0.0811
2026-01-11 21:12:12,033: t15.2023.12.29 val PER: 0.0583
2026-01-11 21:12:12,033: t15.2024.02.25 val PER: 0.0702
2026-01-11 21:12:12,033: t15.2024.03.08 val PER: 0.1622
2026-01-11 21:12:12,034: t15.2024.03.15 val PER: 0.1626
2026-01-11 21:12:12,034: t15.2024.03.17 val PER: 0.0809
2026-01-11 21:12:12,034: t15.2024.05.10 val PER: 0.1189
2026-01-11 21:12:12,034: t15.2024.06.14 val PER: 0.1246
2026-01-11 21:12:12,034: t15.2024.07.19 val PER: 0.1635
2026-01-11 21:12:12,034: t15.2024.07.21 val PER: 0.0503
2026-01-11 21:12:12,034: t15.2024.07.28 val PER: 0.0838
2026-01-11 21:12:12,034: t15.2025.01.10 val PER: 0.2231
2026-01-11 21:12:12,034: t15.2025.01.12 val PER: 0.0862
2026-01-11 21:12:12,034: t15.2025.03.14 val PER: 0.2751
2026-01-11 21:12:12,034: t15.2025.03.16 val PER: 0.1152
2026-01-11 21:12:12,034: t15.2025.03.30 val PER: 0.2069
2026-01-11 21:12:12,034: t15.2025.04.13 val PER: 0.1783
2026-01-11 21:12:12,193: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_91500
2026-01-11 21:12:22,211: Train batch 91600: loss: 0.14 grad norm: 6.41 time: 0.093
2026-01-11 21:12:42,233: Train batch 91800: loss: 0.04 grad norm: 2.96 time: 0.073
2026-01-11 21:13:02,509: Train batch 92000: loss: 0.07 grad norm: 7.30 time: 0.082
2026-01-11 21:13:02,509: Running test after training batch: 92000
2026-01-11 21:13:02,633: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:13:08,841: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 21:13:08,931: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:13:25,890: Val batch 92000: PER (avg): 0.1006 CTC Loss (avg): 34.7176 WER(5gram): 11.41% (n=256) time: 23.381
2026-01-11 21:13:25,892: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 21:13:25,893: t15.2023.08.13 val PER: 0.0738
2026-01-11 21:13:25,893: t15.2023.08.18 val PER: 0.0654
2026-01-11 21:13:25,893: t15.2023.08.20 val PER: 0.0612
2026-01-11 21:13:25,893: t15.2023.08.25 val PER: 0.0648
2026-01-11 21:13:25,893: t15.2023.08.27 val PER: 0.1495
2026-01-11 21:13:25,893: t15.2023.09.01 val PER: 0.0390
2026-01-11 21:13:25,893: t15.2023.09.03 val PER: 0.1081
2026-01-11 21:13:25,893: t15.2023.09.24 val PER: 0.0752
2026-01-11 21:13:25,893: t15.2023.09.29 val PER: 0.1002
2026-01-11 21:13:25,893: t15.2023.10.01 val PER: 0.1347
2026-01-11 21:13:25,893: t15.2023.10.06 val PER: 0.0517
2026-01-11 21:13:25,894: t15.2023.10.08 val PER: 0.1949
2026-01-11 21:13:25,894: t15.2023.10.13 val PER: 0.1497
2026-01-11 21:13:25,894: t15.2023.10.15 val PER: 0.1061
2026-01-11 21:13:25,894: t15.2023.10.20 val PER: 0.1510
2026-01-11 21:13:25,894: t15.2023.10.22 val PER: 0.0891
2026-01-11 21:13:25,894: t15.2023.11.03 val PER: 0.1513
2026-01-11 21:13:25,894: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:13:25,894: t15.2023.11.17 val PER: 0.0202
2026-01-11 21:13:25,895: t15.2023.11.19 val PER: 0.0120
2026-01-11 21:13:25,895: t15.2023.11.26 val PER: 0.0406
2026-01-11 21:13:25,895: t15.2023.12.03 val PER: 0.0462
2026-01-11 21:13:25,895: t15.2023.12.08 val PER: 0.0326
2026-01-11 21:13:25,895: t15.2023.12.10 val PER: 0.0237
2026-01-11 21:13:25,895: t15.2023.12.17 val PER: 0.0800
2026-01-11 21:13:25,895: t15.2023.12.29 val PER: 0.0590
2026-01-11 21:13:25,895: t15.2024.02.25 val PER: 0.0730
2026-01-11 21:13:25,895: t15.2024.03.08 val PER: 0.1664
2026-01-11 21:13:25,895: t15.2024.03.15 val PER: 0.1588
2026-01-11 21:13:25,895: t15.2024.03.17 val PER: 0.0816
2026-01-11 21:13:25,895: t15.2024.05.10 val PER: 0.1159
2026-01-11 21:13:25,895: t15.2024.06.14 val PER: 0.1199
2026-01-11 21:13:25,895: t15.2024.07.19 val PER: 0.1641
2026-01-11 21:13:25,896: t15.2024.07.21 val PER: 0.0517
2026-01-11 21:13:25,896: t15.2024.07.28 val PER: 0.0838
2026-01-11 21:13:25,896: t15.2025.01.10 val PER: 0.2300
2026-01-11 21:13:25,896: t15.2025.01.12 val PER: 0.0855
2026-01-11 21:13:25,896: t15.2025.03.14 val PER: 0.2751
2026-01-11 21:13:25,896: t15.2025.03.16 val PER: 0.1217
2026-01-11 21:13:25,896: t15.2025.03.30 val PER: 0.2023
2026-01-11 21:13:25,896: t15.2025.04.13 val PER: 0.1840
2026-01-11 21:13:26,054: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_92000
2026-01-11 21:13:46,389: Train batch 92200: loss: 0.08 grad norm: 4.34 time: 0.098
2026-01-11 21:14:06,723: Train batch 92400: loss: 0.11 grad norm: 7.66 time: 0.106
2026-01-11 21:14:16,712: Running test after training batch: 92500
2026-01-11 21:14:17,077: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:14:23,459: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 21:14:23,549: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:14:42,726: Val batch 92500: PER (avg): 0.1006 CTC Loss (avg): 34.6984 WER(5gram): 11.41% (n=256) time: 26.013
2026-01-11 21:14:42,728: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-11 21:14:42,730: t15.2023.08.13 val PER: 0.0738
2026-01-11 21:14:42,731: t15.2023.08.18 val PER: 0.0645
2026-01-11 21:14:42,731: t15.2023.08.20 val PER: 0.0635
2026-01-11 21:14:42,731: t15.2023.08.25 val PER: 0.0678
2026-01-11 21:14:42,731: t15.2023.08.27 val PER: 0.1559
2026-01-11 21:14:42,731: t15.2023.09.01 val PER: 0.0357
2026-01-11 21:14:42,731: t15.2023.09.03 val PER: 0.1045
2026-01-11 21:14:42,731: t15.2023.09.24 val PER: 0.0801
2026-01-11 21:14:42,731: t15.2023.09.29 val PER: 0.1027
2026-01-11 21:14:42,731: t15.2023.10.01 val PER: 0.1321
2026-01-11 21:14:42,732: t15.2023.10.06 val PER: 0.0506
2026-01-11 21:14:42,732: t15.2023.10.08 val PER: 0.2070
2026-01-11 21:14:42,732: t15.2023.10.13 val PER: 0.1536
2026-01-11 21:14:42,732: t15.2023.10.15 val PER: 0.1061
2026-01-11 21:14:42,732: t15.2023.10.20 val PER: 0.1544
2026-01-11 21:14:42,732: t15.2023.10.22 val PER: 0.0857
2026-01-11 21:14:42,732: t15.2023.11.03 val PER: 0.1499
2026-01-11 21:14:42,732: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:14:42,732: t15.2023.11.17 val PER: 0.0202
2026-01-11 21:14:42,732: t15.2023.11.19 val PER: 0.0120
2026-01-11 21:14:42,733: t15.2023.11.26 val PER: 0.0413
2026-01-11 21:14:42,733: t15.2023.12.03 val PER: 0.0441
2026-01-11 21:14:42,733: t15.2023.12.08 val PER: 0.0333
2026-01-11 21:14:42,733: t15.2023.12.10 val PER: 0.0250
2026-01-11 21:14:42,733: t15.2023.12.17 val PER: 0.0769
2026-01-11 21:14:42,733: t15.2023.12.29 val PER: 0.0611
2026-01-11 21:14:42,733: t15.2024.02.25 val PER: 0.0716
2026-01-11 21:14:42,733: t15.2024.03.08 val PER: 0.1593
2026-01-11 21:14:42,734: t15.2024.03.15 val PER: 0.1545
2026-01-11 21:14:42,734: t15.2024.03.17 val PER: 0.0816
2026-01-11 21:14:42,734: t15.2024.05.10 val PER: 0.1204
2026-01-11 21:14:42,734: t15.2024.06.14 val PER: 0.1230
2026-01-11 21:14:42,734: t15.2024.07.19 val PER: 0.1655
2026-01-11 21:14:42,734: t15.2024.07.21 val PER: 0.0503
2026-01-11 21:14:42,734: t15.2024.07.28 val PER: 0.0816
2026-01-11 21:14:42,734: t15.2025.01.10 val PER: 0.2314
2026-01-11 21:14:42,734: t15.2025.01.12 val PER: 0.0839
2026-01-11 21:14:42,734: t15.2025.03.14 val PER: 0.2722
2026-01-11 21:14:42,734: t15.2025.03.16 val PER: 0.1165
2026-01-11 21:14:42,734: t15.2025.03.30 val PER: 0.2046
2026-01-11 21:14:42,735: t15.2025.04.13 val PER: 0.1883
2026-01-11 21:14:42,923: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_92500
2026-01-11 21:14:53,000: Train batch 92600: loss: 1.49 grad norm: 19.78 time: 0.086
2026-01-11 21:15:13,295: Train batch 92800: loss: 0.19 grad norm: 18.25 time: 0.080
2026-01-11 21:15:33,713: Train batch 93000: loss: 0.07 grad norm: 3.68 time: 0.081
2026-01-11 21:15:33,714: Running test after training batch: 93000
2026-01-11 21:15:33,951: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:15:41,131: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 21:15:41,214: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:15:58,492: Val batch 93000: PER (avg): 0.1002 CTC Loss (avg): 34.5981 WER(5gram): 10.63% (n=256) time: 24.778
2026-01-11 21:15:58,494: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 21:15:58,494: t15.2023.08.13 val PER: 0.0728
2026-01-11 21:15:58,494: t15.2023.08.18 val PER: 0.0637
2026-01-11 21:15:58,494: t15.2023.08.20 val PER: 0.0635
2026-01-11 21:15:58,495: t15.2023.08.25 val PER: 0.0663
2026-01-11 21:15:58,495: t15.2023.08.27 val PER: 0.1543
2026-01-11 21:15:58,495: t15.2023.09.01 val PER: 0.0357
2026-01-11 21:15:58,495: t15.2023.09.03 val PER: 0.1081
2026-01-11 21:15:58,495: t15.2023.09.24 val PER: 0.0740
2026-01-11 21:15:58,495: t15.2023.09.29 val PER: 0.0996
2026-01-11 21:15:58,495: t15.2023.10.01 val PER: 0.1301
2026-01-11 21:15:58,495: t15.2023.10.06 val PER: 0.0474
2026-01-11 21:15:58,496: t15.2023.10.08 val PER: 0.2043
2026-01-11 21:15:58,496: t15.2023.10.13 val PER: 0.1513
2026-01-11 21:15:58,496: t15.2023.10.15 val PER: 0.1009
2026-01-11 21:15:58,496: t15.2023.10.20 val PER: 0.1510
2026-01-11 21:15:58,496: t15.2023.10.22 val PER: 0.0880
2026-01-11 21:15:58,496: t15.2023.11.03 val PER: 0.1520
2026-01-11 21:15:58,496: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:15:58,496: t15.2023.11.17 val PER: 0.0171
2026-01-11 21:15:58,496: t15.2023.11.19 val PER: 0.0100
2026-01-11 21:15:58,496: t15.2023.11.26 val PER: 0.0384
2026-01-11 21:15:58,497: t15.2023.12.03 val PER: 0.0452
2026-01-11 21:15:58,497: t15.2023.12.08 val PER: 0.0306
2026-01-11 21:15:58,497: t15.2023.12.10 val PER: 0.0237
2026-01-11 21:15:58,497: t15.2023.12.17 val PER: 0.0811
2026-01-11 21:15:58,497: t15.2023.12.29 val PER: 0.0625
2026-01-11 21:15:58,497: t15.2024.02.25 val PER: 0.0772
2026-01-11 21:15:58,497: t15.2024.03.08 val PER: 0.1636
2026-01-11 21:15:58,497: t15.2024.03.15 val PER: 0.1582
2026-01-11 21:15:58,497: t15.2024.03.17 val PER: 0.0802
2026-01-11 21:15:58,497: t15.2024.05.10 val PER: 0.1248
2026-01-11 21:15:58,497: t15.2024.06.14 val PER: 0.1136
2026-01-11 21:15:58,498: t15.2024.07.19 val PER: 0.1641
2026-01-11 21:15:58,498: t15.2024.07.21 val PER: 0.0497
2026-01-11 21:15:58,498: t15.2024.07.28 val PER: 0.0853
2026-01-11 21:15:58,498: t15.2025.01.10 val PER: 0.2259
2026-01-11 21:15:58,498: t15.2025.01.12 val PER: 0.0862
2026-01-11 21:15:58,498: t15.2025.03.14 val PER: 0.2751
2026-01-11 21:15:58,498: t15.2025.03.16 val PER: 0.1191
2026-01-11 21:15:58,498: t15.2025.03.30 val PER: 0.2092
2026-01-11 21:15:58,498: t15.2025.04.13 val PER: 0.1840
2026-01-11 21:15:58,655: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_93000
2026-01-11 21:16:18,599: Train batch 93200: loss: 0.03 grad norm: 3.20 time: 0.079
2026-01-11 21:16:39,032: Train batch 93400: loss: 0.16 grad norm: 10.69 time: 0.082
2026-01-11 21:16:49,145: Running test after training batch: 93500
2026-01-11 21:16:49,317: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:16:55,829: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 21:16:55,913: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:17:12,970: Val batch 93500: PER (avg): 0.1006 CTC Loss (avg): 34.7664 WER(5gram): 11.34% (n=256) time: 23.824
2026-01-11 21:17:12,972: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 21:17:12,972: t15.2023.08.13 val PER: 0.0696
2026-01-11 21:17:12,973: t15.2023.08.18 val PER: 0.0645
2026-01-11 21:17:12,973: t15.2023.08.20 val PER: 0.0643
2026-01-11 21:17:12,973: t15.2023.08.25 val PER: 0.0663
2026-01-11 21:17:12,973: t15.2023.08.27 val PER: 0.1624
2026-01-11 21:17:12,973: t15.2023.09.01 val PER: 0.0381
2026-01-11 21:17:12,973: t15.2023.09.03 val PER: 0.1081
2026-01-11 21:17:12,973: t15.2023.09.24 val PER: 0.0765
2026-01-11 21:17:12,973: t15.2023.09.29 val PER: 0.0983
2026-01-11 21:17:12,973: t15.2023.10.01 val PER: 0.1314
2026-01-11 21:17:12,973: t15.2023.10.06 val PER: 0.0538
2026-01-11 21:17:12,974: t15.2023.10.08 val PER: 0.2003
2026-01-11 21:17:12,974: t15.2023.10.13 val PER: 0.1559
2026-01-11 21:17:12,974: t15.2023.10.15 val PER: 0.1035
2026-01-11 21:17:12,974: t15.2023.10.20 val PER: 0.1510
2026-01-11 21:17:12,974: t15.2023.10.22 val PER: 0.0869
2026-01-11 21:17:12,974: t15.2023.11.03 val PER: 0.1479
2026-01-11 21:17:12,974: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:17:12,974: t15.2023.11.17 val PER: 0.0202
2026-01-11 21:17:12,974: t15.2023.11.19 val PER: 0.0100
2026-01-11 21:17:12,974: t15.2023.11.26 val PER: 0.0391
2026-01-11 21:17:12,974: t15.2023.12.03 val PER: 0.0473
2026-01-11 21:17:12,974: t15.2023.12.08 val PER: 0.0340
2026-01-11 21:17:12,975: t15.2023.12.10 val PER: 0.0250
2026-01-11 21:17:12,975: t15.2023.12.17 val PER: 0.0780
2026-01-11 21:17:12,975: t15.2023.12.29 val PER: 0.0618
2026-01-11 21:17:12,975: t15.2024.02.25 val PER: 0.0716
2026-01-11 21:17:12,975: t15.2024.03.08 val PER: 0.1622
2026-01-11 21:17:12,975: t15.2024.03.15 val PER: 0.1576
2026-01-11 21:17:12,975: t15.2024.03.17 val PER: 0.0816
2026-01-11 21:17:12,975: t15.2024.05.10 val PER: 0.1233
2026-01-11 21:17:12,975: t15.2024.06.14 val PER: 0.1183
2026-01-11 21:17:12,975: t15.2024.07.19 val PER: 0.1655
2026-01-11 21:17:12,975: t15.2024.07.21 val PER: 0.0510
2026-01-11 21:17:12,975: t15.2024.07.28 val PER: 0.0838
2026-01-11 21:17:12,976: t15.2025.01.10 val PER: 0.2231
2026-01-11 21:17:12,976: t15.2025.01.12 val PER: 0.0855
2026-01-11 21:17:12,976: t15.2025.03.14 val PER: 0.2781
2026-01-11 21:17:12,976: t15.2025.03.16 val PER: 0.1204
2026-01-11 21:17:12,976: t15.2025.03.30 val PER: 0.2057
2026-01-11 21:17:12,976: t15.2025.04.13 val PER: 0.1812
2026-01-11 21:17:13,137: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_93500
2026-01-11 21:17:23,108: Train batch 93600: loss: 0.08 grad norm: 6.03 time: 0.093
2026-01-11 21:17:43,402: Train batch 93800: loss: 0.13 grad norm: 18.09 time: 0.071
2026-01-11 21:18:03,657: Train batch 94000: loss: 0.50 grad norm: 27.07 time: 0.075
2026-01-11 21:18:03,657: Running test after training batch: 94000
2026-01-11 21:18:03,777: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:18:10,197: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 21:18:10,284: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:18:31,113: Val batch 94000: PER (avg): 0.1006 CTC Loss (avg): 34.6783 WER(5gram): 11.47% (n=256) time: 27.455
2026-01-11 21:18:31,115: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 21:18:31,116: t15.2023.08.13 val PER: 0.0738
2026-01-11 21:18:31,116: t15.2023.08.18 val PER: 0.0612
2026-01-11 21:18:31,116: t15.2023.08.20 val PER: 0.0620
2026-01-11 21:18:31,117: t15.2023.08.25 val PER: 0.0663
2026-01-11 21:18:31,117: t15.2023.08.27 val PER: 0.1559
2026-01-11 21:18:31,117: t15.2023.09.01 val PER: 0.0365
2026-01-11 21:18:31,117: t15.2023.09.03 val PER: 0.1057
2026-01-11 21:18:31,117: t15.2023.09.24 val PER: 0.0801
2026-01-11 21:18:31,117: t15.2023.09.29 val PER: 0.1008
2026-01-11 21:18:31,118: t15.2023.10.01 val PER: 0.1347
2026-01-11 21:18:31,118: t15.2023.10.06 val PER: 0.0506
2026-01-11 21:18:31,118: t15.2023.10.08 val PER: 0.1962
2026-01-11 21:18:31,118: t15.2023.10.13 val PER: 0.1552
2026-01-11 21:18:31,118: t15.2023.10.15 val PER: 0.0995
2026-01-11 21:18:31,119: t15.2023.10.20 val PER: 0.1477
2026-01-11 21:18:31,119: t15.2023.10.22 val PER: 0.0857
2026-01-11 21:18:31,119: t15.2023.11.03 val PER: 0.1540
2026-01-11 21:18:31,119: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:18:31,119: t15.2023.11.17 val PER: 0.0202
2026-01-11 21:18:31,119: t15.2023.11.19 val PER: 0.0100
2026-01-11 21:18:31,119: t15.2023.11.26 val PER: 0.0399
2026-01-11 21:18:31,119: t15.2023.12.03 val PER: 0.0462
2026-01-11 21:18:31,119: t15.2023.12.08 val PER: 0.0326
2026-01-11 21:18:31,119: t15.2023.12.10 val PER: 0.0250
2026-01-11 21:18:31,119: t15.2023.12.17 val PER: 0.0769
2026-01-11 21:18:31,119: t15.2023.12.29 val PER: 0.0645
2026-01-11 21:18:31,120: t15.2024.02.25 val PER: 0.0730
2026-01-11 21:18:31,120: t15.2024.03.08 val PER: 0.1636
2026-01-11 21:18:31,120: t15.2024.03.15 val PER: 0.1620
2026-01-11 21:18:31,120: t15.2024.03.17 val PER: 0.0795
2026-01-11 21:18:31,120: t15.2024.05.10 val PER: 0.1189
2026-01-11 21:18:31,120: t15.2024.06.14 val PER: 0.1104
2026-01-11 21:18:31,120: t15.2024.07.19 val PER: 0.1622
2026-01-11 21:18:31,120: t15.2024.07.21 val PER: 0.0524
2026-01-11 21:18:31,120: t15.2024.07.28 val PER: 0.0838
2026-01-11 21:18:31,120: t15.2025.01.10 val PER: 0.2273
2026-01-11 21:18:31,120: t15.2025.01.12 val PER: 0.0870
2026-01-11 21:18:31,120: t15.2025.03.14 val PER: 0.2766
2026-01-11 21:18:31,120: t15.2025.03.16 val PER: 0.1217
2026-01-11 21:18:31,121: t15.2025.03.30 val PER: 0.2115
2026-01-11 21:18:31,121: t15.2025.04.13 val PER: 0.1826
2026-01-11 21:18:31,312: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_94000
2026-01-11 21:18:51,464: Train batch 94200: loss: 0.15 grad norm: 10.47 time: 0.091
2026-01-11 21:19:11,970: Train batch 94400: loss: 0.03 grad norm: 3.01 time: 0.074
2026-01-11 21:19:22,159: Running test after training batch: 94500
2026-01-11 21:19:22,339: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:19:28,592: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 21:19:28,687: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:19:44,831: Val batch 94500: PER (avg): 0.1003 CTC Loss (avg): 34.7458 WER(5gram): 11.34% (n=256) time: 22.672
2026-01-11 21:19:44,833: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 21:19:44,834: t15.2023.08.13 val PER: 0.0717
2026-01-11 21:19:44,834: t15.2023.08.18 val PER: 0.0629
2026-01-11 21:19:44,834: t15.2023.08.20 val PER: 0.0620
2026-01-11 21:19:44,834: t15.2023.08.25 val PER: 0.0663
2026-01-11 21:19:44,834: t15.2023.08.27 val PER: 0.1463
2026-01-11 21:19:44,834: t15.2023.09.01 val PER: 0.0349
2026-01-11 21:19:44,834: t15.2023.09.03 val PER: 0.1116
2026-01-11 21:19:44,834: t15.2023.09.24 val PER: 0.0728
2026-01-11 21:19:44,834: t15.2023.09.29 val PER: 0.1008
2026-01-11 21:19:44,834: t15.2023.10.01 val PER: 0.1308
2026-01-11 21:19:44,835: t15.2023.10.06 val PER: 0.0538
2026-01-11 21:19:44,835: t15.2023.10.08 val PER: 0.1935
2026-01-11 21:19:44,835: t15.2023.10.13 val PER: 0.1528
2026-01-11 21:19:44,835: t15.2023.10.15 val PER: 0.1048
2026-01-11 21:19:44,835: t15.2023.10.20 val PER: 0.1510
2026-01-11 21:19:44,835: t15.2023.10.22 val PER: 0.0891
2026-01-11 21:19:44,835: t15.2023.11.03 val PER: 0.1499
2026-01-11 21:19:44,835: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:19:44,835: t15.2023.11.17 val PER: 0.0202
2026-01-11 21:19:44,835: t15.2023.11.19 val PER: 0.0100
2026-01-11 21:19:44,835: t15.2023.11.26 val PER: 0.0391
2026-01-11 21:19:44,836: t15.2023.12.03 val PER: 0.0452
2026-01-11 21:19:44,836: t15.2023.12.08 val PER: 0.0320
2026-01-11 21:19:44,836: t15.2023.12.10 val PER: 0.0250
2026-01-11 21:19:44,836: t15.2023.12.17 val PER: 0.0780
2026-01-11 21:19:44,836: t15.2023.12.29 val PER: 0.0604
2026-01-11 21:19:44,836: t15.2024.02.25 val PER: 0.0744
2026-01-11 21:19:44,836: t15.2024.03.08 val PER: 0.1593
2026-01-11 21:19:44,836: t15.2024.03.15 val PER: 0.1626
2026-01-11 21:19:44,836: t15.2024.03.17 val PER: 0.0823
2026-01-11 21:19:44,836: t15.2024.05.10 val PER: 0.1218
2026-01-11 21:19:44,836: t15.2024.06.14 val PER: 0.1120
2026-01-11 21:19:44,836: t15.2024.07.19 val PER: 0.1648
2026-01-11 21:19:44,836: t15.2024.07.21 val PER: 0.0545
2026-01-11 21:19:44,837: t15.2024.07.28 val PER: 0.0860
2026-01-11 21:19:44,837: t15.2025.01.10 val PER: 0.2314
2026-01-11 21:19:44,837: t15.2025.01.12 val PER: 0.0847
2026-01-11 21:19:44,837: t15.2025.03.14 val PER: 0.2751
2026-01-11 21:19:44,837: t15.2025.03.16 val PER: 0.1178
2026-01-11 21:19:44,837: t15.2025.03.30 val PER: 0.2034
2026-01-11 21:19:44,837: t15.2025.04.13 val PER: 0.1812
2026-01-11 21:19:44,995: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_94500
2026-01-11 21:19:54,955: Train batch 94600: loss: 0.09 grad norm: 8.49 time: 0.100
2026-01-11 21:20:14,846: Train batch 94800: loss: 0.19 grad norm: 13.88 time: 0.064
2026-01-11 21:20:35,138: Train batch 95000: loss: 0.07 grad norm: 7.64 time: 0.070
2026-01-11 21:20:35,138: Running test after training batch: 95000
2026-01-11 21:20:35,328: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:20:41,693: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 21:20:41,784: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:20:58,216: Val batch 95000: PER (avg): 0.1006 CTC Loss (avg): 34.7586 WER(5gram): 11.34% (n=256) time: 23.078
2026-01-11 21:20:58,218: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 21:20:58,218: t15.2023.08.13 val PER: 0.0707
2026-01-11 21:20:58,218: t15.2023.08.18 val PER: 0.0654
2026-01-11 21:20:58,219: t15.2023.08.20 val PER: 0.0651
2026-01-11 21:20:58,219: t15.2023.08.25 val PER: 0.0648
2026-01-11 21:20:58,219: t15.2023.08.27 val PER: 0.1495
2026-01-11 21:20:58,219: t15.2023.09.01 val PER: 0.0365
2026-01-11 21:20:58,219: t15.2023.09.03 val PER: 0.1105
2026-01-11 21:20:58,219: t15.2023.09.24 val PER: 0.0789
2026-01-11 21:20:58,219: t15.2023.09.29 val PER: 0.0989
2026-01-11 21:20:58,219: t15.2023.10.01 val PER: 0.1328
2026-01-11 21:20:58,219: t15.2023.10.06 val PER: 0.0484
2026-01-11 21:20:58,220: t15.2023.10.08 val PER: 0.1922
2026-01-11 21:20:58,220: t15.2023.10.13 val PER: 0.1536
2026-01-11 21:20:58,220: t15.2023.10.15 val PER: 0.1055
2026-01-11 21:20:58,220: t15.2023.10.20 val PER: 0.1409
2026-01-11 21:20:58,220: t15.2023.10.22 val PER: 0.0869
2026-01-11 21:20:58,220: t15.2023.11.03 val PER: 0.1540
2026-01-11 21:20:58,220: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:20:58,220: t15.2023.11.17 val PER: 0.0187
2026-01-11 21:20:58,220: t15.2023.11.19 val PER: 0.0100
2026-01-11 21:20:58,220: t15.2023.11.26 val PER: 0.0413
2026-01-11 21:20:58,220: t15.2023.12.03 val PER: 0.0473
2026-01-11 21:20:58,220: t15.2023.12.08 val PER: 0.0320
2026-01-11 21:20:58,220: t15.2023.12.10 val PER: 0.0250
2026-01-11 21:20:58,221: t15.2023.12.17 val PER: 0.0790
2026-01-11 21:20:58,221: t15.2023.12.29 val PER: 0.0570
2026-01-11 21:20:58,221: t15.2024.02.25 val PER: 0.0744
2026-01-11 21:20:58,221: t15.2024.03.08 val PER: 0.1636
2026-01-11 21:20:58,221: t15.2024.03.15 val PER: 0.1588
2026-01-11 21:20:58,221: t15.2024.03.17 val PER: 0.0816
2026-01-11 21:20:58,221: t15.2024.05.10 val PER: 0.1233
2026-01-11 21:20:58,221: t15.2024.06.14 val PER: 0.1151
2026-01-11 21:20:58,221: t15.2024.07.19 val PER: 0.1641
2026-01-11 21:20:58,221: t15.2024.07.21 val PER: 0.0531
2026-01-11 21:20:58,221: t15.2024.07.28 val PER: 0.0816
2026-01-11 21:20:58,221: t15.2025.01.10 val PER: 0.2300
2026-01-11 21:20:58,221: t15.2025.01.12 val PER: 0.0885
2026-01-11 21:20:58,222: t15.2025.03.14 val PER: 0.2766
2026-01-11 21:20:58,222: t15.2025.03.16 val PER: 0.1217
2026-01-11 21:20:58,222: t15.2025.03.30 val PER: 0.2046
2026-01-11 21:20:58,222: t15.2025.04.13 val PER: 0.1897
2026-01-11 21:20:58,403: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_95000
2026-01-11 21:21:18,526: Train batch 95200: loss: 0.14 grad norm: 10.45 time: 0.081
2026-01-11 21:21:38,985: Train batch 95400: loss: 0.02 grad norm: 1.47 time: 0.073
2026-01-11 21:21:49,055: Running test after training batch: 95500
2026-01-11 21:21:49,184: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:21:55,551: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 21:21:55,631: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:22:12,034: Val batch 95500: PER (avg): 0.1006 CTC Loss (avg): 34.7174 WER(5gram): 11.41% (n=256) time: 22.979
2026-01-11 21:22:12,036: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 21:22:12,036: t15.2023.08.13 val PER: 0.0728
2026-01-11 21:22:12,037: t15.2023.08.18 val PER: 0.0662
2026-01-11 21:22:12,037: t15.2023.08.20 val PER: 0.0651
2026-01-11 21:22:12,037: t15.2023.08.25 val PER: 0.0663
2026-01-11 21:22:12,037: t15.2023.08.27 val PER: 0.1511
2026-01-11 21:22:12,037: t15.2023.09.01 val PER: 0.0357
2026-01-11 21:22:12,037: t15.2023.09.03 val PER: 0.1128
2026-01-11 21:22:12,037: t15.2023.09.24 val PER: 0.0777
2026-01-11 21:22:12,037: t15.2023.09.29 val PER: 0.1002
2026-01-11 21:22:12,038: t15.2023.10.01 val PER: 0.1321
2026-01-11 21:22:12,038: t15.2023.10.06 val PER: 0.0549
2026-01-11 21:22:12,038: t15.2023.10.08 val PER: 0.1949
2026-01-11 21:22:12,038: t15.2023.10.13 val PER: 0.1521
2026-01-11 21:22:12,038: t15.2023.10.15 val PER: 0.1035
2026-01-11 21:22:12,038: t15.2023.10.20 val PER: 0.1477
2026-01-11 21:22:12,038: t15.2023.10.22 val PER: 0.0869
2026-01-11 21:22:12,038: t15.2023.11.03 val PER: 0.1526
2026-01-11 21:22:12,038: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:22:12,038: t15.2023.11.17 val PER: 0.0202
2026-01-11 21:22:12,039: t15.2023.11.19 val PER: 0.0100
2026-01-11 21:22:12,039: t15.2023.11.26 val PER: 0.0406
2026-01-11 21:22:12,039: t15.2023.12.03 val PER: 0.0483
2026-01-11 21:22:12,039: t15.2023.12.08 val PER: 0.0333
2026-01-11 21:22:12,039: t15.2023.12.10 val PER: 0.0250
2026-01-11 21:22:12,039: t15.2023.12.17 val PER: 0.0780
2026-01-11 21:22:12,039: t15.2023.12.29 val PER: 0.0583
2026-01-11 21:22:12,039: t15.2024.02.25 val PER: 0.0730
2026-01-11 21:22:12,039: t15.2024.03.08 val PER: 0.1650
2026-01-11 21:22:12,039: t15.2024.03.15 val PER: 0.1582
2026-01-11 21:22:12,040: t15.2024.03.17 val PER: 0.0816
2026-01-11 21:22:12,040: t15.2024.05.10 val PER: 0.1204
2026-01-11 21:22:12,040: t15.2024.06.14 val PER: 0.1136
2026-01-11 21:22:12,040: t15.2024.07.19 val PER: 0.1622
2026-01-11 21:22:12,040: t15.2024.07.21 val PER: 0.0545
2026-01-11 21:22:12,040: t15.2024.07.28 val PER: 0.0838
2026-01-11 21:22:12,040: t15.2025.01.10 val PER: 0.2287
2026-01-11 21:22:12,040: t15.2025.01.12 val PER: 0.0878
2026-01-11 21:22:12,040: t15.2025.03.14 val PER: 0.2722
2026-01-11 21:22:12,040: t15.2025.03.16 val PER: 0.1191
2026-01-11 21:22:12,041: t15.2025.03.30 val PER: 0.2057
2026-01-11 21:22:12,041: t15.2025.04.13 val PER: 0.1812
2026-01-11 21:22:12,215: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_95500
2026-01-11 21:22:22,366: Train batch 95600: loss: 0.24 grad norm: 21.48 time: 0.099
2026-01-11 21:22:42,535: Train batch 95800: loss: 0.13 grad norm: 10.64 time: 0.068
2026-01-11 21:23:02,903: Train batch 96000: loss: 0.14 grad norm: 12.45 time: 0.095
2026-01-11 21:23:02,904: Running test after training batch: 96000
2026-01-11 21:23:03,017: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:23:09,453: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 21:23:09,530: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:23:26,690: Val batch 96000: PER (avg): 0.1004 CTC Loss (avg): 34.6540 WER(5gram): 11.67% (n=256) time: 23.786
2026-01-11 21:23:26,694: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-11 21:23:26,696: t15.2023.08.13 val PER: 0.0696
2026-01-11 21:23:26,697: t15.2023.08.18 val PER: 0.0662
2026-01-11 21:23:26,697: t15.2023.08.20 val PER: 0.0612
2026-01-11 21:23:26,697: t15.2023.08.25 val PER: 0.0648
2026-01-11 21:23:26,697: t15.2023.08.27 val PER: 0.1559
2026-01-11 21:23:26,697: t15.2023.09.01 val PER: 0.0357
2026-01-11 21:23:26,697: t15.2023.09.03 val PER: 0.1093
2026-01-11 21:23:26,698: t15.2023.09.24 val PER: 0.0765
2026-01-11 21:23:26,698: t15.2023.09.29 val PER: 0.1008
2026-01-11 21:23:26,698: t15.2023.10.01 val PER: 0.1308
2026-01-11 21:23:26,698: t15.2023.10.06 val PER: 0.0560
2026-01-11 21:23:26,698: t15.2023.10.08 val PER: 0.1962
2026-01-11 21:23:26,698: t15.2023.10.13 val PER: 0.1552
2026-01-11 21:23:26,698: t15.2023.10.15 val PER: 0.1028
2026-01-11 21:23:26,698: t15.2023.10.20 val PER: 0.1544
2026-01-11 21:23:26,699: t15.2023.10.22 val PER: 0.0891
2026-01-11 21:23:26,699: t15.2023.11.03 val PER: 0.1499
2026-01-11 21:23:26,699: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:23:26,699: t15.2023.11.17 val PER: 0.0187
2026-01-11 21:23:26,699: t15.2023.11.19 val PER: 0.0100
2026-01-11 21:23:26,699: t15.2023.11.26 val PER: 0.0391
2026-01-11 21:23:26,699: t15.2023.12.03 val PER: 0.0452
2026-01-11 21:23:26,699: t15.2023.12.08 val PER: 0.0326
2026-01-11 21:23:26,699: t15.2023.12.10 val PER: 0.0250
2026-01-11 21:23:26,699: t15.2023.12.17 val PER: 0.0769
2026-01-11 21:23:26,700: t15.2023.12.29 val PER: 0.0590
2026-01-11 21:23:26,700: t15.2024.02.25 val PER: 0.0744
2026-01-11 21:23:26,700: t15.2024.03.08 val PER: 0.1664
2026-01-11 21:23:26,700: t15.2024.03.15 val PER: 0.1588
2026-01-11 21:23:26,700: t15.2024.03.17 val PER: 0.0830
2026-01-11 21:23:26,700: t15.2024.05.10 val PER: 0.1233
2026-01-11 21:23:26,700: t15.2024.06.14 val PER: 0.1167
2026-01-11 21:23:26,700: t15.2024.07.19 val PER: 0.1635
2026-01-11 21:23:26,701: t15.2024.07.21 val PER: 0.0531
2026-01-11 21:23:26,701: t15.2024.07.28 val PER: 0.0816
2026-01-11 21:23:26,701: t15.2025.01.10 val PER: 0.2273
2026-01-11 21:23:26,701: t15.2025.01.12 val PER: 0.0908
2026-01-11 21:23:26,701: t15.2025.03.14 val PER: 0.2766
2026-01-11 21:23:26,701: t15.2025.03.16 val PER: 0.1099
2026-01-11 21:23:26,701: t15.2025.03.30 val PER: 0.2057
2026-01-11 21:23:26,701: t15.2025.04.13 val PER: 0.1797
2026-01-11 21:23:26,916: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_96000
2026-01-11 21:23:46,836: Train batch 96200: loss: 0.14 grad norm: 11.92 time: 0.063
2026-01-11 21:24:07,011: Train batch 96400: loss: 0.05 grad norm: 2.93 time: 0.085
2026-01-11 21:24:16,973: Running test after training batch: 96500
2026-01-11 21:24:17,224: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:24:23,584: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 21:24:23,709: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:24:42,549: Val batch 96500: PER (avg): 0.1006 CTC Loss (avg): 34.7213 WER(5gram): 11.21% (n=256) time: 25.575
2026-01-11 21:24:42,550: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 21:24:42,551: t15.2023.08.13 val PER: 0.0717
2026-01-11 21:24:42,552: t15.2023.08.18 val PER: 0.0662
2026-01-11 21:24:42,552: t15.2023.08.20 val PER: 0.0604
2026-01-11 21:24:42,552: t15.2023.08.25 val PER: 0.0648
2026-01-11 21:24:42,552: t15.2023.08.27 val PER: 0.1511
2026-01-11 21:24:42,552: t15.2023.09.01 val PER: 0.0349
2026-01-11 21:24:42,552: t15.2023.09.03 val PER: 0.1045
2026-01-11 21:24:42,552: t15.2023.09.24 val PER: 0.0765
2026-01-11 21:24:42,552: t15.2023.09.29 val PER: 0.1040
2026-01-11 21:24:42,552: t15.2023.10.01 val PER: 0.1295
2026-01-11 21:24:42,552: t15.2023.10.06 val PER: 0.0517
2026-01-11 21:24:42,552: t15.2023.10.08 val PER: 0.1962
2026-01-11 21:24:42,552: t15.2023.10.13 val PER: 0.1521
2026-01-11 21:24:42,553: t15.2023.10.15 val PER: 0.1028
2026-01-11 21:24:42,553: t15.2023.10.20 val PER: 0.1510
2026-01-11 21:24:42,553: t15.2023.10.22 val PER: 0.0891
2026-01-11 21:24:42,553: t15.2023.11.03 val PER: 0.1493
2026-01-11 21:24:42,553: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:24:42,553: t15.2023.11.17 val PER: 0.0202
2026-01-11 21:24:42,553: t15.2023.11.19 val PER: 0.0120
2026-01-11 21:24:42,553: t15.2023.11.26 val PER: 0.0391
2026-01-11 21:24:42,553: t15.2023.12.03 val PER: 0.0452
2026-01-11 21:24:42,553: t15.2023.12.08 val PER: 0.0346
2026-01-11 21:24:42,553: t15.2023.12.10 val PER: 0.0276
2026-01-11 21:24:42,554: t15.2023.12.17 val PER: 0.0790
2026-01-11 21:24:42,554: t15.2023.12.29 val PER: 0.0611
2026-01-11 21:24:42,554: t15.2024.02.25 val PER: 0.0744
2026-01-11 21:24:42,554: t15.2024.03.08 val PER: 0.1664
2026-01-11 21:24:42,554: t15.2024.03.15 val PER: 0.1570
2026-01-11 21:24:42,555: t15.2024.03.17 val PER: 0.0830
2026-01-11 21:24:42,555: t15.2024.05.10 val PER: 0.1218
2026-01-11 21:24:42,555: t15.2024.06.14 val PER: 0.1262
2026-01-11 21:24:42,555: t15.2024.07.19 val PER: 0.1595
2026-01-11 21:24:42,556: t15.2024.07.21 val PER: 0.0545
2026-01-11 21:24:42,556: t15.2024.07.28 val PER: 0.0838
2026-01-11 21:24:42,556: t15.2025.01.10 val PER: 0.2328
2026-01-11 21:24:42,556: t15.2025.01.12 val PER: 0.0901
2026-01-11 21:24:42,556: t15.2025.03.14 val PER: 0.2766
2026-01-11 21:24:42,556: t15.2025.03.16 val PER: 0.1139
2026-01-11 21:24:42,556: t15.2025.03.30 val PER: 0.2069
2026-01-11 21:24:42,556: t15.2025.04.13 val PER: 0.1812
2026-01-11 21:24:42,747: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_96500
2026-01-11 21:24:52,491: Train batch 96600: loss: 0.14 grad norm: 10.17 time: 0.098
2026-01-11 21:25:12,917: Train batch 96800: loss: 0.09 grad norm: 7.45 time: 0.088
2026-01-11 21:25:33,541: Train batch 97000: loss: 0.54 grad norm: 22.38 time: 0.075
2026-01-11 21:25:33,542: Running test after training batch: 97000
2026-01-11 21:25:33,675: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:25:40,293: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 21:25:40,380: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:25:57,098: Val batch 97000: PER (avg): 0.0999 CTC Loss (avg): 34.7887 WER(5gram): 11.15% (n=256) time: 23.555
2026-01-11 21:25:57,099: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 21:25:57,100: t15.2023.08.13 val PER: 0.0696
2026-01-11 21:25:57,101: t15.2023.08.18 val PER: 0.0645
2026-01-11 21:25:57,101: t15.2023.08.20 val PER: 0.0627
2026-01-11 21:25:57,101: t15.2023.08.25 val PER: 0.0648
2026-01-11 21:25:57,101: t15.2023.08.27 val PER: 0.1559
2026-01-11 21:25:57,101: t15.2023.09.01 val PER: 0.0341
2026-01-11 21:25:57,101: t15.2023.09.03 val PER: 0.1045
2026-01-11 21:25:57,101: t15.2023.09.24 val PER: 0.0777
2026-01-11 21:25:57,101: t15.2023.09.29 val PER: 0.1021
2026-01-11 21:25:57,101: t15.2023.10.01 val PER: 0.1295
2026-01-11 21:25:57,101: t15.2023.10.06 val PER: 0.0484
2026-01-11 21:25:57,102: t15.2023.10.08 val PER: 0.1976
2026-01-11 21:25:57,102: t15.2023.10.13 val PER: 0.1497
2026-01-11 21:25:57,102: t15.2023.10.15 val PER: 0.1022
2026-01-11 21:25:57,102: t15.2023.10.20 val PER: 0.1510
2026-01-11 21:25:57,102: t15.2023.10.22 val PER: 0.0880
2026-01-11 21:25:57,102: t15.2023.11.03 val PER: 0.1479
2026-01-11 21:25:57,102: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:25:57,102: t15.2023.11.17 val PER: 0.0171
2026-01-11 21:25:57,102: t15.2023.11.19 val PER: 0.0120
2026-01-11 21:25:57,102: t15.2023.11.26 val PER: 0.0384
2026-01-11 21:25:57,102: t15.2023.12.03 val PER: 0.0452
2026-01-11 21:25:57,102: t15.2023.12.08 val PER: 0.0340
2026-01-11 21:25:57,103: t15.2023.12.10 val PER: 0.0263
2026-01-11 21:25:57,103: t15.2023.12.17 val PER: 0.0800
2026-01-11 21:25:57,103: t15.2023.12.29 val PER: 0.0563
2026-01-11 21:25:57,103: t15.2024.02.25 val PER: 0.0716
2026-01-11 21:25:57,103: t15.2024.03.08 val PER: 0.1650
2026-01-11 21:25:57,103: t15.2024.03.15 val PER: 0.1576
2026-01-11 21:25:57,103: t15.2024.03.17 val PER: 0.0823
2026-01-11 21:25:57,103: t15.2024.05.10 val PER: 0.1144
2026-01-11 21:25:57,103: t15.2024.06.14 val PER: 0.1183
2026-01-11 21:25:57,103: t15.2024.07.19 val PER: 0.1635
2026-01-11 21:25:57,103: t15.2024.07.21 val PER: 0.0552
2026-01-11 21:25:57,103: t15.2024.07.28 val PER: 0.0824
2026-01-11 21:25:57,103: t15.2025.01.10 val PER: 0.2287
2026-01-11 21:25:57,104: t15.2025.01.12 val PER: 0.0862
2026-01-11 21:25:57,104: t15.2025.03.14 val PER: 0.2796
2026-01-11 21:25:57,104: t15.2025.03.16 val PER: 0.1152
2026-01-11 21:25:57,104: t15.2025.03.30 val PER: 0.2069
2026-01-11 21:25:57,104: t15.2025.04.13 val PER: 0.1912
2026-01-11 21:25:57,285: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_97000
2026-01-11 21:26:17,279: Train batch 97200: loss: 0.30 grad norm: 17.26 time: 0.080
2026-01-11 21:26:37,608: Train batch 97400: loss: 0.15 grad norm: 11.33 time: 0.094
2026-01-11 21:26:47,900: Running test after training batch: 97500
2026-01-11 21:26:48,068: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:26:55,357: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 21:26:55,451: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:27:11,870: Val batch 97500: PER (avg): 0.0999 CTC Loss (avg): 34.7936 WER(5gram): 11.34% (n=256) time: 23.969
2026-01-11 21:27:11,872: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 21:27:11,873: t15.2023.08.13 val PER: 0.0696
2026-01-11 21:27:11,873: t15.2023.08.18 val PER: 0.0620
2026-01-11 21:27:11,873: t15.2023.08.20 val PER: 0.0643
2026-01-11 21:27:11,873: t15.2023.08.25 val PER: 0.0663
2026-01-11 21:27:11,873: t15.2023.08.27 val PER: 0.1559
2026-01-11 21:27:11,873: t15.2023.09.01 val PER: 0.0373
2026-01-11 21:27:11,873: t15.2023.09.03 val PER: 0.1045
2026-01-11 21:27:11,874: t15.2023.09.24 val PER: 0.0777
2026-01-11 21:27:11,874: t15.2023.09.29 val PER: 0.0983
2026-01-11 21:27:11,874: t15.2023.10.01 val PER: 0.1295
2026-01-11 21:27:11,874: t15.2023.10.06 val PER: 0.0517
2026-01-11 21:27:11,874: t15.2023.10.08 val PER: 0.1962
2026-01-11 21:27:11,874: t15.2023.10.13 val PER: 0.1482
2026-01-11 21:27:11,874: t15.2023.10.15 val PER: 0.1015
2026-01-11 21:27:11,874: t15.2023.10.20 val PER: 0.1510
2026-01-11 21:27:11,874: t15.2023.10.22 val PER: 0.0891
2026-01-11 21:27:11,874: t15.2023.11.03 val PER: 0.1465
2026-01-11 21:27:11,875: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:27:11,875: t15.2023.11.17 val PER: 0.0187
2026-01-11 21:27:11,875: t15.2023.11.19 val PER: 0.0120
2026-01-11 21:27:11,875: t15.2023.11.26 val PER: 0.0406
2026-01-11 21:27:11,875: t15.2023.12.03 val PER: 0.0431
2026-01-11 21:27:11,875: t15.2023.12.08 val PER: 0.0306
2026-01-11 21:27:11,875: t15.2023.12.10 val PER: 0.0263
2026-01-11 21:27:11,875: t15.2023.12.17 val PER: 0.0769
2026-01-11 21:27:11,875: t15.2023.12.29 val PER: 0.0590
2026-01-11 21:27:11,875: t15.2024.02.25 val PER: 0.0702
2026-01-11 21:27:11,876: t15.2024.03.08 val PER: 0.1664
2026-01-11 21:27:11,876: t15.2024.03.15 val PER: 0.1607
2026-01-11 21:27:11,876: t15.2024.03.17 val PER: 0.0830
2026-01-11 21:27:11,876: t15.2024.05.10 val PER: 0.1174
2026-01-11 21:27:11,876: t15.2024.06.14 val PER: 0.1215
2026-01-11 21:27:11,876: t15.2024.07.19 val PER: 0.1628
2026-01-11 21:27:11,876: t15.2024.07.21 val PER: 0.0552
2026-01-11 21:27:11,876: t15.2024.07.28 val PER: 0.0846
2026-01-11 21:27:11,876: t15.2025.01.10 val PER: 0.2328
2026-01-11 21:27:11,876: t15.2025.01.12 val PER: 0.0870
2026-01-11 21:27:11,876: t15.2025.03.14 val PER: 0.2737
2026-01-11 21:27:11,877: t15.2025.03.16 val PER: 0.1165
2026-01-11 21:27:11,877: t15.2025.03.30 val PER: 0.2011
2026-01-11 21:27:11,877: t15.2025.04.13 val PER: 0.1883
2026-01-11 21:27:12,057: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_97500
2026-01-11 21:27:22,110: Train batch 97600: loss: 0.15 grad norm: 11.20 time: 0.076
2026-01-11 21:27:42,416: Train batch 97800: loss: 0.09 grad norm: 10.56 time: 0.063
2026-01-11 21:28:02,755: Train batch 98000: loss: 0.10 grad norm: 7.02 time: 0.059
2026-01-11 21:28:02,756: Running test after training batch: 98000
2026-01-11 21:28:02,879: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:28:09,208: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 21:28:09,296: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:28:26,003: Val batch 98000: PER (avg): 0.1000 CTC Loss (avg): 34.9270 WER(5gram): 11.08% (n=256) time: 23.247
2026-01-11 21:28:26,006: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 21:28:26,006: t15.2023.08.13 val PER: 0.0686
2026-01-11 21:28:26,006: t15.2023.08.18 val PER: 0.0612
2026-01-11 21:28:26,006: t15.2023.08.20 val PER: 0.0627
2026-01-11 21:28:26,007: t15.2023.08.25 val PER: 0.0663
2026-01-11 21:28:26,007: t15.2023.08.27 val PER: 0.1543
2026-01-11 21:28:26,007: t15.2023.09.01 val PER: 0.0365
2026-01-11 21:28:26,007: t15.2023.09.03 val PER: 0.1069
2026-01-11 21:28:26,007: t15.2023.09.24 val PER: 0.0752
2026-01-11 21:28:26,007: t15.2023.09.29 val PER: 0.1027
2026-01-11 21:28:26,007: t15.2023.10.01 val PER: 0.1295
2026-01-11 21:28:26,007: t15.2023.10.06 val PER: 0.0527
2026-01-11 21:28:26,008: t15.2023.10.08 val PER: 0.1976
2026-01-11 21:28:26,008: t15.2023.10.13 val PER: 0.1528
2026-01-11 21:28:26,008: t15.2023.10.15 val PER: 0.1022
2026-01-11 21:28:26,008: t15.2023.10.20 val PER: 0.1409
2026-01-11 21:28:26,008: t15.2023.10.22 val PER: 0.0869
2026-01-11 21:28:26,008: t15.2023.11.03 val PER: 0.1499
2026-01-11 21:28:26,008: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:28:26,008: t15.2023.11.17 val PER: 0.0187
2026-01-11 21:28:26,008: t15.2023.11.19 val PER: 0.0100
2026-01-11 21:28:26,008: t15.2023.11.26 val PER: 0.0384
2026-01-11 21:28:26,008: t15.2023.12.03 val PER: 0.0452
2026-01-11 21:28:26,009: t15.2023.12.08 val PER: 0.0313
2026-01-11 21:28:26,009: t15.2023.12.10 val PER: 0.0237
2026-01-11 21:28:26,009: t15.2023.12.17 val PER: 0.0800
2026-01-11 21:28:26,009: t15.2023.12.29 val PER: 0.0604
2026-01-11 21:28:26,009: t15.2024.02.25 val PER: 0.0716
2026-01-11 21:28:26,009: t15.2024.03.08 val PER: 0.1693
2026-01-11 21:28:26,009: t15.2024.03.15 val PER: 0.1582
2026-01-11 21:28:26,009: t15.2024.03.17 val PER: 0.0781
2026-01-11 21:28:26,009: t15.2024.05.10 val PER: 0.1174
2026-01-11 21:28:26,009: t15.2024.06.14 val PER: 0.1246
2026-01-11 21:28:26,010: t15.2024.07.19 val PER: 0.1608
2026-01-11 21:28:26,010: t15.2024.07.21 val PER: 0.0538
2026-01-11 21:28:26,010: t15.2024.07.28 val PER: 0.0846
2026-01-11 21:28:26,010: t15.2025.01.10 val PER: 0.2300
2026-01-11 21:28:26,010: t15.2025.01.12 val PER: 0.0855
2026-01-11 21:28:26,010: t15.2025.03.14 val PER: 0.2811
2026-01-11 21:28:26,010: t15.2025.03.16 val PER: 0.1178
2026-01-11 21:28:26,010: t15.2025.03.30 val PER: 0.2057
2026-01-11 21:28:26,010: t15.2025.04.13 val PER: 0.1812
2026-01-11 21:28:26,197: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_98000
2026-01-11 21:28:46,211: Train batch 98200: loss: 1.22 grad norm: 11.67 time: 0.086
2026-01-11 21:29:06,666: Train batch 98400: loss: 0.34 grad norm: 22.43 time: 0.076
2026-01-11 21:29:16,810: Running test after training batch: 98500
2026-01-11 21:29:16,931: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:29:23,324: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 21:29:23,408: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:29:39,920: Val batch 98500: PER (avg): 0.0996 CTC Loss (avg): 34.8446 WER(5gram): 10.89% (n=256) time: 23.109
2026-01-11 21:29:39,921: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 21:29:39,922: t15.2023.08.13 val PER: 0.0686
2026-01-11 21:29:39,923: t15.2023.08.18 val PER: 0.0620
2026-01-11 21:29:39,923: t15.2023.08.20 val PER: 0.0620
2026-01-11 21:29:39,923: t15.2023.08.25 val PER: 0.0633
2026-01-11 21:29:39,923: t15.2023.08.27 val PER: 0.1559
2026-01-11 21:29:39,923: t15.2023.09.01 val PER: 0.0333
2026-01-11 21:29:39,923: t15.2023.09.03 val PER: 0.1105
2026-01-11 21:29:39,923: t15.2023.09.24 val PER: 0.0752
2026-01-11 21:29:39,923: t15.2023.09.29 val PER: 0.0996
2026-01-11 21:29:39,923: t15.2023.10.01 val PER: 0.1301
2026-01-11 21:29:39,923: t15.2023.10.06 val PER: 0.0549
2026-01-11 21:29:39,923: t15.2023.10.08 val PER: 0.1922
2026-01-11 21:29:39,924: t15.2023.10.13 val PER: 0.1490
2026-01-11 21:29:39,924: t15.2023.10.15 val PER: 0.1035
2026-01-11 21:29:39,924: t15.2023.10.20 val PER: 0.1409
2026-01-11 21:29:39,924: t15.2023.10.22 val PER: 0.0891
2026-01-11 21:29:39,924: t15.2023.11.03 val PER: 0.1499
2026-01-11 21:29:39,924: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:29:39,924: t15.2023.11.17 val PER: 0.0187
2026-01-11 21:29:39,924: t15.2023.11.19 val PER: 0.0100
2026-01-11 21:29:39,924: t15.2023.11.26 val PER: 0.0377
2026-01-11 21:29:39,924: t15.2023.12.03 val PER: 0.0483
2026-01-11 21:29:39,924: t15.2023.12.08 val PER: 0.0306
2026-01-11 21:29:39,924: t15.2023.12.10 val PER: 0.0237
2026-01-11 21:29:39,924: t15.2023.12.17 val PER: 0.0769
2026-01-11 21:29:39,925: t15.2023.12.29 val PER: 0.0597
2026-01-11 21:29:39,925: t15.2024.02.25 val PER: 0.0702
2026-01-11 21:29:39,925: t15.2024.03.08 val PER: 0.1579
2026-01-11 21:29:39,925: t15.2024.03.15 val PER: 0.1570
2026-01-11 21:29:39,925: t15.2024.03.17 val PER: 0.0788
2026-01-11 21:29:39,925: t15.2024.05.10 val PER: 0.1204
2026-01-11 21:29:39,925: t15.2024.06.14 val PER: 0.1183
2026-01-11 21:29:39,925: t15.2024.07.19 val PER: 0.1694
2026-01-11 21:29:39,925: t15.2024.07.21 val PER: 0.0531
2026-01-11 21:29:39,925: t15.2024.07.28 val PER: 0.0846
2026-01-11 21:29:39,925: t15.2025.01.10 val PER: 0.2300
2026-01-11 21:29:39,925: t15.2025.01.12 val PER: 0.0847
2026-01-11 21:29:39,925: t15.2025.03.14 val PER: 0.2781
2026-01-11 21:29:39,925: t15.2025.03.16 val PER: 0.1165
2026-01-11 21:29:39,926: t15.2025.03.30 val PER: 0.2080
2026-01-11 21:29:39,926: t15.2025.04.13 val PER: 0.1797
2026-01-11 21:29:40,104: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_98500
2026-01-11 21:29:50,051: Train batch 98600: loss: 0.17 grad norm: 15.72 time: 0.101
2026-01-11 21:30:10,249: Train batch 98800: loss: 0.25 grad norm: 17.26 time: 0.075
2026-01-11 21:30:30,399: Train batch 99000: loss: 0.06 grad norm: 7.33 time: 0.074
2026-01-11 21:30:30,399: Running test after training batch: 99000
2026-01-11 21:30:30,548: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:30:36,923: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 21:30:37,015: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:30:53,744: Val batch 99000: PER (avg): 0.1006 CTC Loss (avg): 34.8639 WER(5gram): 11.02% (n=256) time: 23.344
2026-01-11 21:30:53,745: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-11 21:30:53,746: t15.2023.08.13 val PER: 0.0717
2026-01-11 21:30:53,746: t15.2023.08.18 val PER: 0.0637
2026-01-11 21:30:53,747: t15.2023.08.20 val PER: 0.0643
2026-01-11 21:30:53,747: t15.2023.08.25 val PER: 0.0633
2026-01-11 21:30:53,747: t15.2023.08.27 val PER: 0.1543
2026-01-11 21:30:53,747: t15.2023.09.01 val PER: 0.0357
2026-01-11 21:30:53,747: t15.2023.09.03 val PER: 0.1093
2026-01-11 21:30:53,747: t15.2023.09.24 val PER: 0.0765
2026-01-11 21:30:53,747: t15.2023.09.29 val PER: 0.1002
2026-01-11 21:30:53,747: t15.2023.10.01 val PER: 0.1321
2026-01-11 21:30:53,747: t15.2023.10.06 val PER: 0.0549
2026-01-11 21:30:53,748: t15.2023.10.08 val PER: 0.1962
2026-01-11 21:30:53,748: t15.2023.10.13 val PER: 0.1505
2026-01-11 21:30:53,748: t15.2023.10.15 val PER: 0.1022
2026-01-11 21:30:53,748: t15.2023.10.20 val PER: 0.1443
2026-01-11 21:30:53,748: t15.2023.10.22 val PER: 0.0857
2026-01-11 21:30:53,748: t15.2023.11.03 val PER: 0.1526
2026-01-11 21:30:53,748: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:30:53,748: t15.2023.11.17 val PER: 0.0202
2026-01-11 21:30:53,748: t15.2023.11.19 val PER: 0.0100
2026-01-11 21:30:53,748: t15.2023.11.26 val PER: 0.0406
2026-01-11 21:30:53,748: t15.2023.12.03 val PER: 0.0473
2026-01-11 21:30:53,749: t15.2023.12.08 val PER: 0.0326
2026-01-11 21:30:53,749: t15.2023.12.10 val PER: 0.0237
2026-01-11 21:30:53,749: t15.2023.12.17 val PER: 0.0738
2026-01-11 21:30:53,749: t15.2023.12.29 val PER: 0.0604
2026-01-11 21:30:53,749: t15.2024.02.25 val PER: 0.0716
2026-01-11 21:30:53,749: t15.2024.03.08 val PER: 0.1679
2026-01-11 21:30:53,749: t15.2024.03.15 val PER: 0.1595
2026-01-11 21:30:53,749: t15.2024.03.17 val PER: 0.0795
2026-01-11 21:30:53,749: t15.2024.05.10 val PER: 0.1233
2026-01-11 21:30:53,749: t15.2024.06.14 val PER: 0.1183
2026-01-11 21:30:53,749: t15.2024.07.19 val PER: 0.1655
2026-01-11 21:30:53,749: t15.2024.07.21 val PER: 0.0531
2026-01-11 21:30:53,749: t15.2024.07.28 val PER: 0.0868
2026-01-11 21:30:53,750: t15.2025.01.10 val PER: 0.2342
2026-01-11 21:30:53,750: t15.2025.01.12 val PER: 0.0839
2026-01-11 21:30:53,750: t15.2025.03.14 val PER: 0.2751
2026-01-11 21:30:53,750: t15.2025.03.16 val PER: 0.1178
2026-01-11 21:30:53,750: t15.2025.03.30 val PER: 0.2046
2026-01-11 21:30:53,750: t15.2025.04.13 val PER: 0.1869
2026-01-11 21:30:53,928: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_99000
2026-01-11 21:31:13,942: Train batch 99200: loss: 0.39 grad norm: 22.27 time: 0.076
2026-01-11 21:31:34,310: Train batch 99400: loss: 0.10 grad norm: 9.50 time: 0.086
2026-01-11 21:31:44,509: Running test after training batch: 99500
2026-01-11 21:31:44,673: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:31:50,965: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 21:31:51,103: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:32:07,912: Val batch 99500: PER (avg): 0.1004 CTC Loss (avg): 35.0223 WER(5gram): 11.21% (n=256) time: 23.403
2026-01-11 21:32:07,914: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 21:32:07,915: t15.2023.08.13 val PER: 0.0717
2026-01-11 21:32:07,915: t15.2023.08.18 val PER: 0.0662
2026-01-11 21:32:07,915: t15.2023.08.20 val PER: 0.0627
2026-01-11 21:32:07,915: t15.2023.08.25 val PER: 0.0648
2026-01-11 21:32:07,915: t15.2023.08.27 val PER: 0.1576
2026-01-11 21:32:07,915: t15.2023.09.01 val PER: 0.0365
2026-01-11 21:32:07,915: t15.2023.09.03 val PER: 0.1069
2026-01-11 21:32:07,915: t15.2023.09.24 val PER: 0.0765
2026-01-11 21:32:07,915: t15.2023.09.29 val PER: 0.1002
2026-01-11 21:32:07,916: t15.2023.10.01 val PER: 0.1334
2026-01-11 21:32:07,916: t15.2023.10.06 val PER: 0.0549
2026-01-11 21:32:07,916: t15.2023.10.08 val PER: 0.1962
2026-01-11 21:32:07,916: t15.2023.10.13 val PER: 0.1497
2026-01-11 21:32:07,916: t15.2023.10.15 val PER: 0.1042
2026-01-11 21:32:07,916: t15.2023.10.20 val PER: 0.1443
2026-01-11 21:32:07,916: t15.2023.10.22 val PER: 0.0891
2026-01-11 21:32:07,916: t15.2023.11.03 val PER: 0.1520
2026-01-11 21:32:07,916: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:32:07,916: t15.2023.11.17 val PER: 0.0202
2026-01-11 21:32:07,917: t15.2023.11.19 val PER: 0.0120
2026-01-11 21:32:07,917: t15.2023.11.26 val PER: 0.0406
2026-01-11 21:32:07,917: t15.2023.12.03 val PER: 0.0473
2026-01-11 21:32:07,917: t15.2023.12.08 val PER: 0.0333
2026-01-11 21:32:07,917: t15.2023.12.10 val PER: 0.0237
2026-01-11 21:32:07,917: t15.2023.12.17 val PER: 0.0759
2026-01-11 21:32:07,917: t15.2023.12.29 val PER: 0.0604
2026-01-11 21:32:07,917: t15.2024.02.25 val PER: 0.0716
2026-01-11 21:32:07,917: t15.2024.03.08 val PER: 0.1664
2026-01-11 21:32:07,917: t15.2024.03.15 val PER: 0.1582
2026-01-11 21:32:07,917: t15.2024.03.17 val PER: 0.0788
2026-01-11 21:32:07,917: t15.2024.05.10 val PER: 0.1144
2026-01-11 21:32:07,917: t15.2024.06.14 val PER: 0.1167
2026-01-11 21:32:07,918: t15.2024.07.19 val PER: 0.1628
2026-01-11 21:32:07,918: t15.2024.07.21 val PER: 0.0531
2026-01-11 21:32:07,918: t15.2024.07.28 val PER: 0.0860
2026-01-11 21:32:07,918: t15.2025.01.10 val PER: 0.2300
2026-01-11 21:32:07,918: t15.2025.01.12 val PER: 0.0855
2026-01-11 21:32:07,918: t15.2025.03.14 val PER: 0.2751
2026-01-11 21:32:07,918: t15.2025.03.16 val PER: 0.1191
2026-01-11 21:32:07,918: t15.2025.03.30 val PER: 0.2057
2026-01-11 21:32:07,918: t15.2025.04.13 val PER: 0.1812
2026-01-11 21:32:08,095: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_99500
2026-01-11 21:32:17,971: Train batch 99600: loss: 0.21 grad norm: 15.17 time: 0.072
2026-01-11 21:32:38,239: Train batch 99800: loss: 0.02 grad norm: 1.06 time: 0.065
2026-01-11 21:32:58,156: Running test after training batch: 99999
2026-01-11 21:32:58,257: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:33:04,434: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as will
2026-01-11 21:33:04,527: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-11 21:33:21,232: Val batch 99999: PER (avg): 0.1010 CTC Loss (avg): 35.0506 WER(5gram): 11.02% (n=256) time: 23.076
2026-01-11 21:33:21,234: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-11 21:33:21,235: t15.2023.08.13 val PER: 0.0707
2026-01-11 21:33:21,236: t15.2023.08.18 val PER: 0.0671
2026-01-11 21:33:21,236: t15.2023.08.20 val PER: 0.0627
2026-01-11 21:33:21,236: t15.2023.08.25 val PER: 0.0648
2026-01-11 21:33:21,236: t15.2023.08.27 val PER: 0.1576
2026-01-11 21:33:21,236: t15.2023.09.01 val PER: 0.0365
2026-01-11 21:33:21,236: t15.2023.09.03 val PER: 0.1093
2026-01-11 21:33:21,237: t15.2023.09.24 val PER: 0.0765
2026-01-11 21:33:21,237: t15.2023.09.29 val PER: 0.1034
2026-01-11 21:33:21,237: t15.2023.10.01 val PER: 0.1328
2026-01-11 21:33:21,237: t15.2023.10.06 val PER: 0.0538
2026-01-11 21:33:21,237: t15.2023.10.08 val PER: 0.2003
2026-01-11 21:33:21,237: t15.2023.10.13 val PER: 0.1505
2026-01-11 21:33:21,237: t15.2023.10.15 val PER: 0.1009
2026-01-11 21:33:21,237: t15.2023.10.20 val PER: 0.1409
2026-01-11 21:33:21,237: t15.2023.10.22 val PER: 0.0891
2026-01-11 21:33:21,237: t15.2023.11.03 val PER: 0.1493
2026-01-11 21:33:21,238: t15.2023.11.04 val PER: 0.0034
2026-01-11 21:33:21,238: t15.2023.11.17 val PER: 0.0202
2026-01-11 21:33:21,238: t15.2023.11.19 val PER: 0.0100
2026-01-11 21:33:21,238: t15.2023.11.26 val PER: 0.0399
2026-01-11 21:33:21,238: t15.2023.12.03 val PER: 0.0473
2026-01-11 21:33:21,238: t15.2023.12.08 val PER: 0.0353
2026-01-11 21:33:21,238: t15.2023.12.10 val PER: 0.0250
2026-01-11 21:33:21,238: t15.2023.12.17 val PER: 0.0769
2026-01-11 21:33:21,238: t15.2023.12.29 val PER: 0.0611
2026-01-11 21:33:21,238: t15.2024.02.25 val PER: 0.0730
2026-01-11 21:33:21,238: t15.2024.03.08 val PER: 0.1636
2026-01-11 21:33:21,239: t15.2024.03.15 val PER: 0.1588
2026-01-11 21:33:21,239: t15.2024.03.17 val PER: 0.0781
2026-01-11 21:33:21,239: t15.2024.05.10 val PER: 0.1159
2026-01-11 21:33:21,239: t15.2024.06.14 val PER: 0.1199
2026-01-11 21:33:21,239: t15.2024.07.19 val PER: 0.1655
2026-01-11 21:33:21,239: t15.2024.07.21 val PER: 0.0545
2026-01-11 21:33:21,239: t15.2024.07.28 val PER: 0.0882
2026-01-11 21:33:21,239: t15.2025.01.10 val PER: 0.2342
2026-01-11 21:33:21,239: t15.2025.01.12 val PER: 0.0870
2026-01-11 21:33:21,239: t15.2025.03.14 val PER: 0.2796
2026-01-11 21:33:21,240: t15.2025.03.16 val PER: 0.1178
2026-01-11 21:33:21,240: t15.2025.03.30 val PER: 0.2057
2026-01-11 21:33:21,240: t15.2025.04.13 val PER: 0.1854
2026-01-11 21:33:21,423: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_99999
2026-01-11 21:33:22,031: Best avg val PER achieved: 0.10343
2026-01-11 21:33:22,032: Total training time: 259.82 minutes

=== RUN diphone_base.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base
2026-01-11 21:36:16,726: Using device: cuda:0
2026-01-11 21:40:12,216: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-11 21:40:12,217: Diphone mode ENABLED: n_classes changed from 41 to 1601
2026-01-11 21:40:12,242: Using 45 sessions after filtering (from 45).
2026-01-11 21:40:13,577: Using torch.compile (if available)
2026-01-11 21:40:13,577: torch.compile not available (torch<2.0). Skipping.
2026-01-11 21:40:13,577: Initialized RNN decoding model
2026-01-11 21:40:13,577: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Identity()
  (out): Linear(in_features=768, out_features=1601, bias=True)
)
2026-01-11 21:40:13,578: Model has 45,514,817 parameters
2026-01-11 21:40:13,578: Model has 11,819,520 day-specific parameters | 25.97% of total parameters
2026-01-11 21:40:14,899: Successfully initialized datasets
2026-01-11 21:40:14,899: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-11 21:40:16,542: Train batch 0: loss: 1387.45 grad norm: 201.84 time: 0.212
2026-01-11 21:40:16,542: Running test after training batch: 0
2026-01-11 21:40:16,659: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:40:24,067: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-11 21:40:25,293: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-11 21:45:16,988: Val batch 0: PER (avg): 4.4638 CTC Loss (avg): 1561.5491 WER(5gram): 100.00% (n=256) time: 300.445
2026-01-11 21:45:16,988: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=0
2026-01-11 21:45:16,988: t15.2023.08.13 val PER: 3.7568
2026-01-11 21:45:16,988: t15.2023.08.18 val PER: 4.0109
2026-01-11 21:45:16,988: t15.2023.08.20 val PER: 3.9738
2026-01-11 21:45:16,989: t15.2023.08.25 val PER: 3.9593
2026-01-11 21:45:16,989: t15.2023.08.27 val PER: 3.7042
2026-01-11 21:45:16,989: t15.2023.09.01 val PER: 4.0593
2026-01-11 21:45:16,989: t15.2023.09.03 val PER: 3.9442
2026-01-11 21:45:16,989: t15.2023.09.24 val PER: 4.6930
2026-01-11 21:45:16,989: t15.2023.09.29 val PER: 4.6362
2026-01-11 21:45:16,989: t15.2023.10.01 val PER: 3.6797
2026-01-11 21:45:16,989: t15.2023.10.06 val PER: 4.4898
2026-01-11 21:45:16,989: t15.2023.10.08 val PER: 3.3559
2026-01-11 21:45:16,989: t15.2023.10.13 val PER: 4.2498
2026-01-11 21:45:16,989: t15.2023.10.15 val PER: 4.7475
2026-01-11 21:45:16,989: t15.2023.10.20 val PER: 4.8188
2026-01-11 21:45:16,989: t15.2023.10.22 val PER: 4.7094
2026-01-11 21:45:16,989: t15.2023.11.03 val PER: 5.0197
2026-01-11 21:45:16,989: t15.2023.11.04 val PER: 6.2526
2026-01-11 21:45:16,990: t15.2023.11.17 val PER: 6.5630
2026-01-11 21:45:16,990: t15.2023.11.19 val PER: 4.9800
2026-01-11 21:45:16,990: t15.2023.11.26 val PER: 4.9536
2026-01-11 21:45:16,990: t15.2023.12.03 val PER: 4.5809
2026-01-11 21:45:16,990: t15.2023.12.08 val PER: 5.0686
2026-01-11 21:45:16,990: t15.2023.12.10 val PER: 5.5046
2026-01-11 21:45:16,990: t15.2023.12.17 val PER: 4.1331
2026-01-11 21:45:16,990: t15.2023.12.29 val PER: 4.4880
2026-01-11 21:45:16,991: t15.2024.02.25 val PER: 4.2360
2026-01-11 21:45:16,991: t15.2024.03.08 val PER: 4.1650
2026-01-11 21:45:16,991: t15.2024.03.15 val PER: 4.0544
2026-01-11 21:45:16,991: t15.2024.03.17 val PER: 4.3417
2026-01-11 21:45:16,991: t15.2024.05.10 val PER: 4.1189
2026-01-11 21:45:16,991: t15.2024.06.14 val PER: 4.6987
2026-01-11 21:45:16,991: t15.2024.07.19 val PER: 3.3223
2026-01-11 21:45:16,991: t15.2024.07.21 val PER: 5.0124
2026-01-11 21:45:16,991: t15.2024.07.28 val PER: 5.2228
2026-01-11 21:45:16,991: t15.2025.01.10 val PER: 3.1336
2026-01-11 21:45:16,991: t15.2025.01.12 val PER: 5.7398
2026-01-11 21:45:16,991: t15.2025.03.14 val PER: 3.0592
2026-01-11 21:45:16,991: t15.2025.03.16 val PER: 5.4817
2026-01-11 21:45:16,992: t15.2025.03.30 val PER: 4.2770
2026-01-11 21:45:16,992: t15.2025.04.13 val PER: 4.8859
2026-01-11 21:45:16,993: New best val WER(5gram) inf% --> 100.00%
2026-01-11 21:45:17,151: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_0
2026-01-11 21:45:37,049: Train batch 200: loss: 186.28 grad norm: 38.07 time: 0.062
2026-01-11 21:45:56,676: Train batch 400: loss: 144.98 grad norm: 32.68 time: 0.070
2026-01-11 21:46:06,644: Running test after training batch: 500
2026-01-11 21:46:07,022: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:46:13,149: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-11 21:46:13,185: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-11 21:46:27,849: Val batch 500: PER (avg): 0.9579 CTC Loss (avg): 169.7588 WER(5gram): 99.41% (n=256) time: 21.204
2026-01-11 21:46:27,849: WER lens: avg_true_words=5.99 avg_pred_words=0.12 max_pred_words=2
2026-01-11 21:46:27,849: t15.2023.08.13 val PER: 0.9459
2026-01-11 21:46:27,849: t15.2023.08.18 val PER: 0.9472
2026-01-11 21:46:27,850: t15.2023.08.20 val PER: 0.9396
2026-01-11 21:46:27,850: t15.2023.08.25 val PER: 0.9488
2026-01-11 21:46:27,850: t15.2023.08.27 val PER: 0.9389
2026-01-11 21:46:27,850: t15.2023.09.01 val PER: 0.9472
2026-01-11 21:46:27,850: t15.2023.09.03 val PER: 0.9584
2026-01-11 21:46:27,850: t15.2023.09.24 val PER: 0.9502
2026-01-11 21:46:27,850: t15.2023.09.29 val PER: 0.9375
2026-01-11 21:46:27,851: t15.2023.10.01 val PER: 0.9491
2026-01-11 21:46:27,851: t15.2023.10.06 val PER: 0.9408
2026-01-11 21:46:27,851: t15.2023.10.08 val PER: 0.9540
2026-01-11 21:46:27,851: t15.2023.10.13 val PER: 0.9465
2026-01-11 21:46:27,851: t15.2023.10.15 val PER: 0.9611
2026-01-11 21:46:27,851: t15.2023.10.20 val PER: 0.9664
2026-01-11 21:46:27,851: t15.2023.10.22 val PER: 0.9510
2026-01-11 21:46:27,851: t15.2023.11.03 val PER: 0.9634
2026-01-11 21:46:27,851: t15.2023.11.04 val PER: 0.9488
2026-01-11 21:46:27,851: t15.2023.11.17 val PER: 0.9611
2026-01-11 21:46:27,852: t15.2023.11.19 val PER: 0.9601
2026-01-11 21:46:27,852: t15.2023.11.26 val PER: 0.9659
2026-01-11 21:46:27,852: t15.2023.12.03 val PER: 0.9632
2026-01-11 21:46:27,852: t15.2023.12.08 val PER: 0.9654
2026-01-11 21:46:27,852: t15.2023.12.10 val PER: 0.9606
2026-01-11 21:46:27,852: t15.2023.12.17 val PER: 0.9688
2026-01-11 21:46:27,852: t15.2023.12.29 val PER: 0.9657
2026-01-11 21:46:27,852: t15.2024.02.25 val PER: 0.9593
2026-01-11 21:46:27,852: t15.2024.03.08 val PER: 0.9644
2026-01-11 21:46:27,852: t15.2024.03.15 val PER: 0.9700
2026-01-11 21:46:27,853: t15.2024.03.17 val PER: 0.9637
2026-01-11 21:46:27,853: t15.2024.05.10 val PER: 0.9629
2026-01-11 21:46:27,853: t15.2024.06.14 val PER: 0.9606
2026-01-11 21:46:27,853: t15.2024.07.19 val PER: 0.9677
2026-01-11 21:46:27,853: t15.2024.07.21 val PER: 0.9683
2026-01-11 21:46:27,853: t15.2024.07.28 val PER: 0.9618
2026-01-11 21:46:27,853: t15.2025.01.10 val PER: 0.9683
2026-01-11 21:46:27,853: t15.2025.01.12 val PER: 0.9615
2026-01-11 21:46:27,853: t15.2025.03.14 val PER: 0.9645
2026-01-11 21:46:27,854: t15.2025.03.16 val PER: 0.9673
2026-01-11 21:46:27,854: t15.2025.03.30 val PER: 0.9632
2026-01-11 21:46:27,854: t15.2025.04.13 val PER: 0.9601
2026-01-11 21:46:27,854: New best val WER(5gram) 100.00% --> 99.41%
2026-01-11 21:46:28,022: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_500
2026-01-11 21:46:37,821: Train batch 600: loss: 154.66 grad norm: 52.97 time: 0.085
2026-01-11 21:46:57,241: Train batch 800: loss: 148.89 grad norm: 42.77 time: 0.065
2026-01-11 21:47:16,786: Train batch 1000: loss: 130.93 grad norm: 39.80 time: 0.073
2026-01-11 21:47:16,787: Running test after training batch: 1000
2026-01-11 21:47:17,220: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:47:23,090: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-11 21:47:23,109: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-11 21:47:27,502: Val batch 1000: PER (avg): 0.9998 CTC Loss (avg): 156.9429 WER(5gram): 100.00% (n=256) time: 10.715
2026-01-11 21:47:27,503: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=0
2026-01-11 21:47:27,503: t15.2023.08.13 val PER: 1.0000
2026-01-11 21:47:27,503: t15.2023.08.18 val PER: 0.9992
2026-01-11 21:47:27,503: t15.2023.08.20 val PER: 1.0000
2026-01-11 21:47:27,503: t15.2023.08.25 val PER: 0.9985
2026-01-11 21:47:27,504: t15.2023.08.27 val PER: 0.9984
2026-01-11 21:47:27,504: t15.2023.09.01 val PER: 0.9992
2026-01-11 21:47:27,504: t15.2023.09.03 val PER: 1.0000
2026-01-11 21:47:27,504: t15.2023.09.24 val PER: 1.0000
2026-01-11 21:47:27,504: t15.2023.09.29 val PER: 1.0000
2026-01-11 21:47:27,504: t15.2023.10.01 val PER: 0.9993
2026-01-11 21:47:27,504: t15.2023.10.06 val PER: 1.0000
2026-01-11 21:47:27,504: t15.2023.10.08 val PER: 0.9986
2026-01-11 21:47:27,504: t15.2023.10.13 val PER: 1.0000
2026-01-11 21:47:27,505: t15.2023.10.15 val PER: 1.0000
2026-01-11 21:47:27,505: t15.2023.10.20 val PER: 1.0000
2026-01-11 21:47:27,505: t15.2023.10.22 val PER: 1.0000
2026-01-11 21:47:27,505: t15.2023.11.03 val PER: 1.0000
2026-01-11 21:47:27,505: t15.2023.11.04 val PER: 1.0000
2026-01-11 21:47:27,505: t15.2023.11.17 val PER: 1.0000
2026-01-11 21:47:27,505: t15.2023.11.19 val PER: 1.0000
2026-01-11 21:47:27,505: t15.2023.11.26 val PER: 1.0000
2026-01-11 21:47:27,505: t15.2023.12.03 val PER: 1.0000
2026-01-11 21:47:27,505: t15.2023.12.08 val PER: 0.9993
2026-01-11 21:47:27,505: t15.2023.12.10 val PER: 1.0000
2026-01-11 21:47:27,506: t15.2023.12.17 val PER: 1.0000
2026-01-11 21:47:27,506: t15.2023.12.29 val PER: 1.0000
2026-01-11 21:47:27,506: t15.2024.02.25 val PER: 0.9986
2026-01-11 21:47:27,506: t15.2024.03.08 val PER: 1.0000
2026-01-11 21:47:27,506: t15.2024.03.15 val PER: 1.0000
2026-01-11 21:47:27,506: t15.2024.03.17 val PER: 0.9993
2026-01-11 21:47:27,506: t15.2024.05.10 val PER: 1.0000
2026-01-11 21:47:27,506: t15.2024.06.14 val PER: 1.0000
2026-01-11 21:47:27,506: t15.2024.07.19 val PER: 1.0000
2026-01-11 21:47:27,506: t15.2024.07.21 val PER: 1.0000
2026-01-11 21:47:27,506: t15.2024.07.28 val PER: 1.0000
2026-01-11 21:47:27,507: t15.2025.01.10 val PER: 1.0000
2026-01-11 21:47:27,507: t15.2025.01.12 val PER: 1.0000
2026-01-11 21:47:27,507: t15.2025.03.14 val PER: 1.0000
2026-01-11 21:47:27,507: t15.2025.03.16 val PER: 1.0000
2026-01-11 21:47:27,507: t15.2025.03.30 val PER: 0.9989
2026-01-11 21:47:27,507: t15.2025.04.13 val PER: 1.0000
2026-01-11 21:47:27,660: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_1000
2026-01-11 21:47:47,037: Train batch 1200: loss: 140.99 grad norm: 102.90 time: 0.076
2026-01-11 21:48:06,713: Train batch 1400: loss: 125.52 grad norm: 171.50 time: 0.067
2026-01-11 21:48:16,492: Running test after training batch: 1500
2026-01-11 21:48:16,670: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:48:22,623: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-11 21:48:22,647: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-11 21:48:28,705: Val batch 1500: PER (avg): 0.9024 CTC Loss (avg): 136.0136 WER(5gram): 100.00% (n=256) time: 12.213
2026-01-11 21:48:28,706: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=1
2026-01-11 21:48:28,706: t15.2023.08.13 val PER: 0.9127
2026-01-11 21:48:28,706: t15.2023.08.18 val PER: 0.8826
2026-01-11 21:48:28,706: t15.2023.08.20 val PER: 0.8848
2026-01-11 21:48:28,706: t15.2023.08.25 val PER: 0.8750
2026-01-11 21:48:28,707: t15.2023.08.27 val PER: 0.9180
2026-01-11 21:48:28,707: t15.2023.09.01 val PER: 0.8750
2026-01-11 21:48:28,707: t15.2023.09.03 val PER: 0.9311
2026-01-11 21:48:28,707: t15.2023.09.24 val PER: 0.9138
2026-01-11 21:48:28,708: t15.2023.09.29 val PER: 0.8832
2026-01-11 21:48:28,708: t15.2023.10.01 val PER: 0.8943
2026-01-11 21:48:28,708: t15.2023.10.06 val PER: 0.8794
2026-01-11 21:48:28,708: t15.2023.10.08 val PER: 0.9134
2026-01-11 21:48:28,708: t15.2023.10.13 val PER: 0.8875
2026-01-11 21:48:28,708: t15.2023.10.15 val PER: 0.8919
2026-01-11 21:48:28,708: t15.2023.10.20 val PER: 0.8893
2026-01-11 21:48:28,708: t15.2023.10.22 val PER: 0.8920
2026-01-11 21:48:28,709: t15.2023.11.03 val PER: 0.9009
2026-01-11 21:48:28,709: t15.2023.11.04 val PER: 0.9420
2026-01-11 21:48:28,709: t15.2023.11.17 val PER: 0.9098
2026-01-11 21:48:28,709: t15.2023.11.19 val PER: 0.9122
2026-01-11 21:48:28,709: t15.2023.11.26 val PER: 0.8833
2026-01-11 21:48:28,709: t15.2023.12.03 val PER: 0.8876
2026-01-11 21:48:28,709: t15.2023.12.08 val PER: 0.8895
2026-01-11 21:48:28,709: t15.2023.12.10 val PER: 0.8883
2026-01-11 21:48:28,709: t15.2023.12.17 val PER: 0.9304
2026-01-11 21:48:28,709: t15.2023.12.29 val PER: 0.9142
2026-01-11 21:48:28,709: t15.2024.02.25 val PER: 0.9017
2026-01-11 21:48:28,710: t15.2024.03.08 val PER: 0.9360
2026-01-11 21:48:28,710: t15.2024.03.15 val PER: 0.9199
2026-01-11 21:48:28,710: t15.2024.03.17 val PER: 0.8954
2026-01-11 21:48:28,710: t15.2024.05.10 val PER: 0.8990
2026-01-11 21:48:28,710: t15.2024.06.14 val PER: 0.8785
2026-01-11 21:48:28,710: t15.2024.07.19 val PER: 0.9387
2026-01-11 21:48:28,710: t15.2024.07.21 val PER: 0.8903
2026-01-11 21:48:28,710: t15.2024.07.28 val PER: 0.8868
2026-01-11 21:48:28,710: t15.2025.01.10 val PER: 0.9504
2026-01-11 21:48:28,710: t15.2025.01.12 val PER: 0.8876
2026-01-11 21:48:28,710: t15.2025.03.14 val PER: 0.9719
2026-01-11 21:48:28,710: t15.2025.03.16 val PER: 0.9136
2026-01-11 21:48:28,711: t15.2025.03.30 val PER: 0.9494
2026-01-11 21:48:28,711: t15.2025.04.13 val PER: 0.9058
2026-01-11 21:48:28,868: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_1500
2026-01-11 21:48:38,495: Train batch 1600: loss: 125.20 grad norm: 47.11 time: 0.072
2026-01-11 21:48:57,936: Train batch 1800: loss: 113.68 grad norm: 73.45 time: 0.098
2026-01-11 21:49:17,418: Train batch 2000: loss: 105.42 grad norm: 67.13 time: 0.074
2026-01-11 21:49:17,418: Running test after training batch: 2000
2026-01-11 21:49:17,538: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:49:24,876: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-11 21:49:24,978: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-11 21:49:43,572: Val batch 2000: PER (avg): 0.6675 CTC Loss (avg): 109.1824 WER(5gram): 99.35% (n=256) time: 26.153
2026-01-11 21:49:43,572: WER lens: avg_true_words=5.99 avg_pred_words=0.06 max_pred_words=1
2026-01-11 21:49:43,572: t15.2023.08.13 val PER: 0.6435
2026-01-11 21:49:43,573: t15.2023.08.18 val PER: 0.6479
2026-01-11 21:49:43,573: t15.2023.08.20 val PER: 0.6473
2026-01-11 21:49:43,573: t15.2023.08.25 val PER: 0.6099
2026-01-11 21:49:43,573: t15.2023.08.27 val PER: 0.6945
2026-01-11 21:49:43,573: t15.2023.09.01 val PER: 0.6258
2026-01-11 21:49:43,573: t15.2023.09.03 val PER: 0.6603
2026-01-11 21:49:43,573: t15.2023.09.24 val PER: 0.6638
2026-01-11 21:49:43,573: t15.2023.09.29 val PER: 0.6592
2026-01-11 21:49:43,573: t15.2023.10.01 val PER: 0.6816
2026-01-11 21:49:43,573: t15.2023.10.06 val PER: 0.6588
2026-01-11 21:49:43,573: t15.2023.10.08 val PER: 0.6942
2026-01-11 21:49:43,574: t15.2023.10.13 val PER: 0.7207
2026-01-11 21:49:43,574: t15.2023.10.15 val PER: 0.6579
2026-01-11 21:49:43,574: t15.2023.10.20 val PER: 0.6544
2026-01-11 21:49:43,574: t15.2023.10.22 val PER: 0.6615
2026-01-11 21:49:43,574: t15.2023.11.03 val PER: 0.6472
2026-01-11 21:49:43,574: t15.2023.11.04 val PER: 0.5870
2026-01-11 21:49:43,574: t15.2023.11.17 val PER: 0.6050
2026-01-11 21:49:43,574: t15.2023.11.19 val PER: 0.5988
2026-01-11 21:49:43,574: t15.2023.11.26 val PER: 0.7043
2026-01-11 21:49:43,574: t15.2023.12.03 val PER: 0.6597
2026-01-11 21:49:43,574: t15.2023.12.08 val PER: 0.6664
2026-01-11 21:49:43,574: t15.2023.12.10 val PER: 0.6597
2026-01-11 21:49:43,575: t15.2023.12.17 val PER: 0.6580
2026-01-11 21:49:43,575: t15.2023.12.29 val PER: 0.6651
2026-01-11 21:49:43,575: t15.2024.02.25 val PER: 0.6475
2026-01-11 21:49:43,575: t15.2024.03.08 val PER: 0.6558
2026-01-11 21:49:43,575: t15.2024.03.15 val PER: 0.6879
2026-01-11 21:49:43,575: t15.2024.03.17 val PER: 0.6681
2026-01-11 21:49:43,575: t15.2024.05.10 val PER: 0.6449
2026-01-11 21:49:43,575: t15.2024.06.14 val PER: 0.6514
2026-01-11 21:49:43,575: t15.2024.07.19 val PER: 0.7113
2026-01-11 21:49:43,575: t15.2024.07.21 val PER: 0.6559
2026-01-11 21:49:43,575: t15.2024.07.28 val PER: 0.6507
2026-01-11 21:49:43,575: t15.2025.01.10 val PER: 0.7466
2026-01-11 21:49:43,575: t15.2025.01.12 val PER: 0.6605
2026-01-11 21:49:43,576: t15.2025.03.14 val PER: 0.7219
2026-01-11 21:49:43,576: t15.2025.03.16 val PER: 0.6793
2026-01-11 21:49:43,576: t15.2025.03.30 val PER: 0.7448
2026-01-11 21:49:43,576: t15.2025.04.13 val PER: 0.6990
2026-01-11 21:49:43,576: New best val WER(5gram) 99.41% --> 99.35%
2026-01-11 21:49:43,742: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_2000
2026-01-11 21:50:02,639: Train batch 2200: loss: 95.06 grad norm: 62.43 time: 0.068
2026-01-11 21:50:21,658: Train batch 2400: loss: 89.16 grad norm: 63.81 time: 0.059
2026-01-11 21:50:31,332: Running test after training batch: 2500
2026-01-11 21:50:31,485: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:50:37,517: WER debug example
  GT : you can see the code at this point as well
  PR : the
2026-01-11 21:50:37,767: WER debug example
  GT : how does it keep the cost down
  PR : in the
2026-01-11 21:51:30,973: Val batch 2500: PER (avg): 0.5362 CTC Loss (avg): 95.1258 WER(5gram): 89.11% (n=256) time: 59.641
2026-01-11 21:51:30,974: WER lens: avg_true_words=5.99 avg_pred_words=1.99 max_pred_words=9
2026-01-11 21:51:30,974: t15.2023.08.13 val PER: 0.5000
2026-01-11 21:51:30,974: t15.2023.08.18 val PER: 0.5046
2026-01-11 21:51:30,974: t15.2023.08.20 val PER: 0.4813
2026-01-11 21:51:30,974: t15.2023.08.25 val PER: 0.4593
2026-01-11 21:51:30,974: t15.2023.08.27 val PER: 0.5756
2026-01-11 21:51:30,974: t15.2023.09.01 val PER: 0.4789
2026-01-11 21:51:30,975: t15.2023.09.03 val PER: 0.5249
2026-01-11 21:51:30,975: t15.2023.09.24 val PER: 0.5109
2026-01-11 21:51:30,975: t15.2023.09.29 val PER: 0.5163
2026-01-11 21:51:30,975: t15.2023.10.01 val PER: 0.5720
2026-01-11 21:51:30,975: t15.2023.10.06 val PER: 0.5210
2026-01-11 21:51:30,975: t15.2023.10.08 val PER: 0.5765
2026-01-11 21:51:30,975: t15.2023.10.13 val PER: 0.6028
2026-01-11 21:51:30,975: t15.2023.10.15 val PER: 0.5307
2026-01-11 21:51:30,975: t15.2023.10.20 val PER: 0.5336
2026-01-11 21:51:30,975: t15.2023.10.22 val PER: 0.5134
2026-01-11 21:51:30,976: t15.2023.11.03 val PER: 0.5156
2026-01-11 21:51:30,976: t15.2023.11.04 val PER: 0.3925
2026-01-11 21:51:30,976: t15.2023.11.17 val PER: 0.4432
2026-01-11 21:51:30,976: t15.2023.11.19 val PER: 0.4551
2026-01-11 21:51:30,976: t15.2023.11.26 val PER: 0.5891
2026-01-11 21:51:30,976: t15.2023.12.03 val PER: 0.5294
2026-01-11 21:51:30,976: t15.2023.12.08 val PER: 0.5293
2026-01-11 21:51:30,976: t15.2023.12.10 val PER: 0.5335
2026-01-11 21:51:30,976: t15.2023.12.17 val PER: 0.5530
2026-01-11 21:51:30,976: t15.2023.12.29 val PER: 0.5456
2026-01-11 21:51:30,976: t15.2024.02.25 val PER: 0.5183
2026-01-11 21:51:30,977: t15.2024.03.08 val PER: 0.5491
2026-01-11 21:51:30,977: t15.2024.03.15 val PER: 0.5472
2026-01-11 21:51:30,977: t15.2024.03.17 val PER: 0.5391
2026-01-11 21:51:30,977: t15.2024.05.10 val PER: 0.5126
2026-01-11 21:51:30,977: t15.2024.06.14 val PER: 0.4890
2026-01-11 21:51:30,977: t15.2024.07.19 val PER: 0.6005
2026-01-11 21:51:30,977: t15.2024.07.21 val PER: 0.5007
2026-01-11 21:51:30,977: t15.2024.07.28 val PER: 0.5147
2026-01-11 21:51:30,977: t15.2025.01.10 val PER: 0.6171
2026-01-11 21:51:30,977: t15.2025.01.12 val PER: 0.5458
2026-01-11 21:51:30,978: t15.2025.03.14 val PER: 0.6243
2026-01-11 21:51:30,978: t15.2025.03.16 val PER: 0.5798
2026-01-11 21:51:30,978: t15.2025.03.30 val PER: 0.6287
2026-01-11 21:51:30,978: t15.2025.04.13 val PER: 0.5806
2026-01-11 21:51:30,978: New best val WER(5gram) 99.35% --> 89.11%
2026-01-11 21:51:31,135: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_2500
2026-01-11 21:51:40,450: Train batch 2600: loss: 97.15 grad norm: 90.53 time: 0.067
2026-01-11 21:51:59,528: Train batch 2800: loss: 77.71 grad norm: 64.05 time: 0.089
2026-01-11 21:52:18,795: Train batch 3000: loss: 89.33 grad norm: 95.79 time: 0.096
2026-01-11 21:52:18,795: Running test after training batch: 3000
2026-01-11 21:52:18,904: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:52:25,257: WER debug example
  GT : you can see the code at this point as well
  PR : this is why
2026-01-11 21:52:25,481: WER debug example
  GT : how does it keep the cost down
  PR : i owe it to the new
2026-01-11 21:53:16,380: Val batch 3000: PER (avg): 0.4863 CTC Loss (avg): 86.3674 WER(5gram): 82.86% (n=256) time: 57.584
2026-01-11 21:53:16,380: WER lens: avg_true_words=5.99 avg_pred_words=3.77 max_pred_words=10
2026-01-11 21:53:16,381: t15.2023.08.13 val PER: 0.4615
2026-01-11 21:53:16,381: t15.2023.08.18 val PER: 0.4484
2026-01-11 21:53:16,381: t15.2023.08.20 val PER: 0.4226
2026-01-11 21:53:16,381: t15.2023.08.25 val PER: 0.4051
2026-01-11 21:53:16,381: t15.2023.08.27 val PER: 0.5225
2026-01-11 21:53:16,381: t15.2023.09.01 val PER: 0.4172
2026-01-11 21:53:16,381: t15.2023.09.03 val PER: 0.4846
2026-01-11 21:53:16,381: t15.2023.09.24 val PER: 0.4612
2026-01-11 21:53:16,381: t15.2023.09.29 val PER: 0.4671
2026-01-11 21:53:16,381: t15.2023.10.01 val PER: 0.5073
2026-01-11 21:53:16,381: t15.2023.10.06 val PER: 0.4532
2026-01-11 21:53:16,381: t15.2023.10.08 val PER: 0.5386
2026-01-11 21:53:16,381: t15.2023.10.13 val PER: 0.5764
2026-01-11 21:53:16,382: t15.2023.10.15 val PER: 0.4924
2026-01-11 21:53:16,382: t15.2023.10.20 val PER: 0.4698
2026-01-11 21:53:16,382: t15.2023.10.22 val PER: 0.4599
2026-01-11 21:53:16,382: t15.2023.11.03 val PER: 0.4640
2026-01-11 21:53:16,382: t15.2023.11.04 val PER: 0.3345
2026-01-11 21:53:16,382: t15.2023.11.17 val PER: 0.3857
2026-01-11 21:53:16,382: t15.2023.11.19 val PER: 0.3453
2026-01-11 21:53:16,382: t15.2023.11.26 val PER: 0.5478
2026-01-11 21:53:16,382: t15.2023.12.03 val PER: 0.4737
2026-01-11 21:53:16,382: t15.2023.12.08 val PER: 0.4847
2026-01-11 21:53:16,382: t15.2023.12.10 val PER: 0.4796
2026-01-11 21:53:16,382: t15.2023.12.17 val PER: 0.4730
2026-01-11 21:53:16,383: t15.2023.12.29 val PER: 0.5051
2026-01-11 21:53:16,383: t15.2024.02.25 val PER: 0.4410
2026-01-11 21:53:16,383: t15.2024.03.08 val PER: 0.5050
2026-01-11 21:53:16,383: t15.2024.03.15 val PER: 0.5003
2026-01-11 21:53:16,383: t15.2024.03.17 val PER: 0.4861
2026-01-11 21:53:16,383: t15.2024.05.10 val PER: 0.4933
2026-01-11 21:53:16,383: t15.2024.06.14 val PER: 0.4401
2026-01-11 21:53:16,383: t15.2024.07.19 val PER: 0.5537
2026-01-11 21:53:16,383: t15.2024.07.21 val PER: 0.4455
2026-01-11 21:53:16,383: t15.2024.07.28 val PER: 0.4750
2026-01-11 21:53:16,384: t15.2025.01.10 val PER: 0.5675
2026-01-11 21:53:16,384: t15.2025.01.12 val PER: 0.4988
2026-01-11 21:53:16,384: t15.2025.03.14 val PER: 0.5725
2026-01-11 21:53:16,384: t15.2025.03.16 val PER: 0.5602
2026-01-11 21:53:16,384: t15.2025.03.30 val PER: 0.5897
2026-01-11 21:53:16,384: t15.2025.04.13 val PER: 0.5278
2026-01-11 21:53:16,384: New best val WER(5gram) 89.11% --> 82.86%
2026-01-11 21:53:16,540: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_3000
2026-01-11 21:53:35,658: Train batch 3200: loss: 76.81 grad norm: 80.71 time: 0.089
2026-01-11 21:53:54,761: Train batch 3400: loss: 64.62 grad norm: 65.18 time: 0.056
2026-01-11 21:54:04,543: Running test after training batch: 3500
2026-01-11 21:54:04,676: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:54:10,686: WER debug example
  GT : you can see the code at this point as well
  PR : you see a guy in the way
2026-01-11 21:54:10,898: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the new
2026-01-11 21:54:59,763: Val batch 3500: PER (avg): 0.4534 CTC Loss (avg): 80.2164 WER(5gram): 77.51% (n=256) time: 55.219
2026-01-11 21:54:59,763: WER lens: avg_true_words=5.99 avg_pred_words=4.50 max_pred_words=10
2026-01-11 21:54:59,763: t15.2023.08.13 val PER: 0.4106
2026-01-11 21:54:59,764: t15.2023.08.18 val PER: 0.4216
2026-01-11 21:54:59,764: t15.2023.08.20 val PER: 0.3979
2026-01-11 21:54:59,764: t15.2023.08.25 val PER: 0.3886
2026-01-11 21:54:59,764: t15.2023.08.27 val PER: 0.4936
2026-01-11 21:54:59,764: t15.2023.09.01 val PER: 0.3880
2026-01-11 21:54:59,764: t15.2023.09.03 val PER: 0.4466
2026-01-11 21:54:59,764: t15.2023.09.24 val PER: 0.4199
2026-01-11 21:54:59,764: t15.2023.09.29 val PER: 0.4371
2026-01-11 21:54:59,764: t15.2023.10.01 val PER: 0.4775
2026-01-11 21:54:59,764: t15.2023.10.06 val PER: 0.4252
2026-01-11 21:54:59,764: t15.2023.10.08 val PER: 0.5223
2026-01-11 21:54:59,764: t15.2023.10.13 val PER: 0.5353
2026-01-11 21:54:59,764: t15.2023.10.15 val PER: 0.4634
2026-01-11 21:54:59,765: t15.2023.10.20 val PER: 0.4564
2026-01-11 21:54:59,765: t15.2023.10.22 val PER: 0.4243
2026-01-11 21:54:59,765: t15.2023.11.03 val PER: 0.4240
2026-01-11 21:54:59,765: t15.2023.11.04 val PER: 0.2730
2026-01-11 21:54:59,765: t15.2023.11.17 val PER: 0.3313
2026-01-11 21:54:59,765: t15.2023.11.19 val PER: 0.3154
2026-01-11 21:54:59,765: t15.2023.11.26 val PER: 0.5036
2026-01-11 21:54:59,765: t15.2023.12.03 val PER: 0.4485
2026-01-11 21:54:59,765: t15.2023.12.08 val PER: 0.4514
2026-01-11 21:54:59,765: t15.2023.12.10 val PER: 0.4271
2026-01-11 21:54:59,765: t15.2023.12.17 val PER: 0.4376
2026-01-11 21:54:59,766: t15.2023.12.29 val PER: 0.4674
2026-01-11 21:54:59,766: t15.2024.02.25 val PER: 0.3961
2026-01-11 21:54:59,766: t15.2024.03.08 val PER: 0.4822
2026-01-11 21:54:59,766: t15.2024.03.15 val PER: 0.4622
2026-01-11 21:54:59,766: t15.2024.03.17 val PER: 0.4498
2026-01-11 21:54:59,766: t15.2024.05.10 val PER: 0.4636
2026-01-11 21:54:59,766: t15.2024.06.14 val PER: 0.4117
2026-01-11 21:54:59,766: t15.2024.07.19 val PER: 0.5399
2026-01-11 21:54:59,766: t15.2024.07.21 val PER: 0.4090
2026-01-11 21:54:59,766: t15.2024.07.28 val PER: 0.4522
2026-01-11 21:54:59,766: t15.2025.01.10 val PER: 0.5331
2026-01-11 21:54:59,766: t15.2025.01.12 val PER: 0.4588
2026-01-11 21:54:59,766: t15.2025.03.14 val PER: 0.5725
2026-01-11 21:54:59,766: t15.2025.03.16 val PER: 0.5209
2026-01-11 21:54:59,767: t15.2025.03.30 val PER: 0.5655
2026-01-11 21:54:59,767: t15.2025.04.13 val PER: 0.4822
2026-01-11 21:54:59,767: New best val WER(5gram) 82.86% --> 77.51%
2026-01-11 21:54:59,915: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_3500
2026-01-11 21:55:09,764: Train batch 3600: loss: 68.68 grad norm: 70.65 time: 0.078
2026-01-11 21:55:29,087: Train batch 3800: loss: 75.70 grad norm: 72.88 time: 0.081
2026-01-11 21:55:48,559: Train batch 4000: loss: 60.49 grad norm: 65.07 time: 0.070
2026-01-11 21:55:48,559: Running test after training batch: 4000
2026-01-11 21:55:48,732: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:55:54,839: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy in the way
2026-01-11 21:55:55,045: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the way the
2026-01-11 21:56:40,633: Val batch 4000: PER (avg): 0.4309 CTC Loss (avg): 75.1473 WER(5gram): 76.60% (n=256) time: 52.073
2026-01-11 21:56:40,634: WER lens: avg_true_words=5.99 avg_pred_words=4.96 max_pred_words=11
2026-01-11 21:56:40,634: t15.2023.08.13 val PER: 0.3909
2026-01-11 21:56:40,634: t15.2023.08.18 val PER: 0.3940
2026-01-11 21:56:40,634: t15.2023.08.20 val PER: 0.3749
2026-01-11 21:56:40,634: t15.2023.08.25 val PER: 0.3660
2026-01-11 21:56:40,634: t15.2023.08.27 val PER: 0.4791
2026-01-11 21:56:40,634: t15.2023.09.01 val PER: 0.3563
2026-01-11 21:56:40,634: t15.2023.09.03 val PER: 0.4442
2026-01-11 21:56:40,634: t15.2023.09.24 val PER: 0.3859
2026-01-11 21:56:40,634: t15.2023.09.29 val PER: 0.4237
2026-01-11 21:56:40,635: t15.2023.10.01 val PER: 0.4571
2026-01-11 21:56:40,635: t15.2023.10.06 val PER: 0.4037
2026-01-11 21:56:40,635: t15.2023.10.08 val PER: 0.4993
2026-01-11 21:56:40,635: t15.2023.10.13 val PER: 0.5306
2026-01-11 21:56:40,635: t15.2023.10.15 val PER: 0.4324
2026-01-11 21:56:40,635: t15.2023.10.20 val PER: 0.4262
2026-01-11 21:56:40,635: t15.2023.10.22 val PER: 0.3875
2026-01-11 21:56:40,635: t15.2023.11.03 val PER: 0.4111
2026-01-11 21:56:40,636: t15.2023.11.04 val PER: 0.2423
2026-01-11 21:56:40,636: t15.2023.11.17 val PER: 0.3064
2026-01-11 21:56:40,636: t15.2023.11.19 val PER: 0.2754
2026-01-11 21:56:40,636: t15.2023.11.26 val PER: 0.4870
2026-01-11 21:56:40,636: t15.2023.12.03 val PER: 0.4139
2026-01-11 21:56:40,636: t15.2023.12.08 val PER: 0.4308
2026-01-11 21:56:40,636: t15.2023.12.10 val PER: 0.4060
2026-01-11 21:56:40,636: t15.2023.12.17 val PER: 0.4033
2026-01-11 21:56:40,636: t15.2023.12.29 val PER: 0.4523
2026-01-11 21:56:40,636: t15.2024.02.25 val PER: 0.3722
2026-01-11 21:56:40,636: t15.2024.03.08 val PER: 0.4751
2026-01-11 21:56:40,636: t15.2024.03.15 val PER: 0.4490
2026-01-11 21:56:40,636: t15.2024.03.17 val PER: 0.4275
2026-01-11 21:56:40,637: t15.2024.05.10 val PER: 0.4339
2026-01-11 21:56:40,637: t15.2024.06.14 val PER: 0.3817
2026-01-11 21:56:40,637: t15.2024.07.19 val PER: 0.5102
2026-01-11 21:56:40,637: t15.2024.07.21 val PER: 0.3786
2026-01-11 21:56:40,637: t15.2024.07.28 val PER: 0.4265
2026-01-11 21:56:40,637: t15.2025.01.10 val PER: 0.5207
2026-01-11 21:56:40,637: t15.2025.01.12 val PER: 0.4303
2026-01-11 21:56:40,637: t15.2025.03.14 val PER: 0.5473
2026-01-11 21:56:40,637: t15.2025.03.16 val PER: 0.4961
2026-01-11 21:56:40,637: t15.2025.03.30 val PER: 0.5322
2026-01-11 21:56:40,637: t15.2025.04.13 val PER: 0.4765
2026-01-11 21:56:40,638: New best val WER(5gram) 77.51% --> 76.60%
2026-01-11 21:56:40,801: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_4000
2026-01-11 21:57:00,166: Train batch 4200: loss: 64.11 grad norm: 104.06 time: 0.085
2026-01-11 21:57:19,592: Train batch 4400: loss: 54.51 grad norm: 69.94 time: 0.075
2026-01-11 21:57:29,419: Running test after training batch: 4500
2026-01-11 21:57:29,563: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:57:35,564: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy in this is why
2026-01-11 21:57:35,745: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the way the
2026-01-11 21:58:15,841: Val batch 4500: PER (avg): 0.4119 CTC Loss (avg): 70.7846 WER(5gram): 74.71% (n=256) time: 46.422
2026-01-11 21:58:15,842: WER lens: avg_true_words=5.99 avg_pred_words=4.84 max_pred_words=11
2026-01-11 21:58:15,842: t15.2023.08.13 val PER: 0.3638
2026-01-11 21:58:15,842: t15.2023.08.18 val PER: 0.3889
2026-01-11 21:58:15,842: t15.2023.08.20 val PER: 0.3550
2026-01-11 21:58:15,842: t15.2023.08.25 val PER: 0.3584
2026-01-11 21:58:15,842: t15.2023.08.27 val PER: 0.4550
2026-01-11 21:58:15,842: t15.2023.09.01 val PER: 0.3442
2026-01-11 21:58:15,843: t15.2023.09.03 val PER: 0.4192
2026-01-11 21:58:15,843: t15.2023.09.24 val PER: 0.3665
2026-01-11 21:58:15,843: t15.2023.09.29 val PER: 0.3931
2026-01-11 21:58:15,843: t15.2023.10.01 val PER: 0.4511
2026-01-11 21:58:15,843: t15.2023.10.06 val PER: 0.3660
2026-01-11 21:58:15,843: t15.2023.10.08 val PER: 0.4885
2026-01-11 21:58:15,843: t15.2023.10.13 val PER: 0.5120
2026-01-11 21:58:15,843: t15.2023.10.15 val PER: 0.4153
2026-01-11 21:58:15,843: t15.2023.10.20 val PER: 0.4295
2026-01-11 21:58:15,843: t15.2023.10.22 val PER: 0.3742
2026-01-11 21:58:15,844: t15.2023.11.03 val PER: 0.3982
2026-01-11 21:58:15,844: t15.2023.11.04 val PER: 0.1843
2026-01-11 21:58:15,844: t15.2023.11.17 val PER: 0.2722
2026-01-11 21:58:15,844: t15.2023.11.19 val PER: 0.2575
2026-01-11 21:58:15,844: t15.2023.11.26 val PER: 0.4688
2026-01-11 21:58:15,844: t15.2023.12.03 val PER: 0.3992
2026-01-11 21:58:15,844: t15.2023.12.08 val PER: 0.4055
2026-01-11 21:58:15,844: t15.2023.12.10 val PER: 0.3863
2026-01-11 21:58:15,844: t15.2023.12.17 val PER: 0.3815
2026-01-11 21:58:15,845: t15.2023.12.29 val PER: 0.4317
2026-01-11 21:58:15,845: t15.2024.02.25 val PER: 0.3511
2026-01-11 21:58:15,845: t15.2024.03.08 val PER: 0.4509
2026-01-11 21:58:15,845: t15.2024.03.15 val PER: 0.4334
2026-01-11 21:58:15,845: t15.2024.03.17 val PER: 0.4121
2026-01-11 21:58:15,845: t15.2024.05.10 val PER: 0.4235
2026-01-11 21:58:15,845: t15.2024.06.14 val PER: 0.3896
2026-01-11 21:58:15,845: t15.2024.07.19 val PER: 0.4687
2026-01-11 21:58:15,845: t15.2024.07.21 val PER: 0.3621
2026-01-11 21:58:15,845: t15.2024.07.28 val PER: 0.4074
2026-01-11 21:58:15,846: t15.2025.01.10 val PER: 0.5055
2026-01-11 21:58:15,846: t15.2025.01.12 val PER: 0.4142
2026-01-11 21:58:15,846: t15.2025.03.14 val PER: 0.5266
2026-01-11 21:58:15,846: t15.2025.03.16 val PER: 0.4699
2026-01-11 21:58:15,846: t15.2025.03.30 val PER: 0.5149
2026-01-11 21:58:15,846: t15.2025.04.13 val PER: 0.4579
2026-01-11 21:58:15,846: New best val WER(5gram) 76.60% --> 74.71%
2026-01-11 21:58:16,008: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_4500
2026-01-11 21:58:25,820: Train batch 4600: loss: 60.33 grad norm: 81.75 time: 0.072
2026-01-11 21:58:45,696: Train batch 4800: loss: 49.01 grad norm: 71.14 time: 0.070
2026-01-11 21:59:05,513: Train batch 5000: loss: 85.62 grad norm: 121.09 time: 0.074
2026-01-11 21:59:05,513: Running test after training batch: 5000
2026-01-11 21:59:05,663: WER debug GT example: You can see the code at this point as well.
2026-01-11 21:59:11,729: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at the
2026-01-11 21:59:11,904: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the way the
2026-01-11 21:59:51,102: Val batch 5000: PER (avg): 0.3957 CTC Loss (avg): 67.2032 WER(5gram): 75.23% (n=256) time: 45.588
2026-01-11 21:59:51,102: WER lens: avg_true_words=5.99 avg_pred_words=5.18 max_pred_words=12
2026-01-11 21:59:51,103: t15.2023.08.13 val PER: 0.3638
2026-01-11 21:59:51,103: t15.2023.08.18 val PER: 0.3713
2026-01-11 21:59:51,103: t15.2023.08.20 val PER: 0.3336
2026-01-11 21:59:51,103: t15.2023.08.25 val PER: 0.3343
2026-01-11 21:59:51,103: t15.2023.08.27 val PER: 0.4341
2026-01-11 21:59:51,103: t15.2023.09.01 val PER: 0.3125
2026-01-11 21:59:51,103: t15.2023.09.03 val PER: 0.3919
2026-01-11 21:59:51,103: t15.2023.09.24 val PER: 0.3495
2026-01-11 21:59:51,103: t15.2023.09.29 val PER: 0.3880
2026-01-11 21:59:51,103: t15.2023.10.01 val PER: 0.4300
2026-01-11 21:59:51,103: t15.2023.10.06 val PER: 0.3402
2026-01-11 21:59:51,103: t15.2023.10.08 val PER: 0.4736
2026-01-11 21:59:51,103: t15.2023.10.13 val PER: 0.4895
2026-01-11 21:59:51,103: t15.2023.10.15 val PER: 0.4034
2026-01-11 21:59:51,104: t15.2023.10.20 val PER: 0.3893
2026-01-11 21:59:51,104: t15.2023.10.22 val PER: 0.3630
2026-01-11 21:59:51,104: t15.2023.11.03 val PER: 0.3779
2026-01-11 21:59:51,104: t15.2023.11.04 val PER: 0.1741
2026-01-11 21:59:51,104: t15.2023.11.17 val PER: 0.2551
2026-01-11 21:59:51,104: t15.2023.11.19 val PER: 0.2415
2026-01-11 21:59:51,104: t15.2023.11.26 val PER: 0.4543
2026-01-11 21:59:51,104: t15.2023.12.03 val PER: 0.3782
2026-01-11 21:59:51,104: t15.2023.12.08 val PER: 0.3995
2026-01-11 21:59:51,104: t15.2023.12.10 val PER: 0.3693
2026-01-11 21:59:51,105: t15.2023.12.17 val PER: 0.3680
2026-01-11 21:59:51,105: t15.2023.12.29 val PER: 0.4180
2026-01-11 21:59:51,105: t15.2024.02.25 val PER: 0.3385
2026-01-11 21:59:51,105: t15.2024.03.08 val PER: 0.4410
2026-01-11 21:59:51,105: t15.2024.03.15 val PER: 0.4296
2026-01-11 21:59:51,105: t15.2024.03.17 val PER: 0.3954
2026-01-11 21:59:51,105: t15.2024.05.10 val PER: 0.3952
2026-01-11 21:59:51,105: t15.2024.06.14 val PER: 0.3644
2026-01-11 21:59:51,105: t15.2024.07.19 val PER: 0.4608
2026-01-11 21:59:51,105: t15.2024.07.21 val PER: 0.3545
2026-01-11 21:59:51,105: t15.2024.07.28 val PER: 0.3978
2026-01-11 21:59:51,105: t15.2025.01.10 val PER: 0.4972
2026-01-11 21:59:51,106: t15.2025.01.12 val PER: 0.3965
2026-01-11 21:59:51,106: t15.2025.03.14 val PER: 0.4956
2026-01-11 21:59:51,106: t15.2025.03.16 val PER: 0.4450
2026-01-11 21:59:51,106: t15.2025.03.30 val PER: 0.4851
2026-01-11 21:59:51,106: t15.2025.04.13 val PER: 0.4408
2026-01-11 21:59:51,258: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_5000
2026-01-11 22:00:10,230: Train batch 5200: loss: 57.77 grad norm: 86.86 time: 0.060
2026-01-11 22:00:29,559: Train batch 5400: loss: 61.58 grad norm: 75.12 time: 0.075
2026-01-11 22:00:39,164: Running test after training batch: 5500
2026-01-11 22:00:39,336: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:00:45,207: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy in the us were
2026-01-11 22:00:45,371: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the sa new
2026-01-11 22:01:21,995: Val batch 5500: PER (avg): 0.3851 CTC Loss (avg): 64.1489 WER(5gram): 71.84% (n=256) time: 42.830
2026-01-11 22:01:21,996: WER lens: avg_true_words=5.99 avg_pred_words=5.08 max_pred_words=12
2026-01-11 22:01:21,996: t15.2023.08.13 val PER: 0.3534
2026-01-11 22:01:21,996: t15.2023.08.18 val PER: 0.3638
2026-01-11 22:01:21,996: t15.2023.08.20 val PER: 0.3360
2026-01-11 22:01:21,996: t15.2023.08.25 val PER: 0.3238
2026-01-11 22:01:21,997: t15.2023.08.27 val PER: 0.4309
2026-01-11 22:01:21,997: t15.2023.09.01 val PER: 0.3011
2026-01-11 22:01:21,997: t15.2023.09.03 val PER: 0.3872
2026-01-11 22:01:21,997: t15.2023.09.24 val PER: 0.3398
2026-01-11 22:01:21,997: t15.2023.09.29 val PER: 0.3669
2026-01-11 22:01:21,997: t15.2023.10.01 val PER: 0.4254
2026-01-11 22:01:21,997: t15.2023.10.06 val PER: 0.3369
2026-01-11 22:01:21,997: t15.2023.10.08 val PER: 0.4709
2026-01-11 22:01:21,997: t15.2023.10.13 val PER: 0.4841
2026-01-11 22:01:21,997: t15.2023.10.15 val PER: 0.3843
2026-01-11 22:01:21,997: t15.2023.10.20 val PER: 0.3926
2026-01-11 22:01:21,997: t15.2023.10.22 val PER: 0.3486
2026-01-11 22:01:21,998: t15.2023.11.03 val PER: 0.3731
2026-01-11 22:01:21,998: t15.2023.11.04 val PER: 0.1468
2026-01-11 22:01:21,998: t15.2023.11.17 val PER: 0.2442
2026-01-11 22:01:21,998: t15.2023.11.19 val PER: 0.2275
2026-01-11 22:01:21,998: t15.2023.11.26 val PER: 0.4507
2026-01-11 22:01:21,998: t15.2023.12.03 val PER: 0.3771
2026-01-11 22:01:21,998: t15.2023.12.08 val PER: 0.3915
2026-01-11 22:01:21,998: t15.2023.12.10 val PER: 0.3601
2026-01-11 22:01:21,999: t15.2023.12.17 val PER: 0.3680
2026-01-11 22:01:21,999: t15.2023.12.29 val PER: 0.4063
2026-01-11 22:01:21,999: t15.2024.02.25 val PER: 0.3441
2026-01-11 22:01:21,999: t15.2024.03.08 val PER: 0.4139
2026-01-11 22:01:21,999: t15.2024.03.15 val PER: 0.4040
2026-01-11 22:01:21,999: t15.2024.03.17 val PER: 0.3717
2026-01-11 22:01:21,999: t15.2024.05.10 val PER: 0.3848
2026-01-11 22:01:21,999: t15.2024.06.14 val PER: 0.3454
2026-01-11 22:01:21,999: t15.2024.07.19 val PER: 0.4436
2026-01-11 22:01:21,999: t15.2024.07.21 val PER: 0.3462
2026-01-11 22:01:21,999: t15.2024.07.28 val PER: 0.3904
2026-01-11 22:01:21,999: t15.2025.01.10 val PER: 0.4766
2026-01-11 22:01:22,000: t15.2025.01.12 val PER: 0.3811
2026-01-11 22:01:22,000: t15.2025.03.14 val PER: 0.4734
2026-01-11 22:01:22,000: t15.2025.03.16 val PER: 0.4411
2026-01-11 22:01:22,000: t15.2025.03.30 val PER: 0.4920
2026-01-11 22:01:22,000: t15.2025.04.13 val PER: 0.4137
2026-01-11 22:01:22,000: New best val WER(5gram) 74.71% --> 71.84%
2026-01-11 22:01:22,157: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_5500
2026-01-11 22:01:31,624: Train batch 5600: loss: 61.06 grad norm: 98.17 time: 0.070
2026-01-11 22:01:50,814: Train batch 5800: loss: 48.04 grad norm: 81.84 time: 0.098
2026-01-11 22:02:10,003: Train batch 6000: loss: 44.95 grad norm: 77.09 time: 0.055
2026-01-11 22:02:10,003: Running test after training batch: 6000
2026-01-11 22:02:10,191: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:02:16,013: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this is why
2026-01-11 22:02:16,183: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the sa
2026-01-11 22:02:55,537: Val batch 6000: PER (avg): 0.3774 CTC Loss (avg): 62.1933 WER(5gram): 69.17% (n=256) time: 45.533
2026-01-11 22:02:55,537: WER lens: avg_true_words=5.99 avg_pred_words=5.15 max_pred_words=12
2026-01-11 22:02:55,538: t15.2023.08.13 val PER: 0.3545
2026-01-11 22:02:55,538: t15.2023.08.18 val PER: 0.3495
2026-01-11 22:02:55,538: t15.2023.08.20 val PER: 0.3193
2026-01-11 22:02:55,538: t15.2023.08.25 val PER: 0.3208
2026-01-11 22:02:55,538: t15.2023.08.27 val PER: 0.4148
2026-01-11 22:02:55,538: t15.2023.09.01 val PER: 0.2938
2026-01-11 22:02:55,538: t15.2023.09.03 val PER: 0.3729
2026-01-11 22:02:55,538: t15.2023.09.24 val PER: 0.3386
2026-01-11 22:02:55,538: t15.2023.09.29 val PER: 0.3612
2026-01-11 22:02:55,538: t15.2023.10.01 val PER: 0.4306
2026-01-11 22:02:55,539: t15.2023.10.06 val PER: 0.3132
2026-01-11 22:02:55,539: t15.2023.10.08 val PER: 0.4696
2026-01-11 22:02:55,539: t15.2023.10.13 val PER: 0.4740
2026-01-11 22:02:55,539: t15.2023.10.15 val PER: 0.3896
2026-01-11 22:02:55,539: t15.2023.10.20 val PER: 0.3926
2026-01-11 22:02:55,539: t15.2023.10.22 val PER: 0.3352
2026-01-11 22:02:55,539: t15.2023.11.03 val PER: 0.3691
2026-01-11 22:02:55,539: t15.2023.11.04 val PER: 0.1570
2026-01-11 22:02:55,539: t15.2023.11.17 val PER: 0.2473
2026-01-11 22:02:55,539: t15.2023.11.19 val PER: 0.2236
2026-01-11 22:02:55,539: t15.2023.11.26 val PER: 0.4377
2026-01-11 22:02:55,539: t15.2023.12.03 val PER: 0.3561
2026-01-11 22:02:55,540: t15.2023.12.08 val PER: 0.3802
2026-01-11 22:02:55,540: t15.2023.12.10 val PER: 0.3482
2026-01-11 22:02:55,540: t15.2023.12.17 val PER: 0.3514
2026-01-11 22:02:55,540: t15.2023.12.29 val PER: 0.3885
2026-01-11 22:02:55,540: t15.2024.02.25 val PER: 0.3244
2026-01-11 22:02:55,540: t15.2024.03.08 val PER: 0.4182
2026-01-11 22:02:55,540: t15.2024.03.15 val PER: 0.4040
2026-01-11 22:02:55,540: t15.2024.03.17 val PER: 0.3675
2026-01-11 22:02:55,540: t15.2024.05.10 val PER: 0.3774
2026-01-11 22:02:55,540: t15.2024.06.14 val PER: 0.3565
2026-01-11 22:02:55,540: t15.2024.07.19 val PER: 0.4285
2026-01-11 22:02:55,540: t15.2024.07.21 val PER: 0.3386
2026-01-11 22:02:55,541: t15.2024.07.28 val PER: 0.3757
2026-01-11 22:02:55,541: t15.2025.01.10 val PER: 0.4862
2026-01-11 22:02:55,541: t15.2025.01.12 val PER: 0.3680
2026-01-11 22:02:55,541: t15.2025.03.14 val PER: 0.4926
2026-01-11 22:02:55,541: t15.2025.03.16 val PER: 0.4162
2026-01-11 22:02:55,541: t15.2025.03.30 val PER: 0.4644
2026-01-11 22:02:55,541: t15.2025.04.13 val PER: 0.4294
2026-01-11 22:02:55,542: New best val WER(5gram) 71.84% --> 69.17%
2026-01-11 22:02:55,700: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_6000
2026-01-11 22:03:14,643: Train batch 6200: loss: 53.70 grad norm: 100.71 time: 0.079
2026-01-11 22:03:33,940: Train batch 6400: loss: 60.95 grad norm: 87.90 time: 0.077
2026-01-11 22:03:43,413: Running test after training batch: 6500
2026-01-11 22:03:43,562: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:03:49,557: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is we
2026-01-11 22:03:49,702: WER debug example
  GT : how does it keep the cost down
  PR : i do see it go the way the
2026-01-11 22:04:22,231: Val batch 6500: PER (avg): 0.3618 CTC Loss (avg): 59.5268 WER(5gram): 66.17% (n=256) time: 38.818
2026-01-11 22:04:22,232: WER lens: avg_true_words=5.99 avg_pred_words=5.32 max_pred_words=12
2026-01-11 22:04:22,232: t15.2023.08.13 val PER: 0.3316
2026-01-11 22:04:22,232: t15.2023.08.18 val PER: 0.3269
2026-01-11 22:04:22,232: t15.2023.08.20 val PER: 0.2939
2026-01-11 22:04:22,232: t15.2023.08.25 val PER: 0.3057
2026-01-11 22:04:22,232: t15.2023.08.27 val PER: 0.4051
2026-01-11 22:04:22,233: t15.2023.09.01 val PER: 0.2695
2026-01-11 22:04:22,233: t15.2023.09.03 val PER: 0.3610
2026-01-11 22:04:22,233: t15.2023.09.24 val PER: 0.3058
2026-01-11 22:04:22,233: t15.2023.09.29 val PER: 0.3414
2026-01-11 22:04:22,233: t15.2023.10.01 val PER: 0.4042
2026-01-11 22:04:22,233: t15.2023.10.06 val PER: 0.3046
2026-01-11 22:04:22,233: t15.2023.10.08 val PER: 0.4547
2026-01-11 22:04:22,233: t15.2023.10.13 val PER: 0.4593
2026-01-11 22:04:22,233: t15.2023.10.15 val PER: 0.3718
2026-01-11 22:04:22,233: t15.2023.10.20 val PER: 0.3792
2026-01-11 22:04:22,233: t15.2023.10.22 val PER: 0.3196
2026-01-11 22:04:22,233: t15.2023.11.03 val PER: 0.3602
2026-01-11 22:04:22,234: t15.2023.11.04 val PER: 0.1331
2026-01-11 22:04:22,234: t15.2023.11.17 val PER: 0.2302
2026-01-11 22:04:22,234: t15.2023.11.19 val PER: 0.2036
2026-01-11 22:04:22,234: t15.2023.11.26 val PER: 0.4167
2026-01-11 22:04:22,234: t15.2023.12.03 val PER: 0.3582
2026-01-11 22:04:22,234: t15.2023.12.08 val PER: 0.3722
2026-01-11 22:04:22,234: t15.2023.12.10 val PER: 0.3377
2026-01-11 22:04:22,234: t15.2023.12.17 val PER: 0.3462
2026-01-11 22:04:22,234: t15.2023.12.29 val PER: 0.3727
2026-01-11 22:04:22,234: t15.2024.02.25 val PER: 0.2893
2026-01-11 22:04:22,234: t15.2024.03.08 val PER: 0.4011
2026-01-11 22:04:22,234: t15.2024.03.15 val PER: 0.3965
2026-01-11 22:04:22,234: t15.2024.03.17 val PER: 0.3612
2026-01-11 22:04:22,234: t15.2024.05.10 val PER: 0.3759
2026-01-11 22:04:22,235: t15.2024.06.14 val PER: 0.3344
2026-01-11 22:04:22,235: t15.2024.07.19 val PER: 0.4179
2026-01-11 22:04:22,235: t15.2024.07.21 val PER: 0.3179
2026-01-11 22:04:22,235: t15.2024.07.28 val PER: 0.3603
2026-01-11 22:04:22,235: t15.2025.01.10 val PER: 0.4601
2026-01-11 22:04:22,235: t15.2025.01.12 val PER: 0.3564
2026-01-11 22:04:22,235: t15.2025.03.14 val PER: 0.4704
2026-01-11 22:04:22,235: t15.2025.03.16 val PER: 0.3992
2026-01-11 22:04:22,235: t15.2025.03.30 val PER: 0.4690
2026-01-11 22:04:22,235: t15.2025.04.13 val PER: 0.4080
2026-01-11 22:04:22,236: New best val WER(5gram) 69.17% --> 66.17%
2026-01-11 22:04:22,403: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_6500
2026-01-11 22:04:31,847: Train batch 6600: loss: 37.90 grad norm: 64.07 time: 0.056
2026-01-11 22:04:51,162: Train batch 6800: loss: 48.64 grad norm: 73.92 time: 0.055
2026-01-11 22:05:10,618: Train batch 7000: loss: 52.01 grad norm: 79.07 time: 0.071
2026-01-11 22:05:10,618: Running test after training batch: 7000
2026-01-11 22:05:11,041: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:05:17,839: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is we
2026-01-11 22:05:17,980: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the u
2026-01-11 22:05:48,763: Val batch 7000: PER (avg): 0.3552 CTC Loss (avg): 57.2960 WER(5gram): 66.88% (n=256) time: 38.144
2026-01-11 22:05:48,763: WER lens: avg_true_words=5.99 avg_pred_words=5.23 max_pred_words=11
2026-01-11 22:05:48,763: t15.2023.08.13 val PER: 0.3264
2026-01-11 22:05:48,764: t15.2023.08.18 val PER: 0.3160
2026-01-11 22:05:48,764: t15.2023.08.20 val PER: 0.2923
2026-01-11 22:05:48,764: t15.2023.08.25 val PER: 0.2846
2026-01-11 22:05:48,764: t15.2023.08.27 val PER: 0.3955
2026-01-11 22:05:48,764: t15.2023.09.01 val PER: 0.2646
2026-01-11 22:05:48,764: t15.2023.09.03 val PER: 0.3599
2026-01-11 22:05:48,765: t15.2023.09.24 val PER: 0.3046
2026-01-11 22:05:48,765: t15.2023.09.29 val PER: 0.3363
2026-01-11 22:05:48,765: t15.2023.10.01 val PER: 0.3996
2026-01-11 22:05:48,765: t15.2023.10.06 val PER: 0.2928
2026-01-11 22:05:48,765: t15.2023.10.08 val PER: 0.4317
2026-01-11 22:05:48,765: t15.2023.10.13 val PER: 0.4600
2026-01-11 22:05:48,765: t15.2023.10.15 val PER: 0.3645
2026-01-11 22:05:48,765: t15.2023.10.20 val PER: 0.3490
2026-01-11 22:05:48,765: t15.2023.10.22 val PER: 0.3151
2026-01-11 22:05:48,765: t15.2023.11.03 val PER: 0.3480
2026-01-11 22:05:48,765: t15.2023.11.04 val PER: 0.1229
2026-01-11 22:05:48,766: t15.2023.11.17 val PER: 0.2317
2026-01-11 22:05:48,766: t15.2023.11.19 val PER: 0.1956
2026-01-11 22:05:48,766: t15.2023.11.26 val PER: 0.4123
2026-01-11 22:05:48,766: t15.2023.12.03 val PER: 0.3498
2026-01-11 22:05:48,766: t15.2023.12.08 val PER: 0.3688
2026-01-11 22:05:48,766: t15.2023.12.10 val PER: 0.3377
2026-01-11 22:05:48,766: t15.2023.12.17 val PER: 0.3389
2026-01-11 22:05:48,766: t15.2023.12.29 val PER: 0.3747
2026-01-11 22:05:48,766: t15.2024.02.25 val PER: 0.2978
2026-01-11 22:05:48,766: t15.2024.03.08 val PER: 0.4068
2026-01-11 22:05:48,766: t15.2024.03.15 val PER: 0.3821
2026-01-11 22:05:48,767: t15.2024.03.17 val PER: 0.3543
2026-01-11 22:05:48,767: t15.2024.05.10 val PER: 0.3670
2026-01-11 22:05:48,767: t15.2024.06.14 val PER: 0.3249
2026-01-11 22:05:48,767: t15.2024.07.19 val PER: 0.4113
2026-01-11 22:05:48,767: t15.2024.07.21 val PER: 0.3083
2026-01-11 22:05:48,767: t15.2024.07.28 val PER: 0.3544
2026-01-11 22:05:48,767: t15.2025.01.10 val PER: 0.4587
2026-01-11 22:05:48,767: t15.2025.01.12 val PER: 0.3487
2026-01-11 22:05:48,767: t15.2025.03.14 val PER: 0.4615
2026-01-11 22:05:48,767: t15.2025.03.16 val PER: 0.3887
2026-01-11 22:05:48,767: t15.2025.03.30 val PER: 0.4552
2026-01-11 22:05:48,767: t15.2025.04.13 val PER: 0.3980
2026-01-11 22:05:48,920: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_7000
2026-01-11 22:06:07,855: Train batch 7200: loss: 47.62 grad norm: 95.70 time: 0.090
2026-01-11 22:06:27,077: Train batch 7400: loss: 48.35 grad norm: 75.89 time: 0.092
2026-01-11 22:06:36,606: Running test after training batch: 7500
2026-01-11 22:06:36,806: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:06:42,380: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this is why
2026-01-11 22:06:42,503: WER debug example
  GT : how does it keep the cost down
  PR : i see it in the sa new
2026-01-11 22:07:11,891: Val batch 7500: PER (avg): 0.3457 CTC Loss (avg): 55.7199 WER(5gram): 63.75% (n=256) time: 35.284
2026-01-11 22:07:11,892: WER lens: avg_true_words=5.99 avg_pred_words=5.39 max_pred_words=11
2026-01-11 22:07:11,892: t15.2023.08.13 val PER: 0.3181
2026-01-11 22:07:11,892: t15.2023.08.18 val PER: 0.3076
2026-01-11 22:07:11,892: t15.2023.08.20 val PER: 0.2859
2026-01-11 22:07:11,892: t15.2023.08.25 val PER: 0.2831
2026-01-11 22:07:11,892: t15.2023.08.27 val PER: 0.3859
2026-01-11 22:07:11,892: t15.2023.09.01 val PER: 0.2630
2026-01-11 22:07:11,893: t15.2023.09.03 val PER: 0.3420
2026-01-11 22:07:11,893: t15.2023.09.24 val PER: 0.2985
2026-01-11 22:07:11,893: t15.2023.09.29 val PER: 0.3293
2026-01-11 22:07:11,893: t15.2023.10.01 val PER: 0.3904
2026-01-11 22:07:11,893: t15.2023.10.06 val PER: 0.2863
2026-01-11 22:07:11,893: t15.2023.10.08 val PER: 0.4249
2026-01-11 22:07:11,893: t15.2023.10.13 val PER: 0.4476
2026-01-11 22:07:11,894: t15.2023.10.15 val PER: 0.3474
2026-01-11 22:07:11,894: t15.2023.10.20 val PER: 0.3456
2026-01-11 22:07:11,894: t15.2023.10.22 val PER: 0.2951
2026-01-11 22:07:11,894: t15.2023.11.03 val PER: 0.3467
2026-01-11 22:07:11,894: t15.2023.11.04 val PER: 0.1126
2026-01-11 22:07:11,894: t15.2023.11.17 val PER: 0.2193
2026-01-11 22:07:11,894: t15.2023.11.19 val PER: 0.1816
2026-01-11 22:07:11,894: t15.2023.11.26 val PER: 0.4043
2026-01-11 22:07:11,894: t15.2023.12.03 val PER: 0.3351
2026-01-11 22:07:11,894: t15.2023.12.08 val PER: 0.3549
2026-01-11 22:07:11,895: t15.2023.12.10 val PER: 0.3285
2026-01-11 22:07:11,895: t15.2023.12.17 val PER: 0.3410
2026-01-11 22:07:11,895: t15.2023.12.29 val PER: 0.3562
2026-01-11 22:07:11,895: t15.2024.02.25 val PER: 0.2893
2026-01-11 22:07:11,895: t15.2024.03.08 val PER: 0.3969
2026-01-11 22:07:11,895: t15.2024.03.15 val PER: 0.3715
2026-01-11 22:07:11,895: t15.2024.03.17 val PER: 0.3480
2026-01-11 22:07:11,895: t15.2024.05.10 val PER: 0.3522
2026-01-11 22:07:11,895: t15.2024.06.14 val PER: 0.3281
2026-01-11 22:07:11,895: t15.2024.07.19 val PER: 0.3942
2026-01-11 22:07:11,896: t15.2024.07.21 val PER: 0.3048
2026-01-11 22:07:11,896: t15.2024.07.28 val PER: 0.3515
2026-01-11 22:07:11,896: t15.2025.01.10 val PER: 0.4339
2026-01-11 22:07:11,896: t15.2025.01.12 val PER: 0.3380
2026-01-11 22:07:11,896: t15.2025.03.14 val PER: 0.4512
2026-01-11 22:07:11,896: t15.2025.03.16 val PER: 0.3757
2026-01-11 22:07:11,896: t15.2025.03.30 val PER: 0.4437
2026-01-11 22:07:11,896: t15.2025.04.13 val PER: 0.3951
2026-01-11 22:07:11,897: New best val WER(5gram) 66.17% --> 63.75%
2026-01-11 22:07:12,065: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_7500
2026-01-11 22:07:21,668: Train batch 7600: loss: 49.70 grad norm: 95.03 time: 0.076
2026-01-11 22:07:40,873: Train batch 7800: loss: 44.51 grad norm: 100.34 time: 0.063
2026-01-11 22:08:00,514: Train batch 8000: loss: 41.28 grad norm: 74.02 time: 0.080
2026-01-11 22:08:00,514: Running test after training batch: 8000
2026-01-11 22:08:00,624: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:08:06,216: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 22:08:06,361: WER debug example
  GT : how does it keep the cost down
  PR : so it is the key to
2026-01-11 22:08:34,768: Val batch 8000: PER (avg): 0.3415 CTC Loss (avg): 54.2244 WER(5gram): 63.23% (n=256) time: 34.253
2026-01-11 22:08:34,768: WER lens: avg_true_words=5.99 avg_pred_words=5.39 max_pred_words=12
2026-01-11 22:08:34,768: t15.2023.08.13 val PER: 0.3035
2026-01-11 22:08:34,768: t15.2023.08.18 val PER: 0.3076
2026-01-11 22:08:34,769: t15.2023.08.20 val PER: 0.2804
2026-01-11 22:08:34,769: t15.2023.08.25 val PER: 0.2831
2026-01-11 22:08:34,769: t15.2023.08.27 val PER: 0.3730
2026-01-11 22:08:34,769: t15.2023.09.01 val PER: 0.2443
2026-01-11 22:08:34,769: t15.2023.09.03 val PER: 0.3492
2026-01-11 22:08:34,769: t15.2023.09.24 val PER: 0.2864
2026-01-11 22:08:34,769: t15.2023.09.29 val PER: 0.3216
2026-01-11 22:08:34,769: t15.2023.10.01 val PER: 0.3910
2026-01-11 22:08:34,769: t15.2023.10.06 val PER: 0.2788
2026-01-11 22:08:34,769: t15.2023.10.08 val PER: 0.4398
2026-01-11 22:08:34,770: t15.2023.10.13 val PER: 0.4461
2026-01-11 22:08:34,770: t15.2023.10.15 val PER: 0.3467
2026-01-11 22:08:34,770: t15.2023.10.20 val PER: 0.3557
2026-01-11 22:08:34,770: t15.2023.10.22 val PER: 0.2996
2026-01-11 22:08:34,770: t15.2023.11.03 val PER: 0.3433
2026-01-11 22:08:34,770: t15.2023.11.04 val PER: 0.1297
2026-01-11 22:08:34,770: t15.2023.11.17 val PER: 0.2177
2026-01-11 22:08:34,770: t15.2023.11.19 val PER: 0.1717
2026-01-11 22:08:34,770: t15.2023.11.26 val PER: 0.3986
2026-01-11 22:08:34,770: t15.2023.12.03 val PER: 0.3246
2026-01-11 22:08:34,770: t15.2023.12.08 val PER: 0.3562
2026-01-11 22:08:34,771: t15.2023.12.10 val PER: 0.3193
2026-01-11 22:08:34,771: t15.2023.12.17 val PER: 0.3306
2026-01-11 22:08:34,771: t15.2023.12.29 val PER: 0.3521
2026-01-11 22:08:34,771: t15.2024.02.25 val PER: 0.2893
2026-01-11 22:08:34,771: t15.2024.03.08 val PER: 0.3755
2026-01-11 22:08:34,771: t15.2024.03.15 val PER: 0.3721
2026-01-11 22:08:34,771: t15.2024.03.17 val PER: 0.3340
2026-01-11 22:08:34,771: t15.2024.05.10 val PER: 0.3522
2026-01-11 22:08:34,771: t15.2024.06.14 val PER: 0.3249
2026-01-11 22:08:34,771: t15.2024.07.19 val PER: 0.3929
2026-01-11 22:08:34,771: t15.2024.07.21 val PER: 0.2931
2026-01-11 22:08:34,771: t15.2024.07.28 val PER: 0.3382
2026-01-11 22:08:34,771: t15.2025.01.10 val PER: 0.4421
2026-01-11 22:08:34,771: t15.2025.01.12 val PER: 0.3326
2026-01-11 22:08:34,772: t15.2025.03.14 val PER: 0.4527
2026-01-11 22:08:34,772: t15.2025.03.16 val PER: 0.3704
2026-01-11 22:08:34,772: t15.2025.03.30 val PER: 0.4529
2026-01-11 22:08:34,772: t15.2025.04.13 val PER: 0.3909
2026-01-11 22:08:34,773: New best val WER(5gram) 63.75% --> 63.23%
2026-01-11 22:08:34,940: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_8000
2026-01-11 22:08:54,057: Train batch 8200: loss: 32.23 grad norm: 68.27 time: 0.064
2026-01-11 22:09:13,394: Train batch 8400: loss: 37.12 grad norm: 64.54 time: 0.076
2026-01-11 22:09:23,161: Running test after training batch: 8500
2026-01-11 22:09:23,266: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:09:28,824: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this is why
2026-01-11 22:09:28,941: WER debug example
  GT : how does it keep the cost down
  PR : i do see it go the way the
2026-01-11 22:09:54,841: Val batch 8500: PER (avg): 0.3351 CTC Loss (avg): 53.0659 WER(5gram): 59.39% (n=256) time: 31.679
2026-01-11 22:09:54,841: WER lens: avg_true_words=5.99 avg_pred_words=5.59 max_pred_words=11
2026-01-11 22:09:54,842: t15.2023.08.13 val PER: 0.3098
2026-01-11 22:09:54,842: t15.2023.08.18 val PER: 0.2984
2026-01-11 22:09:54,842: t15.2023.08.20 val PER: 0.2748
2026-01-11 22:09:54,842: t15.2023.08.25 val PER: 0.2666
2026-01-11 22:09:54,842: t15.2023.08.27 val PER: 0.3682
2026-01-11 22:09:54,842: t15.2023.09.01 val PER: 0.2459
2026-01-11 22:09:54,842: t15.2023.09.03 val PER: 0.3361
2026-01-11 22:09:54,842: t15.2023.09.24 val PER: 0.2791
2026-01-11 22:09:54,843: t15.2023.09.29 val PER: 0.3153
2026-01-11 22:09:54,843: t15.2023.10.01 val PER: 0.3752
2026-01-11 22:09:54,843: t15.2023.10.06 val PER: 0.2831
2026-01-11 22:09:54,843: t15.2023.10.08 val PER: 0.4290
2026-01-11 22:09:54,843: t15.2023.10.13 val PER: 0.4383
2026-01-11 22:09:54,843: t15.2023.10.15 val PER: 0.3375
2026-01-11 22:09:54,843: t15.2023.10.20 val PER: 0.3456
2026-01-11 22:09:54,843: t15.2023.10.22 val PER: 0.2918
2026-01-11 22:09:54,843: t15.2023.11.03 val PER: 0.3250
2026-01-11 22:09:54,843: t15.2023.11.04 val PER: 0.1024
2026-01-11 22:09:54,843: t15.2023.11.17 val PER: 0.2053
2026-01-11 22:09:54,843: t15.2023.11.19 val PER: 0.1677
2026-01-11 22:09:54,843: t15.2023.11.26 val PER: 0.4014
2026-01-11 22:09:54,843: t15.2023.12.03 val PER: 0.3204
2026-01-11 22:09:54,844: t15.2023.12.08 val PER: 0.3435
2026-01-11 22:09:54,844: t15.2023.12.10 val PER: 0.3049
2026-01-11 22:09:54,844: t15.2023.12.17 val PER: 0.3243
2026-01-11 22:09:54,844: t15.2023.12.29 val PER: 0.3480
2026-01-11 22:09:54,844: t15.2024.02.25 val PER: 0.2879
2026-01-11 22:09:54,844: t15.2024.03.08 val PER: 0.3798
2026-01-11 22:09:54,844: t15.2024.03.15 val PER: 0.3709
2026-01-11 22:09:54,844: t15.2024.03.17 val PER: 0.3340
2026-01-11 22:09:54,844: t15.2024.05.10 val PER: 0.3418
2026-01-11 22:09:54,844: t15.2024.06.14 val PER: 0.3028
2026-01-11 22:09:54,844: t15.2024.07.19 val PER: 0.4021
2026-01-11 22:09:54,844: t15.2024.07.21 val PER: 0.2876
2026-01-11 22:09:54,844: t15.2024.07.28 val PER: 0.3368
2026-01-11 22:09:54,845: t15.2025.01.10 val PER: 0.4421
2026-01-11 22:09:54,845: t15.2025.01.12 val PER: 0.3233
2026-01-11 22:09:54,845: t15.2025.03.14 val PER: 0.4482
2026-01-11 22:09:54,845: t15.2025.03.16 val PER: 0.3626
2026-01-11 22:09:54,845: t15.2025.03.30 val PER: 0.4276
2026-01-11 22:09:54,845: t15.2025.04.13 val PER: 0.3852
2026-01-11 22:09:54,845: New best val WER(5gram) 63.23% --> 59.39%
2026-01-11 22:09:55,008: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_8500
2026-01-11 22:10:04,656: Train batch 8600: loss: 46.70 grad norm: 72.54 time: 0.061
2026-01-11 22:10:23,791: Train batch 8800: loss: 52.63 grad norm: 93.94 time: 0.067
2026-01-11 22:10:43,381: Train batch 9000: loss: 49.84 grad norm: 89.01 time: 0.080
2026-01-11 22:10:43,382: Running test after training batch: 9000
2026-01-11 22:10:43,512: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:10:49,049: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 22:10:49,171: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the car at the
2026-01-11 22:11:13,953: Val batch 9000: PER (avg): 0.3293 CTC Loss (avg): 52.1520 WER(5gram): 60.56% (n=256) time: 30.571
2026-01-11 22:11:13,954: WER lens: avg_true_words=5.99 avg_pred_words=5.66 max_pred_words=12
2026-01-11 22:11:13,954: t15.2023.08.13 val PER: 0.3004
2026-01-11 22:11:13,954: t15.2023.08.18 val PER: 0.2909
2026-01-11 22:11:13,954: t15.2023.08.20 val PER: 0.2637
2026-01-11 22:11:13,954: t15.2023.08.25 val PER: 0.2651
2026-01-11 22:11:13,954: t15.2023.08.27 val PER: 0.3521
2026-01-11 22:11:13,954: t15.2023.09.01 val PER: 0.2354
2026-01-11 22:11:13,954: t15.2023.09.03 val PER: 0.3349
2026-01-11 22:11:13,954: t15.2023.09.24 val PER: 0.2767
2026-01-11 22:11:13,954: t15.2023.09.29 val PER: 0.3076
2026-01-11 22:11:13,955: t15.2023.10.01 val PER: 0.3679
2026-01-11 22:11:13,955: t15.2023.10.06 val PER: 0.2831
2026-01-11 22:11:13,955: t15.2023.10.08 val PER: 0.4154
2026-01-11 22:11:13,955: t15.2023.10.13 val PER: 0.4399
2026-01-11 22:11:13,955: t15.2023.10.15 val PER: 0.3467
2026-01-11 22:11:13,955: t15.2023.10.20 val PER: 0.3389
2026-01-11 22:11:13,955: t15.2023.10.22 val PER: 0.2895
2026-01-11 22:11:13,955: t15.2023.11.03 val PER: 0.3202
2026-01-11 22:11:13,955: t15.2023.11.04 val PER: 0.0922
2026-01-11 22:11:13,955: t15.2023.11.17 val PER: 0.1851
2026-01-11 22:11:13,955: t15.2023.11.19 val PER: 0.1617
2026-01-11 22:11:13,955: t15.2023.11.26 val PER: 0.3986
2026-01-11 22:11:13,956: t15.2023.12.03 val PER: 0.3309
2026-01-11 22:11:13,956: t15.2023.12.08 val PER: 0.3389
2026-01-11 22:11:13,956: t15.2023.12.10 val PER: 0.2996
2026-01-11 22:11:13,956: t15.2023.12.17 val PER: 0.3150
2026-01-11 22:11:13,956: t15.2023.12.29 val PER: 0.3342
2026-01-11 22:11:13,956: t15.2024.02.25 val PER: 0.2753
2026-01-11 22:11:13,956: t15.2024.03.08 val PER: 0.3698
2026-01-11 22:11:13,956: t15.2024.03.15 val PER: 0.3640
2026-01-11 22:11:13,956: t15.2024.03.17 val PER: 0.3278
2026-01-11 22:11:13,956: t15.2024.05.10 val PER: 0.3299
2026-01-11 22:11:13,956: t15.2024.06.14 val PER: 0.3139
2026-01-11 22:11:13,956: t15.2024.07.19 val PER: 0.3935
2026-01-11 22:11:13,956: t15.2024.07.21 val PER: 0.2855
2026-01-11 22:11:13,957: t15.2024.07.28 val PER: 0.3272
2026-01-11 22:11:13,957: t15.2025.01.10 val PER: 0.4421
2026-01-11 22:11:13,957: t15.2025.01.12 val PER: 0.3149
2026-01-11 22:11:13,957: t15.2025.03.14 val PER: 0.4408
2026-01-11 22:11:13,957: t15.2025.03.16 val PER: 0.3586
2026-01-11 22:11:13,957: t15.2025.03.30 val PER: 0.4264
2026-01-11 22:11:13,957: t15.2025.04.13 val PER: 0.3666
2026-01-11 22:11:14,105: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_9000
2026-01-11 22:11:33,043: Train batch 9200: loss: 36.75 grad norm: 63.87 time: 0.066
2026-01-11 22:11:51,886: Train batch 9400: loss: 33.09 grad norm: 74.30 time: 0.075
2026-01-11 22:12:01,480: Running test after training batch: 9500
2026-01-11 22:12:01,589: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:12:07,171: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 22:12:07,287: WER debug example
  GT : how does it keep the cost down
  PR : i do see it in the car at the
2026-01-11 22:12:29,791: Val batch 9500: PER (avg): 0.3224 CTC Loss (avg): 50.7653 WER(5gram): 54.89% (n=256) time: 28.311
2026-01-11 22:12:29,792: WER lens: avg_true_words=5.99 avg_pred_words=5.61 max_pred_words=11
2026-01-11 22:12:29,792: t15.2023.08.13 val PER: 0.2900
2026-01-11 22:12:29,792: t15.2023.08.18 val PER: 0.2867
2026-01-11 22:12:29,792: t15.2023.08.20 val PER: 0.2677
2026-01-11 22:12:29,792: t15.2023.08.25 val PER: 0.2485
2026-01-11 22:12:29,792: t15.2023.08.27 val PER: 0.3424
2026-01-11 22:12:29,793: t15.2023.09.01 val PER: 0.2370
2026-01-11 22:12:29,793: t15.2023.09.03 val PER: 0.3266
2026-01-11 22:12:29,793: t15.2023.09.24 val PER: 0.2718
2026-01-11 22:12:29,793: t15.2023.09.29 val PER: 0.2961
2026-01-11 22:12:29,793: t15.2023.10.01 val PER: 0.3593
2026-01-11 22:12:29,793: t15.2023.10.06 val PER: 0.2573
2026-01-11 22:12:29,793: t15.2023.10.08 val PER: 0.4100
2026-01-11 22:12:29,793: t15.2023.10.13 val PER: 0.4321
2026-01-11 22:12:29,794: t15.2023.10.15 val PER: 0.3322
2026-01-11 22:12:29,794: t15.2023.10.20 val PER: 0.3322
2026-01-11 22:12:29,794: t15.2023.10.22 val PER: 0.2884
2026-01-11 22:12:29,794: t15.2023.11.03 val PER: 0.3250
2026-01-11 22:12:29,794: t15.2023.11.04 val PER: 0.0853
2026-01-11 22:12:29,794: t15.2023.11.17 val PER: 0.1820
2026-01-11 22:12:29,794: t15.2023.11.19 val PER: 0.1497
2026-01-11 22:12:29,794: t15.2023.11.26 val PER: 0.3855
2026-01-11 22:12:29,794: t15.2023.12.03 val PER: 0.3151
2026-01-11 22:12:29,794: t15.2023.12.08 val PER: 0.3329
2026-01-11 22:12:29,794: t15.2023.12.10 val PER: 0.2943
2026-01-11 22:12:29,794: t15.2023.12.17 val PER: 0.3108
2026-01-11 22:12:29,794: t15.2023.12.29 val PER: 0.3260
2026-01-11 22:12:29,795: t15.2024.02.25 val PER: 0.2626
2026-01-11 22:12:29,795: t15.2024.03.08 val PER: 0.3727
2026-01-11 22:12:29,795: t15.2024.03.15 val PER: 0.3558
2026-01-11 22:12:29,795: t15.2024.03.17 val PER: 0.3257
2026-01-11 22:12:29,795: t15.2024.05.10 val PER: 0.3358
2026-01-11 22:12:29,795: t15.2024.06.14 val PER: 0.3044
2026-01-11 22:12:29,795: t15.2024.07.19 val PER: 0.3830
2026-01-11 22:12:29,795: t15.2024.07.21 val PER: 0.2683
2026-01-11 22:12:29,795: t15.2024.07.28 val PER: 0.3228
2026-01-11 22:12:29,795: t15.2025.01.10 val PER: 0.4339
2026-01-11 22:12:29,795: t15.2025.01.12 val PER: 0.3218
2026-01-11 22:12:29,795: t15.2025.03.14 val PER: 0.4157
2026-01-11 22:12:29,795: t15.2025.03.16 val PER: 0.3665
2026-01-11 22:12:29,795: t15.2025.03.30 val PER: 0.4126
2026-01-11 22:12:29,796: t15.2025.04.13 val PER: 0.3638
2026-01-11 22:12:29,796: New best val WER(5gram) 59.39% --> 54.89%
2026-01-11 22:12:29,957: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_9500
2026-01-11 22:12:39,307: Train batch 9600: loss: 30.34 grad norm: 80.01 time: 0.079
2026-01-11 22:12:58,002: Train batch 9800: loss: 45.00 grad norm: 107.27 time: 0.070
2026-01-11 22:13:16,921: Train batch 10000: loss: 25.13 grad norm: 65.81 time: 0.067
2026-01-11 22:13:16,921: Running test after training batch: 10000
2026-01-11 22:13:17,053: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:13:23,385: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 22:13:23,536: WER debug example
  GT : how does it keep the cost down
  PR : i see it go the way the
2026-01-11 22:13:47,573: Val batch 10000: PER (avg): 0.3185 CTC Loss (avg): 49.7239 WER(5gram): 55.02% (n=256) time: 30.652
2026-01-11 22:13:47,574: WER lens: avg_true_words=5.99 avg_pred_words=5.57 max_pred_words=12
2026-01-11 22:13:47,574: t15.2023.08.13 val PER: 0.2775
2026-01-11 22:13:47,574: t15.2023.08.18 val PER: 0.2875
2026-01-11 22:13:47,574: t15.2023.08.20 val PER: 0.2605
2026-01-11 22:13:47,574: t15.2023.08.25 val PER: 0.2560
2026-01-11 22:13:47,574: t15.2023.08.27 val PER: 0.3264
2026-01-11 22:13:47,574: t15.2023.09.01 val PER: 0.2297
2026-01-11 22:13:47,574: t15.2023.09.03 val PER: 0.3195
2026-01-11 22:13:47,575: t15.2023.09.24 val PER: 0.2585
2026-01-11 22:13:47,575: t15.2023.09.29 val PER: 0.2929
2026-01-11 22:13:47,575: t15.2023.10.01 val PER: 0.3620
2026-01-11 22:13:47,575: t15.2023.10.06 val PER: 0.2594
2026-01-11 22:13:47,575: t15.2023.10.08 val PER: 0.4127
2026-01-11 22:13:47,575: t15.2023.10.13 val PER: 0.4267
2026-01-11 22:13:47,575: t15.2023.10.15 val PER: 0.3375
2026-01-11 22:13:47,575: t15.2023.10.20 val PER: 0.3423
2026-01-11 22:13:47,575: t15.2023.10.22 val PER: 0.2806
2026-01-11 22:13:47,575: t15.2023.11.03 val PER: 0.3161
2026-01-11 22:13:47,576: t15.2023.11.04 val PER: 0.0853
2026-01-11 22:13:47,576: t15.2023.11.17 val PER: 0.1820
2026-01-11 22:13:47,576: t15.2023.11.19 val PER: 0.1517
2026-01-11 22:13:47,576: t15.2023.11.26 val PER: 0.3790
2026-01-11 22:13:47,576: t15.2023.12.03 val PER: 0.3099
2026-01-11 22:13:47,576: t15.2023.12.08 val PER: 0.3262
2026-01-11 22:13:47,576: t15.2023.12.10 val PER: 0.2983
2026-01-11 22:13:47,576: t15.2023.12.17 val PER: 0.3150
2026-01-11 22:13:47,576: t15.2023.12.29 val PER: 0.3294
2026-01-11 22:13:47,576: t15.2024.02.25 val PER: 0.2654
2026-01-11 22:13:47,576: t15.2024.03.08 val PER: 0.3656
2026-01-11 22:13:47,576: t15.2024.03.15 val PER: 0.3546
2026-01-11 22:13:47,577: t15.2024.03.17 val PER: 0.3131
2026-01-11 22:13:47,577: t15.2024.05.10 val PER: 0.3150
2026-01-11 22:13:47,577: t15.2024.06.14 val PER: 0.3076
2026-01-11 22:13:47,577: t15.2024.07.19 val PER: 0.3810
2026-01-11 22:13:47,577: t15.2024.07.21 val PER: 0.2683
2026-01-11 22:13:47,577: t15.2024.07.28 val PER: 0.3088
2026-01-11 22:13:47,577: t15.2025.01.10 val PER: 0.4256
2026-01-11 22:13:47,577: t15.2025.01.12 val PER: 0.3095
2026-01-11 22:13:47,577: t15.2025.03.14 val PER: 0.4275
2026-01-11 22:13:47,577: t15.2025.03.16 val PER: 0.3573
2026-01-11 22:13:47,577: t15.2025.03.30 val PER: 0.4103
2026-01-11 22:13:47,577: t15.2025.04.13 val PER: 0.3566
2026-01-11 22:13:47,723: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_10000
2026-01-11 22:14:06,261: Train batch 10200: loss: 29.66 grad norm: 77.00 time: 0.057
2026-01-11 22:14:25,233: Train batch 10400: loss: 35.06 grad norm: 78.80 time: 0.078
2026-01-11 22:14:34,593: Running test after training batch: 10500
2026-01-11 22:14:34,702: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:14:40,425: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 22:14:40,536: WER debug example
  GT : how does it keep the cost down
  PR : how does it cover the cost to
2026-01-11 22:15:01,928: Val batch 10500: PER (avg): 0.3130 CTC Loss (avg): 49.0510 WER(5gram): 52.61% (n=256) time: 27.335
2026-01-11 22:15:01,929: WER lens: avg_true_words=5.99 avg_pred_words=5.60 max_pred_words=12
2026-01-11 22:15:01,929: t15.2023.08.13 val PER: 0.2931
2026-01-11 22:15:01,929: t15.2023.08.18 val PER: 0.2749
2026-01-11 22:15:01,929: t15.2023.08.20 val PER: 0.2542
2026-01-11 22:15:01,929: t15.2023.08.25 val PER: 0.2470
2026-01-11 22:15:01,929: t15.2023.08.27 val PER: 0.3424
2026-01-11 22:15:01,929: t15.2023.09.01 val PER: 0.2273
2026-01-11 22:15:01,929: t15.2023.09.03 val PER: 0.3242
2026-01-11 22:15:01,929: t15.2023.09.24 val PER: 0.2536
2026-01-11 22:15:01,929: t15.2023.09.29 val PER: 0.2840
2026-01-11 22:15:01,930: t15.2023.10.01 val PER: 0.3474
2026-01-11 22:15:01,930: t15.2023.10.06 val PER: 0.2443
2026-01-11 22:15:01,930: t15.2023.10.08 val PER: 0.4032
2026-01-11 22:15:01,930: t15.2023.10.13 val PER: 0.4259
2026-01-11 22:15:01,930: t15.2023.10.15 val PER: 0.3223
2026-01-11 22:15:01,930: t15.2023.10.20 val PER: 0.3356
2026-01-11 22:15:01,930: t15.2023.10.22 val PER: 0.2817
2026-01-11 22:15:01,930: t15.2023.11.03 val PER: 0.3182
2026-01-11 22:15:01,930: t15.2023.11.04 val PER: 0.0887
2026-01-11 22:15:01,930: t15.2023.11.17 val PER: 0.1726
2026-01-11 22:15:01,930: t15.2023.11.19 val PER: 0.1537
2026-01-11 22:15:01,930: t15.2023.11.26 val PER: 0.3732
2026-01-11 22:15:01,931: t15.2023.12.03 val PER: 0.3078
2026-01-11 22:15:01,931: t15.2023.12.08 val PER: 0.3202
2026-01-11 22:15:01,931: t15.2023.12.10 val PER: 0.2812
2026-01-11 22:15:01,931: t15.2023.12.17 val PER: 0.2963
2026-01-11 22:15:01,931: t15.2023.12.29 val PER: 0.3185
2026-01-11 22:15:01,931: t15.2024.02.25 val PER: 0.2570
2026-01-11 22:15:01,931: t15.2024.03.08 val PER: 0.3599
2026-01-11 22:15:01,931: t15.2024.03.15 val PER: 0.3458
2026-01-11 22:15:01,931: t15.2024.03.17 val PER: 0.3208
2026-01-11 22:15:01,931: t15.2024.05.10 val PER: 0.3239
2026-01-11 22:15:01,931: t15.2024.06.14 val PER: 0.2997
2026-01-11 22:15:01,931: t15.2024.07.19 val PER: 0.3711
2026-01-11 22:15:01,931: t15.2024.07.21 val PER: 0.2586
2026-01-11 22:15:01,931: t15.2024.07.28 val PER: 0.3096
2026-01-11 22:15:01,931: t15.2025.01.10 val PER: 0.4091
2026-01-11 22:15:01,932: t15.2025.01.12 val PER: 0.3133
2026-01-11 22:15:01,932: t15.2025.03.14 val PER: 0.4098
2026-01-11 22:15:01,932: t15.2025.03.16 val PER: 0.3312
2026-01-11 22:15:01,932: t15.2025.03.30 val PER: 0.4034
2026-01-11 22:15:01,932: t15.2025.04.13 val PER: 0.3766
2026-01-11 22:15:01,932: New best val WER(5gram) 54.89% --> 52.61%
2026-01-11 22:15:02,084: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_10500
2026-01-11 22:15:11,615: Train batch 10600: loss: 33.76 grad norm: 149.08 time: 0.077
2026-01-11 22:15:30,474: Train batch 10800: loss: 47.74 grad norm: 87.69 time: 0.078
2026-01-11 22:15:49,437: Train batch 11000: loss: 50.17 grad norm: 79.82 time: 0.070
2026-01-11 22:15:49,437: Running test after training batch: 11000
2026-01-11 22:15:49,566: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:15:55,397: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 22:15:55,501: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the car at a
2026-01-11 22:16:16,110: Val batch 11000: PER (avg): 0.3096 CTC Loss (avg): 48.2453 WER(5gram): 53.06% (n=256) time: 26.672
2026-01-11 22:16:16,110: WER lens: avg_true_words=5.99 avg_pred_words=5.66 max_pred_words=12
2026-01-11 22:16:16,111: t15.2023.08.13 val PER: 0.2692
2026-01-11 22:16:16,111: t15.2023.08.18 val PER: 0.2724
2026-01-11 22:16:16,111: t15.2023.08.20 val PER: 0.2566
2026-01-11 22:16:16,111: t15.2023.08.25 val PER: 0.2395
2026-01-11 22:16:16,111: t15.2023.08.27 val PER: 0.3312
2026-01-11 22:16:16,111: t15.2023.09.01 val PER: 0.2183
2026-01-11 22:16:16,111: t15.2023.09.03 val PER: 0.3171
2026-01-11 22:16:16,111: t15.2023.09.24 val PER: 0.2415
2026-01-11 22:16:16,111: t15.2023.09.29 val PER: 0.2846
2026-01-11 22:16:16,112: t15.2023.10.01 val PER: 0.3474
2026-01-11 22:16:16,112: t15.2023.10.06 val PER: 0.2443
2026-01-11 22:16:16,112: t15.2023.10.08 val PER: 0.3897
2026-01-11 22:16:16,112: t15.2023.10.13 val PER: 0.4267
2026-01-11 22:16:16,112: t15.2023.10.15 val PER: 0.3197
2026-01-11 22:16:16,112: t15.2023.10.20 val PER: 0.3322
2026-01-11 22:16:16,112: t15.2023.10.22 val PER: 0.2795
2026-01-11 22:16:16,112: t15.2023.11.03 val PER: 0.3107
2026-01-11 22:16:16,112: t15.2023.11.04 val PER: 0.0922
2026-01-11 22:16:16,113: t15.2023.11.17 val PER: 0.1820
2026-01-11 22:16:16,113: t15.2023.11.19 val PER: 0.1517
2026-01-11 22:16:16,113: t15.2023.11.26 val PER: 0.3667
2026-01-11 22:16:16,113: t15.2023.12.03 val PER: 0.2962
2026-01-11 22:16:16,113: t15.2023.12.08 val PER: 0.3209
2026-01-11 22:16:16,113: t15.2023.12.10 val PER: 0.2825
2026-01-11 22:16:16,113: t15.2023.12.17 val PER: 0.3035
2026-01-11 22:16:16,113: t15.2023.12.29 val PER: 0.3089
2026-01-11 22:16:16,113: t15.2024.02.25 val PER: 0.2598
2026-01-11 22:16:16,113: t15.2024.03.08 val PER: 0.3556
2026-01-11 22:16:16,113: t15.2024.03.15 val PER: 0.3477
2026-01-11 22:16:16,113: t15.2024.03.17 val PER: 0.3054
2026-01-11 22:16:16,113: t15.2024.05.10 val PER: 0.3165
2026-01-11 22:16:16,114: t15.2024.06.14 val PER: 0.3013
2026-01-11 22:16:16,114: t15.2024.07.19 val PER: 0.3678
2026-01-11 22:16:16,114: t15.2024.07.21 val PER: 0.2586
2026-01-11 22:16:16,114: t15.2024.07.28 val PER: 0.3066
2026-01-11 22:16:16,114: t15.2025.01.10 val PER: 0.4146
2026-01-11 22:16:16,114: t15.2025.01.12 val PER: 0.3010
2026-01-11 22:16:16,114: t15.2025.03.14 val PER: 0.4231
2026-01-11 22:16:16,114: t15.2025.03.16 val PER: 0.3482
2026-01-11 22:16:16,114: t15.2025.03.30 val PER: 0.3966
2026-01-11 22:16:16,114: t15.2025.04.13 val PER: 0.3666
2026-01-11 22:16:16,262: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_11000
2026-01-11 22:16:34,670: Train batch 11200: loss: 38.54 grad norm: 86.82 time: 0.078
2026-01-11 22:16:53,318: Train batch 11400: loss: 36.06 grad norm: 87.47 time: 0.068
2026-01-11 22:17:02,838: Running test after training batch: 11500
2026-01-11 22:17:02,951: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:17:08,551: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 22:17:08,654: WER debug example
  GT : how does it keep the cost down
  PR : how does it cover the cost
2026-01-11 22:17:29,172: Val batch 11500: PER (avg): 0.3050 CTC Loss (avg): 47.8779 WER(5gram): 49.09% (n=256) time: 26.333
2026-01-11 22:17:29,173: WER lens: avg_true_words=5.99 avg_pred_words=5.81 max_pred_words=12
2026-01-11 22:17:29,173: t15.2023.08.13 val PER: 0.2703
2026-01-11 22:17:29,173: t15.2023.08.18 val PER: 0.2733
2026-01-11 22:17:29,173: t15.2023.08.20 val PER: 0.2486
2026-01-11 22:17:29,173: t15.2023.08.25 val PER: 0.2395
2026-01-11 22:17:29,173: t15.2023.08.27 val PER: 0.3344
2026-01-11 22:17:29,174: t15.2023.09.01 val PER: 0.2159
2026-01-11 22:17:29,174: t15.2023.09.03 val PER: 0.3100
2026-01-11 22:17:29,174: t15.2023.09.24 val PER: 0.2476
2026-01-11 22:17:29,174: t15.2023.09.29 val PER: 0.2846
2026-01-11 22:17:29,174: t15.2023.10.01 val PER: 0.3461
2026-01-11 22:17:29,174: t15.2023.10.06 val PER: 0.2400
2026-01-11 22:17:29,174: t15.2023.10.08 val PER: 0.3829
2026-01-11 22:17:29,174: t15.2023.10.13 val PER: 0.4135
2026-01-11 22:17:29,174: t15.2023.10.15 val PER: 0.3151
2026-01-11 22:17:29,174: t15.2023.10.20 val PER: 0.3221
2026-01-11 22:17:29,174: t15.2023.10.22 val PER: 0.2706
2026-01-11 22:17:29,175: t15.2023.11.03 val PER: 0.3073
2026-01-11 22:17:29,175: t15.2023.11.04 val PER: 0.0887
2026-01-11 22:17:29,175: t15.2023.11.17 val PER: 0.1680
2026-01-11 22:17:29,175: t15.2023.11.19 val PER: 0.1417
2026-01-11 22:17:29,175: t15.2023.11.26 val PER: 0.3667
2026-01-11 22:17:29,175: t15.2023.12.03 val PER: 0.3036
2026-01-11 22:17:29,175: t15.2023.12.08 val PER: 0.3076
2026-01-11 22:17:29,175: t15.2023.12.10 val PER: 0.2641
2026-01-11 22:17:29,175: t15.2023.12.17 val PER: 0.2921
2026-01-11 22:17:29,175: t15.2023.12.29 val PER: 0.3082
2026-01-11 22:17:29,175: t15.2024.02.25 val PER: 0.2500
2026-01-11 22:17:29,175: t15.2024.03.08 val PER: 0.3585
2026-01-11 22:17:29,175: t15.2024.03.15 val PER: 0.3421
2026-01-11 22:17:29,175: t15.2024.03.17 val PER: 0.3026
2026-01-11 22:17:29,176: t15.2024.05.10 val PER: 0.3150
2026-01-11 22:17:29,176: t15.2024.06.14 val PER: 0.2934
2026-01-11 22:17:29,176: t15.2024.07.19 val PER: 0.3659
2026-01-11 22:17:29,176: t15.2024.07.21 val PER: 0.2545
2026-01-11 22:17:29,176: t15.2024.07.28 val PER: 0.2978
2026-01-11 22:17:29,176: t15.2025.01.10 val PER: 0.4201
2026-01-11 22:17:29,176: t15.2025.01.12 val PER: 0.3064
2026-01-11 22:17:29,176: t15.2025.03.14 val PER: 0.4216
2026-01-11 22:17:29,176: t15.2025.03.16 val PER: 0.3259
2026-01-11 22:17:29,176: t15.2025.03.30 val PER: 0.3782
2026-01-11 22:17:29,176: t15.2025.04.13 val PER: 0.3581
2026-01-11 22:17:29,177: New best val WER(5gram) 52.61% --> 49.09%
2026-01-11 22:17:29,335: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_11500
2026-01-11 22:17:38,465: Train batch 11600: loss: 40.68 grad norm: 87.92 time: 0.068
2026-01-11 22:17:56,866: Train batch 11800: loss: 27.24 grad norm: 74.46 time: 0.050
2026-01-11 22:18:15,492: Train batch 12000: loss: 43.04 grad norm: 94.64 time: 0.077
2026-01-11 22:18:15,493: Running test after training batch: 12000
2026-01-11 22:18:15,595: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:18:21,237: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 22:18:21,344: WER debug example
  GT : how does it keep the cost down
  PR : i do see it in the car at the
2026-01-11 22:18:40,571: Val batch 12000: PER (avg): 0.3023 CTC Loss (avg): 46.9319 WER(5gram): 51.30% (n=256) time: 25.078
2026-01-11 22:18:40,572: WER lens: avg_true_words=5.99 avg_pred_words=5.79 max_pred_words=11
2026-01-11 22:18:40,572: t15.2023.08.13 val PER: 0.2630
2026-01-11 22:18:40,572: t15.2023.08.18 val PER: 0.2741
2026-01-11 22:18:40,572: t15.2023.08.20 val PER: 0.2502
2026-01-11 22:18:40,572: t15.2023.08.25 val PER: 0.2319
2026-01-11 22:18:40,572: t15.2023.08.27 val PER: 0.3376
2026-01-11 22:18:40,572: t15.2023.09.01 val PER: 0.2135
2026-01-11 22:18:40,572: t15.2023.09.03 val PER: 0.3112
2026-01-11 22:18:40,572: t15.2023.09.24 val PER: 0.2391
2026-01-11 22:18:40,572: t15.2023.09.29 val PER: 0.2827
2026-01-11 22:18:40,573: t15.2023.10.01 val PER: 0.3435
2026-01-11 22:18:40,573: t15.2023.10.06 val PER: 0.2379
2026-01-11 22:18:40,573: t15.2023.10.08 val PER: 0.3721
2026-01-11 22:18:40,573: t15.2023.10.13 val PER: 0.4182
2026-01-11 22:18:40,573: t15.2023.10.15 val PER: 0.3177
2026-01-11 22:18:40,573: t15.2023.10.20 val PER: 0.3255
2026-01-11 22:18:40,573: t15.2023.10.22 val PER: 0.2706
2026-01-11 22:18:40,573: t15.2023.11.03 val PER: 0.3033
2026-01-11 22:18:40,573: t15.2023.11.04 val PER: 0.0853
2026-01-11 22:18:40,574: t15.2023.11.17 val PER: 0.1664
2026-01-11 22:18:40,574: t15.2023.11.19 val PER: 0.1397
2026-01-11 22:18:40,574: t15.2023.11.26 val PER: 0.3667
2026-01-11 22:18:40,574: t15.2023.12.03 val PER: 0.2878
2026-01-11 22:18:40,574: t15.2023.12.08 val PER: 0.3076
2026-01-11 22:18:40,574: t15.2023.12.10 val PER: 0.2720
2026-01-11 22:18:40,574: t15.2023.12.17 val PER: 0.2796
2026-01-11 22:18:40,574: t15.2023.12.29 val PER: 0.3054
2026-01-11 22:18:40,574: t15.2024.02.25 val PER: 0.2556
2026-01-11 22:18:40,574: t15.2024.03.08 val PER: 0.3457
2026-01-11 22:18:40,575: t15.2024.03.15 val PER: 0.3415
2026-01-11 22:18:40,575: t15.2024.03.17 val PER: 0.2992
2026-01-11 22:18:40,575: t15.2024.05.10 val PER: 0.2897
2026-01-11 22:18:40,575: t15.2024.06.14 val PER: 0.2886
2026-01-11 22:18:40,575: t15.2024.07.19 val PER: 0.3619
2026-01-11 22:18:40,575: t15.2024.07.21 val PER: 0.2483
2026-01-11 22:18:40,575: t15.2024.07.28 val PER: 0.3000
2026-01-11 22:18:40,575: t15.2025.01.10 val PER: 0.4105
2026-01-11 22:18:40,575: t15.2025.01.12 val PER: 0.3048
2026-01-11 22:18:40,575: t15.2025.03.14 val PER: 0.4068
2026-01-11 22:18:40,576: t15.2025.03.16 val PER: 0.3272
2026-01-11 22:18:40,576: t15.2025.03.30 val PER: 0.3862
2026-01-11 22:18:40,576: t15.2025.04.13 val PER: 0.3581
2026-01-11 22:18:40,723: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_12000
2026-01-11 22:18:59,120: Train batch 12200: loss: 29.06 grad norm: 60.78 time: 0.072
2026-01-11 22:19:17,407: Train batch 12400: loss: 22.20 grad norm: 70.74 time: 0.050
2026-01-11 22:19:26,785: Running test after training batch: 12500
2026-01-11 22:19:26,924: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:19:32,439: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 22:19:32,534: WER debug example
  GT : how does it keep the cost down
  PR : how does it cover the cost and
2026-01-11 22:19:51,880: Val batch 12500: PER (avg): 0.2981 CTC Loss (avg): 46.4472 WER(5gram): 46.68% (n=256) time: 25.095
2026-01-11 22:19:51,881: WER lens: avg_true_words=5.99 avg_pred_words=5.72 max_pred_words=11
2026-01-11 22:19:51,881: t15.2023.08.13 val PER: 0.2661
2026-01-11 22:19:51,881: t15.2023.08.18 val PER: 0.2632
2026-01-11 22:19:51,881: t15.2023.08.20 val PER: 0.2446
2026-01-11 22:19:51,881: t15.2023.08.25 val PER: 0.2319
2026-01-11 22:19:51,881: t15.2023.08.27 val PER: 0.3183
2026-01-11 22:19:51,881: t15.2023.09.01 val PER: 0.2086
2026-01-11 22:19:51,881: t15.2023.09.03 val PER: 0.3040
2026-01-11 22:19:51,882: t15.2023.09.24 val PER: 0.2354
2026-01-11 22:19:51,882: t15.2023.09.29 val PER: 0.2789
2026-01-11 22:19:51,882: t15.2023.10.01 val PER: 0.3362
2026-01-11 22:19:51,882: t15.2023.10.06 val PER: 0.2228
2026-01-11 22:19:51,882: t15.2023.10.08 val PER: 0.3762
2026-01-11 22:19:51,882: t15.2023.10.13 val PER: 0.4135
2026-01-11 22:19:51,882: t15.2023.10.15 val PER: 0.3111
2026-01-11 22:19:51,882: t15.2023.10.20 val PER: 0.3154
2026-01-11 22:19:51,882: t15.2023.10.22 val PER: 0.2717
2026-01-11 22:19:51,882: t15.2023.11.03 val PER: 0.3060
2026-01-11 22:19:51,882: t15.2023.11.04 val PER: 0.0751
2026-01-11 22:19:51,882: t15.2023.11.17 val PER: 0.1555
2026-01-11 22:19:51,882: t15.2023.11.19 val PER: 0.1377
2026-01-11 22:19:51,883: t15.2023.11.26 val PER: 0.3565
2026-01-11 22:19:51,883: t15.2023.12.03 val PER: 0.2836
2026-01-11 22:19:51,883: t15.2023.12.08 val PER: 0.3036
2026-01-11 22:19:51,883: t15.2023.12.10 val PER: 0.2720
2026-01-11 22:19:51,883: t15.2023.12.17 val PER: 0.2796
2026-01-11 22:19:51,883: t15.2023.12.29 val PER: 0.2958
2026-01-11 22:19:51,883: t15.2024.02.25 val PER: 0.2458
2026-01-11 22:19:51,883: t15.2024.03.08 val PER: 0.3542
2026-01-11 22:19:51,883: t15.2024.03.15 val PER: 0.3327
2026-01-11 22:19:51,883: t15.2024.03.17 val PER: 0.2950
2026-01-11 22:19:51,883: t15.2024.05.10 val PER: 0.2987
2026-01-11 22:19:51,883: t15.2024.06.14 val PER: 0.2871
2026-01-11 22:19:51,884: t15.2024.07.19 val PER: 0.3619
2026-01-11 22:19:51,884: t15.2024.07.21 val PER: 0.2421
2026-01-11 22:19:51,884: t15.2024.07.28 val PER: 0.2934
2026-01-11 22:19:51,884: t15.2025.01.10 val PER: 0.4187
2026-01-11 22:19:51,884: t15.2025.01.12 val PER: 0.2964
2026-01-11 22:19:51,884: t15.2025.03.14 val PER: 0.4009
2026-01-11 22:19:51,884: t15.2025.03.16 val PER: 0.3246
2026-01-11 22:19:51,884: t15.2025.03.30 val PER: 0.3977
2026-01-11 22:19:51,884: t15.2025.04.13 val PER: 0.3538
2026-01-11 22:19:51,885: New best val WER(5gram) 49.09% --> 46.68%
2026-01-11 22:19:52,032: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_12500
2026-01-11 22:20:01,328: Train batch 12600: loss: 29.90 grad norm: 96.25 time: 0.063
2026-01-11 22:20:20,069: Train batch 12800: loss: 25.73 grad norm: 64.76 time: 0.062
2026-01-11 22:20:38,907: Train batch 13000: loss: 25.47 grad norm: 65.41 time: 0.073
2026-01-11 22:20:38,907: Running test after training batch: 13000
2026-01-11 22:20:39,013: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:20:44,475: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 22:20:44,564: WER debug example
  GT : how does it keep the cost down
  PR : how does it cover the costs and
2026-01-11 22:21:03,610: Val batch 13000: PER (avg): 0.2977 CTC Loss (avg): 46.0999 WER(5gram): 47.52% (n=256) time: 24.703
2026-01-11 22:21:03,611: WER lens: avg_true_words=5.99 avg_pred_words=5.70 max_pred_words=11
2026-01-11 22:21:03,611: t15.2023.08.13 val PER: 0.2609
2026-01-11 22:21:03,611: t15.2023.08.18 val PER: 0.2649
2026-01-11 22:21:03,611: t15.2023.08.20 val PER: 0.2399
2026-01-11 22:21:03,611: t15.2023.08.25 val PER: 0.2349
2026-01-11 22:21:03,612: t15.2023.08.27 val PER: 0.3344
2026-01-11 22:21:03,612: t15.2023.09.01 val PER: 0.2078
2026-01-11 22:21:03,612: t15.2023.09.03 val PER: 0.3017
2026-01-11 22:21:03,612: t15.2023.09.24 val PER: 0.2342
2026-01-11 22:21:03,612: t15.2023.09.29 val PER: 0.2821
2026-01-11 22:21:03,612: t15.2023.10.01 val PER: 0.3303
2026-01-11 22:21:03,612: t15.2023.10.06 val PER: 0.2314
2026-01-11 22:21:03,612: t15.2023.10.08 val PER: 0.3911
2026-01-11 22:21:03,612: t15.2023.10.13 val PER: 0.4127
2026-01-11 22:21:03,612: t15.2023.10.15 val PER: 0.3151
2026-01-11 22:21:03,612: t15.2023.10.20 val PER: 0.3087
2026-01-11 22:21:03,612: t15.2023.10.22 val PER: 0.2706
2026-01-11 22:21:03,613: t15.2023.11.03 val PER: 0.3026
2026-01-11 22:21:03,613: t15.2023.11.04 val PER: 0.0717
2026-01-11 22:21:03,613: t15.2023.11.17 val PER: 0.1602
2026-01-11 22:21:03,613: t15.2023.11.19 val PER: 0.1417
2026-01-11 22:21:03,613: t15.2023.11.26 val PER: 0.3572
2026-01-11 22:21:03,613: t15.2023.12.03 val PER: 0.2899
2026-01-11 22:21:03,613: t15.2023.12.08 val PER: 0.2996
2026-01-11 22:21:03,613: t15.2023.12.10 val PER: 0.2628
2026-01-11 22:21:03,613: t15.2023.12.17 val PER: 0.2848
2026-01-11 22:21:03,613: t15.2023.12.29 val PER: 0.3027
2026-01-11 22:21:03,613: t15.2024.02.25 val PER: 0.2360
2026-01-11 22:21:03,613: t15.2024.03.08 val PER: 0.3385
2026-01-11 22:21:03,613: t15.2024.03.15 val PER: 0.3352
2026-01-11 22:21:03,613: t15.2024.03.17 val PER: 0.2978
2026-01-11 22:21:03,614: t15.2024.05.10 val PER: 0.3091
2026-01-11 22:21:03,614: t15.2024.06.14 val PER: 0.2808
2026-01-11 22:21:03,614: t15.2024.07.19 val PER: 0.3586
2026-01-11 22:21:03,614: t15.2024.07.21 val PER: 0.2434
2026-01-11 22:21:03,614: t15.2024.07.28 val PER: 0.2846
2026-01-11 22:21:03,614: t15.2025.01.10 val PER: 0.4187
2026-01-11 22:21:03,614: t15.2025.01.12 val PER: 0.2987
2026-01-11 22:21:03,614: t15.2025.03.14 val PER: 0.4009
2026-01-11 22:21:03,614: t15.2025.03.16 val PER: 0.3194
2026-01-11 22:21:03,614: t15.2025.03.30 val PER: 0.3828
2026-01-11 22:21:03,614: t15.2025.04.13 val PER: 0.3452
2026-01-11 22:21:03,764: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_13000
2026-01-11 22:21:22,274: Train batch 13200: loss: 45.99 grad norm: 93.94 time: 0.061
2026-01-11 22:21:40,801: Train batch 13400: loss: 30.41 grad norm: 80.25 time: 0.069
2026-01-11 22:21:50,157: Running test after training batch: 13500
2026-01-11 22:21:50,286: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:21:56,176: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 22:21:56,268: WER debug example
  GT : how does it keep the cost down
  PR : how does it cover the cost to
2026-01-11 22:22:15,011: Val batch 13500: PER (avg): 0.2922 CTC Loss (avg): 45.3806 WER(5gram): 46.35% (n=256) time: 24.854
2026-01-11 22:22:15,012: WER lens: avg_true_words=5.99 avg_pred_words=5.82 max_pred_words=12
2026-01-11 22:22:15,012: t15.2023.08.13 val PER: 0.2557
2026-01-11 22:22:15,012: t15.2023.08.18 val PER: 0.2540
2026-01-11 22:22:15,012: t15.2023.08.20 val PER: 0.2383
2026-01-11 22:22:15,012: t15.2023.08.25 val PER: 0.2244
2026-01-11 22:22:15,013: t15.2023.08.27 val PER: 0.3312
2026-01-11 22:22:15,013: t15.2023.09.01 val PER: 0.2021
2026-01-11 22:22:15,013: t15.2023.09.03 val PER: 0.2933
2026-01-11 22:22:15,013: t15.2023.09.24 val PER: 0.2306
2026-01-11 22:22:15,013: t15.2023.09.29 val PER: 0.2757
2026-01-11 22:22:15,013: t15.2023.10.01 val PER: 0.3336
2026-01-11 22:22:15,013: t15.2023.10.06 val PER: 0.2250
2026-01-11 22:22:15,013: t15.2023.10.08 val PER: 0.3748
2026-01-11 22:22:15,013: t15.2023.10.13 val PER: 0.4042
2026-01-11 22:22:15,014: t15.2023.10.15 val PER: 0.3013
2026-01-11 22:22:15,014: t15.2023.10.20 val PER: 0.3087
2026-01-11 22:22:15,014: t15.2023.10.22 val PER: 0.2639
2026-01-11 22:22:15,014: t15.2023.11.03 val PER: 0.2978
2026-01-11 22:22:15,014: t15.2023.11.04 val PER: 0.0785
2026-01-11 22:22:15,014: t15.2023.11.17 val PER: 0.1509
2026-01-11 22:22:15,014: t15.2023.11.19 val PER: 0.1337
2026-01-11 22:22:15,014: t15.2023.11.26 val PER: 0.3543
2026-01-11 22:22:15,014: t15.2023.12.03 val PER: 0.2794
2026-01-11 22:22:15,014: t15.2023.12.08 val PER: 0.2969
2026-01-11 22:22:15,015: t15.2023.12.10 val PER: 0.2681
2026-01-11 22:22:15,015: t15.2023.12.17 val PER: 0.2744
2026-01-11 22:22:15,015: t15.2023.12.29 val PER: 0.2862
2026-01-11 22:22:15,015: t15.2024.02.25 val PER: 0.2360
2026-01-11 22:22:15,015: t15.2024.03.08 val PER: 0.3357
2026-01-11 22:22:15,015: t15.2024.03.15 val PER: 0.3233
2026-01-11 22:22:15,015: t15.2024.03.17 val PER: 0.2915
2026-01-11 22:22:15,015: t15.2024.05.10 val PER: 0.2927
2026-01-11 22:22:15,015: t15.2024.06.14 val PER: 0.2823
2026-01-11 22:22:15,015: t15.2024.07.19 val PER: 0.3553
2026-01-11 22:22:15,016: t15.2024.07.21 val PER: 0.2386
2026-01-11 22:22:15,016: t15.2024.07.28 val PER: 0.2838
2026-01-11 22:22:15,016: t15.2025.01.10 val PER: 0.4022
2026-01-11 22:22:15,016: t15.2025.01.12 val PER: 0.2918
2026-01-11 22:22:15,016: t15.2025.03.14 val PER: 0.3994
2026-01-11 22:22:15,016: t15.2025.03.16 val PER: 0.3194
2026-01-11 22:22:15,016: t15.2025.03.30 val PER: 0.3908
2026-01-11 22:22:15,016: t15.2025.04.13 val PER: 0.3566
2026-01-11 22:22:15,017: New best val WER(5gram) 46.68% --> 46.35%
2026-01-11 22:22:15,168: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_13500
2026-01-11 22:22:24,458: Train batch 13600: loss: 40.32 grad norm: 87.73 time: 0.070
2026-01-11 22:22:43,104: Train batch 13800: loss: 31.71 grad norm: 129.21 time: 0.062
2026-01-11 22:23:01,979: Train batch 14000: loss: 41.74 grad norm: 94.03 time: 0.057
2026-01-11 22:23:01,979: Running test after training batch: 14000
2026-01-11 22:23:02,102: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:23:07,759: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 22:23:07,854: WER debug example
  GT : how does it keep the cost down
  PR : how does it eat the cost to
2026-01-11 22:23:26,502: Val batch 14000: PER (avg): 0.2923 CTC Loss (avg): 45.1677 WER(5gram): 47.13% (n=256) time: 24.522
2026-01-11 22:23:26,502: WER lens: avg_true_words=5.99 avg_pred_words=5.77 max_pred_words=11
2026-01-11 22:23:26,502: t15.2023.08.13 val PER: 0.2505
2026-01-11 22:23:26,502: t15.2023.08.18 val PER: 0.2531
2026-01-11 22:23:26,502: t15.2023.08.20 val PER: 0.2359
2026-01-11 22:23:26,503: t15.2023.08.25 val PER: 0.2289
2026-01-11 22:23:26,503: t15.2023.08.27 val PER: 0.3248
2026-01-11 22:23:26,503: t15.2023.09.01 val PER: 0.2102
2026-01-11 22:23:26,503: t15.2023.09.03 val PER: 0.2957
2026-01-11 22:23:26,503: t15.2023.09.24 val PER: 0.2294
2026-01-11 22:23:26,503: t15.2023.09.29 val PER: 0.2738
2026-01-11 22:23:26,503: t15.2023.10.01 val PER: 0.3329
2026-01-11 22:23:26,503: t15.2023.10.06 val PER: 0.2260
2026-01-11 22:23:26,503: t15.2023.10.08 val PER: 0.3802
2026-01-11 22:23:26,503: t15.2023.10.13 val PER: 0.4104
2026-01-11 22:23:26,504: t15.2023.10.15 val PER: 0.3078
2026-01-11 22:23:26,504: t15.2023.10.20 val PER: 0.3121
2026-01-11 22:23:26,504: t15.2023.10.22 val PER: 0.2617
2026-01-11 22:23:26,504: t15.2023.11.03 val PER: 0.2958
2026-01-11 22:23:26,504: t15.2023.11.04 val PER: 0.0751
2026-01-11 22:23:26,504: t15.2023.11.17 val PER: 0.1571
2026-01-11 22:23:26,504: t15.2023.11.19 val PER: 0.1417
2026-01-11 22:23:26,504: t15.2023.11.26 val PER: 0.3500
2026-01-11 22:23:26,504: t15.2023.12.03 val PER: 0.2826
2026-01-11 22:23:26,504: t15.2023.12.08 val PER: 0.2936
2026-01-11 22:23:26,505: t15.2023.12.10 val PER: 0.2694
2026-01-11 22:23:26,505: t15.2023.12.17 val PER: 0.2734
2026-01-11 22:23:26,505: t15.2023.12.29 val PER: 0.2903
2026-01-11 22:23:26,505: t15.2024.02.25 val PER: 0.2374
2026-01-11 22:23:26,505: t15.2024.03.08 val PER: 0.3485
2026-01-11 22:23:26,505: t15.2024.03.15 val PER: 0.3277
2026-01-11 22:23:26,505: t15.2024.03.17 val PER: 0.2978
2026-01-11 22:23:26,505: t15.2024.05.10 val PER: 0.2972
2026-01-11 22:23:26,505: t15.2024.06.14 val PER: 0.2792
2026-01-11 22:23:26,505: t15.2024.07.19 val PER: 0.3566
2026-01-11 22:23:26,506: t15.2024.07.21 val PER: 0.2372
2026-01-11 22:23:26,506: t15.2024.07.28 val PER: 0.2794
2026-01-11 22:23:26,506: t15.2025.01.10 val PER: 0.3939
2026-01-11 22:23:26,506: t15.2025.01.12 val PER: 0.2848
2026-01-11 22:23:26,506: t15.2025.03.14 val PER: 0.3920
2026-01-11 22:23:26,506: t15.2025.03.16 val PER: 0.3168
2026-01-11 22:23:26,506: t15.2025.03.30 val PER: 0.3747
2026-01-11 22:23:26,506: t15.2025.04.13 val PER: 0.3581
2026-01-11 22:23:26,656: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_14000
2026-01-11 22:23:45,115: Train batch 14200: loss: 35.17 grad norm: 71.93 time: 0.063
2026-01-11 22:24:03,671: Train batch 14400: loss: 26.29 grad norm: 57.94 time: 0.074
2026-01-11 22:24:13,117: Running test after training batch: 14500
2026-01-11 22:24:13,228: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:24:18,693: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 22:24:18,829: WER debug example
  GT : how does it keep the cost down
  PR : just in the cost to
2026-01-11 22:24:37,143: Val batch 14500: PER (avg): 0.2882 CTC Loss (avg): 44.9296 WER(5gram): 47.39% (n=256) time: 24.026
2026-01-11 22:24:37,144: WER lens: avg_true_words=5.99 avg_pred_words=5.84 max_pred_words=11
2026-01-11 22:24:37,144: t15.2023.08.13 val PER: 0.2474
2026-01-11 22:24:37,144: t15.2023.08.18 val PER: 0.2523
2026-01-11 22:24:37,144: t15.2023.08.20 val PER: 0.2311
2026-01-11 22:24:37,145: t15.2023.08.25 val PER: 0.2229
2026-01-11 22:24:37,145: t15.2023.08.27 val PER: 0.3135
2026-01-11 22:24:37,145: t15.2023.09.01 val PER: 0.2029
2026-01-11 22:24:37,147: t15.2023.09.03 val PER: 0.2898
2026-01-11 22:24:37,147: t15.2023.09.24 val PER: 0.2282
2026-01-11 22:24:37,147: t15.2023.09.29 val PER: 0.2706
2026-01-11 22:24:37,147: t15.2023.10.01 val PER: 0.3276
2026-01-11 22:24:37,147: t15.2023.10.06 val PER: 0.2164
2026-01-11 22:24:37,147: t15.2023.10.08 val PER: 0.3775
2026-01-11 22:24:37,147: t15.2023.10.13 val PER: 0.3964
2026-01-11 22:24:37,147: t15.2023.10.15 val PER: 0.3019
2026-01-11 22:24:37,148: t15.2023.10.20 val PER: 0.3087
2026-01-11 22:24:37,148: t15.2023.10.22 val PER: 0.2561
2026-01-11 22:24:37,148: t15.2023.11.03 val PER: 0.2944
2026-01-11 22:24:37,148: t15.2023.11.04 val PER: 0.0751
2026-01-11 22:24:37,148: t15.2023.11.17 val PER: 0.1540
2026-01-11 22:24:37,148: t15.2023.11.19 val PER: 0.1357
2026-01-11 22:24:37,148: t15.2023.11.26 val PER: 0.3449
2026-01-11 22:24:37,148: t15.2023.12.03 val PER: 0.2731
2026-01-11 22:24:37,148: t15.2023.12.08 val PER: 0.2976
2026-01-11 22:24:37,148: t15.2023.12.10 val PER: 0.2536
2026-01-11 22:24:37,148: t15.2023.12.17 val PER: 0.2744
2026-01-11 22:24:37,149: t15.2023.12.29 val PER: 0.2855
2026-01-11 22:24:37,149: t15.2024.02.25 val PER: 0.2402
2026-01-11 22:24:37,149: t15.2024.03.08 val PER: 0.3414
2026-01-11 22:24:37,149: t15.2024.03.15 val PER: 0.3177
2026-01-11 22:24:37,149: t15.2024.03.17 val PER: 0.2964
2026-01-11 22:24:37,149: t15.2024.05.10 val PER: 0.2987
2026-01-11 22:24:37,149: t15.2024.06.14 val PER: 0.2650
2026-01-11 22:24:37,149: t15.2024.07.19 val PER: 0.3494
2026-01-11 22:24:37,149: t15.2024.07.21 val PER: 0.2324
2026-01-11 22:24:37,149: t15.2024.07.28 val PER: 0.2684
2026-01-11 22:24:37,149: t15.2025.01.10 val PER: 0.4077
2026-01-11 22:24:37,150: t15.2025.01.12 val PER: 0.2910
2026-01-11 22:24:37,150: t15.2025.03.14 val PER: 0.3935
2026-01-11 22:24:37,150: t15.2025.03.16 val PER: 0.3181
2026-01-11 22:24:37,150: t15.2025.03.30 val PER: 0.3816
2026-01-11 22:24:37,150: t15.2025.04.13 val PER: 0.3495
2026-01-11 22:24:37,286: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_14500
2026-01-11 22:24:46,525: Train batch 14600: loss: 44.17 grad norm: 126.94 time: 0.071
2026-01-11 22:25:05,192: Train batch 14800: loss: 25.34 grad norm: 77.82 time: 0.056
2026-01-11 22:25:23,828: Train batch 15000: loss: 29.77 grad norm: 63.17 time: 0.058
2026-01-11 22:25:23,829: Running test after training batch: 15000
2026-01-11 22:25:23,958: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:25:29,399: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 22:25:29,498: WER debug example
  GT : how does it keep the cost down
  PR : how does it cover the cost to
2026-01-11 22:25:47,792: Val batch 15000: PER (avg): 0.2879 CTC Loss (avg): 44.4613 WER(5gram): 46.02% (n=256) time: 23.963
2026-01-11 22:25:47,793: WER lens: avg_true_words=5.99 avg_pred_words=5.82 max_pred_words=11
2026-01-11 22:25:47,793: t15.2023.08.13 val PER: 0.2484
2026-01-11 22:25:47,793: t15.2023.08.18 val PER: 0.2490
2026-01-11 22:25:47,793: t15.2023.08.20 val PER: 0.2248
2026-01-11 22:25:47,793: t15.2023.08.25 val PER: 0.2334
2026-01-11 22:25:47,794: t15.2023.08.27 val PER: 0.3248
2026-01-11 22:25:47,794: t15.2023.09.01 val PER: 0.2062
2026-01-11 22:25:47,794: t15.2023.09.03 val PER: 0.2886
2026-01-11 22:25:47,794: t15.2023.09.24 val PER: 0.2269
2026-01-11 22:25:47,794: t15.2023.09.29 val PER: 0.2687
2026-01-11 22:25:47,794: t15.2023.10.01 val PER: 0.3263
2026-01-11 22:25:47,794: t15.2023.10.06 val PER: 0.2153
2026-01-11 22:25:47,794: t15.2023.10.08 val PER: 0.3694
2026-01-11 22:25:47,794: t15.2023.10.13 val PER: 0.4003
2026-01-11 22:25:47,795: t15.2023.10.15 val PER: 0.3013
2026-01-11 22:25:47,795: t15.2023.10.20 val PER: 0.3087
2026-01-11 22:25:47,795: t15.2023.10.22 val PER: 0.2572
2026-01-11 22:25:47,795: t15.2023.11.03 val PER: 0.2931
2026-01-11 22:25:47,795: t15.2023.11.04 val PER: 0.0751
2026-01-11 22:25:47,795: t15.2023.11.17 val PER: 0.1524
2026-01-11 22:25:47,795: t15.2023.11.19 val PER: 0.1357
2026-01-11 22:25:47,795: t15.2023.11.26 val PER: 0.3493
2026-01-11 22:25:47,795: t15.2023.12.03 val PER: 0.2763
2026-01-11 22:25:47,796: t15.2023.12.08 val PER: 0.2943
2026-01-11 22:25:47,796: t15.2023.12.10 val PER: 0.2641
2026-01-11 22:25:47,796: t15.2023.12.17 val PER: 0.2713
2026-01-11 22:25:47,796: t15.2023.12.29 val PER: 0.2855
2026-01-11 22:25:47,796: t15.2024.02.25 val PER: 0.2388
2026-01-11 22:25:47,796: t15.2024.03.08 val PER: 0.3385
2026-01-11 22:25:47,796: t15.2024.03.15 val PER: 0.3133
2026-01-11 22:25:47,796: t15.2024.03.17 val PER: 0.2901
2026-01-11 22:25:47,796: t15.2024.05.10 val PER: 0.3031
2026-01-11 22:25:47,796: t15.2024.06.14 val PER: 0.2729
2026-01-11 22:25:47,797: t15.2024.07.19 val PER: 0.3533
2026-01-11 22:25:47,797: t15.2024.07.21 val PER: 0.2310
2026-01-11 22:25:47,797: t15.2024.07.28 val PER: 0.2765
2026-01-11 22:25:47,797: t15.2025.01.10 val PER: 0.4091
2026-01-11 22:25:47,797: t15.2025.01.12 val PER: 0.2879
2026-01-11 22:25:47,797: t15.2025.03.14 val PER: 0.3861
2026-01-11 22:25:47,797: t15.2025.03.16 val PER: 0.3089
2026-01-11 22:25:47,797: t15.2025.03.30 val PER: 0.3759
2026-01-11 22:25:47,797: t15.2025.04.13 val PER: 0.3452
2026-01-11 22:25:47,798: New best val WER(5gram) 46.35% --> 46.02%
2026-01-11 22:25:47,954: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_15000
2026-01-11 22:26:06,721: Train batch 15200: loss: 24.10 grad norm: 59.62 time: 0.068
2026-01-11 22:26:25,086: Train batch 15400: loss: 38.33 grad norm: 91.10 time: 0.061
2026-01-11 22:26:34,514: Running test after training batch: 15500
2026-01-11 22:26:34,640: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:26:40,279: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 22:26:40,368: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 22:26:58,127: Val batch 15500: PER (avg): 0.2859 CTC Loss (avg): 44.2953 WER(5gram): 45.89% (n=256) time: 23.612
2026-01-11 22:26:58,127: WER lens: avg_true_words=5.99 avg_pred_words=5.80 max_pred_words=12
2026-01-11 22:26:58,127: t15.2023.08.13 val PER: 0.2505
2026-01-11 22:26:58,127: t15.2023.08.18 val PER: 0.2448
2026-01-11 22:26:58,127: t15.2023.08.20 val PER: 0.2295
2026-01-11 22:26:58,127: t15.2023.08.25 val PER: 0.2214
2026-01-11 22:26:58,128: t15.2023.08.27 val PER: 0.3264
2026-01-11 22:26:58,128: t15.2023.09.01 val PER: 0.1989
2026-01-11 22:26:58,128: t15.2023.09.03 val PER: 0.2874
2026-01-11 22:26:58,128: t15.2023.09.24 val PER: 0.2342
2026-01-11 22:26:58,128: t15.2023.09.29 val PER: 0.2687
2026-01-11 22:26:58,128: t15.2023.10.01 val PER: 0.3223
2026-01-11 22:26:58,128: t15.2023.10.06 val PER: 0.2121
2026-01-11 22:26:58,128: t15.2023.10.08 val PER: 0.3775
2026-01-11 22:26:58,128: t15.2023.10.13 val PER: 0.3995
2026-01-11 22:26:58,128: t15.2023.10.15 val PER: 0.3026
2026-01-11 22:26:58,128: t15.2023.10.20 val PER: 0.2953
2026-01-11 22:26:58,128: t15.2023.10.22 val PER: 0.2595
2026-01-11 22:26:58,128: t15.2023.11.03 val PER: 0.2910
2026-01-11 22:26:58,129: t15.2023.11.04 val PER: 0.0751
2026-01-11 22:26:58,129: t15.2023.11.17 val PER: 0.1446
2026-01-11 22:26:58,129: t15.2023.11.19 val PER: 0.1337
2026-01-11 22:26:58,129: t15.2023.11.26 val PER: 0.3413
2026-01-11 22:26:58,129: t15.2023.12.03 val PER: 0.2679
2026-01-11 22:26:58,129: t15.2023.12.08 val PER: 0.2909
2026-01-11 22:26:58,129: t15.2023.12.10 val PER: 0.2576
2026-01-11 22:26:58,129: t15.2023.12.17 val PER: 0.2609
2026-01-11 22:26:58,129: t15.2023.12.29 val PER: 0.2835
2026-01-11 22:26:58,129: t15.2024.02.25 val PER: 0.2374
2026-01-11 22:26:58,129: t15.2024.03.08 val PER: 0.3428
2026-01-11 22:26:58,130: t15.2024.03.15 val PER: 0.3146
2026-01-11 22:26:58,130: t15.2024.03.17 val PER: 0.2908
2026-01-11 22:26:58,130: t15.2024.05.10 val PER: 0.3031
2026-01-11 22:26:58,130: t15.2024.06.14 val PER: 0.2808
2026-01-11 22:26:58,130: t15.2024.07.19 val PER: 0.3487
2026-01-11 22:26:58,130: t15.2024.07.21 val PER: 0.2276
2026-01-11 22:26:58,130: t15.2024.07.28 val PER: 0.2750
2026-01-11 22:26:58,130: t15.2025.01.10 val PER: 0.4091
2026-01-11 22:26:58,130: t15.2025.01.12 val PER: 0.2841
2026-01-11 22:26:58,130: t15.2025.03.14 val PER: 0.3876
2026-01-11 22:26:58,130: t15.2025.03.16 val PER: 0.3063
2026-01-11 22:26:58,130: t15.2025.03.30 val PER: 0.3736
2026-01-11 22:26:58,130: t15.2025.04.13 val PER: 0.3395
2026-01-11 22:26:58,131: New best val WER(5gram) 46.02% --> 45.89%
2026-01-11 22:26:58,294: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_15500
2026-01-11 22:27:07,546: Train batch 15600: loss: 47.50 grad norm: 111.10 time: 0.070
2026-01-11 22:27:25,973: Train batch 15800: loss: 52.30 grad norm: 100.97 time: 0.074
2026-01-11 22:27:44,871: Train batch 16000: loss: 30.99 grad norm: 68.41 time: 0.063
2026-01-11 22:27:44,871: Running test after training batch: 16000
2026-01-11 22:27:45,014: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:27:50,590: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 22:27:50,700: WER debug example
  GT : how does it keep the cost down
  PR : how does it eat the cost to
2026-01-11 22:28:08,863: Val batch 16000: PER (avg): 0.2845 CTC Loss (avg): 44.0712 WER(5gram): 46.15% (n=256) time: 23.991
2026-01-11 22:28:08,863: WER lens: avg_true_words=5.99 avg_pred_words=5.87 max_pred_words=12
2026-01-11 22:28:08,864: t15.2023.08.13 val PER: 0.2464
2026-01-11 22:28:08,864: t15.2023.08.18 val PER: 0.2498
2026-01-11 22:28:08,864: t15.2023.08.20 val PER: 0.2272
2026-01-11 22:28:08,864: t15.2023.08.25 val PER: 0.2259
2026-01-11 22:28:08,864: t15.2023.08.27 val PER: 0.3232
2026-01-11 22:28:08,864: t15.2023.09.01 val PER: 0.2013
2026-01-11 22:28:08,864: t15.2023.09.03 val PER: 0.2886
2026-01-11 22:28:08,864: t15.2023.09.24 val PER: 0.2318
2026-01-11 22:28:08,864: t15.2023.09.29 val PER: 0.2687
2026-01-11 22:28:08,864: t15.2023.10.01 val PER: 0.3243
2026-01-11 22:28:08,864: t15.2023.10.06 val PER: 0.2121
2026-01-11 22:28:08,864: t15.2023.10.08 val PER: 0.3681
2026-01-11 22:28:08,864: t15.2023.10.13 val PER: 0.3988
2026-01-11 22:28:08,865: t15.2023.10.15 val PER: 0.2999
2026-01-11 22:28:08,865: t15.2023.10.20 val PER: 0.3054
2026-01-11 22:28:08,865: t15.2023.10.22 val PER: 0.2561
2026-01-11 22:28:08,865: t15.2023.11.03 val PER: 0.2938
2026-01-11 22:28:08,865: t15.2023.11.04 val PER: 0.0717
2026-01-11 22:28:08,865: t15.2023.11.17 val PER: 0.1384
2026-01-11 22:28:08,865: t15.2023.11.19 val PER: 0.1377
2026-01-11 22:28:08,865: t15.2023.11.26 val PER: 0.3304
2026-01-11 22:28:08,865: t15.2023.12.03 val PER: 0.2658
2026-01-11 22:28:08,865: t15.2023.12.08 val PER: 0.2883
2026-01-11 22:28:08,866: t15.2023.12.10 val PER: 0.2602
2026-01-11 22:28:08,866: t15.2023.12.17 val PER: 0.2620
2026-01-11 22:28:08,866: t15.2023.12.29 val PER: 0.2759
2026-01-11 22:28:08,866: t15.2024.02.25 val PER: 0.2402
2026-01-11 22:28:08,866: t15.2024.03.08 val PER: 0.3428
2026-01-11 22:28:08,866: t15.2024.03.15 val PER: 0.3208
2026-01-11 22:28:08,866: t15.2024.03.17 val PER: 0.2845
2026-01-11 22:28:08,866: t15.2024.05.10 val PER: 0.2972
2026-01-11 22:28:08,866: t15.2024.06.14 val PER: 0.2760
2026-01-11 22:28:08,866: t15.2024.07.19 val PER: 0.3474
2026-01-11 22:28:08,866: t15.2024.07.21 val PER: 0.2228
2026-01-11 22:28:08,866: t15.2024.07.28 val PER: 0.2779
2026-01-11 22:28:08,866: t15.2025.01.10 val PER: 0.3898
2026-01-11 22:28:08,866: t15.2025.01.12 val PER: 0.2856
2026-01-11 22:28:08,867: t15.2025.03.14 val PER: 0.3846
2026-01-11 22:28:08,867: t15.2025.03.16 val PER: 0.3089
2026-01-11 22:28:08,867: t15.2025.03.30 val PER: 0.3713
2026-01-11 22:28:08,867: t15.2025.04.13 val PER: 0.3352
2026-01-11 22:28:09,013: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_16000
2026-01-11 22:28:27,612: Train batch 16200: loss: 29.61 grad norm: 67.69 time: 0.065
2026-01-11 22:28:46,236: Train batch 16400: loss: 34.34 grad norm: 98.05 time: 0.065
2026-01-11 22:28:55,689: Running test after training batch: 16500
2026-01-11 22:28:55,803: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:29:01,286: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 22:29:01,372: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 22:29:19,292: Val batch 16500: PER (avg): 0.2850 CTC Loss (avg): 43.8114 WER(5gram): 45.89% (n=256) time: 23.602
2026-01-11 22:29:19,292: WER lens: avg_true_words=5.99 avg_pred_words=5.80 max_pred_words=12
2026-01-11 22:29:19,292: t15.2023.08.13 val PER: 0.2412
2026-01-11 22:29:19,292: t15.2023.08.18 val PER: 0.2431
2026-01-11 22:29:19,292: t15.2023.08.20 val PER: 0.2272
2026-01-11 22:29:19,292: t15.2023.08.25 val PER: 0.2289
2026-01-11 22:29:19,293: t15.2023.08.27 val PER: 0.3151
2026-01-11 22:29:19,293: t15.2023.09.01 val PER: 0.1989
2026-01-11 22:29:19,293: t15.2023.09.03 val PER: 0.2933
2026-01-11 22:29:19,293: t15.2023.09.24 val PER: 0.2294
2026-01-11 22:29:19,293: t15.2023.09.29 val PER: 0.2687
2026-01-11 22:29:19,293: t15.2023.10.01 val PER: 0.3243
2026-01-11 22:29:19,293: t15.2023.10.06 val PER: 0.2174
2026-01-11 22:29:19,293: t15.2023.10.08 val PER: 0.3694
2026-01-11 22:29:19,294: t15.2023.10.13 val PER: 0.4003
2026-01-11 22:29:19,294: t15.2023.10.15 val PER: 0.3019
2026-01-11 22:29:19,294: t15.2023.10.20 val PER: 0.3087
2026-01-11 22:29:19,294: t15.2023.10.22 val PER: 0.2606
2026-01-11 22:29:19,294: t15.2023.11.03 val PER: 0.2972
2026-01-11 22:29:19,294: t15.2023.11.04 val PER: 0.0785
2026-01-11 22:29:19,294: t15.2023.11.17 val PER: 0.1431
2026-01-11 22:29:19,294: t15.2023.11.19 val PER: 0.1337
2026-01-11 22:29:19,294: t15.2023.11.26 val PER: 0.3355
2026-01-11 22:29:19,294: t15.2023.12.03 val PER: 0.2668
2026-01-11 22:29:19,295: t15.2023.12.08 val PER: 0.2850
2026-01-11 22:29:19,295: t15.2023.12.10 val PER: 0.2628
2026-01-11 22:29:19,295: t15.2023.12.17 val PER: 0.2640
2026-01-11 22:29:19,295: t15.2023.12.29 val PER: 0.2835
2026-01-11 22:29:19,295: t15.2024.02.25 val PER: 0.2388
2026-01-11 22:29:19,295: t15.2024.03.08 val PER: 0.3442
2026-01-11 22:29:19,295: t15.2024.03.15 val PER: 0.3177
2026-01-11 22:29:19,295: t15.2024.03.17 val PER: 0.2859
2026-01-11 22:29:19,295: t15.2024.05.10 val PER: 0.2972
2026-01-11 22:29:19,295: t15.2024.06.14 val PER: 0.2776
2026-01-11 22:29:19,296: t15.2024.07.19 val PER: 0.3434
2026-01-11 22:29:19,296: t15.2024.07.21 val PER: 0.2228
2026-01-11 22:29:19,296: t15.2024.07.28 val PER: 0.2699
2026-01-11 22:29:19,296: t15.2025.01.10 val PER: 0.3981
2026-01-11 22:29:19,296: t15.2025.01.12 val PER: 0.2841
2026-01-11 22:29:19,296: t15.2025.03.14 val PER: 0.3891
2026-01-11 22:29:19,296: t15.2025.03.16 val PER: 0.3102
2026-01-11 22:29:19,296: t15.2025.03.30 val PER: 0.3782
2026-01-11 22:29:19,296: t15.2025.04.13 val PER: 0.3381
2026-01-11 22:29:19,443: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_16500
2026-01-11 22:29:28,745: Train batch 16600: loss: 34.02 grad norm: 75.15 time: 0.060
2026-01-11 22:29:47,424: Train batch 16800: loss: 49.75 grad norm: 131.80 time: 0.069
2026-01-11 22:30:06,248: Train batch 17000: loss: 28.84 grad norm: 73.78 time: 0.088
2026-01-11 22:30:06,248: Running test after training batch: 17000
2026-01-11 22:30:06,347: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:30:11,891: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 22:30:11,993: WER debug example
  GT : how does it keep the cost down
  PR : how does it eat the cost to
2026-01-11 22:30:29,881: Val batch 17000: PER (avg): 0.2824 CTC Loss (avg): 43.6377 WER(5gram): 47.07% (n=256) time: 23.633
2026-01-11 22:30:29,882: WER lens: avg_true_words=5.99 avg_pred_words=5.82 max_pred_words=12
2026-01-11 22:30:29,882: t15.2023.08.13 val PER: 0.2401
2026-01-11 22:30:29,882: t15.2023.08.18 val PER: 0.2473
2026-01-11 22:30:29,882: t15.2023.08.20 val PER: 0.2288
2026-01-11 22:30:29,882: t15.2023.08.25 val PER: 0.2229
2026-01-11 22:30:29,883: t15.2023.08.27 val PER: 0.3087
2026-01-11 22:30:29,883: t15.2023.09.01 val PER: 0.1964
2026-01-11 22:30:29,883: t15.2023.09.03 val PER: 0.2815
2026-01-11 22:30:29,883: t15.2023.09.24 val PER: 0.2209
2026-01-11 22:30:29,883: t15.2023.09.29 val PER: 0.2674
2026-01-11 22:30:29,883: t15.2023.10.01 val PER: 0.3197
2026-01-11 22:30:29,883: t15.2023.10.06 val PER: 0.2131
2026-01-11 22:30:29,883: t15.2023.10.08 val PER: 0.3667
2026-01-11 22:30:29,883: t15.2023.10.13 val PER: 0.3957
2026-01-11 22:30:29,883: t15.2023.10.15 val PER: 0.3006
2026-01-11 22:30:29,883: t15.2023.10.20 val PER: 0.2987
2026-01-11 22:30:29,883: t15.2023.10.22 val PER: 0.2572
2026-01-11 22:30:29,884: t15.2023.11.03 val PER: 0.2951
2026-01-11 22:30:29,884: t15.2023.11.04 val PER: 0.0751
2026-01-11 22:30:29,884: t15.2023.11.17 val PER: 0.1446
2026-01-11 22:30:29,884: t15.2023.11.19 val PER: 0.1297
2026-01-11 22:30:29,884: t15.2023.11.26 val PER: 0.3399
2026-01-11 22:30:29,884: t15.2023.12.03 val PER: 0.2700
2026-01-11 22:30:29,884: t15.2023.12.08 val PER: 0.2843
2026-01-11 22:30:29,884: t15.2023.12.10 val PER: 0.2589
2026-01-11 22:30:29,884: t15.2023.12.17 val PER: 0.2578
2026-01-11 22:30:29,884: t15.2023.12.29 val PER: 0.2780
2026-01-11 22:30:29,884: t15.2024.02.25 val PER: 0.2346
2026-01-11 22:30:29,884: t15.2024.03.08 val PER: 0.3414
2026-01-11 22:30:29,884: t15.2024.03.15 val PER: 0.3139
2026-01-11 22:30:29,885: t15.2024.03.17 val PER: 0.2838
2026-01-11 22:30:29,885: t15.2024.05.10 val PER: 0.2897
2026-01-11 22:30:29,885: t15.2024.06.14 val PER: 0.2744
2026-01-11 22:30:29,885: t15.2024.07.19 val PER: 0.3441
2026-01-11 22:30:29,885: t15.2024.07.21 val PER: 0.2207
2026-01-11 22:30:29,885: t15.2024.07.28 val PER: 0.2750
2026-01-11 22:30:29,885: t15.2025.01.10 val PER: 0.4077
2026-01-11 22:30:29,885: t15.2025.01.12 val PER: 0.2748
2026-01-11 22:30:29,885: t15.2025.03.14 val PER: 0.3757
2026-01-11 22:30:29,885: t15.2025.03.16 val PER: 0.3076
2026-01-11 22:30:29,885: t15.2025.03.30 val PER: 0.3678
2026-01-11 22:30:29,885: t15.2025.04.13 val PER: 0.3352
2026-01-11 22:30:30,031: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_17000
2026-01-11 22:30:48,577: Train batch 17200: loss: 37.54 grad norm: 73.94 time: 0.090
2026-01-11 22:31:07,562: Train batch 17400: loss: 41.95 grad norm: 94.41 time: 0.078
2026-01-11 22:31:16,925: Running test after training batch: 17500
2026-01-11 22:31:17,075: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:31:23,006: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 22:31:23,099: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 22:31:41,960: Val batch 17500: PER (avg): 0.2819 CTC Loss (avg): 43.5348 WER(5gram): 45.96% (n=256) time: 25.035
2026-01-11 22:31:41,961: WER lens: avg_true_words=5.99 avg_pred_words=5.84 max_pred_words=11
2026-01-11 22:31:41,961: t15.2023.08.13 val PER: 0.2401
2026-01-11 22:31:41,961: t15.2023.08.18 val PER: 0.2456
2026-01-11 22:31:41,961: t15.2023.08.20 val PER: 0.2232
2026-01-11 22:31:41,961: t15.2023.08.25 val PER: 0.2229
2026-01-11 22:31:41,961: t15.2023.08.27 val PER: 0.3135
2026-01-11 22:31:41,961: t15.2023.09.01 val PER: 0.1948
2026-01-11 22:31:41,961: t15.2023.09.03 val PER: 0.2838
2026-01-11 22:31:41,962: t15.2023.09.24 val PER: 0.2269
2026-01-11 22:31:41,962: t15.2023.09.29 val PER: 0.2661
2026-01-11 22:31:41,962: t15.2023.10.01 val PER: 0.3170
2026-01-11 22:31:41,962: t15.2023.10.06 val PER: 0.2164
2026-01-11 22:31:41,962: t15.2023.10.08 val PER: 0.3667
2026-01-11 22:31:41,962: t15.2023.10.13 val PER: 0.3887
2026-01-11 22:31:41,962: t15.2023.10.15 val PER: 0.2986
2026-01-11 22:31:41,962: t15.2023.10.20 val PER: 0.2987
2026-01-11 22:31:41,962: t15.2023.10.22 val PER: 0.2539
2026-01-11 22:31:41,962: t15.2023.11.03 val PER: 0.2917
2026-01-11 22:31:41,962: t15.2023.11.04 val PER: 0.0717
2026-01-11 22:31:41,962: t15.2023.11.17 val PER: 0.1415
2026-01-11 22:31:41,963: t15.2023.11.19 val PER: 0.1357
2026-01-11 22:31:41,963: t15.2023.11.26 val PER: 0.3362
2026-01-11 22:31:41,963: t15.2023.12.03 val PER: 0.2647
2026-01-11 22:31:41,963: t15.2023.12.08 val PER: 0.2876
2026-01-11 22:31:41,963: t15.2023.12.10 val PER: 0.2576
2026-01-11 22:31:41,963: t15.2023.12.17 val PER: 0.2526
2026-01-11 22:31:41,963: t15.2023.12.29 val PER: 0.2780
2026-01-11 22:31:41,963: t15.2024.02.25 val PER: 0.2374
2026-01-11 22:31:41,963: t15.2024.03.08 val PER: 0.3371
2026-01-11 22:31:41,963: t15.2024.03.15 val PER: 0.3114
2026-01-11 22:31:41,963: t15.2024.03.17 val PER: 0.2845
2026-01-11 22:31:41,963: t15.2024.05.10 val PER: 0.2987
2026-01-11 22:31:41,964: t15.2024.06.14 val PER: 0.2666
2026-01-11 22:31:41,964: t15.2024.07.19 val PER: 0.3434
2026-01-11 22:31:41,964: t15.2024.07.21 val PER: 0.2166
2026-01-11 22:31:41,964: t15.2024.07.28 val PER: 0.2706
2026-01-11 22:31:41,964: t15.2025.01.10 val PER: 0.4022
2026-01-11 22:31:41,964: t15.2025.01.12 val PER: 0.2841
2026-01-11 22:31:41,964: t15.2025.03.14 val PER: 0.3817
2026-01-11 22:31:41,964: t15.2025.03.16 val PER: 0.3154
2026-01-11 22:31:41,964: t15.2025.03.30 val PER: 0.3701
2026-01-11 22:31:41,964: t15.2025.04.13 val PER: 0.3481
2026-01-11 22:31:42,114: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_17500
2026-01-11 22:31:51,209: Train batch 17600: loss: 42.80 grad norm: 88.98 time: 0.058
2026-01-11 22:32:10,004: Train batch 17800: loss: 25.26 grad norm: 107.27 time: 0.048
2026-01-11 22:32:28,691: Train batch 18000: loss: 35.91 grad norm: 91.14 time: 0.068
2026-01-11 22:32:28,692: Running test after training batch: 18000
2026-01-11 22:32:28,808: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:32:34,288: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 22:32:34,373: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 22:32:52,379: Val batch 18000: PER (avg): 0.2821 CTC Loss (avg): 43.4812 WER(5gram): 46.68% (n=256) time: 23.687
2026-01-11 22:32:52,379: WER lens: avg_true_words=5.99 avg_pred_words=5.80 max_pred_words=12
2026-01-11 22:32:52,380: t15.2023.08.13 val PER: 0.2380
2026-01-11 22:32:52,380: t15.2023.08.18 val PER: 0.2464
2026-01-11 22:32:52,380: t15.2023.08.20 val PER: 0.2208
2026-01-11 22:32:52,380: t15.2023.08.25 val PER: 0.2184
2026-01-11 22:32:52,380: t15.2023.08.27 val PER: 0.3167
2026-01-11 22:32:52,380: t15.2023.09.01 val PER: 0.1932
2026-01-11 22:32:52,380: t15.2023.09.03 val PER: 0.2838
2026-01-11 22:32:52,380: t15.2023.09.24 val PER: 0.2269
2026-01-11 22:32:52,380: t15.2023.09.29 val PER: 0.2623
2026-01-11 22:32:52,381: t15.2023.10.01 val PER: 0.3190
2026-01-11 22:32:52,381: t15.2023.10.06 val PER: 0.2131
2026-01-11 22:32:52,381: t15.2023.10.08 val PER: 0.3708
2026-01-11 22:32:52,381: t15.2023.10.13 val PER: 0.3957
2026-01-11 22:32:52,381: t15.2023.10.15 val PER: 0.2999
2026-01-11 22:32:52,381: t15.2023.10.20 val PER: 0.3054
2026-01-11 22:32:52,381: t15.2023.10.22 val PER: 0.2494
2026-01-11 22:32:52,381: t15.2023.11.03 val PER: 0.2944
2026-01-11 22:32:52,381: t15.2023.11.04 val PER: 0.0785
2026-01-11 22:32:52,381: t15.2023.11.17 val PER: 0.1337
2026-01-11 22:32:52,381: t15.2023.11.19 val PER: 0.1357
2026-01-11 22:32:52,382: t15.2023.11.26 val PER: 0.3362
2026-01-11 22:32:52,382: t15.2023.12.03 val PER: 0.2668
2026-01-11 22:32:52,382: t15.2023.12.08 val PER: 0.2850
2026-01-11 22:32:52,382: t15.2023.12.10 val PER: 0.2615
2026-01-11 22:32:52,382: t15.2023.12.17 val PER: 0.2651
2026-01-11 22:32:52,382: t15.2023.12.29 val PER: 0.2835
2026-01-11 22:32:52,383: t15.2024.02.25 val PER: 0.2346
2026-01-11 22:32:52,383: t15.2024.03.08 val PER: 0.3385
2026-01-11 22:32:52,383: t15.2024.03.15 val PER: 0.3114
2026-01-11 22:32:52,383: t15.2024.03.17 val PER: 0.2810
2026-01-11 22:32:52,383: t15.2024.05.10 val PER: 0.2912
2026-01-11 22:32:52,383: t15.2024.06.14 val PER: 0.2634
2026-01-11 22:32:52,383: t15.2024.07.19 val PER: 0.3441
2026-01-11 22:32:52,383: t15.2024.07.21 val PER: 0.2214
2026-01-11 22:32:52,383: t15.2024.07.28 val PER: 0.2669
2026-01-11 22:32:52,383: t15.2025.01.10 val PER: 0.4050
2026-01-11 22:32:52,383: t15.2025.01.12 val PER: 0.2825
2026-01-11 22:32:52,383: t15.2025.03.14 val PER: 0.3891
2026-01-11 22:32:52,383: t15.2025.03.16 val PER: 0.3115
2026-01-11 22:32:52,383: t15.2025.03.30 val PER: 0.3690
2026-01-11 22:32:52,383: t15.2025.04.13 val PER: 0.3424
2026-01-11 22:32:52,525: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_18000
2026-01-11 22:33:11,464: Train batch 18200: loss: 35.80 grad norm: 90.84 time: 0.082
2026-01-11 22:33:29,967: Train batch 18400: loss: 23.10 grad norm: 81.98 time: 0.064
2026-01-11 22:33:39,249: Running test after training batch: 18500
2026-01-11 22:33:39,500: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:33:44,986: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 22:33:45,077: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 22:34:03,077: Val batch 18500: PER (avg): 0.2813 CTC Loss (avg): 43.3699 WER(5gram): 46.09% (n=256) time: 23.828
2026-01-11 22:34:03,078: WER lens: avg_true_words=5.99 avg_pred_words=5.82 max_pred_words=12
2026-01-11 22:34:03,078: t15.2023.08.13 val PER: 0.2349
2026-01-11 22:34:03,078: t15.2023.08.18 val PER: 0.2422
2026-01-11 22:34:03,078: t15.2023.08.20 val PER: 0.2264
2026-01-11 22:34:03,078: t15.2023.08.25 val PER: 0.2199
2026-01-11 22:34:03,078: t15.2023.08.27 val PER: 0.3199
2026-01-11 22:34:03,079: t15.2023.09.01 val PER: 0.1899
2026-01-11 22:34:03,079: t15.2023.09.03 val PER: 0.2874
2026-01-11 22:34:03,079: t15.2023.09.24 val PER: 0.2257
2026-01-11 22:34:03,079: t15.2023.09.29 val PER: 0.2636
2026-01-11 22:34:03,079: t15.2023.10.01 val PER: 0.3217
2026-01-11 22:34:03,079: t15.2023.10.06 val PER: 0.2164
2026-01-11 22:34:03,079: t15.2023.10.08 val PER: 0.3721
2026-01-11 22:34:03,079: t15.2023.10.13 val PER: 0.3918
2026-01-11 22:34:03,079: t15.2023.10.15 val PER: 0.2947
2026-01-11 22:34:03,079: t15.2023.10.20 val PER: 0.3087
2026-01-11 22:34:03,079: t15.2023.10.22 val PER: 0.2506
2026-01-11 22:34:03,079: t15.2023.11.03 val PER: 0.2965
2026-01-11 22:34:03,079: t15.2023.11.04 val PER: 0.0751
2026-01-11 22:34:03,080: t15.2023.11.17 val PER: 0.1337
2026-01-11 22:34:03,080: t15.2023.11.19 val PER: 0.1337
2026-01-11 22:34:03,080: t15.2023.11.26 val PER: 0.3355
2026-01-11 22:34:03,080: t15.2023.12.03 val PER: 0.2742
2026-01-11 22:34:03,080: t15.2023.12.08 val PER: 0.2843
2026-01-11 22:34:03,080: t15.2023.12.10 val PER: 0.2576
2026-01-11 22:34:03,080: t15.2023.12.17 val PER: 0.2568
2026-01-11 22:34:03,080: t15.2023.12.29 val PER: 0.2759
2026-01-11 22:34:03,080: t15.2024.02.25 val PER: 0.2346
2026-01-11 22:34:03,080: t15.2024.03.08 val PER: 0.3329
2026-01-11 22:34:03,080: t15.2024.03.15 val PER: 0.3114
2026-01-11 22:34:03,080: t15.2024.03.17 val PER: 0.2782
2026-01-11 22:34:03,081: t15.2024.05.10 val PER: 0.2957
2026-01-11 22:34:03,081: t15.2024.06.14 val PER: 0.2618
2026-01-11 22:34:03,081: t15.2024.07.19 val PER: 0.3434
2026-01-11 22:34:03,081: t15.2024.07.21 val PER: 0.2172
2026-01-11 22:34:03,081: t15.2024.07.28 val PER: 0.2706
2026-01-11 22:34:03,081: t15.2025.01.10 val PER: 0.3994
2026-01-11 22:34:03,081: t15.2025.01.12 val PER: 0.2848
2026-01-11 22:34:03,081: t15.2025.03.14 val PER: 0.3861
2026-01-11 22:34:03,081: t15.2025.03.16 val PER: 0.3128
2026-01-11 22:34:03,081: t15.2025.03.30 val PER: 0.3644
2026-01-11 22:34:03,081: t15.2025.04.13 val PER: 0.3424
2026-01-11 22:34:03,236: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_18500
2026-01-11 22:34:12,466: Train batch 18600: loss: 37.77 grad norm: 75.37 time: 0.078
2026-01-11 22:34:30,979: Train batch 18800: loss: 39.47 grad norm: 86.00 time: 0.072
2026-01-11 22:34:49,812: Train batch 19000: loss: 30.85 grad norm: 76.76 time: 0.071
2026-01-11 22:34:49,812: Running test after training batch: 19000
2026-01-11 22:34:49,922: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:34:55,647: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 22:34:55,728: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 22:35:13,867: Val batch 19000: PER (avg): 0.2808 CTC Loss (avg): 43.3464 WER(5gram): 45.63% (n=256) time: 24.055
2026-01-11 22:35:13,867: WER lens: avg_true_words=5.99 avg_pred_words=5.81 max_pred_words=12
2026-01-11 22:35:13,868: t15.2023.08.13 val PER: 0.2349
2026-01-11 22:35:13,868: t15.2023.08.18 val PER: 0.2464
2026-01-11 22:35:13,868: t15.2023.08.20 val PER: 0.2192
2026-01-11 22:35:13,868: t15.2023.08.25 val PER: 0.2184
2026-01-11 22:35:13,868: t15.2023.08.27 val PER: 0.3135
2026-01-11 22:35:13,868: t15.2023.09.01 val PER: 0.1932
2026-01-11 22:35:13,868: t15.2023.09.03 val PER: 0.2850
2026-01-11 22:35:13,868: t15.2023.09.24 val PER: 0.2269
2026-01-11 22:35:13,868: t15.2023.09.29 val PER: 0.2661
2026-01-11 22:35:13,869: t15.2023.10.01 val PER: 0.3184
2026-01-11 22:35:13,869: t15.2023.10.06 val PER: 0.2185
2026-01-11 22:35:13,869: t15.2023.10.08 val PER: 0.3721
2026-01-11 22:35:13,869: t15.2023.10.13 val PER: 0.3910
2026-01-11 22:35:13,869: t15.2023.10.15 val PER: 0.2960
2026-01-11 22:35:13,869: t15.2023.10.20 val PER: 0.3020
2026-01-11 22:35:13,869: t15.2023.10.22 val PER: 0.2461
2026-01-11 22:35:13,869: t15.2023.11.03 val PER: 0.2924
2026-01-11 22:35:13,869: t15.2023.11.04 val PER: 0.0751
2026-01-11 22:35:13,869: t15.2023.11.17 val PER: 0.1400
2026-01-11 22:35:13,869: t15.2023.11.19 val PER: 0.1357
2026-01-11 22:35:13,870: t15.2023.11.26 val PER: 0.3355
2026-01-11 22:35:13,870: t15.2023.12.03 val PER: 0.2668
2026-01-11 22:35:13,870: t15.2023.12.08 val PER: 0.2830
2026-01-11 22:35:13,870: t15.2023.12.10 val PER: 0.2576
2026-01-11 22:35:13,870: t15.2023.12.17 val PER: 0.2620
2026-01-11 22:35:13,870: t15.2023.12.29 val PER: 0.2759
2026-01-11 22:35:13,870: t15.2024.02.25 val PER: 0.2331
2026-01-11 22:35:13,870: t15.2024.03.08 val PER: 0.3343
2026-01-11 22:35:13,870: t15.2024.03.15 val PER: 0.3083
2026-01-11 22:35:13,870: t15.2024.03.17 val PER: 0.2796
2026-01-11 22:35:13,871: t15.2024.05.10 val PER: 0.2942
2026-01-11 22:35:13,871: t15.2024.06.14 val PER: 0.2681
2026-01-11 22:35:13,871: t15.2024.07.19 val PER: 0.3421
2026-01-11 22:35:13,871: t15.2024.07.21 val PER: 0.2152
2026-01-11 22:35:13,871: t15.2024.07.28 val PER: 0.2684
2026-01-11 22:35:13,871: t15.2025.01.10 val PER: 0.3994
2026-01-11 22:35:13,871: t15.2025.01.12 val PER: 0.2818
2026-01-11 22:35:13,871: t15.2025.03.14 val PER: 0.3831
2026-01-11 22:35:13,871: t15.2025.03.16 val PER: 0.3154
2026-01-11 22:35:13,871: t15.2025.03.30 val PER: 0.3701
2026-01-11 22:35:13,871: t15.2025.04.13 val PER: 0.3409
2026-01-11 22:35:13,873: New best val WER(5gram) 45.89% --> 45.63%
2026-01-11 22:35:14,025: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_19000
2026-01-11 22:35:32,280: Train batch 19200: loss: 26.58 grad norm: 75.81 time: 0.069
2026-01-11 22:35:51,091: Train batch 19400: loss: 27.14 grad norm: 59.09 time: 0.064
2026-01-11 22:36:00,612: Running test after training batch: 19500
2026-01-11 22:36:00,846: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:36:06,445: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 22:36:06,523: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 22:36:24,728: Val batch 19500: PER (avg): 0.2801 CTC Loss (avg): 43.2684 WER(5gram): 45.96% (n=256) time: 24.116
2026-01-11 22:36:24,729: WER lens: avg_true_words=5.99 avg_pred_words=5.83 max_pred_words=12
2026-01-11 22:36:24,729: t15.2023.08.13 val PER: 0.2370
2026-01-11 22:36:24,729: t15.2023.08.18 val PER: 0.2414
2026-01-11 22:36:24,729: t15.2023.08.20 val PER: 0.2224
2026-01-11 22:36:24,729: t15.2023.08.25 val PER: 0.2199
2026-01-11 22:36:24,730: t15.2023.08.27 val PER: 0.3087
2026-01-11 22:36:24,730: t15.2023.09.01 val PER: 0.1924
2026-01-11 22:36:24,730: t15.2023.09.03 val PER: 0.2827
2026-01-11 22:36:24,730: t15.2023.09.24 val PER: 0.2184
2026-01-11 22:36:24,730: t15.2023.09.29 val PER: 0.2661
2026-01-11 22:36:24,730: t15.2023.10.01 val PER: 0.3184
2026-01-11 22:36:24,730: t15.2023.10.06 val PER: 0.2174
2026-01-11 22:36:24,730: t15.2023.10.08 val PER: 0.3708
2026-01-11 22:36:24,730: t15.2023.10.13 val PER: 0.3902
2026-01-11 22:36:24,731: t15.2023.10.15 val PER: 0.2966
2026-01-11 22:36:24,731: t15.2023.10.20 val PER: 0.3020
2026-01-11 22:36:24,731: t15.2023.10.22 val PER: 0.2450
2026-01-11 22:36:24,731: t15.2023.11.03 val PER: 0.2931
2026-01-11 22:36:24,731: t15.2023.11.04 val PER: 0.0751
2026-01-11 22:36:24,731: t15.2023.11.17 val PER: 0.1337
2026-01-11 22:36:24,731: t15.2023.11.19 val PER: 0.1357
2026-01-11 22:36:24,731: t15.2023.11.26 val PER: 0.3355
2026-01-11 22:36:24,731: t15.2023.12.03 val PER: 0.2647
2026-01-11 22:36:24,731: t15.2023.12.08 val PER: 0.2830
2026-01-11 22:36:24,732: t15.2023.12.10 val PER: 0.2602
2026-01-11 22:36:24,732: t15.2023.12.17 val PER: 0.2588
2026-01-11 22:36:24,732: t15.2023.12.29 val PER: 0.2752
2026-01-11 22:36:24,732: t15.2024.02.25 val PER: 0.2374
2026-01-11 22:36:24,732: t15.2024.03.08 val PER: 0.3357
2026-01-11 22:36:24,732: t15.2024.03.15 val PER: 0.3064
2026-01-11 22:36:24,732: t15.2024.03.17 val PER: 0.2796
2026-01-11 22:36:24,732: t15.2024.05.10 val PER: 0.2942
2026-01-11 22:36:24,732: t15.2024.06.14 val PER: 0.2650
2026-01-11 22:36:24,733: t15.2024.07.19 val PER: 0.3415
2026-01-11 22:36:24,733: t15.2024.07.21 val PER: 0.2131
2026-01-11 22:36:24,733: t15.2024.07.28 val PER: 0.2706
2026-01-11 22:36:24,733: t15.2025.01.10 val PER: 0.3981
2026-01-11 22:36:24,733: t15.2025.01.12 val PER: 0.2825
2026-01-11 22:36:24,733: t15.2025.03.14 val PER: 0.3876
2026-01-11 22:36:24,733: t15.2025.03.16 val PER: 0.3128
2026-01-11 22:36:24,733: t15.2025.03.30 val PER: 0.3655
2026-01-11 22:36:24,733: t15.2025.04.13 val PER: 0.3424
2026-01-11 22:36:24,881: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_19500
2026-01-11 22:36:34,087: Train batch 19600: loss: 31.48 grad norm: 82.70 time: 0.063
2026-01-11 22:36:52,510: Train batch 19800: loss: 29.59 grad norm: 70.61 time: 0.061
2026-01-11 22:37:11,046: Running test after training batch: 19999
2026-01-11 22:37:11,138: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:37:16,553: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 22:37:16,632: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 22:37:34,859: Val batch 19999: PER (avg): 0.2793 CTC Loss (avg): 43.1499 WER(5gram): 44.92% (n=256) time: 23.813
2026-01-11 22:37:34,860: WER lens: avg_true_words=5.99 avg_pred_words=5.79 max_pred_words=12
2026-01-11 22:37:34,860: t15.2023.08.13 val PER: 0.2349
2026-01-11 22:37:34,860: t15.2023.08.18 val PER: 0.2439
2026-01-11 22:37:34,860: t15.2023.08.20 val PER: 0.2168
2026-01-11 22:37:34,860: t15.2023.08.25 val PER: 0.2169
2026-01-11 22:37:34,860: t15.2023.08.27 val PER: 0.3071
2026-01-11 22:37:34,861: t15.2023.09.01 val PER: 0.1932
2026-01-11 22:37:34,861: t15.2023.09.03 val PER: 0.2827
2026-01-11 22:37:34,861: t15.2023.09.24 val PER: 0.2221
2026-01-11 22:37:34,861: t15.2023.09.29 val PER: 0.2623
2026-01-11 22:37:34,861: t15.2023.10.01 val PER: 0.3177
2026-01-11 22:37:34,861: t15.2023.10.06 val PER: 0.2131
2026-01-11 22:37:34,861: t15.2023.10.08 val PER: 0.3721
2026-01-11 22:37:34,861: t15.2023.10.13 val PER: 0.3848
2026-01-11 22:37:34,861: t15.2023.10.15 val PER: 0.2940
2026-01-11 22:37:34,861: t15.2023.10.20 val PER: 0.2987
2026-01-11 22:37:34,861: t15.2023.10.22 val PER: 0.2472
2026-01-11 22:37:34,861: t15.2023.11.03 val PER: 0.2958
2026-01-11 22:37:34,861: t15.2023.11.04 val PER: 0.0785
2026-01-11 22:37:34,861: t15.2023.11.17 val PER: 0.1337
2026-01-11 22:37:34,861: t15.2023.11.19 val PER: 0.1317
2026-01-11 22:37:34,862: t15.2023.11.26 val PER: 0.3370
2026-01-11 22:37:34,862: t15.2023.12.03 val PER: 0.2679
2026-01-11 22:37:34,862: t15.2023.12.08 val PER: 0.2803
2026-01-11 22:37:34,862: t15.2023.12.10 val PER: 0.2523
2026-01-11 22:37:34,862: t15.2023.12.17 val PER: 0.2640
2026-01-11 22:37:34,862: t15.2023.12.29 val PER: 0.2766
2026-01-11 22:37:34,862: t15.2024.02.25 val PER: 0.2374
2026-01-11 22:37:34,862: t15.2024.03.08 val PER: 0.3400
2026-01-11 22:37:34,862: t15.2024.03.15 val PER: 0.3083
2026-01-11 22:37:34,862: t15.2024.03.17 val PER: 0.2748
2026-01-11 22:37:34,862: t15.2024.05.10 val PER: 0.2912
2026-01-11 22:37:34,863: t15.2024.06.14 val PER: 0.2618
2026-01-11 22:37:34,863: t15.2024.07.19 val PER: 0.3342
2026-01-11 22:37:34,863: t15.2024.07.21 val PER: 0.2159
2026-01-11 22:37:34,863: t15.2024.07.28 val PER: 0.2757
2026-01-11 22:37:34,863: t15.2025.01.10 val PER: 0.4008
2026-01-11 22:37:34,863: t15.2025.01.12 val PER: 0.2802
2026-01-11 22:37:34,863: t15.2025.03.14 val PER: 0.3802
2026-01-11 22:37:34,863: t15.2025.03.16 val PER: 0.3115
2026-01-11 22:37:34,863: t15.2025.03.30 val PER: 0.3667
2026-01-11 22:37:34,863: t15.2025.04.13 val PER: 0.3367
2026-01-11 22:37:34,864: New best val WER(5gram) 45.63% --> 44.92%
2026-01-11 22:37:35,010: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_19999
2026-01-11 22:37:35,513: Best avg val PER achieved: 0.27928
2026-01-11 22:37:35,514: Total training time: 57.34 minutes

=== RUN diphone_long.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long
2026-01-11 22:39:19,477: Using device: cuda:0
2026-01-11 22:43:14,763: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-11 22:43:14,764: Diphone mode ENABLED: n_classes changed from 41 to 1601
2026-01-11 22:43:14,786: Using 45 sessions after filtering (from 45).
2026-01-11 22:43:15,339: Using torch.compile (if available)
2026-01-11 22:43:15,339: torch.compile not available (torch<2.0). Skipping.
2026-01-11 22:43:15,339: Initialized RNN decoding model
2026-01-11 22:43:15,340: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Identity()
  (out): Linear(in_features=768, out_features=1601, bias=True)
)
2026-01-11 22:43:15,340: Model has 45,514,817 parameters
2026-01-11 22:43:15,340: Model has 11,819,520 day-specific parameters | 25.97% of total parameters
2026-01-11 22:43:17,423: Successfully initialized datasets
2026-01-11 22:43:17,424: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-11 22:43:19,021: Train batch 0: loss: 1387.45 grad norm: 201.84 time: 0.199
2026-01-11 22:43:19,022: Running test after training batch: 0
2026-01-11 22:43:19,151: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:43:26,155: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-11 22:43:27,253: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-11 22:47:50,675: Val batch 0: PER (avg): 4.4638 CTC Loss (avg): 1561.5491 WER(5gram): 100.00% (n=256) time: 271.653
2026-01-11 22:47:50,675: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=0
2026-01-11 22:47:50,675: t15.2023.08.13 val PER: 3.7568
2026-01-11 22:47:50,675: t15.2023.08.18 val PER: 4.0109
2026-01-11 22:47:50,676: t15.2023.08.20 val PER: 3.9738
2026-01-11 22:47:50,676: t15.2023.08.25 val PER: 3.9593
2026-01-11 22:47:50,676: t15.2023.08.27 val PER: 3.7042
2026-01-11 22:47:50,676: t15.2023.09.01 val PER: 4.0593
2026-01-11 22:47:50,676: t15.2023.09.03 val PER: 3.9442
2026-01-11 22:47:50,676: t15.2023.09.24 val PER: 4.6930
2026-01-11 22:47:50,676: t15.2023.09.29 val PER: 4.6362
2026-01-11 22:47:50,676: t15.2023.10.01 val PER: 3.6797
2026-01-11 22:47:50,676: t15.2023.10.06 val PER: 4.4898
2026-01-11 22:47:50,676: t15.2023.10.08 val PER: 3.3559
2026-01-11 22:47:50,676: t15.2023.10.13 val PER: 4.2498
2026-01-11 22:47:50,677: t15.2023.10.15 val PER: 4.7475
2026-01-11 22:47:50,677: t15.2023.10.20 val PER: 4.8188
2026-01-11 22:47:50,677: t15.2023.10.22 val PER: 4.7094
2026-01-11 22:47:50,677: t15.2023.11.03 val PER: 5.0197
2026-01-11 22:47:50,677: t15.2023.11.04 val PER: 6.2526
2026-01-11 22:47:50,677: t15.2023.11.17 val PER: 6.5630
2026-01-11 22:47:50,677: t15.2023.11.19 val PER: 4.9800
2026-01-11 22:47:50,677: t15.2023.11.26 val PER: 4.9536
2026-01-11 22:47:50,677: t15.2023.12.03 val PER: 4.5809
2026-01-11 22:47:50,677: t15.2023.12.08 val PER: 5.0686
2026-01-11 22:47:50,677: t15.2023.12.10 val PER: 5.5046
2026-01-11 22:47:50,677: t15.2023.12.17 val PER: 4.1331
2026-01-11 22:47:50,677: t15.2023.12.29 val PER: 4.4880
2026-01-11 22:47:50,678: t15.2024.02.25 val PER: 4.2360
2026-01-11 22:47:50,678: t15.2024.03.08 val PER: 4.1650
2026-01-11 22:47:50,678: t15.2024.03.15 val PER: 4.0544
2026-01-11 22:47:50,678: t15.2024.03.17 val PER: 4.3417
2026-01-11 22:47:50,678: t15.2024.05.10 val PER: 4.1189
2026-01-11 22:47:50,678: t15.2024.06.14 val PER: 4.6987
2026-01-11 22:47:50,678: t15.2024.07.19 val PER: 3.3223
2026-01-11 22:47:50,678: t15.2024.07.21 val PER: 5.0124
2026-01-11 22:47:50,678: t15.2024.07.28 val PER: 5.2228
2026-01-11 22:47:50,678: t15.2025.01.10 val PER: 3.1336
2026-01-11 22:47:50,678: t15.2025.01.12 val PER: 5.7398
2026-01-11 22:47:50,678: t15.2025.03.14 val PER: 3.0592
2026-01-11 22:47:50,678: t15.2025.03.16 val PER: 5.4817
2026-01-11 22:47:50,678: t15.2025.03.30 val PER: 4.2770
2026-01-11 22:47:50,678: t15.2025.04.13 val PER: 4.8859
2026-01-11 22:47:50,679: New best val WER(5gram) inf% --> 100.00%
2026-01-11 22:47:50,826: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_0
2026-01-11 22:48:08,711: Train batch 200: loss: 186.28 grad norm: 37.96 time: 0.060
2026-01-11 22:48:26,145: Train batch 400: loss: 145.69 grad norm: 100.63 time: 0.069
2026-01-11 22:48:34,935: Running test after training batch: 500
2026-01-11 22:48:35,094: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:48:41,473: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-11 22:48:41,510: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-11 22:48:54,948: Val batch 500: PER (avg): 0.9593 CTC Loss (avg): 170.1357 WER(5gram): 99.74% (n=256) time: 20.013
2026-01-11 22:48:54,950: WER lens: avg_true_words=5.99 avg_pred_words=0.07 max_pred_words=2
2026-01-11 22:48:54,950: t15.2023.08.13 val PER: 0.9553
2026-01-11 22:48:54,950: t15.2023.08.18 val PER: 0.9464
2026-01-11 22:48:54,950: t15.2023.08.20 val PER: 0.9452
2026-01-11 22:48:54,950: t15.2023.08.25 val PER: 0.9533
2026-01-11 22:48:54,950: t15.2023.08.27 val PER: 0.9502
2026-01-11 22:48:54,950: t15.2023.09.01 val PER: 0.9505
2026-01-11 22:48:54,950: t15.2023.09.03 val PER: 0.9584
2026-01-11 22:48:54,951: t15.2023.09.24 val PER: 0.9490
2026-01-11 22:48:54,951: t15.2023.09.29 val PER: 0.9515
2026-01-11 22:48:54,951: t15.2023.10.01 val PER: 0.9643
2026-01-11 22:48:54,951: t15.2023.10.06 val PER: 0.9473
2026-01-11 22:48:54,951: t15.2023.10.08 val PER: 0.9621
2026-01-11 22:48:54,951: t15.2023.10.13 val PER: 0.9573
2026-01-11 22:48:54,951: t15.2023.10.15 val PER: 0.9657
2026-01-11 22:48:54,951: t15.2023.10.20 val PER: 0.9698
2026-01-11 22:48:54,951: t15.2023.10.22 val PER: 0.9588
2026-01-11 22:48:54,951: t15.2023.11.03 val PER: 0.9654
2026-01-11 22:48:54,951: t15.2023.11.04 val PER: 0.9488
2026-01-11 22:48:54,951: t15.2023.11.17 val PER: 0.9611
2026-01-11 22:48:54,951: t15.2023.11.19 val PER: 0.9601
2026-01-11 22:48:54,952: t15.2023.11.26 val PER: 0.9645
2026-01-11 22:48:54,952: t15.2023.12.03 val PER: 0.9611
2026-01-11 22:48:54,952: t15.2023.12.08 val PER: 0.9627
2026-01-11 22:48:54,952: t15.2023.12.10 val PER: 0.9593
2026-01-11 22:48:54,952: t15.2023.12.17 val PER: 0.9688
2026-01-11 22:48:54,952: t15.2023.12.29 val PER: 0.9657
2026-01-11 22:48:54,952: t15.2024.02.25 val PER: 0.9368
2026-01-11 22:48:54,952: t15.2024.03.08 val PER: 0.9602
2026-01-11 22:48:54,952: t15.2024.03.15 val PER: 0.9650
2026-01-11 22:48:54,953: t15.2024.03.17 val PER: 0.9596
2026-01-11 22:48:54,953: t15.2024.05.10 val PER: 0.9629
2026-01-11 22:48:54,953: t15.2024.06.14 val PER: 0.9606
2026-01-11 22:48:54,953: t15.2024.07.19 val PER: 0.9684
2026-01-11 22:48:54,953: t15.2024.07.21 val PER: 0.9683
2026-01-11 22:48:54,953: t15.2024.07.28 val PER: 0.9625
2026-01-11 22:48:54,953: t15.2025.01.10 val PER: 0.9669
2026-01-11 22:48:54,953: t15.2025.01.12 val PER: 0.9492
2026-01-11 22:48:54,953: t15.2025.03.14 val PER: 0.9645
2026-01-11 22:48:54,953: t15.2025.03.16 val PER: 0.9634
2026-01-11 22:48:54,953: t15.2025.03.30 val PER: 0.9609
2026-01-11 22:48:54,954: t15.2025.04.13 val PER: 0.9601
2026-01-11 22:48:54,954: New best val WER(5gram) 100.00% --> 99.74%
2026-01-11 22:48:55,108: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_500
2026-01-11 22:49:04,039: Train batch 600: loss: 154.78 grad norm: 129.94 time: 0.085
2026-01-11 22:49:21,565: Train batch 800: loss: 149.02 grad norm: 77.44 time: 0.064
2026-01-11 22:49:39,454: Train batch 1000: loss: 130.82 grad norm: 57.45 time: 0.072
2026-01-11 22:49:39,454: Running test after training batch: 1000
2026-01-11 22:49:39,578: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:49:45,037: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-11 22:49:45,055: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-11 22:49:49,369: Val batch 1000: PER (avg): 0.9998 CTC Loss (avg): 156.7580 WER(5gram): 100.00% (n=256) time: 9.914
2026-01-11 22:49:49,369: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=0
2026-01-11 22:49:49,369: t15.2023.08.13 val PER: 1.0000
2026-01-11 22:49:49,369: t15.2023.08.18 val PER: 0.9992
2026-01-11 22:49:49,370: t15.2023.08.20 val PER: 1.0000
2026-01-11 22:49:49,370: t15.2023.08.25 val PER: 0.9985
2026-01-11 22:49:49,370: t15.2023.08.27 val PER: 1.0000
2026-01-11 22:49:49,370: t15.2023.09.01 val PER: 0.9992
2026-01-11 22:49:49,370: t15.2023.09.03 val PER: 1.0000
2026-01-11 22:49:49,370: t15.2023.09.24 val PER: 1.0000
2026-01-11 22:49:49,370: t15.2023.09.29 val PER: 1.0000
2026-01-11 22:49:49,370: t15.2023.10.01 val PER: 0.9993
2026-01-11 22:49:49,370: t15.2023.10.06 val PER: 1.0000
2026-01-11 22:49:49,370: t15.2023.10.08 val PER: 0.9986
2026-01-11 22:49:49,370: t15.2023.10.13 val PER: 1.0000
2026-01-11 22:49:49,371: t15.2023.10.15 val PER: 1.0000
2026-01-11 22:49:49,371: t15.2023.10.20 val PER: 1.0000
2026-01-11 22:49:49,371: t15.2023.10.22 val PER: 1.0000
2026-01-11 22:49:49,371: t15.2023.11.03 val PER: 1.0000
2026-01-11 22:49:49,371: t15.2023.11.04 val PER: 1.0000
2026-01-11 22:49:49,371: t15.2023.11.17 val PER: 1.0000
2026-01-11 22:49:49,371: t15.2023.11.19 val PER: 1.0000
2026-01-11 22:49:49,371: t15.2023.11.26 val PER: 1.0000
2026-01-11 22:49:49,371: t15.2023.12.03 val PER: 1.0000
2026-01-11 22:49:49,371: t15.2023.12.08 val PER: 0.9993
2026-01-11 22:49:49,371: t15.2023.12.10 val PER: 1.0000
2026-01-11 22:49:49,371: t15.2023.12.17 val PER: 1.0000
2026-01-11 22:49:49,371: t15.2023.12.29 val PER: 1.0000
2026-01-11 22:49:49,371: t15.2024.02.25 val PER: 0.9986
2026-01-11 22:49:49,372: t15.2024.03.08 val PER: 1.0000
2026-01-11 22:49:49,372: t15.2024.03.15 val PER: 1.0000
2026-01-11 22:49:49,372: t15.2024.03.17 val PER: 0.9993
2026-01-11 22:49:49,372: t15.2024.05.10 val PER: 1.0000
2026-01-11 22:49:49,372: t15.2024.06.14 val PER: 1.0000
2026-01-11 22:49:49,372: t15.2024.07.19 val PER: 1.0000
2026-01-11 22:49:49,372: t15.2024.07.21 val PER: 1.0000
2026-01-11 22:49:49,372: t15.2024.07.28 val PER: 1.0000
2026-01-11 22:49:49,372: t15.2025.01.10 val PER: 1.0000
2026-01-11 22:49:49,372: t15.2025.01.12 val PER: 1.0000
2026-01-11 22:49:49,372: t15.2025.03.14 val PER: 1.0000
2026-01-11 22:49:49,372: t15.2025.03.16 val PER: 1.0000
2026-01-11 22:49:49,372: t15.2025.03.30 val PER: 0.9989
2026-01-11 22:49:49,373: t15.2025.04.13 val PER: 1.0000
2026-01-11 22:49:49,516: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_1000
2026-01-11 22:50:07,092: Train batch 1200: loss: 140.97 grad norm: 87.33 time: 0.075
2026-01-11 22:50:24,800: Train batch 1400: loss: 124.98 grad norm: 148.64 time: 0.068
2026-01-11 22:50:33,626: Running test after training batch: 1500
2026-01-11 22:50:33,739: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:50:39,500: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-11 22:50:39,523: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-11 22:50:45,356: Val batch 1500: PER (avg): 0.8944 CTC Loss (avg): 135.3459 WER(5gram): 100.00% (n=256) time: 11.730
2026-01-11 22:50:45,357: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=1
2026-01-11 22:50:45,357: t15.2023.08.13 val PER: 0.9033
2026-01-11 22:50:45,357: t15.2023.08.18 val PER: 0.8726
2026-01-11 22:50:45,357: t15.2023.08.20 val PER: 0.8761
2026-01-11 22:50:45,357: t15.2023.08.25 val PER: 0.8660
2026-01-11 22:50:45,358: t15.2023.08.27 val PER: 0.9084
2026-01-11 22:50:45,358: t15.2023.09.01 val PER: 0.8709
2026-01-11 22:50:45,358: t15.2023.09.03 val PER: 0.9216
2026-01-11 22:50:45,358: t15.2023.09.24 val PER: 0.8981
2026-01-11 22:50:45,358: t15.2023.09.29 val PER: 0.8787
2026-01-11 22:50:45,358: t15.2023.10.01 val PER: 0.8871
2026-01-11 22:50:45,358: t15.2023.10.06 val PER: 0.8730
2026-01-11 22:50:45,358: t15.2023.10.08 val PER: 0.9039
2026-01-11 22:50:45,358: t15.2023.10.13 val PER: 0.8844
2026-01-11 22:50:45,359: t15.2023.10.15 val PER: 0.8879
2026-01-11 22:50:45,359: t15.2023.10.20 val PER: 0.8859
2026-01-11 22:50:45,359: t15.2023.10.22 val PER: 0.8842
2026-01-11 22:50:45,359: t15.2023.11.03 val PER: 0.8915
2026-01-11 22:50:45,359: t15.2023.11.04 val PER: 0.9283
2026-01-11 22:50:45,359: t15.2023.11.17 val PER: 0.9020
2026-01-11 22:50:45,359: t15.2023.11.19 val PER: 0.9102
2026-01-11 22:50:45,359: t15.2023.11.26 val PER: 0.8739
2026-01-11 22:50:45,359: t15.2023.12.03 val PER: 0.8803
2026-01-11 22:50:45,359: t15.2023.12.08 val PER: 0.8835
2026-01-11 22:50:45,359: t15.2023.12.10 val PER: 0.8804
2026-01-11 22:50:45,360: t15.2023.12.17 val PER: 0.9220
2026-01-11 22:50:45,360: t15.2023.12.29 val PER: 0.9087
2026-01-11 22:50:45,360: t15.2024.02.25 val PER: 0.8904
2026-01-11 22:50:45,360: t15.2024.03.08 val PER: 0.9161
2026-01-11 22:50:45,360: t15.2024.03.15 val PER: 0.9149
2026-01-11 22:50:45,360: t15.2024.03.17 val PER: 0.8898
2026-01-11 22:50:45,360: t15.2024.05.10 val PER: 0.8856
2026-01-11 22:50:45,360: t15.2024.06.14 val PER: 0.8691
2026-01-11 22:50:45,360: t15.2024.07.19 val PER: 0.9314
2026-01-11 22:50:45,360: t15.2024.07.21 val PER: 0.8814
2026-01-11 22:50:45,360: t15.2024.07.28 val PER: 0.8779
2026-01-11 22:50:45,360: t15.2025.01.10 val PER: 0.9477
2026-01-11 22:50:45,361: t15.2025.01.12 val PER: 0.8722
2026-01-11 22:50:45,361: t15.2025.03.14 val PER: 0.9675
2026-01-11 22:50:45,361: t15.2025.03.16 val PER: 0.9031
2026-01-11 22:50:45,361: t15.2025.03.30 val PER: 0.9414
2026-01-11 22:50:45,361: t15.2025.04.13 val PER: 0.8944
2026-01-11 22:50:45,506: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_1500
2026-01-11 22:50:54,216: Train batch 1600: loss: 124.61 grad norm: 49.31 time: 0.071
2026-01-11 22:51:11,855: Train batch 1800: loss: 112.72 grad norm: 76.63 time: 0.095
2026-01-11 22:51:29,699: Train batch 2000: loss: 104.65 grad norm: 70.28 time: 0.073
2026-01-11 22:51:29,700: Running test after training batch: 2000
2026-01-11 22:51:30,119: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:51:38,666: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-11 22:51:38,777: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-11 22:51:58,972: Val batch 2000: PER (avg): 0.6595 CTC Loss (avg): 108.3331 WER(5gram): 98.83% (n=256) time: 29.272
2026-01-11 22:51:58,973: WER lens: avg_true_words=5.99 avg_pred_words=0.09 max_pred_words=1
2026-01-11 22:51:58,973: t15.2023.08.13 val PER: 0.6362
2026-01-11 22:51:58,973: t15.2023.08.18 val PER: 0.6429
2026-01-11 22:51:58,973: t15.2023.08.20 val PER: 0.6267
2026-01-11 22:51:58,973: t15.2023.08.25 val PER: 0.6054
2026-01-11 22:51:58,973: t15.2023.08.27 val PER: 0.6913
2026-01-11 22:51:58,973: t15.2023.09.01 val PER: 0.6177
2026-01-11 22:51:58,974: t15.2023.09.03 val PER: 0.6520
2026-01-11 22:51:58,974: t15.2023.09.24 val PER: 0.6553
2026-01-11 22:51:58,974: t15.2023.09.29 val PER: 0.6535
2026-01-11 22:51:58,974: t15.2023.10.01 val PER: 0.6764
2026-01-11 22:51:58,974: t15.2023.10.06 val PER: 0.6437
2026-01-11 22:51:58,974: t15.2023.10.08 val PER: 0.6901
2026-01-11 22:51:58,974: t15.2023.10.13 val PER: 0.7161
2026-01-11 22:51:58,974: t15.2023.10.15 val PER: 0.6467
2026-01-11 22:51:58,975: t15.2023.10.20 val PER: 0.6309
2026-01-11 22:51:58,975: t15.2023.10.22 val PER: 0.6425
2026-01-11 22:51:58,975: t15.2023.11.03 val PER: 0.6391
2026-01-11 22:51:58,975: t15.2023.11.04 val PER: 0.5768
2026-01-11 22:51:58,975: t15.2023.11.17 val PER: 0.5894
2026-01-11 22:51:58,975: t15.2023.11.19 val PER: 0.5848
2026-01-11 22:51:58,975: t15.2023.11.26 val PER: 0.6986
2026-01-11 22:51:58,975: t15.2023.12.03 val PER: 0.6481
2026-01-11 22:51:58,975: t15.2023.12.08 val PER: 0.6631
2026-01-11 22:51:58,976: t15.2023.12.10 val PER: 0.6505
2026-01-11 22:51:58,976: t15.2023.12.17 val PER: 0.6486
2026-01-11 22:51:58,976: t15.2023.12.29 val PER: 0.6541
2026-01-11 22:51:58,976: t15.2024.02.25 val PER: 0.6390
2026-01-11 22:51:58,976: t15.2024.03.08 val PER: 0.6430
2026-01-11 22:51:58,976: t15.2024.03.15 val PER: 0.6792
2026-01-11 22:51:58,976: t15.2024.03.17 val PER: 0.6590
2026-01-11 22:51:58,976: t15.2024.05.10 val PER: 0.6404
2026-01-11 22:51:58,976: t15.2024.06.14 val PER: 0.6420
2026-01-11 22:51:58,977: t15.2024.07.19 val PER: 0.7047
2026-01-11 22:51:58,977: t15.2024.07.21 val PER: 0.6531
2026-01-11 22:51:58,977: t15.2024.07.28 val PER: 0.6515
2026-01-11 22:51:58,977: t15.2025.01.10 val PER: 0.7383
2026-01-11 22:51:58,977: t15.2025.01.12 val PER: 0.6651
2026-01-11 22:51:58,977: t15.2025.03.14 val PER: 0.7101
2026-01-11 22:51:58,977: t15.2025.03.16 val PER: 0.6806
2026-01-11 22:51:58,977: t15.2025.03.30 val PER: 0.7299
2026-01-11 22:51:58,977: t15.2025.04.13 val PER: 0.6847
2026-01-11 22:51:58,978: New best val WER(5gram) 99.74% --> 98.83%
2026-01-11 22:51:59,141: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_2000
2026-01-11 22:52:16,929: Train batch 2200: loss: 94.12 grad norm: 66.10 time: 0.067
2026-01-11 22:52:34,751: Train batch 2400: loss: 89.34 grad norm: 68.54 time: 0.058
2026-01-11 22:52:43,705: Running test after training batch: 2500
2026-01-11 22:52:43,849: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:52:49,502: WER debug example
  GT : you can see the code at this point as well
  PR : the
2026-01-11 22:52:49,752: WER debug example
  GT : how does it keep the cost down
  PR : i see it in the
2026-01-11 22:53:41,116: Val batch 2500: PER (avg): 0.5289 CTC Loss (avg): 94.5603 WER(5gram): 86.96% (n=256) time: 57.410
2026-01-11 22:53:41,116: WER lens: avg_true_words=5.99 avg_pred_words=2.56 max_pred_words=9
2026-01-11 22:53:41,116: t15.2023.08.13 val PER: 0.4927
2026-01-11 22:53:41,116: t15.2023.08.18 val PER: 0.4987
2026-01-11 22:53:41,116: t15.2023.08.20 val PER: 0.4678
2026-01-11 22:53:41,117: t15.2023.08.25 val PER: 0.4684
2026-01-11 22:53:41,117: t15.2023.08.27 val PER: 0.5707
2026-01-11 22:53:41,117: t15.2023.09.01 val PER: 0.4562
2026-01-11 22:53:41,117: t15.2023.09.03 val PER: 0.5214
2026-01-11 22:53:41,117: t15.2023.09.24 val PER: 0.5061
2026-01-11 22:53:41,117: t15.2023.09.29 val PER: 0.5112
2026-01-11 22:53:41,117: t15.2023.10.01 val PER: 0.5575
2026-01-11 22:53:41,117: t15.2023.10.06 val PER: 0.5199
2026-01-11 22:53:41,117: t15.2023.10.08 val PER: 0.5846
2026-01-11 22:53:41,117: t15.2023.10.13 val PER: 0.6074
2026-01-11 22:53:41,117: t15.2023.10.15 val PER: 0.5247
2026-01-11 22:53:41,118: t15.2023.10.20 val PER: 0.5235
2026-01-11 22:53:41,118: t15.2023.10.22 val PER: 0.5056
2026-01-11 22:53:41,118: t15.2023.11.03 val PER: 0.5061
2026-01-11 22:53:41,118: t15.2023.11.04 val PER: 0.3788
2026-01-11 22:53:41,118: t15.2023.11.17 val PER: 0.4292
2026-01-11 22:53:41,118: t15.2023.11.19 val PER: 0.4172
2026-01-11 22:53:41,118: t15.2023.11.26 val PER: 0.5855
2026-01-11 22:53:41,118: t15.2023.12.03 val PER: 0.5189
2026-01-11 22:53:41,118: t15.2023.12.08 val PER: 0.5300
2026-01-11 22:53:41,118: t15.2023.12.10 val PER: 0.5151
2026-01-11 22:53:41,118: t15.2023.12.17 val PER: 0.5437
2026-01-11 22:53:41,119: t15.2023.12.29 val PER: 0.5353
2026-01-11 22:53:41,119: t15.2024.02.25 val PER: 0.5014
2026-01-11 22:53:41,119: t15.2024.03.08 val PER: 0.5548
2026-01-11 22:53:41,119: t15.2024.03.15 val PER: 0.5385
2026-01-11 22:53:41,119: t15.2024.03.17 val PER: 0.5181
2026-01-11 22:53:41,119: t15.2024.05.10 val PER: 0.5082
2026-01-11 22:53:41,119: t15.2024.06.14 val PER: 0.4874
2026-01-11 22:53:41,119: t15.2024.07.19 val PER: 0.5906
2026-01-11 22:53:41,119: t15.2024.07.21 val PER: 0.5041
2026-01-11 22:53:41,119: t15.2024.07.28 val PER: 0.5132
2026-01-11 22:53:41,119: t15.2025.01.10 val PER: 0.6212
2026-01-11 22:53:41,120: t15.2025.01.12 val PER: 0.5412
2026-01-11 22:53:41,120: t15.2025.03.14 val PER: 0.6036
2026-01-11 22:53:41,120: t15.2025.03.16 val PER: 0.5681
2026-01-11 22:53:41,120: t15.2025.03.30 val PER: 0.6264
2026-01-11 22:53:41,120: t15.2025.04.13 val PER: 0.5663
2026-01-11 22:53:41,120: New best val WER(5gram) 98.83% --> 86.96%
2026-01-11 22:53:41,286: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_2500
2026-01-11 22:53:50,075: Train batch 2600: loss: 95.71 grad norm: 88.04 time: 0.064
2026-01-11 22:54:07,747: Train batch 2800: loss: 77.31 grad norm: 62.96 time: 0.087
2026-01-11 22:54:25,407: Train batch 3000: loss: 88.23 grad norm: 93.19 time: 0.090
2026-01-11 22:54:25,408: Running test after training batch: 3000
2026-01-11 22:54:25,506: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:54:31,117: WER debug example
  GT : you can see the code at this point as well
  PR : you know this is why
2026-01-11 22:54:31,333: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the
2026-01-11 22:55:17,815: Val batch 3000: PER (avg): 0.4849 CTC Loss (avg): 85.7584 WER(5gram): 82.99% (n=256) time: 52.407
2026-01-11 22:55:17,815: WER lens: avg_true_words=5.99 avg_pred_words=3.68 max_pred_words=10
2026-01-11 22:55:17,816: t15.2023.08.13 val PER: 0.4626
2026-01-11 22:55:17,816: t15.2023.08.18 val PER: 0.4535
2026-01-11 22:55:17,816: t15.2023.08.20 val PER: 0.4257
2026-01-11 22:55:17,816: t15.2023.08.25 val PER: 0.4247
2026-01-11 22:55:17,816: t15.2023.08.27 val PER: 0.5338
2026-01-11 22:55:17,816: t15.2023.09.01 val PER: 0.4091
2026-01-11 22:55:17,816: t15.2023.09.03 val PER: 0.4727
2026-01-11 22:55:17,816: t15.2023.09.24 val PER: 0.4612
2026-01-11 22:55:17,816: t15.2023.09.29 val PER: 0.4761
2026-01-11 22:55:17,816: t15.2023.10.01 val PER: 0.5132
2026-01-11 22:55:17,816: t15.2023.10.06 val PER: 0.4532
2026-01-11 22:55:17,816: t15.2023.10.08 val PER: 0.5413
2026-01-11 22:55:17,816: t15.2023.10.13 val PER: 0.5772
2026-01-11 22:55:17,816: t15.2023.10.15 val PER: 0.4911
2026-01-11 22:55:17,817: t15.2023.10.20 val PER: 0.4631
2026-01-11 22:55:17,817: t15.2023.10.22 val PER: 0.4543
2026-01-11 22:55:17,817: t15.2023.11.03 val PER: 0.4552
2026-01-11 22:55:17,817: t15.2023.11.04 val PER: 0.3379
2026-01-11 22:55:17,817: t15.2023.11.17 val PER: 0.3670
2026-01-11 22:55:17,817: t15.2023.11.19 val PER: 0.3553
2026-01-11 22:55:17,817: t15.2023.11.26 val PER: 0.5370
2026-01-11 22:55:17,817: t15.2023.12.03 val PER: 0.4706
2026-01-11 22:55:17,817: t15.2023.12.08 val PER: 0.4794
2026-01-11 22:55:17,817: t15.2023.12.10 val PER: 0.4731
2026-01-11 22:55:17,818: t15.2023.12.17 val PER: 0.4771
2026-01-11 22:55:17,818: t15.2023.12.29 val PER: 0.4969
2026-01-11 22:55:17,818: t15.2024.02.25 val PER: 0.4508
2026-01-11 22:55:17,818: t15.2024.03.08 val PER: 0.5164
2026-01-11 22:55:17,818: t15.2024.03.15 val PER: 0.4997
2026-01-11 22:55:17,818: t15.2024.03.17 val PER: 0.4819
2026-01-11 22:55:17,818: t15.2024.05.10 val PER: 0.4799
2026-01-11 22:55:17,818: t15.2024.06.14 val PER: 0.4322
2026-01-11 22:55:17,818: t15.2024.07.19 val PER: 0.5610
2026-01-11 22:55:17,818: t15.2024.07.21 val PER: 0.4407
2026-01-11 22:55:17,818: t15.2024.07.28 val PER: 0.4779
2026-01-11 22:55:17,818: t15.2025.01.10 val PER: 0.5647
2026-01-11 22:55:17,818: t15.2025.01.12 val PER: 0.4973
2026-01-11 22:55:17,818: t15.2025.03.14 val PER: 0.5740
2026-01-11 22:55:17,818: t15.2025.03.16 val PER: 0.5406
2026-01-11 22:55:17,819: t15.2025.03.30 val PER: 0.5816
2026-01-11 22:55:17,819: t15.2025.04.13 val PER: 0.5207
2026-01-11 22:55:17,819: New best val WER(5gram) 86.96% --> 82.99%
2026-01-11 22:55:17,981: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_3000
2026-01-11 22:55:35,709: Train batch 3200: loss: 75.92 grad norm: 73.21 time: 0.083
2026-01-11 22:55:53,473: Train batch 3400: loss: 64.89 grad norm: 65.97 time: 0.055
2026-01-11 22:56:02,361: Running test after training batch: 3500
2026-01-11 22:56:02,479: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:56:08,180: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy in the
2026-01-11 22:56:08,394: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the new
2026-01-11 22:56:54,106: Val batch 3500: PER (avg): 0.4509 CTC Loss (avg): 79.2416 WER(5gram): 78.94% (n=256) time: 51.744
2026-01-11 22:56:54,106: WER lens: avg_true_words=5.99 avg_pred_words=4.34 max_pred_words=11
2026-01-11 22:56:54,106: t15.2023.08.13 val PER: 0.4116
2026-01-11 22:56:54,107: t15.2023.08.18 val PER: 0.4199
2026-01-11 22:56:54,107: t15.2023.08.20 val PER: 0.4035
2026-01-11 22:56:54,107: t15.2023.08.25 val PER: 0.3946
2026-01-11 22:56:54,107: t15.2023.08.27 val PER: 0.4984
2026-01-11 22:56:54,107: t15.2023.09.01 val PER: 0.3904
2026-01-11 22:56:54,107: t15.2023.09.03 val PER: 0.4442
2026-01-11 22:56:54,107: t15.2023.09.24 val PER: 0.4114
2026-01-11 22:56:54,107: t15.2023.09.29 val PER: 0.4352
2026-01-11 22:56:54,108: t15.2023.10.01 val PER: 0.4795
2026-01-11 22:56:54,108: t15.2023.10.06 val PER: 0.4144
2026-01-11 22:56:54,108: t15.2023.10.08 val PER: 0.5237
2026-01-11 22:56:54,108: t15.2023.10.13 val PER: 0.5392
2026-01-11 22:56:54,108: t15.2023.10.15 val PER: 0.4555
2026-01-11 22:56:54,108: t15.2023.10.20 val PER: 0.4564
2026-01-11 22:56:54,108: t15.2023.10.22 val PER: 0.4143
2026-01-11 22:56:54,108: t15.2023.11.03 val PER: 0.4193
2026-01-11 22:56:54,108: t15.2023.11.04 val PER: 0.2696
2026-01-11 22:56:54,108: t15.2023.11.17 val PER: 0.3266
2026-01-11 22:56:54,109: t15.2023.11.19 val PER: 0.2974
2026-01-11 22:56:54,109: t15.2023.11.26 val PER: 0.4986
2026-01-11 22:56:54,109: t15.2023.12.03 val PER: 0.4454
2026-01-11 22:56:54,109: t15.2023.12.08 val PER: 0.4574
2026-01-11 22:56:54,109: t15.2023.12.10 val PER: 0.4350
2026-01-11 22:56:54,109: t15.2023.12.17 val PER: 0.4231
2026-01-11 22:56:54,109: t15.2023.12.29 val PER: 0.4619
2026-01-11 22:56:54,109: t15.2024.02.25 val PER: 0.3862
2026-01-11 22:56:54,109: t15.2024.03.08 val PER: 0.4780
2026-01-11 22:56:54,109: t15.2024.03.15 val PER: 0.4703
2026-01-11 22:56:54,110: t15.2024.03.17 val PER: 0.4414
2026-01-11 22:56:54,110: t15.2024.05.10 val PER: 0.4591
2026-01-11 22:56:54,110: t15.2024.06.14 val PER: 0.3991
2026-01-11 22:56:54,110: t15.2024.07.19 val PER: 0.5339
2026-01-11 22:56:54,110: t15.2024.07.21 val PER: 0.4110
2026-01-11 22:56:54,110: t15.2024.07.28 val PER: 0.4449
2026-01-11 22:56:54,110: t15.2025.01.10 val PER: 0.5372
2026-01-11 22:56:54,110: t15.2025.01.12 val PER: 0.4511
2026-01-11 22:56:54,110: t15.2025.03.14 val PER: 0.5666
2026-01-11 22:56:54,110: t15.2025.03.16 val PER: 0.5065
2026-01-11 22:56:54,110: t15.2025.03.30 val PER: 0.5621
2026-01-11 22:56:54,110: t15.2025.04.13 val PER: 0.4979
2026-01-11 22:56:54,111: New best val WER(5gram) 82.99% --> 78.94%
2026-01-11 22:56:54,273: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_3500
2026-01-11 22:57:03,214: Train batch 3600: loss: 68.14 grad norm: 69.57 time: 0.074
2026-01-11 22:57:20,754: Train batch 3800: loss: 75.13 grad norm: 69.06 time: 0.079
2026-01-11 22:57:38,383: Train batch 4000: loss: 60.30 grad norm: 66.54 time: 0.067
2026-01-11 22:57:38,384: Running test after training batch: 4000
2026-01-11 22:57:38,531: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:57:44,281: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy in the us were
2026-01-11 22:57:44,475: WER debug example
  GT : how does it keep the cost down
  PR : i see it go the way the
2026-01-11 22:58:26,781: Val batch 4000: PER (avg): 0.4241 CTC Loss (avg): 74.2143 WER(5gram): 77.18% (n=256) time: 48.397
2026-01-11 22:58:26,781: WER lens: avg_true_words=5.99 avg_pred_words=4.93 max_pred_words=11
2026-01-11 22:58:26,782: t15.2023.08.13 val PER: 0.3877
2026-01-11 22:58:26,782: t15.2023.08.18 val PER: 0.3906
2026-01-11 22:58:26,782: t15.2023.08.20 val PER: 0.3757
2026-01-11 22:58:26,782: t15.2023.08.25 val PER: 0.3735
2026-01-11 22:58:26,782: t15.2023.08.27 val PER: 0.4678
2026-01-11 22:58:26,782: t15.2023.09.01 val PER: 0.3515
2026-01-11 22:58:26,782: t15.2023.09.03 val PER: 0.4287
2026-01-11 22:58:26,782: t15.2023.09.24 val PER: 0.3823
2026-01-11 22:58:26,783: t15.2023.09.29 val PER: 0.4033
2026-01-11 22:58:26,783: t15.2023.10.01 val PER: 0.4657
2026-01-11 22:58:26,783: t15.2023.10.06 val PER: 0.3918
2026-01-11 22:58:26,783: t15.2023.10.08 val PER: 0.4966
2026-01-11 22:58:26,783: t15.2023.10.13 val PER: 0.5221
2026-01-11 22:58:26,783: t15.2023.10.15 val PER: 0.4364
2026-01-11 22:58:26,783: t15.2023.10.20 val PER: 0.4396
2026-01-11 22:58:26,783: t15.2023.10.22 val PER: 0.3808
2026-01-11 22:58:26,783: t15.2023.11.03 val PER: 0.4098
2026-01-11 22:58:26,783: t15.2023.11.04 val PER: 0.2253
2026-01-11 22:58:26,783: t15.2023.11.17 val PER: 0.2799
2026-01-11 22:58:26,784: t15.2023.11.19 val PER: 0.2774
2026-01-11 22:58:26,784: t15.2023.11.26 val PER: 0.4732
2026-01-11 22:58:26,784: t15.2023.12.03 val PER: 0.4044
2026-01-11 22:58:26,784: t15.2023.12.08 val PER: 0.4188
2026-01-11 22:58:26,784: t15.2023.12.10 val PER: 0.3903
2026-01-11 22:58:26,784: t15.2023.12.17 val PER: 0.3981
2026-01-11 22:58:26,784: t15.2023.12.29 val PER: 0.4420
2026-01-11 22:58:26,784: t15.2024.02.25 val PER: 0.3694
2026-01-11 22:58:26,784: t15.2024.03.08 val PER: 0.4609
2026-01-11 22:58:26,784: t15.2024.03.15 val PER: 0.4447
2026-01-11 22:58:26,784: t15.2024.03.17 val PER: 0.4240
2026-01-11 22:58:26,785: t15.2024.05.10 val PER: 0.4131
2026-01-11 22:58:26,785: t15.2024.06.14 val PER: 0.3785
2026-01-11 22:58:26,785: t15.2024.07.19 val PER: 0.4931
2026-01-11 22:58:26,785: t15.2024.07.21 val PER: 0.3759
2026-01-11 22:58:26,785: t15.2024.07.28 val PER: 0.4191
2026-01-11 22:58:26,785: t15.2025.01.10 val PER: 0.5179
2026-01-11 22:58:26,785: t15.2025.01.12 val PER: 0.4226
2026-01-11 22:58:26,785: t15.2025.03.14 val PER: 0.5370
2026-01-11 22:58:26,785: t15.2025.03.16 val PER: 0.4948
2026-01-11 22:58:26,785: t15.2025.03.30 val PER: 0.5287
2026-01-11 22:58:26,785: t15.2025.04.13 val PER: 0.4650
2026-01-11 22:58:26,786: New best val WER(5gram) 78.94% --> 77.18%
2026-01-11 22:58:26,945: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_4000
2026-01-11 22:58:44,675: Train batch 4200: loss: 62.60 grad norm: 86.52 time: 0.092
2026-01-11 22:59:02,516: Train batch 4400: loss: 55.00 grad norm: 76.61 time: 0.073
2026-01-11 22:59:11,413: Running test after training batch: 4500
2026-01-11 22:59:11,531: WER debug GT example: You can see the code at this point as well.
2026-01-11 22:59:17,192: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy in the way it is why
2026-01-11 22:59:17,362: WER debug example
  GT : how does it keep the cost down
  PR : i do see it go the way the
2026-01-11 23:00:01,190: Val batch 4500: PER (avg): 0.4065 CTC Loss (avg): 69.8045 WER(5gram): 75.29% (n=256) time: 49.776
2026-01-11 23:00:01,191: WER lens: avg_true_words=5.99 avg_pred_words=4.93 max_pred_words=11
2026-01-11 23:00:01,191: t15.2023.08.13 val PER: 0.3701
2026-01-11 23:00:01,191: t15.2023.08.18 val PER: 0.3831
2026-01-11 23:00:01,191: t15.2023.08.20 val PER: 0.3503
2026-01-11 23:00:01,191: t15.2023.08.25 val PER: 0.3404
2026-01-11 23:00:01,192: t15.2023.08.27 val PER: 0.4582
2026-01-11 23:00:01,192: t15.2023.09.01 val PER: 0.3450
2026-01-11 23:00:01,192: t15.2023.09.03 val PER: 0.4169
2026-01-11 23:00:01,192: t15.2023.09.24 val PER: 0.3653
2026-01-11 23:00:01,192: t15.2023.09.29 val PER: 0.3950
2026-01-11 23:00:01,192: t15.2023.10.01 val PER: 0.4432
2026-01-11 23:00:01,192: t15.2023.10.06 val PER: 0.3509
2026-01-11 23:00:01,192: t15.2023.10.08 val PER: 0.4655
2026-01-11 23:00:01,192: t15.2023.10.13 val PER: 0.5027
2026-01-11 23:00:01,193: t15.2023.10.15 val PER: 0.4067
2026-01-11 23:00:01,193: t15.2023.10.20 val PER: 0.4094
2026-01-11 23:00:01,193: t15.2023.10.22 val PER: 0.3742
2026-01-11 23:00:01,193: t15.2023.11.03 val PER: 0.4023
2026-01-11 23:00:01,193: t15.2023.11.04 val PER: 0.1843
2026-01-11 23:00:01,193: t15.2023.11.17 val PER: 0.2753
2026-01-11 23:00:01,193: t15.2023.11.19 val PER: 0.2555
2026-01-11 23:00:01,193: t15.2023.11.26 val PER: 0.4681
2026-01-11 23:00:01,193: t15.2023.12.03 val PER: 0.3918
2026-01-11 23:00:01,194: t15.2023.12.08 val PER: 0.4115
2026-01-11 23:00:01,194: t15.2023.12.10 val PER: 0.3876
2026-01-11 23:00:01,194: t15.2023.12.17 val PER: 0.3815
2026-01-11 23:00:01,194: t15.2023.12.29 val PER: 0.4214
2026-01-11 23:00:01,194: t15.2024.02.25 val PER: 0.3427
2026-01-11 23:00:01,194: t15.2024.03.08 val PER: 0.4282
2026-01-11 23:00:01,194: t15.2024.03.15 val PER: 0.4278
2026-01-11 23:00:01,194: t15.2024.03.17 val PER: 0.3996
2026-01-11 23:00:01,194: t15.2024.05.10 val PER: 0.4175
2026-01-11 23:00:01,195: t15.2024.06.14 val PER: 0.3833
2026-01-11 23:00:01,195: t15.2024.07.19 val PER: 0.4647
2026-01-11 23:00:01,195: t15.2024.07.21 val PER: 0.3593
2026-01-11 23:00:01,195: t15.2024.07.28 val PER: 0.3963
2026-01-11 23:00:01,195: t15.2025.01.10 val PER: 0.5014
2026-01-11 23:00:01,195: t15.2025.01.12 val PER: 0.4095
2026-01-11 23:00:01,195: t15.2025.03.14 val PER: 0.5118
2026-01-11 23:00:01,195: t15.2025.03.16 val PER: 0.4634
2026-01-11 23:00:01,195: t15.2025.03.30 val PER: 0.4943
2026-01-11 23:00:01,195: t15.2025.04.13 val PER: 0.4565
2026-01-11 23:00:01,196: New best val WER(5gram) 77.18% --> 75.29%
2026-01-11 23:00:01,348: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_4500
2026-01-11 23:00:10,278: Train batch 4600: loss: 59.23 grad norm: 86.44 time: 0.070
2026-01-11 23:00:28,289: Train batch 4800: loss: 49.08 grad norm: 79.09 time: 0.070
2026-01-11 23:00:45,992: Train batch 5000: loss: 84.58 grad norm: 118.94 time: 0.072
2026-01-11 23:00:45,992: Running test after training batch: 5000
2026-01-11 23:00:46,105: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:00:51,757: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at the us were
2026-01-11 23:00:51,913: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the way the
2026-01-11 23:01:28,392: Val batch 5000: PER (avg): 0.3907 CTC Loss (avg): 66.0112 WER(5gram): 72.62% (n=256) time: 42.400
2026-01-11 23:01:28,393: WER lens: avg_true_words=5.99 avg_pred_words=5.01 max_pred_words=12
2026-01-11 23:01:28,393: t15.2023.08.13 val PER: 0.3524
2026-01-11 23:01:28,393: t15.2023.08.18 val PER: 0.3671
2026-01-11 23:01:28,393: t15.2023.08.20 val PER: 0.3336
2026-01-11 23:01:28,393: t15.2023.08.25 val PER: 0.3313
2026-01-11 23:01:28,394: t15.2023.08.27 val PER: 0.4293
2026-01-11 23:01:28,394: t15.2023.09.01 val PER: 0.3198
2026-01-11 23:01:28,394: t15.2023.09.03 val PER: 0.3777
2026-01-11 23:01:28,394: t15.2023.09.24 val PER: 0.3374
2026-01-11 23:01:28,394: t15.2023.09.29 val PER: 0.3778
2026-01-11 23:01:28,394: t15.2023.10.01 val PER: 0.4267
2026-01-11 23:01:28,394: t15.2023.10.06 val PER: 0.3326
2026-01-11 23:01:28,394: t15.2023.10.08 val PER: 0.4804
2026-01-11 23:01:28,394: t15.2023.10.13 val PER: 0.4934
2026-01-11 23:01:28,394: t15.2023.10.15 val PER: 0.3962
2026-01-11 23:01:28,394: t15.2023.10.20 val PER: 0.3926
2026-01-11 23:01:28,395: t15.2023.10.22 val PER: 0.3653
2026-01-11 23:01:28,395: t15.2023.11.03 val PER: 0.3772
2026-01-11 23:01:28,395: t15.2023.11.04 val PER: 0.1604
2026-01-11 23:01:28,395: t15.2023.11.17 val PER: 0.2613
2026-01-11 23:01:28,395: t15.2023.11.19 val PER: 0.2375
2026-01-11 23:01:28,395: t15.2023.11.26 val PER: 0.4558
2026-01-11 23:01:28,395: t15.2023.12.03 val PER: 0.3750
2026-01-11 23:01:28,395: t15.2023.12.08 val PER: 0.3928
2026-01-11 23:01:28,395: t15.2023.12.10 val PER: 0.3758
2026-01-11 23:01:28,395: t15.2023.12.17 val PER: 0.3617
2026-01-11 23:01:28,395: t15.2023.12.29 val PER: 0.4125
2026-01-11 23:01:28,396: t15.2024.02.25 val PER: 0.3315
2026-01-11 23:01:28,396: t15.2024.03.08 val PER: 0.4253
2026-01-11 23:01:28,396: t15.2024.03.15 val PER: 0.4203
2026-01-11 23:01:28,396: t15.2024.03.17 val PER: 0.3905
2026-01-11 23:01:28,396: t15.2024.05.10 val PER: 0.3848
2026-01-11 23:01:28,396: t15.2024.06.14 val PER: 0.3596
2026-01-11 23:01:28,396: t15.2024.07.19 val PER: 0.4410
2026-01-11 23:01:28,396: t15.2024.07.21 val PER: 0.3497
2026-01-11 23:01:28,396: t15.2024.07.28 val PER: 0.3831
2026-01-11 23:01:28,396: t15.2025.01.10 val PER: 0.4959
2026-01-11 23:01:28,396: t15.2025.01.12 val PER: 0.3849
2026-01-11 23:01:28,397: t15.2025.03.14 val PER: 0.4956
2026-01-11 23:01:28,397: t15.2025.03.16 val PER: 0.4450
2026-01-11 23:01:28,397: t15.2025.03.30 val PER: 0.4874
2026-01-11 23:01:28,397: t15.2025.04.13 val PER: 0.4322
2026-01-11 23:01:28,398: New best val WER(5gram) 75.29% --> 72.62%
2026-01-11 23:01:28,563: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_5000
2026-01-11 23:01:46,245: Train batch 5200: loss: 56.51 grad norm: 83.90 time: 0.058
2026-01-11 23:02:04,034: Train batch 5400: loss: 60.18 grad norm: 78.48 time: 0.076
2026-01-11 23:02:12,925: Running test after training batch: 5500
2026-01-11 23:02:13,124: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:02:18,746: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at the boy were
2026-01-11 23:02:18,886: WER debug example
  GT : how does it keep the cost down
  PR : i see it in the sa new
2026-01-11 23:02:53,113: Val batch 5500: PER (avg): 0.3805 CTC Loss (avg): 63.0336 WER(5gram): 71.06% (n=256) time: 40.188
2026-01-11 23:02:53,114: WER lens: avg_true_words=5.99 avg_pred_words=5.14 max_pred_words=13
2026-01-11 23:02:53,114: t15.2023.08.13 val PER: 0.3430
2026-01-11 23:02:53,114: t15.2023.08.18 val PER: 0.3521
2026-01-11 23:02:53,114: t15.2023.08.20 val PER: 0.3264
2026-01-11 23:02:53,114: t15.2023.08.25 val PER: 0.3133
2026-01-11 23:02:53,115: t15.2023.08.27 val PER: 0.4373
2026-01-11 23:02:53,115: t15.2023.09.01 val PER: 0.2987
2026-01-11 23:02:53,115: t15.2023.09.03 val PER: 0.3848
2026-01-11 23:02:53,115: t15.2023.09.24 val PER: 0.3265
2026-01-11 23:02:53,115: t15.2023.09.29 val PER: 0.3631
2026-01-11 23:02:53,115: t15.2023.10.01 val PER: 0.4234
2026-01-11 23:02:53,115: t15.2023.10.06 val PER: 0.3165
2026-01-11 23:02:53,115: t15.2023.10.08 val PER: 0.4614
2026-01-11 23:02:53,116: t15.2023.10.13 val PER: 0.4856
2026-01-11 23:02:53,116: t15.2023.10.15 val PER: 0.3837
2026-01-11 23:02:53,116: t15.2023.10.20 val PER: 0.4027
2026-01-11 23:02:53,116: t15.2023.10.22 val PER: 0.3474
2026-01-11 23:02:53,116: t15.2023.11.03 val PER: 0.3772
2026-01-11 23:02:53,116: t15.2023.11.04 val PER: 0.1433
2026-01-11 23:02:53,116: t15.2023.11.17 val PER: 0.2395
2026-01-11 23:02:53,116: t15.2023.11.19 val PER: 0.2136
2026-01-11 23:02:53,116: t15.2023.11.26 val PER: 0.4420
2026-01-11 23:02:53,117: t15.2023.12.03 val PER: 0.3729
2026-01-11 23:02:53,117: t15.2023.12.08 val PER: 0.3895
2026-01-11 23:02:53,117: t15.2023.12.10 val PER: 0.3561
2026-01-11 23:02:53,117: t15.2023.12.17 val PER: 0.3607
2026-01-11 23:02:53,117: t15.2023.12.29 val PER: 0.4097
2026-01-11 23:02:53,117: t15.2024.02.25 val PER: 0.3188
2026-01-11 23:02:53,117: t15.2024.03.08 val PER: 0.4125
2026-01-11 23:02:53,117: t15.2024.03.15 val PER: 0.3971
2026-01-11 23:02:53,117: t15.2024.03.17 val PER: 0.3773
2026-01-11 23:02:53,117: t15.2024.05.10 val PER: 0.3893
2026-01-11 23:02:53,118: t15.2024.06.14 val PER: 0.3486
2026-01-11 23:02:53,118: t15.2024.07.19 val PER: 0.4417
2026-01-11 23:02:53,118: t15.2024.07.21 val PER: 0.3338
2026-01-11 23:02:53,118: t15.2024.07.28 val PER: 0.3824
2026-01-11 23:02:53,118: t15.2025.01.10 val PER: 0.4848
2026-01-11 23:02:53,118: t15.2025.01.12 val PER: 0.3711
2026-01-11 23:02:53,118: t15.2025.03.14 val PER: 0.4793
2026-01-11 23:02:53,118: t15.2025.03.16 val PER: 0.4319
2026-01-11 23:02:53,118: t15.2025.03.30 val PER: 0.4759
2026-01-11 23:02:53,119: t15.2025.04.13 val PER: 0.4108
2026-01-11 23:02:53,119: New best val WER(5gram) 72.62% --> 71.06%
2026-01-11 23:02:56,186: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_5500
2026-01-11 23:03:15,847: Train batch 5600: loss: 58.95 grad norm: 106.64 time: 0.070
2026-01-11 23:03:34,498: Train batch 5800: loss: 45.87 grad norm: 73.87 time: 0.088
2026-01-11 23:03:52,027: Train batch 6000: loss: 43.02 grad norm: 89.70 time: 0.055
2026-01-11 23:03:52,027: Running test after training batch: 6000
2026-01-11 23:03:52,157: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:03:58,523: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is we
2026-01-11 23:03:58,662: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the sa new
2026-01-11 23:04:30,460: Val batch 6000: PER (avg): 0.3683 CTC Loss (avg): 60.7187 WER(5gram): 67.60% (n=256) time: 38.432
2026-01-11 23:04:30,460: WER lens: avg_true_words=5.99 avg_pred_words=5.23 max_pred_words=10
2026-01-11 23:04:30,460: t15.2023.08.13 val PER: 0.3337
2026-01-11 23:04:30,461: t15.2023.08.18 val PER: 0.3370
2026-01-11 23:04:30,461: t15.2023.08.20 val PER: 0.3042
2026-01-11 23:04:30,461: t15.2023.08.25 val PER: 0.3102
2026-01-11 23:04:30,461: t15.2023.08.27 val PER: 0.4084
2026-01-11 23:04:30,461: t15.2023.09.01 val PER: 0.2744
2026-01-11 23:04:30,461: t15.2023.09.03 val PER: 0.3563
2026-01-11 23:04:30,461: t15.2023.09.24 val PER: 0.3192
2026-01-11 23:04:30,461: t15.2023.09.29 val PER: 0.3529
2026-01-11 23:04:30,462: t15.2023.10.01 val PER: 0.4161
2026-01-11 23:04:30,462: t15.2023.10.06 val PER: 0.3111
2026-01-11 23:04:30,462: t15.2023.10.08 val PER: 0.4628
2026-01-11 23:04:30,462: t15.2023.10.13 val PER: 0.4740
2026-01-11 23:04:30,462: t15.2023.10.15 val PER: 0.3790
2026-01-11 23:04:30,462: t15.2023.10.20 val PER: 0.3758
2026-01-11 23:04:30,462: t15.2023.10.22 val PER: 0.3252
2026-01-11 23:04:30,462: t15.2023.11.03 val PER: 0.3636
2026-01-11 23:04:30,462: t15.2023.11.04 val PER: 0.1365
2026-01-11 23:04:30,462: t15.2023.11.17 val PER: 0.2302
2026-01-11 23:04:30,463: t15.2023.11.19 val PER: 0.2116
2026-01-11 23:04:30,463: t15.2023.11.26 val PER: 0.4370
2026-01-11 23:04:30,463: t15.2023.12.03 val PER: 0.3550
2026-01-11 23:04:30,463: t15.2023.12.08 val PER: 0.3762
2026-01-11 23:04:30,463: t15.2023.12.10 val PER: 0.3495
2026-01-11 23:04:30,463: t15.2023.12.17 val PER: 0.3368
2026-01-11 23:04:30,463: t15.2023.12.29 val PER: 0.3761
2026-01-11 23:04:30,463: t15.2024.02.25 val PER: 0.3048
2026-01-11 23:04:30,463: t15.2024.03.08 val PER: 0.4139
2026-01-11 23:04:30,463: t15.2024.03.15 val PER: 0.3946
2026-01-11 23:04:30,464: t15.2024.03.17 val PER: 0.3605
2026-01-11 23:04:30,464: t15.2024.05.10 val PER: 0.3759
2026-01-11 23:04:30,464: t15.2024.06.14 val PER: 0.3407
2026-01-11 23:04:30,464: t15.2024.07.19 val PER: 0.4232
2026-01-11 23:04:30,464: t15.2024.07.21 val PER: 0.3262
2026-01-11 23:04:30,464: t15.2024.07.28 val PER: 0.3721
2026-01-11 23:04:30,464: t15.2025.01.10 val PER: 0.4766
2026-01-11 23:04:30,464: t15.2025.01.12 val PER: 0.3541
2026-01-11 23:04:30,464: t15.2025.03.14 val PER: 0.4867
2026-01-11 23:04:30,464: t15.2025.03.16 val PER: 0.4254
2026-01-11 23:04:30,464: t15.2025.03.30 val PER: 0.4667
2026-01-11 23:04:30,465: t15.2025.04.13 val PER: 0.4080
2026-01-11 23:04:30,465: New best val WER(5gram) 71.06% --> 67.60%
2026-01-11 23:04:30,630: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_6000
2026-01-11 23:04:48,064: Train batch 6200: loss: 52.08 grad norm: 86.02 time: 0.080
2026-01-11 23:05:05,429: Train batch 6400: loss: 59.86 grad norm: 94.96 time: 0.071
2026-01-11 23:05:14,070: Running test after training batch: 6500
2026-01-11 23:05:14,185: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:05:19,702: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is we
2026-01-11 23:05:19,845: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the sa
2026-01-11 23:05:50,436: Val batch 6500: PER (avg): 0.3547 CTC Loss (avg): 58.2236 WER(5gram): 66.30% (n=256) time: 36.365
2026-01-11 23:05:50,437: WER lens: avg_true_words=5.99 avg_pred_words=5.37 max_pred_words=13
2026-01-11 23:05:50,437: t15.2023.08.13 val PER: 0.3222
2026-01-11 23:05:50,437: t15.2023.08.18 val PER: 0.3227
2026-01-11 23:05:50,437: t15.2023.08.20 val PER: 0.2891
2026-01-11 23:05:50,437: t15.2023.08.25 val PER: 0.3117
2026-01-11 23:05:50,437: t15.2023.08.27 val PER: 0.3842
2026-01-11 23:05:50,437: t15.2023.09.01 val PER: 0.2589
2026-01-11 23:05:50,437: t15.2023.09.03 val PER: 0.3492
2026-01-11 23:05:50,437: t15.2023.09.24 val PER: 0.3010
2026-01-11 23:05:50,438: t15.2023.09.29 val PER: 0.3299
2026-01-11 23:05:50,438: t15.2023.10.01 val PER: 0.3963
2026-01-11 23:05:50,438: t15.2023.10.06 val PER: 0.3111
2026-01-11 23:05:50,438: t15.2023.10.08 val PER: 0.4384
2026-01-11 23:05:50,438: t15.2023.10.13 val PER: 0.4616
2026-01-11 23:05:50,438: t15.2023.10.15 val PER: 0.3698
2026-01-11 23:05:50,438: t15.2023.10.20 val PER: 0.3423
2026-01-11 23:05:50,438: t15.2023.10.22 val PER: 0.3096
2026-01-11 23:05:50,438: t15.2023.11.03 val PER: 0.3589
2026-01-11 23:05:50,438: t15.2023.11.04 val PER: 0.1263
2026-01-11 23:05:50,438: t15.2023.11.17 val PER: 0.2193
2026-01-11 23:05:50,438: t15.2023.11.19 val PER: 0.2016
2026-01-11 23:05:50,438: t15.2023.11.26 val PER: 0.4167
2026-01-11 23:05:50,439: t15.2023.12.03 val PER: 0.3466
2026-01-11 23:05:50,439: t15.2023.12.08 val PER: 0.3609
2026-01-11 23:05:50,439: t15.2023.12.10 val PER: 0.3456
2026-01-11 23:05:50,439: t15.2023.12.17 val PER: 0.3274
2026-01-11 23:05:50,439: t15.2023.12.29 val PER: 0.3706
2026-01-11 23:05:50,439: t15.2024.02.25 val PER: 0.2921
2026-01-11 23:05:50,439: t15.2024.03.08 val PER: 0.3954
2026-01-11 23:05:50,439: t15.2024.03.15 val PER: 0.3721
2026-01-11 23:05:50,439: t15.2024.03.17 val PER: 0.3543
2026-01-11 23:05:50,439: t15.2024.05.10 val PER: 0.3700
2026-01-11 23:05:50,439: t15.2024.06.14 val PER: 0.3344
2026-01-11 23:05:50,439: t15.2024.07.19 val PER: 0.4087
2026-01-11 23:05:50,440: t15.2024.07.21 val PER: 0.3055
2026-01-11 23:05:50,440: t15.2024.07.28 val PER: 0.3559
2026-01-11 23:05:50,440: t15.2025.01.10 val PER: 0.4601
2026-01-11 23:05:50,440: t15.2025.01.12 val PER: 0.3503
2026-01-11 23:05:50,440: t15.2025.03.14 val PER: 0.4808
2026-01-11 23:05:50,440: t15.2025.03.16 val PER: 0.3822
2026-01-11 23:05:50,440: t15.2025.03.30 val PER: 0.4552
2026-01-11 23:05:50,440: t15.2025.04.13 val PER: 0.3937
2026-01-11 23:05:50,441: New best val WER(5gram) 67.60% --> 66.30%
2026-01-11 23:05:50,595: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_6500
2026-01-11 23:05:59,224: Train batch 6600: loss: 36.44 grad norm: 52.60 time: 0.053
2026-01-11 23:06:16,720: Train batch 6800: loss: 47.26 grad norm: 71.66 time: 0.055
2026-01-11 23:06:34,285: Train batch 7000: loss: 49.76 grad norm: 89.69 time: 0.068
2026-01-11 23:06:34,285: Running test after training batch: 7000
2026-01-11 23:06:34,430: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:06:40,393: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is we
2026-01-11 23:06:40,539: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the way the
2026-01-11 23:07:08,354: Val batch 7000: PER (avg): 0.3474 CTC Loss (avg): 56.0563 WER(5gram): 61.93% (n=256) time: 34.068
2026-01-11 23:07:08,354: WER lens: avg_true_words=5.99 avg_pred_words=5.33 max_pred_words=13
2026-01-11 23:07:08,354: t15.2023.08.13 val PER: 0.3222
2026-01-11 23:07:08,354: t15.2023.08.18 val PER: 0.3152
2026-01-11 23:07:08,355: t15.2023.08.20 val PER: 0.2859
2026-01-11 23:07:08,355: t15.2023.08.25 val PER: 0.2846
2026-01-11 23:07:08,355: t15.2023.08.27 val PER: 0.3794
2026-01-11 23:07:08,355: t15.2023.09.01 val PER: 0.2541
2026-01-11 23:07:08,355: t15.2023.09.03 val PER: 0.3468
2026-01-11 23:07:08,355: t15.2023.09.24 val PER: 0.2900
2026-01-11 23:07:08,355: t15.2023.09.29 val PER: 0.3306
2026-01-11 23:07:08,355: t15.2023.10.01 val PER: 0.3884
2026-01-11 23:07:08,355: t15.2023.10.06 val PER: 0.2928
2026-01-11 23:07:08,356: t15.2023.10.08 val PER: 0.4303
2026-01-11 23:07:08,356: t15.2023.10.13 val PER: 0.4546
2026-01-11 23:07:08,356: t15.2023.10.15 val PER: 0.3579
2026-01-11 23:07:08,356: t15.2023.10.20 val PER: 0.3523
2026-01-11 23:07:08,356: t15.2023.10.22 val PER: 0.3029
2026-01-11 23:07:08,356: t15.2023.11.03 val PER: 0.3501
2026-01-11 23:07:08,356: t15.2023.11.04 val PER: 0.1092
2026-01-11 23:07:08,356: t15.2023.11.17 val PER: 0.2177
2026-01-11 23:07:08,357: t15.2023.11.19 val PER: 0.1737
2026-01-11 23:07:08,357: t15.2023.11.26 val PER: 0.4094
2026-01-11 23:07:08,357: t15.2023.12.03 val PER: 0.3414
2026-01-11 23:07:08,357: t15.2023.12.08 val PER: 0.3582
2026-01-11 23:07:08,357: t15.2023.12.10 val PER: 0.3233
2026-01-11 23:07:08,357: t15.2023.12.17 val PER: 0.3202
2026-01-11 23:07:08,357: t15.2023.12.29 val PER: 0.3631
2026-01-11 23:07:08,357: t15.2024.02.25 val PER: 0.2823
2026-01-11 23:07:08,357: t15.2024.03.08 val PER: 0.3954
2026-01-11 23:07:08,357: t15.2024.03.15 val PER: 0.3809
2026-01-11 23:07:08,357: t15.2024.03.17 val PER: 0.3445
2026-01-11 23:07:08,357: t15.2024.05.10 val PER: 0.3536
2026-01-11 23:07:08,357: t15.2024.06.14 val PER: 0.3202
2026-01-11 23:07:08,358: t15.2024.07.19 val PER: 0.4080
2026-01-11 23:07:08,358: t15.2024.07.21 val PER: 0.2979
2026-01-11 23:07:08,358: t15.2024.07.28 val PER: 0.3449
2026-01-11 23:07:08,358: t15.2025.01.10 val PER: 0.4711
2026-01-11 23:07:08,358: t15.2025.01.12 val PER: 0.3303
2026-01-11 23:07:08,358: t15.2025.03.14 val PER: 0.4630
2026-01-11 23:07:08,358: t15.2025.03.16 val PER: 0.3678
2026-01-11 23:07:08,358: t15.2025.03.30 val PER: 0.4506
2026-01-11 23:07:08,358: t15.2025.04.13 val PER: 0.3966
2026-01-11 23:07:08,359: New best val WER(5gram) 66.30% --> 61.93%
2026-01-11 23:07:08,524: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_7000
2026-01-11 23:07:26,211: Train batch 7200: loss: 45.42 grad norm: 91.86 time: 0.086
2026-01-11 23:07:43,481: Train batch 7400: loss: 46.83 grad norm: 74.06 time: 0.082
2026-01-11 23:07:52,125: Running test after training batch: 7500
2026-01-11 23:07:52,306: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:07:57,792: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is we
2026-01-11 23:07:57,924: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the car
2026-01-11 23:08:23,813: Val batch 7500: PER (avg): 0.3380 CTC Loss (avg): 53.8887 WER(5gram): 63.10% (n=256) time: 31.688
2026-01-11 23:08:23,814: WER lens: avg_true_words=5.99 avg_pred_words=5.36 max_pred_words=10
2026-01-11 23:08:23,814: t15.2023.08.13 val PER: 0.3191
2026-01-11 23:08:23,814: t15.2023.08.18 val PER: 0.2984
2026-01-11 23:08:23,814: t15.2023.08.20 val PER: 0.2740
2026-01-11 23:08:23,814: t15.2023.08.25 val PER: 0.2846
2026-01-11 23:08:23,815: t15.2023.08.27 val PER: 0.3778
2026-01-11 23:08:23,815: t15.2023.09.01 val PER: 0.2419
2026-01-11 23:08:23,815: t15.2023.09.03 val PER: 0.3468
2026-01-11 23:08:23,815: t15.2023.09.24 val PER: 0.2743
2026-01-11 23:08:23,815: t15.2023.09.29 val PER: 0.3146
2026-01-11 23:08:23,815: t15.2023.10.01 val PER: 0.3752
2026-01-11 23:08:23,815: t15.2023.10.06 val PER: 0.2853
2026-01-11 23:08:23,815: t15.2023.10.08 val PER: 0.4222
2026-01-11 23:08:23,815: t15.2023.10.13 val PER: 0.4484
2026-01-11 23:08:23,816: t15.2023.10.15 val PER: 0.3533
2026-01-11 23:08:23,816: t15.2023.10.20 val PER: 0.3322
2026-01-11 23:08:23,816: t15.2023.10.22 val PER: 0.2929
2026-01-11 23:08:23,816: t15.2023.11.03 val PER: 0.3399
2026-01-11 23:08:23,816: t15.2023.11.04 val PER: 0.1024
2026-01-11 23:08:23,816: t15.2023.11.17 val PER: 0.2131
2026-01-11 23:08:23,816: t15.2023.11.19 val PER: 0.1776
2026-01-11 23:08:23,816: t15.2023.11.26 val PER: 0.3993
2026-01-11 23:08:23,816: t15.2023.12.03 val PER: 0.3214
2026-01-11 23:08:23,816: t15.2023.12.08 val PER: 0.3402
2026-01-11 23:08:23,816: t15.2023.12.10 val PER: 0.3154
2026-01-11 23:08:23,816: t15.2023.12.17 val PER: 0.3035
2026-01-11 23:08:23,816: t15.2023.12.29 val PER: 0.3514
2026-01-11 23:08:23,816: t15.2024.02.25 val PER: 0.2739
2026-01-11 23:08:23,817: t15.2024.03.08 val PER: 0.3997
2026-01-11 23:08:23,817: t15.2024.03.15 val PER: 0.3621
2026-01-11 23:08:23,817: t15.2024.03.17 val PER: 0.3368
2026-01-11 23:08:23,817: t15.2024.05.10 val PER: 0.3477
2026-01-11 23:08:23,817: t15.2024.06.14 val PER: 0.3249
2026-01-11 23:08:23,817: t15.2024.07.19 val PER: 0.4080
2026-01-11 23:08:23,817: t15.2024.07.21 val PER: 0.2917
2026-01-11 23:08:23,817: t15.2024.07.28 val PER: 0.3360
2026-01-11 23:08:23,817: t15.2025.01.10 val PER: 0.4394
2026-01-11 23:08:23,817: t15.2025.01.12 val PER: 0.3264
2026-01-11 23:08:23,817: t15.2025.03.14 val PER: 0.4527
2026-01-11 23:08:23,817: t15.2025.03.16 val PER: 0.3743
2026-01-11 23:08:23,817: t15.2025.03.30 val PER: 0.4310
2026-01-11 23:08:23,817: t15.2025.04.13 val PER: 0.3909
2026-01-11 23:08:23,990: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_7500
2026-01-11 23:08:32,644: Train batch 7600: loss: 48.06 grad norm: 83.92 time: 0.077
2026-01-11 23:08:49,972: Train batch 7800: loss: 42.52 grad norm: 80.13 time: 0.061
2026-01-11 23:09:07,681: Train batch 8000: loss: 38.45 grad norm: 89.40 time: 0.080
2026-01-11 23:09:07,681: Running test after training batch: 8000
2026-01-11 23:09:07,780: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:09:13,230: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 23:09:13,357: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the coup de
2026-01-11 23:09:38,515: Val batch 8000: PER (avg): 0.3301 CTC Loss (avg): 52.1018 WER(5gram): 59.39% (n=256) time: 30.833
2026-01-11 23:09:38,515: WER lens: avg_true_words=5.99 avg_pred_words=5.38 max_pred_words=10
2026-01-11 23:09:38,515: t15.2023.08.13 val PER: 0.3015
2026-01-11 23:09:38,515: t15.2023.08.18 val PER: 0.2976
2026-01-11 23:09:38,516: t15.2023.08.20 val PER: 0.2685
2026-01-11 23:09:38,516: t15.2023.08.25 val PER: 0.2620
2026-01-11 23:09:38,516: t15.2023.08.27 val PER: 0.3489
2026-01-11 23:09:38,516: t15.2023.09.01 val PER: 0.2435
2026-01-11 23:09:38,516: t15.2023.09.03 val PER: 0.3420
2026-01-11 23:09:38,516: t15.2023.09.24 val PER: 0.2743
2026-01-11 23:09:38,516: t15.2023.09.29 val PER: 0.3070
2026-01-11 23:09:38,516: t15.2023.10.01 val PER: 0.3738
2026-01-11 23:09:38,517: t15.2023.10.06 val PER: 0.2594
2026-01-11 23:09:38,517: t15.2023.10.08 val PER: 0.4222
2026-01-11 23:09:38,517: t15.2023.10.13 val PER: 0.4430
2026-01-11 23:09:38,517: t15.2023.10.15 val PER: 0.3415
2026-01-11 23:09:38,517: t15.2023.10.20 val PER: 0.3255
2026-01-11 23:09:38,517: t15.2023.10.22 val PER: 0.3062
2026-01-11 23:09:38,517: t15.2023.11.03 val PER: 0.3304
2026-01-11 23:09:38,517: t15.2023.11.04 val PER: 0.0990
2026-01-11 23:09:38,517: t15.2023.11.17 val PER: 0.2068
2026-01-11 23:09:38,518: t15.2023.11.19 val PER: 0.1657
2026-01-11 23:09:38,518: t15.2023.11.26 val PER: 0.3942
2026-01-11 23:09:38,518: t15.2023.12.03 val PER: 0.3225
2026-01-11 23:09:38,518: t15.2023.12.08 val PER: 0.3369
2026-01-11 23:09:38,518: t15.2023.12.10 val PER: 0.3062
2026-01-11 23:09:38,518: t15.2023.12.17 val PER: 0.3150
2026-01-11 23:09:38,518: t15.2023.12.29 val PER: 0.3363
2026-01-11 23:09:38,518: t15.2024.02.25 val PER: 0.2711
2026-01-11 23:09:38,518: t15.2024.03.08 val PER: 0.3855
2026-01-11 23:09:38,519: t15.2024.03.15 val PER: 0.3565
2026-01-11 23:09:38,519: t15.2024.03.17 val PER: 0.3250
2026-01-11 23:09:38,519: t15.2024.05.10 val PER: 0.3388
2026-01-11 23:09:38,519: t15.2024.06.14 val PER: 0.3186
2026-01-11 23:09:38,519: t15.2024.07.19 val PER: 0.3909
2026-01-11 23:09:38,519: t15.2024.07.21 val PER: 0.2710
2026-01-11 23:09:38,519: t15.2024.07.28 val PER: 0.3272
2026-01-11 23:09:38,519: t15.2025.01.10 val PER: 0.4394
2026-01-11 23:09:38,520: t15.2025.01.12 val PER: 0.3218
2026-01-11 23:09:38,520: t15.2025.03.14 val PER: 0.4467
2026-01-11 23:09:38,520: t15.2025.03.16 val PER: 0.3547
2026-01-11 23:09:38,520: t15.2025.03.30 val PER: 0.4287
2026-01-11 23:09:38,520: t15.2025.04.13 val PER: 0.3638
2026-01-11 23:09:38,520: New best val WER(5gram) 61.93% --> 59.39%
2026-01-11 23:09:38,678: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_8000
2026-01-11 23:09:56,130: Train batch 8200: loss: 30.68 grad norm: 64.33 time: 0.059
2026-01-11 23:10:13,498: Train batch 8400: loss: 34.86 grad norm: 78.55 time: 0.069
2026-01-11 23:10:22,285: Running test after training batch: 8500
2026-01-11 23:10:22,388: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:10:27,846: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 23:10:27,959: WER debug example
  GT : how does it keep the cost down
  PR : i see it go the way the
2026-01-11 23:10:51,027: Val batch 8500: PER (avg): 0.3207 CTC Loss (avg): 50.6327 WER(5gram): 56.39% (n=256) time: 28.742
2026-01-11 23:10:51,028: WER lens: avg_true_words=5.99 avg_pred_words=5.43 max_pred_words=11
2026-01-11 23:10:51,028: t15.2023.08.13 val PER: 0.2921
2026-01-11 23:10:51,028: t15.2023.08.18 val PER: 0.2875
2026-01-11 23:10:51,028: t15.2023.08.20 val PER: 0.2581
2026-01-11 23:10:51,028: t15.2023.08.25 val PER: 0.2636
2026-01-11 23:10:51,028: t15.2023.08.27 val PER: 0.3489
2026-01-11 23:10:51,028: t15.2023.09.01 val PER: 0.2265
2026-01-11 23:10:51,028: t15.2023.09.03 val PER: 0.3135
2026-01-11 23:10:51,029: t15.2023.09.24 val PER: 0.2670
2026-01-11 23:10:51,029: t15.2023.09.29 val PER: 0.2961
2026-01-11 23:10:51,029: t15.2023.10.01 val PER: 0.3567
2026-01-11 23:10:51,029: t15.2023.10.06 val PER: 0.2594
2026-01-11 23:10:51,029: t15.2023.10.08 val PER: 0.4127
2026-01-11 23:10:51,029: t15.2023.10.13 val PER: 0.4329
2026-01-11 23:10:51,029: t15.2023.10.15 val PER: 0.3355
2026-01-11 23:10:51,029: t15.2023.10.20 val PER: 0.3221
2026-01-11 23:10:51,029: t15.2023.10.22 val PER: 0.2806
2026-01-11 23:10:51,029: t15.2023.11.03 val PER: 0.3209
2026-01-11 23:10:51,029: t15.2023.11.04 val PER: 0.0853
2026-01-11 23:10:51,029: t15.2023.11.17 val PER: 0.1851
2026-01-11 23:10:51,030: t15.2023.11.19 val PER: 0.1557
2026-01-11 23:10:51,030: t15.2023.11.26 val PER: 0.3891
2026-01-11 23:10:51,030: t15.2023.12.03 val PER: 0.3088
2026-01-11 23:10:51,030: t15.2023.12.08 val PER: 0.3276
2026-01-11 23:10:51,030: t15.2023.12.10 val PER: 0.2838
2026-01-11 23:10:51,030: t15.2023.12.17 val PER: 0.3004
2026-01-11 23:10:51,030: t15.2023.12.29 val PER: 0.3329
2026-01-11 23:10:51,030: t15.2024.02.25 val PER: 0.2739
2026-01-11 23:10:51,030: t15.2024.03.08 val PER: 0.3770
2026-01-11 23:10:51,030: t15.2024.03.15 val PER: 0.3502
2026-01-11 23:10:51,030: t15.2024.03.17 val PER: 0.3194
2026-01-11 23:10:51,030: t15.2024.05.10 val PER: 0.3224
2026-01-11 23:10:51,030: t15.2024.06.14 val PER: 0.3107
2026-01-11 23:10:51,030: t15.2024.07.19 val PER: 0.3896
2026-01-11 23:10:51,030: t15.2024.07.21 val PER: 0.2690
2026-01-11 23:10:51,031: t15.2024.07.28 val PER: 0.3154
2026-01-11 23:10:51,031: t15.2025.01.10 val PER: 0.4325
2026-01-11 23:10:51,031: t15.2025.01.12 val PER: 0.3087
2026-01-11 23:10:51,031: t15.2025.03.14 val PER: 0.4231
2026-01-11 23:10:51,031: t15.2025.03.16 val PER: 0.3534
2026-01-11 23:10:51,031: t15.2025.03.30 val PER: 0.4126
2026-01-11 23:10:51,031: t15.2025.04.13 val PER: 0.3780
2026-01-11 23:10:51,032: New best val WER(5gram) 59.39% --> 56.39%
2026-01-11 23:10:51,196: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_8500
2026-01-11 23:10:59,926: Train batch 8600: loss: 43.87 grad norm: 66.12 time: 0.065
2026-01-11 23:11:17,145: Train batch 8800: loss: 50.57 grad norm: 116.45 time: 0.067
2026-01-11 23:11:34,743: Train batch 9000: loss: 47.12 grad norm: 83.85 time: 0.085
2026-01-11 23:11:34,743: Running test after training batch: 9000
2026-01-11 23:11:34,849: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:11:40,492: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 23:11:40,621: WER debug example
  GT : how does it keep the cost down
  PR : just in the car at a
2026-01-11 23:12:01,890: Val batch 9000: PER (avg): 0.3160 CTC Loss (avg): 49.2743 WER(5gram): 54.63% (n=256) time: 27.147
2026-01-11 23:12:01,890: WER lens: avg_true_words=5.99 avg_pred_words=5.61 max_pred_words=11
2026-01-11 23:12:01,891: t15.2023.08.13 val PER: 0.2838
2026-01-11 23:12:01,891: t15.2023.08.18 val PER: 0.2842
2026-01-11 23:12:01,891: t15.2023.08.20 val PER: 0.2581
2026-01-11 23:12:01,891: t15.2023.08.25 val PER: 0.2530
2026-01-11 23:12:01,891: t15.2023.08.27 val PER: 0.3344
2026-01-11 23:12:01,891: t15.2023.09.01 val PER: 0.2216
2026-01-11 23:12:01,891: t15.2023.09.03 val PER: 0.3195
2026-01-11 23:12:01,891: t15.2023.09.24 val PER: 0.2536
2026-01-11 23:12:01,891: t15.2023.09.29 val PER: 0.2955
2026-01-11 23:12:01,891: t15.2023.10.01 val PER: 0.3606
2026-01-11 23:12:01,891: t15.2023.10.06 val PER: 0.2487
2026-01-11 23:12:01,891: t15.2023.10.08 val PER: 0.4087
2026-01-11 23:12:01,892: t15.2023.10.13 val PER: 0.4244
2026-01-11 23:12:01,892: t15.2023.10.15 val PER: 0.3349
2026-01-11 23:12:01,892: t15.2023.10.20 val PER: 0.3188
2026-01-11 23:12:01,892: t15.2023.10.22 val PER: 0.2806
2026-01-11 23:12:01,892: t15.2023.11.03 val PER: 0.3121
2026-01-11 23:12:01,892: t15.2023.11.04 val PER: 0.0853
2026-01-11 23:12:01,892: t15.2023.11.17 val PER: 0.1757
2026-01-11 23:12:01,892: t15.2023.11.19 val PER: 0.1457
2026-01-11 23:12:01,892: t15.2023.11.26 val PER: 0.3870
2026-01-11 23:12:01,892: t15.2023.12.03 val PER: 0.3057
2026-01-11 23:12:01,892: t15.2023.12.08 val PER: 0.3169
2026-01-11 23:12:01,892: t15.2023.12.10 val PER: 0.2878
2026-01-11 23:12:01,893: t15.2023.12.17 val PER: 0.2942
2026-01-11 23:12:01,893: t15.2023.12.29 val PER: 0.3212
2026-01-11 23:12:01,893: t15.2024.02.25 val PER: 0.2542
2026-01-11 23:12:01,893: t15.2024.03.08 val PER: 0.3727
2026-01-11 23:12:01,893: t15.2024.03.15 val PER: 0.3483
2026-01-11 23:12:01,893: t15.2024.03.17 val PER: 0.3103
2026-01-11 23:12:01,893: t15.2024.05.10 val PER: 0.3224
2026-01-11 23:12:01,893: t15.2024.06.14 val PER: 0.3028
2026-01-11 23:12:01,893: t15.2024.07.19 val PER: 0.3830
2026-01-11 23:12:01,893: t15.2024.07.21 val PER: 0.2621
2026-01-11 23:12:01,893: t15.2024.07.28 val PER: 0.3140
2026-01-11 23:12:01,893: t15.2025.01.10 val PER: 0.4325
2026-01-11 23:12:01,893: t15.2025.01.12 val PER: 0.3079
2026-01-11 23:12:01,894: t15.2025.03.14 val PER: 0.4260
2026-01-11 23:12:01,894: t15.2025.03.16 val PER: 0.3482
2026-01-11 23:12:01,894: t15.2025.03.30 val PER: 0.4161
2026-01-11 23:12:01,894: t15.2025.04.13 val PER: 0.3666
2026-01-11 23:12:01,894: New best val WER(5gram) 56.39% --> 54.63%
2026-01-11 23:12:02,071: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_9000
2026-01-11 23:12:19,323: Train batch 9200: loss: 34.25 grad norm: 62.18 time: 0.064
2026-01-11 23:12:36,628: Train batch 9400: loss: 30.07 grad norm: 68.81 time: 0.075
2026-01-11 23:12:45,301: Running test after training batch: 9500
2026-01-11 23:12:45,432: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:12:51,291: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 23:12:51,466: WER debug example
  GT : how does it keep the cost down
  PR : i do see it go the way the
2026-01-11 23:13:12,960: Val batch 9500: PER (avg): 0.3076 CTC Loss (avg): 47.8555 WER(5gram): 56.00% (n=256) time: 27.658
2026-01-11 23:13:12,960: WER lens: avg_true_words=5.99 avg_pred_words=5.55 max_pred_words=12
2026-01-11 23:13:12,960: t15.2023.08.13 val PER: 0.2817
2026-01-11 23:13:12,960: t15.2023.08.18 val PER: 0.2682
2026-01-11 23:13:12,961: t15.2023.08.20 val PER: 0.2486
2026-01-11 23:13:12,961: t15.2023.08.25 val PER: 0.2470
2026-01-11 23:13:12,961: t15.2023.08.27 val PER: 0.3344
2026-01-11 23:13:12,961: t15.2023.09.01 val PER: 0.2119
2026-01-11 23:13:12,961: t15.2023.09.03 val PER: 0.3171
2026-01-11 23:13:12,961: t15.2023.09.24 val PER: 0.2609
2026-01-11 23:13:12,961: t15.2023.09.29 val PER: 0.2897
2026-01-11 23:13:12,961: t15.2023.10.01 val PER: 0.3468
2026-01-11 23:13:12,961: t15.2023.10.06 val PER: 0.2379
2026-01-11 23:13:12,961: t15.2023.10.08 val PER: 0.3911
2026-01-11 23:13:12,961: t15.2023.10.13 val PER: 0.4104
2026-01-11 23:13:12,961: t15.2023.10.15 val PER: 0.3204
2026-01-11 23:13:12,961: t15.2023.10.20 val PER: 0.3322
2026-01-11 23:13:12,962: t15.2023.10.22 val PER: 0.2795
2026-01-11 23:13:12,962: t15.2023.11.03 val PER: 0.3141
2026-01-11 23:13:12,962: t15.2023.11.04 val PER: 0.0717
2026-01-11 23:13:12,962: t15.2023.11.17 val PER: 0.1773
2026-01-11 23:13:12,962: t15.2023.11.19 val PER: 0.1477
2026-01-11 23:13:12,962: t15.2023.11.26 val PER: 0.3638
2026-01-11 23:13:12,962: t15.2023.12.03 val PER: 0.2952
2026-01-11 23:13:12,962: t15.2023.12.08 val PER: 0.3142
2026-01-11 23:13:12,962: t15.2023.12.10 val PER: 0.2668
2026-01-11 23:13:12,962: t15.2023.12.17 val PER: 0.2931
2026-01-11 23:13:12,962: t15.2023.12.29 val PER: 0.3089
2026-01-11 23:13:12,963: t15.2024.02.25 val PER: 0.2486
2026-01-11 23:13:12,963: t15.2024.03.08 val PER: 0.3613
2026-01-11 23:13:12,963: t15.2024.03.15 val PER: 0.3452
2026-01-11 23:13:12,963: t15.2024.03.17 val PER: 0.3082
2026-01-11 23:13:12,963: t15.2024.05.10 val PER: 0.3120
2026-01-11 23:13:12,963: t15.2024.06.14 val PER: 0.2981
2026-01-11 23:13:12,963: t15.2024.07.19 val PER: 0.3665
2026-01-11 23:13:12,963: t15.2024.07.21 val PER: 0.2579
2026-01-11 23:13:12,963: t15.2024.07.28 val PER: 0.3007
2026-01-11 23:13:12,964: t15.2025.01.10 val PER: 0.4284
2026-01-11 23:13:12,964: t15.2025.01.12 val PER: 0.3087
2026-01-11 23:13:12,964: t15.2025.03.14 val PER: 0.3979
2026-01-11 23:13:12,964: t15.2025.03.16 val PER: 0.3495
2026-01-11 23:13:12,964: t15.2025.03.30 val PER: 0.3954
2026-01-11 23:13:12,964: t15.2025.04.13 val PER: 0.3481
2026-01-11 23:13:13,118: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_9500
2026-01-11 23:13:21,706: Train batch 9600: loss: 28.23 grad norm: 64.38 time: 0.079
2026-01-11 23:13:39,084: Train batch 9800: loss: 41.82 grad norm: 111.25 time: 0.070
2026-01-11 23:13:56,612: Train batch 10000: loss: 22.64 grad norm: 64.60 time: 0.066
2026-01-11 23:13:56,612: Running test after training batch: 10000
2026-01-11 23:13:56,745: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:14:02,184: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 23:14:02,297: WER debug example
  GT : how does it keep the cost down
  PR : do it in the car at a
2026-01-11 23:14:21,266: Val batch 10000: PER (avg): 0.3039 CTC Loss (avg): 46.8862 WER(5gram): 55.08% (n=256) time: 24.653
2026-01-11 23:14:21,266: WER lens: avg_true_words=5.99 avg_pred_words=5.53 max_pred_words=11
2026-01-11 23:14:21,266: t15.2023.08.13 val PER: 0.2661
2026-01-11 23:14:21,266: t15.2023.08.18 val PER: 0.2699
2026-01-11 23:14:21,266: t15.2023.08.20 val PER: 0.2573
2026-01-11 23:14:21,266: t15.2023.08.25 val PER: 0.2500
2026-01-11 23:14:21,267: t15.2023.08.27 val PER: 0.3264
2026-01-11 23:14:21,267: t15.2023.09.01 val PER: 0.2119
2026-01-11 23:14:21,267: t15.2023.09.03 val PER: 0.3135
2026-01-11 23:14:21,267: t15.2023.09.24 val PER: 0.2524
2026-01-11 23:14:21,267: t15.2023.09.29 val PER: 0.2827
2026-01-11 23:14:21,267: t15.2023.10.01 val PER: 0.3468
2026-01-11 23:14:21,267: t15.2023.10.06 val PER: 0.2304
2026-01-11 23:14:21,267: t15.2023.10.08 val PER: 0.3870
2026-01-11 23:14:21,267: t15.2023.10.13 val PER: 0.4135
2026-01-11 23:14:21,267: t15.2023.10.15 val PER: 0.3197
2026-01-11 23:14:21,267: t15.2023.10.20 val PER: 0.3289
2026-01-11 23:14:21,267: t15.2023.10.22 val PER: 0.2684
2026-01-11 23:14:21,268: t15.2023.11.03 val PER: 0.3060
2026-01-11 23:14:21,268: t15.2023.11.04 val PER: 0.0785
2026-01-11 23:14:21,268: t15.2023.11.17 val PER: 0.1680
2026-01-11 23:14:21,268: t15.2023.11.19 val PER: 0.1517
2026-01-11 23:14:21,268: t15.2023.11.26 val PER: 0.3659
2026-01-11 23:14:21,268: t15.2023.12.03 val PER: 0.2931
2026-01-11 23:14:21,268: t15.2023.12.08 val PER: 0.3063
2026-01-11 23:14:21,268: t15.2023.12.10 val PER: 0.2628
2026-01-11 23:14:21,268: t15.2023.12.17 val PER: 0.2765
2026-01-11 23:14:21,269: t15.2023.12.29 val PER: 0.3095
2026-01-11 23:14:21,269: t15.2024.02.25 val PER: 0.2402
2026-01-11 23:14:21,269: t15.2024.03.08 val PER: 0.3499
2026-01-11 23:14:21,269: t15.2024.03.15 val PER: 0.3358
2026-01-11 23:14:21,269: t15.2024.03.17 val PER: 0.3006
2026-01-11 23:14:21,269: t15.2024.05.10 val PER: 0.3135
2026-01-11 23:14:21,269: t15.2024.06.14 val PER: 0.2965
2026-01-11 23:14:21,269: t15.2024.07.19 val PER: 0.3672
2026-01-11 23:14:21,270: t15.2024.07.21 val PER: 0.2552
2026-01-11 23:14:21,270: t15.2024.07.28 val PER: 0.2993
2026-01-11 23:14:21,270: t15.2025.01.10 val PER: 0.4242
2026-01-11 23:14:21,270: t15.2025.01.12 val PER: 0.2987
2026-01-11 23:14:21,270: t15.2025.03.14 val PER: 0.3891
2026-01-11 23:14:21,270: t15.2025.03.16 val PER: 0.3455
2026-01-11 23:14:21,270: t15.2025.03.30 val PER: 0.3943
2026-01-11 23:14:21,270: t15.2025.04.13 val PER: 0.3495
2026-01-11 23:14:21,427: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_10000
2026-01-11 23:14:38,701: Train batch 10200: loss: 26.57 grad norm: 64.90 time: 0.058
2026-01-11 23:14:56,219: Train batch 10400: loss: 31.58 grad norm: 89.30 time: 0.080
2026-01-11 23:15:05,000: Running test after training batch: 10500
2026-01-11 23:15:05,108: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:15:10,997: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 23:15:11,097: WER debug example
  GT : how does it keep the cost down
  PR : just in the car at the
2026-01-11 23:15:29,775: Val batch 10500: PER (avg): 0.2938 CTC Loss (avg): 45.6392 WER(5gram): 51.89% (n=256) time: 24.775
2026-01-11 23:15:29,776: WER lens: avg_true_words=5.99 avg_pred_words=5.49 max_pred_words=11
2026-01-11 23:15:29,776: t15.2023.08.13 val PER: 0.2651
2026-01-11 23:15:29,776: t15.2023.08.18 val PER: 0.2598
2026-01-11 23:15:29,776: t15.2023.08.20 val PER: 0.2351
2026-01-11 23:15:29,776: t15.2023.08.25 val PER: 0.2289
2026-01-11 23:15:29,776: t15.2023.08.27 val PER: 0.3119
2026-01-11 23:15:29,776: t15.2023.09.01 val PER: 0.2070
2026-01-11 23:15:29,776: t15.2023.09.03 val PER: 0.2981
2026-01-11 23:15:29,776: t15.2023.09.24 val PER: 0.2318
2026-01-11 23:15:29,776: t15.2023.09.29 val PER: 0.2706
2026-01-11 23:15:29,776: t15.2023.10.01 val PER: 0.3362
2026-01-11 23:15:29,776: t15.2023.10.06 val PER: 0.2153
2026-01-11 23:15:29,776: t15.2023.10.08 val PER: 0.3870
2026-01-11 23:15:29,776: t15.2023.10.13 val PER: 0.4034
2026-01-11 23:15:29,777: t15.2023.10.15 val PER: 0.3171
2026-01-11 23:15:29,777: t15.2023.10.20 val PER: 0.3389
2026-01-11 23:15:29,777: t15.2023.10.22 val PER: 0.2639
2026-01-11 23:15:29,777: t15.2023.11.03 val PER: 0.3046
2026-01-11 23:15:29,777: t15.2023.11.04 val PER: 0.0819
2026-01-11 23:15:29,777: t15.2023.11.17 val PER: 0.1617
2026-01-11 23:15:29,777: t15.2023.11.19 val PER: 0.1297
2026-01-11 23:15:29,777: t15.2023.11.26 val PER: 0.3543
2026-01-11 23:15:29,777: t15.2023.12.03 val PER: 0.2826
2026-01-11 23:15:29,777: t15.2023.12.08 val PER: 0.2963
2026-01-11 23:15:29,778: t15.2023.12.10 val PER: 0.2641
2026-01-11 23:15:29,778: t15.2023.12.17 val PER: 0.2661
2026-01-11 23:15:29,778: t15.2023.12.29 val PER: 0.2931
2026-01-11 23:15:29,778: t15.2024.02.25 val PER: 0.2430
2026-01-11 23:15:29,778: t15.2024.03.08 val PER: 0.3514
2026-01-11 23:15:29,778: t15.2024.03.15 val PER: 0.3177
2026-01-11 23:15:29,778: t15.2024.03.17 val PER: 0.2971
2026-01-11 23:15:29,778: t15.2024.05.10 val PER: 0.3016
2026-01-11 23:15:29,778: t15.2024.06.14 val PER: 0.2902
2026-01-11 23:15:29,779: t15.2024.07.19 val PER: 0.3546
2026-01-11 23:15:29,779: t15.2024.07.21 val PER: 0.2310
2026-01-11 23:15:29,779: t15.2024.07.28 val PER: 0.2816
2026-01-11 23:15:29,779: t15.2025.01.10 val PER: 0.4146
2026-01-11 23:15:29,779: t15.2025.01.12 val PER: 0.2956
2026-01-11 23:15:29,779: t15.2025.03.14 val PER: 0.3950
2026-01-11 23:15:29,779: t15.2025.03.16 val PER: 0.3168
2026-01-11 23:15:29,779: t15.2025.03.30 val PER: 0.3885
2026-01-11 23:15:29,779: t15.2025.04.13 val PER: 0.3452
2026-01-11 23:15:29,780: New best val WER(5gram) 54.63% --> 51.89%
2026-01-11 23:15:29,938: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_10500
2026-01-11 23:15:38,728: Train batch 10600: loss: 30.24 grad norm: 105.96 time: 0.081
2026-01-11 23:15:56,099: Train batch 10800: loss: 41.11 grad norm: 91.01 time: 0.071
2026-01-11 23:16:13,390: Train batch 11000: loss: 46.23 grad norm: 146.53 time: 0.067
2026-01-11 23:16:13,391: Running test after training batch: 11000
2026-01-11 23:16:13,521: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:16:18,896: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 23:16:18,994: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the new
2026-01-11 23:16:37,017: Val batch 11000: PER (avg): 0.2846 CTC Loss (avg): 44.9849 WER(5gram): 52.48% (n=256) time: 23.626
2026-01-11 23:16:37,017: WER lens: avg_true_words=5.99 avg_pred_words=5.79 max_pred_words=12
2026-01-11 23:16:37,017: t15.2023.08.13 val PER: 0.2568
2026-01-11 23:16:37,017: t15.2023.08.18 val PER: 0.2456
2026-01-11 23:16:37,018: t15.2023.08.20 val PER: 0.2288
2026-01-11 23:16:37,018: t15.2023.08.25 val PER: 0.2259
2026-01-11 23:16:37,018: t15.2023.08.27 val PER: 0.3183
2026-01-11 23:16:37,018: t15.2023.09.01 val PER: 0.1891
2026-01-11 23:16:37,018: t15.2023.09.03 val PER: 0.3029
2026-01-11 23:16:37,018: t15.2023.09.24 val PER: 0.2269
2026-01-11 23:16:37,018: t15.2023.09.29 val PER: 0.2661
2026-01-11 23:16:37,018: t15.2023.10.01 val PER: 0.3217
2026-01-11 23:16:37,018: t15.2023.10.06 val PER: 0.2153
2026-01-11 23:16:37,018: t15.2023.10.08 val PER: 0.3735
2026-01-11 23:16:37,018: t15.2023.10.13 val PER: 0.3941
2026-01-11 23:16:37,019: t15.2023.10.15 val PER: 0.2993
2026-01-11 23:16:37,019: t15.2023.10.20 val PER: 0.3221
2026-01-11 23:16:37,019: t15.2023.10.22 val PER: 0.2517
2026-01-11 23:16:37,019: t15.2023.11.03 val PER: 0.2897
2026-01-11 23:16:37,019: t15.2023.11.04 val PER: 0.0717
2026-01-11 23:16:37,019: t15.2023.11.17 val PER: 0.1384
2026-01-11 23:16:37,019: t15.2023.11.19 val PER: 0.1297
2026-01-11 23:16:37,019: t15.2023.11.26 val PER: 0.3348
2026-01-11 23:16:37,019: t15.2023.12.03 val PER: 0.2731
2026-01-11 23:16:37,019: t15.2023.12.08 val PER: 0.2936
2026-01-11 23:16:37,019: t15.2023.12.10 val PER: 0.2497
2026-01-11 23:16:37,019: t15.2023.12.17 val PER: 0.2827
2026-01-11 23:16:37,019: t15.2023.12.29 val PER: 0.2924
2026-01-11 23:16:37,020: t15.2024.02.25 val PER: 0.2346
2026-01-11 23:16:37,020: t15.2024.03.08 val PER: 0.3343
2026-01-11 23:16:37,020: t15.2024.03.15 val PER: 0.3208
2026-01-11 23:16:37,020: t15.2024.03.17 val PER: 0.2803
2026-01-11 23:16:37,020: t15.2024.05.10 val PER: 0.2897
2026-01-11 23:16:37,020: t15.2024.06.14 val PER: 0.2713
2026-01-11 23:16:37,020: t15.2024.07.19 val PER: 0.3395
2026-01-11 23:16:37,020: t15.2024.07.21 val PER: 0.2269
2026-01-11 23:16:37,020: t15.2024.07.28 val PER: 0.2691
2026-01-11 23:16:37,020: t15.2025.01.10 val PER: 0.3939
2026-01-11 23:16:37,020: t15.2025.01.12 val PER: 0.2818
2026-01-11 23:16:37,020: t15.2025.03.14 val PER: 0.3876
2026-01-11 23:16:37,021: t15.2025.03.16 val PER: 0.3115
2026-01-11 23:16:37,021: t15.2025.03.30 val PER: 0.3759
2026-01-11 23:16:37,021: t15.2025.04.13 val PER: 0.3409
2026-01-11 23:16:37,175: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_11000
2026-01-11 23:16:54,431: Train batch 11200: loss: 34.55 grad norm: 77.07 time: 0.078
2026-01-11 23:17:11,826: Train batch 11400: loss: 32.31 grad norm: 78.87 time: 0.063
2026-01-11 23:17:20,580: Running test after training batch: 11500
2026-01-11 23:17:20,715: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:17:26,362: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is why
2026-01-11 23:17:26,474: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 23:17:44,724: Val batch 11500: PER (avg): 0.2831 CTC Loss (avg): 44.2022 WER(5gram): 46.41% (n=256) time: 24.144
2026-01-11 23:17:44,724: WER lens: avg_true_words=5.99 avg_pred_words=5.75 max_pred_words=11
2026-01-11 23:17:44,724: t15.2023.08.13 val PER: 0.2505
2026-01-11 23:17:44,724: t15.2023.08.18 val PER: 0.2439
2026-01-11 23:17:44,725: t15.2023.08.20 val PER: 0.2200
2026-01-11 23:17:44,725: t15.2023.08.25 val PER: 0.2244
2026-01-11 23:17:44,725: t15.2023.08.27 val PER: 0.3103
2026-01-11 23:17:44,725: t15.2023.09.01 val PER: 0.1956
2026-01-11 23:17:44,725: t15.2023.09.03 val PER: 0.2850
2026-01-11 23:17:44,725: t15.2023.09.24 val PER: 0.2221
2026-01-11 23:17:44,725: t15.2023.09.29 val PER: 0.2648
2026-01-11 23:17:44,725: t15.2023.10.01 val PER: 0.3184
2026-01-11 23:17:44,725: t15.2023.10.06 val PER: 0.2239
2026-01-11 23:17:44,726: t15.2023.10.08 val PER: 0.3762
2026-01-11 23:17:44,726: t15.2023.10.13 val PER: 0.3918
2026-01-11 23:17:44,726: t15.2023.10.15 val PER: 0.2953
2026-01-11 23:17:44,726: t15.2023.10.20 val PER: 0.3389
2026-01-11 23:17:44,726: t15.2023.10.22 val PER: 0.2550
2026-01-11 23:17:44,726: t15.2023.11.03 val PER: 0.2883
2026-01-11 23:17:44,726: t15.2023.11.04 val PER: 0.0751
2026-01-11 23:17:44,726: t15.2023.11.17 val PER: 0.1337
2026-01-11 23:17:44,726: t15.2023.11.19 val PER: 0.1218
2026-01-11 23:17:44,726: t15.2023.11.26 val PER: 0.3442
2026-01-11 23:17:44,726: t15.2023.12.03 val PER: 0.2626
2026-01-11 23:17:44,727: t15.2023.12.08 val PER: 0.2836
2026-01-11 23:17:44,727: t15.2023.12.10 val PER: 0.2418
2026-01-11 23:17:44,727: t15.2023.12.17 val PER: 0.2672
2026-01-11 23:17:44,727: t15.2023.12.29 val PER: 0.2848
2026-01-11 23:17:44,727: t15.2024.02.25 val PER: 0.2402
2026-01-11 23:17:44,727: t15.2024.03.08 val PER: 0.3343
2026-01-11 23:17:44,727: t15.2024.03.15 val PER: 0.3171
2026-01-11 23:17:44,727: t15.2024.03.17 val PER: 0.2838
2026-01-11 23:17:44,727: t15.2024.05.10 val PER: 0.2897
2026-01-11 23:17:44,727: t15.2024.06.14 val PER: 0.2681
2026-01-11 23:17:44,727: t15.2024.07.19 val PER: 0.3467
2026-01-11 23:17:44,728: t15.2024.07.21 val PER: 0.2248
2026-01-11 23:17:44,728: t15.2024.07.28 val PER: 0.2713
2026-01-11 23:17:44,728: t15.2025.01.10 val PER: 0.4242
2026-01-11 23:17:44,728: t15.2025.01.12 val PER: 0.2779
2026-01-11 23:17:44,728: t15.2025.03.14 val PER: 0.3846
2026-01-11 23:17:44,728: t15.2025.03.16 val PER: 0.3102
2026-01-11 23:17:44,728: t15.2025.03.30 val PER: 0.3782
2026-01-11 23:17:44,728: t15.2025.04.13 val PER: 0.3367
2026-01-11 23:17:44,729: New best val WER(5gram) 51.89% --> 46.41%
2026-01-11 23:17:44,880: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_11500
2026-01-11 23:17:53,336: Train batch 11600: loss: 36.53 grad norm: 87.72 time: 0.067
2026-01-11 23:18:10,657: Train batch 11800: loss: 24.05 grad norm: 51.44 time: 0.053
2026-01-11 23:18:27,848: Train batch 12000: loss: 39.11 grad norm: 80.77 time: 0.076
2026-01-11 23:18:27,848: Running test after training batch: 12000
2026-01-11 23:18:27,948: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:18:33,380: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 23:18:33,479: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the cost to
2026-01-11 23:18:50,119: Val batch 12000: PER (avg): 0.2767 CTC Loss (avg): 42.9841 WER(5gram): 45.83% (n=256) time: 22.271
2026-01-11 23:18:50,120: WER lens: avg_true_words=5.99 avg_pred_words=5.77 max_pred_words=12
2026-01-11 23:18:50,120: t15.2023.08.13 val PER: 0.2318
2026-01-11 23:18:50,120: t15.2023.08.18 val PER: 0.2389
2026-01-11 23:18:50,120: t15.2023.08.20 val PER: 0.2216
2026-01-11 23:18:50,120: t15.2023.08.25 val PER: 0.2108
2026-01-11 23:18:50,120: t15.2023.08.27 val PER: 0.3135
2026-01-11 23:18:50,121: t15.2023.09.01 val PER: 0.1818
2026-01-11 23:18:50,121: t15.2023.09.03 val PER: 0.2850
2026-01-11 23:18:50,121: t15.2023.09.24 val PER: 0.2197
2026-01-11 23:18:50,121: t15.2023.09.29 val PER: 0.2597
2026-01-11 23:18:50,121: t15.2023.10.01 val PER: 0.3058
2026-01-11 23:18:50,121: t15.2023.10.06 val PER: 0.2056
2026-01-11 23:18:50,121: t15.2023.10.08 val PER: 0.3681
2026-01-11 23:18:50,121: t15.2023.10.13 val PER: 0.3902
2026-01-11 23:18:50,121: t15.2023.10.15 val PER: 0.2914
2026-01-11 23:18:50,121: t15.2023.10.20 val PER: 0.3255
2026-01-11 23:18:50,121: t15.2023.10.22 val PER: 0.2472
2026-01-11 23:18:50,122: t15.2023.11.03 val PER: 0.2870
2026-01-11 23:18:50,122: t15.2023.11.04 val PER: 0.0751
2026-01-11 23:18:50,122: t15.2023.11.17 val PER: 0.1260
2026-01-11 23:18:50,122: t15.2023.11.19 val PER: 0.1158
2026-01-11 23:18:50,122: t15.2023.11.26 val PER: 0.3370
2026-01-11 23:18:50,122: t15.2023.12.03 val PER: 0.2658
2026-01-11 23:18:50,122: t15.2023.12.08 val PER: 0.2856
2026-01-11 23:18:50,122: t15.2023.12.10 val PER: 0.2470
2026-01-11 23:18:50,122: t15.2023.12.17 val PER: 0.2547
2026-01-11 23:18:50,122: t15.2023.12.29 val PER: 0.2752
2026-01-11 23:18:50,122: t15.2024.02.25 val PER: 0.2261
2026-01-11 23:18:50,123: t15.2024.03.08 val PER: 0.3314
2026-01-11 23:18:50,123: t15.2024.03.15 val PER: 0.3158
2026-01-11 23:18:50,123: t15.2024.03.17 val PER: 0.2755
2026-01-11 23:18:50,123: t15.2024.05.10 val PER: 0.2838
2026-01-11 23:18:50,123: t15.2024.06.14 val PER: 0.2776
2026-01-11 23:18:50,123: t15.2024.07.19 val PER: 0.3349
2026-01-11 23:18:50,123: t15.2024.07.21 val PER: 0.2138
2026-01-11 23:18:50,123: t15.2024.07.28 val PER: 0.2669
2026-01-11 23:18:50,123: t15.2025.01.10 val PER: 0.3871
2026-01-11 23:18:50,123: t15.2025.01.12 val PER: 0.2810
2026-01-11 23:18:50,123: t15.2025.03.14 val PER: 0.3728
2026-01-11 23:18:50,123: t15.2025.03.16 val PER: 0.3141
2026-01-11 23:18:50,123: t15.2025.03.30 val PER: 0.3575
2026-01-11 23:18:50,124: t15.2025.04.13 val PER: 0.3324
2026-01-11 23:18:50,124: New best val WER(5gram) 46.41% --> 45.83%
2026-01-11 23:18:50,283: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_12000
2026-01-11 23:19:07,516: Train batch 12200: loss: 25.11 grad norm: 82.69 time: 0.072
2026-01-11 23:19:24,601: Train batch 12400: loss: 19.00 grad norm: 50.50 time: 0.045
2026-01-11 23:19:33,359: Running test after training batch: 12500
2026-01-11 23:19:33,490: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:19:39,127: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 23:19:39,219: WER debug example
  GT : how does it keep the cost down
  PR : how do it in the cost to
2026-01-11 23:19:54,633: Val batch 12500: PER (avg): 0.2725 CTC Loss (avg): 42.3385 WER(5gram): 47.26% (n=256) time: 21.274
2026-01-11 23:19:54,633: WER lens: avg_true_words=5.99 avg_pred_words=5.79 max_pred_words=11
2026-01-11 23:19:54,633: t15.2023.08.13 val PER: 0.2328
2026-01-11 23:19:54,633: t15.2023.08.18 val PER: 0.2422
2026-01-11 23:19:54,634: t15.2023.08.20 val PER: 0.2160
2026-01-11 23:19:54,634: t15.2023.08.25 val PER: 0.2108
2026-01-11 23:19:54,634: t15.2023.08.27 val PER: 0.3039
2026-01-11 23:19:54,634: t15.2023.09.01 val PER: 0.1786
2026-01-11 23:19:54,634: t15.2023.09.03 val PER: 0.2660
2026-01-11 23:19:54,634: t15.2023.09.24 val PER: 0.2197
2026-01-11 23:19:54,634: t15.2023.09.29 val PER: 0.2527
2026-01-11 23:19:54,634: t15.2023.10.01 val PER: 0.3078
2026-01-11 23:19:54,634: t15.2023.10.06 val PER: 0.2024
2026-01-11 23:19:54,634: t15.2023.10.08 val PER: 0.3667
2026-01-11 23:19:54,634: t15.2023.10.13 val PER: 0.3879
2026-01-11 23:19:54,634: t15.2023.10.15 val PER: 0.2894
2026-01-11 23:19:54,634: t15.2023.10.20 val PER: 0.3188
2026-01-11 23:19:54,635: t15.2023.10.22 val PER: 0.2372
2026-01-11 23:19:54,635: t15.2023.11.03 val PER: 0.2829
2026-01-11 23:19:54,635: t15.2023.11.04 val PER: 0.0683
2026-01-11 23:19:54,635: t15.2023.11.17 val PER: 0.1229
2026-01-11 23:19:54,635: t15.2023.11.19 val PER: 0.1218
2026-01-11 23:19:54,635: t15.2023.11.26 val PER: 0.3290
2026-01-11 23:19:54,635: t15.2023.12.03 val PER: 0.2616
2026-01-11 23:19:54,636: t15.2023.12.08 val PER: 0.2756
2026-01-11 23:19:54,636: t15.2023.12.10 val PER: 0.2431
2026-01-11 23:19:54,636: t15.2023.12.17 val PER: 0.2588
2026-01-11 23:19:54,636: t15.2023.12.29 val PER: 0.2656
2026-01-11 23:19:54,636: t15.2024.02.25 val PER: 0.2233
2026-01-11 23:19:54,636: t15.2024.03.08 val PER: 0.3215
2026-01-11 23:19:54,636: t15.2024.03.15 val PER: 0.3089
2026-01-11 23:19:54,636: t15.2024.03.17 val PER: 0.2706
2026-01-11 23:19:54,636: t15.2024.05.10 val PER: 0.2912
2026-01-11 23:19:54,636: t15.2024.06.14 val PER: 0.2634
2026-01-11 23:19:54,636: t15.2024.07.19 val PER: 0.3336
2026-01-11 23:19:54,636: t15.2024.07.21 val PER: 0.2110
2026-01-11 23:19:54,637: t15.2024.07.28 val PER: 0.2603
2026-01-11 23:19:54,637: t15.2025.01.10 val PER: 0.3829
2026-01-11 23:19:54,637: t15.2025.01.12 val PER: 0.2741
2026-01-11 23:19:54,637: t15.2025.03.14 val PER: 0.3817
2026-01-11 23:19:54,637: t15.2025.03.16 val PER: 0.2984
2026-01-11 23:19:54,637: t15.2025.03.30 val PER: 0.3575
2026-01-11 23:19:54,637: t15.2025.04.13 val PER: 0.3310
2026-01-11 23:19:54,782: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_12500
2026-01-11 23:20:03,372: Train batch 12600: loss: 26.03 grad norm: 104.24 time: 0.063
2026-01-11 23:20:20,700: Train batch 12800: loss: 21.10 grad norm: 68.42 time: 0.058
2026-01-11 23:20:38,246: Train batch 13000: loss: 20.94 grad norm: 57.89 time: 0.072
2026-01-11 23:20:38,247: Running test after training batch: 13000
2026-01-11 23:20:38,348: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:20:43,738: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 23:20:43,873: WER debug example
  GT : how does it keep the cost down
  PR : how does it eat the cost to
2026-01-11 23:21:01,406: Val batch 13000: PER (avg): 0.2654 CTC Loss (avg): 41.0293 WER(5gram): 48.04% (n=256) time: 23.159
2026-01-11 23:21:01,406: WER lens: avg_true_words=5.99 avg_pred_words=5.75 max_pred_words=11
2026-01-11 23:21:01,406: t15.2023.08.13 val PER: 0.2225
2026-01-11 23:21:01,406: t15.2023.08.18 val PER: 0.2255
2026-01-11 23:21:01,406: t15.2023.08.20 val PER: 0.2113
2026-01-11 23:21:01,407: t15.2023.08.25 val PER: 0.2108
2026-01-11 23:21:01,407: t15.2023.08.27 val PER: 0.2958
2026-01-11 23:21:01,407: t15.2023.09.01 val PER: 0.1696
2026-01-11 23:21:01,407: t15.2023.09.03 val PER: 0.2755
2026-01-11 23:21:01,407: t15.2023.09.24 val PER: 0.2148
2026-01-11 23:21:01,407: t15.2023.09.29 val PER: 0.2502
2026-01-11 23:21:01,407: t15.2023.10.01 val PER: 0.3032
2026-01-11 23:21:01,407: t15.2023.10.06 val PER: 0.2099
2026-01-11 23:21:01,407: t15.2023.10.08 val PER: 0.3654
2026-01-11 23:21:01,407: t15.2023.10.13 val PER: 0.3747
2026-01-11 23:21:01,408: t15.2023.10.15 val PER: 0.2762
2026-01-11 23:21:01,408: t15.2023.10.20 val PER: 0.3121
2026-01-11 23:21:01,408: t15.2023.10.22 val PER: 0.2305
2026-01-11 23:21:01,408: t15.2023.11.03 val PER: 0.2761
2026-01-11 23:21:01,408: t15.2023.11.04 val PER: 0.0580
2026-01-11 23:21:01,408: t15.2023.11.17 val PER: 0.1244
2026-01-11 23:21:01,408: t15.2023.11.19 val PER: 0.1178
2026-01-11 23:21:01,408: t15.2023.11.26 val PER: 0.3109
2026-01-11 23:21:01,408: t15.2023.12.03 val PER: 0.2458
2026-01-11 23:21:01,408: t15.2023.12.08 val PER: 0.2710
2026-01-11 23:21:01,409: t15.2023.12.10 val PER: 0.2365
2026-01-11 23:21:01,409: t15.2023.12.17 val PER: 0.2391
2026-01-11 23:21:01,409: t15.2023.12.29 val PER: 0.2615
2026-01-11 23:21:01,409: t15.2024.02.25 val PER: 0.2107
2026-01-11 23:21:01,409: t15.2024.03.08 val PER: 0.3243
2026-01-11 23:21:01,409: t15.2024.03.15 val PER: 0.2977
2026-01-11 23:21:01,409: t15.2024.03.17 val PER: 0.2720
2026-01-11 23:21:01,409: t15.2024.05.10 val PER: 0.2927
2026-01-11 23:21:01,409: t15.2024.06.14 val PER: 0.2587
2026-01-11 23:21:01,409: t15.2024.07.19 val PER: 0.3263
2026-01-11 23:21:01,410: t15.2024.07.21 val PER: 0.1903
2026-01-11 23:21:01,410: t15.2024.07.28 val PER: 0.2551
2026-01-11 23:21:01,410: t15.2025.01.10 val PER: 0.3733
2026-01-11 23:21:01,410: t15.2025.01.12 val PER: 0.2710
2026-01-11 23:21:01,410: t15.2025.03.14 val PER: 0.3609
2026-01-11 23:21:01,410: t15.2025.03.16 val PER: 0.2906
2026-01-11 23:21:01,410: t15.2025.03.30 val PER: 0.3632
2026-01-11 23:21:01,410: t15.2025.04.13 val PER: 0.3195
2026-01-11 23:21:01,561: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_13000
2026-01-11 23:21:18,888: Train batch 13200: loss: 38.54 grad norm: 98.28 time: 0.061
2026-01-11 23:21:36,094: Train batch 13400: loss: 25.30 grad norm: 99.35 time: 0.074
2026-01-11 23:21:44,683: Running test after training batch: 13500
2026-01-11 23:21:44,784: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:21:50,164: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point it will
2026-01-11 23:21:50,248: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 23:22:05,973: Val batch 13500: PER (avg): 0.2574 CTC Loss (avg): 40.5422 WER(5gram): 40.42% (n=256) time: 21.289
2026-01-11 23:22:05,974: WER lens: avg_true_words=5.99 avg_pred_words=5.87 max_pred_words=11
2026-01-11 23:22:05,974: t15.2023.08.13 val PER: 0.2256
2026-01-11 23:22:05,974: t15.2023.08.18 val PER: 0.2121
2026-01-11 23:22:05,974: t15.2023.08.20 val PER: 0.2017
2026-01-11 23:22:05,974: t15.2023.08.25 val PER: 0.1958
2026-01-11 23:22:05,975: t15.2023.08.27 val PER: 0.2910
2026-01-11 23:22:05,975: t15.2023.09.01 val PER: 0.1631
2026-01-11 23:22:05,975: t15.2023.09.03 val PER: 0.2518
2026-01-11 23:22:05,975: t15.2023.09.24 val PER: 0.2112
2026-01-11 23:22:05,975: t15.2023.09.29 val PER: 0.2399
2026-01-11 23:22:05,975: t15.2023.10.01 val PER: 0.2946
2026-01-11 23:22:05,975: t15.2023.10.06 val PER: 0.1970
2026-01-11 23:22:05,975: t15.2023.10.08 val PER: 0.3559
2026-01-11 23:22:05,975: t15.2023.10.13 val PER: 0.3732
2026-01-11 23:22:05,975: t15.2023.10.15 val PER: 0.2736
2026-01-11 23:22:05,975: t15.2023.10.20 val PER: 0.3087
2026-01-11 23:22:05,975: t15.2023.10.22 val PER: 0.2261
2026-01-11 23:22:05,975: t15.2023.11.03 val PER: 0.2700
2026-01-11 23:22:05,975: t15.2023.11.04 val PER: 0.0546
2026-01-11 23:22:05,976: t15.2023.11.17 val PER: 0.1073
2026-01-11 23:22:05,976: t15.2023.11.19 val PER: 0.1178
2026-01-11 23:22:05,976: t15.2023.11.26 val PER: 0.3022
2026-01-11 23:22:05,976: t15.2023.12.03 val PER: 0.2458
2026-01-11 23:22:05,976: t15.2023.12.08 val PER: 0.2517
2026-01-11 23:22:05,976: t15.2023.12.10 val PER: 0.2365
2026-01-11 23:22:05,976: t15.2023.12.17 val PER: 0.2308
2026-01-11 23:22:05,976: t15.2023.12.29 val PER: 0.2519
2026-01-11 23:22:05,976: t15.2024.02.25 val PER: 0.2135
2026-01-11 23:22:05,976: t15.2024.03.08 val PER: 0.3073
2026-01-11 23:22:05,976: t15.2024.03.15 val PER: 0.2871
2026-01-11 23:22:05,977: t15.2024.03.17 val PER: 0.2566
2026-01-11 23:22:05,977: t15.2024.05.10 val PER: 0.2660
2026-01-11 23:22:05,977: t15.2024.06.14 val PER: 0.2587
2026-01-11 23:22:05,977: t15.2024.07.19 val PER: 0.3184
2026-01-11 23:22:05,977: t15.2024.07.21 val PER: 0.1972
2026-01-11 23:22:05,977: t15.2024.07.28 val PER: 0.2515
2026-01-11 23:22:05,977: t15.2025.01.10 val PER: 0.3815
2026-01-11 23:22:05,977: t15.2025.01.12 val PER: 0.2502
2026-01-11 23:22:05,977: t15.2025.03.14 val PER: 0.3669
2026-01-11 23:22:05,977: t15.2025.03.16 val PER: 0.2775
2026-01-11 23:22:05,977: t15.2025.03.30 val PER: 0.3506
2026-01-11 23:22:05,977: t15.2025.04.13 val PER: 0.3081
2026-01-11 23:22:05,978: New best val WER(5gram) 45.83% --> 40.42%
2026-01-11 23:22:06,136: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_13500
2026-01-11 23:22:14,750: Train batch 13600: loss: 34.04 grad norm: 90.62 time: 0.068
2026-01-11 23:22:32,288: Train batch 13800: loss: 25.77 grad norm: 91.34 time: 0.062
2026-01-11 23:22:49,590: Train batch 14000: loss: 35.97 grad norm: 94.07 time: 0.056
2026-01-11 23:22:49,590: Running test after training batch: 14000
2026-01-11 23:22:49,712: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:22:55,341: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 23:22:55,410: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 23:23:10,127: Val batch 14000: PER (avg): 0.2595 CTC Loss (avg): 39.9086 WER(5gram): 44.72% (n=256) time: 20.537
2026-01-11 23:23:10,128: WER lens: avg_true_words=5.99 avg_pred_words=5.79 max_pred_words=11
2026-01-11 23:23:10,128: t15.2023.08.13 val PER: 0.2162
2026-01-11 23:23:10,128: t15.2023.08.18 val PER: 0.2213
2026-01-11 23:23:10,128: t15.2023.08.20 val PER: 0.2049
2026-01-11 23:23:10,128: t15.2023.08.25 val PER: 0.1973
2026-01-11 23:23:10,128: t15.2023.08.27 val PER: 0.2894
2026-01-11 23:23:10,128: t15.2023.09.01 val PER: 0.1656
2026-01-11 23:23:10,128: t15.2023.09.03 val PER: 0.2672
2026-01-11 23:23:10,128: t15.2023.09.24 val PER: 0.2051
2026-01-11 23:23:10,128: t15.2023.09.29 val PER: 0.2380
2026-01-11 23:23:10,129: t15.2023.10.01 val PER: 0.2952
2026-01-11 23:23:10,129: t15.2023.10.06 val PER: 0.1905
2026-01-11 23:23:10,129: t15.2023.10.08 val PER: 0.3586
2026-01-11 23:23:10,129: t15.2023.10.13 val PER: 0.3724
2026-01-11 23:23:10,129: t15.2023.10.15 val PER: 0.2676
2026-01-11 23:23:10,129: t15.2023.10.20 val PER: 0.2953
2026-01-11 23:23:10,129: t15.2023.10.22 val PER: 0.2339
2026-01-11 23:23:10,129: t15.2023.11.03 val PER: 0.2768
2026-01-11 23:23:10,129: t15.2023.11.04 val PER: 0.0683
2026-01-11 23:23:10,129: t15.2023.11.17 val PER: 0.1135
2026-01-11 23:23:10,130: t15.2023.11.19 val PER: 0.1038
2026-01-11 23:23:10,130: t15.2023.11.26 val PER: 0.3138
2026-01-11 23:23:10,130: t15.2023.12.03 val PER: 0.2437
2026-01-11 23:23:10,130: t15.2023.12.08 val PER: 0.2597
2026-01-11 23:23:10,130: t15.2023.12.10 val PER: 0.2194
2026-01-11 23:23:10,130: t15.2023.12.17 val PER: 0.2287
2026-01-11 23:23:10,130: t15.2023.12.29 val PER: 0.2601
2026-01-11 23:23:10,130: t15.2024.02.25 val PER: 0.2247
2026-01-11 23:23:10,130: t15.2024.03.08 val PER: 0.3272
2026-01-11 23:23:10,130: t15.2024.03.15 val PER: 0.2858
2026-01-11 23:23:10,130: t15.2024.03.17 val PER: 0.2678
2026-01-11 23:23:10,130: t15.2024.05.10 val PER: 0.2719
2026-01-11 23:23:10,130: t15.2024.06.14 val PER: 0.2413
2026-01-11 23:23:10,130: t15.2024.07.19 val PER: 0.3144
2026-01-11 23:23:10,131: t15.2024.07.21 val PER: 0.1931
2026-01-11 23:23:10,131: t15.2024.07.28 val PER: 0.2507
2026-01-11 23:23:10,131: t15.2025.01.10 val PER: 0.3678
2026-01-11 23:23:10,131: t15.2025.01.12 val PER: 0.2594
2026-01-11 23:23:10,131: t15.2025.03.14 val PER: 0.3728
2026-01-11 23:23:10,131: t15.2025.03.16 val PER: 0.2971
2026-01-11 23:23:10,131: t15.2025.03.30 val PER: 0.3609
2026-01-11 23:23:10,131: t15.2025.04.13 val PER: 0.3138
2026-01-11 23:23:10,284: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_14000
2026-01-11 23:23:27,392: Train batch 14200: loss: 29.05 grad norm: 71.79 time: 0.062
2026-01-11 23:23:44,841: Train batch 14400: loss: 21.98 grad norm: 58.88 time: 0.069
2026-01-11 23:23:53,602: Running test after training batch: 14500
2026-01-11 23:23:53,702: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:23:59,111: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 23:23:59,186: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 23:24:13,573: Val batch 14500: PER (avg): 0.2532 CTC Loss (avg): 39.2646 WER(5gram): 44.13% (n=256) time: 19.970
2026-01-11 23:24:13,573: WER lens: avg_true_words=5.99 avg_pred_words=5.86 max_pred_words=11
2026-01-11 23:24:13,573: t15.2023.08.13 val PER: 0.2131
2026-01-11 23:24:13,573: t15.2023.08.18 val PER: 0.2104
2026-01-11 23:24:13,573: t15.2023.08.20 val PER: 0.2041
2026-01-11 23:24:13,573: t15.2023.08.25 val PER: 0.1988
2026-01-11 23:24:13,573: t15.2023.08.27 val PER: 0.2878
2026-01-11 23:24:13,574: t15.2023.09.01 val PER: 0.1591
2026-01-11 23:24:13,574: t15.2023.09.03 val PER: 0.2637
2026-01-11 23:24:13,574: t15.2023.09.24 val PER: 0.2002
2026-01-11 23:24:13,574: t15.2023.09.29 val PER: 0.2348
2026-01-11 23:24:13,574: t15.2023.10.01 val PER: 0.2952
2026-01-11 23:24:13,574: t15.2023.10.06 val PER: 0.1884
2026-01-11 23:24:13,574: t15.2023.10.08 val PER: 0.3532
2026-01-11 23:24:13,574: t15.2023.10.13 val PER: 0.3623
2026-01-11 23:24:13,575: t15.2023.10.15 val PER: 0.2637
2026-01-11 23:24:13,575: t15.2023.10.20 val PER: 0.2953
2026-01-11 23:24:13,575: t15.2023.10.22 val PER: 0.2249
2026-01-11 23:24:13,575: t15.2023.11.03 val PER: 0.2741
2026-01-11 23:24:13,575: t15.2023.11.04 val PER: 0.0614
2026-01-11 23:24:13,575: t15.2023.11.17 val PER: 0.1058
2026-01-11 23:24:13,575: t15.2023.11.19 val PER: 0.0978
2026-01-11 23:24:13,575: t15.2023.11.26 val PER: 0.2964
2026-01-11 23:24:13,575: t15.2023.12.03 val PER: 0.2300
2026-01-11 23:24:13,575: t15.2023.12.08 val PER: 0.2437
2026-01-11 23:24:13,576: t15.2023.12.10 val PER: 0.2221
2026-01-11 23:24:13,576: t15.2023.12.17 val PER: 0.2277
2026-01-11 23:24:13,576: t15.2023.12.29 val PER: 0.2375
2026-01-11 23:24:13,576: t15.2024.02.25 val PER: 0.2037
2026-01-11 23:24:13,576: t15.2024.03.08 val PER: 0.3201
2026-01-11 23:24:13,576: t15.2024.03.15 val PER: 0.2864
2026-01-11 23:24:13,576: t15.2024.03.17 val PER: 0.2559
2026-01-11 23:24:13,576: t15.2024.05.10 val PER: 0.2600
2026-01-11 23:24:13,576: t15.2024.06.14 val PER: 0.2413
2026-01-11 23:24:13,576: t15.2024.07.19 val PER: 0.3171
2026-01-11 23:24:13,577: t15.2024.07.21 val PER: 0.1924
2026-01-11 23:24:13,577: t15.2024.07.28 val PER: 0.2434
2026-01-11 23:24:13,577: t15.2025.01.10 val PER: 0.3774
2026-01-11 23:24:13,577: t15.2025.01.12 val PER: 0.2510
2026-01-11 23:24:13,577: t15.2025.03.14 val PER: 0.3447
2026-01-11 23:24:13,577: t15.2025.03.16 val PER: 0.2984
2026-01-11 23:24:13,577: t15.2025.03.30 val PER: 0.3540
2026-01-11 23:24:13,577: t15.2025.04.13 val PER: 0.3081
2026-01-11 23:24:13,729: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_14500
2026-01-11 23:24:22,408: Train batch 14600: loss: 35.83 grad norm: 89.83 time: 0.066
2026-01-11 23:24:39,739: Train batch 14800: loss: 20.56 grad norm: 72.53 time: 0.056
2026-01-11 23:24:56,859: Train batch 15000: loss: 24.79 grad norm: 92.10 time: 0.057
2026-01-11 23:24:56,860: Running test after training batch: 15000
2026-01-11 23:24:56,966: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:25:02,368: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 23:25:02,439: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 23:25:16,946: Val batch 15000: PER (avg): 0.2453 CTC Loss (avg): 38.7675 WER(5gram): 39.24% (n=256) time: 20.086
2026-01-11 23:25:16,946: WER lens: avg_true_words=5.99 avg_pred_words=5.87 max_pred_words=11
2026-01-11 23:25:16,947: t15.2023.08.13 val PER: 0.2048
2026-01-11 23:25:16,947: t15.2023.08.18 val PER: 0.1978
2026-01-11 23:25:16,947: t15.2023.08.20 val PER: 0.1898
2026-01-11 23:25:16,947: t15.2023.08.25 val PER: 0.1837
2026-01-11 23:25:16,947: t15.2023.08.27 val PER: 0.2781
2026-01-11 23:25:16,947: t15.2023.09.01 val PER: 0.1518
2026-01-11 23:25:16,947: t15.2023.09.03 val PER: 0.2601
2026-01-11 23:25:16,947: t15.2023.09.24 val PER: 0.1905
2026-01-11 23:25:16,947: t15.2023.09.29 val PER: 0.2329
2026-01-11 23:25:16,947: t15.2023.10.01 val PER: 0.2840
2026-01-11 23:25:16,947: t15.2023.10.06 val PER: 0.1776
2026-01-11 23:25:16,948: t15.2023.10.08 val PER: 0.3451
2026-01-11 23:25:16,948: t15.2023.10.13 val PER: 0.3584
2026-01-11 23:25:16,948: t15.2023.10.15 val PER: 0.2525
2026-01-11 23:25:16,948: t15.2023.10.20 val PER: 0.3087
2026-01-11 23:25:16,948: t15.2023.10.22 val PER: 0.2094
2026-01-11 23:25:16,948: t15.2023.11.03 val PER: 0.2632
2026-01-11 23:25:16,948: t15.2023.11.04 val PER: 0.0580
2026-01-11 23:25:16,948: t15.2023.11.17 val PER: 0.1011
2026-01-11 23:25:16,948: t15.2023.11.19 val PER: 0.0998
2026-01-11 23:25:16,948: t15.2023.11.26 val PER: 0.2804
2026-01-11 23:25:16,948: t15.2023.12.03 val PER: 0.2258
2026-01-11 23:25:16,948: t15.2023.12.08 val PER: 0.2403
2026-01-11 23:25:16,948: t15.2023.12.10 val PER: 0.2168
2026-01-11 23:25:16,949: t15.2023.12.17 val PER: 0.2204
2026-01-11 23:25:16,949: t15.2023.12.29 val PER: 0.2395
2026-01-11 23:25:16,949: t15.2024.02.25 val PER: 0.1924
2026-01-11 23:25:16,949: t15.2024.03.08 val PER: 0.3115
2026-01-11 23:25:16,949: t15.2024.03.15 val PER: 0.2764
2026-01-11 23:25:16,949: t15.2024.03.17 val PER: 0.2413
2026-01-11 23:25:16,949: t15.2024.05.10 val PER: 0.2571
2026-01-11 23:25:16,949: t15.2024.06.14 val PER: 0.2366
2026-01-11 23:25:16,949: t15.2024.07.19 val PER: 0.3098
2026-01-11 23:25:16,949: t15.2024.07.21 val PER: 0.1786
2026-01-11 23:25:16,949: t15.2024.07.28 val PER: 0.2404
2026-01-11 23:25:16,950: t15.2025.01.10 val PER: 0.3636
2026-01-11 23:25:16,950: t15.2025.01.12 val PER: 0.2417
2026-01-11 23:25:16,950: t15.2025.03.14 val PER: 0.3831
2026-01-11 23:25:16,950: t15.2025.03.16 val PER: 0.2762
2026-01-11 23:25:16,950: t15.2025.03.30 val PER: 0.3517
2026-01-11 23:25:16,950: t15.2025.04.13 val PER: 0.2839
2026-01-11 23:25:16,951: New best val WER(5gram) 40.42% --> 39.24%
2026-01-11 23:25:17,101: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_15000
2026-01-11 23:25:34,523: Train batch 15200: loss: 18.31 grad norm: 83.25 time: 0.062
2026-01-11 23:25:51,789: Train batch 15400: loss: 31.21 grad norm: 105.95 time: 0.056
2026-01-11 23:26:00,606: Running test after training batch: 15500
2026-01-11 23:26:00,766: WER debug GT example: You can see the code at this point as well.
2026-01-11 23:26:06,426: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 23:26:06,482: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 23:26:21,334: Val batch 15500: PER (avg): 0.2460 CTC Loss (avg): 38.3317 WER(5gram): 40.87% (n=256) time: 20.728
2026-01-11 23:26:21,334: WER lens: avg_true_words=5.99 avg_pred_words=5.73 max_pred_words=11
2026-01-11 23:26:21,335: t15.2023.08.13 val PER: 0.2069
2026-01-11 23:26:21,335: t15.2023.08.18 val PER: 0.2104
2026-01-11 23:26:21,335: t15.2023.08.20 val PER: 0.1914
2026-01-11 23:26:21,335: t15.2023.08.25 val PER: 0.1822
2026-01-11 23:26:21,335: t15.2023.08.27 val PER: 0.2749
2026-01-11 23:26:21,335: t15.2023.09.01 val PER: 0.1583
2026-01-11 23:26:21,335: t15.2023.09.03 val PER: 0.2494
2026-01-11 23:26:21,335: t15.2023.09.24 val PER: 0.1917
2026-01-11 23:26:21,335: t15.2023.09.29 val PER: 0.2221
2026-01-11 23:26:21,335: t15.2023.10.01 val PER: 0.2801
2026-01-11 23:26:21,335: t15.2023.10.06 val PER: 0.1755
2026-01-11 23:26:21,335: t15.2023.10.08 val PER: 0.3478
2026-01-11 23:26:21,336: t15.2023.10.13 val PER: 0.3615
2026-01-11 23:26:21,336: t15.2023.10.15 val PER: 0.2564
2026-01-11 23:26:21,336: t15.2023.10.20 val PER: 0.2987
2026-01-11 23:26:21,336: t15.2023.10.22 val PER: 0.2049
2026-01-11 23:26:21,336: t15.2023.11.03 val PER: 0.2585
2026-01-11 23:26:21,336: t15.2023.11.04 val PER: 0.0512
2026-01-11 23:26:21,336: t15.2023.11.17 val PER: 0.1073
2026-01-11 23:26:21,336: t15.2023.11.19 val PER: 0.1038
2026-01-11 23:26:21,336: t15.2023.11.26 val PER: 0.2877
2026-01-11 23:26:21,336: t15.2023.12.03 val PER: 0.2237
2026-01-11 23:26:21,337: t15.2023.12.08 val PER: 0.2483
2026-01-11 23:26:21,337: t15.2023.12.10 val PER: 0.2063
2026-01-11 23:26:21,337: t15.2023.12.17 val PER: 0.2235
2026-01-11 23:26:21,337: t15.2023.12.29 val PER: 0.2375
2026-01-11 23:26:21,337: t15.2024.02.25 val PER: 0.1994
2026-01-11 23:26:21,337: t15.2024.03.08 val PER: 0.3115
2026-01-11 23:26:21,337: t15.2024.03.15 val PER: 0.2802
2026-01-11 23:26:21,337: t15.2024.03.17 val PER: 0.2469
2026-01-11 23:26:21,337: t15.2024.05.10 val PER: 0.2675
2026-01-11 23:26:21,337: t15.2024.06.14 val PER: 0.2397
2026-01-11 23:26:21,337: t15.2024.07.19 val PER: 0.3072
2026-01-11 23:26:21,337: t15.2024.07.21 val PER: 0.1779
2026-01-11 23:26:21,338: t15.2024.07.28 val PER: 0.2412
2026-01-11 23:26:21,338: t15.2025.01.10 val PER: 0.3595
2026-01-11 23:26:21,338: t15.2025.01.12 val PER: 0.2410
2026-01-11 23:26:21,338: t15.2025.03.14 val PER: 0.3639
2026-01-11 23:26:21,338: t15.2025.03.16 val PER: 0.2906
2026-01-11 23:26:21,338: t15.2025.03.30 val PER: 0.3414
2026-01-11 23:26:21,338: t15.2025.04.13 val PER: 0.3039
2026-01-11 23:26:21,495: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_15500
