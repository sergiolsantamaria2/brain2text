TMPDIR=/home/e12511253/tmp
JOB_TMP=/home/e12511253/tmp/e12511253_b2t_352892
TORCH_EXTENSIONS_DIR=/home/e12511253/tmp/e12511253_b2t_352892/torch_extensions
WANDB_DIR=/home/e12511253/tmp/e12511253_b2t_352892/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/home/e12511253/tmp/e12511253_b2t_352892/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan 10 21:51 /home/e12511253/tmp/e12511253_b2t_352892/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
OUT_ROOT=/home/e12511253/Brain2Text/brain2text/trained_models
==============================================
Job: b2t5g  ID: 352892
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_5gram_only.yaml
Folders: configs/experiments/diphones
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/diphones ==========
Num configs: 4

=== RUN diphone_base.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base
2026-01-10 21:51:16,197: Using device: cuda:0
2026-01-10 21:55:09,080: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-10 21:55:09,083: Diphone mode ENABLED: n_classes changed from 41 to 1601
2026-01-10 21:55:09,104: Using 45 sessions after filtering (from 45).
2026-01-10 21:55:09,499: Using torch.compile (if available)
2026-01-10 21:55:09,500: torch.compile not available (torch<2.0). Skipping.
2026-01-10 21:55:09,500: Initialized RNN decoding model
2026-01-10 21:55:09,500: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Identity()
  (out): Linear(in_features=768, out_features=1601, bias=True)
)
2026-01-10 21:55:09,500: Model has 45,514,817 parameters
2026-01-10 21:55:09,500: Model has 11,819,520 day-specific parameters | 25.97% of total parameters
2026-01-10 21:55:10,774: Successfully initialized datasets
2026-01-10 21:55:10,775: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-10 21:55:12,152: Train batch 0: loss: 1387.21 grad norm: 257.41 time: 0.167
2026-01-10 21:55:12,152: Running test after training batch: 0
2026-01-10 21:55:12,260: WER debug GT example: You can see the code at this point as well.
2026-01-10 21:55:18,626: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-10 21:55:19,721: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-10 21:59:42,404: Val batch 0: PER (avg): 4.1039 CTC Loss (avg): 1561.5700 WER(5gram): 100.00% (n=256) time: 270.251
2026-01-10 21:59:42,406: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=0
2026-01-10 21:59:42,407: t15.2023.08.13 val PER: 3.3836
2026-01-10 21:59:42,407: t15.2023.08.18 val PER: 3.6622
2026-01-10 21:59:42,407: t15.2023.08.20 val PER: 3.6434
2026-01-10 21:59:42,407: t15.2023.08.25 val PER: 3.6702
2026-01-10 21:59:42,407: t15.2023.08.27 val PER: 3.4550
2026-01-10 21:59:42,407: t15.2023.09.01 val PER: 3.7630
2026-01-10 21:59:42,407: t15.2023.09.03 val PER: 3.6425
2026-01-10 21:59:42,407: t15.2023.09.24 val PER: 4.3022
2026-01-10 21:59:42,407: t15.2023.09.29 val PER: 4.3050
2026-01-10 21:59:42,407: t15.2023.10.01 val PER: 3.3098
2026-01-10 21:59:42,407: t15.2023.10.06 val PER: 4.1270
2026-01-10 21:59:42,408: t15.2023.10.08 val PER: 3.0947
2026-01-10 21:59:42,408: t15.2023.10.13 val PER: 3.8937
2026-01-10 21:59:42,408: t15.2023.10.15 val PER: 4.3659
2026-01-10 21:59:42,408: t15.2023.10.20 val PER: 4.5302
2026-01-10 21:59:42,408: t15.2023.10.22 val PER: 4.3318
2026-01-10 21:59:42,408: t15.2023.11.03 val PER: 4.6418
2026-01-10 21:59:42,408: t15.2023.11.04 val PER: 5.5563
2026-01-10 21:59:42,408: t15.2023.11.17 val PER: 5.9036
2026-01-10 21:59:42,408: t15.2023.11.19 val PER: 4.5529
2026-01-10 21:59:42,408: t15.2023.11.26 val PER: 4.6043
2026-01-10 21:59:42,408: t15.2023.12.03 val PER: 4.3382
2026-01-10 21:59:42,408: t15.2023.12.08 val PER: 4.6105
2026-01-10 21:59:42,409: t15.2023.12.10 val PER: 5.0631
2026-01-10 21:59:42,409: t15.2023.12.17 val PER: 3.7453
2026-01-10 21:59:42,409: t15.2023.12.29 val PER: 4.0954
2026-01-10 21:59:42,409: t15.2024.02.25 val PER: 3.8764
2026-01-10 21:59:42,409: t15.2024.03.08 val PER: 3.8777
2026-01-10 21:59:42,409: t15.2024.03.15 val PER: 3.7273
2026-01-10 21:59:42,409: t15.2024.03.17 val PER: 3.9596
2026-01-10 21:59:42,409: t15.2024.05.10 val PER: 3.8484
2026-01-10 21:59:42,409: t15.2024.06.14 val PER: 4.3801
2026-01-10 21:59:42,409: t15.2024.07.19 val PER: 3.0125
2026-01-10 21:59:42,409: t15.2024.07.21 val PER: 4.6117
2026-01-10 21:59:42,409: t15.2024.07.28 val PER: 4.8169
2026-01-10 21:59:42,409: t15.2025.01.10 val PER: 2.8058
2026-01-10 21:59:42,409: t15.2025.01.12 val PER: 5.3195
2026-01-10 21:59:42,410: t15.2025.03.14 val PER: 2.8624
2026-01-10 21:59:42,410: t15.2025.03.16 val PER: 5.0668
2026-01-10 21:59:42,410: t15.2025.03.30 val PER: 3.9977
2026-01-10 21:59:42,410: t15.2025.04.13 val PER: 4.4979
2026-01-10 21:59:42,411: New best val WER(5gram) inf% --> 100.00%
2026-01-10 21:59:42,553: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_0
2026-01-10 21:59:59,230: Train batch 200: loss: 128.72 grad norm: 41.04 time: 0.060
2026-01-10 22:00:15,725: Train batch 400: loss: 96.15 grad norm: 35.20 time: 0.067
2026-01-10 22:00:23,976: Running test after training batch: 500
2026-01-10 22:00:24,097: WER debug GT example: You can see the code at this point as well.
2026-01-10 22:00:30,405: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-10 22:00:31,424: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-10 22:04:27,598: Val batch 500: PER (avg): 0.9362 CTC Loss (avg): 116.8867 WER(5gram): 100.00% (n=256) time: 243.621
2026-01-10 22:04:27,604: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=0
2026-01-10 22:04:27,608: t15.2023.08.13 val PER: 0.9335
2026-01-10 22:04:27,611: t15.2023.08.18 val PER: 0.9229
2026-01-10 22:04:27,613: t15.2023.08.20 val PER: 0.9293
2026-01-10 22:04:27,613: t15.2023.08.25 val PER: 0.9307
2026-01-10 22:04:27,613: t15.2023.08.27 val PER: 0.9277
2026-01-10 22:04:27,613: t15.2023.09.01 val PER: 0.9245
2026-01-10 22:04:27,613: t15.2023.09.03 val PER: 0.9287
2026-01-10 22:04:27,613: t15.2023.09.24 val PER: 0.9211
2026-01-10 22:04:27,613: t15.2023.09.29 val PER: 0.9445
2026-01-10 22:04:27,613: t15.2023.10.01 val PER: 0.9439
2026-01-10 22:04:27,613: t15.2023.10.06 val PER: 0.9290
2026-01-10 22:04:27,613: t15.2023.10.08 val PER: 0.9540
2026-01-10 22:04:27,613: t15.2023.10.13 val PER: 0.9356
2026-01-10 22:04:27,613: t15.2023.10.15 val PER: 0.9473
2026-01-10 22:04:27,613: t15.2023.10.20 val PER: 0.9396
2026-01-10 22:04:27,614: t15.2023.10.22 val PER: 0.9332
2026-01-10 22:04:27,614: t15.2023.11.03 val PER: 0.9355
2026-01-10 22:04:27,614: t15.2023.11.04 val PER: 0.9249
2026-01-10 22:04:27,614: t15.2023.11.17 val PER: 0.9316
2026-01-10 22:04:27,614: t15.2023.11.19 val PER: 0.9341
2026-01-10 22:04:27,614: t15.2023.11.26 val PER: 0.9399
2026-01-10 22:04:27,614: t15.2023.12.03 val PER: 0.9349
2026-01-10 22:04:27,614: t15.2023.12.08 val PER: 0.9387
2026-01-10 22:04:27,614: t15.2023.12.10 val PER: 0.9369
2026-01-10 22:04:27,614: t15.2023.12.17 val PER: 0.9418
2026-01-10 22:04:27,614: t15.2023.12.29 val PER: 0.9375
2026-01-10 22:04:27,614: t15.2024.02.25 val PER: 0.9438
2026-01-10 22:04:27,614: t15.2024.03.08 val PER: 0.9346
2026-01-10 22:04:27,615: t15.2024.03.15 val PER: 0.9431
2026-01-10 22:04:27,615: t15.2024.03.17 val PER: 0.9386
2026-01-10 22:04:27,615: t15.2024.05.10 val PER: 0.9316
2026-01-10 22:04:27,615: t15.2024.06.14 val PER: 0.9274
2026-01-10 22:04:27,615: t15.2024.07.19 val PER: 0.9420
2026-01-10 22:04:27,615: t15.2024.07.21 val PER: 0.9400
2026-01-10 22:04:27,615: t15.2024.07.28 val PER: 0.9338
2026-01-10 22:04:27,615: t15.2025.01.10 val PER: 0.9394
2026-01-10 22:04:27,615: t15.2025.01.12 val PER: 0.9323
2026-01-10 22:04:27,615: t15.2025.03.14 val PER: 0.9320
2026-01-10 22:04:27,615: t15.2025.03.16 val PER: 0.9424
2026-01-10 22:04:27,615: t15.2025.03.30 val PER: 0.9368
2026-01-10 22:04:27,615: t15.2025.04.13 val PER: 0.9330
2026-01-10 22:04:27,752: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_500
2026-01-10 22:04:36,139: Train batch 600: loss: 104.39 grad norm: 48.41 time: 0.082
2026-01-10 22:04:53,023: Train batch 800: loss: 97.47 grad norm: 40.11 time: 0.062
2026-01-10 22:05:09,994: Train batch 1000: loss: 87.37 grad norm: 25.38 time: 0.071
2026-01-10 22:05:09,994: Running test after training batch: 1000
2026-01-10 22:05:10,126: WER debug GT example: You can see the code at this point as well.
2026-01-10 22:05:16,456: WER debug example
  GT : you can see the code at this point as well
  PR : the estimated
2026-01-10 22:05:17,380: WER debug example
  GT : how does it keep the cost down
  PR : the estimated
2026-01-10 22:08:26,727: Val batch 1000: PER (avg): 0.8740 CTC Loss (avg): 104.0108 WER(5gram): 97.85% (n=256) time: 196.733
2026-01-10 22:08:26,734: WER lens: avg_true_words=5.99 avg_pred_words=1.98 max_pred_words=5
2026-01-10 22:08:26,736: t15.2023.08.13 val PER: 0.8929
2026-01-10 22:08:26,738: t15.2023.08.18 val PER: 0.8684
2026-01-10 22:08:26,739: t15.2023.08.20 val PER: 0.8697
2026-01-10 22:08:26,739: t15.2023.08.25 val PER: 0.8524
2026-01-10 22:08:26,739: t15.2023.08.27 val PER: 0.8617
2026-01-10 22:08:26,739: t15.2023.09.01 val PER: 0.8523
2026-01-10 22:08:26,739: t15.2023.09.03 val PER: 0.8646
2026-01-10 22:08:26,739: t15.2023.09.24 val PER: 0.8592
2026-01-10 22:08:26,739: t15.2023.09.29 val PER: 0.8609
2026-01-10 22:08:26,739: t15.2023.10.01 val PER: 0.8415
2026-01-10 22:08:26,740: t15.2023.10.06 val PER: 0.8396
2026-01-10 22:08:26,740: t15.2023.10.08 val PER: 0.8498
2026-01-10 22:08:26,740: t15.2023.10.13 val PER: 0.8518
2026-01-10 22:08:26,740: t15.2023.10.15 val PER: 0.8616
2026-01-10 22:08:26,740: t15.2023.10.20 val PER: 0.8591
2026-01-10 22:08:26,740: t15.2023.10.22 val PER: 0.8608
2026-01-10 22:08:26,740: t15.2023.11.03 val PER: 0.9098
2026-01-10 22:08:26,740: t15.2023.11.04 val PER: 0.9044
2026-01-10 22:08:26,740: t15.2023.11.17 val PER: 0.9129
2026-01-10 22:08:26,740: t15.2023.11.19 val PER: 0.9162
2026-01-10 22:08:26,740: t15.2023.11.26 val PER: 0.8870
2026-01-10 22:08:26,740: t15.2023.12.03 val PER: 0.8855
2026-01-10 22:08:26,740: t15.2023.12.08 val PER: 0.8715
2026-01-10 22:08:26,740: t15.2023.12.10 val PER: 0.9001
2026-01-10 22:08:26,740: t15.2023.12.17 val PER: 0.8763
2026-01-10 22:08:26,741: t15.2023.12.29 val PER: 0.8703
2026-01-10 22:08:26,741: t15.2024.02.25 val PER: 0.8553
2026-01-10 22:08:26,741: t15.2024.03.08 val PER: 0.8307
2026-01-10 22:08:26,741: t15.2024.03.15 val PER: 0.8874
2026-01-10 22:08:26,741: t15.2024.03.17 val PER: 0.8654
2026-01-10 22:08:26,741: t15.2024.05.10 val PER: 0.8559
2026-01-10 22:08:26,741: t15.2024.06.14 val PER: 0.8738
2026-01-10 22:08:26,741: t15.2024.07.19 val PER: 0.8583
2026-01-10 22:08:26,741: t15.2024.07.21 val PER: 0.8862
2026-01-10 22:08:26,741: t15.2024.07.28 val PER: 0.8757
2026-01-10 22:08:26,741: t15.2025.01.10 val PER: 0.9242
2026-01-10 22:08:26,741: t15.2025.01.12 val PER: 0.8930
2026-01-10 22:08:26,741: t15.2025.03.14 val PER: 0.9216
2026-01-10 22:08:26,741: t15.2025.03.16 val PER: 0.8770
2026-01-10 22:08:26,741: t15.2025.03.30 val PER: 0.9207
2026-01-10 22:08:26,741: t15.2025.04.13 val PER: 0.8916
2026-01-10 22:08:26,743: New best val WER(5gram) 100.00% --> 97.85%
2026-01-10 22:08:26,896: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_1000
2026-01-10 22:08:43,760: Train batch 1200: loss: 84.57 grad norm: 32.88 time: 0.074
2026-01-10 22:09:01,339: Train batch 1400: loss: 72.15 grad norm: 81.40 time: 0.065
2026-01-10 22:09:10,047: Running test after training batch: 1500
2026-01-10 22:09:10,196: WER debug GT example: You can see the code at this point as well.
2026-01-10 22:09:16,202: WER debug example
  GT : you can see the code at this point as well
  PR : he hit the hardest hit jorge
2026-01-10 22:09:16,943: WER debug example
  GT : how does it keep the cost down
  PR : i had hoped that the that
2026-01-10 22:11:24,073: Val batch 1500: PER (avg): 0.6907 CTC Loss (avg): 86.2694 WER(5gram): 97.91% (n=256) time: 134.025
2026-01-10 22:11:24,076: WER lens: avg_true_words=5.99 avg_pred_words=5.64 max_pred_words=12
2026-01-10 22:11:24,077: t15.2023.08.13 val PER: 0.6840
2026-01-10 22:11:24,077: t15.2023.08.18 val PER: 0.6756
2026-01-10 22:11:24,078: t15.2023.08.20 val PER: 0.6561
2026-01-10 22:11:24,078: t15.2023.08.25 val PER: 0.6732
2026-01-10 22:11:24,078: t15.2023.08.27 val PER: 0.7251
2026-01-10 22:11:24,078: t15.2023.09.01 val PER: 0.6705
2026-01-10 22:11:24,078: t15.2023.09.03 val PER: 0.6971
2026-01-10 22:11:24,078: t15.2023.09.24 val PER: 0.6881
2026-01-10 22:11:24,078: t15.2023.09.29 val PER: 0.6694
2026-01-10 22:11:24,078: t15.2023.10.01 val PER: 0.7074
2026-01-10 22:11:24,078: t15.2023.10.06 val PER: 0.6749
2026-01-10 22:11:24,078: t15.2023.10.08 val PER: 0.6955
2026-01-10 22:11:24,078: t15.2023.10.13 val PER: 0.7362
2026-01-10 22:11:24,078: t15.2023.10.15 val PER: 0.6928
2026-01-10 22:11:24,078: t15.2023.10.20 val PER: 0.6846
2026-01-10 22:11:24,078: t15.2023.10.22 val PER: 0.6782
2026-01-10 22:11:24,078: t15.2023.11.03 val PER: 0.6825
2026-01-10 22:11:24,079: t15.2023.11.04 val PER: 0.6212
2026-01-10 22:11:24,079: t15.2023.11.17 val PER: 0.6267
2026-01-10 22:11:24,079: t15.2023.11.19 val PER: 0.6068
2026-01-10 22:11:24,079: t15.2023.11.26 val PER: 0.7181
2026-01-10 22:11:24,079: t15.2023.12.03 val PER: 0.7048
2026-01-10 22:11:24,079: t15.2023.12.08 val PER: 0.6784
2026-01-10 22:11:24,079: t15.2023.12.10 val PER: 0.6899
2026-01-10 22:11:24,079: t15.2023.12.17 val PER: 0.6726
2026-01-10 22:11:24,080: t15.2023.12.29 val PER: 0.6788
2026-01-10 22:11:24,080: t15.2024.02.25 val PER: 0.6910
2026-01-10 22:11:24,080: t15.2024.03.08 val PER: 0.6743
2026-01-10 22:11:24,080: t15.2024.03.15 val PER: 0.7017
2026-01-10 22:11:24,080: t15.2024.03.17 val PER: 0.6681
2026-01-10 22:11:24,080: t15.2024.05.10 val PER: 0.6835
2026-01-10 22:11:24,080: t15.2024.06.14 val PER: 0.6751
2026-01-10 22:11:24,080: t15.2024.07.19 val PER: 0.7053
2026-01-10 22:11:24,080: t15.2024.07.21 val PER: 0.6752
2026-01-10 22:11:24,080: t15.2024.07.28 val PER: 0.6919
2026-01-10 22:11:24,080: t15.2025.01.10 val PER: 0.7479
2026-01-10 22:11:24,080: t15.2025.01.12 val PER: 0.7021
2026-01-10 22:11:24,080: t15.2025.03.14 val PER: 0.7633
2026-01-10 22:11:24,080: t15.2025.03.16 val PER: 0.7330
2026-01-10 22:11:24,081: t15.2025.03.30 val PER: 0.7632
2026-01-10 22:11:24,081: t15.2025.04.13 val PER: 0.7019
2026-01-10 22:11:24,225: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_1500
2026-01-10 22:11:32,706: Train batch 1600: loss: 75.30 grad norm: 40.07 time: 0.069
2026-01-10 22:11:50,877: Train batch 1800: loss: 70.09 grad norm: 59.59 time: 0.093
2026-01-10 22:12:08,249: Train batch 2000: loss: 67.54 grad norm: 64.51 time: 0.072
2026-01-10 22:12:08,249: Running test after training batch: 2000
2026-01-10 22:12:08,376: WER debug GT example: You can see the code at this point as well.
2026-01-10 22:12:14,461: WER debug example
  GT : you can see the code at this point as well
  PR : here he doesn't hit this heightened awareness
2026-01-10 22:12:14,984: WER debug example
  GT : how does it keep the cost down
  PR : hadn't conducted the
2026-01-10 22:14:30,214: Val batch 2000: PER (avg): 0.5916 CTC Loss (avg): 72.4518 WER(5gram): 93.35% (n=256) time: 141.965
2026-01-10 22:14:30,218: WER lens: avg_true_words=5.99 avg_pred_words=3.63 max_pred_words=9
2026-01-10 22:14:30,219: t15.2023.08.13 val PER: 0.5728
2026-01-10 22:14:30,219: t15.2023.08.18 val PER: 0.5641
2026-01-10 22:14:30,220: t15.2023.08.20 val PER: 0.5616
2026-01-10 22:14:30,220: t15.2023.08.25 val PER: 0.5542
2026-01-10 22:14:30,220: t15.2023.08.27 val PER: 0.6077
2026-01-10 22:14:30,220: t15.2023.09.01 val PER: 0.5463
2026-01-10 22:14:30,220: t15.2023.09.03 val PER: 0.5962
2026-01-10 22:14:30,221: t15.2023.09.24 val PER: 0.5862
2026-01-10 22:14:30,221: t15.2023.09.29 val PER: 0.5699
2026-01-10 22:14:30,221: t15.2023.10.01 val PER: 0.6044
2026-01-10 22:14:30,221: t15.2023.10.06 val PER: 0.5791
2026-01-10 22:14:30,221: t15.2023.10.08 val PER: 0.6076
2026-01-10 22:14:30,221: t15.2023.10.13 val PER: 0.6587
2026-01-10 22:14:30,221: t15.2023.10.15 val PER: 0.5939
2026-01-10 22:14:30,221: t15.2023.10.20 val PER: 0.5940
2026-01-10 22:14:30,221: t15.2023.10.22 val PER: 0.5657
2026-01-10 22:14:30,221: t15.2023.11.03 val PER: 0.5875
2026-01-10 22:14:30,221: t15.2023.11.04 val PER: 0.4266
2026-01-10 22:14:30,221: t15.2023.11.17 val PER: 0.4899
2026-01-10 22:14:30,221: t15.2023.11.19 val PER: 0.4870
2026-01-10 22:14:30,221: t15.2023.11.26 val PER: 0.6333
2026-01-10 22:14:30,221: t15.2023.12.03 val PER: 0.6008
2026-01-10 22:14:30,221: t15.2023.12.08 val PER: 0.5826
2026-01-10 22:14:30,222: t15.2023.12.10 val PER: 0.5861
2026-01-10 22:14:30,222: t15.2023.12.17 val PER: 0.5665
2026-01-10 22:14:30,222: t15.2023.12.29 val PER: 0.5978
2026-01-10 22:14:30,222: t15.2024.02.25 val PER: 0.5843
2026-01-10 22:14:30,222: t15.2024.03.08 val PER: 0.6159
2026-01-10 22:14:30,222: t15.2024.03.15 val PER: 0.5991
2026-01-10 22:14:30,222: t15.2024.03.17 val PER: 0.5872
2026-01-10 22:14:30,222: t15.2024.05.10 val PER: 0.5854
2026-01-10 22:14:30,222: t15.2024.06.14 val PER: 0.5615
2026-01-10 22:14:30,222: t15.2024.07.19 val PER: 0.6407
2026-01-10 22:14:30,222: t15.2024.07.21 val PER: 0.5766
2026-01-10 22:14:30,222: t15.2024.07.28 val PER: 0.5779
2026-01-10 22:14:30,222: t15.2025.01.10 val PER: 0.6543
2026-01-10 22:14:30,222: t15.2025.01.12 val PER: 0.6051
2026-01-10 22:14:30,222: t15.2025.03.14 val PER: 0.6583
2026-01-10 22:14:30,223: t15.2025.03.16 val PER: 0.6283
2026-01-10 22:14:30,223: t15.2025.03.30 val PER: 0.6747
2026-01-10 22:14:30,223: t15.2025.04.13 val PER: 0.6063
2026-01-10 22:14:30,224: New best val WER(5gram) 97.85% --> 93.35%
2026-01-10 22:14:30,376: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_2000
2026-01-10 22:14:47,292: Train batch 2200: loss: 58.95 grad norm: 48.52 time: 0.065
2026-01-10 22:15:04,488: Train batch 2400: loss: 59.14 grad norm: 51.25 time: 0.057
2026-01-10 22:15:13,022: Running test after training batch: 2500
2026-01-10 22:15:13,124: WER debug GT example: You can see the code at this point as well.
2026-01-10 22:15:19,001: WER debug example
  GT : you can see the code at this point as well
  PR : here he doesn't hit despite his awareness
2026-01-10 22:15:19,536: WER debug example
  GT : how does it keep the cost down
  PR : had assisted the
2026-01-10 22:17:11,963: Val batch 2500: PER (avg): 0.5592 CTC Loss (avg): 64.2473 WER(5gram): 91.66% (n=256) time: 118.941
2026-01-10 22:17:11,966: WER lens: avg_true_words=5.99 avg_pred_words=3.96 max_pred_words=9
2026-01-10 22:17:11,967: t15.2023.08.13 val PER: 0.5478
2026-01-10 22:17:11,968: t15.2023.08.18 val PER: 0.5231
2026-01-10 22:17:11,968: t15.2023.08.20 val PER: 0.5258
2026-01-10 22:17:11,969: t15.2023.08.25 val PER: 0.5151
2026-01-10 22:17:11,969: t15.2023.08.27 val PER: 0.5916
2026-01-10 22:17:11,969: t15.2023.09.01 val PER: 0.5114
2026-01-10 22:17:11,969: t15.2023.09.03 val PER: 0.5713
2026-01-10 22:17:11,969: t15.2023.09.24 val PER: 0.5558
2026-01-10 22:17:11,969: t15.2023.09.29 val PER: 0.5329
2026-01-10 22:17:11,969: t15.2023.10.01 val PER: 0.5812
2026-01-10 22:17:11,969: t15.2023.10.06 val PER: 0.5307
2026-01-10 22:17:11,969: t15.2023.10.08 val PER: 0.5873
2026-01-10 22:17:11,969: t15.2023.10.13 val PER: 0.6439
2026-01-10 22:17:11,969: t15.2023.10.15 val PER: 0.5537
2026-01-10 22:17:11,969: t15.2023.10.20 val PER: 0.5805
2026-01-10 22:17:11,969: t15.2023.10.22 val PER: 0.5501
2026-01-10 22:17:11,969: t15.2023.11.03 val PER: 0.5414
2026-01-10 22:17:11,969: t15.2023.11.04 val PER: 0.3413
2026-01-10 22:17:11,969: t15.2023.11.17 val PER: 0.4541
2026-01-10 22:17:11,970: t15.2023.11.19 val PER: 0.4551
2026-01-10 22:17:11,970: t15.2023.11.26 val PER: 0.6094
2026-01-10 22:17:11,970: t15.2023.12.03 val PER: 0.5546
2026-01-10 22:17:11,970: t15.2023.12.08 val PER: 0.5539
2026-01-10 22:17:11,970: t15.2023.12.10 val PER: 0.5453
2026-01-10 22:17:11,970: t15.2023.12.17 val PER: 0.5405
2026-01-10 22:17:11,970: t15.2023.12.29 val PER: 0.5697
2026-01-10 22:17:11,970: t15.2024.02.25 val PER: 0.5548
2026-01-10 22:17:11,970: t15.2024.03.08 val PER: 0.5989
2026-01-10 22:17:11,970: t15.2024.03.15 val PER: 0.5697
2026-01-10 22:17:11,970: t15.2024.03.17 val PER: 0.5397
2026-01-10 22:17:11,971: t15.2024.05.10 val PER: 0.5587
2026-01-10 22:17:11,971: t15.2024.06.14 val PER: 0.5174
2026-01-10 22:17:11,971: t15.2024.07.19 val PER: 0.6084
2026-01-10 22:17:11,971: t15.2024.07.21 val PER: 0.5221
2026-01-10 22:17:11,971: t15.2024.07.28 val PER: 0.5471
2026-01-10 22:17:11,971: t15.2025.01.10 val PER: 0.6350
2026-01-10 22:17:11,971: t15.2025.01.12 val PER: 0.5735
2026-01-10 22:17:11,971: t15.2025.03.14 val PER: 0.6095
2026-01-10 22:17:11,971: t15.2025.03.16 val PER: 0.6165
2026-01-10 22:17:11,971: t15.2025.03.30 val PER: 0.6483
2026-01-10 22:17:11,971: t15.2025.04.13 val PER: 0.5849
2026-01-10 22:17:11,972: New best val WER(5gram) 93.35% --> 91.66%
2026-01-10 22:17:12,116: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_2500
2026-01-10 22:17:20,792: Train batch 2600: loss: 62.29 grad norm: 58.19 time: 0.060
2026-01-10 22:17:37,891: Train batch 2800: loss: 45.49 grad norm: 54.50 time: 0.085
2026-01-10 22:17:55,030: Train batch 3000: loss: 52.22 grad norm: 67.43 time: 0.087
2026-01-10 22:17:55,030: Running test after training batch: 3000
2026-01-10 22:17:55,131: WER debug GT example: You can see the code at this point as well.
2026-01-10 22:18:00,898: WER debug example
  GT : you can see the code at this point as well
  PR : here he doesn't hit this button will
2026-01-10 22:18:01,396: WER debug example
  GT : how does it keep the cost down
  PR : hadn't submitted the study
2026-01-10 22:19:46,106: Val batch 3000: PER (avg): 0.5357 CTC Loss (avg): 58.0014 WER(5gram): 90.55% (n=256) time: 111.075
2026-01-10 22:19:46,109: WER lens: avg_true_words=5.99 avg_pred_words=4.20 max_pred_words=10
2026-01-10 22:19:46,110: t15.2023.08.13 val PER: 0.5146
2026-01-10 22:19:46,110: t15.2023.08.18 val PER: 0.5038
2026-01-10 22:19:46,110: t15.2023.08.20 val PER: 0.4980
2026-01-10 22:19:46,110: t15.2023.08.25 val PER: 0.4864
2026-01-10 22:19:46,111: t15.2023.08.27 val PER: 0.5675
2026-01-10 22:19:46,111: t15.2023.09.01 val PER: 0.4903
2026-01-10 22:19:46,111: t15.2023.09.03 val PER: 0.5475
2026-01-10 22:19:46,111: t15.2023.09.24 val PER: 0.5182
2026-01-10 22:19:46,111: t15.2023.09.29 val PER: 0.5144
2026-01-10 22:19:46,111: t15.2023.10.01 val PER: 0.5515
2026-01-10 22:19:46,111: t15.2023.10.06 val PER: 0.4962
2026-01-10 22:19:46,111: t15.2023.10.08 val PER: 0.5737
2026-01-10 22:19:46,111: t15.2023.10.13 val PER: 0.6137
2026-01-10 22:19:46,111: t15.2023.10.15 val PER: 0.5419
2026-01-10 22:19:46,111: t15.2023.10.20 val PER: 0.5336
2026-01-10 22:19:46,111: t15.2023.10.22 val PER: 0.5189
2026-01-10 22:19:46,111: t15.2023.11.03 val PER: 0.5265
2026-01-10 22:19:46,112: t15.2023.11.04 val PER: 0.3174
2026-01-10 22:19:46,112: t15.2023.11.17 val PER: 0.3888
2026-01-10 22:19:46,112: t15.2023.11.19 val PER: 0.4232
2026-01-10 22:19:46,112: t15.2023.11.26 val PER: 0.5870
2026-01-10 22:19:46,112: t15.2023.12.03 val PER: 0.5410
2026-01-10 22:19:46,112: t15.2023.12.08 val PER: 0.5300
2026-01-10 22:19:46,112: t15.2023.12.10 val PER: 0.5335
2026-01-10 22:19:46,112: t15.2023.12.17 val PER: 0.5218
2026-01-10 22:19:46,112: t15.2023.12.29 val PER: 0.5402
2026-01-10 22:19:46,112: t15.2024.02.25 val PER: 0.5084
2026-01-10 22:19:46,112: t15.2024.03.08 val PER: 0.5704
2026-01-10 22:19:46,112: t15.2024.03.15 val PER: 0.5497
2026-01-10 22:19:46,112: t15.2024.03.17 val PER: 0.5314
2026-01-10 22:19:46,112: t15.2024.05.10 val PER: 0.5409
2026-01-10 22:19:46,113: t15.2024.06.14 val PER: 0.5000
2026-01-10 22:19:46,113: t15.2024.07.19 val PER: 0.5959
2026-01-10 22:19:46,113: t15.2024.07.21 val PER: 0.4786
2026-01-10 22:19:46,113: t15.2024.07.28 val PER: 0.5375
2026-01-10 22:19:46,113: t15.2025.01.10 val PER: 0.6047
2026-01-10 22:19:46,113: t15.2025.01.12 val PER: 0.5543
2026-01-10 22:19:46,113: t15.2025.03.14 val PER: 0.6021
2026-01-10 22:19:46,113: t15.2025.03.16 val PER: 0.5942
2026-01-10 22:19:46,113: t15.2025.03.30 val PER: 0.6218
2026-01-10 22:19:46,113: t15.2025.04.13 val PER: 0.5635
2026-01-10 22:19:46,114: New best val WER(5gram) 91.66% --> 90.55%
2026-01-10 22:19:46,264: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_3000
2026-01-10 22:20:03,589: Train batch 3200: loss: 44.69 grad norm: 68.69 time: 0.081
2026-01-10 22:20:21,150: Train batch 3400: loss: 36.49 grad norm: 51.65 time: 0.053
2026-01-10 22:20:30,040: Running test after training batch: 3500
2026-01-10 22:20:30,156: WER debug GT example: You can see the code at this point as well.
2026-01-10 22:20:35,922: WER debug example
  GT : you can see the code at this point as well
  PR : here he discusses the had hidden this item will have
2026-01-10 22:20:36,400: WER debug example
  GT : how does it keep the cost down
  PR : home assistant the
2026-01-10 22:22:17,422: Val batch 3500: PER (avg): 0.5075 CTC Loss (avg): 52.8430 WER(5gram): 90.16% (n=256) time: 107.381
2026-01-10 22:22:17,426: WER lens: avg_true_words=5.99 avg_pred_words=4.25 max_pred_words=10
2026-01-10 22:22:17,427: t15.2023.08.13 val PER: 0.5000
2026-01-10 22:22:17,427: t15.2023.08.18 val PER: 0.4736
2026-01-10 22:22:17,427: t15.2023.08.20 val PER: 0.4527
2026-01-10 22:22:17,427: t15.2023.08.25 val PER: 0.4307
2026-01-10 22:22:17,427: t15.2023.08.27 val PER: 0.5338
2026-01-10 22:22:17,427: t15.2023.09.01 val PER: 0.4554
2026-01-10 22:22:17,427: t15.2023.09.03 val PER: 0.5273
2026-01-10 22:22:17,427: t15.2023.09.24 val PER: 0.4818
2026-01-10 22:22:17,427: t15.2023.09.29 val PER: 0.4735
2026-01-10 22:22:17,427: t15.2023.10.01 val PER: 0.5205
2026-01-10 22:22:17,427: t15.2023.10.06 val PER: 0.4779
2026-01-10 22:22:17,428: t15.2023.10.08 val PER: 0.5778
2026-01-10 22:22:17,428: t15.2023.10.13 val PER: 0.5912
2026-01-10 22:22:17,428: t15.2023.10.15 val PER: 0.5096
2026-01-10 22:22:17,428: t15.2023.10.20 val PER: 0.5168
2026-01-10 22:22:17,428: t15.2023.10.22 val PER: 0.4978
2026-01-10 22:22:17,428: t15.2023.11.03 val PER: 0.4891
2026-01-10 22:22:17,428: t15.2023.11.04 val PER: 0.2867
2026-01-10 22:22:17,428: t15.2023.11.17 val PER: 0.3748
2026-01-10 22:22:17,428: t15.2023.11.19 val PER: 0.3553
2026-01-10 22:22:17,428: t15.2023.11.26 val PER: 0.5703
2026-01-10 22:22:17,428: t15.2023.12.03 val PER: 0.5200
2026-01-10 22:22:17,429: t15.2023.12.08 val PER: 0.5080
2026-01-10 22:22:17,429: t15.2023.12.10 val PER: 0.5007
2026-01-10 22:22:17,429: t15.2023.12.17 val PER: 0.4834
2026-01-10 22:22:17,429: t15.2023.12.29 val PER: 0.5230
2026-01-10 22:22:17,429: t15.2024.02.25 val PER: 0.4761
2026-01-10 22:22:17,429: t15.2024.03.08 val PER: 0.5448
2026-01-10 22:22:17,429: t15.2024.03.15 val PER: 0.5116
2026-01-10 22:22:17,429: t15.2024.03.17 val PER: 0.4937
2026-01-10 22:22:17,429: t15.2024.05.10 val PER: 0.5007
2026-01-10 22:22:17,429: t15.2024.06.14 val PER: 0.4732
2026-01-10 22:22:17,429: t15.2024.07.19 val PER: 0.5709
2026-01-10 22:22:17,430: t15.2024.07.21 val PER: 0.4600
2026-01-10 22:22:17,430: t15.2024.07.28 val PER: 0.5103
2026-01-10 22:22:17,430: t15.2025.01.10 val PER: 0.5964
2026-01-10 22:22:17,430: t15.2025.01.12 val PER: 0.5189
2026-01-10 22:22:17,430: t15.2025.03.14 val PER: 0.5695
2026-01-10 22:22:17,430: t15.2025.03.16 val PER: 0.5602
2026-01-10 22:22:17,430: t15.2025.03.30 val PER: 0.6184
2026-01-10 22:22:17,430: t15.2025.04.13 val PER: 0.5492
2026-01-10 22:22:17,431: New best val WER(5gram) 90.55% --> 90.16%
2026-01-10 22:22:17,584: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_3500
2026-01-10 22:22:26,062: Train batch 3600: loss: 40.04 grad norm: 62.61 time: 0.072
2026-01-10 22:22:43,536: Train batch 3800: loss: 47.32 grad norm: 60.47 time: 0.072
2026-01-10 22:23:01,502: Train batch 4000: loss: 35.22 grad norm: 53.96 time: 0.060
2026-01-10 22:23:01,502: Running test after training batch: 4000
2026-01-10 22:23:01,650: WER debug GT example: You can see the code at this point as well.
2026-01-10 22:23:07,648: WER debug example
  GT : you can see the code at this point as well
  PR : here had exceeded the credit system will
2026-01-10 22:23:08,220: WER debug example
  GT : how does it keep the cost down
  PR : hammocks indicating the
2026-01-10 22:24:41,521: Val batch 4000: PER (avg): 0.4863 CTC Loss (avg): 48.3960 WER(5gram): 92.50% (n=256) time: 100.019
2026-01-10 22:24:41,525: WER lens: avg_true_words=5.99 avg_pred_words=4.57 max_pred_words=10
2026-01-10 22:24:41,525: t15.2023.08.13 val PER: 0.4761
2026-01-10 22:24:41,525: t15.2023.08.18 val PER: 0.4560
2026-01-10 22:24:41,526: t15.2023.08.20 val PER: 0.4432
2026-01-10 22:24:41,526: t15.2023.08.25 val PER: 0.4142
2026-01-10 22:24:41,526: t15.2023.08.27 val PER: 0.5257
2026-01-10 22:24:41,526: t15.2023.09.01 val PER: 0.4416
2026-01-10 22:24:41,526: t15.2023.09.03 val PER: 0.4846
2026-01-10 22:24:41,526: t15.2023.09.24 val PER: 0.4624
2026-01-10 22:24:41,526: t15.2023.09.29 val PER: 0.4512
2026-01-10 22:24:41,526: t15.2023.10.01 val PER: 0.5079
2026-01-10 22:24:41,526: t15.2023.10.06 val PER: 0.4435
2026-01-10 22:24:41,526: t15.2023.10.08 val PER: 0.5548
2026-01-10 22:24:41,526: t15.2023.10.13 val PER: 0.5694
2026-01-10 22:24:41,526: t15.2023.10.15 val PER: 0.4918
2026-01-10 22:24:41,526: t15.2023.10.20 val PER: 0.5034
2026-01-10 22:24:41,526: t15.2023.10.22 val PER: 0.4733
2026-01-10 22:24:41,526: t15.2023.11.03 val PER: 0.4613
2026-01-10 22:24:41,527: t15.2023.11.04 val PER: 0.2594
2026-01-10 22:24:41,527: t15.2023.11.17 val PER: 0.3421
2026-01-10 22:24:41,527: t15.2023.11.19 val PER: 0.3473
2026-01-10 22:24:41,527: t15.2023.11.26 val PER: 0.5413
2026-01-10 22:24:41,527: t15.2023.12.03 val PER: 0.4968
2026-01-10 22:24:41,527: t15.2023.12.08 val PER: 0.4913
2026-01-10 22:24:41,527: t15.2023.12.10 val PER: 0.4757
2026-01-10 22:24:41,528: t15.2023.12.17 val PER: 0.4719
2026-01-10 22:24:41,528: t15.2023.12.29 val PER: 0.5038
2026-01-10 22:24:41,528: t15.2024.02.25 val PER: 0.4649
2026-01-10 22:24:41,528: t15.2024.03.08 val PER: 0.5220
2026-01-10 22:24:41,528: t15.2024.03.15 val PER: 0.5028
2026-01-10 22:24:41,528: t15.2024.03.17 val PER: 0.4686
2026-01-10 22:24:41,528: t15.2024.05.10 val PER: 0.4710
2026-01-10 22:24:41,528: t15.2024.06.14 val PER: 0.4621
2026-01-10 22:24:41,529: t15.2024.07.19 val PER: 0.5419
2026-01-10 22:24:41,529: t15.2024.07.21 val PER: 0.4248
2026-01-10 22:24:41,529: t15.2024.07.28 val PER: 0.4963
2026-01-10 22:24:41,529: t15.2025.01.10 val PER: 0.5799
2026-01-10 22:24:41,529: t15.2025.01.12 val PER: 0.4973
2026-01-10 22:24:41,529: t15.2025.03.14 val PER: 0.5651
2026-01-10 22:24:41,529: t15.2025.03.16 val PER: 0.5262
2026-01-10 22:24:41,529: t15.2025.03.30 val PER: 0.5736
2026-01-10 22:24:41,529: t15.2025.04.13 val PER: 0.5364
2026-01-10 22:24:41,673: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_4000
2026-01-10 22:24:58,799: Train batch 4200: loss: 37.84 grad norm: 69.42 time: 0.083
2026-01-10 22:25:15,863: Train batch 4400: loss: 29.69 grad norm: 56.44 time: 0.070
2026-01-10 22:25:24,519: Running test after training batch: 4500
2026-01-10 22:25:24,622: WER debug GT example: You can see the code at this point as well.
2026-01-10 22:25:30,341: WER debug example
  GT : you can see the code at this point as well
  PR : here had succeeded the had hidden this item will have
2026-01-10 22:25:30,828: WER debug example
  GT : how does it keep the cost down
  PR : hmm hits hits hits the
2026-01-10 22:27:03,112: Val batch 4500: PER (avg): 0.4684 CTC Loss (avg): 44.9048 WER(5gram): 89.50% (n=256) time: 98.593
2026-01-10 22:27:03,116: WER lens: avg_true_words=5.99 avg_pred_words=4.69 max_pred_words=12
2026-01-10 22:27:03,116: t15.2023.08.13 val PER: 0.4553
2026-01-10 22:27:03,116: t15.2023.08.18 val PER: 0.4225
2026-01-10 22:27:03,116: t15.2023.08.20 val PER: 0.4186
2026-01-10 22:27:03,117: t15.2023.08.25 val PER: 0.4142
2026-01-10 22:27:03,117: t15.2023.08.27 val PER: 0.5096
2026-01-10 22:27:03,117: t15.2023.09.01 val PER: 0.4269
2026-01-10 22:27:03,117: t15.2023.09.03 val PER: 0.4751
2026-01-10 22:27:03,117: t15.2023.09.24 val PER: 0.4284
2026-01-10 22:27:03,117: t15.2023.09.29 val PER: 0.4410
2026-01-10 22:27:03,117: t15.2023.10.01 val PER: 0.4908
2026-01-10 22:27:03,117: t15.2023.10.06 val PER: 0.4112
2026-01-10 22:27:03,118: t15.2023.10.08 val PER: 0.5372
2026-01-10 22:27:03,118: t15.2023.10.13 val PER: 0.5648
2026-01-10 22:27:03,118: t15.2023.10.15 val PER: 0.4832
2026-01-10 22:27:03,118: t15.2023.10.20 val PER: 0.5000
2026-01-10 22:27:03,118: t15.2023.10.22 val PER: 0.4532
2026-01-10 22:27:03,118: t15.2023.11.03 val PER: 0.4593
2026-01-10 22:27:03,118: t15.2023.11.04 val PER: 0.2287
2026-01-10 22:27:03,118: t15.2023.11.17 val PER: 0.3157
2026-01-10 22:27:03,118: t15.2023.11.19 val PER: 0.3054
2026-01-10 22:27:03,118: t15.2023.11.26 val PER: 0.5406
2026-01-10 22:27:03,118: t15.2023.12.03 val PER: 0.4769
2026-01-10 22:27:03,137: t15.2023.12.08 val PER: 0.4574
2026-01-10 22:27:03,137: t15.2023.12.10 val PER: 0.4704
2026-01-10 22:27:03,137: t15.2023.12.17 val PER: 0.4418
2026-01-10 22:27:03,137: t15.2023.12.29 val PER: 0.4756
2026-01-10 22:27:03,138: t15.2024.02.25 val PER: 0.4059
2026-01-10 22:27:03,138: t15.2024.03.08 val PER: 0.5178
2026-01-10 22:27:03,138: t15.2024.03.15 val PER: 0.4866
2026-01-10 22:27:03,138: t15.2024.03.17 val PER: 0.4519
2026-01-10 22:27:03,138: t15.2024.05.10 val PER: 0.4695
2026-01-10 22:27:03,138: t15.2024.06.14 val PER: 0.4322
2026-01-10 22:27:03,138: t15.2024.07.19 val PER: 0.5208
2026-01-10 22:27:03,138: t15.2024.07.21 val PER: 0.4131
2026-01-10 22:27:03,138: t15.2024.07.28 val PER: 0.4816
2026-01-10 22:27:03,138: t15.2025.01.10 val PER: 0.5496
2026-01-10 22:27:03,138: t15.2025.01.12 val PER: 0.4827
2026-01-10 22:27:03,139: t15.2025.03.14 val PER: 0.5562
2026-01-10 22:27:03,139: t15.2025.03.16 val PER: 0.5079
2026-01-10 22:27:03,139: t15.2025.03.30 val PER: 0.5690
2026-01-10 22:27:03,139: t15.2025.04.13 val PER: 0.5107
2026-01-10 22:27:03,139: New best val WER(5gram) 90.16% --> 89.50%
2026-01-10 22:27:03,298: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_4500
2026-01-10 22:27:11,862: Train batch 4600: loss: 32.32 grad norm: 74.86 time: 0.068
2026-01-10 22:27:29,230: Train batch 4800: loss: 22.93 grad norm: 64.44 time: 0.068
2026-01-10 22:27:46,338: Train batch 5000: loss: 50.85 grad norm: 92.42 time: 0.071
2026-01-10 22:27:46,339: Running test after training batch: 5000
2026-01-10 22:27:46,458: WER debug GT example: You can see the code at this point as well.
2026-01-10 22:27:52,220: WER debug example
  GT : you can see the code at this point as well
  PR : here had exceeded the had hidden this item will have
2026-01-10 22:27:52,675: WER debug example
  GT : how does it keep the cost down
  PR : hmm hits hits hits the
2026-01-10 22:29:22,090: Val batch 5000: PER (avg): 0.4603 CTC Loss (avg): 41.1203 WER(5gram): 88.92% (n=256) time: 95.751
2026-01-10 22:29:22,094: WER lens: avg_true_words=5.99 avg_pred_words=4.96 max_pred_words=10
2026-01-10 22:29:22,094: t15.2023.08.13 val PER: 0.4699
2026-01-10 22:29:22,094: t15.2023.08.18 val PER: 0.4258
2026-01-10 22:29:22,095: t15.2023.08.20 val PER: 0.3956
2026-01-10 22:29:22,095: t15.2023.08.25 val PER: 0.3916
2026-01-10 22:29:22,095: t15.2023.08.27 val PER: 0.5016
2026-01-10 22:29:22,095: t15.2023.09.01 val PER: 0.4026
2026-01-10 22:29:22,095: t15.2023.09.03 val PER: 0.4786
2026-01-10 22:29:22,095: t15.2023.09.24 val PER: 0.4211
2026-01-10 22:29:22,095: t15.2023.09.29 val PER: 0.4250
2026-01-10 22:29:22,095: t15.2023.10.01 val PER: 0.4769
2026-01-10 22:29:22,095: t15.2023.10.06 val PER: 0.4166
2026-01-10 22:29:22,096: t15.2023.10.08 val PER: 0.5399
2026-01-10 22:29:22,096: t15.2023.10.13 val PER: 0.5547
2026-01-10 22:29:22,096: t15.2023.10.15 val PER: 0.4674
2026-01-10 22:29:22,096: t15.2023.10.20 val PER: 0.4832
2026-01-10 22:29:22,096: t15.2023.10.22 val PER: 0.4443
2026-01-10 22:29:22,096: t15.2023.11.03 val PER: 0.4464
2026-01-10 22:29:22,096: t15.2023.11.04 val PER: 0.2184
2026-01-10 22:29:22,096: t15.2023.11.17 val PER: 0.3235
2026-01-10 22:29:22,096: t15.2023.11.19 val PER: 0.3174
2026-01-10 22:29:22,096: t15.2023.11.26 val PER: 0.5217
2026-01-10 22:29:22,096: t15.2023.12.03 val PER: 0.4601
2026-01-10 22:29:22,096: t15.2023.12.08 val PER: 0.4547
2026-01-10 22:29:22,097: t15.2023.12.10 val PER: 0.4534
2026-01-10 22:29:22,097: t15.2023.12.17 val PER: 0.4439
2026-01-10 22:29:22,097: t15.2023.12.29 val PER: 0.4743
2026-01-10 22:29:22,097: t15.2024.02.25 val PER: 0.4115
2026-01-10 22:29:22,097: t15.2024.03.08 val PER: 0.4979
2026-01-10 22:29:22,097: t15.2024.03.15 val PER: 0.4797
2026-01-10 22:29:22,097: t15.2024.03.17 val PER: 0.4505
2026-01-10 22:29:22,097: t15.2024.05.10 val PER: 0.4785
2026-01-10 22:29:22,097: t15.2024.06.14 val PER: 0.4290
2026-01-10 22:29:22,097: t15.2024.07.19 val PER: 0.5142
2026-01-10 22:29:22,097: t15.2024.07.21 val PER: 0.4048
2026-01-10 22:29:22,097: t15.2024.07.28 val PER: 0.4684
2026-01-10 22:29:22,097: t15.2025.01.10 val PER: 0.5482
2026-01-10 22:29:22,098: t15.2025.01.12 val PER: 0.4734
2026-01-10 22:29:22,098: t15.2025.03.14 val PER: 0.5459
2026-01-10 22:29:22,098: t15.2025.03.16 val PER: 0.5013
2026-01-10 22:29:22,098: t15.2025.03.30 val PER: 0.5460
2026-01-10 22:29:22,098: t15.2025.04.13 val PER: 0.4979
2026-01-10 22:29:22,099: New best val WER(5gram) 89.50% --> 88.92%
2026-01-10 22:29:22,252: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_5000
2026-01-10 22:29:39,355: Train batch 5200: loss: 27.18 grad norm: 82.91 time: 0.060
2026-01-10 22:29:57,218: Train batch 5400: loss: 30.38 grad norm: 66.18 time: 0.075
2026-01-10 22:30:05,866: Running test after training batch: 5500
2026-01-10 22:30:06,015: WER debug GT example: You can see the code at this point as well.
2026-01-10 22:30:12,058: WER debug example
  GT : you can see the code at this point as well
  PR : the euro had exceeded the high-end handsets will have
2026-01-10 22:30:12,489: WER debug example
  GT : how does it keep the cost down
  PR : hmm hits hits hits the
2026-01-10 22:31:36,196: Val batch 5500: PER (avg): 0.4433 CTC Loss (avg): 38.1336 WER(5gram): 87.94% (n=256) time: 90.329
2026-01-10 22:31:36,199: WER lens: avg_true_words=5.99 avg_pred_words=4.99 max_pred_words=11
2026-01-10 22:31:36,199: t15.2023.08.13 val PER: 0.4179
2026-01-10 22:31:36,199: t15.2023.08.18 val PER: 0.3973
2026-01-10 22:31:36,200: t15.2023.08.20 val PER: 0.3971
2026-01-10 22:31:36,200: t15.2023.08.25 val PER: 0.3825
2026-01-10 22:31:36,200: t15.2023.08.27 val PER: 0.4727
2026-01-10 22:31:36,200: t15.2023.09.01 val PER: 0.3904
2026-01-10 22:31:36,200: t15.2023.09.03 val PER: 0.4608
2026-01-10 22:31:36,200: t15.2023.09.24 val PER: 0.4029
2026-01-10 22:31:36,200: t15.2023.09.29 val PER: 0.4071
2026-01-10 22:31:36,200: t15.2023.10.01 val PER: 0.4716
2026-01-10 22:31:36,200: t15.2023.10.06 val PER: 0.3983
2026-01-10 22:31:36,200: t15.2023.10.08 val PER: 0.5156
2026-01-10 22:31:36,200: t15.2023.10.13 val PER: 0.5400
2026-01-10 22:31:36,200: t15.2023.10.15 val PER: 0.4502
2026-01-10 22:31:36,201: t15.2023.10.20 val PER: 0.4631
2026-01-10 22:31:36,201: t15.2023.10.22 val PER: 0.4276
2026-01-10 22:31:36,201: t15.2023.11.03 val PER: 0.4369
2026-01-10 22:31:36,201: t15.2023.11.04 val PER: 0.1741
2026-01-10 22:31:36,201: t15.2023.11.17 val PER: 0.2908
2026-01-10 22:31:36,201: t15.2023.11.19 val PER: 0.2874
2026-01-10 22:31:36,201: t15.2023.11.26 val PER: 0.5188
2026-01-10 22:31:36,201: t15.2023.12.03 val PER: 0.4380
2026-01-10 22:31:36,202: t15.2023.12.08 val PER: 0.4361
2026-01-10 22:31:36,202: t15.2023.12.10 val PER: 0.4402
2026-01-10 22:31:36,202: t15.2023.12.17 val PER: 0.4272
2026-01-10 22:31:36,202: t15.2023.12.29 val PER: 0.4502
2026-01-10 22:31:36,202: t15.2024.02.25 val PER: 0.4045
2026-01-10 22:31:36,202: t15.2024.03.08 val PER: 0.4865
2026-01-10 22:31:36,202: t15.2024.03.15 val PER: 0.4503
2026-01-10 22:31:36,202: t15.2024.03.17 val PER: 0.4358
2026-01-10 22:31:36,202: t15.2024.05.10 val PER: 0.4473
2026-01-10 22:31:36,202: t15.2024.06.14 val PER: 0.4006
2026-01-10 22:31:36,203: t15.2024.07.19 val PER: 0.5016
2026-01-10 22:31:36,203: t15.2024.07.21 val PER: 0.3793
2026-01-10 22:31:36,203: t15.2024.07.28 val PER: 0.4537
2026-01-10 22:31:36,203: t15.2025.01.10 val PER: 0.5275
2026-01-10 22:31:36,203: t15.2025.01.12 val PER: 0.4688
2026-01-10 22:31:36,203: t15.2025.03.14 val PER: 0.5251
2026-01-10 22:31:36,203: t15.2025.03.16 val PER: 0.4935
2026-01-10 22:31:36,203: t15.2025.03.30 val PER: 0.5483
2026-01-10 22:31:36,203: t15.2025.04.13 val PER: 0.4950
2026-01-10 22:31:36,204: New best val WER(5gram) 88.92% --> 87.94%
2026-01-10 22:31:36,357: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_5500
2026-01-10 22:31:44,917: Train batch 5600: loss: 32.33 grad norm: 78.89 time: 0.067
2026-01-10 22:32:01,905: Train batch 5800: loss: 16.55 grad norm: 68.81 time: 0.086
2026-01-10 22:32:19,214: Train batch 6000: loss: 17.69 grad norm: 67.64 time: 0.053
2026-01-10 22:32:19,215: Running test after training batch: 6000
2026-01-10 22:32:19,348: WER debug GT example: You can see the code at this point as well.
2026-01-10 22:32:25,979: WER debug example
  GT : you can see the code at this point as well
  PR : hugh had exceeded the had had this fantasy world
2026-01-10 22:32:26,401: WER debug example
  GT : how does it keep the cost down
  PR : hmm hits hits hits the
2026-01-10 22:33:49,037: Val batch 6000: PER (avg): 0.4335 CTC Loss (avg): 35.6045 WER(5gram): 87.74% (n=256) time: 89.822
2026-01-10 22:33:49,040: WER lens: avg_true_words=5.99 avg_pred_words=5.08 max_pred_words=13
2026-01-10 22:33:49,041: t15.2023.08.13 val PER: 0.4335
2026-01-10 22:33:49,041: t15.2023.08.18 val PER: 0.3889
2026-01-10 22:33:49,041: t15.2023.08.20 val PER: 0.3709
2026-01-10 22:33:49,041: t15.2023.08.25 val PER: 0.3599
2026-01-10 22:33:49,041: t15.2023.08.27 val PER: 0.4550
2026-01-10 22:33:49,041: t15.2023.09.01 val PER: 0.3807
2026-01-10 22:33:49,041: t15.2023.09.03 val PER: 0.4596
2026-01-10 22:33:49,041: t15.2023.09.24 val PER: 0.3944
2026-01-10 22:33:49,042: t15.2023.09.29 val PER: 0.4097
2026-01-10 22:33:49,042: t15.2023.10.01 val PER: 0.4643
2026-01-10 22:33:49,042: t15.2023.10.06 val PER: 0.3875
2026-01-10 22:33:49,042: t15.2023.10.08 val PER: 0.5183
2026-01-10 22:33:49,042: t15.2023.10.13 val PER: 0.5299
2026-01-10 22:33:49,042: t15.2023.10.15 val PER: 0.4529
2026-01-10 22:33:49,042: t15.2023.10.20 val PER: 0.4966
2026-01-10 22:33:49,042: t15.2023.10.22 val PER: 0.4198
2026-01-10 22:33:49,042: t15.2023.11.03 val PER: 0.4308
2026-01-10 22:33:49,042: t15.2023.11.04 val PER: 0.1911
2026-01-10 22:33:49,042: t15.2023.11.17 val PER: 0.2768
2026-01-10 22:33:49,043: t15.2023.11.19 val PER: 0.2794
2026-01-10 22:33:49,043: t15.2023.11.26 val PER: 0.4978
2026-01-10 22:33:49,043: t15.2023.12.03 val PER: 0.4286
2026-01-10 22:33:49,043: t15.2023.12.08 val PER: 0.4281
2026-01-10 22:33:49,043: t15.2023.12.10 val PER: 0.4126
2026-01-10 22:33:49,043: t15.2023.12.17 val PER: 0.4127
2026-01-10 22:33:49,043: t15.2023.12.29 val PER: 0.4386
2026-01-10 22:33:49,043: t15.2024.02.25 val PER: 0.3876
2026-01-10 22:33:49,043: t15.2024.03.08 val PER: 0.4694
2026-01-10 22:33:49,043: t15.2024.03.15 val PER: 0.4472
2026-01-10 22:33:49,043: t15.2024.03.17 val PER: 0.4191
2026-01-10 22:33:49,043: t15.2024.05.10 val PER: 0.4324
2026-01-10 22:33:49,043: t15.2024.06.14 val PER: 0.3991
2026-01-10 22:33:49,044: t15.2024.07.19 val PER: 0.4806
2026-01-10 22:33:49,044: t15.2024.07.21 val PER: 0.3793
2026-01-10 22:33:49,044: t15.2024.07.28 val PER: 0.4485
2026-01-10 22:33:49,044: t15.2025.01.10 val PER: 0.5124
2026-01-10 22:33:49,044: t15.2025.01.12 val PER: 0.4480
2026-01-10 22:33:49,044: t15.2025.03.14 val PER: 0.5104
2026-01-10 22:33:49,044: t15.2025.03.16 val PER: 0.4882
2026-01-10 22:33:49,044: t15.2025.03.30 val PER: 0.5138
2026-01-10 22:33:49,045: t15.2025.04.13 val PER: 0.4807
2026-01-10 22:33:49,045: New best val WER(5gram) 87.94% --> 87.74%
2026-01-10 22:33:49,202: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_6000
2026-01-10 22:34:06,248: Train batch 6200: loss: 22.47 grad norm: 85.42 time: 0.075
2026-01-10 22:34:23,261: Train batch 6400: loss: 31.55 grad norm: 66.71 time: 0.068
2026-01-10 22:34:31,722: Running test after training batch: 6500
2026-01-10 22:34:31,849: WER debug GT example: You can see the code at this point as well.
2026-01-10 22:34:37,611: WER debug example
  GT : you can see the code at this point as well
  PR : hugh had exceeded the hot hits points will
2026-01-10 22:34:37,982: WER debug example
  GT : how does it keep the cost down
  PR : the hardest hit hits the
2026-01-10 22:35:57,043: Val batch 6500: PER (avg): 0.4186 CTC Loss (avg): 33.3038 WER(5gram): 86.18% (n=256) time: 85.320
2026-01-10 22:35:57,047: WER lens: avg_true_words=5.99 avg_pred_words=5.03 max_pred_words=11
2026-01-10 22:35:57,048: t15.2023.08.13 val PER: 0.4127
2026-01-10 22:35:57,048: t15.2023.08.18 val PER: 0.3738
2026-01-10 22:35:57,048: t15.2023.08.20 val PER: 0.3638
2026-01-10 22:35:57,048: t15.2023.08.25 val PER: 0.3569
2026-01-10 22:35:57,048: t15.2023.08.27 val PER: 0.4469
2026-01-10 22:35:57,048: t15.2023.09.01 val PER: 0.3612
2026-01-10 22:35:57,048: t15.2023.09.03 val PER: 0.4359
2026-01-10 22:35:57,048: t15.2023.09.24 val PER: 0.3823
2026-01-10 22:35:57,048: t15.2023.09.29 val PER: 0.3918
2026-01-10 22:35:57,048: t15.2023.10.01 val PER: 0.4538
2026-01-10 22:35:57,048: t15.2023.10.06 val PER: 0.3628
2026-01-10 22:35:57,048: t15.2023.10.08 val PER: 0.4953
2026-01-10 22:35:57,049: t15.2023.10.13 val PER: 0.5159
2026-01-10 22:35:57,049: t15.2023.10.15 val PER: 0.4291
2026-01-10 22:35:57,049: t15.2023.10.20 val PER: 0.4664
2026-01-10 22:35:57,049: t15.2023.10.22 val PER: 0.4042
2026-01-10 22:35:57,049: t15.2023.11.03 val PER: 0.4166
2026-01-10 22:35:57,049: t15.2023.11.04 val PER: 0.1672
2026-01-10 22:35:57,049: t15.2023.11.17 val PER: 0.2659
2026-01-10 22:35:57,049: t15.2023.11.19 val PER: 0.2715
2026-01-10 22:35:57,050: t15.2023.11.26 val PER: 0.4797
2026-01-10 22:35:57,050: t15.2023.12.03 val PER: 0.4170
2026-01-10 22:35:57,050: t15.2023.12.08 val PER: 0.4061
2026-01-10 22:35:57,050: t15.2023.12.10 val PER: 0.4100
2026-01-10 22:35:57,051: t15.2023.12.17 val PER: 0.4116
2026-01-10 22:35:57,051: t15.2023.12.29 val PER: 0.4331
2026-01-10 22:35:57,051: t15.2024.02.25 val PER: 0.3610
2026-01-10 22:35:57,051: t15.2024.03.08 val PER: 0.4595
2026-01-10 22:35:57,051: t15.2024.03.15 val PER: 0.4315
2026-01-10 22:35:57,051: t15.2024.03.17 val PER: 0.4114
2026-01-10 22:35:57,051: t15.2024.05.10 val PER: 0.4160
2026-01-10 22:35:57,051: t15.2024.06.14 val PER: 0.3896
2026-01-10 22:35:57,051: t15.2024.07.19 val PER: 0.4522
2026-01-10 22:35:57,051: t15.2024.07.21 val PER: 0.3703
2026-01-10 22:35:57,051: t15.2024.07.28 val PER: 0.4184
2026-01-10 22:35:57,052: t15.2025.01.10 val PER: 0.5083
2026-01-10 22:35:57,052: t15.2025.01.12 val PER: 0.4380
2026-01-10 22:35:57,052: t15.2025.03.14 val PER: 0.5118
2026-01-10 22:35:57,052: t15.2025.03.16 val PER: 0.4555
2026-01-10 22:35:57,052: t15.2025.03.30 val PER: 0.5092
2026-01-10 22:35:57,052: t15.2025.04.13 val PER: 0.4693
2026-01-10 22:35:57,052: New best val WER(5gram) 87.74% --> 86.18%
2026-01-10 22:35:57,207: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_6500
2026-01-10 22:36:05,630: Train batch 6600: loss: 14.82 grad norm: 50.22 time: 0.050
2026-01-10 22:36:22,971: Train batch 6800: loss: 24.44 grad norm: 56.93 time: 0.053
2026-01-10 22:36:40,296: Train batch 7000: loss: 21.73 grad norm: 69.00 time: 0.066
2026-01-10 22:36:40,297: Running test after training batch: 7000
2026-01-10 22:36:40,448: WER debug GT example: You can see the code at this point as well.
2026-01-10 22:36:46,236: WER debug example
  GT : you can see the code at this point as well
  PR : hugh had exceeded the could hit this point
2026-01-10 22:36:46,637: WER debug example
  GT : how does it keep the cost down
  PR : how modest it hit the
2026-01-10 22:38:11,008: Val batch 7000: PER (avg): 0.4126 CTC Loss (avg): 31.0098 WER(5gram): 85.92% (n=256) time: 90.711
2026-01-10 22:38:11,011: WER lens: avg_true_words=5.99 avg_pred_words=5.05 max_pred_words=11
2026-01-10 22:38:11,011: t15.2023.08.13 val PER: 0.3971
2026-01-10 22:38:11,012: t15.2023.08.18 val PER: 0.3713
2026-01-10 22:38:11,012: t15.2023.08.20 val PER: 0.3566
2026-01-10 22:38:11,012: t15.2023.08.25 val PER: 0.3449
2026-01-10 22:38:11,012: t15.2023.08.27 val PER: 0.4486
2026-01-10 22:38:11,012: t15.2023.09.01 val PER: 0.3425
2026-01-10 22:38:11,012: t15.2023.09.03 val PER: 0.4359
2026-01-10 22:38:11,012: t15.2023.09.24 val PER: 0.3786
2026-01-10 22:38:11,012: t15.2023.09.29 val PER: 0.3784
2026-01-10 22:38:11,012: t15.2023.10.01 val PER: 0.4524
2026-01-10 22:38:11,012: t15.2023.10.06 val PER: 0.3541
2026-01-10 22:38:11,012: t15.2023.10.08 val PER: 0.4899
2026-01-10 22:38:11,012: t15.2023.10.13 val PER: 0.5097
2026-01-10 22:38:11,013: t15.2023.10.15 val PER: 0.4305
2026-01-10 22:38:11,013: t15.2023.10.20 val PER: 0.4698
2026-01-10 22:38:11,013: t15.2023.10.22 val PER: 0.3964
2026-01-10 22:38:11,013: t15.2023.11.03 val PER: 0.4084
2026-01-10 22:38:11,013: t15.2023.11.04 val PER: 0.1672
2026-01-10 22:38:11,013: t15.2023.11.17 val PER: 0.2675
2026-01-10 22:38:11,013: t15.2023.11.19 val PER: 0.2754
2026-01-10 22:38:11,013: t15.2023.11.26 val PER: 0.4717
2026-01-10 22:38:11,013: t15.2023.12.03 val PER: 0.3960
2026-01-10 22:38:11,013: t15.2023.12.08 val PER: 0.4008
2026-01-10 22:38:11,013: t15.2023.12.10 val PER: 0.3942
2026-01-10 22:38:11,013: t15.2023.12.17 val PER: 0.4012
2026-01-10 22:38:11,014: t15.2023.12.29 val PER: 0.4166
2026-01-10 22:38:11,014: t15.2024.02.25 val PER: 0.3820
2026-01-10 22:38:11,014: t15.2024.03.08 val PER: 0.4452
2026-01-10 22:38:11,014: t15.2024.03.15 val PER: 0.4259
2026-01-10 22:38:11,014: t15.2024.03.17 val PER: 0.4031
2026-01-10 22:38:11,014: t15.2024.05.10 val PER: 0.4160
2026-01-10 22:38:11,014: t15.2024.06.14 val PER: 0.3754
2026-01-10 22:38:11,014: t15.2024.07.19 val PER: 0.4489
2026-01-10 22:38:11,014: t15.2024.07.21 val PER: 0.3655
2026-01-10 22:38:11,014: t15.2024.07.28 val PER: 0.4265
2026-01-10 22:38:11,014: t15.2025.01.10 val PER: 0.5028
2026-01-10 22:38:11,015: t15.2025.01.12 val PER: 0.4419
2026-01-10 22:38:11,015: t15.2025.03.14 val PER: 0.5104
2026-01-10 22:38:11,015: t15.2025.03.16 val PER: 0.4555
2026-01-10 22:38:11,015: t15.2025.03.30 val PER: 0.5011
2026-01-10 22:38:11,015: t15.2025.04.13 val PER: 0.4522
2026-01-10 22:38:11,016: New best val WER(5gram) 86.18% --> 85.92%
2026-01-10 22:38:11,172: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_7000
2026-01-10 22:38:28,423: Train batch 7200: loss: 18.49 grad norm: 78.66 time: 0.083
2026-01-10 22:38:45,451: Train batch 7400: loss: 20.16 grad norm: 69.80 time: 0.080
2026-01-10 22:38:54,032: Running test after training batch: 7500
2026-01-10 22:38:54,147: WER debug GT example: You can see the code at this point as well.
2026-01-10 22:38:59,922: WER debug example
  GT : you can see the code at this point as well
  PR : hugh had exceeded the had hit this point
2026-01-10 22:39:00,251: WER debug example
  GT : how does it keep the cost down
  PR : anaheim ducks hit hits the
2026-01-10 22:40:21,033: Val batch 7500: PER (avg): 0.3983 CTC Loss (avg): 29.2955 WER(5gram): 85.53% (n=256) time: 87.000
2026-01-10 22:40:21,034: WER lens: avg_true_words=5.99 avg_pred_words=5.13 max_pred_words=12
2026-01-10 22:40:21,035: t15.2023.08.13 val PER: 0.3909
2026-01-10 22:40:21,035: t15.2023.08.18 val PER: 0.3562
2026-01-10 22:40:21,035: t15.2023.08.20 val PER: 0.3400
2026-01-10 22:40:21,035: t15.2023.08.25 val PER: 0.3313
2026-01-10 22:40:21,035: t15.2023.08.27 val PER: 0.4260
2026-01-10 22:40:21,036: t15.2023.09.01 val PER: 0.3304
2026-01-10 22:40:21,036: t15.2023.09.03 val PER: 0.4181
2026-01-10 22:40:21,036: t15.2023.09.24 val PER: 0.3617
2026-01-10 22:40:21,036: t15.2023.09.29 val PER: 0.3618
2026-01-10 22:40:21,036: t15.2023.10.01 val PER: 0.4260
2026-01-10 22:40:21,036: t15.2023.10.06 val PER: 0.3531
2026-01-10 22:40:21,036: t15.2023.10.08 val PER: 0.4939
2026-01-10 22:40:21,036: t15.2023.10.13 val PER: 0.4919
2026-01-10 22:40:21,036: t15.2023.10.15 val PER: 0.4047
2026-01-10 22:40:21,036: t15.2023.10.20 val PER: 0.4430
2026-01-10 22:40:21,036: t15.2023.10.22 val PER: 0.3786
2026-01-10 22:40:21,036: t15.2023.11.03 val PER: 0.3976
2026-01-10 22:40:21,037: t15.2023.11.04 val PER: 0.1638
2026-01-10 22:40:21,037: t15.2023.11.17 val PER: 0.2519
2026-01-10 22:40:21,037: t15.2023.11.19 val PER: 0.2315
2026-01-10 22:40:21,037: t15.2023.11.26 val PER: 0.4522
2026-01-10 22:40:21,037: t15.2023.12.03 val PER: 0.4023
2026-01-10 22:40:21,037: t15.2023.12.08 val PER: 0.3995
2026-01-10 22:40:21,037: t15.2023.12.10 val PER: 0.3706
2026-01-10 22:40:21,037: t15.2023.12.17 val PER: 0.3898
2026-01-10 22:40:21,038: t15.2023.12.29 val PER: 0.4097
2026-01-10 22:40:21,038: t15.2024.02.25 val PER: 0.3525
2026-01-10 22:40:21,038: t15.2024.03.08 val PER: 0.4481
2026-01-10 22:40:21,038: t15.2024.03.15 val PER: 0.4140
2026-01-10 22:40:21,038: t15.2024.03.17 val PER: 0.3954
2026-01-10 22:40:21,038: t15.2024.05.10 val PER: 0.3997
2026-01-10 22:40:21,038: t15.2024.06.14 val PER: 0.3801
2026-01-10 22:40:21,038: t15.2024.07.19 val PER: 0.4443
2026-01-10 22:40:21,039: t15.2024.07.21 val PER: 0.3379
2026-01-10 22:40:21,039: t15.2024.07.28 val PER: 0.4000
2026-01-10 22:40:21,039: t15.2025.01.10 val PER: 0.4931
2026-01-10 22:40:21,039: t15.2025.01.12 val PER: 0.4196
2026-01-10 22:40:21,039: t15.2025.03.14 val PER: 0.4926
2026-01-10 22:40:21,039: t15.2025.03.16 val PER: 0.4424
2026-01-10 22:40:21,039: t15.2025.03.30 val PER: 0.4759
2026-01-10 22:40:21,039: t15.2025.04.13 val PER: 0.4508
2026-01-10 22:40:21,039: New best val WER(5gram) 85.92% --> 85.53%
2026-01-10 22:40:21,198: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_7500
2026-01-10 22:40:29,730: Train batch 7600: loss: 24.57 grad norm: 70.34 time: 0.074
2026-01-10 22:40:47,284: Train batch 7800: loss: 19.32 grad norm: 77.44 time: 0.060
2026-01-10 22:41:05,417: Train batch 8000: loss: 13.94 grad norm: 78.56 time: 0.077
2026-01-10 22:41:05,417: Running test after training batch: 8000
2026-01-10 22:41:05,514: WER debug GT example: You can see the code at this point as well.
2026-01-10 22:41:11,516: WER debug example
  GT : you can see the code at this point as well
  PR : what you had seen the can't hit this point
2026-01-10 22:41:11,879: WER debug example
  GT : how does it keep the cost down
  PR : the hardest hit hits the
