TMPDIR=/home/e12511253/tmp
JOB_TMP=/home/e12511253/tmp/e12511253_b2t_358286
TORCH_EXTENSIONS_DIR=/home/e12511253/tmp/e12511253_b2t_358286/torch_extensions
WANDB_DIR=/home/e12511253/tmp/e12511253_b2t_358286/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/home/e12511253/tmp/e12511253_b2t_358286/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan 17 12:54 /home/e12511253/tmp/e12511253_b2t_358286/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
OUT_ROOT=/home/e12511253/Brain2Text/brain2text/trained_models
==============================================
Job: b2t  ID: 358286
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/diphones
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/diphones ==========
Num configs: 2

=== RUN base.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/diphones/base
2026-01-17 12:54:22,516: Using device: cuda:0
2026-01-17 12:54:24,256: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-17 12:54:33,263: Using 45 sessions after filtering (from 45).
2026-01-17 12:54:33,669: Using torch.compile (if available)
2026-01-17 12:54:33,670: torch.compile not available (torch<2.0). Skipping.
2026-01-17 12:54:33,670: Initialized RNN decoding model
2026-01-17 12:54:33,670: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-17 12:54:33,670: Model has 44,907,305 parameters
2026-01-17 12:54:33,670: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-17 12:54:42,286: Successfully initialized datasets
2026-01-17 12:54:42,287: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-17 12:54:43,288: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.180
2026-01-17 12:54:43,288: Running test after training batch: 0
2026-01-17 12:54:43,397: WER debug GT example: You can see the code at this point as well.
2026-01-17 12:54:48,549: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-17 12:54:49,262: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-17 12:55:22,763: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 39.475
2026-01-17 12:55:22,764: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-17 12:55:22,764: t15.2023.08.13 val PER: 1.3056
2026-01-17 12:55:22,764: t15.2023.08.18 val PER: 1.4208
2026-01-17 12:55:22,764: t15.2023.08.20 val PER: 1.3002
2026-01-17 12:55:22,764: t15.2023.08.25 val PER: 1.3389
2026-01-17 12:55:22,764: t15.2023.08.27 val PER: 1.2460
2026-01-17 12:55:22,764: t15.2023.09.01 val PER: 1.4537
2026-01-17 12:55:22,764: t15.2023.09.03 val PER: 1.3171
2026-01-17 12:55:22,765: t15.2023.09.24 val PER: 1.5461
2026-01-17 12:55:22,765: t15.2023.09.29 val PER: 1.4671
2026-01-17 12:55:22,765: t15.2023.10.01 val PER: 1.2147
2026-01-17 12:55:22,765: t15.2023.10.06 val PER: 1.4876
2026-01-17 12:55:22,765: t15.2023.10.08 val PER: 1.1827
2026-01-17 12:55:22,765: t15.2023.10.13 val PER: 1.3964
2026-01-17 12:55:22,765: t15.2023.10.15 val PER: 1.3889
2026-01-17 12:55:22,765: t15.2023.10.20 val PER: 1.4866
2026-01-17 12:55:22,765: t15.2023.10.22 val PER: 1.3942
2026-01-17 12:55:22,765: t15.2023.11.03 val PER: 1.5923
2026-01-17 12:55:22,765: t15.2023.11.04 val PER: 2.0171
2026-01-17 12:55:22,765: t15.2023.11.17 val PER: 1.9518
2026-01-17 12:55:22,765: t15.2023.11.19 val PER: 1.6707
2026-01-17 12:55:22,765: t15.2023.11.26 val PER: 1.5413
2026-01-17 12:55:22,765: t15.2023.12.03 val PER: 1.4254
2026-01-17 12:55:22,766: t15.2023.12.08 val PER: 1.4487
2026-01-17 12:55:22,766: t15.2023.12.10 val PER: 1.6899
2026-01-17 12:55:22,766: t15.2023.12.17 val PER: 1.3077
2026-01-17 12:55:22,766: t15.2023.12.29 val PER: 1.4063
2026-01-17 12:55:22,766: t15.2024.02.25 val PER: 1.4228
2026-01-17 12:55:22,766: t15.2024.03.08 val PER: 1.3257
2026-01-17 12:55:22,766: t15.2024.03.15 val PER: 1.3196
2026-01-17 12:55:22,766: t15.2024.03.17 val PER: 1.4052
2026-01-17 12:55:22,766: t15.2024.05.10 val PER: 1.3224
2026-01-17 12:55:22,766: t15.2024.06.14 val PER: 1.5315
2026-01-17 12:55:22,766: t15.2024.07.19 val PER: 1.0817
2026-01-17 12:55:22,766: t15.2024.07.21 val PER: 1.6290
2026-01-17 12:55:22,766: t15.2024.07.28 val PER: 1.6588
2026-01-17 12:55:22,766: t15.2025.01.10 val PER: 1.0923
2026-01-17 12:55:22,766: t15.2025.01.12 val PER: 1.7629
2026-01-17 12:55:22,766: t15.2025.03.14 val PER: 1.0414
2026-01-17 12:55:22,766: t15.2025.03.16 val PER: 1.6257
2026-01-17 12:55:22,767: t15.2025.03.30 val PER: 1.2874
2026-01-17 12:55:22,767: t15.2025.04.13 val PER: 1.5949
2026-01-17 12:55:22,768: New best val WER(1gram) inf% --> 100.00%
2026-01-17 12:55:22,768: Checkpointing model
2026-01-17 12:55:22,936: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 12:55:45,493: Train batch 200: loss: 77.58 grad norm: 106.06 time: 0.054
2026-01-17 12:56:02,207: Train batch 400: loss: 53.71 grad norm: 90.17 time: 0.063
2026-01-17 12:56:11,002: Running test after training batch: 500
2026-01-17 12:56:11,113: WER debug GT example: You can see the code at this point as well.
2026-01-17 12:56:16,293: WER debug example
  GT : you can see the code at this point as well
  PR : used and ease thus uhde at this ide is aisle
2026-01-17 12:56:16,324: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as adz
2026-01-17 12:56:18,559: Val batch 500: PER (avg): 0.5206 CTC Loss (avg): 56.0125 WER(1gram): 89.85% (n=64) time: 7.556
2026-01-17 12:56:18,559: WER lens: avg_true_words=6.16 avg_pred_words=5.70 max_pred_words=12
2026-01-17 12:56:18,560: t15.2023.08.13 val PER: 0.4626
2026-01-17 12:56:18,560: t15.2023.08.18 val PER: 0.4434
2026-01-17 12:56:18,560: t15.2023.08.20 val PER: 0.4432
2026-01-17 12:56:18,560: t15.2023.08.25 val PER: 0.4292
2026-01-17 12:56:18,560: t15.2023.08.27 val PER: 0.5289
2026-01-17 12:56:18,560: t15.2023.09.01 val PER: 0.4148
2026-01-17 12:56:18,560: t15.2023.09.03 val PER: 0.4941
2026-01-17 12:56:18,560: t15.2023.09.24 val PER: 0.4430
2026-01-17 12:56:18,560: t15.2023.09.29 val PER: 0.4729
2026-01-17 12:56:18,560: t15.2023.10.01 val PER: 0.5310
2026-01-17 12:56:18,560: t15.2023.10.06 val PER: 0.4360
2026-01-17 12:56:18,560: t15.2023.10.08 val PER: 0.5345
2026-01-17 12:56:18,560: t15.2023.10.13 val PER: 0.5826
2026-01-17 12:56:18,561: t15.2023.10.15 val PER: 0.4997
2026-01-17 12:56:18,561: t15.2023.10.20 val PER: 0.4933
2026-01-17 12:56:18,561: t15.2023.10.22 val PER: 0.4577
2026-01-17 12:56:18,561: t15.2023.11.03 val PER: 0.5068
2026-01-17 12:56:18,561: t15.2023.11.04 val PER: 0.2560
2026-01-17 12:56:18,561: t15.2023.11.17 val PER: 0.3748
2026-01-17 12:56:18,561: t15.2023.11.19 val PER: 0.3293
2026-01-17 12:56:18,561: t15.2023.11.26 val PER: 0.5493
2026-01-17 12:56:18,561: t15.2023.12.03 val PER: 0.4989
2026-01-17 12:56:18,561: t15.2023.12.08 val PER: 0.5280
2026-01-17 12:56:18,561: t15.2023.12.10 val PER: 0.4652
2026-01-17 12:56:18,561: t15.2023.12.17 val PER: 0.5541
2026-01-17 12:56:18,561: t15.2023.12.29 val PER: 0.5546
2026-01-17 12:56:18,561: t15.2024.02.25 val PER: 0.4902
2026-01-17 12:56:18,561: t15.2024.03.08 val PER: 0.6174
2026-01-17 12:56:18,561: t15.2024.03.15 val PER: 0.5566
2026-01-17 12:56:18,561: t15.2024.03.17 val PER: 0.4965
2026-01-17 12:56:18,562: t15.2024.05.10 val PER: 0.5453
2026-01-17 12:56:18,562: t15.2024.06.14 val PER: 0.5095
2026-01-17 12:56:18,562: t15.2024.07.19 val PER: 0.6612
2026-01-17 12:56:18,562: t15.2024.07.21 val PER: 0.4683
2026-01-17 12:56:18,562: t15.2024.07.28 val PER: 0.5154
2026-01-17 12:56:18,562: t15.2025.01.10 val PER: 0.7521
2026-01-17 12:56:18,562: t15.2025.01.12 val PER: 0.5635
2026-01-17 12:56:18,562: t15.2025.03.14 val PER: 0.7485
2026-01-17 12:56:18,562: t15.2025.03.16 val PER: 0.5942
2026-01-17 12:56:18,562: t15.2025.03.30 val PER: 0.7345
2026-01-17 12:56:18,562: t15.2025.04.13 val PER: 0.5920
2026-01-17 12:56:18,563: New best val WER(1gram) 100.00% --> 89.85%
2026-01-17 12:56:18,563: Checkpointing model
2026-01-17 12:56:18,700: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 12:56:27,469: Train batch 600: loss: 49.00 grad norm: 79.15 time: 0.077
2026-01-17 12:56:44,757: Train batch 800: loss: 40.68 grad norm: 87.28 time: 0.057
2026-01-17 12:57:01,626: Train batch 1000: loss: 43.02 grad norm: 81.38 time: 0.066
2026-01-17 12:57:01,627: Running test after training batch: 1000
2026-01-17 12:57:01,745: WER debug GT example: You can see the code at this point as well.
2026-01-17 12:57:06,454: WER debug example
  GT : you can see the code at this point as well
  PR : yield ent ease thus code it this boyde is while
2026-01-17 12:57:06,486: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus wass it
2026-01-17 12:57:08,341: Val batch 1000: PER (avg): 0.4075 CTC Loss (avg): 42.4830 WER(1gram): 80.71% (n=64) time: 6.714
2026-01-17 12:57:08,341: WER lens: avg_true_words=6.16 avg_pred_words=5.61 max_pred_words=12
2026-01-17 12:57:08,341: t15.2023.08.13 val PER: 0.3836
2026-01-17 12:57:08,341: t15.2023.08.18 val PER: 0.3462
2026-01-17 12:57:08,341: t15.2023.08.20 val PER: 0.3471
2026-01-17 12:57:08,341: t15.2023.08.25 val PER: 0.2982
2026-01-17 12:57:08,341: t15.2023.08.27 val PER: 0.4180
2026-01-17 12:57:08,342: t15.2023.09.01 val PER: 0.3036
2026-01-17 12:57:08,342: t15.2023.09.03 val PER: 0.3943
2026-01-17 12:57:08,342: t15.2023.09.24 val PER: 0.3167
2026-01-17 12:57:08,342: t15.2023.09.29 val PER: 0.3638
2026-01-17 12:57:08,342: t15.2023.10.01 val PER: 0.4009
2026-01-17 12:57:08,342: t15.2023.10.06 val PER: 0.3175
2026-01-17 12:57:08,342: t15.2023.10.08 val PER: 0.4506
2026-01-17 12:57:08,342: t15.2023.10.13 val PER: 0.4670
2026-01-17 12:57:08,342: t15.2023.10.15 val PER: 0.3771
2026-01-17 12:57:08,342: t15.2023.10.20 val PER: 0.3658
2026-01-17 12:57:08,342: t15.2023.10.22 val PER: 0.3486
2026-01-17 12:57:08,343: t15.2023.11.03 val PER: 0.4023
2026-01-17 12:57:08,343: t15.2023.11.04 val PER: 0.1638
2026-01-17 12:57:08,343: t15.2023.11.17 val PER: 0.2644
2026-01-17 12:57:08,343: t15.2023.11.19 val PER: 0.2056
2026-01-17 12:57:08,343: t15.2023.11.26 val PER: 0.4435
2026-01-17 12:57:08,343: t15.2023.12.03 val PER: 0.3960
2026-01-17 12:57:08,343: t15.2023.12.08 val PER: 0.4021
2026-01-17 12:57:08,343: t15.2023.12.10 val PER: 0.3495
2026-01-17 12:57:08,343: t15.2023.12.17 val PER: 0.4106
2026-01-17 12:57:08,343: t15.2023.12.29 val PER: 0.4056
2026-01-17 12:57:08,343: t15.2024.02.25 val PER: 0.3539
2026-01-17 12:57:08,343: t15.2024.03.08 val PER: 0.4964
2026-01-17 12:57:08,343: t15.2024.03.15 val PER: 0.4328
2026-01-17 12:57:08,343: t15.2024.03.17 val PER: 0.4059
2026-01-17 12:57:08,343: t15.2024.05.10 val PER: 0.4294
2026-01-17 12:57:08,343: t15.2024.06.14 val PER: 0.3975
2026-01-17 12:57:08,344: t15.2024.07.19 val PER: 0.5326
2026-01-17 12:57:08,344: t15.2024.07.21 val PER: 0.3738
2026-01-17 12:57:08,344: t15.2024.07.28 val PER: 0.4147
2026-01-17 12:57:08,344: t15.2025.01.10 val PER: 0.6033
2026-01-17 12:57:08,344: t15.2025.01.12 val PER: 0.4534
2026-01-17 12:57:08,344: t15.2025.03.14 val PER: 0.6391
2026-01-17 12:57:08,344: t15.2025.03.16 val PER: 0.4751
2026-01-17 12:57:08,344: t15.2025.03.30 val PER: 0.6437
2026-01-17 12:57:08,344: t15.2025.04.13 val PER: 0.5021
2026-01-17 12:57:08,346: New best val WER(1gram) 89.85% --> 80.71%
2026-01-17 12:57:08,346: Checkpointing model
2026-01-17 12:57:08,485: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 12:57:26,193: Train batch 1200: loss: 33.08 grad norm: 73.50 time: 0.067
2026-01-17 12:57:43,880: Train batch 1400: loss: 36.48 grad norm: 82.54 time: 0.060
2026-01-17 12:57:52,456: Running test after training batch: 1500
2026-01-17 12:57:52,608: WER debug GT example: You can see the code at this point as well.
2026-01-17 12:57:57,279: WER debug example
  GT : you can see the code at this point as well
  PR : yule kint e the owed it this boyde is will
2026-01-17 12:57:57,310: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that wass
2026-01-17 12:57:58,872: Val batch 1500: PER (avg): 0.3823 CTC Loss (avg): 37.1862 WER(1gram): 75.13% (n=64) time: 6.415
2026-01-17 12:57:58,872: WER lens: avg_true_words=6.16 avg_pred_words=5.08 max_pred_words=11
2026-01-17 12:57:58,872: t15.2023.08.13 val PER: 0.3503
2026-01-17 12:57:58,872: t15.2023.08.18 val PER: 0.3185
2026-01-17 12:57:58,872: t15.2023.08.20 val PER: 0.3106
2026-01-17 12:57:58,872: t15.2023.08.25 val PER: 0.2575
2026-01-17 12:57:58,873: t15.2023.08.27 val PER: 0.4051
2026-01-17 12:57:58,873: t15.2023.09.01 val PER: 0.2727
2026-01-17 12:57:58,873: t15.2023.09.03 val PER: 0.3765
2026-01-17 12:57:58,873: t15.2023.09.24 val PER: 0.2998
2026-01-17 12:57:58,873: t15.2023.09.29 val PER: 0.3357
2026-01-17 12:57:58,873: t15.2023.10.01 val PER: 0.3983
2026-01-17 12:57:58,873: t15.2023.10.06 val PER: 0.2863
2026-01-17 12:57:58,873: t15.2023.10.08 val PER: 0.4425
2026-01-17 12:57:58,873: t15.2023.10.13 val PER: 0.4476
2026-01-17 12:57:58,873: t15.2023.10.15 val PER: 0.3639
2026-01-17 12:57:58,873: t15.2023.10.20 val PER: 0.3557
2026-01-17 12:57:58,873: t15.2023.10.22 val PER: 0.3107
2026-01-17 12:57:58,873: t15.2023.11.03 val PER: 0.3650
2026-01-17 12:57:58,873: t15.2023.11.04 val PER: 0.1126
2026-01-17 12:57:58,874: t15.2023.11.17 val PER: 0.2286
2026-01-17 12:57:58,874: t15.2023.11.19 val PER: 0.1677
2026-01-17 12:57:58,874: t15.2023.11.26 val PER: 0.4196
2026-01-17 12:57:58,874: t15.2023.12.03 val PER: 0.3645
2026-01-17 12:57:58,874: t15.2023.12.08 val PER: 0.3628
2026-01-17 12:57:58,874: t15.2023.12.10 val PER: 0.3062
2026-01-17 12:57:58,874: t15.2023.12.17 val PER: 0.3805
2026-01-17 12:57:58,874: t15.2023.12.29 val PER: 0.3768
2026-01-17 12:57:58,874: t15.2024.02.25 val PER: 0.3118
2026-01-17 12:57:58,874: t15.2024.03.08 val PER: 0.4651
2026-01-17 12:57:58,874: t15.2024.03.15 val PER: 0.4178
2026-01-17 12:57:58,874: t15.2024.03.17 val PER: 0.3808
2026-01-17 12:57:58,874: t15.2024.05.10 val PER: 0.3908
2026-01-17 12:57:58,874: t15.2024.06.14 val PER: 0.4006
2026-01-17 12:57:58,874: t15.2024.07.19 val PER: 0.5307
2026-01-17 12:57:58,875: t15.2024.07.21 val PER: 0.3490
2026-01-17 12:57:58,875: t15.2024.07.28 val PER: 0.3684
2026-01-17 12:57:58,875: t15.2025.01.10 val PER: 0.6019
2026-01-17 12:57:58,875: t15.2025.01.12 val PER: 0.4280
2026-01-17 12:57:58,875: t15.2025.03.14 val PER: 0.6124
2026-01-17 12:57:58,875: t15.2025.03.16 val PER: 0.4555
2026-01-17 12:57:58,875: t15.2025.03.30 val PER: 0.6402
2026-01-17 12:57:58,875: t15.2025.04.13 val PER: 0.4750
2026-01-17 12:57:58,876: New best val WER(1gram) 80.71% --> 75.13%
2026-01-17 12:57:58,876: Checkpointing model
2026-01-17 12:57:59,015: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 12:58:07,400: Train batch 1600: loss: 37.26 grad norm: 80.63 time: 0.064
2026-01-17 12:58:24,303: Train batch 1800: loss: 34.76 grad norm: 69.48 time: 0.088
2026-01-17 12:58:41,461: Train batch 2000: loss: 33.28 grad norm: 68.36 time: 0.066
2026-01-17 12:58:41,461: Running test after training batch: 2000
2026-01-17 12:58:41,586: WER debug GT example: You can see the code at this point as well.
2026-01-17 12:58:46,252: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt e the owed at this bonde is will
2026-01-17 12:58:46,282: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap thus cus it
2026-01-17 12:58:47,821: Val batch 2000: PER (avg): 0.3252 CTC Loss (avg): 32.6791 WER(1gram): 71.32% (n=64) time: 6.360
2026-01-17 12:58:47,822: WER lens: avg_true_words=6.16 avg_pred_words=5.69 max_pred_words=11
2026-01-17 12:58:47,822: t15.2023.08.13 val PER: 0.2994
2026-01-17 12:58:47,822: t15.2023.08.18 val PER: 0.2557
2026-01-17 12:58:47,822: t15.2023.08.20 val PER: 0.2629
2026-01-17 12:58:47,822: t15.2023.08.25 val PER: 0.2214
2026-01-17 12:58:47,822: t15.2023.08.27 val PER: 0.3360
2026-01-17 12:58:47,822: t15.2023.09.01 val PER: 0.2232
2026-01-17 12:58:47,822: t15.2023.09.03 val PER: 0.3242
2026-01-17 12:58:47,823: t15.2023.09.24 val PER: 0.2476
2026-01-17 12:58:47,823: t15.2023.09.29 val PER: 0.2738
2026-01-17 12:58:47,823: t15.2023.10.01 val PER: 0.3329
2026-01-17 12:58:47,823: t15.2023.10.06 val PER: 0.2325
2026-01-17 12:58:47,823: t15.2023.10.08 val PER: 0.3938
2026-01-17 12:58:47,823: t15.2023.10.13 val PER: 0.3662
2026-01-17 12:58:47,823: t15.2023.10.15 val PER: 0.3032
2026-01-17 12:58:47,823: t15.2023.10.20 val PER: 0.2819
2026-01-17 12:58:47,823: t15.2023.10.22 val PER: 0.2528
2026-01-17 12:58:47,823: t15.2023.11.03 val PER: 0.3121
2026-01-17 12:58:47,823: t15.2023.11.04 val PER: 0.0922
2026-01-17 12:58:47,823: t15.2023.11.17 val PER: 0.1742
2026-01-17 12:58:47,823: t15.2023.11.19 val PER: 0.1357
2026-01-17 12:58:47,823: t15.2023.11.26 val PER: 0.3674
2026-01-17 12:58:47,824: t15.2023.12.03 val PER: 0.3078
2026-01-17 12:58:47,824: t15.2023.12.08 val PER: 0.3103
2026-01-17 12:58:47,824: t15.2023.12.10 val PER: 0.2523
2026-01-17 12:58:47,824: t15.2023.12.17 val PER: 0.3098
2026-01-17 12:58:47,824: t15.2023.12.29 val PER: 0.3171
2026-01-17 12:58:47,824: t15.2024.02.25 val PER: 0.2753
2026-01-17 12:58:47,824: t15.2024.03.08 val PER: 0.3940
2026-01-17 12:58:47,824: t15.2024.03.15 val PER: 0.3590
2026-01-17 12:58:47,824: t15.2024.03.17 val PER: 0.3347
2026-01-17 12:58:47,824: t15.2024.05.10 val PER: 0.3447
2026-01-17 12:58:47,824: t15.2024.06.14 val PER: 0.3470
2026-01-17 12:58:47,825: t15.2024.07.19 val PER: 0.4535
2026-01-17 12:58:47,825: t15.2024.07.21 val PER: 0.2924
2026-01-17 12:58:47,825: t15.2024.07.28 val PER: 0.3294
2026-01-17 12:58:47,825: t15.2025.01.10 val PER: 0.5482
2026-01-17 12:58:47,825: t15.2025.01.12 val PER: 0.3811
2026-01-17 12:58:47,825: t15.2025.03.14 val PER: 0.5518
2026-01-17 12:58:47,825: t15.2025.03.16 val PER: 0.4084
2026-01-17 12:58:47,825: t15.2025.03.30 val PER: 0.5356
2026-01-17 12:58:47,825: t15.2025.04.13 val PER: 0.3966
2026-01-17 12:58:47,827: New best val WER(1gram) 75.13% --> 71.32%
2026-01-17 12:58:47,827: Checkpointing model
2026-01-17 12:58:47,966: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 12:59:05,201: Train batch 2200: loss: 28.11 grad norm: 70.55 time: 0.061
2026-01-17 12:59:24,299: Train batch 2400: loss: 28.91 grad norm: 62.32 time: 0.053
2026-01-17 12:59:33,483: Running test after training batch: 2500
2026-01-17 12:59:33,635: WER debug GT example: You can see the code at this point as well.
2026-01-17 12:59:38,738: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code it this point is will
2026-01-17 12:59:38,769: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost nit
2026-01-17 12:59:40,351: Val batch 2500: PER (avg): 0.3037 CTC Loss (avg): 30.0589 WER(1gram): 68.27% (n=64) time: 6.868
2026-01-17 12:59:40,352: WER lens: avg_true_words=6.16 avg_pred_words=5.56 max_pred_words=10
2026-01-17 12:59:40,352: t15.2023.08.13 val PER: 0.2807
2026-01-17 12:59:40,352: t15.2023.08.18 val PER: 0.2456
2026-01-17 12:59:40,352: t15.2023.08.20 val PER: 0.2423
2026-01-17 12:59:40,352: t15.2023.08.25 val PER: 0.2063
2026-01-17 12:59:40,352: t15.2023.08.27 val PER: 0.3103
2026-01-17 12:59:40,352: t15.2023.09.01 val PER: 0.2094
2026-01-17 12:59:40,352: t15.2023.09.03 val PER: 0.2898
2026-01-17 12:59:40,352: t15.2023.09.24 val PER: 0.2257
2026-01-17 12:59:40,352: t15.2023.09.29 val PER: 0.2559
2026-01-17 12:59:40,352: t15.2023.10.01 val PER: 0.3045
2026-01-17 12:59:40,352: t15.2023.10.06 val PER: 0.2153
2026-01-17 12:59:40,353: t15.2023.10.08 val PER: 0.3708
2026-01-17 12:59:40,353: t15.2023.10.13 val PER: 0.3607
2026-01-17 12:59:40,353: t15.2023.10.15 val PER: 0.2894
2026-01-17 12:59:40,353: t15.2023.10.20 val PER: 0.2819
2026-01-17 12:59:40,353: t15.2023.10.22 val PER: 0.2372
2026-01-17 12:59:40,353: t15.2023.11.03 val PER: 0.2999
2026-01-17 12:59:40,353: t15.2023.11.04 val PER: 0.0785
2026-01-17 12:59:40,353: t15.2023.11.17 val PER: 0.1524
2026-01-17 12:59:40,353: t15.2023.11.19 val PER: 0.1277
2026-01-17 12:59:40,353: t15.2023.11.26 val PER: 0.3399
2026-01-17 12:59:40,353: t15.2023.12.03 val PER: 0.2784
2026-01-17 12:59:40,354: t15.2023.12.08 val PER: 0.2730
2026-01-17 12:59:40,354: t15.2023.12.10 val PER: 0.2405
2026-01-17 12:59:40,354: t15.2023.12.17 val PER: 0.2786
2026-01-17 12:59:40,354: t15.2023.12.29 val PER: 0.3040
2026-01-17 12:59:40,354: t15.2024.02.25 val PER: 0.2303
2026-01-17 12:59:40,354: t15.2024.03.08 val PER: 0.3599
2026-01-17 12:59:40,354: t15.2024.03.15 val PER: 0.3540
2026-01-17 12:59:40,355: t15.2024.03.17 val PER: 0.3124
2026-01-17 12:59:40,355: t15.2024.05.10 val PER: 0.3180
2026-01-17 12:59:40,355: t15.2024.06.14 val PER: 0.3060
2026-01-17 12:59:40,355: t15.2024.07.19 val PER: 0.4515
2026-01-17 12:59:40,355: t15.2024.07.21 val PER: 0.2717
2026-01-17 12:59:40,355: t15.2024.07.28 val PER: 0.2941
2026-01-17 12:59:40,355: t15.2025.01.10 val PER: 0.4931
2026-01-17 12:59:40,356: t15.2025.01.12 val PER: 0.3657
2026-01-17 12:59:40,356: t15.2025.03.14 val PER: 0.4941
2026-01-17 12:59:40,356: t15.2025.03.16 val PER: 0.3639
2026-01-17 12:59:40,356: t15.2025.03.30 val PER: 0.5069
2026-01-17 12:59:40,356: t15.2025.04.13 val PER: 0.3951
2026-01-17 12:59:40,356: New best val WER(1gram) 71.32% --> 68.27%
2026-01-17 12:59:40,356: Checkpointing model
2026-01-17 12:59:40,498: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 12:59:49,149: Train batch 2600: loss: 34.89 grad norm: 82.95 time: 0.054
2026-01-17 13:00:06,409: Train batch 2800: loss: 25.86 grad norm: 74.02 time: 0.080
2026-01-17 13:00:23,744: Train batch 3000: loss: 30.73 grad norm: 71.80 time: 0.082
2026-01-17 13:00:23,744: Running test after training batch: 3000
2026-01-17 13:00:23,848: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:00:28,524: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-17 13:00:28,553: WER debug example
  GT : how does it keep the cost down
  PR : aue des it kipp the cost get
2026-01-17 13:00:30,136: Val batch 3000: PER (avg): 0.2790 CTC Loss (avg): 27.5926 WER(1gram): 64.21% (n=64) time: 6.391
2026-01-17 13:00:30,136: WER lens: avg_true_words=6.16 avg_pred_words=5.72 max_pred_words=11
2026-01-17 13:00:30,136: t15.2023.08.13 val PER: 0.2495
2026-01-17 13:00:30,136: t15.2023.08.18 val PER: 0.2171
2026-01-17 13:00:30,137: t15.2023.08.20 val PER: 0.2121
2026-01-17 13:00:30,137: t15.2023.08.25 val PER: 0.1928
2026-01-17 13:00:30,137: t15.2023.08.27 val PER: 0.2862
2026-01-17 13:00:30,137: t15.2023.09.01 val PER: 0.1834
2026-01-17 13:00:30,137: t15.2023.09.03 val PER: 0.2767
2026-01-17 13:00:30,137: t15.2023.09.24 val PER: 0.2075
2026-01-17 13:00:30,137: t15.2023.09.29 val PER: 0.2348
2026-01-17 13:00:30,137: t15.2023.10.01 val PER: 0.2867
2026-01-17 13:00:30,137: t15.2023.10.06 val PER: 0.1959
2026-01-17 13:00:30,137: t15.2023.10.08 val PER: 0.3491
2026-01-17 13:00:30,137: t15.2023.10.13 val PER: 0.3460
2026-01-17 13:00:30,137: t15.2023.10.15 val PER: 0.2597
2026-01-17 13:00:30,137: t15.2023.10.20 val PER: 0.2517
2026-01-17 13:00:30,138: t15.2023.10.22 val PER: 0.2060
2026-01-17 13:00:30,138: t15.2023.11.03 val PER: 0.2782
2026-01-17 13:00:30,138: t15.2023.11.04 val PER: 0.0751
2026-01-17 13:00:30,138: t15.2023.11.17 val PER: 0.1275
2026-01-17 13:00:30,138: t15.2023.11.19 val PER: 0.1218
2026-01-17 13:00:30,138: t15.2023.11.26 val PER: 0.3014
2026-01-17 13:00:30,138: t15.2023.12.03 val PER: 0.2584
2026-01-17 13:00:30,138: t15.2023.12.08 val PER: 0.2517
2026-01-17 13:00:30,138: t15.2023.12.10 val PER: 0.2116
2026-01-17 13:00:30,138: t15.2023.12.17 val PER: 0.2661
2026-01-17 13:00:30,138: t15.2023.12.29 val PER: 0.2814
2026-01-17 13:00:30,138: t15.2024.02.25 val PER: 0.2331
2026-01-17 13:00:30,138: t15.2024.03.08 val PER: 0.3713
2026-01-17 13:00:30,138: t15.2024.03.15 val PER: 0.3421
2026-01-17 13:00:30,139: t15.2024.03.17 val PER: 0.2824
2026-01-17 13:00:30,139: t15.2024.05.10 val PER: 0.3105
2026-01-17 13:00:30,139: t15.2024.06.14 val PER: 0.2950
2026-01-17 13:00:30,139: t15.2024.07.19 val PER: 0.3869
2026-01-17 13:00:30,139: t15.2024.07.21 val PER: 0.2372
2026-01-17 13:00:30,139: t15.2024.07.28 val PER: 0.2787
2026-01-17 13:00:30,139: t15.2025.01.10 val PER: 0.4835
2026-01-17 13:00:30,139: t15.2025.01.12 val PER: 0.3249
2026-01-17 13:00:30,139: t15.2025.03.14 val PER: 0.4453
2026-01-17 13:00:30,139: t15.2025.03.16 val PER: 0.3128
2026-01-17 13:00:30,139: t15.2025.03.30 val PER: 0.4805
2026-01-17 13:00:30,140: t15.2025.04.13 val PER: 0.3495
2026-01-17 13:00:30,141: New best val WER(1gram) 68.27% --> 64.21%
2026-01-17 13:00:30,141: Checkpointing model
2026-01-17 13:00:30,282: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 13:00:47,728: Train batch 3200: loss: 25.89 grad norm: 65.70 time: 0.075
2026-01-17 13:01:05,031: Train batch 3400: loss: 18.14 grad norm: 53.40 time: 0.048
2026-01-17 13:01:13,928: Running test after training batch: 3500
2026-01-17 13:01:14,018: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:01:18,715: WER debug example
  GT : you can see the code at this point as well
  PR : yule end e the code at this point wheel
2026-01-17 13:01:18,743: WER debug example
  GT : how does it keep the cost down
  PR : houde des it hipp thus est nit
2026-01-17 13:01:20,328: Val batch 3500: PER (avg): 0.2658 CTC Loss (avg): 26.6348 WER(1gram): 68.27% (n=64) time: 6.400
2026-01-17 13:01:20,328: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-17 13:01:20,329: t15.2023.08.13 val PER: 0.2443
2026-01-17 13:01:20,329: t15.2023.08.18 val PER: 0.2037
2026-01-17 13:01:20,329: t15.2023.08.20 val PER: 0.2176
2026-01-17 13:01:20,329: t15.2023.08.25 val PER: 0.1792
2026-01-17 13:01:20,329: t15.2023.08.27 val PER: 0.2830
2026-01-17 13:01:20,329: t15.2023.09.01 val PER: 0.1769
2026-01-17 13:01:20,329: t15.2023.09.03 val PER: 0.2625
2026-01-17 13:01:20,329: t15.2023.09.24 val PER: 0.2197
2026-01-17 13:01:20,329: t15.2023.09.29 val PER: 0.2214
2026-01-17 13:01:20,329: t15.2023.10.01 val PER: 0.2814
2026-01-17 13:01:20,330: t15.2023.10.06 val PER: 0.1873
2026-01-17 13:01:20,330: t15.2023.10.08 val PER: 0.3356
2026-01-17 13:01:20,330: t15.2023.10.13 val PER: 0.3189
2026-01-17 13:01:20,330: t15.2023.10.15 val PER: 0.2531
2026-01-17 13:01:20,330: t15.2023.10.20 val PER: 0.2315
2026-01-17 13:01:20,330: t15.2023.10.22 val PER: 0.2038
2026-01-17 13:01:20,330: t15.2023.11.03 val PER: 0.2626
2026-01-17 13:01:20,330: t15.2023.11.04 val PER: 0.0819
2026-01-17 13:01:20,330: t15.2023.11.17 val PER: 0.1135
2026-01-17 13:01:20,330: t15.2023.11.19 val PER: 0.1058
2026-01-17 13:01:20,330: t15.2023.11.26 val PER: 0.2870
2026-01-17 13:01:20,330: t15.2023.12.03 val PER: 0.2511
2026-01-17 13:01:20,330: t15.2023.12.08 val PER: 0.2357
2026-01-17 13:01:20,330: t15.2023.12.10 val PER: 0.1971
2026-01-17 13:01:20,330: t15.2023.12.17 val PER: 0.2484
2026-01-17 13:01:20,330: t15.2023.12.29 val PER: 0.2588
2026-01-17 13:01:20,331: t15.2024.02.25 val PER: 0.2079
2026-01-17 13:01:20,331: t15.2024.03.08 val PER: 0.3457
2026-01-17 13:01:20,331: t15.2024.03.15 val PER: 0.3158
2026-01-17 13:01:20,331: t15.2024.03.17 val PER: 0.2755
2026-01-17 13:01:20,331: t15.2024.05.10 val PER: 0.2615
2026-01-17 13:01:20,331: t15.2024.06.14 val PER: 0.2744
2026-01-17 13:01:20,331: t15.2024.07.19 val PER: 0.3856
2026-01-17 13:01:20,331: t15.2024.07.21 val PER: 0.2234
2026-01-17 13:01:20,331: t15.2024.07.28 val PER: 0.2713
2026-01-17 13:01:20,331: t15.2025.01.10 val PER: 0.4504
2026-01-17 13:01:20,331: t15.2025.01.12 val PER: 0.2948
2026-01-17 13:01:20,331: t15.2025.03.14 val PER: 0.4497
2026-01-17 13:01:20,332: t15.2025.03.16 val PER: 0.3233
2026-01-17 13:01:20,332: t15.2025.03.30 val PER: 0.4356
2026-01-17 13:01:20,332: t15.2025.04.13 val PER: 0.3424
2026-01-17 13:01:29,130: Train batch 3600: loss: 22.28 grad norm: 60.27 time: 0.065
2026-01-17 13:01:46,728: Train batch 3800: loss: 25.32 grad norm: 72.00 time: 0.067
2026-01-17 13:02:04,246: Train batch 4000: loss: 19.48 grad norm: 55.20 time: 0.055
2026-01-17 13:02:04,246: Running test after training batch: 4000
2026-01-17 13:02:04,392: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:02:09,352: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point will
2026-01-17 13:02:09,381: WER debug example
  GT : how does it keep the cost down
  PR : how des it keep the est et
2026-01-17 13:02:10,979: Val batch 4000: PER (avg): 0.2473 CTC Loss (avg): 24.3199 WER(1gram): 63.71% (n=64) time: 6.733
2026-01-17 13:02:10,980: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-17 13:02:10,980: t15.2023.08.13 val PER: 0.2277
2026-01-17 13:02:10,980: t15.2023.08.18 val PER: 0.1978
2026-01-17 13:02:10,980: t15.2023.08.20 val PER: 0.1906
2026-01-17 13:02:10,980: t15.2023.08.25 val PER: 0.1596
2026-01-17 13:02:10,980: t15.2023.08.27 val PER: 0.2781
2026-01-17 13:02:10,980: t15.2023.09.01 val PER: 0.1542
2026-01-17 13:02:10,980: t15.2023.09.03 val PER: 0.2387
2026-01-17 13:02:10,980: t15.2023.09.24 val PER: 0.1954
2026-01-17 13:02:10,980: t15.2023.09.29 val PER: 0.1978
2026-01-17 13:02:10,981: t15.2023.10.01 val PER: 0.2583
2026-01-17 13:02:10,981: t15.2023.10.06 val PER: 0.1604
2026-01-17 13:02:10,981: t15.2023.10.08 val PER: 0.3275
2026-01-17 13:02:10,981: t15.2023.10.13 val PER: 0.3002
2026-01-17 13:02:10,981: t15.2023.10.15 val PER: 0.2393
2026-01-17 13:02:10,981: t15.2023.10.20 val PER: 0.2584
2026-01-17 13:02:10,981: t15.2023.10.22 val PER: 0.1904
2026-01-17 13:02:10,981: t15.2023.11.03 val PER: 0.2449
2026-01-17 13:02:10,981: t15.2023.11.04 val PER: 0.0614
2026-01-17 13:02:10,981: t15.2023.11.17 val PER: 0.0980
2026-01-17 13:02:10,981: t15.2023.11.19 val PER: 0.0918
2026-01-17 13:02:10,981: t15.2023.11.26 val PER: 0.2529
2026-01-17 13:02:10,982: t15.2023.12.03 val PER: 0.2227
2026-01-17 13:02:10,982: t15.2023.12.08 val PER: 0.2170
2026-01-17 13:02:10,982: t15.2023.12.10 val PER: 0.1892
2026-01-17 13:02:10,982: t15.2023.12.17 val PER: 0.2328
2026-01-17 13:02:10,982: t15.2023.12.29 val PER: 0.2498
2026-01-17 13:02:10,982: t15.2024.02.25 val PER: 0.2149
2026-01-17 13:02:10,982: t15.2024.03.08 val PER: 0.3343
2026-01-17 13:02:10,982: t15.2024.03.15 val PER: 0.3058
2026-01-17 13:02:10,982: t15.2024.03.17 val PER: 0.2497
2026-01-17 13:02:10,982: t15.2024.05.10 val PER: 0.2645
2026-01-17 13:02:10,982: t15.2024.06.14 val PER: 0.2729
2026-01-17 13:02:10,982: t15.2024.07.19 val PER: 0.3626
2026-01-17 13:02:10,982: t15.2024.07.21 val PER: 0.1903
2026-01-17 13:02:10,982: t15.2024.07.28 val PER: 0.2434
2026-01-17 13:02:10,982: t15.2025.01.10 val PER: 0.4132
2026-01-17 13:02:10,982: t15.2025.01.12 val PER: 0.2856
2026-01-17 13:02:10,982: t15.2025.03.14 val PER: 0.4201
2026-01-17 13:02:10,982: t15.2025.03.16 val PER: 0.3168
2026-01-17 13:02:10,983: t15.2025.03.30 val PER: 0.4069
2026-01-17 13:02:10,983: t15.2025.04.13 val PER: 0.3124
2026-01-17 13:02:10,984: New best val WER(1gram) 64.21% --> 63.71%
2026-01-17 13:02:10,984: Checkpointing model
2026-01-17 13:02:11,122: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 13:02:28,458: Train batch 4200: loss: 22.13 grad norm: 62.42 time: 0.079
2026-01-17 13:02:46,070: Train batch 4400: loss: 16.84 grad norm: 56.72 time: 0.066
2026-01-17 13:02:55,046: Running test after training batch: 4500
2026-01-17 13:02:55,171: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:02:59,854: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-17 13:02:59,884: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cussed get
2026-01-17 13:03:01,449: Val batch 4500: PER (avg): 0.2349 CTC Loss (avg): 23.0142 WER(1gram): 61.93% (n=64) time: 6.403
2026-01-17 13:03:01,450: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-17 13:03:01,450: t15.2023.08.13 val PER: 0.1985
2026-01-17 13:03:01,450: t15.2023.08.18 val PER: 0.1844
2026-01-17 13:03:01,450: t15.2023.08.20 val PER: 0.1938
2026-01-17 13:03:01,450: t15.2023.08.25 val PER: 0.1416
2026-01-17 13:03:01,450: t15.2023.08.27 val PER: 0.2508
2026-01-17 13:03:01,450: t15.2023.09.01 val PER: 0.1510
2026-01-17 13:03:01,450: t15.2023.09.03 val PER: 0.2352
2026-01-17 13:03:01,450: t15.2023.09.24 val PER: 0.1833
2026-01-17 13:03:01,451: t15.2023.09.29 val PER: 0.1940
2026-01-17 13:03:01,451: t15.2023.10.01 val PER: 0.2556
2026-01-17 13:03:01,451: t15.2023.10.06 val PER: 0.1561
2026-01-17 13:03:01,451: t15.2023.10.08 val PER: 0.3085
2026-01-17 13:03:01,451: t15.2023.10.13 val PER: 0.2940
2026-01-17 13:03:01,451: t15.2023.10.15 val PER: 0.2221
2026-01-17 13:03:01,451: t15.2023.10.20 val PER: 0.2248
2026-01-17 13:03:01,451: t15.2023.10.22 val PER: 0.1837
2026-01-17 13:03:01,451: t15.2023.11.03 val PER: 0.2388
2026-01-17 13:03:01,451: t15.2023.11.04 val PER: 0.0819
2026-01-17 13:03:01,451: t15.2023.11.17 val PER: 0.0933
2026-01-17 13:03:01,451: t15.2023.11.19 val PER: 0.1038
2026-01-17 13:03:01,451: t15.2023.11.26 val PER: 0.2587
2026-01-17 13:03:01,451: t15.2023.12.03 val PER: 0.2059
2026-01-17 13:03:01,451: t15.2023.12.08 val PER: 0.1997
2026-01-17 13:03:01,452: t15.2023.12.10 val PER: 0.1827
2026-01-17 13:03:01,452: t15.2023.12.17 val PER: 0.2318
2026-01-17 13:03:01,452: t15.2023.12.29 val PER: 0.2272
2026-01-17 13:03:01,452: t15.2024.02.25 val PER: 0.1840
2026-01-17 13:03:01,452: t15.2024.03.08 val PER: 0.3115
2026-01-17 13:03:01,452: t15.2024.03.15 val PER: 0.2896
2026-01-17 13:03:01,452: t15.2024.03.17 val PER: 0.2406
2026-01-17 13:03:01,452: t15.2024.05.10 val PER: 0.2541
2026-01-17 13:03:01,452: t15.2024.06.14 val PER: 0.2366
2026-01-17 13:03:01,452: t15.2024.07.19 val PER: 0.3395
2026-01-17 13:03:01,452: t15.2024.07.21 val PER: 0.1655
2026-01-17 13:03:01,452: t15.2024.07.28 val PER: 0.2176
2026-01-17 13:03:01,452: t15.2025.01.10 val PER: 0.4008
2026-01-17 13:03:01,452: t15.2025.01.12 val PER: 0.2617
2026-01-17 13:03:01,452: t15.2025.03.14 val PER: 0.3964
2026-01-17 13:03:01,452: t15.2025.03.16 val PER: 0.3024
2026-01-17 13:03:01,452: t15.2025.03.30 val PER: 0.4195
2026-01-17 13:03:01,453: t15.2025.04.13 val PER: 0.3024
2026-01-17 13:03:01,454: New best val WER(1gram) 63.71% --> 61.93%
2026-01-17 13:03:01,454: Checkpointing model
2026-01-17 13:03:01,594: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 13:03:10,342: Train batch 4600: loss: 19.99 grad norm: 59.83 time: 0.062
2026-01-17 13:03:27,615: Train batch 4800: loss: 13.49 grad norm: 50.65 time: 0.063
2026-01-17 13:03:44,799: Train batch 5000: loss: 31.47 grad norm: 80.00 time: 0.064
2026-01-17 13:03:44,799: Running test after training batch: 5000
2026-01-17 13:03:44,928: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:03:49,619: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point as will
2026-01-17 13:03:49,648: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the cussed nett
2026-01-17 13:03:51,215: Val batch 5000: PER (avg): 0.2227 CTC Loss (avg): 21.8431 WER(1gram): 60.66% (n=64) time: 6.416
2026-01-17 13:03:51,215: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-17 13:03:51,215: t15.2023.08.13 val PER: 0.1892
2026-01-17 13:03:51,215: t15.2023.08.18 val PER: 0.1618
2026-01-17 13:03:51,215: t15.2023.08.20 val PER: 0.1724
2026-01-17 13:03:51,216: t15.2023.08.25 val PER: 0.1355
2026-01-17 13:03:51,216: t15.2023.08.27 val PER: 0.2235
2026-01-17 13:03:51,216: t15.2023.09.01 val PER: 0.1396
2026-01-17 13:03:51,216: t15.2023.09.03 val PER: 0.2233
2026-01-17 13:03:51,216: t15.2023.09.24 val PER: 0.1833
2026-01-17 13:03:51,216: t15.2023.09.29 val PER: 0.1780
2026-01-17 13:03:51,216: t15.2023.10.01 val PER: 0.2312
2026-01-17 13:03:51,216: t15.2023.10.06 val PER: 0.1475
2026-01-17 13:03:51,217: t15.2023.10.08 val PER: 0.3058
2026-01-17 13:03:51,217: t15.2023.10.13 val PER: 0.2754
2026-01-17 13:03:51,217: t15.2023.10.15 val PER: 0.2195
2026-01-17 13:03:51,217: t15.2023.10.20 val PER: 0.2248
2026-01-17 13:03:51,217: t15.2023.10.22 val PER: 0.1659
2026-01-17 13:03:51,217: t15.2023.11.03 val PER: 0.2300
2026-01-17 13:03:51,217: t15.2023.11.04 val PER: 0.0683
2026-01-17 13:03:51,217: t15.2023.11.17 val PER: 0.0793
2026-01-17 13:03:51,217: t15.2023.11.19 val PER: 0.0778
2026-01-17 13:03:51,218: t15.2023.11.26 val PER: 0.2326
2026-01-17 13:03:51,218: t15.2023.12.03 val PER: 0.1985
2026-01-17 13:03:51,218: t15.2023.12.08 val PER: 0.1931
2026-01-17 13:03:51,218: t15.2023.12.10 val PER: 0.1551
2026-01-17 13:03:51,218: t15.2023.12.17 val PER: 0.2266
2026-01-17 13:03:51,218: t15.2023.12.29 val PER: 0.2148
2026-01-17 13:03:51,218: t15.2024.02.25 val PER: 0.1770
2026-01-17 13:03:51,218: t15.2024.03.08 val PER: 0.3058
2026-01-17 13:03:51,218: t15.2024.03.15 val PER: 0.2783
2026-01-17 13:03:51,218: t15.2024.03.17 val PER: 0.2259
2026-01-17 13:03:51,219: t15.2024.05.10 val PER: 0.2333
2026-01-17 13:03:51,219: t15.2024.06.14 val PER: 0.2508
2026-01-17 13:03:51,219: t15.2024.07.19 val PER: 0.3250
2026-01-17 13:03:51,219: t15.2024.07.21 val PER: 0.1834
2026-01-17 13:03:51,219: t15.2024.07.28 val PER: 0.2074
2026-01-17 13:03:51,219: t15.2025.01.10 val PER: 0.3912
2026-01-17 13:03:51,219: t15.2025.01.12 val PER: 0.2386
2026-01-17 13:03:51,219: t15.2025.03.14 val PER: 0.3935
2026-01-17 13:03:51,219: t15.2025.03.16 val PER: 0.2736
2026-01-17 13:03:51,219: t15.2025.03.30 val PER: 0.3954
2026-01-17 13:03:51,219: t15.2025.04.13 val PER: 0.2996
2026-01-17 13:03:51,220: New best val WER(1gram) 61.93% --> 60.66%
2026-01-17 13:03:51,220: Checkpointing model
2026-01-17 13:03:51,362: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 13:04:08,495: Train batch 5200: loss: 16.52 grad norm: 61.31 time: 0.051
2026-01-17 13:04:25,884: Train batch 5400: loss: 17.06 grad norm: 60.24 time: 0.067
2026-01-17 13:04:34,670: Running test after training batch: 5500
2026-01-17 13:04:34,832: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:04:40,058: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-17 13:04:40,085: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cussed get
2026-01-17 13:04:41,657: Val batch 5500: PER (avg): 0.2135 CTC Loss (avg): 20.7437 WER(1gram): 55.84% (n=64) time: 6.986
2026-01-17 13:04:41,657: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-17 13:04:41,657: t15.2023.08.13 val PER: 0.1746
2026-01-17 13:04:41,657: t15.2023.08.18 val PER: 0.1609
2026-01-17 13:04:41,657: t15.2023.08.20 val PER: 0.1652
2026-01-17 13:04:41,657: t15.2023.08.25 val PER: 0.1190
2026-01-17 13:04:41,658: t15.2023.08.27 val PER: 0.2331
2026-01-17 13:04:41,658: t15.2023.09.01 val PER: 0.1266
2026-01-17 13:04:41,658: t15.2023.09.03 val PER: 0.2268
2026-01-17 13:04:41,658: t15.2023.09.24 val PER: 0.1748
2026-01-17 13:04:41,658: t15.2023.09.29 val PER: 0.1800
2026-01-17 13:04:41,658: t15.2023.10.01 val PER: 0.2299
2026-01-17 13:04:41,658: t15.2023.10.06 val PER: 0.1421
2026-01-17 13:04:41,658: t15.2023.10.08 val PER: 0.2977
2026-01-17 13:04:41,658: t15.2023.10.13 val PER: 0.2746
2026-01-17 13:04:41,658: t15.2023.10.15 val PER: 0.2103
2026-01-17 13:04:41,658: t15.2023.10.20 val PER: 0.2148
2026-01-17 13:04:41,658: t15.2023.10.22 val PER: 0.1481
2026-01-17 13:04:41,658: t15.2023.11.03 val PER: 0.2280
2026-01-17 13:04:41,658: t15.2023.11.04 val PER: 0.0614
2026-01-17 13:04:41,659: t15.2023.11.17 val PER: 0.0793
2026-01-17 13:04:41,659: t15.2023.11.19 val PER: 0.0778
2026-01-17 13:04:41,659: t15.2023.11.26 val PER: 0.2123
2026-01-17 13:04:41,659: t15.2023.12.03 val PER: 0.1765
2026-01-17 13:04:41,659: t15.2023.12.08 val PER: 0.1811
2026-01-17 13:04:41,659: t15.2023.12.10 val PER: 0.1551
2026-01-17 13:04:41,659: t15.2023.12.17 val PER: 0.2193
2026-01-17 13:04:41,659: t15.2023.12.29 val PER: 0.2121
2026-01-17 13:04:41,659: t15.2024.02.25 val PER: 0.1896
2026-01-17 13:04:41,659: t15.2024.03.08 val PER: 0.2745
2026-01-17 13:04:41,659: t15.2024.03.15 val PER: 0.2545
2026-01-17 13:04:41,659: t15.2024.03.17 val PER: 0.2071
2026-01-17 13:04:41,659: t15.2024.05.10 val PER: 0.2348
2026-01-17 13:04:41,659: t15.2024.06.14 val PER: 0.2177
2026-01-17 13:04:41,659: t15.2024.07.19 val PER: 0.3204
2026-01-17 13:04:41,660: t15.2024.07.21 val PER: 0.1648
2026-01-17 13:04:41,660: t15.2024.07.28 val PER: 0.2074
2026-01-17 13:04:41,660: t15.2025.01.10 val PER: 0.3981
2026-01-17 13:04:41,660: t15.2025.01.12 val PER: 0.2248
2026-01-17 13:04:41,660: t15.2025.03.14 val PER: 0.3624
2026-01-17 13:04:41,660: t15.2025.03.16 val PER: 0.2592
2026-01-17 13:04:41,660: t15.2025.03.30 val PER: 0.3644
2026-01-17 13:04:41,660: t15.2025.04.13 val PER: 0.3039
2026-01-17 13:04:41,661: New best val WER(1gram) 60.66% --> 55.84%
2026-01-17 13:04:41,661: Checkpointing model
2026-01-17 13:04:41,800: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 13:04:50,985: Train batch 5600: loss: 19.58 grad norm: 71.08 time: 0.061
2026-01-17 13:05:09,192: Train batch 5800: loss: 13.05 grad norm: 56.54 time: 0.082
2026-01-17 13:05:27,373: Train batch 6000: loss: 13.74 grad norm: 53.13 time: 0.048
2026-01-17 13:05:27,373: Running test after training batch: 6000
2026-01-17 13:05:27,485: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:05:32,101: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the good at this point is will
2026-01-17 13:05:32,130: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-17 13:05:33,758: Val batch 6000: PER (avg): 0.2077 CTC Loss (avg): 20.5805 WER(1gram): 59.39% (n=64) time: 6.385
2026-01-17 13:05:33,758: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-17 13:05:33,759: t15.2023.08.13 val PER: 0.1788
2026-01-17 13:05:33,759: t15.2023.08.18 val PER: 0.1626
2026-01-17 13:05:33,759: t15.2023.08.20 val PER: 0.1668
2026-01-17 13:05:33,759: t15.2023.08.25 val PER: 0.1235
2026-01-17 13:05:33,759: t15.2023.08.27 val PER: 0.2363
2026-01-17 13:05:33,759: t15.2023.09.01 val PER: 0.1250
2026-01-17 13:05:33,759: t15.2023.09.03 val PER: 0.2185
2026-01-17 13:05:33,759: t15.2023.09.24 val PER: 0.1748
2026-01-17 13:05:33,759: t15.2023.09.29 val PER: 0.1646
2026-01-17 13:05:33,759: t15.2023.10.01 val PER: 0.2180
2026-01-17 13:05:33,759: t15.2023.10.06 val PER: 0.1292
2026-01-17 13:05:33,759: t15.2023.10.08 val PER: 0.2855
2026-01-17 13:05:33,759: t15.2023.10.13 val PER: 0.2653
2026-01-17 13:05:33,760: t15.2023.10.15 val PER: 0.2175
2026-01-17 13:05:33,760: t15.2023.10.20 val PER: 0.2248
2026-01-17 13:05:33,760: t15.2023.10.22 val PER: 0.1704
2026-01-17 13:05:33,760: t15.2023.11.03 val PER: 0.2130
2026-01-17 13:05:33,760: t15.2023.11.04 val PER: 0.0648
2026-01-17 13:05:33,760: t15.2023.11.17 val PER: 0.0778
2026-01-17 13:05:33,760: t15.2023.11.19 val PER: 0.0758
2026-01-17 13:05:33,760: t15.2023.11.26 val PER: 0.2138
2026-01-17 13:05:33,760: t15.2023.12.03 val PER: 0.1733
2026-01-17 13:05:33,760: t15.2023.12.08 val PER: 0.1711
2026-01-17 13:05:33,760: t15.2023.12.10 val PER: 0.1445
2026-01-17 13:05:33,760: t15.2023.12.17 val PER: 0.1944
2026-01-17 13:05:33,760: t15.2023.12.29 val PER: 0.2176
2026-01-17 13:05:33,760: t15.2024.02.25 val PER: 0.1573
2026-01-17 13:05:33,761: t15.2024.03.08 val PER: 0.2760
2026-01-17 13:05:33,761: t15.2024.03.15 val PER: 0.2558
2026-01-17 13:05:33,761: t15.2024.03.17 val PER: 0.2036
2026-01-17 13:05:33,761: t15.2024.05.10 val PER: 0.2199
2026-01-17 13:05:33,761: t15.2024.06.14 val PER: 0.2240
2026-01-17 13:05:33,761: t15.2024.07.19 val PER: 0.3013
2026-01-17 13:05:33,761: t15.2024.07.21 val PER: 0.1545
2026-01-17 13:05:33,761: t15.2024.07.28 val PER: 0.1978
2026-01-17 13:05:33,761: t15.2025.01.10 val PER: 0.3733
2026-01-17 13:05:33,761: t15.2025.01.12 val PER: 0.2240
2026-01-17 13:05:33,761: t15.2025.03.14 val PER: 0.3831
2026-01-17 13:05:33,761: t15.2025.03.16 val PER: 0.2408
2026-01-17 13:05:33,761: t15.2025.03.30 val PER: 0.3552
2026-01-17 13:05:33,761: t15.2025.04.13 val PER: 0.2639
2026-01-17 13:05:50,932: Train batch 6200: loss: 15.66 grad norm: 57.39 time: 0.070
2026-01-17 13:06:08,363: Train batch 6400: loss: 18.25 grad norm: 67.72 time: 0.062
2026-01-17 13:06:16,678: Running test after training batch: 6500
2026-01-17 13:06:16,840: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:06:21,573: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-17 13:06:21,603: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the west jed
2026-01-17 13:06:23,192: Val batch 6500: PER (avg): 0.2015 CTC Loss (avg): 19.8646 WER(1gram): 54.82% (n=64) time: 6.514
2026-01-17 13:06:23,193: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-17 13:06:23,193: t15.2023.08.13 val PER: 0.1715
2026-01-17 13:06:23,193: t15.2023.08.18 val PER: 0.1526
2026-01-17 13:06:23,193: t15.2023.08.20 val PER: 0.1533
2026-01-17 13:06:23,193: t15.2023.08.25 val PER: 0.1099
2026-01-17 13:06:23,193: t15.2023.08.27 val PER: 0.2412
2026-01-17 13:06:23,193: t15.2023.09.01 val PER: 0.1185
2026-01-17 13:06:23,194: t15.2023.09.03 val PER: 0.2031
2026-01-17 13:06:23,194: t15.2023.09.24 val PER: 0.1675
2026-01-17 13:06:23,194: t15.2023.09.29 val PER: 0.1666
2026-01-17 13:06:23,194: t15.2023.10.01 val PER: 0.2219
2026-01-17 13:06:23,194: t15.2023.10.06 val PER: 0.1152
2026-01-17 13:06:23,194: t15.2023.10.08 val PER: 0.2936
2026-01-17 13:06:23,194: t15.2023.10.13 val PER: 0.2614
2026-01-17 13:06:23,194: t15.2023.10.15 val PER: 0.2083
2026-01-17 13:06:23,194: t15.2023.10.20 val PER: 0.2148
2026-01-17 13:06:23,194: t15.2023.10.22 val PER: 0.1615
2026-01-17 13:06:23,195: t15.2023.11.03 val PER: 0.2144
2026-01-17 13:06:23,195: t15.2023.11.04 val PER: 0.0648
2026-01-17 13:06:23,195: t15.2023.11.17 val PER: 0.0529
2026-01-17 13:06:23,195: t15.2023.11.19 val PER: 0.0619
2026-01-17 13:06:23,195: t15.2023.11.26 val PER: 0.2065
2026-01-17 13:06:23,195: t15.2023.12.03 val PER: 0.1618
2026-01-17 13:06:23,195: t15.2023.12.08 val PER: 0.1691
2026-01-17 13:06:23,195: t15.2023.12.10 val PER: 0.1406
2026-01-17 13:06:23,195: t15.2023.12.17 val PER: 0.1881
2026-01-17 13:06:23,196: t15.2023.12.29 val PER: 0.1949
2026-01-17 13:06:23,196: t15.2024.02.25 val PER: 0.1713
2026-01-17 13:06:23,196: t15.2024.03.08 val PER: 0.3001
2026-01-17 13:06:23,196: t15.2024.03.15 val PER: 0.2552
2026-01-17 13:06:23,196: t15.2024.03.17 val PER: 0.2015
2026-01-17 13:06:23,196: t15.2024.05.10 val PER: 0.2065
2026-01-17 13:06:23,196: t15.2024.06.14 val PER: 0.2098
2026-01-17 13:06:23,196: t15.2024.07.19 val PER: 0.2940
2026-01-17 13:06:23,196: t15.2024.07.21 val PER: 0.1483
2026-01-17 13:06:23,196: t15.2024.07.28 val PER: 0.1853
2026-01-17 13:06:23,197: t15.2025.01.10 val PER: 0.3760
2026-01-17 13:06:23,197: t15.2025.01.12 val PER: 0.2048
2026-01-17 13:06:23,197: t15.2025.03.14 val PER: 0.3683
2026-01-17 13:06:23,197: t15.2025.03.16 val PER: 0.2382
2026-01-17 13:06:23,197: t15.2025.03.30 val PER: 0.3460
2026-01-17 13:06:23,197: t15.2025.04.13 val PER: 0.2668
2026-01-17 13:06:23,197: New best val WER(1gram) 55.84% --> 54.82%
2026-01-17 13:06:23,197: Checkpointing model
2026-01-17 13:06:23,339: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 13:06:31,613: Train batch 6600: loss: 11.84 grad norm: 51.67 time: 0.044
2026-01-17 13:06:48,775: Train batch 6800: loss: 15.03 grad norm: 57.99 time: 0.048
2026-01-17 13:07:05,881: Train batch 7000: loss: 16.53 grad norm: 64.92 time: 0.061
2026-01-17 13:07:05,882: Running test after training batch: 7000
2026-01-17 13:07:06,034: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:07:11,053: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-17 13:07:11,081: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost nett
2026-01-17 13:07:12,681: Val batch 7000: PER (avg): 0.1906 CTC Loss (avg): 18.8935 WER(1gram): 53.05% (n=64) time: 6.799
2026-01-17 13:07:12,681: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-17 13:07:12,681: t15.2023.08.13 val PER: 0.1590
2026-01-17 13:07:12,681: t15.2023.08.18 val PER: 0.1408
2026-01-17 13:07:12,682: t15.2023.08.20 val PER: 0.1477
2026-01-17 13:07:12,682: t15.2023.08.25 val PER: 0.0934
2026-01-17 13:07:12,682: t15.2023.08.27 val PER: 0.2122
2026-01-17 13:07:12,682: t15.2023.09.01 val PER: 0.1120
2026-01-17 13:07:12,682: t15.2023.09.03 val PER: 0.1924
2026-01-17 13:07:12,682: t15.2023.09.24 val PER: 0.1602
2026-01-17 13:07:12,682: t15.2023.09.29 val PER: 0.1691
2026-01-17 13:07:12,682: t15.2023.10.01 val PER: 0.2094
2026-01-17 13:07:12,682: t15.2023.10.06 val PER: 0.1033
2026-01-17 13:07:12,682: t15.2023.10.08 val PER: 0.2760
2026-01-17 13:07:12,683: t15.2023.10.13 val PER: 0.2521
2026-01-17 13:07:12,683: t15.2023.10.15 val PER: 0.1833
2026-01-17 13:07:12,683: t15.2023.10.20 val PER: 0.2215
2026-01-17 13:07:12,683: t15.2023.10.22 val PER: 0.1448
2026-01-17 13:07:12,683: t15.2023.11.03 val PER: 0.1974
2026-01-17 13:07:12,683: t15.2023.11.04 val PER: 0.0410
2026-01-17 13:07:12,683: t15.2023.11.17 val PER: 0.0591
2026-01-17 13:07:12,683: t15.2023.11.19 val PER: 0.0539
2026-01-17 13:07:12,683: t15.2023.11.26 val PER: 0.1899
2026-01-17 13:07:12,683: t15.2023.12.03 val PER: 0.1607
2026-01-17 13:07:12,683: t15.2023.12.08 val PER: 0.1565
2026-01-17 13:07:12,683: t15.2023.12.10 val PER: 0.1380
2026-01-17 13:07:12,683: t15.2023.12.17 val PER: 0.1736
2026-01-17 13:07:12,683: t15.2023.12.29 val PER: 0.1819
2026-01-17 13:07:12,684: t15.2024.02.25 val PER: 0.1615
2026-01-17 13:07:12,684: t15.2024.03.08 val PER: 0.2717
2026-01-17 13:07:12,684: t15.2024.03.15 val PER: 0.2364
2026-01-17 13:07:12,684: t15.2024.03.17 val PER: 0.1911
2026-01-17 13:07:12,684: t15.2024.05.10 val PER: 0.2065
2026-01-17 13:07:12,684: t15.2024.06.14 val PER: 0.2019
2026-01-17 13:07:12,684: t15.2024.07.19 val PER: 0.2947
2026-01-17 13:07:12,684: t15.2024.07.21 val PER: 0.1359
2026-01-17 13:07:12,684: t15.2024.07.28 val PER: 0.1735
2026-01-17 13:07:12,684: t15.2025.01.10 val PER: 0.3526
2026-01-17 13:07:12,684: t15.2025.01.12 val PER: 0.1955
2026-01-17 13:07:12,684: t15.2025.03.14 val PER: 0.3595
2026-01-17 13:07:12,684: t15.2025.03.16 val PER: 0.2408
2026-01-17 13:07:12,684: t15.2025.03.30 val PER: 0.3471
2026-01-17 13:07:12,685: t15.2025.04.13 val PER: 0.2525
2026-01-17 13:07:12,685: New best val WER(1gram) 54.82% --> 53.05%
2026-01-17 13:07:12,685: Checkpointing model
2026-01-17 13:07:12,830: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 13:07:29,881: Train batch 7200: loss: 13.51 grad norm: 58.91 time: 0.078
2026-01-17 13:07:46,631: Train batch 7400: loss: 12.62 grad norm: 52.45 time: 0.075
2026-01-17 13:07:55,564: Running test after training batch: 7500
2026-01-17 13:07:55,751: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:08:00,371: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point us will
2026-01-17 13:08:00,399: WER debug example
  GT : how does it keep the cost down
  PR : houde dusk it keep the cussed sent
2026-01-17 13:08:01,989: Val batch 7500: PER (avg): 0.1844 CTC Loss (avg): 18.2477 WER(1gram): 52.03% (n=64) time: 6.424
2026-01-17 13:08:01,990: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-17 13:08:01,990: t15.2023.08.13 val PER: 0.1507
2026-01-17 13:08:01,990: t15.2023.08.18 val PER: 0.1257
2026-01-17 13:08:01,990: t15.2023.08.20 val PER: 0.1390
2026-01-17 13:08:01,990: t15.2023.08.25 val PER: 0.1160
2026-01-17 13:08:01,990: t15.2023.08.27 val PER: 0.2058
2026-01-17 13:08:01,990: t15.2023.09.01 val PER: 0.1120
2026-01-17 13:08:01,990: t15.2023.09.03 val PER: 0.1888
2026-01-17 13:08:01,990: t15.2023.09.24 val PER: 0.1493
2026-01-17 13:08:01,990: t15.2023.09.29 val PER: 0.1583
2026-01-17 13:08:01,990: t15.2023.10.01 val PER: 0.2034
2026-01-17 13:08:01,990: t15.2023.10.06 val PER: 0.1087
2026-01-17 13:08:01,991: t15.2023.10.08 val PER: 0.2679
2026-01-17 13:08:01,991: t15.2023.10.13 val PER: 0.2483
2026-01-17 13:08:01,991: t15.2023.10.15 val PER: 0.1918
2026-01-17 13:08:01,991: t15.2023.10.20 val PER: 0.1913
2026-01-17 13:08:01,991: t15.2023.10.22 val PER: 0.1448
2026-01-17 13:08:01,991: t15.2023.11.03 val PER: 0.1974
2026-01-17 13:08:01,991: t15.2023.11.04 val PER: 0.0512
2026-01-17 13:08:01,991: t15.2023.11.17 val PER: 0.0591
2026-01-17 13:08:01,991: t15.2023.11.19 val PER: 0.0499
2026-01-17 13:08:01,991: t15.2023.11.26 val PER: 0.1775
2026-01-17 13:08:01,991: t15.2023.12.03 val PER: 0.1513
2026-01-17 13:08:01,991: t15.2023.12.08 val PER: 0.1418
2026-01-17 13:08:01,991: t15.2023.12.10 val PER: 0.1275
2026-01-17 13:08:01,991: t15.2023.12.17 val PER: 0.1736
2026-01-17 13:08:01,991: t15.2023.12.29 val PER: 0.1764
2026-01-17 13:08:01,992: t15.2024.02.25 val PER: 0.1447
2026-01-17 13:08:01,992: t15.2024.03.08 val PER: 0.2603
2026-01-17 13:08:01,992: t15.2024.03.15 val PER: 0.2320
2026-01-17 13:08:01,992: t15.2024.03.17 val PER: 0.1722
2026-01-17 13:08:01,992: t15.2024.05.10 val PER: 0.2021
2026-01-17 13:08:01,992: t15.2024.06.14 val PER: 0.1972
2026-01-17 13:08:01,992: t15.2024.07.19 val PER: 0.2795
2026-01-17 13:08:01,992: t15.2024.07.21 val PER: 0.1310
2026-01-17 13:08:01,992: t15.2024.07.28 val PER: 0.1691
2026-01-17 13:08:01,992: t15.2025.01.10 val PER: 0.3430
2026-01-17 13:08:01,992: t15.2025.01.12 val PER: 0.1886
2026-01-17 13:08:01,992: t15.2025.03.14 val PER: 0.3787
2026-01-17 13:08:01,993: t15.2025.03.16 val PER: 0.2160
2026-01-17 13:08:01,993: t15.2025.03.30 val PER: 0.3402
2026-01-17 13:08:01,993: t15.2025.04.13 val PER: 0.2439
2026-01-17 13:08:01,994: New best val WER(1gram) 53.05% --> 52.03%
2026-01-17 13:08:01,994: Checkpointing model
2026-01-17 13:08:02,139: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 13:08:10,748: Train batch 7600: loss: 15.46 grad norm: 60.12 time: 0.068
2026-01-17 13:08:28,149: Train batch 7800: loss: 13.66 grad norm: 59.35 time: 0.055
2026-01-17 13:08:46,172: Train batch 8000: loss: 10.30 grad norm: 48.01 time: 0.071
2026-01-17 13:08:46,172: Running test after training batch: 8000
2026-01-17 13:08:46,263: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:08:51,359: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-17 13:08:51,389: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost did
2026-01-17 13:08:53,068: Val batch 8000: PER (avg): 0.1799 CTC Loss (avg): 17.8346 WER(1gram): 55.58% (n=64) time: 6.896
2026-01-17 13:08:53,068: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-17 13:08:53,073: t15.2023.08.13 val PER: 0.1424
2026-01-17 13:08:53,074: t15.2023.08.18 val PER: 0.1165
2026-01-17 13:08:53,074: t15.2023.08.20 val PER: 0.1326
2026-01-17 13:08:53,074: t15.2023.08.25 val PER: 0.1205
2026-01-17 13:08:53,074: t15.2023.08.27 val PER: 0.2170
2026-01-17 13:08:53,074: t15.2023.09.01 val PER: 0.0998
2026-01-17 13:08:53,074: t15.2023.09.03 val PER: 0.1817
2026-01-17 13:08:53,074: t15.2023.09.24 val PER: 0.1456
2026-01-17 13:08:53,074: t15.2023.09.29 val PER: 0.1512
2026-01-17 13:08:53,074: t15.2023.10.01 val PER: 0.1988
2026-01-17 13:08:53,074: t15.2023.10.06 val PER: 0.1216
2026-01-17 13:08:53,074: t15.2023.10.08 val PER: 0.2828
2026-01-17 13:08:53,074: t15.2023.10.13 val PER: 0.2265
2026-01-17 13:08:53,074: t15.2023.10.15 val PER: 0.1826
2026-01-17 13:08:53,074: t15.2023.10.20 val PER: 0.2114
2026-01-17 13:08:53,074: t15.2023.10.22 val PER: 0.1414
2026-01-17 13:08:53,075: t15.2023.11.03 val PER: 0.1995
2026-01-17 13:08:53,075: t15.2023.11.04 val PER: 0.0375
2026-01-17 13:08:53,075: t15.2023.11.17 val PER: 0.0560
2026-01-17 13:08:53,075: t15.2023.11.19 val PER: 0.0579
2026-01-17 13:08:53,075: t15.2023.11.26 val PER: 0.1725
2026-01-17 13:08:53,075: t15.2023.12.03 val PER: 0.1607
2026-01-17 13:08:53,075: t15.2023.12.08 val PER: 0.1398
2026-01-17 13:08:53,075: t15.2023.12.10 val PER: 0.1209
2026-01-17 13:08:53,075: t15.2023.12.17 val PER: 0.1684
2026-01-17 13:08:53,075: t15.2023.12.29 val PER: 0.1716
2026-01-17 13:08:53,075: t15.2024.02.25 val PER: 0.1334
2026-01-17 13:08:53,075: t15.2024.03.08 val PER: 0.2717
2026-01-17 13:08:53,075: t15.2024.03.15 val PER: 0.2301
2026-01-17 13:08:53,075: t15.2024.03.17 val PER: 0.1695
2026-01-17 13:08:53,076: t15.2024.05.10 val PER: 0.1768
2026-01-17 13:08:53,076: t15.2024.06.14 val PER: 0.2066
2026-01-17 13:08:53,076: t15.2024.07.19 val PER: 0.2775
2026-01-17 13:08:53,076: t15.2024.07.21 val PER: 0.1186
2026-01-17 13:08:53,076: t15.2024.07.28 val PER: 0.1603
2026-01-17 13:08:53,076: t15.2025.01.10 val PER: 0.3182
2026-01-17 13:08:53,076: t15.2025.01.12 val PER: 0.1778
2026-01-17 13:08:53,076: t15.2025.03.14 val PER: 0.3713
2026-01-17 13:08:53,076: t15.2025.03.16 val PER: 0.2094
2026-01-17 13:08:53,076: t15.2025.03.30 val PER: 0.3356
2026-01-17 13:08:53,076: t15.2025.04.13 val PER: 0.2511
2026-01-17 13:09:09,998: Train batch 8200: loss: 9.15 grad norm: 48.27 time: 0.054
2026-01-17 13:09:26,997: Train batch 8400: loss: 9.23 grad norm: 46.77 time: 0.063
2026-01-17 13:09:35,561: Running test after training batch: 8500
2026-01-17 13:09:35,655: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:09:40,822: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-17 13:09:40,853: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost ned
2026-01-17 13:09:42,509: Val batch 8500: PER (avg): 0.1739 CTC Loss (avg): 17.1923 WER(1gram): 48.22% (n=64) time: 6.948
2026-01-17 13:09:42,509: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-17 13:09:42,509: t15.2023.08.13 val PER: 0.1341
2026-01-17 13:09:42,510: t15.2023.08.18 val PER: 0.1291
2026-01-17 13:09:42,510: t15.2023.08.20 val PER: 0.1326
2026-01-17 13:09:42,510: t15.2023.08.25 val PER: 0.1175
2026-01-17 13:09:42,510: t15.2023.08.27 val PER: 0.2010
2026-01-17 13:09:42,510: t15.2023.09.01 val PER: 0.0958
2026-01-17 13:09:42,510: t15.2023.09.03 val PER: 0.1900
2026-01-17 13:09:42,510: t15.2023.09.24 val PER: 0.1493
2026-01-17 13:09:42,510: t15.2023.09.29 val PER: 0.1449
2026-01-17 13:09:42,510: t15.2023.10.01 val PER: 0.1935
2026-01-17 13:09:42,510: t15.2023.10.06 val PER: 0.1087
2026-01-17 13:09:42,510: t15.2023.10.08 val PER: 0.2693
2026-01-17 13:09:42,510: t15.2023.10.13 val PER: 0.2265
2026-01-17 13:09:42,510: t15.2023.10.15 val PER: 0.1786
2026-01-17 13:09:42,510: t15.2023.10.20 val PER: 0.1879
2026-01-17 13:09:42,510: t15.2023.10.22 val PER: 0.1414
2026-01-17 13:09:42,511: t15.2023.11.03 val PER: 0.2035
2026-01-17 13:09:42,511: t15.2023.11.04 val PER: 0.0375
2026-01-17 13:09:42,511: t15.2023.11.17 val PER: 0.0498
2026-01-17 13:09:42,511: t15.2023.11.19 val PER: 0.0399
2026-01-17 13:09:42,511: t15.2023.11.26 val PER: 0.1638
2026-01-17 13:09:42,511: t15.2023.12.03 val PER: 0.1439
2026-01-17 13:09:42,511: t15.2023.12.08 val PER: 0.1332
2026-01-17 13:09:42,511: t15.2023.12.10 val PER: 0.1170
2026-01-17 13:09:42,511: t15.2023.12.17 val PER: 0.1549
2026-01-17 13:09:42,511: t15.2023.12.29 val PER: 0.1661
2026-01-17 13:09:42,511: t15.2024.02.25 val PER: 0.1222
2026-01-17 13:09:42,511: t15.2024.03.08 val PER: 0.2504
2026-01-17 13:09:42,511: t15.2024.03.15 val PER: 0.2258
2026-01-17 13:09:42,512: t15.2024.03.17 val PER: 0.1611
2026-01-17 13:09:42,512: t15.2024.05.10 val PER: 0.1783
2026-01-17 13:09:42,512: t15.2024.06.14 val PER: 0.1798
2026-01-17 13:09:42,512: t15.2024.07.19 val PER: 0.2604
2026-01-17 13:09:42,512: t15.2024.07.21 val PER: 0.1207
2026-01-17 13:09:42,512: t15.2024.07.28 val PER: 0.1551
2026-01-17 13:09:42,512: t15.2025.01.10 val PER: 0.3237
2026-01-17 13:09:42,512: t15.2025.01.12 val PER: 0.1801
2026-01-17 13:09:42,512: t15.2025.03.14 val PER: 0.3580
2026-01-17 13:09:42,512: t15.2025.03.16 val PER: 0.2016
2026-01-17 13:09:42,512: t15.2025.03.30 val PER: 0.3253
2026-01-17 13:09:42,512: t15.2025.04.13 val PER: 0.2225
2026-01-17 13:09:42,513: New best val WER(1gram) 52.03% --> 48.22%
2026-01-17 13:09:42,513: Checkpointing model
2026-01-17 13:09:42,655: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 13:09:51,760: Train batch 8600: loss: 15.39 grad norm: 60.65 time: 0.056
2026-01-17 13:10:09,928: Train batch 8800: loss: 13.98 grad norm: 57.99 time: 0.062
2026-01-17 13:10:28,455: Train batch 9000: loss: 14.60 grad norm: 62.98 time: 0.072
2026-01-17 13:10:28,455: Running test after training batch: 9000
2026-01-17 13:10:28,565: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:10:33,210: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-17 13:10:33,240: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost nit
2026-01-17 13:10:34,914: Val batch 9000: PER (avg): 0.1673 CTC Loss (avg): 16.7389 WER(1gram): 51.02% (n=64) time: 6.459
2026-01-17 13:10:34,915: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-17 13:10:34,915: t15.2023.08.13 val PER: 0.1310
2026-01-17 13:10:34,915: t15.2023.08.18 val PER: 0.1199
2026-01-17 13:10:34,915: t15.2023.08.20 val PER: 0.1223
2026-01-17 13:10:34,915: t15.2023.08.25 val PER: 0.0979
2026-01-17 13:10:34,915: t15.2023.08.27 val PER: 0.1961
2026-01-17 13:10:34,916: t15.2023.09.01 val PER: 0.0917
2026-01-17 13:10:34,916: t15.2023.09.03 val PER: 0.1876
2026-01-17 13:10:34,916: t15.2023.09.24 val PER: 0.1396
2026-01-17 13:10:34,916: t15.2023.09.29 val PER: 0.1429
2026-01-17 13:10:34,916: t15.2023.10.01 val PER: 0.1823
2026-01-17 13:10:34,916: t15.2023.10.06 val PER: 0.0883
2026-01-17 13:10:34,916: t15.2023.10.08 val PER: 0.2503
2026-01-17 13:10:34,916: t15.2023.10.13 val PER: 0.2126
2026-01-17 13:10:34,916: t15.2023.10.15 val PER: 0.1655
2026-01-17 13:10:34,917: t15.2023.10.20 val PER: 0.2148
2026-01-17 13:10:34,917: t15.2023.10.22 val PER: 0.1269
2026-01-17 13:10:34,917: t15.2023.11.03 val PER: 0.2022
2026-01-17 13:10:34,917: t15.2023.11.04 val PER: 0.0410
2026-01-17 13:10:34,917: t15.2023.11.17 val PER: 0.0498
2026-01-17 13:10:34,917: t15.2023.11.19 val PER: 0.0399
2026-01-17 13:10:34,917: t15.2023.11.26 val PER: 0.1551
2026-01-17 13:10:34,917: t15.2023.12.03 val PER: 0.1303
2026-01-17 13:10:34,917: t15.2023.12.08 val PER: 0.1205
2026-01-17 13:10:34,917: t15.2023.12.10 val PER: 0.1143
2026-01-17 13:10:34,917: t15.2023.12.17 val PER: 0.1538
2026-01-17 13:10:34,917: t15.2023.12.29 val PER: 0.1537
2026-01-17 13:10:34,918: t15.2024.02.25 val PER: 0.1278
2026-01-17 13:10:34,918: t15.2024.03.08 val PER: 0.2461
2026-01-17 13:10:34,918: t15.2024.03.15 val PER: 0.2258
2026-01-17 13:10:34,918: t15.2024.03.17 val PER: 0.1590
2026-01-17 13:10:34,918: t15.2024.05.10 val PER: 0.1738
2026-01-17 13:10:34,918: t15.2024.06.14 val PER: 0.1751
2026-01-17 13:10:34,918: t15.2024.07.19 val PER: 0.2650
2026-01-17 13:10:34,919: t15.2024.07.21 val PER: 0.1117
2026-01-17 13:10:34,919: t15.2024.07.28 val PER: 0.1537
2026-01-17 13:10:34,919: t15.2025.01.10 val PER: 0.3058
2026-01-17 13:10:34,919: t15.2025.01.12 val PER: 0.1624
2026-01-17 13:10:34,919: t15.2025.03.14 val PER: 0.3343
2026-01-17 13:10:34,919: t15.2025.03.16 val PER: 0.2147
2026-01-17 13:10:34,919: t15.2025.03.30 val PER: 0.3161
2026-01-17 13:10:34,920: t15.2025.04.13 val PER: 0.2368
2026-01-17 13:10:52,430: Train batch 9200: loss: 9.75 grad norm: 49.21 time: 0.055
2026-01-17 13:11:09,347: Train batch 9400: loss: 6.26 grad norm: 39.40 time: 0.067
2026-01-17 13:11:17,845: Running test after training batch: 9500
2026-01-17 13:11:18,008: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:11:22,975: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-17 13:11:23,004: WER debug example
  GT : how does it keep the cost down
  PR : houde dusts it keep the cost it
2026-01-17 13:11:24,657: Val batch 9500: PER (avg): 0.1657 CTC Loss (avg): 16.6091 WER(1gram): 48.98% (n=64) time: 6.812
2026-01-17 13:11:24,657: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-17 13:11:24,658: t15.2023.08.13 val PER: 0.1227
2026-01-17 13:11:24,658: t15.2023.08.18 val PER: 0.1148
2026-01-17 13:11:24,658: t15.2023.08.20 val PER: 0.1168
2026-01-17 13:11:24,658: t15.2023.08.25 val PER: 0.1145
2026-01-17 13:11:24,658: t15.2023.08.27 val PER: 0.1849
2026-01-17 13:11:24,658: t15.2023.09.01 val PER: 0.0966
2026-01-17 13:11:24,658: t15.2023.09.03 val PER: 0.1675
2026-01-17 13:11:24,658: t15.2023.09.24 val PER: 0.1335
2026-01-17 13:11:24,658: t15.2023.09.29 val PER: 0.1372
2026-01-17 13:11:24,658: t15.2023.10.01 val PER: 0.1909
2026-01-17 13:11:24,658: t15.2023.10.06 val PER: 0.1012
2026-01-17 13:11:24,658: t15.2023.10.08 val PER: 0.2571
2026-01-17 13:11:24,658: t15.2023.10.13 val PER: 0.2172
2026-01-17 13:11:24,659: t15.2023.10.15 val PER: 0.1793
2026-01-17 13:11:24,659: t15.2023.10.20 val PER: 0.1913
2026-01-17 13:11:24,659: t15.2023.10.22 val PER: 0.1281
2026-01-17 13:11:24,659: t15.2023.11.03 val PER: 0.1947
2026-01-17 13:11:24,659: t15.2023.11.04 val PER: 0.0341
2026-01-17 13:11:24,659: t15.2023.11.17 val PER: 0.0529
2026-01-17 13:11:24,659: t15.2023.11.19 val PER: 0.0539
2026-01-17 13:11:24,659: t15.2023.11.26 val PER: 0.1384
2026-01-17 13:11:24,659: t15.2023.12.03 val PER: 0.1387
2026-01-17 13:11:24,659: t15.2023.12.08 val PER: 0.1252
2026-01-17 13:11:24,659: t15.2023.12.10 val PER: 0.1064
2026-01-17 13:11:24,659: t15.2023.12.17 val PER: 0.1518
2026-01-17 13:11:24,659: t15.2023.12.29 val PER: 0.1386
2026-01-17 13:11:24,659: t15.2024.02.25 val PER: 0.1222
2026-01-17 13:11:24,659: t15.2024.03.08 val PER: 0.2404
2026-01-17 13:11:24,659: t15.2024.03.15 val PER: 0.2170
2026-01-17 13:11:24,659: t15.2024.03.17 val PER: 0.1562
2026-01-17 13:11:24,660: t15.2024.05.10 val PER: 0.1857
2026-01-17 13:11:24,660: t15.2024.06.14 val PER: 0.1719
2026-01-17 13:11:24,660: t15.2024.07.19 val PER: 0.2518
2026-01-17 13:11:24,660: t15.2024.07.21 val PER: 0.1117
2026-01-17 13:11:24,660: t15.2024.07.28 val PER: 0.1581
2026-01-17 13:11:24,660: t15.2025.01.10 val PER: 0.3017
2026-01-17 13:11:24,660: t15.2025.01.12 val PER: 0.1763
2026-01-17 13:11:24,660: t15.2025.03.14 val PER: 0.3713
2026-01-17 13:11:24,661: t15.2025.03.16 val PER: 0.1937
2026-01-17 13:11:24,661: t15.2025.03.30 val PER: 0.3011
2026-01-17 13:11:24,661: t15.2025.04.13 val PER: 0.2254
2026-01-17 13:11:33,749: Train batch 9600: loss: 7.63 grad norm: 45.32 time: 0.075
2026-01-17 13:11:53,318: Train batch 9800: loss: 10.29 grad norm: 53.34 time: 0.063
2026-01-17 13:12:11,958: Train batch 10000: loss: 4.40 grad norm: 30.34 time: 0.061
2026-01-17 13:12:11,959: Running test after training batch: 10000
2026-01-17 13:12:12,100: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:12:16,722: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-17 13:12:16,752: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-17 13:12:18,445: Val batch 10000: PER (avg): 0.1639 CTC Loss (avg): 16.6385 WER(1gram): 51.27% (n=64) time: 6.486
2026-01-17 13:12:18,445: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-17 13:12:18,446: t15.2023.08.13 val PER: 0.1206
2026-01-17 13:12:18,446: t15.2023.08.18 val PER: 0.1157
2026-01-17 13:12:18,446: t15.2023.08.20 val PER: 0.1215
2026-01-17 13:12:18,446: t15.2023.08.25 val PER: 0.1145
2026-01-17 13:12:18,446: t15.2023.08.27 val PER: 0.1994
2026-01-17 13:12:18,446: t15.2023.09.01 val PER: 0.0893
2026-01-17 13:12:18,446: t15.2023.09.03 val PER: 0.1817
2026-01-17 13:12:18,446: t15.2023.09.24 val PER: 0.1359
2026-01-17 13:12:18,446: t15.2023.09.29 val PER: 0.1449
2026-01-17 13:12:18,446: t15.2023.10.01 val PER: 0.1915
2026-01-17 13:12:18,446: t15.2023.10.06 val PER: 0.1098
2026-01-17 13:12:18,446: t15.2023.10.08 val PER: 0.2585
2026-01-17 13:12:18,446: t15.2023.10.13 val PER: 0.2180
2026-01-17 13:12:18,447: t15.2023.10.15 val PER: 0.1628
2026-01-17 13:12:18,447: t15.2023.10.20 val PER: 0.2013
2026-01-17 13:12:18,447: t15.2023.10.22 val PER: 0.1359
2026-01-17 13:12:18,447: t15.2023.11.03 val PER: 0.1947
2026-01-17 13:12:18,447: t15.2023.11.04 val PER: 0.0410
2026-01-17 13:12:18,447: t15.2023.11.17 val PER: 0.0498
2026-01-17 13:12:18,447: t15.2023.11.19 val PER: 0.0359
2026-01-17 13:12:18,447: t15.2023.11.26 val PER: 0.1413
2026-01-17 13:12:18,448: t15.2023.12.03 val PER: 0.1250
2026-01-17 13:12:18,448: t15.2023.12.08 val PER: 0.1265
2026-01-17 13:12:18,448: t15.2023.12.10 val PER: 0.1051
2026-01-17 13:12:18,448: t15.2023.12.17 val PER: 0.1507
2026-01-17 13:12:18,448: t15.2023.12.29 val PER: 0.1283
2026-01-17 13:12:18,448: t15.2024.02.25 val PER: 0.1222
2026-01-17 13:12:18,448: t15.2024.03.08 val PER: 0.2290
2026-01-17 13:12:18,448: t15.2024.03.15 val PER: 0.2145
2026-01-17 13:12:18,448: t15.2024.03.17 val PER: 0.1548
2026-01-17 13:12:18,448: t15.2024.05.10 val PER: 0.1620
2026-01-17 13:12:18,448: t15.2024.06.14 val PER: 0.1735
2026-01-17 13:12:18,448: t15.2024.07.19 val PER: 0.2564
2026-01-17 13:12:18,448: t15.2024.07.21 val PER: 0.1007
2026-01-17 13:12:18,448: t15.2024.07.28 val PER: 0.1478
2026-01-17 13:12:18,448: t15.2025.01.10 val PER: 0.3072
2026-01-17 13:12:18,453: t15.2025.01.12 val PER: 0.1617
2026-01-17 13:12:18,453: t15.2025.03.14 val PER: 0.3609
2026-01-17 13:12:18,453: t15.2025.03.16 val PER: 0.1976
2026-01-17 13:12:18,453: t15.2025.03.30 val PER: 0.3103
2026-01-17 13:12:18,454: t15.2025.04.13 val PER: 0.2183
2026-01-17 13:12:36,123: Train batch 10200: loss: 5.70 grad norm: 38.45 time: 0.050
2026-01-17 13:12:53,930: Train batch 10400: loss: 7.36 grad norm: 44.13 time: 0.071
2026-01-17 13:13:02,779: Running test after training batch: 10500
2026-01-17 13:13:02,903: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:13:08,034: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-17 13:13:08,063: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-17 13:13:09,748: Val batch 10500: PER (avg): 0.1591 CTC Loss (avg): 16.2902 WER(1gram): 47.46% (n=64) time: 6.969
2026-01-17 13:13:09,748: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-17 13:13:09,748: t15.2023.08.13 val PER: 0.1216
2026-01-17 13:13:09,748: t15.2023.08.18 val PER: 0.1174
2026-01-17 13:13:09,749: t15.2023.08.20 val PER: 0.1128
2026-01-17 13:13:09,749: t15.2023.08.25 val PER: 0.1190
2026-01-17 13:13:09,749: t15.2023.08.27 val PER: 0.1881
2026-01-17 13:13:09,749: t15.2023.09.01 val PER: 0.0909
2026-01-17 13:13:09,749: t15.2023.09.03 val PER: 0.1734
2026-01-17 13:13:09,749: t15.2023.09.24 val PER: 0.1408
2026-01-17 13:13:09,749: t15.2023.09.29 val PER: 0.1474
2026-01-17 13:13:09,749: t15.2023.10.01 val PER: 0.1836
2026-01-17 13:13:09,749: t15.2023.10.06 val PER: 0.0883
2026-01-17 13:13:09,750: t15.2023.10.08 val PER: 0.2490
2026-01-17 13:13:09,750: t15.2023.10.13 val PER: 0.2110
2026-01-17 13:13:09,750: t15.2023.10.15 val PER: 0.1622
2026-01-17 13:13:09,750: t15.2023.10.20 val PER: 0.2081
2026-01-17 13:13:09,750: t15.2023.10.22 val PER: 0.1214
2026-01-17 13:13:09,750: t15.2023.11.03 val PER: 0.1900
2026-01-17 13:13:09,750: t15.2023.11.04 val PER: 0.0307
2026-01-17 13:13:09,750: t15.2023.11.17 val PER: 0.0435
2026-01-17 13:13:09,750: t15.2023.11.19 val PER: 0.0439
2026-01-17 13:13:09,750: t15.2023.11.26 val PER: 0.1304
2026-01-17 13:13:09,750: t15.2023.12.03 val PER: 0.1197
2026-01-17 13:13:09,750: t15.2023.12.08 val PER: 0.1105
2026-01-17 13:13:09,751: t15.2023.12.10 val PER: 0.0933
2026-01-17 13:13:09,751: t15.2023.12.17 val PER: 0.1310
2026-01-17 13:13:09,751: t15.2023.12.29 val PER: 0.1462
2026-01-17 13:13:09,751: t15.2024.02.25 val PER: 0.1194
2026-01-17 13:13:09,751: t15.2024.03.08 val PER: 0.2447
2026-01-17 13:13:09,751: t15.2024.03.15 val PER: 0.2145
2026-01-17 13:13:09,751: t15.2024.03.17 val PER: 0.1444
2026-01-17 13:13:09,751: t15.2024.05.10 val PER: 0.1724
2026-01-17 13:13:09,751: t15.2024.06.14 val PER: 0.1609
2026-01-17 13:13:09,751: t15.2024.07.19 val PER: 0.2393
2026-01-17 13:13:09,751: t15.2024.07.21 val PER: 0.0952
2026-01-17 13:13:09,751: t15.2024.07.28 val PER: 0.1309
2026-01-17 13:13:09,751: t15.2025.01.10 val PER: 0.3127
2026-01-17 13:13:09,751: t15.2025.01.12 val PER: 0.1470
2026-01-17 13:13:09,752: t15.2025.03.14 val PER: 0.3565
2026-01-17 13:13:09,752: t15.2025.03.16 val PER: 0.2003
2026-01-17 13:13:09,752: t15.2025.03.30 val PER: 0.3138
2026-01-17 13:13:09,752: t15.2025.04.13 val PER: 0.2183
2026-01-17 13:13:09,752: New best val WER(1gram) 48.22% --> 47.46%
2026-01-17 13:13:09,753: Checkpointing model
2026-01-17 13:13:09,899: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 13:13:19,004: Train batch 10600: loss: 7.64 grad norm: 53.27 time: 0.071
2026-01-17 13:13:36,313: Train batch 10800: loss: 13.15 grad norm: 62.66 time: 0.063
2026-01-17 13:13:53,978: Train batch 11000: loss: 12.03 grad norm: 63.17 time: 0.057
2026-01-17 13:13:53,979: Running test after training batch: 11000
2026-01-17 13:13:54,140: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:13:58,731: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-17 13:13:58,761: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost sette
2026-01-17 13:14:00,431: Val batch 11000: PER (avg): 0.1544 CTC Loss (avg): 15.9815 WER(1gram): 46.70% (n=64) time: 6.452
2026-01-17 13:14:00,432: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-17 13:14:00,432: t15.2023.08.13 val PER: 0.1123
2026-01-17 13:14:00,432: t15.2023.08.18 val PER: 0.1165
2026-01-17 13:14:00,432: t15.2023.08.20 val PER: 0.1064
2026-01-17 13:14:00,432: t15.2023.08.25 val PER: 0.0979
2026-01-17 13:14:00,432: t15.2023.08.27 val PER: 0.1865
2026-01-17 13:14:00,432: t15.2023.09.01 val PER: 0.0787
2026-01-17 13:14:00,432: t15.2023.09.03 val PER: 0.1663
2026-01-17 13:14:00,432: t15.2023.09.24 val PER: 0.1359
2026-01-17 13:14:00,432: t15.2023.09.29 val PER: 0.1449
2026-01-17 13:14:00,432: t15.2023.10.01 val PER: 0.1988
2026-01-17 13:14:00,432: t15.2023.10.06 val PER: 0.0893
2026-01-17 13:14:00,433: t15.2023.10.08 val PER: 0.2558
2026-01-17 13:14:00,433: t15.2023.10.13 val PER: 0.2040
2026-01-17 13:14:00,433: t15.2023.10.15 val PER: 0.1496
2026-01-17 13:14:00,433: t15.2023.10.20 val PER: 0.2047
2026-01-17 13:14:00,433: t15.2023.10.22 val PER: 0.1125
2026-01-17 13:14:00,433: t15.2023.11.03 val PER: 0.1818
2026-01-17 13:14:00,433: t15.2023.11.04 val PER: 0.0307
2026-01-17 13:14:00,433: t15.2023.11.17 val PER: 0.0435
2026-01-17 13:14:00,433: t15.2023.11.19 val PER: 0.0339
2026-01-17 13:14:00,433: t15.2023.11.26 val PER: 0.1246
2026-01-17 13:14:00,433: t15.2023.12.03 val PER: 0.1113
2026-01-17 13:14:00,433: t15.2023.12.08 val PER: 0.1032
2026-01-17 13:14:00,433: t15.2023.12.10 val PER: 0.0920
2026-01-17 13:14:00,433: t15.2023.12.17 val PER: 0.1268
2026-01-17 13:14:00,433: t15.2023.12.29 val PER: 0.1332
2026-01-17 13:14:00,434: t15.2024.02.25 val PER: 0.1250
2026-01-17 13:14:00,434: t15.2024.03.08 val PER: 0.2319
2026-01-17 13:14:00,434: t15.2024.03.15 val PER: 0.2101
2026-01-17 13:14:00,434: t15.2024.03.17 val PER: 0.1457
2026-01-17 13:14:00,434: t15.2024.05.10 val PER: 0.1560
2026-01-17 13:14:00,434: t15.2024.06.14 val PER: 0.1688
2026-01-17 13:14:00,434: t15.2024.07.19 val PER: 0.2373
2026-01-17 13:14:00,434: t15.2024.07.21 val PER: 0.0972
2026-01-17 13:14:00,434: t15.2024.07.28 val PER: 0.1412
2026-01-17 13:14:00,434: t15.2025.01.10 val PER: 0.3099
2026-01-17 13:14:00,434: t15.2025.01.12 val PER: 0.1432
2026-01-17 13:14:00,434: t15.2025.03.14 val PER: 0.3491
2026-01-17 13:14:00,434: t15.2025.03.16 val PER: 0.1898
2026-01-17 13:14:00,434: t15.2025.03.30 val PER: 0.2839
2026-01-17 13:14:00,434: t15.2025.04.13 val PER: 0.2083
2026-01-17 13:14:00,436: New best val WER(1gram) 47.46% --> 46.70%
2026-01-17 13:14:00,436: Checkpointing model
2026-01-17 13:14:00,587: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 13:14:17,559: Train batch 11200: loss: 9.68 grad norm: 53.04 time: 0.071
2026-01-17 13:14:34,660: Train batch 11400: loss: 7.63 grad norm: 50.26 time: 0.057
2026-01-17 13:14:44,191: Running test after training batch: 11500
2026-01-17 13:14:44,305: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:14:48,953: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-17 13:14:48,983: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost sette
2026-01-17 13:14:50,650: Val batch 11500: PER (avg): 0.1531 CTC Loss (avg): 16.1969 WER(1gram): 49.49% (n=64) time: 6.459
2026-01-17 13:14:50,650: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-17 13:14:50,651: t15.2023.08.13 val PER: 0.1185
2026-01-17 13:14:50,651: t15.2023.08.18 val PER: 0.1031
2026-01-17 13:14:50,651: t15.2023.08.20 val PER: 0.1183
2026-01-17 13:14:50,651: t15.2023.08.25 val PER: 0.1039
2026-01-17 13:14:50,651: t15.2023.08.27 val PER: 0.1833
2026-01-17 13:14:50,651: t15.2023.09.01 val PER: 0.0869
2026-01-17 13:14:50,651: t15.2023.09.03 val PER: 0.1591
2026-01-17 13:14:50,651: t15.2023.09.24 val PER: 0.1371
2026-01-17 13:14:50,651: t15.2023.09.29 val PER: 0.1315
2026-01-17 13:14:50,651: t15.2023.10.01 val PER: 0.1790
2026-01-17 13:14:50,651: t15.2023.10.06 val PER: 0.0926
2026-01-17 13:14:50,652: t15.2023.10.08 val PER: 0.2476
2026-01-17 13:14:50,652: t15.2023.10.13 val PER: 0.1939
2026-01-17 13:14:50,652: t15.2023.10.15 val PER: 0.1430
2026-01-17 13:14:50,652: t15.2023.10.20 val PER: 0.1812
2026-01-17 13:14:50,652: t15.2023.10.22 val PER: 0.1169
2026-01-17 13:14:50,652: t15.2023.11.03 val PER: 0.1845
2026-01-17 13:14:50,652: t15.2023.11.04 val PER: 0.0341
2026-01-17 13:14:50,652: t15.2023.11.17 val PER: 0.0404
2026-01-17 13:14:50,652: t15.2023.11.19 val PER: 0.0319
2026-01-17 13:14:50,652: t15.2023.11.26 val PER: 0.1123
2026-01-17 13:14:50,652: t15.2023.12.03 val PER: 0.1103
2026-01-17 13:14:50,652: t15.2023.12.08 val PER: 0.0979
2026-01-17 13:14:50,652: t15.2023.12.10 val PER: 0.0933
2026-01-17 13:14:50,652: t15.2023.12.17 val PER: 0.1497
2026-01-17 13:14:50,652: t15.2023.12.29 val PER: 0.1318
2026-01-17 13:14:50,653: t15.2024.02.25 val PER: 0.1166
2026-01-17 13:14:50,653: t15.2024.03.08 val PER: 0.2333
2026-01-17 13:14:50,653: t15.2024.03.15 val PER: 0.2095
2026-01-17 13:14:50,653: t15.2024.03.17 val PER: 0.1423
2026-01-17 13:14:50,653: t15.2024.05.10 val PER: 0.1560
2026-01-17 13:14:50,653: t15.2024.06.14 val PER: 0.1672
2026-01-17 13:14:50,653: t15.2024.07.19 val PER: 0.2452
2026-01-17 13:14:50,653: t15.2024.07.21 val PER: 0.0972
2026-01-17 13:14:50,653: t15.2024.07.28 val PER: 0.1368
2026-01-17 13:14:50,653: t15.2025.01.10 val PER: 0.3237
2026-01-17 13:14:50,653: t15.2025.01.12 val PER: 0.1470
2026-01-17 13:14:50,653: t15.2025.03.14 val PER: 0.3506
2026-01-17 13:14:50,653: t15.2025.03.16 val PER: 0.1793
2026-01-17 13:14:50,653: t15.2025.03.30 val PER: 0.2966
2026-01-17 13:14:50,653: t15.2025.04.13 val PER: 0.2126
2026-01-17 13:14:59,313: Train batch 11600: loss: 8.94 grad norm: 46.94 time: 0.062
2026-01-17 13:15:16,995: Train batch 11800: loss: 5.76 grad norm: 38.76 time: 0.044
2026-01-17 13:15:34,772: Train batch 12000: loss: 11.64 grad norm: 51.51 time: 0.070
2026-01-17 13:15:34,773: Running test after training batch: 12000
2026-01-17 13:15:34,860: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:15:40,207: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-17 13:15:40,236: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-17 13:15:41,940: Val batch 12000: PER (avg): 0.1526 CTC Loss (avg): 16.0637 WER(1gram): 50.51% (n=64) time: 7.167
2026-01-17 13:15:41,941: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-17 13:15:41,941: t15.2023.08.13 val PER: 0.1206
2026-01-17 13:15:41,941: t15.2023.08.18 val PER: 0.1073
2026-01-17 13:15:41,941: t15.2023.08.20 val PER: 0.1080
2026-01-17 13:15:41,941: t15.2023.08.25 val PER: 0.0964
2026-01-17 13:15:41,941: t15.2023.08.27 val PER: 0.1881
2026-01-17 13:15:41,941: t15.2023.09.01 val PER: 0.0795
2026-01-17 13:15:41,941: t15.2023.09.03 val PER: 0.1639
2026-01-17 13:15:41,941: t15.2023.09.24 val PER: 0.1177
2026-01-17 13:15:41,942: t15.2023.09.29 val PER: 0.1308
2026-01-17 13:15:41,942: t15.2023.10.01 val PER: 0.1816
2026-01-17 13:15:41,942: t15.2023.10.06 val PER: 0.0850
2026-01-17 13:15:41,942: t15.2023.10.08 val PER: 0.2585
2026-01-17 13:15:41,942: t15.2023.10.13 val PER: 0.2048
2026-01-17 13:15:41,942: t15.2023.10.15 val PER: 0.1490
2026-01-17 13:15:41,942: t15.2023.10.20 val PER: 0.1913
2026-01-17 13:15:41,942: t15.2023.10.22 val PER: 0.1192
2026-01-17 13:15:41,942: t15.2023.11.03 val PER: 0.1798
2026-01-17 13:15:41,942: t15.2023.11.04 val PER: 0.0341
2026-01-17 13:15:41,943: t15.2023.11.17 val PER: 0.0358
2026-01-17 13:15:41,943: t15.2023.11.19 val PER: 0.0259
2026-01-17 13:15:41,943: t15.2023.11.26 val PER: 0.1261
2026-01-17 13:15:41,943: t15.2023.12.03 val PER: 0.1092
2026-01-17 13:15:41,943: t15.2023.12.08 val PER: 0.1025
2026-01-17 13:15:41,943: t15.2023.12.10 val PER: 0.0854
2026-01-17 13:15:41,943: t15.2023.12.17 val PER: 0.1455
2026-01-17 13:15:41,943: t15.2023.12.29 val PER: 0.1338
2026-01-17 13:15:41,943: t15.2024.02.25 val PER: 0.1166
2026-01-17 13:15:41,943: t15.2024.03.08 val PER: 0.2191
2026-01-17 13:15:41,943: t15.2024.03.15 val PER: 0.2026
2026-01-17 13:15:41,943: t15.2024.03.17 val PER: 0.1409
2026-01-17 13:15:41,943: t15.2024.05.10 val PER: 0.1545
2026-01-17 13:15:41,944: t15.2024.06.14 val PER: 0.1767
2026-01-17 13:15:41,944: t15.2024.07.19 val PER: 0.2340
2026-01-17 13:15:41,944: t15.2024.07.21 val PER: 0.1028
2026-01-17 13:15:41,944: t15.2024.07.28 val PER: 0.1434
2026-01-17 13:15:41,944: t15.2025.01.10 val PER: 0.3140
2026-01-17 13:15:41,944: t15.2025.01.12 val PER: 0.1370
2026-01-17 13:15:41,944: t15.2025.03.14 val PER: 0.3417
2026-01-17 13:15:41,944: t15.2025.03.16 val PER: 0.1990
2026-01-17 13:15:41,944: t15.2025.03.30 val PER: 0.2954
2026-01-17 13:15:41,944: t15.2025.04.13 val PER: 0.2168
2026-01-17 13:15:58,858: Train batch 12200: loss: 4.60 grad norm: 40.38 time: 0.064
2026-01-17 13:16:16,039: Train batch 12400: loss: 4.17 grad norm: 33.96 time: 0.041
2026-01-17 13:16:24,920: Running test after training batch: 12500
2026-01-17 13:16:25,070: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:16:29,691: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-17 13:16:29,720: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-17 13:16:31,403: Val batch 12500: PER (avg): 0.1470 CTC Loss (avg): 15.5772 WER(1gram): 44.67% (n=64) time: 6.483
2026-01-17 13:16:31,403: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-17 13:16:31,403: t15.2023.08.13 val PER: 0.1081
2026-01-17 13:16:31,403: t15.2023.08.18 val PER: 0.1014
2026-01-17 13:16:31,404: t15.2023.08.20 val PER: 0.0977
2026-01-17 13:16:31,404: t15.2023.08.25 val PER: 0.0919
2026-01-17 13:16:31,404: t15.2023.08.27 val PER: 0.1833
2026-01-17 13:16:31,404: t15.2023.09.01 val PER: 0.0722
2026-01-17 13:16:31,404: t15.2023.09.03 val PER: 0.1508
2026-01-17 13:16:31,404: t15.2023.09.24 val PER: 0.1117
2026-01-17 13:16:31,404: t15.2023.09.29 val PER: 0.1283
2026-01-17 13:16:31,404: t15.2023.10.01 val PER: 0.1658
2026-01-17 13:16:31,404: t15.2023.10.06 val PER: 0.0829
2026-01-17 13:16:31,404: t15.2023.10.08 val PER: 0.2666
2026-01-17 13:16:31,404: t15.2023.10.13 val PER: 0.1893
2026-01-17 13:16:31,404: t15.2023.10.15 val PER: 0.1457
2026-01-17 13:16:31,405: t15.2023.10.20 val PER: 0.1980
2026-01-17 13:16:31,405: t15.2023.10.22 val PER: 0.1091
2026-01-17 13:16:31,405: t15.2023.11.03 val PER: 0.1805
2026-01-17 13:16:31,405: t15.2023.11.04 val PER: 0.0273
2026-01-17 13:16:31,405: t15.2023.11.17 val PER: 0.0467
2026-01-17 13:16:31,405: t15.2023.11.19 val PER: 0.0279
2026-01-17 13:16:31,405: t15.2023.11.26 val PER: 0.1094
2026-01-17 13:16:31,405: t15.2023.12.03 val PER: 0.1082
2026-01-17 13:16:31,405: t15.2023.12.08 val PER: 0.0932
2026-01-17 13:16:31,405: t15.2023.12.10 val PER: 0.0854
2026-01-17 13:16:31,405: t15.2023.12.17 val PER: 0.1299
2026-01-17 13:16:31,405: t15.2023.12.29 val PER: 0.1283
2026-01-17 13:16:31,405: t15.2024.02.25 val PER: 0.0983
2026-01-17 13:16:31,405: t15.2024.03.08 val PER: 0.2304
2026-01-17 13:16:31,405: t15.2024.03.15 val PER: 0.2001
2026-01-17 13:16:31,406: t15.2024.03.17 val PER: 0.1360
2026-01-17 13:16:31,406: t15.2024.05.10 val PER: 0.1530
2026-01-17 13:16:31,406: t15.2024.06.14 val PER: 0.1640
2026-01-17 13:16:31,406: t15.2024.07.19 val PER: 0.2301
2026-01-17 13:16:31,406: t15.2024.07.21 val PER: 0.0979
2026-01-17 13:16:31,406: t15.2024.07.28 val PER: 0.1316
2026-01-17 13:16:31,406: t15.2025.01.10 val PER: 0.2961
2026-01-17 13:16:31,406: t15.2025.01.12 val PER: 0.1432
2026-01-17 13:16:31,406: t15.2025.03.14 val PER: 0.3565
2026-01-17 13:16:31,406: t15.2025.03.16 val PER: 0.1767
2026-01-17 13:16:31,406: t15.2025.03.30 val PER: 0.3080
2026-01-17 13:16:31,406: t15.2025.04.13 val PER: 0.2111
2026-01-17 13:16:31,408: New best val WER(1gram) 46.70% --> 44.67%
2026-01-17 13:16:31,408: Checkpointing model
2026-01-17 13:16:31,558: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 13:16:40,124: Train batch 12600: loss: 6.35 grad norm: 36.93 time: 0.058
2026-01-17 13:16:58,011: Train batch 12800: loss: 3.88 grad norm: 32.74 time: 0.053
2026-01-17 13:17:15,931: Train batch 13000: loss: 4.55 grad norm: 41.03 time: 0.065
2026-01-17 13:17:15,932: Running test after training batch: 13000
2026-01-17 13:17:16,022: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:17:20,877: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-17 13:17:20,909: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sids
2026-01-17 13:17:22,630: Val batch 13000: PER (avg): 0.1456 CTC Loss (avg): 15.5605 WER(1gram): 45.43% (n=64) time: 6.698
2026-01-17 13:17:22,631: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-17 13:17:22,631: t15.2023.08.13 val PER: 0.1019
2026-01-17 13:17:22,631: t15.2023.08.18 val PER: 0.0972
2026-01-17 13:17:22,631: t15.2023.08.20 val PER: 0.1048
2026-01-17 13:17:22,631: t15.2023.08.25 val PER: 0.0904
2026-01-17 13:17:22,631: t15.2023.08.27 val PER: 0.1865
2026-01-17 13:17:22,631: t15.2023.09.01 val PER: 0.0747
2026-01-17 13:17:22,631: t15.2023.09.03 val PER: 0.1698
2026-01-17 13:17:22,631: t15.2023.09.24 val PER: 0.1250
2026-01-17 13:17:22,631: t15.2023.09.29 val PER: 0.1264
2026-01-17 13:17:22,631: t15.2023.10.01 val PER: 0.1750
2026-01-17 13:17:22,631: t15.2023.10.06 val PER: 0.0850
2026-01-17 13:17:22,631: t15.2023.10.08 val PER: 0.2490
2026-01-17 13:17:22,632: t15.2023.10.13 val PER: 0.1947
2026-01-17 13:17:22,632: t15.2023.10.15 val PER: 0.1397
2026-01-17 13:17:22,632: t15.2023.10.20 val PER: 0.1745
2026-01-17 13:17:22,632: t15.2023.10.22 val PER: 0.1125
2026-01-17 13:17:22,632: t15.2023.11.03 val PER: 0.1771
2026-01-17 13:17:22,632: t15.2023.11.04 val PER: 0.0273
2026-01-17 13:17:22,632: t15.2023.11.17 val PER: 0.0373
2026-01-17 13:17:22,632: t15.2023.11.19 val PER: 0.0339
2026-01-17 13:17:22,632: t15.2023.11.26 val PER: 0.1123
2026-01-17 13:17:22,632: t15.2023.12.03 val PER: 0.1061
2026-01-17 13:17:22,632: t15.2023.12.08 val PER: 0.0979
2026-01-17 13:17:22,632: t15.2023.12.10 val PER: 0.0880
2026-01-17 13:17:22,632: t15.2023.12.17 val PER: 0.1372
2026-01-17 13:17:22,632: t15.2023.12.29 val PER: 0.1297
2026-01-17 13:17:22,633: t15.2024.02.25 val PER: 0.1124
2026-01-17 13:17:22,633: t15.2024.03.08 val PER: 0.2347
2026-01-17 13:17:22,633: t15.2024.03.15 val PER: 0.1920
2026-01-17 13:17:22,633: t15.2024.03.17 val PER: 0.1241
2026-01-17 13:17:22,633: t15.2024.05.10 val PER: 0.1501
2026-01-17 13:17:22,633: t15.2024.06.14 val PER: 0.1546
2026-01-17 13:17:22,633: t15.2024.07.19 val PER: 0.2228
2026-01-17 13:17:22,633: t15.2024.07.21 val PER: 0.0876
2026-01-17 13:17:22,633: t15.2024.07.28 val PER: 0.1368
2026-01-17 13:17:22,633: t15.2025.01.10 val PER: 0.2893
2026-01-17 13:17:22,633: t15.2025.01.12 val PER: 0.1347
2026-01-17 13:17:22,633: t15.2025.03.14 val PER: 0.3358
2026-01-17 13:17:22,633: t15.2025.03.16 val PER: 0.1741
2026-01-17 13:17:22,633: t15.2025.03.30 val PER: 0.2862
2026-01-17 13:17:22,633: t15.2025.04.13 val PER: 0.2154
2026-01-17 13:17:40,707: Train batch 13200: loss: 8.64 grad norm: 52.97 time: 0.054
2026-01-17 13:17:58,881: Train batch 13400: loss: 6.54 grad norm: 50.62 time: 0.062
2026-01-17 13:18:08,019: Running test after training batch: 13500
2026-01-17 13:18:08,109: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:18:12,734: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-17 13:18:12,766: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost nit
2026-01-17 13:18:14,514: Val batch 13500: PER (avg): 0.1427 CTC Loss (avg): 15.1319 WER(1gram): 44.92% (n=64) time: 6.495
2026-01-17 13:18:14,515: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-17 13:18:14,515: t15.2023.08.13 val PER: 0.1050
2026-01-17 13:18:14,515: t15.2023.08.18 val PER: 0.0972
2026-01-17 13:18:14,515: t15.2023.08.20 val PER: 0.0977
2026-01-17 13:18:14,515: t15.2023.08.25 val PER: 0.0964
2026-01-17 13:18:14,515: t15.2023.08.27 val PER: 0.1801
2026-01-17 13:18:14,515: t15.2023.09.01 val PER: 0.0755
2026-01-17 13:18:14,515: t15.2023.09.03 val PER: 0.1532
2026-01-17 13:18:14,516: t15.2023.09.24 val PER: 0.1189
2026-01-17 13:18:14,516: t15.2023.09.29 val PER: 0.1264
2026-01-17 13:18:14,516: t15.2023.10.01 val PER: 0.1671
2026-01-17 13:18:14,516: t15.2023.10.06 val PER: 0.0840
2026-01-17 13:18:14,516: t15.2023.10.08 val PER: 0.2300
2026-01-17 13:18:14,516: t15.2023.10.13 val PER: 0.1971
2026-01-17 13:18:14,516: t15.2023.10.15 val PER: 0.1404
2026-01-17 13:18:14,516: t15.2023.10.20 val PER: 0.1711
2026-01-17 13:18:14,516: t15.2023.10.22 val PER: 0.1069
2026-01-17 13:18:14,516: t15.2023.11.03 val PER: 0.1744
2026-01-17 13:18:14,516: t15.2023.11.04 val PER: 0.0375
2026-01-17 13:18:14,516: t15.2023.11.17 val PER: 0.0358
2026-01-17 13:18:14,516: t15.2023.11.19 val PER: 0.0220
2026-01-17 13:18:14,516: t15.2023.11.26 val PER: 0.1101
2026-01-17 13:18:14,516: t15.2023.12.03 val PER: 0.0956
2026-01-17 13:18:14,516: t15.2023.12.08 val PER: 0.0899
2026-01-17 13:18:14,517: t15.2023.12.10 val PER: 0.0894
2026-01-17 13:18:14,517: t15.2023.12.17 val PER: 0.1289
2026-01-17 13:18:14,517: t15.2023.12.29 val PER: 0.1215
2026-01-17 13:18:14,517: t15.2024.02.25 val PER: 0.1138
2026-01-17 13:18:14,517: t15.2024.03.08 val PER: 0.2162
2026-01-17 13:18:14,517: t15.2024.03.15 val PER: 0.1926
2026-01-17 13:18:14,517: t15.2024.03.17 val PER: 0.1311
2026-01-17 13:18:14,517: t15.2024.05.10 val PER: 0.1545
2026-01-17 13:18:14,517: t15.2024.06.14 val PER: 0.1609
2026-01-17 13:18:14,518: t15.2024.07.19 val PER: 0.2268
2026-01-17 13:18:14,518: t15.2024.07.21 val PER: 0.0834
2026-01-17 13:18:14,518: t15.2024.07.28 val PER: 0.1279
2026-01-17 13:18:14,518: t15.2025.01.10 val PER: 0.2851
2026-01-17 13:18:14,518: t15.2025.01.12 val PER: 0.1378
2026-01-17 13:18:14,518: t15.2025.03.14 val PER: 0.3417
2026-01-17 13:18:14,518: t15.2025.03.16 val PER: 0.1623
2026-01-17 13:18:14,518: t15.2025.03.30 val PER: 0.2874
2026-01-17 13:18:14,518: t15.2025.04.13 val PER: 0.2068
2026-01-17 13:18:24,611: Train batch 13600: loss: 10.04 grad norm: 62.86 time: 0.062
2026-01-17 13:18:41,899: Train batch 13800: loss: 6.62 grad norm: 53.86 time: 0.056
2026-01-17 13:18:58,995: Train batch 14000: loss: 8.66 grad norm: 52.92 time: 0.050
2026-01-17 13:18:58,995: Running test after training batch: 14000
2026-01-17 13:18:59,097: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:19:03,731: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-17 13:19:03,763: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost nett
2026-01-17 13:19:05,530: Val batch 14000: PER (avg): 0.1422 CTC Loss (avg): 15.4943 WER(1gram): 48.22% (n=64) time: 6.535
2026-01-17 13:19:05,530: WER lens: avg_true_words=6.16 avg_pred_words=6.28 max_pred_words=11
2026-01-17 13:19:05,530: t15.2023.08.13 val PER: 0.1133
2026-01-17 13:19:05,530: t15.2023.08.18 val PER: 0.0972
2026-01-17 13:19:05,530: t15.2023.08.20 val PER: 0.1048
2026-01-17 13:19:05,531: t15.2023.08.25 val PER: 0.0889
2026-01-17 13:19:05,531: t15.2023.08.27 val PER: 0.1801
2026-01-17 13:19:05,531: t15.2023.09.01 val PER: 0.0706
2026-01-17 13:19:05,531: t15.2023.09.03 val PER: 0.1580
2026-01-17 13:19:05,531: t15.2023.09.24 val PER: 0.1201
2026-01-17 13:19:05,531: t15.2023.09.29 val PER: 0.1213
2026-01-17 13:19:05,531: t15.2023.10.01 val PER: 0.1631
2026-01-17 13:19:05,531: t15.2023.10.06 val PER: 0.0753
2026-01-17 13:19:05,531: t15.2023.10.08 val PER: 0.2436
2026-01-17 13:19:05,531: t15.2023.10.13 val PER: 0.1854
2026-01-17 13:19:05,531: t15.2023.10.15 val PER: 0.1411
2026-01-17 13:19:05,531: t15.2023.10.20 val PER: 0.1946
2026-01-17 13:19:05,531: t15.2023.10.22 val PER: 0.1125
2026-01-17 13:19:05,532: t15.2023.11.03 val PER: 0.1798
2026-01-17 13:19:05,532: t15.2023.11.04 val PER: 0.0273
2026-01-17 13:19:05,532: t15.2023.11.17 val PER: 0.0451
2026-01-17 13:19:05,532: t15.2023.11.19 val PER: 0.0180
2026-01-17 13:19:05,532: t15.2023.11.26 val PER: 0.1225
2026-01-17 13:19:05,532: t15.2023.12.03 val PER: 0.1103
2026-01-17 13:19:05,532: t15.2023.12.08 val PER: 0.0905
2026-01-17 13:19:05,532: t15.2023.12.10 val PER: 0.0920
2026-01-17 13:19:05,532: t15.2023.12.17 val PER: 0.1289
2026-01-17 13:19:05,532: t15.2023.12.29 val PER: 0.1208
2026-01-17 13:19:05,532: t15.2024.02.25 val PER: 0.1081
2026-01-17 13:19:05,532: t15.2024.03.08 val PER: 0.2134
2026-01-17 13:19:05,532: t15.2024.03.15 val PER: 0.1882
2026-01-17 13:19:05,532: t15.2024.03.17 val PER: 0.1248
2026-01-17 13:19:05,533: t15.2024.05.10 val PER: 0.1382
2026-01-17 13:19:05,533: t15.2024.06.14 val PER: 0.1530
2026-01-17 13:19:05,533: t15.2024.07.19 val PER: 0.2248
2026-01-17 13:19:05,533: t15.2024.07.21 val PER: 0.0821
2026-01-17 13:19:05,533: t15.2024.07.28 val PER: 0.1213
2026-01-17 13:19:05,533: t15.2025.01.10 val PER: 0.2865
2026-01-17 13:19:05,533: t15.2025.01.12 val PER: 0.1363
2026-01-17 13:19:05,533: t15.2025.03.14 val PER: 0.3432
2026-01-17 13:19:05,533: t15.2025.03.16 val PER: 0.1662
2026-01-17 13:19:05,533: t15.2025.03.30 val PER: 0.2782
2026-01-17 13:19:05,533: t15.2025.04.13 val PER: 0.1997
2026-01-17 13:19:22,951: Train batch 14200: loss: 6.60 grad norm: 49.43 time: 0.056
2026-01-17 13:19:40,768: Train batch 14400: loss: 4.44 grad norm: 37.86 time: 0.064
2026-01-17 13:19:49,393: Running test after training batch: 14500
2026-01-17 13:19:49,497: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:19:54,361: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-17 13:19:54,393: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sit
2026-01-17 13:19:56,167: Val batch 14500: PER (avg): 0.1412 CTC Loss (avg): 15.3525 WER(1gram): 48.48% (n=64) time: 6.774
2026-01-17 13:19:56,168: WER lens: avg_true_words=6.16 avg_pred_words=6.27 max_pred_words=11
2026-01-17 13:19:56,168: t15.2023.08.13 val PER: 0.1091
2026-01-17 13:19:56,168: t15.2023.08.18 val PER: 0.0964
2026-01-17 13:19:56,168: t15.2023.08.20 val PER: 0.0945
2026-01-17 13:19:56,168: t15.2023.08.25 val PER: 0.0889
2026-01-17 13:19:56,168: t15.2023.08.27 val PER: 0.1849
2026-01-17 13:19:56,168: t15.2023.09.01 val PER: 0.0722
2026-01-17 13:19:56,168: t15.2023.09.03 val PER: 0.1520
2026-01-17 13:19:56,169: t15.2023.09.24 val PER: 0.1226
2026-01-17 13:19:56,169: t15.2023.09.29 val PER: 0.1206
2026-01-17 13:19:56,169: t15.2023.10.01 val PER: 0.1757
2026-01-17 13:19:56,169: t15.2023.10.06 val PER: 0.0818
2026-01-17 13:19:56,169: t15.2023.10.08 val PER: 0.2436
2026-01-17 13:19:56,169: t15.2023.10.13 val PER: 0.1924
2026-01-17 13:19:56,169: t15.2023.10.15 val PER: 0.1477
2026-01-17 13:19:56,169: t15.2023.10.20 val PER: 0.1644
2026-01-17 13:19:56,169: t15.2023.10.22 val PER: 0.1192
2026-01-17 13:19:56,170: t15.2023.11.03 val PER: 0.1716
2026-01-17 13:19:56,170: t15.2023.11.04 val PER: 0.0273
2026-01-17 13:19:56,170: t15.2023.11.17 val PER: 0.0389
2026-01-17 13:19:56,170: t15.2023.11.19 val PER: 0.0279
2026-01-17 13:19:56,170: t15.2023.11.26 val PER: 0.1080
2026-01-17 13:19:56,170: t15.2023.12.03 val PER: 0.1008
2026-01-17 13:19:56,170: t15.2023.12.08 val PER: 0.0866
2026-01-17 13:19:56,170: t15.2023.12.10 val PER: 0.0841
2026-01-17 13:19:56,170: t15.2023.12.17 val PER: 0.1279
2026-01-17 13:19:56,171: t15.2023.12.29 val PER: 0.1208
2026-01-17 13:19:56,171: t15.2024.02.25 val PER: 0.1152
2026-01-17 13:19:56,171: t15.2024.03.08 val PER: 0.2191
2026-01-17 13:19:56,171: t15.2024.03.15 val PER: 0.1907
2026-01-17 13:19:56,171: t15.2024.03.17 val PER: 0.1297
2026-01-17 13:19:56,171: t15.2024.05.10 val PER: 0.1456
2026-01-17 13:19:56,171: t15.2024.06.14 val PER: 0.1640
2026-01-17 13:19:56,171: t15.2024.07.19 val PER: 0.2090
2026-01-17 13:19:56,171: t15.2024.07.21 val PER: 0.0786
2026-01-17 13:19:56,171: t15.2024.07.28 val PER: 0.1257
2026-01-17 13:19:56,172: t15.2025.01.10 val PER: 0.2755
2026-01-17 13:19:56,172: t15.2025.01.12 val PER: 0.1440
2026-01-17 13:19:56,172: t15.2025.03.14 val PER: 0.3240
2026-01-17 13:19:56,172: t15.2025.03.16 val PER: 0.1662
2026-01-17 13:19:56,172: t15.2025.03.30 val PER: 0.2713
2026-01-17 13:19:56,172: t15.2025.04.13 val PER: 0.1926
2026-01-17 13:20:04,799: Train batch 14600: loss: 8.24 grad norm: 48.68 time: 0.059
2026-01-17 13:20:21,935: Train batch 14800: loss: 3.98 grad norm: 34.87 time: 0.051
2026-01-17 13:20:40,864: Train batch 15000: loss: 4.97 grad norm: 43.40 time: 0.052
2026-01-17 13:20:40,864: Running test after training batch: 15000
2026-01-17 13:20:40,994: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:20:45,615: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the could at this point as will
2026-01-17 13:20:45,648: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost nett
2026-01-17 13:20:47,439: Val batch 15000: PER (avg): 0.1381 CTC Loss (avg): 15.1636 WER(1gram): 44.16% (n=64) time: 6.575
2026-01-17 13:20:47,440: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-17 13:20:47,440: t15.2023.08.13 val PER: 0.0998
2026-01-17 13:20:47,440: t15.2023.08.18 val PER: 0.1014
2026-01-17 13:20:47,440: t15.2023.08.20 val PER: 0.0905
2026-01-17 13:20:47,440: t15.2023.08.25 val PER: 0.0858
2026-01-17 13:20:47,440: t15.2023.08.27 val PER: 0.1704
2026-01-17 13:20:47,440: t15.2023.09.01 val PER: 0.0706
2026-01-17 13:20:47,441: t15.2023.09.03 val PER: 0.1378
2026-01-17 13:20:47,441: t15.2023.09.24 val PER: 0.1129
2026-01-17 13:20:47,441: t15.2023.09.29 val PER: 0.1149
2026-01-17 13:20:47,441: t15.2023.10.01 val PER: 0.1678
2026-01-17 13:20:47,441: t15.2023.10.06 val PER: 0.0872
2026-01-17 13:20:47,441: t15.2023.10.08 val PER: 0.2476
2026-01-17 13:20:47,441: t15.2023.10.13 val PER: 0.1815
2026-01-17 13:20:47,441: t15.2023.10.15 val PER: 0.1378
2026-01-17 13:20:47,441: t15.2023.10.20 val PER: 0.1711
2026-01-17 13:20:47,441: t15.2023.10.22 val PER: 0.1147
2026-01-17 13:20:47,441: t15.2023.11.03 val PER: 0.1689
2026-01-17 13:20:47,441: t15.2023.11.04 val PER: 0.0341
2026-01-17 13:20:47,441: t15.2023.11.17 val PER: 0.0358
2026-01-17 13:20:47,441: t15.2023.11.19 val PER: 0.0299
2026-01-17 13:20:47,441: t15.2023.11.26 val PER: 0.0971
2026-01-17 13:20:47,441: t15.2023.12.03 val PER: 0.1008
2026-01-17 13:20:47,442: t15.2023.12.08 val PER: 0.0799
2026-01-17 13:20:47,442: t15.2023.12.10 val PER: 0.0762
2026-01-17 13:20:47,442: t15.2023.12.17 val PER: 0.1341
2026-01-17 13:20:47,442: t15.2023.12.29 val PER: 0.1235
2026-01-17 13:20:47,442: t15.2024.02.25 val PER: 0.0969
2026-01-17 13:20:47,442: t15.2024.03.08 val PER: 0.2034
2026-01-17 13:20:47,442: t15.2024.03.15 val PER: 0.1826
2026-01-17 13:20:47,442: t15.2024.03.17 val PER: 0.1137
2026-01-17 13:20:47,442: t15.2024.05.10 val PER: 0.1382
2026-01-17 13:20:47,442: t15.2024.06.14 val PER: 0.1451
2026-01-17 13:20:47,442: t15.2024.07.19 val PER: 0.2320
2026-01-17 13:20:47,442: t15.2024.07.21 val PER: 0.0786
2026-01-17 13:20:47,442: t15.2024.07.28 val PER: 0.1235
2026-01-17 13:20:47,443: t15.2025.01.10 val PER: 0.3099
2026-01-17 13:20:47,443: t15.2025.01.12 val PER: 0.1393
2026-01-17 13:20:47,443: t15.2025.03.14 val PER: 0.3210
2026-01-17 13:20:47,443: t15.2025.03.16 val PER: 0.1715
2026-01-17 13:20:47,443: t15.2025.03.30 val PER: 0.2667
2026-01-17 13:20:47,443: t15.2025.04.13 val PER: 0.2054
2026-01-17 13:20:47,444: New best val WER(1gram) 44.67% --> 44.16%
2026-01-17 13:20:47,444: Checkpointing model
2026-01-17 13:20:47,594: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 13:21:05,962: Train batch 15200: loss: 3.37 grad norm: 36.79 time: 0.057
2026-01-17 13:21:23,962: Train batch 15400: loss: 7.47 grad norm: 53.66 time: 0.049
2026-01-17 13:21:32,751: Running test after training batch: 15500
2026-01-17 13:21:32,860: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:21:37,749: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:21:37,782: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nett
2026-01-17 13:21:39,581: Val batch 15500: PER (avg): 0.1350 CTC Loss (avg): 15.1722 WER(1gram): 45.18% (n=64) time: 6.830
2026-01-17 13:21:39,581: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-17 13:21:39,581: t15.2023.08.13 val PER: 0.1029
2026-01-17 13:21:39,582: t15.2023.08.18 val PER: 0.0889
2026-01-17 13:21:39,582: t15.2023.08.20 val PER: 0.0929
2026-01-17 13:21:39,582: t15.2023.08.25 val PER: 0.0919
2026-01-17 13:21:39,582: t15.2023.08.27 val PER: 0.1817
2026-01-17 13:21:39,582: t15.2023.09.01 val PER: 0.0601
2026-01-17 13:21:39,582: t15.2023.09.03 val PER: 0.1508
2026-01-17 13:21:39,582: t15.2023.09.24 val PER: 0.1214
2026-01-17 13:21:39,582: t15.2023.09.29 val PER: 0.1142
2026-01-17 13:21:39,583: t15.2023.10.01 val PER: 0.1631
2026-01-17 13:21:39,583: t15.2023.10.06 val PER: 0.0840
2026-01-17 13:21:39,583: t15.2023.10.08 val PER: 0.2314
2026-01-17 13:21:39,583: t15.2023.10.13 val PER: 0.1932
2026-01-17 13:21:39,583: t15.2023.10.15 val PER: 0.1411
2026-01-17 13:21:39,583: t15.2023.10.20 val PER: 0.1846
2026-01-17 13:21:39,583: t15.2023.10.22 val PER: 0.1125
2026-01-17 13:21:39,583: t15.2023.11.03 val PER: 0.1574
2026-01-17 13:21:39,583: t15.2023.11.04 val PER: 0.0307
2026-01-17 13:21:39,584: t15.2023.11.17 val PER: 0.0233
2026-01-17 13:21:39,584: t15.2023.11.19 val PER: 0.0240
2026-01-17 13:21:39,584: t15.2023.11.26 val PER: 0.0913
2026-01-17 13:21:39,584: t15.2023.12.03 val PER: 0.1008
2026-01-17 13:21:39,584: t15.2023.12.08 val PER: 0.0692
2026-01-17 13:21:39,584: t15.2023.12.10 val PER: 0.0710
2026-01-17 13:21:39,584: t15.2023.12.17 val PER: 0.1258
2026-01-17 13:21:39,584: t15.2023.12.29 val PER: 0.1187
2026-01-17 13:21:39,584: t15.2024.02.25 val PER: 0.1039
2026-01-17 13:21:39,584: t15.2024.03.08 val PER: 0.2119
2026-01-17 13:21:39,584: t15.2024.03.15 val PER: 0.1939
2026-01-17 13:21:39,584: t15.2024.03.17 val PER: 0.1046
2026-01-17 13:21:39,585: t15.2024.05.10 val PER: 0.1367
2026-01-17 13:21:39,585: t15.2024.06.14 val PER: 0.1451
2026-01-17 13:21:39,585: t15.2024.07.19 val PER: 0.2063
2026-01-17 13:21:39,585: t15.2024.07.21 val PER: 0.0876
2026-01-17 13:21:39,585: t15.2024.07.28 val PER: 0.1221
2026-01-17 13:21:39,585: t15.2025.01.10 val PER: 0.2810
2026-01-17 13:21:39,585: t15.2025.01.12 val PER: 0.1255
2026-01-17 13:21:39,585: t15.2025.03.14 val PER: 0.3299
2026-01-17 13:21:39,585: t15.2025.03.16 val PER: 0.1571
2026-01-17 13:21:39,585: t15.2025.03.30 val PER: 0.2655
2026-01-17 13:21:39,585: t15.2025.04.13 val PER: 0.2011
2026-01-17 13:21:48,629: Train batch 15600: loss: 7.07 grad norm: 50.08 time: 0.061
2026-01-17 13:22:06,533: Train batch 15800: loss: 8.52 grad norm: 54.08 time: 0.067
2026-01-17 13:22:24,402: Train batch 16000: loss: 4.92 grad norm: 39.17 time: 0.055
2026-01-17 13:22:24,402: Running test after training batch: 16000
2026-01-17 13:22:24,541: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:22:29,146: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:22:29,178: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nett
2026-01-17 13:22:30,976: Val batch 16000: PER (avg): 0.1365 CTC Loss (avg): 15.3403 WER(1gram): 46.45% (n=64) time: 6.574
2026-01-17 13:22:30,976: WER lens: avg_true_words=6.16 avg_pred_words=6.33 max_pred_words=12
2026-01-17 13:22:30,976: t15.2023.08.13 val PER: 0.0956
2026-01-17 13:22:30,976: t15.2023.08.18 val PER: 0.0956
2026-01-17 13:22:30,976: t15.2023.08.20 val PER: 0.0921
2026-01-17 13:22:30,977: t15.2023.08.25 val PER: 0.0813
2026-01-17 13:22:30,977: t15.2023.08.27 val PER: 0.1656
2026-01-17 13:22:30,977: t15.2023.09.01 val PER: 0.0722
2026-01-17 13:22:30,977: t15.2023.09.03 val PER: 0.1568
2026-01-17 13:22:30,977: t15.2023.09.24 val PER: 0.1080
2026-01-17 13:22:30,977: t15.2023.09.29 val PER: 0.1193
2026-01-17 13:22:30,977: t15.2023.10.01 val PER: 0.1671
2026-01-17 13:22:30,977: t15.2023.10.06 val PER: 0.0840
2026-01-17 13:22:30,977: t15.2023.10.08 val PER: 0.2246
2026-01-17 13:22:30,977: t15.2023.10.13 val PER: 0.1893
2026-01-17 13:22:30,977: t15.2023.10.15 val PER: 0.1411
2026-01-17 13:22:30,977: t15.2023.10.20 val PER: 0.1779
2026-01-17 13:22:30,977: t15.2023.10.22 val PER: 0.1180
2026-01-17 13:22:30,977: t15.2023.11.03 val PER: 0.1655
2026-01-17 13:22:30,977: t15.2023.11.04 val PER: 0.0307
2026-01-17 13:22:30,978: t15.2023.11.17 val PER: 0.0373
2026-01-17 13:22:30,978: t15.2023.11.19 val PER: 0.0259
2026-01-17 13:22:30,978: t15.2023.11.26 val PER: 0.0913
2026-01-17 13:22:30,978: t15.2023.12.03 val PER: 0.0914
2026-01-17 13:22:30,978: t15.2023.12.08 val PER: 0.0779
2026-01-17 13:22:30,978: t15.2023.12.10 val PER: 0.0710
2026-01-17 13:22:30,978: t15.2023.12.17 val PER: 0.1185
2026-01-17 13:22:30,978: t15.2023.12.29 val PER: 0.1112
2026-01-17 13:22:30,978: t15.2024.02.25 val PER: 0.1011
2026-01-17 13:22:30,978: t15.2024.03.08 val PER: 0.2134
2026-01-17 13:22:30,978: t15.2024.03.15 val PER: 0.1857
2026-01-17 13:22:30,978: t15.2024.03.17 val PER: 0.1179
2026-01-17 13:22:30,978: t15.2024.05.10 val PER: 0.1649
2026-01-17 13:22:30,978: t15.2024.06.14 val PER: 0.1483
2026-01-17 13:22:30,978: t15.2024.07.19 val PER: 0.2096
2026-01-17 13:22:30,978: t15.2024.07.21 val PER: 0.0910
2026-01-17 13:22:30,978: t15.2024.07.28 val PER: 0.1228
2026-01-17 13:22:30,979: t15.2025.01.10 val PER: 0.2879
2026-01-17 13:22:30,979: t15.2025.01.12 val PER: 0.1324
2026-01-17 13:22:30,979: t15.2025.03.14 val PER: 0.3299
2026-01-17 13:22:30,979: t15.2025.03.16 val PER: 0.1662
2026-01-17 13:22:30,979: t15.2025.03.30 val PER: 0.2678
2026-01-17 13:22:30,979: t15.2025.04.13 val PER: 0.1926
2026-01-17 13:22:48,207: Train batch 16200: loss: 3.99 grad norm: 42.68 time: 0.056
2026-01-17 13:23:07,131: Train batch 16400: loss: 8.23 grad norm: 68.97 time: 0.057
2026-01-17 13:23:16,556: Running test after training batch: 16500
2026-01-17 13:23:16,682: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:23:21,995: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the could at this point as will
2026-01-17 13:23:22,027: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-17 13:23:23,818: Val batch 16500: PER (avg): 0.1345 CTC Loss (avg): 15.2041 WER(1gram): 44.67% (n=64) time: 7.261
2026-01-17 13:23:23,818: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-17 13:23:23,818: t15.2023.08.13 val PER: 0.1019
2026-01-17 13:23:23,818: t15.2023.08.18 val PER: 0.0889
2026-01-17 13:23:23,818: t15.2023.08.20 val PER: 0.0913
2026-01-17 13:23:23,818: t15.2023.08.25 val PER: 0.0843
2026-01-17 13:23:23,818: t15.2023.08.27 val PER: 0.1752
2026-01-17 13:23:23,819: t15.2023.09.01 val PER: 0.0755
2026-01-17 13:23:23,819: t15.2023.09.03 val PER: 0.1390
2026-01-17 13:23:23,819: t15.2023.09.24 val PER: 0.1165
2026-01-17 13:23:23,819: t15.2023.09.29 val PER: 0.1142
2026-01-17 13:23:23,819: t15.2023.10.01 val PER: 0.1645
2026-01-17 13:23:23,819: t15.2023.10.06 val PER: 0.0786
2026-01-17 13:23:23,819: t15.2023.10.08 val PER: 0.2300
2026-01-17 13:23:23,819: t15.2023.10.13 val PER: 0.1939
2026-01-17 13:23:23,819: t15.2023.10.15 val PER: 0.1365
2026-01-17 13:23:23,819: t15.2023.10.20 val PER: 0.1913
2026-01-17 13:23:23,819: t15.2023.10.22 val PER: 0.1192
2026-01-17 13:23:23,819: t15.2023.11.03 val PER: 0.1649
2026-01-17 13:23:23,819: t15.2023.11.04 val PER: 0.0341
2026-01-17 13:23:23,820: t15.2023.11.17 val PER: 0.0420
2026-01-17 13:23:23,820: t15.2023.11.19 val PER: 0.0240
2026-01-17 13:23:23,820: t15.2023.11.26 val PER: 0.0775
2026-01-17 13:23:23,820: t15.2023.12.03 val PER: 0.0872
2026-01-17 13:23:23,820: t15.2023.12.08 val PER: 0.0672
2026-01-17 13:23:23,820: t15.2023.12.10 val PER: 0.0723
2026-01-17 13:23:23,820: t15.2023.12.17 val PER: 0.1216
2026-01-17 13:23:23,820: t15.2023.12.29 val PER: 0.1064
2026-01-17 13:23:23,820: t15.2024.02.25 val PER: 0.0955
2026-01-17 13:23:23,820: t15.2024.03.08 val PER: 0.2304
2026-01-17 13:23:23,820: t15.2024.03.15 val PER: 0.1901
2026-01-17 13:23:23,820: t15.2024.03.17 val PER: 0.1116
2026-01-17 13:23:23,820: t15.2024.05.10 val PER: 0.1308
2026-01-17 13:23:23,820: t15.2024.06.14 val PER: 0.1514
2026-01-17 13:23:23,820: t15.2024.07.19 val PER: 0.2083
2026-01-17 13:23:23,820: t15.2024.07.21 val PER: 0.0883
2026-01-17 13:23:23,820: t15.2024.07.28 val PER: 0.1169
2026-01-17 13:23:23,821: t15.2025.01.10 val PER: 0.2769
2026-01-17 13:23:23,821: t15.2025.01.12 val PER: 0.1416
2026-01-17 13:23:23,821: t15.2025.03.14 val PER: 0.3284
2026-01-17 13:23:23,821: t15.2025.03.16 val PER: 0.1636
2026-01-17 13:23:23,821: t15.2025.03.30 val PER: 0.2586
2026-01-17 13:23:23,821: t15.2025.04.13 val PER: 0.1969
2026-01-17 13:23:33,221: Train batch 16600: loss: 4.88 grad norm: 44.44 time: 0.053
2026-01-17 13:23:50,902: Train batch 16800: loss: 10.91 grad norm: 67.47 time: 0.061
2026-01-17 13:24:09,642: Train batch 17000: loss: 5.04 grad norm: 40.56 time: 0.082
2026-01-17 13:24:09,642: Running test after training batch: 17000
2026-01-17 13:24:09,749: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:24:14,355: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:24:14,387: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nett
2026-01-17 13:24:16,185: Val batch 17000: PER (avg): 0.1340 CTC Loss (avg): 15.2416 WER(1gram): 44.92% (n=64) time: 6.543
2026-01-17 13:24:16,186: WER lens: avg_true_words=6.16 avg_pred_words=6.33 max_pred_words=12
2026-01-17 13:24:16,186: t15.2023.08.13 val PER: 0.1040
2026-01-17 13:24:16,186: t15.2023.08.18 val PER: 0.0847
2026-01-17 13:24:16,186: t15.2023.08.20 val PER: 0.0945
2026-01-17 13:24:16,186: t15.2023.08.25 val PER: 0.0858
2026-01-17 13:24:16,186: t15.2023.08.27 val PER: 0.1752
2026-01-17 13:24:16,186: t15.2023.09.01 val PER: 0.0617
2026-01-17 13:24:16,186: t15.2023.09.03 val PER: 0.1461
2026-01-17 13:24:16,186: t15.2023.09.24 val PER: 0.1153
2026-01-17 13:24:16,186: t15.2023.09.29 val PER: 0.1161
2026-01-17 13:24:16,187: t15.2023.10.01 val PER: 0.1638
2026-01-17 13:24:16,187: t15.2023.10.06 val PER: 0.0807
2026-01-17 13:24:16,187: t15.2023.10.08 val PER: 0.2327
2026-01-17 13:24:16,187: t15.2023.10.13 val PER: 0.1808
2026-01-17 13:24:16,187: t15.2023.10.15 val PER: 0.1404
2026-01-17 13:24:16,187: t15.2023.10.20 val PER: 0.1846
2026-01-17 13:24:16,187: t15.2023.10.22 val PER: 0.1069
2026-01-17 13:24:16,187: t15.2023.11.03 val PER: 0.1649
2026-01-17 13:24:16,187: t15.2023.11.04 val PER: 0.0341
2026-01-17 13:24:16,187: t15.2023.11.17 val PER: 0.0389
2026-01-17 13:24:16,188: t15.2023.11.19 val PER: 0.0299
2026-01-17 13:24:16,188: t15.2023.11.26 val PER: 0.0812
2026-01-17 13:24:16,188: t15.2023.12.03 val PER: 0.0861
2026-01-17 13:24:16,188: t15.2023.12.08 val PER: 0.0639
2026-01-17 13:24:16,188: t15.2023.12.10 val PER: 0.0762
2026-01-17 13:24:16,188: t15.2023.12.17 val PER: 0.1258
2026-01-17 13:24:16,188: t15.2023.12.29 val PER: 0.1084
2026-01-17 13:24:16,188: t15.2024.02.25 val PER: 0.1025
2026-01-17 13:24:16,188: t15.2024.03.08 val PER: 0.2162
2026-01-17 13:24:16,188: t15.2024.03.15 val PER: 0.1857
2026-01-17 13:24:16,188: t15.2024.03.17 val PER: 0.1192
2026-01-17 13:24:16,188: t15.2024.05.10 val PER: 0.1426
2026-01-17 13:24:16,188: t15.2024.06.14 val PER: 0.1467
2026-01-17 13:24:16,188: t15.2024.07.19 val PER: 0.2024
2026-01-17 13:24:16,188: t15.2024.07.21 val PER: 0.0807
2026-01-17 13:24:16,188: t15.2024.07.28 val PER: 0.1213
2026-01-17 13:24:16,188: t15.2025.01.10 val PER: 0.2824
2026-01-17 13:24:16,189: t15.2025.01.12 val PER: 0.1339
2026-01-17 13:24:16,189: t15.2025.03.14 val PER: 0.3476
2026-01-17 13:24:16,189: t15.2025.03.16 val PER: 0.1610
2026-01-17 13:24:16,189: t15.2025.03.30 val PER: 0.2552
2026-01-17 13:24:16,189: t15.2025.04.13 val PER: 0.2040
2026-01-17 13:24:34,616: Train batch 17200: loss: 5.08 grad norm: 37.76 time: 0.084
2026-01-17 13:24:52,441: Train batch 17400: loss: 6.54 grad norm: 55.73 time: 0.070
2026-01-17 13:25:00,832: Running test after training batch: 17500
2026-01-17 13:25:00,972: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:25:05,730: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:25:05,762: WER debug example
  GT : how does it keep the cost down
  PR : hower dust it keep the cost edds
2026-01-17 13:25:07,587: Val batch 17500: PER (avg): 0.1315 CTC Loss (avg): 15.2741 WER(1gram): 48.48% (n=64) time: 6.754
2026-01-17 13:25:07,587: WER lens: avg_true_words=6.16 avg_pred_words=6.31 max_pred_words=12
2026-01-17 13:25:07,587: t15.2023.08.13 val PER: 0.0977
2026-01-17 13:25:07,588: t15.2023.08.18 val PER: 0.0989
2026-01-17 13:25:07,588: t15.2023.08.20 val PER: 0.0858
2026-01-17 13:25:07,588: t15.2023.08.25 val PER: 0.0828
2026-01-17 13:25:07,588: t15.2023.08.27 val PER: 0.1640
2026-01-17 13:25:07,588: t15.2023.09.01 val PER: 0.0739
2026-01-17 13:25:07,588: t15.2023.09.03 val PER: 0.1401
2026-01-17 13:25:07,588: t15.2023.09.24 val PER: 0.1056
2026-01-17 13:25:07,588: t15.2023.09.29 val PER: 0.1174
2026-01-17 13:25:07,588: t15.2023.10.01 val PER: 0.1605
2026-01-17 13:25:07,588: t15.2023.10.06 val PER: 0.0678
2026-01-17 13:25:07,588: t15.2023.10.08 val PER: 0.2300
2026-01-17 13:25:07,588: t15.2023.10.13 val PER: 0.1753
2026-01-17 13:25:07,588: t15.2023.10.15 val PER: 0.1358
2026-01-17 13:25:07,588: t15.2023.10.20 val PER: 0.1846
2026-01-17 13:25:07,589: t15.2023.10.22 val PER: 0.1002
2026-01-17 13:25:07,589: t15.2023.11.03 val PER: 0.1601
2026-01-17 13:25:07,589: t15.2023.11.04 val PER: 0.0307
2026-01-17 13:25:07,589: t15.2023.11.17 val PER: 0.0295
2026-01-17 13:25:07,589: t15.2023.11.19 val PER: 0.0220
2026-01-17 13:25:07,589: t15.2023.11.26 val PER: 0.0870
2026-01-17 13:25:07,589: t15.2023.12.03 val PER: 0.0840
2026-01-17 13:25:07,589: t15.2023.12.08 val PER: 0.0732
2026-01-17 13:25:07,589: t15.2023.12.10 val PER: 0.0696
2026-01-17 13:25:07,589: t15.2023.12.17 val PER: 0.1195
2026-01-17 13:25:07,589: t15.2023.12.29 val PER: 0.1112
2026-01-17 13:25:07,589: t15.2024.02.25 val PER: 0.0927
2026-01-17 13:25:07,589: t15.2024.03.08 val PER: 0.2006
2026-01-17 13:25:07,589: t15.2024.03.15 val PER: 0.1826
2026-01-17 13:25:07,589: t15.2024.03.17 val PER: 0.1165
2026-01-17 13:25:07,589: t15.2024.05.10 val PER: 0.1441
2026-01-17 13:25:07,589: t15.2024.06.14 val PER: 0.1483
2026-01-17 13:25:07,590: t15.2024.07.19 val PER: 0.2076
2026-01-17 13:25:07,590: t15.2024.07.21 val PER: 0.0807
2026-01-17 13:25:07,590: t15.2024.07.28 val PER: 0.1140
2026-01-17 13:25:07,590: t15.2025.01.10 val PER: 0.2741
2026-01-17 13:25:07,590: t15.2025.01.12 val PER: 0.1309
2026-01-17 13:25:07,590: t15.2025.03.14 val PER: 0.3373
2026-01-17 13:25:07,590: t15.2025.03.16 val PER: 0.1597
2026-01-17 13:25:07,590: t15.2025.03.30 val PER: 0.2575
2026-01-17 13:25:07,590: t15.2025.04.13 val PER: 0.1883
2026-01-17 13:25:16,013: Train batch 17600: loss: 4.73 grad norm: 41.72 time: 0.051
2026-01-17 13:25:34,460: Train batch 17800: loss: 4.11 grad norm: 45.89 time: 0.041
2026-01-17 13:25:53,056: Train batch 18000: loss: 5.39 grad norm: 49.05 time: 0.061
2026-01-17 13:25:53,056: Running test after training batch: 18000
2026-01-17 13:25:53,231: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:25:57,856: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:25:57,889: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sette
2026-01-17 13:25:59,717: Val batch 18000: PER (avg): 0.1266 CTC Loss (avg): 14.8384 WER(1gram): 44.42% (n=64) time: 6.660
2026-01-17 13:25:59,717: WER lens: avg_true_words=6.16 avg_pred_words=6.27 max_pred_words=12
2026-01-17 13:25:59,717: t15.2023.08.13 val PER: 0.0873
2026-01-17 13:25:59,717: t15.2023.08.18 val PER: 0.0847
2026-01-17 13:25:59,717: t15.2023.08.20 val PER: 0.0778
2026-01-17 13:25:59,717: t15.2023.08.25 val PER: 0.0889
2026-01-17 13:25:59,718: t15.2023.08.27 val PER: 0.1479
2026-01-17 13:25:59,718: t15.2023.09.01 val PER: 0.0657
2026-01-17 13:25:59,718: t15.2023.09.03 val PER: 0.1378
2026-01-17 13:25:59,718: t15.2023.09.24 val PER: 0.1056
2026-01-17 13:25:59,718: t15.2023.09.29 val PER: 0.1155
2026-01-17 13:25:59,718: t15.2023.10.01 val PER: 0.1678
2026-01-17 13:25:59,718: t15.2023.10.06 val PER: 0.0710
2026-01-17 13:25:59,718: t15.2023.10.08 val PER: 0.2219
2026-01-17 13:25:59,718: t15.2023.10.13 val PER: 0.1761
2026-01-17 13:25:59,718: t15.2023.10.15 val PER: 0.1292
2026-01-17 13:25:59,718: t15.2023.10.20 val PER: 0.1846
2026-01-17 13:25:59,718: t15.2023.10.22 val PER: 0.0969
2026-01-17 13:25:59,718: t15.2023.11.03 val PER: 0.1588
2026-01-17 13:25:59,719: t15.2023.11.04 val PER: 0.0205
2026-01-17 13:25:59,719: t15.2023.11.17 val PER: 0.0311
2026-01-17 13:25:59,719: t15.2023.11.19 val PER: 0.0220
2026-01-17 13:25:59,719: t15.2023.11.26 val PER: 0.0848
2026-01-17 13:25:59,719: t15.2023.12.03 val PER: 0.0861
2026-01-17 13:25:59,719: t15.2023.12.08 val PER: 0.0686
2026-01-17 13:25:59,719: t15.2023.12.10 val PER: 0.0736
2026-01-17 13:25:59,719: t15.2023.12.17 val PER: 0.1143
2026-01-17 13:25:59,719: t15.2023.12.29 val PER: 0.1057
2026-01-17 13:25:59,719: t15.2024.02.25 val PER: 0.0899
2026-01-17 13:25:59,719: t15.2024.03.08 val PER: 0.1849
2026-01-17 13:25:59,719: t15.2024.03.15 val PER: 0.1776
2026-01-17 13:25:59,719: t15.2024.03.17 val PER: 0.1102
2026-01-17 13:25:59,719: t15.2024.05.10 val PER: 0.1174
2026-01-17 13:25:59,719: t15.2024.06.14 val PER: 0.1356
2026-01-17 13:25:59,719: t15.2024.07.19 val PER: 0.2024
2026-01-17 13:25:59,720: t15.2024.07.21 val PER: 0.0766
2026-01-17 13:25:59,720: t15.2024.07.28 val PER: 0.1007
2026-01-17 13:25:59,720: t15.2025.01.10 val PER: 0.2796
2026-01-17 13:25:59,720: t15.2025.01.12 val PER: 0.1216
2026-01-17 13:25:59,720: t15.2025.03.14 val PER: 0.3269
2026-01-17 13:25:59,720: t15.2025.03.16 val PER: 0.1414
2026-01-17 13:25:59,720: t15.2025.03.30 val PER: 0.2483
2026-01-17 13:25:59,720: t15.2025.04.13 val PER: 0.1954
2026-01-17 13:26:17,507: Train batch 18200: loss: 4.11 grad norm: 42.46 time: 0.073
2026-01-17 13:26:34,687: Train batch 18400: loss: 2.29 grad norm: 38.22 time: 0.058
2026-01-17 13:26:43,322: Running test after training batch: 18500
2026-01-17 13:26:43,459: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:26:48,067: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:26:48,100: WER debug example
  GT : how does it keep the cost down
  PR : hower dusts it keep the cost get
2026-01-17 13:26:49,940: Val batch 18500: PER (avg): 0.1283 CTC Loss (avg): 15.1494 WER(1gram): 44.67% (n=64) time: 6.618
2026-01-17 13:26:49,940: WER lens: avg_true_words=6.16 avg_pred_words=6.33 max_pred_words=12
2026-01-17 13:26:49,941: t15.2023.08.13 val PER: 0.0998
2026-01-17 13:26:49,941: t15.2023.08.18 val PER: 0.0855
2026-01-17 13:26:49,941: t15.2023.08.20 val PER: 0.0834
2026-01-17 13:26:49,941: t15.2023.08.25 val PER: 0.0813
2026-01-17 13:26:49,941: t15.2023.08.27 val PER: 0.1704
2026-01-17 13:26:49,941: t15.2023.09.01 val PER: 0.0625
2026-01-17 13:26:49,941: t15.2023.09.03 val PER: 0.1390
2026-01-17 13:26:49,941: t15.2023.09.24 val PER: 0.0971
2026-01-17 13:26:49,941: t15.2023.09.29 val PER: 0.1066
2026-01-17 13:26:49,942: t15.2023.10.01 val PER: 0.1552
2026-01-17 13:26:49,942: t15.2023.10.06 val PER: 0.0775
2026-01-17 13:26:49,942: t15.2023.10.08 val PER: 0.2287
2026-01-17 13:26:49,942: t15.2023.10.13 val PER: 0.1606
2026-01-17 13:26:49,942: t15.2023.10.15 val PER: 0.1444
2026-01-17 13:26:49,942: t15.2023.10.20 val PER: 0.1711
2026-01-17 13:26:49,942: t15.2023.10.22 val PER: 0.1036
2026-01-17 13:26:49,942: t15.2023.11.03 val PER: 0.1689
2026-01-17 13:26:49,942: t15.2023.11.04 val PER: 0.0273
2026-01-17 13:26:49,942: t15.2023.11.17 val PER: 0.0311
2026-01-17 13:26:49,942: t15.2023.11.19 val PER: 0.0220
2026-01-17 13:26:49,942: t15.2023.11.26 val PER: 0.0826
2026-01-17 13:26:49,942: t15.2023.12.03 val PER: 0.0861
2026-01-17 13:26:49,942: t15.2023.12.08 val PER: 0.0732
2026-01-17 13:26:49,943: t15.2023.12.10 val PER: 0.0631
2026-01-17 13:26:49,943: t15.2023.12.17 val PER: 0.1081
2026-01-17 13:26:49,943: t15.2023.12.29 val PER: 0.0995
2026-01-17 13:26:49,943: t15.2024.02.25 val PER: 0.0899
2026-01-17 13:26:49,943: t15.2024.03.08 val PER: 0.1977
2026-01-17 13:26:49,943: t15.2024.03.15 val PER: 0.1770
2026-01-17 13:26:49,943: t15.2024.03.17 val PER: 0.1039
2026-01-17 13:26:49,943: t15.2024.05.10 val PER: 0.1471
2026-01-17 13:26:49,943: t15.2024.06.14 val PER: 0.1451
2026-01-17 13:26:49,943: t15.2024.07.19 val PER: 0.1978
2026-01-17 13:26:49,943: t15.2024.07.21 val PER: 0.0855
2026-01-17 13:26:49,943: t15.2024.07.28 val PER: 0.1147
2026-01-17 13:26:49,943: t15.2025.01.10 val PER: 0.2603
2026-01-17 13:26:49,943: t15.2025.01.12 val PER: 0.1263
2026-01-17 13:26:49,943: t15.2025.03.14 val PER: 0.3269
2026-01-17 13:26:49,943: t15.2025.03.16 val PER: 0.1584
2026-01-17 13:26:49,944: t15.2025.03.30 val PER: 0.2575
2026-01-17 13:26:49,944: t15.2025.04.13 val PER: 0.2040
2026-01-17 13:26:58,409: Train batch 18600: loss: 6.52 grad norm: 58.28 time: 0.067
2026-01-17 13:27:16,070: Train batch 18800: loss: 4.45 grad norm: 48.67 time: 0.064
2026-01-17 13:27:34,877: Train batch 19000: loss: 4.25 grad norm: 39.81 time: 0.066
2026-01-17 13:27:34,878: Running test after training batch: 19000
2026-01-17 13:27:35,015: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:27:39,998: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:27:40,030: WER debug example
  GT : how does it keep the cost down
  PR : hower dust it kieper the cost sends
2026-01-17 13:27:41,856: Val batch 19000: PER (avg): 0.1262 CTC Loss (avg): 14.9904 WER(1gram): 44.42% (n=64) time: 6.978
2026-01-17 13:27:41,856: WER lens: avg_true_words=6.16 avg_pred_words=6.27 max_pred_words=12
2026-01-17 13:27:41,856: t15.2023.08.13 val PER: 0.0884
2026-01-17 13:27:41,856: t15.2023.08.18 val PER: 0.0838
2026-01-17 13:27:41,857: t15.2023.08.20 val PER: 0.0763
2026-01-17 13:27:41,857: t15.2023.08.25 val PER: 0.0783
2026-01-17 13:27:41,857: t15.2023.08.27 val PER: 0.1576
2026-01-17 13:27:41,857: t15.2023.09.01 val PER: 0.0690
2026-01-17 13:27:41,857: t15.2023.09.03 val PER: 0.1366
2026-01-17 13:27:41,857: t15.2023.09.24 val PER: 0.1117
2026-01-17 13:27:41,857: t15.2023.09.29 val PER: 0.1110
2026-01-17 13:27:41,857: t15.2023.10.01 val PER: 0.1638
2026-01-17 13:27:41,857: t15.2023.10.06 val PER: 0.0807
2026-01-17 13:27:41,857: t15.2023.10.08 val PER: 0.2300
2026-01-17 13:27:41,857: t15.2023.10.13 val PER: 0.1691
2026-01-17 13:27:41,857: t15.2023.10.15 val PER: 0.1338
2026-01-17 13:27:41,858: t15.2023.10.20 val PER: 0.1946
2026-01-17 13:27:41,858: t15.2023.10.22 val PER: 0.0958
2026-01-17 13:27:41,858: t15.2023.11.03 val PER: 0.1588
2026-01-17 13:27:41,858: t15.2023.11.04 val PER: 0.0307
2026-01-17 13:27:41,858: t15.2023.11.17 val PER: 0.0249
2026-01-17 13:27:41,858: t15.2023.11.19 val PER: 0.0160
2026-01-17 13:27:41,858: t15.2023.11.26 val PER: 0.0812
2026-01-17 13:27:41,858: t15.2023.12.03 val PER: 0.0735
2026-01-17 13:27:41,858: t15.2023.12.08 val PER: 0.0646
2026-01-17 13:27:41,858: t15.2023.12.10 val PER: 0.0604
2026-01-17 13:27:41,858: t15.2023.12.17 val PER: 0.0967
2026-01-17 13:27:41,858: t15.2023.12.29 val PER: 0.1084
2026-01-17 13:27:41,858: t15.2024.02.25 val PER: 0.0913
2026-01-17 13:27:41,858: t15.2024.03.08 val PER: 0.1949
2026-01-17 13:27:41,858: t15.2024.03.15 val PER: 0.1720
2026-01-17 13:27:41,858: t15.2024.03.17 val PER: 0.1032
2026-01-17 13:27:41,858: t15.2024.05.10 val PER: 0.1516
2026-01-17 13:27:41,858: t15.2024.06.14 val PER: 0.1546
2026-01-17 13:27:41,859: t15.2024.07.19 val PER: 0.1978
2026-01-17 13:27:41,859: t15.2024.07.21 val PER: 0.0807
2026-01-17 13:27:41,859: t15.2024.07.28 val PER: 0.1096
2026-01-17 13:27:41,859: t15.2025.01.10 val PER: 0.2672
2026-01-17 13:27:41,859: t15.2025.01.12 val PER: 0.1247
2026-01-17 13:27:41,859: t15.2025.03.14 val PER: 0.3299
2026-01-17 13:27:41,859: t15.2025.03.16 val PER: 0.1440
2026-01-17 13:27:41,859: t15.2025.03.30 val PER: 0.2368
2026-01-17 13:27:41,859: t15.2025.04.13 val PER: 0.1997
2026-01-17 13:27:59,334: Train batch 19200: loss: 3.33 grad norm: 40.75 time: 0.062
2026-01-17 13:28:16,963: Train batch 19400: loss: 2.01 grad norm: 34.76 time: 0.052
2026-01-17 13:28:25,774: Running test after training batch: 19500
2026-01-17 13:28:25,864: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:28:30,509: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:28:30,542: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sette
2026-01-17 13:28:32,425: Val batch 19500: PER (avg): 0.1268 CTC Loss (avg): 15.0431 WER(1gram): 45.18% (n=64) time: 6.651
2026-01-17 13:28:32,426: WER lens: avg_true_words=6.16 avg_pred_words=6.31 max_pred_words=12
2026-01-17 13:28:32,426: t15.2023.08.13 val PER: 0.0936
2026-01-17 13:28:32,426: t15.2023.08.18 val PER: 0.0947
2026-01-17 13:28:32,426: t15.2023.08.20 val PER: 0.0810
2026-01-17 13:28:32,426: t15.2023.08.25 val PER: 0.0858
2026-01-17 13:28:32,426: t15.2023.08.27 val PER: 0.1608
2026-01-17 13:28:32,426: t15.2023.09.01 val PER: 0.0609
2026-01-17 13:28:32,427: t15.2023.09.03 val PER: 0.1473
2026-01-17 13:28:32,427: t15.2023.09.24 val PER: 0.1032
2026-01-17 13:28:32,427: t15.2023.09.29 val PER: 0.1136
2026-01-17 13:28:32,427: t15.2023.10.01 val PER: 0.1552
2026-01-17 13:28:32,427: t15.2023.10.06 val PER: 0.0689
2026-01-17 13:28:32,427: t15.2023.10.08 val PER: 0.2219
2026-01-17 13:28:32,427: t15.2023.10.13 val PER: 0.1699
2026-01-17 13:28:32,427: t15.2023.10.15 val PER: 0.1252
2026-01-17 13:28:32,427: t15.2023.10.20 val PER: 0.1779
2026-01-17 13:28:32,427: t15.2023.10.22 val PER: 0.1080
2026-01-17 13:28:32,427: t15.2023.11.03 val PER: 0.1547
2026-01-17 13:28:32,427: t15.2023.11.04 val PER: 0.0239
2026-01-17 13:28:32,428: t15.2023.11.17 val PER: 0.0311
2026-01-17 13:28:32,428: t15.2023.11.19 val PER: 0.0259
2026-01-17 13:28:32,428: t15.2023.11.26 val PER: 0.0783
2026-01-17 13:28:32,428: t15.2023.12.03 val PER: 0.0851
2026-01-17 13:28:32,428: t15.2023.12.08 val PER: 0.0619
2026-01-17 13:28:32,428: t15.2023.12.10 val PER: 0.0578
2026-01-17 13:28:32,428: t15.2023.12.17 val PER: 0.0998
2026-01-17 13:28:32,428: t15.2023.12.29 val PER: 0.1050
2026-01-17 13:28:32,428: t15.2024.02.25 val PER: 0.0899
2026-01-17 13:28:32,428: t15.2024.03.08 val PER: 0.2063
2026-01-17 13:28:32,428: t15.2024.03.15 val PER: 0.1807
2026-01-17 13:28:32,428: t15.2024.03.17 val PER: 0.0976
2026-01-17 13:28:32,429: t15.2024.05.10 val PER: 0.1441
2026-01-17 13:28:32,429: t15.2024.06.14 val PER: 0.1467
2026-01-17 13:28:32,429: t15.2024.07.19 val PER: 0.2017
2026-01-17 13:28:32,429: t15.2024.07.21 val PER: 0.0724
2026-01-17 13:28:32,429: t15.2024.07.28 val PER: 0.1029
2026-01-17 13:28:32,429: t15.2025.01.10 val PER: 0.2713
2026-01-17 13:28:32,429: t15.2025.01.12 val PER: 0.1309
2026-01-17 13:28:32,429: t15.2025.03.14 val PER: 0.3254
2026-01-17 13:28:32,429: t15.2025.03.16 val PER: 0.1597
2026-01-17 13:28:32,430: t15.2025.03.30 val PER: 0.2517
2026-01-17 13:28:32,430: t15.2025.04.13 val PER: 0.2168
2026-01-17 13:28:51,122: Train batch 19600: loss: 3.09 grad norm: 37.67 time: 0.057
2026-01-17 13:29:09,777: Train batch 19800: loss: 3.16 grad norm: 37.13 time: 0.054
2026-01-17 13:29:27,168: Train batch 20000: loss: 3.20 grad norm: 44.72 time: 0.068
2026-01-17 13:29:27,168: Running test after training batch: 20000
2026-01-17 13:29:27,278: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:29:32,635: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:29:32,669: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sitze
2026-01-17 13:29:34,519: Val batch 20000: PER (avg): 0.1260 CTC Loss (avg): 15.0828 WER(1gram): 45.69% (n=64) time: 7.350
2026-01-17 13:29:34,519: WER lens: avg_true_words=6.16 avg_pred_words=6.34 max_pred_words=12
2026-01-17 13:29:34,519: t15.2023.08.13 val PER: 0.1008
2026-01-17 13:29:34,519: t15.2023.08.18 val PER: 0.0821
2026-01-17 13:29:34,519: t15.2023.08.20 val PER: 0.0794
2026-01-17 13:29:34,520: t15.2023.08.25 val PER: 0.0873
2026-01-17 13:29:34,520: t15.2023.08.27 val PER: 0.1543
2026-01-17 13:29:34,520: t15.2023.09.01 val PER: 0.0625
2026-01-17 13:29:34,520: t15.2023.09.03 val PER: 0.1330
2026-01-17 13:29:34,520: t15.2023.09.24 val PER: 0.1019
2026-01-17 13:29:34,520: t15.2023.09.29 val PER: 0.1130
2026-01-17 13:29:34,520: t15.2023.10.01 val PER: 0.1612
2026-01-17 13:29:34,520: t15.2023.10.06 val PER: 0.0764
2026-01-17 13:29:34,520: t15.2023.10.08 val PER: 0.2192
2026-01-17 13:29:34,520: t15.2023.10.13 val PER: 0.1715
2026-01-17 13:29:34,520: t15.2023.10.15 val PER: 0.1259
2026-01-17 13:29:34,520: t15.2023.10.20 val PER: 0.2047
2026-01-17 13:29:34,521: t15.2023.10.22 val PER: 0.0991
2026-01-17 13:29:34,521: t15.2023.11.03 val PER: 0.1533
2026-01-17 13:29:34,521: t15.2023.11.04 val PER: 0.0239
2026-01-17 13:29:34,521: t15.2023.11.17 val PER: 0.0358
2026-01-17 13:29:34,521: t15.2023.11.19 val PER: 0.0200
2026-01-17 13:29:34,521: t15.2023.11.26 val PER: 0.0725
2026-01-17 13:29:34,521: t15.2023.12.03 val PER: 0.0830
2026-01-17 13:29:34,521: t15.2023.12.08 val PER: 0.0606
2026-01-17 13:29:34,521: t15.2023.12.10 val PER: 0.0486
2026-01-17 13:29:34,521: t15.2023.12.17 val PER: 0.1227
2026-01-17 13:29:34,521: t15.2023.12.29 val PER: 0.1030
2026-01-17 13:29:34,521: t15.2024.02.25 val PER: 0.0997
2026-01-17 13:29:34,521: t15.2024.03.08 val PER: 0.2063
2026-01-17 13:29:34,521: t15.2024.03.15 val PER: 0.1814
2026-01-17 13:29:34,521: t15.2024.03.17 val PER: 0.0983
2026-01-17 13:29:34,521: t15.2024.05.10 val PER: 0.1352
2026-01-17 13:29:34,521: t15.2024.06.14 val PER: 0.1593
2026-01-17 13:29:34,522: t15.2024.07.19 val PER: 0.2024
2026-01-17 13:29:34,522: t15.2024.07.21 val PER: 0.0641
2026-01-17 13:29:34,522: t15.2024.07.28 val PER: 0.1074
2026-01-17 13:29:34,522: t15.2025.01.10 val PER: 0.2755
2026-01-17 13:29:34,522: t15.2025.01.12 val PER: 0.1278
2026-01-17 13:29:34,522: t15.2025.03.14 val PER: 0.3195
2026-01-17 13:29:34,522: t15.2025.03.16 val PER: 0.1584
2026-01-17 13:29:34,522: t15.2025.03.30 val PER: 0.2437
2026-01-17 13:29:34,522: t15.2025.04.13 val PER: 0.1926
2026-01-17 13:29:52,274: Train batch 20200: loss: 2.29 grad norm: 36.79 time: 0.060
2026-01-17 13:30:10,631: Train batch 20400: loss: 3.61 grad norm: 41.18 time: 0.063
2026-01-17 13:30:19,415: Running test after training batch: 20500
2026-01-17 13:30:19,553: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:30:24,331: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:30:24,364: WER debug example
  GT : how does it keep the cost down
  PR : hower dust it keep the cost et
2026-01-17 13:30:26,224: Val batch 20500: PER (avg): 0.1247 CTC Loss (avg): 15.1930 WER(1gram): 45.94% (n=64) time: 6.808
2026-01-17 13:30:26,224: WER lens: avg_true_words=6.16 avg_pred_words=6.36 max_pred_words=12
2026-01-17 13:30:26,224: t15.2023.08.13 val PER: 0.0894
2026-01-17 13:30:26,224: t15.2023.08.18 val PER: 0.0830
2026-01-17 13:30:26,224: t15.2023.08.20 val PER: 0.0763
2026-01-17 13:30:26,224: t15.2023.08.25 val PER: 0.0723
2026-01-17 13:30:26,225: t15.2023.08.27 val PER: 0.1704
2026-01-17 13:30:26,225: t15.2023.09.01 val PER: 0.0584
2026-01-17 13:30:26,225: t15.2023.09.03 val PER: 0.1520
2026-01-17 13:30:26,225: t15.2023.09.24 val PER: 0.0947
2026-01-17 13:30:26,225: t15.2023.09.29 val PER: 0.1110
2026-01-17 13:30:26,225: t15.2023.10.01 val PER: 0.1480
2026-01-17 13:30:26,225: t15.2023.10.06 val PER: 0.0732
2026-01-17 13:30:26,225: t15.2023.10.08 val PER: 0.2260
2026-01-17 13:30:26,225: t15.2023.10.13 val PER: 0.1660
2026-01-17 13:30:26,225: t15.2023.10.15 val PER: 0.1246
2026-01-17 13:30:26,225: t15.2023.10.20 val PER: 0.1779
2026-01-17 13:30:26,225: t15.2023.10.22 val PER: 0.0991
2026-01-17 13:30:26,225: t15.2023.11.03 val PER: 0.1608
2026-01-17 13:30:26,225: t15.2023.11.04 val PER: 0.0341
2026-01-17 13:30:26,225: t15.2023.11.17 val PER: 0.0311
2026-01-17 13:30:26,226: t15.2023.11.19 val PER: 0.0180
2026-01-17 13:30:26,226: t15.2023.11.26 val PER: 0.0768
2026-01-17 13:30:26,226: t15.2023.12.03 val PER: 0.0767
2026-01-17 13:30:26,226: t15.2023.12.08 val PER: 0.0579
2026-01-17 13:30:26,226: t15.2023.12.10 val PER: 0.0631
2026-01-17 13:30:26,226: t15.2023.12.17 val PER: 0.1206
2026-01-17 13:30:26,226: t15.2023.12.29 val PER: 0.1002
2026-01-17 13:30:26,226: t15.2024.02.25 val PER: 0.0955
2026-01-17 13:30:26,226: t15.2024.03.08 val PER: 0.2290
2026-01-17 13:30:26,226: t15.2024.03.15 val PER: 0.1714
2026-01-17 13:30:26,226: t15.2024.03.17 val PER: 0.0976
2026-01-17 13:30:26,226: t15.2024.05.10 val PER: 0.1471
2026-01-17 13:30:26,226: t15.2024.06.14 val PER: 0.1498
2026-01-17 13:30:26,227: t15.2024.07.19 val PER: 0.2116
2026-01-17 13:30:26,227: t15.2024.07.21 val PER: 0.0697
2026-01-17 13:30:26,227: t15.2024.07.28 val PER: 0.1000
2026-01-17 13:30:26,227: t15.2025.01.10 val PER: 0.2562
2026-01-17 13:30:26,227: t15.2025.01.12 val PER: 0.1293
2026-01-17 13:30:26,227: t15.2025.03.14 val PER: 0.3136
2026-01-17 13:30:26,227: t15.2025.03.16 val PER: 0.1387
2026-01-17 13:30:26,227: t15.2025.03.30 val PER: 0.2414
2026-01-17 13:30:26,227: t15.2025.04.13 val PER: 0.2026
2026-01-17 13:30:34,838: Train batch 20600: loss: 2.78 grad norm: 36.74 time: 0.057
2026-01-17 13:30:52,108: Train batch 20800: loss: 3.53 grad norm: 45.75 time: 0.054
2026-01-17 13:31:09,092: Train batch 21000: loss: 2.65 grad norm: 40.63 time: 0.053
2026-01-17 13:31:09,092: Running test after training batch: 21000
2026-01-17 13:31:09,226: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:31:14,508: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:31:14,543: WER debug example
  GT : how does it keep the cost down
  PR : hower dust it keep the cost get
2026-01-17 13:31:16,350: Val batch 21000: PER (avg): 0.1235 CTC Loss (avg): 15.2163 WER(1gram): 46.70% (n=64) time: 7.258
2026-01-17 13:31:16,351: WER lens: avg_true_words=6.16 avg_pred_words=6.33 max_pred_words=12
2026-01-17 13:31:16,351: t15.2023.08.13 val PER: 0.0925
2026-01-17 13:31:16,351: t15.2023.08.18 val PER: 0.0914
2026-01-17 13:31:16,351: t15.2023.08.20 val PER: 0.0763
2026-01-17 13:31:16,351: t15.2023.08.25 val PER: 0.0783
2026-01-17 13:31:16,351: t15.2023.08.27 val PER: 0.1576
2026-01-17 13:31:16,351: t15.2023.09.01 val PER: 0.0641
2026-01-17 13:31:16,351: t15.2023.09.03 val PER: 0.1354
2026-01-17 13:31:16,351: t15.2023.09.24 val PER: 0.1007
2026-01-17 13:31:16,351: t15.2023.09.29 val PER: 0.1072
2026-01-17 13:31:16,352: t15.2023.10.01 val PER: 0.1546
2026-01-17 13:31:16,352: t15.2023.10.06 val PER: 0.0678
2026-01-17 13:31:16,352: t15.2023.10.08 val PER: 0.2260
2026-01-17 13:31:16,352: t15.2023.10.13 val PER: 0.1730
2026-01-17 13:31:16,352: t15.2023.10.15 val PER: 0.1246
2026-01-17 13:31:16,352: t15.2023.10.20 val PER: 0.1879
2026-01-17 13:31:16,352: t15.2023.10.22 val PER: 0.0980
2026-01-17 13:31:16,352: t15.2023.11.03 val PER: 0.1574
2026-01-17 13:31:16,352: t15.2023.11.04 val PER: 0.0307
2026-01-17 13:31:16,352: t15.2023.11.17 val PER: 0.0264
2026-01-17 13:31:16,352: t15.2023.11.19 val PER: 0.0240
2026-01-17 13:31:16,352: t15.2023.11.26 val PER: 0.0630
2026-01-17 13:31:16,352: t15.2023.12.03 val PER: 0.0756
2026-01-17 13:31:16,352: t15.2023.12.08 val PER: 0.0606
2026-01-17 13:31:16,353: t15.2023.12.10 val PER: 0.0552
2026-01-17 13:31:16,353: t15.2023.12.17 val PER: 0.0998
2026-01-17 13:31:16,353: t15.2023.12.29 val PER: 0.0954
2026-01-17 13:31:16,353: t15.2024.02.25 val PER: 0.0941
2026-01-17 13:31:16,353: t15.2024.03.08 val PER: 0.2034
2026-01-17 13:31:16,353: t15.2024.03.15 val PER: 0.1689
2026-01-17 13:31:16,353: t15.2024.03.17 val PER: 0.1046
2026-01-17 13:31:16,353: t15.2024.05.10 val PER: 0.1486
2026-01-17 13:31:16,353: t15.2024.06.14 val PER: 0.1435
2026-01-17 13:31:16,353: t15.2024.07.19 val PER: 0.2017
2026-01-17 13:31:16,353: t15.2024.07.21 val PER: 0.0731
2026-01-17 13:31:16,353: t15.2024.07.28 val PER: 0.1074
2026-01-17 13:31:16,354: t15.2025.01.10 val PER: 0.2769
2026-01-17 13:31:16,354: t15.2025.01.12 val PER: 0.1239
2026-01-17 13:31:16,354: t15.2025.03.14 val PER: 0.3166
2026-01-17 13:31:16,354: t15.2025.03.16 val PER: 0.1558
2026-01-17 13:31:16,354: t15.2025.03.30 val PER: 0.2356
2026-01-17 13:31:16,354: t15.2025.04.13 val PER: 0.1826
2026-01-17 13:31:33,809: Train batch 21200: loss: 2.07 grad norm: 28.79 time: 0.078
2026-01-17 13:31:51,215: Train batch 21400: loss: 4.14 grad norm: 39.55 time: 0.059
2026-01-17 13:32:00,002: Running test after training batch: 21500
2026-01-17 13:32:00,096: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:32:05,329: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-17 13:32:05,361: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sends
2026-01-17 13:32:07,195: Val batch 21500: PER (avg): 0.1218 CTC Loss (avg): 15.1479 WER(1gram): 45.69% (n=64) time: 7.193
2026-01-17 13:32:07,196: WER lens: avg_true_words=6.16 avg_pred_words=6.36 max_pred_words=12
2026-01-17 13:32:07,196: t15.2023.08.13 val PER: 0.0904
2026-01-17 13:32:07,196: t15.2023.08.18 val PER: 0.0796
2026-01-17 13:32:07,196: t15.2023.08.20 val PER: 0.0643
2026-01-17 13:32:07,196: t15.2023.08.25 val PER: 0.0783
2026-01-17 13:32:07,196: t15.2023.08.27 val PER: 0.1656
2026-01-17 13:32:07,196: t15.2023.09.01 val PER: 0.0593
2026-01-17 13:32:07,196: t15.2023.09.03 val PER: 0.1401
2026-01-17 13:32:07,196: t15.2023.09.24 val PER: 0.1032
2026-01-17 13:32:07,196: t15.2023.09.29 val PER: 0.1053
2026-01-17 13:32:07,197: t15.2023.10.01 val PER: 0.1513
2026-01-17 13:32:07,197: t15.2023.10.06 val PER: 0.0743
2026-01-17 13:32:07,197: t15.2023.10.08 val PER: 0.2152
2026-01-17 13:32:07,197: t15.2023.10.13 val PER: 0.1761
2026-01-17 13:32:07,197: t15.2023.10.15 val PER: 0.1180
2026-01-17 13:32:07,197: t15.2023.10.20 val PER: 0.1812
2026-01-17 13:32:07,197: t15.2023.10.22 val PER: 0.0891
2026-01-17 13:32:07,197: t15.2023.11.03 val PER: 0.1669
2026-01-17 13:32:07,197: t15.2023.11.04 val PER: 0.0307
2026-01-17 13:32:07,197: t15.2023.11.17 val PER: 0.0327
2026-01-17 13:32:07,197: t15.2023.11.19 val PER: 0.0200
2026-01-17 13:32:07,197: t15.2023.11.26 val PER: 0.0746
2026-01-17 13:32:07,197: t15.2023.12.03 val PER: 0.0714
2026-01-17 13:32:07,197: t15.2023.12.08 val PER: 0.0573
2026-01-17 13:32:07,197: t15.2023.12.10 val PER: 0.0526
2026-01-17 13:32:07,197: t15.2023.12.17 val PER: 0.1050
2026-01-17 13:32:07,198: t15.2023.12.29 val PER: 0.0988
2026-01-17 13:32:07,198: t15.2024.02.25 val PER: 0.0885
2026-01-17 13:32:07,198: t15.2024.03.08 val PER: 0.2034
2026-01-17 13:32:07,198: t15.2024.03.15 val PER: 0.1651
2026-01-17 13:32:07,198: t15.2024.03.17 val PER: 0.0969
2026-01-17 13:32:07,198: t15.2024.05.10 val PER: 0.1367
2026-01-17 13:32:07,198: t15.2024.06.14 val PER: 0.1451
2026-01-17 13:32:07,198: t15.2024.07.19 val PER: 0.1978
2026-01-17 13:32:07,198: t15.2024.07.21 val PER: 0.0655
2026-01-17 13:32:07,199: t15.2024.07.28 val PER: 0.1029
2026-01-17 13:32:07,199: t15.2025.01.10 val PER: 0.2700
2026-01-17 13:32:07,199: t15.2025.01.12 val PER: 0.1232
2026-01-17 13:32:07,199: t15.2025.03.14 val PER: 0.3151
2026-01-17 13:32:07,199: t15.2025.03.16 val PER: 0.1584
2026-01-17 13:32:07,199: t15.2025.03.30 val PER: 0.2322
2026-01-17 13:32:07,199: t15.2025.04.13 val PER: 0.1983
2026-01-17 13:32:16,108: Train batch 21600: loss: 4.14 grad norm: 61.46 time: 0.075
2026-01-17 13:32:34,164: Train batch 21800: loss: 2.41 grad norm: 34.33 time: 0.080
2026-01-17 13:32:52,196: Train batch 22000: loss: 4.70 grad norm: 49.16 time: 0.052
2026-01-17 13:32:52,196: Running test after training batch: 22000
2026-01-17 13:32:52,368: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:32:57,675: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:32:57,707: WER debug example
  GT : how does it keep the cost down
  PR : hower dust it keep the cost sitze
2026-01-17 13:32:59,529: Val batch 22000: PER (avg): 0.1223 CTC Loss (avg): 15.3199 WER(1gram): 48.48% (n=64) time: 7.333
2026-01-17 13:32:59,530: WER lens: avg_true_words=6.16 avg_pred_words=6.41 max_pred_words=12
2026-01-17 13:32:59,530: t15.2023.08.13 val PER: 0.0977
2026-01-17 13:32:59,530: t15.2023.08.18 val PER: 0.0855
2026-01-17 13:32:59,530: t15.2023.08.20 val PER: 0.0763
2026-01-17 13:32:59,530: t15.2023.08.25 val PER: 0.0813
2026-01-17 13:32:59,530: t15.2023.08.27 val PER: 0.1592
2026-01-17 13:32:59,530: t15.2023.09.01 val PER: 0.0552
2026-01-17 13:32:59,530: t15.2023.09.03 val PER: 0.1354
2026-01-17 13:32:59,531: t15.2023.09.24 val PER: 0.1019
2026-01-17 13:32:59,531: t15.2023.09.29 val PER: 0.1002
2026-01-17 13:32:59,531: t15.2023.10.01 val PER: 0.1493
2026-01-17 13:32:59,531: t15.2023.10.06 val PER: 0.0743
2026-01-17 13:32:59,531: t15.2023.10.08 val PER: 0.2233
2026-01-17 13:32:59,531: t15.2023.10.13 val PER: 0.1668
2026-01-17 13:32:59,531: t15.2023.10.15 val PER: 0.1285
2026-01-17 13:32:59,531: t15.2023.10.20 val PER: 0.1577
2026-01-17 13:32:59,531: t15.2023.10.22 val PER: 0.1080
2026-01-17 13:32:59,531: t15.2023.11.03 val PER: 0.1526
2026-01-17 13:32:59,531: t15.2023.11.04 val PER: 0.0273
2026-01-17 13:32:59,531: t15.2023.11.17 val PER: 0.0311
2026-01-17 13:32:59,531: t15.2023.11.19 val PER: 0.0120
2026-01-17 13:32:59,531: t15.2023.11.26 val PER: 0.0688
2026-01-17 13:32:59,531: t15.2023.12.03 val PER: 0.0746
2026-01-17 13:32:59,532: t15.2023.12.08 val PER: 0.0613
2026-01-17 13:32:59,532: t15.2023.12.10 val PER: 0.0578
2026-01-17 13:32:59,532: t15.2023.12.17 val PER: 0.1175
2026-01-17 13:32:59,532: t15.2023.12.29 val PER: 0.0961
2026-01-17 13:32:59,532: t15.2024.02.25 val PER: 0.0927
2026-01-17 13:32:59,532: t15.2024.03.08 val PER: 0.1963
2026-01-17 13:32:59,532: t15.2024.03.15 val PER: 0.1701
2026-01-17 13:32:59,532: t15.2024.03.17 val PER: 0.0990
2026-01-17 13:32:59,532: t15.2024.05.10 val PER: 0.1263
2026-01-17 13:32:59,532: t15.2024.06.14 val PER: 0.1420
2026-01-17 13:32:59,532: t15.2024.07.19 val PER: 0.1991
2026-01-17 13:32:59,532: t15.2024.07.21 val PER: 0.0662
2026-01-17 13:32:59,532: t15.2024.07.28 val PER: 0.1029
2026-01-17 13:32:59,532: t15.2025.01.10 val PER: 0.2617
2026-01-17 13:32:59,532: t15.2025.01.12 val PER: 0.1332
2026-01-17 13:32:59,532: t15.2025.03.14 val PER: 0.3299
2026-01-17 13:32:59,532: t15.2025.03.16 val PER: 0.1479
2026-01-17 13:32:59,532: t15.2025.03.30 val PER: 0.2345
2026-01-17 13:32:59,533: t15.2025.04.13 val PER: 0.1869
2026-01-17 13:33:17,741: Train batch 22200: loss: 2.66 grad norm: 33.05 time: 0.060
2026-01-17 13:33:35,393: Train batch 22400: loss: 2.17 grad norm: 32.57 time: 0.054
2026-01-17 13:33:44,662: Running test after training batch: 22500
2026-01-17 13:33:44,815: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:33:50,154: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:33:50,187: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cussed sitze
2026-01-17 13:33:52,096: Val batch 22500: PER (avg): 0.1212 CTC Loss (avg): 15.5699 WER(1gram): 45.43% (n=64) time: 7.434
2026-01-17 13:33:52,096: WER lens: avg_true_words=6.16 avg_pred_words=6.34 max_pred_words=12
2026-01-17 13:33:52,096: t15.2023.08.13 val PER: 0.0884
2026-01-17 13:33:52,096: t15.2023.08.18 val PER: 0.0880
2026-01-17 13:33:52,096: t15.2023.08.20 val PER: 0.0842
2026-01-17 13:33:52,096: t15.2023.08.25 val PER: 0.0843
2026-01-17 13:33:52,096: t15.2023.08.27 val PER: 0.1785
2026-01-17 13:33:52,097: t15.2023.09.01 val PER: 0.0593
2026-01-17 13:33:52,097: t15.2023.09.03 val PER: 0.1235
2026-01-17 13:33:52,097: t15.2023.09.24 val PER: 0.1019
2026-01-17 13:33:52,097: t15.2023.09.29 val PER: 0.1066
2026-01-17 13:33:52,097: t15.2023.10.01 val PER: 0.1605
2026-01-17 13:33:52,097: t15.2023.10.06 val PER: 0.0614
2026-01-17 13:33:52,097: t15.2023.10.08 val PER: 0.2206
2026-01-17 13:33:52,097: t15.2023.10.13 val PER: 0.1730
2026-01-17 13:33:52,097: t15.2023.10.15 val PER: 0.1233
2026-01-17 13:33:52,097: t15.2023.10.20 val PER: 0.1745
2026-01-17 13:33:52,097: t15.2023.10.22 val PER: 0.0924
2026-01-17 13:33:52,097: t15.2023.11.03 val PER: 0.1601
2026-01-17 13:33:52,097: t15.2023.11.04 val PER: 0.0239
2026-01-17 13:33:52,097: t15.2023.11.17 val PER: 0.0233
2026-01-17 13:33:52,098: t15.2023.11.19 val PER: 0.0160
2026-01-17 13:33:52,098: t15.2023.11.26 val PER: 0.0667
2026-01-17 13:33:52,098: t15.2023.12.03 val PER: 0.0672
2026-01-17 13:33:52,098: t15.2023.12.08 val PER: 0.0566
2026-01-17 13:33:52,098: t15.2023.12.10 val PER: 0.0539
2026-01-17 13:33:52,098: t15.2023.12.17 val PER: 0.0967
2026-01-17 13:33:52,098: t15.2023.12.29 val PER: 0.0961
2026-01-17 13:33:52,098: t15.2024.02.25 val PER: 0.0772
2026-01-17 13:33:52,098: t15.2024.03.08 val PER: 0.2063
2026-01-17 13:33:52,098: t15.2024.03.15 val PER: 0.1732
2026-01-17 13:33:52,098: t15.2024.03.17 val PER: 0.1039
2026-01-17 13:33:52,098: t15.2024.05.10 val PER: 0.1322
2026-01-17 13:33:52,098: t15.2024.06.14 val PER: 0.1356
2026-01-17 13:33:52,098: t15.2024.07.19 val PER: 0.2011
2026-01-17 13:33:52,098: t15.2024.07.21 val PER: 0.0655
2026-01-17 13:33:52,098: t15.2024.07.28 val PER: 0.0993
2026-01-17 13:33:52,099: t15.2025.01.10 val PER: 0.2769
2026-01-17 13:33:52,099: t15.2025.01.12 val PER: 0.1132
2026-01-17 13:33:52,099: t15.2025.03.14 val PER: 0.3047
2026-01-17 13:33:52,099: t15.2025.03.16 val PER: 0.1414
2026-01-17 13:33:52,099: t15.2025.03.30 val PER: 0.2333
2026-01-17 13:33:52,099: t15.2025.04.13 val PER: 0.1912
2026-01-17 13:34:00,861: Train batch 22600: loss: 4.27 grad norm: 45.90 time: 0.059
2026-01-17 13:34:18,538: Train batch 22800: loss: 3.53 grad norm: 43.10 time: 0.058
2026-01-17 13:34:36,298: Train batch 23000: loss: 3.23 grad norm: 46.55 time: 0.062
2026-01-17 13:34:36,299: Running test after training batch: 23000
2026-01-17 13:34:36,450: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:34:42,184: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:34:42,218: WER debug example
  GT : how does it keep the cost down
  PR : hower dust it keep the cost nett
2026-01-17 13:34:44,076: Val batch 23000: PER (avg): 0.1213 CTC Loss (avg): 15.3980 WER(1gram): 45.43% (n=64) time: 7.777
2026-01-17 13:34:44,077: WER lens: avg_true_words=6.16 avg_pred_words=6.28 max_pred_words=12
2026-01-17 13:34:44,077: t15.2023.08.13 val PER: 0.0936
2026-01-17 13:34:44,077: t15.2023.08.18 val PER: 0.0838
2026-01-17 13:34:44,077: t15.2023.08.20 val PER: 0.0715
2026-01-17 13:34:44,077: t15.2023.08.25 val PER: 0.0783
2026-01-17 13:34:44,077: t15.2023.08.27 val PER: 0.1592
2026-01-17 13:34:44,077: t15.2023.09.01 val PER: 0.0519
2026-01-17 13:34:44,077: t15.2023.09.03 val PER: 0.1425
2026-01-17 13:34:44,078: t15.2023.09.24 val PER: 0.1044
2026-01-17 13:34:44,078: t15.2023.09.29 val PER: 0.1098
2026-01-17 13:34:44,078: t15.2023.10.01 val PER: 0.1460
2026-01-17 13:34:44,078: t15.2023.10.06 val PER: 0.0657
2026-01-17 13:34:44,078: t15.2023.10.08 val PER: 0.2111
2026-01-17 13:34:44,078: t15.2023.10.13 val PER: 0.1637
2026-01-17 13:34:44,078: t15.2023.10.15 val PER: 0.1213
2026-01-17 13:34:44,079: t15.2023.10.20 val PER: 0.1846
2026-01-17 13:34:44,079: t15.2023.10.22 val PER: 0.0947
2026-01-17 13:34:44,079: t15.2023.11.03 val PER: 0.1635
2026-01-17 13:34:44,079: t15.2023.11.04 val PER: 0.0239
2026-01-17 13:34:44,079: t15.2023.11.17 val PER: 0.0249
2026-01-17 13:34:44,079: t15.2023.11.19 val PER: 0.0180
2026-01-17 13:34:44,079: t15.2023.11.26 val PER: 0.0696
2026-01-17 13:34:44,079: t15.2023.12.03 val PER: 0.0693
2026-01-17 13:34:44,079: t15.2023.12.08 val PER: 0.0613
2026-01-17 13:34:44,079: t15.2023.12.10 val PER: 0.0447
2026-01-17 13:34:44,080: t15.2023.12.17 val PER: 0.1040
2026-01-17 13:34:44,080: t15.2023.12.29 val PER: 0.0968
2026-01-17 13:34:44,080: t15.2024.02.25 val PER: 0.0843
2026-01-17 13:34:44,080: t15.2024.03.08 val PER: 0.2020
2026-01-17 13:34:44,080: t15.2024.03.15 val PER: 0.1695
2026-01-17 13:34:44,080: t15.2024.03.17 val PER: 0.0914
2026-01-17 13:34:44,080: t15.2024.05.10 val PER: 0.1456
2026-01-17 13:34:44,080: t15.2024.06.14 val PER: 0.1341
2026-01-17 13:34:44,081: t15.2024.07.19 val PER: 0.1984
2026-01-17 13:34:44,081: t15.2024.07.21 val PER: 0.0710
2026-01-17 13:34:44,081: t15.2024.07.28 val PER: 0.1132
2026-01-17 13:34:44,081: t15.2025.01.10 val PER: 0.2741
2026-01-17 13:34:44,081: t15.2025.01.12 val PER: 0.1209
2026-01-17 13:34:44,081: t15.2025.03.14 val PER: 0.3047
2026-01-17 13:34:44,081: t15.2025.03.16 val PER: 0.1322
2026-01-17 13:34:44,081: t15.2025.03.30 val PER: 0.2552
2026-01-17 13:34:44,081: t15.2025.04.13 val PER: 0.2026
2026-01-17 13:35:02,973: Train batch 23200: loss: 3.05 grad norm: 48.98 time: 0.060
2026-01-17 13:35:23,338: Train batch 23400: loss: 1.50 grad norm: 29.75 time: 0.070
2026-01-17 13:35:32,758: Running test after training batch: 23500
2026-01-17 13:35:32,867: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:35:38,933: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:35:38,969: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sette
2026-01-17 13:35:40,887: Val batch 23500: PER (avg): 0.1204 CTC Loss (avg): 15.8626 WER(1gram): 47.46% (n=64) time: 8.129
2026-01-17 13:35:40,888: WER lens: avg_true_words=6.16 avg_pred_words=6.39 max_pred_words=12
2026-01-17 13:35:40,888: t15.2023.08.13 val PER: 0.0956
2026-01-17 13:35:40,888: t15.2023.08.18 val PER: 0.0872
2026-01-17 13:35:40,888: t15.2023.08.20 val PER: 0.0818
2026-01-17 13:35:40,888: t15.2023.08.25 val PER: 0.0723
2026-01-17 13:35:40,889: t15.2023.08.27 val PER: 0.1511
2026-01-17 13:35:40,889: t15.2023.09.01 val PER: 0.0552
2026-01-17 13:35:40,889: t15.2023.09.03 val PER: 0.1461
2026-01-17 13:35:40,889: t15.2023.09.24 val PER: 0.1044
2026-01-17 13:35:40,889: t15.2023.09.29 val PER: 0.1040
2026-01-17 13:35:40,889: t15.2023.10.01 val PER: 0.1473
2026-01-17 13:35:40,889: t15.2023.10.06 val PER: 0.0700
2026-01-17 13:35:40,889: t15.2023.10.08 val PER: 0.2287
2026-01-17 13:35:40,889: t15.2023.10.13 val PER: 0.1544
2026-01-17 13:35:40,890: t15.2023.10.15 val PER: 0.1279
2026-01-17 13:35:40,890: t15.2023.10.20 val PER: 0.1745
2026-01-17 13:35:40,890: t15.2023.10.22 val PER: 0.1013
2026-01-17 13:35:40,891: t15.2023.11.03 val PER: 0.1547
2026-01-17 13:35:40,891: t15.2023.11.04 val PER: 0.0273
2026-01-17 13:35:40,891: t15.2023.11.17 val PER: 0.0264
2026-01-17 13:35:40,891: t15.2023.11.19 val PER: 0.0160
2026-01-17 13:35:40,891: t15.2023.11.26 val PER: 0.0703
2026-01-17 13:35:40,891: t15.2023.12.03 val PER: 0.0735
2026-01-17 13:35:40,892: t15.2023.12.08 val PER: 0.0606
2026-01-17 13:35:40,892: t15.2023.12.10 val PER: 0.0591
2026-01-17 13:35:40,892: t15.2023.12.17 val PER: 0.1081
2026-01-17 13:35:40,892: t15.2023.12.29 val PER: 0.0913
2026-01-17 13:35:40,892: t15.2024.02.25 val PER: 0.0927
2026-01-17 13:35:40,892: t15.2024.03.08 val PER: 0.1963
2026-01-17 13:35:40,892: t15.2024.03.15 val PER: 0.1557
2026-01-17 13:35:40,892: t15.2024.03.17 val PER: 0.0886
2026-01-17 13:35:40,892: t15.2024.05.10 val PER: 0.1426
2026-01-17 13:35:40,892: t15.2024.06.14 val PER: 0.1435
2026-01-17 13:35:40,892: t15.2024.07.19 val PER: 0.1938
2026-01-17 13:35:40,892: t15.2024.07.21 val PER: 0.0662
2026-01-17 13:35:40,892: t15.2024.07.28 val PER: 0.1154
2026-01-17 13:35:40,892: t15.2025.01.10 val PER: 0.2617
2026-01-17 13:35:40,892: t15.2025.01.12 val PER: 0.1255
2026-01-17 13:35:40,892: t15.2025.03.14 val PER: 0.2973
2026-01-17 13:35:40,892: t15.2025.03.16 val PER: 0.1453
2026-01-17 13:35:40,893: t15.2025.03.30 val PER: 0.2356
2026-01-17 13:35:40,893: t15.2025.04.13 val PER: 0.1783
2026-01-17 13:35:51,592: Train batch 23600: loss: 0.57 grad norm: 15.02 time: 0.058
2026-01-17 13:36:10,475: Train batch 23800: loss: 2.72 grad norm: 94.93 time: 0.056
2026-01-17 13:36:29,266: Train batch 24000: loss: 2.36 grad norm: 41.72 time: 0.079
2026-01-17 13:36:29,266: Running test after training batch: 24000
2026-01-17 13:36:29,362: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:36:34,663: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:36:34,697: WER debug example
  GT : how does it keep the cost down
  PR : hower dust it keep the cussed sids
2026-01-17 13:36:36,595: Val batch 24000: PER (avg): 0.1214 CTC Loss (avg): 15.7541 WER(1gram): 46.19% (n=64) time: 7.329
2026-01-17 13:36:36,596: WER lens: avg_true_words=6.16 avg_pred_words=6.33 max_pred_words=12
2026-01-17 13:36:36,596: t15.2023.08.13 val PER: 0.0936
2026-01-17 13:36:36,596: t15.2023.08.18 val PER: 0.0771
2026-01-17 13:36:36,596: t15.2023.08.20 val PER: 0.0802
2026-01-17 13:36:36,596: t15.2023.08.25 val PER: 0.0798
2026-01-17 13:36:36,596: t15.2023.08.27 val PER: 0.1527
2026-01-17 13:36:36,596: t15.2023.09.01 val PER: 0.0601
2026-01-17 13:36:36,597: t15.2023.09.03 val PER: 0.1413
2026-01-17 13:36:36,597: t15.2023.09.24 val PER: 0.0947
2026-01-17 13:36:36,597: t15.2023.09.29 val PER: 0.1021
2026-01-17 13:36:36,597: t15.2023.10.01 val PER: 0.1532
2026-01-17 13:36:36,597: t15.2023.10.06 val PER: 0.0775
2026-01-17 13:36:36,597: t15.2023.10.08 val PER: 0.2165
2026-01-17 13:36:36,597: t15.2023.10.13 val PER: 0.1753
2026-01-17 13:36:36,597: t15.2023.10.15 val PER: 0.1140
2026-01-17 13:36:36,597: t15.2023.10.20 val PER: 0.1678
2026-01-17 13:36:36,597: t15.2023.10.22 val PER: 0.0969
2026-01-17 13:36:36,597: t15.2023.11.03 val PER: 0.1642
2026-01-17 13:36:36,598: t15.2023.11.04 val PER: 0.0307
2026-01-17 13:36:36,598: t15.2023.11.17 val PER: 0.0249
2026-01-17 13:36:36,598: t15.2023.11.19 val PER: 0.0220
2026-01-17 13:36:36,598: t15.2023.11.26 val PER: 0.0710
2026-01-17 13:36:36,598: t15.2023.12.03 val PER: 0.0672
2026-01-17 13:36:36,598: t15.2023.12.08 val PER: 0.0586
2026-01-17 13:36:36,598: t15.2023.12.10 val PER: 0.0447
2026-01-17 13:36:36,598: t15.2023.12.17 val PER: 0.0998
2026-01-17 13:36:36,598: t15.2023.12.29 val PER: 0.0879
2026-01-17 13:36:36,598: t15.2024.02.25 val PER: 0.0899
2026-01-17 13:36:36,598: t15.2024.03.08 val PER: 0.2148
2026-01-17 13:36:36,599: t15.2024.03.15 val PER: 0.1689
2026-01-17 13:36:36,599: t15.2024.03.17 val PER: 0.1011
2026-01-17 13:36:36,599: t15.2024.05.10 val PER: 0.1263
2026-01-17 13:36:36,599: t15.2024.06.14 val PER: 0.1451
2026-01-17 13:36:36,599: t15.2024.07.19 val PER: 0.1918
2026-01-17 13:36:36,599: t15.2024.07.21 val PER: 0.0655
2026-01-17 13:36:36,599: t15.2024.07.28 val PER: 0.1007
2026-01-17 13:36:36,599: t15.2025.01.10 val PER: 0.2617
2026-01-17 13:36:36,599: t15.2025.01.12 val PER: 0.1209
2026-01-17 13:36:36,599: t15.2025.03.14 val PER: 0.3254
2026-01-17 13:36:36,599: t15.2025.03.16 val PER: 0.1662
2026-01-17 13:36:36,599: t15.2025.03.30 val PER: 0.2402
2026-01-17 13:36:36,599: t15.2025.04.13 val PER: 0.2040
2026-01-17 13:36:55,991: Train batch 24200: loss: 1.39 grad norm: 29.64 time: 0.056
2026-01-17 13:37:14,712: Train batch 24400: loss: 4.33 grad norm: 50.41 time: 0.053
2026-01-17 13:37:23,982: Running test after training batch: 24500
2026-01-17 13:37:24,133: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:37:29,487: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:37:29,519: WER debug example
  GT : how does it keep the cost down
  PR : hower dust it keep the cost sets
2026-01-17 13:37:31,386: Val batch 24500: PER (avg): 0.1209 CTC Loss (avg): 16.0143 WER(1gram): 45.94% (n=64) time: 7.403
2026-01-17 13:37:31,386: WER lens: avg_true_words=6.16 avg_pred_words=6.33 max_pred_words=12
2026-01-17 13:37:31,387: t15.2023.08.13 val PER: 0.0894
2026-01-17 13:37:31,387: t15.2023.08.18 val PER: 0.0847
2026-01-17 13:37:31,387: t15.2023.08.20 val PER: 0.0778
2026-01-17 13:37:31,387: t15.2023.08.25 val PER: 0.0813
2026-01-17 13:37:31,387: t15.2023.08.27 val PER: 0.1656
2026-01-17 13:37:31,387: t15.2023.09.01 val PER: 0.0552
2026-01-17 13:37:31,387: t15.2023.09.03 val PER: 0.1330
2026-01-17 13:37:31,387: t15.2023.09.24 val PER: 0.0947
2026-01-17 13:37:31,387: t15.2023.09.29 val PER: 0.1066
2026-01-17 13:37:31,387: t15.2023.10.01 val PER: 0.1480
2026-01-17 13:37:31,387: t15.2023.10.06 val PER: 0.0764
2026-01-17 13:37:31,387: t15.2023.10.08 val PER: 0.2152
2026-01-17 13:37:31,387: t15.2023.10.13 val PER: 0.1715
2026-01-17 13:37:31,387: t15.2023.10.15 val PER: 0.1173
2026-01-17 13:37:31,388: t15.2023.10.20 val PER: 0.1779
2026-01-17 13:37:31,388: t15.2023.10.22 val PER: 0.0902
2026-01-17 13:37:31,388: t15.2023.11.03 val PER: 0.1547
2026-01-17 13:37:31,388: t15.2023.11.04 val PER: 0.0273
2026-01-17 13:37:31,388: t15.2023.11.17 val PER: 0.0264
2026-01-17 13:37:31,388: t15.2023.11.19 val PER: 0.0140
2026-01-17 13:37:31,388: t15.2023.11.26 val PER: 0.0710
2026-01-17 13:37:31,388: t15.2023.12.03 val PER: 0.0641
2026-01-17 13:37:31,388: t15.2023.12.08 val PER: 0.0573
2026-01-17 13:37:31,388: t15.2023.12.10 val PER: 0.0565
2026-01-17 13:37:31,388: t15.2023.12.17 val PER: 0.1008
2026-01-17 13:37:31,388: t15.2023.12.29 val PER: 0.0975
2026-01-17 13:37:31,388: t15.2024.02.25 val PER: 0.0843
2026-01-17 13:37:31,388: t15.2024.03.08 val PER: 0.2105
2026-01-17 13:37:31,389: t15.2024.03.15 val PER: 0.1695
2026-01-17 13:37:31,389: t15.2024.03.17 val PER: 0.0907
2026-01-17 13:37:31,389: t15.2024.05.10 val PER: 0.1471
2026-01-17 13:37:31,389: t15.2024.06.14 val PER: 0.1420
2026-01-17 13:37:31,389: t15.2024.07.19 val PER: 0.1905
2026-01-17 13:37:31,389: t15.2024.07.21 val PER: 0.0717
2026-01-17 13:37:31,389: t15.2024.07.28 val PER: 0.1059
2026-01-17 13:37:31,389: t15.2025.01.10 val PER: 0.2741
2026-01-17 13:37:31,389: t15.2025.01.12 val PER: 0.1278
2026-01-17 13:37:31,389: t15.2025.03.14 val PER: 0.3077
2026-01-17 13:37:31,389: t15.2025.03.16 val PER: 0.1531
2026-01-17 13:37:31,389: t15.2025.03.30 val PER: 0.2310
2026-01-17 13:37:31,389: t15.2025.04.13 val PER: 0.1940
2026-01-17 13:37:41,558: Train batch 24600: loss: 2.68 grad norm: 39.60 time: 0.058
2026-01-17 13:38:01,762: Train batch 24800: loss: 2.42 grad norm: 41.07 time: 0.078
2026-01-17 13:38:21,565: Train batch 25000: loss: 2.66 grad norm: 41.09 time: 0.065
2026-01-17 13:38:21,566: Running test after training batch: 25000
2026-01-17 13:38:21,714: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:38:27,267: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:38:27,303: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sids
2026-01-17 13:38:29,208: Val batch 25000: PER (avg): 0.1175 CTC Loss (avg): 15.9859 WER(1gram): 43.91% (n=64) time: 7.642
2026-01-17 13:38:29,209: WER lens: avg_true_words=6.16 avg_pred_words=6.33 max_pred_words=12
2026-01-17 13:38:29,209: t15.2023.08.13 val PER: 0.0915
2026-01-17 13:38:29,209: t15.2023.08.18 val PER: 0.0771
2026-01-17 13:38:29,209: t15.2023.08.20 val PER: 0.0667
2026-01-17 13:38:29,209: t15.2023.08.25 val PER: 0.0738
2026-01-17 13:38:29,209: t15.2023.08.27 val PER: 0.1543
2026-01-17 13:38:29,209: t15.2023.09.01 val PER: 0.0544
2026-01-17 13:38:29,209: t15.2023.09.03 val PER: 0.1318
2026-01-17 13:38:29,209: t15.2023.09.24 val PER: 0.0934
2026-01-17 13:38:29,209: t15.2023.09.29 val PER: 0.1034
2026-01-17 13:38:29,210: t15.2023.10.01 val PER: 0.1585
2026-01-17 13:38:29,210: t15.2023.10.06 val PER: 0.0678
2026-01-17 13:38:29,210: t15.2023.10.08 val PER: 0.2165
2026-01-17 13:38:29,210: t15.2023.10.13 val PER: 0.1544
2026-01-17 13:38:29,210: t15.2023.10.15 val PER: 0.1154
2026-01-17 13:38:29,210: t15.2023.10.20 val PER: 0.1711
2026-01-17 13:38:29,210: t15.2023.10.22 val PER: 0.0880
2026-01-17 13:38:29,210: t15.2023.11.03 val PER: 0.1472
2026-01-17 13:38:29,210: t15.2023.11.04 val PER: 0.0307
2026-01-17 13:38:29,210: t15.2023.11.17 val PER: 0.0264
2026-01-17 13:38:29,210: t15.2023.11.19 val PER: 0.0200
2026-01-17 13:38:29,210: t15.2023.11.26 val PER: 0.0681
2026-01-17 13:38:29,210: t15.2023.12.03 val PER: 0.0714
2026-01-17 13:38:29,210: t15.2023.12.08 val PER: 0.0533
2026-01-17 13:38:29,211: t15.2023.12.10 val PER: 0.0473
2026-01-17 13:38:29,211: t15.2023.12.17 val PER: 0.0936
2026-01-17 13:38:29,211: t15.2023.12.29 val PER: 0.0927
2026-01-17 13:38:29,211: t15.2024.02.25 val PER: 0.0871
2026-01-17 13:38:29,211: t15.2024.03.08 val PER: 0.1991
2026-01-17 13:38:29,211: t15.2024.03.15 val PER: 0.1632
2026-01-17 13:38:29,211: t15.2024.03.17 val PER: 0.0934
2026-01-17 13:38:29,211: t15.2024.05.10 val PER: 0.1501
2026-01-17 13:38:29,211: t15.2024.06.14 val PER: 0.1483
2026-01-17 13:38:29,211: t15.2024.07.19 val PER: 0.1806
2026-01-17 13:38:29,211: t15.2024.07.21 val PER: 0.0634
2026-01-17 13:38:29,211: t15.2024.07.28 val PER: 0.1088
2026-01-17 13:38:29,211: t15.2025.01.10 val PER: 0.2631
2026-01-17 13:38:29,211: t15.2025.01.12 val PER: 0.1316
2026-01-17 13:38:29,211: t15.2025.03.14 val PER: 0.3166
2026-01-17 13:38:29,211: t15.2025.03.16 val PER: 0.1414
2026-01-17 13:38:29,211: t15.2025.03.30 val PER: 0.2230
2026-01-17 13:38:29,212: t15.2025.04.13 val PER: 0.1883
2026-01-17 13:38:29,213: New best val WER(1gram) 44.16% --> 43.91%
2026-01-17 13:38:29,213: Checkpointing model
2026-01-17 13:38:29,407: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/best_checkpoint
2026-01-17 13:38:50,031: Train batch 25200: loss: 1.37 grad norm: 26.78 time: 0.057
2026-01-17 13:39:09,966: Train batch 25400: loss: 1.99 grad norm: 47.52 time: 0.056
2026-01-17 13:39:19,727: Running test after training batch: 25500
2026-01-17 13:39:19,841: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:39:25,802: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:39:25,838: WER debug example
  GT : how does it keep the cost down
  PR : hower dusts it keep the cost sids
2026-01-17 13:39:27,775: Val batch 25500: PER (avg): 0.1181 CTC Loss (avg): 16.0734 WER(1gram): 45.69% (n=64) time: 8.048
2026-01-17 13:39:27,776: WER lens: avg_true_words=6.16 avg_pred_words=6.34 max_pred_words=12
2026-01-17 13:39:27,776: t15.2023.08.13 val PER: 0.0904
2026-01-17 13:39:27,776: t15.2023.08.18 val PER: 0.0805
2026-01-17 13:39:27,776: t15.2023.08.20 val PER: 0.0715
2026-01-17 13:39:27,776: t15.2023.08.25 val PER: 0.0813
2026-01-17 13:39:27,777: t15.2023.08.27 val PER: 0.1447
2026-01-17 13:39:27,777: t15.2023.09.01 val PER: 0.0609
2026-01-17 13:39:27,777: t15.2023.09.03 val PER: 0.1283
2026-01-17 13:39:27,777: t15.2023.09.24 val PER: 0.0922
2026-01-17 13:39:27,777: t15.2023.09.29 val PER: 0.0938
2026-01-17 13:39:27,777: t15.2023.10.01 val PER: 0.1572
2026-01-17 13:39:27,777: t15.2023.10.06 val PER: 0.0721
2026-01-17 13:39:27,777: t15.2023.10.08 val PER: 0.2273
2026-01-17 13:39:27,778: t15.2023.10.13 val PER: 0.1652
2026-01-17 13:39:27,778: t15.2023.10.15 val PER: 0.1206
2026-01-17 13:39:27,778: t15.2023.10.20 val PER: 0.1577
2026-01-17 13:39:27,778: t15.2023.10.22 val PER: 0.0880
2026-01-17 13:39:27,778: t15.2023.11.03 val PER: 0.1581
2026-01-17 13:39:27,778: t15.2023.11.04 val PER: 0.0273
2026-01-17 13:39:27,778: t15.2023.11.17 val PER: 0.0218
2026-01-17 13:39:27,778: t15.2023.11.19 val PER: 0.0279
2026-01-17 13:39:27,778: t15.2023.11.26 val PER: 0.0638
2026-01-17 13:39:27,778: t15.2023.12.03 val PER: 0.0746
2026-01-17 13:39:27,779: t15.2023.12.08 val PER: 0.0566
2026-01-17 13:39:27,779: t15.2023.12.10 val PER: 0.0539
2026-01-17 13:39:27,779: t15.2023.12.17 val PER: 0.1071
2026-01-17 13:39:27,779: t15.2023.12.29 val PER: 0.1016
2026-01-17 13:39:27,779: t15.2024.02.25 val PER: 0.0829
2026-01-17 13:39:27,779: t15.2024.03.08 val PER: 0.1835
2026-01-17 13:39:27,779: t15.2024.03.15 val PER: 0.1645
2026-01-17 13:39:27,779: t15.2024.03.17 val PER: 0.0934
2026-01-17 13:39:27,779: t15.2024.05.10 val PER: 0.1248
2026-01-17 13:39:27,779: t15.2024.06.14 val PER: 0.1341
2026-01-17 13:39:27,780: t15.2024.07.19 val PER: 0.1905
2026-01-17 13:39:27,780: t15.2024.07.21 val PER: 0.0697
2026-01-17 13:39:27,780: t15.2024.07.28 val PER: 0.1051
2026-01-17 13:39:27,780: t15.2025.01.10 val PER: 0.2534
2026-01-17 13:39:27,780: t15.2025.01.12 val PER: 0.1162
2026-01-17 13:39:27,780: t15.2025.03.14 val PER: 0.2959
2026-01-17 13:39:27,780: t15.2025.03.16 val PER: 0.1479
2026-01-17 13:39:27,780: t15.2025.03.30 val PER: 0.2299
2026-01-17 13:39:27,780: t15.2025.04.13 val PER: 0.1883
2026-01-17 13:39:39,102: Train batch 25600: loss: 3.15 grad norm: 44.33 time: 0.058
2026-01-17 13:39:59,727: Train batch 25800: loss: 1.28 grad norm: 27.96 time: 0.061
2026-01-17 13:40:20,247: Train batch 26000: loss: 1.80 grad norm: 33.26 time: 0.062
2026-01-17 13:40:20,248: Running test after training batch: 26000
2026-01-17 13:40:20,547: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:40:26,317: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:40:26,350: WER debug example
  GT : how does it keep the cost down
  PR : hower dust it keep the cost sids
2026-01-17 13:40:28,240: Val batch 26000: PER (avg): 0.1159 CTC Loss (avg): 15.8616 WER(1gram): 48.22% (n=64) time: 7.992
2026-01-17 13:40:28,240: WER lens: avg_true_words=6.16 avg_pred_words=6.47 max_pred_words=12
2026-01-17 13:40:28,241: t15.2023.08.13 val PER: 0.0894
2026-01-17 13:40:28,241: t15.2023.08.18 val PER: 0.0754
2026-01-17 13:40:28,241: t15.2023.08.20 val PER: 0.0675
2026-01-17 13:40:28,241: t15.2023.08.25 val PER: 0.0813
2026-01-17 13:40:28,241: t15.2023.08.27 val PER: 0.1399
2026-01-17 13:40:28,241: t15.2023.09.01 val PER: 0.0601
2026-01-17 13:40:28,241: t15.2023.09.03 val PER: 0.1295
2026-01-17 13:40:28,241: t15.2023.09.24 val PER: 0.0959
2026-01-17 13:40:28,241: t15.2023.09.29 val PER: 0.1008
2026-01-17 13:40:28,241: t15.2023.10.01 val PER: 0.1539
2026-01-17 13:40:28,241: t15.2023.10.06 val PER: 0.0667
2026-01-17 13:40:28,241: t15.2023.10.08 val PER: 0.2111
2026-01-17 13:40:28,241: t15.2023.10.13 val PER: 0.1544
2026-01-17 13:40:28,242: t15.2023.10.15 val PER: 0.1107
2026-01-17 13:40:28,242: t15.2023.10.20 val PER: 0.1745
2026-01-17 13:40:28,242: t15.2023.10.22 val PER: 0.0857
2026-01-17 13:40:28,242: t15.2023.11.03 val PER: 0.1404
2026-01-17 13:40:28,242: t15.2023.11.04 val PER: 0.0307
2026-01-17 13:40:28,242: t15.2023.11.17 val PER: 0.0249
2026-01-17 13:40:28,242: t15.2023.11.19 val PER: 0.0200
2026-01-17 13:40:28,242: t15.2023.11.26 val PER: 0.0667
2026-01-17 13:40:28,242: t15.2023.12.03 val PER: 0.0662
2026-01-17 13:40:28,242: t15.2023.12.08 val PER: 0.0533
2026-01-17 13:40:28,242: t15.2023.12.10 val PER: 0.0565
2026-01-17 13:40:28,242: t15.2023.12.17 val PER: 0.0936
2026-01-17 13:40:28,243: t15.2023.12.29 val PER: 0.0988
2026-01-17 13:40:28,243: t15.2024.02.25 val PER: 0.0899
2026-01-17 13:40:28,243: t15.2024.03.08 val PER: 0.1963
2026-01-17 13:40:28,243: t15.2024.03.15 val PER: 0.1607
2026-01-17 13:40:28,243: t15.2024.03.17 val PER: 0.0851
2026-01-17 13:40:28,243: t15.2024.05.10 val PER: 0.1293
2026-01-17 13:40:28,243: t15.2024.06.14 val PER: 0.1309
2026-01-17 13:40:28,243: t15.2024.07.19 val PER: 0.1918
2026-01-17 13:40:28,243: t15.2024.07.21 val PER: 0.0669
2026-01-17 13:40:28,244: t15.2024.07.28 val PER: 0.0963
2026-01-17 13:40:28,244: t15.2025.01.10 val PER: 0.2617
2026-01-17 13:40:28,244: t15.2025.01.12 val PER: 0.1170
2026-01-17 13:40:28,244: t15.2025.03.14 val PER: 0.3077
2026-01-17 13:40:28,244: t15.2025.03.16 val PER: 0.1610
2026-01-17 13:40:28,244: t15.2025.03.30 val PER: 0.2207
2026-01-17 13:40:28,244: t15.2025.04.13 val PER: 0.1997
2026-01-17 13:40:49,873: Train batch 26200: loss: 0.56 grad norm: 17.86 time: 0.063
2026-01-17 13:41:11,037: Train batch 26400: loss: 2.13 grad norm: 31.48 time: 0.078
2026-01-17 13:41:22,182: Running test after training batch: 26500
2026-01-17 13:41:22,300: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:41:27,673: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:41:27,709: WER debug example
  GT : how does it keep the cost down
  PR : hower dusts it keep the cost sids
2026-01-17 13:41:29,604: Val batch 26500: PER (avg): 0.1155 CTC Loss (avg): 15.6191 WER(1gram): 46.70% (n=64) time: 7.422
2026-01-17 13:41:29,604: WER lens: avg_true_words=6.16 avg_pred_words=6.34 max_pred_words=12
2026-01-17 13:41:29,605: t15.2023.08.13 val PER: 0.0884
2026-01-17 13:41:29,605: t15.2023.08.18 val PER: 0.0813
2026-01-17 13:41:29,605: t15.2023.08.20 val PER: 0.0683
2026-01-17 13:41:29,605: t15.2023.08.25 val PER: 0.0783
2026-01-17 13:41:29,605: t15.2023.08.27 val PER: 0.1495
2026-01-17 13:41:29,605: t15.2023.09.01 val PER: 0.0576
2026-01-17 13:41:29,605: t15.2023.09.03 val PER: 0.1318
2026-01-17 13:41:29,605: t15.2023.09.24 val PER: 0.0959
2026-01-17 13:41:29,605: t15.2023.09.29 val PER: 0.0964
2026-01-17 13:41:29,605: t15.2023.10.01 val PER: 0.1446
2026-01-17 13:41:29,605: t15.2023.10.06 val PER: 0.0635
2026-01-17 13:41:29,605: t15.2023.10.08 val PER: 0.2124
2026-01-17 13:41:29,606: t15.2023.10.13 val PER: 0.1590
2026-01-17 13:41:29,606: t15.2023.10.15 val PER: 0.1127
2026-01-17 13:41:29,606: t15.2023.10.20 val PER: 0.1812
2026-01-17 13:41:29,606: t15.2023.10.22 val PER: 0.0935
2026-01-17 13:41:29,606: t15.2023.11.03 val PER: 0.1526
2026-01-17 13:41:29,606: t15.2023.11.04 val PER: 0.0239
2026-01-17 13:41:29,606: t15.2023.11.17 val PER: 0.0233
2026-01-17 13:41:29,606: t15.2023.11.19 val PER: 0.0200
2026-01-17 13:41:29,606: t15.2023.11.26 val PER: 0.0674
2026-01-17 13:41:29,606: t15.2023.12.03 val PER: 0.0641
2026-01-17 13:41:29,606: t15.2023.12.08 val PER: 0.0493
2026-01-17 13:41:29,606: t15.2023.12.10 val PER: 0.0604
2026-01-17 13:41:29,606: t15.2023.12.17 val PER: 0.0977
2026-01-17 13:41:29,607: t15.2023.12.29 val PER: 0.0906
2026-01-17 13:41:29,607: t15.2024.02.25 val PER: 0.0815
2026-01-17 13:41:29,607: t15.2024.03.08 val PER: 0.2148
2026-01-17 13:41:29,607: t15.2024.03.15 val PER: 0.1670
2026-01-17 13:41:29,607: t15.2024.03.17 val PER: 0.0858
2026-01-17 13:41:29,607: t15.2024.05.10 val PER: 0.1278
2026-01-17 13:41:29,607: t15.2024.06.14 val PER: 0.1341
2026-01-17 13:41:29,607: t15.2024.07.19 val PER: 0.1846
2026-01-17 13:41:29,607: t15.2024.07.21 val PER: 0.0634
2026-01-17 13:41:29,607: t15.2024.07.28 val PER: 0.0963
2026-01-17 13:41:29,607: t15.2025.01.10 val PER: 0.2631
2026-01-17 13:41:29,607: t15.2025.01.12 val PER: 0.1162
2026-01-17 13:41:29,607: t15.2025.03.14 val PER: 0.3077
2026-01-17 13:41:29,607: t15.2025.03.16 val PER: 0.1440
2026-01-17 13:41:29,607: t15.2025.03.30 val PER: 0.2276
2026-01-17 13:41:29,607: t15.2025.04.13 val PER: 0.1698
2026-01-17 13:41:40,499: Train batch 26600: loss: 1.14 grad norm: 25.04 time: 0.058
2026-01-17 13:42:01,605: Train batch 26800: loss: 3.17 grad norm: 40.25 time: 0.079
2026-01-17 13:42:23,557: Train batch 27000: loss: 2.09 grad norm: 34.35 time: 0.064
2026-01-17 13:42:23,559: Running test after training batch: 27000
2026-01-17 13:42:23,668: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:42:29,083: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:42:29,118: WER debug example
  GT : how does it keep the cost down
  PR : hower dust it keep the cost sids
2026-01-17 13:42:31,072: Val batch 27000: PER (avg): 0.1170 CTC Loss (avg): 16.1191 WER(1gram): 47.97% (n=64) time: 7.512
2026-01-17 13:42:31,072: WER lens: avg_true_words=6.16 avg_pred_words=6.42 max_pred_words=12
2026-01-17 13:42:31,072: t15.2023.08.13 val PER: 0.0894
2026-01-17 13:42:31,072: t15.2023.08.18 val PER: 0.0687
2026-01-17 13:42:31,073: t15.2023.08.20 val PER: 0.0747
2026-01-17 13:42:31,073: t15.2023.08.25 val PER: 0.0678
2026-01-17 13:42:31,073: t15.2023.08.27 val PER: 0.1592
2026-01-17 13:42:31,073: t15.2023.09.01 val PER: 0.0576
2026-01-17 13:42:31,073: t15.2023.09.03 val PER: 0.1437
2026-01-17 13:42:31,073: t15.2023.09.24 val PER: 0.0959
2026-01-17 13:42:31,073: t15.2023.09.29 val PER: 0.0976
2026-01-17 13:42:31,073: t15.2023.10.01 val PER: 0.1466
2026-01-17 13:42:31,073: t15.2023.10.06 val PER: 0.0635
2026-01-17 13:42:31,073: t15.2023.10.08 val PER: 0.2111
2026-01-17 13:42:31,073: t15.2023.10.13 val PER: 0.1707
2026-01-17 13:42:31,073: t15.2023.10.15 val PER: 0.1173
2026-01-17 13:42:31,073: t15.2023.10.20 val PER: 0.1678
2026-01-17 13:42:31,073: t15.2023.10.22 val PER: 0.0891
2026-01-17 13:42:31,074: t15.2023.11.03 val PER: 0.1581
2026-01-17 13:42:31,074: t15.2023.11.04 val PER: 0.0375
2026-01-17 13:42:31,074: t15.2023.11.17 val PER: 0.0249
2026-01-17 13:42:31,074: t15.2023.11.19 val PER: 0.0180
2026-01-17 13:42:31,074: t15.2023.11.26 val PER: 0.0616
2026-01-17 13:42:31,074: t15.2023.12.03 val PER: 0.0641
2026-01-17 13:42:31,074: t15.2023.12.08 val PER: 0.0539
2026-01-17 13:42:31,074: t15.2023.12.10 val PER: 0.0539
2026-01-17 13:42:31,074: t15.2023.12.17 val PER: 0.1081
2026-01-17 13:42:31,074: t15.2023.12.29 val PER: 0.0927
2026-01-17 13:42:31,074: t15.2024.02.25 val PER: 0.0829
2026-01-17 13:42:31,074: t15.2024.03.08 val PER: 0.2119
2026-01-17 13:42:31,074: t15.2024.03.15 val PER: 0.1607
2026-01-17 13:42:31,074: t15.2024.03.17 val PER: 0.0886
2026-01-17 13:42:31,074: t15.2024.05.10 val PER: 0.1308
2026-01-17 13:42:31,074: t15.2024.06.14 val PER: 0.1246
2026-01-17 13:42:31,075: t15.2024.07.19 val PER: 0.1747
2026-01-17 13:42:31,075: t15.2024.07.21 val PER: 0.0614
2026-01-17 13:42:31,075: t15.2024.07.28 val PER: 0.0993
2026-01-17 13:42:31,075: t15.2025.01.10 val PER: 0.2631
2026-01-17 13:42:31,075: t15.2025.01.12 val PER: 0.1247
2026-01-17 13:42:31,075: t15.2025.03.14 val PER: 0.3151
2026-01-17 13:42:31,075: t15.2025.03.16 val PER: 0.1466
2026-01-17 13:42:31,075: t15.2025.03.30 val PER: 0.2379
2026-01-17 13:42:31,075: t15.2025.04.13 val PER: 0.1954
2026-01-17 13:42:53,161: Train batch 27200: loss: 1.81 grad norm: 27.77 time: 0.055
2026-01-17 13:43:14,566: Train batch 27400: loss: 2.65 grad norm: 42.40 time: 0.055
2026-01-17 13:43:25,486: Running test after training batch: 27500
2026-01-17 13:43:25,595: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:43:30,977: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:43:31,011: WER debug example
  GT : how does it keep the cost down
  PR : hower dust it keep the cussed sent
2026-01-17 13:43:32,954: Val batch 27500: PER (avg): 0.1188 CTC Loss (avg): 16.4387 WER(1gram): 47.72% (n=64) time: 7.468
2026-01-17 13:43:32,955: WER lens: avg_true_words=6.16 avg_pred_words=6.44 max_pred_words=12
2026-01-17 13:43:32,955: t15.2023.08.13 val PER: 0.0894
2026-01-17 13:43:32,955: t15.2023.08.18 val PER: 0.0805
2026-01-17 13:43:32,955: t15.2023.08.20 val PER: 0.0699
2026-01-17 13:43:32,955: t15.2023.08.25 val PER: 0.0813
2026-01-17 13:43:32,955: t15.2023.08.27 val PER: 0.1656
2026-01-17 13:43:32,956: t15.2023.09.01 val PER: 0.0511
2026-01-17 13:43:32,956: t15.2023.09.03 val PER: 0.1473
2026-01-17 13:43:32,956: t15.2023.09.24 val PER: 0.1032
2026-01-17 13:43:32,956: t15.2023.09.29 val PER: 0.1021
2026-01-17 13:43:32,956: t15.2023.10.01 val PER: 0.1565
2026-01-17 13:43:32,956: t15.2023.10.06 val PER: 0.0743
2026-01-17 13:43:32,956: t15.2023.10.08 val PER: 0.2165
2026-01-17 13:43:32,956: t15.2023.10.13 val PER: 0.1583
2026-01-17 13:43:32,956: t15.2023.10.15 val PER: 0.1239
2026-01-17 13:43:32,956: t15.2023.10.20 val PER: 0.1913
2026-01-17 13:43:32,956: t15.2023.10.22 val PER: 0.0969
2026-01-17 13:43:32,956: t15.2023.11.03 val PER: 0.1526
2026-01-17 13:43:32,956: t15.2023.11.04 val PER: 0.0341
2026-01-17 13:43:32,956: t15.2023.11.17 val PER: 0.0218
2026-01-17 13:43:32,957: t15.2023.11.19 val PER: 0.0160
2026-01-17 13:43:32,957: t15.2023.11.26 val PER: 0.0565
2026-01-17 13:43:32,957: t15.2023.12.03 val PER: 0.0609
2026-01-17 13:43:32,957: t15.2023.12.08 val PER: 0.0553
2026-01-17 13:43:32,957: t15.2023.12.10 val PER: 0.0552
2026-01-17 13:43:32,957: t15.2023.12.17 val PER: 0.1091
2026-01-17 13:43:32,957: t15.2023.12.29 val PER: 0.0865
2026-01-17 13:43:32,957: t15.2024.02.25 val PER: 0.0899
2026-01-17 13:43:32,957: t15.2024.03.08 val PER: 0.1835
2026-01-17 13:43:32,957: t15.2024.03.15 val PER: 0.1626
2026-01-17 13:43:32,957: t15.2024.03.17 val PER: 0.1032
2026-01-17 13:43:32,957: t15.2024.05.10 val PER: 0.1293
2026-01-17 13:43:32,957: t15.2024.06.14 val PER: 0.1341
2026-01-17 13:43:32,957: t15.2024.07.19 val PER: 0.1866
2026-01-17 13:43:32,957: t15.2024.07.21 val PER: 0.0655
2026-01-17 13:43:32,957: t15.2024.07.28 val PER: 0.1029
2026-01-17 13:43:32,957: t15.2025.01.10 val PER: 0.2590
2026-01-17 13:43:32,957: t15.2025.01.12 val PER: 0.1186
2026-01-17 13:43:32,958: t15.2025.03.14 val PER: 0.3269
2026-01-17 13:43:32,958: t15.2025.03.16 val PER: 0.1387
2026-01-17 13:43:32,958: t15.2025.03.30 val PER: 0.2368
2026-01-17 13:43:32,958: t15.2025.04.13 val PER: 0.2011
2026-01-17 13:43:43,814: Train batch 27600: loss: 4.36 grad norm: 28.59 time: 0.062
2026-01-17 13:44:05,995: Train batch 27800: loss: 2.60 grad norm: 70.85 time: 0.042
2026-01-17 13:44:28,500: Train batch 28000: loss: 0.87 grad norm: 20.36 time: 0.054
2026-01-17 13:44:28,501: Running test after training batch: 28000
2026-01-17 13:44:28,606: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:44:33,988: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:44:34,024: WER debug example
  GT : how does it keep the cost down
  PR : hower dust it keep the cost sids
2026-01-17 13:44:35,996: Val batch 28000: PER (avg): 0.1159 CTC Loss (avg): 16.1225 WER(1gram): 46.95% (n=64) time: 7.495
2026-01-17 13:44:35,997: WER lens: avg_true_words=6.16 avg_pred_words=6.39 max_pred_words=12
2026-01-17 13:44:35,997: t15.2023.08.13 val PER: 0.0863
2026-01-17 13:44:35,997: t15.2023.08.18 val PER: 0.0721
2026-01-17 13:44:35,997: t15.2023.08.20 val PER: 0.0651
2026-01-17 13:44:35,997: t15.2023.08.25 val PER: 0.0723
2026-01-17 13:44:35,997: t15.2023.08.27 val PER: 0.1479
2026-01-17 13:44:35,997: t15.2023.09.01 val PER: 0.0544
2026-01-17 13:44:35,997: t15.2023.09.03 val PER: 0.1295
2026-01-17 13:44:35,997: t15.2023.09.24 val PER: 0.1019
2026-01-17 13:44:35,998: t15.2023.09.29 val PER: 0.1053
2026-01-17 13:44:35,998: t15.2023.10.01 val PER: 0.1526
2026-01-17 13:44:35,998: t15.2023.10.06 val PER: 0.0581
2026-01-17 13:44:35,998: t15.2023.10.08 val PER: 0.2043
2026-01-17 13:44:35,998: t15.2023.10.13 val PER: 0.1707
2026-01-17 13:44:35,998: t15.2023.10.15 val PER: 0.1107
2026-01-17 13:44:35,998: t15.2023.10.20 val PER: 0.1678
2026-01-17 13:44:35,998: t15.2023.10.22 val PER: 0.0958
2026-01-17 13:44:35,998: t15.2023.11.03 val PER: 0.1513
2026-01-17 13:44:35,998: t15.2023.11.04 val PER: 0.0239
2026-01-17 13:44:35,998: t15.2023.11.17 val PER: 0.0311
2026-01-17 13:44:35,998: t15.2023.11.19 val PER: 0.0220
2026-01-17 13:44:35,998: t15.2023.11.26 val PER: 0.0623
2026-01-17 13:44:35,998: t15.2023.12.03 val PER: 0.0693
2026-01-17 13:44:35,999: t15.2023.12.08 val PER: 0.0513
2026-01-17 13:44:35,999: t15.2023.12.10 val PER: 0.0512
2026-01-17 13:44:35,999: t15.2023.12.17 val PER: 0.0915
2026-01-17 13:44:35,999: t15.2023.12.29 val PER: 0.0879
2026-01-17 13:44:35,999: t15.2024.02.25 val PER: 0.0815
2026-01-17 13:44:35,999: t15.2024.03.08 val PER: 0.1963
2026-01-17 13:44:35,999: t15.2024.03.15 val PER: 0.1601
2026-01-17 13:44:35,999: t15.2024.03.17 val PER: 0.0872
2026-01-17 13:44:35,999: t15.2024.05.10 val PER: 0.1352
2026-01-17 13:44:35,999: t15.2024.06.14 val PER: 0.1230
2026-01-17 13:44:35,999: t15.2024.07.19 val PER: 0.1945
2026-01-17 13:44:35,999: t15.2024.07.21 val PER: 0.0662
2026-01-17 13:44:35,999: t15.2024.07.28 val PER: 0.0985
2026-01-17 13:44:35,999: t15.2025.01.10 val PER: 0.2672
2026-01-17 13:44:35,999: t15.2025.01.12 val PER: 0.1247
2026-01-17 13:44:35,999: t15.2025.03.14 val PER: 0.3225
2026-01-17 13:44:36,000: t15.2025.03.16 val PER: 0.1466
2026-01-17 13:44:36,000: t15.2025.03.30 val PER: 0.2241
2026-01-17 13:44:36,000: t15.2025.04.13 val PER: 0.1783
2026-01-17 13:44:58,856: Train batch 28200: loss: 1.07 grad norm: 21.00 time: 0.062
2026-01-17 13:45:22,693: Train batch 28400: loss: 1.58 grad norm: 28.75 time: 0.060
2026-01-17 13:45:33,514: Running test after training batch: 28500
2026-01-17 13:45:33,685: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:45:40,023: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as well
2026-01-17 13:45:40,059: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cents
2026-01-17 13:45:42,010: Val batch 28500: PER (avg): 0.1168 CTC Loss (avg): 16.4664 WER(1gram): 44.67% (n=64) time: 8.495
2026-01-17 13:45:42,010: WER lens: avg_true_words=6.16 avg_pred_words=6.41 max_pred_words=12
2026-01-17 13:45:42,010: t15.2023.08.13 val PER: 0.0915
2026-01-17 13:45:42,011: t15.2023.08.18 val PER: 0.0830
2026-01-17 13:45:42,011: t15.2023.08.20 val PER: 0.0794
2026-01-17 13:45:42,011: t15.2023.08.25 val PER: 0.0753
2026-01-17 13:45:42,011: t15.2023.08.27 val PER: 0.1559
2026-01-17 13:45:42,011: t15.2023.09.01 val PER: 0.0560
2026-01-17 13:45:42,011: t15.2023.09.03 val PER: 0.1366
2026-01-17 13:45:42,011: t15.2023.09.24 val PER: 0.0983
2026-01-17 13:45:42,011: t15.2023.09.29 val PER: 0.0944
2026-01-17 13:45:42,012: t15.2023.10.01 val PER: 0.1453
2026-01-17 13:45:42,012: t15.2023.10.06 val PER: 0.0689
2026-01-17 13:45:42,012: t15.2023.10.08 val PER: 0.2030
2026-01-17 13:45:42,012: t15.2023.10.13 val PER: 0.1544
2026-01-17 13:45:42,012: t15.2023.10.15 val PER: 0.1233
2026-01-17 13:45:42,012: t15.2023.10.20 val PER: 0.1644
2026-01-17 13:45:42,012: t15.2023.10.22 val PER: 0.0857
2026-01-17 13:45:42,012: t15.2023.11.03 val PER: 0.1526
2026-01-17 13:45:42,012: t15.2023.11.04 val PER: 0.0273
2026-01-17 13:45:42,013: t15.2023.11.17 val PER: 0.0358
2026-01-17 13:45:42,013: t15.2023.11.19 val PER: 0.0259
2026-01-17 13:45:42,013: t15.2023.11.26 val PER: 0.0609
2026-01-17 13:45:42,013: t15.2023.12.03 val PER: 0.0735
2026-01-17 13:45:42,013: t15.2023.12.08 val PER: 0.0626
2026-01-17 13:45:42,013: t15.2023.12.10 val PER: 0.0539
2026-01-17 13:45:42,013: t15.2023.12.17 val PER: 0.1029
2026-01-17 13:45:42,013: t15.2023.12.29 val PER: 0.0851
2026-01-17 13:45:42,013: t15.2024.02.25 val PER: 0.0815
2026-01-17 13:45:42,014: t15.2024.03.08 val PER: 0.1835
2026-01-17 13:45:42,014: t15.2024.03.15 val PER: 0.1632
2026-01-17 13:45:42,014: t15.2024.03.17 val PER: 0.0851
2026-01-17 13:45:42,014: t15.2024.05.10 val PER: 0.1352
2026-01-17 13:45:42,014: t15.2024.06.14 val PER: 0.1372
2026-01-17 13:45:42,014: t15.2024.07.19 val PER: 0.1912
2026-01-17 13:45:42,014: t15.2024.07.21 val PER: 0.0600
2026-01-17 13:45:42,014: t15.2024.07.28 val PER: 0.1007
2026-01-17 13:45:42,014: t15.2025.01.10 val PER: 0.2562
2026-01-17 13:45:42,014: t15.2025.01.12 val PER: 0.1263
2026-01-17 13:45:42,015: t15.2025.03.14 val PER: 0.3225
2026-01-17 13:45:42,015: t15.2025.03.16 val PER: 0.1414
2026-01-17 13:45:42,015: t15.2025.03.30 val PER: 0.2184
2026-01-17 13:45:42,015: t15.2025.04.13 val PER: 0.1869
2026-01-17 13:45:54,113: Train batch 28600: loss: 1.83 grad norm: 32.16 time: 0.060
2026-01-17 13:46:16,872: Train batch 28800: loss: 1.56 grad norm: 37.74 time: 0.047
2026-01-17 13:46:41,311: Train batch 29000: loss: 1.36 grad norm: 30.43 time: 0.050
2026-01-17 13:46:41,312: Running test after training batch: 29000
2026-01-17 13:46:41,467: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:46:46,944: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:46:46,980: WER debug example
  GT : how does it keep the cost down
  PR : hower dust it keep the cost sent
2026-01-17 13:46:48,976: Val batch 29000: PER (avg): 0.1147 CTC Loss (avg): 16.1487 WER(1gram): 46.95% (n=64) time: 7.664
2026-01-17 13:46:48,977: WER lens: avg_true_words=6.16 avg_pred_words=6.41 max_pred_words=12
2026-01-17 13:46:48,977: t15.2023.08.13 val PER: 0.0852
2026-01-17 13:46:48,977: t15.2023.08.18 val PER: 0.0763
2026-01-17 13:46:48,977: t15.2023.08.20 val PER: 0.0699
2026-01-17 13:46:48,977: t15.2023.08.25 val PER: 0.0723
2026-01-17 13:46:48,978: t15.2023.08.27 val PER: 0.1447
2026-01-17 13:46:48,978: t15.2023.09.01 val PER: 0.0503
2026-01-17 13:46:48,978: t15.2023.09.03 val PER: 0.1425
2026-01-17 13:46:48,978: t15.2023.09.24 val PER: 0.1007
2026-01-17 13:46:48,978: t15.2023.09.29 val PER: 0.1015
2026-01-17 13:46:48,978: t15.2023.10.01 val PER: 0.1486
2026-01-17 13:46:48,978: t15.2023.10.06 val PER: 0.0732
2026-01-17 13:46:48,978: t15.2023.10.08 val PER: 0.2084
2026-01-17 13:46:48,978: t15.2023.10.13 val PER: 0.1683
2026-01-17 13:46:48,978: t15.2023.10.15 val PER: 0.1147
2026-01-17 13:46:48,978: t15.2023.10.20 val PER: 0.1711
2026-01-17 13:46:48,978: t15.2023.10.22 val PER: 0.0902
2026-01-17 13:46:48,978: t15.2023.11.03 val PER: 0.1486
2026-01-17 13:46:48,978: t15.2023.11.04 val PER: 0.0273
2026-01-17 13:46:48,979: t15.2023.11.17 val PER: 0.0358
2026-01-17 13:46:48,979: t15.2023.11.19 val PER: 0.0160
2026-01-17 13:46:48,979: t15.2023.11.26 val PER: 0.0659
2026-01-17 13:46:48,979: t15.2023.12.03 val PER: 0.0662
2026-01-17 13:46:48,979: t15.2023.12.08 val PER: 0.0466
2026-01-17 13:46:48,979: t15.2023.12.10 val PER: 0.0565
2026-01-17 13:46:48,979: t15.2023.12.17 val PER: 0.1071
2026-01-17 13:46:48,979: t15.2023.12.29 val PER: 0.0906
2026-01-17 13:46:48,979: t15.2024.02.25 val PER: 0.0843
2026-01-17 13:46:48,979: t15.2024.03.08 val PER: 0.1906
2026-01-17 13:46:48,979: t15.2024.03.15 val PER: 0.1645
2026-01-17 13:46:48,979: t15.2024.03.17 val PER: 0.0893
2026-01-17 13:46:48,979: t15.2024.05.10 val PER: 0.1263
2026-01-17 13:46:48,979: t15.2024.06.14 val PER: 0.1325
2026-01-17 13:46:48,980: t15.2024.07.19 val PER: 0.1655
2026-01-17 13:46:48,980: t15.2024.07.21 val PER: 0.0545
2026-01-17 13:46:48,980: t15.2024.07.28 val PER: 0.0934
2026-01-17 13:46:48,980: t15.2025.01.10 val PER: 0.2603
2026-01-17 13:46:48,980: t15.2025.01.12 val PER: 0.1193
2026-01-17 13:46:48,980: t15.2025.03.14 val PER: 0.3136
2026-01-17 13:46:48,980: t15.2025.03.16 val PER: 0.1335
2026-01-17 13:46:48,980: t15.2025.03.30 val PER: 0.2195
2026-01-17 13:46:48,980: t15.2025.04.13 val PER: 0.1926
2026-01-17 13:47:13,295: Train batch 29200: loss: 1.72 grad norm: 34.36 time: 0.067
2026-01-17 13:47:37,203: Train batch 29400: loss: 1.05 grad norm: 27.33 time: 0.060
2026-01-17 13:47:49,301: Running test after training batch: 29500
2026-01-17 13:47:49,408: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:47:55,042: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:47:55,079: WER debug example
  GT : how does it keep the cost down
  PR : hower dust it keep the cost sit
2026-01-17 13:47:57,008: Val batch 29500: PER (avg): 0.1143 CTC Loss (avg): 16.0384 WER(1gram): 45.94% (n=64) time: 7.707
2026-01-17 13:47:57,009: WER lens: avg_true_words=6.16 avg_pred_words=6.39 max_pred_words=12
2026-01-17 13:47:57,009: t15.2023.08.13 val PER: 0.0832
2026-01-17 13:47:57,009: t15.2023.08.18 val PER: 0.0805
2026-01-17 13:47:57,009: t15.2023.08.20 val PER: 0.0683
2026-01-17 13:47:57,009: t15.2023.08.25 val PER: 0.0738
2026-01-17 13:47:57,010: t15.2023.08.27 val PER: 0.1559
2026-01-17 13:47:57,010: t15.2023.09.01 val PER: 0.0479
2026-01-17 13:47:57,010: t15.2023.09.03 val PER: 0.1342
2026-01-17 13:47:57,010: t15.2023.09.24 val PER: 0.0959
2026-01-17 13:47:57,010: t15.2023.09.29 val PER: 0.0989
2026-01-17 13:47:57,010: t15.2023.10.01 val PER: 0.1453
2026-01-17 13:47:57,010: t15.2023.10.06 val PER: 0.0667
2026-01-17 13:47:57,010: t15.2023.10.08 val PER: 0.2084
2026-01-17 13:47:57,010: t15.2023.10.13 val PER: 0.1521
2026-01-17 13:47:57,010: t15.2023.10.15 val PER: 0.1140
2026-01-17 13:47:57,010: t15.2023.10.20 val PER: 0.1644
2026-01-17 13:47:57,010: t15.2023.10.22 val PER: 0.0902
2026-01-17 13:47:57,010: t15.2023.11.03 val PER: 0.1526
2026-01-17 13:47:57,011: t15.2023.11.04 val PER: 0.0239
2026-01-17 13:47:57,011: t15.2023.11.17 val PER: 0.0218
2026-01-17 13:47:57,011: t15.2023.11.19 val PER: 0.0220
2026-01-17 13:47:57,011: t15.2023.11.26 val PER: 0.0638
2026-01-17 13:47:57,011: t15.2023.12.03 val PER: 0.0662
2026-01-17 13:47:57,011: t15.2023.12.08 val PER: 0.0459
2026-01-17 13:47:57,011: t15.2023.12.10 val PER: 0.0460
2026-01-17 13:47:57,011: t15.2023.12.17 val PER: 0.1102
2026-01-17 13:47:57,011: t15.2023.12.29 val PER: 0.0940
2026-01-17 13:47:57,011: t15.2024.02.25 val PER: 0.0815
2026-01-17 13:47:57,011: t15.2024.03.08 val PER: 0.1849
2026-01-17 13:47:57,011: t15.2024.03.15 val PER: 0.1682
2026-01-17 13:47:57,011: t15.2024.03.17 val PER: 0.0872
2026-01-17 13:47:57,011: t15.2024.05.10 val PER: 0.1263
2026-01-17 13:47:57,011: t15.2024.06.14 val PER: 0.1309
2026-01-17 13:47:57,011: t15.2024.07.19 val PER: 0.1721
2026-01-17 13:47:57,011: t15.2024.07.21 val PER: 0.0593
2026-01-17 13:47:57,011: t15.2024.07.28 val PER: 0.1118
2026-01-17 13:47:57,012: t15.2025.01.10 val PER: 0.2603
2026-01-17 13:47:57,012: t15.2025.01.12 val PER: 0.1155
2026-01-17 13:47:57,012: t15.2025.03.14 val PER: 0.2944
2026-01-17 13:47:57,012: t15.2025.03.16 val PER: 0.1440
2026-01-17 13:47:57,012: t15.2025.03.30 val PER: 0.2379
2026-01-17 13:47:57,012: t15.2025.04.13 val PER: 0.1740
2026-01-17 13:48:09,296: Train batch 29600: loss: 2.21 grad norm: 73.20 time: 0.050
2026-01-17 13:48:34,203: Train batch 29800: loss: 1.32 grad norm: 32.14 time: 0.079
2026-01-17 13:48:59,573: Train batch 30000: loss: 1.46 grad norm: 32.59 time: 0.066
2026-01-17 13:48:59,573: Running test after training batch: 30000
2026-01-17 13:48:59,680: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:49:05,255: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:49:05,292: WER debug example
  GT : how does it keep the cost down
  PR : hower dusts it keep the cost sitze
2026-01-17 13:49:07,261: Val batch 30000: PER (avg): 0.1134 CTC Loss (avg): 16.0380 WER(1gram): 45.43% (n=64) time: 7.688
2026-01-17 13:49:07,262: WER lens: avg_true_words=6.16 avg_pred_words=6.39 max_pred_words=12
2026-01-17 13:49:07,262: t15.2023.08.13 val PER: 0.0884
2026-01-17 13:49:07,262: t15.2023.08.18 val PER: 0.0746
2026-01-17 13:49:07,262: t15.2023.08.20 val PER: 0.0667
2026-01-17 13:49:07,262: t15.2023.08.25 val PER: 0.0708
2026-01-17 13:49:07,262: t15.2023.08.27 val PER: 0.1559
2026-01-17 13:49:07,263: t15.2023.09.01 val PER: 0.0601
2026-01-17 13:49:07,263: t15.2023.09.03 val PER: 0.1330
2026-01-17 13:49:07,263: t15.2023.09.24 val PER: 0.1019
2026-01-17 13:49:07,263: t15.2023.09.29 val PER: 0.0951
2026-01-17 13:49:07,263: t15.2023.10.01 val PER: 0.1413
2026-01-17 13:49:07,263: t15.2023.10.06 val PER: 0.0624
2026-01-17 13:49:07,263: t15.2023.10.08 val PER: 0.2043
2026-01-17 13:49:07,263: t15.2023.10.13 val PER: 0.1598
2026-01-17 13:49:07,263: t15.2023.10.15 val PER: 0.1173
2026-01-17 13:49:07,264: t15.2023.10.20 val PER: 0.1711
2026-01-17 13:49:07,264: t15.2023.10.22 val PER: 0.0935
2026-01-17 13:49:07,264: t15.2023.11.03 val PER: 0.1499
2026-01-17 13:49:07,264: t15.2023.11.04 val PER: 0.0273
2026-01-17 13:49:07,264: t15.2023.11.17 val PER: 0.0295
2026-01-17 13:49:07,264: t15.2023.11.19 val PER: 0.0200
2026-01-17 13:49:07,264: t15.2023.11.26 val PER: 0.0558
2026-01-17 13:49:07,264: t15.2023.12.03 val PER: 0.0651
2026-01-17 13:49:07,264: t15.2023.12.08 val PER: 0.0479
2026-01-17 13:49:07,265: t15.2023.12.10 val PER: 0.0447
2026-01-17 13:49:07,265: t15.2023.12.17 val PER: 0.1060
2026-01-17 13:49:07,265: t15.2023.12.29 val PER: 0.0851
2026-01-17 13:49:07,265: t15.2024.02.25 val PER: 0.0758
2026-01-17 13:49:07,265: t15.2024.03.08 val PER: 0.1935
2026-01-17 13:49:07,265: t15.2024.03.15 val PER: 0.1557
2026-01-17 13:49:07,265: t15.2024.03.17 val PER: 0.0865
2026-01-17 13:49:07,265: t15.2024.05.10 val PER: 0.1397
2026-01-17 13:49:07,265: t15.2024.06.14 val PER: 0.1278
2026-01-17 13:49:07,266: t15.2024.07.19 val PER: 0.1846
2026-01-17 13:49:07,266: t15.2024.07.21 val PER: 0.0607
2026-01-17 13:49:07,266: t15.2024.07.28 val PER: 0.0934
2026-01-17 13:49:07,266: t15.2025.01.10 val PER: 0.2700
2026-01-17 13:49:07,266: t15.2025.01.12 val PER: 0.1078
2026-01-17 13:49:07,266: t15.2025.03.14 val PER: 0.3166
2026-01-17 13:49:07,266: t15.2025.03.16 val PER: 0.1270
2026-01-17 13:49:07,266: t15.2025.03.30 val PER: 0.2207
2026-01-17 13:49:07,266: t15.2025.04.13 val PER: 0.1826
2026-01-17 13:49:34,689: Train batch 30200: loss: 1.97 grad norm: 32.37 time: 0.069
2026-01-17 13:50:00,317: Train batch 30400: loss: 0.87 grad norm: 23.87 time: 0.060
2026-01-17 13:50:12,764: Running test after training batch: 30500
2026-01-17 13:50:12,878: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:50:18,460: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:50:18,496: WER debug example
  GT : how does it keep the cost down
  PR : hower dust it keep the cost sids
2026-01-17 13:50:20,430: Val batch 30500: PER (avg): 0.1135 CTC Loss (avg): 16.3412 WER(1gram): 45.69% (n=64) time: 7.666
2026-01-17 13:50:20,431: WER lens: avg_true_words=6.16 avg_pred_words=6.41 max_pred_words=12
2026-01-17 13:50:20,431: t15.2023.08.13 val PER: 0.0800
2026-01-17 13:50:20,431: t15.2023.08.18 val PER: 0.0738
2026-01-17 13:50:20,431: t15.2023.08.20 val PER: 0.0763
2026-01-17 13:50:20,431: t15.2023.08.25 val PER: 0.0738
2026-01-17 13:50:20,431: t15.2023.08.27 val PER: 0.1415
2026-01-17 13:50:20,431: t15.2023.09.01 val PER: 0.0495
2026-01-17 13:50:20,432: t15.2023.09.03 val PER: 0.1271
2026-01-17 13:50:20,432: t15.2023.09.24 val PER: 0.0934
2026-01-17 13:50:20,432: t15.2023.09.29 val PER: 0.1034
2026-01-17 13:50:20,432: t15.2023.10.01 val PER: 0.1367
2026-01-17 13:50:20,432: t15.2023.10.06 val PER: 0.0700
2026-01-17 13:50:20,432: t15.2023.10.08 val PER: 0.2003
2026-01-17 13:50:20,432: t15.2023.10.13 val PER: 0.1583
2026-01-17 13:50:20,432: t15.2023.10.15 val PER: 0.1127
2026-01-17 13:50:20,432: t15.2023.10.20 val PER: 0.1611
2026-01-17 13:50:20,432: t15.2023.10.22 val PER: 0.0991
2026-01-17 13:50:20,432: t15.2023.11.03 val PER: 0.1520
2026-01-17 13:50:20,432: t15.2023.11.04 val PER: 0.0239
2026-01-17 13:50:20,432: t15.2023.11.17 val PER: 0.0264
2026-01-17 13:50:20,433: t15.2023.11.19 val PER: 0.0220
2026-01-17 13:50:20,433: t15.2023.11.26 val PER: 0.0667
2026-01-17 13:50:20,433: t15.2023.12.03 val PER: 0.0777
2026-01-17 13:50:20,433: t15.2023.12.08 val PER: 0.0453
2026-01-17 13:50:20,433: t15.2023.12.10 val PER: 0.0473
2026-01-17 13:50:20,433: t15.2023.12.17 val PER: 0.0925
2026-01-17 13:50:20,433: t15.2023.12.29 val PER: 0.0879
2026-01-17 13:50:20,433: t15.2024.02.25 val PER: 0.0829
2026-01-17 13:50:20,433: t15.2024.03.08 val PER: 0.1935
2026-01-17 13:50:20,433: t15.2024.03.15 val PER: 0.1607
2026-01-17 13:50:20,433: t15.2024.03.17 val PER: 0.0830
2026-01-17 13:50:20,433: t15.2024.05.10 val PER: 0.1293
2026-01-17 13:50:20,433: t15.2024.06.14 val PER: 0.1230
2026-01-17 13:50:20,433: t15.2024.07.19 val PER: 0.1747
2026-01-17 13:50:20,433: t15.2024.07.21 val PER: 0.0628
2026-01-17 13:50:20,433: t15.2024.07.28 val PER: 0.1000
2026-01-17 13:50:20,434: t15.2025.01.10 val PER: 0.2576
2026-01-17 13:50:20,434: t15.2025.01.12 val PER: 0.1109
2026-01-17 13:50:20,434: t15.2025.03.14 val PER: 0.3062
2026-01-17 13:50:20,434: t15.2025.03.16 val PER: 0.1479
2026-01-17 13:50:20,434: t15.2025.03.30 val PER: 0.2391
2026-01-17 13:50:20,434: t15.2025.04.13 val PER: 0.1812
2026-01-17 13:50:34,082: Train batch 30600: loss: 2.10 grad norm: 48.30 time: 0.066
2026-01-17 13:51:03,310: Train batch 30800: loss: 1.03 grad norm: 25.09 time: 0.061
2026-01-17 13:51:30,129: Train batch 31000: loss: 1.36 grad norm: 33.18 time: 0.063
2026-01-17 13:51:30,130: Running test after training batch: 31000
2026-01-17 13:51:30,254: WER debug GT example: You can see the code at this point as well.
2026-01-17 13:51:36,162: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-17 13:51:36,198: WER debug example
  GT : how does it keep the cost down
  PR : hower dusts it keep the cost cents
2026-01-17 13:51:38,161: Val batch 31000: PER (avg): 0.1128 CTC Loss (avg): 16.4704 WER(1gram): 46.70% (n=64) time: 8.030
2026-01-17 13:51:38,161: WER lens: avg_true_words=6.16 avg_pred_words=6.42 max_pred_words=12
2026-01-17 13:51:38,161: t15.2023.08.13 val PER: 0.0925
2026-01-17 13:51:38,161: t15.2023.08.18 val PER: 0.0754
2026-01-17 13:51:38,161: t15.2023.08.20 val PER: 0.0739
2026-01-17 13:51:38,162: t15.2023.08.25 val PER: 0.0723
2026-01-17 13:51:38,162: t15.2023.08.27 val PER: 0.1399
2026-01-17 13:51:38,162: t15.2023.09.01 val PER: 0.0519
2026-01-17 13:51:38,162: t15.2023.09.03 val PER: 0.1223
2026-01-17 13:51:38,162: t15.2023.09.24 val PER: 0.0922
2026-01-17 13:51:38,162: t15.2023.09.29 val PER: 0.1008
2026-01-17 13:51:38,162: t15.2023.10.01 val PER: 0.1466
2026-01-17 13:51:38,162: t15.2023.10.06 val PER: 0.0635
2026-01-17 13:51:38,162: t15.2023.10.08 val PER: 0.2043
2026-01-17 13:51:38,162: t15.2023.10.13 val PER: 0.1373
2026-01-17 13:51:38,162: t15.2023.10.15 val PER: 0.1160
2026-01-17 13:51:38,163: t15.2023.10.20 val PER: 0.1611
2026-01-17 13:51:38,163: t15.2023.10.22 val PER: 0.0880
2026-01-17 13:51:38,163: t15.2023.11.03 val PER: 0.1445
2026-01-17 13:51:38,163: t15.2023.11.04 val PER: 0.0239
2026-01-17 13:51:38,163: t15.2023.11.17 val PER: 0.0233
2026-01-17 13:51:38,163: t15.2023.11.19 val PER: 0.0220
2026-01-17 13:51:38,163: t15.2023.11.26 val PER: 0.0558
2026-01-17 13:51:38,163: t15.2023.12.03 val PER: 0.0599
2026-01-17 13:51:38,163: t15.2023.12.08 val PER: 0.0493
2026-01-17 13:51:38,163: t15.2023.12.10 val PER: 0.0394
2026-01-17 13:51:38,163: t15.2023.12.17 val PER: 0.0956
2026-01-17 13:51:38,163: t15.2023.12.29 val PER: 0.0865
2026-01-17 13:51:38,163: t15.2024.02.25 val PER: 0.0772
2026-01-17 13:51:38,163: t15.2024.03.08 val PER: 0.1878
2026-01-17 13:51:38,163: t15.2024.03.15 val PER: 0.1620
2026-01-17 13:51:38,163: t15.2024.03.17 val PER: 0.0767
2026-01-17 13:51:38,163: t15.2024.05.10 val PER: 0.1278
2026-01-17 13:51:38,164: t15.2024.06.14 val PER: 0.1293
2026-01-17 13:51:38,164: t15.2024.07.19 val PER: 0.1859
2026-01-17 13:51:38,164: t15.2024.07.21 val PER: 0.0607
2026-01-17 13:51:38,164: t15.2024.07.28 val PER: 0.1051
2026-01-17 13:51:38,164: t15.2025.01.10 val PER: 0.2548
2026-01-17 13:51:38,164: t15.2025.01.12 val PER: 0.1270
2026-01-17 13:51:38,164: t15.2025.03.14 val PER: 0.3166
2026-01-17 13:51:38,164: t15.2025.03.16 val PER: 0.1505
2026-01-17 13:51:38,164: t15.2025.03.30 val PER: 0.2356
2026-01-17 13:51:38,164: t15.2025.04.13 val PER: 0.1826
2026-01-17 13:52:14,033: Train batch 31200: loss: 0.86 grad norm: 23.38 time: 0.068
2026-01-17 13:52:46,538: Train batch 31400: loss: 1.82 grad norm: 34.79 time: 0.056
2026-01-17 13:53:02,170: Running test after training batch: 31500
2026-01-17 13:53:02,284: WER debug GT example: You can see the code at this point as well.
