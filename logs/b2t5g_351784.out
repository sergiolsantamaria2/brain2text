TMPDIR=/home/e12511253/tmp
JOB_TMP=/home/e12511253/tmp/e12511253_b2t_351784
TORCH_EXTENSIONS_DIR=/home/e12511253/tmp/e12511253_b2t_351784/torch_extensions
WANDB_DIR=/home/e12511253/tmp/e12511253_b2t_351784/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/home/e12511253/tmp/e12511253_b2t_351784/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  9 11:06 /home/e12511253/tmp/e12511253_b2t_351784/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
OUT_ROOT=/home/e12511253/Brain2Text/brain2text/trained_models
==============================================
Job: b2t5g  ID: 351784
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_5gram_only.yaml
Folders: configs/experiments/gru/ablations/lr_grid
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/gru/ablations/lr_grid ==========
Num configs: 3

=== RUN lr_0030.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030
2026-01-09 11:06:07,905: Using device: cuda:0
2026-01-09 11:09:57,514: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-09 11:10:07,338: Using 45 sessions after filtering (from 45).
2026-01-09 11:10:07,760: Using torch.compile (if available)
2026-01-09 11:10:07,760: torch.compile not available (torch<2.0). Skipping.
2026-01-09 11:10:07,760: Initialized RNN decoding model
2026-01-09 11:10:07,760: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-09 11:10:07,760: Model has 44,907,305 parameters
2026-01-09 11:10:07,761: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-09 11:10:13,336: Successfully initialized datasets
2026-01-09 11:10:13,336: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-09 11:10:15,412: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.354
2026-01-09 11:10:15,412: Running test after training batch: 0
2026-01-09 11:10:15,523: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:10:21,471: WER debug example
  GT : you can see the code at this point as well
  PR : she has from his
2026-01-09 11:10:22,496: WER debug example
  GT : how does it keep the cost down
  PR : money from
2026-01-09 11:14:14,735: Val batch 0: PER (avg): 1.4306 CTC Loss (avg): 633.3639 WER(5gram): 99.67% (n=256) time: 239.323
2026-01-09 11:14:14,739: WER lens: avg_true_words=5.99 avg_pred_words=2.81 max_pred_words=7
2026-01-09 11:14:14,748: t15.2023.08.13 val PER: 1.3046
2026-01-09 11:14:14,754: t15.2023.08.18 val PER: 1.4283
2026-01-09 11:14:14,756: t15.2023.08.20 val PER: 1.3018
2026-01-09 11:14:14,756: t15.2023.08.25 val PER: 1.3358
2026-01-09 11:14:14,756: t15.2023.08.27 val PER: 1.2524
2026-01-09 11:14:14,756: t15.2023.09.01 val PER: 1.4529
2026-01-09 11:14:14,757: t15.2023.09.03 val PER: 1.3171
2026-01-09 11:14:14,757: t15.2023.09.24 val PER: 1.5400
2026-01-09 11:14:14,757: t15.2023.09.29 val PER: 1.4671
2026-01-09 11:14:14,757: t15.2023.10.01 val PER: 1.2173
2026-01-09 11:14:14,757: t15.2023.10.06 val PER: 1.4909
2026-01-09 11:14:14,758: t15.2023.10.08 val PER: 1.1908
2026-01-09 11:14:14,758: t15.2023.10.13 val PER: 1.3933
2026-01-09 11:14:14,758: t15.2023.10.15 val PER: 1.3869
2026-01-09 11:14:14,758: t15.2023.10.20 val PER: 1.5000
2026-01-09 11:14:14,758: t15.2023.10.22 val PER: 1.3886
2026-01-09 11:14:14,758: t15.2023.11.03 val PER: 1.5977
2026-01-09 11:14:14,758: t15.2023.11.04 val PER: 2.0444
2026-01-09 11:14:14,758: t15.2023.11.17 val PER: 1.9580
2026-01-09 11:14:14,758: t15.2023.11.19 val PER: 1.6766
2026-01-09 11:14:14,759: t15.2023.11.26 val PER: 1.5406
2026-01-09 11:14:14,759: t15.2023.12.03 val PER: 1.4338
2026-01-09 11:14:14,759: t15.2023.12.08 val PER: 1.4501
2026-01-09 11:14:14,759: t15.2023.12.10 val PER: 1.6991
2026-01-09 11:14:14,759: t15.2023.12.17 val PER: 1.3087
2026-01-09 11:14:14,759: t15.2023.12.29 val PER: 1.4139
2026-01-09 11:14:14,759: t15.2024.02.25 val PER: 1.4199
2026-01-09 11:14:14,759: t15.2024.03.08 val PER: 1.3243
2026-01-09 11:14:14,759: t15.2024.03.15 val PER: 1.3177
2026-01-09 11:14:14,759: t15.2024.03.17 val PER: 1.4017
2026-01-09 11:14:14,759: t15.2024.05.10 val PER: 1.3284
2026-01-09 11:14:14,760: t15.2024.06.14 val PER: 1.5363
2026-01-09 11:14:14,760: t15.2024.07.19 val PER: 1.0811
2026-01-09 11:14:14,760: t15.2024.07.21 val PER: 1.6317
2026-01-09 11:14:14,760: t15.2024.07.28 val PER: 1.6588
2026-01-09 11:14:14,760: t15.2025.01.10 val PER: 1.0868
2026-01-09 11:14:14,760: t15.2025.01.12 val PER: 1.7644
2026-01-09 11:14:14,760: t15.2025.03.14 val PER: 1.0399
2026-01-09 11:14:14,760: t15.2025.03.16 val PER: 1.6217
2026-01-09 11:14:14,761: t15.2025.03.30 val PER: 1.2920
2026-01-09 11:14:14,761: t15.2025.04.13 val PER: 1.5877
2026-01-09 11:14:14,761: New best val WER(5gram) inf% --> 99.67%
2026-01-09 11:14:14,935: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_0
2026-01-09 11:14:36,376: Train batch 200: loss: 80.58 grad norm: 80.00 time: 0.056
2026-01-09 11:14:53,153: Train batch 400: loss: 57.85 grad norm: 93.45 time: 0.064
2026-01-09 11:15:01,526: Running test after training batch: 500
2026-01-09 11:15:01,662: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:15:07,230: WER debug example
  GT : you can see the code at this point as well
  PR : you'll need is the ease and at this guide is all
2026-01-09 11:15:07,386: WER debug example
  GT : how does it keep the cost down
  PR : and does it do this is as
2026-01-09 11:16:01,166: Val batch 500: PER (avg): 0.5412 CTC Loss (avg): 58.5237 WER(5gram): 79.40% (n=256) time: 59.639
2026-01-09 11:16:01,166: WER lens: avg_true_words=5.99 avg_pred_words=5.74 max_pred_words=12
2026-01-09 11:16:01,166: t15.2023.08.13 val PER: 0.4771
2026-01-09 11:16:01,167: t15.2023.08.18 val PER: 0.4912
2026-01-09 11:16:01,167: t15.2023.08.20 val PER: 0.4758
2026-01-09 11:16:01,167: t15.2023.08.25 val PER: 0.4623
2026-01-09 11:16:01,167: t15.2023.08.27 val PER: 0.5466
2026-01-09 11:16:01,167: t15.2023.09.01 val PER: 0.4367
2026-01-09 11:16:01,167: t15.2023.09.03 val PER: 0.5154
2026-01-09 11:16:01,167: t15.2023.09.24 val PER: 0.4600
2026-01-09 11:16:01,167: t15.2023.09.29 val PER: 0.4907
2026-01-09 11:16:01,167: t15.2023.10.01 val PER: 0.5376
2026-01-09 11:16:01,167: t15.2023.10.06 val PER: 0.4543
2026-01-09 11:16:01,167: t15.2023.10.08 val PER: 0.5737
2026-01-09 11:16:01,167: t15.2023.10.13 val PER: 0.5849
2026-01-09 11:16:01,168: t15.2023.10.15 val PER: 0.5175
2026-01-09 11:16:01,168: t15.2023.10.20 val PER: 0.4799
2026-01-09 11:16:01,168: t15.2023.10.22 val PER: 0.4688
2026-01-09 11:16:01,168: t15.2023.11.03 val PER: 0.5251
2026-01-09 11:16:01,168: t15.2023.11.04 val PER: 0.2935
2026-01-09 11:16:01,168: t15.2023.11.17 val PER: 0.3841
2026-01-09 11:16:01,168: t15.2023.11.19 val PER: 0.3713
2026-01-09 11:16:01,168: t15.2023.11.26 val PER: 0.5783
2026-01-09 11:16:01,168: t15.2023.12.03 val PER: 0.5210
2026-01-09 11:16:01,168: t15.2023.12.08 val PER: 0.5433
2026-01-09 11:16:01,168: t15.2023.12.10 val PER: 0.5033
2026-01-09 11:16:01,168: t15.2023.12.17 val PER: 0.5925
2026-01-09 11:16:01,168: t15.2023.12.29 val PER: 0.5731
2026-01-09 11:16:01,168: t15.2024.02.25 val PER: 0.5070
2026-01-09 11:16:01,168: t15.2024.03.08 val PER: 0.6273
2026-01-09 11:16:01,168: t15.2024.03.15 val PER: 0.5885
2026-01-09 11:16:01,169: t15.2024.03.17 val PER: 0.5209
2026-01-09 11:16:01,169: t15.2024.05.10 val PER: 0.5706
2026-01-09 11:16:01,169: t15.2024.06.14 val PER: 0.5252
2026-01-09 11:16:01,169: t15.2024.07.19 val PER: 0.6915
2026-01-09 11:16:01,169: t15.2024.07.21 val PER: 0.4972
2026-01-09 11:16:01,169: t15.2024.07.28 val PER: 0.5390
2026-01-09 11:16:01,169: t15.2025.01.10 val PER: 0.7521
2026-01-09 11:16:01,169: t15.2025.01.12 val PER: 0.5781
2026-01-09 11:16:01,169: t15.2025.03.14 val PER: 0.7559
2026-01-09 11:16:01,169: t15.2025.03.16 val PER: 0.6073
2026-01-09 11:16:01,169: t15.2025.03.30 val PER: 0.7356
2026-01-09 11:16:01,169: t15.2025.04.13 val PER: 0.5892
2026-01-09 11:16:01,171: New best val WER(5gram) 99.67% --> 79.40%
2026-01-09 11:16:01,364: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_500
2026-01-09 11:16:13,506: Train batch 600: loss: 51.82 grad norm: 80.10 time: 0.079
2026-01-09 11:16:30,842: Train batch 800: loss: 44.06 grad norm: 92.72 time: 0.058
2026-01-09 11:16:47,916: Train batch 1000: loss: 45.02 grad norm: 80.06 time: 0.067
2026-01-09 11:16:47,917: Running test after training batch: 1000
2026-01-09 11:16:48,039: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:16:53,700: WER debug example
  GT : you can see the code at this point as well
  PR : you'd get is that good at it and is well
2026-01-09 11:16:53,799: WER debug example
  GT : how does it keep the cost down
  PR : and as it is that it's not
2026-01-09 11:17:25,439: Val batch 1000: PER (avg): 0.4315 CTC Loss (avg): 45.2180 WER(5gram): 63.04% (n=256) time: 37.522
2026-01-09 11:17:25,440: WER lens: avg_true_words=5.99 avg_pred_words=5.64 max_pred_words=12
2026-01-09 11:17:25,440: t15.2023.08.13 val PER: 0.3971
2026-01-09 11:17:25,440: t15.2023.08.18 val PER: 0.3713
2026-01-09 11:17:25,440: t15.2023.08.20 val PER: 0.3670
2026-01-09 11:17:25,440: t15.2023.08.25 val PER: 0.3223
2026-01-09 11:17:25,440: t15.2023.08.27 val PER: 0.4421
2026-01-09 11:17:25,440: t15.2023.09.01 val PER: 0.3255
2026-01-09 11:17:25,440: t15.2023.09.03 val PER: 0.4252
2026-01-09 11:17:25,441: t15.2023.09.24 val PER: 0.3507
2026-01-09 11:17:25,441: t15.2023.09.29 val PER: 0.3918
2026-01-09 11:17:25,441: t15.2023.10.01 val PER: 0.4240
2026-01-09 11:17:25,441: t15.2023.10.06 val PER: 0.3402
2026-01-09 11:17:25,441: t15.2023.10.08 val PER: 0.4668
2026-01-09 11:17:25,441: t15.2023.10.13 val PER: 0.4856
2026-01-09 11:17:25,441: t15.2023.10.15 val PER: 0.4041
2026-01-09 11:17:25,441: t15.2023.10.20 val PER: 0.3960
2026-01-09 11:17:25,441: t15.2023.10.22 val PER: 0.3719
2026-01-09 11:17:25,441: t15.2023.11.03 val PER: 0.4179
2026-01-09 11:17:25,441: t15.2023.11.04 val PER: 0.1980
2026-01-09 11:17:25,441: t15.2023.11.17 val PER: 0.2784
2026-01-09 11:17:25,441: t15.2023.11.19 val PER: 0.2415
2026-01-09 11:17:25,441: t15.2023.11.26 val PER: 0.4667
2026-01-09 11:17:25,441: t15.2023.12.03 val PER: 0.4349
2026-01-09 11:17:25,441: t15.2023.12.08 val PER: 0.4214
2026-01-09 11:17:25,442: t15.2023.12.10 val PER: 0.3732
2026-01-09 11:17:25,442: t15.2023.12.17 val PER: 0.4470
2026-01-09 11:17:25,442: t15.2023.12.29 val PER: 0.4152
2026-01-09 11:17:25,442: t15.2024.02.25 val PER: 0.3919
2026-01-09 11:17:25,442: t15.2024.03.08 val PER: 0.5235
2026-01-09 11:17:25,442: t15.2024.03.15 val PER: 0.4703
2026-01-09 11:17:25,442: t15.2024.03.17 val PER: 0.4296
2026-01-09 11:17:25,442: t15.2024.05.10 val PER: 0.4517
2026-01-09 11:17:25,443: t15.2024.06.14 val PER: 0.4069
2026-01-09 11:17:25,443: t15.2024.07.19 val PER: 0.5722
2026-01-09 11:17:25,443: t15.2024.07.21 val PER: 0.3966
2026-01-09 11:17:25,443: t15.2024.07.28 val PER: 0.4368
2026-01-09 11:17:25,443: t15.2025.01.10 val PER: 0.6501
2026-01-09 11:17:25,443: t15.2025.01.12 val PER: 0.4734
2026-01-09 11:17:25,443: t15.2025.03.14 val PER: 0.6598
2026-01-09 11:17:25,443: t15.2025.03.16 val PER: 0.4843
2026-01-09 11:17:25,443: t15.2025.03.30 val PER: 0.6586
2026-01-09 11:17:25,443: t15.2025.04.13 val PER: 0.5064
2026-01-09 11:17:25,444: New best val WER(5gram) 79.40% --> 63.04%
2026-01-09 11:17:25,635: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_1000
2026-01-09 11:17:46,399: Train batch 1200: loss: 36.44 grad norm: 81.86 time: 0.068
2026-01-09 11:18:03,554: Train batch 1400: loss: 39.15 grad norm: 85.09 time: 0.062
2026-01-09 11:18:12,334: Running test after training batch: 1500
2026-01-09 11:18:12,482: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:18:17,815: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-09 11:18:17,925: WER debug example
  GT : how does it keep the cost down
  PR : how does it feel that it's not
2026-01-09 11:18:38,626: Val batch 1500: PER (avg): 0.4030 CTC Loss (avg): 39.4786 WER(5gram): 43.61% (n=256) time: 26.292
2026-01-09 11:18:38,627: WER lens: avg_true_words=5.99 avg_pred_words=5.21 max_pred_words=12
2026-01-09 11:18:38,627: t15.2023.08.13 val PER: 0.3877
2026-01-09 11:18:38,627: t15.2023.08.18 val PER: 0.3370
2026-01-09 11:18:38,627: t15.2023.08.20 val PER: 0.3288
2026-01-09 11:18:38,628: t15.2023.08.25 val PER: 0.2741
2026-01-09 11:18:38,628: t15.2023.08.27 val PER: 0.4309
2026-01-09 11:18:38,628: t15.2023.09.01 val PER: 0.2873
2026-01-09 11:18:38,628: t15.2023.09.03 val PER: 0.4074
2026-01-09 11:18:38,628: t15.2023.09.24 val PER: 0.3337
2026-01-09 11:18:38,628: t15.2023.09.29 val PER: 0.3535
2026-01-09 11:18:38,629: t15.2023.10.01 val PER: 0.4168
2026-01-09 11:18:38,629: t15.2023.10.06 val PER: 0.3036
2026-01-09 11:18:38,629: t15.2023.10.08 val PER: 0.4520
2026-01-09 11:18:38,629: t15.2023.10.13 val PER: 0.4585
2026-01-09 11:18:38,629: t15.2023.10.15 val PER: 0.3856
2026-01-09 11:18:38,629: t15.2023.10.20 val PER: 0.3557
2026-01-09 11:18:38,629: t15.2023.10.22 val PER: 0.3441
2026-01-09 11:18:38,629: t15.2023.11.03 val PER: 0.3840
2026-01-09 11:18:38,629: t15.2023.11.04 val PER: 0.1399
2026-01-09 11:18:38,629: t15.2023.11.17 val PER: 0.2582
2026-01-09 11:18:38,630: t15.2023.11.19 val PER: 0.2076
2026-01-09 11:18:38,630: t15.2023.11.26 val PER: 0.4464
2026-01-09 11:18:38,630: t15.2023.12.03 val PER: 0.3950
2026-01-09 11:18:38,630: t15.2023.12.08 val PER: 0.3775
2026-01-09 11:18:38,630: t15.2023.12.10 val PER: 0.3246
2026-01-09 11:18:38,630: t15.2023.12.17 val PER: 0.4075
2026-01-09 11:18:38,630: t15.2023.12.29 val PER: 0.3892
2026-01-09 11:18:38,630: t15.2024.02.25 val PER: 0.3244
2026-01-09 11:18:38,631: t15.2024.03.08 val PER: 0.4651
2026-01-09 11:18:38,631: t15.2024.03.15 val PER: 0.4346
2026-01-09 11:18:38,631: t15.2024.03.17 val PER: 0.3926
2026-01-09 11:18:38,631: t15.2024.05.10 val PER: 0.4220
2026-01-09 11:18:38,631: t15.2024.06.14 val PER: 0.4196
2026-01-09 11:18:38,631: t15.2024.07.19 val PER: 0.5616
2026-01-09 11:18:38,631: t15.2024.07.21 val PER: 0.3676
2026-01-09 11:18:38,631: t15.2024.07.28 val PER: 0.3904
2026-01-09 11:18:38,631: t15.2025.01.10 val PER: 0.6364
2026-01-09 11:18:38,632: t15.2025.01.12 val PER: 0.4450
2026-01-09 11:18:38,632: t15.2025.03.14 val PER: 0.6243
2026-01-09 11:18:38,632: t15.2025.03.16 val PER: 0.4817
2026-01-09 11:18:38,632: t15.2025.03.30 val PER: 0.6621
2026-01-09 11:18:38,632: t15.2025.04.13 val PER: 0.4993
2026-01-09 11:18:38,632: New best val WER(5gram) 63.04% --> 43.61%
2026-01-09 11:18:38,819: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_1500
2026-01-09 11:18:47,339: Train batch 1600: loss: 39.93 grad norm: 83.66 time: 0.067
2026-01-09 11:19:04,446: Train batch 1800: loss: 38.03 grad norm: 75.97 time: 0.090
2026-01-09 11:19:21,623: Train batch 2000: loss: 36.88 grad norm: 77.24 time: 0.069
2026-01-09 11:19:21,623: Running test after training batch: 2000
2026-01-09 11:19:21,738: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:19:26,671: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-09 11:19:26,765: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost at
2026-01-09 11:19:45,470: Val batch 2000: PER (avg): 0.3523 CTC Loss (avg): 35.5323 WER(5gram): 33.12% (n=256) time: 23.847
2026-01-09 11:19:45,471: WER lens: avg_true_words=5.99 avg_pred_words=5.71 max_pred_words=12
2026-01-09 11:19:45,471: t15.2023.08.13 val PER: 0.3233
2026-01-09 11:19:45,471: t15.2023.08.18 val PER: 0.2842
2026-01-09 11:19:45,471: t15.2023.08.20 val PER: 0.2685
2026-01-09 11:19:45,471: t15.2023.08.25 val PER: 0.2395
2026-01-09 11:19:45,472: t15.2023.08.27 val PER: 0.3601
2026-01-09 11:19:45,472: t15.2023.09.01 val PER: 0.2435
2026-01-09 11:19:45,472: t15.2023.09.03 val PER: 0.3563
2026-01-09 11:19:45,472: t15.2023.09.24 val PER: 0.2767
2026-01-09 11:19:45,472: t15.2023.09.29 val PER: 0.3006
2026-01-09 11:19:45,472: t15.2023.10.01 val PER: 0.3554
2026-01-09 11:19:45,472: t15.2023.10.06 val PER: 0.2616
2026-01-09 11:19:45,472: t15.2023.10.08 val PER: 0.4235
2026-01-09 11:19:45,472: t15.2023.10.13 val PER: 0.3949
2026-01-09 11:19:45,472: t15.2023.10.15 val PER: 0.3158
2026-01-09 11:19:45,472: t15.2023.10.20 val PER: 0.3020
2026-01-09 11:19:45,472: t15.2023.10.22 val PER: 0.2739
2026-01-09 11:19:45,473: t15.2023.11.03 val PER: 0.3399
2026-01-09 11:19:45,473: t15.2023.11.04 val PER: 0.1263
2026-01-09 11:19:45,473: t15.2023.11.17 val PER: 0.1944
2026-01-09 11:19:45,473: t15.2023.11.19 val PER: 0.1697
2026-01-09 11:19:45,473: t15.2023.11.26 val PER: 0.3942
2026-01-09 11:19:45,473: t15.2023.12.03 val PER: 0.3225
2026-01-09 11:19:45,473: t15.2023.12.08 val PER: 0.3256
2026-01-09 11:19:45,473: t15.2023.12.10 val PER: 0.2865
2026-01-09 11:19:45,473: t15.2023.12.17 val PER: 0.3482
2026-01-09 11:19:45,473: t15.2023.12.29 val PER: 0.3452
2026-01-09 11:19:45,473: t15.2024.02.25 val PER: 0.3020
2026-01-09 11:19:45,473: t15.2024.03.08 val PER: 0.4282
2026-01-09 11:19:45,473: t15.2024.03.15 val PER: 0.3840
2026-01-09 11:19:45,473: t15.2024.03.17 val PER: 0.3640
2026-01-09 11:19:45,473: t15.2024.05.10 val PER: 0.3744
2026-01-09 11:19:45,473: t15.2024.06.14 val PER: 0.3754
2026-01-09 11:19:45,473: t15.2024.07.19 val PER: 0.4918
2026-01-09 11:19:45,474: t15.2024.07.21 val PER: 0.3317
2026-01-09 11:19:45,474: t15.2024.07.28 val PER: 0.3522
2026-01-09 11:19:45,474: t15.2025.01.10 val PER: 0.5716
2026-01-09 11:19:45,474: t15.2025.01.12 val PER: 0.4180
2026-01-09 11:19:45,474: t15.2025.03.14 val PER: 0.5651
2026-01-09 11:19:45,474: t15.2025.03.16 val PER: 0.4411
2026-01-09 11:19:45,474: t15.2025.03.30 val PER: 0.5977
2026-01-09 11:19:45,474: t15.2025.04.13 val PER: 0.4379
2026-01-09 11:19:45,475: New best val WER(5gram) 43.61% --> 33.12%
2026-01-09 11:19:45,660: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_2000
2026-01-09 11:20:04,864: Train batch 2200: loss: 31.96 grad norm: 83.85 time: 0.062
2026-01-09 11:20:22,018: Train batch 2400: loss: 32.20 grad norm: 66.60 time: 0.053
2026-01-09 11:20:30,511: Running test after training batch: 2500
2026-01-09 11:20:30,650: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:20:36,056: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-09 11:20:36,120: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:20:52,192: Val batch 2500: PER (avg): 0.3291 CTC Loss (avg): 32.6885 WER(5gram): 29.66% (n=256) time: 21.680
2026-01-09 11:20:52,192: WER lens: avg_true_words=5.99 avg_pred_words=5.64 max_pred_words=12
2026-01-09 11:20:52,192: t15.2023.08.13 val PER: 0.3004
2026-01-09 11:20:52,192: t15.2023.08.18 val PER: 0.2724
2026-01-09 11:20:52,193: t15.2023.08.20 val PER: 0.2550
2026-01-09 11:20:52,193: t15.2023.08.25 val PER: 0.2214
2026-01-09 11:20:52,193: t15.2023.08.27 val PER: 0.3408
2026-01-09 11:20:52,193: t15.2023.09.01 val PER: 0.2256
2026-01-09 11:20:52,193: t15.2023.09.03 val PER: 0.3278
2026-01-09 11:20:52,193: t15.2023.09.24 val PER: 0.2694
2026-01-09 11:20:52,193: t15.2023.09.29 val PER: 0.2795
2026-01-09 11:20:52,193: t15.2023.10.01 val PER: 0.3296
2026-01-09 11:20:52,193: t15.2023.10.06 val PER: 0.2304
2026-01-09 11:20:52,193: t15.2023.10.08 val PER: 0.3843
2026-01-09 11:20:52,193: t15.2023.10.13 val PER: 0.3747
2026-01-09 11:20:52,193: t15.2023.10.15 val PER: 0.3118
2026-01-09 11:20:52,193: t15.2023.10.20 val PER: 0.2819
2026-01-09 11:20:52,193: t15.2023.10.22 val PER: 0.2506
2026-01-09 11:20:52,194: t15.2023.11.03 val PER: 0.3087
2026-01-09 11:20:52,194: t15.2023.11.04 val PER: 0.1024
2026-01-09 11:20:52,194: t15.2023.11.17 val PER: 0.1695
2026-01-09 11:20:52,194: t15.2023.11.19 val PER: 0.1457
2026-01-09 11:20:52,194: t15.2023.11.26 val PER: 0.3812
2026-01-09 11:20:52,194: t15.2023.12.03 val PER: 0.3046
2026-01-09 11:20:52,194: t15.2023.12.08 val PER: 0.3156
2026-01-09 11:20:52,194: t15.2023.12.10 val PER: 0.2536
2026-01-09 11:20:52,194: t15.2023.12.17 val PER: 0.3139
2026-01-09 11:20:52,194: t15.2023.12.29 val PER: 0.3288
2026-01-09 11:20:52,194: t15.2024.02.25 val PER: 0.2556
2026-01-09 11:20:52,194: t15.2024.03.08 val PER: 0.3898
2026-01-09 11:20:52,194: t15.2024.03.15 val PER: 0.3621
2026-01-09 11:20:52,194: t15.2024.03.17 val PER: 0.3285
2026-01-09 11:20:52,194: t15.2024.05.10 val PER: 0.3418
2026-01-09 11:20:52,195: t15.2024.06.14 val PER: 0.3423
2026-01-09 11:20:52,195: t15.2024.07.19 val PER: 0.4786
2026-01-09 11:20:52,195: t15.2024.07.21 val PER: 0.3041
2026-01-09 11:20:52,195: t15.2024.07.28 val PER: 0.3250
2026-01-09 11:20:52,195: t15.2025.01.10 val PER: 0.5303
2026-01-09 11:20:52,195: t15.2025.01.12 val PER: 0.3995
2026-01-09 11:20:52,195: t15.2025.03.14 val PER: 0.5429
2026-01-09 11:20:52,195: t15.2025.03.16 val PER: 0.3940
2026-01-09 11:20:52,195: t15.2025.03.30 val PER: 0.5586
2026-01-09 11:20:52,195: t15.2025.04.13 val PER: 0.4308
2026-01-09 11:20:52,196: New best val WER(5gram) 33.12% --> 29.66%
2026-01-09 11:20:52,386: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_2500
2026-01-09 11:21:02,452: Train batch 2600: loss: 38.19 grad norm: 82.60 time: 0.056
2026-01-09 11:21:19,468: Train batch 2800: loss: 28.56 grad norm: 74.95 time: 0.083
2026-01-09 11:21:36,274: Train batch 3000: loss: 34.45 grad norm: 79.39 time: 0.084
2026-01-09 11:21:36,275: Running test after training batch: 3000
2026-01-09 11:21:36,373: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:21:41,429: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-09 11:21:41,482: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost at
2026-01-09 11:21:57,417: Val batch 3000: PER (avg): 0.3060 CTC Loss (avg): 30.6056 WER(5gram): 26.34% (n=256) time: 21.142
2026-01-09 11:21:57,417: WER lens: avg_true_words=5.99 avg_pred_words=5.87 max_pred_words=12
2026-01-09 11:21:57,418: t15.2023.08.13 val PER: 0.2786
2026-01-09 11:21:57,418: t15.2023.08.18 val PER: 0.2397
2026-01-09 11:21:57,418: t15.2023.08.20 val PER: 0.2351
2026-01-09 11:21:57,418: t15.2023.08.25 val PER: 0.2199
2026-01-09 11:21:57,418: t15.2023.08.27 val PER: 0.3264
2026-01-09 11:21:57,418: t15.2023.09.01 val PER: 0.2127
2026-01-09 11:21:57,418: t15.2023.09.03 val PER: 0.3005
2026-01-09 11:21:57,418: t15.2023.09.24 val PER: 0.2257
2026-01-09 11:21:57,419: t15.2023.09.29 val PER: 0.2597
2026-01-09 11:21:57,419: t15.2023.10.01 val PER: 0.3118
2026-01-09 11:21:57,419: t15.2023.10.06 val PER: 0.2185
2026-01-09 11:21:57,419: t15.2023.10.08 val PER: 0.3654
2026-01-09 11:21:57,419: t15.2023.10.13 val PER: 0.3693
2026-01-09 11:21:57,419: t15.2023.10.15 val PER: 0.2933
2026-01-09 11:21:57,419: t15.2023.10.20 val PER: 0.2785
2026-01-09 11:21:57,419: t15.2023.10.22 val PER: 0.2383
2026-01-09 11:21:57,419: t15.2023.11.03 val PER: 0.3012
2026-01-09 11:21:57,419: t15.2023.11.04 val PER: 0.1024
2026-01-09 11:21:57,419: t15.2023.11.17 val PER: 0.1446
2026-01-09 11:21:57,419: t15.2023.11.19 val PER: 0.1377
2026-01-09 11:21:57,419: t15.2023.11.26 val PER: 0.3290
2026-01-09 11:21:57,419: t15.2023.12.03 val PER: 0.2815
2026-01-09 11:21:57,419: t15.2023.12.08 val PER: 0.2776
2026-01-09 11:21:57,420: t15.2023.12.10 val PER: 0.2313
2026-01-09 11:21:57,420: t15.2023.12.17 val PER: 0.3025
2026-01-09 11:21:57,420: t15.2023.12.29 val PER: 0.2986
2026-01-09 11:21:57,420: t15.2024.02.25 val PER: 0.2570
2026-01-09 11:21:57,420: t15.2024.03.08 val PER: 0.3826
2026-01-09 11:21:57,420: t15.2024.03.15 val PER: 0.3508
2026-01-09 11:21:57,420: t15.2024.03.17 val PER: 0.3110
2026-01-09 11:21:57,420: t15.2024.05.10 val PER: 0.3299
2026-01-09 11:21:57,420: t15.2024.06.14 val PER: 0.3297
2026-01-09 11:21:57,420: t15.2024.07.19 val PER: 0.4272
2026-01-09 11:21:57,420: t15.2024.07.21 val PER: 0.2710
2026-01-09 11:21:57,420: t15.2024.07.28 val PER: 0.2963
2026-01-09 11:21:57,420: t15.2025.01.10 val PER: 0.5165
2026-01-09 11:21:57,421: t15.2025.01.12 val PER: 0.3618
2026-01-09 11:21:57,421: t15.2025.03.14 val PER: 0.5059
2026-01-09 11:21:57,421: t15.2025.03.16 val PER: 0.3586
2026-01-09 11:21:57,421: t15.2025.03.30 val PER: 0.5287
2026-01-09 11:21:57,421: t15.2025.04.13 val PER: 0.3780
2026-01-09 11:21:57,422: New best val WER(5gram) 29.66% --> 26.34%
2026-01-09 11:21:57,623: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_3000
2026-01-09 11:22:16,610: Train batch 3200: loss: 30.49 grad norm: 73.34 time: 0.076
2026-01-09 11:22:33,686: Train batch 3400: loss: 21.38 grad norm: 57.90 time: 0.049
2026-01-09 11:22:42,444: Running test after training batch: 3500
2026-01-09 11:22:42,570: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:22:47,521: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-09 11:22:47,577: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost at
2026-01-09 11:23:02,757: Val batch 3500: PER (avg): 0.2950 CTC Loss (avg): 29.2955 WER(5gram): 25.42% (n=256) time: 20.312
2026-01-09 11:23:02,757: WER lens: avg_true_words=5.99 avg_pred_words=5.97 max_pred_words=12
2026-01-09 11:23:02,757: t15.2023.08.13 val PER: 0.2765
2026-01-09 11:23:02,758: t15.2023.08.18 val PER: 0.2347
2026-01-09 11:23:02,758: t15.2023.08.20 val PER: 0.2327
2026-01-09 11:23:02,758: t15.2023.08.25 val PER: 0.2033
2026-01-09 11:23:02,758: t15.2023.08.27 val PER: 0.3039
2026-01-09 11:23:02,758: t15.2023.09.01 val PER: 0.1981
2026-01-09 11:23:02,758: t15.2023.09.03 val PER: 0.2945
2026-01-09 11:23:02,758: t15.2023.09.24 val PER: 0.2184
2026-01-09 11:23:02,758: t15.2023.09.29 val PER: 0.2502
2026-01-09 11:23:02,758: t15.2023.10.01 val PER: 0.3025
2026-01-09 11:23:02,758: t15.2023.10.06 val PER: 0.2088
2026-01-09 11:23:02,758: t15.2023.10.08 val PER: 0.3748
2026-01-09 11:23:02,758: t15.2023.10.13 val PER: 0.3429
2026-01-09 11:23:02,758: t15.2023.10.15 val PER: 0.2775
2026-01-09 11:23:02,759: t15.2023.10.20 val PER: 0.2416
2026-01-09 11:23:02,759: t15.2023.10.22 val PER: 0.2372
2026-01-09 11:23:02,759: t15.2023.11.03 val PER: 0.2870
2026-01-09 11:23:02,759: t15.2023.11.04 val PER: 0.0887
2026-01-09 11:23:02,759: t15.2023.11.17 val PER: 0.1337
2026-01-09 11:23:02,759: t15.2023.11.19 val PER: 0.1357
2026-01-09 11:23:02,759: t15.2023.11.26 val PER: 0.3065
2026-01-09 11:23:02,759: t15.2023.12.03 val PER: 0.2721
2026-01-09 11:23:02,759: t15.2023.12.08 val PER: 0.2736
2026-01-09 11:23:02,759: t15.2023.12.10 val PER: 0.2286
2026-01-09 11:23:02,759: t15.2023.12.17 val PER: 0.2838
2026-01-09 11:23:02,759: t15.2023.12.29 val PER: 0.2848
2026-01-09 11:23:02,759: t15.2024.02.25 val PER: 0.2346
2026-01-09 11:23:02,760: t15.2024.03.08 val PER: 0.3698
2026-01-09 11:23:02,760: t15.2024.03.15 val PER: 0.3371
2026-01-09 11:23:02,760: t15.2024.03.17 val PER: 0.3068
2026-01-09 11:23:02,760: t15.2024.05.10 val PER: 0.3105
2026-01-09 11:23:02,760: t15.2024.06.14 val PER: 0.3076
2026-01-09 11:23:02,760: t15.2024.07.19 val PER: 0.4285
2026-01-09 11:23:02,760: t15.2024.07.21 val PER: 0.2490
2026-01-09 11:23:02,760: t15.2024.07.28 val PER: 0.2949
2026-01-09 11:23:02,760: t15.2025.01.10 val PER: 0.4904
2026-01-09 11:23:02,760: t15.2025.01.12 val PER: 0.3449
2026-01-09 11:23:02,760: t15.2025.03.14 val PER: 0.4882
2026-01-09 11:23:02,760: t15.2025.03.16 val PER: 0.3573
2026-01-09 11:23:02,760: t15.2025.03.30 val PER: 0.5218
2026-01-09 11:23:02,760: t15.2025.04.13 val PER: 0.3766
2026-01-09 11:23:02,761: New best val WER(5gram) 26.34% --> 25.42%
2026-01-09 11:23:02,951: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_3500
2026-01-09 11:23:11,950: Train batch 3600: loss: 25.25 grad norm: 64.65 time: 0.067
2026-01-09 11:23:28,811: Train batch 3800: loss: 28.52 grad norm: 71.80 time: 0.067
2026-01-09 11:23:45,940: Train batch 4000: loss: 22.06 grad norm: 57.76 time: 0.057
2026-01-09 11:23:45,941: Running test after training batch: 4000
2026-01-09 11:23:46,087: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:23:51,056: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-09 11:23:51,112: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us at
2026-01-09 11:24:05,514: Val batch 4000: PER (avg): 0.2774 CTC Loss (avg): 27.0999 WER(5gram): 26.40% (n=256) time: 19.573
2026-01-09 11:24:05,514: WER lens: avg_true_words=5.99 avg_pred_words=5.98 max_pred_words=12
2026-01-09 11:24:05,514: t15.2023.08.13 val PER: 0.2505
2026-01-09 11:24:05,514: t15.2023.08.18 val PER: 0.2221
2026-01-09 11:24:05,515: t15.2023.08.20 val PER: 0.2192
2026-01-09 11:24:05,515: t15.2023.08.25 val PER: 0.1837
2026-01-09 11:24:05,515: t15.2023.08.27 val PER: 0.3023
2026-01-09 11:24:05,515: t15.2023.09.01 val PER: 0.1834
2026-01-09 11:24:05,515: t15.2023.09.03 val PER: 0.2637
2026-01-09 11:24:05,515: t15.2023.09.24 val PER: 0.2039
2026-01-09 11:24:05,515: t15.2023.09.29 val PER: 0.2272
2026-01-09 11:24:05,515: t15.2023.10.01 val PER: 0.2827
2026-01-09 11:24:05,515: t15.2023.10.06 val PER: 0.1981
2026-01-09 11:24:05,515: t15.2023.10.08 val PER: 0.3464
2026-01-09 11:24:05,515: t15.2023.10.13 val PER: 0.3429
2026-01-09 11:24:05,515: t15.2023.10.15 val PER: 0.2584
2026-01-09 11:24:05,515: t15.2023.10.20 val PER: 0.2617
2026-01-09 11:24:05,516: t15.2023.10.22 val PER: 0.2171
2026-01-09 11:24:05,516: t15.2023.11.03 val PER: 0.2700
2026-01-09 11:24:05,516: t15.2023.11.04 val PER: 0.0785
2026-01-09 11:24:05,516: t15.2023.11.17 val PER: 0.1135
2026-01-09 11:24:05,516: t15.2023.11.19 val PER: 0.1098
2026-01-09 11:24:05,516: t15.2023.11.26 val PER: 0.2928
2026-01-09 11:24:05,516: t15.2023.12.03 val PER: 0.2542
2026-01-09 11:24:05,516: t15.2023.12.08 val PER: 0.2597
2026-01-09 11:24:05,516: t15.2023.12.10 val PER: 0.2155
2026-01-09 11:24:05,516: t15.2023.12.17 val PER: 0.2651
2026-01-09 11:24:05,516: t15.2023.12.29 val PER: 0.2814
2026-01-09 11:24:05,516: t15.2024.02.25 val PER: 0.2444
2026-01-09 11:24:05,516: t15.2024.03.08 val PER: 0.3514
2026-01-09 11:24:05,516: t15.2024.03.15 val PER: 0.3283
2026-01-09 11:24:05,516: t15.2024.03.17 val PER: 0.2915
2026-01-09 11:24:05,516: t15.2024.05.10 val PER: 0.2987
2026-01-09 11:24:05,517: t15.2024.06.14 val PER: 0.2871
2026-01-09 11:24:05,517: t15.2024.07.19 val PER: 0.4008
2026-01-09 11:24:05,517: t15.2024.07.21 val PER: 0.2262
2026-01-09 11:24:05,517: t15.2024.07.28 val PER: 0.2691
2026-01-09 11:24:05,517: t15.2025.01.10 val PER: 0.4656
2026-01-09 11:24:05,517: t15.2025.01.12 val PER: 0.3256
2026-01-09 11:24:05,517: t15.2025.03.14 val PER: 0.4512
2026-01-09 11:24:05,517: t15.2025.03.16 val PER: 0.3482
2026-01-09 11:24:05,517: t15.2025.03.30 val PER: 0.4690
2026-01-09 11:24:05,517: t15.2025.04.13 val PER: 0.3438
2026-01-09 11:24:05,661: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_4000
2026-01-09 11:24:23,180: Train batch 4200: loss: 25.95 grad norm: 72.09 time: 0.080
2026-01-09 11:24:40,249: Train batch 4400: loss: 19.89 grad norm: 61.43 time: 0.067
2026-01-09 11:24:48,765: Running test after training batch: 4500
2026-01-09 11:24:48,898: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:24:54,001: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-09 11:24:54,058: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us get
2026-01-09 11:25:07,692: Val batch 4500: PER (avg): 0.2609 CTC Loss (avg): 25.9037 WER(5gram): 23.40% (n=256) time: 18.926
2026-01-09 11:25:07,692: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-09 11:25:07,692: t15.2023.08.13 val PER: 0.2287
2026-01-09 11:25:07,692: t15.2023.08.18 val PER: 0.1970
2026-01-09 11:25:07,692: t15.2023.08.20 val PER: 0.2073
2026-01-09 11:25:07,693: t15.2023.08.25 val PER: 0.1657
2026-01-09 11:25:07,693: t15.2023.08.27 val PER: 0.2846
2026-01-09 11:25:07,693: t15.2023.09.01 val PER: 0.1705
2026-01-09 11:25:07,693: t15.2023.09.03 val PER: 0.2637
2026-01-09 11:25:07,693: t15.2023.09.24 val PER: 0.1857
2026-01-09 11:25:07,693: t15.2023.09.29 val PER: 0.2157
2026-01-09 11:25:07,693: t15.2023.10.01 val PER: 0.2781
2026-01-09 11:25:07,693: t15.2023.10.06 val PER: 0.1841
2026-01-09 11:25:07,693: t15.2023.10.08 val PER: 0.3302
2026-01-09 11:25:07,693: t15.2023.10.13 val PER: 0.3274
2026-01-09 11:25:07,693: t15.2023.10.15 val PER: 0.2413
2026-01-09 11:25:07,693: t15.2023.10.20 val PER: 0.2383
2026-01-09 11:25:07,693: t15.2023.10.22 val PER: 0.2027
2026-01-09 11:25:07,693: t15.2023.11.03 val PER: 0.2639
2026-01-09 11:25:07,693: t15.2023.11.04 val PER: 0.0683
2026-01-09 11:25:07,694: t15.2023.11.17 val PER: 0.1089
2026-01-09 11:25:07,694: t15.2023.11.19 val PER: 0.1078
2026-01-09 11:25:07,694: t15.2023.11.26 val PER: 0.2848
2026-01-09 11:25:07,694: t15.2023.12.03 val PER: 0.2353
2026-01-09 11:25:07,694: t15.2023.12.08 val PER: 0.2337
2026-01-09 11:25:07,694: t15.2023.12.10 val PER: 0.2011
2026-01-09 11:25:07,694: t15.2023.12.17 val PER: 0.2526
2026-01-09 11:25:07,694: t15.2023.12.29 val PER: 0.2656
2026-01-09 11:25:07,694: t15.2024.02.25 val PER: 0.2177
2026-01-09 11:25:07,694: t15.2024.03.08 val PER: 0.3414
2026-01-09 11:25:07,694: t15.2024.03.15 val PER: 0.3183
2026-01-09 11:25:07,694: t15.2024.03.17 val PER: 0.2587
2026-01-09 11:25:07,694: t15.2024.05.10 val PER: 0.2883
2026-01-09 11:25:07,695: t15.2024.06.14 val PER: 0.2681
2026-01-09 11:25:07,695: t15.2024.07.19 val PER: 0.3724
2026-01-09 11:25:07,695: t15.2024.07.21 val PER: 0.2034
2026-01-09 11:25:07,695: t15.2024.07.28 val PER: 0.2588
2026-01-09 11:25:07,695: t15.2025.01.10 val PER: 0.4490
2026-01-09 11:25:07,695: t15.2025.01.12 val PER: 0.2879
2026-01-09 11:25:07,695: t15.2025.03.14 val PER: 0.4334
2026-01-09 11:25:07,695: t15.2025.03.16 val PER: 0.3220
2026-01-09 11:25:07,695: t15.2025.03.30 val PER: 0.4494
2026-01-09 11:25:07,695: t15.2025.04.13 val PER: 0.3310
2026-01-09 11:25:07,696: New best val WER(5gram) 25.42% --> 23.40%
2026-01-09 11:25:07,887: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_4500
2026-01-09 11:25:16,516: Train batch 4600: loss: 24.57 grad norm: 72.76 time: 0.064
2026-01-09 11:25:34,066: Train batch 4800: loss: 16.97 grad norm: 57.99 time: 0.065
2026-01-09 11:25:51,359: Train batch 5000: loss: 36.50 grad norm: 81.92 time: 0.065
2026-01-09 11:25:51,359: Running test after training batch: 5000
2026-01-09 11:25:51,468: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:25:56,403: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-09 11:25:56,453: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:26:10,163: Val batch 5000: PER (avg): 0.2546 CTC Loss (avg): 24.8982 WER(5gram): 24.12% (n=256) time: 18.804
2026-01-09 11:26:10,164: WER lens: avg_true_words=5.99 avg_pred_words=6.03 max_pred_words=12
2026-01-09 11:26:10,164: t15.2023.08.13 val PER: 0.2318
2026-01-09 11:26:10,164: t15.2023.08.18 val PER: 0.2062
2026-01-09 11:26:10,164: t15.2023.08.20 val PER: 0.1938
2026-01-09 11:26:10,164: t15.2023.08.25 val PER: 0.1551
2026-01-09 11:26:10,164: t15.2023.08.27 val PER: 0.2685
2026-01-09 11:26:10,164: t15.2023.09.01 val PER: 0.1729
2026-01-09 11:26:10,164: t15.2023.09.03 val PER: 0.2565
2026-01-09 11:26:10,164: t15.2023.09.24 val PER: 0.1990
2026-01-09 11:26:10,165: t15.2023.09.29 val PER: 0.2042
2026-01-09 11:26:10,165: t15.2023.10.01 val PER: 0.2616
2026-01-09 11:26:10,165: t15.2023.10.06 val PER: 0.1604
2026-01-09 11:26:10,165: t15.2023.10.08 val PER: 0.3288
2026-01-09 11:26:10,165: t15.2023.10.13 val PER: 0.3150
2026-01-09 11:26:10,165: t15.2023.10.15 val PER: 0.2472
2026-01-09 11:26:10,165: t15.2023.10.20 val PER: 0.2315
2026-01-09 11:26:10,165: t15.2023.10.22 val PER: 0.1960
2026-01-09 11:26:10,165: t15.2023.11.03 val PER: 0.2463
2026-01-09 11:26:10,165: t15.2023.11.04 val PER: 0.0785
2026-01-09 11:26:10,165: t15.2023.11.17 val PER: 0.0995
2026-01-09 11:26:10,165: t15.2023.11.19 val PER: 0.1018
2026-01-09 11:26:10,165: t15.2023.11.26 val PER: 0.2667
2026-01-09 11:26:10,165: t15.2023.12.03 val PER: 0.2258
2026-01-09 11:26:10,165: t15.2023.12.08 val PER: 0.2264
2026-01-09 11:26:10,165: t15.2023.12.10 val PER: 0.1800
2026-01-09 11:26:10,166: t15.2023.12.17 val PER: 0.2526
2026-01-09 11:26:10,166: t15.2023.12.29 val PER: 0.2594
2026-01-09 11:26:10,166: t15.2024.02.25 val PER: 0.2219
2026-01-09 11:26:10,166: t15.2024.03.08 val PER: 0.3129
2026-01-09 11:26:10,166: t15.2024.03.15 val PER: 0.3021
2026-01-09 11:26:10,166: t15.2024.03.17 val PER: 0.2559
2026-01-09 11:26:10,166: t15.2024.05.10 val PER: 0.2793
2026-01-09 11:26:10,166: t15.2024.06.14 val PER: 0.2823
2026-01-09 11:26:10,167: t15.2024.07.19 val PER: 0.3856
2026-01-09 11:26:10,167: t15.2024.07.21 val PER: 0.2007
2026-01-09 11:26:10,167: t15.2024.07.28 val PER: 0.2515
2026-01-09 11:26:10,167: t15.2025.01.10 val PER: 0.4435
2026-01-09 11:26:10,167: t15.2025.01.12 val PER: 0.2871
2026-01-09 11:26:10,167: t15.2025.03.14 val PER: 0.4231
2026-01-09 11:26:10,167: t15.2025.03.16 val PER: 0.3010
2026-01-09 11:26:10,167: t15.2025.03.30 val PER: 0.4425
2026-01-09 11:26:10,167: t15.2025.04.13 val PER: 0.3367
2026-01-09 11:26:10,301: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_5000
2026-01-09 11:26:27,465: Train batch 5200: loss: 20.81 grad norm: 71.26 time: 0.052
2026-01-09 11:26:44,778: Train batch 5400: loss: 20.91 grad norm: 63.67 time: 0.069
2026-01-09 11:26:53,633: Running test after training batch: 5500
2026-01-09 11:26:53,790: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:26:58,714: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-09 11:26:58,765: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:27:11,736: Val batch 5500: PER (avg): 0.2430 CTC Loss (avg): 23.6460 WER(5gram): 22.56% (n=256) time: 18.102
2026-01-09 11:27:11,737: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-09 11:27:11,737: t15.2023.08.13 val PER: 0.2110
2026-01-09 11:27:11,737: t15.2023.08.18 val PER: 0.1903
2026-01-09 11:27:11,737: t15.2023.08.20 val PER: 0.2017
2026-01-09 11:27:11,737: t15.2023.08.25 val PER: 0.1461
2026-01-09 11:27:11,737: t15.2023.08.27 val PER: 0.2830
2026-01-09 11:27:11,737: t15.2023.09.01 val PER: 0.1542
2026-01-09 11:27:11,737: t15.2023.09.03 val PER: 0.2399
2026-01-09 11:27:11,737: t15.2023.09.24 val PER: 0.1917
2026-01-09 11:27:11,737: t15.2023.09.29 val PER: 0.1978
2026-01-09 11:27:11,737: t15.2023.10.01 val PER: 0.2589
2026-01-09 11:27:11,737: t15.2023.10.06 val PER: 0.1572
2026-01-09 11:27:11,738: t15.2023.10.08 val PER: 0.3153
2026-01-09 11:27:11,738: t15.2023.10.13 val PER: 0.3010
2026-01-09 11:27:11,738: t15.2023.10.15 val PER: 0.2268
2026-01-09 11:27:11,738: t15.2023.10.20 val PER: 0.2416
2026-01-09 11:27:11,738: t15.2023.10.22 val PER: 0.1949
2026-01-09 11:27:11,738: t15.2023.11.03 val PER: 0.2374
2026-01-09 11:27:11,738: t15.2023.11.04 val PER: 0.0819
2026-01-09 11:27:11,738: t15.2023.11.17 val PER: 0.0995
2026-01-09 11:27:11,738: t15.2023.11.19 val PER: 0.0978
2026-01-09 11:27:11,738: t15.2023.11.26 val PER: 0.2630
2026-01-09 11:27:11,738: t15.2023.12.03 val PER: 0.2206
2026-01-09 11:27:11,738: t15.2023.12.08 val PER: 0.2177
2026-01-09 11:27:11,738: t15.2023.12.10 val PER: 0.1774
2026-01-09 11:27:11,739: t15.2023.12.17 val PER: 0.2412
2026-01-09 11:27:11,739: t15.2023.12.29 val PER: 0.2505
2026-01-09 11:27:11,739: t15.2024.02.25 val PER: 0.2008
2026-01-09 11:27:11,739: t15.2024.03.08 val PER: 0.3115
2026-01-09 11:27:11,739: t15.2024.03.15 val PER: 0.2971
2026-01-09 11:27:11,739: t15.2024.03.17 val PER: 0.2434
2026-01-09 11:27:11,739: t15.2024.05.10 val PER: 0.2645
2026-01-09 11:27:11,739: t15.2024.06.14 val PER: 0.2555
2026-01-09 11:27:11,739: t15.2024.07.19 val PER: 0.3606
2026-01-09 11:27:11,739: t15.2024.07.21 val PER: 0.1779
2026-01-09 11:27:11,739: t15.2024.07.28 val PER: 0.2434
2026-01-09 11:27:11,739: t15.2025.01.10 val PER: 0.4242
2026-01-09 11:27:11,739: t15.2025.01.12 val PER: 0.2640
2026-01-09 11:27:11,739: t15.2025.03.14 val PER: 0.3994
2026-01-09 11:27:11,740: t15.2025.03.16 val PER: 0.2853
2026-01-09 11:27:11,740: t15.2025.03.30 val PER: 0.4023
2026-01-09 11:27:11,740: t15.2025.04.13 val PER: 0.3167
2026-01-09 11:27:11,741: New best val WER(5gram) 23.40% --> 22.56%
2026-01-09 11:27:11,925: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_5500
2026-01-09 11:27:20,883: Train batch 5600: loss: 23.29 grad norm: 67.90 time: 0.063
2026-01-09 11:27:38,138: Train batch 5800: loss: 17.57 grad norm: 67.20 time: 0.083
2026-01-09 11:27:55,055: Train batch 6000: loss: 17.39 grad norm: 59.30 time: 0.050
2026-01-09 11:27:55,055: Running test after training batch: 6000
2026-01-09 11:27:55,158: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:28:00,119: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-09 11:28:00,179: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost of
2026-01-09 11:28:12,978: Val batch 6000: PER (avg): 0.2403 CTC Loss (avg): 23.5877 WER(5gram): 22.49% (n=256) time: 17.922
2026-01-09 11:28:12,978: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-09 11:28:12,979: t15.2023.08.13 val PER: 0.2162
2026-01-09 11:28:12,979: t15.2023.08.18 val PER: 0.1861
2026-01-09 11:28:12,979: t15.2023.08.20 val PER: 0.1859
2026-01-09 11:28:12,979: t15.2023.08.25 val PER: 0.1431
2026-01-09 11:28:12,979: t15.2023.08.27 val PER: 0.2685
2026-01-09 11:28:12,979: t15.2023.09.01 val PER: 0.1599
2026-01-09 11:28:12,979: t15.2023.09.03 val PER: 0.2399
2026-01-09 11:28:12,979: t15.2023.09.24 val PER: 0.1881
2026-01-09 11:28:12,980: t15.2023.09.29 val PER: 0.1908
2026-01-09 11:28:12,980: t15.2023.10.01 val PER: 0.2490
2026-01-09 11:28:12,980: t15.2023.10.06 val PER: 0.1539
2026-01-09 11:28:12,980: t15.2023.10.08 val PER: 0.3072
2026-01-09 11:28:12,980: t15.2023.10.13 val PER: 0.2956
2026-01-09 11:28:12,980: t15.2023.10.15 val PER: 0.2327
2026-01-09 11:28:12,980: t15.2023.10.20 val PER: 0.2416
2026-01-09 11:28:12,980: t15.2023.10.22 val PER: 0.1960
2026-01-09 11:28:12,980: t15.2023.11.03 val PER: 0.2490
2026-01-09 11:28:12,980: t15.2023.11.04 val PER: 0.0785
2026-01-09 11:28:12,981: t15.2023.11.17 val PER: 0.0964
2026-01-09 11:28:12,981: t15.2023.11.19 val PER: 0.0838
2026-01-09 11:28:12,981: t15.2023.11.26 val PER: 0.2536
2026-01-09 11:28:12,981: t15.2023.12.03 val PER: 0.2059
2026-01-09 11:28:12,981: t15.2023.12.08 val PER: 0.2130
2026-01-09 11:28:12,981: t15.2023.12.10 val PER: 0.1774
2026-01-09 11:28:12,981: t15.2023.12.17 val PER: 0.2328
2026-01-09 11:28:12,981: t15.2023.12.29 val PER: 0.2409
2026-01-09 11:28:12,982: t15.2024.02.25 val PER: 0.1966
2026-01-09 11:28:12,982: t15.2024.03.08 val PER: 0.3215
2026-01-09 11:28:12,982: t15.2024.03.15 val PER: 0.2908
2026-01-09 11:28:12,982: t15.2024.03.17 val PER: 0.2497
2026-01-09 11:28:12,982: t15.2024.05.10 val PER: 0.2437
2026-01-09 11:28:12,982: t15.2024.06.14 val PER: 0.2366
2026-01-09 11:28:12,982: t15.2024.07.19 val PER: 0.3527
2026-01-09 11:28:12,982: t15.2024.07.21 val PER: 0.1952
2026-01-09 11:28:12,982: t15.2024.07.28 val PER: 0.2426
2026-01-09 11:28:12,982: t15.2025.01.10 val PER: 0.4160
2026-01-09 11:28:12,983: t15.2025.01.12 val PER: 0.2656
2026-01-09 11:28:12,983: t15.2025.03.14 val PER: 0.4024
2026-01-09 11:28:12,983: t15.2025.03.16 val PER: 0.2906
2026-01-09 11:28:12,983: t15.2025.03.30 val PER: 0.4138
2026-01-09 11:28:12,983: t15.2025.04.13 val PER: 0.3110
2026-01-09 11:28:12,983: New best val WER(5gram) 22.56% --> 22.49%
2026-01-09 11:28:13,168: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_6000
2026-01-09 11:28:31,066: Train batch 6200: loss: 20.23 grad norm: 65.14 time: 0.070
2026-01-09 11:28:48,133: Train batch 6400: loss: 22.99 grad norm: 70.83 time: 0.063
2026-01-09 11:28:56,611: Running test after training batch: 6500
2026-01-09 11:28:56,746: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:29:01,667: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:29:01,722: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:29:13,739: Val batch 6500: PER (avg): 0.2289 CTC Loss (avg): 22.4650 WER(5gram): 21.12% (n=256) time: 17.127
2026-01-09 11:29:13,740: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-09 11:29:13,740: t15.2023.08.13 val PER: 0.1985
2026-01-09 11:29:13,740: t15.2023.08.18 val PER: 0.1693
2026-01-09 11:29:13,740: t15.2023.08.20 val PER: 0.1739
2026-01-09 11:29:13,740: t15.2023.08.25 val PER: 0.1295
2026-01-09 11:29:13,740: t15.2023.08.27 val PER: 0.2315
2026-01-09 11:29:13,740: t15.2023.09.01 val PER: 0.1315
2026-01-09 11:29:13,740: t15.2023.09.03 val PER: 0.2245
2026-01-09 11:29:13,740: t15.2023.09.24 val PER: 0.1808
2026-01-09 11:29:13,740: t15.2023.09.29 val PER: 0.1934
2026-01-09 11:29:13,740: t15.2023.10.01 val PER: 0.2490
2026-01-09 11:29:13,741: t15.2023.10.06 val PER: 0.1432
2026-01-09 11:29:13,741: t15.2023.10.08 val PER: 0.3031
2026-01-09 11:29:13,741: t15.2023.10.13 val PER: 0.2832
2026-01-09 11:29:13,741: t15.2023.10.15 val PER: 0.2320
2026-01-09 11:29:13,741: t15.2023.10.20 val PER: 0.2248
2026-01-09 11:29:13,741: t15.2023.10.22 val PER: 0.1815
2026-01-09 11:29:13,741: t15.2023.11.03 val PER: 0.2449
2026-01-09 11:29:13,741: t15.2023.11.04 val PER: 0.0580
2026-01-09 11:29:13,741: t15.2023.11.17 val PER: 0.0871
2026-01-09 11:29:13,741: t15.2023.11.19 val PER: 0.0878
2026-01-09 11:29:13,741: t15.2023.11.26 val PER: 0.2428
2026-01-09 11:29:13,741: t15.2023.12.03 val PER: 0.2017
2026-01-09 11:29:13,741: t15.2023.12.08 val PER: 0.1971
2026-01-09 11:29:13,742: t15.2023.12.10 val PER: 0.1603
2026-01-09 11:29:13,742: t15.2023.12.17 val PER: 0.2173
2026-01-09 11:29:13,742: t15.2023.12.29 val PER: 0.2423
2026-01-09 11:29:13,742: t15.2024.02.25 val PER: 0.1938
2026-01-09 11:29:13,742: t15.2024.03.08 val PER: 0.3129
2026-01-09 11:29:13,742: t15.2024.03.15 val PER: 0.2846
2026-01-09 11:29:13,742: t15.2024.03.17 val PER: 0.2301
2026-01-09 11:29:13,742: t15.2024.05.10 val PER: 0.2407
2026-01-09 11:29:13,742: t15.2024.06.14 val PER: 0.2492
2026-01-09 11:29:13,742: t15.2024.07.19 val PER: 0.3401
2026-01-09 11:29:13,742: t15.2024.07.21 val PER: 0.1834
2026-01-09 11:29:13,742: t15.2024.07.28 val PER: 0.2199
2026-01-09 11:29:13,742: t15.2025.01.10 val PER: 0.4008
2026-01-09 11:29:13,743: t15.2025.01.12 val PER: 0.2487
2026-01-09 11:29:13,743: t15.2025.03.14 val PER: 0.3935
2026-01-09 11:29:13,743: t15.2025.03.16 val PER: 0.2775
2026-01-09 11:29:13,743: t15.2025.03.30 val PER: 0.3667
2026-01-09 11:29:13,743: t15.2025.04.13 val PER: 0.2981
2026-01-09 11:29:13,744: New best val WER(5gram) 22.49% --> 21.12%
2026-01-09 11:29:13,927: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_6500
2026-01-09 11:29:22,261: Train batch 6600: loss: 14.74 grad norm: 58.34 time: 0.046
2026-01-09 11:29:39,642: Train batch 6800: loss: 19.37 grad norm: 63.24 time: 0.050
2026-01-09 11:29:57,099: Train batch 7000: loss: 21.85 grad norm: 68.37 time: 0.062
2026-01-09 11:29:57,099: Running test after training batch: 7000
2026-01-09 11:29:57,250: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:30:02,173: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-09 11:30:02,233: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost of
2026-01-09 11:30:14,119: Val batch 7000: PER (avg): 0.2197 CTC Loss (avg): 21.6725 WER(5gram): 18.90% (n=256) time: 17.019
2026-01-09 11:30:14,119: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-09 11:30:14,120: t15.2023.08.13 val PER: 0.1809
2026-01-09 11:30:14,120: t15.2023.08.18 val PER: 0.1618
2026-01-09 11:30:14,120: t15.2023.08.20 val PER: 0.1771
2026-01-09 11:30:14,120: t15.2023.08.25 val PER: 0.1190
2026-01-09 11:30:14,120: t15.2023.08.27 val PER: 0.2347
2026-01-09 11:30:14,120: t15.2023.09.01 val PER: 0.1331
2026-01-09 11:30:14,120: t15.2023.09.03 val PER: 0.2150
2026-01-09 11:30:14,120: t15.2023.09.24 val PER: 0.1699
2026-01-09 11:30:14,120: t15.2023.09.29 val PER: 0.1895
2026-01-09 11:30:14,120: t15.2023.10.01 val PER: 0.2371
2026-01-09 11:30:14,120: t15.2023.10.06 val PER: 0.1346
2026-01-09 11:30:14,120: t15.2023.10.08 val PER: 0.2909
2026-01-09 11:30:14,120: t15.2023.10.13 val PER: 0.2847
2026-01-09 11:30:14,120: t15.2023.10.15 val PER: 0.2156
2026-01-09 11:30:14,120: t15.2023.10.20 val PER: 0.2215
2026-01-09 11:30:14,121: t15.2023.10.22 val PER: 0.1659
2026-01-09 11:30:14,121: t15.2023.11.03 val PER: 0.2171
2026-01-09 11:30:14,121: t15.2023.11.04 val PER: 0.0478
2026-01-09 11:30:14,121: t15.2023.11.17 val PER: 0.0778
2026-01-09 11:30:14,121: t15.2023.11.19 val PER: 0.0778
2026-01-09 11:30:14,121: t15.2023.11.26 val PER: 0.2268
2026-01-09 11:30:14,121: t15.2023.12.03 val PER: 0.1964
2026-01-09 11:30:14,121: t15.2023.12.08 val PER: 0.1838
2026-01-09 11:30:14,121: t15.2023.12.10 val PER: 0.1577
2026-01-09 11:30:14,121: t15.2023.12.17 val PER: 0.2121
2026-01-09 11:30:14,121: t15.2023.12.29 val PER: 0.2286
2026-01-09 11:30:14,121: t15.2024.02.25 val PER: 0.1756
2026-01-09 11:30:14,121: t15.2024.03.08 val PER: 0.2973
2026-01-09 11:30:14,121: t15.2024.03.15 val PER: 0.2658
2026-01-09 11:30:14,121: t15.2024.03.17 val PER: 0.2245
2026-01-09 11:30:14,122: t15.2024.05.10 val PER: 0.2556
2026-01-09 11:30:14,122: t15.2024.06.14 val PER: 0.2492
2026-01-09 11:30:14,122: t15.2024.07.19 val PER: 0.3283
2026-01-09 11:30:14,122: t15.2024.07.21 val PER: 0.1697
2026-01-09 11:30:14,122: t15.2024.07.28 val PER: 0.2029
2026-01-09 11:30:14,122: t15.2025.01.10 val PER: 0.3939
2026-01-09 11:30:14,122: t15.2025.01.12 val PER: 0.2425
2026-01-09 11:30:14,122: t15.2025.03.14 val PER: 0.3817
2026-01-09 11:30:14,122: t15.2025.03.16 val PER: 0.2683
2026-01-09 11:30:14,122: t15.2025.03.30 val PER: 0.3828
2026-01-09 11:30:14,122: t15.2025.04.13 val PER: 0.2924
2026-01-09 11:30:14,124: New best val WER(5gram) 21.12% --> 18.90%
2026-01-09 11:30:14,309: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_7000
2026-01-09 11:30:31,582: Train batch 7200: loss: 17.60 grad norm: 60.51 time: 0.079
2026-01-09 11:30:48,622: Train batch 7400: loss: 17.43 grad norm: 59.12 time: 0.076
2026-01-09 11:30:57,128: Running test after training batch: 7500
2026-01-09 11:30:57,301: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:31:02,266: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-09 11:31:02,323: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:31:13,863: Val batch 7500: PER (avg): 0.2155 CTC Loss (avg): 21.0664 WER(5gram): 21.77% (n=256) time: 16.735
2026-01-09 11:31:13,864: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-09 11:31:13,864: t15.2023.08.13 val PER: 0.1757
2026-01-09 11:31:13,864: t15.2023.08.18 val PER: 0.1593
2026-01-09 11:31:13,864: t15.2023.08.20 val PER: 0.1612
2026-01-09 11:31:13,864: t15.2023.08.25 val PER: 0.1205
2026-01-09 11:31:13,864: t15.2023.08.27 val PER: 0.2331
2026-01-09 11:31:13,864: t15.2023.09.01 val PER: 0.1307
2026-01-09 11:31:13,864: t15.2023.09.03 val PER: 0.2138
2026-01-09 11:31:13,864: t15.2023.09.24 val PER: 0.1772
2026-01-09 11:31:13,864: t15.2023.09.29 val PER: 0.1755
2026-01-09 11:31:13,865: t15.2023.10.01 val PER: 0.2477
2026-01-09 11:31:13,865: t15.2023.10.06 val PER: 0.1367
2026-01-09 11:31:13,865: t15.2023.10.08 val PER: 0.2842
2026-01-09 11:31:13,865: t15.2023.10.13 val PER: 0.2770
2026-01-09 11:31:13,865: t15.2023.10.15 val PER: 0.2123
2026-01-09 11:31:13,865: t15.2023.10.20 val PER: 0.2181
2026-01-09 11:31:13,865: t15.2023.10.22 val PER: 0.1659
2026-01-09 11:31:13,865: t15.2023.11.03 val PER: 0.2198
2026-01-09 11:31:13,865: t15.2023.11.04 val PER: 0.0580
2026-01-09 11:31:13,865: t15.2023.11.17 val PER: 0.0809
2026-01-09 11:31:13,865: t15.2023.11.19 val PER: 0.0659
2026-01-09 11:31:13,865: t15.2023.11.26 val PER: 0.2246
2026-01-09 11:31:13,866: t15.2023.12.03 val PER: 0.1744
2026-01-09 11:31:13,866: t15.2023.12.08 val PER: 0.1824
2026-01-09 11:31:13,866: t15.2023.12.10 val PER: 0.1498
2026-01-09 11:31:13,866: t15.2023.12.17 val PER: 0.2110
2026-01-09 11:31:13,866: t15.2023.12.29 val PER: 0.2141
2026-01-09 11:31:13,866: t15.2024.02.25 val PER: 0.1770
2026-01-09 11:31:13,866: t15.2024.03.08 val PER: 0.2959
2026-01-09 11:31:13,866: t15.2024.03.15 val PER: 0.2689
2026-01-09 11:31:13,867: t15.2024.03.17 val PER: 0.2225
2026-01-09 11:31:13,867: t15.2024.05.10 val PER: 0.2422
2026-01-09 11:31:13,867: t15.2024.06.14 val PER: 0.2508
2026-01-09 11:31:13,867: t15.2024.07.19 val PER: 0.3204
2026-01-09 11:31:13,867: t15.2024.07.21 val PER: 0.1559
2026-01-09 11:31:13,867: t15.2024.07.28 val PER: 0.1978
2026-01-09 11:31:13,867: t15.2025.01.10 val PER: 0.3898
2026-01-09 11:31:13,867: t15.2025.01.12 val PER: 0.2225
2026-01-09 11:31:13,867: t15.2025.03.14 val PER: 0.3802
2026-01-09 11:31:13,867: t15.2025.03.16 val PER: 0.2906
2026-01-09 11:31:13,867: t15.2025.03.30 val PER: 0.3678
2026-01-09 11:31:13,867: t15.2025.04.13 val PER: 0.2924
2026-01-09 11:31:14,007: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_7500
2026-01-09 11:31:22,536: Train batch 7600: loss: 19.49 grad norm: 64.93 time: 0.069
2026-01-09 11:31:39,644: Train batch 7800: loss: 18.46 grad norm: 65.14 time: 0.057
2026-01-09 11:31:57,220: Train batch 8000: loss: 14.54 grad norm: 56.81 time: 0.072
2026-01-09 11:31:57,220: Running test after training batch: 8000
2026-01-09 11:31:57,319: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:32:02,201: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-09 11:32:02,260: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:32:13,925: Val batch 8000: PER (avg): 0.2091 CTC Loss (avg): 20.4516 WER(5gram): 21.90% (n=256) time: 16.705
2026-01-09 11:32:13,926: WER lens: avg_true_words=5.99 avg_pred_words=6.24 max_pred_words=12
2026-01-09 11:32:13,926: t15.2023.08.13 val PER: 0.1757
2026-01-09 11:32:13,926: t15.2023.08.18 val PER: 0.1618
2026-01-09 11:32:13,926: t15.2023.08.20 val PER: 0.1636
2026-01-09 11:32:13,926: t15.2023.08.25 val PER: 0.1235
2026-01-09 11:32:13,926: t15.2023.08.27 val PER: 0.2444
2026-01-09 11:32:13,926: t15.2023.09.01 val PER: 0.1226
2026-01-09 11:32:13,926: t15.2023.09.03 val PER: 0.2114
2026-01-09 11:32:13,927: t15.2023.09.24 val PER: 0.1699
2026-01-09 11:32:13,927: t15.2023.09.29 val PER: 0.1678
2026-01-09 11:32:13,927: t15.2023.10.01 val PER: 0.2285
2026-01-09 11:32:13,927: t15.2023.10.06 val PER: 0.1324
2026-01-09 11:32:13,927: t15.2023.10.08 val PER: 0.2923
2026-01-09 11:32:13,927: t15.2023.10.13 val PER: 0.2700
2026-01-09 11:32:13,927: t15.2023.10.15 val PER: 0.2109
2026-01-09 11:32:13,927: t15.2023.10.20 val PER: 0.2248
2026-01-09 11:32:13,927: t15.2023.10.22 val PER: 0.1670
2026-01-09 11:32:13,928: t15.2023.11.03 val PER: 0.2273
2026-01-09 11:32:13,928: t15.2023.11.04 val PER: 0.0444
2026-01-09 11:32:13,928: t15.2023.11.17 val PER: 0.0731
2026-01-09 11:32:13,928: t15.2023.11.19 val PER: 0.0659
2026-01-09 11:32:13,928: t15.2023.11.26 val PER: 0.2116
2026-01-09 11:32:13,928: t15.2023.12.03 val PER: 0.1849
2026-01-09 11:32:13,928: t15.2023.12.08 val PER: 0.1844
2026-01-09 11:32:13,928: t15.2023.12.10 val PER: 0.1459
2026-01-09 11:32:13,928: t15.2023.12.17 val PER: 0.2069
2026-01-09 11:32:13,928: t15.2023.12.29 val PER: 0.2011
2026-01-09 11:32:13,928: t15.2024.02.25 val PER: 0.1699
2026-01-09 11:32:13,928: t15.2024.03.08 val PER: 0.2888
2026-01-09 11:32:13,928: t15.2024.03.15 val PER: 0.2558
2026-01-09 11:32:13,928: t15.2024.03.17 val PER: 0.2036
2026-01-09 11:32:13,928: t15.2024.05.10 val PER: 0.2140
2026-01-09 11:32:13,928: t15.2024.06.14 val PER: 0.2208
2026-01-09 11:32:13,928: t15.2024.07.19 val PER: 0.3138
2026-01-09 11:32:13,929: t15.2024.07.21 val PER: 0.1386
2026-01-09 11:32:13,929: t15.2024.07.28 val PER: 0.1875
2026-01-09 11:32:13,929: t15.2025.01.10 val PER: 0.3719
2026-01-09 11:32:13,929: t15.2025.01.12 val PER: 0.2202
2026-01-09 11:32:13,929: t15.2025.03.14 val PER: 0.3787
2026-01-09 11:32:13,929: t15.2025.03.16 val PER: 0.2696
2026-01-09 11:32:13,929: t15.2025.03.30 val PER: 0.3552
2026-01-09 11:32:13,929: t15.2025.04.13 val PER: 0.2924
2026-01-09 11:32:14,066: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_8000
2026-01-09 11:32:31,072: Train batch 8200: loss: 12.01 grad norm: 56.58 time: 0.055
2026-01-09 11:32:48,680: Train batch 8400: loss: 13.06 grad norm: 53.13 time: 0.065
2026-01-09 11:32:57,321: Running test after training batch: 8500
2026-01-09 11:32:57,424: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:33:02,307: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-09 11:33:02,365: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost net
2026-01-09 11:33:13,685: Val batch 8500: PER (avg): 0.2038 CTC Loss (avg): 20.1167 WER(5gram): 18.90% (n=256) time: 16.364
2026-01-09 11:33:13,685: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=11
2026-01-09 11:33:13,686: t15.2023.08.13 val PER: 0.1663
2026-01-09 11:33:13,686: t15.2023.08.18 val PER: 0.1559
2026-01-09 11:33:13,686: t15.2023.08.20 val PER: 0.1668
2026-01-09 11:33:13,686: t15.2023.08.25 val PER: 0.1220
2026-01-09 11:33:13,686: t15.2023.08.27 val PER: 0.2347
2026-01-09 11:33:13,686: t15.2023.09.01 val PER: 0.1161
2026-01-09 11:33:13,686: t15.2023.09.03 val PER: 0.2150
2026-01-09 11:33:13,686: t15.2023.09.24 val PER: 0.1650
2026-01-09 11:33:13,686: t15.2023.09.29 val PER: 0.1685
2026-01-09 11:33:13,687: t15.2023.10.01 val PER: 0.2186
2026-01-09 11:33:13,687: t15.2023.10.06 val PER: 0.1173
2026-01-09 11:33:13,687: t15.2023.10.08 val PER: 0.2882
2026-01-09 11:33:13,687: t15.2023.10.13 val PER: 0.2614
2026-01-09 11:33:13,687: t15.2023.10.15 val PER: 0.1951
2026-01-09 11:33:13,687: t15.2023.10.20 val PER: 0.2148
2026-01-09 11:33:13,687: t15.2023.10.22 val PER: 0.1459
2026-01-09 11:33:13,687: t15.2023.11.03 val PER: 0.2123
2026-01-09 11:33:13,687: t15.2023.11.04 val PER: 0.0580
2026-01-09 11:33:13,687: t15.2023.11.17 val PER: 0.0700
2026-01-09 11:33:13,687: t15.2023.11.19 val PER: 0.0639
2026-01-09 11:33:13,687: t15.2023.11.26 val PER: 0.2109
2026-01-09 11:33:13,687: t15.2023.12.03 val PER: 0.1754
2026-01-09 11:33:13,687: t15.2023.12.08 val PER: 0.1791
2026-01-09 11:33:13,687: t15.2023.12.10 val PER: 0.1393
2026-01-09 11:33:13,688: t15.2023.12.17 val PER: 0.1861
2026-01-09 11:33:13,688: t15.2023.12.29 val PER: 0.2004
2026-01-09 11:33:13,688: t15.2024.02.25 val PER: 0.1699
2026-01-09 11:33:13,688: t15.2024.03.08 val PER: 0.2760
2026-01-09 11:33:13,688: t15.2024.03.15 val PER: 0.2502
2026-01-09 11:33:13,688: t15.2024.03.17 val PER: 0.2008
2026-01-09 11:33:13,688: t15.2024.05.10 val PER: 0.2273
2026-01-09 11:33:13,688: t15.2024.06.14 val PER: 0.2208
2026-01-09 11:33:13,688: t15.2024.07.19 val PER: 0.3131
2026-01-09 11:33:13,688: t15.2024.07.21 val PER: 0.1448
2026-01-09 11:33:13,688: t15.2024.07.28 val PER: 0.1963
2026-01-09 11:33:13,689: t15.2025.01.10 val PER: 0.3705
2026-01-09 11:33:13,689: t15.2025.01.12 val PER: 0.2256
2026-01-09 11:33:13,689: t15.2025.03.14 val PER: 0.3654
2026-01-09 11:33:13,689: t15.2025.03.16 val PER: 0.2343
2026-01-09 11:33:13,689: t15.2025.03.30 val PER: 0.3621
2026-01-09 11:33:13,689: t15.2025.04.13 val PER: 0.2653
2026-01-09 11:33:13,822: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_8500
2026-01-09 11:33:22,362: Train batch 8600: loss: 19.15 grad norm: 68.04 time: 0.055
2026-01-09 11:33:39,377: Train batch 8800: loss: 20.53 grad norm: 64.91 time: 0.062
2026-01-09 11:33:56,728: Train batch 9000: loss: 20.22 grad norm: 67.14 time: 0.073
2026-01-09 11:33:56,728: Running test after training batch: 9000
2026-01-09 11:33:56,843: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:34:01,747: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:34:01,803: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost net
2026-01-09 11:34:13,369: Val batch 9000: PER (avg): 0.2005 CTC Loss (avg): 19.6234 WER(5gram): 20.27% (n=256) time: 16.641
2026-01-09 11:34:13,370: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=12
2026-01-09 11:34:13,370: t15.2023.08.13 val PER: 0.1757
2026-01-09 11:34:13,370: t15.2023.08.18 val PER: 0.1509
2026-01-09 11:34:13,370: t15.2023.08.20 val PER: 0.1549
2026-01-09 11:34:13,370: t15.2023.08.25 val PER: 0.1084
2026-01-09 11:34:13,370: t15.2023.08.27 val PER: 0.2347
2026-01-09 11:34:13,370: t15.2023.09.01 val PER: 0.1063
2026-01-09 11:34:13,371: t15.2023.09.03 val PER: 0.2090
2026-01-09 11:34:13,371: t15.2023.09.24 val PER: 0.1590
2026-01-09 11:34:13,371: t15.2023.09.29 val PER: 0.1678
2026-01-09 11:34:13,371: t15.2023.10.01 val PER: 0.2232
2026-01-09 11:34:13,371: t15.2023.10.06 val PER: 0.1216
2026-01-09 11:34:13,371: t15.2023.10.08 val PER: 0.2950
2026-01-09 11:34:13,371: t15.2023.10.13 val PER: 0.2583
2026-01-09 11:34:13,371: t15.2023.10.15 val PER: 0.1991
2026-01-09 11:34:13,371: t15.2023.10.20 val PER: 0.2148
2026-01-09 11:34:13,371: t15.2023.10.22 val PER: 0.1526
2026-01-09 11:34:13,371: t15.2023.11.03 val PER: 0.2137
2026-01-09 11:34:13,371: t15.2023.11.04 val PER: 0.0580
2026-01-09 11:34:13,371: t15.2023.11.17 val PER: 0.0669
2026-01-09 11:34:13,371: t15.2023.11.19 val PER: 0.0639
2026-01-09 11:34:13,371: t15.2023.11.26 val PER: 0.2043
2026-01-09 11:34:13,371: t15.2023.12.03 val PER: 0.1775
2026-01-09 11:34:13,371: t15.2023.12.08 val PER: 0.1611
2026-01-09 11:34:13,372: t15.2023.12.10 val PER: 0.1288
2026-01-09 11:34:13,372: t15.2023.12.17 val PER: 0.1975
2026-01-09 11:34:13,372: t15.2023.12.29 val PER: 0.1901
2026-01-09 11:34:13,372: t15.2024.02.25 val PER: 0.1573
2026-01-09 11:34:13,372: t15.2024.03.08 val PER: 0.2788
2026-01-09 11:34:13,372: t15.2024.03.15 val PER: 0.2527
2026-01-09 11:34:13,372: t15.2024.03.17 val PER: 0.2071
2026-01-09 11:34:13,372: t15.2024.05.10 val PER: 0.2080
2026-01-09 11:34:13,372: t15.2024.06.14 val PER: 0.2145
2026-01-09 11:34:13,372: t15.2024.07.19 val PER: 0.3105
2026-01-09 11:34:13,372: t15.2024.07.21 val PER: 0.1407
2026-01-09 11:34:13,372: t15.2024.07.28 val PER: 0.1846
2026-01-09 11:34:13,373: t15.2025.01.10 val PER: 0.3526
2026-01-09 11:34:13,373: t15.2025.01.12 val PER: 0.2102
2026-01-09 11:34:13,373: t15.2025.03.14 val PER: 0.3728
2026-01-09 11:34:13,373: t15.2025.03.16 val PER: 0.2408
2026-01-09 11:34:13,373: t15.2025.03.30 val PER: 0.3460
2026-01-09 11:34:13,373: t15.2025.04.13 val PER: 0.2710
2026-01-09 11:34:13,507: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_9000
2026-01-09 11:34:30,601: Train batch 9200: loss: 14.06 grad norm: 55.15 time: 0.057
2026-01-09 11:34:47,765: Train batch 9400: loss: 10.44 grad norm: 51.23 time: 0.072
2026-01-09 11:34:56,335: Running test after training batch: 9500
2026-01-09 11:34:56,475: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:35:01,331: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:35:01,377: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:35:12,718: Val batch 9500: PER (avg): 0.1961 CTC Loss (avg): 19.3269 WER(5gram): 18.84% (n=256) time: 16.384
2026-01-09 11:35:12,719: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-09 11:35:12,719: t15.2023.08.13 val PER: 0.1622
2026-01-09 11:35:12,719: t15.2023.08.18 val PER: 0.1366
2026-01-09 11:35:12,719: t15.2023.08.20 val PER: 0.1565
2026-01-09 11:35:12,719: t15.2023.08.25 val PER: 0.1190
2026-01-09 11:35:12,719: t15.2023.08.27 val PER: 0.2331
2026-01-09 11:35:12,719: t15.2023.09.01 val PER: 0.1023
2026-01-09 11:35:12,720: t15.2023.09.03 val PER: 0.1924
2026-01-09 11:35:12,720: t15.2023.09.24 val PER: 0.1481
2026-01-09 11:35:12,720: t15.2023.09.29 val PER: 0.1672
2026-01-09 11:35:12,720: t15.2023.10.01 val PER: 0.2034
2026-01-09 11:35:12,720: t15.2023.10.06 val PER: 0.1184
2026-01-09 11:35:12,720: t15.2023.10.08 val PER: 0.2855
2026-01-09 11:35:12,720: t15.2023.10.13 val PER: 0.2560
2026-01-09 11:35:12,720: t15.2023.10.15 val PER: 0.1984
2026-01-09 11:35:12,720: t15.2023.10.20 val PER: 0.1980
2026-01-09 11:35:12,720: t15.2023.10.22 val PER: 0.1437
2026-01-09 11:35:12,720: t15.2023.11.03 val PER: 0.2144
2026-01-09 11:35:12,720: t15.2023.11.04 val PER: 0.0478
2026-01-09 11:35:12,720: t15.2023.11.17 val PER: 0.0715
2026-01-09 11:35:12,720: t15.2023.11.19 val PER: 0.0699
2026-01-09 11:35:12,720: t15.2023.11.26 val PER: 0.1891
2026-01-09 11:35:12,720: t15.2023.12.03 val PER: 0.1628
2026-01-09 11:35:12,720: t15.2023.12.08 val PER: 0.1671
2026-01-09 11:35:12,721: t15.2023.12.10 val PER: 0.1222
2026-01-09 11:35:12,721: t15.2023.12.17 val PER: 0.1892
2026-01-09 11:35:12,721: t15.2023.12.29 val PER: 0.1881
2026-01-09 11:35:12,721: t15.2024.02.25 val PER: 0.1517
2026-01-09 11:35:12,721: t15.2024.03.08 val PER: 0.2575
2026-01-09 11:35:12,721: t15.2024.03.15 val PER: 0.2489
2026-01-09 11:35:12,721: t15.2024.03.17 val PER: 0.1925
2026-01-09 11:35:12,721: t15.2024.05.10 val PER: 0.2110
2026-01-09 11:35:12,721: t15.2024.06.14 val PER: 0.2161
2026-01-09 11:35:12,721: t15.2024.07.19 val PER: 0.2966
2026-01-09 11:35:12,721: t15.2024.07.21 val PER: 0.1428
2026-01-09 11:35:12,721: t15.2024.07.28 val PER: 0.1912
2026-01-09 11:35:12,721: t15.2025.01.10 val PER: 0.3636
2026-01-09 11:35:12,722: t15.2025.01.12 val PER: 0.2163
2026-01-09 11:35:12,722: t15.2025.03.14 val PER: 0.3817
2026-01-09 11:35:12,722: t15.2025.03.16 val PER: 0.2448
2026-01-09 11:35:12,722: t15.2025.03.30 val PER: 0.3437
2026-01-09 11:35:12,722: t15.2025.04.13 val PER: 0.2625
2026-01-09 11:35:12,723: New best val WER(5gram) 18.90% --> 18.84%
2026-01-09 11:35:12,901: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_9500
2026-01-09 11:35:21,400: Train batch 9600: loss: 10.75 grad norm: 48.20 time: 0.074
2026-01-09 11:35:38,658: Train batch 9800: loss: 16.64 grad norm: 64.17 time: 0.064
2026-01-09 11:35:56,477: Train batch 10000: loss: 7.94 grad norm: 46.46 time: 0.063
2026-01-09 11:35:56,477: Running test after training batch: 10000
2026-01-09 11:35:56,607: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:36:01,502: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:36:01,549: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-09 11:36:12,515: Val batch 10000: PER (avg): 0.1923 CTC Loss (avg): 18.9182 WER(5gram): 18.38% (n=256) time: 16.037
2026-01-09 11:36:12,515: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-09 11:36:12,515: t15.2023.08.13 val PER: 0.1528
2026-01-09 11:36:12,515: t15.2023.08.18 val PER: 0.1459
2026-01-09 11:36:12,515: t15.2023.08.20 val PER: 0.1485
2026-01-09 11:36:12,515: t15.2023.08.25 val PER: 0.1130
2026-01-09 11:36:12,515: t15.2023.08.27 val PER: 0.2283
2026-01-09 11:36:12,516: t15.2023.09.01 val PER: 0.1039
2026-01-09 11:36:12,516: t15.2023.09.03 val PER: 0.1924
2026-01-09 11:36:12,516: t15.2023.09.24 val PER: 0.1553
2026-01-09 11:36:12,516: t15.2023.09.29 val PER: 0.1672
2026-01-09 11:36:12,516: t15.2023.10.01 val PER: 0.2081
2026-01-09 11:36:12,516: t15.2023.10.06 val PER: 0.1119
2026-01-09 11:36:12,516: t15.2023.10.08 val PER: 0.2788
2026-01-09 11:36:12,516: t15.2023.10.13 val PER: 0.2475
2026-01-09 11:36:12,516: t15.2023.10.15 val PER: 0.1997
2026-01-09 11:36:12,516: t15.2023.10.20 val PER: 0.1980
2026-01-09 11:36:12,516: t15.2023.10.22 val PER: 0.1403
2026-01-09 11:36:12,516: t15.2023.11.03 val PER: 0.2151
2026-01-09 11:36:12,517: t15.2023.11.04 val PER: 0.0444
2026-01-09 11:36:12,517: t15.2023.11.17 val PER: 0.0529
2026-01-09 11:36:12,517: t15.2023.11.19 val PER: 0.0619
2026-01-09 11:36:12,517: t15.2023.11.26 val PER: 0.1877
2026-01-09 11:36:12,517: t15.2023.12.03 val PER: 0.1649
2026-01-09 11:36:12,517: t15.2023.12.08 val PER: 0.1644
2026-01-09 11:36:12,517: t15.2023.12.10 val PER: 0.1288
2026-01-09 11:36:12,517: t15.2023.12.17 val PER: 0.1902
2026-01-09 11:36:12,517: t15.2023.12.29 val PER: 0.1901
2026-01-09 11:36:12,517: t15.2024.02.25 val PER: 0.1559
2026-01-09 11:36:12,517: t15.2024.03.08 val PER: 0.2560
2026-01-09 11:36:12,517: t15.2024.03.15 val PER: 0.2458
2026-01-09 11:36:12,517: t15.2024.03.17 val PER: 0.1820
2026-01-09 11:36:12,517: t15.2024.05.10 val PER: 0.1976
2026-01-09 11:36:12,517: t15.2024.06.14 val PER: 0.2161
2026-01-09 11:36:12,517: t15.2024.07.19 val PER: 0.2940
2026-01-09 11:36:12,517: t15.2024.07.21 val PER: 0.1421
2026-01-09 11:36:12,518: t15.2024.07.28 val PER: 0.1706
2026-01-09 11:36:12,518: t15.2025.01.10 val PER: 0.3444
2026-01-09 11:36:12,518: t15.2025.01.12 val PER: 0.2002
2026-01-09 11:36:12,518: t15.2025.03.14 val PER: 0.3476
2026-01-09 11:36:12,518: t15.2025.03.16 val PER: 0.2539
2026-01-09 11:36:12,518: t15.2025.03.30 val PER: 0.3310
2026-01-09 11:36:12,518: t15.2025.04.13 val PER: 0.2582
2026-01-09 11:36:12,519: New best val WER(5gram) 18.84% --> 18.38%
2026-01-09 11:36:12,705: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_10000
2026-01-09 11:36:29,619: Train batch 10200: loss: 8.66 grad norm: 43.00 time: 0.051
2026-01-09 11:36:46,953: Train batch 10400: loss: 12.86 grad norm: 56.05 time: 0.078
2026-01-09 11:36:56,229: Running test after training batch: 10500
2026-01-09 11:36:56,350: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:37:01,178: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:37:01,221: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:37:12,426: Val batch 10500: PER (avg): 0.1889 CTC Loss (avg): 18.6745 WER(5gram): 17.60% (n=256) time: 16.197
2026-01-09 11:37:12,427: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-09 11:37:12,427: t15.2023.08.13 val PER: 0.1528
2026-01-09 11:37:12,427: t15.2023.08.18 val PER: 0.1391
2026-01-09 11:37:12,427: t15.2023.08.20 val PER: 0.1469
2026-01-09 11:37:12,427: t15.2023.08.25 val PER: 0.1099
2026-01-09 11:37:12,427: t15.2023.08.27 val PER: 0.2219
2026-01-09 11:37:12,427: t15.2023.09.01 val PER: 0.1161
2026-01-09 11:37:12,428: t15.2023.09.03 val PER: 0.1924
2026-01-09 11:37:12,428: t15.2023.09.24 val PER: 0.1468
2026-01-09 11:37:12,428: t15.2023.09.29 val PER: 0.1576
2026-01-09 11:37:12,428: t15.2023.10.01 val PER: 0.2034
2026-01-09 11:37:12,428: t15.2023.10.06 val PER: 0.1119
2026-01-09 11:37:12,428: t15.2023.10.08 val PER: 0.2760
2026-01-09 11:37:12,428: t15.2023.10.13 val PER: 0.2444
2026-01-09 11:37:12,428: t15.2023.10.15 val PER: 0.1931
2026-01-09 11:37:12,428: t15.2023.10.20 val PER: 0.1980
2026-01-09 11:37:12,428: t15.2023.10.22 val PER: 0.1381
2026-01-09 11:37:12,428: t15.2023.11.03 val PER: 0.2096
2026-01-09 11:37:12,428: t15.2023.11.04 val PER: 0.0512
2026-01-09 11:37:12,428: t15.2023.11.17 val PER: 0.0669
2026-01-09 11:37:12,429: t15.2023.11.19 val PER: 0.0679
2026-01-09 11:37:12,429: t15.2023.11.26 val PER: 0.1812
2026-01-09 11:37:12,429: t15.2023.12.03 val PER: 0.1691
2026-01-09 11:37:12,429: t15.2023.12.08 val PER: 0.1525
2026-01-09 11:37:12,429: t15.2023.12.10 val PER: 0.1275
2026-01-09 11:37:12,429: t15.2023.12.17 val PER: 0.1674
2026-01-09 11:37:12,429: t15.2023.12.29 val PER: 0.1881
2026-01-09 11:37:12,429: t15.2024.02.25 val PER: 0.1615
2026-01-09 11:37:12,429: t15.2024.03.08 val PER: 0.2589
2026-01-09 11:37:12,429: t15.2024.03.15 val PER: 0.2458
2026-01-09 11:37:12,429: t15.2024.03.17 val PER: 0.1834
2026-01-09 11:37:12,429: t15.2024.05.10 val PER: 0.2021
2026-01-09 11:37:12,429: t15.2024.06.14 val PER: 0.1972
2026-01-09 11:37:12,429: t15.2024.07.19 val PER: 0.2821
2026-01-09 11:37:12,429: t15.2024.07.21 val PER: 0.1200
2026-01-09 11:37:12,429: t15.2024.07.28 val PER: 0.1632
2026-01-09 11:37:12,430: t15.2025.01.10 val PER: 0.3512
2026-01-09 11:37:12,430: t15.2025.01.12 val PER: 0.1971
2026-01-09 11:37:12,430: t15.2025.03.14 val PER: 0.3609
2026-01-09 11:37:12,430: t15.2025.03.16 val PER: 0.2487
2026-01-09 11:37:12,430: t15.2025.03.30 val PER: 0.3379
2026-01-09 11:37:12,430: t15.2025.04.13 val PER: 0.2582
2026-01-09 11:37:12,431: New best val WER(5gram) 18.38% --> 17.60%
2026-01-09 11:37:12,607: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_10500
2026-01-09 11:37:21,275: Train batch 10600: loss: 12.31 grad norm: 57.99 time: 0.073
2026-01-09 11:37:38,313: Train batch 10800: loss: 18.56 grad norm: 72.22 time: 0.065
2026-01-09 11:37:55,702: Train batch 11000: loss: 18.71 grad norm: 65.81 time: 0.058
2026-01-09 11:37:55,703: Running test after training batch: 11000
2026-01-09 11:37:55,837: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:38:00,714: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:38:00,763: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:38:11,989: Val batch 11000: PER (avg): 0.1875 CTC Loss (avg): 18.4435 WER(5gram): 18.38% (n=256) time: 16.286
2026-01-09 11:38:11,990: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=12
2026-01-09 11:38:11,990: t15.2023.08.13 val PER: 0.1507
2026-01-09 11:38:11,990: t15.2023.08.18 val PER: 0.1400
2026-01-09 11:38:11,990: t15.2023.08.20 val PER: 0.1493
2026-01-09 11:38:11,990: t15.2023.08.25 val PER: 0.1160
2026-01-09 11:38:11,990: t15.2023.08.27 val PER: 0.2186
2026-01-09 11:38:11,990: t15.2023.09.01 val PER: 0.0990
2026-01-09 11:38:11,990: t15.2023.09.03 val PER: 0.1876
2026-01-09 11:38:11,990: t15.2023.09.24 val PER: 0.1541
2026-01-09 11:38:11,990: t15.2023.09.29 val PER: 0.1640
2026-01-09 11:38:11,990: t15.2023.10.01 val PER: 0.2100
2026-01-09 11:38:11,991: t15.2023.10.06 val PER: 0.1023
2026-01-09 11:38:11,991: t15.2023.10.08 val PER: 0.2639
2026-01-09 11:38:11,991: t15.2023.10.13 val PER: 0.2389
2026-01-09 11:38:11,991: t15.2023.10.15 val PER: 0.1833
2026-01-09 11:38:11,991: t15.2023.10.20 val PER: 0.1913
2026-01-09 11:38:11,991: t15.2023.10.22 val PER: 0.1403
2026-01-09 11:38:11,991: t15.2023.11.03 val PER: 0.2069
2026-01-09 11:38:11,991: t15.2023.11.04 val PER: 0.0546
2026-01-09 11:38:11,991: t15.2023.11.17 val PER: 0.0669
2026-01-09 11:38:11,991: t15.2023.11.19 val PER: 0.0579
2026-01-09 11:38:11,992: t15.2023.11.26 val PER: 0.1833
2026-01-09 11:38:11,992: t15.2023.12.03 val PER: 0.1702
2026-01-09 11:38:11,992: t15.2023.12.08 val PER: 0.1545
2026-01-09 11:38:11,992: t15.2023.12.10 val PER: 0.1261
2026-01-09 11:38:11,992: t15.2023.12.17 val PER: 0.1674
2026-01-09 11:38:11,992: t15.2023.12.29 val PER: 0.1805
2026-01-09 11:38:11,992: t15.2024.02.25 val PER: 0.1475
2026-01-09 11:38:11,992: t15.2024.03.08 val PER: 0.2546
2026-01-09 11:38:11,992: t15.2024.03.15 val PER: 0.2420
2026-01-09 11:38:11,992: t15.2024.03.17 val PER: 0.1848
2026-01-09 11:38:11,992: t15.2024.05.10 val PER: 0.2051
2026-01-09 11:38:11,993: t15.2024.06.14 val PER: 0.1940
2026-01-09 11:38:11,993: t15.2024.07.19 val PER: 0.2907
2026-01-09 11:38:11,993: t15.2024.07.21 val PER: 0.1297
2026-01-09 11:38:11,993: t15.2024.07.28 val PER: 0.1721
2026-01-09 11:38:11,993: t15.2025.01.10 val PER: 0.3375
2026-01-09 11:38:11,993: t15.2025.01.12 val PER: 0.1840
2026-01-09 11:38:11,993: t15.2025.03.14 val PER: 0.3565
2026-01-09 11:38:11,994: t15.2025.03.16 val PER: 0.2369
2026-01-09 11:38:11,994: t15.2025.03.30 val PER: 0.3471
2026-01-09 11:38:11,994: t15.2025.04.13 val PER: 0.2539
2026-01-09 11:38:12,130: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_11000
2026-01-09 11:38:29,165: Train batch 11200: loss: 13.69 grad norm: 56.01 time: 0.072
2026-01-09 11:38:46,148: Train batch 11400: loss: 13.46 grad norm: 60.05 time: 0.058
2026-01-09 11:38:54,799: Running test after training batch: 11500
2026-01-09 11:38:54,951: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:38:59,840: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:38:59,886: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:39:10,806: Val batch 11500: PER (avg): 0.1836 CTC Loss (avg): 18.1834 WER(5gram): 17.54% (n=256) time: 16.007
2026-01-09 11:39:10,807: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=12
2026-01-09 11:39:10,807: t15.2023.08.13 val PER: 0.1424
2026-01-09 11:39:10,807: t15.2023.08.18 val PER: 0.1333
2026-01-09 11:39:10,807: t15.2023.08.20 val PER: 0.1525
2026-01-09 11:39:10,807: t15.2023.08.25 val PER: 0.1024
2026-01-09 11:39:10,807: t15.2023.08.27 val PER: 0.2186
2026-01-09 11:39:10,807: t15.2023.09.01 val PER: 0.1015
2026-01-09 11:39:10,807: t15.2023.09.03 val PER: 0.1924
2026-01-09 11:39:10,807: t15.2023.09.24 val PER: 0.1481
2026-01-09 11:39:10,807: t15.2023.09.29 val PER: 0.1589
2026-01-09 11:39:10,807: t15.2023.10.01 val PER: 0.1982
2026-01-09 11:39:10,807: t15.2023.10.06 val PER: 0.0980
2026-01-09 11:39:10,808: t15.2023.10.08 val PER: 0.2706
2026-01-09 11:39:10,808: t15.2023.10.13 val PER: 0.2444
2026-01-09 11:39:10,808: t15.2023.10.15 val PER: 0.1879
2026-01-09 11:39:10,808: t15.2023.10.20 val PER: 0.2081
2026-01-09 11:39:10,808: t15.2023.10.22 val PER: 0.1414
2026-01-09 11:39:10,808: t15.2023.11.03 val PER: 0.1995
2026-01-09 11:39:10,808: t15.2023.11.04 val PER: 0.0410
2026-01-09 11:39:10,808: t15.2023.11.17 val PER: 0.0607
2026-01-09 11:39:10,808: t15.2023.11.19 val PER: 0.0619
2026-01-09 11:39:10,808: t15.2023.11.26 val PER: 0.1725
2026-01-09 11:39:10,808: t15.2023.12.03 val PER: 0.1460
2026-01-09 11:39:10,809: t15.2023.12.08 val PER: 0.1531
2026-01-09 11:39:10,809: t15.2023.12.10 val PER: 0.1248
2026-01-09 11:39:10,809: t15.2023.12.17 val PER: 0.1715
2026-01-09 11:39:10,809: t15.2023.12.29 val PER: 0.1661
2026-01-09 11:39:10,809: t15.2024.02.25 val PER: 0.1545
2026-01-09 11:39:10,809: t15.2024.03.08 val PER: 0.2589
2026-01-09 11:39:10,809: t15.2024.03.15 val PER: 0.2358
2026-01-09 11:39:10,809: t15.2024.03.17 val PER: 0.1729
2026-01-09 11:39:10,809: t15.2024.05.10 val PER: 0.1961
2026-01-09 11:39:10,809: t15.2024.06.14 val PER: 0.2114
2026-01-09 11:39:10,809: t15.2024.07.19 val PER: 0.2802
2026-01-09 11:39:10,809: t15.2024.07.21 val PER: 0.1221
2026-01-09 11:39:10,809: t15.2024.07.28 val PER: 0.1610
2026-01-09 11:39:10,809: t15.2025.01.10 val PER: 0.3416
2026-01-09 11:39:10,809: t15.2025.01.12 val PER: 0.1948
2026-01-09 11:39:10,809: t15.2025.03.14 val PER: 0.3506
2026-01-09 11:39:10,810: t15.2025.03.16 val PER: 0.2382
2026-01-09 11:39:10,810: t15.2025.03.30 val PER: 0.3253
2026-01-09 11:39:10,810: t15.2025.04.13 val PER: 0.2525
2026-01-09 11:39:10,811: New best val WER(5gram) 17.60% --> 17.54%
2026-01-09 11:39:10,995: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_11500
2026-01-09 11:39:19,304: Train batch 11600: loss: 14.13 grad norm: 54.15 time: 0.062
2026-01-09 11:39:36,303: Train batch 11800: loss: 9.34 grad norm: 45.71 time: 0.045
2026-01-09 11:39:53,379: Train batch 12000: loss: 17.75 grad norm: 58.67 time: 0.072
2026-01-09 11:39:53,379: Running test after training batch: 12000
2026-01-09 11:39:53,475: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:39:58,376: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:39:58,421: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:40:09,558: Val batch 12000: PER (avg): 0.1816 CTC Loss (avg): 17.9334 WER(5gram): 18.64% (n=256) time: 16.179
2026-01-09 11:40:09,559: WER lens: avg_true_words=5.99 avg_pred_words=6.23 max_pred_words=12
2026-01-09 11:40:09,559: t15.2023.08.13 val PER: 0.1414
2026-01-09 11:40:09,559: t15.2023.08.18 val PER: 0.1316
2026-01-09 11:40:09,559: t15.2023.08.20 val PER: 0.1398
2026-01-09 11:40:09,559: t15.2023.08.25 val PER: 0.1160
2026-01-09 11:40:09,559: t15.2023.08.27 val PER: 0.2074
2026-01-09 11:40:09,559: t15.2023.09.01 val PER: 0.1023
2026-01-09 11:40:09,559: t15.2023.09.03 val PER: 0.1793
2026-01-09 11:40:09,559: t15.2023.09.24 val PER: 0.1468
2026-01-09 11:40:09,560: t15.2023.09.29 val PER: 0.1525
2026-01-09 11:40:09,560: t15.2023.10.01 val PER: 0.1988
2026-01-09 11:40:09,560: t15.2023.10.06 val PER: 0.1109
2026-01-09 11:40:09,560: t15.2023.10.08 val PER: 0.2585
2026-01-09 11:40:09,560: t15.2023.10.13 val PER: 0.2358
2026-01-09 11:40:09,560: t15.2023.10.15 val PER: 0.1852
2026-01-09 11:40:09,560: t15.2023.10.20 val PER: 0.1913
2026-01-09 11:40:09,560: t15.2023.10.22 val PER: 0.1414
2026-01-09 11:40:09,560: t15.2023.11.03 val PER: 0.1995
2026-01-09 11:40:09,560: t15.2023.11.04 val PER: 0.0307
2026-01-09 11:40:09,561: t15.2023.11.17 val PER: 0.0638
2026-01-09 11:40:09,561: t15.2023.11.19 val PER: 0.0519
2026-01-09 11:40:09,561: t15.2023.11.26 val PER: 0.1710
2026-01-09 11:40:09,561: t15.2023.12.03 val PER: 0.1565
2026-01-09 11:40:09,561: t15.2023.12.08 val PER: 0.1471
2026-01-09 11:40:09,561: t15.2023.12.10 val PER: 0.1288
2026-01-09 11:40:09,561: t15.2023.12.17 val PER: 0.1622
2026-01-09 11:40:09,561: t15.2023.12.29 val PER: 0.1764
2026-01-09 11:40:09,562: t15.2024.02.25 val PER: 0.1419
2026-01-09 11:40:09,562: t15.2024.03.08 val PER: 0.2717
2026-01-09 11:40:09,562: t15.2024.03.15 val PER: 0.2376
2026-01-09 11:40:09,562: t15.2024.03.17 val PER: 0.1681
2026-01-09 11:40:09,562: t15.2024.05.10 val PER: 0.1961
2026-01-09 11:40:09,562: t15.2024.06.14 val PER: 0.1987
2026-01-09 11:40:09,562: t15.2024.07.19 val PER: 0.2887
2026-01-09 11:40:09,562: t15.2024.07.21 val PER: 0.1262
2026-01-09 11:40:09,562: t15.2024.07.28 val PER: 0.1603
2026-01-09 11:40:09,562: t15.2025.01.10 val PER: 0.3306
2026-01-09 11:40:09,562: t15.2025.01.12 val PER: 0.1840
2026-01-09 11:40:09,562: t15.2025.03.14 val PER: 0.3358
2026-01-09 11:40:09,562: t15.2025.03.16 val PER: 0.2356
2026-01-09 11:40:09,562: t15.2025.03.30 val PER: 0.3276
2026-01-09 11:40:09,562: t15.2025.04.13 val PER: 0.2482
2026-01-09 11:40:09,695: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_12000
2026-01-09 11:40:26,752: Train batch 12200: loss: 8.21 grad norm: 45.40 time: 0.066
2026-01-09 11:40:43,816: Train batch 12400: loss: 6.39 grad norm: 39.57 time: 0.042
2026-01-09 11:40:52,521: Running test after training batch: 12500
2026-01-09 11:40:52,669: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:40:57,571: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:40:57,621: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:41:08,493: Val batch 12500: PER (avg): 0.1786 CTC Loss (avg): 17.7668 WER(5gram): 16.56% (n=256) time: 15.972
2026-01-09 11:41:08,494: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-09 11:41:08,494: t15.2023.08.13 val PER: 0.1424
2026-01-09 11:41:08,494: t15.2023.08.18 val PER: 0.1232
2026-01-09 11:41:08,494: t15.2023.08.20 val PER: 0.1430
2026-01-09 11:41:08,494: t15.2023.08.25 val PER: 0.0949
2026-01-09 11:41:08,494: t15.2023.08.27 val PER: 0.2251
2026-01-09 11:41:08,494: t15.2023.09.01 val PER: 0.0982
2026-01-09 11:41:08,494: t15.2023.09.03 val PER: 0.1805
2026-01-09 11:41:08,494: t15.2023.09.24 val PER: 0.1383
2026-01-09 11:41:08,494: t15.2023.09.29 val PER: 0.1506
2026-01-09 11:41:08,494: t15.2023.10.01 val PER: 0.1843
2026-01-09 11:41:08,495: t15.2023.10.06 val PER: 0.0947
2026-01-09 11:41:08,495: t15.2023.10.08 val PER: 0.2733
2026-01-09 11:41:08,495: t15.2023.10.13 val PER: 0.2413
2026-01-09 11:41:08,495: t15.2023.10.15 val PER: 0.1846
2026-01-09 11:41:08,495: t15.2023.10.20 val PER: 0.1812
2026-01-09 11:41:08,495: t15.2023.10.22 val PER: 0.1269
2026-01-09 11:41:08,495: t15.2023.11.03 val PER: 0.1961
2026-01-09 11:41:08,495: t15.2023.11.04 val PER: 0.0410
2026-01-09 11:41:08,495: t15.2023.11.17 val PER: 0.0622
2026-01-09 11:41:08,495: t15.2023.11.19 val PER: 0.0519
2026-01-09 11:41:08,495: t15.2023.11.26 val PER: 0.1732
2026-01-09 11:41:08,495: t15.2023.12.03 val PER: 0.1471
2026-01-09 11:41:08,495: t15.2023.12.08 val PER: 0.1391
2026-01-09 11:41:08,496: t15.2023.12.10 val PER: 0.1288
2026-01-09 11:41:08,496: t15.2023.12.17 val PER: 0.1746
2026-01-09 11:41:08,496: t15.2023.12.29 val PER: 0.1730
2026-01-09 11:41:08,496: t15.2024.02.25 val PER: 0.1447
2026-01-09 11:41:08,496: t15.2024.03.08 val PER: 0.2560
2026-01-09 11:41:08,496: t15.2024.03.15 val PER: 0.2383
2026-01-09 11:41:08,496: t15.2024.03.17 val PER: 0.1667
2026-01-09 11:41:08,496: t15.2024.05.10 val PER: 0.1828
2026-01-09 11:41:08,496: t15.2024.06.14 val PER: 0.2082
2026-01-09 11:41:08,496: t15.2024.07.19 val PER: 0.2742
2026-01-09 11:41:08,496: t15.2024.07.21 val PER: 0.1159
2026-01-09 11:41:08,496: t15.2024.07.28 val PER: 0.1566
2026-01-09 11:41:08,496: t15.2025.01.10 val PER: 0.3306
2026-01-09 11:41:08,496: t15.2025.01.12 val PER: 0.1786
2026-01-09 11:41:08,496: t15.2025.03.14 val PER: 0.3565
2026-01-09 11:41:08,497: t15.2025.03.16 val PER: 0.2330
2026-01-09 11:41:08,497: t15.2025.03.30 val PER: 0.3322
2026-01-09 11:41:08,497: t15.2025.04.13 val PER: 0.2354
2026-01-09 11:41:08,498: New best val WER(5gram) 17.54% --> 16.56%
2026-01-09 11:41:08,683: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_12500
2026-01-09 11:41:17,255: Train batch 12600: loss: 10.60 grad norm: 50.13 time: 0.058
2026-01-09 11:41:34,549: Train batch 12800: loss: 8.48 grad norm: 46.69 time: 0.054
2026-01-09 11:41:52,601: Train batch 13000: loss: 9.09 grad norm: 48.89 time: 0.067
2026-01-09 11:41:52,602: Running test after training batch: 13000
2026-01-09 11:41:52,704: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:41:57,553: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:41:57,597: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:42:08,835: Val batch 13000: PER (avg): 0.1773 CTC Loss (avg): 17.5526 WER(5gram): 15.78% (n=256) time: 16.233
2026-01-09 11:42:08,835: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-09 11:42:08,835: t15.2023.08.13 val PER: 0.1362
2026-01-09 11:42:08,836: t15.2023.08.18 val PER: 0.1324
2026-01-09 11:42:08,836: t15.2023.08.20 val PER: 0.1319
2026-01-09 11:42:08,836: t15.2023.08.25 val PER: 0.0994
2026-01-09 11:42:08,836: t15.2023.08.27 val PER: 0.2170
2026-01-09 11:42:08,836: t15.2023.09.01 val PER: 0.0950
2026-01-09 11:42:08,836: t15.2023.09.03 val PER: 0.1829
2026-01-09 11:42:08,836: t15.2023.09.24 val PER: 0.1383
2026-01-09 11:42:08,836: t15.2023.09.29 val PER: 0.1493
2026-01-09 11:42:08,836: t15.2023.10.01 val PER: 0.1962
2026-01-09 11:42:08,836: t15.2023.10.06 val PER: 0.0936
2026-01-09 11:42:08,836: t15.2023.10.08 val PER: 0.2612
2026-01-09 11:42:08,836: t15.2023.10.13 val PER: 0.2343
2026-01-09 11:42:08,836: t15.2023.10.15 val PER: 0.1813
2026-01-09 11:42:08,836: t15.2023.10.20 val PER: 0.1879
2026-01-09 11:42:08,836: t15.2023.10.22 val PER: 0.1258
2026-01-09 11:42:08,836: t15.2023.11.03 val PER: 0.1967
2026-01-09 11:42:08,837: t15.2023.11.04 val PER: 0.0375
2026-01-09 11:42:08,837: t15.2023.11.17 val PER: 0.0591
2026-01-09 11:42:08,837: t15.2023.11.19 val PER: 0.0579
2026-01-09 11:42:08,837: t15.2023.11.26 val PER: 0.1616
2026-01-09 11:42:08,837: t15.2023.12.03 val PER: 0.1586
2026-01-09 11:42:08,837: t15.2023.12.08 val PER: 0.1405
2026-01-09 11:42:08,837: t15.2023.12.10 val PER: 0.1261
2026-01-09 11:42:08,837: t15.2023.12.17 val PER: 0.1538
2026-01-09 11:42:08,837: t15.2023.12.29 val PER: 0.1750
2026-01-09 11:42:08,838: t15.2024.02.25 val PER: 0.1334
2026-01-09 11:42:08,838: t15.2024.03.08 val PER: 0.2632
2026-01-09 11:42:08,838: t15.2024.03.15 val PER: 0.2295
2026-01-09 11:42:08,838: t15.2024.03.17 val PER: 0.1646
2026-01-09 11:42:08,838: t15.2024.05.10 val PER: 0.1961
2026-01-09 11:42:08,838: t15.2024.06.14 val PER: 0.2035
2026-01-09 11:42:08,838: t15.2024.07.19 val PER: 0.2729
2026-01-09 11:42:08,838: t15.2024.07.21 val PER: 0.1207
2026-01-09 11:42:08,838: t15.2024.07.28 val PER: 0.1632
2026-01-09 11:42:08,838: t15.2025.01.10 val PER: 0.3320
2026-01-09 11:42:08,838: t15.2025.01.12 val PER: 0.1732
2026-01-09 11:42:08,838: t15.2025.03.14 val PER: 0.3476
2026-01-09 11:42:08,838: t15.2025.03.16 val PER: 0.2264
2026-01-09 11:42:08,838: t15.2025.03.30 val PER: 0.3310
2026-01-09 11:42:08,838: t15.2025.04.13 val PER: 0.2468
2026-01-09 11:42:08,839: New best val WER(5gram) 16.56% --> 15.78%
2026-01-09 11:42:09,022: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_13000
2026-01-09 11:42:26,220: Train batch 13200: loss: 18.01 grad norm: 70.35 time: 0.055
2026-01-09 11:42:43,361: Train batch 13400: loss: 13.22 grad norm: 64.57 time: 0.063
2026-01-09 11:42:51,971: Running test after training batch: 13500
2026-01-09 11:42:52,093: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:42:56,969: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:42:57,012: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:43:08,313: Val batch 13500: PER (avg): 0.1751 CTC Loss (avg): 17.3259 WER(5gram): 16.82% (n=256) time: 16.342
2026-01-09 11:43:08,314: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-09 11:43:08,314: t15.2023.08.13 val PER: 0.1393
2026-01-09 11:43:08,314: t15.2023.08.18 val PER: 0.1299
2026-01-09 11:43:08,314: t15.2023.08.20 val PER: 0.1350
2026-01-09 11:43:08,314: t15.2023.08.25 val PER: 0.1024
2026-01-09 11:43:08,314: t15.2023.08.27 val PER: 0.2170
2026-01-09 11:43:08,315: t15.2023.09.01 val PER: 0.1023
2026-01-09 11:43:08,315: t15.2023.09.03 val PER: 0.1758
2026-01-09 11:43:08,315: t15.2023.09.24 val PER: 0.1408
2026-01-09 11:43:08,315: t15.2023.09.29 val PER: 0.1442
2026-01-09 11:43:08,315: t15.2023.10.01 val PER: 0.1962
2026-01-09 11:43:08,315: t15.2023.10.06 val PER: 0.1012
2026-01-09 11:43:08,315: t15.2023.10.08 val PER: 0.2517
2026-01-09 11:43:08,315: t15.2023.10.13 val PER: 0.2312
2026-01-09 11:43:08,315: t15.2023.10.15 val PER: 0.1800
2026-01-09 11:43:08,315: t15.2023.10.20 val PER: 0.1812
2026-01-09 11:43:08,315: t15.2023.10.22 val PER: 0.1303
2026-01-09 11:43:08,315: t15.2023.11.03 val PER: 0.1927
2026-01-09 11:43:08,315: t15.2023.11.04 val PER: 0.0307
2026-01-09 11:43:08,315: t15.2023.11.17 val PER: 0.0638
2026-01-09 11:43:08,315: t15.2023.11.19 val PER: 0.0519
2026-01-09 11:43:08,315: t15.2023.11.26 val PER: 0.1609
2026-01-09 11:43:08,315: t15.2023.12.03 val PER: 0.1439
2026-01-09 11:43:08,316: t15.2023.12.08 val PER: 0.1398
2026-01-09 11:43:08,316: t15.2023.12.10 val PER: 0.1209
2026-01-09 11:43:08,316: t15.2023.12.17 val PER: 0.1507
2026-01-09 11:43:08,316: t15.2023.12.29 val PER: 0.1627
2026-01-09 11:43:08,316: t15.2024.02.25 val PER: 0.1390
2026-01-09 11:43:08,316: t15.2024.03.08 val PER: 0.2674
2026-01-09 11:43:08,316: t15.2024.03.15 val PER: 0.2320
2026-01-09 11:43:08,316: t15.2024.03.17 val PER: 0.1604
2026-01-09 11:43:08,316: t15.2024.05.10 val PER: 0.1902
2026-01-09 11:43:08,316: t15.2024.06.14 val PER: 0.1845
2026-01-09 11:43:08,316: t15.2024.07.19 val PER: 0.2650
2026-01-09 11:43:08,316: t15.2024.07.21 val PER: 0.1159
2026-01-09 11:43:08,316: t15.2024.07.28 val PER: 0.1588
2026-01-09 11:43:08,316: t15.2025.01.10 val PER: 0.3264
2026-01-09 11:43:08,317: t15.2025.01.12 val PER: 0.1755
2026-01-09 11:43:08,317: t15.2025.03.14 val PER: 0.3491
2026-01-09 11:43:08,317: t15.2025.03.16 val PER: 0.2212
2026-01-09 11:43:08,317: t15.2025.03.30 val PER: 0.3368
2026-01-09 11:43:08,317: t15.2025.04.13 val PER: 0.2354
2026-01-09 11:43:08,453: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_13500
2026-01-09 11:43:17,087: Train batch 13600: loss: 16.47 grad norm: 69.30 time: 0.063
2026-01-09 11:43:34,396: Train batch 13800: loss: 12.55 grad norm: 61.68 time: 0.058
2026-01-09 11:43:51,695: Train batch 14000: loss: 16.05 grad norm: 66.67 time: 0.051
2026-01-09 11:43:51,696: Running test after training batch: 14000
2026-01-09 11:43:51,808: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:43:56,735: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:43:56,777: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:44:08,024: Val batch 14000: PER (avg): 0.1746 CTC Loss (avg): 17.2476 WER(5gram): 16.88% (n=256) time: 16.328
2026-01-09 11:44:08,025: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-09 11:44:08,025: t15.2023.08.13 val PER: 0.1279
2026-01-09 11:44:08,025: t15.2023.08.18 val PER: 0.1291
2026-01-09 11:44:08,025: t15.2023.08.20 val PER: 0.1414
2026-01-09 11:44:08,025: t15.2023.08.25 val PER: 0.1069
2026-01-09 11:44:08,025: t15.2023.08.27 val PER: 0.2074
2026-01-09 11:44:08,025: t15.2023.09.01 val PER: 0.0958
2026-01-09 11:44:08,025: t15.2023.09.03 val PER: 0.1865
2026-01-09 11:44:08,025: t15.2023.09.24 val PER: 0.1371
2026-01-09 11:44:08,026: t15.2023.09.29 val PER: 0.1493
2026-01-09 11:44:08,026: t15.2023.10.01 val PER: 0.1929
2026-01-09 11:44:08,026: t15.2023.10.06 val PER: 0.1012
2026-01-09 11:44:08,026: t15.2023.10.08 val PER: 0.2720
2026-01-09 11:44:08,026: t15.2023.10.13 val PER: 0.2289
2026-01-09 11:44:08,026: t15.2023.10.15 val PER: 0.1826
2026-01-09 11:44:08,026: t15.2023.10.20 val PER: 0.1711
2026-01-09 11:44:08,026: t15.2023.10.22 val PER: 0.1303
2026-01-09 11:44:08,026: t15.2023.11.03 val PER: 0.1995
2026-01-09 11:44:08,026: t15.2023.11.04 val PER: 0.0273
2026-01-09 11:44:08,027: t15.2023.11.17 val PER: 0.0638
2026-01-09 11:44:08,027: t15.2023.11.19 val PER: 0.0519
2026-01-09 11:44:08,027: t15.2023.11.26 val PER: 0.1645
2026-01-09 11:44:08,027: t15.2023.12.03 val PER: 0.1481
2026-01-09 11:44:08,027: t15.2023.12.08 val PER: 0.1298
2026-01-09 11:44:08,027: t15.2023.12.10 val PER: 0.1222
2026-01-09 11:44:08,027: t15.2023.12.17 val PER: 0.1580
2026-01-09 11:44:08,027: t15.2023.12.29 val PER: 0.1627
2026-01-09 11:44:08,027: t15.2024.02.25 val PER: 0.1348
2026-01-09 11:44:08,027: t15.2024.03.08 val PER: 0.2632
2026-01-09 11:44:08,027: t15.2024.03.15 val PER: 0.2339
2026-01-09 11:44:08,027: t15.2024.03.17 val PER: 0.1597
2026-01-09 11:44:08,027: t15.2024.05.10 val PER: 0.1887
2026-01-09 11:44:08,027: t15.2024.06.14 val PER: 0.1798
2026-01-09 11:44:08,027: t15.2024.07.19 val PER: 0.2716
2026-01-09 11:44:08,027: t15.2024.07.21 val PER: 0.1145
2026-01-09 11:44:08,027: t15.2024.07.28 val PER: 0.1588
2026-01-09 11:44:08,028: t15.2025.01.10 val PER: 0.3154
2026-01-09 11:44:08,028: t15.2025.01.12 val PER: 0.1678
2026-01-09 11:44:08,028: t15.2025.03.14 val PER: 0.3388
2026-01-09 11:44:08,028: t15.2025.03.16 val PER: 0.2094
2026-01-09 11:44:08,028: t15.2025.03.30 val PER: 0.3264
2026-01-09 11:44:08,028: t15.2025.04.13 val PER: 0.2439
2026-01-09 11:44:08,161: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_14000
2026-01-09 11:44:25,255: Train batch 14200: loss: 11.38 grad norm: 58.53 time: 0.057
2026-01-09 11:44:42,695: Train batch 14400: loss: 8.58 grad norm: 47.38 time: 0.064
2026-01-09 11:44:51,263: Running test after training batch: 14500
2026-01-09 11:44:51,364: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:44:56,392: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:44:56,437: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:45:07,620: Val batch 14500: PER (avg): 0.1741 CTC Loss (avg): 17.2686 WER(5gram): 15.97% (n=256) time: 16.356
2026-01-09 11:45:07,621: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-09 11:45:07,621: t15.2023.08.13 val PER: 0.1299
2026-01-09 11:45:07,621: t15.2023.08.18 val PER: 0.1274
2026-01-09 11:45:07,621: t15.2023.08.20 val PER: 0.1319
2026-01-09 11:45:07,621: t15.2023.08.25 val PER: 0.0964
2026-01-09 11:45:07,621: t15.2023.08.27 val PER: 0.2026
2026-01-09 11:45:07,621: t15.2023.09.01 val PER: 0.0958
2026-01-09 11:45:07,621: t15.2023.09.03 val PER: 0.1817
2026-01-09 11:45:07,621: t15.2023.09.24 val PER: 0.1396
2026-01-09 11:45:07,622: t15.2023.09.29 val PER: 0.1455
2026-01-09 11:45:07,622: t15.2023.10.01 val PER: 0.1929
2026-01-09 11:45:07,622: t15.2023.10.06 val PER: 0.0947
2026-01-09 11:45:07,622: t15.2023.10.08 val PER: 0.2544
2026-01-09 11:45:07,622: t15.2023.10.13 val PER: 0.2320
2026-01-09 11:45:07,622: t15.2023.10.15 val PER: 0.1786
2026-01-09 11:45:07,622: t15.2023.10.20 val PER: 0.1812
2026-01-09 11:45:07,622: t15.2023.10.22 val PER: 0.1314
2026-01-09 11:45:07,622: t15.2023.11.03 val PER: 0.1995
2026-01-09 11:45:07,622: t15.2023.11.04 val PER: 0.0444
2026-01-09 11:45:07,622: t15.2023.11.17 val PER: 0.0653
2026-01-09 11:45:07,622: t15.2023.11.19 val PER: 0.0499
2026-01-09 11:45:07,622: t15.2023.11.26 val PER: 0.1630
2026-01-09 11:45:07,622: t15.2023.12.03 val PER: 0.1376
2026-01-09 11:45:07,622: t15.2023.12.08 val PER: 0.1358
2026-01-09 11:45:07,622: t15.2023.12.10 val PER: 0.1143
2026-01-09 11:45:07,623: t15.2023.12.17 val PER: 0.1528
2026-01-09 11:45:07,623: t15.2023.12.29 val PER: 0.1620
2026-01-09 11:45:07,623: t15.2024.02.25 val PER: 0.1376
2026-01-09 11:45:07,623: t15.2024.03.08 val PER: 0.2660
2026-01-09 11:45:07,623: t15.2024.03.15 val PER: 0.2314
2026-01-09 11:45:07,623: t15.2024.03.17 val PER: 0.1576
2026-01-09 11:45:07,623: t15.2024.05.10 val PER: 0.1887
2026-01-09 11:45:07,623: t15.2024.06.14 val PER: 0.1940
2026-01-09 11:45:07,623: t15.2024.07.19 val PER: 0.2676
2026-01-09 11:45:07,624: t15.2024.07.21 val PER: 0.1131
2026-01-09 11:45:07,624: t15.2024.07.28 val PER: 0.1588
2026-01-09 11:45:07,624: t15.2025.01.10 val PER: 0.3127
2026-01-09 11:45:07,624: t15.2025.01.12 val PER: 0.1809
2026-01-09 11:45:07,624: t15.2025.03.14 val PER: 0.3609
2026-01-09 11:45:07,624: t15.2025.03.16 val PER: 0.2251
2026-01-09 11:45:07,624: t15.2025.03.30 val PER: 0.3310
2026-01-09 11:45:07,624: t15.2025.04.13 val PER: 0.2297
2026-01-09 11:45:07,757: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_14500
2026-01-09 11:45:16,390: Train batch 14600: loss: 16.81 grad norm: 68.50 time: 0.059
2026-01-09 11:45:33,504: Train batch 14800: loss: 7.82 grad norm: 49.73 time: 0.055
2026-01-09 11:45:50,653: Train batch 15000: loss: 12.12 grad norm: 58.09 time: 0.056
2026-01-09 11:45:50,654: Running test after training batch: 15000
2026-01-09 11:45:50,769: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:45:55,704: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:45:55,749: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:46:06,701: Val batch 15000: PER (avg): 0.1713 CTC Loss (avg): 17.0689 WER(5gram): 15.91% (n=256) time: 16.046
2026-01-09 11:46:06,701: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-09 11:46:06,701: t15.2023.08.13 val PER: 0.1247
2026-01-09 11:46:06,701: t15.2023.08.18 val PER: 0.1266
2026-01-09 11:46:06,702: t15.2023.08.20 val PER: 0.1342
2026-01-09 11:46:06,702: t15.2023.08.25 val PER: 0.0964
2026-01-09 11:46:06,702: t15.2023.08.27 val PER: 0.1945
2026-01-09 11:46:06,702: t15.2023.09.01 val PER: 0.0958
2026-01-09 11:46:06,702: t15.2023.09.03 val PER: 0.1793
2026-01-09 11:46:06,702: t15.2023.09.24 val PER: 0.1383
2026-01-09 11:46:06,702: t15.2023.09.29 val PER: 0.1461
2026-01-09 11:46:06,702: t15.2023.10.01 val PER: 0.1836
2026-01-09 11:46:06,702: t15.2023.10.06 val PER: 0.0915
2026-01-09 11:46:06,702: t15.2023.10.08 val PER: 0.2476
2026-01-09 11:46:06,702: t15.2023.10.13 val PER: 0.2281
2026-01-09 11:46:06,702: t15.2023.10.15 val PER: 0.1826
2026-01-09 11:46:06,702: t15.2023.10.20 val PER: 0.1779
2026-01-09 11:46:06,703: t15.2023.10.22 val PER: 0.1203
2026-01-09 11:46:06,703: t15.2023.11.03 val PER: 0.1927
2026-01-09 11:46:06,703: t15.2023.11.04 val PER: 0.0341
2026-01-09 11:46:06,703: t15.2023.11.17 val PER: 0.0529
2026-01-09 11:46:06,703: t15.2023.11.19 val PER: 0.0499
2026-01-09 11:46:06,703: t15.2023.11.26 val PER: 0.1572
2026-01-09 11:46:06,703: t15.2023.12.03 val PER: 0.1418
2026-01-09 11:46:06,703: t15.2023.12.08 val PER: 0.1318
2026-01-09 11:46:06,703: t15.2023.12.10 val PER: 0.1183
2026-01-09 11:46:06,703: t15.2023.12.17 val PER: 0.1580
2026-01-09 11:46:06,703: t15.2023.12.29 val PER: 0.1613
2026-01-09 11:46:06,703: t15.2024.02.25 val PER: 0.1292
2026-01-09 11:46:06,704: t15.2024.03.08 val PER: 0.2589
2026-01-09 11:46:06,704: t15.2024.03.15 val PER: 0.2295
2026-01-09 11:46:06,704: t15.2024.03.17 val PER: 0.1569
2026-01-09 11:46:06,704: t15.2024.05.10 val PER: 0.1887
2026-01-09 11:46:06,704: t15.2024.06.14 val PER: 0.2035
2026-01-09 11:46:06,704: t15.2024.07.19 val PER: 0.2584
2026-01-09 11:46:06,704: t15.2024.07.21 val PER: 0.1131
2026-01-09 11:46:06,704: t15.2024.07.28 val PER: 0.1537
2026-01-09 11:46:06,704: t15.2025.01.10 val PER: 0.3154
2026-01-09 11:46:06,704: t15.2025.01.12 val PER: 0.1747
2026-01-09 11:46:06,704: t15.2025.03.14 val PER: 0.3595
2026-01-09 11:46:06,704: t15.2025.03.16 val PER: 0.2238
2026-01-09 11:46:06,704: t15.2025.03.30 val PER: 0.3172
2026-01-09 11:46:06,705: t15.2025.04.13 val PER: 0.2325
2026-01-09 11:46:06,841: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_15000
2026-01-09 11:46:24,197: Train batch 15200: loss: 7.19 grad norm: 43.90 time: 0.058
2026-01-09 11:46:41,538: Train batch 15400: loss: 15.00 grad norm: 61.26 time: 0.050
2026-01-09 11:46:50,257: Running test after training batch: 15500
2026-01-09 11:46:50,363: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:46:55,263: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:46:55,310: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:47:06,450: Val batch 15500: PER (avg): 0.1694 CTC Loss (avg): 16.9195 WER(5gram): 15.97% (n=256) time: 16.193
2026-01-09 11:47:06,451: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-09 11:47:06,451: t15.2023.08.13 val PER: 0.1175
2026-01-09 11:47:06,451: t15.2023.08.18 val PER: 0.1291
2026-01-09 11:47:06,451: t15.2023.08.20 val PER: 0.1334
2026-01-09 11:47:06,451: t15.2023.08.25 val PER: 0.0979
2026-01-09 11:47:06,451: t15.2023.08.27 val PER: 0.1994
2026-01-09 11:47:06,451: t15.2023.09.01 val PER: 0.0917
2026-01-09 11:47:06,452: t15.2023.09.03 val PER: 0.1900
2026-01-09 11:47:06,452: t15.2023.09.24 val PER: 0.1299
2026-01-09 11:47:06,452: t15.2023.09.29 val PER: 0.1436
2026-01-09 11:47:06,452: t15.2023.10.01 val PER: 0.1823
2026-01-09 11:47:06,452: t15.2023.10.06 val PER: 0.0947
2026-01-09 11:47:06,452: t15.2023.10.08 val PER: 0.2544
2026-01-09 11:47:06,452: t15.2023.10.13 val PER: 0.2196
2026-01-09 11:47:06,452: t15.2023.10.15 val PER: 0.1747
2026-01-09 11:47:06,452: t15.2023.10.20 val PER: 0.1779
2026-01-09 11:47:06,453: t15.2023.10.22 val PER: 0.1258
2026-01-09 11:47:06,453: t15.2023.11.03 val PER: 0.1866
2026-01-09 11:47:06,453: t15.2023.11.04 val PER: 0.0375
2026-01-09 11:47:06,453: t15.2023.11.17 val PER: 0.0607
2026-01-09 11:47:06,453: t15.2023.11.19 val PER: 0.0579
2026-01-09 11:47:06,453: t15.2023.11.26 val PER: 0.1580
2026-01-09 11:47:06,453: t15.2023.12.03 val PER: 0.1366
2026-01-09 11:47:06,453: t15.2023.12.08 val PER: 0.1272
2026-01-09 11:47:06,453: t15.2023.12.10 val PER: 0.1104
2026-01-09 11:47:06,454: t15.2023.12.17 val PER: 0.1549
2026-01-09 11:47:06,454: t15.2023.12.29 val PER: 0.1572
2026-01-09 11:47:06,454: t15.2024.02.25 val PER: 0.1278
2026-01-09 11:47:06,454: t15.2024.03.08 val PER: 0.2546
2026-01-09 11:47:06,454: t15.2024.03.15 val PER: 0.2233
2026-01-09 11:47:06,454: t15.2024.03.17 val PER: 0.1597
2026-01-09 11:47:06,454: t15.2024.05.10 val PER: 0.1857
2026-01-09 11:47:06,454: t15.2024.06.14 val PER: 0.1909
2026-01-09 11:47:06,454: t15.2024.07.19 val PER: 0.2617
2026-01-09 11:47:06,455: t15.2024.07.21 val PER: 0.1138
2026-01-09 11:47:06,455: t15.2024.07.28 val PER: 0.1522
2026-01-09 11:47:06,455: t15.2025.01.10 val PER: 0.3127
2026-01-09 11:47:06,455: t15.2025.01.12 val PER: 0.1663
2026-01-09 11:47:06,455: t15.2025.03.14 val PER: 0.3506
2026-01-09 11:47:06,455: t15.2025.03.16 val PER: 0.2186
2026-01-09 11:47:06,455: t15.2025.03.30 val PER: 0.3195
2026-01-09 11:47:06,455: t15.2025.04.13 val PER: 0.2340
2026-01-09 11:47:06,590: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_15500
2026-01-09 11:47:15,175: Train batch 15600: loss: 16.96 grad norm: 64.77 time: 0.063
2026-01-09 11:47:32,225: Train batch 15800: loss: 18.19 grad norm: 71.60 time: 0.068
2026-01-09 11:47:49,830: Train batch 16000: loss: 11.74 grad norm: 52.62 time: 0.057
2026-01-09 11:47:49,831: Running test after training batch: 16000
2026-01-09 11:47:49,968: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:47:54,980: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:47:55,029: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:48:06,354: Val batch 16000: PER (avg): 0.1700 CTC Loss (avg): 16.9856 WER(5gram): 15.78% (n=256) time: 16.523
2026-01-09 11:48:06,354: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-09 11:48:06,354: t15.2023.08.13 val PER: 0.1237
2026-01-09 11:48:06,354: t15.2023.08.18 val PER: 0.1266
2026-01-09 11:48:06,355: t15.2023.08.20 val PER: 0.1287
2026-01-09 11:48:06,355: t15.2023.08.25 val PER: 0.0994
2026-01-09 11:48:06,355: t15.2023.08.27 val PER: 0.2010
2026-01-09 11:48:06,355: t15.2023.09.01 val PER: 0.0942
2026-01-09 11:48:06,355: t15.2023.09.03 val PER: 0.1853
2026-01-09 11:48:06,355: t15.2023.09.24 val PER: 0.1299
2026-01-09 11:48:06,355: t15.2023.09.29 val PER: 0.1429
2026-01-09 11:48:06,355: t15.2023.10.01 val PER: 0.1790
2026-01-09 11:48:06,355: t15.2023.10.06 val PER: 0.0926
2026-01-09 11:48:06,355: t15.2023.10.08 val PER: 0.2544
2026-01-09 11:48:06,355: t15.2023.10.13 val PER: 0.2273
2026-01-09 11:48:06,355: t15.2023.10.15 val PER: 0.1767
2026-01-09 11:48:06,355: t15.2023.10.20 val PER: 0.1879
2026-01-09 11:48:06,356: t15.2023.10.22 val PER: 0.1180
2026-01-09 11:48:06,356: t15.2023.11.03 val PER: 0.1927
2026-01-09 11:48:06,356: t15.2023.11.04 val PER: 0.0341
2026-01-09 11:48:06,356: t15.2023.11.17 val PER: 0.0591
2026-01-09 11:48:06,356: t15.2023.11.19 val PER: 0.0619
2026-01-09 11:48:06,356: t15.2023.11.26 val PER: 0.1601
2026-01-09 11:48:06,356: t15.2023.12.03 val PER: 0.1366
2026-01-09 11:48:06,356: t15.2023.12.08 val PER: 0.1292
2026-01-09 11:48:06,356: t15.2023.12.10 val PER: 0.1156
2026-01-09 11:48:06,356: t15.2023.12.17 val PER: 0.1538
2026-01-09 11:48:06,356: t15.2023.12.29 val PER: 0.1572
2026-01-09 11:48:06,356: t15.2024.02.25 val PER: 0.1194
2026-01-09 11:48:06,356: t15.2024.03.08 val PER: 0.2731
2026-01-09 11:48:06,356: t15.2024.03.15 val PER: 0.2251
2026-01-09 11:48:06,356: t15.2024.03.17 val PER: 0.1513
2026-01-09 11:48:06,356: t15.2024.05.10 val PER: 0.1961
2026-01-09 11:48:06,356: t15.2024.06.14 val PER: 0.1845
2026-01-09 11:48:06,357: t15.2024.07.19 val PER: 0.2551
2026-01-09 11:48:06,357: t15.2024.07.21 val PER: 0.1117
2026-01-09 11:48:06,357: t15.2024.07.28 val PER: 0.1596
2026-01-09 11:48:06,357: t15.2025.01.10 val PER: 0.3058
2026-01-09 11:48:06,357: t15.2025.01.12 val PER: 0.1724
2026-01-09 11:48:06,357: t15.2025.03.14 val PER: 0.3447
2026-01-09 11:48:06,357: t15.2025.03.16 val PER: 0.2107
2026-01-09 11:48:06,357: t15.2025.03.30 val PER: 0.3333
2026-01-09 11:48:06,357: t15.2025.04.13 val PER: 0.2411
2026-01-09 11:48:06,493: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_16000
2026-01-09 11:48:23,348: Train batch 16200: loss: 9.27 grad norm: 50.36 time: 0.056
2026-01-09 11:48:40,525: Train batch 16400: loss: 13.84 grad norm: 66.76 time: 0.059
2026-01-09 11:48:49,141: Running test after training batch: 16500
2026-01-09 11:48:49,263: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:48:54,254: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:48:54,298: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:49:05,706: Val batch 16500: PER (avg): 0.1690 CTC Loss (avg): 16.8122 WER(5gram): 16.04% (n=256) time: 16.565
2026-01-09 11:49:05,706: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-09 11:49:05,706: t15.2023.08.13 val PER: 0.1310
2026-01-09 11:49:05,706: t15.2023.08.18 val PER: 0.1291
2026-01-09 11:49:05,706: t15.2023.08.20 val PER: 0.1295
2026-01-09 11:49:05,706: t15.2023.08.25 val PER: 0.0919
2026-01-09 11:49:05,707: t15.2023.08.27 val PER: 0.2026
2026-01-09 11:49:05,707: t15.2023.09.01 val PER: 0.0860
2026-01-09 11:49:05,707: t15.2023.09.03 val PER: 0.1793
2026-01-09 11:49:05,707: t15.2023.09.24 val PER: 0.1347
2026-01-09 11:49:05,707: t15.2023.09.29 val PER: 0.1436
2026-01-09 11:49:05,707: t15.2023.10.01 val PER: 0.1823
2026-01-09 11:49:05,707: t15.2023.10.06 val PER: 0.0958
2026-01-09 11:49:05,707: t15.2023.10.08 val PER: 0.2612
2026-01-09 11:49:05,707: t15.2023.10.13 val PER: 0.2227
2026-01-09 11:49:05,707: t15.2023.10.15 val PER: 0.1727
2026-01-09 11:49:05,707: t15.2023.10.20 val PER: 0.1879
2026-01-09 11:49:05,708: t15.2023.10.22 val PER: 0.1169
2026-01-09 11:49:05,708: t15.2023.11.03 val PER: 0.1940
2026-01-09 11:49:05,708: t15.2023.11.04 val PER: 0.0341
2026-01-09 11:49:05,708: t15.2023.11.17 val PER: 0.0575
2026-01-09 11:49:05,708: t15.2023.11.19 val PER: 0.0579
2026-01-09 11:49:05,708: t15.2023.11.26 val PER: 0.1565
2026-01-09 11:49:05,708: t15.2023.12.03 val PER: 0.1345
2026-01-09 11:49:05,708: t15.2023.12.08 val PER: 0.1345
2026-01-09 11:49:05,708: t15.2023.12.10 val PER: 0.1117
2026-01-09 11:49:05,708: t15.2023.12.17 val PER: 0.1518
2026-01-09 11:49:05,708: t15.2023.12.29 val PER: 0.1510
2026-01-09 11:49:05,708: t15.2024.02.25 val PER: 0.1236
2026-01-09 11:49:05,709: t15.2024.03.08 val PER: 0.2646
2026-01-09 11:49:05,709: t15.2024.03.15 val PER: 0.2245
2026-01-09 11:49:05,709: t15.2024.03.17 val PER: 0.1541
2026-01-09 11:49:05,709: t15.2024.05.10 val PER: 0.1783
2026-01-09 11:49:05,709: t15.2024.06.14 val PER: 0.1814
2026-01-09 11:49:05,709: t15.2024.07.19 val PER: 0.2690
2026-01-09 11:49:05,709: t15.2024.07.21 val PER: 0.1145
2026-01-09 11:49:05,709: t15.2024.07.28 val PER: 0.1485
2026-01-09 11:49:05,709: t15.2025.01.10 val PER: 0.3058
2026-01-09 11:49:05,709: t15.2025.01.12 val PER: 0.1694
2026-01-09 11:49:05,709: t15.2025.03.14 val PER: 0.3447
2026-01-09 11:49:05,709: t15.2025.03.16 val PER: 0.2094
2026-01-09 11:49:05,709: t15.2025.03.30 val PER: 0.3241
2026-01-09 11:49:05,710: t15.2025.04.13 val PER: 0.2354
2026-01-09 11:49:05,846: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_16500
2026-01-09 11:49:14,388: Train batch 16600: loss: 11.42 grad norm: 56.47 time: 0.061
2026-01-09 11:49:31,602: Train batch 16800: loss: 21.55 grad norm: 81.45 time: 0.063
2026-01-09 11:49:48,915: Train batch 17000: loss: 11.09 grad norm: 53.05 time: 0.083
2026-01-09 11:49:48,915: Running test after training batch: 17000
2026-01-09 11:49:49,015: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:49:54,117: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:49:54,160: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-09 11:50:05,846: Val batch 17000: PER (avg): 0.1689 CTC Loss (avg): 16.7560 WER(5gram): 16.10% (n=256) time: 16.930
2026-01-09 11:50:05,846: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-09 11:50:05,846: t15.2023.08.13 val PER: 0.1258
2026-01-09 11:50:05,846: t15.2023.08.18 val PER: 0.1299
2026-01-09 11:50:05,846: t15.2023.08.20 val PER: 0.1271
2026-01-09 11:50:05,846: t15.2023.08.25 val PER: 0.0994
2026-01-09 11:50:05,846: t15.2023.08.27 val PER: 0.2058
2026-01-09 11:50:05,847: t15.2023.09.01 val PER: 0.0909
2026-01-09 11:50:05,847: t15.2023.09.03 val PER: 0.1829
2026-01-09 11:50:05,847: t15.2023.09.24 val PER: 0.1396
2026-01-09 11:50:05,847: t15.2023.09.29 val PER: 0.1417
2026-01-09 11:50:05,847: t15.2023.10.01 val PER: 0.1876
2026-01-09 11:50:05,847: t15.2023.10.06 val PER: 0.1001
2026-01-09 11:50:05,847: t15.2023.10.08 val PER: 0.2612
2026-01-09 11:50:05,847: t15.2023.10.13 val PER: 0.2227
2026-01-09 11:50:05,847: t15.2023.10.15 val PER: 0.1688
2026-01-09 11:50:05,847: t15.2023.10.20 val PER: 0.1812
2026-01-09 11:50:05,847: t15.2023.10.22 val PER: 0.1203
2026-01-09 11:50:05,847: t15.2023.11.03 val PER: 0.1967
2026-01-09 11:50:05,847: t15.2023.11.04 val PER: 0.0478
2026-01-09 11:50:05,847: t15.2023.11.17 val PER: 0.0544
2026-01-09 11:50:05,847: t15.2023.11.19 val PER: 0.0499
2026-01-09 11:50:05,847: t15.2023.11.26 val PER: 0.1529
2026-01-09 11:50:05,848: t15.2023.12.03 val PER: 0.1282
2026-01-09 11:50:05,848: t15.2023.12.08 val PER: 0.1245
2026-01-09 11:50:05,848: t15.2023.12.10 val PER: 0.1051
2026-01-09 11:50:05,848: t15.2023.12.17 val PER: 0.1518
2026-01-09 11:50:05,848: t15.2023.12.29 val PER: 0.1613
2026-01-09 11:50:05,848: t15.2024.02.25 val PER: 0.1236
2026-01-09 11:50:05,848: t15.2024.03.08 val PER: 0.2575
2026-01-09 11:50:05,848: t15.2024.03.15 val PER: 0.2295
2026-01-09 11:50:05,848: t15.2024.03.17 val PER: 0.1583
2026-01-09 11:50:05,848: t15.2024.05.10 val PER: 0.1842
2026-01-09 11:50:05,848: t15.2024.06.14 val PER: 0.1767
2026-01-09 11:50:05,848: t15.2024.07.19 val PER: 0.2657
2026-01-09 11:50:05,848: t15.2024.07.21 val PER: 0.1090
2026-01-09 11:50:05,849: t15.2024.07.28 val PER: 0.1456
2026-01-09 11:50:05,849: t15.2025.01.10 val PER: 0.3058
2026-01-09 11:50:05,849: t15.2025.01.12 val PER: 0.1694
2026-01-09 11:50:05,849: t15.2025.03.14 val PER: 0.3476
2026-01-09 11:50:05,849: t15.2025.03.16 val PER: 0.2081
2026-01-09 11:50:05,849: t15.2025.03.30 val PER: 0.3264
2026-01-09 11:50:05,849: t15.2025.04.13 val PER: 0.2297
2026-01-09 11:50:05,988: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_17000
2026-01-09 11:50:23,035: Train batch 17200: loss: 13.52 grad norm: 56.26 time: 0.085
2026-01-09 11:50:40,251: Train batch 17400: loss: 15.90 grad norm: 65.64 time: 0.072
2026-01-09 11:50:48,837: Running test after training batch: 17500
2026-01-09 11:50:48,979: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:50:53,945: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:50:53,986: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:51:05,343: Val batch 17500: PER (avg): 0.1678 CTC Loss (avg): 16.7189 WER(5gram): 15.12% (n=256) time: 16.506
2026-01-09 11:51:05,343: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-09 11:51:05,343: t15.2023.08.13 val PER: 0.1258
2026-01-09 11:51:05,344: t15.2023.08.18 val PER: 0.1282
2026-01-09 11:51:05,344: t15.2023.08.20 val PER: 0.1303
2026-01-09 11:51:05,344: t15.2023.08.25 val PER: 0.0994
2026-01-09 11:51:05,344: t15.2023.08.27 val PER: 0.1961
2026-01-09 11:51:05,344: t15.2023.09.01 val PER: 0.0885
2026-01-09 11:51:05,344: t15.2023.09.03 val PER: 0.1781
2026-01-09 11:51:05,344: t15.2023.09.24 val PER: 0.1311
2026-01-09 11:51:05,344: t15.2023.09.29 val PER: 0.1417
2026-01-09 11:51:05,344: t15.2023.10.01 val PER: 0.1843
2026-01-09 11:51:05,344: t15.2023.10.06 val PER: 0.0947
2026-01-09 11:51:05,344: t15.2023.10.08 val PER: 0.2598
2026-01-09 11:51:05,345: t15.2023.10.13 val PER: 0.2258
2026-01-09 11:51:05,345: t15.2023.10.15 val PER: 0.1707
2026-01-09 11:51:05,345: t15.2023.10.20 val PER: 0.1879
2026-01-09 11:51:05,345: t15.2023.10.22 val PER: 0.1147
2026-01-09 11:51:05,345: t15.2023.11.03 val PER: 0.1934
2026-01-09 11:51:05,345: t15.2023.11.04 val PER: 0.0375
2026-01-09 11:51:05,345: t15.2023.11.17 val PER: 0.0575
2026-01-09 11:51:05,345: t15.2023.11.19 val PER: 0.0479
2026-01-09 11:51:05,345: t15.2023.11.26 val PER: 0.1529
2026-01-09 11:51:05,345: t15.2023.12.03 val PER: 0.1324
2026-01-09 11:51:05,345: t15.2023.12.08 val PER: 0.1325
2026-01-09 11:51:05,345: t15.2023.12.10 val PER: 0.1078
2026-01-09 11:51:05,345: t15.2023.12.17 val PER: 0.1518
2026-01-09 11:51:05,346: t15.2023.12.29 val PER: 0.1537
2026-01-09 11:51:05,346: t15.2024.02.25 val PER: 0.1250
2026-01-09 11:51:05,346: t15.2024.03.08 val PER: 0.2660
2026-01-09 11:51:05,346: t15.2024.03.15 val PER: 0.2239
2026-01-09 11:51:05,346: t15.2024.03.17 val PER: 0.1548
2026-01-09 11:51:05,346: t15.2024.05.10 val PER: 0.1798
2026-01-09 11:51:05,346: t15.2024.06.14 val PER: 0.1735
2026-01-09 11:51:05,346: t15.2024.07.19 val PER: 0.2617
2026-01-09 11:51:05,346: t15.2024.07.21 val PER: 0.1076
2026-01-09 11:51:05,346: t15.2024.07.28 val PER: 0.1434
2026-01-09 11:51:05,346: t15.2025.01.10 val PER: 0.3113
2026-01-09 11:51:05,346: t15.2025.01.12 val PER: 0.1671
2026-01-09 11:51:05,346: t15.2025.03.14 val PER: 0.3447
2026-01-09 11:51:05,346: t15.2025.03.16 val PER: 0.2081
2026-01-09 11:51:05,346: t15.2025.03.30 val PER: 0.3253
2026-01-09 11:51:05,346: t15.2025.04.13 val PER: 0.2354
2026-01-09 11:51:05,348: New best val WER(5gram) 15.78% --> 15.12%
2026-01-09 11:51:05,534: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_17500
2026-01-09 11:51:14,039: Train batch 17600: loss: 14.60 grad norm: 69.62 time: 0.053
2026-01-09 11:51:31,219: Train batch 17800: loss: 8.63 grad norm: 58.29 time: 0.044
2026-01-09 11:51:48,230: Train batch 18000: loss: 15.33 grad norm: 69.32 time: 0.062
2026-01-09 11:51:48,230: Running test after training batch: 18000
2026-01-09 11:51:48,330: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:51:53,780: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:51:53,823: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:52:05,455: Val batch 18000: PER (avg): 0.1673 CTC Loss (avg): 16.6810 WER(5gram): 16.10% (n=256) time: 17.224
2026-01-09 11:52:05,456: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-09 11:52:05,456: t15.2023.08.13 val PER: 0.1279
2026-01-09 11:52:05,456: t15.2023.08.18 val PER: 0.1215
2026-01-09 11:52:05,456: t15.2023.08.20 val PER: 0.1239
2026-01-09 11:52:05,456: t15.2023.08.25 val PER: 0.0934
2026-01-09 11:52:05,456: t15.2023.08.27 val PER: 0.2058
2026-01-09 11:52:05,456: t15.2023.09.01 val PER: 0.0869
2026-01-09 11:52:05,456: t15.2023.09.03 val PER: 0.1853
2026-01-09 11:52:05,456: t15.2023.09.24 val PER: 0.1371
2026-01-09 11:52:05,456: t15.2023.09.29 val PER: 0.1417
2026-01-09 11:52:05,456: t15.2023.10.01 val PER: 0.1797
2026-01-09 11:52:05,456: t15.2023.10.06 val PER: 0.0904
2026-01-09 11:52:05,457: t15.2023.10.08 val PER: 0.2625
2026-01-09 11:52:05,457: t15.2023.10.13 val PER: 0.2227
2026-01-09 11:52:05,457: t15.2023.10.15 val PER: 0.1701
2026-01-09 11:52:05,457: t15.2023.10.20 val PER: 0.1846
2026-01-09 11:52:05,457: t15.2023.10.22 val PER: 0.1225
2026-01-09 11:52:05,457: t15.2023.11.03 val PER: 0.1954
2026-01-09 11:52:05,457: t15.2023.11.04 val PER: 0.0375
2026-01-09 11:52:05,457: t15.2023.11.17 val PER: 0.0607
2026-01-09 11:52:05,457: t15.2023.11.19 val PER: 0.0539
2026-01-09 11:52:05,457: t15.2023.11.26 val PER: 0.1558
2026-01-09 11:52:05,457: t15.2023.12.03 val PER: 0.1313
2026-01-09 11:52:05,457: t15.2023.12.08 val PER: 0.1312
2026-01-09 11:52:05,457: t15.2023.12.10 val PER: 0.1064
2026-01-09 11:52:05,458: t15.2023.12.17 val PER: 0.1528
2026-01-09 11:52:05,458: t15.2023.12.29 val PER: 0.1558
2026-01-09 11:52:05,458: t15.2024.02.25 val PER: 0.1180
2026-01-09 11:52:05,458: t15.2024.03.08 val PER: 0.2674
2026-01-09 11:52:05,458: t15.2024.03.15 val PER: 0.2264
2026-01-09 11:52:05,458: t15.2024.03.17 val PER: 0.1492
2026-01-09 11:52:05,458: t15.2024.05.10 val PER: 0.1887
2026-01-09 11:52:05,458: t15.2024.06.14 val PER: 0.1719
2026-01-09 11:52:05,458: t15.2024.07.19 val PER: 0.2624
2026-01-09 11:52:05,458: t15.2024.07.21 val PER: 0.1041
2026-01-09 11:52:05,458: t15.2024.07.28 val PER: 0.1478
2026-01-09 11:52:05,458: t15.2025.01.10 val PER: 0.3044
2026-01-09 11:52:05,458: t15.2025.01.12 val PER: 0.1647
2026-01-09 11:52:05,459: t15.2025.03.14 val PER: 0.3343
2026-01-09 11:52:05,459: t15.2025.03.16 val PER: 0.2068
2026-01-09 11:52:05,459: t15.2025.03.30 val PER: 0.3218
2026-01-09 11:52:05,459: t15.2025.04.13 val PER: 0.2297
2026-01-09 11:52:05,594: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_18000
2026-01-09 11:52:22,874: Train batch 18200: loss: 11.39 grad norm: 56.52 time: 0.074
2026-01-09 11:52:39,935: Train batch 18400: loss: 7.97 grad norm: 56.09 time: 0.059
2026-01-09 11:52:48,562: Running test after training batch: 18500
2026-01-09 11:52:48,701: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:52:54,342: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:52:54,390: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:53:06,197: Val batch 18500: PER (avg): 0.1675 CTC Loss (avg): 16.6584 WER(5gram): 15.38% (n=256) time: 17.635
2026-01-09 11:53:06,198: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-09 11:53:06,198: t15.2023.08.13 val PER: 0.1268
2026-01-09 11:53:06,198: t15.2023.08.18 val PER: 0.1266
2026-01-09 11:53:06,198: t15.2023.08.20 val PER: 0.1263
2026-01-09 11:53:06,198: t15.2023.08.25 val PER: 0.0934
2026-01-09 11:53:06,198: t15.2023.08.27 val PER: 0.2058
2026-01-09 11:53:06,198: t15.2023.09.01 val PER: 0.0877
2026-01-09 11:53:06,198: t15.2023.09.03 val PER: 0.1781
2026-01-09 11:53:06,198: t15.2023.09.24 val PER: 0.1335
2026-01-09 11:53:06,199: t15.2023.09.29 val PER: 0.1436
2026-01-09 11:53:06,199: t15.2023.10.01 val PER: 0.1836
2026-01-09 11:53:06,199: t15.2023.10.06 val PER: 0.0969
2026-01-09 11:53:06,199: t15.2023.10.08 val PER: 0.2612
2026-01-09 11:53:06,199: t15.2023.10.13 val PER: 0.2242
2026-01-09 11:53:06,199: t15.2023.10.15 val PER: 0.1740
2026-01-09 11:53:06,199: t15.2023.10.20 val PER: 0.1846
2026-01-09 11:53:06,199: t15.2023.10.22 val PER: 0.1203
2026-01-09 11:53:06,199: t15.2023.11.03 val PER: 0.1974
2026-01-09 11:53:06,199: t15.2023.11.04 val PER: 0.0341
2026-01-09 11:53:06,199: t15.2023.11.17 val PER: 0.0529
2026-01-09 11:53:06,199: t15.2023.11.19 val PER: 0.0519
2026-01-09 11:53:06,199: t15.2023.11.26 val PER: 0.1529
2026-01-09 11:53:06,199: t15.2023.12.03 val PER: 0.1282
2026-01-09 11:53:06,199: t15.2023.12.08 val PER: 0.1238
2026-01-09 11:53:06,199: t15.2023.12.10 val PER: 0.1117
2026-01-09 11:53:06,199: t15.2023.12.17 val PER: 0.1476
2026-01-09 11:53:06,200: t15.2023.12.29 val PER: 0.1517
2026-01-09 11:53:06,200: t15.2024.02.25 val PER: 0.1166
2026-01-09 11:53:06,202: t15.2024.03.08 val PER: 0.2717
2026-01-09 11:53:06,203: t15.2024.03.15 val PER: 0.2245
2026-01-09 11:53:06,203: t15.2024.03.17 val PER: 0.1506
2026-01-09 11:53:06,203: t15.2024.05.10 val PER: 0.1813
2026-01-09 11:53:06,203: t15.2024.06.14 val PER: 0.1751
2026-01-09 11:53:06,203: t15.2024.07.19 val PER: 0.2597
2026-01-09 11:53:06,203: t15.2024.07.21 val PER: 0.1131
2026-01-09 11:53:06,203: t15.2024.07.28 val PER: 0.1493
2026-01-09 11:53:06,203: t15.2025.01.10 val PER: 0.3099
2026-01-09 11:53:06,203: t15.2025.01.12 val PER: 0.1624
2026-01-09 11:53:06,203: t15.2025.03.14 val PER: 0.3417
2026-01-09 11:53:06,203: t15.2025.03.16 val PER: 0.2147
2026-01-09 11:53:06,204: t15.2025.03.30 val PER: 0.3195
2026-01-09 11:53:06,204: t15.2025.04.13 val PER: 0.2325
2026-01-09 11:53:06,338: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_18500
2026-01-09 11:53:14,805: Train batch 18600: loss: 16.44 grad norm: 69.68 time: 0.068
2026-01-09 11:53:31,882: Train batch 18800: loss: 12.09 grad norm: 57.34 time: 0.065
2026-01-09 11:53:49,198: Train batch 19000: loss: 10.98 grad norm: 49.51 time: 0.065
2026-01-09 11:53:49,198: Running test after training batch: 19000
2026-01-09 11:53:49,324: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:53:54,259: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:53:54,303: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:54:06,104: Val batch 19000: PER (avg): 0.1666 CTC Loss (avg): 16.6244 WER(5gram): 14.93% (n=256) time: 16.905
2026-01-09 11:54:06,104: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-09 11:54:06,104: t15.2023.08.13 val PER: 0.1237
2026-01-09 11:54:06,104: t15.2023.08.18 val PER: 0.1232
2026-01-09 11:54:06,105: t15.2023.08.20 val PER: 0.1271
2026-01-09 11:54:06,105: t15.2023.08.25 val PER: 0.0934
2026-01-09 11:54:06,105: t15.2023.08.27 val PER: 0.1977
2026-01-09 11:54:06,105: t15.2023.09.01 val PER: 0.0860
2026-01-09 11:54:06,105: t15.2023.09.03 val PER: 0.1781
2026-01-09 11:54:06,105: t15.2023.09.24 val PER: 0.1359
2026-01-09 11:54:06,105: t15.2023.09.29 val PER: 0.1429
2026-01-09 11:54:06,105: t15.2023.10.01 val PER: 0.1797
2026-01-09 11:54:06,105: t15.2023.10.06 val PER: 0.0893
2026-01-09 11:54:06,105: t15.2023.10.08 val PER: 0.2639
2026-01-09 11:54:06,105: t15.2023.10.13 val PER: 0.2188
2026-01-09 11:54:06,105: t15.2023.10.15 val PER: 0.1674
2026-01-09 11:54:06,105: t15.2023.10.20 val PER: 0.1879
2026-01-09 11:54:06,105: t15.2023.10.22 val PER: 0.1225
2026-01-09 11:54:06,105: t15.2023.11.03 val PER: 0.1967
2026-01-09 11:54:06,105: t15.2023.11.04 val PER: 0.0307
2026-01-09 11:54:06,106: t15.2023.11.17 val PER: 0.0560
2026-01-09 11:54:06,106: t15.2023.11.19 val PER: 0.0479
2026-01-09 11:54:06,106: t15.2023.11.26 val PER: 0.1543
2026-01-09 11:54:06,106: t15.2023.12.03 val PER: 0.1334
2026-01-09 11:54:06,106: t15.2023.12.08 val PER: 0.1258
2026-01-09 11:54:06,106: t15.2023.12.10 val PER: 0.1130
2026-01-09 11:54:06,106: t15.2023.12.17 val PER: 0.1476
2026-01-09 11:54:06,106: t15.2023.12.29 val PER: 0.1531
2026-01-09 11:54:06,106: t15.2024.02.25 val PER: 0.1180
2026-01-09 11:54:06,106: t15.2024.03.08 val PER: 0.2660
2026-01-09 11:54:06,106: t15.2024.03.15 val PER: 0.2189
2026-01-09 11:54:06,106: t15.2024.03.17 val PER: 0.1513
2026-01-09 11:54:06,106: t15.2024.05.10 val PER: 0.1857
2026-01-09 11:54:06,106: t15.2024.06.14 val PER: 0.1782
2026-01-09 11:54:06,107: t15.2024.07.19 val PER: 0.2650
2026-01-09 11:54:06,107: t15.2024.07.21 val PER: 0.1110
2026-01-09 11:54:06,107: t15.2024.07.28 val PER: 0.1426
2026-01-09 11:54:06,107: t15.2025.01.10 val PER: 0.3044
2026-01-09 11:54:06,107: t15.2025.01.12 val PER: 0.1647
2026-01-09 11:54:06,107: t15.2025.03.14 val PER: 0.3432
2026-01-09 11:54:06,107: t15.2025.03.16 val PER: 0.2055
2026-01-09 11:54:06,107: t15.2025.03.30 val PER: 0.3253
2026-01-09 11:54:06,107: t15.2025.04.13 val PER: 0.2311
2026-01-09 11:54:06,108: New best val WER(5gram) 15.12% --> 14.93%
2026-01-09 11:54:06,301: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_19000
2026-01-09 11:54:23,701: Train batch 19200: loss: 8.53 grad norm: 52.84 time: 0.064
2026-01-09 11:54:40,795: Train batch 19400: loss: 7.11 grad norm: 41.14 time: 0.054
2026-01-09 11:54:49,422: Running test after training batch: 19500
2026-01-09 11:54:49,520: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:54:54,952: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:54:55,004: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:55:06,973: Val batch 19500: PER (avg): 0.1663 CTC Loss (avg): 16.5912 WER(5gram): 14.67% (n=256) time: 17.550
2026-01-09 11:55:06,973: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-09 11:55:06,973: t15.2023.08.13 val PER: 0.1216
2026-01-09 11:55:06,973: t15.2023.08.18 val PER: 0.1299
2026-01-09 11:55:06,974: t15.2023.08.20 val PER: 0.1263
2026-01-09 11:55:06,974: t15.2023.08.25 val PER: 0.0934
2026-01-09 11:55:06,974: t15.2023.08.27 val PER: 0.1994
2026-01-09 11:55:06,974: t15.2023.09.01 val PER: 0.0893
2026-01-09 11:55:06,974: t15.2023.09.03 val PER: 0.1770
2026-01-09 11:55:06,974: t15.2023.09.24 val PER: 0.1371
2026-01-09 11:55:06,974: t15.2023.09.29 val PER: 0.1410
2026-01-09 11:55:06,974: t15.2023.10.01 val PER: 0.1777
2026-01-09 11:55:06,974: t15.2023.10.06 val PER: 0.0915
2026-01-09 11:55:06,974: t15.2023.10.08 val PER: 0.2585
2026-01-09 11:55:06,974: t15.2023.10.13 val PER: 0.2211
2026-01-09 11:55:06,974: t15.2023.10.15 val PER: 0.1721
2026-01-09 11:55:06,974: t15.2023.10.20 val PER: 0.1812
2026-01-09 11:55:06,974: t15.2023.10.22 val PER: 0.1203
2026-01-09 11:55:06,974: t15.2023.11.03 val PER: 0.1927
2026-01-09 11:55:06,975: t15.2023.11.04 val PER: 0.0375
2026-01-09 11:55:06,975: t15.2023.11.17 val PER: 0.0575
2026-01-09 11:55:06,975: t15.2023.11.19 val PER: 0.0499
2026-01-09 11:55:06,975: t15.2023.11.26 val PER: 0.1493
2026-01-09 11:55:06,975: t15.2023.12.03 val PER: 0.1271
2026-01-09 11:55:06,975: t15.2023.12.08 val PER: 0.1258
2026-01-09 11:55:06,975: t15.2023.12.10 val PER: 0.1117
2026-01-09 11:55:06,975: t15.2023.12.17 val PER: 0.1455
2026-01-09 11:55:06,975: t15.2023.12.29 val PER: 0.1476
2026-01-09 11:55:06,975: t15.2024.02.25 val PER: 0.1236
2026-01-09 11:55:06,975: t15.2024.03.08 val PER: 0.2688
2026-01-09 11:55:06,975: t15.2024.03.15 val PER: 0.2170
2026-01-09 11:55:06,975: t15.2024.03.17 val PER: 0.1464
2026-01-09 11:55:06,975: t15.2024.05.10 val PER: 0.1872
2026-01-09 11:55:06,976: t15.2024.06.14 val PER: 0.1767
2026-01-09 11:55:06,976: t15.2024.07.19 val PER: 0.2637
2026-01-09 11:55:06,976: t15.2024.07.21 val PER: 0.1090
2026-01-09 11:55:06,976: t15.2024.07.28 val PER: 0.1478
2026-01-09 11:55:06,976: t15.2025.01.10 val PER: 0.3113
2026-01-09 11:55:06,976: t15.2025.01.12 val PER: 0.1671
2026-01-09 11:55:06,976: t15.2025.03.14 val PER: 0.3491
2026-01-09 11:55:06,976: t15.2025.03.16 val PER: 0.2029
2026-01-09 11:55:06,976: t15.2025.03.30 val PER: 0.3241
2026-01-09 11:55:06,976: t15.2025.04.13 val PER: 0.2311
2026-01-09 11:55:06,977: New best val WER(5gram) 14.93% --> 14.67%
2026-01-09 11:55:07,168: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_19500
2026-01-09 11:55:15,558: Train batch 19600: loss: 10.87 grad norm: 53.61 time: 0.058
2026-01-09 11:55:32,523: Train batch 19800: loss: 10.99 grad norm: 58.97 time: 0.056
2026-01-09 11:55:49,744: Running test after training batch: 19999
2026-01-09 11:55:49,833: WER debug GT example: You can see the code at this point as well.
2026-01-09 11:55:54,635: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-09 11:55:54,678: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-09 11:56:06,445: Val batch 19999: PER (avg): 0.1660 CTC Loss (avg): 16.5877 WER(5gram): 15.25% (n=256) time: 16.701
2026-01-09 11:56:06,446: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-09 11:56:06,446: t15.2023.08.13 val PER: 0.1237
2026-01-09 11:56:06,446: t15.2023.08.18 val PER: 0.1308
2026-01-09 11:56:06,446: t15.2023.08.20 val PER: 0.1247
2026-01-09 11:56:06,446: t15.2023.08.25 val PER: 0.0979
2026-01-09 11:56:06,447: t15.2023.08.27 val PER: 0.1994
2026-01-09 11:56:06,447: t15.2023.09.01 val PER: 0.0901
2026-01-09 11:56:06,447: t15.2023.09.03 val PER: 0.1734
2026-01-09 11:56:06,447: t15.2023.09.24 val PER: 0.1371
2026-01-09 11:56:06,447: t15.2023.09.29 val PER: 0.1366
2026-01-09 11:56:06,447: t15.2023.10.01 val PER: 0.1836
2026-01-09 11:56:06,448: t15.2023.10.06 val PER: 0.0883
2026-01-09 11:56:06,448: t15.2023.10.08 val PER: 0.2598
2026-01-09 11:56:06,448: t15.2023.10.13 val PER: 0.2211
2026-01-09 11:56:06,448: t15.2023.10.15 val PER: 0.1694
2026-01-09 11:56:06,448: t15.2023.10.20 val PER: 0.1779
2026-01-09 11:56:06,448: t15.2023.10.22 val PER: 0.1180
2026-01-09 11:56:06,448: t15.2023.11.03 val PER: 0.1947
2026-01-09 11:56:06,448: t15.2023.11.04 val PER: 0.0375
2026-01-09 11:56:06,449: t15.2023.11.17 val PER: 0.0544
2026-01-09 11:56:06,449: t15.2023.11.19 val PER: 0.0499
2026-01-09 11:56:06,449: t15.2023.11.26 val PER: 0.1522
2026-01-09 11:56:06,449: t15.2023.12.03 val PER: 0.1282
2026-01-09 11:56:06,449: t15.2023.12.08 val PER: 0.1245
2026-01-09 11:56:06,449: t15.2023.12.10 val PER: 0.1104
2026-01-09 11:56:06,449: t15.2023.12.17 val PER: 0.1497
2026-01-09 11:56:06,449: t15.2023.12.29 val PER: 0.1469
2026-01-09 11:56:06,449: t15.2024.02.25 val PER: 0.1264
2026-01-09 11:56:06,449: t15.2024.03.08 val PER: 0.2703
2026-01-09 11:56:06,450: t15.2024.03.15 val PER: 0.2214
2026-01-09 11:56:06,450: t15.2024.03.17 val PER: 0.1513
2026-01-09 11:56:06,450: t15.2024.05.10 val PER: 0.1798
2026-01-09 11:56:06,450: t15.2024.06.14 val PER: 0.1719
2026-01-09 11:56:06,450: t15.2024.07.19 val PER: 0.2610
2026-01-09 11:56:06,450: t15.2024.07.21 val PER: 0.1090
2026-01-09 11:56:06,450: t15.2024.07.28 val PER: 0.1449
2026-01-09 11:56:06,450: t15.2025.01.10 val PER: 0.3085
2026-01-09 11:56:06,450: t15.2025.01.12 val PER: 0.1617
2026-01-09 11:56:06,450: t15.2025.03.14 val PER: 0.3491
2026-01-09 11:56:06,450: t15.2025.03.16 val PER: 0.2055
2026-01-09 11:56:06,451: t15.2025.03.30 val PER: 0.3207
2026-01-09 11:56:06,451: t15.2025.04.13 val PER: 0.2225
2026-01-09 11:56:06,586: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0030/checkpoint/checkpoint_batch_19999
2026-01-09 11:56:07,127: Best avg val PER achieved: 0.16634
2026-01-09 11:56:07,127: Total training time: 45.88 minutes

=== RUN lr_0035.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035
2026-01-09 11:57:56,233: Using device: cuda:0
2026-01-09 12:01:46,886: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-09 12:01:46,911: Using 45 sessions after filtering (from 45).
2026-01-09 12:01:47,299: Using torch.compile (if available)
2026-01-09 12:01:47,299: torch.compile not available (torch<2.0). Skipping.
2026-01-09 12:01:47,299: Initialized RNN decoding model
2026-01-09 12:01:47,299: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-09 12:01:47,300: Model has 44,907,305 parameters
2026-01-09 12:01:47,300: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-09 12:01:48,597: Successfully initialized datasets
2026-01-09 12:01:48,598: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-09 12:01:50,420: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.485
2026-01-09 12:01:50,420: Running test after training batch: 0
2026-01-09 12:01:50,567: WER debug GT example: You can see the code at this point as well.
2026-01-09 12:01:56,384: WER debug example
  GT : you can see the code at this point as well
  PR : she has from his
2026-01-09 12:01:57,392: WER debug example
  GT : how does it keep the cost down
  PR : money from
2026-01-09 12:05:48,195: Val batch 0: PER (avg): 1.4301 CTC Loss (avg): 633.2735 WER(5gram): 99.67% (n=256) time: 237.775
2026-01-09 12:05:48,199: WER lens: avg_true_words=5.99 avg_pred_words=2.81 max_pred_words=7
2026-01-09 12:05:48,204: t15.2023.08.13 val PER: 1.3098
2026-01-09 12:05:48,209: t15.2023.08.18 val PER: 1.4267
2026-01-09 12:05:48,209: t15.2023.08.20 val PER: 1.2994
2026-01-09 12:05:48,210: t15.2023.08.25 val PER: 1.3389
2026-01-09 12:05:48,210: t15.2023.08.27 val PER: 1.2572
2026-01-09 12:05:48,210: t15.2023.09.01 val PER: 1.4513
2026-01-09 12:05:48,210: t15.2023.09.03 val PER: 1.3147
2026-01-09 12:05:48,210: t15.2023.09.24 val PER: 1.5461
2026-01-09 12:05:48,210: t15.2023.09.29 val PER: 1.4678
2026-01-09 12:05:48,210: t15.2023.10.01 val PER: 1.2127
2026-01-09 12:05:48,210: t15.2023.10.06 val PER: 1.4898
2026-01-09 12:05:48,210: t15.2023.10.08 val PER: 1.1800
2026-01-09 12:05:48,211: t15.2023.10.13 val PER: 1.3980
2026-01-09 12:05:48,211: t15.2023.10.15 val PER: 1.3929
2026-01-09 12:05:48,211: t15.2023.10.20 val PER: 1.4966
2026-01-09 12:05:48,211: t15.2023.10.22 val PER: 1.3886
2026-01-09 12:05:48,211: t15.2023.11.03 val PER: 1.5957
2026-01-09 12:05:48,211: t15.2023.11.04 val PER: 2.0205
2026-01-09 12:05:48,211: t15.2023.11.17 val PER: 1.9580
2026-01-09 12:05:48,211: t15.2023.11.19 val PER: 1.6786
2026-01-09 12:05:48,211: t15.2023.11.26 val PER: 1.5384
2026-01-09 12:05:48,211: t15.2023.12.03 val PER: 1.4244
2026-01-09 12:05:48,211: t15.2023.12.08 val PER: 1.4514
2026-01-09 12:05:48,211: t15.2023.12.10 val PER: 1.7057
2026-01-09 12:05:48,211: t15.2023.12.17 val PER: 1.3056
2026-01-09 12:05:48,212: t15.2023.12.29 val PER: 1.4063
2026-01-09 12:05:48,212: t15.2024.02.25 val PER: 1.4284
2026-01-09 12:05:48,212: t15.2024.03.08 val PER: 1.3257
2026-01-09 12:05:48,212: t15.2024.03.15 val PER: 1.3177
2026-01-09 12:05:48,212: t15.2024.03.17 val PER: 1.4024
2026-01-09 12:05:48,212: t15.2024.05.10 val PER: 1.3210
2026-01-09 12:05:48,212: t15.2024.06.14 val PER: 1.5347
2026-01-09 12:05:48,212: t15.2024.07.19 val PER: 1.0877
2026-01-09 12:05:48,212: t15.2024.07.21 val PER: 1.6331
2026-01-09 12:05:48,212: t15.2024.07.28 val PER: 1.6537
2026-01-09 12:05:48,212: t15.2025.01.10 val PER: 1.0909
2026-01-09 12:05:48,213: t15.2025.01.12 val PER: 1.7637
2026-01-09 12:05:48,213: t15.2025.03.14 val PER: 1.0355
2026-01-09 12:05:48,213: t15.2025.03.16 val PER: 1.6257
2026-01-09 12:05:48,213: t15.2025.03.30 val PER: 1.2828
2026-01-09 12:05:48,213: t15.2025.04.13 val PER: 1.5877
2026-01-09 12:05:48,214: New best val WER(5gram) inf% --> 99.67%
2026-01-09 12:05:48,392: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_0
2026-01-09 12:06:05,434: Train batch 200: loss: 79.00 grad norm: 88.91 time: 0.055
2026-01-09 12:06:22,093: Train batch 400: loss: 55.57 grad norm: 92.19 time: 0.064
2026-01-09 12:06:30,530: Running test after training batch: 500
2026-01-09 12:06:30,626: WER debug GT example: You can see the code at this point as well.
2026-01-09 12:06:35,718: WER debug example
  GT : you can see the code at this point as well
  PR : you'll need is thus had at this guide is all
2026-01-09 12:06:35,879: WER debug example
  GT : how does it keep the cost down
  PR : and does it do this is as
2026-01-09 12:07:26,733: Val batch 500: PER (avg): 0.5281 CTC Loss (avg): 56.6861 WER(5gram): 76.14% (n=256) time: 56.202
2026-01-09 12:07:26,733: WER lens: avg_true_words=5.99 avg_pred_words=5.68 max_pred_words=12
2026-01-09 12:07:26,733: t15.2023.08.13 val PER: 0.4761
2026-01-09 12:07:26,734: t15.2023.08.18 val PER: 0.4635
2026-01-09 12:07:26,734: t15.2023.08.20 val PER: 0.4567
2026-01-09 12:07:26,734: t15.2023.08.25 val PER: 0.4533
2026-01-09 12:07:26,734: t15.2023.08.27 val PER: 0.5450
2026-01-09 12:07:26,734: t15.2023.09.01 val PER: 0.4286
2026-01-09 12:07:26,734: t15.2023.09.03 val PER: 0.5107
2026-01-09 12:07:26,734: t15.2023.09.24 val PER: 0.4502
2026-01-09 12:07:26,734: t15.2023.09.29 val PER: 0.4786
2026-01-09 12:07:26,734: t15.2023.10.01 val PER: 0.5297
2026-01-09 12:07:26,734: t15.2023.10.06 val PER: 0.4435
2026-01-09 12:07:26,734: t15.2023.10.08 val PER: 0.5535
2026-01-09 12:07:26,734: t15.2023.10.13 val PER: 0.5756
2026-01-09 12:07:26,734: t15.2023.10.15 val PER: 0.5049
2026-01-09 12:07:26,734: t15.2023.10.20 val PER: 0.4631
2026-01-09 12:07:26,734: t15.2023.10.22 val PER: 0.4688
2026-01-09 12:07:26,735: t15.2023.11.03 val PER: 0.5156
2026-01-09 12:07:26,735: t15.2023.11.04 val PER: 0.2935
2026-01-09 12:07:26,735: t15.2023.11.17 val PER: 0.3608
2026-01-09 12:07:26,735: t15.2023.11.19 val PER: 0.3653
2026-01-09 12:07:26,735: t15.2023.11.26 val PER: 0.5630
2026-01-09 12:07:26,735: t15.2023.12.03 val PER: 0.5032
2026-01-09 12:07:26,735: t15.2023.12.08 val PER: 0.5273
2026-01-09 12:07:26,735: t15.2023.12.10 val PER: 0.4573
2026-01-09 12:07:26,735: t15.2023.12.17 val PER: 0.5707
2026-01-09 12:07:26,735: t15.2023.12.29 val PER: 0.5539
2026-01-09 12:07:26,736: t15.2024.02.25 val PER: 0.5000
2026-01-09 12:07:26,736: t15.2024.03.08 val PER: 0.6216
2026-01-09 12:07:26,736: t15.2024.03.15 val PER: 0.5591
2026-01-09 12:07:26,736: t15.2024.03.17 val PER: 0.5132
2026-01-09 12:07:26,736: t15.2024.05.10 val PER: 0.5453
2026-01-09 12:07:26,736: t15.2024.06.14 val PER: 0.5126
2026-01-09 12:07:26,736: t15.2024.07.19 val PER: 0.6816
2026-01-09 12:07:26,736: t15.2024.07.21 val PER: 0.4883
2026-01-09 12:07:26,736: t15.2024.07.28 val PER: 0.5235
2026-01-09 12:07:26,736: t15.2025.01.10 val PER: 0.7521
2026-01-09 12:07:26,736: t15.2025.01.12 val PER: 0.5627
2026-01-09 12:07:26,736: t15.2025.03.14 val PER: 0.7470
2026-01-09 12:07:26,736: t15.2025.03.16 val PER: 0.6073
2026-01-09 12:07:26,737: t15.2025.03.30 val PER: 0.7253
2026-01-09 12:07:26,737: t15.2025.04.13 val PER: 0.5792
2026-01-09 12:07:26,738: New best val WER(5gram) 99.67% --> 76.14%
2026-01-09 12:07:26,919: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_500
2026-01-09 12:07:35,327: Train batch 600: loss: 49.98 grad norm: 82.37 time: 0.078
2026-01-09 12:07:52,543: Train batch 800: loss: 42.66 grad norm: 88.71 time: 0.058
2026-01-09 12:08:09,570: Train batch 1000: loss: 44.00 grad norm: 80.39 time: 0.068
2026-01-09 12:08:09,570: Running test after training batch: 1000
2026-01-09 12:08:09,701: WER debug GT example: You can see the code at this point as well.
2026-01-09 12:08:14,769: WER debug example
  GT : you can see the code at this point as well
  PR : you'd get is that good at it and is well
2026-01-09 12:08:14,887: WER debug example
  GT : how does it keep the cost down
  PR : and as it is that it's at
2026-01-09 12:08:45,543: Val batch 1000: PER (avg): 0.4195 CTC Loss (avg): 43.7190 WER(5gram): 57.50% (n=256) time: 35.972
2026-01-09 12:08:45,543: WER lens: avg_true_words=5.99 avg_pred_words=5.64 max_pred_words=12
2026-01-09 12:08:45,543: t15.2023.08.13 val PER: 0.3960
2026-01-09 12:08:45,543: t15.2023.08.18 val PER: 0.3521
2026-01-09 12:08:45,543: t15.2023.08.20 val PER: 0.3574
2026-01-09 12:08:45,543: t15.2023.08.25 val PER: 0.3012
2026-01-09 12:08:45,544: t15.2023.08.27 val PER: 0.4148
2026-01-09 12:08:45,544: t15.2023.09.01 val PER: 0.3174
2026-01-09 12:08:45,544: t15.2023.09.03 val PER: 0.4038
2026-01-09 12:08:45,544: t15.2023.09.24 val PER: 0.3495
2026-01-09 12:08:45,544: t15.2023.09.29 val PER: 0.3752
2026-01-09 12:08:45,544: t15.2023.10.01 val PER: 0.4089
2026-01-09 12:08:45,544: t15.2023.10.06 val PER: 0.3154
2026-01-09 12:08:45,544: t15.2023.10.08 val PER: 0.4520
2026-01-09 12:08:45,544: t15.2023.10.13 val PER: 0.4701
2026-01-09 12:08:45,544: t15.2023.10.15 val PER: 0.3902
2026-01-09 12:08:45,544: t15.2023.10.20 val PER: 0.4027
2026-01-09 12:08:45,544: t15.2023.10.22 val PER: 0.3575
2026-01-09 12:08:45,544: t15.2023.11.03 val PER: 0.4043
2026-01-09 12:08:45,544: t15.2023.11.04 val PER: 0.1672
2026-01-09 12:08:45,545: t15.2023.11.17 val PER: 0.2753
2026-01-09 12:08:45,545: t15.2023.11.19 val PER: 0.2116
2026-01-09 12:08:45,545: t15.2023.11.26 val PER: 0.4580
2026-01-09 12:08:45,545: t15.2023.12.03 val PER: 0.4097
2026-01-09 12:08:45,545: t15.2023.12.08 val PER: 0.4201
2026-01-09 12:08:45,545: t15.2023.12.10 val PER: 0.3653
2026-01-09 12:08:45,545: t15.2023.12.17 val PER: 0.4116
2026-01-09 12:08:45,545: t15.2023.12.29 val PER: 0.4097
2026-01-09 12:08:45,545: t15.2024.02.25 val PER: 0.3736
2026-01-09 12:08:45,546: t15.2024.03.08 val PER: 0.5149
2026-01-09 12:08:45,546: t15.2024.03.15 val PER: 0.4522
2026-01-09 12:08:45,546: t15.2024.03.17 val PER: 0.4205
2026-01-09 12:08:45,546: t15.2024.05.10 val PER: 0.4339
2026-01-09 12:08:45,546: t15.2024.06.14 val PER: 0.4211
2026-01-09 12:08:45,546: t15.2024.07.19 val PER: 0.5498
2026-01-09 12:08:45,546: t15.2024.07.21 val PER: 0.3869
2026-01-09 12:08:45,546: t15.2024.07.28 val PER: 0.4272
2026-01-09 12:08:45,546: t15.2025.01.10 val PER: 0.6253
2026-01-09 12:08:45,546: t15.2025.01.12 val PER: 0.4650
2026-01-09 12:08:45,546: t15.2025.03.14 val PER: 0.6450
2026-01-09 12:08:45,546: t15.2025.03.16 val PER: 0.4921
2026-01-09 12:08:45,546: t15.2025.03.30 val PER: 0.6747
2026-01-09 12:08:45,547: t15.2025.04.13 val PER: 0.5193
2026-01-09 12:08:45,547: New best val WER(5gram) 76.14% --> 57.50%
2026-01-09 12:08:45,733: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_1000
2026-01-09 12:09:02,590: Train batch 1200: loss: 34.20 grad norm: 74.94 time: 0.069
2026-01-09 12:09:19,677: Train batch 1400: loss: 37.55 grad norm: 82.78 time: 0.061
2026-01-09 12:09:28,241: Running test after training batch: 1500
2026-01-09 12:09:28,394: WER debug GT example: You can see the code at this point as well.
2026-01-09 12:09:33,360: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-09 12:09:33,460: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost of
2026-01-09 12:09:51,968: Val batch 1500: PER (avg): 0.3911 CTC Loss (avg): 38.1992 WER(5gram): 39.05% (n=256) time: 23.727
2026-01-09 12:09:51,969: WER lens: avg_true_words=5.99 avg_pred_words=5.23 max_pred_words=12
2026-01-09 12:09:51,969: t15.2023.08.13 val PER: 0.3628
2026-01-09 12:09:51,969: t15.2023.08.18 val PER: 0.3328
2026-01-09 12:09:51,969: t15.2023.08.20 val PER: 0.3153
2026-01-09 12:09:51,969: t15.2023.08.25 val PER: 0.2636
2026-01-09 12:09:51,969: t15.2023.08.27 val PER: 0.4116
2026-01-09 12:09:51,969: t15.2023.09.01 val PER: 0.2873
2026-01-09 12:09:51,969: t15.2023.09.03 val PER: 0.4002
2026-01-09 12:09:51,969: t15.2023.09.24 val PER: 0.3265
2026-01-09 12:09:51,970: t15.2023.09.29 val PER: 0.3452
2026-01-09 12:09:51,970: t15.2023.10.01 val PER: 0.4069
2026-01-09 12:09:51,970: t15.2023.10.06 val PER: 0.2906
2026-01-09 12:09:51,970: t15.2023.10.08 val PER: 0.4425
2026-01-09 12:09:51,970: t15.2023.10.13 val PER: 0.4500
2026-01-09 12:09:51,970: t15.2023.10.15 val PER: 0.3731
2026-01-09 12:09:51,970: t15.2023.10.20 val PER: 0.3523
2026-01-09 12:09:51,970: t15.2023.10.22 val PER: 0.3229
2026-01-09 12:09:51,970: t15.2023.11.03 val PER: 0.3745
2026-01-09 12:09:51,970: t15.2023.11.04 val PER: 0.1365
2026-01-09 12:09:51,970: t15.2023.11.17 val PER: 0.2286
2026-01-09 12:09:51,970: t15.2023.11.19 val PER: 0.1816
2026-01-09 12:09:51,971: t15.2023.11.26 val PER: 0.4297
2026-01-09 12:09:51,971: t15.2023.12.03 val PER: 0.3761
2026-01-09 12:09:51,971: t15.2023.12.08 val PER: 0.3642
2026-01-09 12:09:51,971: t15.2023.12.10 val PER: 0.3009
2026-01-09 12:09:51,971: t15.2023.12.17 val PER: 0.3753
2026-01-09 12:09:51,971: t15.2023.12.29 val PER: 0.3892
2026-01-09 12:09:51,971: t15.2024.02.25 val PER: 0.3329
2026-01-09 12:09:51,971: t15.2024.03.08 val PER: 0.4595
2026-01-09 12:09:51,971: t15.2024.03.15 val PER: 0.4284
2026-01-09 12:09:51,971: t15.2024.03.17 val PER: 0.3835
2026-01-09 12:09:51,971: t15.2024.05.10 val PER: 0.3982
2026-01-09 12:09:51,971: t15.2024.06.14 val PER: 0.4069
2026-01-09 12:09:51,971: t15.2024.07.19 val PER: 0.5353
2026-01-09 12:09:51,972: t15.2024.07.21 val PER: 0.3524
2026-01-09 12:09:51,972: t15.2024.07.28 val PER: 0.3787
2026-01-09 12:09:51,972: t15.2025.01.10 val PER: 0.6419
2026-01-09 12:09:51,972: t15.2025.01.12 val PER: 0.4334
2026-01-09 12:09:51,972: t15.2025.03.14 val PER: 0.6154
2026-01-09 12:09:51,972: t15.2025.03.16 val PER: 0.4634
2026-01-09 12:09:51,972: t15.2025.03.30 val PER: 0.6621
2026-01-09 12:09:51,972: t15.2025.04.13 val PER: 0.4864
2026-01-09 12:09:51,973: New best val WER(5gram) 57.50% --> 39.05%
2026-01-09 12:09:52,160: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_1500
2026-01-09 12:10:00,531: Train batch 1600: loss: 37.99 grad norm: 82.97 time: 0.065
2026-01-09 12:10:17,773: Train batch 1800: loss: 35.88 grad norm: 71.27 time: 0.090
2026-01-09 12:10:35,163: Train batch 2000: loss: 34.98 grad norm: 70.83 time: 0.067
2026-01-09 12:10:35,163: Running test after training batch: 2000
2026-01-09 12:10:35,266: WER debug GT example: You can see the code at this point as well.
2026-01-09 12:10:40,435: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-09 12:10:40,508: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us and
2026-01-09 12:10:58,392: Val batch 2000: PER (avg): 0.3364 CTC Loss (avg): 33.9397 WER(5gram): 31.10% (n=256) time: 23.228
2026-01-09 12:10:58,392: WER lens: avg_true_words=5.99 avg_pred_words=5.70 max_pred_words=12
2026-01-09 12:10:58,393: t15.2023.08.13 val PER: 0.3056
2026-01-09 12:10:58,393: t15.2023.08.18 val PER: 0.2598
2026-01-09 12:10:58,393: t15.2023.08.20 val PER: 0.2502
2026-01-09 12:10:58,393: t15.2023.08.25 val PER: 0.2289
2026-01-09 12:10:58,393: t15.2023.08.27 val PER: 0.3457
2026-01-09 12:10:58,393: t15.2023.09.01 val PER: 0.2313
2026-01-09 12:10:58,393: t15.2023.09.03 val PER: 0.3432
2026-01-09 12:10:58,393: t15.2023.09.24 val PER: 0.2743
2026-01-09 12:10:58,393: t15.2023.09.29 val PER: 0.2840
2026-01-09 12:10:58,393: t15.2023.10.01 val PER: 0.3415
2026-01-09 12:10:58,393: t15.2023.10.06 val PER: 0.2540
2026-01-09 12:10:58,393: t15.2023.10.08 val PER: 0.4100
2026-01-09 12:10:58,393: t15.2023.10.13 val PER: 0.3887
2026-01-09 12:10:58,393: t15.2023.10.15 val PER: 0.3098
2026-01-09 12:10:58,393: t15.2023.10.20 val PER: 0.2886
2026-01-09 12:10:58,394: t15.2023.10.22 val PER: 0.2673
2026-01-09 12:10:58,394: t15.2023.11.03 val PER: 0.3290
2026-01-09 12:10:58,394: t15.2023.11.04 val PER: 0.0956
2026-01-09 12:10:58,394: t15.2023.11.17 val PER: 0.1726
2026-01-09 12:10:58,394: t15.2023.11.19 val PER: 0.1597
2026-01-09 12:10:58,394: t15.2023.11.26 val PER: 0.3775
2026-01-09 12:10:58,394: t15.2023.12.03 val PER: 0.3246
2026-01-09 12:10:58,394: t15.2023.12.08 val PER: 0.3242
2026-01-09 12:10:58,394: t15.2023.12.10 val PER: 0.2720
2026-01-09 12:10:58,395: t15.2023.12.17 val PER: 0.3160
2026-01-09 12:10:58,395: t15.2023.12.29 val PER: 0.3226
2026-01-09 12:10:58,395: t15.2024.02.25 val PER: 0.2921
2026-01-09 12:10:58,395: t15.2024.03.08 val PER: 0.4139
2026-01-09 12:10:58,395: t15.2024.03.15 val PER: 0.3659
2026-01-09 12:10:58,395: t15.2024.03.17 val PER: 0.3459
2026-01-09 12:10:58,395: t15.2024.05.10 val PER: 0.3566
2026-01-09 12:10:58,395: t15.2024.06.14 val PER: 0.3628
2026-01-09 12:10:58,395: t15.2024.07.19 val PER: 0.4766
2026-01-09 12:10:58,395: t15.2024.07.21 val PER: 0.3021
2026-01-09 12:10:58,395: t15.2024.07.28 val PER: 0.3375
2026-01-09 12:10:58,395: t15.2025.01.10 val PER: 0.5523
2026-01-09 12:10:58,396: t15.2025.01.12 val PER: 0.3895
2026-01-09 12:10:58,396: t15.2025.03.14 val PER: 0.5473
2026-01-09 12:10:58,396: t15.2025.03.16 val PER: 0.4031
2026-01-09 12:10:58,396: t15.2025.03.30 val PER: 0.5609
2026-01-09 12:10:58,396: t15.2025.04.13 val PER: 0.4223
2026-01-09 12:10:58,397: New best val WER(5gram) 39.05% --> 31.10%
2026-01-09 12:10:58,586: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_2000
2026-01-09 12:11:16,108: Train batch 2200: loss: 30.93 grad norm: 77.81 time: 0.061
2026-01-09 12:11:33,181: Train batch 2400: loss: 30.22 grad norm: 63.66 time: 0.053
2026-01-09 12:11:41,735: Running test after training batch: 2500
2026-01-09 12:11:41,871: WER debug GT example: You can see the code at this point as well.
2026-01-09 12:11:46,793: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-09 12:11:46,850: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us in
2026-01-09 12:12:02,495: Val batch 2500: PER (avg): 0.3144 CTC Loss (avg): 31.3033 WER(5gram): 28.49% (n=256) time: 20.759
2026-01-09 12:12:02,495: WER lens: avg_true_words=5.99 avg_pred_words=5.75 max_pred_words=12
2026-01-09 12:12:02,496: t15.2023.08.13 val PER: 0.2973
2026-01-09 12:12:02,496: t15.2023.08.18 val PER: 0.2448
2026-01-09 12:12:02,496: t15.2023.08.20 val PER: 0.2502
2026-01-09 12:12:02,496: t15.2023.08.25 val PER: 0.2139
2026-01-09 12:12:02,496: t15.2023.08.27 val PER: 0.3328
2026-01-09 12:12:02,496: t15.2023.09.01 val PER: 0.2208
2026-01-09 12:12:02,496: t15.2023.09.03 val PER: 0.3147
2026-01-09 12:12:02,496: t15.2023.09.24 val PER: 0.2464
2026-01-09 12:12:02,496: t15.2023.09.29 val PER: 0.2572
2026-01-09 12:12:02,496: t15.2023.10.01 val PER: 0.3124
2026-01-09 12:12:02,496: t15.2023.10.06 val PER: 0.2174
2026-01-09 12:12:02,496: t15.2023.10.08 val PER: 0.3843
2026-01-09 12:12:02,496: t15.2023.10.13 val PER: 0.3654
2026-01-09 12:12:02,496: t15.2023.10.15 val PER: 0.3059
2026-01-09 12:12:02,497: t15.2023.10.20 val PER: 0.2886
2026-01-09 12:12:02,497: t15.2023.10.22 val PER: 0.2394
2026-01-09 12:12:02,497: t15.2023.11.03 val PER: 0.2972
2026-01-09 12:12:02,497: t15.2023.11.04 val PER: 0.1160
2026-01-09 12:12:02,497: t15.2023.11.17 val PER: 0.1602
2026-01-09 12:12:02,497: t15.2023.11.19 val PER: 0.1297
2026-01-09 12:12:02,497: t15.2023.11.26 val PER: 0.3609
2026-01-09 12:12:02,497: t15.2023.12.03 val PER: 0.2973
2026-01-09 12:12:02,497: t15.2023.12.08 val PER: 0.2870
2026-01-09 12:12:02,497: t15.2023.12.10 val PER: 0.2484
2026-01-09 12:12:02,498: t15.2023.12.17 val PER: 0.3035
2026-01-09 12:12:02,498: t15.2023.12.29 val PER: 0.3013
2026-01-09 12:12:02,498: t15.2024.02.25 val PER: 0.2528
2026-01-09 12:12:02,498: t15.2024.03.08 val PER: 0.3755
2026-01-09 12:12:02,498: t15.2024.03.15 val PER: 0.3590
2026-01-09 12:12:02,498: t15.2024.03.17 val PER: 0.3222
2026-01-09 12:12:02,498: t15.2024.05.10 val PER: 0.3150
2026-01-09 12:12:02,498: t15.2024.06.14 val PER: 0.3186
2026-01-09 12:12:02,498: t15.2024.07.19 val PER: 0.4522
2026-01-09 12:12:02,498: t15.2024.07.21 val PER: 0.2697
2026-01-09 12:12:02,498: t15.2024.07.28 val PER: 0.3037
2026-01-09 12:12:02,499: t15.2025.01.10 val PER: 0.5289
2026-01-09 12:12:02,499: t15.2025.01.12 val PER: 0.3834
2026-01-09 12:12:02,499: t15.2025.03.14 val PER: 0.5340
2026-01-09 12:12:02,499: t15.2025.03.16 val PER: 0.3783
2026-01-09 12:12:02,499: t15.2025.03.30 val PER: 0.5264
2026-01-09 12:12:02,499: t15.2025.04.13 val PER: 0.4123
2026-01-09 12:12:02,499: New best val WER(5gram) 31.10% --> 28.49%
2026-01-09 12:12:02,683: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_2500
2026-01-09 12:12:11,144: Train batch 2600: loss: 36.88 grad norm: 85.20 time: 0.055
