TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_349293
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_349293/torch_extensions
WANDB_DIR=/tmp/e12511253_b2t_349293/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/e12511253_b2t_349293/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  5 16:01 /tmp/e12511253_b2t_349293/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
trained_models -> /tmp/e12511253_b2t_349293/trained_models
OUT_ROOT=/tmp/e12511253_b2t_349293/trained_models
==============================================
Job: b2t_exp  ID: 349293
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/patch_stride/lr40_wd1e-5
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/patch_stride/lr40_wd1e-5 ==========
Num configs: 5

=== RUN base_patch_stride_wd1e-5.yaml ===
2026-01-05 16:01:05,203: Using device: cuda:0
2026-01-05 16:01:07,282: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-05 16:01:07,303: Using 45 sessions after filtering (from 45).
2026-01-05 16:01:07,707: Using torch.compile (if available)
2026-01-05 16:01:07,707: torch.compile not available (torch<2.0). Skipping.
2026-01-05 16:01:07,707: Initialized RNN decoding model
2026-01-05 16:01:07,707: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-05 16:01:07,708: Model has 44,907,305 parameters
2026-01-05 16:01:07,708: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-05 16:01:08,990: Successfully initialized datasets
2026-01-05 16:01:08,990: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-05 16:01:09,983: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.181
2026-01-05 16:01:09,983: Running test after training batch: 0
2026-01-05 16:01:10,097: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:01:15,555: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-05 16:01:16,268: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-05 16:01:50,163: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 40.180
2026-01-05 16:01:50,164: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-05 16:01:50,164: t15.2023.08.13 val PER: 1.3056
2026-01-05 16:01:50,164: t15.2023.08.18 val PER: 1.4208
2026-01-05 16:01:50,164: t15.2023.08.20 val PER: 1.3002
2026-01-05 16:01:50,164: t15.2023.08.25 val PER: 1.3389
2026-01-05 16:01:50,164: t15.2023.08.27 val PER: 1.2460
2026-01-05 16:01:50,165: t15.2023.09.01 val PER: 1.4537
2026-01-05 16:01:50,165: t15.2023.09.03 val PER: 1.3171
2026-01-05 16:01:50,165: t15.2023.09.24 val PER: 1.5461
2026-01-05 16:01:50,165: t15.2023.09.29 val PER: 1.4671
2026-01-05 16:01:50,165: t15.2023.10.01 val PER: 1.2147
2026-01-05 16:01:50,165: t15.2023.10.06 val PER: 1.4876
2026-01-05 16:01:50,165: t15.2023.10.08 val PER: 1.1827
2026-01-05 16:01:50,165: t15.2023.10.13 val PER: 1.3964
2026-01-05 16:01:50,165: t15.2023.10.15 val PER: 1.3889
2026-01-05 16:01:50,165: t15.2023.10.20 val PER: 1.4866
2026-01-05 16:01:50,165: t15.2023.10.22 val PER: 1.3942
2026-01-05 16:01:50,165: t15.2023.11.03 val PER: 1.5923
2026-01-05 16:01:50,165: t15.2023.11.04 val PER: 2.0171
2026-01-05 16:01:50,165: t15.2023.11.17 val PER: 1.9518
2026-01-05 16:01:50,165: t15.2023.11.19 val PER: 1.6707
2026-01-05 16:01:50,165: t15.2023.11.26 val PER: 1.5413
2026-01-05 16:01:50,166: t15.2023.12.03 val PER: 1.4254
2026-01-05 16:01:50,166: t15.2023.12.08 val PER: 1.4487
2026-01-05 16:01:50,166: t15.2023.12.10 val PER: 1.6899
2026-01-05 16:01:50,166: t15.2023.12.17 val PER: 1.3077
2026-01-05 16:01:50,166: t15.2023.12.29 val PER: 1.4063
2026-01-05 16:01:50,166: t15.2024.02.25 val PER: 1.4228
2026-01-05 16:01:50,166: t15.2024.03.08 val PER: 1.3257
2026-01-05 16:01:50,166: t15.2024.03.15 val PER: 1.3196
2026-01-05 16:01:50,166: t15.2024.03.17 val PER: 1.4052
2026-01-05 16:01:50,166: t15.2024.05.10 val PER: 1.3224
2026-01-05 16:01:50,167: t15.2024.06.14 val PER: 1.5315
2026-01-05 16:01:50,167: t15.2024.07.19 val PER: 1.0817
2026-01-05 16:01:50,167: t15.2024.07.21 val PER: 1.6290
2026-01-05 16:01:50,167: t15.2024.07.28 val PER: 1.6588
2026-01-05 16:01:50,167: t15.2025.01.10 val PER: 1.0923
2026-01-05 16:01:50,167: t15.2025.01.12 val PER: 1.7629
2026-01-05 16:01:50,167: t15.2025.03.14 val PER: 1.0414
2026-01-05 16:01:50,167: t15.2025.03.16 val PER: 1.6257
2026-01-05 16:01:50,167: t15.2025.03.30 val PER: 1.2874
2026-01-05 16:01:50,167: t15.2025.04.13 val PER: 1.5949
2026-01-05 16:01:50,168: New best val WER(1gram) inf% --> 100.00%
2026-01-05 16:01:50,168: Checkpointing model
2026-01-05 16:01:50,445: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:01:50,754: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_0
2026-01-05 16:02:09,697: Train batch 200: loss: 77.59 grad norm: 106.13 time: 0.055
2026-01-05 16:02:27,594: Train batch 400: loss: 53.78 grad norm: 83.49 time: 0.063
2026-01-05 16:02:36,674: Running test after training batch: 500
2026-01-05 16:02:36,816: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:02:41,823: WER debug example
  GT : you can see the code at this point as well
  PR : yule and ease thus uhde at this uhde is aisle
2026-01-05 16:02:41,858: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as adz
2026-01-05 16:02:44,045: Val batch 500: PER (avg): 0.5223 CTC Loss (avg): 54.9370 WER(1gram): 88.58% (n=64) time: 7.370
2026-01-05 16:02:44,045: WER lens: avg_true_words=6.16 avg_pred_words=5.55 max_pred_words=11
2026-01-05 16:02:44,046: t15.2023.08.13 val PER: 0.4688
2026-01-05 16:02:44,046: t15.2023.08.18 val PER: 0.4560
2026-01-05 16:02:44,046: t15.2023.08.20 val PER: 0.4456
2026-01-05 16:02:44,046: t15.2023.08.25 val PER: 0.4337
2026-01-05 16:02:44,046: t15.2023.08.27 val PER: 0.5354
2026-01-05 16:02:44,046: t15.2023.09.01 val PER: 0.4237
2026-01-05 16:02:44,046: t15.2023.09.03 val PER: 0.5000
2026-01-05 16:02:44,046: t15.2023.09.24 val PER: 0.4223
2026-01-05 16:02:44,047: t15.2023.09.29 val PER: 0.4710
2026-01-05 16:02:44,047: t15.2023.10.01 val PER: 0.5172
2026-01-05 16:02:44,047: t15.2023.10.06 val PER: 0.4263
2026-01-05 16:02:44,047: t15.2023.10.08 val PER: 0.5440
2026-01-05 16:02:44,047: t15.2023.10.13 val PER: 0.5772
2026-01-05 16:02:44,047: t15.2023.10.15 val PER: 0.4944
2026-01-05 16:02:44,047: t15.2023.10.20 val PER: 0.4732
2026-01-05 16:02:44,047: t15.2023.10.22 val PER: 0.4577
2026-01-05 16:02:44,047: t15.2023.11.03 val PER: 0.5027
2026-01-05 16:02:44,047: t15.2023.11.04 val PER: 0.2765
2026-01-05 16:02:44,047: t15.2023.11.17 val PER: 0.3686
2026-01-05 16:02:44,047: t15.2023.11.19 val PER: 0.3234
2026-01-05 16:02:44,047: t15.2023.11.26 val PER: 0.5580
2026-01-05 16:02:44,047: t15.2023.12.03 val PER: 0.5042
2026-01-05 16:02:44,048: t15.2023.12.08 val PER: 0.5233
2026-01-05 16:02:44,048: t15.2023.12.10 val PER: 0.4573
2026-01-05 16:02:44,048: t15.2023.12.17 val PER: 0.5707
2026-01-05 16:02:44,048: t15.2023.12.29 val PER: 0.5408
2026-01-05 16:02:44,048: t15.2024.02.25 val PER: 0.4860
2026-01-05 16:02:44,048: t15.2024.03.08 val PER: 0.6259
2026-01-05 16:02:44,048: t15.2024.03.15 val PER: 0.5566
2026-01-05 16:02:44,048: t15.2024.03.17 val PER: 0.5132
2026-01-05 16:02:44,048: t15.2024.05.10 val PER: 0.5334
2026-01-05 16:02:44,048: t15.2024.06.14 val PER: 0.5268
2026-01-05 16:02:44,048: t15.2024.07.19 val PER: 0.6816
2026-01-05 16:02:44,048: t15.2024.07.21 val PER: 0.4717
2026-01-05 16:02:44,048: t15.2024.07.28 val PER: 0.5265
2026-01-05 16:02:44,048: t15.2025.01.10 val PER: 0.7576
2026-01-05 16:02:44,048: t15.2025.01.12 val PER: 0.5689
2026-01-05 16:02:44,048: t15.2025.03.14 val PER: 0.7633
2026-01-05 16:02:44,049: t15.2025.03.16 val PER: 0.5982
2026-01-05 16:02:44,049: t15.2025.03.30 val PER: 0.7333
2026-01-05 16:02:44,049: t15.2025.04.13 val PER: 0.5763
2026-01-05 16:02:44,050: New best val WER(1gram) 100.00% --> 88.58%
2026-01-05 16:02:44,050: Checkpointing model
2026-01-05 16:02:45,036: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:02:45,312: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_500
2026-01-05 16:02:54,716: Train batch 600: loss: 48.48 grad norm: 75.69 time: 0.079
2026-01-05 16:03:13,676: Train batch 800: loss: 41.45 grad norm: 88.87 time: 0.057
2026-01-05 16:03:32,178: Train batch 1000: loss: 42.60 grad norm: 80.90 time: 0.065
2026-01-05 16:03:32,178: Running test after training batch: 1000
2026-01-05 16:03:32,324: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:03:37,541: WER debug example
  GT : you can see the code at this point as well
  PR : yule wint ease thus good it this uhde is while
2026-01-05 16:03:37,575: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus wass it
2026-01-05 16:03:39,561: Val batch 1000: PER (avg): 0.4076 CTC Loss (avg): 42.2621 WER(1gram): 80.71% (n=64) time: 7.383
2026-01-05 16:03:39,562: WER lens: avg_true_words=6.16 avg_pred_words=5.44 max_pred_words=11
2026-01-05 16:03:39,562: t15.2023.08.13 val PER: 0.3742
2026-01-05 16:03:39,562: t15.2023.08.18 val PER: 0.3328
2026-01-05 16:03:39,562: t15.2023.08.20 val PER: 0.3407
2026-01-05 16:03:39,562: t15.2023.08.25 val PER: 0.2952
2026-01-05 16:03:39,562: t15.2023.08.27 val PER: 0.4228
2026-01-05 16:03:39,563: t15.2023.09.01 val PER: 0.3011
2026-01-05 16:03:39,563: t15.2023.09.03 val PER: 0.4026
2026-01-05 16:03:39,563: t15.2023.09.24 val PER: 0.3155
2026-01-05 16:03:39,563: t15.2023.09.29 val PER: 0.3580
2026-01-05 16:03:39,563: t15.2023.10.01 val PER: 0.4075
2026-01-05 16:03:39,563: t15.2023.10.06 val PER: 0.3143
2026-01-05 16:03:39,563: t15.2023.10.08 val PER: 0.4574
2026-01-05 16:03:39,563: t15.2023.10.13 val PER: 0.4639
2026-01-05 16:03:39,563: t15.2023.10.15 val PER: 0.3790
2026-01-05 16:03:39,564: t15.2023.10.20 val PER: 0.3591
2026-01-05 16:03:39,564: t15.2023.10.22 val PER: 0.3419
2026-01-05 16:03:39,564: t15.2023.11.03 val PER: 0.4043
2026-01-05 16:03:39,564: t15.2023.11.04 val PER: 0.1536
2026-01-05 16:03:39,564: t15.2023.11.17 val PER: 0.2784
2026-01-05 16:03:39,564: t15.2023.11.19 val PER: 0.2056
2026-01-05 16:03:39,564: t15.2023.11.26 val PER: 0.4377
2026-01-05 16:03:39,564: t15.2023.12.03 val PER: 0.4034
2026-01-05 16:03:39,564: t15.2023.12.08 val PER: 0.3995
2026-01-05 16:03:39,564: t15.2023.12.10 val PER: 0.3443
2026-01-05 16:03:39,564: t15.2023.12.17 val PER: 0.4085
2026-01-05 16:03:39,564: t15.2023.12.29 val PER: 0.4097
2026-01-05 16:03:39,564: t15.2024.02.25 val PER: 0.3596
2026-01-05 16:03:39,564: t15.2024.03.08 val PER: 0.4922
2026-01-05 16:03:39,564: t15.2024.03.15 val PER: 0.4353
2026-01-05 16:03:39,565: t15.2024.03.17 val PER: 0.3982
2026-01-05 16:03:39,565: t15.2024.05.10 val PER: 0.4190
2026-01-05 16:03:39,565: t15.2024.06.14 val PER: 0.4069
2026-01-05 16:03:39,565: t15.2024.07.19 val PER: 0.5386
2026-01-05 16:03:39,565: t15.2024.07.21 val PER: 0.3724
2026-01-05 16:03:39,565: t15.2024.07.28 val PER: 0.4199
2026-01-05 16:03:39,565: t15.2025.01.10 val PER: 0.6088
2026-01-05 16:03:39,565: t15.2025.01.12 val PER: 0.4580
2026-01-05 16:03:39,565: t15.2025.03.14 val PER: 0.6435
2026-01-05 16:03:39,565: t15.2025.03.16 val PER: 0.4908
2026-01-05 16:03:39,565: t15.2025.03.30 val PER: 0.6494
2026-01-05 16:03:39,565: t15.2025.04.13 val PER: 0.4964
2026-01-05 16:03:39,567: New best val WER(1gram) 88.58% --> 80.71%
2026-01-05 16:03:39,567: Checkpointing model
2026-01-05 16:03:40,640: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:03:40,922: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_1000
2026-01-05 16:03:59,365: Train batch 1200: loss: 33.05 grad norm: 77.46 time: 0.069
2026-01-05 16:04:17,464: Train batch 1400: loss: 36.23 grad norm: 80.88 time: 0.061
2026-01-05 16:04:26,639: Running test after training batch: 1500
2026-01-05 16:04:26,797: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:04:31,783: WER debug example
  GT : you can see the code at this point as well
  PR : yule end sze the good it this boyde is wheel
2026-01-05 16:04:31,816: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that cost
2026-01-05 16:04:33,388: Val batch 1500: PER (avg): 0.3790 CTC Loss (avg): 37.1439 WER(1gram): 76.14% (n=64) time: 6.748
2026-01-05 16:04:33,388: WER lens: avg_true_words=6.16 avg_pred_words=5.00 max_pred_words=11
2026-01-05 16:04:33,389: t15.2023.08.13 val PER: 0.3514
2026-01-05 16:04:33,389: t15.2023.08.18 val PER: 0.3085
2026-01-05 16:04:33,389: t15.2023.08.20 val PER: 0.3058
2026-01-05 16:04:33,389: t15.2023.08.25 val PER: 0.2651
2026-01-05 16:04:33,389: t15.2023.08.27 val PER: 0.4100
2026-01-05 16:04:33,389: t15.2023.09.01 val PER: 0.2662
2026-01-05 16:04:33,389: t15.2023.09.03 val PER: 0.3836
2026-01-05 16:04:33,389: t15.2023.09.24 val PER: 0.2998
2026-01-05 16:04:33,390: t15.2023.09.29 val PER: 0.3357
2026-01-05 16:04:33,390: t15.2023.10.01 val PER: 0.3996
2026-01-05 16:04:33,390: t15.2023.10.06 val PER: 0.2874
2026-01-05 16:04:33,390: t15.2023.10.08 val PER: 0.4438
2026-01-05 16:04:33,390: t15.2023.10.13 val PER: 0.4438
2026-01-05 16:04:33,390: t15.2023.10.15 val PER: 0.3606
2026-01-05 16:04:33,390: t15.2023.10.20 val PER: 0.3255
2026-01-05 16:04:33,390: t15.2023.10.22 val PER: 0.3129
2026-01-05 16:04:33,390: t15.2023.11.03 val PER: 0.3657
2026-01-05 16:04:33,391: t15.2023.11.04 val PER: 0.1160
2026-01-05 16:04:33,391: t15.2023.11.17 val PER: 0.2208
2026-01-05 16:04:33,391: t15.2023.11.19 val PER: 0.1836
2026-01-05 16:04:33,391: t15.2023.11.26 val PER: 0.4159
2026-01-05 16:04:33,391: t15.2023.12.03 val PER: 0.3634
2026-01-05 16:04:33,391: t15.2023.12.08 val PER: 0.3569
2026-01-05 16:04:33,391: t15.2023.12.10 val PER: 0.3022
2026-01-05 16:04:33,391: t15.2023.12.17 val PER: 0.3732
2026-01-05 16:04:33,391: t15.2023.12.29 val PER: 0.3761
2026-01-05 16:04:33,391: t15.2024.02.25 val PER: 0.3104
2026-01-05 16:04:33,391: t15.2024.03.08 val PER: 0.4623
2026-01-05 16:04:33,391: t15.2024.03.15 val PER: 0.4153
2026-01-05 16:04:33,391: t15.2024.03.17 val PER: 0.3780
2026-01-05 16:04:33,391: t15.2024.05.10 val PER: 0.3848
2026-01-05 16:04:33,391: t15.2024.06.14 val PER: 0.3927
2026-01-05 16:04:33,392: t15.2024.07.19 val PER: 0.5181
2026-01-05 16:04:33,392: t15.2024.07.21 val PER: 0.3434
2026-01-05 16:04:33,392: t15.2024.07.28 val PER: 0.3625
2026-01-05 16:04:33,392: t15.2025.01.10 val PER: 0.6116
2026-01-05 16:04:33,392: t15.2025.01.12 val PER: 0.4273
2026-01-05 16:04:33,392: t15.2025.03.14 val PER: 0.5947
2026-01-05 16:04:33,392: t15.2025.03.16 val PER: 0.4411
2026-01-05 16:04:33,392: t15.2025.03.30 val PER: 0.6149
2026-01-05 16:04:33,392: t15.2025.04.13 val PER: 0.4750
2026-01-05 16:04:33,393: New best val WER(1gram) 80.71% --> 76.14%
2026-01-05 16:04:33,393: Checkpointing model
2026-01-05 16:04:34,501: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:04:34,789: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_1500
2026-01-05 16:04:43,660: Train batch 1600: loss: 36.97 grad norm: 78.27 time: 0.063
2026-01-05 16:05:02,555: Train batch 1800: loss: 35.14 grad norm: 71.32 time: 0.087
2026-01-05 16:05:21,028: Train batch 2000: loss: 34.01 grad norm: 72.45 time: 0.066
2026-01-05 16:05:21,028: Running test after training batch: 2000
2026-01-05 16:05:21,153: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:05:26,329: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this bonde is will
2026-01-05 16:05:26,361: WER debug example
  GT : how does it keep the cost down
  PR : houde buice it heap the kos id
2026-01-05 16:05:27,919: Val batch 2000: PER (avg): 0.3290 CTC Loss (avg): 32.8029 WER(1gram): 70.05% (n=64) time: 6.891
2026-01-05 16:05:27,920: WER lens: avg_true_words=6.16 avg_pred_words=5.50 max_pred_words=10
2026-01-05 16:05:27,920: t15.2023.08.13 val PER: 0.3056
2026-01-05 16:05:27,920: t15.2023.08.18 val PER: 0.2615
2026-01-05 16:05:27,920: t15.2023.08.20 val PER: 0.2534
2026-01-05 16:05:27,920: t15.2023.08.25 val PER: 0.2184
2026-01-05 16:05:27,920: t15.2023.08.27 val PER: 0.3408
2026-01-05 16:05:27,920: t15.2023.09.01 val PER: 0.2248
2026-01-05 16:05:27,921: t15.2023.09.03 val PER: 0.3278
2026-01-05 16:05:27,921: t15.2023.09.24 val PER: 0.2512
2026-01-05 16:05:27,921: t15.2023.09.29 val PER: 0.2821
2026-01-05 16:05:27,921: t15.2023.10.01 val PER: 0.3296
2026-01-05 16:05:27,921: t15.2023.10.06 val PER: 0.2325
2026-01-05 16:05:27,921: t15.2023.10.08 val PER: 0.3978
2026-01-05 16:05:27,921: t15.2023.10.13 val PER: 0.3794
2026-01-05 16:05:27,921: t15.2023.10.15 val PER: 0.3078
2026-01-05 16:05:27,921: t15.2023.10.20 val PER: 0.2953
2026-01-05 16:05:27,921: t15.2023.10.22 val PER: 0.2673
2026-01-05 16:05:27,921: t15.2023.11.03 val PER: 0.3114
2026-01-05 16:05:27,921: t15.2023.11.04 val PER: 0.0853
2026-01-05 16:05:27,921: t15.2023.11.17 val PER: 0.1757
2026-01-05 16:05:27,921: t15.2023.11.19 val PER: 0.1397
2026-01-05 16:05:27,921: t15.2023.11.26 val PER: 0.3696
2026-01-05 16:05:27,922: t15.2023.12.03 val PER: 0.3109
2026-01-05 16:05:27,922: t15.2023.12.08 val PER: 0.3016
2026-01-05 16:05:27,922: t15.2023.12.10 val PER: 0.2668
2026-01-05 16:05:27,922: t15.2023.12.17 val PER: 0.3254
2026-01-05 16:05:27,922: t15.2023.12.29 val PER: 0.3253
2026-01-05 16:05:27,922: t15.2024.02.25 val PER: 0.2767
2026-01-05 16:05:27,922: t15.2024.03.08 val PER: 0.3926
2026-01-05 16:05:27,922: t15.2024.03.15 val PER: 0.3627
2026-01-05 16:05:27,922: t15.2024.03.17 val PER: 0.3375
2026-01-05 16:05:27,922: t15.2024.05.10 val PER: 0.3447
2026-01-05 16:05:27,922: t15.2024.06.14 val PER: 0.3596
2026-01-05 16:05:27,923: t15.2024.07.19 val PER: 0.4746
2026-01-05 16:05:27,923: t15.2024.07.21 val PER: 0.3021
2026-01-05 16:05:27,923: t15.2024.07.28 val PER: 0.3235
2026-01-05 16:05:27,923: t15.2025.01.10 val PER: 0.5413
2026-01-05 16:05:27,923: t15.2025.01.12 val PER: 0.3880
2026-01-05 16:05:27,923: t15.2025.03.14 val PER: 0.5311
2026-01-05 16:05:27,923: t15.2025.03.16 val PER: 0.4031
2026-01-05 16:05:27,923: t15.2025.03.30 val PER: 0.5540
2026-01-05 16:05:27,923: t15.2025.04.13 val PER: 0.4037
2026-01-05 16:05:27,925: New best val WER(1gram) 76.14% --> 70.05%
2026-01-05 16:05:27,925: Checkpointing model
2026-01-05 16:05:29,057: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:05:29,342: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_2000
2026-01-05 16:05:47,080: Train batch 2200: loss: 28.57 grad norm: 71.70 time: 0.060
2026-01-05 16:06:05,100: Train batch 2400: loss: 29.32 grad norm: 63.09 time: 0.053
2026-01-05 16:06:14,182: Running test after training batch: 2500
2026-01-05 16:06:14,343: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:06:19,418: WER debug example
  GT : you can see the code at this point as well
  PR : yule kent e the code at this point is will
2026-01-05 16:06:19,448: WER debug example
  GT : how does it keep the cost down
  PR : houde des it eke thus wass it
2026-01-05 16:06:21,262: Val batch 2500: PER (avg): 0.3047 CTC Loss (avg): 30.4205 WER(1gram): 69.29% (n=64) time: 7.079
2026-01-05 16:06:21,262: WER lens: avg_true_words=6.16 avg_pred_words=5.81 max_pred_words=11
2026-01-05 16:06:21,263: t15.2023.08.13 val PER: 0.2879
2026-01-05 16:06:21,263: t15.2023.08.18 val PER: 0.2473
2026-01-05 16:06:21,263: t15.2023.08.20 val PER: 0.2423
2026-01-05 16:06:21,263: t15.2023.08.25 val PER: 0.2063
2026-01-05 16:06:21,263: t15.2023.08.27 val PER: 0.3296
2026-01-05 16:06:21,263: t15.2023.09.01 val PER: 0.2216
2026-01-05 16:06:21,263: t15.2023.09.03 val PER: 0.2933
2026-01-05 16:06:21,264: t15.2023.09.24 val PER: 0.2354
2026-01-05 16:06:21,264: t15.2023.09.29 val PER: 0.2616
2026-01-05 16:06:21,264: t15.2023.10.01 val PER: 0.3111
2026-01-05 16:06:21,264: t15.2023.10.06 val PER: 0.2228
2026-01-05 16:06:21,264: t15.2023.10.08 val PER: 0.3762
2026-01-05 16:06:21,264: t15.2023.10.13 val PER: 0.3507
2026-01-05 16:06:21,265: t15.2023.10.15 val PER: 0.2861
2026-01-05 16:06:21,265: t15.2023.10.20 val PER: 0.2752
2026-01-05 16:06:21,265: t15.2023.10.22 val PER: 0.2416
2026-01-05 16:06:21,265: t15.2023.11.03 val PER: 0.2978
2026-01-05 16:06:21,265: t15.2023.11.04 val PER: 0.0922
2026-01-05 16:06:21,265: t15.2023.11.17 val PER: 0.1493
2026-01-05 16:06:21,265: t15.2023.11.19 val PER: 0.1198
2026-01-05 16:06:21,266: t15.2023.11.26 val PER: 0.3406
2026-01-05 16:06:21,266: t15.2023.12.03 val PER: 0.2910
2026-01-05 16:06:21,266: t15.2023.12.08 val PER: 0.2790
2026-01-05 16:06:21,266: t15.2023.12.10 val PER: 0.2392
2026-01-05 16:06:21,266: t15.2023.12.17 val PER: 0.2921
2026-01-05 16:06:21,266: t15.2023.12.29 val PER: 0.3082
2026-01-05 16:06:21,266: t15.2024.02.25 val PER: 0.2444
2026-01-05 16:06:21,266: t15.2024.03.08 val PER: 0.3585
2026-01-05 16:06:21,266: t15.2024.03.15 val PER: 0.3421
2026-01-05 16:06:21,266: t15.2024.03.17 val PER: 0.3201
2026-01-05 16:06:21,267: t15.2024.05.10 val PER: 0.3061
2026-01-05 16:06:21,267: t15.2024.06.14 val PER: 0.3076
2026-01-05 16:06:21,267: t15.2024.07.19 val PER: 0.4397
2026-01-05 16:06:21,267: t15.2024.07.21 val PER: 0.2628
2026-01-05 16:06:21,267: t15.2024.07.28 val PER: 0.2956
2026-01-05 16:06:21,267: t15.2025.01.10 val PER: 0.4972
2026-01-05 16:06:21,267: t15.2025.01.12 val PER: 0.3449
2026-01-05 16:06:21,267: t15.2025.03.14 val PER: 0.5178
2026-01-05 16:06:21,267: t15.2025.03.16 val PER: 0.3573
2026-01-05 16:06:21,268: t15.2025.03.30 val PER: 0.5103
2026-01-05 16:06:21,268: t15.2025.04.13 val PER: 0.3866
2026-01-05 16:06:21,268: New best val WER(1gram) 70.05% --> 69.29%
2026-01-05 16:06:21,268: Checkpointing model
2026-01-05 16:06:22,431: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:06:22,715: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_2500
2026-01-05 16:06:32,498: Train batch 2600: loss: 35.00 grad norm: 85.23 time: 0.056
2026-01-05 16:06:51,626: Train batch 2800: loss: 26.08 grad norm: 70.27 time: 0.082
2026-01-05 16:07:10,374: Train batch 3000: loss: 31.50 grad norm: 70.12 time: 0.082
2026-01-05 16:07:10,374: Running test after training batch: 3000
2026-01-05 16:07:10,483: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:07:15,447: WER debug example
  GT : you can see the code at this point as well
  PR : yule end e the code at this point is will
2026-01-05 16:07:15,474: WER debug example
  GT : how does it keep the cost down
  PR : houde des it hipp the rost et
2026-01-05 16:07:17,031: Val batch 3000: PER (avg): 0.2801 CTC Loss (avg): 27.6374 WER(1gram): 64.97% (n=64) time: 6.657
2026-01-05 16:07:17,031: WER lens: avg_true_words=6.16 avg_pred_words=5.78 max_pred_words=11
2026-01-05 16:07:17,031: t15.2023.08.13 val PER: 0.2651
2026-01-05 16:07:17,032: t15.2023.08.18 val PER: 0.2255
2026-01-05 16:07:17,032: t15.2023.08.20 val PER: 0.2121
2026-01-05 16:07:17,032: t15.2023.08.25 val PER: 0.1943
2026-01-05 16:07:17,032: t15.2023.08.27 val PER: 0.3103
2026-01-05 16:07:17,032: t15.2023.09.01 val PER: 0.1737
2026-01-05 16:07:17,032: t15.2023.09.03 val PER: 0.2767
2026-01-05 16:07:17,032: t15.2023.09.24 val PER: 0.2051
2026-01-05 16:07:17,032: t15.2023.09.29 val PER: 0.2336
2026-01-05 16:07:17,032: t15.2023.10.01 val PER: 0.2906
2026-01-05 16:07:17,032: t15.2023.10.06 val PER: 0.1981
2026-01-05 16:07:17,032: t15.2023.10.08 val PER: 0.3383
2026-01-05 16:07:17,032: t15.2023.10.13 val PER: 0.3491
2026-01-05 16:07:17,032: t15.2023.10.15 val PER: 0.2650
2026-01-05 16:07:17,032: t15.2023.10.20 val PER: 0.2483
2026-01-05 16:07:17,033: t15.2023.10.22 val PER: 0.2127
2026-01-05 16:07:17,033: t15.2023.11.03 val PER: 0.2734
2026-01-05 16:07:17,033: t15.2023.11.04 val PER: 0.0580
2026-01-05 16:07:17,033: t15.2023.11.17 val PER: 0.1260
2026-01-05 16:07:17,033: t15.2023.11.19 val PER: 0.1218
2026-01-05 16:07:17,033: t15.2023.11.26 val PER: 0.2942
2026-01-05 16:07:17,033: t15.2023.12.03 val PER: 0.2584
2026-01-05 16:07:17,033: t15.2023.12.08 val PER: 0.2577
2026-01-05 16:07:17,033: t15.2023.12.10 val PER: 0.2076
2026-01-05 16:07:17,034: t15.2023.12.17 val PER: 0.2755
2026-01-05 16:07:17,034: t15.2023.12.29 val PER: 0.2828
2026-01-05 16:07:17,034: t15.2024.02.25 val PER: 0.2331
2026-01-05 16:07:17,034: t15.2024.03.08 val PER: 0.3713
2026-01-05 16:07:17,034: t15.2024.03.15 val PER: 0.3383
2026-01-05 16:07:17,034: t15.2024.03.17 val PER: 0.2803
2026-01-05 16:07:17,034: t15.2024.05.10 val PER: 0.2868
2026-01-05 16:07:17,034: t15.2024.06.14 val PER: 0.2981
2026-01-05 16:07:17,034: t15.2024.07.19 val PER: 0.4054
2026-01-05 16:07:17,034: t15.2024.07.21 val PER: 0.2338
2026-01-05 16:07:17,034: t15.2024.07.28 val PER: 0.2794
2026-01-05 16:07:17,034: t15.2025.01.10 val PER: 0.4780
2026-01-05 16:07:17,034: t15.2025.01.12 val PER: 0.3310
2026-01-05 16:07:17,034: t15.2025.03.14 val PER: 0.4423
2026-01-05 16:07:17,035: t15.2025.03.16 val PER: 0.3416
2026-01-05 16:07:17,035: t15.2025.03.30 val PER: 0.4736
2026-01-05 16:07:17,035: t15.2025.04.13 val PER: 0.3395
2026-01-05 16:07:17,035: New best val WER(1gram) 69.29% --> 64.97%
2026-01-05 16:07:17,036: Checkpointing model
2026-01-05 16:07:18,238: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:07:18,508: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_3000
2026-01-05 16:07:36,542: Train batch 3200: loss: 26.51 grad norm: 69.46 time: 0.077
2026-01-05 16:07:54,432: Train batch 3400: loss: 18.39 grad norm: 55.66 time: 0.049
2026-01-05 16:08:03,471: Running test after training batch: 3500
2026-01-05 16:08:03,597: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:08:08,670: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point will
2026-01-05 16:08:08,699: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep thus cussed get
2026-01-05 16:08:10,312: Val batch 3500: PER (avg): 0.2643 CTC Loss (avg): 26.6166 WER(1gram): 65.48% (n=64) time: 6.840
2026-01-05 16:08:10,313: WER lens: avg_true_words=6.16 avg_pred_words=6.02 max_pred_words=11
2026-01-05 16:08:10,313: t15.2023.08.13 val PER: 0.2339
2026-01-05 16:08:10,313: t15.2023.08.18 val PER: 0.2104
2026-01-05 16:08:10,313: t15.2023.08.20 val PER: 0.2137
2026-01-05 16:08:10,313: t15.2023.08.25 val PER: 0.1762
2026-01-05 16:08:10,313: t15.2023.08.27 val PER: 0.2733
2026-01-05 16:08:10,313: t15.2023.09.01 val PER: 0.1826
2026-01-05 16:08:10,314: t15.2023.09.03 val PER: 0.2530
2026-01-05 16:08:10,314: t15.2023.09.24 val PER: 0.2063
2026-01-05 16:08:10,314: t15.2023.09.29 val PER: 0.2227
2026-01-05 16:08:10,315: t15.2023.10.01 val PER: 0.2682
2026-01-05 16:08:10,315: t15.2023.10.06 val PER: 0.1830
2026-01-05 16:08:10,315: t15.2023.10.08 val PER: 0.3261
2026-01-05 16:08:10,315: t15.2023.10.13 val PER: 0.3126
2026-01-05 16:08:10,315: t15.2023.10.15 val PER: 0.2360
2026-01-05 16:08:10,315: t15.2023.10.20 val PER: 0.2315
2026-01-05 16:08:10,315: t15.2023.10.22 val PER: 0.2049
2026-01-05 16:08:10,315: t15.2023.11.03 val PER: 0.2585
2026-01-05 16:08:10,315: t15.2023.11.04 val PER: 0.0717
2026-01-05 16:08:10,315: t15.2023.11.17 val PER: 0.1198
2026-01-05 16:08:10,315: t15.2023.11.19 val PER: 0.0978
2026-01-05 16:08:10,315: t15.2023.11.26 val PER: 0.2746
2026-01-05 16:08:10,316: t15.2023.12.03 val PER: 0.2332
2026-01-05 16:08:10,316: t15.2023.12.08 val PER: 0.2450
2026-01-05 16:08:10,316: t15.2023.12.10 val PER: 0.2037
2026-01-05 16:08:10,316: t15.2023.12.17 val PER: 0.2609
2026-01-05 16:08:10,316: t15.2023.12.29 val PER: 0.2574
2026-01-05 16:08:10,316: t15.2024.02.25 val PER: 0.2107
2026-01-05 16:08:10,317: t15.2024.03.08 val PER: 0.3499
2026-01-05 16:08:10,317: t15.2024.03.15 val PER: 0.3146
2026-01-05 16:08:10,317: t15.2024.03.17 val PER: 0.2748
2026-01-05 16:08:10,317: t15.2024.05.10 val PER: 0.2689
2026-01-05 16:08:10,317: t15.2024.06.14 val PER: 0.2902
2026-01-05 16:08:10,317: t15.2024.07.19 val PER: 0.3962
2026-01-05 16:08:10,317: t15.2024.07.21 val PER: 0.2131
2026-01-05 16:08:10,317: t15.2024.07.28 val PER: 0.2787
2026-01-05 16:08:10,318: t15.2025.01.10 val PER: 0.4614
2026-01-05 16:08:10,318: t15.2025.01.12 val PER: 0.2979
2026-01-05 16:08:10,318: t15.2025.03.14 val PER: 0.4334
2026-01-05 16:08:10,318: t15.2025.03.16 val PER: 0.3325
2026-01-05 16:08:10,318: t15.2025.03.30 val PER: 0.4437
2026-01-05 16:08:10,318: t15.2025.04.13 val PER: 0.3409
2026-01-05 16:08:10,582: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_3500
2026-01-05 16:08:19,861: Train batch 3600: loss: 22.43 grad norm: 61.82 time: 0.066
2026-01-05 16:08:38,416: Train batch 3800: loss: 25.42 grad norm: 66.84 time: 0.066
2026-01-05 16:08:56,994: Train batch 4000: loss: 19.89 grad norm: 56.67 time: 0.055
2026-01-05 16:08:56,994: Running test after training batch: 4000
2026-01-05 16:08:57,177: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:09:02,200: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-05 16:09:02,229: WER debug example
  GT : how does it keep the cost down
  PR : aue dust it kipp thus cussed nett
2026-01-05 16:09:03,854: Val batch 4000: PER (avg): 0.2490 CTC Loss (avg): 24.4171 WER(1gram): 66.50% (n=64) time: 6.860
2026-01-05 16:09:03,855: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-05 16:09:03,855: t15.2023.08.13 val PER: 0.2266
2026-01-05 16:09:03,855: t15.2023.08.18 val PER: 0.1995
2026-01-05 16:09:03,855: t15.2023.08.20 val PER: 0.1954
2026-01-05 16:09:03,855: t15.2023.08.25 val PER: 0.1566
2026-01-05 16:09:03,855: t15.2023.08.27 val PER: 0.2862
2026-01-05 16:09:03,855: t15.2023.09.01 val PER: 0.1599
2026-01-05 16:09:03,855: t15.2023.09.03 val PER: 0.2387
2026-01-05 16:09:03,855: t15.2023.09.24 val PER: 0.1893
2026-01-05 16:09:03,855: t15.2023.09.29 val PER: 0.1991
2026-01-05 16:09:03,855: t15.2023.10.01 val PER: 0.2550
2026-01-05 16:09:03,856: t15.2023.10.06 val PER: 0.1658
2026-01-05 16:09:03,856: t15.2023.10.08 val PER: 0.3302
2026-01-05 16:09:03,856: t15.2023.10.13 val PER: 0.2987
2026-01-05 16:09:03,856: t15.2023.10.15 val PER: 0.2432
2026-01-05 16:09:03,856: t15.2023.10.20 val PER: 0.2651
2026-01-05 16:09:03,856: t15.2023.10.22 val PER: 0.2082
2026-01-05 16:09:03,856: t15.2023.11.03 val PER: 0.2402
2026-01-05 16:09:03,856: t15.2023.11.04 val PER: 0.0614
2026-01-05 16:09:03,856: t15.2023.11.17 val PER: 0.1011
2026-01-05 16:09:03,856: t15.2023.11.19 val PER: 0.0998
2026-01-05 16:09:03,856: t15.2023.11.26 val PER: 0.2703
2026-01-05 16:09:03,856: t15.2023.12.03 val PER: 0.2185
2026-01-05 16:09:03,856: t15.2023.12.08 val PER: 0.2210
2026-01-05 16:09:03,856: t15.2023.12.10 val PER: 0.1840
2026-01-05 16:09:03,856: t15.2023.12.17 val PER: 0.2484
2026-01-05 16:09:03,857: t15.2023.12.29 val PER: 0.2471
2026-01-05 16:09:03,857: t15.2024.02.25 val PER: 0.2093
2026-01-05 16:09:03,857: t15.2024.03.08 val PER: 0.3215
2026-01-05 16:09:03,857: t15.2024.03.15 val PER: 0.3021
2026-01-05 16:09:03,857: t15.2024.03.17 val PER: 0.2636
2026-01-05 16:09:03,857: t15.2024.05.10 val PER: 0.2704
2026-01-05 16:09:03,857: t15.2024.06.14 val PER: 0.2650
2026-01-05 16:09:03,857: t15.2024.07.19 val PER: 0.3691
2026-01-05 16:09:03,857: t15.2024.07.21 val PER: 0.1841
2026-01-05 16:09:03,857: t15.2024.07.28 val PER: 0.2375
2026-01-05 16:09:03,857: t15.2025.01.10 val PER: 0.4201
2026-01-05 16:09:03,857: t15.2025.01.12 val PER: 0.2810
2026-01-05 16:09:03,857: t15.2025.03.14 val PER: 0.4231
2026-01-05 16:09:03,857: t15.2025.03.16 val PER: 0.3115
2026-01-05 16:09:03,857: t15.2025.03.30 val PER: 0.4149
2026-01-05 16:09:03,857: t15.2025.04.13 val PER: 0.3167
2026-01-05 16:09:04,129: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_4000
2026-01-05 16:09:23,906: Train batch 4200: loss: 22.69 grad norm: 66.50 time: 0.081
2026-01-05 16:09:43,855: Train batch 4400: loss: 17.03 grad norm: 54.57 time: 0.067
2026-01-05 16:09:53,805: Running test after training batch: 4500
2026-01-05 16:09:53,914: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:09:58,903: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-05 16:09:58,932: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cost get
2026-01-05 16:10:00,512: Val batch 4500: PER (avg): 0.2370 CTC Loss (avg): 23.1677 WER(1gram): 58.63% (n=64) time: 6.707
2026-01-05 16:10:00,513: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-05 16:10:00,513: t15.2023.08.13 val PER: 0.1985
2026-01-05 16:10:00,513: t15.2023.08.18 val PER: 0.1802
2026-01-05 16:10:00,513: t15.2023.08.20 val PER: 0.1922
2026-01-05 16:10:00,513: t15.2023.08.25 val PER: 0.1446
2026-01-05 16:10:00,513: t15.2023.08.27 val PER: 0.2395
2026-01-05 16:10:00,513: t15.2023.09.01 val PER: 0.1656
2026-01-05 16:10:00,513: t15.2023.09.03 val PER: 0.2447
2026-01-05 16:10:00,514: t15.2023.09.24 val PER: 0.1772
2026-01-05 16:10:00,514: t15.2023.09.29 val PER: 0.1953
2026-01-05 16:10:00,514: t15.2023.10.01 val PER: 0.2550
2026-01-05 16:10:00,514: t15.2023.10.06 val PER: 0.1615
2026-01-05 16:10:00,514: t15.2023.10.08 val PER: 0.3126
2026-01-05 16:10:00,514: t15.2023.10.13 val PER: 0.2925
2026-01-05 16:10:00,514: t15.2023.10.15 val PER: 0.2281
2026-01-05 16:10:00,514: t15.2023.10.20 val PER: 0.2483
2026-01-05 16:10:00,514: t15.2023.10.22 val PER: 0.1782
2026-01-05 16:10:00,514: t15.2023.11.03 val PER: 0.2354
2026-01-05 16:10:00,514: t15.2023.11.04 val PER: 0.0478
2026-01-05 16:10:00,514: t15.2023.11.17 val PER: 0.1026
2026-01-05 16:10:00,514: t15.2023.11.19 val PER: 0.0998
2026-01-05 16:10:00,514: t15.2023.11.26 val PER: 0.2652
2026-01-05 16:10:00,515: t15.2023.12.03 val PER: 0.2111
2026-01-05 16:10:00,515: t15.2023.12.08 val PER: 0.2144
2026-01-05 16:10:00,515: t15.2023.12.10 val PER: 0.1774
2026-01-05 16:10:00,515: t15.2023.12.17 val PER: 0.2256
2026-01-05 16:10:00,515: t15.2023.12.29 val PER: 0.2375
2026-01-05 16:10:00,515: t15.2024.02.25 val PER: 0.2008
2026-01-05 16:10:00,515: t15.2024.03.08 val PER: 0.3215
2026-01-05 16:10:00,515: t15.2024.03.15 val PER: 0.2883
2026-01-05 16:10:00,515: t15.2024.03.17 val PER: 0.2538
2026-01-05 16:10:00,515: t15.2024.05.10 val PER: 0.2496
2026-01-05 16:10:00,515: t15.2024.06.14 val PER: 0.2382
2026-01-05 16:10:00,515: t15.2024.07.19 val PER: 0.3401
2026-01-05 16:10:00,515: t15.2024.07.21 val PER: 0.1724
2026-01-05 16:10:00,515: t15.2024.07.28 val PER: 0.2235
2026-01-05 16:10:00,515: t15.2025.01.10 val PER: 0.4146
2026-01-05 16:10:00,515: t15.2025.01.12 val PER: 0.2664
2026-01-05 16:10:00,516: t15.2025.03.14 val PER: 0.3905
2026-01-05 16:10:00,516: t15.2025.03.16 val PER: 0.2880
2026-01-05 16:10:00,516: t15.2025.03.30 val PER: 0.3966
2026-01-05 16:10:00,516: t15.2025.04.13 val PER: 0.2953
2026-01-05 16:10:00,517: New best val WER(1gram) 64.97% --> 58.63%
2026-01-05 16:10:00,517: Checkpointing model
2026-01-05 16:10:01,950: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:10:02,226: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_4500
2026-01-05 16:10:11,976: Train batch 4600: loss: 20.20 grad norm: 71.53 time: 0.063
2026-01-05 16:10:31,836: Train batch 4800: loss: 13.55 grad norm: 52.54 time: 0.065
2026-01-05 16:10:51,834: Train batch 5000: loss: 32.26 grad norm: 86.03 time: 0.065
2026-01-05 16:10:51,834: Running test after training batch: 5000
2026-01-05 16:10:51,998: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:10:57,073: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point as will
2026-01-05 16:10:57,102: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it yip the cost get
2026-01-05 16:10:58,694: Val batch 5000: PER (avg): 0.2247 CTC Loss (avg): 21.8586 WER(1gram): 59.90% (n=64) time: 6.860
2026-01-05 16:10:58,695: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-05 16:10:58,695: t15.2023.08.13 val PER: 0.1871
2026-01-05 16:10:58,695: t15.2023.08.18 val PER: 0.1744
2026-01-05 16:10:58,695: t15.2023.08.20 val PER: 0.1771
2026-01-05 16:10:58,695: t15.2023.08.25 val PER: 0.1370
2026-01-05 16:10:58,695: t15.2023.08.27 val PER: 0.2363
2026-01-05 16:10:58,695: t15.2023.09.01 val PER: 0.1347
2026-01-05 16:10:58,695: t15.2023.09.03 val PER: 0.2387
2026-01-05 16:10:58,695: t15.2023.09.24 val PER: 0.1833
2026-01-05 16:10:58,695: t15.2023.09.29 val PER: 0.1780
2026-01-05 16:10:58,696: t15.2023.10.01 val PER: 0.2444
2026-01-05 16:10:58,696: t15.2023.10.06 val PER: 0.1389
2026-01-05 16:10:58,696: t15.2023.10.08 val PER: 0.3085
2026-01-05 16:10:58,696: t15.2023.10.13 val PER: 0.2801
2026-01-05 16:10:58,696: t15.2023.10.15 val PER: 0.2116
2026-01-05 16:10:58,696: t15.2023.10.20 val PER: 0.2248
2026-01-05 16:10:58,696: t15.2023.10.22 val PER: 0.1759
2026-01-05 16:10:58,696: t15.2023.11.03 val PER: 0.2198
2026-01-05 16:10:58,696: t15.2023.11.04 val PER: 0.0512
2026-01-05 16:10:58,696: t15.2023.11.17 val PER: 0.0762
2026-01-05 16:10:58,696: t15.2023.11.19 val PER: 0.0739
2026-01-05 16:10:58,696: t15.2023.11.26 val PER: 0.2290
2026-01-05 16:10:58,696: t15.2023.12.03 val PER: 0.1943
2026-01-05 16:10:58,696: t15.2023.12.08 val PER: 0.2017
2026-01-05 16:10:58,697: t15.2023.12.10 val PER: 0.1656
2026-01-05 16:10:58,697: t15.2023.12.17 val PER: 0.2318
2026-01-05 16:10:58,697: t15.2023.12.29 val PER: 0.2128
2026-01-05 16:10:58,697: t15.2024.02.25 val PER: 0.1924
2026-01-05 16:10:58,697: t15.2024.03.08 val PER: 0.3201
2026-01-05 16:10:58,697: t15.2024.03.15 val PER: 0.2664
2026-01-05 16:10:58,697: t15.2024.03.17 val PER: 0.2420
2026-01-05 16:10:58,697: t15.2024.05.10 val PER: 0.2526
2026-01-05 16:10:58,697: t15.2024.06.14 val PER: 0.2350
2026-01-05 16:10:58,697: t15.2024.07.19 val PER: 0.3342
2026-01-05 16:10:58,697: t15.2024.07.21 val PER: 0.1800
2026-01-05 16:10:58,697: t15.2024.07.28 val PER: 0.2110
2026-01-05 16:10:58,697: t15.2025.01.10 val PER: 0.3912
2026-01-05 16:10:58,698: t15.2025.01.12 val PER: 0.2440
2026-01-05 16:10:58,698: t15.2025.03.14 val PER: 0.3905
2026-01-05 16:10:58,698: t15.2025.03.16 val PER: 0.2762
2026-01-05 16:10:58,698: t15.2025.03.30 val PER: 0.3885
2026-01-05 16:10:58,698: t15.2025.04.13 val PER: 0.3010
2026-01-05 16:10:58,982: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_5000
2026-01-05 16:11:18,864: Train batch 5200: loss: 16.76 grad norm: 62.66 time: 0.052
2026-01-05 16:11:38,701: Train batch 5400: loss: 17.56 grad norm: 58.37 time: 0.070
2026-01-05 16:11:48,396: Running test after training batch: 5500
2026-01-05 16:11:48,496: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:11:53,750: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point will
2026-01-05 16:11:53,779: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost tet
2026-01-05 16:11:55,353: Val batch 5500: PER (avg): 0.2160 CTC Loss (avg): 21.0533 WER(1gram): 57.11% (n=64) time: 6.957
2026-01-05 16:11:55,354: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=11
2026-01-05 16:11:55,354: t15.2023.08.13 val PER: 0.1788
2026-01-05 16:11:55,354: t15.2023.08.18 val PER: 0.1710
2026-01-05 16:11:55,354: t15.2023.08.20 val PER: 0.1724
2026-01-05 16:11:55,354: t15.2023.08.25 val PER: 0.1280
2026-01-05 16:11:55,355: t15.2023.08.27 val PER: 0.2379
2026-01-05 16:11:55,355: t15.2023.09.01 val PER: 0.1234
2026-01-05 16:11:55,355: t15.2023.09.03 val PER: 0.2173
2026-01-05 16:11:55,355: t15.2023.09.24 val PER: 0.1663
2026-01-05 16:11:55,355: t15.2023.09.29 val PER: 0.1729
2026-01-05 16:11:55,355: t15.2023.10.01 val PER: 0.2305
2026-01-05 16:11:55,355: t15.2023.10.06 val PER: 0.1346
2026-01-05 16:11:55,355: t15.2023.10.08 val PER: 0.2991
2026-01-05 16:11:55,355: t15.2023.10.13 val PER: 0.2723
2026-01-05 16:11:55,356: t15.2023.10.15 val PER: 0.2136
2026-01-05 16:11:55,356: t15.2023.10.20 val PER: 0.2315
2026-01-05 16:11:55,356: t15.2023.10.22 val PER: 0.1693
2026-01-05 16:11:55,356: t15.2023.11.03 val PER: 0.2205
2026-01-05 16:11:55,356: t15.2023.11.04 val PER: 0.0580
2026-01-05 16:11:55,356: t15.2023.11.17 val PER: 0.0855
2026-01-05 16:11:55,356: t15.2023.11.19 val PER: 0.0818
2026-01-05 16:11:55,356: t15.2023.11.26 val PER: 0.2217
2026-01-05 16:11:55,356: t15.2023.12.03 val PER: 0.1912
2026-01-05 16:11:55,356: t15.2023.12.08 val PER: 0.1884
2026-01-05 16:11:55,356: t15.2023.12.10 val PER: 0.1498
2026-01-05 16:11:55,356: t15.2023.12.17 val PER: 0.2141
2026-01-05 16:11:55,356: t15.2023.12.29 val PER: 0.2169
2026-01-05 16:11:55,356: t15.2024.02.25 val PER: 0.1798
2026-01-05 16:11:55,356: t15.2024.03.08 val PER: 0.2987
2026-01-05 16:11:55,356: t15.2024.03.15 val PER: 0.2652
2026-01-05 16:11:55,356: t15.2024.03.17 val PER: 0.2169
2026-01-05 16:11:55,357: t15.2024.05.10 val PER: 0.2392
2026-01-05 16:11:55,357: t15.2024.06.14 val PER: 0.2303
2026-01-05 16:11:55,357: t15.2024.07.19 val PER: 0.3151
2026-01-05 16:11:55,357: t15.2024.07.21 val PER: 0.1600
2026-01-05 16:11:55,357: t15.2024.07.28 val PER: 0.2110
2026-01-05 16:11:55,357: t15.2025.01.10 val PER: 0.3884
2026-01-05 16:11:55,357: t15.2025.01.12 val PER: 0.2325
2026-01-05 16:11:55,357: t15.2025.03.14 val PER: 0.3595
2026-01-05 16:11:55,357: t15.2025.03.16 val PER: 0.2801
2026-01-05 16:11:55,357: t15.2025.03.30 val PER: 0.3540
2026-01-05 16:11:55,357: t15.2025.04.13 val PER: 0.2967
2026-01-05 16:11:55,358: New best val WER(1gram) 58.63% --> 57.11%
2026-01-05 16:11:55,359: Checkpointing model
2026-01-05 16:11:56,738: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:11:57,074: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_5500
2026-01-05 16:12:06,535: Train batch 5600: loss: 19.32 grad norm: 64.46 time: 0.063
2026-01-05 16:12:25,883: Train batch 5800: loss: 13.77 grad norm: 58.24 time: 0.083
2026-01-05 16:12:45,099: Train batch 6000: loss: 14.21 grad norm: 58.83 time: 0.050
2026-01-05 16:12:45,099: Running test after training batch: 6000
2026-01-05 16:12:45,222: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:12:50,193: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-05 16:12:50,224: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-05 16:12:51,853: Val batch 6000: PER (avg): 0.2109 CTC Loss (avg): 20.8232 WER(1gram): 57.61% (n=64) time: 6.754
2026-01-05 16:12:51,854: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-05 16:12:51,854: t15.2023.08.13 val PER: 0.1767
2026-01-05 16:12:51,854: t15.2023.08.18 val PER: 0.1660
2026-01-05 16:12:51,855: t15.2023.08.20 val PER: 0.1604
2026-01-05 16:12:51,855: t15.2023.08.25 val PER: 0.1160
2026-01-05 16:12:51,855: t15.2023.08.27 val PER: 0.2412
2026-01-05 16:12:51,855: t15.2023.09.01 val PER: 0.1331
2026-01-05 16:12:51,855: t15.2023.09.03 val PER: 0.2090
2026-01-05 16:12:51,855: t15.2023.09.24 val PER: 0.1638
2026-01-05 16:12:51,855: t15.2023.09.29 val PER: 0.1646
2026-01-05 16:12:51,855: t15.2023.10.01 val PER: 0.2160
2026-01-05 16:12:51,855: t15.2023.10.06 val PER: 0.1356
2026-01-05 16:12:51,855: t15.2023.10.08 val PER: 0.2828
2026-01-05 16:12:51,855: t15.2023.10.13 val PER: 0.2708
2026-01-05 16:12:51,855: t15.2023.10.15 val PER: 0.2156
2026-01-05 16:12:51,855: t15.2023.10.20 val PER: 0.2181
2026-01-05 16:12:51,855: t15.2023.10.22 val PER: 0.1648
2026-01-05 16:12:51,856: t15.2023.11.03 val PER: 0.2212
2026-01-05 16:12:51,856: t15.2023.11.04 val PER: 0.0648
2026-01-05 16:12:51,856: t15.2023.11.17 val PER: 0.0684
2026-01-05 16:12:51,856: t15.2023.11.19 val PER: 0.0918
2026-01-05 16:12:51,856: t15.2023.11.26 val PER: 0.2181
2026-01-05 16:12:51,856: t15.2023.12.03 val PER: 0.1765
2026-01-05 16:12:51,856: t15.2023.12.08 val PER: 0.1811
2026-01-05 16:12:51,856: t15.2023.12.10 val PER: 0.1498
2026-01-05 16:12:51,856: t15.2023.12.17 val PER: 0.1892
2026-01-05 16:12:51,856: t15.2023.12.29 val PER: 0.2141
2026-01-05 16:12:51,856: t15.2024.02.25 val PER: 0.1643
2026-01-05 16:12:51,856: t15.2024.03.08 val PER: 0.3073
2026-01-05 16:12:51,856: t15.2024.03.15 val PER: 0.2639
2026-01-05 16:12:51,856: t15.2024.03.17 val PER: 0.2064
2026-01-05 16:12:51,856: t15.2024.05.10 val PER: 0.2273
2026-01-05 16:12:51,857: t15.2024.06.14 val PER: 0.2114
2026-01-05 16:12:51,857: t15.2024.07.19 val PER: 0.3131
2026-01-05 16:12:51,857: t15.2024.07.21 val PER: 0.1683
2026-01-05 16:12:51,857: t15.2024.07.28 val PER: 0.2022
2026-01-05 16:12:51,857: t15.2025.01.10 val PER: 0.3678
2026-01-05 16:12:51,857: t15.2025.01.12 val PER: 0.2202
2026-01-05 16:12:51,857: t15.2025.03.14 val PER: 0.3905
2026-01-05 16:12:51,857: t15.2025.03.16 val PER: 0.2565
2026-01-05 16:12:51,857: t15.2025.03.30 val PER: 0.3770
2026-01-05 16:12:51,857: t15.2025.04.13 val PER: 0.2753
2026-01-05 16:12:52,137: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_6000
2026-01-05 16:13:10,814: Train batch 6200: loss: 16.75 grad norm: 60.96 time: 0.071
2026-01-05 16:13:29,457: Train batch 6400: loss: 18.54 grad norm: 65.23 time: 0.063
2026-01-05 16:13:38,529: Running test after training batch: 6500
2026-01-05 16:13:38,673: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:13:43,896: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 16:13:43,927: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost get
2026-01-05 16:13:45,703: Val batch 6500: PER (avg): 0.2020 CTC Loss (avg): 19.9861 WER(1gram): 52.54% (n=64) time: 7.174
2026-01-05 16:13:45,704: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 16:13:45,704: t15.2023.08.13 val PER: 0.1632
2026-01-05 16:13:45,704: t15.2023.08.18 val PER: 0.1391
2026-01-05 16:13:45,704: t15.2023.08.20 val PER: 0.1589
2026-01-05 16:13:45,704: t15.2023.08.25 val PER: 0.1114
2026-01-05 16:13:45,704: t15.2023.08.27 val PER: 0.2299
2026-01-05 16:13:45,704: t15.2023.09.01 val PER: 0.1201
2026-01-05 16:13:45,705: t15.2023.09.03 val PER: 0.2043
2026-01-05 16:13:45,705: t15.2023.09.24 val PER: 0.1735
2026-01-05 16:13:45,705: t15.2023.09.29 val PER: 0.1608
2026-01-05 16:13:45,705: t15.2023.10.01 val PER: 0.2232
2026-01-05 16:13:45,705: t15.2023.10.06 val PER: 0.1216
2026-01-05 16:13:45,705: t15.2023.10.08 val PER: 0.2909
2026-01-05 16:13:45,705: t15.2023.10.13 val PER: 0.2614
2026-01-05 16:13:45,705: t15.2023.10.15 val PER: 0.2083
2026-01-05 16:13:45,705: t15.2023.10.20 val PER: 0.2047
2026-01-05 16:13:45,705: t15.2023.10.22 val PER: 0.1659
2026-01-05 16:13:45,705: t15.2023.11.03 val PER: 0.2178
2026-01-05 16:13:45,706: t15.2023.11.04 val PER: 0.0614
2026-01-05 16:13:45,706: t15.2023.11.17 val PER: 0.0653
2026-01-05 16:13:45,706: t15.2023.11.19 val PER: 0.0778
2026-01-05 16:13:45,706: t15.2023.11.26 val PER: 0.2138
2026-01-05 16:13:45,706: t15.2023.12.03 val PER: 0.1702
2026-01-05 16:13:45,706: t15.2023.12.08 val PER: 0.1664
2026-01-05 16:13:45,706: t15.2023.12.10 val PER: 0.1459
2026-01-05 16:13:45,706: t15.2023.12.17 val PER: 0.1809
2026-01-05 16:13:45,706: t15.2023.12.29 val PER: 0.1997
2026-01-05 16:13:45,706: t15.2024.02.25 val PER: 0.1615
2026-01-05 16:13:45,706: t15.2024.03.08 val PER: 0.2731
2026-01-05 16:13:45,707: t15.2024.03.15 val PER: 0.2545
2026-01-05 16:13:45,707: t15.2024.03.17 val PER: 0.1994
2026-01-05 16:13:45,707: t15.2024.05.10 val PER: 0.2244
2026-01-05 16:13:45,707: t15.2024.06.14 val PER: 0.2114
2026-01-05 16:13:45,707: t15.2024.07.19 val PER: 0.2966
2026-01-05 16:13:45,707: t15.2024.07.21 val PER: 0.1476
2026-01-05 16:13:45,707: t15.2024.07.28 val PER: 0.1897
2026-01-05 16:13:45,707: t15.2025.01.10 val PER: 0.3567
2026-01-05 16:13:45,707: t15.2025.01.12 val PER: 0.2117
2026-01-05 16:13:45,707: t15.2025.03.14 val PER: 0.3905
2026-01-05 16:13:45,707: t15.2025.03.16 val PER: 0.2264
2026-01-05 16:13:45,707: t15.2025.03.30 val PER: 0.3494
2026-01-05 16:13:45,708: t15.2025.04.13 val PER: 0.2725
2026-01-05 16:13:45,708: New best val WER(1gram) 57.11% --> 52.54%
2026-01-05 16:13:45,708: Checkpointing model
2026-01-05 16:13:47,284: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:13:47,580: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_6500
2026-01-05 16:13:56,964: Train batch 6600: loss: 12.58 grad norm: 55.37 time: 0.045
2026-01-05 16:14:15,842: Train batch 6800: loss: 15.49 grad norm: 59.82 time: 0.049
2026-01-05 16:14:34,808: Train batch 7000: loss: 17.36 grad norm: 63.64 time: 0.061
2026-01-05 16:14:34,810: Running test after training batch: 7000
2026-01-05 16:14:34,983: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:14:40,216: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 16:14:40,247: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-05 16:14:42,063: Val batch 7000: PER (avg): 0.1940 CTC Loss (avg): 19.0887 WER(1gram): 52.79% (n=64) time: 7.253
2026-01-05 16:14:42,063: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-05 16:14:42,064: t15.2023.08.13 val PER: 0.1538
2026-01-05 16:14:42,064: t15.2023.08.18 val PER: 0.1383
2026-01-05 16:14:42,064: t15.2023.08.20 val PER: 0.1549
2026-01-05 16:14:42,064: t15.2023.08.25 val PER: 0.1024
2026-01-05 16:14:42,064: t15.2023.08.27 val PER: 0.2219
2026-01-05 16:14:42,064: t15.2023.09.01 val PER: 0.1096
2026-01-05 16:14:42,064: t15.2023.09.03 val PER: 0.1948
2026-01-05 16:14:42,064: t15.2023.09.24 val PER: 0.1626
2026-01-05 16:14:42,064: t15.2023.09.29 val PER: 0.1672
2026-01-05 16:14:42,064: t15.2023.10.01 val PER: 0.2008
2026-01-05 16:14:42,064: t15.2023.10.06 val PER: 0.1173
2026-01-05 16:14:42,064: t15.2023.10.08 val PER: 0.2788
2026-01-05 16:14:42,064: t15.2023.10.13 val PER: 0.2490
2026-01-05 16:14:42,065: t15.2023.10.15 val PER: 0.2011
2026-01-05 16:14:42,065: t15.2023.10.20 val PER: 0.2114
2026-01-05 16:14:42,065: t15.2023.10.22 val PER: 0.1359
2026-01-05 16:14:42,065: t15.2023.11.03 val PER: 0.1967
2026-01-05 16:14:42,065: t15.2023.11.04 val PER: 0.0410
2026-01-05 16:14:42,065: t15.2023.11.17 val PER: 0.0638
2026-01-05 16:14:42,065: t15.2023.11.19 val PER: 0.0579
2026-01-05 16:14:42,065: t15.2023.11.26 val PER: 0.1978
2026-01-05 16:14:42,065: t15.2023.12.03 val PER: 0.1660
2026-01-05 16:14:42,065: t15.2023.12.08 val PER: 0.1618
2026-01-05 16:14:42,065: t15.2023.12.10 val PER: 0.1459
2026-01-05 16:14:42,065: t15.2023.12.17 val PER: 0.1819
2026-01-05 16:14:42,065: t15.2023.12.29 val PER: 0.1901
2026-01-05 16:14:42,065: t15.2024.02.25 val PER: 0.1601
2026-01-05 16:14:42,066: t15.2024.03.08 val PER: 0.2802
2026-01-05 16:14:42,066: t15.2024.03.15 val PER: 0.2427
2026-01-05 16:14:42,066: t15.2024.03.17 val PER: 0.1967
2026-01-05 16:14:42,066: t15.2024.05.10 val PER: 0.2021
2026-01-05 16:14:42,066: t15.2024.06.14 val PER: 0.2177
2026-01-05 16:14:42,066: t15.2024.07.19 val PER: 0.3006
2026-01-05 16:14:42,066: t15.2024.07.21 val PER: 0.1269
2026-01-05 16:14:42,066: t15.2024.07.28 val PER: 0.1735
2026-01-05 16:14:42,066: t15.2025.01.10 val PER: 0.3705
2026-01-05 16:14:42,066: t15.2025.01.12 val PER: 0.2017
2026-01-05 16:14:42,067: t15.2025.03.14 val PER: 0.3491
2026-01-05 16:14:42,067: t15.2025.03.16 val PER: 0.2343
2026-01-05 16:14:42,067: t15.2025.03.30 val PER: 0.3621
2026-01-05 16:14:42,067: t15.2025.04.13 val PER: 0.2682
2026-01-05 16:14:42,350: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_7000
2026-01-05 16:15:03,046: Train batch 7200: loss: 14.44 grad norm: 60.89 time: 0.082
2026-01-05 16:15:22,157: Train batch 7400: loss: 13.70 grad norm: 53.21 time: 0.075
2026-01-05 16:15:31,670: Running test after training batch: 7500
2026-01-05 16:15:31,787: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:15:36,815: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-05 16:15:36,844: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost it
2026-01-05 16:15:38,504: Val batch 7500: PER (avg): 0.1886 CTC Loss (avg): 18.6565 WER(1gram): 53.55% (n=64) time: 6.834
2026-01-05 16:15:38,504: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-05 16:15:38,504: t15.2023.08.13 val PER: 0.1518
2026-01-05 16:15:38,505: t15.2023.08.18 val PER: 0.1341
2026-01-05 16:15:38,505: t15.2023.08.20 val PER: 0.1430
2026-01-05 16:15:38,505: t15.2023.08.25 val PER: 0.1130
2026-01-05 16:15:38,505: t15.2023.08.27 val PER: 0.2186
2026-01-05 16:15:38,505: t15.2023.09.01 val PER: 0.1161
2026-01-05 16:15:38,505: t15.2023.09.03 val PER: 0.1865
2026-01-05 16:15:38,505: t15.2023.09.24 val PER: 0.1614
2026-01-05 16:15:38,506: t15.2023.09.29 val PER: 0.1538
2026-01-05 16:15:38,506: t15.2023.10.01 val PER: 0.2034
2026-01-05 16:15:38,506: t15.2023.10.06 val PER: 0.1163
2026-01-05 16:15:38,506: t15.2023.10.08 val PER: 0.2760
2026-01-05 16:15:38,506: t15.2023.10.13 val PER: 0.2568
2026-01-05 16:15:38,506: t15.2023.10.15 val PER: 0.1938
2026-01-05 16:15:38,506: t15.2023.10.20 val PER: 0.1980
2026-01-05 16:15:38,506: t15.2023.10.22 val PER: 0.1359
2026-01-05 16:15:38,506: t15.2023.11.03 val PER: 0.2022
2026-01-05 16:15:38,506: t15.2023.11.04 val PER: 0.0444
2026-01-05 16:15:38,506: t15.2023.11.17 val PER: 0.0653
2026-01-05 16:15:38,506: t15.2023.11.19 val PER: 0.0579
2026-01-05 16:15:38,506: t15.2023.11.26 val PER: 0.1833
2026-01-05 16:15:38,506: t15.2023.12.03 val PER: 0.1607
2026-01-05 16:15:38,507: t15.2023.12.08 val PER: 0.1511
2026-01-05 16:15:38,507: t15.2023.12.10 val PER: 0.1314
2026-01-05 16:15:38,507: t15.2023.12.17 val PER: 0.1674
2026-01-05 16:15:38,507: t15.2023.12.29 val PER: 0.1887
2026-01-05 16:15:38,507: t15.2024.02.25 val PER: 0.1461
2026-01-05 16:15:38,507: t15.2024.03.08 val PER: 0.2731
2026-01-05 16:15:38,507: t15.2024.03.15 val PER: 0.2402
2026-01-05 16:15:38,507: t15.2024.03.17 val PER: 0.1750
2026-01-05 16:15:38,507: t15.2024.05.10 val PER: 0.1991
2026-01-05 16:15:38,507: t15.2024.06.14 val PER: 0.1924
2026-01-05 16:15:38,507: t15.2024.07.19 val PER: 0.2854
2026-01-05 16:15:38,507: t15.2024.07.21 val PER: 0.1393
2026-01-05 16:15:38,507: t15.2024.07.28 val PER: 0.1654
2026-01-05 16:15:38,508: t15.2025.01.10 val PER: 0.3402
2026-01-05 16:15:38,508: t15.2025.01.12 val PER: 0.1909
2026-01-05 16:15:38,508: t15.2025.03.14 val PER: 0.3654
2026-01-05 16:15:38,508: t15.2025.03.16 val PER: 0.2435
2026-01-05 16:15:38,508: t15.2025.03.30 val PER: 0.3517
2026-01-05 16:15:38,508: t15.2025.04.13 val PER: 0.2511
2026-01-05 16:15:38,776: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_7500
2026-01-05 16:15:47,879: Train batch 7600: loss: 16.20 grad norm: 60.83 time: 0.070
2026-01-05 16:16:06,087: Train batch 7800: loss: 14.07 grad norm: 56.68 time: 0.056
2026-01-05 16:16:24,702: Train batch 8000: loss: 11.31 grad norm: 52.15 time: 0.072
2026-01-05 16:16:24,703: Running test after training batch: 8000
2026-01-05 16:16:24,809: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:16:30,027: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-05 16:16:30,062: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-05 16:16:31,996: Val batch 8000: PER (avg): 0.1854 CTC Loss (avg): 18.0925 WER(1gram): 55.84% (n=64) time: 7.293
2026-01-05 16:16:31,996: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-05 16:16:31,997: t15.2023.08.13 val PER: 0.1424
2026-01-05 16:16:31,997: t15.2023.08.18 val PER: 0.1282
2026-01-05 16:16:31,997: t15.2023.08.20 val PER: 0.1398
2026-01-05 16:16:31,997: t15.2023.08.25 val PER: 0.1190
2026-01-05 16:16:31,997: t15.2023.08.27 val PER: 0.2138
2026-01-05 16:16:31,998: t15.2023.09.01 val PER: 0.1039
2026-01-05 16:16:31,998: t15.2023.09.03 val PER: 0.1876
2026-01-05 16:16:31,998: t15.2023.09.24 val PER: 0.1529
2026-01-05 16:16:31,998: t15.2023.09.29 val PER: 0.1493
2026-01-05 16:16:31,998: t15.2023.10.01 val PER: 0.2048
2026-01-05 16:16:31,998: t15.2023.10.06 val PER: 0.1087
2026-01-05 16:16:31,998: t15.2023.10.08 val PER: 0.2652
2026-01-05 16:16:31,998: t15.2023.10.13 val PER: 0.2521
2026-01-05 16:16:31,999: t15.2023.10.15 val PER: 0.1991
2026-01-05 16:16:31,999: t15.2023.10.20 val PER: 0.2047
2026-01-05 16:16:31,999: t15.2023.10.22 val PER: 0.1403
2026-01-05 16:16:31,999: t15.2023.11.03 val PER: 0.2015
2026-01-05 16:16:31,999: t15.2023.11.04 val PER: 0.0375
2026-01-05 16:16:31,999: t15.2023.11.17 val PER: 0.0607
2026-01-05 16:16:31,999: t15.2023.11.19 val PER: 0.0679
2026-01-05 16:16:31,999: t15.2023.11.26 val PER: 0.1790
2026-01-05 16:16:31,999: t15.2023.12.03 val PER: 0.1586
2026-01-05 16:16:31,999: t15.2023.12.08 val PER: 0.1451
2026-01-05 16:16:31,999: t15.2023.12.10 val PER: 0.1301
2026-01-05 16:16:32,000: t15.2023.12.17 val PER: 0.1663
2026-01-05 16:16:32,000: t15.2023.12.29 val PER: 0.1764
2026-01-05 16:16:32,000: t15.2024.02.25 val PER: 0.1475
2026-01-05 16:16:32,000: t15.2024.03.08 val PER: 0.2788
2026-01-05 16:16:32,000: t15.2024.03.15 val PER: 0.2339
2026-01-05 16:16:32,000: t15.2024.03.17 val PER: 0.1785
2026-01-05 16:16:32,000: t15.2024.05.10 val PER: 0.1887
2026-01-05 16:16:32,000: t15.2024.06.14 val PER: 0.1830
2026-01-05 16:16:32,000: t15.2024.07.19 val PER: 0.2947
2026-01-05 16:16:32,000: t15.2024.07.21 val PER: 0.1207
2026-01-05 16:16:32,000: t15.2024.07.28 val PER: 0.1529
2026-01-05 16:16:32,000: t15.2025.01.10 val PER: 0.3320
2026-01-05 16:16:32,001: t15.2025.01.12 val PER: 0.1901
2026-01-05 16:16:32,001: t15.2025.03.14 val PER: 0.3728
2026-01-05 16:16:32,001: t15.2025.03.16 val PER: 0.2330
2026-01-05 16:16:32,001: t15.2025.03.30 val PER: 0.3586
2026-01-05 16:16:32,001: t15.2025.04.13 val PER: 0.2625
2026-01-05 16:16:32,280: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_8000
2026-01-05 16:16:51,062: Train batch 8200: loss: 9.60 grad norm: 48.05 time: 0.057
2026-01-05 16:17:09,709: Train batch 8400: loss: 9.98 grad norm: 47.83 time: 0.064
2026-01-05 16:17:19,173: Running test after training batch: 8500
2026-01-05 16:17:19,295: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:17:24,527: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 16:17:24,557: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-05 16:17:26,258: Val batch 8500: PER (avg): 0.1788 CTC Loss (avg): 17.6881 WER(1gram): 48.98% (n=64) time: 7.084
2026-01-05 16:17:26,258: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-05 16:17:26,258: t15.2023.08.13 val PER: 0.1289
2026-01-05 16:17:26,258: t15.2023.08.18 val PER: 0.1366
2026-01-05 16:17:26,258: t15.2023.08.20 val PER: 0.1350
2026-01-05 16:17:26,259: t15.2023.08.25 val PER: 0.1130
2026-01-05 16:17:26,259: t15.2023.08.27 val PER: 0.2122
2026-01-05 16:17:26,259: t15.2023.09.01 val PER: 0.1023
2026-01-05 16:17:26,259: t15.2023.09.03 val PER: 0.1960
2026-01-05 16:17:26,259: t15.2023.09.24 val PER: 0.1468
2026-01-05 16:17:26,259: t15.2023.09.29 val PER: 0.1455
2026-01-05 16:17:26,259: t15.2023.10.01 val PER: 0.1896
2026-01-05 16:17:26,259: t15.2023.10.06 val PER: 0.0990
2026-01-05 16:17:26,259: t15.2023.10.08 val PER: 0.2639
2026-01-05 16:17:26,259: t15.2023.10.13 val PER: 0.2397
2026-01-05 16:17:26,259: t15.2023.10.15 val PER: 0.1846
2026-01-05 16:17:26,259: t15.2023.10.20 val PER: 0.2349
2026-01-05 16:17:26,259: t15.2023.10.22 val PER: 0.1448
2026-01-05 16:17:26,260: t15.2023.11.03 val PER: 0.1900
2026-01-05 16:17:26,260: t15.2023.11.04 val PER: 0.0512
2026-01-05 16:17:26,260: t15.2023.11.17 val PER: 0.0482
2026-01-05 16:17:26,260: t15.2023.11.19 val PER: 0.0539
2026-01-05 16:17:26,260: t15.2023.11.26 val PER: 0.1739
2026-01-05 16:17:26,260: t15.2023.12.03 val PER: 0.1481
2026-01-05 16:17:26,260: t15.2023.12.08 val PER: 0.1425
2026-01-05 16:17:26,260: t15.2023.12.10 val PER: 0.1183
2026-01-05 16:17:26,260: t15.2023.12.17 val PER: 0.1642
2026-01-05 16:17:26,260: t15.2023.12.29 val PER: 0.1682
2026-01-05 16:17:26,260: t15.2024.02.25 val PER: 0.1433
2026-01-05 16:17:26,260: t15.2024.03.08 val PER: 0.2632
2026-01-05 16:17:26,260: t15.2024.03.15 val PER: 0.2258
2026-01-05 16:17:26,260: t15.2024.03.17 val PER: 0.1653
2026-01-05 16:17:26,260: t15.2024.05.10 val PER: 0.1828
2026-01-05 16:17:26,260: t15.2024.06.14 val PER: 0.1893
2026-01-05 16:17:26,261: t15.2024.07.19 val PER: 0.2762
2026-01-05 16:17:26,261: t15.2024.07.21 val PER: 0.1228
2026-01-05 16:17:26,261: t15.2024.07.28 val PER: 0.1691
2026-01-05 16:17:26,261: t15.2025.01.10 val PER: 0.3306
2026-01-05 16:17:26,261: t15.2025.01.12 val PER: 0.1794
2026-01-05 16:17:26,261: t15.2025.03.14 val PER: 0.3521
2026-01-05 16:17:26,261: t15.2025.03.16 val PER: 0.2107
2026-01-05 16:17:26,261: t15.2025.03.30 val PER: 0.3448
2026-01-05 16:17:26,261: t15.2025.04.13 val PER: 0.2354
2026-01-05 16:17:26,262: New best val WER(1gram) 52.54% --> 48.98%
2026-01-05 16:17:26,262: Checkpointing model
2026-01-05 16:17:27,860: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:17:28,155: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_8500
2026-01-05 16:17:38,080: Train batch 8600: loss: 15.51 grad norm: 58.63 time: 0.055
2026-01-05 16:17:57,303: Train batch 8800: loss: 14.98 grad norm: 58.25 time: 0.061
2026-01-05 16:18:16,648: Train batch 9000: loss: 15.76 grad norm: 64.39 time: 0.073
2026-01-05 16:18:16,648: Running test after training batch: 9000
2026-01-05 16:18:16,787: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:18:21,960: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 16:18:21,992: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost get
2026-01-05 16:18:23,894: Val batch 9000: PER (avg): 0.1735 CTC Loss (avg): 17.2176 WER(1gram): 51.27% (n=64) time: 7.245
2026-01-05 16:18:23,894: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 16:18:23,895: t15.2023.08.13 val PER: 0.1341
2026-01-05 16:18:23,895: t15.2023.08.18 val PER: 0.1224
2026-01-05 16:18:23,895: t15.2023.08.20 val PER: 0.1319
2026-01-05 16:18:23,895: t15.2023.08.25 val PER: 0.1099
2026-01-05 16:18:23,895: t15.2023.08.27 val PER: 0.2170
2026-01-05 16:18:23,895: t15.2023.09.01 val PER: 0.0950
2026-01-05 16:18:23,895: t15.2023.09.03 val PER: 0.1793
2026-01-05 16:18:23,895: t15.2023.09.24 val PER: 0.1481
2026-01-05 16:18:23,895: t15.2023.09.29 val PER: 0.1423
2026-01-05 16:18:23,895: t15.2023.10.01 val PER: 0.1909
2026-01-05 16:18:23,895: t15.2023.10.06 val PER: 0.0915
2026-01-05 16:18:23,895: t15.2023.10.08 val PER: 0.2720
2026-01-05 16:18:23,895: t15.2023.10.13 val PER: 0.2343
2026-01-05 16:18:23,896: t15.2023.10.15 val PER: 0.1753
2026-01-05 16:18:23,896: t15.2023.10.20 val PER: 0.2013
2026-01-05 16:18:23,896: t15.2023.10.22 val PER: 0.1303
2026-01-05 16:18:23,896: t15.2023.11.03 val PER: 0.2035
2026-01-05 16:18:23,896: t15.2023.11.04 val PER: 0.0341
2026-01-05 16:18:23,896: t15.2023.11.17 val PER: 0.0560
2026-01-05 16:18:23,896: t15.2023.11.19 val PER: 0.0559
2026-01-05 16:18:23,896: t15.2023.11.26 val PER: 0.1681
2026-01-05 16:18:23,896: t15.2023.12.03 val PER: 0.1418
2026-01-05 16:18:23,896: t15.2023.12.08 val PER: 0.1272
2026-01-05 16:18:23,896: t15.2023.12.10 val PER: 0.1156
2026-01-05 16:18:23,896: t15.2023.12.17 val PER: 0.1435
2026-01-05 16:18:23,896: t15.2023.12.29 val PER: 0.1572
2026-01-05 16:18:23,896: t15.2024.02.25 val PER: 0.1419
2026-01-05 16:18:23,896: t15.2024.03.08 val PER: 0.2632
2026-01-05 16:18:23,897: t15.2024.03.15 val PER: 0.2289
2026-01-05 16:18:23,897: t15.2024.03.17 val PER: 0.1681
2026-01-05 16:18:23,897: t15.2024.05.10 val PER: 0.1842
2026-01-05 16:18:23,897: t15.2024.06.14 val PER: 0.1782
2026-01-05 16:18:23,897: t15.2024.07.19 val PER: 0.2577
2026-01-05 16:18:23,897: t15.2024.07.21 val PER: 0.1138
2026-01-05 16:18:23,897: t15.2024.07.28 val PER: 0.1559
2026-01-05 16:18:23,897: t15.2025.01.10 val PER: 0.3113
2026-01-05 16:18:23,897: t15.2025.01.12 val PER: 0.1801
2026-01-05 16:18:23,898: t15.2025.03.14 val PER: 0.3624
2026-01-05 16:18:23,898: t15.2025.03.16 val PER: 0.2094
2026-01-05 16:18:23,898: t15.2025.03.30 val PER: 0.3264
2026-01-05 16:18:23,898: t15.2025.04.13 val PER: 0.2368
2026-01-05 16:18:24,189: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_9000
2026-01-05 16:18:42,882: Train batch 9200: loss: 11.01 grad norm: 50.03 time: 0.056
2026-01-05 16:19:01,816: Train batch 9400: loss: 7.54 grad norm: 47.03 time: 0.068
2026-01-05 16:19:11,448: Running test after training batch: 9500
2026-01-05 16:19:11,560: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:19:16,751: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 16:19:16,788: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-05 16:19:18,753: Val batch 9500: PER (avg): 0.1747 CTC Loss (avg): 17.2906 WER(1gram): 50.76% (n=64) time: 7.304
2026-01-05 16:19:18,753: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-05 16:19:18,754: t15.2023.08.13 val PER: 0.1299
2026-01-05 16:19:18,754: t15.2023.08.18 val PER: 0.1232
2026-01-05 16:19:18,754: t15.2023.08.20 val PER: 0.1350
2026-01-05 16:19:18,754: t15.2023.08.25 val PER: 0.1054
2026-01-05 16:19:18,754: t15.2023.08.27 val PER: 0.1994
2026-01-05 16:19:18,754: t15.2023.09.01 val PER: 0.0966
2026-01-05 16:19:18,754: t15.2023.09.03 val PER: 0.1865
2026-01-05 16:19:18,754: t15.2023.09.24 val PER: 0.1505
2026-01-05 16:19:18,755: t15.2023.09.29 val PER: 0.1442
2026-01-05 16:19:18,755: t15.2023.10.01 val PER: 0.1955
2026-01-05 16:19:18,755: t15.2023.10.06 val PER: 0.1012
2026-01-05 16:19:18,755: t15.2023.10.08 val PER: 0.2558
2026-01-05 16:19:18,755: t15.2023.10.13 val PER: 0.2273
2026-01-05 16:19:18,755: t15.2023.10.15 val PER: 0.1833
2026-01-05 16:19:18,755: t15.2023.10.20 val PER: 0.1946
2026-01-05 16:19:18,755: t15.2023.10.22 val PER: 0.1281
2026-01-05 16:19:18,755: t15.2023.11.03 val PER: 0.1988
2026-01-05 16:19:18,756: t15.2023.11.04 val PER: 0.0341
2026-01-05 16:19:18,756: t15.2023.11.17 val PER: 0.0591
2026-01-05 16:19:18,756: t15.2023.11.19 val PER: 0.0539
2026-01-05 16:19:18,756: t15.2023.11.26 val PER: 0.1609
2026-01-05 16:19:18,756: t15.2023.12.03 val PER: 0.1418
2026-01-05 16:19:18,756: t15.2023.12.08 val PER: 0.1411
2026-01-05 16:19:18,756: t15.2023.12.10 val PER: 0.1143
2026-01-05 16:19:18,756: t15.2023.12.17 val PER: 0.1580
2026-01-05 16:19:18,756: t15.2023.12.29 val PER: 0.1496
2026-01-05 16:19:18,756: t15.2024.02.25 val PER: 0.1390
2026-01-05 16:19:18,756: t15.2024.03.08 val PER: 0.2504
2026-01-05 16:19:18,756: t15.2024.03.15 val PER: 0.2289
2026-01-05 16:19:18,756: t15.2024.03.17 val PER: 0.1681
2026-01-05 16:19:18,757: t15.2024.05.10 val PER: 0.1709
2026-01-05 16:19:18,757: t15.2024.06.14 val PER: 0.1735
2026-01-05 16:19:18,757: t15.2024.07.19 val PER: 0.2643
2026-01-05 16:19:18,757: t15.2024.07.21 val PER: 0.1193
2026-01-05 16:19:18,757: t15.2024.07.28 val PER: 0.1596
2026-01-05 16:19:18,757: t15.2025.01.10 val PER: 0.3306
2026-01-05 16:19:18,758: t15.2025.01.12 val PER: 0.1878
2026-01-05 16:19:18,758: t15.2025.03.14 val PER: 0.3817
2026-01-05 16:19:18,758: t15.2025.03.16 val PER: 0.2264
2026-01-05 16:19:18,758: t15.2025.03.30 val PER: 0.3138
2026-01-05 16:19:18,758: t15.2025.04.13 val PER: 0.2382
2026-01-05 16:19:19,033: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_9500
2026-01-05 16:19:28,107: Train batch 9600: loss: 8.55 grad norm: 47.99 time: 0.074
2026-01-05 16:19:47,316: Train batch 9800: loss: 12.00 grad norm: 57.60 time: 0.065
2026-01-05 16:20:06,718: Train batch 10000: loss: 5.65 grad norm: 36.22 time: 0.062
2026-01-05 16:20:06,718: Running test after training batch: 10000
2026-01-05 16:20:06,860: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:20:11,861: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 16:20:11,893: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sette
2026-01-05 16:20:13,646: Val batch 10000: PER (avg): 0.1685 CTC Loss (avg): 16.9136 WER(1gram): 51.27% (n=64) time: 6.928
2026-01-05 16:20:13,647: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-05 16:20:13,647: t15.2023.08.13 val PER: 0.1237
2026-01-05 16:20:13,647: t15.2023.08.18 val PER: 0.1274
2026-01-05 16:20:13,647: t15.2023.08.20 val PER: 0.1239
2026-01-05 16:20:13,647: t15.2023.08.25 val PER: 0.1130
2026-01-05 16:20:13,647: t15.2023.08.27 val PER: 0.1994
2026-01-05 16:20:13,648: t15.2023.09.01 val PER: 0.0860
2026-01-05 16:20:13,648: t15.2023.09.03 val PER: 0.1817
2026-01-05 16:20:13,648: t15.2023.09.24 val PER: 0.1432
2026-01-05 16:20:13,648: t15.2023.09.29 val PER: 0.1442
2026-01-05 16:20:13,648: t15.2023.10.01 val PER: 0.1863
2026-01-05 16:20:13,648: t15.2023.10.06 val PER: 0.1066
2026-01-05 16:20:13,648: t15.2023.10.08 val PER: 0.2585
2026-01-05 16:20:13,648: t15.2023.10.13 val PER: 0.2234
2026-01-05 16:20:13,648: t15.2023.10.15 val PER: 0.1753
2026-01-05 16:20:13,648: t15.2023.10.20 val PER: 0.2148
2026-01-05 16:20:13,648: t15.2023.10.22 val PER: 0.1258
2026-01-05 16:20:13,648: t15.2023.11.03 val PER: 0.1886
2026-01-05 16:20:13,648: t15.2023.11.04 val PER: 0.0444
2026-01-05 16:20:13,649: t15.2023.11.17 val PER: 0.0435
2026-01-05 16:20:13,649: t15.2023.11.19 val PER: 0.0399
2026-01-05 16:20:13,649: t15.2023.11.26 val PER: 0.1543
2026-01-05 16:20:13,649: t15.2023.12.03 val PER: 0.1324
2026-01-05 16:20:13,649: t15.2023.12.08 val PER: 0.1292
2026-01-05 16:20:13,649: t15.2023.12.10 val PER: 0.1222
2026-01-05 16:20:13,649: t15.2023.12.17 val PER: 0.1549
2026-01-05 16:20:13,649: t15.2023.12.29 val PER: 0.1414
2026-01-05 16:20:13,649: t15.2024.02.25 val PER: 0.1292
2026-01-05 16:20:13,649: t15.2024.03.08 val PER: 0.2418
2026-01-05 16:20:13,649: t15.2024.03.15 val PER: 0.2108
2026-01-05 16:20:13,649: t15.2024.03.17 val PER: 0.1618
2026-01-05 16:20:13,649: t15.2024.05.10 val PER: 0.1738
2026-01-05 16:20:13,650: t15.2024.06.14 val PER: 0.1845
2026-01-05 16:20:13,650: t15.2024.07.19 val PER: 0.2709
2026-01-05 16:20:13,650: t15.2024.07.21 val PER: 0.1076
2026-01-05 16:20:13,650: t15.2024.07.28 val PER: 0.1507
2026-01-05 16:20:13,650: t15.2025.01.10 val PER: 0.2934
2026-01-05 16:20:13,650: t15.2025.01.12 val PER: 0.1647
2026-01-05 16:20:13,650: t15.2025.03.14 val PER: 0.3595
2026-01-05 16:20:13,650: t15.2025.03.16 val PER: 0.2264
2026-01-05 16:20:13,650: t15.2025.03.30 val PER: 0.3241
2026-01-05 16:20:13,650: t15.2025.04.13 val PER: 0.2282
2026-01-05 16:20:13,938: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_10000
2026-01-05 16:20:33,192: Train batch 10200: loss: 6.50 grad norm: 39.09 time: 0.049
2026-01-05 16:20:52,631: Train batch 10400: loss: 9.04 grad norm: 49.60 time: 0.072
2026-01-05 16:21:02,288: Running test after training batch: 10500
2026-01-05 16:21:02,452: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:21:07,567: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 16:21:07,597: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-05 16:21:09,306: Val batch 10500: PER (avg): 0.1663 CTC Loss (avg): 16.7250 WER(1gram): 48.48% (n=64) time: 7.018
2026-01-05 16:21:09,307: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 16:21:09,307: t15.2023.08.13 val PER: 0.1237
2026-01-05 16:21:09,307: t15.2023.08.18 val PER: 0.1165
2026-01-05 16:21:09,307: t15.2023.08.20 val PER: 0.1223
2026-01-05 16:21:09,307: t15.2023.08.25 val PER: 0.1084
2026-01-05 16:21:09,307: t15.2023.08.27 val PER: 0.2026
2026-01-05 16:21:09,307: t15.2023.09.01 val PER: 0.0877
2026-01-05 16:21:09,307: t15.2023.09.03 val PER: 0.1746
2026-01-05 16:21:09,307: t15.2023.09.24 val PER: 0.1432
2026-01-05 16:21:09,307: t15.2023.09.29 val PER: 0.1474
2026-01-05 16:21:09,307: t15.2023.10.01 val PER: 0.1915
2026-01-05 16:21:09,307: t15.2023.10.06 val PER: 0.0893
2026-01-05 16:21:09,307: t15.2023.10.08 val PER: 0.2612
2026-01-05 16:21:09,307: t15.2023.10.13 val PER: 0.2188
2026-01-05 16:21:09,308: t15.2023.10.15 val PER: 0.1681
2026-01-05 16:21:09,308: t15.2023.10.20 val PER: 0.2013
2026-01-05 16:21:09,308: t15.2023.10.22 val PER: 0.1169
2026-01-05 16:21:09,308: t15.2023.11.03 val PER: 0.1906
2026-01-05 16:21:09,308: t15.2023.11.04 val PER: 0.0307
2026-01-05 16:21:09,308: t15.2023.11.17 val PER: 0.0529
2026-01-05 16:21:09,308: t15.2023.11.19 val PER: 0.0439
2026-01-05 16:21:09,308: t15.2023.11.26 val PER: 0.1391
2026-01-05 16:21:09,308: t15.2023.12.03 val PER: 0.1313
2026-01-05 16:21:09,308: t15.2023.12.08 val PER: 0.1212
2026-01-05 16:21:09,308: t15.2023.12.10 val PER: 0.0999
2026-01-05 16:21:09,308: t15.2023.12.17 val PER: 0.1518
2026-01-05 16:21:09,308: t15.2023.12.29 val PER: 0.1565
2026-01-05 16:21:09,309: t15.2024.02.25 val PER: 0.1278
2026-01-05 16:21:09,309: t15.2024.03.08 val PER: 0.2418
2026-01-05 16:21:09,309: t15.2024.03.15 val PER: 0.2170
2026-01-05 16:21:09,309: t15.2024.03.17 val PER: 0.1618
2026-01-05 16:21:09,309: t15.2024.05.10 val PER: 0.1724
2026-01-05 16:21:09,309: t15.2024.06.14 val PER: 0.1798
2026-01-05 16:21:09,309: t15.2024.07.19 val PER: 0.2683
2026-01-05 16:21:09,309: t15.2024.07.21 val PER: 0.1041
2026-01-05 16:21:09,309: t15.2024.07.28 val PER: 0.1441
2026-01-05 16:21:09,309: t15.2025.01.10 val PER: 0.3113
2026-01-05 16:21:09,309: t15.2025.01.12 val PER: 0.1724
2026-01-05 16:21:09,310: t15.2025.03.14 val PER: 0.3565
2026-01-05 16:21:09,310: t15.2025.03.16 val PER: 0.1990
2026-01-05 16:21:09,310: t15.2025.03.30 val PER: 0.3149
2026-01-05 16:21:09,310: t15.2025.04.13 val PER: 0.2340
2026-01-05 16:21:09,311: New best val WER(1gram) 48.98% --> 48.48%
2026-01-05 16:21:09,311: Checkpointing model
2026-01-05 16:21:10,947: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:21:11,265: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_10500
2026-01-05 16:21:20,736: Train batch 10600: loss: 9.59 grad norm: 54.55 time: 0.075
2026-01-05 16:21:39,025: Train batch 10800: loss: 14.43 grad norm: 61.39 time: 0.065
2026-01-05 16:21:57,509: Train batch 11000: loss: 14.65 grad norm: 66.15 time: 0.057
2026-01-05 16:21:57,510: Running test after training batch: 11000
2026-01-05 16:21:57,663: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:22:02,546: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 16:22:02,577: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-05 16:22:04,319: Val batch 11000: PER (avg): 0.1644 CTC Loss (avg): 16.4246 WER(1gram): 49.75% (n=64) time: 6.809
2026-01-05 16:22:04,319: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-05 16:22:04,319: t15.2023.08.13 val PER: 0.1268
2026-01-05 16:22:04,319: t15.2023.08.18 val PER: 0.1165
2026-01-05 16:22:04,319: t15.2023.08.20 val PER: 0.1231
2026-01-05 16:22:04,320: t15.2023.08.25 val PER: 0.1024
2026-01-05 16:22:04,320: t15.2023.08.27 val PER: 0.1977
2026-01-05 16:22:04,320: t15.2023.09.01 val PER: 0.0860
2026-01-05 16:22:04,320: t15.2023.09.03 val PER: 0.1841
2026-01-05 16:22:04,320: t15.2023.09.24 val PER: 0.1432
2026-01-05 16:22:04,320: t15.2023.09.29 val PER: 0.1398
2026-01-05 16:22:04,320: t15.2023.10.01 val PER: 0.1962
2026-01-05 16:22:04,320: t15.2023.10.06 val PER: 0.0893
2026-01-05 16:22:04,320: t15.2023.10.08 val PER: 0.2530
2026-01-05 16:22:04,320: t15.2023.10.13 val PER: 0.2196
2026-01-05 16:22:04,320: t15.2023.10.15 val PER: 0.1681
2026-01-05 16:22:04,321: t15.2023.10.20 val PER: 0.2148
2026-01-05 16:22:04,321: t15.2023.10.22 val PER: 0.1236
2026-01-05 16:22:04,321: t15.2023.11.03 val PER: 0.1879
2026-01-05 16:22:04,321: t15.2023.11.04 val PER: 0.0239
2026-01-05 16:22:04,321: t15.2023.11.17 val PER: 0.0544
2026-01-05 16:22:04,321: t15.2023.11.19 val PER: 0.0539
2026-01-05 16:22:04,321: t15.2023.11.26 val PER: 0.1442
2026-01-05 16:22:04,321: t15.2023.12.03 val PER: 0.1239
2026-01-05 16:22:04,321: t15.2023.12.08 val PER: 0.1178
2026-01-05 16:22:04,321: t15.2023.12.10 val PER: 0.1078
2026-01-05 16:22:04,321: t15.2023.12.17 val PER: 0.1455
2026-01-05 16:22:04,321: t15.2023.12.29 val PER: 0.1407
2026-01-05 16:22:04,321: t15.2024.02.25 val PER: 0.1320
2026-01-05 16:22:04,321: t15.2024.03.08 val PER: 0.2390
2026-01-05 16:22:04,321: t15.2024.03.15 val PER: 0.2151
2026-01-05 16:22:04,321: t15.2024.03.17 val PER: 0.1576
2026-01-05 16:22:04,322: t15.2024.05.10 val PER: 0.1768
2026-01-05 16:22:04,322: t15.2024.06.14 val PER: 0.1672
2026-01-05 16:22:04,322: t15.2024.07.19 val PER: 0.2584
2026-01-05 16:22:04,322: t15.2024.07.21 val PER: 0.1090
2026-01-05 16:22:04,322: t15.2024.07.28 val PER: 0.1485
2026-01-05 16:22:04,322: t15.2025.01.10 val PER: 0.3017
2026-01-05 16:22:04,322: t15.2025.01.12 val PER: 0.1624
2026-01-05 16:22:04,322: t15.2025.03.14 val PER: 0.3521
2026-01-05 16:22:04,322: t15.2025.03.16 val PER: 0.2016
2026-01-05 16:22:04,322: t15.2025.03.30 val PER: 0.3069
2026-01-05 16:22:04,322: t15.2025.04.13 val PER: 0.2197
2026-01-05 16:22:04,587: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_11000
2026-01-05 16:22:23,777: Train batch 11200: loss: 10.23 grad norm: 49.63 time: 0.071
2026-01-05 16:22:42,760: Train batch 11400: loss: 9.67 grad norm: 53.10 time: 0.056
2026-01-05 16:22:52,772: Running test after training batch: 11500
2026-01-05 16:22:52,887: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:22:57,816: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 16:22:57,848: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-05 16:22:59,586: Val batch 11500: PER (avg): 0.1613 CTC Loss (avg): 16.3640 WER(1gram): 48.73% (n=64) time: 6.813
2026-01-05 16:22:59,586: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 16:22:59,586: t15.2023.08.13 val PER: 0.1258
2026-01-05 16:22:59,587: t15.2023.08.18 val PER: 0.1115
2026-01-05 16:22:59,587: t15.2023.08.20 val PER: 0.1223
2026-01-05 16:22:59,587: t15.2023.08.25 val PER: 0.1039
2026-01-05 16:22:59,587: t15.2023.08.27 val PER: 0.1945
2026-01-05 16:22:59,587: t15.2023.09.01 val PER: 0.0820
2026-01-05 16:22:59,587: t15.2023.09.03 val PER: 0.1758
2026-01-05 16:22:59,587: t15.2023.09.24 val PER: 0.1347
2026-01-05 16:22:59,587: t15.2023.09.29 val PER: 0.1334
2026-01-05 16:22:59,587: t15.2023.10.01 val PER: 0.1856
2026-01-05 16:22:59,587: t15.2023.10.06 val PER: 0.0861
2026-01-05 16:22:59,587: t15.2023.10.08 val PER: 0.2585
2026-01-05 16:22:59,587: t15.2023.10.13 val PER: 0.2172
2026-01-05 16:22:59,587: t15.2023.10.15 val PER: 0.1635
2026-01-05 16:22:59,588: t15.2023.10.20 val PER: 0.1846
2026-01-05 16:22:59,588: t15.2023.10.22 val PER: 0.1147
2026-01-05 16:22:59,588: t15.2023.11.03 val PER: 0.1839
2026-01-05 16:22:59,588: t15.2023.11.04 val PER: 0.0239
2026-01-05 16:22:59,588: t15.2023.11.17 val PER: 0.0544
2026-01-05 16:22:59,588: t15.2023.11.19 val PER: 0.0479
2026-01-05 16:22:59,588: t15.2023.11.26 val PER: 0.1326
2026-01-05 16:22:59,588: t15.2023.12.03 val PER: 0.1250
2026-01-05 16:22:59,588: t15.2023.12.08 val PER: 0.1119
2026-01-05 16:22:59,588: t15.2023.12.10 val PER: 0.0999
2026-01-05 16:22:59,588: t15.2023.12.17 val PER: 0.1476
2026-01-05 16:22:59,589: t15.2023.12.29 val PER: 0.1414
2026-01-05 16:22:59,589: t15.2024.02.25 val PER: 0.1194
2026-01-05 16:22:59,589: t15.2024.03.08 val PER: 0.2347
2026-01-05 16:22:59,589: t15.2024.03.15 val PER: 0.2133
2026-01-05 16:22:59,589: t15.2024.03.17 val PER: 0.1499
2026-01-05 16:22:59,589: t15.2024.05.10 val PER: 0.1709
2026-01-05 16:22:59,589: t15.2024.06.14 val PER: 0.1656
2026-01-05 16:22:59,589: t15.2024.07.19 val PER: 0.2525
2026-01-05 16:22:59,589: t15.2024.07.21 val PER: 0.1041
2026-01-05 16:22:59,589: t15.2024.07.28 val PER: 0.1500
2026-01-05 16:22:59,589: t15.2025.01.10 val PER: 0.3099
2026-01-05 16:22:59,589: t15.2025.01.12 val PER: 0.1632
2026-01-05 16:22:59,589: t15.2025.03.14 val PER: 0.3595
2026-01-05 16:22:59,589: t15.2025.03.16 val PER: 0.2016
2026-01-05 16:22:59,590: t15.2025.03.30 val PER: 0.3149
2026-01-05 16:22:59,590: t15.2025.04.13 val PER: 0.2382
2026-01-05 16:22:59,856: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_11500
2026-01-05 16:23:08,685: Train batch 11600: loss: 10.70 grad norm: 46.75 time: 0.062
2026-01-05 16:23:27,100: Train batch 11800: loss: 6.36 grad norm: 41.05 time: 0.046
2026-01-05 16:23:45,636: Train batch 12000: loss: 13.37 grad norm: 54.46 time: 0.072
2026-01-05 16:23:45,637: Running test after training batch: 12000
2026-01-05 16:23:45,752: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:23:51,085: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 16:23:51,119: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-05 16:23:53,058: Val batch 12000: PER (avg): 0.1601 CTC Loss (avg): 16.3131 WER(1gram): 49.75% (n=64) time: 7.421
2026-01-05 16:23:53,058: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 16:23:53,059: t15.2023.08.13 val PER: 0.1185
2026-01-05 16:23:53,059: t15.2023.08.18 val PER: 0.1165
2026-01-05 16:23:53,059: t15.2023.08.20 val PER: 0.1160
2026-01-05 16:23:53,059: t15.2023.08.25 val PER: 0.1039
2026-01-05 16:23:53,060: t15.2023.08.27 val PER: 0.1945
2026-01-05 16:23:53,060: t15.2023.09.01 val PER: 0.0804
2026-01-05 16:23:53,060: t15.2023.09.03 val PER: 0.1603
2026-01-05 16:23:53,060: t15.2023.09.24 val PER: 0.1359
2026-01-05 16:23:53,060: t15.2023.09.29 val PER: 0.1353
2026-01-05 16:23:53,060: t15.2023.10.01 val PER: 0.1816
2026-01-05 16:23:53,060: t15.2023.10.06 val PER: 0.0850
2026-01-05 16:23:53,060: t15.2023.10.08 val PER: 0.2571
2026-01-05 16:23:53,060: t15.2023.10.13 val PER: 0.2141
2026-01-05 16:23:53,060: t15.2023.10.15 val PER: 0.1589
2026-01-05 16:23:53,060: t15.2023.10.20 val PER: 0.2013
2026-01-05 16:23:53,060: t15.2023.10.22 val PER: 0.1180
2026-01-05 16:23:53,061: t15.2023.11.03 val PER: 0.1866
2026-01-05 16:23:53,061: t15.2023.11.04 val PER: 0.0410
2026-01-05 16:23:53,061: t15.2023.11.17 val PER: 0.0327
2026-01-05 16:23:53,061: t15.2023.11.19 val PER: 0.0359
2026-01-05 16:23:53,061: t15.2023.11.26 val PER: 0.1297
2026-01-05 16:23:53,064: t15.2023.12.03 val PER: 0.1292
2026-01-05 16:23:53,064: t15.2023.12.08 val PER: 0.1092
2026-01-05 16:23:53,065: t15.2023.12.10 val PER: 0.0959
2026-01-05 16:23:53,065: t15.2023.12.17 val PER: 0.1507
2026-01-05 16:23:53,065: t15.2023.12.29 val PER: 0.1400
2026-01-05 16:23:53,065: t15.2024.02.25 val PER: 0.1264
2026-01-05 16:23:53,065: t15.2024.03.08 val PER: 0.2447
2026-01-05 16:23:53,065: t15.2024.03.15 val PER: 0.2189
2026-01-05 16:23:53,065: t15.2024.03.17 val PER: 0.1492
2026-01-05 16:23:53,065: t15.2024.05.10 val PER: 0.1768
2026-01-05 16:23:53,065: t15.2024.06.14 val PER: 0.1877
2026-01-05 16:23:53,065: t15.2024.07.19 val PER: 0.2525
2026-01-05 16:23:53,065: t15.2024.07.21 val PER: 0.1034
2026-01-05 16:23:53,065: t15.2024.07.28 val PER: 0.1441
2026-01-05 16:23:53,065: t15.2025.01.10 val PER: 0.3030
2026-01-05 16:23:53,065: t15.2025.01.12 val PER: 0.1563
2026-01-05 16:23:53,065: t15.2025.03.14 val PER: 0.3550
2026-01-05 16:23:53,066: t15.2025.03.16 val PER: 0.2094
2026-01-05 16:23:53,066: t15.2025.03.30 val PER: 0.3057
2026-01-05 16:23:53,066: t15.2025.04.13 val PER: 0.2225
2026-01-05 16:23:53,352: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_12000
2026-01-05 16:24:13,306: Train batch 12200: loss: 5.93 grad norm: 42.42 time: 0.067
2026-01-05 16:24:33,081: Train batch 12400: loss: 5.14 grad norm: 36.62 time: 0.041
2026-01-05 16:24:43,404: Running test after training batch: 12500
2026-01-05 16:24:43,528: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:24:48,622: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 16:24:48,658: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 16:24:50,593: Val batch 12500: PER (avg): 0.1564 CTC Loss (avg): 16.0414 WER(1gram): 47.21% (n=64) time: 7.188
2026-01-05 16:24:50,593: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 16:24:50,593: t15.2023.08.13 val PER: 0.1112
2026-01-05 16:24:50,594: t15.2023.08.18 val PER: 0.1140
2026-01-05 16:24:50,594: t15.2023.08.20 val PER: 0.1088
2026-01-05 16:24:50,594: t15.2023.08.25 val PER: 0.0979
2026-01-05 16:24:50,594: t15.2023.08.27 val PER: 0.1977
2026-01-05 16:24:50,594: t15.2023.09.01 val PER: 0.0763
2026-01-05 16:24:50,594: t15.2023.09.03 val PER: 0.1663
2026-01-05 16:24:50,594: t15.2023.09.24 val PER: 0.1323
2026-01-05 16:24:50,594: t15.2023.09.29 val PER: 0.1327
2026-01-05 16:24:50,594: t15.2023.10.01 val PER: 0.1764
2026-01-05 16:24:50,594: t15.2023.10.06 val PER: 0.0883
2026-01-05 16:24:50,594: t15.2023.10.08 val PER: 0.2639
2026-01-05 16:24:50,595: t15.2023.10.13 val PER: 0.2133
2026-01-05 16:24:50,595: t15.2023.10.15 val PER: 0.1536
2026-01-05 16:24:50,595: t15.2023.10.20 val PER: 0.2013
2026-01-05 16:24:50,595: t15.2023.10.22 val PER: 0.1147
2026-01-05 16:24:50,595: t15.2023.11.03 val PER: 0.1798
2026-01-05 16:24:50,595: t15.2023.11.04 val PER: 0.0341
2026-01-05 16:24:50,595: t15.2023.11.17 val PER: 0.0435
2026-01-05 16:24:50,595: t15.2023.11.19 val PER: 0.0439
2026-01-05 16:24:50,595: t15.2023.11.26 val PER: 0.1232
2026-01-05 16:24:50,595: t15.2023.12.03 val PER: 0.1250
2026-01-05 16:24:50,595: t15.2023.12.08 val PER: 0.1065
2026-01-05 16:24:50,596: t15.2023.12.10 val PER: 0.1012
2026-01-05 16:24:50,596: t15.2023.12.17 val PER: 0.1518
2026-01-05 16:24:50,596: t15.2023.12.29 val PER: 0.1414
2026-01-05 16:24:50,596: t15.2024.02.25 val PER: 0.1110
2026-01-05 16:24:50,596: t15.2024.03.08 val PER: 0.2361
2026-01-05 16:24:50,596: t15.2024.03.15 val PER: 0.2126
2026-01-05 16:24:50,596: t15.2024.03.17 val PER: 0.1513
2026-01-05 16:24:50,596: t15.2024.05.10 val PER: 0.1590
2026-01-05 16:24:50,596: t15.2024.06.14 val PER: 0.1672
2026-01-05 16:24:50,596: t15.2024.07.19 val PER: 0.2426
2026-01-05 16:24:50,596: t15.2024.07.21 val PER: 0.0945
2026-01-05 16:24:50,596: t15.2024.07.28 val PER: 0.1309
2026-01-05 16:24:50,596: t15.2025.01.10 val PER: 0.3209
2026-01-05 16:24:50,596: t15.2025.01.12 val PER: 0.1547
2026-01-05 16:24:50,596: t15.2025.03.14 val PER: 0.3580
2026-01-05 16:24:50,597: t15.2025.03.16 val PER: 0.1963
2026-01-05 16:24:50,597: t15.2025.03.30 val PER: 0.2954
2026-01-05 16:24:50,597: t15.2025.04.13 val PER: 0.2211
2026-01-05 16:24:50,598: New best val WER(1gram) 48.48% --> 47.21%
2026-01-05 16:24:50,599: Checkpointing model
2026-01-05 16:24:52,244: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:24:52,545: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_12500
2026-01-05 16:25:02,589: Train batch 12600: loss: 8.01 grad norm: 47.32 time: 0.059
2026-01-05 16:25:21,745: Train batch 12800: loss: 5.74 grad norm: 37.79 time: 0.052
2026-01-05 16:25:41,286: Train batch 13000: loss: 6.46 grad norm: 42.34 time: 0.069
2026-01-05 16:25:41,287: Running test after training batch: 13000
2026-01-05 16:25:41,392: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:25:46,515: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 16:25:46,550: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost it
2026-01-05 16:25:48,519: Val batch 13000: PER (avg): 0.1551 CTC Loss (avg): 15.7538 WER(1gram): 44.92% (n=64) time: 7.232
2026-01-05 16:25:48,519: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 16:25:48,520: t15.2023.08.13 val PER: 0.1123
2026-01-05 16:25:48,520: t15.2023.08.18 val PER: 0.1140
2026-01-05 16:25:48,520: t15.2023.08.20 val PER: 0.1088
2026-01-05 16:25:48,520: t15.2023.08.25 val PER: 0.0919
2026-01-05 16:25:48,520: t15.2023.08.27 val PER: 0.1897
2026-01-05 16:25:48,520: t15.2023.09.01 val PER: 0.0706
2026-01-05 16:25:48,520: t15.2023.09.03 val PER: 0.1698
2026-01-05 16:25:48,520: t15.2023.09.24 val PER: 0.1371
2026-01-05 16:25:48,521: t15.2023.09.29 val PER: 0.1315
2026-01-05 16:25:48,521: t15.2023.10.01 val PER: 0.1757
2026-01-05 16:25:48,521: t15.2023.10.06 val PER: 0.0850
2026-01-05 16:25:48,521: t15.2023.10.08 val PER: 0.2598
2026-01-05 16:25:48,521: t15.2023.10.13 val PER: 0.2118
2026-01-05 16:25:48,521: t15.2023.10.15 val PER: 0.1608
2026-01-05 16:25:48,521: t15.2023.10.20 val PER: 0.1812
2026-01-05 16:25:48,521: t15.2023.10.22 val PER: 0.1136
2026-01-05 16:25:48,521: t15.2023.11.03 val PER: 0.1818
2026-01-05 16:25:48,522: t15.2023.11.04 val PER: 0.0375
2026-01-05 16:25:48,522: t15.2023.11.17 val PER: 0.0389
2026-01-05 16:25:48,522: t15.2023.11.19 val PER: 0.0459
2026-01-05 16:25:48,522: t15.2023.11.26 val PER: 0.1268
2026-01-05 16:25:48,522: t15.2023.12.03 val PER: 0.1218
2026-01-05 16:25:48,522: t15.2023.12.08 val PER: 0.1025
2026-01-05 16:25:48,522: t15.2023.12.10 val PER: 0.1012
2026-01-05 16:25:48,522: t15.2023.12.17 val PER: 0.1466
2026-01-05 16:25:48,522: t15.2023.12.29 val PER: 0.1373
2026-01-05 16:25:48,522: t15.2024.02.25 val PER: 0.1053
2026-01-05 16:25:48,522: t15.2024.03.08 val PER: 0.2404
2026-01-05 16:25:48,523: t15.2024.03.15 val PER: 0.2120
2026-01-05 16:25:48,523: t15.2024.03.17 val PER: 0.1471
2026-01-05 16:25:48,523: t15.2024.05.10 val PER: 0.1590
2026-01-05 16:25:48,523: t15.2024.06.14 val PER: 0.1625
2026-01-05 16:25:48,523: t15.2024.07.19 val PER: 0.2465
2026-01-05 16:25:48,523: t15.2024.07.21 val PER: 0.1007
2026-01-05 16:25:48,523: t15.2024.07.28 val PER: 0.1375
2026-01-05 16:25:48,523: t15.2025.01.10 val PER: 0.2961
2026-01-05 16:25:48,523: t15.2025.01.12 val PER: 0.1455
2026-01-05 16:25:48,523: t15.2025.03.14 val PER: 0.3521
2026-01-05 16:25:48,523: t15.2025.03.16 val PER: 0.1780
2026-01-05 16:25:48,524: t15.2025.03.30 val PER: 0.3069
2026-01-05 16:25:48,524: t15.2025.04.13 val PER: 0.2211
2026-01-05 16:25:48,524: New best val WER(1gram) 47.21% --> 44.92%
2026-01-05 16:25:48,524: Checkpointing model
2026-01-05 16:25:50,122: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:25:50,478: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_13000
2026-01-05 16:26:08,786: Train batch 13200: loss: 11.98 grad norm: 57.72 time: 0.054
2026-01-05 16:26:26,836: Train batch 13400: loss: 8.78 grad norm: 50.94 time: 0.063
2026-01-05 16:26:35,808: Running test after training batch: 13500
2026-01-05 16:26:35,913: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:26:41,216: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 16:26:41,251: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost get
2026-01-05 16:26:43,243: Val batch 13500: PER (avg): 0.1531 CTC Loss (avg): 15.6515 WER(1gram): 47.21% (n=64) time: 7.435
2026-01-05 16:26:43,244: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 16:26:43,244: t15.2023.08.13 val PER: 0.1206
2026-01-05 16:26:43,244: t15.2023.08.18 val PER: 0.1098
2026-01-05 16:26:43,244: t15.2023.08.20 val PER: 0.1104
2026-01-05 16:26:43,244: t15.2023.08.25 val PER: 0.1009
2026-01-05 16:26:43,244: t15.2023.08.27 val PER: 0.1961
2026-01-05 16:26:43,244: t15.2023.09.01 val PER: 0.0795
2026-01-05 16:26:43,245: t15.2023.09.03 val PER: 0.1710
2026-01-05 16:26:43,245: t15.2023.09.24 val PER: 0.1250
2026-01-05 16:26:43,245: t15.2023.09.29 val PER: 0.1302
2026-01-05 16:26:43,245: t15.2023.10.01 val PER: 0.1783
2026-01-05 16:26:43,245: t15.2023.10.06 val PER: 0.0829
2026-01-05 16:26:43,245: t15.2023.10.08 val PER: 0.2530
2026-01-05 16:26:43,245: t15.2023.10.13 val PER: 0.2102
2026-01-05 16:26:43,245: t15.2023.10.15 val PER: 0.1536
2026-01-05 16:26:43,245: t15.2023.10.20 val PER: 0.1812
2026-01-05 16:26:43,245: t15.2023.10.22 val PER: 0.1114
2026-01-05 16:26:43,245: t15.2023.11.03 val PER: 0.1832
2026-01-05 16:26:43,245: t15.2023.11.04 val PER: 0.0341
2026-01-05 16:26:43,246: t15.2023.11.17 val PER: 0.0420
2026-01-05 16:26:43,246: t15.2023.11.19 val PER: 0.0299
2026-01-05 16:26:43,246: t15.2023.11.26 val PER: 0.1225
2026-01-05 16:26:43,246: t15.2023.12.03 val PER: 0.1124
2026-01-05 16:26:43,246: t15.2023.12.08 val PER: 0.1059
2026-01-05 16:26:43,246: t15.2023.12.10 val PER: 0.0972
2026-01-05 16:26:43,246: t15.2023.12.17 val PER: 0.1331
2026-01-05 16:26:43,246: t15.2023.12.29 val PER: 0.1359
2026-01-05 16:26:43,246: t15.2024.02.25 val PER: 0.1124
2026-01-05 16:26:43,246: t15.2024.03.08 val PER: 0.2219
2026-01-05 16:26:43,246: t15.2024.03.15 val PER: 0.2020
2026-01-05 16:26:43,247: t15.2024.03.17 val PER: 0.1548
2026-01-05 16:26:43,247: t15.2024.05.10 val PER: 0.1545
2026-01-05 16:26:43,247: t15.2024.06.14 val PER: 0.1498
2026-01-05 16:26:43,247: t15.2024.07.19 val PER: 0.2399
2026-01-05 16:26:43,247: t15.2024.07.21 val PER: 0.0945
2026-01-05 16:26:43,247: t15.2024.07.28 val PER: 0.1426
2026-01-05 16:26:43,247: t15.2025.01.10 val PER: 0.2934
2026-01-05 16:26:43,247: t15.2025.01.12 val PER: 0.1401
2026-01-05 16:26:43,247: t15.2025.03.14 val PER: 0.3536
2026-01-05 16:26:43,247: t15.2025.03.16 val PER: 0.1911
2026-01-05 16:26:43,247: t15.2025.03.30 val PER: 0.3011
2026-01-05 16:26:43,247: t15.2025.04.13 val PER: 0.2126
2026-01-05 16:26:43,532: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_13500
2026-01-05 16:26:53,983: Train batch 13600: loss: 12.20 grad norm: 62.14 time: 0.064
2026-01-05 16:27:13,221: Train batch 13800: loss: 9.30 grad norm: 56.34 time: 0.057
2026-01-05 16:27:32,317: Train batch 14000: loss: 11.71 grad norm: 58.29 time: 0.051
2026-01-05 16:27:32,318: Running test after training batch: 14000
2026-01-05 16:27:32,416: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:27:37,418: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 16:27:37,449: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-05 16:27:39,252: Val batch 14000: PER (avg): 0.1533 CTC Loss (avg): 15.6885 WER(1gram): 46.45% (n=64) time: 6.934
2026-01-05 16:27:39,252: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 16:27:39,253: t15.2023.08.13 val PER: 0.1091
2026-01-05 16:27:39,253: t15.2023.08.18 val PER: 0.1073
2026-01-05 16:27:39,253: t15.2023.08.20 val PER: 0.1104
2026-01-05 16:27:39,253: t15.2023.08.25 val PER: 0.0994
2026-01-05 16:27:39,253: t15.2023.08.27 val PER: 0.1961
2026-01-05 16:27:39,253: t15.2023.09.01 val PER: 0.0739
2026-01-05 16:27:39,253: t15.2023.09.03 val PER: 0.1698
2026-01-05 16:27:39,253: t15.2023.09.24 val PER: 0.1286
2026-01-05 16:27:39,253: t15.2023.09.29 val PER: 0.1398
2026-01-05 16:27:39,253: t15.2023.10.01 val PER: 0.1790
2026-01-05 16:27:39,253: t15.2023.10.06 val PER: 0.0807
2026-01-05 16:27:39,253: t15.2023.10.08 val PER: 0.2558
2026-01-05 16:27:39,253: t15.2023.10.13 val PER: 0.2025
2026-01-05 16:27:39,254: t15.2023.10.15 val PER: 0.1529
2026-01-05 16:27:39,254: t15.2023.10.20 val PER: 0.2013
2026-01-05 16:27:39,254: t15.2023.10.22 val PER: 0.1114
2026-01-05 16:27:39,254: t15.2023.11.03 val PER: 0.1798
2026-01-05 16:27:39,254: t15.2023.11.04 val PER: 0.0341
2026-01-05 16:27:39,254: t15.2023.11.17 val PER: 0.0404
2026-01-05 16:27:39,254: t15.2023.11.19 val PER: 0.0399
2026-01-05 16:27:39,254: t15.2023.11.26 val PER: 0.1290
2026-01-05 16:27:39,254: t15.2023.12.03 val PER: 0.1208
2026-01-05 16:27:39,254: t15.2023.12.08 val PER: 0.0992
2026-01-05 16:27:39,254: t15.2023.12.10 val PER: 0.0999
2026-01-05 16:27:39,254: t15.2023.12.17 val PER: 0.1445
2026-01-05 16:27:39,254: t15.2023.12.29 val PER: 0.1325
2026-01-05 16:27:39,255: t15.2024.02.25 val PER: 0.1166
2026-01-05 16:27:39,255: t15.2024.03.08 val PER: 0.2390
2026-01-05 16:27:39,255: t15.2024.03.15 val PER: 0.2045
2026-01-05 16:27:39,255: t15.2024.03.17 val PER: 0.1450
2026-01-05 16:27:39,255: t15.2024.05.10 val PER: 0.1560
2026-01-05 16:27:39,255: t15.2024.06.14 val PER: 0.1640
2026-01-05 16:27:39,255: t15.2024.07.19 val PER: 0.2360
2026-01-05 16:27:39,255: t15.2024.07.21 val PER: 0.0917
2026-01-05 16:27:39,255: t15.2024.07.28 val PER: 0.1390
2026-01-05 16:27:39,255: t15.2025.01.10 val PER: 0.2975
2026-01-05 16:27:39,255: t15.2025.01.12 val PER: 0.1409
2026-01-05 16:27:39,255: t15.2025.03.14 val PER: 0.3595
2026-01-05 16:27:39,255: t15.2025.03.16 val PER: 0.1898
2026-01-05 16:27:39,255: t15.2025.03.30 val PER: 0.2885
2026-01-05 16:27:39,255: t15.2025.04.13 val PER: 0.2183
2026-01-05 16:27:39,531: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_14000
2026-01-05 16:27:57,720: Train batch 14200: loss: 7.92 grad norm: 49.44 time: 0.056
2026-01-05 16:28:16,568: Train batch 14400: loss: 5.81 grad norm: 40.03 time: 0.064
2026-01-05 16:28:25,929: Running test after training batch: 14500
2026-01-05 16:28:26,047: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:28:31,047: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 16:28:31,080: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-05 16:28:32,925: Val batch 14500: PER (avg): 0.1522 CTC Loss (avg): 15.6761 WER(1gram): 47.21% (n=64) time: 6.995
2026-01-05 16:28:32,925: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-05 16:28:32,925: t15.2023.08.13 val PER: 0.1091
2026-01-05 16:28:32,926: t15.2023.08.18 val PER: 0.1073
2026-01-05 16:28:32,926: t15.2023.08.20 val PER: 0.1056
2026-01-05 16:28:32,926: t15.2023.08.25 val PER: 0.0934
2026-01-05 16:28:32,926: t15.2023.08.27 val PER: 0.1865
2026-01-05 16:28:32,926: t15.2023.09.01 val PER: 0.0739
2026-01-05 16:28:32,926: t15.2023.09.03 val PER: 0.1663
2026-01-05 16:28:32,926: t15.2023.09.24 val PER: 0.1274
2026-01-05 16:28:32,926: t15.2023.09.29 val PER: 0.1295
2026-01-05 16:28:32,926: t15.2023.10.01 val PER: 0.1770
2026-01-05 16:28:32,926: t15.2023.10.06 val PER: 0.0818
2026-01-05 16:28:32,926: t15.2023.10.08 val PER: 0.2558
2026-01-05 16:28:32,927: t15.2023.10.13 val PER: 0.2017
2026-01-05 16:28:32,927: t15.2023.10.15 val PER: 0.1608
2026-01-05 16:28:32,927: t15.2023.10.20 val PER: 0.1879
2026-01-05 16:28:32,927: t15.2023.10.22 val PER: 0.1091
2026-01-05 16:28:32,927: t15.2023.11.03 val PER: 0.1839
2026-01-05 16:28:32,927: t15.2023.11.04 val PER: 0.0375
2026-01-05 16:28:32,927: t15.2023.11.17 val PER: 0.0358
2026-01-05 16:28:32,927: t15.2023.11.19 val PER: 0.0339
2026-01-05 16:28:32,927: t15.2023.11.26 val PER: 0.1239
2026-01-05 16:28:32,927: t15.2023.12.03 val PER: 0.1134
2026-01-05 16:28:32,927: t15.2023.12.08 val PER: 0.1005
2026-01-05 16:28:32,927: t15.2023.12.10 val PER: 0.0972
2026-01-05 16:28:32,927: t15.2023.12.17 val PER: 0.1528
2026-01-05 16:28:32,927: t15.2023.12.29 val PER: 0.1311
2026-01-05 16:28:32,928: t15.2024.02.25 val PER: 0.1152
2026-01-05 16:28:32,928: t15.2024.03.08 val PER: 0.2319
2026-01-05 16:28:32,928: t15.2024.03.15 val PER: 0.2064
2026-01-05 16:28:32,928: t15.2024.03.17 val PER: 0.1457
2026-01-05 16:28:32,928: t15.2024.05.10 val PER: 0.1471
2026-01-05 16:28:32,928: t15.2024.06.14 val PER: 0.1640
2026-01-05 16:28:32,928: t15.2024.07.19 val PER: 0.2360
2026-01-05 16:28:32,928: t15.2024.07.21 val PER: 0.0924
2026-01-05 16:28:32,928: t15.2024.07.28 val PER: 0.1382
2026-01-05 16:28:32,928: t15.2025.01.10 val PER: 0.2865
2026-01-05 16:28:32,928: t15.2025.01.12 val PER: 0.1447
2026-01-05 16:28:32,928: t15.2025.03.14 val PER: 0.3595
2026-01-05 16:28:32,928: t15.2025.03.16 val PER: 0.1898
2026-01-05 16:28:32,928: t15.2025.03.30 val PER: 0.2966
2026-01-05 16:28:32,928: t15.2025.04.13 val PER: 0.2154
2026-01-05 16:28:33,208: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_14500
2026-01-05 16:28:43,088: Train batch 14600: loss: 12.05 grad norm: 59.42 time: 0.059
2026-01-05 16:29:02,960: Train batch 14800: loss: 5.72 grad norm: 41.67 time: 0.052
2026-01-05 16:29:22,477: Train batch 15000: loss: 8.68 grad norm: 66.80 time: 0.054
2026-01-05 16:29:22,477: Running test after training batch: 15000
2026-01-05 16:29:22,590: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:29:27,512: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-05 16:29:27,545: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-05 16:29:29,364: Val batch 15000: PER (avg): 0.1508 CTC Loss (avg): 15.3984 WER(1gram): 46.70% (n=64) time: 6.887
2026-01-05 16:29:29,364: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 16:29:29,365: t15.2023.08.13 val PER: 0.1060
2026-01-05 16:29:29,365: t15.2023.08.18 val PER: 0.1065
2026-01-05 16:29:29,365: t15.2023.08.20 val PER: 0.1112
2026-01-05 16:29:29,365: t15.2023.08.25 val PER: 0.0994
2026-01-05 16:29:29,365: t15.2023.08.27 val PER: 0.1897
2026-01-05 16:29:29,365: t15.2023.09.01 val PER: 0.0722
2026-01-05 16:29:29,365: t15.2023.09.03 val PER: 0.1520
2026-01-05 16:29:29,366: t15.2023.09.24 val PER: 0.1250
2026-01-05 16:29:29,366: t15.2023.09.29 val PER: 0.1315
2026-01-05 16:29:29,366: t15.2023.10.01 val PER: 0.1744
2026-01-05 16:29:29,366: t15.2023.10.06 val PER: 0.0797
2026-01-05 16:29:29,366: t15.2023.10.08 val PER: 0.2585
2026-01-05 16:29:29,366: t15.2023.10.13 val PER: 0.2025
2026-01-05 16:29:29,366: t15.2023.10.15 val PER: 0.1516
2026-01-05 16:29:29,366: t15.2023.10.20 val PER: 0.2148
2026-01-05 16:29:29,366: t15.2023.10.22 val PER: 0.1080
2026-01-05 16:29:29,366: t15.2023.11.03 val PER: 0.1791
2026-01-05 16:29:29,366: t15.2023.11.04 val PER: 0.0410
2026-01-05 16:29:29,366: t15.2023.11.17 val PER: 0.0373
2026-01-05 16:29:29,366: t15.2023.11.19 val PER: 0.0359
2026-01-05 16:29:29,366: t15.2023.11.26 val PER: 0.1225
2026-01-05 16:29:29,367: t15.2023.12.03 val PER: 0.1092
2026-01-05 16:29:29,367: t15.2023.12.08 val PER: 0.1005
2026-01-05 16:29:29,367: t15.2023.12.10 val PER: 0.0959
2026-01-05 16:29:29,367: t15.2023.12.17 val PER: 0.1455
2026-01-05 16:29:29,367: t15.2023.12.29 val PER: 0.1318
2026-01-05 16:29:29,367: t15.2024.02.25 val PER: 0.1053
2026-01-05 16:29:29,367: t15.2024.03.08 val PER: 0.2191
2026-01-05 16:29:29,367: t15.2024.03.15 val PER: 0.2008
2026-01-05 16:29:29,367: t15.2024.03.17 val PER: 0.1381
2026-01-05 16:29:29,367: t15.2024.05.10 val PER: 0.1545
2026-01-05 16:29:29,367: t15.2024.06.14 val PER: 0.1625
2026-01-05 16:29:29,367: t15.2024.07.19 val PER: 0.2334
2026-01-05 16:29:29,367: t15.2024.07.21 val PER: 0.0924
2026-01-05 16:29:29,367: t15.2024.07.28 val PER: 0.1346
2026-01-05 16:29:29,367: t15.2025.01.10 val PER: 0.2989
2026-01-05 16:29:29,368: t15.2025.01.12 val PER: 0.1463
2026-01-05 16:29:29,368: t15.2025.03.14 val PER: 0.3550
2026-01-05 16:29:29,368: t15.2025.03.16 val PER: 0.1924
2026-01-05 16:29:29,368: t15.2025.03.30 val PER: 0.2966
2026-01-05 16:29:29,368: t15.2025.04.13 val PER: 0.2211
2026-01-05 16:29:29,636: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_15000
2026-01-05 16:29:48,806: Train batch 15200: loss: 4.69 grad norm: 39.47 time: 0.058
2026-01-05 16:30:08,119: Train batch 15400: loss: 10.96 grad norm: 54.39 time: 0.050
2026-01-05 16:30:17,814: Running test after training batch: 15500
2026-01-05 16:30:17,934: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:30:23,139: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-05 16:30:23,179: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost et
2026-01-05 16:30:25,177: Val batch 15500: PER (avg): 0.1490 CTC Loss (avg): 15.2779 WER(1gram): 46.95% (n=64) time: 7.363
2026-01-05 16:30:25,177: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 16:30:25,178: t15.2023.08.13 val PER: 0.1102
2026-01-05 16:30:25,178: t15.2023.08.18 val PER: 0.1048
2026-01-05 16:30:25,178: t15.2023.08.20 val PER: 0.1112
2026-01-05 16:30:25,178: t15.2023.08.25 val PER: 0.1069
2026-01-05 16:30:25,178: t15.2023.08.27 val PER: 0.1817
2026-01-05 16:30:25,178: t15.2023.09.01 val PER: 0.0690
2026-01-05 16:30:25,178: t15.2023.09.03 val PER: 0.1485
2026-01-05 16:30:25,179: t15.2023.09.24 val PER: 0.1262
2026-01-05 16:30:25,179: t15.2023.09.29 val PER: 0.1340
2026-01-05 16:30:25,179: t15.2023.10.01 val PER: 0.1717
2026-01-05 16:30:25,179: t15.2023.10.06 val PER: 0.0753
2026-01-05 16:30:25,179: t15.2023.10.08 val PER: 0.2503
2026-01-05 16:30:25,179: t15.2023.10.13 val PER: 0.1978
2026-01-05 16:30:25,179: t15.2023.10.15 val PER: 0.1483
2026-01-05 16:30:25,179: t15.2023.10.20 val PER: 0.1946
2026-01-05 16:30:25,179: t15.2023.10.22 val PER: 0.1125
2026-01-05 16:30:25,180: t15.2023.11.03 val PER: 0.1723
2026-01-05 16:30:25,180: t15.2023.11.04 val PER: 0.0273
2026-01-05 16:30:25,180: t15.2023.11.17 val PER: 0.0373
2026-01-05 16:30:25,180: t15.2023.11.19 val PER: 0.0399
2026-01-05 16:30:25,180: t15.2023.11.26 val PER: 0.1188
2026-01-05 16:30:25,180: t15.2023.12.03 val PER: 0.1103
2026-01-05 16:30:25,180: t15.2023.12.08 val PER: 0.1012
2026-01-05 16:30:25,180: t15.2023.12.10 val PER: 0.0867
2026-01-05 16:30:25,181: t15.2023.12.17 val PER: 0.1341
2026-01-05 16:30:25,181: t15.2023.12.29 val PER: 0.1345
2026-01-05 16:30:25,181: t15.2024.02.25 val PER: 0.1110
2026-01-05 16:30:25,181: t15.2024.03.08 val PER: 0.2276
2026-01-05 16:30:25,181: t15.2024.03.15 val PER: 0.2058
2026-01-05 16:30:25,181: t15.2024.03.17 val PER: 0.1423
2026-01-05 16:30:25,181: t15.2024.05.10 val PER: 0.1530
2026-01-05 16:30:25,181: t15.2024.06.14 val PER: 0.1577
2026-01-05 16:30:25,182: t15.2024.07.19 val PER: 0.2393
2026-01-05 16:30:25,182: t15.2024.07.21 val PER: 0.0924
2026-01-05 16:30:25,182: t15.2024.07.28 val PER: 0.1368
2026-01-05 16:30:25,182: t15.2025.01.10 val PER: 0.2948
2026-01-05 16:30:25,182: t15.2025.01.12 val PER: 0.1478
2026-01-05 16:30:25,182: t15.2025.03.14 val PER: 0.3358
2026-01-05 16:30:25,182: t15.2025.03.16 val PER: 0.1754
2026-01-05 16:30:25,182: t15.2025.03.30 val PER: 0.2874
2026-01-05 16:30:25,183: t15.2025.04.13 val PER: 0.2040
2026-01-05 16:30:25,462: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_15500
2026-01-05 16:30:34,797: Train batch 15600: loss: 11.31 grad norm: 58.76 time: 0.063
2026-01-05 16:30:53,070: Train batch 15800: loss: 13.69 grad norm: 62.98 time: 0.067
2026-01-05 16:31:11,934: Train batch 16000: loss: 8.50 grad norm: 45.56 time: 0.056
2026-01-05 16:31:11,934: Running test after training batch: 16000
2026-01-05 16:31:12,093: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:31:17,155: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-05 16:31:17,188: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 16:31:19,019: Val batch 16000: PER (avg): 0.1489 CTC Loss (avg): 15.3500 WER(1gram): 46.95% (n=64) time: 7.084
2026-01-05 16:31:19,020: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 16:31:19,020: t15.2023.08.13 val PER: 0.1060
2026-01-05 16:31:19,020: t15.2023.08.18 val PER: 0.1098
2026-01-05 16:31:19,020: t15.2023.08.20 val PER: 0.1025
2026-01-05 16:31:19,020: t15.2023.08.25 val PER: 0.0964
2026-01-05 16:31:19,020: t15.2023.08.27 val PER: 0.1881
2026-01-05 16:31:19,020: t15.2023.09.01 val PER: 0.0722
2026-01-05 16:31:19,020: t15.2023.09.03 val PER: 0.1437
2026-01-05 16:31:19,020: t15.2023.09.24 val PER: 0.1262
2026-01-05 16:31:19,020: t15.2023.09.29 val PER: 0.1321
2026-01-05 16:31:19,020: t15.2023.10.01 val PER: 0.1717
2026-01-05 16:31:19,020: t15.2023.10.06 val PER: 0.0775
2026-01-05 16:31:19,020: t15.2023.10.08 val PER: 0.2598
2026-01-05 16:31:19,021: t15.2023.10.13 val PER: 0.1939
2026-01-05 16:31:19,021: t15.2023.10.15 val PER: 0.1503
2026-01-05 16:31:19,021: t15.2023.10.20 val PER: 0.1812
2026-01-05 16:31:19,021: t15.2023.10.22 val PER: 0.1091
2026-01-05 16:31:19,021: t15.2023.11.03 val PER: 0.1791
2026-01-05 16:31:19,021: t15.2023.11.04 val PER: 0.0273
2026-01-05 16:31:19,021: t15.2023.11.17 val PER: 0.0373
2026-01-05 16:31:19,021: t15.2023.11.19 val PER: 0.0399
2026-01-05 16:31:19,021: t15.2023.11.26 val PER: 0.1138
2026-01-05 16:31:19,021: t15.2023.12.03 val PER: 0.1166
2026-01-05 16:31:19,022: t15.2023.12.08 val PER: 0.0945
2026-01-05 16:31:19,022: t15.2023.12.10 val PER: 0.0907
2026-01-05 16:31:19,022: t15.2023.12.17 val PER: 0.1341
2026-01-05 16:31:19,022: t15.2023.12.29 val PER: 0.1311
2026-01-05 16:31:19,022: t15.2024.02.25 val PER: 0.1039
2026-01-05 16:31:19,022: t15.2024.03.08 val PER: 0.2418
2026-01-05 16:31:19,022: t15.2024.03.15 val PER: 0.2001
2026-01-05 16:31:19,022: t15.2024.03.17 val PER: 0.1450
2026-01-05 16:31:19,022: t15.2024.05.10 val PER: 0.1649
2026-01-05 16:31:19,022: t15.2024.06.14 val PER: 0.1609
2026-01-05 16:31:19,022: t15.2024.07.19 val PER: 0.2347
2026-01-05 16:31:19,022: t15.2024.07.21 val PER: 0.0876
2026-01-05 16:31:19,022: t15.2024.07.28 val PER: 0.1353
2026-01-05 16:31:19,022: t15.2025.01.10 val PER: 0.2948
2026-01-05 16:31:19,022: t15.2025.01.12 val PER: 0.1424
2026-01-05 16:31:19,022: t15.2025.03.14 val PER: 0.3343
2026-01-05 16:31:19,023: t15.2025.03.16 val PER: 0.1950
2026-01-05 16:31:19,023: t15.2025.03.30 val PER: 0.3000
2026-01-05 16:31:19,023: t15.2025.04.13 val PER: 0.2040
2026-01-05 16:31:19,300: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_16000
2026-01-05 16:31:38,273: Train batch 16200: loss: 5.95 grad norm: 41.20 time: 0.056
2026-01-05 16:31:57,152: Train batch 16400: loss: 10.02 grad norm: 60.73 time: 0.058
2026-01-05 16:32:06,445: Running test after training batch: 16500
2026-01-05 16:32:06,546: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:32:11,538: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-05 16:32:11,572: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-05 16:32:13,464: Val batch 16500: PER (avg): 0.1481 CTC Loss (avg): 15.2942 WER(1gram): 45.94% (n=64) time: 7.019
2026-01-05 16:32:13,464: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 16:32:13,465: t15.2023.08.13 val PER: 0.1071
2026-01-05 16:32:13,465: t15.2023.08.18 val PER: 0.1048
2026-01-05 16:32:13,465: t15.2023.08.20 val PER: 0.1080
2026-01-05 16:32:13,465: t15.2023.08.25 val PER: 0.0919
2026-01-05 16:32:13,465: t15.2023.08.27 val PER: 0.1913
2026-01-05 16:32:13,465: t15.2023.09.01 val PER: 0.0682
2026-01-05 16:32:13,465: t15.2023.09.03 val PER: 0.1496
2026-01-05 16:32:13,465: t15.2023.09.24 val PER: 0.1299
2026-01-05 16:32:13,465: t15.2023.09.29 val PER: 0.1334
2026-01-05 16:32:13,465: t15.2023.10.01 val PER: 0.1691
2026-01-05 16:32:13,465: t15.2023.10.06 val PER: 0.0829
2026-01-05 16:32:13,465: t15.2023.10.08 val PER: 0.2517
2026-01-05 16:32:13,466: t15.2023.10.13 val PER: 0.1994
2026-01-05 16:32:13,466: t15.2023.10.15 val PER: 0.1470
2026-01-05 16:32:13,466: t15.2023.10.20 val PER: 0.1846
2026-01-05 16:32:13,466: t15.2023.10.22 val PER: 0.1080
2026-01-05 16:32:13,466: t15.2023.11.03 val PER: 0.1811
2026-01-05 16:32:13,466: t15.2023.11.04 val PER: 0.0273
2026-01-05 16:32:13,466: t15.2023.11.17 val PER: 0.0342
2026-01-05 16:32:13,466: t15.2023.11.19 val PER: 0.0419
2026-01-05 16:32:13,466: t15.2023.11.26 val PER: 0.1109
2026-01-05 16:32:13,467: t15.2023.12.03 val PER: 0.1113
2026-01-05 16:32:13,467: t15.2023.12.08 val PER: 0.0945
2026-01-05 16:32:13,467: t15.2023.12.10 val PER: 0.0854
2026-01-05 16:32:13,467: t15.2023.12.17 val PER: 0.1310
2026-01-05 16:32:13,467: t15.2023.12.29 val PER: 0.1318
2026-01-05 16:32:13,467: t15.2024.02.25 val PER: 0.1067
2026-01-05 16:32:13,467: t15.2024.03.08 val PER: 0.2361
2026-01-05 16:32:13,467: t15.2024.03.15 val PER: 0.1970
2026-01-05 16:32:13,467: t15.2024.03.17 val PER: 0.1395
2026-01-05 16:32:13,467: t15.2024.05.10 val PER: 0.1530
2026-01-05 16:32:13,467: t15.2024.06.14 val PER: 0.1672
2026-01-05 16:32:13,467: t15.2024.07.19 val PER: 0.2307
2026-01-05 16:32:13,467: t15.2024.07.21 val PER: 0.0903
2026-01-05 16:32:13,468: t15.2024.07.28 val PER: 0.1368
2026-01-05 16:32:13,468: t15.2025.01.10 val PER: 0.2865
2026-01-05 16:32:13,468: t15.2025.01.12 val PER: 0.1386
2026-01-05 16:32:13,468: t15.2025.03.14 val PER: 0.3565
2026-01-05 16:32:13,468: t15.2025.03.16 val PER: 0.1872
2026-01-05 16:32:13,468: t15.2025.03.30 val PER: 0.2931
2026-01-05 16:32:13,468: t15.2025.04.13 val PER: 0.2111
2026-01-05 16:32:13,737: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_16500
2026-01-05 16:32:23,502: Train batch 16600: loss: 8.77 grad norm: 53.14 time: 0.053
2026-01-05 16:32:43,122: Train batch 16800: loss: 16.09 grad norm: 73.26 time: 0.062
2026-01-05 16:33:03,125: Train batch 17000: loss: 7.89 grad norm: 49.58 time: 0.082
2026-01-05 16:33:03,126: Running test after training batch: 17000
2026-01-05 16:33:03,232: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:33:08,211: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-05 16:33:08,245: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-05 16:33:10,124: Val batch 17000: PER (avg): 0.1473 CTC Loss (avg): 15.1501 WER(1gram): 46.19% (n=64) time: 6.998
2026-01-05 16:33:10,125: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-05 16:33:10,125: t15.2023.08.13 val PER: 0.1091
2026-01-05 16:33:10,125: t15.2023.08.18 val PER: 0.1073
2026-01-05 16:33:10,125: t15.2023.08.20 val PER: 0.1072
2026-01-05 16:33:10,125: t15.2023.08.25 val PER: 0.1039
2026-01-05 16:33:10,125: t15.2023.08.27 val PER: 0.1913
2026-01-05 16:33:10,125: t15.2023.09.01 val PER: 0.0698
2026-01-05 16:33:10,125: t15.2023.09.03 val PER: 0.1544
2026-01-05 16:33:10,126: t15.2023.09.24 val PER: 0.1274
2026-01-05 16:33:10,126: t15.2023.09.29 val PER: 0.1315
2026-01-05 16:33:10,126: t15.2023.10.01 val PER: 0.1724
2026-01-05 16:33:10,126: t15.2023.10.06 val PER: 0.0807
2026-01-05 16:33:10,126: t15.2023.10.08 val PER: 0.2544
2026-01-05 16:33:10,126: t15.2023.10.13 val PER: 0.1932
2026-01-05 16:33:10,126: t15.2023.10.15 val PER: 0.1457
2026-01-05 16:33:10,126: t15.2023.10.20 val PER: 0.1913
2026-01-05 16:33:10,126: t15.2023.10.22 val PER: 0.1069
2026-01-05 16:33:10,126: t15.2023.11.03 val PER: 0.1744
2026-01-05 16:33:10,127: t15.2023.11.04 val PER: 0.0273
2026-01-05 16:33:10,127: t15.2023.11.17 val PER: 0.0358
2026-01-05 16:33:10,127: t15.2023.11.19 val PER: 0.0339
2026-01-05 16:33:10,127: t15.2023.11.26 val PER: 0.1087
2026-01-05 16:33:10,127: t15.2023.12.03 val PER: 0.1061
2026-01-05 16:33:10,127: t15.2023.12.08 val PER: 0.0925
2026-01-05 16:33:10,127: t15.2023.12.10 val PER: 0.0907
2026-01-05 16:33:10,127: t15.2023.12.17 val PER: 0.1331
2026-01-05 16:33:10,127: t15.2023.12.29 val PER: 0.1277
2026-01-05 16:33:10,127: t15.2024.02.25 val PER: 0.1124
2026-01-05 16:33:10,128: t15.2024.03.08 val PER: 0.2290
2026-01-05 16:33:10,128: t15.2024.03.15 val PER: 0.1995
2026-01-05 16:33:10,128: t15.2024.03.17 val PER: 0.1346
2026-01-05 16:33:10,128: t15.2024.05.10 val PER: 0.1456
2026-01-05 16:33:10,128: t15.2024.06.14 val PER: 0.1625
2026-01-05 16:33:10,128: t15.2024.07.19 val PER: 0.2307
2026-01-05 16:33:10,128: t15.2024.07.21 val PER: 0.0903
2026-01-05 16:33:10,128: t15.2024.07.28 val PER: 0.1324
2026-01-05 16:33:10,128: t15.2025.01.10 val PER: 0.2934
2026-01-05 16:33:10,128: t15.2025.01.12 val PER: 0.1401
2026-01-05 16:33:10,128: t15.2025.03.14 val PER: 0.3506
2026-01-05 16:33:10,129: t15.2025.03.16 val PER: 0.1846
2026-01-05 16:33:10,129: t15.2025.03.30 val PER: 0.2897
2026-01-05 16:33:10,129: t15.2025.04.13 val PER: 0.2154
2026-01-05 16:33:10,403: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_17000
2026-01-05 16:33:30,139: Train batch 17200: loss: 9.29 grad norm: 50.99 time: 0.087
2026-01-05 16:33:50,097: Train batch 17400: loss: 11.79 grad norm: 60.61 time: 0.071
2026-01-05 16:33:59,767: Running test after training batch: 17500
2026-01-05 16:33:59,919: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:34:04,792: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 16:34:04,827: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 16:34:06,733: Val batch 17500: PER (avg): 0.1464 CTC Loss (avg): 15.1486 WER(1gram): 46.45% (n=64) time: 6.966
2026-01-05 16:34:06,733: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 16:34:06,733: t15.2023.08.13 val PER: 0.1071
2026-01-05 16:34:06,733: t15.2023.08.18 val PER: 0.1081
2026-01-05 16:34:06,734: t15.2023.08.20 val PER: 0.1072
2026-01-05 16:34:06,734: t15.2023.08.25 val PER: 0.1024
2026-01-05 16:34:06,734: t15.2023.08.27 val PER: 0.1833
2026-01-05 16:34:06,734: t15.2023.09.01 val PER: 0.0666
2026-01-05 16:34:06,734: t15.2023.09.03 val PER: 0.1473
2026-01-05 16:34:06,734: t15.2023.09.24 val PER: 0.1335
2026-01-05 16:34:06,734: t15.2023.09.29 val PER: 0.1334
2026-01-05 16:34:06,734: t15.2023.10.01 val PER: 0.1717
2026-01-05 16:34:06,734: t15.2023.10.06 val PER: 0.0786
2026-01-05 16:34:06,735: t15.2023.10.08 val PER: 0.2585
2026-01-05 16:34:06,735: t15.2023.10.13 val PER: 0.1908
2026-01-05 16:34:06,735: t15.2023.10.15 val PER: 0.1463
2026-01-05 16:34:06,735: t15.2023.10.20 val PER: 0.2013
2026-01-05 16:34:06,735: t15.2023.10.22 val PER: 0.1024
2026-01-05 16:34:06,735: t15.2023.11.03 val PER: 0.1771
2026-01-05 16:34:06,735: t15.2023.11.04 val PER: 0.0307
2026-01-05 16:34:06,735: t15.2023.11.17 val PER: 0.0327
2026-01-05 16:34:06,736: t15.2023.11.19 val PER: 0.0359
2026-01-05 16:34:06,736: t15.2023.11.26 val PER: 0.1138
2026-01-05 16:34:06,736: t15.2023.12.03 val PER: 0.1061
2026-01-05 16:34:06,736: t15.2023.12.08 val PER: 0.0912
2026-01-05 16:34:06,736: t15.2023.12.10 val PER: 0.0854
2026-01-05 16:34:06,736: t15.2023.12.17 val PER: 0.1310
2026-01-05 16:34:06,736: t15.2023.12.29 val PER: 0.1283
2026-01-05 16:34:06,736: t15.2024.02.25 val PER: 0.1039
2026-01-05 16:34:06,736: t15.2024.03.08 val PER: 0.2262
2026-01-05 16:34:06,736: t15.2024.03.15 val PER: 0.1982
2026-01-05 16:34:06,736: t15.2024.03.17 val PER: 0.1332
2026-01-05 16:34:06,736: t15.2024.05.10 val PER: 0.1501
2026-01-05 16:34:06,736: t15.2024.06.14 val PER: 0.1625
2026-01-05 16:34:06,736: t15.2024.07.19 val PER: 0.2274
2026-01-05 16:34:06,736: t15.2024.07.21 val PER: 0.0883
2026-01-05 16:34:06,736: t15.2024.07.28 val PER: 0.1331
2026-01-05 16:34:06,737: t15.2025.01.10 val PER: 0.2879
2026-01-05 16:34:06,737: t15.2025.01.12 val PER: 0.1378
2026-01-05 16:34:06,737: t15.2025.03.14 val PER: 0.3491
2026-01-05 16:34:06,737: t15.2025.03.16 val PER: 0.1832
2026-01-05 16:34:06,737: t15.2025.03.30 val PER: 0.2874
2026-01-05 16:34:06,737: t15.2025.04.13 val PER: 0.2097
2026-01-05 16:34:07,004: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_17500
2026-01-05 16:34:16,004: Train batch 17600: loss: 9.04 grad norm: 52.54 time: 0.050
2026-01-05 16:34:35,656: Train batch 17800: loss: 6.32 grad norm: 52.04 time: 0.042
2026-01-05 16:34:54,739: Train batch 18000: loss: 10.80 grad norm: 64.40 time: 0.061
2026-01-05 16:34:54,739: Running test after training batch: 18000
2026-01-05 16:34:54,884: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:35:00,027: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 16:35:00,070: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-05 16:35:02,252: Val batch 18000: PER (avg): 0.1457 CTC Loss (avg): 15.1476 WER(1gram): 45.69% (n=64) time: 7.512
2026-01-05 16:35:02,252: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 16:35:02,253: t15.2023.08.13 val PER: 0.1091
2026-01-05 16:35:02,253: t15.2023.08.18 val PER: 0.1023
2026-01-05 16:35:02,253: t15.2023.08.20 val PER: 0.1056
2026-01-05 16:35:02,253: t15.2023.08.25 val PER: 0.1024
2026-01-05 16:35:02,253: t15.2023.08.27 val PER: 0.1897
2026-01-05 16:35:02,253: t15.2023.09.01 val PER: 0.0690
2026-01-05 16:35:02,253: t15.2023.09.03 val PER: 0.1425
2026-01-05 16:35:02,253: t15.2023.09.24 val PER: 0.1274
2026-01-05 16:35:02,254: t15.2023.09.29 val PER: 0.1327
2026-01-05 16:35:02,254: t15.2023.10.01 val PER: 0.1744
2026-01-05 16:35:02,254: t15.2023.10.06 val PER: 0.0764
2026-01-05 16:35:02,254: t15.2023.10.08 val PER: 0.2476
2026-01-05 16:35:02,254: t15.2023.10.13 val PER: 0.1947
2026-01-05 16:35:02,254: t15.2023.10.15 val PER: 0.1543
2026-01-05 16:35:02,254: t15.2023.10.20 val PER: 0.2114
2026-01-05 16:35:02,254: t15.2023.10.22 val PER: 0.1002
2026-01-05 16:35:02,254: t15.2023.11.03 val PER: 0.1696
2026-01-05 16:35:02,254: t15.2023.11.04 val PER: 0.0307
2026-01-05 16:35:02,255: t15.2023.11.17 val PER: 0.0327
2026-01-05 16:35:02,255: t15.2023.11.19 val PER: 0.0359
2026-01-05 16:35:02,255: t15.2023.11.26 val PER: 0.1087
2026-01-05 16:35:02,255: t15.2023.12.03 val PER: 0.1124
2026-01-05 16:35:02,255: t15.2023.12.08 val PER: 0.0932
2026-01-05 16:35:02,255: t15.2023.12.10 val PER: 0.0854
2026-01-05 16:35:02,255: t15.2023.12.17 val PER: 0.1331
2026-01-05 16:35:02,255: t15.2023.12.29 val PER: 0.1290
2026-01-05 16:35:02,255: t15.2024.02.25 val PER: 0.1081
2026-01-05 16:35:02,255: t15.2024.03.08 val PER: 0.2361
2026-01-05 16:35:02,256: t15.2024.03.15 val PER: 0.1964
2026-01-05 16:35:02,256: t15.2024.03.17 val PER: 0.1346
2026-01-05 16:35:02,256: t15.2024.05.10 val PER: 0.1352
2026-01-05 16:35:02,256: t15.2024.06.14 val PER: 0.1577
2026-01-05 16:35:02,256: t15.2024.07.19 val PER: 0.2287
2026-01-05 16:35:02,256: t15.2024.07.21 val PER: 0.0862
2026-01-05 16:35:02,256: t15.2024.07.28 val PER: 0.1309
2026-01-05 16:35:02,256: t15.2025.01.10 val PER: 0.2879
2026-01-05 16:35:02,256: t15.2025.01.12 val PER: 0.1370
2026-01-05 16:35:02,256: t15.2025.03.14 val PER: 0.3417
2026-01-05 16:35:02,256: t15.2025.03.16 val PER: 0.1806
2026-01-05 16:35:02,256: t15.2025.03.30 val PER: 0.2782
2026-01-05 16:35:02,256: t15.2025.04.13 val PER: 0.2068
2026-01-05 16:35:02,539: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_18000
2026-01-05 16:35:21,513: Train batch 18200: loss: 7.46 grad norm: 47.91 time: 0.074
2026-01-05 16:35:39,981: Train batch 18400: loss: 5.00 grad norm: 42.69 time: 0.058
2026-01-05 16:35:49,268: Running test after training batch: 18500
2026-01-05 16:35:49,428: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:35:54,569: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 16:35:54,609: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 16:35:56,802: Val batch 18500: PER (avg): 0.1457 CTC Loss (avg): 15.1200 WER(1gram): 45.18% (n=64) time: 7.534
2026-01-05 16:35:56,803: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-05 16:35:56,803: t15.2023.08.13 val PER: 0.1071
2026-01-05 16:35:56,803: t15.2023.08.18 val PER: 0.1048
2026-01-05 16:35:56,803: t15.2023.08.20 val PER: 0.1056
2026-01-05 16:35:56,803: t15.2023.08.25 val PER: 0.0994
2026-01-05 16:35:56,803: t15.2023.08.27 val PER: 0.1881
2026-01-05 16:35:56,803: t15.2023.09.01 val PER: 0.0666
2026-01-05 16:35:56,803: t15.2023.09.03 val PER: 0.1473
2026-01-05 16:35:56,804: t15.2023.09.24 val PER: 0.1299
2026-01-05 16:35:56,804: t15.2023.09.29 val PER: 0.1353
2026-01-05 16:35:56,804: t15.2023.10.01 val PER: 0.1737
2026-01-05 16:35:56,804: t15.2023.10.06 val PER: 0.0775
2026-01-05 16:35:56,804: t15.2023.10.08 val PER: 0.2571
2026-01-05 16:35:56,804: t15.2023.10.13 val PER: 0.1908
2026-01-05 16:35:56,804: t15.2023.10.15 val PER: 0.1463
2026-01-05 16:35:56,804: t15.2023.10.20 val PER: 0.1946
2026-01-05 16:35:56,804: t15.2023.10.22 val PER: 0.1047
2026-01-05 16:35:56,804: t15.2023.11.03 val PER: 0.1757
2026-01-05 16:35:56,805: t15.2023.11.04 val PER: 0.0273
2026-01-05 16:35:56,805: t15.2023.11.17 val PER: 0.0342
2026-01-05 16:35:56,805: t15.2023.11.19 val PER: 0.0339
2026-01-05 16:35:56,805: t15.2023.11.26 val PER: 0.1080
2026-01-05 16:35:56,805: t15.2023.12.03 val PER: 0.1071
2026-01-05 16:35:56,805: t15.2023.12.08 val PER: 0.0905
2026-01-05 16:35:56,805: t15.2023.12.10 val PER: 0.0828
2026-01-05 16:35:56,805: t15.2023.12.17 val PER: 0.1331
2026-01-05 16:35:56,805: t15.2023.12.29 val PER: 0.1263
2026-01-05 16:35:56,805: t15.2024.02.25 val PER: 0.1067
2026-01-05 16:35:56,805: t15.2024.03.08 val PER: 0.2276
2026-01-05 16:35:56,805: t15.2024.03.15 val PER: 0.2008
2026-01-05 16:35:56,805: t15.2024.03.17 val PER: 0.1297
2026-01-05 16:35:56,805: t15.2024.05.10 val PER: 0.1367
2026-01-05 16:35:56,805: t15.2024.06.14 val PER: 0.1577
2026-01-05 16:35:56,805: t15.2024.07.19 val PER: 0.2268
2026-01-05 16:35:56,805: t15.2024.07.21 val PER: 0.0890
2026-01-05 16:35:56,806: t15.2024.07.28 val PER: 0.1346
2026-01-05 16:35:56,806: t15.2025.01.10 val PER: 0.2851
2026-01-05 16:35:56,806: t15.2025.01.12 val PER: 0.1409
2026-01-05 16:35:56,806: t15.2025.03.14 val PER: 0.3402
2026-01-05 16:35:56,806: t15.2025.03.16 val PER: 0.1859
2026-01-05 16:35:56,806: t15.2025.03.30 val PER: 0.2839
2026-01-05 16:35:56,895: t15.2025.04.13 val PER: 0.2111
2026-01-05 16:35:57,174: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_18500
2026-01-05 16:36:06,484: Train batch 18600: loss: 12.49 grad norm: 63.61 time: 0.068
2026-01-05 16:36:25,152: Train batch 18800: loss: 8.12 grad norm: 51.04 time: 0.065
2026-01-05 16:36:44,428: Train batch 19000: loss: 8.23 grad norm: 45.80 time: 0.066
2026-01-05 16:36:44,428: Running test after training batch: 19000
2026-01-05 16:36:44,570: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:36:49,500: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 16:36:49,537: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-05 16:36:51,474: Val batch 19000: PER (avg): 0.1460 CTC Loss (avg): 15.1308 WER(1gram): 45.43% (n=64) time: 7.045
2026-01-05 16:36:51,475: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-05 16:36:51,475: t15.2023.08.13 val PER: 0.1060
2026-01-05 16:36:51,475: t15.2023.08.18 val PER: 0.0997
2026-01-05 16:36:51,475: t15.2023.08.20 val PER: 0.1064
2026-01-05 16:36:51,475: t15.2023.08.25 val PER: 0.0979
2026-01-05 16:36:51,475: t15.2023.08.27 val PER: 0.1897
2026-01-05 16:36:51,475: t15.2023.09.01 val PER: 0.0682
2026-01-05 16:36:51,475: t15.2023.09.03 val PER: 0.1508
2026-01-05 16:36:51,475: t15.2023.09.24 val PER: 0.1299
2026-01-05 16:36:51,475: t15.2023.09.29 val PER: 0.1327
2026-01-05 16:36:51,475: t15.2023.10.01 val PER: 0.1731
2026-01-05 16:36:51,476: t15.2023.10.06 val PER: 0.0818
2026-01-05 16:36:51,476: t15.2023.10.08 val PER: 0.2503
2026-01-05 16:36:51,476: t15.2023.10.13 val PER: 0.1963
2026-01-05 16:36:51,476: t15.2023.10.15 val PER: 0.1417
2026-01-05 16:36:51,476: t15.2023.10.20 val PER: 0.1846
2026-01-05 16:36:51,477: t15.2023.10.22 val PER: 0.1036
2026-01-05 16:36:51,477: t15.2023.11.03 val PER: 0.1798
2026-01-05 16:36:51,477: t15.2023.11.04 val PER: 0.0273
2026-01-05 16:36:51,477: t15.2023.11.17 val PER: 0.0358
2026-01-05 16:36:51,477: t15.2023.11.19 val PER: 0.0339
2026-01-05 16:36:51,477: t15.2023.11.26 val PER: 0.1116
2026-01-05 16:36:51,477: t15.2023.12.03 val PER: 0.1019
2026-01-05 16:36:51,477: t15.2023.12.08 val PER: 0.0905
2026-01-05 16:36:51,477: t15.2023.12.10 val PER: 0.0854
2026-01-05 16:36:51,477: t15.2023.12.17 val PER: 0.1320
2026-01-05 16:36:51,477: t15.2023.12.29 val PER: 0.1325
2026-01-05 16:36:51,477: t15.2024.02.25 val PER: 0.1067
2026-01-05 16:36:51,477: t15.2024.03.08 val PER: 0.2276
2026-01-05 16:36:51,478: t15.2024.03.15 val PER: 0.1982
2026-01-05 16:36:51,478: t15.2024.03.17 val PER: 0.1332
2026-01-05 16:36:51,478: t15.2024.05.10 val PER: 0.1367
2026-01-05 16:36:51,478: t15.2024.06.14 val PER: 0.1703
2026-01-05 16:36:51,478: t15.2024.07.19 val PER: 0.2268
2026-01-05 16:36:51,478: t15.2024.07.21 val PER: 0.0883
2026-01-05 16:36:51,478: t15.2024.07.28 val PER: 0.1346
2026-01-05 16:36:51,478: t15.2025.01.10 val PER: 0.2879
2026-01-05 16:36:51,478: t15.2025.01.12 val PER: 0.1378
2026-01-05 16:36:51,478: t15.2025.03.14 val PER: 0.3462
2026-01-05 16:36:51,478: t15.2025.03.16 val PER: 0.1911
2026-01-05 16:36:51,478: t15.2025.03.30 val PER: 0.2828
2026-01-05 16:36:51,478: t15.2025.04.13 val PER: 0.2011
2026-01-05 16:36:51,759: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_19000
2026-01-05 16:37:10,873: Train batch 19200: loss: 5.72 grad norm: 44.74 time: 0.065
2026-01-05 16:37:29,479: Train batch 19400: loss: 4.72 grad norm: 40.95 time: 0.053
2026-01-05 16:37:38,706: Running test after training batch: 19500
2026-01-05 16:37:38,854: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:37:43,949: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 16:37:43,988: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 16:37:46,343: Val batch 19500: PER (avg): 0.1462 CTC Loss (avg): 15.0675 WER(1gram): 45.69% (n=64) time: 7.636
2026-01-05 16:37:46,343: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-05 16:37:46,344: t15.2023.08.13 val PER: 0.1071
2026-01-05 16:37:46,344: t15.2023.08.18 val PER: 0.1073
2026-01-05 16:37:46,344: t15.2023.08.20 val PER: 0.1096
2026-01-05 16:37:46,344: t15.2023.08.25 val PER: 0.0994
2026-01-05 16:37:46,344: t15.2023.08.27 val PER: 0.1849
2026-01-05 16:37:46,344: t15.2023.09.01 val PER: 0.0666
2026-01-05 16:37:46,344: t15.2023.09.03 val PER: 0.1449
2026-01-05 16:37:46,344: t15.2023.09.24 val PER: 0.1274
2026-01-05 16:37:46,344: t15.2023.09.29 val PER: 0.1359
2026-01-05 16:37:46,345: t15.2023.10.01 val PER: 0.1731
2026-01-05 16:37:46,345: t15.2023.10.06 val PER: 0.0775
2026-01-05 16:37:46,345: t15.2023.10.08 val PER: 0.2585
2026-01-05 16:37:46,345: t15.2023.10.13 val PER: 0.1963
2026-01-05 16:37:46,345: t15.2023.10.15 val PER: 0.1397
2026-01-05 16:37:46,345: t15.2023.10.20 val PER: 0.1946
2026-01-05 16:37:46,345: t15.2023.10.22 val PER: 0.1047
2026-01-05 16:37:46,345: t15.2023.11.03 val PER: 0.1811
2026-01-05 16:37:46,345: t15.2023.11.04 val PER: 0.0239
2026-01-05 16:37:46,345: t15.2023.11.17 val PER: 0.0327
2026-01-05 16:37:46,345: t15.2023.11.19 val PER: 0.0359
2026-01-05 16:37:46,345: t15.2023.11.26 val PER: 0.1058
2026-01-05 16:37:46,345: t15.2023.12.03 val PER: 0.1040
2026-01-05 16:37:46,345: t15.2023.12.08 val PER: 0.0912
2026-01-05 16:37:46,345: t15.2023.12.10 val PER: 0.0867
2026-01-05 16:37:46,345: t15.2023.12.17 val PER: 0.1320
2026-01-05 16:37:46,346: t15.2023.12.29 val PER: 0.1318
2026-01-05 16:37:46,346: t15.2024.02.25 val PER: 0.1096
2026-01-05 16:37:46,346: t15.2024.03.08 val PER: 0.2333
2026-01-05 16:37:46,346: t15.2024.03.15 val PER: 0.1957
2026-01-05 16:37:46,346: t15.2024.03.17 val PER: 0.1332
2026-01-05 16:37:46,346: t15.2024.05.10 val PER: 0.1367
2026-01-05 16:37:46,346: t15.2024.06.14 val PER: 0.1593
2026-01-05 16:37:46,346: t15.2024.07.19 val PER: 0.2274
2026-01-05 16:37:46,346: t15.2024.07.21 val PER: 0.0876
2026-01-05 16:37:46,347: t15.2024.07.28 val PER: 0.1324
2026-01-05 16:37:46,347: t15.2025.01.10 val PER: 0.2906
2026-01-05 16:37:46,347: t15.2025.01.12 val PER: 0.1393
2026-01-05 16:37:46,347: t15.2025.03.14 val PER: 0.3447
2026-01-05 16:37:46,347: t15.2025.03.16 val PER: 0.1898
2026-01-05 16:37:46,347: t15.2025.03.30 val PER: 0.2862
2026-01-05 16:37:46,347: t15.2025.04.13 val PER: 0.2083
2026-01-05 16:37:46,630: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_19500
2026-01-05 16:37:55,729: Train batch 19600: loss: 7.30 grad norm: 46.47 time: 0.058
2026-01-05 16:38:15,208: Train batch 19800: loss: 7.06 grad norm: 47.47 time: 0.055
2026-01-05 16:38:34,808: Running test after training batch: 19999
2026-01-05 16:38:34,914: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:38:39,880: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 16:38:39,916: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 16:38:41,869: Val batch 19999: PER (avg): 0.1458 CTC Loss (avg): 15.0701 WER(1gram): 44.42% (n=64) time: 7.060
2026-01-05 16:38:41,870: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-05 16:38:41,870: t15.2023.08.13 val PER: 0.1040
2026-01-05 16:38:41,870: t15.2023.08.18 val PER: 0.1014
2026-01-05 16:38:41,870: t15.2023.08.20 val PER: 0.1096
2026-01-05 16:38:41,870: t15.2023.08.25 val PER: 0.1009
2026-01-05 16:38:41,870: t15.2023.08.27 val PER: 0.1865
2026-01-05 16:38:41,870: t15.2023.09.01 val PER: 0.0666
2026-01-05 16:38:41,870: t15.2023.09.03 val PER: 0.1520
2026-01-05 16:38:41,871: t15.2023.09.24 val PER: 0.1299
2026-01-05 16:38:41,871: t15.2023.09.29 val PER: 0.1321
2026-01-05 16:38:41,871: t15.2023.10.01 val PER: 0.1724
2026-01-05 16:38:41,871: t15.2023.10.06 val PER: 0.0743
2026-01-05 16:38:41,871: t15.2023.10.08 val PER: 0.2571
2026-01-05 16:38:41,871: t15.2023.10.13 val PER: 0.1986
2026-01-05 16:38:41,871: t15.2023.10.15 val PER: 0.1450
2026-01-05 16:38:41,871: t15.2023.10.20 val PER: 0.1846
2026-01-05 16:38:41,871: t15.2023.10.22 val PER: 0.1047
2026-01-05 16:38:41,871: t15.2023.11.03 val PER: 0.1818
2026-01-05 16:38:41,871: t15.2023.11.04 val PER: 0.0273
2026-01-05 16:38:41,872: t15.2023.11.17 val PER: 0.0342
2026-01-05 16:38:41,872: t15.2023.11.19 val PER: 0.0379
2026-01-05 16:38:41,872: t15.2023.11.26 val PER: 0.1109
2026-01-05 16:38:41,872: t15.2023.12.03 val PER: 0.1071
2026-01-05 16:38:41,872: t15.2023.12.08 val PER: 0.0885
2026-01-05 16:38:41,872: t15.2023.12.10 val PER: 0.0880
2026-01-05 16:38:41,872: t15.2023.12.17 val PER: 0.1331
2026-01-05 16:38:41,872: t15.2023.12.29 val PER: 0.1277
2026-01-05 16:38:41,872: t15.2024.02.25 val PER: 0.1067
2026-01-05 16:38:41,872: t15.2024.03.08 val PER: 0.2276
2026-01-05 16:38:41,872: t15.2024.03.15 val PER: 0.1951
2026-01-05 16:38:41,872: t15.2024.03.17 val PER: 0.1339
2026-01-05 16:38:41,873: t15.2024.05.10 val PER: 0.1397
2026-01-05 16:38:41,873: t15.2024.06.14 val PER: 0.1546
2026-01-05 16:38:41,873: t15.2024.07.19 val PER: 0.2248
2026-01-05 16:38:41,873: t15.2024.07.21 val PER: 0.0897
2026-01-05 16:38:41,873: t15.2024.07.28 val PER: 0.1338
2026-01-05 16:38:41,873: t15.2025.01.10 val PER: 0.2906
2026-01-05 16:38:41,873: t15.2025.01.12 val PER: 0.1416
2026-01-05 16:38:41,873: t15.2025.03.14 val PER: 0.3388
2026-01-05 16:38:41,873: t15.2025.03.16 val PER: 0.1832
2026-01-05 16:38:41,873: t15.2025.03.30 val PER: 0.2839
2026-01-05 16:38:41,873: t15.2025.04.13 val PER: 0.2054
2026-01-05 16:38:41,875: New best val WER(1gram) 44.92% --> 44.42%
2026-01-05 16:38:41,875: Checkpointing model
2026-01-05 16:38:43,585: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:38:43,871: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/base_patch_stride_wd1e-5/checkpoint/checkpoint_batch_19999
2026-01-05 16:38:43,897: Best avg val PER achieved: 0.14585
2026-01-05 16:38:43,898: Total training time: 37.57 minutes

=== RUN st2_wd1e-5.yaml ===
2026-01-05 16:38:48,684: Using device: cuda:0
2026-01-05 16:38:50,374: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-05 16:38:50,401: Using 45 sessions after filtering (from 45).
2026-01-05 16:38:50,806: Using torch.compile (if available)
2026-01-05 16:38:50,807: torch.compile not available (torch<2.0). Skipping.
2026-01-05 16:38:50,807: Initialized RNN decoding model
2026-01-05 16:38:50,807: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-05 16:38:50,807: Model has 44,907,305 parameters
2026-01-05 16:38:50,807: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-05 16:38:52,075: Successfully initialized datasets
2026-01-05 16:38:52,076: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-05 16:38:53,120: Train batch 0: loss: 1208.79 grad norm: 2472.87 time: 0.241
2026-01-05 16:38:53,121: Running test after training batch: 0
2026-01-05 16:38:53,249: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:39:00,064: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf meehans imhof triomphe
2026-01-05 16:39:01,516: WER debug example
  GT : how does it keep the cost down
  PR : shenanigan brawny africa homophones
2026-01-05 16:40:10,408: Val batch 0: PER (avg): 2.4878 CTC Loss (avg): 1310.9590 WER(1gram): 100.25% (n=64) time: 77.287
2026-01-05 16:40:10,408: WER lens: avg_true_words=6.16 avg_pred_words=3.45 max_pred_words=7
2026-01-05 16:40:10,409: t15.2023.08.13 val PER: 2.1383
2026-01-05 16:40:10,409: t15.2023.08.18 val PER: 2.3562
2026-01-05 16:40:10,409: t15.2023.08.20 val PER: 2.2097
2026-01-05 16:40:10,409: t15.2023.08.25 val PER: 2.3163
2026-01-05 16:40:10,409: t15.2023.08.27 val PER: 2.1061
2026-01-05 16:40:10,409: t15.2023.09.01 val PER: 2.4067
2026-01-05 16:40:10,409: t15.2023.09.03 val PER: 2.1746
2026-01-05 16:40:10,409: t15.2023.09.24 val PER: 2.6262
2026-01-05 16:40:10,409: t15.2023.09.29 val PER: 2.5686
2026-01-05 16:40:10,409: t15.2023.10.01 val PER: 2.0304
2026-01-05 16:40:10,409: t15.2023.10.06 val PER: 2.5942
2026-01-05 16:40:10,410: t15.2023.10.08 val PER: 1.9662
2026-01-05 16:40:10,410: t15.2023.10.13 val PER: 2.3173
2026-01-05 16:40:10,410: t15.2023.10.15 val PER: 2.4107
2026-01-05 16:40:10,410: t15.2023.10.20 val PER: 2.5000
2026-01-05 16:40:10,410: t15.2023.10.22 val PER: 2.5735
2026-01-05 16:40:10,410: t15.2023.11.03 val PER: 2.7897
2026-01-05 16:40:10,410: t15.2023.11.04 val PER: 3.5085
2026-01-05 16:40:10,410: t15.2023.11.17 val PER: 3.5117
2026-01-05 16:40:10,410: t15.2023.11.19 val PER: 2.9641
2026-01-05 16:40:10,410: t15.2023.11.26 val PER: 2.6906
2026-01-05 16:40:10,410: t15.2023.12.03 val PER: 2.5347
2026-01-05 16:40:10,410: t15.2023.12.08 val PER: 2.6272
2026-01-05 16:40:10,410: t15.2023.12.10 val PER: 2.9382
2026-01-05 16:40:10,410: t15.2023.12.17 val PER: 2.3222
2026-01-05 16:40:10,410: t15.2023.12.29 val PER: 2.4516
2026-01-05 16:40:10,410: t15.2024.02.25 val PER: 2.5969
2026-01-05 16:40:10,411: t15.2024.03.08 val PER: 2.3770
2026-01-05 16:40:10,411: t15.2024.03.15 val PER: 2.3014
2026-01-05 16:40:10,411: t15.2024.03.17 val PER: 2.4156
2026-01-05 16:40:10,411: t15.2024.05.10 val PER: 2.3834
2026-01-05 16:40:10,411: t15.2024.06.14 val PER: 2.7461
2026-01-05 16:40:10,411: t15.2024.07.19 val PER: 1.8741
2026-01-05 16:40:10,411: t15.2024.07.21 val PER: 2.8090
2026-01-05 16:40:10,411: t15.2024.07.28 val PER: 2.9956
2026-01-05 16:40:10,411: t15.2025.01.10 val PER: 1.8636
2026-01-05 16:40:10,411: t15.2025.01.12 val PER: 3.1440
2026-01-05 16:40:10,411: t15.2025.03.14 val PER: 1.7648
2026-01-05 16:40:10,411: t15.2025.03.16 val PER: 3.0668
2026-01-05 16:40:10,411: t15.2025.03.30 val PER: 2.2839
2026-01-05 16:40:10,411: t15.2025.04.13 val PER: 2.7118
2026-01-05 16:40:10,413: New best val WER(1gram) inf% --> 100.25%
2026-01-05 16:40:10,413: Checkpointing model
2026-01-05 16:40:10,684: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:40:10,962: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_0
2026-01-05 16:40:33,228: Train batch 200: loss: 97.16 grad norm: 371.49 time: 0.092
2026-01-05 16:40:55,154: Train batch 400: loss: 71.64 grad norm: 86.06 time: 0.110
2026-01-05 16:41:06,046: Running test after training batch: 500
2026-01-05 16:41:06,150: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:41:12,183: WER debug example
  GT : you can see the code at this point as well
  PR : hued hands these mothershead his this hadnot his
2026-01-05 16:41:12,352: WER debug example
  GT : how does it keep the cost down
  PR : noteholders hynds hughley thus handstands
2026-01-05 16:41:24,822: Val batch 500: PER (avg): 0.6488 CTC Loss (avg): 75.5851 WER(1gram): 99.24% (n=64) time: 18.776
2026-01-05 16:41:24,822: WER lens: avg_true_words=6.16 avg_pred_words=4.30 max_pred_words=10
2026-01-05 16:41:24,823: t15.2023.08.13 val PER: 0.6518
2026-01-05 16:41:24,823: t15.2023.08.18 val PER: 0.6320
2026-01-05 16:41:24,823: t15.2023.08.20 val PER: 0.6021
2026-01-05 16:41:24,823: t15.2023.08.25 val PER: 0.5979
2026-01-05 16:41:24,823: t15.2023.08.27 val PER: 0.6624
2026-01-05 16:41:24,823: t15.2023.09.01 val PER: 0.6055
2026-01-05 16:41:24,823: t15.2023.09.03 val PER: 0.6556
2026-01-05 16:41:24,824: t15.2023.09.24 val PER: 0.6214
2026-01-05 16:41:24,824: t15.2023.09.29 val PER: 0.6101
2026-01-05 16:41:24,824: t15.2023.10.01 val PER: 0.6361
2026-01-05 16:41:24,824: t15.2023.10.06 val PER: 0.5974
2026-01-05 16:41:24,824: t15.2023.10.08 val PER: 0.6509
2026-01-05 16:41:24,824: t15.2023.10.13 val PER: 0.6850
2026-01-05 16:41:24,824: t15.2023.10.15 val PER: 0.6678
2026-01-05 16:41:24,824: t15.2023.10.20 val PER: 0.6376
2026-01-05 16:41:24,824: t15.2023.10.22 val PER: 0.6414
2026-01-05 16:41:24,824: t15.2023.11.03 val PER: 0.6750
2026-01-05 16:41:24,824: t15.2023.11.04 val PER: 0.5358
2026-01-05 16:41:24,825: t15.2023.11.17 val PER: 0.5537
2026-01-05 16:41:24,825: t15.2023.11.19 val PER: 0.5230
2026-01-05 16:41:24,825: t15.2023.11.26 val PER: 0.6580
2026-01-05 16:41:24,825: t15.2023.12.03 val PER: 0.6471
2026-01-05 16:41:24,825: t15.2023.12.08 val PER: 0.6158
2026-01-05 16:41:24,825: t15.2023.12.10 val PER: 0.6413
2026-01-05 16:41:24,825: t15.2023.12.17 val PER: 0.6237
2026-01-05 16:41:24,825: t15.2023.12.29 val PER: 0.6177
2026-01-05 16:41:24,825: t15.2024.02.25 val PER: 0.6531
2026-01-05 16:41:24,825: t15.2024.03.08 val PER: 0.6671
2026-01-05 16:41:24,825: t15.2024.03.15 val PER: 0.6498
2026-01-05 16:41:24,825: t15.2024.03.17 val PER: 0.6325
2026-01-05 16:41:24,825: t15.2024.05.10 val PER: 0.6657
2026-01-05 16:41:24,825: t15.2024.06.14 val PER: 0.7114
2026-01-05 16:41:24,825: t15.2024.07.19 val PER: 0.6902
2026-01-05 16:41:24,825: t15.2024.07.21 val PER: 0.6600
2026-01-05 16:41:24,826: t15.2024.07.28 val PER: 0.7265
2026-01-05 16:41:24,826: t15.2025.01.10 val PER: 0.7493
2026-01-05 16:41:24,826: t15.2025.01.12 val PER: 0.6513
2026-01-05 16:41:24,826: t15.2025.03.14 val PER: 0.7293
2026-01-05 16:41:24,826: t15.2025.03.16 val PER: 0.6872
2026-01-05 16:41:24,826: t15.2025.03.30 val PER: 0.7241
2026-01-05 16:41:24,826: t15.2025.04.13 val PER: 0.6619
2026-01-05 16:41:24,827: New best val WER(1gram) 100.25% --> 99.24%
2026-01-05 16:41:24,827: Checkpointing model
2026-01-05 16:41:26,498: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:41:26,776: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_500
2026-01-05 16:41:37,640: Train batch 600: loss: 65.62 grad norm: 86.58 time: 0.136
2026-01-05 16:41:59,293: Train batch 800: loss: 57.18 grad norm: 98.86 time: 0.100
2026-01-05 16:42:21,139: Train batch 1000: loss: 56.61 grad norm: 83.21 time: 0.113
2026-01-05 16:42:21,140: Running test after training batch: 1000
2026-01-05 16:42:21,250: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:42:27,051: WER debug example
  GT : you can see the code at this point as well
  PR : hudy kansas businesses headhunters sparred kozuch
2026-01-05 16:42:27,127: WER debug example
  GT : how does it keep the cost down
  PR : diehards hitting skinks the
2026-01-05 16:42:34,875: Val batch 1000: PER (avg): 0.5475 CTC Loss (avg): 60.0251 WER(1gram): 98.98% (n=64) time: 13.735
2026-01-05 16:42:34,875: WER lens: avg_true_words=6.16 avg_pred_words=4.84 max_pred_words=10
2026-01-05 16:42:34,875: t15.2023.08.13 val PER: 0.5270
2026-01-05 16:42:34,876: t15.2023.08.18 val PER: 0.5038
2026-01-05 16:42:34,876: t15.2023.08.20 val PER: 0.4901
2026-01-05 16:42:34,876: t15.2023.08.25 val PER: 0.4563
2026-01-05 16:42:34,876: t15.2023.08.27 val PER: 0.5418
2026-01-05 16:42:34,876: t15.2023.09.01 val PER: 0.4781
2026-01-05 16:42:34,876: t15.2023.09.03 val PER: 0.5629
2026-01-05 16:42:34,876: t15.2023.09.24 val PER: 0.4867
2026-01-05 16:42:34,876: t15.2023.09.29 val PER: 0.5220
2026-01-05 16:42:34,876: t15.2023.10.01 val PER: 0.5482
2026-01-05 16:42:34,876: t15.2023.10.06 val PER: 0.4801
2026-01-05 16:42:34,876: t15.2023.10.08 val PER: 0.5629
2026-01-05 16:42:34,876: t15.2023.10.13 val PER: 0.6222
2026-01-05 16:42:34,876: t15.2023.10.15 val PER: 0.5320
2026-01-05 16:42:34,877: t15.2023.10.20 val PER: 0.4899
2026-01-05 16:42:34,877: t15.2023.10.22 val PER: 0.4811
2026-01-05 16:42:34,877: t15.2023.11.03 val PER: 0.5631
2026-01-05 16:42:34,877: t15.2023.11.04 val PER: 0.3686
2026-01-05 16:42:34,877: t15.2023.11.17 val PER: 0.4075
2026-01-05 16:42:34,877: t15.2023.11.19 val PER: 0.3593
2026-01-05 16:42:34,877: t15.2023.11.26 val PER: 0.5906
2026-01-05 16:42:34,877: t15.2023.12.03 val PER: 0.5641
2026-01-05 16:42:34,877: t15.2023.12.08 val PER: 0.5566
2026-01-05 16:42:34,877: t15.2023.12.10 val PER: 0.5125
2026-01-05 16:42:34,877: t15.2023.12.17 val PER: 0.5322
2026-01-05 16:42:34,877: t15.2023.12.29 val PER: 0.5292
2026-01-05 16:42:34,877: t15.2024.02.25 val PER: 0.5197
2026-01-05 16:42:34,877: t15.2024.03.08 val PER: 0.5932
2026-01-05 16:42:34,877: t15.2024.03.15 val PER: 0.5353
2026-01-05 16:42:34,878: t15.2024.03.17 val PER: 0.5230
2026-01-05 16:42:34,878: t15.2024.05.10 val PER: 0.5513
2026-01-05 16:42:34,878: t15.2024.06.14 val PER: 0.5426
2026-01-05 16:42:34,878: t15.2024.07.19 val PER: 0.6355
2026-01-05 16:42:34,878: t15.2024.07.21 val PER: 0.4986
2026-01-05 16:42:34,878: t15.2024.07.28 val PER: 0.5581
2026-01-05 16:42:34,878: t15.2025.01.10 val PER: 0.7176
2026-01-05 16:42:34,878: t15.2025.01.12 val PER: 0.6151
2026-01-05 16:42:34,878: t15.2025.03.14 val PER: 0.6953
2026-01-05 16:42:34,878: t15.2025.03.16 val PER: 0.6309
2026-01-05 16:42:34,878: t15.2025.03.30 val PER: 0.7299
2026-01-05 16:42:34,878: t15.2025.04.13 val PER: 0.6177
2026-01-05 16:42:34,879: New best val WER(1gram) 99.24% --> 98.98%
2026-01-05 16:42:34,880: Checkpointing model
2026-01-05 16:42:36,553: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:42:36,832: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_1000
2026-01-05 16:42:58,917: Train batch 1200: loss: 52.93 grad norm: 92.72 time: 0.117
2026-01-05 16:43:20,927: Train batch 1400: loss: 51.57 grad norm: 120.89 time: 0.103
2026-01-05 16:43:31,900: Running test after training batch: 1500
2026-01-05 16:43:32,017: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:43:38,231: WER debug example
  GT : you can see the code at this point as well
  PR : hughley candidacies the stuccoed buckhantz expired kozuch
2026-01-05 16:43:38,314: WER debug example
  GT : how does it keep the cost down
  PR : holds spas hideous caltha succeeds
2026-01-05 16:43:44,678: Val batch 1500: PER (avg): 0.4972 CTC Loss (avg): 54.0804 WER(1gram): 97.97% (n=64) time: 12.777
2026-01-05 16:43:44,678: WER lens: avg_true_words=6.16 avg_pred_words=5.27 max_pred_words=10
2026-01-05 16:43:44,679: t15.2023.08.13 val PER: 0.4657
2026-01-05 16:43:44,679: t15.2023.08.18 val PER: 0.4568
2026-01-05 16:43:44,679: t15.2023.08.20 val PER: 0.4535
2026-01-05 16:43:44,679: t15.2023.08.25 val PER: 0.4006
2026-01-05 16:43:44,679: t15.2023.08.27 val PER: 0.5145
2026-01-05 16:43:44,679: t15.2023.09.01 val PER: 0.4343
2026-01-05 16:43:44,679: t15.2023.09.03 val PER: 0.5261
2026-01-05 16:43:44,679: t15.2023.09.24 val PER: 0.4345
2026-01-05 16:43:44,679: t15.2023.09.29 val PER: 0.4544
2026-01-05 16:43:44,680: t15.2023.10.01 val PER: 0.5053
2026-01-05 16:43:44,680: t15.2023.10.06 val PER: 0.4090
2026-01-05 16:43:44,680: t15.2023.10.08 val PER: 0.5304
2026-01-05 16:43:44,680: t15.2023.10.13 val PER: 0.5663
2026-01-05 16:43:44,680: t15.2023.10.15 val PER: 0.4838
2026-01-05 16:43:44,680: t15.2023.10.20 val PER: 0.4564
2026-01-05 16:43:44,680: t15.2023.10.22 val PER: 0.4310
2026-01-05 16:43:44,680: t15.2023.11.03 val PER: 0.4891
2026-01-05 16:43:44,680: t15.2023.11.04 val PER: 0.2560
2026-01-05 16:43:44,680: t15.2023.11.17 val PER: 0.3515
2026-01-05 16:43:44,680: t15.2023.11.19 val PER: 0.2994
2026-01-05 16:43:44,680: t15.2023.11.26 val PER: 0.5587
2026-01-05 16:43:44,680: t15.2023.12.03 val PER: 0.5000
2026-01-05 16:43:44,680: t15.2023.12.08 val PER: 0.4607
2026-01-05 16:43:44,680: t15.2023.12.10 val PER: 0.4639
2026-01-05 16:43:44,680: t15.2023.12.17 val PER: 0.4906
2026-01-05 16:43:44,681: t15.2023.12.29 val PER: 0.4818
2026-01-05 16:43:44,681: t15.2024.02.25 val PER: 0.4452
2026-01-05 16:43:44,681: t15.2024.03.08 val PER: 0.5505
2026-01-05 16:43:44,681: t15.2024.03.15 val PER: 0.5009
2026-01-05 16:43:44,681: t15.2024.03.17 val PER: 0.4937
2026-01-05 16:43:44,681: t15.2024.05.10 val PER: 0.4785
2026-01-05 16:43:44,681: t15.2024.06.14 val PER: 0.5016
2026-01-05 16:43:44,681: t15.2024.07.19 val PER: 0.6170
2026-01-05 16:43:44,681: t15.2024.07.21 val PER: 0.4434
2026-01-05 16:43:44,681: t15.2024.07.28 val PER: 0.4809
2026-01-05 16:43:44,681: t15.2025.01.10 val PER: 0.7273
2026-01-05 16:43:44,681: t15.2025.01.12 val PER: 0.5466
2026-01-05 16:43:44,681: t15.2025.03.14 val PER: 0.6672
2026-01-05 16:43:44,681: t15.2025.03.16 val PER: 0.5733
2026-01-05 16:43:44,681: t15.2025.03.30 val PER: 0.6816
2026-01-05 16:43:44,681: t15.2025.04.13 val PER: 0.5892
2026-01-05 16:43:44,683: New best val WER(1gram) 98.98% --> 97.97%
2026-01-05 16:43:44,683: Checkpointing model
2026-01-05 16:43:46,288: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/best_checkpoint
2026-01-05 16:43:46,571: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_1500
2026-01-05 16:43:57,612: Train batch 1600: loss: 49.55 grad norm: 92.69 time: 0.110
2026-01-05 16:44:19,308: Train batch 1800: loss: 47.90 grad norm: 89.34 time: 0.153
2026-01-05 16:44:41,466: Train batch 2000: loss: 49.19 grad norm: 85.29 time: 0.115
2026-01-05 16:44:41,466: Running test after training batch: 2000
2026-01-05 16:44:41,588: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:44:47,608: WER debug example
  GT : you can see the code at this point as well
  PR : hudy kintz businesses aholds hotspot kozuch
2026-01-05 16:44:47,735: WER debug example
  GT : how does it keep the cost down
  PR : nother diehards spas sitze skinks thus
2026-01-05 16:44:53,552: Val batch 2000: PER (avg): 0.4444 CTC Loss (avg): 49.0521 WER(1gram): 103.05% (n=64) time: 12.086
2026-01-05 16:44:53,553: WER lens: avg_true_words=6.16 avg_pred_words=5.86 max_pred_words=11
2026-01-05 16:44:53,553: t15.2023.08.13 val PER: 0.4241
2026-01-05 16:44:53,553: t15.2023.08.18 val PER: 0.3831
2026-01-05 16:44:53,553: t15.2023.08.20 val PER: 0.3828
2026-01-05 16:44:53,553: t15.2023.08.25 val PER: 0.3690
2026-01-05 16:44:53,553: t15.2023.08.27 val PER: 0.4743
2026-01-05 16:44:53,553: t15.2023.09.01 val PER: 0.3571
2026-01-05 16:44:53,553: t15.2023.09.03 val PER: 0.4667
2026-01-05 16:44:53,553: t15.2023.09.24 val PER: 0.3762
2026-01-05 16:44:53,553: t15.2023.09.29 val PER: 0.4008
2026-01-05 16:44:53,554: t15.2023.10.01 val PER: 0.4538
2026-01-05 16:44:53,554: t15.2023.10.06 val PER: 0.3606
2026-01-05 16:44:53,554: t15.2023.10.08 val PER: 0.4885
2026-01-05 16:44:53,554: t15.2023.10.13 val PER: 0.5097
2026-01-05 16:44:53,554: t15.2023.10.15 val PER: 0.4219
2026-01-05 16:44:53,554: t15.2023.10.20 val PER: 0.4161
2026-01-05 16:44:53,555: t15.2023.10.22 val PER: 0.3853
2026-01-05 16:44:53,555: t15.2023.11.03 val PER: 0.4274
2026-01-05 16:44:53,555: t15.2023.11.04 val PER: 0.2253
2026-01-05 16:44:53,555: t15.2023.11.17 val PER: 0.2893
2026-01-05 16:44:53,555: t15.2023.11.19 val PER: 0.2735
2026-01-05 16:44:53,555: t15.2023.11.26 val PER: 0.4804
2026-01-05 16:44:53,555: t15.2023.12.03 val PER: 0.4559
2026-01-05 16:44:53,556: t15.2023.12.08 val PER: 0.4394
2026-01-05 16:44:53,556: t15.2023.12.10 val PER: 0.4126
2026-01-05 16:44:53,556: t15.2023.12.17 val PER: 0.3898
2026-01-05 16:44:53,556: t15.2023.12.29 val PER: 0.4345
2026-01-05 16:44:53,556: t15.2024.02.25 val PER: 0.3876
2026-01-05 16:44:53,556: t15.2024.03.08 val PER: 0.5007
2026-01-05 16:44:53,556: t15.2024.03.15 val PER: 0.4440
2026-01-05 16:44:53,556: t15.2024.03.17 val PER: 0.4386
2026-01-05 16:44:53,556: t15.2024.05.10 val PER: 0.4368
2026-01-05 16:44:53,556: t15.2024.06.14 val PER: 0.4716
2026-01-05 16:44:53,556: t15.2024.07.19 val PER: 0.5603
2026-01-05 16:44:53,557: t15.2024.07.21 val PER: 0.4062
2026-01-05 16:44:53,557: t15.2024.07.28 val PER: 0.4294
2026-01-05 16:44:53,557: t15.2025.01.10 val PER: 0.6612
2026-01-05 16:44:53,557: t15.2025.01.12 val PER: 0.5112
2026-01-05 16:44:53,557: t15.2025.03.14 val PER: 0.6183
2026-01-05 16:44:53,557: t15.2025.03.16 val PER: 0.5131
2026-01-05 16:44:53,557: t15.2025.03.30 val PER: 0.6552
2026-01-05 16:44:53,557: t15.2025.04.13 val PER: 0.5435
2026-01-05 16:44:53,826: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_2000
2026-01-05 16:45:15,823: Train batch 2200: loss: 44.69 grad norm: 85.79 time: 0.105
2026-01-05 16:45:37,270: Train batch 2400: loss: 42.20 grad norm: 79.98 time: 0.089
2026-01-05 16:45:48,819: Running test after training batch: 2500
2026-01-05 16:45:48,933: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:45:55,015: WER debug example
  GT : you can see the code at this point as well
  PR : toehold weihe contingencies the toehold hatlestad sparred sisley
2026-01-05 16:45:55,077: WER debug example
  GT : how does it keep the cost down
  PR : noteholders spas hitting skipping thus casters
2026-01-05 16:45:59,221: Val batch 2500: PER (avg): 0.4180 CTC Loss (avg): 45.3796 WER(1gram): 100.76% (n=64) time: 10.402
2026-01-05 16:45:59,221: WER lens: avg_true_words=6.16 avg_pred_words=5.67 max_pred_words=11
2026-01-05 16:45:59,222: t15.2023.08.13 val PER: 0.3836
2026-01-05 16:45:59,222: t15.2023.08.18 val PER: 0.3621
2026-01-05 16:45:59,222: t15.2023.08.20 val PER: 0.3614
2026-01-05 16:45:59,222: t15.2023.08.25 val PER: 0.3494
2026-01-05 16:45:59,222: t15.2023.08.27 val PER: 0.4341
2026-01-05 16:45:59,222: t15.2023.09.01 val PER: 0.3360
2026-01-05 16:45:59,222: t15.2023.09.03 val PER: 0.4287
2026-01-05 16:45:59,222: t15.2023.09.24 val PER: 0.3556
2026-01-05 16:45:59,222: t15.2023.09.29 val PER: 0.3682
2026-01-05 16:45:59,223: t15.2023.10.01 val PER: 0.4148
2026-01-05 16:45:59,223: t15.2023.10.06 val PER: 0.3455
2026-01-05 16:45:59,223: t15.2023.10.08 val PER: 0.4777
2026-01-05 16:45:59,223: t15.2023.10.13 val PER: 0.4903
2026-01-05 16:45:59,223: t15.2023.10.15 val PER: 0.4041
2026-01-05 16:45:59,223: t15.2023.10.20 val PER: 0.3658
2026-01-05 16:45:59,223: t15.2023.10.22 val PER: 0.3552
2026-01-05 16:45:59,223: t15.2023.11.03 val PER: 0.4077
2026-01-05 16:45:59,223: t15.2023.11.04 val PER: 0.1706
2026-01-05 16:45:59,224: t15.2023.11.17 val PER: 0.2488
2026-01-05 16:45:59,224: t15.2023.11.19 val PER: 0.2335
2026-01-05 16:45:59,224: t15.2023.11.26 val PER: 0.4725
2026-01-05 16:45:59,224: t15.2023.12.03 val PER: 0.4128
2026-01-05 16:45:59,224: t15.2023.12.08 val PER: 0.4121
2026-01-05 16:45:59,224: t15.2023.12.10 val PER: 0.3719
2026-01-05 16:45:59,224: t15.2023.12.17 val PER: 0.3721
2026-01-05 16:45:59,224: t15.2023.12.29 val PER: 0.4235
2026-01-05 16:45:59,224: t15.2024.02.25 val PER: 0.3624
2026-01-05 16:45:59,224: t15.2024.03.08 val PER: 0.4708
2026-01-05 16:45:59,224: t15.2024.03.15 val PER: 0.4278
2026-01-05 16:45:59,224: t15.2024.03.17 val PER: 0.4045
2026-01-05 16:45:59,224: t15.2024.05.10 val PER: 0.4220
2026-01-05 16:45:59,224: t15.2024.06.14 val PER: 0.4306
2026-01-05 16:45:59,224: t15.2024.07.19 val PER: 0.5326
2026-01-05 16:45:59,224: t15.2024.07.21 val PER: 0.3628
2026-01-05 16:45:59,225: t15.2024.07.28 val PER: 0.3971
2026-01-05 16:45:59,225: t15.2025.01.10 val PER: 0.6253
2026-01-05 16:45:59,225: t15.2025.01.12 val PER: 0.4904
2026-01-05 16:45:59,225: t15.2025.03.14 val PER: 0.6050
2026-01-05 16:45:59,225: t15.2025.03.16 val PER: 0.4738
2026-01-05 16:45:59,225: t15.2025.03.30 val PER: 0.6425
2026-01-05 16:45:59,225: t15.2025.04.13 val PER: 0.5235
2026-01-05 16:45:59,494: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_2500
2026-01-05 16:46:10,538: Train batch 2600: loss: 49.33 grad norm: 92.38 time: 0.093
2026-01-05 16:46:32,522: Train batch 2800: loss: 39.15 grad norm: 87.86 time: 0.145
2026-01-05 16:46:54,268: Train batch 3000: loss: 40.55 grad norm: 100.91 time: 0.144
2026-01-05 16:46:54,268: Running test after training batch: 3000
2026-01-05 16:46:54,374: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:47:00,132: WER debug example
  GT : you can see the code at this point as well
  PR : toehold hint season the toehold hatz zaucha pundit his hwe
2026-01-05 16:47:00,183: WER debug example
  GT : how does it keep the cost down
  PR : holds spas hitting heaping that squat hasids
2026-01-05 16:47:03,595: Val batch 3000: PER (avg): 0.4025 CTC Loss (avg): 42.9236 WER(1gram): 103.05% (n=64) time: 9.327
2026-01-05 16:47:03,596: WER lens: avg_true_words=6.16 avg_pred_words=6.25 max_pred_words=11
2026-01-05 16:47:03,596: t15.2023.08.13 val PER: 0.3721
2026-01-05 16:47:03,596: t15.2023.08.18 val PER: 0.3512
2026-01-05 16:47:03,596: t15.2023.08.20 val PER: 0.3439
2026-01-05 16:47:03,596: t15.2023.08.25 val PER: 0.3223
2026-01-05 16:47:03,596: t15.2023.08.27 val PER: 0.4244
2026-01-05 16:47:03,596: t15.2023.09.01 val PER: 0.3198
2026-01-05 16:47:03,596: t15.2023.09.03 val PER: 0.4382
2026-01-05 16:47:03,596: t15.2023.09.24 val PER: 0.3398
2026-01-05 16:47:03,596: t15.2023.09.29 val PER: 0.3567
2026-01-05 16:47:03,597: t15.2023.10.01 val PER: 0.4247
2026-01-05 16:47:03,597: t15.2023.10.06 val PER: 0.3348
2026-01-05 16:47:03,597: t15.2023.10.08 val PER: 0.4750
2026-01-05 16:47:03,597: t15.2023.10.13 val PER: 0.4701
2026-01-05 16:47:03,597: t15.2023.10.15 val PER: 0.3869
2026-01-05 16:47:03,597: t15.2023.10.20 val PER: 0.3557
2026-01-05 16:47:03,597: t15.2023.10.22 val PER: 0.3285
2026-01-05 16:47:03,597: t15.2023.11.03 val PER: 0.3853
2026-01-05 16:47:03,597: t15.2023.11.04 val PER: 0.1775
2026-01-05 16:47:03,597: t15.2023.11.17 val PER: 0.2348
2026-01-05 16:47:03,597: t15.2023.11.19 val PER: 0.2255
2026-01-05 16:47:03,597: t15.2023.11.26 val PER: 0.4449
2026-01-05 16:47:03,597: t15.2023.12.03 val PER: 0.3971
2026-01-05 16:47:03,597: t15.2023.12.08 val PER: 0.3755
2026-01-05 16:47:03,598: t15.2023.12.10 val PER: 0.3522
2026-01-05 16:47:03,598: t15.2023.12.17 val PER: 0.3586
2026-01-05 16:47:03,598: t15.2023.12.29 val PER: 0.4008
2026-01-05 16:47:03,598: t15.2024.02.25 val PER: 0.3638
2026-01-05 16:47:03,598: t15.2024.03.08 val PER: 0.4765
2026-01-05 16:47:03,598: t15.2024.03.15 val PER: 0.4153
2026-01-05 16:47:03,598: t15.2024.03.17 val PER: 0.4010
2026-01-05 16:47:03,598: t15.2024.05.10 val PER: 0.3863
2026-01-05 16:47:03,598: t15.2024.06.14 val PER: 0.4117
2026-01-05 16:47:03,598: t15.2024.07.19 val PER: 0.5082
2026-01-05 16:47:03,598: t15.2024.07.21 val PER: 0.3503
2026-01-05 16:47:03,598: t15.2024.07.28 val PER: 0.3779
2026-01-05 16:47:03,598: t15.2025.01.10 val PER: 0.6033
2026-01-05 16:47:03,598: t15.2025.01.12 val PER: 0.4765
2026-01-05 16:47:03,598: t15.2025.03.14 val PER: 0.5858
2026-01-05 16:47:03,599: t15.2025.03.16 val PER: 0.4594
2026-01-05 16:47:03,599: t15.2025.03.30 val PER: 0.6138
2026-01-05 16:47:03,599: t15.2025.04.13 val PER: 0.4979
2026-01-05 16:47:03,863: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_3000
2026-01-05 16:47:26,283: Train batch 3200: loss: 37.36 grad norm: 95.39 time: 0.132
2026-01-05 16:47:48,139: Train batch 3400: loss: 30.76 grad norm: 68.94 time: 0.083
2026-01-05 16:47:59,329: Running test after training batch: 3500
2026-01-05 16:47:59,442: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:48:05,173: WER debug example
  GT : you can see the code at this point as well
  PR : dulude hintze season sutton headhunters assistance points his huff
2026-01-05 16:48:05,229: WER debug example
  GT : how does it keep the cost down
  PR : diehards spies hitty hipkins that
2026-01-05 16:48:08,487: Val batch 3500: PER (avg): 0.3913 CTC Loss (avg): 40.1196 WER(1gram): 107.87% (n=64) time: 9.158
2026-01-05 16:48:08,488: WER lens: avg_true_words=6.16 avg_pred_words=6.62 max_pred_words=12
2026-01-05 16:48:08,488: t15.2023.08.13 val PER: 0.3586
2026-01-05 16:48:08,488: t15.2023.08.18 val PER: 0.3420
2026-01-05 16:48:08,488: t15.2023.08.20 val PER: 0.3376
2026-01-05 16:48:08,488: t15.2023.08.25 val PER: 0.3027
2026-01-05 16:48:08,488: t15.2023.08.27 val PER: 0.4244
2026-01-05 16:48:08,488: t15.2023.09.01 val PER: 0.3149
2026-01-05 16:48:08,488: t15.2023.09.03 val PER: 0.4109
2026-01-05 16:48:08,488: t15.2023.09.24 val PER: 0.3240
2026-01-05 16:48:08,489: t15.2023.09.29 val PER: 0.3478
2026-01-05 16:48:08,489: t15.2023.10.01 val PER: 0.4108
2026-01-05 16:48:08,489: t15.2023.10.06 val PER: 0.3122
2026-01-05 16:48:08,489: t15.2023.10.08 val PER: 0.4723
2026-01-05 16:48:08,489: t15.2023.10.13 val PER: 0.4694
2026-01-05 16:48:08,489: t15.2023.10.15 val PER: 0.3612
2026-01-05 16:48:08,489: t15.2023.10.20 val PER: 0.3624
2026-01-05 16:48:08,489: t15.2023.10.22 val PER: 0.3352
2026-01-05 16:48:08,489: t15.2023.11.03 val PER: 0.3684
2026-01-05 16:48:08,489: t15.2023.11.04 val PER: 0.1672
2026-01-05 16:48:08,489: t15.2023.11.17 val PER: 0.2177
2026-01-05 16:48:08,489: t15.2023.11.19 val PER: 0.2196
2026-01-05 16:48:08,489: t15.2023.11.26 val PER: 0.4290
2026-01-05 16:48:08,489: t15.2023.12.03 val PER: 0.3803
2026-01-05 16:48:08,489: t15.2023.12.08 val PER: 0.3702
2026-01-05 16:48:08,490: t15.2023.12.10 val PER: 0.3311
2026-01-05 16:48:08,490: t15.2023.12.17 val PER: 0.3669
2026-01-05 16:48:08,490: t15.2023.12.29 val PER: 0.3919
2026-01-05 16:48:08,490: t15.2024.02.25 val PER: 0.3343
2026-01-05 16:48:08,490: t15.2024.03.08 val PER: 0.4908
2026-01-05 16:48:08,490: t15.2024.03.15 val PER: 0.4040
2026-01-05 16:48:08,490: t15.2024.03.17 val PER: 0.3912
2026-01-05 16:48:08,490: t15.2024.05.10 val PER: 0.3893
2026-01-05 16:48:08,490: t15.2024.06.14 val PER: 0.4006
2026-01-05 16:48:08,490: t15.2024.07.19 val PER: 0.4904
2026-01-05 16:48:08,490: t15.2024.07.21 val PER: 0.3497
2026-01-05 16:48:08,490: t15.2024.07.28 val PER: 0.3860
2026-01-05 16:48:08,490: t15.2025.01.10 val PER: 0.5716
2026-01-05 16:48:08,490: t15.2025.01.12 val PER: 0.4665
2026-01-05 16:48:08,490: t15.2025.03.14 val PER: 0.5473
2026-01-05 16:48:08,490: t15.2025.03.16 val PER: 0.4555
2026-01-05 16:48:08,491: t15.2025.03.30 val PER: 0.5747
2026-01-05 16:48:08,491: t15.2025.04.13 val PER: 0.4807
2026-01-05 16:48:08,758: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_3500
2026-01-05 16:48:19,909: Train batch 3600: loss: 34.78 grad norm: 72.21 time: 0.115
2026-01-05 16:48:41,647: Train batch 3800: loss: 36.51 grad norm: 69.17 time: 0.115
2026-01-05 16:49:03,812: Train batch 4000: loss: 29.79 grad norm: 75.96 time: 0.096
2026-01-05 16:49:03,813: Running test after training batch: 4000
2026-01-05 16:49:03,927: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:49:10,087: WER debug example
  GT : you can see the code at this point as well
  PR : tuley duhe courtesies the toehold hottest happened his huff
2026-01-05 16:49:10,141: WER debug example
  GT : how does it keep the cost down
  PR : duhe ouzts does hitting hing the toasts
2026-01-05 16:49:13,167: Val batch 4000: PER (avg): 0.3622 CTC Loss (avg): 37.9119 WER(1gram): 106.09% (n=64) time: 9.354
2026-01-05 16:49:13,168: WER lens: avg_true_words=6.16 avg_pred_words=6.62 max_pred_words=12
2026-01-05 16:49:13,168: t15.2023.08.13 val PER: 0.3399
2026-01-05 16:49:13,168: t15.2023.08.18 val PER: 0.3202
2026-01-05 16:49:13,168: t15.2023.08.20 val PER: 0.3137
2026-01-05 16:49:13,168: t15.2023.08.25 val PER: 0.3163
2026-01-05 16:49:13,168: t15.2023.08.27 val PER: 0.4100
2026-01-05 16:49:13,168: t15.2023.09.01 val PER: 0.3028
2026-01-05 16:49:13,168: t15.2023.09.03 val PER: 0.3872
2026-01-05 16:49:13,168: t15.2023.09.24 val PER: 0.3228
2026-01-05 16:49:13,169: t15.2023.09.29 val PER: 0.3210
2026-01-05 16:49:13,169: t15.2023.10.01 val PER: 0.3765
2026-01-05 16:49:13,169: t15.2023.10.06 val PER: 0.2971
2026-01-05 16:49:13,169: t15.2023.10.08 val PER: 0.4425
2026-01-05 16:49:13,169: t15.2023.10.13 val PER: 0.4267
2026-01-05 16:49:13,169: t15.2023.10.15 val PER: 0.3441
2026-01-05 16:49:13,169: t15.2023.10.20 val PER: 0.3658
2026-01-05 16:49:13,169: t15.2023.10.22 val PER: 0.3051
2026-01-05 16:49:13,169: t15.2023.11.03 val PER: 0.3460
2026-01-05 16:49:13,169: t15.2023.11.04 val PER: 0.1502
2026-01-05 16:49:13,170: t15.2023.11.17 val PER: 0.2022
2026-01-05 16:49:13,170: t15.2023.11.19 val PER: 0.2236
2026-01-05 16:49:13,170: t15.2023.11.26 val PER: 0.4043
2026-01-05 16:49:13,170: t15.2023.12.03 val PER: 0.3414
2026-01-05 16:49:13,170: t15.2023.12.08 val PER: 0.3435
2026-01-05 16:49:13,170: t15.2023.12.10 val PER: 0.3022
2026-01-05 16:49:13,170: t15.2023.12.17 val PER: 0.3274
2026-01-05 16:49:13,170: t15.2023.12.29 val PER: 0.3535
2026-01-05 16:49:13,170: t15.2024.02.25 val PER: 0.3202
2026-01-05 16:49:13,170: t15.2024.03.08 val PER: 0.4381
2026-01-05 16:49:13,170: t15.2024.03.15 val PER: 0.3759
2026-01-05 16:49:13,170: t15.2024.03.17 val PER: 0.3647
2026-01-05 16:49:13,170: t15.2024.05.10 val PER: 0.3373
2026-01-05 16:49:13,170: t15.2024.06.14 val PER: 0.3454
2026-01-05 16:49:13,170: t15.2024.07.19 val PER: 0.4548
2026-01-05 16:49:13,170: t15.2024.07.21 val PER: 0.2993
2026-01-05 16:49:13,171: t15.2024.07.28 val PER: 0.3507
2026-01-05 16:49:13,171: t15.2025.01.10 val PER: 0.5220
2026-01-05 16:49:13,171: t15.2025.01.12 val PER: 0.4149
2026-01-05 16:49:13,171: t15.2025.03.14 val PER: 0.5089
2026-01-05 16:49:13,171: t15.2025.03.16 val PER: 0.4293
2026-01-05 16:49:13,171: t15.2025.03.30 val PER: 0.5264
2026-01-05 16:49:13,171: t15.2025.04.13 val PER: 0.4522
2026-01-05 16:49:13,442: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_4000
2026-01-05 16:49:35,730: Train batch 4200: loss: 35.13 grad norm: 84.36 time: 0.137
2026-01-05 16:49:57,546: Train batch 4400: loss: 29.07 grad norm: 69.05 time: 0.113
2026-01-05 16:50:08,379: Running test after training batch: 4500
2026-01-05 16:50:08,492: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:50:14,210: WER debug example
  GT : you can see the code at this point as well
  PR : toyoda hantz misa that headhunters systemhouse point his huff
2026-01-05 16:50:14,256: WER debug example
  GT : how does it keep the cost down
  PR : toehold zietz does hitty hecht the tvsat hasids
2026-01-05 16:50:17,089: Val batch 4500: PER (avg): 0.3531 CTC Loss (avg): 36.3234 WER(1gram): 110.15% (n=64) time: 8.710
2026-01-05 16:50:17,089: WER lens: avg_true_words=6.16 avg_pred_words=6.98 max_pred_words=14
2026-01-05 16:50:17,090: t15.2023.08.13 val PER: 0.3347
2026-01-05 16:50:17,090: t15.2023.08.18 val PER: 0.3152
2026-01-05 16:50:17,090: t15.2023.08.20 val PER: 0.3106
2026-01-05 16:50:17,090: t15.2023.08.25 val PER: 0.2801
2026-01-05 16:50:17,090: t15.2023.08.27 val PER: 0.3617
2026-01-05 16:50:17,090: t15.2023.09.01 val PER: 0.2938
2026-01-05 16:50:17,090: t15.2023.09.03 val PER: 0.3777
2026-01-05 16:50:17,090: t15.2023.09.24 val PER: 0.2949
2026-01-05 16:50:17,090: t15.2023.09.29 val PER: 0.3159
2026-01-05 16:50:17,090: t15.2023.10.01 val PER: 0.3639
2026-01-05 16:50:17,091: t15.2023.10.06 val PER: 0.2777
2026-01-05 16:50:17,091: t15.2023.10.08 val PER: 0.4235
2026-01-05 16:50:17,091: t15.2023.10.13 val PER: 0.4267
2026-01-05 16:50:17,091: t15.2023.10.15 val PER: 0.3579
2026-01-05 16:50:17,091: t15.2023.10.20 val PER: 0.3121
2026-01-05 16:50:17,091: t15.2023.10.22 val PER: 0.3007
2026-01-05 16:50:17,091: t15.2023.11.03 val PER: 0.3467
2026-01-05 16:50:17,091: t15.2023.11.04 val PER: 0.1331
2026-01-05 16:50:17,091: t15.2023.11.17 val PER: 0.1866
2026-01-05 16:50:17,091: t15.2023.11.19 val PER: 0.2056
2026-01-05 16:50:17,091: t15.2023.11.26 val PER: 0.3928
2026-01-05 16:50:17,091: t15.2023.12.03 val PER: 0.3351
2026-01-05 16:50:17,091: t15.2023.12.08 val PER: 0.3249
2026-01-05 16:50:17,092: t15.2023.12.10 val PER: 0.2891
2026-01-05 16:50:17,092: t15.2023.12.17 val PER: 0.3264
2026-01-05 16:50:17,092: t15.2023.12.29 val PER: 0.3418
2026-01-05 16:50:17,092: t15.2024.02.25 val PER: 0.2992
2026-01-05 16:50:17,092: t15.2024.03.08 val PER: 0.4211
2026-01-05 16:50:17,092: t15.2024.03.15 val PER: 0.3734
2026-01-05 16:50:17,092: t15.2024.03.17 val PER: 0.3556
2026-01-05 16:50:17,092: t15.2024.05.10 val PER: 0.3432
2026-01-05 16:50:17,092: t15.2024.06.14 val PER: 0.3438
2026-01-05 16:50:17,092: t15.2024.07.19 val PER: 0.4548
2026-01-05 16:50:17,092: t15.2024.07.21 val PER: 0.2917
2026-01-05 16:50:17,093: t15.2024.07.28 val PER: 0.3441
2026-01-05 16:50:17,093: t15.2025.01.10 val PER: 0.5165
2026-01-05 16:50:17,093: t15.2025.01.12 val PER: 0.4180
2026-01-05 16:50:17,093: t15.2025.03.14 val PER: 0.5000
2026-01-05 16:50:17,093: t15.2025.03.16 val PER: 0.3979
2026-01-05 16:50:17,093: t15.2025.03.30 val PER: 0.5207
2026-01-05 16:50:17,093: t15.2025.04.13 val PER: 0.4379
2026-01-05 16:50:17,367: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_4500
2026-01-05 16:50:28,597: Train batch 4600: loss: 35.52 grad norm: 112.77 time: 0.107
2026-01-05 16:50:50,974: Train batch 4800: loss: 25.59 grad norm: 87.76 time: 0.108
2026-01-05 16:51:12,891: Train batch 5000: loss: 45.78 grad norm: 97.35 time: 0.111
2026-01-05 16:51:12,892: Running test after training batch: 5000
2026-01-05 16:51:13,020: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:51:18,905: WER debug example
  GT : you can see the code at this point as well
  PR : tuten hantz ease the tahiti hatt cysts points his hwa
2026-01-05 16:51:18,955: WER debug example
  GT : how does it keep the cost down
  PR : toehold zietz dust hitty hecht the toasts
2026-01-05 16:51:22,501: Val batch 5000: PER (avg): 0.3381 CTC Loss (avg): 34.7924 WER(1gram): 107.11% (n=64) time: 9.608
2026-01-05 16:51:22,501: WER lens: avg_true_words=6.16 avg_pred_words=6.62 max_pred_words=14
2026-01-05 16:51:22,501: t15.2023.08.13 val PER: 0.3202
2026-01-05 16:51:22,502: t15.2023.08.18 val PER: 0.2984
2026-01-05 16:51:22,502: t15.2023.08.20 val PER: 0.3034
2026-01-05 16:51:22,502: t15.2023.08.25 val PER: 0.2636
2026-01-05 16:51:22,502: t15.2023.08.27 val PER: 0.3569
2026-01-05 16:51:22,502: t15.2023.09.01 val PER: 0.2752
2026-01-05 16:51:22,502: t15.2023.09.03 val PER: 0.3622
2026-01-05 16:51:22,502: t15.2023.09.24 val PER: 0.2888
2026-01-05 16:51:22,502: t15.2023.09.29 val PER: 0.3070
2026-01-05 16:51:22,502: t15.2023.10.01 val PER: 0.3560
2026-01-05 16:51:22,502: t15.2023.10.06 val PER: 0.2583
2026-01-05 16:51:22,503: t15.2023.10.08 val PER: 0.4100
2026-01-05 16:51:22,503: t15.2023.10.13 val PER: 0.4081
2026-01-05 16:51:22,503: t15.2023.10.15 val PER: 0.3296
2026-01-05 16:51:22,503: t15.2023.10.20 val PER: 0.3456
2026-01-05 16:51:22,503: t15.2023.10.22 val PER: 0.2862
2026-01-05 16:51:22,503: t15.2023.11.03 val PER: 0.3331
2026-01-05 16:51:22,503: t15.2023.11.04 val PER: 0.1331
2026-01-05 16:51:22,503: t15.2023.11.17 val PER: 0.1711
2026-01-05 16:51:22,503: t15.2023.11.19 val PER: 0.1936
2026-01-05 16:51:22,503: t15.2023.11.26 val PER: 0.3768
2026-01-05 16:51:22,503: t15.2023.12.03 val PER: 0.3351
2026-01-05 16:51:22,504: t15.2023.12.08 val PER: 0.3229
2026-01-05 16:51:22,504: t15.2023.12.10 val PER: 0.2799
2026-01-05 16:51:22,504: t15.2023.12.17 val PER: 0.2994
2026-01-05 16:51:22,504: t15.2023.12.29 val PER: 0.3342
2026-01-05 16:51:22,504: t15.2024.02.25 val PER: 0.3076
2026-01-05 16:51:22,504: t15.2024.03.08 val PER: 0.4253
2026-01-05 16:51:22,504: t15.2024.03.15 val PER: 0.3565
2026-01-05 16:51:22,504: t15.2024.03.17 val PER: 0.3466
2026-01-05 16:51:22,504: t15.2024.05.10 val PER: 0.3284
2026-01-05 16:51:22,504: t15.2024.06.14 val PER: 0.3107
2026-01-05 16:51:22,504: t15.2024.07.19 val PER: 0.4232
2026-01-05 16:51:22,504: t15.2024.07.21 val PER: 0.2676
2026-01-05 16:51:22,504: t15.2024.07.28 val PER: 0.3309
2026-01-05 16:51:22,504: t15.2025.01.10 val PER: 0.4945
2026-01-05 16:51:22,504: t15.2025.01.12 val PER: 0.3749
2026-01-05 16:51:22,505: t15.2025.03.14 val PER: 0.4956
2026-01-05 16:51:22,505: t15.2025.03.16 val PER: 0.3599
2026-01-05 16:51:22,505: t15.2025.03.30 val PER: 0.5023
2026-01-05 16:51:22,505: t15.2025.04.13 val PER: 0.4194
2026-01-05 16:51:22,791: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_5000
2026-01-05 16:51:44,821: Train batch 5200: loss: 30.30 grad norm: 93.02 time: 0.087
2026-01-05 16:52:06,647: Train batch 5400: loss: 28.93 grad norm: 71.90 time: 0.119
2026-01-05 16:52:17,619: Running test after training batch: 5500
2026-01-05 16:52:17,743: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:52:23,780: WER debug example
  GT : you can see the code at this point as well
  PR : tyus duhe kantz ease sutton headhunter systemhouse points ozanich
2026-01-05 16:52:23,838: WER debug example
  GT : how does it keep the cost down
  PR : toehold zietz just hitt hipkins the tvsat setzer
2026-01-05 16:52:27,790: Val batch 5500: PER (avg): 0.3296 CTC Loss (avg): 34.0988 WER(1gram): 109.14% (n=64) time: 10.170
2026-01-05 16:52:27,791: WER lens: avg_true_words=6.16 avg_pred_words=6.66 max_pred_words=12
2026-01-05 16:52:27,791: t15.2023.08.13 val PER: 0.3233
2026-01-05 16:52:27,791: t15.2023.08.18 val PER: 0.2942
2026-01-05 16:52:27,791: t15.2023.08.20 val PER: 0.3002
2026-01-05 16:52:27,791: t15.2023.08.25 val PER: 0.2651
2026-01-05 16:52:27,791: t15.2023.08.27 val PER: 0.3521
2026-01-05 16:52:27,792: t15.2023.09.01 val PER: 0.2679
2026-01-05 16:52:27,792: t15.2023.09.03 val PER: 0.3587
2026-01-05 16:52:27,792: t15.2023.09.24 val PER: 0.2852
2026-01-05 16:52:27,792: t15.2023.09.29 val PER: 0.2891
2026-01-05 16:52:27,792: t15.2023.10.01 val PER: 0.3322
2026-01-05 16:52:27,792: t15.2023.10.06 val PER: 0.2573
2026-01-05 16:52:27,792: t15.2023.10.08 val PER: 0.4141
2026-01-05 16:52:27,792: t15.2023.10.13 val PER: 0.3980
2026-01-05 16:52:27,792: t15.2023.10.15 val PER: 0.3210
2026-01-05 16:52:27,793: t15.2023.10.20 val PER: 0.3121
2026-01-05 16:52:27,793: t15.2023.10.22 val PER: 0.2817
2026-01-05 16:52:27,793: t15.2023.11.03 val PER: 0.3412
2026-01-05 16:52:27,793: t15.2023.11.04 val PER: 0.1365
2026-01-05 16:52:27,793: t15.2023.11.17 val PER: 0.1680
2026-01-05 16:52:27,793: t15.2023.11.19 val PER: 0.1936
2026-01-05 16:52:27,793: t15.2023.11.26 val PER: 0.3659
2026-01-05 16:52:27,793: t15.2023.12.03 val PER: 0.3162
2026-01-05 16:52:27,793: t15.2023.12.08 val PER: 0.3056
2026-01-05 16:52:27,793: t15.2023.12.10 val PER: 0.2668
2026-01-05 16:52:27,793: t15.2023.12.17 val PER: 0.2911
2026-01-05 16:52:27,793: t15.2023.12.29 val PER: 0.3356
2026-01-05 16:52:27,793: t15.2024.02.25 val PER: 0.2865
2026-01-05 16:52:27,793: t15.2024.03.08 val PER: 0.3983
2026-01-05 16:52:27,793: t15.2024.03.15 val PER: 0.3408
2026-01-05 16:52:27,793: t15.2024.03.17 val PER: 0.3312
2026-01-05 16:52:27,794: t15.2024.05.10 val PER: 0.3180
2026-01-05 16:52:27,794: t15.2024.06.14 val PER: 0.3139
2026-01-05 16:52:27,794: t15.2024.07.19 val PER: 0.4133
2026-01-05 16:52:27,794: t15.2024.07.21 val PER: 0.2566
2026-01-05 16:52:27,794: t15.2024.07.28 val PER: 0.3213
2026-01-05 16:52:27,794: t15.2025.01.10 val PER: 0.4835
2026-01-05 16:52:27,794: t15.2025.01.12 val PER: 0.3680
2026-01-05 16:52:27,794: t15.2025.03.14 val PER: 0.4675
2026-01-05 16:52:27,794: t15.2025.03.16 val PER: 0.3547
2026-01-05 16:52:27,795: t15.2025.03.30 val PER: 0.4989
2026-01-05 16:52:27,795: t15.2025.04.13 val PER: 0.4365
2026-01-05 16:52:28,078: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_5500
2026-01-05 16:52:39,302: Train batch 5600: loss: 32.62 grad norm: 85.25 time: 0.105
2026-01-05 16:53:01,357: Train batch 5800: loss: 24.31 grad norm: 83.91 time: 0.143
2026-01-05 16:53:23,330: Train batch 6000: loss: 22.82 grad norm: 68.94 time: 0.082
2026-01-05 16:53:23,330: Running test after training batch: 6000
2026-01-05 16:53:23,452: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:53:29,328: WER debug example
  GT : you can see the code at this point as well
  PR : skewed huck antsy zietz that heydt hatt cysts point his hwe
2026-01-05 16:53:29,386: WER debug example
  GT : how does it keep the cost down
  PR : toehold zietz dust hitt hecht the teast
2026-01-05 16:53:33,262: Val batch 6000: PER (avg): 0.3204 CTC Loss (avg): 32.9358 WER(1gram): 112.44% (n=64) time: 9.931
2026-01-05 16:53:33,262: WER lens: avg_true_words=6.16 avg_pred_words=7.03 max_pred_words=13
2026-01-05 16:53:33,263: t15.2023.08.13 val PER: 0.2973
2026-01-05 16:53:33,263: t15.2023.08.18 val PER: 0.2800
2026-01-05 16:53:33,263: t15.2023.08.20 val PER: 0.2772
2026-01-05 16:53:33,263: t15.2023.08.25 val PER: 0.2590
2026-01-05 16:53:33,263: t15.2023.08.27 val PER: 0.3569
2026-01-05 16:53:33,263: t15.2023.09.01 val PER: 0.2679
2026-01-05 16:53:33,263: t15.2023.09.03 val PER: 0.3409
2026-01-05 16:53:33,263: t15.2023.09.24 val PER: 0.2791
2026-01-05 16:53:33,263: t15.2023.09.29 val PER: 0.2872
2026-01-05 16:53:33,263: t15.2023.10.01 val PER: 0.3303
2026-01-05 16:53:33,264: t15.2023.10.06 val PER: 0.2551
2026-01-05 16:53:33,264: t15.2023.10.08 val PER: 0.4032
2026-01-05 16:53:33,264: t15.2023.10.13 val PER: 0.4050
2026-01-05 16:53:33,264: t15.2023.10.15 val PER: 0.3151
2026-01-05 16:53:33,264: t15.2023.10.20 val PER: 0.3188
2026-01-05 16:53:33,264: t15.2023.10.22 val PER: 0.2739
2026-01-05 16:53:33,264: t15.2023.11.03 val PER: 0.3263
2026-01-05 16:53:33,264: t15.2023.11.04 val PER: 0.1468
2026-01-05 16:53:33,264: t15.2023.11.17 val PER: 0.1649
2026-01-05 16:53:33,264: t15.2023.11.19 val PER: 0.1836
2026-01-05 16:53:33,264: t15.2023.11.26 val PER: 0.3522
2026-01-05 16:53:33,264: t15.2023.12.03 val PER: 0.3067
2026-01-05 16:53:33,264: t15.2023.12.08 val PER: 0.2989
2026-01-05 16:53:33,265: t15.2023.12.10 val PER: 0.2615
2026-01-05 16:53:33,265: t15.2023.12.17 val PER: 0.2879
2026-01-05 16:53:33,265: t15.2023.12.29 val PER: 0.3191
2026-01-05 16:53:33,265: t15.2024.02.25 val PER: 0.2809
2026-01-05 16:53:33,265: t15.2024.03.08 val PER: 0.3969
2026-01-05 16:53:33,265: t15.2024.03.15 val PER: 0.3471
2026-01-05 16:53:33,265: t15.2024.03.17 val PER: 0.3152
2026-01-05 16:53:33,265: t15.2024.05.10 val PER: 0.2972
2026-01-05 16:53:33,265: t15.2024.06.14 val PER: 0.3186
2026-01-05 16:53:33,265: t15.2024.07.19 val PER: 0.3982
2026-01-05 16:53:33,265: t15.2024.07.21 val PER: 0.2572
2026-01-05 16:53:33,265: t15.2024.07.28 val PER: 0.3007
2026-01-05 16:53:33,265: t15.2025.01.10 val PER: 0.4890
2026-01-05 16:53:33,265: t15.2025.01.12 val PER: 0.3518
2026-01-05 16:53:33,266: t15.2025.03.14 val PER: 0.4541
2026-01-05 16:53:33,266: t15.2025.03.16 val PER: 0.3534
2026-01-05 16:53:33,266: t15.2025.03.30 val PER: 0.4529
2026-01-05 16:53:33,266: t15.2025.04.13 val PER: 0.3980
2026-01-05 16:53:33,546: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_6000
2026-01-05 16:53:55,649: Train batch 6200: loss: 25.27 grad norm: 77.30 time: 0.121
2026-01-05 16:54:17,423: Train batch 6400: loss: 30.39 grad norm: 77.65 time: 0.106
2026-01-05 16:54:28,194: Running test after training batch: 6500
2026-01-05 16:54:28,301: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:54:34,170: WER debug example
  GT : you can see the code at this point as well
  PR : skewed huck antsy xanthe utt headhunter cysts points his huff
2026-01-05 16:54:34,218: WER debug example
  GT : how does it keep the cost down
  PR : toehold zietz dust hitt heaped the toasts
2026-01-05 16:54:36,921: Val batch 6500: PER (avg): 0.3077 CTC Loss (avg): 32.3012 WER(1gram): 112.94% (n=64) time: 8.727
2026-01-05 16:54:36,921: WER lens: avg_true_words=6.16 avg_pred_words=7.19 max_pred_words=13
2026-01-05 16:54:36,921: t15.2023.08.13 val PER: 0.2838
2026-01-05 16:54:36,921: t15.2023.08.18 val PER: 0.2758
2026-01-05 16:54:36,922: t15.2023.08.20 val PER: 0.2542
2026-01-05 16:54:36,922: t15.2023.08.25 val PER: 0.2349
2026-01-05 16:54:36,922: t15.2023.08.27 val PER: 0.3489
2026-01-05 16:54:36,922: t15.2023.09.01 val PER: 0.2459
2026-01-05 16:54:36,922: t15.2023.09.03 val PER: 0.3302
2026-01-05 16:54:36,922: t15.2023.09.24 val PER: 0.2549
2026-01-05 16:54:36,922: t15.2023.09.29 val PER: 0.2821
2026-01-05 16:54:36,922: t15.2023.10.01 val PER: 0.3164
2026-01-05 16:54:36,923: t15.2023.10.06 val PER: 0.2594
2026-01-05 16:54:36,923: t15.2023.10.08 val PER: 0.3938
2026-01-05 16:54:36,923: t15.2023.10.13 val PER: 0.3871
2026-01-05 16:54:36,923: t15.2023.10.15 val PER: 0.3098
2026-01-05 16:54:36,923: t15.2023.10.20 val PER: 0.2919
2026-01-05 16:54:36,923: t15.2023.10.22 val PER: 0.2595
2026-01-05 16:54:36,923: t15.2023.11.03 val PER: 0.3114
2026-01-05 16:54:36,923: t15.2023.11.04 val PER: 0.1468
2026-01-05 16:54:36,924: t15.2023.11.17 val PER: 0.1680
2026-01-05 16:54:36,924: t15.2023.11.19 val PER: 0.1856
2026-01-05 16:54:36,924: t15.2023.11.26 val PER: 0.3406
2026-01-05 16:54:36,924: t15.2023.12.03 val PER: 0.2868
2026-01-05 16:54:36,924: t15.2023.12.08 val PER: 0.2909
2026-01-05 16:54:36,924: t15.2023.12.10 val PER: 0.2562
2026-01-05 16:54:36,924: t15.2023.12.17 val PER: 0.2838
2026-01-05 16:54:36,924: t15.2023.12.29 val PER: 0.3047
2026-01-05 16:54:36,924: t15.2024.02.25 val PER: 0.2683
2026-01-05 16:54:36,925: t15.2024.03.08 val PER: 0.3741
2026-01-05 16:54:36,925: t15.2024.03.15 val PER: 0.3308
2026-01-05 16:54:36,925: t15.2024.03.17 val PER: 0.3089
2026-01-05 16:54:36,925: t15.2024.05.10 val PER: 0.2912
2026-01-05 16:54:36,925: t15.2024.06.14 val PER: 0.2965
2026-01-05 16:54:36,925: t15.2024.07.19 val PER: 0.3698
2026-01-05 16:54:36,925: t15.2024.07.21 val PER: 0.2393
2026-01-05 16:54:36,925: t15.2024.07.28 val PER: 0.3022
2026-01-05 16:54:36,925: t15.2025.01.10 val PER: 0.4628
2026-01-05 16:54:36,925: t15.2025.01.12 val PER: 0.3433
2026-01-05 16:54:36,925: t15.2025.03.14 val PER: 0.4305
2026-01-05 16:54:36,925: t15.2025.03.16 val PER: 0.3259
2026-01-05 16:54:36,925: t15.2025.03.30 val PER: 0.4437
2026-01-05 16:54:36,926: t15.2025.04.13 val PER: 0.3923
2026-01-05 16:54:37,193: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_6500
2026-01-05 16:54:47,839: Train batch 6600: loss: 19.11 grad norm: 53.30 time: 0.075
2026-01-05 16:55:09,621: Train batch 6800: loss: 29.47 grad norm: 77.11 time: 0.082
2026-01-05 16:55:31,471: Train batch 7000: loss: 26.33 grad norm: 93.39 time: 0.104
2026-01-05 16:55:31,471: Running test after training batch: 7000
2026-01-05 16:55:31,585: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:55:38,083: WER debug example
  GT : you can see the code at this point as well
  PR : skew duhe kent sies that heydt hatt cysts points his duhe
2026-01-05 16:55:38,132: WER debug example
  GT : how does it keep the cost down
  PR : toehold zietz just hitt heaped the teast setzer
2026-01-05 16:55:41,198: Val batch 7000: PER (avg): 0.2979 CTC Loss (avg): 31.2196 WER(1gram): 114.47% (n=64) time: 9.727
2026-01-05 16:55:41,199: WER lens: avg_true_words=6.16 avg_pred_words=7.25 max_pred_words=13
2026-01-05 16:55:41,199: t15.2023.08.13 val PER: 0.2827
2026-01-05 16:55:41,199: t15.2023.08.18 val PER: 0.2716
2026-01-05 16:55:41,199: t15.2023.08.20 val PER: 0.2653
2026-01-05 16:55:41,199: t15.2023.08.25 val PER: 0.2440
2026-01-05 16:55:41,200: t15.2023.08.27 val PER: 0.3296
2026-01-05 16:55:41,200: t15.2023.09.01 val PER: 0.2451
2026-01-05 16:55:41,200: t15.2023.09.03 val PER: 0.3183
2026-01-05 16:55:41,200: t15.2023.09.24 val PER: 0.2524
2026-01-05 16:55:41,200: t15.2023.09.29 val PER: 0.2687
2026-01-05 16:55:41,200: t15.2023.10.01 val PER: 0.3012
2026-01-05 16:55:41,200: t15.2023.10.06 val PER: 0.2433
2026-01-05 16:55:41,200: t15.2023.10.08 val PER: 0.3816
2026-01-05 16:55:41,201: t15.2023.10.13 val PER: 0.3732
2026-01-05 16:55:41,201: t15.2023.10.15 val PER: 0.2933
2026-01-05 16:55:41,201: t15.2023.10.20 val PER: 0.3020
2026-01-05 16:55:41,201: t15.2023.10.22 val PER: 0.2361
2026-01-05 16:55:41,201: t15.2023.11.03 val PER: 0.2910
2026-01-05 16:55:41,201: t15.2023.11.04 val PER: 0.1195
2026-01-05 16:55:41,201: t15.2023.11.17 val PER: 0.1462
2026-01-05 16:55:41,201: t15.2023.11.19 val PER: 0.1796
2026-01-05 16:55:41,201: t15.2023.11.26 val PER: 0.3225
2026-01-05 16:55:41,201: t15.2023.12.03 val PER: 0.2773
2026-01-05 16:55:41,202: t15.2023.12.08 val PER: 0.2703
2026-01-05 16:55:41,202: t15.2023.12.10 val PER: 0.2497
2026-01-05 16:55:41,202: t15.2023.12.17 val PER: 0.2661
2026-01-05 16:55:41,202: t15.2023.12.29 val PER: 0.2979
2026-01-05 16:55:41,202: t15.2024.02.25 val PER: 0.2598
2026-01-05 16:55:41,202: t15.2024.03.08 val PER: 0.3613
2026-01-05 16:55:41,202: t15.2024.03.15 val PER: 0.3296
2026-01-05 16:55:41,202: t15.2024.03.17 val PER: 0.3040
2026-01-05 16:55:41,202: t15.2024.05.10 val PER: 0.2779
2026-01-05 16:55:41,202: t15.2024.06.14 val PER: 0.2823
2026-01-05 16:55:41,203: t15.2024.07.19 val PER: 0.3711
2026-01-05 16:55:41,203: t15.2024.07.21 val PER: 0.2338
2026-01-05 16:55:41,203: t15.2024.07.28 val PER: 0.2794
2026-01-05 16:55:41,203: t15.2025.01.10 val PER: 0.4490
2026-01-05 16:55:41,203: t15.2025.01.12 val PER: 0.3256
2026-01-05 16:55:41,203: t15.2025.03.14 val PER: 0.4423
2026-01-05 16:55:41,203: t15.2025.03.16 val PER: 0.3259
2026-01-05 16:55:41,203: t15.2025.03.30 val PER: 0.4471
2026-01-05 16:55:41,204: t15.2025.04.13 val PER: 0.3766
2026-01-05 16:55:41,481: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_7000
2026-01-05 16:56:03,586: Train batch 7200: loss: 25.68 grad norm: 78.39 time: 0.135
2026-01-05 16:56:25,053: Train batch 7400: loss: 25.76 grad norm: 81.56 time: 0.131
2026-01-05 16:56:35,790: Running test after training batch: 7500
2026-01-05 16:56:35,910: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:56:41,817: WER debug example
  GT : you can see the code at this point as well
  PR : skew duhe kantz ease the tuthill daihatsu cysts points his hwe
2026-01-05 16:56:41,870: WER debug example
  GT : how does it keep the cost down
  PR : zeit holdouts dust hitt hecht the toste setzer
2026-01-05 16:56:44,746: Val batch 7500: PER (avg): 0.2914 CTC Loss (avg): 30.4712 WER(1gram): 111.17% (n=64) time: 8.955
2026-01-05 16:56:44,747: WER lens: avg_true_words=6.16 avg_pred_words=7.05 max_pred_words=13
2026-01-05 16:56:44,747: t15.2023.08.13 val PER: 0.2661
2026-01-05 16:56:44,748: t15.2023.08.18 val PER: 0.2515
2026-01-05 16:56:44,748: t15.2023.08.20 val PER: 0.2431
2026-01-05 16:56:44,748: t15.2023.08.25 val PER: 0.2259
2026-01-05 16:56:44,748: t15.2023.08.27 val PER: 0.3376
2026-01-05 16:56:44,748: t15.2023.09.01 val PER: 0.2435
2026-01-05 16:56:44,748: t15.2023.09.03 val PER: 0.3230
2026-01-05 16:56:44,748: t15.2023.09.24 val PER: 0.2621
2026-01-05 16:56:44,748: t15.2023.09.29 val PER: 0.2719
2026-01-05 16:56:44,748: t15.2023.10.01 val PER: 0.2972
2026-01-05 16:56:44,749: t15.2023.10.06 val PER: 0.2379
2026-01-05 16:56:44,749: t15.2023.10.08 val PER: 0.3708
2026-01-05 16:56:44,749: t15.2023.10.13 val PER: 0.3677
2026-01-05 16:56:44,749: t15.2023.10.15 val PER: 0.2980
2026-01-05 16:56:44,749: t15.2023.10.20 val PER: 0.2752
2026-01-05 16:56:44,749: t15.2023.10.22 val PER: 0.2316
2026-01-05 16:56:44,749: t15.2023.11.03 val PER: 0.3060
2026-01-05 16:56:44,749: t15.2023.11.04 val PER: 0.1331
2026-01-05 16:56:44,749: t15.2023.11.17 val PER: 0.1555
2026-01-05 16:56:44,749: t15.2023.11.19 val PER: 0.1677
2026-01-05 16:56:44,749: t15.2023.11.26 val PER: 0.3203
2026-01-05 16:56:44,749: t15.2023.12.03 val PER: 0.2710
2026-01-05 16:56:44,750: t15.2023.12.08 val PER: 0.2543
2026-01-05 16:56:44,750: t15.2023.12.10 val PER: 0.2365
2026-01-05 16:56:44,750: t15.2023.12.17 val PER: 0.2599
2026-01-05 16:56:44,750: t15.2023.12.29 val PER: 0.2835
2026-01-05 16:56:44,750: t15.2024.02.25 val PER: 0.2570
2026-01-05 16:56:44,750: t15.2024.03.08 val PER: 0.3613
2026-01-05 16:56:44,750: t15.2024.03.15 val PER: 0.3221
2026-01-05 16:56:44,750: t15.2024.03.17 val PER: 0.2901
2026-01-05 16:56:44,750: t15.2024.05.10 val PER: 0.2719
2026-01-05 16:56:44,750: t15.2024.06.14 val PER: 0.2886
2026-01-05 16:56:44,750: t15.2024.07.19 val PER: 0.3672
2026-01-05 16:56:44,750: t15.2024.07.21 val PER: 0.2166
2026-01-05 16:56:44,750: t15.2024.07.28 val PER: 0.2794
2026-01-05 16:56:44,750: t15.2025.01.10 val PER: 0.4408
2026-01-05 16:56:44,750: t15.2025.01.12 val PER: 0.3149
2026-01-05 16:56:44,751: t15.2025.03.14 val PER: 0.4127
2026-01-05 16:56:44,751: t15.2025.03.16 val PER: 0.3154
2026-01-05 16:56:44,751: t15.2025.03.30 val PER: 0.4253
2026-01-05 16:56:44,751: t15.2025.04.13 val PER: 0.3723
2026-01-05 16:56:45,035: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_7500
2026-01-05 16:56:56,245: Train batch 7600: loss: 26.36 grad norm: 88.12 time: 0.118
2026-01-05 16:57:18,368: Train batch 7800: loss: 26.89 grad norm: 84.89 time: 0.094
2026-01-05 16:57:40,874: Train batch 8000: loss: 23.93 grad norm: 71.74 time: 0.125
2026-01-05 16:57:40,874: Running test after training batch: 8000
2026-01-05 16:57:40,989: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:57:46,891: WER debug example
  GT : you can see the code at this point as well
  PR : skew duhe kantz ease the tadych colt hatt cysts points his ruhe
2026-01-05 16:57:46,940: WER debug example
  GT : how does it keep the cost down
  PR : toehold eats dust hitt hix the toasts
2026-01-05 16:57:50,082: Val batch 8000: PER (avg): 0.2827 CTC Loss (avg): 29.4851 WER(1gram): 109.90% (n=64) time: 9.207
2026-01-05 16:57:50,082: WER lens: avg_true_words=6.16 avg_pred_words=6.91 max_pred_words=13
2026-01-05 16:57:50,082: t15.2023.08.13 val PER: 0.2578
2026-01-05 16:57:50,083: t15.2023.08.18 val PER: 0.2523
2026-01-05 16:57:50,083: t15.2023.08.20 val PER: 0.2431
2026-01-05 16:57:50,083: t15.2023.08.25 val PER: 0.2380
2026-01-05 16:57:50,083: t15.2023.08.27 val PER: 0.3312
2026-01-05 16:57:50,083: t15.2023.09.01 val PER: 0.2273
2026-01-05 16:57:50,083: t15.2023.09.03 val PER: 0.3005
2026-01-05 16:57:50,084: t15.2023.09.24 val PER: 0.2439
2026-01-05 16:57:50,084: t15.2023.09.29 val PER: 0.2546
2026-01-05 16:57:50,084: t15.2023.10.01 val PER: 0.3032
2026-01-05 16:57:50,084: t15.2023.10.06 val PER: 0.2304
2026-01-05 16:57:50,084: t15.2023.10.08 val PER: 0.3654
2026-01-05 16:57:50,084: t15.2023.10.13 val PER: 0.3623
2026-01-05 16:57:50,084: t15.2023.10.15 val PER: 0.2828
2026-01-05 16:57:50,084: t15.2023.10.20 val PER: 0.2886
2026-01-05 16:57:50,084: t15.2023.10.22 val PER: 0.2339
2026-01-05 16:57:50,085: t15.2023.11.03 val PER: 0.2965
2026-01-05 16:57:50,085: t15.2023.11.04 val PER: 0.1297
2026-01-05 16:57:50,085: t15.2023.11.17 val PER: 0.1400
2026-01-05 16:57:50,085: t15.2023.11.19 val PER: 0.1577
2026-01-05 16:57:50,085: t15.2023.11.26 val PER: 0.3130
2026-01-05 16:57:50,085: t15.2023.12.03 val PER: 0.2542
2026-01-05 16:57:50,085: t15.2023.12.08 val PER: 0.2570
2026-01-05 16:57:50,085: t15.2023.12.10 val PER: 0.2392
2026-01-05 16:57:50,085: t15.2023.12.17 val PER: 0.2536
2026-01-05 16:57:50,085: t15.2023.12.29 val PER: 0.2670
2026-01-05 16:57:50,086: t15.2024.02.25 val PER: 0.2289
2026-01-05 16:57:50,086: t15.2024.03.08 val PER: 0.3400
2026-01-05 16:57:50,086: t15.2024.03.15 val PER: 0.3058
2026-01-05 16:57:50,086: t15.2024.03.17 val PER: 0.2817
2026-01-05 16:57:50,086: t15.2024.05.10 val PER: 0.2675
2026-01-05 16:57:50,086: t15.2024.06.14 val PER: 0.2666
2026-01-05 16:57:50,086: t15.2024.07.19 val PER: 0.3408
2026-01-05 16:57:50,087: t15.2024.07.21 val PER: 0.2152
2026-01-05 16:57:50,087: t15.2024.07.28 val PER: 0.2559
2026-01-05 16:57:50,087: t15.2025.01.10 val PER: 0.4325
2026-01-05 16:57:50,087: t15.2025.01.12 val PER: 0.3187
2026-01-05 16:57:50,087: t15.2025.03.14 val PER: 0.4186
2026-01-05 16:57:50,087: t15.2025.03.16 val PER: 0.3037
2026-01-05 16:57:50,087: t15.2025.03.30 val PER: 0.4287
2026-01-05 16:57:50,087: t15.2025.04.13 val PER: 0.3666
2026-01-05 16:57:50,401: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_8000
2026-01-05 16:58:12,451: Train batch 8200: loss: 20.20 grad norm: 71.34 time: 0.091
2026-01-05 16:58:34,320: Train batch 8400: loss: 20.84 grad norm: 79.56 time: 0.111
2026-01-05 16:58:45,601: Running test after training batch: 8500
2026-01-05 16:58:45,720: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:58:51,565: WER debug example
  GT : you can see the code at this point as well
  PR : sputum huck antsy xanthe utt heydt hatt systemhouse points his hwe
2026-01-05 16:58:51,615: WER debug example
  GT : how does it keep the cost down
  PR : zietz dust hitt hix thus quast setzer
2026-01-05 16:58:54,688: Val batch 8500: PER (avg): 0.2798 CTC Loss (avg): 29.1305 WER(1gram): 112.18% (n=64) time: 9.087
2026-01-05 16:58:54,688: WER lens: avg_true_words=6.16 avg_pred_words=7.11 max_pred_words=14
2026-01-05 16:58:54,689: t15.2023.08.13 val PER: 0.2620
2026-01-05 16:58:54,689: t15.2023.08.18 val PER: 0.2414
2026-01-05 16:58:54,689: t15.2023.08.20 val PER: 0.2407
2026-01-05 16:58:54,689: t15.2023.08.25 val PER: 0.2123
2026-01-05 16:58:54,690: t15.2023.08.27 val PER: 0.3183
2026-01-05 16:58:54,690: t15.2023.09.01 val PER: 0.2208
2026-01-05 16:58:54,690: t15.2023.09.03 val PER: 0.2886
2026-01-05 16:58:54,690: t15.2023.09.24 val PER: 0.2464
2026-01-05 16:58:54,690: t15.2023.09.29 val PER: 0.2629
2026-01-05 16:58:54,690: t15.2023.10.01 val PER: 0.3025
2026-01-05 16:58:54,690: t15.2023.10.06 val PER: 0.2185
2026-01-05 16:58:54,690: t15.2023.10.08 val PER: 0.3613
2026-01-05 16:58:54,690: t15.2023.10.13 val PER: 0.3638
2026-01-05 16:58:54,690: t15.2023.10.15 val PER: 0.2848
2026-01-05 16:58:54,691: t15.2023.10.20 val PER: 0.2785
2026-01-05 16:58:54,691: t15.2023.10.22 val PER: 0.2316
2026-01-05 16:58:54,691: t15.2023.11.03 val PER: 0.2972
2026-01-05 16:58:54,691: t15.2023.11.04 val PER: 0.1263
2026-01-05 16:58:54,691: t15.2023.11.17 val PER: 0.1322
2026-01-05 16:58:54,691: t15.2023.11.19 val PER: 0.1457
2026-01-05 16:58:54,691: t15.2023.11.26 val PER: 0.3043
2026-01-05 16:58:54,691: t15.2023.12.03 val PER: 0.2563
2026-01-05 16:58:54,691: t15.2023.12.08 val PER: 0.2523
2026-01-05 16:58:54,692: t15.2023.12.10 val PER: 0.2300
2026-01-05 16:58:54,692: t15.2023.12.17 val PER: 0.2464
2026-01-05 16:58:54,692: t15.2023.12.29 val PER: 0.2690
2026-01-05 16:58:54,692: t15.2024.02.25 val PER: 0.2528
2026-01-05 16:58:54,692: t15.2024.03.08 val PER: 0.3343
2026-01-05 16:58:54,692: t15.2024.03.15 val PER: 0.2996
2026-01-05 16:58:54,692: t15.2024.03.17 val PER: 0.2720
2026-01-05 16:58:54,692: t15.2024.05.10 val PER: 0.2645
2026-01-05 16:58:54,693: t15.2024.06.14 val PER: 0.2571
2026-01-05 16:58:54,693: t15.2024.07.19 val PER: 0.3448
2026-01-05 16:58:54,693: t15.2024.07.21 val PER: 0.2069
2026-01-05 16:58:54,693: t15.2024.07.28 val PER: 0.2625
2026-01-05 16:58:54,693: t15.2025.01.10 val PER: 0.4311
2026-01-05 16:58:54,693: t15.2025.01.12 val PER: 0.3118
2026-01-05 16:58:54,693: t15.2025.03.14 val PER: 0.4098
2026-01-05 16:58:54,693: t15.2025.03.16 val PER: 0.3272
2026-01-05 16:58:54,693: t15.2025.03.30 val PER: 0.4264
2026-01-05 16:58:54,693: t15.2025.04.13 val PER: 0.3552
2026-01-05 16:58:54,973: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_8500
2026-01-05 16:59:05,742: Train batch 8600: loss: 27.47 grad norm: 71.90 time: 0.093
2026-01-05 16:59:27,720: Train batch 8800: loss: 28.12 grad norm: 92.72 time: 0.102
2026-01-05 16:59:49,865: Train batch 9000: loss: 29.70 grad norm: 96.47 time: 0.127
2026-01-05 16:59:49,866: Running test after training batch: 9000
2026-01-05 16:59:49,983: WER debug GT example: You can see the code at this point as well.
2026-01-05 16:59:55,854: WER debug example
  GT : you can see the code at this point as well
  PR : skew duhe kantz ease the tadych could hatt cysts points his hwa
2026-01-05 16:59:55,898: WER debug example
  GT : how does it keep the cost down
  PR : zeit hodes suss hitt heap coots the teast setzer
2026-01-05 16:59:58,402: Val batch 9000: PER (avg): 0.2750 CTC Loss (avg): 28.5495 WER(1gram): 114.21% (n=64) time: 8.536
2026-01-05 16:59:58,402: WER lens: avg_true_words=6.16 avg_pred_words=7.36 max_pred_words=14
2026-01-05 16:59:58,403: t15.2023.08.13 val PER: 0.2505
2026-01-05 16:59:58,403: t15.2023.08.18 val PER: 0.2422
2026-01-05 16:59:58,403: t15.2023.08.20 val PER: 0.2399
2026-01-05 16:59:58,403: t15.2023.08.25 val PER: 0.2184
2026-01-05 16:59:58,403: t15.2023.08.27 val PER: 0.3103
2026-01-05 16:59:58,403: t15.2023.09.01 val PER: 0.2265
2026-01-05 16:59:58,403: t15.2023.09.03 val PER: 0.3052
2026-01-05 16:59:58,403: t15.2023.09.24 val PER: 0.2330
2026-01-05 16:59:58,403: t15.2023.09.29 val PER: 0.2534
2026-01-05 16:59:58,403: t15.2023.10.01 val PER: 0.2847
2026-01-05 16:59:58,403: t15.2023.10.06 val PER: 0.2131
2026-01-05 16:59:58,403: t15.2023.10.08 val PER: 0.3735
2026-01-05 16:59:58,403: t15.2023.10.13 val PER: 0.3499
2026-01-05 16:59:58,404: t15.2023.10.15 val PER: 0.2808
2026-01-05 16:59:58,404: t15.2023.10.20 val PER: 0.2852
2026-01-05 16:59:58,404: t15.2023.10.22 val PER: 0.2327
2026-01-05 16:59:58,404: t15.2023.11.03 val PER: 0.2836
2026-01-05 16:59:58,404: t15.2023.11.04 val PER: 0.1195
2026-01-05 16:59:58,404: t15.2023.11.17 val PER: 0.1369
2026-01-05 16:59:58,404: t15.2023.11.19 val PER: 0.1497
2026-01-05 16:59:58,404: t15.2023.11.26 val PER: 0.2957
2026-01-05 16:59:58,404: t15.2023.12.03 val PER: 0.2532
2026-01-05 16:59:58,404: t15.2023.12.08 val PER: 0.2470
2026-01-05 16:59:58,404: t15.2023.12.10 val PER: 0.2286
2026-01-05 16:59:58,404: t15.2023.12.17 val PER: 0.2245
2026-01-05 16:59:58,404: t15.2023.12.29 val PER: 0.2704
2026-01-05 16:59:58,404: t15.2024.02.25 val PER: 0.2360
2026-01-05 16:59:58,405: t15.2024.03.08 val PER: 0.3357
2026-01-05 16:59:58,405: t15.2024.03.15 val PER: 0.2983
2026-01-05 16:59:58,405: t15.2024.03.17 val PER: 0.2734
2026-01-05 16:59:58,405: t15.2024.05.10 val PER: 0.2541
2026-01-05 16:59:58,405: t15.2024.06.14 val PER: 0.2555
2026-01-05 16:59:58,405: t15.2024.07.19 val PER: 0.3303
2026-01-05 16:59:58,405: t15.2024.07.21 val PER: 0.2083
2026-01-05 16:59:58,405: t15.2024.07.28 val PER: 0.2559
2026-01-05 16:59:58,405: t15.2025.01.10 val PER: 0.4201
2026-01-05 16:59:58,405: t15.2025.01.12 val PER: 0.3149
2026-01-05 16:59:58,405: t15.2025.03.14 val PER: 0.4024
2026-01-05 16:59:58,406: t15.2025.03.16 val PER: 0.3128
2026-01-05 16:59:58,406: t15.2025.03.30 val PER: 0.4103
2026-01-05 16:59:58,406: t15.2025.04.13 val PER: 0.3581
2026-01-05 16:59:58,680: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_9000
2026-01-05 17:00:20,735: Train batch 9200: loss: 18.99 grad norm: 63.51 time: 0.096
2026-01-05 17:00:42,590: Train batch 9400: loss: 18.63 grad norm: 71.19 time: 0.116
2026-01-05 17:00:53,317: Running test after training batch: 9500
2026-01-05 17:00:53,428: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:00:59,116: WER debug example
  GT : you can see the code at this point as well
  PR : skew duhe kantz ease the tadych colt hatz this tadych points his duhe
2026-01-05 17:00:59,158: WER debug example
  GT : how does it keep the cost down
  PR : zeit holdouts dust hitt heap koos the toasts
2026-01-05 17:01:01,621: Val batch 9500: PER (avg): 0.2742 CTC Loss (avg): 28.4807 WER(1gram): 118.02% (n=64) time: 8.303
2026-01-05 17:01:01,622: WER lens: avg_true_words=6.16 avg_pred_words=7.61 max_pred_words=14
2026-01-05 17:01:01,622: t15.2023.08.13 val PER: 0.2620
2026-01-05 17:01:01,622: t15.2023.08.18 val PER: 0.2389
2026-01-05 17:01:01,622: t15.2023.08.20 val PER: 0.2383
2026-01-05 17:01:01,622: t15.2023.08.25 val PER: 0.2139
2026-01-05 17:01:01,622: t15.2023.08.27 val PER: 0.3215
2026-01-05 17:01:01,622: t15.2023.09.01 val PER: 0.2232
2026-01-05 17:01:01,622: t15.2023.09.03 val PER: 0.3017
2026-01-05 17:01:01,622: t15.2023.09.24 val PER: 0.2282
2026-01-05 17:01:01,623: t15.2023.09.29 val PER: 0.2412
2026-01-05 17:01:01,623: t15.2023.10.01 val PER: 0.2959
2026-01-05 17:01:01,623: t15.2023.10.06 val PER: 0.2142
2026-01-05 17:01:01,623: t15.2023.10.08 val PER: 0.3613
2026-01-05 17:01:01,623: t15.2023.10.13 val PER: 0.3507
2026-01-05 17:01:01,623: t15.2023.10.15 val PER: 0.2848
2026-01-05 17:01:01,623: t15.2023.10.20 val PER: 0.2987
2026-01-05 17:01:01,623: t15.2023.10.22 val PER: 0.2327
2026-01-05 17:01:01,623: t15.2023.11.03 val PER: 0.2829
2026-01-05 17:01:01,624: t15.2023.11.04 val PER: 0.1126
2026-01-05 17:01:01,624: t15.2023.11.17 val PER: 0.1244
2026-01-05 17:01:01,624: t15.2023.11.19 val PER: 0.1577
2026-01-05 17:01:01,624: t15.2023.11.26 val PER: 0.2978
2026-01-05 17:01:01,624: t15.2023.12.03 val PER: 0.2500
2026-01-05 17:01:01,624: t15.2023.12.08 val PER: 0.2443
2026-01-05 17:01:01,624: t15.2023.12.10 val PER: 0.2260
2026-01-05 17:01:01,624: t15.2023.12.17 val PER: 0.2464
2026-01-05 17:01:01,624: t15.2023.12.29 val PER: 0.2690
2026-01-05 17:01:01,624: t15.2024.02.25 val PER: 0.2346
2026-01-05 17:01:01,624: t15.2024.03.08 val PER: 0.3329
2026-01-05 17:01:01,624: t15.2024.03.15 val PER: 0.2958
2026-01-05 17:01:01,624: t15.2024.03.17 val PER: 0.2685
2026-01-05 17:01:01,625: t15.2024.05.10 val PER: 0.2793
2026-01-05 17:01:01,625: t15.2024.06.14 val PER: 0.2539
2026-01-05 17:01:01,625: t15.2024.07.19 val PER: 0.3276
2026-01-05 17:01:01,625: t15.2024.07.21 val PER: 0.1972
2026-01-05 17:01:01,625: t15.2024.07.28 val PER: 0.2522
2026-01-05 17:01:01,625: t15.2025.01.10 val PER: 0.4229
2026-01-05 17:01:01,625: t15.2025.01.12 val PER: 0.3002
2026-01-05 17:01:01,625: t15.2025.03.14 val PER: 0.3920
2026-01-05 17:01:01,625: t15.2025.03.16 val PER: 0.3194
2026-01-05 17:01:01,625: t15.2025.03.30 val PER: 0.4126
2026-01-05 17:01:01,626: t15.2025.04.13 val PER: 0.3623
2026-01-05 17:01:01,895: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_9500
2026-01-05 17:01:12,570: Train batch 9600: loss: 18.54 grad norm: 65.80 time: 0.127
2026-01-05 17:01:34,699: Train batch 9800: loss: 23.98 grad norm: 77.65 time: 0.107
2026-01-05 17:01:56,938: Train batch 10000: loss: 12.95 grad norm: 56.90 time: 0.104
2026-01-05 17:01:56,938: Running test after training batch: 10000
2026-01-05 17:01:57,046: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:02:02,751: WER debug example
  GT : you can see the code at this point as well
  PR : skewed huck antsy teethe utt ahead hatt cysts points his ruhe
2026-01-05 17:02:02,797: WER debug example
  GT : how does it keep the cost down
  PR : zeit holdouts dusty hitt heeke smoothest setzer
2026-01-05 17:02:05,351: Val batch 10000: PER (avg): 0.2704 CTC Loss (avg): 28.2801 WER(1gram): 116.50% (n=64) time: 8.413
2026-01-05 17:02:05,352: WER lens: avg_true_words=6.16 avg_pred_words=7.45 max_pred_words=14
2026-01-05 17:02:05,352: t15.2023.08.13 val PER: 0.2588
2026-01-05 17:02:05,352: t15.2023.08.18 val PER: 0.2481
2026-01-05 17:02:05,353: t15.2023.08.20 val PER: 0.2423
2026-01-05 17:02:05,353: t15.2023.08.25 val PER: 0.2184
2026-01-05 17:02:05,353: t15.2023.08.27 val PER: 0.3055
2026-01-05 17:02:05,353: t15.2023.09.01 val PER: 0.2086
2026-01-05 17:02:05,353: t15.2023.09.03 val PER: 0.2910
2026-01-05 17:02:05,353: t15.2023.09.24 val PER: 0.2257
2026-01-05 17:02:05,353: t15.2023.09.29 val PER: 0.2476
2026-01-05 17:02:05,353: t15.2023.10.01 val PER: 0.2873
2026-01-05 17:02:05,353: t15.2023.10.06 val PER: 0.2260
2026-01-05 17:02:05,353: t15.2023.10.08 val PER: 0.3572
2026-01-05 17:02:05,353: t15.2023.10.13 val PER: 0.3445
2026-01-05 17:02:05,353: t15.2023.10.15 val PER: 0.2762
2026-01-05 17:02:05,353: t15.2023.10.20 val PER: 0.2718
2026-01-05 17:02:05,353: t15.2023.10.22 val PER: 0.2183
2026-01-05 17:02:05,354: t15.2023.11.03 val PER: 0.2741
2026-01-05 17:02:05,354: t15.2023.11.04 val PER: 0.1126
2026-01-05 17:02:05,354: t15.2023.11.17 val PER: 0.1275
2026-01-05 17:02:05,354: t15.2023.11.19 val PER: 0.1397
2026-01-05 17:02:05,354: t15.2023.11.26 val PER: 0.2928
2026-01-05 17:02:05,354: t15.2023.12.03 val PER: 0.2626
2026-01-05 17:02:05,354: t15.2023.12.08 val PER: 0.2383
2026-01-05 17:02:05,354: t15.2023.12.10 val PER: 0.2116
2026-01-05 17:02:05,354: t15.2023.12.17 val PER: 0.2339
2026-01-05 17:02:05,354: t15.2023.12.29 val PER: 0.2649
2026-01-05 17:02:05,354: t15.2024.02.25 val PER: 0.2317
2026-01-05 17:02:05,354: t15.2024.03.08 val PER: 0.3385
2026-01-05 17:02:05,354: t15.2024.03.15 val PER: 0.3008
2026-01-05 17:02:05,354: t15.2024.03.17 val PER: 0.2629
2026-01-05 17:02:05,355: t15.2024.05.10 val PER: 0.2719
2026-01-05 17:02:05,355: t15.2024.06.14 val PER: 0.2681
2026-01-05 17:02:05,355: t15.2024.07.19 val PER: 0.3243
2026-01-05 17:02:05,355: t15.2024.07.21 val PER: 0.1917
2026-01-05 17:02:05,355: t15.2024.07.28 val PER: 0.2559
2026-01-05 17:02:05,355: t15.2025.01.10 val PER: 0.4105
2026-01-05 17:02:05,355: t15.2025.01.12 val PER: 0.2933
2026-01-05 17:02:05,355: t15.2025.03.14 val PER: 0.4186
2026-01-05 17:02:05,355: t15.2025.03.16 val PER: 0.3089
2026-01-05 17:02:05,355: t15.2025.03.30 val PER: 0.3805
2026-01-05 17:02:05,355: t15.2025.04.13 val PER: 0.3538
2026-01-05 17:02:05,639: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_10000
2026-01-05 17:02:28,878: Train batch 10200: loss: 16.83 grad norm: 62.52 time: 0.085
2026-01-05 17:02:50,960: Train batch 10400: loss: 20.85 grad norm: 80.13 time: 0.124
2026-01-05 17:03:01,858: Running test after training batch: 10500
2026-01-05 17:03:01,979: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:03:08,316: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eked the sta heydt hatt cysts points his ruhe
2026-01-05 17:03:08,362: WER debug example
  GT : how does it keep the cost down
  PR : zeit hurts dust hitt heaped the toasts
2026-01-05 17:03:10,926: Val batch 10500: PER (avg): 0.2642 CTC Loss (avg): 27.5794 WER(1gram): 114.21% (n=64) time: 9.068
2026-01-05 17:03:10,927: WER lens: avg_true_words=6.16 avg_pred_words=7.30 max_pred_words=13
2026-01-05 17:03:10,927: t15.2023.08.13 val PER: 0.2484
2026-01-05 17:03:10,927: t15.2023.08.18 val PER: 0.2422
2026-01-05 17:03:10,927: t15.2023.08.20 val PER: 0.2311
2026-01-05 17:03:10,927: t15.2023.08.25 val PER: 0.2048
2026-01-05 17:03:10,927: t15.2023.08.27 val PER: 0.3006
2026-01-05 17:03:10,927: t15.2023.09.01 val PER: 0.2094
2026-01-05 17:03:10,928: t15.2023.09.03 val PER: 0.2933
2026-01-05 17:03:10,928: t15.2023.09.24 val PER: 0.2269
2026-01-05 17:03:10,928: t15.2023.09.29 val PER: 0.2451
2026-01-05 17:03:10,928: t15.2023.10.01 val PER: 0.2807
2026-01-05 17:03:10,928: t15.2023.10.06 val PER: 0.2034
2026-01-05 17:03:10,929: t15.2023.10.08 val PER: 0.3464
2026-01-05 17:03:10,929: t15.2023.10.13 val PER: 0.3344
2026-01-05 17:03:10,929: t15.2023.10.15 val PER: 0.2729
2026-01-05 17:03:10,929: t15.2023.10.20 val PER: 0.2685
2026-01-05 17:03:10,929: t15.2023.10.22 val PER: 0.2105
2026-01-05 17:03:10,929: t15.2023.11.03 val PER: 0.2788
2026-01-05 17:03:10,929: t15.2023.11.04 val PER: 0.0956
2026-01-05 17:03:10,929: t15.2023.11.17 val PER: 0.1198
2026-01-05 17:03:10,929: t15.2023.11.19 val PER: 0.1417
2026-01-05 17:03:10,929: t15.2023.11.26 val PER: 0.2862
2026-01-05 17:03:10,929: t15.2023.12.03 val PER: 0.2426
2026-01-05 17:03:10,929: t15.2023.12.08 val PER: 0.2463
2026-01-05 17:03:10,929: t15.2023.12.10 val PER: 0.2234
2026-01-05 17:03:10,929: t15.2023.12.17 val PER: 0.2235
2026-01-05 17:03:10,929: t15.2023.12.29 val PER: 0.2505
2026-01-05 17:03:10,929: t15.2024.02.25 val PER: 0.2247
2026-01-05 17:03:10,929: t15.2024.03.08 val PER: 0.3201
2026-01-05 17:03:10,930: t15.2024.03.15 val PER: 0.2977
2026-01-05 17:03:10,930: t15.2024.03.17 val PER: 0.2636
2026-01-05 17:03:10,930: t15.2024.05.10 val PER: 0.2541
2026-01-05 17:03:10,930: t15.2024.06.14 val PER: 0.2539
2026-01-05 17:03:10,930: t15.2024.07.19 val PER: 0.3204
2026-01-05 17:03:10,930: t15.2024.07.21 val PER: 0.1910
2026-01-05 17:03:10,931: t15.2024.07.28 val PER: 0.2412
2026-01-05 17:03:10,931: t15.2025.01.10 val PER: 0.4050
2026-01-05 17:03:10,931: t15.2025.01.12 val PER: 0.2948
2026-01-05 17:03:10,931: t15.2025.03.14 val PER: 0.4068
2026-01-05 17:03:10,931: t15.2025.03.16 val PER: 0.2814
2026-01-05 17:03:10,931: t15.2025.03.30 val PER: 0.3770
2026-01-05 17:03:10,931: t15.2025.04.13 val PER: 0.3452
2026-01-05 17:03:11,202: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_10500
2026-01-05 17:03:22,419: Train batch 10600: loss: 18.82 grad norm: 84.14 time: 0.125
2026-01-05 17:03:44,258: Train batch 10800: loss: 26.14 grad norm: 90.22 time: 0.111
2026-01-05 17:04:05,944: Train batch 11000: loss: 27.83 grad norm: 90.85 time: 0.096
2026-01-05 17:04:05,944: Running test after training batch: 11000
2026-01-05 17:04:06,052: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:04:11,826: WER debug example
  GT : you can see the code at this point as well
  PR : skew duhe kantz eked ertha tadych code hatt cysts points his ruhe
2026-01-05 17:04:11,889: WER debug example
  GT : how does it keep the cost down
  PR : teast holdouts dust hitt heaped the toasts
2026-01-05 17:04:15,515: Val batch 11000: PER (avg): 0.2607 CTC Loss (avg): 27.2177 WER(1gram): 115.99% (n=64) time: 9.571
2026-01-05 17:04:15,516: WER lens: avg_true_words=6.16 avg_pred_words=7.55 max_pred_words=13
2026-01-05 17:04:15,516: t15.2023.08.13 val PER: 0.2380
2026-01-05 17:04:15,516: t15.2023.08.18 val PER: 0.2246
2026-01-05 17:04:15,516: t15.2023.08.20 val PER: 0.2280
2026-01-05 17:04:15,517: t15.2023.08.25 val PER: 0.1988
2026-01-05 17:04:15,517: t15.2023.08.27 val PER: 0.3039
2026-01-05 17:04:15,517: t15.2023.09.01 val PER: 0.1989
2026-01-05 17:04:15,517: t15.2023.09.03 val PER: 0.2957
2026-01-05 17:04:15,517: t15.2023.09.24 val PER: 0.2306
2026-01-05 17:04:15,517: t15.2023.09.29 val PER: 0.2393
2026-01-05 17:04:15,517: t15.2023.10.01 val PER: 0.2761
2026-01-05 17:04:15,517: t15.2023.10.06 val PER: 0.2110
2026-01-05 17:04:15,518: t15.2023.10.08 val PER: 0.3545
2026-01-05 17:04:15,518: t15.2023.10.13 val PER: 0.3437
2026-01-05 17:04:15,518: t15.2023.10.15 val PER: 0.2676
2026-01-05 17:04:15,518: t15.2023.10.20 val PER: 0.2685
2026-01-05 17:04:15,518: t15.2023.10.22 val PER: 0.2049
2026-01-05 17:04:15,518: t15.2023.11.03 val PER: 0.2714
2026-01-05 17:04:15,518: t15.2023.11.04 val PER: 0.1058
2026-01-05 17:04:15,519: t15.2023.11.17 val PER: 0.1229
2026-01-05 17:04:15,519: t15.2023.11.19 val PER: 0.1397
2026-01-05 17:04:15,519: t15.2023.11.26 val PER: 0.2783
2026-01-05 17:04:15,519: t15.2023.12.03 val PER: 0.2468
2026-01-05 17:04:15,519: t15.2023.12.08 val PER: 0.2364
2026-01-05 17:04:15,519: t15.2023.12.10 val PER: 0.2116
2026-01-05 17:04:15,519: t15.2023.12.17 val PER: 0.2308
2026-01-05 17:04:15,519: t15.2023.12.29 val PER: 0.2546
2026-01-05 17:04:15,519: t15.2024.02.25 val PER: 0.2177
2026-01-05 17:04:15,520: t15.2024.03.08 val PER: 0.3371
2026-01-05 17:04:15,520: t15.2024.03.15 val PER: 0.2808
2026-01-05 17:04:15,520: t15.2024.03.17 val PER: 0.2483
2026-01-05 17:04:15,520: t15.2024.05.10 val PER: 0.2660
2026-01-05 17:04:15,520: t15.2024.06.14 val PER: 0.2492
2026-01-05 17:04:15,520: t15.2024.07.19 val PER: 0.3191
2026-01-05 17:04:15,520: t15.2024.07.21 val PER: 0.1848
2026-01-05 17:04:15,520: t15.2024.07.28 val PER: 0.2390
2026-01-05 17:04:15,520: t15.2025.01.10 val PER: 0.4091
2026-01-05 17:04:15,521: t15.2025.01.12 val PER: 0.2818
2026-01-05 17:04:15,521: t15.2025.03.14 val PER: 0.3950
2026-01-05 17:04:15,521: t15.2025.03.16 val PER: 0.2880
2026-01-05 17:04:15,521: t15.2025.03.30 val PER: 0.3851
2026-01-05 17:04:15,521: t15.2025.04.13 val PER: 0.3352
2026-01-05 17:04:15,812: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_11000
2026-01-05 17:04:37,697: Train batch 11200: loss: 19.58 grad norm: 75.58 time: 0.123
2026-01-05 17:04:59,074: Train batch 11400: loss: 19.58 grad norm: 95.70 time: 0.097
2026-01-05 17:05:10,038: Running test after training batch: 11500
2026-01-05 17:05:10,160: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:05:15,634: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eaters the stuccoed hatt cysts points his hoff
2026-01-05 17:05:15,678: WER debug example
  GT : how does it keep the cost down
  PR : zeit holdouts dust hitt heaped thus rost setzer
2026-01-05 17:05:18,205: Val batch 11500: PER (avg): 0.2559 CTC Loss (avg): 27.0672 WER(1gram): 112.94% (n=64) time: 8.166
2026-01-05 17:05:18,205: WER lens: avg_true_words=6.16 avg_pred_words=7.14 max_pred_words=13
2026-01-05 17:05:18,205: t15.2023.08.13 val PER: 0.2277
2026-01-05 17:05:18,205: t15.2023.08.18 val PER: 0.2246
2026-01-05 17:05:18,205: t15.2023.08.20 val PER: 0.2192
2026-01-05 17:05:18,206: t15.2023.08.25 val PER: 0.2033
2026-01-05 17:05:18,206: t15.2023.08.27 val PER: 0.3055
2026-01-05 17:05:18,206: t15.2023.09.01 val PER: 0.2070
2026-01-05 17:05:18,206: t15.2023.09.03 val PER: 0.2886
2026-01-05 17:05:18,206: t15.2023.09.24 val PER: 0.2221
2026-01-05 17:05:18,206: t15.2023.09.29 val PER: 0.2304
2026-01-05 17:05:18,206: t15.2023.10.01 val PER: 0.2774
2026-01-05 17:05:18,206: t15.2023.10.06 val PER: 0.2110
2026-01-05 17:05:18,206: t15.2023.10.08 val PER: 0.3275
2026-01-05 17:05:18,206: t15.2023.10.13 val PER: 0.3390
2026-01-05 17:05:18,206: t15.2023.10.15 val PER: 0.2630
2026-01-05 17:05:18,206: t15.2023.10.20 val PER: 0.2651
2026-01-05 17:05:18,206: t15.2023.10.22 val PER: 0.2071
2026-01-05 17:05:18,207: t15.2023.11.03 val PER: 0.2734
2026-01-05 17:05:18,207: t15.2023.11.04 val PER: 0.1024
2026-01-05 17:05:18,207: t15.2023.11.17 val PER: 0.1337
2026-01-05 17:05:18,207: t15.2023.11.19 val PER: 0.1357
2026-01-05 17:05:18,207: t15.2023.11.26 val PER: 0.2812
2026-01-05 17:05:18,207: t15.2023.12.03 val PER: 0.2384
2026-01-05 17:05:18,207: t15.2023.12.08 val PER: 0.2410
2026-01-05 17:05:18,207: t15.2023.12.10 val PER: 0.2037
2026-01-05 17:05:18,207: t15.2023.12.17 val PER: 0.2079
2026-01-05 17:05:18,207: t15.2023.12.29 val PER: 0.2457
2026-01-05 17:05:18,207: t15.2024.02.25 val PER: 0.2135
2026-01-05 17:05:18,207: t15.2024.03.08 val PER: 0.3243
2026-01-05 17:05:18,207: t15.2024.03.15 val PER: 0.2764
2026-01-05 17:05:18,207: t15.2024.03.17 val PER: 0.2545
2026-01-05 17:05:18,207: t15.2024.05.10 val PER: 0.2571
2026-01-05 17:05:18,207: t15.2024.06.14 val PER: 0.2508
2026-01-05 17:05:18,208: t15.2024.07.19 val PER: 0.3059
2026-01-05 17:05:18,208: t15.2024.07.21 val PER: 0.1793
2026-01-05 17:05:18,208: t15.2024.07.28 val PER: 0.2279
2026-01-05 17:05:18,208: t15.2025.01.10 val PER: 0.3981
2026-01-05 17:05:18,208: t15.2025.01.12 val PER: 0.2748
2026-01-05 17:05:18,208: t15.2025.03.14 val PER: 0.3817
2026-01-05 17:05:18,209: t15.2025.03.16 val PER: 0.2866
2026-01-05 17:05:18,209: t15.2025.03.30 val PER: 0.3609
2026-01-05 17:05:18,209: t15.2025.04.13 val PER: 0.3352
2026-01-05 17:05:18,478: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_11500
2026-01-05 17:05:28,936: Train batch 11600: loss: 22.70 grad norm: 78.18 time: 0.103
2026-01-05 17:05:50,643: Train batch 11800: loss: 17.73 grad norm: 59.42 time: 0.075
2026-01-05 17:06:12,363: Train batch 12000: loss: 23.04 grad norm: 71.18 time: 0.125
2026-01-05 17:06:12,363: Running test after training batch: 12000
2026-01-05 17:06:12,465: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:06:17,939: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eaters the tadych kludt hatt cysts points his ruhe
2026-01-05 17:06:17,984: WER debug example
  GT : how does it keep the cost down
  PR : stockholm dietz dust hitt heaped the teast setzer
2026-01-05 17:06:20,491: Val batch 12000: PER (avg): 0.2546 CTC Loss (avg): 26.4990 WER(1gram): 110.66% (n=64) time: 8.128
2026-01-05 17:06:20,492: WER lens: avg_true_words=6.16 avg_pred_words=7.22 max_pred_words=14
2026-01-05 17:06:20,492: t15.2023.08.13 val PER: 0.2412
2026-01-05 17:06:20,492: t15.2023.08.18 val PER: 0.2272
2026-01-05 17:06:20,492: t15.2023.08.20 val PER: 0.2240
2026-01-05 17:06:20,492: t15.2023.08.25 val PER: 0.2063
2026-01-05 17:06:20,492: t15.2023.08.27 val PER: 0.2862
2026-01-05 17:06:20,492: t15.2023.09.01 val PER: 0.1940
2026-01-05 17:06:20,492: t15.2023.09.03 val PER: 0.2827
2026-01-05 17:06:20,493: t15.2023.09.24 val PER: 0.2160
2026-01-05 17:06:20,493: t15.2023.09.29 val PER: 0.2329
2026-01-05 17:06:20,493: t15.2023.10.01 val PER: 0.2754
2026-01-05 17:06:20,493: t15.2023.10.06 val PER: 0.2045
2026-01-05 17:06:20,493: t15.2023.10.08 val PER: 0.3491
2026-01-05 17:06:20,493: t15.2023.10.13 val PER: 0.3328
2026-01-05 17:06:20,493: t15.2023.10.15 val PER: 0.2637
2026-01-05 17:06:20,493: t15.2023.10.20 val PER: 0.2651
2026-01-05 17:06:20,493: t15.2023.10.22 val PER: 0.2060
2026-01-05 17:06:20,493: t15.2023.11.03 val PER: 0.2673
2026-01-05 17:06:20,493: t15.2023.11.04 val PER: 0.1058
2026-01-05 17:06:20,493: t15.2023.11.17 val PER: 0.1229
2026-01-05 17:06:20,493: t15.2023.11.19 val PER: 0.1317
2026-01-05 17:06:20,493: t15.2023.11.26 val PER: 0.2783
2026-01-05 17:06:20,494: t15.2023.12.03 val PER: 0.2437
2026-01-05 17:06:20,494: t15.2023.12.08 val PER: 0.2310
2026-01-05 17:06:20,494: t15.2023.12.10 val PER: 0.2102
2026-01-05 17:06:20,494: t15.2023.12.17 val PER: 0.2121
2026-01-05 17:06:20,494: t15.2023.12.29 val PER: 0.2485
2026-01-05 17:06:20,494: t15.2024.02.25 val PER: 0.2191
2026-01-05 17:06:20,494: t15.2024.03.08 val PER: 0.3001
2026-01-05 17:06:20,494: t15.2024.03.15 val PER: 0.2795
2026-01-05 17:06:20,494: t15.2024.03.17 val PER: 0.2469
2026-01-05 17:06:20,494: t15.2024.05.10 val PER: 0.2526
2026-01-05 17:06:20,494: t15.2024.06.14 val PER: 0.2524
2026-01-05 17:06:20,494: t15.2024.07.19 val PER: 0.3019
2026-01-05 17:06:20,494: t15.2024.07.21 val PER: 0.1731
2026-01-05 17:06:20,494: t15.2024.07.28 val PER: 0.2331
2026-01-05 17:06:20,494: t15.2025.01.10 val PER: 0.4077
2026-01-05 17:06:20,494: t15.2025.01.12 val PER: 0.2764
2026-01-05 17:06:20,494: t15.2025.03.14 val PER: 0.3891
2026-01-05 17:06:20,495: t15.2025.03.16 val PER: 0.2723
2026-01-05 17:06:20,495: t15.2025.03.30 val PER: 0.3701
2026-01-05 17:06:20,495: t15.2025.04.13 val PER: 0.3295
2026-01-05 17:06:20,781: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_12000
2026-01-05 17:06:42,466: Train batch 12200: loss: 15.74 grad norm: 63.63 time: 0.115
2026-01-05 17:07:04,123: Train batch 12400: loss: 11.81 grad norm: 41.86 time: 0.068
2026-01-05 17:07:15,337: Running test after training batch: 12500
2026-01-05 17:07:15,441: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:07:20,893: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eaters the tadych goodtab hatt cysts points his ruhe
2026-01-05 17:07:20,941: WER debug example
  GT : how does it keep the cost down
  PR : east houtz dust hitt heaped the toasts
2026-01-05 17:07:23,461: Val batch 12500: PER (avg): 0.2526 CTC Loss (avg): 26.4029 WER(1gram): 109.90% (n=64) time: 8.123
2026-01-05 17:07:23,461: WER lens: avg_true_words=6.16 avg_pred_words=7.03 max_pred_words=13
2026-01-05 17:07:23,462: t15.2023.08.13 val PER: 0.2422
2026-01-05 17:07:23,462: t15.2023.08.18 val PER: 0.2221
2026-01-05 17:07:23,462: t15.2023.08.20 val PER: 0.2129
2026-01-05 17:07:23,462: t15.2023.08.25 val PER: 0.2033
2026-01-05 17:07:23,462: t15.2023.08.27 val PER: 0.2781
2026-01-05 17:07:23,462: t15.2023.09.01 val PER: 0.1916
2026-01-05 17:07:23,462: t15.2023.09.03 val PER: 0.2838
2026-01-05 17:07:23,462: t15.2023.09.24 val PER: 0.2112
2026-01-05 17:07:23,462: t15.2023.09.29 val PER: 0.2323
2026-01-05 17:07:23,462: t15.2023.10.01 val PER: 0.2715
2026-01-05 17:07:23,463: t15.2023.10.06 val PER: 0.1991
2026-01-05 17:07:23,463: t15.2023.10.08 val PER: 0.3356
2026-01-05 17:07:23,463: t15.2023.10.13 val PER: 0.3344
2026-01-05 17:07:23,463: t15.2023.10.15 val PER: 0.2610
2026-01-05 17:07:23,463: t15.2023.10.20 val PER: 0.2550
2026-01-05 17:07:23,463: t15.2023.10.22 val PER: 0.1938
2026-01-05 17:07:23,463: t15.2023.11.03 val PER: 0.2707
2026-01-05 17:07:23,463: t15.2023.11.04 val PER: 0.0990
2026-01-05 17:07:23,463: t15.2023.11.17 val PER: 0.1229
2026-01-05 17:07:23,463: t15.2023.11.19 val PER: 0.1437
2026-01-05 17:07:23,463: t15.2023.11.26 val PER: 0.2819
2026-01-05 17:07:23,463: t15.2023.12.03 val PER: 0.2374
2026-01-05 17:07:23,463: t15.2023.12.08 val PER: 0.2204
2026-01-05 17:07:23,463: t15.2023.12.10 val PER: 0.2050
2026-01-05 17:07:23,464: t15.2023.12.17 val PER: 0.2173
2026-01-05 17:07:23,464: t15.2023.12.29 val PER: 0.2471
2026-01-05 17:07:23,464: t15.2024.02.25 val PER: 0.2219
2026-01-05 17:07:23,464: t15.2024.03.08 val PER: 0.3129
2026-01-05 17:07:23,464: t15.2024.03.15 val PER: 0.2758
2026-01-05 17:07:23,464: t15.2024.03.17 val PER: 0.2413
2026-01-05 17:07:23,464: t15.2024.05.10 val PER: 0.2541
2026-01-05 17:07:23,464: t15.2024.06.14 val PER: 0.2303
2026-01-05 17:07:23,464: t15.2024.07.19 val PER: 0.2993
2026-01-05 17:07:23,464: t15.2024.07.21 val PER: 0.1662
2026-01-05 17:07:23,464: t15.2024.07.28 val PER: 0.2346
2026-01-05 17:07:23,464: t15.2025.01.10 val PER: 0.4118
2026-01-05 17:07:23,464: t15.2025.01.12 val PER: 0.2802
2026-01-05 17:07:23,464: t15.2025.03.14 val PER: 0.3905
2026-01-05 17:07:23,464: t15.2025.03.16 val PER: 0.2840
2026-01-05 17:07:23,464: t15.2025.03.30 val PER: 0.3667
2026-01-05 17:07:23,465: t15.2025.04.13 val PER: 0.3395
2026-01-05 17:07:23,735: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_12500
2026-01-05 17:07:34,601: Train batch 12600: loss: 16.86 grad norm: 60.80 time: 0.098
2026-01-05 17:07:57,083: Train batch 12800: loss: 15.78 grad norm: 64.42 time: 0.090
2026-01-05 17:08:19,344: Train batch 13000: loss: 15.49 grad norm: 81.41 time: 0.114
2026-01-05 17:08:19,345: Running test after training batch: 13000
2026-01-05 17:08:19,445: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:08:25,077: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz ease the tadych colt hatt cysts points his ruhe
2026-01-05 17:08:25,122: WER debug example
  GT : how does it keep the cost down
  PR : eased hurts dust hitt heaped thus costs
2026-01-05 17:08:27,778: Val batch 13000: PER (avg): 0.2498 CTC Loss (avg): 26.2718 WER(1gram): 114.47% (n=64) time: 8.432
2026-01-05 17:08:27,778: WER lens: avg_true_words=6.16 avg_pred_words=7.36 max_pred_words=14
2026-01-05 17:08:27,778: t15.2023.08.13 val PER: 0.2308
2026-01-05 17:08:27,779: t15.2023.08.18 val PER: 0.2255
2026-01-05 17:08:27,779: t15.2023.08.20 val PER: 0.2176
2026-01-05 17:08:27,779: t15.2023.08.25 val PER: 0.1943
2026-01-05 17:08:27,779: t15.2023.08.27 val PER: 0.2878
2026-01-05 17:08:27,779: t15.2023.09.01 val PER: 0.1891
2026-01-05 17:08:27,779: t15.2023.09.03 val PER: 0.2862
2026-01-05 17:08:27,780: t15.2023.09.24 val PER: 0.2100
2026-01-05 17:08:27,780: t15.2023.09.29 val PER: 0.2265
2026-01-05 17:08:27,780: t15.2023.10.01 val PER: 0.2675
2026-01-05 17:08:27,780: t15.2023.10.06 val PER: 0.1981
2026-01-05 17:08:27,780: t15.2023.10.08 val PER: 0.3329
2026-01-05 17:08:27,780: t15.2023.10.13 val PER: 0.3204
2026-01-05 17:08:27,780: t15.2023.10.15 val PER: 0.2610
2026-01-05 17:08:27,780: t15.2023.10.20 val PER: 0.2450
2026-01-05 17:08:27,780: t15.2023.10.22 val PER: 0.1982
2026-01-05 17:08:27,780: t15.2023.11.03 val PER: 0.2673
2026-01-05 17:08:27,780: t15.2023.11.04 val PER: 0.1024
2026-01-05 17:08:27,781: t15.2023.11.17 val PER: 0.1198
2026-01-05 17:08:27,781: t15.2023.11.19 val PER: 0.1277
2026-01-05 17:08:27,781: t15.2023.11.26 val PER: 0.2754
2026-01-05 17:08:27,781: t15.2023.12.03 val PER: 0.2384
2026-01-05 17:08:27,781: t15.2023.12.08 val PER: 0.2244
2026-01-05 17:08:27,781: t15.2023.12.10 val PER: 0.2050
2026-01-05 17:08:27,781: t15.2023.12.17 val PER: 0.2069
2026-01-05 17:08:27,781: t15.2023.12.29 val PER: 0.2443
2026-01-05 17:08:27,781: t15.2024.02.25 val PER: 0.2135
2026-01-05 17:08:27,781: t15.2024.03.08 val PER: 0.3272
2026-01-05 17:08:27,781: t15.2024.03.15 val PER: 0.2714
2026-01-05 17:08:27,781: t15.2024.03.17 val PER: 0.2517
2026-01-05 17:08:27,781: t15.2024.05.10 val PER: 0.2585
2026-01-05 17:08:27,781: t15.2024.06.14 val PER: 0.2350
2026-01-05 17:08:27,781: t15.2024.07.19 val PER: 0.2966
2026-01-05 17:08:27,781: t15.2024.07.21 val PER: 0.1703
2026-01-05 17:08:27,782: t15.2024.07.28 val PER: 0.2184
2026-01-05 17:08:27,782: t15.2025.01.10 val PER: 0.3884
2026-01-05 17:08:27,782: t15.2025.01.12 val PER: 0.2610
2026-01-05 17:08:27,782: t15.2025.03.14 val PER: 0.3905
2026-01-05 17:08:27,782: t15.2025.03.16 val PER: 0.2814
2026-01-05 17:08:27,782: t15.2025.03.30 val PER: 0.3770
2026-01-05 17:08:27,782: t15.2025.04.13 val PER: 0.3267
2026-01-05 17:08:28,051: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_13000
2026-01-05 17:08:50,220: Train batch 13200: loss: 26.15 grad norm: 88.76 time: 0.095
2026-01-05 17:09:11,917: Train batch 13400: loss: 20.32 grad norm: 85.78 time: 0.106
2026-01-05 17:09:22,541: Running test after training batch: 13500
2026-01-05 17:09:22,654: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:09:28,327: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eaters the tadych code hatz this toughs points his ruhe
2026-01-05 17:09:28,426: WER debug example
  GT : how does it keep the cost down
  PR : zeit houtz dust hitt heaped thus cost setzer
2026-01-05 17:09:32,007: Val batch 13500: PER (avg): 0.2480 CTC Loss (avg): 25.8995 WER(1gram): 115.48% (n=64) time: 9.464
2026-01-05 17:09:32,007: WER lens: avg_true_words=6.16 avg_pred_words=7.47 max_pred_words=14
2026-01-05 17:09:32,008: t15.2023.08.13 val PER: 0.2297
2026-01-05 17:09:32,008: t15.2023.08.18 val PER: 0.2213
2026-01-05 17:09:32,008: t15.2023.08.20 val PER: 0.2129
2026-01-05 17:09:32,008: t15.2023.08.25 val PER: 0.1928
2026-01-05 17:09:32,008: t15.2023.08.27 val PER: 0.2990
2026-01-05 17:09:32,008: t15.2023.09.01 val PER: 0.1859
2026-01-05 17:09:32,008: t15.2023.09.03 val PER: 0.2732
2026-01-05 17:09:32,009: t15.2023.09.24 val PER: 0.2124
2026-01-05 17:09:32,009: t15.2023.09.29 val PER: 0.2272
2026-01-05 17:09:32,009: t15.2023.10.01 val PER: 0.2662
2026-01-05 17:09:32,009: t15.2023.10.06 val PER: 0.2024
2026-01-05 17:09:32,009: t15.2023.10.08 val PER: 0.3329
2026-01-05 17:09:32,009: t15.2023.10.13 val PER: 0.3382
2026-01-05 17:09:32,009: t15.2023.10.15 val PER: 0.2571
2026-01-05 17:09:32,009: t15.2023.10.20 val PER: 0.2349
2026-01-05 17:09:32,009: t15.2023.10.22 val PER: 0.2071
2026-01-05 17:09:32,009: t15.2023.11.03 val PER: 0.2687
2026-01-05 17:09:32,010: t15.2023.11.04 val PER: 0.1160
2026-01-05 17:09:32,010: t15.2023.11.17 val PER: 0.1229
2026-01-05 17:09:32,010: t15.2023.11.19 val PER: 0.1218
2026-01-05 17:09:32,010: t15.2023.11.26 val PER: 0.2732
2026-01-05 17:09:32,010: t15.2023.12.03 val PER: 0.2342
2026-01-05 17:09:32,010: t15.2023.12.08 val PER: 0.2117
2026-01-05 17:09:32,010: t15.2023.12.10 val PER: 0.1971
2026-01-05 17:09:32,010: t15.2023.12.17 val PER: 0.2100
2026-01-05 17:09:32,010: t15.2023.12.29 val PER: 0.2416
2026-01-05 17:09:32,010: t15.2024.02.25 val PER: 0.2149
2026-01-05 17:09:32,011: t15.2024.03.08 val PER: 0.3201
2026-01-05 17:09:32,011: t15.2024.03.15 val PER: 0.2702
2026-01-05 17:09:32,011: t15.2024.03.17 val PER: 0.2490
2026-01-05 17:09:32,011: t15.2024.05.10 val PER: 0.2422
2026-01-05 17:09:32,011: t15.2024.06.14 val PER: 0.2208
2026-01-05 17:09:32,011: t15.2024.07.19 val PER: 0.2861
2026-01-05 17:09:32,011: t15.2024.07.21 val PER: 0.1669
2026-01-05 17:09:32,011: t15.2024.07.28 val PER: 0.2235
2026-01-05 17:09:32,011: t15.2025.01.10 val PER: 0.3815
2026-01-05 17:09:32,011: t15.2025.01.12 val PER: 0.2787
2026-01-05 17:09:32,011: t15.2025.03.14 val PER: 0.3846
2026-01-05 17:09:32,011: t15.2025.03.16 val PER: 0.2736
2026-01-05 17:09:32,011: t15.2025.03.30 val PER: 0.3609
2026-01-05 17:09:32,011: t15.2025.04.13 val PER: 0.3210
2026-01-05 17:09:32,297: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_13500
2026-01-05 17:09:43,272: Train batch 13600: loss: 22.36 grad norm: 79.85 time: 0.106
2026-01-05 17:10:05,644: Train batch 13800: loss: 14.93 grad norm: 74.36 time: 0.098
2026-01-05 17:10:27,811: Train batch 14000: loss: 24.27 grad norm: 91.14 time: 0.086
2026-01-05 17:10:27,811: Running test after training batch: 14000
2026-01-05 17:10:27,926: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:10:33,602: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eaters the tadych code hatt cysts points his duhe
2026-01-05 17:10:33,699: WER debug example
  GT : how does it keep the cost down
  PR : eased holdouts dust hitt heaped thus quast setzer
2026-01-05 17:10:37,579: Val batch 14000: PER (avg): 0.2475 CTC Loss (avg): 25.8362 WER(1gram): 112.69% (n=64) time: 9.767
2026-01-05 17:10:37,579: WER lens: avg_true_words=6.16 avg_pred_words=7.28 max_pred_words=14
2026-01-05 17:10:37,580: t15.2023.08.13 val PER: 0.2339
2026-01-05 17:10:37,580: t15.2023.08.18 val PER: 0.2196
2026-01-05 17:10:37,580: t15.2023.08.20 val PER: 0.2097
2026-01-05 17:10:37,580: t15.2023.08.25 val PER: 0.2108
2026-01-05 17:10:37,580: t15.2023.08.27 val PER: 0.2894
2026-01-05 17:10:37,581: t15.2023.09.01 val PER: 0.1834
2026-01-05 17:10:37,581: t15.2023.09.03 val PER: 0.2874
2026-01-05 17:10:37,581: t15.2023.09.24 val PER: 0.2209
2026-01-05 17:10:37,581: t15.2023.09.29 val PER: 0.2253
2026-01-05 17:10:37,581: t15.2023.10.01 val PER: 0.2695
2026-01-05 17:10:37,581: t15.2023.10.06 val PER: 0.1970
2026-01-05 17:10:37,581: t15.2023.10.08 val PER: 0.3261
2026-01-05 17:10:37,581: t15.2023.10.13 val PER: 0.3274
2026-01-05 17:10:37,581: t15.2023.10.15 val PER: 0.2512
2026-01-05 17:10:37,582: t15.2023.10.20 val PER: 0.2550
2026-01-05 17:10:37,582: t15.2023.10.22 val PER: 0.1949
2026-01-05 17:10:37,582: t15.2023.11.03 val PER: 0.2666
2026-01-05 17:10:37,582: t15.2023.11.04 val PER: 0.1058
2026-01-05 17:10:37,582: t15.2023.11.17 val PER: 0.1229
2026-01-05 17:10:37,582: t15.2023.11.19 val PER: 0.1297
2026-01-05 17:10:37,582: t15.2023.11.26 val PER: 0.2630
2026-01-05 17:10:37,582: t15.2023.12.03 val PER: 0.2342
2026-01-05 17:10:37,582: t15.2023.12.08 val PER: 0.2157
2026-01-05 17:10:37,582: t15.2023.12.10 val PER: 0.2089
2026-01-05 17:10:37,582: t15.2023.12.17 val PER: 0.2017
2026-01-05 17:10:37,583: t15.2023.12.29 val PER: 0.2402
2026-01-05 17:10:37,583: t15.2024.02.25 val PER: 0.2149
2026-01-05 17:10:37,583: t15.2024.03.08 val PER: 0.3044
2026-01-05 17:10:37,583: t15.2024.03.15 val PER: 0.2714
2026-01-05 17:10:37,583: t15.2024.03.17 val PER: 0.2371
2026-01-05 17:10:37,583: t15.2024.05.10 val PER: 0.2318
2026-01-05 17:10:37,583: t15.2024.06.14 val PER: 0.2240
2026-01-05 17:10:37,583: t15.2024.07.19 val PER: 0.3006
2026-01-05 17:10:37,583: t15.2024.07.21 val PER: 0.1607
2026-01-05 17:10:37,584: t15.2024.07.28 val PER: 0.2316
2026-01-05 17:10:37,584: t15.2025.01.10 val PER: 0.3994
2026-01-05 17:10:37,584: t15.2025.01.12 val PER: 0.2702
2026-01-05 17:10:37,584: t15.2025.03.14 val PER: 0.3876
2026-01-05 17:10:37,584: t15.2025.03.16 val PER: 0.2749
2026-01-05 17:10:37,584: t15.2025.03.30 val PER: 0.3747
2026-01-05 17:10:37,584: t15.2025.04.13 val PER: 0.3167
2026-01-05 17:10:37,870: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_14000
2026-01-05 17:10:59,896: Train batch 14200: loss: 18.81 grad norm: 69.67 time: 0.096
2026-01-05 17:11:21,904: Train batch 14400: loss: 14.55 grad norm: 59.37 time: 0.110
2026-01-05 17:11:33,198: Running test after training batch: 14500
2026-01-05 17:11:33,301: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:11:38,991: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eater the tadych colt hatz this toots points his ruhe
2026-01-05 17:11:39,040: WER debug example
  GT : how does it keep the cost down
  PR : teicholz dust hitt heaped thus cost setzer
2026-01-05 17:11:41,954: Val batch 14500: PER (avg): 0.2461 CTC Loss (avg): 25.4956 WER(1gram): 108.38% (n=64) time: 8.755
2026-01-05 17:11:41,954: WER lens: avg_true_words=6.16 avg_pred_words=7.00 max_pred_words=13
2026-01-05 17:11:41,955: t15.2023.08.13 val PER: 0.2308
2026-01-05 17:11:41,955: t15.2023.08.18 val PER: 0.2137
2026-01-05 17:11:41,955: t15.2023.08.20 val PER: 0.1986
2026-01-05 17:11:41,955: t15.2023.08.25 val PER: 0.1988
2026-01-05 17:11:41,955: t15.2023.08.27 val PER: 0.2878
2026-01-05 17:11:41,955: t15.2023.09.01 val PER: 0.1891
2026-01-05 17:11:41,955: t15.2023.09.03 val PER: 0.2732
2026-01-05 17:11:41,956: t15.2023.09.24 val PER: 0.2051
2026-01-05 17:11:41,956: t15.2023.09.29 val PER: 0.2323
2026-01-05 17:11:41,956: t15.2023.10.01 val PER: 0.2629
2026-01-05 17:11:41,956: t15.2023.10.06 val PER: 0.1895
2026-01-05 17:11:41,956: t15.2023.10.08 val PER: 0.3369
2026-01-05 17:11:41,956: t15.2023.10.13 val PER: 0.3235
2026-01-05 17:11:41,956: t15.2023.10.15 val PER: 0.2577
2026-01-05 17:11:41,957: t15.2023.10.20 val PER: 0.2517
2026-01-05 17:11:41,957: t15.2023.10.22 val PER: 0.1971
2026-01-05 17:11:41,957: t15.2023.11.03 val PER: 0.2693
2026-01-05 17:11:41,957: t15.2023.11.04 val PER: 0.1024
2026-01-05 17:11:41,957: t15.2023.11.17 val PER: 0.1182
2026-01-05 17:11:41,957: t15.2023.11.19 val PER: 0.1118
2026-01-05 17:11:41,957: t15.2023.11.26 val PER: 0.2645
2026-01-05 17:11:41,957: t15.2023.12.03 val PER: 0.2342
2026-01-05 17:11:41,957: t15.2023.12.08 val PER: 0.2204
2026-01-05 17:11:41,957: t15.2023.12.10 val PER: 0.1984
2026-01-05 17:11:41,957: t15.2023.12.17 val PER: 0.2006
2026-01-05 17:11:41,957: t15.2023.12.29 val PER: 0.2361
2026-01-05 17:11:41,957: t15.2024.02.25 val PER: 0.2022
2026-01-05 17:11:41,957: t15.2024.03.08 val PER: 0.3073
2026-01-05 17:11:41,957: t15.2024.03.15 val PER: 0.2702
2026-01-05 17:11:41,958: t15.2024.03.17 val PER: 0.2427
2026-01-05 17:11:41,958: t15.2024.05.10 val PER: 0.2481
2026-01-05 17:11:41,958: t15.2024.06.14 val PER: 0.2303
2026-01-05 17:11:41,958: t15.2024.07.19 val PER: 0.2947
2026-01-05 17:11:41,958: t15.2024.07.21 val PER: 0.1724
2026-01-05 17:11:41,958: t15.2024.07.28 val PER: 0.2235
2026-01-05 17:11:41,958: t15.2025.01.10 val PER: 0.3871
2026-01-05 17:11:41,958: t15.2025.01.12 val PER: 0.2664
2026-01-05 17:11:41,958: t15.2025.03.14 val PER: 0.3876
2026-01-05 17:11:41,959: t15.2025.03.16 val PER: 0.2788
2026-01-05 17:11:41,959: t15.2025.03.30 val PER: 0.3667
2026-01-05 17:11:41,959: t15.2025.04.13 val PER: 0.3252
2026-01-05 17:11:42,242: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_14500
2026-01-05 17:11:53,249: Train batch 14600: loss: 23.27 grad norm: 82.01 time: 0.102
2026-01-05 17:12:15,266: Train batch 14800: loss: 14.66 grad norm: 60.03 time: 0.086
2026-01-05 17:12:37,128: Train batch 15000: loss: 16.88 grad norm: 65.09 time: 0.092
2026-01-05 17:12:37,129: Running test after training batch: 15000
2026-01-05 17:12:37,231: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:12:42,688: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eked the tadych kludt hatt cysts points his hoff
2026-01-05 17:12:42,735: WER debug example
  GT : how does it keep the cost down
  PR : east hosts dust hitt heaped thus costs
2026-01-05 17:12:45,276: Val batch 15000: PER (avg): 0.2423 CTC Loss (avg): 25.2861 WER(1gram): 108.88% (n=64) time: 8.146
2026-01-05 17:12:45,276: WER lens: avg_true_words=6.16 avg_pred_words=7.06 max_pred_words=13
2026-01-05 17:12:45,276: t15.2023.08.13 val PER: 0.2266
2026-01-05 17:12:45,276: t15.2023.08.18 val PER: 0.2179
2026-01-05 17:12:45,276: t15.2023.08.20 val PER: 0.2073
2026-01-05 17:12:45,276: t15.2023.08.25 val PER: 0.1883
2026-01-05 17:12:45,277: t15.2023.08.27 val PER: 0.2942
2026-01-05 17:12:45,277: t15.2023.09.01 val PER: 0.1794
2026-01-05 17:12:45,277: t15.2023.09.03 val PER: 0.2767
2026-01-05 17:12:45,277: t15.2023.09.24 val PER: 0.2100
2026-01-05 17:12:45,277: t15.2023.09.29 val PER: 0.2240
2026-01-05 17:12:45,277: t15.2023.10.01 val PER: 0.2616
2026-01-05 17:12:45,277: t15.2023.10.06 val PER: 0.1895
2026-01-05 17:12:45,277: t15.2023.10.08 val PER: 0.3234
2026-01-05 17:12:45,277: t15.2023.10.13 val PER: 0.3243
2026-01-05 17:12:45,277: t15.2023.10.15 val PER: 0.2492
2026-01-05 17:12:45,277: t15.2023.10.20 val PER: 0.2483
2026-01-05 17:12:45,277: t15.2023.10.22 val PER: 0.1837
2026-01-05 17:12:45,277: t15.2023.11.03 val PER: 0.2659
2026-01-05 17:12:45,278: t15.2023.11.04 val PER: 0.0922
2026-01-05 17:12:45,278: t15.2023.11.17 val PER: 0.1135
2026-01-05 17:12:45,278: t15.2023.11.19 val PER: 0.1158
2026-01-05 17:12:45,278: t15.2023.11.26 val PER: 0.2543
2026-01-05 17:12:45,278: t15.2023.12.03 val PER: 0.2227
2026-01-05 17:12:45,278: t15.2023.12.08 val PER: 0.2130
2026-01-05 17:12:45,278: t15.2023.12.10 val PER: 0.1932
2026-01-05 17:12:45,278: t15.2023.12.17 val PER: 0.2027
2026-01-05 17:12:45,278: t15.2023.12.29 val PER: 0.2313
2026-01-05 17:12:45,278: t15.2024.02.25 val PER: 0.2121
2026-01-05 17:12:45,278: t15.2024.03.08 val PER: 0.3058
2026-01-05 17:12:45,278: t15.2024.03.15 val PER: 0.2627
2026-01-05 17:12:45,278: t15.2024.03.17 val PER: 0.2462
2026-01-05 17:12:45,278: t15.2024.05.10 val PER: 0.2422
2026-01-05 17:12:45,278: t15.2024.06.14 val PER: 0.2287
2026-01-05 17:12:45,278: t15.2024.07.19 val PER: 0.2894
2026-01-05 17:12:45,278: t15.2024.07.21 val PER: 0.1641
2026-01-05 17:12:45,279: t15.2024.07.28 val PER: 0.2235
2026-01-05 17:12:45,279: t15.2025.01.10 val PER: 0.3926
2026-01-05 17:12:45,279: t15.2025.01.12 val PER: 0.2664
2026-01-05 17:12:45,279: t15.2025.03.14 val PER: 0.3683
2026-01-05 17:12:45,279: t15.2025.03.16 val PER: 0.2683
2026-01-05 17:12:45,279: t15.2025.03.30 val PER: 0.3563
2026-01-05 17:12:45,279: t15.2025.04.13 val PER: 0.3195
2026-01-05 17:12:45,547: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_15000
2026-01-05 17:13:07,600: Train batch 15200: loss: 13.80 grad norm: 71.57 time: 0.098
2026-01-05 17:13:29,420: Train batch 15400: loss: 21.84 grad norm: 80.85 time: 0.083
2026-01-05 17:13:40,488: Running test after training batch: 15500
2026-01-05 17:13:40,594: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:13:46,013: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eaters the tadych goodtab hatt cysts points has hwe
2026-01-05 17:13:46,058: WER debug example
  GT : how does it keep the cost down
  PR : teicholz dust hitt heaped thus costs
2026-01-05 17:13:48,674: Val batch 15500: PER (avg): 0.2408 CTC Loss (avg): 25.0591 WER(1gram): 108.88% (n=64) time: 8.185
2026-01-05 17:13:48,674: WER lens: avg_true_words=6.16 avg_pred_words=6.98 max_pred_words=14
2026-01-05 17:13:48,674: t15.2023.08.13 val PER: 0.2214
2026-01-05 17:13:48,675: t15.2023.08.18 val PER: 0.2163
2026-01-05 17:13:48,675: t15.2023.08.20 val PER: 0.2049
2026-01-05 17:13:48,675: t15.2023.08.25 val PER: 0.1973
2026-01-05 17:13:48,675: t15.2023.08.27 val PER: 0.2910
2026-01-05 17:13:48,675: t15.2023.09.01 val PER: 0.1713
2026-01-05 17:13:48,675: t15.2023.09.03 val PER: 0.2803
2026-01-05 17:13:48,675: t15.2023.09.24 val PER: 0.2112
2026-01-05 17:13:48,675: t15.2023.09.29 val PER: 0.2221
2026-01-05 17:13:48,676: t15.2023.10.01 val PER: 0.2655
2026-01-05 17:13:48,676: t15.2023.10.06 val PER: 0.1884
2026-01-05 17:13:48,676: t15.2023.10.08 val PER: 0.3234
2026-01-05 17:13:48,676: t15.2023.10.13 val PER: 0.3173
2026-01-05 17:13:48,676: t15.2023.10.15 val PER: 0.2465
2026-01-05 17:13:48,676: t15.2023.10.20 val PER: 0.2416
2026-01-05 17:13:48,676: t15.2023.10.22 val PER: 0.1971
2026-01-05 17:13:48,676: t15.2023.11.03 val PER: 0.2639
2026-01-05 17:13:48,676: t15.2023.11.04 val PER: 0.0990
2026-01-05 17:13:48,676: t15.2023.11.17 val PER: 0.1166
2026-01-05 17:13:48,676: t15.2023.11.19 val PER: 0.1257
2026-01-05 17:13:48,676: t15.2023.11.26 val PER: 0.2623
2026-01-05 17:13:48,676: t15.2023.12.03 val PER: 0.2206
2026-01-05 17:13:48,676: t15.2023.12.08 val PER: 0.2064
2026-01-05 17:13:48,676: t15.2023.12.10 val PER: 0.1892
2026-01-05 17:13:48,677: t15.2023.12.17 val PER: 0.1975
2026-01-05 17:13:48,677: t15.2023.12.29 val PER: 0.2251
2026-01-05 17:13:48,677: t15.2024.02.25 val PER: 0.2008
2026-01-05 17:13:48,677: t15.2024.03.08 val PER: 0.3115
2026-01-05 17:13:48,677: t15.2024.03.15 val PER: 0.2577
2026-01-05 17:13:48,677: t15.2024.03.17 val PER: 0.2343
2026-01-05 17:13:48,677: t15.2024.05.10 val PER: 0.2467
2026-01-05 17:13:48,677: t15.2024.06.14 val PER: 0.2240
2026-01-05 17:13:48,677: t15.2024.07.19 val PER: 0.2894
2026-01-05 17:13:48,677: t15.2024.07.21 val PER: 0.1648
2026-01-05 17:13:48,677: t15.2024.07.28 val PER: 0.2243
2026-01-05 17:13:48,677: t15.2025.01.10 val PER: 0.3788
2026-01-05 17:13:48,677: t15.2025.01.12 val PER: 0.2602
2026-01-05 17:13:48,677: t15.2025.03.14 val PER: 0.3846
2026-01-05 17:13:48,677: t15.2025.03.16 val PER: 0.2736
2026-01-05 17:13:48,677: t15.2025.03.30 val PER: 0.3575
2026-01-05 17:13:48,678: t15.2025.04.13 val PER: 0.3096
2026-01-05 17:13:48,946: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_15500
2026-01-05 17:14:00,149: Train batch 15600: loss: 24.64 grad norm: 81.11 time: 0.107
2026-01-05 17:14:22,004: Train batch 15800: loss: 27.64 grad norm: 98.34 time: 0.116
2026-01-05 17:14:44,060: Train batch 16000: loss: 15.60 grad norm: 59.40 time: 0.095
2026-01-05 17:14:44,060: Running test after training batch: 16000
2026-01-05 17:14:44,163: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:14:49,610: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eat the tadych goodtab hatz this toots points his hwe
2026-01-05 17:14:49,655: WER debug example
  GT : how does it keep the cost down
  PR : teicholz dust hitt heaped thus costs
2026-01-05 17:14:53,434: Val batch 16000: PER (avg): 0.2393 CTC Loss (avg): 24.8742 WER(1gram): 108.38% (n=64) time: 9.374
2026-01-05 17:14:53,434: WER lens: avg_true_words=6.16 avg_pred_words=6.88 max_pred_words=14
2026-01-05 17:14:53,435: t15.2023.08.13 val PER: 0.2100
2026-01-05 17:14:53,435: t15.2023.08.18 val PER: 0.2129
2026-01-05 17:14:53,435: t15.2023.08.20 val PER: 0.1930
2026-01-05 17:14:53,438: t15.2023.08.25 val PER: 0.2018
2026-01-05 17:14:53,438: t15.2023.08.27 val PER: 0.2733
2026-01-05 17:14:53,438: t15.2023.09.01 val PER: 0.1705
2026-01-05 17:14:53,438: t15.2023.09.03 val PER: 0.2708
2026-01-05 17:14:53,439: t15.2023.09.24 val PER: 0.2015
2026-01-05 17:14:53,439: t15.2023.09.29 val PER: 0.2227
2026-01-05 17:14:53,439: t15.2023.10.01 val PER: 0.2550
2026-01-05 17:14:53,439: t15.2023.10.06 val PER: 0.1916
2026-01-05 17:14:53,439: t15.2023.10.08 val PER: 0.3356
2026-01-05 17:14:53,439: t15.2023.10.13 val PER: 0.3204
2026-01-05 17:14:53,439: t15.2023.10.15 val PER: 0.2479
2026-01-05 17:14:53,439: t15.2023.10.20 val PER: 0.2483
2026-01-05 17:14:53,439: t15.2023.10.22 val PER: 0.1837
2026-01-05 17:14:53,440: t15.2023.11.03 val PER: 0.2666
2026-01-05 17:14:53,440: t15.2023.11.04 val PER: 0.0887
2026-01-05 17:14:53,440: t15.2023.11.17 val PER: 0.1198
2026-01-05 17:14:53,440: t15.2023.11.19 val PER: 0.1238
2026-01-05 17:14:53,440: t15.2023.11.26 val PER: 0.2551
2026-01-05 17:14:53,440: t15.2023.12.03 val PER: 0.2279
2026-01-05 17:14:53,440: t15.2023.12.08 val PER: 0.2150
2026-01-05 17:14:53,440: t15.2023.12.10 val PER: 0.1971
2026-01-05 17:14:53,441: t15.2023.12.17 val PER: 0.1881
2026-01-05 17:14:53,441: t15.2023.12.29 val PER: 0.2231
2026-01-05 17:14:53,441: t15.2024.02.25 val PER: 0.1896
2026-01-05 17:14:53,441: t15.2024.03.08 val PER: 0.2959
2026-01-05 17:14:53,441: t15.2024.03.15 val PER: 0.2689
2026-01-05 17:14:53,441: t15.2024.03.17 val PER: 0.2385
2026-01-05 17:14:53,441: t15.2024.05.10 val PER: 0.2422
2026-01-05 17:14:53,441: t15.2024.06.14 val PER: 0.2256
2026-01-05 17:14:53,442: t15.2024.07.19 val PER: 0.2848
2026-01-05 17:14:53,442: t15.2024.07.21 val PER: 0.1634
2026-01-05 17:14:53,442: t15.2024.07.28 val PER: 0.2147
2026-01-05 17:14:53,442: t15.2025.01.10 val PER: 0.3788
2026-01-05 17:14:53,442: t15.2025.01.12 val PER: 0.2748
2026-01-05 17:14:53,442: t15.2025.03.14 val PER: 0.3757
2026-01-05 17:14:53,442: t15.2025.03.16 val PER: 0.2801
2026-01-05 17:14:53,442: t15.2025.03.30 val PER: 0.3506
2026-01-05 17:14:53,442: t15.2025.04.13 val PER: 0.3067
2026-01-05 17:14:53,737: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_16000
2026-01-05 17:15:15,964: Train batch 16200: loss: 15.25 grad norm: 67.93 time: 0.094
2026-01-05 17:15:38,076: Train batch 16400: loss: 18.08 grad norm: 81.29 time: 0.097
2026-01-05 17:15:49,224: Running test after training batch: 16500
2026-01-05 17:15:49,339: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:15:55,009: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eaters the tadych colds hatz this toots points his hoff
2026-01-05 17:15:55,075: WER debug example
  GT : how does it keep the cost down
  PR : teicholz dust hitt heaped thus quast setzer
2026-01-05 17:15:58,884: Val batch 16500: PER (avg): 0.2365 CTC Loss (avg): 24.6838 WER(1gram): 107.11% (n=64) time: 9.660
2026-01-05 17:15:58,884: WER lens: avg_true_words=6.16 avg_pred_words=6.94 max_pred_words=14
2026-01-05 17:15:58,885: t15.2023.08.13 val PER: 0.2089
2026-01-05 17:15:58,885: t15.2023.08.18 val PER: 0.2154
2026-01-05 17:15:58,885: t15.2023.08.20 val PER: 0.1978
2026-01-05 17:15:58,885: t15.2023.08.25 val PER: 0.1913
2026-01-05 17:15:58,885: t15.2023.08.27 val PER: 0.2749
2026-01-05 17:15:58,885: t15.2023.09.01 val PER: 0.1680
2026-01-05 17:15:58,886: t15.2023.09.03 val PER: 0.2637
2026-01-05 17:15:58,886: t15.2023.09.24 val PER: 0.2087
2026-01-05 17:15:58,886: t15.2023.09.29 val PER: 0.2163
2026-01-05 17:15:58,886: t15.2023.10.01 val PER: 0.2635
2026-01-05 17:15:58,886: t15.2023.10.06 val PER: 0.1905
2026-01-05 17:15:58,886: t15.2023.10.08 val PER: 0.3356
2026-01-05 17:15:58,886: t15.2023.10.13 val PER: 0.3251
2026-01-05 17:15:58,886: t15.2023.10.15 val PER: 0.2432
2026-01-05 17:15:58,887: t15.2023.10.20 val PER: 0.2450
2026-01-05 17:15:58,887: t15.2023.10.22 val PER: 0.1849
2026-01-05 17:15:58,887: t15.2023.11.03 val PER: 0.2524
2026-01-05 17:15:58,887: t15.2023.11.04 val PER: 0.0922
2026-01-05 17:15:58,887: t15.2023.11.17 val PER: 0.1120
2026-01-05 17:15:58,887: t15.2023.11.19 val PER: 0.1178
2026-01-05 17:15:58,887: t15.2023.11.26 val PER: 0.2500
2026-01-05 17:15:58,887: t15.2023.12.03 val PER: 0.2174
2026-01-05 17:15:58,887: t15.2023.12.08 val PER: 0.2091
2026-01-05 17:15:58,888: t15.2023.12.10 val PER: 0.1905
2026-01-05 17:15:58,888: t15.2023.12.17 val PER: 0.1944
2026-01-05 17:15:58,888: t15.2023.12.29 val PER: 0.2210
2026-01-05 17:15:58,888: t15.2024.02.25 val PER: 0.2051
2026-01-05 17:15:58,888: t15.2024.03.08 val PER: 0.2945
2026-01-05 17:15:58,888: t15.2024.03.15 val PER: 0.2570
2026-01-05 17:15:58,888: t15.2024.03.17 val PER: 0.2294
2026-01-05 17:15:58,888: t15.2024.05.10 val PER: 0.2422
2026-01-05 17:15:58,888: t15.2024.06.14 val PER: 0.2177
2026-01-05 17:15:58,889: t15.2024.07.19 val PER: 0.2815
2026-01-05 17:15:58,889: t15.2024.07.21 val PER: 0.1676
2026-01-05 17:15:58,889: t15.2024.07.28 val PER: 0.2096
2026-01-05 17:15:58,889: t15.2025.01.10 val PER: 0.3719
2026-01-05 17:15:58,889: t15.2025.01.12 val PER: 0.2687
2026-01-05 17:15:58,889: t15.2025.03.14 val PER: 0.3654
2026-01-05 17:15:58,889: t15.2025.03.16 val PER: 0.2683
2026-01-05 17:15:58,889: t15.2025.03.30 val PER: 0.3540
2026-01-05 17:15:58,889: t15.2025.04.13 val PER: 0.3081
2026-01-05 17:15:59,175: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_16500
2026-01-05 17:16:10,319: Train batch 16600: loss: 21.20 grad norm: 302.67 time: 0.089
2026-01-05 17:16:32,170: Train batch 16800: loss: 26.01 grad norm: 93.92 time: 0.105
2026-01-05 17:16:54,080: Train batch 17000: loss: 17.31 grad norm: 78.51 time: 0.142
2026-01-05 17:16:54,080: Running test after training batch: 17000
2026-01-05 17:16:54,182: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:16:59,793: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eked the tadych goodtab hatz this toots points has ruhe
2026-01-05 17:16:59,855: WER debug example
  GT : how does it keep the cost down
  PR : teicholz dust hitt heaped thus cost setzer
2026-01-05 17:17:03,805: Val batch 17000: PER (avg): 0.2379 CTC Loss (avg): 24.7820 WER(1gram): 107.36% (n=64) time: 9.724
2026-01-05 17:17:03,805: WER lens: avg_true_words=6.16 avg_pred_words=6.97 max_pred_words=13
2026-01-05 17:17:03,806: t15.2023.08.13 val PER: 0.2141
2026-01-05 17:17:03,806: t15.2023.08.18 val PER: 0.2171
2026-01-05 17:17:03,806: t15.2023.08.20 val PER: 0.2073
2026-01-05 17:17:03,806: t15.2023.08.25 val PER: 0.1883
2026-01-05 17:17:03,807: t15.2023.08.27 val PER: 0.2717
2026-01-05 17:17:03,807: t15.2023.09.01 val PER: 0.1705
2026-01-05 17:17:03,807: t15.2023.09.03 val PER: 0.2637
2026-01-05 17:17:03,807: t15.2023.09.24 val PER: 0.2087
2026-01-05 17:17:03,807: t15.2023.09.29 val PER: 0.2227
2026-01-05 17:17:03,807: t15.2023.10.01 val PER: 0.2550
2026-01-05 17:17:03,807: t15.2023.10.06 val PER: 0.1862
2026-01-05 17:17:03,807: t15.2023.10.08 val PER: 0.3275
2026-01-05 17:17:03,807: t15.2023.10.13 val PER: 0.3220
2026-01-05 17:17:03,807: t15.2023.10.15 val PER: 0.2432
2026-01-05 17:17:03,807: t15.2023.10.20 val PER: 0.2383
2026-01-05 17:17:03,808: t15.2023.10.22 val PER: 0.1860
2026-01-05 17:17:03,808: t15.2023.11.03 val PER: 0.2531
2026-01-05 17:17:03,808: t15.2023.11.04 val PER: 0.0887
2026-01-05 17:17:03,808: t15.2023.11.17 val PER: 0.1135
2026-01-05 17:17:03,808: t15.2023.11.19 val PER: 0.1198
2026-01-05 17:17:03,808: t15.2023.11.26 val PER: 0.2580
2026-01-05 17:17:03,808: t15.2023.12.03 val PER: 0.2111
2026-01-05 17:17:03,808: t15.2023.12.08 val PER: 0.2037
2026-01-05 17:17:03,808: t15.2023.12.10 val PER: 0.1879
2026-01-05 17:17:03,808: t15.2023.12.17 val PER: 0.2037
2026-01-05 17:17:03,808: t15.2023.12.29 val PER: 0.2217
2026-01-05 17:17:03,809: t15.2024.02.25 val PER: 0.1924
2026-01-05 17:17:03,809: t15.2024.03.08 val PER: 0.2973
2026-01-05 17:17:03,809: t15.2024.03.15 val PER: 0.2633
2026-01-05 17:17:03,809: t15.2024.03.17 val PER: 0.2336
2026-01-05 17:17:03,809: t15.2024.05.10 val PER: 0.2377
2026-01-05 17:17:03,810: t15.2024.06.14 val PER: 0.2287
2026-01-05 17:17:03,810: t15.2024.07.19 val PER: 0.2900
2026-01-05 17:17:03,810: t15.2024.07.21 val PER: 0.1621
2026-01-05 17:17:03,810: t15.2024.07.28 val PER: 0.2199
2026-01-05 17:17:03,810: t15.2025.01.10 val PER: 0.3760
2026-01-05 17:17:03,810: t15.2025.01.12 val PER: 0.2656
2026-01-05 17:17:03,810: t15.2025.03.14 val PER: 0.3787
2026-01-05 17:17:03,810: t15.2025.03.16 val PER: 0.2788
2026-01-05 17:17:03,811: t15.2025.03.30 val PER: 0.3609
2026-01-05 17:17:03,811: t15.2025.04.13 val PER: 0.3024
2026-01-05 17:17:04,101: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_17000
2026-01-05 17:17:25,688: Train batch 17200: loss: 20.77 grad norm: 75.23 time: 0.145
2026-01-05 17:17:48,003: Train batch 17400: loss: 23.17 grad norm: 83.10 time: 0.125
2026-01-05 17:17:58,674: Running test after training batch: 17500
2026-01-05 17:17:58,789: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:18:04,252: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eaters the tadych goodtab hatt cysts points his hoff
2026-01-05 17:18:04,298: WER debug example
  GT : how does it keep the cost down
  PR : teicholz dust hitt heaped thus costs
2026-01-05 17:18:06,947: Val batch 17500: PER (avg): 0.2372 CTC Loss (avg): 24.7103 WER(1gram): 108.38% (n=64) time: 8.273
2026-01-05 17:18:06,947: WER lens: avg_true_words=6.16 avg_pred_words=6.95 max_pred_words=14
2026-01-05 17:18:06,948: t15.2023.08.13 val PER: 0.2152
2026-01-05 17:18:06,948: t15.2023.08.18 val PER: 0.2146
2026-01-05 17:18:06,948: t15.2023.08.20 val PER: 0.1986
2026-01-05 17:18:06,948: t15.2023.08.25 val PER: 0.2003
2026-01-05 17:18:06,948: t15.2023.08.27 val PER: 0.2749
2026-01-05 17:18:06,948: t15.2023.09.01 val PER: 0.1721
2026-01-05 17:18:06,948: t15.2023.09.03 val PER: 0.2660
2026-01-05 17:18:06,948: t15.2023.09.24 val PER: 0.2087
2026-01-05 17:18:06,948: t15.2023.09.29 val PER: 0.2176
2026-01-05 17:18:06,948: t15.2023.10.01 val PER: 0.2536
2026-01-05 17:18:06,948: t15.2023.10.06 val PER: 0.1895
2026-01-05 17:18:06,948: t15.2023.10.08 val PER: 0.3275
2026-01-05 17:18:06,948: t15.2023.10.13 val PER: 0.3220
2026-01-05 17:18:06,949: t15.2023.10.15 val PER: 0.2419
2026-01-05 17:18:06,949: t15.2023.10.20 val PER: 0.2685
2026-01-05 17:18:06,949: t15.2023.10.22 val PER: 0.1904
2026-01-05 17:18:06,949: t15.2023.11.03 val PER: 0.2551
2026-01-05 17:18:06,949: t15.2023.11.04 val PER: 0.0853
2026-01-05 17:18:06,949: t15.2023.11.17 val PER: 0.1135
2026-01-05 17:18:06,949: t15.2023.11.19 val PER: 0.1238
2026-01-05 17:18:06,949: t15.2023.11.26 val PER: 0.2464
2026-01-05 17:18:06,949: t15.2023.12.03 val PER: 0.2185
2026-01-05 17:18:06,949: t15.2023.12.08 val PER: 0.2104
2026-01-05 17:18:06,949: t15.2023.12.10 val PER: 0.1866
2026-01-05 17:18:06,949: t15.2023.12.17 val PER: 0.2006
2026-01-05 17:18:06,949: t15.2023.12.29 val PER: 0.2272
2026-01-05 17:18:06,949: t15.2024.02.25 val PER: 0.2008
2026-01-05 17:18:06,949: t15.2024.03.08 val PER: 0.2973
2026-01-05 17:18:06,949: t15.2024.03.15 val PER: 0.2577
2026-01-05 17:18:06,950: t15.2024.03.17 val PER: 0.2329
2026-01-05 17:18:06,950: t15.2024.05.10 val PER: 0.2422
2026-01-05 17:18:06,950: t15.2024.06.14 val PER: 0.2240
2026-01-05 17:18:06,950: t15.2024.07.19 val PER: 0.2815
2026-01-05 17:18:06,950: t15.2024.07.21 val PER: 0.1600
2026-01-05 17:18:06,950: t15.2024.07.28 val PER: 0.2213
2026-01-05 17:18:06,950: t15.2025.01.10 val PER: 0.3678
2026-01-05 17:18:06,950: t15.2025.01.12 val PER: 0.2648
2026-01-05 17:18:06,950: t15.2025.03.14 val PER: 0.3787
2026-01-05 17:18:06,950: t15.2025.03.16 val PER: 0.2709
2026-01-05 17:18:06,950: t15.2025.03.30 val PER: 0.3402
2026-01-05 17:18:06,950: t15.2025.04.13 val PER: 0.3110
2026-01-05 17:18:07,222: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_17500
2026-01-05 17:18:18,080: Train batch 17600: loss: 23.75 grad norm: 83.09 time: 0.086
2026-01-05 17:18:39,928: Train batch 17800: loss: 14.33 grad norm: 68.19 time: 0.069
2026-01-05 17:19:01,726: Train batch 18000: loss: 20.01 grad norm: 86.09 time: 0.106
2026-01-05 17:19:01,726: Running test after training batch: 18000
2026-01-05 17:19:01,827: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:19:07,426: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eked the tadych codes hatt cysts points has hoff
2026-01-05 17:19:07,472: WER debug example
  GT : how does it keep the cost down
  PR : totes dusty hitt heaped thus cost setzer
2026-01-05 17:19:10,176: Val batch 18000: PER (avg): 0.2352 CTC Loss (avg): 24.5489 WER(1gram): 106.60% (n=64) time: 8.449
2026-01-05 17:19:10,176: WER lens: avg_true_words=6.16 avg_pred_words=6.89 max_pred_words=13
2026-01-05 17:19:10,176: t15.2023.08.13 val PER: 0.2162
2026-01-05 17:19:10,176: t15.2023.08.18 val PER: 0.2062
2026-01-05 17:19:10,176: t15.2023.08.20 val PER: 0.2010
2026-01-05 17:19:10,177: t15.2023.08.25 val PER: 0.2018
2026-01-05 17:19:10,177: t15.2023.08.27 val PER: 0.2749
2026-01-05 17:19:10,177: t15.2023.09.01 val PER: 0.1753
2026-01-05 17:19:10,177: t15.2023.09.03 val PER: 0.2637
2026-01-05 17:19:10,177: t15.2023.09.24 val PER: 0.1942
2026-01-05 17:19:10,177: t15.2023.09.29 val PER: 0.2214
2026-01-05 17:19:10,177: t15.2023.10.01 val PER: 0.2629
2026-01-05 17:19:10,177: t15.2023.10.06 val PER: 0.1948
2026-01-05 17:19:10,178: t15.2023.10.08 val PER: 0.3275
2026-01-05 17:19:10,178: t15.2023.10.13 val PER: 0.3165
2026-01-05 17:19:10,178: t15.2023.10.15 val PER: 0.2479
2026-01-05 17:19:10,178: t15.2023.10.20 val PER: 0.2550
2026-01-05 17:19:10,178: t15.2023.10.22 val PER: 0.1849
2026-01-05 17:19:10,178: t15.2023.11.03 val PER: 0.2463
2026-01-05 17:19:10,178: t15.2023.11.04 val PER: 0.0887
2026-01-05 17:19:10,178: t15.2023.11.17 val PER: 0.1104
2026-01-05 17:19:10,178: t15.2023.11.19 val PER: 0.1178
2026-01-05 17:19:10,178: t15.2023.11.26 val PER: 0.2536
2026-01-05 17:19:10,181: t15.2023.12.03 val PER: 0.2143
2026-01-05 17:19:10,181: t15.2023.12.08 val PER: 0.2031
2026-01-05 17:19:10,181: t15.2023.12.10 val PER: 0.1827
2026-01-05 17:19:10,181: t15.2023.12.17 val PER: 0.1996
2026-01-05 17:19:10,181: t15.2023.12.29 val PER: 0.2176
2026-01-05 17:19:10,181: t15.2024.02.25 val PER: 0.1924
2026-01-05 17:19:10,182: t15.2024.03.08 val PER: 0.3073
2026-01-05 17:19:10,182: t15.2024.03.15 val PER: 0.2527
2026-01-05 17:19:10,182: t15.2024.03.17 val PER: 0.2301
2026-01-05 17:19:10,182: t15.2024.05.10 val PER: 0.2288
2026-01-05 17:19:10,182: t15.2024.06.14 val PER: 0.2287
2026-01-05 17:19:10,182: t15.2024.07.19 val PER: 0.2775
2026-01-05 17:19:10,182: t15.2024.07.21 val PER: 0.1648
2026-01-05 17:19:10,182: t15.2024.07.28 val PER: 0.2191
2026-01-05 17:19:10,182: t15.2025.01.10 val PER: 0.3678
2026-01-05 17:19:10,182: t15.2025.01.12 val PER: 0.2556
2026-01-05 17:19:10,182: t15.2025.03.14 val PER: 0.3639
2026-01-05 17:19:10,182: t15.2025.03.16 val PER: 0.2605
2026-01-05 17:19:10,182: t15.2025.03.30 val PER: 0.3437
2026-01-05 17:19:10,182: t15.2025.04.13 val PER: 0.3124
2026-01-05 17:19:10,465: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_18000
2026-01-05 17:19:33,196: Train batch 18200: loss: 19.67 grad norm: 74.48 time: 0.131
2026-01-05 17:19:55,122: Train batch 18400: loss: 11.94 grad norm: 70.67 time: 0.100
2026-01-05 17:20:06,092: Running test after training batch: 18500
2026-01-05 17:20:06,320: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:20:11,764: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eked the tadych goodtab hatz this toots points his ruhe
2026-01-05 17:20:11,809: WER debug example
  GT : how does it keep the cost down
  PR : totes dusty hitt heaped thus costs
2026-01-05 17:20:14,508: Val batch 18500: PER (avg): 0.2364 CTC Loss (avg): 24.6356 WER(1gram): 105.84% (n=64) time: 8.415
2026-01-05 17:20:14,509: WER lens: avg_true_words=6.16 avg_pred_words=6.84 max_pred_words=13
2026-01-05 17:20:14,509: t15.2023.08.13 val PER: 0.2141
2026-01-05 17:20:14,509: t15.2023.08.18 val PER: 0.2104
2026-01-05 17:20:14,509: t15.2023.08.20 val PER: 0.2010
2026-01-05 17:20:14,509: t15.2023.08.25 val PER: 0.1928
2026-01-05 17:20:14,509: t15.2023.08.27 val PER: 0.2878
2026-01-05 17:20:14,509: t15.2023.09.01 val PER: 0.1680
2026-01-05 17:20:14,509: t15.2023.09.03 val PER: 0.2672
2026-01-05 17:20:14,509: t15.2023.09.24 val PER: 0.1978
2026-01-05 17:20:14,510: t15.2023.09.29 val PER: 0.2208
2026-01-05 17:20:14,510: t15.2023.10.01 val PER: 0.2609
2026-01-05 17:20:14,510: t15.2023.10.06 val PER: 0.1819
2026-01-05 17:20:14,510: t15.2023.10.08 val PER: 0.3261
2026-01-05 17:20:14,510: t15.2023.10.13 val PER: 0.3251
2026-01-05 17:20:14,510: t15.2023.10.15 val PER: 0.2452
2026-01-05 17:20:14,510: t15.2023.10.20 val PER: 0.2685
2026-01-05 17:20:14,510: t15.2023.10.22 val PER: 0.1915
2026-01-05 17:20:14,510: t15.2023.11.03 val PER: 0.2531
2026-01-05 17:20:14,510: t15.2023.11.04 val PER: 0.0819
2026-01-05 17:20:14,510: t15.2023.11.17 val PER: 0.1104
2026-01-05 17:20:14,510: t15.2023.11.19 val PER: 0.1218
2026-01-05 17:20:14,510: t15.2023.11.26 val PER: 0.2522
2026-01-05 17:20:14,511: t15.2023.12.03 val PER: 0.2111
2026-01-05 17:20:14,511: t15.2023.12.08 val PER: 0.1997
2026-01-05 17:20:14,511: t15.2023.12.10 val PER: 0.1905
2026-01-05 17:20:14,511: t15.2023.12.17 val PER: 0.1985
2026-01-05 17:20:14,511: t15.2023.12.29 val PER: 0.2189
2026-01-05 17:20:14,511: t15.2024.02.25 val PER: 0.2022
2026-01-05 17:20:14,511: t15.2024.03.08 val PER: 0.2873
2026-01-05 17:20:14,511: t15.2024.03.15 val PER: 0.2545
2026-01-05 17:20:14,511: t15.2024.03.17 val PER: 0.2350
2026-01-05 17:20:14,511: t15.2024.05.10 val PER: 0.2318
2026-01-05 17:20:14,511: t15.2024.06.14 val PER: 0.2208
2026-01-05 17:20:14,511: t15.2024.07.19 val PER: 0.2874
2026-01-05 17:20:14,511: t15.2024.07.21 val PER: 0.1552
2026-01-05 17:20:14,511: t15.2024.07.28 val PER: 0.2221
2026-01-05 17:20:14,511: t15.2025.01.10 val PER: 0.3705
2026-01-05 17:20:14,511: t15.2025.01.12 val PER: 0.2594
2026-01-05 17:20:14,511: t15.2025.03.14 val PER: 0.3817
2026-01-05 17:20:14,512: t15.2025.03.16 val PER: 0.2749
2026-01-05 17:20:14,512: t15.2025.03.30 val PER: 0.3437
2026-01-05 17:20:14,512: t15.2025.04.13 val PER: 0.3167
2026-01-05 17:20:14,789: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_18500
2026-01-05 17:20:25,895: Train batch 18600: loss: 22.87 grad norm: 93.64 time: 0.116
2026-01-05 17:20:47,727: Train batch 18800: loss: 20.28 grad norm: 97.32 time: 0.111
2026-01-05 17:21:10,067: Train batch 19000: loss: 17.36 grad norm: 70.70 time: 0.110
2026-01-05 17:21:10,067: Running test after training batch: 19000
2026-01-05 17:21:10,171: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:21:15,640: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eked the tadych codes hatz this toots points his ruhe
2026-01-05 17:21:15,685: WER debug example
  GT : how does it keep the cost down
  PR : totes dust hitt heaped thus quast setzer
2026-01-05 17:21:18,418: Val batch 19000: PER (avg): 0.2350 CTC Loss (avg): 24.3936 WER(1gram): 107.36% (n=64) time: 8.351
2026-01-05 17:21:18,418: WER lens: avg_true_words=6.16 avg_pred_words=6.94 max_pred_words=14
2026-01-05 17:21:18,419: t15.2023.08.13 val PER: 0.2193
2026-01-05 17:21:18,419: t15.2023.08.18 val PER: 0.2079
2026-01-05 17:21:18,419: t15.2023.08.20 val PER: 0.1978
2026-01-05 17:21:18,419: t15.2023.08.25 val PER: 0.1928
2026-01-05 17:21:18,419: t15.2023.08.27 val PER: 0.2846
2026-01-05 17:21:18,419: t15.2023.09.01 val PER: 0.1607
2026-01-05 17:21:18,419: t15.2023.09.03 val PER: 0.2637
2026-01-05 17:21:18,419: t15.2023.09.24 val PER: 0.2015
2026-01-05 17:21:18,419: t15.2023.09.29 val PER: 0.2176
2026-01-05 17:21:18,419: t15.2023.10.01 val PER: 0.2556
2026-01-05 17:21:18,419: t15.2023.10.06 val PER: 0.1862
2026-01-05 17:21:18,420: t15.2023.10.08 val PER: 0.3356
2026-01-05 17:21:18,420: t15.2023.10.13 val PER: 0.3173
2026-01-05 17:21:18,420: t15.2023.10.15 val PER: 0.2492
2026-01-05 17:21:18,420: t15.2023.10.20 val PER: 0.2718
2026-01-05 17:21:18,420: t15.2023.10.22 val PER: 0.1882
2026-01-05 17:21:18,420: t15.2023.11.03 val PER: 0.2524
2026-01-05 17:21:18,420: t15.2023.11.04 val PER: 0.0785
2026-01-05 17:21:18,420: t15.2023.11.17 val PER: 0.1120
2026-01-05 17:21:18,420: t15.2023.11.19 val PER: 0.1138
2026-01-05 17:21:18,420: t15.2023.11.26 val PER: 0.2565
2026-01-05 17:21:18,420: t15.2023.12.03 val PER: 0.2174
2026-01-05 17:21:18,420: t15.2023.12.08 val PER: 0.1964
2026-01-05 17:21:18,421: t15.2023.12.10 val PER: 0.1879
2026-01-05 17:21:18,421: t15.2023.12.17 val PER: 0.1954
2026-01-05 17:21:18,421: t15.2023.12.29 val PER: 0.2128
2026-01-05 17:21:18,421: t15.2024.02.25 val PER: 0.2008
2026-01-05 17:21:18,421: t15.2024.03.08 val PER: 0.2873
2026-01-05 17:21:18,421: t15.2024.03.15 val PER: 0.2514
2026-01-05 17:21:18,421: t15.2024.03.17 val PER: 0.2329
2026-01-05 17:21:18,421: t15.2024.05.10 val PER: 0.2333
2026-01-05 17:21:18,421: t15.2024.06.14 val PER: 0.2177
2026-01-05 17:21:18,422: t15.2024.07.19 val PER: 0.2769
2026-01-05 17:21:18,422: t15.2024.07.21 val PER: 0.1621
2026-01-05 17:21:18,422: t15.2024.07.28 val PER: 0.2118
2026-01-05 17:21:18,422: t15.2025.01.10 val PER: 0.3581
2026-01-05 17:21:18,422: t15.2025.01.12 val PER: 0.2664
2026-01-05 17:21:18,422: t15.2025.03.14 val PER: 0.3905
2026-01-05 17:21:18,422: t15.2025.03.16 val PER: 0.2683
2026-01-05 17:21:18,422: t15.2025.03.30 val PER: 0.3563
2026-01-05 17:21:18,422: t15.2025.04.13 val PER: 0.3081
2026-01-05 17:21:18,690: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_19000
2026-01-05 17:21:41,073: Train batch 19200: loss: 14.38 grad norm: 91.18 time: 0.108
2026-01-05 17:22:03,484: Train batch 19400: loss: 15.34 grad norm: 68.19 time: 0.089
2026-01-05 17:22:14,350: Running test after training batch: 19500
2026-01-05 17:22:14,452: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:22:19,976: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eked the tadych codes hatt cysts points his ruhe
2026-01-05 17:22:20,023: WER debug example
  GT : how does it keep the cost down
  PR : totes dusty hitt heaped thus quast setzer
2026-01-05 17:22:22,742: Val batch 19500: PER (avg): 0.2357 CTC Loss (avg): 24.2317 WER(1gram): 106.35% (n=64) time: 8.391
2026-01-05 17:22:22,742: WER lens: avg_true_words=6.16 avg_pred_words=6.88 max_pred_words=13
2026-01-05 17:22:22,742: t15.2023.08.13 val PER: 0.2069
2026-01-05 17:22:22,742: t15.2023.08.18 val PER: 0.2112
2026-01-05 17:22:22,743: t15.2023.08.20 val PER: 0.1986
2026-01-05 17:22:22,743: t15.2023.08.25 val PER: 0.1973
2026-01-05 17:22:22,743: t15.2023.08.27 val PER: 0.2814
2026-01-05 17:22:22,743: t15.2023.09.01 val PER: 0.1721
2026-01-05 17:22:22,743: t15.2023.09.03 val PER: 0.2565
2026-01-05 17:22:22,743: t15.2023.09.24 val PER: 0.2002
2026-01-05 17:22:22,743: t15.2023.09.29 val PER: 0.2195
2026-01-05 17:22:22,744: t15.2023.10.01 val PER: 0.2563
2026-01-05 17:22:22,744: t15.2023.10.06 val PER: 0.1851
2026-01-05 17:22:22,744: t15.2023.10.08 val PER: 0.3248
2026-01-05 17:22:22,744: t15.2023.10.13 val PER: 0.3111
2026-01-05 17:22:22,744: t15.2023.10.15 val PER: 0.2479
2026-01-05 17:22:22,744: t15.2023.10.20 val PER: 0.2752
2026-01-05 17:22:22,744: t15.2023.10.22 val PER: 0.1826
2026-01-05 17:22:22,744: t15.2023.11.03 val PER: 0.2531
2026-01-05 17:22:22,744: t15.2023.11.04 val PER: 0.0785
2026-01-05 17:22:22,744: t15.2023.11.17 val PER: 0.1073
2026-01-05 17:22:22,744: t15.2023.11.19 val PER: 0.1138
2026-01-05 17:22:22,744: t15.2023.11.26 val PER: 0.2645
2026-01-05 17:22:22,744: t15.2023.12.03 val PER: 0.2090
2026-01-05 17:22:22,744: t15.2023.12.08 val PER: 0.2017
2026-01-05 17:22:22,745: t15.2023.12.10 val PER: 0.1879
2026-01-05 17:22:22,745: t15.2023.12.17 val PER: 0.1985
2026-01-05 17:22:22,745: t15.2023.12.29 val PER: 0.2203
2026-01-05 17:22:22,745: t15.2024.02.25 val PER: 0.1994
2026-01-05 17:22:22,745: t15.2024.03.08 val PER: 0.3073
2026-01-05 17:22:22,745: t15.2024.03.15 val PER: 0.2577
2026-01-05 17:22:22,745: t15.2024.03.17 val PER: 0.2301
2026-01-05 17:22:22,745: t15.2024.05.10 val PER: 0.2363
2026-01-05 17:22:22,745: t15.2024.06.14 val PER: 0.2145
2026-01-05 17:22:22,745: t15.2024.07.19 val PER: 0.2841
2026-01-05 17:22:22,745: t15.2024.07.21 val PER: 0.1579
2026-01-05 17:22:22,745: t15.2024.07.28 val PER: 0.2125
2026-01-05 17:22:22,745: t15.2025.01.10 val PER: 0.3733
2026-01-05 17:22:22,746: t15.2025.01.12 val PER: 0.2610
2026-01-05 17:22:22,746: t15.2025.03.14 val PER: 0.3683
2026-01-05 17:22:22,746: t15.2025.03.16 val PER: 0.2723
2026-01-05 17:22:22,746: t15.2025.03.30 val PER: 0.3621
2026-01-05 17:22:22,746: t15.2025.04.13 val PER: 0.3124
2026-01-05 17:22:23,030: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_19500
2026-01-05 17:22:34,086: Train batch 19600: loss: 19.46 grad norm: 76.96 time: 0.098
2026-01-05 17:22:55,595: Train batch 19800: loss: 14.57 grad norm: 76.13 time: 0.093
2026-01-05 17:23:17,289: Running test after training batch: 19999
2026-01-05 17:23:17,387: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:23:22,792: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eked the tadych colt hatt cysts points his ruhe
2026-01-05 17:23:22,841: WER debug example
  GT : how does it keep the cost down
  PR : totes dust hitt heaped thus quast setzer
2026-01-05 17:23:25,602: Val batch 19999: PER (avg): 0.2338 CTC Loss (avg): 24.1828 WER(1gram): 106.60% (n=64) time: 8.313
2026-01-05 17:23:25,603: WER lens: avg_true_words=6.16 avg_pred_words=6.88 max_pred_words=13
2026-01-05 17:23:25,603: t15.2023.08.13 val PER: 0.2069
2026-01-05 17:23:25,603: t15.2023.08.18 val PER: 0.2062
2026-01-05 17:23:25,603: t15.2023.08.20 val PER: 0.1970
2026-01-05 17:23:25,603: t15.2023.08.25 val PER: 0.1928
2026-01-05 17:23:25,603: t15.2023.08.27 val PER: 0.2878
2026-01-05 17:23:25,604: t15.2023.09.01 val PER: 0.1631
2026-01-05 17:23:25,604: t15.2023.09.03 val PER: 0.2589
2026-01-05 17:23:25,604: t15.2023.09.24 val PER: 0.1954
2026-01-05 17:23:25,604: t15.2023.09.29 val PER: 0.2214
2026-01-05 17:23:25,604: t15.2023.10.01 val PER: 0.2629
2026-01-05 17:23:25,604: t15.2023.10.06 val PER: 0.1765
2026-01-05 17:23:25,604: t15.2023.10.08 val PER: 0.3329
2026-01-05 17:23:25,604: t15.2023.10.13 val PER: 0.3142
2026-01-05 17:23:25,604: t15.2023.10.15 val PER: 0.2439
2026-01-05 17:23:25,604: t15.2023.10.20 val PER: 0.2517
2026-01-05 17:23:25,604: t15.2023.10.22 val PER: 0.1804
2026-01-05 17:23:25,604: t15.2023.11.03 val PER: 0.2517
2026-01-05 17:23:25,604: t15.2023.11.04 val PER: 0.0819
2026-01-05 17:23:25,604: t15.2023.11.17 val PER: 0.1073
2026-01-05 17:23:25,605: t15.2023.11.19 val PER: 0.1178
2026-01-05 17:23:25,605: t15.2023.11.26 val PER: 0.2587
2026-01-05 17:23:25,605: t15.2023.12.03 val PER: 0.2048
2026-01-05 17:23:25,605: t15.2023.12.08 val PER: 0.1957
2026-01-05 17:23:25,605: t15.2023.12.10 val PER: 0.1827
2026-01-05 17:23:25,605: t15.2023.12.17 val PER: 0.1985
2026-01-05 17:23:25,605: t15.2023.12.29 val PER: 0.2100
2026-01-05 17:23:25,605: t15.2024.02.25 val PER: 0.1952
2026-01-05 17:23:25,605: t15.2024.03.08 val PER: 0.3001
2026-01-05 17:23:25,605: t15.2024.03.15 val PER: 0.2545
2026-01-05 17:23:25,605: t15.2024.03.17 val PER: 0.2225
2026-01-05 17:23:25,605: t15.2024.05.10 val PER: 0.2348
2026-01-05 17:23:25,605: t15.2024.06.14 val PER: 0.2192
2026-01-05 17:23:25,605: t15.2024.07.19 val PER: 0.2887
2026-01-05 17:23:25,605: t15.2024.07.21 val PER: 0.1621
2026-01-05 17:23:25,605: t15.2024.07.28 val PER: 0.2147
2026-01-05 17:23:25,606: t15.2025.01.10 val PER: 0.3760
2026-01-05 17:23:25,606: t15.2025.01.12 val PER: 0.2556
2026-01-05 17:23:25,606: t15.2025.03.14 val PER: 0.3683
2026-01-05 17:23:25,606: t15.2025.03.16 val PER: 0.2709
2026-01-05 17:23:25,606: t15.2025.03.30 val PER: 0.3563
2026-01-05 17:23:25,606: t15.2025.04.13 val PER: 0.3067
2026-01-05 17:23:25,885: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st2_wd1e-5/checkpoint/checkpoint_batch_19999
2026-01-05 17:23:25,913: Best avg val PER achieved: 0.49717
2026-01-05 17:23:25,913: Total training time: 44.56 minutes

=== RUN st3_wd1e-5.yaml ===
2026-01-05 17:23:31,956: Using device: cuda:0
2026-01-05 17:23:33,929: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-05 17:23:33,952: Using 45 sessions after filtering (from 45).
2026-01-05 17:23:34,357: Using torch.compile (if available)
2026-01-05 17:23:34,358: torch.compile not available (torch<2.0). Skipping.
2026-01-05 17:23:34,358: Initialized RNN decoding model
2026-01-05 17:23:34,358: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-05 17:23:34,358: Model has 44,907,305 parameters
2026-01-05 17:23:34,358: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-05 17:23:35,631: Successfully initialized datasets
2026-01-05 17:23:35,632: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-05 17:23:37,236: Train batch 0: loss: 783.66 grad norm: 1815.17 time: 0.220
2026-01-05 17:23:37,236: Running test after training batch: 0
2026-01-05 17:23:37,349: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:23:42,940: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf heinzmann frome
2026-01-05 17:23:43,994: WER debug example
  GT : how does it keep the cost down
  PR : loughney laughman
2026-01-05 17:24:31,707: Val batch 0: PER (avg): 1.7809 CTC Loss (avg): 856.2325 WER(1gram): 100.00% (n=64) time: 54.471
2026-01-05 17:24:31,708: WER lens: avg_true_words=6.16 avg_pred_words=2.44 max_pred_words=6
2026-01-05 17:24:31,708: t15.2023.08.13 val PER: 1.5748
2026-01-05 17:24:31,708: t15.2023.08.18 val PER: 1.7376
2026-01-05 17:24:31,708: t15.2023.08.20 val PER: 1.6148
2026-01-05 17:24:31,708: t15.2023.08.25 val PER: 1.6762
2026-01-05 17:24:31,708: t15.2023.08.27 val PER: 1.5418
2026-01-05 17:24:31,708: t15.2023.09.01 val PER: 1.8019
2026-01-05 17:24:31,708: t15.2023.09.03 val PER: 1.6247
2026-01-05 17:24:31,708: t15.2023.09.24 val PER: 1.9017
2026-01-05 17:24:31,708: t15.2023.09.29 val PER: 1.8398
2026-01-05 17:24:31,709: t15.2023.10.01 val PER: 1.4590
2026-01-05 17:24:31,709: t15.2023.10.06 val PER: 1.8859
2026-01-05 17:24:31,709: t15.2023.10.08 val PER: 1.4276
2026-01-05 17:24:31,709: t15.2023.10.13 val PER: 1.7114
2026-01-05 17:24:31,709: t15.2023.10.15 val PER: 1.7337
2026-01-05 17:24:31,709: t15.2023.10.20 val PER: 1.8859
2026-01-05 17:24:31,709: t15.2023.10.22 val PER: 1.7862
2026-01-05 17:24:31,709: t15.2023.11.03 val PER: 2.0102
2026-01-05 17:24:31,709: t15.2023.11.04 val PER: 2.5324
2026-01-05 17:24:31,709: t15.2023.11.17 val PER: 2.5070
2026-01-05 17:24:31,709: t15.2023.11.19 val PER: 2.0699
2026-01-05 17:24:31,709: t15.2023.11.26 val PER: 1.9036
2026-01-05 17:24:31,709: t15.2023.12.03 val PER: 1.8036
2026-01-05 17:24:31,709: t15.2023.12.08 val PER: 1.8309
2026-01-05 17:24:31,709: t15.2023.12.10 val PER: 2.1104
2026-01-05 17:24:31,710: t15.2023.12.17 val PER: 1.6019
2026-01-05 17:24:31,710: t15.2023.12.29 val PER: 1.7529
2026-01-05 17:24:31,710: t15.2024.02.25 val PER: 1.7725
2026-01-05 17:24:31,710: t15.2024.03.08 val PER: 1.5932
2026-01-05 17:24:31,710: t15.2024.03.15 val PER: 1.6523
2026-01-05 17:24:31,710: t15.2024.03.17 val PER: 1.7476
2026-01-05 17:24:31,710: t15.2024.05.10 val PER: 1.6999
2026-01-05 17:24:31,710: t15.2024.06.14 val PER: 1.9385
2026-01-05 17:24:31,710: t15.2024.07.19 val PER: 1.3309
2026-01-05 17:24:31,710: t15.2024.07.21 val PER: 2.0152
2026-01-05 17:24:31,710: t15.2024.07.28 val PER: 2.1132
2026-01-05 17:24:31,711: t15.2025.01.10 val PER: 1.3623
2026-01-05 17:24:31,711: t15.2025.01.12 val PER: 2.2325
2026-01-05 17:24:31,711: t15.2025.03.14 val PER: 1.2456
2026-01-05 17:24:31,711: t15.2025.03.16 val PER: 2.1060
2026-01-05 17:24:31,711: t15.2025.03.30 val PER: 1.5793
2026-01-05 17:24:31,711: t15.2025.04.13 val PER: 1.9786
2026-01-05 17:24:31,712: New best val WER(1gram) inf% --> 100.00%
2026-01-05 17:24:31,712: Checkpointing model
2026-01-05 17:24:31,989: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/best_checkpoint
2026-01-05 17:24:32,267: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_0
2026-01-05 17:24:52,159: Train batch 200: loss: 92.33 grad norm: 147.40 time: 0.067
2026-01-05 17:25:11,529: Train batch 400: loss: 67.13 grad norm: 76.07 time: 0.079
2026-01-05 17:25:21,259: Running test after training batch: 500
2026-01-05 17:25:21,358: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:25:26,595: WER debug example
  GT : you can see the code at this point as well
  PR : hollyhead teething had his this introduces
2026-01-05 17:25:26,688: WER debug example
  GT : how does it keep the cost down
  PR : tuthill transiting tuthill thus this those
2026-01-05 17:25:34,952: Val batch 500: PER (avg): 0.6454 CTC Loss (avg): 71.1859 WER(1gram): 97.46% (n=64) time: 13.693
2026-01-05 17:25:34,953: WER lens: avg_true_words=6.16 avg_pred_words=4.50 max_pred_words=9
2026-01-05 17:25:34,953: t15.2023.08.13 val PER: 0.6403
2026-01-05 17:25:34,953: t15.2023.08.18 val PER: 0.6027
2026-01-05 17:25:34,953: t15.2023.08.20 val PER: 0.5933
2026-01-05 17:25:34,953: t15.2023.08.25 val PER: 0.5617
2026-01-05 17:25:34,953: t15.2023.08.27 val PER: 0.6576
2026-01-05 17:25:34,953: t15.2023.09.01 val PER: 0.5933
2026-01-05 17:25:34,953: t15.2023.09.03 val PER: 0.6366
2026-01-05 17:25:34,954: t15.2023.09.24 val PER: 0.5947
2026-01-05 17:25:34,954: t15.2023.09.29 val PER: 0.5884
2026-01-05 17:25:34,954: t15.2023.10.01 val PER: 0.6480
2026-01-05 17:25:34,954: t15.2023.10.06 val PER: 0.6028
2026-01-05 17:25:34,954: t15.2023.10.08 val PER: 0.6441
2026-01-05 17:25:34,954: t15.2023.10.13 val PER: 0.6687
2026-01-05 17:25:34,954: t15.2023.10.15 val PER: 0.6440
2026-01-05 17:25:34,955: t15.2023.10.20 val PER: 0.5940
2026-01-05 17:25:34,955: t15.2023.10.22 val PER: 0.6058
2026-01-05 17:25:34,955: t15.2023.11.03 val PER: 0.6635
2026-01-05 17:25:34,955: t15.2023.11.04 val PER: 0.5051
2026-01-05 17:25:34,955: t15.2023.11.17 val PER: 0.5365
2026-01-05 17:25:34,955: t15.2023.11.19 val PER: 0.5090
2026-01-05 17:25:34,955: t15.2023.11.26 val PER: 0.6674
2026-01-05 17:25:34,955: t15.2023.12.03 val PER: 0.6303
2026-01-05 17:25:34,955: t15.2023.12.08 val PER: 0.6258
2026-01-05 17:25:34,955: t15.2023.12.10 val PER: 0.5861
2026-01-05 17:25:34,955: t15.2023.12.17 val PER: 0.6476
2026-01-05 17:25:34,956: t15.2023.12.29 val PER: 0.6527
2026-01-05 17:25:34,956: t15.2024.02.25 val PER: 0.6236
2026-01-05 17:25:34,956: t15.2024.03.08 val PER: 0.6956
2026-01-05 17:25:34,956: t15.2024.03.15 val PER: 0.6911
2026-01-05 17:25:34,956: t15.2024.03.17 val PER: 0.6332
2026-01-05 17:25:34,956: t15.2024.05.10 val PER: 0.6404
2026-01-05 17:25:34,957: t15.2024.06.14 val PER: 0.6924
2026-01-05 17:25:34,957: t15.2024.07.19 val PER: 0.7258
2026-01-05 17:25:34,957: t15.2024.07.21 val PER: 0.6138
2026-01-05 17:25:34,957: t15.2024.07.28 val PER: 0.6750
2026-01-05 17:25:34,957: t15.2025.01.10 val PER: 0.7865
2026-01-05 17:25:34,957: t15.2025.01.12 val PER: 0.6690
2026-01-05 17:25:34,957: t15.2025.03.14 val PER: 0.7751
2026-01-05 17:25:34,957: t15.2025.03.16 val PER: 0.6976
2026-01-05 17:25:34,957: t15.2025.03.30 val PER: 0.7816
2026-01-05 17:25:34,957: t15.2025.04.13 val PER: 0.6904
2026-01-05 17:25:34,958: New best val WER(1gram) 100.00% --> 97.46%
2026-01-05 17:25:34,958: Checkpointing model
2026-01-05 17:25:36,718: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/best_checkpoint
2026-01-05 17:25:37,000: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_500
2026-01-05 17:25:46,849: Train batch 600: loss: 64.64 grad norm: 118.01 time: 0.097
2026-01-05 17:26:06,531: Train batch 800: loss: 54.23 grad norm: 89.53 time: 0.070
2026-01-05 17:26:26,276: Train batch 1000: loss: 54.38 grad norm: 95.14 time: 0.083
2026-01-05 17:26:26,276: Running test after training batch: 1000
2026-01-05 17:26:26,380: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:26:31,484: WER debug example
  GT : you can see the code at this point as well
  PR : huling hands sies the headhunters this spuds tis
2026-01-05 17:26:31,538: WER debug example
  GT : how does it keep the cost down
  PR : handsaws hitting hing thus quicksands
2026-01-05 17:26:35,383: Val batch 1000: PER (avg): 0.5454 CTC Loss (avg): 57.0112 WER(1gram): 97.21% (n=64) time: 9.107
2026-01-05 17:26:35,384: WER lens: avg_true_words=6.16 avg_pred_words=4.42 max_pred_words=9
2026-01-05 17:26:35,384: t15.2023.08.13 val PER: 0.5052
2026-01-05 17:26:35,384: t15.2023.08.18 val PER: 0.4987
2026-01-05 17:26:35,384: t15.2023.08.20 val PER: 0.5131
2026-01-05 17:26:35,384: t15.2023.08.25 val PER: 0.4367
2026-01-05 17:26:35,384: t15.2023.08.27 val PER: 0.5498
2026-01-05 17:26:35,385: t15.2023.09.01 val PER: 0.4594
2026-01-05 17:26:35,385: t15.2023.09.03 val PER: 0.5891
2026-01-05 17:26:35,385: t15.2023.09.24 val PER: 0.4903
2026-01-05 17:26:35,385: t15.2023.09.29 val PER: 0.5392
2026-01-05 17:26:35,385: t15.2023.10.01 val PER: 0.5469
2026-01-05 17:26:35,385: t15.2023.10.06 val PER: 0.4822
2026-01-05 17:26:35,385: t15.2023.10.08 val PER: 0.5507
2026-01-05 17:26:35,385: t15.2023.10.13 val PER: 0.6323
2026-01-05 17:26:35,386: t15.2023.10.15 val PER: 0.5432
2026-01-05 17:26:35,386: t15.2023.10.20 val PER: 0.4966
2026-01-05 17:26:35,386: t15.2023.10.22 val PER: 0.4733
2026-01-05 17:26:35,386: t15.2023.11.03 val PER: 0.5828
2026-01-05 17:26:35,386: t15.2023.11.04 val PER: 0.3618
2026-01-05 17:26:35,386: t15.2023.11.17 val PER: 0.4215
2026-01-05 17:26:35,386: t15.2023.11.19 val PER: 0.3593
2026-01-05 17:26:35,386: t15.2023.11.26 val PER: 0.6181
2026-01-05 17:26:35,386: t15.2023.12.03 val PER: 0.5735
2026-01-05 17:26:35,387: t15.2023.12.08 val PER: 0.5539
2026-01-05 17:26:35,387: t15.2023.12.10 val PER: 0.4980
2026-01-05 17:26:35,387: t15.2023.12.17 val PER: 0.5426
2026-01-05 17:26:35,387: t15.2023.12.29 val PER: 0.5202
2026-01-05 17:26:35,387: t15.2024.02.25 val PER: 0.4888
2026-01-05 17:26:35,387: t15.2024.03.08 val PER: 0.5889
2026-01-05 17:26:35,387: t15.2024.03.15 val PER: 0.5303
2026-01-05 17:26:35,387: t15.2024.03.17 val PER: 0.5188
2026-01-05 17:26:35,387: t15.2024.05.10 val PER: 0.5379
2026-01-05 17:26:35,387: t15.2024.06.14 val PER: 0.5489
2026-01-05 17:26:35,388: t15.2024.07.19 val PER: 0.6440
2026-01-05 17:26:35,388: t15.2024.07.21 val PER: 0.4759
2026-01-05 17:26:35,388: t15.2024.07.28 val PER: 0.5353
2026-01-05 17:26:35,388: t15.2025.01.10 val PER: 0.7314
2026-01-05 17:26:35,388: t15.2025.01.12 val PER: 0.5643
2026-01-05 17:26:35,388: t15.2025.03.14 val PER: 0.7145
2026-01-05 17:26:35,388: t15.2025.03.16 val PER: 0.5916
2026-01-05 17:26:35,388: t15.2025.03.30 val PER: 0.7161
2026-01-05 17:26:35,388: t15.2025.04.13 val PER: 0.6034
2026-01-05 17:26:35,389: New best val WER(1gram) 97.46% --> 97.21%
2026-01-05 17:26:35,389: Checkpointing model
2026-01-05 17:26:37,156: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/best_checkpoint
2026-01-05 17:26:37,439: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_1000
2026-01-05 17:26:57,069: Train batch 1200: loss: 48.11 grad norm: 85.99 time: 0.084
2026-01-05 17:27:16,660: Train batch 1400: loss: 46.98 grad norm: 89.27 time: 0.075
2026-01-05 17:27:26,345: Running test after training batch: 1500
2026-01-05 17:27:26,449: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:27:31,548: WER debug example
  GT : you can see the code at this point as well
  PR : hudy hands sies mothershead hitz this hupp dietzel
2026-01-05 17:27:31,593: WER debug example
  GT : how does it keep the cost down
  PR : handsaws hitters hing thus hasids
2026-01-05 17:27:34,370: Val batch 1500: PER (avg): 0.4673 CTC Loss (avg): 49.1637 WER(1gram): 98.73% (n=64) time: 8.025
2026-01-05 17:27:34,370: WER lens: avg_true_words=6.16 avg_pred_words=4.80 max_pred_words=10
2026-01-05 17:27:34,371: t15.2023.08.13 val PER: 0.4356
2026-01-05 17:27:34,371: t15.2023.08.18 val PER: 0.4158
2026-01-05 17:27:34,371: t15.2023.08.20 val PER: 0.4114
2026-01-05 17:27:34,371: t15.2023.08.25 val PER: 0.3750
2026-01-05 17:27:34,371: t15.2023.08.27 val PER: 0.4823
2026-01-05 17:27:34,371: t15.2023.09.01 val PER: 0.3994
2026-01-05 17:27:34,371: t15.2023.09.03 val PER: 0.4929
2026-01-05 17:27:34,371: t15.2023.09.24 val PER: 0.3944
2026-01-05 17:27:34,371: t15.2023.09.29 val PER: 0.4142
2026-01-05 17:27:34,372: t15.2023.10.01 val PER: 0.4736
2026-01-05 17:27:34,372: t15.2023.10.06 val PER: 0.3854
2026-01-05 17:27:34,372: t15.2023.10.08 val PER: 0.5304
2026-01-05 17:27:34,372: t15.2023.10.13 val PER: 0.5283
2026-01-05 17:27:34,372: t15.2023.10.15 val PER: 0.4436
2026-01-05 17:27:34,372: t15.2023.10.20 val PER: 0.4027
2026-01-05 17:27:34,372: t15.2023.10.22 val PER: 0.3976
2026-01-05 17:27:34,372: t15.2023.11.03 val PER: 0.4600
2026-01-05 17:27:34,372: t15.2023.11.04 val PER: 0.2116
2026-01-05 17:27:34,372: t15.2023.11.17 val PER: 0.2830
2026-01-05 17:27:34,372: t15.2023.11.19 val PER: 0.2854
2026-01-05 17:27:34,372: t15.2023.11.26 val PER: 0.5130
2026-01-05 17:27:34,372: t15.2023.12.03 val PER: 0.4569
2026-01-05 17:27:34,373: t15.2023.12.08 val PER: 0.4441
2026-01-05 17:27:34,373: t15.2023.12.10 val PER: 0.3890
2026-01-05 17:27:34,373: t15.2023.12.17 val PER: 0.4532
2026-01-05 17:27:34,373: t15.2023.12.29 val PER: 0.4708
2026-01-05 17:27:34,373: t15.2024.02.25 val PER: 0.4199
2026-01-05 17:27:34,373: t15.2024.03.08 val PER: 0.5420
2026-01-05 17:27:34,373: t15.2024.03.15 val PER: 0.4947
2026-01-05 17:27:34,373: t15.2024.03.17 val PER: 0.4575
2026-01-05 17:27:34,373: t15.2024.05.10 val PER: 0.4785
2026-01-05 17:27:34,373: t15.2024.06.14 val PER: 0.5079
2026-01-05 17:27:34,373: t15.2024.07.19 val PER: 0.6032
2026-01-05 17:27:34,373: t15.2024.07.21 val PER: 0.4200
2026-01-05 17:27:34,373: t15.2024.07.28 val PER: 0.4566
2026-01-05 17:27:34,373: t15.2025.01.10 val PER: 0.6791
2026-01-05 17:27:34,373: t15.2025.01.12 val PER: 0.5058
2026-01-05 17:27:34,374: t15.2025.03.14 val PER: 0.6701
2026-01-05 17:27:34,374: t15.2025.03.16 val PER: 0.5092
2026-01-05 17:27:34,374: t15.2025.03.30 val PER: 0.6874
2026-01-05 17:27:34,374: t15.2025.04.13 val PER: 0.5478
2026-01-05 17:27:34,645: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_1500
2026-01-05 17:27:44,172: Train batch 1600: loss: 46.70 grad norm: 80.16 time: 0.079
2026-01-05 17:28:02,854: Train batch 1800: loss: 43.33 grad norm: 76.63 time: 0.110
2026-01-05 17:28:22,246: Train batch 2000: loss: 43.26 grad norm: 77.26 time: 0.084
2026-01-05 17:28:22,247: Running test after training batch: 2000
2026-01-05 17:28:22,383: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:28:27,444: WER debug example
  GT : you can see the code at this point as well
  PR : hudy hintze sies stutters hodes hintze this hoyt his files
2026-01-05 17:28:27,488: WER debug example
  GT : how does it keep the cost down
  PR : diehards spawns hitters heaping that hasids
2026-01-05 17:28:30,467: Val batch 2000: PER (avg): 0.4226 CTC Loss (avg): 44.3207 WER(1gram): 101.02% (n=64) time: 8.220
2026-01-05 17:28:30,467: WER lens: avg_true_words=6.16 avg_pred_words=5.61 max_pred_words=12
2026-01-05 17:28:30,468: t15.2023.08.13 val PER: 0.4002
2026-01-05 17:28:30,468: t15.2023.08.18 val PER: 0.3671
2026-01-05 17:28:30,468: t15.2023.08.20 val PER: 0.3789
2026-01-05 17:28:30,468: t15.2023.08.25 val PER: 0.3328
2026-01-05 17:28:30,468: t15.2023.08.27 val PER: 0.4566
2026-01-05 17:28:30,468: t15.2023.09.01 val PER: 0.3393
2026-01-05 17:28:30,468: t15.2023.09.03 val PER: 0.4537
2026-01-05 17:28:30,468: t15.2023.09.24 val PER: 0.3677
2026-01-05 17:28:30,468: t15.2023.09.29 val PER: 0.3650
2026-01-05 17:28:30,468: t15.2023.10.01 val PER: 0.4326
2026-01-05 17:28:30,469: t15.2023.10.06 val PER: 0.3520
2026-01-05 17:28:30,469: t15.2023.10.08 val PER: 0.4696
2026-01-05 17:28:30,469: t15.2023.10.13 val PER: 0.5066
2026-01-05 17:28:30,469: t15.2023.10.15 val PER: 0.4100
2026-01-05 17:28:30,469: t15.2023.10.20 val PER: 0.3826
2026-01-05 17:28:30,469: t15.2023.10.22 val PER: 0.3608
2026-01-05 17:28:30,469: t15.2023.11.03 val PER: 0.4016
2026-01-05 17:28:30,469: t15.2023.11.04 val PER: 0.2082
2026-01-05 17:28:30,469: t15.2023.11.17 val PER: 0.2566
2026-01-05 17:28:30,469: t15.2023.11.19 val PER: 0.2655
2026-01-05 17:28:30,469: t15.2023.11.26 val PER: 0.4812
2026-01-05 17:28:30,469: t15.2023.12.03 val PER: 0.4307
2026-01-05 17:28:30,469: t15.2023.12.08 val PER: 0.4075
2026-01-05 17:28:30,469: t15.2023.12.10 val PER: 0.3640
2026-01-05 17:28:30,470: t15.2023.12.17 val PER: 0.3929
2026-01-05 17:28:30,470: t15.2023.12.29 val PER: 0.4118
2026-01-05 17:28:30,470: t15.2024.02.25 val PER: 0.3947
2026-01-05 17:28:30,470: t15.2024.03.08 val PER: 0.4765
2026-01-05 17:28:30,470: t15.2024.03.15 val PER: 0.4284
2026-01-05 17:28:30,470: t15.2024.03.17 val PER: 0.4275
2026-01-05 17:28:30,470: t15.2024.05.10 val PER: 0.4413
2026-01-05 17:28:30,470: t15.2024.06.14 val PER: 0.4306
2026-01-05 17:28:30,470: t15.2024.07.19 val PER: 0.5267
2026-01-05 17:28:30,470: t15.2024.07.21 val PER: 0.3883
2026-01-05 17:28:30,470: t15.2024.07.28 val PER: 0.4081
2026-01-05 17:28:30,470: t15.2025.01.10 val PER: 0.6061
2026-01-05 17:28:30,470: t15.2025.01.12 val PER: 0.4696
2026-01-05 17:28:30,470: t15.2025.03.14 val PER: 0.5858
2026-01-05 17:28:30,470: t15.2025.03.16 val PER: 0.4673
2026-01-05 17:28:30,470: t15.2025.03.30 val PER: 0.6149
2026-01-05 17:28:30,470: t15.2025.04.13 val PER: 0.4836
2026-01-05 17:28:30,759: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_2000
2026-01-05 17:28:50,214: Train batch 2200: loss: 39.24 grad norm: 77.93 time: 0.074
2026-01-05 17:29:09,943: Train batch 2400: loss: 37.75 grad norm: 67.71 time: 0.068
2026-01-05 17:29:19,809: Running test after training batch: 2500
2026-01-05 17:29:19,952: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:29:25,418: WER debug example
  GT : you can see the code at this point as well
  PR : hudy hintze sies the toehold hatz this hoyt his
2026-01-05 17:29:25,480: WER debug example
  GT : how does it keep the cost down
  PR : diehards spins hitting hing that hust snitzer
2026-01-05 17:29:28,706: Val batch 2500: PER (avg): 0.3998 CTC Loss (avg): 41.0618 WER(1gram): 101.78% (n=64) time: 8.897
2026-01-05 17:29:28,707: WER lens: avg_true_words=6.16 avg_pred_words=5.70 max_pred_words=11
2026-01-05 17:29:28,707: t15.2023.08.13 val PER: 0.3690
2026-01-05 17:29:28,707: t15.2023.08.18 val PER: 0.3512
2026-01-05 17:29:28,708: t15.2023.08.20 val PER: 0.3479
2026-01-05 17:29:28,708: t15.2023.08.25 val PER: 0.3163
2026-01-05 17:29:28,708: t15.2023.08.27 val PER: 0.4228
2026-01-05 17:29:28,708: t15.2023.09.01 val PER: 0.3295
2026-01-05 17:29:28,708: t15.2023.09.03 val PER: 0.4252
2026-01-05 17:29:28,708: t15.2023.09.24 val PER: 0.3350
2026-01-05 17:29:28,708: t15.2023.09.29 val PER: 0.3465
2026-01-05 17:29:28,709: t15.2023.10.01 val PER: 0.4042
2026-01-05 17:29:28,709: t15.2023.10.06 val PER: 0.3208
2026-01-05 17:29:28,709: t15.2023.10.08 val PER: 0.4574
2026-01-05 17:29:28,709: t15.2023.10.13 val PER: 0.4748
2026-01-05 17:29:28,709: t15.2023.10.15 val PER: 0.3738
2026-01-05 17:29:28,709: t15.2023.10.20 val PER: 0.3658
2026-01-05 17:29:28,709: t15.2023.10.22 val PER: 0.3486
2026-01-05 17:29:28,709: t15.2023.11.03 val PER: 0.3969
2026-01-05 17:29:28,709: t15.2023.11.04 val PER: 0.1741
2026-01-05 17:29:28,710: t15.2023.11.17 val PER: 0.2333
2026-01-05 17:29:28,710: t15.2023.11.19 val PER: 0.2335
2026-01-05 17:29:28,710: t15.2023.11.26 val PER: 0.4594
2026-01-05 17:29:28,710: t15.2023.12.03 val PER: 0.3981
2026-01-05 17:29:28,710: t15.2023.12.08 val PER: 0.3868
2026-01-05 17:29:28,710: t15.2023.12.10 val PER: 0.3246
2026-01-05 17:29:28,710: t15.2023.12.17 val PER: 0.3805
2026-01-05 17:29:28,710: t15.2023.12.29 val PER: 0.3981
2026-01-05 17:29:28,710: t15.2024.02.25 val PER: 0.3441
2026-01-05 17:29:28,710: t15.2024.03.08 val PER: 0.4509
2026-01-05 17:29:28,711: t15.2024.03.15 val PER: 0.4109
2026-01-05 17:29:28,711: t15.2024.03.17 val PER: 0.4003
2026-01-05 17:29:28,711: t15.2024.05.10 val PER: 0.3967
2026-01-05 17:29:28,711: t15.2024.06.14 val PER: 0.3801
2026-01-05 17:29:28,711: t15.2024.07.19 val PER: 0.5208
2026-01-05 17:29:28,711: t15.2024.07.21 val PER: 0.3538
2026-01-05 17:29:28,711: t15.2024.07.28 val PER: 0.3735
2026-01-05 17:29:28,711: t15.2025.01.10 val PER: 0.6061
2026-01-05 17:29:28,711: t15.2025.01.12 val PER: 0.4565
2026-01-05 17:29:28,712: t15.2025.03.14 val PER: 0.5695
2026-01-05 17:29:28,712: t15.2025.03.16 val PER: 0.4503
2026-01-05 17:29:28,712: t15.2025.03.30 val PER: 0.6103
2026-01-05 17:29:28,712: t15.2025.04.13 val PER: 0.4807
2026-01-05 17:29:29,011: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_2500
2026-01-05 17:29:38,545: Train batch 2600: loss: 43.86 grad norm: 112.84 time: 0.068
2026-01-05 17:29:57,860: Train batch 2800: loss: 34.42 grad norm: 69.43 time: 0.102
2026-01-05 17:30:16,790: Train batch 3000: loss: 36.24 grad norm: 70.03 time: 0.103
2026-01-05 17:30:16,791: Running test after training batch: 3000
2026-01-05 17:30:16,888: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:30:22,574: WER debug example
  GT : you can see the code at this point as well
  PR : hudy hintz jesus that hodes hatt cyst huppert his
2026-01-05 17:30:22,623: WER debug example
  GT : how does it keep the cost down
  PR : holds spurs hitty hingst the truscott setzer
2026-01-05 17:30:25,665: Val batch 3000: PER (avg): 0.3692 CTC Loss (avg): 37.7375 WER(1gram): 103.30% (n=64) time: 8.874
2026-01-05 17:30:25,665: WER lens: avg_true_words=6.16 avg_pred_words=5.94 max_pred_words=12
2026-01-05 17:30:25,666: t15.2023.08.13 val PER: 0.3524
2026-01-05 17:30:25,666: t15.2023.08.18 val PER: 0.3345
2026-01-05 17:30:25,666: t15.2023.08.20 val PER: 0.3137
2026-01-05 17:30:25,666: t15.2023.08.25 val PER: 0.2937
2026-01-05 17:30:25,667: t15.2023.08.27 val PER: 0.4180
2026-01-05 17:30:25,667: t15.2023.09.01 val PER: 0.3003
2026-01-05 17:30:25,667: t15.2023.09.03 val PER: 0.3836
2026-01-05 17:30:25,667: t15.2023.09.24 val PER: 0.3083
2026-01-05 17:30:25,667: t15.2023.09.29 val PER: 0.3127
2026-01-05 17:30:25,667: t15.2023.10.01 val PER: 0.3666
2026-01-05 17:30:25,667: t15.2023.10.06 val PER: 0.3079
2026-01-05 17:30:25,667: t15.2023.10.08 val PER: 0.4208
2026-01-05 17:30:25,667: t15.2023.10.13 val PER: 0.4383
2026-01-05 17:30:25,668: t15.2023.10.15 val PER: 0.3540
2026-01-05 17:30:25,668: t15.2023.10.20 val PER: 0.3356
2026-01-05 17:30:25,668: t15.2023.10.22 val PER: 0.3096
2026-01-05 17:30:25,668: t15.2023.11.03 val PER: 0.3623
2026-01-05 17:30:25,668: t15.2023.11.04 val PER: 0.1672
2026-01-05 17:30:25,668: t15.2023.11.17 val PER: 0.2053
2026-01-05 17:30:25,668: t15.2023.11.19 val PER: 0.2156
2026-01-05 17:30:25,668: t15.2023.11.26 val PER: 0.4072
2026-01-05 17:30:25,669: t15.2023.12.03 val PER: 0.3519
2026-01-05 17:30:25,669: t15.2023.12.08 val PER: 0.3529
2026-01-05 17:30:25,669: t15.2023.12.10 val PER: 0.2996
2026-01-05 17:30:25,669: t15.2023.12.17 val PER: 0.3638
2026-01-05 17:30:25,669: t15.2023.12.29 val PER: 0.3596
2026-01-05 17:30:25,669: t15.2024.02.25 val PER: 0.3343
2026-01-05 17:30:25,669: t15.2024.03.08 val PER: 0.4509
2026-01-05 17:30:25,669: t15.2024.03.15 val PER: 0.3890
2026-01-05 17:30:25,670: t15.2024.03.17 val PER: 0.3787
2026-01-05 17:30:25,670: t15.2024.05.10 val PER: 0.3789
2026-01-05 17:30:25,670: t15.2024.06.14 val PER: 0.3738
2026-01-05 17:30:25,670: t15.2024.07.19 val PER: 0.4661
2026-01-05 17:30:25,670: t15.2024.07.21 val PER: 0.2993
2026-01-05 17:30:25,670: t15.2024.07.28 val PER: 0.3500
2026-01-05 17:30:25,670: t15.2025.01.10 val PER: 0.5510
2026-01-05 17:30:25,670: t15.2025.01.12 val PER: 0.4242
2026-01-05 17:30:25,670: t15.2025.03.14 val PER: 0.5680
2026-01-05 17:30:25,670: t15.2025.03.16 val PER: 0.4031
2026-01-05 17:30:25,670: t15.2025.03.30 val PER: 0.5586
2026-01-05 17:30:25,671: t15.2025.04.13 val PER: 0.4494
2026-01-05 17:30:25,952: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_3000
2026-01-05 17:30:45,580: Train batch 3200: loss: 31.23 grad norm: 65.13 time: 0.095
2026-01-05 17:31:06,060: Train batch 3400: loss: 28.21 grad norm: 61.07 time: 0.060
2026-01-05 17:31:16,098: Running test after training batch: 3500
2026-01-05 17:31:16,238: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:31:21,951: WER debug example
  GT : you can see the code at this point as well
  PR : hudy hantz easiest headhunters cyst hoyt his hwe
2026-01-05 17:31:21,984: WER debug example
  GT : how does it keep the cost down
  PR : holds sisti hitty hix that setzer
2026-01-05 17:31:24,004: Val batch 3500: PER (avg): 0.3590 CTC Loss (avg): 36.3014 WER(1gram): 103.30% (n=64) time: 7.906
2026-01-05 17:31:24,005: WER lens: avg_true_words=6.16 avg_pred_words=5.80 max_pred_words=11
2026-01-05 17:31:24,005: t15.2023.08.13 val PER: 0.3295
2026-01-05 17:31:24,005: t15.2023.08.18 val PER: 0.3252
2026-01-05 17:31:24,005: t15.2023.08.20 val PER: 0.3145
2026-01-05 17:31:24,005: t15.2023.08.25 val PER: 0.2696
2026-01-05 17:31:24,005: t15.2023.08.27 val PER: 0.3971
2026-01-05 17:31:24,005: t15.2023.09.01 val PER: 0.2955
2026-01-05 17:31:24,005: t15.2023.09.03 val PER: 0.3789
2026-01-05 17:31:24,005: t15.2023.09.24 val PER: 0.3010
2026-01-05 17:31:24,006: t15.2023.09.29 val PER: 0.3153
2026-01-05 17:31:24,006: t15.2023.10.01 val PER: 0.3633
2026-01-05 17:31:24,006: t15.2023.10.06 val PER: 0.2939
2026-01-05 17:31:24,006: t15.2023.10.08 val PER: 0.4290
2026-01-05 17:31:24,006: t15.2023.10.13 val PER: 0.4259
2026-01-05 17:31:24,006: t15.2023.10.15 val PER: 0.3375
2026-01-05 17:31:24,006: t15.2023.10.20 val PER: 0.3322
2026-01-05 17:31:24,006: t15.2023.10.22 val PER: 0.3118
2026-01-05 17:31:24,006: t15.2023.11.03 val PER: 0.3480
2026-01-05 17:31:24,006: t15.2023.11.04 val PER: 0.1468
2026-01-05 17:31:24,006: t15.2023.11.17 val PER: 0.1882
2026-01-05 17:31:24,006: t15.2023.11.19 val PER: 0.1996
2026-01-05 17:31:24,006: t15.2023.11.26 val PER: 0.3862
2026-01-05 17:31:24,006: t15.2023.12.03 val PER: 0.3393
2026-01-05 17:31:24,006: t15.2023.12.08 val PER: 0.3276
2026-01-05 17:31:24,007: t15.2023.12.10 val PER: 0.2904
2026-01-05 17:31:24,007: t15.2023.12.17 val PER: 0.3378
2026-01-05 17:31:24,007: t15.2023.12.29 val PER: 0.3507
2026-01-05 17:31:24,007: t15.2024.02.25 val PER: 0.2992
2026-01-05 17:31:24,007: t15.2024.03.08 val PER: 0.4339
2026-01-05 17:31:24,007: t15.2024.03.15 val PER: 0.3771
2026-01-05 17:31:24,007: t15.2024.03.17 val PER: 0.3745
2026-01-05 17:31:24,007: t15.2024.05.10 val PER: 0.3730
2026-01-05 17:31:24,007: t15.2024.06.14 val PER: 0.3596
2026-01-05 17:31:24,007: t15.2024.07.19 val PER: 0.4568
2026-01-05 17:31:24,007: t15.2024.07.21 val PER: 0.3034
2026-01-05 17:31:24,007: t15.2024.07.28 val PER: 0.3603
2026-01-05 17:31:24,007: t15.2025.01.10 val PER: 0.5331
2026-01-05 17:31:24,007: t15.2025.01.12 val PER: 0.4149
2026-01-05 17:31:24,008: t15.2025.03.14 val PER: 0.5385
2026-01-05 17:31:24,008: t15.2025.03.16 val PER: 0.4136
2026-01-05 17:31:24,008: t15.2025.03.30 val PER: 0.5529
2026-01-05 17:31:24,008: t15.2025.04.13 val PER: 0.4265
2026-01-05 17:31:24,286: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_3500
2026-01-05 17:31:33,674: Train batch 3600: loss: 31.08 grad norm: 71.95 time: 0.083
2026-01-05 17:31:52,556: Train batch 3800: loss: 34.59 grad norm: 73.63 time: 0.083
2026-01-05 17:32:11,711: Train batch 4000: loss: 27.73 grad norm: 63.82 time: 0.069
2026-01-05 17:32:11,712: Running test after training batch: 4000
2026-01-05 17:32:11,859: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:32:17,082: WER debug example
  GT : you can see the code at this point as well
  PR : hudy hantz eats the toehold hatt cyst huppert his
2026-01-05 17:32:17,122: WER debug example
  GT : how does it keep the cost down
  PR : howls stutz hitty hix the teast setzer
2026-01-05 17:32:19,345: Val batch 4000: PER (avg): 0.3346 CTC Loss (avg): 34.5175 WER(1gram): 103.81% (n=64) time: 7.632
2026-01-05 17:32:19,345: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 17:32:19,345: t15.2023.08.13 val PER: 0.3056
2026-01-05 17:32:19,346: t15.2023.08.18 val PER: 0.3110
2026-01-05 17:32:19,346: t15.2023.08.20 val PER: 0.2907
2026-01-05 17:32:19,346: t15.2023.08.25 val PER: 0.2831
2026-01-05 17:32:19,346: t15.2023.08.27 val PER: 0.3650
2026-01-05 17:32:19,346: t15.2023.09.01 val PER: 0.2898
2026-01-05 17:32:19,346: t15.2023.09.03 val PER: 0.3527
2026-01-05 17:32:19,346: t15.2023.09.24 val PER: 0.2888
2026-01-05 17:32:19,347: t15.2023.09.29 val PER: 0.2929
2026-01-05 17:32:19,347: t15.2023.10.01 val PER: 0.3329
2026-01-05 17:32:19,347: t15.2023.10.06 val PER: 0.2702
2026-01-05 17:32:19,347: t15.2023.10.08 val PER: 0.4195
2026-01-05 17:32:19,347: t15.2023.10.13 val PER: 0.4050
2026-01-05 17:32:19,347: t15.2023.10.15 val PER: 0.3276
2026-01-05 17:32:19,347: t15.2023.10.20 val PER: 0.3389
2026-01-05 17:32:19,347: t15.2023.10.22 val PER: 0.2940
2026-01-05 17:32:19,347: t15.2023.11.03 val PER: 0.3161
2026-01-05 17:32:19,348: t15.2023.11.04 val PER: 0.1399
2026-01-05 17:32:19,348: t15.2023.11.17 val PER: 0.1804
2026-01-05 17:32:19,348: t15.2023.11.19 val PER: 0.1976
2026-01-05 17:32:19,348: t15.2023.11.26 val PER: 0.3580
2026-01-05 17:32:19,348: t15.2023.12.03 val PER: 0.3130
2026-01-05 17:32:19,348: t15.2023.12.08 val PER: 0.3036
2026-01-05 17:32:19,348: t15.2023.12.10 val PER: 0.2641
2026-01-05 17:32:19,348: t15.2023.12.17 val PER: 0.3212
2026-01-05 17:32:19,349: t15.2023.12.29 val PER: 0.3137
2026-01-05 17:32:19,349: t15.2024.02.25 val PER: 0.3020
2026-01-05 17:32:19,349: t15.2024.03.08 val PER: 0.3883
2026-01-05 17:32:19,349: t15.2024.03.15 val PER: 0.3533
2026-01-05 17:32:19,349: t15.2024.03.17 val PER: 0.3389
2026-01-05 17:32:19,349: t15.2024.05.10 val PER: 0.3536
2026-01-05 17:32:19,349: t15.2024.06.14 val PER: 0.3391
2026-01-05 17:32:19,349: t15.2024.07.19 val PER: 0.4173
2026-01-05 17:32:19,349: t15.2024.07.21 val PER: 0.2752
2026-01-05 17:32:19,349: t15.2024.07.28 val PER: 0.3132
2026-01-05 17:32:19,349: t15.2025.01.10 val PER: 0.4862
2026-01-05 17:32:19,350: t15.2025.01.12 val PER: 0.3818
2026-01-05 17:32:19,350: t15.2025.03.14 val PER: 0.5222
2026-01-05 17:32:19,350: t15.2025.03.16 val PER: 0.4071
2026-01-05 17:32:19,350: t15.2025.03.30 val PER: 0.4747
2026-01-05 17:32:19,350: t15.2025.04.13 val PER: 0.4137
2026-01-05 17:32:19,636: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_4000
2026-01-05 17:32:39,553: Train batch 4200: loss: 31.78 grad norm: 66.29 time: 0.099
2026-01-05 17:32:58,895: Train batch 4400: loss: 25.89 grad norm: 56.73 time: 0.082
2026-01-05 17:33:08,677: Running test after training batch: 4500
2026-01-05 17:33:08,811: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:33:14,071: WER debug example
  GT : you can see the code at this point as well
  PR : hudy hantz eats that holt hatz nist humped his hwe
2026-01-05 17:33:14,132: WER debug example
  GT : how does it keep the cost down
  PR : leasehold zietz dusty hitty hix the toasts
2026-01-05 17:33:16,475: Val batch 4500: PER (avg): 0.3191 CTC Loss (avg): 32.7211 WER(1gram): 107.11% (n=64) time: 7.798
2026-01-05 17:33:16,476: WER lens: avg_true_words=6.16 avg_pred_words=6.52 max_pred_words=12
2026-01-05 17:33:16,476: t15.2023.08.13 val PER: 0.2931
2026-01-05 17:33:16,476: t15.2023.08.18 val PER: 0.2867
2026-01-05 17:33:16,476: t15.2023.08.20 val PER: 0.2867
2026-01-05 17:33:16,477: t15.2023.08.25 val PER: 0.2560
2026-01-05 17:33:16,477: t15.2023.08.27 val PER: 0.3553
2026-01-05 17:33:16,477: t15.2023.09.01 val PER: 0.2622
2026-01-05 17:33:16,477: t15.2023.09.03 val PER: 0.3480
2026-01-05 17:33:16,477: t15.2023.09.24 val PER: 0.2755
2026-01-05 17:33:16,477: t15.2023.09.29 val PER: 0.2872
2026-01-05 17:33:16,477: t15.2023.10.01 val PER: 0.3164
2026-01-05 17:33:16,477: t15.2023.10.06 val PER: 0.2562
2026-01-05 17:33:16,477: t15.2023.10.08 val PER: 0.3992
2026-01-05 17:33:16,477: t15.2023.10.13 val PER: 0.3918
2026-01-05 17:33:16,477: t15.2023.10.15 val PER: 0.3184
2026-01-05 17:33:16,477: t15.2023.10.20 val PER: 0.3020
2026-01-05 17:33:16,477: t15.2023.10.22 val PER: 0.2673
2026-01-05 17:33:16,477: t15.2023.11.03 val PER: 0.3250
2026-01-05 17:33:16,478: t15.2023.11.04 val PER: 0.1433
2026-01-05 17:33:16,478: t15.2023.11.17 val PER: 0.1633
2026-01-05 17:33:16,478: t15.2023.11.19 val PER: 0.1776
2026-01-05 17:33:16,478: t15.2023.11.26 val PER: 0.3486
2026-01-05 17:33:16,478: t15.2023.12.03 val PER: 0.2889
2026-01-05 17:33:16,478: t15.2023.12.08 val PER: 0.2936
2026-01-05 17:33:16,478: t15.2023.12.10 val PER: 0.2536
2026-01-05 17:33:16,478: t15.2023.12.17 val PER: 0.3108
2026-01-05 17:33:16,478: t15.2023.12.29 val PER: 0.3075
2026-01-05 17:33:16,478: t15.2024.02.25 val PER: 0.2626
2026-01-05 17:33:16,478: t15.2024.03.08 val PER: 0.3869
2026-01-05 17:33:16,479: t15.2024.03.15 val PER: 0.3315
2026-01-05 17:33:16,479: t15.2024.03.17 val PER: 0.3152
2026-01-05 17:33:16,479: t15.2024.05.10 val PER: 0.3210
2026-01-05 17:33:16,479: t15.2024.06.14 val PER: 0.3028
2026-01-05 17:33:16,479: t15.2024.07.19 val PER: 0.3883
2026-01-05 17:33:16,479: t15.2024.07.21 val PER: 0.2510
2026-01-05 17:33:16,479: t15.2024.07.28 val PER: 0.3110
2026-01-05 17:33:16,479: t15.2025.01.10 val PER: 0.4807
2026-01-05 17:33:16,479: t15.2025.01.12 val PER: 0.3695
2026-01-05 17:33:16,479: t15.2025.03.14 val PER: 0.4586
2026-01-05 17:33:16,479: t15.2025.03.16 val PER: 0.3861
2026-01-05 17:33:16,479: t15.2025.03.30 val PER: 0.4828
2026-01-05 17:33:16,479: t15.2025.04.13 val PER: 0.3980
2026-01-05 17:33:16,760: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_4500
2026-01-05 17:33:26,089: Train batch 4600: loss: 27.76 grad norm: 68.06 time: 0.078
2026-01-05 17:33:45,138: Train batch 4800: loss: 22.69 grad norm: 59.66 time: 0.079
2026-01-05 17:34:04,462: Train batch 5000: loss: 39.53 grad norm: 86.93 time: 0.080
2026-01-05 17:34:04,462: Running test after training batch: 5000
2026-01-05 17:34:04,588: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:34:09,974: WER debug example
  GT : you can see the code at this point as well
  PR : hudy hantz ease the tadych colt hatt cysts points his swells
2026-01-05 17:34:10,014: WER debug example
  GT : how does it keep the cost down
  PR : postholes stutz hitt hix the toasts
2026-01-05 17:34:12,188: Val batch 5000: PER (avg): 0.3093 CTC Loss (avg): 31.4142 WER(1gram): 104.57% (n=64) time: 7.726
2026-01-05 17:34:12,189: WER lens: avg_true_words=6.16 avg_pred_words=6.33 max_pred_words=12
2026-01-05 17:34:12,189: t15.2023.08.13 val PER: 0.2859
2026-01-05 17:34:12,189: t15.2023.08.18 val PER: 0.2825
2026-01-05 17:34:12,190: t15.2023.08.20 val PER: 0.2756
2026-01-05 17:34:12,190: t15.2023.08.25 val PER: 0.2425
2026-01-05 17:34:12,190: t15.2023.08.27 val PER: 0.3392
2026-01-05 17:34:12,190: t15.2023.09.01 val PER: 0.2362
2026-01-05 17:34:12,190: t15.2023.09.03 val PER: 0.3444
2026-01-05 17:34:12,190: t15.2023.09.24 val PER: 0.2755
2026-01-05 17:34:12,190: t15.2023.09.29 val PER: 0.2757
2026-01-05 17:34:12,190: t15.2023.10.01 val PER: 0.3124
2026-01-05 17:34:12,190: t15.2023.10.06 val PER: 0.2336
2026-01-05 17:34:12,190: t15.2023.10.08 val PER: 0.3857
2026-01-05 17:34:12,191: t15.2023.10.13 val PER: 0.3732
2026-01-05 17:34:12,191: t15.2023.10.15 val PER: 0.3019
2026-01-05 17:34:12,191: t15.2023.10.20 val PER: 0.2785
2026-01-05 17:34:12,191: t15.2023.10.22 val PER: 0.2528
2026-01-05 17:34:12,191: t15.2023.11.03 val PER: 0.3019
2026-01-05 17:34:12,191: t15.2023.11.04 val PER: 0.1570
2026-01-05 17:34:12,191: t15.2023.11.17 val PER: 0.1649
2026-01-05 17:34:12,191: t15.2023.11.19 val PER: 0.1597
2026-01-05 17:34:12,191: t15.2023.11.26 val PER: 0.3428
2026-01-05 17:34:12,192: t15.2023.12.03 val PER: 0.2889
2026-01-05 17:34:12,192: t15.2023.12.08 val PER: 0.2843
2026-01-05 17:34:12,192: t15.2023.12.10 val PER: 0.2562
2026-01-05 17:34:12,192: t15.2023.12.17 val PER: 0.2973
2026-01-05 17:34:12,192: t15.2023.12.29 val PER: 0.2944
2026-01-05 17:34:12,192: t15.2024.02.25 val PER: 0.2893
2026-01-05 17:34:12,192: t15.2024.03.08 val PER: 0.3770
2026-01-05 17:34:12,192: t15.2024.03.15 val PER: 0.3346
2026-01-05 17:34:12,192: t15.2024.03.17 val PER: 0.3152
2026-01-05 17:34:12,192: t15.2024.05.10 val PER: 0.3091
2026-01-05 17:34:12,192: t15.2024.06.14 val PER: 0.3186
2026-01-05 17:34:12,192: t15.2024.07.19 val PER: 0.3784
2026-01-05 17:34:12,193: t15.2024.07.21 val PER: 0.2379
2026-01-05 17:34:12,193: t15.2024.07.28 val PER: 0.2956
2026-01-05 17:34:12,193: t15.2025.01.10 val PER: 0.4601
2026-01-05 17:34:12,193: t15.2025.01.12 val PER: 0.3518
2026-01-05 17:34:12,193: t15.2025.03.14 val PER: 0.4719
2026-01-05 17:34:12,193: t15.2025.03.16 val PER: 0.3639
2026-01-05 17:34:12,193: t15.2025.03.30 val PER: 0.4575
2026-01-05 17:34:12,193: t15.2025.04.13 val PER: 0.3866
2026-01-05 17:34:12,475: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_5000
2026-01-05 17:34:31,349: Train batch 5200: loss: 25.38 grad norm: 69.74 time: 0.064
2026-01-05 17:34:50,418: Train batch 5400: loss: 26.31 grad norm: 65.05 time: 0.085
2026-01-05 17:34:59,850: Running test after training batch: 5500
2026-01-05 17:35:00,049: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:35:05,061: WER debug example
  GT : you can see the code at this point as well
  PR : hudy hantz eakes the tadych colt hatt cysts points his hwe
2026-01-05 17:35:05,092: WER debug example
  GT : how does it keep the cost down
  PR : shead stacy hitty hix the tsetse
2026-01-05 17:35:06,901: Val batch 5500: PER (avg): 0.3015 CTC Loss (avg): 30.2045 WER(1gram): 105.33% (n=64) time: 7.051
2026-01-05 17:35:06,901: WER lens: avg_true_words=6.16 avg_pred_words=6.39 max_pred_words=12
2026-01-05 17:35:06,901: t15.2023.08.13 val PER: 0.2827
2026-01-05 17:35:06,901: t15.2023.08.18 val PER: 0.2875
2026-01-05 17:35:06,902: t15.2023.08.20 val PER: 0.2756
2026-01-05 17:35:06,902: t15.2023.08.25 val PER: 0.2470
2026-01-05 17:35:06,902: t15.2023.08.27 val PER: 0.3360
2026-01-05 17:35:06,902: t15.2023.09.01 val PER: 0.2346
2026-01-05 17:35:06,902: t15.2023.09.03 val PER: 0.3456
2026-01-05 17:35:06,902: t15.2023.09.24 val PER: 0.2609
2026-01-05 17:35:06,902: t15.2023.09.29 val PER: 0.2629
2026-01-05 17:35:06,902: t15.2023.10.01 val PER: 0.3151
2026-01-05 17:35:06,903: t15.2023.10.06 val PER: 0.2390
2026-01-05 17:35:06,903: t15.2023.10.08 val PER: 0.3857
2026-01-05 17:35:06,903: t15.2023.10.13 val PER: 0.3693
2026-01-05 17:35:06,903: t15.2023.10.15 val PER: 0.3013
2026-01-05 17:35:06,903: t15.2023.10.20 val PER: 0.2886
2026-01-05 17:35:06,903: t15.2023.10.22 val PER: 0.2550
2026-01-05 17:35:06,903: t15.2023.11.03 val PER: 0.3087
2026-01-05 17:35:06,903: t15.2023.11.04 val PER: 0.1365
2026-01-05 17:35:06,903: t15.2023.11.17 val PER: 0.1680
2026-01-05 17:35:06,903: t15.2023.11.19 val PER: 0.1756
2026-01-05 17:35:06,903: t15.2023.11.26 val PER: 0.3312
2026-01-05 17:35:06,903: t15.2023.12.03 val PER: 0.2836
2026-01-05 17:35:06,903: t15.2023.12.08 val PER: 0.2723
2026-01-05 17:35:06,904: t15.2023.12.10 val PER: 0.2392
2026-01-05 17:35:06,904: t15.2023.12.17 val PER: 0.2827
2026-01-05 17:35:06,904: t15.2023.12.29 val PER: 0.2876
2026-01-05 17:35:06,904: t15.2024.02.25 val PER: 0.2528
2026-01-05 17:35:06,904: t15.2024.03.08 val PER: 0.3698
2026-01-05 17:35:06,904: t15.2024.03.15 val PER: 0.3208
2026-01-05 17:35:06,904: t15.2024.03.17 val PER: 0.3103
2026-01-05 17:35:06,904: t15.2024.05.10 val PER: 0.2972
2026-01-05 17:35:06,904: t15.2024.06.14 val PER: 0.2981
2026-01-05 17:35:06,904: t15.2024.07.19 val PER: 0.3659
2026-01-05 17:35:06,904: t15.2024.07.21 val PER: 0.2179
2026-01-05 17:35:06,904: t15.2024.07.28 val PER: 0.2824
2026-01-05 17:35:06,905: t15.2025.01.10 val PER: 0.4573
2026-01-05 17:35:06,905: t15.2025.01.12 val PER: 0.3387
2026-01-05 17:35:06,905: t15.2025.03.14 val PER: 0.4349
2026-01-05 17:35:06,905: t15.2025.03.16 val PER: 0.3325
2026-01-05 17:35:06,905: t15.2025.03.30 val PER: 0.4471
2026-01-05 17:35:06,905: t15.2025.04.13 val PER: 0.3609
2026-01-05 17:35:07,180: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_5500
2026-01-05 17:35:17,120: Train batch 5600: loss: 27.45 grad norm: 68.08 time: 0.078
2026-01-05 17:35:36,874: Train batch 5800: loss: 21.17 grad norm: 64.34 time: 0.103
2026-01-05 17:35:56,266: Train batch 6000: loss: 21.01 grad norm: 59.26 time: 0.061
2026-01-05 17:35:56,266: Running test after training batch: 6000
2026-01-05 17:35:56,402: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:36:01,399: WER debug example
  GT : you can see the code at this point as well
  PR : hued hantz eats that holt hatz nist huh point his hwe
2026-01-05 17:36:01,426: WER debug example
  GT : how does it keep the cost down
  PR : teast hounds stutz hitt hix the teast setzer
2026-01-05 17:36:03,123: Val batch 6000: PER (avg): 0.2901 CTC Loss (avg): 29.8340 WER(1gram): 107.11% (n=64) time: 6.856
2026-01-05 17:36:03,123: WER lens: avg_true_words=6.16 avg_pred_words=6.64 max_pred_words=14
2026-01-05 17:36:03,124: t15.2023.08.13 val PER: 0.2744
2026-01-05 17:36:03,124: t15.2023.08.18 val PER: 0.2531
2026-01-05 17:36:03,124: t15.2023.08.20 val PER: 0.2629
2026-01-05 17:36:03,124: t15.2023.08.25 val PER: 0.2349
2026-01-05 17:36:03,124: t15.2023.08.27 val PER: 0.3264
2026-01-05 17:36:03,124: t15.2023.09.01 val PER: 0.2289
2026-01-05 17:36:03,124: t15.2023.09.03 val PER: 0.3219
2026-01-05 17:36:03,124: t15.2023.09.24 val PER: 0.2585
2026-01-05 17:36:03,124: t15.2023.09.29 val PER: 0.2648
2026-01-05 17:36:03,124: t15.2023.10.01 val PER: 0.2873
2026-01-05 17:36:03,124: t15.2023.10.06 val PER: 0.2260
2026-01-05 17:36:03,124: t15.2023.10.08 val PER: 0.3599
2026-01-05 17:36:03,124: t15.2023.10.13 val PER: 0.3615
2026-01-05 17:36:03,125: t15.2023.10.15 val PER: 0.2960
2026-01-05 17:36:03,125: t15.2023.10.20 val PER: 0.2685
2026-01-05 17:36:03,125: t15.2023.10.22 val PER: 0.2461
2026-01-05 17:36:03,125: t15.2023.11.03 val PER: 0.2931
2026-01-05 17:36:03,125: t15.2023.11.04 val PER: 0.1365
2026-01-05 17:36:03,125: t15.2023.11.17 val PER: 0.1509
2026-01-05 17:36:03,125: t15.2023.11.19 val PER: 0.1617
2026-01-05 17:36:03,125: t15.2023.11.26 val PER: 0.3261
2026-01-05 17:36:03,125: t15.2023.12.03 val PER: 0.2542
2026-01-05 17:36:03,125: t15.2023.12.08 val PER: 0.2597
2026-01-05 17:36:03,125: t15.2023.12.10 val PER: 0.2221
2026-01-05 17:36:03,125: t15.2023.12.17 val PER: 0.2692
2026-01-05 17:36:03,125: t15.2023.12.29 val PER: 0.2814
2026-01-05 17:36:03,125: t15.2024.02.25 val PER: 0.2416
2026-01-05 17:36:03,126: t15.2024.03.08 val PER: 0.3570
2026-01-05 17:36:03,126: t15.2024.03.15 val PER: 0.3114
2026-01-05 17:36:03,126: t15.2024.03.17 val PER: 0.2929
2026-01-05 17:36:03,126: t15.2024.05.10 val PER: 0.2808
2026-01-05 17:36:03,126: t15.2024.06.14 val PER: 0.2965
2026-01-05 17:36:03,126: t15.2024.07.19 val PER: 0.3546
2026-01-05 17:36:03,126: t15.2024.07.21 val PER: 0.2207
2026-01-05 17:36:03,126: t15.2024.07.28 val PER: 0.2654
2026-01-05 17:36:03,126: t15.2025.01.10 val PER: 0.4504
2026-01-05 17:36:03,126: t15.2025.01.12 val PER: 0.3256
2026-01-05 17:36:03,126: t15.2025.03.14 val PER: 0.4438
2026-01-05 17:36:03,126: t15.2025.03.16 val PER: 0.3272
2026-01-05 17:36:03,126: t15.2025.03.30 val PER: 0.4437
2026-01-05 17:36:03,126: t15.2025.04.13 val PER: 0.3595
2026-01-05 17:36:03,390: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_6000
2026-01-05 17:36:23,595: Train batch 6200: loss: 23.98 grad norm: 70.55 time: 0.087
2026-01-05 17:36:43,159: Train batch 6400: loss: 27.44 grad norm: 68.67 time: 0.077
2026-01-05 17:36:53,392: Running test after training batch: 6500
2026-01-05 17:36:53,496: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:36:58,496: WER debug example
  GT : you can see the code at this point as well
  PR : hued kantz eats that hoods hatt cysts points his swells
2026-01-05 17:36:58,527: WER debug example
  GT : how does it keep the cost down
  PR : soho ouzts dusts hitt heap kiess the toasts
2026-01-05 17:37:00,241: Val batch 6500: PER (avg): 0.2806 CTC Loss (avg): 28.9258 WER(1gram): 105.33% (n=64) time: 6.848
2026-01-05 17:37:00,241: WER lens: avg_true_words=6.16 avg_pred_words=6.55 max_pred_words=13
2026-01-05 17:37:00,241: t15.2023.08.13 val PER: 0.2723
2026-01-05 17:37:00,242: t15.2023.08.18 val PER: 0.2473
2026-01-05 17:37:00,242: t15.2023.08.20 val PER: 0.2566
2026-01-05 17:37:00,242: t15.2023.08.25 val PER: 0.2184
2026-01-05 17:37:00,242: t15.2023.08.27 val PER: 0.3167
2026-01-05 17:37:00,242: t15.2023.09.01 val PER: 0.2135
2026-01-05 17:37:00,242: t15.2023.09.03 val PER: 0.2969
2026-01-05 17:37:00,242: t15.2023.09.24 val PER: 0.2524
2026-01-05 17:37:00,242: t15.2023.09.29 val PER: 0.2463
2026-01-05 17:37:00,242: t15.2023.10.01 val PER: 0.2853
2026-01-05 17:37:00,242: t15.2023.10.06 val PER: 0.2196
2026-01-05 17:37:00,242: t15.2023.10.08 val PER: 0.3627
2026-01-05 17:37:00,243: t15.2023.10.13 val PER: 0.3507
2026-01-05 17:37:00,243: t15.2023.10.15 val PER: 0.2716
2026-01-05 17:37:00,243: t15.2023.10.20 val PER: 0.2819
2026-01-05 17:37:00,243: t15.2023.10.22 val PER: 0.2405
2026-01-05 17:37:00,243: t15.2023.11.03 val PER: 0.2788
2026-01-05 17:37:00,243: t15.2023.11.04 val PER: 0.1263
2026-01-05 17:37:00,243: t15.2023.11.17 val PER: 0.1462
2026-01-05 17:37:00,243: t15.2023.11.19 val PER: 0.1557
2026-01-05 17:37:00,243: t15.2023.11.26 val PER: 0.3065
2026-01-05 17:37:00,243: t15.2023.12.03 val PER: 0.2500
2026-01-05 17:37:00,243: t15.2023.12.08 val PER: 0.2517
2026-01-05 17:37:00,243: t15.2023.12.10 val PER: 0.2273
2026-01-05 17:37:00,244: t15.2023.12.17 val PER: 0.2661
2026-01-05 17:37:00,244: t15.2023.12.29 val PER: 0.2814
2026-01-05 17:37:00,244: t15.2024.02.25 val PER: 0.2303
2026-01-05 17:37:00,244: t15.2024.03.08 val PER: 0.3499
2026-01-05 17:37:00,244: t15.2024.03.15 val PER: 0.2952
2026-01-05 17:37:00,244: t15.2024.03.17 val PER: 0.2768
2026-01-05 17:37:00,244: t15.2024.05.10 val PER: 0.2779
2026-01-05 17:37:00,244: t15.2024.06.14 val PER: 0.2792
2026-01-05 17:37:00,244: t15.2024.07.19 val PER: 0.3434
2026-01-05 17:37:00,244: t15.2024.07.21 val PER: 0.2110
2026-01-05 17:37:00,244: t15.2024.07.28 val PER: 0.2691
2026-01-05 17:37:00,244: t15.2025.01.10 val PER: 0.4256
2026-01-05 17:37:00,244: t15.2025.01.12 val PER: 0.3102
2026-01-05 17:37:00,245: t15.2025.03.14 val PER: 0.4275
2026-01-05 17:37:00,245: t15.2025.03.16 val PER: 0.3272
2026-01-05 17:37:00,245: t15.2025.03.30 val PER: 0.4287
2026-01-05 17:37:00,245: t15.2025.04.13 val PER: 0.3709
2026-01-05 17:37:00,513: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_6500
2026-01-05 17:37:10,141: Train batch 6600: loss: 18.49 grad norm: 49.63 time: 0.055
2026-01-05 17:37:29,903: Train batch 6800: loss: 24.26 grad norm: 53.31 time: 0.059
2026-01-05 17:37:50,528: Train batch 7000: loss: 24.91 grad norm: 83.53 time: 0.075
2026-01-05 17:37:50,529: Running test after training batch: 7000
2026-01-05 17:37:50,634: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:37:55,839: WER debug example
  GT : you can see the code at this point as well
  PR : hued kantz eakes the toehold hatt cysts points his swells
2026-01-05 17:37:55,869: WER debug example
  GT : how does it keep the cost down
  PR : tese holmes stutz hitt hix the tsetse
2026-01-05 17:37:57,581: Val batch 7000: PER (avg): 0.2722 CTC Loss (avg): 27.8306 WER(1gram): 106.09% (n=64) time: 7.052
2026-01-05 17:37:57,581: WER lens: avg_true_words=6.16 avg_pred_words=6.58 max_pred_words=13
2026-01-05 17:37:57,582: t15.2023.08.13 val PER: 0.2505
2026-01-05 17:37:57,582: t15.2023.08.18 val PER: 0.2397
2026-01-05 17:37:57,582: t15.2023.08.20 val PER: 0.2462
2026-01-05 17:37:57,582: t15.2023.08.25 val PER: 0.2169
2026-01-05 17:37:57,582: t15.2023.08.27 val PER: 0.3055
2026-01-05 17:37:57,582: t15.2023.09.01 val PER: 0.2054
2026-01-05 17:37:57,582: t15.2023.09.03 val PER: 0.2993
2026-01-05 17:37:57,582: t15.2023.09.24 val PER: 0.2464
2026-01-05 17:37:57,583: t15.2023.09.29 val PER: 0.2457
2026-01-05 17:37:57,583: t15.2023.10.01 val PER: 0.2794
2026-01-05 17:37:57,583: t15.2023.10.06 val PER: 0.2099
2026-01-05 17:37:57,583: t15.2023.10.08 val PER: 0.3775
2026-01-05 17:37:57,583: t15.2023.10.13 val PER: 0.3429
2026-01-05 17:37:57,583: t15.2023.10.15 val PER: 0.2762
2026-01-05 17:37:57,583: t15.2023.10.20 val PER: 0.2685
2026-01-05 17:37:57,583: t15.2023.10.22 val PER: 0.2082
2026-01-05 17:37:57,583: t15.2023.11.03 val PER: 0.2720
2026-01-05 17:37:57,583: t15.2023.11.04 val PER: 0.1263
2026-01-05 17:37:57,583: t15.2023.11.17 val PER: 0.1446
2026-01-05 17:37:57,583: t15.2023.11.19 val PER: 0.1537
2026-01-05 17:37:57,583: t15.2023.11.26 val PER: 0.2986
2026-01-05 17:37:57,583: t15.2023.12.03 val PER: 0.2458
2026-01-05 17:37:57,584: t15.2023.12.08 val PER: 0.2337
2026-01-05 17:37:57,584: t15.2023.12.10 val PER: 0.2181
2026-01-05 17:37:57,584: t15.2023.12.17 val PER: 0.2536
2026-01-05 17:37:57,584: t15.2023.12.29 val PER: 0.2697
2026-01-05 17:37:57,584: t15.2024.02.25 val PER: 0.2388
2026-01-05 17:37:57,584: t15.2024.03.08 val PER: 0.3385
2026-01-05 17:37:57,584: t15.2024.03.15 val PER: 0.2952
2026-01-05 17:37:57,584: t15.2024.03.17 val PER: 0.2762
2026-01-05 17:37:57,584: t15.2024.05.10 val PER: 0.2734
2026-01-05 17:37:57,584: t15.2024.06.14 val PER: 0.2823
2026-01-05 17:37:57,584: t15.2024.07.19 val PER: 0.3276
2026-01-05 17:37:57,584: t15.2024.07.21 val PER: 0.1917
2026-01-05 17:37:57,584: t15.2024.07.28 val PER: 0.2441
2026-01-05 17:37:57,584: t15.2025.01.10 val PER: 0.4256
2026-01-05 17:37:57,584: t15.2025.01.12 val PER: 0.2910
2026-01-05 17:37:57,584: t15.2025.03.14 val PER: 0.4349
2026-01-05 17:37:57,585: t15.2025.03.16 val PER: 0.3115
2026-01-05 17:37:57,585: t15.2025.03.30 val PER: 0.4161
2026-01-05 17:37:57,585: t15.2025.04.13 val PER: 0.3438
2026-01-05 17:37:57,860: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_7000
2026-01-05 17:38:17,746: Train batch 7200: loss: 22.67 grad norm: 66.94 time: 0.098
2026-01-05 17:38:36,760: Train batch 7400: loss: 22.20 grad norm: 66.66 time: 0.094
2026-01-05 17:38:46,563: Running test after training batch: 7500
2026-01-05 17:38:46,666: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:38:51,980: WER debug example
  GT : you can see the code at this point as well
  PR : hued hantz eats the tadych colds hatt cysts points his swells
2026-01-05 17:38:52,021: WER debug example
  GT : how does it keep the cost down
  PR : assholes suss hitz heap kiess the toste setzer
2026-01-05 17:38:54,256: Val batch 7500: PER (avg): 0.2680 CTC Loss (avg): 27.6946 WER(1gram): 107.61% (n=64) time: 7.693
2026-01-05 17:38:54,256: WER lens: avg_true_words=6.16 avg_pred_words=6.69 max_pred_words=12
2026-01-05 17:38:54,257: t15.2023.08.13 val PER: 0.2484
2026-01-05 17:38:54,257: t15.2023.08.18 val PER: 0.2381
2026-01-05 17:38:54,257: t15.2023.08.20 val PER: 0.2359
2026-01-05 17:38:54,257: t15.2023.08.25 val PER: 0.2123
2026-01-05 17:38:54,257: t15.2023.08.27 val PER: 0.3006
2026-01-05 17:38:54,257: t15.2023.09.01 val PER: 0.2127
2026-01-05 17:38:54,257: t15.2023.09.03 val PER: 0.2874
2026-01-05 17:38:54,258: t15.2023.09.24 val PER: 0.2342
2026-01-05 17:38:54,258: t15.2023.09.29 val PER: 0.2361
2026-01-05 17:38:54,258: t15.2023.10.01 val PER: 0.2820
2026-01-05 17:38:54,258: t15.2023.10.06 val PER: 0.1991
2026-01-05 17:38:54,258: t15.2023.10.08 val PER: 0.3559
2026-01-05 17:38:54,258: t15.2023.10.13 val PER: 0.3382
2026-01-05 17:38:54,258: t15.2023.10.15 val PER: 0.2643
2026-01-05 17:38:54,258: t15.2023.10.20 val PER: 0.2718
2026-01-05 17:38:54,258: t15.2023.10.22 val PER: 0.2116
2026-01-05 17:38:54,258: t15.2023.11.03 val PER: 0.2754
2026-01-05 17:38:54,259: t15.2023.11.04 val PER: 0.1263
2026-01-05 17:38:54,259: t15.2023.11.17 val PER: 0.1415
2026-01-05 17:38:54,259: t15.2023.11.19 val PER: 0.1557
2026-01-05 17:38:54,259: t15.2023.11.26 val PER: 0.2949
2026-01-05 17:38:54,259: t15.2023.12.03 val PER: 0.2384
2026-01-05 17:38:54,259: t15.2023.12.08 val PER: 0.2383
2026-01-05 17:38:54,259: t15.2023.12.10 val PER: 0.2129
2026-01-05 17:38:54,259: t15.2023.12.17 val PER: 0.2536
2026-01-05 17:38:54,259: t15.2023.12.29 val PER: 0.2588
2026-01-05 17:38:54,259: t15.2024.02.25 val PER: 0.2219
2026-01-05 17:38:54,259: t15.2024.03.08 val PER: 0.3329
2026-01-05 17:38:54,259: t15.2024.03.15 val PER: 0.2864
2026-01-05 17:38:54,259: t15.2024.03.17 val PER: 0.2727
2026-01-05 17:38:54,260: t15.2024.05.10 val PER: 0.2734
2026-01-05 17:38:54,260: t15.2024.06.14 val PER: 0.2618
2026-01-05 17:38:54,260: t15.2024.07.19 val PER: 0.3250
2026-01-05 17:38:54,260: t15.2024.07.21 val PER: 0.1910
2026-01-05 17:38:54,260: t15.2024.07.28 val PER: 0.2449
2026-01-05 17:38:54,260: t15.2025.01.10 val PER: 0.4063
2026-01-05 17:38:54,260: t15.2025.01.12 val PER: 0.2972
2026-01-05 17:38:54,260: t15.2025.03.14 val PER: 0.4408
2026-01-05 17:38:54,260: t15.2025.03.16 val PER: 0.3050
2026-01-05 17:38:54,260: t15.2025.03.30 val PER: 0.4149
2026-01-05 17:38:54,260: t15.2025.04.13 val PER: 0.3452
2026-01-05 17:38:54,553: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_7500
2026-01-05 17:39:04,259: Train batch 7600: loss: 23.62 grad norm: 62.08 time: 0.088
2026-01-05 17:39:24,179: Train batch 7800: loss: 22.36 grad norm: 59.16 time: 0.068
2026-01-05 17:39:43,821: Train batch 8000: loss: 21.24 grad norm: 59.33 time: 0.094
2026-01-05 17:39:43,821: Running test after training batch: 8000
2026-01-05 17:39:43,934: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:39:48,932: WER debug example
  GT : you can see the code at this point as well
  PR : hued kantz eakes the tadych colt hatt cysts points his swells
2026-01-05 17:39:48,962: WER debug example
  GT : how does it keep the cost down
  PR : teast holmes suss hitty hix thus tsetse
2026-01-05 17:39:50,668: Val batch 8000: PER (avg): 0.2576 CTC Loss (avg): 26.7743 WER(1gram): 106.09% (n=64) time: 6.847
2026-01-05 17:39:50,669: WER lens: avg_true_words=6.16 avg_pred_words=6.61 max_pred_words=12
2026-01-05 17:39:50,669: t15.2023.08.13 val PER: 0.2443
2026-01-05 17:39:50,669: t15.2023.08.18 val PER: 0.2263
2026-01-05 17:39:50,669: t15.2023.08.20 val PER: 0.2319
2026-01-05 17:39:50,669: t15.2023.08.25 val PER: 0.2063
2026-01-05 17:39:50,669: t15.2023.08.27 val PER: 0.2862
2026-01-05 17:39:50,669: t15.2023.09.01 val PER: 0.1859
2026-01-05 17:39:50,669: t15.2023.09.03 val PER: 0.2874
2026-01-05 17:39:50,669: t15.2023.09.24 val PER: 0.2427
2026-01-05 17:39:50,669: t15.2023.09.29 val PER: 0.2323
2026-01-05 17:39:50,669: t15.2023.10.01 val PER: 0.2616
2026-01-05 17:39:50,670: t15.2023.10.06 val PER: 0.1905
2026-01-05 17:39:50,670: t15.2023.10.08 val PER: 0.3451
2026-01-05 17:39:50,670: t15.2023.10.13 val PER: 0.3344
2026-01-05 17:39:50,670: t15.2023.10.15 val PER: 0.2722
2026-01-05 17:39:50,670: t15.2023.10.20 val PER: 0.2617
2026-01-05 17:39:50,670: t15.2023.10.22 val PER: 0.1993
2026-01-05 17:39:50,670: t15.2023.11.03 val PER: 0.2632
2026-01-05 17:39:50,670: t15.2023.11.04 val PER: 0.1331
2026-01-05 17:39:50,670: t15.2023.11.17 val PER: 0.1446
2026-01-05 17:39:50,671: t15.2023.11.19 val PER: 0.1497
2026-01-05 17:39:50,671: t15.2023.11.26 val PER: 0.2877
2026-01-05 17:39:50,671: t15.2023.12.03 val PER: 0.2332
2026-01-05 17:39:50,671: t15.2023.12.08 val PER: 0.2184
2026-01-05 17:39:50,671: t15.2023.12.10 val PER: 0.2050
2026-01-05 17:39:50,671: t15.2023.12.17 val PER: 0.2391
2026-01-05 17:39:50,671: t15.2023.12.29 val PER: 0.2519
2026-01-05 17:39:50,671: t15.2024.02.25 val PER: 0.2107
2026-01-05 17:39:50,671: t15.2024.03.08 val PER: 0.3129
2026-01-05 17:39:50,671: t15.2024.03.15 val PER: 0.2702
2026-01-05 17:39:50,671: t15.2024.03.17 val PER: 0.2476
2026-01-05 17:39:50,671: t15.2024.05.10 val PER: 0.2600
2026-01-05 17:39:50,671: t15.2024.06.14 val PER: 0.2397
2026-01-05 17:39:50,672: t15.2024.07.19 val PER: 0.3210
2026-01-05 17:39:50,672: t15.2024.07.21 val PER: 0.1731
2026-01-05 17:39:50,672: t15.2024.07.28 val PER: 0.2250
2026-01-05 17:39:50,672: t15.2025.01.10 val PER: 0.4036
2026-01-05 17:39:50,672: t15.2025.01.12 val PER: 0.2771
2026-01-05 17:39:50,672: t15.2025.03.14 val PER: 0.4231
2026-01-05 17:39:50,672: t15.2025.03.16 val PER: 0.3102
2026-01-05 17:39:50,672: t15.2025.03.30 val PER: 0.4046
2026-01-05 17:39:50,672: t15.2025.04.13 val PER: 0.3381
2026-01-05 17:39:50,955: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_8000
2026-01-05 17:40:10,736: Train batch 8200: loss: 19.22 grad norm: 78.63 time: 0.067
2026-01-05 17:40:29,068: Train batch 8400: loss: 17.36 grad norm: 60.98 time: 0.079
2026-01-05 17:40:38,283: Running test after training batch: 8500
2026-01-05 17:40:38,390: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:40:43,382: WER debug example
  GT : you can see the code at this point as well
  PR : skewed huck aunts eakes the tadych colds hatz this points his swells
2026-01-05 17:40:43,412: WER debug example
  GT : how does it keep the cost down
  PR : teast holmes suss hitt hix thus setzer
2026-01-05 17:40:45,247: Val batch 8500: PER (avg): 0.2507 CTC Loss (avg): 26.2153 WER(1gram): 106.35% (n=64) time: 6.963
2026-01-05 17:40:45,247: WER lens: avg_true_words=6.16 avg_pred_words=6.53 max_pred_words=12
2026-01-05 17:40:45,247: t15.2023.08.13 val PER: 0.2401
2026-01-05 17:40:45,247: t15.2023.08.18 val PER: 0.2305
2026-01-05 17:40:45,247: t15.2023.08.20 val PER: 0.2208
2026-01-05 17:40:45,247: t15.2023.08.25 val PER: 0.2078
2026-01-05 17:40:45,248: t15.2023.08.27 val PER: 0.2846
2026-01-05 17:40:45,248: t15.2023.09.01 val PER: 0.1786
2026-01-05 17:40:45,248: t15.2023.09.03 val PER: 0.2779
2026-01-05 17:40:45,248: t15.2023.09.24 val PER: 0.2184
2026-01-05 17:40:45,248: t15.2023.09.29 val PER: 0.2259
2026-01-05 17:40:45,248: t15.2023.10.01 val PER: 0.2589
2026-01-05 17:40:45,248: t15.2023.10.06 val PER: 0.1873
2026-01-05 17:40:45,248: t15.2023.10.08 val PER: 0.3478
2026-01-05 17:40:45,248: t15.2023.10.13 val PER: 0.3204
2026-01-05 17:40:45,249: t15.2023.10.15 val PER: 0.2584
2026-01-05 17:40:45,249: t15.2023.10.20 val PER: 0.2550
2026-01-05 17:40:45,249: t15.2023.10.22 val PER: 0.2038
2026-01-05 17:40:45,249: t15.2023.11.03 val PER: 0.2659
2026-01-05 17:40:45,249: t15.2023.11.04 val PER: 0.1160
2026-01-05 17:40:45,250: t15.2023.11.17 val PER: 0.1291
2026-01-05 17:40:45,250: t15.2023.11.19 val PER: 0.1337
2026-01-05 17:40:45,250: t15.2023.11.26 val PER: 0.2667
2026-01-05 17:40:45,250: t15.2023.12.03 val PER: 0.2227
2026-01-05 17:40:45,250: t15.2023.12.08 val PER: 0.2137
2026-01-05 17:40:45,250: t15.2023.12.10 val PER: 0.1971
2026-01-05 17:40:45,250: t15.2023.12.17 val PER: 0.2245
2026-01-05 17:40:45,250: t15.2023.12.29 val PER: 0.2560
2026-01-05 17:40:45,251: t15.2024.02.25 val PER: 0.2065
2026-01-05 17:40:45,251: t15.2024.03.08 val PER: 0.3129
2026-01-05 17:40:45,251: t15.2024.03.15 val PER: 0.2695
2026-01-05 17:40:45,251: t15.2024.03.17 val PER: 0.2434
2026-01-05 17:40:45,251: t15.2024.05.10 val PER: 0.2511
2026-01-05 17:40:45,251: t15.2024.06.14 val PER: 0.2413
2026-01-05 17:40:45,251: t15.2024.07.19 val PER: 0.3065
2026-01-05 17:40:45,251: t15.2024.07.21 val PER: 0.1703
2026-01-05 17:40:45,251: t15.2024.07.28 val PER: 0.2147
2026-01-05 17:40:45,252: t15.2025.01.10 val PER: 0.3912
2026-01-05 17:40:45,252: t15.2025.01.12 val PER: 0.2694
2026-01-05 17:40:45,252: t15.2025.03.14 val PER: 0.4275
2026-01-05 17:40:45,252: t15.2025.03.16 val PER: 0.3024
2026-01-05 17:40:45,252: t15.2025.03.30 val PER: 0.3793
2026-01-05 17:40:45,252: t15.2025.04.13 val PER: 0.3252
2026-01-05 17:40:45,523: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_8500
2026-01-05 17:40:55,946: Train batch 8600: loss: 25.41 grad norm: 60.69 time: 0.068
2026-01-05 17:41:16,846: Train batch 8800: loss: 22.77 grad norm: 64.65 time: 0.074
2026-01-05 17:41:37,008: Train batch 9000: loss: 25.02 grad norm: 75.07 time: 0.090
2026-01-05 17:41:37,009: Running test after training batch: 9000
2026-01-05 17:41:37,120: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:41:42,317: WER debug example
  GT : you can see the code at this point as well
  PR : hued huck antsy zeus the toehold hatz this tahoe points his hwe
2026-01-05 17:41:42,354: WER debug example
  GT : how does it keep the cost down
  PR : soho ouzts dusts hitty heap kiess thus costs
2026-01-05 17:41:44,667: Val batch 9000: PER (avg): 0.2481 CTC Loss (avg): 25.7606 WER(1gram): 108.63% (n=64) time: 7.658
2026-01-05 17:41:44,668: WER lens: avg_true_words=6.16 avg_pred_words=6.91 max_pred_words=13
2026-01-05 17:41:44,668: t15.2023.08.13 val PER: 0.2245
2026-01-05 17:41:44,669: t15.2023.08.18 val PER: 0.2221
2026-01-05 17:41:44,669: t15.2023.08.20 val PER: 0.2232
2026-01-05 17:41:44,669: t15.2023.08.25 val PER: 0.1898
2026-01-05 17:41:44,669: t15.2023.08.27 val PER: 0.2765
2026-01-05 17:41:44,669: t15.2023.09.01 val PER: 0.1818
2026-01-05 17:41:44,669: t15.2023.09.03 val PER: 0.2910
2026-01-05 17:41:44,669: t15.2023.09.24 val PER: 0.2318
2026-01-05 17:41:44,669: t15.2023.09.29 val PER: 0.2234
2026-01-05 17:41:44,669: t15.2023.10.01 val PER: 0.2517
2026-01-05 17:41:44,669: t15.2023.10.06 val PER: 0.1819
2026-01-05 17:41:44,669: t15.2023.10.08 val PER: 0.3315
2026-01-05 17:41:44,669: t15.2023.10.13 val PER: 0.3119
2026-01-05 17:41:44,670: t15.2023.10.15 val PER: 0.2492
2026-01-05 17:41:44,670: t15.2023.10.20 val PER: 0.2450
2026-01-05 17:41:44,670: t15.2023.10.22 val PER: 0.1982
2026-01-05 17:41:44,670: t15.2023.11.03 val PER: 0.2598
2026-01-05 17:41:44,670: t15.2023.11.04 val PER: 0.1263
2026-01-05 17:41:44,670: t15.2023.11.17 val PER: 0.1306
2026-01-05 17:41:44,670: t15.2023.11.19 val PER: 0.1357
2026-01-05 17:41:44,670: t15.2023.11.26 val PER: 0.2761
2026-01-05 17:41:44,670: t15.2023.12.03 val PER: 0.2195
2026-01-05 17:41:44,670: t15.2023.12.08 val PER: 0.2077
2026-01-05 17:41:44,671: t15.2023.12.10 val PER: 0.1853
2026-01-05 17:41:44,671: t15.2023.12.17 val PER: 0.2297
2026-01-05 17:41:44,671: t15.2023.12.29 val PER: 0.2505
2026-01-05 17:41:44,671: t15.2024.02.25 val PER: 0.1938
2026-01-05 17:41:44,671: t15.2024.03.08 val PER: 0.3186
2026-01-05 17:41:44,671: t15.2024.03.15 val PER: 0.2714
2026-01-05 17:41:44,671: t15.2024.03.17 val PER: 0.2497
2026-01-05 17:41:44,671: t15.2024.05.10 val PER: 0.2541
2026-01-05 17:41:44,671: t15.2024.06.14 val PER: 0.2524
2026-01-05 17:41:44,671: t15.2024.07.19 val PER: 0.3151
2026-01-05 17:41:44,671: t15.2024.07.21 val PER: 0.1648
2026-01-05 17:41:44,671: t15.2024.07.28 val PER: 0.2110
2026-01-05 17:41:44,671: t15.2025.01.10 val PER: 0.3967
2026-01-05 17:41:44,671: t15.2025.01.12 val PER: 0.2710
2026-01-05 17:41:44,671: t15.2025.03.14 val PER: 0.4024
2026-01-05 17:41:44,672: t15.2025.03.16 val PER: 0.2906
2026-01-05 17:41:44,672: t15.2025.03.30 val PER: 0.3862
2026-01-05 17:41:44,672: t15.2025.04.13 val PER: 0.3067
2026-01-05 17:41:44,957: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_9000
2026-01-05 17:42:03,593: Train batch 9200: loss: 17.01 grad norm: 49.95 time: 0.070
2026-01-05 17:42:23,303: Train batch 9400: loss: 16.21 grad norm: 52.15 time: 0.086
2026-01-05 17:42:32,983: Running test after training batch: 9500
2026-01-05 17:42:33,085: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:42:38,640: WER debug example
  GT : you can see the code at this point as well
  PR : hued kantz eakes the toehold hatz this tahoe pointed his hwe
2026-01-05 17:42:38,677: WER debug example
  GT : how does it keep the cost down
  PR : east holmes stutz hitty heap kiess thus costs
2026-01-05 17:42:40,936: Val batch 9500: PER (avg): 0.2460 CTC Loss (avg): 25.5887 WER(1gram): 108.12% (n=64) time: 7.952
2026-01-05 17:42:40,937: WER lens: avg_true_words=6.16 avg_pred_words=6.86 max_pred_words=13
2026-01-05 17:42:40,937: t15.2023.08.13 val PER: 0.2173
2026-01-05 17:42:40,938: t15.2023.08.18 val PER: 0.2188
2026-01-05 17:42:40,938: t15.2023.08.20 val PER: 0.2248
2026-01-05 17:42:40,938: t15.2023.08.25 val PER: 0.1883
2026-01-05 17:42:40,938: t15.2023.08.27 val PER: 0.2781
2026-01-05 17:42:40,938: t15.2023.09.01 val PER: 0.1778
2026-01-05 17:42:40,938: t15.2023.09.03 val PER: 0.2803
2026-01-05 17:42:40,939: t15.2023.09.24 val PER: 0.2306
2026-01-05 17:42:40,939: t15.2023.09.29 val PER: 0.2189
2026-01-05 17:42:40,939: t15.2023.10.01 val PER: 0.2517
2026-01-05 17:42:40,939: t15.2023.10.06 val PER: 0.1948
2026-01-05 17:42:40,939: t15.2023.10.08 val PER: 0.3302
2026-01-05 17:42:40,939: t15.2023.10.13 val PER: 0.3142
2026-01-05 17:42:40,939: t15.2023.10.15 val PER: 0.2518
2026-01-05 17:42:40,939: t15.2023.10.20 val PER: 0.2550
2026-01-05 17:42:40,939: t15.2023.10.22 val PER: 0.2049
2026-01-05 17:42:40,939: t15.2023.11.03 val PER: 0.2585
2026-01-05 17:42:40,940: t15.2023.11.04 val PER: 0.1263
2026-01-05 17:42:40,940: t15.2023.11.17 val PER: 0.1384
2026-01-05 17:42:40,940: t15.2023.11.19 val PER: 0.1477
2026-01-05 17:42:40,940: t15.2023.11.26 val PER: 0.2710
2026-01-05 17:42:40,940: t15.2023.12.03 val PER: 0.2321
2026-01-05 17:42:40,940: t15.2023.12.08 val PER: 0.2024
2026-01-05 17:42:40,940: t15.2023.12.10 val PER: 0.1879
2026-01-05 17:42:40,940: t15.2023.12.17 val PER: 0.2391
2026-01-05 17:42:40,940: t15.2023.12.29 val PER: 0.2327
2026-01-05 17:42:40,940: t15.2024.02.25 val PER: 0.1854
2026-01-05 17:42:40,940: t15.2024.03.08 val PER: 0.3016
2026-01-05 17:42:40,940: t15.2024.03.15 val PER: 0.2739
2026-01-05 17:42:40,940: t15.2024.03.17 val PER: 0.2322
2026-01-05 17:42:40,940: t15.2024.05.10 val PER: 0.2585
2026-01-05 17:42:40,940: t15.2024.06.14 val PER: 0.2397
2026-01-05 17:42:40,940: t15.2024.07.19 val PER: 0.3032
2026-01-05 17:42:40,940: t15.2024.07.21 val PER: 0.1676
2026-01-05 17:42:40,941: t15.2024.07.28 val PER: 0.2213
2026-01-05 17:42:40,941: t15.2025.01.10 val PER: 0.3871
2026-01-05 17:42:40,941: t15.2025.01.12 val PER: 0.2710
2026-01-05 17:42:40,941: t15.2025.03.14 val PER: 0.3891
2026-01-05 17:42:40,941: t15.2025.03.16 val PER: 0.2788
2026-01-05 17:42:40,941: t15.2025.03.30 val PER: 0.3839
2026-01-05 17:42:40,941: t15.2025.04.13 val PER: 0.3010
2026-01-05 17:42:41,245: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_9500
2026-01-05 17:42:50,316: Train batch 9600: loss: 16.83 grad norm: 51.18 time: 0.092
2026-01-05 17:43:09,329: Train batch 9800: loss: 17.90 grad norm: 61.36 time: 0.078
2026-01-05 17:43:28,008: Train batch 10000: loss: 11.64 grad norm: 42.99 time: 0.075
2026-01-05 17:43:28,008: Running test after training batch: 10000
2026-01-05 17:43:28,136: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:43:33,329: WER debug example
  GT : you can see the code at this point as well
  PR : sputum kantz eats the toehold hatz this turnham pointed his hwe
2026-01-05 17:43:33,368: WER debug example
  GT : how does it keep the cost down
  PR : soho ouzts dusts hitt heap kiess thus costs
2026-01-05 17:43:35,569: Val batch 10000: PER (avg): 0.2396 CTC Loss (avg): 25.0484 WER(1gram): 106.09% (n=64) time: 7.560
2026-01-05 17:43:35,569: WER lens: avg_true_words=6.16 avg_pred_words=6.64 max_pred_words=12
2026-01-05 17:43:35,570: t15.2023.08.13 val PER: 0.2141
2026-01-05 17:43:35,570: t15.2023.08.18 val PER: 0.2154
2026-01-05 17:43:35,571: t15.2023.08.20 val PER: 0.2113
2026-01-05 17:43:35,571: t15.2023.08.25 val PER: 0.1958
2026-01-05 17:43:35,571: t15.2023.08.27 val PER: 0.2669
2026-01-05 17:43:35,571: t15.2023.09.01 val PER: 0.1688
2026-01-05 17:43:35,571: t15.2023.09.03 val PER: 0.2648
2026-01-05 17:43:35,571: t15.2023.09.24 val PER: 0.2209
2026-01-05 17:43:35,571: t15.2023.09.29 val PER: 0.2208
2026-01-05 17:43:35,571: t15.2023.10.01 val PER: 0.2536
2026-01-05 17:43:35,571: t15.2023.10.06 val PER: 0.1755
2026-01-05 17:43:35,571: t15.2023.10.08 val PER: 0.3153
2026-01-05 17:43:35,572: t15.2023.10.13 val PER: 0.2971
2026-01-05 17:43:35,572: t15.2023.10.15 val PER: 0.2386
2026-01-05 17:43:35,576: t15.2023.10.20 val PER: 0.2215
2026-01-05 17:43:35,576: t15.2023.10.22 val PER: 0.1837
2026-01-05 17:43:35,576: t15.2023.11.03 val PER: 0.2524
2026-01-05 17:43:35,576: t15.2023.11.04 val PER: 0.1126
2026-01-05 17:43:35,576: t15.2023.11.17 val PER: 0.1213
2026-01-05 17:43:35,576: t15.2023.11.19 val PER: 0.1377
2026-01-05 17:43:35,576: t15.2023.11.26 val PER: 0.2522
2026-01-05 17:43:35,576: t15.2023.12.03 val PER: 0.2164
2026-01-05 17:43:35,576: t15.2023.12.08 val PER: 0.1957
2026-01-05 17:43:35,577: t15.2023.12.10 val PER: 0.1932
2026-01-05 17:43:35,577: t15.2023.12.17 val PER: 0.2256
2026-01-05 17:43:35,577: t15.2023.12.29 val PER: 0.2313
2026-01-05 17:43:35,577: t15.2024.02.25 val PER: 0.2037
2026-01-05 17:43:35,577: t15.2024.03.08 val PER: 0.2902
2026-01-05 17:43:35,577: t15.2024.03.15 val PER: 0.2702
2026-01-05 17:43:35,577: t15.2024.03.17 val PER: 0.2350
2026-01-05 17:43:35,577: t15.2024.05.10 val PER: 0.2422
2026-01-05 17:43:35,577: t15.2024.06.14 val PER: 0.2445
2026-01-05 17:43:35,577: t15.2024.07.19 val PER: 0.2966
2026-01-05 17:43:35,577: t15.2024.07.21 val PER: 0.1752
2026-01-05 17:43:35,577: t15.2024.07.28 val PER: 0.2147
2026-01-05 17:43:35,577: t15.2025.01.10 val PER: 0.3843
2026-01-05 17:43:35,577: t15.2025.01.12 val PER: 0.2656
2026-01-05 17:43:35,577: t15.2025.03.14 val PER: 0.3757
2026-01-05 17:43:35,577: t15.2025.03.16 val PER: 0.2866
2026-01-05 17:43:35,577: t15.2025.03.30 val PER: 0.3747
2026-01-05 17:43:35,578: t15.2025.04.13 val PER: 0.3138
2026-01-05 17:43:35,902: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_10000
2026-01-05 17:43:55,650: Train batch 10200: loss: 15.39 grad norm: 48.75 time: 0.063
2026-01-05 17:44:15,536: Train batch 10400: loss: 18.40 grad norm: 98.66 time: 0.090
2026-01-05 17:44:25,526: Running test after training batch: 10500
2026-01-05 17:44:25,668: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:44:30,709: WER debug example
  GT : you can see the code at this point as well
  PR : hugh kantz eakes the troha codes hatz this tahoe points his swells
2026-01-05 17:44:30,738: WER debug example
  GT : how does it keep the cost down
  PR : soho ouzts dusts hitt heap keys thus costs
2026-01-05 17:44:32,438: Val batch 10500: PER (avg): 0.2365 CTC Loss (avg): 24.9861 WER(1gram): 109.14% (n=64) time: 6.912
2026-01-05 17:44:32,438: WER lens: avg_true_words=6.16 avg_pred_words=6.81 max_pred_words=12
2026-01-05 17:44:32,438: t15.2023.08.13 val PER: 0.2048
2026-01-05 17:44:32,438: t15.2023.08.18 val PER: 0.2096
2026-01-05 17:44:32,439: t15.2023.08.20 val PER: 0.2049
2026-01-05 17:44:32,439: t15.2023.08.25 val PER: 0.1928
2026-01-05 17:44:32,439: t15.2023.08.27 val PER: 0.2717
2026-01-05 17:44:32,439: t15.2023.09.01 val PER: 0.1745
2026-01-05 17:44:32,439: t15.2023.09.03 val PER: 0.2708
2026-01-05 17:44:32,439: t15.2023.09.24 val PER: 0.2282
2026-01-05 17:44:32,439: t15.2023.09.29 val PER: 0.2157
2026-01-05 17:44:32,439: t15.2023.10.01 val PER: 0.2417
2026-01-05 17:44:32,439: t15.2023.10.06 val PER: 0.1776
2026-01-05 17:44:32,439: t15.2023.10.08 val PER: 0.3139
2026-01-05 17:44:32,439: t15.2023.10.13 val PER: 0.2971
2026-01-05 17:44:32,439: t15.2023.10.15 val PER: 0.2373
2026-01-05 17:44:32,439: t15.2023.10.20 val PER: 0.2483
2026-01-05 17:44:32,440: t15.2023.10.22 val PER: 0.1938
2026-01-05 17:44:32,440: t15.2023.11.03 val PER: 0.2619
2026-01-05 17:44:32,440: t15.2023.11.04 val PER: 0.1058
2026-01-05 17:44:32,440: t15.2023.11.17 val PER: 0.1229
2026-01-05 17:44:32,440: t15.2023.11.19 val PER: 0.1397
2026-01-05 17:44:32,440: t15.2023.11.26 val PER: 0.2500
2026-01-05 17:44:32,440: t15.2023.12.03 val PER: 0.2227
2026-01-05 17:44:32,440: t15.2023.12.08 val PER: 0.2031
2026-01-05 17:44:32,440: t15.2023.12.10 val PER: 0.1958
2026-01-05 17:44:32,440: t15.2023.12.17 val PER: 0.2017
2026-01-05 17:44:32,440: t15.2023.12.29 val PER: 0.2286
2026-01-05 17:44:32,440: t15.2024.02.25 val PER: 0.1994
2026-01-05 17:44:32,440: t15.2024.03.08 val PER: 0.3001
2026-01-05 17:44:32,441: t15.2024.03.15 val PER: 0.2570
2026-01-05 17:44:32,441: t15.2024.03.17 val PER: 0.2392
2026-01-05 17:44:32,441: t15.2024.05.10 val PER: 0.2407
2026-01-05 17:44:32,441: t15.2024.06.14 val PER: 0.2303
2026-01-05 17:44:32,441: t15.2024.07.19 val PER: 0.2835
2026-01-05 17:44:32,441: t15.2024.07.21 val PER: 0.1497
2026-01-05 17:44:32,441: t15.2024.07.28 val PER: 0.2066
2026-01-05 17:44:32,441: t15.2025.01.10 val PER: 0.3829
2026-01-05 17:44:32,441: t15.2025.01.12 val PER: 0.2525
2026-01-05 17:44:32,441: t15.2025.03.14 val PER: 0.3950
2026-01-05 17:44:32,441: t15.2025.03.16 val PER: 0.2644
2026-01-05 17:44:32,441: t15.2025.03.30 val PER: 0.3747
2026-01-05 17:44:32,441: t15.2025.04.13 val PER: 0.3067
2026-01-05 17:44:32,736: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_10500
2026-01-05 17:44:42,812: Train batch 10600: loss: 14.52 grad norm: 87.90 time: 0.093
2026-01-05 17:45:02,423: Train batch 10800: loss: 21.81 grad norm: 68.48 time: 0.081
2026-01-05 17:45:22,181: Train batch 11000: loss: 22.83 grad norm: 74.28 time: 0.070
2026-01-05 17:45:22,181: Running test after training batch: 11000
2026-01-05 17:45:22,332: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:45:27,479: WER debug example
  GT : you can see the code at this point as well
  PR : skewed kantz eats the tadych colds hatz this tahoe pointed his hwe
2026-01-05 17:45:27,525: WER debug example
  GT : how does it keep the cost down
  PR : shead stutz hitt heaps thus tsetse
2026-01-05 17:45:29,713: Val batch 11000: PER (avg): 0.2330 CTC Loss (avg): 24.4153 WER(1gram): 106.09% (n=64) time: 7.532
2026-01-05 17:45:29,714: WER lens: avg_true_words=6.16 avg_pred_words=6.61 max_pred_words=14
2026-01-05 17:45:29,714: t15.2023.08.13 val PER: 0.2017
2026-01-05 17:45:29,714: t15.2023.08.18 val PER: 0.2070
2026-01-05 17:45:29,714: t15.2023.08.20 val PER: 0.2065
2026-01-05 17:45:29,714: t15.2023.08.25 val PER: 0.1822
2026-01-05 17:45:29,715: t15.2023.08.27 val PER: 0.2749
2026-01-05 17:45:29,715: t15.2023.09.01 val PER: 0.1688
2026-01-05 17:45:29,715: t15.2023.09.03 val PER: 0.2696
2026-01-05 17:45:29,715: t15.2023.09.24 val PER: 0.2172
2026-01-05 17:45:29,715: t15.2023.09.29 val PER: 0.2112
2026-01-05 17:45:29,715: t15.2023.10.01 val PER: 0.2536
2026-01-05 17:45:29,715: t15.2023.10.06 val PER: 0.1755
2026-01-05 17:45:29,715: t15.2023.10.08 val PER: 0.3275
2026-01-05 17:45:29,715: t15.2023.10.13 val PER: 0.2971
2026-01-05 17:45:29,716: t15.2023.10.15 val PER: 0.2419
2026-01-05 17:45:29,716: t15.2023.10.20 val PER: 0.2517
2026-01-05 17:45:29,716: t15.2023.10.22 val PER: 0.1860
2026-01-05 17:45:29,716: t15.2023.11.03 val PER: 0.2497
2026-01-05 17:45:29,716: t15.2023.11.04 val PER: 0.1024
2026-01-05 17:45:29,716: t15.2023.11.17 val PER: 0.1229
2026-01-05 17:45:29,716: t15.2023.11.19 val PER: 0.1257
2026-01-05 17:45:29,717: t15.2023.11.26 val PER: 0.2486
2026-01-05 17:45:29,717: t15.2023.12.03 val PER: 0.2048
2026-01-05 17:45:29,717: t15.2023.12.08 val PER: 0.1957
2026-01-05 17:45:29,717: t15.2023.12.10 val PER: 0.1813
2026-01-05 17:45:29,717: t15.2023.12.17 val PER: 0.2131
2026-01-05 17:45:29,717: t15.2023.12.29 val PER: 0.2251
2026-01-05 17:45:29,717: t15.2024.02.25 val PER: 0.2079
2026-01-05 17:45:29,717: t15.2024.03.08 val PER: 0.2973
2026-01-05 17:45:29,717: t15.2024.03.15 val PER: 0.2639
2026-01-05 17:45:29,717: t15.2024.03.17 val PER: 0.2301
2026-01-05 17:45:29,717: t15.2024.05.10 val PER: 0.2377
2026-01-05 17:45:29,717: t15.2024.06.14 val PER: 0.2224
2026-01-05 17:45:29,717: t15.2024.07.19 val PER: 0.2828
2026-01-05 17:45:29,717: t15.2024.07.21 val PER: 0.1552
2026-01-05 17:45:29,718: t15.2024.07.28 val PER: 0.1993
2026-01-05 17:45:29,718: t15.2025.01.10 val PER: 0.3623
2026-01-05 17:45:29,718: t15.2025.01.12 val PER: 0.2471
2026-01-05 17:45:29,718: t15.2025.03.14 val PER: 0.3743
2026-01-05 17:45:29,718: t15.2025.03.16 val PER: 0.2526
2026-01-05 17:45:29,718: t15.2025.03.30 val PER: 0.3563
2026-01-05 17:45:29,718: t15.2025.04.13 val PER: 0.2939
2026-01-05 17:45:30,046: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_11000
2026-01-05 17:45:48,929: Train batch 11200: loss: 16.96 grad norm: 61.86 time: 0.089
2026-01-05 17:46:08,652: Train batch 11400: loss: 15.37 grad norm: 51.32 time: 0.070
2026-01-05 17:46:18,185: Running test after training batch: 11500
2026-01-05 17:46:18,298: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:46:23,878: WER debug example
  GT : you can see the code at this point as well
  PR : skewed kantz eakes the tadych colds hatz this tahoe points his swells
2026-01-05 17:46:23,908: WER debug example
  GT : how does it keep the cost down
  PR : shead suss hitt heaps thus costs
2026-01-05 17:46:25,663: Val batch 11500: PER (avg): 0.2334 CTC Loss (avg): 24.5151 WER(1gram): 105.84% (n=64) time: 7.478
2026-01-05 17:46:25,663: WER lens: avg_true_words=6.16 avg_pred_words=6.58 max_pred_words=12
2026-01-05 17:46:25,664: t15.2023.08.13 val PER: 0.2173
2026-01-05 17:46:25,664: t15.2023.08.18 val PER: 0.2146
2026-01-05 17:46:25,664: t15.2023.08.20 val PER: 0.2089
2026-01-05 17:46:25,664: t15.2023.08.25 val PER: 0.1943
2026-01-05 17:46:25,664: t15.2023.08.27 val PER: 0.2717
2026-01-05 17:46:25,664: t15.2023.09.01 val PER: 0.1761
2026-01-05 17:46:25,664: t15.2023.09.03 val PER: 0.2684
2026-01-05 17:46:25,664: t15.2023.09.24 val PER: 0.2197
2026-01-05 17:46:25,664: t15.2023.09.29 val PER: 0.2119
2026-01-05 17:46:25,664: t15.2023.10.01 val PER: 0.2464
2026-01-05 17:46:25,665: t15.2023.10.06 val PER: 0.1744
2026-01-05 17:46:25,665: t15.2023.10.08 val PER: 0.3099
2026-01-05 17:46:25,665: t15.2023.10.13 val PER: 0.3072
2026-01-05 17:46:25,665: t15.2023.10.15 val PER: 0.2439
2026-01-05 17:46:25,665: t15.2023.10.20 val PER: 0.2282
2026-01-05 17:46:25,665: t15.2023.10.22 val PER: 0.1782
2026-01-05 17:46:25,665: t15.2023.11.03 val PER: 0.2517
2026-01-05 17:46:25,665: t15.2023.11.04 val PER: 0.1092
2026-01-05 17:46:25,665: t15.2023.11.17 val PER: 0.1275
2026-01-05 17:46:25,665: t15.2023.11.19 val PER: 0.1357
2026-01-05 17:46:25,665: t15.2023.11.26 val PER: 0.2377
2026-01-05 17:46:25,665: t15.2023.12.03 val PER: 0.2059
2026-01-05 17:46:25,665: t15.2023.12.08 val PER: 0.1957
2026-01-05 17:46:25,666: t15.2023.12.10 val PER: 0.1669
2026-01-05 17:46:25,666: t15.2023.12.17 val PER: 0.2173
2026-01-05 17:46:25,666: t15.2023.12.29 val PER: 0.2272
2026-01-05 17:46:25,666: t15.2024.02.25 val PER: 0.1868
2026-01-05 17:46:25,666: t15.2024.03.08 val PER: 0.2859
2026-01-05 17:46:25,666: t15.2024.03.15 val PER: 0.2627
2026-01-05 17:46:25,666: t15.2024.03.17 val PER: 0.2232
2026-01-05 17:46:25,666: t15.2024.05.10 val PER: 0.2407
2026-01-05 17:46:25,666: t15.2024.06.14 val PER: 0.2319
2026-01-05 17:46:25,666: t15.2024.07.19 val PER: 0.2861
2026-01-05 17:46:25,666: t15.2024.07.21 val PER: 0.1510
2026-01-05 17:46:25,666: t15.2024.07.28 val PER: 0.1926
2026-01-05 17:46:25,666: t15.2025.01.10 val PER: 0.3829
2026-01-05 17:46:25,666: t15.2025.01.12 val PER: 0.2525
2026-01-05 17:46:25,667: t15.2025.03.14 val PER: 0.3831
2026-01-05 17:46:25,667: t15.2025.03.16 val PER: 0.2696
2026-01-05 17:46:25,667: t15.2025.03.30 val PER: 0.3529
2026-01-05 17:46:25,667: t15.2025.04.13 val PER: 0.2967
2026-01-05 17:46:25,957: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_11500
2026-01-05 17:46:35,172: Train batch 11600: loss: 20.45 grad norm: 64.21 time: 0.075
2026-01-05 17:46:54,355: Train batch 11800: loss: 14.87 grad norm: 51.30 time: 0.055
2026-01-05 17:47:13,019: Train batch 12000: loss: 20.08 grad norm: 64.40 time: 0.089
2026-01-05 17:47:13,020: Running test after training batch: 12000
2026-01-05 17:47:13,115: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:47:18,103: WER debug example
  GT : you can see the code at this point as well
  PR : skew kantz eats the tadych colds hatz this tahoe pointed his hwe
2026-01-05 17:47:18,134: WER debug example
  GT : how does it keep the cost down
  PR : soho ouzts dusts hitt heaps thus costs
2026-01-05 17:47:19,868: Val batch 12000: PER (avg): 0.2270 CTC Loss (avg): 24.1942 WER(1gram): 106.60% (n=64) time: 6.848
2026-01-05 17:47:19,868: WER lens: avg_true_words=6.16 avg_pred_words=6.62 max_pred_words=12
2026-01-05 17:47:19,868: t15.2023.08.13 val PER: 0.2100
2026-01-05 17:47:19,869: t15.2023.08.18 val PER: 0.2070
2026-01-05 17:47:19,869: t15.2023.08.20 val PER: 0.2017
2026-01-05 17:47:19,869: t15.2023.08.25 val PER: 0.1822
2026-01-05 17:47:19,869: t15.2023.08.27 val PER: 0.2797
2026-01-05 17:47:19,869: t15.2023.09.01 val PER: 0.1607
2026-01-05 17:47:19,869: t15.2023.09.03 val PER: 0.2553
2026-01-05 17:47:19,869: t15.2023.09.24 val PER: 0.2184
2026-01-05 17:47:19,869: t15.2023.09.29 val PER: 0.2080
2026-01-05 17:47:19,869: t15.2023.10.01 val PER: 0.2398
2026-01-05 17:47:19,869: t15.2023.10.06 val PER: 0.1733
2026-01-05 17:47:19,869: t15.2023.10.08 val PER: 0.3058
2026-01-05 17:47:19,869: t15.2023.10.13 val PER: 0.2863
2026-01-05 17:47:19,869: t15.2023.10.15 val PER: 0.2281
2026-01-05 17:47:19,869: t15.2023.10.20 val PER: 0.2349
2026-01-05 17:47:19,870: t15.2023.10.22 val PER: 0.1826
2026-01-05 17:47:19,870: t15.2023.11.03 val PER: 0.2476
2026-01-05 17:47:19,870: t15.2023.11.04 val PER: 0.1092
2026-01-05 17:47:19,870: t15.2023.11.17 val PER: 0.1198
2026-01-05 17:47:19,870: t15.2023.11.19 val PER: 0.1317
2026-01-05 17:47:19,870: t15.2023.11.26 val PER: 0.2362
2026-01-05 17:47:19,870: t15.2023.12.03 val PER: 0.2069
2026-01-05 17:47:19,870: t15.2023.12.08 val PER: 0.1917
2026-01-05 17:47:19,870: t15.2023.12.10 val PER: 0.1695
2026-01-05 17:47:19,870: t15.2023.12.17 val PER: 0.2089
2026-01-05 17:47:19,870: t15.2023.12.29 val PER: 0.2286
2026-01-05 17:47:19,870: t15.2024.02.25 val PER: 0.1854
2026-01-05 17:47:19,870: t15.2024.03.08 val PER: 0.2717
2026-01-05 17:47:19,871: t15.2024.03.15 val PER: 0.2495
2026-01-05 17:47:19,871: t15.2024.03.17 val PER: 0.2183
2026-01-05 17:47:19,871: t15.2024.05.10 val PER: 0.2363
2026-01-05 17:47:19,871: t15.2024.06.14 val PER: 0.2177
2026-01-05 17:47:19,871: t15.2024.07.19 val PER: 0.2821
2026-01-05 17:47:19,871: t15.2024.07.21 val PER: 0.1434
2026-01-05 17:47:19,871: t15.2024.07.28 val PER: 0.1912
2026-01-05 17:47:19,871: t15.2025.01.10 val PER: 0.3691
2026-01-05 17:47:19,871: t15.2025.01.12 val PER: 0.2456
2026-01-05 17:47:19,871: t15.2025.03.14 val PER: 0.3728
2026-01-05 17:47:19,871: t15.2025.03.16 val PER: 0.2683
2026-01-05 17:47:19,871: t15.2025.03.30 val PER: 0.3391
2026-01-05 17:47:19,871: t15.2025.04.13 val PER: 0.2867
2026-01-05 17:47:20,195: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_12000
2026-01-05 17:47:39,790: Train batch 12200: loss: 14.16 grad norm: 48.54 time: 0.081
2026-01-05 17:47:59,055: Train batch 12400: loss: 10.94 grad norm: 38.28 time: 0.051
2026-01-05 17:48:09,012: Running test after training batch: 12500
2026-01-05 17:48:09,154: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:48:14,121: WER debug example
  GT : you can see the code at this point as well
  PR : skew kantz eakes the tadych codes hatz this tahoe pointed his swells
2026-01-05 17:48:14,152: WER debug example
  GT : how does it keep the cost down
  PR : shouse zietz dusts hitt heaps thus costs
2026-01-05 17:48:15,938: Val batch 12500: PER (avg): 0.2246 CTC Loss (avg): 23.8666 WER(1gram): 105.33% (n=64) time: 6.926
2026-01-05 17:48:15,938: WER lens: avg_true_words=6.16 avg_pred_words=6.52 max_pred_words=13
2026-01-05 17:48:15,938: t15.2023.08.13 val PER: 0.2006
2026-01-05 17:48:15,938: t15.2023.08.18 val PER: 0.2087
2026-01-05 17:48:15,939: t15.2023.08.20 val PER: 0.1954
2026-01-05 17:48:15,939: t15.2023.08.25 val PER: 0.1822
2026-01-05 17:48:15,939: t15.2023.08.27 val PER: 0.2621
2026-01-05 17:48:15,939: t15.2023.09.01 val PER: 0.1615
2026-01-05 17:48:15,939: t15.2023.09.03 val PER: 0.2518
2026-01-05 17:48:15,939: t15.2023.09.24 val PER: 0.2027
2026-01-05 17:48:15,939: t15.2023.09.29 val PER: 0.2112
2026-01-05 17:48:15,939: t15.2023.10.01 val PER: 0.2378
2026-01-05 17:48:15,939: t15.2023.10.06 val PER: 0.1733
2026-01-05 17:48:15,939: t15.2023.10.08 val PER: 0.3112
2026-01-05 17:48:15,939: t15.2023.10.13 val PER: 0.2933
2026-01-05 17:48:15,939: t15.2023.10.15 val PER: 0.2334
2026-01-05 17:48:15,940: t15.2023.10.20 val PER: 0.2349
2026-01-05 17:48:15,940: t15.2023.10.22 val PER: 0.1592
2026-01-05 17:48:15,940: t15.2023.11.03 val PER: 0.2408
2026-01-05 17:48:15,940: t15.2023.11.04 val PER: 0.1160
2026-01-05 17:48:15,940: t15.2023.11.17 val PER: 0.1198
2026-01-05 17:48:15,940: t15.2023.11.19 val PER: 0.1218
2026-01-05 17:48:15,940: t15.2023.11.26 val PER: 0.2391
2026-01-05 17:48:15,940: t15.2023.12.03 val PER: 0.1954
2026-01-05 17:48:15,940: t15.2023.12.08 val PER: 0.1838
2026-01-05 17:48:15,940: t15.2023.12.10 val PER: 0.1708
2026-01-05 17:48:15,940: t15.2023.12.17 val PER: 0.2089
2026-01-05 17:48:15,940: t15.2023.12.29 val PER: 0.2162
2026-01-05 17:48:15,941: t15.2024.02.25 val PER: 0.1784
2026-01-05 17:48:15,941: t15.2024.03.08 val PER: 0.2845
2026-01-05 17:48:15,941: t15.2024.03.15 val PER: 0.2452
2026-01-05 17:48:15,941: t15.2024.03.17 val PER: 0.2162
2026-01-05 17:48:15,941: t15.2024.05.10 val PER: 0.2318
2026-01-05 17:48:15,941: t15.2024.06.14 val PER: 0.2208
2026-01-05 17:48:15,941: t15.2024.07.19 val PER: 0.2736
2026-01-05 17:48:15,941: t15.2024.07.21 val PER: 0.1428
2026-01-05 17:48:15,941: t15.2024.07.28 val PER: 0.1875
2026-01-05 17:48:15,941: t15.2025.01.10 val PER: 0.3747
2026-01-05 17:48:15,941: t15.2025.01.12 val PER: 0.2433
2026-01-05 17:48:15,941: t15.2025.03.14 val PER: 0.3757
2026-01-05 17:48:15,941: t15.2025.03.16 val PER: 0.2579
2026-01-05 17:48:15,941: t15.2025.03.30 val PER: 0.3598
2026-01-05 17:48:15,942: t15.2025.04.13 val PER: 0.2867
2026-01-05 17:48:16,235: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_12500
2026-01-05 17:48:26,134: Train batch 12600: loss: 13.98 grad norm: 60.06 time: 0.072
2026-01-05 17:48:45,796: Train batch 12800: loss: 13.70 grad norm: 52.90 time: 0.064
2026-01-05 17:49:05,618: Train batch 13000: loss: 12.75 grad norm: 52.34 time: 0.083
2026-01-05 17:49:05,618: Running test after training batch: 13000
2026-01-05 17:49:05,721: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:49:10,818: WER debug example
  GT : you can see the code at this point as well
  PR : skewed kantz eakes the tadych colds hatz this tahoe pointed has hwe
2026-01-05 17:49:10,851: WER debug example
  GT : how does it keep the cost down
  PR : soho ouzts dust hitt heaps thus costs
2026-01-05 17:49:12,652: Val batch 13000: PER (avg): 0.2213 CTC Loss (avg): 23.6386 WER(1gram): 105.08% (n=64) time: 7.034
2026-01-05 17:49:12,653: WER lens: avg_true_words=6.16 avg_pred_words=6.62 max_pred_words=12
2026-01-05 17:49:12,653: t15.2023.08.13 val PER: 0.2079
2026-01-05 17:49:12,653: t15.2023.08.18 val PER: 0.1987
2026-01-05 17:49:12,653: t15.2023.08.20 val PER: 0.2033
2026-01-05 17:49:12,653: t15.2023.08.25 val PER: 0.1747
2026-01-05 17:49:12,653: t15.2023.08.27 val PER: 0.2733
2026-01-05 17:49:12,653: t15.2023.09.01 val PER: 0.1567
2026-01-05 17:49:12,653: t15.2023.09.03 val PER: 0.2506
2026-01-05 17:49:12,653: t15.2023.09.24 val PER: 0.2063
2026-01-05 17:49:12,654: t15.2023.09.29 val PER: 0.2176
2026-01-05 17:49:12,654: t15.2023.10.01 val PER: 0.2345
2026-01-05 17:49:12,654: t15.2023.10.06 val PER: 0.1701
2026-01-05 17:49:12,654: t15.2023.10.08 val PER: 0.2950
2026-01-05 17:49:12,654: t15.2023.10.13 val PER: 0.2777
2026-01-05 17:49:12,654: t15.2023.10.15 val PER: 0.2215
2026-01-05 17:49:12,654: t15.2023.10.20 val PER: 0.2282
2026-01-05 17:49:12,655: t15.2023.10.22 val PER: 0.1771
2026-01-05 17:49:12,655: t15.2023.11.03 val PER: 0.2429
2026-01-05 17:49:12,655: t15.2023.11.04 val PER: 0.1024
2026-01-05 17:49:12,655: t15.2023.11.17 val PER: 0.1260
2026-01-05 17:49:12,655: t15.2023.11.19 val PER: 0.1238
2026-01-05 17:49:12,655: t15.2023.11.26 val PER: 0.2275
2026-01-05 17:49:12,655: t15.2023.12.03 val PER: 0.1912
2026-01-05 17:49:12,655: t15.2023.12.08 val PER: 0.1711
2026-01-05 17:49:12,655: t15.2023.12.10 val PER: 0.1682
2026-01-05 17:49:12,655: t15.2023.12.17 val PER: 0.1954
2026-01-05 17:49:12,655: t15.2023.12.29 val PER: 0.2217
2026-01-05 17:49:12,655: t15.2024.02.25 val PER: 0.1728
2026-01-05 17:49:12,655: t15.2024.03.08 val PER: 0.2788
2026-01-05 17:49:12,655: t15.2024.03.15 val PER: 0.2364
2026-01-05 17:49:12,655: t15.2024.03.17 val PER: 0.2106
2026-01-05 17:49:12,656: t15.2024.05.10 val PER: 0.2288
2026-01-05 17:49:12,656: t15.2024.06.14 val PER: 0.2129
2026-01-05 17:49:12,656: t15.2024.07.19 val PER: 0.2729
2026-01-05 17:49:12,656: t15.2024.07.21 val PER: 0.1476
2026-01-05 17:49:12,656: t15.2024.07.28 val PER: 0.1816
2026-01-05 17:49:12,656: t15.2025.01.10 val PER: 0.3719
2026-01-05 17:49:12,656: t15.2025.01.12 val PER: 0.2402
2026-01-05 17:49:12,656: t15.2025.03.14 val PER: 0.3506
2026-01-05 17:49:12,656: t15.2025.03.16 val PER: 0.2644
2026-01-05 17:49:12,656: t15.2025.03.30 val PER: 0.3494
2026-01-05 17:49:12,656: t15.2025.04.13 val PER: 0.2867
2026-01-05 17:49:12,944: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_13000
2026-01-05 17:49:31,859: Train batch 13200: loss: 20.95 grad norm: 77.34 time: 0.067
2026-01-05 17:49:51,247: Train batch 13400: loss: 15.66 grad norm: 66.98 time: 0.080
2026-01-05 17:50:00,728: Running test after training batch: 13500
2026-01-05 17:50:00,850: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:50:06,008: WER debug example
  GT : you can see the code at this point as well
  PR : dudes kantz eakes the troha colds hatz this tahoe pointed his swells
2026-01-05 17:50:06,051: WER debug example
  GT : how does it keep the cost down
  PR : seaholm zietz dusts hitt heaps thus costs
2026-01-05 17:50:08,556: Val batch 13500: PER (avg): 0.2199 CTC Loss (avg): 23.5711 WER(1gram): 107.11% (n=64) time: 7.827
2026-01-05 17:50:08,556: WER lens: avg_true_words=6.16 avg_pred_words=6.72 max_pred_words=13
2026-01-05 17:50:08,556: t15.2023.08.13 val PER: 0.2100
2026-01-05 17:50:08,557: t15.2023.08.18 val PER: 0.1945
2026-01-05 17:50:08,557: t15.2023.08.20 val PER: 0.2002
2026-01-05 17:50:08,557: t15.2023.08.25 val PER: 0.1717
2026-01-05 17:50:08,557: t15.2023.08.27 val PER: 0.2637
2026-01-05 17:50:08,557: t15.2023.09.01 val PER: 0.1558
2026-01-05 17:50:08,557: t15.2023.09.03 val PER: 0.2625
2026-01-05 17:50:08,558: t15.2023.09.24 val PER: 0.2100
2026-01-05 17:50:08,558: t15.2023.09.29 val PER: 0.2119
2026-01-05 17:50:08,558: t15.2023.10.01 val PER: 0.2272
2026-01-05 17:50:08,558: t15.2023.10.06 val PER: 0.1712
2026-01-05 17:50:08,558: t15.2023.10.08 val PER: 0.3018
2026-01-05 17:50:08,558: t15.2023.10.13 val PER: 0.2847
2026-01-05 17:50:08,558: t15.2023.10.15 val PER: 0.2274
2026-01-05 17:50:08,558: t15.2023.10.20 val PER: 0.2282
2026-01-05 17:50:08,558: t15.2023.10.22 val PER: 0.1726
2026-01-05 17:50:08,559: t15.2023.11.03 val PER: 0.2381
2026-01-05 17:50:08,559: t15.2023.11.04 val PER: 0.1058
2026-01-05 17:50:08,559: t15.2023.11.17 val PER: 0.1135
2026-01-05 17:50:08,559: t15.2023.11.19 val PER: 0.1257
2026-01-05 17:50:08,559: t15.2023.11.26 val PER: 0.2268
2026-01-05 17:50:08,559: t15.2023.12.03 val PER: 0.1901
2026-01-05 17:50:08,559: t15.2023.12.08 val PER: 0.1844
2026-01-05 17:50:08,559: t15.2023.12.10 val PER: 0.1629
2026-01-05 17:50:08,560: t15.2023.12.17 val PER: 0.1913
2026-01-05 17:50:08,560: t15.2023.12.29 val PER: 0.2114
2026-01-05 17:50:08,560: t15.2024.02.25 val PER: 0.1671
2026-01-05 17:50:08,560: t15.2024.03.08 val PER: 0.2674
2026-01-05 17:50:08,560: t15.2024.03.15 val PER: 0.2383
2026-01-05 17:50:08,560: t15.2024.03.17 val PER: 0.2127
2026-01-05 17:50:08,560: t15.2024.05.10 val PER: 0.2244
2026-01-05 17:50:08,560: t15.2024.06.14 val PER: 0.2082
2026-01-05 17:50:08,560: t15.2024.07.19 val PER: 0.2703
2026-01-05 17:50:08,561: t15.2024.07.21 val PER: 0.1359
2026-01-05 17:50:08,561: t15.2024.07.28 val PER: 0.1868
2026-01-05 17:50:08,561: t15.2025.01.10 val PER: 0.3691
2026-01-05 17:50:08,561: t15.2025.01.12 val PER: 0.2348
2026-01-05 17:50:08,561: t15.2025.03.14 val PER: 0.3521
2026-01-05 17:50:08,561: t15.2025.03.16 val PER: 0.2513
2026-01-05 17:50:08,561: t15.2025.03.30 val PER: 0.3621
2026-01-05 17:50:08,561: t15.2025.04.13 val PER: 0.2867
2026-01-05 17:50:08,866: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_13500
2026-01-05 17:50:18,679: Train batch 13600: loss: 18.73 grad norm: 64.99 time: 0.077
2026-01-05 17:50:38,077: Train batch 13800: loss: 15.04 grad norm: 61.18 time: 0.069
2026-01-05 17:50:57,730: Train batch 14000: loss: 20.73 grad norm: 71.85 time: 0.063
2026-01-05 17:50:57,730: Running test after training batch: 14000
2026-01-05 17:50:57,852: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:51:03,044: WER debug example
  GT : you can see the code at this point as well
  PR : skewed canned sikhs thus hodes hatz this pointed his swells
2026-01-05 17:51:03,097: WER debug example
  GT : how does it keep the cost down
  PR : soho ouzts dusts hitt heaps thus setzer
2026-01-05 17:51:05,791: Val batch 14000: PER (avg): 0.2179 CTC Loss (avg): 23.5141 WER(1gram): 106.85% (n=64) time: 8.061
2026-01-05 17:51:05,791: WER lens: avg_true_words=6.16 avg_pred_words=6.67 max_pred_words=13
2026-01-05 17:51:05,792: t15.2023.08.13 val PER: 0.1965
2026-01-05 17:51:05,792: t15.2023.08.18 val PER: 0.1920
2026-01-05 17:51:05,792: t15.2023.08.20 val PER: 0.2002
2026-01-05 17:51:05,792: t15.2023.08.25 val PER: 0.1792
2026-01-05 17:51:05,792: t15.2023.08.27 val PER: 0.2508
2026-01-05 17:51:05,792: t15.2023.09.01 val PER: 0.1583
2026-01-05 17:51:05,793: t15.2023.09.03 val PER: 0.2518
2026-01-05 17:51:05,793: t15.2023.09.24 val PER: 0.2027
2026-01-05 17:51:05,793: t15.2023.09.29 val PER: 0.2112
2026-01-05 17:51:05,793: t15.2023.10.01 val PER: 0.2305
2026-01-05 17:51:05,793: t15.2023.10.06 val PER: 0.1690
2026-01-05 17:51:05,793: t15.2023.10.08 val PER: 0.2977
2026-01-05 17:51:05,793: t15.2023.10.13 val PER: 0.2785
2026-01-05 17:51:05,793: t15.2023.10.15 val PER: 0.2175
2026-01-05 17:51:05,793: t15.2023.10.20 val PER: 0.2215
2026-01-05 17:51:05,793: t15.2023.10.22 val PER: 0.1682
2026-01-05 17:51:05,794: t15.2023.11.03 val PER: 0.2524
2026-01-05 17:51:05,794: t15.2023.11.04 val PER: 0.1126
2026-01-05 17:51:05,794: t15.2023.11.17 val PER: 0.1229
2026-01-05 17:51:05,794: t15.2023.11.19 val PER: 0.1178
2026-01-05 17:51:05,794: t15.2023.11.26 val PER: 0.2261
2026-01-05 17:51:05,794: t15.2023.12.03 val PER: 0.1954
2026-01-05 17:51:05,794: t15.2023.12.08 val PER: 0.1724
2026-01-05 17:51:05,794: t15.2023.12.10 val PER: 0.1551
2026-01-05 17:51:05,794: t15.2023.12.17 val PER: 0.1892
2026-01-05 17:51:05,794: t15.2023.12.29 val PER: 0.2128
2026-01-05 17:51:05,794: t15.2024.02.25 val PER: 0.1742
2026-01-05 17:51:05,795: t15.2024.03.08 val PER: 0.2617
2026-01-05 17:51:05,795: t15.2024.03.15 val PER: 0.2427
2026-01-05 17:51:05,795: t15.2024.03.17 val PER: 0.2134
2026-01-05 17:51:05,795: t15.2024.05.10 val PER: 0.2363
2026-01-05 17:51:05,795: t15.2024.06.14 val PER: 0.2192
2026-01-05 17:51:05,795: t15.2024.07.19 val PER: 0.2657
2026-01-05 17:51:05,795: t15.2024.07.21 val PER: 0.1331
2026-01-05 17:51:05,795: t15.2024.07.28 val PER: 0.1735
2026-01-05 17:51:05,795: t15.2025.01.10 val PER: 0.3623
2026-01-05 17:51:05,795: t15.2025.01.12 val PER: 0.2325
2026-01-05 17:51:05,795: t15.2025.03.14 val PER: 0.3580
2026-01-05 17:51:05,796: t15.2025.03.16 val PER: 0.2435
2026-01-05 17:51:05,796: t15.2025.03.30 val PER: 0.3471
2026-01-05 17:51:05,796: t15.2025.04.13 val PER: 0.2867
2026-01-05 17:51:06,123: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_14000
2026-01-05 17:51:25,266: Train batch 14200: loss: 16.56 grad norm: 54.01 time: 0.069
2026-01-05 17:51:44,692: Train batch 14400: loss: 12.45 grad norm: 46.62 time: 0.079
2026-01-05 17:51:54,289: Running test after training batch: 14500
2026-01-05 17:51:54,387: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:51:59,559: WER debug example
  GT : you can see the code at this point as well
  PR : seiyu kantz eakes the tadych colds hatz this tahoe pointed has swells
2026-01-05 17:51:59,614: WER debug example
  GT : how does it keep the cost down
  PR : soho ouzts dusts hitt heaps thus costs
2026-01-05 17:52:02,398: Val batch 14500: PER (avg): 0.2182 CTC Loss (avg): 23.5867 WER(1gram): 108.88% (n=64) time: 8.108
2026-01-05 17:52:02,398: WER lens: avg_true_words=6.16 avg_pred_words=6.86 max_pred_words=13
2026-01-05 17:52:02,398: t15.2023.08.13 val PER: 0.2037
2026-01-05 17:52:02,399: t15.2023.08.18 val PER: 0.1928
2026-01-05 17:52:02,399: t15.2023.08.20 val PER: 0.1978
2026-01-05 17:52:02,399: t15.2023.08.25 val PER: 0.1747
2026-01-05 17:52:02,399: t15.2023.08.27 val PER: 0.2588
2026-01-05 17:52:02,399: t15.2023.09.01 val PER: 0.1599
2026-01-05 17:52:02,399: t15.2023.09.03 val PER: 0.2399
2026-01-05 17:52:02,399: t15.2023.09.24 val PER: 0.2087
2026-01-05 17:52:02,399: t15.2023.09.29 val PER: 0.2074
2026-01-05 17:52:02,400: t15.2023.10.01 val PER: 0.2332
2026-01-05 17:52:02,400: t15.2023.10.06 val PER: 0.1701
2026-01-05 17:52:02,400: t15.2023.10.08 val PER: 0.2963
2026-01-05 17:52:02,400: t15.2023.10.13 val PER: 0.2777
2026-01-05 17:52:02,400: t15.2023.10.15 val PER: 0.2175
2026-01-05 17:52:02,400: t15.2023.10.20 val PER: 0.2148
2026-01-05 17:52:02,400: t15.2023.10.22 val PER: 0.1659
2026-01-05 17:52:02,401: t15.2023.11.03 val PER: 0.2469
2026-01-05 17:52:02,401: t15.2023.11.04 val PER: 0.1024
2026-01-05 17:52:02,401: t15.2023.11.17 val PER: 0.1166
2026-01-05 17:52:02,401: t15.2023.11.19 val PER: 0.1218
2026-01-05 17:52:02,401: t15.2023.11.26 val PER: 0.2217
2026-01-05 17:52:02,401: t15.2023.12.03 val PER: 0.1891
2026-01-05 17:52:02,401: t15.2023.12.08 val PER: 0.1684
2026-01-05 17:52:02,401: t15.2023.12.10 val PER: 0.1616
2026-01-05 17:52:02,401: t15.2023.12.17 val PER: 0.1913
2026-01-05 17:52:02,402: t15.2023.12.29 val PER: 0.2169
2026-01-05 17:52:02,402: t15.2024.02.25 val PER: 0.1756
2026-01-05 17:52:02,402: t15.2024.03.08 val PER: 0.2617
2026-01-05 17:52:02,402: t15.2024.03.15 val PER: 0.2445
2026-01-05 17:52:02,402: t15.2024.03.17 val PER: 0.2120
2026-01-05 17:52:02,402: t15.2024.05.10 val PER: 0.2318
2026-01-05 17:52:02,402: t15.2024.06.14 val PER: 0.2129
2026-01-05 17:52:02,402: t15.2024.07.19 val PER: 0.2676
2026-01-05 17:52:02,402: t15.2024.07.21 val PER: 0.1393
2026-01-05 17:52:02,402: t15.2024.07.28 val PER: 0.1794
2026-01-05 17:52:02,403: t15.2025.01.10 val PER: 0.3595
2026-01-05 17:52:02,403: t15.2025.01.12 val PER: 0.2410
2026-01-05 17:52:02,403: t15.2025.03.14 val PER: 0.3639
2026-01-05 17:52:02,403: t15.2025.03.16 val PER: 0.2526
2026-01-05 17:52:02,403: t15.2025.03.30 val PER: 0.3471
2026-01-05 17:52:02,403: t15.2025.04.13 val PER: 0.2825
2026-01-05 17:52:02,708: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_14500
2026-01-05 17:52:12,541: Train batch 14600: loss: 19.41 grad norm: 67.25 time: 0.075
2026-01-05 17:52:32,135: Train batch 14800: loss: 12.89 grad norm: 46.48 time: 0.063
2026-01-05 17:52:51,215: Train batch 15000: loss: 15.95 grad norm: 58.24 time: 0.063
2026-01-05 17:52:51,215: Running test after training batch: 15000
2026-01-05 17:52:51,334: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:52:56,317: WER debug example
  GT : you can see the code at this point as well
  PR : cued kantz eakes the tadych colds hatt cysts points has swells
2026-01-05 17:52:56,349: WER debug example
  GT : how does it keep the cost down
  PR : soho ouzts dusts hitt heaps thus costs dentzer
2026-01-05 17:52:58,132: Val batch 15000: PER (avg): 0.2144 CTC Loss (avg): 23.1610 WER(1gram): 107.87% (n=64) time: 6.917
2026-01-05 17:52:58,133: WER lens: avg_true_words=6.16 avg_pred_words=6.69 max_pred_words=13
2026-01-05 17:52:58,133: t15.2023.08.13 val PER: 0.1871
2026-01-05 17:52:58,133: t15.2023.08.18 val PER: 0.1861
2026-01-05 17:52:58,133: t15.2023.08.20 val PER: 0.1946
2026-01-05 17:52:58,133: t15.2023.08.25 val PER: 0.1807
2026-01-05 17:52:58,133: t15.2023.08.27 val PER: 0.2637
2026-01-05 17:52:58,133: t15.2023.09.01 val PER: 0.1494
2026-01-05 17:52:58,134: t15.2023.09.03 val PER: 0.2518
2026-01-05 17:52:58,134: t15.2023.09.24 val PER: 0.1893
2026-01-05 17:52:58,134: t15.2023.09.29 val PER: 0.2125
2026-01-05 17:52:58,134: t15.2023.10.01 val PER: 0.2246
2026-01-05 17:52:58,134: t15.2023.10.06 val PER: 0.1647
2026-01-05 17:52:58,134: t15.2023.10.08 val PER: 0.3004
2026-01-05 17:52:58,134: t15.2023.10.13 val PER: 0.2746
2026-01-05 17:52:58,134: t15.2023.10.15 val PER: 0.2136
2026-01-05 17:52:58,134: t15.2023.10.20 val PER: 0.2315
2026-01-05 17:52:58,134: t15.2023.10.22 val PER: 0.1570
2026-01-05 17:52:58,134: t15.2023.11.03 val PER: 0.2436
2026-01-05 17:52:58,134: t15.2023.11.04 val PER: 0.1058
2026-01-05 17:52:58,134: t15.2023.11.17 val PER: 0.1151
2026-01-05 17:52:58,135: t15.2023.11.19 val PER: 0.1098
2026-01-05 17:52:58,135: t15.2023.11.26 val PER: 0.2283
2026-01-05 17:52:58,135: t15.2023.12.03 val PER: 0.1849
2026-01-05 17:52:58,135: t15.2023.12.08 val PER: 0.1671
2026-01-05 17:52:58,135: t15.2023.12.10 val PER: 0.1695
2026-01-05 17:52:58,135: t15.2023.12.17 val PER: 0.1840
2026-01-05 17:52:58,135: t15.2023.12.29 val PER: 0.2100
2026-01-05 17:52:58,135: t15.2024.02.25 val PER: 0.1756
2026-01-05 17:52:58,135: t15.2024.03.08 val PER: 0.2603
2026-01-05 17:52:58,136: t15.2024.03.15 val PER: 0.2301
2026-01-05 17:52:58,136: t15.2024.03.17 val PER: 0.2022
2026-01-05 17:52:58,136: t15.2024.05.10 val PER: 0.2229
2026-01-05 17:52:58,136: t15.2024.06.14 val PER: 0.2177
2026-01-05 17:52:58,136: t15.2024.07.19 val PER: 0.2604
2026-01-05 17:52:58,136: t15.2024.07.21 val PER: 0.1386
2026-01-05 17:52:58,136: t15.2024.07.28 val PER: 0.1772
2026-01-05 17:52:58,136: t15.2025.01.10 val PER: 0.3691
2026-01-05 17:52:58,136: t15.2025.01.12 val PER: 0.2317
2026-01-05 17:52:58,136: t15.2025.03.14 val PER: 0.3536
2026-01-05 17:52:58,136: t15.2025.03.16 val PER: 0.2448
2026-01-05 17:52:58,136: t15.2025.03.30 val PER: 0.3414
2026-01-05 17:52:58,136: t15.2025.04.13 val PER: 0.2839
2026-01-05 17:52:58,428: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_15000
2026-01-05 17:53:17,278: Train batch 15200: loss: 12.27 grad norm: 47.16 time: 0.071
2026-01-05 17:53:36,275: Train batch 15400: loss: 18.52 grad norm: 63.34 time: 0.061
2026-01-05 17:53:46,298: Running test after training batch: 15500
2026-01-05 17:53:46,396: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:53:51,457: WER debug example
  GT : you can see the code at this point as well
  PR : dudes canned seats the tadych colds hatz this tahoe pointed has swells
2026-01-05 17:53:51,492: WER debug example
  GT : how does it keep the cost down
  PR : duhe ouzts dusts hitt heaps thus costs
2026-01-05 17:53:53,347: Val batch 15500: PER (avg): 0.2153 CTC Loss (avg): 23.0068 WER(1gram): 105.84% (n=64) time: 7.049
2026-01-05 17:53:53,348: WER lens: avg_true_words=6.16 avg_pred_words=6.62 max_pred_words=12
2026-01-05 17:53:53,348: t15.2023.08.13 val PER: 0.1944
2026-01-05 17:53:53,348: t15.2023.08.18 val PER: 0.1852
2026-01-05 17:53:53,348: t15.2023.08.20 val PER: 0.1890
2026-01-05 17:53:53,348: t15.2023.08.25 val PER: 0.1732
2026-01-05 17:53:53,349: t15.2023.08.27 val PER: 0.2669
2026-01-05 17:53:53,349: t15.2023.09.01 val PER: 0.1461
2026-01-05 17:53:53,349: t15.2023.09.03 val PER: 0.2470
2026-01-05 17:53:53,349: t15.2023.09.24 val PER: 0.1978
2026-01-05 17:53:53,349: t15.2023.09.29 val PER: 0.2061
2026-01-05 17:53:53,349: t15.2023.10.01 val PER: 0.2246
2026-01-05 17:53:53,349: t15.2023.10.06 val PER: 0.1658
2026-01-05 17:53:53,349: t15.2023.10.08 val PER: 0.2977
2026-01-05 17:53:53,350: t15.2023.10.13 val PER: 0.2793
2026-01-05 17:53:53,350: t15.2023.10.15 val PER: 0.2156
2026-01-05 17:53:53,350: t15.2023.10.20 val PER: 0.2248
2026-01-05 17:53:53,350: t15.2023.10.22 val PER: 0.1648
2026-01-05 17:53:53,350: t15.2023.11.03 val PER: 0.2469
2026-01-05 17:53:53,350: t15.2023.11.04 val PER: 0.0956
2026-01-05 17:53:53,350: t15.2023.11.17 val PER: 0.1166
2026-01-05 17:53:53,350: t15.2023.11.19 val PER: 0.1198
2026-01-05 17:53:53,350: t15.2023.11.26 val PER: 0.2203
2026-01-05 17:53:53,350: t15.2023.12.03 val PER: 0.1859
2026-01-05 17:53:53,350: t15.2023.12.08 val PER: 0.1631
2026-01-05 17:53:53,350: t15.2023.12.10 val PER: 0.1616
2026-01-05 17:53:53,350: t15.2023.12.17 val PER: 0.1871
2026-01-05 17:53:53,350: t15.2023.12.29 val PER: 0.2135
2026-01-05 17:53:53,350: t15.2024.02.25 val PER: 0.1643
2026-01-05 17:53:53,350: t15.2024.03.08 val PER: 0.2575
2026-01-05 17:53:53,350: t15.2024.03.15 val PER: 0.2427
2026-01-05 17:53:53,351: t15.2024.03.17 val PER: 0.2064
2026-01-05 17:53:53,351: t15.2024.05.10 val PER: 0.2229
2026-01-05 17:53:53,351: t15.2024.06.14 val PER: 0.2145
2026-01-05 17:53:53,351: t15.2024.07.19 val PER: 0.2657
2026-01-05 17:53:53,351: t15.2024.07.21 val PER: 0.1407
2026-01-05 17:53:53,351: t15.2024.07.28 val PER: 0.1801
2026-01-05 17:53:53,351: t15.2025.01.10 val PER: 0.3581
2026-01-05 17:53:53,351: t15.2025.01.12 val PER: 0.2417
2026-01-05 17:53:53,351: t15.2025.03.14 val PER: 0.3491
2026-01-05 17:53:53,351: t15.2025.03.16 val PER: 0.2592
2026-01-05 17:53:53,351: t15.2025.03.30 val PER: 0.3471
2026-01-05 17:53:53,351: t15.2025.04.13 val PER: 0.2839
2026-01-05 17:53:53,655: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_15500
2026-01-05 17:54:03,450: Train batch 15600: loss: 21.75 grad norm: 70.20 time: 0.076
2026-01-05 17:54:23,361: Train batch 15800: loss: 23.18 grad norm: 72.88 time: 0.084
2026-01-05 17:54:42,536: Train batch 16000: loss: 14.70 grad norm: 57.59 time: 0.068
2026-01-05 17:54:42,536: Running test after training batch: 16000
2026-01-05 17:54:42,664: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:54:47,639: WER debug example
  GT : you can see the code at this point as well
  PR : yu kantz eats the tadych colds hatz this tahoe pointed his swells
2026-01-05 17:54:47,671: WER debug example
  GT : how does it keep the cost down
  PR : duhe ouzts dusts hitt heaps thus cost setzer
2026-01-05 17:54:49,527: Val batch 16000: PER (avg): 0.2134 CTC Loss (avg): 23.0869 WER(1gram): 104.57% (n=64) time: 6.990
2026-01-05 17:54:49,527: WER lens: avg_true_words=6.16 avg_pred_words=6.61 max_pred_words=12
2026-01-05 17:54:49,527: t15.2023.08.13 val PER: 0.1944
2026-01-05 17:54:49,527: t15.2023.08.18 val PER: 0.1886
2026-01-05 17:54:49,528: t15.2023.08.20 val PER: 0.1851
2026-01-05 17:54:49,528: t15.2023.08.25 val PER: 0.1717
2026-01-05 17:54:49,528: t15.2023.08.27 val PER: 0.2701
2026-01-05 17:54:49,528: t15.2023.09.01 val PER: 0.1477
2026-01-05 17:54:49,528: t15.2023.09.03 val PER: 0.2482
2026-01-05 17:54:49,528: t15.2023.09.24 val PER: 0.1978
2026-01-05 17:54:49,528: t15.2023.09.29 val PER: 0.2042
2026-01-05 17:54:49,528: t15.2023.10.01 val PER: 0.2226
2026-01-05 17:54:49,528: t15.2023.10.06 val PER: 0.1647
2026-01-05 17:54:49,528: t15.2023.10.08 val PER: 0.2882
2026-01-05 17:54:49,528: t15.2023.10.13 val PER: 0.2770
2026-01-05 17:54:49,529: t15.2023.10.15 val PER: 0.2189
2026-01-05 17:54:49,529: t15.2023.10.20 val PER: 0.2349
2026-01-05 17:54:49,529: t15.2023.10.22 val PER: 0.1637
2026-01-05 17:54:49,529: t15.2023.11.03 val PER: 0.2422
2026-01-05 17:54:49,529: t15.2023.11.04 val PER: 0.0922
2026-01-05 17:54:49,529: t15.2023.11.17 val PER: 0.1135
2026-01-05 17:54:49,529: t15.2023.11.19 val PER: 0.1178
2026-01-05 17:54:49,529: t15.2023.11.26 val PER: 0.2210
2026-01-05 17:54:49,529: t15.2023.12.03 val PER: 0.1870
2026-01-05 17:54:49,529: t15.2023.12.08 val PER: 0.1638
2026-01-05 17:54:49,529: t15.2023.12.10 val PER: 0.1695
2026-01-05 17:54:49,529: t15.2023.12.17 val PER: 0.1798
2026-01-05 17:54:49,529: t15.2023.12.29 val PER: 0.2032
2026-01-05 17:54:49,530: t15.2024.02.25 val PER: 0.1601
2026-01-05 17:54:49,530: t15.2024.03.08 val PER: 0.2603
2026-01-05 17:54:49,530: t15.2024.03.15 val PER: 0.2376
2026-01-05 17:54:49,530: t15.2024.03.17 val PER: 0.2050
2026-01-05 17:54:49,530: t15.2024.05.10 val PER: 0.2229
2026-01-05 17:54:49,530: t15.2024.06.14 val PER: 0.2114
2026-01-05 17:54:49,530: t15.2024.07.19 val PER: 0.2544
2026-01-05 17:54:49,530: t15.2024.07.21 val PER: 0.1400
2026-01-05 17:54:49,530: t15.2024.07.28 val PER: 0.1787
2026-01-05 17:54:49,530: t15.2025.01.10 val PER: 0.3691
2026-01-05 17:54:49,530: t15.2025.01.12 val PER: 0.2309
2026-01-05 17:54:49,530: t15.2025.03.14 val PER: 0.3447
2026-01-05 17:54:49,530: t15.2025.03.16 val PER: 0.2552
2026-01-05 17:54:49,530: t15.2025.03.30 val PER: 0.3437
2026-01-05 17:54:49,530: t15.2025.04.13 val PER: 0.2796
2026-01-05 17:54:49,846: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_16000
2026-01-05 17:55:09,596: Train batch 16200: loss: 13.35 grad norm: 54.15 time: 0.069
2026-01-05 17:55:29,714: Train batch 16400: loss: 15.66 grad norm: 72.45 time: 0.072
2026-01-05 17:55:39,643: Running test after training batch: 16500
2026-01-05 17:55:39,764: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:55:44,728: WER debug example
  GT : you can see the code at this point as well
  PR : cued kantz eakes the tadych colds hatz this tahoe pointed has swells
2026-01-05 17:55:44,760: WER debug example
  GT : how does it keep the cost down
  PR : duhe ouzts dusts hitt heaps thus costs
2026-01-05 17:55:46,575: Val batch 16500: PER (avg): 0.2126 CTC Loss (avg): 23.0784 WER(1gram): 105.84% (n=64) time: 6.931
2026-01-05 17:55:46,575: WER lens: avg_true_words=6.16 avg_pred_words=6.55 max_pred_words=12
2026-01-05 17:55:46,576: t15.2023.08.13 val PER: 0.1881
2026-01-05 17:55:46,576: t15.2023.08.18 val PER: 0.1852
2026-01-05 17:55:46,576: t15.2023.08.20 val PER: 0.1851
2026-01-05 17:55:46,576: t15.2023.08.25 val PER: 0.1672
2026-01-05 17:55:46,576: t15.2023.08.27 val PER: 0.2556
2026-01-05 17:55:46,576: t15.2023.09.01 val PER: 0.1437
2026-01-05 17:55:46,576: t15.2023.09.03 val PER: 0.2375
2026-01-05 17:55:46,577: t15.2023.09.24 val PER: 0.1905
2026-01-05 17:55:46,577: t15.2023.09.29 val PER: 0.2042
2026-01-05 17:55:46,577: t15.2023.10.01 val PER: 0.2312
2026-01-05 17:55:46,577: t15.2023.10.06 val PER: 0.1625
2026-01-05 17:55:46,577: t15.2023.10.08 val PER: 0.2909
2026-01-05 17:55:46,577: t15.2023.10.13 val PER: 0.2762
2026-01-05 17:55:46,577: t15.2023.10.15 val PER: 0.2129
2026-01-05 17:55:46,577: t15.2023.10.20 val PER: 0.2315
2026-01-05 17:55:46,577: t15.2023.10.22 val PER: 0.1615
2026-01-05 17:55:46,577: t15.2023.11.03 val PER: 0.2374
2026-01-05 17:55:46,578: t15.2023.11.04 val PER: 0.0990
2026-01-05 17:55:46,578: t15.2023.11.17 val PER: 0.1120
2026-01-05 17:55:46,578: t15.2023.11.19 val PER: 0.1118
2026-01-05 17:55:46,578: t15.2023.11.26 val PER: 0.2145
2026-01-05 17:55:46,578: t15.2023.12.03 val PER: 0.1807
2026-01-05 17:55:46,578: t15.2023.12.08 val PER: 0.1658
2026-01-05 17:55:46,578: t15.2023.12.10 val PER: 0.1643
2026-01-05 17:55:46,578: t15.2023.12.17 val PER: 0.1871
2026-01-05 17:55:46,578: t15.2023.12.29 val PER: 0.2073
2026-01-05 17:55:46,578: t15.2024.02.25 val PER: 0.1629
2026-01-05 17:55:46,578: t15.2024.03.08 val PER: 0.2632
2026-01-05 17:55:46,579: t15.2024.03.15 val PER: 0.2276
2026-01-05 17:55:46,579: t15.2024.03.17 val PER: 0.2008
2026-01-05 17:55:46,579: t15.2024.05.10 val PER: 0.2333
2026-01-05 17:55:46,579: t15.2024.06.14 val PER: 0.2161
2026-01-05 17:55:46,579: t15.2024.07.19 val PER: 0.2690
2026-01-05 17:55:46,579: t15.2024.07.21 val PER: 0.1379
2026-01-05 17:55:46,579: t15.2024.07.28 val PER: 0.1691
2026-01-05 17:55:46,579: t15.2025.01.10 val PER: 0.3581
2026-01-05 17:55:46,579: t15.2025.01.12 val PER: 0.2340
2026-01-05 17:55:46,579: t15.2025.03.14 val PER: 0.3609
2026-01-05 17:55:46,580: t15.2025.03.16 val PER: 0.2579
2026-01-05 17:55:46,580: t15.2025.03.30 val PER: 0.3586
2026-01-05 17:55:46,580: t15.2025.04.13 val PER: 0.2853
2026-01-05 17:55:46,871: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_16500
2026-01-05 17:55:56,824: Train batch 16600: loss: 16.93 grad norm: 53.49 time: 0.065
2026-01-05 17:56:16,734: Train batch 16800: loss: 21.65 grad norm: 68.51 time: 0.077
2026-01-05 17:56:36,595: Train batch 17000: loss: 14.11 grad norm: 51.48 time: 0.102
2026-01-05 17:56:36,595: Running test after training batch: 17000
2026-01-05 17:56:36,691: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:56:41,640: WER debug example
  GT : you can see the code at this point as well
  PR : viewed kantz eakes the tadych colds hatz this tahoe pointed has swells
2026-01-05 17:56:41,673: WER debug example
  GT : how does it keep the cost down
  PR : duhe ouzts dusts hitt heaps thus costs
2026-01-05 17:56:43,470: Val batch 17000: PER (avg): 0.2127 CTC Loss (avg): 22.7864 WER(1gram): 104.82% (n=64) time: 6.875
2026-01-05 17:56:43,470: WER lens: avg_true_words=6.16 avg_pred_words=6.48 max_pred_words=12
2026-01-05 17:56:43,470: t15.2023.08.13 val PER: 0.1892
2026-01-05 17:56:43,470: t15.2023.08.18 val PER: 0.1869
2026-01-05 17:56:43,470: t15.2023.08.20 val PER: 0.1859
2026-01-05 17:56:43,471: t15.2023.08.25 val PER: 0.1792
2026-01-05 17:56:43,471: t15.2023.08.27 val PER: 0.2685
2026-01-05 17:56:43,471: t15.2023.09.01 val PER: 0.1534
2026-01-05 17:56:43,471: t15.2023.09.03 val PER: 0.2447
2026-01-05 17:56:43,471: t15.2023.09.24 val PER: 0.2015
2026-01-05 17:56:43,471: t15.2023.09.29 val PER: 0.2042
2026-01-05 17:56:43,471: t15.2023.10.01 val PER: 0.2213
2026-01-05 17:56:43,471: t15.2023.10.06 val PER: 0.1647
2026-01-05 17:56:43,471: t15.2023.10.08 val PER: 0.2909
2026-01-05 17:56:43,471: t15.2023.10.13 val PER: 0.2793
2026-01-05 17:56:43,471: t15.2023.10.15 val PER: 0.2116
2026-01-05 17:56:43,471: t15.2023.10.20 val PER: 0.2148
2026-01-05 17:56:43,471: t15.2023.10.22 val PER: 0.1604
2026-01-05 17:56:43,471: t15.2023.11.03 val PER: 0.2442
2026-01-05 17:56:43,472: t15.2023.11.04 val PER: 0.0990
2026-01-05 17:56:43,472: t15.2023.11.17 val PER: 0.1135
2026-01-05 17:56:43,472: t15.2023.11.19 val PER: 0.1138
2026-01-05 17:56:43,472: t15.2023.11.26 val PER: 0.2109
2026-01-05 17:56:43,472: t15.2023.12.03 val PER: 0.1880
2026-01-05 17:56:43,472: t15.2023.12.08 val PER: 0.1611
2026-01-05 17:56:43,472: t15.2023.12.10 val PER: 0.1603
2026-01-05 17:56:43,472: t15.2023.12.17 val PER: 0.1850
2026-01-05 17:56:43,472: t15.2023.12.29 val PER: 0.1997
2026-01-05 17:56:43,472: t15.2024.02.25 val PER: 0.1657
2026-01-05 17:56:43,472: t15.2024.03.08 val PER: 0.2617
2026-01-05 17:56:43,472: t15.2024.03.15 val PER: 0.2283
2026-01-05 17:56:43,472: t15.2024.03.17 val PER: 0.2057
2026-01-05 17:56:43,472: t15.2024.05.10 val PER: 0.2155
2026-01-05 17:56:43,472: t15.2024.06.14 val PER: 0.2129
2026-01-05 17:56:43,472: t15.2024.07.19 val PER: 0.2597
2026-01-05 17:56:43,472: t15.2024.07.21 val PER: 0.1421
2026-01-05 17:56:43,473: t15.2024.07.28 val PER: 0.1735
2026-01-05 17:56:43,473: t15.2025.01.10 val PER: 0.3581
2026-01-05 17:56:43,473: t15.2025.01.12 val PER: 0.2325
2026-01-05 17:56:43,473: t15.2025.03.14 val PER: 0.3580
2026-01-05 17:56:43,473: t15.2025.03.16 val PER: 0.2592
2026-01-05 17:56:43,473: t15.2025.03.30 val PER: 0.3483
2026-01-05 17:56:43,473: t15.2025.04.13 val PER: 0.2882
2026-01-05 17:56:43,770: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_17000
2026-01-05 17:57:02,986: Train batch 17200: loss: 17.22 grad norm: 67.78 time: 0.105
2026-01-05 17:57:21,944: Train batch 17400: loss: 20.30 grad norm: 77.99 time: 0.088
2026-01-05 17:57:31,416: Running test after training batch: 17500
2026-01-05 17:57:31,557: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:57:36,753: WER debug example
  GT : you can see the code at this point as well
  PR : viewed kantz eats the tadych colds hatz this tahoe pointed has swells
2026-01-05 17:57:36,795: WER debug example
  GT : how does it keep the cost down
  PR : duhe ouzts dusts hitt heaps thus costs
2026-01-05 17:57:39,686: Val batch 17500: PER (avg): 0.2100 CTC Loss (avg): 22.7234 WER(1gram): 105.84% (n=64) time: 8.270
2026-01-05 17:57:39,686: WER lens: avg_true_words=6.16 avg_pred_words=6.53 max_pred_words=12
2026-01-05 17:57:39,686: t15.2023.08.13 val PER: 0.1819
2026-01-05 17:57:39,687: t15.2023.08.18 val PER: 0.1861
2026-01-05 17:57:39,687: t15.2023.08.20 val PER: 0.1859
2026-01-05 17:57:39,687: t15.2023.08.25 val PER: 0.1717
2026-01-05 17:57:39,687: t15.2023.08.27 val PER: 0.2653
2026-01-05 17:57:39,687: t15.2023.09.01 val PER: 0.1445
2026-01-05 17:57:39,687: t15.2023.09.03 val PER: 0.2387
2026-01-05 17:57:39,687: t15.2023.09.24 val PER: 0.1990
2026-01-05 17:57:39,688: t15.2023.09.29 val PER: 0.1991
2026-01-05 17:57:39,688: t15.2023.10.01 val PER: 0.2219
2026-01-05 17:57:39,688: t15.2023.10.06 val PER: 0.1615
2026-01-05 17:57:39,688: t15.2023.10.08 val PER: 0.3018
2026-01-05 17:57:39,688: t15.2023.10.13 val PER: 0.2739
2026-01-05 17:57:39,688: t15.2023.10.15 val PER: 0.2096
2026-01-05 17:57:39,688: t15.2023.10.20 val PER: 0.2215
2026-01-05 17:57:39,688: t15.2023.10.22 val PER: 0.1592
2026-01-05 17:57:39,688: t15.2023.11.03 val PER: 0.2395
2026-01-05 17:57:39,688: t15.2023.11.04 val PER: 0.0990
2026-01-05 17:57:39,689: t15.2023.11.17 val PER: 0.1120
2026-01-05 17:57:39,689: t15.2023.11.19 val PER: 0.1138
2026-01-05 17:57:39,689: t15.2023.11.26 val PER: 0.2087
2026-01-05 17:57:39,689: t15.2023.12.03 val PER: 0.1786
2026-01-05 17:57:39,689: t15.2023.12.08 val PER: 0.1585
2026-01-05 17:57:39,689: t15.2023.12.10 val PER: 0.1577
2026-01-05 17:57:39,689: t15.2023.12.17 val PER: 0.1757
2026-01-05 17:57:39,689: t15.2023.12.29 val PER: 0.2052
2026-01-05 17:57:39,689: t15.2024.02.25 val PER: 0.1657
2026-01-05 17:57:39,689: t15.2024.03.08 val PER: 0.2560
2026-01-05 17:57:39,689: t15.2024.03.15 val PER: 0.2276
2026-01-05 17:57:39,690: t15.2024.03.17 val PER: 0.2015
2026-01-05 17:57:39,690: t15.2024.05.10 val PER: 0.2244
2026-01-05 17:57:39,690: t15.2024.06.14 val PER: 0.2224
2026-01-05 17:57:39,690: t15.2024.07.19 val PER: 0.2531
2026-01-05 17:57:39,690: t15.2024.07.21 val PER: 0.1372
2026-01-05 17:57:39,690: t15.2024.07.28 val PER: 0.1691
2026-01-05 17:57:39,690: t15.2025.01.10 val PER: 0.3595
2026-01-05 17:57:39,690: t15.2025.01.12 val PER: 0.2279
2026-01-05 17:57:39,690: t15.2025.03.14 val PER: 0.3580
2026-01-05 17:57:39,690: t15.2025.03.16 val PER: 0.2539
2026-01-05 17:57:39,690: t15.2025.03.30 val PER: 0.3460
2026-01-05 17:57:39,690: t15.2025.04.13 val PER: 0.2782
2026-01-05 17:57:40,005: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_17500
2026-01-05 17:57:49,483: Train batch 17600: loss: 19.06 grad norm: 62.31 time: 0.062
2026-01-05 17:58:08,511: Train batch 17800: loss: 12.46 grad norm: 60.54 time: 0.051
2026-01-05 17:58:28,274: Train batch 18000: loss: 15.79 grad norm: 61.91 time: 0.075
2026-01-05 17:58:28,274: Running test after training batch: 18000
2026-01-05 17:58:28,412: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:58:33,345: WER debug example
  GT : you can see the code at this point as well
  PR : viewed kantz eats the troha colds hatz this tahoe pointed has swells
2026-01-05 17:58:33,376: WER debug example
  GT : how does it keep the cost down
  PR : duhe ouzts dusts hitt heaps thus costs
2026-01-05 17:58:35,211: Val batch 18000: PER (avg): 0.2090 CTC Loss (avg): 22.5933 WER(1gram): 104.06% (n=64) time: 6.937
2026-01-05 17:58:35,212: WER lens: avg_true_words=6.16 avg_pred_words=6.42 max_pred_words=12
2026-01-05 17:58:35,212: t15.2023.08.13 val PER: 0.1830
2026-01-05 17:58:35,212: t15.2023.08.18 val PER: 0.1836
2026-01-05 17:58:35,212: t15.2023.08.20 val PER: 0.1787
2026-01-05 17:58:35,212: t15.2023.08.25 val PER: 0.1657
2026-01-05 17:58:35,212: t15.2023.08.27 val PER: 0.2621
2026-01-05 17:58:35,212: t15.2023.09.01 val PER: 0.1445
2026-01-05 17:58:35,212: t15.2023.09.03 val PER: 0.2458
2026-01-05 17:58:35,213: t15.2023.09.24 val PER: 0.1954
2026-01-05 17:58:35,213: t15.2023.09.29 val PER: 0.2004
2026-01-05 17:58:35,213: t15.2023.10.01 val PER: 0.2219
2026-01-05 17:58:35,213: t15.2023.10.06 val PER: 0.1572
2026-01-05 17:58:35,213: t15.2023.10.08 val PER: 0.2923
2026-01-05 17:58:35,213: t15.2023.10.13 val PER: 0.2739
2026-01-05 17:58:35,213: t15.2023.10.15 val PER: 0.2070
2026-01-05 17:58:35,213: t15.2023.10.20 val PER: 0.2282
2026-01-05 17:58:35,213: t15.2023.10.22 val PER: 0.1604
2026-01-05 17:58:35,213: t15.2023.11.03 val PER: 0.2361
2026-01-05 17:58:35,214: t15.2023.11.04 val PER: 0.0990
2026-01-05 17:58:35,214: t15.2023.11.17 val PER: 0.1104
2026-01-05 17:58:35,214: t15.2023.11.19 val PER: 0.1098
2026-01-05 17:58:35,214: t15.2023.11.26 val PER: 0.2080
2026-01-05 17:58:35,214: t15.2023.12.03 val PER: 0.1817
2026-01-05 17:58:35,214: t15.2023.12.08 val PER: 0.1651
2026-01-05 17:58:35,214: t15.2023.12.10 val PER: 0.1590
2026-01-05 17:58:35,214: t15.2023.12.17 val PER: 0.1861
2026-01-05 17:58:35,214: t15.2023.12.29 val PER: 0.1990
2026-01-05 17:58:35,214: t15.2024.02.25 val PER: 0.1601
2026-01-05 17:58:35,214: t15.2024.03.08 val PER: 0.2589
2026-01-05 17:58:35,214: t15.2024.03.15 val PER: 0.2208
2026-01-05 17:58:35,214: t15.2024.03.17 val PER: 0.2043
2026-01-05 17:58:35,214: t15.2024.05.10 val PER: 0.2244
2026-01-05 17:58:35,214: t15.2024.06.14 val PER: 0.2129
2026-01-05 17:58:35,214: t15.2024.07.19 val PER: 0.2577
2026-01-05 17:58:35,215: t15.2024.07.21 val PER: 0.1338
2026-01-05 17:58:35,215: t15.2024.07.28 val PER: 0.1684
2026-01-05 17:58:35,215: t15.2025.01.10 val PER: 0.3636
2026-01-05 17:58:35,215: t15.2025.01.12 val PER: 0.2209
2026-01-05 17:58:35,215: t15.2025.03.14 val PER: 0.3491
2026-01-05 17:58:35,215: t15.2025.03.16 val PER: 0.2474
2026-01-05 17:58:35,215: t15.2025.03.30 val PER: 0.3552
2026-01-05 17:58:35,215: t15.2025.04.13 val PER: 0.2767
2026-01-05 17:58:35,521: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_18000
2026-01-05 17:58:55,515: Train batch 18200: loss: 17.26 grad norm: 132.96 time: 0.094
2026-01-05 17:59:15,153: Train batch 18400: loss: 10.34 grad norm: 54.82 time: 0.074
2026-01-05 17:59:24,537: Running test after training batch: 18500
2026-01-05 17:59:24,644: WER debug GT example: You can see the code at this point as well.
2026-01-05 17:59:29,637: WER debug example
  GT : you can see the code at this point as well
  PR : yu kantz eakes the troha colds hatz this tahoe pointed has swells
2026-01-05 17:59:29,670: WER debug example
  GT : how does it keep the cost down
  PR : duhe ouzts dusts hitt heaps thus costs
2026-01-05 17:59:31,499: Val batch 18500: PER (avg): 0.2098 CTC Loss (avg): 22.6208 WER(1gram): 105.84% (n=64) time: 6.961
2026-01-05 17:59:31,499: WER lens: avg_true_words=6.16 avg_pred_words=6.48 max_pred_words=12
2026-01-05 17:59:31,499: t15.2023.08.13 val PER: 0.1819
2026-01-05 17:59:31,499: t15.2023.08.18 val PER: 0.1894
2026-01-05 17:59:31,500: t15.2023.08.20 val PER: 0.1803
2026-01-05 17:59:31,500: t15.2023.08.25 val PER: 0.1762
2026-01-05 17:59:31,500: t15.2023.08.27 val PER: 0.2540
2026-01-05 17:59:31,500: t15.2023.09.01 val PER: 0.1461
2026-01-05 17:59:31,500: t15.2023.09.03 val PER: 0.2470
2026-01-05 17:59:31,500: t15.2023.09.24 val PER: 0.1978
2026-01-05 17:59:31,500: t15.2023.09.29 val PER: 0.2004
2026-01-05 17:59:31,500: t15.2023.10.01 val PER: 0.2232
2026-01-05 17:59:31,500: t15.2023.10.06 val PER: 0.1604
2026-01-05 17:59:31,501: t15.2023.10.08 val PER: 0.2977
2026-01-05 17:59:31,501: t15.2023.10.13 val PER: 0.2754
2026-01-05 17:59:31,501: t15.2023.10.15 val PER: 0.2083
2026-01-05 17:59:31,501: t15.2023.10.20 val PER: 0.2181
2026-01-05 17:59:31,501: t15.2023.10.22 val PER: 0.1637
2026-01-05 17:59:31,501: t15.2023.11.03 val PER: 0.2429
2026-01-05 17:59:31,501: t15.2023.11.04 val PER: 0.0956
2026-01-05 17:59:31,501: t15.2023.11.17 val PER: 0.1089
2026-01-05 17:59:31,501: t15.2023.11.19 val PER: 0.1118
2026-01-05 17:59:31,501: t15.2023.11.26 val PER: 0.2080
2026-01-05 17:59:31,501: t15.2023.12.03 val PER: 0.1765
2026-01-05 17:59:31,501: t15.2023.12.08 val PER: 0.1605
2026-01-05 17:59:31,501: t15.2023.12.10 val PER: 0.1629
2026-01-05 17:59:31,502: t15.2023.12.17 val PER: 0.1746
2026-01-05 17:59:31,502: t15.2023.12.29 val PER: 0.2004
2026-01-05 17:59:31,502: t15.2024.02.25 val PER: 0.1643
2026-01-05 17:59:31,502: t15.2024.03.08 val PER: 0.2546
2026-01-05 17:59:31,502: t15.2024.03.15 val PER: 0.2214
2026-01-05 17:59:31,502: t15.2024.03.17 val PER: 0.2036
2026-01-05 17:59:31,502: t15.2024.05.10 val PER: 0.2140
2026-01-05 17:59:31,502: t15.2024.06.14 val PER: 0.2224
2026-01-05 17:59:31,502: t15.2024.07.19 val PER: 0.2591
2026-01-05 17:59:31,502: t15.2024.07.21 val PER: 0.1352
2026-01-05 17:59:31,502: t15.2024.07.28 val PER: 0.1713
2026-01-05 17:59:31,502: t15.2025.01.10 val PER: 0.3664
2026-01-05 17:59:31,502: t15.2025.01.12 val PER: 0.2256
2026-01-05 17:59:31,502: t15.2025.03.14 val PER: 0.3565
2026-01-05 17:59:31,502: t15.2025.03.16 val PER: 0.2448
2026-01-05 17:59:31,502: t15.2025.03.30 val PER: 0.3471
2026-01-05 17:59:31,503: t15.2025.04.13 val PER: 0.2767
2026-01-05 17:59:31,790: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_18500
2026-01-05 17:59:41,288: Train batch 18600: loss: 18.99 grad norm: 65.85 time: 0.083
2026-01-05 18:00:01,460: Train batch 18800: loss: 17.11 grad norm: 62.61 time: 0.082
2026-01-05 18:00:21,161: Train batch 19000: loss: 14.91 grad norm: 58.47 time: 0.080
2026-01-05 18:00:21,161: Running test after training batch: 19000
2026-01-05 18:00:21,289: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:00:26,329: WER debug example
  GT : you can see the code at this point as well
  PR : viewed kantz eakes the troha colds hatz this tahoe pointed has swells
2026-01-05 18:00:26,362: WER debug example
  GT : how does it keep the cost down
  PR : duhe ouzts dusts hitt heaps thus costs
2026-01-05 18:00:28,224: Val batch 19000: PER (avg): 0.2092 CTC Loss (avg): 22.6291 WER(1gram): 104.82% (n=64) time: 7.062
2026-01-05 18:00:28,224: WER lens: avg_true_words=6.16 avg_pred_words=6.44 max_pred_words=12
2026-01-05 18:00:28,224: t15.2023.08.13 val PER: 0.1830
2026-01-05 18:00:28,224: t15.2023.08.18 val PER: 0.1869
2026-01-05 18:00:28,224: t15.2023.08.20 val PER: 0.1835
2026-01-05 18:00:28,224: t15.2023.08.25 val PER: 0.1777
2026-01-05 18:00:28,225: t15.2023.08.27 val PER: 0.2540
2026-01-05 18:00:28,225: t15.2023.09.01 val PER: 0.1420
2026-01-05 18:00:28,225: t15.2023.09.03 val PER: 0.2458
2026-01-05 18:00:28,225: t15.2023.09.24 val PER: 0.2002
2026-01-05 18:00:28,225: t15.2023.09.29 val PER: 0.1966
2026-01-05 18:00:28,225: t15.2023.10.01 val PER: 0.2219
2026-01-05 18:00:28,225: t15.2023.10.06 val PER: 0.1582
2026-01-05 18:00:28,225: t15.2023.10.08 val PER: 0.2936
2026-01-05 18:00:28,226: t15.2023.10.13 val PER: 0.2669
2026-01-05 18:00:28,226: t15.2023.10.15 val PER: 0.2063
2026-01-05 18:00:28,226: t15.2023.10.20 val PER: 0.2248
2026-01-05 18:00:28,226: t15.2023.10.22 val PER: 0.1682
2026-01-05 18:00:28,226: t15.2023.11.03 val PER: 0.2402
2026-01-05 18:00:28,226: t15.2023.11.04 val PER: 0.0956
2026-01-05 18:00:28,226: t15.2023.11.17 val PER: 0.1104
2026-01-05 18:00:28,226: t15.2023.11.19 val PER: 0.1158
2026-01-05 18:00:28,226: t15.2023.11.26 val PER: 0.2072
2026-01-05 18:00:28,227: t15.2023.12.03 val PER: 0.1807
2026-01-05 18:00:28,227: t15.2023.12.08 val PER: 0.1578
2026-01-05 18:00:28,227: t15.2023.12.10 val PER: 0.1577
2026-01-05 18:00:28,227: t15.2023.12.17 val PER: 0.1778
2026-01-05 18:00:28,227: t15.2023.12.29 val PER: 0.1997
2026-01-05 18:00:28,227: t15.2024.02.25 val PER: 0.1629
2026-01-05 18:00:28,227: t15.2024.03.08 val PER: 0.2560
2026-01-05 18:00:28,227: t15.2024.03.15 val PER: 0.2245
2026-01-05 18:00:28,228: t15.2024.03.17 val PER: 0.2001
2026-01-05 18:00:28,228: t15.2024.05.10 val PER: 0.2199
2026-01-05 18:00:28,228: t15.2024.06.14 val PER: 0.2161
2026-01-05 18:00:28,228: t15.2024.07.19 val PER: 0.2637
2026-01-05 18:00:28,228: t15.2024.07.21 val PER: 0.1352
2026-01-05 18:00:28,228: t15.2024.07.28 val PER: 0.1721
2026-01-05 18:00:28,228: t15.2025.01.10 val PER: 0.3595
2026-01-05 18:00:28,228: t15.2025.01.12 val PER: 0.2225
2026-01-05 18:00:28,228: t15.2025.03.14 val PER: 0.3521
2026-01-05 18:00:28,229: t15.2025.03.16 val PER: 0.2539
2026-01-05 18:00:28,229: t15.2025.03.30 val PER: 0.3414
2026-01-05 18:00:28,229: t15.2025.04.13 val PER: 0.2839
2026-01-05 18:00:28,523: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_19000
2026-01-05 18:00:47,954: Train batch 19200: loss: 12.68 grad norm: 58.23 time: 0.078
2026-01-05 18:01:07,808: Train batch 19400: loss: 12.96 grad norm: 49.06 time: 0.065
2026-01-05 18:01:17,786: Running test after training batch: 19500
2026-01-05 18:01:17,884: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:01:22,912: WER debug example
  GT : you can see the code at this point as well
  PR : yu kantz eats the troha colds hatz this tahoe pointed has swells
2026-01-05 18:01:22,944: WER debug example
  GT : how does it keep the cost down
  PR : duhe ouzts dusts hitt heaps thus costs
2026-01-05 18:01:24,818: Val batch 19500: PER (avg): 0.2084 CTC Loss (avg): 22.6688 WER(1gram): 104.57% (n=64) time: 7.032
2026-01-05 18:01:24,819: WER lens: avg_true_words=6.16 avg_pred_words=6.45 max_pred_words=12
2026-01-05 18:01:24,819: t15.2023.08.13 val PER: 0.1819
2026-01-05 18:01:24,819: t15.2023.08.18 val PER: 0.1869
2026-01-05 18:01:24,819: t15.2023.08.20 val PER: 0.1843
2026-01-05 18:01:24,819: t15.2023.08.25 val PER: 0.1747
2026-01-05 18:01:24,819: t15.2023.08.27 val PER: 0.2524
2026-01-05 18:01:24,819: t15.2023.09.01 val PER: 0.1437
2026-01-05 18:01:24,819: t15.2023.09.03 val PER: 0.2458
2026-01-05 18:01:24,820: t15.2023.09.24 val PER: 0.1990
2026-01-05 18:01:24,820: t15.2023.09.29 val PER: 0.1991
2026-01-05 18:01:24,820: t15.2023.10.01 val PER: 0.2279
2026-01-05 18:01:24,820: t15.2023.10.06 val PER: 0.1572
2026-01-05 18:01:24,820: t15.2023.10.08 val PER: 0.3018
2026-01-05 18:01:24,820: t15.2023.10.13 val PER: 0.2661
2026-01-05 18:01:24,820: t15.2023.10.15 val PER: 0.2037
2026-01-05 18:01:24,820: t15.2023.10.20 val PER: 0.2315
2026-01-05 18:01:24,820: t15.2023.10.22 val PER: 0.1604
2026-01-05 18:01:24,821: t15.2023.11.03 val PER: 0.2354
2026-01-05 18:01:24,821: t15.2023.11.04 val PER: 0.0990
2026-01-05 18:01:24,821: t15.2023.11.17 val PER: 0.1089
2026-01-05 18:01:24,821: t15.2023.11.19 val PER: 0.1078
2026-01-05 18:01:24,821: t15.2023.11.26 val PER: 0.2094
2026-01-05 18:01:24,821: t15.2023.12.03 val PER: 0.1733
2026-01-05 18:01:24,821: t15.2023.12.08 val PER: 0.1551
2026-01-05 18:01:24,821: t15.2023.12.10 val PER: 0.1564
2026-01-05 18:01:24,821: t15.2023.12.17 val PER: 0.1798
2026-01-05 18:01:24,821: t15.2023.12.29 val PER: 0.2045
2026-01-05 18:01:24,821: t15.2024.02.25 val PER: 0.1629
2026-01-05 18:01:24,821: t15.2024.03.08 val PER: 0.2589
2026-01-05 18:01:24,821: t15.2024.03.15 val PER: 0.2183
2026-01-05 18:01:24,821: t15.2024.03.17 val PER: 0.2064
2026-01-05 18:01:24,821: t15.2024.05.10 val PER: 0.2199
2026-01-05 18:01:24,821: t15.2024.06.14 val PER: 0.2145
2026-01-05 18:01:24,822: t15.2024.07.19 val PER: 0.2571
2026-01-05 18:01:24,822: t15.2024.07.21 val PER: 0.1324
2026-01-05 18:01:24,822: t15.2024.07.28 val PER: 0.1647
2026-01-05 18:01:24,822: t15.2025.01.10 val PER: 0.3526
2026-01-05 18:01:24,822: t15.2025.01.12 val PER: 0.2263
2026-01-05 18:01:24,822: t15.2025.03.14 val PER: 0.3550
2026-01-05 18:01:24,822: t15.2025.03.16 val PER: 0.2539
2026-01-05 18:01:24,822: t15.2025.03.30 val PER: 0.3402
2026-01-05 18:01:24,822: t15.2025.04.13 val PER: 0.2782
2026-01-05 18:01:25,111: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_19500
2026-01-05 18:01:34,762: Train batch 19600: loss: 18.37 grad norm: 61.08 time: 0.070
2026-01-05 18:01:54,367: Train batch 19800: loss: 11.00 grad norm: 51.83 time: 0.068
2026-01-05 18:02:13,191: Running test after training batch: 19999
2026-01-05 18:02:13,286: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:02:18,185: WER debug example
  GT : you can see the code at this point as well
  PR : yu kantz eats the troha colds hatz this tahoe pointed has swells
2026-01-05 18:02:18,217: WER debug example
  GT : how does it keep the cost down
  PR : duhe ouzts dusts hitt heaps thus costs
2026-01-05 18:02:20,075: Val batch 19999: PER (avg): 0.2088 CTC Loss (avg): 22.4402 WER(1gram): 105.08% (n=64) time: 6.884
2026-01-05 18:02:20,075: WER lens: avg_true_words=6.16 avg_pred_words=6.53 max_pred_words=13
2026-01-05 18:02:20,076: t15.2023.08.13 val PER: 0.1788
2026-01-05 18:02:20,076: t15.2023.08.18 val PER: 0.1844
2026-01-05 18:02:20,076: t15.2023.08.20 val PER: 0.1843
2026-01-05 18:02:20,076: t15.2023.08.25 val PER: 0.1747
2026-01-05 18:02:20,076: t15.2023.08.27 val PER: 0.2540
2026-01-05 18:02:20,076: t15.2023.09.01 val PER: 0.1453
2026-01-05 18:02:20,076: t15.2023.09.03 val PER: 0.2482
2026-01-05 18:02:20,076: t15.2023.09.24 val PER: 0.2002
2026-01-05 18:02:20,076: t15.2023.09.29 val PER: 0.2004
2026-01-05 18:02:20,077: t15.2023.10.01 val PER: 0.2199
2026-01-05 18:02:20,077: t15.2023.10.06 val PER: 0.1572
2026-01-05 18:02:20,077: t15.2023.10.08 val PER: 0.2936
2026-01-05 18:02:20,077: t15.2023.10.13 val PER: 0.2746
2026-01-05 18:02:20,077: t15.2023.10.15 val PER: 0.2123
2026-01-05 18:02:20,077: t15.2023.10.20 val PER: 0.2148
2026-01-05 18:02:20,077: t15.2023.10.22 val PER: 0.1604
2026-01-05 18:02:20,077: t15.2023.11.03 val PER: 0.2429
2026-01-05 18:02:20,077: t15.2023.11.04 val PER: 0.0990
2026-01-05 18:02:20,077: t15.2023.11.17 val PER: 0.1073
2026-01-05 18:02:20,077: t15.2023.11.19 val PER: 0.0998
2026-01-05 18:02:20,077: t15.2023.11.26 val PER: 0.2051
2026-01-05 18:02:20,077: t15.2023.12.03 val PER: 0.1744
2026-01-05 18:02:20,077: t15.2023.12.08 val PER: 0.1598
2026-01-05 18:02:20,077: t15.2023.12.10 val PER: 0.1564
2026-01-05 18:02:20,077: t15.2023.12.17 val PER: 0.1767
2026-01-05 18:02:20,078: t15.2023.12.29 val PER: 0.2018
2026-01-05 18:02:20,078: t15.2024.02.25 val PER: 0.1601
2026-01-05 18:02:20,078: t15.2024.03.08 val PER: 0.2589
2026-01-05 18:02:20,078: t15.2024.03.15 val PER: 0.2276
2026-01-05 18:02:20,078: t15.2024.03.17 val PER: 0.1980
2026-01-05 18:02:20,078: t15.2024.05.10 val PER: 0.2273
2026-01-05 18:02:20,078: t15.2024.06.14 val PER: 0.2082
2026-01-05 18:02:20,078: t15.2024.07.19 val PER: 0.2637
2026-01-05 18:02:20,078: t15.2024.07.21 val PER: 0.1372
2026-01-05 18:02:20,078: t15.2024.07.28 val PER: 0.1684
2026-01-05 18:02:20,078: t15.2025.01.10 val PER: 0.3526
2026-01-05 18:02:20,078: t15.2025.01.12 val PER: 0.2279
2026-01-05 18:02:20,078: t15.2025.03.14 val PER: 0.3521
2026-01-05 18:02:20,078: t15.2025.03.16 val PER: 0.2435
2026-01-05 18:02:20,078: t15.2025.03.30 val PER: 0.3368
2026-01-05 18:02:20,078: t15.2025.04.13 val PER: 0.2782
2026-01-05 18:02:20,387: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st3_wd1e-5/checkpoint/checkpoint_batch_19999
2026-01-05 18:02:20,411: Best avg val PER achieved: 0.54540
2026-01-05 18:02:20,411: Total training time: 38.73 minutes

=== RUN st4_wd1e-5.yaml ===
2026-01-05 18:02:25,118: Using device: cuda:0
2026-01-05 18:02:26,686: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-05 18:02:26,712: Using 45 sessions after filtering (from 45).
2026-01-05 18:02:27,168: Using torch.compile (if available)
2026-01-05 18:02:27,169: torch.compile not available (torch<2.0). Skipping.
2026-01-05 18:02:27,169: Initialized RNN decoding model
2026-01-05 18:02:27,169: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-05 18:02:27,170: Model has 44,907,305 parameters
2026-01-05 18:02:27,170: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-05 18:02:28,447: Successfully initialized datasets
2026-01-05 18:02:28,447: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-05 18:02:29,461: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.208
2026-01-05 18:02:29,461: Running test after training batch: 0
2026-01-05 18:02:29,573: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:02:34,840: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-05 18:02:35,574: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-05 18:03:10,953: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 41.491
2026-01-05 18:03:10,953: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-05 18:03:10,953: t15.2023.08.13 val PER: 1.3056
2026-01-05 18:03:10,954: t15.2023.08.18 val PER: 1.4208
2026-01-05 18:03:10,954: t15.2023.08.20 val PER: 1.3002
2026-01-05 18:03:10,954: t15.2023.08.25 val PER: 1.3389
2026-01-05 18:03:10,954: t15.2023.08.27 val PER: 1.2460
2026-01-05 18:03:10,954: t15.2023.09.01 val PER: 1.4537
2026-01-05 18:03:10,954: t15.2023.09.03 val PER: 1.3171
2026-01-05 18:03:10,955: t15.2023.09.24 val PER: 1.5461
2026-01-05 18:03:10,955: t15.2023.09.29 val PER: 1.4671
2026-01-05 18:03:10,955: t15.2023.10.01 val PER: 1.2147
2026-01-05 18:03:10,955: t15.2023.10.06 val PER: 1.4876
2026-01-05 18:03:10,955: t15.2023.10.08 val PER: 1.1827
2026-01-05 18:03:10,955: t15.2023.10.13 val PER: 1.3964
2026-01-05 18:03:10,955: t15.2023.10.15 val PER: 1.3889
2026-01-05 18:03:10,955: t15.2023.10.20 val PER: 1.4866
2026-01-05 18:03:10,955: t15.2023.10.22 val PER: 1.3942
2026-01-05 18:03:10,955: t15.2023.11.03 val PER: 1.5923
2026-01-05 18:03:10,955: t15.2023.11.04 val PER: 2.0171
2026-01-05 18:03:10,955: t15.2023.11.17 val PER: 1.9518
2026-01-05 18:03:10,955: t15.2023.11.19 val PER: 1.6707
2026-01-05 18:03:10,955: t15.2023.11.26 val PER: 1.5413
2026-01-05 18:03:10,956: t15.2023.12.03 val PER: 1.4254
2026-01-05 18:03:10,956: t15.2023.12.08 val PER: 1.4487
2026-01-05 18:03:10,956: t15.2023.12.10 val PER: 1.6899
2026-01-05 18:03:10,956: t15.2023.12.17 val PER: 1.3077
2026-01-05 18:03:10,956: t15.2023.12.29 val PER: 1.4063
2026-01-05 18:03:10,956: t15.2024.02.25 val PER: 1.4228
2026-01-05 18:03:10,956: t15.2024.03.08 val PER: 1.3257
2026-01-05 18:03:10,956: t15.2024.03.15 val PER: 1.3196
2026-01-05 18:03:10,956: t15.2024.03.17 val PER: 1.4052
2026-01-05 18:03:10,957: t15.2024.05.10 val PER: 1.3224
2026-01-05 18:03:10,957: t15.2024.06.14 val PER: 1.5315
2026-01-05 18:03:10,957: t15.2024.07.19 val PER: 1.0817
2026-01-05 18:03:10,957: t15.2024.07.21 val PER: 1.6290
2026-01-05 18:03:10,957: t15.2024.07.28 val PER: 1.6588
2026-01-05 18:03:10,957: t15.2025.01.10 val PER: 1.0923
2026-01-05 18:03:10,957: t15.2025.01.12 val PER: 1.7629
2026-01-05 18:03:10,957: t15.2025.03.14 val PER: 1.0414
2026-01-05 18:03:10,957: t15.2025.03.16 val PER: 1.6257
2026-01-05 18:03:10,957: t15.2025.03.30 val PER: 1.2874
2026-01-05 18:03:10,958: t15.2025.04.13 val PER: 1.5949
2026-01-05 18:03:10,958: New best val WER(1gram) inf% --> 100.00%
2026-01-05 18:03:10,958: Checkpointing model
2026-01-05 18:03:11,248: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:03:11,544: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_0
2026-01-05 18:03:31,501: Train batch 200: loss: 77.59 grad norm: 106.03 time: 0.055
2026-01-05 18:03:50,616: Train batch 400: loss: 53.88 grad norm: 94.30 time: 0.064
2026-01-05 18:04:00,296: Running test after training batch: 500
2026-01-05 18:04:00,445: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:04:05,457: WER debug example
  GT : you can see the code at this point as well
  PR : used and ease thus uhde at this ide is aisle
2026-01-05 18:04:05,505: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as adz
2026-01-05 18:04:08,579: Val batch 500: PER (avg): 0.5233 CTC Loss (avg): 55.1639 WER(1gram): 88.07% (n=64) time: 8.283
2026-01-05 18:04:08,580: WER lens: avg_true_words=6.16 avg_pred_words=5.59 max_pred_words=12
2026-01-05 18:04:08,581: t15.2023.08.13 val PER: 0.4699
2026-01-05 18:04:08,581: t15.2023.08.18 val PER: 0.4593
2026-01-05 18:04:08,581: t15.2023.08.20 val PER: 0.4392
2026-01-05 18:04:08,581: t15.2023.08.25 val PER: 0.4428
2026-01-05 18:04:08,581: t15.2023.08.27 val PER: 0.5305
2026-01-05 18:04:08,581: t15.2023.09.01 val PER: 0.4278
2026-01-05 18:04:08,582: t15.2023.09.03 val PER: 0.5024
2026-01-05 18:04:08,582: t15.2023.09.24 val PER: 0.4369
2026-01-05 18:04:08,582: t15.2023.09.29 val PER: 0.4754
2026-01-05 18:04:08,582: t15.2023.10.01 val PER: 0.5291
2026-01-05 18:04:08,582: t15.2023.10.06 val PER: 0.4316
2026-01-05 18:04:08,582: t15.2023.10.08 val PER: 0.5386
2026-01-05 18:04:08,582: t15.2023.10.13 val PER: 0.5842
2026-01-05 18:04:08,582: t15.2023.10.15 val PER: 0.4984
2026-01-05 18:04:08,582: t15.2023.10.20 val PER: 0.4664
2026-01-05 18:04:08,583: t15.2023.10.22 val PER: 0.4532
2026-01-05 18:04:08,583: t15.2023.11.03 val PER: 0.5136
2026-01-05 18:04:08,583: t15.2023.11.04 val PER: 0.2560
2026-01-05 18:04:08,583: t15.2023.11.17 val PER: 0.3624
2026-01-05 18:04:08,583: t15.2023.11.19 val PER: 0.3253
2026-01-05 18:04:08,583: t15.2023.11.26 val PER: 0.5543
2026-01-05 18:04:08,583: t15.2023.12.03 val PER: 0.5126
2026-01-05 18:04:08,583: t15.2023.12.08 val PER: 0.5246
2026-01-05 18:04:08,583: t15.2023.12.10 val PER: 0.4520
2026-01-05 18:04:08,583: t15.2023.12.17 val PER: 0.5696
2026-01-05 18:04:08,583: t15.2023.12.29 val PER: 0.5463
2026-01-05 18:04:08,583: t15.2024.02.25 val PER: 0.4916
2026-01-05 18:04:08,583: t15.2024.03.08 val PER: 0.6088
2026-01-05 18:04:08,583: t15.2024.03.15 val PER: 0.5560
2026-01-05 18:04:08,583: t15.2024.03.17 val PER: 0.5188
2026-01-05 18:04:08,583: t15.2024.05.10 val PER: 0.5542
2026-01-05 18:04:08,584: t15.2024.06.14 val PER: 0.5158
2026-01-05 18:04:08,584: t15.2024.07.19 val PER: 0.6664
2026-01-05 18:04:08,584: t15.2024.07.21 val PER: 0.4766
2026-01-05 18:04:08,584: t15.2024.07.28 val PER: 0.5184
2026-01-05 18:04:08,584: t15.2025.01.10 val PER: 0.7534
2026-01-05 18:04:08,584: t15.2025.01.12 val PER: 0.5658
2026-01-05 18:04:08,584: t15.2025.03.14 val PER: 0.7574
2026-01-05 18:04:08,584: t15.2025.03.16 val PER: 0.5995
2026-01-05 18:04:08,584: t15.2025.03.30 val PER: 0.7345
2026-01-05 18:04:08,584: t15.2025.04.13 val PER: 0.5820
2026-01-05 18:04:08,585: New best val WER(1gram) 100.00% --> 88.07%
2026-01-05 18:04:08,586: Checkpointing model
2026-01-05 18:04:10,322: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:04:10,612: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_500
2026-01-05 18:04:20,323: Train batch 600: loss: 48.90 grad norm: 81.85 time: 0.079
2026-01-05 18:04:40,480: Train batch 800: loss: 41.18 grad norm: 86.01 time: 0.058
2026-01-05 18:05:00,083: Train batch 1000: loss: 42.25 grad norm: 75.95 time: 0.067
2026-01-05 18:05:00,084: Running test after training batch: 1000
2026-01-05 18:05:00,271: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:05:05,099: WER debug example
  GT : you can see the code at this point as well
  PR : used ent ease thus good it this and is while
2026-01-05 18:05:05,132: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke that wass it
2026-01-05 18:05:06,981: Val batch 1000: PER (avg): 0.4082 CTC Loss (avg): 42.5010 WER(1gram): 80.46% (n=64) time: 6.897
2026-01-05 18:05:06,982: WER lens: avg_true_words=6.16 avg_pred_words=5.55 max_pred_words=12
2026-01-05 18:05:06,982: t15.2023.08.13 val PER: 0.3701
2026-01-05 18:05:06,982: t15.2023.08.18 val PER: 0.3345
2026-01-05 18:05:06,982: t15.2023.08.20 val PER: 0.3447
2026-01-05 18:05:06,982: t15.2023.08.25 val PER: 0.2982
2026-01-05 18:05:06,983: t15.2023.08.27 val PER: 0.4325
2026-01-05 18:05:06,983: t15.2023.09.01 val PER: 0.3044
2026-01-05 18:05:06,983: t15.2023.09.03 val PER: 0.3979
2026-01-05 18:05:06,983: t15.2023.09.24 val PER: 0.3228
2026-01-05 18:05:06,983: t15.2023.09.29 val PER: 0.3682
2026-01-05 18:05:06,983: t15.2023.10.01 val PER: 0.4016
2026-01-05 18:05:06,983: t15.2023.10.06 val PER: 0.3089
2026-01-05 18:05:06,983: t15.2023.10.08 val PER: 0.4655
2026-01-05 18:05:06,983: t15.2023.10.13 val PER: 0.4678
2026-01-05 18:05:06,983: t15.2023.10.15 val PER: 0.3823
2026-01-05 18:05:06,983: t15.2023.10.20 val PER: 0.3691
2026-01-05 18:05:06,983: t15.2023.10.22 val PER: 0.3486
2026-01-05 18:05:06,983: t15.2023.11.03 val PER: 0.3962
2026-01-05 18:05:06,983: t15.2023.11.04 val PER: 0.1604
2026-01-05 18:05:06,984: t15.2023.11.17 val PER: 0.2722
2026-01-05 18:05:06,984: t15.2023.11.19 val PER: 0.2176
2026-01-05 18:05:06,984: t15.2023.11.26 val PER: 0.4464
2026-01-05 18:05:06,984: t15.2023.12.03 val PER: 0.4023
2026-01-05 18:05:06,984: t15.2023.12.08 val PER: 0.4028
2026-01-05 18:05:06,984: t15.2023.12.10 val PER: 0.3469
2026-01-05 18:05:06,984: t15.2023.12.17 val PER: 0.4064
2026-01-05 18:05:06,984: t15.2023.12.29 val PER: 0.4049
2026-01-05 18:05:06,984: t15.2024.02.25 val PER: 0.3581
2026-01-05 18:05:06,984: t15.2024.03.08 val PER: 0.4964
2026-01-05 18:05:06,984: t15.2024.03.15 val PER: 0.4484
2026-01-05 18:05:06,984: t15.2024.03.17 val PER: 0.4052
2026-01-05 18:05:06,985: t15.2024.05.10 val PER: 0.4205
2026-01-05 18:05:06,985: t15.2024.06.14 val PER: 0.4022
2026-01-05 18:05:06,985: t15.2024.07.19 val PER: 0.5386
2026-01-05 18:05:06,985: t15.2024.07.21 val PER: 0.3738
2026-01-05 18:05:06,985: t15.2024.07.28 val PER: 0.4176
2026-01-05 18:05:06,985: t15.2025.01.10 val PER: 0.6061
2026-01-05 18:05:06,985: t15.2025.01.12 val PER: 0.4503
2026-01-05 18:05:06,985: t15.2025.03.14 val PER: 0.6302
2026-01-05 18:05:06,985: t15.2025.03.16 val PER: 0.4830
2026-01-05 18:05:06,985: t15.2025.03.30 val PER: 0.6322
2026-01-05 18:05:06,985: t15.2025.04.13 val PER: 0.4879
2026-01-05 18:05:06,986: New best val WER(1gram) 88.07% --> 80.46%
2026-01-05 18:05:06,986: Checkpointing model
2026-01-05 18:05:08,610: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:05:08,895: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_1000
2026-01-05 18:05:27,342: Train batch 1200: loss: 33.15 grad norm: 74.91 time: 0.069
2026-01-05 18:05:45,589: Train batch 1400: loss: 35.84 grad norm: 76.00 time: 0.061
2026-01-05 18:05:54,757: Running test after training batch: 1500
2026-01-05 18:05:54,861: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:05:59,794: WER debug example
  GT : you can see the code at this point as well
  PR : yule kint e the good it this boyde is will
2026-01-05 18:05:59,831: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke that us
2026-01-05 18:06:01,766: Val batch 1500: PER (avg): 0.3778 CTC Loss (avg): 37.2592 WER(1gram): 76.90% (n=64) time: 7.008
2026-01-05 18:06:01,766: WER lens: avg_true_words=6.16 avg_pred_words=5.16 max_pred_words=11
2026-01-05 18:06:01,767: t15.2023.08.13 val PER: 0.3482
2026-01-05 18:06:01,767: t15.2023.08.18 val PER: 0.3152
2026-01-05 18:06:01,767: t15.2023.08.20 val PER: 0.3129
2026-01-05 18:06:01,767: t15.2023.08.25 val PER: 0.2590
2026-01-05 18:06:01,767: t15.2023.08.27 val PER: 0.3810
2026-01-05 18:06:01,767: t15.2023.09.01 val PER: 0.2792
2026-01-05 18:06:01,767: t15.2023.09.03 val PER: 0.3646
2026-01-05 18:06:01,767: t15.2023.09.24 val PER: 0.3083
2026-01-05 18:06:01,768: t15.2023.09.29 val PER: 0.3389
2026-01-05 18:06:01,768: t15.2023.10.01 val PER: 0.3937
2026-01-05 18:06:01,768: t15.2023.10.06 val PER: 0.2809
2026-01-05 18:06:01,768: t15.2023.10.08 val PER: 0.4276
2026-01-05 18:06:01,768: t15.2023.10.13 val PER: 0.4375
2026-01-05 18:06:01,768: t15.2023.10.15 val PER: 0.3540
2026-01-05 18:06:01,768: t15.2023.10.20 val PER: 0.3523
2026-01-05 18:06:01,768: t15.2023.10.22 val PER: 0.3218
2026-01-05 18:06:01,768: t15.2023.11.03 val PER: 0.3616
2026-01-05 18:06:01,768: t15.2023.11.04 val PER: 0.1365
2026-01-05 18:06:01,768: t15.2023.11.17 val PER: 0.2193
2026-01-05 18:06:01,768: t15.2023.11.19 val PER: 0.1697
2026-01-05 18:06:01,768: t15.2023.11.26 val PER: 0.4196
2026-01-05 18:06:01,768: t15.2023.12.03 val PER: 0.3697
2026-01-05 18:06:01,769: t15.2023.12.08 val PER: 0.3522
2026-01-05 18:06:01,769: t15.2023.12.10 val PER: 0.2943
2026-01-05 18:06:01,769: t15.2023.12.17 val PER: 0.3669
2026-01-05 18:06:01,769: t15.2023.12.29 val PER: 0.3693
2026-01-05 18:06:01,769: t15.2024.02.25 val PER: 0.3090
2026-01-05 18:06:01,769: t15.2024.03.08 val PER: 0.4637
2026-01-05 18:06:01,769: t15.2024.03.15 val PER: 0.4159
2026-01-05 18:06:01,769: t15.2024.03.17 val PER: 0.3801
2026-01-05 18:06:01,769: t15.2024.05.10 val PER: 0.3923
2026-01-05 18:06:01,769: t15.2024.06.14 val PER: 0.3991
2026-01-05 18:06:01,769: t15.2024.07.19 val PER: 0.5129
2026-01-05 18:06:01,769: t15.2024.07.21 val PER: 0.3503
2026-01-05 18:06:01,770: t15.2024.07.28 val PER: 0.3618
2026-01-05 18:06:01,770: t15.2025.01.10 val PER: 0.6061
2026-01-05 18:06:01,770: t15.2025.01.12 val PER: 0.4196
2026-01-05 18:06:01,770: t15.2025.03.14 val PER: 0.5843
2026-01-05 18:06:01,770: t15.2025.03.16 val PER: 0.4529
2026-01-05 18:06:01,770: t15.2025.03.30 val PER: 0.6126
2026-01-05 18:06:01,770: t15.2025.04.13 val PER: 0.4765
2026-01-05 18:06:01,771: New best val WER(1gram) 80.46% --> 76.90%
2026-01-05 18:06:01,771: Checkpointing model
2026-01-05 18:06:03,452: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:06:03,756: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_1500
2026-01-05 18:06:13,475: Train batch 1600: loss: 36.94 grad norm: 80.38 time: 0.063
2026-01-05 18:06:32,661: Train batch 1800: loss: 35.24 grad norm: 70.68 time: 0.090
2026-01-05 18:06:51,297: Train batch 2000: loss: 34.44 grad norm: 77.15 time: 0.069
2026-01-05 18:06:51,298: Running test after training batch: 2000
2026-01-05 18:06:51,441: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:06:56,190: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this bonde is will
2026-01-05 18:06:56,219: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap thus wass it
2026-01-05 18:06:57,779: Val batch 2000: PER (avg): 0.3258 CTC Loss (avg): 32.8062 WER(1gram): 71.32% (n=64) time: 6.482
2026-01-05 18:06:57,780: WER lens: avg_true_words=6.16 avg_pred_words=5.64 max_pred_words=11
2026-01-05 18:06:57,780: t15.2023.08.13 val PER: 0.3119
2026-01-05 18:06:57,780: t15.2023.08.18 val PER: 0.2624
2026-01-05 18:06:57,780: t15.2023.08.20 val PER: 0.2589
2026-01-05 18:06:57,780: t15.2023.08.25 val PER: 0.2364
2026-01-05 18:06:57,780: t15.2023.08.27 val PER: 0.3424
2026-01-05 18:06:57,780: t15.2023.09.01 val PER: 0.2346
2026-01-05 18:06:57,781: t15.2023.09.03 val PER: 0.3385
2026-01-05 18:06:57,781: t15.2023.09.24 val PER: 0.2476
2026-01-05 18:06:57,781: t15.2023.09.29 val PER: 0.2744
2026-01-05 18:06:57,781: t15.2023.10.01 val PER: 0.3289
2026-01-05 18:06:57,781: t15.2023.10.06 val PER: 0.2325
2026-01-05 18:06:57,781: t15.2023.10.08 val PER: 0.3938
2026-01-05 18:06:57,781: t15.2023.10.13 val PER: 0.3685
2026-01-05 18:06:57,781: t15.2023.10.15 val PER: 0.3032
2026-01-05 18:06:57,781: t15.2023.10.20 val PER: 0.2886
2026-01-05 18:06:57,781: t15.2023.10.22 val PER: 0.2517
2026-01-05 18:06:57,781: t15.2023.11.03 val PER: 0.3229
2026-01-05 18:06:57,781: t15.2023.11.04 val PER: 0.0956
2026-01-05 18:06:57,781: t15.2023.11.17 val PER: 0.1680
2026-01-05 18:06:57,782: t15.2023.11.19 val PER: 0.1198
2026-01-05 18:06:57,782: t15.2023.11.26 val PER: 0.3638
2026-01-05 18:06:57,782: t15.2023.12.03 val PER: 0.3162
2026-01-05 18:06:57,782: t15.2023.12.08 val PER: 0.3029
2026-01-05 18:06:57,782: t15.2023.12.10 val PER: 0.2523
2026-01-05 18:06:57,782: t15.2023.12.17 val PER: 0.3098
2026-01-05 18:06:57,782: t15.2023.12.29 val PER: 0.3219
2026-01-05 18:06:57,782: t15.2024.02.25 val PER: 0.2781
2026-01-05 18:06:57,782: t15.2024.03.08 val PER: 0.3855
2026-01-05 18:06:57,782: t15.2024.03.15 val PER: 0.3527
2026-01-05 18:06:57,782: t15.2024.03.17 val PER: 0.3438
2026-01-05 18:06:57,782: t15.2024.05.10 val PER: 0.3314
2026-01-05 18:06:57,782: t15.2024.06.14 val PER: 0.3470
2026-01-05 18:06:57,782: t15.2024.07.19 val PER: 0.4693
2026-01-05 18:06:57,782: t15.2024.07.21 val PER: 0.2959
2026-01-05 18:06:57,782: t15.2024.07.28 val PER: 0.3221
2026-01-05 18:06:57,782: t15.2025.01.10 val PER: 0.5303
2026-01-05 18:06:57,783: t15.2025.01.12 val PER: 0.3811
2026-01-05 18:06:57,783: t15.2025.03.14 val PER: 0.5133
2026-01-05 18:06:57,783: t15.2025.03.16 val PER: 0.3966
2026-01-05 18:06:57,783: t15.2025.03.30 val PER: 0.5379
2026-01-05 18:06:57,783: t15.2025.04.13 val PER: 0.4009
2026-01-05 18:06:57,784: New best val WER(1gram) 76.90% --> 71.32%
2026-01-05 18:06:57,785: Checkpointing model
2026-01-05 18:06:59,414: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:06:59,704: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_2000
2026-01-05 18:07:19,342: Train batch 2200: loss: 29.17 grad norm: 76.76 time: 0.060
2026-01-05 18:07:39,192: Train batch 2400: loss: 29.22 grad norm: 62.08 time: 0.051
2026-01-05 18:07:48,722: Running test after training batch: 2500
2026-01-05 18:07:48,880: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:07:53,678: WER debug example
  GT : you can see the code at this point as well
  PR : yule end e the code at this point is will
2026-01-05 18:07:53,705: WER debug example
  GT : how does it keep the cost down
  PR : houde just it eke the us it
2026-01-05 18:07:55,356: Val batch 2500: PER (avg): 0.3022 CTC Loss (avg): 30.0583 WER(1gram): 68.53% (n=64) time: 6.634
2026-01-05 18:07:55,356: WER lens: avg_true_words=6.16 avg_pred_words=5.73 max_pred_words=10
2026-01-05 18:07:55,357: t15.2023.08.13 val PER: 0.2911
2026-01-05 18:07:55,357: t15.2023.08.18 val PER: 0.2456
2026-01-05 18:07:55,357: t15.2023.08.20 val PER: 0.2359
2026-01-05 18:07:55,357: t15.2023.08.25 val PER: 0.2078
2026-01-05 18:07:55,357: t15.2023.08.27 val PER: 0.3199
2026-01-05 18:07:55,357: t15.2023.09.01 val PER: 0.2062
2026-01-05 18:07:55,357: t15.2023.09.03 val PER: 0.2957
2026-01-05 18:07:55,357: t15.2023.09.24 val PER: 0.2245
2026-01-05 18:07:55,357: t15.2023.09.29 val PER: 0.2521
2026-01-05 18:07:55,357: t15.2023.10.01 val PER: 0.3144
2026-01-05 18:07:55,357: t15.2023.10.06 val PER: 0.2207
2026-01-05 18:07:55,357: t15.2023.10.08 val PER: 0.3816
2026-01-05 18:07:55,357: t15.2023.10.13 val PER: 0.3553
2026-01-05 18:07:55,357: t15.2023.10.15 val PER: 0.2808
2026-01-05 18:07:55,358: t15.2023.10.20 val PER: 0.2718
2026-01-05 18:07:55,358: t15.2023.10.22 val PER: 0.2238
2026-01-05 18:07:55,358: t15.2023.11.03 val PER: 0.3039
2026-01-05 18:07:55,358: t15.2023.11.04 val PER: 0.0785
2026-01-05 18:07:55,358: t15.2023.11.17 val PER: 0.1415
2026-01-05 18:07:55,358: t15.2023.11.19 val PER: 0.1218
2026-01-05 18:07:55,358: t15.2023.11.26 val PER: 0.3413
2026-01-05 18:07:55,359: t15.2023.12.03 val PER: 0.2826
2026-01-05 18:07:55,359: t15.2023.12.08 val PER: 0.2763
2026-01-05 18:07:55,359: t15.2023.12.10 val PER: 0.2418
2026-01-05 18:07:55,359: t15.2023.12.17 val PER: 0.2775
2026-01-05 18:07:55,359: t15.2023.12.29 val PER: 0.3061
2026-01-05 18:07:55,359: t15.2024.02.25 val PER: 0.2430
2026-01-05 18:07:55,359: t15.2024.03.08 val PER: 0.3713
2026-01-05 18:07:55,359: t15.2024.03.15 val PER: 0.3390
2026-01-05 18:07:55,359: t15.2024.03.17 val PER: 0.3110
2026-01-05 18:07:55,359: t15.2024.05.10 val PER: 0.3091
2026-01-05 18:07:55,359: t15.2024.06.14 val PER: 0.3170
2026-01-05 18:07:55,360: t15.2024.07.19 val PER: 0.4370
2026-01-05 18:07:55,360: t15.2024.07.21 val PER: 0.2648
2026-01-05 18:07:55,360: t15.2024.07.28 val PER: 0.2971
2026-01-05 18:07:55,360: t15.2025.01.10 val PER: 0.4904
2026-01-05 18:07:55,360: t15.2025.01.12 val PER: 0.3533
2026-01-05 18:07:55,360: t15.2025.03.14 val PER: 0.4867
2026-01-05 18:07:55,360: t15.2025.03.16 val PER: 0.3691
2026-01-05 18:07:55,360: t15.2025.03.30 val PER: 0.5069
2026-01-05 18:07:55,360: t15.2025.04.13 val PER: 0.3766
2026-01-05 18:07:55,361: New best val WER(1gram) 71.32% --> 68.53%
2026-01-05 18:07:55,361: Checkpointing model
2026-01-05 18:07:57,019: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:07:57,309: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_2500
2026-01-05 18:08:06,315: Train batch 2600: loss: 35.10 grad norm: 84.64 time: 0.056
2026-01-05 18:08:24,510: Train batch 2800: loss: 25.55 grad norm: 67.58 time: 0.082
2026-01-05 18:08:43,354: Train batch 3000: loss: 31.58 grad norm: 82.48 time: 0.084
2026-01-05 18:08:43,354: Running test after training batch: 3000
2026-01-05 18:08:43,465: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:08:48,280: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-05 18:08:48,311: WER debug example
  GT : how does it keep the cost down
  PR : houde des it hipp the cost get
2026-01-05 18:08:49,951: Val batch 3000: PER (avg): 0.2815 CTC Loss (avg): 27.8245 WER(1gram): 66.24% (n=64) time: 6.597
2026-01-05 18:08:49,952: WER lens: avg_true_words=6.16 avg_pred_words=5.80 max_pred_words=11
2026-01-05 18:08:49,952: t15.2023.08.13 val PER: 0.2651
2026-01-05 18:08:49,952: t15.2023.08.18 val PER: 0.2188
2026-01-05 18:08:49,952: t15.2023.08.20 val PER: 0.2129
2026-01-05 18:08:49,953: t15.2023.08.25 val PER: 0.2033
2026-01-05 18:08:49,953: t15.2023.08.27 val PER: 0.2942
2026-01-05 18:08:49,953: t15.2023.09.01 val PER: 0.1883
2026-01-05 18:08:49,953: t15.2023.09.03 val PER: 0.2910
2026-01-05 18:08:49,953: t15.2023.09.24 val PER: 0.2039
2026-01-05 18:08:49,953: t15.2023.09.29 val PER: 0.2387
2026-01-05 18:08:49,953: t15.2023.10.01 val PER: 0.2906
2026-01-05 18:08:49,953: t15.2023.10.06 val PER: 0.1991
2026-01-05 18:08:49,953: t15.2023.10.08 val PER: 0.3532
2026-01-05 18:08:49,953: t15.2023.10.13 val PER: 0.3382
2026-01-05 18:08:49,953: t15.2023.10.15 val PER: 0.2630
2026-01-05 18:08:49,953: t15.2023.10.20 val PER: 0.2517
2026-01-05 18:08:49,954: t15.2023.10.22 val PER: 0.2016
2026-01-05 18:08:49,954: t15.2023.11.03 val PER: 0.2653
2026-01-05 18:08:49,954: t15.2023.11.04 val PER: 0.0887
2026-01-05 18:08:49,954: t15.2023.11.17 val PER: 0.1275
2026-01-05 18:08:49,954: t15.2023.11.19 val PER: 0.1238
2026-01-05 18:08:49,954: t15.2023.11.26 val PER: 0.3000
2026-01-05 18:08:49,954: t15.2023.12.03 val PER: 0.2626
2026-01-05 18:08:49,954: t15.2023.12.08 val PER: 0.2550
2026-01-05 18:08:49,954: t15.2023.12.10 val PER: 0.2142
2026-01-05 18:08:49,954: t15.2023.12.17 val PER: 0.2713
2026-01-05 18:08:49,955: t15.2023.12.29 val PER: 0.2855
2026-01-05 18:08:49,955: t15.2024.02.25 val PER: 0.2331
2026-01-05 18:08:49,955: t15.2024.03.08 val PER: 0.3542
2026-01-05 18:08:49,955: t15.2024.03.15 val PER: 0.3358
2026-01-05 18:08:49,955: t15.2024.03.17 val PER: 0.2866
2026-01-05 18:08:49,955: t15.2024.05.10 val PER: 0.3001
2026-01-05 18:08:49,955: t15.2024.06.14 val PER: 0.2886
2026-01-05 18:08:49,955: t15.2024.07.19 val PER: 0.4186
2026-01-05 18:08:49,955: t15.2024.07.21 val PER: 0.2359
2026-01-05 18:08:49,955: t15.2024.07.28 val PER: 0.2801
2026-01-05 18:08:49,955: t15.2025.01.10 val PER: 0.4821
2026-01-05 18:08:49,955: t15.2025.01.12 val PER: 0.3326
2026-01-05 18:08:49,955: t15.2025.03.14 val PER: 0.4512
2026-01-05 18:08:49,955: t15.2025.03.16 val PER: 0.3272
2026-01-05 18:08:49,956: t15.2025.03.30 val PER: 0.4851
2026-01-05 18:08:49,956: t15.2025.04.13 val PER: 0.3438
2026-01-05 18:08:49,957: New best val WER(1gram) 68.53% --> 66.24%
2026-01-05 18:08:49,957: Checkpointing model
2026-01-05 18:08:51,684: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:08:51,972: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_3000
2026-01-05 18:09:11,753: Train batch 3200: loss: 26.21 grad norm: 65.51 time: 0.076
2026-01-05 18:09:32,016: Train batch 3400: loss: 18.32 grad norm: 54.49 time: 0.049
2026-01-05 18:09:41,813: Running test after training batch: 3500
2026-01-05 18:09:41,968: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:09:46,800: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point will
2026-01-05 18:09:46,830: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it epp thus cussed get
2026-01-05 18:09:48,400: Val batch 3500: PER (avg): 0.2666 CTC Loss (avg): 26.6534 WER(1gram): 66.50% (n=64) time: 6.586
2026-01-05 18:09:48,400: WER lens: avg_true_words=6.16 avg_pred_words=6.02 max_pred_words=11
2026-01-05 18:09:48,400: t15.2023.08.13 val PER: 0.2412
2026-01-05 18:09:48,400: t15.2023.08.18 val PER: 0.2079
2026-01-05 18:09:48,401: t15.2023.08.20 val PER: 0.2168
2026-01-05 18:09:48,401: t15.2023.08.25 val PER: 0.1762
2026-01-05 18:09:48,401: t15.2023.08.27 val PER: 0.2701
2026-01-05 18:09:48,401: t15.2023.09.01 val PER: 0.1745
2026-01-05 18:09:48,401: t15.2023.09.03 val PER: 0.2553
2026-01-05 18:09:48,401: t15.2023.09.24 val PER: 0.2124
2026-01-05 18:09:48,401: t15.2023.09.29 val PER: 0.2189
2026-01-05 18:09:48,401: t15.2023.10.01 val PER: 0.2774
2026-01-05 18:09:48,401: t15.2023.10.06 val PER: 0.1905
2026-01-05 18:09:48,401: t15.2023.10.08 val PER: 0.3559
2026-01-05 18:09:48,402: t15.2023.10.13 val PER: 0.3126
2026-01-05 18:09:48,402: t15.2023.10.15 val PER: 0.2485
2026-01-05 18:09:48,402: t15.2023.10.20 val PER: 0.2450
2026-01-05 18:09:48,402: t15.2023.10.22 val PER: 0.2138
2026-01-05 18:09:48,402: t15.2023.11.03 val PER: 0.2653
2026-01-05 18:09:48,402: t15.2023.11.04 val PER: 0.0819
2026-01-05 18:09:48,402: t15.2023.11.17 val PER: 0.1182
2026-01-05 18:09:48,402: t15.2023.11.19 val PER: 0.0938
2026-01-05 18:09:48,402: t15.2023.11.26 val PER: 0.2841
2026-01-05 18:09:48,402: t15.2023.12.03 val PER: 0.2363
2026-01-05 18:09:48,402: t15.2023.12.08 val PER: 0.2390
2026-01-05 18:09:48,402: t15.2023.12.10 val PER: 0.1997
2026-01-05 18:09:48,403: t15.2023.12.17 val PER: 0.2536
2026-01-05 18:09:48,403: t15.2023.12.29 val PER: 0.2642
2026-01-05 18:09:48,403: t15.2024.02.25 val PER: 0.2037
2026-01-05 18:09:48,403: t15.2024.03.08 val PER: 0.3442
2026-01-05 18:09:48,403: t15.2024.03.15 val PER: 0.3114
2026-01-05 18:09:48,403: t15.2024.03.17 val PER: 0.2734
2026-01-05 18:09:48,403: t15.2024.05.10 val PER: 0.2749
2026-01-05 18:09:48,403: t15.2024.06.14 val PER: 0.2871
2026-01-05 18:09:48,403: t15.2024.07.19 val PER: 0.3942
2026-01-05 18:09:48,403: t15.2024.07.21 val PER: 0.2138
2026-01-05 18:09:48,403: t15.2024.07.28 val PER: 0.2735
2026-01-05 18:09:48,404: t15.2025.01.10 val PER: 0.4697
2026-01-05 18:09:48,404: t15.2025.01.12 val PER: 0.3010
2026-01-05 18:09:48,404: t15.2025.03.14 val PER: 0.4541
2026-01-05 18:09:48,404: t15.2025.03.16 val PER: 0.3455
2026-01-05 18:09:48,404: t15.2025.03.30 val PER: 0.4460
2026-01-05 18:09:48,404: t15.2025.04.13 val PER: 0.3367
2026-01-05 18:09:48,701: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_3500
2026-01-05 18:09:57,931: Train batch 3600: loss: 22.38 grad norm: 61.09 time: 0.067
2026-01-05 18:10:16,129: Train batch 3800: loss: 25.55 grad norm: 67.65 time: 0.068
2026-01-05 18:10:34,806: Train batch 4000: loss: 19.17 grad norm: 54.71 time: 0.056
2026-01-05 18:10:34,806: Running test after training batch: 4000
2026-01-05 18:10:34,910: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:10:39,730: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-05 18:10:39,759: WER debug example
  GT : how does it keep the cost down
  PR : aue dust it hipp thus cussed nett
2026-01-05 18:10:41,413: Val batch 4000: PER (avg): 0.2488 CTC Loss (avg): 24.4204 WER(1gram): 63.45% (n=64) time: 6.607
2026-01-05 18:10:41,413: WER lens: avg_true_words=6.16 avg_pred_words=6.00 max_pred_words=10
2026-01-05 18:10:41,414: t15.2023.08.13 val PER: 0.2204
2026-01-05 18:10:41,414: t15.2023.08.18 val PER: 0.1886
2026-01-05 18:10:41,414: t15.2023.08.20 val PER: 0.2033
2026-01-05 18:10:41,414: t15.2023.08.25 val PER: 0.1642
2026-01-05 18:10:41,414: t15.2023.08.27 val PER: 0.2749
2026-01-05 18:10:41,414: t15.2023.09.01 val PER: 0.1640
2026-01-05 18:10:41,414: t15.2023.09.03 val PER: 0.2423
2026-01-05 18:10:41,414: t15.2023.09.24 val PER: 0.2027
2026-01-05 18:10:41,414: t15.2023.09.29 val PER: 0.1972
2026-01-05 18:10:41,414: t15.2023.10.01 val PER: 0.2635
2026-01-05 18:10:41,414: t15.2023.10.06 val PER: 0.1765
2026-01-05 18:10:41,414: t15.2023.10.08 val PER: 0.3207
2026-01-05 18:10:41,415: t15.2023.10.13 val PER: 0.3088
2026-01-05 18:10:41,415: t15.2023.10.15 val PER: 0.2399
2026-01-05 18:10:41,415: t15.2023.10.20 val PER: 0.2349
2026-01-05 18:10:41,415: t15.2023.10.22 val PER: 0.1971
2026-01-05 18:10:41,415: t15.2023.11.03 val PER: 0.2476
2026-01-05 18:10:41,415: t15.2023.11.04 val PER: 0.0785
2026-01-05 18:10:41,415: t15.2023.11.17 val PER: 0.0980
2026-01-05 18:10:41,415: t15.2023.11.19 val PER: 0.0918
2026-01-05 18:10:41,415: t15.2023.11.26 val PER: 0.2616
2026-01-05 18:10:41,415: t15.2023.12.03 val PER: 0.2237
2026-01-05 18:10:41,415: t15.2023.12.08 val PER: 0.2217
2026-01-05 18:10:41,416: t15.2023.12.10 val PER: 0.1892
2026-01-05 18:10:41,416: t15.2023.12.17 val PER: 0.2464
2026-01-05 18:10:41,416: t15.2023.12.29 val PER: 0.2588
2026-01-05 18:10:41,416: t15.2024.02.25 val PER: 0.2163
2026-01-05 18:10:41,416: t15.2024.03.08 val PER: 0.3186
2026-01-05 18:10:41,416: t15.2024.03.15 val PER: 0.2989
2026-01-05 18:10:41,416: t15.2024.03.17 val PER: 0.2497
2026-01-05 18:10:41,416: t15.2024.05.10 val PER: 0.2675
2026-01-05 18:10:41,416: t15.2024.06.14 val PER: 0.2603
2026-01-05 18:10:41,416: t15.2024.07.19 val PER: 0.3764
2026-01-05 18:10:41,416: t15.2024.07.21 val PER: 0.1938
2026-01-05 18:10:41,416: t15.2024.07.28 val PER: 0.2419
2026-01-05 18:10:41,416: t15.2025.01.10 val PER: 0.4091
2026-01-05 18:10:41,416: t15.2025.01.12 val PER: 0.2787
2026-01-05 18:10:41,416: t15.2025.03.14 val PER: 0.4201
2026-01-05 18:10:41,416: t15.2025.03.16 val PER: 0.2932
2026-01-05 18:10:41,416: t15.2025.03.30 val PER: 0.3943
2026-01-05 18:10:41,417: t15.2025.04.13 val PER: 0.3167
2026-01-05 18:10:41,417: New best val WER(1gram) 66.24% --> 63.45%
2026-01-05 18:10:41,418: Checkpointing model
2026-01-05 18:10:43,045: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:10:43,337: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_4000
2026-01-05 18:11:01,581: Train batch 4200: loss: 22.67 grad norm: 62.69 time: 0.080
2026-01-05 18:11:20,607: Train batch 4400: loss: 16.85 grad norm: 54.13 time: 0.069
2026-01-05 18:11:30,285: Running test after training batch: 4500
2026-01-05 18:11:30,394: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:11:35,377: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the code at this point is will
2026-01-05 18:11:35,409: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap thus cost nett
2026-01-05 18:11:37,209: Val batch 4500: PER (avg): 0.2346 CTC Loss (avg): 23.1063 WER(1gram): 61.17% (n=64) time: 6.923
2026-01-05 18:11:37,209: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-05 18:11:37,210: t15.2023.08.13 val PER: 0.2069
2026-01-05 18:11:37,210: t15.2023.08.18 val PER: 0.1802
2026-01-05 18:11:37,210: t15.2023.08.20 val PER: 0.1890
2026-01-05 18:11:37,210: t15.2023.08.25 val PER: 0.1370
2026-01-05 18:11:37,211: t15.2023.08.27 val PER: 0.2588
2026-01-05 18:11:37,211: t15.2023.09.01 val PER: 0.1510
2026-01-05 18:11:37,211: t15.2023.09.03 val PER: 0.2304
2026-01-05 18:11:37,211: t15.2023.09.24 val PER: 0.1687
2026-01-05 18:11:37,211: t15.2023.09.29 val PER: 0.1863
2026-01-05 18:11:37,211: t15.2023.10.01 val PER: 0.2477
2026-01-05 18:11:37,211: t15.2023.10.06 val PER: 0.1518
2026-01-05 18:11:37,211: t15.2023.10.08 val PER: 0.3153
2026-01-05 18:11:37,211: t15.2023.10.13 val PER: 0.3010
2026-01-05 18:11:37,212: t15.2023.10.15 val PER: 0.2228
2026-01-05 18:11:37,212: t15.2023.10.20 val PER: 0.2114
2026-01-05 18:11:37,212: t15.2023.10.22 val PER: 0.1804
2026-01-05 18:11:37,212: t15.2023.11.03 val PER: 0.2408
2026-01-05 18:11:37,212: t15.2023.11.04 val PER: 0.0580
2026-01-05 18:11:37,212: t15.2023.11.17 val PER: 0.0964
2026-01-05 18:11:37,212: t15.2023.11.19 val PER: 0.0858
2026-01-05 18:11:37,212: t15.2023.11.26 val PER: 0.2572
2026-01-05 18:11:37,212: t15.2023.12.03 val PER: 0.2038
2026-01-05 18:11:37,212: t15.2023.12.08 val PER: 0.2124
2026-01-05 18:11:37,212: t15.2023.12.10 val PER: 0.1708
2026-01-05 18:11:37,213: t15.2023.12.17 val PER: 0.2297
2026-01-05 18:11:37,213: t15.2023.12.29 val PER: 0.2409
2026-01-05 18:11:37,213: t15.2024.02.25 val PER: 0.2022
2026-01-05 18:11:37,213: t15.2024.03.08 val PER: 0.3243
2026-01-05 18:11:37,213: t15.2024.03.15 val PER: 0.2821
2026-01-05 18:11:37,213: t15.2024.03.17 val PER: 0.2350
2026-01-05 18:11:37,213: t15.2024.05.10 val PER: 0.2541
2026-01-05 18:11:37,213: t15.2024.06.14 val PER: 0.2429
2026-01-05 18:11:37,213: t15.2024.07.19 val PER: 0.3289
2026-01-05 18:11:37,213: t15.2024.07.21 val PER: 0.1738
2026-01-05 18:11:37,214: t15.2024.07.28 val PER: 0.2279
2026-01-05 18:11:37,214: t15.2025.01.10 val PER: 0.4118
2026-01-05 18:11:37,214: t15.2025.01.12 val PER: 0.2725
2026-01-05 18:11:37,214: t15.2025.03.14 val PER: 0.3950
2026-01-05 18:11:37,214: t15.2025.03.16 val PER: 0.2880
2026-01-05 18:11:37,214: t15.2025.03.30 val PER: 0.4126
2026-01-05 18:11:37,214: t15.2025.04.13 val PER: 0.2896
2026-01-05 18:11:37,215: New best val WER(1gram) 63.45% --> 61.17%
2026-01-05 18:11:37,215: Checkpointing model
2026-01-05 18:11:38,911: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:11:39,220: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_4500
2026-01-05 18:11:48,867: Train batch 4600: loss: 20.50 grad norm: 62.30 time: 0.063
2026-01-05 18:12:08,051: Train batch 4800: loss: 13.45 grad norm: 49.65 time: 0.065
2026-01-05 18:12:26,581: Train batch 5000: loss: 31.17 grad norm: 79.00 time: 0.065
2026-01-05 18:12:26,581: Running test after training batch: 5000
2026-01-05 18:12:26,719: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:12:31,606: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-05 18:12:31,643: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cussed nit
2026-01-05 18:12:33,540: Val batch 5000: PER (avg): 0.2240 CTC Loss (avg): 21.9609 WER(1gram): 61.42% (n=64) time: 6.959
2026-01-05 18:12:33,541: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-05 18:12:33,542: t15.2023.08.13 val PER: 0.1954
2026-01-05 18:12:33,542: t15.2023.08.18 val PER: 0.1710
2026-01-05 18:12:33,542: t15.2023.08.20 val PER: 0.1676
2026-01-05 18:12:33,542: t15.2023.08.25 val PER: 0.1310
2026-01-05 18:12:33,542: t15.2023.08.27 val PER: 0.2299
2026-01-05 18:12:33,542: t15.2023.09.01 val PER: 0.1364
2026-01-05 18:12:33,543: t15.2023.09.03 val PER: 0.2257
2026-01-05 18:12:33,543: t15.2023.09.24 val PER: 0.1723
2026-01-05 18:12:33,543: t15.2023.09.29 val PER: 0.1819
2026-01-05 18:12:33,543: t15.2023.10.01 val PER: 0.2305
2026-01-05 18:12:33,543: t15.2023.10.06 val PER: 0.1399
2026-01-05 18:12:33,543: t15.2023.10.08 val PER: 0.3099
2026-01-05 18:12:33,543: t15.2023.10.13 val PER: 0.2855
2026-01-05 18:12:33,543: t15.2023.10.15 val PER: 0.2274
2026-01-05 18:12:33,543: t15.2023.10.20 val PER: 0.2282
2026-01-05 18:12:33,543: t15.2023.10.22 val PER: 0.1659
2026-01-05 18:12:33,543: t15.2023.11.03 val PER: 0.2252
2026-01-05 18:12:33,544: t15.2023.11.04 val PER: 0.0478
2026-01-05 18:12:33,544: t15.2023.11.17 val PER: 0.0886
2026-01-05 18:12:33,544: t15.2023.11.19 val PER: 0.0719
2026-01-05 18:12:33,544: t15.2023.11.26 val PER: 0.2283
2026-01-05 18:12:33,544: t15.2023.12.03 val PER: 0.1996
2026-01-05 18:12:33,544: t15.2023.12.08 val PER: 0.1991
2026-01-05 18:12:33,544: t15.2023.12.10 val PER: 0.1603
2026-01-05 18:12:33,544: t15.2023.12.17 val PER: 0.2256
2026-01-05 18:12:33,544: t15.2023.12.29 val PER: 0.2196
2026-01-05 18:12:33,544: t15.2024.02.25 val PER: 0.1868
2026-01-05 18:12:33,544: t15.2024.03.08 val PER: 0.3115
2026-01-05 18:12:33,544: t15.2024.03.15 val PER: 0.2808
2026-01-05 18:12:33,544: t15.2024.03.17 val PER: 0.2329
2026-01-05 18:12:33,545: t15.2024.05.10 val PER: 0.2452
2026-01-05 18:12:33,545: t15.2024.06.14 val PER: 0.2539
2026-01-05 18:12:33,545: t15.2024.07.19 val PER: 0.3303
2026-01-05 18:12:33,545: t15.2024.07.21 val PER: 0.1766
2026-01-05 18:12:33,545: t15.2024.07.28 val PER: 0.2103
2026-01-05 18:12:33,545: t15.2025.01.10 val PER: 0.3829
2026-01-05 18:12:33,545: t15.2025.01.12 val PER: 0.2448
2026-01-05 18:12:33,545: t15.2025.03.14 val PER: 0.3846
2026-01-05 18:12:33,545: t15.2025.03.16 val PER: 0.2670
2026-01-05 18:12:33,545: t15.2025.03.30 val PER: 0.3862
2026-01-05 18:12:33,545: t15.2025.04.13 val PER: 0.3067
2026-01-05 18:12:33,859: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_5000
2026-01-05 18:12:52,414: Train batch 5200: loss: 16.16 grad norm: 64.01 time: 0.052
2026-01-05 18:13:10,921: Train batch 5400: loss: 17.43 grad norm: 59.24 time: 0.069
2026-01-05 18:13:20,362: Running test after training batch: 5500
2026-01-05 18:13:20,474: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:13:25,300: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point will
2026-01-05 18:13:25,330: WER debug example
  GT : how does it keep the cost down
  PR : aue dust it keep the cost tit
2026-01-05 18:13:26,877: Val batch 5500: PER (avg): 0.2158 CTC Loss (avg): 21.1185 WER(1gram): 56.09% (n=64) time: 6.514
2026-01-05 18:13:26,877: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=10
2026-01-05 18:13:26,878: t15.2023.08.13 val PER: 0.1757
2026-01-05 18:13:26,878: t15.2023.08.18 val PER: 0.1576
2026-01-05 18:13:26,878: t15.2023.08.20 val PER: 0.1747
2026-01-05 18:13:26,878: t15.2023.08.25 val PER: 0.1280
2026-01-05 18:13:26,878: t15.2023.08.27 val PER: 0.2331
2026-01-05 18:13:26,878: t15.2023.09.01 val PER: 0.1234
2026-01-05 18:13:26,879: t15.2023.09.03 val PER: 0.2292
2026-01-05 18:13:26,879: t15.2023.09.24 val PER: 0.1784
2026-01-05 18:13:26,879: t15.2023.09.29 val PER: 0.1717
2026-01-05 18:13:26,879: t15.2023.10.01 val PER: 0.2378
2026-01-05 18:13:26,879: t15.2023.10.06 val PER: 0.1378
2026-01-05 18:13:26,879: t15.2023.10.08 val PER: 0.3112
2026-01-05 18:13:26,879: t15.2023.10.13 val PER: 0.2793
2026-01-05 18:13:26,879: t15.2023.10.15 val PER: 0.2149
2026-01-05 18:13:26,879: t15.2023.10.20 val PER: 0.2416
2026-01-05 18:13:26,879: t15.2023.10.22 val PER: 0.1659
2026-01-05 18:13:26,879: t15.2023.11.03 val PER: 0.2157
2026-01-05 18:13:26,879: t15.2023.11.04 val PER: 0.0580
2026-01-05 18:13:26,879: t15.2023.11.17 val PER: 0.0793
2026-01-05 18:13:26,880: t15.2023.11.19 val PER: 0.0599
2026-01-05 18:13:26,880: t15.2023.11.26 val PER: 0.2188
2026-01-05 18:13:26,880: t15.2023.12.03 val PER: 0.1912
2026-01-05 18:13:26,880: t15.2023.12.08 val PER: 0.1877
2026-01-05 18:13:26,880: t15.2023.12.10 val PER: 0.1459
2026-01-05 18:13:26,880: t15.2023.12.17 val PER: 0.2183
2026-01-05 18:13:26,880: t15.2023.12.29 val PER: 0.2066
2026-01-05 18:13:26,880: t15.2024.02.25 val PER: 0.1784
2026-01-05 18:13:26,880: t15.2024.03.08 val PER: 0.2959
2026-01-05 18:13:26,880: t15.2024.03.15 val PER: 0.2627
2026-01-05 18:13:26,880: t15.2024.03.17 val PER: 0.2099
2026-01-05 18:13:26,880: t15.2024.05.10 val PER: 0.2318
2026-01-05 18:13:26,880: t15.2024.06.14 val PER: 0.2397
2026-01-05 18:13:26,880: t15.2024.07.19 val PER: 0.3204
2026-01-05 18:13:26,880: t15.2024.07.21 val PER: 0.1586
2026-01-05 18:13:26,880: t15.2024.07.28 val PER: 0.2118
2026-01-05 18:13:26,880: t15.2025.01.10 val PER: 0.3994
2026-01-05 18:13:26,881: t15.2025.01.12 val PER: 0.2363
2026-01-05 18:13:26,881: t15.2025.03.14 val PER: 0.3669
2026-01-05 18:13:26,881: t15.2025.03.16 val PER: 0.2631
2026-01-05 18:13:26,881: t15.2025.03.30 val PER: 0.3713
2026-01-05 18:13:26,881: t15.2025.04.13 val PER: 0.2853
2026-01-05 18:13:26,882: New best val WER(1gram) 61.17% --> 56.09%
2026-01-05 18:13:26,882: Checkpointing model
2026-01-05 18:13:28,641: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:13:28,943: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_5500
2026-01-05 18:13:38,688: Train batch 5600: loss: 19.39 grad norm: 69.22 time: 0.064
2026-01-05 18:13:59,002: Train batch 5800: loss: 13.71 grad norm: 58.31 time: 0.083
2026-01-05 18:14:18,227: Train batch 6000: loss: 13.98 grad norm: 55.95 time: 0.048
2026-01-05 18:14:18,227: Running test after training batch: 6000
2026-01-05 18:14:18,447: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:14:23,542: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-05 18:14:23,577: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nit
2026-01-05 18:14:25,467: Val batch 6000: PER (avg): 0.2120 CTC Loss (avg): 20.8293 WER(1gram): 57.11% (n=64) time: 7.240
2026-01-05 18:14:25,468: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 18:14:25,468: t15.2023.08.13 val PER: 0.1746
2026-01-05 18:14:25,468: t15.2023.08.18 val PER: 0.1668
2026-01-05 18:14:25,469: t15.2023.08.20 val PER: 0.1628
2026-01-05 18:14:25,469: t15.2023.08.25 val PER: 0.1114
2026-01-05 18:14:25,469: t15.2023.08.27 val PER: 0.2428
2026-01-05 18:14:25,469: t15.2023.09.01 val PER: 0.1323
2026-01-05 18:14:25,469: t15.2023.09.03 val PER: 0.2150
2026-01-05 18:14:25,469: t15.2023.09.24 val PER: 0.1699
2026-01-05 18:14:25,469: t15.2023.09.29 val PER: 0.1749
2026-01-05 18:14:25,469: t15.2023.10.01 val PER: 0.2239
2026-01-05 18:14:25,469: t15.2023.10.06 val PER: 0.1313
2026-01-05 18:14:25,469: t15.2023.10.08 val PER: 0.2991
2026-01-05 18:14:25,469: t15.2023.10.13 val PER: 0.2754
2026-01-05 18:14:25,469: t15.2023.10.15 val PER: 0.2096
2026-01-05 18:14:25,470: t15.2023.10.20 val PER: 0.2148
2026-01-05 18:14:25,470: t15.2023.10.22 val PER: 0.1682
2026-01-05 18:14:25,470: t15.2023.11.03 val PER: 0.2191
2026-01-05 18:14:25,470: t15.2023.11.04 val PER: 0.0546
2026-01-05 18:14:25,470: t15.2023.11.17 val PER: 0.0793
2026-01-05 18:14:25,470: t15.2023.11.19 val PER: 0.0778
2026-01-05 18:14:25,470: t15.2023.11.26 val PER: 0.2283
2026-01-05 18:14:25,470: t15.2023.12.03 val PER: 0.1849
2026-01-05 18:14:25,471: t15.2023.12.08 val PER: 0.1744
2026-01-05 18:14:25,471: t15.2023.12.10 val PER: 0.1459
2026-01-05 18:14:25,471: t15.2023.12.17 val PER: 0.2141
2026-01-05 18:14:25,471: t15.2023.12.29 val PER: 0.2237
2026-01-05 18:14:25,471: t15.2024.02.25 val PER: 0.1629
2026-01-05 18:14:25,471: t15.2024.03.08 val PER: 0.2973
2026-01-05 18:14:25,471: t15.2024.03.15 val PER: 0.2677
2026-01-05 18:14:25,471: t15.2024.03.17 val PER: 0.2120
2026-01-05 18:14:25,471: t15.2024.05.10 val PER: 0.2169
2026-01-05 18:14:25,471: t15.2024.06.14 val PER: 0.2240
2026-01-05 18:14:25,471: t15.2024.07.19 val PER: 0.3065
2026-01-05 18:14:25,471: t15.2024.07.21 val PER: 0.1621
2026-01-05 18:14:25,471: t15.2024.07.28 val PER: 0.1926
2026-01-05 18:14:25,471: t15.2025.01.10 val PER: 0.3788
2026-01-05 18:14:25,471: t15.2025.01.12 val PER: 0.2194
2026-01-05 18:14:25,471: t15.2025.03.14 val PER: 0.3831
2026-01-05 18:14:25,472: t15.2025.03.16 val PER: 0.2552
2026-01-05 18:14:25,472: t15.2025.03.30 val PER: 0.3598
2026-01-05 18:14:25,472: t15.2025.04.13 val PER: 0.2739
2026-01-05 18:14:25,774: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_6000
2026-01-05 18:14:44,492: Train batch 6200: loss: 17.04 grad norm: 64.56 time: 0.070
2026-01-05 18:15:03,173: Train batch 6400: loss: 19.18 grad norm: 66.11 time: 0.063
2026-01-05 18:15:12,339: Running test after training batch: 6500
2026-01-05 18:15:12,485: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:15:17,368: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 18:15:17,401: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 18:15:19,191: Val batch 6500: PER (avg): 0.2048 CTC Loss (avg): 20.1140 WER(1gram): 53.55% (n=64) time: 6.852
2026-01-05 18:15:19,192: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 18:15:19,192: t15.2023.08.13 val PER: 0.1705
2026-01-05 18:15:19,192: t15.2023.08.18 val PER: 0.1400
2026-01-05 18:15:19,192: t15.2023.08.20 val PER: 0.1597
2026-01-05 18:15:19,192: t15.2023.08.25 val PER: 0.1130
2026-01-05 18:15:19,192: t15.2023.08.27 val PER: 0.2347
2026-01-05 18:15:19,192: t15.2023.09.01 val PER: 0.1071
2026-01-05 18:15:19,193: t15.2023.09.03 val PER: 0.2138
2026-01-05 18:15:19,193: t15.2023.09.24 val PER: 0.1663
2026-01-05 18:15:19,193: t15.2023.09.29 val PER: 0.1698
2026-01-05 18:15:19,193: t15.2023.10.01 val PER: 0.2266
2026-01-05 18:15:19,193: t15.2023.10.06 val PER: 0.1346
2026-01-05 18:15:19,193: t15.2023.10.08 val PER: 0.2909
2026-01-05 18:15:19,193: t15.2023.10.13 val PER: 0.2777
2026-01-05 18:15:19,193: t15.2023.10.15 val PER: 0.2136
2026-01-05 18:15:19,193: t15.2023.10.20 val PER: 0.1980
2026-01-05 18:15:19,193: t15.2023.10.22 val PER: 0.1648
2026-01-05 18:15:19,193: t15.2023.11.03 val PER: 0.2259
2026-01-05 18:15:19,193: t15.2023.11.04 val PER: 0.0546
2026-01-05 18:15:19,193: t15.2023.11.17 val PER: 0.0715
2026-01-05 18:15:19,194: t15.2023.11.19 val PER: 0.0739
2026-01-05 18:15:19,194: t15.2023.11.26 val PER: 0.2123
2026-01-05 18:15:19,194: t15.2023.12.03 val PER: 0.1681
2026-01-05 18:15:19,194: t15.2023.12.08 val PER: 0.1678
2026-01-05 18:15:19,194: t15.2023.12.10 val PER: 0.1380
2026-01-05 18:15:19,194: t15.2023.12.17 val PER: 0.1902
2026-01-05 18:15:19,194: t15.2023.12.29 val PER: 0.2038
2026-01-05 18:15:19,194: t15.2024.02.25 val PER: 0.1657
2026-01-05 18:15:19,194: t15.2024.03.08 val PER: 0.2945
2026-01-05 18:15:19,194: t15.2024.03.15 val PER: 0.2602
2026-01-05 18:15:19,194: t15.2024.03.17 val PER: 0.2113
2026-01-05 18:15:19,194: t15.2024.05.10 val PER: 0.2140
2026-01-05 18:15:19,194: t15.2024.06.14 val PER: 0.2129
2026-01-05 18:15:19,195: t15.2024.07.19 val PER: 0.2973
2026-01-05 18:15:19,195: t15.2024.07.21 val PER: 0.1462
2026-01-05 18:15:19,195: t15.2024.07.28 val PER: 0.1912
2026-01-05 18:15:19,195: t15.2025.01.10 val PER: 0.3664
2026-01-05 18:15:19,195: t15.2025.01.12 val PER: 0.2163
2026-01-05 18:15:19,195: t15.2025.03.14 val PER: 0.3669
2026-01-05 18:15:19,195: t15.2025.03.16 val PER: 0.2421
2026-01-05 18:15:19,195: t15.2025.03.30 val PER: 0.3391
2026-01-05 18:15:19,195: t15.2025.04.13 val PER: 0.2782
2026-01-05 18:15:19,196: New best val WER(1gram) 56.09% --> 53.55%
2026-01-05 18:15:19,196: Checkpointing model
2026-01-05 18:15:20,890: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:15:21,252: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_6500
2026-01-05 18:15:30,727: Train batch 6600: loss: 12.42 grad norm: 54.48 time: 0.046
2026-01-05 18:15:49,891: Train batch 6800: loss: 15.56 grad norm: 56.16 time: 0.049
2026-01-05 18:16:08,765: Train batch 7000: loss: 17.09 grad norm: 65.70 time: 0.061
2026-01-05 18:16:08,765: Running test after training batch: 7000
2026-01-05 18:16:08,871: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:16:13,654: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:16:13,684: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost nett
2026-01-05 18:16:15,325: Val batch 7000: PER (avg): 0.1952 CTC Loss (avg): 19.3773 WER(1gram): 53.81% (n=64) time: 6.559
2026-01-05 18:16:15,325: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 18:16:15,326: t15.2023.08.13 val PER: 0.1570
2026-01-05 18:16:15,326: t15.2023.08.18 val PER: 0.1383
2026-01-05 18:16:15,326: t15.2023.08.20 val PER: 0.1390
2026-01-05 18:16:15,326: t15.2023.08.25 val PER: 0.0979
2026-01-05 18:16:15,326: t15.2023.08.27 val PER: 0.2074
2026-01-05 18:16:15,326: t15.2023.09.01 val PER: 0.1120
2026-01-05 18:16:15,326: t15.2023.09.03 val PER: 0.1805
2026-01-05 18:16:15,326: t15.2023.09.24 val PER: 0.1650
2026-01-05 18:16:15,326: t15.2023.09.29 val PER: 0.1685
2026-01-05 18:16:15,326: t15.2023.10.01 val PER: 0.2127
2026-01-05 18:16:15,327: t15.2023.10.06 val PER: 0.1130
2026-01-05 18:16:15,327: t15.2023.10.08 val PER: 0.2842
2026-01-05 18:16:15,327: t15.2023.10.13 val PER: 0.2599
2026-01-05 18:16:15,327: t15.2023.10.15 val PER: 0.1918
2026-01-05 18:16:15,327: t15.2023.10.20 val PER: 0.1946
2026-01-05 18:16:15,327: t15.2023.10.22 val PER: 0.1437
2026-01-05 18:16:15,327: t15.2023.11.03 val PER: 0.2130
2026-01-05 18:16:15,327: t15.2023.11.04 val PER: 0.0410
2026-01-05 18:16:15,327: t15.2023.11.17 val PER: 0.0607
2026-01-05 18:16:15,327: t15.2023.11.19 val PER: 0.0499
2026-01-05 18:16:15,327: t15.2023.11.26 val PER: 0.2000
2026-01-05 18:16:15,327: t15.2023.12.03 val PER: 0.1649
2026-01-05 18:16:15,327: t15.2023.12.08 val PER: 0.1598
2026-01-05 18:16:15,328: t15.2023.12.10 val PER: 0.1393
2026-01-05 18:16:15,328: t15.2023.12.17 val PER: 0.1830
2026-01-05 18:16:15,328: t15.2023.12.29 val PER: 0.2018
2026-01-05 18:16:15,328: t15.2024.02.25 val PER: 0.1657
2026-01-05 18:16:15,328: t15.2024.03.08 val PER: 0.2817
2026-01-05 18:16:15,328: t15.2024.03.15 val PER: 0.2458
2026-01-05 18:16:15,328: t15.2024.03.17 val PER: 0.2036
2026-01-05 18:16:15,328: t15.2024.05.10 val PER: 0.2021
2026-01-05 18:16:15,328: t15.2024.06.14 val PER: 0.2114
2026-01-05 18:16:15,328: t15.2024.07.19 val PER: 0.3006
2026-01-05 18:16:15,328: t15.2024.07.21 val PER: 0.1400
2026-01-05 18:16:15,328: t15.2024.07.28 val PER: 0.1728
2026-01-05 18:16:15,328: t15.2025.01.10 val PER: 0.3650
2026-01-05 18:16:15,328: t15.2025.01.12 val PER: 0.1986
2026-01-05 18:16:15,328: t15.2025.03.14 val PER: 0.3565
2026-01-05 18:16:15,329: t15.2025.03.16 val PER: 0.2435
2026-01-05 18:16:15,329: t15.2025.03.30 val PER: 0.3644
2026-01-05 18:16:15,329: t15.2025.04.13 val PER: 0.2639
2026-01-05 18:16:15,598: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_7000
2026-01-05 18:16:34,165: Train batch 7200: loss: 14.04 grad norm: 62.16 time: 0.079
2026-01-05 18:16:52,540: Train batch 7400: loss: 13.64 grad norm: 53.47 time: 0.075
2026-01-05 18:17:01,631: Running test after training batch: 7500
2026-01-05 18:17:01,831: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:17:06,691: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-05 18:17:06,725: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-05 18:17:08,622: Val batch 7500: PER (avg): 0.1897 CTC Loss (avg): 18.6308 WER(1gram): 54.82% (n=64) time: 6.990
2026-01-05 18:17:08,622: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-05 18:17:08,622: t15.2023.08.13 val PER: 0.1486
2026-01-05 18:17:08,623: t15.2023.08.18 val PER: 0.1425
2026-01-05 18:17:08,623: t15.2023.08.20 val PER: 0.1438
2026-01-05 18:17:08,623: t15.2023.08.25 val PER: 0.1099
2026-01-05 18:17:08,623: t15.2023.08.27 val PER: 0.2138
2026-01-05 18:17:08,623: t15.2023.09.01 val PER: 0.1161
2026-01-05 18:17:08,623: t15.2023.09.03 val PER: 0.1936
2026-01-05 18:17:08,623: t15.2023.09.24 val PER: 0.1566
2026-01-05 18:17:08,623: t15.2023.09.29 val PER: 0.1583
2026-01-05 18:17:08,623: t15.2023.10.01 val PER: 0.2094
2026-01-05 18:17:08,623: t15.2023.10.06 val PER: 0.1173
2026-01-05 18:17:08,623: t15.2023.10.08 val PER: 0.2612
2026-01-05 18:17:08,623: t15.2023.10.13 val PER: 0.2459
2026-01-05 18:17:08,624: t15.2023.10.15 val PER: 0.1945
2026-01-05 18:17:08,624: t15.2023.10.20 val PER: 0.1879
2026-01-05 18:17:08,624: t15.2023.10.22 val PER: 0.1392
2026-01-05 18:17:08,624: t15.2023.11.03 val PER: 0.2069
2026-01-05 18:17:08,624: t15.2023.11.04 val PER: 0.0512
2026-01-05 18:17:08,624: t15.2023.11.17 val PER: 0.0622
2026-01-05 18:17:08,624: t15.2023.11.19 val PER: 0.0559
2026-01-05 18:17:08,624: t15.2023.11.26 val PER: 0.1899
2026-01-05 18:17:08,624: t15.2023.12.03 val PER: 0.1513
2026-01-05 18:17:08,624: t15.2023.12.08 val PER: 0.1518
2026-01-05 18:17:08,624: t15.2023.12.10 val PER: 0.1314
2026-01-05 18:17:08,624: t15.2023.12.17 val PER: 0.1767
2026-01-05 18:17:08,624: t15.2023.12.29 val PER: 0.1881
2026-01-05 18:17:08,624: t15.2024.02.25 val PER: 0.1461
2026-01-05 18:17:08,625: t15.2024.03.08 val PER: 0.2745
2026-01-05 18:17:08,625: t15.2024.03.15 val PER: 0.2495
2026-01-05 18:17:08,625: t15.2024.03.17 val PER: 0.1841
2026-01-05 18:17:08,625: t15.2024.05.10 val PER: 0.2155
2026-01-05 18:17:08,625: t15.2024.06.14 val PER: 0.1909
2026-01-05 18:17:08,625: t15.2024.07.19 val PER: 0.2828
2026-01-05 18:17:08,626: t15.2024.07.21 val PER: 0.1331
2026-01-05 18:17:08,626: t15.2024.07.28 val PER: 0.1647
2026-01-05 18:17:08,626: t15.2025.01.10 val PER: 0.3609
2026-01-05 18:17:08,626: t15.2025.01.12 val PER: 0.1925
2026-01-05 18:17:08,626: t15.2025.03.14 val PER: 0.3698
2026-01-05 18:17:08,626: t15.2025.03.16 val PER: 0.2408
2026-01-05 18:17:08,626: t15.2025.03.30 val PER: 0.3448
2026-01-05 18:17:08,626: t15.2025.04.13 val PER: 0.2482
2026-01-05 18:17:08,912: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_7500
2026-01-05 18:17:18,524: Train batch 7600: loss: 16.78 grad norm: 60.82 time: 0.069
2026-01-05 18:17:38,100: Train batch 7800: loss: 14.08 grad norm: 56.50 time: 0.057
2026-01-05 18:17:58,660: Train batch 8000: loss: 11.46 grad norm: 55.18 time: 0.075
2026-01-05 18:17:58,661: Running test after training batch: 8000
2026-01-05 18:17:58,791: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:18:03,569: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-05 18:18:03,599: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-05 18:18:05,285: Val batch 8000: PER (avg): 0.1852 CTC Loss (avg): 18.2154 WER(1gram): 57.11% (n=64) time: 6.624
2026-01-05 18:18:05,286: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-05 18:18:05,286: t15.2023.08.13 val PER: 0.1466
2026-01-05 18:18:05,286: t15.2023.08.18 val PER: 0.1324
2026-01-05 18:18:05,287: t15.2023.08.20 val PER: 0.1422
2026-01-05 18:18:05,287: t15.2023.08.25 val PER: 0.1024
2026-01-05 18:18:05,287: t15.2023.08.27 val PER: 0.2154
2026-01-05 18:18:05,287: t15.2023.09.01 val PER: 0.1039
2026-01-05 18:18:05,287: t15.2023.09.03 val PER: 0.1936
2026-01-05 18:18:05,287: t15.2023.09.24 val PER: 0.1553
2026-01-05 18:18:05,287: t15.2023.09.29 val PER: 0.1461
2026-01-05 18:18:05,287: t15.2023.10.01 val PER: 0.1995
2026-01-05 18:18:05,287: t15.2023.10.06 val PER: 0.1119
2026-01-05 18:18:05,287: t15.2023.10.08 val PER: 0.2666
2026-01-05 18:18:05,287: t15.2023.10.13 val PER: 0.2397
2026-01-05 18:18:05,287: t15.2023.10.15 val PER: 0.1951
2026-01-05 18:18:05,287: t15.2023.10.20 val PER: 0.2013
2026-01-05 18:18:05,287: t15.2023.10.22 val PER: 0.1381
2026-01-05 18:18:05,288: t15.2023.11.03 val PER: 0.2090
2026-01-05 18:18:05,288: t15.2023.11.04 val PER: 0.0375
2026-01-05 18:18:05,288: t15.2023.11.17 val PER: 0.0591
2026-01-05 18:18:05,288: t15.2023.11.19 val PER: 0.0539
2026-01-05 18:18:05,288: t15.2023.11.26 val PER: 0.1812
2026-01-05 18:18:05,288: t15.2023.12.03 val PER: 0.1607
2026-01-05 18:18:05,288: t15.2023.12.08 val PER: 0.1458
2026-01-05 18:18:05,288: t15.2023.12.10 val PER: 0.1367
2026-01-05 18:18:05,288: t15.2023.12.17 val PER: 0.1757
2026-01-05 18:18:05,288: t15.2023.12.29 val PER: 0.1764
2026-01-05 18:18:05,288: t15.2024.02.25 val PER: 0.1447
2026-01-05 18:18:05,288: t15.2024.03.08 val PER: 0.2703
2026-01-05 18:18:05,288: t15.2024.03.15 val PER: 0.2408
2026-01-05 18:18:05,288: t15.2024.03.17 val PER: 0.1799
2026-01-05 18:18:05,289: t15.2024.05.10 val PER: 0.1976
2026-01-05 18:18:05,289: t15.2024.06.14 val PER: 0.2066
2026-01-05 18:18:05,289: t15.2024.07.19 val PER: 0.2933
2026-01-05 18:18:05,289: t15.2024.07.21 val PER: 0.1172
2026-01-05 18:18:05,289: t15.2024.07.28 val PER: 0.1632
2026-01-05 18:18:05,289: t15.2025.01.10 val PER: 0.3209
2026-01-05 18:18:05,289: t15.2025.01.12 val PER: 0.1855
2026-01-05 18:18:05,289: t15.2025.03.14 val PER: 0.3639
2026-01-05 18:18:05,289: t15.2025.03.16 val PER: 0.2238
2026-01-05 18:18:05,289: t15.2025.03.30 val PER: 0.3425
2026-01-05 18:18:05,289: t15.2025.04.13 val PER: 0.2596
2026-01-05 18:18:05,602: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_8000
2026-01-05 18:18:24,950: Train batch 8200: loss: 9.42 grad norm: 47.63 time: 0.054
2026-01-05 18:18:43,673: Train batch 8400: loss: 10.33 grad norm: 48.11 time: 0.064
2026-01-05 18:18:53,104: Running test after training batch: 8500
2026-01-05 18:18:53,221: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:18:58,084: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 18:18:58,118: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-05 18:19:00,069: Val batch 8500: PER (avg): 0.1785 CTC Loss (avg): 17.7351 WER(1gram): 50.00% (n=64) time: 6.964
2026-01-05 18:19:00,069: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 18:19:00,070: t15.2023.08.13 val PER: 0.1331
2026-01-05 18:19:00,070: t15.2023.08.18 val PER: 0.1291
2026-01-05 18:19:00,070: t15.2023.08.20 val PER: 0.1319
2026-01-05 18:19:00,070: t15.2023.08.25 val PER: 0.1039
2026-01-05 18:19:00,070: t15.2023.08.27 val PER: 0.2058
2026-01-05 18:19:00,070: t15.2023.09.01 val PER: 0.0974
2026-01-05 18:19:00,070: t15.2023.09.03 val PER: 0.1948
2026-01-05 18:19:00,071: t15.2023.09.24 val PER: 0.1432
2026-01-05 18:19:00,071: t15.2023.09.29 val PER: 0.1417
2026-01-05 18:19:00,071: t15.2023.10.01 val PER: 0.1962
2026-01-05 18:19:00,071: t15.2023.10.06 val PER: 0.1001
2026-01-05 18:19:00,071: t15.2023.10.08 val PER: 0.2585
2026-01-05 18:19:00,071: t15.2023.10.13 val PER: 0.2374
2026-01-05 18:19:00,072: t15.2023.10.15 val PER: 0.1931
2026-01-05 18:19:00,072: t15.2023.10.20 val PER: 0.1879
2026-01-05 18:19:00,072: t15.2023.10.22 val PER: 0.1336
2026-01-05 18:19:00,072: t15.2023.11.03 val PER: 0.2001
2026-01-05 18:19:00,072: t15.2023.11.04 val PER: 0.0375
2026-01-05 18:19:00,072: t15.2023.11.17 val PER: 0.0544
2026-01-05 18:19:00,072: t15.2023.11.19 val PER: 0.0519
2026-01-05 18:19:00,072: t15.2023.11.26 val PER: 0.1783
2026-01-05 18:19:00,073: t15.2023.12.03 val PER: 0.1429
2026-01-05 18:19:00,073: t15.2023.12.08 val PER: 0.1438
2026-01-05 18:19:00,073: t15.2023.12.10 val PER: 0.1248
2026-01-05 18:19:00,073: t15.2023.12.17 val PER: 0.1684
2026-01-05 18:19:00,073: t15.2023.12.29 val PER: 0.1709
2026-01-05 18:19:00,073: t15.2024.02.25 val PER: 0.1376
2026-01-05 18:19:00,073: t15.2024.03.08 val PER: 0.2660
2026-01-05 18:19:00,073: t15.2024.03.15 val PER: 0.2320
2026-01-05 18:19:00,073: t15.2024.03.17 val PER: 0.1681
2026-01-05 18:19:00,073: t15.2024.05.10 val PER: 0.1947
2026-01-05 18:19:00,074: t15.2024.06.14 val PER: 0.1845
2026-01-05 18:19:00,074: t15.2024.07.19 val PER: 0.2795
2026-01-05 18:19:00,074: t15.2024.07.21 val PER: 0.1200
2026-01-05 18:19:00,074: t15.2024.07.28 val PER: 0.1699
2026-01-05 18:19:00,075: t15.2025.01.10 val PER: 0.3196
2026-01-05 18:19:00,075: t15.2025.01.12 val PER: 0.1863
2026-01-05 18:19:00,075: t15.2025.03.14 val PER: 0.3447
2026-01-05 18:19:00,076: t15.2025.03.16 val PER: 0.2055
2026-01-05 18:19:00,076: t15.2025.03.30 val PER: 0.3264
2026-01-05 18:19:00,076: t15.2025.04.13 val PER: 0.2439
2026-01-05 18:19:00,076: New best val WER(1gram) 53.55% --> 50.00%
2026-01-05 18:19:00,076: Checkpointing model
2026-01-05 18:19:01,754: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:19:02,051: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_8500
2026-01-05 18:19:11,297: Train batch 8600: loss: 16.12 grad norm: 63.50 time: 0.055
2026-01-05 18:19:29,410: Train batch 8800: loss: 15.14 grad norm: 60.33 time: 0.061
2026-01-05 18:19:48,102: Train batch 9000: loss: 16.18 grad norm: 63.83 time: 0.073
2026-01-05 18:19:48,103: Running test after training batch: 9000
2026-01-05 18:19:48,242: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:19:52,952: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 18:19:52,983: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nit
2026-01-05 18:19:54,694: Val batch 9000: PER (avg): 0.1719 CTC Loss (avg): 17.2458 WER(1gram): 51.27% (n=64) time: 6.591
2026-01-05 18:19:54,694: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 18:19:54,695: t15.2023.08.13 val PER: 0.1279
2026-01-05 18:19:54,695: t15.2023.08.18 val PER: 0.1299
2026-01-05 18:19:54,695: t15.2023.08.20 val PER: 0.1176
2026-01-05 18:19:54,695: t15.2023.08.25 val PER: 0.0949
2026-01-05 18:19:54,695: t15.2023.08.27 val PER: 0.1977
2026-01-05 18:19:54,695: t15.2023.09.01 val PER: 0.0852
2026-01-05 18:19:54,695: t15.2023.09.03 val PER: 0.1770
2026-01-05 18:19:54,695: t15.2023.09.24 val PER: 0.1444
2026-01-05 18:19:54,695: t15.2023.09.29 val PER: 0.1442
2026-01-05 18:19:54,695: t15.2023.10.01 val PER: 0.1968
2026-01-05 18:19:54,695: t15.2023.10.06 val PER: 0.0958
2026-01-05 18:19:54,695: t15.2023.10.08 val PER: 0.2490
2026-01-05 18:19:54,695: t15.2023.10.13 val PER: 0.2343
2026-01-05 18:19:54,695: t15.2023.10.15 val PER: 0.1806
2026-01-05 18:19:54,696: t15.2023.10.20 val PER: 0.1846
2026-01-05 18:19:54,696: t15.2023.10.22 val PER: 0.1292
2026-01-05 18:19:54,696: t15.2023.11.03 val PER: 0.1906
2026-01-05 18:19:54,696: t15.2023.11.04 val PER: 0.0410
2026-01-05 18:19:54,696: t15.2023.11.17 val PER: 0.0544
2026-01-05 18:19:54,696: t15.2023.11.19 val PER: 0.0539
2026-01-05 18:19:54,696: t15.2023.11.26 val PER: 0.1761
2026-01-05 18:19:54,696: t15.2023.12.03 val PER: 0.1387
2026-01-05 18:19:54,696: t15.2023.12.08 val PER: 0.1338
2026-01-05 18:19:54,696: t15.2023.12.10 val PER: 0.1130
2026-01-05 18:19:54,696: t15.2023.12.17 val PER: 0.1622
2026-01-05 18:19:54,697: t15.2023.12.29 val PER: 0.1640
2026-01-05 18:19:54,697: t15.2024.02.25 val PER: 0.1320
2026-01-05 18:19:54,697: t15.2024.03.08 val PER: 0.2532
2026-01-05 18:19:54,697: t15.2024.03.15 val PER: 0.2189
2026-01-05 18:19:54,697: t15.2024.03.17 val PER: 0.1750
2026-01-05 18:19:54,697: t15.2024.05.10 val PER: 0.1932
2026-01-05 18:19:54,697: t15.2024.06.14 val PER: 0.1814
2026-01-05 18:19:54,697: t15.2024.07.19 val PER: 0.2657
2026-01-05 18:19:54,697: t15.2024.07.21 val PER: 0.1159
2026-01-05 18:19:54,697: t15.2024.07.28 val PER: 0.1529
2026-01-05 18:19:54,697: t15.2025.01.10 val PER: 0.3030
2026-01-05 18:19:54,697: t15.2025.01.12 val PER: 0.1701
2026-01-05 18:19:54,698: t15.2025.03.14 val PER: 0.3521
2026-01-05 18:19:54,698: t15.2025.03.16 val PER: 0.2081
2026-01-05 18:19:54,698: t15.2025.03.30 val PER: 0.3138
2026-01-05 18:19:54,698: t15.2025.04.13 val PER: 0.2382
2026-01-05 18:19:54,984: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_9000
2026-01-05 18:20:25,884: Train batch 9200: loss: 10.79 grad norm: 49.87 time: 0.057
2026-01-05 18:20:43,243: Train batch 9400: loss: 7.71 grad norm: 46.24 time: 0.069
2026-01-05 18:20:52,040: Running test after training batch: 9500
2026-01-05 18:20:52,200: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:20:57,163: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:20:57,197: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-05 18:20:59,298: Val batch 9500: PER (avg): 0.1731 CTC Loss (avg): 17.1496 WER(1gram): 49.75% (n=64) time: 7.257
2026-01-05 18:20:59,298: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 18:20:59,299: t15.2023.08.13 val PER: 0.1310
2026-01-05 18:20:59,299: t15.2023.08.18 val PER: 0.1207
2026-01-05 18:20:59,299: t15.2023.08.20 val PER: 0.1287
2026-01-05 18:20:59,299: t15.2023.08.25 val PER: 0.1039
2026-01-05 18:20:59,299: t15.2023.08.27 val PER: 0.2026
2026-01-05 18:20:59,300: t15.2023.09.01 val PER: 0.0909
2026-01-05 18:20:59,300: t15.2023.09.03 val PER: 0.1746
2026-01-05 18:20:59,300: t15.2023.09.24 val PER: 0.1444
2026-01-05 18:20:59,300: t15.2023.09.29 val PER: 0.1487
2026-01-05 18:20:59,300: t15.2023.10.01 val PER: 0.1909
2026-01-05 18:20:59,300: t15.2023.10.06 val PER: 0.0958
2026-01-05 18:20:59,300: t15.2023.10.08 val PER: 0.2625
2026-01-05 18:20:59,300: t15.2023.10.13 val PER: 0.2281
2026-01-05 18:20:59,300: t15.2023.10.15 val PER: 0.1866
2026-01-05 18:20:59,300: t15.2023.10.20 val PER: 0.1779
2026-01-05 18:20:59,301: t15.2023.10.22 val PER: 0.1281
2026-01-05 18:20:59,301: t15.2023.11.03 val PER: 0.1981
2026-01-05 18:20:59,301: t15.2023.11.04 val PER: 0.0307
2026-01-05 18:20:59,301: t15.2023.11.17 val PER: 0.0467
2026-01-05 18:20:59,301: t15.2023.11.19 val PER: 0.0559
2026-01-05 18:20:59,301: t15.2023.11.26 val PER: 0.1667
2026-01-05 18:20:59,301: t15.2023.12.03 val PER: 0.1450
2026-01-05 18:20:59,301: t15.2023.12.08 val PER: 0.1325
2026-01-05 18:20:59,301: t15.2023.12.10 val PER: 0.1183
2026-01-05 18:20:59,301: t15.2023.12.17 val PER: 0.1674
2026-01-05 18:20:59,301: t15.2023.12.29 val PER: 0.1476
2026-01-05 18:20:59,301: t15.2024.02.25 val PER: 0.1362
2026-01-05 18:20:59,301: t15.2024.03.08 val PER: 0.2617
2026-01-05 18:20:59,301: t15.2024.03.15 val PER: 0.2195
2026-01-05 18:20:59,301: t15.2024.03.17 val PER: 0.1639
2026-01-05 18:20:59,301: t15.2024.05.10 val PER: 0.1947
2026-01-05 18:20:59,301: t15.2024.06.14 val PER: 0.1861
2026-01-05 18:20:59,302: t15.2024.07.19 val PER: 0.2709
2026-01-05 18:20:59,302: t15.2024.07.21 val PER: 0.1172
2026-01-05 18:20:59,302: t15.2024.07.28 val PER: 0.1588
2026-01-05 18:20:59,302: t15.2025.01.10 val PER: 0.3251
2026-01-05 18:20:59,302: t15.2025.01.12 val PER: 0.1824
2026-01-05 18:20:59,302: t15.2025.03.14 val PER: 0.3639
2026-01-05 18:20:59,302: t15.2025.03.16 val PER: 0.2003
2026-01-05 18:20:59,302: t15.2025.03.30 val PER: 0.3092
2026-01-05 18:20:59,302: t15.2025.04.13 val PER: 0.2368
2026-01-05 18:20:59,303: New best val WER(1gram) 50.00% --> 49.75%
2026-01-05 18:20:59,303: Checkpointing model
2026-01-05 18:21:00,929: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:21:01,284: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_9500
2026-01-05 18:21:09,843: Train batch 9600: loss: 8.11 grad norm: 43.91 time: 0.075
2026-01-05 18:21:27,435: Train batch 9800: loss: 12.22 grad norm: 58.49 time: 0.063
2026-01-05 18:21:46,487: Train batch 10000: loss: 5.27 grad norm: 36.68 time: 0.061
2026-01-05 18:21:46,488: Running test after training batch: 10000
2026-01-05 18:21:46,626: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:21:51,555: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 18:21:51,591: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sit
2026-01-05 18:21:53,807: Val batch 10000: PER (avg): 0.1685 CTC Loss (avg): 16.8954 WER(1gram): 52.03% (n=64) time: 7.319
2026-01-05 18:21:53,808: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 18:21:53,808: t15.2023.08.13 val PER: 0.1258
2026-01-05 18:21:53,809: t15.2023.08.18 val PER: 0.1224
2026-01-05 18:21:53,809: t15.2023.08.20 val PER: 0.1215
2026-01-05 18:21:53,809: t15.2023.08.25 val PER: 0.1009
2026-01-05 18:21:53,809: t15.2023.08.27 val PER: 0.2058
2026-01-05 18:21:53,809: t15.2023.09.01 val PER: 0.0901
2026-01-05 18:21:53,809: t15.2023.09.03 val PER: 0.1793
2026-01-05 18:21:53,809: t15.2023.09.24 val PER: 0.1468
2026-01-05 18:21:53,809: t15.2023.09.29 val PER: 0.1423
2026-01-05 18:21:53,809: t15.2023.10.01 val PER: 0.1869
2026-01-05 18:21:53,809: t15.2023.10.06 val PER: 0.1044
2026-01-05 18:21:53,809: t15.2023.10.08 val PER: 0.2395
2026-01-05 18:21:53,809: t15.2023.10.13 val PER: 0.2265
2026-01-05 18:21:53,810: t15.2023.10.15 val PER: 0.1681
2026-01-05 18:21:53,810: t15.2023.10.20 val PER: 0.1846
2026-01-05 18:21:53,810: t15.2023.10.22 val PER: 0.1325
2026-01-05 18:21:53,810: t15.2023.11.03 val PER: 0.1961
2026-01-05 18:21:53,810: t15.2023.11.04 val PER: 0.0307
2026-01-05 18:21:53,810: t15.2023.11.17 val PER: 0.0467
2026-01-05 18:21:53,810: t15.2023.11.19 val PER: 0.0399
2026-01-05 18:21:53,810: t15.2023.11.26 val PER: 0.1551
2026-01-05 18:21:53,810: t15.2023.12.03 val PER: 0.1345
2026-01-05 18:21:53,810: t15.2023.12.08 val PER: 0.1332
2026-01-05 18:21:53,810: t15.2023.12.10 val PER: 0.1104
2026-01-05 18:21:53,811: t15.2023.12.17 val PER: 0.1538
2026-01-05 18:21:53,811: t15.2023.12.29 val PER: 0.1510
2026-01-05 18:21:53,811: t15.2024.02.25 val PER: 0.1461
2026-01-05 18:21:53,811: t15.2024.03.08 val PER: 0.2475
2026-01-05 18:21:53,811: t15.2024.03.15 val PER: 0.2201
2026-01-05 18:21:53,811: t15.2024.03.17 val PER: 0.1604
2026-01-05 18:21:53,811: t15.2024.05.10 val PER: 0.1813
2026-01-05 18:21:53,811: t15.2024.06.14 val PER: 0.1861
2026-01-05 18:21:53,811: t15.2024.07.19 val PER: 0.2577
2026-01-05 18:21:53,811: t15.2024.07.21 val PER: 0.1103
2026-01-05 18:21:53,811: t15.2024.07.28 val PER: 0.1537
2026-01-05 18:21:53,811: t15.2025.01.10 val PER: 0.2975
2026-01-05 18:21:53,811: t15.2025.01.12 val PER: 0.1717
2026-01-05 18:21:53,811: t15.2025.03.14 val PER: 0.3447
2026-01-05 18:21:53,811: t15.2025.03.16 val PER: 0.2055
2026-01-05 18:21:53,812: t15.2025.03.30 val PER: 0.3276
2026-01-05 18:21:53,812: t15.2025.04.13 val PER: 0.2282
2026-01-05 18:21:54,106: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_10000
2026-01-05 18:22:12,898: Train batch 10200: loss: 6.45 grad norm: 40.89 time: 0.053
2026-01-05 18:22:32,134: Train batch 10400: loss: 9.22 grad norm: 63.27 time: 0.074
2026-01-05 18:22:41,761: Running test after training batch: 10500
2026-01-05 18:22:41,902: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:22:46,744: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:22:46,784: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-05 18:22:48,884: Val batch 10500: PER (avg): 0.1664 CTC Loss (avg): 16.6572 WER(1gram): 50.00% (n=64) time: 7.123
2026-01-05 18:22:48,885: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 18:22:48,885: t15.2023.08.13 val PER: 0.1185
2026-01-05 18:22:48,885: t15.2023.08.18 val PER: 0.1241
2026-01-05 18:22:48,885: t15.2023.08.20 val PER: 0.1247
2026-01-05 18:22:48,885: t15.2023.08.25 val PER: 0.1069
2026-01-05 18:22:48,886: t15.2023.08.27 val PER: 0.2090
2026-01-05 18:22:48,886: t15.2023.09.01 val PER: 0.0893
2026-01-05 18:22:48,886: t15.2023.09.03 val PER: 0.1770
2026-01-05 18:22:48,886: t15.2023.09.24 val PER: 0.1541
2026-01-05 18:22:48,886: t15.2023.09.29 val PER: 0.1500
2026-01-05 18:22:48,886: t15.2023.10.01 val PER: 0.1922
2026-01-05 18:22:48,886: t15.2023.10.06 val PER: 0.0958
2026-01-05 18:22:48,886: t15.2023.10.08 val PER: 0.2530
2026-01-05 18:22:48,886: t15.2023.10.13 val PER: 0.2234
2026-01-05 18:22:48,886: t15.2023.10.15 val PER: 0.1806
2026-01-05 18:22:48,886: t15.2023.10.20 val PER: 0.1913
2026-01-05 18:22:48,887: t15.2023.10.22 val PER: 0.1269
2026-01-05 18:22:48,887: t15.2023.11.03 val PER: 0.1866
2026-01-05 18:22:48,887: t15.2023.11.04 val PER: 0.0444
2026-01-05 18:22:48,887: t15.2023.11.17 val PER: 0.0451
2026-01-05 18:22:48,887: t15.2023.11.19 val PER: 0.0559
2026-01-05 18:22:48,887: t15.2023.11.26 val PER: 0.1348
2026-01-05 18:22:48,887: t15.2023.12.03 val PER: 0.1334
2026-01-05 18:22:48,887: t15.2023.12.08 val PER: 0.1218
2026-01-05 18:22:48,887: t15.2023.12.10 val PER: 0.1104
2026-01-05 18:22:48,887: t15.2023.12.17 val PER: 0.1507
2026-01-05 18:22:48,887: t15.2023.12.29 val PER: 0.1585
2026-01-05 18:22:48,887: t15.2024.02.25 val PER: 0.1250
2026-01-05 18:22:48,887: t15.2024.03.08 val PER: 0.2504
2026-01-05 18:22:48,887: t15.2024.03.15 val PER: 0.2164
2026-01-05 18:22:48,888: t15.2024.03.17 val PER: 0.1520
2026-01-05 18:22:48,888: t15.2024.05.10 val PER: 0.1783
2026-01-05 18:22:48,888: t15.2024.06.14 val PER: 0.1735
2026-01-05 18:22:48,888: t15.2024.07.19 val PER: 0.2538
2026-01-05 18:22:48,888: t15.2024.07.21 val PER: 0.1055
2026-01-05 18:22:48,888: t15.2024.07.28 val PER: 0.1404
2026-01-05 18:22:48,888: t15.2025.01.10 val PER: 0.3072
2026-01-05 18:22:48,888: t15.2025.01.12 val PER: 0.1671
2026-01-05 18:22:48,888: t15.2025.03.14 val PER: 0.3565
2026-01-05 18:22:48,888: t15.2025.03.16 val PER: 0.1911
2026-01-05 18:22:48,888: t15.2025.03.30 val PER: 0.3080
2026-01-05 18:22:48,888: t15.2025.04.13 val PER: 0.2297
2026-01-05 18:22:49,188: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_10500
2026-01-05 18:22:58,851: Train batch 10600: loss: 8.96 grad norm: 56.26 time: 0.072
2026-01-05 18:23:16,801: Train batch 10800: loss: 14.99 grad norm: 67.07 time: 0.065
2026-01-05 18:23:35,348: Train batch 11000: loss: 13.70 grad norm: 60.85 time: 0.059
2026-01-05 18:23:35,350: Running test after training batch: 11000
2026-01-05 18:23:35,464: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:23:40,573: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:23:40,613: WER debug example
  GT : how does it keep the cost down
  PR : houde dusts it keep the cost get
2026-01-05 18:23:42,674: Val batch 11000: PER (avg): 0.1634 CTC Loss (avg): 16.5086 WER(1gram): 48.48% (n=64) time: 7.324
2026-01-05 18:23:42,675: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 18:23:42,675: t15.2023.08.13 val PER: 0.1195
2026-01-05 18:23:42,675: t15.2023.08.18 val PER: 0.1174
2026-01-05 18:23:42,675: t15.2023.08.20 val PER: 0.1239
2026-01-05 18:23:42,676: t15.2023.08.25 val PER: 0.0889
2026-01-05 18:23:42,676: t15.2023.08.27 val PER: 0.1897
2026-01-05 18:23:42,676: t15.2023.09.01 val PER: 0.0755
2026-01-05 18:23:42,676: t15.2023.09.03 val PER: 0.1746
2026-01-05 18:23:42,676: t15.2023.09.24 val PER: 0.1456
2026-01-05 18:23:42,676: t15.2023.09.29 val PER: 0.1455
2026-01-05 18:23:42,676: t15.2023.10.01 val PER: 0.1869
2026-01-05 18:23:42,676: t15.2023.10.06 val PER: 0.0936
2026-01-05 18:23:42,676: t15.2023.10.08 val PER: 0.2571
2026-01-05 18:23:42,676: t15.2023.10.13 val PER: 0.2133
2026-01-05 18:23:42,676: t15.2023.10.15 val PER: 0.1668
2026-01-05 18:23:42,677: t15.2023.10.20 val PER: 0.1980
2026-01-05 18:23:42,677: t15.2023.10.22 val PER: 0.1247
2026-01-05 18:23:42,677: t15.2023.11.03 val PER: 0.1927
2026-01-05 18:23:42,677: t15.2023.11.04 val PER: 0.0478
2026-01-05 18:23:42,677: t15.2023.11.17 val PER: 0.0513
2026-01-05 18:23:42,677: t15.2023.11.19 val PER: 0.0419
2026-01-05 18:23:42,677: t15.2023.11.26 val PER: 0.1449
2026-01-05 18:23:42,677: t15.2023.12.03 val PER: 0.1355
2026-01-05 18:23:42,677: t15.2023.12.08 val PER: 0.1205
2026-01-05 18:23:42,677: t15.2023.12.10 val PER: 0.0986
2026-01-05 18:23:42,677: t15.2023.12.17 val PER: 0.1559
2026-01-05 18:23:42,677: t15.2023.12.29 val PER: 0.1414
2026-01-05 18:23:42,677: t15.2024.02.25 val PER: 0.1390
2026-01-05 18:23:42,677: t15.2024.03.08 val PER: 0.2432
2026-01-05 18:23:42,677: t15.2024.03.15 val PER: 0.2195
2026-01-05 18:23:42,678: t15.2024.03.17 val PER: 0.1464
2026-01-05 18:23:42,678: t15.2024.05.10 val PER: 0.1783
2026-01-05 18:23:42,678: t15.2024.06.14 val PER: 0.1688
2026-01-05 18:23:42,678: t15.2024.07.19 val PER: 0.2518
2026-01-05 18:23:42,678: t15.2024.07.21 val PER: 0.0979
2026-01-05 18:23:42,678: t15.2024.07.28 val PER: 0.1493
2026-01-05 18:23:42,678: t15.2025.01.10 val PER: 0.3168
2026-01-05 18:23:42,678: t15.2025.01.12 val PER: 0.1570
2026-01-05 18:23:42,678: t15.2025.03.14 val PER: 0.3550
2026-01-05 18:23:42,678: t15.2025.03.16 val PER: 0.2003
2026-01-05 18:23:42,678: t15.2025.03.30 val PER: 0.3000
2026-01-05 18:23:42,678: t15.2025.04.13 val PER: 0.2297
2026-01-05 18:23:42,680: New best val WER(1gram) 49.75% --> 48.48%
2026-01-05 18:23:42,680: Checkpointing model
2026-01-05 18:23:44,428: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:23:44,739: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_11000
2026-01-05 18:24:03,461: Train batch 11200: loss: 10.48 grad norm: 52.48 time: 0.072
2026-01-05 18:24:22,528: Train batch 11400: loss: 9.71 grad norm: 53.88 time: 0.057
2026-01-05 18:24:31,684: Running test after training batch: 11500
2026-01-05 18:24:31,851: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:24:36,555: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:24:36,586: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-05 18:24:38,358: Val batch 11500: PER (avg): 0.1603 CTC Loss (avg): 16.3974 WER(1gram): 50.51% (n=64) time: 6.674
2026-01-05 18:24:38,358: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-05 18:24:38,359: t15.2023.08.13 val PER: 0.1206
2026-01-05 18:24:38,359: t15.2023.08.18 val PER: 0.1182
2026-01-05 18:24:38,359: t15.2023.08.20 val PER: 0.1160
2026-01-05 18:24:38,359: t15.2023.08.25 val PER: 0.1009
2026-01-05 18:24:38,359: t15.2023.08.27 val PER: 0.1977
2026-01-05 18:24:38,359: t15.2023.09.01 val PER: 0.0860
2026-01-05 18:24:38,359: t15.2023.09.03 val PER: 0.1627
2026-01-05 18:24:38,359: t15.2023.09.24 val PER: 0.1335
2026-01-05 18:24:38,359: t15.2023.09.29 val PER: 0.1366
2026-01-05 18:24:38,359: t15.2023.10.01 val PER: 0.1836
2026-01-05 18:24:38,359: t15.2023.10.06 val PER: 0.0850
2026-01-05 18:24:38,360: t15.2023.10.08 val PER: 0.2490
2026-01-05 18:24:38,360: t15.2023.10.13 val PER: 0.2149
2026-01-05 18:24:38,360: t15.2023.10.15 val PER: 0.1701
2026-01-05 18:24:38,360: t15.2023.10.20 val PER: 0.1879
2026-01-05 18:24:38,360: t15.2023.10.22 val PER: 0.1192
2026-01-05 18:24:38,360: t15.2023.11.03 val PER: 0.1805
2026-01-05 18:24:38,360: t15.2023.11.04 val PER: 0.0307
2026-01-05 18:24:38,360: t15.2023.11.17 val PER: 0.0404
2026-01-05 18:24:38,360: t15.2023.11.19 val PER: 0.0459
2026-01-05 18:24:38,360: t15.2023.11.26 val PER: 0.1370
2026-01-05 18:24:38,361: t15.2023.12.03 val PER: 0.1239
2026-01-05 18:24:38,361: t15.2023.12.08 val PER: 0.1152
2026-01-05 18:24:38,361: t15.2023.12.10 val PER: 0.0986
2026-01-05 18:24:38,361: t15.2023.12.17 val PER: 0.1445
2026-01-05 18:24:38,361: t15.2023.12.29 val PER: 0.1352
2026-01-05 18:24:38,361: t15.2024.02.25 val PER: 0.1222
2026-01-05 18:24:38,361: t15.2024.03.08 val PER: 0.2361
2026-01-05 18:24:38,361: t15.2024.03.15 val PER: 0.2139
2026-01-05 18:24:38,361: t15.2024.03.17 val PER: 0.1485
2026-01-05 18:24:38,361: t15.2024.05.10 val PER: 0.1783
2026-01-05 18:24:38,361: t15.2024.06.14 val PER: 0.1719
2026-01-05 18:24:38,361: t15.2024.07.19 val PER: 0.2558
2026-01-05 18:24:38,361: t15.2024.07.21 val PER: 0.1048
2026-01-05 18:24:38,361: t15.2024.07.28 val PER: 0.1426
2026-01-05 18:24:38,361: t15.2025.01.10 val PER: 0.3044
2026-01-05 18:24:38,361: t15.2025.01.12 val PER: 0.1586
2026-01-05 18:24:38,362: t15.2025.03.14 val PER: 0.3491
2026-01-05 18:24:38,362: t15.2025.03.16 val PER: 0.2212
2026-01-05 18:24:38,362: t15.2025.03.30 val PER: 0.2989
2026-01-05 18:24:38,362: t15.2025.04.13 val PER: 0.2268
2026-01-05 18:24:38,642: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_11500
2026-01-05 18:24:47,178: Train batch 11600: loss: 10.86 grad norm: 48.40 time: 0.062
2026-01-05 18:25:04,361: Train batch 11800: loss: 6.83 grad norm: 45.58 time: 0.046
2026-01-05 18:25:22,985: Train batch 12000: loss: 13.56 grad norm: 53.20 time: 0.072
2026-01-05 18:25:22,986: Running test after training batch: 12000
2026-01-05 18:25:23,087: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:25:27,912: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:25:27,948: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 18:25:30,102: Val batch 12000: PER (avg): 0.1588 CTC Loss (avg): 16.1678 WER(1gram): 50.00% (n=64) time: 7.116
2026-01-05 18:25:30,103: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-05 18:25:30,103: t15.2023.08.13 val PER: 0.1185
2026-01-05 18:25:30,103: t15.2023.08.18 val PER: 0.1132
2026-01-05 18:25:30,103: t15.2023.08.20 val PER: 0.1160
2026-01-05 18:25:30,103: t15.2023.08.25 val PER: 0.0934
2026-01-05 18:25:30,104: t15.2023.08.27 val PER: 0.1929
2026-01-05 18:25:30,104: t15.2023.09.01 val PER: 0.0860
2026-01-05 18:25:30,104: t15.2023.09.03 val PER: 0.1615
2026-01-05 18:25:30,106: t15.2023.09.24 val PER: 0.1238
2026-01-05 18:25:30,107: t15.2023.09.29 val PER: 0.1372
2026-01-05 18:25:30,107: t15.2023.10.01 val PER: 0.1750
2026-01-05 18:25:30,107: t15.2023.10.06 val PER: 0.0872
2026-01-05 18:25:30,107: t15.2023.10.08 val PER: 0.2558
2026-01-05 18:25:30,107: t15.2023.10.13 val PER: 0.2126
2026-01-05 18:25:30,107: t15.2023.10.15 val PER: 0.1674
2026-01-05 18:25:30,107: t15.2023.10.20 val PER: 0.1913
2026-01-05 18:25:30,107: t15.2023.10.22 val PER: 0.1258
2026-01-05 18:25:30,107: t15.2023.11.03 val PER: 0.1832
2026-01-05 18:25:30,107: t15.2023.11.04 val PER: 0.0375
2026-01-05 18:25:30,107: t15.2023.11.17 val PER: 0.0342
2026-01-05 18:25:30,108: t15.2023.11.19 val PER: 0.0359
2026-01-05 18:25:30,108: t15.2023.11.26 val PER: 0.1312
2026-01-05 18:25:30,108: t15.2023.12.03 val PER: 0.1134
2026-01-05 18:25:30,108: t15.2023.12.08 val PER: 0.1039
2026-01-05 18:25:30,108: t15.2023.12.10 val PER: 0.0933
2026-01-05 18:25:30,108: t15.2023.12.17 val PER: 0.1445
2026-01-05 18:25:30,108: t15.2023.12.29 val PER: 0.1421
2026-01-05 18:25:30,108: t15.2024.02.25 val PER: 0.1208
2026-01-05 18:25:30,108: t15.2024.03.08 val PER: 0.2390
2026-01-05 18:25:30,108: t15.2024.03.15 val PER: 0.2101
2026-01-05 18:25:30,108: t15.2024.03.17 val PER: 0.1450
2026-01-05 18:25:30,109: t15.2024.05.10 val PER: 0.1902
2026-01-05 18:25:30,109: t15.2024.06.14 val PER: 0.1814
2026-01-05 18:25:30,109: t15.2024.07.19 val PER: 0.2617
2026-01-05 18:25:30,109: t15.2024.07.21 val PER: 0.1041
2026-01-05 18:25:30,109: t15.2024.07.28 val PER: 0.1456
2026-01-05 18:25:30,109: t15.2025.01.10 val PER: 0.3044
2026-01-05 18:25:30,109: t15.2025.01.12 val PER: 0.1540
2026-01-05 18:25:30,109: t15.2025.03.14 val PER: 0.3654
2026-01-05 18:25:30,109: t15.2025.03.16 val PER: 0.1963
2026-01-05 18:25:30,109: t15.2025.03.30 val PER: 0.3023
2026-01-05 18:25:30,109: t15.2025.04.13 val PER: 0.2168
2026-01-05 18:25:30,411: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_12000
2026-01-05 18:25:49,438: Train batch 12200: loss: 5.59 grad norm: 39.29 time: 0.067
2026-01-05 18:26:07,788: Train batch 12400: loss: 4.92 grad norm: 36.33 time: 0.041
2026-01-05 18:26:16,570: Running test after training batch: 12500
2026-01-05 18:26:16,672: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:26:21,635: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 18:26:21,729: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-05 18:26:24,336: Val batch 12500: PER (avg): 0.1563 CTC Loss (avg): 15.9921 WER(1gram): 48.98% (n=64) time: 7.765
2026-01-05 18:26:24,336: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-05 18:26:24,336: t15.2023.08.13 val PER: 0.1299
2026-01-05 18:26:24,337: t15.2023.08.18 val PER: 0.1081
2026-01-05 18:26:24,337: t15.2023.08.20 val PER: 0.1096
2026-01-05 18:26:24,337: t15.2023.08.25 val PER: 0.0934
2026-01-05 18:26:24,337: t15.2023.08.27 val PER: 0.2026
2026-01-05 18:26:24,337: t15.2023.09.01 val PER: 0.0828
2026-01-05 18:26:24,337: t15.2023.09.03 val PER: 0.1591
2026-01-05 18:26:24,337: t15.2023.09.24 val PER: 0.1286
2026-01-05 18:26:24,337: t15.2023.09.29 val PER: 0.1359
2026-01-05 18:26:24,337: t15.2023.10.01 val PER: 0.1777
2026-01-05 18:26:24,338: t15.2023.10.06 val PER: 0.0829
2026-01-05 18:26:24,338: t15.2023.10.08 val PER: 0.2612
2026-01-05 18:26:24,338: t15.2023.10.13 val PER: 0.2180
2026-01-05 18:26:24,338: t15.2023.10.15 val PER: 0.1595
2026-01-05 18:26:24,338: t15.2023.10.20 val PER: 0.1879
2026-01-05 18:26:24,338: t15.2023.10.22 val PER: 0.1114
2026-01-05 18:26:24,338: t15.2023.11.03 val PER: 0.1771
2026-01-05 18:26:24,338: t15.2023.11.04 val PER: 0.0307
2026-01-05 18:26:24,338: t15.2023.11.17 val PER: 0.0451
2026-01-05 18:26:24,338: t15.2023.11.19 val PER: 0.0319
2026-01-05 18:26:24,338: t15.2023.11.26 val PER: 0.1239
2026-01-05 18:26:24,338: t15.2023.12.03 val PER: 0.1218
2026-01-05 18:26:24,338: t15.2023.12.08 val PER: 0.1045
2026-01-05 18:26:24,339: t15.2023.12.10 val PER: 0.0920
2026-01-05 18:26:24,339: t15.2023.12.17 val PER: 0.1383
2026-01-05 18:26:24,339: t15.2023.12.29 val PER: 0.1428
2026-01-05 18:26:24,339: t15.2024.02.25 val PER: 0.1180
2026-01-05 18:26:24,339: t15.2024.03.08 val PER: 0.2361
2026-01-05 18:26:24,339: t15.2024.03.15 val PER: 0.2164
2026-01-05 18:26:24,339: t15.2024.03.17 val PER: 0.1541
2026-01-05 18:26:24,339: t15.2024.05.10 val PER: 0.1709
2026-01-05 18:26:24,339: t15.2024.06.14 val PER: 0.1735
2026-01-05 18:26:24,339: t15.2024.07.19 val PER: 0.2432
2026-01-05 18:26:24,340: t15.2024.07.21 val PER: 0.0979
2026-01-05 18:26:24,340: t15.2024.07.28 val PER: 0.1353
2026-01-05 18:26:24,340: t15.2025.01.10 val PER: 0.2906
2026-01-05 18:26:24,340: t15.2025.01.12 val PER: 0.1532
2026-01-05 18:26:24,340: t15.2025.03.14 val PER: 0.3580
2026-01-05 18:26:24,340: t15.2025.03.16 val PER: 0.1924
2026-01-05 18:26:24,340: t15.2025.03.30 val PER: 0.3023
2026-01-05 18:26:24,340: t15.2025.04.13 val PER: 0.2111
2026-01-05 18:26:24,648: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_12500
2026-01-05 18:26:33,405: Train batch 12600: loss: 7.75 grad norm: 43.22 time: 0.058
2026-01-05 18:26:52,130: Train batch 12800: loss: 5.68 grad norm: 35.97 time: 0.054
2026-01-05 18:27:11,420: Train batch 13000: loss: 6.24 grad norm: 41.18 time: 0.068
2026-01-05 18:27:11,421: Running test after training batch: 13000
2026-01-05 18:27:11,516: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:27:16,304: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:27:16,340: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost it
2026-01-05 18:27:18,259: Val batch 13000: PER (avg): 0.1541 CTC Loss (avg): 15.7789 WER(1gram): 46.19% (n=64) time: 6.838
2026-01-05 18:27:18,260: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 18:27:18,260: t15.2023.08.13 val PER: 0.1060
2026-01-05 18:27:18,260: t15.2023.08.18 val PER: 0.1174
2026-01-05 18:27:18,260: t15.2023.08.20 val PER: 0.1048
2026-01-05 18:27:18,260: t15.2023.08.25 val PER: 0.0843
2026-01-05 18:27:18,260: t15.2023.08.27 val PER: 0.1881
2026-01-05 18:27:18,260: t15.2023.09.01 val PER: 0.0771
2026-01-05 18:27:18,261: t15.2023.09.03 val PER: 0.1532
2026-01-05 18:27:18,261: t15.2023.09.24 val PER: 0.1286
2026-01-05 18:27:18,261: t15.2023.09.29 val PER: 0.1315
2026-01-05 18:27:18,261: t15.2023.10.01 val PER: 0.1783
2026-01-05 18:27:18,261: t15.2023.10.06 val PER: 0.0883
2026-01-05 18:27:18,261: t15.2023.10.08 val PER: 0.2585
2026-01-05 18:27:18,261: t15.2023.10.13 val PER: 0.2064
2026-01-05 18:27:18,261: t15.2023.10.15 val PER: 0.1602
2026-01-05 18:27:18,261: t15.2023.10.20 val PER: 0.1812
2026-01-05 18:27:18,262: t15.2023.10.22 val PER: 0.1158
2026-01-05 18:27:18,262: t15.2023.11.03 val PER: 0.1839
2026-01-05 18:27:18,262: t15.2023.11.04 val PER: 0.0307
2026-01-05 18:27:18,262: t15.2023.11.17 val PER: 0.0404
2026-01-05 18:27:18,262: t15.2023.11.19 val PER: 0.0439
2026-01-05 18:27:18,262: t15.2023.11.26 val PER: 0.1261
2026-01-05 18:27:18,262: t15.2023.12.03 val PER: 0.1208
2026-01-05 18:27:18,262: t15.2023.12.08 val PER: 0.1052
2026-01-05 18:27:18,262: t15.2023.12.10 val PER: 0.0933
2026-01-05 18:27:18,262: t15.2023.12.17 val PER: 0.1351
2026-01-05 18:27:18,263: t15.2023.12.29 val PER: 0.1345
2026-01-05 18:27:18,263: t15.2024.02.25 val PER: 0.1180
2026-01-05 18:27:18,263: t15.2024.03.08 val PER: 0.2333
2026-01-05 18:27:18,263: t15.2024.03.15 val PER: 0.2101
2026-01-05 18:27:18,263: t15.2024.03.17 val PER: 0.1444
2026-01-05 18:27:18,263: t15.2024.05.10 val PER: 0.1709
2026-01-05 18:27:18,263: t15.2024.06.14 val PER: 0.1719
2026-01-05 18:27:18,263: t15.2024.07.19 val PER: 0.2459
2026-01-05 18:27:18,263: t15.2024.07.21 val PER: 0.1021
2026-01-05 18:27:18,263: t15.2024.07.28 val PER: 0.1360
2026-01-05 18:27:18,263: t15.2025.01.10 val PER: 0.3044
2026-01-05 18:27:18,264: t15.2025.01.12 val PER: 0.1409
2026-01-05 18:27:18,264: t15.2025.03.14 val PER: 0.3462
2026-01-05 18:27:18,264: t15.2025.03.16 val PER: 0.1832
2026-01-05 18:27:18,264: t15.2025.03.30 val PER: 0.2977
2026-01-05 18:27:18,264: t15.2025.04.13 val PER: 0.2225
2026-01-05 18:27:18,264: New best val WER(1gram) 48.48% --> 46.19%
2026-01-05 18:27:18,265: Checkpointing model
2026-01-05 18:27:19,874: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:27:20,232: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_13000
2026-01-05 18:27:38,738: Train batch 13200: loss: 12.37 grad norm: 57.92 time: 0.054
2026-01-05 18:27:57,143: Train batch 13400: loss: 8.74 grad norm: 52.59 time: 0.063
2026-01-05 18:28:05,803: Running test after training batch: 13500
2026-01-05 18:28:05,907: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:28:10,924: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 18:28:10,967: WER debug example
  GT : how does it keep the cost down
  PR : houde dusts it keep the cost nit
2026-01-05 18:28:13,091: Val batch 13500: PER (avg): 0.1533 CTC Loss (avg): 15.5855 WER(1gram): 48.73% (n=64) time: 7.288
2026-01-05 18:28:13,091: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-05 18:28:13,092: t15.2023.08.13 val PER: 0.1195
2026-01-05 18:28:13,092: t15.2023.08.18 val PER: 0.1048
2026-01-05 18:28:13,093: t15.2023.08.20 val PER: 0.1041
2026-01-05 18:28:13,093: t15.2023.08.25 val PER: 0.0858
2026-01-05 18:28:13,093: t15.2023.08.27 val PER: 0.1961
2026-01-05 18:28:13,093: t15.2023.09.01 val PER: 0.0779
2026-01-05 18:28:13,094: t15.2023.09.03 val PER: 0.1651
2026-01-05 18:28:13,094: t15.2023.09.24 val PER: 0.1299
2026-01-05 18:28:13,094: t15.2023.09.29 val PER: 0.1334
2026-01-05 18:28:13,095: t15.2023.10.01 val PER: 0.1744
2026-01-05 18:28:13,095: t15.2023.10.06 val PER: 0.0829
2026-01-05 18:28:13,095: t15.2023.10.08 val PER: 0.2598
2026-01-05 18:28:13,096: t15.2023.10.13 val PER: 0.2102
2026-01-05 18:28:13,096: t15.2023.10.15 val PER: 0.1622
2026-01-05 18:28:13,096: t15.2023.10.20 val PER: 0.1846
2026-01-05 18:28:13,096: t15.2023.10.22 val PER: 0.1225
2026-01-05 18:28:13,097: t15.2023.11.03 val PER: 0.1798
2026-01-05 18:28:13,097: t15.2023.11.04 val PER: 0.0375
2026-01-05 18:28:13,097: t15.2023.11.17 val PER: 0.0513
2026-01-05 18:28:13,097: t15.2023.11.19 val PER: 0.0299
2026-01-05 18:28:13,097: t15.2023.11.26 val PER: 0.1297
2026-01-05 18:28:13,097: t15.2023.12.03 val PER: 0.1145
2026-01-05 18:28:13,097: t15.2023.12.08 val PER: 0.1065
2026-01-05 18:28:13,097: t15.2023.12.10 val PER: 0.0999
2026-01-05 18:28:13,098: t15.2023.12.17 val PER: 0.1331
2026-01-05 18:28:13,098: t15.2023.12.29 val PER: 0.1194
2026-01-05 18:28:13,098: t15.2024.02.25 val PER: 0.1180
2026-01-05 18:28:13,099: t15.2024.03.08 val PER: 0.2319
2026-01-05 18:28:13,099: t15.2024.03.15 val PER: 0.2039
2026-01-05 18:28:13,099: t15.2024.03.17 val PER: 0.1464
2026-01-05 18:28:13,100: t15.2024.05.10 val PER: 0.1679
2026-01-05 18:28:13,100: t15.2024.06.14 val PER: 0.1656
2026-01-05 18:28:13,100: t15.2024.07.19 val PER: 0.2380
2026-01-05 18:28:13,100: t15.2024.07.21 val PER: 0.0972
2026-01-05 18:28:13,100: t15.2024.07.28 val PER: 0.1390
2026-01-05 18:28:13,100: t15.2025.01.10 val PER: 0.2879
2026-01-05 18:28:13,100: t15.2025.01.12 val PER: 0.1493
2026-01-05 18:28:13,100: t15.2025.03.14 val PER: 0.3491
2026-01-05 18:28:13,100: t15.2025.03.16 val PER: 0.1728
2026-01-05 18:28:13,101: t15.2025.03.30 val PER: 0.3046
2026-01-05 18:28:13,101: t15.2025.04.13 val PER: 0.2154
2026-01-05 18:28:13,448: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_13500
2026-01-05 18:28:22,130: Train batch 13600: loss: 12.44 grad norm: 64.92 time: 0.063
2026-01-05 18:28:39,914: Train batch 13800: loss: 8.80 grad norm: 55.18 time: 0.057
2026-01-05 18:28:58,110: Train batch 14000: loss: 11.58 grad norm: 56.33 time: 0.051
2026-01-05 18:28:58,111: Running test after training batch: 14000
2026-01-05 18:28:58,275: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:29:03,099: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:29:03,135: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-05 18:29:05,779: Val batch 14000: PER (avg): 0.1509 CTC Loss (avg): 15.4962 WER(1gram): 47.72% (n=64) time: 7.668
2026-01-05 18:29:05,779: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 18:29:05,779: t15.2023.08.13 val PER: 0.1143
2026-01-05 18:29:05,780: t15.2023.08.18 val PER: 0.1056
2026-01-05 18:29:05,780: t15.2023.08.20 val PER: 0.0993
2026-01-05 18:29:05,780: t15.2023.08.25 val PER: 0.1039
2026-01-05 18:29:05,780: t15.2023.08.27 val PER: 0.1929
2026-01-05 18:29:05,780: t15.2023.09.01 val PER: 0.0795
2026-01-05 18:29:05,780: t15.2023.09.03 val PER: 0.1663
2026-01-05 18:29:05,780: t15.2023.09.24 val PER: 0.1226
2026-01-05 18:29:05,780: t15.2023.09.29 val PER: 0.1340
2026-01-05 18:29:05,780: t15.2023.10.01 val PER: 0.1757
2026-01-05 18:29:05,780: t15.2023.10.06 val PER: 0.0807
2026-01-05 18:29:05,780: t15.2023.10.08 val PER: 0.2530
2026-01-05 18:29:05,780: t15.2023.10.13 val PER: 0.2040
2026-01-05 18:29:05,781: t15.2023.10.15 val PER: 0.1536
2026-01-05 18:29:05,781: t15.2023.10.20 val PER: 0.1711
2026-01-05 18:29:05,781: t15.2023.10.22 val PER: 0.1114
2026-01-05 18:29:05,781: t15.2023.11.03 val PER: 0.1805
2026-01-05 18:29:05,781: t15.2023.11.04 val PER: 0.0307
2026-01-05 18:29:05,781: t15.2023.11.17 val PER: 0.0435
2026-01-05 18:29:05,781: t15.2023.11.19 val PER: 0.0319
2026-01-05 18:29:05,781: t15.2023.11.26 val PER: 0.1283
2026-01-05 18:29:05,781: t15.2023.12.03 val PER: 0.1250
2026-01-05 18:29:05,782: t15.2023.12.08 val PER: 0.1032
2026-01-05 18:29:05,782: t15.2023.12.10 val PER: 0.0972
2026-01-05 18:29:05,782: t15.2023.12.17 val PER: 0.1351
2026-01-05 18:29:05,782: t15.2023.12.29 val PER: 0.1283
2026-01-05 18:29:05,782: t15.2024.02.25 val PER: 0.1110
2026-01-05 18:29:05,782: t15.2024.03.08 val PER: 0.2176
2026-01-05 18:29:05,782: t15.2024.03.15 val PER: 0.1989
2026-01-05 18:29:05,782: t15.2024.03.17 val PER: 0.1450
2026-01-05 18:29:05,782: t15.2024.05.10 val PER: 0.1620
2026-01-05 18:29:05,782: t15.2024.06.14 val PER: 0.1672
2026-01-05 18:29:05,782: t15.2024.07.19 val PER: 0.2221
2026-01-05 18:29:05,782: t15.2024.07.21 val PER: 0.0972
2026-01-05 18:29:05,782: t15.2024.07.28 val PER: 0.1309
2026-01-05 18:29:05,782: t15.2025.01.10 val PER: 0.2851
2026-01-05 18:29:05,782: t15.2025.01.12 val PER: 0.1509
2026-01-05 18:29:05,782: t15.2025.03.14 val PER: 0.3358
2026-01-05 18:29:05,782: t15.2025.03.16 val PER: 0.1859
2026-01-05 18:29:05,786: t15.2025.03.30 val PER: 0.2943
2026-01-05 18:29:05,786: t15.2025.04.13 val PER: 0.2225
2026-01-05 18:29:06,107: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_14000
2026-01-05 18:29:23,233: Train batch 14200: loss: 8.12 grad norm: 50.52 time: 0.056
2026-01-05 18:29:42,684: Train batch 14400: loss: 5.74 grad norm: 37.59 time: 0.065
2026-01-05 18:29:52,487: Running test after training batch: 14500
2026-01-05 18:29:52,593: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:29:57,529: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:29:57,576: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-05 18:30:00,615: Val batch 14500: PER (avg): 0.1512 CTC Loss (avg): 15.4957 WER(1gram): 48.22% (n=64) time: 8.128
2026-01-05 18:30:00,616: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 18:30:00,616: t15.2023.08.13 val PER: 0.1091
2026-01-05 18:30:00,616: t15.2023.08.18 val PER: 0.1098
2026-01-05 18:30:00,616: t15.2023.08.20 val PER: 0.0993
2026-01-05 18:30:00,617: t15.2023.08.25 val PER: 0.0904
2026-01-05 18:30:00,617: t15.2023.08.27 val PER: 0.1977
2026-01-05 18:30:00,617: t15.2023.09.01 val PER: 0.0787
2026-01-05 18:30:00,617: t15.2023.09.03 val PER: 0.1603
2026-01-05 18:30:00,617: t15.2023.09.24 val PER: 0.1274
2026-01-05 18:30:00,617: t15.2023.09.29 val PER: 0.1251
2026-01-05 18:30:00,617: t15.2023.10.01 val PER: 0.1810
2026-01-05 18:30:00,617: t15.2023.10.06 val PER: 0.0861
2026-01-05 18:30:00,617: t15.2023.10.08 val PER: 0.2463
2026-01-05 18:30:00,617: t15.2023.10.13 val PER: 0.2056
2026-01-05 18:30:00,617: t15.2023.10.15 val PER: 0.1562
2026-01-05 18:30:00,618: t15.2023.10.20 val PER: 0.1711
2026-01-05 18:30:00,618: t15.2023.10.22 val PER: 0.1147
2026-01-05 18:30:00,618: t15.2023.11.03 val PER: 0.1818
2026-01-05 18:30:00,618: t15.2023.11.04 val PER: 0.0341
2026-01-05 18:30:00,618: t15.2023.11.17 val PER: 0.0389
2026-01-05 18:30:00,618: t15.2023.11.19 val PER: 0.0220
2026-01-05 18:30:00,618: t15.2023.11.26 val PER: 0.1246
2026-01-05 18:30:00,618: t15.2023.12.03 val PER: 0.1113
2026-01-05 18:30:00,618: t15.2023.12.08 val PER: 0.1025
2026-01-05 18:30:00,618: t15.2023.12.10 val PER: 0.0920
2026-01-05 18:30:00,619: t15.2023.12.17 val PER: 0.1362
2026-01-05 18:30:00,619: t15.2023.12.29 val PER: 0.1270
2026-01-05 18:30:00,619: t15.2024.02.25 val PER: 0.1152
2026-01-05 18:30:00,619: t15.2024.03.08 val PER: 0.2319
2026-01-05 18:30:00,619: t15.2024.03.15 val PER: 0.2045
2026-01-05 18:30:00,619: t15.2024.03.17 val PER: 0.1423
2026-01-05 18:30:00,619: t15.2024.05.10 val PER: 0.1471
2026-01-05 18:30:00,619: t15.2024.06.14 val PER: 0.1672
2026-01-05 18:30:00,619: t15.2024.07.19 val PER: 0.2386
2026-01-05 18:30:00,620: t15.2024.07.21 val PER: 0.0979
2026-01-05 18:30:00,620: t15.2024.07.28 val PER: 0.1331
2026-01-05 18:30:00,620: t15.2025.01.10 val PER: 0.2824
2026-01-05 18:30:00,620: t15.2025.01.12 val PER: 0.1501
2026-01-05 18:30:00,620: t15.2025.03.14 val PER: 0.3595
2026-01-05 18:30:00,620: t15.2025.03.16 val PER: 0.1859
2026-01-05 18:30:00,620: t15.2025.03.30 val PER: 0.2954
2026-01-05 18:30:00,620: t15.2025.04.13 val PER: 0.2111
2026-01-05 18:30:00,942: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_14500
2026-01-05 18:30:10,475: Train batch 14600: loss: 12.11 grad norm: 60.06 time: 0.061
2026-01-05 18:30:30,726: Train batch 14800: loss: 5.73 grad norm: 43.83 time: 0.052
2026-01-05 18:30:49,912: Train batch 15000: loss: 8.58 grad norm: 50.32 time: 0.054
2026-01-05 18:30:49,913: Running test after training batch: 15000
2026-01-05 18:30:50,073: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:30:54,943: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:30:54,977: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-05 18:30:56,820: Val batch 15000: PER (avg): 0.1489 CTC Loss (avg): 15.2450 WER(1gram): 45.43% (n=64) time: 6.907
2026-01-05 18:30:56,821: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 18:30:56,821: t15.2023.08.13 val PER: 0.1029
2026-01-05 18:30:56,821: t15.2023.08.18 val PER: 0.0989
2026-01-05 18:30:56,821: t15.2023.08.20 val PER: 0.1080
2026-01-05 18:30:56,821: t15.2023.08.25 val PER: 0.0964
2026-01-05 18:30:56,821: t15.2023.08.27 val PER: 0.1881
2026-01-05 18:30:56,821: t15.2023.09.01 val PER: 0.0763
2026-01-05 18:30:56,822: t15.2023.09.03 val PER: 0.1401
2026-01-05 18:30:56,822: t15.2023.09.24 val PER: 0.1262
2026-01-05 18:30:56,822: t15.2023.09.29 val PER: 0.1321
2026-01-05 18:30:56,822: t15.2023.10.01 val PER: 0.1737
2026-01-05 18:30:56,822: t15.2023.10.06 val PER: 0.0732
2026-01-05 18:30:56,822: t15.2023.10.08 val PER: 0.2503
2026-01-05 18:30:56,822: t15.2023.10.13 val PER: 0.1971
2026-01-05 18:30:56,822: t15.2023.10.15 val PER: 0.1503
2026-01-05 18:30:56,822: t15.2023.10.20 val PER: 0.1913
2026-01-05 18:30:56,822: t15.2023.10.22 val PER: 0.1180
2026-01-05 18:30:56,822: t15.2023.11.03 val PER: 0.1777
2026-01-05 18:30:56,822: t15.2023.11.04 val PER: 0.0375
2026-01-05 18:30:56,823: t15.2023.11.17 val PER: 0.0373
2026-01-05 18:30:56,823: t15.2023.11.19 val PER: 0.0399
2026-01-05 18:30:56,823: t15.2023.11.26 val PER: 0.1203
2026-01-05 18:30:56,823: t15.2023.12.03 val PER: 0.1134
2026-01-05 18:30:56,823: t15.2023.12.08 val PER: 0.1019
2026-01-05 18:30:56,823: t15.2023.12.10 val PER: 0.0907
2026-01-05 18:30:56,823: t15.2023.12.17 val PER: 0.1341
2026-01-05 18:30:56,823: t15.2023.12.29 val PER: 0.1283
2026-01-05 18:30:56,824: t15.2024.02.25 val PER: 0.1053
2026-01-05 18:30:56,824: t15.2024.03.08 val PER: 0.2205
2026-01-05 18:30:56,824: t15.2024.03.15 val PER: 0.1995
2026-01-05 18:30:56,824: t15.2024.03.17 val PER: 0.1423
2026-01-05 18:30:56,824: t15.2024.05.10 val PER: 0.1605
2026-01-05 18:30:56,824: t15.2024.06.14 val PER: 0.1640
2026-01-05 18:30:56,824: t15.2024.07.19 val PER: 0.2307
2026-01-05 18:30:56,824: t15.2024.07.21 val PER: 0.0924
2026-01-05 18:30:56,824: t15.2024.07.28 val PER: 0.1324
2026-01-05 18:30:56,824: t15.2025.01.10 val PER: 0.2920
2026-01-05 18:30:56,824: t15.2025.01.12 val PER: 0.1470
2026-01-05 18:30:56,825: t15.2025.03.14 val PER: 0.3462
2026-01-05 18:30:56,825: t15.2025.03.16 val PER: 0.1793
2026-01-05 18:30:56,825: t15.2025.03.30 val PER: 0.2954
2026-01-05 18:30:56,825: t15.2025.04.13 val PER: 0.2211
2026-01-05 18:30:56,826: New best val WER(1gram) 46.19% --> 45.43%
2026-01-05 18:30:56,826: Checkpointing model
2026-01-05 18:30:58,493: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:30:58,801: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_15000
2026-01-05 18:31:17,765: Train batch 15200: loss: 4.95 grad norm: 40.16 time: 0.056
2026-01-05 18:31:36,457: Train batch 15400: loss: 11.06 grad norm: 54.97 time: 0.051
2026-01-05 18:31:45,580: Running test after training batch: 15500
2026-01-05 18:31:45,751: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:31:50,648: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:31:50,688: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 18:31:53,070: Val batch 15500: PER (avg): 0.1476 CTC Loss (avg): 15.2013 WER(1gram): 45.18% (n=64) time: 7.490
2026-01-05 18:31:53,070: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 18:31:53,071: t15.2023.08.13 val PER: 0.1081
2026-01-05 18:31:53,071: t15.2023.08.18 val PER: 0.1014
2026-01-05 18:31:53,071: t15.2023.08.20 val PER: 0.1048
2026-01-05 18:31:53,071: t15.2023.08.25 val PER: 0.0873
2026-01-05 18:31:53,071: t15.2023.08.27 val PER: 0.1849
2026-01-05 18:31:53,071: t15.2023.09.01 val PER: 0.0706
2026-01-05 18:31:53,071: t15.2023.09.03 val PER: 0.1496
2026-01-05 18:31:53,071: t15.2023.09.24 val PER: 0.1214
2026-01-05 18:31:53,071: t15.2023.09.29 val PER: 0.1264
2026-01-05 18:31:53,071: t15.2023.10.01 val PER: 0.1737
2026-01-05 18:31:53,072: t15.2023.10.06 val PER: 0.0829
2026-01-05 18:31:53,072: t15.2023.10.08 val PER: 0.2449
2026-01-05 18:31:53,072: t15.2023.10.13 val PER: 0.2025
2026-01-05 18:31:53,072: t15.2023.10.15 val PER: 0.1543
2026-01-05 18:31:53,072: t15.2023.10.20 val PER: 0.1779
2026-01-05 18:31:53,072: t15.2023.10.22 val PER: 0.1125
2026-01-05 18:31:53,072: t15.2023.11.03 val PER: 0.1730
2026-01-05 18:31:53,072: t15.2023.11.04 val PER: 0.0307
2026-01-05 18:31:53,073: t15.2023.11.17 val PER: 0.0327
2026-01-05 18:31:53,073: t15.2023.11.19 val PER: 0.0299
2026-01-05 18:31:53,073: t15.2023.11.26 val PER: 0.1196
2026-01-05 18:31:53,073: t15.2023.12.03 val PER: 0.1134
2026-01-05 18:31:53,073: t15.2023.12.08 val PER: 0.1019
2026-01-05 18:31:53,073: t15.2023.12.10 val PER: 0.0946
2026-01-05 18:31:53,073: t15.2023.12.17 val PER: 0.1289
2026-01-05 18:31:53,073: t15.2023.12.29 val PER: 0.1263
2026-01-05 18:31:53,073: t15.2024.02.25 val PER: 0.1138
2026-01-05 18:31:53,073: t15.2024.03.08 val PER: 0.2219
2026-01-05 18:31:53,073: t15.2024.03.15 val PER: 0.1914
2026-01-05 18:31:53,073: t15.2024.03.17 val PER: 0.1388
2026-01-05 18:31:53,073: t15.2024.05.10 val PER: 0.1605
2026-01-05 18:31:53,073: t15.2024.06.14 val PER: 0.1546
2026-01-05 18:31:53,073: t15.2024.07.19 val PER: 0.2281
2026-01-05 18:31:53,074: t15.2024.07.21 val PER: 0.0979
2026-01-05 18:31:53,074: t15.2024.07.28 val PER: 0.1368
2026-01-05 18:31:53,074: t15.2025.01.10 val PER: 0.2824
2026-01-05 18:31:53,074: t15.2025.01.12 val PER: 0.1470
2026-01-05 18:31:53,074: t15.2025.03.14 val PER: 0.3491
2026-01-05 18:31:53,074: t15.2025.03.16 val PER: 0.1806
2026-01-05 18:31:53,074: t15.2025.03.30 val PER: 0.2839
2026-01-05 18:31:53,074: t15.2025.04.13 val PER: 0.2197
2026-01-05 18:31:53,075: New best val WER(1gram) 45.43% --> 45.18%
2026-01-05 18:31:53,076: Checkpointing model
2026-01-05 18:31:54,806: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:31:55,134: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_15500
2026-01-05 18:32:03,778: Train batch 15600: loss: 12.10 grad norm: 59.90 time: 0.064
2026-01-05 18:32:21,044: Train batch 15800: loss: 13.23 grad norm: 61.35 time: 0.068
2026-01-05 18:32:38,498: Train batch 16000: loss: 8.79 grad norm: 50.00 time: 0.057
2026-01-05 18:32:38,498: Running test after training batch: 16000
2026-01-05 18:32:38,643: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:32:43,443: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:32:43,501: WER debug example
  GT : how does it keep the cost down
  PR : houde dusts it keep the cost get
2026-01-05 18:32:45,749: Val batch 16000: PER (avg): 0.1488 CTC Loss (avg): 15.3310 WER(1gram): 45.94% (n=64) time: 7.251
2026-01-05 18:32:45,750: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-05 18:32:45,750: t15.2023.08.13 val PER: 0.1102
2026-01-05 18:32:45,750: t15.2023.08.18 val PER: 0.1056
2026-01-05 18:32:45,750: t15.2023.08.20 val PER: 0.1112
2026-01-05 18:32:45,751: t15.2023.08.25 val PER: 0.0889
2026-01-05 18:32:45,751: t15.2023.08.27 val PER: 0.1929
2026-01-05 18:32:45,751: t15.2023.09.01 val PER: 0.0795
2026-01-05 18:32:45,751: t15.2023.09.03 val PER: 0.1425
2026-01-05 18:32:45,751: t15.2023.09.24 val PER: 0.1250
2026-01-05 18:32:45,751: t15.2023.09.29 val PER: 0.1295
2026-01-05 18:32:45,751: t15.2023.10.01 val PER: 0.1737
2026-01-05 18:32:45,751: t15.2023.10.06 val PER: 0.0829
2026-01-05 18:32:45,751: t15.2023.10.08 val PER: 0.2571
2026-01-05 18:32:45,752: t15.2023.10.13 val PER: 0.1939
2026-01-05 18:32:45,752: t15.2023.10.15 val PER: 0.1470
2026-01-05 18:32:45,752: t15.2023.10.20 val PER: 0.1846
2026-01-05 18:32:45,752: t15.2023.10.22 val PER: 0.1080
2026-01-05 18:32:45,752: t15.2023.11.03 val PER: 0.1757
2026-01-05 18:32:45,752: t15.2023.11.04 val PER: 0.0341
2026-01-05 18:32:45,752: t15.2023.11.17 val PER: 0.0342
2026-01-05 18:32:45,752: t15.2023.11.19 val PER: 0.0379
2026-01-05 18:32:45,752: t15.2023.11.26 val PER: 0.1159
2026-01-05 18:32:45,752: t15.2023.12.03 val PER: 0.1176
2026-01-05 18:32:45,753: t15.2023.12.08 val PER: 0.0985
2026-01-05 18:32:45,753: t15.2023.12.10 val PER: 0.0907
2026-01-05 18:32:45,753: t15.2023.12.17 val PER: 0.1237
2026-01-05 18:32:45,753: t15.2023.12.29 val PER: 0.1283
2026-01-05 18:32:45,753: t15.2024.02.25 val PER: 0.1081
2026-01-05 18:32:45,753: t15.2024.03.08 val PER: 0.2276
2026-01-05 18:32:45,753: t15.2024.03.15 val PER: 0.1976
2026-01-05 18:32:45,753: t15.2024.03.17 val PER: 0.1325
2026-01-05 18:32:45,753: t15.2024.05.10 val PER: 0.1738
2026-01-05 18:32:45,753: t15.2024.06.14 val PER: 0.1625
2026-01-05 18:32:45,754: t15.2024.07.19 val PER: 0.2386
2026-01-05 18:32:45,754: t15.2024.07.21 val PER: 0.0952
2026-01-05 18:32:45,754: t15.2024.07.28 val PER: 0.1375
2026-01-05 18:32:45,754: t15.2025.01.10 val PER: 0.2865
2026-01-05 18:32:45,754: t15.2025.01.12 val PER: 0.1440
2026-01-05 18:32:45,754: t15.2025.03.14 val PER: 0.3521
2026-01-05 18:32:45,754: t15.2025.03.16 val PER: 0.1806
2026-01-05 18:32:45,758: t15.2025.03.30 val PER: 0.2943
2026-01-05 18:32:45,758: t15.2025.04.13 val PER: 0.2126
2026-01-05 18:32:46,062: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_16000
2026-01-05 18:33:04,067: Train batch 16200: loss: 6.38 grad norm: 43.70 time: 0.055
2026-01-05 18:33:22,493: Train batch 16400: loss: 10.43 grad norm: 60.19 time: 0.057
2026-01-05 18:33:31,912: Running test after training batch: 16500
2026-01-05 18:33:32,045: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:33:36,877: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:33:36,912: WER debug example
  GT : how does it keep the cost down
  PR : houde dusts it keep the cost et
2026-01-05 18:33:38,770: Val batch 16500: PER (avg): 0.1474 CTC Loss (avg): 15.1848 WER(1gram): 45.43% (n=64) time: 6.858
2026-01-05 18:33:38,771: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 18:33:38,771: t15.2023.08.13 val PER: 0.1112
2026-01-05 18:33:38,771: t15.2023.08.18 val PER: 0.0997
2026-01-05 18:33:38,771: t15.2023.08.20 val PER: 0.1025
2026-01-05 18:33:38,771: t15.2023.08.25 val PER: 0.0813
2026-01-05 18:33:38,771: t15.2023.08.27 val PER: 0.1849
2026-01-05 18:33:38,771: t15.2023.09.01 val PER: 0.0771
2026-01-05 18:33:38,772: t15.2023.09.03 val PER: 0.1496
2026-01-05 18:33:38,772: t15.2023.09.24 val PER: 0.1250
2026-01-05 18:33:38,772: t15.2023.09.29 val PER: 0.1308
2026-01-05 18:33:38,772: t15.2023.10.01 val PER: 0.1684
2026-01-05 18:33:38,772: t15.2023.10.06 val PER: 0.0764
2026-01-05 18:33:38,772: t15.2023.10.08 val PER: 0.2530
2026-01-05 18:33:38,772: t15.2023.10.13 val PER: 0.1955
2026-01-05 18:33:38,772: t15.2023.10.15 val PER: 0.1549
2026-01-05 18:33:38,772: t15.2023.10.20 val PER: 0.1980
2026-01-05 18:33:38,772: t15.2023.10.22 val PER: 0.1136
2026-01-05 18:33:38,772: t15.2023.11.03 val PER: 0.1784
2026-01-05 18:33:38,772: t15.2023.11.04 val PER: 0.0307
2026-01-05 18:33:38,773: t15.2023.11.17 val PER: 0.0342
2026-01-05 18:33:38,773: t15.2023.11.19 val PER: 0.0419
2026-01-05 18:33:38,773: t15.2023.11.26 val PER: 0.1138
2026-01-05 18:33:38,773: t15.2023.12.03 val PER: 0.1124
2026-01-05 18:33:38,773: t15.2023.12.08 val PER: 0.1039
2026-01-05 18:33:38,773: t15.2023.12.10 val PER: 0.0894
2026-01-05 18:33:38,773: t15.2023.12.17 val PER: 0.1216
2026-01-05 18:33:38,773: t15.2023.12.29 val PER: 0.1242
2026-01-05 18:33:38,773: t15.2024.02.25 val PER: 0.1081
2026-01-05 18:33:38,773: t15.2024.03.08 val PER: 0.2233
2026-01-05 18:33:38,773: t15.2024.03.15 val PER: 0.1964
2026-01-05 18:33:38,773: t15.2024.03.17 val PER: 0.1367
2026-01-05 18:33:38,773: t15.2024.05.10 val PER: 0.1560
2026-01-05 18:33:38,773: t15.2024.06.14 val PER: 0.1625
2026-01-05 18:33:38,773: t15.2024.07.19 val PER: 0.2360
2026-01-05 18:33:38,773: t15.2024.07.21 val PER: 0.0924
2026-01-05 18:33:38,773: t15.2024.07.28 val PER: 0.1279
2026-01-05 18:33:38,774: t15.2025.01.10 val PER: 0.2796
2026-01-05 18:33:38,774: t15.2025.01.12 val PER: 0.1386
2026-01-05 18:33:38,774: t15.2025.03.14 val PER: 0.3565
2026-01-05 18:33:38,774: t15.2025.03.16 val PER: 0.1859
2026-01-05 18:33:38,774: t15.2025.03.30 val PER: 0.2943
2026-01-05 18:33:38,774: t15.2025.04.13 val PER: 0.2068
2026-01-05 18:33:39,058: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_16500
2026-01-05 18:33:47,790: Train batch 16600: loss: 8.64 grad norm: 54.55 time: 0.053
2026-01-05 18:34:05,312: Train batch 16800: loss: 16.42 grad norm: 73.76 time: 0.062
2026-01-05 18:34:24,541: Train batch 17000: loss: 7.92 grad norm: 47.78 time: 0.086
2026-01-05 18:34:24,541: Running test after training batch: 17000
2026-01-05 18:34:24,652: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:34:29,480: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:34:29,518: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-05 18:34:31,918: Val batch 17000: PER (avg): 0.1460 CTC Loss (avg): 15.0437 WER(1gram): 45.94% (n=64) time: 7.377
2026-01-05 18:34:31,919: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-05 18:34:31,919: t15.2023.08.13 val PER: 0.1071
2026-01-05 18:34:31,919: t15.2023.08.18 val PER: 0.1023
2026-01-05 18:34:31,919: t15.2023.08.20 val PER: 0.0993
2026-01-05 18:34:31,920: t15.2023.08.25 val PER: 0.0858
2026-01-05 18:34:31,920: t15.2023.08.27 val PER: 0.1913
2026-01-05 18:34:31,920: t15.2023.09.01 val PER: 0.0739
2026-01-05 18:34:31,920: t15.2023.09.03 val PER: 0.1425
2026-01-05 18:34:31,920: t15.2023.09.24 val PER: 0.1250
2026-01-05 18:34:31,920: t15.2023.09.29 val PER: 0.1308
2026-01-05 18:34:31,920: t15.2023.10.01 val PER: 0.1704
2026-01-05 18:34:31,920: t15.2023.10.06 val PER: 0.0764
2026-01-05 18:34:31,920: t15.2023.10.08 val PER: 0.2409
2026-01-05 18:34:31,920: t15.2023.10.13 val PER: 0.1908
2026-01-05 18:34:31,921: t15.2023.10.15 val PER: 0.1516
2026-01-05 18:34:31,921: t15.2023.10.20 val PER: 0.1846
2026-01-05 18:34:31,921: t15.2023.10.22 val PER: 0.1080
2026-01-05 18:34:31,921: t15.2023.11.03 val PER: 0.1811
2026-01-05 18:34:31,921: t15.2023.11.04 val PER: 0.0341
2026-01-05 18:34:31,921: t15.2023.11.17 val PER: 0.0311
2026-01-05 18:34:31,921: t15.2023.11.19 val PER: 0.0399
2026-01-05 18:34:31,921: t15.2023.11.26 val PER: 0.1145
2026-01-05 18:34:31,921: t15.2023.12.03 val PER: 0.1103
2026-01-05 18:34:31,921: t15.2023.12.08 val PER: 0.0959
2026-01-05 18:34:31,921: t15.2023.12.10 val PER: 0.0894
2026-01-05 18:34:31,921: t15.2023.12.17 val PER: 0.1237
2026-01-05 18:34:31,922: t15.2023.12.29 val PER: 0.1249
2026-01-05 18:34:31,922: t15.2024.02.25 val PER: 0.1124
2026-01-05 18:34:31,922: t15.2024.03.08 val PER: 0.2262
2026-01-05 18:34:31,922: t15.2024.03.15 val PER: 0.1964
2026-01-05 18:34:31,922: t15.2024.03.17 val PER: 0.1409
2026-01-05 18:34:31,922: t15.2024.05.10 val PER: 0.1605
2026-01-05 18:34:31,922: t15.2024.06.14 val PER: 0.1530
2026-01-05 18:34:31,922: t15.2024.07.19 val PER: 0.2301
2026-01-05 18:34:31,922: t15.2024.07.21 val PER: 0.0903
2026-01-05 18:34:31,922: t15.2024.07.28 val PER: 0.1287
2026-01-05 18:34:31,922: t15.2025.01.10 val PER: 0.2796
2026-01-05 18:34:31,922: t15.2025.01.12 val PER: 0.1416
2026-01-05 18:34:31,922: t15.2025.03.14 val PER: 0.3506
2026-01-05 18:34:31,922: t15.2025.03.16 val PER: 0.1754
2026-01-05 18:34:31,922: t15.2025.03.30 val PER: 0.2851
2026-01-05 18:34:31,922: t15.2025.04.13 val PER: 0.2111
2026-01-05 18:34:32,232: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_17000
2026-01-05 18:34:51,128: Train batch 17200: loss: 9.41 grad norm: 48.52 time: 0.086
2026-01-05 18:35:10,588: Train batch 17400: loss: 11.72 grad norm: 58.11 time: 0.072
2026-01-05 18:35:20,275: Running test after training batch: 17500
2026-01-05 18:35:20,531: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:35:25,407: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:35:25,463: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-05 18:35:27,879: Val batch 17500: PER (avg): 0.1459 CTC Loss (avg): 15.0323 WER(1gram): 46.45% (n=64) time: 7.604
2026-01-05 18:35:27,879: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-05 18:35:27,880: t15.2023.08.13 val PER: 0.1081
2026-01-05 18:35:27,880: t15.2023.08.18 val PER: 0.1014
2026-01-05 18:35:27,880: t15.2023.08.20 val PER: 0.1080
2026-01-05 18:35:27,880: t15.2023.08.25 val PER: 0.0919
2026-01-05 18:35:27,881: t15.2023.08.27 val PER: 0.1881
2026-01-05 18:35:27,881: t15.2023.09.01 val PER: 0.0755
2026-01-05 18:35:27,881: t15.2023.09.03 val PER: 0.1473
2026-01-05 18:35:27,881: t15.2023.09.24 val PER: 0.1238
2026-01-05 18:35:27,881: t15.2023.09.29 val PER: 0.1302
2026-01-05 18:35:27,881: t15.2023.10.01 val PER: 0.1731
2026-01-05 18:35:27,881: t15.2023.10.06 val PER: 0.0721
2026-01-05 18:35:27,881: t15.2023.10.08 val PER: 0.2463
2026-01-05 18:35:27,881: t15.2023.10.13 val PER: 0.1870
2026-01-05 18:35:27,882: t15.2023.10.15 val PER: 0.1496
2026-01-05 18:35:27,882: t15.2023.10.20 val PER: 0.1879
2026-01-05 18:35:27,882: t15.2023.10.22 val PER: 0.1080
2026-01-05 18:35:27,882: t15.2023.11.03 val PER: 0.1764
2026-01-05 18:35:27,882: t15.2023.11.04 val PER: 0.0341
2026-01-05 18:35:27,882: t15.2023.11.17 val PER: 0.0295
2026-01-05 18:35:27,882: t15.2023.11.19 val PER: 0.0379
2026-01-05 18:35:27,882: t15.2023.11.26 val PER: 0.1094
2026-01-05 18:35:27,882: t15.2023.12.03 val PER: 0.1040
2026-01-05 18:35:27,882: t15.2023.12.08 val PER: 0.1019
2026-01-05 18:35:27,883: t15.2023.12.10 val PER: 0.0880
2026-01-05 18:35:27,883: t15.2023.12.17 val PER: 0.1216
2026-01-05 18:35:27,883: t15.2023.12.29 val PER: 0.1283
2026-01-05 18:35:27,883: t15.2024.02.25 val PER: 0.1067
2026-01-05 18:35:27,883: t15.2024.03.08 val PER: 0.2233
2026-01-05 18:35:27,883: t15.2024.03.15 val PER: 0.1895
2026-01-05 18:35:27,883: t15.2024.03.17 val PER: 0.1395
2026-01-05 18:35:27,883: t15.2024.05.10 val PER: 0.1590
2026-01-05 18:35:27,883: t15.2024.06.14 val PER: 0.1530
2026-01-05 18:35:27,883: t15.2024.07.19 val PER: 0.2301
2026-01-05 18:35:27,883: t15.2024.07.21 val PER: 0.0972
2026-01-05 18:35:27,883: t15.2024.07.28 val PER: 0.1324
2026-01-05 18:35:27,883: t15.2025.01.10 val PER: 0.2796
2026-01-05 18:35:27,883: t15.2025.01.12 val PER: 0.1363
2026-01-05 18:35:27,883: t15.2025.03.14 val PER: 0.3550
2026-01-05 18:35:27,884: t15.2025.03.16 val PER: 0.1754
2026-01-05 18:35:27,884: t15.2025.03.30 val PER: 0.2851
2026-01-05 18:35:27,884: t15.2025.04.13 val PER: 0.2154
2026-01-05 18:35:28,187: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_17500
2026-01-05 18:35:37,548: Train batch 17600: loss: 9.40 grad norm: 56.04 time: 0.052
2026-01-05 18:35:55,353: Train batch 17800: loss: 6.18 grad norm: 51.02 time: 0.045
2026-01-05 18:36:14,461: Train batch 18000: loss: 10.99 grad norm: 61.25 time: 0.064
2026-01-05 18:36:14,461: Running test after training batch: 18000
2026-01-05 18:36:14,608: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:36:19,422: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:36:19,463: WER debug example
  GT : how does it keep the cost down
  PR : houde dusts it keep the cost it
2026-01-05 18:36:21,944: Val batch 18000: PER (avg): 0.1449 CTC Loss (avg): 15.0092 WER(1gram): 45.18% (n=64) time: 7.482
2026-01-05 18:36:21,944: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-05 18:36:21,945: t15.2023.08.13 val PER: 0.1081
2026-01-05 18:36:21,945: t15.2023.08.18 val PER: 0.0972
2026-01-05 18:36:21,945: t15.2023.08.20 val PER: 0.1001
2026-01-05 18:36:21,945: t15.2023.08.25 val PER: 0.0858
2026-01-05 18:36:21,946: t15.2023.08.27 val PER: 0.1897
2026-01-05 18:36:21,946: t15.2023.09.01 val PER: 0.0771
2026-01-05 18:36:21,946: t15.2023.09.03 val PER: 0.1425
2026-01-05 18:36:21,946: t15.2023.09.24 val PER: 0.1238
2026-01-05 18:36:21,946: t15.2023.09.29 val PER: 0.1283
2026-01-05 18:36:21,946: t15.2023.10.01 val PER: 0.1724
2026-01-05 18:36:21,946: t15.2023.10.06 val PER: 0.0753
2026-01-05 18:36:21,946: t15.2023.10.08 val PER: 0.2422
2026-01-05 18:36:21,946: t15.2023.10.13 val PER: 0.1908
2026-01-05 18:36:21,947: t15.2023.10.15 val PER: 0.1496
2026-01-05 18:36:21,947: t15.2023.10.20 val PER: 0.1946
2026-01-05 18:36:21,947: t15.2023.10.22 val PER: 0.1069
2026-01-05 18:36:21,947: t15.2023.11.03 val PER: 0.1771
2026-01-05 18:36:21,947: t15.2023.11.04 val PER: 0.0307
2026-01-05 18:36:21,947: t15.2023.11.17 val PER: 0.0327
2026-01-05 18:36:21,947: t15.2023.11.19 val PER: 0.0339
2026-01-05 18:36:21,947: t15.2023.11.26 val PER: 0.1109
2026-01-05 18:36:21,947: t15.2023.12.03 val PER: 0.1113
2026-01-05 18:36:21,947: t15.2023.12.08 val PER: 0.0999
2026-01-05 18:36:21,947: t15.2023.12.10 val PER: 0.0894
2026-01-05 18:36:21,947: t15.2023.12.17 val PER: 0.1195
2026-01-05 18:36:21,947: t15.2023.12.29 val PER: 0.1235
2026-01-05 18:36:21,948: t15.2024.02.25 val PER: 0.1081
2026-01-05 18:36:21,948: t15.2024.03.08 val PER: 0.2176
2026-01-05 18:36:21,948: t15.2024.03.15 val PER: 0.1901
2026-01-05 18:36:21,948: t15.2024.03.17 val PER: 0.1374
2026-01-05 18:36:21,948: t15.2024.05.10 val PER: 0.1471
2026-01-05 18:36:21,948: t15.2024.06.14 val PER: 0.1546
2026-01-05 18:36:21,948: t15.2024.07.19 val PER: 0.2287
2026-01-05 18:36:21,948: t15.2024.07.21 val PER: 0.0945
2026-01-05 18:36:21,948: t15.2024.07.28 val PER: 0.1309
2026-01-05 18:36:21,948: t15.2025.01.10 val PER: 0.2865
2026-01-05 18:36:21,948: t15.2025.01.12 val PER: 0.1386
2026-01-05 18:36:21,949: t15.2025.03.14 val PER: 0.3550
2026-01-05 18:36:21,949: t15.2025.03.16 val PER: 0.1728
2026-01-05 18:36:21,949: t15.2025.03.30 val PER: 0.2816
2026-01-05 18:36:21,949: t15.2025.04.13 val PER: 0.2111
2026-01-05 18:36:22,254: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_18000
2026-01-05 18:36:41,512: Train batch 18200: loss: 7.46 grad norm: 49.61 time: 0.074
2026-01-05 18:36:59,354: Train batch 18400: loss: 5.03 grad norm: 48.42 time: 0.059
2026-01-05 18:37:08,147: Running test after training batch: 18500
2026-01-05 18:37:08,252: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:37:13,082: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:37:13,132: WER debug example
  GT : how does it keep the cost down
  PR : houde dusts it keep the cost et
2026-01-05 18:37:15,558: Val batch 18500: PER (avg): 0.1446 CTC Loss (avg): 15.0067 WER(1gram): 44.92% (n=64) time: 7.411
2026-01-05 18:37:15,558: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-05 18:37:15,558: t15.2023.08.13 val PER: 0.1050
2026-01-05 18:37:15,559: t15.2023.08.18 val PER: 0.0981
2026-01-05 18:37:15,559: t15.2023.08.20 val PER: 0.1009
2026-01-05 18:37:15,559: t15.2023.08.25 val PER: 0.0858
2026-01-05 18:37:15,559: t15.2023.08.27 val PER: 0.1913
2026-01-05 18:37:15,559: t15.2023.09.01 val PER: 0.0763
2026-01-05 18:37:15,559: t15.2023.09.03 val PER: 0.1437
2026-01-05 18:37:15,559: t15.2023.09.24 val PER: 0.1262
2026-01-05 18:37:15,559: t15.2023.09.29 val PER: 0.1302
2026-01-05 18:37:15,559: t15.2023.10.01 val PER: 0.1704
2026-01-05 18:37:15,559: t15.2023.10.06 val PER: 0.0753
2026-01-05 18:37:15,560: t15.2023.10.08 val PER: 0.2517
2026-01-05 18:37:15,560: t15.2023.10.13 val PER: 0.1908
2026-01-05 18:37:15,560: t15.2023.10.15 val PER: 0.1510
2026-01-05 18:37:15,560: t15.2023.10.20 val PER: 0.1812
2026-01-05 18:37:15,560: t15.2023.10.22 val PER: 0.1024
2026-01-05 18:37:15,560: t15.2023.11.03 val PER: 0.1798
2026-01-05 18:37:15,560: t15.2023.11.04 val PER: 0.0341
2026-01-05 18:37:15,560: t15.2023.11.17 val PER: 0.0342
2026-01-05 18:37:15,561: t15.2023.11.19 val PER: 0.0319
2026-01-05 18:37:15,561: t15.2023.11.26 val PER: 0.1130
2026-01-05 18:37:15,561: t15.2023.12.03 val PER: 0.1050
2026-01-05 18:37:15,561: t15.2023.12.08 val PER: 0.1012
2026-01-05 18:37:15,561: t15.2023.12.10 val PER: 0.0880
2026-01-05 18:37:15,561: t15.2023.12.17 val PER: 0.1195
2026-01-05 18:37:15,561: t15.2023.12.29 val PER: 0.1174
2026-01-05 18:37:15,561: t15.2024.02.25 val PER: 0.1053
2026-01-05 18:37:15,561: t15.2024.03.08 val PER: 0.2191
2026-01-05 18:37:15,561: t15.2024.03.15 val PER: 0.1901
2026-01-05 18:37:15,561: t15.2024.03.17 val PER: 0.1395
2026-01-05 18:37:15,561: t15.2024.05.10 val PER: 0.1456
2026-01-05 18:37:15,561: t15.2024.06.14 val PER: 0.1609
2026-01-05 18:37:15,561: t15.2024.07.19 val PER: 0.2287
2026-01-05 18:37:15,561: t15.2024.07.21 val PER: 0.0897
2026-01-05 18:37:15,561: t15.2024.07.28 val PER: 0.1279
2026-01-05 18:37:15,562: t15.2025.01.10 val PER: 0.2755
2026-01-05 18:37:15,562: t15.2025.01.12 val PER: 0.1440
2026-01-05 18:37:15,562: t15.2025.03.14 val PER: 0.3491
2026-01-05 18:37:15,562: t15.2025.03.16 val PER: 0.1741
2026-01-05 18:37:15,562: t15.2025.03.30 val PER: 0.2839
2026-01-05 18:37:15,562: t15.2025.04.13 val PER: 0.2054
2026-01-05 18:37:15,563: New best val WER(1gram) 45.18% --> 44.92%
2026-01-05 18:37:15,563: Checkpointing model
2026-01-05 18:37:17,311: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:37:17,630: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_18500
2026-01-05 18:37:26,318: Train batch 18600: loss: 11.98 grad norm: 58.96 time: 0.068
2026-01-05 18:37:44,594: Train batch 18800: loss: 8.17 grad norm: 49.70 time: 0.067
2026-01-05 18:38:03,944: Train batch 19000: loss: 8.13 grad norm: 45.53 time: 0.064
2026-01-05 18:38:03,944: Running test after training batch: 19000
2026-01-05 18:38:04,085: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:38:08,808: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:38:08,842: WER debug example
  GT : how does it keep the cost down
  PR : houde dusts it keep the cost it
2026-01-05 18:38:10,757: Val batch 19000: PER (avg): 0.1453 CTC Loss (avg): 15.0374 WER(1gram): 45.43% (n=64) time: 6.812
2026-01-05 18:38:10,757: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-05 18:38:10,757: t15.2023.08.13 val PER: 0.1050
2026-01-05 18:38:10,757: t15.2023.08.18 val PER: 0.0956
2026-01-05 18:38:10,758: t15.2023.08.20 val PER: 0.0993
2026-01-05 18:38:10,758: t15.2023.08.25 val PER: 0.0783
2026-01-05 18:38:10,758: t15.2023.08.27 val PER: 0.1897
2026-01-05 18:38:10,758: t15.2023.09.01 val PER: 0.0787
2026-01-05 18:38:10,758: t15.2023.09.03 val PER: 0.1449
2026-01-05 18:38:10,758: t15.2023.09.24 val PER: 0.1286
2026-01-05 18:38:10,758: t15.2023.09.29 val PER: 0.1295
2026-01-05 18:38:10,758: t15.2023.10.01 val PER: 0.1744
2026-01-05 18:38:10,758: t15.2023.10.06 val PER: 0.0764
2026-01-05 18:38:10,758: t15.2023.10.08 val PER: 0.2463
2026-01-05 18:38:10,758: t15.2023.10.13 val PER: 0.1901
2026-01-05 18:38:10,758: t15.2023.10.15 val PER: 0.1490
2026-01-05 18:38:10,759: t15.2023.10.20 val PER: 0.1946
2026-01-05 18:38:10,759: t15.2023.10.22 val PER: 0.1024
2026-01-05 18:38:10,759: t15.2023.11.03 val PER: 0.1818
2026-01-05 18:38:10,759: t15.2023.11.04 val PER: 0.0307
2026-01-05 18:38:10,759: t15.2023.11.17 val PER: 0.0342
2026-01-05 18:38:10,759: t15.2023.11.19 val PER: 0.0379
2026-01-05 18:38:10,759: t15.2023.11.26 val PER: 0.1123
2026-01-05 18:38:10,759: t15.2023.12.03 val PER: 0.1050
2026-01-05 18:38:10,759: t15.2023.12.08 val PER: 0.0999
2026-01-05 18:38:10,759: t15.2023.12.10 val PER: 0.0920
2026-01-05 18:38:10,759: t15.2023.12.17 val PER: 0.1164
2026-01-05 18:38:10,759: t15.2023.12.29 val PER: 0.1215
2026-01-05 18:38:10,759: t15.2024.02.25 val PER: 0.1096
2026-01-05 18:38:10,759: t15.2024.03.08 val PER: 0.2191
2026-01-05 18:38:10,760: t15.2024.03.15 val PER: 0.1914
2026-01-05 18:38:10,760: t15.2024.03.17 val PER: 0.1381
2026-01-05 18:38:10,760: t15.2024.05.10 val PER: 0.1456
2026-01-05 18:38:10,760: t15.2024.06.14 val PER: 0.1593
2026-01-05 18:38:10,760: t15.2024.07.19 val PER: 0.2347
2026-01-05 18:38:10,760: t15.2024.07.21 val PER: 0.0903
2026-01-05 18:38:10,760: t15.2024.07.28 val PER: 0.1346
2026-01-05 18:38:10,760: t15.2025.01.10 val PER: 0.2851
2026-01-05 18:38:10,760: t15.2025.01.12 val PER: 0.1370
2026-01-05 18:38:10,760: t15.2025.03.14 val PER: 0.3595
2026-01-05 18:38:10,760: t15.2025.03.16 val PER: 0.1715
2026-01-05 18:38:10,760: t15.2025.03.30 val PER: 0.2828
2026-01-05 18:38:10,760: t15.2025.04.13 val PER: 0.2068
2026-01-05 18:38:11,039: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_19000
2026-01-05 18:38:29,543: Train batch 19200: loss: 5.72 grad norm: 44.41 time: 0.063
2026-01-05 18:38:48,480: Train batch 19400: loss: 4.69 grad norm: 34.57 time: 0.052
2026-01-05 18:38:57,938: Running test after training batch: 19500
2026-01-05 18:38:58,083: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:39:02,919: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:39:02,964: WER debug example
  GT : how does it keep the cost down
  PR : houde dusts it keep the cost et
2026-01-05 18:39:05,597: Val batch 19500: PER (avg): 0.1443 CTC Loss (avg): 14.9878 WER(1gram): 45.18% (n=64) time: 7.659
2026-01-05 18:39:05,598: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-05 18:39:05,598: t15.2023.08.13 val PER: 0.1071
2026-01-05 18:39:05,598: t15.2023.08.18 val PER: 0.0989
2026-01-05 18:39:05,599: t15.2023.08.20 val PER: 0.1001
2026-01-05 18:39:05,599: t15.2023.08.25 val PER: 0.0843
2026-01-05 18:39:05,599: t15.2023.08.27 val PER: 0.1801
2026-01-05 18:39:05,599: t15.2023.09.01 val PER: 0.0739
2026-01-05 18:39:05,599: t15.2023.09.03 val PER: 0.1437
2026-01-05 18:39:05,599: t15.2023.09.24 val PER: 0.1238
2026-01-05 18:39:05,599: t15.2023.09.29 val PER: 0.1315
2026-01-05 18:39:05,599: t15.2023.10.01 val PER: 0.1744
2026-01-05 18:39:05,599: t15.2023.10.06 val PER: 0.0775
2026-01-05 18:39:05,599: t15.2023.10.08 val PER: 0.2436
2026-01-05 18:39:05,599: t15.2023.10.13 val PER: 0.1893
2026-01-05 18:39:05,599: t15.2023.10.15 val PER: 0.1490
2026-01-05 18:39:05,600: t15.2023.10.20 val PER: 0.1913
2026-01-05 18:39:05,600: t15.2023.10.22 val PER: 0.1047
2026-01-05 18:39:05,600: t15.2023.11.03 val PER: 0.1784
2026-01-05 18:39:05,600: t15.2023.11.04 val PER: 0.0341
2026-01-05 18:39:05,600: t15.2023.11.17 val PER: 0.0327
2026-01-05 18:39:05,600: t15.2023.11.19 val PER: 0.0339
2026-01-05 18:39:05,600: t15.2023.11.26 val PER: 0.1094
2026-01-05 18:39:05,600: t15.2023.12.03 val PER: 0.1050
2026-01-05 18:39:05,600: t15.2023.12.08 val PER: 0.0965
2026-01-05 18:39:05,600: t15.2023.12.10 val PER: 0.0854
2026-01-05 18:39:05,600: t15.2023.12.17 val PER: 0.1216
2026-01-05 18:39:05,600: t15.2023.12.29 val PER: 0.1229
2026-01-05 18:39:05,600: t15.2024.02.25 val PER: 0.1039
2026-01-05 18:39:05,600: t15.2024.03.08 val PER: 0.2134
2026-01-05 18:39:05,600: t15.2024.03.15 val PER: 0.1882
2026-01-05 18:39:05,600: t15.2024.03.17 val PER: 0.1339
2026-01-05 18:39:05,601: t15.2024.05.10 val PER: 0.1501
2026-01-05 18:39:05,601: t15.2024.06.14 val PER: 0.1530
2026-01-05 18:39:05,601: t15.2024.07.19 val PER: 0.2287
2026-01-05 18:39:05,601: t15.2024.07.21 val PER: 0.0952
2026-01-05 18:39:05,601: t15.2024.07.28 val PER: 0.1316
2026-01-05 18:39:05,601: t15.2025.01.10 val PER: 0.2782
2026-01-05 18:39:05,601: t15.2025.01.12 val PER: 0.1409
2026-01-05 18:39:05,601: t15.2025.03.14 val PER: 0.3565
2026-01-05 18:39:05,601: t15.2025.03.16 val PER: 0.1754
2026-01-05 18:39:05,601: t15.2025.03.30 val PER: 0.2828
2026-01-05 18:39:05,601: t15.2025.04.13 val PER: 0.2126
2026-01-05 18:39:05,913: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_19500
2026-01-05 18:39:14,575: Train batch 19600: loss: 7.60 grad norm: 47.13 time: 0.058
2026-01-05 18:39:31,928: Train batch 19800: loss: 7.48 grad norm: 50.27 time: 0.056
2026-01-05 18:39:49,032: Running test after training batch: 19999
2026-01-05 18:39:49,123: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:39:53,945: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 18:39:53,987: WER debug example
  GT : how does it keep the cost down
  PR : houde dusts it keep the cost it
2026-01-05 18:39:56,846: Val batch 19999: PER (avg): 0.1438 CTC Loss (avg): 14.9838 WER(1gram): 44.92% (n=64) time: 7.813
2026-01-05 18:39:56,846: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-05 18:39:56,846: t15.2023.08.13 val PER: 0.1040
2026-01-05 18:39:56,847: t15.2023.08.18 val PER: 0.0997
2026-01-05 18:39:56,847: t15.2023.08.20 val PER: 0.1001
2026-01-05 18:39:56,847: t15.2023.08.25 val PER: 0.0858
2026-01-05 18:39:56,847: t15.2023.08.27 val PER: 0.1833
2026-01-05 18:39:56,847: t15.2023.09.01 val PER: 0.0763
2026-01-05 18:39:56,847: t15.2023.09.03 val PER: 0.1413
2026-01-05 18:39:56,847: t15.2023.09.24 val PER: 0.1262
2026-01-05 18:39:56,848: t15.2023.09.29 val PER: 0.1276
2026-01-05 18:39:56,848: t15.2023.10.01 val PER: 0.1731
2026-01-05 18:39:56,848: t15.2023.10.06 val PER: 0.0732
2026-01-05 18:39:56,848: t15.2023.10.08 val PER: 0.2449
2026-01-05 18:39:56,848: t15.2023.10.13 val PER: 0.1955
2026-01-05 18:39:56,848: t15.2023.10.15 val PER: 0.1503
2026-01-05 18:39:56,848: t15.2023.10.20 val PER: 0.1913
2026-01-05 18:39:56,848: t15.2023.10.22 val PER: 0.1036
2026-01-05 18:39:56,848: t15.2023.11.03 val PER: 0.1818
2026-01-05 18:39:56,848: t15.2023.11.04 val PER: 0.0341
2026-01-05 18:39:56,848: t15.2023.11.17 val PER: 0.0327
2026-01-05 18:39:56,848: t15.2023.11.19 val PER: 0.0319
2026-01-05 18:39:56,849: t15.2023.11.26 val PER: 0.1065
2026-01-05 18:39:56,849: t15.2023.12.03 val PER: 0.1050
2026-01-05 18:39:56,849: t15.2023.12.08 val PER: 0.0985
2026-01-05 18:39:56,849: t15.2023.12.10 val PER: 0.0867
2026-01-05 18:39:56,849: t15.2023.12.17 val PER: 0.1175
2026-01-05 18:39:56,849: t15.2023.12.29 val PER: 0.1194
2026-01-05 18:39:56,849: t15.2024.02.25 val PER: 0.1039
2026-01-05 18:39:56,849: t15.2024.03.08 val PER: 0.2191
2026-01-05 18:39:56,849: t15.2024.03.15 val PER: 0.1876
2026-01-05 18:39:56,849: t15.2024.03.17 val PER: 0.1325
2026-01-05 18:39:56,849: t15.2024.05.10 val PER: 0.1412
2026-01-05 18:39:56,849: t15.2024.06.14 val PER: 0.1562
2026-01-05 18:39:56,849: t15.2024.07.19 val PER: 0.2307
2026-01-05 18:39:56,849: t15.2024.07.21 val PER: 0.0931
2026-01-05 18:39:56,849: t15.2024.07.28 val PER: 0.1301
2026-01-05 18:39:56,849: t15.2025.01.10 val PER: 0.2755
2026-01-05 18:39:56,850: t15.2025.01.12 val PER: 0.1393
2026-01-05 18:39:56,850: t15.2025.03.14 val PER: 0.3565
2026-01-05 18:39:56,850: t15.2025.03.16 val PER: 0.1702
2026-01-05 18:39:56,850: t15.2025.03.30 val PER: 0.2828
2026-01-05 18:39:56,850: t15.2025.04.13 val PER: 0.2068
2026-01-05 18:39:57,150: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st4_wd1e-5/checkpoint/checkpoint_batch_19999
2026-01-05 18:39:57,182: Best avg val PER achieved: 0.14457
2026-01-05 18:39:57,182: Total training time: 37.47 minutes

=== RUN st6_wd1e-5.yaml ===
2026-01-05 18:40:03,151: Using device: cuda:0
2026-01-05 18:40:04,976: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-05 18:40:04,997: Using 45 sessions after filtering (from 45).
2026-01-05 18:40:05,401: Using torch.compile (if available)
2026-01-05 18:40:05,402: torch.compile not available (torch<2.0). Skipping.
2026-01-05 18:40:05,402: Initialized RNN decoding model
2026-01-05 18:40:05,402: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-05 18:40:05,402: Model has 44,907,305 parameters
2026-01-05 18:40:05,402: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-05 18:40:06,670: Successfully initialized datasets
2026-01-05 18:40:06,671: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-05 18:40:07,667: Train batch 0: loss: 380.40 grad norm: 1000.45 time: 0.190
2026-01-05 18:40:07,667: Running test after training batch: 0
2026-01-05 18:40:07,775: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:40:12,408: WER debug example
  GT : you can see the code at this point as well
  PR : hempfling
2026-01-05 18:40:12,764: WER debug example
  GT : how does it keep the cost down
  PR : nonfarm
2026-01-05 18:40:33,934: Val batch 0: PER (avg): 1.0959 CTC Loss (avg): 415.7360 WER(1gram): 100.00% (n=64) time: 26.267
2026-01-05 18:40:33,934: WER lens: avg_true_words=6.16 avg_pred_words=0.98 max_pred_words=2
2026-01-05 18:40:33,935: t15.2023.08.13 val PER: 1.0229
2026-01-05 18:40:33,935: t15.2023.08.18 val PER: 1.0545
2026-01-05 18:40:33,935: t15.2023.08.20 val PER: 1.0016
2026-01-05 18:40:33,935: t15.2023.08.25 val PER: 1.0557
2026-01-05 18:40:33,935: t15.2023.08.27 val PER: 1.0032
2026-01-05 18:40:33,935: t15.2023.09.01 val PER: 1.0576
2026-01-05 18:40:33,936: t15.2023.09.03 val PER: 1.0333
2026-01-05 18:40:33,936: t15.2023.09.24 val PER: 1.1966
2026-01-05 18:40:33,936: t15.2023.09.29 val PER: 1.1002
2026-01-05 18:40:33,936: t15.2023.10.01 val PER: 0.9762
2026-01-05 18:40:33,936: t15.2023.10.06 val PER: 1.1378
2026-01-05 18:40:33,936: t15.2023.10.08 val PER: 0.9648
2026-01-05 18:40:33,936: t15.2023.10.13 val PER: 1.0683
2026-01-05 18:40:33,936: t15.2023.10.15 val PER: 1.0534
2026-01-05 18:40:33,936: t15.2023.10.20 val PER: 1.1174
2026-01-05 18:40:33,936: t15.2023.10.22 val PER: 1.0746
2026-01-05 18:40:33,936: t15.2023.11.03 val PER: 1.1927
2026-01-05 18:40:33,937: t15.2023.11.04 val PER: 1.4198
2026-01-05 18:40:33,937: t15.2023.11.17 val PER: 1.4246
2026-01-05 18:40:33,937: t15.2023.11.19 val PER: 1.2315
2026-01-05 18:40:33,937: t15.2023.11.26 val PER: 1.1565
2026-01-05 18:40:33,937: t15.2023.12.03 val PER: 1.0935
2026-01-05 18:40:33,937: t15.2023.12.08 val PER: 1.1059
2026-01-05 18:40:33,937: t15.2023.12.10 val PER: 1.2654
2026-01-05 18:40:33,937: t15.2023.12.17 val PER: 1.0000
2026-01-05 18:40:33,937: t15.2023.12.29 val PER: 1.0707
2026-01-05 18:40:33,937: t15.2024.02.25 val PER: 1.0927
2026-01-05 18:40:33,937: t15.2024.03.08 val PER: 1.0626
2026-01-05 18:40:33,938: t15.2024.03.15 val PER: 1.0557
2026-01-05 18:40:33,938: t15.2024.03.17 val PER: 1.0732
2026-01-05 18:40:33,938: t15.2024.05.10 val PER: 1.0535
2026-01-05 18:40:33,938: t15.2024.06.14 val PER: 1.1562
2026-01-05 18:40:33,938: t15.2024.07.19 val PER: 0.9321
2026-01-05 18:40:33,938: t15.2024.07.21 val PER: 1.1821
2026-01-05 18:40:33,938: t15.2024.07.28 val PER: 1.2199
2026-01-05 18:40:33,938: t15.2025.01.10 val PER: 0.9394
2026-01-05 18:40:33,938: t15.2025.01.12 val PER: 1.2764
2026-01-05 18:40:33,938: t15.2025.03.14 val PER: 0.9379
2026-01-05 18:40:33,938: t15.2025.03.16 val PER: 1.2147
2026-01-05 18:40:33,938: t15.2025.03.30 val PER: 1.0667
2026-01-05 18:40:33,938: t15.2025.04.13 val PER: 1.1826
2026-01-05 18:40:33,939: New best val WER(1gram) inf% --> 100.00%
2026-01-05 18:40:33,939: Checkpointing model
2026-01-05 18:40:34,212: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st6_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:40:34,492: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st6_wd1e-5/checkpoint/checkpoint_batch_0
2026-01-05 18:40:53,389: Train batch 200: loss: 75.84 grad norm: 110.07 time: 0.044
2026-01-05 18:41:07,640: Non-finite grad norm at step 357: nan. Skipping optimizer step.
2026-01-05 18:41:11,468: Train batch 400: loss: 52.43 grad norm: 71.93 time: 0.050
2026-01-05 18:41:20,649: Running test after training batch: 500
2026-01-05 18:41:20,745: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:41:25,364: WER debug example
  GT : you can see the code at this point as well
  PR : ude and e uhde at de is
2026-01-05 18:41:25,381: WER debug example
  GT : how does it keep the cost down
  PR : ide is it ou us
2026-01-05 18:41:26,490: Val batch 500: PER (avg): 0.5127 CTC Loss (avg): 52.7634 WER(1gram): 94.67% (n=64) time: 5.841
2026-01-05 18:41:26,491: WER lens: avg_true_words=6.16 avg_pred_words=4.52 max_pred_words=8
2026-01-05 18:41:26,491: t15.2023.08.13 val PER: 0.4844
2026-01-05 18:41:26,491: t15.2023.08.18 val PER: 0.4443
2026-01-05 18:41:26,491: t15.2023.08.20 val PER: 0.4583
2026-01-05 18:41:26,491: t15.2023.08.25 val PER: 0.4367
2026-01-05 18:41:26,491: t15.2023.08.27 val PER: 0.5482
2026-01-05 18:41:26,491: t15.2023.09.01 val PER: 0.4115
2026-01-05 18:41:26,491: t15.2023.09.03 val PER: 0.4929
2026-01-05 18:41:26,491: t15.2023.09.24 val PER: 0.4248
2026-01-05 18:41:26,491: t15.2023.09.29 val PER: 0.4448
2026-01-05 18:41:26,491: t15.2023.10.01 val PER: 0.5112
2026-01-05 18:41:26,491: t15.2023.10.06 val PER: 0.4284
2026-01-05 18:41:26,491: t15.2023.10.08 val PER: 0.5359
2026-01-05 18:41:26,492: t15.2023.10.13 val PER: 0.5252
2026-01-05 18:41:26,492: t15.2023.10.15 val PER: 0.4977
2026-01-05 18:41:26,492: t15.2023.10.20 val PER: 0.4631
2026-01-05 18:41:26,492: t15.2023.10.22 val PER: 0.4432
2026-01-05 18:41:26,492: t15.2023.11.03 val PER: 0.4749
2026-01-05 18:41:26,492: t15.2023.11.04 val PER: 0.2935
2026-01-05 18:41:26,492: t15.2023.11.17 val PER: 0.3670
2026-01-05 18:41:26,492: t15.2023.11.19 val PER: 0.3333
2026-01-05 18:41:26,492: t15.2023.11.26 val PER: 0.5362
2026-01-05 18:41:26,492: t15.2023.12.03 val PER: 0.4842
2026-01-05 18:41:26,492: t15.2023.12.08 val PER: 0.5133
2026-01-05 18:41:26,492: t15.2023.12.10 val PER: 0.4442
2026-01-05 18:41:26,492: t15.2023.12.17 val PER: 0.5884
2026-01-05 18:41:26,492: t15.2023.12.29 val PER: 0.5539
2026-01-05 18:41:26,493: t15.2024.02.25 val PER: 0.4438
2026-01-05 18:41:26,493: t15.2024.03.08 val PER: 0.6302
2026-01-05 18:41:26,493: t15.2024.03.15 val PER: 0.5616
2026-01-05 18:41:26,493: t15.2024.03.17 val PER: 0.5091
2026-01-05 18:41:26,493: t15.2024.05.10 val PER: 0.5260
2026-01-05 18:41:26,493: t15.2024.06.14 val PER: 0.5284
2026-01-05 18:41:26,493: t15.2024.07.19 val PER: 0.6908
2026-01-05 18:41:26,493: t15.2024.07.21 val PER: 0.4814
2026-01-05 18:41:26,493: t15.2024.07.28 val PER: 0.4897
2026-01-05 18:41:26,493: t15.2025.01.10 val PER: 0.7204
2026-01-05 18:41:26,493: t15.2025.01.12 val PER: 0.5420
2026-01-05 18:41:26,493: t15.2025.03.14 val PER: 0.7308
2026-01-05 18:41:26,493: t15.2025.03.16 val PER: 0.5733
2026-01-05 18:41:26,493: t15.2025.03.30 val PER: 0.6966
2026-01-05 18:41:26,493: t15.2025.04.13 val PER: 0.5549
2026-01-05 18:41:26,494: New best val WER(1gram) 100.00% --> 94.67%
2026-01-05 18:41:26,495: Checkpointing model
2026-01-05 18:41:28,161: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st6_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:41:28,445: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st6_wd1e-5/checkpoint/checkpoint_batch_500
2026-01-05 18:41:33,179: Non-finite grad norm at step 552: nan. Skipping optimizer step.
2026-01-05 18:41:36,025: Non-finite grad norm at step 583: nan. Skipping optimizer step.
2026-01-05 18:41:37,660: Train batch 600: loss: 48.86 grad norm: 80.68 time: 0.061
2026-01-05 18:41:56,044: Train batch 800: loss: 39.16 grad norm: 70.83 time: 0.046
2026-01-05 18:41:58,217: Non-finite grad norm at step 824: nan. Skipping optimizer step.
2026-01-05 18:42:00,109: Non-finite grad norm at step 845: nan. Skipping optimizer step.
2026-01-05 18:42:15,223: Train batch 1000: loss: 42.11 grad norm: 66.84 time: 0.054
2026-01-05 18:42:15,223: Running test after training batch: 1000
2026-01-05 18:42:15,358: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:42:19,861: WER debug example
  GT : you can see the code at this point as well
  PR : yule and ia a uhde it this uhde is li
2026-01-05 18:42:19,879: WER debug example
  GT : how does it keep the cost down
  PR : out us it pu the us utt
2026-01-05 18:42:20,888: Val batch 1000: PER (avg): 0.4140 CTC Loss (avg): 41.2820 WER(1gram): 86.04% (n=64) time: 5.664
2026-01-05 18:42:20,888: WER lens: avg_true_words=6.16 avg_pred_words=5.66 max_pred_words=10
2026-01-05 18:42:20,888: t15.2023.08.13 val PER: 0.3753
2026-01-05 18:42:20,888: t15.2023.08.18 val PER: 0.3571
2026-01-05 18:42:20,889: t15.2023.08.20 val PER: 0.3606
2026-01-05 18:42:20,889: t15.2023.08.25 val PER: 0.3178
2026-01-05 18:42:20,889: t15.2023.08.27 val PER: 0.4453
2026-01-05 18:42:20,889: t15.2023.09.01 val PER: 0.3231
2026-01-05 18:42:20,889: t15.2023.09.03 val PER: 0.4026
2026-01-05 18:42:20,889: t15.2023.09.24 val PER: 0.3447
2026-01-05 18:42:20,889: t15.2023.09.29 val PER: 0.3772
2026-01-05 18:42:20,889: t15.2023.10.01 val PER: 0.4135
2026-01-05 18:42:20,889: t15.2023.10.06 val PER: 0.3391
2026-01-05 18:42:20,889: t15.2023.10.08 val PER: 0.4533
2026-01-05 18:42:20,889: t15.2023.10.13 val PER: 0.4709
2026-01-05 18:42:20,889: t15.2023.10.15 val PER: 0.3830
2026-01-05 18:42:20,889: t15.2023.10.20 val PER: 0.3523
2026-01-05 18:42:20,890: t15.2023.10.22 val PER: 0.3508
2026-01-05 18:42:20,890: t15.2023.11.03 val PER: 0.3921
2026-01-05 18:42:20,890: t15.2023.11.04 val PER: 0.1911
2026-01-05 18:42:20,890: t15.2023.11.17 val PER: 0.2675
2026-01-05 18:42:20,890: t15.2023.11.19 val PER: 0.2335
2026-01-05 18:42:20,890: t15.2023.11.26 val PER: 0.4428
2026-01-05 18:42:20,890: t15.2023.12.03 val PER: 0.3897
2026-01-05 18:42:20,890: t15.2023.12.08 val PER: 0.4128
2026-01-05 18:42:20,890: t15.2023.12.10 val PER: 0.3653
2026-01-05 18:42:20,890: t15.2023.12.17 val PER: 0.4241
2026-01-05 18:42:20,890: t15.2023.12.29 val PER: 0.4139
2026-01-05 18:42:20,890: t15.2024.02.25 val PER: 0.3525
2026-01-05 18:42:20,890: t15.2024.03.08 val PER: 0.5149
2026-01-05 18:42:20,890: t15.2024.03.15 val PER: 0.4459
2026-01-05 18:42:20,890: t15.2024.03.17 val PER: 0.4156
2026-01-05 18:42:20,890: t15.2024.05.10 val PER: 0.4413
2026-01-05 18:42:20,891: t15.2024.06.14 val PER: 0.4148
2026-01-05 18:42:20,891: t15.2024.07.19 val PER: 0.5616
2026-01-05 18:42:20,891: t15.2024.07.21 val PER: 0.3766
2026-01-05 18:42:20,891: t15.2024.07.28 val PER: 0.4007
2026-01-05 18:42:20,891: t15.2025.01.10 val PER: 0.6267
2026-01-05 18:42:20,891: t15.2025.01.12 val PER: 0.4426
2026-01-05 18:42:20,891: t15.2025.03.14 val PER: 0.6228
2026-01-05 18:42:20,891: t15.2025.03.16 val PER: 0.4777
2026-01-05 18:42:20,891: t15.2025.03.30 val PER: 0.6161
2026-01-05 18:42:20,891: t15.2025.04.13 val PER: 0.4565
2026-01-05 18:42:20,892: New best val WER(1gram) 94.67% --> 86.04%
2026-01-05 18:42:20,892: Checkpointing model
2026-01-05 18:42:22,711: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st6_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:42:22,996: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st6_wd1e-5/checkpoint/checkpoint_batch_1000
2026-01-05 18:42:34,362: Non-finite grad norm at step 1125: nan. Skipping optimizer step.
2026-01-05 18:42:36,751: Non-finite grad norm at step 1152: nan. Skipping optimizer step.
2026-01-05 18:42:41,031: Train batch 1200: loss: 35.19 grad norm: 70.92 time: 0.053
2026-01-05 18:42:49,716: Non-finite grad norm at step 1293: nan. Skipping optimizer step.
2026-01-05 18:42:59,212: Non-finite grad norm at step 1397: nan. Skipping optimizer step.
2026-01-05 18:42:59,463: Train batch 1400: loss: 36.65 grad norm: 77.04 time: 0.048
2026-01-05 18:43:06,643: Non-finite grad norm at step 1478: nan. Skipping optimizer step.
2026-01-05 18:43:08,710: Running test after training batch: 1500
2026-01-05 18:43:08,881: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:43:13,479: WER debug example
  GT : you can see the code at this point as well
  PR : ou nie e a durr at this it is li
2026-01-05 18:43:13,499: WER debug example
  GT : how does it keep the cost down
  PR : is it pu the us
2026-01-05 18:43:14,670: Val batch 1500: PER (avg): 0.3608 CTC Loss (avg): 35.3846 WER(1gram): 82.23% (n=64) time: 5.960
2026-01-05 18:43:14,670: WER lens: avg_true_words=6.16 avg_pred_words=5.52 max_pred_words=10
2026-01-05 18:43:14,671: t15.2023.08.13 val PER: 0.3337
2026-01-05 18:43:14,671: t15.2023.08.18 val PER: 0.2967
2026-01-05 18:43:14,671: t15.2023.08.20 val PER: 0.2923
2026-01-05 18:43:14,671: t15.2023.08.25 val PER: 0.2560
2026-01-05 18:43:14,672: t15.2023.08.27 val PER: 0.3891
2026-01-05 18:43:14,672: t15.2023.09.01 val PER: 0.2865
2026-01-05 18:43:14,672: t15.2023.09.03 val PER: 0.3587
2026-01-05 18:43:14,672: t15.2023.09.24 val PER: 0.2779
2026-01-05 18:43:14,672: t15.2023.09.29 val PER: 0.3299
2026-01-05 18:43:14,672: t15.2023.10.01 val PER: 0.3719
2026-01-05 18:43:14,672: t15.2023.10.06 val PER: 0.2842
2026-01-05 18:43:14,672: t15.2023.10.08 val PER: 0.4005
2026-01-05 18:43:14,672: t15.2023.10.13 val PER: 0.4065
2026-01-05 18:43:14,672: t15.2023.10.15 val PER: 0.3415
2026-01-05 18:43:14,673: t15.2023.10.20 val PER: 0.3289
2026-01-05 18:43:14,673: t15.2023.10.22 val PER: 0.3096
2026-01-05 18:43:14,673: t15.2023.11.03 val PER: 0.3474
2026-01-05 18:43:14,673: t15.2023.11.04 val PER: 0.1126
2026-01-05 18:43:14,673: t15.2023.11.17 val PER: 0.2115
2026-01-05 18:43:14,673: t15.2023.11.19 val PER: 0.1717
2026-01-05 18:43:14,673: t15.2023.11.26 val PER: 0.3703
2026-01-05 18:43:14,673: t15.2023.12.03 val PER: 0.3288
2026-01-05 18:43:14,673: t15.2023.12.08 val PER: 0.3462
2026-01-05 18:43:14,673: t15.2023.12.10 val PER: 0.2996
2026-01-05 18:43:14,673: t15.2023.12.17 val PER: 0.3576
2026-01-05 18:43:14,674: t15.2023.12.29 val PER: 0.3672
2026-01-05 18:43:14,674: t15.2024.02.25 val PER: 0.2992
2026-01-05 18:43:14,674: t15.2024.03.08 val PER: 0.4395
2026-01-05 18:43:14,674: t15.2024.03.15 val PER: 0.3865
2026-01-05 18:43:14,674: t15.2024.03.17 val PER: 0.3584
2026-01-05 18:43:14,674: t15.2024.05.10 val PER: 0.3759
2026-01-05 18:43:14,674: t15.2024.06.14 val PER: 0.3785
2026-01-05 18:43:14,674: t15.2024.07.19 val PER: 0.4970
2026-01-05 18:43:14,674: t15.2024.07.21 val PER: 0.3234
2026-01-05 18:43:14,674: t15.2024.07.28 val PER: 0.3500
2026-01-05 18:43:14,674: t15.2025.01.10 val PER: 0.5661
2026-01-05 18:43:14,674: t15.2025.01.12 val PER: 0.3911
2026-01-05 18:43:14,674: t15.2025.03.14 val PER: 0.5725
2026-01-05 18:43:14,674: t15.2025.03.16 val PER: 0.4411
2026-01-05 18:43:14,675: t15.2025.03.30 val PER: 0.5920
2026-01-05 18:43:14,675: t15.2025.04.13 val PER: 0.4479
2026-01-05 18:43:14,676: New best val WER(1gram) 86.04% --> 82.23%
2026-01-05 18:43:14,676: Checkpointing model
2026-01-05 18:43:16,420: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st6_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:43:16,723: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st6_wd1e-5/checkpoint/checkpoint_batch_1500
2026-01-05 18:43:25,680: Train batch 1600: loss: 37.14 grad norm: 73.92 time: 0.050
2026-01-05 18:43:34,096: Non-finite grad norm at step 1693: nan. Skipping optimizer step.
2026-01-05 18:43:35,248: Non-finite grad norm at step 1706: nan. Skipping optimizer step.
2026-01-05 18:43:36,068: Non-finite grad norm at step 1715: nan. Skipping optimizer step.
2026-01-05 18:43:43,772: Train batch 1800: loss: 36.73 grad norm: 73.39 time: 0.068
2026-01-05 18:44:01,998: Train batch 2000: loss: 32.08 grad norm: 64.00 time: 0.053
2026-01-05 18:44:01,998: Running test after training batch: 2000
2026-01-05 18:44:02,147: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:44:06,852: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e a could at this point is wyllie
2026-01-05 18:44:06,872: WER debug example
  GT : how does it keep the cost down
  PR : aue us it pu the kos
2026-01-05 18:44:08,210: Val batch 2000: PER (avg): 0.3213 CTC Loss (avg): 31.7477 WER(1gram): 70.30% (n=64) time: 6.211
2026-01-05 18:44:08,210: WER lens: avg_true_words=6.16 avg_pred_words=5.66 max_pred_words=11
2026-01-05 18:44:08,211: t15.2023.08.13 val PER: 0.2963
2026-01-05 18:44:08,211: t15.2023.08.18 val PER: 0.2464
2026-01-05 18:44:08,211: t15.2023.08.20 val PER: 0.2431
2026-01-05 18:44:08,211: t15.2023.08.25 val PER: 0.2184
2026-01-05 18:44:08,211: t15.2023.08.27 val PER: 0.3376
2026-01-05 18:44:08,212: t15.2023.09.01 val PER: 0.2200
2026-01-05 18:44:08,212: t15.2023.09.03 val PER: 0.3266
2026-01-05 18:44:08,213: t15.2023.09.24 val PER: 0.2439
2026-01-05 18:44:08,213: t15.2023.09.29 val PER: 0.2731
2026-01-05 18:44:08,213: t15.2023.10.01 val PER: 0.3329
2026-01-05 18:44:08,213: t15.2023.10.06 val PER: 0.2422
2026-01-05 18:44:08,213: t15.2023.10.08 val PER: 0.3748
2026-01-05 18:44:08,213: t15.2023.10.13 val PER: 0.3794
2026-01-05 18:44:08,214: t15.2023.10.15 val PER: 0.3032
2026-01-05 18:44:08,214: t15.2023.10.20 val PER: 0.2886
2026-01-05 18:44:08,214: t15.2023.10.22 val PER: 0.2572
2026-01-05 18:44:08,214: t15.2023.11.03 val PER: 0.3168
2026-01-05 18:44:08,214: t15.2023.11.04 val PER: 0.1126
2026-01-05 18:44:08,214: t15.2023.11.17 val PER: 0.1680
2026-01-05 18:44:08,214: t15.2023.11.19 val PER: 0.1617
2026-01-05 18:44:08,214: t15.2023.11.26 val PER: 0.3319
2026-01-05 18:44:08,214: t15.2023.12.03 val PER: 0.2952
2026-01-05 18:44:08,214: t15.2023.12.08 val PER: 0.2976
2026-01-05 18:44:08,214: t15.2023.12.10 val PER: 0.2707
2026-01-05 18:44:08,214: t15.2023.12.17 val PER: 0.3077
2026-01-05 18:44:08,215: t15.2023.12.29 val PER: 0.3150
2026-01-05 18:44:08,215: t15.2024.02.25 val PER: 0.2640
2026-01-05 18:44:08,215: t15.2024.03.08 val PER: 0.3912
2026-01-05 18:44:08,215: t15.2024.03.15 val PER: 0.3527
2026-01-05 18:44:08,215: t15.2024.03.17 val PER: 0.3333
2026-01-05 18:44:08,215: t15.2024.05.10 val PER: 0.3269
2026-01-05 18:44:08,215: t15.2024.06.14 val PER: 0.3391
2026-01-05 18:44:08,215: t15.2024.07.19 val PER: 0.4581
2026-01-05 18:44:08,215: t15.2024.07.21 val PER: 0.2979
2026-01-05 18:44:08,215: t15.2024.07.28 val PER: 0.3176
2026-01-05 18:44:08,215: t15.2025.01.10 val PER: 0.5152
2026-01-05 18:44:08,216: t15.2025.01.12 val PER: 0.3641
2026-01-05 18:44:08,216: t15.2025.03.14 val PER: 0.5192
2026-01-05 18:44:08,216: t15.2025.03.16 val PER: 0.4097
2026-01-05 18:44:08,216: t15.2025.03.30 val PER: 0.5540
2026-01-05 18:44:08,216: t15.2025.04.13 val PER: 0.4037
2026-01-05 18:44:08,216: New best val WER(1gram) 82.23% --> 70.30%
2026-01-05 18:44:08,216: Checkpointing model
2026-01-05 18:44:09,927: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st6_wd1e-5/checkpoint/best_checkpoint
2026-01-05 18:44:10,207: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st6_wd1e-5/checkpoint/checkpoint_batch_2000
2026-01-05 18:44:11,875: Non-finite grad norm at step 2019: nan. Skipping optimizer step.
2026-01-05 18:44:13,954: Non-finite grad norm at step 2042: nan. Skipping optimizer step.
2026-01-05 18:44:24,637: Non-finite grad norm at step 2159: nan. Skipping optimizer step.
2026-01-05 18:44:28,386: Train batch 2200: loss: 30.15 grad norm: 70.85 time: 0.048
2026-01-05 18:44:46,779: Train batch 2400: loss: 28.31 grad norm: 61.32 time: 0.042
2026-01-05 18:44:54,469: Non-finite grad norm at step 2483: nan. Skipping optimizer step.
2026-01-05 18:44:55,973: Running test after training batch: 2500
2026-01-05 18:44:56,078: WER debug GT example: You can see the code at this point as well.
2026-01-05 18:45:00,538: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-05 18:45:00,554: WER debug example
  GT : how does it keep the cost down
  PR : aue denz it keep the cost
2026-01-05 18:45:01,675: Val batch 2500: PER (avg): 0.2989 CTC Loss (avg): 29.0227 WER(1gram): 65.99% (n=64) time: 5.702
2026-01-05 18:45:01,675: WER lens: avg_true_words=6.16 avg_pred_words=5.56 max_pred_words=10
2026-01-05 18:45:01,675: t15.2023.08.13 val PER: 0.2734
2026-01-05 18:45:01,676: t15.2023.08.18 val PER: 0.2330
2026-01-05 18:45:01,676: t15.2023.08.20 val PER: 0.2319
2026-01-05 18:45:01,676: t15.2023.08.25 val PER: 0.2018
2026-01-05 18:45:01,676: t15.2023.08.27 val PER: 0.3264
2026-01-05 18:45:01,676: t15.2023.09.01 val PER: 0.2013
2026-01-05 18:45:01,676: t15.2023.09.03 val PER: 0.2957
2026-01-05 18:45:01,676: t15.2023.09.24 val PER: 0.2379
2026-01-05 18:45:01,676: t15.2023.09.29 val PER: 0.2419
2026-01-05 18:45:01,676: t15.2023.10.01 val PER: 0.3025
2026-01-05 18:45:01,676: t15.2023.10.06 val PER: 0.2185
2026-01-05 18:45:01,676: t15.2023.10.08 val PER: 0.3613
2026-01-05 18:45:01,676: t15.2023.10.13 val PER: 0.3545
2026-01-05 18:45:01,676: t15.2023.10.15 val PER: 0.2755
2026-01-05 18:45:01,676: t15.2023.10.20 val PER: 0.2852
2026-01-05 18:45:01,677: t15.2023.10.22 val PER: 0.2472
2026-01-05 18:45:01,677: t15.2023.11.03 val PER: 0.3012
2026-01-05 18:45:01,677: t15.2023.11.04 val PER: 0.0922
2026-01-05 18:45:01,677: t15.2023.11.17 val PER: 0.1571
2026-01-05 18:45:01,677: t15.2023.11.19 val PER: 0.1497
2026-01-05 18:45:01,677: t15.2023.11.26 val PER: 0.3246
2026-01-05 18:45:01,677: t15.2023.12.03 val PER: 0.2836
2026-01-05 18:45:01,677: t15.2023.12.08 val PER: 0.2736
2026-01-05 18:45:01,677: t15.2023.12.10 val PER: 0.2378
2026-01-05 18:45:01,677: t15.2023.12.17 val PER: 0.2869
2026-01-05 18:45:01,677: t15.2023.12.29 val PER: 0.3020
2026-01-05 18:45:01,677: t15.2024.02.25 val PER: 0.2486
2026-01-05 18:45:01,677: t15.2024.03.08 val PER: 0.3684
2026-01-05 18:45:01,677: t15.2024.03.15 val PER: 0.3296
2026-01-05 18:45:01,677: t15.2024.03.17 val PER: 0.2992
2026-01-05 18:45:01,677: t15.2024.05.10 val PER: 0.2808
2026-01-05 18:45:01,678: t15.2024.06.14 val PER: 0.3139
2026-01-05 18:45:01,678: t15.2024.07.19 val PER: 0.4192
2026-01-05 18:45:01,678: t15.2024.07.21 val PER: 0.2572
2026-01-05 18:45:01,678: t15.2024.07.28 val PER: 0.2934
2026-01-05 18:45:01,678: t15.2025.01.10 val PER: 0.4904
2026-01-05 18:45:01,678: t15.2025.01.12 val PER: 0.3464
2026-01-05 18:45:01,678: t15.2025.03.14 val PER: 0.4837
2026-01-05 18:45:01,678: t15.2025.03.16 val PER: 0.3691
2026-01-05 18:45:01,678: t15.2025.03.30 val PER: 0.5218
2026-01-05 18:45:01,678: t15.2025.04.13 val PER: 0.4108
2026-01-05 18:45:01,679: New best val WER(1gram) 70.30% --> 65.99%
2026-01-05 18:45:01,679: Checkpointing model
2026-01-05 18:45:01,976: Saved model to checkpoint: /tmp/e12511253_b2t_349293/trained_models/patch_stride/lr40_wd1e-5/st6_wd1e-5/checkpoint/best_checkpoint
[1;34mwandb[0m: 
[1;34mwandb[0m:  View run [33mst6_wd1e-5[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../tmp/e12511253_b2t_349293/wandb/wandb/run-20260105_184003-l59cbepk/logs[0m
