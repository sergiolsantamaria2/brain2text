[2026-01-10T16:29:58.183] error: TMPDIR [/tmp] is not writeable
[2026-01-10T16:29:58.183] error: Setting TMPDIR to /tmp
/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/utils/cpp_extension.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging  # type: ignore[attr-defined]
wandb: Currently logged in as: sergiolsantamaria (sergiolsantamaria-tu-wien) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run he3ndat1
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in /home/e12511253/tmp/e12511253_b2t_352822/wandb/wandb/run-20260110_163007-he3ndat1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run diphone_base
wandb: â­ï¸ View project at https://wandb.ai/sergiolsantamaria-tu-wien/brain2text
wandb: ðŸš€ View run at https://wandb.ai/sergiolsantamaria-tu-wien/brain2text/runs/he3ndat1
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0110 16:30:08.752811 345239 brain_speech_decoder.h:52] Reading fst /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel/TLG.fst
I0110 16:31:28.836649 345239 brain_speech_decoder.h:58] Reading lm fst /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel/TLG.fst
I0110 16:33:48.346573 345239 brain_speech_decoder.h:81] Reading symbol table /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel/words.txt
/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Traceback (most recent call last):
  File "<stdin>", line 19, in <module>
  File "/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/runpy.py", line 228, in run_module
    return _run_code(code, {}, init_globals, run_name, mod_spec)
  File "/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/e12511253/Brain2Text/brain2text/src/brain2text/model_training/train_model.py", line 98, in <module>
    main()
  File "/home/e12511253/Brain2Text/brain2text/src/brain2text/model_training/train_model.py", line 91, in main
    trainer.train()
  File "/home/e12511253/Brain2Text/brain2text/src/brain2text/model_training/rnn_trainer.py", line 995, in train
    val_metrics = self.validation(loader = self.val_loader, return_logits = self.args['save_val_logits'], return_data = self.args['save_val_data'])
  File "/home/e12511253/Brain2Text/brain2text/src/brain2text/model_training/rnn_trainer.py", line 1306, in validation
    metrics['losses'].append(loss.detach().item())
ValueError: only one element tensors can be converted to Python scalars
