TMPDIR=/home/e12511253/tmp
JOB_TMP=/home/e12511253/tmp/e12511253_b2t_352233
TORCH_EXTENSIONS_DIR=/home/e12511253/tmp/e12511253_b2t_352233/torch_extensions
WANDB_DIR=/home/e12511253/tmp/e12511253_b2t_352233/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/home/e12511253/tmp/e12511253_b2t_352233/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan 10 06:40 /home/e12511253/tmp/e12511253_b2t_352233/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
OUT_ROOT=/home/e12511253/Brain2Text/brain2text/trained_models
==============================================
Job: b2t5g  ID: 352233
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_5gram_only.yaml
Folders: configs/experiments/gru/ablations/lr_grid
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/gru/ablations/lr_grid ==========
Num configs: 2

=== RUN lr_0035.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035
2026-01-10 06:40:59,568: Using device: cuda:0
2026-01-10 06:44:47,526: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-10 06:44:57,456: Using 45 sessions after filtering (from 45).
2026-01-10 06:44:57,994: Using torch.compile (if available)
2026-01-10 06:44:57,994: torch.compile not available (torch<2.0). Skipping.
2026-01-10 06:44:57,995: Initialized RNN decoding model
2026-01-10 06:44:57,995: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-10 06:44:57,995: Model has 44,907,305 parameters
2026-01-10 06:44:57,995: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-10 06:45:03,649: Successfully initialized datasets
2026-01-10 06:45:03,650: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-10 06:45:06,305: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.810
2026-01-10 06:45:06,306: Running test after training batch: 0
2026-01-10 06:45:06,585: WER debug GT example: You can see the code at this point as well.
2026-01-10 06:45:12,833: WER debug example
  GT : you can see the code at this point as well
  PR : she has from his
2026-01-10 06:45:13,959: WER debug example
  GT : how does it keep the cost down
  PR : money from
2026-01-10 06:49:02,016: Val batch 0: PER (avg): 1.4301 CTC Loss (avg): 633.2735 WER(5gram): 99.67% (n=256) time: 235.710
2026-01-10 06:49:02,023: WER lens: avg_true_words=5.99 avg_pred_words=2.81 max_pred_words=7
2026-01-10 06:49:02,028: t15.2023.08.13 val PER: 1.3098
2026-01-10 06:49:02,031: t15.2023.08.18 val PER: 1.4267
2026-01-10 06:49:02,032: t15.2023.08.20 val PER: 1.2994
2026-01-10 06:49:02,033: t15.2023.08.25 val PER: 1.3389
2026-01-10 06:49:02,033: t15.2023.08.27 val PER: 1.2572
2026-01-10 06:49:02,033: t15.2023.09.01 val PER: 1.4513
2026-01-10 06:49:02,033: t15.2023.09.03 val PER: 1.3147
2026-01-10 06:49:02,033: t15.2023.09.24 val PER: 1.5461
2026-01-10 06:49:02,033: t15.2023.09.29 val PER: 1.4678
2026-01-10 06:49:02,033: t15.2023.10.01 val PER: 1.2127
2026-01-10 06:49:02,033: t15.2023.10.06 val PER: 1.4898
2026-01-10 06:49:02,033: t15.2023.10.08 val PER: 1.1800
2026-01-10 06:49:02,033: t15.2023.10.13 val PER: 1.3980
2026-01-10 06:49:02,033: t15.2023.10.15 val PER: 1.3929
2026-01-10 06:49:02,033: t15.2023.10.20 val PER: 1.4966
2026-01-10 06:49:02,034: t15.2023.10.22 val PER: 1.3886
2026-01-10 06:49:02,034: t15.2023.11.03 val PER: 1.5957
2026-01-10 06:49:02,034: t15.2023.11.04 val PER: 2.0205
2026-01-10 06:49:02,034: t15.2023.11.17 val PER: 1.9580
2026-01-10 06:49:02,034: t15.2023.11.19 val PER: 1.6786
2026-01-10 06:49:02,034: t15.2023.11.26 val PER: 1.5384
2026-01-10 06:49:02,034: t15.2023.12.03 val PER: 1.4244
2026-01-10 06:49:02,034: t15.2023.12.08 val PER: 1.4514
2026-01-10 06:49:02,034: t15.2023.12.10 val PER: 1.7057
2026-01-10 06:49:02,034: t15.2023.12.17 val PER: 1.3056
2026-01-10 06:49:02,034: t15.2023.12.29 val PER: 1.4063
2026-01-10 06:49:02,034: t15.2024.02.25 val PER: 1.4284
2026-01-10 06:49:02,034: t15.2024.03.08 val PER: 1.3257
2026-01-10 06:49:02,034: t15.2024.03.15 val PER: 1.3177
2026-01-10 06:49:02,035: t15.2024.03.17 val PER: 1.4024
2026-01-10 06:49:02,035: t15.2024.05.10 val PER: 1.3210
2026-01-10 06:49:02,035: t15.2024.06.14 val PER: 1.5347
2026-01-10 06:49:02,035: t15.2024.07.19 val PER: 1.0877
2026-01-10 06:49:02,035: t15.2024.07.21 val PER: 1.6331
2026-01-10 06:49:02,035: t15.2024.07.28 val PER: 1.6537
2026-01-10 06:49:02,036: t15.2025.01.10 val PER: 1.0909
2026-01-10 06:49:02,036: t15.2025.01.12 val PER: 1.7637
2026-01-10 06:49:02,036: t15.2025.03.14 val PER: 1.0355
2026-01-10 06:49:02,036: t15.2025.03.16 val PER: 1.6257
2026-01-10 06:49:02,036: t15.2025.03.30 val PER: 1.2828
2026-01-10 06:49:02,036: t15.2025.04.13 val PER: 1.5877
2026-01-10 06:49:02,037: New best val WER(5gram) inf% --> 99.67%
2026-01-10 06:49:02,219: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_0
2026-01-10 06:49:20,377: Train batch 200: loss: 79.00 grad norm: 88.75 time: 0.054
2026-01-10 06:49:38,350: Train batch 400: loss: 55.64 grad norm: 93.70 time: 0.063
2026-01-10 06:49:47,284: Running test after training batch: 500
2026-01-10 06:49:47,467: WER debug GT example: You can see the code at this point as well.
2026-01-10 06:49:52,898: WER debug example
  GT : you can see the code at this point as well
  PR : you'll need is the ease and at this ride is all
2026-01-10 06:49:53,044: WER debug example
  GT : how does it keep the cost down
  PR : and does it do this is as
2026-01-10 06:50:43,486: Val batch 500: PER (avg): 0.5258 CTC Loss (avg): 56.8785 WER(5gram): 77.05% (n=256) time: 56.202
2026-01-10 06:50:43,487: WER lens: avg_true_words=5.99 avg_pred_words=5.76 max_pred_words=12
2026-01-10 06:50:43,487: t15.2023.08.13 val PER: 0.4751
2026-01-10 06:50:43,487: t15.2023.08.18 val PER: 0.4652
2026-01-10 06:50:43,487: t15.2023.08.20 val PER: 0.4535
2026-01-10 06:50:43,488: t15.2023.08.25 val PER: 0.4352
2026-01-10 06:50:43,488: t15.2023.08.27 val PER: 0.5354
2026-01-10 06:50:43,488: t15.2023.09.01 val PER: 0.4269
2026-01-10 06:50:43,488: t15.2023.09.03 val PER: 0.5012
2026-01-10 06:50:43,488: t15.2023.09.24 val PER: 0.4539
2026-01-10 06:50:43,488: t15.2023.09.29 val PER: 0.4869
2026-01-10 06:50:43,488: t15.2023.10.01 val PER: 0.5277
2026-01-10 06:50:43,489: t15.2023.10.06 val PER: 0.4349
2026-01-10 06:50:43,489: t15.2023.10.08 val PER: 0.5589
2026-01-10 06:50:43,489: t15.2023.10.13 val PER: 0.5764
2026-01-10 06:50:43,489: t15.2023.10.15 val PER: 0.5030
2026-01-10 06:50:43,489: t15.2023.10.20 val PER: 0.4530
2026-01-10 06:50:43,489: t15.2023.10.22 val PER: 0.4555
2026-01-10 06:50:43,489: t15.2023.11.03 val PER: 0.5088
2026-01-10 06:50:43,489: t15.2023.11.04 val PER: 0.3003
2026-01-10 06:50:43,489: t15.2023.11.17 val PER: 0.3717
2026-01-10 06:50:43,489: t15.2023.11.19 val PER: 0.3573
2026-01-10 06:50:43,489: t15.2023.11.26 val PER: 0.5536
2026-01-10 06:50:43,490: t15.2023.12.03 val PER: 0.5032
2026-01-10 06:50:43,490: t15.2023.12.08 val PER: 0.5293
2026-01-10 06:50:43,490: t15.2023.12.10 val PER: 0.4586
2026-01-10 06:50:43,490: t15.2023.12.17 val PER: 0.5707
2026-01-10 06:50:43,490: t15.2023.12.29 val PER: 0.5607
2026-01-10 06:50:43,490: t15.2024.02.25 val PER: 0.4958
2026-01-10 06:50:43,490: t15.2024.03.08 val PER: 0.6159
2026-01-10 06:50:43,490: t15.2024.03.15 val PER: 0.5622
2026-01-10 06:50:43,490: t15.2024.03.17 val PER: 0.5167
2026-01-10 06:50:43,490: t15.2024.05.10 val PER: 0.5305
2026-01-10 06:50:43,490: t15.2024.06.14 val PER: 0.5300
2026-01-10 06:50:43,491: t15.2024.07.19 val PER: 0.6777
2026-01-10 06:50:43,491: t15.2024.07.21 val PER: 0.4772
2026-01-10 06:50:43,491: t15.2024.07.28 val PER: 0.5279
2026-01-10 06:50:43,491: t15.2025.01.10 val PER: 0.7314
2026-01-10 06:50:43,491: t15.2025.01.12 val PER: 0.5574
2026-01-10 06:50:43,491: t15.2025.03.14 val PER: 0.7426
2026-01-10 06:50:43,491: t15.2025.03.16 val PER: 0.5969
2026-01-10 06:50:43,491: t15.2025.03.30 val PER: 0.7195
2026-01-10 06:50:43,491: t15.2025.04.13 val PER: 0.5763
2026-01-10 06:50:43,492: New best val WER(5gram) 99.67% --> 77.05%
2026-01-10 06:50:43,670: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_500
2026-01-10 06:50:53,636: Train batch 600: loss: 51.07 grad norm: 83.27 time: 0.078
2026-01-10 06:51:11,944: Train batch 800: loss: 42.73 grad norm: 91.46 time: 0.057
2026-01-10 06:51:30,433: Train batch 1000: loss: 44.12 grad norm: 79.69 time: 0.066
2026-01-10 06:51:30,434: Running test after training batch: 1000
2026-01-10 06:51:30,561: WER debug GT example: You can see the code at this point as well.
2026-01-10 06:51:35,724: WER debug example
  GT : you can see the code at this point as well
  PR : you'd get is that good at it and is well
2026-01-10 06:51:35,841: WER debug example
  GT : how does it keep the cost down
  PR : and as it is that it's not
2026-01-10 06:52:11,041: Val batch 1000: PER (avg): 0.4244 CTC Loss (avg): 43.8068 WER(5gram): 57.69% (n=256) time: 40.607
2026-01-10 06:52:11,042: WER lens: avg_true_words=5.99 avg_pred_words=5.58 max_pred_words=12
2026-01-10 06:52:11,042: t15.2023.08.13 val PER: 0.3909
2026-01-10 06:52:11,042: t15.2023.08.18 val PER: 0.3638
2026-01-10 06:52:11,042: t15.2023.08.20 val PER: 0.3614
2026-01-10 06:52:11,042: t15.2023.08.25 val PER: 0.3072
2026-01-10 06:52:11,042: t15.2023.08.27 val PER: 0.4293
2026-01-10 06:52:11,042: t15.2023.09.01 val PER: 0.3174
2026-01-10 06:52:11,043: t15.2023.09.03 val PER: 0.4157
2026-01-10 06:52:11,043: t15.2023.09.24 val PER: 0.3568
2026-01-10 06:52:11,043: t15.2023.09.29 val PER: 0.3867
2026-01-10 06:52:11,043: t15.2023.10.01 val PER: 0.4280
2026-01-10 06:52:11,043: t15.2023.10.06 val PER: 0.3305
2026-01-10 06:52:11,043: t15.2023.10.08 val PER: 0.4709
2026-01-10 06:52:11,043: t15.2023.10.13 val PER: 0.4779
2026-01-10 06:52:11,043: t15.2023.10.15 val PER: 0.3942
2026-01-10 06:52:11,043: t15.2023.10.20 val PER: 0.3926
2026-01-10 06:52:11,043: t15.2023.10.22 val PER: 0.3608
2026-01-10 06:52:11,043: t15.2023.11.03 val PER: 0.4050
2026-01-10 06:52:11,043: t15.2023.11.04 val PER: 0.1706
2026-01-10 06:52:11,043: t15.2023.11.17 val PER: 0.2815
2026-01-10 06:52:11,043: t15.2023.11.19 val PER: 0.2176
2026-01-10 06:52:11,043: t15.2023.11.26 val PER: 0.4652
2026-01-10 06:52:11,044: t15.2023.12.03 val PER: 0.4296
2026-01-10 06:52:11,044: t15.2023.12.08 val PER: 0.4174
2026-01-10 06:52:11,044: t15.2023.12.10 val PER: 0.3679
2026-01-10 06:52:11,044: t15.2023.12.17 val PER: 0.4345
2026-01-10 06:52:11,044: t15.2023.12.29 val PER: 0.4070
2026-01-10 06:52:11,044: t15.2024.02.25 val PER: 0.3736
2026-01-10 06:52:11,045: t15.2024.03.08 val PER: 0.5178
2026-01-10 06:52:11,045: t15.2024.03.15 val PER: 0.4559
2026-01-10 06:52:11,045: t15.2024.03.17 val PER: 0.4191
2026-01-10 06:52:11,045: t15.2024.05.10 val PER: 0.4502
2026-01-10 06:52:11,045: t15.2024.06.14 val PER: 0.4196
2026-01-10 06:52:11,045: t15.2024.07.19 val PER: 0.5517
2026-01-10 06:52:11,045: t15.2024.07.21 val PER: 0.3917
2026-01-10 06:52:11,045: t15.2024.07.28 val PER: 0.4346
2026-01-10 06:52:11,045: t15.2025.01.10 val PER: 0.6322
2026-01-10 06:52:11,045: t15.2025.01.12 val PER: 0.4634
2026-01-10 06:52:11,045: t15.2025.03.14 val PER: 0.6331
2026-01-10 06:52:11,045: t15.2025.03.16 val PER: 0.4908
2026-01-10 06:52:11,045: t15.2025.03.30 val PER: 0.6701
2026-01-10 06:52:11,045: t15.2025.04.13 val PER: 0.5021
2026-01-10 06:52:11,046: New best val WER(5gram) 77.05% --> 57.69%
2026-01-10 06:52:11,237: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_1000
2026-01-10 06:52:30,310: Train batch 1200: loss: 35.14 grad norm: 81.06 time: 0.068
2026-01-10 06:52:49,653: Train batch 1400: loss: 37.91 grad norm: 83.60 time: 0.062
2026-01-10 06:52:59,247: Running test after training batch: 1500
2026-01-10 06:52:59,361: WER debug GT example: You can see the code at this point as well.
2026-01-10 06:53:04,709: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-10 06:53:04,823: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us in
2026-01-10 06:53:29,116: Val batch 1500: PER (avg): 0.3909 CTC Loss (avg): 38.3773 WER(5gram): 39.31% (n=256) time: 29.869
2026-01-10 06:53:29,117: WER lens: avg_true_words=5.99 avg_pred_words=5.25 max_pred_words=12
2026-01-10 06:53:29,117: t15.2023.08.13 val PER: 0.3597
2026-01-10 06:53:29,117: t15.2023.08.18 val PER: 0.3236
2026-01-10 06:53:29,117: t15.2023.08.20 val PER: 0.3201
2026-01-10 06:53:29,117: t15.2023.08.25 val PER: 0.2831
2026-01-10 06:53:29,117: t15.2023.08.27 val PER: 0.4164
2026-01-10 06:53:29,118: t15.2023.09.01 val PER: 0.2906
2026-01-10 06:53:29,118: t15.2023.09.03 val PER: 0.3943
2026-01-10 06:53:29,118: t15.2023.09.24 val PER: 0.3119
2026-01-10 06:53:29,118: t15.2023.09.29 val PER: 0.3440
2026-01-10 06:53:29,118: t15.2023.10.01 val PER: 0.4069
2026-01-10 06:53:29,118: t15.2023.10.06 val PER: 0.2917
2026-01-10 06:53:29,118: t15.2023.10.08 val PER: 0.4506
2026-01-10 06:53:29,118: t15.2023.10.13 val PER: 0.4461
2026-01-10 06:53:29,118: t15.2023.10.15 val PER: 0.3738
2026-01-10 06:53:29,118: t15.2023.10.20 val PER: 0.3523
2026-01-10 06:53:29,118: t15.2023.10.22 val PER: 0.3218
2026-01-10 06:53:29,119: t15.2023.11.03 val PER: 0.3765
2026-01-10 06:53:29,119: t15.2023.11.04 val PER: 0.1297
2026-01-10 06:53:29,119: t15.2023.11.17 val PER: 0.2317
2026-01-10 06:53:29,119: t15.2023.11.19 val PER: 0.1796
2026-01-10 06:53:29,119: t15.2023.11.26 val PER: 0.4232
2026-01-10 06:53:29,119: t15.2023.12.03 val PER: 0.3782
2026-01-10 06:53:29,119: t15.2023.12.08 val PER: 0.3622
2026-01-10 06:53:29,119: t15.2023.12.10 val PER: 0.3075
2026-01-10 06:53:29,119: t15.2023.12.17 val PER: 0.3711
2026-01-10 06:53:29,119: t15.2023.12.29 val PER: 0.3830
2026-01-10 06:53:29,119: t15.2024.02.25 val PER: 0.2992
2026-01-10 06:53:29,119: t15.2024.03.08 val PER: 0.4708
2026-01-10 06:53:29,119: t15.2024.03.15 val PER: 0.4259
2026-01-10 06:53:29,119: t15.2024.03.17 val PER: 0.3773
2026-01-10 06:53:29,119: t15.2024.05.10 val PER: 0.4086
2026-01-10 06:53:29,119: t15.2024.06.14 val PER: 0.4117
2026-01-10 06:53:29,119: t15.2024.07.19 val PER: 0.5452
2026-01-10 06:53:29,120: t15.2024.07.21 val PER: 0.3559
2026-01-10 06:53:29,120: t15.2024.07.28 val PER: 0.3765
2026-01-10 06:53:29,120: t15.2025.01.10 val PER: 0.6267
2026-01-10 06:53:29,120: t15.2025.01.12 val PER: 0.4419
2026-01-10 06:53:29,120: t15.2025.03.14 val PER: 0.6376
2026-01-10 06:53:29,120: t15.2025.03.16 val PER: 0.4764
2026-01-10 06:53:29,120: t15.2025.03.30 val PER: 0.6540
2026-01-10 06:53:29,120: t15.2025.04.13 val PER: 0.4822
2026-01-10 06:53:29,123: New best val WER(5gram) 57.69% --> 39.31%
2026-01-10 06:53:29,318: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_1500
2026-01-10 06:53:38,541: Train batch 1600: loss: 37.71 grad norm: 79.54 time: 0.064
2026-01-10 06:53:57,287: Train batch 1800: loss: 36.52 grad norm: 71.06 time: 0.091
2026-01-10 06:54:16,224: Train batch 2000: loss: 34.92 grad norm: 73.51 time: 0.068
2026-01-10 06:54:16,225: Running test after training batch: 2000
2026-01-10 06:54:16,382: WER debug GT example: You can see the code at this point as well.
2026-01-10 06:54:21,862: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-10 06:54:21,948: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us in
2026-01-10 06:54:44,899: Val batch 2000: PER (avg): 0.3397 CTC Loss (avg): 34.0085 WER(5gram): 29.60% (n=256) time: 28.674
2026-01-10 06:54:44,900: WER lens: avg_true_words=5.99 avg_pred_words=5.72 max_pred_words=12
2026-01-10 06:54:44,900: t15.2023.08.13 val PER: 0.3274
2026-01-10 06:54:44,900: t15.2023.08.18 val PER: 0.2640
2026-01-10 06:54:44,900: t15.2023.08.20 val PER: 0.2597
2026-01-10 06:54:44,900: t15.2023.08.25 val PER: 0.2395
2026-01-10 06:54:44,900: t15.2023.08.27 val PER: 0.3553
2026-01-10 06:54:44,900: t15.2023.09.01 val PER: 0.2403
2026-01-10 06:54:44,901: t15.2023.09.03 val PER: 0.3385
2026-01-10 06:54:44,901: t15.2023.09.24 val PER: 0.2658
2026-01-10 06:54:44,901: t15.2023.09.29 val PER: 0.2808
2026-01-10 06:54:44,901: t15.2023.10.01 val PER: 0.3441
2026-01-10 06:54:44,901: t15.2023.10.06 val PER: 0.2411
2026-01-10 06:54:44,901: t15.2023.10.08 val PER: 0.4087
2026-01-10 06:54:44,901: t15.2023.10.13 val PER: 0.3794
2026-01-10 06:54:44,901: t15.2023.10.15 val PER: 0.3131
2026-01-10 06:54:44,901: t15.2023.10.20 val PER: 0.2919
2026-01-10 06:54:44,901: t15.2023.10.22 val PER: 0.2706
2026-01-10 06:54:44,901: t15.2023.11.03 val PER: 0.3243
2026-01-10 06:54:44,901: t15.2023.11.04 val PER: 0.1024
2026-01-10 06:54:44,902: t15.2023.11.17 val PER: 0.1882
2026-01-10 06:54:44,902: t15.2023.11.19 val PER: 0.1377
2026-01-10 06:54:44,902: t15.2023.11.26 val PER: 0.3783
2026-01-10 06:54:44,902: t15.2023.12.03 val PER: 0.3141
2026-01-10 06:54:44,902: t15.2023.12.08 val PER: 0.3182
2026-01-10 06:54:44,902: t15.2023.12.10 val PER: 0.2825
2026-01-10 06:54:44,902: t15.2023.12.17 val PER: 0.3202
2026-01-10 06:54:44,902: t15.2023.12.29 val PER: 0.3391
2026-01-10 06:54:44,902: t15.2024.02.25 val PER: 0.2851
2026-01-10 06:54:44,902: t15.2024.03.08 val PER: 0.3997
2026-01-10 06:54:44,902: t15.2024.03.15 val PER: 0.3790
2026-01-10 06:54:44,902: t15.2024.03.17 val PER: 0.3501
2026-01-10 06:54:44,902: t15.2024.05.10 val PER: 0.3655
2026-01-10 06:54:44,902: t15.2024.06.14 val PER: 0.3738
2026-01-10 06:54:44,902: t15.2024.07.19 val PER: 0.4792
2026-01-10 06:54:44,903: t15.2024.07.21 val PER: 0.3103
2026-01-10 06:54:44,903: t15.2024.07.28 val PER: 0.3412
2026-01-10 06:54:44,903: t15.2025.01.10 val PER: 0.5551
2026-01-10 06:54:44,903: t15.2025.01.12 val PER: 0.3957
2026-01-10 06:54:44,903: t15.2025.03.14 val PER: 0.5503
2026-01-10 06:54:44,903: t15.2025.03.16 val PER: 0.4188
2026-01-10 06:54:44,903: t15.2025.03.30 val PER: 0.5851
2026-01-10 06:54:44,903: t15.2025.04.13 val PER: 0.4180
2026-01-10 06:54:44,904: New best val WER(5gram) 39.31% --> 29.60%
2026-01-10 06:54:45,096: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_2000
2026-01-10 06:55:03,715: Train batch 2200: loss: 30.66 grad norm: 76.50 time: 0.060
2026-01-10 06:55:22,810: Train batch 2400: loss: 30.47 grad norm: 68.37 time: 0.053
2026-01-10 06:55:32,497: Running test after training batch: 2500
2026-01-10 06:55:32,650: WER debug GT example: You can see the code at this point as well.
2026-01-10 06:55:37,890: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-10 06:55:37,959: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-10 06:55:57,769: Val batch 2500: PER (avg): 0.3158 CTC Loss (avg): 31.3819 WER(5gram): 28.68% (n=256) time: 25.272
2026-01-10 06:55:57,770: WER lens: avg_true_words=5.99 avg_pred_words=5.78 max_pred_words=12
2026-01-10 06:55:57,770: t15.2023.08.13 val PER: 0.2921
2026-01-10 06:55:57,770: t15.2023.08.18 val PER: 0.2590
2026-01-10 06:55:57,770: t15.2023.08.20 val PER: 0.2478
2026-01-10 06:55:57,771: t15.2023.08.25 val PER: 0.2048
2026-01-10 06:55:57,771: t15.2023.08.27 val PER: 0.3280
2026-01-10 06:55:57,771: t15.2023.09.01 val PER: 0.2110
2026-01-10 06:55:57,771: t15.2023.09.03 val PER: 0.3076
2026-01-10 06:55:57,771: t15.2023.09.24 val PER: 0.2476
2026-01-10 06:55:57,771: t15.2023.09.29 val PER: 0.2674
2026-01-10 06:55:57,771: t15.2023.10.01 val PER: 0.3170
2026-01-10 06:55:57,771: t15.2023.10.06 val PER: 0.2250
2026-01-10 06:55:57,772: t15.2023.10.08 val PER: 0.3829
2026-01-10 06:55:57,772: t15.2023.10.13 val PER: 0.3615
2026-01-10 06:55:57,772: t15.2023.10.15 val PER: 0.2980
2026-01-10 06:55:57,772: t15.2023.10.20 val PER: 0.2651
2026-01-10 06:55:57,772: t15.2023.10.22 val PER: 0.2483
2026-01-10 06:55:57,772: t15.2023.11.03 val PER: 0.3012
2026-01-10 06:55:57,772: t15.2023.11.04 val PER: 0.0922
2026-01-10 06:55:57,772: t15.2023.11.17 val PER: 0.1617
2026-01-10 06:55:57,772: t15.2023.11.19 val PER: 0.1397
2026-01-10 06:55:57,773: t15.2023.11.26 val PER: 0.3674
2026-01-10 06:55:57,773: t15.2023.12.03 val PER: 0.2931
2026-01-10 06:55:57,773: t15.2023.12.08 val PER: 0.2903
2026-01-10 06:55:57,773: t15.2023.12.10 val PER: 0.2418
2026-01-10 06:55:57,773: t15.2023.12.17 val PER: 0.3025
2026-01-10 06:55:57,773: t15.2023.12.29 val PER: 0.3150
2026-01-10 06:55:57,773: t15.2024.02.25 val PER: 0.2486
2026-01-10 06:55:57,773: t15.2024.03.08 val PER: 0.3798
2026-01-10 06:55:57,773: t15.2024.03.15 val PER: 0.3558
2026-01-10 06:55:57,774: t15.2024.03.17 val PER: 0.3187
2026-01-10 06:55:57,774: t15.2024.05.10 val PER: 0.3373
2026-01-10 06:55:57,774: t15.2024.06.14 val PER: 0.3312
2026-01-10 06:55:57,774: t15.2024.07.19 val PER: 0.4535
2026-01-10 06:55:57,774: t15.2024.07.21 val PER: 0.2752
2026-01-10 06:55:57,774: t15.2024.07.28 val PER: 0.3199
2026-01-10 06:55:57,774: t15.2025.01.10 val PER: 0.5331
2026-01-10 06:55:57,774: t15.2025.01.12 val PER: 0.3695
2026-01-10 06:55:57,775: t15.2025.03.14 val PER: 0.5251
2026-01-10 06:55:57,775: t15.2025.03.16 val PER: 0.3848
2026-01-10 06:55:57,775: t15.2025.03.30 val PER: 0.5322
2026-01-10 06:55:57,775: t15.2025.04.13 val PER: 0.4080
2026-01-10 06:55:57,775: New best val WER(5gram) 29.60% --> 28.68%
2026-01-10 06:55:57,981: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_2500
2026-01-10 06:56:07,344: Train batch 2600: loss: 36.64 grad norm: 84.10 time: 0.055
2026-01-10 06:56:26,131: Train batch 2800: loss: 27.05 grad norm: 74.99 time: 0.083
2026-01-10 06:56:45,148: Train batch 3000: loss: 32.97 grad norm: 75.56 time: 0.084
2026-01-10 06:56:45,149: Running test after training batch: 3000
2026-01-10 06:56:45,262: WER debug GT example: You can see the code at this point as well.
2026-01-10 06:56:50,427: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-10 06:56:50,498: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost at
2026-01-10 06:57:09,499: Val batch 3000: PER (avg): 0.2916 CTC Loss (avg): 29.1041 WER(5gram): 25.75% (n=256) time: 24.351
2026-01-10 06:57:09,500: WER lens: avg_true_words=5.99 avg_pred_words=5.95 max_pred_words=12
2026-01-10 06:57:09,500: t15.2023.08.13 val PER: 0.2661
2026-01-10 06:57:09,500: t15.2023.08.18 val PER: 0.2255
2026-01-10 06:57:09,501: t15.2023.08.20 val PER: 0.2319
2026-01-10 06:57:09,501: t15.2023.08.25 val PER: 0.2108
2026-01-10 06:57:09,501: t15.2023.08.27 val PER: 0.2958
2026-01-10 06:57:09,501: t15.2023.09.01 val PER: 0.1948
2026-01-10 06:57:09,501: t15.2023.09.03 val PER: 0.2898
2026-01-10 06:57:09,501: t15.2023.09.24 val PER: 0.2148
2026-01-10 06:57:09,501: t15.2023.09.29 val PER: 0.2438
2026-01-10 06:57:09,501: t15.2023.10.01 val PER: 0.2979
2026-01-10 06:57:09,501: t15.2023.10.06 val PER: 0.2121
2026-01-10 06:57:09,501: t15.2023.10.08 val PER: 0.3505
2026-01-10 06:57:09,502: t15.2023.10.13 val PER: 0.3545
2026-01-10 06:57:09,502: t15.2023.10.15 val PER: 0.2795
2026-01-10 06:57:09,502: t15.2023.10.20 val PER: 0.2852
2026-01-10 06:57:09,502: t15.2023.10.22 val PER: 0.2149
2026-01-10 06:57:09,502: t15.2023.11.03 val PER: 0.2768
2026-01-10 06:57:09,502: t15.2023.11.04 val PER: 0.0853
2026-01-10 06:57:09,502: t15.2023.11.17 val PER: 0.1477
2026-01-10 06:57:09,502: t15.2023.11.19 val PER: 0.1198
2026-01-10 06:57:09,502: t15.2023.11.26 val PER: 0.3109
2026-01-10 06:57:09,503: t15.2023.12.03 val PER: 0.2647
2026-01-10 06:57:09,503: t15.2023.12.08 val PER: 0.2690
2026-01-10 06:57:09,503: t15.2023.12.10 val PER: 0.2247
2026-01-10 06:57:09,503: t15.2023.12.17 val PER: 0.2827
2026-01-10 06:57:09,503: t15.2023.12.29 val PER: 0.2917
2026-01-10 06:57:09,503: t15.2024.02.25 val PER: 0.2542
2026-01-10 06:57:09,503: t15.2024.03.08 val PER: 0.3656
2026-01-10 06:57:09,503: t15.2024.03.15 val PER: 0.3433
2026-01-10 06:57:09,503: t15.2024.03.17 val PER: 0.2950
2026-01-10 06:57:09,503: t15.2024.05.10 val PER: 0.3076
2026-01-10 06:57:09,503: t15.2024.06.14 val PER: 0.3091
2026-01-10 06:57:09,504: t15.2024.07.19 val PER: 0.4146
2026-01-10 06:57:09,504: t15.2024.07.21 val PER: 0.2476
2026-01-10 06:57:09,504: t15.2024.07.28 val PER: 0.2868
2026-01-10 06:57:09,504: t15.2025.01.10 val PER: 0.5083
2026-01-10 06:57:09,504: t15.2025.01.12 val PER: 0.3449
2026-01-10 06:57:09,504: t15.2025.03.14 val PER: 0.4630
2026-01-10 06:57:09,504: t15.2025.03.16 val PER: 0.3364
2026-01-10 06:57:09,504: t15.2025.03.30 val PER: 0.5103
2026-01-10 06:57:09,504: t15.2025.04.13 val PER: 0.3680
2026-01-10 06:57:09,505: New best val WER(5gram) 28.68% --> 25.75%
2026-01-10 06:57:09,724: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_3000
2026-01-10 06:57:28,533: Train batch 3200: loss: 28.27 grad norm: 71.05 time: 0.077
2026-01-10 06:57:47,493: Train batch 3400: loss: 19.86 grad norm: 56.56 time: 0.049
2026-01-10 06:57:57,150: Running test after training batch: 3500
2026-01-10 06:57:57,369: WER debug GT example: You can see the code at this point as well.
2026-01-10 06:58:02,492: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-10 06:58:02,557: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us get
2026-01-10 06:58:20,779: Val batch 3500: PER (avg): 0.2783 CTC Loss (avg): 27.6700 WER(5gram): 24.64% (n=256) time: 23.629
2026-01-10 06:58:20,780: WER lens: avg_true_words=5.99 avg_pred_words=6.02 max_pred_words=12
2026-01-10 06:58:20,780: t15.2023.08.13 val PER: 0.2536
2026-01-10 06:58:20,780: t15.2023.08.18 val PER: 0.2137
2026-01-10 06:58:20,780: t15.2023.08.20 val PER: 0.2184
2026-01-10 06:58:20,780: t15.2023.08.25 val PER: 0.1852
2026-01-10 06:58:20,780: t15.2023.08.27 val PER: 0.2797
2026-01-10 06:58:20,780: t15.2023.09.01 val PER: 0.1891
2026-01-10 06:58:20,780: t15.2023.09.03 val PER: 0.2779
2026-01-10 06:58:20,780: t15.2023.09.24 val PER: 0.2136
2026-01-10 06:58:20,780: t15.2023.09.29 val PER: 0.2278
2026-01-10 06:58:20,781: t15.2023.10.01 val PER: 0.2893
2026-01-10 06:58:20,781: t15.2023.10.06 val PER: 0.2131
2026-01-10 06:58:20,781: t15.2023.10.08 val PER: 0.3464
2026-01-10 06:58:20,781: t15.2023.10.13 val PER: 0.3266
2026-01-10 06:58:20,781: t15.2023.10.15 val PER: 0.2597
2026-01-10 06:58:20,781: t15.2023.10.20 val PER: 0.2450
2026-01-10 06:58:20,781: t15.2023.10.22 val PER: 0.2238
2026-01-10 06:58:20,781: t15.2023.11.03 val PER: 0.2626
2026-01-10 06:58:20,781: t15.2023.11.04 val PER: 0.0648
2026-01-10 06:58:20,781: t15.2023.11.17 val PER: 0.1275
2026-01-10 06:58:20,781: t15.2023.11.19 val PER: 0.1078
2026-01-10 06:58:20,781: t15.2023.11.26 val PER: 0.3072
2026-01-10 06:58:20,781: t15.2023.12.03 val PER: 0.2574
2026-01-10 06:58:20,781: t15.2023.12.08 val PER: 0.2530
2026-01-10 06:58:20,782: t15.2023.12.10 val PER: 0.2181
2026-01-10 06:58:20,782: t15.2023.12.17 val PER: 0.2620
2026-01-10 06:58:20,782: t15.2023.12.29 val PER: 0.2588
2026-01-10 06:58:20,782: t15.2024.02.25 val PER: 0.2346
2026-01-10 06:58:20,782: t15.2024.03.08 val PER: 0.3442
2026-01-10 06:58:20,782: t15.2024.03.15 val PER: 0.3265
2026-01-10 06:58:20,782: t15.2024.03.17 val PER: 0.2880
2026-01-10 06:58:20,782: t15.2024.05.10 val PER: 0.2749
2026-01-10 06:58:20,782: t15.2024.06.14 val PER: 0.2950
2026-01-10 06:58:20,782: t15.2024.07.19 val PER: 0.4120
2026-01-10 06:58:20,782: t15.2024.07.21 val PER: 0.2324
2026-01-10 06:58:20,782: t15.2024.07.28 val PER: 0.2912
2026-01-10 06:58:20,782: t15.2025.01.10 val PER: 0.4848
2026-01-10 06:58:20,782: t15.2025.01.12 val PER: 0.3218
2026-01-10 06:58:20,782: t15.2025.03.14 val PER: 0.4719
2026-01-10 06:58:20,782: t15.2025.03.16 val PER: 0.3455
2026-01-10 06:58:20,783: t15.2025.03.30 val PER: 0.4701
2026-01-10 06:58:20,783: t15.2025.04.13 val PER: 0.3495
2026-01-10 06:58:20,784: New best val WER(5gram) 25.75% --> 24.64%
2026-01-10 06:58:20,987: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_3500
2026-01-10 06:58:30,519: Train batch 3600: loss: 23.56 grad norm: 61.42 time: 0.069
2026-01-10 06:58:49,765: Train batch 3800: loss: 27.02 grad norm: 74.34 time: 0.066
2026-01-10 06:59:09,557: Train batch 4000: loss: 21.01 grad norm: 58.87 time: 0.057
2026-01-10 06:59:09,557: Running test after training batch: 4000
2026-01-10 06:59:09,676: WER debug GT example: You can see the code at this point as well.
2026-01-10 06:59:14,821: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-10 06:59:14,897: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-10 06:59:32,466: Val batch 4000: PER (avg): 0.2648 CTC Loss (avg): 26.1283 WER(5gram): 25.55% (n=256) time: 22.908
2026-01-10 06:59:32,466: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-10 06:59:32,466: t15.2023.08.13 val PER: 0.2349
2026-01-10 06:59:32,467: t15.2023.08.18 val PER: 0.2070
2026-01-10 06:59:32,467: t15.2023.08.20 val PER: 0.2208
2026-01-10 06:59:32,467: t15.2023.08.25 val PER: 0.1687
2026-01-10 06:59:32,467: t15.2023.08.27 val PER: 0.2942
2026-01-10 06:59:32,467: t15.2023.09.01 val PER: 0.1721
2026-01-10 06:59:32,467: t15.2023.09.03 val PER: 0.2684
2026-01-10 06:59:32,467: t15.2023.09.24 val PER: 0.2087
2026-01-10 06:59:32,467: t15.2023.09.29 val PER: 0.2183
2026-01-10 06:59:32,467: t15.2023.10.01 val PER: 0.2682
2026-01-10 06:59:32,467: t15.2023.10.06 val PER: 0.1927
2026-01-10 06:59:32,467: t15.2023.10.08 val PER: 0.3396
2026-01-10 06:59:32,467: t15.2023.10.13 val PER: 0.3134
2026-01-10 06:59:32,467: t15.2023.10.15 val PER: 0.2459
2026-01-10 06:59:32,467: t15.2023.10.20 val PER: 0.2685
2026-01-10 06:59:32,468: t15.2023.10.22 val PER: 0.2071
2026-01-10 06:59:32,468: t15.2023.11.03 val PER: 0.2571
2026-01-10 06:59:32,468: t15.2023.11.04 val PER: 0.0717
2026-01-10 06:59:32,468: t15.2023.11.17 val PER: 0.1073
2026-01-10 06:59:32,468: t15.2023.11.19 val PER: 0.0998
2026-01-10 06:59:32,468: t15.2023.11.26 val PER: 0.2775
2026-01-10 06:59:32,468: t15.2023.12.03 val PER: 0.2374
2026-01-10 06:59:32,468: t15.2023.12.08 val PER: 0.2377
2026-01-10 06:59:32,468: t15.2023.12.10 val PER: 0.1971
2026-01-10 06:59:32,468: t15.2023.12.17 val PER: 0.2692
2026-01-10 06:59:32,468: t15.2023.12.29 val PER: 0.2690
2026-01-10 06:59:32,468: t15.2024.02.25 val PER: 0.2289
2026-01-10 06:59:32,468: t15.2024.03.08 val PER: 0.3442
2026-01-10 06:59:32,468: t15.2024.03.15 val PER: 0.3133
2026-01-10 06:59:32,468: t15.2024.03.17 val PER: 0.2629
2026-01-10 06:59:32,468: t15.2024.05.10 val PER: 0.2838
2026-01-10 06:59:32,469: t15.2024.06.14 val PER: 0.2839
2026-01-10 06:59:32,469: t15.2024.07.19 val PER: 0.3869
2026-01-10 06:59:32,469: t15.2024.07.21 val PER: 0.2200
2026-01-10 06:59:32,469: t15.2024.07.28 val PER: 0.2574
2026-01-10 06:59:32,469: t15.2025.01.10 val PER: 0.4490
2026-01-10 06:59:32,469: t15.2025.01.12 val PER: 0.3002
2026-01-10 06:59:32,469: t15.2025.03.14 val PER: 0.4497
2026-01-10 06:59:32,469: t15.2025.03.16 val PER: 0.3154
2026-01-10 06:59:32,469: t15.2025.03.30 val PER: 0.4529
2026-01-10 06:59:32,469: t15.2025.04.13 val PER: 0.3324
2026-01-10 06:59:32,605: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_4000
2026-01-10 06:59:52,447: Train batch 4200: loss: 23.98 grad norm: 68.83 time: 0.080
2026-01-10 07:00:11,940: Train batch 4400: loss: 18.46 grad norm: 57.64 time: 0.066
2026-01-10 07:00:22,181: Running test after training batch: 4500
2026-01-10 07:00:22,327: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:00:27,430: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-10 07:00:27,505: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost at
2026-01-10 07:00:44,102: Val batch 4500: PER (avg): 0.2499 CTC Loss (avg): 24.5113 WER(5gram): 22.10% (n=256) time: 21.919
2026-01-10 07:00:44,102: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-10 07:00:44,102: t15.2023.08.13 val PER: 0.2225
2026-01-10 07:00:44,103: t15.2023.08.18 val PER: 0.2045
2026-01-10 07:00:44,103: t15.2023.08.20 val PER: 0.1946
2026-01-10 07:00:44,103: t15.2023.08.25 val PER: 0.1581
2026-01-10 07:00:44,103: t15.2023.08.27 val PER: 0.2653
2026-01-10 07:00:44,103: t15.2023.09.01 val PER: 0.1575
2026-01-10 07:00:44,103: t15.2023.09.03 val PER: 0.2494
2026-01-10 07:00:44,103: t15.2023.09.24 val PER: 0.1796
2026-01-10 07:00:44,103: t15.2023.09.29 val PER: 0.2093
2026-01-10 07:00:44,103: t15.2023.10.01 val PER: 0.2609
2026-01-10 07:00:44,103: t15.2023.10.06 val PER: 0.1690
2026-01-10 07:00:44,103: t15.2023.10.08 val PER: 0.3288
2026-01-10 07:00:44,104: t15.2023.10.13 val PER: 0.3002
2026-01-10 07:00:44,104: t15.2023.10.15 val PER: 0.2386
2026-01-10 07:00:44,104: t15.2023.10.20 val PER: 0.2114
2026-01-10 07:00:44,104: t15.2023.10.22 val PER: 0.2071
2026-01-10 07:00:44,104: t15.2023.11.03 val PER: 0.2524
2026-01-10 07:00:44,104: t15.2023.11.04 val PER: 0.0717
2026-01-10 07:00:44,104: t15.2023.11.17 val PER: 0.0995
2026-01-10 07:00:44,104: t15.2023.11.19 val PER: 0.1098
2026-01-10 07:00:44,104: t15.2023.11.26 val PER: 0.2797
2026-01-10 07:00:44,104: t15.2023.12.03 val PER: 0.2342
2026-01-10 07:00:44,104: t15.2023.12.08 val PER: 0.2230
2026-01-10 07:00:44,104: t15.2023.12.10 val PER: 0.1827
2026-01-10 07:00:44,104: t15.2023.12.17 val PER: 0.2453
2026-01-10 07:00:44,104: t15.2023.12.29 val PER: 0.2636
2026-01-10 07:00:44,104: t15.2024.02.25 val PER: 0.2107
2026-01-10 07:00:44,105: t15.2024.03.08 val PER: 0.3272
2026-01-10 07:00:44,105: t15.2024.03.15 val PER: 0.2964
2026-01-10 07:00:44,105: t15.2024.03.17 val PER: 0.2469
2026-01-10 07:00:44,105: t15.2024.05.10 val PER: 0.2704
2026-01-10 07:00:44,105: t15.2024.06.14 val PER: 0.2650
2026-01-10 07:00:44,105: t15.2024.07.19 val PER: 0.3612
2026-01-10 07:00:44,105: t15.2024.07.21 val PER: 0.1890
2026-01-10 07:00:44,105: t15.2024.07.28 val PER: 0.2382
2026-01-10 07:00:44,105: t15.2025.01.10 val PER: 0.4394
2026-01-10 07:00:44,105: t15.2025.01.12 val PER: 0.2748
2026-01-10 07:00:44,105: t15.2025.03.14 val PER: 0.4231
2026-01-10 07:00:44,105: t15.2025.03.16 val PER: 0.2945
2026-01-10 07:00:44,105: t15.2025.03.30 val PER: 0.4276
2026-01-10 07:00:44,105: t15.2025.04.13 val PER: 0.3138
2026-01-10 07:00:44,106: New best val WER(5gram) 24.64% --> 22.10%
2026-01-10 07:00:44,303: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_4500
2026-01-10 07:00:53,853: Train batch 4600: loss: 22.00 grad norm: 66.56 time: 0.065
2026-01-10 07:01:13,339: Train batch 4800: loss: 15.21 grad norm: 57.51 time: 0.065
2026-01-10 07:01:32,833: Train batch 5000: loss: 34.01 grad norm: 83.27 time: 0.064
2026-01-10 07:01:32,833: Running test after training batch: 5000
2026-01-10 07:01:32,972: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:01:38,148: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:01:38,230: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-10 07:01:54,605: Val batch 5000: PER (avg): 0.2357 CTC Loss (avg): 23.3239 WER(5gram): 22.03% (n=256) time: 21.772
2026-01-10 07:01:54,606: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-10 07:01:54,606: t15.2023.08.13 val PER: 0.2089
2026-01-10 07:01:54,606: t15.2023.08.18 val PER: 0.1752
2026-01-10 07:01:54,606: t15.2023.08.20 val PER: 0.1962
2026-01-10 07:01:54,606: t15.2023.08.25 val PER: 0.1310
2026-01-10 07:01:54,606: t15.2023.08.27 val PER: 0.2492
2026-01-10 07:01:54,606: t15.2023.09.01 val PER: 0.1429
2026-01-10 07:01:54,606: t15.2023.09.03 val PER: 0.2352
2026-01-10 07:01:54,606: t15.2023.09.24 val PER: 0.1845
2026-01-10 07:01:54,606: t15.2023.09.29 val PER: 0.1953
2026-01-10 07:01:54,606: t15.2023.10.01 val PER: 0.2404
2026-01-10 07:01:54,607: t15.2023.10.06 val PER: 0.1636
2026-01-10 07:01:54,607: t15.2023.10.08 val PER: 0.3261
2026-01-10 07:01:54,607: t15.2023.10.13 val PER: 0.2901
2026-01-10 07:01:54,607: t15.2023.10.15 val PER: 0.2248
2026-01-10 07:01:54,607: t15.2023.10.20 val PER: 0.2315
2026-01-10 07:01:54,607: t15.2023.10.22 val PER: 0.1759
2026-01-10 07:01:54,607: t15.2023.11.03 val PER: 0.2300
2026-01-10 07:01:54,607: t15.2023.11.04 val PER: 0.0717
2026-01-10 07:01:54,607: t15.2023.11.17 val PER: 0.0855
2026-01-10 07:01:54,607: t15.2023.11.19 val PER: 0.0858
2026-01-10 07:01:54,607: t15.2023.11.26 val PER: 0.2449
2026-01-10 07:01:54,607: t15.2023.12.03 val PER: 0.2080
2026-01-10 07:01:54,607: t15.2023.12.08 val PER: 0.2150
2026-01-10 07:01:54,607: t15.2023.12.10 val PER: 0.1577
2026-01-10 07:01:54,607: t15.2023.12.17 val PER: 0.2391
2026-01-10 07:01:54,608: t15.2023.12.29 val PER: 0.2457
2026-01-10 07:01:54,608: t15.2024.02.25 val PER: 0.1896
2026-01-10 07:01:54,608: t15.2024.03.08 val PER: 0.3129
2026-01-10 07:01:54,608: t15.2024.03.15 val PER: 0.2921
2026-01-10 07:01:54,608: t15.2024.03.17 val PER: 0.2490
2026-01-10 07:01:54,608: t15.2024.05.10 val PER: 0.2481
2026-01-10 07:01:54,608: t15.2024.06.14 val PER: 0.2461
2026-01-10 07:01:54,608: t15.2024.07.19 val PER: 0.3454
2026-01-10 07:01:54,608: t15.2024.07.21 val PER: 0.1855
2026-01-10 07:01:54,608: t15.2024.07.28 val PER: 0.2331
2026-01-10 07:01:54,608: t15.2025.01.10 val PER: 0.4105
2026-01-10 07:01:54,608: t15.2025.01.12 val PER: 0.2602
2026-01-10 07:01:54,609: t15.2025.03.14 val PER: 0.3935
2026-01-10 07:01:54,609: t15.2025.03.16 val PER: 0.2814
2026-01-10 07:01:54,609: t15.2025.03.30 val PER: 0.4126
2026-01-10 07:01:54,609: t15.2025.04.13 val PER: 0.3024
2026-01-10 07:01:54,610: New best val WER(5gram) 22.10% --> 22.03%
2026-01-10 07:01:54,805: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_5000
2026-01-10 07:02:13,639: Train batch 5200: loss: 18.96 grad norm: 68.00 time: 0.051
2026-01-10 07:02:32,604: Train batch 5400: loss: 19.15 grad norm: 63.17 time: 0.068
2026-01-10 07:02:42,593: Running test after training batch: 5500
2026-01-10 07:02:42,783: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:02:47,889: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-10 07:02:47,959: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-10 07:03:03,782: Val batch 5500: PER (avg): 0.2264 CTC Loss (avg): 22.1723 WER(5gram): 21.19% (n=256) time: 21.189
2026-01-10 07:03:03,783: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-10 07:03:03,783: t15.2023.08.13 val PER: 0.2017
2026-01-10 07:03:03,783: t15.2023.08.18 val PER: 0.1735
2026-01-10 07:03:03,783: t15.2023.08.20 val PER: 0.1835
2026-01-10 07:03:03,783: t15.2023.08.25 val PER: 0.1310
2026-01-10 07:03:03,783: t15.2023.08.27 val PER: 0.2556
2026-01-10 07:03:03,784: t15.2023.09.01 val PER: 0.1364
2026-01-10 07:03:03,784: t15.2023.09.03 val PER: 0.2435
2026-01-10 07:03:03,784: t15.2023.09.24 val PER: 0.1760
2026-01-10 07:03:03,784: t15.2023.09.29 val PER: 0.1876
2026-01-10 07:03:03,784: t15.2023.10.01 val PER: 0.2411
2026-01-10 07:03:03,784: t15.2023.10.06 val PER: 0.1529
2026-01-10 07:03:03,784: t15.2023.10.08 val PER: 0.3085
2026-01-10 07:03:03,784: t15.2023.10.13 val PER: 0.2863
2026-01-10 07:03:03,784: t15.2023.10.15 val PER: 0.2136
2026-01-10 07:03:03,784: t15.2023.10.20 val PER: 0.2349
2026-01-10 07:03:03,784: t15.2023.10.22 val PER: 0.1715
2026-01-10 07:03:03,784: t15.2023.11.03 val PER: 0.2313
2026-01-10 07:03:03,784: t15.2023.11.04 val PER: 0.0717
2026-01-10 07:03:03,784: t15.2023.11.17 val PER: 0.0824
2026-01-10 07:03:03,784: t15.2023.11.19 val PER: 0.0739
2026-01-10 07:03:03,784: t15.2023.11.26 val PER: 0.2333
2026-01-10 07:03:03,784: t15.2023.12.03 val PER: 0.1870
2026-01-10 07:03:03,785: t15.2023.12.08 val PER: 0.2077
2026-01-10 07:03:03,785: t15.2023.12.10 val PER: 0.1656
2026-01-10 07:03:03,785: t15.2023.12.17 val PER: 0.2308
2026-01-10 07:03:03,785: t15.2023.12.29 val PER: 0.2244
2026-01-10 07:03:03,785: t15.2024.02.25 val PER: 0.1868
2026-01-10 07:03:03,785: t15.2024.03.08 val PER: 0.3044
2026-01-10 07:03:03,785: t15.2024.03.15 val PER: 0.2695
2026-01-10 07:03:03,785: t15.2024.03.17 val PER: 0.2232
2026-01-10 07:03:03,785: t15.2024.05.10 val PER: 0.2496
2026-01-10 07:03:03,785: t15.2024.06.14 val PER: 0.2397
2026-01-10 07:03:03,785: t15.2024.07.19 val PER: 0.3237
2026-01-10 07:03:03,785: t15.2024.07.21 val PER: 0.1786
2026-01-10 07:03:03,786: t15.2024.07.28 val PER: 0.2279
2026-01-10 07:03:03,786: t15.2025.01.10 val PER: 0.4063
2026-01-10 07:03:03,786: t15.2025.01.12 val PER: 0.2502
2026-01-10 07:03:03,786: t15.2025.03.14 val PER: 0.3817
2026-01-10 07:03:03,786: t15.2025.03.16 val PER: 0.2749
2026-01-10 07:03:03,786: t15.2025.03.30 val PER: 0.3621
2026-01-10 07:03:03,786: t15.2025.04.13 val PER: 0.3010
2026-01-10 07:03:03,787: New best val WER(5gram) 22.03% --> 21.19%
2026-01-10 07:03:03,984: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_5500
2026-01-10 07:03:13,972: Train batch 5600: loss: 21.16 grad norm: 66.76 time: 0.062
2026-01-10 07:03:32,989: Train batch 5800: loss: 15.30 grad norm: 59.28 time: 0.082
2026-01-10 07:03:52,042: Train batch 6000: loss: 15.79 grad norm: 60.23 time: 0.049
2026-01-10 07:03:52,043: Running test after training batch: 6000
2026-01-10 07:03:52,175: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:03:57,282: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:03:57,360: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-10 07:04:12,731: Val batch 6000: PER (avg): 0.2213 CTC Loss (avg): 21.8526 WER(5gram): 22.82% (n=256) time: 20.688
2026-01-10 07:04:12,732: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-10 07:04:12,732: t15.2023.08.13 val PER: 0.1933
2026-01-10 07:04:12,732: t15.2023.08.18 val PER: 0.1685
2026-01-10 07:04:12,732: t15.2023.08.20 val PER: 0.1779
2026-01-10 07:04:12,733: t15.2023.08.25 val PER: 0.1235
2026-01-10 07:04:12,733: t15.2023.08.27 val PER: 0.2540
2026-01-10 07:04:12,733: t15.2023.09.01 val PER: 0.1388
2026-01-10 07:04:12,733: t15.2023.09.03 val PER: 0.2173
2026-01-10 07:04:12,733: t15.2023.09.24 val PER: 0.1796
2026-01-10 07:04:12,733: t15.2023.09.29 val PER: 0.1819
2026-01-10 07:04:12,733: t15.2023.10.01 val PER: 0.2332
2026-01-10 07:04:12,733: t15.2023.10.06 val PER: 0.1475
2026-01-10 07:04:12,733: t15.2023.10.08 val PER: 0.2923
2026-01-10 07:04:12,734: t15.2023.10.13 val PER: 0.2746
2026-01-10 07:04:12,734: t15.2023.10.15 val PER: 0.2254
2026-01-10 07:04:12,734: t15.2023.10.20 val PER: 0.2517
2026-01-10 07:04:12,734: t15.2023.10.22 val PER: 0.1826
2026-01-10 07:04:12,734: t15.2023.11.03 val PER: 0.2320
2026-01-10 07:04:12,734: t15.2023.11.04 val PER: 0.0580
2026-01-10 07:04:12,734: t15.2023.11.17 val PER: 0.0824
2026-01-10 07:04:12,734: t15.2023.11.19 val PER: 0.0818
2026-01-10 07:04:12,734: t15.2023.11.26 val PER: 0.2348
2026-01-10 07:04:12,734: t15.2023.12.03 val PER: 0.1786
2026-01-10 07:04:12,735: t15.2023.12.08 val PER: 0.1877
2026-01-10 07:04:12,735: t15.2023.12.10 val PER: 0.1669
2026-01-10 07:04:12,735: t15.2023.12.17 val PER: 0.2214
2026-01-10 07:04:12,735: t15.2023.12.29 val PER: 0.2237
2026-01-10 07:04:12,735: t15.2024.02.25 val PER: 0.1671
2026-01-10 07:04:12,735: t15.2024.03.08 val PER: 0.2930
2026-01-10 07:04:12,735: t15.2024.03.15 val PER: 0.2714
2026-01-10 07:04:12,735: t15.2024.03.17 val PER: 0.2315
2026-01-10 07:04:12,735: t15.2024.05.10 val PER: 0.2259
2026-01-10 07:04:12,735: t15.2024.06.14 val PER: 0.2319
2026-01-10 07:04:12,735: t15.2024.07.19 val PER: 0.3197
2026-01-10 07:04:12,736: t15.2024.07.21 val PER: 0.1731
2026-01-10 07:04:12,736: t15.2024.07.28 val PER: 0.2096
2026-01-10 07:04:12,736: t15.2025.01.10 val PER: 0.3857
2026-01-10 07:04:12,736: t15.2025.01.12 val PER: 0.2248
2026-01-10 07:04:12,736: t15.2025.03.14 val PER: 0.3935
2026-01-10 07:04:12,736: t15.2025.03.16 val PER: 0.2696
2026-01-10 07:04:12,736: t15.2025.03.30 val PER: 0.3897
2026-01-10 07:04:12,736: t15.2025.04.13 val PER: 0.2767
2026-01-10 07:04:12,879: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_6000
2026-01-10 07:04:31,561: Train batch 6200: loss: 17.75 grad norm: 62.38 time: 0.070
2026-01-10 07:04:50,593: Train batch 6400: loss: 20.43 grad norm: 67.63 time: 0.062
2026-01-10 07:05:00,449: Running test after training batch: 6500
2026-01-10 07:05:00,595: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:05:05,772: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:05:05,840: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-10 07:05:20,540: Val batch 6500: PER (avg): 0.2150 CTC Loss (avg): 20.9996 WER(5gram): 20.60% (n=256) time: 20.091
2026-01-10 07:05:20,541: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-10 07:05:20,541: t15.2023.08.13 val PER: 0.1892
2026-01-10 07:05:20,541: t15.2023.08.18 val PER: 0.1601
2026-01-10 07:05:20,541: t15.2023.08.20 val PER: 0.1755
2026-01-10 07:05:20,541: t15.2023.08.25 val PER: 0.1220
2026-01-10 07:05:20,541: t15.2023.08.27 val PER: 0.2476
2026-01-10 07:05:20,541: t15.2023.09.01 val PER: 0.1274
2026-01-10 07:05:20,541: t15.2023.09.03 val PER: 0.2102
2026-01-10 07:05:20,541: t15.2023.09.24 val PER: 0.1711
2026-01-10 07:05:20,542: t15.2023.09.29 val PER: 0.1844
2026-01-10 07:05:20,542: t15.2023.10.01 val PER: 0.2332
2026-01-10 07:05:20,542: t15.2023.10.06 val PER: 0.1399
2026-01-10 07:05:20,542: t15.2023.10.08 val PER: 0.3004
2026-01-10 07:05:20,542: t15.2023.10.13 val PER: 0.2754
2026-01-10 07:05:20,542: t15.2023.10.15 val PER: 0.2189
2026-01-10 07:05:20,542: t15.2023.10.20 val PER: 0.2248
2026-01-10 07:05:20,542: t15.2023.10.22 val PER: 0.1726
2026-01-10 07:05:20,542: t15.2023.11.03 val PER: 0.2212
2026-01-10 07:05:20,542: t15.2023.11.04 val PER: 0.0580
2026-01-10 07:05:20,542: t15.2023.11.17 val PER: 0.0793
2026-01-10 07:05:20,542: t15.2023.11.19 val PER: 0.0798
2026-01-10 07:05:20,542: t15.2023.11.26 val PER: 0.2210
2026-01-10 07:05:20,543: t15.2023.12.03 val PER: 0.1901
2026-01-10 07:05:20,543: t15.2023.12.08 val PER: 0.1858
2026-01-10 07:05:20,543: t15.2023.12.10 val PER: 0.1498
2026-01-10 07:05:20,543: t15.2023.12.17 val PER: 0.2037
2026-01-10 07:05:20,543: t15.2023.12.29 val PER: 0.2176
2026-01-10 07:05:20,543: t15.2024.02.25 val PER: 0.1601
2026-01-10 07:05:20,544: t15.2024.03.08 val PER: 0.2916
2026-01-10 07:05:20,544: t15.2024.03.15 val PER: 0.2645
2026-01-10 07:05:20,544: t15.2024.03.17 val PER: 0.2141
2026-01-10 07:05:20,544: t15.2024.05.10 val PER: 0.2377
2026-01-10 07:05:20,544: t15.2024.06.14 val PER: 0.2271
2026-01-10 07:05:20,544: t15.2024.07.19 val PER: 0.3098
2026-01-10 07:05:20,544: t15.2024.07.21 val PER: 0.1593
2026-01-10 07:05:20,544: t15.2024.07.28 val PER: 0.2007
2026-01-10 07:05:20,544: t15.2025.01.10 val PER: 0.3857
2026-01-10 07:05:20,544: t15.2025.01.12 val PER: 0.2248
2026-01-10 07:05:20,545: t15.2025.03.14 val PER: 0.3713
2026-01-10 07:05:20,545: t15.2025.03.16 val PER: 0.2592
2026-01-10 07:05:20,545: t15.2025.03.30 val PER: 0.3667
2026-01-10 07:05:20,545: t15.2025.04.13 val PER: 0.2853
2026-01-10 07:05:20,546: New best val WER(5gram) 21.19% --> 20.60%
2026-01-10 07:05:20,741: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_6500
2026-01-10 07:05:29,840: Train batch 6600: loss: 13.58 grad norm: 55.88 time: 0.045
2026-01-10 07:05:49,307: Train batch 6800: loss: 17.02 grad norm: 56.69 time: 0.048
2026-01-10 07:06:08,247: Train batch 7000: loss: 18.96 grad norm: 69.29 time: 0.062
2026-01-10 07:06:08,248: Running test after training batch: 7000
2026-01-10 07:06:08,408: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:06:13,812: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:06:13,875: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-10 07:06:28,188: Val batch 7000: PER (avg): 0.2055 CTC Loss (avg): 20.2576 WER(5gram): 18.84% (n=256) time: 19.940
2026-01-10 07:06:28,188: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-10 07:06:28,189: t15.2023.08.13 val PER: 0.1674
2026-01-10 07:06:28,189: t15.2023.08.18 val PER: 0.1500
2026-01-10 07:06:28,189: t15.2023.08.20 val PER: 0.1628
2026-01-10 07:06:28,189: t15.2023.08.25 val PER: 0.1114
2026-01-10 07:06:28,189: t15.2023.08.27 val PER: 0.2170
2026-01-10 07:06:28,189: t15.2023.09.01 val PER: 0.1274
2026-01-10 07:06:28,189: t15.2023.09.03 val PER: 0.2043
2026-01-10 07:06:28,189: t15.2023.09.24 val PER: 0.1578
2026-01-10 07:06:28,189: t15.2023.09.29 val PER: 0.1736
2026-01-10 07:06:28,189: t15.2023.10.01 val PER: 0.2199
2026-01-10 07:06:28,189: t15.2023.10.06 val PER: 0.1270
2026-01-10 07:06:28,189: t15.2023.10.08 val PER: 0.2936
2026-01-10 07:06:28,189: t15.2023.10.13 val PER: 0.2715
2026-01-10 07:06:28,190: t15.2023.10.15 val PER: 0.1991
2026-01-10 07:06:28,190: t15.2023.10.20 val PER: 0.2047
2026-01-10 07:06:28,190: t15.2023.10.22 val PER: 0.1537
2026-01-10 07:06:28,190: t15.2023.11.03 val PER: 0.2110
2026-01-10 07:06:28,190: t15.2023.11.04 val PER: 0.0341
2026-01-10 07:06:28,190: t15.2023.11.17 val PER: 0.0747
2026-01-10 07:06:28,190: t15.2023.11.19 val PER: 0.0579
2026-01-10 07:06:28,190: t15.2023.11.26 val PER: 0.2152
2026-01-10 07:06:28,190: t15.2023.12.03 val PER: 0.1807
2026-01-10 07:06:28,190: t15.2023.12.08 val PER: 0.1671
2026-01-10 07:06:28,190: t15.2023.12.10 val PER: 0.1498
2026-01-10 07:06:28,191: t15.2023.12.17 val PER: 0.1913
2026-01-10 07:06:28,191: t15.2023.12.29 val PER: 0.2107
2026-01-10 07:06:28,191: t15.2024.02.25 val PER: 0.1713
2026-01-10 07:06:28,191: t15.2024.03.08 val PER: 0.2831
2026-01-10 07:06:28,191: t15.2024.03.15 val PER: 0.2545
2026-01-10 07:06:28,191: t15.2024.03.17 val PER: 0.2078
2026-01-10 07:06:28,191: t15.2024.05.10 val PER: 0.2214
2026-01-10 07:06:28,191: t15.2024.06.14 val PER: 0.2287
2026-01-10 07:06:28,191: t15.2024.07.19 val PER: 0.3177
2026-01-10 07:06:28,191: t15.2024.07.21 val PER: 0.1441
2026-01-10 07:06:28,191: t15.2024.07.28 val PER: 0.1934
2026-01-10 07:06:28,191: t15.2025.01.10 val PER: 0.3747
2026-01-10 07:06:28,191: t15.2025.01.12 val PER: 0.2202
2026-01-10 07:06:28,191: t15.2025.03.14 val PER: 0.3713
2026-01-10 07:06:28,191: t15.2025.03.16 val PER: 0.2526
2026-01-10 07:06:28,192: t15.2025.03.30 val PER: 0.3598
2026-01-10 07:06:28,192: t15.2025.04.13 val PER: 0.2710
2026-01-10 07:06:28,192: New best val WER(5gram) 20.60% --> 18.84%
2026-01-10 07:06:28,385: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_7000
2026-01-10 07:06:47,528: Train batch 7200: loss: 15.97 grad norm: 61.26 time: 0.079
2026-01-10 07:07:07,697: Train batch 7400: loss: 15.46 grad norm: 59.12 time: 0.076
2026-01-10 07:07:17,235: Running test after training batch: 7500
2026-01-10 07:07:17,363: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:07:22,774: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:07:22,825: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-10 07:07:37,370: Val batch 7500: PER (avg): 0.2012 CTC Loss (avg): 19.9018 WER(5gram): 19.56% (n=256) time: 20.134
2026-01-10 07:07:37,370: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-10 07:07:37,371: t15.2023.08.13 val PER: 0.1778
2026-01-10 07:07:37,371: t15.2023.08.18 val PER: 0.1618
2026-01-10 07:07:37,371: t15.2023.08.20 val PER: 0.1604
2026-01-10 07:07:37,371: t15.2023.08.25 val PER: 0.1069
2026-01-10 07:07:37,371: t15.2023.08.27 val PER: 0.2267
2026-01-10 07:07:37,371: t15.2023.09.01 val PER: 0.1161
2026-01-10 07:07:37,371: t15.2023.09.03 val PER: 0.2055
2026-01-10 07:07:37,371: t15.2023.09.24 val PER: 0.1578
2026-01-10 07:07:37,371: t15.2023.09.29 val PER: 0.1717
2026-01-10 07:07:37,371: t15.2023.10.01 val PER: 0.2120
2026-01-10 07:07:37,371: t15.2023.10.06 val PER: 0.1270
2026-01-10 07:07:37,371: t15.2023.10.08 val PER: 0.2855
2026-01-10 07:07:37,371: t15.2023.10.13 val PER: 0.2560
2026-01-10 07:07:37,372: t15.2023.10.15 val PER: 0.1925
2026-01-10 07:07:37,372: t15.2023.10.20 val PER: 0.1946
2026-01-10 07:07:37,372: t15.2023.10.22 val PER: 0.1392
2026-01-10 07:07:37,372: t15.2023.11.03 val PER: 0.2137
2026-01-10 07:07:37,372: t15.2023.11.04 val PER: 0.0546
2026-01-10 07:07:37,372: t15.2023.11.17 val PER: 0.0653
2026-01-10 07:07:37,372: t15.2023.11.19 val PER: 0.0599
2026-01-10 07:07:37,373: t15.2023.11.26 val PER: 0.2043
2026-01-10 07:07:37,373: t15.2023.12.03 val PER: 0.1744
2026-01-10 07:07:37,373: t15.2023.12.08 val PER: 0.1651
2026-01-10 07:07:37,373: t15.2023.12.10 val PER: 0.1393
2026-01-10 07:07:37,373: t15.2023.12.17 val PER: 0.1902
2026-01-10 07:07:37,373: t15.2023.12.29 val PER: 0.2025
2026-01-10 07:07:37,373: t15.2024.02.25 val PER: 0.1503
2026-01-10 07:07:37,373: t15.2024.03.08 val PER: 0.2745
2026-01-10 07:07:37,373: t15.2024.03.15 val PER: 0.2514
2026-01-10 07:07:37,373: t15.2024.03.17 val PER: 0.1946
2026-01-10 07:07:37,373: t15.2024.05.10 val PER: 0.2125
2026-01-10 07:07:37,373: t15.2024.06.14 val PER: 0.2192
2026-01-10 07:07:37,373: t15.2024.07.19 val PER: 0.3032
2026-01-10 07:07:37,373: t15.2024.07.21 val PER: 0.1524
2026-01-10 07:07:37,373: t15.2024.07.28 val PER: 0.1846
2026-01-10 07:07:37,373: t15.2025.01.10 val PER: 0.3774
2026-01-10 07:07:37,373: t15.2025.01.12 val PER: 0.2171
2026-01-10 07:07:37,374: t15.2025.03.14 val PER: 0.3743
2026-01-10 07:07:37,374: t15.2025.03.16 val PER: 0.2500
2026-01-10 07:07:37,374: t15.2025.03.30 val PER: 0.3655
2026-01-10 07:07:37,374: t15.2025.04.13 val PER: 0.2653
2026-01-10 07:07:37,517: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_7500
2026-01-10 07:07:46,846: Train batch 7600: loss: 17.46 grad norm: 61.08 time: 0.069
2026-01-10 07:08:06,022: Train batch 7800: loss: 15.78 grad norm: 57.60 time: 0.056
2026-01-10 07:08:25,282: Train batch 8000: loss: 12.77 grad norm: 51.93 time: 0.072
2026-01-10 07:08:25,283: Running test after training batch: 8000
2026-01-10 07:08:25,393: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:08:30,511: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-10 07:08:30,568: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-10 07:08:44,848: Val batch 8000: PER (avg): 0.1943 CTC Loss (avg): 19.0792 WER(5gram): 19.10% (n=256) time: 19.565
2026-01-10 07:08:44,849: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-10 07:08:44,849: t15.2023.08.13 val PER: 0.1611
2026-01-10 07:08:44,849: t15.2023.08.18 val PER: 0.1391
2026-01-10 07:08:44,849: t15.2023.08.20 val PER: 0.1422
2026-01-10 07:08:44,849: t15.2023.08.25 val PER: 0.1039
2026-01-10 07:08:44,849: t15.2023.08.27 val PER: 0.2186
2026-01-10 07:08:44,849: t15.2023.09.01 val PER: 0.1153
2026-01-10 07:08:44,849: t15.2023.09.03 val PER: 0.2031
2026-01-10 07:08:44,850: t15.2023.09.24 val PER: 0.1614
2026-01-10 07:08:44,850: t15.2023.09.29 val PER: 0.1621
2026-01-10 07:08:44,850: t15.2023.10.01 val PER: 0.2074
2026-01-10 07:08:44,850: t15.2023.10.06 val PER: 0.1259
2026-01-10 07:08:44,850: t15.2023.10.08 val PER: 0.2801
2026-01-10 07:08:44,850: t15.2023.10.13 val PER: 0.2583
2026-01-10 07:08:44,850: t15.2023.10.15 val PER: 0.1958
2026-01-10 07:08:44,850: t15.2023.10.20 val PER: 0.2114
2026-01-10 07:08:44,850: t15.2023.10.22 val PER: 0.1537
2026-01-10 07:08:44,850: t15.2023.11.03 val PER: 0.2062
2026-01-10 07:08:44,850: t15.2023.11.04 val PER: 0.0478
2026-01-10 07:08:44,850: t15.2023.11.17 val PER: 0.0560
2026-01-10 07:08:44,850: t15.2023.11.19 val PER: 0.0619
2026-01-10 07:08:44,851: t15.2023.11.26 val PER: 0.2036
2026-01-10 07:08:44,851: t15.2023.12.03 val PER: 0.1639
2026-01-10 07:08:44,851: t15.2023.12.08 val PER: 0.1591
2026-01-10 07:08:44,851: t15.2023.12.10 val PER: 0.1327
2026-01-10 07:08:44,851: t15.2023.12.17 val PER: 0.1840
2026-01-10 07:08:44,851: t15.2023.12.29 val PER: 0.1846
2026-01-10 07:08:44,851: t15.2024.02.25 val PER: 0.1545
2026-01-10 07:08:44,851: t15.2024.03.08 val PER: 0.2674
2026-01-10 07:08:44,851: t15.2024.03.15 val PER: 0.2420
2026-01-10 07:08:44,851: t15.2024.03.17 val PER: 0.1841
2026-01-10 07:08:44,851: t15.2024.05.10 val PER: 0.2080
2026-01-10 07:08:44,851: t15.2024.06.14 val PER: 0.2098
2026-01-10 07:08:44,851: t15.2024.07.19 val PER: 0.3045
2026-01-10 07:08:44,851: t15.2024.07.21 val PER: 0.1386
2026-01-10 07:08:44,851: t15.2024.07.28 val PER: 0.1706
2026-01-10 07:08:44,851: t15.2025.01.10 val PER: 0.3499
2026-01-10 07:08:44,852: t15.2025.01.12 val PER: 0.2002
2026-01-10 07:08:44,852: t15.2025.03.14 val PER: 0.3565
2026-01-10 07:08:44,852: t15.2025.03.16 val PER: 0.2435
2026-01-10 07:08:44,852: t15.2025.03.30 val PER: 0.3690
2026-01-10 07:08:44,852: t15.2025.04.13 val PER: 0.2568
2026-01-10 07:08:44,993: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_8000
2026-01-10 07:09:03,976: Train batch 8200: loss: 11.34 grad norm: 57.72 time: 0.054
2026-01-10 07:09:22,929: Train batch 8400: loss: 11.64 grad norm: 49.32 time: 0.064
2026-01-10 07:09:32,553: Running test after training batch: 8500
2026-01-10 07:09:32,692: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:09:37,883: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:09:37,962: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-10 07:09:52,814: Val batch 8500: PER (avg): 0.1900 CTC Loss (avg): 18.6838 WER(5gram): 17.86% (n=256) time: 20.260
2026-01-10 07:09:52,814: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-10 07:09:52,815: t15.2023.08.13 val PER: 0.1549
2026-01-10 07:09:52,815: t15.2023.08.18 val PER: 0.1492
2026-01-10 07:09:52,815: t15.2023.08.20 val PER: 0.1549
2026-01-10 07:09:52,815: t15.2023.08.25 val PER: 0.1145
2026-01-10 07:09:52,815: t15.2023.08.27 val PER: 0.2074
2026-01-10 07:09:52,815: t15.2023.09.01 val PER: 0.1144
2026-01-10 07:09:52,815: t15.2023.09.03 val PER: 0.1995
2026-01-10 07:09:52,815: t15.2023.09.24 val PER: 0.1541
2026-01-10 07:09:52,815: t15.2023.09.29 val PER: 0.1595
2026-01-10 07:09:52,815: t15.2023.10.01 val PER: 0.1975
2026-01-10 07:09:52,815: t15.2023.10.06 val PER: 0.1109
2026-01-10 07:09:52,816: t15.2023.10.08 val PER: 0.2788
2026-01-10 07:09:52,816: t15.2023.10.13 val PER: 0.2452
2026-01-10 07:09:52,816: t15.2023.10.15 val PER: 0.1892
2026-01-10 07:09:52,816: t15.2023.10.20 val PER: 0.1980
2026-01-10 07:09:52,816: t15.2023.10.22 val PER: 0.1514
2026-01-10 07:09:52,816: t15.2023.11.03 val PER: 0.2022
2026-01-10 07:09:52,816: t15.2023.11.04 val PER: 0.0546
2026-01-10 07:09:52,816: t15.2023.11.17 val PER: 0.0684
2026-01-10 07:09:52,816: t15.2023.11.19 val PER: 0.0519
2026-01-10 07:09:52,816: t15.2023.11.26 val PER: 0.1928
2026-01-10 07:09:52,816: t15.2023.12.03 val PER: 0.1565
2026-01-10 07:09:52,816: t15.2023.12.08 val PER: 0.1538
2026-01-10 07:09:52,816: t15.2023.12.10 val PER: 0.1275
2026-01-10 07:09:52,816: t15.2023.12.17 val PER: 0.1840
2026-01-10 07:09:52,816: t15.2023.12.29 val PER: 0.1805
2026-01-10 07:09:52,817: t15.2024.02.25 val PER: 0.1390
2026-01-10 07:09:52,817: t15.2024.03.08 val PER: 0.2902
2026-01-10 07:09:52,817: t15.2024.03.15 val PER: 0.2395
2026-01-10 07:09:52,817: t15.2024.03.17 val PER: 0.1862
2026-01-10 07:09:52,817: t15.2024.05.10 val PER: 0.1932
2026-01-10 07:09:52,817: t15.2024.06.14 val PER: 0.1972
2026-01-10 07:09:52,817: t15.2024.07.19 val PER: 0.2868
2026-01-10 07:09:52,817: t15.2024.07.21 val PER: 0.1317
2026-01-10 07:09:52,817: t15.2024.07.28 val PER: 0.1801
2026-01-10 07:09:52,817: t15.2025.01.10 val PER: 0.3361
2026-01-10 07:09:52,817: t15.2025.01.12 val PER: 0.1963
2026-01-10 07:09:52,817: t15.2025.03.14 val PER: 0.3624
2026-01-10 07:09:52,818: t15.2025.03.16 val PER: 0.2343
2026-01-10 07:09:52,818: t15.2025.03.30 val PER: 0.3437
2026-01-10 07:09:52,818: t15.2025.04.13 val PER: 0.2511
2026-01-10 07:09:52,819: New best val WER(5gram) 18.84% --> 17.86%
2026-01-10 07:09:53,018: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_8500
2026-01-10 07:10:02,495: Train batch 8600: loss: 17.56 grad norm: 67.72 time: 0.057
2026-01-10 07:10:21,359: Train batch 8800: loss: 17.33 grad norm: 62.65 time: 0.060
2026-01-10 07:10:40,511: Train batch 9000: loss: 17.48 grad norm: 63.71 time: 0.072
2026-01-10 07:10:40,511: Running test after training batch: 9000
2026-01-10 07:10:40,612: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:10:45,734: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:10:45,784: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost is
2026-01-10 07:11:00,269: Val batch 9000: PER (avg): 0.1853 CTC Loss (avg): 18.1790 WER(5gram): 18.77% (n=256) time: 19.758
2026-01-10 07:11:00,270: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-10 07:11:00,270: t15.2023.08.13 val PER: 0.1486
2026-01-10 07:11:00,270: t15.2023.08.18 val PER: 0.1459
2026-01-10 07:11:00,270: t15.2023.08.20 val PER: 0.1406
2026-01-10 07:11:00,270: t15.2023.08.25 val PER: 0.0994
2026-01-10 07:11:00,270: t15.2023.08.27 val PER: 0.2058
2026-01-10 07:11:00,270: t15.2023.09.01 val PER: 0.1063
2026-01-10 07:11:00,270: t15.2023.09.03 val PER: 0.1900
2026-01-10 07:11:00,270: t15.2023.09.24 val PER: 0.1517
2026-01-10 07:11:00,271: t15.2023.09.29 val PER: 0.1608
2026-01-10 07:11:00,271: t15.2023.10.01 val PER: 0.2067
2026-01-10 07:11:00,271: t15.2023.10.06 val PER: 0.0936
2026-01-10 07:11:00,271: t15.2023.10.08 val PER: 0.2679
2026-01-10 07:11:00,271: t15.2023.10.13 val PER: 0.2459
2026-01-10 07:11:00,271: t15.2023.10.15 val PER: 0.1859
2026-01-10 07:11:00,271: t15.2023.10.20 val PER: 0.2047
2026-01-10 07:11:00,271: t15.2023.10.22 val PER: 0.1414
2026-01-10 07:11:00,271: t15.2023.11.03 val PER: 0.2090
2026-01-10 07:11:00,271: t15.2023.11.04 val PER: 0.0410
2026-01-10 07:11:00,271: t15.2023.11.17 val PER: 0.0575
2026-01-10 07:11:00,271: t15.2023.11.19 val PER: 0.0519
2026-01-10 07:11:00,271: t15.2023.11.26 val PER: 0.1783
2026-01-10 07:11:00,271: t15.2023.12.03 val PER: 0.1597
2026-01-10 07:11:00,271: t15.2023.12.08 val PER: 0.1431
2026-01-10 07:11:00,271: t15.2023.12.10 val PER: 0.1209
2026-01-10 07:11:00,271: t15.2023.12.17 val PER: 0.1778
2026-01-10 07:11:00,272: t15.2023.12.29 val PER: 0.1757
2026-01-10 07:11:00,272: t15.2024.02.25 val PER: 0.1489
2026-01-10 07:11:00,272: t15.2024.03.08 val PER: 0.2632
2026-01-10 07:11:00,272: t15.2024.03.15 val PER: 0.2364
2026-01-10 07:11:00,272: t15.2024.03.17 val PER: 0.1869
2026-01-10 07:11:00,272: t15.2024.05.10 val PER: 0.1976
2026-01-10 07:11:00,272: t15.2024.06.14 val PER: 0.1924
2026-01-10 07:11:00,272: t15.2024.07.19 val PER: 0.2894
2026-01-10 07:11:00,272: t15.2024.07.21 val PER: 0.1186
2026-01-10 07:11:00,272: t15.2024.07.28 val PER: 0.1640
2026-01-10 07:11:00,272: t15.2025.01.10 val PER: 0.3361
2026-01-10 07:11:00,272: t15.2025.01.12 val PER: 0.1894
2026-01-10 07:11:00,273: t15.2025.03.14 val PER: 0.3654
2026-01-10 07:11:00,273: t15.2025.03.16 val PER: 0.2317
2026-01-10 07:11:00,273: t15.2025.03.30 val PER: 0.3414
2026-01-10 07:11:00,273: t15.2025.04.13 val PER: 0.2496
2026-01-10 07:11:00,406: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_9000
2026-01-10 07:11:19,040: Train batch 9200: loss: 12.26 grad norm: 51.18 time: 0.055
2026-01-10 07:11:37,605: Train batch 9400: loss: 8.91 grad norm: 48.33 time: 0.071
2026-01-10 07:11:47,005: Running test after training batch: 9500
2026-01-10 07:11:47,120: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:11:52,308: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:11:52,357: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-10 07:12:06,554: Val batch 9500: PER (avg): 0.1837 CTC Loss (avg): 18.1653 WER(5gram): 17.80% (n=256) time: 19.548
2026-01-10 07:12:06,554: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-10 07:12:06,555: t15.2023.08.13 val PER: 0.1403
2026-01-10 07:12:06,555: t15.2023.08.18 val PER: 0.1383
2026-01-10 07:12:06,555: t15.2023.08.20 val PER: 0.1501
2026-01-10 07:12:06,555: t15.2023.08.25 val PER: 0.0949
2026-01-10 07:12:06,555: t15.2023.08.27 val PER: 0.2058
2026-01-10 07:12:06,555: t15.2023.09.01 val PER: 0.1047
2026-01-10 07:12:06,555: t15.2023.09.03 val PER: 0.1900
2026-01-10 07:12:06,555: t15.2023.09.24 val PER: 0.1505
2026-01-10 07:12:06,555: t15.2023.09.29 val PER: 0.1506
2026-01-10 07:12:06,555: t15.2023.10.01 val PER: 0.2021
2026-01-10 07:12:06,555: t15.2023.10.06 val PER: 0.1098
2026-01-10 07:12:06,555: t15.2023.10.08 val PER: 0.2747
2026-01-10 07:12:06,556: t15.2023.10.13 val PER: 0.2320
2026-01-10 07:12:06,556: t15.2023.10.15 val PER: 0.1931
2026-01-10 07:12:06,556: t15.2023.10.20 val PER: 0.1913
2026-01-10 07:12:06,556: t15.2023.10.22 val PER: 0.1314
2026-01-10 07:12:06,556: t15.2023.11.03 val PER: 0.2022
2026-01-10 07:12:06,556: t15.2023.11.04 val PER: 0.0410
2026-01-10 07:12:06,556: t15.2023.11.17 val PER: 0.0622
2026-01-10 07:12:06,556: t15.2023.11.19 val PER: 0.0519
2026-01-10 07:12:06,556: t15.2023.11.26 val PER: 0.1710
2026-01-10 07:12:06,556: t15.2023.12.03 val PER: 0.1544
2026-01-10 07:12:06,556: t15.2023.12.08 val PER: 0.1558
2026-01-10 07:12:06,556: t15.2023.12.10 val PER: 0.1288
2026-01-10 07:12:06,556: t15.2023.12.17 val PER: 0.1684
2026-01-10 07:12:06,556: t15.2023.12.29 val PER: 0.1791
2026-01-10 07:12:06,556: t15.2024.02.25 val PER: 0.1404
2026-01-10 07:12:06,557: t15.2024.03.08 val PER: 0.2717
2026-01-10 07:12:06,557: t15.2024.03.15 val PER: 0.2408
2026-01-10 07:12:06,557: t15.2024.03.17 val PER: 0.1820
2026-01-10 07:12:06,557: t15.2024.05.10 val PER: 0.1902
2026-01-10 07:12:06,557: t15.2024.06.14 val PER: 0.1924
2026-01-10 07:12:06,557: t15.2024.07.19 val PER: 0.2815
2026-01-10 07:12:06,557: t15.2024.07.21 val PER: 0.1262
2026-01-10 07:12:06,557: t15.2024.07.28 val PER: 0.1640
2026-01-10 07:12:06,557: t15.2025.01.10 val PER: 0.3237
2026-01-10 07:12:06,557: t15.2025.01.12 val PER: 0.1855
2026-01-10 07:12:06,557: t15.2025.03.14 val PER: 0.3669
2026-01-10 07:12:06,557: t15.2025.03.16 val PER: 0.2186
2026-01-10 07:12:06,557: t15.2025.03.30 val PER: 0.3368
2026-01-10 07:12:06,558: t15.2025.04.13 val PER: 0.2525
2026-01-10 07:12:06,559: New best val WER(5gram) 17.86% --> 17.80%
2026-01-10 07:12:06,751: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_9500
2026-01-10 07:12:15,996: Train batch 9600: loss: 9.69 grad norm: 50.34 time: 0.078
2026-01-10 07:12:34,837: Train batch 9800: loss: 14.30 grad norm: 61.38 time: 0.064
2026-01-10 07:12:53,994: Train batch 10000: loss: 6.63 grad norm: 39.08 time: 0.061
2026-01-10 07:12:53,994: Running test after training batch: 10000
2026-01-10 07:12:54,194: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:12:59,336: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:12:59,400: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-10 07:13:12,814: Val batch 10000: PER (avg): 0.1806 CTC Loss (avg): 17.7426 WER(5gram): 18.12% (n=256) time: 18.819
2026-01-10 07:13:12,814: WER lens: avg_true_words=5.99 avg_pred_words=6.23 max_pred_words=12
2026-01-10 07:13:12,814: t15.2023.08.13 val PER: 0.1351
2026-01-10 07:13:12,814: t15.2023.08.18 val PER: 0.1375
2026-01-10 07:13:12,815: t15.2023.08.20 val PER: 0.1477
2026-01-10 07:13:12,815: t15.2023.08.25 val PER: 0.1039
2026-01-10 07:13:12,815: t15.2023.08.27 val PER: 0.2074
2026-01-10 07:13:12,815: t15.2023.09.01 val PER: 0.0990
2026-01-10 07:13:12,815: t15.2023.09.03 val PER: 0.1924
2026-01-10 07:13:12,815: t15.2023.09.24 val PER: 0.1517
2026-01-10 07:13:12,815: t15.2023.09.29 val PER: 0.1551
2026-01-10 07:13:12,815: t15.2023.10.01 val PER: 0.1863
2026-01-10 07:13:12,815: t15.2023.10.06 val PER: 0.1130
2026-01-10 07:13:12,815: t15.2023.10.08 val PER: 0.2666
2026-01-10 07:13:12,816: t15.2023.10.13 val PER: 0.2320
2026-01-10 07:13:12,816: t15.2023.10.15 val PER: 0.1833
2026-01-10 07:13:12,816: t15.2023.10.20 val PER: 0.1846
2026-01-10 07:13:12,816: t15.2023.10.22 val PER: 0.1392
2026-01-10 07:13:12,816: t15.2023.11.03 val PER: 0.1913
2026-01-10 07:13:12,816: t15.2023.11.04 val PER: 0.0410
2026-01-10 07:13:12,816: t15.2023.11.17 val PER: 0.0529
2026-01-10 07:13:12,816: t15.2023.11.19 val PER: 0.0519
2026-01-10 07:13:12,817: t15.2023.11.26 val PER: 0.1659
2026-01-10 07:13:12,817: t15.2023.12.03 val PER: 0.1607
2026-01-10 07:13:12,817: t15.2023.12.08 val PER: 0.1511
2026-01-10 07:13:12,817: t15.2023.12.10 val PER: 0.1183
2026-01-10 07:13:12,817: t15.2023.12.17 val PER: 0.1736
2026-01-10 07:13:12,817: t15.2023.12.29 val PER: 0.1702
2026-01-10 07:13:12,817: t15.2024.02.25 val PER: 0.1531
2026-01-10 07:13:12,817: t15.2024.03.08 val PER: 0.2632
2026-01-10 07:13:12,817: t15.2024.03.15 val PER: 0.2326
2026-01-10 07:13:12,817: t15.2024.03.17 val PER: 0.1743
2026-01-10 07:13:12,818: t15.2024.05.10 val PER: 0.1842
2026-01-10 07:13:12,818: t15.2024.06.14 val PER: 0.1861
2026-01-10 07:13:12,818: t15.2024.07.19 val PER: 0.2762
2026-01-10 07:13:12,818: t15.2024.07.21 val PER: 0.1255
2026-01-10 07:13:12,818: t15.2024.07.28 val PER: 0.1654
2026-01-10 07:13:12,818: t15.2025.01.10 val PER: 0.3209
2026-01-10 07:13:12,818: t15.2025.01.12 val PER: 0.1855
2026-01-10 07:13:12,818: t15.2025.03.14 val PER: 0.3521
2026-01-10 07:13:12,818: t15.2025.03.16 val PER: 0.2382
2026-01-10 07:13:12,818: t15.2025.03.30 val PER: 0.3345
2026-01-10 07:13:12,818: t15.2025.04.13 val PER: 0.2368
2026-01-10 07:13:12,960: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_10000
2026-01-10 07:13:31,493: Train batch 10200: loss: 7.19 grad norm: 39.26 time: 0.050
2026-01-10 07:13:50,775: Train batch 10400: loss: 10.74 grad norm: 57.38 time: 0.072
2026-01-10 07:14:00,435: Running test after training batch: 10500
2026-01-10 07:14:00,538: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:14:05,748: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:14:05,820: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-10 07:14:20,661: Val batch 10500: PER (avg): 0.1793 CTC Loss (avg): 17.6947 WER(5gram): 17.08% (n=256) time: 20.225
2026-01-10 07:14:20,661: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-10 07:14:20,661: t15.2023.08.13 val PER: 0.1393
2026-01-10 07:14:20,661: t15.2023.08.18 val PER: 0.1324
2026-01-10 07:14:20,661: t15.2023.08.20 val PER: 0.1446
2026-01-10 07:14:20,661: t15.2023.08.25 val PER: 0.1039
2026-01-10 07:14:20,661: t15.2023.08.27 val PER: 0.2186
2026-01-10 07:14:20,662: t15.2023.09.01 val PER: 0.1006
2026-01-10 07:14:20,662: t15.2023.09.03 val PER: 0.1817
2026-01-10 07:14:20,662: t15.2023.09.24 val PER: 0.1553
2026-01-10 07:14:20,662: t15.2023.09.29 val PER: 0.1506
2026-01-10 07:14:20,662: t15.2023.10.01 val PER: 0.1995
2026-01-10 07:14:20,662: t15.2023.10.06 val PER: 0.1044
2026-01-10 07:14:20,662: t15.2023.10.08 val PER: 0.2625
2026-01-10 07:14:20,662: t15.2023.10.13 val PER: 0.2265
2026-01-10 07:14:20,662: t15.2023.10.15 val PER: 0.1872
2026-01-10 07:14:20,662: t15.2023.10.20 val PER: 0.1980
2026-01-10 07:14:20,662: t15.2023.10.22 val PER: 0.1281
2026-01-10 07:14:20,662: t15.2023.11.03 val PER: 0.2001
2026-01-10 07:14:20,662: t15.2023.11.04 val PER: 0.0546
2026-01-10 07:14:20,662: t15.2023.11.17 val PER: 0.0638
2026-01-10 07:14:20,663: t15.2023.11.19 val PER: 0.0519
2026-01-10 07:14:20,663: t15.2023.11.26 val PER: 0.1630
2026-01-10 07:14:20,663: t15.2023.12.03 val PER: 0.1429
2026-01-10 07:14:20,663: t15.2023.12.08 val PER: 0.1451
2026-01-10 07:14:20,663: t15.2023.12.10 val PER: 0.1288
2026-01-10 07:14:20,663: t15.2023.12.17 val PER: 0.1590
2026-01-10 07:14:20,663: t15.2023.12.29 val PER: 0.1784
2026-01-10 07:14:20,663: t15.2024.02.25 val PER: 0.1503
2026-01-10 07:14:20,663: t15.2024.03.08 val PER: 0.2560
2026-01-10 07:14:20,663: t15.2024.03.15 val PER: 0.2245
2026-01-10 07:14:20,663: t15.2024.03.17 val PER: 0.1785
2026-01-10 07:14:20,663: t15.2024.05.10 val PER: 0.1828
2026-01-10 07:14:20,664: t15.2024.06.14 val PER: 0.1893
2026-01-10 07:14:20,664: t15.2024.07.19 val PER: 0.2742
2026-01-10 07:14:20,664: t15.2024.07.21 val PER: 0.1214
2026-01-10 07:14:20,664: t15.2024.07.28 val PER: 0.1500
2026-01-10 07:14:20,664: t15.2025.01.10 val PER: 0.3306
2026-01-10 07:14:20,664: t15.2025.01.12 val PER: 0.1755
2026-01-10 07:14:20,664: t15.2025.03.14 val PER: 0.3609
2026-01-10 07:14:20,664: t15.2025.03.16 val PER: 0.2291
2026-01-10 07:14:20,664: t15.2025.03.30 val PER: 0.3322
2026-01-10 07:14:20,664: t15.2025.04.13 val PER: 0.2496
2026-01-10 07:14:20,665: New best val WER(5gram) 17.80% --> 17.08%
2026-01-10 07:14:20,862: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_10500
2026-01-10 07:14:30,339: Train batch 10600: loss: 10.32 grad norm: 55.61 time: 0.072
2026-01-10 07:14:49,217: Train batch 10800: loss: 16.56 grad norm: 70.11 time: 0.065
2026-01-10 07:15:09,307: Train batch 11000: loss: 16.71 grad norm: 63.94 time: 0.057
2026-01-10 07:15:09,308: Running test after training batch: 11000
2026-01-10 07:15:09,439: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:15:14,650: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:15:14,716: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-10 07:15:29,608: Val batch 11000: PER (avg): 0.1742 CTC Loss (avg): 17.3475 WER(5gram): 16.30% (n=256) time: 20.300
2026-01-10 07:15:29,608: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-10 07:15:29,608: t15.2023.08.13 val PER: 0.1237
2026-01-10 07:15:29,608: t15.2023.08.18 val PER: 0.1316
2026-01-10 07:15:29,609: t15.2023.08.20 val PER: 0.1311
2026-01-10 07:15:29,609: t15.2023.08.25 val PER: 0.0949
2026-01-10 07:15:29,609: t15.2023.08.27 val PER: 0.2203
2026-01-10 07:15:29,609: t15.2023.09.01 val PER: 0.0990
2026-01-10 07:15:29,609: t15.2023.09.03 val PER: 0.1805
2026-01-10 07:15:29,609: t15.2023.09.24 val PER: 0.1468
2026-01-10 07:15:29,609: t15.2023.09.29 val PER: 0.1391
2026-01-10 07:15:29,609: t15.2023.10.01 val PER: 0.1915
2026-01-10 07:15:29,609: t15.2023.10.06 val PER: 0.0947
2026-01-10 07:15:29,610: t15.2023.10.08 val PER: 0.2788
2026-01-10 07:15:29,610: t15.2023.10.13 val PER: 0.2304
2026-01-10 07:15:29,610: t15.2023.10.15 val PER: 0.1826
2026-01-10 07:15:29,610: t15.2023.10.20 val PER: 0.2114
2026-01-10 07:15:29,610: t15.2023.10.22 val PER: 0.1247
2026-01-10 07:15:29,610: t15.2023.11.03 val PER: 0.2001
2026-01-10 07:15:29,610: t15.2023.11.04 val PER: 0.0546
2026-01-10 07:15:29,610: t15.2023.11.17 val PER: 0.0575
2026-01-10 07:15:29,610: t15.2023.11.19 val PER: 0.0499
2026-01-10 07:15:29,610: t15.2023.11.26 val PER: 0.1594
2026-01-10 07:15:29,611: t15.2023.12.03 val PER: 0.1387
2026-01-10 07:15:29,611: t15.2023.12.08 val PER: 0.1411
2026-01-10 07:15:29,611: t15.2023.12.10 val PER: 0.1091
2026-01-10 07:15:29,611: t15.2023.12.17 val PER: 0.1715
2026-01-10 07:15:29,611: t15.2023.12.29 val PER: 0.1551
2026-01-10 07:15:29,611: t15.2024.02.25 val PER: 0.1292
2026-01-10 07:15:29,611: t15.2024.03.08 val PER: 0.2560
2026-01-10 07:15:29,611: t15.2024.03.15 val PER: 0.2220
2026-01-10 07:15:29,611: t15.2024.03.17 val PER: 0.1632
2026-01-10 07:15:29,611: t15.2024.05.10 val PER: 0.1798
2026-01-10 07:15:29,611: t15.2024.06.14 val PER: 0.1767
2026-01-10 07:15:29,612: t15.2024.07.19 val PER: 0.2742
2026-01-10 07:15:29,612: t15.2024.07.21 val PER: 0.1179
2026-01-10 07:15:29,612: t15.2024.07.28 val PER: 0.1566
2026-01-10 07:15:29,612: t15.2025.01.10 val PER: 0.3278
2026-01-10 07:15:29,612: t15.2025.01.12 val PER: 0.1747
2026-01-10 07:15:29,612: t15.2025.03.14 val PER: 0.3580
2026-01-10 07:15:29,612: t15.2025.03.16 val PER: 0.2094
2026-01-10 07:15:29,612: t15.2025.03.30 val PER: 0.3264
2026-01-10 07:15:29,612: t15.2025.04.13 val PER: 0.2397
2026-01-10 07:15:29,613: New best val WER(5gram) 17.08% --> 16.30%
2026-01-10 07:15:29,815: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_11000
2026-01-10 07:15:49,172: Train batch 11200: loss: 11.88 grad norm: 54.22 time: 0.071
2026-01-10 07:16:08,981: Train batch 11400: loss: 10.79 grad norm: 53.19 time: 0.057
2026-01-10 07:16:18,593: Running test after training batch: 11500
2026-01-10 07:16:18,701: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:16:24,073: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:16:24,155: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-10 07:16:38,869: Val batch 11500: PER (avg): 0.1720 CTC Loss (avg): 17.1461 WER(5gram): 17.41% (n=256) time: 20.276
2026-01-10 07:16:38,870: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-10 07:16:38,870: t15.2023.08.13 val PER: 0.1310
2026-01-10 07:16:38,870: t15.2023.08.18 val PER: 0.1266
2026-01-10 07:16:38,870: t15.2023.08.20 val PER: 0.1342
2026-01-10 07:16:38,870: t15.2023.08.25 val PER: 0.0949
2026-01-10 07:16:38,870: t15.2023.08.27 val PER: 0.2122
2026-01-10 07:16:38,871: t15.2023.09.01 val PER: 0.0966
2026-01-10 07:16:38,871: t15.2023.09.03 val PER: 0.1758
2026-01-10 07:16:38,871: t15.2023.09.24 val PER: 0.1371
2026-01-10 07:16:38,871: t15.2023.09.29 val PER: 0.1436
2026-01-10 07:16:38,871: t15.2023.10.01 val PER: 0.1889
2026-01-10 07:16:38,871: t15.2023.10.06 val PER: 0.0893
2026-01-10 07:16:38,871: t15.2023.10.08 val PER: 0.2679
2026-01-10 07:16:38,871: t15.2023.10.13 val PER: 0.2196
2026-01-10 07:16:38,871: t15.2023.10.15 val PER: 0.1773
2026-01-10 07:16:38,871: t15.2023.10.20 val PER: 0.1913
2026-01-10 07:16:38,871: t15.2023.10.22 val PER: 0.1225
2026-01-10 07:16:38,871: t15.2023.11.03 val PER: 0.1879
2026-01-10 07:16:38,871: t15.2023.11.04 val PER: 0.0410
2026-01-10 07:16:38,871: t15.2023.11.17 val PER: 0.0498
2026-01-10 07:16:38,872: t15.2023.11.19 val PER: 0.0559
2026-01-10 07:16:38,872: t15.2023.11.26 val PER: 0.1522
2026-01-10 07:16:38,872: t15.2023.12.03 val PER: 0.1397
2026-01-10 07:16:38,872: t15.2023.12.08 val PER: 0.1378
2026-01-10 07:16:38,872: t15.2023.12.10 val PER: 0.1156
2026-01-10 07:16:38,872: t15.2023.12.17 val PER: 0.1580
2026-01-10 07:16:38,872: t15.2023.12.29 val PER: 0.1613
2026-01-10 07:16:38,872: t15.2024.02.25 val PER: 0.1306
2026-01-10 07:16:38,872: t15.2024.03.08 val PER: 0.2603
2026-01-10 07:16:38,872: t15.2024.03.15 val PER: 0.2258
2026-01-10 07:16:38,873: t15.2024.03.17 val PER: 0.1674
2026-01-10 07:16:38,873: t15.2024.05.10 val PER: 0.1753
2026-01-10 07:16:38,873: t15.2024.06.14 val PER: 0.1909
2026-01-10 07:16:38,873: t15.2024.07.19 val PER: 0.2755
2026-01-10 07:16:38,873: t15.2024.07.21 val PER: 0.1152
2026-01-10 07:16:38,873: t15.2024.07.28 val PER: 0.1544
2026-01-10 07:16:38,873: t15.2025.01.10 val PER: 0.3320
2026-01-10 07:16:38,873: t15.2025.01.12 val PER: 0.1647
2026-01-10 07:16:38,873: t15.2025.03.14 val PER: 0.3580
2026-01-10 07:16:38,873: t15.2025.03.16 val PER: 0.2120
2026-01-10 07:16:38,873: t15.2025.03.30 val PER: 0.3126
2026-01-10 07:16:38,873: t15.2025.04.13 val PER: 0.2439
2026-01-10 07:16:39,014: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_11500
2026-01-10 07:16:48,206: Train batch 11600: loss: 12.36 grad norm: 52.25 time: 0.061
2026-01-10 07:17:07,050: Train batch 11800: loss: 8.03 grad norm: 43.51 time: 0.050
2026-01-10 07:17:25,752: Train batch 12000: loss: 15.01 grad norm: 55.62 time: 0.072
2026-01-10 07:17:25,752: Running test after training batch: 12000
2026-01-10 07:17:25,856: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:17:30,926: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:17:30,995: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-10 07:17:46,080: Val batch 12000: PER (avg): 0.1699 CTC Loss (avg): 17.0575 WER(5gram): 16.82% (n=256) time: 20.327
2026-01-10 07:17:46,080: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-10 07:17:46,080: t15.2023.08.13 val PER: 0.1331
2026-01-10 07:17:46,081: t15.2023.08.18 val PER: 0.1182
2026-01-10 07:17:46,081: t15.2023.08.20 val PER: 0.1263
2026-01-10 07:17:46,081: t15.2023.08.25 val PER: 0.1084
2026-01-10 07:17:46,081: t15.2023.08.27 val PER: 0.2138
2026-01-10 07:17:46,081: t15.2023.09.01 val PER: 0.0974
2026-01-10 07:17:46,081: t15.2023.09.03 val PER: 0.1710
2026-01-10 07:17:46,081: t15.2023.09.24 val PER: 0.1432
2026-01-10 07:17:46,081: t15.2023.09.29 val PER: 0.1404
2026-01-10 07:17:46,081: t15.2023.10.01 val PER: 0.1823
2026-01-10 07:17:46,081: t15.2023.10.06 val PER: 0.0980
2026-01-10 07:17:46,081: t15.2023.10.08 val PER: 0.2625
2026-01-10 07:17:46,081: t15.2023.10.13 val PER: 0.2219
2026-01-10 07:17:46,082: t15.2023.10.15 val PER: 0.1747
2026-01-10 07:17:46,082: t15.2023.10.20 val PER: 0.1980
2026-01-10 07:17:46,082: t15.2023.10.22 val PER: 0.1303
2026-01-10 07:17:46,082: t15.2023.11.03 val PER: 0.1981
2026-01-10 07:17:46,082: t15.2023.11.04 val PER: 0.0444
2026-01-10 07:17:46,082: t15.2023.11.17 val PER: 0.0435
2026-01-10 07:17:46,082: t15.2023.11.19 val PER: 0.0439
2026-01-10 07:17:46,082: t15.2023.11.26 val PER: 0.1514
2026-01-10 07:17:46,082: t15.2023.12.03 val PER: 0.1229
2026-01-10 07:17:46,082: t15.2023.12.08 val PER: 0.1305
2026-01-10 07:17:46,082: t15.2023.12.10 val PER: 0.1104
2026-01-10 07:17:46,082: t15.2023.12.17 val PER: 0.1559
2026-01-10 07:17:46,082: t15.2023.12.29 val PER: 0.1544
2026-01-10 07:17:46,082: t15.2024.02.25 val PER: 0.1194
2026-01-10 07:17:46,082: t15.2024.03.08 val PER: 0.2603
2026-01-10 07:17:46,082: t15.2024.03.15 val PER: 0.2226
2026-01-10 07:17:46,083: t15.2024.03.17 val PER: 0.1695
2026-01-10 07:17:46,083: t15.2024.05.10 val PER: 0.1783
2026-01-10 07:17:46,083: t15.2024.06.14 val PER: 0.1845
2026-01-10 07:17:46,083: t15.2024.07.19 val PER: 0.2709
2026-01-10 07:17:46,083: t15.2024.07.21 val PER: 0.1138
2026-01-10 07:17:46,083: t15.2024.07.28 val PER: 0.1500
2026-01-10 07:17:46,083: t15.2025.01.10 val PER: 0.3182
2026-01-10 07:17:46,083: t15.2025.01.12 val PER: 0.1617
2026-01-10 07:17:46,083: t15.2025.03.14 val PER: 0.3536
2026-01-10 07:17:46,083: t15.2025.03.16 val PER: 0.2199
2026-01-10 07:17:46,083: t15.2025.03.30 val PER: 0.3230
2026-01-10 07:17:46,083: t15.2025.04.13 val PER: 0.2282
2026-01-10 07:17:46,222: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_12000
2026-01-10 07:18:05,035: Train batch 12200: loss: 7.02 grad norm: 45.39 time: 0.066
2026-01-10 07:18:24,085: Train batch 12400: loss: 5.55 grad norm: 38.26 time: 0.041
2026-01-10 07:18:33,597: Running test after training batch: 12500
2026-01-10 07:18:33,742: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:18:39,304: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:18:39,383: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost is
2026-01-10 07:18:54,413: Val batch 12500: PER (avg): 0.1680 CTC Loss (avg): 16.9214 WER(5gram): 14.93% (n=256) time: 20.815
2026-01-10 07:18:54,413: WER lens: avg_true_words=5.99 avg_pred_words=6.15 max_pred_words=12
2026-01-10 07:18:54,413: t15.2023.08.13 val PER: 0.1247
2026-01-10 07:18:54,413: t15.2023.08.18 val PER: 0.1190
2026-01-10 07:18:54,414: t15.2023.08.20 val PER: 0.1247
2026-01-10 07:18:54,414: t15.2023.08.25 val PER: 0.0919
2026-01-10 07:18:54,414: t15.2023.08.27 val PER: 0.2170
2026-01-10 07:18:54,414: t15.2023.09.01 val PER: 0.0917
2026-01-10 07:18:54,414: t15.2023.09.03 val PER: 0.1758
2026-01-10 07:18:54,414: t15.2023.09.24 val PER: 0.1432
2026-01-10 07:18:54,414: t15.2023.09.29 val PER: 0.1347
2026-01-10 07:18:54,414: t15.2023.10.01 val PER: 0.1816
2026-01-10 07:18:54,414: t15.2023.10.06 val PER: 0.0958
2026-01-10 07:18:54,415: t15.2023.10.08 val PER: 0.2693
2026-01-10 07:18:54,415: t15.2023.10.13 val PER: 0.2188
2026-01-10 07:18:54,415: t15.2023.10.15 val PER: 0.1707
2026-01-10 07:18:54,415: t15.2023.10.20 val PER: 0.1879
2026-01-10 07:18:54,415: t15.2023.10.22 val PER: 0.1247
2026-01-10 07:18:54,415: t15.2023.11.03 val PER: 0.1906
2026-01-10 07:18:54,415: t15.2023.11.04 val PER: 0.0512
2026-01-10 07:18:54,415: t15.2023.11.17 val PER: 0.0513
2026-01-10 07:18:54,416: t15.2023.11.19 val PER: 0.0399
2026-01-10 07:18:54,416: t15.2023.11.26 val PER: 0.1486
2026-01-10 07:18:54,416: t15.2023.12.03 val PER: 0.1397
2026-01-10 07:18:54,416: t15.2023.12.08 val PER: 0.1298
2026-01-10 07:18:54,416: t15.2023.12.10 val PER: 0.1130
2026-01-10 07:18:54,416: t15.2023.12.17 val PER: 0.1601
2026-01-10 07:18:54,416: t15.2023.12.29 val PER: 0.1599
2026-01-10 07:18:54,416: t15.2024.02.25 val PER: 0.1124
2026-01-10 07:18:54,416: t15.2024.03.08 val PER: 0.2475
2026-01-10 07:18:54,416: t15.2024.03.15 val PER: 0.2220
2026-01-10 07:18:54,417: t15.2024.03.17 val PER: 0.1611
2026-01-10 07:18:54,417: t15.2024.05.10 val PER: 0.1679
2026-01-10 07:18:54,417: t15.2024.06.14 val PER: 0.1956
2026-01-10 07:18:54,417: t15.2024.07.19 val PER: 0.2683
2026-01-10 07:18:54,417: t15.2024.07.21 val PER: 0.1021
2026-01-10 07:18:54,417: t15.2024.07.28 val PER: 0.1419
2026-01-10 07:18:54,417: t15.2025.01.10 val PER: 0.3251
2026-01-10 07:18:54,417: t15.2025.01.12 val PER: 0.1671
2026-01-10 07:18:54,417: t15.2025.03.14 val PER: 0.3698
2026-01-10 07:18:54,417: t15.2025.03.16 val PER: 0.2068
2026-01-10 07:18:54,417: t15.2025.03.30 val PER: 0.3138
2026-01-10 07:18:54,418: t15.2025.04.13 val PER: 0.2297
2026-01-10 07:18:54,418: New best val WER(5gram) 16.30% --> 14.93%
2026-01-10 07:18:54,612: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_12500
2026-01-10 07:19:03,961: Train batch 12600: loss: 9.23 grad norm: 46.84 time: 0.057
2026-01-10 07:19:24,551: Train batch 12800: loss: 6.75 grad norm: 41.64 time: 0.053
2026-01-10 07:19:43,926: Train batch 13000: loss: 7.34 grad norm: 43.29 time: 0.066
2026-01-10 07:19:43,926: Running test after training batch: 13000
2026-01-10 07:19:44,036: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:19:49,106: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:19:49,167: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost is
2026-01-10 07:20:05,611: Val batch 13000: PER (avg): 0.1644 CTC Loss (avg): 16.4188 WER(5gram): 15.12% (n=256) time: 21.685
2026-01-10 07:20:05,612: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-10 07:20:05,612: t15.2023.08.13 val PER: 0.1247
2026-01-10 07:20:05,612: t15.2023.08.18 val PER: 0.1224
2026-01-10 07:20:05,612: t15.2023.08.20 val PER: 0.1120
2026-01-10 07:20:05,612: t15.2023.08.25 val PER: 0.0934
2026-01-10 07:20:05,612: t15.2023.08.27 val PER: 0.2090
2026-01-10 07:20:05,612: t15.2023.09.01 val PER: 0.0860
2026-01-10 07:20:05,612: t15.2023.09.03 val PER: 0.1722
2026-01-10 07:20:05,612: t15.2023.09.24 val PER: 0.1347
2026-01-10 07:20:05,612: t15.2023.09.29 val PER: 0.1366
2026-01-10 07:20:05,612: t15.2023.10.01 val PER: 0.1843
2026-01-10 07:20:05,612: t15.2023.10.06 val PER: 0.0969
2026-01-10 07:20:05,613: t15.2023.10.08 val PER: 0.2571
2026-01-10 07:20:05,613: t15.2023.10.13 val PER: 0.2242
2026-01-10 07:20:05,613: t15.2023.10.15 val PER: 0.1655
2026-01-10 07:20:05,613: t15.2023.10.20 val PER: 0.1812
2026-01-10 07:20:05,613: t15.2023.10.22 val PER: 0.1136
2026-01-10 07:20:05,613: t15.2023.11.03 val PER: 0.1906
2026-01-10 07:20:05,613: t15.2023.11.04 val PER: 0.0375
2026-01-10 07:20:05,613: t15.2023.11.17 val PER: 0.0482
2026-01-10 07:20:05,613: t15.2023.11.19 val PER: 0.0459
2026-01-10 07:20:05,613: t15.2023.11.26 val PER: 0.1442
2026-01-10 07:20:05,614: t15.2023.12.03 val PER: 0.1397
2026-01-10 07:20:05,614: t15.2023.12.08 val PER: 0.1278
2026-01-10 07:20:05,614: t15.2023.12.10 val PER: 0.1025
2026-01-10 07:20:05,614: t15.2023.12.17 val PER: 0.1486
2026-01-10 07:20:05,614: t15.2023.12.29 val PER: 0.1524
2026-01-10 07:20:05,614: t15.2024.02.25 val PER: 0.1124
2026-01-10 07:20:05,614: t15.2024.03.08 val PER: 0.2461
2026-01-10 07:20:05,614: t15.2024.03.15 val PER: 0.2158
2026-01-10 07:20:05,614: t15.2024.03.17 val PER: 0.1625
2026-01-10 07:20:05,614: t15.2024.05.10 val PER: 0.1709
2026-01-10 07:20:05,614: t15.2024.06.14 val PER: 0.1830
2026-01-10 07:20:05,614: t15.2024.07.19 val PER: 0.2617
2026-01-10 07:20:05,615: t15.2024.07.21 val PER: 0.1083
2026-01-10 07:20:05,615: t15.2024.07.28 val PER: 0.1397
2026-01-10 07:20:05,615: t15.2025.01.10 val PER: 0.2975
2026-01-10 07:20:05,615: t15.2025.01.12 val PER: 0.1647
2026-01-10 07:20:05,615: t15.2025.03.14 val PER: 0.3462
2026-01-10 07:20:05,615: t15.2025.03.16 val PER: 0.2003
2026-01-10 07:20:05,615: t15.2025.03.30 val PER: 0.3230
2026-01-10 07:20:05,615: t15.2025.04.13 val PER: 0.2254
2026-01-10 07:20:05,746: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_13000
2026-01-10 07:20:24,400: Train batch 13200: loss: 15.33 grad norm: 63.63 time: 0.054
2026-01-10 07:20:43,182: Train batch 13400: loss: 10.74 grad norm: 56.93 time: 0.062
2026-01-10 07:20:52,629: Running test after training batch: 13500
2026-01-10 07:20:52,755: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:20:57,823: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:20:57,883: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-10 07:21:13,226: Val batch 13500: PER (avg): 0.1620 CTC Loss (avg): 16.3173 WER(5gram): 17.01% (n=256) time: 20.597
2026-01-10 07:21:13,227: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-10 07:21:13,227: t15.2023.08.13 val PER: 0.1154
2026-01-10 07:21:13,227: t15.2023.08.18 val PER: 0.1224
2026-01-10 07:21:13,227: t15.2023.08.20 val PER: 0.1247
2026-01-10 07:21:13,227: t15.2023.08.25 val PER: 0.0828
2026-01-10 07:21:13,227: t15.2023.08.27 val PER: 0.2026
2026-01-10 07:21:13,227: t15.2023.09.01 val PER: 0.0852
2026-01-10 07:21:13,228: t15.2023.09.03 val PER: 0.1627
2026-01-10 07:21:13,228: t15.2023.09.24 val PER: 0.1383
2026-01-10 07:21:13,228: t15.2023.09.29 val PER: 0.1340
2026-01-10 07:21:13,228: t15.2023.10.01 val PER: 0.1777
2026-01-10 07:21:13,228: t15.2023.10.06 val PER: 0.0969
2026-01-10 07:21:13,228: t15.2023.10.08 val PER: 0.2612
2026-01-10 07:21:13,228: t15.2023.10.13 val PER: 0.2258
2026-01-10 07:21:13,228: t15.2023.10.15 val PER: 0.1668
2026-01-10 07:21:13,228: t15.2023.10.20 val PER: 0.1711
2026-01-10 07:21:13,228: t15.2023.10.22 val PER: 0.1203
2026-01-10 07:21:13,228: t15.2023.11.03 val PER: 0.1940
2026-01-10 07:21:13,229: t15.2023.11.04 val PER: 0.0410
2026-01-10 07:21:13,229: t15.2023.11.17 val PER: 0.0467
2026-01-10 07:21:13,229: t15.2023.11.19 val PER: 0.0339
2026-01-10 07:21:13,229: t15.2023.11.26 val PER: 0.1362
2026-01-10 07:21:13,229: t15.2023.12.03 val PER: 0.1303
2026-01-10 07:21:13,229: t15.2023.12.08 val PER: 0.1285
2026-01-10 07:21:13,229: t15.2023.12.10 val PER: 0.0999
2026-01-10 07:21:13,229: t15.2023.12.17 val PER: 0.1445
2026-01-10 07:21:13,229: t15.2023.12.29 val PER: 0.1366
2026-01-10 07:21:13,229: t15.2024.02.25 val PER: 0.1124
2026-01-10 07:21:13,230: t15.2024.03.08 val PER: 0.2390
2026-01-10 07:21:13,230: t15.2024.03.15 val PER: 0.2139
2026-01-10 07:21:13,230: t15.2024.03.17 val PER: 0.1527
2026-01-10 07:21:13,230: t15.2024.05.10 val PER: 0.1738
2026-01-10 07:21:13,230: t15.2024.06.14 val PER: 0.1767
2026-01-10 07:21:13,230: t15.2024.07.19 val PER: 0.2531
2026-01-10 07:21:13,230: t15.2024.07.21 val PER: 0.0993
2026-01-10 07:21:13,230: t15.2024.07.28 val PER: 0.1404
2026-01-10 07:21:13,230: t15.2025.01.10 val PER: 0.3209
2026-01-10 07:21:13,230: t15.2025.01.12 val PER: 0.1563
2026-01-10 07:21:13,230: t15.2025.03.14 val PER: 0.3550
2026-01-10 07:21:13,231: t15.2025.03.16 val PER: 0.1976
2026-01-10 07:21:13,231: t15.2025.03.30 val PER: 0.3264
2026-01-10 07:21:13,231: t15.2025.04.13 val PER: 0.2225
2026-01-10 07:21:13,366: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_13500
2026-01-10 07:21:22,843: Train batch 13600: loss: 14.22 grad norm: 65.31 time: 0.062
2026-01-10 07:21:41,973: Train batch 13800: loss: 10.86 grad norm: 61.34 time: 0.056
2026-01-10 07:22:01,635: Train batch 14000: loss: 13.35 grad norm: 59.04 time: 0.054
2026-01-10 07:22:01,636: Running test after training batch: 14000
2026-01-10 07:22:01,761: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:22:07,936: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:22:08,006: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-10 07:22:23,458: Val batch 14000: PER (avg): 0.1611 CTC Loss (avg): 16.2973 WER(5gram): 15.65% (n=256) time: 21.822
2026-01-10 07:22:23,459: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-10 07:22:23,459: t15.2023.08.13 val PER: 0.1227
2026-01-10 07:22:23,459: t15.2023.08.18 val PER: 0.1123
2026-01-10 07:22:23,459: t15.2023.08.20 val PER: 0.1239
2026-01-10 07:22:23,459: t15.2023.08.25 val PER: 0.0979
2026-01-10 07:22:23,459: t15.2023.08.27 val PER: 0.1945
2026-01-10 07:22:23,459: t15.2023.09.01 val PER: 0.0852
2026-01-10 07:22:23,459: t15.2023.09.03 val PER: 0.1770
2026-01-10 07:22:23,459: t15.2023.09.24 val PER: 0.1359
2026-01-10 07:22:23,460: t15.2023.09.29 val PER: 0.1378
2026-01-10 07:22:23,460: t15.2023.10.01 val PER: 0.1797
2026-01-10 07:22:23,460: t15.2023.10.06 val PER: 0.0926
2026-01-10 07:22:23,460: t15.2023.10.08 val PER: 0.2571
2026-01-10 07:22:23,460: t15.2023.10.13 val PER: 0.2203
2026-01-10 07:22:23,460: t15.2023.10.15 val PER: 0.1628
2026-01-10 07:22:23,460: t15.2023.10.20 val PER: 0.2013
2026-01-10 07:22:23,460: t15.2023.10.22 val PER: 0.1147
2026-01-10 07:22:23,460: t15.2023.11.03 val PER: 0.1845
2026-01-10 07:22:23,460: t15.2023.11.04 val PER: 0.0375
2026-01-10 07:22:23,460: t15.2023.11.17 val PER: 0.0482
2026-01-10 07:22:23,461: t15.2023.11.19 val PER: 0.0419
2026-01-10 07:22:23,461: t15.2023.11.26 val PER: 0.1428
2026-01-10 07:22:23,461: t15.2023.12.03 val PER: 0.1303
2026-01-10 07:22:23,461: t15.2023.12.08 val PER: 0.1258
2026-01-10 07:22:23,461: t15.2023.12.10 val PER: 0.1130
2026-01-10 07:22:23,461: t15.2023.12.17 val PER: 0.1476
2026-01-10 07:22:23,461: t15.2023.12.29 val PER: 0.1352
2026-01-10 07:22:23,461: t15.2024.02.25 val PER: 0.1110
2026-01-10 07:22:23,461: t15.2024.03.08 val PER: 0.2432
2026-01-10 07:22:23,461: t15.2024.03.15 val PER: 0.2076
2026-01-10 07:22:23,461: t15.2024.03.17 val PER: 0.1534
2026-01-10 07:22:23,462: t15.2024.05.10 val PER: 0.1649
2026-01-10 07:22:23,462: t15.2024.06.14 val PER: 0.1703
2026-01-10 07:22:23,462: t15.2024.07.19 val PER: 0.2512
2026-01-10 07:22:23,462: t15.2024.07.21 val PER: 0.1014
2026-01-10 07:22:23,462: t15.2024.07.28 val PER: 0.1412
2026-01-10 07:22:23,462: t15.2025.01.10 val PER: 0.3085
2026-01-10 07:22:23,462: t15.2025.01.12 val PER: 0.1509
2026-01-10 07:22:23,462: t15.2025.03.14 val PER: 0.3476
2026-01-10 07:22:23,462: t15.2025.03.16 val PER: 0.2029
2026-01-10 07:22:23,462: t15.2025.03.30 val PER: 0.3103
2026-01-10 07:22:23,462: t15.2025.04.13 val PER: 0.2297
2026-01-10 07:22:23,607: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_14000
2026-01-10 07:22:43,351: Train batch 14200: loss: 9.43 grad norm: 52.26 time: 0.057
2026-01-10 07:23:02,844: Train batch 14400: loss: 6.68 grad norm: 40.11 time: 0.064
2026-01-10 07:23:12,420: Running test after training batch: 14500
2026-01-10 07:23:12,532: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:23:17,562: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:23:17,629: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-10 07:23:33,396: Val batch 14500: PER (avg): 0.1603 CTC Loss (avg): 16.2908 WER(5gram): 16.36% (n=256) time: 20.976
2026-01-10 07:23:33,397: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-10 07:23:33,397: t15.2023.08.13 val PER: 0.1247
2026-01-10 07:23:33,397: t15.2023.08.18 val PER: 0.1115
2026-01-10 07:23:33,397: t15.2023.08.20 val PER: 0.1183
2026-01-10 07:23:33,397: t15.2023.08.25 val PER: 0.0979
2026-01-10 07:23:33,397: t15.2023.08.27 val PER: 0.2026
2026-01-10 07:23:33,397: t15.2023.09.01 val PER: 0.0885
2026-01-10 07:23:33,397: t15.2023.09.03 val PER: 0.1686
2026-01-10 07:23:33,397: t15.2023.09.24 val PER: 0.1347
2026-01-10 07:23:33,397: t15.2023.09.29 val PER: 0.1315
2026-01-10 07:23:33,397: t15.2023.10.01 val PER: 0.1836
2026-01-10 07:23:33,397: t15.2023.10.06 val PER: 0.0850
2026-01-10 07:23:33,397: t15.2023.10.08 val PER: 0.2463
2026-01-10 07:23:33,398: t15.2023.10.13 val PER: 0.2273
2026-01-10 07:23:33,398: t15.2023.10.15 val PER: 0.1622
2026-01-10 07:23:33,398: t15.2023.10.20 val PER: 0.1711
2026-01-10 07:23:33,398: t15.2023.10.22 val PER: 0.1247
2026-01-10 07:23:33,398: t15.2023.11.03 val PER: 0.1900
2026-01-10 07:23:33,398: t15.2023.11.04 val PER: 0.0410
2026-01-10 07:23:33,398: t15.2023.11.17 val PER: 0.0482
2026-01-10 07:23:33,398: t15.2023.11.19 val PER: 0.0459
2026-01-10 07:23:33,398: t15.2023.11.26 val PER: 0.1420
2026-01-10 07:23:33,398: t15.2023.12.03 val PER: 0.1229
2026-01-10 07:23:33,398: t15.2023.12.08 val PER: 0.1212
2026-01-10 07:23:33,398: t15.2023.12.10 val PER: 0.1038
2026-01-10 07:23:33,398: t15.2023.12.17 val PER: 0.1570
2026-01-10 07:23:33,398: t15.2023.12.29 val PER: 0.1345
2026-01-10 07:23:33,399: t15.2024.02.25 val PER: 0.1081
2026-01-10 07:23:33,399: t15.2024.03.08 val PER: 0.2361
2026-01-10 07:23:33,399: t15.2024.03.15 val PER: 0.2026
2026-01-10 07:23:33,399: t15.2024.03.17 val PER: 0.1492
2026-01-10 07:23:33,399: t15.2024.05.10 val PER: 0.1530
2026-01-10 07:23:33,399: t15.2024.06.14 val PER: 0.1672
2026-01-10 07:23:33,399: t15.2024.07.19 val PER: 0.2617
2026-01-10 07:23:33,400: t15.2024.07.21 val PER: 0.0979
2026-01-10 07:23:33,400: t15.2024.07.28 val PER: 0.1390
2026-01-10 07:23:33,400: t15.2025.01.10 val PER: 0.3072
2026-01-10 07:23:33,400: t15.2025.01.12 val PER: 0.1563
2026-01-10 07:23:33,400: t15.2025.03.14 val PER: 0.3536
2026-01-10 07:23:33,400: t15.2025.03.16 val PER: 0.2055
2026-01-10 07:23:33,400: t15.2025.03.30 val PER: 0.3103
2026-01-10 07:23:33,400: t15.2025.04.13 val PER: 0.2225
2026-01-10 07:23:33,538: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_14500
2026-01-10 07:23:43,182: Train batch 14600: loss: 13.97 grad norm: 62.37 time: 0.059
2026-01-10 07:24:02,246: Train batch 14800: loss: 6.77 grad norm: 46.76 time: 0.050
2026-01-10 07:24:21,342: Train batch 15000: loss: 10.34 grad norm: 49.96 time: 0.052
2026-01-10 07:24:21,342: Running test after training batch: 15000
2026-01-10 07:24:21,472: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:24:26,673: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:24:26,742: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-10 07:24:41,690: Val batch 15000: PER (avg): 0.1585 CTC Loss (avg): 16.0331 WER(5gram): 15.97% (n=256) time: 20.347
2026-01-10 07:24:41,690: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-10 07:24:41,690: t15.2023.08.13 val PER: 0.1091
2026-01-10 07:24:41,690: t15.2023.08.18 val PER: 0.1106
2026-01-10 07:24:41,690: t15.2023.08.20 val PER: 0.1191
2026-01-10 07:24:41,691: t15.2023.08.25 val PER: 0.0873
2026-01-10 07:24:41,691: t15.2023.08.27 val PER: 0.2090
2026-01-10 07:24:41,691: t15.2023.09.01 val PER: 0.0893
2026-01-10 07:24:41,691: t15.2023.09.03 val PER: 0.1568
2026-01-10 07:24:41,691: t15.2023.09.24 val PER: 0.1396
2026-01-10 07:24:41,691: t15.2023.09.29 val PER: 0.1295
2026-01-10 07:24:41,691: t15.2023.10.01 val PER: 0.1764
2026-01-10 07:24:41,691: t15.2023.10.06 val PER: 0.0883
2026-01-10 07:24:41,691: t15.2023.10.08 val PER: 0.2517
2026-01-10 07:24:41,691: t15.2023.10.13 val PER: 0.2188
2026-01-10 07:24:41,692: t15.2023.10.15 val PER: 0.1648
2026-01-10 07:24:41,692: t15.2023.10.20 val PER: 0.1879
2026-01-10 07:24:41,692: t15.2023.10.22 val PER: 0.1180
2026-01-10 07:24:41,692: t15.2023.11.03 val PER: 0.1920
2026-01-10 07:24:41,692: t15.2023.11.04 val PER: 0.0444
2026-01-10 07:24:41,692: t15.2023.11.17 val PER: 0.0467
2026-01-10 07:24:41,692: t15.2023.11.19 val PER: 0.0419
2026-01-10 07:24:41,692: t15.2023.11.26 val PER: 0.1370
2026-01-10 07:24:41,692: t15.2023.12.03 val PER: 0.1208
2026-01-10 07:24:41,692: t15.2023.12.08 val PER: 0.1212
2026-01-10 07:24:41,692: t15.2023.12.10 val PER: 0.0999
2026-01-10 07:24:41,692: t15.2023.12.17 val PER: 0.1549
2026-01-10 07:24:41,692: t15.2023.12.29 val PER: 0.1373
2026-01-10 07:24:41,693: t15.2024.02.25 val PER: 0.1053
2026-01-10 07:24:41,693: t15.2024.03.08 val PER: 0.2390
2026-01-10 07:24:41,693: t15.2024.03.15 val PER: 0.2070
2026-01-10 07:24:41,693: t15.2024.03.17 val PER: 0.1409
2026-01-10 07:24:41,693: t15.2024.05.10 val PER: 0.1694
2026-01-10 07:24:41,693: t15.2024.06.14 val PER: 0.1782
2026-01-10 07:24:41,693: t15.2024.07.19 val PER: 0.2485
2026-01-10 07:24:41,693: t15.2024.07.21 val PER: 0.0931
2026-01-10 07:24:41,693: t15.2024.07.28 val PER: 0.1316
2026-01-10 07:24:41,693: t15.2025.01.10 val PER: 0.3072
2026-01-10 07:24:41,693: t15.2025.01.12 val PER: 0.1547
2026-01-10 07:24:41,693: t15.2025.03.14 val PER: 0.3491
2026-01-10 07:24:41,693: t15.2025.03.16 val PER: 0.2068
2026-01-10 07:24:41,693: t15.2025.03.30 val PER: 0.3069
2026-01-10 07:24:41,693: t15.2025.04.13 val PER: 0.2282
2026-01-10 07:24:41,833: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_15000
2026-01-10 07:25:00,534: Train batch 15200: loss: 6.14 grad norm: 45.51 time: 0.057
2026-01-10 07:25:19,169: Train batch 15400: loss: 13.08 grad norm: 57.92 time: 0.050
2026-01-10 07:25:29,518: Running test after training batch: 15500
2026-01-10 07:25:29,641: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:25:34,690: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:25:34,760: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-10 07:25:49,714: Val batch 15500: PER (avg): 0.1580 CTC Loss (avg): 15.9626 WER(5gram): 15.38% (n=256) time: 20.195
2026-01-10 07:25:49,714: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-10 07:25:49,714: t15.2023.08.13 val PER: 0.1143
2026-01-10 07:25:49,714: t15.2023.08.18 val PER: 0.1174
2026-01-10 07:25:49,714: t15.2023.08.20 val PER: 0.1191
2026-01-10 07:25:49,714: t15.2023.08.25 val PER: 0.0979
2026-01-10 07:25:49,715: t15.2023.08.27 val PER: 0.1961
2026-01-10 07:25:49,715: t15.2023.09.01 val PER: 0.0844
2026-01-10 07:25:49,715: t15.2023.09.03 val PER: 0.1675
2026-01-10 07:25:49,715: t15.2023.09.24 val PER: 0.1359
2026-01-10 07:25:49,715: t15.2023.09.29 val PER: 0.1321
2026-01-10 07:25:49,715: t15.2023.10.01 val PER: 0.1737
2026-01-10 07:25:49,715: t15.2023.10.06 val PER: 0.0904
2026-01-10 07:25:49,715: t15.2023.10.08 val PER: 0.2639
2026-01-10 07:25:49,715: t15.2023.10.13 val PER: 0.2102
2026-01-10 07:25:49,715: t15.2023.10.15 val PER: 0.1635
2026-01-10 07:25:49,716: t15.2023.10.20 val PER: 0.1913
2026-01-10 07:25:49,716: t15.2023.10.22 val PER: 0.1236
2026-01-10 07:25:49,716: t15.2023.11.03 val PER: 0.1825
2026-01-10 07:25:49,716: t15.2023.11.04 val PER: 0.0410
2026-01-10 07:25:49,716: t15.2023.11.17 val PER: 0.0404
2026-01-10 07:25:49,716: t15.2023.11.19 val PER: 0.0419
2026-01-10 07:25:49,716: t15.2023.11.26 val PER: 0.1377
2026-01-10 07:25:49,716: t15.2023.12.03 val PER: 0.1229
2026-01-10 07:25:49,716: t15.2023.12.08 val PER: 0.1178
2026-01-10 07:25:49,717: t15.2023.12.10 val PER: 0.1012
2026-01-10 07:25:49,717: t15.2023.12.17 val PER: 0.1507
2026-01-10 07:25:49,717: t15.2023.12.29 val PER: 0.1345
2026-01-10 07:25:49,717: t15.2024.02.25 val PER: 0.1096
2026-01-10 07:25:49,717: t15.2024.03.08 val PER: 0.2347
2026-01-10 07:25:49,717: t15.2024.03.15 val PER: 0.2089
2026-01-10 07:25:49,717: t15.2024.03.17 val PER: 0.1492
2026-01-10 07:25:49,717: t15.2024.05.10 val PER: 0.1501
2026-01-10 07:25:49,718: t15.2024.06.14 val PER: 0.1703
2026-01-10 07:25:49,718: t15.2024.07.19 val PER: 0.2518
2026-01-10 07:25:49,718: t15.2024.07.21 val PER: 0.0945
2026-01-10 07:25:49,718: t15.2024.07.28 val PER: 0.1368
2026-01-10 07:25:49,718: t15.2025.01.10 val PER: 0.3085
2026-01-10 07:25:49,718: t15.2025.01.12 val PER: 0.1517
2026-01-10 07:25:49,718: t15.2025.03.14 val PER: 0.3536
2026-01-10 07:25:49,718: t15.2025.03.16 val PER: 0.1976
2026-01-10 07:25:49,718: t15.2025.03.30 val PER: 0.3023
2026-01-10 07:25:49,719: t15.2025.04.13 val PER: 0.2168
2026-01-10 07:25:49,851: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_15500
2026-01-10 07:25:59,086: Train batch 15600: loss: 13.98 grad norm: 59.23 time: 0.062
2026-01-10 07:26:18,411: Train batch 15800: loss: 15.53 grad norm: 65.68 time: 0.067
2026-01-10 07:26:37,653: Train batch 16000: loss: 9.73 grad norm: 48.18 time: 0.055
2026-01-10 07:26:37,653: Running test after training batch: 16000
2026-01-10 07:26:37,793: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:26:42,857: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:26:42,932: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-10 07:26:58,040: Val batch 16000: PER (avg): 0.1585 CTC Loss (avg): 16.0747 WER(5gram): 15.78% (n=256) time: 20.387
2026-01-10 07:26:58,041: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-10 07:26:58,041: t15.2023.08.13 val PER: 0.1102
2026-01-10 07:26:58,041: t15.2023.08.18 val PER: 0.1098
2026-01-10 07:26:58,041: t15.2023.08.20 val PER: 0.1199
2026-01-10 07:26:58,041: t15.2023.08.25 val PER: 0.0934
2026-01-10 07:26:58,041: t15.2023.08.27 val PER: 0.1977
2026-01-10 07:26:58,041: t15.2023.09.01 val PER: 0.0860
2026-01-10 07:26:58,041: t15.2023.09.03 val PER: 0.1651
2026-01-10 07:26:58,042: t15.2023.09.24 val PER: 0.1311
2026-01-10 07:26:58,042: t15.2023.09.29 val PER: 0.1302
2026-01-10 07:26:58,042: t15.2023.10.01 val PER: 0.1810
2026-01-10 07:26:58,042: t15.2023.10.06 val PER: 0.0936
2026-01-10 07:26:58,042: t15.2023.10.08 val PER: 0.2517
2026-01-10 07:26:58,042: t15.2023.10.13 val PER: 0.2126
2026-01-10 07:26:58,042: t15.2023.10.15 val PER: 0.1595
2026-01-10 07:26:58,042: t15.2023.10.20 val PER: 0.1913
2026-01-10 07:26:58,042: t15.2023.10.22 val PER: 0.1180
2026-01-10 07:26:58,042: t15.2023.11.03 val PER: 0.1818
2026-01-10 07:26:58,043: t15.2023.11.04 val PER: 0.0341
2026-01-10 07:26:58,043: t15.2023.11.17 val PER: 0.0435
2026-01-10 07:26:58,043: t15.2023.11.19 val PER: 0.0519
2026-01-10 07:26:58,043: t15.2023.11.26 val PER: 0.1413
2026-01-10 07:26:58,043: t15.2023.12.03 val PER: 0.1292
2026-01-10 07:26:58,043: t15.2023.12.08 val PER: 0.1185
2026-01-10 07:26:58,043: t15.2023.12.10 val PER: 0.0986
2026-01-10 07:26:58,043: t15.2023.12.17 val PER: 0.1424
2026-01-10 07:26:58,043: t15.2023.12.29 val PER: 0.1318
2026-01-10 07:26:58,043: t15.2024.02.25 val PER: 0.1152
2026-01-10 07:26:58,043: t15.2024.03.08 val PER: 0.2432
2026-01-10 07:26:58,044: t15.2024.03.15 val PER: 0.2070
2026-01-10 07:26:58,044: t15.2024.03.17 val PER: 0.1471
2026-01-10 07:26:58,044: t15.2024.05.10 val PER: 0.1679
2026-01-10 07:26:58,044: t15.2024.06.14 val PER: 0.1640
2026-01-10 07:26:58,044: t15.2024.07.19 val PER: 0.2485
2026-01-10 07:26:58,044: t15.2024.07.21 val PER: 0.0931
2026-01-10 07:26:58,044: t15.2024.07.28 val PER: 0.1412
2026-01-10 07:26:58,044: t15.2025.01.10 val PER: 0.3182
2026-01-10 07:26:58,044: t15.2025.01.12 val PER: 0.1532
2026-01-10 07:26:58,044: t15.2025.03.14 val PER: 0.3521
2026-01-10 07:26:58,044: t15.2025.03.16 val PER: 0.2068
2026-01-10 07:26:58,045: t15.2025.03.30 val PER: 0.3115
2026-01-10 07:26:58,045: t15.2025.04.13 val PER: 0.2268
2026-01-10 07:26:58,179: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_16000
2026-01-10 07:27:16,945: Train batch 16200: loss: 7.56 grad norm: 46.42 time: 0.055
2026-01-10 07:27:35,959: Train batch 16400: loss: 12.13 grad norm: 69.34 time: 0.057
2026-01-10 07:27:45,684: Running test after training batch: 16500
2026-01-10 07:27:45,825: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:27:51,097: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:27:51,165: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-10 07:28:07,459: Val batch 16500: PER (avg): 0.1568 CTC Loss (avg): 15.9026 WER(5gram): 16.30% (n=256) time: 21.775
2026-01-10 07:28:07,459: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=12
2026-01-10 07:28:07,460: t15.2023.08.13 val PER: 0.1112
2026-01-10 07:28:07,460: t15.2023.08.18 val PER: 0.1098
2026-01-10 07:28:07,460: t15.2023.08.20 val PER: 0.1144
2026-01-10 07:28:07,460: t15.2023.08.25 val PER: 0.0828
2026-01-10 07:28:07,460: t15.2023.08.27 val PER: 0.2010
2026-01-10 07:28:07,460: t15.2023.09.01 val PER: 0.0836
2026-01-10 07:28:07,460: t15.2023.09.03 val PER: 0.1639
2026-01-10 07:28:07,460: t15.2023.09.24 val PER: 0.1347
2026-01-10 07:28:07,460: t15.2023.09.29 val PER: 0.1289
2026-01-10 07:28:07,460: t15.2023.10.01 val PER: 0.1797
2026-01-10 07:28:07,460: t15.2023.10.06 val PER: 0.0926
2026-01-10 07:28:07,461: t15.2023.10.08 val PER: 0.2585
2026-01-10 07:28:07,461: t15.2023.10.13 val PER: 0.2110
2026-01-10 07:28:07,461: t15.2023.10.15 val PER: 0.1536
2026-01-10 07:28:07,461: t15.2023.10.20 val PER: 0.1913
2026-01-10 07:28:07,461: t15.2023.10.22 val PER: 0.1180
2026-01-10 07:28:07,461: t15.2023.11.03 val PER: 0.1866
2026-01-10 07:28:07,461: t15.2023.11.04 val PER: 0.0444
2026-01-10 07:28:07,461: t15.2023.11.17 val PER: 0.0482
2026-01-10 07:28:07,461: t15.2023.11.19 val PER: 0.0459
2026-01-10 07:28:07,461: t15.2023.11.26 val PER: 0.1370
2026-01-10 07:28:07,461: t15.2023.12.03 val PER: 0.1239
2026-01-10 07:28:07,461: t15.2023.12.08 val PER: 0.1185
2026-01-10 07:28:07,461: t15.2023.12.10 val PER: 0.0920
2026-01-10 07:28:07,462: t15.2023.12.17 val PER: 0.1528
2026-01-10 07:28:07,462: t15.2023.12.29 val PER: 0.1290
2026-01-10 07:28:07,462: t15.2024.02.25 val PER: 0.1081
2026-01-10 07:28:07,462: t15.2024.03.08 val PER: 0.2418
2026-01-10 07:28:07,462: t15.2024.03.15 val PER: 0.2008
2026-01-10 07:28:07,462: t15.2024.03.17 val PER: 0.1437
2026-01-10 07:28:07,462: t15.2024.05.10 val PER: 0.1590
2026-01-10 07:28:07,462: t15.2024.06.14 val PER: 0.1767
2026-01-10 07:28:07,462: t15.2024.07.19 val PER: 0.2505
2026-01-10 07:28:07,462: t15.2024.07.21 val PER: 0.0910
2026-01-10 07:28:07,462: t15.2024.07.28 val PER: 0.1382
2026-01-10 07:28:07,462: t15.2025.01.10 val PER: 0.3017
2026-01-10 07:28:07,462: t15.2025.01.12 val PER: 0.1555
2026-01-10 07:28:07,463: t15.2025.03.14 val PER: 0.3432
2026-01-10 07:28:07,463: t15.2025.03.16 val PER: 0.2042
2026-01-10 07:28:07,463: t15.2025.03.30 val PER: 0.3103
2026-01-10 07:28:07,463: t15.2025.04.13 val PER: 0.2197
2026-01-10 07:28:07,606: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_16500
2026-01-10 07:28:16,990: Train batch 16600: loss: 9.85 grad norm: 55.23 time: 0.053
2026-01-10 07:28:36,464: Train batch 16800: loss: 18.77 grad norm: 74.62 time: 0.062
2026-01-10 07:28:55,639: Train batch 17000: loss: 9.64 grad norm: 53.64 time: 0.082
2026-01-10 07:28:55,639: Running test after training batch: 17000
2026-01-10 07:28:55,742: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:29:00,831: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:29:00,900: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-10 07:29:16,947: Val batch 17000: PER (avg): 0.1559 CTC Loss (avg): 15.7977 WER(5gram): 15.78% (n=256) time: 21.308
2026-01-10 07:29:16,948: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-10 07:29:16,948: t15.2023.08.13 val PER: 0.1164
2026-01-10 07:29:16,948: t15.2023.08.18 val PER: 0.1132
2026-01-10 07:29:16,948: t15.2023.08.20 val PER: 0.1120
2026-01-10 07:29:16,948: t15.2023.08.25 val PER: 0.0873
2026-01-10 07:29:16,949: t15.2023.08.27 val PER: 0.2010
2026-01-10 07:29:16,949: t15.2023.09.01 val PER: 0.0820
2026-01-10 07:29:16,949: t15.2023.09.03 val PER: 0.1651
2026-01-10 07:29:16,949: t15.2023.09.24 val PER: 0.1323
2026-01-10 07:29:16,949: t15.2023.09.29 val PER: 0.1340
2026-01-10 07:29:16,949: t15.2023.10.01 val PER: 0.1704
2026-01-10 07:29:16,949: t15.2023.10.06 val PER: 0.0861
2026-01-10 07:29:16,949: t15.2023.10.08 val PER: 0.2517
2026-01-10 07:29:16,949: t15.2023.10.13 val PER: 0.2071
2026-01-10 07:29:16,949: t15.2023.10.15 val PER: 0.1608
2026-01-10 07:29:16,949: t15.2023.10.20 val PER: 0.1980
2026-01-10 07:29:16,949: t15.2023.10.22 val PER: 0.1091
2026-01-10 07:29:16,949: t15.2023.11.03 val PER: 0.1839
2026-01-10 07:29:16,949: t15.2023.11.04 val PER: 0.0410
2026-01-10 07:29:16,949: t15.2023.11.17 val PER: 0.0451
2026-01-10 07:29:16,949: t15.2023.11.19 val PER: 0.0399
2026-01-10 07:29:16,949: t15.2023.11.26 val PER: 0.1283
2026-01-10 07:29:16,950: t15.2023.12.03 val PER: 0.1218
2026-01-10 07:29:16,950: t15.2023.12.08 val PER: 0.1119
2026-01-10 07:29:16,950: t15.2023.12.10 val PER: 0.0972
2026-01-10 07:29:16,950: t15.2023.12.17 val PER: 0.1435
2026-01-10 07:29:16,950: t15.2023.12.29 val PER: 0.1297
2026-01-10 07:29:16,950: t15.2024.02.25 val PER: 0.1152
2026-01-10 07:29:16,950: t15.2024.03.08 val PER: 0.2376
2026-01-10 07:29:16,950: t15.2024.03.15 val PER: 0.2045
2026-01-10 07:29:16,950: t15.2024.03.17 val PER: 0.1450
2026-01-10 07:29:16,950: t15.2024.05.10 val PER: 0.1664
2026-01-10 07:29:16,951: t15.2024.06.14 val PER: 0.1672
2026-01-10 07:29:16,951: t15.2024.07.19 val PER: 0.2512
2026-01-10 07:29:16,951: t15.2024.07.21 val PER: 0.0897
2026-01-10 07:29:16,951: t15.2024.07.28 val PER: 0.1331
2026-01-10 07:29:16,951: t15.2025.01.10 val PER: 0.2975
2026-01-10 07:29:16,951: t15.2025.01.12 val PER: 0.1540
2026-01-10 07:29:16,951: t15.2025.03.14 val PER: 0.3491
2026-01-10 07:29:16,951: t15.2025.03.16 val PER: 0.1990
2026-01-10 07:29:16,951: t15.2025.03.30 val PER: 0.3195
2026-01-10 07:29:16,952: t15.2025.04.13 val PER: 0.2282
2026-01-10 07:29:17,086: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_17000
2026-01-10 07:29:35,480: Train batch 17200: loss: 11.74 grad norm: 55.21 time: 0.084
2026-01-10 07:29:54,594: Train batch 17400: loss: 13.74 grad norm: 61.90 time: 0.071
2026-01-10 07:30:03,953: Running test after training batch: 17500
2026-01-10 07:30:04,060: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:30:09,283: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:30:09,352: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-10 07:30:25,701: Val batch 17500: PER (avg): 0.1564 CTC Loss (avg): 15.7816 WER(5gram): 15.78% (n=256) time: 21.747
2026-01-10 07:30:25,702: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-10 07:30:25,702: t15.2023.08.13 val PER: 0.1185
2026-01-10 07:30:25,702: t15.2023.08.18 val PER: 0.1132
2026-01-10 07:30:25,702: t15.2023.08.20 val PER: 0.1176
2026-01-10 07:30:25,702: t15.2023.08.25 val PER: 0.0904
2026-01-10 07:30:25,702: t15.2023.08.27 val PER: 0.1849
2026-01-10 07:30:25,702: t15.2023.09.01 val PER: 0.0820
2026-01-10 07:30:25,702: t15.2023.09.03 val PER: 0.1639
2026-01-10 07:30:25,702: t15.2023.09.24 val PER: 0.1359
2026-01-10 07:30:25,702: t15.2023.09.29 val PER: 0.1308
2026-01-10 07:30:25,702: t15.2023.10.01 val PER: 0.1764
2026-01-10 07:30:25,703: t15.2023.10.06 val PER: 0.0904
2026-01-10 07:30:25,703: t15.2023.10.08 val PER: 0.2571
2026-01-10 07:30:25,703: t15.2023.10.13 val PER: 0.2118
2026-01-10 07:30:25,703: t15.2023.10.15 val PER: 0.1582
2026-01-10 07:30:25,703: t15.2023.10.20 val PER: 0.1879
2026-01-10 07:30:25,703: t15.2023.10.22 val PER: 0.1114
2026-01-10 07:30:25,703: t15.2023.11.03 val PER: 0.1832
2026-01-10 07:30:25,703: t15.2023.11.04 val PER: 0.0341
2026-01-10 07:30:25,703: t15.2023.11.17 val PER: 0.0389
2026-01-10 07:30:25,703: t15.2023.11.19 val PER: 0.0399
2026-01-10 07:30:25,703: t15.2023.11.26 val PER: 0.1319
2026-01-10 07:30:25,703: t15.2023.12.03 val PER: 0.1239
2026-01-10 07:30:25,703: t15.2023.12.08 val PER: 0.1138
2026-01-10 07:30:25,704: t15.2023.12.10 val PER: 0.1078
2026-01-10 07:30:25,704: t15.2023.12.17 val PER: 0.1507
2026-01-10 07:30:25,704: t15.2023.12.29 val PER: 0.1386
2026-01-10 07:30:25,704: t15.2024.02.25 val PER: 0.1096
2026-01-10 07:30:25,704: t15.2024.03.08 val PER: 0.2390
2026-01-10 07:30:25,704: t15.2024.03.15 val PER: 0.1964
2026-01-10 07:30:25,704: t15.2024.03.17 val PER: 0.1457
2026-01-10 07:30:25,704: t15.2024.05.10 val PER: 0.1620
2026-01-10 07:30:25,704: t15.2024.06.14 val PER: 0.1656
2026-01-10 07:30:25,704: t15.2024.07.19 val PER: 0.2446
2026-01-10 07:30:25,704: t15.2024.07.21 val PER: 0.0931
2026-01-10 07:30:25,704: t15.2024.07.28 val PER: 0.1309
2026-01-10 07:30:25,704: t15.2025.01.10 val PER: 0.3085
2026-01-10 07:30:25,704: t15.2025.01.12 val PER: 0.1486
2026-01-10 07:30:25,704: t15.2025.03.14 val PER: 0.3595
2026-01-10 07:30:25,704: t15.2025.03.16 val PER: 0.1950
2026-01-10 07:30:25,705: t15.2025.03.30 val PER: 0.3184
2026-01-10 07:30:25,705: t15.2025.04.13 val PER: 0.2254
2026-01-10 07:30:25,836: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_17500
2026-01-10 07:30:35,077: Train batch 17600: loss: 11.53 grad norm: 58.32 time: 0.055
2026-01-10 07:30:53,916: Train batch 17800: loss: 7.22 grad norm: 52.78 time: 0.044
2026-01-10 07:31:12,684: Train batch 18000: loss: 12.80 grad norm: 66.24 time: 0.063
2026-01-10 07:31:12,685: Running test after training batch: 18000
2026-01-10 07:31:12,793: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:31:17,930: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:31:17,999: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-10 07:31:34,625: Val batch 18000: PER (avg): 0.1559 CTC Loss (avg): 15.7349 WER(5gram): 16.10% (n=256) time: 21.939
2026-01-10 07:31:34,625: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-10 07:31:34,625: t15.2023.08.13 val PER: 0.1112
2026-01-10 07:31:34,625: t15.2023.08.18 val PER: 0.1140
2026-01-10 07:31:34,625: t15.2023.08.20 val PER: 0.1120
2026-01-10 07:31:34,625: t15.2023.08.25 val PER: 0.0889
2026-01-10 07:31:34,625: t15.2023.08.27 val PER: 0.1929
2026-01-10 07:31:34,626: t15.2023.09.01 val PER: 0.0836
2026-01-10 07:31:34,626: t15.2023.09.03 val PER: 0.1591
2026-01-10 07:31:34,626: t15.2023.09.24 val PER: 0.1311
2026-01-10 07:31:34,626: t15.2023.09.29 val PER: 0.1315
2026-01-10 07:31:34,626: t15.2023.10.01 val PER: 0.1816
2026-01-10 07:31:34,626: t15.2023.10.06 val PER: 0.0915
2026-01-10 07:31:34,627: t15.2023.10.08 val PER: 0.2530
2026-01-10 07:31:34,627: t15.2023.10.13 val PER: 0.2133
2026-01-10 07:31:34,627: t15.2023.10.15 val PER: 0.1608
2026-01-10 07:31:34,627: t15.2023.10.20 val PER: 0.1879
2026-01-10 07:31:34,627: t15.2023.10.22 val PER: 0.1091
2026-01-10 07:31:34,627: t15.2023.11.03 val PER: 0.1866
2026-01-10 07:31:34,627: t15.2023.11.04 val PER: 0.0444
2026-01-10 07:31:34,627: t15.2023.11.17 val PER: 0.0404
2026-01-10 07:31:34,627: t15.2023.11.19 val PER: 0.0379
2026-01-10 07:31:34,627: t15.2023.11.26 val PER: 0.1290
2026-01-10 07:31:34,627: t15.2023.12.03 val PER: 0.1176
2026-01-10 07:31:34,627: t15.2023.12.08 val PER: 0.1132
2026-01-10 07:31:34,628: t15.2023.12.10 val PER: 0.0999
2026-01-10 07:31:34,628: t15.2023.12.17 val PER: 0.1414
2026-01-10 07:31:34,628: t15.2023.12.29 val PER: 0.1359
2026-01-10 07:31:34,628: t15.2024.02.25 val PER: 0.1081
2026-01-10 07:31:34,628: t15.2024.03.08 val PER: 0.2404
2026-01-10 07:31:34,628: t15.2024.03.15 val PER: 0.2070
2026-01-10 07:31:34,628: t15.2024.03.17 val PER: 0.1437
2026-01-10 07:31:34,628: t15.2024.05.10 val PER: 0.1560
2026-01-10 07:31:34,628: t15.2024.06.14 val PER: 0.1546
2026-01-10 07:31:34,628: t15.2024.07.19 val PER: 0.2518
2026-01-10 07:31:34,628: t15.2024.07.21 val PER: 0.0993
2026-01-10 07:31:34,628: t15.2024.07.28 val PER: 0.1331
2026-01-10 07:31:34,628: t15.2025.01.10 val PER: 0.3044
2026-01-10 07:31:34,629: t15.2025.01.12 val PER: 0.1486
2026-01-10 07:31:34,629: t15.2025.03.14 val PER: 0.3447
2026-01-10 07:31:34,629: t15.2025.03.16 val PER: 0.1963
2026-01-10 07:31:34,629: t15.2025.03.30 val PER: 0.3046
2026-01-10 07:31:34,629: t15.2025.04.13 val PER: 0.2268
2026-01-10 07:31:34,764: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_18000
2026-01-10 07:31:53,491: Train batch 18200: loss: 9.02 grad norm: 52.64 time: 0.074
2026-01-10 07:32:12,037: Train batch 18400: loss: 5.83 grad norm: 44.66 time: 0.058
2026-01-10 07:32:21,410: Running test after training batch: 18500
2026-01-10 07:32:21,565: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:32:26,765: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:32:26,835: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-10 07:32:42,982: Val batch 18500: PER (avg): 0.1552 CTC Loss (avg): 15.7301 WER(5gram): 15.84% (n=256) time: 21.572
2026-01-10 07:32:42,982: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-10 07:32:42,983: t15.2023.08.13 val PER: 0.1175
2026-01-10 07:32:42,983: t15.2023.08.18 val PER: 0.1090
2026-01-10 07:32:42,983: t15.2023.08.20 val PER: 0.1191
2026-01-10 07:32:42,983: t15.2023.08.25 val PER: 0.0858
2026-01-10 07:32:42,983: t15.2023.08.27 val PER: 0.1945
2026-01-10 07:32:42,983: t15.2023.09.01 val PER: 0.0820
2026-01-10 07:32:42,983: t15.2023.09.03 val PER: 0.1675
2026-01-10 07:32:42,983: t15.2023.09.24 val PER: 0.1335
2026-01-10 07:32:42,983: t15.2023.09.29 val PER: 0.1334
2026-01-10 07:32:42,983: t15.2023.10.01 val PER: 0.1704
2026-01-10 07:32:42,983: t15.2023.10.06 val PER: 0.0893
2026-01-10 07:32:42,983: t15.2023.10.08 val PER: 0.2476
2026-01-10 07:32:42,983: t15.2023.10.13 val PER: 0.2110
2026-01-10 07:32:42,983: t15.2023.10.15 val PER: 0.1602
2026-01-10 07:32:42,984: t15.2023.10.20 val PER: 0.1913
2026-01-10 07:32:42,984: t15.2023.10.22 val PER: 0.1114
2026-01-10 07:32:42,984: t15.2023.11.03 val PER: 0.1839
2026-01-10 07:32:42,984: t15.2023.11.04 val PER: 0.0375
2026-01-10 07:32:42,984: t15.2023.11.17 val PER: 0.0420
2026-01-10 07:32:42,984: t15.2023.11.19 val PER: 0.0379
2026-01-10 07:32:42,984: t15.2023.11.26 val PER: 0.1283
2026-01-10 07:32:42,984: t15.2023.12.03 val PER: 0.1218
2026-01-10 07:32:42,984: t15.2023.12.08 val PER: 0.1119
2026-01-10 07:32:42,984: t15.2023.12.10 val PER: 0.0986
2026-01-10 07:32:42,984: t15.2023.12.17 val PER: 0.1424
2026-01-10 07:32:42,984: t15.2023.12.29 val PER: 0.1352
2026-01-10 07:32:42,984: t15.2024.02.25 val PER: 0.1067
2026-01-10 07:32:42,985: t15.2024.03.08 val PER: 0.2404
2026-01-10 07:32:42,985: t15.2024.03.15 val PER: 0.2039
2026-01-10 07:32:42,985: t15.2024.03.17 val PER: 0.1457
2026-01-10 07:32:42,985: t15.2024.05.10 val PER: 0.1560
2026-01-10 07:32:42,985: t15.2024.06.14 val PER: 0.1688
2026-01-10 07:32:42,985: t15.2024.07.19 val PER: 0.2505
2026-01-10 07:32:42,985: t15.2024.07.21 val PER: 0.0903
2026-01-10 07:32:42,985: t15.2024.07.28 val PER: 0.1279
2026-01-10 07:32:42,985: t15.2025.01.10 val PER: 0.2989
2026-01-10 07:32:42,985: t15.2025.01.12 val PER: 0.1524
2026-01-10 07:32:42,986: t15.2025.03.14 val PER: 0.3580
2026-01-10 07:32:42,986: t15.2025.03.16 val PER: 0.1924
2026-01-10 07:32:42,986: t15.2025.03.30 val PER: 0.2977
2026-01-10 07:32:42,986: t15.2025.04.13 val PER: 0.2268
2026-01-10 07:32:43,121: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_18500
2026-01-10 07:32:52,253: Train batch 18600: loss: 13.59 grad norm: 63.08 time: 0.067
2026-01-10 07:33:10,890: Train batch 18800: loss: 10.06 grad norm: 55.02 time: 0.064
2026-01-10 07:33:29,947: Train batch 19000: loss: 9.88 grad norm: 49.01 time: 0.064
2026-01-10 07:33:29,947: Running test after training batch: 19000
2026-01-10 07:33:30,083: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:33:35,140: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:33:35,218: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-10 07:33:51,355: Val batch 19000: PER (avg): 0.1547 CTC Loss (avg): 15.7257 WER(5gram): 15.71% (n=256) time: 21.407
2026-01-10 07:33:51,355: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-10 07:33:51,355: t15.2023.08.13 val PER: 0.1175
2026-01-10 07:33:51,355: t15.2023.08.18 val PER: 0.1132
2026-01-10 07:33:51,356: t15.2023.08.20 val PER: 0.1152
2026-01-10 07:33:51,356: t15.2023.08.25 val PER: 0.0904
2026-01-10 07:33:51,356: t15.2023.08.27 val PER: 0.1897
2026-01-10 07:33:51,356: t15.2023.09.01 val PER: 0.0828
2026-01-10 07:33:51,356: t15.2023.09.03 val PER: 0.1615
2026-01-10 07:33:51,356: t15.2023.09.24 val PER: 0.1311
2026-01-10 07:33:51,356: t15.2023.09.29 val PER: 0.1302
2026-01-10 07:33:51,356: t15.2023.10.01 val PER: 0.1731
2026-01-10 07:33:51,357: t15.2023.10.06 val PER: 0.0861
2026-01-10 07:33:51,357: t15.2023.10.08 val PER: 0.2490
2026-01-10 07:33:51,357: t15.2023.10.13 val PER: 0.2118
2026-01-10 07:33:51,357: t15.2023.10.15 val PER: 0.1628
2026-01-10 07:33:51,357: t15.2023.10.20 val PER: 0.1946
2026-01-10 07:33:51,357: t15.2023.10.22 val PER: 0.1080
2026-01-10 07:33:51,357: t15.2023.11.03 val PER: 0.1845
2026-01-10 07:33:51,357: t15.2023.11.04 val PER: 0.0375
2026-01-10 07:33:51,357: t15.2023.11.17 val PER: 0.0435
2026-01-10 07:33:51,357: t15.2023.11.19 val PER: 0.0379
2026-01-10 07:33:51,357: t15.2023.11.26 val PER: 0.1333
2026-01-10 07:33:51,357: t15.2023.12.03 val PER: 0.1218
2026-01-10 07:33:51,358: t15.2023.12.08 val PER: 0.1099
2026-01-10 07:33:51,358: t15.2023.12.10 val PER: 0.0986
2026-01-10 07:33:51,358: t15.2023.12.17 val PER: 0.1424
2026-01-10 07:33:51,358: t15.2023.12.29 val PER: 0.1311
2026-01-10 07:33:51,358: t15.2024.02.25 val PER: 0.1067
2026-01-10 07:33:51,358: t15.2024.03.08 val PER: 0.2404
2026-01-10 07:33:51,358: t15.2024.03.15 val PER: 0.1976
2026-01-10 07:33:51,358: t15.2024.03.17 val PER: 0.1485
2026-01-10 07:33:51,358: t15.2024.05.10 val PER: 0.1649
2026-01-10 07:33:51,358: t15.2024.06.14 val PER: 0.1640
2026-01-10 07:33:51,358: t15.2024.07.19 val PER: 0.2459
2026-01-10 07:33:51,358: t15.2024.07.21 val PER: 0.0917
2026-01-10 07:33:51,359: t15.2024.07.28 val PER: 0.1309
2026-01-10 07:33:51,359: t15.2025.01.10 val PER: 0.2975
2026-01-10 07:33:51,359: t15.2025.01.12 val PER: 0.1486
2026-01-10 07:33:51,359: t15.2025.03.14 val PER: 0.3550
2026-01-10 07:33:51,359: t15.2025.03.16 val PER: 0.1963
2026-01-10 07:33:51,359: t15.2025.03.30 val PER: 0.2943
2026-01-10 07:33:51,359: t15.2025.04.13 val PER: 0.2240
2026-01-10 07:33:51,639: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_19000
2026-01-10 07:34:10,276: Train batch 19200: loss: 6.82 grad norm: 49.20 time: 0.064
2026-01-10 07:34:29,147: Train batch 19400: loss: 5.89 grad norm: 40.99 time: 0.053
2026-01-10 07:34:38,691: Running test after training batch: 19500
2026-01-10 07:34:38,834: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:34:43,858: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:34:43,939: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-10 07:34:58,669: Val batch 19500: PER (avg): 0.1545 CTC Loss (avg): 15.6673 WER(5gram): 15.91% (n=256) time: 19.977
2026-01-10 07:34:58,669: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-10 07:34:58,670: t15.2023.08.13 val PER: 0.1112
2026-01-10 07:34:58,670: t15.2023.08.18 val PER: 0.1090
2026-01-10 07:34:58,670: t15.2023.08.20 val PER: 0.1168
2026-01-10 07:34:58,670: t15.2023.08.25 val PER: 0.0858
2026-01-10 07:34:58,670: t15.2023.08.27 val PER: 0.1929
2026-01-10 07:34:58,670: t15.2023.09.01 val PER: 0.0828
2026-01-10 07:34:58,670: t15.2023.09.03 val PER: 0.1591
2026-01-10 07:34:58,670: t15.2023.09.24 val PER: 0.1335
2026-01-10 07:34:58,670: t15.2023.09.29 val PER: 0.1347
2026-01-10 07:34:58,670: t15.2023.10.01 val PER: 0.1737
2026-01-10 07:34:58,670: t15.2023.10.06 val PER: 0.0861
2026-01-10 07:34:58,671: t15.2023.10.08 val PER: 0.2503
2026-01-10 07:34:58,671: t15.2023.10.13 val PER: 0.2133
2026-01-10 07:34:58,671: t15.2023.10.15 val PER: 0.1595
2026-01-10 07:34:58,671: t15.2023.10.20 val PER: 0.1879
2026-01-10 07:34:58,671: t15.2023.10.22 val PER: 0.1125
2026-01-10 07:34:58,671: t15.2023.11.03 val PER: 0.1811
2026-01-10 07:34:58,671: t15.2023.11.04 val PER: 0.0375
2026-01-10 07:34:58,671: t15.2023.11.17 val PER: 0.0420
2026-01-10 07:34:58,671: t15.2023.11.19 val PER: 0.0399
2026-01-10 07:34:58,671: t15.2023.11.26 val PER: 0.1297
2026-01-10 07:34:58,671: t15.2023.12.03 val PER: 0.1197
2026-01-10 07:34:58,671: t15.2023.12.08 val PER: 0.1112
2026-01-10 07:34:58,672: t15.2023.12.10 val PER: 0.0946
2026-01-10 07:34:58,672: t15.2023.12.17 val PER: 0.1455
2026-01-10 07:34:58,672: t15.2023.12.29 val PER: 0.1332
2026-01-10 07:34:58,672: t15.2024.02.25 val PER: 0.1039
2026-01-10 07:34:58,672: t15.2024.03.08 val PER: 0.2390
2026-01-10 07:34:58,672: t15.2024.03.15 val PER: 0.1951
2026-01-10 07:34:58,672: t15.2024.03.17 val PER: 0.1416
2026-01-10 07:34:58,672: t15.2024.05.10 val PER: 0.1620
2026-01-10 07:34:58,672: t15.2024.06.14 val PER: 0.1735
2026-01-10 07:34:58,672: t15.2024.07.19 val PER: 0.2446
2026-01-10 07:34:58,672: t15.2024.07.21 val PER: 0.0931
2026-01-10 07:34:58,672: t15.2024.07.28 val PER: 0.1331
2026-01-10 07:34:58,672: t15.2025.01.10 val PER: 0.2893
2026-01-10 07:34:58,673: t15.2025.01.12 val PER: 0.1501
2026-01-10 07:34:58,673: t15.2025.03.14 val PER: 0.3595
2026-01-10 07:34:58,673: t15.2025.03.16 val PER: 0.1990
2026-01-10 07:34:58,673: t15.2025.03.30 val PER: 0.3034
2026-01-10 07:34:58,673: t15.2025.04.13 val PER: 0.2254
2026-01-10 07:34:58,811: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_19500
2026-01-10 07:35:08,054: Train batch 19600: loss: 8.71 grad norm: 49.75 time: 0.057
2026-01-10 07:35:26,599: Train batch 19800: loss: 9.35 grad norm: 56.21 time: 0.055
2026-01-10 07:35:45,528: Running test after training batch: 19999
2026-01-10 07:35:45,623: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:35:50,623: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-10 07:35:50,703: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-10 07:36:07,549: Val batch 19999: PER (avg): 0.1542 CTC Loss (avg): 15.7012 WER(5gram): 15.51% (n=256) time: 22.021
2026-01-10 07:36:07,549: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-10 07:36:07,550: t15.2023.08.13 val PER: 0.1143
2026-01-10 07:36:07,550: t15.2023.08.18 val PER: 0.1148
2026-01-10 07:36:07,550: t15.2023.08.20 val PER: 0.1144
2026-01-10 07:36:07,550: t15.2023.08.25 val PER: 0.0873
2026-01-10 07:36:07,550: t15.2023.08.27 val PER: 0.1961
2026-01-10 07:36:07,550: t15.2023.09.01 val PER: 0.0844
2026-01-10 07:36:07,550: t15.2023.09.03 val PER: 0.1615
2026-01-10 07:36:07,550: t15.2023.09.24 val PER: 0.1323
2026-01-10 07:36:07,550: t15.2023.09.29 val PER: 0.1334
2026-01-10 07:36:07,551: t15.2023.10.01 val PER: 0.1711
2026-01-10 07:36:07,551: t15.2023.10.06 val PER: 0.0904
2026-01-10 07:36:07,551: t15.2023.10.08 val PER: 0.2490
2026-01-10 07:36:07,551: t15.2023.10.13 val PER: 0.2110
2026-01-10 07:36:07,551: t15.2023.10.15 val PER: 0.1595
2026-01-10 07:36:07,551: t15.2023.10.20 val PER: 0.1846
2026-01-10 07:36:07,551: t15.2023.10.22 val PER: 0.1158
2026-01-10 07:36:07,551: t15.2023.11.03 val PER: 0.1839
2026-01-10 07:36:07,551: t15.2023.11.04 val PER: 0.0341
2026-01-10 07:36:07,551: t15.2023.11.17 val PER: 0.0404
2026-01-10 07:36:07,551: t15.2023.11.19 val PER: 0.0399
2026-01-10 07:36:07,551: t15.2023.11.26 val PER: 0.1283
2026-01-10 07:36:07,551: t15.2023.12.03 val PER: 0.1187
2026-01-10 07:36:07,551: t15.2023.12.08 val PER: 0.1112
2026-01-10 07:36:07,551: t15.2023.12.10 val PER: 0.0986
2026-01-10 07:36:07,552: t15.2023.12.17 val PER: 0.1424
2026-01-10 07:36:07,552: t15.2023.12.29 val PER: 0.1283
2026-01-10 07:36:07,552: t15.2024.02.25 val PER: 0.1039
2026-01-10 07:36:07,552: t15.2024.03.08 val PER: 0.2461
2026-01-10 07:36:07,552: t15.2024.03.15 val PER: 0.2001
2026-01-10 07:36:07,552: t15.2024.03.17 val PER: 0.1423
2026-01-10 07:36:07,552: t15.2024.05.10 val PER: 0.1590
2026-01-10 07:36:07,552: t15.2024.06.14 val PER: 0.1640
2026-01-10 07:36:07,552: t15.2024.07.19 val PER: 0.2485
2026-01-10 07:36:07,552: t15.2024.07.21 val PER: 0.0890
2026-01-10 07:36:07,552: t15.2024.07.28 val PER: 0.1331
2026-01-10 07:36:07,552: t15.2025.01.10 val PER: 0.2920
2026-01-10 07:36:07,553: t15.2025.01.12 val PER: 0.1455
2026-01-10 07:36:07,553: t15.2025.03.14 val PER: 0.3536
2026-01-10 07:36:07,553: t15.2025.03.16 val PER: 0.1924
2026-01-10 07:36:07,553: t15.2025.03.30 val PER: 0.3023
2026-01-10 07:36:07,553: t15.2025.04.13 val PER: 0.2140
2026-01-10 07:36:07,692: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0035/checkpoint/checkpoint_batch_19999
2026-01-10 07:36:08,197: Best avg val PER achieved: 0.16796
2026-01-10 07:36:08,197: Total training time: 51.06 minutes

=== RUN lr_0045.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045
2026-01-10 07:37:59,432: Using device: cuda:0
2026-01-10 07:41:49,262: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-10 07:41:49,284: Using 45 sessions after filtering (from 45).
2026-01-10 07:41:49,736: Using torch.compile (if available)
2026-01-10 07:41:49,737: torch.compile not available (torch<2.0). Skipping.
2026-01-10 07:41:49,737: Initialized RNN decoding model
2026-01-10 07:41:49,737: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-10 07:41:49,737: Model has 44,907,305 parameters
2026-01-10 07:41:49,737: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-10 07:41:51,033: Successfully initialized datasets
2026-01-10 07:41:51,033: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-10 07:41:53,111: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.188
2026-01-10 07:41:53,112: Running test after training batch: 0
2026-01-10 07:41:53,223: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:41:59,320: WER debug example
  GT : you can see the code at this point as well
  PR : she has from his
2026-01-10 07:42:00,503: WER debug example
  GT : how does it keep the cost down
  PR : money from
2026-01-10 07:46:34,458: Val batch 0: PER (avg): 1.4292 CTC Loss (avg): 633.0765 WER(5gram): 99.67% (n=256) time: 281.346
2026-01-10 07:46:34,464: WER lens: avg_true_words=5.99 avg_pred_words=2.82 max_pred_words=7
2026-01-10 07:46:34,470: t15.2023.08.13 val PER: 1.3077
2026-01-10 07:46:34,476: t15.2023.08.18 val PER: 1.4233
2026-01-10 07:46:34,478: t15.2023.08.20 val PER: 1.2994
2026-01-10 07:46:34,478: t15.2023.08.25 val PER: 1.3404
2026-01-10 07:46:34,479: t15.2023.08.27 val PER: 1.2508
2026-01-10 07:46:34,479: t15.2023.09.01 val PER: 1.4562
2026-01-10 07:46:34,479: t15.2023.09.03 val PER: 1.3147
2026-01-10 07:46:34,479: t15.2023.09.24 val PER: 1.5376
2026-01-10 07:46:34,479: t15.2023.09.29 val PER: 1.4684
2026-01-10 07:46:34,479: t15.2023.10.01 val PER: 1.2114
2026-01-10 07:46:34,479: t15.2023.10.06 val PER: 1.4855
2026-01-10 07:46:34,479: t15.2023.10.08 val PER: 1.1867
2026-01-10 07:46:34,479: t15.2023.10.13 val PER: 1.3918
2026-01-10 07:46:34,480: t15.2023.10.15 val PER: 1.3843
2026-01-10 07:46:34,480: t15.2023.10.20 val PER: 1.4966
2026-01-10 07:46:34,480: t15.2023.10.22 val PER: 1.3909
2026-01-10 07:46:34,480: t15.2023.11.03 val PER: 1.5909
2026-01-10 07:46:34,480: t15.2023.11.04 val PER: 2.0375
2026-01-10 07:46:34,481: t15.2023.11.17 val PER: 1.9549
2026-01-10 07:46:34,481: t15.2023.11.19 val PER: 1.6747
2026-01-10 07:46:34,481: t15.2023.11.26 val PER: 1.5428
2026-01-10 07:46:34,481: t15.2023.12.03 val PER: 1.4212
2026-01-10 07:46:34,481: t15.2023.12.08 val PER: 1.4501
2026-01-10 07:46:34,481: t15.2023.12.10 val PER: 1.6978
2026-01-10 07:46:34,481: t15.2023.12.17 val PER: 1.3087
2026-01-10 07:46:34,481: t15.2023.12.29 val PER: 1.4070
2026-01-10 07:46:34,481: t15.2024.02.25 val PER: 1.4284
2026-01-10 07:46:34,482: t15.2024.03.08 val PER: 1.3215
2026-01-10 07:46:34,482: t15.2024.03.15 val PER: 1.3177
2026-01-10 07:46:34,482: t15.2024.03.17 val PER: 1.4038
2026-01-10 07:46:34,482: t15.2024.05.10 val PER: 1.3150
2026-01-10 07:46:34,482: t15.2024.06.14 val PER: 1.5237
2026-01-10 07:46:34,482: t15.2024.07.19 val PER: 1.0831
2026-01-10 07:46:34,482: t15.2024.07.21 val PER: 1.6345
2026-01-10 07:46:34,482: t15.2024.07.28 val PER: 1.6588
2026-01-10 07:46:34,483: t15.2025.01.10 val PER: 1.0950
2026-01-10 07:46:34,483: t15.2025.01.12 val PER: 1.7652
2026-01-10 07:46:34,483: t15.2025.03.14 val PER: 1.0355
2026-01-10 07:46:34,483: t15.2025.03.16 val PER: 1.6217
2026-01-10 07:46:34,483: t15.2025.03.30 val PER: 1.2897
2026-01-10 07:46:34,483: t15.2025.04.13 val PER: 1.5892
2026-01-10 07:46:34,484: New best val WER(5gram) inf% --> 99.67%
2026-01-10 07:46:34,670: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_0
2026-01-10 07:46:53,441: Train batch 200: loss: 77.91 grad norm: 137.46 time: 0.054
2026-01-10 07:47:11,972: Train batch 400: loss: 52.15 grad norm: 80.40 time: 0.064
2026-01-10 07:47:21,464: Running test after training batch: 500
2026-01-10 07:47:21,776: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:47:27,390: WER debug example
  GT : you can see the code at this point as well
  PR : e e t e t t t t
2026-01-10 07:47:27,631: WER debug example
  GT : how does it keep the cost down
  PR : is it to the us
2026-01-10 07:48:20,198: Val batch 500: PER (avg): 0.4913 CTC Loss (avg): 51.5691 WER(5gram): 97.91% (n=256) time: 58.734
2026-01-10 07:48:20,199: WER lens: avg_true_words=5.99 avg_pred_words=4.75 max_pred_words=11
2026-01-10 07:48:20,199: t15.2023.08.13 val PER: 0.4470
2026-01-10 07:48:20,199: t15.2023.08.18 val PER: 0.4392
2026-01-10 07:48:20,199: t15.2023.08.20 val PER: 0.4369
2026-01-10 07:48:20,199: t15.2023.08.25 val PER: 0.4081
2026-01-10 07:48:20,200: t15.2023.08.27 val PER: 0.5177
2026-01-10 07:48:20,200: t15.2023.09.01 val PER: 0.4115
2026-01-10 07:48:20,200: t15.2023.09.03 val PER: 0.4774
2026-01-10 07:48:20,200: t15.2023.09.24 val PER: 0.4248
2026-01-10 07:48:20,200: t15.2023.09.29 val PER: 0.4237
2026-01-10 07:48:20,200: t15.2023.10.01 val PER: 0.4841
2026-01-10 07:48:20,200: t15.2023.10.06 val PER: 0.4101
2026-01-10 07:48:20,201: t15.2023.10.08 val PER: 0.5359
2026-01-10 07:48:20,201: t15.2023.10.13 val PER: 0.5314
2026-01-10 07:48:20,201: t15.2023.10.15 val PER: 0.4680
2026-01-10 07:48:20,201: t15.2023.10.20 val PER: 0.4329
2026-01-10 07:48:20,201: t15.2023.10.22 val PER: 0.4399
2026-01-10 07:48:20,201: t15.2023.11.03 val PER: 0.4668
2026-01-10 07:48:20,201: t15.2023.11.04 val PER: 0.2867
2026-01-10 07:48:20,201: t15.2023.11.17 val PER: 0.3437
2026-01-10 07:48:20,201: t15.2023.11.19 val PER: 0.3194
2026-01-10 07:48:20,201: t15.2023.11.26 val PER: 0.5109
2026-01-10 07:48:20,201: t15.2023.12.03 val PER: 0.4737
2026-01-10 07:48:20,201: t15.2023.12.08 val PER: 0.4933
2026-01-10 07:48:20,201: t15.2023.12.10 val PER: 0.4205
2026-01-10 07:48:20,201: t15.2023.12.17 val PER: 0.5343
2026-01-10 07:48:20,201: t15.2023.12.29 val PER: 0.5072
2026-01-10 07:48:20,201: t15.2024.02.25 val PER: 0.4621
2026-01-10 07:48:20,202: t15.2024.03.08 val PER: 0.5491
2026-01-10 07:48:20,202: t15.2024.03.15 val PER: 0.5260
2026-01-10 07:48:20,202: t15.2024.03.17 val PER: 0.4847
2026-01-10 07:48:20,202: t15.2024.05.10 val PER: 0.4963
2026-01-10 07:48:20,202: t15.2024.06.14 val PER: 0.5205
2026-01-10 07:48:20,202: t15.2024.07.19 val PER: 0.6480
2026-01-10 07:48:20,202: t15.2024.07.21 val PER: 0.4724
2026-01-10 07:48:20,202: t15.2024.07.28 val PER: 0.4772
2026-01-10 07:48:20,202: t15.2025.01.10 val PER: 0.6997
2026-01-10 07:48:20,202: t15.2025.01.12 val PER: 0.5173
2026-01-10 07:48:20,202: t15.2025.03.14 val PER: 0.6820
2026-01-10 07:48:20,202: t15.2025.03.16 val PER: 0.5654
2026-01-10 07:48:20,203: t15.2025.03.30 val PER: 0.6839
2026-01-10 07:48:20,203: t15.2025.04.13 val PER: 0.5207
2026-01-10 07:48:20,204: New best val WER(5gram) 99.67% --> 97.91%
2026-01-10 07:48:20,392: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_500
2026-01-10 07:48:29,775: Train batch 600: loss: 47.00 grad norm: 75.25 time: 0.079
2026-01-10 07:48:48,471: Train batch 800: loss: 40.42 grad norm: 83.49 time: 0.057
2026-01-10 07:49:07,371: Train batch 1000: loss: 42.09 grad norm: 73.36 time: 0.067
2026-01-10 07:49:07,372: Running test after training batch: 1000
2026-01-10 07:49:07,525: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:49:12,755: WER debug example
  GT : you can see the code at this point as well
  PR : you ou a t e t t t t t t y li
2026-01-10 07:49:12,878: WER debug example
  GT : how does it keep the cost down
  PR : d c c t e p a c to
2026-01-10 07:49:43,837: Val batch 1000: PER (avg): 0.4152 CTC Loss (avg): 41.1897 WER(5gram): 113.10% (n=256) time: 36.460
2026-01-10 07:49:43,837: WER lens: avg_true_words=5.99 avg_pred_words=6.97 max_pred_words=17
2026-01-10 07:49:43,837: t15.2023.08.13 val PER: 0.4012
2026-01-10 07:49:43,837: t15.2023.08.18 val PER: 0.3479
2026-01-10 07:49:43,838: t15.2023.08.20 val PER: 0.3431
2026-01-10 07:49:43,838: t15.2023.08.25 val PER: 0.3208
2026-01-10 07:49:43,838: t15.2023.08.27 val PER: 0.4293
2026-01-10 07:49:43,838: t15.2023.09.01 val PER: 0.3109
2026-01-10 07:49:43,838: t15.2023.09.03 val PER: 0.3860
2026-01-10 07:49:43,838: t15.2023.09.24 val PER: 0.3544
2026-01-10 07:49:43,838: t15.2023.09.29 val PER: 0.3714
2026-01-10 07:49:43,838: t15.2023.10.01 val PER: 0.4267
2026-01-10 07:49:43,838: t15.2023.10.06 val PER: 0.3272
2026-01-10 07:49:43,839: t15.2023.10.08 val PER: 0.4533
2026-01-10 07:49:43,839: t15.2023.10.13 val PER: 0.4663
2026-01-10 07:49:43,839: t15.2023.10.15 val PER: 0.4008
2026-01-10 07:49:43,839: t15.2023.10.20 val PER: 0.3490
2026-01-10 07:49:43,839: t15.2023.10.22 val PER: 0.3597
2026-01-10 07:49:43,839: t15.2023.11.03 val PER: 0.4071
2026-01-10 07:49:43,839: t15.2023.11.04 val PER: 0.1877
2026-01-10 07:49:43,839: t15.2023.11.17 val PER: 0.2628
2026-01-10 07:49:43,839: t15.2023.11.19 val PER: 0.2355
2026-01-10 07:49:43,840: t15.2023.11.26 val PER: 0.4420
2026-01-10 07:49:43,840: t15.2023.12.03 val PER: 0.3929
2026-01-10 07:49:43,840: t15.2023.12.08 val PER: 0.4101
2026-01-10 07:49:43,840: t15.2023.12.10 val PER: 0.3601
2026-01-10 07:49:43,840: t15.2023.12.17 val PER: 0.4116
2026-01-10 07:49:43,840: t15.2023.12.29 val PER: 0.4166
2026-01-10 07:49:43,840: t15.2024.02.25 val PER: 0.3385
2026-01-10 07:49:43,840: t15.2024.03.08 val PER: 0.4979
2026-01-10 07:49:43,840: t15.2024.03.15 val PER: 0.4522
2026-01-10 07:49:43,841: t15.2024.03.17 val PER: 0.4184
2026-01-10 07:49:43,841: t15.2024.05.10 val PER: 0.4175
2026-01-10 07:49:43,841: t15.2024.06.14 val PER: 0.4117
2026-01-10 07:49:43,841: t15.2024.07.19 val PER: 0.5445
2026-01-10 07:49:43,841: t15.2024.07.21 val PER: 0.3931
2026-01-10 07:49:43,841: t15.2024.07.28 val PER: 0.4096
2026-01-10 07:49:43,841: t15.2025.01.10 val PER: 0.6281
2026-01-10 07:49:43,841: t15.2025.01.12 val PER: 0.4365
2026-01-10 07:49:43,841: t15.2025.03.14 val PER: 0.6331
2026-01-10 07:49:43,842: t15.2025.03.16 val PER: 0.5118
2026-01-10 07:49:43,842: t15.2025.03.30 val PER: 0.6471
2026-01-10 07:49:43,842: t15.2025.04.13 val PER: 0.4836
2026-01-10 07:49:43,980: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_1000
2026-01-10 07:50:02,518: Train batch 1200: loss: 33.47 grad norm: 75.58 time: 0.069
2026-01-10 07:50:21,921: Train batch 1400: loss: 35.67 grad norm: 74.12 time: 0.061
2026-01-10 07:50:31,418: Running test after training batch: 1500
2026-01-10 07:50:31,526: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:50:36,872: WER debug example
  GT : you can see the code at this point as well
  PR : ou no e a d d d y
2026-01-10 07:50:36,928: WER debug example
  GT : how does it keep the cost down
  PR : c t p a c and
2026-01-10 07:51:00,856: Val batch 1500: PER (avg): 0.3742 CTC Loss (avg): 36.1177 WER(5gram): 103.26% (n=256) time: 29.437
2026-01-10 07:51:00,856: WER lens: avg_true_words=5.99 avg_pred_words=5.96 max_pred_words=14
2026-01-10 07:51:00,857: t15.2023.08.13 val PER: 0.3326
2026-01-10 07:51:00,857: t15.2023.08.18 val PER: 0.3143
2026-01-10 07:51:00,857: t15.2023.08.20 val PER: 0.3066
2026-01-10 07:51:00,857: t15.2023.08.25 val PER: 0.2681
2026-01-10 07:51:00,857: t15.2023.08.27 val PER: 0.3746
2026-01-10 07:51:00,857: t15.2023.09.01 val PER: 0.2654
2026-01-10 07:51:00,857: t15.2023.09.03 val PER: 0.3575
2026-01-10 07:51:00,857: t15.2023.09.24 val PER: 0.3070
2026-01-10 07:51:00,858: t15.2023.09.29 val PER: 0.3248
2026-01-10 07:51:00,858: t15.2023.10.01 val PER: 0.3811
2026-01-10 07:51:00,858: t15.2023.10.06 val PER: 0.2949
2026-01-10 07:51:00,858: t15.2023.10.08 val PER: 0.4235
2026-01-10 07:51:00,858: t15.2023.10.13 val PER: 0.4337
2026-01-10 07:51:00,858: t15.2023.10.15 val PER: 0.3599
2026-01-10 07:51:00,858: t15.2023.10.20 val PER: 0.3289
2026-01-10 07:51:00,858: t15.2023.10.22 val PER: 0.3062
2026-01-10 07:51:00,858: t15.2023.11.03 val PER: 0.3684
2026-01-10 07:51:00,858: t15.2023.11.04 val PER: 0.1365
2026-01-10 07:51:00,859: t15.2023.11.17 val PER: 0.2457
2026-01-10 07:51:00,859: t15.2023.11.19 val PER: 0.1976
2026-01-10 07:51:00,859: t15.2023.11.26 val PER: 0.4203
2026-01-10 07:51:00,859: t15.2023.12.03 val PER: 0.3382
2026-01-10 07:51:00,859: t15.2023.12.08 val PER: 0.3575
2026-01-10 07:51:00,859: t15.2023.12.10 val PER: 0.3022
2026-01-10 07:51:00,859: t15.2023.12.17 val PER: 0.3773
2026-01-10 07:51:00,859: t15.2023.12.29 val PER: 0.3747
2026-01-10 07:51:00,859: t15.2024.02.25 val PER: 0.3104
2026-01-10 07:51:00,859: t15.2024.03.08 val PER: 0.4438
2026-01-10 07:51:00,860: t15.2024.03.15 val PER: 0.4134
2026-01-10 07:51:00,860: t15.2024.03.17 val PER: 0.3745
2026-01-10 07:51:00,860: t15.2024.05.10 val PER: 0.3952
2026-01-10 07:51:00,860: t15.2024.06.14 val PER: 0.3991
2026-01-10 07:51:00,860: t15.2024.07.19 val PER: 0.5162
2026-01-10 07:51:00,860: t15.2024.07.21 val PER: 0.3393
2026-01-10 07:51:00,860: t15.2024.07.28 val PER: 0.3559
2026-01-10 07:51:00,860: t15.2025.01.10 val PER: 0.6047
2026-01-10 07:51:00,860: t15.2025.01.12 val PER: 0.3888
2026-01-10 07:51:00,860: t15.2025.03.14 val PER: 0.5917
2026-01-10 07:51:00,861: t15.2025.03.16 val PER: 0.4306
2026-01-10 07:51:00,861: t15.2025.03.30 val PER: 0.6402
2026-01-10 07:51:00,861: t15.2025.04.13 val PER: 0.4379
2026-01-10 07:51:00,997: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_1500
2026-01-10 07:51:10,233: Train batch 1600: loss: 35.52 grad norm: 79.20 time: 0.063
2026-01-10 07:51:28,943: Train batch 1800: loss: 34.58 grad norm: 73.37 time: 0.088
2026-01-10 07:51:47,785: Train batch 2000: loss: 32.61 grad norm: 74.67 time: 0.067
2026-01-10 07:51:47,786: Running test after training batch: 2000
2026-01-10 07:51:47,957: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:51:53,336: WER debug example
  GT : you can see the code at this point as well
  PR : you ou no e a due to the us and we
2026-01-10 07:51:53,395: WER debug example
  GT : how does it keep the cost down
  PR : c t e p a a
2026-01-10 07:52:13,568: Val batch 2000: PER (avg): 0.3320 CTC Loss (avg): 31.9532 WER(5gram): 96.54% (n=256) time: 25.782
2026-01-10 07:52:13,569: WER lens: avg_true_words=5.99 avg_pred_words=5.87 max_pred_words=14
2026-01-10 07:52:13,569: t15.2023.08.13 val PER: 0.3046
2026-01-10 07:52:13,569: t15.2023.08.18 val PER: 0.2783
2026-01-10 07:52:13,569: t15.2023.08.20 val PER: 0.2510
2026-01-10 07:52:13,569: t15.2023.08.25 val PER: 0.2319
2026-01-10 07:52:13,569: t15.2023.08.27 val PER: 0.3408
2026-01-10 07:52:13,569: t15.2023.09.01 val PER: 0.2321
2026-01-10 07:52:13,569: t15.2023.09.03 val PER: 0.3361
2026-01-10 07:52:13,569: t15.2023.09.24 val PER: 0.2621
2026-01-10 07:52:13,569: t15.2023.09.29 val PER: 0.2897
2026-01-10 07:52:13,570: t15.2023.10.01 val PER: 0.3349
2026-01-10 07:52:13,570: t15.2023.10.06 val PER: 0.2465
2026-01-10 07:52:13,570: t15.2023.10.08 val PER: 0.3857
2026-01-10 07:52:13,570: t15.2023.10.13 val PER: 0.3747
2026-01-10 07:52:13,570: t15.2023.10.15 val PER: 0.2960
2026-01-10 07:52:13,570: t15.2023.10.20 val PER: 0.3121
2026-01-10 07:52:13,570: t15.2023.10.22 val PER: 0.2650
2026-01-10 07:52:13,570: t15.2023.11.03 val PER: 0.3297
2026-01-10 07:52:13,570: t15.2023.11.04 val PER: 0.1092
2026-01-10 07:52:13,570: t15.2023.11.17 val PER: 0.1991
2026-01-10 07:52:13,570: t15.2023.11.19 val PER: 0.1597
2026-01-10 07:52:13,570: t15.2023.11.26 val PER: 0.3565
2026-01-10 07:52:13,571: t15.2023.12.03 val PER: 0.3057
2026-01-10 07:52:13,571: t15.2023.12.08 val PER: 0.3149
2026-01-10 07:52:13,571: t15.2023.12.10 val PER: 0.2707
2026-01-10 07:52:13,571: t15.2023.12.17 val PER: 0.3025
2026-01-10 07:52:13,571: t15.2023.12.29 val PER: 0.3397
2026-01-10 07:52:13,571: t15.2024.02.25 val PER: 0.2809
2026-01-10 07:52:13,571: t15.2024.03.08 val PER: 0.4011
2026-01-10 07:52:13,571: t15.2024.03.15 val PER: 0.3702
2026-01-10 07:52:13,571: t15.2024.03.17 val PER: 0.3431
2026-01-10 07:52:13,571: t15.2024.05.10 val PER: 0.3224
2026-01-10 07:52:13,571: t15.2024.06.14 val PER: 0.3407
2026-01-10 07:52:13,571: t15.2024.07.19 val PER: 0.4740
2026-01-10 07:52:13,571: t15.2024.07.21 val PER: 0.3014
2026-01-10 07:52:13,571: t15.2024.07.28 val PER: 0.3287
2026-01-10 07:52:13,571: t15.2025.01.10 val PER: 0.5413
2026-01-10 07:52:13,572: t15.2025.01.12 val PER: 0.3695
2026-01-10 07:52:13,572: t15.2025.03.14 val PER: 0.5429
2026-01-10 07:52:13,572: t15.2025.03.16 val PER: 0.3822
2026-01-10 07:52:13,572: t15.2025.03.30 val PER: 0.5954
2026-01-10 07:52:13,572: t15.2025.04.13 val PER: 0.4108
2026-01-10 07:52:13,573: New best val WER(5gram) 97.91% --> 96.54%
2026-01-10 07:52:13,765: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_2000
2026-01-10 07:52:32,548: Train batch 2200: loss: 28.66 grad norm: 65.65 time: 0.063
2026-01-10 07:52:51,240: Train batch 2400: loss: 28.46 grad norm: 67.45 time: 0.053
2026-01-10 07:53:00,810: Running test after training batch: 2500
2026-01-10 07:53:00,987: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:53:06,296: WER debug example
  GT : you can see the code at this point as well
  PR : you ou no e a do it this way
2026-01-10 07:53:06,333: WER debug example
  GT : how does it keep the cost down
  PR : how is it a
2026-01-10 07:53:22,609: Val batch 2500: PER (avg): 0.3068 CTC Loss (avg): 28.9688 WER(5gram): 82.99% (n=256) time: 21.798
2026-01-10 07:53:22,610: WER lens: avg_true_words=5.99 avg_pred_words=5.66 max_pred_words=12
2026-01-10 07:53:22,610: t15.2023.08.13 val PER: 0.2661
2026-01-10 07:53:22,610: t15.2023.08.18 val PER: 0.2490
2026-01-10 07:53:22,610: t15.2023.08.20 val PER: 0.2343
2026-01-10 07:53:22,610: t15.2023.08.25 val PER: 0.2169
2026-01-10 07:53:22,610: t15.2023.08.27 val PER: 0.3328
2026-01-10 07:53:22,611: t15.2023.09.01 val PER: 0.2183
2026-01-10 07:53:22,611: t15.2023.09.03 val PER: 0.3017
2026-01-10 07:53:22,611: t15.2023.09.24 val PER: 0.2391
2026-01-10 07:53:22,611: t15.2023.09.29 val PER: 0.2623
2026-01-10 07:53:22,611: t15.2023.10.01 val PER: 0.3170
2026-01-10 07:53:22,611: t15.2023.10.06 val PER: 0.2217
2026-01-10 07:53:22,611: t15.2023.10.08 val PER: 0.3613
2026-01-10 07:53:22,611: t15.2023.10.13 val PER: 0.3600
2026-01-10 07:53:22,611: t15.2023.10.15 val PER: 0.2782
2026-01-10 07:53:22,611: t15.2023.10.20 val PER: 0.2953
2026-01-10 07:53:22,612: t15.2023.10.22 val PER: 0.2606
2026-01-10 07:53:22,612: t15.2023.11.03 val PER: 0.3012
2026-01-10 07:53:22,612: t15.2023.11.04 val PER: 0.0922
2026-01-10 07:53:22,612: t15.2023.11.17 val PER: 0.1586
2026-01-10 07:53:22,612: t15.2023.11.19 val PER: 0.1377
2026-01-10 07:53:22,612: t15.2023.11.26 val PER: 0.3254
2026-01-10 07:53:22,612: t15.2023.12.03 val PER: 0.2595
2026-01-10 07:53:22,612: t15.2023.12.08 val PER: 0.2863
2026-01-10 07:53:22,612: t15.2023.12.10 val PER: 0.2352
2026-01-10 07:53:22,612: t15.2023.12.17 val PER: 0.2911
2026-01-10 07:53:22,612: t15.2023.12.29 val PER: 0.3109
2026-01-10 07:53:22,612: t15.2024.02.25 val PER: 0.2472
2026-01-10 07:53:22,613: t15.2024.03.08 val PER: 0.3514
2026-01-10 07:53:22,613: t15.2024.03.15 val PER: 0.3421
2026-01-10 07:53:22,613: t15.2024.03.17 val PER: 0.3257
2026-01-10 07:53:22,613: t15.2024.05.10 val PER: 0.2957
2026-01-10 07:53:22,613: t15.2024.06.14 val PER: 0.3233
2026-01-10 07:53:22,613: t15.2024.07.19 val PER: 0.4496
2026-01-10 07:53:22,613: t15.2024.07.21 val PER: 0.2669
2026-01-10 07:53:22,613: t15.2024.07.28 val PER: 0.2993
2026-01-10 07:53:22,613: t15.2025.01.10 val PER: 0.5083
2026-01-10 07:53:22,613: t15.2025.01.12 val PER: 0.3610
2026-01-10 07:53:22,613: t15.2025.03.14 val PER: 0.5089
2026-01-10 07:53:22,613: t15.2025.03.16 val PER: 0.3613
2026-01-10 07:53:22,614: t15.2025.03.30 val PER: 0.5437
2026-01-10 07:53:22,614: t15.2025.04.13 val PER: 0.3980
2026-01-10 07:53:22,614: New best val WER(5gram) 96.54% --> 82.99%
2026-01-10 07:53:22,813: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_2500
2026-01-10 07:53:32,154: Train batch 2600: loss: 33.60 grad norm: 77.22 time: 0.055
2026-01-10 07:53:50,776: Train batch 2800: loss: 25.32 grad norm: 76.24 time: 0.082
2026-01-10 07:54:09,520: Train batch 3000: loss: 31.93 grad norm: 80.11 time: 0.085
2026-01-10 07:54:09,521: Running test after training batch: 3000
2026-01-10 07:54:09,643: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:54:14,729: WER debug example
  GT : you can see the code at this point as well
  PR : u d e a do this we
2026-01-10 07:54:14,772: WER debug example
  GT : how does it keep the cost down
  PR : how is it the
2026-01-10 07:54:28,631: Val batch 3000: PER (avg): 0.2841 CTC Loss (avg): 27.1884 WER(5gram): 79.40% (n=256) time: 19.110
2026-01-10 07:54:28,632: WER lens: avg_true_words=5.99 avg_pred_words=5.75 max_pred_words=14
2026-01-10 07:54:28,632: t15.2023.08.13 val PER: 0.2536
2026-01-10 07:54:28,632: t15.2023.08.18 val PER: 0.2313
2026-01-10 07:54:28,633: t15.2023.08.20 val PER: 0.2240
2026-01-10 07:54:28,633: t15.2023.08.25 val PER: 0.1928
2026-01-10 07:54:28,633: t15.2023.08.27 val PER: 0.3087
2026-01-10 07:54:28,633: t15.2023.09.01 val PER: 0.1899
2026-01-10 07:54:28,633: t15.2023.09.03 val PER: 0.2910
2026-01-10 07:54:28,633: t15.2023.09.24 val PER: 0.2160
2026-01-10 07:54:28,633: t15.2023.09.29 val PER: 0.2361
2026-01-10 07:54:28,633: t15.2023.10.01 val PER: 0.2979
2026-01-10 07:54:28,634: t15.2023.10.06 val PER: 0.2024
2026-01-10 07:54:28,634: t15.2023.10.08 val PER: 0.3491
2026-01-10 07:54:28,634: t15.2023.10.13 val PER: 0.3421
2026-01-10 07:54:28,634: t15.2023.10.15 val PER: 0.2551
2026-01-10 07:54:28,634: t15.2023.10.20 val PER: 0.2718
2026-01-10 07:54:28,634: t15.2023.10.22 val PER: 0.2238
2026-01-10 07:54:28,634: t15.2023.11.03 val PER: 0.2788
2026-01-10 07:54:28,634: t15.2023.11.04 val PER: 0.0956
2026-01-10 07:54:28,634: t15.2023.11.17 val PER: 0.1540
2026-01-10 07:54:28,634: t15.2023.11.19 val PER: 0.1218
2026-01-10 07:54:28,635: t15.2023.11.26 val PER: 0.3051
2026-01-10 07:54:28,635: t15.2023.12.03 val PER: 0.2553
2026-01-10 07:54:28,635: t15.2023.12.08 val PER: 0.2690
2026-01-10 07:54:28,635: t15.2023.12.10 val PER: 0.2378
2026-01-10 07:54:28,635: t15.2023.12.17 val PER: 0.2723
2026-01-10 07:54:28,635: t15.2023.12.29 val PER: 0.2910
2026-01-10 07:54:28,635: t15.2024.02.25 val PER: 0.2542
2026-01-10 07:54:28,635: t15.2024.03.08 val PER: 0.3485
2026-01-10 07:54:28,635: t15.2024.03.15 val PER: 0.3321
2026-01-10 07:54:28,635: t15.2024.03.17 val PER: 0.2831
2026-01-10 07:54:28,636: t15.2024.05.10 val PER: 0.2897
2026-01-10 07:54:28,636: t15.2024.06.14 val PER: 0.2950
2026-01-10 07:54:28,636: t15.2024.07.19 val PER: 0.4001
2026-01-10 07:54:28,636: t15.2024.07.21 val PER: 0.2338
2026-01-10 07:54:28,636: t15.2024.07.28 val PER: 0.2868
2026-01-10 07:54:28,636: t15.2025.01.10 val PER: 0.4725
2026-01-10 07:54:28,636: t15.2025.01.12 val PER: 0.3133
2026-01-10 07:54:28,636: t15.2025.03.14 val PER: 0.4645
2026-01-10 07:54:28,636: t15.2025.03.16 val PER: 0.3338
2026-01-10 07:54:28,637: t15.2025.03.30 val PER: 0.4874
2026-01-10 07:54:28,637: t15.2025.04.13 val PER: 0.3509
2026-01-10 07:54:28,637: New best val WER(5gram) 82.99% --> 79.40%
2026-01-10 07:54:28,833: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_3000
2026-01-10 07:54:47,584: Train batch 3200: loss: 25.46 grad norm: 76.43 time: 0.076
2026-01-10 07:55:06,325: Train batch 3400: loss: 18.32 grad norm: 60.54 time: 0.049
2026-01-10 07:55:15,887: Running test after training batch: 3500
2026-01-10 07:55:16,034: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:55:21,442: WER debug example
  GT : you can see the code at this point as well
  PR : you can see a cold at this point we
2026-01-10 07:55:21,490: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 07:55:35,195: Val batch 3500: PER (avg): 0.2723 CTC Loss (avg): 26.2072 WER(5gram): 63.10% (n=256) time: 19.307
2026-01-10 07:55:35,196: WER lens: avg_true_words=5.99 avg_pred_words=5.71 max_pred_words=13
2026-01-10 07:55:35,196: t15.2023.08.13 val PER: 0.2297
2026-01-10 07:55:35,196: t15.2023.08.18 val PER: 0.2238
2026-01-10 07:55:35,196: t15.2023.08.20 val PER: 0.2160
2026-01-10 07:55:35,196: t15.2023.08.25 val PER: 0.1852
2026-01-10 07:55:35,197: t15.2023.08.27 val PER: 0.2878
2026-01-10 07:55:35,197: t15.2023.09.01 val PER: 0.1810
2026-01-10 07:55:35,197: t15.2023.09.03 val PER: 0.2637
2026-01-10 07:55:35,197: t15.2023.09.24 val PER: 0.1954
2026-01-10 07:55:35,197: t15.2023.09.29 val PER: 0.2355
2026-01-10 07:55:35,197: t15.2023.10.01 val PER: 0.2893
2026-01-10 07:55:35,197: t15.2023.10.06 val PER: 0.1841
2026-01-10 07:55:35,197: t15.2023.10.08 val PER: 0.3586
2026-01-10 07:55:35,197: t15.2023.10.13 val PER: 0.3251
2026-01-10 07:55:35,197: t15.2023.10.15 val PER: 0.2512
2026-01-10 07:55:35,198: t15.2023.10.20 val PER: 0.2315
2026-01-10 07:55:35,198: t15.2023.10.22 val PER: 0.2094
2026-01-10 07:55:35,198: t15.2023.11.03 val PER: 0.2619
2026-01-10 07:55:35,198: t15.2023.11.04 val PER: 0.0785
2026-01-10 07:55:35,198: t15.2023.11.17 val PER: 0.1213
2026-01-10 07:55:35,198: t15.2023.11.19 val PER: 0.1198
2026-01-10 07:55:35,198: t15.2023.11.26 val PER: 0.2703
2026-01-10 07:55:35,198: t15.2023.12.03 val PER: 0.2616
2026-01-10 07:55:35,198: t15.2023.12.08 val PER: 0.2557
2026-01-10 07:55:35,198: t15.2023.12.10 val PER: 0.2011
2026-01-10 07:55:35,198: t15.2023.12.17 val PER: 0.2557
2026-01-10 07:55:35,198: t15.2023.12.29 val PER: 0.2677
2026-01-10 07:55:35,199: t15.2024.02.25 val PER: 0.2191
2026-01-10 07:55:35,199: t15.2024.03.08 val PER: 0.3229
2026-01-10 07:55:35,199: t15.2024.03.15 val PER: 0.3258
2026-01-10 07:55:35,199: t15.2024.03.17 val PER: 0.2971
2026-01-10 07:55:35,199: t15.2024.05.10 val PER: 0.2630
2026-01-10 07:55:35,199: t15.2024.06.14 val PER: 0.2902
2026-01-10 07:55:35,199: t15.2024.07.19 val PER: 0.3988
2026-01-10 07:55:35,200: t15.2024.07.21 val PER: 0.2324
2026-01-10 07:55:35,200: t15.2024.07.28 val PER: 0.2691
2026-01-10 07:55:35,200: t15.2025.01.10 val PER: 0.4807
2026-01-10 07:55:35,200: t15.2025.01.12 val PER: 0.2987
2026-01-10 07:55:35,200: t15.2025.03.14 val PER: 0.4689
2026-01-10 07:55:35,200: t15.2025.03.16 val PER: 0.3298
2026-01-10 07:55:35,200: t15.2025.03.30 val PER: 0.4690
2026-01-10 07:55:35,200: t15.2025.04.13 val PER: 0.3552
2026-01-10 07:55:35,201: New best val WER(5gram) 79.40% --> 63.10%
2026-01-10 07:55:35,400: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_3500
2026-01-10 07:55:45,022: Train batch 3600: loss: 21.71 grad norm: 63.16 time: 0.067
2026-01-10 07:56:03,878: Train batch 3800: loss: 24.41 grad norm: 69.81 time: 0.067
2026-01-10 07:56:22,818: Train batch 4000: loss: 18.94 grad norm: 53.65 time: 0.056
2026-01-10 07:56:22,818: Running test after training batch: 4000
2026-01-10 07:56:22,984: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:56:28,174: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the co due to this point we
2026-01-10 07:56:28,218: WER debug example
  GT : how does it keep the cost down
  PR : how i see it keep the cost
2026-01-10 07:56:41,058: Val batch 4000: PER (avg): 0.2463 CTC Loss (avg): 23.5120 WER(5gram): 57.50% (n=256) time: 18.239
2026-01-10 07:56:41,058: WER lens: avg_true_words=5.99 avg_pred_words=5.93 max_pred_words=11
2026-01-10 07:56:41,058: t15.2023.08.13 val PER: 0.2131
2026-01-10 07:56:41,058: t15.2023.08.18 val PER: 0.2062
2026-01-10 07:56:41,058: t15.2023.08.20 val PER: 0.1898
2026-01-10 07:56:41,059: t15.2023.08.25 val PER: 0.1506
2026-01-10 07:56:41,059: t15.2023.08.27 val PER: 0.2846
2026-01-10 07:56:41,059: t15.2023.09.01 val PER: 0.1558
2026-01-10 07:56:41,059: t15.2023.09.03 val PER: 0.2506
2026-01-10 07:56:41,059: t15.2023.09.24 val PER: 0.1990
2026-01-10 07:56:41,059: t15.2023.09.29 val PER: 0.2042
2026-01-10 07:56:41,059: t15.2023.10.01 val PER: 0.2530
2026-01-10 07:56:41,059: t15.2023.10.06 val PER: 0.1658
2026-01-10 07:56:41,059: t15.2023.10.08 val PER: 0.3288
2026-01-10 07:56:41,060: t15.2023.10.13 val PER: 0.3010
2026-01-10 07:56:41,060: t15.2023.10.15 val PER: 0.2228
2026-01-10 07:56:41,060: t15.2023.10.20 val PER: 0.2315
2026-01-10 07:56:41,060: t15.2023.10.22 val PER: 0.1826
2026-01-10 07:56:41,060: t15.2023.11.03 val PER: 0.2259
2026-01-10 07:56:41,060: t15.2023.11.04 val PER: 0.0683
2026-01-10 07:56:41,060: t15.2023.11.17 val PER: 0.1073
2026-01-10 07:56:41,060: t15.2023.11.19 val PER: 0.0958
2026-01-10 07:56:41,060: t15.2023.11.26 val PER: 0.2493
2026-01-10 07:56:41,060: t15.2023.12.03 val PER: 0.2174
2026-01-10 07:56:41,061: t15.2023.12.08 val PER: 0.2224
2026-01-10 07:56:41,061: t15.2023.12.10 val PER: 0.1905
2026-01-10 07:56:41,061: t15.2023.12.17 val PER: 0.2391
2026-01-10 07:56:41,061: t15.2023.12.29 val PER: 0.2588
2026-01-10 07:56:41,061: t15.2024.02.25 val PER: 0.2037
2026-01-10 07:56:41,061: t15.2024.03.08 val PER: 0.3186
2026-01-10 07:56:41,061: t15.2024.03.15 val PER: 0.2814
2026-01-10 07:56:41,061: t15.2024.03.17 val PER: 0.2490
2026-01-10 07:56:41,061: t15.2024.05.10 val PER: 0.2437
2026-01-10 07:56:41,061: t15.2024.06.14 val PER: 0.2776
2026-01-10 07:56:41,062: t15.2024.07.19 val PER: 0.3724
2026-01-10 07:56:41,062: t15.2024.07.21 val PER: 0.1938
2026-01-10 07:56:41,062: t15.2024.07.28 val PER: 0.2449
2026-01-10 07:56:41,062: t15.2025.01.10 val PER: 0.4408
2026-01-10 07:56:41,062: t15.2025.01.12 val PER: 0.2802
2026-01-10 07:56:41,062: t15.2025.03.14 val PER: 0.4112
2026-01-10 07:56:41,062: t15.2025.03.16 val PER: 0.3128
2026-01-10 07:56:41,062: t15.2025.03.30 val PER: 0.4253
2026-01-10 07:56:41,062: t15.2025.04.13 val PER: 0.3238
2026-01-10 07:56:41,063: New best val WER(5gram) 63.10% --> 57.50%
2026-01-10 07:56:41,268: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_4000
2026-01-10 07:57:00,999: Train batch 4200: loss: 21.94 grad norm: 65.84 time: 0.084
2026-01-10 07:57:19,869: Train batch 4400: loss: 16.64 grad norm: 58.19 time: 0.066
2026-01-10 07:57:29,308: Running test after training batch: 4500
2026-01-10 07:57:29,423: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:57:34,562: WER debug example
  GT : you can see the code at this point as well
  PR : you can see a go at this point will
2026-01-10 07:57:34,603: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 07:57:47,276: Val batch 4500: PER (avg): 0.2379 CTC Loss (avg): 22.7119 WER(5gram): 59.45% (n=256) time: 17.968
2026-01-10 07:57:47,277: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=13
2026-01-10 07:57:47,277: t15.2023.08.13 val PER: 0.2027
2026-01-10 07:57:47,277: t15.2023.08.18 val PER: 0.1878
2026-01-10 07:57:47,277: t15.2023.08.20 val PER: 0.1898
2026-01-10 07:57:47,277: t15.2023.08.25 val PER: 0.1536
2026-01-10 07:57:47,278: t15.2023.08.27 val PER: 0.2556
2026-01-10 07:57:47,278: t15.2023.09.01 val PER: 0.1485
2026-01-10 07:57:47,278: t15.2023.09.03 val PER: 0.2328
2026-01-10 07:57:47,278: t15.2023.09.24 val PER: 0.1857
2026-01-10 07:57:47,278: t15.2023.09.29 val PER: 0.2080
2026-01-10 07:57:47,278: t15.2023.10.01 val PER: 0.2523
2026-01-10 07:57:47,278: t15.2023.10.06 val PER: 0.1539
2026-01-10 07:57:47,278: t15.2023.10.08 val PER: 0.3112
2026-01-10 07:57:47,278: t15.2023.10.13 val PER: 0.3010
2026-01-10 07:57:47,278: t15.2023.10.15 val PER: 0.2221
2026-01-10 07:57:47,279: t15.2023.10.20 val PER: 0.2450
2026-01-10 07:57:47,279: t15.2023.10.22 val PER: 0.1960
2026-01-10 07:57:47,279: t15.2023.11.03 val PER: 0.2449
2026-01-10 07:57:47,279: t15.2023.11.04 val PER: 0.0580
2026-01-10 07:57:47,279: t15.2023.11.17 val PER: 0.0933
2026-01-10 07:57:47,279: t15.2023.11.19 val PER: 0.0958
2026-01-10 07:57:47,279: t15.2023.11.26 val PER: 0.2449
2026-01-10 07:57:47,279: t15.2023.12.03 val PER: 0.2090
2026-01-10 07:57:47,279: t15.2023.12.08 val PER: 0.2091
2026-01-10 07:57:47,279: t15.2023.12.10 val PER: 0.1905
2026-01-10 07:57:47,279: t15.2023.12.17 val PER: 0.2193
2026-01-10 07:57:47,279: t15.2023.12.29 val PER: 0.2395
2026-01-10 07:57:47,279: t15.2024.02.25 val PER: 0.1910
2026-01-10 07:57:47,279: t15.2024.03.08 val PER: 0.3030
2026-01-10 07:57:47,280: t15.2024.03.15 val PER: 0.2914
2026-01-10 07:57:47,280: t15.2024.03.17 val PER: 0.2399
2026-01-10 07:57:47,280: t15.2024.05.10 val PER: 0.2467
2026-01-10 07:57:47,280: t15.2024.06.14 val PER: 0.2524
2026-01-10 07:57:47,280: t15.2024.07.19 val PER: 0.3408
2026-01-10 07:57:47,280: t15.2024.07.21 val PER: 0.1848
2026-01-10 07:57:47,280: t15.2024.07.28 val PER: 0.2265
2026-01-10 07:57:47,280: t15.2025.01.10 val PER: 0.4160
2026-01-10 07:57:47,280: t15.2025.01.12 val PER: 0.2687
2026-01-10 07:57:47,280: t15.2025.03.14 val PER: 0.3920
2026-01-10 07:57:47,280: t15.2025.03.16 val PER: 0.2932
2026-01-10 07:57:47,280: t15.2025.03.30 val PER: 0.4253
2026-01-10 07:57:47,280: t15.2025.04.13 val PER: 0.3024
2026-01-10 07:57:47,420: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_4500
2026-01-10 07:57:56,841: Train batch 4600: loss: 20.54 grad norm: 63.13 time: 0.063
2026-01-10 07:58:15,635: Train batch 4800: loss: 14.38 grad norm: 54.90 time: 0.063
2026-01-10 07:58:34,624: Train batch 5000: loss: 31.42 grad norm: 81.99 time: 0.064
2026-01-10 07:58:34,624: Running test after training batch: 5000
2026-01-10 07:58:34,803: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:58:40,359: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point will
2026-01-10 07:58:40,404: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 07:58:52,833: Val batch 5000: PER (avg): 0.2220 CTC Loss (avg): 21.5093 WER(5gram): 56.78% (n=256) time: 18.208
2026-01-10 07:58:52,834: WER lens: avg_true_words=5.99 avg_pred_words=6.03 max_pred_words=13
2026-01-10 07:58:52,834: t15.2023.08.13 val PER: 0.1913
2026-01-10 07:58:52,834: t15.2023.08.18 val PER: 0.1752
2026-01-10 07:58:52,834: t15.2023.08.20 val PER: 0.1644
2026-01-10 07:58:52,834: t15.2023.08.25 val PER: 0.1461
2026-01-10 07:58:52,834: t15.2023.08.27 val PER: 0.2363
2026-01-10 07:58:52,834: t15.2023.09.01 val PER: 0.1380
2026-01-10 07:58:52,834: t15.2023.09.03 val PER: 0.2257
2026-01-10 07:58:52,834: t15.2023.09.24 val PER: 0.1650
2026-01-10 07:58:52,834: t15.2023.09.29 val PER: 0.1838
2026-01-10 07:58:52,835: t15.2023.10.01 val PER: 0.2312
2026-01-10 07:58:52,835: t15.2023.10.06 val PER: 0.1539
2026-01-10 07:58:52,835: t15.2023.10.08 val PER: 0.3004
2026-01-10 07:58:52,835: t15.2023.10.13 val PER: 0.2801
2026-01-10 07:58:52,835: t15.2023.10.15 val PER: 0.2057
2026-01-10 07:58:52,835: t15.2023.10.20 val PER: 0.2383
2026-01-10 07:58:52,835: t15.2023.10.22 val PER: 0.1748
2026-01-10 07:58:52,835: t15.2023.11.03 val PER: 0.2280
2026-01-10 07:58:52,835: t15.2023.11.04 val PER: 0.0512
2026-01-10 07:58:52,835: t15.2023.11.17 val PER: 0.0871
2026-01-10 07:58:52,835: t15.2023.11.19 val PER: 0.0719
2026-01-10 07:58:52,835: t15.2023.11.26 val PER: 0.2290
2026-01-10 07:58:52,835: t15.2023.12.03 val PER: 0.2048
2026-01-10 07:58:52,835: t15.2023.12.08 val PER: 0.1911
2026-01-10 07:58:52,836: t15.2023.12.10 val PER: 0.1656
2026-01-10 07:58:52,836: t15.2023.12.17 val PER: 0.2006
2026-01-10 07:58:52,836: t15.2023.12.29 val PER: 0.2237
2026-01-10 07:58:52,836: t15.2024.02.25 val PER: 0.1784
2026-01-10 07:58:52,836: t15.2024.03.08 val PER: 0.3016
2026-01-10 07:58:52,836: t15.2024.03.15 val PER: 0.2627
2026-01-10 07:58:52,836: t15.2024.03.17 val PER: 0.2301
2026-01-10 07:58:52,836: t15.2024.05.10 val PER: 0.2259
2026-01-10 07:58:52,836: t15.2024.06.14 val PER: 0.2429
2026-01-10 07:58:52,837: t15.2024.07.19 val PER: 0.3276
2026-01-10 07:58:52,837: t15.2024.07.21 val PER: 0.1800
2026-01-10 07:58:52,837: t15.2024.07.28 val PER: 0.2199
2026-01-10 07:58:52,837: t15.2025.01.10 val PER: 0.4022
2026-01-10 07:58:52,837: t15.2025.01.12 val PER: 0.2271
2026-01-10 07:58:52,837: t15.2025.03.14 val PER: 0.3831
2026-01-10 07:58:52,837: t15.2025.03.16 val PER: 0.2723
2026-01-10 07:58:52,837: t15.2025.03.30 val PER: 0.3931
2026-01-10 07:58:52,837: t15.2025.04.13 val PER: 0.3039
2026-01-10 07:58:52,838: New best val WER(5gram) 57.50% --> 56.78%
2026-01-10 07:58:53,027: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_5000
2026-01-10 07:59:11,845: Train batch 5200: loss: 15.75 grad norm: 54.42 time: 0.052
2026-01-10 07:59:30,622: Train batch 5400: loss: 16.60 grad norm: 55.84 time: 0.069
2026-01-10 07:59:40,363: Running test after training batch: 5500
2026-01-10 07:59:40,484: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:59:45,651: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point will
2026-01-10 07:59:45,691: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the cost
2026-01-10 07:59:57,114: Val batch 5500: PER (avg): 0.2116 CTC Loss (avg): 20.3427 WER(5gram): 53.91% (n=256) time: 16.751
2026-01-10 07:59:57,115: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=11
2026-01-10 07:59:57,115: t15.2023.08.13 val PER: 0.1653
2026-01-10 07:59:57,115: t15.2023.08.18 val PER: 0.1693
2026-01-10 07:59:57,115: t15.2023.08.20 val PER: 0.1533
2026-01-10 07:59:57,115: t15.2023.08.25 val PER: 0.1340
2026-01-10 07:59:57,115: t15.2023.08.27 val PER: 0.2460
2026-01-10 07:59:57,115: t15.2023.09.01 val PER: 0.1282
2026-01-10 07:59:57,116: t15.2023.09.03 val PER: 0.2197
2026-01-10 07:59:57,116: t15.2023.09.24 val PER: 0.1602
2026-01-10 07:59:57,116: t15.2023.09.29 val PER: 0.1774
2026-01-10 07:59:57,116: t15.2023.10.01 val PER: 0.2239
2026-01-10 07:59:57,116: t15.2023.10.06 val PER: 0.1335
2026-01-10 07:59:57,116: t15.2023.10.08 val PER: 0.2815
2026-01-10 07:59:57,116: t15.2023.10.13 val PER: 0.2676
2026-01-10 07:59:57,116: t15.2023.10.15 val PER: 0.2050
2026-01-10 07:59:57,116: t15.2023.10.20 val PER: 0.2517
2026-01-10 07:59:57,116: t15.2023.10.22 val PER: 0.1637
2026-01-10 07:59:57,116: t15.2023.11.03 val PER: 0.2185
2026-01-10 07:59:57,116: t15.2023.11.04 val PER: 0.0375
2026-01-10 07:59:57,116: t15.2023.11.17 val PER: 0.0793
2026-01-10 07:59:57,116: t15.2023.11.19 val PER: 0.0798
2026-01-10 07:59:57,116: t15.2023.11.26 val PER: 0.2188
2026-01-10 07:59:57,116: t15.2023.12.03 val PER: 0.1765
2026-01-10 07:59:57,116: t15.2023.12.08 val PER: 0.1851
2026-01-10 07:59:57,117: t15.2023.12.10 val PER: 0.1577
2026-01-10 07:59:57,117: t15.2023.12.17 val PER: 0.1985
2026-01-10 07:59:57,117: t15.2023.12.29 val PER: 0.2093
2026-01-10 07:59:57,117: t15.2024.02.25 val PER: 0.1629
2026-01-10 07:59:57,117: t15.2024.03.08 val PER: 0.2788
2026-01-10 07:59:57,117: t15.2024.03.15 val PER: 0.2508
2026-01-10 07:59:57,117: t15.2024.03.17 val PER: 0.2078
2026-01-10 07:59:57,117: t15.2024.05.10 val PER: 0.2259
2026-01-10 07:59:57,117: t15.2024.06.14 val PER: 0.2319
2026-01-10 07:59:57,118: t15.2024.07.19 val PER: 0.3210
2026-01-10 07:59:57,118: t15.2024.07.21 val PER: 0.1628
2026-01-10 07:59:57,118: t15.2024.07.28 val PER: 0.2221
2026-01-10 07:59:57,118: t15.2025.01.10 val PER: 0.3760
2026-01-10 07:59:57,118: t15.2025.01.12 val PER: 0.2302
2026-01-10 07:59:57,118: t15.2025.03.14 val PER: 0.3639
2026-01-10 07:59:57,118: t15.2025.03.16 val PER: 0.2631
2026-01-10 07:59:57,118: t15.2025.03.30 val PER: 0.3655
2026-01-10 07:59:57,118: t15.2025.04.13 val PER: 0.2882
2026-01-10 07:59:57,119: New best val WER(5gram) 56.78% --> 53.91%
2026-01-10 07:59:57,314: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_5500
2026-01-10 08:00:06,726: Train batch 5600: loss: 19.64 grad norm: 67.17 time: 0.063
2026-01-10 08:00:25,653: Train batch 5800: loss: 13.48 grad norm: 57.42 time: 0.083
2026-01-10 08:00:44,644: Train batch 6000: loss: 13.36 grad norm: 53.37 time: 0.049
2026-01-10 08:00:44,645: Running test after training batch: 6000
2026-01-10 08:00:44,770: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:00:50,039: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the go to this point will
2026-01-10 08:00:50,079: WER debug example
  GT : how does it keep the cost down
  PR : how i see it keep the cost
2026-01-10 08:01:01,588: Val batch 6000: PER (avg): 0.2077 CTC Loss (avg): 20.0000 WER(5gram): 60.37% (n=256) time: 16.943
2026-01-10 08:01:01,589: WER lens: avg_true_words=5.99 avg_pred_words=5.96 max_pred_words=12
2026-01-10 08:01:01,589: t15.2023.08.13 val PER: 0.1767
2026-01-10 08:01:01,589: t15.2023.08.18 val PER: 0.1643
2026-01-10 08:01:01,589: t15.2023.08.20 val PER: 0.1652
2026-01-10 08:01:01,589: t15.2023.08.25 val PER: 0.1461
2026-01-10 08:01:01,589: t15.2023.08.27 val PER: 0.2428
2026-01-10 08:01:01,589: t15.2023.09.01 val PER: 0.1299
2026-01-10 08:01:01,590: t15.2023.09.03 val PER: 0.2162
2026-01-10 08:01:01,590: t15.2023.09.24 val PER: 0.1748
2026-01-10 08:01:01,590: t15.2023.09.29 val PER: 0.1640
2026-01-10 08:01:01,590: t15.2023.10.01 val PER: 0.2160
2026-01-10 08:01:01,590: t15.2023.10.06 val PER: 0.1324
2026-01-10 08:01:01,590: t15.2023.10.08 val PER: 0.2882
2026-01-10 08:01:01,590: t15.2023.10.13 val PER: 0.2638
2026-01-10 08:01:01,590: t15.2023.10.15 val PER: 0.2044
2026-01-10 08:01:01,590: t15.2023.10.20 val PER: 0.2282
2026-01-10 08:01:01,590: t15.2023.10.22 val PER: 0.1693
2026-01-10 08:01:01,591: t15.2023.11.03 val PER: 0.2178
2026-01-10 08:01:01,591: t15.2023.11.04 val PER: 0.0512
2026-01-10 08:01:01,591: t15.2023.11.17 val PER: 0.0684
2026-01-10 08:01:01,591: t15.2023.11.19 val PER: 0.0739
2026-01-10 08:01:01,591: t15.2023.11.26 val PER: 0.2101
2026-01-10 08:01:01,591: t15.2023.12.03 val PER: 0.1765
2026-01-10 08:01:01,591: t15.2023.12.08 val PER: 0.1778
2026-01-10 08:01:01,591: t15.2023.12.10 val PER: 0.1603
2026-01-10 08:01:01,591: t15.2023.12.17 val PER: 0.1871
2026-01-10 08:01:01,591: t15.2023.12.29 val PER: 0.2080
2026-01-10 08:01:01,592: t15.2024.02.25 val PER: 0.1615
2026-01-10 08:01:01,592: t15.2024.03.08 val PER: 0.2745
2026-01-10 08:01:01,592: t15.2024.03.15 val PER: 0.2577
2026-01-10 08:01:01,592: t15.2024.03.17 val PER: 0.2078
2026-01-10 08:01:01,592: t15.2024.05.10 val PER: 0.2036
2026-01-10 08:01:01,592: t15.2024.06.14 val PER: 0.2066
2026-01-10 08:01:01,592: t15.2024.07.19 val PER: 0.3171
2026-01-10 08:01:01,592: t15.2024.07.21 val PER: 0.1503
2026-01-10 08:01:01,592: t15.2024.07.28 val PER: 0.1956
2026-01-10 08:01:01,593: t15.2025.01.10 val PER: 0.3747
2026-01-10 08:01:01,593: t15.2025.01.12 val PER: 0.2117
2026-01-10 08:01:01,593: t15.2025.03.14 val PER: 0.3698
2026-01-10 08:01:01,593: t15.2025.03.16 val PER: 0.2592
2026-01-10 08:01:01,593: t15.2025.03.30 val PER: 0.3586
2026-01-10 08:01:01,593: t15.2025.04.13 val PER: 0.2767
2026-01-10 08:01:01,730: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_6000
2026-01-10 08:01:20,361: Train batch 6200: loss: 14.84 grad norm: 56.38 time: 0.070
2026-01-10 08:01:39,043: Train batch 6400: loss: 20.39 grad norm: 68.45 time: 0.063
2026-01-10 08:01:48,293: Running test after training batch: 6500
2026-01-10 08:01:48,444: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:01:53,978: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point i will
2026-01-10 08:01:54,020: WER debug example
  GT : how does it keep the cost down
  PR : how i see it keep the cost
2026-01-10 08:02:05,213: Val batch 6500: PER (avg): 0.1989 CTC Loss (avg): 19.5619 WER(5gram): 41.85% (n=256) time: 16.920
2026-01-10 08:02:05,214: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-10 08:02:05,214: t15.2023.08.13 val PER: 0.1653
2026-01-10 08:02:05,214: t15.2023.08.18 val PER: 0.1509
2026-01-10 08:02:05,214: t15.2023.08.20 val PER: 0.1557
2026-01-10 08:02:05,214: t15.2023.08.25 val PER: 0.1265
2026-01-10 08:02:05,214: t15.2023.08.27 val PER: 0.2299
2026-01-10 08:02:05,215: t15.2023.09.01 val PER: 0.1120
2026-01-10 08:02:05,215: t15.2023.09.03 val PER: 0.1995
2026-01-10 08:02:05,215: t15.2023.09.24 val PER: 0.1638
2026-01-10 08:02:05,215: t15.2023.09.29 val PER: 0.1723
2026-01-10 08:02:05,215: t15.2023.10.01 val PER: 0.2114
2026-01-10 08:02:05,215: t15.2023.10.06 val PER: 0.1292
2026-01-10 08:02:05,215: t15.2023.10.08 val PER: 0.2815
2026-01-10 08:02:05,215: t15.2023.10.13 val PER: 0.2490
2026-01-10 08:02:05,215: t15.2023.10.15 val PER: 0.1978
2026-01-10 08:02:05,215: t15.2023.10.20 val PER: 0.2148
2026-01-10 08:02:05,216: t15.2023.10.22 val PER: 0.1537
2026-01-10 08:02:05,216: t15.2023.11.03 val PER: 0.2157
2026-01-10 08:02:05,216: t15.2023.11.04 val PER: 0.0375
2026-01-10 08:02:05,216: t15.2023.11.17 val PER: 0.0731
2026-01-10 08:02:05,216: t15.2023.11.19 val PER: 0.0739
2026-01-10 08:02:05,216: t15.2023.11.26 val PER: 0.1819
2026-01-10 08:02:05,216: t15.2023.12.03 val PER: 0.1618
2026-01-10 08:02:05,216: t15.2023.12.08 val PER: 0.1684
2026-01-10 08:02:05,216: t15.2023.12.10 val PER: 0.1511
2026-01-10 08:02:05,216: t15.2023.12.17 val PER: 0.1840
2026-01-10 08:02:05,217: t15.2023.12.29 val PER: 0.2038
2026-01-10 08:02:05,217: t15.2024.02.25 val PER: 0.1517
2026-01-10 08:02:05,217: t15.2024.03.08 val PER: 0.2688
2026-01-10 08:02:05,217: t15.2024.03.15 val PER: 0.2445
2026-01-10 08:02:05,217: t15.2024.03.17 val PER: 0.1994
2026-01-10 08:02:05,217: t15.2024.05.10 val PER: 0.2065
2026-01-10 08:02:05,217: t15.2024.06.14 val PER: 0.1940
2026-01-10 08:02:05,217: t15.2024.07.19 val PER: 0.3032
2026-01-10 08:02:05,217: t15.2024.07.21 val PER: 0.1414
2026-01-10 08:02:05,217: t15.2024.07.28 val PER: 0.2029
2026-01-10 08:02:05,217: t15.2025.01.10 val PER: 0.3871
2026-01-10 08:02:05,217: t15.2025.01.12 val PER: 0.2017
2026-01-10 08:02:05,218: t15.2025.03.14 val PER: 0.3743
2026-01-10 08:02:05,218: t15.2025.03.16 val PER: 0.2448
2026-01-10 08:02:05,218: t15.2025.03.30 val PER: 0.3333
2026-01-10 08:02:05,218: t15.2025.04.13 val PER: 0.2553
2026-01-10 08:02:05,218: New best val WER(5gram) 53.91% --> 41.85%
2026-01-10 08:02:05,411: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_6500
2026-01-10 08:02:14,673: Train batch 6600: loss: 11.79 grad norm: 49.78 time: 0.045
2026-01-10 08:02:33,269: Train batch 6800: loss: 14.29 grad norm: 54.34 time: 0.049
2026-01-10 08:02:52,040: Train batch 7000: loss: 16.24 grad norm: 66.60 time: 0.061
2026-01-10 08:02:52,041: Running test after training batch: 7000
2026-01-10 08:02:52,165: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:02:58,363: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point will
2026-01-10 08:02:58,414: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the cost
2026-01-10 08:03:09,865: Val batch 7000: PER (avg): 0.1915 CTC Loss (avg): 18.6781 WER(5gram): 44.07% (n=256) time: 17.824
2026-01-10 08:03:09,865: WER lens: avg_true_words=5.99 avg_pred_words=5.96 max_pred_words=12
2026-01-10 08:03:09,865: t15.2023.08.13 val PER: 0.1466
2026-01-10 08:03:09,866: t15.2023.08.18 val PER: 0.1475
2026-01-10 08:03:09,866: t15.2023.08.20 val PER: 0.1485
2026-01-10 08:03:09,866: t15.2023.08.25 val PER: 0.1340
2026-01-10 08:03:09,866: t15.2023.08.27 val PER: 0.2331
2026-01-10 08:03:09,866: t15.2023.09.01 val PER: 0.1104
2026-01-10 08:03:09,866: t15.2023.09.03 val PER: 0.1995
2026-01-10 08:03:09,866: t15.2023.09.24 val PER: 0.1590
2026-01-10 08:03:09,866: t15.2023.09.29 val PER: 0.1538
2026-01-10 08:03:09,866: t15.2023.10.01 val PER: 0.2127
2026-01-10 08:03:09,866: t15.2023.10.06 val PER: 0.1173
2026-01-10 08:03:09,866: t15.2023.10.08 val PER: 0.2788
2026-01-10 08:03:09,866: t15.2023.10.13 val PER: 0.2335
2026-01-10 08:03:09,866: t15.2023.10.15 val PER: 0.1813
2026-01-10 08:03:09,866: t15.2023.10.20 val PER: 0.2215
2026-01-10 08:03:09,866: t15.2023.10.22 val PER: 0.1425
2026-01-10 08:03:09,866: t15.2023.11.03 val PER: 0.2069
2026-01-10 08:03:09,867: t15.2023.11.04 val PER: 0.0341
2026-01-10 08:03:09,867: t15.2023.11.17 val PER: 0.0669
2026-01-10 08:03:09,867: t15.2023.11.19 val PER: 0.0559
2026-01-10 08:03:09,867: t15.2023.11.26 val PER: 0.1848
2026-01-10 08:03:09,867: t15.2023.12.03 val PER: 0.1534
2026-01-10 08:03:09,867: t15.2023.12.08 val PER: 0.1531
2026-01-10 08:03:09,867: t15.2023.12.10 val PER: 0.1432
2026-01-10 08:03:09,867: t15.2023.12.17 val PER: 0.1757
2026-01-10 08:03:09,867: t15.2023.12.29 val PER: 0.1887
2026-01-10 08:03:09,867: t15.2024.02.25 val PER: 0.1559
2026-01-10 08:03:09,867: t15.2024.03.08 val PER: 0.2504
2026-01-10 08:03:09,867: t15.2024.03.15 val PER: 0.2420
2026-01-10 08:03:09,867: t15.2024.03.17 val PER: 0.1841
2026-01-10 08:03:09,867: t15.2024.05.10 val PER: 0.2155
2026-01-10 08:03:09,868: t15.2024.06.14 val PER: 0.2003
2026-01-10 08:03:09,868: t15.2024.07.19 val PER: 0.3078
2026-01-10 08:03:09,868: t15.2024.07.21 val PER: 0.1276
2026-01-10 08:03:09,868: t15.2024.07.28 val PER: 0.1779
2026-01-10 08:03:09,868: t15.2025.01.10 val PER: 0.3554
2026-01-10 08:03:09,868: t15.2025.01.12 val PER: 0.2048
2026-01-10 08:03:09,868: t15.2025.03.14 val PER: 0.3565
2026-01-10 08:03:09,868: t15.2025.03.16 val PER: 0.2356
2026-01-10 08:03:09,868: t15.2025.03.30 val PER: 0.3506
2026-01-10 08:03:09,868: t15.2025.04.13 val PER: 0.2596
2026-01-10 08:03:10,008: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_7000
2026-01-10 08:03:28,981: Train batch 7200: loss: 13.41 grad norm: 54.34 time: 0.080
2026-01-10 08:03:47,980: Train batch 7400: loss: 13.67 grad norm: 58.67 time: 0.075
2026-01-10 08:03:57,474: Running test after training batch: 7500
2026-01-10 08:03:57,685: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:04:03,064: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point will
2026-01-10 08:04:03,112: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 08:04:14,773: Val batch 7500: PER (avg): 0.1858 CTC Loss (avg): 18.0344 WER(5gram): 45.96% (n=256) time: 17.299
2026-01-10 08:04:14,774: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-10 08:04:14,774: t15.2023.08.13 val PER: 0.1362
2026-01-10 08:04:14,774: t15.2023.08.18 val PER: 0.1358
2026-01-10 08:04:14,774: t15.2023.08.20 val PER: 0.1334
2026-01-10 08:04:14,774: t15.2023.08.25 val PER: 0.1235
2026-01-10 08:04:14,774: t15.2023.08.27 val PER: 0.2170
2026-01-10 08:04:14,774: t15.2023.09.01 val PER: 0.1128
2026-01-10 08:04:14,775: t15.2023.09.03 val PER: 0.1841
2026-01-10 08:04:14,775: t15.2023.09.24 val PER: 0.1614
2026-01-10 08:04:14,775: t15.2023.09.29 val PER: 0.1532
2026-01-10 08:04:14,775: t15.2023.10.01 val PER: 0.1962
2026-01-10 08:04:14,775: t15.2023.10.06 val PER: 0.1141
2026-01-10 08:04:14,775: t15.2023.10.08 val PER: 0.2815
2026-01-10 08:04:14,775: t15.2023.10.13 val PER: 0.2327
2026-01-10 08:04:14,775: t15.2023.10.15 val PER: 0.1839
2026-01-10 08:04:14,775: t15.2023.10.20 val PER: 0.2081
2026-01-10 08:04:14,775: t15.2023.10.22 val PER: 0.1526
2026-01-10 08:04:14,776: t15.2023.11.03 val PER: 0.2083
2026-01-10 08:04:14,776: t15.2023.11.04 val PER: 0.0375
2026-01-10 08:04:14,776: t15.2023.11.17 val PER: 0.0560
2026-01-10 08:04:14,776: t15.2023.11.19 val PER: 0.0659
2026-01-10 08:04:14,776: t15.2023.11.26 val PER: 0.1754
2026-01-10 08:04:14,776: t15.2023.12.03 val PER: 0.1502
2026-01-10 08:04:14,776: t15.2023.12.08 val PER: 0.1531
2026-01-10 08:04:14,776: t15.2023.12.10 val PER: 0.1340
2026-01-10 08:04:14,776: t15.2023.12.17 val PER: 0.1653
2026-01-10 08:04:14,776: t15.2023.12.29 val PER: 0.1846
2026-01-10 08:04:14,777: t15.2024.02.25 val PER: 0.1447
2026-01-10 08:04:14,777: t15.2024.03.08 val PER: 0.2504
2026-01-10 08:04:14,777: t15.2024.03.15 val PER: 0.2408
2026-01-10 08:04:14,777: t15.2024.03.17 val PER: 0.1736
2026-01-10 08:04:14,777: t15.2024.05.10 val PER: 0.1961
2026-01-10 08:04:14,777: t15.2024.06.14 val PER: 0.2035
2026-01-10 08:04:14,777: t15.2024.07.19 val PER: 0.2874
2026-01-10 08:04:14,777: t15.2024.07.21 val PER: 0.1303
2026-01-10 08:04:14,777: t15.2024.07.28 val PER: 0.1676
2026-01-10 08:04:14,777: t15.2025.01.10 val PER: 0.3512
2026-01-10 08:04:14,777: t15.2025.01.12 val PER: 0.1925
2026-01-10 08:04:14,777: t15.2025.03.14 val PER: 0.3565
2026-01-10 08:04:14,778: t15.2025.03.16 val PER: 0.2395
2026-01-10 08:04:14,778: t15.2025.03.30 val PER: 0.3345
2026-01-10 08:04:14,778: t15.2025.04.13 val PER: 0.2553
2026-01-10 08:04:14,926: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_7500
2026-01-10 08:04:24,157: Train batch 7600: loss: 15.50 grad norm: 60.98 time: 0.069
2026-01-10 08:04:43,016: Train batch 7800: loss: 13.47 grad norm: 58.70 time: 0.056
2026-01-10 08:05:02,179: Train batch 8000: loss: 11.00 grad norm: 51.29 time: 0.072
2026-01-10 08:05:02,179: Running test after training batch: 8000
2026-01-10 08:05:02,300: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:05:07,462: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:05:07,519: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the cost
2026-01-10 08:05:18,634: Val batch 8000: PER (avg): 0.1806 CTC Loss (avg): 17.4670 WER(5gram): 38.98% (n=256) time: 16.454
2026-01-10 08:05:18,634: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=11
2026-01-10 08:05:18,634: t15.2023.08.13 val PER: 0.1403
2026-01-10 08:05:18,634: t15.2023.08.18 val PER: 0.1249
2026-01-10 08:05:18,634: t15.2023.08.20 val PER: 0.1255
2026-01-10 08:05:18,634: t15.2023.08.25 val PER: 0.1265
2026-01-10 08:05:18,635: t15.2023.08.27 val PER: 0.2379
2026-01-10 08:05:18,635: t15.2023.09.01 val PER: 0.0990
2026-01-10 08:05:18,635: t15.2023.09.03 val PER: 0.1936
2026-01-10 08:05:18,635: t15.2023.09.24 val PER: 0.1493
2026-01-10 08:05:18,635: t15.2023.09.29 val PER: 0.1474
2026-01-10 08:05:18,635: t15.2023.10.01 val PER: 0.1948
2026-01-10 08:05:18,635: t15.2023.10.06 val PER: 0.1119
2026-01-10 08:05:18,635: t15.2023.10.08 val PER: 0.2625
2026-01-10 08:05:18,636: t15.2023.10.13 val PER: 0.2366
2026-01-10 08:05:18,636: t15.2023.10.15 val PER: 0.1846
2026-01-10 08:05:18,636: t15.2023.10.20 val PER: 0.2148
2026-01-10 08:05:18,636: t15.2023.10.22 val PER: 0.1381
2026-01-10 08:05:18,636: t15.2023.11.03 val PER: 0.2076
2026-01-10 08:05:18,636: t15.2023.11.04 val PER: 0.0375
2026-01-10 08:05:18,636: t15.2023.11.17 val PER: 0.0498
2026-01-10 08:05:18,636: t15.2023.11.19 val PER: 0.0599
2026-01-10 08:05:18,636: t15.2023.11.26 val PER: 0.1674
2026-01-10 08:05:18,636: t15.2023.12.03 val PER: 0.1586
2026-01-10 08:05:18,636: t15.2023.12.08 val PER: 0.1418
2026-01-10 08:05:18,636: t15.2023.12.10 val PER: 0.1235
2026-01-10 08:05:18,636: t15.2023.12.17 val PER: 0.1611
2026-01-10 08:05:18,636: t15.2023.12.29 val PER: 0.1784
2026-01-10 08:05:18,636: t15.2024.02.25 val PER: 0.1390
2026-01-10 08:05:18,637: t15.2024.03.08 val PER: 0.2632
2026-01-10 08:05:18,637: t15.2024.03.15 val PER: 0.2333
2026-01-10 08:05:18,637: t15.2024.03.17 val PER: 0.1771
2026-01-10 08:05:18,637: t15.2024.05.10 val PER: 0.1887
2026-01-10 08:05:18,637: t15.2024.06.14 val PER: 0.1972
2026-01-10 08:05:18,637: t15.2024.07.19 val PER: 0.2795
2026-01-10 08:05:18,637: t15.2024.07.21 val PER: 0.1138
2026-01-10 08:05:18,637: t15.2024.07.28 val PER: 0.1640
2026-01-10 08:05:18,637: t15.2025.01.10 val PER: 0.3127
2026-01-10 08:05:18,637: t15.2025.01.12 val PER: 0.1809
2026-01-10 08:05:18,637: t15.2025.03.14 val PER: 0.3521
2026-01-10 08:05:18,637: t15.2025.03.16 val PER: 0.2147
2026-01-10 08:05:18,637: t15.2025.03.30 val PER: 0.3356
2026-01-10 08:05:18,637: t15.2025.04.13 val PER: 0.2582
2026-01-10 08:05:18,639: New best val WER(5gram) 41.85% --> 38.98%
2026-01-10 08:05:18,827: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_8000
2026-01-10 08:05:37,709: Train batch 8200: loss: 8.65 grad norm: 46.83 time: 0.055
2026-01-10 08:05:57,056: Train batch 8400: loss: 10.02 grad norm: 51.29 time: 0.066
2026-01-10 08:06:06,880: Running test after training batch: 8500
2026-01-10 08:06:06,983: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:06:12,128: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point will
2026-01-10 08:06:12,183: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the cost
2026-01-10 08:06:23,703: Val batch 8500: PER (avg): 0.1755 CTC Loss (avg): 17.2775 WER(5gram): 41.00% (n=256) time: 16.823
2026-01-10 08:06:23,704: WER lens: avg_true_words=5.99 avg_pred_words=6.04 max_pred_words=11
2026-01-10 08:06:23,704: t15.2023.08.13 val PER: 0.1362
2026-01-10 08:06:23,704: t15.2023.08.18 val PER: 0.1350
2026-01-10 08:06:23,704: t15.2023.08.20 val PER: 0.1334
2026-01-10 08:06:23,704: t15.2023.08.25 val PER: 0.1295
2026-01-10 08:06:23,704: t15.2023.08.27 val PER: 0.1994
2026-01-10 08:06:23,705: t15.2023.09.01 val PER: 0.0942
2026-01-10 08:06:23,705: t15.2023.09.03 val PER: 0.1817
2026-01-10 08:06:23,705: t15.2023.09.24 val PER: 0.1371
2026-01-10 08:06:23,705: t15.2023.09.29 val PER: 0.1410
2026-01-10 08:06:23,705: t15.2023.10.01 val PER: 0.1915
2026-01-10 08:06:23,705: t15.2023.10.06 val PER: 0.1109
2026-01-10 08:06:23,705: t15.2023.10.08 val PER: 0.2625
2026-01-10 08:06:23,705: t15.2023.10.13 val PER: 0.2196
2026-01-10 08:06:23,705: t15.2023.10.15 val PER: 0.1852
2026-01-10 08:06:23,705: t15.2023.10.20 val PER: 0.2315
2026-01-10 08:06:23,705: t15.2023.10.22 val PER: 0.1414
2026-01-10 08:06:23,705: t15.2023.11.03 val PER: 0.1947
2026-01-10 08:06:23,705: t15.2023.11.04 val PER: 0.0375
2026-01-10 08:06:23,705: t15.2023.11.17 val PER: 0.0591
2026-01-10 08:06:23,705: t15.2023.11.19 val PER: 0.0539
2026-01-10 08:06:23,705: t15.2023.11.26 val PER: 0.1681
2026-01-10 08:06:23,705: t15.2023.12.03 val PER: 0.1439
2026-01-10 08:06:23,706: t15.2023.12.08 val PER: 0.1398
2026-01-10 08:06:23,706: t15.2023.12.10 val PER: 0.1209
2026-01-10 08:06:23,706: t15.2023.12.17 val PER: 0.1559
2026-01-10 08:06:23,706: t15.2023.12.29 val PER: 0.1668
2026-01-10 08:06:23,706: t15.2024.02.25 val PER: 0.1306
2026-01-10 08:06:23,706: t15.2024.03.08 val PER: 0.2518
2026-01-10 08:06:23,706: t15.2024.03.15 val PER: 0.2308
2026-01-10 08:06:23,706: t15.2024.03.17 val PER: 0.1695
2026-01-10 08:06:23,706: t15.2024.05.10 val PER: 0.1902
2026-01-10 08:06:23,706: t15.2024.06.14 val PER: 0.1845
2026-01-10 08:06:23,706: t15.2024.07.19 val PER: 0.2716
2026-01-10 08:06:23,706: t15.2024.07.21 val PER: 0.1152
2026-01-10 08:06:23,706: t15.2024.07.28 val PER: 0.1515
2026-01-10 08:06:23,706: t15.2025.01.10 val PER: 0.3113
2026-01-10 08:06:23,706: t15.2025.01.12 val PER: 0.1794
2026-01-10 08:06:23,707: t15.2025.03.14 val PER: 0.3609
2026-01-10 08:06:23,707: t15.2025.03.16 val PER: 0.2173
2026-01-10 08:06:23,707: t15.2025.03.30 val PER: 0.3057
2026-01-10 08:06:23,707: t15.2025.04.13 val PER: 0.2411
2026-01-10 08:06:23,845: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_8500
2026-01-10 08:06:33,310: Train batch 8600: loss: 14.98 grad norm: 56.10 time: 0.054
2026-01-10 08:06:52,240: Train batch 8800: loss: 13.61 grad norm: 60.10 time: 0.060
2026-01-10 08:07:11,148: Train batch 9000: loss: 14.39 grad norm: 63.00 time: 0.073
2026-01-10 08:07:11,149: Running test after training batch: 9000
2026-01-10 08:07:11,290: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:07:16,338: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point will
2026-01-10 08:07:16,398: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the cost
2026-01-10 08:07:27,784: Val batch 9000: PER (avg): 0.1683 CTC Loss (avg): 16.5741 WER(5gram): 40.42% (n=256) time: 16.635
2026-01-10 08:07:27,785: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-10 08:07:27,785: t15.2023.08.13 val PER: 0.1227
2026-01-10 08:07:27,785: t15.2023.08.18 val PER: 0.1207
2026-01-10 08:07:27,785: t15.2023.08.20 val PER: 0.1231
2026-01-10 08:07:27,785: t15.2023.08.25 val PER: 0.1145
2026-01-10 08:07:27,785: t15.2023.08.27 val PER: 0.1977
2026-01-10 08:07:27,785: t15.2023.09.01 val PER: 0.0885
2026-01-10 08:07:27,785: t15.2023.09.03 val PER: 0.1781
2026-01-10 08:07:27,785: t15.2023.09.24 val PER: 0.1408
2026-01-10 08:07:27,785: t15.2023.09.29 val PER: 0.1404
2026-01-10 08:07:27,785: t15.2023.10.01 val PER: 0.1882
2026-01-10 08:07:27,785: t15.2023.10.06 val PER: 0.0980
2026-01-10 08:07:27,786: t15.2023.10.08 val PER: 0.2612
2026-01-10 08:07:27,786: t15.2023.10.13 val PER: 0.2265
2026-01-10 08:07:27,786: t15.2023.10.15 val PER: 0.1681
2026-01-10 08:07:27,786: t15.2023.10.20 val PER: 0.2081
2026-01-10 08:07:27,786: t15.2023.10.22 val PER: 0.1325
2026-01-10 08:07:27,786: t15.2023.11.03 val PER: 0.1974
2026-01-10 08:07:27,786: t15.2023.11.04 val PER: 0.0444
2026-01-10 08:07:27,786: t15.2023.11.17 val PER: 0.0451
2026-01-10 08:07:27,786: t15.2023.11.19 val PER: 0.0519
2026-01-10 08:07:27,786: t15.2023.11.26 val PER: 0.1652
2026-01-10 08:07:27,786: t15.2023.12.03 val PER: 0.1292
2026-01-10 08:07:27,786: t15.2023.12.08 val PER: 0.1198
2026-01-10 08:07:27,786: t15.2023.12.10 val PER: 0.1104
2026-01-10 08:07:27,786: t15.2023.12.17 val PER: 0.1424
2026-01-10 08:07:27,786: t15.2023.12.29 val PER: 0.1613
2026-01-10 08:07:27,787: t15.2024.02.25 val PER: 0.1517
2026-01-10 08:07:27,787: t15.2024.03.08 val PER: 0.2475
2026-01-10 08:07:27,787: t15.2024.03.15 val PER: 0.2126
2026-01-10 08:07:27,787: t15.2024.03.17 val PER: 0.1534
2026-01-10 08:07:27,787: t15.2024.05.10 val PER: 0.1768
2026-01-10 08:07:27,787: t15.2024.06.14 val PER: 0.1688
2026-01-10 08:07:27,787: t15.2024.07.19 val PER: 0.2663
2026-01-10 08:07:27,787: t15.2024.07.21 val PER: 0.1097
2026-01-10 08:07:27,787: t15.2024.07.28 val PER: 0.1471
2026-01-10 08:07:27,787: t15.2025.01.10 val PER: 0.2975
2026-01-10 08:07:27,787: t15.2025.01.12 val PER: 0.1747
2026-01-10 08:07:27,787: t15.2025.03.14 val PER: 0.3254
2026-01-10 08:07:27,787: t15.2025.03.16 val PER: 0.2251
2026-01-10 08:07:27,787: t15.2025.03.30 val PER: 0.3172
2026-01-10 08:07:27,787: t15.2025.04.13 val PER: 0.2439
2026-01-10 08:07:27,926: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_9000
2026-01-10 08:07:46,564: Train batch 9200: loss: 10.00 grad norm: 52.29 time: 0.055
2026-01-10 08:08:05,502: Train batch 9400: loss: 6.88 grad norm: 42.73 time: 0.068
2026-01-10 08:08:14,965: Running test after training batch: 9500
2026-01-10 08:08:15,142: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:08:20,640: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:08:20,700: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the cost
2026-01-10 08:08:32,580: Val batch 9500: PER (avg): 0.1671 CTC Loss (avg): 16.6246 WER(5gram): 29.73% (n=256) time: 17.614
2026-01-10 08:08:32,581: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-10 08:08:32,581: t15.2023.08.13 val PER: 0.1362
2026-01-10 08:08:32,581: t15.2023.08.18 val PER: 0.1232
2026-01-10 08:08:32,581: t15.2023.08.20 val PER: 0.1207
2026-01-10 08:08:32,581: t15.2023.08.25 val PER: 0.1069
2026-01-10 08:08:32,581: t15.2023.08.27 val PER: 0.2042
2026-01-10 08:08:32,581: t15.2023.09.01 val PER: 0.0901
2026-01-10 08:08:32,581: t15.2023.09.03 val PER: 0.1746
2026-01-10 08:08:32,581: t15.2023.09.24 val PER: 0.1311
2026-01-10 08:08:32,581: t15.2023.09.29 val PER: 0.1385
2026-01-10 08:08:32,581: t15.2023.10.01 val PER: 0.1856
2026-01-10 08:08:32,581: t15.2023.10.06 val PER: 0.0915
2026-01-10 08:08:32,581: t15.2023.10.08 val PER: 0.2625
2026-01-10 08:08:32,581: t15.2023.10.13 val PER: 0.2258
2026-01-10 08:08:32,582: t15.2023.10.15 val PER: 0.1714
2026-01-10 08:08:32,582: t15.2023.10.20 val PER: 0.2081
2026-01-10 08:08:32,582: t15.2023.10.22 val PER: 0.1292
2026-01-10 08:08:32,582: t15.2023.11.03 val PER: 0.1859
2026-01-10 08:08:32,582: t15.2023.11.04 val PER: 0.0444
2026-01-10 08:08:32,582: t15.2023.11.17 val PER: 0.0529
2026-01-10 08:08:32,582: t15.2023.11.19 val PER: 0.0519
2026-01-10 08:08:32,582: t15.2023.11.26 val PER: 0.1399
2026-01-10 08:08:32,583: t15.2023.12.03 val PER: 0.1271
2026-01-10 08:08:32,583: t15.2023.12.08 val PER: 0.1205
2026-01-10 08:08:32,583: t15.2023.12.10 val PER: 0.1143
2026-01-10 08:08:32,583: t15.2023.12.17 val PER: 0.1507
2026-01-10 08:08:32,583: t15.2023.12.29 val PER: 0.1469
2026-01-10 08:08:32,583: t15.2024.02.25 val PER: 0.1278
2026-01-10 08:08:32,583: t15.2024.03.08 val PER: 0.2376
2026-01-10 08:08:32,583: t15.2024.03.15 val PER: 0.2270
2026-01-10 08:08:32,583: t15.2024.03.17 val PER: 0.1506
2026-01-10 08:08:32,583: t15.2024.05.10 val PER: 0.1813
2026-01-10 08:08:32,583: t15.2024.06.14 val PER: 0.1735
2026-01-10 08:08:32,583: t15.2024.07.19 val PER: 0.2670
2026-01-10 08:08:32,584: t15.2024.07.21 val PER: 0.1069
2026-01-10 08:08:32,584: t15.2024.07.28 val PER: 0.1537
2026-01-10 08:08:32,584: t15.2025.01.10 val PER: 0.3209
2026-01-10 08:08:32,584: t15.2025.01.12 val PER: 0.1786
2026-01-10 08:08:32,584: t15.2025.03.14 val PER: 0.3417
2026-01-10 08:08:32,584: t15.2025.03.16 val PER: 0.2186
2026-01-10 08:08:32,584: t15.2025.03.30 val PER: 0.3023
2026-01-10 08:08:32,584: t15.2025.04.13 val PER: 0.2340
2026-01-10 08:08:32,585: New best val WER(5gram) 38.98% --> 29.73%
2026-01-10 08:08:32,771: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_9500
2026-01-10 08:08:42,729: Train batch 9600: loss: 7.75 grad norm: 42.89 time: 0.074
2026-01-10 08:09:01,278: Train batch 9800: loss: 12.15 grad norm: 60.97 time: 0.063
2026-01-10 08:09:21,194: Train batch 10000: loss: 5.40 grad norm: 44.02 time: 0.061
2026-01-10 08:09:21,195: Running test after training batch: 10000
2026-01-10 08:09:21,619: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:09:27,018: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point will
2026-01-10 08:09:27,072: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-10 08:09:39,580: Val batch 10000: PER (avg): 0.1631 CTC Loss (avg): 16.2385 WER(5gram): 30.77% (n=256) time: 18.385
2026-01-10 08:09:39,580: WER lens: avg_true_words=5.99 avg_pred_words=6.27 max_pred_words=13
2026-01-10 08:09:39,581: t15.2023.08.13 val PER: 0.1237
2026-01-10 08:09:39,581: t15.2023.08.18 val PER: 0.1199
2026-01-10 08:09:39,581: t15.2023.08.20 val PER: 0.1088
2026-01-10 08:09:39,581: t15.2023.08.25 val PER: 0.1220
2026-01-10 08:09:39,581: t15.2023.08.27 val PER: 0.1977
2026-01-10 08:09:39,581: t15.2023.09.01 val PER: 0.0844
2026-01-10 08:09:39,581: t15.2023.09.03 val PER: 0.1793
2026-01-10 08:09:39,581: t15.2023.09.24 val PER: 0.1432
2026-01-10 08:09:39,582: t15.2023.09.29 val PER: 0.1295
2026-01-10 08:09:39,582: t15.2023.10.01 val PER: 0.1777
2026-01-10 08:09:39,582: t15.2023.10.06 val PER: 0.0926
2026-01-10 08:09:39,582: t15.2023.10.08 val PER: 0.2598
2026-01-10 08:09:39,582: t15.2023.10.13 val PER: 0.2118
2026-01-10 08:09:39,582: t15.2023.10.15 val PER: 0.1688
2026-01-10 08:09:39,582: t15.2023.10.20 val PER: 0.1980
2026-01-10 08:09:39,582: t15.2023.10.22 val PER: 0.1336
2026-01-10 08:09:39,582: t15.2023.11.03 val PER: 0.1798
2026-01-10 08:09:39,582: t15.2023.11.04 val PER: 0.0375
2026-01-10 08:09:39,583: t15.2023.11.17 val PER: 0.0513
2026-01-10 08:09:39,583: t15.2023.11.19 val PER: 0.0419
2026-01-10 08:09:39,583: t15.2023.11.26 val PER: 0.1348
2026-01-10 08:09:39,583: t15.2023.12.03 val PER: 0.1218
2026-01-10 08:09:39,583: t15.2023.12.08 val PER: 0.1252
2026-01-10 08:09:39,583: t15.2023.12.10 val PER: 0.1104
2026-01-10 08:09:39,583: t15.2023.12.17 val PER: 0.1445
2026-01-10 08:09:39,583: t15.2023.12.29 val PER: 0.1407
2026-01-10 08:09:39,583: t15.2024.02.25 val PER: 0.1278
2026-01-10 08:09:39,584: t15.2024.03.08 val PER: 0.2361
2026-01-10 08:09:39,584: t15.2024.03.15 val PER: 0.2220
2026-01-10 08:09:39,584: t15.2024.03.17 val PER: 0.1562
2026-01-10 08:09:39,584: t15.2024.05.10 val PER: 0.1560
2026-01-10 08:09:39,584: t15.2024.06.14 val PER: 0.1751
2026-01-10 08:09:39,584: t15.2024.07.19 val PER: 0.2531
2026-01-10 08:09:39,584: t15.2024.07.21 val PER: 0.1131
2026-01-10 08:09:39,584: t15.2024.07.28 val PER: 0.1456
2026-01-10 08:09:39,584: t15.2025.01.10 val PER: 0.3209
2026-01-10 08:09:39,584: t15.2025.01.12 val PER: 0.1686
2026-01-10 08:09:39,584: t15.2025.03.14 val PER: 0.3476
2026-01-10 08:09:39,585: t15.2025.03.16 val PER: 0.2068
2026-01-10 08:09:39,585: t15.2025.03.30 val PER: 0.2943
2026-01-10 08:09:39,585: t15.2025.04.13 val PER: 0.2397
2026-01-10 08:09:39,728: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_10000
2026-01-10 08:09:58,414: Train batch 10200: loss: 5.73 grad norm: 37.69 time: 0.050
2026-01-10 08:10:17,752: Train batch 10400: loss: 8.26 grad norm: 46.32 time: 0.074
2026-01-10 08:10:27,545: Running test after training batch: 10500
2026-01-10 08:10:27,908: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:10:33,016: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:10:33,069: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 08:10:45,105: Val batch 10500: PER (avg): 0.1596 CTC Loss (avg): 15.8893 WER(5gram): 30.31% (n=256) time: 17.559
2026-01-10 08:10:45,105: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=13
2026-01-10 08:10:45,105: t15.2023.08.13 val PER: 0.1195
2026-01-10 08:10:45,105: t15.2023.08.18 val PER: 0.1140
2026-01-10 08:10:45,105: t15.2023.08.20 val PER: 0.1112
2026-01-10 08:10:45,106: t15.2023.08.25 val PER: 0.1205
2026-01-10 08:10:45,106: t15.2023.08.27 val PER: 0.1849
2026-01-10 08:10:45,106: t15.2023.09.01 val PER: 0.0974
2026-01-10 08:10:45,106: t15.2023.09.03 val PER: 0.1841
2026-01-10 08:10:45,106: t15.2023.09.24 val PER: 0.1311
2026-01-10 08:10:45,106: t15.2023.09.29 val PER: 0.1340
2026-01-10 08:10:45,106: t15.2023.10.01 val PER: 0.1843
2026-01-10 08:10:45,106: t15.2023.10.06 val PER: 0.0936
2026-01-10 08:10:45,106: t15.2023.10.08 val PER: 0.2517
2026-01-10 08:10:45,106: t15.2023.10.13 val PER: 0.2009
2026-01-10 08:10:45,106: t15.2023.10.15 val PER: 0.1668
2026-01-10 08:10:45,106: t15.2023.10.20 val PER: 0.1913
2026-01-10 08:10:45,106: t15.2023.10.22 val PER: 0.1370
2026-01-10 08:10:45,107: t15.2023.11.03 val PER: 0.1839
2026-01-10 08:10:45,107: t15.2023.11.04 val PER: 0.0341
2026-01-10 08:10:45,107: t15.2023.11.17 val PER: 0.0467
2026-01-10 08:10:45,107: t15.2023.11.19 val PER: 0.0579
2026-01-10 08:10:45,107: t15.2023.11.26 val PER: 0.1406
2026-01-10 08:10:45,107: t15.2023.12.03 val PER: 0.1208
2026-01-10 08:10:45,107: t15.2023.12.08 val PER: 0.1165
2026-01-10 08:10:45,107: t15.2023.12.10 val PER: 0.0972
2026-01-10 08:10:45,107: t15.2023.12.17 val PER: 0.1393
2026-01-10 08:10:45,107: t15.2023.12.29 val PER: 0.1476
2026-01-10 08:10:45,107: t15.2024.02.25 val PER: 0.1110
2026-01-10 08:10:45,107: t15.2024.03.08 val PER: 0.2276
2026-01-10 08:10:45,107: t15.2024.03.15 val PER: 0.2058
2026-01-10 08:10:45,108: t15.2024.03.17 val PER: 0.1478
2026-01-10 08:10:45,108: t15.2024.05.10 val PER: 0.1605
2026-01-10 08:10:45,108: t15.2024.06.14 val PER: 0.1782
2026-01-10 08:10:45,108: t15.2024.07.19 val PER: 0.2432
2026-01-10 08:10:45,108: t15.2024.07.21 val PER: 0.1041
2026-01-10 08:10:45,108: t15.2024.07.28 val PER: 0.1412
2026-01-10 08:10:45,108: t15.2025.01.10 val PER: 0.2989
2026-01-10 08:10:45,108: t15.2025.01.12 val PER: 0.1601
2026-01-10 08:10:45,108: t15.2025.03.14 val PER: 0.3373
2026-01-10 08:10:45,108: t15.2025.03.16 val PER: 0.1898
2026-01-10 08:10:45,108: t15.2025.03.30 val PER: 0.2977
2026-01-10 08:10:45,108: t15.2025.04.13 val PER: 0.2397
2026-01-10 08:10:45,256: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_10500
2026-01-10 08:10:54,821: Train batch 10600: loss: 8.32 grad norm: 60.77 time: 0.074
2026-01-10 08:11:13,975: Train batch 10800: loss: 14.31 grad norm: 65.38 time: 0.065
2026-01-10 08:11:33,273: Train batch 11000: loss: 12.62 grad norm: 55.92 time: 0.057
2026-01-10 08:11:33,273: Running test after training batch: 11000
2026-01-10 08:11:33,457: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:11:38,711: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:11:38,776: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 08:11:51,069: Val batch 11000: PER (avg): 0.1581 CTC Loss (avg): 15.7881 WER(5gram): 33.38% (n=256) time: 17.796
2026-01-10 08:11:51,070: WER lens: avg_true_words=5.99 avg_pred_words=6.30 max_pred_words=13
2026-01-10 08:11:51,070: t15.2023.08.13 val PER: 0.1175
2026-01-10 08:11:51,070: t15.2023.08.18 val PER: 0.1157
2026-01-10 08:11:51,070: t15.2023.08.20 val PER: 0.1088
2026-01-10 08:11:51,070: t15.2023.08.25 val PER: 0.1054
2026-01-10 08:11:51,070: t15.2023.08.27 val PER: 0.1849
2026-01-10 08:11:51,070: t15.2023.09.01 val PER: 0.0885
2026-01-10 08:11:51,070: t15.2023.09.03 val PER: 0.1746
2026-01-10 08:11:51,071: t15.2023.09.24 val PER: 0.1311
2026-01-10 08:11:51,071: t15.2023.09.29 val PER: 0.1321
2026-01-10 08:11:51,071: t15.2023.10.01 val PER: 0.1777
2026-01-10 08:11:51,071: t15.2023.10.06 val PER: 0.0872
2026-01-10 08:11:51,071: t15.2023.10.08 val PER: 0.2517
2026-01-10 08:11:51,071: t15.2023.10.13 val PER: 0.2025
2026-01-10 08:11:51,071: t15.2023.10.15 val PER: 0.1641
2026-01-10 08:11:51,071: t15.2023.10.20 val PER: 0.2047
2026-01-10 08:11:51,071: t15.2023.10.22 val PER: 0.1303
2026-01-10 08:11:51,071: t15.2023.11.03 val PER: 0.1872
2026-01-10 08:11:51,072: t15.2023.11.04 val PER: 0.0375
2026-01-10 08:11:51,072: t15.2023.11.17 val PER: 0.0435
2026-01-10 08:11:51,072: t15.2023.11.19 val PER: 0.0479
2026-01-10 08:11:51,072: t15.2023.11.26 val PER: 0.1341
2026-01-10 08:11:51,072: t15.2023.12.03 val PER: 0.1155
2026-01-10 08:11:51,072: t15.2023.12.08 val PER: 0.1205
2026-01-10 08:11:51,072: t15.2023.12.10 val PER: 0.1078
2026-01-10 08:11:51,072: t15.2023.12.17 val PER: 0.1372
2026-01-10 08:11:51,072: t15.2023.12.29 val PER: 0.1359
2026-01-10 08:11:51,072: t15.2024.02.25 val PER: 0.1264
2026-01-10 08:11:51,072: t15.2024.03.08 val PER: 0.2361
2026-01-10 08:11:51,072: t15.2024.03.15 val PER: 0.2220
2026-01-10 08:11:51,072: t15.2024.03.17 val PER: 0.1437
2026-01-10 08:11:51,072: t15.2024.05.10 val PER: 0.1813
2026-01-10 08:11:51,073: t15.2024.06.14 val PER: 0.1656
2026-01-10 08:11:51,073: t15.2024.07.19 val PER: 0.2498
2026-01-10 08:11:51,073: t15.2024.07.21 val PER: 0.0979
2026-01-10 08:11:51,073: t15.2024.07.28 val PER: 0.1404
2026-01-10 08:11:51,073: t15.2025.01.10 val PER: 0.2934
2026-01-10 08:11:51,073: t15.2025.01.12 val PER: 0.1409
2026-01-10 08:11:51,073: t15.2025.03.14 val PER: 0.3388
2026-01-10 08:11:51,073: t15.2025.03.16 val PER: 0.1937
2026-01-10 08:11:51,073: t15.2025.03.30 val PER: 0.3011
2026-01-10 08:11:51,073: t15.2025.04.13 val PER: 0.2282
2026-01-10 08:11:51,218: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_11000
2026-01-10 08:12:10,227: Train batch 11200: loss: 10.33 grad norm: 53.41 time: 0.072
2026-01-10 08:12:29,003: Train batch 11400: loss: 9.12 grad norm: 53.92 time: 0.057
2026-01-10 08:12:38,497: Running test after training batch: 11500
2026-01-10 08:12:38,690: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:12:43,800: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:12:43,853: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 08:12:55,904: Val batch 11500: PER (avg): 0.1550 CTC Loss (avg): 15.6946 WER(5gram): 32.99% (n=256) time: 17.406
2026-01-10 08:12:55,904: WER lens: avg_true_words=5.99 avg_pred_words=6.41 max_pred_words=12
2026-01-10 08:12:55,904: t15.2023.08.13 val PER: 0.1164
2026-01-10 08:12:55,905: t15.2023.08.18 val PER: 0.1081
2026-01-10 08:12:55,905: t15.2023.08.20 val PER: 0.1112
2026-01-10 08:12:55,905: t15.2023.08.25 val PER: 0.1099
2026-01-10 08:12:55,905: t15.2023.08.27 val PER: 0.1977
2026-01-10 08:12:55,905: t15.2023.09.01 val PER: 0.0820
2026-01-10 08:12:55,905: t15.2023.09.03 val PER: 0.1663
2026-01-10 08:12:55,905: t15.2023.09.24 val PER: 0.1250
2026-01-10 08:12:55,905: t15.2023.09.29 val PER: 0.1353
2026-01-10 08:12:55,905: t15.2023.10.01 val PER: 0.1750
2026-01-10 08:12:55,905: t15.2023.10.06 val PER: 0.0861
2026-01-10 08:12:55,906: t15.2023.10.08 val PER: 0.2341
2026-01-10 08:12:55,906: t15.2023.10.13 val PER: 0.2033
2026-01-10 08:12:55,906: t15.2023.10.15 val PER: 0.1589
2026-01-10 08:12:55,906: t15.2023.10.20 val PER: 0.1946
2026-01-10 08:12:55,906: t15.2023.10.22 val PER: 0.1203
2026-01-10 08:12:55,906: t15.2023.11.03 val PER: 0.1764
2026-01-10 08:12:55,906: t15.2023.11.04 val PER: 0.0273
2026-01-10 08:12:55,906: t15.2023.11.17 val PER: 0.0404
2026-01-10 08:12:55,906: t15.2023.11.19 val PER: 0.0339
2026-01-10 08:12:55,906: t15.2023.11.26 val PER: 0.1341
2026-01-10 08:12:55,906: t15.2023.12.03 val PER: 0.1113
2026-01-10 08:12:55,907: t15.2023.12.08 val PER: 0.1032
2026-01-10 08:12:55,907: t15.2023.12.10 val PER: 0.0959
2026-01-10 08:12:55,907: t15.2023.12.17 val PER: 0.1320
2026-01-10 08:12:55,907: t15.2023.12.29 val PER: 0.1304
2026-01-10 08:12:55,907: t15.2024.02.25 val PER: 0.1236
2026-01-10 08:12:55,907: t15.2024.03.08 val PER: 0.2119
2026-01-10 08:12:55,907: t15.2024.03.15 val PER: 0.2139
2026-01-10 08:12:55,907: t15.2024.03.17 val PER: 0.1416
2026-01-10 08:12:55,907: t15.2024.05.10 val PER: 0.1709
2026-01-10 08:12:55,907: t15.2024.06.14 val PER: 0.1546
2026-01-10 08:12:55,907: t15.2024.07.19 val PER: 0.2525
2026-01-10 08:12:55,908: t15.2024.07.21 val PER: 0.1055
2026-01-10 08:12:55,908: t15.2024.07.28 val PER: 0.1426
2026-01-10 08:12:55,908: t15.2025.01.10 val PER: 0.2989
2026-01-10 08:12:55,908: t15.2025.01.12 val PER: 0.1517
2026-01-10 08:12:55,908: t15.2025.03.14 val PER: 0.3506
2026-01-10 08:12:55,908: t15.2025.03.16 val PER: 0.1911
2026-01-10 08:12:55,908: t15.2025.03.30 val PER: 0.2920
2026-01-10 08:12:55,908: t15.2025.04.13 val PER: 0.2439
2026-01-10 08:12:56,048: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_11500
2026-01-10 08:13:05,179: Train batch 11600: loss: 10.50 grad norm: 48.14 time: 0.061
2026-01-10 08:13:23,832: Train batch 11800: loss: 6.00 grad norm: 40.94 time: 0.044
2026-01-10 08:13:42,590: Train batch 12000: loss: 12.46 grad norm: 53.49 time: 0.073
2026-01-10 08:13:42,591: Running test after training batch: 12000
2026-01-10 08:13:42,707: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:13:47,770: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:13:47,827: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 08:14:00,749: Val batch 12000: PER (avg): 0.1514 CTC Loss (avg): 15.6160 WER(5gram): 33.96% (n=256) time: 18.158
2026-01-10 08:14:00,749: WER lens: avg_true_words=5.99 avg_pred_words=6.43 max_pred_words=12
2026-01-10 08:14:00,749: t15.2023.08.13 val PER: 0.1268
2026-01-10 08:14:00,750: t15.2023.08.18 val PER: 0.1031
2026-01-10 08:14:00,750: t15.2023.08.20 val PER: 0.1064
2026-01-10 08:14:00,750: t15.2023.08.25 val PER: 0.1069
2026-01-10 08:14:00,750: t15.2023.08.27 val PER: 0.1833
2026-01-10 08:14:00,750: t15.2023.09.01 val PER: 0.0795
2026-01-10 08:14:00,750: t15.2023.09.03 val PER: 0.1722
2026-01-10 08:14:00,750: t15.2023.09.24 val PER: 0.1286
2026-01-10 08:14:00,750: t15.2023.09.29 val PER: 0.1244
2026-01-10 08:14:00,751: t15.2023.10.01 val PER: 0.1757
2026-01-10 08:14:00,751: t15.2023.10.06 val PER: 0.0861
2026-01-10 08:14:00,751: t15.2023.10.08 val PER: 0.2287
2026-01-10 08:14:00,751: t15.2023.10.13 val PER: 0.1901
2026-01-10 08:14:00,751: t15.2023.10.15 val PER: 0.1615
2026-01-10 08:14:00,751: t15.2023.10.20 val PER: 0.1779
2026-01-10 08:14:00,751: t15.2023.10.22 val PER: 0.1203
2026-01-10 08:14:00,751: t15.2023.11.03 val PER: 0.1777
2026-01-10 08:14:00,751: t15.2023.11.04 val PER: 0.0273
2026-01-10 08:14:00,751: t15.2023.11.17 val PER: 0.0373
2026-01-10 08:14:00,751: t15.2023.11.19 val PER: 0.0299
2026-01-10 08:14:00,752: t15.2023.11.26 val PER: 0.1239
2026-01-10 08:14:00,752: t15.2023.12.03 val PER: 0.1040
2026-01-10 08:14:00,752: t15.2023.12.08 val PER: 0.1019
2026-01-10 08:14:00,752: t15.2023.12.10 val PER: 0.1012
2026-01-10 08:14:00,752: t15.2023.12.17 val PER: 0.1279
2026-01-10 08:14:00,752: t15.2023.12.29 val PER: 0.1386
2026-01-10 08:14:00,752: t15.2024.02.25 val PER: 0.1110
2026-01-10 08:14:00,752: t15.2024.03.08 val PER: 0.2119
2026-01-10 08:14:00,752: t15.2024.03.15 val PER: 0.2114
2026-01-10 08:14:00,752: t15.2024.03.17 val PER: 0.1346
2026-01-10 08:14:00,753: t15.2024.05.10 val PER: 0.1694
2026-01-10 08:14:00,753: t15.2024.06.14 val PER: 0.1703
2026-01-10 08:14:00,753: t15.2024.07.19 val PER: 0.2518
2026-01-10 08:14:00,753: t15.2024.07.21 val PER: 0.0993
2026-01-10 08:14:00,753: t15.2024.07.28 val PER: 0.1309
2026-01-10 08:14:00,753: t15.2025.01.10 val PER: 0.2810
2026-01-10 08:14:00,753: t15.2025.01.12 val PER: 0.1470
2026-01-10 08:14:00,753: t15.2025.03.14 val PER: 0.3314
2026-01-10 08:14:00,753: t15.2025.03.16 val PER: 0.1832
2026-01-10 08:14:00,753: t15.2025.03.30 val PER: 0.2943
2026-01-10 08:14:00,753: t15.2025.04.13 val PER: 0.2240
2026-01-10 08:14:00,896: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_12000
2026-01-10 08:14:19,600: Train batch 12200: loss: 4.96 grad norm: 36.08 time: 0.068
2026-01-10 08:14:38,149: Train batch 12400: loss: 4.17 grad norm: 35.63 time: 0.041
2026-01-10 08:14:47,753: Running test after training batch: 12500
2026-01-10 08:14:47,910: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:14:52,995: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:14:53,063: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-10 08:15:05,971: Val batch 12500: PER (avg): 0.1494 CTC Loss (avg): 15.2132 WER(5gram): 27.71% (n=256) time: 18.217
2026-01-10 08:15:05,971: WER lens: avg_true_words=5.99 avg_pred_words=6.32 max_pred_words=13
2026-01-10 08:15:05,972: t15.2023.08.13 val PER: 0.1071
2026-01-10 08:15:05,972: t15.2023.08.18 val PER: 0.1006
2026-01-10 08:15:05,972: t15.2023.08.20 val PER: 0.1096
2026-01-10 08:15:05,972: t15.2023.08.25 val PER: 0.1024
2026-01-10 08:15:05,972: t15.2023.08.27 val PER: 0.1881
2026-01-10 08:15:05,972: t15.2023.09.01 val PER: 0.0706
2026-01-10 08:15:05,972: t15.2023.09.03 val PER: 0.1556
2026-01-10 08:15:05,972: t15.2023.09.24 val PER: 0.1262
2026-01-10 08:15:05,972: t15.2023.09.29 val PER: 0.1264
2026-01-10 08:15:05,973: t15.2023.10.01 val PER: 0.1757
2026-01-10 08:15:05,973: t15.2023.10.06 val PER: 0.0840
2026-01-10 08:15:05,973: t15.2023.10.08 val PER: 0.2449
2026-01-10 08:15:05,973: t15.2023.10.13 val PER: 0.1986
2026-01-10 08:15:05,973: t15.2023.10.15 val PER: 0.1589
2026-01-10 08:15:05,973: t15.2023.10.20 val PER: 0.1812
2026-01-10 08:15:05,973: t15.2023.10.22 val PER: 0.1114
2026-01-10 08:15:05,973: t15.2023.11.03 val PER: 0.1750
2026-01-10 08:15:05,973: t15.2023.11.04 val PER: 0.0307
2026-01-10 08:15:05,973: t15.2023.11.17 val PER: 0.0435
2026-01-10 08:15:05,973: t15.2023.11.19 val PER: 0.0359
2026-01-10 08:15:05,973: t15.2023.11.26 val PER: 0.1225
2026-01-10 08:15:05,974: t15.2023.12.03 val PER: 0.1092
2026-01-10 08:15:05,974: t15.2023.12.08 val PER: 0.1012
2026-01-10 08:15:05,974: t15.2023.12.10 val PER: 0.0959
2026-01-10 08:15:05,974: t15.2023.12.17 val PER: 0.1310
2026-01-10 08:15:05,974: t15.2023.12.29 val PER: 0.1325
2026-01-10 08:15:05,974: t15.2024.02.25 val PER: 0.1222
2026-01-10 08:15:05,974: t15.2024.03.08 val PER: 0.2020
2026-01-10 08:15:05,974: t15.2024.03.15 val PER: 0.2101
2026-01-10 08:15:05,974: t15.2024.03.17 val PER: 0.1346
2026-01-10 08:15:05,974: t15.2024.05.10 val PER: 0.1634
2026-01-10 08:15:05,974: t15.2024.06.14 val PER: 0.1656
2026-01-10 08:15:05,975: t15.2024.07.19 val PER: 0.2380
2026-01-10 08:15:05,975: t15.2024.07.21 val PER: 0.0959
2026-01-10 08:15:05,975: t15.2024.07.28 val PER: 0.1301
2026-01-10 08:15:05,975: t15.2025.01.10 val PER: 0.2686
2026-01-10 08:15:05,975: t15.2025.01.12 val PER: 0.1470
2026-01-10 08:15:05,975: t15.2025.03.14 val PER: 0.3343
2026-01-10 08:15:05,975: t15.2025.03.16 val PER: 0.1819
2026-01-10 08:15:05,975: t15.2025.03.30 val PER: 0.2954
2026-01-10 08:15:05,975: t15.2025.04.13 val PER: 0.2225
2026-01-10 08:15:05,976: New best val WER(5gram) 29.73% --> 27.71%
2026-01-10 08:15:06,163: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_12500
2026-01-10 08:15:15,645: Train batch 12600: loss: 6.93 grad norm: 40.41 time: 0.058
2026-01-10 08:15:35,027: Train batch 12800: loss: 5.10 grad norm: 37.54 time: 0.052
2026-01-10 08:15:54,480: Train batch 13000: loss: 5.47 grad norm: 39.38 time: 0.067
2026-01-10 08:15:54,480: Running test after training batch: 13000
2026-01-10 08:15:54,581: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:15:59,650: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:15:59,703: WER debug example
  GT : how does it keep the cost down
  PR : how dense it keep the cost
2026-01-10 08:16:11,843: Val batch 13000: PER (avg): 0.1482 CTC Loss (avg): 15.0647 WER(5gram): 28.36% (n=256) time: 17.363
2026-01-10 08:16:11,844: WER lens: avg_true_words=5.99 avg_pred_words=6.26 max_pred_words=12
2026-01-10 08:16:11,844: t15.2023.08.13 val PER: 0.1102
2026-01-10 08:16:11,844: t15.2023.08.18 val PER: 0.0981
2026-01-10 08:16:11,844: t15.2023.08.20 val PER: 0.0969
2026-01-10 08:16:11,845: t15.2023.08.25 val PER: 0.0994
2026-01-10 08:16:11,845: t15.2023.08.27 val PER: 0.1945
2026-01-10 08:16:11,845: t15.2023.09.01 val PER: 0.0722
2026-01-10 08:16:11,845: t15.2023.09.03 val PER: 0.1615
2026-01-10 08:16:11,845: t15.2023.09.24 val PER: 0.1189
2026-01-10 08:16:11,845: t15.2023.09.29 val PER: 0.1321
2026-01-10 08:16:11,845: t15.2023.10.01 val PER: 0.1731
2026-01-10 08:16:11,845: t15.2023.10.06 val PER: 0.0872
2026-01-10 08:16:11,845: t15.2023.10.08 val PER: 0.2341
2026-01-10 08:16:11,846: t15.2023.10.13 val PER: 0.1932
2026-01-10 08:16:11,846: t15.2023.10.15 val PER: 0.1562
2026-01-10 08:16:11,846: t15.2023.10.20 val PER: 0.1846
2026-01-10 08:16:11,846: t15.2023.10.22 val PER: 0.1136
2026-01-10 08:16:11,846: t15.2023.11.03 val PER: 0.1757
2026-01-10 08:16:11,846: t15.2023.11.04 val PER: 0.0273
2026-01-10 08:16:11,846: t15.2023.11.17 val PER: 0.0327
2026-01-10 08:16:11,846: t15.2023.11.19 val PER: 0.0399
2026-01-10 08:16:11,846: t15.2023.11.26 val PER: 0.1239
2026-01-10 08:16:11,846: t15.2023.12.03 val PER: 0.1155
2026-01-10 08:16:11,847: t15.2023.12.08 val PER: 0.1005
2026-01-10 08:16:11,847: t15.2023.12.10 val PER: 0.0920
2026-01-10 08:16:11,847: t15.2023.12.17 val PER: 0.1310
2026-01-10 08:16:11,847: t15.2023.12.29 val PER: 0.1263
2026-01-10 08:16:11,847: t15.2024.02.25 val PER: 0.1096
2026-01-10 08:16:11,847: t15.2024.03.08 val PER: 0.2020
2026-01-10 08:16:11,847: t15.2024.03.15 val PER: 0.2051
2026-01-10 08:16:11,847: t15.2024.03.17 val PER: 0.1283
2026-01-10 08:16:11,847: t15.2024.05.10 val PER: 0.1664
2026-01-10 08:16:11,847: t15.2024.06.14 val PER: 0.1498
2026-01-10 08:16:11,847: t15.2024.07.19 val PER: 0.2472
2026-01-10 08:16:11,848: t15.2024.07.21 val PER: 0.0910
2026-01-10 08:16:11,848: t15.2024.07.28 val PER: 0.1316
2026-01-10 08:16:11,848: t15.2025.01.10 val PER: 0.2769
2026-01-10 08:16:11,848: t15.2025.01.12 val PER: 0.1463
2026-01-10 08:16:11,848: t15.2025.03.14 val PER: 0.3284
2026-01-10 08:16:11,848: t15.2025.03.16 val PER: 0.1885
2026-01-10 08:16:11,848: t15.2025.03.30 val PER: 0.2931
2026-01-10 08:16:11,848: t15.2025.04.13 val PER: 0.2211
2026-01-10 08:16:11,999: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_13000
2026-01-10 08:16:31,030: Train batch 13200: loss: 12.40 grad norm: 68.27 time: 0.054
2026-01-10 08:16:50,063: Train batch 13400: loss: 8.41 grad norm: 54.80 time: 0.062
2026-01-10 08:16:59,613: Running test after training batch: 13500
2026-01-10 08:16:59,738: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:17:04,773: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:17:04,826: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 08:17:17,081: Val batch 13500: PER (avg): 0.1451 CTC Loss (avg): 14.8914 WER(5gram): 30.90% (n=256) time: 17.468
2026-01-10 08:17:17,081: WER lens: avg_true_words=5.99 avg_pred_words=6.39 max_pred_words=14
2026-01-10 08:17:17,082: t15.2023.08.13 val PER: 0.1081
2026-01-10 08:17:17,082: t15.2023.08.18 val PER: 0.0947
2026-01-10 08:17:17,082: t15.2023.08.20 val PER: 0.1001
2026-01-10 08:17:17,082: t15.2023.08.25 val PER: 0.1084
2026-01-10 08:17:17,082: t15.2023.08.27 val PER: 0.1817
2026-01-10 08:17:17,082: t15.2023.09.01 val PER: 0.0763
2026-01-10 08:17:17,082: t15.2023.09.03 val PER: 0.1675
2026-01-10 08:17:17,082: t15.2023.09.24 val PER: 0.1201
2026-01-10 08:17:17,082: t15.2023.09.29 val PER: 0.1232
2026-01-10 08:17:17,083: t15.2023.10.01 val PER: 0.1744
2026-01-10 08:17:17,083: t15.2023.10.06 val PER: 0.0850
2026-01-10 08:17:17,083: t15.2023.10.08 val PER: 0.2422
2026-01-10 08:17:17,083: t15.2023.10.13 val PER: 0.1893
2026-01-10 08:17:17,083: t15.2023.10.15 val PER: 0.1562
2026-01-10 08:17:17,083: t15.2023.10.20 val PER: 0.1745
2026-01-10 08:17:17,083: t15.2023.10.22 val PER: 0.1091
2026-01-10 08:17:17,083: t15.2023.11.03 val PER: 0.1710
2026-01-10 08:17:17,083: t15.2023.11.04 val PER: 0.0307
2026-01-10 08:17:17,083: t15.2023.11.17 val PER: 0.0420
2026-01-10 08:17:17,084: t15.2023.11.19 val PER: 0.0200
2026-01-10 08:17:17,084: t15.2023.11.26 val PER: 0.1225
2026-01-10 08:17:17,084: t15.2023.12.03 val PER: 0.0987
2026-01-10 08:17:17,084: t15.2023.12.08 val PER: 0.0972
2026-01-10 08:17:17,084: t15.2023.12.10 val PER: 0.0933
2026-01-10 08:17:17,084: t15.2023.12.17 val PER: 0.1133
2026-01-10 08:17:17,084: t15.2023.12.29 val PER: 0.1263
2026-01-10 08:17:17,084: t15.2024.02.25 val PER: 0.1053
2026-01-10 08:17:17,084: t15.2024.03.08 val PER: 0.1991
2026-01-10 08:17:17,084: t15.2024.03.15 val PER: 0.1976
2026-01-10 08:17:17,084: t15.2024.03.17 val PER: 0.1297
2026-01-10 08:17:17,085: t15.2024.05.10 val PER: 0.1709
2026-01-10 08:17:17,085: t15.2024.06.14 val PER: 0.1451
2026-01-10 08:17:17,085: t15.2024.07.19 val PER: 0.2353
2026-01-10 08:17:17,085: t15.2024.07.21 val PER: 0.0897
2026-01-10 08:17:17,085: t15.2024.07.28 val PER: 0.1309
2026-01-10 08:17:17,085: t15.2025.01.10 val PER: 0.2741
2026-01-10 08:17:17,085: t15.2025.01.12 val PER: 0.1355
2026-01-10 08:17:17,085: t15.2025.03.14 val PER: 0.3328
2026-01-10 08:17:17,085: t15.2025.03.16 val PER: 0.1832
2026-01-10 08:17:17,085: t15.2025.03.30 val PER: 0.2828
2026-01-10 08:17:17,086: t15.2025.04.13 val PER: 0.2197
2026-01-10 08:17:17,227: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_13500
2026-01-10 08:17:26,807: Train batch 13600: loss: 11.40 grad norm: 62.17 time: 0.062
2026-01-10 08:17:45,713: Train batch 13800: loss: 7.67 grad norm: 53.10 time: 0.056
2026-01-10 08:18:04,117: Train batch 14000: loss: 10.89 grad norm: 62.98 time: 0.051
2026-01-10 08:18:04,118: Running test after training batch: 14000
2026-01-10 08:18:04,253: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:18:09,404: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:18:09,461: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 08:18:22,040: Val batch 14000: PER (avg): 0.1441 CTC Loss (avg): 14.7315 WER(5gram): 31.75% (n=256) time: 17.922
2026-01-10 08:18:22,041: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=12
2026-01-10 08:18:22,041: t15.2023.08.13 val PER: 0.1071
2026-01-10 08:18:22,042: t15.2023.08.18 val PER: 0.0972
2026-01-10 08:18:22,042: t15.2023.08.20 val PER: 0.1025
2026-01-10 08:18:22,042: t15.2023.08.25 val PER: 0.1054
2026-01-10 08:18:22,042: t15.2023.08.27 val PER: 0.1752
2026-01-10 08:18:22,042: t15.2023.09.01 val PER: 0.0698
2026-01-10 08:18:22,042: t15.2023.09.03 val PER: 0.1686
2026-01-10 08:18:22,043: t15.2023.09.24 val PER: 0.1274
2026-01-10 08:18:22,043: t15.2023.09.29 val PER: 0.1238
2026-01-10 08:18:22,043: t15.2023.10.01 val PER: 0.1697
2026-01-10 08:18:22,043: t15.2023.10.06 val PER: 0.0829
2026-01-10 08:18:22,043: t15.2023.10.08 val PER: 0.2314
2026-01-10 08:18:22,043: t15.2023.10.13 val PER: 0.1831
2026-01-10 08:18:22,043: t15.2023.10.15 val PER: 0.1549
2026-01-10 08:18:22,043: t15.2023.10.20 val PER: 0.1946
2026-01-10 08:18:22,043: t15.2023.10.22 val PER: 0.1114
2026-01-10 08:18:22,044: t15.2023.11.03 val PER: 0.1784
2026-01-10 08:18:22,044: t15.2023.11.04 val PER: 0.0273
2026-01-10 08:18:22,044: t15.2023.11.17 val PER: 0.0373
2026-01-10 08:18:22,044: t15.2023.11.19 val PER: 0.0319
2026-01-10 08:18:22,044: t15.2023.11.26 val PER: 0.1152
2026-01-10 08:18:22,044: t15.2023.12.03 val PER: 0.1061
2026-01-10 08:18:22,044: t15.2023.12.08 val PER: 0.0985
2026-01-10 08:18:22,044: t15.2023.12.10 val PER: 0.0933
2026-01-10 08:18:22,044: t15.2023.12.17 val PER: 0.1175
2026-01-10 08:18:22,044: t15.2023.12.29 val PER: 0.1229
2026-01-10 08:18:22,044: t15.2024.02.25 val PER: 0.1067
2026-01-10 08:18:22,044: t15.2024.03.08 val PER: 0.1977
2026-01-10 08:18:22,045: t15.2024.03.15 val PER: 0.1995
2026-01-10 08:18:22,045: t15.2024.03.17 val PER: 0.1255
2026-01-10 08:18:22,045: t15.2024.05.10 val PER: 0.1620
2026-01-10 08:18:22,045: t15.2024.06.14 val PER: 0.1467
2026-01-10 08:18:22,045: t15.2024.07.19 val PER: 0.2320
2026-01-10 08:18:22,045: t15.2024.07.21 val PER: 0.0862
2026-01-10 08:18:22,045: t15.2024.07.28 val PER: 0.1360
2026-01-10 08:18:22,045: t15.2025.01.10 val PER: 0.2645
2026-01-10 08:18:22,045: t15.2025.01.12 val PER: 0.1378
2026-01-10 08:18:22,045: t15.2025.03.14 val PER: 0.3166
2026-01-10 08:18:22,045: t15.2025.03.16 val PER: 0.1741
2026-01-10 08:18:22,045: t15.2025.03.30 val PER: 0.2851
2026-01-10 08:18:22,045: t15.2025.04.13 val PER: 0.2154
2026-01-10 08:18:22,188: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_14000
2026-01-10 08:18:40,828: Train batch 14200: loss: 6.53 grad norm: 48.92 time: 0.056
2026-01-10 08:18:59,671: Train batch 14400: loss: 5.59 grad norm: 38.59 time: 0.064
2026-01-10 08:19:09,182: Running test after training batch: 14500
2026-01-10 08:19:09,335: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:19:14,367: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:19:14,427: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 08:19:27,184: Val batch 14500: PER (avg): 0.1441 CTC Loss (avg): 14.8056 WER(5gram): 32.79% (n=256) time: 18.001
2026-01-10 08:19:27,184: WER lens: avg_true_words=5.99 avg_pred_words=6.43 max_pred_words=14
2026-01-10 08:19:27,185: t15.2023.08.13 val PER: 0.1050
2026-01-10 08:19:27,185: t15.2023.08.18 val PER: 0.0872
2026-01-10 08:19:27,185: t15.2023.08.20 val PER: 0.0969
2026-01-10 08:19:27,185: t15.2023.08.25 val PER: 0.0964
2026-01-10 08:19:27,185: t15.2023.08.27 val PER: 0.1736
2026-01-10 08:19:27,185: t15.2023.09.01 val PER: 0.0714
2026-01-10 08:19:27,185: t15.2023.09.03 val PER: 0.1520
2026-01-10 08:19:27,185: t15.2023.09.24 val PER: 0.1250
2026-01-10 08:19:27,185: t15.2023.09.29 val PER: 0.1187
2026-01-10 08:19:27,185: t15.2023.10.01 val PER: 0.1691
2026-01-10 08:19:27,186: t15.2023.10.06 val PER: 0.0829
2026-01-10 08:19:27,186: t15.2023.10.08 val PER: 0.2300
2026-01-10 08:19:27,186: t15.2023.10.13 val PER: 0.1932
2026-01-10 08:19:27,186: t15.2023.10.15 val PER: 0.1536
2026-01-10 08:19:27,186: t15.2023.10.20 val PER: 0.1812
2026-01-10 08:19:27,186: t15.2023.10.22 val PER: 0.1225
2026-01-10 08:19:27,186: t15.2023.11.03 val PER: 0.1750
2026-01-10 08:19:27,186: t15.2023.11.04 val PER: 0.0239
2026-01-10 08:19:27,186: t15.2023.11.17 val PER: 0.0373
2026-01-10 08:19:27,186: t15.2023.11.19 val PER: 0.0259
2026-01-10 08:19:27,187: t15.2023.11.26 val PER: 0.1087
2026-01-10 08:19:27,187: t15.2023.12.03 val PER: 0.0956
2026-01-10 08:19:27,187: t15.2023.12.08 val PER: 0.0965
2026-01-10 08:19:27,187: t15.2023.12.10 val PER: 0.0841
2026-01-10 08:19:27,187: t15.2023.12.17 val PER: 0.1154
2026-01-10 08:19:27,187: t15.2023.12.29 val PER: 0.1208
2026-01-10 08:19:27,187: t15.2024.02.25 val PER: 0.1194
2026-01-10 08:19:27,187: t15.2024.03.08 val PER: 0.2162
2026-01-10 08:19:27,187: t15.2024.03.15 val PER: 0.2058
2026-01-10 08:19:27,187: t15.2024.03.17 val PER: 0.1290
2026-01-10 08:19:27,187: t15.2024.05.10 val PER: 0.1605
2026-01-10 08:19:27,187: t15.2024.06.14 val PER: 0.1498
2026-01-10 08:19:27,187: t15.2024.07.19 val PER: 0.2340
2026-01-10 08:19:27,187: t15.2024.07.21 val PER: 0.0931
2026-01-10 08:19:27,187: t15.2024.07.28 val PER: 0.1301
2026-01-10 08:19:27,187: t15.2025.01.10 val PER: 0.2672
2026-01-10 08:19:27,188: t15.2025.01.12 val PER: 0.1432
2026-01-10 08:19:27,188: t15.2025.03.14 val PER: 0.3240
2026-01-10 08:19:27,188: t15.2025.03.16 val PER: 0.1846
2026-01-10 08:19:27,188: t15.2025.03.30 val PER: 0.2931
2026-01-10 08:19:27,188: t15.2025.04.13 val PER: 0.2254
2026-01-10 08:19:27,327: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_14500
2026-01-10 08:19:37,504: Train batch 14600: loss: 10.71 grad norm: 55.21 time: 0.059
2026-01-10 08:19:56,446: Train batch 14800: loss: 4.97 grad norm: 40.11 time: 0.051
2026-01-10 08:20:15,212: Train batch 15000: loss: 7.73 grad norm: 46.28 time: 0.052
2026-01-10 08:20:15,213: Running test after training batch: 15000
2026-01-10 08:20:15,352: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:20:20,531: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:20:20,586: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 08:20:33,760: Val batch 15000: PER (avg): 0.1427 CTC Loss (avg): 14.7247 WER(5gram): 31.29% (n=256) time: 18.547
2026-01-10 08:20:33,760: WER lens: avg_true_words=5.99 avg_pred_words=6.41 max_pred_words=14
2026-01-10 08:20:33,760: t15.2023.08.13 val PER: 0.1040
2026-01-10 08:20:33,761: t15.2023.08.18 val PER: 0.0956
2026-01-10 08:20:33,761: t15.2023.08.20 val PER: 0.0977
2026-01-10 08:20:33,761: t15.2023.08.25 val PER: 0.0934
2026-01-10 08:20:33,761: t15.2023.08.27 val PER: 0.1752
2026-01-10 08:20:33,761: t15.2023.09.01 val PER: 0.0747
2026-01-10 08:20:33,761: t15.2023.09.03 val PER: 0.1627
2026-01-10 08:20:33,761: t15.2023.09.24 val PER: 0.1214
2026-01-10 08:20:33,761: t15.2023.09.29 val PER: 0.1187
2026-01-10 08:20:33,762: t15.2023.10.01 val PER: 0.1618
2026-01-10 08:20:33,762: t15.2023.10.06 val PER: 0.0840
2026-01-10 08:20:33,762: t15.2023.10.08 val PER: 0.2314
2026-01-10 08:20:33,762: t15.2023.10.13 val PER: 0.1831
2026-01-10 08:20:33,762: t15.2023.10.15 val PER: 0.1470
2026-01-10 08:20:33,762: t15.2023.10.20 val PER: 0.1879
2026-01-10 08:20:33,762: t15.2023.10.22 val PER: 0.1114
2026-01-10 08:20:33,762: t15.2023.11.03 val PER: 0.1716
2026-01-10 08:20:33,762: t15.2023.11.04 val PER: 0.0273
2026-01-10 08:20:33,762: t15.2023.11.17 val PER: 0.0295
2026-01-10 08:20:33,762: t15.2023.11.19 val PER: 0.0240
2026-01-10 08:20:33,763: t15.2023.11.26 val PER: 0.1159
2026-01-10 08:20:33,763: t15.2023.12.03 val PER: 0.1019
2026-01-10 08:20:33,763: t15.2023.12.08 val PER: 0.0932
2026-01-10 08:20:33,763: t15.2023.12.10 val PER: 0.0880
2026-01-10 08:20:33,763: t15.2023.12.17 val PER: 0.1154
2026-01-10 08:20:33,763: t15.2023.12.29 val PER: 0.1229
2026-01-10 08:20:33,763: t15.2024.02.25 val PER: 0.1152
2026-01-10 08:20:33,763: t15.2024.03.08 val PER: 0.2091
2026-01-10 08:20:33,763: t15.2024.03.15 val PER: 0.2058
2026-01-10 08:20:33,763: t15.2024.03.17 val PER: 0.1255
2026-01-10 08:20:33,764: t15.2024.05.10 val PER: 0.1664
2026-01-10 08:20:33,764: t15.2024.06.14 val PER: 0.1483
2026-01-10 08:20:33,764: t15.2024.07.19 val PER: 0.2294
2026-01-10 08:20:33,764: t15.2024.07.21 val PER: 0.0883
2026-01-10 08:20:33,764: t15.2024.07.28 val PER: 0.1287
2026-01-10 08:20:33,764: t15.2025.01.10 val PER: 0.2672
2026-01-10 08:20:33,764: t15.2025.01.12 val PER: 0.1455
2026-01-10 08:20:33,764: t15.2025.03.14 val PER: 0.3358
2026-01-10 08:20:33,764: t15.2025.03.16 val PER: 0.1688
2026-01-10 08:20:33,764: t15.2025.03.30 val PER: 0.2805
2026-01-10 08:20:33,764: t15.2025.04.13 val PER: 0.2126
2026-01-10 08:20:33,905: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_15000
2026-01-10 08:20:52,971: Train batch 15200: loss: 3.78 grad norm: 31.57 time: 0.058
2026-01-10 08:21:11,645: Train batch 15400: loss: 9.77 grad norm: 54.87 time: 0.049
2026-01-10 08:21:21,126: Running test after training batch: 15500
2026-01-10 08:21:21,305: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:21:26,439: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:21:26,492: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 08:21:39,551: Val batch 15500: PER (avg): 0.1409 CTC Loss (avg): 14.6600 WER(5gram): 32.01% (n=256) time: 18.424
2026-01-10 08:21:39,551: WER lens: avg_true_words=5.99 avg_pred_words=6.39 max_pred_words=14
2026-01-10 08:21:39,551: t15.2023.08.13 val PER: 0.1019
2026-01-10 08:21:39,551: t15.2023.08.18 val PER: 0.0897
2026-01-10 08:21:39,551: t15.2023.08.20 val PER: 0.0961
2026-01-10 08:21:39,552: t15.2023.08.25 val PER: 0.1039
2026-01-10 08:21:39,552: t15.2023.08.27 val PER: 0.1801
2026-01-10 08:21:39,552: t15.2023.09.01 val PER: 0.0698
2026-01-10 08:21:39,552: t15.2023.09.03 val PER: 0.1556
2026-01-10 08:21:39,552: t15.2023.09.24 val PER: 0.1214
2026-01-10 08:21:39,552: t15.2023.09.29 val PER: 0.1225
2026-01-10 08:21:39,552: t15.2023.10.01 val PER: 0.1638
2026-01-10 08:21:39,552: t15.2023.10.06 val PER: 0.0872
2026-01-10 08:21:39,552: t15.2023.10.08 val PER: 0.2314
2026-01-10 08:21:39,552: t15.2023.10.13 val PER: 0.1932
2026-01-10 08:21:39,552: t15.2023.10.15 val PER: 0.1483
2026-01-10 08:21:39,553: t15.2023.10.20 val PER: 0.1913
2026-01-10 08:21:39,553: t15.2023.10.22 val PER: 0.1058
2026-01-10 08:21:39,553: t15.2023.11.03 val PER: 0.1710
2026-01-10 08:21:39,553: t15.2023.11.04 val PER: 0.0273
2026-01-10 08:21:39,553: t15.2023.11.17 val PER: 0.0327
2026-01-10 08:21:39,553: t15.2023.11.19 val PER: 0.0379
2026-01-10 08:21:39,553: t15.2023.11.26 val PER: 0.1101
2026-01-10 08:21:39,553: t15.2023.12.03 val PER: 0.0935
2026-01-10 08:21:39,553: t15.2023.12.08 val PER: 0.0866
2026-01-10 08:21:39,553: t15.2023.12.10 val PER: 0.0815
2026-01-10 08:21:39,554: t15.2023.12.17 val PER: 0.1154
2026-01-10 08:21:39,554: t15.2023.12.29 val PER: 0.1181
2026-01-10 08:21:39,554: t15.2024.02.25 val PER: 0.1124
2026-01-10 08:21:39,554: t15.2024.03.08 val PER: 0.2006
2026-01-10 08:21:39,554: t15.2024.03.15 val PER: 0.2001
2026-01-10 08:21:39,554: t15.2024.03.17 val PER: 0.1276
2026-01-10 08:21:39,554: t15.2024.05.10 val PER: 0.1545
2026-01-10 08:21:39,554: t15.2024.06.14 val PER: 0.1546
2026-01-10 08:21:39,554: t15.2024.07.19 val PER: 0.2294
2026-01-10 08:21:39,554: t15.2024.07.21 val PER: 0.0848
2026-01-10 08:21:39,555: t15.2024.07.28 val PER: 0.1235
2026-01-10 08:21:39,555: t15.2025.01.10 val PER: 0.2658
2026-01-10 08:21:39,555: t15.2025.01.12 val PER: 0.1339
2026-01-10 08:21:39,555: t15.2025.03.14 val PER: 0.3195
2026-01-10 08:21:39,555: t15.2025.03.16 val PER: 0.1702
2026-01-10 08:21:39,555: t15.2025.03.30 val PER: 0.2770
2026-01-10 08:21:39,555: t15.2025.04.13 val PER: 0.2197
2026-01-10 08:21:39,700: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_15500
2026-01-10 08:21:49,144: Train batch 15600: loss: 9.84 grad norm: 53.73 time: 0.062
2026-01-10 08:22:07,959: Train batch 15800: loss: 12.22 grad norm: 61.13 time: 0.066
2026-01-10 08:22:27,118: Train batch 16000: loss: 6.95 grad norm: 49.32 time: 0.056
2026-01-10 08:22:27,118: Running test after training batch: 16000
2026-01-10 08:22:27,285: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:22:32,344: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:22:32,402: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 08:22:46,329: Val batch 16000: PER (avg): 0.1412 CTC Loss (avg): 14.6429 WER(5gram): 30.44% (n=256) time: 19.211
2026-01-10 08:22:46,330: WER lens: avg_true_words=5.99 avg_pred_words=6.39 max_pred_words=14
2026-01-10 08:22:46,330: t15.2023.08.13 val PER: 0.1112
2026-01-10 08:22:46,330: t15.2023.08.18 val PER: 0.0897
2026-01-10 08:22:46,330: t15.2023.08.20 val PER: 0.0953
2026-01-10 08:22:46,330: t15.2023.08.25 val PER: 0.1069
2026-01-10 08:22:46,331: t15.2023.08.27 val PER: 0.1897
2026-01-10 08:22:46,331: t15.2023.09.01 val PER: 0.0698
2026-01-10 08:22:46,331: t15.2023.09.03 val PER: 0.1473
2026-01-10 08:22:46,331: t15.2023.09.24 val PER: 0.1250
2026-01-10 08:22:46,331: t15.2023.09.29 val PER: 0.1193
2026-01-10 08:22:46,331: t15.2023.10.01 val PER: 0.1658
2026-01-10 08:22:46,331: t15.2023.10.06 val PER: 0.0829
2026-01-10 08:22:46,331: t15.2023.10.08 val PER: 0.2287
2026-01-10 08:22:46,331: t15.2023.10.13 val PER: 0.1854
2026-01-10 08:22:46,332: t15.2023.10.15 val PER: 0.1483
2026-01-10 08:22:46,332: t15.2023.10.20 val PER: 0.1913
2026-01-10 08:22:46,332: t15.2023.10.22 val PER: 0.1047
2026-01-10 08:22:46,332: t15.2023.11.03 val PER: 0.1723
2026-01-10 08:22:46,332: t15.2023.11.04 val PER: 0.0307
2026-01-10 08:22:46,332: t15.2023.11.17 val PER: 0.0264
2026-01-10 08:22:46,332: t15.2023.11.19 val PER: 0.0299
2026-01-10 08:22:46,332: t15.2023.11.26 val PER: 0.1058
2026-01-10 08:22:46,332: t15.2023.12.03 val PER: 0.0966
2026-01-10 08:22:46,332: t15.2023.12.08 val PER: 0.0852
2026-01-10 08:22:46,333: t15.2023.12.10 val PER: 0.0841
2026-01-10 08:22:46,333: t15.2023.12.17 val PER: 0.1216
2026-01-10 08:22:46,333: t15.2023.12.29 val PER: 0.1187
2026-01-10 08:22:46,333: t15.2024.02.25 val PER: 0.1152
2026-01-10 08:22:46,333: t15.2024.03.08 val PER: 0.2063
2026-01-10 08:22:46,333: t15.2024.03.15 val PER: 0.2014
2026-01-10 08:22:46,333: t15.2024.03.17 val PER: 0.1213
2026-01-10 08:22:46,333: t15.2024.05.10 val PER: 0.1620
2026-01-10 08:22:46,333: t15.2024.06.14 val PER: 0.1514
2026-01-10 08:22:46,333: t15.2024.07.19 val PER: 0.2334
2026-01-10 08:22:46,334: t15.2024.07.21 val PER: 0.0841
2026-01-10 08:22:46,334: t15.2024.07.28 val PER: 0.1265
2026-01-10 08:22:46,334: t15.2025.01.10 val PER: 0.2727
2026-01-10 08:22:46,334: t15.2025.01.12 val PER: 0.1347
2026-01-10 08:22:46,334: t15.2025.03.14 val PER: 0.3166
2026-01-10 08:22:46,334: t15.2025.03.16 val PER: 0.1728
2026-01-10 08:22:46,334: t15.2025.03.30 val PER: 0.2920
2026-01-10 08:22:46,334: t15.2025.04.13 val PER: 0.2083
2026-01-10 08:22:46,475: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_16000
2026-01-10 08:23:05,332: Train batch 16200: loss: 5.33 grad norm: 42.40 time: 0.056
2026-01-10 08:23:24,183: Train batch 16400: loss: 9.32 grad norm: 55.72 time: 0.057
2026-01-10 08:23:33,710: Running test after training batch: 16500
2026-01-10 08:23:33,844: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:23:39,110: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:23:39,172: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 08:23:52,417: Val batch 16500: PER (avg): 0.1412 CTC Loss (avg): 14.5247 WER(5gram): 29.99% (n=256) time: 18.707
2026-01-10 08:23:52,418: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-10 08:23:52,418: t15.2023.08.13 val PER: 0.1112
2026-01-10 08:23:52,418: t15.2023.08.18 val PER: 0.0905
2026-01-10 08:23:52,418: t15.2023.08.20 val PER: 0.0945
2026-01-10 08:23:52,419: t15.2023.08.25 val PER: 0.1084
2026-01-10 08:23:52,419: t15.2023.08.27 val PER: 0.1881
2026-01-10 08:23:52,419: t15.2023.09.01 val PER: 0.0682
2026-01-10 08:23:52,419: t15.2023.09.03 val PER: 0.1520
2026-01-10 08:23:52,419: t15.2023.09.24 val PER: 0.1226
2026-01-10 08:23:52,419: t15.2023.09.29 val PER: 0.1181
2026-01-10 08:23:52,419: t15.2023.10.01 val PER: 0.1704
2026-01-10 08:23:52,419: t15.2023.10.06 val PER: 0.0829
2026-01-10 08:23:52,420: t15.2023.10.08 val PER: 0.2314
2026-01-10 08:23:52,420: t15.2023.10.13 val PER: 0.1854
2026-01-10 08:23:52,420: t15.2023.10.15 val PER: 0.1503
2026-01-10 08:23:52,420: t15.2023.10.20 val PER: 0.1913
2026-01-10 08:23:52,420: t15.2023.10.22 val PER: 0.1136
2026-01-10 08:23:52,420: t15.2023.11.03 val PER: 0.1676
2026-01-10 08:23:52,420: t15.2023.11.04 val PER: 0.0273
2026-01-10 08:23:52,420: t15.2023.11.17 val PER: 0.0311
2026-01-10 08:23:52,420: t15.2023.11.19 val PER: 0.0299
2026-01-10 08:23:52,420: t15.2023.11.26 val PER: 0.1022
2026-01-10 08:23:52,421: t15.2023.12.03 val PER: 0.1008
2026-01-10 08:23:52,421: t15.2023.12.08 val PER: 0.0819
2026-01-10 08:23:52,421: t15.2023.12.10 val PER: 0.0788
2026-01-10 08:23:52,421: t15.2023.12.17 val PER: 0.1175
2026-01-10 08:23:52,421: t15.2023.12.29 val PER: 0.1201
2026-01-10 08:23:52,421: t15.2024.02.25 val PER: 0.1081
2026-01-10 08:23:52,421: t15.2024.03.08 val PER: 0.2105
2026-01-10 08:23:52,421: t15.2024.03.15 val PER: 0.2020
2026-01-10 08:23:52,421: t15.2024.03.17 val PER: 0.1269
2026-01-10 08:23:52,422: t15.2024.05.10 val PER: 0.1501
2026-01-10 08:23:52,422: t15.2024.06.14 val PER: 0.1546
2026-01-10 08:23:52,422: t15.2024.07.19 val PER: 0.2386
2026-01-10 08:23:52,422: t15.2024.07.21 val PER: 0.0793
2026-01-10 08:23:52,422: t15.2024.07.28 val PER: 0.1257
2026-01-10 08:23:52,422: t15.2025.01.10 val PER: 0.2727
2026-01-10 08:23:52,422: t15.2025.01.12 val PER: 0.1286
2026-01-10 08:23:52,422: t15.2025.03.14 val PER: 0.3240
2026-01-10 08:23:52,422: t15.2025.03.16 val PER: 0.1728
2026-01-10 08:23:52,422: t15.2025.03.30 val PER: 0.2851
2026-01-10 08:23:52,423: t15.2025.04.13 val PER: 0.2154
2026-01-10 08:23:52,559: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_16500
2026-01-10 08:24:02,018: Train batch 16600: loss: 7.22 grad norm: 51.61 time: 0.052
2026-01-10 08:24:21,226: Train batch 16800: loss: 15.12 grad norm: 71.06 time: 0.061
2026-01-10 08:24:40,040: Train batch 17000: loss: 6.83 grad norm: 46.44 time: 0.082
2026-01-10 08:24:40,040: Running test after training batch: 17000
2026-01-10 08:24:40,149: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:24:45,197: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:24:45,259: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 08:24:59,221: Val batch 17000: PER (avg): 0.1403 CTC Loss (avg): 14.5030 WER(5gram): 29.60% (n=256) time: 19.181
2026-01-10 08:24:59,222: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=13
2026-01-10 08:24:59,222: t15.2023.08.13 val PER: 0.1060
2026-01-10 08:24:59,222: t15.2023.08.18 val PER: 0.0889
2026-01-10 08:24:59,222: t15.2023.08.20 val PER: 0.0977
2026-01-10 08:24:59,222: t15.2023.08.25 val PER: 0.1054
2026-01-10 08:24:59,222: t15.2023.08.27 val PER: 0.1849
2026-01-10 08:24:59,222: t15.2023.09.01 val PER: 0.0714
2026-01-10 08:24:59,222: t15.2023.09.03 val PER: 0.1532
2026-01-10 08:24:59,222: t15.2023.09.24 val PER: 0.1286
2026-01-10 08:24:59,223: t15.2023.09.29 val PER: 0.1244
2026-01-10 08:24:59,223: t15.2023.10.01 val PER: 0.1671
2026-01-10 08:24:59,223: t15.2023.10.06 val PER: 0.0807
2026-01-10 08:24:59,223: t15.2023.10.08 val PER: 0.2327
2026-01-10 08:24:59,223: t15.2023.10.13 val PER: 0.1877
2026-01-10 08:24:59,223: t15.2023.10.15 val PER: 0.1470
2026-01-10 08:24:59,223: t15.2023.10.20 val PER: 0.1913
2026-01-10 08:24:59,223: t15.2023.10.22 val PER: 0.1091
2026-01-10 08:24:59,223: t15.2023.11.03 val PER: 0.1716
2026-01-10 08:24:59,223: t15.2023.11.04 val PER: 0.0205
2026-01-10 08:24:59,223: t15.2023.11.17 val PER: 0.0280
2026-01-10 08:24:59,223: t15.2023.11.19 val PER: 0.0279
2026-01-10 08:24:59,223: t15.2023.11.26 val PER: 0.0993
2026-01-10 08:24:59,223: t15.2023.12.03 val PER: 0.0966
2026-01-10 08:24:59,223: t15.2023.12.08 val PER: 0.0839
2026-01-10 08:24:59,223: t15.2023.12.10 val PER: 0.0775
2026-01-10 08:24:59,224: t15.2023.12.17 val PER: 0.1133
2026-01-10 08:24:59,224: t15.2023.12.29 val PER: 0.1208
2026-01-10 08:24:59,224: t15.2024.02.25 val PER: 0.1124
2026-01-10 08:24:59,224: t15.2024.03.08 val PER: 0.2077
2026-01-10 08:24:59,224: t15.2024.03.15 val PER: 0.2058
2026-01-10 08:24:59,224: t15.2024.03.17 val PER: 0.1220
2026-01-10 08:24:59,224: t15.2024.05.10 val PER: 0.1560
2026-01-10 08:24:59,224: t15.2024.06.14 val PER: 0.1498
2026-01-10 08:24:59,224: t15.2024.07.19 val PER: 0.2320
2026-01-10 08:24:59,225: t15.2024.07.21 val PER: 0.0814
2026-01-10 08:24:59,225: t15.2024.07.28 val PER: 0.1213
2026-01-10 08:24:59,225: t15.2025.01.10 val PER: 0.2603
2026-01-10 08:24:59,225: t15.2025.01.12 val PER: 0.1347
2026-01-10 08:24:59,225: t15.2025.03.14 val PER: 0.3195
2026-01-10 08:24:59,225: t15.2025.03.16 val PER: 0.1662
2026-01-10 08:24:59,225: t15.2025.03.30 val PER: 0.2667
2026-01-10 08:24:59,225: t15.2025.04.13 val PER: 0.2268
2026-01-10 08:24:59,369: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_17000
2026-01-10 08:25:18,086: Train batch 17200: loss: 8.72 grad norm: 53.26 time: 0.085
2026-01-10 08:25:37,201: Train batch 17400: loss: 10.02 grad norm: 56.70 time: 0.073
2026-01-10 08:25:46,610: Running test after training batch: 17500
2026-01-10 08:25:46,760: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:25:51,799: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:25:51,862: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 08:26:05,905: Val batch 17500: PER (avg): 0.1398 CTC Loss (avg): 14.5520 WER(5gram): 30.96% (n=256) time: 19.295
2026-01-10 08:26:05,906: WER lens: avg_true_words=5.99 avg_pred_words=6.45 max_pred_words=14
2026-01-10 08:26:05,906: t15.2023.08.13 val PER: 0.1071
2026-01-10 08:26:05,906: t15.2023.08.18 val PER: 0.0889
2026-01-10 08:26:05,906: t15.2023.08.20 val PER: 0.0953
2026-01-10 08:26:05,906: t15.2023.08.25 val PER: 0.1054
2026-01-10 08:26:05,906: t15.2023.08.27 val PER: 0.1833
2026-01-10 08:26:05,907: t15.2023.09.01 val PER: 0.0714
2026-01-10 08:26:05,907: t15.2023.09.03 val PER: 0.1532
2026-01-10 08:26:05,907: t15.2023.09.24 val PER: 0.1201
2026-01-10 08:26:05,907: t15.2023.09.29 val PER: 0.1213
2026-01-10 08:26:05,907: t15.2023.10.01 val PER: 0.1724
2026-01-10 08:26:05,907: t15.2023.10.06 val PER: 0.0797
2026-01-10 08:26:05,907: t15.2023.10.08 val PER: 0.2341
2026-01-10 08:26:05,907: t15.2023.10.13 val PER: 0.1831
2026-01-10 08:26:05,907: t15.2023.10.15 val PER: 0.1490
2026-01-10 08:26:05,907: t15.2023.10.20 val PER: 0.1812
2026-01-10 08:26:05,907: t15.2023.10.22 val PER: 0.1024
2026-01-10 08:26:05,907: t15.2023.11.03 val PER: 0.1730
2026-01-10 08:26:05,907: t15.2023.11.04 val PER: 0.0239
2026-01-10 08:26:05,908: t15.2023.11.17 val PER: 0.0311
2026-01-10 08:26:05,908: t15.2023.11.19 val PER: 0.0319
2026-01-10 08:26:05,908: t15.2023.11.26 val PER: 0.1007
2026-01-10 08:26:05,908: t15.2023.12.03 val PER: 0.1019
2026-01-10 08:26:05,908: t15.2023.12.08 val PER: 0.0826
2026-01-10 08:26:05,908: t15.2023.12.10 val PER: 0.0775
2026-01-10 08:26:05,908: t15.2023.12.17 val PER: 0.1175
2026-01-10 08:26:05,908: t15.2023.12.29 val PER: 0.1235
2026-01-10 08:26:05,908: t15.2024.02.25 val PER: 0.1067
2026-01-10 08:26:05,909: t15.2024.03.08 val PER: 0.2020
2026-01-10 08:26:05,909: t15.2024.03.15 val PER: 0.2008
2026-01-10 08:26:05,909: t15.2024.03.17 val PER: 0.1248
2026-01-10 08:26:05,909: t15.2024.05.10 val PER: 0.1590
2026-01-10 08:26:05,909: t15.2024.06.14 val PER: 0.1546
2026-01-10 08:26:05,909: t15.2024.07.19 val PER: 0.2221
2026-01-10 08:26:05,909: t15.2024.07.21 val PER: 0.0800
2026-01-10 08:26:05,909: t15.2024.07.28 val PER: 0.1191
2026-01-10 08:26:05,909: t15.2025.01.10 val PER: 0.2631
2026-01-10 08:26:05,909: t15.2025.01.12 val PER: 0.1332
2026-01-10 08:26:05,909: t15.2025.03.14 val PER: 0.3240
2026-01-10 08:26:05,909: t15.2025.03.16 val PER: 0.1754
2026-01-10 08:26:05,910: t15.2025.03.30 val PER: 0.2678
2026-01-10 08:26:05,910: t15.2025.04.13 val PER: 0.2183
2026-01-10 08:26:06,057: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_17500
2026-01-10 08:26:15,420: Train batch 17600: loss: 7.94 grad norm: 50.85 time: 0.051
2026-01-10 08:26:34,453: Train batch 17800: loss: 5.86 grad norm: 46.48 time: 0.042
2026-01-10 08:26:53,253: Train batch 18000: loss: 9.30 grad norm: 55.32 time: 0.061
2026-01-10 08:26:53,253: Running test after training batch: 18000
2026-01-10 08:26:53,368: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:26:58,470: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:26:58,526: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 08:27:11,729: Val batch 18000: PER (avg): 0.1377 CTC Loss (avg): 14.4668 WER(5gram): 28.88% (n=256) time: 18.476
2026-01-10 08:27:11,730: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=14
2026-01-10 08:27:11,730: t15.2023.08.13 val PER: 0.0998
2026-01-10 08:27:11,731: t15.2023.08.18 val PER: 0.0880
2026-01-10 08:27:11,731: t15.2023.08.20 val PER: 0.0898
2026-01-10 08:27:11,731: t15.2023.08.25 val PER: 0.1084
2026-01-10 08:27:11,731: t15.2023.08.27 val PER: 0.1817
2026-01-10 08:27:11,731: t15.2023.09.01 val PER: 0.0682
2026-01-10 08:27:11,731: t15.2023.09.03 val PER: 0.1461
2026-01-10 08:27:11,731: t15.2023.09.24 val PER: 0.1141
2026-01-10 08:27:11,731: t15.2023.09.29 val PER: 0.1206
2026-01-10 08:27:11,731: t15.2023.10.01 val PER: 0.1664
2026-01-10 08:27:11,732: t15.2023.10.06 val PER: 0.0797
2026-01-10 08:27:11,732: t15.2023.10.08 val PER: 0.2355
2026-01-10 08:27:11,732: t15.2023.10.13 val PER: 0.1823
2026-01-10 08:27:11,732: t15.2023.10.15 val PER: 0.1483
2026-01-10 08:27:11,732: t15.2023.10.20 val PER: 0.1812
2026-01-10 08:27:11,732: t15.2023.10.22 val PER: 0.1024
2026-01-10 08:27:11,732: t15.2023.11.03 val PER: 0.1649
2026-01-10 08:27:11,732: t15.2023.11.04 val PER: 0.0273
2026-01-10 08:27:11,732: t15.2023.11.17 val PER: 0.0311
2026-01-10 08:27:11,733: t15.2023.11.19 val PER: 0.0299
2026-01-10 08:27:11,733: t15.2023.11.26 val PER: 0.0957
2026-01-10 08:27:11,733: t15.2023.12.03 val PER: 0.0956
2026-01-10 08:27:11,733: t15.2023.12.08 val PER: 0.0826
2026-01-10 08:27:11,733: t15.2023.12.10 val PER: 0.0788
2026-01-10 08:27:11,733: t15.2023.12.17 val PER: 0.1133
2026-01-10 08:27:11,733: t15.2023.12.29 val PER: 0.1194
2026-01-10 08:27:11,733: t15.2024.02.25 val PER: 0.1053
2026-01-10 08:27:11,733: t15.2024.03.08 val PER: 0.2020
2026-01-10 08:27:11,734: t15.2024.03.15 val PER: 0.2020
2026-01-10 08:27:11,734: t15.2024.03.17 val PER: 0.1220
2026-01-10 08:27:11,734: t15.2024.05.10 val PER: 0.1545
2026-01-10 08:27:11,734: t15.2024.06.14 val PER: 0.1609
2026-01-10 08:27:11,734: t15.2024.07.19 val PER: 0.2268
2026-01-10 08:27:11,734: t15.2024.07.21 val PER: 0.0814
2026-01-10 08:27:11,734: t15.2024.07.28 val PER: 0.1184
2026-01-10 08:27:11,734: t15.2025.01.10 val PER: 0.2590
2026-01-10 08:27:11,734: t15.2025.01.12 val PER: 0.1309
2026-01-10 08:27:11,735: t15.2025.03.14 val PER: 0.3254
2026-01-10 08:27:11,735: t15.2025.03.16 val PER: 0.1741
2026-01-10 08:27:11,735: t15.2025.03.30 val PER: 0.2575
2026-01-10 08:27:11,735: t15.2025.04.13 val PER: 0.2126
2026-01-10 08:27:11,873: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_18000
2026-01-10 08:27:31,063: Train batch 18200: loss: 7.44 grad norm: 49.54 time: 0.075
2026-01-10 08:27:50,116: Train batch 18400: loss: 3.77 grad norm: 37.80 time: 0.058
2026-01-10 08:27:59,690: Running test after training batch: 18500
2026-01-10 08:27:59,845: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:28:04,828: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:28:04,897: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 08:28:18,823: Val batch 18500: PER (avg): 0.1390 CTC Loss (avg): 14.4482 WER(5gram): 29.34% (n=256) time: 19.133
2026-01-10 08:28:18,824: WER lens: avg_true_words=5.99 avg_pred_words=6.41 max_pred_words=13
2026-01-10 08:28:18,824: t15.2023.08.13 val PER: 0.1029
2026-01-10 08:28:18,824: t15.2023.08.18 val PER: 0.0889
2026-01-10 08:28:18,824: t15.2023.08.20 val PER: 0.0945
2026-01-10 08:28:18,825: t15.2023.08.25 val PER: 0.1084
2026-01-10 08:28:18,825: t15.2023.08.27 val PER: 0.1913
2026-01-10 08:28:18,825: t15.2023.09.01 val PER: 0.0682
2026-01-10 08:28:18,825: t15.2023.09.03 val PER: 0.1449
2026-01-10 08:28:18,825: t15.2023.09.24 val PER: 0.1165
2026-01-10 08:28:18,825: t15.2023.09.29 val PER: 0.1213
2026-01-10 08:28:18,825: t15.2023.10.01 val PER: 0.1664
2026-01-10 08:28:18,825: t15.2023.10.06 val PER: 0.0775
2026-01-10 08:28:18,825: t15.2023.10.08 val PER: 0.2368
2026-01-10 08:28:18,825: t15.2023.10.13 val PER: 0.1815
2026-01-10 08:28:18,826: t15.2023.10.15 val PER: 0.1483
2026-01-10 08:28:18,826: t15.2023.10.20 val PER: 0.1678
2026-01-10 08:28:18,826: t15.2023.10.22 val PER: 0.1013
2026-01-10 08:28:18,826: t15.2023.11.03 val PER: 0.1716
2026-01-10 08:28:18,826: t15.2023.11.04 val PER: 0.0273
2026-01-10 08:28:18,826: t15.2023.11.17 val PER: 0.0342
2026-01-10 08:28:18,826: t15.2023.11.19 val PER: 0.0359
2026-01-10 08:28:18,826: t15.2023.11.26 val PER: 0.0971
2026-01-10 08:28:18,826: t15.2023.12.03 val PER: 0.0935
2026-01-10 08:28:18,827: t15.2023.12.08 val PER: 0.0826
2026-01-10 08:28:18,827: t15.2023.12.10 val PER: 0.0762
2026-01-10 08:28:18,827: t15.2023.12.17 val PER: 0.1143
2026-01-10 08:28:18,827: t15.2023.12.29 val PER: 0.1194
2026-01-10 08:28:18,827: t15.2024.02.25 val PER: 0.1110
2026-01-10 08:28:18,827: t15.2024.03.08 val PER: 0.2006
2026-01-10 08:28:18,827: t15.2024.03.15 val PER: 0.1982
2026-01-10 08:28:18,827: t15.2024.03.17 val PER: 0.1199
2026-01-10 08:28:18,827: t15.2024.05.10 val PER: 0.1560
2026-01-10 08:28:18,828: t15.2024.06.14 val PER: 0.1577
2026-01-10 08:28:18,828: t15.2024.07.19 val PER: 0.2281
2026-01-10 08:28:18,828: t15.2024.07.21 val PER: 0.0821
2026-01-10 08:28:18,828: t15.2024.07.28 val PER: 0.1272
2026-01-10 08:28:18,828: t15.2025.01.10 val PER: 0.2700
2026-01-10 08:28:18,828: t15.2025.01.12 val PER: 0.1363
2026-01-10 08:28:18,828: t15.2025.03.14 val PER: 0.3210
2026-01-10 08:28:18,828: t15.2025.03.16 val PER: 0.1728
2026-01-10 08:28:18,828: t15.2025.03.30 val PER: 0.2759
2026-01-10 08:28:18,829: t15.2025.04.13 val PER: 0.2054
2026-01-10 08:28:18,969: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_18500
2026-01-10 08:28:28,407: Train batch 18600: loss: 11.39 grad norm: 60.41 time: 0.067
2026-01-10 08:28:47,521: Train batch 18800: loss: 7.13 grad norm: 48.44 time: 0.066
2026-01-10 08:29:06,632: Train batch 19000: loss: 7.25 grad norm: 48.58 time: 0.064
2026-01-10 08:29:06,633: Running test after training batch: 19000
2026-01-10 08:29:06,906: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:29:12,019: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:29:12,088: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost
2026-01-10 08:29:26,201: Val batch 19000: PER (avg): 0.1379 CTC Loss (avg): 14.5115 WER(5gram): 29.01% (n=256) time: 19.568
2026-01-10 08:29:26,202: WER lens: avg_true_words=5.99 avg_pred_words=6.40 max_pred_words=13
2026-01-10 08:29:26,202: t15.2023.08.13 val PER: 0.1029
2026-01-10 08:29:26,202: t15.2023.08.18 val PER: 0.0930
2026-01-10 08:29:26,202: t15.2023.08.20 val PER: 0.0937
2026-01-10 08:29:26,202: t15.2023.08.25 val PER: 0.1084
2026-01-10 08:29:26,202: t15.2023.08.27 val PER: 0.1768
2026-01-10 08:29:26,202: t15.2023.09.01 val PER: 0.0698
2026-01-10 08:29:26,202: t15.2023.09.03 val PER: 0.1485
2026-01-10 08:29:26,203: t15.2023.09.24 val PER: 0.1214
2026-01-10 08:29:26,203: t15.2023.09.29 val PER: 0.1219
2026-01-10 08:29:26,203: t15.2023.10.01 val PER: 0.1638
2026-01-10 08:29:26,203: t15.2023.10.06 val PER: 0.0797
2026-01-10 08:29:26,203: t15.2023.10.08 val PER: 0.2341
2026-01-10 08:29:26,203: t15.2023.10.13 val PER: 0.1877
2026-01-10 08:29:26,203: t15.2023.10.15 val PER: 0.1444
2026-01-10 08:29:26,203: t15.2023.10.20 val PER: 0.1711
2026-01-10 08:29:26,203: t15.2023.10.22 val PER: 0.1080
2026-01-10 08:29:26,203: t15.2023.11.03 val PER: 0.1716
2026-01-10 08:29:26,203: t15.2023.11.04 val PER: 0.0239
2026-01-10 08:29:26,203: t15.2023.11.17 val PER: 0.0327
2026-01-10 08:29:26,204: t15.2023.11.19 val PER: 0.0299
2026-01-10 08:29:26,204: t15.2023.11.26 val PER: 0.0957
2026-01-10 08:29:26,204: t15.2023.12.03 val PER: 0.0903
2026-01-10 08:29:26,204: t15.2023.12.08 val PER: 0.0846
2026-01-10 08:29:26,204: t15.2023.12.10 val PER: 0.0802
2026-01-10 08:29:26,204: t15.2023.12.17 val PER: 0.1154
2026-01-10 08:29:26,204: t15.2023.12.29 val PER: 0.1181
2026-01-10 08:29:26,204: t15.2024.02.25 val PER: 0.1053
2026-01-10 08:29:26,204: t15.2024.03.08 val PER: 0.1977
2026-01-10 08:29:26,205: t15.2024.03.15 val PER: 0.1970
2026-01-10 08:29:26,205: t15.2024.03.17 val PER: 0.1220
2026-01-10 08:29:26,205: t15.2024.05.10 val PER: 0.1620
2026-01-10 08:29:26,205: t15.2024.06.14 val PER: 0.1514
2026-01-10 08:29:26,205: t15.2024.07.19 val PER: 0.2175
2026-01-10 08:29:26,205: t15.2024.07.21 val PER: 0.0786
2026-01-10 08:29:26,205: t15.2024.07.28 val PER: 0.1176
2026-01-10 08:29:26,205: t15.2025.01.10 val PER: 0.2658
2026-01-10 08:29:26,205: t15.2025.01.12 val PER: 0.1332
2026-01-10 08:29:26,205: t15.2025.03.14 val PER: 0.3195
2026-01-10 08:29:26,206: t15.2025.03.16 val PER: 0.1767
2026-01-10 08:29:26,206: t15.2025.03.30 val PER: 0.2632
2026-01-10 08:29:26,206: t15.2025.04.13 val PER: 0.2111
2026-01-10 08:29:26,347: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_19000
2026-01-10 08:29:45,970: Train batch 19200: loss: 5.30 grad norm: 46.49 time: 0.063
2026-01-10 08:30:05,095: Train batch 19400: loss: 4.28 grad norm: 38.90 time: 0.053
2026-01-10 08:30:14,447: Running test after training batch: 19500
2026-01-10 08:30:14,593: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:30:19,659: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:30:19,721: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-10 08:30:33,743: Val batch 19500: PER (avg): 0.1378 CTC Loss (avg): 14.4206 WER(5gram): 30.18% (n=256) time: 19.295
2026-01-10 08:30:33,743: WER lens: avg_true_words=5.99 avg_pred_words=6.45 max_pred_words=14
2026-01-10 08:30:33,743: t15.2023.08.13 val PER: 0.1091
2026-01-10 08:30:33,743: t15.2023.08.18 val PER: 0.0863
2026-01-10 08:30:33,744: t15.2023.08.20 val PER: 0.0858
2026-01-10 08:30:33,744: t15.2023.08.25 val PER: 0.1069
2026-01-10 08:30:33,744: t15.2023.08.27 val PER: 0.1768
2026-01-10 08:30:33,744: t15.2023.09.01 val PER: 0.0657
2026-01-10 08:30:33,744: t15.2023.09.03 val PER: 0.1473
2026-01-10 08:30:33,744: t15.2023.09.24 val PER: 0.1177
2026-01-10 08:30:33,744: t15.2023.09.29 val PER: 0.1213
2026-01-10 08:30:33,744: t15.2023.10.01 val PER: 0.1678
2026-01-10 08:30:33,744: t15.2023.10.06 val PER: 0.0775
2026-01-10 08:30:33,744: t15.2023.10.08 val PER: 0.2327
2026-01-10 08:30:33,744: t15.2023.10.13 val PER: 0.1831
2026-01-10 08:30:33,744: t15.2023.10.15 val PER: 0.1477
2026-01-10 08:30:33,744: t15.2023.10.20 val PER: 0.1745
2026-01-10 08:30:33,744: t15.2023.10.22 val PER: 0.0991
2026-01-10 08:30:33,744: t15.2023.11.03 val PER: 0.1676
2026-01-10 08:30:33,745: t15.2023.11.04 val PER: 0.0307
2026-01-10 08:30:33,745: t15.2023.11.17 val PER: 0.0280
2026-01-10 08:30:33,745: t15.2023.11.19 val PER: 0.0319
2026-01-10 08:30:33,745: t15.2023.11.26 val PER: 0.0986
2026-01-10 08:30:33,745: t15.2023.12.03 val PER: 0.0966
2026-01-10 08:30:33,745: t15.2023.12.08 val PER: 0.0826
2026-01-10 08:30:33,745: t15.2023.12.10 val PER: 0.0788
2026-01-10 08:30:33,745: t15.2023.12.17 val PER: 0.1143
2026-01-10 08:30:33,745: t15.2023.12.29 val PER: 0.1208
2026-01-10 08:30:33,746: t15.2024.02.25 val PER: 0.1138
2026-01-10 08:30:33,746: t15.2024.03.08 val PER: 0.1977
2026-01-10 08:30:33,746: t15.2024.03.15 val PER: 0.1970
2026-01-10 08:30:33,746: t15.2024.03.17 val PER: 0.1234
2026-01-10 08:30:33,746: t15.2024.05.10 val PER: 0.1560
2026-01-10 08:30:33,746: t15.2024.06.14 val PER: 0.1514
2026-01-10 08:30:33,746: t15.2024.07.19 val PER: 0.2235
2026-01-10 08:30:33,746: t15.2024.07.21 val PER: 0.0800
2026-01-10 08:30:33,746: t15.2024.07.28 val PER: 0.1221
2026-01-10 08:30:33,746: t15.2025.01.10 val PER: 0.2631
2026-01-10 08:30:33,746: t15.2025.01.12 val PER: 0.1339
2026-01-10 08:30:33,746: t15.2025.03.14 val PER: 0.3210
2026-01-10 08:30:33,746: t15.2025.03.16 val PER: 0.1715
2026-01-10 08:30:33,746: t15.2025.03.30 val PER: 0.2667
2026-01-10 08:30:33,746: t15.2025.04.13 val PER: 0.2111
2026-01-10 08:30:33,892: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_19500
2026-01-10 08:30:43,155: Train batch 19600: loss: 6.29 grad norm: 45.81 time: 0.057
2026-01-10 08:31:01,982: Train batch 19800: loss: 6.38 grad norm: 48.83 time: 0.056
2026-01-10 08:31:21,127: Running test after training batch: 19999
2026-01-10 08:31:21,220: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:31:26,321: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-10 08:31:26,381: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost
2026-01-10 08:31:40,739: Val batch 19999: PER (avg): 0.1371 CTC Loss (avg): 14.4062 WER(5gram): 29.20% (n=256) time: 19.611
2026-01-10 08:31:40,739: WER lens: avg_true_words=5.99 avg_pred_words=6.40 max_pred_words=14
2026-01-10 08:31:40,739: t15.2023.08.13 val PER: 0.1019
2026-01-10 08:31:40,739: t15.2023.08.18 val PER: 0.0863
2026-01-10 08:31:40,740: t15.2023.08.20 val PER: 0.0913
2026-01-10 08:31:40,740: t15.2023.08.25 val PER: 0.1099
2026-01-10 08:31:40,740: t15.2023.08.27 val PER: 0.1801
2026-01-10 08:31:40,740: t15.2023.09.01 val PER: 0.0682
2026-01-10 08:31:40,740: t15.2023.09.03 val PER: 0.1496
2026-01-10 08:31:40,740: t15.2023.09.24 val PER: 0.1189
2026-01-10 08:31:40,740: t15.2023.09.29 val PER: 0.1232
2026-01-10 08:31:40,740: t15.2023.10.01 val PER: 0.1671
2026-01-10 08:31:40,740: t15.2023.10.06 val PER: 0.0797
2026-01-10 08:31:40,740: t15.2023.10.08 val PER: 0.2341
2026-01-10 08:31:40,740: t15.2023.10.13 val PER: 0.1784
2026-01-10 08:31:40,740: t15.2023.10.15 val PER: 0.1397
2026-01-10 08:31:40,740: t15.2023.10.20 val PER: 0.1745
2026-01-10 08:31:40,740: t15.2023.10.22 val PER: 0.1024
2026-01-10 08:31:40,741: t15.2023.11.03 val PER: 0.1655
2026-01-10 08:31:40,741: t15.2023.11.04 val PER: 0.0273
2026-01-10 08:31:40,741: t15.2023.11.17 val PER: 0.0311
2026-01-10 08:31:40,741: t15.2023.11.19 val PER: 0.0299
2026-01-10 08:31:40,741: t15.2023.11.26 val PER: 0.0949
2026-01-10 08:31:40,741: t15.2023.12.03 val PER: 0.0914
2026-01-10 08:31:40,741: t15.2023.12.08 val PER: 0.0792
2026-01-10 08:31:40,741: t15.2023.12.10 val PER: 0.0788
2026-01-10 08:31:40,742: t15.2023.12.17 val PER: 0.1133
2026-01-10 08:31:40,742: t15.2023.12.29 val PER: 0.1229
2026-01-10 08:31:40,742: t15.2024.02.25 val PER: 0.1053
2026-01-10 08:31:40,742: t15.2024.03.08 val PER: 0.2006
2026-01-10 08:31:40,742: t15.2024.03.15 val PER: 0.1982
2026-01-10 08:31:40,742: t15.2024.03.17 val PER: 0.1234
2026-01-10 08:31:40,742: t15.2024.05.10 val PER: 0.1501
2026-01-10 08:31:40,742: t15.2024.06.14 val PER: 0.1530
2026-01-10 08:31:40,742: t15.2024.07.19 val PER: 0.2221
2026-01-10 08:31:40,742: t15.2024.07.21 val PER: 0.0821
2026-01-10 08:31:40,742: t15.2024.07.28 val PER: 0.1221
2026-01-10 08:31:40,742: t15.2025.01.10 val PER: 0.2590
2026-01-10 08:31:40,742: t15.2025.01.12 val PER: 0.1316
2026-01-10 08:31:40,742: t15.2025.03.14 val PER: 0.3121
2026-01-10 08:31:40,742: t15.2025.03.16 val PER: 0.1702
2026-01-10 08:31:40,742: t15.2025.03.30 val PER: 0.2701
2026-01-10 08:31:40,743: t15.2025.04.13 val PER: 0.2126
2026-01-10 08:31:40,884: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr_grid/lr_0045/checkpoint/checkpoint_batch_19999
2026-01-10 08:31:41,472: Best avg val PER achieved: 0.14945
2026-01-10 08:31:41,472: Total training time: 49.82 minutes
All runs finished. Outputs in: /home/e12511253/Brain2Text/brain2text/trained_models
