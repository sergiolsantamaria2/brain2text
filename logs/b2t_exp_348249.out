TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_348249
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_348249/torch_extensions
WANDB_DIR=/tmp/e12511253_b2t_348249/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/e12511253_b2t_348249/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  3 22:15 /tmp/e12511253_b2t_348249/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
trained_models -> /tmp/e12511253_b2t_348249/trained_models
OUT_ROOT=/tmp/e12511253_b2t_348249/trained_models
==============================================
Job: b2t_exp  ID: 348249
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/gru/rnn_dropout/lr40
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/gru/rnn_dropout/lr40 ==========
Num configs: 5

=== RUN base.yaml ===
2026-01-03 22:15:17,382: Using device: cuda:0
2026-01-03 22:15:18,909: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-03 22:15:18,930: Using 45 sessions after filtering (from 45).
2026-01-03 22:15:19,333: Using torch.compile (if available)
2026-01-03 22:15:19,333: torch.compile not available (torch<2.0). Skipping.
2026-01-03 22:15:19,334: Initialized RNN decoding model
2026-01-03 22:15:19,334: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 22:15:19,334: Model has 44,907,305 parameters
2026-01-03 22:15:19,334: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 22:15:20,602: Successfully initialized datasets
2026-01-03 22:15:20,603: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 22:15:22,611: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.167
2026-01-03 22:15:22,611: Running test after training batch: 0
2026-01-03 22:15:22,737: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:15:27,940: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-03 22:15:28,654: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-03 22:16:02,043: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 39.432
2026-01-03 22:16:02,043: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-03 22:16:02,044: t15.2023.08.13 val PER: 1.3056
2026-01-03 22:16:02,044: t15.2023.08.18 val PER: 1.4208
2026-01-03 22:16:02,044: t15.2023.08.20 val PER: 1.3002
2026-01-03 22:16:02,044: t15.2023.08.25 val PER: 1.3389
2026-01-03 22:16:02,044: t15.2023.08.27 val PER: 1.2460
2026-01-03 22:16:02,044: t15.2023.09.01 val PER: 1.4537
2026-01-03 22:16:02,044: t15.2023.09.03 val PER: 1.3171
2026-01-03 22:16:02,044: t15.2023.09.24 val PER: 1.5461
2026-01-03 22:16:02,044: t15.2023.09.29 val PER: 1.4671
2026-01-03 22:16:02,044: t15.2023.10.01 val PER: 1.2147
2026-01-03 22:16:02,044: t15.2023.10.06 val PER: 1.4876
2026-01-03 22:16:02,044: t15.2023.10.08 val PER: 1.1827
2026-01-03 22:16:02,044: t15.2023.10.13 val PER: 1.3964
2026-01-03 22:16:02,044: t15.2023.10.15 val PER: 1.3889
2026-01-03 22:16:02,045: t15.2023.10.20 val PER: 1.4866
2026-01-03 22:16:02,045: t15.2023.10.22 val PER: 1.3942
2026-01-03 22:16:02,045: t15.2023.11.03 val PER: 1.5923
2026-01-03 22:16:02,045: t15.2023.11.04 val PER: 2.0171
2026-01-03 22:16:02,045: t15.2023.11.17 val PER: 1.9518
2026-01-03 22:16:02,045: t15.2023.11.19 val PER: 1.6707
2026-01-03 22:16:02,045: t15.2023.11.26 val PER: 1.5413
2026-01-03 22:16:02,045: t15.2023.12.03 val PER: 1.4254
2026-01-03 22:16:02,045: t15.2023.12.08 val PER: 1.4487
2026-01-03 22:16:02,045: t15.2023.12.10 val PER: 1.6899
2026-01-03 22:16:02,045: t15.2023.12.17 val PER: 1.3077
2026-01-03 22:16:02,045: t15.2023.12.29 val PER: 1.4063
2026-01-03 22:16:02,045: t15.2024.02.25 val PER: 1.4228
2026-01-03 22:16:02,045: t15.2024.03.08 val PER: 1.3257
2026-01-03 22:16:02,046: t15.2024.03.15 val PER: 1.3196
2026-01-03 22:16:02,046: t15.2024.03.17 val PER: 1.4052
2026-01-03 22:16:02,046: t15.2024.05.10 val PER: 1.3224
2026-01-03 22:16:02,046: t15.2024.06.14 val PER: 1.5315
2026-01-03 22:16:02,046: t15.2024.07.19 val PER: 1.0817
2026-01-03 22:16:02,046: t15.2024.07.21 val PER: 1.6290
2026-01-03 22:16:02,046: t15.2024.07.28 val PER: 1.6588
2026-01-03 22:16:02,046: t15.2025.01.10 val PER: 1.0923
2026-01-03 22:16:02,046: t15.2025.01.12 val PER: 1.7629
2026-01-03 22:16:02,046: t15.2025.03.14 val PER: 1.0414
2026-01-03 22:16:02,046: t15.2025.03.16 val PER: 1.6257
2026-01-03 22:16:02,047: t15.2025.03.30 val PER: 1.2874
2026-01-03 22:16:02,047: t15.2025.04.13 val PER: 1.5949
2026-01-03 22:16:02,048: New best val WER(1gram) inf% --> 100.00%
2026-01-03 22:16:02,048: Checkpointing model
2026-01-03 22:16:02,289: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:16:02,537: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_0
2026-01-03 22:16:21,050: Train batch 200: loss: 77.59 grad norm: 105.87 time: 0.055
2026-01-03 22:16:39,168: Train batch 400: loss: 53.51 grad norm: 84.08 time: 0.063
2026-01-03 22:16:48,083: Running test after training batch: 500
2026-01-03 22:16:48,214: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:16:53,054: WER debug example
  GT : you can see the code at this point as well
  PR : yule and ease thus uhde at this ide is aisle
2026-01-03 22:16:53,087: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as adz
2026-01-03 22:16:55,337: Val batch 500: PER (avg): 0.5191 CTC Loss (avg): 55.2750 WER(1gram): 87.82% (n=64) time: 7.254
2026-01-03 22:16:55,338: WER lens: avg_true_words=6.16 avg_pred_words=5.58 max_pred_words=11
2026-01-03 22:16:55,338: t15.2023.08.13 val PER: 0.4626
2026-01-03 22:16:55,338: t15.2023.08.18 val PER: 0.4568
2026-01-03 22:16:55,338: t15.2023.08.20 val PER: 0.4464
2026-01-03 22:16:55,338: t15.2023.08.25 val PER: 0.4307
2026-01-03 22:16:55,338: t15.2023.08.27 val PER: 0.5257
2026-01-03 22:16:55,338: t15.2023.09.01 val PER: 0.4196
2026-01-03 22:16:55,339: t15.2023.09.03 val PER: 0.5036
2026-01-03 22:16:55,339: t15.2023.09.24 val PER: 0.4296
2026-01-03 22:16:55,339: t15.2023.09.29 val PER: 0.4722
2026-01-03 22:16:55,339: t15.2023.10.01 val PER: 0.5277
2026-01-03 22:16:55,339: t15.2023.10.06 val PER: 0.4295
2026-01-03 22:16:55,339: t15.2023.10.08 val PER: 0.5386
2026-01-03 22:16:55,339: t15.2023.10.13 val PER: 0.5803
2026-01-03 22:16:55,339: t15.2023.10.15 val PER: 0.4957
2026-01-03 22:16:55,339: t15.2023.10.20 val PER: 0.4497
2026-01-03 22:16:55,339: t15.2023.10.22 val PER: 0.4477
2026-01-03 22:16:55,339: t15.2023.11.03 val PER: 0.5034
2026-01-03 22:16:55,340: t15.2023.11.04 val PER: 0.2799
2026-01-03 22:16:55,340: t15.2023.11.17 val PER: 0.3593
2026-01-03 22:16:55,340: t15.2023.11.19 val PER: 0.3453
2026-01-03 22:16:55,340: t15.2023.11.26 val PER: 0.5514
2026-01-03 22:16:55,340: t15.2023.12.03 val PER: 0.4989
2026-01-03 22:16:55,340: t15.2023.12.08 val PER: 0.5246
2026-01-03 22:16:55,340: t15.2023.12.10 val PER: 0.4573
2026-01-03 22:16:55,340: t15.2023.12.17 val PER: 0.5665
2026-01-03 22:16:55,340: t15.2023.12.29 val PER: 0.5415
2026-01-03 22:16:55,340: t15.2024.02.25 val PER: 0.4888
2026-01-03 22:16:55,340: t15.2024.03.08 val PER: 0.6159
2026-01-03 22:16:55,340: t15.2024.03.15 val PER: 0.5535
2026-01-03 22:16:55,341: t15.2024.03.17 val PER: 0.5056
2026-01-03 22:16:55,341: t15.2024.05.10 val PER: 0.5394
2026-01-03 22:16:55,341: t15.2024.06.14 val PER: 0.5063
2026-01-03 22:16:55,341: t15.2024.07.19 val PER: 0.6717
2026-01-03 22:16:55,341: t15.2024.07.21 val PER: 0.4745
2026-01-03 22:16:55,341: t15.2024.07.28 val PER: 0.5044
2026-01-03 22:16:55,341: t15.2025.01.10 val PER: 0.7383
2026-01-03 22:16:55,341: t15.2025.01.12 val PER: 0.5574
2026-01-03 22:16:55,341: t15.2025.03.14 val PER: 0.7589
2026-01-03 22:16:55,341: t15.2025.03.16 val PER: 0.5877
2026-01-03 22:16:55,341: t15.2025.03.30 val PER: 0.7241
2026-01-03 22:16:55,341: t15.2025.04.13 val PER: 0.5692
2026-01-03 22:16:55,343: New best val WER(1gram) 100.00% --> 87.82%
2026-01-03 22:16:55,343: Checkpointing model
2026-01-03 22:16:55,925: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:16:56,171: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_500
2026-01-03 22:17:04,929: Train batch 600: loss: 48.73 grad norm: 79.58 time: 0.078
2026-01-03 22:17:22,141: Train batch 800: loss: 41.01 grad norm: 86.26 time: 0.057
2026-01-03 22:17:39,351: Train batch 1000: loss: 42.29 grad norm: 76.03 time: 0.066
2026-01-03 22:17:39,352: Running test after training batch: 1000
2026-01-03 22:17:39,480: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:17:44,251: WER debug example
  GT : you can see the code at this point as well
  PR : wooed ent ease thus good it this uhde is oui
2026-01-03 22:17:44,283: WER debug example
  GT : how does it keep the cost down
  PR : houde is it eke that wass it
2026-01-03 22:17:46,091: Val batch 1000: PER (avg): 0.4095 CTC Loss (avg): 42.3164 WER(1gram): 82.74% (n=64) time: 6.738
2026-01-03 22:17:46,091: WER lens: avg_true_words=6.16 avg_pred_words=5.58 max_pred_words=12
2026-01-03 22:17:46,091: t15.2023.08.13 val PER: 0.3815
2026-01-03 22:17:46,091: t15.2023.08.18 val PER: 0.3386
2026-01-03 22:17:46,092: t15.2023.08.20 val PER: 0.3487
2026-01-03 22:17:46,092: t15.2023.08.25 val PER: 0.3102
2026-01-03 22:17:46,092: t15.2023.08.27 val PER: 0.4341
2026-01-03 22:17:46,092: t15.2023.09.01 val PER: 0.3019
2026-01-03 22:17:46,092: t15.2023.09.03 val PER: 0.4050
2026-01-03 22:17:46,092: t15.2023.09.24 val PER: 0.3252
2026-01-03 22:17:46,092: t15.2023.09.29 val PER: 0.3580
2026-01-03 22:17:46,092: t15.2023.10.01 val PER: 0.4062
2026-01-03 22:17:46,092: t15.2023.10.06 val PER: 0.3165
2026-01-03 22:17:46,092: t15.2023.10.08 val PER: 0.4438
2026-01-03 22:17:46,092: t15.2023.10.13 val PER: 0.4608
2026-01-03 22:17:46,092: t15.2023.10.15 val PER: 0.3731
2026-01-03 22:17:46,093: t15.2023.10.20 val PER: 0.3725
2026-01-03 22:17:46,093: t15.2023.10.22 val PER: 0.3541
2026-01-03 22:17:46,093: t15.2023.11.03 val PER: 0.3969
2026-01-03 22:17:46,093: t15.2023.11.04 val PER: 0.1468
2026-01-03 22:17:46,094: t15.2023.11.17 val PER: 0.2628
2026-01-03 22:17:46,094: t15.2023.11.19 val PER: 0.2156
2026-01-03 22:17:46,095: t15.2023.11.26 val PER: 0.4486
2026-01-03 22:17:46,095: t15.2023.12.03 val PER: 0.4023
2026-01-03 22:17:46,095: t15.2023.12.08 val PER: 0.4061
2026-01-03 22:17:46,095: t15.2023.12.10 val PER: 0.3430
2026-01-03 22:17:46,095: t15.2023.12.17 val PER: 0.4085
2026-01-03 22:17:46,095: t15.2023.12.29 val PER: 0.4022
2026-01-03 22:17:46,095: t15.2024.02.25 val PER: 0.3638
2026-01-03 22:17:46,095: t15.2024.03.08 val PER: 0.5036
2026-01-03 22:17:46,095: t15.2024.03.15 val PER: 0.4390
2026-01-03 22:17:46,095: t15.2024.03.17 val PER: 0.4135
2026-01-03 22:17:46,096: t15.2024.05.10 val PER: 0.4339
2026-01-03 22:17:46,096: t15.2024.06.14 val PER: 0.4132
2026-01-03 22:17:46,096: t15.2024.07.19 val PER: 0.5313
2026-01-03 22:17:46,096: t15.2024.07.21 val PER: 0.3862
2026-01-03 22:17:46,096: t15.2024.07.28 val PER: 0.4081
2026-01-03 22:17:46,096: t15.2025.01.10 val PER: 0.6047
2026-01-03 22:17:46,096: t15.2025.01.12 val PER: 0.4557
2026-01-03 22:17:46,096: t15.2025.03.14 val PER: 0.6479
2026-01-03 22:17:46,096: t15.2025.03.16 val PER: 0.4791
2026-01-03 22:17:46,096: t15.2025.03.30 val PER: 0.6552
2026-01-03 22:17:46,097: t15.2025.04.13 val PER: 0.5036
2026-01-03 22:17:46,097: New best val WER(1gram) 87.82% --> 82.74%
2026-01-03 22:17:46,097: Checkpointing model
2026-01-03 22:17:46,709: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:17:46,957: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_1000
2026-01-03 22:18:03,883: Train batch 1200: loss: 32.98 grad norm: 73.57 time: 0.068
2026-01-03 22:18:21,068: Train batch 1400: loss: 35.49 grad norm: 78.05 time: 0.060
2026-01-03 22:18:29,922: Running test after training batch: 1500
2026-01-03 22:18:30,015: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:18:34,783: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt e the owed at this boyde is will
2026-01-03 22:18:34,813: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap thus us
2026-01-03 22:18:36,384: Val batch 1500: PER (avg): 0.3788 CTC Loss (avg): 37.1657 WER(1gram): 75.63% (n=64) time: 6.462
2026-01-03 22:18:36,385: WER lens: avg_true_words=6.16 avg_pred_words=5.08 max_pred_words=11
2026-01-03 22:18:36,385: t15.2023.08.13 val PER: 0.3389
2026-01-03 22:18:36,385: t15.2023.08.18 val PER: 0.3160
2026-01-03 22:18:36,385: t15.2023.08.20 val PER: 0.3074
2026-01-03 22:18:36,385: t15.2023.08.25 val PER: 0.2545
2026-01-03 22:18:36,385: t15.2023.08.27 val PER: 0.4164
2026-01-03 22:18:36,385: t15.2023.09.01 val PER: 0.2719
2026-01-03 22:18:36,386: t15.2023.09.03 val PER: 0.3765
2026-01-03 22:18:36,386: t15.2023.09.24 val PER: 0.3046
2026-01-03 22:18:36,386: t15.2023.09.29 val PER: 0.3369
2026-01-03 22:18:36,386: t15.2023.10.01 val PER: 0.3963
2026-01-03 22:18:36,386: t15.2023.10.06 val PER: 0.2777
2026-01-03 22:18:36,386: t15.2023.10.08 val PER: 0.4357
2026-01-03 22:18:36,386: t15.2023.10.13 val PER: 0.4407
2026-01-03 22:18:36,386: t15.2023.10.15 val PER: 0.3586
2026-01-03 22:18:36,386: t15.2023.10.20 val PER: 0.3322
2026-01-03 22:18:36,386: t15.2023.10.22 val PER: 0.3185
2026-01-03 22:18:36,387: t15.2023.11.03 val PER: 0.3609
2026-01-03 22:18:36,387: t15.2023.11.04 val PER: 0.1092
2026-01-03 22:18:36,387: t15.2023.11.17 val PER: 0.2193
2026-01-03 22:18:36,387: t15.2023.11.19 val PER: 0.1717
2026-01-03 22:18:36,387: t15.2023.11.26 val PER: 0.4152
2026-01-03 22:18:36,387: t15.2023.12.03 val PER: 0.3739
2026-01-03 22:18:36,387: t15.2023.12.08 val PER: 0.3502
2026-01-03 22:18:36,387: t15.2023.12.10 val PER: 0.3075
2026-01-03 22:18:36,387: t15.2023.12.17 val PER: 0.3773
2026-01-03 22:18:36,387: t15.2023.12.29 val PER: 0.3699
2026-01-03 22:18:36,387: t15.2024.02.25 val PER: 0.3132
2026-01-03 22:18:36,388: t15.2024.03.08 val PER: 0.4509
2026-01-03 22:18:36,388: t15.2024.03.15 val PER: 0.4190
2026-01-03 22:18:36,388: t15.2024.03.17 val PER: 0.3745
2026-01-03 22:18:36,388: t15.2024.05.10 val PER: 0.3834
2026-01-03 22:18:36,388: t15.2024.06.14 val PER: 0.3817
2026-01-03 22:18:36,388: t15.2024.07.19 val PER: 0.5181
2026-01-03 22:18:36,388: t15.2024.07.21 val PER: 0.3448
2026-01-03 22:18:36,388: t15.2024.07.28 val PER: 0.3662
2026-01-03 22:18:36,388: t15.2025.01.10 val PER: 0.6281
2026-01-03 22:18:36,388: t15.2025.01.12 val PER: 0.4180
2026-01-03 22:18:36,388: t15.2025.03.14 val PER: 0.6095
2026-01-03 22:18:36,388: t15.2025.03.16 val PER: 0.4516
2026-01-03 22:18:36,389: t15.2025.03.30 val PER: 0.6287
2026-01-03 22:18:36,389: t15.2025.04.13 val PER: 0.4822
2026-01-03 22:18:36,389: New best val WER(1gram) 82.74% --> 75.63%
2026-01-03 22:18:36,389: Checkpointing model
2026-01-03 22:18:36,994: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:18:37,242: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_1500
2026-01-03 22:18:45,608: Train batch 1600: loss: 37.26 grad norm: 80.82 time: 0.064
2026-01-03 22:19:02,523: Train batch 1800: loss: 34.80 grad norm: 69.05 time: 0.088
2026-01-03 22:19:19,558: Train batch 2000: loss: 33.41 grad norm: 70.23 time: 0.066
2026-01-03 22:19:19,558: Running test after training batch: 2000
2026-01-03 22:19:19,675: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:19:24,867: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt sze the code at this and is will
2026-01-03 22:19:24,897: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heeke thus us id
2026-01-03 22:19:26,431: Val batch 2000: PER (avg): 0.3254 CTC Loss (avg): 32.7620 WER(1gram): 69.29% (n=64) time: 6.872
2026-01-03 22:19:26,431: WER lens: avg_true_words=6.16 avg_pred_words=5.61 max_pred_words=11
2026-01-03 22:19:26,431: t15.2023.08.13 val PER: 0.2994
2026-01-03 22:19:26,432: t15.2023.08.18 val PER: 0.2473
2026-01-03 22:19:26,432: t15.2023.08.20 val PER: 0.2566
2026-01-03 22:19:26,432: t15.2023.08.25 val PER: 0.2259
2026-01-03 22:19:26,432: t15.2023.08.27 val PER: 0.3441
2026-01-03 22:19:26,432: t15.2023.09.01 val PER: 0.2240
2026-01-03 22:19:26,432: t15.2023.09.03 val PER: 0.3266
2026-01-03 22:19:26,432: t15.2023.09.24 val PER: 0.2561
2026-01-03 22:19:26,432: t15.2023.09.29 val PER: 0.2795
2026-01-03 22:19:26,432: t15.2023.10.01 val PER: 0.3276
2026-01-03 22:19:26,432: t15.2023.10.06 val PER: 0.2400
2026-01-03 22:19:26,432: t15.2023.10.08 val PER: 0.3870
2026-01-03 22:19:26,432: t15.2023.10.13 val PER: 0.3763
2026-01-03 22:19:26,432: t15.2023.10.15 val PER: 0.3026
2026-01-03 22:19:26,432: t15.2023.10.20 val PER: 0.2886
2026-01-03 22:19:26,432: t15.2023.10.22 val PER: 0.2572
2026-01-03 22:19:26,432: t15.2023.11.03 val PER: 0.3128
2026-01-03 22:19:26,433: t15.2023.11.04 val PER: 0.0887
2026-01-03 22:19:26,433: t15.2023.11.17 val PER: 0.1633
2026-01-03 22:19:26,433: t15.2023.11.19 val PER: 0.1377
2026-01-03 22:19:26,433: t15.2023.11.26 val PER: 0.3630
2026-01-03 22:19:26,433: t15.2023.12.03 val PER: 0.3162
2026-01-03 22:19:26,433: t15.2023.12.08 val PER: 0.3142
2026-01-03 22:19:26,433: t15.2023.12.10 val PER: 0.2484
2026-01-03 22:19:26,433: t15.2023.12.17 val PER: 0.3170
2026-01-03 22:19:26,433: t15.2023.12.29 val PER: 0.3219
2026-01-03 22:19:26,433: t15.2024.02.25 val PER: 0.2823
2026-01-03 22:19:26,433: t15.2024.03.08 val PER: 0.3855
2026-01-03 22:19:26,433: t15.2024.03.15 val PER: 0.3640
2026-01-03 22:19:26,433: t15.2024.03.17 val PER: 0.3417
2026-01-03 22:19:26,433: t15.2024.05.10 val PER: 0.3536
2026-01-03 22:19:26,433: t15.2024.06.14 val PER: 0.3297
2026-01-03 22:19:26,434: t15.2024.07.19 val PER: 0.4654
2026-01-03 22:19:26,434: t15.2024.07.21 val PER: 0.2917
2026-01-03 22:19:26,434: t15.2024.07.28 val PER: 0.3176
2026-01-03 22:19:26,434: t15.2025.01.10 val PER: 0.5468
2026-01-03 22:19:26,434: t15.2025.01.12 val PER: 0.3749
2026-01-03 22:19:26,434: t15.2025.03.14 val PER: 0.5340
2026-01-03 22:19:26,434: t15.2025.03.16 val PER: 0.3887
2026-01-03 22:19:26,434: t15.2025.03.30 val PER: 0.5322
2026-01-03 22:19:26,434: t15.2025.04.13 val PER: 0.3980
2026-01-03 22:19:26,435: New best val WER(1gram) 75.63% --> 69.29%
2026-01-03 22:19:26,435: Checkpointing model
2026-01-03 22:19:27,059: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:19:27,301: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_2000
2026-01-03 22:19:44,620: Train batch 2200: loss: 29.12 grad norm: 72.38 time: 0.059
2026-01-03 22:20:02,064: Train batch 2400: loss: 28.81 grad norm: 67.76 time: 0.051
2026-01-03 22:20:11,061: Running test after training batch: 2500
2026-01-03 22:20:11,161: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:20:15,958: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-03 22:20:15,987: WER debug example
  GT : how does it keep the cost down
  PR : howl does it kipp the us it
2026-01-03 22:20:17,572: Val batch 2500: PER (avg): 0.3055 CTC Loss (avg): 30.2924 WER(1gram): 67.51% (n=64) time: 6.511
2026-01-03 22:20:17,573: WER lens: avg_true_words=6.16 avg_pred_words=5.59 max_pred_words=11
2026-01-03 22:20:17,573: t15.2023.08.13 val PER: 0.2848
2026-01-03 22:20:17,573: t15.2023.08.18 val PER: 0.2347
2026-01-03 22:20:17,573: t15.2023.08.20 val PER: 0.2431
2026-01-03 22:20:17,573: t15.2023.08.25 val PER: 0.1943
2026-01-03 22:20:17,573: t15.2023.08.27 val PER: 0.3232
2026-01-03 22:20:17,573: t15.2023.09.01 val PER: 0.2070
2026-01-03 22:20:17,573: t15.2023.09.03 val PER: 0.3005
2026-01-03 22:20:17,574: t15.2023.09.24 val PER: 0.2282
2026-01-03 22:20:17,574: t15.2023.09.29 val PER: 0.2616
2026-01-03 22:20:17,574: t15.2023.10.01 val PER: 0.3131
2026-01-03 22:20:17,574: t15.2023.10.06 val PER: 0.2110
2026-01-03 22:20:17,574: t15.2023.10.08 val PER: 0.3775
2026-01-03 22:20:17,574: t15.2023.10.13 val PER: 0.3530
2026-01-03 22:20:17,574: t15.2023.10.15 val PER: 0.2848
2026-01-03 22:20:17,574: t15.2023.10.20 val PER: 0.2685
2026-01-03 22:20:17,574: t15.2023.10.22 val PER: 0.2416
2026-01-03 22:20:17,574: t15.2023.11.03 val PER: 0.2897
2026-01-03 22:20:17,574: t15.2023.11.04 val PER: 0.0853
2026-01-03 22:20:17,575: t15.2023.11.17 val PER: 0.1509
2026-01-03 22:20:17,575: t15.2023.11.19 val PER: 0.1377
2026-01-03 22:20:17,575: t15.2023.11.26 val PER: 0.3493
2026-01-03 22:20:17,575: t15.2023.12.03 val PER: 0.2773
2026-01-03 22:20:17,575: t15.2023.12.08 val PER: 0.2796
2026-01-03 22:20:17,575: t15.2023.12.10 val PER: 0.2444
2026-01-03 22:20:17,575: t15.2023.12.17 val PER: 0.2838
2026-01-03 22:20:17,575: t15.2023.12.29 val PER: 0.3068
2026-01-03 22:20:17,575: t15.2024.02.25 val PER: 0.2458
2026-01-03 22:20:17,575: t15.2024.03.08 val PER: 0.3784
2026-01-03 22:20:17,575: t15.2024.03.15 val PER: 0.3496
2026-01-03 22:20:17,575: t15.2024.03.17 val PER: 0.3166
2026-01-03 22:20:17,575: t15.2024.05.10 val PER: 0.3224
2026-01-03 22:20:17,575: t15.2024.06.14 val PER: 0.3218
2026-01-03 22:20:17,575: t15.2024.07.19 val PER: 0.4436
2026-01-03 22:20:17,575: t15.2024.07.21 val PER: 0.2579
2026-01-03 22:20:17,576: t15.2024.07.28 val PER: 0.3088
2026-01-03 22:20:17,576: t15.2025.01.10 val PER: 0.5110
2026-01-03 22:20:17,576: t15.2025.01.12 val PER: 0.3634
2026-01-03 22:20:17,576: t15.2025.03.14 val PER: 0.4985
2026-01-03 22:20:17,576: t15.2025.03.16 val PER: 0.3626
2026-01-03 22:20:17,576: t15.2025.03.30 val PER: 0.5161
2026-01-03 22:20:17,576: t15.2025.04.13 val PER: 0.3966
2026-01-03 22:20:17,577: New best val WER(1gram) 69.29% --> 67.51%
2026-01-03 22:20:17,577: Checkpointing model
2026-01-03 22:20:18,182: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:20:18,427: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_2500
2026-01-03 22:20:26,683: Train batch 2600: loss: 35.21 grad norm: 82.51 time: 0.054
2026-01-03 22:20:43,576: Train batch 2800: loss: 25.42 grad norm: 71.00 time: 0.081
2026-01-03 22:21:00,899: Train batch 3000: loss: 31.19 grad norm: 70.19 time: 0.082
2026-01-03 22:21:00,899: Running test after training batch: 3000
2026-01-03 22:21:00,998: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:21:05,757: WER debug example
  GT : you can see the code at this point as well
  PR : yule end eke the could at this point is will
2026-01-03 22:21:05,785: WER debug example
  GT : how does it keep the cost down
  PR : houde des it kipp the cost get
2026-01-03 22:21:07,387: Val batch 3000: PER (avg): 0.2809 CTC Loss (avg): 27.7855 WER(1gram): 65.48% (n=64) time: 6.488
2026-01-03 22:21:07,388: WER lens: avg_true_words=6.16 avg_pred_words=5.73 max_pred_words=11
2026-01-03 22:21:07,388: t15.2023.08.13 val PER: 0.2609
2026-01-03 22:21:07,388: t15.2023.08.18 val PER: 0.2154
2026-01-03 22:21:07,388: t15.2023.08.20 val PER: 0.2089
2026-01-03 22:21:07,388: t15.2023.08.25 val PER: 0.2018
2026-01-03 22:21:07,388: t15.2023.08.27 val PER: 0.2990
2026-01-03 22:21:07,388: t15.2023.09.01 val PER: 0.1916
2026-01-03 22:21:07,388: t15.2023.09.03 val PER: 0.2874
2026-01-03 22:21:07,388: t15.2023.09.24 val PER: 0.2197
2026-01-03 22:21:07,388: t15.2023.09.29 val PER: 0.2310
2026-01-03 22:21:07,388: t15.2023.10.01 val PER: 0.2893
2026-01-03 22:21:07,388: t15.2023.10.06 val PER: 0.1938
2026-01-03 22:21:07,389: t15.2023.10.08 val PER: 0.3437
2026-01-03 22:21:07,389: t15.2023.10.13 val PER: 0.3483
2026-01-03 22:21:07,389: t15.2023.10.15 val PER: 0.2604
2026-01-03 22:21:07,389: t15.2023.10.20 val PER: 0.2685
2026-01-03 22:21:07,389: t15.2023.10.22 val PER: 0.2060
2026-01-03 22:21:07,389: t15.2023.11.03 val PER: 0.2714
2026-01-03 22:21:07,389: t15.2023.11.04 val PER: 0.0853
2026-01-03 22:21:07,389: t15.2023.11.17 val PER: 0.1244
2026-01-03 22:21:07,389: t15.2023.11.19 val PER: 0.1218
2026-01-03 22:21:07,389: t15.2023.11.26 val PER: 0.3000
2026-01-03 22:21:07,389: t15.2023.12.03 val PER: 0.2511
2026-01-03 22:21:07,389: t15.2023.12.08 val PER: 0.2603
2026-01-03 22:21:07,389: t15.2023.12.10 val PER: 0.2116
2026-01-03 22:21:07,389: t15.2023.12.17 val PER: 0.2734
2026-01-03 22:21:07,390: t15.2023.12.29 val PER: 0.2896
2026-01-03 22:21:07,390: t15.2024.02.25 val PER: 0.2346
2026-01-03 22:21:07,390: t15.2024.03.08 val PER: 0.3670
2026-01-03 22:21:07,390: t15.2024.03.15 val PER: 0.3271
2026-01-03 22:21:07,390: t15.2024.03.17 val PER: 0.2824
2026-01-03 22:21:07,390: t15.2024.05.10 val PER: 0.3016
2026-01-03 22:21:07,390: t15.2024.06.14 val PER: 0.2934
2026-01-03 22:21:07,390: t15.2024.07.19 val PER: 0.3995
2026-01-03 22:21:07,390: t15.2024.07.21 val PER: 0.2393
2026-01-03 22:21:07,390: t15.2024.07.28 val PER: 0.2765
2026-01-03 22:21:07,390: t15.2025.01.10 val PER: 0.5000
2026-01-03 22:21:07,390: t15.2025.01.12 val PER: 0.3264
2026-01-03 22:21:07,391: t15.2025.03.14 val PER: 0.4512
2026-01-03 22:21:07,391: t15.2025.03.16 val PER: 0.3272
2026-01-03 22:21:07,391: t15.2025.03.30 val PER: 0.4816
2026-01-03 22:21:07,391: t15.2025.04.13 val PER: 0.3524
2026-01-03 22:21:07,392: New best val WER(1gram) 67.51% --> 65.48%
2026-01-03 22:21:07,392: Checkpointing model
2026-01-03 22:21:08,011: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:21:08,254: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_3000
2026-01-03 22:21:26,256: Train batch 3200: loss: 26.64 grad norm: 70.48 time: 0.076
2026-01-03 22:21:43,783: Train batch 3400: loss: 18.22 grad norm: 53.33 time: 0.048
2026-01-03 22:21:52,950: Running test after training batch: 3500
2026-01-03 22:21:53,079: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:21:57,826: WER debug example
  GT : you can see the code at this point as well
  PR : yu end sci the code at this point will
2026-01-03 22:21:57,854: WER debug example
  GT : how does it keep the cost down
  PR : houde des it eke thus cussed get
2026-01-03 22:21:59,431: Val batch 3500: PER (avg): 0.2667 CTC Loss (avg): 26.5538 WER(1gram): 66.24% (n=64) time: 6.481
2026-01-03 22:21:59,432: WER lens: avg_true_words=6.16 avg_pred_words=5.91 max_pred_words=11
2026-01-03 22:21:59,432: t15.2023.08.13 val PER: 0.2349
2026-01-03 22:21:59,432: t15.2023.08.18 val PER: 0.2079
2026-01-03 22:21:59,432: t15.2023.08.20 val PER: 0.2192
2026-01-03 22:21:59,432: t15.2023.08.25 val PER: 0.1837
2026-01-03 22:21:59,432: t15.2023.08.27 val PER: 0.2749
2026-01-03 22:21:59,433: t15.2023.09.01 val PER: 0.1810
2026-01-03 22:21:59,433: t15.2023.09.03 val PER: 0.2565
2026-01-03 22:21:59,433: t15.2023.09.24 val PER: 0.2160
2026-01-03 22:21:59,433: t15.2023.09.29 val PER: 0.2157
2026-01-03 22:21:59,433: t15.2023.10.01 val PER: 0.2761
2026-01-03 22:21:59,433: t15.2023.10.06 val PER: 0.1895
2026-01-03 22:21:59,433: t15.2023.10.08 val PER: 0.3396
2026-01-03 22:21:59,433: t15.2023.10.13 val PER: 0.3189
2026-01-03 22:21:59,433: t15.2023.10.15 val PER: 0.2367
2026-01-03 22:21:59,433: t15.2023.10.20 val PER: 0.2483
2026-01-03 22:21:59,433: t15.2023.10.22 val PER: 0.2183
2026-01-03 22:21:59,433: t15.2023.11.03 val PER: 0.2612
2026-01-03 22:21:59,433: t15.2023.11.04 val PER: 0.0853
2026-01-03 22:21:59,433: t15.2023.11.17 val PER: 0.1213
2026-01-03 22:21:59,434: t15.2023.11.19 val PER: 0.1058
2026-01-03 22:21:59,434: t15.2023.11.26 val PER: 0.2855
2026-01-03 22:21:59,434: t15.2023.12.03 val PER: 0.2416
2026-01-03 22:21:59,434: t15.2023.12.08 val PER: 0.2430
2026-01-03 22:21:59,434: t15.2023.12.10 val PER: 0.2024
2026-01-03 22:21:59,434: t15.2023.12.17 val PER: 0.2516
2026-01-03 22:21:59,434: t15.2023.12.29 val PER: 0.2629
2026-01-03 22:21:59,434: t15.2024.02.25 val PER: 0.2107
2026-01-03 22:21:59,434: t15.2024.03.08 val PER: 0.3457
2026-01-03 22:21:59,434: t15.2024.03.15 val PER: 0.3233
2026-01-03 22:21:59,434: t15.2024.03.17 val PER: 0.2650
2026-01-03 22:21:59,434: t15.2024.05.10 val PER: 0.2793
2026-01-03 22:21:59,435: t15.2024.06.14 val PER: 0.2871
2026-01-03 22:21:59,435: t15.2024.07.19 val PER: 0.3942
2026-01-03 22:21:59,435: t15.2024.07.21 val PER: 0.2248
2026-01-03 22:21:59,435: t15.2024.07.28 val PER: 0.2838
2026-01-03 22:21:59,435: t15.2025.01.10 val PER: 0.4545
2026-01-03 22:21:59,435: t15.2025.01.12 val PER: 0.2995
2026-01-03 22:21:59,435: t15.2025.03.14 val PER: 0.4275
2026-01-03 22:21:59,435: t15.2025.03.16 val PER: 0.3233
2026-01-03 22:21:59,435: t15.2025.03.30 val PER: 0.4483
2026-01-03 22:21:59,435: t15.2025.04.13 val PER: 0.3310
2026-01-03 22:21:59,665: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_3500
2026-01-03 22:22:08,331: Train batch 3600: loss: 22.22 grad norm: 59.87 time: 0.068
2026-01-03 22:22:25,630: Train batch 3800: loss: 25.84 grad norm: 69.23 time: 0.068
2026-01-03 22:22:43,174: Train batch 4000: loss: 19.89 grad norm: 57.35 time: 0.056
2026-01-03 22:22:43,174: Running test after training batch: 4000
2026-01-03 22:22:43,265: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:22:47,984: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point while
2026-01-03 22:22:48,013: WER debug example
  GT : how does it keep the cost down
  PR : how dust it hipp the cost nett
2026-01-03 22:22:49,613: Val batch 4000: PER (avg): 0.2513 CTC Loss (avg): 24.4583 WER(1gram): 64.21% (n=64) time: 6.439
2026-01-03 22:22:49,614: WER lens: avg_true_words=6.16 avg_pred_words=5.94 max_pred_words=11
2026-01-03 22:22:49,614: t15.2023.08.13 val PER: 0.2266
2026-01-03 22:22:49,614: t15.2023.08.18 val PER: 0.2112
2026-01-03 22:22:49,614: t15.2023.08.20 val PER: 0.2033
2026-01-03 22:22:49,614: t15.2023.08.25 val PER: 0.1536
2026-01-03 22:22:49,614: t15.2023.08.27 val PER: 0.2862
2026-01-03 22:22:49,614: t15.2023.09.01 val PER: 0.1623
2026-01-03 22:22:49,614: t15.2023.09.03 val PER: 0.2375
2026-01-03 22:22:49,614: t15.2023.09.24 val PER: 0.2002
2026-01-03 22:22:49,614: t15.2023.09.29 val PER: 0.2029
2026-01-03 22:22:49,614: t15.2023.10.01 val PER: 0.2662
2026-01-03 22:22:49,615: t15.2023.10.06 val PER: 0.1744
2026-01-03 22:22:49,615: t15.2023.10.08 val PER: 0.3194
2026-01-03 22:22:49,615: t15.2023.10.13 val PER: 0.2995
2026-01-03 22:22:49,615: t15.2023.10.15 val PER: 0.2439
2026-01-03 22:22:49,615: t15.2023.10.20 val PER: 0.2483
2026-01-03 22:22:49,615: t15.2023.10.22 val PER: 0.2049
2026-01-03 22:22:49,615: t15.2023.11.03 val PER: 0.2469
2026-01-03 22:22:49,615: t15.2023.11.04 val PER: 0.0785
2026-01-03 22:22:49,615: t15.2023.11.17 val PER: 0.1026
2026-01-03 22:22:49,615: t15.2023.11.19 val PER: 0.0958
2026-01-03 22:22:49,615: t15.2023.11.26 val PER: 0.2623
2026-01-03 22:22:49,615: t15.2023.12.03 val PER: 0.2195
2026-01-03 22:22:49,616: t15.2023.12.08 val PER: 0.2297
2026-01-03 22:22:49,616: t15.2023.12.10 val PER: 0.1708
2026-01-03 22:22:49,616: t15.2023.12.17 val PER: 0.2495
2026-01-03 22:22:49,616: t15.2023.12.29 val PER: 0.2553
2026-01-03 22:22:49,616: t15.2024.02.25 val PER: 0.2121
2026-01-03 22:22:49,616: t15.2024.03.08 val PER: 0.3357
2026-01-03 22:22:49,616: t15.2024.03.15 val PER: 0.3021
2026-01-03 22:22:49,616: t15.2024.03.17 val PER: 0.2629
2026-01-03 22:22:49,616: t15.2024.05.10 val PER: 0.2689
2026-01-03 22:22:49,616: t15.2024.06.14 val PER: 0.2571
2026-01-03 22:22:49,616: t15.2024.07.19 val PER: 0.3619
2026-01-03 22:22:49,616: t15.2024.07.21 val PER: 0.1869
2026-01-03 22:22:49,616: t15.2024.07.28 val PER: 0.2375
2026-01-03 22:22:49,616: t15.2025.01.10 val PER: 0.4366
2026-01-03 22:22:49,616: t15.2025.01.12 val PER: 0.2941
2026-01-03 22:22:49,616: t15.2025.03.14 val PER: 0.4216
2026-01-03 22:22:49,617: t15.2025.03.16 val PER: 0.3024
2026-01-03 22:22:49,617: t15.2025.03.30 val PER: 0.4230
2026-01-03 22:22:49,617: t15.2025.04.13 val PER: 0.3153
2026-01-03 22:22:49,618: New best val WER(1gram) 65.48% --> 64.21%
2026-01-03 22:22:49,618: Checkpointing model
2026-01-03 22:22:50,243: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:22:50,489: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_4000
2026-01-03 22:23:08,812: Train batch 4200: loss: 21.99 grad norm: 67.12 time: 0.080
2026-01-03 22:23:27,105: Train batch 4400: loss: 17.25 grad norm: 55.37 time: 0.066
2026-01-03 22:23:36,358: Running test after training batch: 4500
2026-01-03 22:23:36,486: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:23:41,487: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the code at this point us will
2026-01-03 22:23:41,517: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap thus cost get
2026-01-03 22:23:43,109: Val batch 4500: PER (avg): 0.2360 CTC Loss (avg): 23.2696 WER(1gram): 61.93% (n=64) time: 6.750
2026-01-03 22:23:43,109: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 22:23:43,109: t15.2023.08.13 val PER: 0.1923
2026-01-03 22:23:43,109: t15.2023.08.18 val PER: 0.1852
2026-01-03 22:23:43,109: t15.2023.08.20 val PER: 0.1867
2026-01-03 22:23:43,110: t15.2023.08.25 val PER: 0.1476
2026-01-03 22:23:43,110: t15.2023.08.27 val PER: 0.2524
2026-01-03 22:23:43,110: t15.2023.09.01 val PER: 0.1469
2026-01-03 22:23:43,110: t15.2023.09.03 val PER: 0.2387
2026-01-03 22:23:43,110: t15.2023.09.24 val PER: 0.1748
2026-01-03 22:23:43,110: t15.2023.09.29 val PER: 0.2029
2026-01-03 22:23:43,110: t15.2023.10.01 val PER: 0.2543
2026-01-03 22:23:43,110: t15.2023.10.06 val PER: 0.1485
2026-01-03 22:23:43,110: t15.2023.10.08 val PER: 0.3018
2026-01-03 22:23:43,110: t15.2023.10.13 val PER: 0.3010
2026-01-03 22:23:43,110: t15.2023.10.15 val PER: 0.2287
2026-01-03 22:23:43,110: t15.2023.10.20 val PER: 0.2315
2026-01-03 22:23:43,110: t15.2023.10.22 val PER: 0.1860
2026-01-03 22:23:43,110: t15.2023.11.03 val PER: 0.2429
2026-01-03 22:23:43,110: t15.2023.11.04 val PER: 0.0546
2026-01-03 22:23:43,110: t15.2023.11.17 val PER: 0.1058
2026-01-03 22:23:43,111: t15.2023.11.19 val PER: 0.0918
2026-01-03 22:23:43,111: t15.2023.11.26 val PER: 0.2594
2026-01-03 22:23:43,111: t15.2023.12.03 val PER: 0.2069
2026-01-03 22:23:43,111: t15.2023.12.08 val PER: 0.2077
2026-01-03 22:23:43,111: t15.2023.12.10 val PER: 0.1577
2026-01-03 22:23:43,111: t15.2023.12.17 val PER: 0.2401
2026-01-03 22:23:43,111: t15.2023.12.29 val PER: 0.2457
2026-01-03 22:23:43,111: t15.2024.02.25 val PER: 0.1910
2026-01-03 22:23:43,111: t15.2024.03.08 val PER: 0.3144
2026-01-03 22:23:43,111: t15.2024.03.15 val PER: 0.2927
2026-01-03 22:23:43,111: t15.2024.03.17 val PER: 0.2364
2026-01-03 22:23:43,111: t15.2024.05.10 val PER: 0.2467
2026-01-03 22:23:43,111: t15.2024.06.14 val PER: 0.2382
2026-01-03 22:23:43,111: t15.2024.07.19 val PER: 0.3263
2026-01-03 22:23:43,111: t15.2024.07.21 val PER: 0.1800
2026-01-03 22:23:43,112: t15.2024.07.28 val PER: 0.2265
2026-01-03 22:23:43,112: t15.2025.01.10 val PER: 0.4105
2026-01-03 22:23:43,112: t15.2025.01.12 val PER: 0.2733
2026-01-03 22:23:43,112: t15.2025.03.14 val PER: 0.4127
2026-01-03 22:23:43,112: t15.2025.03.16 val PER: 0.2853
2026-01-03 22:23:43,112: t15.2025.03.30 val PER: 0.3977
2026-01-03 22:23:43,112: t15.2025.04.13 val PER: 0.2924
2026-01-03 22:23:43,114: New best val WER(1gram) 64.21% --> 61.93%
2026-01-03 22:23:43,114: Checkpointing model
2026-01-03 22:23:43,707: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:23:43,953: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_4500
2026-01-03 22:23:52,631: Train batch 4600: loss: 20.76 grad norm: 68.72 time: 0.063
2026-01-03 22:24:10,178: Train batch 4800: loss: 13.66 grad norm: 50.76 time: 0.063
2026-01-03 22:24:27,170: Train batch 5000: loss: 32.19 grad norm: 85.47 time: 0.064
2026-01-03 22:24:27,171: Running test after training batch: 5000
2026-01-03 22:24:27,293: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:24:32,030: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-03 22:24:32,059: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cost nett
2026-01-03 22:24:33,680: Val batch 5000: PER (avg): 0.2281 CTC Loss (avg): 22.2048 WER(1gram): 61.42% (n=64) time: 6.509
2026-01-03 22:24:33,681: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-03 22:24:33,681: t15.2023.08.13 val PER: 0.1954
2026-01-03 22:24:33,681: t15.2023.08.18 val PER: 0.1794
2026-01-03 22:24:33,681: t15.2023.08.20 val PER: 0.1708
2026-01-03 22:24:33,681: t15.2023.08.25 val PER: 0.1235
2026-01-03 22:24:33,681: t15.2023.08.27 val PER: 0.2379
2026-01-03 22:24:33,681: t15.2023.09.01 val PER: 0.1356
2026-01-03 22:24:33,681: t15.2023.09.03 val PER: 0.2363
2026-01-03 22:24:33,681: t15.2023.09.24 val PER: 0.1893
2026-01-03 22:24:33,682: t15.2023.09.29 val PER: 0.1812
2026-01-03 22:24:33,682: t15.2023.10.01 val PER: 0.2450
2026-01-03 22:24:33,682: t15.2023.10.06 val PER: 0.1475
2026-01-03 22:24:33,682: t15.2023.10.08 val PER: 0.3126
2026-01-03 22:24:33,682: t15.2023.10.13 val PER: 0.2878
2026-01-03 22:24:33,682: t15.2023.10.15 val PER: 0.2261
2026-01-03 22:24:33,682: t15.2023.10.20 val PER: 0.2282
2026-01-03 22:24:33,682: t15.2023.10.22 val PER: 0.1759
2026-01-03 22:24:33,682: t15.2023.11.03 val PER: 0.2205
2026-01-03 22:24:33,682: t15.2023.11.04 val PER: 0.0683
2026-01-03 22:24:33,682: t15.2023.11.17 val PER: 0.0871
2026-01-03 22:24:33,682: t15.2023.11.19 val PER: 0.0818
2026-01-03 22:24:33,682: t15.2023.11.26 val PER: 0.2471
2026-01-03 22:24:33,683: t15.2023.12.03 val PER: 0.2069
2026-01-03 22:24:33,683: t15.2023.12.08 val PER: 0.1977
2026-01-03 22:24:33,683: t15.2023.12.10 val PER: 0.1643
2026-01-03 22:24:33,683: t15.2023.12.17 val PER: 0.2277
2026-01-03 22:24:33,683: t15.2023.12.29 val PER: 0.2279
2026-01-03 22:24:33,684: t15.2024.02.25 val PER: 0.1938
2026-01-03 22:24:33,684: t15.2024.03.08 val PER: 0.3215
2026-01-03 22:24:33,684: t15.2024.03.15 val PER: 0.2914
2026-01-03 22:24:33,684: t15.2024.03.17 val PER: 0.2434
2026-01-03 22:24:33,684: t15.2024.05.10 val PER: 0.2377
2026-01-03 22:24:33,684: t15.2024.06.14 val PER: 0.2476
2026-01-03 22:24:33,684: t15.2024.07.19 val PER: 0.3322
2026-01-03 22:24:33,684: t15.2024.07.21 val PER: 0.1779
2026-01-03 22:24:33,684: t15.2024.07.28 val PER: 0.2059
2026-01-03 22:24:33,685: t15.2025.01.10 val PER: 0.3967
2026-01-03 22:24:33,685: t15.2025.01.12 val PER: 0.2386
2026-01-03 22:24:33,685: t15.2025.03.14 val PER: 0.3935
2026-01-03 22:24:33,685: t15.2025.03.16 val PER: 0.2552
2026-01-03 22:24:33,685: t15.2025.03.30 val PER: 0.3908
2026-01-03 22:24:33,685: t15.2025.04.13 val PER: 0.3110
2026-01-03 22:24:33,685: New best val WER(1gram) 61.93% --> 61.42%
2026-01-03 22:24:33,686: Checkpointing model
2026-01-03 22:24:34,308: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:24:34,552: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_5000
2026-01-03 22:24:51,442: Train batch 5200: loss: 16.21 grad norm: 56.48 time: 0.051
2026-01-03 22:25:08,414: Train batch 5400: loss: 17.47 grad norm: 59.09 time: 0.068
2026-01-03 22:25:16,942: Running test after training batch: 5500
2026-01-03 22:25:17,040: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:25:22,083: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point will
2026-01-03 22:25:22,112: WER debug example
  GT : how does it keep the cost down
  PR : aue dust it keep the cost tet
2026-01-03 22:25:23,670: Val batch 5500: PER (avg): 0.2167 CTC Loss (avg): 21.1713 WER(1gram): 57.87% (n=64) time: 6.728
2026-01-03 22:25:23,670: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-03 22:25:23,671: t15.2023.08.13 val PER: 0.2037
2026-01-03 22:25:23,671: t15.2023.08.18 val PER: 0.1609
2026-01-03 22:25:23,671: t15.2023.08.20 val PER: 0.1612
2026-01-03 22:25:23,671: t15.2023.08.25 val PER: 0.1401
2026-01-03 22:25:23,671: t15.2023.08.27 val PER: 0.2283
2026-01-03 22:25:23,671: t15.2023.09.01 val PER: 0.1372
2026-01-03 22:25:23,671: t15.2023.09.03 val PER: 0.2245
2026-01-03 22:25:23,672: t15.2023.09.24 val PER: 0.1772
2026-01-03 22:25:23,672: t15.2023.09.29 val PER: 0.1736
2026-01-03 22:25:23,672: t15.2023.10.01 val PER: 0.2424
2026-01-03 22:25:23,672: t15.2023.10.06 val PER: 0.1292
2026-01-03 22:25:23,672: t15.2023.10.08 val PER: 0.2977
2026-01-03 22:25:23,672: t15.2023.10.13 val PER: 0.2762
2026-01-03 22:25:23,672: t15.2023.10.15 val PER: 0.2076
2026-01-03 22:25:23,672: t15.2023.10.20 val PER: 0.2215
2026-01-03 22:25:23,673: t15.2023.10.22 val PER: 0.1615
2026-01-03 22:25:23,673: t15.2023.11.03 val PER: 0.2266
2026-01-03 22:25:23,673: t15.2023.11.04 val PER: 0.0785
2026-01-03 22:25:23,673: t15.2023.11.17 val PER: 0.0747
2026-01-03 22:25:23,673: t15.2023.11.19 val PER: 0.0639
2026-01-03 22:25:23,673: t15.2023.11.26 val PER: 0.2123
2026-01-03 22:25:23,673: t15.2023.12.03 val PER: 0.1954
2026-01-03 22:25:23,673: t15.2023.12.08 val PER: 0.1891
2026-01-03 22:25:23,673: t15.2023.12.10 val PER: 0.1537
2026-01-03 22:25:23,673: t15.2023.12.17 val PER: 0.2245
2026-01-03 22:25:23,674: t15.2023.12.29 val PER: 0.2169
2026-01-03 22:25:23,674: t15.2024.02.25 val PER: 0.1657
2026-01-03 22:25:23,674: t15.2024.03.08 val PER: 0.2987
2026-01-03 22:25:23,674: t15.2024.03.15 val PER: 0.2702
2026-01-03 22:25:23,674: t15.2024.03.17 val PER: 0.2176
2026-01-03 22:25:23,674: t15.2024.05.10 val PER: 0.2318
2026-01-03 22:25:23,674: t15.2024.06.14 val PER: 0.2271
2026-01-03 22:25:23,674: t15.2024.07.19 val PER: 0.3217
2026-01-03 22:25:23,674: t15.2024.07.21 val PER: 0.1566
2026-01-03 22:25:23,674: t15.2024.07.28 val PER: 0.2118
2026-01-03 22:25:23,675: t15.2025.01.10 val PER: 0.3857
2026-01-03 22:25:23,675: t15.2025.01.12 val PER: 0.2340
2026-01-03 22:25:23,675: t15.2025.03.14 val PER: 0.3624
2026-01-03 22:25:23,675: t15.2025.03.16 val PER: 0.2592
2026-01-03 22:25:23,675: t15.2025.03.30 val PER: 0.3655
2026-01-03 22:25:23,675: t15.2025.04.13 val PER: 0.3010
2026-01-03 22:25:23,675: New best val WER(1gram) 61.42% --> 57.87%
2026-01-03 22:25:23,675: Checkpointing model
2026-01-03 22:25:24,268: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:25:24,511: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_5500
2026-01-03 22:25:33,092: Train batch 5600: loss: 19.58 grad norm: 63.48 time: 0.061
2026-01-03 22:25:50,196: Train batch 5800: loss: 13.89 grad norm: 56.82 time: 0.082
2026-01-03 22:26:07,134: Train batch 6000: loss: 14.70 grad norm: 56.57 time: 0.048
2026-01-03 22:26:07,134: Running test after training batch: 6000
2026-01-03 22:26:07,241: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:26:12,184: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the good at this point is will
2026-01-03 22:26:12,213: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 22:26:13,857: Val batch 6000: PER (avg): 0.2139 CTC Loss (avg): 21.0891 WER(1gram): 59.39% (n=64) time: 6.723
2026-01-03 22:26:13,858: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 22:26:13,858: t15.2023.08.13 val PER: 0.1871
2026-01-03 22:26:13,858: t15.2023.08.18 val PER: 0.1668
2026-01-03 22:26:13,858: t15.2023.08.20 val PER: 0.1668
2026-01-03 22:26:13,858: t15.2023.08.25 val PER: 0.1310
2026-01-03 22:26:13,858: t15.2023.08.27 val PER: 0.2444
2026-01-03 22:26:13,858: t15.2023.09.01 val PER: 0.1347
2026-01-03 22:26:13,858: t15.2023.09.03 val PER: 0.2114
2026-01-03 22:26:13,858: t15.2023.09.24 val PER: 0.1650
2026-01-03 22:26:13,858: t15.2023.09.29 val PER: 0.1723
2026-01-03 22:26:13,859: t15.2023.10.01 val PER: 0.2272
2026-01-03 22:26:13,859: t15.2023.10.06 val PER: 0.1270
2026-01-03 22:26:13,859: t15.2023.10.08 val PER: 0.2963
2026-01-03 22:26:13,859: t15.2023.10.13 val PER: 0.2669
2026-01-03 22:26:13,859: t15.2023.10.15 val PER: 0.2116
2026-01-03 22:26:13,859: t15.2023.10.20 val PER: 0.2282
2026-01-03 22:26:13,859: t15.2023.10.22 val PER: 0.1726
2026-01-03 22:26:13,859: t15.2023.11.03 val PER: 0.2361
2026-01-03 22:26:13,859: t15.2023.11.04 val PER: 0.0512
2026-01-03 22:26:13,859: t15.2023.11.17 val PER: 0.0747
2026-01-03 22:26:13,859: t15.2023.11.19 val PER: 0.0798
2026-01-03 22:26:13,859: t15.2023.11.26 val PER: 0.2261
2026-01-03 22:26:13,859: t15.2023.12.03 val PER: 0.1786
2026-01-03 22:26:13,860: t15.2023.12.08 val PER: 0.1811
2026-01-03 22:26:13,860: t15.2023.12.10 val PER: 0.1551
2026-01-03 22:26:13,860: t15.2023.12.17 val PER: 0.2058
2026-01-03 22:26:13,860: t15.2023.12.29 val PER: 0.2203
2026-01-03 22:26:13,860: t15.2024.02.25 val PER: 0.1629
2026-01-03 22:26:13,860: t15.2024.03.08 val PER: 0.2930
2026-01-03 22:26:13,860: t15.2024.03.15 val PER: 0.2652
2026-01-03 22:26:13,860: t15.2024.03.17 val PER: 0.2071
2026-01-03 22:26:13,860: t15.2024.05.10 val PER: 0.2303
2026-01-03 22:26:13,860: t15.2024.06.14 val PER: 0.2161
2026-01-03 22:26:13,860: t15.2024.07.19 val PER: 0.3072
2026-01-03 22:26:13,860: t15.2024.07.21 val PER: 0.1676
2026-01-03 22:26:13,860: t15.2024.07.28 val PER: 0.2059
2026-01-03 22:26:13,860: t15.2025.01.10 val PER: 0.3760
2026-01-03 22:26:13,860: t15.2025.01.12 val PER: 0.2317
2026-01-03 22:26:13,860: t15.2025.03.14 val PER: 0.3772
2026-01-03 22:26:13,860: t15.2025.03.16 val PER: 0.2644
2026-01-03 22:26:13,861: t15.2025.03.30 val PER: 0.3690
2026-01-03 22:26:13,861: t15.2025.04.13 val PER: 0.2682
2026-01-03 22:26:14,096: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_6000
2026-01-03 22:26:31,157: Train batch 6200: loss: 16.61 grad norm: 57.88 time: 0.070
2026-01-03 22:26:47,861: Train batch 6400: loss: 18.96 grad norm: 64.20 time: 0.062
2026-01-03 22:26:56,083: Running test after training batch: 6500
2026-01-03 22:26:56,215: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:27:00,932: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 22:27:00,961: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 22:27:02,558: Val batch 6500: PER (avg): 0.2042 CTC Loss (avg): 20.2049 WER(1gram): 53.30% (n=64) time: 6.475
2026-01-03 22:27:02,559: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 22:27:02,559: t15.2023.08.13 val PER: 0.1757
2026-01-03 22:27:02,559: t15.2023.08.18 val PER: 0.1534
2026-01-03 22:27:02,559: t15.2023.08.20 val PER: 0.1557
2026-01-03 22:27:02,559: t15.2023.08.25 val PER: 0.1114
2026-01-03 22:27:02,559: t15.2023.08.27 val PER: 0.2283
2026-01-03 22:27:02,559: t15.2023.09.01 val PER: 0.1136
2026-01-03 22:27:02,559: t15.2023.09.03 val PER: 0.2019
2026-01-03 22:27:02,559: t15.2023.09.24 val PER: 0.1772
2026-01-03 22:27:02,559: t15.2023.09.29 val PER: 0.1672
2026-01-03 22:27:02,559: t15.2023.10.01 val PER: 0.2193
2026-01-03 22:27:02,559: t15.2023.10.06 val PER: 0.1259
2026-01-03 22:27:02,560: t15.2023.10.08 val PER: 0.2977
2026-01-03 22:27:02,560: t15.2023.10.13 val PER: 0.2630
2026-01-03 22:27:02,560: t15.2023.10.15 val PER: 0.2096
2026-01-03 22:27:02,560: t15.2023.10.20 val PER: 0.2248
2026-01-03 22:27:02,560: t15.2023.10.22 val PER: 0.1570
2026-01-03 22:27:02,560: t15.2023.11.03 val PER: 0.2171
2026-01-03 22:27:02,560: t15.2023.11.04 val PER: 0.0512
2026-01-03 22:27:02,560: t15.2023.11.17 val PER: 0.0638
2026-01-03 22:27:02,560: t15.2023.11.19 val PER: 0.0699
2026-01-03 22:27:02,560: t15.2023.11.26 val PER: 0.2145
2026-01-03 22:27:02,560: t15.2023.12.03 val PER: 0.1733
2026-01-03 22:27:02,560: t15.2023.12.08 val PER: 0.1738
2026-01-03 22:27:02,560: t15.2023.12.10 val PER: 0.1459
2026-01-03 22:27:02,560: t15.2023.12.17 val PER: 0.1965
2026-01-03 22:27:02,561: t15.2023.12.29 val PER: 0.2093
2026-01-03 22:27:02,561: t15.2024.02.25 val PER: 0.1615
2026-01-03 22:27:02,561: t15.2024.03.08 val PER: 0.2788
2026-01-03 22:27:02,561: t15.2024.03.15 val PER: 0.2595
2026-01-03 22:27:02,561: t15.2024.03.17 val PER: 0.2043
2026-01-03 22:27:02,561: t15.2024.05.10 val PER: 0.2140
2026-01-03 22:27:02,561: t15.2024.06.14 val PER: 0.2035
2026-01-03 22:27:02,561: t15.2024.07.19 val PER: 0.3019
2026-01-03 22:27:02,561: t15.2024.07.21 val PER: 0.1428
2026-01-03 22:27:02,561: t15.2024.07.28 val PER: 0.1846
2026-01-03 22:27:02,561: t15.2025.01.10 val PER: 0.3843
2026-01-03 22:27:02,561: t15.2025.01.12 val PER: 0.2117
2026-01-03 22:27:02,561: t15.2025.03.14 val PER: 0.3831
2026-01-03 22:27:02,561: t15.2025.03.16 val PER: 0.2421
2026-01-03 22:27:02,561: t15.2025.03.30 val PER: 0.3552
2026-01-03 22:27:02,561: t15.2025.04.13 val PER: 0.2639
2026-01-03 22:27:02,563: New best val WER(1gram) 57.87% --> 53.30%
2026-01-03 22:27:02,563: Checkpointing model
2026-01-03 22:27:03,188: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:27:03,431: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_6500
2026-01-03 22:27:11,829: Train batch 6600: loss: 12.51 grad norm: 54.96 time: 0.044
2026-01-03 22:27:29,866: Train batch 6800: loss: 15.64 grad norm: 55.81 time: 0.048
2026-01-03 22:27:48,422: Train batch 7000: loss: 17.12 grad norm: 65.56 time: 0.061
2026-01-03 22:27:48,422: Running test after training batch: 7000
2026-01-03 22:27:48,583: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:27:53,440: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-03 22:27:53,470: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost nett
2026-01-03 22:27:55,136: Val batch 7000: PER (avg): 0.1947 CTC Loss (avg): 19.1935 WER(1gram): 55.84% (n=64) time: 6.714
2026-01-03 22:27:55,137: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-03 22:27:55,137: t15.2023.08.13 val PER: 0.1538
2026-01-03 22:27:55,137: t15.2023.08.18 val PER: 0.1433
2026-01-03 22:27:55,137: t15.2023.08.20 val PER: 0.1541
2026-01-03 22:27:55,137: t15.2023.08.25 val PER: 0.0964
2026-01-03 22:27:55,137: t15.2023.08.27 val PER: 0.2235
2026-01-03 22:27:55,137: t15.2023.09.01 val PER: 0.1047
2026-01-03 22:27:55,137: t15.2023.09.03 val PER: 0.1888
2026-01-03 22:27:55,137: t15.2023.09.24 val PER: 0.1566
2026-01-03 22:27:55,137: t15.2023.09.29 val PER: 0.1691
2026-01-03 22:27:55,137: t15.2023.10.01 val PER: 0.2107
2026-01-03 22:27:55,137: t15.2023.10.06 val PER: 0.1119
2026-01-03 22:27:55,137: t15.2023.10.08 val PER: 0.2706
2026-01-03 22:27:55,138: t15.2023.10.13 val PER: 0.2545
2026-01-03 22:27:55,138: t15.2023.10.15 val PER: 0.1964
2026-01-03 22:27:55,138: t15.2023.10.20 val PER: 0.2181
2026-01-03 22:27:55,138: t15.2023.10.22 val PER: 0.1459
2026-01-03 22:27:55,138: t15.2023.11.03 val PER: 0.1961
2026-01-03 22:27:55,138: t15.2023.11.04 val PER: 0.0444
2026-01-03 22:27:55,138: t15.2023.11.17 val PER: 0.0607
2026-01-03 22:27:55,138: t15.2023.11.19 val PER: 0.0519
2026-01-03 22:27:55,138: t15.2023.11.26 val PER: 0.1993
2026-01-03 22:27:55,138: t15.2023.12.03 val PER: 0.1586
2026-01-03 22:27:55,138: t15.2023.12.08 val PER: 0.1618
2026-01-03 22:27:55,138: t15.2023.12.10 val PER: 0.1432
2026-01-03 22:27:55,138: t15.2023.12.17 val PER: 0.1913
2026-01-03 22:27:55,138: t15.2023.12.29 val PER: 0.1901
2026-01-03 22:27:55,138: t15.2024.02.25 val PER: 0.1559
2026-01-03 22:27:55,138: t15.2024.03.08 val PER: 0.2831
2026-01-03 22:27:55,139: t15.2024.03.15 val PER: 0.2464
2026-01-03 22:27:55,139: t15.2024.03.17 val PER: 0.1974
2026-01-03 22:27:55,139: t15.2024.05.10 val PER: 0.2065
2026-01-03 22:27:55,139: t15.2024.06.14 val PER: 0.2145
2026-01-03 22:27:55,139: t15.2024.07.19 val PER: 0.2993
2026-01-03 22:27:55,139: t15.2024.07.21 val PER: 0.1297
2026-01-03 22:27:55,139: t15.2024.07.28 val PER: 0.1801
2026-01-03 22:27:55,139: t15.2025.01.10 val PER: 0.3691
2026-01-03 22:27:55,139: t15.2025.01.12 val PER: 0.2063
2026-01-03 22:27:55,139: t15.2025.03.14 val PER: 0.3639
2026-01-03 22:27:55,139: t15.2025.03.16 val PER: 0.2317
2026-01-03 22:27:55,139: t15.2025.03.30 val PER: 0.3621
2026-01-03 22:27:55,140: t15.2025.04.13 val PER: 0.2696
2026-01-03 22:27:55,372: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_7000
2026-01-03 22:28:12,839: Train batch 7200: loss: 14.18 grad norm: 54.51 time: 0.078
2026-01-03 22:28:29,615: Train batch 7400: loss: 14.00 grad norm: 55.45 time: 0.075
2026-01-03 22:28:38,002: Running test after training batch: 7500
2026-01-03 22:28:38,117: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:28:42,831: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 22:28:42,862: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-03 22:28:44,527: Val batch 7500: PER (avg): 0.1908 CTC Loss (avg): 18.6216 WER(1gram): 54.57% (n=64) time: 6.525
2026-01-03 22:28:44,528: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 22:28:44,528: t15.2023.08.13 val PER: 0.1528
2026-01-03 22:28:44,528: t15.2023.08.18 val PER: 0.1408
2026-01-03 22:28:44,528: t15.2023.08.20 val PER: 0.1454
2026-01-03 22:28:44,528: t15.2023.08.25 val PER: 0.1099
2026-01-03 22:28:44,528: t15.2023.08.27 val PER: 0.2170
2026-01-03 22:28:44,528: t15.2023.09.01 val PER: 0.1120
2026-01-03 22:28:44,529: t15.2023.09.03 val PER: 0.1924
2026-01-03 22:28:44,529: t15.2023.09.24 val PER: 0.1468
2026-01-03 22:28:44,529: t15.2023.09.29 val PER: 0.1615
2026-01-03 22:28:44,529: t15.2023.10.01 val PER: 0.2061
2026-01-03 22:28:44,529: t15.2023.10.06 val PER: 0.1130
2026-01-03 22:28:44,529: t15.2023.10.08 val PER: 0.2733
2026-01-03 22:28:44,529: t15.2023.10.13 val PER: 0.2459
2026-01-03 22:28:44,529: t15.2023.10.15 val PER: 0.1905
2026-01-03 22:28:44,529: t15.2023.10.20 val PER: 0.2148
2026-01-03 22:28:44,529: t15.2023.10.22 val PER: 0.1392
2026-01-03 22:28:44,529: t15.2023.11.03 val PER: 0.2083
2026-01-03 22:28:44,529: t15.2023.11.04 val PER: 0.0546
2026-01-03 22:28:44,529: t15.2023.11.17 val PER: 0.0607
2026-01-03 22:28:44,530: t15.2023.11.19 val PER: 0.0559
2026-01-03 22:28:44,530: t15.2023.11.26 val PER: 0.1935
2026-01-03 22:28:44,530: t15.2023.12.03 val PER: 0.1544
2026-01-03 22:28:44,530: t15.2023.12.08 val PER: 0.1531
2026-01-03 22:28:44,530: t15.2023.12.10 val PER: 0.1301
2026-01-03 22:28:44,530: t15.2023.12.17 val PER: 0.1819
2026-01-03 22:28:44,530: t15.2023.12.29 val PER: 0.1826
2026-01-03 22:28:44,530: t15.2024.02.25 val PER: 0.1573
2026-01-03 22:28:44,530: t15.2024.03.08 val PER: 0.2731
2026-01-03 22:28:44,530: t15.2024.03.15 val PER: 0.2445
2026-01-03 22:28:44,530: t15.2024.03.17 val PER: 0.1890
2026-01-03 22:28:44,530: t15.2024.05.10 val PER: 0.1976
2026-01-03 22:28:44,530: t15.2024.06.14 val PER: 0.2035
2026-01-03 22:28:44,531: t15.2024.07.19 val PER: 0.2920
2026-01-03 22:28:44,531: t15.2024.07.21 val PER: 0.1324
2026-01-03 22:28:44,531: t15.2024.07.28 val PER: 0.1691
2026-01-03 22:28:44,531: t15.2025.01.10 val PER: 0.3567
2026-01-03 22:28:44,531: t15.2025.01.12 val PER: 0.1986
2026-01-03 22:28:44,531: t15.2025.03.14 val PER: 0.3772
2026-01-03 22:28:44,531: t15.2025.03.16 val PER: 0.2317
2026-01-03 22:28:44,531: t15.2025.03.30 val PER: 0.3529
2026-01-03 22:28:44,531: t15.2025.04.13 val PER: 0.2568
2026-01-03 22:28:44,763: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_7500
2026-01-03 22:28:53,772: Train batch 7600: loss: 16.00 grad norm: 61.45 time: 0.069
2026-01-03 22:29:11,762: Train batch 7800: loss: 14.55 grad norm: 58.86 time: 0.055
2026-01-03 22:29:29,695: Train batch 8000: loss: 11.30 grad norm: 49.63 time: 0.071
2026-01-03 22:29:29,695: Running test after training batch: 8000
2026-01-03 22:29:29,795: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:29:34,494: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 22:29:34,526: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nit
2026-01-03 22:29:36,199: Val batch 8000: PER (avg): 0.1865 CTC Loss (avg): 18.3498 WER(1gram): 53.55% (n=64) time: 6.503
2026-01-03 22:29:36,199: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 22:29:36,200: t15.2023.08.13 val PER: 0.1559
2026-01-03 22:29:36,200: t15.2023.08.18 val PER: 0.1282
2026-01-03 22:29:36,200: t15.2023.08.20 val PER: 0.1366
2026-01-03 22:29:36,200: t15.2023.08.25 val PER: 0.1130
2026-01-03 22:29:36,200: t15.2023.08.27 val PER: 0.2154
2026-01-03 22:29:36,200: t15.2023.09.01 val PER: 0.0942
2026-01-03 22:29:36,200: t15.2023.09.03 val PER: 0.1817
2026-01-03 22:29:36,200: t15.2023.09.24 val PER: 0.1541
2026-01-03 22:29:36,200: t15.2023.09.29 val PER: 0.1500
2026-01-03 22:29:36,200: t15.2023.10.01 val PER: 0.1975
2026-01-03 22:29:36,200: t15.2023.10.06 val PER: 0.1098
2026-01-03 22:29:36,200: t15.2023.10.08 val PER: 0.2842
2026-01-03 22:29:36,200: t15.2023.10.13 val PER: 0.2389
2026-01-03 22:29:36,201: t15.2023.10.15 val PER: 0.1918
2026-01-03 22:29:36,201: t15.2023.10.20 val PER: 0.2047
2026-01-03 22:29:36,201: t15.2023.10.22 val PER: 0.1425
2026-01-03 22:29:36,201: t15.2023.11.03 val PER: 0.2123
2026-01-03 22:29:36,201: t15.2023.11.04 val PER: 0.0375
2026-01-03 22:29:36,201: t15.2023.11.17 val PER: 0.0575
2026-01-03 22:29:36,201: t15.2023.11.19 val PER: 0.0659
2026-01-03 22:29:36,201: t15.2023.11.26 val PER: 0.1790
2026-01-03 22:29:36,201: t15.2023.12.03 val PER: 0.1681
2026-01-03 22:29:36,201: t15.2023.12.08 val PER: 0.1458
2026-01-03 22:29:36,201: t15.2023.12.10 val PER: 0.1275
2026-01-03 22:29:36,202: t15.2023.12.17 val PER: 0.1798
2026-01-03 22:29:36,202: t15.2023.12.29 val PER: 0.1812
2026-01-03 22:29:36,202: t15.2024.02.25 val PER: 0.1447
2026-01-03 22:29:36,202: t15.2024.03.08 val PER: 0.2617
2026-01-03 22:29:36,203: t15.2024.03.15 val PER: 0.2402
2026-01-03 22:29:36,203: t15.2024.03.17 val PER: 0.1834
2026-01-03 22:29:36,203: t15.2024.05.10 val PER: 0.1902
2026-01-03 22:29:36,204: t15.2024.06.14 val PER: 0.2003
2026-01-03 22:29:36,204: t15.2024.07.19 val PER: 0.2900
2026-01-03 22:29:36,204: t15.2024.07.21 val PER: 0.1248
2026-01-03 22:29:36,204: t15.2024.07.28 val PER: 0.1669
2026-01-03 22:29:36,204: t15.2025.01.10 val PER: 0.3388
2026-01-03 22:29:36,204: t15.2025.01.12 val PER: 0.1994
2026-01-03 22:29:36,204: t15.2025.03.14 val PER: 0.3506
2026-01-03 22:29:36,204: t15.2025.03.16 val PER: 0.2369
2026-01-03 22:29:36,205: t15.2025.03.30 val PER: 0.3448
2026-01-03 22:29:36,205: t15.2025.04.13 val PER: 0.2710
2026-01-03 22:29:36,442: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_8000
2026-01-03 22:29:53,796: Train batch 8200: loss: 9.79 grad norm: 46.68 time: 0.054
2026-01-03 22:30:10,937: Train batch 8400: loss: 10.16 grad norm: 46.48 time: 0.064
2026-01-03 22:30:19,531: Running test after training batch: 8500
2026-01-03 22:30:19,637: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:30:24,554: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 22:30:24,584: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-03 22:30:26,244: Val batch 8500: PER (avg): 0.1806 CTC Loss (avg): 17.8125 WER(1gram): 50.76% (n=64) time: 6.712
2026-01-03 22:30:26,244: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 22:30:26,244: t15.2023.08.13 val PER: 0.1507
2026-01-03 22:30:26,244: t15.2023.08.18 val PER: 0.1366
2026-01-03 22:30:26,244: t15.2023.08.20 val PER: 0.1334
2026-01-03 22:30:26,245: t15.2023.08.25 val PER: 0.1145
2026-01-03 22:30:26,245: t15.2023.08.27 val PER: 0.2122
2026-01-03 22:30:26,245: t15.2023.09.01 val PER: 0.0974
2026-01-03 22:30:26,245: t15.2023.09.03 val PER: 0.1995
2026-01-03 22:30:26,245: t15.2023.09.24 val PER: 0.1468
2026-01-03 22:30:26,245: t15.2023.09.29 val PER: 0.1538
2026-01-03 22:30:26,245: t15.2023.10.01 val PER: 0.1988
2026-01-03 22:30:26,245: t15.2023.10.06 val PER: 0.1012
2026-01-03 22:30:26,245: t15.2023.10.08 val PER: 0.2612
2026-01-03 22:30:26,245: t15.2023.10.13 val PER: 0.2397
2026-01-03 22:30:26,245: t15.2023.10.15 val PER: 0.1826
2026-01-03 22:30:26,245: t15.2023.10.20 val PER: 0.1980
2026-01-03 22:30:26,245: t15.2023.10.22 val PER: 0.1403
2026-01-03 22:30:26,245: t15.2023.11.03 val PER: 0.1920
2026-01-03 22:30:26,245: t15.2023.11.04 val PER: 0.0375
2026-01-03 22:30:26,246: t15.2023.11.17 val PER: 0.0575
2026-01-03 22:30:26,246: t15.2023.11.19 val PER: 0.0439
2026-01-03 22:30:26,246: t15.2023.11.26 val PER: 0.1703
2026-01-03 22:30:26,246: t15.2023.12.03 val PER: 0.1408
2026-01-03 22:30:26,246: t15.2023.12.08 val PER: 0.1451
2026-01-03 22:30:26,246: t15.2023.12.10 val PER: 0.1209
2026-01-03 22:30:26,247: t15.2023.12.17 val PER: 0.1726
2026-01-03 22:30:26,247: t15.2023.12.29 val PER: 0.1702
2026-01-03 22:30:26,247: t15.2024.02.25 val PER: 0.1447
2026-01-03 22:30:26,247: t15.2024.03.08 val PER: 0.2518
2026-01-03 22:30:26,247: t15.2024.03.15 val PER: 0.2326
2026-01-03 22:30:26,247: t15.2024.03.17 val PER: 0.1722
2026-01-03 22:30:26,247: t15.2024.05.10 val PER: 0.1947
2026-01-03 22:30:26,247: t15.2024.06.14 val PER: 0.1861
2026-01-03 22:30:26,247: t15.2024.07.19 val PER: 0.2736
2026-01-03 22:30:26,247: t15.2024.07.21 val PER: 0.1234
2026-01-03 22:30:26,247: t15.2024.07.28 val PER: 0.1684
2026-01-03 22:30:26,247: t15.2025.01.10 val PER: 0.3292
2026-01-03 22:30:26,247: t15.2025.01.12 val PER: 0.1894
2026-01-03 22:30:26,247: t15.2025.03.14 val PER: 0.3728
2026-01-03 22:30:26,247: t15.2025.03.16 val PER: 0.2147
2026-01-03 22:30:26,247: t15.2025.03.30 val PER: 0.3402
2026-01-03 22:30:26,248: t15.2025.04.13 val PER: 0.2397
2026-01-03 22:30:26,248: New best val WER(1gram) 53.30% --> 50.76%
2026-01-03 22:30:26,248: Checkpointing model
2026-01-03 22:30:26,840: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:30:27,083: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_8500
2026-01-03 22:30:35,658: Train batch 8600: loss: 15.93 grad norm: 63.31 time: 0.053
2026-01-03 22:30:53,276: Train batch 8800: loss: 15.71 grad norm: 61.14 time: 0.059
2026-01-03 22:31:10,878: Train batch 9000: loss: 15.78 grad norm: 68.59 time: 0.071
2026-01-03 22:31:10,878: Running test after training batch: 9000
2026-01-03 22:31:10,996: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:31:15,719: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 22:31:15,749: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 22:31:17,447: Val batch 9000: PER (avg): 0.1748 CTC Loss (avg): 17.4048 WER(1gram): 51.02% (n=64) time: 6.569
2026-01-03 22:31:17,448: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 22:31:17,448: t15.2023.08.13 val PER: 0.1299
2026-01-03 22:31:17,448: t15.2023.08.18 val PER: 0.1291
2026-01-03 22:31:17,448: t15.2023.08.20 val PER: 0.1311
2026-01-03 22:31:17,448: t15.2023.08.25 val PER: 0.0964
2026-01-03 22:31:17,448: t15.2023.08.27 val PER: 0.2170
2026-01-03 22:31:17,448: t15.2023.09.01 val PER: 0.0893
2026-01-03 22:31:17,448: t15.2023.09.03 val PER: 0.1876
2026-01-03 22:31:17,448: t15.2023.09.24 val PER: 0.1420
2026-01-03 22:31:17,448: t15.2023.09.29 val PER: 0.1468
2026-01-03 22:31:17,448: t15.2023.10.01 val PER: 0.1968
2026-01-03 22:31:17,449: t15.2023.10.06 val PER: 0.0980
2026-01-03 22:31:17,449: t15.2023.10.08 val PER: 0.2544
2026-01-03 22:31:17,449: t15.2023.10.13 val PER: 0.2366
2026-01-03 22:31:17,449: t15.2023.10.15 val PER: 0.1721
2026-01-03 22:31:17,449: t15.2023.10.20 val PER: 0.2013
2026-01-03 22:31:17,449: t15.2023.10.22 val PER: 0.1281
2026-01-03 22:31:17,449: t15.2023.11.03 val PER: 0.2090
2026-01-03 22:31:17,449: t15.2023.11.04 val PER: 0.0375
2026-01-03 22:31:17,449: t15.2023.11.17 val PER: 0.0591
2026-01-03 22:31:17,450: t15.2023.11.19 val PER: 0.0459
2026-01-03 22:31:17,450: t15.2023.11.26 val PER: 0.1703
2026-01-03 22:31:17,450: t15.2023.12.03 val PER: 0.1397
2026-01-03 22:31:17,450: t15.2023.12.08 val PER: 0.1325
2026-01-03 22:31:17,450: t15.2023.12.10 val PER: 0.1117
2026-01-03 22:31:17,450: t15.2023.12.17 val PER: 0.1580
2026-01-03 22:31:17,450: t15.2023.12.29 val PER: 0.1633
2026-01-03 22:31:17,450: t15.2024.02.25 val PER: 0.1475
2026-01-03 22:31:17,450: t15.2024.03.08 val PER: 0.2376
2026-01-03 22:31:17,450: t15.2024.03.15 val PER: 0.2270
2026-01-03 22:31:17,450: t15.2024.03.17 val PER: 0.1757
2026-01-03 22:31:17,450: t15.2024.05.10 val PER: 0.1857
2026-01-03 22:31:17,450: t15.2024.06.14 val PER: 0.1814
2026-01-03 22:31:17,450: t15.2024.07.19 val PER: 0.2657
2026-01-03 22:31:17,451: t15.2024.07.21 val PER: 0.1110
2026-01-03 22:31:17,451: t15.2024.07.28 val PER: 0.1566
2026-01-03 22:31:17,451: t15.2025.01.10 val PER: 0.3237
2026-01-03 22:31:17,451: t15.2025.01.12 val PER: 0.1786
2026-01-03 22:31:17,451: t15.2025.03.14 val PER: 0.3550
2026-01-03 22:31:17,451: t15.2025.03.16 val PER: 0.2134
2026-01-03 22:31:17,451: t15.2025.03.30 val PER: 0.3345
2026-01-03 22:31:17,451: t15.2025.04.13 val PER: 0.2468
2026-01-03 22:31:17,684: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_9000
2026-01-03 22:31:35,830: Train batch 9200: loss: 10.91 grad norm: 48.12 time: 0.055
2026-01-03 22:31:53,531: Train batch 9400: loss: 7.80 grad norm: 45.81 time: 0.067
2026-01-03 22:32:02,503: Running test after training batch: 9500
2026-01-03 22:32:02,643: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:32:08,566: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 22:32:08,595: WER debug example
  GT : how does it keep the cost down
  PR : houde dusts it keep the cost get
2026-01-03 22:32:10,302: Val batch 9500: PER (avg): 0.1748 CTC Loss (avg): 17.2342 WER(1gram): 49.24% (n=64) time: 7.799
2026-01-03 22:32:10,303: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 22:32:10,303: t15.2023.08.13 val PER: 0.1320
2026-01-03 22:32:10,303: t15.2023.08.18 val PER: 0.1140
2026-01-03 22:32:10,303: t15.2023.08.20 val PER: 0.1326
2026-01-03 22:32:10,303: t15.2023.08.25 val PER: 0.0979
2026-01-03 22:32:10,303: t15.2023.08.27 val PER: 0.1994
2026-01-03 22:32:10,304: t15.2023.09.01 val PER: 0.0885
2026-01-03 22:32:10,304: t15.2023.09.03 val PER: 0.1675
2026-01-03 22:32:10,304: t15.2023.09.24 val PER: 0.1396
2026-01-03 22:32:10,304: t15.2023.09.29 val PER: 0.1487
2026-01-03 22:32:10,304: t15.2023.10.01 val PER: 0.1942
2026-01-03 22:32:10,304: t15.2023.10.06 val PER: 0.1087
2026-01-03 22:32:10,304: t15.2023.10.08 val PER: 0.2666
2026-01-03 22:32:10,304: t15.2023.10.13 val PER: 0.2250
2026-01-03 22:32:10,304: t15.2023.10.15 val PER: 0.1806
2026-01-03 22:32:10,304: t15.2023.10.20 val PER: 0.2081
2026-01-03 22:32:10,304: t15.2023.10.22 val PER: 0.1281
2026-01-03 22:32:10,304: t15.2023.11.03 val PER: 0.1988
2026-01-03 22:32:10,305: t15.2023.11.04 val PER: 0.0273
2026-01-03 22:32:10,305: t15.2023.11.17 val PER: 0.0591
2026-01-03 22:32:10,305: t15.2023.11.19 val PER: 0.0579
2026-01-03 22:32:10,305: t15.2023.11.26 val PER: 0.1638
2026-01-03 22:32:10,305: t15.2023.12.03 val PER: 0.1439
2026-01-03 22:32:10,305: t15.2023.12.08 val PER: 0.1378
2026-01-03 22:32:10,306: t15.2023.12.10 val PER: 0.1222
2026-01-03 22:32:10,306: t15.2023.12.17 val PER: 0.1632
2026-01-03 22:32:10,306: t15.2023.12.29 val PER: 0.1517
2026-01-03 22:32:10,306: t15.2024.02.25 val PER: 0.1348
2026-01-03 22:32:10,306: t15.2024.03.08 val PER: 0.2632
2026-01-03 22:32:10,306: t15.2024.03.15 val PER: 0.2351
2026-01-03 22:32:10,306: t15.2024.03.17 val PER: 0.1660
2026-01-03 22:32:10,307: t15.2024.05.10 val PER: 0.1961
2026-01-03 22:32:10,307: t15.2024.06.14 val PER: 0.1877
2026-01-03 22:32:10,307: t15.2024.07.19 val PER: 0.2755
2026-01-03 22:32:10,307: t15.2024.07.21 val PER: 0.1186
2026-01-03 22:32:10,307: t15.2024.07.28 val PER: 0.1581
2026-01-03 22:32:10,307: t15.2025.01.10 val PER: 0.3168
2026-01-03 22:32:10,307: t15.2025.01.12 val PER: 0.1809
2026-01-03 22:32:10,307: t15.2025.03.14 val PER: 0.3654
2026-01-03 22:32:10,308: t15.2025.03.16 val PER: 0.2081
2026-01-03 22:32:10,308: t15.2025.03.30 val PER: 0.3287
2026-01-03 22:32:10,308: t15.2025.04.13 val PER: 0.2411
2026-01-03 22:32:10,308: New best val WER(1gram) 50.76% --> 49.24%
2026-01-03 22:32:10,308: Checkpointing model
2026-01-03 22:32:10,944: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:32:11,189: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_9500
2026-01-03 22:32:19,696: Train batch 9600: loss: 8.79 grad norm: 49.02 time: 0.073
2026-01-03 22:32:37,261: Train batch 9800: loss: 12.79 grad norm: 59.56 time: 0.062
2026-01-03 22:32:54,964: Train batch 10000: loss: 5.42 grad norm: 34.72 time: 0.060
2026-01-03 22:32:54,965: Running test after training batch: 10000
2026-01-03 22:32:55,090: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:32:59,734: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 22:32:59,764: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sit
2026-01-03 22:33:01,449: Val batch 10000: PER (avg): 0.1701 CTC Loss (avg): 16.9704 WER(1gram): 52.03% (n=64) time: 6.484
2026-01-03 22:33:01,450: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 22:33:01,450: t15.2023.08.13 val PER: 0.1268
2026-01-03 22:33:01,450: t15.2023.08.18 val PER: 0.1241
2026-01-03 22:33:01,450: t15.2023.08.20 val PER: 0.1311
2026-01-03 22:33:01,450: t15.2023.08.25 val PER: 0.1039
2026-01-03 22:33:01,450: t15.2023.08.27 val PER: 0.2042
2026-01-03 22:33:01,450: t15.2023.09.01 val PER: 0.0893
2026-01-03 22:33:01,450: t15.2023.09.03 val PER: 0.1829
2026-01-03 22:33:01,450: t15.2023.09.24 val PER: 0.1396
2026-01-03 22:33:01,450: t15.2023.09.29 val PER: 0.1468
2026-01-03 22:33:01,451: t15.2023.10.01 val PER: 0.1843
2026-01-03 22:33:01,451: t15.2023.10.06 val PER: 0.1001
2026-01-03 22:33:01,451: t15.2023.10.08 val PER: 0.2449
2026-01-03 22:33:01,451: t15.2023.10.13 val PER: 0.2258
2026-01-03 22:33:01,455: t15.2023.10.15 val PER: 0.1734
2026-01-03 22:33:01,456: t15.2023.10.20 val PER: 0.1812
2026-01-03 22:33:01,456: t15.2023.10.22 val PER: 0.1314
2026-01-03 22:33:01,456: t15.2023.11.03 val PER: 0.1927
2026-01-03 22:33:01,456: t15.2023.11.04 val PER: 0.0341
2026-01-03 22:33:01,456: t15.2023.11.17 val PER: 0.0482
2026-01-03 22:33:01,456: t15.2023.11.19 val PER: 0.0459
2026-01-03 22:33:01,456: t15.2023.11.26 val PER: 0.1572
2026-01-03 22:33:01,456: t15.2023.12.03 val PER: 0.1376
2026-01-03 22:33:01,456: t15.2023.12.08 val PER: 0.1265
2026-01-03 22:33:01,456: t15.2023.12.10 val PER: 0.1222
2026-01-03 22:33:01,456: t15.2023.12.17 val PER: 0.1622
2026-01-03 22:33:01,456: t15.2023.12.29 val PER: 0.1531
2026-01-03 22:33:01,456: t15.2024.02.25 val PER: 0.1390
2026-01-03 22:33:01,457: t15.2024.03.08 val PER: 0.2361
2026-01-03 22:33:01,457: t15.2024.03.15 val PER: 0.2183
2026-01-03 22:33:01,457: t15.2024.03.17 val PER: 0.1562
2026-01-03 22:33:01,457: t15.2024.05.10 val PER: 0.1709
2026-01-03 22:33:01,457: t15.2024.06.14 val PER: 0.1861
2026-01-03 22:33:01,457: t15.2024.07.19 val PER: 0.2709
2026-01-03 22:33:01,457: t15.2024.07.21 val PER: 0.1214
2026-01-03 22:33:01,457: t15.2024.07.28 val PER: 0.1522
2026-01-03 22:33:01,457: t15.2025.01.10 val PER: 0.3058
2026-01-03 22:33:01,457: t15.2025.01.12 val PER: 0.1778
2026-01-03 22:33:01,457: t15.2025.03.14 val PER: 0.3491
2026-01-03 22:33:01,458: t15.2025.03.16 val PER: 0.2173
2026-01-03 22:33:01,458: t15.2025.03.30 val PER: 0.3230
2026-01-03 22:33:01,458: t15.2025.04.13 val PER: 0.2340
2026-01-03 22:33:02,080: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_10000
2026-01-03 22:33:19,613: Train batch 10200: loss: 6.74 grad norm: 39.96 time: 0.050
2026-01-03 22:33:37,522: Train batch 10400: loss: 9.47 grad norm: 59.02 time: 0.071
2026-01-03 22:33:46,189: Running test after training batch: 10500
2026-01-03 22:33:46,313: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:33:50,967: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 22:33:50,998: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-03 22:33:52,718: Val batch 10500: PER (avg): 0.1674 CTC Loss (avg): 16.7334 WER(1gram): 49.49% (n=64) time: 6.529
2026-01-03 22:33:52,719: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 22:33:52,719: t15.2023.08.13 val PER: 0.1362
2026-01-03 22:33:52,719: t15.2023.08.18 val PER: 0.1140
2026-01-03 22:33:52,719: t15.2023.08.20 val PER: 0.1231
2026-01-03 22:33:52,719: t15.2023.08.25 val PER: 0.1009
2026-01-03 22:33:52,719: t15.2023.08.27 val PER: 0.2026
2026-01-03 22:33:52,719: t15.2023.09.01 val PER: 0.0958
2026-01-03 22:33:52,719: t15.2023.09.03 val PER: 0.1710
2026-01-03 22:33:52,720: t15.2023.09.24 val PER: 0.1481
2026-01-03 22:33:52,720: t15.2023.09.29 val PER: 0.1538
2026-01-03 22:33:52,720: t15.2023.10.01 val PER: 0.1856
2026-01-03 22:33:52,720: t15.2023.10.06 val PER: 0.0926
2026-01-03 22:33:52,720: t15.2023.10.08 val PER: 0.2476
2026-01-03 22:33:52,720: t15.2023.10.13 val PER: 0.2149
2026-01-03 22:33:52,720: t15.2023.10.15 val PER: 0.1747
2026-01-03 22:33:52,720: t15.2023.10.20 val PER: 0.2081
2026-01-03 22:33:52,720: t15.2023.10.22 val PER: 0.1236
2026-01-03 22:33:52,720: t15.2023.11.03 val PER: 0.1913
2026-01-03 22:33:52,720: t15.2023.11.04 val PER: 0.0341
2026-01-03 22:33:52,720: t15.2023.11.17 val PER: 0.0529
2026-01-03 22:33:52,720: t15.2023.11.19 val PER: 0.0579
2026-01-03 22:33:52,720: t15.2023.11.26 val PER: 0.1384
2026-01-03 22:33:52,721: t15.2023.12.03 val PER: 0.1345
2026-01-03 22:33:52,721: t15.2023.12.08 val PER: 0.1218
2026-01-03 22:33:52,721: t15.2023.12.10 val PER: 0.0999
2026-01-03 22:33:52,721: t15.2023.12.17 val PER: 0.1559
2026-01-03 22:33:52,721: t15.2023.12.29 val PER: 0.1496
2026-01-03 22:33:52,721: t15.2024.02.25 val PER: 0.1334
2026-01-03 22:33:52,721: t15.2024.03.08 val PER: 0.2603
2026-01-03 22:33:52,721: t15.2024.03.15 val PER: 0.2233
2026-01-03 22:33:52,721: t15.2024.03.17 val PER: 0.1590
2026-01-03 22:33:52,721: t15.2024.05.10 val PER: 0.1783
2026-01-03 22:33:52,721: t15.2024.06.14 val PER: 0.1830
2026-01-03 22:33:52,721: t15.2024.07.19 val PER: 0.2624
2026-01-03 22:33:52,721: t15.2024.07.21 val PER: 0.1110
2026-01-03 22:33:52,721: t15.2024.07.28 val PER: 0.1375
2026-01-03 22:33:52,721: t15.2025.01.10 val PER: 0.3140
2026-01-03 22:33:52,721: t15.2025.01.12 val PER: 0.1763
2026-01-03 22:33:52,721: t15.2025.03.14 val PER: 0.3595
2026-01-03 22:33:52,722: t15.2025.03.16 val PER: 0.1950
2026-01-03 22:33:52,722: t15.2025.03.30 val PER: 0.3149
2026-01-03 22:33:52,722: t15.2025.04.13 val PER: 0.2183
2026-01-03 22:33:53,314: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_10500
2026-01-03 22:34:01,944: Train batch 10600: loss: 8.74 grad norm: 51.51 time: 0.072
2026-01-03 22:34:19,009: Train batch 10800: loss: 14.83 grad norm: 65.01 time: 0.064
2026-01-03 22:34:36,963: Train batch 11000: loss: 14.52 grad norm: 62.78 time: 0.057
2026-01-03 22:34:36,964: Running test after training batch: 11000
2026-01-03 22:34:37,067: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:34:41,770: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 22:34:41,803: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 22:34:43,574: Val batch 11000: PER (avg): 0.1646 CTC Loss (avg): 16.6721 WER(1gram): 48.73% (n=64) time: 6.610
2026-01-03 22:34:43,575: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 22:34:43,575: t15.2023.08.13 val PER: 0.1258
2026-01-03 22:34:43,575: t15.2023.08.18 val PER: 0.1190
2026-01-03 22:34:43,575: t15.2023.08.20 val PER: 0.1287
2026-01-03 22:34:43,575: t15.2023.08.25 val PER: 0.0949
2026-01-03 22:34:43,575: t15.2023.08.27 val PER: 0.1897
2026-01-03 22:34:43,575: t15.2023.09.01 val PER: 0.0869
2026-01-03 22:34:43,575: t15.2023.09.03 val PER: 0.1758
2026-01-03 22:34:43,575: t15.2023.09.24 val PER: 0.1541
2026-01-03 22:34:43,575: t15.2023.09.29 val PER: 0.1372
2026-01-03 22:34:43,576: t15.2023.10.01 val PER: 0.1843
2026-01-03 22:34:43,576: t15.2023.10.06 val PER: 0.0872
2026-01-03 22:34:43,576: t15.2023.10.08 val PER: 0.2517
2026-01-03 22:34:43,576: t15.2023.10.13 val PER: 0.2188
2026-01-03 22:34:43,576: t15.2023.10.15 val PER: 0.1740
2026-01-03 22:34:43,576: t15.2023.10.20 val PER: 0.1879
2026-01-03 22:34:43,576: t15.2023.10.22 val PER: 0.1169
2026-01-03 22:34:43,576: t15.2023.11.03 val PER: 0.1988
2026-01-03 22:34:43,576: t15.2023.11.04 val PER: 0.0341
2026-01-03 22:34:43,576: t15.2023.11.17 val PER: 0.0529
2026-01-03 22:34:43,576: t15.2023.11.19 val PER: 0.0399
2026-01-03 22:34:43,576: t15.2023.11.26 val PER: 0.1493
2026-01-03 22:34:43,576: t15.2023.12.03 val PER: 0.1271
2026-01-03 22:34:43,577: t15.2023.12.08 val PER: 0.1158
2026-01-03 22:34:43,577: t15.2023.12.10 val PER: 0.1104
2026-01-03 22:34:43,577: t15.2023.12.17 val PER: 0.1455
2026-01-03 22:34:43,577: t15.2023.12.29 val PER: 0.1414
2026-01-03 22:34:43,577: t15.2024.02.25 val PER: 0.1376
2026-01-03 22:34:43,577: t15.2024.03.08 val PER: 0.2361
2026-01-03 22:34:43,577: t15.2024.03.15 val PER: 0.2220
2026-01-03 22:34:43,577: t15.2024.03.17 val PER: 0.1534
2026-01-03 22:34:43,577: t15.2024.05.10 val PER: 0.1857
2026-01-03 22:34:43,577: t15.2024.06.14 val PER: 0.1735
2026-01-03 22:34:43,577: t15.2024.07.19 val PER: 0.2558
2026-01-03 22:34:43,577: t15.2024.07.21 val PER: 0.1076
2026-01-03 22:34:43,577: t15.2024.07.28 val PER: 0.1419
2026-01-03 22:34:43,577: t15.2025.01.10 val PER: 0.3196
2026-01-03 22:34:43,577: t15.2025.01.12 val PER: 0.1663
2026-01-03 22:34:43,577: t15.2025.03.14 val PER: 0.3550
2026-01-03 22:34:43,578: t15.2025.03.16 val PER: 0.1859
2026-01-03 22:34:43,578: t15.2025.03.30 val PER: 0.3069
2026-01-03 22:34:43,578: t15.2025.04.13 val PER: 0.2297
2026-01-03 22:34:43,579: New best val WER(1gram) 49.24% --> 48.73%
2026-01-03 22:34:43,579: Checkpointing model
2026-01-03 22:34:44,210: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:34:44,454: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_11000
2026-01-03 22:35:01,467: Train batch 11200: loss: 10.93 grad norm: 52.53 time: 0.071
2026-01-03 22:35:18,699: Train batch 11400: loss: 9.84 grad norm: 52.40 time: 0.057
2026-01-03 22:35:27,295: Running test after training batch: 11500
2026-01-03 22:35:27,446: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:35:32,110: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 22:35:32,143: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-03 22:35:33,889: Val batch 11500: PER (avg): 0.1645 CTC Loss (avg): 16.5766 WER(1gram): 48.73% (n=64) time: 6.594
2026-01-03 22:35:33,890: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 22:35:33,890: t15.2023.08.13 val PER: 0.1216
2026-01-03 22:35:33,890: t15.2023.08.18 val PER: 0.1157
2026-01-03 22:35:33,890: t15.2023.08.20 val PER: 0.1231
2026-01-03 22:35:33,890: t15.2023.08.25 val PER: 0.1039
2026-01-03 22:35:33,890: t15.2023.08.27 val PER: 0.1913
2026-01-03 22:35:33,890: t15.2023.09.01 val PER: 0.0860
2026-01-03 22:35:33,891: t15.2023.09.03 val PER: 0.1698
2026-01-03 22:35:33,891: t15.2023.09.24 val PER: 0.1311
2026-01-03 22:35:33,891: t15.2023.09.29 val PER: 0.1423
2026-01-03 22:35:33,891: t15.2023.10.01 val PER: 0.1803
2026-01-03 22:35:33,891: t15.2023.10.06 val PER: 0.0818
2026-01-03 22:35:33,891: t15.2023.10.08 val PER: 0.2585
2026-01-03 22:35:33,891: t15.2023.10.13 val PER: 0.2172
2026-01-03 22:35:33,891: t15.2023.10.15 val PER: 0.1694
2026-01-03 22:35:33,891: t15.2023.10.20 val PER: 0.1946
2026-01-03 22:35:33,891: t15.2023.10.22 val PER: 0.1236
2026-01-03 22:35:33,891: t15.2023.11.03 val PER: 0.1879
2026-01-03 22:35:33,892: t15.2023.11.04 val PER: 0.0307
2026-01-03 22:35:33,892: t15.2023.11.17 val PER: 0.0467
2026-01-03 22:35:33,892: t15.2023.11.19 val PER: 0.0479
2026-01-03 22:35:33,892: t15.2023.11.26 val PER: 0.1391
2026-01-03 22:35:33,892: t15.2023.12.03 val PER: 0.1450
2026-01-03 22:35:33,892: t15.2023.12.08 val PER: 0.1252
2026-01-03 22:35:33,892: t15.2023.12.10 val PER: 0.1038
2026-01-03 22:35:33,892: t15.2023.12.17 val PER: 0.1507
2026-01-03 22:35:33,892: t15.2023.12.29 val PER: 0.1462
2026-01-03 22:35:33,892: t15.2024.02.25 val PER: 0.1264
2026-01-03 22:35:33,893: t15.2024.03.08 val PER: 0.2376
2026-01-03 22:35:33,893: t15.2024.03.15 val PER: 0.2101
2026-01-03 22:35:33,893: t15.2024.03.17 val PER: 0.1527
2026-01-03 22:35:33,893: t15.2024.05.10 val PER: 0.1753
2026-01-03 22:35:33,893: t15.2024.06.14 val PER: 0.1877
2026-01-03 22:35:33,893: t15.2024.07.19 val PER: 0.2643
2026-01-03 22:35:33,893: t15.2024.07.21 val PER: 0.1055
2026-01-03 22:35:33,893: t15.2024.07.28 val PER: 0.1456
2026-01-03 22:35:33,893: t15.2025.01.10 val PER: 0.3140
2026-01-03 22:35:33,893: t15.2025.01.12 val PER: 0.1709
2026-01-03 22:35:33,893: t15.2025.03.14 val PER: 0.3654
2026-01-03 22:35:33,893: t15.2025.03.16 val PER: 0.2160
2026-01-03 22:35:33,894: t15.2025.03.30 val PER: 0.3080
2026-01-03 22:35:33,894: t15.2025.04.13 val PER: 0.2297
2026-01-03 22:35:34,125: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_11500
2026-01-03 22:35:42,424: Train batch 11600: loss: 11.22 grad norm: 49.17 time: 0.060
2026-01-03 22:36:00,015: Train batch 11800: loss: 6.74 grad norm: 40.44 time: 0.044
2026-01-03 22:36:17,032: Train batch 12000: loss: 14.19 grad norm: 57.61 time: 0.071
2026-01-03 22:36:17,033: Running test after training batch: 12000
2026-01-03 22:36:17,123: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:36:21,871: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 22:36:21,904: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-03 22:36:23,653: Val batch 12000: PER (avg): 0.1605 CTC Loss (avg): 16.2361 WER(1gram): 48.98% (n=64) time: 6.620
2026-01-03 22:36:23,654: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 22:36:23,654: t15.2023.08.13 val PER: 0.1268
2026-01-03 22:36:23,654: t15.2023.08.18 val PER: 0.1056
2026-01-03 22:36:23,654: t15.2023.08.20 val PER: 0.1168
2026-01-03 22:36:23,654: t15.2023.08.25 val PER: 0.0934
2026-01-03 22:36:23,655: t15.2023.08.27 val PER: 0.1881
2026-01-03 22:36:23,655: t15.2023.09.01 val PER: 0.0877
2026-01-03 22:36:23,655: t15.2023.09.03 val PER: 0.1580
2026-01-03 22:36:23,655: t15.2023.09.24 val PER: 0.1335
2026-01-03 22:36:23,655: t15.2023.09.29 val PER: 0.1436
2026-01-03 22:36:23,655: t15.2023.10.01 val PER: 0.1803
2026-01-03 22:36:23,655: t15.2023.10.06 val PER: 0.0840
2026-01-03 22:36:23,655: t15.2023.10.08 val PER: 0.2530
2026-01-03 22:36:23,655: t15.2023.10.13 val PER: 0.2079
2026-01-03 22:36:23,655: t15.2023.10.15 val PER: 0.1674
2026-01-03 22:36:23,655: t15.2023.10.20 val PER: 0.1879
2026-01-03 22:36:23,656: t15.2023.10.22 val PER: 0.1169
2026-01-03 22:36:23,656: t15.2023.11.03 val PER: 0.1940
2026-01-03 22:36:23,656: t15.2023.11.04 val PER: 0.0307
2026-01-03 22:36:23,656: t15.2023.11.17 val PER: 0.0420
2026-01-03 22:36:23,656: t15.2023.11.19 val PER: 0.0379
2026-01-03 22:36:23,656: t15.2023.11.26 val PER: 0.1341
2026-01-03 22:36:23,656: t15.2023.12.03 val PER: 0.1303
2026-01-03 22:36:23,656: t15.2023.12.08 val PER: 0.1119
2026-01-03 22:36:23,656: t15.2023.12.10 val PER: 0.0959
2026-01-03 22:36:23,656: t15.2023.12.17 val PER: 0.1466
2026-01-03 22:36:23,656: t15.2023.12.29 val PER: 0.1393
2026-01-03 22:36:23,656: t15.2024.02.25 val PER: 0.1208
2026-01-03 22:36:23,657: t15.2024.03.08 val PER: 0.2319
2026-01-03 22:36:23,657: t15.2024.03.15 val PER: 0.2145
2026-01-03 22:36:23,657: t15.2024.03.17 val PER: 0.1583
2026-01-03 22:36:23,657: t15.2024.05.10 val PER: 0.1828
2026-01-03 22:36:23,657: t15.2024.06.14 val PER: 0.1940
2026-01-03 22:36:23,657: t15.2024.07.19 val PER: 0.2617
2026-01-03 22:36:23,657: t15.2024.07.21 val PER: 0.1083
2026-01-03 22:36:23,657: t15.2024.07.28 val PER: 0.1368
2026-01-03 22:36:23,657: t15.2025.01.10 val PER: 0.2975
2026-01-03 22:36:23,657: t15.2025.01.12 val PER: 0.1532
2026-01-03 22:36:23,657: t15.2025.03.14 val PER: 0.3550
2026-01-03 22:36:23,657: t15.2025.03.16 val PER: 0.2094
2026-01-03 22:36:23,657: t15.2025.03.30 val PER: 0.3080
2026-01-03 22:36:23,657: t15.2025.04.13 val PER: 0.2154
2026-01-03 22:36:23,896: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_12000
2026-01-03 22:36:40,847: Train batch 12200: loss: 5.92 grad norm: 40.23 time: 0.065
2026-01-03 22:36:57,635: Train batch 12400: loss: 5.11 grad norm: 36.73 time: 0.041
2026-01-03 22:37:06,229: Running test after training batch: 12500
2026-01-03 22:37:06,366: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:37:11,182: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 22:37:11,217: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-03 22:37:13,008: Val batch 12500: PER (avg): 0.1575 CTC Loss (avg): 16.0266 WER(1gram): 48.98% (n=64) time: 6.779
2026-01-03 22:37:13,009: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-03 22:37:13,009: t15.2023.08.13 val PER: 0.1279
2026-01-03 22:37:13,009: t15.2023.08.18 val PER: 0.1031
2026-01-03 22:37:13,009: t15.2023.08.20 val PER: 0.1136
2026-01-03 22:37:13,009: t15.2023.08.25 val PER: 0.0949
2026-01-03 22:37:13,009: t15.2023.08.27 val PER: 0.2026
2026-01-03 22:37:13,009: t15.2023.09.01 val PER: 0.0860
2026-01-03 22:37:13,009: t15.2023.09.03 val PER: 0.1627
2026-01-03 22:37:13,009: t15.2023.09.24 val PER: 0.1323
2026-01-03 22:37:13,009: t15.2023.09.29 val PER: 0.1321
2026-01-03 22:37:13,010: t15.2023.10.01 val PER: 0.1691
2026-01-03 22:37:13,010: t15.2023.10.06 val PER: 0.0850
2026-01-03 22:37:13,010: t15.2023.10.08 val PER: 0.2558
2026-01-03 22:37:13,010: t15.2023.10.13 val PER: 0.2079
2026-01-03 22:37:13,010: t15.2023.10.15 val PER: 0.1648
2026-01-03 22:37:13,010: t15.2023.10.20 val PER: 0.1779
2026-01-03 22:37:13,010: t15.2023.10.22 val PER: 0.1180
2026-01-03 22:37:13,010: t15.2023.11.03 val PER: 0.1893
2026-01-03 22:37:13,010: t15.2023.11.04 val PER: 0.0239
2026-01-03 22:37:13,010: t15.2023.11.17 val PER: 0.0482
2026-01-03 22:37:13,010: t15.2023.11.19 val PER: 0.0419
2026-01-03 22:37:13,010: t15.2023.11.26 val PER: 0.1326
2026-01-03 22:37:13,010: t15.2023.12.03 val PER: 0.1229
2026-01-03 22:37:13,010: t15.2023.12.08 val PER: 0.1125
2026-01-03 22:37:13,011: t15.2023.12.10 val PER: 0.0933
2026-01-03 22:37:13,011: t15.2023.12.17 val PER: 0.1528
2026-01-03 22:37:13,011: t15.2023.12.29 val PER: 0.1373
2026-01-03 22:37:13,011: t15.2024.02.25 val PER: 0.1039
2026-01-03 22:37:13,011: t15.2024.03.08 val PER: 0.2248
2026-01-03 22:37:13,011: t15.2024.03.15 val PER: 0.2220
2026-01-03 22:37:13,011: t15.2024.03.17 val PER: 0.1444
2026-01-03 22:37:13,011: t15.2024.05.10 val PER: 0.1724
2026-01-03 22:37:13,011: t15.2024.06.14 val PER: 0.1830
2026-01-03 22:37:13,011: t15.2024.07.19 val PER: 0.2465
2026-01-03 22:37:13,011: t15.2024.07.21 val PER: 0.0979
2026-01-03 22:37:13,011: t15.2024.07.28 val PER: 0.1360
2026-01-03 22:37:13,011: t15.2025.01.10 val PER: 0.3182
2026-01-03 22:37:13,011: t15.2025.01.12 val PER: 0.1547
2026-01-03 22:37:13,011: t15.2025.03.14 val PER: 0.3565
2026-01-03 22:37:13,011: t15.2025.03.16 val PER: 0.1963
2026-01-03 22:37:13,012: t15.2025.03.30 val PER: 0.2989
2026-01-03 22:37:13,012: t15.2025.04.13 val PER: 0.2111
2026-01-03 22:37:13,247: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_12500
2026-01-03 22:37:21,759: Train batch 12600: loss: 7.73 grad norm: 41.82 time: 0.057
2026-01-03 22:37:39,171: Train batch 12800: loss: 5.84 grad norm: 38.93 time: 0.052
2026-01-03 22:37:56,297: Train batch 13000: loss: 6.41 grad norm: 42.08 time: 0.066
2026-01-03 22:37:56,297: Running test after training batch: 13000
2026-01-03 22:37:56,400: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:38:01,061: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-03 22:38:01,095: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 22:38:02,887: Val batch 13000: PER (avg): 0.1566 CTC Loss (avg): 15.9052 WER(1gram): 46.70% (n=64) time: 6.590
2026-01-03 22:38:02,888: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-03 22:38:02,888: t15.2023.08.13 val PER: 0.1185
2026-01-03 22:38:02,888: t15.2023.08.18 val PER: 0.1039
2026-01-03 22:38:02,888: t15.2023.08.20 val PER: 0.1048
2026-01-03 22:38:02,888: t15.2023.08.25 val PER: 0.0919
2026-01-03 22:38:02,888: t15.2023.08.27 val PER: 0.1977
2026-01-03 22:38:02,888: t15.2023.09.01 val PER: 0.0787
2026-01-03 22:38:02,888: t15.2023.09.03 val PER: 0.1698
2026-01-03 22:38:02,888: t15.2023.09.24 val PER: 0.1335
2026-01-03 22:38:02,888: t15.2023.09.29 val PER: 0.1347
2026-01-03 22:38:02,888: t15.2023.10.01 val PER: 0.1777
2026-01-03 22:38:02,888: t15.2023.10.06 val PER: 0.0861
2026-01-03 22:38:02,889: t15.2023.10.08 val PER: 0.2558
2026-01-03 22:38:02,889: t15.2023.10.13 val PER: 0.2056
2026-01-03 22:38:02,889: t15.2023.10.15 val PER: 0.1602
2026-01-03 22:38:02,889: t15.2023.10.20 val PER: 0.1779
2026-01-03 22:38:02,889: t15.2023.10.22 val PER: 0.1114
2026-01-03 22:38:02,889: t15.2023.11.03 val PER: 0.1900
2026-01-03 22:38:02,889: t15.2023.11.04 val PER: 0.0273
2026-01-03 22:38:02,889: t15.2023.11.17 val PER: 0.0404
2026-01-03 22:38:02,889: t15.2023.11.19 val PER: 0.0539
2026-01-03 22:38:02,889: t15.2023.11.26 val PER: 0.1297
2026-01-03 22:38:02,889: t15.2023.12.03 val PER: 0.1197
2026-01-03 22:38:02,889: t15.2023.12.08 val PER: 0.1172
2026-01-03 22:38:02,889: t15.2023.12.10 val PER: 0.0999
2026-01-03 22:38:02,890: t15.2023.12.17 val PER: 0.1393
2026-01-03 22:38:02,890: t15.2023.12.29 val PER: 0.1462
2026-01-03 22:38:02,890: t15.2024.02.25 val PER: 0.1166
2026-01-03 22:38:02,890: t15.2024.03.08 val PER: 0.2319
2026-01-03 22:38:02,890: t15.2024.03.15 val PER: 0.2114
2026-01-03 22:38:02,890: t15.2024.03.17 val PER: 0.1416
2026-01-03 22:38:02,890: t15.2024.05.10 val PER: 0.1783
2026-01-03 22:38:02,890: t15.2024.06.14 val PER: 0.1735
2026-01-03 22:38:02,890: t15.2024.07.19 val PER: 0.2446
2026-01-03 22:38:02,890: t15.2024.07.21 val PER: 0.0993
2026-01-03 22:38:02,891: t15.2024.07.28 val PER: 0.1434
2026-01-03 22:38:02,891: t15.2025.01.10 val PER: 0.3072
2026-01-03 22:38:02,891: t15.2025.01.12 val PER: 0.1555
2026-01-03 22:38:02,891: t15.2025.03.14 val PER: 0.3506
2026-01-03 22:38:02,891: t15.2025.03.16 val PER: 0.1819
2026-01-03 22:38:02,891: t15.2025.03.30 val PER: 0.2943
2026-01-03 22:38:02,891: t15.2025.04.13 val PER: 0.2154
2026-01-03 22:38:02,892: New best val WER(1gram) 48.73% --> 46.70%
2026-01-03 22:38:02,892: Checkpointing model
2026-01-03 22:38:03,481: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:38:03,723: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_13000
2026-01-03 22:38:20,606: Train batch 13200: loss: 12.56 grad norm: 58.23 time: 0.054
2026-01-03 22:38:37,530: Train batch 13400: loss: 9.05 grad norm: 52.21 time: 0.062
2026-01-03 22:38:45,986: Running test after training batch: 13500
2026-01-03 22:38:46,099: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:38:50,741: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 22:38:50,774: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost get
2026-01-03 22:38:52,574: Val batch 13500: PER (avg): 0.1546 CTC Loss (avg): 15.6599 WER(1gram): 46.19% (n=64) time: 6.588
2026-01-03 22:38:52,575: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 22:38:52,575: t15.2023.08.13 val PER: 0.1216
2026-01-03 22:38:52,575: t15.2023.08.18 val PER: 0.1048
2026-01-03 22:38:52,575: t15.2023.08.20 val PER: 0.1120
2026-01-03 22:38:52,575: t15.2023.08.25 val PER: 0.0889
2026-01-03 22:38:52,575: t15.2023.08.27 val PER: 0.1945
2026-01-03 22:38:52,575: t15.2023.09.01 val PER: 0.0828
2026-01-03 22:38:52,575: t15.2023.09.03 val PER: 0.1639
2026-01-03 22:38:52,576: t15.2023.09.24 val PER: 0.1335
2026-01-03 22:38:52,576: t15.2023.09.29 val PER: 0.1289
2026-01-03 22:38:52,576: t15.2023.10.01 val PER: 0.1823
2026-01-03 22:38:52,576: t15.2023.10.06 val PER: 0.0915
2026-01-03 22:38:52,576: t15.2023.10.08 val PER: 0.2517
2026-01-03 22:38:52,576: t15.2023.10.13 val PER: 0.2025
2026-01-03 22:38:52,576: t15.2023.10.15 val PER: 0.1529
2026-01-03 22:38:52,576: t15.2023.10.20 val PER: 0.1879
2026-01-03 22:38:52,576: t15.2023.10.22 val PER: 0.1192
2026-01-03 22:38:52,576: t15.2023.11.03 val PER: 0.1798
2026-01-03 22:38:52,576: t15.2023.11.04 val PER: 0.0307
2026-01-03 22:38:52,576: t15.2023.11.17 val PER: 0.0451
2026-01-03 22:38:52,576: t15.2023.11.19 val PER: 0.0319
2026-01-03 22:38:52,576: t15.2023.11.26 val PER: 0.1341
2026-01-03 22:38:52,576: t15.2023.12.03 val PER: 0.1134
2026-01-03 22:38:52,577: t15.2023.12.08 val PER: 0.1105
2026-01-03 22:38:52,577: t15.2023.12.10 val PER: 0.0972
2026-01-03 22:38:52,577: t15.2023.12.17 val PER: 0.1383
2026-01-03 22:38:52,577: t15.2023.12.29 val PER: 0.1345
2026-01-03 22:38:52,577: t15.2024.02.25 val PER: 0.1138
2026-01-03 22:38:52,577: t15.2024.03.08 val PER: 0.2333
2026-01-03 22:38:52,577: t15.2024.03.15 val PER: 0.2064
2026-01-03 22:38:52,577: t15.2024.03.17 val PER: 0.1492
2026-01-03 22:38:52,577: t15.2024.05.10 val PER: 0.1649
2026-01-03 22:38:52,577: t15.2024.06.14 val PER: 0.1782
2026-01-03 22:38:52,577: t15.2024.07.19 val PER: 0.2439
2026-01-03 22:38:52,577: t15.2024.07.21 val PER: 0.1000
2026-01-03 22:38:52,577: t15.2024.07.28 val PER: 0.1412
2026-01-03 22:38:52,577: t15.2025.01.10 val PER: 0.2879
2026-01-03 22:38:52,577: t15.2025.01.12 val PER: 0.1478
2026-01-03 22:38:52,577: t15.2025.03.14 val PER: 0.3476
2026-01-03 22:38:52,577: t15.2025.03.16 val PER: 0.1859
2026-01-03 22:38:52,578: t15.2025.03.30 val PER: 0.2931
2026-01-03 22:38:52,578: t15.2025.04.13 val PER: 0.2097
2026-01-03 22:38:52,579: New best val WER(1gram) 46.70% --> 46.19%
2026-01-03 22:38:52,579: Checkpointing model
2026-01-03 22:38:53,190: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:38:53,450: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_13500
2026-01-03 22:39:02,049: Train batch 13600: loss: 12.88 grad norm: 65.22 time: 0.062
2026-01-03 22:39:19,416: Train batch 13800: loss: 9.01 grad norm: 57.15 time: 0.056
2026-01-03 22:39:36,891: Train batch 14000: loss: 11.80 grad norm: 56.53 time: 0.051
2026-01-03 22:39:36,892: Running test after training batch: 14000
2026-01-03 22:39:36,998: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:39:41,751: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 22:39:41,785: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 22:39:43,651: Val batch 14000: PER (avg): 0.1525 CTC Loss (avg): 15.5616 WER(1gram): 44.67% (n=64) time: 6.759
2026-01-03 22:39:43,651: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=10
2026-01-03 22:39:43,652: t15.2023.08.13 val PER: 0.1154
2026-01-03 22:39:43,652: t15.2023.08.18 val PER: 0.0981
2026-01-03 22:39:43,652: t15.2023.08.20 val PER: 0.0977
2026-01-03 22:39:43,652: t15.2023.08.25 val PER: 0.0994
2026-01-03 22:39:43,652: t15.2023.08.27 val PER: 0.1833
2026-01-03 22:39:43,652: t15.2023.09.01 val PER: 0.0771
2026-01-03 22:39:43,652: t15.2023.09.03 val PER: 0.1758
2026-01-03 22:39:43,652: t15.2023.09.24 val PER: 0.1323
2026-01-03 22:39:43,652: t15.2023.09.29 val PER: 0.1340
2026-01-03 22:39:43,652: t15.2023.10.01 val PER: 0.1757
2026-01-03 22:39:43,652: t15.2023.10.06 val PER: 0.0850
2026-01-03 22:39:43,652: t15.2023.10.08 val PER: 0.2544
2026-01-03 22:39:43,653: t15.2023.10.13 val PER: 0.1955
2026-01-03 22:39:43,653: t15.2023.10.15 val PER: 0.1635
2026-01-03 22:39:43,653: t15.2023.10.20 val PER: 0.1946
2026-01-03 22:39:43,653: t15.2023.10.22 val PER: 0.1180
2026-01-03 22:39:43,653: t15.2023.11.03 val PER: 0.1811
2026-01-03 22:39:43,653: t15.2023.11.04 val PER: 0.0273
2026-01-03 22:39:43,653: t15.2023.11.17 val PER: 0.0498
2026-01-03 22:39:43,653: t15.2023.11.19 val PER: 0.0319
2026-01-03 22:39:43,653: t15.2023.11.26 val PER: 0.1290
2026-01-03 22:39:43,653: t15.2023.12.03 val PER: 0.1197
2026-01-03 22:39:43,653: t15.2023.12.08 val PER: 0.1039
2026-01-03 22:39:43,653: t15.2023.12.10 val PER: 0.0920
2026-01-03 22:39:43,653: t15.2023.12.17 val PER: 0.1372
2026-01-03 22:39:43,653: t15.2023.12.29 val PER: 0.1373
2026-01-03 22:39:43,653: t15.2024.02.25 val PER: 0.1138
2026-01-03 22:39:43,654: t15.2024.03.08 val PER: 0.2319
2026-01-03 22:39:43,654: t15.2024.03.15 val PER: 0.2014
2026-01-03 22:39:43,654: t15.2024.03.17 val PER: 0.1457
2026-01-03 22:39:43,654: t15.2024.05.10 val PER: 0.1575
2026-01-03 22:39:43,654: t15.2024.06.14 val PER: 0.1672
2026-01-03 22:39:43,654: t15.2024.07.19 val PER: 0.2373
2026-01-03 22:39:43,655: t15.2024.07.21 val PER: 0.0966
2026-01-03 22:39:43,655: t15.2024.07.28 val PER: 0.1338
2026-01-03 22:39:43,655: t15.2025.01.10 val PER: 0.2865
2026-01-03 22:39:43,655: t15.2025.01.12 val PER: 0.1501
2026-01-03 22:39:43,655: t15.2025.03.14 val PER: 0.3388
2026-01-03 22:39:43,655: t15.2025.03.16 val PER: 0.1832
2026-01-03 22:39:43,655: t15.2025.03.30 val PER: 0.2977
2026-01-03 22:39:43,655: t15.2025.04.13 val PER: 0.2111
2026-01-03 22:39:43,656: New best val WER(1gram) 46.19% --> 44.67%
2026-01-03 22:39:43,656: Checkpointing model
2026-01-03 22:39:44,243: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:39:44,513: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_14000
2026-01-03 22:40:01,453: Train batch 14200: loss: 8.42 grad norm: 52.65 time: 0.056
2026-01-03 22:40:19,023: Train batch 14400: loss: 5.84 grad norm: 38.71 time: 0.063
2026-01-03 22:40:27,754: Running test after training batch: 14500
2026-01-03 22:40:27,847: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:40:32,511: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 22:40:32,545: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 22:40:34,423: Val batch 14500: PER (avg): 0.1537 CTC Loss (avg): 15.5710 WER(1gram): 46.45% (n=64) time: 6.669
2026-01-03 22:40:34,423: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 22:40:34,424: t15.2023.08.13 val PER: 0.1185
2026-01-03 22:40:34,424: t15.2023.08.18 val PER: 0.1048
2026-01-03 22:40:34,424: t15.2023.08.20 val PER: 0.1088
2026-01-03 22:40:34,424: t15.2023.08.25 val PER: 0.0934
2026-01-03 22:40:34,424: t15.2023.08.27 val PER: 0.1881
2026-01-03 22:40:34,424: t15.2023.09.01 val PER: 0.0763
2026-01-03 22:40:34,424: t15.2023.09.03 val PER: 0.1698
2026-01-03 22:40:34,424: t15.2023.09.24 val PER: 0.1347
2026-01-03 22:40:34,424: t15.2023.09.29 val PER: 0.1283
2026-01-03 22:40:34,424: t15.2023.10.01 val PER: 0.1810
2026-01-03 22:40:34,424: t15.2023.10.06 val PER: 0.0861
2026-01-03 22:40:34,424: t15.2023.10.08 val PER: 0.2530
2026-01-03 22:40:34,425: t15.2023.10.13 val PER: 0.2071
2026-01-03 22:40:34,425: t15.2023.10.15 val PER: 0.1635
2026-01-03 22:40:34,425: t15.2023.10.20 val PER: 0.1745
2026-01-03 22:40:34,425: t15.2023.10.22 val PER: 0.1147
2026-01-03 22:40:34,425: t15.2023.11.03 val PER: 0.1818
2026-01-03 22:40:34,425: t15.2023.11.04 val PER: 0.0307
2026-01-03 22:40:34,425: t15.2023.11.17 val PER: 0.0482
2026-01-03 22:40:34,425: t15.2023.11.19 val PER: 0.0399
2026-01-03 22:40:34,425: t15.2023.11.26 val PER: 0.1362
2026-01-03 22:40:34,425: t15.2023.12.03 val PER: 0.1082
2026-01-03 22:40:34,425: t15.2023.12.08 val PER: 0.1025
2026-01-03 22:40:34,425: t15.2023.12.10 val PER: 0.0880
2026-01-03 22:40:34,425: t15.2023.12.17 val PER: 0.1393
2026-01-03 22:40:34,426: t15.2023.12.29 val PER: 0.1386
2026-01-03 22:40:34,426: t15.2024.02.25 val PER: 0.1222
2026-01-03 22:40:34,426: t15.2024.03.08 val PER: 0.2248
2026-01-03 22:40:34,426: t15.2024.03.15 val PER: 0.2089
2026-01-03 22:40:34,426: t15.2024.03.17 val PER: 0.1360
2026-01-03 22:40:34,426: t15.2024.05.10 val PER: 0.1634
2026-01-03 22:40:34,426: t15.2024.06.14 val PER: 0.1688
2026-01-03 22:40:34,426: t15.2024.07.19 val PER: 0.2472
2026-01-03 22:40:34,426: t15.2024.07.21 val PER: 0.0959
2026-01-03 22:40:34,426: t15.2024.07.28 val PER: 0.1368
2026-01-03 22:40:34,426: t15.2025.01.10 val PER: 0.2865
2026-01-03 22:40:34,426: t15.2025.01.12 val PER: 0.1532
2026-01-03 22:40:34,426: t15.2025.03.14 val PER: 0.3550
2026-01-03 22:40:34,426: t15.2025.03.16 val PER: 0.1806
2026-01-03 22:40:34,426: t15.2025.03.30 val PER: 0.2908
2026-01-03 22:40:34,426: t15.2025.04.13 val PER: 0.1997
2026-01-03 22:40:34,681: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_14500
2026-01-03 22:40:43,195: Train batch 14600: loss: 12.83 grad norm: 63.18 time: 0.058
2026-01-03 22:41:00,638: Train batch 14800: loss: 5.81 grad norm: 44.11 time: 0.050
2026-01-03 22:41:18,298: Train batch 15000: loss: 9.31 grad norm: 45.19 time: 0.052
2026-01-03 22:41:18,299: Running test after training batch: 15000
2026-01-03 22:41:18,419: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:41:23,241: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 22:41:23,274: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 22:41:25,116: Val batch 15000: PER (avg): 0.1502 CTC Loss (avg): 15.4097 WER(1gram): 46.19% (n=64) time: 6.817
2026-01-03 22:41:25,117: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-03 22:41:25,117: t15.2023.08.13 val PER: 0.1091
2026-01-03 22:41:25,117: t15.2023.08.18 val PER: 0.0997
2026-01-03 22:41:25,117: t15.2023.08.20 val PER: 0.1104
2026-01-03 22:41:25,117: t15.2023.08.25 val PER: 0.0889
2026-01-03 22:41:25,117: t15.2023.08.27 val PER: 0.1849
2026-01-03 22:41:25,117: t15.2023.09.01 val PER: 0.0731
2026-01-03 22:41:25,117: t15.2023.09.03 val PER: 0.1556
2026-01-03 22:41:25,117: t15.2023.09.24 val PER: 0.1299
2026-01-03 22:41:25,117: t15.2023.09.29 val PER: 0.1327
2026-01-03 22:41:25,117: t15.2023.10.01 val PER: 0.1750
2026-01-03 22:41:25,117: t15.2023.10.06 val PER: 0.0829
2026-01-03 22:41:25,117: t15.2023.10.08 val PER: 0.2585
2026-01-03 22:41:25,118: t15.2023.10.13 val PER: 0.1986
2026-01-03 22:41:25,118: t15.2023.10.15 val PER: 0.1503
2026-01-03 22:41:25,118: t15.2023.10.20 val PER: 0.1980
2026-01-03 22:41:25,118: t15.2023.10.22 val PER: 0.1125
2026-01-03 22:41:25,118: t15.2023.11.03 val PER: 0.1811
2026-01-03 22:41:25,118: t15.2023.11.04 val PER: 0.0341
2026-01-03 22:41:25,118: t15.2023.11.17 val PER: 0.0435
2026-01-03 22:41:25,118: t15.2023.11.19 val PER: 0.0399
2026-01-03 22:41:25,118: t15.2023.11.26 val PER: 0.1217
2026-01-03 22:41:25,118: t15.2023.12.03 val PER: 0.1145
2026-01-03 22:41:25,118: t15.2023.12.08 val PER: 0.0979
2026-01-03 22:41:25,119: t15.2023.12.10 val PER: 0.0894
2026-01-03 22:41:25,119: t15.2023.12.17 val PER: 0.1486
2026-01-03 22:41:25,119: t15.2023.12.29 val PER: 0.1290
2026-01-03 22:41:25,119: t15.2024.02.25 val PER: 0.1011
2026-01-03 22:41:25,119: t15.2024.03.08 val PER: 0.2248
2026-01-03 22:41:25,119: t15.2024.03.15 val PER: 0.2033
2026-01-03 22:41:25,119: t15.2024.03.17 val PER: 0.1416
2026-01-03 22:41:25,119: t15.2024.05.10 val PER: 0.1664
2026-01-03 22:41:25,119: t15.2024.06.14 val PER: 0.1688
2026-01-03 22:41:25,119: t15.2024.07.19 val PER: 0.2254
2026-01-03 22:41:25,119: t15.2024.07.21 val PER: 0.0952
2026-01-03 22:41:25,119: t15.2024.07.28 val PER: 0.1309
2026-01-03 22:41:25,119: t15.2025.01.10 val PER: 0.2879
2026-01-03 22:41:25,119: t15.2025.01.12 val PER: 0.1501
2026-01-03 22:41:25,120: t15.2025.03.14 val PER: 0.3462
2026-01-03 22:41:25,120: t15.2025.03.16 val PER: 0.1793
2026-01-03 22:41:25,120: t15.2025.03.30 val PER: 0.2989
2026-01-03 22:41:25,120: t15.2025.04.13 val PER: 0.2154
2026-01-03 22:41:25,369: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_15000
2026-01-03 22:41:42,614: Train batch 15200: loss: 4.92 grad norm: 38.87 time: 0.057
2026-01-03 22:41:59,534: Train batch 15400: loss: 11.22 grad norm: 55.37 time: 0.049
2026-01-03 22:42:08,081: Running test after training batch: 15500
2026-01-03 22:42:08,190: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:42:12,905: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 22:42:12,940: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 22:42:14,834: Val batch 15500: PER (avg): 0.1496 CTC Loss (avg): 15.3725 WER(1gram): 44.92% (n=64) time: 6.753
2026-01-03 22:42:14,834: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 22:42:14,835: t15.2023.08.13 val PER: 0.1081
2026-01-03 22:42:14,835: t15.2023.08.18 val PER: 0.0981
2026-01-03 22:42:14,835: t15.2023.08.20 val PER: 0.1096
2026-01-03 22:42:14,835: t15.2023.08.25 val PER: 0.0979
2026-01-03 22:42:14,835: t15.2023.08.27 val PER: 0.1785
2026-01-03 22:42:14,835: t15.2023.09.01 val PER: 0.0804
2026-01-03 22:42:14,835: t15.2023.09.03 val PER: 0.1568
2026-01-03 22:42:14,835: t15.2023.09.24 val PER: 0.1250
2026-01-03 22:42:14,835: t15.2023.09.29 val PER: 0.1276
2026-01-03 22:42:14,835: t15.2023.10.01 val PER: 0.1731
2026-01-03 22:42:14,835: t15.2023.10.06 val PER: 0.0893
2026-01-03 22:42:14,835: t15.2023.10.08 val PER: 0.2490
2026-01-03 22:42:14,836: t15.2023.10.13 val PER: 0.2009
2026-01-03 22:42:14,836: t15.2023.10.15 val PER: 0.1529
2026-01-03 22:42:14,836: t15.2023.10.20 val PER: 0.1846
2026-01-03 22:42:14,836: t15.2023.10.22 val PER: 0.1192
2026-01-03 22:42:14,836: t15.2023.11.03 val PER: 0.1764
2026-01-03 22:42:14,836: t15.2023.11.04 val PER: 0.0341
2026-01-03 22:42:14,836: t15.2023.11.17 val PER: 0.0389
2026-01-03 22:42:14,836: t15.2023.11.19 val PER: 0.0399
2026-01-03 22:42:14,836: t15.2023.11.26 val PER: 0.1181
2026-01-03 22:42:14,836: t15.2023.12.03 val PER: 0.1145
2026-01-03 22:42:14,836: t15.2023.12.08 val PER: 0.1032
2026-01-03 22:42:14,836: t15.2023.12.10 val PER: 0.0894
2026-01-03 22:42:14,837: t15.2023.12.17 val PER: 0.1383
2026-01-03 22:42:14,837: t15.2023.12.29 val PER: 0.1277
2026-01-03 22:42:14,837: t15.2024.02.25 val PER: 0.1025
2026-01-03 22:42:14,837: t15.2024.03.08 val PER: 0.2176
2026-01-03 22:42:14,837: t15.2024.03.15 val PER: 0.2001
2026-01-03 22:42:14,837: t15.2024.03.17 val PER: 0.1416
2026-01-03 22:42:14,837: t15.2024.05.10 val PER: 0.1590
2026-01-03 22:42:14,837: t15.2024.06.14 val PER: 0.1656
2026-01-03 22:42:14,837: t15.2024.07.19 val PER: 0.2294
2026-01-03 22:42:14,837: t15.2024.07.21 val PER: 0.0959
2026-01-03 22:42:14,837: t15.2024.07.28 val PER: 0.1375
2026-01-03 22:42:14,837: t15.2025.01.10 val PER: 0.2837
2026-01-03 22:42:14,837: t15.2025.01.12 val PER: 0.1532
2026-01-03 22:42:14,837: t15.2025.03.14 val PER: 0.3402
2026-01-03 22:42:14,837: t15.2025.03.16 val PER: 0.1793
2026-01-03 22:42:14,838: t15.2025.03.30 val PER: 0.2977
2026-01-03 22:42:14,838: t15.2025.04.13 val PER: 0.2097
2026-01-03 22:42:15,092: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_15500
2026-01-03 22:42:23,958: Train batch 15600: loss: 12.03 grad norm: 55.06 time: 0.063
2026-01-03 22:42:41,594: Train batch 15800: loss: 13.59 grad norm: 63.48 time: 0.068
2026-01-03 22:42:59,640: Train batch 16000: loss: 8.62 grad norm: 45.56 time: 0.055
2026-01-03 22:42:59,641: Running test after training batch: 16000
2026-01-03 22:42:59,738: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:43:04,462: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 22:43:04,497: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost get
2026-01-03 22:43:06,422: Val batch 16000: PER (avg): 0.1503 CTC Loss (avg): 15.3858 WER(1gram): 44.42% (n=64) time: 6.782
2026-01-03 22:43:06,423: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-03 22:43:06,423: t15.2023.08.13 val PER: 0.1081
2026-01-03 22:43:06,423: t15.2023.08.18 val PER: 0.1014
2026-01-03 22:43:06,423: t15.2023.08.20 val PER: 0.1064
2026-01-03 22:43:06,423: t15.2023.08.25 val PER: 0.0994
2026-01-03 22:43:06,423: t15.2023.08.27 val PER: 0.1929
2026-01-03 22:43:06,423: t15.2023.09.01 val PER: 0.0795
2026-01-03 22:43:06,423: t15.2023.09.03 val PER: 0.1544
2026-01-03 22:43:06,424: t15.2023.09.24 val PER: 0.1299
2026-01-03 22:43:06,424: t15.2023.09.29 val PER: 0.1302
2026-01-03 22:43:06,424: t15.2023.10.01 val PER: 0.1724
2026-01-03 22:43:06,424: t15.2023.10.06 val PER: 0.0861
2026-01-03 22:43:06,424: t15.2023.10.08 val PER: 0.2598
2026-01-03 22:43:06,424: t15.2023.10.13 val PER: 0.2017
2026-01-03 22:43:06,424: t15.2023.10.15 val PER: 0.1523
2026-01-03 22:43:06,424: t15.2023.10.20 val PER: 0.1946
2026-01-03 22:43:06,424: t15.2023.10.22 val PER: 0.1125
2026-01-03 22:43:06,424: t15.2023.11.03 val PER: 0.1791
2026-01-03 22:43:06,424: t15.2023.11.04 val PER: 0.0273
2026-01-03 22:43:06,425: t15.2023.11.17 val PER: 0.0420
2026-01-03 22:43:06,425: t15.2023.11.19 val PER: 0.0419
2026-01-03 22:43:06,425: t15.2023.11.26 val PER: 0.1174
2026-01-03 22:43:06,425: t15.2023.12.03 val PER: 0.1166
2026-01-03 22:43:06,425: t15.2023.12.08 val PER: 0.1045
2026-01-03 22:43:06,425: t15.2023.12.10 val PER: 0.0907
2026-01-03 22:43:06,425: t15.2023.12.17 val PER: 0.1372
2026-01-03 22:43:06,425: t15.2023.12.29 val PER: 0.1277
2026-01-03 22:43:06,425: t15.2024.02.25 val PER: 0.0997
2026-01-03 22:43:06,425: t15.2024.03.08 val PER: 0.2262
2026-01-03 22:43:06,425: t15.2024.03.15 val PER: 0.2008
2026-01-03 22:43:06,425: t15.2024.03.17 val PER: 0.1402
2026-01-03 22:43:06,425: t15.2024.05.10 val PER: 0.1709
2026-01-03 22:43:06,425: t15.2024.06.14 val PER: 0.1767
2026-01-03 22:43:06,425: t15.2024.07.19 val PER: 0.2327
2026-01-03 22:43:06,425: t15.2024.07.21 val PER: 0.0890
2026-01-03 22:43:06,426: t15.2024.07.28 val PER: 0.1346
2026-01-03 22:43:06,426: t15.2025.01.10 val PER: 0.2906
2026-01-03 22:43:06,426: t15.2025.01.12 val PER: 0.1493
2026-01-03 22:43:06,426: t15.2025.03.14 val PER: 0.3417
2026-01-03 22:43:06,426: t15.2025.03.16 val PER: 0.1754
2026-01-03 22:43:06,426: t15.2025.03.30 val PER: 0.2920
2026-01-03 22:43:06,426: t15.2025.04.13 val PER: 0.2140
2026-01-03 22:43:06,427: New best val WER(1gram) 44.67% --> 44.42%
2026-01-03 22:43:06,427: Checkpointing model
2026-01-03 22:43:07,053: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:43:07,320: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_16000
2026-01-03 22:43:24,166: Train batch 16200: loss: 6.71 grad norm: 43.54 time: 0.055
2026-01-03 22:43:41,264: Train batch 16400: loss: 10.60 grad norm: 65.16 time: 0.057
2026-01-03 22:43:49,917: Running test after training batch: 16500
2026-01-03 22:43:50,012: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:43:55,038: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 22:43:55,073: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost et
2026-01-03 22:43:56,962: Val batch 16500: PER (avg): 0.1487 CTC Loss (avg): 15.2408 WER(1gram): 43.15% (n=64) time: 7.045
2026-01-03 22:43:56,963: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-03 22:43:56,963: t15.2023.08.13 val PER: 0.1123
2026-01-03 22:43:56,963: t15.2023.08.18 val PER: 0.1006
2026-01-03 22:43:56,963: t15.2023.08.20 val PER: 0.1080
2026-01-03 22:43:56,963: t15.2023.08.25 val PER: 0.0873
2026-01-03 22:43:56,963: t15.2023.08.27 val PER: 0.1833
2026-01-03 22:43:56,963: t15.2023.09.01 val PER: 0.0763
2026-01-03 22:43:56,964: t15.2023.09.03 val PER: 0.1580
2026-01-03 22:43:56,964: t15.2023.09.24 val PER: 0.1299
2026-01-03 22:43:56,964: t15.2023.09.29 val PER: 0.1289
2026-01-03 22:43:56,964: t15.2023.10.01 val PER: 0.1737
2026-01-03 22:43:56,964: t15.2023.10.06 val PER: 0.0786
2026-01-03 22:43:56,964: t15.2023.10.08 val PER: 0.2544
2026-01-03 22:43:56,964: t15.2023.10.13 val PER: 0.1939
2026-01-03 22:43:56,964: t15.2023.10.15 val PER: 0.1516
2026-01-03 22:43:56,964: t15.2023.10.20 val PER: 0.1879
2026-01-03 22:43:56,964: t15.2023.10.22 val PER: 0.1114
2026-01-03 22:43:56,965: t15.2023.11.03 val PER: 0.1811
2026-01-03 22:43:56,965: t15.2023.11.04 val PER: 0.0273
2026-01-03 22:43:56,965: t15.2023.11.17 val PER: 0.0404
2026-01-03 22:43:56,965: t15.2023.11.19 val PER: 0.0359
2026-01-03 22:43:56,965: t15.2023.11.26 val PER: 0.1196
2026-01-03 22:43:56,965: t15.2023.12.03 val PER: 0.1103
2026-01-03 22:43:56,965: t15.2023.12.08 val PER: 0.1005
2026-01-03 22:43:56,965: t15.2023.12.10 val PER: 0.0867
2026-01-03 22:43:56,965: t15.2023.12.17 val PER: 0.1362
2026-01-03 22:43:56,965: t15.2023.12.29 val PER: 0.1187
2026-01-03 22:43:56,966: t15.2024.02.25 val PER: 0.1081
2026-01-03 22:43:56,966: t15.2024.03.08 val PER: 0.2134
2026-01-03 22:43:56,966: t15.2024.03.15 val PER: 0.2039
2026-01-03 22:43:56,966: t15.2024.03.17 val PER: 0.1339
2026-01-03 22:43:56,966: t15.2024.05.10 val PER: 0.1620
2026-01-03 22:43:56,966: t15.2024.06.14 val PER: 0.1703
2026-01-03 22:43:56,966: t15.2024.07.19 val PER: 0.2367
2026-01-03 22:43:56,966: t15.2024.07.21 val PER: 0.0966
2026-01-03 22:43:56,966: t15.2024.07.28 val PER: 0.1316
2026-01-03 22:43:56,966: t15.2025.01.10 val PER: 0.2906
2026-01-03 22:43:56,966: t15.2025.01.12 val PER: 0.1540
2026-01-03 22:43:56,966: t15.2025.03.14 val PER: 0.3402
2026-01-03 22:43:56,967: t15.2025.03.16 val PER: 0.1780
2026-01-03 22:43:56,967: t15.2025.03.30 val PER: 0.2931
2026-01-03 22:43:56,967: t15.2025.04.13 val PER: 0.1983
2026-01-03 22:43:56,967: New best val WER(1gram) 44.42% --> 43.15%
2026-01-03 22:43:56,967: Checkpointing model
2026-01-03 22:43:57,588: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:43:57,852: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_16500
2026-01-03 22:44:06,816: Train batch 16600: loss: 8.41 grad norm: 51.52 time: 0.052
2026-01-03 22:44:24,582: Train batch 16800: loss: 16.85 grad norm: 71.74 time: 0.062
2026-01-03 22:44:41,917: Train batch 17000: loss: 8.20 grad norm: 48.60 time: 0.081
2026-01-03 22:44:41,917: Running test after training batch: 17000
2026-01-03 22:44:42,010: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:44:46,676: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 22:44:46,711: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 22:44:48,607: Val batch 17000: PER (avg): 0.1473 CTC Loss (avg): 15.1737 WER(1gram): 45.69% (n=64) time: 6.690
2026-01-03 22:44:48,608: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-03 22:44:48,608: t15.2023.08.13 val PER: 0.1081
2026-01-03 22:44:48,608: t15.2023.08.18 val PER: 0.0981
2026-01-03 22:44:48,608: t15.2023.08.20 val PER: 0.1017
2026-01-03 22:44:48,608: t15.2023.08.25 val PER: 0.1009
2026-01-03 22:44:48,608: t15.2023.08.27 val PER: 0.1817
2026-01-03 22:44:48,608: t15.2023.09.01 val PER: 0.0731
2026-01-03 22:44:48,608: t15.2023.09.03 val PER: 0.1639
2026-01-03 22:44:48,608: t15.2023.09.24 val PER: 0.1286
2026-01-03 22:44:48,608: t15.2023.09.29 val PER: 0.1302
2026-01-03 22:44:48,608: t15.2023.10.01 val PER: 0.1684
2026-01-03 22:44:48,608: t15.2023.10.06 val PER: 0.0807
2026-01-03 22:44:48,608: t15.2023.10.08 val PER: 0.2503
2026-01-03 22:44:48,609: t15.2023.10.13 val PER: 0.1994
2026-01-03 22:44:48,609: t15.2023.10.15 val PER: 0.1477
2026-01-03 22:44:48,609: t15.2023.10.20 val PER: 0.1779
2026-01-03 22:44:48,609: t15.2023.10.22 val PER: 0.1102
2026-01-03 22:44:48,609: t15.2023.11.03 val PER: 0.1777
2026-01-03 22:44:48,609: t15.2023.11.04 val PER: 0.0307
2026-01-03 22:44:48,609: t15.2023.11.17 val PER: 0.0420
2026-01-03 22:44:48,609: t15.2023.11.19 val PER: 0.0379
2026-01-03 22:44:48,609: t15.2023.11.26 val PER: 0.1152
2026-01-03 22:44:48,609: t15.2023.12.03 val PER: 0.1134
2026-01-03 22:44:48,609: t15.2023.12.08 val PER: 0.0985
2026-01-03 22:44:48,609: t15.2023.12.10 val PER: 0.0907
2026-01-03 22:44:48,609: t15.2023.12.17 val PER: 0.1331
2026-01-03 22:44:48,609: t15.2023.12.29 val PER: 0.1229
2026-01-03 22:44:48,610: t15.2024.02.25 val PER: 0.0941
2026-01-03 22:44:48,610: t15.2024.03.08 val PER: 0.2105
2026-01-03 22:44:48,610: t15.2024.03.15 val PER: 0.2020
2026-01-03 22:44:48,610: t15.2024.03.17 val PER: 0.1388
2026-01-03 22:44:48,610: t15.2024.05.10 val PER: 0.1605
2026-01-03 22:44:48,610: t15.2024.06.14 val PER: 0.1609
2026-01-03 22:44:48,610: t15.2024.07.19 val PER: 0.2347
2026-01-03 22:44:48,610: t15.2024.07.21 val PER: 0.0945
2026-01-03 22:44:48,610: t15.2024.07.28 val PER: 0.1301
2026-01-03 22:44:48,610: t15.2025.01.10 val PER: 0.2879
2026-01-03 22:44:48,610: t15.2025.01.12 val PER: 0.1463
2026-01-03 22:44:48,611: t15.2025.03.14 val PER: 0.3343
2026-01-03 22:44:48,611: t15.2025.03.16 val PER: 0.1780
2026-01-03 22:44:48,611: t15.2025.03.30 val PER: 0.2908
2026-01-03 22:44:48,611: t15.2025.04.13 val PER: 0.2011
2026-01-03 22:44:48,863: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_17000
2026-01-03 22:45:05,852: Train batch 17200: loss: 9.85 grad norm: 50.79 time: 0.084
2026-01-03 22:45:23,100: Train batch 17400: loss: 12.00 grad norm: 59.95 time: 0.072
2026-01-03 22:45:32,242: Running test after training batch: 17500
2026-01-03 22:45:32,406: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:45:37,093: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 22:45:37,128: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost get
2026-01-03 22:45:39,010: Val batch 17500: PER (avg): 0.1474 CTC Loss (avg): 15.1040 WER(1gram): 45.43% (n=64) time: 6.768
2026-01-03 22:45:39,010: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-03 22:45:39,010: t15.2023.08.13 val PER: 0.1071
2026-01-03 22:45:39,010: t15.2023.08.18 val PER: 0.0997
2026-01-03 22:45:39,010: t15.2023.08.20 val PER: 0.1033
2026-01-03 22:45:39,011: t15.2023.08.25 val PER: 0.0919
2026-01-03 22:45:39,011: t15.2023.08.27 val PER: 0.1833
2026-01-03 22:45:39,011: t15.2023.09.01 val PER: 0.0731
2026-01-03 22:45:39,011: t15.2023.09.03 val PER: 0.1591
2026-01-03 22:45:39,011: t15.2023.09.24 val PER: 0.1347
2026-01-03 22:45:39,011: t15.2023.09.29 val PER: 0.1283
2026-01-03 22:45:39,011: t15.2023.10.01 val PER: 0.1717
2026-01-03 22:45:39,011: t15.2023.10.06 val PER: 0.0818
2026-01-03 22:45:39,011: t15.2023.10.08 val PER: 0.2571
2026-01-03 22:45:39,011: t15.2023.10.13 val PER: 0.1877
2026-01-03 22:45:39,011: t15.2023.10.15 val PER: 0.1510
2026-01-03 22:45:39,011: t15.2023.10.20 val PER: 0.2013
2026-01-03 22:45:39,011: t15.2023.10.22 val PER: 0.1091
2026-01-03 22:45:39,011: t15.2023.11.03 val PER: 0.1710
2026-01-03 22:45:39,011: t15.2023.11.04 val PER: 0.0307
2026-01-03 22:45:39,012: t15.2023.11.17 val PER: 0.0404
2026-01-03 22:45:39,012: t15.2023.11.19 val PER: 0.0319
2026-01-03 22:45:39,012: t15.2023.11.26 val PER: 0.1188
2026-01-03 22:45:39,012: t15.2023.12.03 val PER: 0.1134
2026-01-03 22:45:39,012: t15.2023.12.08 val PER: 0.0992
2026-01-03 22:45:39,012: t15.2023.12.10 val PER: 0.0907
2026-01-03 22:45:39,012: t15.2023.12.17 val PER: 0.1299
2026-01-03 22:45:39,012: t15.2023.12.29 val PER: 0.1235
2026-01-03 22:45:39,012: t15.2024.02.25 val PER: 0.0983
2026-01-03 22:45:39,012: t15.2024.03.08 val PER: 0.2219
2026-01-03 22:45:39,012: t15.2024.03.15 val PER: 0.1976
2026-01-03 22:45:39,012: t15.2024.03.17 val PER: 0.1374
2026-01-03 22:45:39,012: t15.2024.05.10 val PER: 0.1709
2026-01-03 22:45:39,012: t15.2024.06.14 val PER: 0.1688
2026-01-03 22:45:39,012: t15.2024.07.19 val PER: 0.2235
2026-01-03 22:45:39,012: t15.2024.07.21 val PER: 0.0924
2026-01-03 22:45:39,013: t15.2024.07.28 val PER: 0.1294
2026-01-03 22:45:39,013: t15.2025.01.10 val PER: 0.2824
2026-01-03 22:45:39,013: t15.2025.01.12 val PER: 0.1470
2026-01-03 22:45:39,013: t15.2025.03.14 val PER: 0.3447
2026-01-03 22:45:39,013: t15.2025.03.16 val PER: 0.1793
2026-01-03 22:45:39,013: t15.2025.03.30 val PER: 0.2943
2026-01-03 22:45:39,013: t15.2025.04.13 val PER: 0.2168
2026-01-03 22:45:39,271: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_17500
2026-01-03 22:45:48,227: Train batch 17600: loss: 10.12 grad norm: 50.56 time: 0.051
2026-01-03 22:46:05,386: Train batch 17800: loss: 6.33 grad norm: 47.66 time: 0.041
2026-01-03 22:46:22,917: Train batch 18000: loss: 10.94 grad norm: 63.89 time: 0.061
2026-01-03 22:46:22,917: Running test after training batch: 18000
2026-01-03 22:46:23,067: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:46:27,763: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 22:46:27,799: WER debug example
  GT : how does it keep the cost down
  PR : houde dusts it keep the cost get
2026-01-03 22:46:29,742: Val batch 18000: PER (avg): 0.1466 CTC Loss (avg): 15.1218 WER(1gram): 45.18% (n=64) time: 6.825
2026-01-03 22:46:29,743: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-03 22:46:29,743: t15.2023.08.13 val PER: 0.1071
2026-01-03 22:46:29,743: t15.2023.08.18 val PER: 0.0964
2026-01-03 22:46:29,743: t15.2023.08.20 val PER: 0.1048
2026-01-03 22:46:29,743: t15.2023.08.25 val PER: 0.0889
2026-01-03 22:46:29,743: t15.2023.08.27 val PER: 0.1929
2026-01-03 22:46:29,744: t15.2023.09.01 val PER: 0.0747
2026-01-03 22:46:29,744: t15.2023.09.03 val PER: 0.1615
2026-01-03 22:46:29,744: t15.2023.09.24 val PER: 0.1335
2026-01-03 22:46:29,744: t15.2023.09.29 val PER: 0.1257
2026-01-03 22:46:29,744: t15.2023.10.01 val PER: 0.1717
2026-01-03 22:46:29,744: t15.2023.10.06 val PER: 0.0829
2026-01-03 22:46:29,744: t15.2023.10.08 val PER: 0.2571
2026-01-03 22:46:29,744: t15.2023.10.13 val PER: 0.1963
2026-01-03 22:46:29,744: t15.2023.10.15 val PER: 0.1523
2026-01-03 22:46:29,745: t15.2023.10.20 val PER: 0.1846
2026-01-03 22:46:29,745: t15.2023.10.22 val PER: 0.1047
2026-01-03 22:46:29,745: t15.2023.11.03 val PER: 0.1750
2026-01-03 22:46:29,745: t15.2023.11.04 val PER: 0.0307
2026-01-03 22:46:29,745: t15.2023.11.17 val PER: 0.0373
2026-01-03 22:46:29,745: t15.2023.11.19 val PER: 0.0319
2026-01-03 22:46:29,745: t15.2023.11.26 val PER: 0.1138
2026-01-03 22:46:29,745: t15.2023.12.03 val PER: 0.1103
2026-01-03 22:46:29,745: t15.2023.12.08 val PER: 0.0999
2026-01-03 22:46:29,745: t15.2023.12.10 val PER: 0.0907
2026-01-03 22:46:29,746: t15.2023.12.17 val PER: 0.1310
2026-01-03 22:46:29,746: t15.2023.12.29 val PER: 0.1222
2026-01-03 22:46:29,746: t15.2024.02.25 val PER: 0.0997
2026-01-03 22:46:29,746: t15.2024.03.08 val PER: 0.2248
2026-01-03 22:46:29,746: t15.2024.03.15 val PER: 0.1995
2026-01-03 22:46:29,746: t15.2024.03.17 val PER: 0.1367
2026-01-03 22:46:29,746: t15.2024.05.10 val PER: 0.1590
2026-01-03 22:46:29,746: t15.2024.06.14 val PER: 0.1577
2026-01-03 22:46:29,746: t15.2024.07.19 val PER: 0.2261
2026-01-03 22:46:29,746: t15.2024.07.21 val PER: 0.0917
2026-01-03 22:46:29,747: t15.2024.07.28 val PER: 0.1316
2026-01-03 22:46:29,747: t15.2025.01.10 val PER: 0.2879
2026-01-03 22:46:29,747: t15.2025.01.12 val PER: 0.1424
2026-01-03 22:46:29,747: t15.2025.03.14 val PER: 0.3373
2026-01-03 22:46:29,747: t15.2025.03.16 val PER: 0.1780
2026-01-03 22:46:29,747: t15.2025.03.30 val PER: 0.2862
2026-01-03 22:46:29,747: t15.2025.04.13 val PER: 0.1997
2026-01-03 22:46:30,000: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_18000
2026-01-03 22:46:47,732: Train batch 18200: loss: 8.28 grad norm: 50.88 time: 0.074
2026-01-03 22:47:05,419: Train batch 18400: loss: 4.98 grad norm: 44.10 time: 0.058
2026-01-03 22:47:14,394: Running test after training batch: 18500
2026-01-03 22:47:14,532: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:47:19,209: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 22:47:19,244: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost get
2026-01-03 22:47:21,182: Val batch 18500: PER (avg): 0.1467 CTC Loss (avg): 15.1073 WER(1gram): 43.91% (n=64) time: 6.788
2026-01-03 22:47:21,182: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-03 22:47:21,182: t15.2023.08.13 val PER: 0.1081
2026-01-03 22:47:21,182: t15.2023.08.18 val PER: 0.0989
2026-01-03 22:47:21,183: t15.2023.08.20 val PER: 0.1017
2026-01-03 22:47:21,183: t15.2023.08.25 val PER: 0.0919
2026-01-03 22:47:21,183: t15.2023.08.27 val PER: 0.1817
2026-01-03 22:47:21,183: t15.2023.09.01 val PER: 0.0755
2026-01-03 22:47:21,183: t15.2023.09.03 val PER: 0.1580
2026-01-03 22:47:21,183: t15.2023.09.24 val PER: 0.1262
2026-01-03 22:47:21,183: t15.2023.09.29 val PER: 0.1283
2026-01-03 22:47:21,184: t15.2023.10.01 val PER: 0.1711
2026-01-03 22:47:21,184: t15.2023.10.06 val PER: 0.0861
2026-01-03 22:47:21,184: t15.2023.10.08 val PER: 0.2558
2026-01-03 22:47:21,184: t15.2023.10.13 val PER: 0.1908
2026-01-03 22:47:21,184: t15.2023.10.15 val PER: 0.1510
2026-01-03 22:47:21,184: t15.2023.10.20 val PER: 0.1980
2026-01-03 22:47:21,184: t15.2023.10.22 val PER: 0.1091
2026-01-03 22:47:21,184: t15.2023.11.03 val PER: 0.1737
2026-01-03 22:47:21,184: t15.2023.11.04 val PER: 0.0239
2026-01-03 22:47:21,185: t15.2023.11.17 val PER: 0.0420
2026-01-03 22:47:21,185: t15.2023.11.19 val PER: 0.0319
2026-01-03 22:47:21,185: t15.2023.11.26 val PER: 0.1138
2026-01-03 22:47:21,185: t15.2023.12.03 val PER: 0.1124
2026-01-03 22:47:21,185: t15.2023.12.08 val PER: 0.0945
2026-01-03 22:47:21,185: t15.2023.12.10 val PER: 0.0867
2026-01-03 22:47:21,185: t15.2023.12.17 val PER: 0.1258
2026-01-03 22:47:21,185: t15.2023.12.29 val PER: 0.1167
2026-01-03 22:47:21,185: t15.2024.02.25 val PER: 0.1011
2026-01-03 22:47:21,185: t15.2024.03.08 val PER: 0.2248
2026-01-03 22:47:21,185: t15.2024.03.15 val PER: 0.2001
2026-01-03 22:47:21,185: t15.2024.03.17 val PER: 0.1374
2026-01-03 22:47:21,185: t15.2024.05.10 val PER: 0.1575
2026-01-03 22:47:21,186: t15.2024.06.14 val PER: 0.1719
2026-01-03 22:47:21,186: t15.2024.07.19 val PER: 0.2307
2026-01-03 22:47:21,186: t15.2024.07.21 val PER: 0.0910
2026-01-03 22:47:21,186: t15.2024.07.28 val PER: 0.1331
2026-01-03 22:47:21,186: t15.2025.01.10 val PER: 0.2851
2026-01-03 22:47:21,186: t15.2025.01.12 val PER: 0.1432
2026-01-03 22:47:21,186: t15.2025.03.14 val PER: 0.3462
2026-01-03 22:47:21,186: t15.2025.03.16 val PER: 0.1819
2026-01-03 22:47:21,186: t15.2025.03.30 val PER: 0.2851
2026-01-03 22:47:21,186: t15.2025.04.13 val PER: 0.2126
2026-01-03 22:47:21,429: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_18500
2026-01-03 22:47:29,961: Train batch 18600: loss: 12.32 grad norm: 63.62 time: 0.067
2026-01-03 22:47:47,026: Train batch 18800: loss: 8.33 grad norm: 50.74 time: 0.064
2026-01-03 22:48:04,844: Train batch 19000: loss: 8.04 grad norm: 45.11 time: 0.064
2026-01-03 22:48:04,844: Running test after training batch: 19000
2026-01-03 22:48:04,970: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:48:09,834: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 22:48:09,871: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 22:48:11,846: Val batch 19000: PER (avg): 0.1463 CTC Loss (avg): 15.0819 WER(1gram): 44.16% (n=64) time: 7.002
2026-01-03 22:48:11,847: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-03 22:48:11,847: t15.2023.08.13 val PER: 0.1071
2026-01-03 22:48:11,847: t15.2023.08.18 val PER: 0.0956
2026-01-03 22:48:11,847: t15.2023.08.20 val PER: 0.1048
2026-01-03 22:48:11,847: t15.2023.08.25 val PER: 0.0828
2026-01-03 22:48:11,847: t15.2023.08.27 val PER: 0.1817
2026-01-03 22:48:11,847: t15.2023.09.01 val PER: 0.0763
2026-01-03 22:48:11,847: t15.2023.09.03 val PER: 0.1615
2026-01-03 22:48:11,847: t15.2023.09.24 val PER: 0.1299
2026-01-03 22:48:11,847: t15.2023.09.29 val PER: 0.1244
2026-01-03 22:48:11,847: t15.2023.10.01 val PER: 0.1724
2026-01-03 22:48:11,847: t15.2023.10.06 val PER: 0.0797
2026-01-03 22:48:11,848: t15.2023.10.08 val PER: 0.2503
2026-01-03 22:48:11,848: t15.2023.10.13 val PER: 0.1893
2026-01-03 22:48:11,848: t15.2023.10.15 val PER: 0.1496
2026-01-03 22:48:11,848: t15.2023.10.20 val PER: 0.1846
2026-01-03 22:48:11,848: t15.2023.10.22 val PER: 0.1080
2026-01-03 22:48:11,848: t15.2023.11.03 val PER: 0.1784
2026-01-03 22:48:11,848: t15.2023.11.04 val PER: 0.0273
2026-01-03 22:48:11,849: t15.2023.11.17 val PER: 0.0435
2026-01-03 22:48:11,849: t15.2023.11.19 val PER: 0.0299
2026-01-03 22:48:11,849: t15.2023.11.26 val PER: 0.1167
2026-01-03 22:48:11,849: t15.2023.12.03 val PER: 0.1103
2026-01-03 22:48:11,849: t15.2023.12.08 val PER: 0.0952
2026-01-03 22:48:11,849: t15.2023.12.10 val PER: 0.0907
2026-01-03 22:48:11,849: t15.2023.12.17 val PER: 0.1310
2026-01-03 22:48:11,849: t15.2023.12.29 val PER: 0.1229
2026-01-03 22:48:11,849: t15.2024.02.25 val PER: 0.1011
2026-01-03 22:48:11,849: t15.2024.03.08 val PER: 0.2262
2026-01-03 22:48:11,849: t15.2024.03.15 val PER: 0.1939
2026-01-03 22:48:11,849: t15.2024.03.17 val PER: 0.1388
2026-01-03 22:48:11,849: t15.2024.05.10 val PER: 0.1590
2026-01-03 22:48:11,849: t15.2024.06.14 val PER: 0.1703
2026-01-03 22:48:11,849: t15.2024.07.19 val PER: 0.2228
2026-01-03 22:48:11,850: t15.2024.07.21 val PER: 0.0945
2026-01-03 22:48:11,850: t15.2024.07.28 val PER: 0.1324
2026-01-03 22:48:11,850: t15.2025.01.10 val PER: 0.2865
2026-01-03 22:48:11,850: t15.2025.01.12 val PER: 0.1393
2026-01-03 22:48:11,850: t15.2025.03.14 val PER: 0.3447
2026-01-03 22:48:11,850: t15.2025.03.16 val PER: 0.1780
2026-01-03 22:48:11,850: t15.2025.03.30 val PER: 0.2862
2026-01-03 22:48:11,850: t15.2025.04.13 val PER: 0.2111
2026-01-03 22:48:12,109: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_19000
2026-01-03 22:48:29,216: Train batch 19200: loss: 5.89 grad norm: 46.12 time: 0.063
2026-01-03 22:48:46,303: Train batch 19400: loss: 4.96 grad norm: 36.79 time: 0.052
2026-01-03 22:48:54,886: Running test after training batch: 19500
2026-01-03 22:48:55,027: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:48:59,733: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 22:48:59,769: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost get
2026-01-03 22:49:01,739: Val batch 19500: PER (avg): 0.1469 CTC Loss (avg): 15.0575 WER(1gram): 44.42% (n=64) time: 6.853
2026-01-03 22:49:01,739: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-03 22:49:01,740: t15.2023.08.13 val PER: 0.1050
2026-01-03 22:49:01,740: t15.2023.08.18 val PER: 0.0981
2026-01-03 22:49:01,740: t15.2023.08.20 val PER: 0.1048
2026-01-03 22:49:01,740: t15.2023.08.25 val PER: 0.0873
2026-01-03 22:49:01,740: t15.2023.08.27 val PER: 0.1785
2026-01-03 22:49:01,740: t15.2023.09.01 val PER: 0.0771
2026-01-03 22:49:01,741: t15.2023.09.03 val PER: 0.1639
2026-01-03 22:49:01,741: t15.2023.09.24 val PER: 0.1311
2026-01-03 22:49:01,741: t15.2023.09.29 val PER: 0.1276
2026-01-03 22:49:01,741: t15.2023.10.01 val PER: 0.1777
2026-01-03 22:49:01,741: t15.2023.10.06 val PER: 0.0840
2026-01-03 22:49:01,742: t15.2023.10.08 val PER: 0.2503
2026-01-03 22:49:01,742: t15.2023.10.13 val PER: 0.1947
2026-01-03 22:49:01,742: t15.2023.10.15 val PER: 0.1490
2026-01-03 22:49:01,742: t15.2023.10.20 val PER: 0.1980
2026-01-03 22:49:01,742: t15.2023.10.22 val PER: 0.1114
2026-01-03 22:49:01,742: t15.2023.11.03 val PER: 0.1764
2026-01-03 22:49:01,742: t15.2023.11.04 val PER: 0.0307
2026-01-03 22:49:01,743: t15.2023.11.17 val PER: 0.0435
2026-01-03 22:49:01,743: t15.2023.11.19 val PER: 0.0319
2026-01-03 22:49:01,743: t15.2023.11.26 val PER: 0.1159
2026-01-03 22:49:01,743: t15.2023.12.03 val PER: 0.1134
2026-01-03 22:49:01,743: t15.2023.12.08 val PER: 0.0979
2026-01-03 22:49:01,743: t15.2023.12.10 val PER: 0.0854
2026-01-03 22:49:01,743: t15.2023.12.17 val PER: 0.1279
2026-01-03 22:49:01,743: t15.2023.12.29 val PER: 0.1194
2026-01-03 22:49:01,743: t15.2024.02.25 val PER: 0.1011
2026-01-03 22:49:01,744: t15.2024.03.08 val PER: 0.2205
2026-01-03 22:49:01,744: t15.2024.03.15 val PER: 0.1920
2026-01-03 22:49:01,744: t15.2024.03.17 val PER: 0.1325
2026-01-03 22:49:01,744: t15.2024.05.10 val PER: 0.1620
2026-01-03 22:49:01,744: t15.2024.06.14 val PER: 0.1688
2026-01-03 22:49:01,744: t15.2024.07.19 val PER: 0.2287
2026-01-03 22:49:01,744: t15.2024.07.21 val PER: 0.0890
2026-01-03 22:49:01,744: t15.2024.07.28 val PER: 0.1309
2026-01-03 22:49:01,744: t15.2025.01.10 val PER: 0.2893
2026-01-03 22:49:01,745: t15.2025.01.12 val PER: 0.1486
2026-01-03 22:49:01,745: t15.2025.03.14 val PER: 0.3447
2026-01-03 22:49:01,745: t15.2025.03.16 val PER: 0.1728
2026-01-03 22:49:01,745: t15.2025.03.30 val PER: 0.2874
2026-01-03 22:49:01,745: t15.2025.04.13 val PER: 0.2154
2026-01-03 22:49:02,003: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_19500
2026-01-03 22:49:10,454: Train batch 19600: loss: 7.54 grad norm: 47.07 time: 0.057
2026-01-03 22:49:27,405: Train batch 19800: loss: 7.64 grad norm: 50.17 time: 0.055
2026-01-03 22:49:44,354: Running test after training batch: 19999
2026-01-03 22:49:44,443: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:49:49,057: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 22:49:49,093: WER debug example
  GT : how does it keep the cost down
  PR : houde dusts it keep the cost get
2026-01-03 22:49:51,091: Val batch 19999: PER (avg): 0.1461 CTC Loss (avg): 15.0674 WER(1gram): 44.92% (n=64) time: 6.737
2026-01-03 22:49:51,091: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-03 22:49:51,091: t15.2023.08.13 val PER: 0.1071
2026-01-03 22:49:51,091: t15.2023.08.18 val PER: 0.0989
2026-01-03 22:49:51,091: t15.2023.08.20 val PER: 0.1017
2026-01-03 22:49:51,091: t15.2023.08.25 val PER: 0.0904
2026-01-03 22:49:51,092: t15.2023.08.27 val PER: 0.1736
2026-01-03 22:49:51,092: t15.2023.09.01 val PER: 0.0755
2026-01-03 22:49:51,092: t15.2023.09.03 val PER: 0.1568
2026-01-03 22:49:51,092: t15.2023.09.24 val PER: 0.1335
2026-01-03 22:49:51,092: t15.2023.09.29 val PER: 0.1251
2026-01-03 22:49:51,092: t15.2023.10.01 val PER: 0.1750
2026-01-03 22:49:51,092: t15.2023.10.06 val PER: 0.0840
2026-01-03 22:49:51,092: t15.2023.10.08 val PER: 0.2503
2026-01-03 22:49:51,092: t15.2023.10.13 val PER: 0.1963
2026-01-03 22:49:51,093: t15.2023.10.15 val PER: 0.1477
2026-01-03 22:49:51,093: t15.2023.10.20 val PER: 0.1980
2026-01-03 22:49:51,093: t15.2023.10.22 val PER: 0.1091
2026-01-03 22:49:51,093: t15.2023.11.03 val PER: 0.1750
2026-01-03 22:49:51,094: t15.2023.11.04 val PER: 0.0273
2026-01-03 22:49:51,094: t15.2023.11.17 val PER: 0.0404
2026-01-03 22:49:51,094: t15.2023.11.19 val PER: 0.0359
2026-01-03 22:49:51,094: t15.2023.11.26 val PER: 0.1109
2026-01-03 22:49:51,094: t15.2023.12.03 val PER: 0.1103
2026-01-03 22:49:51,094: t15.2023.12.08 val PER: 0.0979
2026-01-03 22:49:51,094: t15.2023.12.10 val PER: 0.0854
2026-01-03 22:49:51,095: t15.2023.12.17 val PER: 0.1247
2026-01-03 22:49:51,095: t15.2023.12.29 val PER: 0.1249
2026-01-03 22:49:51,095: t15.2024.02.25 val PER: 0.1039
2026-01-03 22:49:51,095: t15.2024.03.08 val PER: 0.2148
2026-01-03 22:49:51,095: t15.2024.03.15 val PER: 0.1951
2026-01-03 22:49:51,095: t15.2024.03.17 val PER: 0.1311
2026-01-03 22:49:51,095: t15.2024.05.10 val PER: 0.1575
2026-01-03 22:49:51,096: t15.2024.06.14 val PER: 0.1640
2026-01-03 22:49:51,096: t15.2024.07.19 val PER: 0.2268
2026-01-03 22:49:51,096: t15.2024.07.21 val PER: 0.0910
2026-01-03 22:49:51,096: t15.2024.07.28 val PER: 0.1294
2026-01-03 22:49:51,096: t15.2025.01.10 val PER: 0.2879
2026-01-03 22:49:51,096: t15.2025.01.12 val PER: 0.1463
2026-01-03 22:49:51,096: t15.2025.03.14 val PER: 0.3491
2026-01-03 22:49:51,096: t15.2025.03.16 val PER: 0.1728
2026-01-03 22:49:51,096: t15.2025.03.30 val PER: 0.2897
2026-01-03 22:49:51,096: t15.2025.04.13 val PER: 0.2140
2026-01-03 22:49:51,351: Saved model to checkpoint: /tmp/e12511253_b2t_348249/trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_19999
2026-01-03 22:49:51,369: Best avg val PER achieved: 0.14865
2026-01-03 22:49:51,369: Total training time: 34.49 minutes

=== RUN d05.yaml ===
