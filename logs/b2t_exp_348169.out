torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  3 18:57 /tmp/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
PYTHONFAULTHANDLER=1
MAX_JOBS=8
TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_348169
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_348169/torch_extensions
TORCH_CUDA_ARCH_LIST=8.0
WANDB_DIR=/tmp/e12511253_b2t_348169/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib64:/tmp/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  3 18:57 /tmp/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
PYTHONFAULTHANDLER=1
MAX_JOBS=8
TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_348169
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_348169/torch_extensions
TORCH_CUDA_ARCH_LIST=8.0
WANDB_DIR=/tmp/e12511253_b2t_348169/wandb
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/pkgs/cuda-toolkit/lib64/libcudart.so.11.7.60
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/pkgs/cuda-toolkit/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
trained_models -> /tmp/e12511253_b2t_348169/trained_models
==============================================
Job: b2t_exp  ID: 348169
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Global override 2: configs/overrides/disk_conservative.yaml
Folders: configs/experiments/gru/rnn_dropout
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/gru/rnn_dropout ==========
Num configs: 6

=== RUN base.yaml ===
2026-01-03 18:58:04,033: Using device: cuda:0
2026-01-03 18:58:05,882: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-03 18:58:12,601: Using 45 sessions after filtering (from 45).
2026-01-03 18:58:13,040: Using torch.compile (if available)
2026-01-03 18:58:13,041: torch.compile not available (torch<2.0). Skipping.
2026-01-03 18:58:13,041: Initialized RNN decoding model
2026-01-03 18:58:13,041: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 18:58:13,041: Model has 44,907,305 parameters
2026-01-03 18:58:13,041: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 18:58:16,840: Successfully initialized datasets
2026-01-03 18:58:16,841: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 18:58:18,443: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.330
2026-01-03 18:58:18,444: Running test after training batch: 0
2026-01-03 18:58:18,554: WER debug GT example: You can see the code at this point as well.
2026-01-03 18:58:23,936: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-03 18:58:24,645: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-03 18:58:58,253: Val batch 0: PER (avg): 1.4306 CTC Loss (avg): 633.3639 WER(1gram): 100.00% (n=64) time: 39.809
2026-01-03 18:58:58,254: WER lens: avg_true_words=6.16 avg_pred_words=1.78 max_pred_words=4
2026-01-03 18:58:58,254: t15.2023.08.13 val PER: 1.3046
2026-01-03 18:58:58,254: t15.2023.08.18 val PER: 1.4283
2026-01-03 18:58:58,254: t15.2023.08.20 val PER: 1.3018
2026-01-03 18:58:58,254: t15.2023.08.25 val PER: 1.3358
2026-01-03 18:58:58,254: t15.2023.08.27 val PER: 1.2524
2026-01-03 18:58:58,254: t15.2023.09.01 val PER: 1.4529
2026-01-03 18:58:58,254: t15.2023.09.03 val PER: 1.3171
2026-01-03 18:58:58,254: t15.2023.09.24 val PER: 1.5400
2026-01-03 18:58:58,254: t15.2023.09.29 val PER: 1.4671
2026-01-03 18:58:58,254: t15.2023.10.01 val PER: 1.2173
2026-01-03 18:58:58,254: t15.2023.10.06 val PER: 1.4909
2026-01-03 18:58:58,255: t15.2023.10.08 val PER: 1.1908
2026-01-03 18:58:58,255: t15.2023.10.13 val PER: 1.3933
2026-01-03 18:58:58,255: t15.2023.10.15 val PER: 1.3869
2026-01-03 18:58:58,255: t15.2023.10.20 val PER: 1.5000
2026-01-03 18:58:58,255: t15.2023.10.22 val PER: 1.3886
2026-01-03 18:58:58,255: t15.2023.11.03 val PER: 1.5977
2026-01-03 18:58:58,255: t15.2023.11.04 val PER: 2.0444
2026-01-03 18:58:58,255: t15.2023.11.17 val PER: 1.9580
2026-01-03 18:58:58,255: t15.2023.11.19 val PER: 1.6766
2026-01-03 18:58:58,255: t15.2023.11.26 val PER: 1.5406
2026-01-03 18:58:58,255: t15.2023.12.03 val PER: 1.4338
2026-01-03 18:58:58,256: t15.2023.12.08 val PER: 1.4501
2026-01-03 18:58:58,256: t15.2023.12.10 val PER: 1.6991
2026-01-03 18:58:58,256: t15.2023.12.17 val PER: 1.3087
2026-01-03 18:58:58,256: t15.2023.12.29 val PER: 1.4139
2026-01-03 18:58:58,256: t15.2024.02.25 val PER: 1.4199
2026-01-03 18:58:58,256: t15.2024.03.08 val PER: 1.3243
2026-01-03 18:58:58,256: t15.2024.03.15 val PER: 1.3177
2026-01-03 18:58:58,256: t15.2024.03.17 val PER: 1.4017
2026-01-03 18:58:58,256: t15.2024.05.10 val PER: 1.3284
2026-01-03 18:58:58,256: t15.2024.06.14 val PER: 1.5363
2026-01-03 18:58:58,256: t15.2024.07.19 val PER: 1.0811
2026-01-03 18:58:58,256: t15.2024.07.21 val PER: 1.6317
2026-01-03 18:58:58,256: t15.2024.07.28 val PER: 1.6588
2026-01-03 18:58:58,256: t15.2025.01.10 val PER: 1.0868
2026-01-03 18:58:58,256: t15.2025.01.12 val PER: 1.7644
2026-01-03 18:58:58,256: t15.2025.03.14 val PER: 1.0399
2026-01-03 18:58:58,257: t15.2025.03.16 val PER: 1.6217
2026-01-03 18:58:58,257: t15.2025.03.30 val PER: 1.2920
2026-01-03 18:58:58,257: t15.2025.04.13 val PER: 1.5877
2026-01-03 18:58:58,258: New best val WER(1gram) inf% --> 100.00%
2026-01-03 18:58:58,498: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_0
2026-01-03 18:59:15,613: Train batch 200: loss: 80.59 grad norm: 80.20 time: 0.054
2026-01-03 18:59:32,539: Train batch 400: loss: 57.89 grad norm: 93.18 time: 0.063
2026-01-03 18:59:41,251: Running test after training batch: 500
2026-01-03 18:59:41,340: WER debug GT example: You can see the code at this point as well.
2026-01-03 18:59:46,062: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt ooohs thus uhde at this due is aisle
2026-01-03 18:59:46,100: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ou thus as ooohs
2026-01-03 18:59:48,971: Val batch 500: PER (avg): 0.5368 CTC Loss (avg): 58.6759 WER(1gram): 90.61% (n=64) time: 7.720
2026-01-03 18:59:48,972: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=12
2026-01-03 18:59:48,972: t15.2023.08.13 val PER: 0.4865
2026-01-03 18:59:48,972: t15.2023.08.18 val PER: 0.4786
2026-01-03 18:59:48,972: t15.2023.08.20 val PER: 0.4718
2026-01-03 18:59:48,972: t15.2023.08.25 val PER: 0.4669
2026-01-03 18:59:48,972: t15.2023.08.27 val PER: 0.5466
2026-01-03 18:59:48,972: t15.2023.09.01 val PER: 0.4416
2026-01-03 18:59:48,972: t15.2023.09.03 val PER: 0.5107
2026-01-03 18:59:48,972: t15.2023.09.24 val PER: 0.4745
2026-01-03 18:59:48,973: t15.2023.09.29 val PER: 0.4799
2026-01-03 18:59:48,973: t15.2023.10.01 val PER: 0.5396
2026-01-03 18:59:48,973: t15.2023.10.06 val PER: 0.4510
2026-01-03 18:59:48,973: t15.2023.10.08 val PER: 0.5710
2026-01-03 18:59:48,973: t15.2023.10.13 val PER: 0.5811
2026-01-03 18:59:48,973: t15.2023.10.15 val PER: 0.5162
2026-01-03 18:59:48,973: t15.2023.10.20 val PER: 0.4832
2026-01-03 18:59:48,973: t15.2023.10.22 val PER: 0.4655
2026-01-03 18:59:48,973: t15.2023.11.03 val PER: 0.5176
2026-01-03 18:59:48,973: t15.2023.11.04 val PER: 0.2901
2026-01-03 18:59:48,973: t15.2023.11.17 val PER: 0.3919
2026-01-03 18:59:48,973: t15.2023.11.19 val PER: 0.3733
2026-01-03 18:59:48,973: t15.2023.11.26 val PER: 0.5725
2026-01-03 18:59:48,973: t15.2023.12.03 val PER: 0.5095
2026-01-03 18:59:48,974: t15.2023.12.08 val PER: 0.5313
2026-01-03 18:59:48,974: t15.2023.12.10 val PER: 0.4888
2026-01-03 18:59:48,974: t15.2023.12.17 val PER: 0.5852
2026-01-03 18:59:48,974: t15.2023.12.29 val PER: 0.5655
2026-01-03 18:59:48,974: t15.2024.02.25 val PER: 0.4972
2026-01-03 18:59:48,974: t15.2024.03.08 val PER: 0.6302
2026-01-03 18:59:48,974: t15.2024.03.15 val PER: 0.5797
2026-01-03 18:59:48,974: t15.2024.03.17 val PER: 0.5181
2026-01-03 18:59:48,974: t15.2024.05.10 val PER: 0.5632
2026-01-03 18:59:48,974: t15.2024.06.14 val PER: 0.5363
2026-01-03 18:59:48,975: t15.2024.07.19 val PER: 0.6882
2026-01-03 18:59:48,975: t15.2024.07.21 val PER: 0.4979
2026-01-03 18:59:48,975: t15.2024.07.28 val PER: 0.5412
2026-01-03 18:59:48,975: t15.2025.01.10 val PER: 0.7273
2026-01-03 18:59:48,975: t15.2025.01.12 val PER: 0.5620
2026-01-03 18:59:48,975: t15.2025.03.14 val PER: 0.7456
2026-01-03 18:59:48,975: t15.2025.03.16 val PER: 0.6008
2026-01-03 18:59:48,975: t15.2025.03.30 val PER: 0.7092
2026-01-03 18:59:48,975: t15.2025.04.13 val PER: 0.6020
2026-01-03 18:59:48,976: New best val WER(1gram) 100.00% --> 90.61%
2026-01-03 18:59:49,218: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_500
2026-01-03 18:59:57,695: Train batch 600: loss: 51.99 grad norm: 89.00 time: 0.077
2026-01-03 19:00:14,586: Train batch 800: loss: 43.78 grad norm: 86.78 time: 0.056
2026-01-03 19:00:31,769: Train batch 1000: loss: 44.98 grad norm: 77.78 time: 0.065
2026-01-03 19:00:31,770: Running test after training batch: 1000
2026-01-03 19:00:31,912: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:00:36,803: WER debug example
  GT : you can see the code at this point as well
  PR : ude wint ease thus uhde it this ide is while
2026-01-03 19:00:36,835: WER debug example
  GT : how does it keep the cost down
  PR : houde is it ink that wass id
2026-01-03 19:00:38,889: Val batch 1000: PER (avg): 0.4310 CTC Loss (avg): 45.5788 WER(1gram): 86.80% (n=64) time: 7.120
2026-01-03 19:00:38,890: WER lens: avg_true_words=6.16 avg_pred_words=5.69 max_pred_words=12
2026-01-03 19:00:38,890: t15.2023.08.13 val PER: 0.4075
2026-01-03 19:00:38,890: t15.2023.08.18 val PER: 0.3588
2026-01-03 19:00:38,890: t15.2023.08.20 val PER: 0.3678
2026-01-03 19:00:38,890: t15.2023.08.25 val PER: 0.3133
2026-01-03 19:00:38,890: t15.2023.08.27 val PER: 0.4534
2026-01-03 19:00:38,890: t15.2023.09.01 val PER: 0.3239
2026-01-03 19:00:38,890: t15.2023.09.03 val PER: 0.4145
2026-01-03 19:00:38,890: t15.2023.09.24 val PER: 0.3629
2026-01-03 19:00:38,891: t15.2023.09.29 val PER: 0.3906
2026-01-03 19:00:38,891: t15.2023.10.01 val PER: 0.4201
2026-01-03 19:00:38,891: t15.2023.10.06 val PER: 0.3348
2026-01-03 19:00:38,891: t15.2023.10.08 val PER: 0.4668
2026-01-03 19:00:38,891: t15.2023.10.13 val PER: 0.4919
2026-01-03 19:00:38,891: t15.2023.10.15 val PER: 0.4074
2026-01-03 19:00:38,891: t15.2023.10.20 val PER: 0.3960
2026-01-03 19:00:38,891: t15.2023.10.22 val PER: 0.3842
2026-01-03 19:00:38,891: t15.2023.11.03 val PER: 0.4172
2026-01-03 19:00:38,892: t15.2023.11.04 val PER: 0.1843
2026-01-03 19:00:38,892: t15.2023.11.17 val PER: 0.2753
2026-01-03 19:00:38,892: t15.2023.11.19 val PER: 0.2335
2026-01-03 19:00:38,892: t15.2023.11.26 val PER: 0.4616
2026-01-03 19:00:38,892: t15.2023.12.03 val PER: 0.4286
2026-01-03 19:00:38,892: t15.2023.12.08 val PER: 0.4314
2026-01-03 19:00:38,892: t15.2023.12.10 val PER: 0.3601
2026-01-03 19:00:38,892: t15.2023.12.17 val PER: 0.4366
2026-01-03 19:00:38,892: t15.2023.12.29 val PER: 0.4269
2026-01-03 19:00:38,892: t15.2024.02.25 val PER: 0.3876
2026-01-03 19:00:38,892: t15.2024.03.08 val PER: 0.5178
2026-01-03 19:00:38,892: t15.2024.03.15 val PER: 0.4597
2026-01-03 19:00:38,892: t15.2024.03.17 val PER: 0.4219
2026-01-03 19:00:38,892: t15.2024.05.10 val PER: 0.4502
2026-01-03 19:00:38,892: t15.2024.06.14 val PER: 0.4180
2026-01-03 19:00:38,892: t15.2024.07.19 val PER: 0.5669
2026-01-03 19:00:38,892: t15.2024.07.21 val PER: 0.3959
2026-01-03 19:00:38,893: t15.2024.07.28 val PER: 0.4441
2026-01-03 19:00:38,893: t15.2025.01.10 val PER: 0.6501
2026-01-03 19:00:38,893: t15.2025.01.12 val PER: 0.4727
2026-01-03 19:00:38,893: t15.2025.03.14 val PER: 0.6657
2026-01-03 19:00:38,893: t15.2025.03.16 val PER: 0.5013
2026-01-03 19:00:38,893: t15.2025.03.30 val PER: 0.6575
2026-01-03 19:00:38,893: t15.2025.04.13 val PER: 0.5021
2026-01-03 19:00:38,894: New best val WER(1gram) 90.61% --> 86.80%
2026-01-03 19:00:39,143: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_1000
2026-01-03 19:00:56,304: Train batch 1200: loss: 36.49 grad norm: 78.97 time: 0.067
2026-01-03 19:01:13,836: Train batch 1400: loss: 39.20 grad norm: 83.23 time: 0.061
2026-01-03 19:01:22,571: Running test after training batch: 1500
2026-01-03 19:01:22,720: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:01:27,462: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt ease utt owed it this boyde is wheel
2026-01-03 19:01:27,495: WER debug example
  GT : how does it keep the cost down
  PR : houde is it heap that wass it
2026-01-03 19:01:29,233: Val batch 1500: PER (avg): 0.3967 CTC Loss (avg): 39.5982 WER(1gram): 82.99% (n=64) time: 6.662
2026-01-03 19:01:29,233: WER lens: avg_true_words=6.16 avg_pred_words=5.20 max_pred_words=11
2026-01-03 19:01:29,234: t15.2023.08.13 val PER: 0.3576
2026-01-03 19:01:29,234: t15.2023.08.18 val PER: 0.3336
2026-01-03 19:01:29,234: t15.2023.08.20 val PER: 0.3249
2026-01-03 19:01:29,234: t15.2023.08.25 val PER: 0.2771
2026-01-03 19:01:29,234: t15.2023.08.27 val PER: 0.4148
2026-01-03 19:01:29,234: t15.2023.09.01 val PER: 0.2857
2026-01-03 19:01:29,234: t15.2023.09.03 val PER: 0.4026
2026-01-03 19:01:29,234: t15.2023.09.24 val PER: 0.3265
2026-01-03 19:01:29,235: t15.2023.09.29 val PER: 0.3561
2026-01-03 19:01:29,235: t15.2023.10.01 val PER: 0.4055
2026-01-03 19:01:29,235: t15.2023.10.06 val PER: 0.3003
2026-01-03 19:01:29,235: t15.2023.10.08 val PER: 0.4493
2026-01-03 19:01:29,235: t15.2023.10.13 val PER: 0.4523
2026-01-03 19:01:29,235: t15.2023.10.15 val PER: 0.3823
2026-01-03 19:01:29,235: t15.2023.10.20 val PER: 0.3490
2026-01-03 19:01:29,235: t15.2023.10.22 val PER: 0.3252
2026-01-03 19:01:29,235: t15.2023.11.03 val PER: 0.3765
2026-01-03 19:01:29,235: t15.2023.11.04 val PER: 0.1297
2026-01-03 19:01:29,235: t15.2023.11.17 val PER: 0.2395
2026-01-03 19:01:29,235: t15.2023.11.19 val PER: 0.1916
2026-01-03 19:01:29,235: t15.2023.11.26 val PER: 0.4384
2026-01-03 19:01:29,235: t15.2023.12.03 val PER: 0.3960
2026-01-03 19:01:29,236: t15.2023.12.08 val PER: 0.3782
2026-01-03 19:01:29,236: t15.2023.12.10 val PER: 0.3193
2026-01-03 19:01:29,236: t15.2023.12.17 val PER: 0.3825
2026-01-03 19:01:29,236: t15.2023.12.29 val PER: 0.3844
2026-01-03 19:01:29,236: t15.2024.02.25 val PER: 0.3329
2026-01-03 19:01:29,236: t15.2024.03.08 val PER: 0.4666
2026-01-03 19:01:29,236: t15.2024.03.15 val PER: 0.4353
2026-01-03 19:01:29,236: t15.2024.03.17 val PER: 0.3905
2026-01-03 19:01:29,236: t15.2024.05.10 val PER: 0.4042
2026-01-03 19:01:29,236: t15.2024.06.14 val PER: 0.4180
2026-01-03 19:01:29,236: t15.2024.07.19 val PER: 0.5458
2026-01-03 19:01:29,236: t15.2024.07.21 val PER: 0.3662
2026-01-03 19:01:29,236: t15.2024.07.28 val PER: 0.3868
2026-01-03 19:01:29,236: t15.2025.01.10 val PER: 0.6515
2026-01-03 19:01:29,236: t15.2025.01.12 val PER: 0.4365
2026-01-03 19:01:29,236: t15.2025.03.14 val PER: 0.6287
2026-01-03 19:01:29,236: t15.2025.03.16 val PER: 0.4725
2026-01-03 19:01:29,237: t15.2025.03.30 val PER: 0.6368
2026-01-03 19:01:29,237: t15.2025.04.13 val PER: 0.4836
2026-01-03 19:01:29,238: New best val WER(1gram) 86.80% --> 82.99%
2026-01-03 19:01:29,481: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_1500
2026-01-03 19:01:38,231: Train batch 1600: loss: 39.30 grad norm: 82.47 time: 0.063
2026-01-03 19:01:56,182: Train batch 1800: loss: 38.22 grad norm: 70.81 time: 0.088
2026-01-03 19:02:13,773: Train batch 2000: loss: 37.55 grad norm: 79.49 time: 0.066
2026-01-03 19:02:13,774: Running test after training batch: 2000
2026-01-03 19:02:13,893: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:02:18,701: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt ease the code at this and is will
2026-01-03 19:02:18,736: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap thus wass it
2026-01-03 19:02:20,426: Val batch 2000: PER (avg): 0.3494 CTC Loss (avg): 35.4794 WER(1gram): 73.60% (n=64) time: 6.653
2026-01-03 19:02:20,427: WER lens: avg_true_words=6.16 avg_pred_words=5.50 max_pred_words=11
2026-01-03 19:02:20,427: t15.2023.08.13 val PER: 0.3150
2026-01-03 19:02:20,427: t15.2023.08.18 val PER: 0.2850
2026-01-03 19:02:20,427: t15.2023.08.20 val PER: 0.2780
2026-01-03 19:02:20,427: t15.2023.08.25 val PER: 0.2440
2026-01-03 19:02:20,427: t15.2023.08.27 val PER: 0.3762
2026-01-03 19:02:20,428: t15.2023.09.01 val PER: 0.2459
2026-01-03 19:02:20,428: t15.2023.09.03 val PER: 0.3599
2026-01-03 19:02:20,428: t15.2023.09.24 val PER: 0.2694
2026-01-03 19:02:20,428: t15.2023.09.29 val PER: 0.2891
2026-01-03 19:02:20,428: t15.2023.10.01 val PER: 0.3421
2026-01-03 19:02:20,428: t15.2023.10.06 val PER: 0.2562
2026-01-03 19:02:20,428: t15.2023.10.08 val PER: 0.4141
2026-01-03 19:02:20,428: t15.2023.10.13 val PER: 0.3949
2026-01-03 19:02:20,428: t15.2023.10.15 val PER: 0.3223
2026-01-03 19:02:20,428: t15.2023.10.20 val PER: 0.3020
2026-01-03 19:02:20,428: t15.2023.10.22 val PER: 0.2795
2026-01-03 19:02:20,428: t15.2023.11.03 val PER: 0.3345
2026-01-03 19:02:20,428: t15.2023.11.04 val PER: 0.1229
2026-01-03 19:02:20,428: t15.2023.11.17 val PER: 0.1975
2026-01-03 19:02:20,429: t15.2023.11.19 val PER: 0.1677
2026-01-03 19:02:20,429: t15.2023.11.26 val PER: 0.3862
2026-01-03 19:02:20,429: t15.2023.12.03 val PER: 0.3330
2026-01-03 19:02:20,429: t15.2023.12.08 val PER: 0.3189
2026-01-03 19:02:20,429: t15.2023.12.10 val PER: 0.2878
2026-01-03 19:02:20,429: t15.2023.12.17 val PER: 0.3254
2026-01-03 19:02:20,429: t15.2023.12.29 val PER: 0.3439
2026-01-03 19:02:20,429: t15.2024.02.25 val PER: 0.3062
2026-01-03 19:02:20,429: t15.2024.03.08 val PER: 0.4367
2026-01-03 19:02:20,429: t15.2024.03.15 val PER: 0.3777
2026-01-03 19:02:20,430: t15.2024.03.17 val PER: 0.3591
2026-01-03 19:02:20,430: t15.2024.05.10 val PER: 0.3700
2026-01-03 19:02:20,430: t15.2024.06.14 val PER: 0.3612
2026-01-03 19:02:20,430: t15.2024.07.19 val PER: 0.4858
2026-01-03 19:02:20,430: t15.2024.07.21 val PER: 0.3276
2026-01-03 19:02:20,430: t15.2024.07.28 val PER: 0.3434
2026-01-03 19:02:20,430: t15.2025.01.10 val PER: 0.5647
2026-01-03 19:02:20,430: t15.2025.01.12 val PER: 0.4065
2026-01-03 19:02:20,430: t15.2025.03.14 val PER: 0.5695
2026-01-03 19:02:20,430: t15.2025.03.16 val PER: 0.4359
2026-01-03 19:02:20,430: t15.2025.03.30 val PER: 0.5989
2026-01-03 19:02:20,430: t15.2025.04.13 val PER: 0.4437
2026-01-03 19:02:20,431: New best val WER(1gram) 82.99% --> 73.60%
2026-01-03 19:02:20,684: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_2000
2026-01-03 19:02:38,369: Train batch 2200: loss: 32.13 grad norm: 81.37 time: 0.060
2026-01-03 19:02:56,396: Train batch 2400: loss: 32.12 grad norm: 68.16 time: 0.052
2026-01-03 19:03:05,279: Running test after training batch: 2500
2026-01-03 19:03:05,433: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:03:10,295: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt sze the code at this boyde is wheel
2026-01-03 19:03:10,326: WER debug example
  GT : how does it keep the cost down
  PR : houde does it geac the cost it
2026-01-03 19:03:11,990: Val batch 2500: PER (avg): 0.3276 CTC Loss (avg): 32.7024 WER(1gram): 70.30% (n=64) time: 6.710
2026-01-03 19:03:11,990: WER lens: avg_true_words=6.16 avg_pred_words=5.50 max_pred_words=11
2026-01-03 19:03:11,990: t15.2023.08.13 val PER: 0.2994
2026-01-03 19:03:11,990: t15.2023.08.18 val PER: 0.2640
2026-01-03 19:03:11,990: t15.2023.08.20 val PER: 0.2510
2026-01-03 19:03:11,990: t15.2023.08.25 val PER: 0.2274
2026-01-03 19:03:11,991: t15.2023.08.27 val PER: 0.3360
2026-01-03 19:03:11,991: t15.2023.09.01 val PER: 0.2281
2026-01-03 19:03:11,991: t15.2023.09.03 val PER: 0.3409
2026-01-03 19:03:11,991: t15.2023.09.24 val PER: 0.2524
2026-01-03 19:03:11,991: t15.2023.09.29 val PER: 0.2795
2026-01-03 19:03:11,991: t15.2023.10.01 val PER: 0.3276
2026-01-03 19:03:11,991: t15.2023.10.06 val PER: 0.2347
2026-01-03 19:03:11,991: t15.2023.10.08 val PER: 0.3829
2026-01-03 19:03:11,991: t15.2023.10.13 val PER: 0.3801
2026-01-03 19:03:11,992: t15.2023.10.15 val PER: 0.3111
2026-01-03 19:03:11,992: t15.2023.10.20 val PER: 0.2886
2026-01-03 19:03:11,992: t15.2023.10.22 val PER: 0.2628
2026-01-03 19:03:11,992: t15.2023.11.03 val PER: 0.3202
2026-01-03 19:03:11,992: t15.2023.11.04 val PER: 0.0717
2026-01-03 19:03:11,992: t15.2023.11.17 val PER: 0.1617
2026-01-03 19:03:11,992: t15.2023.11.19 val PER: 0.1337
2026-01-03 19:03:11,992: t15.2023.11.26 val PER: 0.3652
2026-01-03 19:03:11,992: t15.2023.12.03 val PER: 0.3130
2026-01-03 19:03:11,992: t15.2023.12.08 val PER: 0.3043
2026-01-03 19:03:11,992: t15.2023.12.10 val PER: 0.2510
2026-01-03 19:03:11,992: t15.2023.12.17 val PER: 0.3067
2026-01-03 19:03:11,992: t15.2023.12.29 val PER: 0.3301
2026-01-03 19:03:11,992: t15.2024.02.25 val PER: 0.2528
2026-01-03 19:03:11,992: t15.2024.03.08 val PER: 0.3883
2026-01-03 19:03:11,993: t15.2024.03.15 val PER: 0.3665
2026-01-03 19:03:11,993: t15.2024.03.17 val PER: 0.3278
2026-01-03 19:03:11,993: t15.2024.05.10 val PER: 0.3403
2026-01-03 19:03:11,993: t15.2024.06.14 val PER: 0.3423
2026-01-03 19:03:11,993: t15.2024.07.19 val PER: 0.4647
2026-01-03 19:03:11,993: t15.2024.07.21 val PER: 0.2986
2026-01-03 19:03:11,993: t15.2024.07.28 val PER: 0.3301
2026-01-03 19:03:11,993: t15.2025.01.10 val PER: 0.5331
2026-01-03 19:03:11,993: t15.2025.01.12 val PER: 0.4018
2026-01-03 19:03:11,993: t15.2025.03.14 val PER: 0.5429
2026-01-03 19:03:11,993: t15.2025.03.16 val PER: 0.4045
2026-01-03 19:03:11,993: t15.2025.03.30 val PER: 0.5425
2026-01-03 19:03:11,993: t15.2025.04.13 val PER: 0.4251
2026-01-03 19:03:11,994: New best val WER(1gram) 73.60% --> 70.30%
2026-01-03 19:03:12,239: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_2500
2026-01-03 19:03:20,927: Train batch 2600: loss: 38.97 grad norm: 90.21 time: 0.054
2026-01-03 19:03:38,405: Train batch 2800: loss: 29.03 grad norm: 76.53 time: 0.080
2026-01-03 19:03:55,524: Train batch 3000: loss: 35.02 grad norm: 85.16 time: 0.082
2026-01-03 19:03:55,524: Running test after training batch: 3000
2026-01-03 19:03:55,628: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:04:00,406: WER debug example
  GT : you can see the code at this point as well
  PR : yule end e the good at this point is will
2026-01-03 19:04:00,436: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap the cost it
2026-01-03 19:04:02,101: Val batch 3000: PER (avg): 0.3043 CTC Loss (avg): 30.4459 WER(1gram): 67.51% (n=64) time: 6.577
2026-01-03 19:04:02,101: WER lens: avg_true_words=6.16 avg_pred_words=5.61 max_pred_words=11
2026-01-03 19:04:02,101: t15.2023.08.13 val PER: 0.2703
2026-01-03 19:04:02,101: t15.2023.08.18 val PER: 0.2355
2026-01-03 19:04:02,102: t15.2023.08.20 val PER: 0.2240
2026-01-03 19:04:02,102: t15.2023.08.25 val PER: 0.2154
2026-01-03 19:04:02,102: t15.2023.08.27 val PER: 0.3280
2026-01-03 19:04:02,102: t15.2023.09.01 val PER: 0.2029
2026-01-03 19:04:02,102: t15.2023.09.03 val PER: 0.3135
2026-01-03 19:04:02,102: t15.2023.09.24 val PER: 0.2379
2026-01-03 19:04:02,102: t15.2023.09.29 val PER: 0.2597
2026-01-03 19:04:02,102: t15.2023.10.01 val PER: 0.3058
2026-01-03 19:04:02,102: t15.2023.10.06 val PER: 0.2271
2026-01-03 19:04:02,102: t15.2023.10.08 val PER: 0.3802
2026-01-03 19:04:02,102: t15.2023.10.13 val PER: 0.3584
2026-01-03 19:04:02,102: t15.2023.10.15 val PER: 0.2881
2026-01-03 19:04:02,103: t15.2023.10.20 val PER: 0.2718
2026-01-03 19:04:02,103: t15.2023.10.22 val PER: 0.2261
2026-01-03 19:04:02,103: t15.2023.11.03 val PER: 0.2944
2026-01-03 19:04:02,103: t15.2023.11.04 val PER: 0.0956
2026-01-03 19:04:02,103: t15.2023.11.17 val PER: 0.1477
2026-01-03 19:04:02,103: t15.2023.11.19 val PER: 0.1297
2026-01-03 19:04:02,103: t15.2023.11.26 val PER: 0.3254
2026-01-03 19:04:02,103: t15.2023.12.03 val PER: 0.2710
2026-01-03 19:04:02,103: t15.2023.12.08 val PER: 0.2816
2026-01-03 19:04:02,103: t15.2023.12.10 val PER: 0.2405
2026-01-03 19:04:02,103: t15.2023.12.17 val PER: 0.2963
2026-01-03 19:04:02,104: t15.2023.12.29 val PER: 0.2965
2026-01-03 19:04:02,104: t15.2024.02.25 val PER: 0.2598
2026-01-03 19:04:02,104: t15.2024.03.08 val PER: 0.3883
2026-01-03 19:04:02,104: t15.2024.03.15 val PER: 0.3558
2026-01-03 19:04:02,104: t15.2024.03.17 val PER: 0.3013
2026-01-03 19:04:02,104: t15.2024.05.10 val PER: 0.3105
2026-01-03 19:04:02,104: t15.2024.06.14 val PER: 0.3155
2026-01-03 19:04:02,104: t15.2024.07.19 val PER: 0.4272
2026-01-03 19:04:02,104: t15.2024.07.21 val PER: 0.2634
2026-01-03 19:04:02,104: t15.2024.07.28 val PER: 0.3074
2026-01-03 19:04:02,104: t15.2025.01.10 val PER: 0.5262
2026-01-03 19:04:02,104: t15.2025.01.12 val PER: 0.3649
2026-01-03 19:04:02,104: t15.2025.03.14 val PER: 0.5015
2026-01-03 19:04:02,104: t15.2025.03.16 val PER: 0.3626
2026-01-03 19:04:02,105: t15.2025.03.30 val PER: 0.5287
2026-01-03 19:04:02,105: t15.2025.04.13 val PER: 0.3852
2026-01-03 19:04:02,106: New best val WER(1gram) 70.30% --> 67.51%
2026-01-03 19:04:02,350: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_3000
2026-01-03 19:04:19,866: Train batch 3200: loss: 30.40 grad norm: 71.19 time: 0.074
2026-01-03 19:04:37,331: Train batch 3400: loss: 21.50 grad norm: 59.70 time: 0.048
2026-01-03 19:04:46,136: Running test after training batch: 3500
2026-01-03 19:04:46,270: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:04:51,227: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point wheel
2026-01-03 19:04:51,254: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap thus wass note
2026-01-03 19:04:52,929: Val batch 3500: PER (avg): 0.2941 CTC Loss (avg): 29.4175 WER(1gram): 68.02% (n=64) time: 6.793
2026-01-03 19:04:52,929: WER lens: avg_true_words=6.16 avg_pred_words=5.83 max_pred_words=11
2026-01-03 19:04:52,930: t15.2023.08.13 val PER: 0.2661
2026-01-03 19:04:52,930: t15.2023.08.18 val PER: 0.2355
2026-01-03 19:04:52,930: t15.2023.08.20 val PER: 0.2343
2026-01-03 19:04:52,930: t15.2023.08.25 val PER: 0.1943
2026-01-03 19:04:52,930: t15.2023.08.27 val PER: 0.3023
2026-01-03 19:04:52,930: t15.2023.09.01 val PER: 0.2037
2026-01-03 19:04:52,930: t15.2023.09.03 val PER: 0.2720
2026-01-03 19:04:52,930: t15.2023.09.24 val PER: 0.2197
2026-01-03 19:04:52,930: t15.2023.09.29 val PER: 0.2457
2026-01-03 19:04:52,930: t15.2023.10.01 val PER: 0.2999
2026-01-03 19:04:52,930: t15.2023.10.06 val PER: 0.2121
2026-01-03 19:04:52,930: t15.2023.10.08 val PER: 0.3613
2026-01-03 19:04:52,931: t15.2023.10.13 val PER: 0.3545
2026-01-03 19:04:52,931: t15.2023.10.15 val PER: 0.2749
2026-01-03 19:04:52,931: t15.2023.10.20 val PER: 0.2550
2026-01-03 19:04:52,931: t15.2023.10.22 val PER: 0.2205
2026-01-03 19:04:52,931: t15.2023.11.03 val PER: 0.2788
2026-01-03 19:04:52,931: t15.2023.11.04 val PER: 0.1024
2026-01-03 19:04:52,931: t15.2023.11.17 val PER: 0.1369
2026-01-03 19:04:52,931: t15.2023.11.19 val PER: 0.1158
2026-01-03 19:04:52,931: t15.2023.11.26 val PER: 0.3152
2026-01-03 19:04:52,931: t15.2023.12.03 val PER: 0.2815
2026-01-03 19:04:52,931: t15.2023.12.08 val PER: 0.2810
2026-01-03 19:04:52,931: t15.2023.12.10 val PER: 0.2339
2026-01-03 19:04:52,931: t15.2023.12.17 val PER: 0.2713
2026-01-03 19:04:52,931: t15.2023.12.29 val PER: 0.2883
2026-01-03 19:04:52,931: t15.2024.02.25 val PER: 0.2303
2026-01-03 19:04:52,932: t15.2024.03.08 val PER: 0.3542
2026-01-03 19:04:52,932: t15.2024.03.15 val PER: 0.3452
2026-01-03 19:04:52,932: t15.2024.03.17 val PER: 0.3033
2026-01-03 19:04:52,932: t15.2024.05.10 val PER: 0.3076
2026-01-03 19:04:52,932: t15.2024.06.14 val PER: 0.2950
2026-01-03 19:04:52,932: t15.2024.07.19 val PER: 0.4258
2026-01-03 19:04:52,932: t15.2024.07.21 val PER: 0.2538
2026-01-03 19:04:52,932: t15.2024.07.28 val PER: 0.3044
2026-01-03 19:04:52,932: t15.2025.01.10 val PER: 0.4848
2026-01-03 19:04:52,932: t15.2025.01.12 val PER: 0.3457
2026-01-03 19:04:52,932: t15.2025.03.14 val PER: 0.5074
2026-01-03 19:04:52,932: t15.2025.03.16 val PER: 0.3613
2026-01-03 19:04:52,932: t15.2025.03.30 val PER: 0.5103
2026-01-03 19:04:52,932: t15.2025.04.13 val PER: 0.3623
2026-01-03 19:04:53,171: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_3500
2026-01-03 19:05:01,764: Train batch 3600: loss: 25.44 grad norm: 65.23 time: 0.067
2026-01-03 19:05:18,996: Train batch 3800: loss: 28.89 grad norm: 72.08 time: 0.066
2026-01-03 19:05:36,483: Train batch 4000: loss: 21.89 grad norm: 56.15 time: 0.056
2026-01-03 19:05:36,483: Running test after training batch: 4000
2026-01-03 19:05:36,627: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:05:41,576: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-03 19:05:41,606: WER debug example
  GT : how does it keep the cost down
  PR : aue dust it keep the cussed nett
2026-01-03 19:05:43,301: Val batch 4000: PER (avg): 0.2750 CTC Loss (avg): 27.4070 WER(1gram): 65.99% (n=64) time: 6.818
2026-01-03 19:05:43,302: WER lens: avg_true_words=6.16 avg_pred_words=5.80 max_pred_words=11
2026-01-03 19:05:43,302: t15.2023.08.13 val PER: 0.2432
2026-01-03 19:05:43,302: t15.2023.08.18 val PER: 0.2230
2026-01-03 19:05:43,302: t15.2023.08.20 val PER: 0.2280
2026-01-03 19:05:43,302: t15.2023.08.25 val PER: 0.1807
2026-01-03 19:05:43,302: t15.2023.08.27 val PER: 0.2974
2026-01-03 19:05:43,303: t15.2023.09.01 val PER: 0.1851
2026-01-03 19:05:43,303: t15.2023.09.03 val PER: 0.2803
2026-01-03 19:05:43,303: t15.2023.09.24 val PER: 0.2015
2026-01-03 19:05:43,303: t15.2023.09.29 val PER: 0.2221
2026-01-03 19:05:43,303: t15.2023.10.01 val PER: 0.2741
2026-01-03 19:05:43,303: t15.2023.10.06 val PER: 0.1895
2026-01-03 19:05:43,303: t15.2023.10.08 val PER: 0.3396
2026-01-03 19:05:43,303: t15.2023.10.13 val PER: 0.3274
2026-01-03 19:05:43,303: t15.2023.10.15 val PER: 0.2610
2026-01-03 19:05:43,303: t15.2023.10.20 val PER: 0.2450
2026-01-03 19:05:43,303: t15.2023.10.22 val PER: 0.2160
2026-01-03 19:05:43,303: t15.2023.11.03 val PER: 0.2592
2026-01-03 19:05:43,304: t15.2023.11.04 val PER: 0.0751
2026-01-03 19:05:43,304: t15.2023.11.17 val PER: 0.1182
2026-01-03 19:05:43,304: t15.2023.11.19 val PER: 0.1098
2026-01-03 19:05:43,304: t15.2023.11.26 val PER: 0.2971
2026-01-03 19:05:43,304: t15.2023.12.03 val PER: 0.2458
2026-01-03 19:05:43,304: t15.2023.12.08 val PER: 0.2557
2026-01-03 19:05:43,304: t15.2023.12.10 val PER: 0.2102
2026-01-03 19:05:43,304: t15.2023.12.17 val PER: 0.2713
2026-01-03 19:05:43,304: t15.2023.12.29 val PER: 0.2759
2026-01-03 19:05:43,304: t15.2024.02.25 val PER: 0.2416
2026-01-03 19:05:43,304: t15.2024.03.08 val PER: 0.3514
2026-01-03 19:05:43,304: t15.2024.03.15 val PER: 0.3358
2026-01-03 19:05:43,305: t15.2024.03.17 val PER: 0.2713
2026-01-03 19:05:43,305: t15.2024.05.10 val PER: 0.2942
2026-01-03 19:05:43,305: t15.2024.06.14 val PER: 0.2902
2026-01-03 19:05:43,305: t15.2024.07.19 val PER: 0.3995
2026-01-03 19:05:43,305: t15.2024.07.21 val PER: 0.2166
2026-01-03 19:05:43,305: t15.2024.07.28 val PER: 0.2713
2026-01-03 19:05:43,305: t15.2025.01.10 val PER: 0.4683
2026-01-03 19:05:43,305: t15.2025.01.12 val PER: 0.3272
2026-01-03 19:05:43,305: t15.2025.03.14 val PER: 0.4438
2026-01-03 19:05:43,305: t15.2025.03.16 val PER: 0.3455
2026-01-03 19:05:43,306: t15.2025.03.30 val PER: 0.4747
2026-01-03 19:05:43,306: t15.2025.04.13 val PER: 0.3438
2026-01-03 19:05:43,306: New best val WER(1gram) 67.51% --> 65.99%
2026-01-03 19:05:43,565: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_4000
2026-01-03 19:06:01,258: Train batch 4200: loss: 25.53 grad norm: 72.80 time: 0.079
2026-01-03 19:06:18,754: Train batch 4400: loss: 20.27 grad norm: 60.85 time: 0.066
2026-01-03 19:06:27,427: Running test after training batch: 4500
2026-01-03 19:06:27,526: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:06:32,254: WER debug example
  GT : you can see the code at this point as well
  PR : yule end sze the code at this point is will
2026-01-03 19:06:32,282: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cost et
2026-01-03 19:06:33,968: Val batch 4500: PER (avg): 0.2636 CTC Loss (avg): 26.0426 WER(1gram): 64.72% (n=64) time: 6.541
2026-01-03 19:06:33,968: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=11
2026-01-03 19:06:33,968: t15.2023.08.13 val PER: 0.2443
2026-01-03 19:06:33,968: t15.2023.08.18 val PER: 0.2003
2026-01-03 19:06:33,969: t15.2023.08.20 val PER: 0.2057
2026-01-03 19:06:33,969: t15.2023.08.25 val PER: 0.1566
2026-01-03 19:06:33,969: t15.2023.08.27 val PER: 0.2878
2026-01-03 19:06:33,969: t15.2023.09.01 val PER: 0.1761
2026-01-03 19:06:33,969: t15.2023.09.03 val PER: 0.2637
2026-01-03 19:06:33,969: t15.2023.09.24 val PER: 0.1917
2026-01-03 19:06:33,969: t15.2023.09.29 val PER: 0.2208
2026-01-03 19:06:33,969: t15.2023.10.01 val PER: 0.2774
2026-01-03 19:06:33,969: t15.2023.10.06 val PER: 0.1701
2026-01-03 19:06:33,969: t15.2023.10.08 val PER: 0.3315
2026-01-03 19:06:33,969: t15.2023.10.13 val PER: 0.3220
2026-01-03 19:06:33,970: t15.2023.10.15 val PER: 0.2353
2026-01-03 19:06:33,970: t15.2023.10.20 val PER: 0.2517
2026-01-03 19:06:33,970: t15.2023.10.22 val PER: 0.2004
2026-01-03 19:06:33,970: t15.2023.11.03 val PER: 0.2646
2026-01-03 19:06:33,970: t15.2023.11.04 val PER: 0.0717
2026-01-03 19:06:33,970: t15.2023.11.17 val PER: 0.1229
2026-01-03 19:06:33,970: t15.2023.11.19 val PER: 0.1198
2026-01-03 19:06:33,970: t15.2023.11.26 val PER: 0.2935
2026-01-03 19:06:33,970: t15.2023.12.03 val PER: 0.2468
2026-01-03 19:06:33,970: t15.2023.12.08 val PER: 0.2503
2026-01-03 19:06:33,970: t15.2023.12.10 val PER: 0.2050
2026-01-03 19:06:33,970: t15.2023.12.17 val PER: 0.2630
2026-01-03 19:06:33,970: t15.2023.12.29 val PER: 0.2704
2026-01-03 19:06:33,970: t15.2024.02.25 val PER: 0.2163
2026-01-03 19:06:33,970: t15.2024.03.08 val PER: 0.3485
2026-01-03 19:06:33,970: t15.2024.03.15 val PER: 0.3227
2026-01-03 19:06:33,971: t15.2024.03.17 val PER: 0.2650
2026-01-03 19:06:33,971: t15.2024.05.10 val PER: 0.2868
2026-01-03 19:06:33,971: t15.2024.06.14 val PER: 0.2744
2026-01-03 19:06:33,971: t15.2024.07.19 val PER: 0.3751
2026-01-03 19:06:33,971: t15.2024.07.21 val PER: 0.2083
2026-01-03 19:06:33,971: t15.2024.07.28 val PER: 0.2559
2026-01-03 19:06:33,971: t15.2025.01.10 val PER: 0.4380
2026-01-03 19:06:33,971: t15.2025.01.12 val PER: 0.2995
2026-01-03 19:06:33,971: t15.2025.03.14 val PER: 0.4364
2026-01-03 19:06:33,971: t15.2025.03.16 val PER: 0.3233
2026-01-03 19:06:33,971: t15.2025.03.30 val PER: 0.4448
2026-01-03 19:06:33,971: t15.2025.04.13 val PER: 0.3081
2026-01-03 19:06:33,973: New best val WER(1gram) 65.99% --> 64.72%
2026-01-03 19:06:34,215: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_4500
2026-01-03 19:06:42,883: Train batch 4600: loss: 24.37 grad norm: 70.71 time: 0.062
2026-01-03 19:07:00,240: Train batch 4800: loss: 17.00 grad norm: 59.64 time: 0.063
2026-01-03 19:07:18,004: Train batch 5000: loss: 37.39 grad norm: 80.71 time: 0.063
2026-01-03 19:07:18,005: Running test after training batch: 5000
2026-01-03 19:07:18,132: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:07:23,247: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-03 19:07:23,276: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the cussed nett
2026-01-03 19:07:24,976: Val batch 5000: PER (avg): 0.2534 CTC Loss (avg): 24.9233 WER(1gram): 63.71% (n=64) time: 6.971
2026-01-03 19:07:24,976: WER lens: avg_true_words=6.16 avg_pred_words=5.91 max_pred_words=11
2026-01-03 19:07:24,977: t15.2023.08.13 val PER: 0.2328
2026-01-03 19:07:24,977: t15.2023.08.18 val PER: 0.1945
2026-01-03 19:07:24,977: t15.2023.08.20 val PER: 0.1962
2026-01-03 19:07:24,977: t15.2023.08.25 val PER: 0.1536
2026-01-03 19:07:24,977: t15.2023.08.27 val PER: 0.2717
2026-01-03 19:07:24,977: t15.2023.09.01 val PER: 0.1721
2026-01-03 19:07:24,977: t15.2023.09.03 val PER: 0.2482
2026-01-03 19:07:24,977: t15.2023.09.24 val PER: 0.1954
2026-01-03 19:07:24,977: t15.2023.09.29 val PER: 0.2112
2026-01-03 19:07:24,977: t15.2023.10.01 val PER: 0.2517
2026-01-03 19:07:24,977: t15.2023.10.06 val PER: 0.1712
2026-01-03 19:07:24,977: t15.2023.10.08 val PER: 0.3315
2026-01-03 19:07:24,977: t15.2023.10.13 val PER: 0.3010
2026-01-03 19:07:24,978: t15.2023.10.15 val PER: 0.2367
2026-01-03 19:07:24,978: t15.2023.10.20 val PER: 0.2450
2026-01-03 19:07:24,978: t15.2023.10.22 val PER: 0.1871
2026-01-03 19:07:24,978: t15.2023.11.03 val PER: 0.2456
2026-01-03 19:07:24,978: t15.2023.11.04 val PER: 0.0717
2026-01-03 19:07:24,978: t15.2023.11.17 val PER: 0.0949
2026-01-03 19:07:24,978: t15.2023.11.19 val PER: 0.0958
2026-01-03 19:07:24,978: t15.2023.11.26 val PER: 0.2732
2026-01-03 19:07:24,978: t15.2023.12.03 val PER: 0.2258
2026-01-03 19:07:24,978: t15.2023.12.08 val PER: 0.2357
2026-01-03 19:07:24,978: t15.2023.12.10 val PER: 0.1840
2026-01-03 19:07:24,978: t15.2023.12.17 val PER: 0.2516
2026-01-03 19:07:24,978: t15.2023.12.29 val PER: 0.2601
2026-01-03 19:07:24,978: t15.2024.02.25 val PER: 0.2121
2026-01-03 19:07:24,978: t15.2024.03.08 val PER: 0.3385
2026-01-03 19:07:24,978: t15.2024.03.15 val PER: 0.3027
2026-01-03 19:07:24,979: t15.2024.03.17 val PER: 0.2552
2026-01-03 19:07:24,979: t15.2024.05.10 val PER: 0.2734
2026-01-03 19:07:24,979: t15.2024.06.14 val PER: 0.2713
2026-01-03 19:07:24,979: t15.2024.07.19 val PER: 0.3757
2026-01-03 19:07:24,979: t15.2024.07.21 val PER: 0.1979
2026-01-03 19:07:24,979: t15.2024.07.28 val PER: 0.2529
2026-01-03 19:07:24,979: t15.2025.01.10 val PER: 0.4435
2026-01-03 19:07:24,979: t15.2025.01.12 val PER: 0.2933
2026-01-03 19:07:24,980: t15.2025.03.14 val PER: 0.4157
2026-01-03 19:07:24,980: t15.2025.03.16 val PER: 0.2997
2026-01-03 19:07:24,980: t15.2025.03.30 val PER: 0.4471
2026-01-03 19:07:24,980: t15.2025.04.13 val PER: 0.3295
2026-01-03 19:07:24,981: New best val WER(1gram) 64.72% --> 63.71%
2026-01-03 19:07:25,222: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_5000
2026-01-03 19:07:42,251: Train batch 5200: loss: 20.93 grad norm: 69.92 time: 0.051
2026-01-03 19:07:59,436: Train batch 5400: loss: 20.95 grad norm: 63.54 time: 0.067
2026-01-03 19:08:08,177: Running test after training batch: 5500
2026-01-03 19:08:08,319: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:08:13,066: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point will
2026-01-03 19:08:13,096: WER debug example
  GT : how does it keep the cost down
  PR : houde des it keep the cussed it
2026-01-03 19:08:14,775: Val batch 5500: PER (avg): 0.2454 CTC Loss (avg): 23.8146 WER(1gram): 63.71% (n=64) time: 6.597
2026-01-03 19:08:14,776: WER lens: avg_true_words=6.16 avg_pred_words=5.95 max_pred_words=11
2026-01-03 19:08:14,776: t15.2023.08.13 val PER: 0.2245
2026-01-03 19:08:14,776: t15.2023.08.18 val PER: 0.1936
2026-01-03 19:08:14,776: t15.2023.08.20 val PER: 0.1994
2026-01-03 19:08:14,776: t15.2023.08.25 val PER: 0.1431
2026-01-03 19:08:14,776: t15.2023.08.27 val PER: 0.2621
2026-01-03 19:08:14,776: t15.2023.09.01 val PER: 0.1583
2026-01-03 19:08:14,776: t15.2023.09.03 val PER: 0.2458
2026-01-03 19:08:14,776: t15.2023.09.24 val PER: 0.1966
2026-01-03 19:08:14,776: t15.2023.09.29 val PER: 0.2004
2026-01-03 19:08:14,776: t15.2023.10.01 val PER: 0.2543
2026-01-03 19:08:14,777: t15.2023.10.06 val PER: 0.1733
2026-01-03 19:08:14,777: t15.2023.10.08 val PER: 0.3139
2026-01-03 19:08:14,777: t15.2023.10.13 val PER: 0.3010
2026-01-03 19:08:14,777: t15.2023.10.15 val PER: 0.2320
2026-01-03 19:08:14,777: t15.2023.10.20 val PER: 0.2315
2026-01-03 19:08:14,777: t15.2023.10.22 val PER: 0.1904
2026-01-03 19:08:14,777: t15.2023.11.03 val PER: 0.2388
2026-01-03 19:08:14,777: t15.2023.11.04 val PER: 0.0717
2026-01-03 19:08:14,777: t15.2023.11.17 val PER: 0.0933
2026-01-03 19:08:14,777: t15.2023.11.19 val PER: 0.0918
2026-01-03 19:08:14,777: t15.2023.11.26 val PER: 0.2703
2026-01-03 19:08:14,777: t15.2023.12.03 val PER: 0.2227
2026-01-03 19:08:14,777: t15.2023.12.08 val PER: 0.2230
2026-01-03 19:08:14,777: t15.2023.12.10 val PER: 0.1721
2026-01-03 19:08:14,777: t15.2023.12.17 val PER: 0.2464
2026-01-03 19:08:14,777: t15.2023.12.29 val PER: 0.2485
2026-01-03 19:08:14,778: t15.2024.02.25 val PER: 0.2022
2026-01-03 19:08:14,778: t15.2024.03.08 val PER: 0.3286
2026-01-03 19:08:14,778: t15.2024.03.15 val PER: 0.3077
2026-01-03 19:08:14,778: t15.2024.03.17 val PER: 0.2531
2026-01-03 19:08:14,778: t15.2024.05.10 val PER: 0.2719
2026-01-03 19:08:14,778: t15.2024.06.14 val PER: 0.2666
2026-01-03 19:08:14,778: t15.2024.07.19 val PER: 0.3573
2026-01-03 19:08:14,778: t15.2024.07.21 val PER: 0.1821
2026-01-03 19:08:14,778: t15.2024.07.28 val PER: 0.2434
2026-01-03 19:08:14,778: t15.2025.01.10 val PER: 0.4118
2026-01-03 19:08:14,779: t15.2025.01.12 val PER: 0.2748
2026-01-03 19:08:14,779: t15.2025.03.14 val PER: 0.3935
2026-01-03 19:08:14,779: t15.2025.03.16 val PER: 0.2984
2026-01-03 19:08:14,779: t15.2025.03.30 val PER: 0.3920
2026-01-03 19:08:14,779: t15.2025.04.13 val PER: 0.3210
2026-01-03 19:08:15,020: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_5500
2026-01-03 19:08:23,828: Train batch 5600: loss: 24.04 grad norm: 68.07 time: 0.062
2026-01-03 19:08:41,465: Train batch 5800: loss: 17.47 grad norm: 63.50 time: 0.081
2026-01-03 19:08:59,274: Train batch 6000: loss: 17.88 grad norm: 62.59 time: 0.054
2026-01-03 19:08:59,275: Running test after training batch: 6000
2026-01-03 19:08:59,460: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:09:04,191: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-03 19:09:04,222: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the cussed nit
2026-01-03 19:09:05,917: Val batch 6000: PER (avg): 0.2405 CTC Loss (avg): 23.5197 WER(1gram): 62.69% (n=64) time: 6.642
2026-01-03 19:09:05,918: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-03 19:09:05,918: t15.2023.08.13 val PER: 0.2131
2026-01-03 19:09:05,918: t15.2023.08.18 val PER: 0.1920
2026-01-03 19:09:05,918: t15.2023.08.20 val PER: 0.1962
2026-01-03 19:09:05,918: t15.2023.08.25 val PER: 0.1431
2026-01-03 19:09:05,918: t15.2023.08.27 val PER: 0.2572
2026-01-03 19:09:05,918: t15.2023.09.01 val PER: 0.1688
2026-01-03 19:09:05,918: t15.2023.09.03 val PER: 0.2352
2026-01-03 19:09:05,918: t15.2023.09.24 val PER: 0.1845
2026-01-03 19:09:05,918: t15.2023.09.29 val PER: 0.2017
2026-01-03 19:09:05,919: t15.2023.10.01 val PER: 0.2543
2026-01-03 19:09:05,919: t15.2023.10.06 val PER: 0.1561
2026-01-03 19:09:05,919: t15.2023.10.08 val PER: 0.3180
2026-01-03 19:09:05,919: t15.2023.10.13 val PER: 0.2971
2026-01-03 19:09:05,919: t15.2023.10.15 val PER: 0.2287
2026-01-03 19:09:05,919: t15.2023.10.20 val PER: 0.2315
2026-01-03 19:09:05,919: t15.2023.10.22 val PER: 0.1904
2026-01-03 19:09:05,919: t15.2023.11.03 val PER: 0.2510
2026-01-03 19:09:05,919: t15.2023.11.04 val PER: 0.0717
2026-01-03 19:09:05,919: t15.2023.11.17 val PER: 0.1058
2026-01-03 19:09:05,920: t15.2023.11.19 val PER: 0.0938
2026-01-03 19:09:05,920: t15.2023.11.26 val PER: 0.2594
2026-01-03 19:09:05,920: t15.2023.12.03 val PER: 0.1996
2026-01-03 19:09:05,920: t15.2023.12.08 val PER: 0.2084
2026-01-03 19:09:05,920: t15.2023.12.10 val PER: 0.1695
2026-01-03 19:09:05,920: t15.2023.12.17 val PER: 0.2308
2026-01-03 19:09:05,920: t15.2023.12.29 val PER: 0.2512
2026-01-03 19:09:05,920: t15.2024.02.25 val PER: 0.2008
2026-01-03 19:09:05,920: t15.2024.03.08 val PER: 0.3115
2026-01-03 19:09:05,920: t15.2024.03.15 val PER: 0.2996
2026-01-03 19:09:05,920: t15.2024.03.17 val PER: 0.2490
2026-01-03 19:09:05,920: t15.2024.05.10 val PER: 0.2452
2026-01-03 19:09:05,920: t15.2024.06.14 val PER: 0.2681
2026-01-03 19:09:05,920: t15.2024.07.19 val PER: 0.3448
2026-01-03 19:09:05,920: t15.2024.07.21 val PER: 0.1786
2026-01-03 19:09:05,920: t15.2024.07.28 val PER: 0.2265
2026-01-03 19:09:05,921: t15.2025.01.10 val PER: 0.4187
2026-01-03 19:09:05,921: t15.2025.01.12 val PER: 0.2664
2026-01-03 19:09:05,921: t15.2025.03.14 val PER: 0.3964
2026-01-03 19:09:05,921: t15.2025.03.16 val PER: 0.2906
2026-01-03 19:09:05,921: t15.2025.03.30 val PER: 0.3966
2026-01-03 19:09:05,921: t15.2025.04.13 val PER: 0.3110
2026-01-03 19:09:05,922: New best val WER(1gram) 63.71% --> 62.69%
2026-01-03 19:09:06,176: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_6000
2026-01-03 19:09:23,400: Train batch 6200: loss: 20.14 grad norm: 63.46 time: 0.069
2026-01-03 19:09:40,451: Train batch 6400: loss: 23.35 grad norm: 73.23 time: 0.061
2026-01-03 19:09:48,833: Running test after training batch: 6500
2026-01-03 19:09:48,967: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:09:54,120: WER debug example
  GT : you can see the code at this point as well
  PR : yule kent sze the could at this point is will
2026-01-03 19:09:54,151: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the cost nett
2026-01-03 19:09:55,791: Val batch 6500: PER (avg): 0.2304 CTC Loss (avg): 22.5177 WER(1gram): 58.12% (n=64) time: 6.958
2026-01-03 19:09:55,792: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 19:09:55,792: t15.2023.08.13 val PER: 0.2079
2026-01-03 19:09:55,792: t15.2023.08.18 val PER: 0.1668
2026-01-03 19:09:55,792: t15.2023.08.20 val PER: 0.1827
2026-01-03 19:09:55,792: t15.2023.08.25 val PER: 0.1295
2026-01-03 19:09:55,793: t15.2023.08.27 val PER: 0.2428
2026-01-03 19:09:55,793: t15.2023.09.01 val PER: 0.1477
2026-01-03 19:09:55,793: t15.2023.09.03 val PER: 0.2185
2026-01-03 19:09:55,793: t15.2023.09.24 val PER: 0.1978
2026-01-03 19:09:55,793: t15.2023.09.29 val PER: 0.1946
2026-01-03 19:09:55,793: t15.2023.10.01 val PER: 0.2517
2026-01-03 19:09:55,793: t15.2023.10.06 val PER: 0.1464
2026-01-03 19:09:55,793: t15.2023.10.08 val PER: 0.3031
2026-01-03 19:09:55,793: t15.2023.10.13 val PER: 0.2940
2026-01-03 19:09:55,793: t15.2023.10.15 val PER: 0.2294
2026-01-03 19:09:55,793: t15.2023.10.20 val PER: 0.2181
2026-01-03 19:09:55,793: t15.2023.10.22 val PER: 0.1860
2026-01-03 19:09:55,793: t15.2023.11.03 val PER: 0.2429
2026-01-03 19:09:55,793: t15.2023.11.04 val PER: 0.0683
2026-01-03 19:09:55,794: t15.2023.11.17 val PER: 0.0824
2026-01-03 19:09:55,794: t15.2023.11.19 val PER: 0.0918
2026-01-03 19:09:55,794: t15.2023.11.26 val PER: 0.2486
2026-01-03 19:09:55,794: t15.2023.12.03 val PER: 0.2059
2026-01-03 19:09:55,794: t15.2023.12.08 val PER: 0.1991
2026-01-03 19:09:55,794: t15.2023.12.10 val PER: 0.1629
2026-01-03 19:09:55,794: t15.2023.12.17 val PER: 0.2204
2026-01-03 19:09:55,794: t15.2023.12.29 val PER: 0.2327
2026-01-03 19:09:55,794: t15.2024.02.25 val PER: 0.1840
2026-01-03 19:09:55,794: t15.2024.03.08 val PER: 0.3087
2026-01-03 19:09:55,794: t15.2024.03.15 val PER: 0.2802
2026-01-03 19:09:55,794: t15.2024.03.17 val PER: 0.2350
2026-01-03 19:09:55,794: t15.2024.05.10 val PER: 0.2377
2026-01-03 19:09:55,794: t15.2024.06.14 val PER: 0.2461
2026-01-03 19:09:55,794: t15.2024.07.19 val PER: 0.3362
2026-01-03 19:09:55,794: t15.2024.07.21 val PER: 0.1683
2026-01-03 19:09:55,795: t15.2024.07.28 val PER: 0.2199
2026-01-03 19:09:55,795: t15.2025.01.10 val PER: 0.4270
2026-01-03 19:09:55,795: t15.2025.01.12 val PER: 0.2502
2026-01-03 19:09:55,795: t15.2025.03.14 val PER: 0.3905
2026-01-03 19:09:55,795: t15.2025.03.16 val PER: 0.2775
2026-01-03 19:09:55,795: t15.2025.03.30 val PER: 0.3667
2026-01-03 19:09:55,795: t15.2025.04.13 val PER: 0.3039
2026-01-03 19:09:55,796: New best val WER(1gram) 62.69% --> 58.12%
2026-01-03 19:09:56,041: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_6500
2026-01-03 19:10:04,370: Train batch 6600: loss: 15.49 grad norm: 59.04 time: 0.044
2026-01-03 19:10:21,619: Train batch 6800: loss: 19.18 grad norm: 60.97 time: 0.049
2026-01-03 19:10:39,765: Train batch 7000: loss: 21.89 grad norm: 71.51 time: 0.060
2026-01-03 19:10:39,766: Running test after training batch: 7000
2026-01-03 19:10:39,916: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:10:44,611: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-03 19:10:44,642: WER debug example
  GT : how does it keep the cost down
  PR : houde just it heap the cost nett
2026-01-03 19:10:46,329: Val batch 7000: PER (avg): 0.2206 CTC Loss (avg): 21.7005 WER(1gram): 58.63% (n=64) time: 6.563
2026-01-03 19:10:46,329: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=11
2026-01-03 19:10:46,329: t15.2023.08.13 val PER: 0.1944
2026-01-03 19:10:46,329: t15.2023.08.18 val PER: 0.1643
2026-01-03 19:10:46,330: t15.2023.08.20 val PER: 0.1716
2026-01-03 19:10:46,330: t15.2023.08.25 val PER: 0.1205
2026-01-03 19:10:46,330: t15.2023.08.27 val PER: 0.2315
2026-01-03 19:10:46,330: t15.2023.09.01 val PER: 0.1339
2026-01-03 19:10:46,330: t15.2023.09.03 val PER: 0.2102
2026-01-03 19:10:46,330: t15.2023.09.24 val PER: 0.1699
2026-01-03 19:10:46,330: t15.2023.09.29 val PER: 0.1908
2026-01-03 19:10:46,330: t15.2023.10.01 val PER: 0.2312
2026-01-03 19:10:46,330: t15.2023.10.06 val PER: 0.1346
2026-01-03 19:10:46,330: t15.2023.10.08 val PER: 0.2991
2026-01-03 19:10:46,331: t15.2023.10.13 val PER: 0.2801
2026-01-03 19:10:46,331: t15.2023.10.15 val PER: 0.2248
2026-01-03 19:10:46,331: t15.2023.10.20 val PER: 0.2148
2026-01-03 19:10:46,331: t15.2023.10.22 val PER: 0.1659
2026-01-03 19:10:46,331: t15.2023.11.03 val PER: 0.2218
2026-01-03 19:10:46,331: t15.2023.11.04 val PER: 0.0546
2026-01-03 19:10:46,331: t15.2023.11.17 val PER: 0.0809
2026-01-03 19:10:46,331: t15.2023.11.19 val PER: 0.0778
2026-01-03 19:10:46,331: t15.2023.11.26 val PER: 0.2246
2026-01-03 19:10:46,331: t15.2023.12.03 val PER: 0.1933
2026-01-03 19:10:46,331: t15.2023.12.08 val PER: 0.1931
2026-01-03 19:10:46,332: t15.2023.12.10 val PER: 0.1577
2026-01-03 19:10:46,332: t15.2023.12.17 val PER: 0.2204
2026-01-03 19:10:46,332: t15.2023.12.29 val PER: 0.2279
2026-01-03 19:10:46,332: t15.2024.02.25 val PER: 0.1812
2026-01-03 19:10:46,332: t15.2024.03.08 val PER: 0.2916
2026-01-03 19:10:46,332: t15.2024.03.15 val PER: 0.2752
2026-01-03 19:10:46,332: t15.2024.03.17 val PER: 0.2183
2026-01-03 19:10:46,332: t15.2024.05.10 val PER: 0.2467
2026-01-03 19:10:46,332: t15.2024.06.14 val PER: 0.2413
2026-01-03 19:10:46,332: t15.2024.07.19 val PER: 0.3362
2026-01-03 19:10:46,332: t15.2024.07.21 val PER: 0.1614
2026-01-03 19:10:46,332: t15.2024.07.28 val PER: 0.2015
2026-01-03 19:10:46,332: t15.2025.01.10 val PER: 0.3967
2026-01-03 19:10:46,332: t15.2025.01.12 val PER: 0.2540
2026-01-03 19:10:46,332: t15.2025.03.14 val PER: 0.3743
2026-01-03 19:10:46,332: t15.2025.03.16 val PER: 0.2657
2026-01-03 19:10:46,332: t15.2025.03.30 val PER: 0.3747
2026-01-03 19:10:46,332: t15.2025.04.13 val PER: 0.3010
2026-01-03 19:10:46,568: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_7000
2026-01-03 19:11:03,779: Train batch 7200: loss: 16.91 grad norm: 55.89 time: 0.078
2026-01-03 19:11:21,013: Train batch 7400: loss: 17.82 grad norm: 61.45 time: 0.075
2026-01-03 19:11:29,569: Running test after training batch: 7500
2026-01-03 19:11:29,675: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:11:34,387: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 19:11:34,418: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cussed nit
2026-01-03 19:11:36,093: Val batch 7500: PER (avg): 0.2164 CTC Loss (avg): 21.2039 WER(1gram): 60.41% (n=64) time: 6.524
2026-01-03 19:11:36,094: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 19:11:36,094: t15.2023.08.13 val PER: 0.1850
2026-01-03 19:11:36,094: t15.2023.08.18 val PER: 0.1685
2026-01-03 19:11:36,094: t15.2023.08.20 val PER: 0.1684
2026-01-03 19:11:36,094: t15.2023.08.25 val PER: 0.1205
2026-01-03 19:11:36,094: t15.2023.08.27 val PER: 0.2251
2026-01-03 19:11:36,094: t15.2023.09.01 val PER: 0.1388
2026-01-03 19:11:36,094: t15.2023.09.03 val PER: 0.2102
2026-01-03 19:11:36,094: t15.2023.09.24 val PER: 0.1687
2026-01-03 19:11:36,095: t15.2023.09.29 val PER: 0.1825
2026-01-03 19:11:36,095: t15.2023.10.01 val PER: 0.2351
2026-01-03 19:11:36,095: t15.2023.10.06 val PER: 0.1313
2026-01-03 19:11:36,095: t15.2023.10.08 val PER: 0.2882
2026-01-03 19:11:36,095: t15.2023.10.13 val PER: 0.2746
2026-01-03 19:11:36,095: t15.2023.10.15 val PER: 0.2195
2026-01-03 19:11:36,095: t15.2023.10.20 val PER: 0.2114
2026-01-03 19:11:36,095: t15.2023.10.22 val PER: 0.1581
2026-01-03 19:11:36,095: t15.2023.11.03 val PER: 0.2273
2026-01-03 19:11:36,095: t15.2023.11.04 val PER: 0.0512
2026-01-03 19:11:36,095: t15.2023.11.17 val PER: 0.0793
2026-01-03 19:11:36,095: t15.2023.11.19 val PER: 0.0559
2026-01-03 19:11:36,095: t15.2023.11.26 val PER: 0.2348
2026-01-03 19:11:36,096: t15.2023.12.03 val PER: 0.1765
2026-01-03 19:11:36,096: t15.2023.12.08 val PER: 0.1911
2026-01-03 19:11:36,096: t15.2023.12.10 val PER: 0.1511
2026-01-03 19:11:36,096: t15.2023.12.17 val PER: 0.2110
2026-01-03 19:11:36,096: t15.2023.12.29 val PER: 0.2128
2026-01-03 19:11:36,096: t15.2024.02.25 val PER: 0.1882
2026-01-03 19:11:36,096: t15.2024.03.08 val PER: 0.2873
2026-01-03 19:11:36,096: t15.2024.03.15 val PER: 0.2714
2026-01-03 19:11:36,097: t15.2024.03.17 val PER: 0.2127
2026-01-03 19:11:36,097: t15.2024.05.10 val PER: 0.2259
2026-01-03 19:11:36,097: t15.2024.06.14 val PER: 0.2413
2026-01-03 19:11:36,097: t15.2024.07.19 val PER: 0.3184
2026-01-03 19:11:36,097: t15.2024.07.21 val PER: 0.1600
2026-01-03 19:11:36,097: t15.2024.07.28 val PER: 0.2044
2026-01-03 19:11:36,097: t15.2025.01.10 val PER: 0.3912
2026-01-03 19:11:36,097: t15.2025.01.12 val PER: 0.2356
2026-01-03 19:11:36,097: t15.2025.03.14 val PER: 0.3654
2026-01-03 19:11:36,097: t15.2025.03.16 val PER: 0.2762
2026-01-03 19:11:36,097: t15.2025.03.30 val PER: 0.3713
2026-01-03 19:11:36,097: t15.2025.04.13 val PER: 0.3010
2026-01-03 19:11:36,338: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_7500
2026-01-03 19:11:44,966: Train batch 7600: loss: 19.94 grad norm: 64.34 time: 0.068
2026-01-03 19:12:02,033: Train batch 7800: loss: 18.66 grad norm: 67.19 time: 0.054
2026-01-03 19:12:19,830: Train batch 8000: loss: 14.78 grad norm: 59.16 time: 0.071
2026-01-03 19:12:19,830: Running test after training batch: 8000
2026-01-03 19:12:20,212: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:12:25,759: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-03 19:12:25,790: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cost nit
2026-01-03 19:12:27,490: Val batch 8000: PER (avg): 0.2112 CTC Loss (avg): 20.5967 WER(1gram): 59.14% (n=64) time: 7.660
2026-01-03 19:12:27,491: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-03 19:12:27,491: t15.2023.08.13 val PER: 0.1892
2026-01-03 19:12:27,491: t15.2023.08.18 val PER: 0.1702
2026-01-03 19:12:27,491: t15.2023.08.20 val PER: 0.1660
2026-01-03 19:12:27,491: t15.2023.08.25 val PER: 0.1190
2026-01-03 19:12:27,491: t15.2023.08.27 val PER: 0.2331
2026-01-03 19:12:27,491: t15.2023.09.01 val PER: 0.1258
2026-01-03 19:12:27,491: t15.2023.09.03 val PER: 0.2078
2026-01-03 19:12:27,491: t15.2023.09.24 val PER: 0.1808
2026-01-03 19:12:27,491: t15.2023.09.29 val PER: 0.1646
2026-01-03 19:12:27,491: t15.2023.10.01 val PER: 0.2186
2026-01-03 19:12:27,492: t15.2023.10.06 val PER: 0.1335
2026-01-03 19:12:27,492: t15.2023.10.08 val PER: 0.2963
2026-01-03 19:12:27,492: t15.2023.10.13 val PER: 0.2622
2026-01-03 19:12:27,492: t15.2023.10.15 val PER: 0.2116
2026-01-03 19:12:27,492: t15.2023.10.20 val PER: 0.2349
2026-01-03 19:12:27,493: t15.2023.10.22 val PER: 0.1637
2026-01-03 19:12:27,493: t15.2023.11.03 val PER: 0.2293
2026-01-03 19:12:27,493: t15.2023.11.04 val PER: 0.0478
2026-01-03 19:12:27,493: t15.2023.11.17 val PER: 0.0731
2026-01-03 19:12:27,493: t15.2023.11.19 val PER: 0.0679
2026-01-03 19:12:27,493: t15.2023.11.26 val PER: 0.2188
2026-01-03 19:12:27,493: t15.2023.12.03 val PER: 0.1765
2026-01-03 19:12:27,493: t15.2023.12.08 val PER: 0.1884
2026-01-03 19:12:27,498: t15.2023.12.10 val PER: 0.1551
2026-01-03 19:12:27,498: t15.2023.12.17 val PER: 0.2100
2026-01-03 19:12:27,498: t15.2023.12.29 val PER: 0.2162
2026-01-03 19:12:27,498: t15.2024.02.25 val PER: 0.1657
2026-01-03 19:12:27,498: t15.2024.03.08 val PER: 0.3030
2026-01-03 19:12:27,498: t15.2024.03.15 val PER: 0.2564
2026-01-03 19:12:27,498: t15.2024.03.17 val PER: 0.2099
2026-01-03 19:12:27,498: t15.2024.05.10 val PER: 0.2110
2026-01-03 19:12:27,498: t15.2024.06.14 val PER: 0.2271
2026-01-03 19:12:27,499: t15.2024.07.19 val PER: 0.3191
2026-01-03 19:12:27,499: t15.2024.07.21 val PER: 0.1400
2026-01-03 19:12:27,499: t15.2024.07.28 val PER: 0.1846
2026-01-03 19:12:27,499: t15.2025.01.10 val PER: 0.3595
2026-01-03 19:12:27,499: t15.2025.01.12 val PER: 0.2271
2026-01-03 19:12:27,499: t15.2025.03.14 val PER: 0.3831
2026-01-03 19:12:27,499: t15.2025.03.16 val PER: 0.2657
2026-01-03 19:12:27,499: t15.2025.03.30 val PER: 0.3759
2026-01-03 19:12:27,499: t15.2025.04.13 val PER: 0.2924
2026-01-03 19:12:27,749: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_8000
2026-01-03 19:12:45,683: Train batch 8200: loss: 12.17 grad norm: 51.16 time: 0.053
2026-01-03 19:13:03,982: Train batch 8400: loss: 13.03 grad norm: 52.57 time: 0.064
2026-01-03 19:13:12,729: Running test after training batch: 8500
2026-01-03 19:13:12,841: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:13:17,522: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-03 19:13:17,554: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nett
2026-01-03 19:13:19,293: Val batch 8500: PER (avg): 0.2053 CTC Loss (avg): 20.1847 WER(1gram): 55.33% (n=64) time: 6.562
2026-01-03 19:13:19,293: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 19:13:19,293: t15.2023.08.13 val PER: 0.1788
2026-01-03 19:13:19,294: t15.2023.08.18 val PER: 0.1676
2026-01-03 19:13:19,294: t15.2023.08.20 val PER: 0.1628
2026-01-03 19:13:19,294: t15.2023.08.25 val PER: 0.1355
2026-01-03 19:13:19,294: t15.2023.08.27 val PER: 0.2251
2026-01-03 19:13:19,294: t15.2023.09.01 val PER: 0.1128
2026-01-03 19:13:19,294: t15.2023.09.03 val PER: 0.2197
2026-01-03 19:13:19,294: t15.2023.09.24 val PER: 0.1553
2026-01-03 19:13:19,294: t15.2023.09.29 val PER: 0.1736
2026-01-03 19:13:19,294: t15.2023.10.01 val PER: 0.2266
2026-01-03 19:13:19,295: t15.2023.10.06 val PER: 0.1227
2026-01-03 19:13:19,295: t15.2023.10.08 val PER: 0.2896
2026-01-03 19:13:19,295: t15.2023.10.13 val PER: 0.2676
2026-01-03 19:13:19,295: t15.2023.10.15 val PER: 0.2044
2026-01-03 19:13:19,295: t15.2023.10.20 val PER: 0.2047
2026-01-03 19:13:19,295: t15.2023.10.22 val PER: 0.1626
2026-01-03 19:13:19,295: t15.2023.11.03 val PER: 0.2076
2026-01-03 19:13:19,295: t15.2023.11.04 val PER: 0.0478
2026-01-03 19:13:19,295: t15.2023.11.17 val PER: 0.0731
2026-01-03 19:13:19,295: t15.2023.11.19 val PER: 0.0619
2026-01-03 19:13:19,295: t15.2023.11.26 val PER: 0.2145
2026-01-03 19:13:19,296: t15.2023.12.03 val PER: 0.1691
2026-01-03 19:13:19,296: t15.2023.12.08 val PER: 0.1811
2026-01-03 19:13:19,296: t15.2023.12.10 val PER: 0.1459
2026-01-03 19:13:19,296: t15.2023.12.17 val PER: 0.2089
2026-01-03 19:13:19,296: t15.2023.12.29 val PER: 0.2038
2026-01-03 19:13:19,296: t15.2024.02.25 val PER: 0.1573
2026-01-03 19:13:19,296: t15.2024.03.08 val PER: 0.2831
2026-01-03 19:13:19,296: t15.2024.03.15 val PER: 0.2552
2026-01-03 19:13:19,296: t15.2024.03.17 val PER: 0.2078
2026-01-03 19:13:19,297: t15.2024.05.10 val PER: 0.2110
2026-01-03 19:13:19,297: t15.2024.06.14 val PER: 0.2240
2026-01-03 19:13:19,297: t15.2024.07.19 val PER: 0.3013
2026-01-03 19:13:19,297: t15.2024.07.21 val PER: 0.1352
2026-01-03 19:13:19,297: t15.2024.07.28 val PER: 0.1963
2026-01-03 19:13:19,297: t15.2025.01.10 val PER: 0.3623
2026-01-03 19:13:19,297: t15.2025.01.12 val PER: 0.2240
2026-01-03 19:13:19,297: t15.2025.03.14 val PER: 0.3609
2026-01-03 19:13:19,298: t15.2025.03.16 val PER: 0.2408
2026-01-03 19:13:19,298: t15.2025.03.30 val PER: 0.3563
2026-01-03 19:13:19,298: t15.2025.04.13 val PER: 0.2653
2026-01-03 19:13:19,298: New best val WER(1gram) 58.12% --> 55.33%
2026-01-03 19:13:19,544: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_8500
2026-01-03 19:13:28,306: Train batch 8600: loss: 18.66 grad norm: 60.31 time: 0.055
2026-01-03 19:13:46,631: Train batch 8800: loss: 20.43 grad norm: 65.95 time: 0.060
2026-01-03 19:14:04,640: Train batch 9000: loss: 20.36 grad norm: 68.93 time: 0.072
2026-01-03 19:14:04,641: Running test after training batch: 9000
2026-01-03 19:14:04,758: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:14:09,586: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point us will
2026-01-03 19:14:09,618: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nett
2026-01-03 19:14:11,356: Val batch 9000: PER (avg): 0.2019 CTC Loss (avg): 19.7451 WER(1gram): 57.36% (n=64) time: 6.715
2026-01-03 19:14:11,356: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 19:14:11,356: t15.2023.08.13 val PER: 0.1726
2026-01-03 19:14:11,356: t15.2023.08.18 val PER: 0.1601
2026-01-03 19:14:11,356: t15.2023.08.20 val PER: 0.1533
2026-01-03 19:14:11,357: t15.2023.08.25 val PER: 0.1145
2026-01-03 19:14:11,357: t15.2023.08.27 val PER: 0.2299
2026-01-03 19:14:11,357: t15.2023.09.01 val PER: 0.1177
2026-01-03 19:14:11,357: t15.2023.09.03 val PER: 0.1960
2026-01-03 19:14:11,357: t15.2023.09.24 val PER: 0.1529
2026-01-03 19:14:11,357: t15.2023.09.29 val PER: 0.1678
2026-01-03 19:14:11,357: t15.2023.10.01 val PER: 0.2232
2026-01-03 19:14:11,357: t15.2023.10.06 val PER: 0.1238
2026-01-03 19:14:11,357: t15.2023.10.08 val PER: 0.2936
2026-01-03 19:14:11,358: t15.2023.10.13 val PER: 0.2692
2026-01-03 19:14:11,358: t15.2023.10.15 val PER: 0.2109
2026-01-03 19:14:11,358: t15.2023.10.20 val PER: 0.1946
2026-01-03 19:14:11,358: t15.2023.10.22 val PER: 0.1670
2026-01-03 19:14:11,358: t15.2023.11.03 val PER: 0.2205
2026-01-03 19:14:11,358: t15.2023.11.04 val PER: 0.0410
2026-01-03 19:14:11,358: t15.2023.11.17 val PER: 0.0715
2026-01-03 19:14:11,358: t15.2023.11.19 val PER: 0.0619
2026-01-03 19:14:11,358: t15.2023.11.26 val PER: 0.2152
2026-01-03 19:14:11,358: t15.2023.12.03 val PER: 0.1628
2026-01-03 19:14:11,359: t15.2023.12.08 val PER: 0.1651
2026-01-03 19:14:11,359: t15.2023.12.10 val PER: 0.1301
2026-01-03 19:14:11,359: t15.2023.12.17 val PER: 0.1933
2026-01-03 19:14:11,359: t15.2023.12.29 val PER: 0.1949
2026-01-03 19:14:11,359: t15.2024.02.25 val PER: 0.1643
2026-01-03 19:14:11,359: t15.2024.03.08 val PER: 0.2802
2026-01-03 19:14:11,359: t15.2024.03.15 val PER: 0.2452
2026-01-03 19:14:11,359: t15.2024.03.17 val PER: 0.2092
2026-01-03 19:14:11,359: t15.2024.05.10 val PER: 0.2140
2026-01-03 19:14:11,360: t15.2024.06.14 val PER: 0.2114
2026-01-03 19:14:11,360: t15.2024.07.19 val PER: 0.2993
2026-01-03 19:14:11,360: t15.2024.07.21 val PER: 0.1407
2026-01-03 19:14:11,360: t15.2024.07.28 val PER: 0.1882
2026-01-03 19:14:11,360: t15.2025.01.10 val PER: 0.3650
2026-01-03 19:14:11,360: t15.2025.01.12 val PER: 0.2132
2026-01-03 19:14:11,360: t15.2025.03.14 val PER: 0.3624
2026-01-03 19:14:11,360: t15.2025.03.16 val PER: 0.2435
2026-01-03 19:14:11,360: t15.2025.03.30 val PER: 0.3391
2026-01-03 19:14:11,360: t15.2025.04.13 val PER: 0.2782
2026-01-03 19:14:11,599: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_9000
2026-01-03 19:14:29,045: Train batch 9200: loss: 13.97 grad norm: 54.20 time: 0.055
2026-01-03 19:14:46,835: Train batch 9400: loss: 10.75 grad norm: 53.83 time: 0.067
2026-01-03 19:14:55,550: Running test after training batch: 9500
2026-01-03 19:14:55,704: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:15:00,411: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-03 19:15:00,443: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-03 19:15:02,161: Val batch 9500: PER (avg): 0.1965 CTC Loss (avg): 19.4479 WER(1gram): 56.60% (n=64) time: 6.611
2026-01-03 19:15:02,162: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 19:15:02,162: t15.2023.08.13 val PER: 0.1570
2026-01-03 19:15:02,162: t15.2023.08.18 val PER: 0.1500
2026-01-03 19:15:02,162: t15.2023.08.20 val PER: 0.1533
2026-01-03 19:15:02,162: t15.2023.08.25 val PER: 0.1114
2026-01-03 19:15:02,162: t15.2023.08.27 val PER: 0.2186
2026-01-03 19:15:02,162: t15.2023.09.01 val PER: 0.1112
2026-01-03 19:15:02,162: t15.2023.09.03 val PER: 0.1924
2026-01-03 19:15:02,162: t15.2023.09.24 val PER: 0.1566
2026-01-03 19:15:02,163: t15.2023.09.29 val PER: 0.1634
2026-01-03 19:15:02,163: t15.2023.10.01 val PER: 0.2100
2026-01-03 19:15:02,163: t15.2023.10.06 val PER: 0.1206
2026-01-03 19:15:02,163: t15.2023.10.08 val PER: 0.2801
2026-01-03 19:15:02,163: t15.2023.10.13 val PER: 0.2552
2026-01-03 19:15:02,163: t15.2023.10.15 val PER: 0.1984
2026-01-03 19:15:02,163: t15.2023.10.20 val PER: 0.1913
2026-01-03 19:15:02,163: t15.2023.10.22 val PER: 0.1559
2026-01-03 19:15:02,163: t15.2023.11.03 val PER: 0.2151
2026-01-03 19:15:02,163: t15.2023.11.04 val PER: 0.0410
2026-01-03 19:15:02,163: t15.2023.11.17 val PER: 0.0669
2026-01-03 19:15:02,163: t15.2023.11.19 val PER: 0.0719
2026-01-03 19:15:02,163: t15.2023.11.26 val PER: 0.1986
2026-01-03 19:15:02,164: t15.2023.12.03 val PER: 0.1586
2026-01-03 19:15:02,164: t15.2023.12.08 val PER: 0.1684
2026-01-03 19:15:02,164: t15.2023.12.10 val PER: 0.1301
2026-01-03 19:15:02,164: t15.2023.12.17 val PER: 0.1788
2026-01-03 19:15:02,164: t15.2023.12.29 val PER: 0.1860
2026-01-03 19:15:02,164: t15.2024.02.25 val PER: 0.1419
2026-01-03 19:15:02,164: t15.2024.03.08 val PER: 0.2674
2026-01-03 19:15:02,164: t15.2024.03.15 val PER: 0.2520
2026-01-03 19:15:02,164: t15.2024.03.17 val PER: 0.1987
2026-01-03 19:15:02,164: t15.2024.05.10 val PER: 0.2065
2026-01-03 19:15:02,165: t15.2024.06.14 val PER: 0.2098
2026-01-03 19:15:02,165: t15.2024.07.19 val PER: 0.2914
2026-01-03 19:15:02,165: t15.2024.07.21 val PER: 0.1462
2026-01-03 19:15:02,165: t15.2024.07.28 val PER: 0.1890
2026-01-03 19:15:02,165: t15.2025.01.10 val PER: 0.3512
2026-01-03 19:15:02,165: t15.2025.01.12 val PER: 0.2163
2026-01-03 19:15:02,165: t15.2025.03.14 val PER: 0.3787
2026-01-03 19:15:02,165: t15.2025.03.16 val PER: 0.2461
2026-01-03 19:15:02,165: t15.2025.03.30 val PER: 0.3425
2026-01-03 19:15:02,165: t15.2025.04.13 val PER: 0.2639
2026-01-03 19:15:02,405: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_9500
2026-01-03 19:15:11,784: Train batch 9600: loss: 11.53 grad norm: 52.66 time: 0.073
2026-01-03 19:15:28,959: Train batch 9800: loss: 17.10 grad norm: 64.89 time: 0.062
2026-01-03 19:15:46,423: Train batch 10000: loss: 7.97 grad norm: 43.40 time: 0.061
2026-01-03 19:15:46,424: Running test after training batch: 10000
2026-01-03 19:15:46,625: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:15:51,333: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-03 19:15:51,364: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-03 19:15:53,088: Val batch 10000: PER (avg): 0.1942 CTC Loss (avg): 18.9676 WER(1gram): 57.36% (n=64) time: 6.664
2026-01-03 19:15:53,088: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 19:15:53,088: t15.2023.08.13 val PER: 0.1642
2026-01-03 19:15:53,089: t15.2023.08.18 val PER: 0.1551
2026-01-03 19:15:53,089: t15.2023.08.20 val PER: 0.1446
2026-01-03 19:15:53,089: t15.2023.08.25 val PER: 0.1190
2026-01-03 19:15:53,089: t15.2023.08.27 val PER: 0.2203
2026-01-03 19:15:53,089: t15.2023.09.01 val PER: 0.1096
2026-01-03 19:15:53,089: t15.2023.09.03 val PER: 0.1983
2026-01-03 19:15:53,089: t15.2023.09.24 val PER: 0.1481
2026-01-03 19:15:53,089: t15.2023.09.29 val PER: 0.1672
2026-01-03 19:15:53,089: t15.2023.10.01 val PER: 0.2041
2026-01-03 19:15:53,090: t15.2023.10.06 val PER: 0.1195
2026-01-03 19:15:53,090: t15.2023.10.08 val PER: 0.2760
2026-01-03 19:15:53,090: t15.2023.10.13 val PER: 0.2483
2026-01-03 19:15:53,090: t15.2023.10.15 val PER: 0.1905
2026-01-03 19:15:53,090: t15.2023.10.20 val PER: 0.1913
2026-01-03 19:15:53,090: t15.2023.10.22 val PER: 0.1537
2026-01-03 19:15:53,090: t15.2023.11.03 val PER: 0.2123
2026-01-03 19:15:53,090: t15.2023.11.04 val PER: 0.0444
2026-01-03 19:15:53,090: t15.2023.11.17 val PER: 0.0575
2026-01-03 19:15:53,090: t15.2023.11.19 val PER: 0.0579
2026-01-03 19:15:53,090: t15.2023.11.26 val PER: 0.1935
2026-01-03 19:15:53,091: t15.2023.12.03 val PER: 0.1534
2026-01-03 19:15:53,091: t15.2023.12.08 val PER: 0.1691
2026-01-03 19:15:53,091: t15.2023.12.10 val PER: 0.1288
2026-01-03 19:15:53,091: t15.2023.12.17 val PER: 0.1913
2026-01-03 19:15:53,091: t15.2023.12.29 val PER: 0.1867
2026-01-03 19:15:53,091: t15.2024.02.25 val PER: 0.1559
2026-01-03 19:15:53,091: t15.2024.03.08 val PER: 0.2688
2026-01-03 19:15:53,091: t15.2024.03.15 val PER: 0.2539
2026-01-03 19:15:53,091: t15.2024.03.17 val PER: 0.1946
2026-01-03 19:15:53,091: t15.2024.05.10 val PER: 0.2080
2026-01-03 19:15:53,092: t15.2024.06.14 val PER: 0.2114
2026-01-03 19:15:53,092: t15.2024.07.19 val PER: 0.2900
2026-01-03 19:15:53,092: t15.2024.07.21 val PER: 0.1345
2026-01-03 19:15:53,092: t15.2024.07.28 val PER: 0.1809
2026-01-03 19:15:53,092: t15.2025.01.10 val PER: 0.3471
2026-01-03 19:15:53,092: t15.2025.01.12 val PER: 0.2055
2026-01-03 19:15:53,092: t15.2025.03.14 val PER: 0.3624
2026-01-03 19:15:53,092: t15.2025.03.16 val PER: 0.2474
2026-01-03 19:15:53,092: t15.2025.03.30 val PER: 0.3356
2026-01-03 19:15:53,092: t15.2025.04.13 val PER: 0.2639
2026-01-03 19:15:53,352: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_10000
2026-01-03 19:16:10,569: Train batch 10200: loss: 8.82 grad norm: 44.58 time: 0.049
2026-01-03 19:16:28,429: Train batch 10400: loss: 12.84 grad norm: 54.64 time: 0.072
2026-01-03 19:16:37,220: Running test after training batch: 10500
2026-01-03 19:16:37,350: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:16:42,044: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-03 19:16:42,077: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-03 19:16:43,870: Val batch 10500: PER (avg): 0.1930 CTC Loss (avg): 18.9397 WER(1gram): 57.36% (n=64) time: 6.650
2026-01-03 19:16:43,870: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 19:16:43,871: t15.2023.08.13 val PER: 0.1611
2026-01-03 19:16:43,871: t15.2023.08.18 val PER: 0.1492
2026-01-03 19:16:43,871: t15.2023.08.20 val PER: 0.1509
2026-01-03 19:16:43,871: t15.2023.08.25 val PER: 0.1145
2026-01-03 19:16:43,871: t15.2023.08.27 val PER: 0.2219
2026-01-03 19:16:43,871: t15.2023.09.01 val PER: 0.1112
2026-01-03 19:16:43,871: t15.2023.09.03 val PER: 0.1888
2026-01-03 19:16:43,871: t15.2023.09.24 val PER: 0.1687
2026-01-03 19:16:43,871: t15.2023.09.29 val PER: 0.1640
2026-01-03 19:16:43,871: t15.2023.10.01 val PER: 0.2041
2026-01-03 19:16:43,871: t15.2023.10.06 val PER: 0.1109
2026-01-03 19:16:43,871: t15.2023.10.08 val PER: 0.2801
2026-01-03 19:16:43,871: t15.2023.10.13 val PER: 0.2436
2026-01-03 19:16:43,872: t15.2023.10.15 val PER: 0.1951
2026-01-03 19:16:43,872: t15.2023.10.20 val PER: 0.1913
2026-01-03 19:16:43,872: t15.2023.10.22 val PER: 0.1437
2026-01-03 19:16:43,872: t15.2023.11.03 val PER: 0.2137
2026-01-03 19:16:43,872: t15.2023.11.04 val PER: 0.0580
2026-01-03 19:16:43,872: t15.2023.11.17 val PER: 0.0684
2026-01-03 19:16:43,872: t15.2023.11.19 val PER: 0.0679
2026-01-03 19:16:43,872: t15.2023.11.26 val PER: 0.1942
2026-01-03 19:16:43,872: t15.2023.12.03 val PER: 0.1649
2026-01-03 19:16:43,872: t15.2023.12.08 val PER: 0.1545
2026-01-03 19:16:43,872: t15.2023.12.10 val PER: 0.1340
2026-01-03 19:16:43,872: t15.2023.12.17 val PER: 0.1767
2026-01-03 19:16:43,872: t15.2023.12.29 val PER: 0.1908
2026-01-03 19:16:43,872: t15.2024.02.25 val PER: 0.1545
2026-01-03 19:16:43,873: t15.2024.03.08 val PER: 0.2617
2026-01-03 19:16:43,873: t15.2024.03.15 val PER: 0.2477
2026-01-03 19:16:43,873: t15.2024.03.17 val PER: 0.1848
2026-01-03 19:16:43,873: t15.2024.05.10 val PER: 0.2006
2026-01-03 19:16:43,873: t15.2024.06.14 val PER: 0.2114
2026-01-03 19:16:43,873: t15.2024.07.19 val PER: 0.2854
2026-01-03 19:16:43,873: t15.2024.07.21 val PER: 0.1283
2026-01-03 19:16:43,873: t15.2024.07.28 val PER: 0.1713
2026-01-03 19:16:43,873: t15.2025.01.10 val PER: 0.3444
2026-01-03 19:16:43,873: t15.2025.01.12 val PER: 0.2071
2026-01-03 19:16:43,873: t15.2025.03.14 val PER: 0.3831
2026-01-03 19:16:43,873: t15.2025.03.16 val PER: 0.2474
2026-01-03 19:16:43,873: t15.2025.03.30 val PER: 0.3460
2026-01-03 19:16:43,873: t15.2025.04.13 val PER: 0.2611
2026-01-03 19:16:44,132: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_10500
2026-01-03 19:16:53,035: Train batch 10600: loss: 12.66 grad norm: 60.37 time: 0.072
2026-01-03 19:17:10,460: Train batch 10800: loss: 19.31 grad norm: 77.58 time: 0.063
2026-01-03 19:17:27,671: Train batch 11000: loss: 19.25 grad norm: 66.71 time: 0.056
2026-01-03 19:17:27,671: Running test after training batch: 11000
2026-01-03 19:17:27,814: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:17:32,489: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 19:17:32,521: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 19:17:34,306: Val batch 11000: PER (avg): 0.1890 CTC Loss (avg): 18.5647 WER(1gram): 53.05% (n=64) time: 6.635
2026-01-03 19:17:34,307: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 19:17:34,307: t15.2023.08.13 val PER: 0.1466
2026-01-03 19:17:34,307: t15.2023.08.18 val PER: 0.1500
2026-01-03 19:17:34,307: t15.2023.08.20 val PER: 0.1477
2026-01-03 19:17:34,307: t15.2023.08.25 val PER: 0.1054
2026-01-03 19:17:34,307: t15.2023.08.27 val PER: 0.2315
2026-01-03 19:17:34,307: t15.2023.09.01 val PER: 0.1031
2026-01-03 19:17:34,307: t15.2023.09.03 val PER: 0.2007
2026-01-03 19:17:34,307: t15.2023.09.24 val PER: 0.1481
2026-01-03 19:17:34,307: t15.2023.09.29 val PER: 0.1608
2026-01-03 19:17:34,308: t15.2023.10.01 val PER: 0.2147
2026-01-03 19:17:34,308: t15.2023.10.06 val PER: 0.1012
2026-01-03 19:17:34,308: t15.2023.10.08 val PER: 0.2693
2026-01-03 19:17:34,308: t15.2023.10.13 val PER: 0.2459
2026-01-03 19:17:34,308: t15.2023.10.15 val PER: 0.1892
2026-01-03 19:17:34,308: t15.2023.10.20 val PER: 0.1946
2026-01-03 19:17:34,308: t15.2023.10.22 val PER: 0.1359
2026-01-03 19:17:34,308: t15.2023.11.03 val PER: 0.2062
2026-01-03 19:17:34,308: t15.2023.11.04 val PER: 0.0478
2026-01-03 19:17:34,308: t15.2023.11.17 val PER: 0.0622
2026-01-03 19:17:34,308: t15.2023.11.19 val PER: 0.0599
2026-01-03 19:17:34,308: t15.2023.11.26 val PER: 0.1891
2026-01-03 19:17:34,308: t15.2023.12.03 val PER: 0.1544
2026-01-03 19:17:34,309: t15.2023.12.08 val PER: 0.1558
2026-01-03 19:17:34,309: t15.2023.12.10 val PER: 0.1261
2026-01-03 19:17:34,309: t15.2023.12.17 val PER: 0.1788
2026-01-03 19:17:34,309: t15.2023.12.29 val PER: 0.1743
2026-01-03 19:17:34,309: t15.2024.02.25 val PER: 0.1615
2026-01-03 19:17:34,309: t15.2024.03.08 val PER: 0.2603
2026-01-03 19:17:34,309: t15.2024.03.15 val PER: 0.2489
2026-01-03 19:17:34,309: t15.2024.03.17 val PER: 0.1869
2026-01-03 19:17:34,309: t15.2024.05.10 val PER: 0.2006
2026-01-03 19:17:34,309: t15.2024.06.14 val PER: 0.2019
2026-01-03 19:17:34,309: t15.2024.07.19 val PER: 0.2795
2026-01-03 19:17:34,309: t15.2024.07.21 val PER: 0.1331
2026-01-03 19:17:34,309: t15.2024.07.28 val PER: 0.1684
2026-01-03 19:17:34,309: t15.2025.01.10 val PER: 0.3499
2026-01-03 19:17:34,309: t15.2025.01.12 val PER: 0.1871
2026-01-03 19:17:34,309: t15.2025.03.14 val PER: 0.3624
2026-01-03 19:17:34,310: t15.2025.03.16 val PER: 0.2317
2026-01-03 19:17:34,310: t15.2025.03.30 val PER: 0.3460
2026-01-03 19:17:34,310: t15.2025.04.13 val PER: 0.2639
2026-01-03 19:17:34,311: New best val WER(1gram) 55.33% --> 53.05%
2026-01-03 19:17:34,576: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_11000
2026-01-03 19:17:51,795: Train batch 11200: loss: 14.11 grad norm: 59.79 time: 0.071
2026-01-03 19:18:10,008: Train batch 11400: loss: 13.41 grad norm: 61.82 time: 0.056
2026-01-03 19:18:18,700: Running test after training batch: 11500
2026-01-03 19:18:18,855: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:18:23,728: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-03 19:18:23,760: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-03 19:18:25,554: Val batch 11500: PER (avg): 0.1847 CTC Loss (avg): 18.3380 WER(1gram): 54.06% (n=64) time: 6.854
2026-01-03 19:18:25,555: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 19:18:25,555: t15.2023.08.13 val PER: 0.1435
2026-01-03 19:18:25,555: t15.2023.08.18 val PER: 0.1425
2026-01-03 19:18:25,555: t15.2023.08.20 val PER: 0.1469
2026-01-03 19:18:25,555: t15.2023.08.25 val PER: 0.1069
2026-01-03 19:18:25,555: t15.2023.08.27 val PER: 0.2186
2026-01-03 19:18:25,555: t15.2023.09.01 val PER: 0.1031
2026-01-03 19:18:25,555: t15.2023.09.03 val PER: 0.1936
2026-01-03 19:18:25,555: t15.2023.09.24 val PER: 0.1335
2026-01-03 19:18:25,555: t15.2023.09.29 val PER: 0.1557
2026-01-03 19:18:25,556: t15.2023.10.01 val PER: 0.1982
2026-01-03 19:18:25,556: t15.2023.10.06 val PER: 0.1066
2026-01-03 19:18:25,556: t15.2023.10.08 val PER: 0.2706
2026-01-03 19:18:25,556: t15.2023.10.13 val PER: 0.2436
2026-01-03 19:18:25,556: t15.2023.10.15 val PER: 0.1852
2026-01-03 19:18:25,556: t15.2023.10.20 val PER: 0.2047
2026-01-03 19:18:25,556: t15.2023.10.22 val PER: 0.1359
2026-01-03 19:18:25,556: t15.2023.11.03 val PER: 0.1967
2026-01-03 19:18:25,556: t15.2023.11.04 val PER: 0.0444
2026-01-03 19:18:25,556: t15.2023.11.17 val PER: 0.0591
2026-01-03 19:18:25,556: t15.2023.11.19 val PER: 0.0679
2026-01-03 19:18:25,556: t15.2023.11.26 val PER: 0.1804
2026-01-03 19:18:25,556: t15.2023.12.03 val PER: 0.1544
2026-01-03 19:18:25,556: t15.2023.12.08 val PER: 0.1491
2026-01-03 19:18:25,556: t15.2023.12.10 val PER: 0.1209
2026-01-03 19:18:25,557: t15.2023.12.17 val PER: 0.1778
2026-01-03 19:18:25,557: t15.2023.12.29 val PER: 0.1661
2026-01-03 19:18:25,557: t15.2024.02.25 val PER: 0.1503
2026-01-03 19:18:25,557: t15.2024.03.08 val PER: 0.2617
2026-01-03 19:18:25,557: t15.2024.03.15 val PER: 0.2351
2026-01-03 19:18:25,557: t15.2024.03.17 val PER: 0.1771
2026-01-03 19:18:25,557: t15.2024.05.10 val PER: 0.2006
2026-01-03 19:18:25,557: t15.2024.06.14 val PER: 0.2114
2026-01-03 19:18:25,557: t15.2024.07.19 val PER: 0.2828
2026-01-03 19:18:25,557: t15.2024.07.21 val PER: 0.1276
2026-01-03 19:18:25,557: t15.2024.07.28 val PER: 0.1662
2026-01-03 19:18:25,557: t15.2025.01.10 val PER: 0.3526
2026-01-03 19:18:25,557: t15.2025.01.12 val PER: 0.1878
2026-01-03 19:18:25,557: t15.2025.03.14 val PER: 0.3639
2026-01-03 19:18:25,557: t15.2025.03.16 val PER: 0.2356
2026-01-03 19:18:25,557: t15.2025.03.30 val PER: 0.3345
2026-01-03 19:18:25,558: t15.2025.04.13 val PER: 0.2525
2026-01-03 19:18:25,816: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_11500
2026-01-03 19:18:34,233: Train batch 11600: loss: 14.42 grad norm: 53.16 time: 0.060
2026-01-03 19:18:51,444: Train batch 11800: loss: 9.29 grad norm: 47.21 time: 0.044
2026-01-03 19:19:09,103: Train batch 12000: loss: 17.76 grad norm: 58.92 time: 0.071
2026-01-03 19:19:09,104: Running test after training batch: 12000
2026-01-03 19:19:09,199: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:19:13,885: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-03 19:19:13,917: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 19:19:15,724: Val batch 12000: PER (avg): 0.1832 CTC Loss (avg): 18.0919 WER(1gram): 54.57% (n=64) time: 6.620
2026-01-03 19:19:15,724: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 19:19:15,724: t15.2023.08.13 val PER: 0.1403
2026-01-03 19:19:15,724: t15.2023.08.18 val PER: 0.1408
2026-01-03 19:19:15,724: t15.2023.08.20 val PER: 0.1461
2026-01-03 19:19:15,724: t15.2023.08.25 val PER: 0.1039
2026-01-03 19:19:15,725: t15.2023.08.27 val PER: 0.2154
2026-01-03 19:19:15,725: t15.2023.09.01 val PER: 0.1055
2026-01-03 19:19:15,725: t15.2023.09.03 val PER: 0.1841
2026-01-03 19:19:15,725: t15.2023.09.24 val PER: 0.1493
2026-01-03 19:19:15,725: t15.2023.09.29 val PER: 0.1532
2026-01-03 19:19:15,725: t15.2023.10.01 val PER: 0.1975
2026-01-03 19:19:15,725: t15.2023.10.06 val PER: 0.1033
2026-01-03 19:19:15,725: t15.2023.10.08 val PER: 0.2652
2026-01-03 19:19:15,725: t15.2023.10.13 val PER: 0.2382
2026-01-03 19:19:15,725: t15.2023.10.15 val PER: 0.1879
2026-01-03 19:19:15,725: t15.2023.10.20 val PER: 0.1980
2026-01-03 19:19:15,725: t15.2023.10.22 val PER: 0.1303
2026-01-03 19:19:15,725: t15.2023.11.03 val PER: 0.1913
2026-01-03 19:19:15,725: t15.2023.11.04 val PER: 0.0410
2026-01-03 19:19:15,726: t15.2023.11.17 val PER: 0.0607
2026-01-03 19:19:15,726: t15.2023.11.19 val PER: 0.0539
2026-01-03 19:19:15,726: t15.2023.11.26 val PER: 0.1754
2026-01-03 19:19:15,726: t15.2023.12.03 val PER: 0.1607
2026-01-03 19:19:15,726: t15.2023.12.08 val PER: 0.1525
2026-01-03 19:19:15,726: t15.2023.12.10 val PER: 0.1301
2026-01-03 19:19:15,726: t15.2023.12.17 val PER: 0.1674
2026-01-03 19:19:15,726: t15.2023.12.29 val PER: 0.1730
2026-01-03 19:19:15,726: t15.2024.02.25 val PER: 0.1433
2026-01-03 19:19:15,726: t15.2024.03.08 val PER: 0.2802
2026-01-03 19:19:15,726: t15.2024.03.15 val PER: 0.2452
2026-01-03 19:19:15,726: t15.2024.03.17 val PER: 0.1778
2026-01-03 19:19:15,727: t15.2024.05.10 val PER: 0.1902
2026-01-03 19:19:15,727: t15.2024.06.14 val PER: 0.2035
2026-01-03 19:19:15,727: t15.2024.07.19 val PER: 0.2762
2026-01-03 19:19:15,727: t15.2024.07.21 val PER: 0.1317
2026-01-03 19:19:15,727: t15.2024.07.28 val PER: 0.1640
2026-01-03 19:19:15,727: t15.2025.01.10 val PER: 0.3388
2026-01-03 19:19:15,727: t15.2025.01.12 val PER: 0.1863
2026-01-03 19:19:15,727: t15.2025.03.14 val PER: 0.3521
2026-01-03 19:19:15,727: t15.2025.03.16 val PER: 0.2395
2026-01-03 19:19:15,727: t15.2025.03.30 val PER: 0.3218
2026-01-03 19:19:15,727: t15.2025.04.13 val PER: 0.2325
2026-01-03 19:19:16,207: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_12000
2026-01-03 19:19:33,790: Train batch 12200: loss: 8.31 grad norm: 46.91 time: 0.067
2026-01-03 19:19:51,316: Train batch 12400: loss: 6.63 grad norm: 40.22 time: 0.041
2026-01-03 19:20:00,357: Running test after training batch: 12500
2026-01-03 19:20:00,457: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:20:05,419: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 19:20:05,452: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 19:20:07,256: Val batch 12500: PER (avg): 0.1802 CTC Loss (avg): 17.7861 WER(1gram): 52.03% (n=64) time: 6.899
2026-01-03 19:20:07,257: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 19:20:07,257: t15.2023.08.13 val PER: 0.1435
2026-01-03 19:20:07,257: t15.2023.08.18 val PER: 0.1257
2026-01-03 19:20:07,257: t15.2023.08.20 val PER: 0.1422
2026-01-03 19:20:07,257: t15.2023.08.25 val PER: 0.0949
2026-01-03 19:20:07,257: t15.2023.08.27 val PER: 0.2138
2026-01-03 19:20:07,257: t15.2023.09.01 val PER: 0.1006
2026-01-03 19:20:07,257: t15.2023.09.03 val PER: 0.1781
2026-01-03 19:20:07,257: t15.2023.09.24 val PER: 0.1444
2026-01-03 19:20:07,258: t15.2023.09.29 val PER: 0.1532
2026-01-03 19:20:07,258: t15.2023.10.01 val PER: 0.1968
2026-01-03 19:20:07,258: t15.2023.10.06 val PER: 0.1012
2026-01-03 19:20:07,258: t15.2023.10.08 val PER: 0.2652
2026-01-03 19:20:07,258: t15.2023.10.13 val PER: 0.2428
2026-01-03 19:20:07,258: t15.2023.10.15 val PER: 0.1813
2026-01-03 19:20:07,258: t15.2023.10.20 val PER: 0.1779
2026-01-03 19:20:07,258: t15.2023.10.22 val PER: 0.1336
2026-01-03 19:20:07,258: t15.2023.11.03 val PER: 0.1967
2026-01-03 19:20:07,258: t15.2023.11.04 val PER: 0.0375
2026-01-03 19:20:07,258: t15.2023.11.17 val PER: 0.0638
2026-01-03 19:20:07,258: t15.2023.11.19 val PER: 0.0559
2026-01-03 19:20:07,258: t15.2023.11.26 val PER: 0.1790
2026-01-03 19:20:07,258: t15.2023.12.03 val PER: 0.1502
2026-01-03 19:20:07,258: t15.2023.12.08 val PER: 0.1438
2026-01-03 19:20:07,259: t15.2023.12.10 val PER: 0.1196
2026-01-03 19:20:07,259: t15.2023.12.17 val PER: 0.1798
2026-01-03 19:20:07,259: t15.2023.12.29 val PER: 0.1606
2026-01-03 19:20:07,259: t15.2024.02.25 val PER: 0.1447
2026-01-03 19:20:07,259: t15.2024.03.08 val PER: 0.2489
2026-01-03 19:20:07,259: t15.2024.03.15 val PER: 0.2358
2026-01-03 19:20:07,259: t15.2024.03.17 val PER: 0.1722
2026-01-03 19:20:07,259: t15.2024.05.10 val PER: 0.1976
2026-01-03 19:20:07,259: t15.2024.06.14 val PER: 0.2066
2026-01-03 19:20:07,259: t15.2024.07.19 val PER: 0.2808
2026-01-03 19:20:07,260: t15.2024.07.21 val PER: 0.1234
2026-01-03 19:20:07,260: t15.2024.07.28 val PER: 0.1588
2026-01-03 19:20:07,260: t15.2025.01.10 val PER: 0.3402
2026-01-03 19:20:07,260: t15.2025.01.12 val PER: 0.1771
2026-01-03 19:20:07,260: t15.2025.03.14 val PER: 0.3536
2026-01-03 19:20:07,260: t15.2025.03.16 val PER: 0.2330
2026-01-03 19:20:07,260: t15.2025.03.30 val PER: 0.3345
2026-01-03 19:20:07,260: t15.2025.04.13 val PER: 0.2411
2026-01-03 19:20:07,261: New best val WER(1gram) 53.05% --> 52.03%
2026-01-03 19:20:07,529: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_12500
2026-01-03 19:20:16,263: Train batch 12600: loss: 11.05 grad norm: 53.51 time: 0.057
2026-01-03 19:20:34,049: Train batch 12800: loss: 8.92 grad norm: 47.91 time: 0.052
2026-01-03 19:20:51,863: Train batch 13000: loss: 9.43 grad norm: 49.89 time: 0.065
2026-01-03 19:20:51,863: Running test after training batch: 13000
2026-01-03 19:20:51,973: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:20:56,642: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-03 19:20:56,674: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 19:20:58,536: Val batch 13000: PER (avg): 0.1786 CTC Loss (avg): 17.6750 WER(1gram): 51.02% (n=64) time: 6.672
2026-01-03 19:20:58,536: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 19:20:58,536: t15.2023.08.13 val PER: 0.1424
2026-01-03 19:20:58,537: t15.2023.08.18 val PER: 0.1257
2026-01-03 19:20:58,537: t15.2023.08.20 val PER: 0.1342
2026-01-03 19:20:58,537: t15.2023.08.25 val PER: 0.0949
2026-01-03 19:20:58,537: t15.2023.08.27 val PER: 0.2235
2026-01-03 19:20:58,537: t15.2023.09.01 val PER: 0.0982
2026-01-03 19:20:58,537: t15.2023.09.03 val PER: 0.1876
2026-01-03 19:20:58,537: t15.2023.09.24 val PER: 0.1408
2026-01-03 19:20:58,537: t15.2023.09.29 val PER: 0.1442
2026-01-03 19:20:58,537: t15.2023.10.01 val PER: 0.1942
2026-01-03 19:20:58,537: t15.2023.10.06 val PER: 0.1012
2026-01-03 19:20:58,538: t15.2023.10.08 val PER: 0.2544
2026-01-03 19:20:58,538: t15.2023.10.13 val PER: 0.2335
2026-01-03 19:20:58,538: t15.2023.10.15 val PER: 0.1800
2026-01-03 19:20:58,538: t15.2023.10.20 val PER: 0.1779
2026-01-03 19:20:58,538: t15.2023.10.22 val PER: 0.1325
2026-01-03 19:20:58,538: t15.2023.11.03 val PER: 0.1927
2026-01-03 19:20:58,538: t15.2023.11.04 val PER: 0.0341
2026-01-03 19:20:58,538: t15.2023.11.17 val PER: 0.0560
2026-01-03 19:20:58,538: t15.2023.11.19 val PER: 0.0519
2026-01-03 19:20:58,538: t15.2023.11.26 val PER: 0.1710
2026-01-03 19:20:58,539: t15.2023.12.03 val PER: 0.1576
2026-01-03 19:20:58,539: t15.2023.12.08 val PER: 0.1445
2026-01-03 19:20:58,539: t15.2023.12.10 val PER: 0.1301
2026-01-03 19:20:58,539: t15.2023.12.17 val PER: 0.1580
2026-01-03 19:20:58,539: t15.2023.12.29 val PER: 0.1819
2026-01-03 19:20:58,539: t15.2024.02.25 val PER: 0.1447
2026-01-03 19:20:58,539: t15.2024.03.08 val PER: 0.2674
2026-01-03 19:20:58,539: t15.2024.03.15 val PER: 0.2314
2026-01-03 19:20:58,539: t15.2024.03.17 val PER: 0.1702
2026-01-03 19:20:58,539: t15.2024.05.10 val PER: 0.1917
2026-01-03 19:20:58,539: t15.2024.06.14 val PER: 0.2082
2026-01-03 19:20:58,539: t15.2024.07.19 val PER: 0.2736
2026-01-03 19:20:58,539: t15.2024.07.21 val PER: 0.1193
2026-01-03 19:20:58,540: t15.2024.07.28 val PER: 0.1632
2026-01-03 19:20:58,540: t15.2025.01.10 val PER: 0.3278
2026-01-03 19:20:58,540: t15.2025.01.12 val PER: 0.1840
2026-01-03 19:20:58,540: t15.2025.03.14 val PER: 0.3506
2026-01-03 19:20:58,540: t15.2025.03.16 val PER: 0.2186
2026-01-03 19:20:58,540: t15.2025.03.30 val PER: 0.3253
2026-01-03 19:20:58,540: t15.2025.04.13 val PER: 0.2496
2026-01-03 19:20:58,541: New best val WER(1gram) 52.03% --> 51.02%
2026-01-03 19:20:58,807: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_13000
2026-01-03 19:21:16,017: Train batch 13200: loss: 17.86 grad norm: 69.84 time: 0.053
2026-01-03 19:21:33,380: Train batch 13400: loss: 13.28 grad norm: 62.80 time: 0.062
2026-01-03 19:21:43,234: Running test after training batch: 13500
2026-01-03 19:21:43,354: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:21:48,071: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sikh the could at this point is will
2026-01-03 19:21:48,105: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 19:21:49,955: Val batch 13500: PER (avg): 0.1770 CTC Loss (avg): 17.4679 WER(1gram): 50.76% (n=64) time: 6.720
2026-01-03 19:21:49,955: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 19:21:49,955: t15.2023.08.13 val PER: 0.1341
2026-01-03 19:21:49,955: t15.2023.08.18 val PER: 0.1333
2026-01-03 19:21:49,955: t15.2023.08.20 val PER: 0.1430
2026-01-03 19:21:49,956: t15.2023.08.25 val PER: 0.1009
2026-01-03 19:21:49,956: t15.2023.08.27 val PER: 0.2219
2026-01-03 19:21:49,956: t15.2023.09.01 val PER: 0.0990
2026-01-03 19:21:49,956: t15.2023.09.03 val PER: 0.1817
2026-01-03 19:21:49,956: t15.2023.09.24 val PER: 0.1420
2026-01-03 19:21:49,956: t15.2023.09.29 val PER: 0.1468
2026-01-03 19:21:49,956: t15.2023.10.01 val PER: 0.1962
2026-01-03 19:21:49,956: t15.2023.10.06 val PER: 0.1012
2026-01-03 19:21:49,956: t15.2023.10.08 val PER: 0.2585
2026-01-03 19:21:49,956: t15.2023.10.13 val PER: 0.2335
2026-01-03 19:21:49,956: t15.2023.10.15 val PER: 0.1819
2026-01-03 19:21:49,956: t15.2023.10.20 val PER: 0.1812
2026-01-03 19:21:49,956: t15.2023.10.22 val PER: 0.1325
2026-01-03 19:21:49,957: t15.2023.11.03 val PER: 0.1981
2026-01-03 19:21:49,957: t15.2023.11.04 val PER: 0.0410
2026-01-03 19:21:49,957: t15.2023.11.17 val PER: 0.0575
2026-01-03 19:21:49,957: t15.2023.11.19 val PER: 0.0459
2026-01-03 19:21:49,957: t15.2023.11.26 val PER: 0.1609
2026-01-03 19:21:49,957: t15.2023.12.03 val PER: 0.1429
2026-01-03 19:21:49,957: t15.2023.12.08 val PER: 0.1411
2026-01-03 19:21:49,957: t15.2023.12.10 val PER: 0.1248
2026-01-03 19:21:49,957: t15.2023.12.17 val PER: 0.1559
2026-01-03 19:21:49,957: t15.2023.12.29 val PER: 0.1695
2026-01-03 19:21:49,957: t15.2024.02.25 val PER: 0.1447
2026-01-03 19:21:49,957: t15.2024.03.08 val PER: 0.2717
2026-01-03 19:21:49,957: t15.2024.03.15 val PER: 0.2320
2026-01-03 19:21:49,957: t15.2024.03.17 val PER: 0.1688
2026-01-03 19:21:49,957: t15.2024.05.10 val PER: 0.1857
2026-01-03 19:21:49,957: t15.2024.06.14 val PER: 0.1972
2026-01-03 19:21:49,957: t15.2024.07.19 val PER: 0.2690
2026-01-03 19:21:49,957: t15.2024.07.21 val PER: 0.1179
2026-01-03 19:21:49,958: t15.2024.07.28 val PER: 0.1566
2026-01-03 19:21:49,958: t15.2025.01.10 val PER: 0.3320
2026-01-03 19:21:49,958: t15.2025.01.12 val PER: 0.1809
2026-01-03 19:21:49,958: t15.2025.03.14 val PER: 0.3447
2026-01-03 19:21:49,958: t15.2025.03.16 val PER: 0.2094
2026-01-03 19:21:49,958: t15.2025.03.30 val PER: 0.3299
2026-01-03 19:21:49,958: t15.2025.04.13 val PER: 0.2425
2026-01-03 19:21:49,959: New best val WER(1gram) 51.02% --> 50.76%
2026-01-03 19:21:50,229: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_13500
2026-01-03 19:21:59,214: Train batch 13600: loss: 16.33 grad norm: 68.67 time: 0.062
2026-01-03 19:22:16,809: Train batch 13800: loss: 12.84 grad norm: 64.72 time: 0.055
2026-01-03 19:22:33,942: Train batch 14000: loss: 16.23 grad norm: 67.20 time: 0.050
2026-01-03 19:22:33,942: Running test after training batch: 14000
2026-01-03 19:22:34,049: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:22:39,193: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sikh the code at this point is will
2026-01-03 19:22:39,227: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 19:22:41,114: Val batch 14000: PER (avg): 0.1760 CTC Loss (avg): 17.3475 WER(1gram): 51.78% (n=64) time: 7.171
2026-01-03 19:22:41,114: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 19:22:41,114: t15.2023.08.13 val PER: 0.1414
2026-01-03 19:22:41,115: t15.2023.08.18 val PER: 0.1316
2026-01-03 19:22:41,115: t15.2023.08.20 val PER: 0.1430
2026-01-03 19:22:41,115: t15.2023.08.25 val PER: 0.1145
2026-01-03 19:22:41,115: t15.2023.08.27 val PER: 0.2170
2026-01-03 19:22:41,115: t15.2023.09.01 val PER: 0.0966
2026-01-03 19:22:41,115: t15.2023.09.03 val PER: 0.1853
2026-01-03 19:22:41,115: t15.2023.09.24 val PER: 0.1420
2026-01-03 19:22:41,115: t15.2023.09.29 val PER: 0.1500
2026-01-03 19:22:41,115: t15.2023.10.01 val PER: 0.1882
2026-01-03 19:22:41,115: t15.2023.10.06 val PER: 0.0990
2026-01-03 19:22:41,115: t15.2023.10.08 val PER: 0.2666
2026-01-03 19:22:41,115: t15.2023.10.13 val PER: 0.2327
2026-01-03 19:22:41,115: t15.2023.10.15 val PER: 0.1839
2026-01-03 19:22:41,115: t15.2023.10.20 val PER: 0.1846
2026-01-03 19:22:41,115: t15.2023.10.22 val PER: 0.1281
2026-01-03 19:22:41,115: t15.2023.11.03 val PER: 0.1947
2026-01-03 19:22:41,116: t15.2023.11.04 val PER: 0.0307
2026-01-03 19:22:41,116: t15.2023.11.17 val PER: 0.0622
2026-01-03 19:22:41,116: t15.2023.11.19 val PER: 0.0539
2026-01-03 19:22:41,116: t15.2023.11.26 val PER: 0.1703
2026-01-03 19:22:41,116: t15.2023.12.03 val PER: 0.1450
2026-01-03 19:22:41,116: t15.2023.12.08 val PER: 0.1358
2026-01-03 19:22:41,116: t15.2023.12.10 val PER: 0.1209
2026-01-03 19:22:41,116: t15.2023.12.17 val PER: 0.1549
2026-01-03 19:22:41,116: t15.2023.12.29 val PER: 0.1613
2026-01-03 19:22:41,116: t15.2024.02.25 val PER: 0.1390
2026-01-03 19:22:41,116: t15.2024.03.08 val PER: 0.2646
2026-01-03 19:22:41,116: t15.2024.03.15 val PER: 0.2339
2026-01-03 19:22:41,116: t15.2024.03.17 val PER: 0.1660
2026-01-03 19:22:41,116: t15.2024.05.10 val PER: 0.1724
2026-01-03 19:22:41,117: t15.2024.06.14 val PER: 0.1814
2026-01-03 19:22:41,117: t15.2024.07.19 val PER: 0.2722
2026-01-03 19:22:41,117: t15.2024.07.21 val PER: 0.1193
2026-01-03 19:22:41,117: t15.2024.07.28 val PER: 0.1566
2026-01-03 19:22:41,117: t15.2025.01.10 val PER: 0.3278
2026-01-03 19:22:41,117: t15.2025.01.12 val PER: 0.1724
2026-01-03 19:22:41,117: t15.2025.03.14 val PER: 0.3432
2026-01-03 19:22:41,117: t15.2025.03.16 val PER: 0.2199
2026-01-03 19:22:41,117: t15.2025.03.30 val PER: 0.3207
2026-01-03 19:22:41,117: t15.2025.04.13 val PER: 0.2368
2026-01-03 19:22:41,383: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_14000
2026-01-03 19:22:59,732: Train batch 14200: loss: 11.62 grad norm: 57.52 time: 0.056
2026-01-03 19:23:17,382: Train batch 14400: loss: 8.48 grad norm: 46.73 time: 0.064
2026-01-03 19:23:26,311: Running test after training batch: 14500
2026-01-03 19:23:26,424: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:23:31,117: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 19:23:31,151: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-03 19:23:33,061: Val batch 14500: PER (avg): 0.1751 CTC Loss (avg): 17.4273 WER(1gram): 51.02% (n=64) time: 6.749
2026-01-03 19:23:33,062: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 19:23:33,062: t15.2023.08.13 val PER: 0.1351
2026-01-03 19:23:33,062: t15.2023.08.18 val PER: 0.1266
2026-01-03 19:23:33,062: t15.2023.08.20 val PER: 0.1382
2026-01-03 19:23:33,062: t15.2023.08.25 val PER: 0.0994
2026-01-03 19:23:33,062: t15.2023.08.27 val PER: 0.2106
2026-01-03 19:23:33,062: t15.2023.09.01 val PER: 0.0925
2026-01-03 19:23:33,062: t15.2023.09.03 val PER: 0.1853
2026-01-03 19:23:33,062: t15.2023.09.24 val PER: 0.1420
2026-01-03 19:23:33,062: t15.2023.09.29 val PER: 0.1474
2026-01-03 19:23:33,063: t15.2023.10.01 val PER: 0.1869
2026-01-03 19:23:33,063: t15.2023.10.06 val PER: 0.0936
2026-01-03 19:23:33,063: t15.2023.10.08 val PER: 0.2571
2026-01-03 19:23:33,063: t15.2023.10.13 val PER: 0.2312
2026-01-03 19:23:33,063: t15.2023.10.15 val PER: 0.1800
2026-01-03 19:23:33,063: t15.2023.10.20 val PER: 0.1745
2026-01-03 19:23:33,063: t15.2023.10.22 val PER: 0.1247
2026-01-03 19:23:33,063: t15.2023.11.03 val PER: 0.1954
2026-01-03 19:23:33,063: t15.2023.11.04 val PER: 0.0410
2026-01-03 19:23:33,064: t15.2023.11.17 val PER: 0.0575
2026-01-03 19:23:33,064: t15.2023.11.19 val PER: 0.0479
2026-01-03 19:23:33,064: t15.2023.11.26 val PER: 0.1725
2026-01-03 19:23:33,064: t15.2023.12.03 val PER: 0.1397
2026-01-03 19:23:33,064: t15.2023.12.08 val PER: 0.1372
2026-01-03 19:23:33,064: t15.2023.12.10 val PER: 0.1196
2026-01-03 19:23:33,064: t15.2023.12.17 val PER: 0.1601
2026-01-03 19:23:33,064: t15.2023.12.29 val PER: 0.1661
2026-01-03 19:23:33,064: t15.2024.02.25 val PER: 0.1390
2026-01-03 19:23:33,064: t15.2024.03.08 val PER: 0.2617
2026-01-03 19:23:33,064: t15.2024.03.15 val PER: 0.2270
2026-01-03 19:23:33,064: t15.2024.03.17 val PER: 0.1632
2026-01-03 19:23:33,064: t15.2024.05.10 val PER: 0.1887
2026-01-03 19:23:33,064: t15.2024.06.14 val PER: 0.1956
2026-01-03 19:23:33,064: t15.2024.07.19 val PER: 0.2696
2026-01-03 19:23:33,065: t15.2024.07.21 val PER: 0.1152
2026-01-03 19:23:33,065: t15.2024.07.28 val PER: 0.1537
2026-01-03 19:23:33,065: t15.2025.01.10 val PER: 0.3237
2026-01-03 19:23:33,065: t15.2025.01.12 val PER: 0.1801
2026-01-03 19:23:33,065: t15.2025.03.14 val PER: 0.3743
2026-01-03 19:23:33,065: t15.2025.03.16 val PER: 0.2199
2026-01-03 19:23:33,065: t15.2025.03.30 val PER: 0.3287
2026-01-03 19:23:33,065: t15.2025.04.13 val PER: 0.2354
2026-01-03 19:23:33,330: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_14500
2026-01-03 19:23:42,143: Train batch 14600: loss: 17.36 grad norm: 73.61 time: 0.057
2026-01-03 19:24:00,204: Train batch 14800: loss: 8.19 grad norm: 50.11 time: 0.050
2026-01-03 19:24:18,276: Train batch 15000: loss: 12.41 grad norm: 57.11 time: 0.051
2026-01-03 19:24:18,276: Running test after training batch: 15000
2026-01-03 19:24:18,370: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:24:23,217: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 19:24:23,252: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 19:24:25,096: Val batch 15000: PER (avg): 0.1724 CTC Loss (avg): 17.1986 WER(1gram): 49.75% (n=64) time: 6.819
2026-01-03 19:24:25,096: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 19:24:25,096: t15.2023.08.13 val PER: 0.1299
2026-01-03 19:24:25,096: t15.2023.08.18 val PER: 0.1249
2026-01-03 19:24:25,096: t15.2023.08.20 val PER: 0.1326
2026-01-03 19:24:25,096: t15.2023.08.25 val PER: 0.1039
2026-01-03 19:24:25,097: t15.2023.08.27 val PER: 0.2026
2026-01-03 19:24:25,097: t15.2023.09.01 val PER: 0.0909
2026-01-03 19:24:25,097: t15.2023.09.03 val PER: 0.1758
2026-01-03 19:24:25,097: t15.2023.09.24 val PER: 0.1468
2026-01-03 19:24:25,097: t15.2023.09.29 val PER: 0.1429
2026-01-03 19:24:25,097: t15.2023.10.01 val PER: 0.1856
2026-01-03 19:24:25,097: t15.2023.10.06 val PER: 0.0926
2026-01-03 19:24:25,097: t15.2023.10.08 val PER: 0.2544
2026-01-03 19:24:25,097: t15.2023.10.13 val PER: 0.2335
2026-01-03 19:24:25,097: t15.2023.10.15 val PER: 0.1767
2026-01-03 19:24:25,097: t15.2023.10.20 val PER: 0.1879
2026-01-03 19:24:25,097: t15.2023.10.22 val PER: 0.1236
2026-01-03 19:24:25,097: t15.2023.11.03 val PER: 0.1859
2026-01-03 19:24:25,098: t15.2023.11.04 val PER: 0.0375
2026-01-03 19:24:25,098: t15.2023.11.17 val PER: 0.0498
2026-01-03 19:24:25,098: t15.2023.11.19 val PER: 0.0419
2026-01-03 19:24:25,098: t15.2023.11.26 val PER: 0.1645
2026-01-03 19:24:25,098: t15.2023.12.03 val PER: 0.1429
2026-01-03 19:24:25,098: t15.2023.12.08 val PER: 0.1378
2026-01-03 19:24:25,098: t15.2023.12.10 val PER: 0.1143
2026-01-03 19:24:25,098: t15.2023.12.17 val PER: 0.1570
2026-01-03 19:24:25,098: t15.2023.12.29 val PER: 0.1627
2026-01-03 19:24:25,098: t15.2024.02.25 val PER: 0.1404
2026-01-03 19:24:25,098: t15.2024.03.08 val PER: 0.2504
2026-01-03 19:24:25,098: t15.2024.03.15 val PER: 0.2326
2026-01-03 19:24:25,098: t15.2024.03.17 val PER: 0.1653
2026-01-03 19:24:25,098: t15.2024.05.10 val PER: 0.1917
2026-01-03 19:24:25,098: t15.2024.06.14 val PER: 0.2035
2026-01-03 19:24:25,098: t15.2024.07.19 val PER: 0.2604
2026-01-03 19:24:25,099: t15.2024.07.21 val PER: 0.1124
2026-01-03 19:24:25,099: t15.2024.07.28 val PER: 0.1507
2026-01-03 19:24:25,099: t15.2025.01.10 val PER: 0.3196
2026-01-03 19:24:25,099: t15.2025.01.12 val PER: 0.1778
2026-01-03 19:24:25,099: t15.2025.03.14 val PER: 0.3550
2026-01-03 19:24:25,099: t15.2025.03.16 val PER: 0.2120
2026-01-03 19:24:25,099: t15.2025.03.30 val PER: 0.3287
2026-01-03 19:24:25,099: t15.2025.04.13 val PER: 0.2325
2026-01-03 19:24:25,100: New best val WER(1gram) 50.76% --> 49.75%
2026-01-03 19:24:25,371: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_15000
2026-01-03 19:24:42,699: Train batch 15200: loss: 7.15 grad norm: 43.40 time: 0.057
2026-01-03 19:24:59,648: Train batch 15400: loss: 15.12 grad norm: 65.92 time: 0.049
2026-01-03 19:25:08,998: Running test after training batch: 15500
2026-01-03 19:25:09,098: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:25:13,860: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 19:25:13,894: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 19:25:15,751: Val batch 15500: PER (avg): 0.1707 CTC Loss (avg): 17.0342 WER(1gram): 49.24% (n=64) time: 6.753
2026-01-03 19:25:15,752: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 19:25:15,752: t15.2023.08.13 val PER: 0.1247
2026-01-03 19:25:15,752: t15.2023.08.18 val PER: 0.1274
2026-01-03 19:25:15,752: t15.2023.08.20 val PER: 0.1342
2026-01-03 19:25:15,752: t15.2023.08.25 val PER: 0.1069
2026-01-03 19:25:15,752: t15.2023.08.27 val PER: 0.2010
2026-01-03 19:25:15,752: t15.2023.09.01 val PER: 0.0885
2026-01-03 19:25:15,752: t15.2023.09.03 val PER: 0.1829
2026-01-03 19:25:15,752: t15.2023.09.24 val PER: 0.1396
2026-01-03 19:25:15,752: t15.2023.09.29 val PER: 0.1417
2026-01-03 19:25:15,752: t15.2023.10.01 val PER: 0.1849
2026-01-03 19:25:15,753: t15.2023.10.06 val PER: 0.0926
2026-01-03 19:25:15,753: t15.2023.10.08 val PER: 0.2476
2026-01-03 19:25:15,753: t15.2023.10.13 val PER: 0.2312
2026-01-03 19:25:15,753: t15.2023.10.15 val PER: 0.1734
2026-01-03 19:25:15,753: t15.2023.10.20 val PER: 0.1577
2026-01-03 19:25:15,753: t15.2023.10.22 val PER: 0.1247
2026-01-03 19:25:15,753: t15.2023.11.03 val PER: 0.1900
2026-01-03 19:25:15,753: t15.2023.11.04 val PER: 0.0410
2026-01-03 19:25:15,753: t15.2023.11.17 val PER: 0.0513
2026-01-03 19:25:15,753: t15.2023.11.19 val PER: 0.0479
2026-01-03 19:25:15,753: t15.2023.11.26 val PER: 0.1543
2026-01-03 19:25:15,753: t15.2023.12.03 val PER: 0.1387
2026-01-03 19:25:15,753: t15.2023.12.08 val PER: 0.1338
2026-01-03 19:25:15,753: t15.2023.12.10 val PER: 0.1130
2026-01-03 19:25:15,753: t15.2023.12.17 val PER: 0.1580
2026-01-03 19:25:15,753: t15.2023.12.29 val PER: 0.1537
2026-01-03 19:25:15,753: t15.2024.02.25 val PER: 0.1404
2026-01-03 19:25:15,754: t15.2024.03.08 val PER: 0.2688
2026-01-03 19:25:15,754: t15.2024.03.15 val PER: 0.2289
2026-01-03 19:25:15,754: t15.2024.03.17 val PER: 0.1618
2026-01-03 19:25:15,754: t15.2024.05.10 val PER: 0.1887
2026-01-03 19:25:15,754: t15.2024.06.14 val PER: 0.1972
2026-01-03 19:25:15,754: t15.2024.07.19 val PER: 0.2610
2026-01-03 19:25:15,754: t15.2024.07.21 val PER: 0.1145
2026-01-03 19:25:15,754: t15.2024.07.28 val PER: 0.1485
2026-01-03 19:25:15,754: t15.2025.01.10 val PER: 0.3140
2026-01-03 19:25:15,754: t15.2025.01.12 val PER: 0.1701
2026-01-03 19:25:15,754: t15.2025.03.14 val PER: 0.3521
2026-01-03 19:25:15,754: t15.2025.03.16 val PER: 0.2173
2026-01-03 19:25:15,754: t15.2025.03.30 val PER: 0.3264
2026-01-03 19:25:15,754: t15.2025.04.13 val PER: 0.2340
2026-01-03 19:25:15,755: New best val WER(1gram) 49.75% --> 49.24%
2026-01-03 19:25:16,025: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_15500
2026-01-03 19:25:24,807: Train batch 15600: loss: 16.77 grad norm: 63.71 time: 0.062
2026-01-03 19:25:43,025: Train batch 15800: loss: 18.43 grad norm: 69.55 time: 0.067
2026-01-03 19:26:00,801: Train batch 16000: loss: 11.69 grad norm: 50.77 time: 0.055
2026-01-03 19:26:00,801: Running test after training batch: 16000
2026-01-03 19:26:00,899: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:26:05,744: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-03 19:26:05,777: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-03 19:26:07,663: Val batch 16000: PER (avg): 0.1713 CTC Loss (avg): 17.0846 WER(1gram): 49.75% (n=64) time: 6.861
2026-01-03 19:26:07,663: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 19:26:07,663: t15.2023.08.13 val PER: 0.1247
2026-01-03 19:26:07,663: t15.2023.08.18 val PER: 0.1274
2026-01-03 19:26:07,664: t15.2023.08.20 val PER: 0.1319
2026-01-03 19:26:07,664: t15.2023.08.25 val PER: 0.0964
2026-01-03 19:26:07,664: t15.2023.08.27 val PER: 0.2042
2026-01-03 19:26:07,664: t15.2023.09.01 val PER: 0.0869
2026-01-03 19:26:07,664: t15.2023.09.03 val PER: 0.1793
2026-01-03 19:26:07,664: t15.2023.09.24 val PER: 0.1432
2026-01-03 19:26:07,664: t15.2023.09.29 val PER: 0.1481
2026-01-03 19:26:07,664: t15.2023.10.01 val PER: 0.1816
2026-01-03 19:26:07,664: t15.2023.10.06 val PER: 0.0947
2026-01-03 19:26:07,664: t15.2023.10.08 val PER: 0.2544
2026-01-03 19:26:07,664: t15.2023.10.13 val PER: 0.2320
2026-01-03 19:26:07,664: t15.2023.10.15 val PER: 0.1800
2026-01-03 19:26:07,664: t15.2023.10.20 val PER: 0.1745
2026-01-03 19:26:07,665: t15.2023.10.22 val PER: 0.1169
2026-01-03 19:26:07,665: t15.2023.11.03 val PER: 0.1886
2026-01-03 19:26:07,665: t15.2023.11.04 val PER: 0.0307
2026-01-03 19:26:07,665: t15.2023.11.17 val PER: 0.0575
2026-01-03 19:26:07,665: t15.2023.11.19 val PER: 0.0459
2026-01-03 19:26:07,665: t15.2023.11.26 val PER: 0.1558
2026-01-03 19:26:07,665: t15.2023.12.03 val PER: 0.1387
2026-01-03 19:26:07,665: t15.2023.12.08 val PER: 0.1318
2026-01-03 19:26:07,665: t15.2023.12.10 val PER: 0.1143
2026-01-03 19:26:07,665: t15.2023.12.17 val PER: 0.1549
2026-01-03 19:26:07,665: t15.2023.12.29 val PER: 0.1565
2026-01-03 19:26:07,665: t15.2024.02.25 val PER: 0.1348
2026-01-03 19:26:07,665: t15.2024.03.08 val PER: 0.2632
2026-01-03 19:26:07,665: t15.2024.03.15 val PER: 0.2283
2026-01-03 19:26:07,665: t15.2024.03.17 val PER: 0.1625
2026-01-03 19:26:07,666: t15.2024.05.10 val PER: 0.1857
2026-01-03 19:26:07,666: t15.2024.06.14 val PER: 0.1782
2026-01-03 19:26:07,666: t15.2024.07.19 val PER: 0.2624
2026-01-03 19:26:07,666: t15.2024.07.21 val PER: 0.1193
2026-01-03 19:26:07,666: t15.2024.07.28 val PER: 0.1515
2026-01-03 19:26:07,666: t15.2025.01.10 val PER: 0.3237
2026-01-03 19:26:07,666: t15.2025.01.12 val PER: 0.1771
2026-01-03 19:26:07,666: t15.2025.03.14 val PER: 0.3491
2026-01-03 19:26:07,666: t15.2025.03.16 val PER: 0.2081
2026-01-03 19:26:07,666: t15.2025.03.30 val PER: 0.3391
2026-01-03 19:26:07,666: t15.2025.04.13 val PER: 0.2397
2026-01-03 19:26:07,930: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_16000
2026-01-03 19:26:25,892: Train batch 16200: loss: 9.34 grad norm: 50.77 time: 0.055
2026-01-03 19:26:43,496: Train batch 16400: loss: 13.93 grad norm: 65.52 time: 0.057
2026-01-03 19:26:52,293: Running test after training batch: 16500
2026-01-03 19:26:52,421: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:26:57,839: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 19:26:57,874: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost it
2026-01-03 19:26:59,773: Val batch 16500: PER (avg): 0.1702 CTC Loss (avg): 16.9473 WER(1gram): 49.75% (n=64) time: 7.480
2026-01-03 19:26:59,773: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 19:26:59,773: t15.2023.08.13 val PER: 0.1237
2026-01-03 19:26:59,773: t15.2023.08.18 val PER: 0.1232
2026-01-03 19:26:59,774: t15.2023.08.20 val PER: 0.1287
2026-01-03 19:26:59,774: t15.2023.08.25 val PER: 0.0934
2026-01-03 19:26:59,774: t15.2023.08.27 val PER: 0.2026
2026-01-03 19:26:59,774: t15.2023.09.01 val PER: 0.0869
2026-01-03 19:26:59,774: t15.2023.09.03 val PER: 0.1686
2026-01-03 19:26:59,774: t15.2023.09.24 val PER: 0.1444
2026-01-03 19:26:59,774: t15.2023.09.29 val PER: 0.1417
2026-01-03 19:26:59,774: t15.2023.10.01 val PER: 0.1830
2026-01-03 19:26:59,774: t15.2023.10.06 val PER: 0.0990
2026-01-03 19:26:59,774: t15.2023.10.08 val PER: 0.2625
2026-01-03 19:26:59,774: t15.2023.10.13 val PER: 0.2250
2026-01-03 19:26:59,774: t15.2023.10.15 val PER: 0.1780
2026-01-03 19:26:59,775: t15.2023.10.20 val PER: 0.1812
2026-01-03 19:26:59,775: t15.2023.10.22 val PER: 0.1236
2026-01-03 19:26:59,775: t15.2023.11.03 val PER: 0.1920
2026-01-03 19:26:59,775: t15.2023.11.04 val PER: 0.0375
2026-01-03 19:26:59,775: t15.2023.11.17 val PER: 0.0529
2026-01-03 19:26:59,775: t15.2023.11.19 val PER: 0.0399
2026-01-03 19:26:59,775: t15.2023.11.26 val PER: 0.1601
2026-01-03 19:26:59,775: t15.2023.12.03 val PER: 0.1334
2026-01-03 19:26:59,775: t15.2023.12.08 val PER: 0.1352
2026-01-03 19:26:59,775: t15.2023.12.10 val PER: 0.1183
2026-01-03 19:26:59,775: t15.2023.12.17 val PER: 0.1518
2026-01-03 19:26:59,775: t15.2023.12.29 val PER: 0.1599
2026-01-03 19:26:59,775: t15.2024.02.25 val PER: 0.1292
2026-01-03 19:26:59,775: t15.2024.03.08 val PER: 0.2632
2026-01-03 19:26:59,775: t15.2024.03.15 val PER: 0.2283
2026-01-03 19:26:59,775: t15.2024.03.17 val PER: 0.1632
2026-01-03 19:26:59,776: t15.2024.05.10 val PER: 0.1783
2026-01-03 19:26:59,776: t15.2024.06.14 val PER: 0.1798
2026-01-03 19:26:59,776: t15.2024.07.19 val PER: 0.2650
2026-01-03 19:26:59,776: t15.2024.07.21 val PER: 0.1117
2026-01-03 19:26:59,776: t15.2024.07.28 val PER: 0.1485
2026-01-03 19:26:59,776: t15.2025.01.10 val PER: 0.3099
2026-01-03 19:26:59,776: t15.2025.01.12 val PER: 0.1732
2026-01-03 19:26:59,776: t15.2025.03.14 val PER: 0.3580
2026-01-03 19:26:59,776: t15.2025.03.16 val PER: 0.2094
2026-01-03 19:26:59,776: t15.2025.03.30 val PER: 0.3333
2026-01-03 19:26:59,776: t15.2025.04.13 val PER: 0.2325
2026-01-03 19:27:00,025: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_16500
2026-01-03 19:27:15,336: Train batch 16600: loss: 11.95 grad norm: 58.27 time: 0.052
2026-01-03 19:27:33,504: Train batch 16800: loss: 22.27 grad norm: 83.13 time: 0.061
2026-01-03 19:27:51,438: Train batch 17000: loss: 10.81 grad norm: 54.05 time: 0.080
2026-01-03 19:27:51,439: Running test after training batch: 17000
2026-01-03 19:27:51,701: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:27:57,053: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-03 19:27:57,087: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-03 19:27:59,000: Val batch 17000: PER (avg): 0.1688 CTC Loss (avg): 16.8337 WER(1gram): 49.24% (n=64) time: 7.561
2026-01-03 19:27:59,000: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 19:27:59,001: t15.2023.08.13 val PER: 0.1268
2026-01-03 19:27:59,001: t15.2023.08.18 val PER: 0.1274
2026-01-03 19:27:59,001: t15.2023.08.20 val PER: 0.1303
2026-01-03 19:27:59,001: t15.2023.08.25 val PER: 0.0964
2026-01-03 19:27:59,001: t15.2023.08.27 val PER: 0.2106
2026-01-03 19:27:59,001: t15.2023.09.01 val PER: 0.0909
2026-01-03 19:27:59,001: t15.2023.09.03 val PER: 0.1698
2026-01-03 19:27:59,001: t15.2023.09.24 val PER: 0.1420
2026-01-03 19:27:59,002: t15.2023.09.29 val PER: 0.1410
2026-01-03 19:27:59,002: t15.2023.10.01 val PER: 0.1856
2026-01-03 19:27:59,002: t15.2023.10.06 val PER: 0.0947
2026-01-03 19:27:59,002: t15.2023.10.08 val PER: 0.2585
2026-01-03 19:27:59,002: t15.2023.10.13 val PER: 0.2242
2026-01-03 19:27:59,002: t15.2023.10.15 val PER: 0.1767
2026-01-03 19:27:59,002: t15.2023.10.20 val PER: 0.1711
2026-01-03 19:27:59,002: t15.2023.10.22 val PER: 0.1180
2026-01-03 19:27:59,002: t15.2023.11.03 val PER: 0.1879
2026-01-03 19:27:59,003: t15.2023.11.04 val PER: 0.0341
2026-01-03 19:27:59,003: t15.2023.11.17 val PER: 0.0575
2026-01-03 19:27:59,003: t15.2023.11.19 val PER: 0.0459
2026-01-03 19:27:59,003: t15.2023.11.26 val PER: 0.1572
2026-01-03 19:27:59,003: t15.2023.12.03 val PER: 0.1345
2026-01-03 19:27:59,003: t15.2023.12.08 val PER: 0.1298
2026-01-03 19:27:59,003: t15.2023.12.10 val PER: 0.1117
2026-01-03 19:27:59,003: t15.2023.12.17 val PER: 0.1528
2026-01-03 19:27:59,003: t15.2023.12.29 val PER: 0.1531
2026-01-03 19:27:59,003: t15.2024.02.25 val PER: 0.1264
2026-01-03 19:27:59,004: t15.2024.03.08 val PER: 0.2617
2026-01-03 19:27:59,004: t15.2024.03.15 val PER: 0.2301
2026-01-03 19:27:59,004: t15.2024.03.17 val PER: 0.1520
2026-01-03 19:27:59,004: t15.2024.05.10 val PER: 0.1857
2026-01-03 19:27:59,004: t15.2024.06.14 val PER: 0.1814
2026-01-03 19:27:59,004: t15.2024.07.19 val PER: 0.2591
2026-01-03 19:27:59,004: t15.2024.07.21 val PER: 0.1131
2026-01-03 19:27:59,004: t15.2024.07.28 val PER: 0.1456
2026-01-03 19:27:59,004: t15.2025.01.10 val PER: 0.3099
2026-01-03 19:27:59,004: t15.2025.01.12 val PER: 0.1724
2026-01-03 19:27:59,005: t15.2025.03.14 val PER: 0.3476
2026-01-03 19:27:59,005: t15.2025.03.16 val PER: 0.2081
2026-01-03 19:27:59,005: t15.2025.03.30 val PER: 0.3241
2026-01-03 19:27:59,005: t15.2025.04.13 val PER: 0.2340
2026-01-03 19:27:59,245: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_17000
2026-01-03 19:28:16,236: Train batch 17200: loss: 13.84 grad norm: 56.45 time: 0.084
2026-01-03 19:28:33,523: Train batch 17400: loss: 16.19 grad norm: 64.42 time: 0.070
2026-01-03 19:28:42,068: Running test after training batch: 17500
2026-01-03 19:28:42,164: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:28:46,886: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 19:28:46,922: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-03 19:28:48,828: Val batch 17500: PER (avg): 0.1688 CTC Loss (avg): 16.7975 WER(1gram): 49.75% (n=64) time: 6.760
2026-01-03 19:28:48,829: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 19:28:48,829: t15.2023.08.13 val PER: 0.1227
2026-01-03 19:28:48,829: t15.2023.08.18 val PER: 0.1266
2026-01-03 19:28:48,829: t15.2023.08.20 val PER: 0.1295
2026-01-03 19:28:48,829: t15.2023.08.25 val PER: 0.0934
2026-01-03 19:28:48,829: t15.2023.08.27 val PER: 0.1994
2026-01-03 19:28:48,829: t15.2023.09.01 val PER: 0.0893
2026-01-03 19:28:48,829: t15.2023.09.03 val PER: 0.1758
2026-01-03 19:28:48,829: t15.2023.09.24 val PER: 0.1396
2026-01-03 19:28:48,829: t15.2023.09.29 val PER: 0.1410
2026-01-03 19:28:48,829: t15.2023.10.01 val PER: 0.1856
2026-01-03 19:28:48,830: t15.2023.10.06 val PER: 0.0958
2026-01-03 19:28:48,830: t15.2023.10.08 val PER: 0.2571
2026-01-03 19:28:48,830: t15.2023.10.13 val PER: 0.2258
2026-01-03 19:28:48,830: t15.2023.10.15 val PER: 0.1727
2026-01-03 19:28:48,830: t15.2023.10.20 val PER: 0.1812
2026-01-03 19:28:48,830: t15.2023.10.22 val PER: 0.1225
2026-01-03 19:28:48,830: t15.2023.11.03 val PER: 0.1845
2026-01-03 19:28:48,830: t15.2023.11.04 val PER: 0.0375
2026-01-03 19:28:48,830: t15.2023.11.17 val PER: 0.0575
2026-01-03 19:28:48,830: t15.2023.11.19 val PER: 0.0359
2026-01-03 19:28:48,830: t15.2023.11.26 val PER: 0.1580
2026-01-03 19:28:48,831: t15.2023.12.03 val PER: 0.1366
2026-01-03 19:28:48,831: t15.2023.12.08 val PER: 0.1312
2026-01-03 19:28:48,831: t15.2023.12.10 val PER: 0.1104
2026-01-03 19:28:48,831: t15.2023.12.17 val PER: 0.1559
2026-01-03 19:28:48,831: t15.2023.12.29 val PER: 0.1572
2026-01-03 19:28:48,831: t15.2024.02.25 val PER: 0.1292
2026-01-03 19:28:48,831: t15.2024.03.08 val PER: 0.2532
2026-01-03 19:28:48,831: t15.2024.03.15 val PER: 0.2201
2026-01-03 19:28:48,831: t15.2024.03.17 val PER: 0.1576
2026-01-03 19:28:48,831: t15.2024.05.10 val PER: 0.1813
2026-01-03 19:28:48,831: t15.2024.06.14 val PER: 0.1845
2026-01-03 19:28:48,831: t15.2024.07.19 val PER: 0.2650
2026-01-03 19:28:48,831: t15.2024.07.21 val PER: 0.1166
2026-01-03 19:28:48,831: t15.2024.07.28 val PER: 0.1500
2026-01-03 19:28:48,831: t15.2025.01.10 val PER: 0.3154
2026-01-03 19:28:48,832: t15.2025.01.12 val PER: 0.1717
2026-01-03 19:28:48,832: t15.2025.03.14 val PER: 0.3402
2026-01-03 19:28:48,832: t15.2025.03.16 val PER: 0.2055
2026-01-03 19:28:48,832: t15.2025.03.30 val PER: 0.3333
2026-01-03 19:28:48,832: t15.2025.04.13 val PER: 0.2282
2026-01-03 19:28:49,114: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_17500
2026-01-03 19:28:58,072: Train batch 17600: loss: 14.34 grad norm: 62.47 time: 0.050
2026-01-03 19:29:16,977: Train batch 17800: loss: 9.08 grad norm: 55.95 time: 0.044
2026-01-03 19:29:34,431: Train batch 18000: loss: 15.82 grad norm: 69.35 time: 0.060
2026-01-03 19:29:34,432: Running test after training batch: 18000
2026-01-03 19:29:34,567: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:29:39,575: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 19:29:39,610: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-03 19:29:41,540: Val batch 18000: PER (avg): 0.1685 CTC Loss (avg): 16.7623 WER(1gram): 49.24% (n=64) time: 7.108
2026-01-03 19:29:41,540: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 19:29:41,541: t15.2023.08.13 val PER: 0.1247
2026-01-03 19:29:41,541: t15.2023.08.18 val PER: 0.1249
2026-01-03 19:29:41,541: t15.2023.08.20 val PER: 0.1239
2026-01-03 19:29:41,541: t15.2023.08.25 val PER: 0.0919
2026-01-03 19:29:41,541: t15.2023.08.27 val PER: 0.2058
2026-01-03 19:29:41,541: t15.2023.09.01 val PER: 0.0877
2026-01-03 19:29:41,541: t15.2023.09.03 val PER: 0.1793
2026-01-03 19:29:41,541: t15.2023.09.24 val PER: 0.1371
2026-01-03 19:29:41,541: t15.2023.09.29 val PER: 0.1442
2026-01-03 19:29:41,541: t15.2023.10.01 val PER: 0.1823
2026-01-03 19:29:41,541: t15.2023.10.06 val PER: 0.0947
2026-01-03 19:29:41,541: t15.2023.10.08 val PER: 0.2598
2026-01-03 19:29:41,541: t15.2023.10.13 val PER: 0.2265
2026-01-03 19:29:41,541: t15.2023.10.15 val PER: 0.1760
2026-01-03 19:29:41,542: t15.2023.10.20 val PER: 0.1913
2026-01-03 19:29:41,542: t15.2023.10.22 val PER: 0.1247
2026-01-03 19:29:41,542: t15.2023.11.03 val PER: 0.1872
2026-01-03 19:29:41,542: t15.2023.11.04 val PER: 0.0444
2026-01-03 19:29:41,542: t15.2023.11.17 val PER: 0.0482
2026-01-03 19:29:41,542: t15.2023.11.19 val PER: 0.0339
2026-01-03 19:29:41,542: t15.2023.11.26 val PER: 0.1580
2026-01-03 19:29:41,542: t15.2023.12.03 val PER: 0.1345
2026-01-03 19:29:41,542: t15.2023.12.08 val PER: 0.1298
2026-01-03 19:29:41,542: t15.2023.12.10 val PER: 0.1078
2026-01-03 19:29:41,542: t15.2023.12.17 val PER: 0.1590
2026-01-03 19:29:41,542: t15.2023.12.29 val PER: 0.1565
2026-01-03 19:29:41,542: t15.2024.02.25 val PER: 0.1278
2026-01-03 19:29:41,543: t15.2024.03.08 val PER: 0.2632
2026-01-03 19:29:41,543: t15.2024.03.15 val PER: 0.2220
2026-01-03 19:29:41,543: t15.2024.03.17 val PER: 0.1541
2026-01-03 19:29:41,543: t15.2024.05.10 val PER: 0.1872
2026-01-03 19:29:41,543: t15.2024.06.14 val PER: 0.1814
2026-01-03 19:29:41,543: t15.2024.07.19 val PER: 0.2597
2026-01-03 19:29:41,543: t15.2024.07.21 val PER: 0.1145
2026-01-03 19:29:41,543: t15.2024.07.28 val PER: 0.1500
2026-01-03 19:29:41,543: t15.2025.01.10 val PER: 0.3223
2026-01-03 19:29:41,543: t15.2025.01.12 val PER: 0.1686
2026-01-03 19:29:41,543: t15.2025.03.14 val PER: 0.3358
2026-01-03 19:29:41,543: t15.2025.03.16 val PER: 0.2094
2026-01-03 19:29:41,544: t15.2025.03.30 val PER: 0.3230
2026-01-03 19:29:41,544: t15.2025.04.13 val PER: 0.2268
2026-01-03 19:29:41,861: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_18000
2026-01-03 19:30:00,191: Train batch 18200: loss: 11.59 grad norm: 58.05 time: 0.073
2026-01-03 19:30:18,012: Train batch 18400: loss: 8.14 grad norm: 54.50 time: 0.057
2026-01-03 19:30:27,012: Running test after training batch: 18500
2026-01-03 19:30:27,107: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:30:31,836: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 19:30:31,871: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-03 19:30:33,845: Val batch 18500: PER (avg): 0.1676 CTC Loss (avg): 16.7655 WER(1gram): 48.98% (n=64) time: 6.833
2026-01-03 19:30:33,845: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 19:30:33,846: t15.2023.08.13 val PER: 0.1268
2026-01-03 19:30:33,846: t15.2023.08.18 val PER: 0.1249
2026-01-03 19:30:33,846: t15.2023.08.20 val PER: 0.1231
2026-01-03 19:30:33,846: t15.2023.08.25 val PER: 0.0919
2026-01-03 19:30:33,846: t15.2023.08.27 val PER: 0.2058
2026-01-03 19:30:33,846: t15.2023.09.01 val PER: 0.0885
2026-01-03 19:30:33,846: t15.2023.09.03 val PER: 0.1734
2026-01-03 19:30:33,846: t15.2023.09.24 val PER: 0.1371
2026-01-03 19:30:33,847: t15.2023.09.29 val PER: 0.1455
2026-01-03 19:30:33,847: t15.2023.10.01 val PER: 0.1810
2026-01-03 19:30:33,847: t15.2023.10.06 val PER: 0.0958
2026-01-03 19:30:33,847: t15.2023.10.08 val PER: 0.2544
2026-01-03 19:30:33,847: t15.2023.10.13 val PER: 0.2234
2026-01-03 19:30:33,847: t15.2023.10.15 val PER: 0.1727
2026-01-03 19:30:33,847: t15.2023.10.20 val PER: 0.1812
2026-01-03 19:30:33,847: t15.2023.10.22 val PER: 0.1214
2026-01-03 19:30:33,847: t15.2023.11.03 val PER: 0.1947
2026-01-03 19:30:33,847: t15.2023.11.04 val PER: 0.0341
2026-01-03 19:30:33,847: t15.2023.11.17 val PER: 0.0544
2026-01-03 19:30:33,848: t15.2023.11.19 val PER: 0.0359
2026-01-03 19:30:33,848: t15.2023.11.26 val PER: 0.1543
2026-01-03 19:30:33,848: t15.2023.12.03 val PER: 0.1292
2026-01-03 19:30:33,848: t15.2023.12.08 val PER: 0.1278
2026-01-03 19:30:33,848: t15.2023.12.10 val PER: 0.1078
2026-01-03 19:30:33,848: t15.2023.12.17 val PER: 0.1528
2026-01-03 19:30:33,848: t15.2023.12.29 val PER: 0.1572
2026-01-03 19:30:33,848: t15.2024.02.25 val PER: 0.1292
2026-01-03 19:30:33,848: t15.2024.03.08 val PER: 0.2589
2026-01-03 19:30:33,848: t15.2024.03.15 val PER: 0.2233
2026-01-03 19:30:33,848: t15.2024.03.17 val PER: 0.1534
2026-01-03 19:30:33,849: t15.2024.05.10 val PER: 0.1828
2026-01-03 19:30:33,849: t15.2024.06.14 val PER: 0.1782
2026-01-03 19:30:33,849: t15.2024.07.19 val PER: 0.2617
2026-01-03 19:30:33,849: t15.2024.07.21 val PER: 0.1152
2026-01-03 19:30:33,849: t15.2024.07.28 val PER: 0.1485
2026-01-03 19:30:33,849: t15.2025.01.10 val PER: 0.3140
2026-01-03 19:30:33,849: t15.2025.01.12 val PER: 0.1655
2026-01-03 19:30:33,849: t15.2025.03.14 val PER: 0.3388
2026-01-03 19:30:33,849: t15.2025.03.16 val PER: 0.2107
2026-01-03 19:30:33,849: t15.2025.03.30 val PER: 0.3218
2026-01-03 19:30:33,849: t15.2025.04.13 val PER: 0.2297
2026-01-03 19:30:33,850: New best val WER(1gram) 49.24% --> 48.98%
2026-01-03 19:30:34,130: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_18500
2026-01-03 19:30:42,930: Train batch 18600: loss: 16.88 grad norm: 68.93 time: 0.066
2026-01-03 19:31:00,587: Train batch 18800: loss: 12.53 grad norm: 60.09 time: 0.063
2026-01-03 19:31:18,183: Train batch 19000: loss: 11.17 grad norm: 47.84 time: 0.063
2026-01-03 19:31:18,184: Running test after training batch: 19000
2026-01-03 19:31:18,319: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:31:23,839: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 19:31:23,875: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-03 19:31:25,864: Val batch 19000: PER (avg): 0.1679 CTC Loss (avg): 16.7291 WER(1gram): 48.48% (n=64) time: 7.680
2026-01-03 19:31:25,864: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 19:31:25,865: t15.2023.08.13 val PER: 0.1227
2026-01-03 19:31:25,865: t15.2023.08.18 val PER: 0.1241
2026-01-03 19:31:25,865: t15.2023.08.20 val PER: 0.1231
2026-01-03 19:31:25,865: t15.2023.08.25 val PER: 0.0964
2026-01-03 19:31:25,865: t15.2023.08.27 val PER: 0.2026
2026-01-03 19:31:25,865: t15.2023.09.01 val PER: 0.0852
2026-01-03 19:31:25,865: t15.2023.09.03 val PER: 0.1758
2026-01-03 19:31:25,865: t15.2023.09.24 val PER: 0.1383
2026-01-03 19:31:25,865: t15.2023.09.29 val PER: 0.1442
2026-01-03 19:31:25,866: t15.2023.10.01 val PER: 0.1790
2026-01-03 19:31:25,866: t15.2023.10.06 val PER: 0.0969
2026-01-03 19:31:25,866: t15.2023.10.08 val PER: 0.2585
2026-01-03 19:31:25,866: t15.2023.10.13 val PER: 0.2234
2026-01-03 19:31:25,866: t15.2023.10.15 val PER: 0.1773
2026-01-03 19:31:25,866: t15.2023.10.20 val PER: 0.1913
2026-01-03 19:31:25,866: t15.2023.10.22 val PER: 0.1225
2026-01-03 19:31:25,866: t15.2023.11.03 val PER: 0.1927
2026-01-03 19:31:25,866: t15.2023.11.04 val PER: 0.0307
2026-01-03 19:31:25,866: t15.2023.11.17 val PER: 0.0513
2026-01-03 19:31:25,866: t15.2023.11.19 val PER: 0.0359
2026-01-03 19:31:25,867: t15.2023.11.26 val PER: 0.1536
2026-01-03 19:31:25,867: t15.2023.12.03 val PER: 0.1366
2026-01-03 19:31:25,867: t15.2023.12.08 val PER: 0.1298
2026-01-03 19:31:25,867: t15.2023.12.10 val PER: 0.1117
2026-01-03 19:31:25,867: t15.2023.12.17 val PER: 0.1507
2026-01-03 19:31:25,867: t15.2023.12.29 val PER: 0.1551
2026-01-03 19:31:25,867: t15.2024.02.25 val PER: 0.1292
2026-01-03 19:31:25,867: t15.2024.03.08 val PER: 0.2532
2026-01-03 19:31:25,867: t15.2024.03.15 val PER: 0.2214
2026-01-03 19:31:25,867: t15.2024.03.17 val PER: 0.1520
2026-01-03 19:31:25,868: t15.2024.05.10 val PER: 0.1857
2026-01-03 19:31:25,868: t15.2024.06.14 val PER: 0.1767
2026-01-03 19:31:25,868: t15.2024.07.19 val PER: 0.2630
2026-01-03 19:31:25,868: t15.2024.07.21 val PER: 0.1138
2026-01-03 19:31:25,868: t15.2024.07.28 val PER: 0.1485
2026-01-03 19:31:25,868: t15.2025.01.10 val PER: 0.3223
2026-01-03 19:31:25,868: t15.2025.01.12 val PER: 0.1678
2026-01-03 19:31:25,868: t15.2025.03.14 val PER: 0.3476
2026-01-03 19:31:25,868: t15.2025.03.16 val PER: 0.2055
2026-01-03 19:31:25,868: t15.2025.03.30 val PER: 0.3241
2026-01-03 19:31:25,868: t15.2025.04.13 val PER: 0.2311
2026-01-03 19:31:25,869: New best val WER(1gram) 48.98% --> 48.48%
2026-01-03 19:31:26,169: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_19000
2026-01-03 19:31:44,401: Train batch 19200: loss: 8.65 grad norm: 51.07 time: 0.062
2026-01-03 19:32:02,274: Train batch 19400: loss: 7.37 grad norm: 44.85 time: 0.052
2026-01-03 19:32:11,090: Running test after training batch: 19500
2026-01-03 19:32:11,244: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:32:16,407: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 19:32:16,444: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-03 19:32:18,446: Val batch 19500: PER (avg): 0.1673 CTC Loss (avg): 16.7014 WER(1gram): 48.48% (n=64) time: 7.355
2026-01-03 19:32:18,446: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 19:32:18,447: t15.2023.08.13 val PER: 0.1227
2026-01-03 19:32:18,447: t15.2023.08.18 val PER: 0.1224
2026-01-03 19:32:18,447: t15.2023.08.20 val PER: 0.1207
2026-01-03 19:32:18,447: t15.2023.08.25 val PER: 0.0979
2026-01-03 19:32:18,447: t15.2023.08.27 val PER: 0.2010
2026-01-03 19:32:18,447: t15.2023.09.01 val PER: 0.0852
2026-01-03 19:32:18,447: t15.2023.09.03 val PER: 0.1710
2026-01-03 19:32:18,447: t15.2023.09.24 val PER: 0.1371
2026-01-03 19:32:18,447: t15.2023.09.29 val PER: 0.1436
2026-01-03 19:32:18,447: t15.2023.10.01 val PER: 0.1803
2026-01-03 19:32:18,447: t15.2023.10.06 val PER: 0.0980
2026-01-03 19:32:18,447: t15.2023.10.08 val PER: 0.2639
2026-01-03 19:32:18,447: t15.2023.10.13 val PER: 0.2265
2026-01-03 19:32:18,448: t15.2023.10.15 val PER: 0.1786
2026-01-03 19:32:18,448: t15.2023.10.20 val PER: 0.1812
2026-01-03 19:32:18,448: t15.2023.10.22 val PER: 0.1214
2026-01-03 19:32:18,448: t15.2023.11.03 val PER: 0.1872
2026-01-03 19:32:18,448: t15.2023.11.04 val PER: 0.0375
2026-01-03 19:32:18,448: t15.2023.11.17 val PER: 0.0544
2026-01-03 19:32:18,448: t15.2023.11.19 val PER: 0.0319
2026-01-03 19:32:18,448: t15.2023.11.26 val PER: 0.1601
2026-01-03 19:32:18,448: t15.2023.12.03 val PER: 0.1282
2026-01-03 19:32:18,448: t15.2023.12.08 val PER: 0.1305
2026-01-03 19:32:18,448: t15.2023.12.10 val PER: 0.1091
2026-01-03 19:32:18,449: t15.2023.12.17 val PER: 0.1538
2026-01-03 19:32:18,449: t15.2023.12.29 val PER: 0.1496
2026-01-03 19:32:18,449: t15.2024.02.25 val PER: 0.1320
2026-01-03 19:32:18,449: t15.2024.03.08 val PER: 0.2546
2026-01-03 19:32:18,449: t15.2024.03.15 val PER: 0.2239
2026-01-03 19:32:18,449: t15.2024.03.17 val PER: 0.1527
2026-01-03 19:32:18,449: t15.2024.05.10 val PER: 0.1828
2026-01-03 19:32:18,449: t15.2024.06.14 val PER: 0.1782
2026-01-03 19:32:18,449: t15.2024.07.19 val PER: 0.2571
2026-01-03 19:32:18,449: t15.2024.07.21 val PER: 0.1138
2026-01-03 19:32:18,449: t15.2024.07.28 val PER: 0.1515
2026-01-03 19:32:18,449: t15.2025.01.10 val PER: 0.3182
2026-01-03 19:32:18,449: t15.2025.01.12 val PER: 0.1632
2026-01-03 19:32:18,449: t15.2025.03.14 val PER: 0.3432
2026-01-03 19:32:18,449: t15.2025.03.16 val PER: 0.2016
2026-01-03 19:32:18,449: t15.2025.03.30 val PER: 0.3218
2026-01-03 19:32:18,449: t15.2025.04.13 val PER: 0.2340
2026-01-03 19:32:18,728: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_19500
2026-01-03 19:32:27,554: Train batch 19600: loss: 10.79 grad norm: 55.90 time: 0.057
2026-01-03 19:32:45,180: Train batch 19800: loss: 11.07 grad norm: 58.16 time: 0.055
2026-01-03 19:33:02,812: Running test after training batch: 19999
2026-01-03 19:33:02,906: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:33:08,330: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 19:33:08,366: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-03 19:33:10,357: Val batch 19999: PER (avg): 0.1672 CTC Loss (avg): 16.6983 WER(1gram): 48.98% (n=64) time: 7.545
2026-01-03 19:33:10,358: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 19:33:10,358: t15.2023.08.13 val PER: 0.1216
2026-01-03 19:33:10,358: t15.2023.08.18 val PER: 0.1241
2026-01-03 19:33:10,358: t15.2023.08.20 val PER: 0.1295
2026-01-03 19:33:10,358: t15.2023.08.25 val PER: 0.0949
2026-01-03 19:33:10,358: t15.2023.08.27 val PER: 0.2010
2026-01-03 19:33:10,358: t15.2023.09.01 val PER: 0.0852
2026-01-03 19:33:10,358: t15.2023.09.03 val PER: 0.1663
2026-01-03 19:33:10,359: t15.2023.09.24 val PER: 0.1420
2026-01-03 19:33:10,359: t15.2023.09.29 val PER: 0.1391
2026-01-03 19:33:10,359: t15.2023.10.01 val PER: 0.1816
2026-01-03 19:33:10,359: t15.2023.10.06 val PER: 0.0980
2026-01-03 19:33:10,359: t15.2023.10.08 val PER: 0.2558
2026-01-03 19:33:10,359: t15.2023.10.13 val PER: 0.2289
2026-01-03 19:33:10,359: t15.2023.10.15 val PER: 0.1734
2026-01-03 19:33:10,359: t15.2023.10.20 val PER: 0.1846
2026-01-03 19:33:10,359: t15.2023.10.22 val PER: 0.1214
2026-01-03 19:33:10,359: t15.2023.11.03 val PER: 0.1920
2026-01-03 19:33:10,359: t15.2023.11.04 val PER: 0.0375
2026-01-03 19:33:10,360: t15.2023.11.17 val PER: 0.0529
2026-01-03 19:33:10,360: t15.2023.11.19 val PER: 0.0379
2026-01-03 19:33:10,360: t15.2023.11.26 val PER: 0.1529
2026-01-03 19:33:10,360: t15.2023.12.03 val PER: 0.1303
2026-01-03 19:33:10,360: t15.2023.12.08 val PER: 0.1278
2026-01-03 19:33:10,360: t15.2023.12.10 val PER: 0.1104
2026-01-03 19:33:10,360: t15.2023.12.17 val PER: 0.1486
2026-01-03 19:33:10,360: t15.2023.12.29 val PER: 0.1524
2026-01-03 19:33:10,360: t15.2024.02.25 val PER: 0.1320
2026-01-03 19:33:10,360: t15.2024.03.08 val PER: 0.2532
2026-01-03 19:33:10,360: t15.2024.03.15 val PER: 0.2226
2026-01-03 19:33:10,360: t15.2024.03.17 val PER: 0.1569
2026-01-03 19:33:10,360: t15.2024.05.10 val PER: 0.1857
2026-01-03 19:33:10,360: t15.2024.06.14 val PER: 0.1735
2026-01-03 19:33:10,360: t15.2024.07.19 val PER: 0.2591
2026-01-03 19:33:10,361: t15.2024.07.21 val PER: 0.1145
2026-01-03 19:33:10,361: t15.2024.07.28 val PER: 0.1485
2026-01-03 19:33:10,361: t15.2025.01.10 val PER: 0.3223
2026-01-03 19:33:10,361: t15.2025.01.12 val PER: 0.1686
2026-01-03 19:33:10,361: t15.2025.03.14 val PER: 0.3462
2026-01-03 19:33:10,361: t15.2025.03.16 val PER: 0.1976
2026-01-03 19:33:10,361: t15.2025.03.30 val PER: 0.3207
2026-01-03 19:33:10,361: t15.2025.04.13 val PER: 0.2268
2026-01-03 19:33:10,667: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_19999
2026-01-03 19:33:10,735: Best avg val PER achieved: 0.16791
2026-01-03 19:33:10,736: Total training time: 34.89 minutes

=== RUN d10.yaml ===
2026-01-03 19:33:16,881: Using device: cuda:0
2026-01-03 19:33:18,688: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-03 19:33:18,711: Using 45 sessions after filtering (from 45).
2026-01-03 19:33:19,150: Using torch.compile (if available)
2026-01-03 19:33:19,150: torch.compile not available (torch<2.0). Skipping.
2026-01-03 19:33:19,150: Initialized RNN decoding model
2026-01-03 19:33:19,151: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.1)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 19:33:19,151: Model has 44,907,305 parameters
2026-01-03 19:33:19,151: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 19:33:20,514: Successfully initialized datasets
2026-01-03 19:33:20,514: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 19:33:22,884: Train batch 0: loss: 575.84 grad norm: 1474.69 time: 0.283
2026-01-03 19:33:22,884: Running test after training batch: 0
2026-01-03 19:33:22,996: WER debug GT example: You can see the code at this point as well.
2026-01-03 19:33:28,236: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-03 19:33:28,944: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-03 19:34:02,264: Val batch 0: PER (avg): 1.4291 CTC Loss (avg): 633.3644 WER(1gram): 100.00% (n=64) time: 39.379
2026-01-03 19:34:02,264: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-03 19:34:02,264: t15.2023.08.13 val PER: 1.3067
2026-01-03 19:34:02,265: t15.2023.08.18 val PER: 1.4250
2026-01-03 19:34:02,265: t15.2023.08.20 val PER: 1.2994
2026-01-03 19:34:02,265: t15.2023.08.25 val PER: 1.3343
2026-01-03 19:34:02,265: t15.2023.08.27 val PER: 1.2556
2026-01-03 19:34:02,265: t15.2023.09.01 val PER: 1.4505
2026-01-03 19:34:02,265: t15.2023.09.03 val PER: 1.3159
2026-01-03 19:34:02,265: t15.2023.09.24 val PER: 1.5461
2026-01-03 19:34:02,265: t15.2023.09.29 val PER: 1.4690
2026-01-03 19:34:02,265: t15.2023.10.01 val PER: 1.2120
2026-01-03 19:34:02,265: t15.2023.10.06 val PER: 1.4919
2026-01-03 19:34:02,265: t15.2023.10.08 val PER: 1.1827
2026-01-03 19:34:02,265: t15.2023.10.13 val PER: 1.3941
2026-01-03 19:34:02,265: t15.2023.10.15 val PER: 1.3889
2026-01-03 19:34:02,265: t15.2023.10.20 val PER: 1.4899
2026-01-03 19:34:02,265: t15.2023.10.22 val PER: 1.3886
2026-01-03 19:34:02,265: t15.2023.11.03 val PER: 1.5902
2026-01-03 19:34:02,266: t15.2023.11.04 val PER: 2.0239
2026-01-03 19:34:02,266: t15.2023.11.17 val PER: 1.9611
2026-01-03 19:34:02,266: t15.2023.11.19 val PER: 1.6766
2026-01-03 19:34:02,266: t15.2023.11.26 val PER: 1.5348
2026-01-03 19:34:02,266: t15.2023.12.03 val PER: 1.4286
2026-01-03 19:34:02,266: t15.2023.12.08 val PER: 1.4514
2026-01-03 19:34:02,266: t15.2023.12.10 val PER: 1.7057
2026-01-03 19:34:02,266: t15.2023.12.17 val PER: 1.3077
2026-01-03 19:34:02,266: t15.2023.12.29 val PER: 1.4118
2026-01-03 19:34:02,266: t15.2024.02.25 val PER: 1.4242
2026-01-03 19:34:02,266: t15.2024.03.08 val PER: 1.3257
2026-01-03 19:34:02,266: t15.2024.03.15 val PER: 1.3227
2026-01-03 19:34:02,266: t15.2024.03.17 val PER: 1.4045
2026-01-03 19:34:02,266: t15.2024.05.10 val PER: 1.3165
2026-01-03 19:34:02,267: t15.2024.06.14 val PER: 1.5221
2026-01-03 19:34:02,267: t15.2024.07.19 val PER: 1.0798
2026-01-03 19:34:02,267: t15.2024.07.21 val PER: 1.6262
2026-01-03 19:34:02,267: t15.2024.07.28 val PER: 1.6596
2026-01-03 19:34:02,267: t15.2025.01.10 val PER: 1.0840
2026-01-03 19:34:02,267: t15.2025.01.12 val PER: 1.7652
2026-01-03 19:34:02,267: t15.2025.03.14 val PER: 1.0355
2026-01-03 19:34:02,267: t15.2025.03.16 val PER: 1.6152
2026-01-03 19:34:02,267: t15.2025.03.30 val PER: 1.2839
2026-01-03 19:34:02,267: t15.2025.04.13 val PER: 1.5863
2026-01-03 19:34:02,269: New best val WER(1gram) inf% --> 100.00%
2026-01-03 19:34:02,852: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_0
2026-01-03 19:34:21,000: Train batch 200: loss: 79.39 grad norm: 86.00 time: 0.054
