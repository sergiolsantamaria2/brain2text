torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  3 20:39 /tmp/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
PYTHONFAULTHANDLER=1
MAX_JOBS=8
TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_348229
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_348229/torch_extensions
TORCH_CUDA_ARCH_LIST=8.0
WANDB_DIR=/tmp/e12511253_b2t_348229/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib64:/tmp/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  3 20:39 /tmp/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
PYTHONFAULTHANDLER=1
MAX_JOBS=8
TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_348229
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_348229/torch_extensions
TORCH_CUDA_ARCH_LIST=8.0
WANDB_DIR=/tmp/e12511253_b2t_348229/wandb
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/pkgs/cuda-toolkit/lib64/libcudart.so.11.7.60
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/pkgs/cuda-toolkit/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
trained_models -> /tmp/e12511253_b2t_348229/trained_models
==============================================
Job: b2t_exp  ID: 348229
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/gru/rnn_dropout/lr40
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/gru/rnn_dropout/lr40 ==========
Num configs: 5

=== RUN base.yaml ===
2026-01-03 20:39:46,809: Using device: cuda:0
2026-01-03 20:39:48,199: Local LM WER requested (eval.compute_wer=true) but could not initialize lm_decoder. Disabling WER for this run and continuing. Reason: name 'Path' is not defined
2026-01-03 20:39:48,220: Using 45 sessions after filtering (from 45).
2026-01-03 20:39:48,634: Using torch.compile (if available)
2026-01-03 20:39:48,634: torch.compile not available (torch<2.0). Skipping.
2026-01-03 20:39:48,634: Initialized RNN decoding model
2026-01-03 20:39:48,634: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 20:39:48,635: Model has 44,907,305 parameters
2026-01-03 20:39:48,635: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 20:39:49,911: Successfully initialized datasets
2026-01-03 20:39:49,911: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 20:39:52,515: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.171
2026-01-03 20:39:52,515: Running test after training batch: 0
2026-01-03 20:39:57,316: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): nan% (n=0) time: 4.801
2026-01-03 20:39:57,317: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:39:57,317: t15.2023.08.13 val PER: 1.3056
2026-01-03 20:39:57,317: t15.2023.08.18 val PER: 1.4208
2026-01-03 20:39:57,317: t15.2023.08.20 val PER: 1.3002
2026-01-03 20:39:57,318: t15.2023.08.25 val PER: 1.3389
2026-01-03 20:39:57,318: t15.2023.08.27 val PER: 1.2460
2026-01-03 20:39:57,318: t15.2023.09.01 val PER: 1.4537
2026-01-03 20:39:57,318: t15.2023.09.03 val PER: 1.3171
2026-01-03 20:39:57,318: t15.2023.09.24 val PER: 1.5461
2026-01-03 20:39:57,318: t15.2023.09.29 val PER: 1.4671
2026-01-03 20:39:57,318: t15.2023.10.01 val PER: 1.2147
2026-01-03 20:39:57,318: t15.2023.10.06 val PER: 1.4876
2026-01-03 20:39:57,318: t15.2023.10.08 val PER: 1.1827
2026-01-03 20:39:57,318: t15.2023.10.13 val PER: 1.3964
2026-01-03 20:39:57,319: t15.2023.10.15 val PER: 1.3889
2026-01-03 20:39:57,319: t15.2023.10.20 val PER: 1.4866
2026-01-03 20:39:57,319: t15.2023.10.22 val PER: 1.3942
2026-01-03 20:39:57,319: t15.2023.11.03 val PER: 1.5923
2026-01-03 20:39:57,319: t15.2023.11.04 val PER: 2.0171
2026-01-03 20:39:57,319: t15.2023.11.17 val PER: 1.9518
2026-01-03 20:39:57,319: t15.2023.11.19 val PER: 1.6707
2026-01-03 20:39:57,319: t15.2023.11.26 val PER: 1.5413
2026-01-03 20:39:57,319: t15.2023.12.03 val PER: 1.4254
2026-01-03 20:39:57,319: t15.2023.12.08 val PER: 1.4487
2026-01-03 20:39:57,319: t15.2023.12.10 val PER: 1.6899
2026-01-03 20:39:57,320: t15.2023.12.17 val PER: 1.3077
2026-01-03 20:39:57,320: t15.2023.12.29 val PER: 1.4063
2026-01-03 20:39:57,320: t15.2024.02.25 val PER: 1.4228
2026-01-03 20:39:57,320: t15.2024.03.08 val PER: 1.3257
2026-01-03 20:39:57,320: t15.2024.03.15 val PER: 1.3196
2026-01-03 20:39:57,320: t15.2024.03.17 val PER: 1.4052
2026-01-03 20:39:57,320: t15.2024.05.10 val PER: 1.3224
2026-01-03 20:39:57,320: t15.2024.06.14 val PER: 1.5315
2026-01-03 20:39:57,320: t15.2024.07.19 val PER: 1.0817
2026-01-03 20:39:57,320: t15.2024.07.21 val PER: 1.6290
2026-01-03 20:39:57,320: t15.2024.07.28 val PER: 1.6588
2026-01-03 20:39:57,321: t15.2025.01.10 val PER: 1.0923
2026-01-03 20:39:57,321: t15.2025.01.12 val PER: 1.7629
2026-01-03 20:39:57,321: t15.2025.03.14 val PER: 1.0414
2026-01-03 20:39:57,321: t15.2025.03.16 val PER: 1.6257
2026-01-03 20:39:57,321: t15.2025.03.30 val PER: 1.2874
2026-01-03 20:39:57,321: t15.2025.04.13 val PER: 1.5949
2026-01-03 20:39:57,321: New best val PER inf --> 1.4293
2026-01-03 20:39:57,321: Checkpointing model
2026-01-03 20:39:57,561: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:39:57,803: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_0
2026-01-03 20:40:14,959: Train batch 200: loss: 77.60 grad norm: 105.94 time: 0.054
2026-01-03 20:40:31,799: Train batch 400: loss: 53.79 grad norm: 93.70 time: 0.063
2026-01-03 20:40:40,463: Running test after training batch: 500
2026-01-03 20:40:45,255: Val batch 500: PER (avg): 0.5298 CTC Loss (avg): 55.4246 WER(1gram): nan% (n=0) time: 4.792
2026-01-03 20:40:45,256: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:40:45,256: t15.2023.08.13 val PER: 0.4667
2026-01-03 20:40:45,256: t15.2023.08.18 val PER: 0.4577
2026-01-03 20:40:45,256: t15.2023.08.20 val PER: 0.4448
2026-01-03 20:40:45,256: t15.2023.08.25 val PER: 0.4367
2026-01-03 20:40:45,257: t15.2023.08.27 val PER: 0.5322
2026-01-03 20:40:45,257: t15.2023.09.01 val PER: 0.4318
2026-01-03 20:40:45,257: t15.2023.09.03 val PER: 0.5119
2026-01-03 20:40:45,257: t15.2023.09.24 val PER: 0.4260
2026-01-03 20:40:45,257: t15.2023.09.29 val PER: 0.4767
2026-01-03 20:40:45,257: t15.2023.10.01 val PER: 0.5363
2026-01-03 20:40:45,257: t15.2023.10.06 val PER: 0.4338
2026-01-03 20:40:45,257: t15.2023.10.08 val PER: 0.5467
2026-01-03 20:40:45,257: t15.2023.10.13 val PER: 0.5857
2026-01-03 20:40:45,258: t15.2023.10.15 val PER: 0.5010
2026-01-03 20:40:45,258: t15.2023.10.20 val PER: 0.4564
2026-01-03 20:40:45,258: t15.2023.10.22 val PER: 0.4599
2026-01-03 20:40:45,258: t15.2023.11.03 val PER: 0.5142
2026-01-03 20:40:45,258: t15.2023.11.04 val PER: 0.2594
2026-01-03 20:40:45,258: t15.2023.11.17 val PER: 0.3655
2026-01-03 20:40:45,258: t15.2023.11.19 val PER: 0.3453
2026-01-03 20:40:45,258: t15.2023.11.26 val PER: 0.5659
2026-01-03 20:40:45,258: t15.2023.12.03 val PER: 0.5273
2026-01-03 20:40:45,259: t15.2023.12.08 val PER: 0.5379
2026-01-03 20:40:45,259: t15.2023.12.10 val PER: 0.4652
2026-01-03 20:40:45,259: t15.2023.12.17 val PER: 0.5707
2026-01-03 20:40:45,259: t15.2023.12.29 val PER: 0.5498
2026-01-03 20:40:45,259: t15.2024.02.25 val PER: 0.4944
2026-01-03 20:40:45,259: t15.2024.03.08 val PER: 0.6373
2026-01-03 20:40:45,259: t15.2024.03.15 val PER: 0.5585
2026-01-03 20:40:45,259: t15.2024.03.17 val PER: 0.5202
2026-01-03 20:40:45,259: t15.2024.05.10 val PER: 0.5632
2026-01-03 20:40:45,260: t15.2024.06.14 val PER: 0.5237
2026-01-03 20:40:45,260: t15.2024.07.19 val PER: 0.6895
2026-01-03 20:40:45,260: t15.2024.07.21 val PER: 0.4786
2026-01-03 20:40:45,260: t15.2024.07.28 val PER: 0.5331
2026-01-03 20:40:45,260: t15.2025.01.10 val PER: 0.7562
2026-01-03 20:40:45,260: t15.2025.01.12 val PER: 0.5789
2026-01-03 20:40:45,260: t15.2025.03.14 val PER: 0.7618
2026-01-03 20:40:45,260: t15.2025.03.16 val PER: 0.6126
2026-01-03 20:40:45,261: t15.2025.03.30 val PER: 0.7506
2026-01-03 20:40:45,261: t15.2025.04.13 val PER: 0.5991
2026-01-03 20:40:45,261: New best val PER 1.4293 --> 0.5298
2026-01-03 20:40:45,261: Checkpointing model
2026-01-03 20:40:45,836: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:40:46,077: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_500
2026-01-03 20:40:54,805: Train batch 600: loss: 48.51 grad norm: 81.94 time: 0.078
2026-01-03 20:41:12,189: Train batch 800: loss: 40.88 grad norm: 84.06 time: 0.057
2026-01-03 20:41:30,087: Train batch 1000: loss: 42.78 grad norm: 79.09 time: 0.066
2026-01-03 20:41:30,087: Running test after training batch: 1000
2026-01-03 20:41:34,846: Val batch 1000: PER (avg): 0.4105 CTC Loss (avg): 42.4415 WER(1gram): nan% (n=0) time: 4.759
2026-01-03 20:41:34,847: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:41:34,847: t15.2023.08.13 val PER: 0.3773
2026-01-03 20:41:34,847: t15.2023.08.18 val PER: 0.3437
2026-01-03 20:41:34,847: t15.2023.08.20 val PER: 0.3550
2026-01-03 20:41:34,847: t15.2023.08.25 val PER: 0.2997
2026-01-03 20:41:34,847: t15.2023.08.27 val PER: 0.4244
2026-01-03 20:41:34,848: t15.2023.09.01 val PER: 0.3076
2026-01-03 20:41:34,848: t15.2023.09.03 val PER: 0.4062
2026-01-03 20:41:34,848: t15.2023.09.24 val PER: 0.3216
2026-01-03 20:41:34,848: t15.2023.09.29 val PER: 0.3644
2026-01-03 20:41:34,848: t15.2023.10.01 val PER: 0.4049
2026-01-03 20:41:34,848: t15.2023.10.06 val PER: 0.3068
2026-01-03 20:41:34,848: t15.2023.10.08 val PER: 0.4560
2026-01-03 20:41:34,848: t15.2023.10.13 val PER: 0.4694
2026-01-03 20:41:34,848: t15.2023.10.15 val PER: 0.3705
2026-01-03 20:41:34,849: t15.2023.10.20 val PER: 0.3792
2026-01-03 20:41:34,849: t15.2023.10.22 val PER: 0.3608
2026-01-03 20:41:34,849: t15.2023.11.03 val PER: 0.4064
2026-01-03 20:41:34,849: t15.2023.11.04 val PER: 0.1536
2026-01-03 20:41:34,849: t15.2023.11.17 val PER: 0.2566
2026-01-03 20:41:34,849: t15.2023.11.19 val PER: 0.2076
2026-01-03 20:41:34,849: t15.2023.11.26 val PER: 0.4493
2026-01-03 20:41:34,849: t15.2023.12.03 val PER: 0.4149
2026-01-03 20:41:34,849: t15.2023.12.08 val PER: 0.4041
2026-01-03 20:41:34,849: t15.2023.12.10 val PER: 0.3535
2026-01-03 20:41:34,850: t15.2023.12.17 val PER: 0.4044
2026-01-03 20:41:34,850: t15.2023.12.29 val PER: 0.4091
2026-01-03 20:41:34,850: t15.2024.02.25 val PER: 0.3525
2026-01-03 20:41:34,850: t15.2024.03.08 val PER: 0.4979
2026-01-03 20:41:34,850: t15.2024.03.15 val PER: 0.4459
2026-01-03 20:41:34,850: t15.2024.03.17 val PER: 0.4093
2026-01-03 20:41:34,850: t15.2024.05.10 val PER: 0.4250
2026-01-03 20:41:34,850: t15.2024.06.14 val PER: 0.4069
2026-01-03 20:41:34,850: t15.2024.07.19 val PER: 0.5379
2026-01-03 20:41:34,850: t15.2024.07.21 val PER: 0.3786
2026-01-03 20:41:34,851: t15.2024.07.28 val PER: 0.4118
2026-01-03 20:41:34,851: t15.2025.01.10 val PER: 0.6088
2026-01-03 20:41:34,851: t15.2025.01.12 val PER: 0.4565
2026-01-03 20:41:34,851: t15.2025.03.14 val PER: 0.6435
2026-01-03 20:41:34,851: t15.2025.03.16 val PER: 0.4791
2026-01-03 20:41:34,851: t15.2025.03.30 val PER: 0.6471
2026-01-03 20:41:34,851: t15.2025.04.13 val PER: 0.5036
2026-01-03 20:41:34,851: New best val PER 0.5298 --> 0.4105
2026-01-03 20:41:34,851: Checkpointing model
2026-01-03 20:41:35,430: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:41:35,674: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_1000
2026-01-03 20:41:52,937: Train batch 1200: loss: 33.08 grad norm: 72.78 time: 0.068
2026-01-03 20:42:11,002: Train batch 1400: loss: 35.86 grad norm: 77.36 time: 0.060
2026-01-03 20:42:19,646: Running test after training batch: 1500
2026-01-03 20:42:24,396: Val batch 1500: PER (avg): 0.3816 CTC Loss (avg): 37.3228 WER(1gram): nan% (n=0) time: 4.750
2026-01-03 20:42:24,397: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:42:24,397: t15.2023.08.13 val PER: 0.3545
2026-01-03 20:42:24,397: t15.2023.08.18 val PER: 0.3168
2026-01-03 20:42:24,398: t15.2023.08.20 val PER: 0.3145
2026-01-03 20:42:24,398: t15.2023.08.25 val PER: 0.2560
2026-01-03 20:42:24,398: t15.2023.08.27 val PER: 0.4068
2026-01-03 20:42:24,398: t15.2023.09.01 val PER: 0.2833
2026-01-03 20:42:24,398: t15.2023.09.03 val PER: 0.3765
2026-01-03 20:42:24,398: t15.2023.09.24 val PER: 0.3143
2026-01-03 20:42:24,398: t15.2023.09.29 val PER: 0.3293
2026-01-03 20:42:24,398: t15.2023.10.01 val PER: 0.3983
2026-01-03 20:42:24,398: t15.2023.10.06 val PER: 0.2820
2026-01-03 20:42:24,398: t15.2023.10.08 val PER: 0.4344
2026-01-03 20:42:24,398: t15.2023.10.13 val PER: 0.4523
2026-01-03 20:42:24,399: t15.2023.10.15 val PER: 0.3751
2026-01-03 20:42:24,399: t15.2023.10.20 val PER: 0.3423
2026-01-03 20:42:24,399: t15.2023.10.22 val PER: 0.3163
2026-01-03 20:42:24,399: t15.2023.11.03 val PER: 0.3616
2026-01-03 20:42:24,399: t15.2023.11.04 val PER: 0.1195
2026-01-03 20:42:24,399: t15.2023.11.17 val PER: 0.2208
2026-01-03 20:42:24,399: t15.2023.11.19 val PER: 0.1796
2026-01-03 20:42:24,399: t15.2023.11.26 val PER: 0.4196
2026-01-03 20:42:24,399: t15.2023.12.03 val PER: 0.3708
2026-01-03 20:42:24,399: t15.2023.12.08 val PER: 0.3515
2026-01-03 20:42:24,399: t15.2023.12.10 val PER: 0.3049
2026-01-03 20:42:24,400: t15.2023.12.17 val PER: 0.3857
2026-01-03 20:42:24,400: t15.2023.12.29 val PER: 0.3651
2026-01-03 20:42:24,400: t15.2024.02.25 val PER: 0.2978
2026-01-03 20:42:24,400: t15.2024.03.08 val PER: 0.4680
2026-01-03 20:42:24,400: t15.2024.03.15 val PER: 0.4246
2026-01-03 20:42:24,400: t15.2024.03.17 val PER: 0.3835
2026-01-03 20:42:24,400: t15.2024.05.10 val PER: 0.3878
2026-01-03 20:42:24,400: t15.2024.06.14 val PER: 0.3959
2026-01-03 20:42:24,400: t15.2024.07.19 val PER: 0.5267
2026-01-03 20:42:24,400: t15.2024.07.21 val PER: 0.3497
2026-01-03 20:42:24,400: t15.2024.07.28 val PER: 0.3684
2026-01-03 20:42:24,400: t15.2025.01.10 val PER: 0.6074
2026-01-03 20:42:24,401: t15.2025.01.12 val PER: 0.4303
2026-01-03 20:42:24,401: t15.2025.03.14 val PER: 0.5888
2026-01-03 20:42:24,401: t15.2025.03.16 val PER: 0.4594
2026-01-03 20:42:24,401: t15.2025.03.30 val PER: 0.6138
2026-01-03 20:42:24,401: t15.2025.04.13 val PER: 0.4622
2026-01-03 20:42:24,402: New best val PER 0.4105 --> 0.3816
2026-01-03 20:42:24,402: Checkpointing model
2026-01-03 20:42:25,009: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:42:25,251: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_1500
2026-01-03 20:42:33,673: Train batch 1600: loss: 36.75 grad norm: 78.02 time: 0.063
2026-01-03 20:42:51,004: Train batch 1800: loss: 35.12 grad norm: 69.65 time: 0.088
2026-01-03 20:43:08,987: Train batch 2000: loss: 33.79 grad norm: 71.20 time: 0.066
2026-01-03 20:43:08,987: Running test after training batch: 2000
2026-01-03 20:43:13,751: Val batch 2000: PER (avg): 0.3265 CTC Loss (avg): 32.6708 WER(1gram): nan% (n=0) time: 4.764
2026-01-03 20:43:13,751: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:43:13,752: t15.2023.08.13 val PER: 0.3035
2026-01-03 20:43:13,752: t15.2023.08.18 val PER: 0.2540
2026-01-03 20:43:13,752: t15.2023.08.20 val PER: 0.2566
2026-01-03 20:43:13,752: t15.2023.08.25 val PER: 0.2319
2026-01-03 20:43:13,752: t15.2023.08.27 val PER: 0.3553
2026-01-03 20:43:13,752: t15.2023.09.01 val PER: 0.2378
2026-01-03 20:43:13,752: t15.2023.09.03 val PER: 0.3302
2026-01-03 20:43:13,752: t15.2023.09.24 val PER: 0.2464
2026-01-03 20:43:13,752: t15.2023.09.29 val PER: 0.2744
2026-01-03 20:43:13,752: t15.2023.10.01 val PER: 0.3230
2026-01-03 20:43:13,752: t15.2023.10.06 val PER: 0.2293
2026-01-03 20:43:13,753: t15.2023.10.08 val PER: 0.3816
2026-01-03 20:43:13,753: t15.2023.10.13 val PER: 0.3654
2026-01-03 20:43:13,753: t15.2023.10.15 val PER: 0.2980
2026-01-03 20:43:13,753: t15.2023.10.20 val PER: 0.2953
2026-01-03 20:43:13,753: t15.2023.10.22 val PER: 0.2650
2026-01-03 20:43:13,753: t15.2023.11.03 val PER: 0.3236
2026-01-03 20:43:13,753: t15.2023.11.04 val PER: 0.1092
2026-01-03 20:43:13,753: t15.2023.11.17 val PER: 0.1835
2026-01-03 20:43:13,753: t15.2023.11.19 val PER: 0.1357
2026-01-03 20:43:13,753: t15.2023.11.26 val PER: 0.3601
2026-01-03 20:43:13,753: t15.2023.12.03 val PER: 0.3225
2026-01-03 20:43:13,753: t15.2023.12.08 val PER: 0.3136
2026-01-03 20:43:13,753: t15.2023.12.10 val PER: 0.2628
2026-01-03 20:43:13,753: t15.2023.12.17 val PER: 0.3108
2026-01-03 20:43:13,753: t15.2023.12.29 val PER: 0.3274
2026-01-03 20:43:13,754: t15.2024.02.25 val PER: 0.2711
2026-01-03 20:43:13,754: t15.2024.03.08 val PER: 0.3969
2026-01-03 20:43:13,754: t15.2024.03.15 val PER: 0.3596
2026-01-03 20:43:13,754: t15.2024.03.17 val PER: 0.3466
2026-01-03 20:43:13,754: t15.2024.05.10 val PER: 0.3403
2026-01-03 20:43:13,754: t15.2024.06.14 val PER: 0.3312
2026-01-03 20:43:13,754: t15.2024.07.19 val PER: 0.4443
2026-01-03 20:43:13,754: t15.2024.07.21 val PER: 0.2986
2026-01-03 20:43:13,754: t15.2024.07.28 val PER: 0.3176
2026-01-03 20:43:13,754: t15.2025.01.10 val PER: 0.5331
2026-01-03 20:43:13,754: t15.2025.01.12 val PER: 0.3888
2026-01-03 20:43:13,754: t15.2025.03.14 val PER: 0.5325
2026-01-03 20:43:13,754: t15.2025.03.16 val PER: 0.3914
2026-01-03 20:43:13,754: t15.2025.03.30 val PER: 0.5494
2026-01-03 20:43:13,754: t15.2025.04.13 val PER: 0.4108
2026-01-03 20:43:13,756: New best val PER 0.3816 --> 0.3265
2026-01-03 20:43:13,756: Checkpointing model
2026-01-03 20:43:14,001: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:43:14,249: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_2000
2026-01-03 20:43:31,593: Train batch 2200: loss: 29.41 grad norm: 69.38 time: 0.059
2026-01-03 20:43:48,865: Train batch 2400: loss: 29.04 grad norm: 62.19 time: 0.052
2026-01-03 20:43:57,528: Running test after training batch: 2500
2026-01-03 20:44:02,319: Val batch 2500: PER (avg): 0.3041 CTC Loss (avg): 30.2848 WER(1gram): nan% (n=0) time: 4.791
2026-01-03 20:44:02,320: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:44:02,320: t15.2023.08.13 val PER: 0.2838
2026-01-03 20:44:02,320: t15.2023.08.18 val PER: 0.2431
2026-01-03 20:44:02,320: t15.2023.08.20 val PER: 0.2327
2026-01-03 20:44:02,320: t15.2023.08.25 val PER: 0.2033
2026-01-03 20:44:02,320: t15.2023.08.27 val PER: 0.3215
2026-01-03 20:44:02,320: t15.2023.09.01 val PER: 0.2159
2026-01-03 20:44:02,320: t15.2023.09.03 val PER: 0.2874
2026-01-03 20:44:02,320: t15.2023.09.24 val PER: 0.2221
2026-01-03 20:44:02,320: t15.2023.09.29 val PER: 0.2508
2026-01-03 20:44:02,320: t15.2023.10.01 val PER: 0.3131
2026-01-03 20:44:02,320: t15.2023.10.06 val PER: 0.2282
2026-01-03 20:44:02,321: t15.2023.10.08 val PER: 0.3816
2026-01-03 20:44:02,321: t15.2023.10.13 val PER: 0.3545
2026-01-03 20:44:02,321: t15.2023.10.15 val PER: 0.2835
2026-01-03 20:44:02,321: t15.2023.10.20 val PER: 0.2852
2026-01-03 20:44:02,321: t15.2023.10.22 val PER: 0.2327
2026-01-03 20:44:02,321: t15.2023.11.03 val PER: 0.2985
2026-01-03 20:44:02,321: t15.2023.11.04 val PER: 0.0785
2026-01-03 20:44:02,321: t15.2023.11.17 val PER: 0.1384
2026-01-03 20:44:02,321: t15.2023.11.19 val PER: 0.1218
2026-01-03 20:44:02,321: t15.2023.11.26 val PER: 0.3457
2026-01-03 20:44:02,321: t15.2023.12.03 val PER: 0.2952
2026-01-03 20:44:02,321: t15.2023.12.08 val PER: 0.2836
2026-01-03 20:44:02,321: t15.2023.12.10 val PER: 0.2286
2026-01-03 20:44:02,321: t15.2023.12.17 val PER: 0.2869
2026-01-03 20:44:02,321: t15.2023.12.29 val PER: 0.3089
2026-01-03 20:44:02,322: t15.2024.02.25 val PER: 0.2444
2026-01-03 20:44:02,322: t15.2024.03.08 val PER: 0.3642
2026-01-03 20:44:02,322: t15.2024.03.15 val PER: 0.3440
2026-01-03 20:44:02,322: t15.2024.03.17 val PER: 0.3166
2026-01-03 20:44:02,322: t15.2024.05.10 val PER: 0.3091
2026-01-03 20:44:02,322: t15.2024.06.14 val PER: 0.3044
2026-01-03 20:44:02,322: t15.2024.07.19 val PER: 0.4397
2026-01-03 20:44:02,322: t15.2024.07.21 val PER: 0.2579
2026-01-03 20:44:02,322: t15.2024.07.28 val PER: 0.2985
2026-01-03 20:44:02,322: t15.2025.01.10 val PER: 0.5096
2026-01-03 20:44:02,322: t15.2025.01.12 val PER: 0.3549
2026-01-03 20:44:02,322: t15.2025.03.14 val PER: 0.4941
2026-01-03 20:44:02,322: t15.2025.03.16 val PER: 0.3678
2026-01-03 20:44:02,322: t15.2025.03.30 val PER: 0.5276
2026-01-03 20:44:02,322: t15.2025.04.13 val PER: 0.3980
2026-01-03 20:44:02,324: New best val PER 0.3265 --> 0.3041
2026-01-03 20:44:02,324: Checkpointing model
2026-01-03 20:44:02,591: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:44:02,837: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_2500
2026-01-03 20:44:11,378: Train batch 2600: loss: 35.20 grad norm: 84.85 time: 0.054
2026-01-03 20:44:28,449: Train batch 2800: loss: 25.81 grad norm: 74.87 time: 0.081
2026-01-03 20:44:45,513: Train batch 3000: loss: 31.12 grad norm: 76.64 time: 0.082
2026-01-03 20:44:45,514: Running test after training batch: 3000
2026-01-03 20:44:50,286: Val batch 3000: PER (avg): 0.2813 CTC Loss (avg): 27.7359 WER(1gram): nan% (n=0) time: 4.772
2026-01-03 20:44:50,286: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:44:50,286: t15.2023.08.13 val PER: 0.2484
2026-01-03 20:44:50,286: t15.2023.08.18 val PER: 0.2238
2026-01-03 20:44:50,287: t15.2023.08.20 val PER: 0.2145
2026-01-03 20:44:50,287: t15.2023.08.25 val PER: 0.1913
2026-01-03 20:44:50,287: t15.2023.08.27 val PER: 0.2926
2026-01-03 20:44:50,287: t15.2023.09.01 val PER: 0.1924
2026-01-03 20:44:50,287: t15.2023.09.03 val PER: 0.2898
2026-01-03 20:44:50,287: t15.2023.09.24 val PER: 0.2197
2026-01-03 20:44:50,287: t15.2023.09.29 val PER: 0.2374
2026-01-03 20:44:50,287: t15.2023.10.01 val PER: 0.2913
2026-01-03 20:44:50,287: t15.2023.10.06 val PER: 0.1927
2026-01-03 20:44:50,288: t15.2023.10.08 val PER: 0.3410
2026-01-03 20:44:50,288: t15.2023.10.13 val PER: 0.3522
2026-01-03 20:44:50,288: t15.2023.10.15 val PER: 0.2690
2026-01-03 20:44:50,288: t15.2023.10.20 val PER: 0.2584
2026-01-03 20:44:50,288: t15.2023.10.22 val PER: 0.2060
2026-01-03 20:44:50,288: t15.2023.11.03 val PER: 0.2720
2026-01-03 20:44:50,288: t15.2023.11.04 val PER: 0.0853
2026-01-03 20:44:50,288: t15.2023.11.17 val PER: 0.1384
2026-01-03 20:44:50,288: t15.2023.11.19 val PER: 0.1078
2026-01-03 20:44:50,288: t15.2023.11.26 val PER: 0.3109
2026-01-03 20:44:50,288: t15.2023.12.03 val PER: 0.2752
2026-01-03 20:44:50,288: t15.2023.12.08 val PER: 0.2570
2026-01-03 20:44:50,289: t15.2023.12.10 val PER: 0.2063
2026-01-03 20:44:50,289: t15.2023.12.17 val PER: 0.2827
2026-01-03 20:44:50,289: t15.2023.12.29 val PER: 0.2828
2026-01-03 20:44:50,289: t15.2024.02.25 val PER: 0.2444
2026-01-03 20:44:50,289: t15.2024.03.08 val PER: 0.3670
2026-01-03 20:44:50,289: t15.2024.03.15 val PER: 0.3271
2026-01-03 20:44:50,289: t15.2024.03.17 val PER: 0.2901
2026-01-03 20:44:50,289: t15.2024.05.10 val PER: 0.2897
2026-01-03 20:44:50,289: t15.2024.06.14 val PER: 0.3060
2026-01-03 20:44:50,289: t15.2024.07.19 val PER: 0.3955
2026-01-03 20:44:50,290: t15.2024.07.21 val PER: 0.2276
2026-01-03 20:44:50,290: t15.2024.07.28 val PER: 0.2757
2026-01-03 20:44:50,290: t15.2025.01.10 val PER: 0.4780
2026-01-03 20:44:50,290: t15.2025.01.12 val PER: 0.3087
2026-01-03 20:44:50,290: t15.2025.03.14 val PER: 0.4660
2026-01-03 20:44:50,290: t15.2025.03.16 val PER: 0.3312
2026-01-03 20:44:50,290: t15.2025.03.30 val PER: 0.4770
2026-01-03 20:44:50,290: t15.2025.04.13 val PER: 0.3481
2026-01-03 20:44:50,291: New best val PER 0.3041 --> 0.2813
2026-01-03 20:44:50,291: Checkpointing model
2026-01-03 20:44:50,555: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:44:50,805: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_3000
2026-01-03 20:45:08,106: Train batch 3200: loss: 26.69 grad norm: 73.35 time: 0.075
