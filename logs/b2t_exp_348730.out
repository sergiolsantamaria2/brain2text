TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_348730
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_348730/torch_extensions
WANDB_DIR=/tmp/e12511253_b2t_348730/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/e12511253_b2t_348730/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  4 11:29 /tmp/e12511253_b2t_348730/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
trained_models -> /tmp/e12511253_b2t_348730/trained_models
OUT_ROOT=/tmp/e12511253_b2t_348730/trained_models
==============================================
Job: b2t_exp  ID: 348730
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/weight_decay/lr40
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/weight_decay/lr40 ==========
Num configs: 5

=== RUN base.yaml ===
2026-01-04 11:29:08,170: Using device: cuda:0
2026-01-04 11:29:13,185: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-04 11:29:13,208: Using 45 sessions after filtering (from 45).
2026-01-04 11:29:17,742: Using torch.compile (if available)
2026-01-04 11:29:17,743: torch.compile not available (torch<2.0). Skipping.
2026-01-04 11:29:17,744: Initialized RNN decoding model
2026-01-04 11:29:17,744: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-04 11:29:17,744: Model has 44,907,305 parameters
2026-01-04 11:29:17,745: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-04 11:29:19,091: Successfully initialized datasets
2026-01-04 11:29:19,092: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-04 11:29:20,514: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.483
2026-01-04 11:29:20,514: Running test after training batch: 0
2026-01-04 11:29:21,158: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:29:33,036: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-04 11:29:34,002: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-04 11:30:21,674: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 61.160
2026-01-04 11:30:21,675: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-04 11:30:21,675: t15.2023.08.13 val PER: 1.3056
2026-01-04 11:30:21,675: t15.2023.08.18 val PER: 1.4208
2026-01-04 11:30:21,675: t15.2023.08.20 val PER: 1.3002
2026-01-04 11:30:21,676: t15.2023.08.25 val PER: 1.3389
2026-01-04 11:30:21,676: t15.2023.08.27 val PER: 1.2460
2026-01-04 11:30:21,676: t15.2023.09.01 val PER: 1.4537
2026-01-04 11:30:21,676: t15.2023.09.03 val PER: 1.3171
2026-01-04 11:30:21,676: t15.2023.09.24 val PER: 1.5461
2026-01-04 11:30:21,676: t15.2023.09.29 val PER: 1.4671
2026-01-04 11:30:21,676: t15.2023.10.01 val PER: 1.2147
2026-01-04 11:30:21,677: t15.2023.10.06 val PER: 1.4876
2026-01-04 11:30:21,677: t15.2023.10.08 val PER: 1.1827
2026-01-04 11:30:21,677: t15.2023.10.13 val PER: 1.3964
2026-01-04 11:30:21,677: t15.2023.10.15 val PER: 1.3889
2026-01-04 11:30:21,677: t15.2023.10.20 val PER: 1.4866
2026-01-04 11:30:21,677: t15.2023.10.22 val PER: 1.3942
2026-01-04 11:30:21,677: t15.2023.11.03 val PER: 1.5923
2026-01-04 11:30:21,677: t15.2023.11.04 val PER: 2.0171
2026-01-04 11:30:21,677: t15.2023.11.17 val PER: 1.9518
2026-01-04 11:30:21,678: t15.2023.11.19 val PER: 1.6707
2026-01-04 11:30:21,678: t15.2023.11.26 val PER: 1.5413
2026-01-04 11:30:21,678: t15.2023.12.03 val PER: 1.4254
2026-01-04 11:30:21,678: t15.2023.12.08 val PER: 1.4487
2026-01-04 11:30:21,678: t15.2023.12.10 val PER: 1.6899
2026-01-04 11:30:21,678: t15.2023.12.17 val PER: 1.3077
2026-01-04 11:30:21,678: t15.2023.12.29 val PER: 1.4063
2026-01-04 11:30:21,678: t15.2024.02.25 val PER: 1.4228
2026-01-04 11:30:21,678: t15.2024.03.08 val PER: 1.3257
2026-01-04 11:30:21,678: t15.2024.03.15 val PER: 1.3196
2026-01-04 11:30:21,678: t15.2024.03.17 val PER: 1.4052
2026-01-04 11:30:21,679: t15.2024.05.10 val PER: 1.3224
2026-01-04 11:30:21,679: t15.2024.06.14 val PER: 1.5315
2026-01-04 11:30:21,679: t15.2024.07.19 val PER: 1.0817
2026-01-04 11:30:21,679: t15.2024.07.21 val PER: 1.6290
2026-01-04 11:30:21,679: t15.2024.07.28 val PER: 1.6588
2026-01-04 11:30:21,679: t15.2025.01.10 val PER: 1.0923
2026-01-04 11:30:21,679: t15.2025.01.12 val PER: 1.7629
2026-01-04 11:30:21,679: t15.2025.03.14 val PER: 1.0414
2026-01-04 11:30:21,679: t15.2025.03.16 val PER: 1.6257
2026-01-04 11:30:21,679: t15.2025.03.30 val PER: 1.2874
2026-01-04 11:30:21,679: t15.2025.04.13 val PER: 1.5949
2026-01-04 11:30:21,680: New best val WER(1gram) inf% --> 100.00%
2026-01-04 11:30:21,680: Checkpointing model
2026-01-04 11:30:21,963: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/best_checkpoint
2026-01-04 11:30:22,255: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_0
2026-01-04 11:30:41,808: Train batch 200: loss: 77.59 grad norm: 106.07 time: 0.054
2026-01-04 11:31:00,443: Train batch 400: loss: 54.31 grad norm: 115.12 time: 0.063
2026-01-04 11:31:09,818: Running test after training batch: 500
2026-01-04 11:31:10,050: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:31:15,304: WER debug example
  GT : you can see the code at this point as well
  PR : used and ease thus uhde at this ide is aisle
2026-01-04 11:31:15,344: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as adz
2026-01-04 11:31:18,244: Val batch 500: PER (avg): 0.5166 CTC Loss (avg): 55.5376 WER(1gram): 89.09% (n=64) time: 8.425
2026-01-04 11:31:18,244: WER lens: avg_true_words=6.16 avg_pred_words=5.72 max_pred_words=11
2026-01-04 11:31:18,245: t15.2023.08.13 val PER: 0.4574
2026-01-04 11:31:18,245: t15.2023.08.18 val PER: 0.4493
2026-01-04 11:31:18,245: t15.2023.08.20 val PER: 0.4432
2026-01-04 11:31:18,245: t15.2023.08.25 val PER: 0.4322
2026-01-04 11:31:18,245: t15.2023.08.27 val PER: 0.5145
2026-01-04 11:31:18,245: t15.2023.09.01 val PER: 0.4050
2026-01-04 11:31:18,245: t15.2023.09.03 val PER: 0.4952
2026-01-04 11:31:18,245: t15.2023.09.24 val PER: 0.4333
2026-01-04 11:31:18,245: t15.2023.09.29 val PER: 0.4659
2026-01-04 11:31:18,246: t15.2023.10.01 val PER: 0.5244
2026-01-04 11:31:18,246: t15.2023.10.06 val PER: 0.4295
2026-01-04 11:31:18,246: t15.2023.10.08 val PER: 0.5440
2026-01-04 11:31:18,246: t15.2023.10.13 val PER: 0.5826
2026-01-04 11:31:18,246: t15.2023.10.15 val PER: 0.4865
2026-01-04 11:31:18,247: t15.2023.10.20 val PER: 0.4463
2026-01-04 11:31:18,247: t15.2023.10.22 val PER: 0.4432
2026-01-04 11:31:18,247: t15.2023.11.03 val PER: 0.4986
2026-01-04 11:31:18,247: t15.2023.11.04 val PER: 0.2730
2026-01-04 11:31:18,247: t15.2023.11.17 val PER: 0.3561
2026-01-04 11:31:18,248: t15.2023.11.19 val PER: 0.3293
2026-01-04 11:31:18,248: t15.2023.11.26 val PER: 0.5478
2026-01-04 11:31:18,248: t15.2023.12.03 val PER: 0.4905
2026-01-04 11:31:18,248: t15.2023.12.08 val PER: 0.5107
2026-01-04 11:31:18,248: t15.2023.12.10 val PER: 0.4455
2026-01-04 11:31:18,248: t15.2023.12.17 val PER: 0.5457
2026-01-04 11:31:18,248: t15.2023.12.29 val PER: 0.5470
2026-01-04 11:31:18,248: t15.2024.02.25 val PER: 0.4747
2026-01-04 11:31:18,248: t15.2024.03.08 val PER: 0.6188
2026-01-04 11:31:18,249: t15.2024.03.15 val PER: 0.5560
2026-01-04 11:31:18,249: t15.2024.03.17 val PER: 0.5042
2026-01-04 11:31:18,249: t15.2024.05.10 val PER: 0.5498
2026-01-04 11:31:18,249: t15.2024.06.14 val PER: 0.5047
2026-01-04 11:31:18,249: t15.2024.07.19 val PER: 0.6730
2026-01-04 11:31:18,249: t15.2024.07.21 val PER: 0.4717
2026-01-04 11:31:18,249: t15.2024.07.28 val PER: 0.5074
2026-01-04 11:31:18,249: t15.2025.01.10 val PER: 0.7355
2026-01-04 11:31:18,249: t15.2025.01.12 val PER: 0.5627
2026-01-04 11:31:18,250: t15.2025.03.14 val PER: 0.7663
2026-01-04 11:31:18,250: t15.2025.03.16 val PER: 0.6021
2026-01-04 11:31:18,250: t15.2025.03.30 val PER: 0.7402
2026-01-04 11:31:18,250: t15.2025.04.13 val PER: 0.5806
2026-01-04 11:31:18,250: New best val WER(1gram) 100.00% --> 89.09%
2026-01-04 11:31:18,250: Checkpointing model
2026-01-04 11:31:18,859: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/best_checkpoint
2026-01-04 11:31:19,153: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_500
2026-01-04 11:31:29,021: Train batch 600: loss: 49.11 grad norm: 75.76 time: 0.078
2026-01-04 11:31:49,391: Train batch 800: loss: 40.99 grad norm: 85.94 time: 0.056
2026-01-04 11:32:10,054: Train batch 1000: loss: 43.20 grad norm: 84.22 time: 0.069
2026-01-04 11:32:10,056: Running test after training batch: 1000
2026-01-04 11:32:10,202: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:32:15,368: WER debug example
  GT : you can see the code at this point as well
  PR : used went ease thus good at this boyde is while
2026-01-04 11:32:15,405: WER debug example
  GT : how does it keep the cost down
  PR : houde is it eke that wass it
2026-01-04 11:32:17,660: Val batch 1000: PER (avg): 0.4087 CTC Loss (avg): 42.6940 WER(1gram): 82.23% (n=64) time: 7.604
2026-01-04 11:32:17,661: WER lens: avg_true_words=6.16 avg_pred_words=5.59 max_pred_words=12
2026-01-04 11:32:17,661: t15.2023.08.13 val PER: 0.3846
2026-01-04 11:32:17,661: t15.2023.08.18 val PER: 0.3403
2026-01-04 11:32:17,661: t15.2023.08.20 val PER: 0.3400
2026-01-04 11:32:17,661: t15.2023.08.25 val PER: 0.2952
2026-01-04 11:32:17,661: t15.2023.08.27 val PER: 0.4260
2026-01-04 11:32:17,662: t15.2023.09.01 val PER: 0.2971
2026-01-04 11:32:17,662: t15.2023.09.03 val PER: 0.3895
2026-01-04 11:32:17,662: t15.2023.09.24 val PER: 0.3313
2026-01-04 11:32:17,662: t15.2023.09.29 val PER: 0.3676
2026-01-04 11:32:17,662: t15.2023.10.01 val PER: 0.4042
2026-01-04 11:32:17,662: t15.2023.10.06 val PER: 0.3175
2026-01-04 11:32:17,662: t15.2023.10.08 val PER: 0.4682
2026-01-04 11:32:17,662: t15.2023.10.13 val PER: 0.4663
2026-01-04 11:32:17,662: t15.2023.10.15 val PER: 0.3869
2026-01-04 11:32:17,662: t15.2023.10.20 val PER: 0.3826
2026-01-04 11:32:17,663: t15.2023.10.22 val PER: 0.3541
2026-01-04 11:32:17,663: t15.2023.11.03 val PER: 0.4023
2026-01-04 11:32:17,663: t15.2023.11.04 val PER: 0.1604
2026-01-04 11:32:17,663: t15.2023.11.17 val PER: 0.2566
2026-01-04 11:32:17,663: t15.2023.11.19 val PER: 0.2116
2026-01-04 11:32:17,663: t15.2023.11.26 val PER: 0.4406
2026-01-04 11:32:17,663: t15.2023.12.03 val PER: 0.3929
2026-01-04 11:32:17,663: t15.2023.12.08 val PER: 0.4095
2026-01-04 11:32:17,663: t15.2023.12.10 val PER: 0.3443
2026-01-04 11:32:17,663: t15.2023.12.17 val PER: 0.4023
2026-01-04 11:32:17,663: t15.2023.12.29 val PER: 0.4036
2026-01-04 11:32:17,663: t15.2024.02.25 val PER: 0.3567
2026-01-04 11:32:17,663: t15.2024.03.08 val PER: 0.4993
2026-01-04 11:32:17,664: t15.2024.03.15 val PER: 0.4409
2026-01-04 11:32:17,664: t15.2024.03.17 val PER: 0.4045
2026-01-04 11:32:17,664: t15.2024.05.10 val PER: 0.4220
2026-01-04 11:32:17,664: t15.2024.06.14 val PER: 0.4006
2026-01-04 11:32:17,664: t15.2024.07.19 val PER: 0.5320
2026-01-04 11:32:17,664: t15.2024.07.21 val PER: 0.3841
2026-01-04 11:32:17,664: t15.2024.07.28 val PER: 0.4169
2026-01-04 11:32:17,664: t15.2025.01.10 val PER: 0.6074
2026-01-04 11:32:17,664: t15.2025.01.12 val PER: 0.4511
2026-01-04 11:32:17,664: t15.2025.03.14 val PER: 0.6376
2026-01-04 11:32:17,664: t15.2025.03.16 val PER: 0.4830
2026-01-04 11:32:17,664: t15.2025.03.30 val PER: 0.6379
2026-01-04 11:32:17,665: t15.2025.04.13 val PER: 0.4993
2026-01-04 11:32:17,666: New best val WER(1gram) 89.09% --> 82.23%
2026-01-04 11:32:17,666: Checkpointing model
2026-01-04 11:32:18,312: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/best_checkpoint
2026-01-04 11:32:18,605: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_1000
2026-01-04 11:32:37,289: Train batch 1200: loss: 32.99 grad norm: 74.24 time: 0.067
2026-01-04 11:32:57,218: Train batch 1400: loss: 36.11 grad norm: 78.62 time: 0.061
2026-01-04 11:33:07,523: Running test after training batch: 1500
2026-01-04 11:33:07,707: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:33:12,859: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt e the good it this boyde is will
2026-01-04 11:33:12,892: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that os
2026-01-04 11:33:14,658: Val batch 1500: PER (avg): 0.3795 CTC Loss (avg): 37.2457 WER(1gram): 76.40% (n=64) time: 7.134
2026-01-04 11:33:14,658: WER lens: avg_true_words=6.16 avg_pred_words=5.11 max_pred_words=11
2026-01-04 11:33:14,659: t15.2023.08.13 val PER: 0.3399
2026-01-04 11:33:14,659: t15.2023.08.18 val PER: 0.3185
2026-01-04 11:33:14,659: t15.2023.08.20 val PER: 0.3082
2026-01-04 11:33:14,659: t15.2023.08.25 val PER: 0.2575
2026-01-04 11:33:14,659: t15.2023.08.27 val PER: 0.4084
2026-01-04 11:33:14,659: t15.2023.09.01 val PER: 0.2735
2026-01-04 11:33:14,659: t15.2023.09.03 val PER: 0.3753
2026-01-04 11:33:14,659: t15.2023.09.24 val PER: 0.3143
2026-01-04 11:33:14,659: t15.2023.09.29 val PER: 0.3376
2026-01-04 11:33:14,660: t15.2023.10.01 val PER: 0.4029
2026-01-04 11:33:14,660: t15.2023.10.06 val PER: 0.2863
2026-01-04 11:33:14,660: t15.2023.10.08 val PER: 0.4425
2026-01-04 11:33:14,660: t15.2023.10.13 val PER: 0.4422
2026-01-04 11:33:14,660: t15.2023.10.15 val PER: 0.3619
2026-01-04 11:33:14,660: t15.2023.10.20 val PER: 0.3322
2026-01-04 11:33:14,660: t15.2023.10.22 val PER: 0.3107
2026-01-04 11:33:14,660: t15.2023.11.03 val PER: 0.3630
2026-01-04 11:33:14,660: t15.2023.11.04 val PER: 0.1160
2026-01-04 11:33:14,660: t15.2023.11.17 val PER: 0.2286
2026-01-04 11:33:14,660: t15.2023.11.19 val PER: 0.1816
2026-01-04 11:33:14,660: t15.2023.11.26 val PER: 0.4217
2026-01-04 11:33:14,660: t15.2023.12.03 val PER: 0.3718
2026-01-04 11:33:14,660: t15.2023.12.08 val PER: 0.3475
2026-01-04 11:33:14,660: t15.2023.12.10 val PER: 0.2996
2026-01-04 11:33:14,661: t15.2023.12.17 val PER: 0.3690
2026-01-04 11:33:14,661: t15.2023.12.29 val PER: 0.3693
2026-01-04 11:33:14,661: t15.2024.02.25 val PER: 0.3146
2026-01-04 11:33:14,661: t15.2024.03.08 val PER: 0.4467
2026-01-04 11:33:14,661: t15.2024.03.15 val PER: 0.4184
2026-01-04 11:33:14,661: t15.2024.03.17 val PER: 0.3821
2026-01-04 11:33:14,661: t15.2024.05.10 val PER: 0.3789
2026-01-04 11:33:14,661: t15.2024.06.14 val PER: 0.3959
2026-01-04 11:33:14,661: t15.2024.07.19 val PER: 0.5155
2026-01-04 11:33:14,661: t15.2024.07.21 val PER: 0.3428
2026-01-04 11:33:14,661: t15.2024.07.28 val PER: 0.3743
2026-01-04 11:33:14,661: t15.2025.01.10 val PER: 0.6074
2026-01-04 11:33:14,661: t15.2025.01.12 val PER: 0.4196
2026-01-04 11:33:14,661: t15.2025.03.14 val PER: 0.6021
2026-01-04 11:33:14,661: t15.2025.03.16 val PER: 0.4568
2026-01-04 11:33:14,662: t15.2025.03.30 val PER: 0.6264
2026-01-04 11:33:14,662: t15.2025.04.13 val PER: 0.4665
2026-01-04 11:33:14,663: New best val WER(1gram) 82.23% --> 76.40%
2026-01-04 11:33:14,663: Checkpointing model
2026-01-04 11:33:15,274: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/best_checkpoint
2026-01-04 11:33:15,565: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_1500
2026-01-04 11:33:24,780: Train batch 1600: loss: 36.94 grad norm: 79.94 time: 0.064
2026-01-04 11:33:43,576: Train batch 1800: loss: 35.56 grad norm: 70.11 time: 0.089
2026-01-04 11:34:02,624: Train batch 2000: loss: 34.20 grad norm: 70.99 time: 0.067
2026-01-04 11:34:02,625: Running test after training batch: 2000
2026-01-04 11:34:02,774: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:34:08,066: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this bonde is wheel
2026-01-04 11:34:08,100: WER debug example
  GT : how does it keep the cost down
  PR : houde des it eke thus wass it
2026-01-04 11:34:09,908: Val batch 2000: PER (avg): 0.3288 CTC Loss (avg): 33.0150 WER(1gram): 71.57% (n=64) time: 7.283
2026-01-04 11:34:09,909: WER lens: avg_true_words=6.16 avg_pred_words=5.61 max_pred_words=11
2026-01-04 11:34:09,909: t15.2023.08.13 val PER: 0.3139
2026-01-04 11:34:09,909: t15.2023.08.18 val PER: 0.2615
2026-01-04 11:34:09,909: t15.2023.08.20 val PER: 0.2589
2026-01-04 11:34:09,909: t15.2023.08.25 val PER: 0.2410
2026-01-04 11:34:09,910: t15.2023.08.27 val PER: 0.3473
2026-01-04 11:34:09,910: t15.2023.09.01 val PER: 0.2281
2026-01-04 11:34:09,910: t15.2023.09.03 val PER: 0.3242
2026-01-04 11:34:09,910: t15.2023.09.24 val PER: 0.2633
2026-01-04 11:34:09,910: t15.2023.09.29 val PER: 0.2712
2026-01-04 11:34:09,910: t15.2023.10.01 val PER: 0.3303
2026-01-04 11:34:09,910: t15.2023.10.06 val PER: 0.2347
2026-01-04 11:34:09,910: t15.2023.10.08 val PER: 0.3978
2026-01-04 11:34:09,910: t15.2023.10.13 val PER: 0.3832
2026-01-04 11:34:09,910: t15.2023.10.15 val PER: 0.3019
2026-01-04 11:34:09,910: t15.2023.10.20 val PER: 0.2953
2026-01-04 11:34:09,910: t15.2023.10.22 val PER: 0.2606
2026-01-04 11:34:09,911: t15.2023.11.03 val PER: 0.3223
2026-01-04 11:34:09,911: t15.2023.11.04 val PER: 0.0990
2026-01-04 11:34:09,911: t15.2023.11.17 val PER: 0.1773
2026-01-04 11:34:09,911: t15.2023.11.19 val PER: 0.1277
2026-01-04 11:34:09,911: t15.2023.11.26 val PER: 0.3703
2026-01-04 11:34:09,911: t15.2023.12.03 val PER: 0.3183
2026-01-04 11:34:09,911: t15.2023.12.08 val PER: 0.3109
2026-01-04 11:34:09,911: t15.2023.12.10 val PER: 0.2628
2026-01-04 11:34:09,912: t15.2023.12.17 val PER: 0.3119
2026-01-04 11:34:09,912: t15.2023.12.29 val PER: 0.3315
2026-01-04 11:34:09,912: t15.2024.02.25 val PER: 0.2781
2026-01-04 11:34:09,912: t15.2024.03.08 val PER: 0.3954
2026-01-04 11:34:09,912: t15.2024.03.15 val PER: 0.3646
2026-01-04 11:34:09,912: t15.2024.03.17 val PER: 0.3431
2026-01-04 11:34:09,912: t15.2024.05.10 val PER: 0.3224
2026-01-04 11:34:09,912: t15.2024.06.14 val PER: 0.3470
2026-01-04 11:34:09,912: t15.2024.07.19 val PER: 0.4707
2026-01-04 11:34:09,912: t15.2024.07.21 val PER: 0.2938
2026-01-04 11:34:09,912: t15.2024.07.28 val PER: 0.3235
2026-01-04 11:34:09,912: t15.2025.01.10 val PER: 0.5427
2026-01-04 11:34:09,912: t15.2025.01.12 val PER: 0.3780
2026-01-04 11:34:09,913: t15.2025.03.14 val PER: 0.5266
2026-01-04 11:34:09,913: t15.2025.03.16 val PER: 0.3953
2026-01-04 11:34:09,913: t15.2025.03.30 val PER: 0.5494
2026-01-04 11:34:09,913: t15.2025.04.13 val PER: 0.3951
2026-01-04 11:34:09,914: New best val WER(1gram) 76.40% --> 71.57%
2026-01-04 11:34:09,914: Checkpointing model
2026-01-04 11:34:10,550: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/best_checkpoint
2026-01-04 11:34:10,840: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_2000
2026-01-04 11:34:31,069: Train batch 2200: loss: 28.99 grad norm: 74.52 time: 0.060
2026-01-04 11:34:51,602: Train batch 2400: loss: 29.31 grad norm: 64.18 time: 0.053
2026-01-04 11:35:01,972: Running test after training batch: 2500
2026-01-04 11:35:02,089: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:35:07,279: WER debug example
  GT : you can see the code at this point as well
  PR : yule end e the code at this point is will
2026-01-04 11:35:07,309: WER debug example
  GT : how does it keep the cost down
  PR : houde des it keep the us it
2026-01-04 11:35:09,132: Val batch 2500: PER (avg): 0.3065 CTC Loss (avg): 30.2864 WER(1gram): 67.51% (n=64) time: 7.159
2026-01-04 11:35:09,132: WER lens: avg_true_words=6.16 avg_pred_words=5.58 max_pred_words=10
2026-01-04 11:35:09,132: t15.2023.08.13 val PER: 0.2765
2026-01-04 11:35:09,132: t15.2023.08.18 val PER: 0.2456
2026-01-04 11:35:09,133: t15.2023.08.20 val PER: 0.2470
2026-01-04 11:35:09,133: t15.2023.08.25 val PER: 0.2139
2026-01-04 11:35:09,133: t15.2023.08.27 val PER: 0.3103
2026-01-04 11:35:09,133: t15.2023.09.01 val PER: 0.2037
2026-01-04 11:35:09,133: t15.2023.09.03 val PER: 0.3040
2026-01-04 11:35:09,133: t15.2023.09.24 val PER: 0.2294
2026-01-04 11:35:09,133: t15.2023.09.29 val PER: 0.2674
2026-01-04 11:35:09,133: t15.2023.10.01 val PER: 0.3085
2026-01-04 11:35:09,133: t15.2023.10.06 val PER: 0.2250
2026-01-04 11:35:09,133: t15.2023.10.08 val PER: 0.3708
2026-01-04 11:35:09,133: t15.2023.10.13 val PER: 0.3685
2026-01-04 11:35:09,133: t15.2023.10.15 val PER: 0.2835
2026-01-04 11:35:09,134: t15.2023.10.20 val PER: 0.2685
2026-01-04 11:35:09,134: t15.2023.10.22 val PER: 0.2327
2026-01-04 11:35:09,134: t15.2023.11.03 val PER: 0.2999
2026-01-04 11:35:09,134: t15.2023.11.04 val PER: 0.0853
2026-01-04 11:35:09,134: t15.2023.11.17 val PER: 0.1602
2026-01-04 11:35:09,135: t15.2023.11.19 val PER: 0.1178
2026-01-04 11:35:09,135: t15.2023.11.26 val PER: 0.3478
2026-01-04 11:35:09,135: t15.2023.12.03 val PER: 0.2910
2026-01-04 11:35:09,135: t15.2023.12.08 val PER: 0.2850
2026-01-04 11:35:09,135: t15.2023.12.10 val PER: 0.2365
2026-01-04 11:35:09,135: t15.2023.12.17 val PER: 0.2848
2026-01-04 11:35:09,135: t15.2023.12.29 val PER: 0.3095
2026-01-04 11:35:09,135: t15.2024.02.25 val PER: 0.2542
2026-01-04 11:35:09,136: t15.2024.03.08 val PER: 0.3642
2026-01-04 11:35:09,136: t15.2024.03.15 val PER: 0.3490
2026-01-04 11:35:09,136: t15.2024.03.17 val PER: 0.3131
2026-01-04 11:35:09,136: t15.2024.05.10 val PER: 0.3120
2026-01-04 11:35:09,136: t15.2024.06.14 val PER: 0.3170
2026-01-04 11:35:09,136: t15.2024.07.19 val PER: 0.4390
2026-01-04 11:35:09,136: t15.2024.07.21 val PER: 0.2579
2026-01-04 11:35:09,137: t15.2024.07.28 val PER: 0.3096
2026-01-04 11:35:09,138: t15.2025.01.10 val PER: 0.5028
2026-01-04 11:35:09,138: t15.2025.01.12 val PER: 0.3687
2026-01-04 11:35:09,138: t15.2025.03.14 val PER: 0.4956
2026-01-04 11:35:09,138: t15.2025.03.16 val PER: 0.3678
2026-01-04 11:35:09,138: t15.2025.03.30 val PER: 0.5069
2026-01-04 11:35:09,138: t15.2025.04.13 val PER: 0.4066
2026-01-04 11:35:09,139: New best val WER(1gram) 71.57% --> 67.51%
2026-01-04 11:35:09,139: Checkpointing model
2026-01-04 11:35:09,755: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/best_checkpoint
2026-01-04 11:35:10,044: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_2500
2026-01-04 11:35:20,154: Train batch 2600: loss: 35.42 grad norm: 83.55 time: 0.057
2026-01-04 11:35:40,625: Train batch 2800: loss: 26.19 grad norm: 73.58 time: 0.086
2026-01-04 11:36:01,012: Train batch 3000: loss: 31.13 grad norm: 73.69 time: 0.085
2026-01-04 11:36:01,012: Running test after training batch: 3000
2026-01-04 11:36:01,131: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:36:06,395: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the good at this point is will
2026-01-04 11:36:06,427: WER debug example
  GT : how does it keep the cost down
  PR : houde des it hipp thus cost get
2026-01-04 11:36:08,293: Val batch 3000: PER (avg): 0.2812 CTC Loss (avg): 27.8985 WER(1gram): 67.77% (n=64) time: 7.281
2026-01-04 11:36:08,294: WER lens: avg_true_words=6.16 avg_pred_words=5.77 max_pred_words=11
2026-01-04 11:36:08,294: t15.2023.08.13 val PER: 0.2588
2026-01-04 11:36:08,294: t15.2023.08.18 val PER: 0.2188
2026-01-04 11:36:08,294: t15.2023.08.20 val PER: 0.2129
2026-01-04 11:36:08,294: t15.2023.08.25 val PER: 0.1973
2026-01-04 11:36:08,294: t15.2023.08.27 val PER: 0.2990
2026-01-04 11:36:08,294: t15.2023.09.01 val PER: 0.1818
2026-01-04 11:36:08,294: t15.2023.09.03 val PER: 0.2910
2026-01-04 11:36:08,294: t15.2023.09.24 val PER: 0.2087
2026-01-04 11:36:08,294: t15.2023.09.29 val PER: 0.2374
2026-01-04 11:36:08,295: t15.2023.10.01 val PER: 0.2820
2026-01-04 11:36:08,295: t15.2023.10.06 val PER: 0.1959
2026-01-04 11:36:08,295: t15.2023.10.08 val PER: 0.3599
2026-01-04 11:36:08,295: t15.2023.10.13 val PER: 0.3507
2026-01-04 11:36:08,295: t15.2023.10.15 val PER: 0.2630
2026-01-04 11:36:08,295: t15.2023.10.20 val PER: 0.2718
2026-01-04 11:36:08,295: t15.2023.10.22 val PER: 0.2038
2026-01-04 11:36:08,295: t15.2023.11.03 val PER: 0.2768
2026-01-04 11:36:08,295: t15.2023.11.04 val PER: 0.0751
2026-01-04 11:36:08,295: t15.2023.11.17 val PER: 0.1213
2026-01-04 11:36:08,295: t15.2023.11.19 val PER: 0.1198
2026-01-04 11:36:08,296: t15.2023.11.26 val PER: 0.3043
2026-01-04 11:36:08,296: t15.2023.12.03 val PER: 0.2563
2026-01-04 11:36:08,296: t15.2023.12.08 val PER: 0.2557
2026-01-04 11:36:08,296: t15.2023.12.10 val PER: 0.2037
2026-01-04 11:36:08,296: t15.2023.12.17 val PER: 0.2630
2026-01-04 11:36:08,297: t15.2023.12.29 val PER: 0.2862
2026-01-04 11:36:08,297: t15.2024.02.25 val PER: 0.2303
2026-01-04 11:36:08,297: t15.2024.03.08 val PER: 0.3698
2026-01-04 11:36:08,297: t15.2024.03.15 val PER: 0.3358
2026-01-04 11:36:08,297: t15.2024.03.17 val PER: 0.2852
2026-01-04 11:36:08,297: t15.2024.05.10 val PER: 0.2942
2026-01-04 11:36:08,297: t15.2024.06.14 val PER: 0.2981
2026-01-04 11:36:08,298: t15.2024.07.19 val PER: 0.4021
2026-01-04 11:36:08,298: t15.2024.07.21 val PER: 0.2345
2026-01-04 11:36:08,298: t15.2024.07.28 val PER: 0.2831
2026-01-04 11:36:08,298: t15.2025.01.10 val PER: 0.4835
2026-01-04 11:36:08,298: t15.2025.01.12 val PER: 0.3364
2026-01-04 11:36:08,298: t15.2025.03.14 val PER: 0.4290
2026-01-04 11:36:08,298: t15.2025.03.16 val PER: 0.3259
2026-01-04 11:36:08,298: t15.2025.03.30 val PER: 0.4920
2026-01-04 11:36:08,299: t15.2025.04.13 val PER: 0.3595
2026-01-04 11:36:08,575: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_3000
2026-01-04 11:36:29,068: Train batch 3200: loss: 26.53 grad norm: 64.11 time: 0.077
2026-01-04 11:36:49,680: Train batch 3400: loss: 18.30 grad norm: 54.31 time: 0.049
2026-01-04 11:37:00,054: Running test after training batch: 3500
2026-01-04 11:37:00,164: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:37:05,298: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 11:37:05,330: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it eke thus cussed get
2026-01-04 11:37:07,100: Val batch 3500: PER (avg): 0.2654 CTC Loss (avg): 26.5922 WER(1gram): 65.23% (n=64) time: 7.045
2026-01-04 11:37:07,100: WER lens: avg_true_words=6.16 avg_pred_words=5.94 max_pred_words=11
2026-01-04 11:37:07,101: t15.2023.08.13 val PER: 0.2401
2026-01-04 11:37:07,101: t15.2023.08.18 val PER: 0.2079
2026-01-04 11:37:07,101: t15.2023.08.20 val PER: 0.2065
2026-01-04 11:37:07,101: t15.2023.08.25 val PER: 0.1807
2026-01-04 11:37:07,101: t15.2023.08.27 val PER: 0.2749
2026-01-04 11:37:07,101: t15.2023.09.01 val PER: 0.1713
2026-01-04 11:37:07,101: t15.2023.09.03 val PER: 0.2530
2026-01-04 11:37:07,101: t15.2023.09.24 val PER: 0.2136
2026-01-04 11:37:07,101: t15.2023.09.29 val PER: 0.2119
2026-01-04 11:37:07,101: t15.2023.10.01 val PER: 0.2761
2026-01-04 11:37:07,102: t15.2023.10.06 val PER: 0.1916
2026-01-04 11:37:07,102: t15.2023.10.08 val PER: 0.3437
2026-01-04 11:37:07,102: t15.2023.10.13 val PER: 0.3150
2026-01-04 11:37:07,102: t15.2023.10.15 val PER: 0.2505
2026-01-04 11:37:07,102: t15.2023.10.20 val PER: 0.2416
2026-01-04 11:37:07,102: t15.2023.10.22 val PER: 0.2082
2026-01-04 11:37:07,102: t15.2023.11.03 val PER: 0.2585
2026-01-04 11:37:07,102: t15.2023.11.04 val PER: 0.0683
2026-01-04 11:37:07,102: t15.2023.11.17 val PER: 0.1182
2026-01-04 11:37:07,102: t15.2023.11.19 val PER: 0.1018
2026-01-04 11:37:07,103: t15.2023.11.26 val PER: 0.2891
2026-01-04 11:37:07,103: t15.2023.12.03 val PER: 0.2395
2026-01-04 11:37:07,103: t15.2023.12.08 val PER: 0.2483
2026-01-04 11:37:07,103: t15.2023.12.10 val PER: 0.1997
2026-01-04 11:37:07,103: t15.2023.12.17 val PER: 0.2588
2026-01-04 11:37:07,103: t15.2023.12.29 val PER: 0.2615
2026-01-04 11:37:07,103: t15.2024.02.25 val PER: 0.2177
2026-01-04 11:37:07,104: t15.2024.03.08 val PER: 0.3357
2026-01-04 11:37:07,104: t15.2024.03.15 val PER: 0.3208
2026-01-04 11:37:07,104: t15.2024.03.17 val PER: 0.2782
2026-01-04 11:37:07,104: t15.2024.05.10 val PER: 0.2585
2026-01-04 11:37:07,104: t15.2024.06.14 val PER: 0.2729
2026-01-04 11:37:07,104: t15.2024.07.19 val PER: 0.3883
2026-01-04 11:37:07,104: t15.2024.07.21 val PER: 0.2248
2026-01-04 11:37:07,104: t15.2024.07.28 val PER: 0.2750
2026-01-04 11:37:07,105: t15.2025.01.10 val PER: 0.4601
2026-01-04 11:37:07,105: t15.2025.01.12 val PER: 0.2948
2026-01-04 11:37:07,105: t15.2025.03.14 val PER: 0.4364
2026-01-04 11:37:07,105: t15.2025.03.16 val PER: 0.3194
2026-01-04 11:37:07,105: t15.2025.03.30 val PER: 0.4471
2026-01-04 11:37:07,105: t15.2025.04.13 val PER: 0.3324
2026-01-04 11:37:07,106: New best val WER(1gram) 67.51% --> 65.23%
2026-01-04 11:37:07,106: Checkpointing model
2026-01-04 11:37:07,776: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/best_checkpoint
2026-01-04 11:37:08,064: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_3500
2026-01-04 11:37:18,514: Train batch 3600: loss: 22.42 grad norm: 60.47 time: 0.069
2026-01-04 11:37:38,953: Train batch 3800: loss: 25.80 grad norm: 67.35 time: 0.071
2026-01-04 11:37:59,640: Train batch 4000: loss: 19.28 grad norm: 54.47 time: 0.058
2026-01-04 11:37:59,641: Running test after training batch: 4000
2026-01-04 11:37:59,772: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:38:04,932: WER debug example
  GT : you can see the code at this point as well
  PR : yule kent sci the code at this point is while
2026-01-04 11:38:04,964: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep thus cussed nett
2026-01-04 11:38:06,802: Val batch 4000: PER (avg): 0.2505 CTC Loss (avg): 24.6662 WER(1gram): 65.23% (n=64) time: 7.161
2026-01-04 11:38:06,803: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-04 11:38:06,803: t15.2023.08.13 val PER: 0.2235
2026-01-04 11:38:06,803: t15.2023.08.18 val PER: 0.2112
2026-01-04 11:38:06,803: t15.2023.08.20 val PER: 0.1986
2026-01-04 11:38:06,803: t15.2023.08.25 val PER: 0.1506
2026-01-04 11:38:06,804: t15.2023.08.27 val PER: 0.2894
2026-01-04 11:38:06,804: t15.2023.09.01 val PER: 0.1623
2026-01-04 11:38:06,804: t15.2023.09.03 val PER: 0.2553
2026-01-04 11:38:06,804: t15.2023.09.24 val PER: 0.1857
2026-01-04 11:38:06,805: t15.2023.09.29 val PER: 0.2036
2026-01-04 11:38:06,805: t15.2023.10.01 val PER: 0.2622
2026-01-04 11:38:06,805: t15.2023.10.06 val PER: 0.1636
2026-01-04 11:38:06,805: t15.2023.10.08 val PER: 0.3369
2026-01-04 11:38:06,805: t15.2023.10.13 val PER: 0.3080
2026-01-04 11:38:06,805: t15.2023.10.15 val PER: 0.2419
2026-01-04 11:38:06,805: t15.2023.10.20 val PER: 0.2517
2026-01-04 11:38:06,805: t15.2023.10.22 val PER: 0.1971
2026-01-04 11:38:06,806: t15.2023.11.03 val PER: 0.2463
2026-01-04 11:38:06,806: t15.2023.11.04 val PER: 0.0580
2026-01-04 11:38:06,806: t15.2023.11.17 val PER: 0.1058
2026-01-04 11:38:06,806: t15.2023.11.19 val PER: 0.1018
2026-01-04 11:38:06,806: t15.2023.11.26 val PER: 0.2543
2026-01-04 11:38:06,806: t15.2023.12.03 val PER: 0.2153
2026-01-04 11:38:06,806: t15.2023.12.08 val PER: 0.2270
2026-01-04 11:38:06,806: t15.2023.12.10 val PER: 0.1905
2026-01-04 11:38:06,806: t15.2023.12.17 val PER: 0.2401
2026-01-04 11:38:06,806: t15.2023.12.29 val PER: 0.2512
2026-01-04 11:38:06,806: t15.2024.02.25 val PER: 0.2093
2026-01-04 11:38:06,806: t15.2024.03.08 val PER: 0.3300
2026-01-04 11:38:06,807: t15.2024.03.15 val PER: 0.3002
2026-01-04 11:38:06,807: t15.2024.03.17 val PER: 0.2685
2026-01-04 11:38:06,807: t15.2024.05.10 val PER: 0.2704
2026-01-04 11:38:06,807: t15.2024.06.14 val PER: 0.2618
2026-01-04 11:38:06,807: t15.2024.07.19 val PER: 0.3652
2026-01-04 11:38:06,807: t15.2024.07.21 val PER: 0.1938
2026-01-04 11:38:06,807: t15.2024.07.28 val PER: 0.2390
2026-01-04 11:38:06,807: t15.2025.01.10 val PER: 0.4242
2026-01-04 11:38:06,807: t15.2025.01.12 val PER: 0.2741
2026-01-04 11:38:06,807: t15.2025.03.14 val PER: 0.4112
2026-01-04 11:38:06,807: t15.2025.03.16 val PER: 0.3063
2026-01-04 11:38:06,808: t15.2025.03.30 val PER: 0.4230
2026-01-04 11:38:06,808: t15.2025.04.13 val PER: 0.3338
2026-01-04 11:38:07,085: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_4000
2026-01-04 11:38:27,455: Train batch 4200: loss: 22.65 grad norm: 65.29 time: 0.084
2026-01-04 11:38:47,982: Train batch 4400: loss: 17.02 grad norm: 53.66 time: 0.070
2026-01-04 11:38:58,313: Running test after training batch: 4500
2026-01-04 11:38:58,462: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:39:03,620: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-04 11:39:03,654: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap thus cost get
2026-01-04 11:39:05,462: Val batch 4500: PER (avg): 0.2385 CTC Loss (avg): 23.4941 WER(1gram): 60.15% (n=64) time: 7.148
2026-01-04 11:39:05,462: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-04 11:39:05,463: t15.2023.08.13 val PER: 0.2183
2026-01-04 11:39:05,463: t15.2023.08.18 val PER: 0.1785
2026-01-04 11:39:05,463: t15.2023.08.20 val PER: 0.1851
2026-01-04 11:39:05,463: t15.2023.08.25 val PER: 0.1446
2026-01-04 11:39:05,463: t15.2023.08.27 val PER: 0.2460
2026-01-04 11:39:05,463: t15.2023.09.01 val PER: 0.1591
2026-01-04 11:39:05,463: t15.2023.09.03 val PER: 0.2411
2026-01-04 11:39:05,463: t15.2023.09.24 val PER: 0.1808
2026-01-04 11:39:05,463: t15.2023.09.29 val PER: 0.2010
2026-01-04 11:39:05,464: t15.2023.10.01 val PER: 0.2622
2026-01-04 11:39:05,464: t15.2023.10.06 val PER: 0.1604
2026-01-04 11:39:05,464: t15.2023.10.08 val PER: 0.3342
2026-01-04 11:39:05,464: t15.2023.10.13 val PER: 0.2894
2026-01-04 11:39:05,464: t15.2023.10.15 val PER: 0.2274
2026-01-04 11:39:05,464: t15.2023.10.20 val PER: 0.2349
2026-01-04 11:39:05,464: t15.2023.10.22 val PER: 0.1837
2026-01-04 11:39:05,464: t15.2023.11.03 val PER: 0.2402
2026-01-04 11:39:05,464: t15.2023.11.04 val PER: 0.0580
2026-01-04 11:39:05,465: t15.2023.11.17 val PER: 0.0949
2026-01-04 11:39:05,465: t15.2023.11.19 val PER: 0.0898
2026-01-04 11:39:05,465: t15.2023.11.26 val PER: 0.2659
2026-01-04 11:39:05,465: t15.2023.12.03 val PER: 0.2153
2026-01-04 11:39:05,465: t15.2023.12.08 val PER: 0.2130
2026-01-04 11:39:05,465: t15.2023.12.10 val PER: 0.1721
2026-01-04 11:39:05,465: t15.2023.12.17 val PER: 0.2277
2026-01-04 11:39:05,465: t15.2023.12.29 val PER: 0.2354
2026-01-04 11:39:05,465: t15.2024.02.25 val PER: 0.1952
2026-01-04 11:39:05,465: t15.2024.03.08 val PER: 0.3257
2026-01-04 11:39:05,465: t15.2024.03.15 val PER: 0.2896
2026-01-04 11:39:05,466: t15.2024.03.17 val PER: 0.2420
2026-01-04 11:39:05,466: t15.2024.05.10 val PER: 0.2600
2026-01-04 11:39:05,466: t15.2024.06.14 val PER: 0.2413
2026-01-04 11:39:05,466: t15.2024.07.19 val PER: 0.3395
2026-01-04 11:39:05,466: t15.2024.07.21 val PER: 0.1738
2026-01-04 11:39:05,466: t15.2024.07.28 val PER: 0.2265
2026-01-04 11:39:05,466: t15.2025.01.10 val PER: 0.4118
2026-01-04 11:39:05,466: t15.2025.01.12 val PER: 0.2679
2026-01-04 11:39:05,466: t15.2025.03.14 val PER: 0.4068
2026-01-04 11:39:05,466: t15.2025.03.16 val PER: 0.2827
2026-01-04 11:39:05,466: t15.2025.03.30 val PER: 0.4218
2026-01-04 11:39:05,466: t15.2025.04.13 val PER: 0.3024
2026-01-04 11:39:05,467: New best val WER(1gram) 65.23% --> 60.15%
2026-01-04 11:39:05,467: Checkpointing model
2026-01-04 11:39:06,122: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/best_checkpoint
2026-01-04 11:39:06,423: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_4500
2026-01-04 11:39:16,909: Train batch 4600: loss: 20.27 grad norm: 69.79 time: 0.065
2026-01-04 11:39:37,833: Train batch 4800: loss: 13.62 grad norm: 52.93 time: 0.066
2026-01-04 11:39:58,607: Train batch 5000: loss: 32.85 grad norm: 82.79 time: 0.064
2026-01-04 11:39:58,608: Running test after training batch: 5000
2026-01-04 11:39:58,764: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:40:03,918: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 11:40:03,949: WER debug example
  GT : how does it keep the cost down
  PR : houde just it heap the cussed nett
2026-01-04 11:40:05,777: Val batch 5000: PER (avg): 0.2260 CTC Loss (avg): 22.1307 WER(1gram): 60.41% (n=64) time: 7.169
2026-01-04 11:40:05,778: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-04 11:40:05,778: t15.2023.08.13 val PER: 0.1975
2026-01-04 11:40:05,778: t15.2023.08.18 val PER: 0.1727
2026-01-04 11:40:05,778: t15.2023.08.20 val PER: 0.1827
2026-01-04 11:40:05,778: t15.2023.08.25 val PER: 0.1235
2026-01-04 11:40:05,778: t15.2023.08.27 val PER: 0.2444
2026-01-04 11:40:05,779: t15.2023.09.01 val PER: 0.1315
2026-01-04 11:40:05,779: t15.2023.09.03 val PER: 0.2292
2026-01-04 11:40:05,779: t15.2023.09.24 val PER: 0.1930
2026-01-04 11:40:05,779: t15.2023.09.29 val PER: 0.1761
2026-01-04 11:40:05,779: t15.2023.10.01 val PER: 0.2431
2026-01-04 11:40:05,779: t15.2023.10.06 val PER: 0.1550
2026-01-04 11:40:05,779: t15.2023.10.08 val PER: 0.3085
2026-01-04 11:40:05,779: t15.2023.10.13 val PER: 0.2801
2026-01-04 11:40:05,780: t15.2023.10.15 val PER: 0.2215
2026-01-04 11:40:05,780: t15.2023.10.20 val PER: 0.2248
2026-01-04 11:40:05,780: t15.2023.10.22 val PER: 0.1514
2026-01-04 11:40:05,780: t15.2023.11.03 val PER: 0.2246
2026-01-04 11:40:05,780: t15.2023.11.04 val PER: 0.0478
2026-01-04 11:40:05,781: t15.2023.11.17 val PER: 0.0918
2026-01-04 11:40:05,781: t15.2023.11.19 val PER: 0.0798
2026-01-04 11:40:05,781: t15.2023.11.26 val PER: 0.2290
2026-01-04 11:40:05,781: t15.2023.12.03 val PER: 0.2006
2026-01-04 11:40:05,781: t15.2023.12.08 val PER: 0.2004
2026-01-04 11:40:05,781: t15.2023.12.10 val PER: 0.1629
2026-01-04 11:40:05,782: t15.2023.12.17 val PER: 0.2318
2026-01-04 11:40:05,782: t15.2023.12.29 val PER: 0.2224
2026-01-04 11:40:05,782: t15.2024.02.25 val PER: 0.1938
2026-01-04 11:40:05,782: t15.2024.03.08 val PER: 0.3044
2026-01-04 11:40:05,782: t15.2024.03.15 val PER: 0.2839
2026-01-04 11:40:05,782: t15.2024.03.17 val PER: 0.2245
2026-01-04 11:40:05,782: t15.2024.05.10 val PER: 0.2467
2026-01-04 11:40:05,782: t15.2024.06.14 val PER: 0.2524
2026-01-04 11:40:05,783: t15.2024.07.19 val PER: 0.3375
2026-01-04 11:40:05,783: t15.2024.07.21 val PER: 0.1731
2026-01-04 11:40:05,783: t15.2024.07.28 val PER: 0.2110
2026-01-04 11:40:05,783: t15.2025.01.10 val PER: 0.3898
2026-01-04 11:40:05,783: t15.2025.01.12 val PER: 0.2517
2026-01-04 11:40:05,783: t15.2025.03.14 val PER: 0.3802
2026-01-04 11:40:05,783: t15.2025.03.16 val PER: 0.2775
2026-01-04 11:40:05,784: t15.2025.03.30 val PER: 0.4034
2026-01-04 11:40:05,784: t15.2025.04.13 val PER: 0.2953
2026-01-04 11:40:06,062: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_5000
2026-01-04 11:40:26,494: Train batch 5200: loss: 16.77 grad norm: 60.14 time: 0.052
2026-01-04 11:40:46,972: Train batch 5400: loss: 17.75 grad norm: 59.03 time: 0.068
2026-01-04 11:40:57,355: Running test after training batch: 5500
2026-01-04 11:40:57,544: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:41:02,703: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 11:41:02,737: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost tit
2026-01-04 11:41:04,532: Val batch 5500: PER (avg): 0.2169 CTC Loss (avg): 21.1903 WER(1gram): 59.39% (n=64) time: 7.176
2026-01-04 11:41:04,533: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=10
2026-01-04 11:41:04,533: t15.2023.08.13 val PER: 0.1788
2026-01-04 11:41:04,533: t15.2023.08.18 val PER: 0.1685
2026-01-04 11:41:04,533: t15.2023.08.20 val PER: 0.1724
2026-01-04 11:41:04,534: t15.2023.08.25 val PER: 0.1220
2026-01-04 11:41:04,534: t15.2023.08.27 val PER: 0.2395
2026-01-04 11:41:04,534: t15.2023.09.01 val PER: 0.1420
2026-01-04 11:41:04,534: t15.2023.09.03 val PER: 0.2209
2026-01-04 11:41:04,534: t15.2023.09.24 val PER: 0.1699
2026-01-04 11:41:04,534: t15.2023.09.29 val PER: 0.1755
2026-01-04 11:41:04,534: t15.2023.10.01 val PER: 0.2358
2026-01-04 11:41:04,534: t15.2023.10.06 val PER: 0.1421
2026-01-04 11:41:04,534: t15.2023.10.08 val PER: 0.2950
2026-01-04 11:41:04,535: t15.2023.10.13 val PER: 0.2754
2026-01-04 11:41:04,535: t15.2023.10.15 val PER: 0.2129
2026-01-04 11:41:04,535: t15.2023.10.20 val PER: 0.2215
2026-01-04 11:41:04,535: t15.2023.10.22 val PER: 0.1581
2026-01-04 11:41:04,535: t15.2023.11.03 val PER: 0.2205
2026-01-04 11:41:04,535: t15.2023.11.04 val PER: 0.0512
2026-01-04 11:41:04,535: t15.2023.11.17 val PER: 0.0747
2026-01-04 11:41:04,535: t15.2023.11.19 val PER: 0.0739
2026-01-04 11:41:04,535: t15.2023.11.26 val PER: 0.2210
2026-01-04 11:41:04,536: t15.2023.12.03 val PER: 0.1933
2026-01-04 11:41:04,536: t15.2023.12.08 val PER: 0.1864
2026-01-04 11:41:04,536: t15.2023.12.10 val PER: 0.1524
2026-01-04 11:41:04,536: t15.2023.12.17 val PER: 0.2214
2026-01-04 11:41:04,536: t15.2023.12.29 val PER: 0.2176
2026-01-04 11:41:04,536: t15.2024.02.25 val PER: 0.1812
2026-01-04 11:41:04,536: t15.2024.03.08 val PER: 0.2859
2026-01-04 11:41:04,536: t15.2024.03.15 val PER: 0.2552
2026-01-04 11:41:04,536: t15.2024.03.17 val PER: 0.2127
2026-01-04 11:41:04,536: t15.2024.05.10 val PER: 0.2348
2026-01-04 11:41:04,536: t15.2024.06.14 val PER: 0.2303
2026-01-04 11:41:04,536: t15.2024.07.19 val PER: 0.3256
2026-01-04 11:41:04,536: t15.2024.07.21 val PER: 0.1634
2026-01-04 11:41:04,536: t15.2024.07.28 val PER: 0.2191
2026-01-04 11:41:04,537: t15.2025.01.10 val PER: 0.3953
2026-01-04 11:41:04,537: t15.2025.01.12 val PER: 0.2340
2026-01-04 11:41:04,537: t15.2025.03.14 val PER: 0.3580
2026-01-04 11:41:04,537: t15.2025.03.16 val PER: 0.2736
2026-01-04 11:41:04,537: t15.2025.03.30 val PER: 0.3678
2026-01-04 11:41:04,537: t15.2025.04.13 val PER: 0.2981
2026-01-04 11:41:04,538: New best val WER(1gram) 60.15% --> 59.39%
2026-01-04 11:41:04,538: Checkpointing model
2026-01-04 11:41:05,191: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/best_checkpoint
2026-01-04 11:41:05,505: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_5500
2026-01-04 11:41:15,908: Train batch 5600: loss: 19.66 grad norm: 72.15 time: 0.062
2026-01-04 11:41:36,487: Train batch 5800: loss: 13.71 grad norm: 56.01 time: 0.087
2026-01-04 11:41:56,894: Train batch 6000: loss: 14.83 grad norm: 56.97 time: 0.051
2026-01-04 11:41:56,894: Running test after training batch: 6000
2026-01-04 11:41:57,023: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:42:02,155: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-04 11:42:02,191: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nit
2026-01-04 11:42:04,058: Val batch 6000: PER (avg): 0.2122 CTC Loss (avg): 20.8980 WER(1gram): 57.61% (n=64) time: 7.164
2026-01-04 11:42:04,059: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 11:42:04,059: t15.2023.08.13 val PER: 0.1715
2026-01-04 11:42:04,059: t15.2023.08.18 val PER: 0.1567
2026-01-04 11:42:04,059: t15.2023.08.20 val PER: 0.1644
2026-01-04 11:42:04,059: t15.2023.08.25 val PER: 0.1160
2026-01-04 11:42:04,059: t15.2023.08.27 val PER: 0.2460
2026-01-04 11:42:04,060: t15.2023.09.01 val PER: 0.1396
2026-01-04 11:42:04,060: t15.2023.09.03 val PER: 0.2126
2026-01-04 11:42:04,060: t15.2023.09.24 val PER: 0.1663
2026-01-04 11:42:04,060: t15.2023.09.29 val PER: 0.1704
2026-01-04 11:42:04,060: t15.2023.10.01 val PER: 0.2153
2026-01-04 11:42:04,060: t15.2023.10.06 val PER: 0.1367
2026-01-04 11:42:04,060: t15.2023.10.08 val PER: 0.2882
2026-01-04 11:42:04,060: t15.2023.10.13 val PER: 0.2669
2026-01-04 11:42:04,060: t15.2023.10.15 val PER: 0.2189
2026-01-04 11:42:04,060: t15.2023.10.20 val PER: 0.2148
2026-01-04 11:42:04,060: t15.2023.10.22 val PER: 0.1704
2026-01-04 11:42:04,060: t15.2023.11.03 val PER: 0.2205
2026-01-04 11:42:04,060: t15.2023.11.04 val PER: 0.0648
2026-01-04 11:42:04,060: t15.2023.11.17 val PER: 0.0809
2026-01-04 11:42:04,061: t15.2023.11.19 val PER: 0.0778
2026-01-04 11:42:04,061: t15.2023.11.26 val PER: 0.2196
2026-01-04 11:42:04,061: t15.2023.12.03 val PER: 0.1723
2026-01-04 11:42:04,061: t15.2023.12.08 val PER: 0.1731
2026-01-04 11:42:04,061: t15.2023.12.10 val PER: 0.1445
2026-01-04 11:42:04,061: t15.2023.12.17 val PER: 0.1996
2026-01-04 11:42:04,061: t15.2023.12.29 val PER: 0.2203
2026-01-04 11:42:04,061: t15.2024.02.25 val PER: 0.1756
2026-01-04 11:42:04,061: t15.2024.03.08 val PER: 0.2831
2026-01-04 11:42:04,061: t15.2024.03.15 val PER: 0.2689
2026-01-04 11:42:04,062: t15.2024.03.17 val PER: 0.2245
2026-01-04 11:42:04,062: t15.2024.05.10 val PER: 0.2259
2026-01-04 11:42:04,062: t15.2024.06.14 val PER: 0.2256
2026-01-04 11:42:04,062: t15.2024.07.19 val PER: 0.3065
2026-01-04 11:42:04,062: t15.2024.07.21 val PER: 0.1614
2026-01-04 11:42:04,062: t15.2024.07.28 val PER: 0.2110
2026-01-04 11:42:04,062: t15.2025.01.10 val PER: 0.3884
2026-01-04 11:42:04,062: t15.2025.01.12 val PER: 0.2248
2026-01-04 11:42:04,062: t15.2025.03.14 val PER: 0.3787
2026-01-04 11:42:04,062: t15.2025.03.16 val PER: 0.2539
2026-01-04 11:42:04,062: t15.2025.03.30 val PER: 0.3759
2026-01-04 11:42:04,062: t15.2025.04.13 val PER: 0.2696
2026-01-04 11:42:04,063: New best val WER(1gram) 59.39% --> 57.61%
2026-01-04 11:42:04,063: Checkpointing model
2026-01-04 11:42:04,738: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/best_checkpoint
2026-01-04 11:42:05,042: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_6000
2026-01-04 11:42:25,507: Train batch 6200: loss: 16.73 grad norm: 60.81 time: 0.070
2026-01-04 11:42:45,769: Train batch 6400: loss: 19.32 grad norm: 66.94 time: 0.064
2026-01-04 11:42:55,777: Running test after training batch: 6500
2026-01-04 11:42:55,930: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:43:01,083: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 11:43:01,117: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 11:43:02,971: Val batch 6500: PER (avg): 0.2031 CTC Loss (avg): 20.0813 WER(1gram): 53.05% (n=64) time: 7.193
2026-01-04 11:43:02,972: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 11:43:02,972: t15.2023.08.13 val PER: 0.1674
2026-01-04 11:43:02,972: t15.2023.08.18 val PER: 0.1417
2026-01-04 11:43:02,972: t15.2023.08.20 val PER: 0.1549
2026-01-04 11:43:02,973: t15.2023.08.25 val PER: 0.1190
2026-01-04 11:43:02,973: t15.2023.08.27 val PER: 0.2331
2026-01-04 11:43:02,973: t15.2023.09.01 val PER: 0.1185
2026-01-04 11:43:02,973: t15.2023.09.03 val PER: 0.2067
2026-01-04 11:43:02,973: t15.2023.09.24 val PER: 0.1760
2026-01-04 11:43:02,973: t15.2023.09.29 val PER: 0.1717
2026-01-04 11:43:02,973: t15.2023.10.01 val PER: 0.2180
2026-01-04 11:43:02,974: t15.2023.10.06 val PER: 0.1249
2026-01-04 11:43:02,974: t15.2023.10.08 val PER: 0.3072
2026-01-04 11:43:02,974: t15.2023.10.13 val PER: 0.2622
2026-01-04 11:43:02,974: t15.2023.10.15 val PER: 0.2109
2026-01-04 11:43:02,974: t15.2023.10.20 val PER: 0.2181
2026-01-04 11:43:02,974: t15.2023.10.22 val PER: 0.1615
2026-01-04 11:43:02,974: t15.2023.11.03 val PER: 0.2123
2026-01-04 11:43:02,975: t15.2023.11.04 val PER: 0.0512
2026-01-04 11:43:02,975: t15.2023.11.17 val PER: 0.0575
2026-01-04 11:43:02,975: t15.2023.11.19 val PER: 0.0739
2026-01-04 11:43:02,975: t15.2023.11.26 val PER: 0.2152
2026-01-04 11:43:02,975: t15.2023.12.03 val PER: 0.1733
2026-01-04 11:43:02,975: t15.2023.12.08 val PER: 0.1724
2026-01-04 11:43:02,975: t15.2023.12.10 val PER: 0.1340
2026-01-04 11:43:02,975: t15.2023.12.17 val PER: 0.1809
2026-01-04 11:43:02,975: t15.2023.12.29 val PER: 0.1977
2026-01-04 11:43:02,975: t15.2024.02.25 val PER: 0.1601
2026-01-04 11:43:02,976: t15.2024.03.08 val PER: 0.2831
2026-01-04 11:43:02,976: t15.2024.03.15 val PER: 0.2577
2026-01-04 11:43:02,976: t15.2024.03.17 val PER: 0.2064
2026-01-04 11:43:02,976: t15.2024.05.10 val PER: 0.2184
2026-01-04 11:43:02,976: t15.2024.06.14 val PER: 0.2082
2026-01-04 11:43:02,976: t15.2024.07.19 val PER: 0.3059
2026-01-04 11:43:02,976: t15.2024.07.21 val PER: 0.1483
2026-01-04 11:43:02,977: t15.2024.07.28 val PER: 0.1882
2026-01-04 11:43:02,977: t15.2025.01.10 val PER: 0.3650
2026-01-04 11:43:02,977: t15.2025.01.12 val PER: 0.2125
2026-01-04 11:43:02,977: t15.2025.03.14 val PER: 0.3683
2026-01-04 11:43:02,977: t15.2025.03.16 val PER: 0.2369
2026-01-04 11:43:02,977: t15.2025.03.30 val PER: 0.3494
2026-01-04 11:43:02,977: t15.2025.04.13 val PER: 0.2668
2026-01-04 11:43:02,978: New best val WER(1gram) 57.61% --> 53.05%
2026-01-04 11:43:02,978: Checkpointing model
2026-01-04 11:43:03,612: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/best_checkpoint
2026-01-04 11:43:03,903: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_6500
2026-01-04 11:43:13,906: Train batch 6600: loss: 12.95 grad norm: 57.67 time: 0.045
2026-01-04 11:43:34,348: Train batch 6800: loss: 15.90 grad norm: 56.85 time: 0.051
2026-01-04 11:43:55,053: Train batch 7000: loss: 17.51 grad norm: 62.98 time: 0.063
2026-01-04 11:43:55,054: Running test after training batch: 7000
2026-01-04 11:43:55,173: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:44:00,406: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 11:44:00,438: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-04 11:44:02,302: Val batch 7000: PER (avg): 0.1956 CTC Loss (avg): 19.1726 WER(1gram): 54.31% (n=64) time: 7.248
2026-01-04 11:44:02,302: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-04 11:44:02,302: t15.2023.08.13 val PER: 0.1486
2026-01-04 11:44:02,303: t15.2023.08.18 val PER: 0.1408
2026-01-04 11:44:02,303: t15.2023.08.20 val PER: 0.1477
2026-01-04 11:44:02,303: t15.2023.08.25 val PER: 0.1069
2026-01-04 11:44:02,303: t15.2023.08.27 val PER: 0.2219
2026-01-04 11:44:02,303: t15.2023.09.01 val PER: 0.1071
2026-01-04 11:44:02,303: t15.2023.09.03 val PER: 0.1971
2026-01-04 11:44:02,304: t15.2023.09.24 val PER: 0.1663
2026-01-04 11:44:02,304: t15.2023.09.29 val PER: 0.1691
2026-01-04 11:44:02,304: t15.2023.10.01 val PER: 0.2061
2026-01-04 11:44:02,304: t15.2023.10.06 val PER: 0.1152
2026-01-04 11:44:02,304: t15.2023.10.08 val PER: 0.2869
2026-01-04 11:44:02,305: t15.2023.10.13 val PER: 0.2576
2026-01-04 11:44:02,305: t15.2023.10.15 val PER: 0.1958
2026-01-04 11:44:02,305: t15.2023.10.20 val PER: 0.2081
2026-01-04 11:44:02,305: t15.2023.10.22 val PER: 0.1425
2026-01-04 11:44:02,306: t15.2023.11.03 val PER: 0.2008
2026-01-04 11:44:02,306: t15.2023.11.04 val PER: 0.0410
2026-01-04 11:44:02,306: t15.2023.11.17 val PER: 0.0669
2026-01-04 11:44:02,307: t15.2023.11.19 val PER: 0.0539
2026-01-04 11:44:02,307: t15.2023.11.26 val PER: 0.1964
2026-01-04 11:44:02,307: t15.2023.12.03 val PER: 0.1691
2026-01-04 11:44:02,307: t15.2023.12.08 val PER: 0.1625
2026-01-04 11:44:02,307: t15.2023.12.10 val PER: 0.1406
2026-01-04 11:44:02,307: t15.2023.12.17 val PER: 0.1871
2026-01-04 11:44:02,307: t15.2023.12.29 val PER: 0.1901
2026-01-04 11:44:02,307: t15.2024.02.25 val PER: 0.1615
2026-01-04 11:44:02,308: t15.2024.03.08 val PER: 0.2760
2026-01-04 11:44:02,308: t15.2024.03.15 val PER: 0.2433
2026-01-04 11:44:02,308: t15.2024.03.17 val PER: 0.2001
2026-01-04 11:44:02,308: t15.2024.05.10 val PER: 0.1961
2026-01-04 11:44:02,308: t15.2024.06.14 val PER: 0.2019
2026-01-04 11:44:02,308: t15.2024.07.19 val PER: 0.3013
2026-01-04 11:44:02,308: t15.2024.07.21 val PER: 0.1331
2026-01-04 11:44:02,308: t15.2024.07.28 val PER: 0.1750
2026-01-04 11:44:02,309: t15.2025.01.10 val PER: 0.3719
2026-01-04 11:44:02,309: t15.2025.01.12 val PER: 0.2186
2026-01-04 11:44:02,309: t15.2025.03.14 val PER: 0.3743
2026-01-04 11:44:02,309: t15.2025.03.16 val PER: 0.2408
2026-01-04 11:44:02,309: t15.2025.03.30 val PER: 0.3563
2026-01-04 11:44:02,309: t15.2025.04.13 val PER: 0.2696
2026-01-04 11:44:02,586: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_7000
2026-01-04 11:44:23,051: Train batch 7200: loss: 14.54 grad norm: 56.96 time: 0.080
2026-01-04 11:44:43,450: Train batch 7400: loss: 13.34 grad norm: 53.16 time: 0.079
2026-01-04 11:44:53,645: Running test after training batch: 7500
2026-01-04 11:44:53,861: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:44:58,991: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 11:44:59,026: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost tint
2026-01-04 11:45:00,905: Val batch 7500: PER (avg): 0.1901 CTC Loss (avg): 18.8876 WER(1gram): 55.08% (n=64) time: 7.260
2026-01-04 11:45:00,907: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 11:45:00,907: t15.2023.08.13 val PER: 0.1414
2026-01-04 11:45:00,907: t15.2023.08.18 val PER: 0.1366
2026-01-04 11:45:00,907: t15.2023.08.20 val PER: 0.1398
2026-01-04 11:45:00,907: t15.2023.08.25 val PER: 0.0994
2026-01-04 11:45:00,908: t15.2023.08.27 val PER: 0.2042
2026-01-04 11:45:00,908: t15.2023.09.01 val PER: 0.1153
2026-01-04 11:45:00,908: t15.2023.09.03 val PER: 0.1853
2026-01-04 11:45:00,908: t15.2023.09.24 val PER: 0.1578
2026-01-04 11:45:00,908: t15.2023.09.29 val PER: 0.1640
2026-01-04 11:45:00,908: t15.2023.10.01 val PER: 0.2054
2026-01-04 11:45:00,909: t15.2023.10.06 val PER: 0.1087
2026-01-04 11:45:00,909: t15.2023.10.08 val PER: 0.2679
2026-01-04 11:45:00,909: t15.2023.10.13 val PER: 0.2459
2026-01-04 11:45:00,909: t15.2023.10.15 val PER: 0.1925
2026-01-04 11:45:00,909: t15.2023.10.20 val PER: 0.2114
2026-01-04 11:45:00,909: t15.2023.10.22 val PER: 0.1392
2026-01-04 11:45:00,909: t15.2023.11.03 val PER: 0.2110
2026-01-04 11:45:00,909: t15.2023.11.04 val PER: 0.0478
2026-01-04 11:45:00,909: t15.2023.11.17 val PER: 0.0731
2026-01-04 11:45:00,909: t15.2023.11.19 val PER: 0.0519
2026-01-04 11:45:00,909: t15.2023.11.26 val PER: 0.1942
2026-01-04 11:45:00,909: t15.2023.12.03 val PER: 0.1607
2026-01-04 11:45:00,910: t15.2023.12.08 val PER: 0.1451
2026-01-04 11:45:00,910: t15.2023.12.10 val PER: 0.1275
2026-01-04 11:45:00,910: t15.2023.12.17 val PER: 0.1757
2026-01-04 11:45:00,910: t15.2023.12.29 val PER: 0.1812
2026-01-04 11:45:00,910: t15.2024.02.25 val PER: 0.1559
2026-01-04 11:45:00,910: t15.2024.03.08 val PER: 0.2774
2026-01-04 11:45:00,910: t15.2024.03.15 val PER: 0.2545
2026-01-04 11:45:00,910: t15.2024.03.17 val PER: 0.1750
2026-01-04 11:45:00,910: t15.2024.05.10 val PER: 0.1991
2026-01-04 11:45:00,910: t15.2024.06.14 val PER: 0.2082
2026-01-04 11:45:00,910: t15.2024.07.19 val PER: 0.2907
2026-01-04 11:45:00,910: t15.2024.07.21 val PER: 0.1345
2026-01-04 11:45:00,910: t15.2024.07.28 val PER: 0.1809
2026-01-04 11:45:00,910: t15.2025.01.10 val PER: 0.3581
2026-01-04 11:45:00,911: t15.2025.01.12 val PER: 0.1994
2026-01-04 11:45:00,911: t15.2025.03.14 val PER: 0.3698
2026-01-04 11:45:00,911: t15.2025.03.16 val PER: 0.2474
2026-01-04 11:45:00,911: t15.2025.03.30 val PER: 0.3379
2026-01-04 11:45:00,911: t15.2025.04.13 val PER: 0.2525
2026-01-04 11:45:01,191: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_7500
2026-01-04 11:45:11,351: Train batch 7600: loss: 16.53 grad norm: 60.68 time: 0.075
2026-01-04 11:45:32,026: Train batch 7800: loss: 14.26 grad norm: 58.94 time: 0.058
2026-01-04 11:45:53,021: Train batch 8000: loss: 10.97 grad norm: 48.75 time: 0.075
2026-01-04 11:45:53,021: Running test after training batch: 8000
2026-01-04 11:45:53,143: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:45:58,258: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point us will
2026-01-04 11:45:58,293: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nit
2026-01-04 11:46:00,200: Val batch 8000: PER (avg): 0.1856 CTC Loss (avg): 18.1644 WER(1gram): 54.57% (n=64) time: 7.179
2026-01-04 11:46:00,200: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-04 11:46:00,201: t15.2023.08.13 val PER: 0.1435
2026-01-04 11:46:00,201: t15.2023.08.18 val PER: 0.1232
2026-01-04 11:46:00,201: t15.2023.08.20 val PER: 0.1461
2026-01-04 11:46:00,201: t15.2023.08.25 val PER: 0.1190
2026-01-04 11:46:00,201: t15.2023.08.27 val PER: 0.2170
2026-01-04 11:46:00,201: t15.2023.09.01 val PER: 0.1015
2026-01-04 11:46:00,201: t15.2023.09.03 val PER: 0.1805
2026-01-04 11:46:00,202: t15.2023.09.24 val PER: 0.1481
2026-01-04 11:46:00,202: t15.2023.09.29 val PER: 0.1519
2026-01-04 11:46:00,202: t15.2023.10.01 val PER: 0.2021
2026-01-04 11:46:00,202: t15.2023.10.06 val PER: 0.1238
2026-01-04 11:46:00,202: t15.2023.10.08 val PER: 0.2788
2026-01-04 11:46:00,202: t15.2023.10.13 val PER: 0.2428
2026-01-04 11:46:00,203: t15.2023.10.15 val PER: 0.1918
2026-01-04 11:46:00,203: t15.2023.10.20 val PER: 0.1980
2026-01-04 11:46:00,203: t15.2023.10.22 val PER: 0.1359
2026-01-04 11:46:00,203: t15.2023.11.03 val PER: 0.2035
2026-01-04 11:46:00,203: t15.2023.11.04 val PER: 0.0341
2026-01-04 11:46:00,203: t15.2023.11.17 val PER: 0.0653
2026-01-04 11:46:00,203: t15.2023.11.19 val PER: 0.0639
2026-01-04 11:46:00,203: t15.2023.11.26 val PER: 0.1790
2026-01-04 11:46:00,203: t15.2023.12.03 val PER: 0.1565
2026-01-04 11:46:00,203: t15.2023.12.08 val PER: 0.1438
2026-01-04 11:46:00,203: t15.2023.12.10 val PER: 0.1353
2026-01-04 11:46:00,204: t15.2023.12.17 val PER: 0.1726
2026-01-04 11:46:00,204: t15.2023.12.29 val PER: 0.1798
2026-01-04 11:46:00,204: t15.2024.02.25 val PER: 0.1376
2026-01-04 11:46:00,204: t15.2024.03.08 val PER: 0.2504
2026-01-04 11:46:00,204: t15.2024.03.15 val PER: 0.2464
2026-01-04 11:46:00,204: t15.2024.03.17 val PER: 0.1750
2026-01-04 11:46:00,204: t15.2024.05.10 val PER: 0.2021
2026-01-04 11:46:00,204: t15.2024.06.14 val PER: 0.2177
2026-01-04 11:46:00,204: t15.2024.07.19 val PER: 0.2874
2026-01-04 11:46:00,204: t15.2024.07.21 val PER: 0.1207
2026-01-04 11:46:00,204: t15.2024.07.28 val PER: 0.1662
2026-01-04 11:46:00,204: t15.2025.01.10 val PER: 0.3375
2026-01-04 11:46:00,205: t15.2025.01.12 val PER: 0.1901
2026-01-04 11:46:00,205: t15.2025.03.14 val PER: 0.3624
2026-01-04 11:46:00,205: t15.2025.03.16 val PER: 0.2238
2026-01-04 11:46:00,205: t15.2025.03.30 val PER: 0.3517
2026-01-04 11:46:00,205: t15.2025.04.13 val PER: 0.2539
2026-01-04 11:46:00,484: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_8000
2026-01-04 11:46:21,006: Train batch 8200: loss: 9.93 grad norm: 48.54 time: 0.056
2026-01-04 11:46:41,530: Train batch 8400: loss: 10.08 grad norm: 46.71 time: 0.066
2026-01-04 11:46:51,974: Running test after training batch: 8500
2026-01-04 11:46:52,078: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:46:57,179: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 11:46:57,214: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost et
2026-01-04 11:46:59,149: Val batch 8500: PER (avg): 0.1826 CTC Loss (avg): 17.9460 WER(1gram): 53.05% (n=64) time: 7.174
2026-01-04 11:46:59,149: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 11:46:59,149: t15.2023.08.13 val PER: 0.1424
2026-01-04 11:46:59,149: t15.2023.08.18 val PER: 0.1324
2026-01-04 11:46:59,149: t15.2023.08.20 val PER: 0.1438
2026-01-04 11:46:59,150: t15.2023.08.25 val PER: 0.1084
2026-01-04 11:46:59,150: t15.2023.08.27 val PER: 0.2058
2026-01-04 11:46:59,150: t15.2023.09.01 val PER: 0.1039
2026-01-04 11:46:59,150: t15.2023.09.03 val PER: 0.1983
2026-01-04 11:46:59,150: t15.2023.09.24 val PER: 0.1468
2026-01-04 11:46:59,150: t15.2023.09.29 val PER: 0.1608
2026-01-04 11:46:59,150: t15.2023.10.01 val PER: 0.1995
2026-01-04 11:46:59,150: t15.2023.10.06 val PER: 0.1173
2026-01-04 11:46:59,150: t15.2023.10.08 val PER: 0.2639
2026-01-04 11:46:59,150: t15.2023.10.13 val PER: 0.2420
2026-01-04 11:46:59,151: t15.2023.10.15 val PER: 0.1892
2026-01-04 11:46:59,151: t15.2023.10.20 val PER: 0.2013
2026-01-04 11:46:59,151: t15.2023.10.22 val PER: 0.1381
2026-01-04 11:46:59,151: t15.2023.11.03 val PER: 0.1913
2026-01-04 11:46:59,151: t15.2023.11.04 val PER: 0.0512
2026-01-04 11:46:59,151: t15.2023.11.17 val PER: 0.0575
2026-01-04 11:46:59,151: t15.2023.11.19 val PER: 0.0559
2026-01-04 11:46:59,151: t15.2023.11.26 val PER: 0.1761
2026-01-04 11:46:59,151: t15.2023.12.03 val PER: 0.1513
2026-01-04 11:46:59,152: t15.2023.12.08 val PER: 0.1385
2026-01-04 11:46:59,152: t15.2023.12.10 val PER: 0.1183
2026-01-04 11:46:59,152: t15.2023.12.17 val PER: 0.1705
2026-01-04 11:46:59,152: t15.2023.12.29 val PER: 0.1716
2026-01-04 11:46:59,152: t15.2024.02.25 val PER: 0.1461
2026-01-04 11:46:59,152: t15.2024.03.08 val PER: 0.2674
2026-01-04 11:46:59,152: t15.2024.03.15 val PER: 0.2326
2026-01-04 11:46:59,152: t15.2024.03.17 val PER: 0.1827
2026-01-04 11:46:59,152: t15.2024.05.10 val PER: 0.1902
2026-01-04 11:46:59,152: t15.2024.06.14 val PER: 0.1940
2026-01-04 11:46:59,153: t15.2024.07.19 val PER: 0.2821
2026-01-04 11:46:59,153: t15.2024.07.21 val PER: 0.1179
2026-01-04 11:46:59,153: t15.2024.07.28 val PER: 0.1743
2026-01-04 11:46:59,153: t15.2025.01.10 val PER: 0.3251
2026-01-04 11:46:59,153: t15.2025.01.12 val PER: 0.1917
2026-01-04 11:46:59,153: t15.2025.03.14 val PER: 0.3536
2026-01-04 11:46:59,153: t15.2025.03.16 val PER: 0.2186
2026-01-04 11:46:59,153: t15.2025.03.30 val PER: 0.3333
2026-01-04 11:46:59,154: t15.2025.04.13 val PER: 0.2425
2026-01-04 11:46:59,434: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_8500
2026-01-04 11:47:09,705: Train batch 8600: loss: 15.73 grad norm: 59.66 time: 0.057
2026-01-04 11:47:30,165: Train batch 8800: loss: 15.12 grad norm: 56.77 time: 0.063
2026-01-04 11:47:50,945: Train batch 9000: loss: 15.89 grad norm: 61.66 time: 0.072
2026-01-04 11:47:50,945: Running test after training batch: 9000
2026-01-04 11:47:51,096: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:47:56,232: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 11:47:56,269: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-04 11:47:58,246: Val batch 9000: PER (avg): 0.1753 CTC Loss (avg): 17.3137 WER(1gram): 51.52% (n=64) time: 7.300
2026-01-04 11:47:58,246: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 11:47:58,246: t15.2023.08.13 val PER: 0.1414
2026-01-04 11:47:58,247: t15.2023.08.18 val PER: 0.1324
2026-01-04 11:47:58,247: t15.2023.08.20 val PER: 0.1382
2026-01-04 11:47:58,247: t15.2023.08.25 val PER: 0.0964
2026-01-04 11:47:58,247: t15.2023.08.27 val PER: 0.2122
2026-01-04 11:47:58,247: t15.2023.09.01 val PER: 0.0982
2026-01-04 11:47:58,247: t15.2023.09.03 val PER: 0.1841
2026-01-04 11:47:58,247: t15.2023.09.24 val PER: 0.1493
2026-01-04 11:47:58,247: t15.2023.09.29 val PER: 0.1474
2026-01-04 11:47:58,247: t15.2023.10.01 val PER: 0.1935
2026-01-04 11:47:58,248: t15.2023.10.06 val PER: 0.0990
2026-01-04 11:47:58,248: t15.2023.10.08 val PER: 0.2598
2026-01-04 11:47:58,248: t15.2023.10.13 val PER: 0.2296
2026-01-04 11:47:58,248: t15.2023.10.15 val PER: 0.1833
2026-01-04 11:47:58,248: t15.2023.10.20 val PER: 0.2114
2026-01-04 11:47:58,248: t15.2023.10.22 val PER: 0.1303
2026-01-04 11:47:58,248: t15.2023.11.03 val PER: 0.1988
2026-01-04 11:47:58,249: t15.2023.11.04 val PER: 0.0375
2026-01-04 11:47:58,249: t15.2023.11.17 val PER: 0.0482
2026-01-04 11:47:58,249: t15.2023.11.19 val PER: 0.0579
2026-01-04 11:47:58,249: t15.2023.11.26 val PER: 0.1667
2026-01-04 11:47:58,249: t15.2023.12.03 val PER: 0.1345
2026-01-04 11:47:58,249: t15.2023.12.08 val PER: 0.1292
2026-01-04 11:47:58,249: t15.2023.12.10 val PER: 0.1130
2026-01-04 11:47:58,250: t15.2023.12.17 val PER: 0.1590
2026-01-04 11:47:58,250: t15.2023.12.29 val PER: 0.1675
2026-01-04 11:47:58,250: t15.2024.02.25 val PER: 0.1362
2026-01-04 11:47:58,250: t15.2024.03.08 val PER: 0.2532
2026-01-04 11:47:58,250: t15.2024.03.15 val PER: 0.2301
2026-01-04 11:47:58,250: t15.2024.03.17 val PER: 0.1688
2026-01-04 11:47:58,250: t15.2024.05.10 val PER: 0.1753
2026-01-04 11:47:58,250: t15.2024.06.14 val PER: 0.1814
2026-01-04 11:47:58,251: t15.2024.07.19 val PER: 0.2591
2026-01-04 11:47:58,251: t15.2024.07.21 val PER: 0.1172
2026-01-04 11:47:58,251: t15.2024.07.28 val PER: 0.1551
2026-01-04 11:47:58,251: t15.2025.01.10 val PER: 0.3209
2026-01-04 11:47:58,251: t15.2025.01.12 val PER: 0.1824
2026-01-04 11:47:58,251: t15.2025.03.14 val PER: 0.3609
2026-01-04 11:47:58,251: t15.2025.03.16 val PER: 0.2134
2026-01-04 11:47:58,251: t15.2025.03.30 val PER: 0.3276
2026-01-04 11:47:58,252: t15.2025.04.13 val PER: 0.2525
2026-01-04 11:47:58,252: New best val WER(1gram) 53.05% --> 51.52%
2026-01-04 11:47:58,252: Checkpointing model
2026-01-04 11:47:58,901: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/best_checkpoint
2026-01-04 11:47:59,190: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_9000
2026-01-04 11:48:19,467: Train batch 9200: loss: 10.82 grad norm: 50.61 time: 0.059
2026-01-04 11:48:39,753: Train batch 9400: loss: 7.88 grad norm: 45.46 time: 0.068
2026-01-04 11:48:49,962: Running test after training batch: 9500
2026-01-04 11:48:50,144: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:48:55,448: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 11:48:55,485: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-04 11:48:57,462: Val batch 9500: PER (avg): 0.1745 CTC Loss (avg): 17.2086 WER(1gram): 48.48% (n=64) time: 7.499
2026-01-04 11:48:57,462: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 11:48:57,463: t15.2023.08.13 val PER: 0.1279
2026-01-04 11:48:57,463: t15.2023.08.18 val PER: 0.1174
2026-01-04 11:48:57,463: t15.2023.08.20 val PER: 0.1430
2026-01-04 11:48:57,463: t15.2023.08.25 val PER: 0.0964
2026-01-04 11:48:57,463: t15.2023.08.27 val PER: 0.2074
2026-01-04 11:48:57,463: t15.2023.09.01 val PER: 0.0942
2026-01-04 11:48:57,463: t15.2023.09.03 val PER: 0.1758
2026-01-04 11:48:57,463: t15.2023.09.24 val PER: 0.1420
2026-01-04 11:48:57,463: t15.2023.09.29 val PER: 0.1436
2026-01-04 11:48:57,463: t15.2023.10.01 val PER: 0.1889
2026-01-04 11:48:57,463: t15.2023.10.06 val PER: 0.1152
2026-01-04 11:48:57,463: t15.2023.10.08 val PER: 0.2652
2026-01-04 11:48:57,464: t15.2023.10.13 val PER: 0.2296
2026-01-04 11:48:57,464: t15.2023.10.15 val PER: 0.1813
2026-01-04 11:48:57,464: t15.2023.10.20 val PER: 0.2081
2026-01-04 11:48:57,464: t15.2023.10.22 val PER: 0.1247
2026-01-04 11:48:57,464: t15.2023.11.03 val PER: 0.1900
2026-01-04 11:48:57,464: t15.2023.11.04 val PER: 0.0341
2026-01-04 11:48:57,464: t15.2023.11.17 val PER: 0.0513
2026-01-04 11:48:57,464: t15.2023.11.19 val PER: 0.0459
2026-01-04 11:48:57,465: t15.2023.11.26 val PER: 0.1616
2026-01-04 11:48:57,465: t15.2023.12.03 val PER: 0.1418
2026-01-04 11:48:57,465: t15.2023.12.08 val PER: 0.1438
2026-01-04 11:48:57,465: t15.2023.12.10 val PER: 0.1143
2026-01-04 11:48:57,465: t15.2023.12.17 val PER: 0.1559
2026-01-04 11:48:57,465: t15.2023.12.29 val PER: 0.1544
2026-01-04 11:48:57,465: t15.2024.02.25 val PER: 0.1362
2026-01-04 11:48:57,465: t15.2024.03.08 val PER: 0.2674
2026-01-04 11:48:57,465: t15.2024.03.15 val PER: 0.2251
2026-01-04 11:48:57,465: t15.2024.03.17 val PER: 0.1674
2026-01-04 11:48:57,465: t15.2024.05.10 val PER: 0.1783
2026-01-04 11:48:57,465: t15.2024.06.14 val PER: 0.1861
2026-01-04 11:48:57,465: t15.2024.07.19 val PER: 0.2716
2026-01-04 11:48:57,465: t15.2024.07.21 val PER: 0.1228
2026-01-04 11:48:57,465: t15.2024.07.28 val PER: 0.1596
2026-01-04 11:48:57,466: t15.2025.01.10 val PER: 0.3347
2026-01-04 11:48:57,466: t15.2025.01.12 val PER: 0.1778
2026-01-04 11:48:57,466: t15.2025.03.14 val PER: 0.3757
2026-01-04 11:48:57,466: t15.2025.03.16 val PER: 0.2120
2026-01-04 11:48:57,466: t15.2025.03.30 val PER: 0.3207
2026-01-04 11:48:57,466: t15.2025.04.13 val PER: 0.2340
2026-01-04 11:48:57,467: New best val WER(1gram) 51.52% --> 48.48%
2026-01-04 11:48:57,467: Checkpointing model
2026-01-04 11:48:58,097: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/best_checkpoint
2026-01-04 11:48:58,386: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_9500
2026-01-04 11:49:08,547: Train batch 9600: loss: 8.72 grad norm: 47.79 time: 0.074
2026-01-04 11:49:28,990: Train batch 9800: loss: 12.69 grad norm: 56.42 time: 0.066
2026-01-04 11:49:49,647: Train batch 10000: loss: 5.66 grad norm: 38.73 time: 0.063
2026-01-04 11:49:49,647: Running test after training batch: 10000
2026-01-04 11:49:49,795: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:49:55,273: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 11:49:55,308: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-04 11:49:57,283: Val batch 10000: PER (avg): 0.1698 CTC Loss (avg): 16.8640 WER(1gram): 50.76% (n=64) time: 7.635
2026-01-04 11:49:57,284: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 11:49:57,284: t15.2023.08.13 val PER: 0.1258
2026-01-04 11:49:57,284: t15.2023.08.18 val PER: 0.1266
2026-01-04 11:49:57,284: t15.2023.08.20 val PER: 0.1271
2026-01-04 11:49:57,284: t15.2023.08.25 val PER: 0.0979
2026-01-04 11:49:57,284: t15.2023.08.27 val PER: 0.2042
2026-01-04 11:49:57,285: t15.2023.09.01 val PER: 0.0966
2026-01-04 11:49:57,285: t15.2023.09.03 val PER: 0.1805
2026-01-04 11:49:57,285: t15.2023.09.24 val PER: 0.1420
2026-01-04 11:49:57,285: t15.2023.09.29 val PER: 0.1468
2026-01-04 11:49:57,286: t15.2023.10.01 val PER: 0.1816
2026-01-04 11:49:57,286: t15.2023.10.06 val PER: 0.1023
2026-01-04 11:49:57,286: t15.2023.10.08 val PER: 0.2503
2026-01-04 11:49:57,286: t15.2023.10.13 val PER: 0.2250
2026-01-04 11:49:57,286: t15.2023.10.15 val PER: 0.1734
2026-01-04 11:49:57,286: t15.2023.10.20 val PER: 0.1980
2026-01-04 11:49:57,287: t15.2023.10.22 val PER: 0.1370
2026-01-04 11:49:57,287: t15.2023.11.03 val PER: 0.1886
2026-01-04 11:49:57,287: t15.2023.11.04 val PER: 0.0375
2026-01-04 11:49:57,287: t15.2023.11.17 val PER: 0.0467
2026-01-04 11:49:57,287: t15.2023.11.19 val PER: 0.0499
2026-01-04 11:49:57,287: t15.2023.11.26 val PER: 0.1428
2026-01-04 11:49:57,287: t15.2023.12.03 val PER: 0.1418
2026-01-04 11:49:57,287: t15.2023.12.08 val PER: 0.1352
2026-01-04 11:49:57,287: t15.2023.12.10 val PER: 0.1196
2026-01-04 11:49:57,288: t15.2023.12.17 val PER: 0.1507
2026-01-04 11:49:57,288: t15.2023.12.29 val PER: 0.1489
2026-01-04 11:49:57,288: t15.2024.02.25 val PER: 0.1404
2026-01-04 11:49:57,288: t15.2024.03.08 val PER: 0.2475
2026-01-04 11:49:57,288: t15.2024.03.15 val PER: 0.2226
2026-01-04 11:49:57,288: t15.2024.03.17 val PER: 0.1660
2026-01-04 11:49:57,288: t15.2024.05.10 val PER: 0.1724
2026-01-04 11:49:57,288: t15.2024.06.14 val PER: 0.1782
2026-01-04 11:49:57,289: t15.2024.07.19 val PER: 0.2630
2026-01-04 11:49:57,289: t15.2024.07.21 val PER: 0.1097
2026-01-04 11:49:57,289: t15.2024.07.28 val PER: 0.1581
2026-01-04 11:49:57,289: t15.2025.01.10 val PER: 0.3182
2026-01-04 11:49:57,289: t15.2025.01.12 val PER: 0.1694
2026-01-04 11:49:57,289: t15.2025.03.14 val PER: 0.3536
2026-01-04 11:49:57,289: t15.2025.03.16 val PER: 0.2225
2026-01-04 11:49:57,289: t15.2025.03.30 val PER: 0.3138
2026-01-04 11:49:57,289: t15.2025.04.13 val PER: 0.2325
2026-01-04 11:49:57,564: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_10000
2026-01-04 11:50:17,868: Train batch 10200: loss: 6.51 grad norm: 40.74 time: 0.052
2026-01-04 11:50:38,650: Train batch 10400: loss: 8.90 grad norm: 52.89 time: 0.073
2026-01-04 11:50:48,866: Running test after training batch: 10500
2026-01-04 11:50:49,029: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:50:54,317: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 11:50:54,353: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-04 11:50:56,349: Val batch 10500: PER (avg): 0.1673 CTC Loss (avg): 16.7630 WER(1gram): 49.75% (n=64) time: 7.482
2026-01-04 11:50:56,349: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 11:50:56,349: t15.2023.08.13 val PER: 0.1268
2026-01-04 11:50:56,350: t15.2023.08.18 val PER: 0.1282
2026-01-04 11:50:56,350: t15.2023.08.20 val PER: 0.1263
2026-01-04 11:50:56,350: t15.2023.08.25 val PER: 0.1069
2026-01-04 11:50:56,350: t15.2023.08.27 val PER: 0.2106
2026-01-04 11:50:56,350: t15.2023.09.01 val PER: 0.0942
2026-01-04 11:50:56,350: t15.2023.09.03 val PER: 0.1876
2026-01-04 11:50:56,350: t15.2023.09.24 val PER: 0.1408
2026-01-04 11:50:56,350: t15.2023.09.29 val PER: 0.1519
2026-01-04 11:50:56,350: t15.2023.10.01 val PER: 0.1909
2026-01-04 11:50:56,350: t15.2023.10.06 val PER: 0.0915
2026-01-04 11:50:56,350: t15.2023.10.08 val PER: 0.2517
2026-01-04 11:50:56,351: t15.2023.10.13 val PER: 0.2157
2026-01-04 11:50:56,351: t15.2023.10.15 val PER: 0.1701
2026-01-04 11:50:56,351: t15.2023.10.20 val PER: 0.1879
2026-01-04 11:50:56,351: t15.2023.10.22 val PER: 0.1192
2026-01-04 11:50:56,351: t15.2023.11.03 val PER: 0.1879
2026-01-04 11:50:56,351: t15.2023.11.04 val PER: 0.0341
2026-01-04 11:50:56,351: t15.2023.11.17 val PER: 0.0560
2026-01-04 11:50:56,351: t15.2023.11.19 val PER: 0.0579
2026-01-04 11:50:56,352: t15.2023.11.26 val PER: 0.1384
2026-01-04 11:50:56,353: t15.2023.12.03 val PER: 0.1292
2026-01-04 11:50:56,353: t15.2023.12.08 val PER: 0.1198
2026-01-04 11:50:56,353: t15.2023.12.10 val PER: 0.1143
2026-01-04 11:50:56,353: t15.2023.12.17 val PER: 0.1570
2026-01-04 11:50:56,353: t15.2023.12.29 val PER: 0.1551
2026-01-04 11:50:56,354: t15.2024.02.25 val PER: 0.1250
2026-01-04 11:50:56,354: t15.2024.03.08 val PER: 0.2432
2026-01-04 11:50:56,354: t15.2024.03.15 val PER: 0.2151
2026-01-04 11:50:56,354: t15.2024.03.17 val PER: 0.1660
2026-01-04 11:50:56,354: t15.2024.05.10 val PER: 0.1679
2026-01-04 11:50:56,354: t15.2024.06.14 val PER: 0.1798
2026-01-04 11:50:56,354: t15.2024.07.19 val PER: 0.2604
2026-01-04 11:50:56,354: t15.2024.07.21 val PER: 0.1021
2026-01-04 11:50:56,355: t15.2024.07.28 val PER: 0.1412
2026-01-04 11:50:56,355: t15.2025.01.10 val PER: 0.3196
2026-01-04 11:50:56,355: t15.2025.01.12 val PER: 0.1655
2026-01-04 11:50:56,355: t15.2025.03.14 val PER: 0.3683
2026-01-04 11:50:56,355: t15.2025.03.16 val PER: 0.2068
2026-01-04 11:50:56,355: t15.2025.03.30 val PER: 0.3138
2026-01-04 11:50:56,355: t15.2025.04.13 val PER: 0.2282
2026-01-04 11:50:56,629: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_10500
2026-01-04 11:51:07,074: Train batch 10600: loss: 9.14 grad norm: 54.99 time: 0.073
2026-01-04 11:51:27,493: Train batch 10800: loss: 15.01 grad norm: 68.25 time: 0.066
2026-01-04 11:51:48,237: Train batch 11000: loss: 14.92 grad norm: 63.63 time: 0.057
2026-01-04 11:51:48,238: Running test after training batch: 11000
2026-01-04 11:51:48,353: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:51:53,511: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 11:51:53,548: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 11:51:55,563: Val batch 11000: PER (avg): 0.1637 CTC Loss (avg): 16.4693 WER(1gram): 46.95% (n=64) time: 7.325
2026-01-04 11:51:55,563: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 11:51:55,563: t15.2023.08.13 val PER: 0.1227
2026-01-04 11:51:55,564: t15.2023.08.18 val PER: 0.1148
2026-01-04 11:51:55,564: t15.2023.08.20 val PER: 0.1239
2026-01-04 11:51:55,564: t15.2023.08.25 val PER: 0.0934
2026-01-04 11:51:55,564: t15.2023.08.27 val PER: 0.1865
2026-01-04 11:51:55,564: t15.2023.09.01 val PER: 0.0885
2026-01-04 11:51:55,564: t15.2023.09.03 val PER: 0.1793
2026-01-04 11:51:55,564: t15.2023.09.24 val PER: 0.1420
2026-01-04 11:51:55,564: t15.2023.09.29 val PER: 0.1423
2026-01-04 11:51:55,564: t15.2023.10.01 val PER: 0.1915
2026-01-04 11:51:55,565: t15.2023.10.06 val PER: 0.0861
2026-01-04 11:51:55,565: t15.2023.10.08 val PER: 0.2571
2026-01-04 11:51:55,565: t15.2023.10.13 val PER: 0.2141
2026-01-04 11:51:55,565: t15.2023.10.15 val PER: 0.1721
2026-01-04 11:51:55,565: t15.2023.10.20 val PER: 0.1946
2026-01-04 11:51:55,565: t15.2023.10.22 val PER: 0.1258
2026-01-04 11:51:55,565: t15.2023.11.03 val PER: 0.1947
2026-01-04 11:51:55,565: t15.2023.11.04 val PER: 0.0341
2026-01-04 11:51:55,565: t15.2023.11.17 val PER: 0.0544
2026-01-04 11:51:55,565: t15.2023.11.19 val PER: 0.0419
2026-01-04 11:51:55,565: t15.2023.11.26 val PER: 0.1435
2026-01-04 11:51:55,566: t15.2023.12.03 val PER: 0.1208
2026-01-04 11:51:55,566: t15.2023.12.08 val PER: 0.1205
2026-01-04 11:51:55,566: t15.2023.12.10 val PER: 0.0972
2026-01-04 11:51:55,566: t15.2023.12.17 val PER: 0.1497
2026-01-04 11:51:55,566: t15.2023.12.29 val PER: 0.1359
2026-01-04 11:51:55,566: t15.2024.02.25 val PER: 0.1362
2026-01-04 11:51:55,566: t15.2024.03.08 val PER: 0.2432
2026-01-04 11:51:55,566: t15.2024.03.15 val PER: 0.2208
2026-01-04 11:51:55,566: t15.2024.03.17 val PER: 0.1562
2026-01-04 11:51:55,566: t15.2024.05.10 val PER: 0.1738
2026-01-04 11:51:55,566: t15.2024.06.14 val PER: 0.1625
2026-01-04 11:51:55,566: t15.2024.07.19 val PER: 0.2538
2026-01-04 11:51:55,566: t15.2024.07.21 val PER: 0.1000
2026-01-04 11:51:55,566: t15.2024.07.28 val PER: 0.1500
2026-01-04 11:51:55,566: t15.2025.01.10 val PER: 0.3003
2026-01-04 11:51:55,566: t15.2025.01.12 val PER: 0.1624
2026-01-04 11:51:55,567: t15.2025.03.14 val PER: 0.3624
2026-01-04 11:51:55,567: t15.2025.03.16 val PER: 0.1990
2026-01-04 11:51:55,567: t15.2025.03.30 val PER: 0.3057
2026-01-04 11:51:55,567: t15.2025.04.13 val PER: 0.2340
2026-01-04 11:51:55,568: New best val WER(1gram) 48.48% --> 46.95%
2026-01-04 11:51:55,568: Checkpointing model
2026-01-04 11:51:56,199: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/best_checkpoint
2026-01-04 11:51:56,489: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_11000
2026-01-04 11:52:16,748: Train batch 11200: loss: 10.56 grad norm: 53.29 time: 0.071
2026-01-04 11:52:37,005: Train batch 11400: loss: 10.26 grad norm: 56.96 time: 0.059
2026-01-04 11:52:47,426: Running test after training batch: 11500
2026-01-04 11:52:47,553: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:52:52,643: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 11:52:52,682: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-04 11:52:54,724: Val batch 11500: PER (avg): 0.1629 CTC Loss (avg): 16.4128 WER(1gram): 49.49% (n=64) time: 7.297
2026-01-04 11:52:54,724: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 11:52:54,724: t15.2023.08.13 val PER: 0.1206
2026-01-04 11:52:54,724: t15.2023.08.18 val PER: 0.1148
2026-01-04 11:52:54,724: t15.2023.08.20 val PER: 0.1223
2026-01-04 11:52:54,724: t15.2023.08.25 val PER: 0.1024
2026-01-04 11:52:54,725: t15.2023.08.27 val PER: 0.2074
2026-01-04 11:52:54,725: t15.2023.09.01 val PER: 0.0885
2026-01-04 11:52:54,725: t15.2023.09.03 val PER: 0.1817
2026-01-04 11:52:54,725: t15.2023.09.24 val PER: 0.1274
2026-01-04 11:52:54,725: t15.2023.09.29 val PER: 0.1398
2026-01-04 11:52:54,725: t15.2023.10.01 val PER: 0.1810
2026-01-04 11:52:54,725: t15.2023.10.06 val PER: 0.0850
2026-01-04 11:52:54,725: t15.2023.10.08 val PER: 0.2625
2026-01-04 11:52:54,725: t15.2023.10.13 val PER: 0.2219
2026-01-04 11:52:54,725: t15.2023.10.15 val PER: 0.1668
2026-01-04 11:52:54,725: t15.2023.10.20 val PER: 0.2081
2026-01-04 11:52:54,726: t15.2023.10.22 val PER: 0.1158
2026-01-04 11:52:54,726: t15.2023.11.03 val PER: 0.1866
2026-01-04 11:52:54,726: t15.2023.11.04 val PER: 0.0273
2026-01-04 11:52:54,726: t15.2023.11.17 val PER: 0.0467
2026-01-04 11:52:54,726: t15.2023.11.19 val PER: 0.0459
2026-01-04 11:52:54,726: t15.2023.11.26 val PER: 0.1348
2026-01-04 11:52:54,726: t15.2023.12.03 val PER: 0.1282
2026-01-04 11:52:54,726: t15.2023.12.08 val PER: 0.1138
2026-01-04 11:52:54,726: t15.2023.12.10 val PER: 0.1117
2026-01-04 11:52:54,727: t15.2023.12.17 val PER: 0.1507
2026-01-04 11:52:54,727: t15.2023.12.29 val PER: 0.1414
2026-01-04 11:52:54,727: t15.2024.02.25 val PER: 0.1166
2026-01-04 11:52:54,727: t15.2024.03.08 val PER: 0.2347
2026-01-04 11:52:54,727: t15.2024.03.15 val PER: 0.2120
2026-01-04 11:52:54,727: t15.2024.03.17 val PER: 0.1569
2026-01-04 11:52:54,727: t15.2024.05.10 val PER: 0.1753
2026-01-04 11:52:54,727: t15.2024.06.14 val PER: 0.1782
2026-01-04 11:52:54,727: t15.2024.07.19 val PER: 0.2472
2026-01-04 11:52:54,727: t15.2024.07.21 val PER: 0.1048
2026-01-04 11:52:54,727: t15.2024.07.28 val PER: 0.1471
2026-01-04 11:52:54,727: t15.2025.01.10 val PER: 0.3113
2026-01-04 11:52:54,727: t15.2025.01.12 val PER: 0.1701
2026-01-04 11:52:54,727: t15.2025.03.14 val PER: 0.3536
2026-01-04 11:52:54,728: t15.2025.03.16 val PER: 0.2029
2026-01-04 11:52:54,728: t15.2025.03.30 val PER: 0.3126
2026-01-04 11:52:54,728: t15.2025.04.13 val PER: 0.2354
2026-01-04 11:52:55,002: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_11500
2026-01-04 11:53:04,921: Train batch 11600: loss: 10.68 grad norm: 44.95 time: 0.063
2026-01-04 11:53:25,398: Train batch 11800: loss: 6.91 grad norm: 42.39 time: 0.045
2026-01-04 11:53:45,800: Train batch 12000: loss: 13.87 grad norm: 51.85 time: 0.072
2026-01-04 11:53:45,800: Running test after training batch: 12000
2026-01-04 11:53:45,912: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:53:51,019: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 11:53:51,056: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sindt
2026-01-04 11:53:53,090: Val batch 12000: PER (avg): 0.1602 CTC Loss (avg): 16.1641 WER(1gram): 51.78% (n=64) time: 7.289
2026-01-04 11:53:53,090: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 11:53:53,090: t15.2023.08.13 val PER: 0.1185
2026-01-04 11:53:53,090: t15.2023.08.18 val PER: 0.1148
2026-01-04 11:53:53,091: t15.2023.08.20 val PER: 0.1144
2026-01-04 11:53:53,091: t15.2023.08.25 val PER: 0.0919
2026-01-04 11:53:53,091: t15.2023.08.27 val PER: 0.2042
2026-01-04 11:53:53,091: t15.2023.09.01 val PER: 0.0852
2026-01-04 11:53:53,091: t15.2023.09.03 val PER: 0.1627
2026-01-04 11:53:53,091: t15.2023.09.24 val PER: 0.1286
2026-01-04 11:53:53,091: t15.2023.09.29 val PER: 0.1391
2026-01-04 11:53:53,091: t15.2023.10.01 val PER: 0.1764
2026-01-04 11:53:53,091: t15.2023.10.06 val PER: 0.0947
2026-01-04 11:53:53,091: t15.2023.10.08 val PER: 0.2530
2026-01-04 11:53:53,091: t15.2023.10.13 val PER: 0.2188
2026-01-04 11:53:53,091: t15.2023.10.15 val PER: 0.1516
2026-01-04 11:53:53,091: t15.2023.10.20 val PER: 0.1946
2026-01-04 11:53:53,092: t15.2023.10.22 val PER: 0.1180
2026-01-04 11:53:53,092: t15.2023.11.03 val PER: 0.1845
2026-01-04 11:53:53,092: t15.2023.11.04 val PER: 0.0341
2026-01-04 11:53:53,093: t15.2023.11.17 val PER: 0.0435
2026-01-04 11:53:53,093: t15.2023.11.19 val PER: 0.0419
2026-01-04 11:53:53,094: t15.2023.11.26 val PER: 0.1312
2026-01-04 11:53:53,094: t15.2023.12.03 val PER: 0.1261
2026-01-04 11:53:53,094: t15.2023.12.08 val PER: 0.1185
2026-01-04 11:53:53,094: t15.2023.12.10 val PER: 0.1104
2026-01-04 11:53:53,094: t15.2023.12.17 val PER: 0.1528
2026-01-04 11:53:53,094: t15.2023.12.29 val PER: 0.1318
2026-01-04 11:53:53,094: t15.2024.02.25 val PER: 0.1166
2026-01-04 11:53:53,094: t15.2024.03.08 val PER: 0.2432
2026-01-04 11:53:53,094: t15.2024.03.15 val PER: 0.2145
2026-01-04 11:53:53,095: t15.2024.03.17 val PER: 0.1513
2026-01-04 11:53:53,095: t15.2024.05.10 val PER: 0.1798
2026-01-04 11:53:53,095: t15.2024.06.14 val PER: 0.1893
2026-01-04 11:53:53,095: t15.2024.07.19 val PER: 0.2525
2026-01-04 11:53:53,095: t15.2024.07.21 val PER: 0.1062
2026-01-04 11:53:53,095: t15.2024.07.28 val PER: 0.1412
2026-01-04 11:53:53,095: t15.2025.01.10 val PER: 0.3182
2026-01-04 11:53:53,095: t15.2025.01.12 val PER: 0.1555
2026-01-04 11:53:53,096: t15.2025.03.14 val PER: 0.3565
2026-01-04 11:53:53,096: t15.2025.03.16 val PER: 0.2120
2026-01-04 11:53:53,096: t15.2025.03.30 val PER: 0.2943
2026-01-04 11:53:53,096: t15.2025.04.13 val PER: 0.2183
2026-01-04 11:53:53,368: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_12000
2026-01-04 11:54:13,966: Train batch 12200: loss: 5.82 grad norm: 42.73 time: 0.071
2026-01-04 11:54:34,202: Train batch 12400: loss: 4.79 grad norm: 36.77 time: 0.043
2026-01-04 11:54:44,646: Running test after training batch: 12500
2026-01-04 11:54:44,814: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:54:49,906: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 11:54:49,943: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-04 11:54:51,966: Val batch 12500: PER (avg): 0.1568 CTC Loss (avg): 15.9057 WER(1gram): 48.22% (n=64) time: 7.319
2026-01-04 11:54:51,966: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 11:54:51,966: t15.2023.08.13 val PER: 0.1195
2026-01-04 11:54:51,967: t15.2023.08.18 val PER: 0.1090
2026-01-04 11:54:51,967: t15.2023.08.20 val PER: 0.1136
2026-01-04 11:54:51,967: t15.2023.08.25 val PER: 0.0813
2026-01-04 11:54:51,967: t15.2023.08.27 val PER: 0.2042
2026-01-04 11:54:51,967: t15.2023.09.01 val PER: 0.0804
2026-01-04 11:54:51,967: t15.2023.09.03 val PER: 0.1710
2026-01-04 11:54:51,967: t15.2023.09.24 val PER: 0.1286
2026-01-04 11:54:51,968: t15.2023.09.29 val PER: 0.1372
2026-01-04 11:54:51,968: t15.2023.10.01 val PER: 0.1737
2026-01-04 11:54:51,968: t15.2023.10.06 val PER: 0.0786
2026-01-04 11:54:51,968: t15.2023.10.08 val PER: 0.2558
2026-01-04 11:54:51,968: t15.2023.10.13 val PER: 0.2087
2026-01-04 11:54:51,968: t15.2023.10.15 val PER: 0.1556
2026-01-04 11:54:51,968: t15.2023.10.20 val PER: 0.2047
2026-01-04 11:54:51,968: t15.2023.10.22 val PER: 0.1125
2026-01-04 11:54:51,968: t15.2023.11.03 val PER: 0.1818
2026-01-04 11:54:51,968: t15.2023.11.04 val PER: 0.0341
2026-01-04 11:54:51,969: t15.2023.11.17 val PER: 0.0513
2026-01-04 11:54:51,969: t15.2023.11.19 val PER: 0.0359
2026-01-04 11:54:51,969: t15.2023.11.26 val PER: 0.1254
2026-01-04 11:54:51,969: t15.2023.12.03 val PER: 0.1187
2026-01-04 11:54:51,969: t15.2023.12.08 val PER: 0.1112
2026-01-04 11:54:51,969: t15.2023.12.10 val PER: 0.0972
2026-01-04 11:54:51,969: t15.2023.12.17 val PER: 0.1466
2026-01-04 11:54:51,969: t15.2023.12.29 val PER: 0.1393
2026-01-04 11:54:51,969: t15.2024.02.25 val PER: 0.1138
2026-01-04 11:54:51,969: t15.2024.03.08 val PER: 0.2233
2026-01-04 11:54:51,970: t15.2024.03.15 val PER: 0.2101
2026-01-04 11:54:51,970: t15.2024.03.17 val PER: 0.1513
2026-01-04 11:54:51,970: t15.2024.05.10 val PER: 0.1724
2026-01-04 11:54:51,970: t15.2024.06.14 val PER: 0.1814
2026-01-04 11:54:51,970: t15.2024.07.19 val PER: 0.2452
2026-01-04 11:54:51,970: t15.2024.07.21 val PER: 0.1000
2026-01-04 11:54:51,970: t15.2024.07.28 val PER: 0.1360
2026-01-04 11:54:51,970: t15.2025.01.10 val PER: 0.3154
2026-01-04 11:54:51,970: t15.2025.01.12 val PER: 0.1524
2026-01-04 11:54:51,970: t15.2025.03.14 val PER: 0.3476
2026-01-04 11:54:51,971: t15.2025.03.16 val PER: 0.2016
2026-01-04 11:54:51,971: t15.2025.03.30 val PER: 0.3092
2026-01-04 11:54:51,971: t15.2025.04.13 val PER: 0.2225
2026-01-04 11:54:52,243: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_12500
2026-01-04 11:55:02,557: Train batch 12600: loss: 8.09 grad norm: 45.79 time: 0.061
2026-01-04 11:55:23,476: Train batch 12800: loss: 5.98 grad norm: 37.85 time: 0.055
2026-01-04 11:55:44,279: Train batch 13000: loss: 6.67 grad norm: 41.39 time: 0.070
2026-01-04 11:55:44,279: Running test after training batch: 13000
2026-01-04 11:55:44,421: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:55:49,518: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-04 11:55:49,556: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost tint
2026-01-04 11:55:51,609: Val batch 13000: PER (avg): 0.1566 CTC Loss (avg): 15.8356 WER(1gram): 44.92% (n=64) time: 7.329
2026-01-04 11:55:51,609: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 11:55:51,609: t15.2023.08.13 val PER: 0.1154
2026-01-04 11:55:51,610: t15.2023.08.18 val PER: 0.1174
2026-01-04 11:55:51,610: t15.2023.08.20 val PER: 0.1088
2026-01-04 11:55:51,610: t15.2023.08.25 val PER: 0.0934
2026-01-04 11:55:51,610: t15.2023.08.27 val PER: 0.1913
2026-01-04 11:55:51,610: t15.2023.09.01 val PER: 0.0828
2026-01-04 11:55:51,610: t15.2023.09.03 val PER: 0.1675
2026-01-04 11:55:51,610: t15.2023.09.24 val PER: 0.1262
2026-01-04 11:55:51,610: t15.2023.09.29 val PER: 0.1302
2026-01-04 11:55:51,610: t15.2023.10.01 val PER: 0.1790
2026-01-04 11:55:51,610: t15.2023.10.06 val PER: 0.0883
2026-01-04 11:55:51,610: t15.2023.10.08 val PER: 0.2517
2026-01-04 11:55:51,610: t15.2023.10.13 val PER: 0.2033
2026-01-04 11:55:51,610: t15.2023.10.15 val PER: 0.1529
2026-01-04 11:55:51,610: t15.2023.10.20 val PER: 0.1946
2026-01-04 11:55:51,611: t15.2023.10.22 val PER: 0.1180
2026-01-04 11:55:51,611: t15.2023.11.03 val PER: 0.1818
2026-01-04 11:55:51,611: t15.2023.11.04 val PER: 0.0375
2026-01-04 11:55:51,611: t15.2023.11.17 val PER: 0.0404
2026-01-04 11:55:51,611: t15.2023.11.19 val PER: 0.0499
2026-01-04 11:55:51,611: t15.2023.11.26 val PER: 0.1217
2026-01-04 11:55:51,611: t15.2023.12.03 val PER: 0.1187
2026-01-04 11:55:51,611: t15.2023.12.08 val PER: 0.1152
2026-01-04 11:55:51,611: t15.2023.12.10 val PER: 0.1038
2026-01-04 11:55:51,612: t15.2023.12.17 val PER: 0.1466
2026-01-04 11:55:51,612: t15.2023.12.29 val PER: 0.1434
2026-01-04 11:55:51,612: t15.2024.02.25 val PER: 0.1208
2026-01-04 11:55:51,612: t15.2024.03.08 val PER: 0.2432
2026-01-04 11:55:51,612: t15.2024.03.15 val PER: 0.2076
2026-01-04 11:55:51,612: t15.2024.03.17 val PER: 0.1471
2026-01-04 11:55:51,612: t15.2024.05.10 val PER: 0.1679
2026-01-04 11:55:51,612: t15.2024.06.14 val PER: 0.1703
2026-01-04 11:55:51,612: t15.2024.07.19 val PER: 0.2525
2026-01-04 11:55:51,612: t15.2024.07.21 val PER: 0.0952
2026-01-04 11:55:51,612: t15.2024.07.28 val PER: 0.1404
2026-01-04 11:55:51,612: t15.2025.01.10 val PER: 0.3030
2026-01-04 11:55:51,612: t15.2025.01.12 val PER: 0.1478
2026-01-04 11:55:51,613: t15.2025.03.14 val PER: 0.3447
2026-01-04 11:55:51,613: t15.2025.03.16 val PER: 0.1832
2026-01-04 11:55:51,613: t15.2025.03.30 val PER: 0.3149
2026-01-04 11:55:51,613: t15.2025.04.13 val PER: 0.2297
2026-01-04 11:55:51,615: New best val WER(1gram) 46.95% --> 44.92%
2026-01-04 11:55:51,615: Checkpointing model
2026-01-04 11:55:52,259: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/best_checkpoint
2026-01-04 11:55:52,546: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_13000
2026-01-04 11:56:12,880: Train batch 13200: loss: 12.73 grad norm: 58.59 time: 0.056
2026-01-04 11:56:33,401: Train batch 13400: loss: 8.67 grad norm: 50.66 time: 0.065
2026-01-04 11:56:43,751: Running test after training batch: 13500
2026-01-04 11:56:43,905: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:56:48,994: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 11:56:49,031: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost it
2026-01-04 11:56:51,112: Val batch 13500: PER (avg): 0.1551 CTC Loss (avg): 15.5782 WER(1gram): 48.48% (n=64) time: 7.361
2026-01-04 11:56:51,112: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 11:56:51,113: t15.2023.08.13 val PER: 0.1185
2026-01-04 11:56:51,113: t15.2023.08.18 val PER: 0.1098
2026-01-04 11:56:51,113: t15.2023.08.20 val PER: 0.1152
2026-01-04 11:56:51,113: t15.2023.08.25 val PER: 0.0858
2026-01-04 11:56:51,113: t15.2023.08.27 val PER: 0.1961
2026-01-04 11:56:51,113: t15.2023.09.01 val PER: 0.0836
2026-01-04 11:56:51,113: t15.2023.09.03 val PER: 0.1722
2026-01-04 11:56:51,113: t15.2023.09.24 val PER: 0.1250
2026-01-04 11:56:51,113: t15.2023.09.29 val PER: 0.1315
2026-01-04 11:56:51,113: t15.2023.10.01 val PER: 0.1764
2026-01-04 11:56:51,113: t15.2023.10.06 val PER: 0.0883
2026-01-04 11:56:51,114: t15.2023.10.08 val PER: 0.2436
2026-01-04 11:56:51,114: t15.2023.10.13 val PER: 0.2149
2026-01-04 11:56:51,114: t15.2023.10.15 val PER: 0.1523
2026-01-04 11:56:51,114: t15.2023.10.20 val PER: 0.2013
2026-01-04 11:56:51,114: t15.2023.10.22 val PER: 0.1147
2026-01-04 11:56:51,114: t15.2023.11.03 val PER: 0.1737
2026-01-04 11:56:51,114: t15.2023.11.04 val PER: 0.0375
2026-01-04 11:56:51,115: t15.2023.11.17 val PER: 0.0420
2026-01-04 11:56:51,115: t15.2023.11.19 val PER: 0.0379
2026-01-04 11:56:51,115: t15.2023.11.26 val PER: 0.1283
2026-01-04 11:56:51,115: t15.2023.12.03 val PER: 0.1250
2026-01-04 11:56:51,115: t15.2023.12.08 val PER: 0.1138
2026-01-04 11:56:51,116: t15.2023.12.10 val PER: 0.0972
2026-01-04 11:56:51,116: t15.2023.12.17 val PER: 0.1268
2026-01-04 11:56:51,116: t15.2023.12.29 val PER: 0.1325
2026-01-04 11:56:51,116: t15.2024.02.25 val PER: 0.1222
2026-01-04 11:56:51,116: t15.2024.03.08 val PER: 0.2432
2026-01-04 11:56:51,116: t15.2024.03.15 val PER: 0.2058
2026-01-04 11:56:51,116: t15.2024.03.17 val PER: 0.1506
2026-01-04 11:56:51,116: t15.2024.05.10 val PER: 0.1694
2026-01-04 11:56:51,116: t15.2024.06.14 val PER: 0.1703
2026-01-04 11:56:51,117: t15.2024.07.19 val PER: 0.2399
2026-01-04 11:56:51,117: t15.2024.07.21 val PER: 0.0931
2026-01-04 11:56:51,117: t15.2024.07.28 val PER: 0.1434
2026-01-04 11:56:51,117: t15.2025.01.10 val PER: 0.3017
2026-01-04 11:56:51,117: t15.2025.01.12 val PER: 0.1440
2026-01-04 11:56:51,117: t15.2025.03.14 val PER: 0.3536
2026-01-04 11:56:51,117: t15.2025.03.16 val PER: 0.1806
2026-01-04 11:56:51,117: t15.2025.03.30 val PER: 0.3080
2026-01-04 11:56:51,117: t15.2025.04.13 val PER: 0.2225
2026-01-04 11:56:51,402: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_13500
2026-01-04 11:57:01,724: Train batch 13600: loss: 12.72 grad norm: 63.38 time: 0.064
2026-01-04 11:57:22,344: Train batch 13800: loss: 9.53 grad norm: 60.08 time: 0.056
2026-01-04 11:57:42,937: Train batch 14000: loss: 12.01 grad norm: 57.40 time: 0.052
2026-01-04 11:57:42,937: Running test after training batch: 14000
2026-01-04 11:57:43,057: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:57:48,081: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 11:57:48,120: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 11:57:50,231: Val batch 14000: PER (avg): 0.1531 CTC Loss (avg): 15.5416 WER(1gram): 47.97% (n=64) time: 7.293
2026-01-04 11:57:50,232: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-04 11:57:50,232: t15.2023.08.13 val PER: 0.1102
2026-01-04 11:57:50,232: t15.2023.08.18 val PER: 0.1123
2026-01-04 11:57:50,232: t15.2023.08.20 val PER: 0.1112
2026-01-04 11:57:50,232: t15.2023.08.25 val PER: 0.1024
2026-01-04 11:57:50,232: t15.2023.08.27 val PER: 0.1913
2026-01-04 11:57:50,232: t15.2023.09.01 val PER: 0.0771
2026-01-04 11:57:50,232: t15.2023.09.03 val PER: 0.1627
2026-01-04 11:57:50,233: t15.2023.09.24 val PER: 0.1262
2026-01-04 11:57:50,233: t15.2023.09.29 val PER: 0.1385
2026-01-04 11:57:50,233: t15.2023.10.01 val PER: 0.1731
2026-01-04 11:57:50,233: t15.2023.10.06 val PER: 0.0840
2026-01-04 11:57:50,233: t15.2023.10.08 val PER: 0.2544
2026-01-04 11:57:50,233: t15.2023.10.13 val PER: 0.2064
2026-01-04 11:57:50,233: t15.2023.10.15 val PER: 0.1496
2026-01-04 11:57:50,233: t15.2023.10.20 val PER: 0.1846
2026-01-04 11:57:50,234: t15.2023.10.22 val PER: 0.1136
2026-01-04 11:57:50,234: t15.2023.11.03 val PER: 0.1744
2026-01-04 11:57:50,234: t15.2023.11.04 val PER: 0.0341
2026-01-04 11:57:50,234: t15.2023.11.17 val PER: 0.0498
2026-01-04 11:57:50,234: t15.2023.11.19 val PER: 0.0379
2026-01-04 11:57:50,234: t15.2023.11.26 val PER: 0.1217
2026-01-04 11:57:50,234: t15.2023.12.03 val PER: 0.1218
2026-01-04 11:57:50,234: t15.2023.12.08 val PER: 0.1065
2026-01-04 11:57:50,235: t15.2023.12.10 val PER: 0.0972
2026-01-04 11:57:50,235: t15.2023.12.17 val PER: 0.1393
2026-01-04 11:57:50,235: t15.2023.12.29 val PER: 0.1428
2026-01-04 11:57:50,235: t15.2024.02.25 val PER: 0.1081
2026-01-04 11:57:50,235: t15.2024.03.08 val PER: 0.2404
2026-01-04 11:57:50,235: t15.2024.03.15 val PER: 0.2045
2026-01-04 11:57:50,235: t15.2024.03.17 val PER: 0.1478
2026-01-04 11:57:50,235: t15.2024.05.10 val PER: 0.1516
2026-01-04 11:57:50,235: t15.2024.06.14 val PER: 0.1688
2026-01-04 11:57:50,236: t15.2024.07.19 val PER: 0.2294
2026-01-04 11:57:50,236: t15.2024.07.21 val PER: 0.0979
2026-01-04 11:57:50,236: t15.2024.07.28 val PER: 0.1375
2026-01-04 11:57:50,236: t15.2025.01.10 val PER: 0.3154
2026-01-04 11:57:50,236: t15.2025.01.12 val PER: 0.1401
2026-01-04 11:57:50,236: t15.2025.03.14 val PER: 0.3373
2026-01-04 11:57:50,237: t15.2025.03.16 val PER: 0.1898
2026-01-04 11:57:50,237: t15.2025.03.30 val PER: 0.2931
2026-01-04 11:57:50,237: t15.2025.04.13 val PER: 0.2168
2026-01-04 11:57:50,538: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_14000
2026-01-04 11:58:10,895: Train batch 14200: loss: 8.09 grad norm: 49.83 time: 0.058
2026-01-04 11:58:31,466: Train batch 14400: loss: 6.06 grad norm: 39.71 time: 0.064
2026-01-04 11:58:41,837: Running test after training batch: 14500
2026-01-04 11:58:41,974: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:58:47,063: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 11:58:47,101: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 11:58:49,181: Val batch 14500: PER (avg): 0.1514 CTC Loss (avg): 15.4836 WER(1gram): 47.97% (n=64) time: 7.344
2026-01-04 11:58:49,182: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-04 11:58:49,182: t15.2023.08.13 val PER: 0.1123
2026-01-04 11:58:49,182: t15.2023.08.18 val PER: 0.1065
2026-01-04 11:58:49,182: t15.2023.08.20 val PER: 0.1072
2026-01-04 11:58:49,182: t15.2023.08.25 val PER: 0.0889
2026-01-04 11:58:49,182: t15.2023.08.27 val PER: 0.1945
2026-01-04 11:58:49,182: t15.2023.09.01 val PER: 0.0804
2026-01-04 11:58:49,183: t15.2023.09.03 val PER: 0.1686
2026-01-04 11:58:49,183: t15.2023.09.24 val PER: 0.1262
2026-01-04 11:58:49,183: t15.2023.09.29 val PER: 0.1315
2026-01-04 11:58:49,183: t15.2023.10.01 val PER: 0.1750
2026-01-04 11:58:49,183: t15.2023.10.06 val PER: 0.0775
2026-01-04 11:58:49,183: t15.2023.10.08 val PER: 0.2530
2026-01-04 11:58:49,183: t15.2023.10.13 val PER: 0.2002
2026-01-04 11:58:49,183: t15.2023.10.15 val PER: 0.1490
2026-01-04 11:58:49,183: t15.2023.10.20 val PER: 0.1913
2026-01-04 11:58:49,184: t15.2023.10.22 val PER: 0.1069
2026-01-04 11:58:49,184: t15.2023.11.03 val PER: 0.1825
2026-01-04 11:58:49,184: t15.2023.11.04 val PER: 0.0341
2026-01-04 11:58:49,184: t15.2023.11.17 val PER: 0.0513
2026-01-04 11:58:49,184: t15.2023.11.19 val PER: 0.0399
2026-01-04 11:58:49,184: t15.2023.11.26 val PER: 0.1203
2026-01-04 11:58:49,184: t15.2023.12.03 val PER: 0.1124
2026-01-04 11:58:49,184: t15.2023.12.08 val PER: 0.1045
2026-01-04 11:58:49,184: t15.2023.12.10 val PER: 0.0972
2026-01-04 11:58:49,185: t15.2023.12.17 val PER: 0.1393
2026-01-04 11:58:49,185: t15.2023.12.29 val PER: 0.1311
2026-01-04 11:58:49,185: t15.2024.02.25 val PER: 0.1081
2026-01-04 11:58:49,185: t15.2024.03.08 val PER: 0.2390
2026-01-04 11:58:49,185: t15.2024.03.15 val PER: 0.2039
2026-01-04 11:58:49,185: t15.2024.03.17 val PER: 0.1416
2026-01-04 11:58:49,185: t15.2024.05.10 val PER: 0.1441
2026-01-04 11:58:49,186: t15.2024.06.14 val PER: 0.1640
2026-01-04 11:58:49,186: t15.2024.07.19 val PER: 0.2386
2026-01-04 11:58:49,186: t15.2024.07.21 val PER: 0.0890
2026-01-04 11:58:49,186: t15.2024.07.28 val PER: 0.1397
2026-01-04 11:58:49,186: t15.2025.01.10 val PER: 0.2975
2026-01-04 11:58:49,186: t15.2025.01.12 val PER: 0.1486
2026-01-04 11:58:49,186: t15.2025.03.14 val PER: 0.3388
2026-01-04 11:58:49,186: t15.2025.03.16 val PER: 0.1819
2026-01-04 11:58:49,186: t15.2025.03.30 val PER: 0.2943
2026-01-04 11:58:49,187: t15.2025.04.13 val PER: 0.2183
2026-01-04 11:58:49,488: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_14500
2026-01-04 11:58:59,814: Train batch 14600: loss: 12.17 grad norm: 59.24 time: 0.059
2026-01-04 11:59:20,648: Train batch 14800: loss: 5.98 grad norm: 46.27 time: 0.053
2026-01-04 11:59:41,515: Train batch 15000: loss: 9.16 grad norm: 50.30 time: 0.052
2026-01-04 11:59:41,515: Running test after training batch: 15000
2026-01-04 11:59:41,664: WER debug GT example: You can see the code at this point as well.
2026-01-04 11:59:46,784: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 11:59:46,824: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 11:59:48,901: Val batch 15000: PER (avg): 0.1505 CTC Loss (avg): 15.2862 WER(1gram): 46.70% (n=64) time: 7.385
2026-01-04 11:59:48,901: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 11:59:48,901: t15.2023.08.13 val PER: 0.1091
2026-01-04 11:59:48,901: t15.2023.08.18 val PER: 0.1039
2026-01-04 11:59:48,901: t15.2023.08.20 val PER: 0.1112
2026-01-04 11:59:48,902: t15.2023.08.25 val PER: 0.0949
2026-01-04 11:59:48,902: t15.2023.08.27 val PER: 0.1929
2026-01-04 11:59:48,902: t15.2023.09.01 val PER: 0.0739
2026-01-04 11:59:48,902: t15.2023.09.03 val PER: 0.1556
2026-01-04 11:59:48,902: t15.2023.09.24 val PER: 0.1299
2026-01-04 11:59:48,902: t15.2023.09.29 val PER: 0.1308
2026-01-04 11:59:48,902: t15.2023.10.01 val PER: 0.1704
2026-01-04 11:59:48,902: t15.2023.10.06 val PER: 0.0764
2026-01-04 11:59:48,902: t15.2023.10.08 val PER: 0.2625
2026-01-04 11:59:48,902: t15.2023.10.13 val PER: 0.2017
2026-01-04 11:59:48,902: t15.2023.10.15 val PER: 0.1477
2026-01-04 11:59:48,902: t15.2023.10.20 val PER: 0.2047
2026-01-04 11:59:48,902: t15.2023.10.22 val PER: 0.1125
2026-01-04 11:59:48,903: t15.2023.11.03 val PER: 0.1777
2026-01-04 11:59:48,903: t15.2023.11.04 val PER: 0.0410
2026-01-04 11:59:48,903: t15.2023.11.17 val PER: 0.0482
2026-01-04 11:59:48,903: t15.2023.11.19 val PER: 0.0399
2026-01-04 11:59:48,903: t15.2023.11.26 val PER: 0.1210
2026-01-04 11:59:48,903: t15.2023.12.03 val PER: 0.1250
2026-01-04 11:59:48,903: t15.2023.12.08 val PER: 0.0985
2026-01-04 11:59:48,903: t15.2023.12.10 val PER: 0.0972
2026-01-04 11:59:48,903: t15.2023.12.17 val PER: 0.1486
2026-01-04 11:59:48,903: t15.2023.12.29 val PER: 0.1366
2026-01-04 11:59:48,903: t15.2024.02.25 val PER: 0.1081
2026-01-04 11:59:48,903: t15.2024.03.08 val PER: 0.2304
2026-01-04 11:59:48,903: t15.2024.03.15 val PER: 0.2001
2026-01-04 11:59:48,903: t15.2024.03.17 val PER: 0.1353
2026-01-04 11:59:48,903: t15.2024.05.10 val PER: 0.1590
2026-01-04 11:59:48,903: t15.2024.06.14 val PER: 0.1672
2026-01-04 11:59:48,904: t15.2024.07.19 val PER: 0.2208
2026-01-04 11:59:48,904: t15.2024.07.21 val PER: 0.0924
2026-01-04 11:59:48,904: t15.2024.07.28 val PER: 0.1338
2026-01-04 11:59:48,904: t15.2025.01.10 val PER: 0.2961
2026-01-04 11:59:48,904: t15.2025.01.12 val PER: 0.1463
2026-01-04 11:59:48,904: t15.2025.03.14 val PER: 0.3402
2026-01-04 11:59:48,905: t15.2025.03.16 val PER: 0.1963
2026-01-04 11:59:48,905: t15.2025.03.30 val PER: 0.2908
2026-01-04 11:59:48,905: t15.2025.04.13 val PER: 0.2054
2026-01-04 11:59:49,213: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_15000
2026-01-04 12:00:10,374: Train batch 15200: loss: 5.44 grad norm: 40.60 time: 0.058
2026-01-04 12:00:30,821: Train batch 15400: loss: 11.76 grad norm: 57.80 time: 0.052
2026-01-04 12:00:41,375: Running test after training batch: 15500
2026-01-04 12:00:41,497: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:00:46,604: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-04 12:00:46,644: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 12:00:48,788: Val batch 15500: PER (avg): 0.1486 CTC Loss (avg): 15.2536 WER(1gram): 46.45% (n=64) time: 7.412
2026-01-04 12:00:48,789: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 12:00:48,789: t15.2023.08.13 val PER: 0.1112
2026-01-04 12:00:48,789: t15.2023.08.18 val PER: 0.1039
2026-01-04 12:00:48,789: t15.2023.08.20 val PER: 0.1120
2026-01-04 12:00:48,789: t15.2023.08.25 val PER: 0.0904
2026-01-04 12:00:48,789: t15.2023.08.27 val PER: 0.1865
2026-01-04 12:00:48,789: t15.2023.09.01 val PER: 0.0714
2026-01-04 12:00:48,790: t15.2023.09.03 val PER: 0.1603
2026-01-04 12:00:48,790: t15.2023.09.24 val PER: 0.1226
2026-01-04 12:00:48,790: t15.2023.09.29 val PER: 0.1257
2026-01-04 12:00:48,790: t15.2023.10.01 val PER: 0.1684
2026-01-04 12:00:48,790: t15.2023.10.06 val PER: 0.0829
2026-01-04 12:00:48,790: t15.2023.10.08 val PER: 0.2517
2026-01-04 12:00:48,790: t15.2023.10.13 val PER: 0.1994
2026-01-04 12:00:48,790: t15.2023.10.15 val PER: 0.1430
2026-01-04 12:00:48,791: t15.2023.10.20 val PER: 0.1946
2026-01-04 12:00:48,791: t15.2023.10.22 val PER: 0.1158
2026-01-04 12:00:48,791: t15.2023.11.03 val PER: 0.1710
2026-01-04 12:00:48,791: t15.2023.11.04 val PER: 0.0273
2026-01-04 12:00:48,791: t15.2023.11.17 val PER: 0.0435
2026-01-04 12:00:48,791: t15.2023.11.19 val PER: 0.0359
2026-01-04 12:00:48,791: t15.2023.11.26 val PER: 0.1217
2026-01-04 12:00:48,791: t15.2023.12.03 val PER: 0.1208
2026-01-04 12:00:48,791: t15.2023.12.08 val PER: 0.1059
2026-01-04 12:00:48,791: t15.2023.12.10 val PER: 0.0894
2026-01-04 12:00:48,791: t15.2023.12.17 val PER: 0.1341
2026-01-04 12:00:48,791: t15.2023.12.29 val PER: 0.1283
2026-01-04 12:00:48,791: t15.2024.02.25 val PER: 0.1053
2026-01-04 12:00:48,791: t15.2024.03.08 val PER: 0.2290
2026-01-04 12:00:48,792: t15.2024.03.15 val PER: 0.2070
2026-01-04 12:00:48,792: t15.2024.03.17 val PER: 0.1353
2026-01-04 12:00:48,792: t15.2024.05.10 val PER: 0.1516
2026-01-04 12:00:48,792: t15.2024.06.14 val PER: 0.1593
2026-01-04 12:00:48,792: t15.2024.07.19 val PER: 0.2274
2026-01-04 12:00:48,792: t15.2024.07.21 val PER: 0.0938
2026-01-04 12:00:48,792: t15.2024.07.28 val PER: 0.1382
2026-01-04 12:00:48,792: t15.2025.01.10 val PER: 0.3017
2026-01-04 12:00:48,792: t15.2025.01.12 val PER: 0.1416
2026-01-04 12:00:48,793: t15.2025.03.14 val PER: 0.3343
2026-01-04 12:00:48,793: t15.2025.03.16 val PER: 0.1963
2026-01-04 12:00:48,793: t15.2025.03.30 val PER: 0.2805
2026-01-04 12:00:48,793: t15.2025.04.13 val PER: 0.2011
2026-01-04 12:00:49,097: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_15500
2026-01-04 12:00:59,471: Train batch 15600: loss: 11.91 grad norm: 56.54 time: 0.063
2026-01-04 12:01:20,199: Train batch 15800: loss: 13.72 grad norm: 63.88 time: 0.067
2026-01-04 12:01:41,312: Train batch 16000: loss: 8.66 grad norm: 43.96 time: 0.056
2026-01-04 12:01:41,312: Running test after training batch: 16000
2026-01-04 12:01:41,477: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:01:46,611: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:01:46,650: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 12:01:48,834: Val batch 16000: PER (avg): 0.1497 CTC Loss (avg): 15.3485 WER(1gram): 47.21% (n=64) time: 7.521
2026-01-04 12:01:48,834: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 12:01:48,834: t15.2023.08.13 val PER: 0.1081
2026-01-04 12:01:48,834: t15.2023.08.18 val PER: 0.1039
2026-01-04 12:01:48,834: t15.2023.08.20 val PER: 0.1041
2026-01-04 12:01:48,835: t15.2023.08.25 val PER: 0.0979
2026-01-04 12:01:48,835: t15.2023.08.27 val PER: 0.1833
2026-01-04 12:01:48,835: t15.2023.09.01 val PER: 0.0771
2026-01-04 12:01:48,835: t15.2023.09.03 val PER: 0.1627
2026-01-04 12:01:48,835: t15.2023.09.24 val PER: 0.1262
2026-01-04 12:01:48,835: t15.2023.09.29 val PER: 0.1321
2026-01-04 12:01:48,835: t15.2023.10.01 val PER: 0.1704
2026-01-04 12:01:48,835: t15.2023.10.06 val PER: 0.0872
2026-01-04 12:01:48,835: t15.2023.10.08 val PER: 0.2544
2026-01-04 12:01:48,835: t15.2023.10.13 val PER: 0.2033
2026-01-04 12:01:48,835: t15.2023.10.15 val PER: 0.1417
2026-01-04 12:01:48,835: t15.2023.10.20 val PER: 0.1879
2026-01-04 12:01:48,835: t15.2023.10.22 val PER: 0.1102
2026-01-04 12:01:48,835: t15.2023.11.03 val PER: 0.1791
2026-01-04 12:01:48,835: t15.2023.11.04 val PER: 0.0375
2026-01-04 12:01:48,835: t15.2023.11.17 val PER: 0.0482
2026-01-04 12:01:48,836: t15.2023.11.19 val PER: 0.0419
2026-01-04 12:01:48,836: t15.2023.11.26 val PER: 0.1181
2026-01-04 12:01:48,836: t15.2023.12.03 val PER: 0.1250
2026-01-04 12:01:48,836: t15.2023.12.08 val PER: 0.0985
2026-01-04 12:01:48,840: t15.2023.12.10 val PER: 0.0972
2026-01-04 12:01:48,840: t15.2023.12.17 val PER: 0.1383
2026-01-04 12:01:48,841: t15.2023.12.29 val PER: 0.1283
2026-01-04 12:01:48,841: t15.2024.02.25 val PER: 0.1039
2026-01-04 12:01:48,841: t15.2024.03.08 val PER: 0.2418
2026-01-04 12:01:48,841: t15.2024.03.15 val PER: 0.2008
2026-01-04 12:01:48,841: t15.2024.03.17 val PER: 0.1332
2026-01-04 12:01:48,841: t15.2024.05.10 val PER: 0.1471
2026-01-04 12:01:48,841: t15.2024.06.14 val PER: 0.1593
2026-01-04 12:01:48,842: t15.2024.07.19 val PER: 0.2320
2026-01-04 12:01:48,842: t15.2024.07.21 val PER: 0.0890
2026-01-04 12:01:48,842: t15.2024.07.28 val PER: 0.1375
2026-01-04 12:01:48,842: t15.2025.01.10 val PER: 0.3003
2026-01-04 12:01:48,842: t15.2025.01.12 val PER: 0.1540
2026-01-04 12:01:48,842: t15.2025.03.14 val PER: 0.3254
2026-01-04 12:01:48,842: t15.2025.03.16 val PER: 0.1898
2026-01-04 12:01:48,842: t15.2025.03.30 val PER: 0.2885
2026-01-04 12:01:48,842: t15.2025.04.13 val PER: 0.2083
2026-01-04 12:01:49,135: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_16000
2026-01-04 12:02:11,090: Train batch 16200: loss: 6.12 grad norm: 44.14 time: 0.057
2026-01-04 12:02:32,133: Train batch 16400: loss: 10.21 grad norm: 58.17 time: 0.058
2026-01-04 12:02:43,340: Running test after training batch: 16500
2026-01-04 12:02:43,491: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:02:48,606: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:02:48,645: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 12:02:50,856: Val batch 16500: PER (avg): 0.1473 CTC Loss (avg): 15.1577 WER(1gram): 45.69% (n=64) time: 7.515
2026-01-04 12:02:50,856: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 12:02:50,857: t15.2023.08.13 val PER: 0.1071
2026-01-04 12:02:50,857: t15.2023.08.18 val PER: 0.0989
2026-01-04 12:02:50,857: t15.2023.08.20 val PER: 0.1064
2026-01-04 12:02:50,857: t15.2023.08.25 val PER: 0.0873
2026-01-04 12:02:50,857: t15.2023.08.27 val PER: 0.1817
2026-01-04 12:02:50,857: t15.2023.09.01 val PER: 0.0755
2026-01-04 12:02:50,857: t15.2023.09.03 val PER: 0.1615
2026-01-04 12:02:50,857: t15.2023.09.24 val PER: 0.1286
2026-01-04 12:02:50,857: t15.2023.09.29 val PER: 0.1257
2026-01-04 12:02:50,857: t15.2023.10.01 val PER: 0.1737
2026-01-04 12:02:50,857: t15.2023.10.06 val PER: 0.0818
2026-01-04 12:02:50,857: t15.2023.10.08 val PER: 0.2530
2026-01-04 12:02:50,858: t15.2023.10.13 val PER: 0.1971
2026-01-04 12:02:50,858: t15.2023.10.15 val PER: 0.1411
2026-01-04 12:02:50,858: t15.2023.10.20 val PER: 0.1980
2026-01-04 12:02:50,858: t15.2023.10.22 val PER: 0.1080
2026-01-04 12:02:50,858: t15.2023.11.03 val PER: 0.1737
2026-01-04 12:02:50,858: t15.2023.11.04 val PER: 0.0375
2026-01-04 12:02:50,858: t15.2023.11.17 val PER: 0.0451
2026-01-04 12:02:50,858: t15.2023.11.19 val PER: 0.0419
2026-01-04 12:02:50,858: t15.2023.11.26 val PER: 0.1138
2026-01-04 12:02:50,859: t15.2023.12.03 val PER: 0.1166
2026-01-04 12:02:50,859: t15.2023.12.08 val PER: 0.0972
2026-01-04 12:02:50,859: t15.2023.12.10 val PER: 0.0920
2026-01-04 12:02:50,859: t15.2023.12.17 val PER: 0.1216
2026-01-04 12:02:50,859: t15.2023.12.29 val PER: 0.1256
2026-01-04 12:02:50,859: t15.2024.02.25 val PER: 0.1011
2026-01-04 12:02:50,859: t15.2024.03.08 val PER: 0.2333
2026-01-04 12:02:50,860: t15.2024.03.15 val PER: 0.2020
2026-01-04 12:02:50,860: t15.2024.03.17 val PER: 0.1339
2026-01-04 12:02:50,860: t15.2024.05.10 val PER: 0.1560
2026-01-04 12:02:50,860: t15.2024.06.14 val PER: 0.1609
2026-01-04 12:02:50,860: t15.2024.07.19 val PER: 0.2294
2026-01-04 12:02:50,860: t15.2024.07.21 val PER: 0.0931
2026-01-04 12:02:50,861: t15.2024.07.28 val PER: 0.1353
2026-01-04 12:02:50,861: t15.2025.01.10 val PER: 0.2782
2026-01-04 12:02:50,861: t15.2025.01.12 val PER: 0.1424
2026-01-04 12:02:50,861: t15.2025.03.14 val PER: 0.3388
2026-01-04 12:02:50,861: t15.2025.03.16 val PER: 0.1950
2026-01-04 12:02:50,861: t15.2025.03.30 val PER: 0.2862
2026-01-04 12:02:50,861: t15.2025.04.13 val PER: 0.2111
2026-01-04 12:02:51,184: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_16500
2026-01-04 12:03:01,582: Train batch 16600: loss: 8.51 grad norm: 52.46 time: 0.055
2026-01-04 12:03:22,698: Train batch 16800: loss: 16.49 grad norm: 73.61 time: 0.062
2026-01-04 12:03:43,832: Train batch 17000: loss: 8.03 grad norm: 45.96 time: 0.082
2026-01-04 12:03:43,832: Running test after training batch: 17000
2026-01-04 12:03:43,954: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:03:49,048: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:03:49,091: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 12:03:51,248: Val batch 17000: PER (avg): 0.1466 CTC Loss (avg): 15.0423 WER(1gram): 45.43% (n=64) time: 7.415
2026-01-04 12:03:51,248: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 12:03:51,249: t15.2023.08.13 val PER: 0.1102
2026-01-04 12:03:51,249: t15.2023.08.18 val PER: 0.1065
2026-01-04 12:03:51,249: t15.2023.08.20 val PER: 0.1064
2026-01-04 12:03:51,249: t15.2023.08.25 val PER: 0.0873
2026-01-04 12:03:51,249: t15.2023.08.27 val PER: 0.1768
2026-01-04 12:03:51,249: t15.2023.09.01 val PER: 0.0739
2026-01-04 12:03:51,249: t15.2023.09.03 val PER: 0.1591
2026-01-04 12:03:51,249: t15.2023.09.24 val PER: 0.1250
2026-01-04 12:03:51,249: t15.2023.09.29 val PER: 0.1264
2026-01-04 12:03:51,249: t15.2023.10.01 val PER: 0.1678
2026-01-04 12:03:51,249: t15.2023.10.06 val PER: 0.0807
2026-01-04 12:03:51,249: t15.2023.10.08 val PER: 0.2463
2026-01-04 12:03:51,250: t15.2023.10.13 val PER: 0.1986
2026-01-04 12:03:51,250: t15.2023.10.15 val PER: 0.1444
2026-01-04 12:03:51,250: t15.2023.10.20 val PER: 0.1846
2026-01-04 12:03:51,250: t15.2023.10.22 val PER: 0.1047
2026-01-04 12:03:51,250: t15.2023.11.03 val PER: 0.1723
2026-01-04 12:03:51,250: t15.2023.11.04 val PER: 0.0341
2026-01-04 12:03:51,250: t15.2023.11.17 val PER: 0.0435
2026-01-04 12:03:51,250: t15.2023.11.19 val PER: 0.0419
2026-01-04 12:03:51,251: t15.2023.11.26 val PER: 0.1109
2026-01-04 12:03:51,251: t15.2023.12.03 val PER: 0.1155
2026-01-04 12:03:51,251: t15.2023.12.08 val PER: 0.0979
2026-01-04 12:03:51,251: t15.2023.12.10 val PER: 0.0894
2026-01-04 12:03:51,251: t15.2023.12.17 val PER: 0.1279
2026-01-04 12:03:51,251: t15.2023.12.29 val PER: 0.1263
2026-01-04 12:03:51,251: t15.2024.02.25 val PER: 0.1067
2026-01-04 12:03:51,251: t15.2024.03.08 val PER: 0.2290
2026-01-04 12:03:51,251: t15.2024.03.15 val PER: 0.1976
2026-01-04 12:03:51,251: t15.2024.03.17 val PER: 0.1290
2026-01-04 12:03:51,251: t15.2024.05.10 val PER: 0.1530
2026-01-04 12:03:51,251: t15.2024.06.14 val PER: 0.1625
2026-01-04 12:03:51,252: t15.2024.07.19 val PER: 0.2274
2026-01-04 12:03:51,252: t15.2024.07.21 val PER: 0.0903
2026-01-04 12:03:51,252: t15.2024.07.28 val PER: 0.1324
2026-01-04 12:03:51,252: t15.2025.01.10 val PER: 0.3030
2026-01-04 12:03:51,252: t15.2025.01.12 val PER: 0.1386
2026-01-04 12:03:51,252: t15.2025.03.14 val PER: 0.3402
2026-01-04 12:03:51,252: t15.2025.03.16 val PER: 0.1806
2026-01-04 12:03:51,252: t15.2025.03.30 val PER: 0.2954
2026-01-04 12:03:51,253: t15.2025.04.13 val PER: 0.2140
2026-01-04 12:03:51,560: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_17000
2026-01-04 12:04:11,951: Train batch 17200: loss: 9.63 grad norm: 50.47 time: 0.085
2026-01-04 12:04:32,853: Train batch 17400: loss: 11.87 grad norm: 61.31 time: 0.071
2026-01-04 12:04:43,060: Running test after training batch: 17500
2026-01-04 12:04:43,233: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:04:48,332: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:04:48,373: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 12:04:50,589: Val batch 17500: PER (avg): 0.1472 CTC Loss (avg): 15.0345 WER(1gram): 46.70% (n=64) time: 7.528
2026-01-04 12:04:50,590: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 12:04:50,590: t15.2023.08.13 val PER: 0.1040
2026-01-04 12:04:50,590: t15.2023.08.18 val PER: 0.1023
2026-01-04 12:04:50,590: t15.2023.08.20 val PER: 0.1120
2026-01-04 12:04:50,590: t15.2023.08.25 val PER: 0.0889
2026-01-04 12:04:50,590: t15.2023.08.27 val PER: 0.1881
2026-01-04 12:04:50,590: t15.2023.09.01 val PER: 0.0722
2026-01-04 12:04:50,591: t15.2023.09.03 val PER: 0.1663
2026-01-04 12:04:50,591: t15.2023.09.24 val PER: 0.1286
2026-01-04 12:04:50,591: t15.2023.09.29 val PER: 0.1283
2026-01-04 12:04:50,591: t15.2023.10.01 val PER: 0.1658
2026-01-04 12:04:50,591: t15.2023.10.06 val PER: 0.0797
2026-01-04 12:04:50,591: t15.2023.10.08 val PER: 0.2490
2026-01-04 12:04:50,591: t15.2023.10.13 val PER: 0.1932
2026-01-04 12:04:50,591: t15.2023.10.15 val PER: 0.1457
2026-01-04 12:04:50,591: t15.2023.10.20 val PER: 0.2114
2026-01-04 12:04:50,592: t15.2023.10.22 val PER: 0.1058
2026-01-04 12:04:50,592: t15.2023.11.03 val PER: 0.1723
2026-01-04 12:04:50,592: t15.2023.11.04 val PER: 0.0341
2026-01-04 12:04:50,592: t15.2023.11.17 val PER: 0.0467
2026-01-04 12:04:50,592: t15.2023.11.19 val PER: 0.0439
2026-01-04 12:04:50,592: t15.2023.11.26 val PER: 0.1094
2026-01-04 12:04:50,592: t15.2023.12.03 val PER: 0.1113
2026-01-04 12:04:50,592: t15.2023.12.08 val PER: 0.0965
2026-01-04 12:04:50,592: t15.2023.12.10 val PER: 0.0933
2026-01-04 12:04:50,592: t15.2023.12.17 val PER: 0.1320
2026-01-04 12:04:50,592: t15.2023.12.29 val PER: 0.1290
2026-01-04 12:04:50,592: t15.2024.02.25 val PER: 0.0997
2026-01-04 12:04:50,592: t15.2024.03.08 val PER: 0.2276
2026-01-04 12:04:50,592: t15.2024.03.15 val PER: 0.1982
2026-01-04 12:04:50,592: t15.2024.03.17 val PER: 0.1360
2026-01-04 12:04:50,593: t15.2024.05.10 val PER: 0.1620
2026-01-04 12:04:50,593: t15.2024.06.14 val PER: 0.1609
2026-01-04 12:04:50,593: t15.2024.07.19 val PER: 0.2248
2026-01-04 12:04:50,593: t15.2024.07.21 val PER: 0.0910
2026-01-04 12:04:50,593: t15.2024.07.28 val PER: 0.1324
2026-01-04 12:04:50,593: t15.2025.01.10 val PER: 0.2879
2026-01-04 12:04:50,593: t15.2025.01.12 val PER: 0.1393
2026-01-04 12:04:50,593: t15.2025.03.14 val PER: 0.3476
2026-01-04 12:04:50,593: t15.2025.03.16 val PER: 0.1872
2026-01-04 12:04:50,593: t15.2025.03.30 val PER: 0.2874
2026-01-04 12:04:50,593: t15.2025.04.13 val PER: 0.2140
2026-01-04 12:04:50,917: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_17500
2026-01-04 12:05:01,207: Train batch 17600: loss: 9.62 grad norm: 53.57 time: 0.051
2026-01-04 12:05:22,103: Train batch 17800: loss: 6.20 grad norm: 50.12 time: 0.045
2026-01-04 12:05:42,896: Train batch 18000: loss: 11.18 grad norm: 64.29 time: 0.061
2026-01-04 12:05:42,897: Running test after training batch: 18000
2026-01-04 12:05:43,069: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:05:48,179: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:05:48,221: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 12:05:50,474: Val batch 18000: PER (avg): 0.1453 CTC Loss (avg): 15.0701 WER(1gram): 47.46% (n=64) time: 7.577
2026-01-04 12:05:50,475: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-04 12:05:50,475: t15.2023.08.13 val PER: 0.1071
2026-01-04 12:05:50,475: t15.2023.08.18 val PER: 0.1014
2026-01-04 12:05:50,475: t15.2023.08.20 val PER: 0.1064
2026-01-04 12:05:50,475: t15.2023.08.25 val PER: 0.0934
2026-01-04 12:05:50,476: t15.2023.08.27 val PER: 0.1913
2026-01-04 12:05:50,476: t15.2023.09.01 val PER: 0.0763
2026-01-04 12:05:50,476: t15.2023.09.03 val PER: 0.1556
2026-01-04 12:05:50,476: t15.2023.09.24 val PER: 0.1262
2026-01-04 12:05:50,476: t15.2023.09.29 val PER: 0.1257
2026-01-04 12:05:50,476: t15.2023.10.01 val PER: 0.1684
2026-01-04 12:05:50,476: t15.2023.10.06 val PER: 0.0775
2026-01-04 12:05:50,476: t15.2023.10.08 val PER: 0.2503
2026-01-04 12:05:50,477: t15.2023.10.13 val PER: 0.1947
2026-01-04 12:05:50,477: t15.2023.10.15 val PER: 0.1437
2026-01-04 12:05:50,477: t15.2023.10.20 val PER: 0.2047
2026-01-04 12:05:50,477: t15.2023.10.22 val PER: 0.1013
2026-01-04 12:05:50,477: t15.2023.11.03 val PER: 0.1723
2026-01-04 12:05:50,477: t15.2023.11.04 val PER: 0.0341
2026-01-04 12:05:50,477: t15.2023.11.17 val PER: 0.0420
2026-01-04 12:05:50,478: t15.2023.11.19 val PER: 0.0379
2026-01-04 12:05:50,478: t15.2023.11.26 val PER: 0.1094
2026-01-04 12:05:50,478: t15.2023.12.03 val PER: 0.1113
2026-01-04 12:05:50,478: t15.2023.12.08 val PER: 0.0939
2026-01-04 12:05:50,478: t15.2023.12.10 val PER: 0.0972
2026-01-04 12:05:50,478: t15.2023.12.17 val PER: 0.1299
2026-01-04 12:05:50,478: t15.2023.12.29 val PER: 0.1277
2026-01-04 12:05:50,478: t15.2024.02.25 val PER: 0.1053
2026-01-04 12:05:50,478: t15.2024.03.08 val PER: 0.2205
2026-01-04 12:05:50,479: t15.2024.03.15 val PER: 0.1982
2026-01-04 12:05:50,479: t15.2024.03.17 val PER: 0.1255
2026-01-04 12:05:50,479: t15.2024.05.10 val PER: 0.1530
2026-01-04 12:05:50,479: t15.2024.06.14 val PER: 0.1562
2026-01-04 12:05:50,479: t15.2024.07.19 val PER: 0.2235
2026-01-04 12:05:50,482: t15.2024.07.21 val PER: 0.0897
2026-01-04 12:05:50,482: t15.2024.07.28 val PER: 0.1375
2026-01-04 12:05:50,482: t15.2025.01.10 val PER: 0.2837
2026-01-04 12:05:50,482: t15.2025.01.12 val PER: 0.1401
2026-01-04 12:05:50,482: t15.2025.03.14 val PER: 0.3358
2026-01-04 12:05:50,482: t15.2025.03.16 val PER: 0.1846
2026-01-04 12:05:50,483: t15.2025.03.30 val PER: 0.2759
2026-01-04 12:05:50,483: t15.2025.04.13 val PER: 0.2026
2026-01-04 12:05:50,793: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_18000
2026-01-04 12:06:11,852: Train batch 18200: loss: 7.31 grad norm: 46.33 time: 0.074
2026-01-04 12:06:32,467: Train batch 18400: loss: 4.65 grad norm: 41.28 time: 0.061
2026-01-04 12:06:43,020: Running test after training batch: 18500
2026-01-04 12:06:43,136: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:06:48,303: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:06:48,345: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 12:06:50,611: Val batch 18500: PER (avg): 0.1460 CTC Loss (avg): 15.0322 WER(1gram): 46.70% (n=64) time: 7.590
2026-01-04 12:06:50,611: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-04 12:06:50,611: t15.2023.08.13 val PER: 0.1071
2026-01-04 12:06:50,611: t15.2023.08.18 val PER: 0.1014
2026-01-04 12:06:50,611: t15.2023.08.20 val PER: 0.1136
2026-01-04 12:06:50,612: t15.2023.08.25 val PER: 0.0949
2026-01-04 12:06:50,612: t15.2023.08.27 val PER: 0.1897
2026-01-04 12:06:50,612: t15.2023.09.01 val PER: 0.0747
2026-01-04 12:06:50,612: t15.2023.09.03 val PER: 0.1651
2026-01-04 12:06:50,612: t15.2023.09.24 val PER: 0.1214
2026-01-04 12:06:50,612: t15.2023.09.29 val PER: 0.1283
2026-01-04 12:06:50,612: t15.2023.10.01 val PER: 0.1671
2026-01-04 12:06:50,612: t15.2023.10.06 val PER: 0.0829
2026-01-04 12:06:50,612: t15.2023.10.08 val PER: 0.2503
2026-01-04 12:06:50,612: t15.2023.10.13 val PER: 0.1932
2026-01-04 12:06:50,612: t15.2023.10.15 val PER: 0.1430
2026-01-04 12:06:50,612: t15.2023.10.20 val PER: 0.2013
2026-01-04 12:06:50,612: t15.2023.10.22 val PER: 0.1013
2026-01-04 12:06:50,613: t15.2023.11.03 val PER: 0.1737
2026-01-04 12:06:50,613: t15.2023.11.04 val PER: 0.0307
2026-01-04 12:06:50,613: t15.2023.11.17 val PER: 0.0389
2026-01-04 12:06:50,613: t15.2023.11.19 val PER: 0.0399
2026-01-04 12:06:50,613: t15.2023.11.26 val PER: 0.1123
2026-01-04 12:06:50,613: t15.2023.12.03 val PER: 0.1103
2026-01-04 12:06:50,613: t15.2023.12.08 val PER: 0.0919
2026-01-04 12:06:50,613: t15.2023.12.10 val PER: 0.0933
2026-01-04 12:06:50,613: t15.2023.12.17 val PER: 0.1268
2026-01-04 12:06:50,613: t15.2023.12.29 val PER: 0.1256
2026-01-04 12:06:50,613: t15.2024.02.25 val PER: 0.1025
2026-01-04 12:06:50,613: t15.2024.03.08 val PER: 0.2304
2026-01-04 12:06:50,613: t15.2024.03.15 val PER: 0.1982
2026-01-04 12:06:50,613: t15.2024.03.17 val PER: 0.1283
2026-01-04 12:06:50,613: t15.2024.05.10 val PER: 0.1530
2026-01-04 12:06:50,614: t15.2024.06.14 val PER: 0.1593
2026-01-04 12:06:50,614: t15.2024.07.19 val PER: 0.2248
2026-01-04 12:06:50,614: t15.2024.07.21 val PER: 0.0883
2026-01-04 12:06:50,614: t15.2024.07.28 val PER: 0.1287
2026-01-04 12:06:50,614: t15.2025.01.10 val PER: 0.2893
2026-01-04 12:06:50,614: t15.2025.01.12 val PER: 0.1432
2026-01-04 12:06:50,614: t15.2025.03.14 val PER: 0.3373
2026-01-04 12:06:50,614: t15.2025.03.16 val PER: 0.1859
2026-01-04 12:06:50,615: t15.2025.03.30 val PER: 0.2897
2026-01-04 12:06:50,615: t15.2025.04.13 val PER: 0.2083
2026-01-04 12:06:50,936: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_18500
2026-01-04 12:07:01,226: Train batch 18600: loss: 12.83 grad norm: 67.43 time: 0.070
2026-01-04 12:07:21,950: Train batch 18800: loss: 8.33 grad norm: 48.97 time: 0.064
2026-01-04 12:07:42,989: Train batch 19000: loss: 8.25 grad norm: 46.65 time: 0.065
2026-01-04 12:07:42,991: Running test after training batch: 19000
2026-01-04 12:07:43,148: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:07:48,240: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:07:48,281: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 12:07:50,560: Val batch 19000: PER (avg): 0.1463 CTC Loss (avg): 15.0231 WER(1gram): 46.19% (n=64) time: 7.569
2026-01-04 12:07:50,560: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 12:07:50,561: t15.2023.08.13 val PER: 0.1060
2026-01-04 12:07:50,561: t15.2023.08.18 val PER: 0.0989
2026-01-04 12:07:50,561: t15.2023.08.20 val PER: 0.1088
2026-01-04 12:07:50,561: t15.2023.08.25 val PER: 0.0873
2026-01-04 12:07:50,561: t15.2023.08.27 val PER: 0.1865
2026-01-04 12:07:50,561: t15.2023.09.01 val PER: 0.0763
2026-01-04 12:07:50,561: t15.2023.09.03 val PER: 0.1615
2026-01-04 12:07:50,561: t15.2023.09.24 val PER: 0.1311
2026-01-04 12:07:50,561: t15.2023.09.29 val PER: 0.1315
2026-01-04 12:07:50,561: t15.2023.10.01 val PER: 0.1671
2026-01-04 12:07:50,561: t15.2023.10.06 val PER: 0.0840
2026-01-04 12:07:50,562: t15.2023.10.08 val PER: 0.2463
2026-01-04 12:07:50,562: t15.2023.10.13 val PER: 0.1893
2026-01-04 12:07:50,562: t15.2023.10.15 val PER: 0.1450
2026-01-04 12:07:50,562: t15.2023.10.20 val PER: 0.1980
2026-01-04 12:07:50,562: t15.2023.10.22 val PER: 0.1024
2026-01-04 12:07:50,562: t15.2023.11.03 val PER: 0.1791
2026-01-04 12:07:50,562: t15.2023.11.04 val PER: 0.0273
2026-01-04 12:07:50,563: t15.2023.11.17 val PER: 0.0451
2026-01-04 12:07:50,563: t15.2023.11.19 val PER: 0.0339
2026-01-04 12:07:50,563: t15.2023.11.26 val PER: 0.1109
2026-01-04 12:07:50,563: t15.2023.12.03 val PER: 0.1113
2026-01-04 12:07:50,563: t15.2023.12.08 val PER: 0.0905
2026-01-04 12:07:50,563: t15.2023.12.10 val PER: 0.0920
2026-01-04 12:07:50,563: t15.2023.12.17 val PER: 0.1310
2026-01-04 12:07:50,563: t15.2023.12.29 val PER: 0.1277
2026-01-04 12:07:50,563: t15.2024.02.25 val PER: 0.1039
2026-01-04 12:07:50,563: t15.2024.03.08 val PER: 0.2262
2026-01-04 12:07:50,563: t15.2024.03.15 val PER: 0.1989
2026-01-04 12:07:50,563: t15.2024.03.17 val PER: 0.1339
2026-01-04 12:07:50,563: t15.2024.05.10 val PER: 0.1516
2026-01-04 12:07:50,563: t15.2024.06.14 val PER: 0.1577
2026-01-04 12:07:50,563: t15.2024.07.19 val PER: 0.2274
2026-01-04 12:07:50,563: t15.2024.07.21 val PER: 0.0897
2026-01-04 12:07:50,563: t15.2024.07.28 val PER: 0.1316
2026-01-04 12:07:50,564: t15.2025.01.10 val PER: 0.2934
2026-01-04 12:07:50,660: t15.2025.01.12 val PER: 0.1432
2026-01-04 12:07:50,660: t15.2025.03.14 val PER: 0.3462
2026-01-04 12:07:50,660: t15.2025.03.16 val PER: 0.1832
2026-01-04 12:07:50,661: t15.2025.03.30 val PER: 0.2793
2026-01-04 12:07:50,661: t15.2025.04.13 val PER: 0.2054
2026-01-04 12:07:50,989: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_19000
2026-01-04 12:08:11,684: Train batch 19200: loss: 5.48 grad norm: 43.33 time: 0.063
2026-01-04 12:08:32,741: Train batch 19400: loss: 4.89 grad norm: 35.45 time: 0.053
2026-01-04 12:08:43,075: Running test after training batch: 19500
2026-01-04 12:08:43,233: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:08:48,329: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:08:48,371: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 12:08:50,670: Val batch 19500: PER (avg): 0.1461 CTC Loss (avg): 14.9748 WER(1gram): 46.19% (n=64) time: 7.595
2026-01-04 12:08:50,671: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 12:08:50,671: t15.2023.08.13 val PER: 0.1050
2026-01-04 12:08:50,671: t15.2023.08.18 val PER: 0.1014
2026-01-04 12:08:50,671: t15.2023.08.20 val PER: 0.1096
2026-01-04 12:08:50,671: t15.2023.08.25 val PER: 0.0919
2026-01-04 12:08:50,671: t15.2023.08.27 val PER: 0.1865
2026-01-04 12:08:50,671: t15.2023.09.01 val PER: 0.0722
2026-01-04 12:08:50,671: t15.2023.09.03 val PER: 0.1603
2026-01-04 12:08:50,671: t15.2023.09.24 val PER: 0.1299
2026-01-04 12:08:50,672: t15.2023.09.29 val PER: 0.1276
2026-01-04 12:08:50,672: t15.2023.10.01 val PER: 0.1678
2026-01-04 12:08:50,672: t15.2023.10.06 val PER: 0.0797
2026-01-04 12:08:50,672: t15.2023.10.08 val PER: 0.2544
2026-01-04 12:08:50,672: t15.2023.10.13 val PER: 0.1885
2026-01-04 12:08:50,672: t15.2023.10.15 val PER: 0.1417
2026-01-04 12:08:50,672: t15.2023.10.20 val PER: 0.2081
2026-01-04 12:08:50,673: t15.2023.10.22 val PER: 0.1080
2026-01-04 12:08:50,673: t15.2023.11.03 val PER: 0.1723
2026-01-04 12:08:50,673: t15.2023.11.04 val PER: 0.0307
2026-01-04 12:08:50,673: t15.2023.11.17 val PER: 0.0451
2026-01-04 12:08:50,673: t15.2023.11.19 val PER: 0.0339
2026-01-04 12:08:50,673: t15.2023.11.26 val PER: 0.1101
2026-01-04 12:08:50,673: t15.2023.12.03 val PER: 0.1103
2026-01-04 12:08:50,673: t15.2023.12.08 val PER: 0.0932
2026-01-04 12:08:50,673: t15.2023.12.10 val PER: 0.0933
2026-01-04 12:08:50,674: t15.2023.12.17 val PER: 0.1268
2026-01-04 12:08:50,674: t15.2023.12.29 val PER: 0.1304
2026-01-04 12:08:50,674: t15.2024.02.25 val PER: 0.0997
2026-01-04 12:08:50,674: t15.2024.03.08 val PER: 0.2376
2026-01-04 12:08:50,674: t15.2024.03.15 val PER: 0.1989
2026-01-04 12:08:50,675: t15.2024.03.17 val PER: 0.1353
2026-01-04 12:08:50,675: t15.2024.05.10 val PER: 0.1426
2026-01-04 12:08:50,675: t15.2024.06.14 val PER: 0.1546
2026-01-04 12:08:50,675: t15.2024.07.19 val PER: 0.2261
2026-01-04 12:08:50,675: t15.2024.07.21 val PER: 0.0897
2026-01-04 12:08:50,675: t15.2024.07.28 val PER: 0.1346
2026-01-04 12:08:50,675: t15.2025.01.10 val PER: 0.2920
2026-01-04 12:08:50,675: t15.2025.01.12 val PER: 0.1409
2026-01-04 12:08:50,676: t15.2025.03.14 val PER: 0.3417
2026-01-04 12:08:50,676: t15.2025.03.16 val PER: 0.1819
2026-01-04 12:08:50,676: t15.2025.03.30 val PER: 0.2828
2026-01-04 12:08:50,676: t15.2025.04.13 val PER: 0.2068
2026-01-04 12:08:50,990: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_19500
2026-01-04 12:09:01,228: Train batch 19600: loss: 7.55 grad norm: 48.45 time: 0.060
2026-01-04 12:09:21,770: Train batch 19800: loss: 7.51 grad norm: 51.63 time: 0.057
2026-01-04 12:09:42,626: Running test after training batch: 19999
2026-01-04 12:09:42,724: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:09:47,731: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:09:47,775: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 12:09:50,080: Val batch 19999: PER (avg): 0.1462 CTC Loss (avg): 14.9608 WER(1gram): 46.70% (n=64) time: 7.454
2026-01-04 12:09:50,081: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 12:09:50,081: t15.2023.08.13 val PER: 0.1029
2026-01-04 12:09:50,081: t15.2023.08.18 val PER: 0.1031
2026-01-04 12:09:50,081: t15.2023.08.20 val PER: 0.1112
2026-01-04 12:09:50,081: t15.2023.08.25 val PER: 0.0889
2026-01-04 12:09:50,081: t15.2023.08.27 val PER: 0.1833
2026-01-04 12:09:50,082: t15.2023.09.01 val PER: 0.0731
2026-01-04 12:09:50,082: t15.2023.09.03 val PER: 0.1615
2026-01-04 12:09:50,082: t15.2023.09.24 val PER: 0.1299
2026-01-04 12:09:50,082: t15.2023.09.29 val PER: 0.1270
2026-01-04 12:09:50,082: t15.2023.10.01 val PER: 0.1717
2026-01-04 12:09:50,082: t15.2023.10.06 val PER: 0.0797
2026-01-04 12:09:50,082: t15.2023.10.08 val PER: 0.2503
2026-01-04 12:09:50,082: t15.2023.10.13 val PER: 0.1908
2026-01-04 12:09:50,082: t15.2023.10.15 val PER: 0.1450
2026-01-04 12:09:50,082: t15.2023.10.20 val PER: 0.2013
2026-01-04 12:09:50,083: t15.2023.10.22 val PER: 0.1058
2026-01-04 12:09:50,083: t15.2023.11.03 val PER: 0.1737
2026-01-04 12:09:50,083: t15.2023.11.04 val PER: 0.0273
2026-01-04 12:09:50,083: t15.2023.11.17 val PER: 0.0435
2026-01-04 12:09:50,083: t15.2023.11.19 val PER: 0.0359
2026-01-04 12:09:50,083: t15.2023.11.26 val PER: 0.1094
2026-01-04 12:09:50,083: t15.2023.12.03 val PER: 0.1155
2026-01-04 12:09:50,083: t15.2023.12.08 val PER: 0.0945
2026-01-04 12:09:50,083: t15.2023.12.10 val PER: 0.0920
2026-01-04 12:09:50,084: t15.2023.12.17 val PER: 0.1216
2026-01-04 12:09:50,084: t15.2023.12.29 val PER: 0.1270
2026-01-04 12:09:50,084: t15.2024.02.25 val PER: 0.1011
2026-01-04 12:09:50,084: t15.2024.03.08 val PER: 0.2376
2026-01-04 12:09:50,085: t15.2024.03.15 val PER: 0.1957
2026-01-04 12:09:50,085: t15.2024.03.17 val PER: 0.1325
2026-01-04 12:09:50,085: t15.2024.05.10 val PER: 0.1471
2026-01-04 12:09:50,085: t15.2024.06.14 val PER: 0.1562
2026-01-04 12:09:50,085: t15.2024.07.19 val PER: 0.2274
2026-01-04 12:09:50,085: t15.2024.07.21 val PER: 0.0903
2026-01-04 12:09:50,085: t15.2024.07.28 val PER: 0.1331
2026-01-04 12:09:50,085: t15.2025.01.10 val PER: 0.2934
2026-01-04 12:09:50,085: t15.2025.01.12 val PER: 0.1424
2026-01-04 12:09:50,085: t15.2025.03.14 val PER: 0.3373
2026-01-04 12:09:50,085: t15.2025.03.16 val PER: 0.1806
2026-01-04 12:09:50,086: t15.2025.03.30 val PER: 0.2885
2026-01-04 12:09:50,086: t15.2025.04.13 val PER: 0.2097
2026-01-04 12:09:50,394: Saved model to checkpoint: /tmp/e12511253_b2t_348730/trained_models/weight_decay/lr40/base/checkpoint/checkpoint_batch_19999
2026-01-04 12:09:50,425: Best avg val PER achieved: 0.15662
2026-01-04 12:09:50,425: Total training time: 40.51 minutes

=== RUN wd1e-3.yaml ===
