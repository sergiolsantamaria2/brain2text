TMPDIR=/home/e12511253/tmp
JOB_TMP=/home/e12511253/tmp/e12511253_b2t_353635
TORCH_EXTENSIONS_DIR=/home/e12511253/tmp/e12511253_b2t_353635/torch_extensions
WANDB_DIR=/home/e12511253/tmp/e12511253_b2t_353635/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/home/e12511253/tmp/e12511253_b2t_353635/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan 11 14:59 /home/e12511253/tmp/e12511253_b2t_353635/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
OUT_ROOT=/home/e12511253/Brain2Text/brain2text/trained_models
==============================================
Job: b2t5g  ID: 353635
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_5gram_only.yaml
Folders: configs/experiments/diphones
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/diphones ==========
Num configs: 5

=== RUN diphone_40k.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k
2026-01-11 15:00:05,027: Using device: cuda:0
2026-01-11 15:04:38,259: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-11 15:04:38,261: Diphone mode ENABLED: n_classes changed from 41 to 1601
2026-01-11 15:04:49,146: Using 45 sessions after filtering (from 45).
2026-01-11 15:04:49,698: Using torch.compile (if available)
2026-01-11 15:04:49,698: torch.compile not available (torch<2.0). Skipping.
2026-01-11 15:04:49,698: Initialized RNN decoding model
2026-01-11 15:04:49,699: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Identity()
  (out): Linear(in_features=768, out_features=1601, bias=True)
)
2026-01-11 15:04:49,699: Model has 45,514,817 parameters
2026-01-11 15:04:49,699: Model has 11,819,520 day-specific parameters | 25.97% of total parameters
2026-01-11 15:04:56,881: Successfully initialized datasets
2026-01-11 15:04:56,882: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-11 15:04:59,143: Train batch 0: loss: 1387.45 grad norm: 201.84 time: 0.376
2026-01-11 15:04:59,144: Running test after training batch: 0
2026-01-11 15:04:59,265: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:05:06,628: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-11 15:05:07,910: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-11 15:10:33,422: Val batch 0: PER (avg): 4.4643 CTC Loss (avg): 1561.5818 WER(5gram): 100.00% (n=256) time: 334.278
2026-01-11 15:10:33,423: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=0
2026-01-11 15:10:33,424: t15.2023.08.13 val PER: 3.7588
2026-01-11 15:10:33,424: t15.2023.08.18 val PER: 4.0226
2026-01-11 15:10:33,424: t15.2023.08.20 val PER: 3.9674
2026-01-11 15:10:33,424: t15.2023.08.25 val PER: 3.9578
2026-01-11 15:10:33,424: t15.2023.08.27 val PER: 3.6897
2026-01-11 15:10:33,424: t15.2023.09.01 val PER: 4.0649
2026-01-11 15:10:33,424: t15.2023.09.03 val PER: 3.9430
2026-01-11 15:10:33,424: t15.2023.09.24 val PER: 4.6917
2026-01-11 15:10:33,425: t15.2023.09.29 val PER: 4.6254
2026-01-11 15:10:33,425: t15.2023.10.01 val PER: 3.6552
2026-01-11 15:10:33,425: t15.2023.10.06 val PER: 4.5178
2026-01-11 15:10:33,425: t15.2023.10.08 val PER: 3.3816
2026-01-11 15:10:33,425: t15.2023.10.13 val PER: 4.2475
2026-01-11 15:10:33,425: t15.2023.10.15 val PER: 4.7337
2026-01-11 15:10:33,425: t15.2023.10.20 val PER: 4.8859
2026-01-11 15:10:33,425: t15.2023.10.22 val PER: 4.7071
2026-01-11 15:10:33,425: t15.2023.11.03 val PER: 5.0102
2026-01-11 15:10:33,425: t15.2023.11.04 val PER: 6.2253
2026-01-11 15:10:33,426: t15.2023.11.17 val PER: 6.5490
2026-01-11 15:10:33,426: t15.2023.11.19 val PER: 4.9721
2026-01-11 15:10:33,426: t15.2023.11.26 val PER: 4.9486
2026-01-11 15:10:33,426: t15.2023.12.03 val PER: 4.5840
2026-01-11 15:10:33,426: t15.2023.12.08 val PER: 5.0752
2026-01-11 15:10:33,426: t15.2023.12.10 val PER: 5.5177
2026-01-11 15:10:33,427: t15.2023.12.17 val PER: 4.1258
2026-01-11 15:10:33,427: t15.2023.12.29 val PER: 4.4859
2026-01-11 15:10:33,427: t15.2024.02.25 val PER: 4.2275
2026-01-11 15:10:33,427: t15.2024.03.08 val PER: 4.2077
2026-01-11 15:10:33,427: t15.2024.03.15 val PER: 4.0638
2026-01-11 15:10:33,427: t15.2024.03.17 val PER: 4.3501
2026-01-11 15:10:33,427: t15.2024.05.10 val PER: 4.1530
2026-01-11 15:10:33,427: t15.2024.06.14 val PER: 4.7177
2026-01-11 15:10:33,427: t15.2024.07.19 val PER: 3.3072
2026-01-11 15:10:33,427: t15.2024.07.21 val PER: 5.0034
2026-01-11 15:10:33,428: t15.2024.07.28 val PER: 5.2147
2026-01-11 15:10:33,428: t15.2025.01.10 val PER: 3.1501
2026-01-11 15:10:33,428: t15.2025.01.12 val PER: 5.7567
2026-01-11 15:10:33,428: t15.2025.03.14 val PER: 3.0444
2026-01-11 15:10:33,428: t15.2025.03.16 val PER: 5.4974
2026-01-11 15:10:33,428: t15.2025.03.30 val PER: 4.2816
2026-01-11 15:10:33,428: t15.2025.04.13 val PER: 4.8631
2026-01-11 15:10:33,431: New best val WER(5gram) inf% --> 100.00%
2026-01-11 15:10:33,620: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_0
2026-01-11 15:10:54,183: Train batch 200: loss: 204.34 grad norm: 49.11 time: 0.063
2026-01-11 15:11:14,058: Train batch 400: loss: 149.15 grad norm: 48.04 time: 0.074
2026-01-11 15:11:24,096: Running test after training batch: 500
2026-01-11 15:11:24,268: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:11:31,063: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-11 15:11:31,236: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-11 15:12:11,648: Val batch 500: PER (avg): 0.9642 CTC Loss (avg): 171.3308 WER(5gram): 100.00% (n=256) time: 47.550
2026-01-11 15:12:11,649: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=0
2026-01-11 15:12:11,649: t15.2023.08.13 val PER: 0.9636
2026-01-11 15:12:11,649: t15.2023.08.18 val PER: 0.9564
2026-01-11 15:12:11,650: t15.2023.08.20 val PER: 0.9595
2026-01-11 15:12:11,650: t15.2023.08.25 val PER: 0.9608
2026-01-11 15:12:11,650: t15.2023.08.27 val PER: 0.9598
2026-01-11 15:12:11,650: t15.2023.09.01 val PER: 0.9586
2026-01-11 15:12:11,650: t15.2023.09.03 val PER: 0.9596
2026-01-11 15:12:11,650: t15.2023.09.24 val PER: 0.9563
2026-01-11 15:12:11,651: t15.2023.09.29 val PER: 0.9617
2026-01-11 15:12:11,651: t15.2023.10.01 val PER: 0.9703
2026-01-11 15:12:11,651: t15.2023.10.06 val PER: 0.9569
2026-01-11 15:12:11,651: t15.2023.10.08 val PER: 0.9702
2026-01-11 15:12:11,651: t15.2023.10.13 val PER: 0.9604
2026-01-11 15:12:11,652: t15.2023.10.15 val PER: 0.9677
2026-01-11 15:12:11,652: t15.2023.10.20 val PER: 0.9698
2026-01-11 15:12:11,652: t15.2023.10.22 val PER: 0.9599
2026-01-11 15:12:11,652: t15.2023.11.03 val PER: 0.9661
2026-01-11 15:12:11,652: t15.2023.11.04 val PER: 0.9488
2026-01-11 15:12:11,652: t15.2023.11.17 val PER: 0.9611
2026-01-11 15:12:11,652: t15.2023.11.19 val PER: 0.9601
2026-01-11 15:12:11,652: t15.2023.11.26 val PER: 0.9674
2026-01-11 15:12:11,653: t15.2023.12.03 val PER: 0.9643
2026-01-11 15:12:11,653: t15.2023.12.08 val PER: 0.9660
2026-01-11 15:12:11,653: t15.2023.12.10 val PER: 0.9671
2026-01-11 15:12:11,653: t15.2023.12.17 val PER: 0.9688
2026-01-11 15:12:11,653: t15.2023.12.29 val PER: 0.9657
2026-01-11 15:12:11,653: t15.2024.02.25 val PER: 0.9677
2026-01-11 15:12:11,653: t15.2024.03.08 val PER: 0.9630
2026-01-11 15:12:11,653: t15.2024.03.15 val PER: 0.9700
2026-01-11 15:12:11,654: t15.2024.03.17 val PER: 0.9658
2026-01-11 15:12:11,654: t15.2024.05.10 val PER: 0.9629
2026-01-11 15:12:11,654: t15.2024.06.14 val PER: 0.9606
2026-01-11 15:12:11,654: t15.2024.07.19 val PER: 0.9684
2026-01-11 15:12:11,654: t15.2024.07.21 val PER: 0.9683
2026-01-11 15:12:11,654: t15.2024.07.28 val PER: 0.9647
2026-01-11 15:12:11,654: t15.2025.01.10 val PER: 0.9683
2026-01-11 15:12:11,654: t15.2025.01.12 val PER: 0.9615
2026-01-11 15:12:11,654: t15.2025.03.14 val PER: 0.9645
2026-01-11 15:12:11,654: t15.2025.03.16 val PER: 0.9686
2026-01-11 15:12:11,655: t15.2025.03.30 val PER: 0.9644
2026-01-11 15:12:11,655: t15.2025.04.13 val PER: 0.9643
2026-01-11 15:12:11,832: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_500
2026-01-11 15:12:21,891: Train batch 600: loss: 157.04 grad norm: 29.64 time: 0.084
2026-01-11 15:12:41,934: Train batch 800: loss: 154.24 grad norm: 156.50 time: 0.072
2026-01-11 15:13:02,143: Train batch 1000: loss: 135.09 grad norm: 28.17 time: 0.073
2026-01-11 15:13:02,143: Running test after training batch: 1000
2026-01-11 15:13:02,258: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:13:08,818: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-11 15:13:08,837: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-11 15:13:16,416: Val batch 1000: PER (avg): 0.9851 CTC Loss (avg): 162.1572 WER(5gram): 99.87% (n=256) time: 14.272
2026-01-11 15:13:16,417: WER lens: avg_true_words=5.99 avg_pred_words=0.05 max_pred_words=1
2026-01-11 15:13:16,417: t15.2023.08.13 val PER: 0.9875
2026-01-11 15:13:16,417: t15.2023.08.18 val PER: 0.9866
2026-01-11 15:13:16,417: t15.2023.08.20 val PER: 0.9873
2026-01-11 15:13:16,417: t15.2023.08.25 val PER: 0.9819
2026-01-11 15:13:16,418: t15.2023.08.27 val PER: 0.9678
2026-01-11 15:13:16,418: t15.2023.09.01 val PER: 0.9846
2026-01-11 15:13:16,418: t15.2023.09.03 val PER: 0.9834
2026-01-11 15:13:16,418: t15.2023.09.24 val PER: 0.9915
2026-01-11 15:13:16,418: t15.2023.09.29 val PER: 0.9936
2026-01-11 15:13:16,418: t15.2023.10.01 val PER: 0.9934
2026-01-11 15:13:16,418: t15.2023.10.06 val PER: 0.9871
2026-01-11 15:13:16,418: t15.2023.10.08 val PER: 0.9946
2026-01-11 15:13:16,419: t15.2023.10.13 val PER: 0.9907
2026-01-11 15:13:16,419: t15.2023.10.15 val PER: 0.9895
2026-01-11 15:13:16,419: t15.2023.10.20 val PER: 0.9899
2026-01-11 15:13:16,419: t15.2023.10.22 val PER: 0.9855
2026-01-11 15:13:16,419: t15.2023.11.03 val PER: 0.9783
2026-01-11 15:13:16,419: t15.2023.11.04 val PER: 0.9829
2026-01-11 15:13:16,419: t15.2023.11.17 val PER: 0.9705
2026-01-11 15:13:16,419: t15.2023.11.19 val PER: 0.9820
2026-01-11 15:13:16,419: t15.2023.11.26 val PER: 0.9841
2026-01-11 15:13:16,420: t15.2023.12.03 val PER: 0.9811
2026-01-11 15:13:16,420: t15.2023.12.08 val PER: 0.9854
2026-01-11 15:13:16,420: t15.2023.12.10 val PER: 0.9842
2026-01-11 15:13:16,420: t15.2023.12.17 val PER: 0.9834
2026-01-11 15:13:16,420: t15.2023.12.29 val PER: 0.9822
2026-01-11 15:13:16,420: t15.2024.02.25 val PER: 0.9930
2026-01-11 15:13:16,420: t15.2024.03.08 val PER: 0.9801
2026-01-11 15:13:16,421: t15.2024.03.15 val PER: 0.9881
2026-01-11 15:13:16,421: t15.2024.03.17 val PER: 0.9944
2026-01-11 15:13:16,421: t15.2024.05.10 val PER: 0.9881
2026-01-11 15:13:16,421: t15.2024.06.14 val PER: 0.9826
2026-01-11 15:13:16,421: t15.2024.07.19 val PER: 0.9868
2026-01-11 15:13:16,421: t15.2024.07.21 val PER: 0.9883
2026-01-11 15:13:16,421: t15.2024.07.28 val PER: 0.9868
2026-01-11 15:13:16,421: t15.2025.01.10 val PER: 0.9725
2026-01-11 15:13:16,421: t15.2025.01.12 val PER: 0.9823
2026-01-11 15:13:16,421: t15.2025.03.14 val PER: 0.9689
2026-01-11 15:13:16,422: t15.2025.03.16 val PER: 0.9804
2026-01-11 15:13:16,422: t15.2025.03.30 val PER: 0.9736
2026-01-11 15:13:16,422: t15.2025.04.13 val PER: 0.9729
2026-01-11 15:13:16,423: New best val WER(5gram) 100.00% --> 99.87%
2026-01-11 15:13:16,600: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_1000
2026-01-11 15:13:37,078: Train batch 1200: loss: 149.47 grad norm: 42.41 time: 0.083
2026-01-11 15:13:57,821: Train batch 1400: loss: 134.80 grad norm: 85.87 time: 0.068
2026-01-11 15:14:07,751: Running test after training batch: 1500
2026-01-11 15:14:07,934: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:14:14,435: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-11 15:14:14,468: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-11 15:14:22,736: Val batch 1500: PER (avg): 0.9831 CTC Loss (avg): 146.2243 WER(5gram): 100.00% (n=256) time: 14.984
2026-01-11 15:14:22,736: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=1
2026-01-11 15:14:22,737: t15.2023.08.13 val PER: 0.9958
2026-01-11 15:14:22,737: t15.2023.08.18 val PER: 0.9807
2026-01-11 15:14:22,737: t15.2023.08.20 val PER: 0.9865
2026-01-11 15:14:22,737: t15.2023.08.25 val PER: 0.9729
2026-01-11 15:14:22,737: t15.2023.08.27 val PER: 0.9855
2026-01-11 15:14:22,737: t15.2023.09.01 val PER: 0.9651
2026-01-11 15:14:22,737: t15.2023.09.03 val PER: 0.9893
2026-01-11 15:14:22,737: t15.2023.09.24 val PER: 0.9745
2026-01-11 15:14:22,737: t15.2023.09.29 val PER: 0.9732
2026-01-11 15:14:22,737: t15.2023.10.01 val PER: 0.9828
2026-01-11 15:14:22,738: t15.2023.10.06 val PER: 0.9763
2026-01-11 15:14:22,738: t15.2023.10.08 val PER: 0.9892
2026-01-11 15:14:22,738: t15.2023.10.13 val PER: 0.9674
2026-01-11 15:14:22,738: t15.2023.10.15 val PER: 0.9644
2026-01-11 15:14:22,738: t15.2023.10.20 val PER: 0.9765
2026-01-11 15:14:22,738: t15.2023.10.22 val PER: 0.9733
2026-01-11 15:14:22,738: t15.2023.11.03 val PER: 0.9830
2026-01-11 15:14:22,738: t15.2023.11.04 val PER: 0.9898
2026-01-11 15:14:22,738: t15.2023.11.17 val PER: 0.9829
2026-01-11 15:14:22,738: t15.2023.11.19 val PER: 0.9940
2026-01-11 15:14:22,738: t15.2023.11.26 val PER: 0.9855
2026-01-11 15:14:22,739: t15.2023.12.03 val PER: 0.9895
2026-01-11 15:14:22,739: t15.2023.12.08 val PER: 0.9814
2026-01-11 15:14:22,739: t15.2023.12.10 val PER: 0.9816
2026-01-11 15:14:22,739: t15.2023.12.17 val PER: 0.9958
2026-01-11 15:14:22,739: t15.2023.12.29 val PER: 0.9911
2026-01-11 15:14:22,739: t15.2024.02.25 val PER: 0.9888
2026-01-11 15:14:22,739: t15.2024.03.08 val PER: 0.9972
2026-01-11 15:14:22,739: t15.2024.03.15 val PER: 0.9887
2026-01-11 15:14:22,739: t15.2024.03.17 val PER: 0.9854
2026-01-11 15:14:22,739: t15.2024.05.10 val PER: 0.9881
2026-01-11 15:14:22,739: t15.2024.06.14 val PER: 0.9826
2026-01-11 15:14:22,739: t15.2024.07.19 val PER: 0.9960
2026-01-11 15:14:22,740: t15.2024.07.21 val PER: 0.9828
2026-01-11 15:14:22,740: t15.2024.07.28 val PER: 0.9779
2026-01-11 15:14:22,740: t15.2025.01.10 val PER: 0.9904
2026-01-11 15:14:22,740: t15.2025.01.12 val PER: 0.9769
2026-01-11 15:14:22,740: t15.2025.03.14 val PER: 0.9985
2026-01-11 15:14:22,740: t15.2025.03.16 val PER: 0.9751
2026-01-11 15:14:22,740: t15.2025.03.30 val PER: 0.9966
2026-01-11 15:14:22,740: t15.2025.04.13 val PER: 0.9843
2026-01-11 15:14:22,908: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_1500
2026-01-11 15:14:32,960: Train batch 1600: loss: 134.01 grad norm: 38.08 time: 0.073
2026-01-11 15:14:53,343: Train batch 1800: loss: 124.11 grad norm: 78.34 time: 0.095
2026-01-11 15:15:13,657: Train batch 2000: loss: 117.38 grad norm: 72.44 time: 0.078
2026-01-11 15:15:13,658: Running test after training batch: 2000
2026-01-11 15:15:13,791: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:15:20,132: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-11 15:15:20,161: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-11 15:15:27,435: Val batch 2000: PER (avg): 0.8022 CTC Loss (avg): 121.6002 WER(5gram): 100.00% (n=256) time: 13.776
2026-01-11 15:15:27,435: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=1
2026-01-11 15:15:27,435: t15.2023.08.13 val PER: 0.7869
2026-01-11 15:15:27,435: t15.2023.08.18 val PER: 0.7770
2026-01-11 15:15:27,435: t15.2023.08.20 val PER: 0.7776
2026-01-11 15:15:27,436: t15.2023.08.25 val PER: 0.7816
2026-01-11 15:15:27,436: t15.2023.08.27 val PER: 0.8135
2026-01-11 15:15:27,436: t15.2023.09.01 val PER: 0.7841
2026-01-11 15:15:27,436: t15.2023.09.03 val PER: 0.8124
2026-01-11 15:15:27,436: t15.2023.09.24 val PER: 0.7913
2026-01-11 15:15:27,436: t15.2023.09.29 val PER: 0.7824
2026-01-11 15:15:27,436: t15.2023.10.01 val PER: 0.8025
2026-01-11 15:15:27,437: t15.2023.10.06 val PER: 0.7815
2026-01-11 15:15:27,437: t15.2023.10.08 val PER: 0.8160
2026-01-11 15:15:27,437: t15.2023.10.13 val PER: 0.8278
2026-01-11 15:15:27,437: t15.2023.10.15 val PER: 0.7956
2026-01-11 15:15:27,437: t15.2023.10.20 val PER: 0.7752
2026-01-11 15:15:27,437: t15.2023.10.22 val PER: 0.7895
2026-01-11 15:15:27,438: t15.2023.11.03 val PER: 0.8039
2026-01-11 15:15:27,438: t15.2023.11.04 val PER: 0.7509
2026-01-11 15:15:27,438: t15.2023.11.17 val PER: 0.7745
2026-01-11 15:15:27,438: t15.2023.11.19 val PER: 0.7585
2026-01-11 15:15:27,438: t15.2023.11.26 val PER: 0.8101
2026-01-11 15:15:27,438: t15.2023.12.03 val PER: 0.7878
2026-01-11 15:15:27,438: t15.2023.12.08 val PER: 0.7989
2026-01-11 15:15:27,439: t15.2023.12.10 val PER: 0.8003
2026-01-11 15:15:27,439: t15.2023.12.17 val PER: 0.8295
2026-01-11 15:15:27,439: t15.2023.12.29 val PER: 0.8126
2026-01-11 15:15:27,439: t15.2024.02.25 val PER: 0.7907
2026-01-11 15:15:27,440: t15.2024.03.08 val PER: 0.8137
2026-01-11 15:15:27,440: t15.2024.03.15 val PER: 0.8311
2026-01-11 15:15:27,440: t15.2024.03.17 val PER: 0.7978
2026-01-11 15:15:27,440: t15.2024.05.10 val PER: 0.7949
2026-01-11 15:15:27,440: t15.2024.06.14 val PER: 0.7965
2026-01-11 15:15:27,440: t15.2024.07.19 val PER: 0.8299
2026-01-11 15:15:27,440: t15.2024.07.21 val PER: 0.7862
2026-01-11 15:15:27,441: t15.2024.07.28 val PER: 0.7971
2026-01-11 15:15:27,441: t15.2025.01.10 val PER: 0.8719
2026-01-11 15:15:27,441: t15.2025.01.12 val PER: 0.7868
2026-01-11 15:15:27,441: t15.2025.03.14 val PER: 0.8669
2026-01-11 15:15:27,441: t15.2025.03.16 val PER: 0.7984
2026-01-11 15:15:27,441: t15.2025.03.30 val PER: 0.8598
2026-01-11 15:15:27,441: t15.2025.04.13 val PER: 0.7960
2026-01-11 15:15:27,607: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_2000
2026-01-11 15:15:47,778: Train batch 2200: loss: 103.29 grad norm: 58.03 time: 0.071
2026-01-11 15:16:08,657: Train batch 2400: loss: 95.11 grad norm: 56.57 time: 0.061
2026-01-11 15:16:19,057: Running test after training batch: 2500
2026-01-11 15:16:19,238: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:16:26,064: WER debug example
  GT : you can see the code at this point as well
  PR : the
2026-01-11 15:16:26,357: WER debug example
  GT : how does it keep the cost down
  PR : in the
2026-01-11 15:17:37,931: Val batch 2500: PER (avg): 0.5707 CTC Loss (avg): 101.3642 WER(5gram): 94.72% (n=256) time: 78.873
2026-01-11 15:17:37,932: WER lens: avg_true_words=5.99 avg_pred_words=0.73 max_pred_words=4
2026-01-11 15:17:37,932: t15.2023.08.13 val PER: 0.5364
2026-01-11 15:17:37,932: t15.2023.08.18 val PER: 0.5423
2026-01-11 15:17:37,932: t15.2023.08.20 val PER: 0.5226
2026-01-11 15:17:37,933: t15.2023.08.25 val PER: 0.5211
2026-01-11 15:17:37,933: t15.2023.08.27 val PER: 0.6158
2026-01-11 15:17:37,933: t15.2023.09.01 val PER: 0.5154
2026-01-11 15:17:37,933: t15.2023.09.03 val PER: 0.5689
2026-01-11 15:17:37,933: t15.2023.09.24 val PER: 0.5619
2026-01-11 15:17:37,933: t15.2023.09.29 val PER: 0.5533
2026-01-11 15:17:37,933: t15.2023.10.01 val PER: 0.6011
2026-01-11 15:17:37,933: t15.2023.10.06 val PER: 0.5597
2026-01-11 15:17:37,933: t15.2023.10.08 val PER: 0.6089
2026-01-11 15:17:37,934: t15.2023.10.13 val PER: 0.6439
2026-01-11 15:17:37,934: t15.2023.10.15 val PER: 0.5715
2026-01-11 15:17:37,934: t15.2023.10.20 val PER: 0.5671
2026-01-11 15:17:37,934: t15.2023.10.22 val PER: 0.5379
2026-01-11 15:17:37,934: t15.2023.11.03 val PER: 0.5434
2026-01-11 15:17:37,934: t15.2023.11.04 val PER: 0.4369
2026-01-11 15:17:37,934: t15.2023.11.17 val PER: 0.4712
2026-01-11 15:17:37,934: t15.2023.11.19 val PER: 0.4790
2026-01-11 15:17:37,934: t15.2023.11.26 val PER: 0.6167
2026-01-11 15:17:37,935: t15.2023.12.03 val PER: 0.5651
2026-01-11 15:17:37,935: t15.2023.12.08 val PER: 0.5652
2026-01-11 15:17:37,935: t15.2023.12.10 val PER: 0.5545
2026-01-11 15:17:37,935: t15.2023.12.17 val PER: 0.5696
2026-01-11 15:17:37,935: t15.2023.12.29 val PER: 0.5731
2026-01-11 15:17:37,935: t15.2024.02.25 val PER: 0.5421
2026-01-11 15:17:37,935: t15.2024.03.08 val PER: 0.5690
2026-01-11 15:17:37,935: t15.2024.03.15 val PER: 0.5810
2026-01-11 15:17:37,935: t15.2024.03.17 val PER: 0.5732
2026-01-11 15:17:37,935: t15.2024.05.10 val PER: 0.5483
2026-01-11 15:17:37,935: t15.2024.06.14 val PER: 0.5363
2026-01-11 15:17:37,935: t15.2024.07.19 val PER: 0.6256
2026-01-11 15:17:37,936: t15.2024.07.21 val PER: 0.5386
2026-01-11 15:17:37,936: t15.2024.07.28 val PER: 0.5647
2026-01-11 15:17:37,936: t15.2025.01.10 val PER: 0.6556
2026-01-11 15:17:37,936: t15.2025.01.12 val PER: 0.5912
2026-01-11 15:17:37,936: t15.2025.03.14 val PER: 0.6405
2026-01-11 15:17:37,936: t15.2025.03.16 val PER: 0.6217
2026-01-11 15:17:37,936: t15.2025.03.30 val PER: 0.6540
2026-01-11 15:17:37,936: t15.2025.04.13 val PER: 0.6006
2026-01-11 15:17:37,937: New best val WER(5gram) 99.87% --> 94.72%
2026-01-11 15:17:38,117: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_2500
2026-01-11 15:17:48,077: Train batch 2600: loss: 102.34 grad norm: 91.09 time: 0.069
2026-01-11 15:18:08,950: Train batch 2800: loss: 81.30 grad norm: 58.80 time: 0.089
2026-01-11 15:18:29,424: Train batch 3000: loss: 91.98 grad norm: 95.67 time: 0.089
2026-01-11 15:18:29,425: Running test after training batch: 3000
2026-01-11 15:18:29,542: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:18:36,068: WER debug example
  GT : you can see the code at this point as well
  PR : this is why
2026-01-11 15:18:36,318: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the
2026-01-11 15:19:51,917: Val batch 3000: PER (avg): 0.5090 CTC Loss (avg): 90.5815 WER(5gram): 85.53% (n=256) time: 82.492
2026-01-11 15:19:51,917: WER lens: avg_true_words=5.99 avg_pred_words=3.05 max_pred_words=10
2026-01-11 15:19:51,917: t15.2023.08.13 val PER: 0.4834
2026-01-11 15:19:51,918: t15.2023.08.18 val PER: 0.4736
2026-01-11 15:19:51,918: t15.2023.08.20 val PER: 0.4512
2026-01-11 15:19:51,918: t15.2023.08.25 val PER: 0.4503
2026-01-11 15:19:51,918: t15.2023.08.27 val PER: 0.5547
2026-01-11 15:19:51,918: t15.2023.09.01 val PER: 0.4383
2026-01-11 15:19:51,918: t15.2023.09.03 val PER: 0.5119
2026-01-11 15:19:51,918: t15.2023.09.24 val PER: 0.4806
2026-01-11 15:19:51,918: t15.2023.09.29 val PER: 0.4907
2026-01-11 15:19:51,919: t15.2023.10.01 val PER: 0.5297
2026-01-11 15:19:51,919: t15.2023.10.06 val PER: 0.4855
2026-01-11 15:19:51,919: t15.2023.10.08 val PER: 0.5670
2026-01-11 15:19:51,919: t15.2023.10.13 val PER: 0.5912
2026-01-11 15:19:51,919: t15.2023.10.15 val PER: 0.5135
2026-01-11 15:19:51,919: t15.2023.10.20 val PER: 0.5101
2026-01-11 15:19:51,919: t15.2023.10.22 val PER: 0.4822
2026-01-11 15:19:51,919: t15.2023.11.03 val PER: 0.4919
2026-01-11 15:19:51,919: t15.2023.11.04 val PER: 0.3652
2026-01-11 15:19:51,920: t15.2023.11.17 val PER: 0.4044
2026-01-11 15:19:51,920: t15.2023.11.19 val PER: 0.3832
2026-01-11 15:19:51,920: t15.2023.11.26 val PER: 0.5674
2026-01-11 15:19:51,920: t15.2023.12.03 val PER: 0.4989
2026-01-11 15:19:51,920: t15.2023.12.08 val PER: 0.5067
2026-01-11 15:19:51,920: t15.2023.12.10 val PER: 0.5085
2026-01-11 15:19:51,920: t15.2023.12.17 val PER: 0.5094
2026-01-11 15:19:51,920: t15.2023.12.29 val PER: 0.5244
2026-01-11 15:19:51,920: t15.2024.02.25 val PER: 0.4621
2026-01-11 15:19:51,920: t15.2024.03.08 val PER: 0.5220
2026-01-11 15:19:51,920: t15.2024.03.15 val PER: 0.5366
2026-01-11 15:19:51,921: t15.2024.03.17 val PER: 0.4930
2026-01-11 15:19:51,921: t15.2024.05.10 val PER: 0.5037
2026-01-11 15:19:51,921: t15.2024.06.14 val PER: 0.4700
2026-01-11 15:19:51,921: t15.2024.07.19 val PER: 0.5636
2026-01-11 15:19:51,921: t15.2024.07.21 val PER: 0.4614
2026-01-11 15:19:51,922: t15.2024.07.28 val PER: 0.4956
2026-01-11 15:19:51,922: t15.2025.01.10 val PER: 0.5882
2026-01-11 15:19:51,922: t15.2025.01.12 val PER: 0.5212
2026-01-11 15:19:51,922: t15.2025.03.14 val PER: 0.6080
2026-01-11 15:19:51,922: t15.2025.03.16 val PER: 0.5589
2026-01-11 15:19:51,922: t15.2025.03.30 val PER: 0.6092
2026-01-11 15:19:51,922: t15.2025.04.13 val PER: 0.5435
2026-01-11 15:19:51,923: New best val WER(5gram) 94.72% --> 85.53%
2026-01-11 15:19:52,106: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_3000
2026-01-11 15:20:12,732: Train batch 3200: loss: 79.89 grad norm: 72.17 time: 0.084
2026-01-11 15:20:32,773: Train batch 3400: loss: 67.75 grad norm: 66.40 time: 0.058
2026-01-11 15:20:42,678: Running test after training batch: 3500
2026-01-11 15:20:42,829: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:20:50,116: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy in the
2026-01-11 15:20:50,511: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the
2026-01-11 15:21:57,694: Val batch 3500: PER (avg): 0.4670 CTC Loss (avg): 83.3959 WER(5gram): 78.55% (n=256) time: 75.015
2026-01-11 15:21:57,695: WER lens: avg_true_words=5.99 avg_pred_words=4.40 max_pred_words=11
2026-01-11 15:21:57,695: t15.2023.08.13 val PER: 0.4220
2026-01-11 15:21:57,696: t15.2023.08.18 val PER: 0.4367
2026-01-11 15:21:57,696: t15.2023.08.20 val PER: 0.4178
2026-01-11 15:21:57,699: t15.2023.08.25 val PER: 0.3946
2026-01-11 15:21:57,699: t15.2023.08.27 val PER: 0.5113
2026-01-11 15:21:57,699: t15.2023.09.01 val PER: 0.3961
2026-01-11 15:21:57,699: t15.2023.09.03 val PER: 0.4549
2026-01-11 15:21:57,699: t15.2023.09.24 val PER: 0.4478
2026-01-11 15:21:57,699: t15.2023.09.29 val PER: 0.4505
2026-01-11 15:21:57,699: t15.2023.10.01 val PER: 0.4848
2026-01-11 15:21:57,699: t15.2023.10.06 val PER: 0.4403
2026-01-11 15:21:57,700: t15.2023.10.08 val PER: 0.5386
2026-01-11 15:21:57,700: t15.2023.10.13 val PER: 0.5493
2026-01-11 15:21:57,700: t15.2023.10.15 val PER: 0.4720
2026-01-11 15:21:57,700: t15.2023.10.20 val PER: 0.4732
2026-01-11 15:21:57,700: t15.2023.10.22 val PER: 0.4477
2026-01-11 15:21:57,700: t15.2023.11.03 val PER: 0.4315
2026-01-11 15:21:57,700: t15.2023.11.04 val PER: 0.2799
2026-01-11 15:21:57,700: t15.2023.11.17 val PER: 0.3453
2026-01-11 15:21:57,700: t15.2023.11.19 val PER: 0.3273
2026-01-11 15:21:57,700: t15.2023.11.26 val PER: 0.5109
2026-01-11 15:21:57,700: t15.2023.12.03 val PER: 0.4580
2026-01-11 15:21:57,700: t15.2023.12.08 val PER: 0.4621
2026-01-11 15:21:57,701: t15.2023.12.10 val PER: 0.4402
2026-01-11 15:21:57,701: t15.2023.12.17 val PER: 0.4501
2026-01-11 15:21:57,701: t15.2023.12.29 val PER: 0.4777
2026-01-11 15:21:57,701: t15.2024.02.25 val PER: 0.4115
2026-01-11 15:21:57,701: t15.2024.03.08 val PER: 0.4993
2026-01-11 15:21:57,701: t15.2024.03.15 val PER: 0.4828
2026-01-11 15:21:57,701: t15.2024.03.17 val PER: 0.4651
2026-01-11 15:21:57,702: t15.2024.05.10 val PER: 0.4591
2026-01-11 15:21:57,702: t15.2024.06.14 val PER: 0.4353
2026-01-11 15:21:57,702: t15.2024.07.19 val PER: 0.5564
2026-01-11 15:21:57,702: t15.2024.07.21 val PER: 0.4352
2026-01-11 15:21:57,702: t15.2024.07.28 val PER: 0.4566
2026-01-11 15:21:57,702: t15.2025.01.10 val PER: 0.5537
2026-01-11 15:21:57,702: t15.2025.01.12 val PER: 0.4711
2026-01-11 15:21:57,702: t15.2025.03.14 val PER: 0.5828
2026-01-11 15:21:57,702: t15.2025.03.16 val PER: 0.5288
2026-01-11 15:21:57,703: t15.2025.03.30 val PER: 0.5885
2026-01-11 15:21:57,703: t15.2025.04.13 val PER: 0.5007
2026-01-11 15:21:57,705: New best val WER(5gram) 85.53% --> 78.55%
2026-01-11 15:21:57,887: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_3500
2026-01-11 15:22:08,291: Train batch 3600: loss: 71.61 grad norm: 71.50 time: 0.075
2026-01-11 15:22:29,258: Train batch 3800: loss: 78.41 grad norm: 72.60 time: 0.079
2026-01-11 15:22:50,130: Train batch 4000: loss: 62.57 grad norm: 68.56 time: 0.063
2026-01-11 15:22:50,130: Running test after training batch: 4000
2026-01-11 15:22:50,253: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:22:57,346: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy in the
2026-01-11 15:22:57,667: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the way the
2026-01-11 15:24:14,207: Val batch 4000: PER (avg): 0.4399 CTC Loss (avg): 77.3057 WER(5gram): 76.79% (n=256) time: 84.077
2026-01-11 15:24:14,208: WER lens: avg_true_words=5.99 avg_pred_words=4.87 max_pred_words=10
2026-01-11 15:24:14,208: t15.2023.08.13 val PER: 0.3992
2026-01-11 15:24:14,209: t15.2023.08.18 val PER: 0.3998
2026-01-11 15:24:14,209: t15.2023.08.20 val PER: 0.3908
2026-01-11 15:24:14,209: t15.2023.08.25 val PER: 0.3735
2026-01-11 15:24:14,209: t15.2023.08.27 val PER: 0.4678
2026-01-11 15:24:14,209: t15.2023.09.01 val PER: 0.3807
2026-01-11 15:24:14,210: t15.2023.09.03 val PER: 0.4371
2026-01-11 15:24:14,210: t15.2023.09.24 val PER: 0.4029
2026-01-11 15:24:14,210: t15.2023.09.29 val PER: 0.4314
2026-01-11 15:24:14,210: t15.2023.10.01 val PER: 0.4676
2026-01-11 15:24:14,210: t15.2023.10.06 val PER: 0.4080
2026-01-11 15:24:14,211: t15.2023.10.08 val PER: 0.5074
2026-01-11 15:24:14,211: t15.2023.10.13 val PER: 0.5330
2026-01-11 15:24:14,211: t15.2023.10.15 val PER: 0.4436
2026-01-11 15:24:14,211: t15.2023.10.20 val PER: 0.4396
2026-01-11 15:24:14,211: t15.2023.10.22 val PER: 0.4065
2026-01-11 15:24:14,211: t15.2023.11.03 val PER: 0.4138
2026-01-11 15:24:14,211: t15.2023.11.04 val PER: 0.2491
2026-01-11 15:24:14,212: t15.2023.11.17 val PER: 0.3126
2026-01-11 15:24:14,212: t15.2023.11.19 val PER: 0.2874
2026-01-11 15:24:14,212: t15.2023.11.26 val PER: 0.5051
2026-01-11 15:24:14,212: t15.2023.12.03 val PER: 0.4317
2026-01-11 15:24:14,212: t15.2023.12.08 val PER: 0.4414
2026-01-11 15:24:14,212: t15.2023.12.10 val PER: 0.4218
2026-01-11 15:24:14,212: t15.2023.12.17 val PER: 0.4189
2026-01-11 15:24:14,213: t15.2023.12.29 val PER: 0.4653
2026-01-11 15:24:14,213: t15.2024.02.25 val PER: 0.4017
2026-01-11 15:24:14,213: t15.2024.03.08 val PER: 0.4651
2026-01-11 15:24:14,213: t15.2024.03.15 val PER: 0.4640
2026-01-11 15:24:14,213: t15.2024.03.17 val PER: 0.4296
2026-01-11 15:24:14,213: t15.2024.05.10 val PER: 0.4398
2026-01-11 15:24:14,213: t15.2024.06.14 val PER: 0.3770
2026-01-11 15:24:14,213: t15.2024.07.19 val PER: 0.5122
2026-01-11 15:24:14,213: t15.2024.07.21 val PER: 0.3972
2026-01-11 15:24:14,213: t15.2024.07.28 val PER: 0.4412
2026-01-11 15:24:14,213: t15.2025.01.10 val PER: 0.5234
2026-01-11 15:24:14,213: t15.2025.01.12 val PER: 0.4419
2026-01-11 15:24:14,213: t15.2025.03.14 val PER: 0.5311
2026-01-11 15:24:14,214: t15.2025.03.16 val PER: 0.4908
2026-01-11 15:24:14,214: t15.2025.03.30 val PER: 0.5368
2026-01-11 15:24:14,214: t15.2025.04.13 val PER: 0.4822
2026-01-11 15:24:14,216: New best val WER(5gram) 78.55% --> 76.79%
2026-01-11 15:24:14,399: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_4000
2026-01-11 15:24:34,820: Train batch 4200: loss: 65.07 grad norm: 84.63 time: 0.087
2026-01-11 15:24:55,853: Train batch 4400: loss: 57.18 grad norm: 69.33 time: 0.082
2026-01-11 15:25:05,902: Running test after training batch: 4500
2026-01-11 15:25:06,056: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:25:12,847: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code this is why
2026-01-11 15:25:13,115: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the
2026-01-11 15:26:21,391: Val batch 4500: PER (avg): 0.4175 CTC Loss (avg): 72.3406 WER(5gram): 74.58% (n=256) time: 75.488
2026-01-11 15:26:21,391: WER lens: avg_true_words=5.99 avg_pred_words=4.86 max_pred_words=11
2026-01-11 15:26:21,392: t15.2023.08.13 val PER: 0.3846
2026-01-11 15:26:21,392: t15.2023.08.18 val PER: 0.3772
2026-01-11 15:26:21,392: t15.2023.08.20 val PER: 0.3693
2026-01-11 15:26:21,392: t15.2023.08.25 val PER: 0.3494
2026-01-11 15:26:21,392: t15.2023.08.27 val PER: 0.4630
2026-01-11 15:26:21,392: t15.2023.09.01 val PER: 0.3515
2026-01-11 15:26:21,392: t15.2023.09.03 val PER: 0.4264
2026-01-11 15:26:21,393: t15.2023.09.24 val PER: 0.3714
2026-01-11 15:26:21,393: t15.2023.09.29 val PER: 0.4020
2026-01-11 15:26:21,393: t15.2023.10.01 val PER: 0.4524
2026-01-11 15:26:21,393: t15.2023.10.06 val PER: 0.3649
2026-01-11 15:26:21,393: t15.2023.10.08 val PER: 0.4871
2026-01-11 15:26:21,393: t15.2023.10.13 val PER: 0.5136
2026-01-11 15:26:21,393: t15.2023.10.15 val PER: 0.4186
2026-01-11 15:26:21,393: t15.2023.10.20 val PER: 0.4463
2026-01-11 15:26:21,393: t15.2023.10.22 val PER: 0.3786
2026-01-11 15:26:21,393: t15.2023.11.03 val PER: 0.4057
2026-01-11 15:26:21,393: t15.2023.11.04 val PER: 0.1877
2026-01-11 15:26:21,394: t15.2023.11.17 val PER: 0.2877
2026-01-11 15:26:21,394: t15.2023.11.19 val PER: 0.2675
2026-01-11 15:26:21,394: t15.2023.11.26 val PER: 0.4732
2026-01-11 15:26:21,394: t15.2023.12.03 val PER: 0.3908
2026-01-11 15:26:21,394: t15.2023.12.08 val PER: 0.4261
2026-01-11 15:26:21,394: t15.2023.12.10 val PER: 0.3890
2026-01-11 15:26:21,394: t15.2023.12.17 val PER: 0.4054
2026-01-11 15:26:21,395: t15.2023.12.29 val PER: 0.4338
2026-01-11 15:26:21,395: t15.2024.02.25 val PER: 0.3596
2026-01-11 15:26:21,395: t15.2024.03.08 val PER: 0.4438
2026-01-11 15:26:21,395: t15.2024.03.15 val PER: 0.4447
2026-01-11 15:26:21,395: t15.2024.03.17 val PER: 0.4156
2026-01-11 15:26:21,395: t15.2024.05.10 val PER: 0.4086
2026-01-11 15:26:21,395: t15.2024.06.14 val PER: 0.3770
2026-01-11 15:26:21,395: t15.2024.07.19 val PER: 0.4898
2026-01-11 15:26:21,395: t15.2024.07.21 val PER: 0.3710
2026-01-11 15:26:21,395: t15.2024.07.28 val PER: 0.4184
2026-01-11 15:26:21,395: t15.2025.01.10 val PER: 0.5014
2026-01-11 15:26:21,396: t15.2025.01.12 val PER: 0.4219
2026-01-11 15:26:21,396: t15.2025.03.14 val PER: 0.5281
2026-01-11 15:26:21,396: t15.2025.03.16 val PER: 0.4634
2026-01-11 15:26:21,396: t15.2025.03.30 val PER: 0.5230
2026-01-11 15:26:21,396: t15.2025.04.13 val PER: 0.4608
2026-01-11 15:26:21,397: New best val WER(5gram) 76.79% --> 74.58%
2026-01-11 15:26:21,594: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_4500
2026-01-11 15:26:31,425: Train batch 4600: loss: 61.93 grad norm: 82.25 time: 0.071
2026-01-11 15:26:52,087: Train batch 4800: loss: 50.46 grad norm: 71.96 time: 0.073
2026-01-11 15:27:12,867: Train batch 5000: loss: 87.08 grad norm: 105.69 time: 0.086
2026-01-11 15:27:12,868: Running test after training batch: 5000
2026-01-11 15:27:12,999: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:27:19,545: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at the
2026-01-11 15:27:19,751: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the way the
2026-01-11 15:28:07,275: Val batch 5000: PER (avg): 0.4002 CTC Loss (avg): 68.1534 WER(5gram): 71.32% (n=256) time: 54.407
2026-01-11 15:28:07,276: WER lens: avg_true_words=5.99 avg_pred_words=5.05 max_pred_words=10
2026-01-11 15:28:07,276: t15.2023.08.13 val PER: 0.3628
2026-01-11 15:28:07,276: t15.2023.08.18 val PER: 0.3764
2026-01-11 15:28:07,276: t15.2023.08.20 val PER: 0.3463
2026-01-11 15:28:07,276: t15.2023.08.25 val PER: 0.3419
2026-01-11 15:28:07,276: t15.2023.08.27 val PER: 0.4437
2026-01-11 15:28:07,276: t15.2023.09.01 val PER: 0.3287
2026-01-11 15:28:07,276: t15.2023.09.03 val PER: 0.4002
2026-01-11 15:28:07,277: t15.2023.09.24 val PER: 0.3483
2026-01-11 15:28:07,277: t15.2023.09.29 val PER: 0.3867
2026-01-11 15:28:07,277: t15.2023.10.01 val PER: 0.4386
2026-01-11 15:28:07,277: t15.2023.10.06 val PER: 0.3477
2026-01-11 15:28:07,277: t15.2023.10.08 val PER: 0.4668
2026-01-11 15:28:07,277: t15.2023.10.13 val PER: 0.4950
2026-01-11 15:28:07,277: t15.2023.10.15 val PER: 0.4094
2026-01-11 15:28:07,277: t15.2023.10.20 val PER: 0.3993
2026-01-11 15:28:07,277: t15.2023.10.22 val PER: 0.3597
2026-01-11 15:28:07,278: t15.2023.11.03 val PER: 0.3725
2026-01-11 15:28:07,278: t15.2023.11.04 val PER: 0.1877
2026-01-11 15:28:07,278: t15.2023.11.17 val PER: 0.2799
2026-01-11 15:28:07,278: t15.2023.11.19 val PER: 0.2435
2026-01-11 15:28:07,278: t15.2023.11.26 val PER: 0.4710
2026-01-11 15:28:07,278: t15.2023.12.03 val PER: 0.3929
2026-01-11 15:28:07,278: t15.2023.12.08 val PER: 0.4108
2026-01-11 15:28:07,278: t15.2023.12.10 val PER: 0.3837
2026-01-11 15:28:07,278: t15.2023.12.17 val PER: 0.3763
2026-01-11 15:28:07,279: t15.2023.12.29 val PER: 0.4159
2026-01-11 15:28:07,279: t15.2024.02.25 val PER: 0.3427
2026-01-11 15:28:07,279: t15.2024.03.08 val PER: 0.4353
2026-01-11 15:28:07,279: t15.2024.03.15 val PER: 0.4246
2026-01-11 15:28:07,279: t15.2024.03.17 val PER: 0.3968
2026-01-11 15:28:07,279: t15.2024.05.10 val PER: 0.4056
2026-01-11 15:28:07,279: t15.2024.06.14 val PER: 0.3533
2026-01-11 15:28:07,279: t15.2024.07.19 val PER: 0.4621
2026-01-11 15:28:07,279: t15.2024.07.21 val PER: 0.3614
2026-01-11 15:28:07,280: t15.2024.07.28 val PER: 0.3882
2026-01-11 15:28:07,280: t15.2025.01.10 val PER: 0.4931
2026-01-11 15:28:07,280: t15.2025.01.12 val PER: 0.3949
2026-01-11 15:28:07,280: t15.2025.03.14 val PER: 0.5178
2026-01-11 15:28:07,280: t15.2025.03.16 val PER: 0.4529
2026-01-11 15:28:07,280: t15.2025.03.30 val PER: 0.4897
2026-01-11 15:28:07,280: t15.2025.04.13 val PER: 0.4465
2026-01-11 15:28:07,281: New best val WER(5gram) 74.58% --> 71.32%
2026-01-11 15:28:07,450: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_5000
2026-01-11 15:28:28,356: Train batch 5200: loss: 57.93 grad norm: 78.59 time: 0.064
2026-01-11 15:28:49,186: Train batch 5400: loss: 62.35 grad norm: 74.92 time: 0.084
2026-01-11 15:28:59,714: Running test after training batch: 5500
2026-01-11 15:28:59,935: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:29:06,997: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at the
2026-01-11 15:29:07,194: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the sa new
2026-01-11 15:30:02,211: Val batch 5500: PER (avg): 0.3885 CTC Loss (avg): 64.9343 WER(5gram): 72.10% (n=256) time: 62.497
2026-01-11 15:30:02,212: WER lens: avg_true_words=5.99 avg_pred_words=4.94 max_pred_words=12
2026-01-11 15:30:02,212: t15.2023.08.13 val PER: 0.3607
2026-01-11 15:30:02,212: t15.2023.08.18 val PER: 0.3663
2026-01-11 15:30:02,212: t15.2023.08.20 val PER: 0.3392
2026-01-11 15:30:02,212: t15.2023.08.25 val PER: 0.3193
2026-01-11 15:30:02,212: t15.2023.08.27 val PER: 0.4453
2026-01-11 15:30:02,213: t15.2023.09.01 val PER: 0.3093
2026-01-11 15:30:02,213: t15.2023.09.03 val PER: 0.3872
2026-01-11 15:30:02,213: t15.2023.09.24 val PER: 0.3483
2026-01-11 15:30:02,213: t15.2023.09.29 val PER: 0.3714
2026-01-11 15:30:02,213: t15.2023.10.01 val PER: 0.4333
2026-01-11 15:30:02,213: t15.2023.10.06 val PER: 0.3315
2026-01-11 15:30:02,213: t15.2023.10.08 val PER: 0.4655
2026-01-11 15:30:02,213: t15.2023.10.13 val PER: 0.4856
2026-01-11 15:30:02,213: t15.2023.10.15 val PER: 0.3896
2026-01-11 15:30:02,213: t15.2023.10.20 val PER: 0.3960
2026-01-11 15:30:02,214: t15.2023.10.22 val PER: 0.3497
2026-01-11 15:30:02,214: t15.2023.11.03 val PER: 0.3765
2026-01-11 15:30:02,214: t15.2023.11.04 val PER: 0.1604
2026-01-11 15:30:02,214: t15.2023.11.17 val PER: 0.2457
2026-01-11 15:30:02,214: t15.2023.11.19 val PER: 0.2495
2026-01-11 15:30:02,214: t15.2023.11.26 val PER: 0.4406
2026-01-11 15:30:02,214: t15.2023.12.03 val PER: 0.3750
2026-01-11 15:30:02,214: t15.2023.12.08 val PER: 0.3961
2026-01-11 15:30:02,214: t15.2023.12.10 val PER: 0.3627
2026-01-11 15:30:02,214: t15.2023.12.17 val PER: 0.3701
2026-01-11 15:30:02,214: t15.2023.12.29 val PER: 0.4118
2026-01-11 15:30:02,214: t15.2024.02.25 val PER: 0.3343
2026-01-11 15:30:02,214: t15.2024.03.08 val PER: 0.4211
2026-01-11 15:30:02,215: t15.2024.03.15 val PER: 0.4078
2026-01-11 15:30:02,215: t15.2024.03.17 val PER: 0.3731
2026-01-11 15:30:02,215: t15.2024.05.10 val PER: 0.3938
2026-01-11 15:30:02,215: t15.2024.06.14 val PER: 0.3628
2026-01-11 15:30:02,215: t15.2024.07.19 val PER: 0.4529
2026-01-11 15:30:02,215: t15.2024.07.21 val PER: 0.3352
2026-01-11 15:30:02,215: t15.2024.07.28 val PER: 0.3794
2026-01-11 15:30:02,215: t15.2025.01.10 val PER: 0.4835
2026-01-11 15:30:02,216: t15.2025.01.12 val PER: 0.3888
2026-01-11 15:30:02,216: t15.2025.03.14 val PER: 0.4941
2026-01-11 15:30:02,216: t15.2025.03.16 val PER: 0.4542
2026-01-11 15:30:02,216: t15.2025.03.30 val PER: 0.4851
2026-01-11 15:30:02,216: t15.2025.04.13 val PER: 0.4365
2026-01-11 15:30:02,383: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_5500
2026-01-11 15:30:12,864: Train batch 5600: loss: 61.33 grad norm: 98.02 time: 0.080
2026-01-11 15:30:33,893: Train batch 5800: loss: 48.15 grad norm: 86.78 time: 0.091
2026-01-11 15:30:55,119: Train batch 6000: loss: 44.17 grad norm: 83.16 time: 0.057
2026-01-11 15:30:55,119: Running test after training batch: 6000
2026-01-11 15:30:55,584: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:31:05,711: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this is why
2026-01-11 15:31:05,962: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the sa new
2026-01-11 15:31:48,454: Val batch 6000: PER (avg): 0.3765 CTC Loss (avg): 62.2799 WER(5gram): 67.14% (n=256) time: 53.335
2026-01-11 15:31:48,455: WER lens: avg_true_words=5.99 avg_pred_words=5.41 max_pred_words=12
2026-01-11 15:31:48,455: t15.2023.08.13 val PER: 0.3514
2026-01-11 15:31:48,455: t15.2023.08.18 val PER: 0.3403
2026-01-11 15:31:48,455: t15.2023.08.20 val PER: 0.3145
2026-01-11 15:31:48,455: t15.2023.08.25 val PER: 0.3117
2026-01-11 15:31:48,456: t15.2023.08.27 val PER: 0.4293
2026-01-11 15:31:48,456: t15.2023.09.01 val PER: 0.2971
2026-01-11 15:31:48,456: t15.2023.09.03 val PER: 0.3705
2026-01-11 15:31:48,456: t15.2023.09.24 val PER: 0.3289
2026-01-11 15:31:48,456: t15.2023.09.29 val PER: 0.3599
2026-01-11 15:31:48,456: t15.2023.10.01 val PER: 0.4155
2026-01-11 15:31:48,456: t15.2023.10.06 val PER: 0.3165
2026-01-11 15:31:48,456: t15.2023.10.08 val PER: 0.4682
2026-01-11 15:31:48,456: t15.2023.10.13 val PER: 0.4763
2026-01-11 15:31:48,456: t15.2023.10.15 val PER: 0.3883
2026-01-11 15:31:48,456: t15.2023.10.20 val PER: 0.4027
2026-01-11 15:31:48,457: t15.2023.10.22 val PER: 0.3419
2026-01-11 15:31:48,457: t15.2023.11.03 val PER: 0.3691
2026-01-11 15:31:48,457: t15.2023.11.04 val PER: 0.1399
2026-01-11 15:31:48,457: t15.2023.11.17 val PER: 0.2395
2026-01-11 15:31:48,457: t15.2023.11.19 val PER: 0.2216
2026-01-11 15:31:48,457: t15.2023.11.26 val PER: 0.4348
2026-01-11 15:31:48,457: t15.2023.12.03 val PER: 0.3582
2026-01-11 15:31:48,457: t15.2023.12.08 val PER: 0.3795
2026-01-11 15:31:48,457: t15.2023.12.10 val PER: 0.3548
2026-01-11 15:31:48,457: t15.2023.12.17 val PER: 0.3607
2026-01-11 15:31:48,457: t15.2023.12.29 val PER: 0.4036
2026-01-11 15:31:48,457: t15.2024.02.25 val PER: 0.3104
2026-01-11 15:31:48,457: t15.2024.03.08 val PER: 0.4168
2026-01-11 15:31:48,458: t15.2024.03.15 val PER: 0.4090
2026-01-11 15:31:48,458: t15.2024.03.17 val PER: 0.3689
2026-01-11 15:31:48,458: t15.2024.05.10 val PER: 0.3774
2026-01-11 15:31:48,458: t15.2024.06.14 val PER: 0.3423
2026-01-11 15:31:48,458: t15.2024.07.19 val PER: 0.4344
2026-01-11 15:31:48,458: t15.2024.07.21 val PER: 0.3352
2026-01-11 15:31:48,458: t15.2024.07.28 val PER: 0.3721
2026-01-11 15:31:48,458: t15.2025.01.10 val PER: 0.4738
2026-01-11 15:31:48,458: t15.2025.01.12 val PER: 0.3634
2026-01-11 15:31:48,458: t15.2025.03.14 val PER: 0.4749
2026-01-11 15:31:48,458: t15.2025.03.16 val PER: 0.4267
2026-01-11 15:31:48,458: t15.2025.03.30 val PER: 0.4805
2026-01-11 15:31:48,459: t15.2025.04.13 val PER: 0.4180
2026-01-11 15:31:48,459: New best val WER(5gram) 71.32% --> 67.14%
2026-01-11 15:31:48,636: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_6000
2026-01-11 15:32:08,878: Train batch 6200: loss: 53.50 grad norm: 105.73 time: 0.079
2026-01-11 15:32:29,895: Train batch 6400: loss: 61.24 grad norm: 87.72 time: 0.071
2026-01-11 15:32:40,087: Running test after training batch: 6500
2026-01-11 15:32:40,283: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:32:46,718: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is we
2026-01-11 15:32:47,067: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the way the
2026-01-11 15:33:39,272: Val batch 6500: PER (avg): 0.3628 CTC Loss (avg): 59.3561 WER(5gram): 62.58% (n=256) time: 59.185
2026-01-11 15:33:39,272: WER lens: avg_true_words=5.99 avg_pred_words=5.32 max_pred_words=12
2026-01-11 15:33:39,273: t15.2023.08.13 val PER: 0.3274
2026-01-11 15:33:39,273: t15.2023.08.18 val PER: 0.3420
2026-01-11 15:33:39,273: t15.2023.08.20 val PER: 0.2979
2026-01-11 15:33:39,273: t15.2023.08.25 val PER: 0.3087
2026-01-11 15:33:39,273: t15.2023.08.27 val PER: 0.4084
2026-01-11 15:33:39,273: t15.2023.09.01 val PER: 0.2703
2026-01-11 15:33:39,273: t15.2023.09.03 val PER: 0.3575
2026-01-11 15:33:39,273: t15.2023.09.24 val PER: 0.3204
2026-01-11 15:33:39,273: t15.2023.09.29 val PER: 0.3357
2026-01-11 15:33:39,273: t15.2023.10.01 val PER: 0.4055
2026-01-11 15:33:39,273: t15.2023.10.06 val PER: 0.3057
2026-01-11 15:33:39,273: t15.2023.10.08 val PER: 0.4493
2026-01-11 15:33:39,273: t15.2023.10.13 val PER: 0.4647
2026-01-11 15:33:39,274: t15.2023.10.15 val PER: 0.3757
2026-01-11 15:33:39,274: t15.2023.10.20 val PER: 0.3758
2026-01-11 15:33:39,274: t15.2023.10.22 val PER: 0.3252
2026-01-11 15:33:39,274: t15.2023.11.03 val PER: 0.3623
2026-01-11 15:33:39,274: t15.2023.11.04 val PER: 0.1433
2026-01-11 15:33:39,274: t15.2023.11.17 val PER: 0.2379
2026-01-11 15:33:39,274: t15.2023.11.19 val PER: 0.2255
2026-01-11 15:33:39,275: t15.2023.11.26 val PER: 0.4203
2026-01-11 15:33:39,275: t15.2023.12.03 val PER: 0.3477
2026-01-11 15:33:39,275: t15.2023.12.08 val PER: 0.3722
2026-01-11 15:33:39,275: t15.2023.12.10 val PER: 0.3482
2026-01-11 15:33:39,275: t15.2023.12.17 val PER: 0.3347
2026-01-11 15:33:39,275: t15.2023.12.29 val PER: 0.3802
2026-01-11 15:33:39,275: t15.2024.02.25 val PER: 0.3006
2026-01-11 15:33:39,275: t15.2024.03.08 val PER: 0.4026
2026-01-11 15:33:39,275: t15.2024.03.15 val PER: 0.3909
2026-01-11 15:33:39,276: t15.2024.03.17 val PER: 0.3591
2026-01-11 15:33:39,276: t15.2024.05.10 val PER: 0.3774
2026-01-11 15:33:39,276: t15.2024.06.14 val PER: 0.3454
2026-01-11 15:33:39,276: t15.2024.07.19 val PER: 0.4133
2026-01-11 15:33:39,276: t15.2024.07.21 val PER: 0.3179
2026-01-11 15:33:39,276: t15.2024.07.28 val PER: 0.3588
2026-01-11 15:33:39,276: t15.2025.01.10 val PER: 0.4601
2026-01-11 15:33:39,277: t15.2025.01.12 val PER: 0.3564
2026-01-11 15:33:39,277: t15.2025.03.14 val PER: 0.4763
2026-01-11 15:33:39,277: t15.2025.03.16 val PER: 0.3940
2026-01-11 15:33:39,277: t15.2025.03.30 val PER: 0.4460
2026-01-11 15:33:39,277: t15.2025.04.13 val PER: 0.4108
2026-01-11 15:33:39,278: New best val WER(5gram) 67.14% --> 62.58%
2026-01-11 15:33:39,446: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_6500
2026-01-11 15:33:49,395: Train batch 6600: loss: 37.70 grad norm: 57.09 time: 0.050
2026-01-11 15:34:09,886: Train batch 6800: loss: 47.99 grad norm: 62.12 time: 0.055
2026-01-11 15:34:31,083: Train batch 7000: loss: 50.68 grad norm: 87.77 time: 0.067
2026-01-11 15:34:31,084: Running test after training batch: 7000
2026-01-11 15:34:31,328: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:34:40,990: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is we
2026-01-11 15:34:41,185: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the car at the
2026-01-11 15:35:16,961: Val batch 7000: PER (avg): 0.3520 CTC Loss (avg): 56.7964 WER(5gram): 63.89% (n=256) time: 45.877
2026-01-11 15:35:16,962: WER lens: avg_true_words=5.99 avg_pred_words=5.21 max_pred_words=12
2026-01-11 15:35:16,962: t15.2023.08.13 val PER: 0.3181
2026-01-11 15:35:16,962: t15.2023.08.18 val PER: 0.3168
2026-01-11 15:35:16,962: t15.2023.08.20 val PER: 0.2915
2026-01-11 15:35:16,963: t15.2023.08.25 val PER: 0.2907
2026-01-11 15:35:16,963: t15.2023.08.27 val PER: 0.3987
2026-01-11 15:35:16,963: t15.2023.09.01 val PER: 0.2654
2026-01-11 15:35:16,963: t15.2023.09.03 val PER: 0.3480
2026-01-11 15:35:16,963: t15.2023.09.24 val PER: 0.3010
2026-01-11 15:35:16,963: t15.2023.09.29 val PER: 0.3325
2026-01-11 15:35:16,963: t15.2023.10.01 val PER: 0.4009
2026-01-11 15:35:16,963: t15.2023.10.06 val PER: 0.2982
2026-01-11 15:35:16,964: t15.2023.10.08 val PER: 0.4290
2026-01-11 15:35:16,964: t15.2023.10.13 val PER: 0.4655
2026-01-11 15:35:16,964: t15.2023.10.15 val PER: 0.3652
2026-01-11 15:35:16,964: t15.2023.10.20 val PER: 0.3591
2026-01-11 15:35:16,964: t15.2023.10.22 val PER: 0.3118
2026-01-11 15:35:16,964: t15.2023.11.03 val PER: 0.3379
2026-01-11 15:35:16,965: t15.2023.11.04 val PER: 0.1229
2026-01-11 15:35:16,965: t15.2023.11.17 val PER: 0.2302
2026-01-11 15:35:16,966: t15.2023.11.19 val PER: 0.1836
2026-01-11 15:35:16,966: t15.2023.11.26 val PER: 0.4159
2026-01-11 15:35:16,966: t15.2023.12.03 val PER: 0.3382
2026-01-11 15:35:16,966: t15.2023.12.08 val PER: 0.3595
2026-01-11 15:35:16,966: t15.2023.12.10 val PER: 0.3285
2026-01-11 15:35:16,966: t15.2023.12.17 val PER: 0.3337
2026-01-11 15:35:16,966: t15.2023.12.29 val PER: 0.3713
2026-01-11 15:35:16,967: t15.2024.02.25 val PER: 0.2865
2026-01-11 15:35:16,967: t15.2024.03.08 val PER: 0.3969
2026-01-11 15:35:16,967: t15.2024.03.15 val PER: 0.3827
2026-01-11 15:35:16,967: t15.2024.03.17 val PER: 0.3487
2026-01-11 15:35:16,967: t15.2024.05.10 val PER: 0.3522
2026-01-11 15:35:16,967: t15.2024.06.14 val PER: 0.3233
2026-01-11 15:35:16,967: t15.2024.07.19 val PER: 0.4061
2026-01-11 15:35:16,967: t15.2024.07.21 val PER: 0.3097
2026-01-11 15:35:16,967: t15.2024.07.28 val PER: 0.3515
2026-01-11 15:35:16,967: t15.2025.01.10 val PER: 0.4587
2026-01-11 15:35:16,967: t15.2025.01.12 val PER: 0.3387
2026-01-11 15:35:16,967: t15.2025.03.14 val PER: 0.4571
2026-01-11 15:35:16,967: t15.2025.03.16 val PER: 0.3809
2026-01-11 15:35:16,968: t15.2025.03.30 val PER: 0.4575
2026-01-11 15:35:16,968: t15.2025.04.13 val PER: 0.3937
2026-01-11 15:35:17,148: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_7000
2026-01-11 15:35:37,348: Train batch 7200: loss: 46.27 grad norm: 90.07 time: 0.084
2026-01-11 15:35:57,349: Train batch 7400: loss: 47.31 grad norm: 78.86 time: 0.082
2026-01-11 15:36:07,609: Running test after training batch: 7500
2026-01-11 15:36:08,078: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:36:16,372: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is we
2026-01-11 15:36:16,529: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the car at a
2026-01-11 15:36:54,794: Val batch 7500: PER (avg): 0.3404 CTC Loss (avg): 54.8870 WER(5gram): 58.87% (n=256) time: 47.184
2026-01-11 15:36:54,794: WER lens: avg_true_words=5.99 avg_pred_words=5.50 max_pred_words=11
2026-01-11 15:36:54,795: t15.2023.08.13 val PER: 0.3170
2026-01-11 15:36:54,795: t15.2023.08.18 val PER: 0.3026
2026-01-11 15:36:54,795: t15.2023.08.20 val PER: 0.2875
2026-01-11 15:36:54,795: t15.2023.08.25 val PER: 0.2651
2026-01-11 15:36:54,795: t15.2023.08.27 val PER: 0.3907
2026-01-11 15:36:54,795: t15.2023.09.01 val PER: 0.2451
2026-01-11 15:36:54,796: t15.2023.09.03 val PER: 0.3492
2026-01-11 15:36:54,796: t15.2023.09.24 val PER: 0.2864
2026-01-11 15:36:54,796: t15.2023.09.29 val PER: 0.3216
2026-01-11 15:36:54,796: t15.2023.10.01 val PER: 0.3798
2026-01-11 15:36:54,796: t15.2023.10.06 val PER: 0.2713
2026-01-11 15:36:54,796: t15.2023.10.08 val PER: 0.4208
2026-01-11 15:36:54,796: t15.2023.10.13 val PER: 0.4507
2026-01-11 15:36:54,796: t15.2023.10.15 val PER: 0.3566
2026-01-11 15:36:54,797: t15.2023.10.20 val PER: 0.3557
2026-01-11 15:36:54,797: t15.2023.10.22 val PER: 0.2984
2026-01-11 15:36:54,797: t15.2023.11.03 val PER: 0.3399
2026-01-11 15:36:54,797: t15.2023.11.04 val PER: 0.1195
2026-01-11 15:36:54,797: t15.2023.11.17 val PER: 0.2006
2026-01-11 15:36:54,797: t15.2023.11.19 val PER: 0.1717
2026-01-11 15:36:54,797: t15.2023.11.26 val PER: 0.4029
2026-01-11 15:36:54,797: t15.2023.12.03 val PER: 0.3235
2026-01-11 15:36:54,798: t15.2023.12.08 val PER: 0.3402
2026-01-11 15:36:54,798: t15.2023.12.10 val PER: 0.3298
2026-01-11 15:36:54,798: t15.2023.12.17 val PER: 0.3202
2026-01-11 15:36:54,798: t15.2023.12.29 val PER: 0.3521
2026-01-11 15:36:54,798: t15.2024.02.25 val PER: 0.2837
2026-01-11 15:36:54,798: t15.2024.03.08 val PER: 0.3898
2026-01-11 15:36:54,798: t15.2024.03.15 val PER: 0.3646
2026-01-11 15:36:54,798: t15.2024.03.17 val PER: 0.3410
2026-01-11 15:36:54,799: t15.2024.05.10 val PER: 0.3462
2026-01-11 15:36:54,799: t15.2024.06.14 val PER: 0.3218
2026-01-11 15:36:54,799: t15.2024.07.19 val PER: 0.4008
2026-01-11 15:36:54,799: t15.2024.07.21 val PER: 0.2903
2026-01-11 15:36:54,799: t15.2024.07.28 val PER: 0.3390
2026-01-11 15:36:54,799: t15.2025.01.10 val PER: 0.4435
2026-01-11 15:36:54,799: t15.2025.01.12 val PER: 0.3295
2026-01-11 15:36:54,799: t15.2025.03.14 val PER: 0.4527
2026-01-11 15:36:54,799: t15.2025.03.16 val PER: 0.3730
2026-01-11 15:36:54,799: t15.2025.03.30 val PER: 0.4345
2026-01-11 15:36:54,800: t15.2025.04.13 val PER: 0.3980
2026-01-11 15:36:54,800: New best val WER(5gram) 62.58% --> 58.87%
2026-01-11 15:36:54,980: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_7500
2026-01-11 15:37:05,031: Train batch 7600: loss: 48.43 grad norm: 77.44 time: 0.077
2026-01-11 15:37:25,476: Train batch 7800: loss: 44.47 grad norm: 99.38 time: 0.061
2026-01-11 15:37:46,306: Train batch 8000: loss: 39.38 grad norm: 67.91 time: 0.079
2026-01-11 15:37:46,307: Running test after training batch: 8000
2026-01-11 15:37:46,757: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:37:55,472: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is we
2026-01-11 15:37:55,675: WER debug example
  GT : how does it keep the cost down
  PR : i do it in the car at the
2026-01-11 15:38:36,087: Val batch 8000: PER (avg): 0.3334 CTC Loss (avg): 53.1979 WER(5gram): 55.93% (n=256) time: 49.780
2026-01-11 15:38:36,088: WER lens: avg_true_words=5.99 avg_pred_words=5.52 max_pred_words=11
2026-01-11 15:38:36,088: t15.2023.08.13 val PER: 0.3087
2026-01-11 15:38:36,088: t15.2023.08.18 val PER: 0.3001
2026-01-11 15:38:36,088: t15.2023.08.20 val PER: 0.2748
2026-01-11 15:38:36,088: t15.2023.08.25 val PER: 0.2545
2026-01-11 15:38:36,089: t15.2023.08.27 val PER: 0.3746
2026-01-11 15:38:36,089: t15.2023.09.01 val PER: 0.2451
2026-01-11 15:38:36,089: t15.2023.09.03 val PER: 0.3397
2026-01-11 15:38:36,089: t15.2023.09.24 val PER: 0.2718
2026-01-11 15:38:36,089: t15.2023.09.29 val PER: 0.3133
2026-01-11 15:38:36,089: t15.2023.10.01 val PER: 0.3752
2026-01-11 15:38:36,089: t15.2023.10.06 val PER: 0.2702
2026-01-11 15:38:36,089: t15.2023.10.08 val PER: 0.4235
2026-01-11 15:38:36,090: t15.2023.10.13 val PER: 0.4360
2026-01-11 15:38:36,090: t15.2023.10.15 val PER: 0.3454
2026-01-11 15:38:36,090: t15.2023.10.20 val PER: 0.3490
2026-01-11 15:38:36,090: t15.2023.10.22 val PER: 0.2918
2026-01-11 15:38:36,090: t15.2023.11.03 val PER: 0.3345
2026-01-11 15:38:36,090: t15.2023.11.04 val PER: 0.0887
2026-01-11 15:38:36,090: t15.2023.11.17 val PER: 0.2037
2026-01-11 15:38:36,090: t15.2023.11.19 val PER: 0.1617
2026-01-11 15:38:36,090: t15.2023.11.26 val PER: 0.3971
2026-01-11 15:38:36,091: t15.2023.12.03 val PER: 0.3120
2026-01-11 15:38:36,091: t15.2023.12.08 val PER: 0.3395
2026-01-11 15:38:36,091: t15.2023.12.10 val PER: 0.3049
2026-01-11 15:38:36,091: t15.2023.12.17 val PER: 0.3233
2026-01-11 15:38:36,091: t15.2023.12.29 val PER: 0.3514
2026-01-11 15:38:36,091: t15.2024.02.25 val PER: 0.2753
2026-01-11 15:38:36,091: t15.2024.03.08 val PER: 0.3812
2026-01-11 15:38:36,091: t15.2024.03.15 val PER: 0.3615
2026-01-11 15:38:36,091: t15.2024.03.17 val PER: 0.3354
2026-01-11 15:38:36,091: t15.2024.05.10 val PER: 0.3447
2026-01-11 15:38:36,092: t15.2024.06.14 val PER: 0.3091
2026-01-11 15:38:36,092: t15.2024.07.19 val PER: 0.3876
2026-01-11 15:38:36,092: t15.2024.07.21 val PER: 0.2814
2026-01-11 15:38:36,092: t15.2024.07.28 val PER: 0.3257
2026-01-11 15:38:36,092: t15.2025.01.10 val PER: 0.4573
2026-01-11 15:38:36,092: t15.2025.01.12 val PER: 0.3179
2026-01-11 15:38:36,092: t15.2025.03.14 val PER: 0.4541
2026-01-11 15:38:36,092: t15.2025.03.16 val PER: 0.3573
2026-01-11 15:38:36,092: t15.2025.03.30 val PER: 0.4471
2026-01-11 15:38:36,092: t15.2025.04.13 val PER: 0.3780
2026-01-11 15:38:36,093: New best val WER(5gram) 58.87% --> 55.93%
2026-01-11 15:38:36,273: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_8000
2026-01-11 15:38:56,970: Train batch 8200: loss: 30.99 grad norm: 57.51 time: 0.063
2026-01-11 15:39:17,647: Train batch 8400: loss: 35.78 grad norm: 71.99 time: 0.069
2026-01-11 15:39:28,154: Running test after training batch: 8500
2026-01-11 15:39:28,293: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:39:34,629: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 15:39:34,765: WER debug example
  GT : how does it keep the cost down
  PR : how do it in the coup de
2026-01-11 15:40:03,304: Val batch 8500: PER (avg): 0.3257 CTC Loss (avg): 51.4993 WER(5gram): 54.82% (n=256) time: 35.150
2026-01-11 15:40:03,304: WER lens: avg_true_words=5.99 avg_pred_words=5.48 max_pred_words=11
2026-01-11 15:40:03,305: t15.2023.08.13 val PER: 0.3025
2026-01-11 15:40:03,305: t15.2023.08.18 val PER: 0.2925
2026-01-11 15:40:03,305: t15.2023.08.20 val PER: 0.2677
2026-01-11 15:40:03,305: t15.2023.08.25 val PER: 0.2605
2026-01-11 15:40:03,306: t15.2023.08.27 val PER: 0.3633
2026-01-11 15:40:03,306: t15.2023.09.01 val PER: 0.2281
2026-01-11 15:40:03,306: t15.2023.09.03 val PER: 0.3290
2026-01-11 15:40:03,306: t15.2023.09.24 val PER: 0.2694
2026-01-11 15:40:03,306: t15.2023.09.29 val PER: 0.3050
2026-01-11 15:40:03,306: t15.2023.10.01 val PER: 0.3666
2026-01-11 15:40:03,306: t15.2023.10.06 val PER: 0.2562
2026-01-11 15:40:03,306: t15.2023.10.08 val PER: 0.4208
2026-01-11 15:40:03,306: t15.2023.10.13 val PER: 0.4313
2026-01-11 15:40:03,307: t15.2023.10.15 val PER: 0.3362
2026-01-11 15:40:03,307: t15.2023.10.20 val PER: 0.3423
2026-01-11 15:40:03,307: t15.2023.10.22 val PER: 0.2829
2026-01-11 15:40:03,307: t15.2023.11.03 val PER: 0.3229
2026-01-11 15:40:03,307: t15.2023.11.04 val PER: 0.0887
2026-01-11 15:40:03,308: t15.2023.11.17 val PER: 0.1804
2026-01-11 15:40:03,308: t15.2023.11.19 val PER: 0.1717
2026-01-11 15:40:03,308: t15.2023.11.26 val PER: 0.3877
2026-01-11 15:40:03,308: t15.2023.12.03 val PER: 0.3046
2026-01-11 15:40:03,308: t15.2023.12.08 val PER: 0.3269
2026-01-11 15:40:03,308: t15.2023.12.10 val PER: 0.2930
2026-01-11 15:40:03,309: t15.2023.12.17 val PER: 0.3108
2026-01-11 15:40:03,309: t15.2023.12.29 val PER: 0.3397
2026-01-11 15:40:03,309: t15.2024.02.25 val PER: 0.2739
2026-01-11 15:40:03,309: t15.2024.03.08 val PER: 0.3585
2026-01-11 15:40:03,309: t15.2024.03.15 val PER: 0.3602
2026-01-11 15:40:03,309: t15.2024.03.17 val PER: 0.3222
2026-01-11 15:40:03,309: t15.2024.05.10 val PER: 0.3373
2026-01-11 15:40:03,309: t15.2024.06.14 val PER: 0.3139
2026-01-11 15:40:03,309: t15.2024.07.19 val PER: 0.3962
2026-01-11 15:40:03,310: t15.2024.07.21 val PER: 0.2710
2026-01-11 15:40:03,310: t15.2024.07.28 val PER: 0.3301
2026-01-11 15:40:03,310: t15.2025.01.10 val PER: 0.4394
2026-01-11 15:40:03,310: t15.2025.01.12 val PER: 0.3164
2026-01-11 15:40:03,310: t15.2025.03.14 val PER: 0.4305
2026-01-11 15:40:03,310: t15.2025.03.16 val PER: 0.3547
2026-01-11 15:40:03,310: t15.2025.03.30 val PER: 0.4207
2026-01-11 15:40:03,310: t15.2025.04.13 val PER: 0.3909
2026-01-11 15:40:03,311: New best val WER(5gram) 55.93% --> 54.82%
2026-01-11 15:40:03,494: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_8500
2026-01-11 15:40:14,099: Train batch 8600: loss: 45.15 grad norm: 76.33 time: 0.071
2026-01-11 15:40:35,477: Train batch 8800: loss: 51.26 grad norm: 95.66 time: 0.070
2026-01-11 15:40:56,624: Train batch 9000: loss: 48.32 grad norm: 101.98 time: 0.081
2026-01-11 15:40:56,624: Running test after training batch: 9000
2026-01-11 15:40:56,791: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:41:03,056: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 15:41:03,183: WER debug example
  GT : how does it keep the cost down
  PR : how do it in the case
2026-01-11 15:41:31,998: Val batch 9000: PER (avg): 0.3197 CTC Loss (avg): 50.2394 WER(5gram): 53.85% (n=256) time: 35.373
2026-01-11 15:41:31,998: WER lens: avg_true_words=5.99 avg_pred_words=5.56 max_pred_words=12
2026-01-11 15:41:31,999: t15.2023.08.13 val PER: 0.2859
2026-01-11 15:41:31,999: t15.2023.08.18 val PER: 0.2875
2026-01-11 15:41:31,999: t15.2023.08.20 val PER: 0.2653
2026-01-11 15:41:31,999: t15.2023.08.25 val PER: 0.2545
2026-01-11 15:41:31,999: t15.2023.08.27 val PER: 0.3392
2026-01-11 15:41:31,999: t15.2023.09.01 val PER: 0.2216
2026-01-11 15:41:31,999: t15.2023.09.03 val PER: 0.3290
2026-01-11 15:41:31,999: t15.2023.09.24 val PER: 0.2597
2026-01-11 15:41:31,999: t15.2023.09.29 val PER: 0.3031
2026-01-11 15:41:31,999: t15.2023.10.01 val PER: 0.3639
2026-01-11 15:41:31,999: t15.2023.10.06 val PER: 0.2713
2026-01-11 15:41:32,000: t15.2023.10.08 val PER: 0.4127
2026-01-11 15:41:32,000: t15.2023.10.13 val PER: 0.4321
2026-01-11 15:41:32,000: t15.2023.10.15 val PER: 0.3283
2026-01-11 15:41:32,000: t15.2023.10.20 val PER: 0.3356
2026-01-11 15:41:32,000: t15.2023.10.22 val PER: 0.2840
2026-01-11 15:41:32,000: t15.2023.11.03 val PER: 0.3195
2026-01-11 15:41:32,000: t15.2023.11.04 val PER: 0.0887
2026-01-11 15:41:32,000: t15.2023.11.17 val PER: 0.1804
2026-01-11 15:41:32,000: t15.2023.11.19 val PER: 0.1497
2026-01-11 15:41:32,000: t15.2023.11.26 val PER: 0.3870
2026-01-11 15:41:32,000: t15.2023.12.03 val PER: 0.3109
2026-01-11 15:41:32,001: t15.2023.12.08 val PER: 0.3189
2026-01-11 15:41:32,001: t15.2023.12.10 val PER: 0.2852
2026-01-11 15:41:32,001: t15.2023.12.17 val PER: 0.2963
2026-01-11 15:41:32,001: t15.2023.12.29 val PER: 0.3253
2026-01-11 15:41:32,001: t15.2024.02.25 val PER: 0.2542
2026-01-11 15:41:32,001: t15.2024.03.08 val PER: 0.3556
2026-01-11 15:41:32,001: t15.2024.03.15 val PER: 0.3527
2026-01-11 15:41:32,001: t15.2024.03.17 val PER: 0.3166
2026-01-11 15:41:32,001: t15.2024.05.10 val PER: 0.3328
2026-01-11 15:41:32,001: t15.2024.06.14 val PER: 0.3091
2026-01-11 15:41:32,002: t15.2024.07.19 val PER: 0.3830
2026-01-11 15:41:32,002: t15.2024.07.21 val PER: 0.2703
2026-01-11 15:41:32,002: t15.2024.07.28 val PER: 0.3221
2026-01-11 15:41:32,002: t15.2025.01.10 val PER: 0.4380
2026-01-11 15:41:32,002: t15.2025.01.12 val PER: 0.3118
2026-01-11 15:41:32,002: t15.2025.03.14 val PER: 0.4305
2026-01-11 15:41:32,002: t15.2025.03.16 val PER: 0.3442
2026-01-11 15:41:32,002: t15.2025.03.30 val PER: 0.4069
2026-01-11 15:41:32,002: t15.2025.04.13 val PER: 0.3709
2026-01-11 15:41:32,003: New best val WER(5gram) 54.82% --> 53.85%
2026-01-11 15:41:32,172: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_9000
2026-01-11 15:41:52,393: Train batch 9200: loss: 34.70 grad norm: 62.08 time: 0.065
2026-01-11 15:42:13,301: Train batch 9400: loss: 30.87 grad norm: 65.43 time: 0.074
2026-01-11 15:42:23,640: Running test after training batch: 9500
2026-01-11 15:42:23,821: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:42:30,048: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 15:42:30,179: WER debug example
  GT : how does it keep the cost down
  PR : i owe it to the cost and
2026-01-11 15:42:55,347: Val batch 9500: PER (avg): 0.3139 CTC Loss (avg): 48.3923 WER(5gram): 53.78% (n=256) time: 31.706
2026-01-11 15:42:55,347: WER lens: avg_true_words=5.99 avg_pred_words=5.48 max_pred_words=11
2026-01-11 15:42:55,347: t15.2023.08.13 val PER: 0.2775
2026-01-11 15:42:55,347: t15.2023.08.18 val PER: 0.2666
2026-01-11 15:42:55,348: t15.2023.08.20 val PER: 0.2478
2026-01-11 15:42:55,348: t15.2023.08.25 val PER: 0.2455
2026-01-11 15:42:55,348: t15.2023.08.27 val PER: 0.3505
2026-01-11 15:42:55,348: t15.2023.09.01 val PER: 0.2208
2026-01-11 15:42:55,348: t15.2023.09.03 val PER: 0.3135
2026-01-11 15:42:55,348: t15.2023.09.24 val PER: 0.2755
2026-01-11 15:42:55,348: t15.2023.09.29 val PER: 0.2993
2026-01-11 15:42:55,348: t15.2023.10.01 val PER: 0.3606
2026-01-11 15:42:55,348: t15.2023.10.06 val PER: 0.2476
2026-01-11 15:42:55,348: t15.2023.10.08 val PER: 0.4032
2026-01-11 15:42:55,349: t15.2023.10.13 val PER: 0.4174
2026-01-11 15:42:55,349: t15.2023.10.15 val PER: 0.3237
2026-01-11 15:42:55,349: t15.2023.10.20 val PER: 0.3356
2026-01-11 15:42:55,349: t15.2023.10.22 val PER: 0.2829
2026-01-11 15:42:55,349: t15.2023.11.03 val PER: 0.3175
2026-01-11 15:42:55,349: t15.2023.11.04 val PER: 0.0819
2026-01-11 15:42:55,349: t15.2023.11.17 val PER: 0.1695
2026-01-11 15:42:55,349: t15.2023.11.19 val PER: 0.1437
2026-01-11 15:42:55,350: t15.2023.11.26 val PER: 0.3768
2026-01-11 15:42:55,350: t15.2023.12.03 val PER: 0.3004
2026-01-11 15:42:55,350: t15.2023.12.08 val PER: 0.3202
2026-01-11 15:42:55,350: t15.2023.12.10 val PER: 0.2904
2026-01-11 15:42:55,350: t15.2023.12.17 val PER: 0.2879
2026-01-11 15:42:55,350: t15.2023.12.29 val PER: 0.3178
2026-01-11 15:42:55,350: t15.2024.02.25 val PER: 0.2542
2026-01-11 15:42:55,351: t15.2024.03.08 val PER: 0.3599
2026-01-11 15:42:55,351: t15.2024.03.15 val PER: 0.3477
2026-01-11 15:42:55,351: t15.2024.03.17 val PER: 0.3180
2026-01-11 15:42:55,351: t15.2024.05.10 val PER: 0.3239
2026-01-11 15:42:55,351: t15.2024.06.14 val PER: 0.3155
2026-01-11 15:42:55,351: t15.2024.07.19 val PER: 0.3731
2026-01-11 15:42:55,351: t15.2024.07.21 val PER: 0.2669
2026-01-11 15:42:55,351: t15.2024.07.28 val PER: 0.3074
2026-01-11 15:42:55,351: t15.2025.01.10 val PER: 0.4270
2026-01-11 15:42:55,352: t15.2025.01.12 val PER: 0.3079
2026-01-11 15:42:55,352: t15.2025.03.14 val PER: 0.4186
2026-01-11 15:42:55,352: t15.2025.03.16 val PER: 0.3547
2026-01-11 15:42:55,352: t15.2025.03.30 val PER: 0.3989
2026-01-11 15:42:55,352: t15.2025.04.13 val PER: 0.3695
2026-01-11 15:42:55,353: New best val WER(5gram) 53.85% --> 53.78%
2026-01-11 15:42:55,542: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_9500
2026-01-11 15:43:05,913: Train batch 9600: loss: 29.53 grad norm: 79.96 time: 0.080
2026-01-11 15:43:26,182: Train batch 9800: loss: 41.61 grad norm: 100.60 time: 0.070
2026-01-11 15:43:47,327: Train batch 10000: loss: 23.04 grad norm: 70.65 time: 0.065
2026-01-11 15:43:47,327: Running test after training batch: 10000
2026-01-11 15:43:47,481: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:43:53,873: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 15:43:54,012: WER debug example
  GT : how does it keep the cost down
  PR : how does it cover the cost to
2026-01-11 15:44:22,419: Val batch 10000: PER (avg): 0.3061 CTC Loss (avg): 47.6906 WER(5gram): 47.33% (n=256) time: 35.091
2026-01-11 15:44:22,421: WER lens: avg_true_words=5.99 avg_pred_words=5.73 max_pred_words=13
2026-01-11 15:44:22,421: t15.2023.08.13 val PER: 0.2734
2026-01-11 15:44:22,421: t15.2023.08.18 val PER: 0.2615
2026-01-11 15:44:22,421: t15.2023.08.20 val PER: 0.2494
2026-01-11 15:44:22,421: t15.2023.08.25 val PER: 0.2319
2026-01-11 15:44:22,421: t15.2023.08.27 val PER: 0.3344
2026-01-11 15:44:22,421: t15.2023.09.01 val PER: 0.2183
2026-01-11 15:44:22,422: t15.2023.09.03 val PER: 0.3112
2026-01-11 15:44:22,422: t15.2023.09.24 val PER: 0.2549
2026-01-11 15:44:22,422: t15.2023.09.29 val PER: 0.2814
2026-01-11 15:44:22,422: t15.2023.10.01 val PER: 0.3494
2026-01-11 15:44:22,422: t15.2023.10.06 val PER: 0.2260
2026-01-11 15:44:22,422: t15.2023.10.08 val PER: 0.4019
2026-01-11 15:44:22,422: t15.2023.10.13 val PER: 0.4104
2026-01-11 15:44:22,422: t15.2023.10.15 val PER: 0.3151
2026-01-11 15:44:22,422: t15.2023.10.20 val PER: 0.3221
2026-01-11 15:44:22,423: t15.2023.10.22 val PER: 0.2762
2026-01-11 15:44:22,423: t15.2023.11.03 val PER: 0.3114
2026-01-11 15:44:22,423: t15.2023.11.04 val PER: 0.0751
2026-01-11 15:44:22,423: t15.2023.11.17 val PER: 0.1664
2026-01-11 15:44:22,423: t15.2023.11.19 val PER: 0.1417
2026-01-11 15:44:22,423: t15.2023.11.26 val PER: 0.3674
2026-01-11 15:44:22,423: t15.2023.12.03 val PER: 0.2941
2026-01-11 15:44:22,424: t15.2023.12.08 val PER: 0.3156
2026-01-11 15:44:22,424: t15.2023.12.10 val PER: 0.2799
2026-01-11 15:44:22,424: t15.2023.12.17 val PER: 0.2827
2026-01-11 15:44:22,424: t15.2023.12.29 val PER: 0.3095
2026-01-11 15:44:22,424: t15.2024.02.25 val PER: 0.2542
2026-01-11 15:44:22,425: t15.2024.03.08 val PER: 0.3599
2026-01-11 15:44:22,426: t15.2024.03.15 val PER: 0.3340
2026-01-11 15:44:22,426: t15.2024.03.17 val PER: 0.2971
2026-01-11 15:44:22,426: t15.2024.05.10 val PER: 0.3105
2026-01-11 15:44:22,426: t15.2024.06.14 val PER: 0.2997
2026-01-11 15:44:22,426: t15.2024.07.19 val PER: 0.3757
2026-01-11 15:44:22,426: t15.2024.07.21 val PER: 0.2586
2026-01-11 15:44:22,426: t15.2024.07.28 val PER: 0.3066
2026-01-11 15:44:22,426: t15.2025.01.10 val PER: 0.4325
2026-01-11 15:44:22,426: t15.2025.01.12 val PER: 0.3018
2026-01-11 15:44:22,427: t15.2025.03.14 val PER: 0.4024
2026-01-11 15:44:22,427: t15.2025.03.16 val PER: 0.3495
2026-01-11 15:44:22,427: t15.2025.03.30 val PER: 0.3954
2026-01-11 15:44:22,427: t15.2025.04.13 val PER: 0.3581
2026-01-11 15:44:22,428: New best val WER(5gram) 53.78% --> 47.33%
2026-01-11 15:44:22,614: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_10000
2026-01-11 15:44:42,831: Train batch 10200: loss: 27.20 grad norm: 59.10 time: 0.055
2026-01-11 15:45:03,280: Train batch 10400: loss: 32.32 grad norm: 86.26 time: 0.082
2026-01-11 15:45:13,636: Running test after training batch: 10500
2026-01-11 15:45:14,120: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:45:27,106: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 15:45:27,255: WER debug example
  GT : how does it keep the cost down
  PR : how does it give the guy at the
2026-01-11 15:45:53,231: Val batch 10500: PER (avg): 0.2982 CTC Loss (avg): 46.6142 WER(5gram): 51.96% (n=256) time: 39.594
2026-01-11 15:45:53,231: WER lens: avg_true_words=5.99 avg_pred_words=5.66 max_pred_words=11
2026-01-11 15:45:53,232: t15.2023.08.13 val PER: 0.2630
2026-01-11 15:45:53,232: t15.2023.08.18 val PER: 0.2540
2026-01-11 15:45:53,232: t15.2023.08.20 val PER: 0.2431
2026-01-11 15:45:53,232: t15.2023.08.25 val PER: 0.2274
2026-01-11 15:45:53,232: t15.2023.08.27 val PER: 0.3199
2026-01-11 15:45:53,232: t15.2023.09.01 val PER: 0.2119
2026-01-11 15:45:53,232: t15.2023.09.03 val PER: 0.3147
2026-01-11 15:45:53,232: t15.2023.09.24 val PER: 0.2439
2026-01-11 15:45:53,233: t15.2023.09.29 val PER: 0.2770
2026-01-11 15:45:53,233: t15.2023.10.01 val PER: 0.3382
2026-01-11 15:45:53,233: t15.2023.10.06 val PER: 0.2293
2026-01-11 15:45:53,233: t15.2023.10.08 val PER: 0.3897
2026-01-11 15:45:53,233: t15.2023.10.13 val PER: 0.4034
2026-01-11 15:45:53,234: t15.2023.10.15 val PER: 0.3118
2026-01-11 15:45:53,234: t15.2023.10.20 val PER: 0.3356
2026-01-11 15:45:53,234: t15.2023.10.22 val PER: 0.2673
2026-01-11 15:45:53,234: t15.2023.11.03 val PER: 0.3012
2026-01-11 15:45:53,234: t15.2023.11.04 val PER: 0.0683
2026-01-11 15:45:53,234: t15.2023.11.17 val PER: 0.1664
2026-01-11 15:45:53,234: t15.2023.11.19 val PER: 0.1437
2026-01-11 15:45:53,235: t15.2023.11.26 val PER: 0.3522
2026-01-11 15:45:53,235: t15.2023.12.03 val PER: 0.2910
2026-01-11 15:45:53,235: t15.2023.12.08 val PER: 0.3083
2026-01-11 15:45:53,235: t15.2023.12.10 val PER: 0.2694
2026-01-11 15:45:53,235: t15.2023.12.17 val PER: 0.2807
2026-01-11 15:45:53,235: t15.2023.12.29 val PER: 0.3006
2026-01-11 15:45:53,235: t15.2024.02.25 val PER: 0.2416
2026-01-11 15:45:53,235: t15.2024.03.08 val PER: 0.3542
2026-01-11 15:45:53,235: t15.2024.03.15 val PER: 0.3252
2026-01-11 15:45:53,235: t15.2024.03.17 val PER: 0.2992
2026-01-11 15:45:53,235: t15.2024.05.10 val PER: 0.3091
2026-01-11 15:45:53,235: t15.2024.06.14 val PER: 0.2934
2026-01-11 15:45:53,236: t15.2024.07.19 val PER: 0.3705
2026-01-11 15:45:53,236: t15.2024.07.21 val PER: 0.2407
2026-01-11 15:45:53,236: t15.2024.07.28 val PER: 0.2926
2026-01-11 15:45:53,236: t15.2025.01.10 val PER: 0.4160
2026-01-11 15:45:53,236: t15.2025.01.12 val PER: 0.2964
2026-01-11 15:45:53,236: t15.2025.03.14 val PER: 0.3964
2026-01-11 15:45:53,236: t15.2025.03.16 val PER: 0.3115
2026-01-11 15:45:53,236: t15.2025.03.30 val PER: 0.3828
2026-01-11 15:45:53,236: t15.2025.04.13 val PER: 0.3495
2026-01-11 15:45:53,418: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_10500
2026-01-11 15:46:04,240: Train batch 10600: loss: 30.76 grad norm: 82.63 time: 0.077
2026-01-11 15:46:25,007: Train batch 10800: loss: 42.86 grad norm: 110.24 time: 0.071
2026-01-11 15:46:45,982: Train batch 11000: loss: 45.99 grad norm: 125.50 time: 0.070
2026-01-11 15:46:45,983: Running test after training batch: 11000
2026-01-11 15:46:46,114: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:46:52,734: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 15:46:52,866: WER debug example
  GT : how does it keep the cost down
  PR : how does it cover the cost and
2026-01-11 15:47:23,075: Val batch 11000: PER (avg): 0.2870 CTC Loss (avg): 45.3935 WER(5gram): 45.18% (n=256) time: 37.092
2026-01-11 15:47:23,076: WER lens: avg_true_words=5.99 avg_pred_words=5.89 max_pred_words=12
2026-01-11 15:47:23,076: t15.2023.08.13 val PER: 0.2536
2026-01-11 15:47:23,076: t15.2023.08.18 val PER: 0.2431
2026-01-11 15:47:23,076: t15.2023.08.20 val PER: 0.2311
2026-01-11 15:47:23,076: t15.2023.08.25 val PER: 0.2093
2026-01-11 15:47:23,077: t15.2023.08.27 val PER: 0.3103
2026-01-11 15:47:23,077: t15.2023.09.01 val PER: 0.1956
2026-01-11 15:47:23,077: t15.2023.09.03 val PER: 0.2850
2026-01-11 15:47:23,077: t15.2023.09.24 val PER: 0.2294
2026-01-11 15:47:23,077: t15.2023.09.29 val PER: 0.2680
2026-01-11 15:47:23,077: t15.2023.10.01 val PER: 0.3276
2026-01-11 15:47:23,078: t15.2023.10.06 val PER: 0.2196
2026-01-11 15:47:23,078: t15.2023.10.08 val PER: 0.3721
2026-01-11 15:47:23,078: t15.2023.10.13 val PER: 0.3995
2026-01-11 15:47:23,078: t15.2023.10.15 val PER: 0.2953
2026-01-11 15:47:23,078: t15.2023.10.20 val PER: 0.3221
2026-01-11 15:47:23,078: t15.2023.10.22 val PER: 0.2517
2026-01-11 15:47:23,078: t15.2023.11.03 val PER: 0.2951
2026-01-11 15:47:23,078: t15.2023.11.04 val PER: 0.0717
2026-01-11 15:47:23,078: t15.2023.11.17 val PER: 0.1509
2026-01-11 15:47:23,079: t15.2023.11.19 val PER: 0.1397
2026-01-11 15:47:23,079: t15.2023.11.26 val PER: 0.3478
2026-01-11 15:47:23,079: t15.2023.12.03 val PER: 0.2710
2026-01-11 15:47:23,079: t15.2023.12.08 val PER: 0.2963
2026-01-11 15:47:23,079: t15.2023.12.10 val PER: 0.2641
2026-01-11 15:47:23,079: t15.2023.12.17 val PER: 0.2640
2026-01-11 15:47:23,079: t15.2023.12.29 val PER: 0.2848
2026-01-11 15:47:23,079: t15.2024.02.25 val PER: 0.2303
2026-01-11 15:47:23,080: t15.2024.03.08 val PER: 0.3428
2026-01-11 15:47:23,080: t15.2024.03.15 val PER: 0.3196
2026-01-11 15:47:23,080: t15.2024.03.17 val PER: 0.2831
2026-01-11 15:47:23,080: t15.2024.05.10 val PER: 0.2987
2026-01-11 15:47:23,080: t15.2024.06.14 val PER: 0.2713
2026-01-11 15:47:23,080: t15.2024.07.19 val PER: 0.3494
2026-01-11 15:47:23,080: t15.2024.07.21 val PER: 0.2338
2026-01-11 15:47:23,080: t15.2024.07.28 val PER: 0.2787
2026-01-11 15:47:23,080: t15.2025.01.10 val PER: 0.4091
2026-01-11 15:47:23,081: t15.2025.01.12 val PER: 0.2848
2026-01-11 15:47:23,081: t15.2025.03.14 val PER: 0.3964
2026-01-11 15:47:23,081: t15.2025.03.16 val PER: 0.3220
2026-01-11 15:47:23,081: t15.2025.03.30 val PER: 0.3816
2026-01-11 15:47:23,081: t15.2025.04.13 val PER: 0.3452
2026-01-11 15:47:23,081: New best val WER(5gram) 47.33% --> 45.18%
2026-01-11 15:47:23,270: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_11000
2026-01-11 15:47:44,466: Train batch 11200: loss: 34.59 grad norm: 73.40 time: 0.084
2026-01-11 15:48:04,866: Train batch 11400: loss: 33.41 grad norm: 76.26 time: 0.065
2026-01-11 15:48:15,367: Running test after training batch: 11500
2026-01-11 15:48:15,608: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:48:22,210: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-11 15:48:22,325: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 15:48:52,388: Val batch 11500: PER (avg): 0.2863 CTC Loss (avg): 44.3423 WER(5gram): 43.87% (n=256) time: 37.021
2026-01-11 15:48:52,388: WER lens: avg_true_words=5.99 avg_pred_words=5.76 max_pred_words=11
2026-01-11 15:48:52,389: t15.2023.08.13 val PER: 0.2432
2026-01-11 15:48:52,389: t15.2023.08.18 val PER: 0.2397
2026-01-11 15:48:52,389: t15.2023.08.20 val PER: 0.2248
2026-01-11 15:48:52,389: t15.2023.08.25 val PER: 0.2259
2026-01-11 15:48:52,389: t15.2023.08.27 val PER: 0.3071
2026-01-11 15:48:52,389: t15.2023.09.01 val PER: 0.1964
2026-01-11 15:48:52,389: t15.2023.09.03 val PER: 0.2803
2026-01-11 15:48:52,389: t15.2023.09.24 val PER: 0.2318
2026-01-11 15:48:52,389: t15.2023.09.29 val PER: 0.2661
2026-01-11 15:48:52,390: t15.2023.10.01 val PER: 0.3303
2026-01-11 15:48:52,390: t15.2023.10.06 val PER: 0.2282
2026-01-11 15:48:52,390: t15.2023.10.08 val PER: 0.3775
2026-01-11 15:48:52,390: t15.2023.10.13 val PER: 0.3926
2026-01-11 15:48:52,391: t15.2023.10.15 val PER: 0.2953
2026-01-11 15:48:52,391: t15.2023.10.20 val PER: 0.3087
2026-01-11 15:48:52,391: t15.2023.10.22 val PER: 0.2628
2026-01-11 15:48:52,391: t15.2023.11.03 val PER: 0.2931
2026-01-11 15:48:52,391: t15.2023.11.04 val PER: 0.0717
2026-01-11 15:48:52,391: t15.2023.11.17 val PER: 0.1524
2026-01-11 15:48:52,392: t15.2023.11.19 val PER: 0.1277
2026-01-11 15:48:52,392: t15.2023.11.26 val PER: 0.3435
2026-01-11 15:48:52,392: t15.2023.12.03 val PER: 0.2658
2026-01-11 15:48:52,393: t15.2023.12.08 val PER: 0.2836
2026-01-11 15:48:52,393: t15.2023.12.10 val PER: 0.2457
2026-01-11 15:48:52,393: t15.2023.12.17 val PER: 0.2703
2026-01-11 15:48:52,393: t15.2023.12.29 val PER: 0.2841
2026-01-11 15:48:52,394: t15.2024.02.25 val PER: 0.2430
2026-01-11 15:48:52,394: t15.2024.03.08 val PER: 0.3414
2026-01-11 15:48:52,394: t15.2024.03.15 val PER: 0.3221
2026-01-11 15:48:52,394: t15.2024.03.17 val PER: 0.2880
2026-01-11 15:48:52,394: t15.2024.05.10 val PER: 0.2927
2026-01-11 15:48:52,394: t15.2024.06.14 val PER: 0.2808
2026-01-11 15:48:52,394: t15.2024.07.19 val PER: 0.3487
2026-01-11 15:48:52,394: t15.2024.07.21 val PER: 0.2400
2026-01-11 15:48:52,394: t15.2024.07.28 val PER: 0.2824
2026-01-11 15:48:52,394: t15.2025.01.10 val PER: 0.4187
2026-01-11 15:48:52,395: t15.2025.01.12 val PER: 0.2856
2026-01-11 15:48:52,395: t15.2025.03.14 val PER: 0.3891
2026-01-11 15:48:52,395: t15.2025.03.16 val PER: 0.3233
2026-01-11 15:48:52,395: t15.2025.03.30 val PER: 0.3621
2026-01-11 15:48:52,395: t15.2025.04.13 val PER: 0.3452
2026-01-11 15:48:52,396: New best val WER(5gram) 45.18% --> 43.87%
2026-01-11 15:48:52,583: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_11500
2026-01-11 15:49:02,347: Train batch 11600: loss: 37.87 grad norm: 97.96 time: 0.071
2026-01-11 15:49:23,095: Train batch 11800: loss: 24.42 grad norm: 54.65 time: 0.050
2026-01-11 15:49:43,919: Train batch 12000: loss: 38.58 grad norm: 75.57 time: 0.076
2026-01-11 15:49:43,919: Running test after training batch: 12000
2026-01-11 15:49:44,392: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:49:58,592: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 15:49:58,713: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost and
2026-01-11 15:50:22,654: Val batch 12000: PER (avg): 0.2757 CTC Loss (avg): 43.6086 WER(5gram): 41.13% (n=256) time: 38.734
2026-01-11 15:50:22,655: WER lens: avg_true_words=5.99 avg_pred_words=5.80 max_pred_words=11
2026-01-11 15:50:22,655: t15.2023.08.13 val PER: 0.2318
2026-01-11 15:50:22,655: t15.2023.08.18 val PER: 0.2414
2026-01-11 15:50:22,656: t15.2023.08.20 val PER: 0.2272
2026-01-11 15:50:22,656: t15.2023.08.25 val PER: 0.2123
2026-01-11 15:50:22,656: t15.2023.08.27 val PER: 0.3023
2026-01-11 15:50:22,656: t15.2023.09.01 val PER: 0.1859
2026-01-11 15:50:22,657: t15.2023.09.03 val PER: 0.2732
2026-01-11 15:50:22,657: t15.2023.09.24 val PER: 0.2184
2026-01-11 15:50:22,657: t15.2023.09.29 val PER: 0.2540
2026-01-11 15:50:22,657: t15.2023.10.01 val PER: 0.3164
2026-01-11 15:50:22,657: t15.2023.10.06 val PER: 0.2099
2026-01-11 15:50:22,657: t15.2023.10.08 val PER: 0.3708
2026-01-11 15:50:22,657: t15.2023.10.13 val PER: 0.3848
2026-01-11 15:50:22,657: t15.2023.10.15 val PER: 0.2874
2026-01-11 15:50:22,658: t15.2023.10.20 val PER: 0.3020
2026-01-11 15:50:22,658: t15.2023.10.22 val PER: 0.2506
2026-01-11 15:50:22,658: t15.2023.11.03 val PER: 0.2904
2026-01-11 15:50:22,658: t15.2023.11.04 val PER: 0.0717
2026-01-11 15:50:22,658: t15.2023.11.17 val PER: 0.1275
2026-01-11 15:50:22,658: t15.2023.11.19 val PER: 0.1238
2026-01-11 15:50:22,659: t15.2023.11.26 val PER: 0.3370
2026-01-11 15:50:22,659: t15.2023.12.03 val PER: 0.2563
2026-01-11 15:50:22,659: t15.2023.12.08 val PER: 0.2756
2026-01-11 15:50:22,659: t15.2023.12.10 val PER: 0.2523
2026-01-11 15:50:22,659: t15.2023.12.17 val PER: 0.2599
2026-01-11 15:50:22,659: t15.2023.12.29 val PER: 0.2732
2026-01-11 15:50:22,659: t15.2024.02.25 val PER: 0.2303
2026-01-11 15:50:22,659: t15.2024.03.08 val PER: 0.3272
2026-01-11 15:50:22,659: t15.2024.03.15 val PER: 0.3089
2026-01-11 15:50:22,659: t15.2024.03.17 val PER: 0.2678
2026-01-11 15:50:22,660: t15.2024.05.10 val PER: 0.2734
2026-01-11 15:50:22,660: t15.2024.06.14 val PER: 0.2618
2026-01-11 15:50:22,660: t15.2024.07.19 val PER: 0.3368
2026-01-11 15:50:22,661: t15.2024.07.21 val PER: 0.2179
2026-01-11 15:50:22,661: t15.2024.07.28 val PER: 0.2721
2026-01-11 15:50:22,661: t15.2025.01.10 val PER: 0.3939
2026-01-11 15:50:22,661: t15.2025.01.12 val PER: 0.2733
2026-01-11 15:50:22,661: t15.2025.03.14 val PER: 0.3817
2026-01-11 15:50:22,662: t15.2025.03.16 val PER: 0.3010
2026-01-11 15:50:22,662: t15.2025.03.30 val PER: 0.3563
2026-01-11 15:50:22,662: t15.2025.04.13 val PER: 0.3395
2026-01-11 15:50:22,663: New best val WER(5gram) 43.87% --> 41.13%
2026-01-11 15:50:22,856: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_12000
2026-01-11 15:50:43,490: Train batch 12200: loss: 25.92 grad norm: 90.63 time: 0.073
2026-01-11 15:51:03,726: Train batch 12400: loss: 19.52 grad norm: 68.11 time: 0.046
2026-01-11 15:51:14,028: Running test after training batch: 12500
2026-01-11 15:51:14,205: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:51:20,641: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 15:51:20,773: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 15:51:45,020: Val batch 12500: PER (avg): 0.2712 CTC Loss (avg): 42.8189 WER(5gram): 42.05% (n=256) time: 30.992
2026-01-11 15:51:45,021: WER lens: avg_true_words=5.99 avg_pred_words=5.82 max_pred_words=11
2026-01-11 15:51:45,022: t15.2023.08.13 val PER: 0.2204
2026-01-11 15:51:45,022: t15.2023.08.18 val PER: 0.2439
2026-01-11 15:51:45,022: t15.2023.08.20 val PER: 0.2081
2026-01-11 15:51:45,022: t15.2023.08.25 val PER: 0.2063
2026-01-11 15:51:45,023: t15.2023.08.27 val PER: 0.3071
2026-01-11 15:51:45,023: t15.2023.09.01 val PER: 0.1778
2026-01-11 15:51:45,023: t15.2023.09.03 val PER: 0.2708
2026-01-11 15:51:45,023: t15.2023.09.24 val PER: 0.2124
2026-01-11 15:51:45,023: t15.2023.09.29 val PER: 0.2489
2026-01-11 15:51:45,024: t15.2023.10.01 val PER: 0.3058
2026-01-11 15:51:45,024: t15.2023.10.06 val PER: 0.1970
2026-01-11 15:51:45,024: t15.2023.10.08 val PER: 0.3599
2026-01-11 15:51:45,024: t15.2023.10.13 val PER: 0.3701
2026-01-11 15:51:45,025: t15.2023.10.15 val PER: 0.2835
2026-01-11 15:51:45,025: t15.2023.10.20 val PER: 0.3087
2026-01-11 15:51:45,025: t15.2023.10.22 val PER: 0.2350
2026-01-11 15:51:45,025: t15.2023.11.03 val PER: 0.2795
2026-01-11 15:51:45,025: t15.2023.11.04 val PER: 0.0683
2026-01-11 15:51:45,025: t15.2023.11.17 val PER: 0.1151
2026-01-11 15:51:45,025: t15.2023.11.19 val PER: 0.1277
2026-01-11 15:51:45,026: t15.2023.11.26 val PER: 0.3203
2026-01-11 15:51:45,026: t15.2023.12.03 val PER: 0.2637
2026-01-11 15:51:45,026: t15.2023.12.08 val PER: 0.2790
2026-01-11 15:51:45,026: t15.2023.12.10 val PER: 0.2470
2026-01-11 15:51:45,026: t15.2023.12.17 val PER: 0.2568
2026-01-11 15:51:45,026: t15.2023.12.29 val PER: 0.2800
2026-01-11 15:51:45,026: t15.2024.02.25 val PER: 0.2261
2026-01-11 15:51:45,026: t15.2024.03.08 val PER: 0.3286
2026-01-11 15:51:45,027: t15.2024.03.15 val PER: 0.3021
2026-01-11 15:51:45,027: t15.2024.03.17 val PER: 0.2643
2026-01-11 15:51:45,027: t15.2024.05.10 val PER: 0.2793
2026-01-11 15:51:45,027: t15.2024.06.14 val PER: 0.2650
2026-01-11 15:51:45,027: t15.2024.07.19 val PER: 0.3362
2026-01-11 15:51:45,027: t15.2024.07.21 val PER: 0.2131
2026-01-11 15:51:45,027: t15.2024.07.28 val PER: 0.2610
2026-01-11 15:51:45,027: t15.2025.01.10 val PER: 0.3953
2026-01-11 15:51:45,027: t15.2025.01.12 val PER: 0.2725
2026-01-11 15:51:45,027: t15.2025.03.14 val PER: 0.3979
2026-01-11 15:51:45,028: t15.2025.03.16 val PER: 0.3037
2026-01-11 15:51:45,028: t15.2025.03.30 val PER: 0.3644
2026-01-11 15:51:45,028: t15.2025.04.13 val PER: 0.3281
2026-01-11 15:51:45,205: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_12500
2026-01-11 15:51:55,775: Train batch 12600: loss: 26.02 grad norm: 105.10 time: 0.063
2026-01-11 15:52:16,584: Train batch 12800: loss: 21.62 grad norm: 62.27 time: 0.062
2026-01-11 15:52:37,735: Train batch 13000: loss: 20.95 grad norm: 62.00 time: 0.072
2026-01-11 15:52:37,735: Running test after training batch: 13000
2026-01-11 15:52:38,193: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:52:48,593: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point it will
2026-01-11 15:52:48,703: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 15:53:08,088: Val batch 13000: PER (avg): 0.2695 CTC Loss (avg): 41.6202 WER(5gram): 42.44% (n=256) time: 30.353
2026-01-11 15:53:08,089: WER lens: avg_true_words=5.99 avg_pred_words=5.67 max_pred_words=12
2026-01-11 15:53:08,089: t15.2023.08.13 val PER: 0.2277
2026-01-11 15:53:08,090: t15.2023.08.18 val PER: 0.2355
2026-01-11 15:53:08,090: t15.2023.08.20 val PER: 0.2073
2026-01-11 15:53:08,090: t15.2023.08.25 val PER: 0.2078
2026-01-11 15:53:08,090: t15.2023.08.27 val PER: 0.2926
2026-01-11 15:53:08,090: t15.2023.09.01 val PER: 0.1778
2026-01-11 15:53:08,090: t15.2023.09.03 val PER: 0.2672
2026-01-11 15:53:08,090: t15.2023.09.24 val PER: 0.2087
2026-01-11 15:53:08,090: t15.2023.09.29 val PER: 0.2610
2026-01-11 15:53:08,091: t15.2023.10.01 val PER: 0.3111
2026-01-11 15:53:08,091: t15.2023.10.06 val PER: 0.2045
2026-01-11 15:53:08,091: t15.2023.10.08 val PER: 0.3681
2026-01-11 15:53:08,091: t15.2023.10.13 val PER: 0.3770
2026-01-11 15:53:08,091: t15.2023.10.15 val PER: 0.2788
2026-01-11 15:53:08,091: t15.2023.10.20 val PER: 0.3087
2026-01-11 15:53:08,092: t15.2023.10.22 val PER: 0.2350
2026-01-11 15:53:08,092: t15.2023.11.03 val PER: 0.2863
2026-01-11 15:53:08,092: t15.2023.11.04 val PER: 0.0683
2026-01-11 15:53:08,092: t15.2023.11.17 val PER: 0.1229
2026-01-11 15:53:08,092: t15.2023.11.19 val PER: 0.1317
2026-01-11 15:53:08,092: t15.2023.11.26 val PER: 0.3297
2026-01-11 15:53:08,092: t15.2023.12.03 val PER: 0.2521
2026-01-11 15:53:08,092: t15.2023.12.08 val PER: 0.2776
2026-01-11 15:53:08,092: t15.2023.12.10 val PER: 0.2549
2026-01-11 15:53:08,093: t15.2023.12.17 val PER: 0.2505
2026-01-11 15:53:08,093: t15.2023.12.29 val PER: 0.2663
2026-01-11 15:53:08,093: t15.2024.02.25 val PER: 0.2261
2026-01-11 15:53:08,093: t15.2024.03.08 val PER: 0.3115
2026-01-11 15:53:08,093: t15.2024.03.15 val PER: 0.3089
2026-01-11 15:53:08,093: t15.2024.03.17 val PER: 0.2664
2026-01-11 15:53:08,093: t15.2024.05.10 val PER: 0.2675
2026-01-11 15:53:08,094: t15.2024.06.14 val PER: 0.2587
2026-01-11 15:53:08,094: t15.2024.07.19 val PER: 0.3362
2026-01-11 15:53:08,094: t15.2024.07.21 val PER: 0.2055
2026-01-11 15:53:08,094: t15.2024.07.28 val PER: 0.2574
2026-01-11 15:53:08,094: t15.2025.01.10 val PER: 0.3857
2026-01-11 15:53:08,094: t15.2025.01.12 val PER: 0.2687
2026-01-11 15:53:08,094: t15.2025.03.14 val PER: 0.3565
2026-01-11 15:53:08,094: t15.2025.03.16 val PER: 0.2880
2026-01-11 15:53:08,094: t15.2025.03.30 val PER: 0.3552
2026-01-11 15:53:08,095: t15.2025.04.13 val PER: 0.3195
2026-01-11 15:53:08,271: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_13000
2026-01-11 15:53:29,268: Train batch 13200: loss: 40.14 grad norm: 103.73 time: 0.062
2026-01-11 15:53:49,282: Train batch 13400: loss: 25.10 grad norm: 89.64 time: 0.068
2026-01-11 15:53:59,537: Running test after training batch: 13500
2026-01-11 15:53:59,686: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:54:06,379: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point it will
2026-01-11 15:54:06,475: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 15:54:29,173: Val batch 13500: PER (avg): 0.2601 CTC Loss (avg): 41.0008 WER(5gram): 40.22% (n=256) time: 29.636
2026-01-11 15:54:29,174: WER lens: avg_true_words=5.99 avg_pred_words=5.84 max_pred_words=11
2026-01-11 15:54:29,175: t15.2023.08.13 val PER: 0.2235
2026-01-11 15:54:29,175: t15.2023.08.18 val PER: 0.2213
2026-01-11 15:54:29,175: t15.2023.08.20 val PER: 0.2017
2026-01-11 15:54:29,175: t15.2023.08.25 val PER: 0.2003
2026-01-11 15:54:29,175: t15.2023.08.27 val PER: 0.2942
2026-01-11 15:54:29,176: t15.2023.09.01 val PER: 0.1656
2026-01-11 15:54:29,176: t15.2023.09.03 val PER: 0.2601
2026-01-11 15:54:29,176: t15.2023.09.24 val PER: 0.2075
2026-01-11 15:54:29,176: t15.2023.09.29 val PER: 0.2406
2026-01-11 15:54:29,176: t15.2023.10.01 val PER: 0.3038
2026-01-11 15:54:29,176: t15.2023.10.06 val PER: 0.2078
2026-01-11 15:54:29,176: t15.2023.10.08 val PER: 0.3572
2026-01-11 15:54:29,177: t15.2023.10.13 val PER: 0.3584
2026-01-11 15:54:29,177: t15.2023.10.15 val PER: 0.2736
2026-01-11 15:54:29,177: t15.2023.10.20 val PER: 0.3054
2026-01-11 15:54:29,177: t15.2023.10.22 val PER: 0.2294
2026-01-11 15:54:29,177: t15.2023.11.03 val PER: 0.2782
2026-01-11 15:54:29,177: t15.2023.11.04 val PER: 0.0648
2026-01-11 15:54:29,177: t15.2023.11.17 val PER: 0.1073
2026-01-11 15:54:29,177: t15.2023.11.19 val PER: 0.1178
2026-01-11 15:54:29,177: t15.2023.11.26 val PER: 0.3101
2026-01-11 15:54:29,177: t15.2023.12.03 val PER: 0.2447
2026-01-11 15:54:29,177: t15.2023.12.08 val PER: 0.2603
2026-01-11 15:54:29,178: t15.2023.12.10 val PER: 0.2286
2026-01-11 15:54:29,178: t15.2023.12.17 val PER: 0.2370
2026-01-11 15:54:29,178: t15.2023.12.29 val PER: 0.2574
2026-01-11 15:54:29,178: t15.2024.02.25 val PER: 0.2149
2026-01-11 15:54:29,178: t15.2024.03.08 val PER: 0.3073
2026-01-11 15:54:29,178: t15.2024.03.15 val PER: 0.2958
2026-01-11 15:54:29,178: t15.2024.03.17 val PER: 0.2566
2026-01-11 15:54:29,178: t15.2024.05.10 val PER: 0.2615
2026-01-11 15:54:29,179: t15.2024.06.14 val PER: 0.2461
2026-01-11 15:54:29,179: t15.2024.07.19 val PER: 0.3237
2026-01-11 15:54:29,179: t15.2024.07.21 val PER: 0.1972
2026-01-11 15:54:29,179: t15.2024.07.28 val PER: 0.2581
2026-01-11 15:54:29,179: t15.2025.01.10 val PER: 0.3788
2026-01-11 15:54:29,179: t15.2025.01.12 val PER: 0.2579
2026-01-11 15:54:29,179: t15.2025.03.14 val PER: 0.3624
2026-01-11 15:54:29,180: t15.2025.03.16 val PER: 0.2880
2026-01-11 15:54:29,180: t15.2025.03.30 val PER: 0.3483
2026-01-11 15:54:29,181: t15.2025.04.13 val PER: 0.3124
2026-01-11 15:54:29,182: New best val WER(5gram) 41.13% --> 40.22%
2026-01-11 15:54:29,366: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_13500
2026-01-11 15:54:39,672: Train batch 13600: loss: 34.89 grad norm: 90.10 time: 0.072
2026-01-11 15:55:00,273: Train batch 13800: loss: 25.92 grad norm: 76.22 time: 0.062
2026-01-11 15:55:21,234: Train batch 14000: loss: 36.17 grad norm: 97.66 time: 0.062
2026-01-11 15:55:21,234: Running test after training batch: 14000
2026-01-11 15:55:21,569: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:55:28,210: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 15:55:28,330: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 15:55:53,577: Val batch 14000: PER (avg): 0.2585 CTC Loss (avg): 40.1593 WER(5gram): 39.77% (n=256) time: 32.342
2026-01-11 15:55:53,578: WER lens: avg_true_words=5.99 avg_pred_words=5.82 max_pred_words=12
2026-01-11 15:55:53,578: t15.2023.08.13 val PER: 0.2141
2026-01-11 15:55:53,578: t15.2023.08.18 val PER: 0.2205
2026-01-11 15:55:53,578: t15.2023.08.20 val PER: 0.2137
2026-01-11 15:55:53,579: t15.2023.08.25 val PER: 0.1852
2026-01-11 15:55:53,579: t15.2023.08.27 val PER: 0.2910
2026-01-11 15:55:53,579: t15.2023.09.01 val PER: 0.1696
2026-01-11 15:55:53,579: t15.2023.09.03 val PER: 0.2518
2026-01-11 15:55:53,579: t15.2023.09.24 val PER: 0.1954
2026-01-11 15:55:53,579: t15.2023.09.29 val PER: 0.2425
2026-01-11 15:55:53,580: t15.2023.10.01 val PER: 0.3012
2026-01-11 15:55:53,580: t15.2023.10.06 val PER: 0.1851
2026-01-11 15:55:53,580: t15.2023.10.08 val PER: 0.3599
2026-01-11 15:55:53,580: t15.2023.10.13 val PER: 0.3623
2026-01-11 15:55:53,580: t15.2023.10.15 val PER: 0.2729
2026-01-11 15:55:53,581: t15.2023.10.20 val PER: 0.2953
2026-01-11 15:55:53,581: t15.2023.10.22 val PER: 0.2238
2026-01-11 15:55:53,581: t15.2023.11.03 val PER: 0.2720
2026-01-11 15:55:53,581: t15.2023.11.04 val PER: 0.0512
2026-01-11 15:55:53,581: t15.2023.11.17 val PER: 0.1073
2026-01-11 15:55:53,581: t15.2023.11.19 val PER: 0.1118
2026-01-11 15:55:53,581: t15.2023.11.26 val PER: 0.3094
2026-01-11 15:55:53,582: t15.2023.12.03 val PER: 0.2426
2026-01-11 15:55:53,582: t15.2023.12.08 val PER: 0.2690
2026-01-11 15:55:53,582: t15.2023.12.10 val PER: 0.2221
2026-01-11 15:55:53,582: t15.2023.12.17 val PER: 0.2308
2026-01-11 15:55:53,582: t15.2023.12.29 val PER: 0.2485
2026-01-11 15:55:53,582: t15.2024.02.25 val PER: 0.2247
2026-01-11 15:55:53,582: t15.2024.03.08 val PER: 0.3272
2026-01-11 15:55:53,583: t15.2024.03.15 val PER: 0.2921
2026-01-11 15:55:53,583: t15.2024.03.17 val PER: 0.2587
2026-01-11 15:55:53,583: t15.2024.05.10 val PER: 0.2719
2026-01-11 15:55:53,583: t15.2024.06.14 val PER: 0.2382
2026-01-11 15:55:53,583: t15.2024.07.19 val PER: 0.3164
2026-01-11 15:55:53,584: t15.2024.07.21 val PER: 0.1924
2026-01-11 15:55:53,584: t15.2024.07.28 val PER: 0.2493
2026-01-11 15:55:53,584: t15.2025.01.10 val PER: 0.3678
2026-01-11 15:55:53,584: t15.2025.01.12 val PER: 0.2617
2026-01-11 15:55:53,584: t15.2025.03.14 val PER: 0.3772
2026-01-11 15:55:53,585: t15.2025.03.16 val PER: 0.3010
2026-01-11 15:55:53,585: t15.2025.03.30 val PER: 0.3506
2026-01-11 15:55:53,585: t15.2025.04.13 val PER: 0.3124
2026-01-11 15:55:53,585: New best val WER(5gram) 40.22% --> 39.77%
2026-01-11 15:55:53,769: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_14000
2026-01-11 15:56:14,425: Train batch 14200: loss: 29.44 grad norm: 69.03 time: 0.062
2026-01-11 15:56:35,438: Train batch 14400: loss: 22.09 grad norm: 53.85 time: 0.075
2026-01-11 15:56:45,337: Running test after training batch: 14500
2026-01-11 15:56:45,516: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:56:52,438: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 15:56:52,560: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost and
2026-01-11 15:57:13,386: Val batch 14500: PER (avg): 0.2543 CTC Loss (avg): 39.8537 WER(5gram): 36.25% (n=256) time: 28.049
2026-01-11 15:57:13,387: WER lens: avg_true_words=5.99 avg_pred_words=5.81 max_pred_words=11
2026-01-11 15:57:13,387: t15.2023.08.13 val PER: 0.2152
2026-01-11 15:57:13,388: t15.2023.08.18 val PER: 0.2137
2026-01-11 15:57:13,388: t15.2023.08.20 val PER: 0.2049
2026-01-11 15:57:13,388: t15.2023.08.25 val PER: 0.2018
2026-01-11 15:57:13,388: t15.2023.08.27 val PER: 0.2974
2026-01-11 15:57:13,388: t15.2023.09.01 val PER: 0.1583
2026-01-11 15:57:13,388: t15.2023.09.03 val PER: 0.2542
2026-01-11 15:57:13,389: t15.2023.09.24 val PER: 0.1954
2026-01-11 15:57:13,389: t15.2023.09.29 val PER: 0.2387
2026-01-11 15:57:13,389: t15.2023.10.01 val PER: 0.2913
2026-01-11 15:57:13,389: t15.2023.10.06 val PER: 0.1905
2026-01-11 15:57:13,389: t15.2023.10.08 val PER: 0.3491
2026-01-11 15:57:13,389: t15.2023.10.13 val PER: 0.3646
2026-01-11 15:57:13,389: t15.2023.10.15 val PER: 0.2650
2026-01-11 15:57:13,389: t15.2023.10.20 val PER: 0.2852
2026-01-11 15:57:13,389: t15.2023.10.22 val PER: 0.2194
2026-01-11 15:57:13,389: t15.2023.11.03 val PER: 0.2700
2026-01-11 15:57:13,389: t15.2023.11.04 val PER: 0.0512
2026-01-11 15:57:13,390: t15.2023.11.17 val PER: 0.1011
2026-01-11 15:57:13,390: t15.2023.11.19 val PER: 0.1118
2026-01-11 15:57:13,390: t15.2023.11.26 val PER: 0.2978
2026-01-11 15:57:13,390: t15.2023.12.03 val PER: 0.2384
2026-01-11 15:57:13,390: t15.2023.12.08 val PER: 0.2570
2026-01-11 15:57:13,390: t15.2023.12.10 val PER: 0.2181
2026-01-11 15:57:13,390: t15.2023.12.17 val PER: 0.2141
2026-01-11 15:57:13,390: t15.2023.12.29 val PER: 0.2437
2026-01-11 15:57:13,390: t15.2024.02.25 val PER: 0.2008
2026-01-11 15:57:13,390: t15.2024.03.08 val PER: 0.3044
2026-01-11 15:57:13,391: t15.2024.03.15 val PER: 0.2902
2026-01-11 15:57:13,391: t15.2024.03.17 val PER: 0.2538
2026-01-11 15:57:13,391: t15.2024.05.10 val PER: 0.2600
2026-01-11 15:57:13,391: t15.2024.06.14 val PER: 0.2587
2026-01-11 15:57:13,391: t15.2024.07.19 val PER: 0.3204
2026-01-11 15:57:13,392: t15.2024.07.21 val PER: 0.1924
2026-01-11 15:57:13,392: t15.2024.07.28 val PER: 0.2493
2026-01-11 15:57:13,392: t15.2025.01.10 val PER: 0.3912
2026-01-11 15:57:13,392: t15.2025.01.12 val PER: 0.2571
2026-01-11 15:57:13,393: t15.2025.03.14 val PER: 0.3683
2026-01-11 15:57:13,393: t15.2025.03.16 val PER: 0.2814
2026-01-11 15:57:13,393: t15.2025.03.30 val PER: 0.3448
2026-01-11 15:57:13,393: t15.2025.04.13 val PER: 0.3096
2026-01-11 15:57:13,394: New best val WER(5gram) 39.77% --> 36.25%
2026-01-11 15:57:13,574: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_14500
2026-01-11 15:57:23,459: Train batch 14600: loss: 36.33 grad norm: 115.74 time: 0.075
2026-01-11 15:57:44,346: Train batch 14800: loss: 20.20 grad norm: 72.13 time: 0.058
2026-01-11 15:58:05,391: Train batch 15000: loss: 25.15 grad norm: 68.92 time: 0.060
2026-01-11 15:58:05,392: Running test after training batch: 15000
2026-01-11 15:58:05,582: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:58:12,248: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 15:58:12,358: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 15:58:36,899: Val batch 15000: PER (avg): 0.2445 CTC Loss (avg): 39.2298 WER(5gram): 32.79% (n=256) time: 31.506
2026-01-11 15:58:36,900: WER lens: avg_true_words=5.99 avg_pred_words=5.92 max_pred_words=11
2026-01-11 15:58:36,900: t15.2023.08.13 val PER: 0.1996
2026-01-11 15:58:36,900: t15.2023.08.18 val PER: 0.2020
2026-01-11 15:58:36,900: t15.2023.08.20 val PER: 0.1851
2026-01-11 15:58:36,900: t15.2023.08.25 val PER: 0.1792
2026-01-11 15:58:36,901: t15.2023.08.27 val PER: 0.2830
2026-01-11 15:58:36,901: t15.2023.09.01 val PER: 0.1502
2026-01-11 15:58:36,901: t15.2023.09.03 val PER: 0.2447
2026-01-11 15:58:36,901: t15.2023.09.24 val PER: 0.1833
2026-01-11 15:58:36,901: t15.2023.09.29 val PER: 0.2253
2026-01-11 15:58:36,901: t15.2023.10.01 val PER: 0.2820
2026-01-11 15:58:36,901: t15.2023.10.06 val PER: 0.1884
2026-01-11 15:58:36,902: t15.2023.10.08 val PER: 0.3464
2026-01-11 15:58:36,902: t15.2023.10.13 val PER: 0.3452
2026-01-11 15:58:36,902: t15.2023.10.15 val PER: 0.2531
2026-01-11 15:58:36,902: t15.2023.10.20 val PER: 0.2987
2026-01-11 15:58:36,902: t15.2023.10.22 val PER: 0.2238
2026-01-11 15:58:36,902: t15.2023.11.03 val PER: 0.2605
2026-01-11 15:58:36,902: t15.2023.11.04 val PER: 0.0478
2026-01-11 15:58:36,902: t15.2023.11.17 val PER: 0.0886
2026-01-11 15:58:36,902: t15.2023.11.19 val PER: 0.0978
2026-01-11 15:58:36,903: t15.2023.11.26 val PER: 0.2928
2026-01-11 15:58:36,903: t15.2023.12.03 val PER: 0.2174
2026-01-11 15:58:36,903: t15.2023.12.08 val PER: 0.2463
2026-01-11 15:58:36,903: t15.2023.12.10 val PER: 0.2076
2026-01-11 15:58:36,903: t15.2023.12.17 val PER: 0.2048
2026-01-11 15:58:36,903: t15.2023.12.29 val PER: 0.2395
2026-01-11 15:58:36,903: t15.2024.02.25 val PER: 0.2093
2026-01-11 15:58:36,904: t15.2024.03.08 val PER: 0.3058
2026-01-11 15:58:36,904: t15.2024.03.15 val PER: 0.2883
2026-01-11 15:58:36,904: t15.2024.03.17 val PER: 0.2413
2026-01-11 15:58:36,904: t15.2024.05.10 val PER: 0.2571
2026-01-11 15:58:36,905: t15.2024.06.14 val PER: 0.2334
2026-01-11 15:58:36,905: t15.2024.07.19 val PER: 0.3039
2026-01-11 15:58:36,905: t15.2024.07.21 val PER: 0.1821
2026-01-11 15:58:36,905: t15.2024.07.28 val PER: 0.2419
2026-01-11 15:58:36,906: t15.2025.01.10 val PER: 0.3705
2026-01-11 15:58:36,906: t15.2025.01.12 val PER: 0.2479
2026-01-11 15:58:36,906: t15.2025.03.14 val PER: 0.3861
2026-01-11 15:58:36,906: t15.2025.03.16 val PER: 0.2788
2026-01-11 15:58:36,906: t15.2025.03.30 val PER: 0.3345
2026-01-11 15:58:36,906: t15.2025.04.13 val PER: 0.2782
2026-01-11 15:58:36,907: New best val WER(5gram) 36.25% --> 32.79%
2026-01-11 15:58:37,088: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_15000
2026-01-11 15:58:57,881: Train batch 15200: loss: 18.57 grad norm: 64.32 time: 0.064
2026-01-11 15:59:18,189: Train batch 15400: loss: 32.05 grad norm: 80.76 time: 0.058
2026-01-11 15:59:28,887: Running test after training batch: 15500
2026-01-11 15:59:29,358: WER debug GT example: You can see the code at this point as well.
2026-01-11 15:59:35,806: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 15:59:35,878: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 15:59:56,932: Val batch 15500: PER (avg): 0.2434 CTC Loss (avg): 38.7761 WER(5gram): 33.96% (n=256) time: 28.044
2026-01-11 15:59:56,933: WER lens: avg_true_words=5.99 avg_pred_words=5.93 max_pred_words=11
2026-01-11 15:59:56,933: t15.2023.08.13 val PER: 0.2089
2026-01-11 15:59:56,933: t15.2023.08.18 val PER: 0.2037
2026-01-11 15:59:56,933: t15.2023.08.20 val PER: 0.1882
2026-01-11 15:59:56,933: t15.2023.08.25 val PER: 0.1747
2026-01-11 15:59:56,934: t15.2023.08.27 val PER: 0.2862
2026-01-11 15:59:56,934: t15.2023.09.01 val PER: 0.1510
2026-01-11 15:59:56,934: t15.2023.09.03 val PER: 0.2470
2026-01-11 15:59:56,934: t15.2023.09.24 val PER: 0.1917
2026-01-11 15:59:56,934: t15.2023.09.29 val PER: 0.2227
2026-01-11 15:59:56,934: t15.2023.10.01 val PER: 0.2853
2026-01-11 15:59:56,934: t15.2023.10.06 val PER: 0.1755
2026-01-11 15:59:56,934: t15.2023.10.08 val PER: 0.3424
2026-01-11 15:59:56,934: t15.2023.10.13 val PER: 0.3584
2026-01-11 15:59:56,935: t15.2023.10.15 val PER: 0.2498
2026-01-11 15:59:56,935: t15.2023.10.20 val PER: 0.2987
2026-01-11 15:59:56,935: t15.2023.10.22 val PER: 0.2116
2026-01-11 15:59:56,935: t15.2023.11.03 val PER: 0.2639
2026-01-11 15:59:56,935: t15.2023.11.04 val PER: 0.0478
2026-01-11 15:59:56,935: t15.2023.11.17 val PER: 0.0964
2026-01-11 15:59:56,935: t15.2023.11.19 val PER: 0.1038
2026-01-11 15:59:56,935: t15.2023.11.26 val PER: 0.2877
2026-01-11 15:59:56,935: t15.2023.12.03 val PER: 0.2080
2026-01-11 15:59:56,936: t15.2023.12.08 val PER: 0.2390
2026-01-11 15:59:56,936: t15.2023.12.10 val PER: 0.2011
2026-01-11 15:59:56,936: t15.2023.12.17 val PER: 0.1975
2026-01-11 15:59:56,936: t15.2023.12.29 val PER: 0.2375
2026-01-11 15:59:56,936: t15.2024.02.25 val PER: 0.1938
2026-01-11 15:59:56,936: t15.2024.03.08 val PER: 0.3073
2026-01-11 15:59:56,936: t15.2024.03.15 val PER: 0.2871
2026-01-11 15:59:56,937: t15.2024.03.17 val PER: 0.2392
2026-01-11 15:59:56,937: t15.2024.05.10 val PER: 0.2526
2026-01-11 15:59:56,937: t15.2024.06.14 val PER: 0.2350
2026-01-11 15:59:56,938: t15.2024.07.19 val PER: 0.3013
2026-01-11 15:59:56,938: t15.2024.07.21 val PER: 0.1841
2026-01-11 15:59:56,938: t15.2024.07.28 val PER: 0.2485
2026-01-11 15:59:56,938: t15.2025.01.10 val PER: 0.3567
2026-01-11 15:59:56,938: t15.2025.01.12 val PER: 0.2417
2026-01-11 15:59:56,938: t15.2025.03.14 val PER: 0.3624
2026-01-11 15:59:56,938: t15.2025.03.16 val PER: 0.2866
2026-01-11 15:59:56,938: t15.2025.03.30 val PER: 0.3437
2026-01-11 15:59:56,938: t15.2025.04.13 val PER: 0.2839
2026-01-11 15:59:57,114: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_15500
2026-01-11 16:00:07,591: Train batch 15600: loss: 38.75 grad norm: 99.22 time: 0.078
2026-01-11 16:00:27,824: Train batch 15800: loss: 42.80 grad norm: 93.57 time: 0.076
2026-01-11 16:00:49,492: Train batch 16000: loss: 25.27 grad norm: 65.96 time: 0.071
2026-01-11 16:00:49,492: Running test after training batch: 16000
2026-01-11 16:00:49,645: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:00:56,237: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:00:56,314: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 16:01:16,223: Val batch 16000: PER (avg): 0.2351 CTC Loss (avg): 38.2665 WER(5gram): 31.68% (n=256) time: 26.730
2026-01-11 16:01:16,224: WER lens: avg_true_words=5.99 avg_pred_words=5.97 max_pred_words=12
2026-01-11 16:01:16,224: t15.2023.08.13 val PER: 0.1985
2026-01-11 16:01:16,225: t15.2023.08.18 val PER: 0.1936
2026-01-11 16:01:16,225: t15.2023.08.20 val PER: 0.1652
2026-01-11 16:01:16,225: t15.2023.08.25 val PER: 0.1762
2026-01-11 16:01:16,225: t15.2023.08.27 val PER: 0.2717
2026-01-11 16:01:16,225: t15.2023.09.01 val PER: 0.1429
2026-01-11 16:01:16,225: t15.2023.09.03 val PER: 0.2316
2026-01-11 16:01:16,226: t15.2023.09.24 val PER: 0.1893
2026-01-11 16:01:16,226: t15.2023.09.29 val PER: 0.2125
2026-01-11 16:01:16,226: t15.2023.10.01 val PER: 0.2721
2026-01-11 16:01:16,226: t15.2023.10.06 val PER: 0.1733
2026-01-11 16:01:16,226: t15.2023.10.08 val PER: 0.3342
2026-01-11 16:01:16,227: t15.2023.10.13 val PER: 0.3367
2026-01-11 16:01:16,227: t15.2023.10.15 val PER: 0.2479
2026-01-11 16:01:16,227: t15.2023.10.20 val PER: 0.2785
2026-01-11 16:01:16,227: t15.2023.10.22 val PER: 0.2216
2026-01-11 16:01:16,227: t15.2023.11.03 val PER: 0.2524
2026-01-11 16:01:16,228: t15.2023.11.04 val PER: 0.0444
2026-01-11 16:01:16,228: t15.2023.11.17 val PER: 0.0886
2026-01-11 16:01:16,228: t15.2023.11.19 val PER: 0.1018
2026-01-11 16:01:16,228: t15.2023.11.26 val PER: 0.2768
2026-01-11 16:01:16,228: t15.2023.12.03 val PER: 0.2101
2026-01-11 16:01:16,228: t15.2023.12.08 val PER: 0.2230
2026-01-11 16:01:16,228: t15.2023.12.10 val PER: 0.2037
2026-01-11 16:01:16,228: t15.2023.12.17 val PER: 0.2006
2026-01-11 16:01:16,228: t15.2023.12.29 val PER: 0.2258
2026-01-11 16:01:16,229: t15.2024.02.25 val PER: 0.1854
2026-01-11 16:01:16,229: t15.2024.03.08 val PER: 0.2987
2026-01-11 16:01:16,229: t15.2024.03.15 val PER: 0.2783
2026-01-11 16:01:16,229: t15.2024.03.17 val PER: 0.2322
2026-01-11 16:01:16,229: t15.2024.05.10 val PER: 0.2407
2026-01-11 16:01:16,230: t15.2024.06.14 val PER: 0.2208
2026-01-11 16:01:16,230: t15.2024.07.19 val PER: 0.2927
2026-01-11 16:01:16,230: t15.2024.07.21 val PER: 0.1800
2026-01-11 16:01:16,230: t15.2024.07.28 val PER: 0.2412
2026-01-11 16:01:16,230: t15.2025.01.10 val PER: 0.3719
2026-01-11 16:01:16,230: t15.2025.01.12 val PER: 0.2317
2026-01-11 16:01:16,230: t15.2025.03.14 val PER: 0.3432
2026-01-11 16:01:16,230: t15.2025.03.16 val PER: 0.2801
2026-01-11 16:01:16,230: t15.2025.03.30 val PER: 0.3184
2026-01-11 16:01:16,230: t15.2025.04.13 val PER: 0.2953
2026-01-11 16:01:16,232: New best val WER(5gram) 32.79% --> 31.68%
2026-01-11 16:01:16,422: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_16000
2026-01-11 16:01:37,381: Train batch 16200: loss: 23.16 grad norm: 56.90 time: 0.068
2026-01-11 16:01:58,589: Train batch 16400: loss: 27.02 grad norm: 81.31 time: 0.068
2026-01-11 16:02:09,390: Running test after training batch: 16500
2026-01-11 16:02:09,559: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:02:15,697: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:02:15,775: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 16:02:32,797: Val batch 16500: PER (avg): 0.2363 CTC Loss (avg): 37.2190 WER(5gram): 38.59% (n=256) time: 23.407
2026-01-11 16:02:32,798: WER lens: avg_true_words=5.99 avg_pred_words=5.90 max_pred_words=12
2026-01-11 16:02:32,798: t15.2023.08.13 val PER: 0.1913
2026-01-11 16:02:32,799: t15.2023.08.18 val PER: 0.2012
2026-01-11 16:02:32,799: t15.2023.08.20 val PER: 0.1739
2026-01-11 16:02:32,799: t15.2023.08.25 val PER: 0.1777
2026-01-11 16:02:32,799: t15.2023.08.27 val PER: 0.2733
2026-01-11 16:02:32,799: t15.2023.09.01 val PER: 0.1477
2026-01-11 16:02:32,799: t15.2023.09.03 val PER: 0.2268
2026-01-11 16:02:32,799: t15.2023.09.24 val PER: 0.1869
2026-01-11 16:02:32,799: t15.2023.09.29 val PER: 0.2087
2026-01-11 16:02:32,799: t15.2023.10.01 val PER: 0.2642
2026-01-11 16:02:32,799: t15.2023.10.06 val PER: 0.1722
2026-01-11 16:02:32,800: t15.2023.10.08 val PER: 0.3396
2026-01-11 16:02:32,800: t15.2023.10.13 val PER: 0.3398
2026-01-11 16:02:32,800: t15.2023.10.15 val PER: 0.2413
2026-01-11 16:02:32,800: t15.2023.10.20 val PER: 0.2886
2026-01-11 16:02:32,800: t15.2023.10.22 val PER: 0.2038
2026-01-11 16:02:32,800: t15.2023.11.03 val PER: 0.2551
2026-01-11 16:02:32,800: t15.2023.11.04 val PER: 0.0375
2026-01-11 16:02:32,800: t15.2023.11.17 val PER: 0.0902
2026-01-11 16:02:32,801: t15.2023.11.19 val PER: 0.1018
2026-01-11 16:02:32,801: t15.2023.11.26 val PER: 0.2797
2026-01-11 16:02:32,801: t15.2023.12.03 val PER: 0.2153
2026-01-11 16:02:32,801: t15.2023.12.08 val PER: 0.2257
2026-01-11 16:02:32,801: t15.2023.12.10 val PER: 0.2011
2026-01-11 16:02:32,801: t15.2023.12.17 val PER: 0.2100
2026-01-11 16:02:32,801: t15.2023.12.29 val PER: 0.2292
2026-01-11 16:02:32,801: t15.2024.02.25 val PER: 0.1938
2026-01-11 16:02:32,801: t15.2024.03.08 val PER: 0.3073
2026-01-11 16:02:32,801: t15.2024.03.15 val PER: 0.2839
2026-01-11 16:02:32,801: t15.2024.03.17 val PER: 0.2385
2026-01-11 16:02:32,801: t15.2024.05.10 val PER: 0.2318
2026-01-11 16:02:32,802: t15.2024.06.14 val PER: 0.2192
2026-01-11 16:02:32,802: t15.2024.07.19 val PER: 0.2947
2026-01-11 16:02:32,802: t15.2024.07.21 val PER: 0.1731
2026-01-11 16:02:32,802: t15.2024.07.28 val PER: 0.2353
2026-01-11 16:02:32,802: t15.2025.01.10 val PER: 0.3664
2026-01-11 16:02:32,802: t15.2025.01.12 val PER: 0.2363
2026-01-11 16:02:32,802: t15.2025.03.14 val PER: 0.3757
2026-01-11 16:02:32,802: t15.2025.03.16 val PER: 0.2762
2026-01-11 16:02:32,802: t15.2025.03.30 val PER: 0.3425
2026-01-11 16:02:32,802: t15.2025.04.13 val PER: 0.2810
2026-01-11 16:02:32,984: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_16500
2026-01-11 16:02:43,615: Train batch 16600: loss: 27.42 grad norm: 80.68 time: 0.062
2026-01-11 16:03:04,584: Train batch 16800: loss: 39.69 grad norm: 107.44 time: 0.073
2026-01-11 16:03:24,964: Train batch 17000: loss: 21.27 grad norm: 62.34 time: 0.088
2026-01-11 16:03:24,964: Running test after training batch: 17000
2026-01-11 16:03:25,082: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:03:31,617: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:03:31,713: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost and
2026-01-11 16:03:51,932: Val batch 17000: PER (avg): 0.2280 CTC Loss (avg): 37.3062 WER(5gram): 30.05% (n=256) time: 26.968
2026-01-11 16:03:51,933: WER lens: avg_true_words=5.99 avg_pred_words=5.95 max_pred_words=11
2026-01-11 16:03:51,934: t15.2023.08.13 val PER: 0.1902
2026-01-11 16:03:51,934: t15.2023.08.18 val PER: 0.1920
2026-01-11 16:03:51,934: t15.2023.08.20 val PER: 0.1732
2026-01-11 16:03:51,935: t15.2023.08.25 val PER: 0.1687
2026-01-11 16:03:51,935: t15.2023.08.27 val PER: 0.2637
2026-01-11 16:03:51,935: t15.2023.09.01 val PER: 0.1404
2026-01-11 16:03:51,935: t15.2023.09.03 val PER: 0.2280
2026-01-11 16:03:51,935: t15.2023.09.24 val PER: 0.1723
2026-01-11 16:03:51,935: t15.2023.09.29 val PER: 0.2042
2026-01-11 16:03:51,935: t15.2023.10.01 val PER: 0.2695
2026-01-11 16:03:51,935: t15.2023.10.06 val PER: 0.1572
2026-01-11 16:03:51,935: t15.2023.10.08 val PER: 0.3356
2026-01-11 16:03:51,936: t15.2023.10.13 val PER: 0.3274
2026-01-11 16:03:51,936: t15.2023.10.15 val PER: 0.2419
2026-01-11 16:03:51,936: t15.2023.10.20 val PER: 0.2785
2026-01-11 16:03:51,936: t15.2023.10.22 val PER: 0.2071
2026-01-11 16:03:51,936: t15.2023.11.03 val PER: 0.2517
2026-01-11 16:03:51,936: t15.2023.11.04 val PER: 0.0444
2026-01-11 16:03:51,936: t15.2023.11.17 val PER: 0.0855
2026-01-11 16:03:51,936: t15.2023.11.19 val PER: 0.1038
2026-01-11 16:03:51,936: t15.2023.11.26 val PER: 0.2667
2026-01-11 16:03:51,936: t15.2023.12.03 val PER: 0.2059
2026-01-11 16:03:51,937: t15.2023.12.08 val PER: 0.2164
2026-01-11 16:03:51,937: t15.2023.12.10 val PER: 0.1879
2026-01-11 16:03:51,937: t15.2023.12.17 val PER: 0.1965
2026-01-11 16:03:51,938: t15.2023.12.29 val PER: 0.2162
2026-01-11 16:03:51,938: t15.2024.02.25 val PER: 0.1840
2026-01-11 16:03:51,938: t15.2024.03.08 val PER: 0.2831
2026-01-11 16:03:51,938: t15.2024.03.15 val PER: 0.2745
2026-01-11 16:03:51,938: t15.2024.03.17 val PER: 0.2218
2026-01-11 16:03:51,938: t15.2024.05.10 val PER: 0.2407
2026-01-11 16:03:51,939: t15.2024.06.14 val PER: 0.2082
2026-01-11 16:03:51,939: t15.2024.07.19 val PER: 0.2795
2026-01-11 16:03:51,939: t15.2024.07.21 val PER: 0.1655
2026-01-11 16:03:51,939: t15.2024.07.28 val PER: 0.2279
2026-01-11 16:03:51,939: t15.2025.01.10 val PER: 0.3416
2026-01-11 16:03:51,939: t15.2025.01.12 val PER: 0.2179
2026-01-11 16:03:51,939: t15.2025.03.14 val PER: 0.3521
2026-01-11 16:03:51,939: t15.2025.03.16 val PER: 0.2618
2026-01-11 16:03:51,939: t15.2025.03.30 val PER: 0.3425
2026-01-11 16:03:51,939: t15.2025.04.13 val PER: 0.2767
2026-01-11 16:03:51,941: New best val WER(5gram) 31.68% --> 30.05%
2026-01-11 16:03:52,118: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_17000
2026-01-11 16:04:13,337: Train batch 17200: loss: 28.40 grad norm: 71.66 time: 0.100
2026-01-11 16:04:34,490: Train batch 17400: loss: 33.68 grad norm: 85.27 time: 0.079
2026-01-11 16:04:44,743: Running test after training batch: 17500
2026-01-11 16:04:44,876: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:04:51,386: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-11 16:04:51,477: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs and
2026-01-11 16:05:09,563: Val batch 17500: PER (avg): 0.2278 CTC Loss (avg): 36.6445 WER(5gram): 29.73% (n=256) time: 24.819
2026-01-11 16:05:09,564: WER lens: avg_true_words=5.99 avg_pred_words=5.92 max_pred_words=11
2026-01-11 16:05:09,565: t15.2023.08.13 val PER: 0.1871
2026-01-11 16:05:09,565: t15.2023.08.18 val PER: 0.1945
2026-01-11 16:05:09,565: t15.2023.08.20 val PER: 0.1668
2026-01-11 16:05:09,565: t15.2023.08.25 val PER: 0.1732
2026-01-11 16:05:09,565: t15.2023.08.27 val PER: 0.2605
2026-01-11 16:05:09,565: t15.2023.09.01 val PER: 0.1347
2026-01-11 16:05:09,565: t15.2023.09.03 val PER: 0.2280
2026-01-11 16:05:09,566: t15.2023.09.24 val PER: 0.1808
2026-01-11 16:05:09,566: t15.2023.09.29 val PER: 0.1991
2026-01-11 16:05:09,566: t15.2023.10.01 val PER: 0.2682
2026-01-11 16:05:09,566: t15.2023.10.06 val PER: 0.1744
2026-01-11 16:05:09,566: t15.2023.10.08 val PER: 0.3315
2026-01-11 16:05:09,566: t15.2023.10.13 val PER: 0.3336
2026-01-11 16:05:09,566: t15.2023.10.15 val PER: 0.2399
2026-01-11 16:05:09,567: t15.2023.10.20 val PER: 0.2886
2026-01-11 16:05:09,567: t15.2023.10.22 val PER: 0.2027
2026-01-11 16:05:09,567: t15.2023.11.03 val PER: 0.2456
2026-01-11 16:05:09,567: t15.2023.11.04 val PER: 0.0375
2026-01-11 16:05:09,567: t15.2023.11.17 val PER: 0.0855
2026-01-11 16:05:09,567: t15.2023.11.19 val PER: 0.0978
2026-01-11 16:05:09,568: t15.2023.11.26 val PER: 0.2587
2026-01-11 16:05:09,568: t15.2023.12.03 val PER: 0.2027
2026-01-11 16:05:09,568: t15.2023.12.08 val PER: 0.2150
2026-01-11 16:05:09,568: t15.2023.12.10 val PER: 0.1919
2026-01-11 16:05:09,569: t15.2023.12.17 val PER: 0.2027
2026-01-11 16:05:09,569: t15.2023.12.29 val PER: 0.2176
2026-01-11 16:05:09,569: t15.2024.02.25 val PER: 0.1826
2026-01-11 16:05:09,569: t15.2024.03.08 val PER: 0.2888
2026-01-11 16:05:09,569: t15.2024.03.15 val PER: 0.2733
2026-01-11 16:05:09,569: t15.2024.03.17 val PER: 0.2273
2026-01-11 16:05:09,570: t15.2024.05.10 val PER: 0.2363
2026-01-11 16:05:09,570: t15.2024.06.14 val PER: 0.2003
2026-01-11 16:05:09,570: t15.2024.07.19 val PER: 0.2828
2026-01-11 16:05:09,570: t15.2024.07.21 val PER: 0.1717
2026-01-11 16:05:09,570: t15.2024.07.28 val PER: 0.2316
2026-01-11 16:05:09,570: t15.2025.01.10 val PER: 0.3526
2026-01-11 16:05:09,570: t15.2025.01.12 val PER: 0.2225
2026-01-11 16:05:09,570: t15.2025.03.14 val PER: 0.3476
2026-01-11 16:05:09,570: t15.2025.03.16 val PER: 0.2618
2026-01-11 16:05:09,570: t15.2025.03.30 val PER: 0.3138
2026-01-11 16:05:09,571: t15.2025.04.13 val PER: 0.2867
2026-01-11 16:05:09,572: New best val WER(5gram) 30.05% --> 29.73%
2026-01-11 16:05:09,757: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_17500
2026-01-11 16:05:19,714: Train batch 17600: loss: 32.11 grad norm: 101.38 time: 0.060
2026-01-11 16:05:40,584: Train batch 17800: loss: 17.11 grad norm: 143.25 time: 0.047
2026-01-11 16:06:01,016: Train batch 18000: loss: 27.08 grad norm: 81.52 time: 0.067
2026-01-11 16:06:01,016: Running test after training batch: 18000
2026-01-11 16:06:01,180: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:06:08,229: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:06:08,364: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost k
2026-01-11 16:06:29,532: Val batch 18000: PER (avg): 0.2262 CTC Loss (avg): 36.1398 WER(5gram): 32.33% (n=256) time: 28.515
2026-01-11 16:06:29,533: WER lens: avg_true_words=5.99 avg_pred_words=6.00 max_pred_words=13
2026-01-11 16:06:29,533: t15.2023.08.13 val PER: 0.1881
2026-01-11 16:06:29,533: t15.2023.08.18 val PER: 0.1953
2026-01-11 16:06:29,534: t15.2023.08.20 val PER: 0.1620
2026-01-11 16:06:29,534: t15.2023.08.25 val PER: 0.1717
2026-01-11 16:06:29,534: t15.2023.08.27 val PER: 0.2492
2026-01-11 16:06:29,534: t15.2023.09.01 val PER: 0.1437
2026-01-11 16:06:29,534: t15.2023.09.03 val PER: 0.2209
2026-01-11 16:06:29,534: t15.2023.09.24 val PER: 0.1723
2026-01-11 16:06:29,534: t15.2023.09.29 val PER: 0.1972
2026-01-11 16:06:29,534: t15.2023.10.01 val PER: 0.2589
2026-01-11 16:06:29,534: t15.2023.10.06 val PER: 0.1744
2026-01-11 16:06:29,534: t15.2023.10.08 val PER: 0.3342
2026-01-11 16:06:29,535: t15.2023.10.13 val PER: 0.3258
2026-01-11 16:06:29,535: t15.2023.10.15 val PER: 0.2320
2026-01-11 16:06:29,535: t15.2023.10.20 val PER: 0.2752
2026-01-11 16:06:29,535: t15.2023.10.22 val PER: 0.1982
2026-01-11 16:06:29,535: t15.2023.11.03 val PER: 0.2510
2026-01-11 16:06:29,535: t15.2023.11.04 val PER: 0.0273
2026-01-11 16:06:29,536: t15.2023.11.17 val PER: 0.0902
2026-01-11 16:06:29,536: t15.2023.11.19 val PER: 0.0918
2026-01-11 16:06:29,536: t15.2023.11.26 val PER: 0.2514
2026-01-11 16:06:29,536: t15.2023.12.03 val PER: 0.2027
2026-01-11 16:06:29,536: t15.2023.12.08 val PER: 0.2230
2026-01-11 16:06:29,537: t15.2023.12.10 val PER: 0.1787
2026-01-11 16:06:29,537: t15.2023.12.17 val PER: 0.1965
2026-01-11 16:06:29,538: t15.2023.12.29 val PER: 0.2141
2026-01-11 16:06:29,538: t15.2024.02.25 val PER: 0.1812
2026-01-11 16:06:29,538: t15.2024.03.08 val PER: 0.3001
2026-01-11 16:06:29,538: t15.2024.03.15 val PER: 0.2639
2026-01-11 16:06:29,538: t15.2024.03.17 val PER: 0.2252
2026-01-11 16:06:29,538: t15.2024.05.10 val PER: 0.2318
2026-01-11 16:06:29,539: t15.2024.06.14 val PER: 0.2114
2026-01-11 16:06:29,539: t15.2024.07.19 val PER: 0.2914
2026-01-11 16:06:29,539: t15.2024.07.21 val PER: 0.1648
2026-01-11 16:06:29,539: t15.2024.07.28 val PER: 0.2346
2026-01-11 16:06:29,539: t15.2025.01.10 val PER: 0.3457
2026-01-11 16:06:29,539: t15.2025.01.12 val PER: 0.2294
2026-01-11 16:06:29,540: t15.2025.03.14 val PER: 0.3417
2026-01-11 16:06:29,540: t15.2025.03.16 val PER: 0.2513
2026-01-11 16:06:29,540: t15.2025.03.30 val PER: 0.3356
2026-01-11 16:06:29,540: t15.2025.04.13 val PER: 0.2782
2026-01-11 16:06:29,710: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_18000
2026-01-11 16:06:50,528: Train batch 18200: loss: 27.38 grad norm: 75.71 time: 0.086
2026-01-11 16:07:11,037: Train batch 18400: loss: 15.47 grad norm: 86.87 time: 0.071
2026-01-11 16:07:21,486: Running test after training batch: 18500
2026-01-11 16:07:21,746: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:07:28,058: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:07:28,128: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 16:07:48,498: Val batch 18500: PER (avg): 0.2205 CTC Loss (avg): 35.3205 WER(5gram): 34.22% (n=256) time: 27.012
2026-01-11 16:07:48,499: WER lens: avg_true_words=5.99 avg_pred_words=5.90 max_pred_words=11
2026-01-11 16:07:48,499: t15.2023.08.13 val PER: 0.1726
2026-01-11 16:07:48,500: t15.2023.08.18 val PER: 0.1903
2026-01-11 16:07:48,500: t15.2023.08.20 val PER: 0.1652
2026-01-11 16:07:48,501: t15.2023.08.25 val PER: 0.1687
2026-01-11 16:07:48,501: t15.2023.08.27 val PER: 0.2621
2026-01-11 16:07:48,501: t15.2023.09.01 val PER: 0.1315
2026-01-11 16:07:48,501: t15.2023.09.03 val PER: 0.2292
2026-01-11 16:07:48,501: t15.2023.09.24 val PER: 0.1638
2026-01-11 16:07:48,501: t15.2023.09.29 val PER: 0.1959
2026-01-11 16:07:48,501: t15.2023.10.01 val PER: 0.2622
2026-01-11 16:07:48,501: t15.2023.10.06 val PER: 0.1615
2026-01-11 16:07:48,501: t15.2023.10.08 val PER: 0.3315
2026-01-11 16:07:48,502: t15.2023.10.13 val PER: 0.3274
2026-01-11 16:07:48,502: t15.2023.10.15 val PER: 0.2347
2026-01-11 16:07:48,502: t15.2023.10.20 val PER: 0.2718
2026-01-11 16:07:48,502: t15.2023.10.22 val PER: 0.1971
2026-01-11 16:07:48,502: t15.2023.11.03 val PER: 0.2415
2026-01-11 16:07:48,502: t15.2023.11.04 val PER: 0.0307
2026-01-11 16:07:48,502: t15.2023.11.17 val PER: 0.0747
2026-01-11 16:07:48,502: t15.2023.11.19 val PER: 0.0818
2026-01-11 16:07:48,502: t15.2023.11.26 val PER: 0.2493
2026-01-11 16:07:48,503: t15.2023.12.03 val PER: 0.1817
2026-01-11 16:07:48,503: t15.2023.12.08 val PER: 0.2111
2026-01-11 16:07:48,503: t15.2023.12.10 val PER: 0.1761
2026-01-11 16:07:48,503: t15.2023.12.17 val PER: 0.1933
2026-01-11 16:07:48,503: t15.2023.12.29 val PER: 0.2080
2026-01-11 16:07:48,503: t15.2024.02.25 val PER: 0.1728
2026-01-11 16:07:48,503: t15.2024.03.08 val PER: 0.2902
2026-01-11 16:07:48,504: t15.2024.03.15 val PER: 0.2552
2026-01-11 16:07:48,504: t15.2024.03.17 val PER: 0.2134
2026-01-11 16:07:48,504: t15.2024.05.10 val PER: 0.2273
2026-01-11 16:07:48,504: t15.2024.06.14 val PER: 0.2003
2026-01-11 16:07:48,504: t15.2024.07.19 val PER: 0.2782
2026-01-11 16:07:48,504: t15.2024.07.21 val PER: 0.1614
2026-01-11 16:07:48,504: t15.2024.07.28 val PER: 0.2191
2026-01-11 16:07:48,504: t15.2025.01.10 val PER: 0.3595
2026-01-11 16:07:48,505: t15.2025.01.12 val PER: 0.2132
2026-01-11 16:07:48,505: t15.2025.03.14 val PER: 0.3388
2026-01-11 16:07:48,505: t15.2025.03.16 val PER: 0.2683
2026-01-11 16:07:48,505: t15.2025.03.30 val PER: 0.3115
2026-01-11 16:07:48,505: t15.2025.04.13 val PER: 0.2796
2026-01-11 16:07:48,679: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_18500
2026-01-11 16:07:59,215: Train batch 18600: loss: 28.81 grad norm: 72.26 time: 0.075
2026-01-11 16:08:19,844: Train batch 18800: loss: 27.38 grad norm: 88.57 time: 0.071
2026-01-11 16:08:40,492: Train batch 19000: loss: 23.06 grad norm: 68.03 time: 0.071
2026-01-11 16:08:40,492: Running test after training batch: 19000
2026-01-11 16:08:40,650: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:08:46,859: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:08:46,925: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost and
2026-01-11 16:09:09,466: Val batch 19000: PER (avg): 0.2184 CTC Loss (avg): 35.0832 WER(5gram): 30.57% (n=256) time: 28.973
2026-01-11 16:09:09,466: WER lens: avg_true_words=5.99 avg_pred_words=6.04 max_pred_words=11
2026-01-11 16:09:09,467: t15.2023.08.13 val PER: 0.1705
2026-01-11 16:09:09,467: t15.2023.08.18 val PER: 0.1827
2026-01-11 16:09:09,467: t15.2023.08.20 val PER: 0.1620
2026-01-11 16:09:09,467: t15.2023.08.25 val PER: 0.1611
2026-01-11 16:09:09,467: t15.2023.08.27 val PER: 0.2508
2026-01-11 16:09:09,467: t15.2023.09.01 val PER: 0.1291
2026-01-11 16:09:09,467: t15.2023.09.03 val PER: 0.2221
2026-01-11 16:09:09,468: t15.2023.09.24 val PER: 0.1760
2026-01-11 16:09:09,468: t15.2023.09.29 val PER: 0.1876
2026-01-11 16:09:09,468: t15.2023.10.01 val PER: 0.2616
2026-01-11 16:09:09,468: t15.2023.10.06 val PER: 0.1668
2026-01-11 16:09:09,468: t15.2023.10.08 val PER: 0.3234
2026-01-11 16:09:09,468: t15.2023.10.13 val PER: 0.3251
2026-01-11 16:09:09,468: t15.2023.10.15 val PER: 0.2294
2026-01-11 16:09:09,469: t15.2023.10.20 val PER: 0.2685
2026-01-11 16:09:09,469: t15.2023.10.22 val PER: 0.1871
2026-01-11 16:09:09,469: t15.2023.11.03 val PER: 0.2402
2026-01-11 16:09:09,469: t15.2023.11.04 val PER: 0.0307
2026-01-11 16:09:09,469: t15.2023.11.17 val PER: 0.0731
2026-01-11 16:09:09,469: t15.2023.11.19 val PER: 0.0838
2026-01-11 16:09:09,469: t15.2023.11.26 val PER: 0.2435
2026-01-11 16:09:09,469: t15.2023.12.03 val PER: 0.1880
2026-01-11 16:09:09,470: t15.2023.12.08 val PER: 0.2044
2026-01-11 16:09:09,470: t15.2023.12.10 val PER: 0.1682
2026-01-11 16:09:09,470: t15.2023.12.17 val PER: 0.1861
2026-01-11 16:09:09,470: t15.2023.12.29 val PER: 0.2121
2026-01-11 16:09:09,470: t15.2024.02.25 val PER: 0.1826
2026-01-11 16:09:09,470: t15.2024.03.08 val PER: 0.2774
2026-01-11 16:09:09,470: t15.2024.03.15 val PER: 0.2645
2026-01-11 16:09:09,470: t15.2024.03.17 val PER: 0.2162
2026-01-11 16:09:09,470: t15.2024.05.10 val PER: 0.2199
2026-01-11 16:09:09,470: t15.2024.06.14 val PER: 0.2161
2026-01-11 16:09:09,471: t15.2024.07.19 val PER: 0.2815
2026-01-11 16:09:09,471: t15.2024.07.21 val PER: 0.1586
2026-01-11 16:09:09,471: t15.2024.07.28 val PER: 0.2191
2026-01-11 16:09:09,471: t15.2025.01.10 val PER: 0.3402
2026-01-11 16:09:09,471: t15.2025.01.12 val PER: 0.2163
2026-01-11 16:09:09,471: t15.2025.03.14 val PER: 0.3254
2026-01-11 16:09:09,471: t15.2025.03.16 val PER: 0.2592
2026-01-11 16:09:09,471: t15.2025.03.30 val PER: 0.3184
2026-01-11 16:09:09,472: t15.2025.04.13 val PER: 0.2725
2026-01-11 16:09:09,647: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_19000
2026-01-11 16:09:30,010: Train batch 19200: loss: 17.42 grad norm: 79.04 time: 0.071
2026-01-11 16:09:49,889: Train batch 19400: loss: 17.33 grad norm: 80.98 time: 0.059
2026-01-11 16:10:00,092: Running test after training batch: 19500
2026-01-11 16:10:00,194: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:10:07,826: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:10:07,937: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 16:10:27,407: Val batch 19500: PER (avg): 0.2199 CTC Loss (avg): 35.0737 WER(5gram): 28.75% (n=256) time: 27.315
2026-01-11 16:10:27,408: WER lens: avg_true_words=5.99 avg_pred_words=6.02 max_pred_words=12
2026-01-11 16:10:27,409: t15.2023.08.13 val PER: 0.1715
2026-01-11 16:10:27,409: t15.2023.08.18 val PER: 0.1819
2026-01-11 16:10:27,409: t15.2023.08.20 val PER: 0.1732
2026-01-11 16:10:27,409: t15.2023.08.25 val PER: 0.1611
2026-01-11 16:10:27,409: t15.2023.08.27 val PER: 0.2331
2026-01-11 16:10:27,409: t15.2023.09.01 val PER: 0.1339
2026-01-11 16:10:27,409: t15.2023.09.03 val PER: 0.2209
2026-01-11 16:10:27,409: t15.2023.09.24 val PER: 0.1699
2026-01-11 16:10:27,409: t15.2023.09.29 val PER: 0.1953
2026-01-11 16:10:27,409: t15.2023.10.01 val PER: 0.2609
2026-01-11 16:10:27,410: t15.2023.10.06 val PER: 0.1668
2026-01-11 16:10:27,410: t15.2023.10.08 val PER: 0.3302
2026-01-11 16:10:27,410: t15.2023.10.13 val PER: 0.3235
2026-01-11 16:10:27,410: t15.2023.10.15 val PER: 0.2340
2026-01-11 16:10:27,410: t15.2023.10.20 val PER: 0.2785
2026-01-11 16:10:27,410: t15.2023.10.22 val PER: 0.1915
2026-01-11 16:10:27,410: t15.2023.11.03 val PER: 0.2395
2026-01-11 16:10:27,410: t15.2023.11.04 val PER: 0.0307
2026-01-11 16:10:27,410: t15.2023.11.17 val PER: 0.0731
2026-01-11 16:10:27,410: t15.2023.11.19 val PER: 0.0858
2026-01-11 16:10:27,411: t15.2023.11.26 val PER: 0.2514
2026-01-11 16:10:27,412: t15.2023.12.03 val PER: 0.1891
2026-01-11 16:10:27,412: t15.2023.12.08 val PER: 0.1997
2026-01-11 16:10:27,412: t15.2023.12.10 val PER: 0.1813
2026-01-11 16:10:27,412: t15.2023.12.17 val PER: 0.1798
2026-01-11 16:10:27,412: t15.2023.12.29 val PER: 0.2128
2026-01-11 16:10:27,413: t15.2024.02.25 val PER: 0.1812
2026-01-11 16:10:27,413: t15.2024.03.08 val PER: 0.2930
2026-01-11 16:10:27,413: t15.2024.03.15 val PER: 0.2583
2026-01-11 16:10:27,413: t15.2024.03.17 val PER: 0.2120
2026-01-11 16:10:27,413: t15.2024.05.10 val PER: 0.2273
2026-01-11 16:10:27,413: t15.2024.06.14 val PER: 0.2145
2026-01-11 16:10:27,413: t15.2024.07.19 val PER: 0.2769
2026-01-11 16:10:27,413: t15.2024.07.21 val PER: 0.1621
2026-01-11 16:10:27,413: t15.2024.07.28 val PER: 0.2221
2026-01-11 16:10:27,413: t15.2025.01.10 val PER: 0.3444
2026-01-11 16:10:27,413: t15.2025.01.12 val PER: 0.2156
2026-01-11 16:10:27,414: t15.2025.03.14 val PER: 0.3476
2026-01-11 16:10:27,414: t15.2025.03.16 val PER: 0.2552
2026-01-11 16:10:27,414: t15.2025.03.30 val PER: 0.3241
2026-01-11 16:10:27,414: t15.2025.04.13 val PER: 0.2753
2026-01-11 16:10:27,416: New best val WER(5gram) 29.73% --> 28.75%
2026-01-11 16:10:27,601: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_19500
2026-01-11 16:10:37,781: Train batch 19600: loss: 21.60 grad norm: 64.96 time: 0.065
2026-01-11 16:10:58,317: Train batch 19800: loss: 21.01 grad norm: 70.06 time: 0.060
2026-01-11 16:11:18,906: Train batch 20000: loss: 19.71 grad norm: 72.73 time: 0.076
2026-01-11 16:11:18,906: Running test after training batch: 20000
2026-01-11 16:11:19,038: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:11:25,501: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:11:25,570: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 16:11:48,796: Val batch 20000: PER (avg): 0.2131 CTC Loss (avg): 34.6587 WER(5gram): 28.94% (n=256) time: 29.890
2026-01-11 16:11:48,797: WER lens: avg_true_words=5.99 avg_pred_words=5.98 max_pred_words=13
2026-01-11 16:11:48,798: t15.2023.08.13 val PER: 0.1778
2026-01-11 16:11:48,798: t15.2023.08.18 val PER: 0.1777
2026-01-11 16:11:48,798: t15.2023.08.20 val PER: 0.1573
2026-01-11 16:11:48,798: t15.2023.08.25 val PER: 0.1672
2026-01-11 16:11:48,798: t15.2023.08.27 val PER: 0.2492
2026-01-11 16:11:48,798: t15.2023.09.01 val PER: 0.1299
2026-01-11 16:11:48,799: t15.2023.09.03 val PER: 0.2197
2026-01-11 16:11:48,799: t15.2023.09.24 val PER: 0.1699
2026-01-11 16:11:48,799: t15.2023.09.29 val PER: 0.1838
2026-01-11 16:11:48,799: t15.2023.10.01 val PER: 0.2490
2026-01-11 16:11:48,799: t15.2023.10.06 val PER: 0.1593
2026-01-11 16:11:48,799: t15.2023.10.08 val PER: 0.3248
2026-01-11 16:11:48,799: t15.2023.10.13 val PER: 0.3181
2026-01-11 16:11:48,799: t15.2023.10.15 val PER: 0.2254
2026-01-11 16:11:48,799: t15.2023.10.20 val PER: 0.2651
2026-01-11 16:11:48,799: t15.2023.10.22 val PER: 0.1860
2026-01-11 16:11:48,799: t15.2023.11.03 val PER: 0.2381
2026-01-11 16:11:48,799: t15.2023.11.04 val PER: 0.0307
2026-01-11 16:11:48,800: t15.2023.11.17 val PER: 0.0669
2026-01-11 16:11:48,800: t15.2023.11.19 val PER: 0.0758
2026-01-11 16:11:48,800: t15.2023.11.26 val PER: 0.2478
2026-01-11 16:11:48,800: t15.2023.12.03 val PER: 0.1712
2026-01-11 16:11:48,800: t15.2023.12.08 val PER: 0.1911
2026-01-11 16:11:48,800: t15.2023.12.10 val PER: 0.1787
2026-01-11 16:11:48,800: t15.2023.12.17 val PER: 0.1757
2026-01-11 16:11:48,800: t15.2023.12.29 val PER: 0.1956
2026-01-11 16:11:48,801: t15.2024.02.25 val PER: 0.1756
2026-01-11 16:11:48,801: t15.2024.03.08 val PER: 0.2845
2026-01-11 16:11:48,801: t15.2024.03.15 val PER: 0.2577
2026-01-11 16:11:48,801: t15.2024.03.17 val PER: 0.2001
2026-01-11 16:11:48,801: t15.2024.05.10 val PER: 0.2110
2026-01-11 16:11:48,801: t15.2024.06.14 val PER: 0.2035
2026-01-11 16:11:48,801: t15.2024.07.19 val PER: 0.2736
2026-01-11 16:11:48,801: t15.2024.07.21 val PER: 0.1559
2026-01-11 16:11:48,802: t15.2024.07.28 val PER: 0.2213
2026-01-11 16:11:48,802: t15.2025.01.10 val PER: 0.3471
2026-01-11 16:11:48,803: t15.2025.01.12 val PER: 0.2017
2026-01-11 16:11:48,803: t15.2025.03.14 val PER: 0.3314
2026-01-11 16:11:48,803: t15.2025.03.16 val PER: 0.2513
2026-01-11 16:11:48,803: t15.2025.03.30 val PER: 0.3011
2026-01-11 16:11:48,803: t15.2025.04.13 val PER: 0.2653
2026-01-11 16:11:48,979: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_20000
2026-01-11 16:12:09,513: Train batch 20200: loss: 15.54 grad norm: 86.46 time: 0.066
2026-01-11 16:12:30,473: Train batch 20400: loss: 17.42 grad norm: 94.46 time: 0.074
2026-01-11 16:12:40,576: Running test after training batch: 20500
2026-01-11 16:12:40,732: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:12:47,269: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:12:47,355: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost k
2026-01-11 16:13:07,666: Val batch 20500: PER (avg): 0.2156 CTC Loss (avg): 34.3909 WER(5gram): 33.31% (n=256) time: 27.089
2026-01-11 16:13:07,667: WER lens: avg_true_words=5.99 avg_pred_words=5.96 max_pred_words=13
2026-01-11 16:13:07,667: t15.2023.08.13 val PER: 0.1715
2026-01-11 16:13:07,667: t15.2023.08.18 val PER: 0.1785
2026-01-11 16:13:07,667: t15.2023.08.20 val PER: 0.1581
2026-01-11 16:13:07,667: t15.2023.08.25 val PER: 0.1687
2026-01-11 16:13:07,667: t15.2023.08.27 val PER: 0.2412
2026-01-11 16:13:07,668: t15.2023.09.01 val PER: 0.1364
2026-01-11 16:13:07,668: t15.2023.09.03 val PER: 0.2268
2026-01-11 16:13:07,668: t15.2023.09.24 val PER: 0.1784
2026-01-11 16:13:07,668: t15.2023.09.29 val PER: 0.1934
2026-01-11 16:13:07,668: t15.2023.10.01 val PER: 0.2596
2026-01-11 16:13:07,668: t15.2023.10.06 val PER: 0.1658
2026-01-11 16:13:07,668: t15.2023.10.08 val PER: 0.3248
2026-01-11 16:13:07,668: t15.2023.10.13 val PER: 0.3181
2026-01-11 16:13:07,668: t15.2023.10.15 val PER: 0.2287
2026-01-11 16:13:07,668: t15.2023.10.20 val PER: 0.2685
2026-01-11 16:13:07,668: t15.2023.10.22 val PER: 0.1904
2026-01-11 16:13:07,668: t15.2023.11.03 val PER: 0.2402
2026-01-11 16:13:07,668: t15.2023.11.04 val PER: 0.0341
2026-01-11 16:13:07,669: t15.2023.11.17 val PER: 0.0715
2026-01-11 16:13:07,669: t15.2023.11.19 val PER: 0.0878
2026-01-11 16:13:07,669: t15.2023.11.26 val PER: 0.2362
2026-01-11 16:13:07,669: t15.2023.12.03 val PER: 0.1786
2026-01-11 16:13:07,669: t15.2023.12.08 val PER: 0.1924
2026-01-11 16:13:07,669: t15.2023.12.10 val PER: 0.1827
2026-01-11 16:13:07,669: t15.2023.12.17 val PER: 0.1788
2026-01-11 16:13:07,669: t15.2023.12.29 val PER: 0.2011
2026-01-11 16:13:07,670: t15.2024.02.25 val PER: 0.1784
2026-01-11 16:13:07,670: t15.2024.03.08 val PER: 0.2774
2026-01-11 16:13:07,670: t15.2024.03.15 val PER: 0.2602
2026-01-11 16:13:07,670: t15.2024.03.17 val PER: 0.2148
2026-01-11 16:13:07,670: t15.2024.05.10 val PER: 0.2184
2026-01-11 16:13:07,670: t15.2024.06.14 val PER: 0.2035
2026-01-11 16:13:07,670: t15.2024.07.19 val PER: 0.2696
2026-01-11 16:13:07,671: t15.2024.07.21 val PER: 0.1497
2026-01-11 16:13:07,671: t15.2024.07.28 val PER: 0.2191
2026-01-11 16:13:07,671: t15.2025.01.10 val PER: 0.3278
2026-01-11 16:13:07,671: t15.2025.01.12 val PER: 0.2102
2026-01-11 16:13:07,671: t15.2025.03.14 val PER: 0.3536
2026-01-11 16:13:07,671: t15.2025.03.16 val PER: 0.2539
2026-01-11 16:13:07,671: t15.2025.03.30 val PER: 0.3011
2026-01-11 16:13:07,671: t15.2025.04.13 val PER: 0.2653
2026-01-11 16:13:07,859: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_20500
2026-01-11 16:13:18,003: Train batch 20600: loss: 23.44 grad norm: 102.55 time: 0.063
2026-01-11 16:13:38,419: Train batch 20800: loss: 21.54 grad norm: 82.15 time: 0.063
2026-01-11 16:13:59,321: Train batch 21000: loss: 28.81 grad norm: 116.96 time: 0.063
2026-01-11 16:13:59,322: Running test after training batch: 21000
2026-01-11 16:13:59,488: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:14:05,950: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:14:06,065: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost and
2026-01-11 16:14:24,704: Val batch 21000: PER (avg): 0.2105 CTC Loss (avg): 34.0707 WER(5gram): 29.14% (n=256) time: 25.381
2026-01-11 16:14:24,705: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-11 16:14:24,706: t15.2023.08.13 val PER: 0.1611
2026-01-11 16:14:24,706: t15.2023.08.18 val PER: 0.1702
2026-01-11 16:14:24,706: t15.2023.08.20 val PER: 0.1485
2026-01-11 16:14:24,706: t15.2023.08.25 val PER: 0.1611
2026-01-11 16:14:24,706: t15.2023.08.27 val PER: 0.2428
2026-01-11 16:14:24,707: t15.2023.09.01 val PER: 0.1209
2026-01-11 16:14:24,707: t15.2023.09.03 val PER: 0.2150
2026-01-11 16:14:24,707: t15.2023.09.24 val PER: 0.1663
2026-01-11 16:14:24,707: t15.2023.09.29 val PER: 0.1819
2026-01-11 16:14:24,707: t15.2023.10.01 val PER: 0.2556
2026-01-11 16:14:24,707: t15.2023.10.06 val PER: 0.1593
2026-01-11 16:14:24,707: t15.2023.10.08 val PER: 0.3194
2026-01-11 16:14:24,707: t15.2023.10.13 val PER: 0.3111
2026-01-11 16:14:24,707: t15.2023.10.15 val PER: 0.2208
2026-01-11 16:14:24,707: t15.2023.10.20 val PER: 0.2785
2026-01-11 16:14:24,707: t15.2023.10.22 val PER: 0.1804
2026-01-11 16:14:24,708: t15.2023.11.03 val PER: 0.2449
2026-01-11 16:14:24,708: t15.2023.11.04 val PER: 0.0341
2026-01-11 16:14:24,708: t15.2023.11.17 val PER: 0.0700
2026-01-11 16:14:24,708: t15.2023.11.19 val PER: 0.0858
2026-01-11 16:14:24,708: t15.2023.11.26 val PER: 0.2355
2026-01-11 16:14:24,708: t15.2023.12.03 val PER: 0.1733
2026-01-11 16:14:24,708: t15.2023.12.08 val PER: 0.1858
2026-01-11 16:14:24,708: t15.2023.12.10 val PER: 0.1761
2026-01-11 16:14:24,708: t15.2023.12.17 val PER: 0.1767
2026-01-11 16:14:24,708: t15.2023.12.29 val PER: 0.1935
2026-01-11 16:14:24,708: t15.2024.02.25 val PER: 0.1770
2026-01-11 16:14:24,709: t15.2024.03.08 val PER: 0.2760
2026-01-11 16:14:24,710: t15.2024.03.15 val PER: 0.2558
2026-01-11 16:14:24,710: t15.2024.03.17 val PER: 0.2050
2026-01-11 16:14:24,710: t15.2024.05.10 val PER: 0.2214
2026-01-11 16:14:24,710: t15.2024.06.14 val PER: 0.2003
2026-01-11 16:14:24,710: t15.2024.07.19 val PER: 0.2643
2026-01-11 16:14:24,710: t15.2024.07.21 val PER: 0.1476
2026-01-11 16:14:24,711: t15.2024.07.28 val PER: 0.2110
2026-01-11 16:14:24,711: t15.2025.01.10 val PER: 0.3333
2026-01-11 16:14:24,711: t15.2025.01.12 val PER: 0.2156
2026-01-11 16:14:24,711: t15.2025.03.14 val PER: 0.3373
2026-01-11 16:14:24,711: t15.2025.03.16 val PER: 0.2487
2026-01-11 16:14:24,712: t15.2025.03.30 val PER: 0.2989
2026-01-11 16:14:24,712: t15.2025.04.13 val PER: 0.2696
2026-01-11 16:14:24,888: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_21000
2026-01-11 16:14:45,470: Train batch 21200: loss: 18.85 grad norm: 59.83 time: 0.085
2026-01-11 16:15:06,119: Train batch 21400: loss: 27.41 grad norm: 121.66 time: 0.067
2026-01-11 16:15:16,349: Running test after training batch: 21500
2026-01-11 16:15:16,458: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:15:23,230: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:15:23,298: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost k
2026-01-11 16:15:40,716: Val batch 21500: PER (avg): 0.2049 CTC Loss (avg): 33.9119 WER(5gram): 27.71% (n=256) time: 24.367
2026-01-11 16:15:40,717: WER lens: avg_true_words=5.99 avg_pred_words=6.04 max_pred_words=12
2026-01-11 16:15:40,717: t15.2023.08.13 val PER: 0.1684
2026-01-11 16:15:40,717: t15.2023.08.18 val PER: 0.1651
2026-01-11 16:15:40,718: t15.2023.08.20 val PER: 0.1438
2026-01-11 16:15:40,718: t15.2023.08.25 val PER: 0.1642
2026-01-11 16:15:40,718: t15.2023.08.27 val PER: 0.2315
2026-01-11 16:15:40,718: t15.2023.09.01 val PER: 0.1161
2026-01-11 16:15:40,718: t15.2023.09.03 val PER: 0.2067
2026-01-11 16:15:40,718: t15.2023.09.24 val PER: 0.1723
2026-01-11 16:15:40,718: t15.2023.09.29 val PER: 0.1793
2026-01-11 16:15:40,718: t15.2023.10.01 val PER: 0.2444
2026-01-11 16:15:40,719: t15.2023.10.06 val PER: 0.1561
2026-01-11 16:15:40,719: t15.2023.10.08 val PER: 0.3180
2026-01-11 16:15:40,719: t15.2023.10.13 val PER: 0.3010
2026-01-11 16:15:40,719: t15.2023.10.15 val PER: 0.2123
2026-01-11 16:15:40,719: t15.2023.10.20 val PER: 0.2718
2026-01-11 16:15:40,719: t15.2023.10.22 val PER: 0.1737
2026-01-11 16:15:40,719: t15.2023.11.03 val PER: 0.2334
2026-01-11 16:15:40,719: t15.2023.11.04 val PER: 0.0307
2026-01-11 16:15:40,719: t15.2023.11.17 val PER: 0.0607
2026-01-11 16:15:40,720: t15.2023.11.19 val PER: 0.0739
2026-01-11 16:15:40,720: t15.2023.11.26 val PER: 0.2225
2026-01-11 16:15:40,720: t15.2023.12.03 val PER: 0.1786
2026-01-11 16:15:40,720: t15.2023.12.08 val PER: 0.1824
2026-01-11 16:15:40,720: t15.2023.12.10 val PER: 0.1603
2026-01-11 16:15:40,720: t15.2023.12.17 val PER: 0.1726
2026-01-11 16:15:40,720: t15.2023.12.29 val PER: 0.1915
2026-01-11 16:15:40,720: t15.2024.02.25 val PER: 0.1699
2026-01-11 16:15:40,720: t15.2024.03.08 val PER: 0.2745
2026-01-11 16:15:40,720: t15.2024.03.15 val PER: 0.2477
2026-01-11 16:15:40,720: t15.2024.03.17 val PER: 0.1967
2026-01-11 16:15:40,721: t15.2024.05.10 val PER: 0.2080
2026-01-11 16:15:40,721: t15.2024.06.14 val PER: 0.1814
2026-01-11 16:15:40,721: t15.2024.07.19 val PER: 0.2558
2026-01-11 16:15:40,721: t15.2024.07.21 val PER: 0.1448
2026-01-11 16:15:40,721: t15.2024.07.28 val PER: 0.2162
2026-01-11 16:15:40,721: t15.2025.01.10 val PER: 0.3471
2026-01-11 16:15:40,721: t15.2025.01.12 val PER: 0.2025
2026-01-11 16:15:40,721: t15.2025.03.14 val PER: 0.3388
2026-01-11 16:15:40,722: t15.2025.03.16 val PER: 0.2421
2026-01-11 16:15:40,722: t15.2025.03.30 val PER: 0.2943
2026-01-11 16:15:40,722: t15.2025.04.13 val PER: 0.2568
2026-01-11 16:15:40,722: New best val WER(5gram) 28.75% --> 27.71%
2026-01-11 16:15:40,900: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_21500
2026-01-11 16:15:51,104: Train batch 21600: loss: 29.09 grad norm: 83.98 time: 0.083
2026-01-11 16:16:11,728: Train batch 21800: loss: 22.92 grad norm: 94.15 time: 0.089
2026-01-11 16:16:32,243: Train batch 22000: loss: 33.85 grad norm: 150.96 time: 0.061
2026-01-11 16:16:32,244: Running test after training batch: 22000
2026-01-11 16:16:32,512: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:16:39,322: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the go at this point it will
2026-01-11 16:16:39,408: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost k
2026-01-11 16:16:59,248: Val batch 22000: PER (avg): 0.2070 CTC Loss (avg): 33.7798 WER(5gram): 29.20% (n=256) time: 27.003
2026-01-11 16:16:59,249: WER lens: avg_true_words=5.99 avg_pred_words=6.00 max_pred_words=11
2026-01-11 16:16:59,249: t15.2023.08.13 val PER: 0.1674
2026-01-11 16:16:59,250: t15.2023.08.18 val PER: 0.1651
2026-01-11 16:16:59,250: t15.2023.08.20 val PER: 0.1469
2026-01-11 16:16:59,250: t15.2023.08.25 val PER: 0.1551
2026-01-11 16:16:59,250: t15.2023.08.27 val PER: 0.2379
2026-01-11 16:16:59,250: t15.2023.09.01 val PER: 0.1201
2026-01-11 16:16:59,250: t15.2023.09.03 val PER: 0.2114
2026-01-11 16:16:59,250: t15.2023.09.24 val PER: 0.1590
2026-01-11 16:16:59,250: t15.2023.09.29 val PER: 0.1806
2026-01-11 16:16:59,250: t15.2023.10.01 val PER: 0.2450
2026-01-11 16:16:59,250: t15.2023.10.06 val PER: 0.1550
2026-01-11 16:16:59,250: t15.2023.10.08 val PER: 0.3207
2026-01-11 16:16:59,251: t15.2023.10.13 val PER: 0.3072
2026-01-11 16:16:59,251: t15.2023.10.15 val PER: 0.2195
2026-01-11 16:16:59,251: t15.2023.10.20 val PER: 0.2584
2026-01-11 16:16:59,251: t15.2023.10.22 val PER: 0.1815
2026-01-11 16:16:59,251: t15.2023.11.03 val PER: 0.2327
2026-01-11 16:16:59,252: t15.2023.11.04 val PER: 0.0410
2026-01-11 16:16:59,252: t15.2023.11.17 val PER: 0.0591
2026-01-11 16:16:59,252: t15.2023.11.19 val PER: 0.0679
2026-01-11 16:16:59,252: t15.2023.11.26 val PER: 0.2348
2026-01-11 16:16:59,252: t15.2023.12.03 val PER: 0.1807
2026-01-11 16:16:59,252: t15.2023.12.08 val PER: 0.1831
2026-01-11 16:16:59,252: t15.2023.12.10 val PER: 0.1695
2026-01-11 16:16:59,253: t15.2023.12.17 val PER: 0.1788
2026-01-11 16:16:59,253: t15.2023.12.29 val PER: 0.1970
2026-01-11 16:16:59,253: t15.2024.02.25 val PER: 0.1756
2026-01-11 16:16:59,253: t15.2024.03.08 val PER: 0.2788
2026-01-11 16:16:59,253: t15.2024.03.15 val PER: 0.2445
2026-01-11 16:16:59,253: t15.2024.03.17 val PER: 0.1974
2026-01-11 16:16:59,253: t15.2024.05.10 val PER: 0.1961
2026-01-11 16:16:59,253: t15.2024.06.14 val PER: 0.2003
2026-01-11 16:16:59,253: t15.2024.07.19 val PER: 0.2643
2026-01-11 16:16:59,253: t15.2024.07.21 val PER: 0.1407
2026-01-11 16:16:59,254: t15.2024.07.28 val PER: 0.2059
2026-01-11 16:16:59,254: t15.2025.01.10 val PER: 0.3457
2026-01-11 16:16:59,254: t15.2025.01.12 val PER: 0.2102
2026-01-11 16:16:59,254: t15.2025.03.14 val PER: 0.3299
2026-01-11 16:16:59,254: t15.2025.03.16 val PER: 0.2448
2026-01-11 16:16:59,254: t15.2025.03.30 val PER: 0.3023
2026-01-11 16:16:59,254: t15.2025.04.13 val PER: 0.2753
2026-01-11 16:16:59,427: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_22000
2026-01-11 16:17:20,080: Train batch 22200: loss: 24.20 grad norm: 87.23 time: 0.077
2026-01-11 16:17:40,567: Train batch 22400: loss: 20.67 grad norm: 67.79 time: 0.060
2026-01-11 16:17:50,969: Running test after training batch: 22500
2026-01-11 16:17:51,600: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:17:58,144: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:17:58,230: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost and
2026-01-11 16:18:20,496: Val batch 22500: PER (avg): 0.2041 CTC Loss (avg): 33.7549 WER(5gram): 28.49% (n=256) time: 29.526
2026-01-11 16:18:20,497: WER lens: avg_true_words=5.99 avg_pred_words=6.01 max_pred_words=13
2026-01-11 16:18:20,497: t15.2023.08.13 val PER: 0.1663
2026-01-11 16:18:20,498: t15.2023.08.18 val PER: 0.1534
2026-01-11 16:18:20,498: t15.2023.08.20 val PER: 0.1430
2026-01-11 16:18:20,498: t15.2023.08.25 val PER: 0.1566
2026-01-11 16:18:20,498: t15.2023.08.27 val PER: 0.2331
2026-01-11 16:18:20,498: t15.2023.09.01 val PER: 0.1185
2026-01-11 16:18:20,499: t15.2023.09.03 val PER: 0.1971
2026-01-11 16:18:20,499: t15.2023.09.24 val PER: 0.1578
2026-01-11 16:18:20,499: t15.2023.09.29 val PER: 0.1755
2026-01-11 16:18:20,499: t15.2023.10.01 val PER: 0.2398
2026-01-11 16:18:20,499: t15.2023.10.06 val PER: 0.1561
2026-01-11 16:18:20,499: t15.2023.10.08 val PER: 0.3166
2026-01-11 16:18:20,499: t15.2023.10.13 val PER: 0.2995
2026-01-11 16:18:20,499: t15.2023.10.15 val PER: 0.2195
2026-01-11 16:18:20,499: t15.2023.10.20 val PER: 0.2584
2026-01-11 16:18:20,499: t15.2023.10.22 val PER: 0.1804
2026-01-11 16:18:20,499: t15.2023.11.03 val PER: 0.2320
2026-01-11 16:18:20,500: t15.2023.11.04 val PER: 0.0341
2026-01-11 16:18:20,500: t15.2023.11.17 val PER: 0.0544
2026-01-11 16:18:20,500: t15.2023.11.19 val PER: 0.0758
2026-01-11 16:18:20,500: t15.2023.11.26 val PER: 0.2290
2026-01-11 16:18:20,500: t15.2023.12.03 val PER: 0.1765
2026-01-11 16:18:20,500: t15.2023.12.08 val PER: 0.1764
2026-01-11 16:18:20,500: t15.2023.12.10 val PER: 0.1616
2026-01-11 16:18:20,500: t15.2023.12.17 val PER: 0.1653
2026-01-11 16:18:20,500: t15.2023.12.29 val PER: 0.1908
2026-01-11 16:18:20,500: t15.2024.02.25 val PER: 0.1713
2026-01-11 16:18:20,501: t15.2024.03.08 val PER: 0.2774
2026-01-11 16:18:20,502: t15.2024.03.15 val PER: 0.2508
2026-01-11 16:18:20,502: t15.2024.03.17 val PER: 0.1939
2026-01-11 16:18:20,502: t15.2024.05.10 val PER: 0.2288
2026-01-11 16:18:20,502: t15.2024.06.14 val PER: 0.1987
2026-01-11 16:18:20,503: t15.2024.07.19 val PER: 0.2643
2026-01-11 16:18:20,503: t15.2024.07.21 val PER: 0.1421
2026-01-11 16:18:20,503: t15.2024.07.28 val PER: 0.2007
2026-01-11 16:18:20,503: t15.2025.01.10 val PER: 0.3333
2026-01-11 16:18:20,503: t15.2025.01.12 val PER: 0.1994
2026-01-11 16:18:20,504: t15.2025.03.14 val PER: 0.3595
2026-01-11 16:18:20,504: t15.2025.03.16 val PER: 0.2356
2026-01-11 16:18:20,504: t15.2025.03.30 val PER: 0.3011
2026-01-11 16:18:20,504: t15.2025.04.13 val PER: 0.2596
2026-01-11 16:18:20,675: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_22500
2026-01-11 16:18:31,141: Train batch 22600: loss: 32.91 grad norm: 100.07 time: 0.075
2026-01-11 16:18:52,206: Train batch 22800: loss: 24.35 grad norm: 124.00 time: 0.064
2026-01-11 16:19:12,413: Train batch 23000: loss: 27.16 grad norm: 123.81 time: 0.068
2026-01-11 16:19:12,414: Running test after training batch: 23000
2026-01-11 16:19:12,584: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:19:19,022: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point is why
2026-01-11 16:19:19,111: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-11 16:19:39,807: Val batch 23000: PER (avg): 0.2037 CTC Loss (avg): 33.1781 WER(5gram): 27.97% (n=256) time: 27.393
2026-01-11 16:19:39,808: WER lens: avg_true_words=5.99 avg_pred_words=5.97 max_pred_words=13
2026-01-11 16:19:39,808: t15.2023.08.13 val PER: 0.1632
2026-01-11 16:19:39,808: t15.2023.08.18 val PER: 0.1660
2026-01-11 16:19:39,808: t15.2023.08.20 val PER: 0.1446
2026-01-11 16:19:39,809: t15.2023.08.25 val PER: 0.1461
2026-01-11 16:19:39,809: t15.2023.08.27 val PER: 0.2363
2026-01-11 16:19:39,809: t15.2023.09.01 val PER: 0.1185
2026-01-11 16:19:39,809: t15.2023.09.03 val PER: 0.1983
2026-01-11 16:19:39,809: t15.2023.09.24 val PER: 0.1602
2026-01-11 16:19:39,809: t15.2023.09.29 val PER: 0.1832
2026-01-11 16:19:39,809: t15.2023.10.01 val PER: 0.2470
2026-01-11 16:19:39,809: t15.2023.10.06 val PER: 0.1475
2026-01-11 16:19:39,810: t15.2023.10.08 val PER: 0.3126
2026-01-11 16:19:39,810: t15.2023.10.13 val PER: 0.3126
2026-01-11 16:19:39,810: t15.2023.10.15 val PER: 0.2103
2026-01-11 16:19:39,810: t15.2023.10.20 val PER: 0.2617
2026-01-11 16:19:39,810: t15.2023.10.22 val PER: 0.1748
2026-01-11 16:19:39,810: t15.2023.11.03 val PER: 0.2280
2026-01-11 16:19:39,810: t15.2023.11.04 val PER: 0.0341
2026-01-11 16:19:39,811: t15.2023.11.17 val PER: 0.0544
2026-01-11 16:19:39,811: t15.2023.11.19 val PER: 0.0818
2026-01-11 16:19:39,811: t15.2023.11.26 val PER: 0.2290
2026-01-11 16:19:39,811: t15.2023.12.03 val PER: 0.1754
2026-01-11 16:19:39,811: t15.2023.12.08 val PER: 0.1764
2026-01-11 16:19:39,811: t15.2023.12.10 val PER: 0.1577
2026-01-11 16:19:39,811: t15.2023.12.17 val PER: 0.1674
2026-01-11 16:19:39,811: t15.2023.12.29 val PER: 0.1833
2026-01-11 16:19:39,811: t15.2024.02.25 val PER: 0.1699
2026-01-11 16:19:39,812: t15.2024.03.08 val PER: 0.2817
2026-01-11 16:19:39,812: t15.2024.03.15 val PER: 0.2414
2026-01-11 16:19:39,812: t15.2024.03.17 val PER: 0.1932
2026-01-11 16:19:39,812: t15.2024.05.10 val PER: 0.2036
2026-01-11 16:19:39,812: t15.2024.06.14 val PER: 0.2066
2026-01-11 16:19:39,812: t15.2024.07.19 val PER: 0.2643
2026-01-11 16:19:39,812: t15.2024.07.21 val PER: 0.1414
2026-01-11 16:19:39,812: t15.2024.07.28 val PER: 0.2081
2026-01-11 16:19:39,813: t15.2025.01.10 val PER: 0.3457
2026-01-11 16:19:39,813: t15.2025.01.12 val PER: 0.2040
2026-01-11 16:19:39,813: t15.2025.03.14 val PER: 0.3388
2026-01-11 16:19:39,813: t15.2025.03.16 val PER: 0.2448
2026-01-11 16:19:39,813: t15.2025.03.30 val PER: 0.2874
2026-01-11 16:19:39,813: t15.2025.04.13 val PER: 0.2582
2026-01-11 16:19:39,986: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_23000
2026-01-11 16:20:00,694: Train batch 23200: loss: 25.91 grad norm: 100.21 time: 0.067
2026-01-11 16:20:21,554: Train batch 23400: loss: 17.06 grad norm: 71.81 time: 0.079
2026-01-11 16:20:31,855: Running test after training batch: 23500
2026-01-11 16:20:31,984: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:20:39,472: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:20:39,568: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 16:21:02,147: Val batch 23500: PER (avg): 0.2001 CTC Loss (avg): 32.9924 WER(5gram): 26.79% (n=256) time: 30.291
2026-01-11 16:21:02,148: WER lens: avg_true_words=5.99 avg_pred_words=6.00 max_pred_words=12
2026-01-11 16:21:02,148: t15.2023.08.13 val PER: 0.1580
2026-01-11 16:21:02,149: t15.2023.08.18 val PER: 0.1660
2026-01-11 16:21:02,149: t15.2023.08.20 val PER: 0.1374
2026-01-11 16:21:02,149: t15.2023.08.25 val PER: 0.1461
2026-01-11 16:21:02,149: t15.2023.08.27 val PER: 0.2347
2026-01-11 16:21:02,149: t15.2023.09.01 val PER: 0.1104
2026-01-11 16:21:02,149: t15.2023.09.03 val PER: 0.1983
2026-01-11 16:21:02,149: t15.2023.09.24 val PER: 0.1638
2026-01-11 16:21:02,149: t15.2023.09.29 val PER: 0.1615
2026-01-11 16:21:02,150: t15.2023.10.01 val PER: 0.2424
2026-01-11 16:21:02,150: t15.2023.10.06 val PER: 0.1421
2026-01-11 16:21:02,150: t15.2023.10.08 val PER: 0.3126
2026-01-11 16:21:02,150: t15.2023.10.13 val PER: 0.3018
2026-01-11 16:21:02,150: t15.2023.10.15 val PER: 0.2149
2026-01-11 16:21:02,150: t15.2023.10.20 val PER: 0.2685
2026-01-11 16:21:02,150: t15.2023.10.22 val PER: 0.1759
2026-01-11 16:21:02,150: t15.2023.11.03 val PER: 0.2341
2026-01-11 16:21:02,150: t15.2023.11.04 val PER: 0.0307
2026-01-11 16:21:02,150: t15.2023.11.17 val PER: 0.0591
2026-01-11 16:21:02,151: t15.2023.11.19 val PER: 0.0758
2026-01-11 16:21:02,151: t15.2023.11.26 val PER: 0.2217
2026-01-11 16:21:02,151: t15.2023.12.03 val PER: 0.1723
2026-01-11 16:21:02,152: t15.2023.12.08 val PER: 0.1664
2026-01-11 16:21:02,152: t15.2023.12.10 val PER: 0.1485
2026-01-11 16:21:02,152: t15.2023.12.17 val PER: 0.1570
2026-01-11 16:21:02,153: t15.2023.12.29 val PER: 0.1860
2026-01-11 16:21:02,153: t15.2024.02.25 val PER: 0.1671
2026-01-11 16:21:02,153: t15.2024.03.08 val PER: 0.2774
2026-01-11 16:21:02,153: t15.2024.03.15 val PER: 0.2458
2026-01-11 16:21:02,154: t15.2024.03.17 val PER: 0.1897
2026-01-11 16:21:02,154: t15.2024.05.10 val PER: 0.2065
2026-01-11 16:21:02,154: t15.2024.06.14 val PER: 0.1893
2026-01-11 16:21:02,154: t15.2024.07.19 val PER: 0.2485
2026-01-11 16:21:02,154: t15.2024.07.21 val PER: 0.1393
2026-01-11 16:21:02,154: t15.2024.07.28 val PER: 0.2088
2026-01-11 16:21:02,154: t15.2025.01.10 val PER: 0.3471
2026-01-11 16:21:02,154: t15.2025.01.12 val PER: 0.2071
2026-01-11 16:21:02,154: t15.2025.03.14 val PER: 0.3373
2026-01-11 16:21:02,155: t15.2025.03.16 val PER: 0.2356
2026-01-11 16:21:02,155: t15.2025.03.30 val PER: 0.2897
2026-01-11 16:21:02,155: t15.2025.04.13 val PER: 0.2496
2026-01-11 16:21:02,156: New best val WER(5gram) 27.71% --> 26.79%
2026-01-11 16:21:02,332: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_23500
2026-01-11 16:21:12,743: Train batch 23600: loss: 9.82 grad norm: 47.79 time: 0.067
2026-01-11 16:21:33,487: Train batch 23800: loss: 18.59 grad norm: 73.19 time: 0.065
2026-01-11 16:21:54,743: Train batch 24000: loss: 22.99 grad norm: 83.27 time: 0.097
2026-01-11 16:21:54,744: Running test after training batch: 24000
2026-01-11 16:21:54,866: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:22:01,920: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:22:01,995: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-11 16:22:23,637: Val batch 24000: PER (avg): 0.1992 CTC Loss (avg): 32.4566 WER(5gram): 29.66% (n=256) time: 28.893
2026-01-11 16:22:23,638: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-11 16:22:23,638: t15.2023.08.13 val PER: 0.1601
2026-01-11 16:22:23,638: t15.2023.08.18 val PER: 0.1702
2026-01-11 16:22:23,639: t15.2023.08.20 val PER: 0.1350
2026-01-11 16:22:23,639: t15.2023.08.25 val PER: 0.1521
2026-01-11 16:22:23,639: t15.2023.08.27 val PER: 0.2267
2026-01-11 16:22:23,639: t15.2023.09.01 val PER: 0.1120
2026-01-11 16:22:23,639: t15.2023.09.03 val PER: 0.1983
2026-01-11 16:22:23,639: t15.2023.09.24 val PER: 0.1529
2026-01-11 16:22:23,639: t15.2023.09.29 val PER: 0.1800
2026-01-11 16:22:23,640: t15.2023.10.01 val PER: 0.2371
2026-01-11 16:22:23,640: t15.2023.10.06 val PER: 0.1496
2026-01-11 16:22:23,640: t15.2023.10.08 val PER: 0.3018
2026-01-11 16:22:23,640: t15.2023.10.13 val PER: 0.3041
2026-01-11 16:22:23,641: t15.2023.10.15 val PER: 0.2116
2026-01-11 16:22:23,641: t15.2023.10.20 val PER: 0.2483
2026-01-11 16:22:23,641: t15.2023.10.22 val PER: 0.1782
2026-01-11 16:22:23,641: t15.2023.11.03 val PER: 0.2347
2026-01-11 16:22:23,641: t15.2023.11.04 val PER: 0.0273
2026-01-11 16:22:23,641: t15.2023.11.17 val PER: 0.0544
2026-01-11 16:22:23,641: t15.2023.11.19 val PER: 0.0699
2026-01-11 16:22:23,641: t15.2023.11.26 val PER: 0.2203
2026-01-11 16:22:23,641: t15.2023.12.03 val PER: 0.1597
2026-01-11 16:22:23,641: t15.2023.12.08 val PER: 0.1638
2026-01-11 16:22:23,642: t15.2023.12.10 val PER: 0.1616
2026-01-11 16:22:23,642: t15.2023.12.17 val PER: 0.1694
2026-01-11 16:22:23,643: t15.2023.12.29 val PER: 0.1881
2026-01-11 16:22:23,643: t15.2024.02.25 val PER: 0.1685
2026-01-11 16:22:23,643: t15.2024.03.08 val PER: 0.2660
2026-01-11 16:22:23,643: t15.2024.03.15 val PER: 0.2464
2026-01-11 16:22:23,643: t15.2024.03.17 val PER: 0.1918
2026-01-11 16:22:23,643: t15.2024.05.10 val PER: 0.1961
2026-01-11 16:22:23,643: t15.2024.06.14 val PER: 0.1861
2026-01-11 16:22:23,643: t15.2024.07.19 val PER: 0.2558
2026-01-11 16:22:23,644: t15.2024.07.21 val PER: 0.1414
2026-01-11 16:22:23,644: t15.2024.07.28 val PER: 0.2037
2026-01-11 16:22:23,644: t15.2025.01.10 val PER: 0.3333
2026-01-11 16:22:23,644: t15.2025.01.12 val PER: 0.1963
2026-01-11 16:22:23,644: t15.2025.03.14 val PER: 0.3314
2026-01-11 16:22:23,644: t15.2025.03.16 val PER: 0.2212
2026-01-11 16:22:23,644: t15.2025.03.30 val PER: 0.2816
2026-01-11 16:22:23,644: t15.2025.04.13 val PER: 0.2539
2026-01-11 16:22:23,823: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_24000
2026-01-11 16:22:44,680: Train batch 24200: loss: 14.63 grad norm: 109.89 time: 0.064
2026-01-11 16:23:05,150: Train batch 24400: loss: 25.21 grad norm: 108.08 time: 0.064
2026-01-11 16:23:15,098: Running test after training batch: 24500
2026-01-11 16:23:15,256: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:23:22,057: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:23:22,134: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost of
2026-01-11 16:23:44,939: Val batch 24500: PER (avg): 0.1982 CTC Loss (avg): 32.4027 WER(5gram): 27.64% (n=256) time: 29.840
2026-01-11 16:23:44,939: WER lens: avg_true_words=5.99 avg_pred_words=6.04 max_pred_words=13
2026-01-11 16:23:44,940: t15.2023.08.13 val PER: 0.1507
2026-01-11 16:23:44,940: t15.2023.08.18 val PER: 0.1702
2026-01-11 16:23:44,940: t15.2023.08.20 val PER: 0.1342
2026-01-11 16:23:44,940: t15.2023.08.25 val PER: 0.1521
2026-01-11 16:23:44,940: t15.2023.08.27 val PER: 0.2251
2026-01-11 16:23:44,940: t15.2023.09.01 val PER: 0.1128
2026-01-11 16:23:44,941: t15.2023.09.03 val PER: 0.1971
2026-01-11 16:23:44,941: t15.2023.09.24 val PER: 0.1590
2026-01-11 16:23:44,941: t15.2023.09.29 val PER: 0.1742
2026-01-11 16:23:44,941: t15.2023.10.01 val PER: 0.2305
2026-01-11 16:23:44,941: t15.2023.10.06 val PER: 0.1464
2026-01-11 16:23:44,941: t15.2023.10.08 val PER: 0.3018
2026-01-11 16:23:44,941: t15.2023.10.13 val PER: 0.2971
2026-01-11 16:23:44,941: t15.2023.10.15 val PER: 0.2103
2026-01-11 16:23:44,941: t15.2023.10.20 val PER: 0.2483
2026-01-11 16:23:44,942: t15.2023.10.22 val PER: 0.1793
2026-01-11 16:23:44,942: t15.2023.11.03 val PER: 0.2286
2026-01-11 16:23:44,942: t15.2023.11.04 val PER: 0.0307
2026-01-11 16:23:44,942: t15.2023.11.17 val PER: 0.0607
2026-01-11 16:23:44,942: t15.2023.11.19 val PER: 0.0758
2026-01-11 16:23:44,942: t15.2023.11.26 val PER: 0.2217
2026-01-11 16:23:44,942: t15.2023.12.03 val PER: 0.1607
2026-01-11 16:23:44,942: t15.2023.12.08 val PER: 0.1671
2026-01-11 16:23:44,942: t15.2023.12.10 val PER: 0.1498
2026-01-11 16:23:44,943: t15.2023.12.17 val PER: 0.1715
2026-01-11 16:23:44,943: t15.2023.12.29 val PER: 0.1853
2026-01-11 16:23:44,943: t15.2024.02.25 val PER: 0.1629
2026-01-11 16:23:44,943: t15.2024.03.08 val PER: 0.2674
2026-01-11 16:23:44,943: t15.2024.03.15 val PER: 0.2470
2026-01-11 16:23:44,943: t15.2024.03.17 val PER: 0.1939
2026-01-11 16:23:44,943: t15.2024.05.10 val PER: 0.2155
2026-01-11 16:23:44,943: t15.2024.06.14 val PER: 0.1924
2026-01-11 16:23:44,943: t15.2024.07.19 val PER: 0.2472
2026-01-11 16:23:44,943: t15.2024.07.21 val PER: 0.1386
2026-01-11 16:23:44,944: t15.2024.07.28 val PER: 0.2015
2026-01-11 16:23:44,944: t15.2025.01.10 val PER: 0.3251
2026-01-11 16:23:44,944: t15.2025.01.12 val PER: 0.1909
2026-01-11 16:23:44,944: t15.2025.03.14 val PER: 0.3240
2026-01-11 16:23:44,944: t15.2025.03.16 val PER: 0.2317
2026-01-11 16:23:44,944: t15.2025.03.30 val PER: 0.2920
2026-01-11 16:23:44,944: t15.2025.04.13 val PER: 0.2582
2026-01-11 16:23:45,112: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_24500
2026-01-11 16:23:55,444: Train batch 24600: loss: 24.77 grad norm: 114.67 time: 0.068
2026-01-11 16:24:16,213: Train batch 24800: loss: 26.61 grad norm: 167.85 time: 0.086
2026-01-11 16:24:36,320: Train batch 25000: loss: 24.04 grad norm: 86.42 time: 0.075
2026-01-11 16:24:36,321: Running test after training batch: 25000
2026-01-11 16:24:36,695: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:24:43,655: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point is why
2026-01-11 16:24:43,733: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost gay
2026-01-11 16:25:05,000: Val batch 25000: PER (avg): 0.1946 CTC Loss (avg): 32.1661 WER(5gram): 27.97% (n=256) time: 28.679
2026-01-11 16:25:05,000: WER lens: avg_true_words=5.99 avg_pred_words=6.03 max_pred_words=12
2026-01-11 16:25:05,001: t15.2023.08.13 val PER: 0.1497
2026-01-11 16:25:05,001: t15.2023.08.18 val PER: 0.1593
2026-01-11 16:25:05,001: t15.2023.08.20 val PER: 0.1390
2026-01-11 16:25:05,001: t15.2023.08.25 val PER: 0.1551
2026-01-11 16:25:05,001: t15.2023.08.27 val PER: 0.2170
2026-01-11 16:25:05,001: t15.2023.09.01 val PER: 0.1096
2026-01-11 16:25:05,001: t15.2023.09.03 val PER: 0.1912
2026-01-11 16:25:05,002: t15.2023.09.24 val PER: 0.1456
2026-01-11 16:25:05,002: t15.2023.09.29 val PER: 0.1666
2026-01-11 16:25:05,002: t15.2023.10.01 val PER: 0.2371
2026-01-11 16:25:05,002: t15.2023.10.06 val PER: 0.1518
2026-01-11 16:25:05,002: t15.2023.10.08 val PER: 0.3099
2026-01-11 16:25:05,002: t15.2023.10.13 val PER: 0.2855
2026-01-11 16:25:05,002: t15.2023.10.15 val PER: 0.2030
2026-01-11 16:25:05,002: t15.2023.10.20 val PER: 0.2617
2026-01-11 16:25:05,002: t15.2023.10.22 val PER: 0.1682
2026-01-11 16:25:05,002: t15.2023.11.03 val PER: 0.2259
2026-01-11 16:25:05,003: t15.2023.11.04 val PER: 0.0307
2026-01-11 16:25:05,003: t15.2023.11.17 val PER: 0.0575
2026-01-11 16:25:05,003: t15.2023.11.19 val PER: 0.0659
2026-01-11 16:25:05,003: t15.2023.11.26 val PER: 0.2051
2026-01-11 16:25:05,003: t15.2023.12.03 val PER: 0.1555
2026-01-11 16:25:05,003: t15.2023.12.08 val PER: 0.1611
2026-01-11 16:25:05,003: t15.2023.12.10 val PER: 0.1498
2026-01-11 16:25:05,003: t15.2023.12.17 val PER: 0.1559
2026-01-11 16:25:05,003: t15.2023.12.29 val PER: 0.1833
2026-01-11 16:25:05,004: t15.2024.02.25 val PER: 0.1615
2026-01-11 16:25:05,004: t15.2024.03.08 val PER: 0.2760
2026-01-11 16:25:05,004: t15.2024.03.15 val PER: 0.2376
2026-01-11 16:25:05,004: t15.2024.03.17 val PER: 0.1911
2026-01-11 16:25:05,004: t15.2024.05.10 val PER: 0.2065
2026-01-11 16:25:05,004: t15.2024.06.14 val PER: 0.1830
2026-01-11 16:25:05,004: t15.2024.07.19 val PER: 0.2518
2026-01-11 16:25:05,004: t15.2024.07.21 val PER: 0.1400
2026-01-11 16:25:05,004: t15.2024.07.28 val PER: 0.1963
2026-01-11 16:25:05,004: t15.2025.01.10 val PER: 0.3223
2026-01-11 16:25:05,005: t15.2025.01.12 val PER: 0.1886
2026-01-11 16:25:05,005: t15.2025.03.14 val PER: 0.3166
2026-01-11 16:25:05,005: t15.2025.03.16 val PER: 0.2343
2026-01-11 16:25:05,005: t15.2025.03.30 val PER: 0.2908
2026-01-11 16:25:05,005: t15.2025.04.13 val PER: 0.2639
2026-01-11 16:25:05,179: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_25000
2026-01-11 16:25:25,351: Train batch 25200: loss: 16.52 grad norm: 82.40 time: 0.071
2026-01-11 16:25:46,028: Train batch 25400: loss: 19.15 grad norm: 81.41 time: 0.067
2026-01-11 16:25:56,546: Running test after training batch: 25500
2026-01-11 16:25:56,676: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:26:02,898: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point as well
2026-01-11 16:26:02,982: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs they may
2026-01-11 16:26:26,046: Val batch 25500: PER (avg): 0.1947 CTC Loss (avg): 32.0613 WER(5gram): 28.75% (n=256) time: 29.499
2026-01-11 16:26:26,047: WER lens: avg_true_words=5.99 avg_pred_words=6.06 max_pred_words=12
2026-01-11 16:26:26,047: t15.2023.08.13 val PER: 0.1507
2026-01-11 16:26:26,047: t15.2023.08.18 val PER: 0.1567
2026-01-11 16:26:26,047: t15.2023.08.20 val PER: 0.1311
2026-01-11 16:26:26,048: t15.2023.08.25 val PER: 0.1476
2026-01-11 16:26:26,048: t15.2023.08.27 val PER: 0.2154
2026-01-11 16:26:26,048: t15.2023.09.01 val PER: 0.1120
2026-01-11 16:26:26,048: t15.2023.09.03 val PER: 0.1865
2026-01-11 16:26:26,048: t15.2023.09.24 val PER: 0.1578
2026-01-11 16:26:26,048: t15.2023.09.29 val PER: 0.1627
2026-01-11 16:26:26,048: t15.2023.10.01 val PER: 0.2312
2026-01-11 16:26:26,048: t15.2023.10.06 val PER: 0.1464
2026-01-11 16:26:26,048: t15.2023.10.08 val PER: 0.3031
2026-01-11 16:26:26,049: t15.2023.10.13 val PER: 0.2987
2026-01-11 16:26:26,049: t15.2023.10.15 val PER: 0.2030
2026-01-11 16:26:26,049: t15.2023.10.20 val PER: 0.2383
2026-01-11 16:26:26,049: t15.2023.10.22 val PER: 0.1726
2026-01-11 16:26:26,049: t15.2023.11.03 val PER: 0.2212
2026-01-11 16:26:26,049: t15.2023.11.04 val PER: 0.0273
2026-01-11 16:26:26,049: t15.2023.11.17 val PER: 0.0482
2026-01-11 16:26:26,049: t15.2023.11.19 val PER: 0.0758
2026-01-11 16:26:26,049: t15.2023.11.26 val PER: 0.2116
2026-01-11 16:26:26,049: t15.2023.12.03 val PER: 0.1576
2026-01-11 16:26:26,049: t15.2023.12.08 val PER: 0.1598
2026-01-11 16:26:26,050: t15.2023.12.10 val PER: 0.1485
2026-01-11 16:26:26,050: t15.2023.12.17 val PER: 0.1559
2026-01-11 16:26:26,050: t15.2023.12.29 val PER: 0.1791
2026-01-11 16:26:26,050: t15.2024.02.25 val PER: 0.1587
2026-01-11 16:26:26,050: t15.2024.03.08 val PER: 0.2717
2026-01-11 16:26:26,050: t15.2024.03.15 val PER: 0.2402
2026-01-11 16:26:26,050: t15.2024.03.17 val PER: 0.1869
2026-01-11 16:26:26,050: t15.2024.05.10 val PER: 0.1947
2026-01-11 16:26:26,051: t15.2024.06.14 val PER: 0.1893
2026-01-11 16:26:26,051: t15.2024.07.19 val PER: 0.2591
2026-01-11 16:26:26,052: t15.2024.07.21 val PER: 0.1386
2026-01-11 16:26:26,052: t15.2024.07.28 val PER: 0.2015
2026-01-11 16:26:26,052: t15.2025.01.10 val PER: 0.3361
2026-01-11 16:26:26,052: t15.2025.01.12 val PER: 0.1940
2026-01-11 16:26:26,052: t15.2025.03.14 val PER: 0.3240
2026-01-11 16:26:26,052: t15.2025.03.16 val PER: 0.2435
2026-01-11 16:26:26,052: t15.2025.03.30 val PER: 0.2931
2026-01-11 16:26:26,052: t15.2025.04.13 val PER: 0.2582
2026-01-11 16:26:26,236: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_25500
2026-01-11 16:26:36,307: Train batch 25600: loss: 21.63 grad norm: 79.62 time: 0.071
2026-01-11 16:26:56,688: Train batch 25800: loss: 14.52 grad norm: 80.88 time: 0.070
2026-01-11 16:27:16,797: Train batch 26000: loss: 15.50 grad norm: 85.62 time: 0.072
2026-01-11 16:27:16,797: Running test after training batch: 26000
2026-01-11 16:27:16,992: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:27:23,936: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:27:24,042: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost k
2026-01-11 16:27:44,809: Val batch 26000: PER (avg): 0.1968 CTC Loss (avg): 32.1132 WER(5gram): 30.51% (n=256) time: 28.011
2026-01-11 16:27:44,810: WER lens: avg_true_words=5.99 avg_pred_words=6.06 max_pred_words=13
2026-01-11 16:27:44,810: t15.2023.08.13 val PER: 0.1580
2026-01-11 16:27:44,810: t15.2023.08.18 val PER: 0.1576
2026-01-11 16:27:44,810: t15.2023.08.20 val PER: 0.1319
2026-01-11 16:27:44,810: t15.2023.08.25 val PER: 0.1416
2026-01-11 16:27:44,811: t15.2023.08.27 val PER: 0.2186
2026-01-11 16:27:44,811: t15.2023.09.01 val PER: 0.1128
2026-01-11 16:27:44,811: t15.2023.09.03 val PER: 0.2019
2026-01-11 16:27:44,811: t15.2023.09.24 val PER: 0.1553
2026-01-11 16:27:44,811: t15.2023.09.29 val PER: 0.1691
2026-01-11 16:27:44,811: t15.2023.10.01 val PER: 0.2391
2026-01-11 16:27:44,812: t15.2023.10.06 val PER: 0.1475
2026-01-11 16:27:44,812: t15.2023.10.08 val PER: 0.3072
2026-01-11 16:27:44,812: t15.2023.10.13 val PER: 0.3018
2026-01-11 16:27:44,812: t15.2023.10.15 val PER: 0.2037
2026-01-11 16:27:44,813: t15.2023.10.20 val PER: 0.2483
2026-01-11 16:27:44,813: t15.2023.10.22 val PER: 0.1748
2026-01-11 16:27:44,813: t15.2023.11.03 val PER: 0.2259
2026-01-11 16:27:44,813: t15.2023.11.04 val PER: 0.0307
2026-01-11 16:27:44,813: t15.2023.11.17 val PER: 0.0591
2026-01-11 16:27:44,813: t15.2023.11.19 val PER: 0.0659
2026-01-11 16:27:44,814: t15.2023.11.26 val PER: 0.2116
2026-01-11 16:27:44,814: t15.2023.12.03 val PER: 0.1544
2026-01-11 16:27:44,814: t15.2023.12.08 val PER: 0.1578
2026-01-11 16:27:44,814: t15.2023.12.10 val PER: 0.1564
2026-01-11 16:27:44,814: t15.2023.12.17 val PER: 0.1559
2026-01-11 16:27:44,814: t15.2023.12.29 val PER: 0.1791
2026-01-11 16:27:44,814: t15.2024.02.25 val PER: 0.1629
2026-01-11 16:27:44,814: t15.2024.03.08 val PER: 0.2603
2026-01-11 16:27:44,814: t15.2024.03.15 val PER: 0.2464
2026-01-11 16:27:44,814: t15.2024.03.17 val PER: 0.1890
2026-01-11 16:27:44,815: t15.2024.05.10 val PER: 0.2065
2026-01-11 16:27:44,815: t15.2024.06.14 val PER: 0.1830
2026-01-11 16:27:44,815: t15.2024.07.19 val PER: 0.2617
2026-01-11 16:27:44,815: t15.2024.07.21 val PER: 0.1379
2026-01-11 16:27:44,815: t15.2024.07.28 val PER: 0.2007
2026-01-11 16:27:44,823: t15.2025.01.10 val PER: 0.3361
2026-01-11 16:27:44,823: t15.2025.01.12 val PER: 0.1955
2026-01-11 16:27:44,823: t15.2025.03.14 val PER: 0.3417
2026-01-11 16:27:44,823: t15.2025.03.16 val PER: 0.2291
2026-01-11 16:27:44,824: t15.2025.03.30 val PER: 0.2966
2026-01-11 16:27:44,824: t15.2025.04.13 val PER: 0.2611
2026-01-11 16:27:44,995: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_26000
2026-01-11 16:28:05,528: Train batch 26200: loss: 15.01 grad norm: 55.58 time: 0.069
2026-01-11 16:28:26,799: Train batch 26400: loss: 21.10 grad norm: 70.90 time: 0.085
2026-01-11 16:28:37,300: Running test after training batch: 26500
2026-01-11 16:28:37,413: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:28:43,943: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:28:44,024: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost k
2026-01-11 16:29:05,448: Val batch 26500: PER (avg): 0.1941 CTC Loss (avg): 31.9209 WER(5gram): 27.57% (n=256) time: 28.147
2026-01-11 16:29:05,448: WER lens: avg_true_words=5.99 avg_pred_words=6.03 max_pred_words=12
2026-01-11 16:29:05,449: t15.2023.08.13 val PER: 0.1622
2026-01-11 16:29:05,449: t15.2023.08.18 val PER: 0.1484
2026-01-11 16:29:05,449: t15.2023.08.20 val PER: 0.1311
2026-01-11 16:29:05,449: t15.2023.08.25 val PER: 0.1431
2026-01-11 16:29:05,449: t15.2023.08.27 val PER: 0.2299
2026-01-11 16:29:05,450: t15.2023.09.01 val PER: 0.1128
2026-01-11 16:29:05,450: t15.2023.09.03 val PER: 0.1971
2026-01-11 16:29:05,450: t15.2023.09.24 val PER: 0.1468
2026-01-11 16:29:05,450: t15.2023.09.29 val PER: 0.1666
2026-01-11 16:29:05,450: t15.2023.10.01 val PER: 0.2391
2026-01-11 16:29:05,450: t15.2023.10.06 val PER: 0.1410
2026-01-11 16:29:05,450: t15.2023.10.08 val PER: 0.3153
2026-01-11 16:29:05,450: t15.2023.10.13 val PER: 0.2995
2026-01-11 16:29:05,450: t15.2023.10.15 val PER: 0.2050
2026-01-11 16:29:05,451: t15.2023.10.20 val PER: 0.2483
2026-01-11 16:29:05,451: t15.2023.10.22 val PER: 0.1771
2026-01-11 16:29:05,451: t15.2023.11.03 val PER: 0.2273
2026-01-11 16:29:05,451: t15.2023.11.04 val PER: 0.0239
2026-01-11 16:29:05,451: t15.2023.11.17 val PER: 0.0513
2026-01-11 16:29:05,451: t15.2023.11.19 val PER: 0.0679
2026-01-11 16:29:05,451: t15.2023.11.26 val PER: 0.2116
2026-01-11 16:29:05,452: t15.2023.12.03 val PER: 0.1586
2026-01-11 16:29:05,452: t15.2023.12.08 val PER: 0.1558
2026-01-11 16:29:05,452: t15.2023.12.10 val PER: 0.1524
2026-01-11 16:29:05,452: t15.2023.12.17 val PER: 0.1559
2026-01-11 16:29:05,452: t15.2023.12.29 val PER: 0.1819
2026-01-11 16:29:05,452: t15.2024.02.25 val PER: 0.1531
2026-01-11 16:29:05,452: t15.2024.03.08 val PER: 0.2646
2026-01-11 16:29:05,452: t15.2024.03.15 val PER: 0.2395
2026-01-11 16:29:05,452: t15.2024.03.17 val PER: 0.1848
2026-01-11 16:29:05,453: t15.2024.05.10 val PER: 0.1961
2026-01-11 16:29:05,453: t15.2024.06.14 val PER: 0.1830
2026-01-11 16:29:05,453: t15.2024.07.19 val PER: 0.2505
2026-01-11 16:29:05,453: t15.2024.07.21 val PER: 0.1352
2026-01-11 16:29:05,453: t15.2024.07.28 val PER: 0.1971
2026-01-11 16:29:05,453: t15.2025.01.10 val PER: 0.3320
2026-01-11 16:29:05,453: t15.2025.01.12 val PER: 0.1925
2026-01-11 16:29:05,454: t15.2025.03.14 val PER: 0.3284
2026-01-11 16:29:05,454: t15.2025.03.16 val PER: 0.2199
2026-01-11 16:29:05,454: t15.2025.03.30 val PER: 0.2908
2026-01-11 16:29:05,454: t15.2025.04.13 val PER: 0.2439
2026-01-11 16:29:05,641: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_26500
2026-01-11 16:29:16,041: Train batch 26600: loss: 15.59 grad norm: 70.93 time: 0.067
2026-01-11 16:29:37,218: Train batch 26800: loss: 29.84 grad norm: 129.85 time: 0.089
2026-01-11 16:29:57,542: Train batch 27000: loss: 19.80 grad norm: 85.24 time: 0.072
2026-01-11 16:29:57,543: Running test after training batch: 27000
2026-01-11 16:29:57,666: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:30:03,934: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:30:04,018: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost gay
2026-01-11 16:30:24,386: Val batch 27000: PER (avg): 0.1919 CTC Loss (avg): 31.4433 WER(5gram): 27.71% (n=256) time: 26.843
2026-01-11 16:30:24,387: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=13
2026-01-11 16:30:24,387: t15.2023.08.13 val PER: 0.1570
2026-01-11 16:30:24,388: t15.2023.08.18 val PER: 0.1567
2026-01-11 16:30:24,388: t15.2023.08.20 val PER: 0.1255
2026-01-11 16:30:24,388: t15.2023.08.25 val PER: 0.1446
2026-01-11 16:30:24,388: t15.2023.08.27 val PER: 0.2203
2026-01-11 16:30:24,388: t15.2023.09.01 val PER: 0.1112
2026-01-11 16:30:24,388: t15.2023.09.03 val PER: 0.1983
2026-01-11 16:30:24,389: t15.2023.09.24 val PER: 0.1541
2026-01-11 16:30:24,389: t15.2023.09.29 val PER: 0.1653
2026-01-11 16:30:24,389: t15.2023.10.01 val PER: 0.2332
2026-01-11 16:30:24,389: t15.2023.10.06 val PER: 0.1421
2026-01-11 16:30:24,389: t15.2023.10.08 val PER: 0.3004
2026-01-11 16:30:24,389: t15.2023.10.13 val PER: 0.2886
2026-01-11 16:30:24,389: t15.2023.10.15 val PER: 0.1945
2026-01-11 16:30:24,389: t15.2023.10.20 val PER: 0.2550
2026-01-11 16:30:24,390: t15.2023.10.22 val PER: 0.1759
2026-01-11 16:30:24,391: t15.2023.11.03 val PER: 0.2225
2026-01-11 16:30:24,391: t15.2023.11.04 val PER: 0.0307
2026-01-11 16:30:24,391: t15.2023.11.17 val PER: 0.0529
2026-01-11 16:30:24,391: t15.2023.11.19 val PER: 0.0659
2026-01-11 16:30:24,391: t15.2023.11.26 val PER: 0.2087
2026-01-11 16:30:24,391: t15.2023.12.03 val PER: 0.1544
2026-01-11 16:30:24,392: t15.2023.12.08 val PER: 0.1538
2026-01-11 16:30:24,392: t15.2023.12.10 val PER: 0.1432
2026-01-11 16:30:24,392: t15.2023.12.17 val PER: 0.1538
2026-01-11 16:30:24,392: t15.2023.12.29 val PER: 0.1798
2026-01-11 16:30:24,392: t15.2024.02.25 val PER: 0.1643
2026-01-11 16:30:24,392: t15.2024.03.08 val PER: 0.2632
2026-01-11 16:30:24,392: t15.2024.03.15 val PER: 0.2358
2026-01-11 16:30:24,392: t15.2024.03.17 val PER: 0.1862
2026-01-11 16:30:24,392: t15.2024.05.10 val PER: 0.1932
2026-01-11 16:30:24,393: t15.2024.06.14 val PER: 0.1861
2026-01-11 16:30:24,393: t15.2024.07.19 val PER: 0.2465
2026-01-11 16:30:24,393: t15.2024.07.21 val PER: 0.1331
2026-01-11 16:30:24,393: t15.2024.07.28 val PER: 0.1868
2026-01-11 16:30:24,393: t15.2025.01.10 val PER: 0.3292
2026-01-11 16:30:24,393: t15.2025.01.12 val PER: 0.1901
2026-01-11 16:30:24,393: t15.2025.03.14 val PER: 0.3180
2026-01-11 16:30:24,393: t15.2025.03.16 val PER: 0.2277
2026-01-11 16:30:24,394: t15.2025.03.30 val PER: 0.3000
2026-01-11 16:30:24,394: t15.2025.04.13 val PER: 0.2539
2026-01-11 16:30:24,569: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_27000
2026-01-11 16:30:44,658: Train batch 27200: loss: 20.01 grad norm: 70.76 time: 0.073
2026-01-11 16:31:05,639: Train batch 27400: loss: 25.32 grad norm: 87.13 time: 0.066
2026-01-11 16:31:16,173: Running test after training batch: 27500
2026-01-11 16:31:16,371: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:31:23,298: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:31:23,384: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost gay
2026-01-11 16:31:44,295: Val batch 27500: PER (avg): 0.1917 CTC Loss (avg): 31.5117 WER(5gram): 27.31% (n=256) time: 28.122
2026-01-11 16:31:44,296: WER lens: avg_true_words=5.99 avg_pred_words=6.04 max_pred_words=13
2026-01-11 16:31:44,296: t15.2023.08.13 val PER: 0.1549
2026-01-11 16:31:44,297: t15.2023.08.18 val PER: 0.1559
2026-01-11 16:31:44,297: t15.2023.08.20 val PER: 0.1295
2026-01-11 16:31:44,297: t15.2023.08.25 val PER: 0.1431
2026-01-11 16:31:44,297: t15.2023.08.27 val PER: 0.2251
2026-01-11 16:31:44,297: t15.2023.09.01 val PER: 0.1080
2026-01-11 16:31:44,297: t15.2023.09.03 val PER: 0.1912
2026-01-11 16:31:44,297: t15.2023.09.24 val PER: 0.1505
2026-01-11 16:31:44,297: t15.2023.09.29 val PER: 0.1653
2026-01-11 16:31:44,298: t15.2023.10.01 val PER: 0.2279
2026-01-11 16:31:44,298: t15.2023.10.06 val PER: 0.1410
2026-01-11 16:31:44,298: t15.2023.10.08 val PER: 0.2963
2026-01-11 16:31:44,298: t15.2023.10.13 val PER: 0.2901
2026-01-11 16:31:44,298: t15.2023.10.15 val PER: 0.2011
2026-01-11 16:31:44,298: t15.2023.10.20 val PER: 0.2517
2026-01-11 16:31:44,298: t15.2023.10.22 val PER: 0.1748
2026-01-11 16:31:44,298: t15.2023.11.03 val PER: 0.2218
2026-01-11 16:31:44,299: t15.2023.11.04 val PER: 0.0341
2026-01-11 16:31:44,299: t15.2023.11.17 val PER: 0.0560
2026-01-11 16:31:44,299: t15.2023.11.19 val PER: 0.0599
2026-01-11 16:31:44,299: t15.2023.11.26 val PER: 0.2065
2026-01-11 16:31:44,299: t15.2023.12.03 val PER: 0.1586
2026-01-11 16:31:44,299: t15.2023.12.08 val PER: 0.1585
2026-01-11 16:31:44,299: t15.2023.12.10 val PER: 0.1472
2026-01-11 16:31:44,300: t15.2023.12.17 val PER: 0.1518
2026-01-11 16:31:44,300: t15.2023.12.29 val PER: 0.1730
2026-01-11 16:31:44,300: t15.2024.02.25 val PER: 0.1573
2026-01-11 16:31:44,300: t15.2024.03.08 val PER: 0.2603
2026-01-11 16:31:44,300: t15.2024.03.15 val PER: 0.2408
2026-01-11 16:31:44,300: t15.2024.03.17 val PER: 0.1869
2026-01-11 16:31:44,300: t15.2024.05.10 val PER: 0.1976
2026-01-11 16:31:44,300: t15.2024.06.14 val PER: 0.1830
2026-01-11 16:31:44,300: t15.2024.07.19 val PER: 0.2485
2026-01-11 16:31:44,301: t15.2024.07.21 val PER: 0.1338
2026-01-11 16:31:44,301: t15.2024.07.28 val PER: 0.1956
2026-01-11 16:31:44,301: t15.2025.01.10 val PER: 0.3182
2026-01-11 16:31:44,301: t15.2025.01.12 val PER: 0.1886
2026-01-11 16:31:44,301: t15.2025.03.14 val PER: 0.3210
2026-01-11 16:31:44,301: t15.2025.03.16 val PER: 0.2251
2026-01-11 16:31:44,302: t15.2025.03.30 val PER: 0.2966
2026-01-11 16:31:44,302: t15.2025.04.13 val PER: 0.2482
2026-01-11 16:31:44,478: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_27500
2026-01-11 16:31:54,827: Train batch 27600: loss: 22.82 grad norm: 65.28 time: 0.067
2026-01-11 16:32:15,030: Train batch 27800: loss: 18.81 grad norm: 61.54 time: 0.050
2026-01-11 16:32:35,997: Train batch 28000: loss: 15.70 grad norm: 170.62 time: 0.065
2026-01-11 16:32:35,998: Running test after training batch: 28000
2026-01-11 16:32:36,450: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:32:42,852: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:32:42,928: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost k
2026-01-11 16:33:02,635: Val batch 28000: PER (avg): 0.1893 CTC Loss (avg): 31.6767 WER(5gram): 26.86% (n=256) time: 26.637
2026-01-11 16:33:02,636: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=13
2026-01-11 16:33:02,637: t15.2023.08.13 val PER: 0.1538
2026-01-11 16:33:02,637: t15.2023.08.18 val PER: 0.1500
2026-01-11 16:33:02,637: t15.2023.08.20 val PER: 0.1239
2026-01-11 16:33:02,637: t15.2023.08.25 val PER: 0.1370
2026-01-11 16:33:02,637: t15.2023.08.27 val PER: 0.2315
2026-01-11 16:33:02,638: t15.2023.09.01 val PER: 0.1080
2026-01-11 16:33:02,638: t15.2023.09.03 val PER: 0.1983
2026-01-11 16:33:02,638: t15.2023.09.24 val PER: 0.1541
2026-01-11 16:33:02,639: t15.2023.09.29 val PER: 0.1627
2026-01-11 16:33:02,639: t15.2023.10.01 val PER: 0.2292
2026-01-11 16:33:02,639: t15.2023.10.06 val PER: 0.1367
2026-01-11 16:33:02,639: t15.2023.10.08 val PER: 0.2950
2026-01-11 16:33:02,639: t15.2023.10.13 val PER: 0.2839
2026-01-11 16:33:02,639: t15.2023.10.15 val PER: 0.1971
2026-01-11 16:33:02,639: t15.2023.10.20 val PER: 0.2349
2026-01-11 16:33:02,639: t15.2023.10.22 val PER: 0.1726
2026-01-11 16:33:02,639: t15.2023.11.03 val PER: 0.2185
2026-01-11 16:33:02,639: t15.2023.11.04 val PER: 0.0341
2026-01-11 16:33:02,640: t15.2023.11.17 val PER: 0.0451
2026-01-11 16:33:02,640: t15.2023.11.19 val PER: 0.0619
2026-01-11 16:33:02,640: t15.2023.11.26 val PER: 0.2014
2026-01-11 16:33:02,640: t15.2023.12.03 val PER: 0.1660
2026-01-11 16:33:02,641: t15.2023.12.08 val PER: 0.1531
2026-01-11 16:33:02,642: t15.2023.12.10 val PER: 0.1564
2026-01-11 16:33:02,642: t15.2023.12.17 val PER: 0.1497
2026-01-11 16:33:02,642: t15.2023.12.29 val PER: 0.1736
2026-01-11 16:33:02,642: t15.2024.02.25 val PER: 0.1461
2026-01-11 16:33:02,642: t15.2024.03.08 val PER: 0.2646
2026-01-11 16:33:02,642: t15.2024.03.15 val PER: 0.2351
2026-01-11 16:33:02,642: t15.2024.03.17 val PER: 0.1813
2026-01-11 16:33:02,642: t15.2024.05.10 val PER: 0.1991
2026-01-11 16:33:02,643: t15.2024.06.14 val PER: 0.1814
2026-01-11 16:33:02,643: t15.2024.07.19 val PER: 0.2485
2026-01-11 16:33:02,643: t15.2024.07.21 val PER: 0.1331
2026-01-11 16:33:02,643: t15.2024.07.28 val PER: 0.1926
2026-01-11 16:33:02,643: t15.2025.01.10 val PER: 0.3168
2026-01-11 16:33:02,643: t15.2025.01.12 val PER: 0.1925
2026-01-11 16:33:02,644: t15.2025.03.14 val PER: 0.3107
2026-01-11 16:33:02,644: t15.2025.03.16 val PER: 0.2081
2026-01-11 16:33:02,644: t15.2025.03.30 val PER: 0.2897
2026-01-11 16:33:02,644: t15.2025.04.13 val PER: 0.2439
2026-01-11 16:33:02,817: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_28000
2026-01-11 16:33:23,469: Train batch 28200: loss: 13.48 grad norm: 70.21 time: 0.071
2026-01-11 16:33:44,447: Train batch 28400: loss: 16.62 grad norm: 74.74 time: 0.067
2026-01-11 16:33:54,821: Running test after training batch: 28500
2026-01-11 16:33:55,083: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:34:01,356: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point as well
2026-01-11 16:34:01,442: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-11 16:34:24,614: Val batch 28500: PER (avg): 0.1894 CTC Loss (avg): 31.2305 WER(5gram): 29.47% (n=256) time: 29.792
2026-01-11 16:34:24,615: WER lens: avg_true_words=5.99 avg_pred_words=6.04 max_pred_words=13
2026-01-11 16:34:24,615: t15.2023.08.13 val PER: 0.1601
2026-01-11 16:34:24,615: t15.2023.08.18 val PER: 0.1475
2026-01-11 16:34:24,616: t15.2023.08.20 val PER: 0.1303
2026-01-11 16:34:24,616: t15.2023.08.25 val PER: 0.1325
2026-01-11 16:34:24,616: t15.2023.08.27 val PER: 0.2235
2026-01-11 16:34:24,616: t15.2023.09.01 val PER: 0.1128
2026-01-11 16:34:24,616: t15.2023.09.03 val PER: 0.1900
2026-01-11 16:34:24,616: t15.2023.09.24 val PER: 0.1541
2026-01-11 16:34:24,616: t15.2023.09.29 val PER: 0.1621
2026-01-11 16:34:24,616: t15.2023.10.01 val PER: 0.2345
2026-01-11 16:34:24,616: t15.2023.10.06 val PER: 0.1410
2026-01-11 16:34:24,617: t15.2023.10.08 val PER: 0.2977
2026-01-11 16:34:24,617: t15.2023.10.13 val PER: 0.2894
2026-01-11 16:34:24,617: t15.2023.10.15 val PER: 0.1971
2026-01-11 16:34:24,617: t15.2023.10.20 val PER: 0.2416
2026-01-11 16:34:24,617: t15.2023.10.22 val PER: 0.1726
2026-01-11 16:34:24,617: t15.2023.11.03 val PER: 0.2123
2026-01-11 16:34:24,617: t15.2023.11.04 val PER: 0.0273
2026-01-11 16:34:24,617: t15.2023.11.17 val PER: 0.0529
2026-01-11 16:34:24,617: t15.2023.11.19 val PER: 0.0619
2026-01-11 16:34:24,618: t15.2023.11.26 val PER: 0.2000
2026-01-11 16:34:24,618: t15.2023.12.03 val PER: 0.1492
2026-01-11 16:34:24,618: t15.2023.12.08 val PER: 0.1551
2026-01-11 16:34:24,618: t15.2023.12.10 val PER: 0.1419
2026-01-11 16:34:24,618: t15.2023.12.17 val PER: 0.1424
2026-01-11 16:34:24,618: t15.2023.12.29 val PER: 0.1716
2026-01-11 16:34:24,618: t15.2024.02.25 val PER: 0.1531
2026-01-11 16:34:24,618: t15.2024.03.08 val PER: 0.2731
2026-01-11 16:34:24,618: t15.2024.03.15 val PER: 0.2308
2026-01-11 16:34:24,618: t15.2024.03.17 val PER: 0.1911
2026-01-11 16:34:24,619: t15.2024.05.10 val PER: 0.1947
2026-01-11 16:34:24,619: t15.2024.06.14 val PER: 0.1814
2026-01-11 16:34:24,619: t15.2024.07.19 val PER: 0.2512
2026-01-11 16:34:24,619: t15.2024.07.21 val PER: 0.1303
2026-01-11 16:34:24,619: t15.2024.07.28 val PER: 0.1919
2026-01-11 16:34:24,619: t15.2025.01.10 val PER: 0.3223
2026-01-11 16:34:24,619: t15.2025.01.12 val PER: 0.1871
2026-01-11 16:34:24,619: t15.2025.03.14 val PER: 0.3107
2026-01-11 16:34:24,619: t15.2025.03.16 val PER: 0.2238
2026-01-11 16:34:24,620: t15.2025.03.30 val PER: 0.2805
2026-01-11 16:34:24,620: t15.2025.04.13 val PER: 0.2525
2026-01-11 16:34:24,792: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_28500
2026-01-11 16:34:35,098: Train batch 28600: loss: 22.07 grad norm: 72.91 time: 0.073
2026-01-11 16:34:55,990: Train batch 28800: loss: 18.37 grad norm: 103.05 time: 0.055
2026-01-11 16:35:16,499: Train batch 29000: loss: 16.56 grad norm: 71.92 time: 0.057
2026-01-11 16:35:16,499: Running test after training batch: 29000
2026-01-11 16:35:16,670: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:35:23,682: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point as well
2026-01-11 16:35:23,763: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost gay
2026-01-11 16:35:43,099: Val batch 29000: PER (avg): 0.1884 CTC Loss (avg): 31.4893 WER(5gram): 28.36% (n=256) time: 26.599
2026-01-11 16:35:43,100: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=13
2026-01-11 16:35:43,100: t15.2023.08.13 val PER: 0.1497
2026-01-11 16:35:43,100: t15.2023.08.18 val PER: 0.1475
2026-01-11 16:35:43,101: t15.2023.08.20 val PER: 0.1295
2026-01-11 16:35:43,101: t15.2023.08.25 val PER: 0.1491
2026-01-11 16:35:43,101: t15.2023.08.27 val PER: 0.2219
2026-01-11 16:35:43,101: t15.2023.09.01 val PER: 0.1104
2026-01-11 16:35:43,101: t15.2023.09.03 val PER: 0.1888
2026-01-11 16:35:43,101: t15.2023.09.24 val PER: 0.1493
2026-01-11 16:35:43,101: t15.2023.09.29 val PER: 0.1621
2026-01-11 16:35:43,101: t15.2023.10.01 val PER: 0.2292
2026-01-11 16:35:43,101: t15.2023.10.06 val PER: 0.1442
2026-01-11 16:35:43,102: t15.2023.10.08 val PER: 0.2991
2026-01-11 16:35:43,102: t15.2023.10.13 val PER: 0.2793
2026-01-11 16:35:43,102: t15.2023.10.15 val PER: 0.1971
2026-01-11 16:35:43,102: t15.2023.10.20 val PER: 0.2450
2026-01-11 16:35:43,102: t15.2023.10.22 val PER: 0.1648
2026-01-11 16:35:43,102: t15.2023.11.03 val PER: 0.2198
2026-01-11 16:35:43,102: t15.2023.11.04 val PER: 0.0307
2026-01-11 16:35:43,102: t15.2023.11.17 val PER: 0.0467
2026-01-11 16:35:43,102: t15.2023.11.19 val PER: 0.0639
2026-01-11 16:35:43,102: t15.2023.11.26 val PER: 0.1993
2026-01-11 16:35:43,103: t15.2023.12.03 val PER: 0.1639
2026-01-11 16:35:43,103: t15.2023.12.08 val PER: 0.1505
2026-01-11 16:35:43,104: t15.2023.12.10 val PER: 0.1459
2026-01-11 16:35:43,104: t15.2023.12.17 val PER: 0.1486
2026-01-11 16:35:43,104: t15.2023.12.29 val PER: 0.1702
2026-01-11 16:35:43,104: t15.2024.02.25 val PER: 0.1559
2026-01-11 16:35:43,104: t15.2024.03.08 val PER: 0.2575
2026-01-11 16:35:43,104: t15.2024.03.15 val PER: 0.2339
2026-01-11 16:35:43,104: t15.2024.03.17 val PER: 0.1792
2026-01-11 16:35:43,104: t15.2024.05.10 val PER: 0.1947
2026-01-11 16:35:43,105: t15.2024.06.14 val PER: 0.1877
2026-01-11 16:35:43,105: t15.2024.07.19 val PER: 0.2459
2026-01-11 16:35:43,105: t15.2024.07.21 val PER: 0.1310
2026-01-11 16:35:43,105: t15.2024.07.28 val PER: 0.1919
2026-01-11 16:35:43,105: t15.2025.01.10 val PER: 0.3278
2026-01-11 16:35:43,105: t15.2025.01.12 val PER: 0.1909
2026-01-11 16:35:43,105: t15.2025.03.14 val PER: 0.3195
2026-01-11 16:35:43,105: t15.2025.03.16 val PER: 0.2029
2026-01-11 16:35:43,105: t15.2025.03.30 val PER: 0.2839
2026-01-11 16:35:43,106: t15.2025.04.13 val PER: 0.2411
2026-01-11 16:35:43,278: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_29000
2026-01-11 16:36:03,674: Train batch 29200: loss: 17.36 grad norm: 92.74 time: 0.072
2026-01-11 16:36:24,892: Train batch 29400: loss: 14.93 grad norm: 96.66 time: 0.066
2026-01-11 16:36:35,331: Running test after training batch: 29500
2026-01-11 16:36:35,503: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:36:42,132: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:36:42,208: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost gay
2026-01-11 16:37:02,232: Val batch 29500: PER (avg): 0.1860 CTC Loss (avg): 31.4495 WER(5gram): 25.62% (n=256) time: 26.901
2026-01-11 16:37:02,233: WER lens: avg_true_words=5.99 avg_pred_words=6.04 max_pred_words=13
2026-01-11 16:37:02,233: t15.2023.08.13 val PER: 0.1570
2026-01-11 16:37:02,234: t15.2023.08.18 val PER: 0.1475
2026-01-11 16:37:02,234: t15.2023.08.20 val PER: 0.1303
2026-01-11 16:37:02,234: t15.2023.08.25 val PER: 0.1416
2026-01-11 16:37:02,234: t15.2023.08.27 val PER: 0.2235
2026-01-11 16:37:02,234: t15.2023.09.01 val PER: 0.0990
2026-01-11 16:37:02,234: t15.2023.09.03 val PER: 0.1900
2026-01-11 16:37:02,234: t15.2023.09.24 val PER: 0.1517
2026-01-11 16:37:02,234: t15.2023.09.29 val PER: 0.1589
2026-01-11 16:37:02,235: t15.2023.10.01 val PER: 0.2305
2026-01-11 16:37:02,235: t15.2023.10.06 val PER: 0.1421
2026-01-11 16:37:02,235: t15.2023.10.08 val PER: 0.3031
2026-01-11 16:37:02,235: t15.2023.10.13 val PER: 0.2777
2026-01-11 16:37:02,235: t15.2023.10.15 val PER: 0.1898
2026-01-11 16:37:02,235: t15.2023.10.20 val PER: 0.2383
2026-01-11 16:37:02,235: t15.2023.10.22 val PER: 0.1693
2026-01-11 16:37:02,235: t15.2023.11.03 val PER: 0.2157
2026-01-11 16:37:02,235: t15.2023.11.04 val PER: 0.0239
2026-01-11 16:37:02,236: t15.2023.11.17 val PER: 0.0467
2026-01-11 16:37:02,236: t15.2023.11.19 val PER: 0.0579
2026-01-11 16:37:02,236: t15.2023.11.26 val PER: 0.1993
2026-01-11 16:37:02,236: t15.2023.12.03 val PER: 0.1555
2026-01-11 16:37:02,236: t15.2023.12.08 val PER: 0.1445
2026-01-11 16:37:02,236: t15.2023.12.10 val PER: 0.1498
2026-01-11 16:37:02,236: t15.2023.12.17 val PER: 0.1383
2026-01-11 16:37:02,237: t15.2023.12.29 val PER: 0.1730
2026-01-11 16:37:02,237: t15.2024.02.25 val PER: 0.1545
2026-01-11 16:37:02,237: t15.2024.03.08 val PER: 0.2632
2026-01-11 16:37:02,237: t15.2024.03.15 val PER: 0.2283
2026-01-11 16:37:02,237: t15.2024.03.17 val PER: 0.1771
2026-01-11 16:37:02,237: t15.2024.05.10 val PER: 0.1842
2026-01-11 16:37:02,237: t15.2024.06.14 val PER: 0.1830
2026-01-11 16:37:02,237: t15.2024.07.19 val PER: 0.2432
2026-01-11 16:37:02,238: t15.2024.07.21 val PER: 0.1310
2026-01-11 16:37:02,238: t15.2024.07.28 val PER: 0.1853
2026-01-11 16:37:02,238: t15.2025.01.10 val PER: 0.3127
2026-01-11 16:37:02,238: t15.2025.01.12 val PER: 0.1909
2026-01-11 16:37:02,238: t15.2025.03.14 val PER: 0.3225
2026-01-11 16:37:02,238: t15.2025.03.16 val PER: 0.2173
2026-01-11 16:37:02,238: t15.2025.03.30 val PER: 0.2667
2026-01-11 16:37:02,238: t15.2025.04.13 val PER: 0.2382
2026-01-11 16:37:02,239: New best val WER(5gram) 26.79% --> 25.62%
2026-01-11 16:37:02,417: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_29500
2026-01-11 16:37:12,674: Train batch 29600: loss: 20.79 grad norm: 116.35 time: 0.056
2026-01-11 16:37:32,871: Train batch 29800: loss: 19.94 grad norm: 119.31 time: 0.086
2026-01-11 16:37:53,167: Train batch 30000: loss: 20.13 grad norm: 88.85 time: 0.073
2026-01-11 16:37:53,167: Running test after training batch: 30000
2026-01-11 16:37:53,371: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:37:59,597: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:37:59,676: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost gay
2026-01-11 16:38:17,384: Val batch 30000: PER (avg): 0.1871 CTC Loss (avg): 30.8854 WER(5gram): 28.29% (n=256) time: 24.216
2026-01-11 16:38:17,385: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=13
2026-01-11 16:38:17,385: t15.2023.08.13 val PER: 0.1445
2026-01-11 16:38:17,385: t15.2023.08.18 val PER: 0.1500
2026-01-11 16:38:17,385: t15.2023.08.20 val PER: 0.1295
2026-01-11 16:38:17,385: t15.2023.08.25 val PER: 0.1325
2026-01-11 16:38:17,385: t15.2023.08.27 val PER: 0.2170
2026-01-11 16:38:17,385: t15.2023.09.01 val PER: 0.1055
2026-01-11 16:38:17,385: t15.2023.09.03 val PER: 0.1888
2026-01-11 16:38:17,385: t15.2023.09.24 val PER: 0.1566
2026-01-11 16:38:17,385: t15.2023.09.29 val PER: 0.1627
2026-01-11 16:38:17,386: t15.2023.10.01 val PER: 0.2239
2026-01-11 16:38:17,386: t15.2023.10.06 val PER: 0.1421
2026-01-11 16:38:17,386: t15.2023.10.08 val PER: 0.2950
2026-01-11 16:38:17,386: t15.2023.10.13 val PER: 0.2785
2026-01-11 16:38:17,386: t15.2023.10.15 val PER: 0.1918
2026-01-11 16:38:17,386: t15.2023.10.20 val PER: 0.2315
2026-01-11 16:38:17,386: t15.2023.10.22 val PER: 0.1715
2026-01-11 16:38:17,386: t15.2023.11.03 val PER: 0.2198
2026-01-11 16:38:17,387: t15.2023.11.04 val PER: 0.0273
2026-01-11 16:38:17,387: t15.2023.11.17 val PER: 0.0482
2026-01-11 16:38:17,387: t15.2023.11.19 val PER: 0.0639
2026-01-11 16:38:17,387: t15.2023.11.26 val PER: 0.1993
2026-01-11 16:38:17,387: t15.2023.12.03 val PER: 0.1481
2026-01-11 16:38:17,387: t15.2023.12.08 val PER: 0.1485
2026-01-11 16:38:17,387: t15.2023.12.10 val PER: 0.1459
2026-01-11 16:38:17,387: t15.2023.12.17 val PER: 0.1393
2026-01-11 16:38:17,387: t15.2023.12.29 val PER: 0.1730
2026-01-11 16:38:17,387: t15.2024.02.25 val PER: 0.1573
2026-01-11 16:38:17,387: t15.2024.03.08 val PER: 0.2617
2026-01-11 16:38:17,387: t15.2024.03.15 val PER: 0.2283
2026-01-11 16:38:17,388: t15.2024.03.17 val PER: 0.1820
2026-01-11 16:38:17,388: t15.2024.05.10 val PER: 0.1917
2026-01-11 16:38:17,388: t15.2024.06.14 val PER: 0.1814
2026-01-11 16:38:17,388: t15.2024.07.19 val PER: 0.2544
2026-01-11 16:38:17,388: t15.2024.07.21 val PER: 0.1283
2026-01-11 16:38:17,388: t15.2024.07.28 val PER: 0.1882
2026-01-11 16:38:17,388: t15.2025.01.10 val PER: 0.3182
2026-01-11 16:38:17,388: t15.2025.01.12 val PER: 0.1901
2026-01-11 16:38:17,388: t15.2025.03.14 val PER: 0.3254
2026-01-11 16:38:17,388: t15.2025.03.16 val PER: 0.2199
2026-01-11 16:38:17,389: t15.2025.03.30 val PER: 0.2839
2026-01-11 16:38:17,389: t15.2025.04.13 val PER: 0.2397
2026-01-11 16:38:17,563: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_30000
2026-01-11 16:38:38,195: Train batch 30200: loss: 22.43 grad norm: 91.24 time: 0.075
2026-01-11 16:38:58,702: Train batch 30400: loss: 12.23 grad norm: 75.63 time: 0.066
2026-01-11 16:39:09,387: Running test after training batch: 30500
2026-01-11 16:39:09,549: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:39:15,788: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:39:15,863: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost k
2026-01-11 16:39:38,078: Val batch 30500: PER (avg): 0.1875 CTC Loss (avg): 30.8688 WER(5gram): 27.05% (n=256) time: 28.690
2026-01-11 16:39:38,079: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=13
2026-01-11 16:39:38,079: t15.2023.08.13 val PER: 0.1549
2026-01-11 16:39:38,079: t15.2023.08.18 val PER: 0.1467
2026-01-11 16:39:38,080: t15.2023.08.20 val PER: 0.1271
2026-01-11 16:39:38,080: t15.2023.08.25 val PER: 0.1370
2026-01-11 16:39:38,080: t15.2023.08.27 val PER: 0.2251
2026-01-11 16:39:38,081: t15.2023.09.01 val PER: 0.1071
2026-01-11 16:39:38,081: t15.2023.09.03 val PER: 0.1888
2026-01-11 16:39:38,081: t15.2023.09.24 val PER: 0.1481
2026-01-11 16:39:38,081: t15.2023.09.29 val PER: 0.1595
2026-01-11 16:39:38,081: t15.2023.10.01 val PER: 0.2292
2026-01-11 16:39:38,081: t15.2023.10.06 val PER: 0.1346
2026-01-11 16:39:38,081: t15.2023.10.08 val PER: 0.2923
2026-01-11 16:39:38,081: t15.2023.10.13 val PER: 0.2793
2026-01-11 16:39:38,082: t15.2023.10.15 val PER: 0.1925
2026-01-11 16:39:38,082: t15.2023.10.20 val PER: 0.2282
2026-01-11 16:39:38,082: t15.2023.10.22 val PER: 0.1659
2026-01-11 16:39:38,082: t15.2023.11.03 val PER: 0.2212
2026-01-11 16:39:38,082: t15.2023.11.04 val PER: 0.0341
2026-01-11 16:39:38,082: t15.2023.11.17 val PER: 0.0467
2026-01-11 16:39:38,082: t15.2023.11.19 val PER: 0.0619
2026-01-11 16:39:38,082: t15.2023.11.26 val PER: 0.1986
2026-01-11 16:39:38,082: t15.2023.12.03 val PER: 0.1460
2026-01-11 16:39:38,082: t15.2023.12.08 val PER: 0.1405
2026-01-11 16:39:38,082: t15.2023.12.10 val PER: 0.1393
2026-01-11 16:39:38,082: t15.2023.12.17 val PER: 0.1424
2026-01-11 16:39:38,083: t15.2023.12.29 val PER: 0.1716
2026-01-11 16:39:38,083: t15.2024.02.25 val PER: 0.1559
2026-01-11 16:39:38,083: t15.2024.03.08 val PER: 0.2504
2026-01-11 16:39:38,083: t15.2024.03.15 val PER: 0.2370
2026-01-11 16:39:38,083: t15.2024.03.17 val PER: 0.1799
2026-01-11 16:39:38,083: t15.2024.05.10 val PER: 0.1947
2026-01-11 16:39:38,083: t15.2024.06.14 val PER: 0.1877
2026-01-11 16:39:38,083: t15.2024.07.19 val PER: 0.2459
2026-01-11 16:39:38,083: t15.2024.07.21 val PER: 0.1317
2026-01-11 16:39:38,084: t15.2024.07.28 val PER: 0.1897
2026-01-11 16:39:38,084: t15.2025.01.10 val PER: 0.3127
2026-01-11 16:39:38,085: t15.2025.01.12 val PER: 0.1925
2026-01-11 16:39:38,085: t15.2025.03.14 val PER: 0.3314
2026-01-11 16:39:38,085: t15.2025.03.16 val PER: 0.2304
2026-01-11 16:39:38,085: t15.2025.03.30 val PER: 0.3000
2026-01-11 16:39:38,086: t15.2025.04.13 val PER: 0.2525
2026-01-11 16:39:38,259: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_30500
2026-01-11 16:39:48,229: Train batch 30600: loss: 11.80 grad norm: 72.18 time: 0.072
2026-01-11 16:40:08,486: Train batch 30800: loss: 14.43 grad norm: 113.88 time: 0.068
2026-01-11 16:40:28,468: Train batch 31000: loss: 15.81 grad norm: 103.12 time: 0.069
2026-01-11 16:40:28,468: Running test after training batch: 31000
2026-01-11 16:40:28,627: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:40:35,282: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:40:35,376: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost gained
2026-01-11 16:40:55,737: Val batch 31000: PER (avg): 0.1847 CTC Loss (avg): 30.8697 WER(5gram): 26.34% (n=256) time: 27.268
2026-01-11 16:40:55,737: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-11 16:40:55,738: t15.2023.08.13 val PER: 0.1497
2026-01-11 16:40:55,738: t15.2023.08.18 val PER: 0.1517
2026-01-11 16:40:55,738: t15.2023.08.20 val PER: 0.1271
2026-01-11 16:40:55,738: t15.2023.08.25 val PER: 0.1310
2026-01-11 16:40:55,738: t15.2023.08.27 val PER: 0.2203
2026-01-11 16:40:55,739: t15.2023.09.01 val PER: 0.1063
2026-01-11 16:40:55,739: t15.2023.09.03 val PER: 0.1888
2026-01-11 16:40:55,739: t15.2023.09.24 val PER: 0.1505
2026-01-11 16:40:55,739: t15.2023.09.29 val PER: 0.1595
2026-01-11 16:40:55,739: t15.2023.10.01 val PER: 0.2252
2026-01-11 16:40:55,739: t15.2023.10.06 val PER: 0.1324
2026-01-11 16:40:55,739: t15.2023.10.08 val PER: 0.2963
2026-01-11 16:40:55,739: t15.2023.10.13 val PER: 0.2801
2026-01-11 16:40:55,739: t15.2023.10.15 val PER: 0.1925
2026-01-11 16:40:55,739: t15.2023.10.20 val PER: 0.2282
2026-01-11 16:40:55,739: t15.2023.10.22 val PER: 0.1726
2026-01-11 16:40:55,739: t15.2023.11.03 val PER: 0.2171
2026-01-11 16:40:55,739: t15.2023.11.04 val PER: 0.0307
2026-01-11 16:40:55,740: t15.2023.11.17 val PER: 0.0389
2026-01-11 16:40:55,740: t15.2023.11.19 val PER: 0.0599
2026-01-11 16:40:55,740: t15.2023.11.26 val PER: 0.1935
2026-01-11 16:40:55,740: t15.2023.12.03 val PER: 0.1450
2026-01-11 16:40:55,740: t15.2023.12.08 val PER: 0.1431
2026-01-11 16:40:55,741: t15.2023.12.10 val PER: 0.1353
2026-01-11 16:40:55,741: t15.2023.12.17 val PER: 0.1455
2026-01-11 16:40:55,741: t15.2023.12.29 val PER: 0.1675
2026-01-11 16:40:55,741: t15.2024.02.25 val PER: 0.1517
2026-01-11 16:40:55,741: t15.2024.03.08 val PER: 0.2589
2026-01-11 16:40:55,741: t15.2024.03.15 val PER: 0.2289
2026-01-11 16:40:55,741: t15.2024.03.17 val PER: 0.1813
2026-01-11 16:40:55,742: t15.2024.05.10 val PER: 0.1828
2026-01-11 16:40:55,742: t15.2024.06.14 val PER: 0.1767
2026-01-11 16:40:55,742: t15.2024.07.19 val PER: 0.2413
2026-01-11 16:40:55,742: t15.2024.07.21 val PER: 0.1276
2026-01-11 16:40:55,742: t15.2024.07.28 val PER: 0.1860
2026-01-11 16:40:55,742: t15.2025.01.10 val PER: 0.3085
2026-01-11 16:40:55,742: t15.2025.01.12 val PER: 0.1886
2026-01-11 16:40:55,742: t15.2025.03.14 val PER: 0.3314
2026-01-11 16:40:55,742: t15.2025.03.16 val PER: 0.2120
2026-01-11 16:40:55,742: t15.2025.03.30 val PER: 0.2851
2026-01-11 16:40:55,742: t15.2025.04.13 val PER: 0.2340
2026-01-11 16:40:55,918: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_31000
2026-01-11 16:41:16,548: Train batch 31200: loss: 13.58 grad norm: 81.03 time: 0.079
2026-01-11 16:41:36,986: Train batch 31400: loss: 12.34 grad norm: 87.05 time: 0.063
2026-01-11 16:41:47,228: Running test after training batch: 31500
2026-01-11 16:41:47,341: WER debug GT example: You can see the code at this point as well.
2026-01-11 16:41:54,842: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point it will
2026-01-11 16:41:54,926: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost k
2026-01-11 16:42:14,604: Val batch 31500: PER (avg): 0.1834 CTC Loss (avg): 30.8742 WER(5gram): 26.21% (n=256) time: 27.375
2026-01-11 16:42:14,605: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=13
2026-01-11 16:42:14,605: t15.2023.08.13 val PER: 0.1486
2026-01-11 16:42:14,605: t15.2023.08.18 val PER: 0.1509
2026-01-11 16:42:14,605: t15.2023.08.20 val PER: 0.1183
2026-01-11 16:42:14,605: t15.2023.08.25 val PER: 0.1370
2026-01-11 16:42:14,605: t15.2023.08.27 val PER: 0.2186
2026-01-11 16:42:14,605: t15.2023.09.01 val PER: 0.1047
2026-01-11 16:42:14,605: t15.2023.09.03 val PER: 0.1876
2026-01-11 16:42:14,605: t15.2023.09.24 val PER: 0.1420
2026-01-11 16:42:14,605: t15.2023.09.29 val PER: 0.1589
2026-01-11 16:42:14,606: t15.2023.10.01 val PER: 0.2232
2026-01-11 16:42:14,606: t15.2023.10.06 val PER: 0.1324
2026-01-11 16:42:14,606: t15.2023.10.08 val PER: 0.2936
2026-01-11 16:42:14,606: t15.2023.10.13 val PER: 0.2638
2026-01-11 16:42:14,606: t15.2023.10.15 val PER: 0.1991
2026-01-11 16:42:14,606: t15.2023.10.20 val PER: 0.2282
2026-01-11 16:42:14,606: t15.2023.10.22 val PER: 0.1693
2026-01-11 16:42:14,607: t15.2023.11.03 val PER: 0.2171
2026-01-11 16:42:14,607: t15.2023.11.04 val PER: 0.0273
2026-01-11 16:42:14,607: t15.2023.11.17 val PER: 0.0420
2026-01-11 16:42:14,607: t15.2023.11.19 val PER: 0.0539
2026-01-11 16:42:14,607: t15.2023.11.26 val PER: 0.1862
2026-01-11 16:42:14,607: t15.2023.12.03 val PER: 0.1523
2026-01-11 16:42:14,608: t15.2023.12.08 val PER: 0.1418
2026-01-11 16:42:14,608: t15.2023.12.10 val PER: 0.1353
2026-01-11 16:42:14,608: t15.2023.12.17 val PER: 0.1455
2026-01-11 16:42:14,608: t15.2023.12.29 val PER: 0.1613
2026-01-11 16:42:14,608: t15.2024.02.25 val PER: 0.1475
2026-01-11 16:42:14,609: t15.2024.03.08 val PER: 0.2489
2026-01-11 16:42:14,609: t15.2024.03.15 val PER: 0.2301
2026-01-11 16:42:14,610: t15.2024.03.17 val PER: 0.1757
2026-01-11 16:42:14,610: t15.2024.05.10 val PER: 0.1842
2026-01-11 16:42:14,610: t15.2024.06.14 val PER: 0.1814
2026-01-11 16:42:14,610: t15.2024.07.19 val PER: 0.2439
2026-01-11 16:42:14,610: t15.2024.07.21 val PER: 0.1255
2026-01-11 16:42:14,610: t15.2024.07.28 val PER: 0.1897
2026-01-11 16:42:14,610: t15.2025.01.10 val PER: 0.3085
2026-01-11 16:42:14,610: t15.2025.01.12 val PER: 0.1932
2026-01-11 16:42:14,610: t15.2025.03.14 val PER: 0.3284
2026-01-11 16:42:14,610: t15.2025.03.16 val PER: 0.2016
2026-01-11 16:42:14,611: t15.2025.03.30 val PER: 0.2920
2026-01-11 16:42:14,611: t15.2025.04.13 val PER: 0.2439
2026-01-11 16:42:14,780: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_40k/checkpoint/checkpoint_batch_31500
