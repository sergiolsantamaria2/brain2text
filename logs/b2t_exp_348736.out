TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_348736
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_348736/torch_extensions
WANDB_DIR=/tmp/e12511253_b2t_348736/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/e12511253_b2t_348736/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  4 12:11 /tmp/e12511253_b2t_348736/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
trained_models -> /tmp/e12511253_b2t_348736/trained_models
OUT_ROOT=/tmp/e12511253_b2t_348736/trained_models
==============================================
Job: b2t_exp  ID: 348736
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/weight_decay/lr40
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/weight_decay/lr40 ==========
Num configs: 4

=== RUN wd1e-3.yaml ===
2026-01-04 12:11:52,020: Using device: cuda:0
2026-01-04 12:11:53,792: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-04 12:11:53,816: Using 45 sessions after filtering (from 45).
2026-01-04 12:11:55,944: Using torch.compile (if available)
2026-01-04 12:11:55,945: torch.compile not available (torch<2.0). Skipping.
2026-01-04 12:11:55,945: Initialized RNN decoding model
2026-01-04 12:11:55,945: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-04 12:11:55,946: Model has 44,907,305 parameters
2026-01-04 12:11:55,946: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-04 12:11:57,313: Successfully initialized datasets
2026-01-04 12:11:57,314: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-04 12:11:58,482: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.217
2026-01-04 12:11:58,482: Running test after training batch: 0
2026-01-04 12:11:58,606: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:12:04,802: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-04 12:12:05,933: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-04 12:13:03,203: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 64.720
2026-01-04 12:13:03,203: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-04 12:13:03,204: t15.2023.08.13 val PER: 1.3056
2026-01-04 12:13:03,204: t15.2023.08.18 val PER: 1.4208
2026-01-04 12:13:03,204: t15.2023.08.20 val PER: 1.3002
2026-01-04 12:13:03,204: t15.2023.08.25 val PER: 1.3389
2026-01-04 12:13:03,204: t15.2023.08.27 val PER: 1.2460
2026-01-04 12:13:03,204: t15.2023.09.01 val PER: 1.4537
2026-01-04 12:13:03,204: t15.2023.09.03 val PER: 1.3171
2026-01-04 12:13:03,205: t15.2023.09.24 val PER: 1.5461
2026-01-04 12:13:03,205: t15.2023.09.29 val PER: 1.4671
2026-01-04 12:13:03,205: t15.2023.10.01 val PER: 1.2147
2026-01-04 12:13:03,205: t15.2023.10.06 val PER: 1.4876
2026-01-04 12:13:03,205: t15.2023.10.08 val PER: 1.1827
2026-01-04 12:13:03,205: t15.2023.10.13 val PER: 1.3964
2026-01-04 12:13:03,205: t15.2023.10.15 val PER: 1.3889
2026-01-04 12:13:03,205: t15.2023.10.20 val PER: 1.4866
2026-01-04 12:13:03,205: t15.2023.10.22 val PER: 1.3942
2026-01-04 12:13:03,205: t15.2023.11.03 val PER: 1.5923
2026-01-04 12:13:03,205: t15.2023.11.04 val PER: 2.0171
2026-01-04 12:13:03,205: t15.2023.11.17 val PER: 1.9518
2026-01-04 12:13:03,205: t15.2023.11.19 val PER: 1.6707
2026-01-04 12:13:03,206: t15.2023.11.26 val PER: 1.5413
2026-01-04 12:13:03,206: t15.2023.12.03 val PER: 1.4254
2026-01-04 12:13:03,206: t15.2023.12.08 val PER: 1.4487
2026-01-04 12:13:03,206: t15.2023.12.10 val PER: 1.6899
2026-01-04 12:13:03,206: t15.2023.12.17 val PER: 1.3077
2026-01-04 12:13:03,206: t15.2023.12.29 val PER: 1.4063
2026-01-04 12:13:03,206: t15.2024.02.25 val PER: 1.4228
2026-01-04 12:13:03,206: t15.2024.03.08 val PER: 1.3257
2026-01-04 12:13:03,206: t15.2024.03.15 val PER: 1.3196
2026-01-04 12:13:03,206: t15.2024.03.17 val PER: 1.4052
2026-01-04 12:13:03,206: t15.2024.05.10 val PER: 1.3224
2026-01-04 12:13:03,206: t15.2024.06.14 val PER: 1.5315
2026-01-04 12:13:03,206: t15.2024.07.19 val PER: 1.0817
2026-01-04 12:13:03,206: t15.2024.07.21 val PER: 1.6290
2026-01-04 12:13:03,206: t15.2024.07.28 val PER: 1.6588
2026-01-04 12:13:03,207: t15.2025.01.10 val PER: 1.0923
2026-01-04 12:13:03,207: t15.2025.01.12 val PER: 1.7629
2026-01-04 12:13:03,207: t15.2025.03.14 val PER: 1.0414
2026-01-04 12:13:03,207: t15.2025.03.16 val PER: 1.6257
2026-01-04 12:13:03,207: t15.2025.03.30 val PER: 1.2874
2026-01-04 12:13:03,207: t15.2025.04.13 val PER: 1.5949
2026-01-04 12:13:03,208: New best val WER(1gram) inf% --> 100.00%
2026-01-04 12:13:03,209: Checkpointing model
2026-01-04 12:13:03,496: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:13:03,794: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_0
2026-01-04 12:13:24,079: Train batch 200: loss: 77.59 grad norm: 105.99 time: 0.054
2026-01-04 12:13:44,251: Train batch 400: loss: 53.46 grad norm: 86.06 time: 0.066
2026-01-04 12:13:54,350: Running test after training batch: 500
2026-01-04 12:13:54,459: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:13:59,875: WER debug example
  GT : you can see the code at this point as well
  PR : used aunt ease thus uhde at this ide is aisle
2026-01-04 12:13:59,937: WER debug example
  GT : how does it keep the cost down
  PR : houde oz it ink thus as adz
2026-01-04 12:14:03,845: Val batch 500: PER (avg): 0.5204 CTC Loss (avg): 55.2239 WER(1gram): 89.85% (n=64) time: 9.494
2026-01-04 12:14:03,845: WER lens: avg_true_words=6.16 avg_pred_words=5.70 max_pred_words=12
2026-01-04 12:14:03,846: t15.2023.08.13 val PER: 0.4657
2026-01-04 12:14:03,846: t15.2023.08.18 val PER: 0.4535
2026-01-04 12:14:03,846: t15.2023.08.20 val PER: 0.4424
2026-01-04 12:14:03,846: t15.2023.08.25 val PER: 0.4352
2026-01-04 12:14:03,846: t15.2023.08.27 val PER: 0.5257
2026-01-04 12:14:03,846: t15.2023.09.01 val PER: 0.4205
2026-01-04 12:14:03,846: t15.2023.09.03 val PER: 0.4988
2026-01-04 12:14:03,846: t15.2023.09.24 val PER: 0.4393
2026-01-04 12:14:03,846: t15.2023.09.29 val PER: 0.4729
2026-01-04 12:14:03,846: t15.2023.10.01 val PER: 0.5165
2026-01-04 12:14:03,846: t15.2023.10.06 val PER: 0.4360
2026-01-04 12:14:03,847: t15.2023.10.08 val PER: 0.5304
2026-01-04 12:14:03,847: t15.2023.10.13 val PER: 0.5617
2026-01-04 12:14:03,847: t15.2023.10.15 val PER: 0.4944
2026-01-04 12:14:03,847: t15.2023.10.20 val PER: 0.4597
2026-01-04 12:14:03,847: t15.2023.10.22 val PER: 0.4633
2026-01-04 12:14:03,847: t15.2023.11.03 val PER: 0.5122
2026-01-04 12:14:03,847: t15.2023.11.04 val PER: 0.2662
2026-01-04 12:14:03,847: t15.2023.11.17 val PER: 0.3655
2026-01-04 12:14:03,847: t15.2023.11.19 val PER: 0.3393
2026-01-04 12:14:03,847: t15.2023.11.26 val PER: 0.5616
2026-01-04 12:14:03,847: t15.2023.12.03 val PER: 0.4947
2026-01-04 12:14:03,847: t15.2023.12.08 val PER: 0.5186
2026-01-04 12:14:03,848: t15.2023.12.10 val PER: 0.4507
2026-01-04 12:14:03,848: t15.2023.12.17 val PER: 0.5676
2026-01-04 12:14:03,848: t15.2023.12.29 val PER: 0.5360
2026-01-04 12:14:03,848: t15.2024.02.25 val PER: 0.4860
2026-01-04 12:14:03,848: t15.2024.03.08 val PER: 0.6202
2026-01-04 12:14:03,848: t15.2024.03.15 val PER: 0.5572
2026-01-04 12:14:03,848: t15.2024.03.17 val PER: 0.5021
2026-01-04 12:14:03,848: t15.2024.05.10 val PER: 0.5364
2026-01-04 12:14:03,849: t15.2024.06.14 val PER: 0.5221
2026-01-04 12:14:03,849: t15.2024.07.19 val PER: 0.6823
2026-01-04 12:14:03,849: t15.2024.07.21 val PER: 0.4883
2026-01-04 12:14:03,849: t15.2024.07.28 val PER: 0.5110
2026-01-04 12:14:03,849: t15.2025.01.10 val PER: 0.7424
2026-01-04 12:14:03,849: t15.2025.01.12 val PER: 0.5612
2026-01-04 12:14:03,849: t15.2025.03.14 val PER: 0.7574
2026-01-04 12:14:03,849: t15.2025.03.16 val PER: 0.5903
2026-01-04 12:14:03,849: t15.2025.03.30 val PER: 0.7310
2026-01-04 12:14:03,849: t15.2025.04.13 val PER: 0.5835
2026-01-04 12:14:03,851: New best val WER(1gram) 100.00% --> 89.85%
2026-01-04 12:14:03,851: Checkpointing model
2026-01-04 12:14:04,484: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:14:04,796: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_500
2026-01-04 12:14:14,314: Train batch 600: loss: 48.95 grad norm: 83.84 time: 0.079
2026-01-04 12:14:32,936: Train batch 800: loss: 40.66 grad norm: 84.48 time: 0.058
2026-01-04 12:14:51,793: Train batch 1000: loss: 42.67 grad norm: 81.74 time: 0.066
2026-01-04 12:14:51,794: Running test after training batch: 1000
2026-01-04 12:14:51,931: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:14:57,352: WER debug example
  GT : you can see the code at this point as well
  PR : yield ent ease thus good it this boyde is while
2026-01-04 12:14:57,411: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke that wass it
2026-01-04 12:15:00,557: Val batch 1000: PER (avg): 0.4102 CTC Loss (avg): 42.5113 WER(1gram): 82.49% (n=64) time: 8.763
2026-01-04 12:15:00,558: WER lens: avg_true_words=6.16 avg_pred_words=5.59 max_pred_words=12
2026-01-04 12:15:00,558: t15.2023.08.13 val PER: 0.3836
2026-01-04 12:15:00,558: t15.2023.08.18 val PER: 0.3294
2026-01-04 12:15:00,559: t15.2023.08.20 val PER: 0.3527
2026-01-04 12:15:00,559: t15.2023.08.25 val PER: 0.2937
2026-01-04 12:15:00,559: t15.2023.08.27 val PER: 0.4228
2026-01-04 12:15:00,559: t15.2023.09.01 val PER: 0.2979
2026-01-04 12:15:00,559: t15.2023.09.03 val PER: 0.3943
2026-01-04 12:15:00,559: t15.2023.09.24 val PER: 0.3277
2026-01-04 12:15:00,559: t15.2023.09.29 val PER: 0.3644
2026-01-04 12:15:00,559: t15.2023.10.01 val PER: 0.4016
2026-01-04 12:15:00,559: t15.2023.10.06 val PER: 0.3143
2026-01-04 12:15:00,559: t15.2023.10.08 val PER: 0.4547
2026-01-04 12:15:00,560: t15.2023.10.13 val PER: 0.4670
2026-01-04 12:15:00,560: t15.2023.10.15 val PER: 0.3823
2026-01-04 12:15:00,560: t15.2023.10.20 val PER: 0.3624
2026-01-04 12:15:00,560: t15.2023.10.22 val PER: 0.3597
2026-01-04 12:15:00,560: t15.2023.11.03 val PER: 0.4009
2026-01-04 12:15:00,560: t15.2023.11.04 val PER: 0.1468
2026-01-04 12:15:00,560: t15.2023.11.17 val PER: 0.2722
2026-01-04 12:15:00,561: t15.2023.11.19 val PER: 0.2116
2026-01-04 12:15:00,561: t15.2023.11.26 val PER: 0.4464
2026-01-04 12:15:00,561: t15.2023.12.03 val PER: 0.4097
2026-01-04 12:15:00,561: t15.2023.12.08 val PER: 0.4128
2026-01-04 12:15:00,561: t15.2023.12.10 val PER: 0.3548
2026-01-04 12:15:00,561: t15.2023.12.17 val PER: 0.3992
2026-01-04 12:15:00,561: t15.2023.12.29 val PER: 0.4056
2026-01-04 12:15:00,561: t15.2024.02.25 val PER: 0.3624
2026-01-04 12:15:00,561: t15.2024.03.08 val PER: 0.4993
2026-01-04 12:15:00,561: t15.2024.03.15 val PER: 0.4478
2026-01-04 12:15:00,561: t15.2024.03.17 val PER: 0.4045
2026-01-04 12:15:00,561: t15.2024.05.10 val PER: 0.4368
2026-01-04 12:15:00,561: t15.2024.06.14 val PER: 0.4054
2026-01-04 12:15:00,561: t15.2024.07.19 val PER: 0.5359
2026-01-04 12:15:00,562: t15.2024.07.21 val PER: 0.3738
2026-01-04 12:15:00,562: t15.2024.07.28 val PER: 0.4162
2026-01-04 12:15:00,562: t15.2025.01.10 val PER: 0.6253
2026-01-04 12:15:00,562: t15.2025.01.12 val PER: 0.4627
2026-01-04 12:15:00,562: t15.2025.03.14 val PER: 0.6243
2026-01-04 12:15:00,562: t15.2025.03.16 val PER: 0.4921
2026-01-04 12:15:00,562: t15.2025.03.30 val PER: 0.6552
2026-01-04 12:15:00,562: t15.2025.04.13 val PER: 0.4879
2026-01-04 12:15:00,563: New best val WER(1gram) 89.85% --> 82.49%
2026-01-04 12:15:00,563: Checkpointing model
2026-01-04 12:15:01,227: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:15:01,525: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_1000
2026-01-04 12:15:21,522: Train batch 1200: loss: 33.76 grad norm: 76.10 time: 0.068
2026-01-04 12:15:42,167: Train batch 1400: loss: 36.26 grad norm: 82.06 time: 0.063
2026-01-04 12:15:52,735: Running test after training batch: 1500
2026-01-04 12:15:53,144: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:15:58,531: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt sze the good at this boyde is will
2026-01-04 12:15:58,585: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that os it
2026-01-04 12:16:01,185: Val batch 1500: PER (avg): 0.3797 CTC Loss (avg): 37.1912 WER(1gram): 75.38% (n=64) time: 8.449
2026-01-04 12:16:01,185: WER lens: avg_true_words=6.16 avg_pred_words=5.06 max_pred_words=11
2026-01-04 12:16:01,186: t15.2023.08.13 val PER: 0.3462
2026-01-04 12:16:01,186: t15.2023.08.18 val PER: 0.3135
2026-01-04 12:16:01,186: t15.2023.08.20 val PER: 0.3106
2026-01-04 12:16:01,186: t15.2023.08.25 val PER: 0.2605
2026-01-04 12:16:01,186: t15.2023.08.27 val PER: 0.3923
2026-01-04 12:16:01,186: t15.2023.09.01 val PER: 0.2792
2026-01-04 12:16:01,186: t15.2023.09.03 val PER: 0.3753
2026-01-04 12:16:01,186: t15.2023.09.24 val PER: 0.3046
2026-01-04 12:16:01,187: t15.2023.09.29 val PER: 0.3331
2026-01-04 12:16:01,187: t15.2023.10.01 val PER: 0.3930
2026-01-04 12:16:01,187: t15.2023.10.06 val PER: 0.2906
2026-01-04 12:16:01,187: t15.2023.10.08 val PER: 0.4330
2026-01-04 12:16:01,187: t15.2023.10.13 val PER: 0.4360
2026-01-04 12:16:01,187: t15.2023.10.15 val PER: 0.3606
2026-01-04 12:16:01,187: t15.2023.10.20 val PER: 0.3356
2026-01-04 12:16:01,188: t15.2023.10.22 val PER: 0.3196
2026-01-04 12:16:01,188: t15.2023.11.03 val PER: 0.3609
2026-01-04 12:16:01,188: t15.2023.11.04 val PER: 0.1092
2026-01-04 12:16:01,188: t15.2023.11.17 val PER: 0.2240
2026-01-04 12:16:01,188: t15.2023.11.19 val PER: 0.1796
2026-01-04 12:16:01,188: t15.2023.11.26 val PER: 0.4138
2026-01-04 12:16:01,188: t15.2023.12.03 val PER: 0.3708
2026-01-04 12:16:01,188: t15.2023.12.08 val PER: 0.3549
2026-01-04 12:16:01,188: t15.2023.12.10 val PER: 0.2917
2026-01-04 12:16:01,188: t15.2023.12.17 val PER: 0.3773
2026-01-04 12:16:01,188: t15.2023.12.29 val PER: 0.3802
2026-01-04 12:16:01,188: t15.2024.02.25 val PER: 0.3188
2026-01-04 12:16:01,188: t15.2024.03.08 val PER: 0.4495
2026-01-04 12:16:01,189: t15.2024.03.15 val PER: 0.4184
2026-01-04 12:16:01,189: t15.2024.03.17 val PER: 0.3759
2026-01-04 12:16:01,189: t15.2024.05.10 val PER: 0.3923
2026-01-04 12:16:01,189: t15.2024.06.14 val PER: 0.4006
2026-01-04 12:16:01,189: t15.2024.07.19 val PER: 0.5135
2026-01-04 12:16:01,189: t15.2024.07.21 val PER: 0.3524
2026-01-04 12:16:01,189: t15.2024.07.28 val PER: 0.3647
2026-01-04 12:16:01,189: t15.2025.01.10 val PER: 0.6171
2026-01-04 12:16:01,189: t15.2025.01.12 val PER: 0.4211
2026-01-04 12:16:01,189: t15.2025.03.14 val PER: 0.6065
2026-01-04 12:16:01,189: t15.2025.03.16 val PER: 0.4437
2026-01-04 12:16:01,189: t15.2025.03.30 val PER: 0.6391
2026-01-04 12:16:01,190: t15.2025.04.13 val PER: 0.4807
2026-01-04 12:16:01,191: New best val WER(1gram) 82.49% --> 75.38%
2026-01-04 12:16:01,191: Checkpointing model
2026-01-04 12:16:01,831: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:16:02,126: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_1500
2026-01-04 12:16:11,841: Train batch 1600: loss: 36.82 grad norm: 80.42 time: 0.063
2026-01-04 12:16:32,011: Train batch 1800: loss: 35.44 grad norm: 71.46 time: 0.090
2026-01-04 12:16:53,081: Train batch 2000: loss: 33.69 grad norm: 74.07 time: 0.069
2026-01-04 12:16:53,081: Running test after training batch: 2000
2026-01-04 12:16:53,219: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:16:58,588: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt e the code at this bonde is will
2026-01-04 12:16:58,640: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap thus las id
2026-01-04 12:17:01,236: Val batch 2000: PER (avg): 0.3250 CTC Loss (avg): 32.7422 WER(1gram): 71.32% (n=64) time: 8.154
2026-01-04 12:17:01,236: WER lens: avg_true_words=6.16 avg_pred_words=5.64 max_pred_words=11
2026-01-04 12:17:01,236: t15.2023.08.13 val PER: 0.2931
2026-01-04 12:17:01,237: t15.2023.08.18 val PER: 0.2565
2026-01-04 12:17:01,237: t15.2023.08.20 val PER: 0.2550
2026-01-04 12:17:01,237: t15.2023.08.25 val PER: 0.2289
2026-01-04 12:17:01,237: t15.2023.08.27 val PER: 0.3408
2026-01-04 12:17:01,237: t15.2023.09.01 val PER: 0.2265
2026-01-04 12:17:01,237: t15.2023.09.03 val PER: 0.3290
2026-01-04 12:17:01,237: t15.2023.09.24 val PER: 0.2500
2026-01-04 12:17:01,237: t15.2023.09.29 val PER: 0.2814
2026-01-04 12:17:01,237: t15.2023.10.01 val PER: 0.3217
2026-01-04 12:17:01,237: t15.2023.10.06 val PER: 0.2443
2026-01-04 12:17:01,237: t15.2023.10.08 val PER: 0.3870
2026-01-04 12:17:01,238: t15.2023.10.13 val PER: 0.3778
2026-01-04 12:17:01,238: t15.2023.10.15 val PER: 0.3026
2026-01-04 12:17:01,238: t15.2023.10.20 val PER: 0.2987
2026-01-04 12:17:01,238: t15.2023.10.22 val PER: 0.2483
2026-01-04 12:17:01,238: t15.2023.11.03 val PER: 0.3256
2026-01-04 12:17:01,238: t15.2023.11.04 val PER: 0.0922
2026-01-04 12:17:01,238: t15.2023.11.17 val PER: 0.1773
2026-01-04 12:17:01,238: t15.2023.11.19 val PER: 0.1257
2026-01-04 12:17:01,238: t15.2023.11.26 val PER: 0.3630
2026-01-04 12:17:01,238: t15.2023.12.03 val PER: 0.3109
2026-01-04 12:17:01,238: t15.2023.12.08 val PER: 0.3083
2026-01-04 12:17:01,238: t15.2023.12.10 val PER: 0.2562
2026-01-04 12:17:01,239: t15.2023.12.17 val PER: 0.3181
2026-01-04 12:17:01,239: t15.2023.12.29 val PER: 0.3075
2026-01-04 12:17:01,239: t15.2024.02.25 val PER: 0.2640
2026-01-04 12:17:01,239: t15.2024.03.08 val PER: 0.3969
2026-01-04 12:17:01,239: t15.2024.03.15 val PER: 0.3640
2026-01-04 12:17:01,239: t15.2024.03.17 val PER: 0.3368
2026-01-04 12:17:01,239: t15.2024.05.10 val PER: 0.3358
2026-01-04 12:17:01,239: t15.2024.06.14 val PER: 0.3549
2026-01-04 12:17:01,239: t15.2024.07.19 val PER: 0.4588
2026-01-04 12:17:01,239: t15.2024.07.21 val PER: 0.2869
2026-01-04 12:17:01,239: t15.2024.07.28 val PER: 0.3235
2026-01-04 12:17:01,239: t15.2025.01.10 val PER: 0.5372
2026-01-04 12:17:01,240: t15.2025.01.12 val PER: 0.3757
2026-01-04 12:17:01,240: t15.2025.03.14 val PER: 0.5414
2026-01-04 12:17:01,240: t15.2025.03.16 val PER: 0.4018
2026-01-04 12:17:01,240: t15.2025.03.30 val PER: 0.5264
2026-01-04 12:17:01,240: t15.2025.04.13 val PER: 0.4037
2026-01-04 12:17:01,241: New best val WER(1gram) 75.38% --> 71.32%
2026-01-04 12:17:01,241: Checkpointing model
2026-01-04 12:17:01,915: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:17:02,214: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_2000
2026-01-04 12:17:20,682: Train batch 2200: loss: 28.84 grad norm: 72.99 time: 0.061
2026-01-04 12:17:39,441: Train batch 2400: loss: 29.30 grad norm: 66.89 time: 0.053
2026-01-04 12:17:49,248: Running test after training batch: 2500
2026-01-04 12:17:49,388: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:17:54,975: WER debug example
  GT : you can see the code at this point as well
  PR : yule end e the could at this point is will
2026-01-04 12:17:55,026: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the us it
2026-01-04 12:17:57,869: Val batch 2500: PER (avg): 0.3016 CTC Loss (avg): 30.0194 WER(1gram): 69.04% (n=64) time: 8.620
2026-01-04 12:17:57,869: WER lens: avg_true_words=6.16 avg_pred_words=5.62 max_pred_words=11
2026-01-04 12:17:57,870: t15.2023.08.13 val PER: 0.2921
2026-01-04 12:17:57,870: t15.2023.08.18 val PER: 0.2381
2026-01-04 12:17:57,870: t15.2023.08.20 val PER: 0.2288
2026-01-04 12:17:57,870: t15.2023.08.25 val PER: 0.2063
2026-01-04 12:17:57,870: t15.2023.08.27 val PER: 0.3183
2026-01-04 12:17:57,870: t15.2023.09.01 val PER: 0.2078
2026-01-04 12:17:57,871: t15.2023.09.03 val PER: 0.2957
2026-01-04 12:17:57,871: t15.2023.09.24 val PER: 0.2245
2026-01-04 12:17:57,871: t15.2023.09.29 val PER: 0.2559
2026-01-04 12:17:57,871: t15.2023.10.01 val PER: 0.3025
2026-01-04 12:17:57,871: t15.2023.10.06 val PER: 0.2099
2026-01-04 12:17:57,871: t15.2023.10.08 val PER: 0.3816
2026-01-04 12:17:57,871: t15.2023.10.13 val PER: 0.3530
2026-01-04 12:17:57,871: t15.2023.10.15 val PER: 0.2828
2026-01-04 12:17:57,871: t15.2023.10.20 val PER: 0.2752
2026-01-04 12:17:57,871: t15.2023.10.22 val PER: 0.2327
2026-01-04 12:17:57,872: t15.2023.11.03 val PER: 0.2965
2026-01-04 12:17:57,872: t15.2023.11.04 val PER: 0.0887
2026-01-04 12:17:57,872: t15.2023.11.17 val PER: 0.1509
2026-01-04 12:17:57,872: t15.2023.11.19 val PER: 0.1257
2026-01-04 12:17:57,872: t15.2023.11.26 val PER: 0.3370
2026-01-04 12:17:57,872: t15.2023.12.03 val PER: 0.2878
2026-01-04 12:17:57,872: t15.2023.12.08 val PER: 0.2750
2026-01-04 12:17:57,872: t15.2023.12.10 val PER: 0.2431
2026-01-04 12:17:57,872: t15.2023.12.17 val PER: 0.2942
2026-01-04 12:17:57,872: t15.2023.12.29 val PER: 0.2992
2026-01-04 12:17:57,872: t15.2024.02.25 val PER: 0.2346
2026-01-04 12:17:57,872: t15.2024.03.08 val PER: 0.3684
2026-01-04 12:17:57,872: t15.2024.03.15 val PER: 0.3483
2026-01-04 12:17:57,873: t15.2024.03.17 val PER: 0.3020
2026-01-04 12:17:57,873: t15.2024.05.10 val PER: 0.2987
2026-01-04 12:17:57,873: t15.2024.06.14 val PER: 0.3202
2026-01-04 12:17:57,873: t15.2024.07.19 val PER: 0.4370
2026-01-04 12:17:57,873: t15.2024.07.21 val PER: 0.2600
2026-01-04 12:17:57,873: t15.2024.07.28 val PER: 0.3022
2026-01-04 12:17:57,873: t15.2025.01.10 val PER: 0.5028
2026-01-04 12:17:57,873: t15.2025.01.12 val PER: 0.3587
2026-01-04 12:17:57,873: t15.2025.03.14 val PER: 0.4822
2026-01-04 12:17:57,873: t15.2025.03.16 val PER: 0.3639
2026-01-04 12:17:57,873: t15.2025.03.30 val PER: 0.5023
2026-01-04 12:17:57,873: t15.2025.04.13 val PER: 0.3880
2026-01-04 12:17:57,875: New best val WER(1gram) 71.32% --> 69.04%
2026-01-04 12:17:57,875: Checkpointing model
2026-01-04 12:17:58,525: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:17:58,825: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_2500
2026-01-04 12:18:09,101: Train batch 2600: loss: 35.34 grad norm: 85.61 time: 0.057
2026-01-04 12:18:30,112: Train batch 2800: loss: 25.75 grad norm: 71.53 time: 0.087
2026-01-04 12:18:50,968: Train batch 3000: loss: 31.19 grad norm: 74.73 time: 0.087
2026-01-04 12:18:50,968: Running test after training batch: 3000
2026-01-04 12:18:51,117: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:18:56,324: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e the code at this point is will
2026-01-04 12:18:56,358: WER debug example
  GT : how does it keep the cost down
  PR : houde does it kipp the cost et
2026-01-04 12:18:58,192: Val batch 3000: PER (avg): 0.2805 CTC Loss (avg): 27.6431 WER(1gram): 65.48% (n=64) time: 7.223
2026-01-04 12:18:58,193: WER lens: avg_true_words=6.16 avg_pred_words=5.77 max_pred_words=11
2026-01-04 12:18:58,193: t15.2023.08.13 val PER: 0.2557
2026-01-04 12:18:58,193: t15.2023.08.18 val PER: 0.2221
2026-01-04 12:18:58,193: t15.2023.08.20 val PER: 0.2160
2026-01-04 12:18:58,193: t15.2023.08.25 val PER: 0.1973
2026-01-04 12:18:58,193: t15.2023.08.27 val PER: 0.3039
2026-01-04 12:18:58,193: t15.2023.09.01 val PER: 0.1891
2026-01-04 12:18:58,193: t15.2023.09.03 val PER: 0.2827
2026-01-04 12:18:58,193: t15.2023.09.24 val PER: 0.2100
2026-01-04 12:18:58,194: t15.2023.09.29 val PER: 0.2419
2026-01-04 12:18:58,194: t15.2023.10.01 val PER: 0.2880
2026-01-04 12:18:58,194: t15.2023.10.06 val PER: 0.1862
2026-01-04 12:18:58,194: t15.2023.10.08 val PER: 0.3491
2026-01-04 12:18:58,194: t15.2023.10.13 val PER: 0.3452
2026-01-04 12:18:58,194: t15.2023.10.15 val PER: 0.2749
2026-01-04 12:18:58,194: t15.2023.10.20 val PER: 0.2752
2026-01-04 12:18:58,195: t15.2023.10.22 val PER: 0.2160
2026-01-04 12:18:58,195: t15.2023.11.03 val PER: 0.2727
2026-01-04 12:18:58,195: t15.2023.11.04 val PER: 0.0751
2026-01-04 12:18:58,195: t15.2023.11.17 val PER: 0.1384
2026-01-04 12:18:58,195: t15.2023.11.19 val PER: 0.1118
2026-01-04 12:18:58,195: t15.2023.11.26 val PER: 0.2993
2026-01-04 12:18:58,196: t15.2023.12.03 val PER: 0.2542
2026-01-04 12:18:58,196: t15.2023.12.08 val PER: 0.2530
2026-01-04 12:18:58,196: t15.2023.12.10 val PER: 0.2208
2026-01-04 12:18:58,196: t15.2023.12.17 val PER: 0.2734
2026-01-04 12:18:58,196: t15.2023.12.29 val PER: 0.2773
2026-01-04 12:18:58,196: t15.2024.02.25 val PER: 0.2317
2026-01-04 12:18:58,196: t15.2024.03.08 val PER: 0.3585
2026-01-04 12:18:58,196: t15.2024.03.15 val PER: 0.3358
2026-01-04 12:18:58,197: t15.2024.03.17 val PER: 0.2880
2026-01-04 12:18:58,197: t15.2024.05.10 val PER: 0.2942
2026-01-04 12:18:58,197: t15.2024.06.14 val PER: 0.2950
2026-01-04 12:18:58,197: t15.2024.07.19 val PER: 0.3962
2026-01-04 12:18:58,197: t15.2024.07.21 val PER: 0.2310
2026-01-04 12:18:58,197: t15.2024.07.28 val PER: 0.2838
2026-01-04 12:18:58,197: t15.2025.01.10 val PER: 0.4931
2026-01-04 12:18:58,197: t15.2025.01.12 val PER: 0.3172
2026-01-04 12:18:58,197: t15.2025.03.14 val PER: 0.4349
2026-01-04 12:18:58,197: t15.2025.03.16 val PER: 0.3272
2026-01-04 12:18:58,197: t15.2025.03.30 val PER: 0.4759
2026-01-04 12:18:58,197: t15.2025.04.13 val PER: 0.3409
2026-01-04 12:18:58,199: New best val WER(1gram) 69.04% --> 65.48%
2026-01-04 12:18:58,199: Checkpointing model
2026-01-04 12:18:58,855: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:18:59,141: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_3000
2026-01-04 12:19:19,689: Train batch 3200: loss: 26.34 grad norm: 69.97 time: 0.077
2026-01-04 12:19:41,180: Train batch 3400: loss: 18.66 grad norm: 55.32 time: 0.052
2026-01-04 12:19:51,738: Running test after training batch: 3500
2026-01-04 12:19:52,193: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:19:59,927: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point will
2026-01-04 12:19:59,972: WER debug example
  GT : how does it keep the cost down
  PR : houde des it kipp thus us ent
2026-01-04 12:20:02,589: Val batch 3500: PER (avg): 0.2668 CTC Loss (avg): 26.4078 WER(1gram): 66.50% (n=64) time: 10.851
2026-01-04 12:20:02,590: WER lens: avg_true_words=6.16 avg_pred_words=5.84 max_pred_words=11
2026-01-04 12:20:02,590: t15.2023.08.13 val PER: 0.2328
2026-01-04 12:20:02,590: t15.2023.08.18 val PER: 0.2154
2026-01-04 12:20:02,590: t15.2023.08.20 val PER: 0.2160
2026-01-04 12:20:02,590: t15.2023.08.25 val PER: 0.1762
2026-01-04 12:20:02,591: t15.2023.08.27 val PER: 0.2733
2026-01-04 12:20:02,591: t15.2023.09.01 val PER: 0.1737
2026-01-04 12:20:02,591: t15.2023.09.03 val PER: 0.2494
2026-01-04 12:20:02,591: t15.2023.09.24 val PER: 0.2051
2026-01-04 12:20:02,591: t15.2023.09.29 val PER: 0.2163
2026-01-04 12:20:02,591: t15.2023.10.01 val PER: 0.2741
2026-01-04 12:20:02,591: t15.2023.10.06 val PER: 0.1884
2026-01-04 12:20:02,592: t15.2023.10.08 val PER: 0.3545
2026-01-04 12:20:02,592: t15.2023.10.13 val PER: 0.3243
2026-01-04 12:20:02,592: t15.2023.10.15 val PER: 0.2432
2026-01-04 12:20:02,592: t15.2023.10.20 val PER: 0.2517
2026-01-04 12:20:02,592: t15.2023.10.22 val PER: 0.2016
2026-01-04 12:20:02,592: t15.2023.11.03 val PER: 0.2619
2026-01-04 12:20:02,592: t15.2023.11.04 val PER: 0.0717
2026-01-04 12:20:02,592: t15.2023.11.17 val PER: 0.1135
2026-01-04 12:20:02,592: t15.2023.11.19 val PER: 0.0978
2026-01-04 12:20:02,592: t15.2023.11.26 val PER: 0.2855
2026-01-04 12:20:02,592: t15.2023.12.03 val PER: 0.2532
2026-01-04 12:20:02,592: t15.2023.12.08 val PER: 0.2570
2026-01-04 12:20:02,593: t15.2023.12.10 val PER: 0.2011
2026-01-04 12:20:02,593: t15.2023.12.17 val PER: 0.2692
2026-01-04 12:20:02,593: t15.2023.12.29 val PER: 0.2539
2026-01-04 12:20:02,593: t15.2024.02.25 val PER: 0.2107
2026-01-04 12:20:02,593: t15.2024.03.08 val PER: 0.3428
2026-01-04 12:20:02,593: t15.2024.03.15 val PER: 0.3208
2026-01-04 12:20:02,593: t15.2024.03.17 val PER: 0.2789
2026-01-04 12:20:02,593: t15.2024.05.10 val PER: 0.2675
2026-01-04 12:20:02,593: t15.2024.06.14 val PER: 0.2839
2026-01-04 12:20:02,593: t15.2024.07.19 val PER: 0.3922
2026-01-04 12:20:02,593: t15.2024.07.21 val PER: 0.2145
2026-01-04 12:20:02,594: t15.2024.07.28 val PER: 0.2809
2026-01-04 12:20:02,594: t15.2025.01.10 val PER: 0.4449
2026-01-04 12:20:02,594: t15.2025.01.12 val PER: 0.2995
2026-01-04 12:20:02,594: t15.2025.03.14 val PER: 0.4334
2026-01-04 12:20:02,594: t15.2025.03.16 val PER: 0.3220
2026-01-04 12:20:02,594: t15.2025.03.30 val PER: 0.4529
2026-01-04 12:20:02,594: t15.2025.04.13 val PER: 0.3524
2026-01-04 12:20:02,882: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_3500
2026-01-04 12:20:12,603: Train batch 3600: loss: 22.66 grad norm: 63.53 time: 0.067
2026-01-04 12:20:32,640: Train batch 3800: loss: 25.11 grad norm: 66.85 time: 0.067
2026-01-04 12:20:52,381: Train batch 4000: loss: 20.20 grad norm: 56.35 time: 0.056
2026-01-04 12:20:52,382: Running test after training batch: 4000
2026-01-04 12:20:52,592: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:20:57,687: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point will
2026-01-04 12:20:57,719: WER debug example
  GT : how does it keep the cost down
  PR : aue dust it hipp the cost nit
2026-01-04 12:20:59,535: Val batch 4000: PER (avg): 0.2506 CTC Loss (avg): 24.4419 WER(1gram): 64.21% (n=64) time: 7.152
2026-01-04 12:20:59,535: WER lens: avg_true_words=6.16 avg_pred_words=5.91 max_pred_words=11
2026-01-04 12:20:59,535: t15.2023.08.13 val PER: 0.2193
2026-01-04 12:20:59,536: t15.2023.08.18 val PER: 0.2037
2026-01-04 12:20:59,536: t15.2023.08.20 val PER: 0.2033
2026-01-04 12:20:59,536: t15.2023.08.25 val PER: 0.1566
2026-01-04 12:20:59,536: t15.2023.08.27 val PER: 0.2749
2026-01-04 12:20:59,536: t15.2023.09.01 val PER: 0.1713
2026-01-04 12:20:59,536: t15.2023.09.03 val PER: 0.2482
2026-01-04 12:20:59,536: t15.2023.09.24 val PER: 0.1930
2026-01-04 12:20:59,536: t15.2023.09.29 val PER: 0.1946
2026-01-04 12:20:59,537: t15.2023.10.01 val PER: 0.2563
2026-01-04 12:20:59,537: t15.2023.10.06 val PER: 0.1712
2026-01-04 12:20:59,537: t15.2023.10.08 val PER: 0.3396
2026-01-04 12:20:59,537: t15.2023.10.13 val PER: 0.3026
2026-01-04 12:20:59,537: t15.2023.10.15 val PER: 0.2386
2026-01-04 12:20:59,537: t15.2023.10.20 val PER: 0.2450
2026-01-04 12:20:59,537: t15.2023.10.22 val PER: 0.2094
2026-01-04 12:20:59,537: t15.2023.11.03 val PER: 0.2395
2026-01-04 12:20:59,537: t15.2023.11.04 val PER: 0.0614
2026-01-04 12:20:59,537: t15.2023.11.17 val PER: 0.0995
2026-01-04 12:20:59,538: t15.2023.11.19 val PER: 0.0978
2026-01-04 12:20:59,538: t15.2023.11.26 val PER: 0.2703
2026-01-04 12:20:59,538: t15.2023.12.03 val PER: 0.2311
2026-01-04 12:20:59,538: t15.2023.12.08 val PER: 0.2190
2026-01-04 12:20:59,538: t15.2023.12.10 val PER: 0.1748
2026-01-04 12:20:59,538: t15.2023.12.17 val PER: 0.2609
2026-01-04 12:20:59,538: t15.2023.12.29 val PER: 0.2512
2026-01-04 12:20:59,538: t15.2024.02.25 val PER: 0.2247
2026-01-04 12:20:59,538: t15.2024.03.08 val PER: 0.3371
2026-01-04 12:20:59,538: t15.2024.03.15 val PER: 0.3021
2026-01-04 12:20:59,538: t15.2024.03.17 val PER: 0.2510
2026-01-04 12:20:59,538: t15.2024.05.10 val PER: 0.2779
2026-01-04 12:20:59,539: t15.2024.06.14 val PER: 0.2555
2026-01-04 12:20:59,539: t15.2024.07.19 val PER: 0.3639
2026-01-04 12:20:59,539: t15.2024.07.21 val PER: 0.1959
2026-01-04 12:20:59,539: t15.2024.07.28 val PER: 0.2404
2026-01-04 12:20:59,539: t15.2025.01.10 val PER: 0.4229
2026-01-04 12:20:59,539: t15.2025.01.12 val PER: 0.2825
2026-01-04 12:20:59,539: t15.2025.03.14 val PER: 0.4172
2026-01-04 12:20:59,539: t15.2025.03.16 val PER: 0.3207
2026-01-04 12:20:59,539: t15.2025.03.30 val PER: 0.4080
2026-01-04 12:20:59,539: t15.2025.04.13 val PER: 0.3267
2026-01-04 12:20:59,541: New best val WER(1gram) 65.48% --> 64.21%
2026-01-04 12:20:59,541: Checkpointing model
2026-01-04 12:21:00,163: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:21:00,450: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_4000
2026-01-04 12:21:21,196: Train batch 4200: loss: 22.90 grad norm: 65.95 time: 0.079
2026-01-04 12:21:41,917: Train batch 4400: loss: 17.22 grad norm: 55.07 time: 0.067
2026-01-04 12:21:52,422: Running test after training batch: 4500
2026-01-04 12:21:52,614: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:21:57,961: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point us will
2026-01-04 12:21:58,013: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cost get
2026-01-04 12:22:00,957: Val batch 4500: PER (avg): 0.2378 CTC Loss (avg): 23.2194 WER(1gram): 61.42% (n=64) time: 8.534
2026-01-04 12:22:00,958: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=11
2026-01-04 12:22:00,958: t15.2023.08.13 val PER: 0.2141
2026-01-04 12:22:00,958: t15.2023.08.18 val PER: 0.1852
2026-01-04 12:22:00,958: t15.2023.08.20 val PER: 0.1906
2026-01-04 12:22:00,958: t15.2023.08.25 val PER: 0.1476
2026-01-04 12:22:00,959: t15.2023.08.27 val PER: 0.2556
2026-01-04 12:22:00,959: t15.2023.09.01 val PER: 0.1607
2026-01-04 12:22:00,959: t15.2023.09.03 val PER: 0.2257
2026-01-04 12:22:00,959: t15.2023.09.24 val PER: 0.1711
2026-01-04 12:22:00,959: t15.2023.09.29 val PER: 0.2023
2026-01-04 12:22:00,959: t15.2023.10.01 val PER: 0.2609
2026-01-04 12:22:00,959: t15.2023.10.06 val PER: 0.1529
2026-01-04 12:22:00,959: t15.2023.10.08 val PER: 0.3194
2026-01-04 12:22:00,959: t15.2023.10.13 val PER: 0.2925
2026-01-04 12:22:00,959: t15.2023.10.15 val PER: 0.2287
2026-01-04 12:22:00,960: t15.2023.10.20 val PER: 0.2215
2026-01-04 12:22:00,960: t15.2023.10.22 val PER: 0.1826
2026-01-04 12:22:00,960: t15.2023.11.03 val PER: 0.2469
2026-01-04 12:22:00,960: t15.2023.11.04 val PER: 0.0614
2026-01-04 12:22:00,960: t15.2023.11.17 val PER: 0.0949
2026-01-04 12:22:00,960: t15.2023.11.19 val PER: 0.0818
2026-01-04 12:22:00,960: t15.2023.11.26 val PER: 0.2652
2026-01-04 12:22:00,960: t15.2023.12.03 val PER: 0.2132
2026-01-04 12:22:00,960: t15.2023.12.08 val PER: 0.2097
2026-01-04 12:22:00,960: t15.2023.12.10 val PER: 0.1827
2026-01-04 12:22:00,960: t15.2023.12.17 val PER: 0.2287
2026-01-04 12:22:00,960: t15.2023.12.29 val PER: 0.2320
2026-01-04 12:22:00,960: t15.2024.02.25 val PER: 0.1938
2026-01-04 12:22:00,960: t15.2024.03.08 val PER: 0.3144
2026-01-04 12:22:00,961: t15.2024.03.15 val PER: 0.2858
2026-01-04 12:22:00,961: t15.2024.03.17 val PER: 0.2490
2026-01-04 12:22:00,961: t15.2024.05.10 val PER: 0.2467
2026-01-04 12:22:00,961: t15.2024.06.14 val PER: 0.2476
2026-01-04 12:22:00,961: t15.2024.07.19 val PER: 0.3395
2026-01-04 12:22:00,961: t15.2024.07.21 val PER: 0.1738
2026-01-04 12:22:00,961: t15.2024.07.28 val PER: 0.2265
2026-01-04 12:22:00,961: t15.2025.01.10 val PER: 0.4146
2026-01-04 12:22:00,961: t15.2025.01.12 val PER: 0.2640
2026-01-04 12:22:00,961: t15.2025.03.14 val PER: 0.4009
2026-01-04 12:22:00,961: t15.2025.03.16 val PER: 0.2919
2026-01-04 12:22:00,961: t15.2025.03.30 val PER: 0.4103
2026-01-04 12:22:00,961: t15.2025.04.13 val PER: 0.3096
2026-01-04 12:22:00,963: New best val WER(1gram) 64.21% --> 61.42%
2026-01-04 12:22:00,963: Checkpointing model
2026-01-04 12:22:01,625: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:22:01,924: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_4500
2026-01-04 12:22:11,228: Train batch 4600: loss: 20.04 grad norm: 65.83 time: 0.063
2026-01-04 12:22:30,152: Train batch 4800: loss: 13.82 grad norm: 53.14 time: 0.066
2026-01-04 12:22:49,040: Train batch 5000: loss: 31.73 grad norm: 82.03 time: 0.064
2026-01-04 12:22:49,041: Running test after training batch: 5000
2026-01-04 12:22:49,178: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:22:54,813: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point as will
2026-01-04 12:22:54,866: WER debug example
  GT : how does it keep the cost down
  PR : how des it keep the cost get
2026-01-04 12:22:57,941: Val batch 5000: PER (avg): 0.2259 CTC Loss (avg): 21.8327 WER(1gram): 61.17% (n=64) time: 8.900
2026-01-04 12:22:57,942: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-04 12:22:57,942: t15.2023.08.13 val PER: 0.1892
2026-01-04 12:22:57,942: t15.2023.08.18 val PER: 0.1744
2026-01-04 12:22:57,943: t15.2023.08.20 val PER: 0.1684
2026-01-04 12:22:57,943: t15.2023.08.25 val PER: 0.1355
2026-01-04 12:22:57,943: t15.2023.08.27 val PER: 0.2379
2026-01-04 12:22:57,943: t15.2023.09.01 val PER: 0.1404
2026-01-04 12:22:57,943: t15.2023.09.03 val PER: 0.2257
2026-01-04 12:22:57,943: t15.2023.09.24 val PER: 0.1905
2026-01-04 12:22:57,943: t15.2023.09.29 val PER: 0.1857
2026-01-04 12:22:57,943: t15.2023.10.01 val PER: 0.2391
2026-01-04 12:22:57,944: t15.2023.10.06 val PER: 0.1346
2026-01-04 12:22:57,944: t15.2023.10.08 val PER: 0.3112
2026-01-04 12:22:57,944: t15.2023.10.13 val PER: 0.2770
2026-01-04 12:22:57,944: t15.2023.10.15 val PER: 0.2116
2026-01-04 12:22:57,944: t15.2023.10.20 val PER: 0.2215
2026-01-04 12:22:57,944: t15.2023.10.22 val PER: 0.1704
2026-01-04 12:22:57,944: t15.2023.11.03 val PER: 0.2185
2026-01-04 12:22:57,944: t15.2023.11.04 val PER: 0.0341
2026-01-04 12:22:57,944: t15.2023.11.17 val PER: 0.0824
2026-01-04 12:22:57,944: t15.2023.11.19 val PER: 0.0798
2026-01-04 12:22:57,945: t15.2023.11.26 val PER: 0.2413
2026-01-04 12:22:57,945: t15.2023.12.03 val PER: 0.1975
2026-01-04 12:22:57,945: t15.2023.12.08 val PER: 0.2084
2026-01-04 12:22:57,945: t15.2023.12.10 val PER: 0.1603
2026-01-04 12:22:57,945: t15.2023.12.17 val PER: 0.2225
2026-01-04 12:22:57,945: t15.2023.12.29 val PER: 0.2128
2026-01-04 12:22:57,945: t15.2024.02.25 val PER: 0.1868
2026-01-04 12:22:57,946: t15.2024.03.08 val PER: 0.3215
2026-01-04 12:22:57,946: t15.2024.03.15 val PER: 0.2833
2026-01-04 12:22:57,946: t15.2024.03.17 val PER: 0.2392
2026-01-04 12:22:57,946: t15.2024.05.10 val PER: 0.2452
2026-01-04 12:22:57,946: t15.2024.06.14 val PER: 0.2492
2026-01-04 12:22:57,946: t15.2024.07.19 val PER: 0.3336
2026-01-04 12:22:57,946: t15.2024.07.21 val PER: 0.1752
2026-01-04 12:22:57,947: t15.2024.07.28 val PER: 0.2147
2026-01-04 12:22:57,947: t15.2025.01.10 val PER: 0.3802
2026-01-04 12:22:57,947: t15.2025.01.12 val PER: 0.2502
2026-01-04 12:22:57,947: t15.2025.03.14 val PER: 0.3964
2026-01-04 12:22:57,947: t15.2025.03.16 val PER: 0.2827
2026-01-04 12:22:57,947: t15.2025.03.30 val PER: 0.4023
2026-01-04 12:22:57,947: t15.2025.04.13 val PER: 0.3067
2026-01-04 12:22:57,948: New best val WER(1gram) 61.42% --> 61.17%
2026-01-04 12:22:57,948: Checkpointing model
2026-01-04 12:22:58,590: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:22:58,887: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_5000
2026-01-04 12:23:19,663: Train batch 5200: loss: 16.80 grad norm: 59.24 time: 0.054
2026-01-04 12:23:40,631: Train batch 5400: loss: 18.04 grad norm: 60.57 time: 0.069
2026-01-04 12:23:51,244: Running test after training batch: 5500
2026-01-04 12:23:51,651: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:23:57,036: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point will
2026-01-04 12:23:57,095: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost tet
2026-01-04 12:24:00,237: Val batch 5500: PER (avg): 0.2147 CTC Loss (avg): 21.1008 WER(1gram): 56.09% (n=64) time: 8.992
2026-01-04 12:24:00,238: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=11
2026-01-04 12:24:00,238: t15.2023.08.13 val PER: 0.1757
2026-01-04 12:24:00,238: t15.2023.08.18 val PER: 0.1601
2026-01-04 12:24:00,238: t15.2023.08.20 val PER: 0.1716
2026-01-04 12:24:00,238: t15.2023.08.25 val PER: 0.1370
2026-01-04 12:24:00,238: t15.2023.08.27 val PER: 0.2508
2026-01-04 12:24:00,238: t15.2023.09.01 val PER: 0.1258
2026-01-04 12:24:00,238: t15.2023.09.03 val PER: 0.2221
2026-01-04 12:24:00,239: t15.2023.09.24 val PER: 0.1723
2026-01-04 12:24:00,239: t15.2023.09.29 val PER: 0.1723
2026-01-04 12:24:00,239: t15.2023.10.01 val PER: 0.2226
2026-01-04 12:24:00,239: t15.2023.10.06 val PER: 0.1324
2026-01-04 12:24:00,239: t15.2023.10.08 val PER: 0.3004
2026-01-04 12:24:00,239: t15.2023.10.13 val PER: 0.2739
2026-01-04 12:24:00,239: t15.2023.10.15 val PER: 0.2116
2026-01-04 12:24:00,239: t15.2023.10.20 val PER: 0.2248
2026-01-04 12:24:00,239: t15.2023.10.22 val PER: 0.1537
2026-01-04 12:24:00,239: t15.2023.11.03 val PER: 0.2320
2026-01-04 12:24:00,239: t15.2023.11.04 val PER: 0.0648
2026-01-04 12:24:00,239: t15.2023.11.17 val PER: 0.0886
2026-01-04 12:24:00,239: t15.2023.11.19 val PER: 0.0719
2026-01-04 12:24:00,240: t15.2023.11.26 val PER: 0.2152
2026-01-04 12:24:00,240: t15.2023.12.03 val PER: 0.1796
2026-01-04 12:24:00,240: t15.2023.12.08 val PER: 0.1864
2026-01-04 12:24:00,240: t15.2023.12.10 val PER: 0.1643
2026-01-04 12:24:00,240: t15.2023.12.17 val PER: 0.2058
2026-01-04 12:24:00,240: t15.2023.12.29 val PER: 0.2155
2026-01-04 12:24:00,240: t15.2024.02.25 val PER: 0.1854
2026-01-04 12:24:00,240: t15.2024.03.08 val PER: 0.2888
2026-01-04 12:24:00,241: t15.2024.03.15 val PER: 0.2545
2026-01-04 12:24:00,241: t15.2024.03.17 val PER: 0.2162
2026-01-04 12:24:00,241: t15.2024.05.10 val PER: 0.2318
2026-01-04 12:24:00,241: t15.2024.06.14 val PER: 0.2192
2026-01-04 12:24:00,241: t15.2024.07.19 val PER: 0.3191
2026-01-04 12:24:00,241: t15.2024.07.21 val PER: 0.1662
2026-01-04 12:24:00,241: t15.2024.07.28 val PER: 0.2118
2026-01-04 12:24:00,241: t15.2025.01.10 val PER: 0.3760
2026-01-04 12:24:00,241: t15.2025.01.12 val PER: 0.2302
2026-01-04 12:24:00,241: t15.2025.03.14 val PER: 0.3698
2026-01-04 12:24:00,241: t15.2025.03.16 val PER: 0.2618
2026-01-04 12:24:00,241: t15.2025.03.30 val PER: 0.3747
2026-01-04 12:24:00,242: t15.2025.04.13 val PER: 0.2896
2026-01-04 12:24:00,242: New best val WER(1gram) 61.17% --> 56.09%
2026-01-04 12:24:00,242: Checkpointing model
2026-01-04 12:24:00,908: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:24:01,208: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_5500
2026-01-04 12:24:11,452: Train batch 5600: loss: 19.11 grad norm: 69.74 time: 0.062
2026-01-04 12:24:30,298: Train batch 5800: loss: 13.84 grad norm: 54.58 time: 0.083
2026-01-04 12:24:48,984: Train batch 6000: loss: 14.38 grad norm: 55.55 time: 0.049
2026-01-04 12:24:48,984: Running test after training batch: 6000
2026-01-04 12:24:49,175: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:24:54,608: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-04 12:24:54,670: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost nit
2026-01-04 12:24:57,934: Val batch 6000: PER (avg): 0.2114 CTC Loss (avg): 20.7842 WER(1gram): 57.87% (n=64) time: 8.950
2026-01-04 12:24:57,935: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 12:24:57,935: t15.2023.08.13 val PER: 0.1778
2026-01-04 12:24:57,935: t15.2023.08.18 val PER: 0.1609
2026-01-04 12:24:57,935: t15.2023.08.20 val PER: 0.1628
2026-01-04 12:24:57,936: t15.2023.08.25 val PER: 0.1160
2026-01-04 12:24:57,936: t15.2023.08.27 val PER: 0.2524
2026-01-04 12:24:57,936: t15.2023.09.01 val PER: 0.1347
2026-01-04 12:24:57,936: t15.2023.09.03 val PER: 0.2150
2026-01-04 12:24:57,936: t15.2023.09.24 val PER: 0.1663
2026-01-04 12:24:57,936: t15.2023.09.29 val PER: 0.1710
2026-01-04 12:24:57,936: t15.2023.10.01 val PER: 0.2193
2026-01-04 12:24:57,936: t15.2023.10.06 val PER: 0.1313
2026-01-04 12:24:57,936: t15.2023.10.08 val PER: 0.2801
2026-01-04 12:24:57,936: t15.2023.10.13 val PER: 0.2770
2026-01-04 12:24:57,936: t15.2023.10.15 val PER: 0.2142
2026-01-04 12:24:57,937: t15.2023.10.20 val PER: 0.2081
2026-01-04 12:24:57,937: t15.2023.10.22 val PER: 0.1670
2026-01-04 12:24:57,937: t15.2023.11.03 val PER: 0.2205
2026-01-04 12:24:57,937: t15.2023.11.04 val PER: 0.0648
2026-01-04 12:24:57,937: t15.2023.11.17 val PER: 0.0715
2026-01-04 12:24:57,937: t15.2023.11.19 val PER: 0.0778
2026-01-04 12:24:57,937: t15.2023.11.26 val PER: 0.2159
2026-01-04 12:24:57,937: t15.2023.12.03 val PER: 0.1775
2026-01-04 12:24:57,937: t15.2023.12.08 val PER: 0.1738
2026-01-04 12:24:57,937: t15.2023.12.10 val PER: 0.1485
2026-01-04 12:24:57,937: t15.2023.12.17 val PER: 0.1933
2026-01-04 12:24:57,937: t15.2023.12.29 val PER: 0.2237
2026-01-04 12:24:57,937: t15.2024.02.25 val PER: 0.1713
2026-01-04 12:24:57,938: t15.2024.03.08 val PER: 0.2902
2026-01-04 12:24:57,938: t15.2024.03.15 val PER: 0.2652
2026-01-04 12:24:57,938: t15.2024.03.17 val PER: 0.2029
2026-01-04 12:24:57,938: t15.2024.05.10 val PER: 0.2184
2026-01-04 12:24:57,938: t15.2024.06.14 val PER: 0.2319
2026-01-04 12:24:57,938: t15.2024.07.19 val PER: 0.3164
2026-01-04 12:24:57,938: t15.2024.07.21 val PER: 0.1607
2026-01-04 12:24:57,938: t15.2024.07.28 val PER: 0.2007
2026-01-04 12:24:57,938: t15.2025.01.10 val PER: 0.3884
2026-01-04 12:24:57,939: t15.2025.01.12 val PER: 0.2186
2026-01-04 12:24:57,939: t15.2025.03.14 val PER: 0.3861
2026-01-04 12:24:57,939: t15.2025.03.16 val PER: 0.2657
2026-01-04 12:24:57,939: t15.2025.03.30 val PER: 0.3736
2026-01-04 12:24:57,939: t15.2025.04.13 val PER: 0.2611
2026-01-04 12:24:58,221: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_6000
2026-01-04 12:25:18,950: Train batch 6200: loss: 17.21 grad norm: 61.15 time: 0.069
2026-01-04 12:25:38,938: Train batch 6400: loss: 18.90 grad norm: 64.86 time: 0.062
2026-01-04 12:25:49,154: Running test after training batch: 6500
2026-01-04 12:25:49,286: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:25:54,901: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point as will
2026-01-04 12:25:54,965: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost jett
2026-01-04 12:25:58,377: Val batch 6500: PER (avg): 0.2054 CTC Loss (avg): 20.1713 WER(1gram): 53.05% (n=64) time: 9.222
2026-01-04 12:25:58,378: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-04 12:25:58,379: t15.2023.08.13 val PER: 0.1705
2026-01-04 12:25:58,379: t15.2023.08.18 val PER: 0.1484
2026-01-04 12:25:58,379: t15.2023.08.20 val PER: 0.1549
2026-01-04 12:25:58,379: t15.2023.08.25 val PER: 0.1099
2026-01-04 12:25:58,379: t15.2023.08.27 val PER: 0.2299
2026-01-04 12:25:58,379: t15.2023.09.01 val PER: 0.1209
2026-01-04 12:25:58,379: t15.2023.09.03 val PER: 0.2162
2026-01-04 12:25:58,379: t15.2023.09.24 val PER: 0.1699
2026-01-04 12:25:58,380: t15.2023.09.29 val PER: 0.1698
2026-01-04 12:25:58,380: t15.2023.10.01 val PER: 0.2213
2026-01-04 12:25:58,380: t15.2023.10.06 val PER: 0.1206
2026-01-04 12:25:58,380: t15.2023.10.08 val PER: 0.3004
2026-01-04 12:25:58,380: t15.2023.10.13 val PER: 0.2653
2026-01-04 12:25:58,380: t15.2023.10.15 val PER: 0.2129
2026-01-04 12:25:58,380: t15.2023.10.20 val PER: 0.2081
2026-01-04 12:25:58,380: t15.2023.10.22 val PER: 0.1604
2026-01-04 12:25:58,381: t15.2023.11.03 val PER: 0.2178
2026-01-04 12:25:58,381: t15.2023.11.04 val PER: 0.0546
2026-01-04 12:25:58,381: t15.2023.11.17 val PER: 0.0731
2026-01-04 12:25:58,381: t15.2023.11.19 val PER: 0.0699
2026-01-04 12:25:58,381: t15.2023.11.26 val PER: 0.2109
2026-01-04 12:25:58,381: t15.2023.12.03 val PER: 0.1681
2026-01-04 12:25:58,381: t15.2023.12.08 val PER: 0.1738
2026-01-04 12:25:58,381: t15.2023.12.10 val PER: 0.1459
2026-01-04 12:25:58,381: t15.2023.12.17 val PER: 0.1892
2026-01-04 12:25:58,381: t15.2023.12.29 val PER: 0.2086
2026-01-04 12:25:58,381: t15.2024.02.25 val PER: 0.1629
2026-01-04 12:25:58,381: t15.2024.03.08 val PER: 0.2802
2026-01-04 12:25:58,381: t15.2024.03.15 val PER: 0.2677
2026-01-04 12:25:58,382: t15.2024.03.17 val PER: 0.2071
2026-01-04 12:25:58,382: t15.2024.05.10 val PER: 0.2244
2026-01-04 12:25:58,382: t15.2024.06.14 val PER: 0.2066
2026-01-04 12:25:58,382: t15.2024.07.19 val PER: 0.2980
2026-01-04 12:25:58,382: t15.2024.07.21 val PER: 0.1531
2026-01-04 12:25:58,382: t15.2024.07.28 val PER: 0.1897
2026-01-04 12:25:58,382: t15.2025.01.10 val PER: 0.3733
2026-01-04 12:25:58,382: t15.2025.01.12 val PER: 0.2179
2026-01-04 12:25:58,382: t15.2025.03.14 val PER: 0.3831
2026-01-04 12:25:58,382: t15.2025.03.16 val PER: 0.2382
2026-01-04 12:25:58,382: t15.2025.03.30 val PER: 0.3552
2026-01-04 12:25:58,382: t15.2025.04.13 val PER: 0.2696
2026-01-04 12:25:58,384: New best val WER(1gram) 56.09% --> 53.05%
2026-01-04 12:25:58,385: Checkpointing model
2026-01-04 12:25:59,070: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:25:59,356: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_6500
2026-01-04 12:26:08,456: Train batch 6600: loss: 12.46 grad norm: 54.66 time: 0.045
2026-01-04 12:26:29,362: Train batch 6800: loss: 15.45 grad norm: 57.30 time: 0.051
2026-01-04 12:26:50,037: Train batch 7000: loss: 17.81 grad norm: 67.60 time: 0.063
2026-01-04 12:26:50,037: Running test after training batch: 7000
2026-01-04 12:26:50,151: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:26:55,998: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 12:26:56,063: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-04 12:26:59,696: Val batch 7000: PER (avg): 0.1959 CTC Loss (avg): 19.2480 WER(1gram): 54.31% (n=64) time: 9.659
2026-01-04 12:26:59,697: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-04 12:26:59,697: t15.2023.08.13 val PER: 0.1642
2026-01-04 12:26:59,697: t15.2023.08.18 val PER: 0.1492
2026-01-04 12:26:59,697: t15.2023.08.20 val PER: 0.1549
2026-01-04 12:26:59,697: t15.2023.08.25 val PER: 0.0979
2026-01-04 12:26:59,697: t15.2023.08.27 val PER: 0.2219
2026-01-04 12:26:59,698: t15.2023.09.01 val PER: 0.1128
2026-01-04 12:26:59,698: t15.2023.09.03 val PER: 0.1912
2026-01-04 12:26:59,698: t15.2023.09.24 val PER: 0.1578
2026-01-04 12:26:59,698: t15.2023.09.29 val PER: 0.1729
2026-01-04 12:26:59,698: t15.2023.10.01 val PER: 0.2067
2026-01-04 12:26:59,698: t15.2023.10.06 val PER: 0.1119
2026-01-04 12:26:59,698: t15.2023.10.08 val PER: 0.2774
2026-01-04 12:26:59,698: t15.2023.10.13 val PER: 0.2506
2026-01-04 12:26:59,698: t15.2023.10.15 val PER: 0.1925
2026-01-04 12:26:59,698: t15.2023.10.20 val PER: 0.2181
2026-01-04 12:26:59,698: t15.2023.10.22 val PER: 0.1470
2026-01-04 12:26:59,699: t15.2023.11.03 val PER: 0.2028
2026-01-04 12:26:59,699: t15.2023.11.04 val PER: 0.0580
2026-01-04 12:26:59,699: t15.2023.11.17 val PER: 0.0669
2026-01-04 12:26:59,699: t15.2023.11.19 val PER: 0.0619
2026-01-04 12:26:59,699: t15.2023.11.26 val PER: 0.2000
2026-01-04 12:26:59,699: t15.2023.12.03 val PER: 0.1712
2026-01-04 12:26:59,699: t15.2023.12.08 val PER: 0.1525
2026-01-04 12:26:59,699: t15.2023.12.10 val PER: 0.1485
2026-01-04 12:26:59,699: t15.2023.12.17 val PER: 0.1798
2026-01-04 12:26:59,699: t15.2023.12.29 val PER: 0.1956
2026-01-04 12:26:59,699: t15.2024.02.25 val PER: 0.1573
2026-01-04 12:26:59,699: t15.2024.03.08 val PER: 0.2859
2026-01-04 12:26:59,699: t15.2024.03.15 val PER: 0.2439
2026-01-04 12:26:59,700: t15.2024.03.17 val PER: 0.1967
2026-01-04 12:26:59,700: t15.2024.05.10 val PER: 0.1991
2026-01-04 12:26:59,700: t15.2024.06.14 val PER: 0.2161
2026-01-04 12:26:59,700: t15.2024.07.19 val PER: 0.3092
2026-01-04 12:26:59,700: t15.2024.07.21 val PER: 0.1352
2026-01-04 12:26:59,700: t15.2024.07.28 val PER: 0.1699
2026-01-04 12:26:59,700: t15.2025.01.10 val PER: 0.3650
2026-01-04 12:26:59,700: t15.2025.01.12 val PER: 0.2048
2026-01-04 12:26:59,700: t15.2025.03.14 val PER: 0.3536
2026-01-04 12:26:59,700: t15.2025.03.16 val PER: 0.2382
2026-01-04 12:26:59,700: t15.2025.03.30 val PER: 0.3690
2026-01-04 12:26:59,700: t15.2025.04.13 val PER: 0.2639
2026-01-04 12:26:59,991: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_7000
2026-01-04 12:27:19,880: Train batch 7200: loss: 14.38 grad norm: 56.89 time: 0.079
2026-01-04 12:27:39,038: Train batch 7400: loss: 13.70 grad norm: 55.27 time: 0.077
2026-01-04 12:27:48,618: Running test after training batch: 7500
2026-01-04 12:27:48,726: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:27:54,264: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 12:27:54,339: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nit
2026-01-04 12:27:58,067: Val batch 7500: PER (avg): 0.1908 CTC Loss (avg): 18.6845 WER(1gram): 55.33% (n=64) time: 9.449
2026-01-04 12:27:58,068: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 12:27:58,068: t15.2023.08.13 val PER: 0.1549
2026-01-04 12:27:58,068: t15.2023.08.18 val PER: 0.1467
2026-01-04 12:27:58,068: t15.2023.08.20 val PER: 0.1438
2026-01-04 12:27:58,069: t15.2023.08.25 val PER: 0.1099
2026-01-04 12:27:58,069: t15.2023.08.27 val PER: 0.2074
2026-01-04 12:27:58,069: t15.2023.09.01 val PER: 0.1120
2026-01-04 12:27:58,069: t15.2023.09.03 val PER: 0.1983
2026-01-04 12:27:58,069: t15.2023.09.24 val PER: 0.1541
2026-01-04 12:27:58,069: t15.2023.09.29 val PER: 0.1583
2026-01-04 12:27:58,069: t15.2023.10.01 val PER: 0.2120
2026-01-04 12:27:58,070: t15.2023.10.06 val PER: 0.1098
2026-01-04 12:27:58,070: t15.2023.10.08 val PER: 0.2706
2026-01-04 12:27:58,070: t15.2023.10.13 val PER: 0.2475
2026-01-04 12:27:58,070: t15.2023.10.15 val PER: 0.1839
2026-01-04 12:27:58,070: t15.2023.10.20 val PER: 0.2013
2026-01-04 12:27:58,070: t15.2023.10.22 val PER: 0.1425
2026-01-04 12:27:58,070: t15.2023.11.03 val PER: 0.2096
2026-01-04 12:27:58,070: t15.2023.11.04 val PER: 0.0478
2026-01-04 12:27:58,070: t15.2023.11.17 val PER: 0.0669
2026-01-04 12:27:58,070: t15.2023.11.19 val PER: 0.0579
2026-01-04 12:27:58,071: t15.2023.11.26 val PER: 0.1899
2026-01-04 12:27:58,071: t15.2023.12.03 val PER: 0.1649
2026-01-04 12:27:58,071: t15.2023.12.08 val PER: 0.1585
2026-01-04 12:27:58,071: t15.2023.12.10 val PER: 0.1314
2026-01-04 12:27:58,071: t15.2023.12.17 val PER: 0.1798
2026-01-04 12:27:58,071: t15.2023.12.29 val PER: 0.1853
2026-01-04 12:27:58,071: t15.2024.02.25 val PER: 0.1587
2026-01-04 12:27:58,071: t15.2024.03.08 val PER: 0.2802
2026-01-04 12:27:58,071: t15.2024.03.15 val PER: 0.2414
2026-01-04 12:27:58,071: t15.2024.03.17 val PER: 0.1862
2026-01-04 12:27:58,071: t15.2024.05.10 val PER: 0.2036
2026-01-04 12:27:58,071: t15.2024.06.14 val PER: 0.1956
2026-01-04 12:27:58,072: t15.2024.07.19 val PER: 0.2894
2026-01-04 12:27:58,072: t15.2024.07.21 val PER: 0.1290
2026-01-04 12:27:58,072: t15.2024.07.28 val PER: 0.1779
2026-01-04 12:27:58,072: t15.2025.01.10 val PER: 0.3512
2026-01-04 12:27:58,072: t15.2025.01.12 val PER: 0.1955
2026-01-04 12:27:58,072: t15.2025.03.14 val PER: 0.3595
2026-01-04 12:27:58,072: t15.2025.03.16 val PER: 0.2474
2026-01-04 12:27:58,072: t15.2025.03.30 val PER: 0.3529
2026-01-04 12:27:58,072: t15.2025.04.13 val PER: 0.2397
2026-01-04 12:27:58,355: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_7500
2026-01-04 12:28:08,554: Train batch 7600: loss: 16.23 grad norm: 58.21 time: 0.073
2026-01-04 12:28:27,465: Train batch 7800: loss: 14.36 grad norm: 57.72 time: 0.057
2026-01-04 12:28:46,482: Train batch 8000: loss: 10.90 grad norm: 51.38 time: 0.073
2026-01-04 12:28:46,483: Running test after training batch: 8000
2026-01-04 12:28:46,598: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:28:52,059: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 12:28:52,130: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nit
2026-01-04 12:28:55,930: Val batch 8000: PER (avg): 0.1852 CTC Loss (avg): 18.1842 WER(1gram): 55.08% (n=64) time: 9.446
2026-01-04 12:28:55,930: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 12:28:55,931: t15.2023.08.13 val PER: 0.1507
2026-01-04 12:28:55,931: t15.2023.08.18 val PER: 0.1291
2026-01-04 12:28:55,931: t15.2023.08.20 val PER: 0.1446
2026-01-04 12:28:55,931: t15.2023.08.25 val PER: 0.1114
2026-01-04 12:28:55,931: t15.2023.08.27 val PER: 0.2138
2026-01-04 12:28:55,931: t15.2023.09.01 val PER: 0.1031
2026-01-04 12:28:55,931: t15.2023.09.03 val PER: 0.1924
2026-01-04 12:28:55,932: t15.2023.09.24 val PER: 0.1505
2026-01-04 12:28:55,932: t15.2023.09.29 val PER: 0.1551
2026-01-04 12:28:55,932: t15.2023.10.01 val PER: 0.2081
2026-01-04 12:28:55,932: t15.2023.10.06 val PER: 0.1141
2026-01-04 12:28:55,932: t15.2023.10.08 val PER: 0.2788
2026-01-04 12:28:55,932: t15.2023.10.13 val PER: 0.2459
2026-01-04 12:28:55,932: t15.2023.10.15 val PER: 0.1898
2026-01-04 12:28:55,932: t15.2023.10.20 val PER: 0.1879
2026-01-04 12:28:55,932: t15.2023.10.22 val PER: 0.1414
2026-01-04 12:28:55,933: t15.2023.11.03 val PER: 0.2008
2026-01-04 12:28:55,933: t15.2023.11.04 val PER: 0.0341
2026-01-04 12:28:55,933: t15.2023.11.17 val PER: 0.0684
2026-01-04 12:28:55,933: t15.2023.11.19 val PER: 0.0659
2026-01-04 12:28:55,933: t15.2023.11.26 val PER: 0.1804
2026-01-04 12:28:55,933: t15.2023.12.03 val PER: 0.1534
2026-01-04 12:28:55,933: t15.2023.12.08 val PER: 0.1458
2026-01-04 12:28:55,933: t15.2023.12.10 val PER: 0.1288
2026-01-04 12:28:55,933: t15.2023.12.17 val PER: 0.1663
2026-01-04 12:28:55,933: t15.2023.12.29 val PER: 0.1750
2026-01-04 12:28:55,933: t15.2024.02.25 val PER: 0.1419
2026-01-04 12:28:55,933: t15.2024.03.08 val PER: 0.2575
2026-01-04 12:28:55,934: t15.2024.03.15 val PER: 0.2420
2026-01-04 12:28:55,934: t15.2024.03.17 val PER: 0.1771
2026-01-04 12:28:55,934: t15.2024.05.10 val PER: 0.2021
2026-01-04 12:28:55,934: t15.2024.06.14 val PER: 0.2098
2026-01-04 12:28:55,934: t15.2024.07.19 val PER: 0.2848
2026-01-04 12:28:55,934: t15.2024.07.21 val PER: 0.1166
2026-01-04 12:28:55,934: t15.2024.07.28 val PER: 0.1596
2026-01-04 12:28:55,934: t15.2025.01.10 val PER: 0.3430
2026-01-04 12:28:55,934: t15.2025.01.12 val PER: 0.1848
2026-01-04 12:28:55,934: t15.2025.03.14 val PER: 0.3521
2026-01-04 12:28:55,934: t15.2025.03.16 val PER: 0.2382
2026-01-04 12:28:55,935: t15.2025.03.30 val PER: 0.3448
2026-01-04 12:28:55,935: t15.2025.04.13 val PER: 0.2553
2026-01-04 12:28:56,217: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_8000
2026-01-04 12:29:17,176: Train batch 8200: loss: 9.56 grad norm: 45.05 time: 0.054
2026-01-04 12:29:38,129: Train batch 8400: loss: 10.15 grad norm: 45.90 time: 0.064
2026-01-04 12:29:48,854: Running test after training batch: 8500
2026-01-04 12:29:48,967: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:29:54,441: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 12:29:54,516: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost nett
2026-01-04 12:29:56,630: Val batch 8500: PER (avg): 0.1801 CTC Loss (avg): 17.8463 WER(1gram): 50.76% (n=64) time: 7.776
2026-01-04 12:29:56,631: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-04 12:29:56,631: t15.2023.08.13 val PER: 0.1372
2026-01-04 12:29:56,631: t15.2023.08.18 val PER: 0.1341
2026-01-04 12:29:56,631: t15.2023.08.20 val PER: 0.1398
2026-01-04 12:29:56,631: t15.2023.08.25 val PER: 0.1130
2026-01-04 12:29:56,631: t15.2023.08.27 val PER: 0.2090
2026-01-04 12:29:56,631: t15.2023.09.01 val PER: 0.0990
2026-01-04 12:29:56,632: t15.2023.09.03 val PER: 0.1971
2026-01-04 12:29:56,632: t15.2023.09.24 val PER: 0.1590
2026-01-04 12:29:56,632: t15.2023.09.29 val PER: 0.1461
2026-01-04 12:29:56,632: t15.2023.10.01 val PER: 0.1922
2026-01-04 12:29:56,632: t15.2023.10.06 val PER: 0.1109
2026-01-04 12:29:56,632: t15.2023.10.08 val PER: 0.2503
2026-01-04 12:29:56,632: t15.2023.10.13 val PER: 0.2351
2026-01-04 12:29:56,633: t15.2023.10.15 val PER: 0.1819
2026-01-04 12:29:56,633: t15.2023.10.20 val PER: 0.1913
2026-01-04 12:29:56,633: t15.2023.10.22 val PER: 0.1448
2026-01-04 12:29:56,633: t15.2023.11.03 val PER: 0.1947
2026-01-04 12:29:56,633: t15.2023.11.04 val PER: 0.0410
2026-01-04 12:29:56,634: t15.2023.11.17 val PER: 0.0544
2026-01-04 12:29:56,634: t15.2023.11.19 val PER: 0.0539
2026-01-04 12:29:56,634: t15.2023.11.26 val PER: 0.1783
2026-01-04 12:29:56,634: t15.2023.12.03 val PER: 0.1450
2026-01-04 12:29:56,634: t15.2023.12.08 val PER: 0.1365
2026-01-04 12:29:56,634: t15.2023.12.10 val PER: 0.1235
2026-01-04 12:29:56,634: t15.2023.12.17 val PER: 0.1674
2026-01-04 12:29:56,634: t15.2023.12.29 val PER: 0.1757
2026-01-04 12:29:56,634: t15.2024.02.25 val PER: 0.1264
2026-01-04 12:29:56,634: t15.2024.03.08 val PER: 0.2646
2026-01-04 12:29:56,635: t15.2024.03.15 val PER: 0.2402
2026-01-04 12:29:56,635: t15.2024.03.17 val PER: 0.1736
2026-01-04 12:29:56,635: t15.2024.05.10 val PER: 0.1917
2026-01-04 12:29:56,635: t15.2024.06.14 val PER: 0.1909
2026-01-04 12:29:56,635: t15.2024.07.19 val PER: 0.2683
2026-01-04 12:29:56,635: t15.2024.07.21 val PER: 0.1221
2026-01-04 12:29:56,635: t15.2024.07.28 val PER: 0.1669
2026-01-04 12:29:56,636: t15.2025.01.10 val PER: 0.3278
2026-01-04 12:29:56,636: t15.2025.01.12 val PER: 0.1901
2026-01-04 12:29:56,636: t15.2025.03.14 val PER: 0.3683
2026-01-04 12:29:56,636: t15.2025.03.16 val PER: 0.2186
2026-01-04 12:29:56,636: t15.2025.03.30 val PER: 0.3207
2026-01-04 12:29:56,636: t15.2025.04.13 val PER: 0.2511
2026-01-04 12:29:56,637: New best val WER(1gram) 53.05% --> 50.76%
2026-01-04 12:29:56,637: Checkpointing model
2026-01-04 12:29:57,299: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:29:57,589: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_8500
2026-01-04 12:30:08,053: Train batch 8600: loss: 16.42 grad norm: 62.85 time: 0.055
2026-01-04 12:30:28,953: Train batch 8800: loss: 15.73 grad norm: 59.67 time: 0.063
2026-01-04 12:30:50,173: Train batch 9000: loss: 16.47 grad norm: 63.61 time: 0.077
2026-01-04 12:30:50,173: Running test after training batch: 9000
2026-01-04 12:30:50,290: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:30:55,639: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 12:30:55,724: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 12:31:00,210: Val batch 9000: PER (avg): 0.1757 CTC Loss (avg): 17.3482 WER(1gram): 51.02% (n=64) time: 10.037
2026-01-04 12:31:00,211: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-04 12:31:00,211: t15.2023.08.13 val PER: 0.1299
2026-01-04 12:31:00,211: t15.2023.08.18 val PER: 0.1282
2026-01-04 12:31:00,212: t15.2023.08.20 val PER: 0.1334
2026-01-04 12:31:00,213: t15.2023.08.25 val PER: 0.1054
2026-01-04 12:31:00,213: t15.2023.08.27 val PER: 0.2058
2026-01-04 12:31:00,213: t15.2023.09.01 val PER: 0.0917
2026-01-04 12:31:00,213: t15.2023.09.03 val PER: 0.1900
2026-01-04 12:31:00,213: t15.2023.09.24 val PER: 0.1529
2026-01-04 12:31:00,213: t15.2023.09.29 val PER: 0.1481
2026-01-04 12:31:00,214: t15.2023.10.01 val PER: 0.1988
2026-01-04 12:31:00,214: t15.2023.10.06 val PER: 0.1119
2026-01-04 12:31:00,214: t15.2023.10.08 val PER: 0.2598
2026-01-04 12:31:00,214: t15.2023.10.13 val PER: 0.2296
2026-01-04 12:31:00,214: t15.2023.10.15 val PER: 0.1760
2026-01-04 12:31:00,214: t15.2023.10.20 val PER: 0.2081
2026-01-04 12:31:00,214: t15.2023.10.22 val PER: 0.1325
2026-01-04 12:31:00,215: t15.2023.11.03 val PER: 0.2042
2026-01-04 12:31:00,215: t15.2023.11.04 val PER: 0.0410
2026-01-04 12:31:00,215: t15.2023.11.17 val PER: 0.0513
2026-01-04 12:31:00,215: t15.2023.11.19 val PER: 0.0459
2026-01-04 12:31:00,215: t15.2023.11.26 val PER: 0.1710
2026-01-04 12:31:00,216: t15.2023.12.03 val PER: 0.1429
2026-01-04 12:31:00,216: t15.2023.12.08 val PER: 0.1338
2026-01-04 12:31:00,216: t15.2023.12.10 val PER: 0.1078
2026-01-04 12:31:00,216: t15.2023.12.17 val PER: 0.1590
2026-01-04 12:31:00,216: t15.2023.12.29 val PER: 0.1606
2026-01-04 12:31:00,216: t15.2024.02.25 val PER: 0.1390
2026-01-04 12:31:00,216: t15.2024.03.08 val PER: 0.2589
2026-01-04 12:31:00,216: t15.2024.03.15 val PER: 0.2320
2026-01-04 12:31:00,216: t15.2024.03.17 val PER: 0.1695
2026-01-04 12:31:00,217: t15.2024.05.10 val PER: 0.1917
2026-01-04 12:31:00,217: t15.2024.06.14 val PER: 0.1830
2026-01-04 12:31:00,217: t15.2024.07.19 val PER: 0.2663
2026-01-04 12:31:00,217: t15.2024.07.21 val PER: 0.1179
2026-01-04 12:31:00,217: t15.2024.07.28 val PER: 0.1551
2026-01-04 12:31:00,217: t15.2025.01.10 val PER: 0.3154
2026-01-04 12:31:00,217: t15.2025.01.12 val PER: 0.1771
2026-01-04 12:31:00,217: t15.2025.03.14 val PER: 0.3506
2026-01-04 12:31:00,218: t15.2025.03.16 val PER: 0.2068
2026-01-04 12:31:00,218: t15.2025.03.30 val PER: 0.3368
2026-01-04 12:31:00,218: t15.2025.04.13 val PER: 0.2439
2026-01-04 12:31:00,505: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_9000
2026-01-04 12:31:21,405: Train batch 9200: loss: 10.58 grad norm: 49.89 time: 0.058
2026-01-04 12:31:42,186: Train batch 9400: loss: 7.92 grad norm: 47.92 time: 0.069
2026-01-04 12:31:52,579: Running test after training batch: 9500
2026-01-04 12:31:52,757: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:31:58,036: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 12:31:58,115: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost it
2026-01-04 12:32:02,257: Val batch 9500: PER (avg): 0.1733 CTC Loss (avg): 17.3438 WER(1gram): 50.25% (n=64) time: 9.677
2026-01-04 12:32:02,258: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 12:32:02,258: t15.2023.08.13 val PER: 0.1351
2026-01-04 12:32:02,258: t15.2023.08.18 val PER: 0.1199
2026-01-04 12:32:02,258: t15.2023.08.20 val PER: 0.1287
2026-01-04 12:32:02,258: t15.2023.08.25 val PER: 0.1099
2026-01-04 12:32:02,259: t15.2023.08.27 val PER: 0.1977
2026-01-04 12:32:02,259: t15.2023.09.01 val PER: 0.1006
2026-01-04 12:32:02,259: t15.2023.09.03 val PER: 0.1853
2026-01-04 12:32:02,259: t15.2023.09.24 val PER: 0.1553
2026-01-04 12:32:02,259: t15.2023.09.29 val PER: 0.1474
2026-01-04 12:32:02,259: t15.2023.10.01 val PER: 0.1889
2026-01-04 12:32:02,259: t15.2023.10.06 val PER: 0.1044
2026-01-04 12:32:02,259: t15.2023.10.08 val PER: 0.2679
2026-01-04 12:32:02,259: t15.2023.10.13 val PER: 0.2358
2026-01-04 12:32:02,260: t15.2023.10.15 val PER: 0.1859
2026-01-04 12:32:02,260: t15.2023.10.20 val PER: 0.1879
2026-01-04 12:32:02,260: t15.2023.10.22 val PER: 0.1292
2026-01-04 12:32:02,260: t15.2023.11.03 val PER: 0.1967
2026-01-04 12:32:02,260: t15.2023.11.04 val PER: 0.0307
2026-01-04 12:32:02,260: t15.2023.11.17 val PER: 0.0529
2026-01-04 12:32:02,260: t15.2023.11.19 val PER: 0.0479
2026-01-04 12:32:02,260: t15.2023.11.26 val PER: 0.1594
2026-01-04 12:32:02,260: t15.2023.12.03 val PER: 0.1502
2026-01-04 12:32:02,260: t15.2023.12.08 val PER: 0.1358
2026-01-04 12:32:02,260: t15.2023.12.10 val PER: 0.1275
2026-01-04 12:32:02,260: t15.2023.12.17 val PER: 0.1590
2026-01-04 12:32:02,260: t15.2023.12.29 val PER: 0.1572
2026-01-04 12:32:02,260: t15.2024.02.25 val PER: 0.1278
2026-01-04 12:32:02,261: t15.2024.03.08 val PER: 0.2589
2026-01-04 12:32:02,261: t15.2024.03.15 val PER: 0.2295
2026-01-04 12:32:02,261: t15.2024.03.17 val PER: 0.1569
2026-01-04 12:32:02,261: t15.2024.05.10 val PER: 0.1828
2026-01-04 12:32:02,261: t15.2024.06.14 val PER: 0.1798
2026-01-04 12:32:02,261: t15.2024.07.19 val PER: 0.2643
2026-01-04 12:32:02,261: t15.2024.07.21 val PER: 0.1097
2026-01-04 12:32:02,261: t15.2024.07.28 val PER: 0.1574
2026-01-04 12:32:02,261: t15.2025.01.10 val PER: 0.3085
2026-01-04 12:32:02,261: t15.2025.01.12 val PER: 0.1701
2026-01-04 12:32:02,261: t15.2025.03.14 val PER: 0.3669
2026-01-04 12:32:02,261: t15.2025.03.16 val PER: 0.2094
2026-01-04 12:32:02,261: t15.2025.03.30 val PER: 0.3126
2026-01-04 12:32:02,261: t15.2025.04.13 val PER: 0.2268
2026-01-04 12:32:02,263: New best val WER(1gram) 50.76% --> 50.25%
2026-01-04 12:32:02,263: Checkpointing model
2026-01-04 12:32:02,941: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:32:03,238: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_9500
2026-01-04 12:32:13,568: Train batch 9600: loss: 8.60 grad norm: 43.07 time: 0.075
2026-01-04 12:32:34,466: Train batch 9800: loss: 12.51 grad norm: 58.65 time: 0.065
2026-01-04 12:32:55,672: Train batch 10000: loss: 5.28 grad norm: 36.43 time: 0.061
2026-01-04 12:32:55,673: Running test after training batch: 10000
2026-01-04 12:32:55,799: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:33:01,076: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 12:33:01,156: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sindt
2026-01-04 12:33:05,293: Val batch 10000: PER (avg): 0.1691 CTC Loss (avg): 16.8007 WER(1gram): 51.27% (n=64) time: 9.620
2026-01-04 12:33:05,295: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 12:33:05,295: t15.2023.08.13 val PER: 0.1372
2026-01-04 12:33:05,295: t15.2023.08.18 val PER: 0.1257
2026-01-04 12:33:05,296: t15.2023.08.20 val PER: 0.1239
2026-01-04 12:33:05,296: t15.2023.08.25 val PER: 0.1099
2026-01-04 12:33:05,296: t15.2023.08.27 val PER: 0.2106
2026-01-04 12:33:05,296: t15.2023.09.01 val PER: 0.0869
2026-01-04 12:33:05,296: t15.2023.09.03 val PER: 0.1793
2026-01-04 12:33:05,296: t15.2023.09.24 val PER: 0.1529
2026-01-04 12:33:05,296: t15.2023.09.29 val PER: 0.1461
2026-01-04 12:33:05,296: t15.2023.10.01 val PER: 0.1849
2026-01-04 12:33:05,297: t15.2023.10.06 val PER: 0.1066
2026-01-04 12:33:05,297: t15.2023.10.08 val PER: 0.2463
2026-01-04 12:33:05,297: t15.2023.10.13 val PER: 0.2250
2026-01-04 12:33:05,297: t15.2023.10.15 val PER: 0.1721
2026-01-04 12:33:05,297: t15.2023.10.20 val PER: 0.1812
2026-01-04 12:33:05,297: t15.2023.10.22 val PER: 0.1359
2026-01-04 12:33:05,297: t15.2023.11.03 val PER: 0.1859
2026-01-04 12:33:05,298: t15.2023.11.04 val PER: 0.0307
2026-01-04 12:33:05,298: t15.2023.11.17 val PER: 0.0529
2026-01-04 12:33:05,298: t15.2023.11.19 val PER: 0.0539
2026-01-04 12:33:05,298: t15.2023.11.26 val PER: 0.1514
2026-01-04 12:33:05,298: t15.2023.12.03 val PER: 0.1324
2026-01-04 12:33:05,298: t15.2023.12.08 val PER: 0.1252
2026-01-04 12:33:05,298: t15.2023.12.10 val PER: 0.1130
2026-01-04 12:33:05,298: t15.2023.12.17 val PER: 0.1435
2026-01-04 12:33:05,298: t15.2023.12.29 val PER: 0.1489
2026-01-04 12:33:05,299: t15.2024.02.25 val PER: 0.1489
2026-01-04 12:33:05,299: t15.2024.03.08 val PER: 0.2461
2026-01-04 12:33:05,299: t15.2024.03.15 val PER: 0.2239
2026-01-04 12:33:05,299: t15.2024.03.17 val PER: 0.1541
2026-01-04 12:33:05,299: t15.2024.05.10 val PER: 0.1872
2026-01-04 12:33:05,299: t15.2024.06.14 val PER: 0.1688
2026-01-04 12:33:05,299: t15.2024.07.19 val PER: 0.2617
2026-01-04 12:33:05,299: t15.2024.07.21 val PER: 0.1159
2026-01-04 12:33:05,299: t15.2024.07.28 val PER: 0.1529
2026-01-04 12:33:05,299: t15.2025.01.10 val PER: 0.2906
2026-01-04 12:33:05,299: t15.2025.01.12 val PER: 0.1771
2026-01-04 12:33:05,299: t15.2025.03.14 val PER: 0.3521
2026-01-04 12:33:05,300: t15.2025.03.16 val PER: 0.2160
2026-01-04 12:33:05,300: t15.2025.03.30 val PER: 0.3080
2026-01-04 12:33:05,300: t15.2025.04.13 val PER: 0.2454
2026-01-04 12:33:05,589: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_10000
2026-01-04 12:33:26,388: Train batch 10200: loss: 6.57 grad norm: 40.53 time: 0.052
2026-01-04 12:33:47,599: Train batch 10400: loss: 9.42 grad norm: 49.82 time: 0.075
2026-01-04 12:33:58,222: Running test after training batch: 10500
2026-01-04 12:33:58,395: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:34:03,718: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:34:03,797: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost it
2026-01-04 12:34:07,994: Val batch 10500: PER (avg): 0.1662 CTC Loss (avg): 16.7246 WER(1gram): 50.76% (n=64) time: 9.772
2026-01-04 12:34:07,995: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 12:34:07,995: t15.2023.08.13 val PER: 0.1289
2026-01-04 12:34:07,995: t15.2023.08.18 val PER: 0.1266
2026-01-04 12:34:07,996: t15.2023.08.20 val PER: 0.1199
2026-01-04 12:34:07,996: t15.2023.08.25 val PER: 0.1130
2026-01-04 12:34:07,996: t15.2023.08.27 val PER: 0.2090
2026-01-04 12:34:07,996: t15.2023.09.01 val PER: 0.0942
2026-01-04 12:34:07,996: t15.2023.09.03 val PER: 0.1770
2026-01-04 12:34:07,996: t15.2023.09.24 val PER: 0.1505
2026-01-04 12:34:07,996: t15.2023.09.29 val PER: 0.1493
2026-01-04 12:34:07,996: t15.2023.10.01 val PER: 0.1849
2026-01-04 12:34:07,996: t15.2023.10.06 val PER: 0.0904
2026-01-04 12:34:07,996: t15.2023.10.08 val PER: 0.2463
2026-01-04 12:34:07,996: t15.2023.10.13 val PER: 0.2141
2026-01-04 12:34:07,997: t15.2023.10.15 val PER: 0.1767
2026-01-04 12:34:07,997: t15.2023.10.20 val PER: 0.1812
2026-01-04 12:34:07,997: t15.2023.10.22 val PER: 0.1247
2026-01-04 12:34:07,997: t15.2023.11.03 val PER: 0.2015
2026-01-04 12:34:07,997: t15.2023.11.04 val PER: 0.0375
2026-01-04 12:34:07,997: t15.2023.11.17 val PER: 0.0544
2026-01-04 12:34:07,997: t15.2023.11.19 val PER: 0.0559
2026-01-04 12:34:07,998: t15.2023.11.26 val PER: 0.1362
2026-01-04 12:34:07,998: t15.2023.12.03 val PER: 0.1334
2026-01-04 12:34:07,998: t15.2023.12.08 val PER: 0.1158
2026-01-04 12:34:07,998: t15.2023.12.10 val PER: 0.0986
2026-01-04 12:34:07,998: t15.2023.12.17 val PER: 0.1424
2026-01-04 12:34:07,998: t15.2023.12.29 val PER: 0.1572
2026-01-04 12:34:07,998: t15.2024.02.25 val PER: 0.1264
2026-01-04 12:34:07,998: t15.2024.03.08 val PER: 0.2447
2026-01-04 12:34:07,998: t15.2024.03.15 val PER: 0.2189
2026-01-04 12:34:07,998: t15.2024.03.17 val PER: 0.1597
2026-01-04 12:34:07,998: t15.2024.05.10 val PER: 0.1828
2026-01-04 12:34:07,998: t15.2024.06.14 val PER: 0.1719
2026-01-04 12:34:07,999: t15.2024.07.19 val PER: 0.2446
2026-01-04 12:34:07,999: t15.2024.07.21 val PER: 0.1041
2026-01-04 12:34:07,999: t15.2024.07.28 val PER: 0.1412
2026-01-04 12:34:07,999: t15.2025.01.10 val PER: 0.3168
2026-01-04 12:34:07,999: t15.2025.01.12 val PER: 0.1686
2026-01-04 12:34:07,999: t15.2025.03.14 val PER: 0.3639
2026-01-04 12:34:08,000: t15.2025.03.16 val PER: 0.1990
2026-01-04 12:34:08,000: t15.2025.03.30 val PER: 0.3172
2026-01-04 12:34:08,000: t15.2025.04.13 val PER: 0.2282
2026-01-04 12:34:08,282: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_10500
2026-01-04 12:34:18,927: Train batch 10600: loss: 9.31 grad norm: 55.76 time: 0.072
2026-01-04 12:34:40,531: Train batch 10800: loss: 14.96 grad norm: 64.07 time: 0.068
2026-01-04 12:35:01,492: Train batch 11000: loss: 14.76 grad norm: 60.69 time: 0.060
2026-01-04 12:35:01,493: Running test after training batch: 11000
2026-01-04 12:35:01,611: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:35:07,019: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:35:07,100: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 12:35:11,236: Val batch 11000: PER (avg): 0.1655 CTC Loss (avg): 16.5001 WER(1gram): 48.73% (n=64) time: 9.743
2026-01-04 12:35:11,236: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 12:35:11,237: t15.2023.08.13 val PER: 0.1310
2026-01-04 12:35:11,237: t15.2023.08.18 val PER: 0.1207
2026-01-04 12:35:11,237: t15.2023.08.20 val PER: 0.1223
2026-01-04 12:35:11,237: t15.2023.08.25 val PER: 0.0979
2026-01-04 12:35:11,237: t15.2023.08.27 val PER: 0.1961
2026-01-04 12:35:11,237: t15.2023.09.01 val PER: 0.0844
2026-01-04 12:35:11,238: t15.2023.09.03 val PER: 0.1793
2026-01-04 12:35:11,238: t15.2023.09.24 val PER: 0.1396
2026-01-04 12:35:11,238: t15.2023.09.29 val PER: 0.1410
2026-01-04 12:35:11,238: t15.2023.10.01 val PER: 0.1856
2026-01-04 12:35:11,238: t15.2023.10.06 val PER: 0.0947
2026-01-04 12:35:11,239: t15.2023.10.08 val PER: 0.2612
2026-01-04 12:35:11,239: t15.2023.10.13 val PER: 0.2203
2026-01-04 12:35:11,239: t15.2023.10.15 val PER: 0.1740
2026-01-04 12:35:11,239: t15.2023.10.20 val PER: 0.1846
2026-01-04 12:35:11,239: t15.2023.10.22 val PER: 0.1281
2026-01-04 12:35:11,239: t15.2023.11.03 val PER: 0.1839
2026-01-04 12:35:11,239: t15.2023.11.04 val PER: 0.0375
2026-01-04 12:35:11,240: t15.2023.11.17 val PER: 0.0467
2026-01-04 12:35:11,240: t15.2023.11.19 val PER: 0.0479
2026-01-04 12:35:11,240: t15.2023.11.26 val PER: 0.1420
2026-01-04 12:35:11,241: t15.2023.12.03 val PER: 0.1376
2026-01-04 12:35:11,241: t15.2023.12.08 val PER: 0.1238
2026-01-04 12:35:11,241: t15.2023.12.10 val PER: 0.0933
2026-01-04 12:35:11,241: t15.2023.12.17 val PER: 0.1507
2026-01-04 12:35:11,241: t15.2023.12.29 val PER: 0.1428
2026-01-04 12:35:11,241: t15.2024.02.25 val PER: 0.1362
2026-01-04 12:35:11,241: t15.2024.03.08 val PER: 0.2333
2026-01-04 12:35:11,241: t15.2024.03.15 val PER: 0.2214
2026-01-04 12:35:11,241: t15.2024.03.17 val PER: 0.1625
2026-01-04 12:35:11,241: t15.2024.05.10 val PER: 0.1753
2026-01-04 12:35:11,242: t15.2024.06.14 val PER: 0.1688
2026-01-04 12:35:11,242: t15.2024.07.19 val PER: 0.2505
2026-01-04 12:35:11,242: t15.2024.07.21 val PER: 0.1124
2026-01-04 12:35:11,242: t15.2024.07.28 val PER: 0.1529
2026-01-04 12:35:11,242: t15.2025.01.10 val PER: 0.3209
2026-01-04 12:35:11,242: t15.2025.01.12 val PER: 0.1624
2026-01-04 12:35:11,242: t15.2025.03.14 val PER: 0.3595
2026-01-04 12:35:11,242: t15.2025.03.16 val PER: 0.2029
2026-01-04 12:35:11,242: t15.2025.03.30 val PER: 0.3080
2026-01-04 12:35:11,243: t15.2025.04.13 val PER: 0.2282
2026-01-04 12:35:11,243: New best val WER(1gram) 50.25% --> 48.73%
2026-01-04 12:35:11,243: Checkpointing model
2026-01-04 12:35:11,935: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:35:12,229: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_11000
2026-01-04 12:35:33,343: Train batch 11200: loss: 10.80 grad norm: 52.63 time: 0.073
2026-01-04 12:35:54,388: Train batch 11400: loss: 9.59 grad norm: 50.88 time: 0.058
2026-01-04 12:36:05,065: Running test after training batch: 11500
2026-01-04 12:36:05,186: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:36:10,691: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 12:36:10,772: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost nit
2026-01-04 12:36:15,002: Val batch 11500: PER (avg): 0.1629 CTC Loss (avg): 16.3428 WER(1gram): 48.48% (n=64) time: 9.937
2026-01-04 12:36:15,003: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 12:36:15,003: t15.2023.08.13 val PER: 0.1268
2026-01-04 12:36:15,003: t15.2023.08.18 val PER: 0.1165
2026-01-04 12:36:15,003: t15.2023.08.20 val PER: 0.1223
2026-01-04 12:36:15,003: t15.2023.08.25 val PER: 0.0979
2026-01-04 12:36:15,004: t15.2023.08.27 val PER: 0.1961
2026-01-04 12:36:15,004: t15.2023.09.01 val PER: 0.0844
2026-01-04 12:36:15,004: t15.2023.09.03 val PER: 0.1793
2026-01-04 12:36:15,004: t15.2023.09.24 val PER: 0.1311
2026-01-04 12:36:15,005: t15.2023.09.29 val PER: 0.1442
2026-01-04 12:36:15,005: t15.2023.10.01 val PER: 0.1797
2026-01-04 12:36:15,005: t15.2023.10.06 val PER: 0.0883
2026-01-04 12:36:15,005: t15.2023.10.08 val PER: 0.2571
2026-01-04 12:36:15,005: t15.2023.10.13 val PER: 0.2172
2026-01-04 12:36:15,005: t15.2023.10.15 val PER: 0.1648
2026-01-04 12:36:15,006: t15.2023.10.20 val PER: 0.1946
2026-01-04 12:36:15,006: t15.2023.10.22 val PER: 0.1214
2026-01-04 12:36:15,006: t15.2023.11.03 val PER: 0.1825
2026-01-04 12:36:15,006: t15.2023.11.04 val PER: 0.0273
2026-01-04 12:36:15,006: t15.2023.11.17 val PER: 0.0498
2026-01-04 12:36:15,006: t15.2023.11.19 val PER: 0.0499
2026-01-04 12:36:15,006: t15.2023.11.26 val PER: 0.1355
2026-01-04 12:36:15,006: t15.2023.12.03 val PER: 0.1239
2026-01-04 12:36:15,006: t15.2023.12.08 val PER: 0.1178
2026-01-04 12:36:15,007: t15.2023.12.10 val PER: 0.0999
2026-01-04 12:36:15,007: t15.2023.12.17 val PER: 0.1559
2026-01-04 12:36:15,007: t15.2023.12.29 val PER: 0.1414
2026-01-04 12:36:15,007: t15.2024.02.25 val PER: 0.1194
2026-01-04 12:36:15,007: t15.2024.03.08 val PER: 0.2404
2026-01-04 12:36:15,007: t15.2024.03.15 val PER: 0.2214
2026-01-04 12:36:15,007: t15.2024.03.17 val PER: 0.1583
2026-01-04 12:36:15,007: t15.2024.05.10 val PER: 0.1872
2026-01-04 12:36:15,007: t15.2024.06.14 val PER: 0.1798
2026-01-04 12:36:15,008: t15.2024.07.19 val PER: 0.2452
2026-01-04 12:36:15,008: t15.2024.07.21 val PER: 0.1021
2026-01-04 12:36:15,008: t15.2024.07.28 val PER: 0.1426
2026-01-04 12:36:15,008: t15.2025.01.10 val PER: 0.3251
2026-01-04 12:36:15,008: t15.2025.01.12 val PER: 0.1663
2026-01-04 12:36:15,008: t15.2025.03.14 val PER: 0.3580
2026-01-04 12:36:15,008: t15.2025.03.16 val PER: 0.2055
2026-01-04 12:36:15,008: t15.2025.03.30 val PER: 0.3057
2026-01-04 12:36:15,008: t15.2025.04.13 val PER: 0.2225
2026-01-04 12:36:15,009: New best val WER(1gram) 48.73% --> 48.48%
2026-01-04 12:36:15,009: Checkpointing model
2026-01-04 12:36:15,679: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:36:15,975: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_11500
2026-01-04 12:36:26,104: Train batch 11600: loss: 11.21 grad norm: 49.54 time: 0.063
2026-01-04 12:36:47,054: Train batch 11800: loss: 6.78 grad norm: 39.84 time: 0.045
2026-01-04 12:37:07,751: Train batch 12000: loss: 13.92 grad norm: 52.43 time: 0.073
2026-01-04 12:37:07,751: Running test after training batch: 12000
2026-01-04 12:37:07,878: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:37:13,161: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 12:37:13,240: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nit
2026-01-04 12:37:17,392: Val batch 12000: PER (avg): 0.1602 CTC Loss (avg): 16.1757 WER(1gram): 51.27% (n=64) time: 9.640
2026-01-04 12:37:17,392: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 12:37:17,393: t15.2023.08.13 val PER: 0.1268
2026-01-04 12:37:17,393: t15.2023.08.18 val PER: 0.1048
2026-01-04 12:37:17,393: t15.2023.08.20 val PER: 0.1183
2026-01-04 12:37:17,393: t15.2023.08.25 val PER: 0.1039
2026-01-04 12:37:17,393: t15.2023.08.27 val PER: 0.1913
2026-01-04 12:37:17,393: t15.2023.09.01 val PER: 0.0852
2026-01-04 12:37:17,394: t15.2023.09.03 val PER: 0.1615
2026-01-04 12:37:17,394: t15.2023.09.24 val PER: 0.1286
2026-01-04 12:37:17,394: t15.2023.09.29 val PER: 0.1417
2026-01-04 12:37:17,394: t15.2023.10.01 val PER: 0.1790
2026-01-04 12:37:17,394: t15.2023.10.06 val PER: 0.0926
2026-01-04 12:37:17,394: t15.2023.10.08 val PER: 0.2409
2026-01-04 12:37:17,394: t15.2023.10.13 val PER: 0.2079
2026-01-04 12:37:17,394: t15.2023.10.15 val PER: 0.1694
2026-01-04 12:37:17,394: t15.2023.10.20 val PER: 0.2047
2026-01-04 12:37:17,394: t15.2023.10.22 val PER: 0.1225
2026-01-04 12:37:17,394: t15.2023.11.03 val PER: 0.1872
2026-01-04 12:37:17,394: t15.2023.11.04 val PER: 0.0273
2026-01-04 12:37:17,394: t15.2023.11.17 val PER: 0.0451
2026-01-04 12:37:17,395: t15.2023.11.19 val PER: 0.0359
2026-01-04 12:37:17,395: t15.2023.11.26 val PER: 0.1304
2026-01-04 12:37:17,395: t15.2023.12.03 val PER: 0.1239
2026-01-04 12:37:17,395: t15.2023.12.08 val PER: 0.1192
2026-01-04 12:37:17,395: t15.2023.12.10 val PER: 0.0972
2026-01-04 12:37:17,395: t15.2023.12.17 val PER: 0.1414
2026-01-04 12:37:17,395: t15.2023.12.29 val PER: 0.1386
2026-01-04 12:37:17,395: t15.2024.02.25 val PER: 0.1348
2026-01-04 12:37:17,395: t15.2024.03.08 val PER: 0.2404
2026-01-04 12:37:17,395: t15.2024.03.15 val PER: 0.2170
2026-01-04 12:37:17,395: t15.2024.03.17 val PER: 0.1478
2026-01-04 12:37:17,395: t15.2024.05.10 val PER: 0.1932
2026-01-04 12:37:17,395: t15.2024.06.14 val PER: 0.1909
2026-01-04 12:37:17,396: t15.2024.07.19 val PER: 0.2439
2026-01-04 12:37:17,396: t15.2024.07.21 val PER: 0.1048
2026-01-04 12:37:17,396: t15.2024.07.28 val PER: 0.1544
2026-01-04 12:37:17,396: t15.2025.01.10 val PER: 0.2906
2026-01-04 12:37:17,396: t15.2025.01.12 val PER: 0.1501
2026-01-04 12:37:17,396: t15.2025.03.14 val PER: 0.3550
2026-01-04 12:37:17,396: t15.2025.03.16 val PER: 0.2003
2026-01-04 12:37:17,396: t15.2025.03.30 val PER: 0.3023
2026-01-04 12:37:17,396: t15.2025.04.13 val PER: 0.2240
2026-01-04 12:37:17,683: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_12000
2026-01-04 12:37:38,518: Train batch 12200: loss: 5.63 grad norm: 40.07 time: 0.067
2026-01-04 12:37:59,140: Train batch 12400: loss: 5.01 grad norm: 35.64 time: 0.043
2026-01-04 12:38:09,834: Running test after training batch: 12500
2026-01-04 12:38:10,019: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:38:15,349: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 12:38:15,428: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-04 12:38:19,893: Val batch 12500: PER (avg): 0.1576 CTC Loss (avg): 15.9787 WER(1gram): 48.22% (n=64) time: 10.058
2026-01-04 12:38:19,893: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 12:38:19,893: t15.2023.08.13 val PER: 0.1175
2026-01-04 12:38:19,894: t15.2023.08.18 val PER: 0.1132
2026-01-04 12:38:19,894: t15.2023.08.20 val PER: 0.1199
2026-01-04 12:38:19,894: t15.2023.08.25 val PER: 0.0934
2026-01-04 12:38:19,894: t15.2023.08.27 val PER: 0.1994
2026-01-04 12:38:19,894: t15.2023.09.01 val PER: 0.0869
2026-01-04 12:38:19,894: t15.2023.09.03 val PER: 0.1675
2026-01-04 12:38:19,894: t15.2023.09.24 val PER: 0.1335
2026-01-04 12:38:19,895: t15.2023.09.29 val PER: 0.1353
2026-01-04 12:38:19,895: t15.2023.10.01 val PER: 0.1731
2026-01-04 12:38:19,895: t15.2023.10.06 val PER: 0.0840
2026-01-04 12:38:19,895: t15.2023.10.08 val PER: 0.2558
2026-01-04 12:38:19,895: t15.2023.10.13 val PER: 0.2180
2026-01-04 12:38:19,895: t15.2023.10.15 val PER: 0.1602
2026-01-04 12:38:19,895: t15.2023.10.20 val PER: 0.1946
2026-01-04 12:38:19,895: t15.2023.10.22 val PER: 0.1258
2026-01-04 12:38:19,895: t15.2023.11.03 val PER: 0.1805
2026-01-04 12:38:19,895: t15.2023.11.04 val PER: 0.0341
2026-01-04 12:38:19,896: t15.2023.11.17 val PER: 0.0482
2026-01-04 12:38:19,896: t15.2023.11.19 val PER: 0.0379
2026-01-04 12:38:19,896: t15.2023.11.26 val PER: 0.1283
2026-01-04 12:38:19,896: t15.2023.12.03 val PER: 0.1218
2026-01-04 12:38:19,896: t15.2023.12.08 val PER: 0.1085
2026-01-04 12:38:19,896: t15.2023.12.10 val PER: 0.0959
2026-01-04 12:38:19,896: t15.2023.12.17 val PER: 0.1455
2026-01-04 12:38:19,896: t15.2023.12.29 val PER: 0.1366
2026-01-04 12:38:19,896: t15.2024.02.25 val PER: 0.1166
2026-01-04 12:38:19,896: t15.2024.03.08 val PER: 0.2304
2026-01-04 12:38:19,897: t15.2024.03.15 val PER: 0.2226
2026-01-04 12:38:19,897: t15.2024.03.17 val PER: 0.1478
2026-01-04 12:38:19,897: t15.2024.05.10 val PER: 0.1724
2026-01-04 12:38:19,897: t15.2024.06.14 val PER: 0.1719
2026-01-04 12:38:19,897: t15.2024.07.19 val PER: 0.2360
2026-01-04 12:38:19,897: t15.2024.07.21 val PER: 0.1007
2026-01-04 12:38:19,897: t15.2024.07.28 val PER: 0.1375
2026-01-04 12:38:19,897: t15.2025.01.10 val PER: 0.2893
2026-01-04 12:38:19,898: t15.2025.01.12 val PER: 0.1540
2026-01-04 12:38:19,898: t15.2025.03.14 val PER: 0.3624
2026-01-04 12:38:19,898: t15.2025.03.16 val PER: 0.2016
2026-01-04 12:38:19,898: t15.2025.03.30 val PER: 0.2954
2026-01-04 12:38:19,898: t15.2025.04.13 val PER: 0.2225
2026-01-04 12:38:19,899: New best val WER(1gram) 48.48% --> 48.22%
2026-01-04 12:38:19,899: Checkpointing model
2026-01-04 12:38:20,578: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:38:20,886: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_12500
2026-01-04 12:38:31,319: Train batch 12600: loss: 7.94 grad norm: 44.60 time: 0.059
2026-01-04 12:38:52,345: Train batch 12800: loss: 5.95 grad norm: 39.47 time: 0.053
2026-01-04 12:39:13,616: Train batch 13000: loss: 6.62 grad norm: 41.86 time: 0.068
2026-01-04 12:39:13,617: Running test after training batch: 13000
2026-01-04 12:39:13,747: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:39:19,084: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 12:39:19,165: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost it
2026-01-04 12:39:23,580: Val batch 13000: PER (avg): 0.1574 CTC Loss (avg): 15.7913 WER(1gram): 46.45% (n=64) time: 9.963
2026-01-04 12:39:23,581: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 12:39:23,581: t15.2023.08.13 val PER: 0.1206
2026-01-04 12:39:23,582: t15.2023.08.18 val PER: 0.1165
2026-01-04 12:39:23,582: t15.2023.08.20 val PER: 0.1096
2026-01-04 12:39:23,582: t15.2023.08.25 val PER: 0.0979
2026-01-04 12:39:23,582: t15.2023.08.27 val PER: 0.1849
2026-01-04 12:39:23,583: t15.2023.09.01 val PER: 0.0885
2026-01-04 12:39:23,583: t15.2023.09.03 val PER: 0.1663
2026-01-04 12:39:23,583: t15.2023.09.24 val PER: 0.1323
2026-01-04 12:39:23,583: t15.2023.09.29 val PER: 0.1372
2026-01-04 12:39:23,583: t15.2023.10.01 val PER: 0.1790
2026-01-04 12:39:23,583: t15.2023.10.06 val PER: 0.0893
2026-01-04 12:39:23,583: t15.2023.10.08 val PER: 0.2463
2026-01-04 12:39:23,583: t15.2023.10.13 val PER: 0.2095
2026-01-04 12:39:23,583: t15.2023.10.15 val PER: 0.1602
2026-01-04 12:39:23,583: t15.2023.10.20 val PER: 0.1745
2026-01-04 12:39:23,584: t15.2023.10.22 val PER: 0.1203
2026-01-04 12:39:23,584: t15.2023.11.03 val PER: 0.1872
2026-01-04 12:39:23,584: t15.2023.11.04 val PER: 0.0341
2026-01-04 12:39:23,584: t15.2023.11.17 val PER: 0.0389
2026-01-04 12:39:23,584: t15.2023.11.19 val PER: 0.0479
2026-01-04 12:39:23,584: t15.2023.11.26 val PER: 0.1312
2026-01-04 12:39:23,584: t15.2023.12.03 val PER: 0.1197
2026-01-04 12:39:23,584: t15.2023.12.08 val PER: 0.1072
2026-01-04 12:39:23,585: t15.2023.12.10 val PER: 0.0972
2026-01-04 12:39:23,585: t15.2023.12.17 val PER: 0.1518
2026-01-04 12:39:23,585: t15.2023.12.29 val PER: 0.1400
2026-01-04 12:39:23,585: t15.2024.02.25 val PER: 0.1166
2026-01-04 12:39:23,585: t15.2024.03.08 val PER: 0.2404
2026-01-04 12:39:23,586: t15.2024.03.15 val PER: 0.2101
2026-01-04 12:39:23,586: t15.2024.03.17 val PER: 0.1478
2026-01-04 12:39:23,586: t15.2024.05.10 val PER: 0.1575
2026-01-04 12:39:23,586: t15.2024.06.14 val PER: 0.1688
2026-01-04 12:39:23,586: t15.2024.07.19 val PER: 0.2465
2026-01-04 12:39:23,586: t15.2024.07.21 val PER: 0.1000
2026-01-04 12:39:23,586: t15.2024.07.28 val PER: 0.1412
2026-01-04 12:39:23,586: t15.2025.01.10 val PER: 0.3030
2026-01-04 12:39:23,586: t15.2025.01.12 val PER: 0.1540
2026-01-04 12:39:23,587: t15.2025.03.14 val PER: 0.3476
2026-01-04 12:39:23,587: t15.2025.03.16 val PER: 0.1806
2026-01-04 12:39:23,587: t15.2025.03.30 val PER: 0.3069
2026-01-04 12:39:23,587: t15.2025.04.13 val PER: 0.2311
2026-01-04 12:39:23,587: New best val WER(1gram) 48.22% --> 46.45%
2026-01-04 12:39:23,587: Checkpointing model
2026-01-04 12:39:24,305: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:39:24,644: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_13000
2026-01-04 12:39:45,951: Train batch 13200: loss: 12.95 grad norm: 60.55 time: 0.054
2026-01-04 12:40:06,726: Train batch 13400: loss: 9.05 grad norm: 55.04 time: 0.064
2026-01-04 12:40:17,383: Running test after training batch: 13500
2026-01-04 12:40:17,503: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:40:22,931: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 12:40:23,015: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost nit
2026-01-04 12:40:27,578: Val batch 13500: PER (avg): 0.1530 CTC Loss (avg): 15.6116 WER(1gram): 48.98% (n=64) time: 10.195
2026-01-04 12:40:27,579: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 12:40:27,579: t15.2023.08.13 val PER: 0.1206
2026-01-04 12:40:27,579: t15.2023.08.18 val PER: 0.1039
2026-01-04 12:40:27,579: t15.2023.08.20 val PER: 0.1104
2026-01-04 12:40:27,579: t15.2023.08.25 val PER: 0.0979
2026-01-04 12:40:27,579: t15.2023.08.27 val PER: 0.1929
2026-01-04 12:40:27,579: t15.2023.09.01 val PER: 0.0852
2026-01-04 12:40:27,579: t15.2023.09.03 val PER: 0.1627
2026-01-04 12:40:27,580: t15.2023.09.24 val PER: 0.1286
2026-01-04 12:40:27,580: t15.2023.09.29 val PER: 0.1302
2026-01-04 12:40:27,580: t15.2023.10.01 val PER: 0.1684
2026-01-04 12:40:27,580: t15.2023.10.06 val PER: 0.0872
2026-01-04 12:40:27,580: t15.2023.10.08 val PER: 0.2503
2026-01-04 12:40:27,581: t15.2023.10.13 val PER: 0.2118
2026-01-04 12:40:27,581: t15.2023.10.15 val PER: 0.1582
2026-01-04 12:40:27,581: t15.2023.10.20 val PER: 0.1879
2026-01-04 12:40:27,581: t15.2023.10.22 val PER: 0.1169
2026-01-04 12:40:27,581: t15.2023.11.03 val PER: 0.1791
2026-01-04 12:40:27,581: t15.2023.11.04 val PER: 0.0341
2026-01-04 12:40:27,581: t15.2023.11.17 val PER: 0.0435
2026-01-04 12:40:27,581: t15.2023.11.19 val PER: 0.0259
2026-01-04 12:40:27,581: t15.2023.11.26 val PER: 0.1268
2026-01-04 12:40:27,582: t15.2023.12.03 val PER: 0.1187
2026-01-04 12:40:27,582: t15.2023.12.08 val PER: 0.1025
2026-01-04 12:40:27,582: t15.2023.12.10 val PER: 0.0894
2026-01-04 12:40:27,582: t15.2023.12.17 val PER: 0.1362
2026-01-04 12:40:27,582: t15.2023.12.29 val PER: 0.1283
2026-01-04 12:40:27,582: t15.2024.02.25 val PER: 0.1096
2026-01-04 12:40:27,582: t15.2024.03.08 val PER: 0.2432
2026-01-04 12:40:27,582: t15.2024.03.15 val PER: 0.2001
2026-01-04 12:40:27,582: t15.2024.03.17 val PER: 0.1430
2026-01-04 12:40:27,582: t15.2024.05.10 val PER: 0.1649
2026-01-04 12:40:27,583: t15.2024.06.14 val PER: 0.1640
2026-01-04 12:40:27,583: t15.2024.07.19 val PER: 0.2314
2026-01-04 12:40:27,583: t15.2024.07.21 val PER: 0.0993
2026-01-04 12:40:27,583: t15.2024.07.28 val PER: 0.1390
2026-01-04 12:40:27,583: t15.2025.01.10 val PER: 0.2989
2026-01-04 12:40:27,583: t15.2025.01.12 val PER: 0.1486
2026-01-04 12:40:27,583: t15.2025.03.14 val PER: 0.3550
2026-01-04 12:40:27,583: t15.2025.03.16 val PER: 0.1846
2026-01-04 12:40:27,583: t15.2025.03.30 val PER: 0.3034
2026-01-04 12:40:27,584: t15.2025.04.13 val PER: 0.2183
2026-01-04 12:40:27,887: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_13500
2026-01-04 12:40:38,636: Train batch 13600: loss: 12.77 grad norm: 62.57 time: 0.065
2026-01-04 12:40:59,856: Train batch 13800: loss: 9.18 grad norm: 55.80 time: 0.056
2026-01-04 12:41:20,836: Train batch 14000: loss: 11.96 grad norm: 58.64 time: 0.054
2026-01-04 12:41:20,837: Running test after training batch: 14000
2026-01-04 12:41:20,972: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:41:26,323: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:41:26,412: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost it
2026-01-04 12:41:31,057: Val batch 14000: PER (avg): 0.1538 CTC Loss (avg): 15.5570 WER(1gram): 46.95% (n=64) time: 10.220
2026-01-04 12:41:31,057: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 12:41:31,058: t15.2023.08.13 val PER: 0.1143
2026-01-04 12:41:31,058: t15.2023.08.18 val PER: 0.1031
2026-01-04 12:41:31,058: t15.2023.08.20 val PER: 0.1136
2026-01-04 12:41:31,058: t15.2023.08.25 val PER: 0.1024
2026-01-04 12:41:31,058: t15.2023.08.27 val PER: 0.1929
2026-01-04 12:41:31,058: t15.2023.09.01 val PER: 0.0804
2026-01-04 12:41:31,058: t15.2023.09.03 val PER: 0.1805
2026-01-04 12:41:31,058: t15.2023.09.24 val PER: 0.1311
2026-01-04 12:41:31,058: t15.2023.09.29 val PER: 0.1321
2026-01-04 12:41:31,058: t15.2023.10.01 val PER: 0.1777
2026-01-04 12:41:31,058: t15.2023.10.06 val PER: 0.0893
2026-01-04 12:41:31,058: t15.2023.10.08 val PER: 0.2530
2026-01-04 12:41:31,058: t15.2023.10.13 val PER: 0.2095
2026-01-04 12:41:31,059: t15.2023.10.15 val PER: 0.1543
2026-01-04 12:41:31,059: t15.2023.10.20 val PER: 0.1846
2026-01-04 12:41:31,059: t15.2023.10.22 val PER: 0.1147
2026-01-04 12:41:31,059: t15.2023.11.03 val PER: 0.1811
2026-01-04 12:41:31,059: t15.2023.11.04 val PER: 0.0307
2026-01-04 12:41:31,059: t15.2023.11.17 val PER: 0.0451
2026-01-04 12:41:31,059: t15.2023.11.19 val PER: 0.0379
2026-01-04 12:41:31,059: t15.2023.11.26 val PER: 0.1225
2026-01-04 12:41:31,059: t15.2023.12.03 val PER: 0.1197
2026-01-04 12:41:31,059: t15.2023.12.08 val PER: 0.1065
2026-01-04 12:41:31,059: t15.2023.12.10 val PER: 0.0933
2026-01-04 12:41:31,060: t15.2023.12.17 val PER: 0.1362
2026-01-04 12:41:31,060: t15.2023.12.29 val PER: 0.1311
2026-01-04 12:41:31,060: t15.2024.02.25 val PER: 0.1138
2026-01-04 12:41:31,060: t15.2024.03.08 val PER: 0.2290
2026-01-04 12:41:31,060: t15.2024.03.15 val PER: 0.2014
2026-01-04 12:41:31,060: t15.2024.03.17 val PER: 0.1444
2026-01-04 12:41:31,060: t15.2024.05.10 val PER: 0.1530
2026-01-04 12:41:31,060: t15.2024.06.14 val PER: 0.1672
2026-01-04 12:41:31,060: t15.2024.07.19 val PER: 0.2413
2026-01-04 12:41:31,060: t15.2024.07.21 val PER: 0.0897
2026-01-04 12:41:31,060: t15.2024.07.28 val PER: 0.1360
2026-01-04 12:41:31,060: t15.2025.01.10 val PER: 0.2975
2026-01-04 12:41:31,060: t15.2025.01.12 val PER: 0.1440
2026-01-04 12:41:31,061: t15.2025.03.14 val PER: 0.3536
2026-01-04 12:41:31,061: t15.2025.03.16 val PER: 0.1976
2026-01-04 12:41:31,061: t15.2025.03.30 val PER: 0.2966
2026-01-04 12:41:31,061: t15.2025.04.13 val PER: 0.2311
2026-01-04 12:41:31,374: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_14000
2026-01-04 12:41:49,723: Train batch 14200: loss: 8.47 grad norm: 52.02 time: 0.056
2026-01-04 12:42:10,707: Train batch 14400: loss: 5.95 grad norm: 40.61 time: 0.066
2026-01-04 12:42:21,278: Running test after training batch: 14500
2026-01-04 12:42:21,393: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:42:26,861: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:42:26,950: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost it
2026-01-04 12:42:31,668: Val batch 14500: PER (avg): 0.1534 CTC Loss (avg): 15.5960 WER(1gram): 46.70% (n=64) time: 10.389
2026-01-04 12:42:31,668: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 12:42:31,668: t15.2023.08.13 val PER: 0.1185
2026-01-04 12:42:31,668: t15.2023.08.18 val PER: 0.1048
2026-01-04 12:42:31,669: t15.2023.08.20 val PER: 0.1088
2026-01-04 12:42:31,669: t15.2023.08.25 val PER: 0.0873
2026-01-04 12:42:31,669: t15.2023.08.27 val PER: 0.1929
2026-01-04 12:42:31,669: t15.2023.09.01 val PER: 0.0869
2026-01-04 12:42:31,669: t15.2023.09.03 val PER: 0.1639
2026-01-04 12:42:31,669: t15.2023.09.24 val PER: 0.1311
2026-01-04 12:42:31,669: t15.2023.09.29 val PER: 0.1327
2026-01-04 12:42:31,669: t15.2023.10.01 val PER: 0.1797
2026-01-04 12:42:31,669: t15.2023.10.06 val PER: 0.0947
2026-01-04 12:42:31,669: t15.2023.10.08 val PER: 0.2463
2026-01-04 12:42:31,669: t15.2023.10.13 val PER: 0.2095
2026-01-04 12:42:31,669: t15.2023.10.15 val PER: 0.1615
2026-01-04 12:42:31,670: t15.2023.10.20 val PER: 0.1678
2026-01-04 12:42:31,670: t15.2023.10.22 val PER: 0.1180
2026-01-04 12:42:31,670: t15.2023.11.03 val PER: 0.1811
2026-01-04 12:42:31,670: t15.2023.11.04 val PER: 0.0410
2026-01-04 12:42:31,670: t15.2023.11.17 val PER: 0.0389
2026-01-04 12:42:31,670: t15.2023.11.19 val PER: 0.0419
2026-01-04 12:42:31,670: t15.2023.11.26 val PER: 0.1290
2026-01-04 12:42:31,670: t15.2023.12.03 val PER: 0.1113
2026-01-04 12:42:31,670: t15.2023.12.08 val PER: 0.1052
2026-01-04 12:42:31,670: t15.2023.12.10 val PER: 0.0880
2026-01-04 12:42:31,670: t15.2023.12.17 val PER: 0.1445
2026-01-04 12:42:31,670: t15.2023.12.29 val PER: 0.1332
2026-01-04 12:42:31,670: t15.2024.02.25 val PER: 0.1096
2026-01-04 12:42:31,671: t15.2024.03.08 val PER: 0.2504
2026-01-04 12:42:31,671: t15.2024.03.15 val PER: 0.2039
2026-01-04 12:42:31,671: t15.2024.03.17 val PER: 0.1388
2026-01-04 12:42:31,671: t15.2024.05.10 val PER: 0.1575
2026-01-04 12:42:31,671: t15.2024.06.14 val PER: 0.1688
2026-01-04 12:42:31,671: t15.2024.07.19 val PER: 0.2399
2026-01-04 12:42:31,671: t15.2024.07.21 val PER: 0.0876
2026-01-04 12:42:31,671: t15.2024.07.28 val PER: 0.1346
2026-01-04 12:42:31,671: t15.2025.01.10 val PER: 0.2975
2026-01-04 12:42:31,671: t15.2025.01.12 val PER: 0.1370
2026-01-04 12:42:31,671: t15.2025.03.14 val PER: 0.3550
2026-01-04 12:42:31,671: t15.2025.03.16 val PER: 0.1793
2026-01-04 12:42:31,671: t15.2025.03.30 val PER: 0.2966
2026-01-04 12:42:31,671: t15.2025.04.13 val PER: 0.2254
2026-01-04 12:42:31,993: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_14500
2026-01-04 12:42:42,633: Train batch 14600: loss: 12.04 grad norm: 57.06 time: 0.058
2026-01-04 12:43:03,474: Train batch 14800: loss: 5.96 grad norm: 45.20 time: 0.050
2026-01-04 12:43:24,139: Train batch 15000: loss: 8.83 grad norm: 53.68 time: 0.052
2026-01-04 12:43:24,139: Running test after training batch: 15000
2026-01-04 12:43:24,369: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:43:29,679: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:43:29,769: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost get
2026-01-04 12:43:34,584: Val batch 15000: PER (avg): 0.1499 CTC Loss (avg): 15.3253 WER(1gram): 46.19% (n=64) time: 10.445
2026-01-04 12:43:34,585: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 12:43:34,585: t15.2023.08.13 val PER: 0.1154
2026-01-04 12:43:34,585: t15.2023.08.18 val PER: 0.1023
2026-01-04 12:43:34,585: t15.2023.08.20 val PER: 0.1128
2026-01-04 12:43:34,585: t15.2023.08.25 val PER: 0.0934
2026-01-04 12:43:34,585: t15.2023.08.27 val PER: 0.1785
2026-01-04 12:43:34,586: t15.2023.09.01 val PER: 0.0844
2026-01-04 12:43:34,586: t15.2023.09.03 val PER: 0.1675
2026-01-04 12:43:34,586: t15.2023.09.24 val PER: 0.1311
2026-01-04 12:43:34,586: t15.2023.09.29 val PER: 0.1238
2026-01-04 12:43:34,586: t15.2023.10.01 val PER: 0.1777
2026-01-04 12:43:34,586: t15.2023.10.06 val PER: 0.0850
2026-01-04 12:43:34,586: t15.2023.10.08 val PER: 0.2544
2026-01-04 12:43:34,586: t15.2023.10.13 val PER: 0.2002
2026-01-04 12:43:34,586: t15.2023.10.15 val PER: 0.1523
2026-01-04 12:43:34,586: t15.2023.10.20 val PER: 0.2047
2026-01-04 12:43:34,587: t15.2023.10.22 val PER: 0.1169
2026-01-04 12:43:34,587: t15.2023.11.03 val PER: 0.1750
2026-01-04 12:43:34,587: t15.2023.11.04 val PER: 0.0341
2026-01-04 12:43:34,587: t15.2023.11.17 val PER: 0.0327
2026-01-04 12:43:34,587: t15.2023.11.19 val PER: 0.0319
2026-01-04 12:43:34,587: t15.2023.11.26 val PER: 0.1232
2026-01-04 12:43:34,587: t15.2023.12.03 val PER: 0.1155
2026-01-04 12:43:34,587: t15.2023.12.08 val PER: 0.0972
2026-01-04 12:43:34,587: t15.2023.12.10 val PER: 0.0933
2026-01-04 12:43:34,588: t15.2023.12.17 val PER: 0.1476
2026-01-04 12:43:34,588: t15.2023.12.29 val PER: 0.1270
2026-01-04 12:43:34,588: t15.2024.02.25 val PER: 0.1025
2026-01-04 12:43:34,588: t15.2024.03.08 val PER: 0.2219
2026-01-04 12:43:34,588: t15.2024.03.15 val PER: 0.1970
2026-01-04 12:43:34,588: t15.2024.03.17 val PER: 0.1353
2026-01-04 12:43:34,588: t15.2024.05.10 val PER: 0.1664
2026-01-04 12:43:34,588: t15.2024.06.14 val PER: 0.1703
2026-01-04 12:43:34,588: t15.2024.07.19 val PER: 0.2314
2026-01-04 12:43:34,588: t15.2024.07.21 val PER: 0.0897
2026-01-04 12:43:34,588: t15.2024.07.28 val PER: 0.1287
2026-01-04 12:43:34,588: t15.2025.01.10 val PER: 0.2948
2026-01-04 12:43:34,589: t15.2025.01.12 val PER: 0.1401
2026-01-04 12:43:34,589: t15.2025.03.14 val PER: 0.3565
2026-01-04 12:43:34,589: t15.2025.03.16 val PER: 0.1767
2026-01-04 12:43:34,589: t15.2025.03.30 val PER: 0.2874
2026-01-04 12:43:34,589: t15.2025.04.13 val PER: 0.2197
2026-01-04 12:43:34,590: New best val WER(1gram) 46.45% --> 46.19%
2026-01-04 12:43:34,590: Checkpointing model
2026-01-04 12:43:35,316: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:43:35,690: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_15000
2026-01-04 12:43:54,410: Train batch 15200: loss: 5.38 grad norm: 44.33 time: 0.058
2026-01-04 12:44:12,828: Train batch 15400: loss: 11.62 grad norm: 55.89 time: 0.050
2026-01-04 12:44:22,168: Running test after training batch: 15500
2026-01-04 12:44:22,348: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:44:27,698: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-04 12:44:27,794: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost it
2026-01-04 12:44:32,640: Val batch 15500: PER (avg): 0.1500 CTC Loss (avg): 15.2457 WER(1gram): 46.70% (n=64) time: 10.471
2026-01-04 12:44:32,640: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 12:44:32,641: t15.2023.08.13 val PER: 0.1102
2026-01-04 12:44:32,641: t15.2023.08.18 val PER: 0.0981
2026-01-04 12:44:32,641: t15.2023.08.20 val PER: 0.1096
2026-01-04 12:44:32,641: t15.2023.08.25 val PER: 0.0964
2026-01-04 12:44:32,641: t15.2023.08.27 val PER: 0.1881
2026-01-04 12:44:32,641: t15.2023.09.01 val PER: 0.0828
2026-01-04 12:44:32,641: t15.2023.09.03 val PER: 0.1722
2026-01-04 12:44:32,642: t15.2023.09.24 val PER: 0.1262
2026-01-04 12:44:32,642: t15.2023.09.29 val PER: 0.1257
2026-01-04 12:44:32,642: t15.2023.10.01 val PER: 0.1704
2026-01-04 12:44:32,642: t15.2023.10.06 val PER: 0.0829
2026-01-04 12:44:32,642: t15.2023.10.08 val PER: 0.2463
2026-01-04 12:44:32,642: t15.2023.10.13 val PER: 0.1994
2026-01-04 12:44:32,642: t15.2023.10.15 val PER: 0.1523
2026-01-04 12:44:32,642: t15.2023.10.20 val PER: 0.1846
2026-01-04 12:44:32,642: t15.2023.10.22 val PER: 0.1180
2026-01-04 12:44:32,643: t15.2023.11.03 val PER: 0.1737
2026-01-04 12:44:32,643: t15.2023.11.04 val PER: 0.0375
2026-01-04 12:44:32,643: t15.2023.11.17 val PER: 0.0373
2026-01-04 12:44:32,643: t15.2023.11.19 val PER: 0.0419
2026-01-04 12:44:32,643: t15.2023.11.26 val PER: 0.1196
2026-01-04 12:44:32,643: t15.2023.12.03 val PER: 0.1103
2026-01-04 12:44:32,643: t15.2023.12.08 val PER: 0.1072
2026-01-04 12:44:32,643: t15.2023.12.10 val PER: 0.0854
2026-01-04 12:44:32,643: t15.2023.12.17 val PER: 0.1435
2026-01-04 12:44:32,643: t15.2023.12.29 val PER: 0.1311
2026-01-04 12:44:32,643: t15.2024.02.25 val PER: 0.0997
2026-01-04 12:44:32,643: t15.2024.03.08 val PER: 0.2233
2026-01-04 12:44:32,644: t15.2024.03.15 val PER: 0.1989
2026-01-04 12:44:32,644: t15.2024.03.17 val PER: 0.1457
2026-01-04 12:44:32,644: t15.2024.05.10 val PER: 0.1560
2026-01-04 12:44:32,644: t15.2024.06.14 val PER: 0.1593
2026-01-04 12:44:32,644: t15.2024.07.19 val PER: 0.2426
2026-01-04 12:44:32,644: t15.2024.07.21 val PER: 0.0883
2026-01-04 12:44:32,644: t15.2024.07.28 val PER: 0.1390
2026-01-04 12:44:32,644: t15.2025.01.10 val PER: 0.2851
2026-01-04 12:44:32,644: t15.2025.01.12 val PER: 0.1517
2026-01-04 12:44:32,644: t15.2025.03.14 val PER: 0.3491
2026-01-04 12:44:32,645: t15.2025.03.16 val PER: 0.1741
2026-01-04 12:44:32,645: t15.2025.03.30 val PER: 0.2874
2026-01-04 12:44:32,645: t15.2025.04.13 val PER: 0.2097
2026-01-04 12:44:32,972: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_15500
2026-01-04 12:44:43,428: Train batch 15600: loss: 11.97 grad norm: 58.81 time: 0.064
2026-01-04 12:45:04,279: Train batch 15800: loss: 13.43 grad norm: 64.36 time: 0.069
2026-01-04 12:45:25,793: Train batch 16000: loss: 8.64 grad norm: 46.19 time: 0.057
2026-01-04 12:45:25,794: Running test after training batch: 16000
2026-01-04 12:45:25,960: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:45:31,379: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:45:31,475: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost et
2026-01-04 12:45:36,426: Val batch 16000: PER (avg): 0.1501 CTC Loss (avg): 15.3335 WER(1gram): 45.94% (n=64) time: 10.632
2026-01-04 12:45:36,427: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 12:45:36,427: t15.2023.08.13 val PER: 0.1175
2026-01-04 12:45:36,427: t15.2023.08.18 val PER: 0.1039
2026-01-04 12:45:36,427: t15.2023.08.20 val PER: 0.1064
2026-01-04 12:45:36,427: t15.2023.08.25 val PER: 0.0934
2026-01-04 12:45:36,428: t15.2023.08.27 val PER: 0.1913
2026-01-04 12:45:36,428: t15.2023.09.01 val PER: 0.0812
2026-01-04 12:45:36,428: t15.2023.09.03 val PER: 0.1627
2026-01-04 12:45:36,428: t15.2023.09.24 val PER: 0.1238
2026-01-04 12:45:36,428: t15.2023.09.29 val PER: 0.1276
2026-01-04 12:45:36,428: t15.2023.10.01 val PER: 0.1678
2026-01-04 12:45:36,428: t15.2023.10.06 val PER: 0.0883
2026-01-04 12:45:36,428: t15.2023.10.08 val PER: 0.2517
2026-01-04 12:45:36,428: t15.2023.10.13 val PER: 0.2009
2026-01-04 12:45:36,428: t15.2023.10.15 val PER: 0.1424
2026-01-04 12:45:36,428: t15.2023.10.20 val PER: 0.1846
2026-01-04 12:45:36,428: t15.2023.10.22 val PER: 0.1102
2026-01-04 12:45:36,428: t15.2023.11.03 val PER: 0.1716
2026-01-04 12:45:36,428: t15.2023.11.04 val PER: 0.0341
2026-01-04 12:45:36,428: t15.2023.11.17 val PER: 0.0389
2026-01-04 12:45:36,429: t15.2023.11.19 val PER: 0.0439
2026-01-04 12:45:36,429: t15.2023.11.26 val PER: 0.1188
2026-01-04 12:45:36,429: t15.2023.12.03 val PER: 0.1197
2026-01-04 12:45:36,429: t15.2023.12.08 val PER: 0.1052
2026-01-04 12:45:36,429: t15.2023.12.10 val PER: 0.0867
2026-01-04 12:45:36,429: t15.2023.12.17 val PER: 0.1507
2026-01-04 12:45:36,429: t15.2023.12.29 val PER: 0.1277
2026-01-04 12:45:36,429: t15.2024.02.25 val PER: 0.1011
2026-01-04 12:45:36,429: t15.2024.03.08 val PER: 0.2390
2026-01-04 12:45:36,429: t15.2024.03.15 val PER: 0.2039
2026-01-04 12:45:36,429: t15.2024.03.17 val PER: 0.1409
2026-01-04 12:45:36,430: t15.2024.05.10 val PER: 0.1634
2026-01-04 12:45:36,430: t15.2024.06.14 val PER: 0.1577
2026-01-04 12:45:36,430: t15.2024.07.19 val PER: 0.2327
2026-01-04 12:45:36,430: t15.2024.07.21 val PER: 0.0869
2026-01-04 12:45:36,430: t15.2024.07.28 val PER: 0.1324
2026-01-04 12:45:36,430: t15.2025.01.10 val PER: 0.3030
2026-01-04 12:45:36,430: t15.2025.01.12 val PER: 0.1478
2026-01-04 12:45:36,430: t15.2025.03.14 val PER: 0.3328
2026-01-04 12:45:36,430: t15.2025.03.16 val PER: 0.1937
2026-01-04 12:45:36,430: t15.2025.03.30 val PER: 0.2920
2026-01-04 12:45:36,430: t15.2025.04.13 val PER: 0.2168
2026-01-04 12:45:36,431: New best val WER(1gram) 46.19% --> 45.94%
2026-01-04 12:45:36,431: Checkpointing model
2026-01-04 12:45:37,099: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:45:37,451: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_16000
2026-01-04 12:45:58,677: Train batch 16200: loss: 6.36 grad norm: 44.96 time: 0.057
2026-01-04 12:46:19,846: Train batch 16400: loss: 10.63 grad norm: 63.15 time: 0.057
2026-01-04 12:46:30,461: Running test after training batch: 16500
2026-01-04 12:46:30,691: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:46:36,078: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:46:36,172: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost it
2026-01-04 12:46:41,125: Val batch 16500: PER (avg): 0.1484 CTC Loss (avg): 15.1671 WER(1gram): 43.91% (n=64) time: 10.663
2026-01-04 12:46:41,125: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 12:46:41,126: t15.2023.08.13 val PER: 0.1091
2026-01-04 12:46:41,126: t15.2023.08.18 val PER: 0.1014
2026-01-04 12:46:41,126: t15.2023.08.20 val PER: 0.1056
2026-01-04 12:46:41,126: t15.2023.08.25 val PER: 0.0873
2026-01-04 12:46:41,126: t15.2023.08.27 val PER: 0.1833
2026-01-04 12:46:41,126: t15.2023.09.01 val PER: 0.0860
2026-01-04 12:46:41,126: t15.2023.09.03 val PER: 0.1675
2026-01-04 12:46:41,126: t15.2023.09.24 val PER: 0.1286
2026-01-04 12:46:41,126: t15.2023.09.29 val PER: 0.1289
2026-01-04 12:46:41,126: t15.2023.10.01 val PER: 0.1731
2026-01-04 12:46:41,126: t15.2023.10.06 val PER: 0.0861
2026-01-04 12:46:41,126: t15.2023.10.08 val PER: 0.2449
2026-01-04 12:46:41,126: t15.2023.10.13 val PER: 0.1978
2026-01-04 12:46:41,127: t15.2023.10.15 val PER: 0.1536
2026-01-04 12:46:41,127: t15.2023.10.20 val PER: 0.1980
2026-01-04 12:46:41,127: t15.2023.10.22 val PER: 0.1158
2026-01-04 12:46:41,127: t15.2023.11.03 val PER: 0.1723
2026-01-04 12:46:41,127: t15.2023.11.04 val PER: 0.0375
2026-01-04 12:46:41,127: t15.2023.11.17 val PER: 0.0373
2026-01-04 12:46:41,127: t15.2023.11.19 val PER: 0.0379
2026-01-04 12:46:41,127: t15.2023.11.26 val PER: 0.1145
2026-01-04 12:46:41,127: t15.2023.12.03 val PER: 0.1145
2026-01-04 12:46:41,128: t15.2023.12.08 val PER: 0.0985
2026-01-04 12:46:41,128: t15.2023.12.10 val PER: 0.0854
2026-01-04 12:46:41,128: t15.2023.12.17 val PER: 0.1341
2026-01-04 12:46:41,128: t15.2023.12.29 val PER: 0.1222
2026-01-04 12:46:41,128: t15.2024.02.25 val PER: 0.1011
2026-01-04 12:46:41,128: t15.2024.03.08 val PER: 0.2219
2026-01-04 12:46:41,128: t15.2024.03.15 val PER: 0.1964
2026-01-04 12:46:41,128: t15.2024.03.17 val PER: 0.1402
2026-01-04 12:46:41,128: t15.2024.05.10 val PER: 0.1530
2026-01-04 12:46:41,128: t15.2024.06.14 val PER: 0.1593
2026-01-04 12:46:41,128: t15.2024.07.19 val PER: 0.2373
2026-01-04 12:46:41,128: t15.2024.07.21 val PER: 0.0890
2026-01-04 12:46:41,128: t15.2024.07.28 val PER: 0.1287
2026-01-04 12:46:41,128: t15.2025.01.10 val PER: 0.2837
2026-01-04 12:46:41,128: t15.2025.01.12 val PER: 0.1440
2026-01-04 12:46:41,128: t15.2025.03.14 val PER: 0.3491
2026-01-04 12:46:41,129: t15.2025.03.16 val PER: 0.1885
2026-01-04 12:46:41,129: t15.2025.03.30 val PER: 0.2874
2026-01-04 12:46:41,129: t15.2025.04.13 val PER: 0.2211
2026-01-04 12:46:41,130: New best val WER(1gram) 45.94% --> 43.91%
2026-01-04 12:46:41,130: Checkpointing model
2026-01-04 12:46:41,472: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/best_checkpoint
2026-01-04 12:46:41,821: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_16500
2026-01-04 12:46:51,037: Train batch 16600: loss: 8.71 grad norm: 54.42 time: 0.053
2026-01-04 12:47:09,618: Train batch 16800: loss: 16.86 grad norm: 75.15 time: 0.062
2026-01-04 12:47:28,181: Train batch 17000: loss: 8.37 grad norm: 50.83 time: 0.083
2026-01-04 12:47:28,181: Running test after training batch: 17000
2026-01-04 12:47:28,292: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:47:33,669: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-04 12:47:33,768: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost it
2026-01-04 12:47:38,871: Val batch 17000: PER (avg): 0.1471 CTC Loss (avg): 15.0833 WER(1gram): 45.43% (n=64) time: 10.689
2026-01-04 12:47:38,871: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 12:47:38,871: t15.2023.08.13 val PER: 0.1112
2026-01-04 12:47:38,872: t15.2023.08.18 val PER: 0.1006
2026-01-04 12:47:38,872: t15.2023.08.20 val PER: 0.1104
2026-01-04 12:47:38,872: t15.2023.08.25 val PER: 0.0858
2026-01-04 12:47:38,872: t15.2023.08.27 val PER: 0.1817
2026-01-04 12:47:38,872: t15.2023.09.01 val PER: 0.0771
2026-01-04 12:47:38,872: t15.2023.09.03 val PER: 0.1675
2026-01-04 12:47:38,872: t15.2023.09.24 val PER: 0.1238
2026-01-04 12:47:38,872: t15.2023.09.29 val PER: 0.1276
2026-01-04 12:47:38,872: t15.2023.10.01 val PER: 0.1585
2026-01-04 12:47:38,873: t15.2023.10.06 val PER: 0.0829
2026-01-04 12:47:38,873: t15.2023.10.08 val PER: 0.2503
2026-01-04 12:47:38,873: t15.2023.10.13 val PER: 0.2002
2026-01-04 12:47:38,873: t15.2023.10.15 val PER: 0.1483
2026-01-04 12:47:38,873: t15.2023.10.20 val PER: 0.1812
2026-01-04 12:47:38,873: t15.2023.10.22 val PER: 0.1114
2026-01-04 12:47:38,873: t15.2023.11.03 val PER: 0.1757
2026-01-04 12:47:38,873: t15.2023.11.04 val PER: 0.0375
2026-01-04 12:47:38,873: t15.2023.11.17 val PER: 0.0404
2026-01-04 12:47:38,874: t15.2023.11.19 val PER: 0.0399
2026-01-04 12:47:38,874: t15.2023.11.26 val PER: 0.1130
2026-01-04 12:47:38,874: t15.2023.12.03 val PER: 0.1124
2026-01-04 12:47:38,874: t15.2023.12.08 val PER: 0.0959
2026-01-04 12:47:38,874: t15.2023.12.10 val PER: 0.0867
2026-01-04 12:47:38,874: t15.2023.12.17 val PER: 0.1372
2026-01-04 12:47:38,874: t15.2023.12.29 val PER: 0.1208
2026-01-04 12:47:38,874: t15.2024.02.25 val PER: 0.1011
2026-01-04 12:47:38,874: t15.2024.03.08 val PER: 0.2276
2026-01-04 12:47:38,875: t15.2024.03.15 val PER: 0.2058
2026-01-04 12:47:38,875: t15.2024.03.17 val PER: 0.1395
2026-01-04 12:47:38,875: t15.2024.05.10 val PER: 0.1560
2026-01-04 12:47:38,875: t15.2024.06.14 val PER: 0.1546
2026-01-04 12:47:38,875: t15.2024.07.19 val PER: 0.2334
2026-01-04 12:47:38,875: t15.2024.07.21 val PER: 0.0883
2026-01-04 12:47:38,875: t15.2024.07.28 val PER: 0.1265
2026-01-04 12:47:38,875: t15.2025.01.10 val PER: 0.2851
2026-01-04 12:47:38,876: t15.2025.01.12 val PER: 0.1432
2026-01-04 12:47:38,876: t15.2025.03.14 val PER: 0.3447
2026-01-04 12:47:38,876: t15.2025.03.16 val PER: 0.1806
2026-01-04 12:47:38,876: t15.2025.03.30 val PER: 0.2839
2026-01-04 12:47:38,876: t15.2025.04.13 val PER: 0.2154
2026-01-04 12:47:39,214: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_17000
2026-01-04 12:48:00,109: Train batch 17200: loss: 9.60 grad norm: 52.87 time: 0.086
2026-01-04 12:48:21,386: Train batch 17400: loss: 11.83 grad norm: 60.59 time: 0.071
2026-01-04 12:48:31,869: Running test after training batch: 17500
2026-01-04 12:48:32,007: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:48:37,364: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:48:37,457: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost it
2026-01-04 12:48:42,509: Val batch 17500: PER (avg): 0.1463 CTC Loss (avg): 15.0365 WER(1gram): 45.94% (n=64) time: 10.639
2026-01-04 12:48:42,509: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 12:48:42,509: t15.2023.08.13 val PER: 0.1102
2026-01-04 12:48:42,509: t15.2023.08.18 val PER: 0.1006
2026-01-04 12:48:42,510: t15.2023.08.20 val PER: 0.1072
2026-01-04 12:48:42,510: t15.2023.08.25 val PER: 0.0889
2026-01-04 12:48:42,510: t15.2023.08.27 val PER: 0.1801
2026-01-04 12:48:42,510: t15.2023.09.01 val PER: 0.0747
2026-01-04 12:48:42,510: t15.2023.09.03 val PER: 0.1675
2026-01-04 12:48:42,510: t15.2023.09.24 val PER: 0.1262
2026-01-04 12:48:42,510: t15.2023.09.29 val PER: 0.1270
2026-01-04 12:48:42,511: t15.2023.10.01 val PER: 0.1645
2026-01-04 12:48:42,511: t15.2023.10.06 val PER: 0.0829
2026-01-04 12:48:42,511: t15.2023.10.08 val PER: 0.2530
2026-01-04 12:48:42,511: t15.2023.10.13 val PER: 0.1986
2026-01-04 12:48:42,511: t15.2023.10.15 val PER: 0.1516
2026-01-04 12:48:42,511: t15.2023.10.20 val PER: 0.1913
2026-01-04 12:48:42,511: t15.2023.10.22 val PER: 0.1125
2026-01-04 12:48:42,511: t15.2023.11.03 val PER: 0.1737
2026-01-04 12:48:42,511: t15.2023.11.04 val PER: 0.0307
2026-01-04 12:48:42,511: t15.2023.11.17 val PER: 0.0420
2026-01-04 12:48:42,511: t15.2023.11.19 val PER: 0.0359
2026-01-04 12:48:42,512: t15.2023.11.26 val PER: 0.1109
2026-01-04 12:48:42,512: t15.2023.12.03 val PER: 0.1050
2026-01-04 12:48:42,512: t15.2023.12.08 val PER: 0.0985
2026-01-04 12:48:42,512: t15.2023.12.10 val PER: 0.0854
2026-01-04 12:48:42,512: t15.2023.12.17 val PER: 0.1383
2026-01-04 12:48:42,512: t15.2023.12.29 val PER: 0.1235
2026-01-04 12:48:42,512: t15.2024.02.25 val PER: 0.0941
2026-01-04 12:48:42,512: t15.2024.03.08 val PER: 0.2233
2026-01-04 12:48:42,512: t15.2024.03.15 val PER: 0.1901
2026-01-04 12:48:42,512: t15.2024.03.17 val PER: 0.1402
2026-01-04 12:48:42,512: t15.2024.05.10 val PER: 0.1605
2026-01-04 12:48:42,512: t15.2024.06.14 val PER: 0.1530
2026-01-04 12:48:42,512: t15.2024.07.19 val PER: 0.2281
2026-01-04 12:48:42,512: t15.2024.07.21 val PER: 0.0814
2026-01-04 12:48:42,512: t15.2024.07.28 val PER: 0.1294
2026-01-04 12:48:42,513: t15.2025.01.10 val PER: 0.2989
2026-01-04 12:48:42,513: t15.2025.01.12 val PER: 0.1386
2026-01-04 12:48:42,513: t15.2025.03.14 val PER: 0.3476
2026-01-04 12:48:42,513: t15.2025.03.16 val PER: 0.1767
2026-01-04 12:48:42,513: t15.2025.03.30 val PER: 0.2839
2026-01-04 12:48:42,513: t15.2025.04.13 val PER: 0.2168
2026-01-04 12:48:42,844: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_17500
2026-01-04 12:48:53,288: Train batch 17600: loss: 9.96 grad norm: 56.90 time: 0.053
2026-01-04 12:49:14,786: Train batch 17800: loss: 6.31 grad norm: 47.69 time: 0.045
2026-01-04 12:49:35,913: Train batch 18000: loss: 10.87 grad norm: 64.04 time: 0.063
2026-01-04 12:49:35,914: Running test after training batch: 18000
2026-01-04 12:49:36,032: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:49:41,422: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:49:41,521: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost it
2026-01-04 12:49:46,609: Val batch 18000: PER (avg): 0.1453 CTC Loss (avg): 15.0528 WER(1gram): 45.69% (n=64) time: 10.695
2026-01-04 12:49:46,610: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 12:49:46,610: t15.2023.08.13 val PER: 0.1123
2026-01-04 12:49:46,611: t15.2023.08.18 val PER: 0.1048
2026-01-04 12:49:46,611: t15.2023.08.20 val PER: 0.1041
2026-01-04 12:49:46,611: t15.2023.08.25 val PER: 0.0919
2026-01-04 12:49:46,611: t15.2023.08.27 val PER: 0.1817
2026-01-04 12:49:46,611: t15.2023.09.01 val PER: 0.0771
2026-01-04 12:49:46,611: t15.2023.09.03 val PER: 0.1615
2026-01-04 12:49:46,611: t15.2023.09.24 val PER: 0.1250
2026-01-04 12:49:46,611: t15.2023.09.29 val PER: 0.1276
2026-01-04 12:49:46,611: t15.2023.10.01 val PER: 0.1671
2026-01-04 12:49:46,611: t15.2023.10.06 val PER: 0.0818
2026-01-04 12:49:46,612: t15.2023.10.08 val PER: 0.2476
2026-01-04 12:49:46,612: t15.2023.10.13 val PER: 0.1963
2026-01-04 12:49:46,612: t15.2023.10.15 val PER: 0.1457
2026-01-04 12:49:46,612: t15.2023.10.20 val PER: 0.1879
2026-01-04 12:49:46,612: t15.2023.10.22 val PER: 0.1136
2026-01-04 12:49:46,612: t15.2023.11.03 val PER: 0.1730
2026-01-04 12:49:46,612: t15.2023.11.04 val PER: 0.0341
2026-01-04 12:49:46,612: t15.2023.11.17 val PER: 0.0389
2026-01-04 12:49:46,612: t15.2023.11.19 val PER: 0.0379
2026-01-04 12:49:46,612: t15.2023.11.26 val PER: 0.1109
2026-01-04 12:49:46,612: t15.2023.12.03 val PER: 0.1113
2026-01-04 12:49:46,613: t15.2023.12.08 val PER: 0.0945
2026-01-04 12:49:46,613: t15.2023.12.10 val PER: 0.0828
2026-01-04 12:49:46,613: t15.2023.12.17 val PER: 0.1362
2026-01-04 12:49:46,613: t15.2023.12.29 val PER: 0.1201
2026-01-04 12:49:46,613: t15.2024.02.25 val PER: 0.0955
2026-01-04 12:49:46,613: t15.2024.03.08 val PER: 0.2176
2026-01-04 12:49:46,613: t15.2024.03.15 val PER: 0.1932
2026-01-04 12:49:46,613: t15.2024.03.17 val PER: 0.1360
2026-01-04 12:49:46,613: t15.2024.05.10 val PER: 0.1486
2026-01-04 12:49:46,613: t15.2024.06.14 val PER: 0.1514
2026-01-04 12:49:46,613: t15.2024.07.19 val PER: 0.2307
2026-01-04 12:49:46,613: t15.2024.07.21 val PER: 0.0855
2026-01-04 12:49:46,613: t15.2024.07.28 val PER: 0.1287
2026-01-04 12:49:46,613: t15.2025.01.10 val PER: 0.2824
2026-01-04 12:49:46,614: t15.2025.01.12 val PER: 0.1409
2026-01-04 12:49:46,614: t15.2025.03.14 val PER: 0.3417
2026-01-04 12:49:46,614: t15.2025.03.16 val PER: 0.1846
2026-01-04 12:49:46,614: t15.2025.03.30 val PER: 0.2747
2026-01-04 12:49:46,614: t15.2025.04.13 val PER: 0.2168
2026-01-04 12:49:46,945: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_18000
2026-01-04 12:50:08,613: Train batch 18200: loss: 7.60 grad norm: 47.56 time: 0.075
2026-01-04 12:50:29,850: Train batch 18400: loss: 4.64 grad norm: 38.94 time: 0.058
2026-01-04 12:50:40,372: Running test after training batch: 18500
2026-01-04 12:50:40,799: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:50:46,243: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:50:46,344: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost it
2026-01-04 12:50:51,661: Val batch 18500: PER (avg): 0.1470 CTC Loss (avg): 15.0623 WER(1gram): 46.19% (n=64) time: 11.288
2026-01-04 12:50:51,661: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 12:50:51,661: t15.2023.08.13 val PER: 0.1102
2026-01-04 12:50:51,662: t15.2023.08.18 val PER: 0.1039
2026-01-04 12:50:51,662: t15.2023.08.20 val PER: 0.1080
2026-01-04 12:50:51,662: t15.2023.08.25 val PER: 0.0858
2026-01-04 12:50:51,662: t15.2023.08.27 val PER: 0.1849
2026-01-04 12:50:51,662: t15.2023.09.01 val PER: 0.0779
2026-01-04 12:50:51,662: t15.2023.09.03 val PER: 0.1615
2026-01-04 12:50:51,662: t15.2023.09.24 val PER: 0.1274
2026-01-04 12:50:51,662: t15.2023.09.29 val PER: 0.1276
2026-01-04 12:50:51,663: t15.2023.10.01 val PER: 0.1618
2026-01-04 12:50:51,663: t15.2023.10.06 val PER: 0.0861
2026-01-04 12:50:51,663: t15.2023.10.08 val PER: 0.2544
2026-01-04 12:50:51,663: t15.2023.10.13 val PER: 0.2002
2026-01-04 12:50:51,663: t15.2023.10.15 val PER: 0.1470
2026-01-04 12:50:51,663: t15.2023.10.20 val PER: 0.1913
2026-01-04 12:50:51,663: t15.2023.10.22 val PER: 0.1102
2026-01-04 12:50:51,663: t15.2023.11.03 val PER: 0.1723
2026-01-04 12:50:51,663: t15.2023.11.04 val PER: 0.0307
2026-01-04 12:50:51,663: t15.2023.11.17 val PER: 0.0404
2026-01-04 12:50:51,664: t15.2023.11.19 val PER: 0.0379
2026-01-04 12:50:51,664: t15.2023.11.26 val PER: 0.1130
2026-01-04 12:50:51,664: t15.2023.12.03 val PER: 0.1134
2026-01-04 12:50:51,664: t15.2023.12.08 val PER: 0.0985
2026-01-04 12:50:51,664: t15.2023.12.10 val PER: 0.0867
2026-01-04 12:50:51,664: t15.2023.12.17 val PER: 0.1414
2026-01-04 12:50:51,664: t15.2023.12.29 val PER: 0.1242
2026-01-04 12:50:51,664: t15.2024.02.25 val PER: 0.0955
2026-01-04 12:50:51,665: t15.2024.03.08 val PER: 0.2290
2026-01-04 12:50:51,665: t15.2024.03.15 val PER: 0.1964
2026-01-04 12:50:51,665: t15.2024.03.17 val PER: 0.1402
2026-01-04 12:50:51,665: t15.2024.05.10 val PER: 0.1486
2026-01-04 12:50:51,665: t15.2024.06.14 val PER: 0.1530
2026-01-04 12:50:51,665: t15.2024.07.19 val PER: 0.2294
2026-01-04 12:50:51,665: t15.2024.07.21 val PER: 0.0834
2026-01-04 12:50:51,665: t15.2024.07.28 val PER: 0.1338
2026-01-04 12:50:51,665: t15.2025.01.10 val PER: 0.2879
2026-01-04 12:50:51,665: t15.2025.01.12 val PER: 0.1440
2026-01-04 12:50:51,665: t15.2025.03.14 val PER: 0.3432
2026-01-04 12:50:51,666: t15.2025.03.16 val PER: 0.1819
2026-01-04 12:50:51,666: t15.2025.03.30 val PER: 0.2828
2026-01-04 12:50:51,666: t15.2025.04.13 val PER: 0.2140
2026-01-04 12:50:52,002: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_18500
2026-01-04 12:51:02,560: Train batch 18600: loss: 12.25 grad norm: 61.78 time: 0.067
2026-01-04 12:51:23,751: Train batch 18800: loss: 8.26 grad norm: 49.60 time: 0.065
2026-01-04 12:51:45,284: Train batch 19000: loss: 8.33 grad norm: 46.14 time: 0.064
2026-01-04 12:51:45,284: Running test after training batch: 19000
2026-01-04 12:51:45,397: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:51:50,796: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-04 12:51:50,901: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost it
2026-01-04 12:51:56,285: Val batch 19000: PER (avg): 0.1467 CTC Loss (avg): 15.0338 WER(1gram): 45.43% (n=64) time: 11.000
2026-01-04 12:51:56,285: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 12:51:56,286: t15.2023.08.13 val PER: 0.1071
2026-01-04 12:51:56,286: t15.2023.08.18 val PER: 0.0989
2026-01-04 12:51:56,286: t15.2023.08.20 val PER: 0.1112
2026-01-04 12:51:56,286: t15.2023.08.25 val PER: 0.0889
2026-01-04 12:51:56,287: t15.2023.08.27 val PER: 0.1752
2026-01-04 12:51:56,287: t15.2023.09.01 val PER: 0.0731
2026-01-04 12:51:56,287: t15.2023.09.03 val PER: 0.1615
2026-01-04 12:51:56,287: t15.2023.09.24 val PER: 0.1286
2026-01-04 12:51:56,287: t15.2023.09.29 val PER: 0.1289
2026-01-04 12:51:56,287: t15.2023.10.01 val PER: 0.1664
2026-01-04 12:51:56,287: t15.2023.10.06 val PER: 0.0861
2026-01-04 12:51:56,287: t15.2023.10.08 val PER: 0.2558
2026-01-04 12:51:56,288: t15.2023.10.13 val PER: 0.1947
2026-01-04 12:51:56,288: t15.2023.10.15 val PER: 0.1457
2026-01-04 12:51:56,288: t15.2023.10.20 val PER: 0.1980
2026-01-04 12:51:56,288: t15.2023.10.22 val PER: 0.1136
2026-01-04 12:51:56,288: t15.2023.11.03 val PER: 0.1784
2026-01-04 12:51:56,288: t15.2023.11.04 val PER: 0.0307
2026-01-04 12:51:56,288: t15.2023.11.17 val PER: 0.0373
2026-01-04 12:51:56,288: t15.2023.11.19 val PER: 0.0359
2026-01-04 12:51:56,288: t15.2023.11.26 val PER: 0.1130
2026-01-04 12:51:56,288: t15.2023.12.03 val PER: 0.1134
2026-01-04 12:51:56,288: t15.2023.12.08 val PER: 0.0959
2026-01-04 12:51:56,289: t15.2023.12.10 val PER: 0.0907
2026-01-04 12:51:56,289: t15.2023.12.17 val PER: 0.1351
2026-01-04 12:51:56,289: t15.2023.12.29 val PER: 0.1277
2026-01-04 12:51:56,289: t15.2024.02.25 val PER: 0.0983
2026-01-04 12:51:56,289: t15.2024.03.08 val PER: 0.2262
2026-01-04 12:51:56,289: t15.2024.03.15 val PER: 0.1882
2026-01-04 12:51:56,289: t15.2024.03.17 val PER: 0.1402
2026-01-04 12:51:56,289: t15.2024.05.10 val PER: 0.1426
2026-01-04 12:51:56,289: t15.2024.06.14 val PER: 0.1577
2026-01-04 12:51:56,289: t15.2024.07.19 val PER: 0.2261
2026-01-04 12:51:56,290: t15.2024.07.21 val PER: 0.0876
2026-01-04 12:51:56,290: t15.2024.07.28 val PER: 0.1324
2026-01-04 12:51:56,290: t15.2025.01.10 val PER: 0.2893
2026-01-04 12:51:56,290: t15.2025.01.12 val PER: 0.1432
2026-01-04 12:51:56,290: t15.2025.03.14 val PER: 0.3462
2026-01-04 12:51:56,290: t15.2025.03.16 val PER: 0.1846
2026-01-04 12:51:56,290: t15.2025.03.30 val PER: 0.2839
2026-01-04 12:51:56,290: t15.2025.04.13 val PER: 0.2211
2026-01-04 12:51:56,630: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_19000
2026-01-04 12:52:17,792: Train batch 19200: loss: 5.98 grad norm: 46.77 time: 0.062
2026-01-04 12:52:38,232: Train batch 19400: loss: 5.11 grad norm: 36.28 time: 0.052
2026-01-04 12:52:48,573: Running test after training batch: 19500
2026-01-04 12:52:49,007: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:52:54,910: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:52:55,009: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost it
2026-01-04 12:53:00,368: Val batch 19500: PER (avg): 0.1470 CTC Loss (avg): 14.9794 WER(1gram): 44.67% (n=64) time: 11.795
2026-01-04 12:53:00,368: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 12:53:00,369: t15.2023.08.13 val PER: 0.1102
2026-01-04 12:53:00,369: t15.2023.08.18 val PER: 0.0989
2026-01-04 12:53:00,369: t15.2023.08.20 val PER: 0.1096
2026-01-04 12:53:00,369: t15.2023.08.25 val PER: 0.0873
2026-01-04 12:53:00,369: t15.2023.08.27 val PER: 0.1768
2026-01-04 12:53:00,369: t15.2023.09.01 val PER: 0.0747
2026-01-04 12:53:00,369: t15.2023.09.03 val PER: 0.1591
2026-01-04 12:53:00,370: t15.2023.09.24 val PER: 0.1274
2026-01-04 12:53:00,370: t15.2023.09.29 val PER: 0.1264
2026-01-04 12:53:00,370: t15.2023.10.01 val PER: 0.1684
2026-01-04 12:53:00,370: t15.2023.10.06 val PER: 0.0840
2026-01-04 12:53:00,370: t15.2023.10.08 val PER: 0.2544
2026-01-04 12:53:00,370: t15.2023.10.13 val PER: 0.1924
2026-01-04 12:53:00,370: t15.2023.10.15 val PER: 0.1444
2026-01-04 12:53:00,370: t15.2023.10.20 val PER: 0.1879
2026-01-04 12:53:00,370: t15.2023.10.22 val PER: 0.1169
2026-01-04 12:53:00,371: t15.2023.11.03 val PER: 0.1771
2026-01-04 12:53:00,371: t15.2023.11.04 val PER: 0.0307
2026-01-04 12:53:00,371: t15.2023.11.17 val PER: 0.0373
2026-01-04 12:53:00,371: t15.2023.11.19 val PER: 0.0379
2026-01-04 12:53:00,371: t15.2023.11.26 val PER: 0.1138
2026-01-04 12:53:00,371: t15.2023.12.03 val PER: 0.1092
2026-01-04 12:53:00,371: t15.2023.12.08 val PER: 0.0939
2026-01-04 12:53:00,371: t15.2023.12.10 val PER: 0.0894
2026-01-04 12:53:00,371: t15.2023.12.17 val PER: 0.1331
2026-01-04 12:53:00,371: t15.2023.12.29 val PER: 0.1201
2026-01-04 12:53:00,371: t15.2024.02.25 val PER: 0.0983
2026-01-04 12:53:00,372: t15.2024.03.08 val PER: 0.2290
2026-01-04 12:53:00,372: t15.2024.03.15 val PER: 0.1945
2026-01-04 12:53:00,372: t15.2024.03.17 val PER: 0.1388
2026-01-04 12:53:00,372: t15.2024.05.10 val PER: 0.1560
2026-01-04 12:53:00,372: t15.2024.06.14 val PER: 0.1546
2026-01-04 12:53:00,372: t15.2024.07.19 val PER: 0.2340
2026-01-04 12:53:00,372: t15.2024.07.21 val PER: 0.0862
2026-01-04 12:53:00,372: t15.2024.07.28 val PER: 0.1316
2026-01-04 12:53:00,373: t15.2025.01.10 val PER: 0.3017
2026-01-04 12:53:00,373: t15.2025.01.12 val PER: 0.1440
2026-01-04 12:53:00,373: t15.2025.03.14 val PER: 0.3491
2026-01-04 12:53:00,373: t15.2025.03.16 val PER: 0.1806
2026-01-04 12:53:00,373: t15.2025.03.30 val PER: 0.2908
2026-01-04 12:53:00,373: t15.2025.04.13 val PER: 0.2211
2026-01-04 12:53:00,700: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_19500
2026-01-04 12:53:10,438: Train batch 19600: loss: 7.77 grad norm: 47.93 time: 0.057
2026-01-04 12:53:31,005: Train batch 19800: loss: 7.54 grad norm: 52.03 time: 0.055
2026-01-04 12:53:49,720: Running test after training batch: 19999
2026-01-04 12:53:49,823: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:53:55,191: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 12:53:55,305: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost it
2026-01-04 12:54:01,296: Val batch 19999: PER (avg): 0.1467 CTC Loss (avg): 15.0036 WER(1gram): 45.43% (n=64) time: 11.575
2026-01-04 12:54:01,296: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 12:54:01,297: t15.2023.08.13 val PER: 0.1143
2026-01-04 12:54:01,297: t15.2023.08.18 val PER: 0.1048
2026-01-04 12:54:01,297: t15.2023.08.20 val PER: 0.1112
2026-01-04 12:54:01,297: t15.2023.08.25 val PER: 0.0843
2026-01-04 12:54:01,297: t15.2023.08.27 val PER: 0.1817
2026-01-04 12:54:01,297: t15.2023.09.01 val PER: 0.0739
2026-01-04 12:54:01,297: t15.2023.09.03 val PER: 0.1568
2026-01-04 12:54:01,298: t15.2023.09.24 val PER: 0.1286
2026-01-04 12:54:01,298: t15.2023.09.29 val PER: 0.1283
2026-01-04 12:54:01,298: t15.2023.10.01 val PER: 0.1658
2026-01-04 12:54:01,301: t15.2023.10.06 val PER: 0.0840
2026-01-04 12:54:01,302: t15.2023.10.08 val PER: 0.2530
2026-01-04 12:54:01,302: t15.2023.10.13 val PER: 0.1939
2026-01-04 12:54:01,302: t15.2023.10.15 val PER: 0.1457
2026-01-04 12:54:01,302: t15.2023.10.20 val PER: 0.1946
2026-01-04 12:54:01,303: t15.2023.10.22 val PER: 0.1136
2026-01-04 12:54:01,303: t15.2023.11.03 val PER: 0.1696
2026-01-04 12:54:01,303: t15.2023.11.04 val PER: 0.0307
2026-01-04 12:54:01,303: t15.2023.11.17 val PER: 0.0373
2026-01-04 12:54:01,303: t15.2023.11.19 val PER: 0.0379
2026-01-04 12:54:01,303: t15.2023.11.26 val PER: 0.1174
2026-01-04 12:54:01,303: t15.2023.12.03 val PER: 0.1113
2026-01-04 12:54:01,303: t15.2023.12.08 val PER: 0.0985
2026-01-04 12:54:01,304: t15.2023.12.10 val PER: 0.0854
2026-01-04 12:54:01,304: t15.2023.12.17 val PER: 0.1362
2026-01-04 12:54:01,304: t15.2023.12.29 val PER: 0.1229
2026-01-04 12:54:01,304: t15.2024.02.25 val PER: 0.0983
2026-01-04 12:54:01,304: t15.2024.03.08 val PER: 0.2361
2026-01-04 12:54:01,304: t15.2024.03.15 val PER: 0.1895
2026-01-04 12:54:01,304: t15.2024.03.17 val PER: 0.1339
2026-01-04 12:54:01,304: t15.2024.05.10 val PER: 0.1471
2026-01-04 12:54:01,305: t15.2024.06.14 val PER: 0.1562
2026-01-04 12:54:01,305: t15.2024.07.19 val PER: 0.2347
2026-01-04 12:54:01,305: t15.2024.07.21 val PER: 0.0869
2026-01-04 12:54:01,305: t15.2024.07.28 val PER: 0.1294
2026-01-04 12:54:01,305: t15.2025.01.10 val PER: 0.2989
2026-01-04 12:54:01,305: t15.2025.01.12 val PER: 0.1424
2026-01-04 12:54:01,305: t15.2025.03.14 val PER: 0.3417
2026-01-04 12:54:01,306: t15.2025.03.16 val PER: 0.1780
2026-01-04 12:54:01,306: t15.2025.03.30 val PER: 0.2897
2026-01-04 12:54:01,306: t15.2025.04.13 val PER: 0.2168
2026-01-04 12:54:01,624: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-3/checkpoint/checkpoint_batch_19999
2026-01-04 12:54:01,658: Best avg val PER achieved: 0.14843
2026-01-04 12:54:01,658: Total training time: 42.06 minutes

=== RUN wd1e-4.yaml ===
2026-01-04 12:54:07,309: Using device: cuda:0
2026-01-04 12:54:09,021: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-04 12:54:09,051: Using 45 sessions after filtering (from 45).
2026-01-04 12:54:13,477: Using torch.compile (if available)
2026-01-04 12:54:13,478: torch.compile not available (torch<2.0). Skipping.
2026-01-04 12:54:13,478: Initialized RNN decoding model
2026-01-04 12:54:13,478: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-04 12:54:13,479: Model has 44,907,305 parameters
2026-01-04 12:54:13,479: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-04 12:54:14,876: Successfully initialized datasets
2026-01-04 12:54:14,877: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-04 12:54:16,061: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.222
2026-01-04 12:54:16,062: Running test after training batch: 0
2026-01-04 12:54:16,189: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:54:22,680: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-04 12:54:23,880: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-04 12:55:23,058: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 66.996
2026-01-04 12:55:23,059: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-04 12:55:23,059: t15.2023.08.13 val PER: 1.3056
2026-01-04 12:55:23,059: t15.2023.08.18 val PER: 1.4208
2026-01-04 12:55:23,060: t15.2023.08.20 val PER: 1.3002
2026-01-04 12:55:23,060: t15.2023.08.25 val PER: 1.3389
2026-01-04 12:55:23,060: t15.2023.08.27 val PER: 1.2460
2026-01-04 12:55:23,060: t15.2023.09.01 val PER: 1.4537
2026-01-04 12:55:23,060: t15.2023.09.03 val PER: 1.3171
2026-01-04 12:55:23,060: t15.2023.09.24 val PER: 1.5461
2026-01-04 12:55:23,060: t15.2023.09.29 val PER: 1.4671
2026-01-04 12:55:23,060: t15.2023.10.01 val PER: 1.2147
2026-01-04 12:55:23,060: t15.2023.10.06 val PER: 1.4876
2026-01-04 12:55:23,061: t15.2023.10.08 val PER: 1.1827
2026-01-04 12:55:23,061: t15.2023.10.13 val PER: 1.3964
2026-01-04 12:55:23,061: t15.2023.10.15 val PER: 1.3889
2026-01-04 12:55:23,061: t15.2023.10.20 val PER: 1.4866
2026-01-04 12:55:23,061: t15.2023.10.22 val PER: 1.3942
2026-01-04 12:55:23,061: t15.2023.11.03 val PER: 1.5923
2026-01-04 12:55:23,061: t15.2023.11.04 val PER: 2.0171
2026-01-04 12:55:23,061: t15.2023.11.17 val PER: 1.9518
2026-01-04 12:55:23,061: t15.2023.11.19 val PER: 1.6707
2026-01-04 12:55:23,061: t15.2023.11.26 val PER: 1.5413
2026-01-04 12:55:23,061: t15.2023.12.03 val PER: 1.4254
2026-01-04 12:55:23,062: t15.2023.12.08 val PER: 1.4487
2026-01-04 12:55:23,062: t15.2023.12.10 val PER: 1.6899
2026-01-04 12:55:23,062: t15.2023.12.17 val PER: 1.3077
2026-01-04 12:55:23,062: t15.2023.12.29 val PER: 1.4063
2026-01-04 12:55:23,062: t15.2024.02.25 val PER: 1.4228
2026-01-04 12:55:23,062: t15.2024.03.08 val PER: 1.3257
2026-01-04 12:55:23,062: t15.2024.03.15 val PER: 1.3196
2026-01-04 12:55:23,062: t15.2024.03.17 val PER: 1.4052
2026-01-04 12:55:23,062: t15.2024.05.10 val PER: 1.3224
2026-01-04 12:55:23,063: t15.2024.06.14 val PER: 1.5315
2026-01-04 12:55:23,063: t15.2024.07.19 val PER: 1.0817
2026-01-04 12:55:23,063: t15.2024.07.21 val PER: 1.6290
2026-01-04 12:55:23,063: t15.2024.07.28 val PER: 1.6588
2026-01-04 12:55:23,063: t15.2025.01.10 val PER: 1.0923
2026-01-04 12:55:23,063: t15.2025.01.12 val PER: 1.7629
2026-01-04 12:55:23,063: t15.2025.03.14 val PER: 1.0414
2026-01-04 12:55:23,063: t15.2025.03.16 val PER: 1.6257
2026-01-04 12:55:23,063: t15.2025.03.30 val PER: 1.2874
2026-01-04 12:55:23,063: t15.2025.04.13 val PER: 1.5949
2026-01-04 12:55:23,065: New best val WER(1gram) inf% --> 100.00%
2026-01-04 12:55:23,065: Checkpointing model
2026-01-04 12:55:23,371: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 12:55:23,681: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_0
2026-01-04 12:55:43,426: Train batch 200: loss: 77.58 grad norm: 106.15 time: 0.054
2026-01-04 12:56:02,883: Train batch 400: loss: 54.05 grad norm: 93.82 time: 0.063
2026-01-04 12:56:12,669: Running test after training batch: 500
2026-01-04 12:56:12,846: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:56:17,936: WER debug example
  GT : you can see the code at this point as well
  PR : used and ease thus uhde at this ide is aisle
2026-01-04 12:56:17,972: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as adz
2026-01-04 12:56:20,624: Val batch 500: PER (avg): 0.5189 CTC Loss (avg): 55.2346 WER(1gram): 88.58% (n=64) time: 7.954
2026-01-04 12:56:20,625: WER lens: avg_true_words=6.16 avg_pred_words=5.69 max_pred_words=12
2026-01-04 12:56:20,625: t15.2023.08.13 val PER: 0.4563
2026-01-04 12:56:20,625: t15.2023.08.18 val PER: 0.4493
2026-01-04 12:56:20,625: t15.2023.08.20 val PER: 0.4384
2026-01-04 12:56:20,625: t15.2023.08.25 val PER: 0.4277
2026-01-04 12:56:20,625: t15.2023.08.27 val PER: 0.5193
2026-01-04 12:56:20,625: t15.2023.09.01 val PER: 0.4213
2026-01-04 12:56:20,625: t15.2023.09.03 val PER: 0.4988
2026-01-04 12:56:20,626: t15.2023.09.24 val PER: 0.4357
2026-01-04 12:56:20,626: t15.2023.09.29 val PER: 0.4646
2026-01-04 12:56:20,626: t15.2023.10.01 val PER: 0.5211
2026-01-04 12:56:20,626: t15.2023.10.06 val PER: 0.4327
2026-01-04 12:56:20,626: t15.2023.10.08 val PER: 0.5453
2026-01-04 12:56:20,626: t15.2023.10.13 val PER: 0.5834
2026-01-04 12:56:20,626: t15.2023.10.15 val PER: 0.4904
2026-01-04 12:56:20,626: t15.2023.10.20 val PER: 0.4463
2026-01-04 12:56:20,627: t15.2023.10.22 val PER: 0.4510
2026-01-04 12:56:20,627: t15.2023.11.03 val PER: 0.5041
2026-01-04 12:56:20,627: t15.2023.11.04 val PER: 0.2628
2026-01-04 12:56:20,627: t15.2023.11.17 val PER: 0.3421
2026-01-04 12:56:20,627: t15.2023.11.19 val PER: 0.3413
2026-01-04 12:56:20,627: t15.2023.11.26 val PER: 0.5543
2026-01-04 12:56:20,627: t15.2023.12.03 val PER: 0.4958
2026-01-04 12:56:20,627: t15.2023.12.08 val PER: 0.5253
2026-01-04 12:56:20,627: t15.2023.12.10 val PER: 0.4494
2026-01-04 12:56:20,627: t15.2023.12.17 val PER: 0.5738
2026-01-04 12:56:20,627: t15.2023.12.29 val PER: 0.5456
2026-01-04 12:56:20,628: t15.2024.02.25 val PER: 0.4916
2026-01-04 12:56:20,628: t15.2024.03.08 val PER: 0.6202
2026-01-04 12:56:20,628: t15.2024.03.15 val PER: 0.5578
2026-01-04 12:56:20,628: t15.2024.03.17 val PER: 0.5167
2026-01-04 12:56:20,628: t15.2024.05.10 val PER: 0.5572
2026-01-04 12:56:20,628: t15.2024.06.14 val PER: 0.5063
2026-01-04 12:56:20,628: t15.2024.07.19 val PER: 0.6678
2026-01-04 12:56:20,628: t15.2024.07.21 val PER: 0.4703
2026-01-04 12:56:20,628: t15.2024.07.28 val PER: 0.5096
2026-01-04 12:56:20,628: t15.2025.01.10 val PER: 0.7534
2026-01-04 12:56:20,628: t15.2025.01.12 val PER: 0.5612
2026-01-04 12:56:20,629: t15.2025.03.14 val PER: 0.7322
2026-01-04 12:56:20,629: t15.2025.03.16 val PER: 0.5969
2026-01-04 12:56:20,629: t15.2025.03.30 val PER: 0.7287
2026-01-04 12:56:20,629: t15.2025.04.13 val PER: 0.5706
2026-01-04 12:56:20,629: New best val WER(1gram) 100.00% --> 88.58%
2026-01-04 12:56:20,630: Checkpointing model
2026-01-04 12:56:21,311: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 12:56:21,608: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_500
2026-01-04 12:56:31,511: Train batch 600: loss: 49.33 grad norm: 72.89 time: 0.078
2026-01-04 12:56:51,006: Train batch 800: loss: 41.39 grad norm: 85.50 time: 0.058
2026-01-04 12:57:09,631: Train batch 1000: loss: 42.65 grad norm: 75.30 time: 0.066
2026-01-04 12:57:09,632: Running test after training batch: 1000
2026-01-04 12:57:09,776: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:57:15,200: WER debug example
  GT : you can see the code at this point as well
  PR : used ent ease thus owed it this and is while
2026-01-04 12:57:15,253: WER debug example
  GT : how does it keep the cost down
  PR : houde is it eke thus wass it
2026-01-04 12:57:18,302: Val batch 1000: PER (avg): 0.4094 CTC Loss (avg): 42.5472 WER(1gram): 82.23% (n=64) time: 8.670
2026-01-04 12:57:18,303: WER lens: avg_true_words=6.16 avg_pred_words=5.47 max_pred_words=12
2026-01-04 12:57:18,303: t15.2023.08.13 val PER: 0.3898
2026-01-04 12:57:18,303: t15.2023.08.18 val PER: 0.3395
2026-01-04 12:57:18,303: t15.2023.08.20 val PER: 0.3384
2026-01-04 12:57:18,303: t15.2023.08.25 val PER: 0.2997
2026-01-04 12:57:18,303: t15.2023.08.27 val PER: 0.4244
2026-01-04 12:57:18,303: t15.2023.09.01 val PER: 0.3003
2026-01-04 12:57:18,303: t15.2023.09.03 val PER: 0.3943
2026-01-04 12:57:18,304: t15.2023.09.24 val PER: 0.3325
2026-01-04 12:57:18,304: t15.2023.09.29 val PER: 0.3599
2026-01-04 12:57:18,304: t15.2023.10.01 val PER: 0.4042
2026-01-04 12:57:18,304: t15.2023.10.06 val PER: 0.3186
2026-01-04 12:57:18,304: t15.2023.10.08 val PER: 0.4547
2026-01-04 12:57:18,305: t15.2023.10.13 val PER: 0.4546
2026-01-04 12:57:18,305: t15.2023.10.15 val PER: 0.3843
2026-01-04 12:57:18,305: t15.2023.10.20 val PER: 0.3859
2026-01-04 12:57:18,305: t15.2023.10.22 val PER: 0.3463
2026-01-04 12:57:18,305: t15.2023.11.03 val PER: 0.4077
2026-01-04 12:57:18,305: t15.2023.11.04 val PER: 0.1604
2026-01-04 12:57:18,305: t15.2023.11.17 val PER: 0.2737
2026-01-04 12:57:18,305: t15.2023.11.19 val PER: 0.2036
2026-01-04 12:57:18,305: t15.2023.11.26 val PER: 0.4522
2026-01-04 12:57:18,305: t15.2023.12.03 val PER: 0.4023
2026-01-04 12:57:18,305: t15.2023.12.08 val PER: 0.4035
2026-01-04 12:57:18,306: t15.2023.12.10 val PER: 0.3443
2026-01-04 12:57:18,306: t15.2023.12.17 val PER: 0.4137
2026-01-04 12:57:18,306: t15.2023.12.29 val PER: 0.4022
2026-01-04 12:57:18,306: t15.2024.02.25 val PER: 0.3581
2026-01-04 12:57:18,306: t15.2024.03.08 val PER: 0.4950
2026-01-04 12:57:18,306: t15.2024.03.15 val PER: 0.4415
2026-01-04 12:57:18,306: t15.2024.03.17 val PER: 0.4052
2026-01-04 12:57:18,306: t15.2024.05.10 val PER: 0.4235
2026-01-04 12:57:18,306: t15.2024.06.14 val PER: 0.4054
2026-01-04 12:57:18,306: t15.2024.07.19 val PER: 0.5339
2026-01-04 12:57:18,306: t15.2024.07.21 val PER: 0.3752
2026-01-04 12:57:18,307: t15.2024.07.28 val PER: 0.4191
2026-01-04 12:57:18,307: t15.2025.01.10 val PER: 0.6116
2026-01-04 12:57:18,307: t15.2025.01.12 val PER: 0.4519
2026-01-04 12:57:18,307: t15.2025.03.14 val PER: 0.6405
2026-01-04 12:57:18,309: t15.2025.03.16 val PER: 0.4856
2026-01-04 12:57:18,309: t15.2025.03.30 val PER: 0.6517
2026-01-04 12:57:18,309: t15.2025.04.13 val PER: 0.5050
2026-01-04 12:57:18,309: New best val WER(1gram) 88.58% --> 82.23%
2026-01-04 12:57:18,309: Checkpointing model
2026-01-04 12:57:18,984: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 12:57:19,302: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_1000
2026-01-04 12:57:40,102: Train batch 1200: loss: 32.91 grad norm: 76.62 time: 0.069
2026-01-04 12:58:01,315: Train batch 1400: loss: 36.04 grad norm: 78.89 time: 0.062
2026-01-04 12:58:11,887: Running test after training batch: 1500
2026-01-04 12:58:12,011: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:58:17,337: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt sze the good at this boyde is will
2026-01-04 12:58:17,392: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that us
2026-01-04 12:58:19,823: Val batch 1500: PER (avg): 0.3786 CTC Loss (avg): 37.2266 WER(1gram): 75.63% (n=64) time: 7.935
2026-01-04 12:58:19,824: WER lens: avg_true_words=6.16 avg_pred_words=5.03 max_pred_words=11
2026-01-04 12:58:19,824: t15.2023.08.13 val PER: 0.3462
2026-01-04 12:58:19,825: t15.2023.08.18 val PER: 0.3168
2026-01-04 12:58:19,825: t15.2023.08.20 val PER: 0.3042
2026-01-04 12:58:19,825: t15.2023.08.25 val PER: 0.2545
2026-01-04 12:58:19,825: t15.2023.08.27 val PER: 0.4051
2026-01-04 12:58:19,825: t15.2023.09.01 val PER: 0.2719
2026-01-04 12:58:19,825: t15.2023.09.03 val PER: 0.3789
2026-01-04 12:58:19,825: t15.2023.09.24 val PER: 0.3083
2026-01-04 12:58:19,825: t15.2023.09.29 val PER: 0.3344
2026-01-04 12:58:19,825: t15.2023.10.01 val PER: 0.3943
2026-01-04 12:58:19,826: t15.2023.10.06 val PER: 0.2896
2026-01-04 12:58:19,826: t15.2023.10.08 val PER: 0.4411
2026-01-04 12:58:19,826: t15.2023.10.13 val PER: 0.4399
2026-01-04 12:58:19,826: t15.2023.10.15 val PER: 0.3672
2026-01-04 12:58:19,826: t15.2023.10.20 val PER: 0.3322
2026-01-04 12:58:19,826: t15.2023.10.22 val PER: 0.3185
2026-01-04 12:58:19,826: t15.2023.11.03 val PER: 0.3677
2026-01-04 12:58:19,826: t15.2023.11.04 val PER: 0.1024
2026-01-04 12:58:19,826: t15.2023.11.17 val PER: 0.2224
2026-01-04 12:58:19,826: t15.2023.11.19 val PER: 0.1756
2026-01-04 12:58:19,826: t15.2023.11.26 val PER: 0.4072
2026-01-04 12:58:19,826: t15.2023.12.03 val PER: 0.3782
2026-01-04 12:58:19,826: t15.2023.12.08 val PER: 0.3529
2026-01-04 12:58:19,826: t15.2023.12.10 val PER: 0.2970
2026-01-04 12:58:19,826: t15.2023.12.17 val PER: 0.3607
2026-01-04 12:58:19,826: t15.2023.12.29 val PER: 0.3802
2026-01-04 12:58:19,827: t15.2024.02.25 val PER: 0.3090
2026-01-04 12:58:19,827: t15.2024.03.08 val PER: 0.4552
2026-01-04 12:58:19,827: t15.2024.03.15 val PER: 0.4140
2026-01-04 12:58:19,827: t15.2024.03.17 val PER: 0.3752
2026-01-04 12:58:19,827: t15.2024.05.10 val PER: 0.3834
2026-01-04 12:58:19,827: t15.2024.06.14 val PER: 0.3927
2026-01-04 12:58:19,827: t15.2024.07.19 val PER: 0.5234
2026-01-04 12:58:19,827: t15.2024.07.21 val PER: 0.3503
2026-01-04 12:58:19,827: t15.2024.07.28 val PER: 0.3566
2026-01-04 12:58:19,827: t15.2025.01.10 val PER: 0.6212
2026-01-04 12:58:19,827: t15.2025.01.12 val PER: 0.4149
2026-01-04 12:58:19,827: t15.2025.03.14 val PER: 0.6006
2026-01-04 12:58:19,827: t15.2025.03.16 val PER: 0.4568
2026-01-04 12:58:19,827: t15.2025.03.30 val PER: 0.6195
2026-01-04 12:58:19,828: t15.2025.04.13 val PER: 0.4608
2026-01-04 12:58:19,829: New best val WER(1gram) 82.23% --> 75.63%
2026-01-04 12:58:19,829: Checkpointing model
2026-01-04 12:58:20,533: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 12:58:20,883: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_1500
2026-01-04 12:58:31,255: Train batch 1600: loss: 36.60 grad norm: 77.97 time: 0.065
2026-01-04 12:58:52,342: Train batch 1800: loss: 34.59 grad norm: 69.89 time: 0.091
2026-01-04 12:59:13,541: Train batch 2000: loss: 34.24 grad norm: 84.42 time: 0.069
2026-01-04 12:59:13,541: Running test after training batch: 2000
2026-01-04 12:59:13,661: WER debug GT example: You can see the code at this point as well.
2026-01-04 12:59:19,006: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this and wheel
2026-01-04 12:59:19,062: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap thus us it
2026-01-04 12:59:21,557: Val batch 2000: PER (avg): 0.3293 CTC Loss (avg): 32.9780 WER(1gram): 70.81% (n=64) time: 8.016
2026-01-04 12:59:21,557: WER lens: avg_true_words=6.16 avg_pred_words=5.58 max_pred_words=11
2026-01-04 12:59:21,558: t15.2023.08.13 val PER: 0.3098
2026-01-04 12:59:21,558: t15.2023.08.18 val PER: 0.2565
2026-01-04 12:59:21,558: t15.2023.08.20 val PER: 0.2542
2026-01-04 12:59:21,558: t15.2023.08.25 val PER: 0.2274
2026-01-04 12:59:21,559: t15.2023.08.27 val PER: 0.3360
2026-01-04 12:59:21,559: t15.2023.09.01 val PER: 0.2370
2026-01-04 12:59:21,559: t15.2023.09.03 val PER: 0.3219
2026-01-04 12:59:21,559: t15.2023.09.24 val PER: 0.2585
2026-01-04 12:59:21,559: t15.2023.09.29 val PER: 0.2712
2026-01-04 12:59:21,559: t15.2023.10.01 val PER: 0.3395
2026-01-04 12:59:21,559: t15.2023.10.06 val PER: 0.2336
2026-01-04 12:59:21,559: t15.2023.10.08 val PER: 0.3829
2026-01-04 12:59:21,559: t15.2023.10.13 val PER: 0.3840
2026-01-04 12:59:21,559: t15.2023.10.15 val PER: 0.3065
2026-01-04 12:59:21,559: t15.2023.10.20 val PER: 0.2953
2026-01-04 12:59:21,560: t15.2023.10.22 val PER: 0.2595
2026-01-04 12:59:21,560: t15.2023.11.03 val PER: 0.3284
2026-01-04 12:59:21,560: t15.2023.11.04 val PER: 0.1092
2026-01-04 12:59:21,560: t15.2023.11.17 val PER: 0.1586
2026-01-04 12:59:21,560: t15.2023.11.19 val PER: 0.1377
2026-01-04 12:59:21,560: t15.2023.11.26 val PER: 0.3652
2026-01-04 12:59:21,560: t15.2023.12.03 val PER: 0.3162
2026-01-04 12:59:21,560: t15.2023.12.08 val PER: 0.3016
2026-01-04 12:59:21,560: t15.2023.12.10 val PER: 0.2510
2026-01-04 12:59:21,560: t15.2023.12.17 val PER: 0.3285
2026-01-04 12:59:21,561: t15.2023.12.29 val PER: 0.3336
2026-01-04 12:59:21,561: t15.2024.02.25 val PER: 0.2837
2026-01-04 12:59:21,561: t15.2024.03.08 val PER: 0.4196
2026-01-04 12:59:21,561: t15.2024.03.15 val PER: 0.3602
2026-01-04 12:59:21,561: t15.2024.03.17 val PER: 0.3403
2026-01-04 12:59:21,561: t15.2024.05.10 val PER: 0.3551
2026-01-04 12:59:21,561: t15.2024.06.14 val PER: 0.3391
2026-01-04 12:59:21,561: t15.2024.07.19 val PER: 0.4687
2026-01-04 12:59:21,561: t15.2024.07.21 val PER: 0.2966
2026-01-04 12:59:21,561: t15.2024.07.28 val PER: 0.3235
2026-01-04 12:59:21,561: t15.2025.01.10 val PER: 0.5468
2026-01-04 12:59:21,561: t15.2025.01.12 val PER: 0.3803
2026-01-04 12:59:21,562: t15.2025.03.14 val PER: 0.5370
2026-01-04 12:59:21,562: t15.2025.03.16 val PER: 0.4058
2026-01-04 12:59:21,562: t15.2025.03.30 val PER: 0.5483
2026-01-04 12:59:21,562: t15.2025.04.13 val PER: 0.3951
2026-01-04 12:59:21,563: New best val WER(1gram) 75.63% --> 70.81%
2026-01-04 12:59:21,563: Checkpointing model
2026-01-04 12:59:22,242: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 12:59:22,558: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_2000
2026-01-04 12:59:43,439: Train batch 2200: loss: 29.04 grad norm: 72.75 time: 0.060
2026-01-04 13:00:04,574: Train batch 2400: loss: 29.14 grad norm: 62.57 time: 0.052
2026-01-04 13:00:15,411: Running test after training batch: 2500
2026-01-04 13:00:15,733: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:00:21,133: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-04 13:00:21,181: WER debug example
  GT : how does it keep the cost down
  PR : houde des it eke the wass it
2026-01-04 13:00:23,801: Val batch 2500: PER (avg): 0.3035 CTC Loss (avg): 30.0697 WER(1gram): 67.51% (n=64) time: 8.389
2026-01-04 13:00:23,801: WER lens: avg_true_words=6.16 avg_pred_words=5.69 max_pred_words=11
2026-01-04 13:00:23,802: t15.2023.08.13 val PER: 0.2931
2026-01-04 13:00:23,802: t15.2023.08.18 val PER: 0.2422
2026-01-04 13:00:23,802: t15.2023.08.20 val PER: 0.2383
2026-01-04 13:00:23,802: t15.2023.08.25 val PER: 0.2093
2026-01-04 13:00:23,802: t15.2023.08.27 val PER: 0.3151
2026-01-04 13:00:23,802: t15.2023.09.01 val PER: 0.2135
2026-01-04 13:00:23,802: t15.2023.09.03 val PER: 0.2969
2026-01-04 13:00:23,802: t15.2023.09.24 val PER: 0.2269
2026-01-04 13:00:23,802: t15.2023.09.29 val PER: 0.2546
2026-01-04 13:00:23,803: t15.2023.10.01 val PER: 0.3104
2026-01-04 13:00:23,803: t15.2023.10.06 val PER: 0.2164
2026-01-04 13:00:23,803: t15.2023.10.08 val PER: 0.3829
2026-01-04 13:00:23,803: t15.2023.10.13 val PER: 0.3538
2026-01-04 13:00:23,803: t15.2023.10.15 val PER: 0.2802
2026-01-04 13:00:23,803: t15.2023.10.20 val PER: 0.2852
2026-01-04 13:00:23,803: t15.2023.10.22 val PER: 0.2327
2026-01-04 13:00:23,803: t15.2023.11.03 val PER: 0.3019
2026-01-04 13:00:23,803: t15.2023.11.04 val PER: 0.0853
2026-01-04 13:00:23,803: t15.2023.11.17 val PER: 0.1462
2026-01-04 13:00:23,803: t15.2023.11.19 val PER: 0.1238
2026-01-04 13:00:23,803: t15.2023.11.26 val PER: 0.3442
2026-01-04 13:00:23,804: t15.2023.12.03 val PER: 0.2700
2026-01-04 13:00:23,804: t15.2023.12.08 val PER: 0.2870
2026-01-04 13:00:23,804: t15.2023.12.10 val PER: 0.2247
2026-01-04 13:00:23,804: t15.2023.12.17 val PER: 0.2796
2026-01-04 13:00:23,804: t15.2023.12.29 val PER: 0.3027
2026-01-04 13:00:23,804: t15.2024.02.25 val PER: 0.2458
2026-01-04 13:00:23,804: t15.2024.03.08 val PER: 0.3599
2026-01-04 13:00:23,804: t15.2024.03.15 val PER: 0.3552
2026-01-04 13:00:23,804: t15.2024.03.17 val PER: 0.3173
2026-01-04 13:00:23,804: t15.2024.05.10 val PER: 0.3105
2026-01-04 13:00:23,804: t15.2024.06.14 val PER: 0.3155
2026-01-04 13:00:23,804: t15.2024.07.19 val PER: 0.4364
2026-01-04 13:00:23,804: t15.2024.07.21 val PER: 0.2648
2026-01-04 13:00:23,804: t15.2024.07.28 val PER: 0.2926
2026-01-04 13:00:23,804: t15.2025.01.10 val PER: 0.4890
2026-01-04 13:00:23,804: t15.2025.01.12 val PER: 0.3587
2026-01-04 13:00:23,805: t15.2025.03.14 val PER: 0.5015
2026-01-04 13:00:23,805: t15.2025.03.16 val PER: 0.3652
2026-01-04 13:00:23,805: t15.2025.03.30 val PER: 0.5069
2026-01-04 13:00:23,805: t15.2025.04.13 val PER: 0.3909
2026-01-04 13:00:23,806: New best val WER(1gram) 70.81% --> 67.51%
2026-01-04 13:00:23,806: Checkpointing model
2026-01-04 13:00:24,506: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 13:00:24,823: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_2500
2026-01-04 13:00:33,849: Train batch 2600: loss: 35.02 grad norm: 80.64 time: 0.055
2026-01-04 13:00:52,798: Train batch 2800: loss: 25.75 grad norm: 70.43 time: 0.081
2026-01-04 13:01:12,066: Train batch 3000: loss: 30.92 grad norm: 69.17 time: 0.083
2026-01-04 13:01:12,066: Running test after training batch: 3000
2026-01-04 13:01:12,174: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:01:17,552: WER debug example
  GT : you can see the code at this point as well
  PR : yule end sheik the code at this point is will
2026-01-04 13:01:17,602: WER debug example
  GT : how does it keep the cost down
  PR : houde des it hipp the cost et
2026-01-04 13:01:20,315: Val batch 3000: PER (avg): 0.2804 CTC Loss (avg): 27.6993 WER(1gram): 66.24% (n=64) time: 8.249
2026-01-04 13:01:20,316: WER lens: avg_true_words=6.16 avg_pred_words=5.83 max_pred_words=11
2026-01-04 13:01:20,316: t15.2023.08.13 val PER: 0.2557
2026-01-04 13:01:20,316: t15.2023.08.18 val PER: 0.2213
2026-01-04 13:01:20,317: t15.2023.08.20 val PER: 0.2176
2026-01-04 13:01:20,317: t15.2023.08.25 val PER: 0.1913
2026-01-04 13:01:20,317: t15.2023.08.27 val PER: 0.3006
2026-01-04 13:01:20,317: t15.2023.09.01 val PER: 0.1907
2026-01-04 13:01:20,317: t15.2023.09.03 val PER: 0.2827
2026-01-04 13:01:20,317: t15.2023.09.24 val PER: 0.2112
2026-01-04 13:01:20,317: t15.2023.09.29 val PER: 0.2297
2026-01-04 13:01:20,318: t15.2023.10.01 val PER: 0.2966
2026-01-04 13:01:20,318: t15.2023.10.06 val PER: 0.1916
2026-01-04 13:01:20,318: t15.2023.10.08 val PER: 0.3518
2026-01-04 13:01:20,318: t15.2023.10.13 val PER: 0.3421
2026-01-04 13:01:20,318: t15.2023.10.15 val PER: 0.2663
2026-01-04 13:01:20,318: t15.2023.10.20 val PER: 0.2718
2026-01-04 13:01:20,318: t15.2023.10.22 val PER: 0.2071
2026-01-04 13:01:20,318: t15.2023.11.03 val PER: 0.2741
2026-01-04 13:01:20,318: t15.2023.11.04 val PER: 0.0751
2026-01-04 13:01:20,319: t15.2023.11.17 val PER: 0.1244
2026-01-04 13:01:20,319: t15.2023.11.19 val PER: 0.1118
2026-01-04 13:01:20,319: t15.2023.11.26 val PER: 0.2928
2026-01-04 13:01:20,319: t15.2023.12.03 val PER: 0.2532
2026-01-04 13:01:20,319: t15.2023.12.08 val PER: 0.2570
2026-01-04 13:01:20,319: t15.2023.12.10 val PER: 0.2129
2026-01-04 13:01:20,319: t15.2023.12.17 val PER: 0.2807
2026-01-04 13:01:20,319: t15.2023.12.29 val PER: 0.2800
2026-01-04 13:01:20,319: t15.2024.02.25 val PER: 0.2275
2026-01-04 13:01:20,320: t15.2024.03.08 val PER: 0.3585
2026-01-04 13:01:20,320: t15.2024.03.15 val PER: 0.3371
2026-01-04 13:01:20,320: t15.2024.03.17 val PER: 0.2852
2026-01-04 13:01:20,320: t15.2024.05.10 val PER: 0.3091
2026-01-04 13:01:20,320: t15.2024.06.14 val PER: 0.2965
2026-01-04 13:01:20,320: t15.2024.07.19 val PER: 0.4001
2026-01-04 13:01:20,320: t15.2024.07.21 val PER: 0.2214
2026-01-04 13:01:20,320: t15.2024.07.28 val PER: 0.2890
2026-01-04 13:01:20,320: t15.2025.01.10 val PER: 0.4876
2026-01-04 13:01:20,320: t15.2025.01.12 val PER: 0.3202
2026-01-04 13:01:20,321: t15.2025.03.14 val PER: 0.4467
2026-01-04 13:01:20,321: t15.2025.03.16 val PER: 0.3233
2026-01-04 13:01:20,321: t15.2025.03.30 val PER: 0.5011
2026-01-04 13:01:20,321: t15.2025.04.13 val PER: 0.3381
2026-01-04 13:01:20,321: New best val WER(1gram) 67.51% --> 66.24%
2026-01-04 13:01:20,321: Checkpointing model
2026-01-04 13:01:21,035: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 13:01:21,369: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_3000
2026-01-04 13:01:41,874: Train batch 3200: loss: 26.61 grad norm: 65.65 time: 0.079
2026-01-04 13:02:02,762: Train batch 3400: loss: 18.38 grad norm: 55.10 time: 0.050
2026-01-04 13:02:13,345: Running test after training batch: 3500
2026-01-04 13:02:13,517: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:02:18,827: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point will
2026-01-04 13:02:18,871: WER debug example
  GT : how does it keep the cost down
  PR : houde des it yip thus us get
2026-01-04 13:02:21,418: Val batch 3500: PER (avg): 0.2659 CTC Loss (avg): 26.3600 WER(1gram): 64.72% (n=64) time: 8.072
2026-01-04 13:02:21,419: WER lens: avg_true_words=6.16 avg_pred_words=5.89 max_pred_words=11
2026-01-04 13:02:21,419: t15.2023.08.13 val PER: 0.2422
2026-01-04 13:02:21,419: t15.2023.08.18 val PER: 0.2070
2026-01-04 13:02:21,419: t15.2023.08.20 val PER: 0.2184
2026-01-04 13:02:21,420: t15.2023.08.25 val PER: 0.1898
2026-01-04 13:02:21,420: t15.2023.08.27 val PER: 0.2717
2026-01-04 13:02:21,420: t15.2023.09.01 val PER: 0.1810
2026-01-04 13:02:21,420: t15.2023.09.03 val PER: 0.2684
2026-01-04 13:02:21,420: t15.2023.09.24 val PER: 0.2087
2026-01-04 13:02:21,420: t15.2023.09.29 val PER: 0.2176
2026-01-04 13:02:21,420: t15.2023.10.01 val PER: 0.2820
2026-01-04 13:02:21,420: t15.2023.10.06 val PER: 0.1905
2026-01-04 13:02:21,420: t15.2023.10.08 val PER: 0.3410
2026-01-04 13:02:21,421: t15.2023.10.13 val PER: 0.3134
2026-01-04 13:02:21,421: t15.2023.10.15 val PER: 0.2492
2026-01-04 13:02:21,421: t15.2023.10.20 val PER: 0.2248
2026-01-04 13:02:21,421: t15.2023.10.22 val PER: 0.2105
2026-01-04 13:02:21,421: t15.2023.11.03 val PER: 0.2517
2026-01-04 13:02:21,421: t15.2023.11.04 val PER: 0.0853
2026-01-04 13:02:21,421: t15.2023.11.17 val PER: 0.1151
2026-01-04 13:02:21,421: t15.2023.11.19 val PER: 0.0998
2026-01-04 13:02:21,421: t15.2023.11.26 val PER: 0.2797
2026-01-04 13:02:21,421: t15.2023.12.03 val PER: 0.2416
2026-01-04 13:02:21,421: t15.2023.12.08 val PER: 0.2410
2026-01-04 13:02:21,421: t15.2023.12.10 val PER: 0.1945
2026-01-04 13:02:21,421: t15.2023.12.17 val PER: 0.2547
2026-01-04 13:02:21,422: t15.2023.12.29 val PER: 0.2512
2026-01-04 13:02:21,422: t15.2024.02.25 val PER: 0.2093
2026-01-04 13:02:21,422: t15.2024.03.08 val PER: 0.3514
2026-01-04 13:02:21,422: t15.2024.03.15 val PER: 0.3171
2026-01-04 13:02:21,422: t15.2024.03.17 val PER: 0.2762
2026-01-04 13:02:21,422: t15.2024.05.10 val PER: 0.2689
2026-01-04 13:02:21,422: t15.2024.06.14 val PER: 0.2776
2026-01-04 13:02:21,422: t15.2024.07.19 val PER: 0.3968
2026-01-04 13:02:21,422: t15.2024.07.21 val PER: 0.2193
2026-01-04 13:02:21,422: t15.2024.07.28 val PER: 0.2713
2026-01-04 13:02:21,422: t15.2025.01.10 val PER: 0.4642
2026-01-04 13:02:21,422: t15.2025.01.12 val PER: 0.2902
2026-01-04 13:02:21,423: t15.2025.03.14 val PER: 0.4393
2026-01-04 13:02:21,423: t15.2025.03.16 val PER: 0.3220
2026-01-04 13:02:21,423: t15.2025.03.30 val PER: 0.4667
2026-01-04 13:02:21,423: t15.2025.04.13 val PER: 0.3381
2026-01-04 13:02:21,424: New best val WER(1gram) 66.24% --> 64.72%
2026-01-04 13:02:21,424: Checkpointing model
2026-01-04 13:02:22,134: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 13:02:22,453: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_3500
2026-01-04 13:02:32,850: Train batch 3600: loss: 22.29 grad norm: 67.44 time: 0.067
2026-01-04 13:02:53,812: Train batch 3800: loss: 26.12 grad norm: 69.26 time: 0.066
2026-01-04 13:03:15,028: Train batch 4000: loss: 19.85 grad norm: 56.10 time: 0.057
2026-01-04 13:03:15,029: Running test after training batch: 4000
2026-01-04 13:03:15,202: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:03:20,603: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point will
2026-01-04 13:03:20,649: WER debug example
  GT : how does it keep the cost down
  PR : how dust it kipp thus cost nit
2026-01-04 13:03:23,352: Val batch 4000: PER (avg): 0.2499 CTC Loss (avg): 24.3102 WER(1gram): 63.71% (n=64) time: 8.323
2026-01-04 13:03:23,353: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-04 13:03:23,353: t15.2023.08.13 val PER: 0.2318
2026-01-04 13:03:23,353: t15.2023.08.18 val PER: 0.2079
2026-01-04 13:03:23,353: t15.2023.08.20 val PER: 0.1898
2026-01-04 13:03:23,353: t15.2023.08.25 val PER: 0.1657
2026-01-04 13:03:23,354: t15.2023.08.27 val PER: 0.2846
2026-01-04 13:03:23,354: t15.2023.09.01 val PER: 0.1526
2026-01-04 13:03:23,354: t15.2023.09.03 val PER: 0.2387
2026-01-04 13:03:23,354: t15.2023.09.24 val PER: 0.1893
2026-01-04 13:03:23,354: t15.2023.09.29 val PER: 0.1997
2026-01-04 13:03:23,354: t15.2023.10.01 val PER: 0.2642
2026-01-04 13:03:23,354: t15.2023.10.06 val PER: 0.1647
2026-01-04 13:03:23,354: t15.2023.10.08 val PER: 0.3383
2026-01-04 13:03:23,354: t15.2023.10.13 val PER: 0.3041
2026-01-04 13:03:23,354: t15.2023.10.15 val PER: 0.2360
2026-01-04 13:03:23,354: t15.2023.10.20 val PER: 0.2282
2026-01-04 13:03:23,354: t15.2023.10.22 val PER: 0.1982
2026-01-04 13:03:23,354: t15.2023.11.03 val PER: 0.2503
2026-01-04 13:03:23,354: t15.2023.11.04 val PER: 0.0717
2026-01-04 13:03:23,355: t15.2023.11.17 val PER: 0.1213
2026-01-04 13:03:23,355: t15.2023.11.19 val PER: 0.0958
2026-01-04 13:03:23,355: t15.2023.11.26 val PER: 0.2630
2026-01-04 13:03:23,355: t15.2023.12.03 val PER: 0.2153
2026-01-04 13:03:23,355: t15.2023.12.08 val PER: 0.2210
2026-01-04 13:03:23,355: t15.2023.12.10 val PER: 0.1945
2026-01-04 13:03:23,355: t15.2023.12.17 val PER: 0.2370
2026-01-04 13:03:23,355: t15.2023.12.29 val PER: 0.2615
2026-01-04 13:03:23,355: t15.2024.02.25 val PER: 0.2121
2026-01-04 13:03:23,355: t15.2024.03.08 val PER: 0.3428
2026-01-04 13:03:23,355: t15.2024.03.15 val PER: 0.2952
2026-01-04 13:03:23,355: t15.2024.03.17 val PER: 0.2517
2026-01-04 13:03:23,356: t15.2024.05.10 val PER: 0.2630
2026-01-04 13:03:23,356: t15.2024.06.14 val PER: 0.2587
2026-01-04 13:03:23,356: t15.2024.07.19 val PER: 0.3705
2026-01-04 13:03:23,356: t15.2024.07.21 val PER: 0.1903
2026-01-04 13:03:23,356: t15.2024.07.28 val PER: 0.2515
2026-01-04 13:03:23,356: t15.2025.01.10 val PER: 0.4201
2026-01-04 13:03:23,357: t15.2025.01.12 val PER: 0.2833
2026-01-04 13:03:23,357: t15.2025.03.14 val PER: 0.4112
2026-01-04 13:03:23,357: t15.2025.03.16 val PER: 0.3207
2026-01-04 13:03:23,357: t15.2025.03.30 val PER: 0.4103
2026-01-04 13:03:23,357: t15.2025.04.13 val PER: 0.3110
2026-01-04 13:03:23,359: New best val WER(1gram) 64.72% --> 63.71%
2026-01-04 13:03:23,359: Checkpointing model
2026-01-04 13:03:24,051: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 13:03:24,376: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_4000
2026-01-04 13:03:45,461: Train batch 4200: loss: 22.96 grad norm: 63.27 time: 0.080
2026-01-04 13:04:06,607: Train batch 4400: loss: 16.89 grad norm: 54.42 time: 0.070
2026-01-04 13:04:17,095: Running test after training batch: 4500
2026-01-04 13:04:17,542: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:04:22,883: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-04 13:04:22,914: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it yip the cussed get
2026-01-04 13:04:24,623: Val batch 4500: PER (avg): 0.2381 CTC Loss (avg): 23.0426 WER(1gram): 59.90% (n=64) time: 7.527
2026-01-04 13:04:24,623: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=11
2026-01-04 13:04:24,623: t15.2023.08.13 val PER: 0.2006
2026-01-04 13:04:24,624: t15.2023.08.18 val PER: 0.1936
2026-01-04 13:04:24,624: t15.2023.08.20 val PER: 0.1827
2026-01-04 13:04:24,624: t15.2023.08.25 val PER: 0.1386
2026-01-04 13:04:24,624: t15.2023.08.27 val PER: 0.2540
2026-01-04 13:04:24,624: t15.2023.09.01 val PER: 0.1494
2026-01-04 13:04:24,624: t15.2023.09.03 val PER: 0.2375
2026-01-04 13:04:24,624: t15.2023.09.24 val PER: 0.1735
2026-01-04 13:04:24,624: t15.2023.09.29 val PER: 0.2023
2026-01-04 13:04:24,624: t15.2023.10.01 val PER: 0.2589
2026-01-04 13:04:24,625: t15.2023.10.06 val PER: 0.1507
2026-01-04 13:04:24,625: t15.2023.10.08 val PER: 0.3221
2026-01-04 13:04:24,625: t15.2023.10.13 val PER: 0.2995
2026-01-04 13:04:24,625: t15.2023.10.15 val PER: 0.2307
2026-01-04 13:04:24,625: t15.2023.10.20 val PER: 0.2282
2026-01-04 13:04:24,625: t15.2023.10.22 val PER: 0.1904
2026-01-04 13:04:24,625: t15.2023.11.03 val PER: 0.2476
2026-01-04 13:04:24,625: t15.2023.11.04 val PER: 0.0614
2026-01-04 13:04:24,625: t15.2023.11.17 val PER: 0.0949
2026-01-04 13:04:24,625: t15.2023.11.19 val PER: 0.0858
2026-01-04 13:04:24,626: t15.2023.11.26 val PER: 0.2703
2026-01-04 13:04:24,626: t15.2023.12.03 val PER: 0.2122
2026-01-04 13:04:24,626: t15.2023.12.08 val PER: 0.2037
2026-01-04 13:04:24,626: t15.2023.12.10 val PER: 0.1682
2026-01-04 13:04:24,626: t15.2023.12.17 val PER: 0.2287
2026-01-04 13:04:24,626: t15.2023.12.29 val PER: 0.2491
2026-01-04 13:04:24,626: t15.2024.02.25 val PER: 0.2008
2026-01-04 13:04:24,626: t15.2024.03.08 val PER: 0.3385
2026-01-04 13:04:24,626: t15.2024.03.15 val PER: 0.2852
2026-01-04 13:04:24,626: t15.2024.03.17 val PER: 0.2406
2026-01-04 13:04:24,627: t15.2024.05.10 val PER: 0.2511
2026-01-04 13:04:24,627: t15.2024.06.14 val PER: 0.2476
2026-01-04 13:04:24,627: t15.2024.07.19 val PER: 0.3388
2026-01-04 13:04:24,627: t15.2024.07.21 val PER: 0.1731
2026-01-04 13:04:24,627: t15.2024.07.28 val PER: 0.2228
2026-01-04 13:04:24,627: t15.2025.01.10 val PER: 0.4063
2026-01-04 13:04:24,627: t15.2025.01.12 val PER: 0.2694
2026-01-04 13:04:24,627: t15.2025.03.14 val PER: 0.3979
2026-01-04 13:04:24,627: t15.2025.03.16 val PER: 0.2906
2026-01-04 13:04:24,627: t15.2025.03.30 val PER: 0.4080
2026-01-04 13:04:24,627: t15.2025.04.13 val PER: 0.3081
2026-01-04 13:04:24,628: New best val WER(1gram) 63.71% --> 59.90%
2026-01-04 13:04:24,628: Checkpointing model
2026-01-04 13:04:25,311: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 13:04:25,611: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_4500
2026-01-04 13:04:35,688: Train batch 4600: loss: 20.16 grad norm: 64.49 time: 0.061
2026-01-04 13:04:56,094: Train batch 4800: loss: 13.75 grad norm: 52.69 time: 0.063
2026-01-04 13:05:16,620: Train batch 5000: loss: 31.04 grad norm: 82.46 time: 0.063
2026-01-04 13:05:16,621: Running test after training batch: 5000
2026-01-04 13:05:17,050: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:05:22,945: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 13:05:22,975: WER debug example
  GT : how does it keep the cost down
  PR : houde des it yip the cost nett
2026-01-04 13:05:24,692: Val batch 5000: PER (avg): 0.2246 CTC Loss (avg): 21.8248 WER(1gram): 61.17% (n=64) time: 8.072
2026-01-04 13:05:24,693: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-04 13:05:24,693: t15.2023.08.13 val PER: 0.1933
2026-01-04 13:05:24,693: t15.2023.08.18 val PER: 0.1744
2026-01-04 13:05:24,693: t15.2023.08.20 val PER: 0.1819
2026-01-04 13:05:24,694: t15.2023.08.25 val PER: 0.1295
2026-01-04 13:05:24,694: t15.2023.08.27 val PER: 0.2444
2026-01-04 13:05:24,694: t15.2023.09.01 val PER: 0.1372
2026-01-04 13:05:24,694: t15.2023.09.03 val PER: 0.2292
2026-01-04 13:05:24,694: t15.2023.09.24 val PER: 0.1845
2026-01-04 13:05:24,694: t15.2023.09.29 val PER: 0.1780
2026-01-04 13:05:24,694: t15.2023.10.01 val PER: 0.2332
2026-01-04 13:05:24,694: t15.2023.10.06 val PER: 0.1529
2026-01-04 13:05:24,694: t15.2023.10.08 val PER: 0.3085
2026-01-04 13:05:24,694: t15.2023.10.13 val PER: 0.2870
2026-01-04 13:05:24,694: t15.2023.10.15 val PER: 0.2274
2026-01-04 13:05:24,695: t15.2023.10.20 val PER: 0.2315
2026-01-04 13:05:24,695: t15.2023.10.22 val PER: 0.1670
2026-01-04 13:05:24,695: t15.2023.11.03 val PER: 0.2218
2026-01-04 13:05:24,695: t15.2023.11.04 val PER: 0.0410
2026-01-04 13:05:24,695: t15.2023.11.17 val PER: 0.0793
2026-01-04 13:05:24,695: t15.2023.11.19 val PER: 0.0798
2026-01-04 13:05:24,695: t15.2023.11.26 val PER: 0.2304
2026-01-04 13:05:24,695: t15.2023.12.03 val PER: 0.1933
2026-01-04 13:05:24,695: t15.2023.12.08 val PER: 0.1924
2026-01-04 13:05:24,696: t15.2023.12.10 val PER: 0.1603
2026-01-04 13:05:24,696: t15.2023.12.17 val PER: 0.2141
2026-01-04 13:05:24,696: t15.2023.12.29 val PER: 0.2155
2026-01-04 13:05:24,696: t15.2024.02.25 val PER: 0.1896
2026-01-04 13:05:24,696: t15.2024.03.08 val PER: 0.3115
2026-01-04 13:05:24,696: t15.2024.03.15 val PER: 0.2708
2026-01-04 13:05:24,696: t15.2024.03.17 val PER: 0.2329
2026-01-04 13:05:24,696: t15.2024.05.10 val PER: 0.2467
2026-01-04 13:05:24,696: t15.2024.06.14 val PER: 0.2429
2026-01-04 13:05:24,696: t15.2024.07.19 val PER: 0.3256
2026-01-04 13:05:24,696: t15.2024.07.21 val PER: 0.1766
2026-01-04 13:05:24,697: t15.2024.07.28 val PER: 0.2096
2026-01-04 13:05:24,697: t15.2025.01.10 val PER: 0.3871
2026-01-04 13:05:24,697: t15.2025.01.12 val PER: 0.2564
2026-01-04 13:05:24,697: t15.2025.03.14 val PER: 0.3846
2026-01-04 13:05:24,697: t15.2025.03.16 val PER: 0.2893
2026-01-04 13:05:24,697: t15.2025.03.30 val PER: 0.3839
2026-01-04 13:05:24,697: t15.2025.04.13 val PER: 0.3096
2026-01-04 13:05:24,981: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_5000
2026-01-04 13:05:43,470: Train batch 5200: loss: 16.95 grad norm: 57.15 time: 0.052
2026-01-04 13:06:02,220: Train batch 5400: loss: 17.55 grad norm: 59.38 time: 0.068
2026-01-04 13:06:11,618: Running test after training batch: 5500
2026-01-04 13:06:11,912: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:06:16,978: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point will
2026-01-04 13:06:17,008: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost tit
2026-01-04 13:06:18,713: Val batch 5500: PER (avg): 0.2123 CTC Loss (avg): 20.7495 WER(1gram): 56.60% (n=64) time: 7.094
2026-01-04 13:06:18,714: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=10
2026-01-04 13:06:18,714: t15.2023.08.13 val PER: 0.1746
2026-01-04 13:06:18,714: t15.2023.08.18 val PER: 0.1584
2026-01-04 13:06:18,714: t15.2023.08.20 val PER: 0.1628
2026-01-04 13:06:18,714: t15.2023.08.25 val PER: 0.1220
2026-01-04 13:06:18,714: t15.2023.08.27 val PER: 0.2283
2026-01-04 13:06:18,715: t15.2023.09.01 val PER: 0.1266
2026-01-04 13:06:18,715: t15.2023.09.03 val PER: 0.2363
2026-01-04 13:06:18,715: t15.2023.09.24 val PER: 0.1772
2026-01-04 13:06:18,715: t15.2023.09.29 val PER: 0.1672
2026-01-04 13:06:18,715: t15.2023.10.01 val PER: 0.2266
2026-01-04 13:06:18,715: t15.2023.10.06 val PER: 0.1346
2026-01-04 13:06:18,715: t15.2023.10.08 val PER: 0.2963
2026-01-04 13:06:18,715: t15.2023.10.13 val PER: 0.2715
2026-01-04 13:06:18,715: t15.2023.10.15 val PER: 0.2123
2026-01-04 13:06:18,715: t15.2023.10.20 val PER: 0.2315
2026-01-04 13:06:18,716: t15.2023.10.22 val PER: 0.1537
2026-01-04 13:06:18,716: t15.2023.11.03 val PER: 0.2191
2026-01-04 13:06:18,716: t15.2023.11.04 val PER: 0.0648
2026-01-04 13:06:18,716: t15.2023.11.17 val PER: 0.0809
2026-01-04 13:06:18,716: t15.2023.11.19 val PER: 0.0619
2026-01-04 13:06:18,716: t15.2023.11.26 val PER: 0.2167
2026-01-04 13:06:18,716: t15.2023.12.03 val PER: 0.1849
2026-01-04 13:06:18,716: t15.2023.12.08 val PER: 0.1784
2026-01-04 13:06:18,716: t15.2023.12.10 val PER: 0.1656
2026-01-04 13:06:18,716: t15.2023.12.17 val PER: 0.2110
2026-01-04 13:06:18,716: t15.2023.12.29 val PER: 0.2141
2026-01-04 13:06:18,716: t15.2024.02.25 val PER: 0.1770
2026-01-04 13:06:18,716: t15.2024.03.08 val PER: 0.2973
2026-01-04 13:06:18,716: t15.2024.03.15 val PER: 0.2564
2026-01-04 13:06:18,716: t15.2024.03.17 val PER: 0.2169
2026-01-04 13:06:18,717: t15.2024.05.10 val PER: 0.2333
2026-01-04 13:06:18,717: t15.2024.06.14 val PER: 0.2240
2026-01-04 13:06:18,717: t15.2024.07.19 val PER: 0.3118
2026-01-04 13:06:18,717: t15.2024.07.21 val PER: 0.1545
2026-01-04 13:06:18,717: t15.2024.07.28 val PER: 0.1993
2026-01-04 13:06:18,717: t15.2025.01.10 val PER: 0.3967
2026-01-04 13:06:18,717: t15.2025.01.12 val PER: 0.2279
2026-01-04 13:06:18,717: t15.2025.03.14 val PER: 0.3713
2026-01-04 13:06:18,717: t15.2025.03.16 val PER: 0.2631
2026-01-04 13:06:18,717: t15.2025.03.30 val PER: 0.3563
2026-01-04 13:06:18,717: t15.2025.04.13 val PER: 0.2796
2026-01-04 13:06:18,719: New best val WER(1gram) 59.90% --> 56.60%
2026-01-04 13:06:18,719: Checkpointing model
2026-01-04 13:06:19,355: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 13:06:19,657: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_5500
2026-01-04 13:06:30,132: Train batch 5600: loss: 19.50 grad norm: 69.56 time: 0.063
2026-01-04 13:06:50,460: Train batch 5800: loss: 13.79 grad norm: 56.51 time: 0.082
2026-01-04 13:07:10,693: Train batch 6000: loss: 13.80 grad norm: 55.04 time: 0.048
2026-01-04 13:07:10,694: Running test after training batch: 6000
2026-01-04 13:07:11,368: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:07:21,692: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 13:07:21,724: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nit
2026-01-04 13:07:23,468: Val batch 6000: PER (avg): 0.2102 CTC Loss (avg): 20.7257 WER(1gram): 58.88% (n=64) time: 12.774
2026-01-04 13:07:23,469: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-04 13:07:23,469: t15.2023.08.13 val PER: 0.1705
2026-01-04 13:07:23,469: t15.2023.08.18 val PER: 0.1718
2026-01-04 13:07:23,469: t15.2023.08.20 val PER: 0.1660
2026-01-04 13:07:23,470: t15.2023.08.25 val PER: 0.1145
2026-01-04 13:07:23,470: t15.2023.08.27 val PER: 0.2476
2026-01-04 13:07:23,470: t15.2023.09.01 val PER: 0.1323
2026-01-04 13:07:23,470: t15.2023.09.03 val PER: 0.2185
2026-01-04 13:07:23,470: t15.2023.09.24 val PER: 0.1735
2026-01-04 13:07:23,470: t15.2023.09.29 val PER: 0.1710
2026-01-04 13:07:23,470: t15.2023.10.01 val PER: 0.2239
2026-01-04 13:07:23,470: t15.2023.10.06 val PER: 0.1324
2026-01-04 13:07:23,470: t15.2023.10.08 val PER: 0.3004
2026-01-04 13:07:23,470: t15.2023.10.13 val PER: 0.2692
2026-01-04 13:07:23,470: t15.2023.10.15 val PER: 0.2070
2026-01-04 13:07:23,471: t15.2023.10.20 val PER: 0.2013
2026-01-04 13:07:23,471: t15.2023.10.22 val PER: 0.1615
2026-01-04 13:07:23,471: t15.2023.11.03 val PER: 0.2239
2026-01-04 13:07:23,471: t15.2023.11.04 val PER: 0.0410
2026-01-04 13:07:23,471: t15.2023.11.17 val PER: 0.0747
2026-01-04 13:07:23,471: t15.2023.11.19 val PER: 0.0758
2026-01-04 13:07:23,471: t15.2023.11.26 val PER: 0.2217
2026-01-04 13:07:23,471: t15.2023.12.03 val PER: 0.1744
2026-01-04 13:07:23,472: t15.2023.12.08 val PER: 0.1698
2026-01-04 13:07:23,472: t15.2023.12.10 val PER: 0.1472
2026-01-04 13:07:23,472: t15.2023.12.17 val PER: 0.1954
2026-01-04 13:07:23,472: t15.2023.12.29 val PER: 0.2128
2026-01-04 13:07:23,472: t15.2024.02.25 val PER: 0.1545
2026-01-04 13:07:23,472: t15.2024.03.08 val PER: 0.3101
2026-01-04 13:07:23,472: t15.2024.03.15 val PER: 0.2608
2026-01-04 13:07:23,472: t15.2024.03.17 val PER: 0.2092
2026-01-04 13:07:23,472: t15.2024.05.10 val PER: 0.2229
2026-01-04 13:07:23,472: t15.2024.06.14 val PER: 0.2271
2026-01-04 13:07:23,472: t15.2024.07.19 val PER: 0.2980
2026-01-04 13:07:23,472: t15.2024.07.21 val PER: 0.1662
2026-01-04 13:07:23,472: t15.2024.07.28 val PER: 0.2000
2026-01-04 13:07:23,472: t15.2025.01.10 val PER: 0.3760
2026-01-04 13:07:23,472: t15.2025.01.12 val PER: 0.2109
2026-01-04 13:07:23,473: t15.2025.03.14 val PER: 0.3698
2026-01-04 13:07:23,473: t15.2025.03.16 val PER: 0.2565
2026-01-04 13:07:23,473: t15.2025.03.30 val PER: 0.3770
2026-01-04 13:07:23,473: t15.2025.04.13 val PER: 0.2782
2026-01-04 13:07:23,757: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_6000
2026-01-04 13:07:42,191: Train batch 6200: loss: 16.88 grad norm: 60.87 time: 0.070
2026-01-04 13:08:00,567: Train batch 6400: loss: 18.85 grad norm: 66.03 time: 0.063
2026-01-04 13:08:09,668: Running test after training batch: 6500
2026-01-04 13:08:09,861: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:08:15,211: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point as will
2026-01-04 13:08:15,265: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-04 13:08:18,367: Val batch 6500: PER (avg): 0.2040 CTC Loss (avg): 20.0518 WER(1gram): 52.28% (n=64) time: 8.698
2026-01-04 13:08:18,367: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 13:08:18,367: t15.2023.08.13 val PER: 0.1684
2026-01-04 13:08:18,368: t15.2023.08.18 val PER: 0.1517
2026-01-04 13:08:18,368: t15.2023.08.20 val PER: 0.1581
2026-01-04 13:08:18,368: t15.2023.08.25 val PER: 0.1054
2026-01-04 13:08:18,368: t15.2023.08.27 val PER: 0.2251
2026-01-04 13:08:18,368: t15.2023.09.01 val PER: 0.1242
2026-01-04 13:08:18,368: t15.2023.09.03 val PER: 0.2090
2026-01-04 13:08:18,368: t15.2023.09.24 val PER: 0.1760
2026-01-04 13:08:18,368: t15.2023.09.29 val PER: 0.1653
2026-01-04 13:08:18,368: t15.2023.10.01 val PER: 0.2114
2026-01-04 13:08:18,368: t15.2023.10.06 val PER: 0.1292
2026-01-04 13:08:18,369: t15.2023.10.08 val PER: 0.2801
2026-01-04 13:08:18,369: t15.2023.10.13 val PER: 0.2684
2026-01-04 13:08:18,369: t15.2023.10.15 val PER: 0.2096
2026-01-04 13:08:18,369: t15.2023.10.20 val PER: 0.2047
2026-01-04 13:08:18,369: t15.2023.10.22 val PER: 0.1470
2026-01-04 13:08:18,369: t15.2023.11.03 val PER: 0.2164
2026-01-04 13:08:18,369: t15.2023.11.04 val PER: 0.0512
2026-01-04 13:08:18,369: t15.2023.11.17 val PER: 0.0669
2026-01-04 13:08:18,369: t15.2023.11.19 val PER: 0.0739
2026-01-04 13:08:18,370: t15.2023.11.26 val PER: 0.2080
2026-01-04 13:08:18,370: t15.2023.12.03 val PER: 0.1649
2026-01-04 13:08:18,370: t15.2023.12.08 val PER: 0.1684
2026-01-04 13:08:18,370: t15.2023.12.10 val PER: 0.1445
2026-01-04 13:08:18,370: t15.2023.12.17 val PER: 0.1933
2026-01-04 13:08:18,370: t15.2023.12.29 val PER: 0.2066
2026-01-04 13:08:18,370: t15.2024.02.25 val PER: 0.1657
2026-01-04 13:08:18,370: t15.2024.03.08 val PER: 0.2959
2026-01-04 13:08:18,370: t15.2024.03.15 val PER: 0.2589
2026-01-04 13:08:18,370: t15.2024.03.17 val PER: 0.2008
2026-01-04 13:08:18,370: t15.2024.05.10 val PER: 0.2259
2026-01-04 13:08:18,370: t15.2024.06.14 val PER: 0.2177
2026-01-04 13:08:18,370: t15.2024.07.19 val PER: 0.3019
2026-01-04 13:08:18,370: t15.2024.07.21 val PER: 0.1600
2026-01-04 13:08:18,370: t15.2024.07.28 val PER: 0.1912
2026-01-04 13:08:18,371: t15.2025.01.10 val PER: 0.3802
2026-01-04 13:08:18,371: t15.2025.01.12 val PER: 0.2109
2026-01-04 13:08:18,371: t15.2025.03.14 val PER: 0.3728
2026-01-04 13:08:18,371: t15.2025.03.16 val PER: 0.2435
2026-01-04 13:08:18,371: t15.2025.03.30 val PER: 0.3471
2026-01-04 13:08:18,371: t15.2025.04.13 val PER: 0.2710
2026-01-04 13:08:18,372: New best val WER(1gram) 56.60% --> 52.28%
2026-01-04 13:08:18,372: Checkpointing model
2026-01-04 13:08:19,065: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 13:08:19,381: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_6500
2026-01-04 13:08:29,549: Train batch 6600: loss: 12.19 grad norm: 53.05 time: 0.045
2026-01-04 13:08:50,605: Train batch 6800: loss: 15.94 grad norm: 57.80 time: 0.051
2026-01-04 13:09:12,026: Train batch 7000: loss: 17.30 grad norm: 67.36 time: 0.063
2026-01-04 13:09:12,027: Running test after training batch: 7000
2026-01-04 13:09:12,169: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:09:17,490: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 13:09:17,549: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-04 13:09:20,732: Val batch 7000: PER (avg): 0.1953 CTC Loss (avg): 19.0968 WER(1gram): 53.05% (n=64) time: 8.705
2026-01-04 13:09:20,733: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-04 13:09:20,733: t15.2023.08.13 val PER: 0.1580
2026-01-04 13:09:20,733: t15.2023.08.18 val PER: 0.1350
2026-01-04 13:09:20,733: t15.2023.08.20 val PER: 0.1509
2026-01-04 13:09:20,733: t15.2023.08.25 val PER: 0.0964
2026-01-04 13:09:20,733: t15.2023.08.27 val PER: 0.2154
2026-01-04 13:09:20,734: t15.2023.09.01 val PER: 0.1071
2026-01-04 13:09:20,734: t15.2023.09.03 val PER: 0.1865
2026-01-04 13:09:20,734: t15.2023.09.24 val PER: 0.1553
2026-01-04 13:09:20,734: t15.2023.09.29 val PER: 0.1691
2026-01-04 13:09:20,734: t15.2023.10.01 val PER: 0.2160
2026-01-04 13:09:20,734: t15.2023.10.06 val PER: 0.1098
2026-01-04 13:09:20,734: t15.2023.10.08 val PER: 0.2733
2026-01-04 13:09:20,734: t15.2023.10.13 val PER: 0.2591
2026-01-04 13:09:20,735: t15.2023.10.15 val PER: 0.1912
2026-01-04 13:09:20,735: t15.2023.10.20 val PER: 0.2148
2026-01-04 13:09:20,735: t15.2023.10.22 val PER: 0.1314
2026-01-04 13:09:20,735: t15.2023.11.03 val PER: 0.2062
2026-01-04 13:09:20,735: t15.2023.11.04 val PER: 0.0341
2026-01-04 13:09:20,735: t15.2023.11.17 val PER: 0.0669
2026-01-04 13:09:20,735: t15.2023.11.19 val PER: 0.0559
2026-01-04 13:09:20,735: t15.2023.11.26 val PER: 0.2014
2026-01-04 13:09:20,735: t15.2023.12.03 val PER: 0.1576
2026-01-04 13:09:20,735: t15.2023.12.08 val PER: 0.1618
2026-01-04 13:09:20,735: t15.2023.12.10 val PER: 0.1367
2026-01-04 13:09:20,735: t15.2023.12.17 val PER: 0.1850
2026-01-04 13:09:20,735: t15.2023.12.29 val PER: 0.1949
2026-01-04 13:09:20,735: t15.2024.02.25 val PER: 0.1517
2026-01-04 13:09:20,736: t15.2024.03.08 val PER: 0.2902
2026-01-04 13:09:20,736: t15.2024.03.15 val PER: 0.2445
2026-01-04 13:09:20,736: t15.2024.03.17 val PER: 0.1967
2026-01-04 13:09:20,736: t15.2024.05.10 val PER: 0.2021
2026-01-04 13:09:20,736: t15.2024.06.14 val PER: 0.2082
2026-01-04 13:09:20,736: t15.2024.07.19 val PER: 0.3039
2026-01-04 13:09:20,736: t15.2024.07.21 val PER: 0.1393
2026-01-04 13:09:20,736: t15.2024.07.28 val PER: 0.1706
2026-01-04 13:09:20,736: t15.2025.01.10 val PER: 0.3705
2026-01-04 13:09:20,736: t15.2025.01.12 val PER: 0.2171
2026-01-04 13:09:20,736: t15.2025.03.14 val PER: 0.3654
2026-01-04 13:09:20,736: t15.2025.03.16 val PER: 0.2435
2026-01-04 13:09:20,736: t15.2025.03.30 val PER: 0.3644
2026-01-04 13:09:20,736: t15.2025.04.13 val PER: 0.2739
2026-01-04 13:09:21,050: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_7000
2026-01-04 13:09:42,126: Train batch 7200: loss: 14.26 grad norm: 59.23 time: 0.080
2026-01-04 13:10:03,233: Train batch 7400: loss: 13.28 grad norm: 54.65 time: 0.076
2026-01-04 13:10:13,862: Running test after training batch: 7500
2026-01-04 13:10:14,191: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:10:20,546: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point us will
2026-01-04 13:10:20,606: WER debug example
  GT : how does it keep the cost down
  PR : houde dusk it keep the cost nit
2026-01-04 13:10:23,921: Val batch 7500: PER (avg): 0.1882 CTC Loss (avg): 18.5477 WER(1gram): 56.85% (n=64) time: 10.059
2026-01-04 13:10:23,922: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 13:10:23,922: t15.2023.08.13 val PER: 0.1507
2026-01-04 13:10:23,922: t15.2023.08.18 val PER: 0.1291
2026-01-04 13:10:23,922: t15.2023.08.20 val PER: 0.1493
2026-01-04 13:10:23,922: t15.2023.08.25 val PER: 0.1039
2026-01-04 13:10:23,922: t15.2023.08.27 val PER: 0.2106
2026-01-04 13:10:23,922: t15.2023.09.01 val PER: 0.1218
2026-01-04 13:10:23,922: t15.2023.09.03 val PER: 0.1841
2026-01-04 13:10:23,922: t15.2023.09.24 val PER: 0.1456
2026-01-04 13:10:23,922: t15.2023.09.29 val PER: 0.1595
2026-01-04 13:10:23,923: t15.2023.10.01 val PER: 0.2034
2026-01-04 13:10:23,923: t15.2023.10.06 val PER: 0.1152
2026-01-04 13:10:23,923: t15.2023.10.08 val PER: 0.2666
2026-01-04 13:10:23,923: t15.2023.10.13 val PER: 0.2529
2026-01-04 13:10:23,923: t15.2023.10.15 val PER: 0.1846
2026-01-04 13:10:23,923: t15.2023.10.20 val PER: 0.1946
2026-01-04 13:10:23,923: t15.2023.10.22 val PER: 0.1470
2026-01-04 13:10:23,923: t15.2023.11.03 val PER: 0.2062
2026-01-04 13:10:23,923: t15.2023.11.04 val PER: 0.0478
2026-01-04 13:10:23,923: t15.2023.11.17 val PER: 0.0622
2026-01-04 13:10:23,923: t15.2023.11.19 val PER: 0.0599
2026-01-04 13:10:23,923: t15.2023.11.26 val PER: 0.1891
2026-01-04 13:10:23,923: t15.2023.12.03 val PER: 0.1597
2026-01-04 13:10:23,923: t15.2023.12.08 val PER: 0.1451
2026-01-04 13:10:23,924: t15.2023.12.10 val PER: 0.1353
2026-01-04 13:10:23,924: t15.2023.12.17 val PER: 0.1684
2026-01-04 13:10:23,924: t15.2023.12.29 val PER: 0.1826
2026-01-04 13:10:23,924: t15.2024.02.25 val PER: 0.1461
2026-01-04 13:10:23,924: t15.2024.03.08 val PER: 0.2717
2026-01-04 13:10:23,924: t15.2024.03.15 val PER: 0.2464
2026-01-04 13:10:23,924: t15.2024.03.17 val PER: 0.1792
2026-01-04 13:10:23,924: t15.2024.05.10 val PER: 0.2095
2026-01-04 13:10:23,924: t15.2024.06.14 val PER: 0.1940
2026-01-04 13:10:23,924: t15.2024.07.19 val PER: 0.2914
2026-01-04 13:10:23,924: t15.2024.07.21 val PER: 0.1283
2026-01-04 13:10:23,924: t15.2024.07.28 val PER: 0.1713
2026-01-04 13:10:23,924: t15.2025.01.10 val PER: 0.3512
2026-01-04 13:10:23,924: t15.2025.01.12 val PER: 0.1878
2026-01-04 13:10:23,924: t15.2025.03.14 val PER: 0.3521
2026-01-04 13:10:23,925: t15.2025.03.16 val PER: 0.2369
2026-01-04 13:10:23,925: t15.2025.03.30 val PER: 0.3437
2026-01-04 13:10:23,925: t15.2025.04.13 val PER: 0.2596
2026-01-04 13:10:24,226: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_7500
2026-01-04 13:10:34,774: Train batch 7600: loss: 16.42 grad norm: 61.21 time: 0.069
2026-01-04 13:10:55,695: Train batch 7800: loss: 14.10 grad norm: 58.84 time: 0.057
2026-01-04 13:11:18,280: Train batch 8000: loss: 11.32 grad norm: 50.02 time: 0.073
2026-01-04 13:11:18,281: Running test after training batch: 8000
2026-01-04 13:11:18,413: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:11:24,603: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 13:11:24,677: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nit
2026-01-04 13:11:28,149: Val batch 8000: PER (avg): 0.1824 CTC Loss (avg): 18.0088 WER(1gram): 54.82% (n=64) time: 9.867
2026-01-04 13:11:28,149: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-04 13:11:28,150: t15.2023.08.13 val PER: 0.1435
2026-01-04 13:11:28,150: t15.2023.08.18 val PER: 0.1282
2026-01-04 13:11:28,150: t15.2023.08.20 val PER: 0.1454
2026-01-04 13:11:28,150: t15.2023.08.25 val PER: 0.1084
2026-01-04 13:11:28,150: t15.2023.08.27 val PER: 0.2090
2026-01-04 13:11:28,150: t15.2023.09.01 val PER: 0.0990
2026-01-04 13:11:28,150: t15.2023.09.03 val PER: 0.1888
2026-01-04 13:11:28,150: t15.2023.09.24 val PER: 0.1481
2026-01-04 13:11:28,150: t15.2023.09.29 val PER: 0.1512
2026-01-04 13:11:28,151: t15.2023.10.01 val PER: 0.1968
2026-01-04 13:11:28,151: t15.2023.10.06 val PER: 0.1098
2026-01-04 13:11:28,151: t15.2023.10.08 val PER: 0.2652
2026-01-04 13:11:28,151: t15.2023.10.13 val PER: 0.2459
2026-01-04 13:11:28,151: t15.2023.10.15 val PER: 0.1826
2026-01-04 13:11:28,151: t15.2023.10.20 val PER: 0.2114
2026-01-04 13:11:28,151: t15.2023.10.22 val PER: 0.1347
2026-01-04 13:11:28,151: t15.2023.11.03 val PER: 0.2028
2026-01-04 13:11:28,151: t15.2023.11.04 val PER: 0.0341
2026-01-04 13:11:28,151: t15.2023.11.17 val PER: 0.0575
2026-01-04 13:11:28,151: t15.2023.11.19 val PER: 0.0639
2026-01-04 13:11:28,151: t15.2023.11.26 val PER: 0.1775
2026-01-04 13:11:28,152: t15.2023.12.03 val PER: 0.1513
2026-01-04 13:11:28,152: t15.2023.12.08 val PER: 0.1425
2026-01-04 13:11:28,152: t15.2023.12.10 val PER: 0.1288
2026-01-04 13:11:28,152: t15.2023.12.17 val PER: 0.1726
2026-01-04 13:11:28,152: t15.2023.12.29 val PER: 0.1702
2026-01-04 13:11:28,152: t15.2024.02.25 val PER: 0.1334
2026-01-04 13:11:28,152: t15.2024.03.08 val PER: 0.2546
2026-01-04 13:11:28,152: t15.2024.03.15 val PER: 0.2402
2026-01-04 13:11:28,152: t15.2024.03.17 val PER: 0.1722
2026-01-04 13:11:28,152: t15.2024.05.10 val PER: 0.2021
2026-01-04 13:11:28,152: t15.2024.06.14 val PER: 0.2035
2026-01-04 13:11:28,152: t15.2024.07.19 val PER: 0.2874
2026-01-04 13:11:28,152: t15.2024.07.21 val PER: 0.1159
2026-01-04 13:11:28,153: t15.2024.07.28 val PER: 0.1596
2026-01-04 13:11:28,153: t15.2025.01.10 val PER: 0.3333
2026-01-04 13:11:28,153: t15.2025.01.12 val PER: 0.1817
2026-01-04 13:11:28,153: t15.2025.03.14 val PER: 0.3521
2026-01-04 13:11:28,153: t15.2025.03.16 val PER: 0.2343
2026-01-04 13:11:28,153: t15.2025.03.30 val PER: 0.3437
2026-01-04 13:11:28,153: t15.2025.04.13 val PER: 0.2611
2026-01-04 13:11:28,481: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_8000
2026-01-04 13:11:49,554: Train batch 8200: loss: 9.84 grad norm: 48.46 time: 0.055
2026-01-04 13:12:10,771: Train batch 8400: loss: 10.10 grad norm: 49.63 time: 0.064
2026-01-04 13:12:21,443: Running test after training batch: 8500
2026-01-04 13:12:21,612: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:12:26,971: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 13:12:27,035: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sent
2026-01-04 13:12:30,677: Val batch 8500: PER (avg): 0.1775 CTC Loss (avg): 17.5858 WER(1gram): 52.54% (n=64) time: 9.234
2026-01-04 13:12:30,678: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-04 13:12:30,678: t15.2023.08.13 val PER: 0.1341
2026-01-04 13:12:30,679: t15.2023.08.18 val PER: 0.1308
2026-01-04 13:12:30,679: t15.2023.08.20 val PER: 0.1414
2026-01-04 13:12:30,679: t15.2023.08.25 val PER: 0.1009
2026-01-04 13:12:30,679: t15.2023.08.27 val PER: 0.2090
2026-01-04 13:12:30,679: t15.2023.09.01 val PER: 0.1031
2026-01-04 13:12:30,679: t15.2023.09.03 val PER: 0.1888
2026-01-04 13:12:30,679: t15.2023.09.24 val PER: 0.1529
2026-01-04 13:12:30,679: t15.2023.09.29 val PER: 0.1512
2026-01-04 13:12:30,680: t15.2023.10.01 val PER: 0.1889
2026-01-04 13:12:30,680: t15.2023.10.06 val PER: 0.1076
2026-01-04 13:12:30,680: t15.2023.10.08 val PER: 0.2571
2026-01-04 13:12:30,680: t15.2023.10.13 val PER: 0.2389
2026-01-04 13:12:30,680: t15.2023.10.15 val PER: 0.1826
2026-01-04 13:12:30,680: t15.2023.10.20 val PER: 0.1913
2026-01-04 13:12:30,681: t15.2023.10.22 val PER: 0.1392
2026-01-04 13:12:30,681: t15.2023.11.03 val PER: 0.1940
2026-01-04 13:12:30,681: t15.2023.11.04 val PER: 0.0478
2026-01-04 13:12:30,681: t15.2023.11.17 val PER: 0.0544
2026-01-04 13:12:30,681: t15.2023.11.19 val PER: 0.0419
2026-01-04 13:12:30,681: t15.2023.11.26 val PER: 0.1703
2026-01-04 13:12:30,681: t15.2023.12.03 val PER: 0.1418
2026-01-04 13:12:30,681: t15.2023.12.08 val PER: 0.1358
2026-01-04 13:12:30,681: t15.2023.12.10 val PER: 0.1170
2026-01-04 13:12:30,681: t15.2023.12.17 val PER: 0.1590
2026-01-04 13:12:30,681: t15.2023.12.29 val PER: 0.1592
2026-01-04 13:12:30,681: t15.2024.02.25 val PER: 0.1362
2026-01-04 13:12:30,681: t15.2024.03.08 val PER: 0.2646
2026-01-04 13:12:30,681: t15.2024.03.15 val PER: 0.2345
2026-01-04 13:12:30,682: t15.2024.03.17 val PER: 0.1736
2026-01-04 13:12:30,682: t15.2024.05.10 val PER: 0.1902
2026-01-04 13:12:30,682: t15.2024.06.14 val PER: 0.1924
2026-01-04 13:12:30,682: t15.2024.07.19 val PER: 0.2703
2026-01-04 13:12:30,682: t15.2024.07.21 val PER: 0.1269
2026-01-04 13:12:30,682: t15.2024.07.28 val PER: 0.1610
2026-01-04 13:12:30,682: t15.2025.01.10 val PER: 0.3320
2026-01-04 13:12:30,682: t15.2025.01.12 val PER: 0.1763
2026-01-04 13:12:30,682: t15.2025.03.14 val PER: 0.3521
2026-01-04 13:12:30,682: t15.2025.03.16 val PER: 0.2186
2026-01-04 13:12:30,682: t15.2025.03.30 val PER: 0.3287
2026-01-04 13:12:30,682: t15.2025.04.13 val PER: 0.2268
2026-01-04 13:12:31,021: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_8500
2026-01-04 13:12:41,563: Train batch 8600: loss: 15.81 grad norm: 59.17 time: 0.055
2026-01-04 13:13:02,318: Train batch 8800: loss: 15.37 grad norm: 59.85 time: 0.060
2026-01-04 13:13:22,943: Train batch 9000: loss: 16.00 grad norm: 66.73 time: 0.071
2026-01-04 13:13:22,944: Running test after training batch: 9000
2026-01-04 13:13:23,096: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:13:28,102: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 13:13:28,134: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 13:13:29,946: Val batch 9000: PER (avg): 0.1735 CTC Loss (avg): 17.2463 WER(1gram): 50.25% (n=64) time: 7.002
2026-01-04 13:13:29,946: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 13:13:29,947: t15.2023.08.13 val PER: 0.1351
2026-01-04 13:13:29,947: t15.2023.08.18 val PER: 0.1249
2026-01-04 13:13:29,947: t15.2023.08.20 val PER: 0.1295
2026-01-04 13:13:29,947: t15.2023.08.25 val PER: 0.0949
2026-01-04 13:13:29,947: t15.2023.08.27 val PER: 0.2042
2026-01-04 13:13:29,947: t15.2023.09.01 val PER: 0.0982
2026-01-04 13:13:29,948: t15.2023.09.03 val PER: 0.1841
2026-01-04 13:13:29,948: t15.2023.09.24 val PER: 0.1444
2026-01-04 13:13:29,948: t15.2023.09.29 val PER: 0.1455
2026-01-04 13:13:29,948: t15.2023.10.01 val PER: 0.1869
2026-01-04 13:13:29,948: t15.2023.10.06 val PER: 0.0936
2026-01-04 13:13:29,948: t15.2023.10.08 val PER: 0.2544
2026-01-04 13:13:29,948: t15.2023.10.13 val PER: 0.2351
2026-01-04 13:13:29,949: t15.2023.10.15 val PER: 0.1780
2026-01-04 13:13:29,949: t15.2023.10.20 val PER: 0.1946
2026-01-04 13:13:29,949: t15.2023.10.22 val PER: 0.1303
2026-01-04 13:13:29,949: t15.2023.11.03 val PER: 0.2056
2026-01-04 13:13:29,949: t15.2023.11.04 val PER: 0.0341
2026-01-04 13:13:29,949: t15.2023.11.17 val PER: 0.0544
2026-01-04 13:13:29,949: t15.2023.11.19 val PER: 0.0619
2026-01-04 13:13:29,949: t15.2023.11.26 val PER: 0.1674
2026-01-04 13:13:29,949: t15.2023.12.03 val PER: 0.1366
2026-01-04 13:13:29,949: t15.2023.12.08 val PER: 0.1265
2026-01-04 13:13:29,950: t15.2023.12.10 val PER: 0.1130
2026-01-04 13:13:29,950: t15.2023.12.17 val PER: 0.1559
2026-01-04 13:13:29,950: t15.2023.12.29 val PER: 0.1668
2026-01-04 13:13:29,950: t15.2024.02.25 val PER: 0.1433
2026-01-04 13:13:29,950: t15.2024.03.08 val PER: 0.2660
2026-01-04 13:13:29,950: t15.2024.03.15 val PER: 0.2289
2026-01-04 13:13:29,950: t15.2024.03.17 val PER: 0.1729
2026-01-04 13:13:29,950: t15.2024.05.10 val PER: 0.1842
2026-01-04 13:13:29,950: t15.2024.06.14 val PER: 0.1861
2026-01-04 13:13:29,950: t15.2024.07.19 val PER: 0.2630
2026-01-04 13:13:29,950: t15.2024.07.21 val PER: 0.1124
2026-01-04 13:13:29,950: t15.2024.07.28 val PER: 0.1449
2026-01-04 13:13:29,950: t15.2025.01.10 val PER: 0.3072
2026-01-04 13:13:29,950: t15.2025.01.12 val PER: 0.1755
2026-01-04 13:13:29,950: t15.2025.03.14 val PER: 0.3565
2026-01-04 13:13:29,951: t15.2025.03.16 val PER: 0.2147
2026-01-04 13:13:29,951: t15.2025.03.30 val PER: 0.3138
2026-01-04 13:13:29,951: t15.2025.04.13 val PER: 0.2553
2026-01-04 13:13:29,952: New best val WER(1gram) 52.28% --> 50.25%
2026-01-04 13:13:29,952: Checkpointing model
2026-01-04 13:13:30,629: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 13:13:30,946: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_9000
2026-01-04 13:13:49,663: Train batch 9200: loss: 10.64 grad norm: 49.08 time: 0.057
2026-01-04 13:14:09,812: Train batch 9400: loss: 7.45 grad norm: 44.27 time: 0.068
2026-01-04 13:14:19,102: Running test after training batch: 9500
2026-01-04 13:14:19,276: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:14:24,610: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 13:14:24,681: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-04 13:14:28,462: Val batch 9500: PER (avg): 0.1728 CTC Loss (avg): 17.0522 WER(1gram): 50.25% (n=64) time: 9.360
2026-01-04 13:14:28,463: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 13:14:28,463: t15.2023.08.13 val PER: 0.1216
2026-01-04 13:14:28,463: t15.2023.08.18 val PER: 0.1241
2026-01-04 13:14:28,463: t15.2023.08.20 val PER: 0.1271
2026-01-04 13:14:28,464: t15.2023.08.25 val PER: 0.0934
2026-01-04 13:14:28,464: t15.2023.08.27 val PER: 0.2074
2026-01-04 13:14:28,464: t15.2023.09.01 val PER: 0.0966
2026-01-04 13:14:28,464: t15.2023.09.03 val PER: 0.1912
2026-01-04 13:14:28,464: t15.2023.09.24 val PER: 0.1468
2026-01-04 13:14:28,464: t15.2023.09.29 val PER: 0.1442
2026-01-04 13:14:28,464: t15.2023.10.01 val PER: 0.1889
2026-01-04 13:14:28,464: t15.2023.10.06 val PER: 0.1076
2026-01-04 13:14:28,464: t15.2023.10.08 val PER: 0.2666
2026-01-04 13:14:28,464: t15.2023.10.13 val PER: 0.2397
2026-01-04 13:14:28,464: t15.2023.10.15 val PER: 0.1734
2026-01-04 13:14:28,465: t15.2023.10.20 val PER: 0.1946
2026-01-04 13:14:28,465: t15.2023.10.22 val PER: 0.1269
2026-01-04 13:14:28,465: t15.2023.11.03 val PER: 0.1900
2026-01-04 13:14:28,465: t15.2023.11.04 val PER: 0.0375
2026-01-04 13:14:28,465: t15.2023.11.17 val PER: 0.0498
2026-01-04 13:14:28,465: t15.2023.11.19 val PER: 0.0519
2026-01-04 13:14:28,465: t15.2023.11.26 val PER: 0.1565
2026-01-04 13:14:28,466: t15.2023.12.03 val PER: 0.1376
2026-01-04 13:14:28,466: t15.2023.12.08 val PER: 0.1305
2026-01-04 13:14:28,466: t15.2023.12.10 val PER: 0.1156
2026-01-04 13:14:28,466: t15.2023.12.17 val PER: 0.1622
2026-01-04 13:14:28,467: t15.2023.12.29 val PER: 0.1579
2026-01-04 13:14:28,467: t15.2024.02.25 val PER: 0.1348
2026-01-04 13:14:28,467: t15.2024.03.08 val PER: 0.2560
2026-01-04 13:14:28,467: t15.2024.03.15 val PER: 0.2264
2026-01-04 13:14:28,467: t15.2024.03.17 val PER: 0.1639
2026-01-04 13:14:28,467: t15.2024.05.10 val PER: 0.1842
2026-01-04 13:14:28,467: t15.2024.06.14 val PER: 0.1814
2026-01-04 13:14:28,467: t15.2024.07.19 val PER: 0.2696
2026-01-04 13:14:28,467: t15.2024.07.21 val PER: 0.1097
2026-01-04 13:14:28,468: t15.2024.07.28 val PER: 0.1551
2026-01-04 13:14:28,468: t15.2025.01.10 val PER: 0.3278
2026-01-04 13:14:28,468: t15.2025.01.12 val PER: 0.1771
2026-01-04 13:14:28,468: t15.2025.03.14 val PER: 0.3728
2026-01-04 13:14:28,468: t15.2025.03.16 val PER: 0.2055
2026-01-04 13:14:28,468: t15.2025.03.30 val PER: 0.3241
2026-01-04 13:14:28,468: t15.2025.04.13 val PER: 0.2354
2026-01-04 13:14:28,794: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_9500
2026-01-04 13:14:38,864: Train batch 9600: loss: 8.50 grad norm: 48.12 time: 0.074
2026-01-04 13:14:59,249: Train batch 9800: loss: 12.73 grad norm: 59.40 time: 0.064
2026-01-04 13:15:20,283: Train batch 10000: loss: 5.08 grad norm: 33.47 time: 0.060
2026-01-04 13:15:20,283: Running test after training batch: 10000
2026-01-04 13:15:20,699: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:15:25,988: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 13:15:26,022: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 13:15:27,877: Val batch 10000: PER (avg): 0.1689 CTC Loss (avg): 16.7212 WER(1gram): 50.25% (n=64) time: 7.594
2026-01-04 13:15:27,877: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-04 13:15:27,877: t15.2023.08.13 val PER: 0.1268
2026-01-04 13:15:27,878: t15.2023.08.18 val PER: 0.1249
2026-01-04 13:15:27,878: t15.2023.08.20 val PER: 0.1271
2026-01-04 13:15:27,878: t15.2023.08.25 val PER: 0.0964
2026-01-04 13:15:27,878: t15.2023.08.27 val PER: 0.2090
2026-01-04 13:15:27,878: t15.2023.09.01 val PER: 0.0917
2026-01-04 13:15:27,878: t15.2023.09.03 val PER: 0.1817
2026-01-04 13:15:27,878: t15.2023.09.24 val PER: 0.1481
2026-01-04 13:15:27,878: t15.2023.09.29 val PER: 0.1455
2026-01-04 13:15:27,879: t15.2023.10.01 val PER: 0.1863
2026-01-04 13:15:27,879: t15.2023.10.06 val PER: 0.1119
2026-01-04 13:15:27,879: t15.2023.10.08 val PER: 0.2544
2026-01-04 13:15:27,879: t15.2023.10.13 val PER: 0.2203
2026-01-04 13:15:27,879: t15.2023.10.15 val PER: 0.1694
2026-01-04 13:15:27,879: t15.2023.10.20 val PER: 0.1913
2026-01-04 13:15:27,879: t15.2023.10.22 val PER: 0.1214
2026-01-04 13:15:27,879: t15.2023.11.03 val PER: 0.1852
2026-01-04 13:15:27,879: t15.2023.11.04 val PER: 0.0375
2026-01-04 13:15:27,880: t15.2023.11.17 val PER: 0.0451
2026-01-04 13:15:27,880: t15.2023.11.19 val PER: 0.0459
2026-01-04 13:15:27,880: t15.2023.11.26 val PER: 0.1486
2026-01-04 13:15:27,880: t15.2023.12.03 val PER: 0.1324
2026-01-04 13:15:27,880: t15.2023.12.08 val PER: 0.1332
2026-01-04 13:15:27,880: t15.2023.12.10 val PER: 0.1091
2026-01-04 13:15:27,880: t15.2023.12.17 val PER: 0.1663
2026-01-04 13:15:27,880: t15.2023.12.29 val PER: 0.1510
2026-01-04 13:15:27,880: t15.2024.02.25 val PER: 0.1390
2026-01-04 13:15:27,880: t15.2024.03.08 val PER: 0.2546
2026-01-04 13:15:27,880: t15.2024.03.15 val PER: 0.2158
2026-01-04 13:15:27,880: t15.2024.03.17 val PER: 0.1597
2026-01-04 13:15:27,880: t15.2024.05.10 val PER: 0.1724
2026-01-04 13:15:27,881: t15.2024.06.14 val PER: 0.1845
2026-01-04 13:15:27,881: t15.2024.07.19 val PER: 0.2749
2026-01-04 13:15:27,881: t15.2024.07.21 val PER: 0.1110
2026-01-04 13:15:27,881: t15.2024.07.28 val PER: 0.1471
2026-01-04 13:15:27,881: t15.2025.01.10 val PER: 0.3072
2026-01-04 13:15:27,881: t15.2025.01.12 val PER: 0.1709
2026-01-04 13:15:27,882: t15.2025.03.14 val PER: 0.3476
2026-01-04 13:15:27,882: t15.2025.03.16 val PER: 0.2173
2026-01-04 13:15:27,882: t15.2025.03.30 val PER: 0.3103
2026-01-04 13:15:27,882: t15.2025.04.13 val PER: 0.2268
2026-01-04 13:15:28,194: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_10000
2026-01-04 13:15:47,211: Train batch 10200: loss: 6.43 grad norm: 38.20 time: 0.050
2026-01-04 13:16:06,272: Train batch 10400: loss: 9.30 grad norm: 58.12 time: 0.072
2026-01-04 13:16:15,732: Running test after training batch: 10500
2026-01-04 13:16:16,082: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:16:23,363: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 13:16:23,470: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-04 13:16:29,419: Val batch 10500: PER (avg): 0.1663 CTC Loss (avg): 16.4968 WER(1gram): 49.75% (n=64) time: 13.687
2026-01-04 13:16:29,420: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-04 13:16:29,420: t15.2023.08.13 val PER: 0.1206
2026-01-04 13:16:29,420: t15.2023.08.18 val PER: 0.1174
2026-01-04 13:16:29,420: t15.2023.08.20 val PER: 0.1215
2026-01-04 13:16:29,421: t15.2023.08.25 val PER: 0.1009
2026-01-04 13:16:29,421: t15.2023.08.27 val PER: 0.2010
2026-01-04 13:16:29,421: t15.2023.09.01 val PER: 0.0885
2026-01-04 13:16:29,421: t15.2023.09.03 val PER: 0.1770
2026-01-04 13:16:29,421: t15.2023.09.24 val PER: 0.1529
2026-01-04 13:16:29,421: t15.2023.09.29 val PER: 0.1493
2026-01-04 13:16:29,421: t15.2023.10.01 val PER: 0.1889
2026-01-04 13:16:29,421: t15.2023.10.06 val PER: 0.0969
2026-01-04 13:16:29,421: t15.2023.10.08 val PER: 0.2544
2026-01-04 13:16:29,421: t15.2023.10.13 val PER: 0.2141
2026-01-04 13:16:29,421: t15.2023.10.15 val PER: 0.1753
2026-01-04 13:16:29,422: t15.2023.10.20 val PER: 0.1779
2026-01-04 13:16:29,422: t15.2023.10.22 val PER: 0.1114
2026-01-04 13:16:29,422: t15.2023.11.03 val PER: 0.1961
2026-01-04 13:16:29,422: t15.2023.11.04 val PER: 0.0375
2026-01-04 13:16:29,422: t15.2023.11.17 val PER: 0.0482
2026-01-04 13:16:29,422: t15.2023.11.19 val PER: 0.0519
2026-01-04 13:16:29,422: t15.2023.11.26 val PER: 0.1362
2026-01-04 13:16:29,422: t15.2023.12.03 val PER: 0.1355
2026-01-04 13:16:29,422: t15.2023.12.08 val PER: 0.1138
2026-01-04 13:16:29,422: t15.2023.12.10 val PER: 0.1064
2026-01-04 13:16:29,422: t15.2023.12.17 val PER: 0.1601
2026-01-04 13:16:29,422: t15.2023.12.29 val PER: 0.1531
2026-01-04 13:16:29,422: t15.2024.02.25 val PER: 0.1208
2026-01-04 13:16:29,422: t15.2024.03.08 val PER: 0.2546
2026-01-04 13:16:29,422: t15.2024.03.15 val PER: 0.2214
2026-01-04 13:16:29,423: t15.2024.03.17 val PER: 0.1583
2026-01-04 13:16:29,423: t15.2024.05.10 val PER: 0.1753
2026-01-04 13:16:29,423: t15.2024.06.14 val PER: 0.1782
2026-01-04 13:16:29,423: t15.2024.07.19 val PER: 0.2637
2026-01-04 13:16:29,423: t15.2024.07.21 val PER: 0.1076
2026-01-04 13:16:29,423: t15.2024.07.28 val PER: 0.1419
2026-01-04 13:16:29,423: t15.2025.01.10 val PER: 0.3182
2026-01-04 13:16:29,423: t15.2025.01.12 val PER: 0.1724
2026-01-04 13:16:29,423: t15.2025.03.14 val PER: 0.3580
2026-01-04 13:16:29,423: t15.2025.03.16 val PER: 0.1950
2026-01-04 13:16:29,423: t15.2025.03.30 val PER: 0.3103
2026-01-04 13:16:29,423: t15.2025.04.13 val PER: 0.2282
2026-01-04 13:16:29,425: New best val WER(1gram) 50.25% --> 49.75%
2026-01-04 13:16:29,425: Checkpointing model
2026-01-04 13:16:30,137: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 13:16:30,494: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_10500
2026-01-04 13:16:40,239: Train batch 10600: loss: 9.07 grad norm: 52.39 time: 0.076
2026-01-04 13:16:59,019: Train batch 10800: loss: 14.75 grad norm: 64.19 time: 0.065
2026-01-04 13:17:17,809: Train batch 11000: loss: 14.49 grad norm: 65.60 time: 0.057
2026-01-04 13:17:17,809: Running test after training batch: 11000
2026-01-04 13:17:17,971: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:17:23,124: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-04 13:17:23,157: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-04 13:17:25,054: Val batch 11000: PER (avg): 0.1634 CTC Loss (avg): 16.3960 WER(1gram): 49.24% (n=64) time: 7.244
2026-01-04 13:17:25,054: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 13:17:25,054: t15.2023.08.13 val PER: 0.1247
2026-01-04 13:17:25,054: t15.2023.08.18 val PER: 0.1157
2026-01-04 13:17:25,055: t15.2023.08.20 val PER: 0.1255
2026-01-04 13:17:25,055: t15.2023.08.25 val PER: 0.1009
2026-01-04 13:17:25,055: t15.2023.08.27 val PER: 0.1994
2026-01-04 13:17:25,055: t15.2023.09.01 val PER: 0.0860
2026-01-04 13:17:25,055: t15.2023.09.03 val PER: 0.1793
2026-01-04 13:17:25,055: t15.2023.09.24 val PER: 0.1444
2026-01-04 13:17:25,055: t15.2023.09.29 val PER: 0.1372
2026-01-04 13:17:25,055: t15.2023.10.01 val PER: 0.1909
2026-01-04 13:17:25,055: t15.2023.10.06 val PER: 0.0840
2026-01-04 13:17:25,055: t15.2023.10.08 val PER: 0.2666
2026-01-04 13:17:25,055: t15.2023.10.13 val PER: 0.2258
2026-01-04 13:17:25,056: t15.2023.10.15 val PER: 0.1734
2026-01-04 13:17:25,056: t15.2023.10.20 val PER: 0.1946
2026-01-04 13:17:25,056: t15.2023.10.22 val PER: 0.1158
2026-01-04 13:17:25,056: t15.2023.11.03 val PER: 0.1866
2026-01-04 13:17:25,056: t15.2023.11.04 val PER: 0.0375
2026-01-04 13:17:25,056: t15.2023.11.17 val PER: 0.0451
2026-01-04 13:17:25,056: t15.2023.11.19 val PER: 0.0359
2026-01-04 13:17:25,056: t15.2023.11.26 val PER: 0.1420
2026-01-04 13:17:25,056: t15.2023.12.03 val PER: 0.1229
2026-01-04 13:17:25,057: t15.2023.12.08 val PER: 0.1225
2026-01-04 13:17:25,057: t15.2023.12.10 val PER: 0.1051
2026-01-04 13:17:25,057: t15.2023.12.17 val PER: 0.1466
2026-01-04 13:17:25,057: t15.2023.12.29 val PER: 0.1455
2026-01-04 13:17:25,057: t15.2024.02.25 val PER: 0.1278
2026-01-04 13:17:25,057: t15.2024.03.08 val PER: 0.2418
2026-01-04 13:17:25,057: t15.2024.03.15 val PER: 0.2214
2026-01-04 13:17:25,057: t15.2024.03.17 val PER: 0.1499
2026-01-04 13:17:25,057: t15.2024.05.10 val PER: 0.1753
2026-01-04 13:17:25,057: t15.2024.06.14 val PER: 0.1719
2026-01-04 13:17:25,058: t15.2024.07.19 val PER: 0.2472
2026-01-04 13:17:25,058: t15.2024.07.21 val PER: 0.1041
2026-01-04 13:17:25,058: t15.2024.07.28 val PER: 0.1449
2026-01-04 13:17:25,058: t15.2025.01.10 val PER: 0.3044
2026-01-04 13:17:25,058: t15.2025.01.12 val PER: 0.1617
2026-01-04 13:17:25,058: t15.2025.03.14 val PER: 0.3550
2026-01-04 13:17:25,059: t15.2025.03.16 val PER: 0.1937
2026-01-04 13:17:25,059: t15.2025.03.30 val PER: 0.3000
2026-01-04 13:17:25,059: t15.2025.04.13 val PER: 0.2297
2026-01-04 13:17:25,060: New best val WER(1gram) 49.75% --> 49.24%
2026-01-04 13:17:25,060: Checkpointing model
2026-01-04 13:17:25,747: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 13:17:26,091: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_11000
2026-01-04 13:17:44,890: Train batch 11200: loss: 10.51 grad norm: 52.27 time: 0.072
2026-01-04 13:18:03,533: Train batch 11400: loss: 9.68 grad norm: 52.64 time: 0.057
2026-01-04 13:18:12,999: Running test after training batch: 11500
2026-01-04 13:18:13,181: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:18:18,526: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-04 13:18:18,612: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-04 13:18:23,469: Val batch 11500: PER (avg): 0.1614 CTC Loss (avg): 16.3761 WER(1gram): 51.27% (n=64) time: 10.469
2026-01-04 13:18:23,469: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 13:18:23,470: t15.2023.08.13 val PER: 0.1185
2026-01-04 13:18:23,470: t15.2023.08.18 val PER: 0.1165
2026-01-04 13:18:23,470: t15.2023.08.20 val PER: 0.1120
2026-01-04 13:18:23,470: t15.2023.08.25 val PER: 0.0979
2026-01-04 13:18:23,470: t15.2023.08.27 val PER: 0.2010
2026-01-04 13:18:23,470: t15.2023.09.01 val PER: 0.0852
2026-01-04 13:18:23,470: t15.2023.09.03 val PER: 0.1710
2026-01-04 13:18:23,471: t15.2023.09.24 val PER: 0.1335
2026-01-04 13:18:23,471: t15.2023.09.29 val PER: 0.1359
2026-01-04 13:18:23,471: t15.2023.10.01 val PER: 0.1889
2026-01-04 13:18:23,471: t15.2023.10.06 val PER: 0.0861
2026-01-04 13:18:23,471: t15.2023.10.08 val PER: 0.2585
2026-01-04 13:18:23,471: t15.2023.10.13 val PER: 0.2141
2026-01-04 13:18:23,471: t15.2023.10.15 val PER: 0.1622
2026-01-04 13:18:23,471: t15.2023.10.20 val PER: 0.2047
2026-01-04 13:18:23,471: t15.2023.10.22 val PER: 0.1158
2026-01-04 13:18:23,471: t15.2023.11.03 val PER: 0.1832
2026-01-04 13:18:23,472: t15.2023.11.04 val PER: 0.0341
2026-01-04 13:18:23,472: t15.2023.11.17 val PER: 0.0467
2026-01-04 13:18:23,472: t15.2023.11.19 val PER: 0.0519
2026-01-04 13:18:23,472: t15.2023.11.26 val PER: 0.1304
2026-01-04 13:18:23,472: t15.2023.12.03 val PER: 0.1261
2026-01-04 13:18:23,472: t15.2023.12.08 val PER: 0.1138
2026-01-04 13:18:23,472: t15.2023.12.10 val PER: 0.1012
2026-01-04 13:18:23,472: t15.2023.12.17 val PER: 0.1497
2026-01-04 13:18:23,472: t15.2023.12.29 val PER: 0.1400
2026-01-04 13:18:23,472: t15.2024.02.25 val PER: 0.1250
2026-01-04 13:18:23,473: t15.2024.03.08 val PER: 0.2361
2026-01-04 13:18:23,473: t15.2024.03.15 val PER: 0.2189
2026-01-04 13:18:23,473: t15.2024.03.17 val PER: 0.1506
2026-01-04 13:18:23,473: t15.2024.05.10 val PER: 0.1724
2026-01-04 13:18:23,473: t15.2024.06.14 val PER: 0.1909
2026-01-04 13:18:23,473: t15.2024.07.19 val PER: 0.2591
2026-01-04 13:18:23,473: t15.2024.07.21 val PER: 0.1069
2026-01-04 13:18:23,473: t15.2024.07.28 val PER: 0.1456
2026-01-04 13:18:23,473: t15.2025.01.10 val PER: 0.3044
2026-01-04 13:18:23,474: t15.2025.01.12 val PER: 0.1570
2026-01-04 13:18:23,474: t15.2025.03.14 val PER: 0.3521
2026-01-04 13:18:23,474: t15.2025.03.16 val PER: 0.2029
2026-01-04 13:18:23,474: t15.2025.03.30 val PER: 0.3103
2026-01-04 13:18:23,474: t15.2025.04.13 val PER: 0.2211
2026-01-04 13:18:23,810: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_11500
2026-01-04 13:18:33,688: Train batch 11600: loss: 11.28 grad norm: 47.16 time: 0.061
2026-01-04 13:18:53,918: Train batch 11800: loss: 6.84 grad norm: 44.49 time: 0.045
2026-01-04 13:19:13,949: Train batch 12000: loss: 13.85 grad norm: 56.92 time: 0.072
2026-01-04 13:19:13,949: Running test after training batch: 12000
2026-01-04 13:19:14,054: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:19:19,354: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 13:19:19,436: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-04 13:19:24,204: Val batch 12000: PER (avg): 0.1588 CTC Loss (avg): 16.1747 WER(1gram): 50.00% (n=64) time: 10.254
2026-01-04 13:19:24,205: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 13:19:24,205: t15.2023.08.13 val PER: 0.1247
2026-01-04 13:19:24,205: t15.2023.08.18 val PER: 0.1165
2026-01-04 13:19:24,205: t15.2023.08.20 val PER: 0.1128
2026-01-04 13:19:24,205: t15.2023.08.25 val PER: 0.0934
2026-01-04 13:19:24,205: t15.2023.08.27 val PER: 0.1929
2026-01-04 13:19:24,205: t15.2023.09.01 val PER: 0.0836
2026-01-04 13:19:24,205: t15.2023.09.03 val PER: 0.1627
2026-01-04 13:19:24,205: t15.2023.09.24 val PER: 0.1359
2026-01-04 13:19:24,205: t15.2023.09.29 val PER: 0.1359
2026-01-04 13:19:24,206: t15.2023.10.01 val PER: 0.1717
2026-01-04 13:19:24,206: t15.2023.10.06 val PER: 0.0883
2026-01-04 13:19:24,206: t15.2023.10.08 val PER: 0.2558
2026-01-04 13:19:24,206: t15.2023.10.13 val PER: 0.2133
2026-01-04 13:19:24,206: t15.2023.10.15 val PER: 0.1575
2026-01-04 13:19:24,206: t15.2023.10.20 val PER: 0.1846
2026-01-04 13:19:24,206: t15.2023.10.22 val PER: 0.1169
2026-01-04 13:19:24,206: t15.2023.11.03 val PER: 0.1859
2026-01-04 13:19:24,206: t15.2023.11.04 val PER: 0.0375
2026-01-04 13:19:24,207: t15.2023.11.17 val PER: 0.0451
2026-01-04 13:19:24,207: t15.2023.11.19 val PER: 0.0439
2026-01-04 13:19:24,207: t15.2023.11.26 val PER: 0.1275
2026-01-04 13:19:24,207: t15.2023.12.03 val PER: 0.1176
2026-01-04 13:19:24,207: t15.2023.12.08 val PER: 0.1085
2026-01-04 13:19:24,207: t15.2023.12.10 val PER: 0.0986
2026-01-04 13:19:24,207: t15.2023.12.17 val PER: 0.1435
2026-01-04 13:19:24,207: t15.2023.12.29 val PER: 0.1407
2026-01-04 13:19:24,207: t15.2024.02.25 val PER: 0.1124
2026-01-04 13:19:24,208: t15.2024.03.08 val PER: 0.2404
2026-01-04 13:19:24,208: t15.2024.03.15 val PER: 0.2126
2026-01-04 13:19:24,208: t15.2024.03.17 val PER: 0.1506
2026-01-04 13:19:24,208: t15.2024.05.10 val PER: 0.1842
2026-01-04 13:19:24,208: t15.2024.06.14 val PER: 0.1861
2026-01-04 13:19:24,208: t15.2024.07.19 val PER: 0.2657
2026-01-04 13:19:24,209: t15.2024.07.21 val PER: 0.1021
2026-01-04 13:19:24,209: t15.2024.07.28 val PER: 0.1434
2026-01-04 13:19:24,209: t15.2025.01.10 val PER: 0.3003
2026-01-04 13:19:24,209: t15.2025.01.12 val PER: 0.1455
2026-01-04 13:19:24,209: t15.2025.03.14 val PER: 0.3536
2026-01-04 13:19:24,209: t15.2025.03.16 val PER: 0.2042
2026-01-04 13:19:24,209: t15.2025.03.30 val PER: 0.3069
2026-01-04 13:19:24,209: t15.2025.04.13 val PER: 0.2211
2026-01-04 13:19:24,546: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_12000
2026-01-04 13:19:45,727: Train batch 12200: loss: 5.60 grad norm: 41.78 time: 0.065
2026-01-04 13:20:06,716: Train batch 12400: loss: 4.77 grad norm: 37.47 time: 0.043
2026-01-04 13:20:17,512: Running test after training batch: 12500
2026-01-04 13:20:17,942: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:20:23,280: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 13:20:23,321: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-04 13:20:27,753: Val batch 12500: PER (avg): 0.1565 CTC Loss (avg): 15.9871 WER(1gram): 48.73% (n=64) time: 10.241
2026-01-04 13:20:27,754: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-04 13:20:27,754: t15.2023.08.13 val PER: 0.1247
2026-01-04 13:20:27,754: t15.2023.08.18 val PER: 0.1115
2026-01-04 13:20:27,754: t15.2023.08.20 val PER: 0.1136
2026-01-04 13:20:27,754: t15.2023.08.25 val PER: 0.0858
2026-01-04 13:20:27,755: t15.2023.08.27 val PER: 0.1929
2026-01-04 13:20:27,755: t15.2023.09.01 val PER: 0.0836
2026-01-04 13:20:27,755: t15.2023.09.03 val PER: 0.1591
2026-01-04 13:20:27,755: t15.2023.09.24 val PER: 0.1214
2026-01-04 13:20:27,755: t15.2023.09.29 val PER: 0.1347
2026-01-04 13:20:27,755: t15.2023.10.01 val PER: 0.1684
2026-01-04 13:20:27,755: t15.2023.10.06 val PER: 0.0861
2026-01-04 13:20:27,755: t15.2023.10.08 val PER: 0.2503
2026-01-04 13:20:27,755: t15.2023.10.13 val PER: 0.2064
2026-01-04 13:20:27,755: t15.2023.10.15 val PER: 0.1628
2026-01-04 13:20:27,755: t15.2023.10.20 val PER: 0.1980
2026-01-04 13:20:27,756: t15.2023.10.22 val PER: 0.1091
2026-01-04 13:20:27,756: t15.2023.11.03 val PER: 0.1852
2026-01-04 13:20:27,756: t15.2023.11.04 val PER: 0.0307
2026-01-04 13:20:27,756: t15.2023.11.17 val PER: 0.0451
2026-01-04 13:20:27,756: t15.2023.11.19 val PER: 0.0259
2026-01-04 13:20:27,756: t15.2023.11.26 val PER: 0.1268
2026-01-04 13:20:27,756: t15.2023.12.03 val PER: 0.1197
2026-01-04 13:20:27,756: t15.2023.12.08 val PER: 0.1119
2026-01-04 13:20:27,756: t15.2023.12.10 val PER: 0.0933
2026-01-04 13:20:27,756: t15.2023.12.17 val PER: 0.1538
2026-01-04 13:20:27,756: t15.2023.12.29 val PER: 0.1359
2026-01-04 13:20:27,757: t15.2024.02.25 val PER: 0.1222
2026-01-04 13:20:27,757: t15.2024.03.08 val PER: 0.2290
2026-01-04 13:20:27,757: t15.2024.03.15 val PER: 0.2220
2026-01-04 13:20:27,757: t15.2024.03.17 val PER: 0.1506
2026-01-04 13:20:27,757: t15.2024.05.10 val PER: 0.1753
2026-01-04 13:20:27,757: t15.2024.06.14 val PER: 0.1719
2026-01-04 13:20:27,757: t15.2024.07.19 val PER: 0.2446
2026-01-04 13:20:27,757: t15.2024.07.21 val PER: 0.1000
2026-01-04 13:20:27,757: t15.2024.07.28 val PER: 0.1353
2026-01-04 13:20:27,757: t15.2025.01.10 val PER: 0.2865
2026-01-04 13:20:27,758: t15.2025.01.12 val PER: 0.1493
2026-01-04 13:20:27,758: t15.2025.03.14 val PER: 0.3565
2026-01-04 13:20:27,758: t15.2025.03.16 val PER: 0.2042
2026-01-04 13:20:27,758: t15.2025.03.30 val PER: 0.3023
2026-01-04 13:20:27,758: t15.2025.04.13 val PER: 0.2268
2026-01-04 13:20:27,759: New best val WER(1gram) 49.24% --> 48.73%
2026-01-04 13:20:27,759: Checkpointing model
2026-01-04 13:20:28,466: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 13:20:28,814: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_12500
2026-01-04 13:20:39,349: Train batch 12600: loss: 7.82 grad norm: 43.28 time: 0.058
2026-01-04 13:21:00,160: Train batch 12800: loss: 5.77 grad norm: 40.42 time: 0.053
2026-01-04 13:21:21,159: Train batch 13000: loss: 6.13 grad norm: 39.27 time: 0.066
2026-01-04 13:21:21,159: Running test after training batch: 13000
2026-01-04 13:21:21,579: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:21:28,075: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-04 13:21:28,182: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost it
2026-01-04 13:21:34,024: Val batch 13000: PER (avg): 0.1548 CTC Loss (avg): 15.7606 WER(1gram): 47.46% (n=64) time: 12.864
2026-01-04 13:21:34,024: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 13:21:34,024: t15.2023.08.13 val PER: 0.1185
2026-01-04 13:21:34,025: t15.2023.08.18 val PER: 0.1090
2026-01-04 13:21:34,025: t15.2023.08.20 val PER: 0.1104
2026-01-04 13:21:34,025: t15.2023.08.25 val PER: 0.1009
2026-01-04 13:21:34,025: t15.2023.08.27 val PER: 0.1929
2026-01-04 13:21:34,025: t15.2023.09.01 val PER: 0.0844
2026-01-04 13:21:34,025: t15.2023.09.03 val PER: 0.1615
2026-01-04 13:21:34,025: t15.2023.09.24 val PER: 0.1286
2026-01-04 13:21:34,025: t15.2023.09.29 val PER: 0.1340
2026-01-04 13:21:34,026: t15.2023.10.01 val PER: 0.1711
2026-01-04 13:21:34,026: t15.2023.10.06 val PER: 0.0883
2026-01-04 13:21:34,026: t15.2023.10.08 val PER: 0.2476
2026-01-04 13:21:34,026: t15.2023.10.13 val PER: 0.2056
2026-01-04 13:21:34,026: t15.2023.10.15 val PER: 0.1575
2026-01-04 13:21:34,026: t15.2023.10.20 val PER: 0.1812
2026-01-04 13:21:34,026: t15.2023.10.22 val PER: 0.1058
2026-01-04 13:21:34,026: t15.2023.11.03 val PER: 0.1866
2026-01-04 13:21:34,026: t15.2023.11.04 val PER: 0.0341
2026-01-04 13:21:34,026: t15.2023.11.17 val PER: 0.0358
2026-01-04 13:21:34,026: t15.2023.11.19 val PER: 0.0399
2026-01-04 13:21:34,026: t15.2023.11.26 val PER: 0.1217
2026-01-04 13:21:34,026: t15.2023.12.03 val PER: 0.1250
2026-01-04 13:21:34,027: t15.2023.12.08 val PER: 0.1112
2026-01-04 13:21:34,027: t15.2023.12.10 val PER: 0.0986
2026-01-04 13:21:34,027: t15.2023.12.17 val PER: 0.1393
2026-01-04 13:21:34,027: t15.2023.12.29 val PER: 0.1373
2026-01-04 13:21:34,027: t15.2024.02.25 val PER: 0.1096
2026-01-04 13:21:34,027: t15.2024.03.08 val PER: 0.2290
2026-01-04 13:21:34,027: t15.2024.03.15 val PER: 0.2083
2026-01-04 13:21:34,027: t15.2024.03.17 val PER: 0.1485
2026-01-04 13:21:34,027: t15.2024.05.10 val PER: 0.1649
2026-01-04 13:21:34,027: t15.2024.06.14 val PER: 0.1703
2026-01-04 13:21:34,027: t15.2024.07.19 val PER: 0.2518
2026-01-04 13:21:34,027: t15.2024.07.21 val PER: 0.0952
2026-01-04 13:21:34,027: t15.2024.07.28 val PER: 0.1449
2026-01-04 13:21:34,027: t15.2025.01.10 val PER: 0.2879
2026-01-04 13:21:34,028: t15.2025.01.12 val PER: 0.1416
2026-01-04 13:21:34,028: t15.2025.03.14 val PER: 0.3432
2026-01-04 13:21:34,028: t15.2025.03.16 val PER: 0.1846
2026-01-04 13:21:34,028: t15.2025.03.30 val PER: 0.3103
2026-01-04 13:21:34,028: t15.2025.04.13 val PER: 0.2211
2026-01-04 13:21:34,030: New best val WER(1gram) 48.73% --> 47.46%
2026-01-04 13:21:34,030: Checkpointing model
2026-01-04 13:21:34,749: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 13:21:35,117: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_13000
2026-01-04 13:21:55,703: Train batch 13200: loss: 12.33 grad norm: 60.29 time: 0.054
2026-01-04 13:22:15,904: Train batch 13400: loss: 8.87 grad norm: 56.80 time: 0.061
2026-01-04 13:22:26,099: Running test after training batch: 13500
2026-01-04 13:22:26,352: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:22:31,341: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 13:22:31,376: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-04 13:22:33,370: Val batch 13500: PER (avg): 0.1525 CTC Loss (avg): 15.5493 WER(1gram): 47.72% (n=64) time: 7.269
2026-01-04 13:22:33,370: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 13:22:33,370: t15.2023.08.13 val PER: 0.1216
2026-01-04 13:22:33,370: t15.2023.08.18 val PER: 0.1123
2026-01-04 13:22:33,371: t15.2023.08.20 val PER: 0.1104
2026-01-04 13:22:33,371: t15.2023.08.25 val PER: 0.0949
2026-01-04 13:22:33,372: t15.2023.08.27 val PER: 0.1994
2026-01-04 13:22:33,372: t15.2023.09.01 val PER: 0.0860
2026-01-04 13:22:33,372: t15.2023.09.03 val PER: 0.1639
2026-01-04 13:22:33,372: t15.2023.09.24 val PER: 0.1274
2026-01-04 13:22:33,373: t15.2023.09.29 val PER: 0.1270
2026-01-04 13:22:33,373: t15.2023.10.01 val PER: 0.1731
2026-01-04 13:22:33,373: t15.2023.10.06 val PER: 0.0850
2026-01-04 13:22:33,373: t15.2023.10.08 val PER: 0.2598
2026-01-04 13:22:33,373: t15.2023.10.13 val PER: 0.2102
2026-01-04 13:22:33,373: t15.2023.10.15 val PER: 0.1569
2026-01-04 13:22:33,373: t15.2023.10.20 val PER: 0.1812
2026-01-04 13:22:33,374: t15.2023.10.22 val PER: 0.1047
2026-01-04 13:22:33,374: t15.2023.11.03 val PER: 0.1866
2026-01-04 13:22:33,374: t15.2023.11.04 val PER: 0.0341
2026-01-04 13:22:33,374: t15.2023.11.17 val PER: 0.0404
2026-01-04 13:22:33,374: t15.2023.11.19 val PER: 0.0319
2026-01-04 13:22:33,374: t15.2023.11.26 val PER: 0.1188
2026-01-04 13:22:33,374: t15.2023.12.03 val PER: 0.1103
2026-01-04 13:22:33,374: t15.2023.12.08 val PER: 0.1025
2026-01-04 13:22:33,375: t15.2023.12.10 val PER: 0.0894
2026-01-04 13:22:33,375: t15.2023.12.17 val PER: 0.1289
2026-01-04 13:22:33,375: t15.2023.12.29 val PER: 0.1277
2026-01-04 13:22:33,375: t15.2024.02.25 val PER: 0.1180
2026-01-04 13:22:33,375: t15.2024.03.08 val PER: 0.2248
2026-01-04 13:22:33,375: t15.2024.03.15 val PER: 0.2033
2026-01-04 13:22:33,375: t15.2024.03.17 val PER: 0.1450
2026-01-04 13:22:33,375: t15.2024.05.10 val PER: 0.1560
2026-01-04 13:22:33,375: t15.2024.06.14 val PER: 0.1672
2026-01-04 13:22:33,375: t15.2024.07.19 val PER: 0.2393
2026-01-04 13:22:33,376: t15.2024.07.21 val PER: 0.0890
2026-01-04 13:22:33,376: t15.2024.07.28 val PER: 0.1353
2026-01-04 13:22:33,376: t15.2025.01.10 val PER: 0.3003
2026-01-04 13:22:33,376: t15.2025.01.12 val PER: 0.1463
2026-01-04 13:22:33,376: t15.2025.03.14 val PER: 0.3639
2026-01-04 13:22:33,376: t15.2025.03.16 val PER: 0.1793
2026-01-04 13:22:33,376: t15.2025.03.30 val PER: 0.3092
2026-01-04 13:22:33,376: t15.2025.04.13 val PER: 0.2083
2026-01-04 13:22:33,687: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_13500
2026-01-04 13:22:43,945: Train batch 13600: loss: 12.91 grad norm: 65.06 time: 0.063
2026-01-04 13:23:04,737: Train batch 13800: loss: 8.96 grad norm: 58.41 time: 0.056
2026-01-04 13:23:25,814: Train batch 14000: loss: 11.50 grad norm: 55.50 time: 0.051
2026-01-04 13:23:25,814: Running test after training batch: 14000
2026-01-04 13:23:26,024: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:23:31,274: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 13:23:31,370: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-04 13:23:36,521: Val batch 14000: PER (avg): 0.1512 CTC Loss (avg): 15.5178 WER(1gram): 47.46% (n=64) time: 10.706
2026-01-04 13:23:36,521: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=11
2026-01-04 13:23:36,521: t15.2023.08.13 val PER: 0.1112
2026-01-04 13:23:36,521: t15.2023.08.18 val PER: 0.1006
2026-01-04 13:23:36,522: t15.2023.08.20 val PER: 0.1056
2026-01-04 13:23:36,522: t15.2023.08.25 val PER: 0.1114
2026-01-04 13:23:36,522: t15.2023.08.27 val PER: 0.1897
2026-01-04 13:23:36,522: t15.2023.09.01 val PER: 0.0804
2026-01-04 13:23:36,522: t15.2023.09.03 val PER: 0.1770
2026-01-04 13:23:36,522: t15.2023.09.24 val PER: 0.1250
2026-01-04 13:23:36,522: t15.2023.09.29 val PER: 0.1295
2026-01-04 13:23:36,522: t15.2023.10.01 val PER: 0.1750
2026-01-04 13:23:36,522: t15.2023.10.06 val PER: 0.0786
2026-01-04 13:23:36,522: t15.2023.10.08 val PER: 0.2571
2026-01-04 13:23:36,522: t15.2023.10.13 val PER: 0.2009
2026-01-04 13:23:36,522: t15.2023.10.15 val PER: 0.1516
2026-01-04 13:23:36,522: t15.2023.10.20 val PER: 0.1846
2026-01-04 13:23:36,522: t15.2023.10.22 val PER: 0.1091
2026-01-04 13:23:36,523: t15.2023.11.03 val PER: 0.1723
2026-01-04 13:23:36,523: t15.2023.11.04 val PER: 0.0273
2026-01-04 13:23:36,523: t15.2023.11.17 val PER: 0.0389
2026-01-04 13:23:36,523: t15.2023.11.19 val PER: 0.0240
2026-01-04 13:23:36,523: t15.2023.11.26 val PER: 0.1275
2026-01-04 13:23:36,524: t15.2023.12.03 val PER: 0.1229
2026-01-04 13:23:36,524: t15.2023.12.08 val PER: 0.1052
2026-01-04 13:23:36,524: t15.2023.12.10 val PER: 0.0920
2026-01-04 13:23:36,524: t15.2023.12.17 val PER: 0.1320
2026-01-04 13:23:36,524: t15.2023.12.29 val PER: 0.1297
2026-01-04 13:23:36,524: t15.2024.02.25 val PER: 0.1067
2026-01-04 13:23:36,524: t15.2024.03.08 val PER: 0.2191
2026-01-04 13:23:36,524: t15.2024.03.15 val PER: 0.2001
2026-01-04 13:23:36,524: t15.2024.03.17 val PER: 0.1444
2026-01-04 13:23:36,524: t15.2024.05.10 val PER: 0.1560
2026-01-04 13:23:36,524: t15.2024.06.14 val PER: 0.1688
2026-01-04 13:23:36,525: t15.2024.07.19 val PER: 0.2432
2026-01-04 13:23:36,525: t15.2024.07.21 val PER: 0.0910
2026-01-04 13:23:36,525: t15.2024.07.28 val PER: 0.1353
2026-01-04 13:23:36,525: t15.2025.01.10 val PER: 0.2948
2026-01-04 13:23:36,525: t15.2025.01.12 val PER: 0.1409
2026-01-04 13:23:36,526: t15.2025.03.14 val PER: 0.3491
2026-01-04 13:23:36,526: t15.2025.03.16 val PER: 0.1963
2026-01-04 13:23:36,526: t15.2025.03.30 val PER: 0.2966
2026-01-04 13:23:36,526: t15.2025.04.13 val PER: 0.2154
2026-01-04 13:23:36,847: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_14000
2026-01-04 13:23:57,044: Train batch 14200: loss: 8.24 grad norm: 52.34 time: 0.056
2026-01-04 13:24:18,070: Train batch 14400: loss: 6.13 grad norm: 41.52 time: 0.064
2026-01-04 13:24:28,463: Running test after training batch: 14500
2026-01-04 13:24:28,580: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:24:33,886: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 13:24:33,978: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-04 13:24:39,001: Val batch 14500: PER (avg): 0.1516 CTC Loss (avg): 15.5573 WER(1gram): 48.48% (n=64) time: 10.537
2026-01-04 13:24:39,001: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-04 13:24:39,002: t15.2023.08.13 val PER: 0.1154
2026-01-04 13:24:39,002: t15.2023.08.18 val PER: 0.1090
2026-01-04 13:24:39,002: t15.2023.08.20 val PER: 0.1056
2026-01-04 13:24:39,002: t15.2023.08.25 val PER: 0.0919
2026-01-04 13:24:39,002: t15.2023.08.27 val PER: 0.1833
2026-01-04 13:24:39,002: t15.2023.09.01 val PER: 0.0779
2026-01-04 13:24:39,002: t15.2023.09.03 val PER: 0.1675
2026-01-04 13:24:39,003: t15.2023.09.24 val PER: 0.1299
2026-01-04 13:24:39,003: t15.2023.09.29 val PER: 0.1295
2026-01-04 13:24:39,003: t15.2023.10.01 val PER: 0.1777
2026-01-04 13:24:39,003: t15.2023.10.06 val PER: 0.0872
2026-01-04 13:24:39,003: t15.2023.10.08 val PER: 0.2544
2026-01-04 13:24:39,003: t15.2023.10.13 val PER: 0.2110
2026-01-04 13:24:39,003: t15.2023.10.15 val PER: 0.1562
2026-01-04 13:24:39,003: t15.2023.10.20 val PER: 0.1644
2026-01-04 13:24:39,003: t15.2023.10.22 val PER: 0.1114
2026-01-04 13:24:39,004: t15.2023.11.03 val PER: 0.1805
2026-01-04 13:24:39,004: t15.2023.11.04 val PER: 0.0341
2026-01-04 13:24:39,004: t15.2023.11.17 val PER: 0.0420
2026-01-04 13:24:39,004: t15.2023.11.19 val PER: 0.0319
2026-01-04 13:24:39,004: t15.2023.11.26 val PER: 0.1239
2026-01-04 13:24:39,004: t15.2023.12.03 val PER: 0.1145
2026-01-04 13:24:39,004: t15.2023.12.08 val PER: 0.1005
2026-01-04 13:24:39,004: t15.2023.12.10 val PER: 0.0907
2026-01-04 13:24:39,005: t15.2023.12.17 val PER: 0.1331
2026-01-04 13:24:39,005: t15.2023.12.29 val PER: 0.1311
2026-01-04 13:24:39,005: t15.2024.02.25 val PER: 0.1053
2026-01-04 13:24:39,005: t15.2024.03.08 val PER: 0.2262
2026-01-04 13:24:39,005: t15.2024.03.15 val PER: 0.2076
2026-01-04 13:24:39,005: t15.2024.03.17 val PER: 0.1388
2026-01-04 13:24:39,005: t15.2024.05.10 val PER: 0.1486
2026-01-04 13:24:39,005: t15.2024.06.14 val PER: 0.1640
2026-01-04 13:24:39,005: t15.2024.07.19 val PER: 0.2426
2026-01-04 13:24:39,006: t15.2024.07.21 val PER: 0.0931
2026-01-04 13:24:39,006: t15.2024.07.28 val PER: 0.1338
2026-01-04 13:24:39,006: t15.2025.01.10 val PER: 0.2920
2026-01-04 13:24:39,006: t15.2025.01.12 val PER: 0.1393
2026-01-04 13:24:39,006: t15.2025.03.14 val PER: 0.3536
2026-01-04 13:24:39,006: t15.2025.03.16 val PER: 0.1937
2026-01-04 13:24:39,006: t15.2025.03.30 val PER: 0.2966
2026-01-04 13:24:39,006: t15.2025.04.13 val PER: 0.2168
2026-01-04 13:24:39,323: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_14500
2026-01-04 13:24:49,843: Train batch 14600: loss: 12.34 grad norm: 61.26 time: 0.060
2026-01-04 13:25:11,148: Train batch 14800: loss: 5.58 grad norm: 41.52 time: 0.053
2026-01-04 13:25:32,410: Train batch 15000: loss: 9.08 grad norm: 46.76 time: 0.053
2026-01-04 13:25:32,412: Running test after training batch: 15000
2026-01-04 13:25:32,592: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:25:38,157: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-04 13:25:38,250: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost it
2026-01-04 13:25:43,308: Val batch 15000: PER (avg): 0.1497 CTC Loss (avg): 15.2488 WER(1gram): 45.94% (n=64) time: 10.896
2026-01-04 13:25:43,309: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 13:25:43,309: t15.2023.08.13 val PER: 0.1060
2026-01-04 13:25:43,309: t15.2023.08.18 val PER: 0.1023
2026-01-04 13:25:43,309: t15.2023.08.20 val PER: 0.1072
2026-01-04 13:25:43,309: t15.2023.08.25 val PER: 0.0934
2026-01-04 13:25:43,309: t15.2023.08.27 val PER: 0.1817
2026-01-04 13:25:43,309: t15.2023.09.01 val PER: 0.0763
2026-01-04 13:25:43,309: t15.2023.09.03 val PER: 0.1508
2026-01-04 13:25:43,309: t15.2023.09.24 val PER: 0.1323
2026-01-04 13:25:43,310: t15.2023.09.29 val PER: 0.1251
2026-01-04 13:25:43,310: t15.2023.10.01 val PER: 0.1803
2026-01-04 13:25:43,310: t15.2023.10.06 val PER: 0.0829
2026-01-04 13:25:43,310: t15.2023.10.08 val PER: 0.2530
2026-01-04 13:25:43,310: t15.2023.10.13 val PER: 0.2025
2026-01-04 13:25:43,310: t15.2023.10.15 val PER: 0.1463
2026-01-04 13:25:43,310: t15.2023.10.20 val PER: 0.1946
2026-01-04 13:25:43,310: t15.2023.10.22 val PER: 0.1091
2026-01-04 13:25:43,310: t15.2023.11.03 val PER: 0.1811
2026-01-04 13:25:43,310: t15.2023.11.04 val PER: 0.0341
2026-01-04 13:25:43,310: t15.2023.11.17 val PER: 0.0404
2026-01-04 13:25:43,311: t15.2023.11.19 val PER: 0.0359
2026-01-04 13:25:43,311: t15.2023.11.26 val PER: 0.1225
2026-01-04 13:25:43,311: t15.2023.12.03 val PER: 0.1124
2026-01-04 13:25:43,311: t15.2023.12.08 val PER: 0.0985
2026-01-04 13:25:43,311: t15.2023.12.10 val PER: 0.0920
2026-01-04 13:25:43,311: t15.2023.12.17 val PER: 0.1435
2026-01-04 13:25:43,311: t15.2023.12.29 val PER: 0.1304
2026-01-04 13:25:43,311: t15.2024.02.25 val PER: 0.1053
2026-01-04 13:25:43,311: t15.2024.03.08 val PER: 0.2262
2026-01-04 13:25:43,311: t15.2024.03.15 val PER: 0.2051
2026-01-04 13:25:43,311: t15.2024.03.17 val PER: 0.1311
2026-01-04 13:25:43,311: t15.2024.05.10 val PER: 0.1724
2026-01-04 13:25:43,311: t15.2024.06.14 val PER: 0.1656
2026-01-04 13:25:43,311: t15.2024.07.19 val PER: 0.2380
2026-01-04 13:25:43,312: t15.2024.07.21 val PER: 0.0917
2026-01-04 13:25:43,312: t15.2024.07.28 val PER: 0.1309
2026-01-04 13:25:43,312: t15.2025.01.10 val PER: 0.3030
2026-01-04 13:25:43,312: t15.2025.01.12 val PER: 0.1409
2026-01-04 13:25:43,313: t15.2025.03.14 val PER: 0.3432
2026-01-04 13:25:43,313: t15.2025.03.16 val PER: 0.1780
2026-01-04 13:25:43,313: t15.2025.03.30 val PER: 0.2851
2026-01-04 13:25:43,313: t15.2025.04.13 val PER: 0.2197
2026-01-04 13:25:43,314: New best val WER(1gram) 47.46% --> 45.94%
2026-01-04 13:25:43,314: Checkpointing model
2026-01-04 13:25:43,972: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 13:25:44,295: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_15000
2026-01-04 13:26:05,610: Train batch 15200: loss: 4.60 grad norm: 38.34 time: 0.058
2026-01-04 13:26:26,510: Train batch 15400: loss: 11.10 grad norm: 54.29 time: 0.050
2026-01-04 13:26:37,079: Running test after training batch: 15500
2026-01-04 13:26:37,193: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:26:42,496: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-04 13:26:42,590: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost get
2026-01-04 13:26:47,858: Val batch 15500: PER (avg): 0.1485 CTC Loss (avg): 15.2369 WER(1gram): 43.91% (n=64) time: 10.779
2026-01-04 13:26:47,859: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 13:26:47,859: t15.2023.08.13 val PER: 0.1143
2026-01-04 13:26:47,859: t15.2023.08.18 val PER: 0.1048
2026-01-04 13:26:47,860: t15.2023.08.20 val PER: 0.1088
2026-01-04 13:26:47,860: t15.2023.08.25 val PER: 0.0934
2026-01-04 13:26:47,860: t15.2023.08.27 val PER: 0.1817
2026-01-04 13:26:47,860: t15.2023.09.01 val PER: 0.0731
2026-01-04 13:26:47,861: t15.2023.09.03 val PER: 0.1580
2026-01-04 13:26:47,861: t15.2023.09.24 val PER: 0.1274
2026-01-04 13:26:47,861: t15.2023.09.29 val PER: 0.1283
2026-01-04 13:26:47,861: t15.2023.10.01 val PER: 0.1724
2026-01-04 13:26:47,861: t15.2023.10.06 val PER: 0.0807
2026-01-04 13:26:47,861: t15.2023.10.08 val PER: 0.2476
2026-01-04 13:26:47,861: t15.2023.10.13 val PER: 0.1971
2026-01-04 13:26:47,862: t15.2023.10.15 val PER: 0.1496
2026-01-04 13:26:47,862: t15.2023.10.20 val PER: 0.1846
2026-01-04 13:26:47,862: t15.2023.10.22 val PER: 0.1080
2026-01-04 13:26:47,862: t15.2023.11.03 val PER: 0.1784
2026-01-04 13:26:47,862: t15.2023.11.04 val PER: 0.0341
2026-01-04 13:26:47,862: t15.2023.11.17 val PER: 0.0373
2026-01-04 13:26:47,862: t15.2023.11.19 val PER: 0.0419
2026-01-04 13:26:47,862: t15.2023.11.26 val PER: 0.1159
2026-01-04 13:26:47,863: t15.2023.12.03 val PER: 0.1166
2026-01-04 13:26:47,863: t15.2023.12.08 val PER: 0.1019
2026-01-04 13:26:47,863: t15.2023.12.10 val PER: 0.0815
2026-01-04 13:26:47,863: t15.2023.12.17 val PER: 0.1424
2026-01-04 13:26:47,863: t15.2023.12.29 val PER: 0.1229
2026-01-04 13:26:47,863: t15.2024.02.25 val PER: 0.1025
2026-01-04 13:26:47,863: t15.2024.03.08 val PER: 0.2262
2026-01-04 13:26:47,863: t15.2024.03.15 val PER: 0.1945
2026-01-04 13:26:47,864: t15.2024.03.17 val PER: 0.1325
2026-01-04 13:26:47,864: t15.2024.05.10 val PER: 0.1545
2026-01-04 13:26:47,864: t15.2024.06.14 val PER: 0.1703
2026-01-04 13:26:47,864: t15.2024.07.19 val PER: 0.2459
2026-01-04 13:26:47,864: t15.2024.07.21 val PER: 0.0903
2026-01-04 13:26:47,864: t15.2024.07.28 val PER: 0.1353
2026-01-04 13:26:47,864: t15.2025.01.10 val PER: 0.2879
2026-01-04 13:26:47,864: t15.2025.01.12 val PER: 0.1463
2026-01-04 13:26:47,865: t15.2025.03.14 val PER: 0.3402
2026-01-04 13:26:47,865: t15.2025.03.16 val PER: 0.1767
2026-01-04 13:26:47,865: t15.2025.03.30 val PER: 0.2862
2026-01-04 13:26:47,865: t15.2025.04.13 val PER: 0.2154
2026-01-04 13:26:47,866: New best val WER(1gram) 45.94% --> 43.91%
2026-01-04 13:26:47,866: Checkpointing model
2026-01-04 13:26:48,501: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/best_checkpoint
2026-01-04 13:26:48,829: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_15500
2026-01-04 13:26:59,236: Train batch 15600: loss: 11.83 grad norm: 58.76 time: 0.062
2026-01-04 13:27:19,200: Train batch 15800: loss: 13.30 grad norm: 62.34 time: 0.066
2026-01-04 13:27:39,957: Train batch 16000: loss: 8.36 grad norm: 44.94 time: 0.056
2026-01-04 13:27:39,957: Running test after training batch: 16000
2026-01-04 13:27:40,127: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:27:45,435: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-04 13:27:45,540: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-04 13:27:50,932: Val batch 16000: PER (avg): 0.1486 CTC Loss (avg): 15.2810 WER(1gram): 45.94% (n=64) time: 10.975
2026-01-04 13:27:50,932: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 13:27:50,933: t15.2023.08.13 val PER: 0.1133
2026-01-04 13:27:50,933: t15.2023.08.18 val PER: 0.1081
2026-01-04 13:27:50,933: t15.2023.08.20 val PER: 0.1048
2026-01-04 13:27:50,933: t15.2023.08.25 val PER: 0.0934
2026-01-04 13:27:50,933: t15.2023.08.27 val PER: 0.1913
2026-01-04 13:27:50,933: t15.2023.09.01 val PER: 0.0787
2026-01-04 13:27:50,933: t15.2023.09.03 val PER: 0.1556
2026-01-04 13:27:50,933: t15.2023.09.24 val PER: 0.1274
2026-01-04 13:27:50,933: t15.2023.09.29 val PER: 0.1295
2026-01-04 13:27:50,933: t15.2023.10.01 val PER: 0.1704
2026-01-04 13:27:50,934: t15.2023.10.06 val PER: 0.0872
2026-01-04 13:27:50,934: t15.2023.10.08 val PER: 0.2530
2026-01-04 13:27:50,934: t15.2023.10.13 val PER: 0.2002
2026-01-04 13:27:50,934: t15.2023.10.15 val PER: 0.1477
2026-01-04 13:27:50,934: t15.2023.10.20 val PER: 0.1846
2026-01-04 13:27:50,934: t15.2023.10.22 val PER: 0.1069
2026-01-04 13:27:50,934: t15.2023.11.03 val PER: 0.1777
2026-01-04 13:27:50,934: t15.2023.11.04 val PER: 0.0341
2026-01-04 13:27:50,934: t15.2023.11.17 val PER: 0.0295
2026-01-04 13:27:50,935: t15.2023.11.19 val PER: 0.0359
2026-01-04 13:27:50,935: t15.2023.11.26 val PER: 0.1181
2026-01-04 13:27:50,935: t15.2023.12.03 val PER: 0.1103
2026-01-04 13:27:50,935: t15.2023.12.08 val PER: 0.0925
2026-01-04 13:27:50,935: t15.2023.12.10 val PER: 0.0894
2026-01-04 13:27:50,935: t15.2023.12.17 val PER: 0.1289
2026-01-04 13:27:50,935: t15.2023.12.29 val PER: 0.1249
2026-01-04 13:27:50,936: t15.2024.02.25 val PER: 0.1039
2026-01-04 13:27:50,936: t15.2024.03.08 val PER: 0.2290
2026-01-04 13:27:50,936: t15.2024.03.15 val PER: 0.1970
2026-01-04 13:27:50,936: t15.2024.03.17 val PER: 0.1332
2026-01-04 13:27:50,936: t15.2024.05.10 val PER: 0.1679
2026-01-04 13:27:50,936: t15.2024.06.14 val PER: 0.1640
2026-01-04 13:27:50,937: t15.2024.07.19 val PER: 0.2367
2026-01-04 13:27:50,937: t15.2024.07.21 val PER: 0.0890
2026-01-04 13:27:50,937: t15.2024.07.28 val PER: 0.1419
2026-01-04 13:27:50,937: t15.2025.01.10 val PER: 0.2851
2026-01-04 13:27:50,937: t15.2025.01.12 val PER: 0.1432
2026-01-04 13:27:50,937: t15.2025.03.14 val PER: 0.3506
2026-01-04 13:27:50,937: t15.2025.03.16 val PER: 0.1924
2026-01-04 13:27:50,938: t15.2025.03.30 val PER: 0.2885
2026-01-04 13:27:50,938: t15.2025.04.13 val PER: 0.2097
2026-01-04 13:27:51,255: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_16000
2026-01-04 13:28:11,437: Train batch 16200: loss: 6.11 grad norm: 42.79 time: 0.055
2026-01-04 13:28:31,919: Train batch 16400: loss: 10.41 grad norm: 62.11 time: 0.059
2026-01-04 13:28:42,607: Running test after training batch: 16500
2026-01-04 13:28:42,715: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:28:47,995: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-04 13:28:48,094: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 13:28:53,419: Val batch 16500: PER (avg): 0.1478 CTC Loss (avg): 15.1341 WER(1gram): 45.43% (n=64) time: 10.811
2026-01-04 13:28:53,419: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 13:28:53,419: t15.2023.08.13 val PER: 0.1091
2026-01-04 13:28:53,419: t15.2023.08.18 val PER: 0.1031
2026-01-04 13:28:53,420: t15.2023.08.20 val PER: 0.1064
2026-01-04 13:28:53,420: t15.2023.08.25 val PER: 0.0904
2026-01-04 13:28:53,420: t15.2023.08.27 val PER: 0.1801
2026-01-04 13:28:53,420: t15.2023.09.01 val PER: 0.0779
2026-01-04 13:28:53,420: t15.2023.09.03 val PER: 0.1710
2026-01-04 13:28:53,420: t15.2023.09.24 val PER: 0.1262
2026-01-04 13:28:53,420: t15.2023.09.29 val PER: 0.1270
2026-01-04 13:28:53,420: t15.2023.10.01 val PER: 0.1731
2026-01-04 13:28:53,420: t15.2023.10.06 val PER: 0.0850
2026-01-04 13:28:53,420: t15.2023.10.08 val PER: 0.2503
2026-01-04 13:28:53,420: t15.2023.10.13 val PER: 0.2025
2026-01-04 13:28:53,420: t15.2023.10.15 val PER: 0.1503
2026-01-04 13:28:53,421: t15.2023.10.20 val PER: 0.1812
2026-01-04 13:28:53,421: t15.2023.10.22 val PER: 0.1069
2026-01-04 13:28:53,421: t15.2023.11.03 val PER: 0.1825
2026-01-04 13:28:53,421: t15.2023.11.04 val PER: 0.0341
2026-01-04 13:28:53,421: t15.2023.11.17 val PER: 0.0342
2026-01-04 13:28:53,421: t15.2023.11.19 val PER: 0.0379
2026-01-04 13:28:53,421: t15.2023.11.26 val PER: 0.1072
2026-01-04 13:28:53,421: t15.2023.12.03 val PER: 0.1103
2026-01-04 13:28:53,421: t15.2023.12.08 val PER: 0.0885
2026-01-04 13:28:53,421: t15.2023.12.10 val PER: 0.0841
2026-01-04 13:28:53,421: t15.2023.12.17 val PER: 0.1331
2026-01-04 13:28:53,421: t15.2023.12.29 val PER: 0.1174
2026-01-04 13:28:53,422: t15.2024.02.25 val PER: 0.1067
2026-01-04 13:28:53,422: t15.2024.03.08 val PER: 0.2276
2026-01-04 13:28:53,422: t15.2024.03.15 val PER: 0.1982
2026-01-04 13:28:53,422: t15.2024.03.17 val PER: 0.1374
2026-01-04 13:28:53,422: t15.2024.05.10 val PER: 0.1649
2026-01-04 13:28:53,422: t15.2024.06.14 val PER: 0.1656
2026-01-04 13:28:53,422: t15.2024.07.19 val PER: 0.2373
2026-01-04 13:28:53,422: t15.2024.07.21 val PER: 0.0848
2026-01-04 13:28:53,422: t15.2024.07.28 val PER: 0.1324
2026-01-04 13:28:53,423: t15.2025.01.10 val PER: 0.2906
2026-01-04 13:28:53,423: t15.2025.01.12 val PER: 0.1386
2026-01-04 13:28:53,423: t15.2025.03.14 val PER: 0.3536
2026-01-04 13:28:53,423: t15.2025.03.16 val PER: 0.1911
2026-01-04 13:28:53,423: t15.2025.03.30 val PER: 0.2897
2026-01-04 13:28:53,423: t15.2025.04.13 val PER: 0.2126
2026-01-04 13:28:53,739: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_16500
2026-01-04 13:29:03,513: Train batch 16600: loss: 8.44 grad norm: 56.06 time: 0.052
2026-01-04 13:29:23,841: Train batch 16800: loss: 16.22 grad norm: 72.77 time: 0.064
2026-01-04 13:29:44,478: Train batch 17000: loss: 8.22 grad norm: 51.23 time: 0.083
2026-01-04 13:29:44,479: Running test after training batch: 17000
2026-01-04 13:29:44,584: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:29:49,919: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-04 13:29:50,025: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost et
2026-01-04 13:29:55,524: Val batch 17000: PER (avg): 0.1472 CTC Loss (avg): 14.9996 WER(1gram): 45.94% (n=64) time: 11.045
2026-01-04 13:29:55,525: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-04 13:29:55,525: t15.2023.08.13 val PER: 0.1102
2026-01-04 13:29:55,525: t15.2023.08.18 val PER: 0.1056
2026-01-04 13:29:55,526: t15.2023.08.20 val PER: 0.1041
2026-01-04 13:29:55,526: t15.2023.08.25 val PER: 0.0919
2026-01-04 13:29:55,526: t15.2023.08.27 val PER: 0.1865
2026-01-04 13:29:55,526: t15.2023.09.01 val PER: 0.0779
2026-01-04 13:29:55,526: t15.2023.09.03 val PER: 0.1556
2026-01-04 13:29:55,526: t15.2023.09.24 val PER: 0.1238
2026-01-04 13:29:55,526: t15.2023.09.29 val PER: 0.1308
2026-01-04 13:29:55,526: t15.2023.10.01 val PER: 0.1711
2026-01-04 13:29:55,526: t15.2023.10.06 val PER: 0.0829
2026-01-04 13:29:55,526: t15.2023.10.08 val PER: 0.2382
2026-01-04 13:29:55,526: t15.2023.10.13 val PER: 0.1986
2026-01-04 13:29:55,527: t15.2023.10.15 val PER: 0.1496
2026-01-04 13:29:55,527: t15.2023.10.20 val PER: 0.1678
2026-01-04 13:29:55,527: t15.2023.10.22 val PER: 0.1069
2026-01-04 13:29:55,527: t15.2023.11.03 val PER: 0.1839
2026-01-04 13:29:55,527: t15.2023.11.04 val PER: 0.0341
2026-01-04 13:29:55,527: t15.2023.11.17 val PER: 0.0373
2026-01-04 13:29:55,527: t15.2023.11.19 val PER: 0.0359
2026-01-04 13:29:55,527: t15.2023.11.26 val PER: 0.1072
2026-01-04 13:29:55,527: t15.2023.12.03 val PER: 0.1166
2026-01-04 13:29:55,527: t15.2023.12.08 val PER: 0.0919
2026-01-04 13:29:55,527: t15.2023.12.10 val PER: 0.0894
2026-01-04 13:29:55,527: t15.2023.12.17 val PER: 0.1299
2026-01-04 13:29:55,527: t15.2023.12.29 val PER: 0.1222
2026-01-04 13:29:55,527: t15.2024.02.25 val PER: 0.1053
2026-01-04 13:29:55,527: t15.2024.03.08 val PER: 0.2304
2026-01-04 13:29:55,527: t15.2024.03.15 val PER: 0.1989
2026-01-04 13:29:55,528: t15.2024.03.17 val PER: 0.1395
2026-01-04 13:29:55,528: t15.2024.05.10 val PER: 0.1575
2026-01-04 13:29:55,528: t15.2024.06.14 val PER: 0.1609
2026-01-04 13:29:55,528: t15.2024.07.19 val PER: 0.2281
2026-01-04 13:29:55,528: t15.2024.07.21 val PER: 0.0883
2026-01-04 13:29:55,528: t15.2024.07.28 val PER: 0.1331
2026-01-04 13:29:55,528: t15.2025.01.10 val PER: 0.2879
2026-01-04 13:29:55,528: t15.2025.01.12 val PER: 0.1370
2026-01-04 13:29:55,528: t15.2025.03.14 val PER: 0.3432
2026-01-04 13:29:55,529: t15.2025.03.16 val PER: 0.1859
2026-01-04 13:29:55,529: t15.2025.03.30 val PER: 0.2885
2026-01-04 13:29:55,529: t15.2025.04.13 val PER: 0.2211
2026-01-04 13:29:55,846: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_17000
2026-01-04 13:30:16,920: Train batch 17200: loss: 9.45 grad norm: 48.28 time: 0.085
2026-01-04 13:30:38,487: Train batch 17400: loss: 11.65 grad norm: 56.72 time: 0.073
2026-01-04 13:30:49,088: Running test after training batch: 17500
2026-01-04 13:30:49,218: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:30:54,534: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 13:30:54,630: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-04 13:30:59,885: Val batch 17500: PER (avg): 0.1456 CTC Loss (avg): 14.9998 WER(1gram): 45.18% (n=64) time: 10.797
2026-01-04 13:30:59,885: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 13:30:59,886: t15.2023.08.13 val PER: 0.1081
2026-01-04 13:30:59,886: t15.2023.08.18 val PER: 0.0997
2026-01-04 13:30:59,886: t15.2023.08.20 val PER: 0.1128
2026-01-04 13:30:59,886: t15.2023.08.25 val PER: 0.0904
2026-01-04 13:30:59,886: t15.2023.08.27 val PER: 0.1849
2026-01-04 13:30:59,886: t15.2023.09.01 val PER: 0.0795
2026-01-04 13:30:59,887: t15.2023.09.03 val PER: 0.1532
2026-01-04 13:30:59,887: t15.2023.09.24 val PER: 0.1262
2026-01-04 13:30:59,887: t15.2023.09.29 val PER: 0.1283
2026-01-04 13:30:59,887: t15.2023.10.01 val PER: 0.1664
2026-01-04 13:30:59,887: t15.2023.10.06 val PER: 0.0764
2026-01-04 13:30:59,887: t15.2023.10.08 val PER: 0.2422
2026-01-04 13:30:59,887: t15.2023.10.13 val PER: 0.1939
2026-01-04 13:30:59,887: t15.2023.10.15 val PER: 0.1463
2026-01-04 13:30:59,887: t15.2023.10.20 val PER: 0.1846
2026-01-04 13:30:59,888: t15.2023.10.22 val PER: 0.1047
2026-01-04 13:30:59,888: t15.2023.11.03 val PER: 0.1784
2026-01-04 13:30:59,888: t15.2023.11.04 val PER: 0.0307
2026-01-04 13:30:59,888: t15.2023.11.17 val PER: 0.0373
2026-01-04 13:30:59,888: t15.2023.11.19 val PER: 0.0319
2026-01-04 13:30:59,888: t15.2023.11.26 val PER: 0.1094
2026-01-04 13:30:59,888: t15.2023.12.03 val PER: 0.1145
2026-01-04 13:30:59,888: t15.2023.12.08 val PER: 0.0839
2026-01-04 13:30:59,888: t15.2023.12.10 val PER: 0.0841
2026-01-04 13:30:59,888: t15.2023.12.17 val PER: 0.1268
2026-01-04 13:30:59,888: t15.2023.12.29 val PER: 0.1222
2026-01-04 13:30:59,888: t15.2024.02.25 val PER: 0.1039
2026-01-04 13:30:59,889: t15.2024.03.08 val PER: 0.2262
2026-01-04 13:30:59,889: t15.2024.03.15 val PER: 0.1951
2026-01-04 13:30:59,889: t15.2024.03.17 val PER: 0.1311
2026-01-04 13:30:59,889: t15.2024.05.10 val PER: 0.1620
2026-01-04 13:30:59,889: t15.2024.06.14 val PER: 0.1688
2026-01-04 13:30:59,889: t15.2024.07.19 val PER: 0.2254
2026-01-04 13:30:59,889: t15.2024.07.21 val PER: 0.0848
2026-01-04 13:30:59,889: t15.2024.07.28 val PER: 0.1331
2026-01-04 13:30:59,889: t15.2025.01.10 val PER: 0.2865
2026-01-04 13:30:59,889: t15.2025.01.12 val PER: 0.1378
2026-01-04 13:30:59,889: t15.2025.03.14 val PER: 0.3506
2026-01-04 13:30:59,889: t15.2025.03.16 val PER: 0.1885
2026-01-04 13:30:59,889: t15.2025.03.30 val PER: 0.2977
2026-01-04 13:30:59,889: t15.2025.04.13 val PER: 0.2126
2026-01-04 13:31:00,288: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_17500
2026-01-04 13:31:10,713: Train batch 17600: loss: 9.53 grad norm: 54.43 time: 0.051
2026-01-04 13:31:29,456: Train batch 17800: loss: 6.28 grad norm: 52.45 time: 0.042
2026-01-04 13:31:47,966: Train batch 18000: loss: 11.32 grad norm: 69.07 time: 0.061
2026-01-04 13:31:47,967: Running test after training batch: 18000
2026-01-04 13:31:48,151: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:31:53,651: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 13:31:53,740: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-04 13:31:58,836: Val batch 18000: PER (avg): 0.1453 CTC Loss (avg): 14.9752 WER(1gram): 45.18% (n=64) time: 10.869
2026-01-04 13:31:58,837: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 13:31:58,841: t15.2023.08.13 val PER: 0.1050
2026-01-04 13:31:58,841: t15.2023.08.18 val PER: 0.1031
2026-01-04 13:31:58,841: t15.2023.08.20 val PER: 0.1056
2026-01-04 13:31:58,841: t15.2023.08.25 val PER: 0.0934
2026-01-04 13:31:58,842: t15.2023.08.27 val PER: 0.1865
2026-01-04 13:31:58,842: t15.2023.09.01 val PER: 0.0779
2026-01-04 13:31:58,842: t15.2023.09.03 val PER: 0.1485
2026-01-04 13:31:58,842: t15.2023.09.24 val PER: 0.1286
2026-01-04 13:31:58,842: t15.2023.09.29 val PER: 0.1264
2026-01-04 13:31:58,842: t15.2023.10.01 val PER: 0.1678
2026-01-04 13:31:58,842: t15.2023.10.06 val PER: 0.0775
2026-01-04 13:31:58,842: t15.2023.10.08 val PER: 0.2463
2026-01-04 13:31:58,842: t15.2023.10.13 val PER: 0.1994
2026-01-04 13:31:58,843: t15.2023.10.15 val PER: 0.1490
2026-01-04 13:31:58,843: t15.2023.10.20 val PER: 0.1846
2026-01-04 13:31:58,843: t15.2023.10.22 val PER: 0.1024
2026-01-04 13:31:58,843: t15.2023.11.03 val PER: 0.1777
2026-01-04 13:31:58,843: t15.2023.11.04 val PER: 0.0307
2026-01-04 13:31:58,844: t15.2023.11.17 val PER: 0.0358
2026-01-04 13:31:58,844: t15.2023.11.19 val PER: 0.0339
2026-01-04 13:31:58,844: t15.2023.11.26 val PER: 0.1051
2026-01-04 13:31:58,844: t15.2023.12.03 val PER: 0.1103
2026-01-04 13:31:58,844: t15.2023.12.08 val PER: 0.0892
2026-01-04 13:31:58,844: t15.2023.12.10 val PER: 0.0828
2026-01-04 13:31:58,844: t15.2023.12.17 val PER: 0.1310
2026-01-04 13:31:58,845: t15.2023.12.29 val PER: 0.1222
2026-01-04 13:31:58,845: t15.2024.02.25 val PER: 0.1025
2026-01-04 13:31:58,845: t15.2024.03.08 val PER: 0.2191
2026-01-04 13:31:58,845: t15.2024.03.15 val PER: 0.1951
2026-01-04 13:31:58,845: t15.2024.03.17 val PER: 0.1374
2026-01-04 13:31:58,845: t15.2024.05.10 val PER: 0.1560
2026-01-04 13:31:58,845: t15.2024.06.14 val PER: 0.1609
2026-01-04 13:31:58,845: t15.2024.07.19 val PER: 0.2281
2026-01-04 13:31:58,845: t15.2024.07.21 val PER: 0.0848
2026-01-04 13:31:58,845: t15.2024.07.28 val PER: 0.1346
2026-01-04 13:31:58,846: t15.2025.01.10 val PER: 0.2948
2026-01-04 13:31:58,846: t15.2025.01.12 val PER: 0.1363
2026-01-04 13:31:58,846: t15.2025.03.14 val PER: 0.3432
2026-01-04 13:31:58,846: t15.2025.03.16 val PER: 0.1793
2026-01-04 13:31:58,846: t15.2025.03.30 val PER: 0.2874
2026-01-04 13:31:58,846: t15.2025.04.13 val PER: 0.2140
2026-01-04 13:31:59,144: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_18000
2026-01-04 13:32:20,765: Train batch 18200: loss: 7.70 grad norm: 48.77 time: 0.075
2026-01-04 13:32:41,603: Train batch 18400: loss: 4.69 grad norm: 41.37 time: 0.058
2026-01-04 13:32:50,935: Running test after training batch: 18500
2026-01-04 13:32:51,085: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:32:56,506: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 13:32:56,604: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 13:33:01,784: Val batch 18500: PER (avg): 0.1458 CTC Loss (avg): 14.9744 WER(1gram): 45.43% (n=64) time: 10.848
2026-01-04 13:33:01,785: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-04 13:33:01,785: t15.2023.08.13 val PER: 0.1050
2026-01-04 13:33:01,785: t15.2023.08.18 val PER: 0.1014
2026-01-04 13:33:01,785: t15.2023.08.20 val PER: 0.1088
2026-01-04 13:33:01,785: t15.2023.08.25 val PER: 0.0904
2026-01-04 13:33:01,785: t15.2023.08.27 val PER: 0.1833
2026-01-04 13:33:01,786: t15.2023.09.01 val PER: 0.0779
2026-01-04 13:33:01,786: t15.2023.09.03 val PER: 0.1568
2026-01-04 13:33:01,786: t15.2023.09.24 val PER: 0.1262
2026-01-04 13:33:01,786: t15.2023.09.29 val PER: 0.1283
2026-01-04 13:33:01,786: t15.2023.10.01 val PER: 0.1664
2026-01-04 13:33:01,786: t15.2023.10.06 val PER: 0.0786
2026-01-04 13:33:01,786: t15.2023.10.08 val PER: 0.2503
2026-01-04 13:33:01,786: t15.2023.10.13 val PER: 0.1994
2026-01-04 13:33:01,786: t15.2023.10.15 val PER: 0.1503
2026-01-04 13:33:01,786: t15.2023.10.20 val PER: 0.1745
2026-01-04 13:33:01,786: t15.2023.10.22 val PER: 0.1002
2026-01-04 13:33:01,786: t15.2023.11.03 val PER: 0.1825
2026-01-04 13:33:01,786: t15.2023.11.04 val PER: 0.0307
2026-01-04 13:33:01,787: t15.2023.11.17 val PER: 0.0358
2026-01-04 13:33:01,787: t15.2023.11.19 val PER: 0.0339
2026-01-04 13:33:01,787: t15.2023.11.26 val PER: 0.1116
2026-01-04 13:33:01,787: t15.2023.12.03 val PER: 0.1082
2026-01-04 13:33:01,787: t15.2023.12.08 val PER: 0.0912
2026-01-04 13:33:01,787: t15.2023.12.10 val PER: 0.0815
2026-01-04 13:33:01,787: t15.2023.12.17 val PER: 0.1362
2026-01-04 13:33:01,787: t15.2023.12.29 val PER: 0.1201
2026-01-04 13:33:01,787: t15.2024.02.25 val PER: 0.1067
2026-01-04 13:33:01,787: t15.2024.03.08 val PER: 0.2191
2026-01-04 13:33:01,787: t15.2024.03.15 val PER: 0.1932
2026-01-04 13:33:01,788: t15.2024.03.17 val PER: 0.1367
2026-01-04 13:33:01,788: t15.2024.05.10 val PER: 0.1560
2026-01-04 13:33:01,788: t15.2024.06.14 val PER: 0.1625
2026-01-04 13:33:01,788: t15.2024.07.19 val PER: 0.2334
2026-01-04 13:33:01,788: t15.2024.07.21 val PER: 0.0828
2026-01-04 13:33:01,788: t15.2024.07.28 val PER: 0.1301
2026-01-04 13:33:01,788: t15.2025.01.10 val PER: 0.2824
2026-01-04 13:33:01,788: t15.2025.01.12 val PER: 0.1370
2026-01-04 13:33:01,789: t15.2025.03.14 val PER: 0.3432
2026-01-04 13:33:01,789: t15.2025.03.16 val PER: 0.1832
2026-01-04 13:33:01,789: t15.2025.03.30 val PER: 0.2897
2026-01-04 13:33:01,789: t15.2025.04.13 val PER: 0.2126
2026-01-04 13:33:02,104: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_18500
2026-01-04 13:33:12,519: Train batch 18600: loss: 12.53 grad norm: 62.94 time: 0.069
2026-01-04 13:33:33,946: Train batch 18800: loss: 7.99 grad norm: 48.25 time: 0.066
2026-01-04 13:33:54,817: Train batch 19000: loss: 8.28 grad norm: 46.58 time: 0.064
2026-01-04 13:33:54,817: Running test after training batch: 19000
2026-01-04 13:33:54,925: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:34:00,171: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-04 13:34:00,267: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 13:34:05,529: Val batch 19000: PER (avg): 0.1453 CTC Loss (avg): 14.9700 WER(1gram): 45.69% (n=64) time: 10.711
2026-01-04 13:34:05,529: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 13:34:05,530: t15.2023.08.13 val PER: 0.1060
2026-01-04 13:34:05,530: t15.2023.08.18 val PER: 0.1014
2026-01-04 13:34:05,530: t15.2023.08.20 val PER: 0.1096
2026-01-04 13:34:05,530: t15.2023.08.25 val PER: 0.0889
2026-01-04 13:34:05,530: t15.2023.08.27 val PER: 0.1833
2026-01-04 13:34:05,531: t15.2023.09.01 val PER: 0.0779
2026-01-04 13:34:05,531: t15.2023.09.03 val PER: 0.1496
2026-01-04 13:34:05,531: t15.2023.09.24 val PER: 0.1323
2026-01-04 13:34:05,531: t15.2023.09.29 val PER: 0.1244
2026-01-04 13:34:05,531: t15.2023.10.01 val PER: 0.1658
2026-01-04 13:34:05,531: t15.2023.10.06 val PER: 0.0743
2026-01-04 13:34:05,531: t15.2023.10.08 val PER: 0.2436
2026-01-04 13:34:05,531: t15.2023.10.13 val PER: 0.1916
2026-01-04 13:34:05,532: t15.2023.10.15 val PER: 0.1510
2026-01-04 13:34:05,532: t15.2023.10.20 val PER: 0.1812
2026-01-04 13:34:05,532: t15.2023.10.22 val PER: 0.1036
2026-01-04 13:34:05,532: t15.2023.11.03 val PER: 0.1825
2026-01-04 13:34:05,532: t15.2023.11.04 val PER: 0.0273
2026-01-04 13:34:05,532: t15.2023.11.17 val PER: 0.0327
2026-01-04 13:34:05,532: t15.2023.11.19 val PER: 0.0279
2026-01-04 13:34:05,532: t15.2023.11.26 val PER: 0.1087
2026-01-04 13:34:05,532: t15.2023.12.03 val PER: 0.1082
2026-01-04 13:34:05,533: t15.2023.12.08 val PER: 0.0905
2026-01-04 13:34:05,533: t15.2023.12.10 val PER: 0.0841
2026-01-04 13:34:05,533: t15.2023.12.17 val PER: 0.1279
2026-01-04 13:34:05,533: t15.2023.12.29 val PER: 0.1235
2026-01-04 13:34:05,533: t15.2024.02.25 val PER: 0.1039
2026-01-04 13:34:05,533: t15.2024.03.08 val PER: 0.2162
2026-01-04 13:34:05,533: t15.2024.03.15 val PER: 0.1901
2026-01-04 13:34:05,533: t15.2024.03.17 val PER: 0.1332
2026-01-04 13:34:05,533: t15.2024.05.10 val PER: 0.1486
2026-01-04 13:34:05,534: t15.2024.06.14 val PER: 0.1703
2026-01-04 13:34:05,534: t15.2024.07.19 val PER: 0.2373
2026-01-04 13:34:05,534: t15.2024.07.21 val PER: 0.0883
2026-01-04 13:34:05,534: t15.2024.07.28 val PER: 0.1346
2026-01-04 13:34:05,534: t15.2025.01.10 val PER: 0.2865
2026-01-04 13:34:05,534: t15.2025.01.12 val PER: 0.1378
2026-01-04 13:34:05,534: t15.2025.03.14 val PER: 0.3491
2026-01-04 13:34:05,534: t15.2025.03.16 val PER: 0.1859
2026-01-04 13:34:05,534: t15.2025.03.30 val PER: 0.2885
2026-01-04 13:34:05,534: t15.2025.04.13 val PER: 0.2083
2026-01-04 13:34:05,869: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_19000
2026-01-04 13:34:26,132: Train batch 19200: loss: 6.06 grad norm: 47.76 time: 0.064
2026-01-04 13:34:47,469: Train batch 19400: loss: 4.89 grad norm: 37.11 time: 0.056
2026-01-04 13:34:58,118: Running test after training batch: 19500
2026-01-04 13:34:58,537: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:35:03,805: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 13:35:03,908: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 13:35:09,167: Val batch 19500: PER (avg): 0.1451 CTC Loss (avg): 14.9111 WER(1gram): 45.94% (n=64) time: 11.049
2026-01-04 13:35:09,168: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 13:35:09,168: t15.2023.08.13 val PER: 0.1050
2026-01-04 13:35:09,168: t15.2023.08.18 val PER: 0.1023
2026-01-04 13:35:09,169: t15.2023.08.20 val PER: 0.1088
2026-01-04 13:35:09,169: t15.2023.08.25 val PER: 0.0904
2026-01-04 13:35:09,169: t15.2023.08.27 val PER: 0.1817
2026-01-04 13:35:09,169: t15.2023.09.01 val PER: 0.0795
2026-01-04 13:35:09,169: t15.2023.09.03 val PER: 0.1473
2026-01-04 13:35:09,169: t15.2023.09.24 val PER: 0.1274
2026-01-04 13:35:09,169: t15.2023.09.29 val PER: 0.1289
2026-01-04 13:35:09,169: t15.2023.10.01 val PER: 0.1671
2026-01-04 13:35:09,169: t15.2023.10.06 val PER: 0.0797
2026-01-04 13:35:09,169: t15.2023.10.08 val PER: 0.2422
2026-01-04 13:35:09,169: t15.2023.10.13 val PER: 0.1955
2026-01-04 13:35:09,169: t15.2023.10.15 val PER: 0.1516
2026-01-04 13:35:09,170: t15.2023.10.20 val PER: 0.1711
2026-01-04 13:35:09,170: t15.2023.10.22 val PER: 0.1036
2026-01-04 13:35:09,170: t15.2023.11.03 val PER: 0.1825
2026-01-04 13:35:09,170: t15.2023.11.04 val PER: 0.0273
2026-01-04 13:35:09,170: t15.2023.11.17 val PER: 0.0295
2026-01-04 13:35:09,170: t15.2023.11.19 val PER: 0.0319
2026-01-04 13:35:09,170: t15.2023.11.26 val PER: 0.1116
2026-01-04 13:35:09,170: t15.2023.12.03 val PER: 0.1071
2026-01-04 13:35:09,170: t15.2023.12.08 val PER: 0.0899
2026-01-04 13:35:09,170: t15.2023.12.10 val PER: 0.0880
2026-01-04 13:35:09,170: t15.2023.12.17 val PER: 0.1268
2026-01-04 13:35:09,170: t15.2023.12.29 val PER: 0.1201
2026-01-04 13:35:09,170: t15.2024.02.25 val PER: 0.1081
2026-01-04 13:35:09,170: t15.2024.03.08 val PER: 0.2176
2026-01-04 13:35:09,170: t15.2024.03.15 val PER: 0.1882
2026-01-04 13:35:09,170: t15.2024.03.17 val PER: 0.1318
2026-01-04 13:35:09,171: t15.2024.05.10 val PER: 0.1620
2026-01-04 13:35:09,171: t15.2024.06.14 val PER: 0.1625
2026-01-04 13:35:09,171: t15.2024.07.19 val PER: 0.2287
2026-01-04 13:35:09,171: t15.2024.07.21 val PER: 0.0841
2026-01-04 13:35:09,171: t15.2024.07.28 val PER: 0.1316
2026-01-04 13:35:09,171: t15.2025.01.10 val PER: 0.2824
2026-01-04 13:35:09,171: t15.2025.01.12 val PER: 0.1416
2026-01-04 13:35:09,171: t15.2025.03.14 val PER: 0.3491
2026-01-04 13:35:09,171: t15.2025.03.16 val PER: 0.1832
2026-01-04 13:35:09,171: t15.2025.03.30 val PER: 0.2874
2026-01-04 13:35:09,171: t15.2025.04.13 val PER: 0.2111
2026-01-04 13:35:09,486: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_19500
2026-01-04 13:35:18,882: Train batch 19600: loss: 7.33 grad norm: 46.98 time: 0.058
2026-01-04 13:35:37,910: Train batch 19800: loss: 7.14 grad norm: 50.34 time: 0.055
2026-01-04 13:35:56,469: Running test after training batch: 19999
2026-01-04 13:35:56,567: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:36:01,764: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 13:36:01,865: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 13:36:07,237: Val batch 19999: PER (avg): 0.1445 CTC Loss (avg): 14.9378 WER(1gram): 46.19% (n=64) time: 10.767
2026-01-04 13:36:07,237: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 13:36:07,238: t15.2023.08.13 val PER: 0.1071
2026-01-04 13:36:07,238: t15.2023.08.18 val PER: 0.1014
2026-01-04 13:36:07,238: t15.2023.08.20 val PER: 0.1072
2026-01-04 13:36:07,238: t15.2023.08.25 val PER: 0.0904
2026-01-04 13:36:07,238: t15.2023.08.27 val PER: 0.1833
2026-01-04 13:36:07,238: t15.2023.09.01 val PER: 0.0820
2026-01-04 13:36:07,238: t15.2023.09.03 val PER: 0.1437
2026-01-04 13:36:07,238: t15.2023.09.24 val PER: 0.1286
2026-01-04 13:36:07,238: t15.2023.09.29 val PER: 0.1257
2026-01-04 13:36:07,239: t15.2023.10.01 val PER: 0.1691
2026-01-04 13:36:07,239: t15.2023.10.06 val PER: 0.0743
2026-01-04 13:36:07,239: t15.2023.10.08 val PER: 0.2476
2026-01-04 13:36:07,239: t15.2023.10.13 val PER: 0.1901
2026-01-04 13:36:07,239: t15.2023.10.15 val PER: 0.1477
2026-01-04 13:36:07,239: t15.2023.10.20 val PER: 0.1779
2026-01-04 13:36:07,239: t15.2023.10.22 val PER: 0.1024
2026-01-04 13:36:07,240: t15.2023.11.03 val PER: 0.1818
2026-01-04 13:36:07,240: t15.2023.11.04 val PER: 0.0273
2026-01-04 13:36:07,240: t15.2023.11.17 val PER: 0.0342
2026-01-04 13:36:07,240: t15.2023.11.19 val PER: 0.0319
2026-01-04 13:36:07,240: t15.2023.11.26 val PER: 0.1065
2026-01-04 13:36:07,240: t15.2023.12.03 val PER: 0.1071
2026-01-04 13:36:07,240: t15.2023.12.08 val PER: 0.0912
2026-01-04 13:36:07,240: t15.2023.12.10 val PER: 0.0867
2026-01-04 13:36:07,244: t15.2023.12.17 val PER: 0.1289
2026-01-04 13:36:07,244: t15.2023.12.29 val PER: 0.1208
2026-01-04 13:36:07,244: t15.2024.02.25 val PER: 0.1025
2026-01-04 13:36:07,244: t15.2024.03.08 val PER: 0.2191
2026-01-04 13:36:07,245: t15.2024.03.15 val PER: 0.1851
2026-01-04 13:36:07,245: t15.2024.03.17 val PER: 0.1269
2026-01-04 13:36:07,245: t15.2024.05.10 val PER: 0.1530
2026-01-04 13:36:07,245: t15.2024.06.14 val PER: 0.1640
2026-01-04 13:36:07,245: t15.2024.07.19 val PER: 0.2254
2026-01-04 13:36:07,245: t15.2024.07.21 val PER: 0.0883
2026-01-04 13:36:07,245: t15.2024.07.28 val PER: 0.1331
2026-01-04 13:36:07,245: t15.2025.01.10 val PER: 0.2810
2026-01-04 13:36:07,245: t15.2025.01.12 val PER: 0.1370
2026-01-04 13:36:07,245: t15.2025.03.14 val PER: 0.3506
2026-01-04 13:36:07,245: t15.2025.03.16 val PER: 0.1832
2026-01-04 13:36:07,246: t15.2025.03.30 val PER: 0.3000
2026-01-04 13:36:07,246: t15.2025.04.13 val PER: 0.2154
2026-01-04 13:36:07,561: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-4/checkpoint/checkpoint_batch_19999
2026-01-04 13:36:07,599: Best avg val PER achieved: 0.14846
2026-01-04 13:36:07,600: Total training time: 41.87 minutes

=== RUN wd1e-5.yaml ===
2026-01-04 13:36:13,778: Using device: cuda:0
2026-01-04 13:36:15,557: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-04 13:36:15,580: Using 45 sessions after filtering (from 45).
2026-01-04 13:36:18,237: Using torch.compile (if available)
2026-01-04 13:36:18,238: torch.compile not available (torch<2.0). Skipping.
2026-01-04 13:36:18,239: Initialized RNN decoding model
2026-01-04 13:36:18,239: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-04 13:36:18,239: Model has 44,907,305 parameters
2026-01-04 13:36:18,240: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-04 13:36:19,605: Successfully initialized datasets
2026-01-04 13:36:19,606: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-04 13:36:22,170: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.203
2026-01-04 13:36:22,170: Running test after training batch: 0
2026-01-04 13:36:22,291: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:36:28,393: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-04 13:36:29,501: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-04 13:37:25,553: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 63.383
2026-01-04 13:37:25,554: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-04 13:37:25,555: t15.2023.08.13 val PER: 1.3056
2026-01-04 13:37:25,555: t15.2023.08.18 val PER: 1.4208
2026-01-04 13:37:25,555: t15.2023.08.20 val PER: 1.3002
2026-01-04 13:37:25,555: t15.2023.08.25 val PER: 1.3389
2026-01-04 13:37:25,555: t15.2023.08.27 val PER: 1.2460
2026-01-04 13:37:25,555: t15.2023.09.01 val PER: 1.4537
2026-01-04 13:37:25,555: t15.2023.09.03 val PER: 1.3171
2026-01-04 13:37:25,556: t15.2023.09.24 val PER: 1.5461
2026-01-04 13:37:25,556: t15.2023.09.29 val PER: 1.4671
2026-01-04 13:37:25,556: t15.2023.10.01 val PER: 1.2147
2026-01-04 13:37:25,556: t15.2023.10.06 val PER: 1.4876
2026-01-04 13:37:25,556: t15.2023.10.08 val PER: 1.1827
2026-01-04 13:37:25,556: t15.2023.10.13 val PER: 1.3964
2026-01-04 13:37:25,556: t15.2023.10.15 val PER: 1.3889
2026-01-04 13:37:25,556: t15.2023.10.20 val PER: 1.4866
2026-01-04 13:37:25,556: t15.2023.10.22 val PER: 1.3942
2026-01-04 13:37:25,556: t15.2023.11.03 val PER: 1.5923
2026-01-04 13:37:25,557: t15.2023.11.04 val PER: 2.0171
2026-01-04 13:37:25,557: t15.2023.11.17 val PER: 1.9518
2026-01-04 13:37:25,557: t15.2023.11.19 val PER: 1.6707
2026-01-04 13:37:25,557: t15.2023.11.26 val PER: 1.5413
2026-01-04 13:37:25,557: t15.2023.12.03 val PER: 1.4254
2026-01-04 13:37:25,557: t15.2023.12.08 val PER: 1.4487
2026-01-04 13:37:25,557: t15.2023.12.10 val PER: 1.6899
2026-01-04 13:37:25,557: t15.2023.12.17 val PER: 1.3077
2026-01-04 13:37:25,557: t15.2023.12.29 val PER: 1.4063
2026-01-04 13:37:25,557: t15.2024.02.25 val PER: 1.4228
2026-01-04 13:37:25,557: t15.2024.03.08 val PER: 1.3257
2026-01-04 13:37:25,557: t15.2024.03.15 val PER: 1.3196
2026-01-04 13:37:25,557: t15.2024.03.17 val PER: 1.4052
2026-01-04 13:37:25,558: t15.2024.05.10 val PER: 1.3224
2026-01-04 13:37:25,558: t15.2024.06.14 val PER: 1.5315
2026-01-04 13:37:25,558: t15.2024.07.19 val PER: 1.0817
2026-01-04 13:37:25,558: t15.2024.07.21 val PER: 1.6290
2026-01-04 13:37:25,558: t15.2024.07.28 val PER: 1.6588
2026-01-04 13:37:25,558: t15.2025.01.10 val PER: 1.0923
2026-01-04 13:37:25,558: t15.2025.01.12 val PER: 1.7629
2026-01-04 13:37:25,558: t15.2025.03.14 val PER: 1.0414
2026-01-04 13:37:25,558: t15.2025.03.16 val PER: 1.6257
2026-01-04 13:37:25,558: t15.2025.03.30 val PER: 1.2874
2026-01-04 13:37:25,558: t15.2025.04.13 val PER: 1.5949
2026-01-04 13:37:25,560: New best val WER(1gram) inf% --> 100.00%
2026-01-04 13:37:25,560: Checkpointing model
2026-01-04 13:37:25,859: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:37:26,162: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_0
2026-01-04 13:37:45,937: Train batch 200: loss: 77.58 grad norm: 106.09 time: 0.055
2026-01-04 13:38:04,007: Train batch 400: loss: 54.17 grad norm: 99.30 time: 0.063
2026-01-04 13:38:13,080: Running test after training batch: 500
2026-01-04 13:38:13,233: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:38:18,610: WER debug example
  GT : you can see the code at this point as well
  PR : used and ease thus uhde at this ide is aisle
2026-01-04 13:38:18,663: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as adz
2026-01-04 13:38:22,892: Val batch 500: PER (avg): 0.5157 CTC Loss (avg): 55.6097 WER(1gram): 89.85% (n=64) time: 9.812
2026-01-04 13:38:22,893: WER lens: avg_true_words=6.16 avg_pred_words=5.67 max_pred_words=11
2026-01-04 13:38:22,893: t15.2023.08.13 val PER: 0.4719
2026-01-04 13:38:22,893: t15.2023.08.18 val PER: 0.4484
2026-01-04 13:38:22,894: t15.2023.08.20 val PER: 0.4464
2026-01-04 13:38:22,894: t15.2023.08.25 val PER: 0.4307
2026-01-04 13:38:22,894: t15.2023.08.27 val PER: 0.5289
2026-01-04 13:38:22,894: t15.2023.09.01 val PER: 0.4221
2026-01-04 13:38:22,895: t15.2023.09.03 val PER: 0.4917
2026-01-04 13:38:22,895: t15.2023.09.24 val PER: 0.4260
2026-01-04 13:38:22,895: t15.2023.09.29 val PER: 0.4697
2026-01-04 13:38:22,895: t15.2023.10.01 val PER: 0.5185
2026-01-04 13:38:22,895: t15.2023.10.06 val PER: 0.4209
2026-01-04 13:38:22,895: t15.2023.10.08 val PER: 0.5399
2026-01-04 13:38:22,895: t15.2023.10.13 val PER: 0.5687
2026-01-04 13:38:22,895: t15.2023.10.15 val PER: 0.4957
2026-01-04 13:38:22,895: t15.2023.10.20 val PER: 0.4597
2026-01-04 13:38:22,895: t15.2023.10.22 val PER: 0.4488
2026-01-04 13:38:22,896: t15.2023.11.03 val PER: 0.5075
2026-01-04 13:38:22,896: t15.2023.11.04 val PER: 0.2662
2026-01-04 13:38:22,896: t15.2023.11.17 val PER: 0.3608
2026-01-04 13:38:22,896: t15.2023.11.19 val PER: 0.3353
2026-01-04 13:38:22,896: t15.2023.11.26 val PER: 0.5507
2026-01-04 13:38:22,896: t15.2023.12.03 val PER: 0.4895
2026-01-04 13:38:22,896: t15.2023.12.08 val PER: 0.5133
2026-01-04 13:38:22,896: t15.2023.12.10 val PER: 0.4534
2026-01-04 13:38:22,896: t15.2023.12.17 val PER: 0.5520
2026-01-04 13:38:22,896: t15.2023.12.29 val PER: 0.5422
2026-01-04 13:38:22,897: t15.2024.02.25 val PER: 0.4719
2026-01-04 13:38:22,897: t15.2024.03.08 val PER: 0.6273
2026-01-04 13:38:22,897: t15.2024.03.15 val PER: 0.5541
2026-01-04 13:38:22,897: t15.2024.03.17 val PER: 0.5070
2026-01-04 13:38:22,897: t15.2024.05.10 val PER: 0.5483
2026-01-04 13:38:22,897: t15.2024.06.14 val PER: 0.5126
2026-01-04 13:38:22,897: t15.2024.07.19 val PER: 0.6638
2026-01-04 13:38:22,897: t15.2024.07.21 val PER: 0.4724
2026-01-04 13:38:22,897: t15.2024.07.28 val PER: 0.5022
2026-01-04 13:38:22,897: t15.2025.01.10 val PER: 0.7397
2026-01-04 13:38:22,897: t15.2025.01.12 val PER: 0.5527
2026-01-04 13:38:22,898: t15.2025.03.14 val PER: 0.7101
2026-01-04 13:38:22,898: t15.2025.03.16 val PER: 0.5916
2026-01-04 13:38:22,898: t15.2025.03.30 val PER: 0.7138
2026-01-04 13:38:22,898: t15.2025.04.13 val PER: 0.5749
2026-01-04 13:38:22,899: New best val WER(1gram) 100.00% --> 89.85%
2026-01-04 13:38:22,899: Checkpointing model
2026-01-04 13:38:23,556: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:38:23,863: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_500
2026-01-04 13:38:34,187: Train batch 600: loss: 49.19 grad norm: 83.47 time: 0.081
2026-01-04 13:38:53,012: Train batch 800: loss: 40.71 grad norm: 83.74 time: 0.057
2026-01-04 13:39:11,631: Train batch 1000: loss: 42.85 grad norm: 80.85 time: 0.066
2026-01-04 13:39:11,631: Running test after training batch: 1000
2026-01-04 13:39:11,770: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:39:17,020: WER debug example
  GT : you can see the code at this point as well
  PR : used ent ease thus owed it this uhde is will
2026-01-04 13:39:17,075: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus wass
2026-01-04 13:39:20,168: Val batch 1000: PER (avg): 0.4049 CTC Loss (avg): 42.4492 WER(1gram): 81.22% (n=64) time: 8.537
2026-01-04 13:39:20,169: WER lens: avg_true_words=6.16 avg_pred_words=5.67 max_pred_words=12
2026-01-04 13:39:20,169: t15.2023.08.13 val PER: 0.3732
2026-01-04 13:39:20,169: t15.2023.08.18 val PER: 0.3286
2026-01-04 13:39:20,170: t15.2023.08.20 val PER: 0.3392
2026-01-04 13:39:20,170: t15.2023.08.25 val PER: 0.2892
2026-01-04 13:39:20,170: t15.2023.08.27 val PER: 0.4148
2026-01-04 13:39:20,170: t15.2023.09.01 val PER: 0.2979
2026-01-04 13:39:20,171: t15.2023.09.03 val PER: 0.3967
2026-01-04 13:39:20,171: t15.2023.09.24 val PER: 0.3228
2026-01-04 13:39:20,171: t15.2023.09.29 val PER: 0.3555
2026-01-04 13:39:20,171: t15.2023.10.01 val PER: 0.4055
2026-01-04 13:39:20,171: t15.2023.10.06 val PER: 0.3025
2026-01-04 13:39:20,171: t15.2023.10.08 val PER: 0.4425
2026-01-04 13:39:20,171: t15.2023.10.13 val PER: 0.4600
2026-01-04 13:39:20,171: t15.2023.10.15 val PER: 0.3751
2026-01-04 13:39:20,171: t15.2023.10.20 val PER: 0.3893
2026-01-04 13:39:20,172: t15.2023.10.22 val PER: 0.3508
2026-01-04 13:39:20,172: t15.2023.11.03 val PER: 0.3989
2026-01-04 13:39:20,172: t15.2023.11.04 val PER: 0.1570
2026-01-04 13:39:20,172: t15.2023.11.17 val PER: 0.2691
2026-01-04 13:39:20,172: t15.2023.11.19 val PER: 0.2216
2026-01-04 13:39:20,172: t15.2023.11.26 val PER: 0.4464
2026-01-04 13:39:20,172: t15.2023.12.03 val PER: 0.3897
2026-01-04 13:39:20,172: t15.2023.12.08 val PER: 0.4061
2026-01-04 13:39:20,172: t15.2023.12.10 val PER: 0.3430
2026-01-04 13:39:20,172: t15.2023.12.17 val PER: 0.4075
2026-01-04 13:39:20,173: t15.2023.12.29 val PER: 0.4008
2026-01-04 13:39:20,173: t15.2024.02.25 val PER: 0.3427
2026-01-04 13:39:20,173: t15.2024.03.08 val PER: 0.4979
2026-01-04 13:39:20,173: t15.2024.03.15 val PER: 0.4365
2026-01-04 13:39:20,173: t15.2024.03.17 val PER: 0.4114
2026-01-04 13:39:20,173: t15.2024.05.10 val PER: 0.4086
2026-01-04 13:39:20,173: t15.2024.06.14 val PER: 0.3975
2026-01-04 13:39:20,173: t15.2024.07.19 val PER: 0.5300
2026-01-04 13:39:20,173: t15.2024.07.21 val PER: 0.3766
2026-01-04 13:39:20,173: t15.2024.07.28 val PER: 0.4110
2026-01-04 13:39:20,173: t15.2025.01.10 val PER: 0.6240
2026-01-04 13:39:20,173: t15.2025.01.12 val PER: 0.4542
2026-01-04 13:39:20,174: t15.2025.03.14 val PER: 0.6346
2026-01-04 13:39:20,174: t15.2025.03.16 val PER: 0.4804
2026-01-04 13:39:20,174: t15.2025.03.30 val PER: 0.6184
2026-01-04 13:39:20,174: t15.2025.04.13 val PER: 0.4922
2026-01-04 13:39:20,174: New best val WER(1gram) 89.85% --> 81.22%
2026-01-04 13:39:20,175: Checkpointing model
2026-01-04 13:39:20,878: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:39:21,193: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_1000
2026-01-04 13:39:39,954: Train batch 1200: loss: 33.45 grad norm: 74.54 time: 0.068
2026-01-04 13:39:59,582: Train batch 1400: loss: 36.04 grad norm: 81.54 time: 0.062
2026-01-04 13:40:08,861: Running test after training batch: 1500
2026-01-04 13:40:09,277: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:40:14,485: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt sze the code it this boyde is will
2026-01-04 13:40:14,531: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap thus cost
2026-01-04 13:40:16,570: Val batch 1500: PER (avg): 0.3789 CTC Loss (avg): 37.0745 WER(1gram): 74.87% (n=64) time: 7.708
2026-01-04 13:40:16,570: WER lens: avg_true_words=6.16 avg_pred_words=5.02 max_pred_words=11
2026-01-04 13:40:16,571: t15.2023.08.13 val PER: 0.3472
2026-01-04 13:40:16,571: t15.2023.08.18 val PER: 0.3143
2026-01-04 13:40:16,571: t15.2023.08.20 val PER: 0.3050
2026-01-04 13:40:16,571: t15.2023.08.25 val PER: 0.2605
2026-01-04 13:40:16,571: t15.2023.08.27 val PER: 0.4035
2026-01-04 13:40:16,571: t15.2023.09.01 val PER: 0.2776
2026-01-04 13:40:16,571: t15.2023.09.03 val PER: 0.3836
2026-01-04 13:40:16,572: t15.2023.09.24 val PER: 0.3070
2026-01-04 13:40:16,572: t15.2023.09.29 val PER: 0.3452
2026-01-04 13:40:16,572: t15.2023.10.01 val PER: 0.3937
2026-01-04 13:40:16,572: t15.2023.10.06 val PER: 0.2906
2026-01-04 13:40:16,572: t15.2023.10.08 val PER: 0.4371
2026-01-04 13:40:16,572: t15.2023.10.13 val PER: 0.4422
2026-01-04 13:40:16,572: t15.2023.10.15 val PER: 0.3573
2026-01-04 13:40:16,572: t15.2023.10.20 val PER: 0.3289
2026-01-04 13:40:16,572: t15.2023.10.22 val PER: 0.3040
2026-01-04 13:40:16,573: t15.2023.11.03 val PER: 0.3596
2026-01-04 13:40:16,573: t15.2023.11.04 val PER: 0.1126
2026-01-04 13:40:16,573: t15.2023.11.17 val PER: 0.2302
2026-01-04 13:40:16,573: t15.2023.11.19 val PER: 0.1697
2026-01-04 13:40:16,573: t15.2023.11.26 val PER: 0.4145
2026-01-04 13:40:16,573: t15.2023.12.03 val PER: 0.3782
2026-01-04 13:40:16,573: t15.2023.12.08 val PER: 0.3555
2026-01-04 13:40:16,573: t15.2023.12.10 val PER: 0.2943
2026-01-04 13:40:16,573: t15.2023.12.17 val PER: 0.3721
2026-01-04 13:40:16,573: t15.2023.12.29 val PER: 0.3741
2026-01-04 13:40:16,574: t15.2024.02.25 val PER: 0.3146
2026-01-04 13:40:16,574: t15.2024.03.08 val PER: 0.4552
2026-01-04 13:40:16,574: t15.2024.03.15 val PER: 0.4165
2026-01-04 13:40:16,574: t15.2024.03.17 val PER: 0.3731
2026-01-04 13:40:16,574: t15.2024.05.10 val PER: 0.3863
2026-01-04 13:40:16,574: t15.2024.06.14 val PER: 0.3912
2026-01-04 13:40:16,574: t15.2024.07.19 val PER: 0.5155
2026-01-04 13:40:16,574: t15.2024.07.21 val PER: 0.3414
2026-01-04 13:40:16,574: t15.2024.07.28 val PER: 0.3640
2026-01-04 13:40:16,574: t15.2025.01.10 val PER: 0.5964
2026-01-04 13:40:16,574: t15.2025.01.12 val PER: 0.4373
2026-01-04 13:40:16,575: t15.2025.03.14 val PER: 0.6006
2026-01-04 13:40:16,575: t15.2025.03.16 val PER: 0.4490
2026-01-04 13:40:16,575: t15.2025.03.30 val PER: 0.6195
2026-01-04 13:40:16,575: t15.2025.04.13 val PER: 0.4722
2026-01-04 13:40:16,575: New best val WER(1gram) 81.22% --> 74.87%
2026-01-04 13:40:16,576: Checkpointing model
2026-01-04 13:40:17,243: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:40:17,543: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_1500
2026-01-04 13:40:27,093: Train batch 1600: loss: 36.83 grad norm: 77.61 time: 0.064
2026-01-04 13:40:45,540: Train batch 1800: loss: 35.31 grad norm: 69.66 time: 0.088
2026-01-04 13:41:04,239: Train batch 2000: loss: 33.76 grad norm: 69.87 time: 0.067
2026-01-04 13:41:04,239: Running test after training batch: 2000
2026-01-04 13:41:04,660: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:41:10,475: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this boyte is will
2026-01-04 13:41:10,515: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap thus us id
2026-01-04 13:41:12,600: Val batch 2000: PER (avg): 0.3247 CTC Loss (avg): 32.6476 WER(1gram): 70.81% (n=64) time: 8.361
2026-01-04 13:41:12,601: WER lens: avg_true_words=6.16 avg_pred_words=5.55 max_pred_words=11
2026-01-04 13:41:12,601: t15.2023.08.13 val PER: 0.3004
2026-01-04 13:41:12,602: t15.2023.08.18 val PER: 0.2448
2026-01-04 13:41:12,602: t15.2023.08.20 val PER: 0.2438
2026-01-04 13:41:12,602: t15.2023.08.25 val PER: 0.2274
2026-01-04 13:41:12,602: t15.2023.08.27 val PER: 0.3344
2026-01-04 13:41:12,602: t15.2023.09.01 val PER: 0.2240
2026-01-04 13:41:12,602: t15.2023.09.03 val PER: 0.3349
2026-01-04 13:41:12,603: t15.2023.09.24 val PER: 0.2464
2026-01-04 13:41:12,603: t15.2023.09.29 val PER: 0.2706
2026-01-04 13:41:12,603: t15.2023.10.01 val PER: 0.3203
2026-01-04 13:41:12,603: t15.2023.10.06 val PER: 0.2336
2026-01-04 13:41:12,603: t15.2023.10.08 val PER: 0.3911
2026-01-04 13:41:12,603: t15.2023.10.13 val PER: 0.3701
2026-01-04 13:41:12,603: t15.2023.10.15 val PER: 0.2940
2026-01-04 13:41:12,603: t15.2023.10.20 val PER: 0.3020
2026-01-04 13:41:12,603: t15.2023.10.22 val PER: 0.2595
2026-01-04 13:41:12,603: t15.2023.11.03 val PER: 0.3277
2026-01-04 13:41:12,604: t15.2023.11.04 val PER: 0.1024
2026-01-04 13:41:12,604: t15.2023.11.17 val PER: 0.1757
2026-01-04 13:41:12,604: t15.2023.11.19 val PER: 0.1457
2026-01-04 13:41:12,604: t15.2023.11.26 val PER: 0.3616
2026-01-04 13:41:12,604: t15.2023.12.03 val PER: 0.3120
2026-01-04 13:41:12,604: t15.2023.12.08 val PER: 0.3156
2026-01-04 13:41:12,604: t15.2023.12.10 val PER: 0.2523
2026-01-04 13:41:12,604: t15.2023.12.17 val PER: 0.3129
2026-01-04 13:41:12,604: t15.2023.12.29 val PER: 0.3294
2026-01-04 13:41:12,604: t15.2024.02.25 val PER: 0.2851
2026-01-04 13:41:12,604: t15.2024.03.08 val PER: 0.3898
2026-01-04 13:41:12,605: t15.2024.03.15 val PER: 0.3602
2026-01-04 13:41:12,605: t15.2024.03.17 val PER: 0.3326
2026-01-04 13:41:12,605: t15.2024.05.10 val PER: 0.3373
2026-01-04 13:41:12,605: t15.2024.06.14 val PER: 0.3407
2026-01-04 13:41:12,606: t15.2024.07.19 val PER: 0.4634
2026-01-04 13:41:12,606: t15.2024.07.21 val PER: 0.2855
2026-01-04 13:41:12,606: t15.2024.07.28 val PER: 0.3199
2026-01-04 13:41:12,606: t15.2025.01.10 val PER: 0.5551
2026-01-04 13:41:12,606: t15.2025.01.12 val PER: 0.3734
2026-01-04 13:41:12,606: t15.2025.03.14 val PER: 0.5399
2026-01-04 13:41:12,606: t15.2025.03.16 val PER: 0.3861
2026-01-04 13:41:12,606: t15.2025.03.30 val PER: 0.5483
2026-01-04 13:41:12,606: t15.2025.04.13 val PER: 0.3994
2026-01-04 13:41:12,607: New best val WER(1gram) 74.87% --> 70.81%
2026-01-04 13:41:12,607: Checkpointing model
2026-01-04 13:41:13,299: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:41:13,600: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_2000
2026-01-04 13:41:32,274: Train batch 2200: loss: 28.95 grad norm: 75.96 time: 0.061
2026-01-04 13:41:50,664: Train batch 2400: loss: 29.13 grad norm: 63.47 time: 0.052
2026-01-04 13:41:59,655: Running test after training batch: 2500
2026-01-04 13:42:00,128: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:42:05,460: WER debug example
  GT : you can see the code at this point as well
  PR : yule end e the good at this point is will
2026-01-04 13:42:05,508: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-04 13:42:08,084: Val batch 2500: PER (avg): 0.3022 CTC Loss (avg): 29.9292 WER(1gram): 67.51% (n=64) time: 8.429
2026-01-04 13:42:08,085: WER lens: avg_true_words=6.16 avg_pred_words=5.45 max_pred_words=10
2026-01-04 13:42:08,085: t15.2023.08.13 val PER: 0.2744
2026-01-04 13:42:08,085: t15.2023.08.18 val PER: 0.2347
2026-01-04 13:42:08,085: t15.2023.08.20 val PER: 0.2359
2026-01-04 13:42:08,085: t15.2023.08.25 val PER: 0.2018
2026-01-04 13:42:08,085: t15.2023.08.27 val PER: 0.3232
2026-01-04 13:42:08,086: t15.2023.09.01 val PER: 0.2062
2026-01-04 13:42:08,086: t15.2023.09.03 val PER: 0.3029
2026-01-04 13:42:08,086: t15.2023.09.24 val PER: 0.2269
2026-01-04 13:42:08,086: t15.2023.09.29 val PER: 0.2514
2026-01-04 13:42:08,086: t15.2023.10.01 val PER: 0.2992
2026-01-04 13:42:08,086: t15.2023.10.06 val PER: 0.2088
2026-01-04 13:42:08,087: t15.2023.10.08 val PER: 0.3775
2026-01-04 13:42:08,087: t15.2023.10.13 val PER: 0.3623
2026-01-04 13:42:08,087: t15.2023.10.15 val PER: 0.2874
2026-01-04 13:42:08,087: t15.2023.10.20 val PER: 0.2752
2026-01-04 13:42:08,087: t15.2023.10.22 val PER: 0.2372
2026-01-04 13:42:08,087: t15.2023.11.03 val PER: 0.2890
2026-01-04 13:42:08,088: t15.2023.11.04 val PER: 0.0819
2026-01-04 13:42:08,088: t15.2023.11.17 val PER: 0.1493
2026-01-04 13:42:08,088: t15.2023.11.19 val PER: 0.1337
2026-01-04 13:42:08,088: t15.2023.11.26 val PER: 0.3471
2026-01-04 13:42:08,088: t15.2023.12.03 val PER: 0.2868
2026-01-04 13:42:08,088: t15.2023.12.08 val PER: 0.2803
2026-01-04 13:42:08,088: t15.2023.12.10 val PER: 0.2326
2026-01-04 13:42:08,088: t15.2023.12.17 val PER: 0.2869
2026-01-04 13:42:08,089: t15.2023.12.29 val PER: 0.2965
2026-01-04 13:42:08,089: t15.2024.02.25 val PER: 0.2430
2026-01-04 13:42:08,089: t15.2024.03.08 val PER: 0.3599
2026-01-04 13:42:08,089: t15.2024.03.15 val PER: 0.3496
2026-01-04 13:42:08,089: t15.2024.03.17 val PER: 0.3096
2026-01-04 13:42:08,089: t15.2024.05.10 val PER: 0.3150
2026-01-04 13:42:08,089: t15.2024.06.14 val PER: 0.3091
2026-01-04 13:42:08,089: t15.2024.07.19 val PER: 0.4436
2026-01-04 13:42:08,090: t15.2024.07.21 val PER: 0.2669
2026-01-04 13:42:08,090: t15.2024.07.28 val PER: 0.2934
2026-01-04 13:42:08,090: t15.2025.01.10 val PER: 0.4931
2026-01-04 13:42:08,090: t15.2025.01.12 val PER: 0.3595
2026-01-04 13:42:08,090: t15.2025.03.14 val PER: 0.4896
2026-01-04 13:42:08,090: t15.2025.03.16 val PER: 0.3626
2026-01-04 13:42:08,090: t15.2025.03.30 val PER: 0.5080
2026-01-04 13:42:08,090: t15.2025.04.13 val PER: 0.3937
2026-01-04 13:42:08,091: New best val WER(1gram) 70.81% --> 67.51%
2026-01-04 13:42:08,091: Checkpointing model
2026-01-04 13:42:08,768: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:42:09,067: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_2500
2026-01-04 13:42:18,846: Train batch 2600: loss: 34.78 grad norm: 83.60 time: 0.057
2026-01-04 13:42:37,797: Train batch 2800: loss: 25.85 grad norm: 68.64 time: 0.083
2026-01-04 13:42:56,090: Train batch 3000: loss: 31.70 grad norm: 73.20 time: 0.084
2026-01-04 13:42:56,090: Running test after training batch: 3000
2026-01-04 13:42:56,203: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:43:01,380: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-04 13:43:01,423: WER debug example
  GT : how does it keep the cost down
  PR : houde des it kipp the cost get
2026-01-04 13:43:04,113: Val batch 3000: PER (avg): 0.2788 CTC Loss (avg): 27.6519 WER(1gram): 65.48% (n=64) time: 8.022
2026-01-04 13:43:04,113: WER lens: avg_true_words=6.16 avg_pred_words=5.88 max_pred_words=11
2026-01-04 13:43:04,113: t15.2023.08.13 val PER: 0.2505
2026-01-04 13:43:04,113: t15.2023.08.18 val PER: 0.2221
2026-01-04 13:43:04,114: t15.2023.08.20 val PER: 0.2145
2026-01-04 13:43:04,114: t15.2023.08.25 val PER: 0.1852
2026-01-04 13:43:04,114: t15.2023.08.27 val PER: 0.2942
2026-01-04 13:43:04,114: t15.2023.09.01 val PER: 0.1924
2026-01-04 13:43:04,114: t15.2023.09.03 val PER: 0.2957
2026-01-04 13:43:04,114: t15.2023.09.24 val PER: 0.2197
2026-01-04 13:43:04,114: t15.2023.09.29 val PER: 0.2285
2026-01-04 13:43:04,114: t15.2023.10.01 val PER: 0.2933
2026-01-04 13:43:04,114: t15.2023.10.06 val PER: 0.1938
2026-01-04 13:43:04,114: t15.2023.10.08 val PER: 0.3478
2026-01-04 13:43:04,115: t15.2023.10.13 val PER: 0.3421
2026-01-04 13:43:04,115: t15.2023.10.15 val PER: 0.2604
2026-01-04 13:43:04,115: t15.2023.10.20 val PER: 0.2651
2026-01-04 13:43:04,115: t15.2023.10.22 val PER: 0.2071
2026-01-04 13:43:04,115: t15.2023.11.03 val PER: 0.2632
2026-01-04 13:43:04,115: t15.2023.11.04 val PER: 0.0819
2026-01-04 13:43:04,115: t15.2023.11.17 val PER: 0.1229
2026-01-04 13:43:04,115: t15.2023.11.19 val PER: 0.1218
2026-01-04 13:43:04,115: t15.2023.11.26 val PER: 0.3058
2026-01-04 13:43:04,115: t15.2023.12.03 val PER: 0.2658
2026-01-04 13:43:04,115: t15.2023.12.08 val PER: 0.2563
2026-01-04 13:43:04,115: t15.2023.12.10 val PER: 0.2050
2026-01-04 13:43:04,115: t15.2023.12.17 val PER: 0.2682
2026-01-04 13:43:04,115: t15.2023.12.29 val PER: 0.2759
2026-01-04 13:43:04,115: t15.2024.02.25 val PER: 0.2346
2026-01-04 13:43:04,115: t15.2024.03.08 val PER: 0.3499
2026-01-04 13:43:04,116: t15.2024.03.15 val PER: 0.3346
2026-01-04 13:43:04,116: t15.2024.03.17 val PER: 0.2866
2026-01-04 13:43:04,116: t15.2024.05.10 val PER: 0.2823
2026-01-04 13:43:04,116: t15.2024.06.14 val PER: 0.2918
2026-01-04 13:43:04,116: t15.2024.07.19 val PER: 0.4001
2026-01-04 13:43:04,116: t15.2024.07.21 val PER: 0.2241
2026-01-04 13:43:04,116: t15.2024.07.28 val PER: 0.2757
2026-01-04 13:43:04,116: t15.2025.01.10 val PER: 0.4890
2026-01-04 13:43:04,116: t15.2025.01.12 val PER: 0.3233
2026-01-04 13:43:04,116: t15.2025.03.14 val PER: 0.4408
2026-01-04 13:43:04,116: t15.2025.03.16 val PER: 0.3312
2026-01-04 13:43:04,116: t15.2025.03.30 val PER: 0.4724
2026-01-04 13:43:04,116: t15.2025.04.13 val PER: 0.3524
2026-01-04 13:43:04,117: New best val WER(1gram) 67.51% --> 65.48%
2026-01-04 13:43:04,117: Checkpointing model
2026-01-04 13:43:04,801: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:43:05,100: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_3000
2026-01-04 13:43:23,686: Train batch 3200: loss: 26.43 grad norm: 67.41 time: 0.076
2026-01-04 13:43:42,283: Train batch 3400: loss: 18.30 grad norm: 54.95 time: 0.049
2026-01-04 13:43:51,531: Running test after training batch: 3500
2026-01-04 13:43:51,665: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:43:56,794: WER debug example
  GT : you can see the code at this point as well
  PR : yule end sci the code at this point will
2026-01-04 13:43:56,838: WER debug example
  GT : how does it keep the cost down
  PR : houde des it hipp thus us ent
2026-01-04 13:43:59,364: Val batch 3500: PER (avg): 0.2688 CTC Loss (avg): 26.4666 WER(1gram): 67.51% (n=64) time: 7.833
2026-01-04 13:43:59,365: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=11
2026-01-04 13:43:59,365: t15.2023.08.13 val PER: 0.2464
2026-01-04 13:43:59,365: t15.2023.08.18 val PER: 0.2129
2026-01-04 13:43:59,366: t15.2023.08.20 val PER: 0.2121
2026-01-04 13:43:59,366: t15.2023.08.25 val PER: 0.1807
2026-01-04 13:43:59,366: t15.2023.08.27 val PER: 0.2701
2026-01-04 13:43:59,366: t15.2023.09.01 val PER: 0.1778
2026-01-04 13:43:59,366: t15.2023.09.03 val PER: 0.2648
2026-01-04 13:43:59,366: t15.2023.09.24 val PER: 0.2087
2026-01-04 13:43:59,366: t15.2023.09.29 val PER: 0.2176
2026-01-04 13:43:59,367: t15.2023.10.01 val PER: 0.2807
2026-01-04 13:43:59,367: t15.2023.10.06 val PER: 0.1776
2026-01-04 13:43:59,367: t15.2023.10.08 val PER: 0.3451
2026-01-04 13:43:59,367: t15.2023.10.13 val PER: 0.3196
2026-01-04 13:43:59,367: t15.2023.10.15 val PER: 0.2479
2026-01-04 13:43:59,367: t15.2023.10.20 val PER: 0.2584
2026-01-04 13:43:59,367: t15.2023.10.22 val PER: 0.2105
2026-01-04 13:43:59,367: t15.2023.11.03 val PER: 0.2693
2026-01-04 13:43:59,368: t15.2023.11.04 val PER: 0.0683
2026-01-04 13:43:59,368: t15.2023.11.17 val PER: 0.1229
2026-01-04 13:43:59,368: t15.2023.11.19 val PER: 0.1098
2026-01-04 13:43:59,368: t15.2023.11.26 val PER: 0.2812
2026-01-04 13:43:59,368: t15.2023.12.03 val PER: 0.2542
2026-01-04 13:43:59,368: t15.2023.12.08 val PER: 0.2403
2026-01-04 13:43:59,368: t15.2023.12.10 val PER: 0.1958
2026-01-04 13:43:59,368: t15.2023.12.17 val PER: 0.2796
2026-01-04 13:43:59,368: t15.2023.12.29 val PER: 0.2567
2026-01-04 13:43:59,368: t15.2024.02.25 val PER: 0.2065
2026-01-04 13:43:59,368: t15.2024.03.08 val PER: 0.3471
2026-01-04 13:43:59,368: t15.2024.03.15 val PER: 0.3208
2026-01-04 13:43:59,368: t15.2024.03.17 val PER: 0.2796
2026-01-04 13:43:59,368: t15.2024.05.10 val PER: 0.2868
2026-01-04 13:43:59,369: t15.2024.06.14 val PER: 0.2808
2026-01-04 13:43:59,369: t15.2024.07.19 val PER: 0.3902
2026-01-04 13:43:59,369: t15.2024.07.21 val PER: 0.2221
2026-01-04 13:43:59,369: t15.2024.07.28 val PER: 0.2824
2026-01-04 13:43:59,369: t15.2025.01.10 val PER: 0.4697
2026-01-04 13:43:59,369: t15.2025.01.12 val PER: 0.2972
2026-01-04 13:43:59,369: t15.2025.03.14 val PER: 0.4438
2026-01-04 13:43:59,369: t15.2025.03.16 val PER: 0.3259
2026-01-04 13:43:59,369: t15.2025.03.30 val PER: 0.4632
2026-01-04 13:43:59,369: t15.2025.04.13 val PER: 0.3438
2026-01-04 13:43:59,653: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_3500
2026-01-04 13:44:08,968: Train batch 3600: loss: 22.50 grad norm: 61.02 time: 0.067
2026-01-04 13:44:27,505: Train batch 3800: loss: 25.57 grad norm: 69.16 time: 0.066
2026-01-04 13:44:46,364: Train batch 4000: loss: 19.31 grad norm: 54.88 time: 0.056
2026-01-04 13:44:46,364: Running test after training batch: 4000
2026-01-04 13:44:46,481: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:44:51,351: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point while
2026-01-04 13:44:51,380: WER debug example
  GT : how does it keep the cost down
  PR : how dust it kipp the cost nett
2026-01-04 13:44:52,987: Val batch 4000: PER (avg): 0.2510 CTC Loss (avg): 24.4901 WER(1gram): 64.47% (n=64) time: 6.623
2026-01-04 13:44:52,987: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=11
2026-01-04 13:44:52,988: t15.2023.08.13 val PER: 0.2173
2026-01-04 13:44:52,988: t15.2023.08.18 val PER: 0.2037
2026-01-04 13:44:52,988: t15.2023.08.20 val PER: 0.2041
2026-01-04 13:44:52,988: t15.2023.08.25 val PER: 0.1536
2026-01-04 13:44:52,988: t15.2023.08.27 val PER: 0.2733
2026-01-04 13:44:52,988: t15.2023.09.01 val PER: 0.1615
2026-01-04 13:44:52,988: t15.2023.09.03 val PER: 0.2577
2026-01-04 13:44:52,988: t15.2023.09.24 val PER: 0.1966
2026-01-04 13:44:52,988: t15.2023.09.29 val PER: 0.2055
2026-01-04 13:44:52,988: t15.2023.10.01 val PER: 0.2649
2026-01-04 13:44:52,988: t15.2023.10.06 val PER: 0.1776
2026-01-04 13:44:52,988: t15.2023.10.08 val PER: 0.3275
2026-01-04 13:44:52,988: t15.2023.10.13 val PER: 0.2995
2026-01-04 13:44:52,989: t15.2023.10.15 val PER: 0.2347
2026-01-04 13:44:52,989: t15.2023.10.20 val PER: 0.2383
2026-01-04 13:44:52,989: t15.2023.10.22 val PER: 0.2116
2026-01-04 13:44:52,989: t15.2023.11.03 val PER: 0.2415
2026-01-04 13:44:52,989: t15.2023.11.04 val PER: 0.0648
2026-01-04 13:44:52,989: t15.2023.11.17 val PER: 0.1073
2026-01-04 13:44:52,989: t15.2023.11.19 val PER: 0.0978
2026-01-04 13:44:52,989: t15.2023.11.26 val PER: 0.2645
2026-01-04 13:44:52,989: t15.2023.12.03 val PER: 0.2206
2026-01-04 13:44:52,989: t15.2023.12.08 val PER: 0.2150
2026-01-04 13:44:52,989: t15.2023.12.10 val PER: 0.1787
2026-01-04 13:44:52,989: t15.2023.12.17 val PER: 0.2557
2026-01-04 13:44:52,989: t15.2023.12.29 val PER: 0.2567
2026-01-04 13:44:52,989: t15.2024.02.25 val PER: 0.2191
2026-01-04 13:44:52,990: t15.2024.03.08 val PER: 0.3343
2026-01-04 13:44:52,990: t15.2024.03.15 val PER: 0.3008
2026-01-04 13:44:52,990: t15.2024.03.17 val PER: 0.2643
2026-01-04 13:44:52,990: t15.2024.05.10 val PER: 0.2704
2026-01-04 13:44:52,990: t15.2024.06.14 val PER: 0.2618
2026-01-04 13:44:52,990: t15.2024.07.19 val PER: 0.3731
2026-01-04 13:44:52,990: t15.2024.07.21 val PER: 0.1903
2026-01-04 13:44:52,990: t15.2024.07.28 val PER: 0.2463
2026-01-04 13:44:52,990: t15.2025.01.10 val PER: 0.4408
2026-01-04 13:44:52,990: t15.2025.01.12 val PER: 0.2725
2026-01-04 13:44:52,990: t15.2025.03.14 val PER: 0.4068
2026-01-04 13:44:52,990: t15.2025.03.16 val PER: 0.3089
2026-01-04 13:44:52,990: t15.2025.03.30 val PER: 0.4069
2026-01-04 13:44:52,990: t15.2025.04.13 val PER: 0.3338
2026-01-04 13:44:52,992: New best val WER(1gram) 65.48% --> 64.47%
2026-01-04 13:44:52,992: Checkpointing model
2026-01-04 13:44:53,636: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:44:53,911: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_4000
2026-01-04 13:45:12,874: Train batch 4200: loss: 22.64 grad norm: 67.46 time: 0.078
2026-01-04 13:45:32,057: Train batch 4400: loss: 16.99 grad norm: 57.49 time: 0.066
2026-01-04 13:45:40,984: Running test after training batch: 4500
2026-01-04 13:45:41,250: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:45:46,323: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 13:45:46,373: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cost get
2026-01-04 13:45:48,883: Val batch 4500: PER (avg): 0.2392 CTC Loss (avg): 23.1858 WER(1gram): 60.91% (n=64) time: 7.898
2026-01-04 13:45:48,883: WER lens: avg_true_words=6.16 avg_pred_words=6.00 max_pred_words=11
2026-01-04 13:45:48,884: t15.2023.08.13 val PER: 0.2089
2026-01-04 13:45:48,884: t15.2023.08.18 val PER: 0.1928
2026-01-04 13:45:48,884: t15.2023.08.20 val PER: 0.1890
2026-01-04 13:45:48,884: t15.2023.08.25 val PER: 0.1506
2026-01-04 13:45:48,884: t15.2023.08.27 val PER: 0.2588
2026-01-04 13:45:48,884: t15.2023.09.01 val PER: 0.1567
2026-01-04 13:45:48,884: t15.2023.09.03 val PER: 0.2411
2026-01-04 13:45:48,884: t15.2023.09.24 val PER: 0.1748
2026-01-04 13:45:48,884: t15.2023.09.29 val PER: 0.1997
2026-01-04 13:45:48,884: t15.2023.10.01 val PER: 0.2622
2026-01-04 13:45:48,884: t15.2023.10.06 val PER: 0.1496
2026-01-04 13:45:48,885: t15.2023.10.08 val PER: 0.3207
2026-01-04 13:45:48,885: t15.2023.10.13 val PER: 0.2925
2026-01-04 13:45:48,885: t15.2023.10.15 val PER: 0.2314
2026-01-04 13:45:48,885: t15.2023.10.20 val PER: 0.2181
2026-01-04 13:45:48,885: t15.2023.10.22 val PER: 0.1871
2026-01-04 13:45:48,885: t15.2023.11.03 val PER: 0.2442
2026-01-04 13:45:48,885: t15.2023.11.04 val PER: 0.0580
2026-01-04 13:45:48,885: t15.2023.11.17 val PER: 0.0918
2026-01-04 13:45:48,885: t15.2023.11.19 val PER: 0.0898
2026-01-04 13:45:48,885: t15.2023.11.26 val PER: 0.2688
2026-01-04 13:45:48,886: t15.2023.12.03 val PER: 0.2195
2026-01-04 13:45:48,886: t15.2023.12.08 val PER: 0.2071
2026-01-04 13:45:48,886: t15.2023.12.10 val PER: 0.1840
2026-01-04 13:45:48,886: t15.2023.12.17 val PER: 0.2287
2026-01-04 13:45:48,886: t15.2023.12.29 val PER: 0.2409
2026-01-04 13:45:48,886: t15.2024.02.25 val PER: 0.2107
2026-01-04 13:45:48,886: t15.2024.03.08 val PER: 0.3215
2026-01-04 13:45:48,886: t15.2024.03.15 val PER: 0.2927
2026-01-04 13:45:48,886: t15.2024.03.17 val PER: 0.2434
2026-01-04 13:45:48,886: t15.2024.05.10 val PER: 0.2526
2026-01-04 13:45:48,886: t15.2024.06.14 val PER: 0.2413
2026-01-04 13:45:48,886: t15.2024.07.19 val PER: 0.3434
2026-01-04 13:45:48,886: t15.2024.07.21 val PER: 0.1759
2026-01-04 13:45:48,886: t15.2024.07.28 val PER: 0.2324
2026-01-04 13:45:48,886: t15.2025.01.10 val PER: 0.4174
2026-01-04 13:45:48,886: t15.2025.01.12 val PER: 0.2602
2026-01-04 13:45:48,887: t15.2025.03.14 val PER: 0.4053
2026-01-04 13:45:48,887: t15.2025.03.16 val PER: 0.2866
2026-01-04 13:45:48,887: t15.2025.03.30 val PER: 0.4069
2026-01-04 13:45:48,887: t15.2025.04.13 val PER: 0.2953
2026-01-04 13:45:48,888: New best val WER(1gram) 64.47% --> 60.91%
2026-01-04 13:45:48,888: Checkpointing model
2026-01-04 13:45:49,534: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:45:49,839: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_4500
2026-01-04 13:45:59,408: Train batch 4600: loss: 20.09 grad norm: 63.71 time: 0.062
2026-01-04 13:46:17,359: Train batch 4800: loss: 13.43 grad norm: 51.26 time: 0.064
2026-01-04 13:46:35,825: Train batch 5000: loss: 31.47 grad norm: 86.48 time: 0.064
2026-01-04 13:46:35,825: Running test after training batch: 5000
2026-01-04 13:46:35,961: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:46:41,356: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point as will
2026-01-04 13:46:41,396: WER debug example
  GT : how does it keep the cost down
  PR : howled des it heap the cost nett
2026-01-04 13:46:43,737: Val batch 5000: PER (avg): 0.2256 CTC Loss (avg): 21.9360 WER(1gram): 59.90% (n=64) time: 7.912
2026-01-04 13:46:43,738: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-04 13:46:43,738: t15.2023.08.13 val PER: 0.1923
2026-01-04 13:46:43,738: t15.2023.08.18 val PER: 0.1660
2026-01-04 13:46:43,738: t15.2023.08.20 val PER: 0.1716
2026-01-04 13:46:43,738: t15.2023.08.25 val PER: 0.1220
2026-01-04 13:46:43,738: t15.2023.08.27 val PER: 0.2363
2026-01-04 13:46:43,738: t15.2023.09.01 val PER: 0.1396
2026-01-04 13:46:43,739: t15.2023.09.03 val PER: 0.2292
2026-01-04 13:46:43,739: t15.2023.09.24 val PER: 0.1881
2026-01-04 13:46:43,739: t15.2023.09.29 val PER: 0.1806
2026-01-04 13:46:43,739: t15.2023.10.01 val PER: 0.2398
2026-01-04 13:46:43,739: t15.2023.10.06 val PER: 0.1518
2026-01-04 13:46:43,739: t15.2023.10.08 val PER: 0.3112
2026-01-04 13:46:43,739: t15.2023.10.13 val PER: 0.2793
2026-01-04 13:46:43,739: t15.2023.10.15 val PER: 0.2248
2026-01-04 13:46:43,739: t15.2023.10.20 val PER: 0.2315
2026-01-04 13:46:43,739: t15.2023.10.22 val PER: 0.1637
2026-01-04 13:46:43,739: t15.2023.11.03 val PER: 0.2137
2026-01-04 13:46:43,739: t15.2023.11.04 val PER: 0.0410
2026-01-04 13:46:43,740: t15.2023.11.17 val PER: 0.0762
2026-01-04 13:46:43,740: t15.2023.11.19 val PER: 0.0858
2026-01-04 13:46:43,740: t15.2023.11.26 val PER: 0.2348
2026-01-04 13:46:43,740: t15.2023.12.03 val PER: 0.2006
2026-01-04 13:46:43,740: t15.2023.12.08 val PER: 0.2024
2026-01-04 13:46:43,741: t15.2023.12.10 val PER: 0.1524
2026-01-04 13:46:43,741: t15.2023.12.17 val PER: 0.2277
2026-01-04 13:46:43,741: t15.2023.12.29 val PER: 0.2203
2026-01-04 13:46:43,741: t15.2024.02.25 val PER: 0.1882
2026-01-04 13:46:43,741: t15.2024.03.08 val PER: 0.3129
2026-01-04 13:46:43,741: t15.2024.03.15 val PER: 0.2846
2026-01-04 13:46:43,742: t15.2024.03.17 val PER: 0.2336
2026-01-04 13:46:43,742: t15.2024.05.10 val PER: 0.2526
2026-01-04 13:46:43,742: t15.2024.06.14 val PER: 0.2476
2026-01-04 13:46:43,742: t15.2024.07.19 val PER: 0.3316
2026-01-04 13:46:43,742: t15.2024.07.21 val PER: 0.1717
2026-01-04 13:46:43,742: t15.2024.07.28 val PER: 0.2250
2026-01-04 13:46:43,742: t15.2025.01.10 val PER: 0.3802
2026-01-04 13:46:43,742: t15.2025.01.12 val PER: 0.2494
2026-01-04 13:46:43,742: t15.2025.03.14 val PER: 0.4009
2026-01-04 13:46:43,742: t15.2025.03.16 val PER: 0.2788
2026-01-04 13:46:43,742: t15.2025.03.30 val PER: 0.3989
2026-01-04 13:46:43,743: t15.2025.04.13 val PER: 0.2924
2026-01-04 13:46:43,744: New best val WER(1gram) 60.91% --> 59.90%
2026-01-04 13:46:43,744: Checkpointing model
2026-01-04 13:46:44,403: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:46:44,691: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_5000
2026-01-04 13:47:02,739: Train batch 5200: loss: 16.47 grad norm: 60.82 time: 0.053
2026-01-04 13:47:20,478: Train batch 5400: loss: 17.81 grad norm: 61.23 time: 0.067
2026-01-04 13:47:30,222: Running test after training batch: 5500
2026-01-04 13:47:30,426: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:47:35,522: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e the could at this point will
2026-01-04 13:47:35,570: WER debug example
  GT : how does it keep the cost down
  PR : aue just it keep the cost tet
2026-01-04 13:47:38,238: Val batch 5500: PER (avg): 0.2167 CTC Loss (avg): 21.0546 WER(1gram): 57.36% (n=64) time: 8.015
2026-01-04 13:47:38,238: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-04 13:47:38,239: t15.2023.08.13 val PER: 0.1871
2026-01-04 13:47:38,239: t15.2023.08.18 val PER: 0.1651
2026-01-04 13:47:38,239: t15.2023.08.20 val PER: 0.1716
2026-01-04 13:47:38,239: t15.2023.08.25 val PER: 0.1295
2026-01-04 13:47:38,239: t15.2023.08.27 val PER: 0.2379
2026-01-04 13:47:38,239: t15.2023.09.01 val PER: 0.1339
2026-01-04 13:47:38,239: t15.2023.09.03 val PER: 0.2257
2026-01-04 13:47:38,240: t15.2023.09.24 val PER: 0.1760
2026-01-04 13:47:38,240: t15.2023.09.29 val PER: 0.1825
2026-01-04 13:47:38,240: t15.2023.10.01 val PER: 0.2325
2026-01-04 13:47:38,240: t15.2023.10.06 val PER: 0.1292
2026-01-04 13:47:38,240: t15.2023.10.08 val PER: 0.3031
2026-01-04 13:47:38,240: t15.2023.10.13 val PER: 0.2839
2026-01-04 13:47:38,240: t15.2023.10.15 val PER: 0.2162
2026-01-04 13:47:38,240: t15.2023.10.20 val PER: 0.2114
2026-01-04 13:47:38,240: t15.2023.10.22 val PER: 0.1581
2026-01-04 13:47:38,240: t15.2023.11.03 val PER: 0.2266
2026-01-04 13:47:38,240: t15.2023.11.04 val PER: 0.0648
2026-01-04 13:47:38,241: t15.2023.11.17 val PER: 0.0731
2026-01-04 13:47:38,241: t15.2023.11.19 val PER: 0.0798
2026-01-04 13:47:38,241: t15.2023.11.26 val PER: 0.2152
2026-01-04 13:47:38,241: t15.2023.12.03 val PER: 0.1828
2026-01-04 13:47:38,241: t15.2023.12.08 val PER: 0.1871
2026-01-04 13:47:38,241: t15.2023.12.10 val PER: 0.1669
2026-01-04 13:47:38,241: t15.2023.12.17 val PER: 0.2110
2026-01-04 13:47:38,241: t15.2023.12.29 val PER: 0.2121
2026-01-04 13:47:38,241: t15.2024.02.25 val PER: 0.1812
2026-01-04 13:47:38,241: t15.2024.03.08 val PER: 0.2973
2026-01-04 13:47:38,241: t15.2024.03.15 val PER: 0.2602
2026-01-04 13:47:38,241: t15.2024.03.17 val PER: 0.2148
2026-01-04 13:47:38,241: t15.2024.05.10 val PER: 0.2244
2026-01-04 13:47:38,241: t15.2024.06.14 val PER: 0.2366
2026-01-04 13:47:38,241: t15.2024.07.19 val PER: 0.3243
2026-01-04 13:47:38,241: t15.2024.07.21 val PER: 0.1593
2026-01-04 13:47:38,242: t15.2024.07.28 val PER: 0.2154
2026-01-04 13:47:38,242: t15.2025.01.10 val PER: 0.3747
2026-01-04 13:47:38,242: t15.2025.01.12 val PER: 0.2402
2026-01-04 13:47:38,242: t15.2025.03.14 val PER: 0.3772
2026-01-04 13:47:38,242: t15.2025.03.16 val PER: 0.2657
2026-01-04 13:47:38,242: t15.2025.03.30 val PER: 0.3598
2026-01-04 13:47:38,242: t15.2025.04.13 val PER: 0.2810
2026-01-04 13:47:38,243: New best val WER(1gram) 59.90% --> 57.36%
2026-01-04 13:47:38,243: Checkpointing model
2026-01-04 13:47:38,880: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:47:39,170: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_5500
2026-01-04 13:47:49,056: Train batch 5600: loss: 19.02 grad norm: 66.86 time: 0.062
2026-01-04 13:48:06,832: Train batch 5800: loss: 13.31 grad norm: 54.40 time: 0.083
2026-01-04 13:48:24,381: Train batch 6000: loss: 14.04 grad norm: 54.98 time: 0.049
2026-01-04 13:48:24,381: Running test after training batch: 6000
2026-01-04 13:48:24,579: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:48:29,637: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point as will
2026-01-04 13:48:29,693: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-04 13:48:32,582: Val batch 6000: PER (avg): 0.2103 CTC Loss (avg): 20.6628 WER(1gram): 56.60% (n=64) time: 8.200
2026-01-04 13:48:32,583: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 13:48:32,583: t15.2023.08.13 val PER: 0.1684
2026-01-04 13:48:32,583: t15.2023.08.18 val PER: 0.1643
2026-01-04 13:48:32,584: t15.2023.08.20 val PER: 0.1668
2026-01-04 13:48:32,584: t15.2023.08.25 val PER: 0.1130
2026-01-04 13:48:32,584: t15.2023.08.27 val PER: 0.2379
2026-01-04 13:48:32,584: t15.2023.09.01 val PER: 0.1364
2026-01-04 13:48:32,584: t15.2023.09.03 val PER: 0.1995
2026-01-04 13:48:32,584: t15.2023.09.24 val PER: 0.1663
2026-01-04 13:48:32,584: t15.2023.09.29 val PER: 0.1710
2026-01-04 13:48:32,584: t15.2023.10.01 val PER: 0.2226
2026-01-04 13:48:32,584: t15.2023.10.06 val PER: 0.1313
2026-01-04 13:48:32,584: t15.2023.10.08 val PER: 0.2950
2026-01-04 13:48:32,584: t15.2023.10.13 val PER: 0.2723
2026-01-04 13:48:32,584: t15.2023.10.15 val PER: 0.2103
2026-01-04 13:48:32,585: t15.2023.10.20 val PER: 0.2215
2026-01-04 13:48:32,585: t15.2023.10.22 val PER: 0.1726
2026-01-04 13:48:32,585: t15.2023.11.03 val PER: 0.2280
2026-01-04 13:48:32,585: t15.2023.11.04 val PER: 0.0648
2026-01-04 13:48:32,585: t15.2023.11.17 val PER: 0.0715
2026-01-04 13:48:32,585: t15.2023.11.19 val PER: 0.0699
2026-01-04 13:48:32,585: t15.2023.11.26 val PER: 0.2225
2026-01-04 13:48:32,585: t15.2023.12.03 val PER: 0.1670
2026-01-04 13:48:32,585: t15.2023.12.08 val PER: 0.1744
2026-01-04 13:48:32,585: t15.2023.12.10 val PER: 0.1511
2026-01-04 13:48:32,585: t15.2023.12.17 val PER: 0.2006
2026-01-04 13:48:32,585: t15.2023.12.29 val PER: 0.2080
2026-01-04 13:48:32,585: t15.2024.02.25 val PER: 0.1615
2026-01-04 13:48:32,586: t15.2024.03.08 val PER: 0.2888
2026-01-04 13:48:32,586: t15.2024.03.15 val PER: 0.2658
2026-01-04 13:48:32,586: t15.2024.03.17 val PER: 0.2050
2026-01-04 13:48:32,586: t15.2024.05.10 val PER: 0.2229
2026-01-04 13:48:32,586: t15.2024.06.14 val PER: 0.2256
2026-01-04 13:48:32,586: t15.2024.07.19 val PER: 0.3085
2026-01-04 13:48:32,586: t15.2024.07.21 val PER: 0.1600
2026-01-04 13:48:32,586: t15.2024.07.28 val PER: 0.2007
2026-01-04 13:48:32,586: t15.2025.01.10 val PER: 0.3747
2026-01-04 13:48:32,586: t15.2025.01.12 val PER: 0.2132
2026-01-04 13:48:32,586: t15.2025.03.14 val PER: 0.3891
2026-01-04 13:48:32,586: t15.2025.03.16 val PER: 0.2552
2026-01-04 13:48:32,586: t15.2025.03.30 val PER: 0.3747
2026-01-04 13:48:32,586: t15.2025.04.13 val PER: 0.2753
2026-01-04 13:48:32,588: New best val WER(1gram) 57.36% --> 56.60%
2026-01-04 13:48:32,588: Checkpointing model
2026-01-04 13:48:33,242: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:48:33,532: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_6000
2026-01-04 13:48:51,859: Train batch 6200: loss: 16.28 grad norm: 59.81 time: 0.070
2026-01-04 13:49:10,337: Train batch 6400: loss: 18.39 grad norm: 64.85 time: 0.061
2026-01-04 13:49:19,390: Running test after training batch: 6500
2026-01-04 13:49:19,578: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:49:24,640: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point as will
2026-01-04 13:49:24,693: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 13:49:27,585: Val batch 6500: PER (avg): 0.2051 CTC Loss (avg): 20.1510 WER(1gram): 53.05% (n=64) time: 8.195
2026-01-04 13:49:27,586: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 13:49:27,586: t15.2023.08.13 val PER: 0.1809
2026-01-04 13:49:27,586: t15.2023.08.18 val PER: 0.1492
2026-01-04 13:49:27,586: t15.2023.08.20 val PER: 0.1581
2026-01-04 13:49:27,586: t15.2023.08.25 val PER: 0.1099
2026-01-04 13:49:27,586: t15.2023.08.27 val PER: 0.2363
2026-01-04 13:49:27,586: t15.2023.09.01 val PER: 0.1185
2026-01-04 13:49:27,586: t15.2023.09.03 val PER: 0.2055
2026-01-04 13:49:27,586: t15.2023.09.24 val PER: 0.1735
2026-01-04 13:49:27,586: t15.2023.09.29 val PER: 0.1736
2026-01-04 13:49:27,586: t15.2023.10.01 val PER: 0.2272
2026-01-04 13:49:27,586: t15.2023.10.06 val PER: 0.1335
2026-01-04 13:49:27,587: t15.2023.10.08 val PER: 0.2977
2026-01-04 13:49:27,587: t15.2023.10.13 val PER: 0.2661
2026-01-04 13:49:27,587: t15.2023.10.15 val PER: 0.2103
2026-01-04 13:49:27,587: t15.2023.10.20 val PER: 0.2282
2026-01-04 13:49:27,587: t15.2023.10.22 val PER: 0.1604
2026-01-04 13:49:27,587: t15.2023.11.03 val PER: 0.2171
2026-01-04 13:49:27,588: t15.2023.11.04 val PER: 0.0444
2026-01-04 13:49:27,588: t15.2023.11.17 val PER: 0.0591
2026-01-04 13:49:27,588: t15.2023.11.19 val PER: 0.0778
2026-01-04 13:49:27,588: t15.2023.11.26 val PER: 0.2000
2026-01-04 13:49:27,588: t15.2023.12.03 val PER: 0.1691
2026-01-04 13:49:27,588: t15.2023.12.08 val PER: 0.1704
2026-01-04 13:49:27,588: t15.2023.12.10 val PER: 0.1459
2026-01-04 13:49:27,588: t15.2023.12.17 val PER: 0.1861
2026-01-04 13:49:27,588: t15.2023.12.29 val PER: 0.2011
2026-01-04 13:49:27,588: t15.2024.02.25 val PER: 0.1531
2026-01-04 13:49:27,588: t15.2024.03.08 val PER: 0.2731
2026-01-04 13:49:27,589: t15.2024.03.15 val PER: 0.2495
2026-01-04 13:49:27,589: t15.2024.03.17 val PER: 0.2001
2026-01-04 13:49:27,589: t15.2024.05.10 val PER: 0.2214
2026-01-04 13:49:27,589: t15.2024.06.14 val PER: 0.2224
2026-01-04 13:49:27,589: t15.2024.07.19 val PER: 0.3111
2026-01-04 13:49:27,589: t15.2024.07.21 val PER: 0.1572
2026-01-04 13:49:27,589: t15.2024.07.28 val PER: 0.1963
2026-01-04 13:49:27,589: t15.2025.01.10 val PER: 0.3609
2026-01-04 13:49:27,589: t15.2025.01.12 val PER: 0.2225
2026-01-04 13:49:27,589: t15.2025.03.14 val PER: 0.3905
2026-01-04 13:49:27,589: t15.2025.03.16 val PER: 0.2343
2026-01-04 13:49:27,589: t15.2025.03.30 val PER: 0.3552
2026-01-04 13:49:27,589: t15.2025.04.13 val PER: 0.2710
2026-01-04 13:49:27,591: New best val WER(1gram) 56.60% --> 53.05%
2026-01-04 13:49:27,591: Checkpointing model
2026-01-04 13:49:28,212: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:49:28,493: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_6500
2026-01-04 13:49:37,469: Train batch 6600: loss: 12.29 grad norm: 52.06 time: 0.045
2026-01-04 13:49:55,607: Train batch 6800: loss: 15.40 grad norm: 56.11 time: 0.048
2026-01-04 13:50:14,800: Train batch 7000: loss: 17.26 grad norm: 63.45 time: 0.061
2026-01-04 13:50:14,801: Running test after training batch: 7000
2026-01-04 13:50:14,908: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:50:20,037: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 13:50:20,093: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-04 13:50:23,229: Val batch 7000: PER (avg): 0.1934 CTC Loss (avg): 19.2405 WER(1gram): 51.02% (n=64) time: 8.428
2026-01-04 13:50:23,230: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 13:50:23,230: t15.2023.08.13 val PER: 0.1486
2026-01-04 13:50:23,230: t15.2023.08.18 val PER: 0.1391
2026-01-04 13:50:23,230: t15.2023.08.20 val PER: 0.1406
2026-01-04 13:50:23,230: t15.2023.08.25 val PER: 0.1009
2026-01-04 13:50:23,231: t15.2023.08.27 val PER: 0.2138
2026-01-04 13:50:23,231: t15.2023.09.01 val PER: 0.1128
2026-01-04 13:50:23,231: t15.2023.09.03 val PER: 0.1829
2026-01-04 13:50:23,231: t15.2023.09.24 val PER: 0.1638
2026-01-04 13:50:23,231: t15.2023.09.29 val PER: 0.1589
2026-01-04 13:50:23,231: t15.2023.10.01 val PER: 0.2114
2026-01-04 13:50:23,231: t15.2023.10.06 val PER: 0.1076
2026-01-04 13:50:23,231: t15.2023.10.08 val PER: 0.2936
2026-01-04 13:50:23,231: t15.2023.10.13 val PER: 0.2630
2026-01-04 13:50:23,232: t15.2023.10.15 val PER: 0.1918
2026-01-04 13:50:23,232: t15.2023.10.20 val PER: 0.2081
2026-01-04 13:50:23,232: t15.2023.10.22 val PER: 0.1336
2026-01-04 13:50:23,232: t15.2023.11.03 val PER: 0.2096
2026-01-04 13:50:23,232: t15.2023.11.04 val PER: 0.0341
2026-01-04 13:50:23,232: t15.2023.11.17 val PER: 0.0622
2026-01-04 13:50:23,232: t15.2023.11.19 val PER: 0.0459
2026-01-04 13:50:23,232: t15.2023.11.26 val PER: 0.1913
2026-01-04 13:50:23,232: t15.2023.12.03 val PER: 0.1586
2026-01-04 13:50:23,232: t15.2023.12.08 val PER: 0.1538
2026-01-04 13:50:23,232: t15.2023.12.10 val PER: 0.1511
2026-01-04 13:50:23,232: t15.2023.12.17 val PER: 0.1819
2026-01-04 13:50:23,232: t15.2023.12.29 val PER: 0.1901
2026-01-04 13:50:23,232: t15.2024.02.25 val PER: 0.1545
2026-01-04 13:50:23,233: t15.2024.03.08 val PER: 0.2774
2026-01-04 13:50:23,233: t15.2024.03.15 val PER: 0.2414
2026-01-04 13:50:23,233: t15.2024.03.17 val PER: 0.2092
2026-01-04 13:50:23,233: t15.2024.05.10 val PER: 0.2065
2026-01-04 13:50:23,233: t15.2024.06.14 val PER: 0.2035
2026-01-04 13:50:23,233: t15.2024.07.19 val PER: 0.3039
2026-01-04 13:50:23,233: t15.2024.07.21 val PER: 0.1255
2026-01-04 13:50:23,233: t15.2024.07.28 val PER: 0.1735
2026-01-04 13:50:23,233: t15.2025.01.10 val PER: 0.3650
2026-01-04 13:50:23,233: t15.2025.01.12 val PER: 0.2032
2026-01-04 13:50:23,233: t15.2025.03.14 val PER: 0.3580
2026-01-04 13:50:23,234: t15.2025.03.16 val PER: 0.2382
2026-01-04 13:50:23,234: t15.2025.03.30 val PER: 0.3575
2026-01-04 13:50:23,234: t15.2025.04.13 val PER: 0.2810
2026-01-04 13:50:23,234: New best val WER(1gram) 53.05% --> 51.02%
2026-01-04 13:50:23,234: Checkpointing model
2026-01-04 13:50:23,904: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:50:24,190: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_7000
2026-01-04 13:50:44,261: Train batch 7200: loss: 14.29 grad norm: 57.37 time: 0.079
2026-01-04 13:51:02,568: Train batch 7400: loss: 13.01 grad norm: 53.54 time: 0.076
2026-01-04 13:51:12,056: Running test after training batch: 7500
2026-01-04 13:51:12,170: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:51:16,959: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 13:51:16,988: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-04 13:51:18,666: Val batch 7500: PER (avg): 0.1898 CTC Loss (avg): 18.7271 WER(1gram): 54.06% (n=64) time: 6.609
2026-01-04 13:51:18,666: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 13:51:18,666: t15.2023.08.13 val PER: 0.1570
2026-01-04 13:51:18,667: t15.2023.08.18 val PER: 0.1350
2026-01-04 13:51:18,667: t15.2023.08.20 val PER: 0.1461
2026-01-04 13:51:18,667: t15.2023.08.25 val PER: 0.1039
2026-01-04 13:51:18,667: t15.2023.08.27 val PER: 0.2026
2026-01-04 13:51:18,667: t15.2023.09.01 val PER: 0.1169
2026-01-04 13:51:18,667: t15.2023.09.03 val PER: 0.1865
2026-01-04 13:51:18,667: t15.2023.09.24 val PER: 0.1493
2026-01-04 13:51:18,667: t15.2023.09.29 val PER: 0.1608
2026-01-04 13:51:18,668: t15.2023.10.01 val PER: 0.2041
2026-01-04 13:51:18,668: t15.2023.10.06 val PER: 0.1066
2026-01-04 13:51:18,668: t15.2023.10.08 val PER: 0.2706
2026-01-04 13:51:18,668: t15.2023.10.13 val PER: 0.2599
2026-01-04 13:51:18,668: t15.2023.10.15 val PER: 0.1984
2026-01-04 13:51:18,668: t15.2023.10.20 val PER: 0.1980
2026-01-04 13:51:18,668: t15.2023.10.22 val PER: 0.1425
2026-01-04 13:51:18,668: t15.2023.11.03 val PER: 0.2049
2026-01-04 13:51:18,668: t15.2023.11.04 val PER: 0.0410
2026-01-04 13:51:18,668: t15.2023.11.17 val PER: 0.0715
2026-01-04 13:51:18,668: t15.2023.11.19 val PER: 0.0619
2026-01-04 13:51:18,669: t15.2023.11.26 val PER: 0.1746
2026-01-04 13:51:18,669: t15.2023.12.03 val PER: 0.1555
2026-01-04 13:51:18,669: t15.2023.12.08 val PER: 0.1578
2026-01-04 13:51:18,669: t15.2023.12.10 val PER: 0.1380
2026-01-04 13:51:18,669: t15.2023.12.17 val PER: 0.1871
2026-01-04 13:51:18,669: t15.2023.12.29 val PER: 0.1894
2026-01-04 13:51:18,669: t15.2024.02.25 val PER: 0.1433
2026-01-04 13:51:18,669: t15.2024.03.08 val PER: 0.2745
2026-01-04 13:51:18,669: t15.2024.03.15 val PER: 0.2402
2026-01-04 13:51:18,669: t15.2024.03.17 val PER: 0.1827
2026-01-04 13:51:18,669: t15.2024.05.10 val PER: 0.2036
2026-01-04 13:51:18,669: t15.2024.06.14 val PER: 0.1987
2026-01-04 13:51:18,669: t15.2024.07.19 val PER: 0.2828
2026-01-04 13:51:18,669: t15.2024.07.21 val PER: 0.1393
2026-01-04 13:51:18,670: t15.2024.07.28 val PER: 0.1721
2026-01-04 13:51:18,670: t15.2025.01.10 val PER: 0.3471
2026-01-04 13:51:18,670: t15.2025.01.12 val PER: 0.1925
2026-01-04 13:51:18,670: t15.2025.03.14 val PER: 0.3683
2026-01-04 13:51:18,670: t15.2025.03.16 val PER: 0.2291
2026-01-04 13:51:18,670: t15.2025.03.30 val PER: 0.3586
2026-01-04 13:51:18,670: t15.2025.04.13 val PER: 0.2482
2026-01-04 13:51:18,931: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_7500
2026-01-04 13:51:27,829: Train batch 7600: loss: 16.41 grad norm: 61.96 time: 0.068
2026-01-04 13:51:46,523: Train batch 7800: loss: 14.27 grad norm: 60.24 time: 0.056
2026-01-04 13:52:04,744: Train batch 8000: loss: 11.20 grad norm: 51.46 time: 0.072
2026-01-04 13:52:04,744: Running test after training batch: 8000
2026-01-04 13:52:04,857: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:52:10,189: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 13:52:10,259: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-04 13:52:13,813: Val batch 8000: PER (avg): 0.1834 CTC Loss (avg): 18.1973 WER(1gram): 54.57% (n=64) time: 9.068
2026-01-04 13:52:13,813: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-04 13:52:13,813: t15.2023.08.13 val PER: 0.1476
2026-01-04 13:52:13,814: t15.2023.08.18 val PER: 0.1291
2026-01-04 13:52:13,814: t15.2023.08.20 val PER: 0.1398
2026-01-04 13:52:13,814: t15.2023.08.25 val PER: 0.1084
2026-01-04 13:52:13,814: t15.2023.08.27 val PER: 0.2122
2026-01-04 13:52:13,814: t15.2023.09.01 val PER: 0.1055
2026-01-04 13:52:13,814: t15.2023.09.03 val PER: 0.1960
2026-01-04 13:52:13,814: t15.2023.09.24 val PER: 0.1553
2026-01-04 13:52:13,814: t15.2023.09.29 val PER: 0.1500
2026-01-04 13:52:13,814: t15.2023.10.01 val PER: 0.2008
2026-01-04 13:52:13,814: t15.2023.10.06 val PER: 0.1055
2026-01-04 13:52:13,814: t15.2023.10.08 val PER: 0.2693
2026-01-04 13:52:13,814: t15.2023.10.13 val PER: 0.2436
2026-01-04 13:52:13,814: t15.2023.10.15 val PER: 0.1938
2026-01-04 13:52:13,814: t15.2023.10.20 val PER: 0.1711
2026-01-04 13:52:13,815: t15.2023.10.22 val PER: 0.1336
2026-01-04 13:52:13,815: t15.2023.11.03 val PER: 0.2069
2026-01-04 13:52:13,815: t15.2023.11.04 val PER: 0.0375
2026-01-04 13:52:13,815: t15.2023.11.17 val PER: 0.0638
2026-01-04 13:52:13,815: t15.2023.11.19 val PER: 0.0639
2026-01-04 13:52:13,815: t15.2023.11.26 val PER: 0.1659
2026-01-04 13:52:13,815: t15.2023.12.03 val PER: 0.1492
2026-01-04 13:52:13,815: t15.2023.12.08 val PER: 0.1485
2026-01-04 13:52:13,815: t15.2023.12.10 val PER: 0.1301
2026-01-04 13:52:13,815: t15.2023.12.17 val PER: 0.1736
2026-01-04 13:52:13,815: t15.2023.12.29 val PER: 0.1709
2026-01-04 13:52:13,815: t15.2024.02.25 val PER: 0.1306
2026-01-04 13:52:13,816: t15.2024.03.08 val PER: 0.2731
2026-01-04 13:52:13,816: t15.2024.03.15 val PER: 0.2351
2026-01-04 13:52:13,816: t15.2024.03.17 val PER: 0.1674
2026-01-04 13:52:13,816: t15.2024.05.10 val PER: 0.1991
2026-01-04 13:52:13,816: t15.2024.06.14 val PER: 0.2066
2026-01-04 13:52:13,816: t15.2024.07.19 val PER: 0.2966
2026-01-04 13:52:13,816: t15.2024.07.21 val PER: 0.1193
2026-01-04 13:52:13,816: t15.2024.07.28 val PER: 0.1596
2026-01-04 13:52:13,816: t15.2025.01.10 val PER: 0.3278
2026-01-04 13:52:13,816: t15.2025.01.12 val PER: 0.1809
2026-01-04 13:52:13,816: t15.2025.03.14 val PER: 0.3639
2026-01-04 13:52:13,816: t15.2025.03.16 val PER: 0.2199
2026-01-04 13:52:13,816: t15.2025.03.30 val PER: 0.3494
2026-01-04 13:52:13,816: t15.2025.04.13 val PER: 0.2596
2026-01-04 13:52:14,097: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_8000
2026-01-04 13:52:31,564: Train batch 8200: loss: 9.66 grad norm: 47.12 time: 0.055
2026-01-04 13:52:49,158: Train batch 8400: loss: 10.05 grad norm: 48.54 time: 0.064
2026-01-04 13:52:58,666: Running test after training batch: 8500
2026-01-04 13:52:59,012: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:53:04,459: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point us will
2026-01-04 13:53:04,492: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost ned
2026-01-04 13:53:06,368: Val batch 8500: PER (avg): 0.1805 CTC Loss (avg): 17.8012 WER(1gram): 49.75% (n=64) time: 7.701
2026-01-04 13:53:06,368: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 13:53:06,369: t15.2023.08.13 val PER: 0.1455
2026-01-04 13:53:06,369: t15.2023.08.18 val PER: 0.1316
2026-01-04 13:53:06,369: t15.2023.08.20 val PER: 0.1350
2026-01-04 13:53:06,369: t15.2023.08.25 val PER: 0.1069
2026-01-04 13:53:06,369: t15.2023.08.27 val PER: 0.2042
2026-01-04 13:53:06,369: t15.2023.09.01 val PER: 0.0950
2026-01-04 13:53:06,369: t15.2023.09.03 val PER: 0.2007
2026-01-04 13:53:06,369: t15.2023.09.24 val PER: 0.1553
2026-01-04 13:53:06,369: t15.2023.09.29 val PER: 0.1487
2026-01-04 13:53:06,370: t15.2023.10.01 val PER: 0.2015
2026-01-04 13:53:06,370: t15.2023.10.06 val PER: 0.1066
2026-01-04 13:53:06,370: t15.2023.10.08 val PER: 0.2571
2026-01-04 13:53:06,370: t15.2023.10.13 val PER: 0.2413
2026-01-04 13:53:06,370: t15.2023.10.15 val PER: 0.1879
2026-01-04 13:53:06,370: t15.2023.10.20 val PER: 0.1879
2026-01-04 13:53:06,370: t15.2023.10.22 val PER: 0.1448
2026-01-04 13:53:06,370: t15.2023.11.03 val PER: 0.1974
2026-01-04 13:53:06,371: t15.2023.11.04 val PER: 0.0375
2026-01-04 13:53:06,371: t15.2023.11.17 val PER: 0.0638
2026-01-04 13:53:06,371: t15.2023.11.19 val PER: 0.0439
2026-01-04 13:53:06,371: t15.2023.11.26 val PER: 0.1761
2026-01-04 13:53:06,371: t15.2023.12.03 val PER: 0.1418
2026-01-04 13:53:06,371: t15.2023.12.08 val PER: 0.1405
2026-01-04 13:53:06,371: t15.2023.12.10 val PER: 0.1091
2026-01-04 13:53:06,371: t15.2023.12.17 val PER: 0.1684
2026-01-04 13:53:06,371: t15.2023.12.29 val PER: 0.1723
2026-01-04 13:53:06,371: t15.2024.02.25 val PER: 0.1376
2026-01-04 13:53:06,371: t15.2024.03.08 val PER: 0.2646
2026-01-04 13:53:06,372: t15.2024.03.15 val PER: 0.2376
2026-01-04 13:53:06,372: t15.2024.03.17 val PER: 0.1660
2026-01-04 13:53:06,372: t15.2024.05.10 val PER: 0.1917
2026-01-04 13:53:06,372: t15.2024.06.14 val PER: 0.1940
2026-01-04 13:53:06,372: t15.2024.07.19 val PER: 0.2808
2026-01-04 13:53:06,372: t15.2024.07.21 val PER: 0.1207
2026-01-04 13:53:06,372: t15.2024.07.28 val PER: 0.1684
2026-01-04 13:53:06,372: t15.2025.01.10 val PER: 0.3278
2026-01-04 13:53:06,372: t15.2025.01.12 val PER: 0.1878
2026-01-04 13:53:06,372: t15.2025.03.14 val PER: 0.3654
2026-01-04 13:53:06,372: t15.2025.03.16 val PER: 0.2186
2026-01-04 13:53:06,372: t15.2025.03.30 val PER: 0.3310
2026-01-04 13:53:06,373: t15.2025.04.13 val PER: 0.2340
2026-01-04 13:53:06,374: New best val WER(1gram) 51.02% --> 49.75%
2026-01-04 13:53:06,374: Checkpointing model
2026-01-04 13:53:07,028: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:53:07,331: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_8500
2026-01-04 13:53:17,352: Train batch 8600: loss: 15.74 grad norm: 61.51 time: 0.056
2026-01-04 13:53:36,898: Train batch 8800: loss: 15.05 grad norm: 57.30 time: 0.060
2026-01-04 13:53:56,638: Train batch 9000: loss: 16.01 grad norm: 63.27 time: 0.072
2026-01-04 13:53:56,639: Running test after training batch: 9000
2026-01-04 13:53:56,767: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:54:01,601: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 13:54:01,635: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-04 13:54:03,551: Val batch 9000: PER (avg): 0.1756 CTC Loss (avg): 17.3757 WER(1gram): 50.76% (n=64) time: 6.911
2026-01-04 13:54:03,551: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 13:54:03,552: t15.2023.08.13 val PER: 0.1424
2026-01-04 13:54:03,552: t15.2023.08.18 val PER: 0.1266
2026-01-04 13:54:03,552: t15.2023.08.20 val PER: 0.1326
2026-01-04 13:54:03,552: t15.2023.08.25 val PER: 0.1024
2026-01-04 13:54:03,552: t15.2023.08.27 val PER: 0.2010
2026-01-04 13:54:03,552: t15.2023.09.01 val PER: 0.0950
2026-01-04 13:54:03,552: t15.2023.09.03 val PER: 0.1817
2026-01-04 13:54:03,552: t15.2023.09.24 val PER: 0.1456
2026-01-04 13:54:03,552: t15.2023.09.29 val PER: 0.1506
2026-01-04 13:54:03,552: t15.2023.10.01 val PER: 0.1889
2026-01-04 13:54:03,553: t15.2023.10.06 val PER: 0.1023
2026-01-04 13:54:03,553: t15.2023.10.08 val PER: 0.2679
2026-01-04 13:54:03,554: t15.2023.10.13 val PER: 0.2296
2026-01-04 13:54:03,554: t15.2023.10.15 val PER: 0.1780
2026-01-04 13:54:03,554: t15.2023.10.20 val PER: 0.1946
2026-01-04 13:54:03,554: t15.2023.10.22 val PER: 0.1370
2026-01-04 13:54:03,554: t15.2023.11.03 val PER: 0.2083
2026-01-04 13:54:03,554: t15.2023.11.04 val PER: 0.0410
2026-01-04 13:54:03,554: t15.2023.11.17 val PER: 0.0638
2026-01-04 13:54:03,554: t15.2023.11.19 val PER: 0.0539
2026-01-04 13:54:03,554: t15.2023.11.26 val PER: 0.1652
2026-01-04 13:54:03,554: t15.2023.12.03 val PER: 0.1397
2026-01-04 13:54:03,555: t15.2023.12.08 val PER: 0.1272
2026-01-04 13:54:03,555: t15.2023.12.10 val PER: 0.1078
2026-01-04 13:54:03,555: t15.2023.12.17 val PER: 0.1674
2026-01-04 13:54:03,555: t15.2023.12.29 val PER: 0.1654
2026-01-04 13:54:03,555: t15.2024.02.25 val PER: 0.1419
2026-01-04 13:54:03,555: t15.2024.03.08 val PER: 0.2575
2026-01-04 13:54:03,555: t15.2024.03.15 val PER: 0.2301
2026-01-04 13:54:03,555: t15.2024.03.17 val PER: 0.1722
2026-01-04 13:54:03,555: t15.2024.05.10 val PER: 0.1783
2026-01-04 13:54:03,555: t15.2024.06.14 val PER: 0.1861
2026-01-04 13:54:03,555: t15.2024.07.19 val PER: 0.2742
2026-01-04 13:54:03,555: t15.2024.07.21 val PER: 0.1097
2026-01-04 13:54:03,555: t15.2024.07.28 val PER: 0.1537
2026-01-04 13:54:03,555: t15.2025.01.10 val PER: 0.3072
2026-01-04 13:54:03,555: t15.2025.01.12 val PER: 0.1747
2026-01-04 13:54:03,556: t15.2025.03.14 val PER: 0.3743
2026-01-04 13:54:03,556: t15.2025.03.16 val PER: 0.2173
2026-01-04 13:54:03,556: t15.2025.03.30 val PER: 0.3287
2026-01-04 13:54:03,556: t15.2025.04.13 val PER: 0.2425
2026-01-04 13:54:03,842: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_9000
2026-01-04 13:54:24,276: Train batch 9200: loss: 10.92 grad norm: 52.44 time: 0.057
2026-01-04 13:54:44,338: Train batch 9400: loss: 7.46 grad norm: 45.30 time: 0.070
2026-01-04 13:54:54,593: Running test after training batch: 9500
2026-01-04 13:54:54,702: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:55:00,173: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 13:55:00,208: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 13:55:02,128: Val batch 9500: PER (avg): 0.1725 CTC Loss (avg): 17.2117 WER(1gram): 48.73% (n=64) time: 7.534
2026-01-04 13:55:02,128: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-04 13:55:02,128: t15.2023.08.13 val PER: 0.1185
2026-01-04 13:55:02,128: t15.2023.08.18 val PER: 0.1182
2026-01-04 13:55:02,129: t15.2023.08.20 val PER: 0.1319
2026-01-04 13:55:02,129: t15.2023.08.25 val PER: 0.1024
2026-01-04 13:55:02,129: t15.2023.08.27 val PER: 0.1929
2026-01-04 13:55:02,129: t15.2023.09.01 val PER: 0.0950
2026-01-04 13:55:02,129: t15.2023.09.03 val PER: 0.1829
2026-01-04 13:55:02,129: t15.2023.09.24 val PER: 0.1408
2026-01-04 13:55:02,129: t15.2023.09.29 val PER: 0.1506
2026-01-04 13:55:02,129: t15.2023.10.01 val PER: 0.1929
2026-01-04 13:55:02,129: t15.2023.10.06 val PER: 0.0969
2026-01-04 13:55:02,129: t15.2023.10.08 val PER: 0.2693
2026-01-04 13:55:02,130: t15.2023.10.13 val PER: 0.2258
2026-01-04 13:55:02,130: t15.2023.10.15 val PER: 0.1852
2026-01-04 13:55:02,130: t15.2023.10.20 val PER: 0.1846
2026-01-04 13:55:02,130: t15.2023.10.22 val PER: 0.1258
2026-01-04 13:55:02,130: t15.2023.11.03 val PER: 0.1954
2026-01-04 13:55:02,130: t15.2023.11.04 val PER: 0.0307
2026-01-04 13:55:02,130: t15.2023.11.17 val PER: 0.0513
2026-01-04 13:55:02,130: t15.2023.11.19 val PER: 0.0519
2026-01-04 13:55:02,130: t15.2023.11.26 val PER: 0.1587
2026-01-04 13:55:02,130: t15.2023.12.03 val PER: 0.1513
2026-01-04 13:55:02,130: t15.2023.12.08 val PER: 0.1332
2026-01-04 13:55:02,130: t15.2023.12.10 val PER: 0.1170
2026-01-04 13:55:02,130: t15.2023.12.17 val PER: 0.1486
2026-01-04 13:55:02,131: t15.2023.12.29 val PER: 0.1565
2026-01-04 13:55:02,131: t15.2024.02.25 val PER: 0.1334
2026-01-04 13:55:02,131: t15.2024.03.08 val PER: 0.2518
2026-01-04 13:55:02,131: t15.2024.03.15 val PER: 0.2270
2026-01-04 13:55:02,131: t15.2024.03.17 val PER: 0.1590
2026-01-04 13:55:02,131: t15.2024.05.10 val PER: 0.1828
2026-01-04 13:55:02,131: t15.2024.06.14 val PER: 0.1814
2026-01-04 13:55:02,131: t15.2024.07.19 val PER: 0.2657
2026-01-04 13:55:02,131: t15.2024.07.21 val PER: 0.1172
2026-01-04 13:55:02,131: t15.2024.07.28 val PER: 0.1566
2026-01-04 13:55:02,131: t15.2025.01.10 val PER: 0.3264
2026-01-04 13:55:02,132: t15.2025.01.12 val PER: 0.1694
2026-01-04 13:55:02,132: t15.2025.03.14 val PER: 0.3743
2026-01-04 13:55:02,132: t15.2025.03.16 val PER: 0.2147
2026-01-04 13:55:02,132: t15.2025.03.30 val PER: 0.3299
2026-01-04 13:55:02,132: t15.2025.04.13 val PER: 0.2282
2026-01-04 13:55:02,133: New best val WER(1gram) 49.75% --> 48.73%
2026-01-04 13:55:02,134: Checkpointing model
2026-01-04 13:55:02,774: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:55:03,092: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_9500
2026-01-04 13:55:12,408: Train batch 9600: loss: 8.25 grad norm: 46.60 time: 0.073
2026-01-04 13:55:31,072: Train batch 9800: loss: 12.59 grad norm: 58.61 time: 0.063
2026-01-04 13:55:49,102: Train batch 10000: loss: 5.25 grad norm: 32.95 time: 0.062
2026-01-04 13:55:49,102: Running test after training batch: 10000
2026-01-04 13:55:49,518: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:55:54,509: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 13:55:54,540: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-04 13:55:56,303: Val batch 10000: PER (avg): 0.1713 CTC Loss (avg): 16.9050 WER(1gram): 51.27% (n=64) time: 7.200
2026-01-04 13:55:56,303: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 13:55:56,303: t15.2023.08.13 val PER: 0.1279
2026-01-04 13:55:56,303: t15.2023.08.18 val PER: 0.1249
2026-01-04 13:55:56,303: t15.2023.08.20 val PER: 0.1303
2026-01-04 13:55:56,304: t15.2023.08.25 val PER: 0.0979
2026-01-04 13:55:56,304: t15.2023.08.27 val PER: 0.2058
2026-01-04 13:55:56,304: t15.2023.09.01 val PER: 0.0909
2026-01-04 13:55:56,304: t15.2023.09.03 val PER: 0.1805
2026-01-04 13:55:56,304: t15.2023.09.24 val PER: 0.1493
2026-01-04 13:55:56,304: t15.2023.09.29 val PER: 0.1442
2026-01-04 13:55:56,304: t15.2023.10.01 val PER: 0.1856
2026-01-04 13:55:56,304: t15.2023.10.06 val PER: 0.1066
2026-01-04 13:55:56,304: t15.2023.10.08 val PER: 0.2625
2026-01-04 13:55:56,304: t15.2023.10.13 val PER: 0.2296
2026-01-04 13:55:56,304: t15.2023.10.15 val PER: 0.1747
2026-01-04 13:55:56,304: t15.2023.10.20 val PER: 0.1846
2026-01-04 13:55:56,305: t15.2023.10.22 val PER: 0.1281
2026-01-04 13:55:56,305: t15.2023.11.03 val PER: 0.1961
2026-01-04 13:55:56,305: t15.2023.11.04 val PER: 0.0307
2026-01-04 13:55:56,305: t15.2023.11.17 val PER: 0.0513
2026-01-04 13:55:56,305: t15.2023.11.19 val PER: 0.0419
2026-01-04 13:55:56,305: t15.2023.11.26 val PER: 0.1500
2026-01-04 13:55:56,305: t15.2023.12.03 val PER: 0.1366
2026-01-04 13:55:56,305: t15.2023.12.08 val PER: 0.1285
2026-01-04 13:55:56,305: t15.2023.12.10 val PER: 0.1170
2026-01-04 13:55:56,305: t15.2023.12.17 val PER: 0.1590
2026-01-04 13:55:56,305: t15.2023.12.29 val PER: 0.1489
2026-01-04 13:55:56,305: t15.2024.02.25 val PER: 0.1433
2026-01-04 13:55:56,305: t15.2024.03.08 val PER: 0.2432
2026-01-04 13:55:56,306: t15.2024.03.15 val PER: 0.2233
2026-01-04 13:55:56,306: t15.2024.03.17 val PER: 0.1520
2026-01-04 13:55:56,306: t15.2024.05.10 val PER: 0.1738
2026-01-04 13:55:56,306: t15.2024.06.14 val PER: 0.1893
2026-01-04 13:55:56,306: t15.2024.07.19 val PER: 0.2663
2026-01-04 13:55:56,306: t15.2024.07.21 val PER: 0.1200
2026-01-04 13:55:56,306: t15.2024.07.28 val PER: 0.1603
2026-01-04 13:55:56,306: t15.2025.01.10 val PER: 0.3140
2026-01-04 13:55:56,306: t15.2025.01.12 val PER: 0.1724
2026-01-04 13:55:56,306: t15.2025.03.14 val PER: 0.3639
2026-01-04 13:55:56,306: t15.2025.03.16 val PER: 0.2251
2026-01-04 13:55:56,306: t15.2025.03.30 val PER: 0.3333
2026-01-04 13:55:56,306: t15.2025.04.13 val PER: 0.2325
2026-01-04 13:55:56,595: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_10000
2026-01-04 13:56:16,152: Train batch 10200: loss: 6.48 grad norm: 40.82 time: 0.050
2026-01-04 13:56:36,123: Train batch 10400: loss: 9.24 grad norm: 52.69 time: 0.072
2026-01-04 13:56:45,813: Running test after training batch: 10500
2026-01-04 13:56:46,157: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:56:50,851: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 13:56:50,883: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-04 13:56:52,652: Val batch 10500: PER (avg): 0.1659 CTC Loss (avg): 16.6161 WER(1gram): 48.48% (n=64) time: 6.839
2026-01-04 13:56:52,653: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 13:56:52,653: t15.2023.08.13 val PER: 0.1310
2026-01-04 13:56:52,653: t15.2023.08.18 val PER: 0.1182
2026-01-04 13:56:52,653: t15.2023.08.20 val PER: 0.1199
2026-01-04 13:56:52,653: t15.2023.08.25 val PER: 0.1024
2026-01-04 13:56:52,653: t15.2023.08.27 val PER: 0.1961
2026-01-04 13:56:52,653: t15.2023.09.01 val PER: 0.0942
2026-01-04 13:56:52,654: t15.2023.09.03 val PER: 0.1734
2026-01-04 13:56:52,654: t15.2023.09.24 val PER: 0.1468
2026-01-04 13:56:52,654: t15.2023.09.29 val PER: 0.1468
2026-01-04 13:56:52,654: t15.2023.10.01 val PER: 0.1962
2026-01-04 13:56:52,654: t15.2023.10.06 val PER: 0.0958
2026-01-04 13:56:52,654: t15.2023.10.08 val PER: 0.2625
2026-01-04 13:56:52,654: t15.2023.10.13 val PER: 0.2087
2026-01-04 13:56:52,654: t15.2023.10.15 val PER: 0.1707
2026-01-04 13:56:52,654: t15.2023.10.20 val PER: 0.1711
2026-01-04 13:56:52,654: t15.2023.10.22 val PER: 0.1281
2026-01-04 13:56:52,654: t15.2023.11.03 val PER: 0.1961
2026-01-04 13:56:52,654: t15.2023.11.04 val PER: 0.0341
2026-01-04 13:56:52,655: t15.2023.11.17 val PER: 0.0467
2026-01-04 13:56:52,655: t15.2023.11.19 val PER: 0.0559
2026-01-04 13:56:52,655: t15.2023.11.26 val PER: 0.1341
2026-01-04 13:56:52,655: t15.2023.12.03 val PER: 0.1366
2026-01-04 13:56:52,655: t15.2023.12.08 val PER: 0.1265
2026-01-04 13:56:52,655: t15.2023.12.10 val PER: 0.1012
2026-01-04 13:56:52,655: t15.2023.12.17 val PER: 0.1403
2026-01-04 13:56:52,655: t15.2023.12.29 val PER: 0.1592
2026-01-04 13:56:52,656: t15.2024.02.25 val PER: 0.1194
2026-01-04 13:56:52,656: t15.2024.03.08 val PER: 0.2504
2026-01-04 13:56:52,656: t15.2024.03.15 val PER: 0.2195
2026-01-04 13:56:52,656: t15.2024.03.17 val PER: 0.1527
2026-01-04 13:56:52,656: t15.2024.05.10 val PER: 0.1724
2026-01-04 13:56:52,656: t15.2024.06.14 val PER: 0.1703
2026-01-04 13:56:52,656: t15.2024.07.19 val PER: 0.2610
2026-01-04 13:56:52,656: t15.2024.07.21 val PER: 0.1083
2026-01-04 13:56:52,656: t15.2024.07.28 val PER: 0.1331
2026-01-04 13:56:52,656: t15.2025.01.10 val PER: 0.3154
2026-01-04 13:56:52,656: t15.2025.01.12 val PER: 0.1640
2026-01-04 13:56:52,656: t15.2025.03.14 val PER: 0.3624
2026-01-04 13:56:52,656: t15.2025.03.16 val PER: 0.2120
2026-01-04 13:56:52,657: t15.2025.03.30 val PER: 0.3184
2026-01-04 13:56:52,657: t15.2025.04.13 val PER: 0.2168
2026-01-04 13:56:52,658: New best val WER(1gram) 48.73% --> 48.48%
2026-01-04 13:56:52,658: Checkpointing model
2026-01-04 13:56:53,272: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:56:53,592: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_10500
2026-01-04 13:57:03,795: Train batch 10600: loss: 9.41 grad norm: 54.60 time: 0.073
2026-01-04 13:57:22,537: Train batch 10800: loss: 14.76 grad norm: 64.41 time: 0.065
2026-01-04 13:57:42,077: Train batch 11000: loss: 14.42 grad norm: 61.54 time: 0.057
2026-01-04 13:57:42,077: Running test after training batch: 11000
2026-01-04 13:57:42,242: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:57:47,205: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 13:57:47,289: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 13:57:51,235: Val batch 11000: PER (avg): 0.1632 CTC Loss (avg): 16.4320 WER(1gram): 47.46% (n=64) time: 9.157
2026-01-04 13:57:51,235: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 13:57:51,235: t15.2023.08.13 val PER: 0.1185
2026-01-04 13:57:51,235: t15.2023.08.18 val PER: 0.1215
2026-01-04 13:57:51,236: t15.2023.08.20 val PER: 0.1183
2026-01-04 13:57:51,236: t15.2023.08.25 val PER: 0.0964
2026-01-04 13:57:51,236: t15.2023.08.27 val PER: 0.1849
2026-01-04 13:57:51,236: t15.2023.09.01 val PER: 0.0877
2026-01-04 13:57:51,236: t15.2023.09.03 val PER: 0.1770
2026-01-04 13:57:51,236: t15.2023.09.24 val PER: 0.1420
2026-01-04 13:57:51,236: t15.2023.09.29 val PER: 0.1398
2026-01-04 13:57:51,236: t15.2023.10.01 val PER: 0.1909
2026-01-04 13:57:51,236: t15.2023.10.06 val PER: 0.0926
2026-01-04 13:57:51,236: t15.2023.10.08 val PER: 0.2558
2026-01-04 13:57:51,236: t15.2023.10.13 val PER: 0.2219
2026-01-04 13:57:51,236: t15.2023.10.15 val PER: 0.1701
2026-01-04 13:57:51,237: t15.2023.10.20 val PER: 0.1980
2026-01-04 13:57:51,237: t15.2023.10.22 val PER: 0.1192
2026-01-04 13:57:51,237: t15.2023.11.03 val PER: 0.1845
2026-01-04 13:57:51,237: t15.2023.11.04 val PER: 0.0341
2026-01-04 13:57:51,237: t15.2023.11.17 val PER: 0.0467
2026-01-04 13:57:51,237: t15.2023.11.19 val PER: 0.0479
2026-01-04 13:57:51,237: t15.2023.11.26 val PER: 0.1370
2026-01-04 13:57:51,237: t15.2023.12.03 val PER: 0.1261
2026-01-04 13:57:51,237: t15.2023.12.08 val PER: 0.1238
2026-01-04 13:57:51,237: t15.2023.12.10 val PER: 0.0986
2026-01-04 13:57:51,237: t15.2023.12.17 val PER: 0.1538
2026-01-04 13:57:51,237: t15.2023.12.29 val PER: 0.1414
2026-01-04 13:57:51,238: t15.2024.02.25 val PER: 0.1334
2026-01-04 13:57:51,238: t15.2024.03.08 val PER: 0.2262
2026-01-04 13:57:51,238: t15.2024.03.15 val PER: 0.2164
2026-01-04 13:57:51,238: t15.2024.03.17 val PER: 0.1611
2026-01-04 13:57:51,238: t15.2024.05.10 val PER: 0.1842
2026-01-04 13:57:51,238: t15.2024.06.14 val PER: 0.1735
2026-01-04 13:57:51,239: t15.2024.07.19 val PER: 0.2518
2026-01-04 13:57:51,239: t15.2024.07.21 val PER: 0.1014
2026-01-04 13:57:51,239: t15.2024.07.28 val PER: 0.1441
2026-01-04 13:57:51,239: t15.2025.01.10 val PER: 0.3072
2026-01-04 13:57:51,239: t15.2025.01.12 val PER: 0.1578
2026-01-04 13:57:51,239: t15.2025.03.14 val PER: 0.3550
2026-01-04 13:57:51,239: t15.2025.03.16 val PER: 0.2042
2026-01-04 13:57:51,239: t15.2025.03.30 val PER: 0.3126
2026-01-04 13:57:51,239: t15.2025.04.13 val PER: 0.2225
2026-01-04 13:57:51,240: New best val WER(1gram) 48.48% --> 47.46%
2026-01-04 13:57:51,240: Checkpointing model
2026-01-04 13:57:51,912: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 13:57:52,225: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_11000
2026-01-04 13:58:11,839: Train batch 11200: loss: 10.48 grad norm: 52.55 time: 0.071
2026-01-04 13:58:31,450: Train batch 11400: loss: 9.34 grad norm: 51.36 time: 0.057
2026-01-04 13:58:41,353: Running test after training batch: 11500
2026-01-04 13:58:41,771: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:58:47,620: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 13:58:47,700: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost jett
2026-01-04 13:58:51,761: Val batch 11500: PER (avg): 0.1618 CTC Loss (avg): 16.3701 WER(1gram): 47.46% (n=64) time: 10.407
2026-01-04 13:58:51,761: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 13:58:51,761: t15.2023.08.13 val PER: 0.1216
2026-01-04 13:58:51,761: t15.2023.08.18 val PER: 0.1157
2026-01-04 13:58:51,762: t15.2023.08.20 val PER: 0.1199
2026-01-04 13:58:51,762: t15.2023.08.25 val PER: 0.1099
2026-01-04 13:58:51,762: t15.2023.08.27 val PER: 0.1945
2026-01-04 13:58:51,762: t15.2023.09.01 val PER: 0.0771
2026-01-04 13:58:51,762: t15.2023.09.03 val PER: 0.1675
2026-01-04 13:58:51,762: t15.2023.09.24 val PER: 0.1311
2026-01-04 13:58:51,762: t15.2023.09.29 val PER: 0.1366
2026-01-04 13:58:51,762: t15.2023.10.01 val PER: 0.1896
2026-01-04 13:58:51,763: t15.2023.10.06 val PER: 0.0850
2026-01-04 13:58:51,763: t15.2023.10.08 val PER: 0.2598
2026-01-04 13:58:51,763: t15.2023.10.13 val PER: 0.2118
2026-01-04 13:58:51,763: t15.2023.10.15 val PER: 0.1641
2026-01-04 13:58:51,763: t15.2023.10.20 val PER: 0.1879
2026-01-04 13:58:51,763: t15.2023.10.22 val PER: 0.1125
2026-01-04 13:58:51,763: t15.2023.11.03 val PER: 0.1832
2026-01-04 13:58:51,763: t15.2023.11.04 val PER: 0.0307
2026-01-04 13:58:51,763: t15.2023.11.17 val PER: 0.0373
2026-01-04 13:58:51,763: t15.2023.11.19 val PER: 0.0479
2026-01-04 13:58:51,763: t15.2023.11.26 val PER: 0.1261
2026-01-04 13:58:51,763: t15.2023.12.03 val PER: 0.1218
2026-01-04 13:58:51,763: t15.2023.12.08 val PER: 0.1132
2026-01-04 13:58:51,764: t15.2023.12.10 val PER: 0.1078
2026-01-04 13:58:51,764: t15.2023.12.17 val PER: 0.1466
2026-01-04 13:58:51,764: t15.2023.12.29 val PER: 0.1531
2026-01-04 13:58:51,764: t15.2024.02.25 val PER: 0.1166
2026-01-04 13:58:51,764: t15.2024.03.08 val PER: 0.2248
2026-01-04 13:58:51,764: t15.2024.03.15 val PER: 0.2089
2026-01-04 13:58:51,764: t15.2024.03.17 val PER: 0.1576
2026-01-04 13:58:51,764: t15.2024.05.10 val PER: 0.1842
2026-01-04 13:58:51,764: t15.2024.06.14 val PER: 0.1735
2026-01-04 13:58:51,764: t15.2024.07.19 val PER: 0.2538
2026-01-04 13:58:51,764: t15.2024.07.21 val PER: 0.1124
2026-01-04 13:58:51,764: t15.2024.07.28 val PER: 0.1500
2026-01-04 13:58:51,764: t15.2025.01.10 val PER: 0.3154
2026-01-04 13:58:51,765: t15.2025.01.12 val PER: 0.1609
2026-01-04 13:58:51,765: t15.2025.03.14 val PER: 0.3654
2026-01-04 13:58:51,765: t15.2025.03.16 val PER: 0.2186
2026-01-04 13:58:51,765: t15.2025.03.30 val PER: 0.3092
2026-01-04 13:58:51,765: t15.2025.04.13 val PER: 0.2311
2026-01-04 13:58:52,081: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_11500
2026-01-04 13:59:01,304: Train batch 11600: loss: 10.57 grad norm: 46.81 time: 0.060
2026-01-04 13:59:18,899: Train batch 11800: loss: 6.71 grad norm: 40.88 time: 0.045
2026-01-04 13:59:37,404: Train batch 12000: loss: 13.91 grad norm: 60.21 time: 0.071
2026-01-04 13:59:37,404: Running test after training batch: 12000
2026-01-04 13:59:37,504: WER debug GT example: You can see the code at this point as well.
2026-01-04 13:59:42,525: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 13:59:42,604: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 13:59:46,813: Val batch 12000: PER (avg): 0.1590 CTC Loss (avg): 16.1632 WER(1gram): 48.48% (n=64) time: 9.408
2026-01-04 13:59:46,813: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 13:59:46,813: t15.2023.08.13 val PER: 0.1154
2026-01-04 13:59:46,813: t15.2023.08.18 val PER: 0.1115
2026-01-04 13:59:46,814: t15.2023.08.20 val PER: 0.1215
2026-01-04 13:59:46,814: t15.2023.08.25 val PER: 0.0964
2026-01-04 13:59:46,814: t15.2023.08.27 val PER: 0.1913
2026-01-04 13:59:46,815: t15.2023.09.01 val PER: 0.0852
2026-01-04 13:59:46,815: t15.2023.09.03 val PER: 0.1627
2026-01-04 13:59:46,815: t15.2023.09.24 val PER: 0.1371
2026-01-04 13:59:46,815: t15.2023.09.29 val PER: 0.1378
2026-01-04 13:59:46,815: t15.2023.10.01 val PER: 0.1757
2026-01-04 13:59:46,815: t15.2023.10.06 val PER: 0.0926
2026-01-04 13:59:46,815: t15.2023.10.08 val PER: 0.2558
2026-01-04 13:59:46,815: t15.2023.10.13 val PER: 0.2118
2026-01-04 13:59:46,815: t15.2023.10.15 val PER: 0.1635
2026-01-04 13:59:46,815: t15.2023.10.20 val PER: 0.2114
2026-01-04 13:59:46,815: t15.2023.10.22 val PER: 0.1136
2026-01-04 13:59:46,815: t15.2023.11.03 val PER: 0.1832
2026-01-04 13:59:46,815: t15.2023.11.04 val PER: 0.0410
2026-01-04 13:59:46,816: t15.2023.11.17 val PER: 0.0373
2026-01-04 13:59:46,816: t15.2023.11.19 val PER: 0.0399
2026-01-04 13:59:46,816: t15.2023.11.26 val PER: 0.1232
2026-01-04 13:59:46,816: t15.2023.12.03 val PER: 0.1197
2026-01-04 13:59:46,816: t15.2023.12.08 val PER: 0.1105
2026-01-04 13:59:46,816: t15.2023.12.10 val PER: 0.0933
2026-01-04 13:59:46,816: t15.2023.12.17 val PER: 0.1424
2026-01-04 13:59:46,816: t15.2023.12.29 val PER: 0.1455
2026-01-04 13:59:46,816: t15.2024.02.25 val PER: 0.1222
2026-01-04 13:59:46,816: t15.2024.03.08 val PER: 0.2376
2026-01-04 13:59:46,816: t15.2024.03.15 val PER: 0.2083
2026-01-04 13:59:46,816: t15.2024.03.17 val PER: 0.1464
2026-01-04 13:59:46,817: t15.2024.05.10 val PER: 0.1783
2026-01-04 13:59:46,817: t15.2024.06.14 val PER: 0.1909
2026-01-04 13:59:46,817: t15.2024.07.19 val PER: 0.2465
2026-01-04 13:59:46,817: t15.2024.07.21 val PER: 0.1069
2026-01-04 13:59:46,817: t15.2024.07.28 val PER: 0.1397
2026-01-04 13:59:46,817: t15.2025.01.10 val PER: 0.3085
2026-01-04 13:59:46,817: t15.2025.01.12 val PER: 0.1517
2026-01-04 13:59:46,817: t15.2025.03.14 val PER: 0.3595
2026-01-04 13:59:46,817: t15.2025.03.16 val PER: 0.2094
2026-01-04 13:59:46,817: t15.2025.03.30 val PER: 0.3103
2026-01-04 13:59:46,817: t15.2025.04.13 val PER: 0.2254
2026-01-04 13:59:47,122: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_12000
2026-01-04 14:00:05,494: Train batch 12200: loss: 5.99 grad norm: 40.96 time: 0.066
2026-01-04 14:00:22,909: Train batch 12400: loss: 4.79 grad norm: 35.73 time: 0.041
2026-01-04 14:00:32,200: Running test after training batch: 12500
2026-01-04 14:00:32,353: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:00:37,676: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 14:00:37,756: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:00:42,119: Val batch 12500: PER (avg): 0.1568 CTC Loss (avg): 15.9493 WER(1gram): 46.95% (n=64) time: 9.918
2026-01-04 14:00:42,119: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 14:00:42,120: t15.2023.08.13 val PER: 0.1185
2026-01-04 14:00:42,120: t15.2023.08.18 val PER: 0.1115
2026-01-04 14:00:42,120: t15.2023.08.20 val PER: 0.1080
2026-01-04 14:00:42,120: t15.2023.08.25 val PER: 0.0919
2026-01-04 14:00:42,120: t15.2023.08.27 val PER: 0.1977
2026-01-04 14:00:42,120: t15.2023.09.01 val PER: 0.0779
2026-01-04 14:00:42,120: t15.2023.09.03 val PER: 0.1615
2026-01-04 14:00:42,120: t15.2023.09.24 val PER: 0.1347
2026-01-04 14:00:42,120: t15.2023.09.29 val PER: 0.1372
2026-01-04 14:00:42,120: t15.2023.10.01 val PER: 0.1777
2026-01-04 14:00:42,120: t15.2023.10.06 val PER: 0.0883
2026-01-04 14:00:42,121: t15.2023.10.08 val PER: 0.2598
2026-01-04 14:00:42,121: t15.2023.10.13 val PER: 0.2141
2026-01-04 14:00:42,121: t15.2023.10.15 val PER: 0.1575
2026-01-04 14:00:42,121: t15.2023.10.20 val PER: 0.1879
2026-01-04 14:00:42,121: t15.2023.10.22 val PER: 0.1147
2026-01-04 14:00:42,121: t15.2023.11.03 val PER: 0.1859
2026-01-04 14:00:42,121: t15.2023.11.04 val PER: 0.0307
2026-01-04 14:00:42,122: t15.2023.11.17 val PER: 0.0373
2026-01-04 14:00:42,122: t15.2023.11.19 val PER: 0.0339
2026-01-04 14:00:42,122: t15.2023.11.26 val PER: 0.1254
2026-01-04 14:00:42,122: t15.2023.12.03 val PER: 0.1155
2026-01-04 14:00:42,122: t15.2023.12.08 val PER: 0.1045
2026-01-04 14:00:42,122: t15.2023.12.10 val PER: 0.1025
2026-01-04 14:00:42,122: t15.2023.12.17 val PER: 0.1476
2026-01-04 14:00:42,123: t15.2023.12.29 val PER: 0.1469
2026-01-04 14:00:42,123: t15.2024.02.25 val PER: 0.1152
2026-01-04 14:00:42,123: t15.2024.03.08 val PER: 0.2376
2026-01-04 14:00:42,123: t15.2024.03.15 val PER: 0.2164
2026-01-04 14:00:42,123: t15.2024.03.17 val PER: 0.1527
2026-01-04 14:00:42,123: t15.2024.05.10 val PER: 0.1649
2026-01-04 14:00:42,123: t15.2024.06.14 val PER: 0.1703
2026-01-04 14:00:42,123: t15.2024.07.19 val PER: 0.2360
2026-01-04 14:00:42,123: t15.2024.07.21 val PER: 0.0966
2026-01-04 14:00:42,123: t15.2024.07.28 val PER: 0.1375
2026-01-04 14:00:42,123: t15.2025.01.10 val PER: 0.3154
2026-01-04 14:00:42,123: t15.2025.01.12 val PER: 0.1547
2026-01-04 14:00:42,124: t15.2025.03.14 val PER: 0.3550
2026-01-04 14:00:42,124: t15.2025.03.16 val PER: 0.2003
2026-01-04 14:00:42,124: t15.2025.03.30 val PER: 0.2897
2026-01-04 14:00:42,124: t15.2025.04.13 val PER: 0.2225
2026-01-04 14:00:42,124: New best val WER(1gram) 47.46% --> 46.95%
2026-01-04 14:00:42,124: Checkpointing model
2026-01-04 14:00:42,793: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 14:00:43,113: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_12500
2026-01-04 14:00:53,132: Train batch 12600: loss: 7.85 grad norm: 42.28 time: 0.058
2026-01-04 14:01:11,735: Train batch 12800: loss: 5.85 grad norm: 39.62 time: 0.052
2026-01-04 14:01:29,308: Train batch 13000: loss: 6.51 grad norm: 43.28 time: 0.066
2026-01-04 14:01:29,309: Running test after training batch: 13000
2026-01-04 14:01:29,423: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:01:34,414: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:01:34,492: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 14:01:38,803: Val batch 13000: PER (avg): 0.1552 CTC Loss (avg): 15.8217 WER(1gram): 45.43% (n=64) time: 9.494
2026-01-04 14:01:38,804: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 14:01:38,804: t15.2023.08.13 val PER: 0.1102
2026-01-04 14:01:38,804: t15.2023.08.18 val PER: 0.1157
2026-01-04 14:01:38,804: t15.2023.08.20 val PER: 0.1104
2026-01-04 14:01:38,804: t15.2023.08.25 val PER: 0.1009
2026-01-04 14:01:38,804: t15.2023.08.27 val PER: 0.1881
2026-01-04 14:01:38,804: t15.2023.09.01 val PER: 0.0804
2026-01-04 14:01:38,804: t15.2023.09.03 val PER: 0.1580
2026-01-04 14:01:38,804: t15.2023.09.24 val PER: 0.1347
2026-01-04 14:01:38,804: t15.2023.09.29 val PER: 0.1347
2026-01-04 14:01:38,805: t15.2023.10.01 val PER: 0.1684
2026-01-04 14:01:38,805: t15.2023.10.06 val PER: 0.0915
2026-01-04 14:01:38,805: t15.2023.10.08 val PER: 0.2585
2026-01-04 14:01:38,805: t15.2023.10.13 val PER: 0.2141
2026-01-04 14:01:38,805: t15.2023.10.15 val PER: 0.1602
2026-01-04 14:01:38,805: t15.2023.10.20 val PER: 0.1846
2026-01-04 14:01:38,805: t15.2023.10.22 val PER: 0.1069
2026-01-04 14:01:38,805: t15.2023.11.03 val PER: 0.1689
2026-01-04 14:01:38,805: t15.2023.11.04 val PER: 0.0273
2026-01-04 14:01:38,805: t15.2023.11.17 val PER: 0.0404
2026-01-04 14:01:38,805: t15.2023.11.19 val PER: 0.0439
2026-01-04 14:01:38,806: t15.2023.11.26 val PER: 0.1268
2026-01-04 14:01:38,806: t15.2023.12.03 val PER: 0.1208
2026-01-04 14:01:38,806: t15.2023.12.08 val PER: 0.1099
2026-01-04 14:01:38,806: t15.2023.12.10 val PER: 0.0959
2026-01-04 14:01:38,806: t15.2023.12.17 val PER: 0.1466
2026-01-04 14:01:38,806: t15.2023.12.29 val PER: 0.1448
2026-01-04 14:01:38,806: t15.2024.02.25 val PER: 0.1138
2026-01-04 14:01:38,807: t15.2024.03.08 val PER: 0.2376
2026-01-04 14:01:38,807: t15.2024.03.15 val PER: 0.2070
2026-01-04 14:01:38,807: t15.2024.03.17 val PER: 0.1464
2026-01-04 14:01:38,807: t15.2024.05.10 val PER: 0.1664
2026-01-04 14:01:38,807: t15.2024.06.14 val PER: 0.1640
2026-01-04 14:01:38,807: t15.2024.07.19 val PER: 0.2446
2026-01-04 14:01:38,807: t15.2024.07.21 val PER: 0.1034
2026-01-04 14:01:38,807: t15.2024.07.28 val PER: 0.1338
2026-01-04 14:01:38,807: t15.2025.01.10 val PER: 0.2948
2026-01-04 14:01:38,807: t15.2025.01.12 val PER: 0.1486
2026-01-04 14:01:38,807: t15.2025.03.14 val PER: 0.3462
2026-01-04 14:01:38,808: t15.2025.03.16 val PER: 0.1924
2026-01-04 14:01:38,808: t15.2025.03.30 val PER: 0.3057
2026-01-04 14:01:38,808: t15.2025.04.13 val PER: 0.2197
2026-01-04 14:01:38,809: New best val WER(1gram) 46.95% --> 45.43%
2026-01-04 14:01:38,809: Checkpointing model
2026-01-04 14:01:39,445: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 14:01:39,747: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_13000
2026-01-04 14:01:58,605: Train batch 13200: loss: 12.81 grad norm: 61.92 time: 0.054
2026-01-04 14:02:16,632: Train batch 13400: loss: 8.97 grad norm: 54.37 time: 0.061
2026-01-04 14:02:25,665: Running test after training batch: 13500
2026-01-04 14:02:25,963: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:02:30,959: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 14:02:31,038: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 14:02:35,499: Val batch 13500: PER (avg): 0.1540 CTC Loss (avg): 15.6230 WER(1gram): 47.46% (n=64) time: 9.833
2026-01-04 14:02:35,499: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 14:02:35,500: t15.2023.08.13 val PER: 0.1195
2026-01-04 14:02:35,500: t15.2023.08.18 val PER: 0.1065
2026-01-04 14:02:35,500: t15.2023.08.20 val PER: 0.1112
2026-01-04 14:02:35,500: t15.2023.08.25 val PER: 0.0919
2026-01-04 14:02:35,500: t15.2023.08.27 val PER: 0.1881
2026-01-04 14:02:35,500: t15.2023.09.01 val PER: 0.0812
2026-01-04 14:02:35,500: t15.2023.09.03 val PER: 0.1627
2026-01-04 14:02:35,501: t15.2023.09.24 val PER: 0.1347
2026-01-04 14:02:35,501: t15.2023.09.29 val PER: 0.1283
2026-01-04 14:02:35,501: t15.2023.10.01 val PER: 0.1856
2026-01-04 14:02:35,501: t15.2023.10.06 val PER: 0.0893
2026-01-04 14:02:35,501: t15.2023.10.08 val PER: 0.2571
2026-01-04 14:02:35,501: t15.2023.10.13 val PER: 0.2126
2026-01-04 14:02:35,501: t15.2023.10.15 val PER: 0.1569
2026-01-04 14:02:35,501: t15.2023.10.20 val PER: 0.1779
2026-01-04 14:02:35,501: t15.2023.10.22 val PER: 0.1091
2026-01-04 14:02:35,501: t15.2023.11.03 val PER: 0.1805
2026-01-04 14:02:35,501: t15.2023.11.04 val PER: 0.0307
2026-01-04 14:02:35,501: t15.2023.11.17 val PER: 0.0389
2026-01-04 14:02:35,502: t15.2023.11.19 val PER: 0.0359
2026-01-04 14:02:35,502: t15.2023.11.26 val PER: 0.1210
2026-01-04 14:02:35,502: t15.2023.12.03 val PER: 0.1197
2026-01-04 14:02:35,502: t15.2023.12.08 val PER: 0.1079
2026-01-04 14:02:35,502: t15.2023.12.10 val PER: 0.0959
2026-01-04 14:02:35,502: t15.2023.12.17 val PER: 0.1362
2026-01-04 14:02:35,502: t15.2023.12.29 val PER: 0.1393
2026-01-04 14:02:35,502: t15.2024.02.25 val PER: 0.1053
2026-01-04 14:02:35,502: t15.2024.03.08 val PER: 0.2290
2026-01-04 14:02:35,502: t15.2024.03.15 val PER: 0.2020
2026-01-04 14:02:35,502: t15.2024.03.17 val PER: 0.1492
2026-01-04 14:02:35,502: t15.2024.05.10 val PER: 0.1649
2026-01-04 14:02:35,502: t15.2024.06.14 val PER: 0.1514
2026-01-04 14:02:35,502: t15.2024.07.19 val PER: 0.2327
2026-01-04 14:02:35,502: t15.2024.07.21 val PER: 0.1055
2026-01-04 14:02:35,502: t15.2024.07.28 val PER: 0.1346
2026-01-04 14:02:35,503: t15.2025.01.10 val PER: 0.3003
2026-01-04 14:02:35,503: t15.2025.01.12 val PER: 0.1463
2026-01-04 14:02:35,503: t15.2025.03.14 val PER: 0.3506
2026-01-04 14:02:35,503: t15.2025.03.16 val PER: 0.1963
2026-01-04 14:02:35,503: t15.2025.03.30 val PER: 0.3034
2026-01-04 14:02:35,503: t15.2025.04.13 val PER: 0.2154
2026-01-04 14:02:35,811: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_13500
2026-01-04 14:02:44,808: Train batch 13600: loss: 12.31 grad norm: 61.23 time: 0.062
2026-01-04 14:03:02,862: Train batch 13800: loss: 9.10 grad norm: 55.32 time: 0.055
2026-01-04 14:03:20,635: Train batch 14000: loss: 11.77 grad norm: 57.99 time: 0.050
2026-01-04 14:03:20,636: Running test after training batch: 14000
2026-01-04 14:03:20,863: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:03:25,776: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 14:03:25,815: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:03:27,881: Val batch 14000: PER (avg): 0.1527 CTC Loss (avg): 15.5392 WER(1gram): 46.45% (n=64) time: 7.245
2026-01-04 14:03:27,882: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 14:03:27,882: t15.2023.08.13 val PER: 0.1091
2026-01-04 14:03:27,882: t15.2023.08.18 val PER: 0.1073
2026-01-04 14:03:27,886: t15.2023.08.20 val PER: 0.1072
2026-01-04 14:03:27,886: t15.2023.08.25 val PER: 0.0979
2026-01-04 14:03:27,886: t15.2023.08.27 val PER: 0.1929
2026-01-04 14:03:27,886: t15.2023.09.01 val PER: 0.0812
2026-01-04 14:03:27,886: t15.2023.09.03 val PER: 0.1722
2026-01-04 14:03:27,886: t15.2023.09.24 val PER: 0.1311
2026-01-04 14:03:27,886: t15.2023.09.29 val PER: 0.1327
2026-01-04 14:03:27,887: t15.2023.10.01 val PER: 0.1783
2026-01-04 14:03:27,887: t15.2023.10.06 val PER: 0.0861
2026-01-04 14:03:27,887: t15.2023.10.08 val PER: 0.2666
2026-01-04 14:03:27,887: t15.2023.10.13 val PER: 0.2126
2026-01-04 14:03:27,887: t15.2023.10.15 val PER: 0.1562
2026-01-04 14:03:27,887: t15.2023.10.20 val PER: 0.1913
2026-01-04 14:03:27,887: t15.2023.10.22 val PER: 0.1024
2026-01-04 14:03:27,887: t15.2023.11.03 val PER: 0.1791
2026-01-04 14:03:27,887: t15.2023.11.04 val PER: 0.0307
2026-01-04 14:03:27,887: t15.2023.11.17 val PER: 0.0327
2026-01-04 14:03:27,888: t15.2023.11.19 val PER: 0.0299
2026-01-04 14:03:27,888: t15.2023.11.26 val PER: 0.1210
2026-01-04 14:03:27,888: t15.2023.12.03 val PER: 0.1208
2026-01-04 14:03:27,888: t15.2023.12.08 val PER: 0.1039
2026-01-04 14:03:27,888: t15.2023.12.10 val PER: 0.0986
2026-01-04 14:03:27,888: t15.2023.12.17 val PER: 0.1299
2026-01-04 14:03:27,888: t15.2023.12.29 val PER: 0.1311
2026-01-04 14:03:27,888: t15.2024.02.25 val PER: 0.1138
2026-01-04 14:03:27,888: t15.2024.03.08 val PER: 0.2276
2026-01-04 14:03:27,888: t15.2024.03.15 val PER: 0.2020
2026-01-04 14:03:27,888: t15.2024.03.17 val PER: 0.1499
2026-01-04 14:03:27,888: t15.2024.05.10 val PER: 0.1545
2026-01-04 14:03:27,889: t15.2024.06.14 val PER: 0.1609
2026-01-04 14:03:27,889: t15.2024.07.19 val PER: 0.2347
2026-01-04 14:03:27,889: t15.2024.07.21 val PER: 0.0938
2026-01-04 14:03:27,889: t15.2024.07.28 val PER: 0.1338
2026-01-04 14:03:27,889: t15.2025.01.10 val PER: 0.3044
2026-01-04 14:03:27,889: t15.2025.01.12 val PER: 0.1447
2026-01-04 14:03:27,889: t15.2025.03.14 val PER: 0.3521
2026-01-04 14:03:27,889: t15.2025.03.16 val PER: 0.1872
2026-01-04 14:03:27,889: t15.2025.03.30 val PER: 0.2977
2026-01-04 14:03:27,889: t15.2025.04.13 val PER: 0.2197
2026-01-04 14:03:28,161: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_14000
2026-01-04 14:03:46,607: Train batch 14200: loss: 8.18 grad norm: 50.75 time: 0.057
2026-01-04 14:04:06,174: Train batch 14400: loss: 5.75 grad norm: 40.33 time: 0.063
2026-01-04 14:04:15,318: Running test after training batch: 14500
2026-01-04 14:04:15,419: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:04:20,513: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 14:04:20,604: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:04:25,336: Val batch 14500: PER (avg): 0.1527 CTC Loss (avg): 15.5986 WER(1gram): 47.21% (n=64) time: 10.017
2026-01-04 14:04:25,336: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 14:04:25,336: t15.2023.08.13 val PER: 0.1143
2026-01-04 14:04:25,337: t15.2023.08.18 val PER: 0.1090
2026-01-04 14:04:25,337: t15.2023.08.20 val PER: 0.1064
2026-01-04 14:04:25,337: t15.2023.08.25 val PER: 0.0934
2026-01-04 14:04:25,337: t15.2023.08.27 val PER: 0.1849
2026-01-04 14:04:25,337: t15.2023.09.01 val PER: 0.0771
2026-01-04 14:04:25,337: t15.2023.09.03 val PER: 0.1651
2026-01-04 14:04:25,337: t15.2023.09.24 val PER: 0.1311
2026-01-04 14:04:25,337: t15.2023.09.29 val PER: 0.1213
2026-01-04 14:04:25,338: t15.2023.10.01 val PER: 0.1849
2026-01-04 14:04:25,338: t15.2023.10.06 val PER: 0.0872
2026-01-04 14:04:25,338: t15.2023.10.08 val PER: 0.2625
2026-01-04 14:04:25,338: t15.2023.10.13 val PER: 0.2110
2026-01-04 14:04:25,338: t15.2023.10.15 val PER: 0.1549
2026-01-04 14:04:25,338: t15.2023.10.20 val PER: 0.1879
2026-01-04 14:04:25,338: t15.2023.10.22 val PER: 0.1069
2026-01-04 14:04:25,338: t15.2023.11.03 val PER: 0.1737
2026-01-04 14:04:25,338: t15.2023.11.04 val PER: 0.0341
2026-01-04 14:04:25,338: t15.2023.11.17 val PER: 0.0373
2026-01-04 14:04:25,338: t15.2023.11.19 val PER: 0.0299
2026-01-04 14:04:25,338: t15.2023.11.26 val PER: 0.1174
2026-01-04 14:04:25,339: t15.2023.12.03 val PER: 0.1124
2026-01-04 14:04:25,339: t15.2023.12.08 val PER: 0.0999
2026-01-04 14:04:25,339: t15.2023.12.10 val PER: 0.0959
2026-01-04 14:04:25,339: t15.2023.12.17 val PER: 0.1414
2026-01-04 14:04:25,339: t15.2023.12.29 val PER: 0.1345
2026-01-04 14:04:25,339: t15.2024.02.25 val PER: 0.1208
2026-01-04 14:04:25,339: t15.2024.03.08 val PER: 0.2333
2026-01-04 14:04:25,339: t15.2024.03.15 val PER: 0.2039
2026-01-04 14:04:25,339: t15.2024.03.17 val PER: 0.1513
2026-01-04 14:04:25,339: t15.2024.05.10 val PER: 0.1634
2026-01-04 14:04:25,339: t15.2024.06.14 val PER: 0.1656
2026-01-04 14:04:25,339: t15.2024.07.19 val PER: 0.2314
2026-01-04 14:04:25,339: t15.2024.07.21 val PER: 0.0972
2026-01-04 14:04:25,339: t15.2024.07.28 val PER: 0.1346
2026-01-04 14:04:25,339: t15.2025.01.10 val PER: 0.2906
2026-01-04 14:04:25,340: t15.2025.01.12 val PER: 0.1432
2026-01-04 14:04:25,340: t15.2025.03.14 val PER: 0.3698
2026-01-04 14:04:25,340: t15.2025.03.16 val PER: 0.1937
2026-01-04 14:04:25,340: t15.2025.03.30 val PER: 0.3034
2026-01-04 14:04:25,340: t15.2025.04.13 val PER: 0.2168
2026-01-04 14:04:25,636: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_14500
2026-01-04 14:04:35,581: Train batch 14600: loss: 12.11 grad norm: 61.22 time: 0.058
2026-01-04 14:04:54,838: Train batch 14800: loss: 5.56 grad norm: 40.99 time: 0.050
2026-01-04 14:05:13,840: Train batch 15000: loss: 8.77 grad norm: 50.34 time: 0.052
2026-01-04 14:05:13,841: Running test after training batch: 15000
2026-01-04 14:05:13,980: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:05:18,923: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:05:19,006: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:05:23,531: Val batch 15000: PER (avg): 0.1492 CTC Loss (avg): 15.3910 WER(1gram): 45.94% (n=64) time: 9.690
2026-01-04 14:05:23,531: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 14:05:23,531: t15.2023.08.13 val PER: 0.1133
2026-01-04 14:05:23,531: t15.2023.08.18 val PER: 0.1090
2026-01-04 14:05:23,531: t15.2023.08.20 val PER: 0.1088
2026-01-04 14:05:23,532: t15.2023.08.25 val PER: 0.1024
2026-01-04 14:05:23,532: t15.2023.08.27 val PER: 0.1817
2026-01-04 14:05:23,532: t15.2023.09.01 val PER: 0.0763
2026-01-04 14:05:23,532: t15.2023.09.03 val PER: 0.1496
2026-01-04 14:05:23,532: t15.2023.09.24 val PER: 0.1299
2026-01-04 14:05:23,532: t15.2023.09.29 val PER: 0.1289
2026-01-04 14:05:23,532: t15.2023.10.01 val PER: 0.1750
2026-01-04 14:05:23,532: t15.2023.10.06 val PER: 0.0861
2026-01-04 14:05:23,532: t15.2023.10.08 val PER: 0.2585
2026-01-04 14:05:23,532: t15.2023.10.13 val PER: 0.2048
2026-01-04 14:05:23,533: t15.2023.10.15 val PER: 0.1490
2026-01-04 14:05:23,533: t15.2023.10.20 val PER: 0.1745
2026-01-04 14:05:23,533: t15.2023.10.22 val PER: 0.1069
2026-01-04 14:05:23,533: t15.2023.11.03 val PER: 0.1764
2026-01-04 14:05:23,533: t15.2023.11.04 val PER: 0.0375
2026-01-04 14:05:23,533: t15.2023.11.17 val PER: 0.0327
2026-01-04 14:05:23,533: t15.2023.11.19 val PER: 0.0319
2026-01-04 14:05:23,533: t15.2023.11.26 val PER: 0.1196
2026-01-04 14:05:23,533: t15.2023.12.03 val PER: 0.1124
2026-01-04 14:05:23,533: t15.2023.12.08 val PER: 0.1019
2026-01-04 14:05:23,533: t15.2023.12.10 val PER: 0.0946
2026-01-04 14:05:23,533: t15.2023.12.17 val PER: 0.1414
2026-01-04 14:05:23,533: t15.2023.12.29 val PER: 0.1256
2026-01-04 14:05:23,533: t15.2024.02.25 val PER: 0.1067
2026-01-04 14:05:23,533: t15.2024.03.08 val PER: 0.2248
2026-01-04 14:05:23,533: t15.2024.03.15 val PER: 0.2020
2026-01-04 14:05:23,534: t15.2024.03.17 val PER: 0.1416
2026-01-04 14:05:23,534: t15.2024.05.10 val PER: 0.1560
2026-01-04 14:05:23,534: t15.2024.06.14 val PER: 0.1688
2026-01-04 14:05:23,534: t15.2024.07.19 val PER: 0.2294
2026-01-04 14:05:23,534: t15.2024.07.21 val PER: 0.0897
2026-01-04 14:05:23,534: t15.2024.07.28 val PER: 0.1279
2026-01-04 14:05:23,534: t15.2025.01.10 val PER: 0.3017
2026-01-04 14:05:23,534: t15.2025.01.12 val PER: 0.1378
2026-01-04 14:05:23,534: t15.2025.03.14 val PER: 0.3432
2026-01-04 14:05:23,534: t15.2025.03.16 val PER: 0.1872
2026-01-04 14:05:23,534: t15.2025.03.30 val PER: 0.2839
2026-01-04 14:05:23,534: t15.2025.04.13 val PER: 0.2111
2026-01-04 14:05:23,821: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_15000
2026-01-04 14:05:43,901: Train batch 15200: loss: 4.94 grad norm: 40.44 time: 0.057
2026-01-04 14:06:01,728: Train batch 15400: loss: 11.59 grad norm: 56.46 time: 0.050
2026-01-04 14:06:11,265: Running test after training batch: 15500
2026-01-04 14:06:11,384: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:06:16,298: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:06:16,387: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:06:21,018: Val batch 15500: PER (avg): 0.1492 CTC Loss (avg): 15.2804 WER(1gram): 45.43% (n=64) time: 9.753
2026-01-04 14:06:21,019: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 14:06:21,019: t15.2023.08.13 val PER: 0.1091
2026-01-04 14:06:21,019: t15.2023.08.18 val PER: 0.1039
2026-01-04 14:06:21,019: t15.2023.08.20 val PER: 0.1120
2026-01-04 14:06:21,019: t15.2023.08.25 val PER: 0.1024
2026-01-04 14:06:21,019: t15.2023.08.27 val PER: 0.1801
2026-01-04 14:06:21,020: t15.2023.09.01 val PER: 0.0755
2026-01-04 14:06:21,020: t15.2023.09.03 val PER: 0.1532
2026-01-04 14:06:21,020: t15.2023.09.24 val PER: 0.1250
2026-01-04 14:06:21,020: t15.2023.09.29 val PER: 0.1225
2026-01-04 14:06:21,020: t15.2023.10.01 val PER: 0.1731
2026-01-04 14:06:21,021: t15.2023.10.06 val PER: 0.0829
2026-01-04 14:06:21,021: t15.2023.10.08 val PER: 0.2598
2026-01-04 14:06:21,021: t15.2023.10.13 val PER: 0.1994
2026-01-04 14:06:21,021: t15.2023.10.15 val PER: 0.1463
2026-01-04 14:06:21,021: t15.2023.10.20 val PER: 0.1946
2026-01-04 14:06:21,021: t15.2023.10.22 val PER: 0.1147
2026-01-04 14:06:21,021: t15.2023.11.03 val PER: 0.1723
2026-01-04 14:06:21,021: t15.2023.11.04 val PER: 0.0341
2026-01-04 14:06:21,021: t15.2023.11.17 val PER: 0.0327
2026-01-04 14:06:21,021: t15.2023.11.19 val PER: 0.0359
2026-01-04 14:06:21,021: t15.2023.11.26 val PER: 0.1152
2026-01-04 14:06:21,022: t15.2023.12.03 val PER: 0.1071
2026-01-04 14:06:21,022: t15.2023.12.08 val PER: 0.1039
2026-01-04 14:06:21,022: t15.2023.12.10 val PER: 0.0894
2026-01-04 14:06:21,022: t15.2023.12.17 val PER: 0.1341
2026-01-04 14:06:21,022: t15.2023.12.29 val PER: 0.1311
2026-01-04 14:06:21,022: t15.2024.02.25 val PER: 0.1152
2026-01-04 14:06:21,022: t15.2024.03.08 val PER: 0.2347
2026-01-04 14:06:21,022: t15.2024.03.15 val PER: 0.2045
2026-01-04 14:06:21,022: t15.2024.03.17 val PER: 0.1437
2026-01-04 14:06:21,022: t15.2024.05.10 val PER: 0.1545
2026-01-04 14:06:21,022: t15.2024.06.14 val PER: 0.1672
2026-01-04 14:06:21,022: t15.2024.07.19 val PER: 0.2268
2026-01-04 14:06:21,022: t15.2024.07.21 val PER: 0.0938
2026-01-04 14:06:21,023: t15.2024.07.28 val PER: 0.1353
2026-01-04 14:06:21,023: t15.2025.01.10 val PER: 0.2906
2026-01-04 14:06:21,023: t15.2025.01.12 val PER: 0.1447
2026-01-04 14:06:21,023: t15.2025.03.14 val PER: 0.3447
2026-01-04 14:06:21,023: t15.2025.03.16 val PER: 0.1911
2026-01-04 14:06:21,023: t15.2025.03.30 val PER: 0.2885
2026-01-04 14:06:21,023: t15.2025.04.13 val PER: 0.2083
2026-01-04 14:06:21,312: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_15500
2026-01-04 14:06:31,279: Train batch 15600: loss: 11.47 grad norm: 56.99 time: 0.064
2026-01-04 14:06:51,499: Train batch 15800: loss: 13.07 grad norm: 61.79 time: 0.067
2026-01-04 14:07:11,377: Train batch 16000: loss: 8.49 grad norm: 45.48 time: 0.056
2026-01-04 14:07:11,378: Running test after training batch: 16000
2026-01-04 14:07:11,481: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:07:16,411: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:07:16,502: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:07:21,076: Val batch 16000: PER (avg): 0.1491 CTC Loss (avg): 15.3336 WER(1gram): 46.45% (n=64) time: 9.697
2026-01-04 14:07:21,076: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 14:07:21,076: t15.2023.08.13 val PER: 0.1164
2026-01-04 14:07:21,077: t15.2023.08.18 val PER: 0.1039
2026-01-04 14:07:21,077: t15.2023.08.20 val PER: 0.1080
2026-01-04 14:07:21,077: t15.2023.08.25 val PER: 0.0934
2026-01-04 14:07:21,077: t15.2023.08.27 val PER: 0.1833
2026-01-04 14:07:21,077: t15.2023.09.01 val PER: 0.0763
2026-01-04 14:07:21,077: t15.2023.09.03 val PER: 0.1568
2026-01-04 14:07:21,077: t15.2023.09.24 val PER: 0.1274
2026-01-04 14:07:21,077: t15.2023.09.29 val PER: 0.1302
2026-01-04 14:07:21,077: t15.2023.10.01 val PER: 0.1731
2026-01-04 14:07:21,077: t15.2023.10.06 val PER: 0.0861
2026-01-04 14:07:21,077: t15.2023.10.08 val PER: 0.2625
2026-01-04 14:07:21,077: t15.2023.10.13 val PER: 0.1932
2026-01-04 14:07:21,078: t15.2023.10.15 val PER: 0.1483
2026-01-04 14:07:21,078: t15.2023.10.20 val PER: 0.1913
2026-01-04 14:07:21,078: t15.2023.10.22 val PER: 0.1114
2026-01-04 14:07:21,078: t15.2023.11.03 val PER: 0.1710
2026-01-04 14:07:21,078: t15.2023.11.04 val PER: 0.0307
2026-01-04 14:07:21,078: t15.2023.11.17 val PER: 0.0404
2026-01-04 14:07:21,078: t15.2023.11.19 val PER: 0.0419
2026-01-04 14:07:21,078: t15.2023.11.26 val PER: 0.1109
2026-01-04 14:07:21,078: t15.2023.12.03 val PER: 0.1103
2026-01-04 14:07:21,078: t15.2023.12.08 val PER: 0.0999
2026-01-04 14:07:21,078: t15.2023.12.10 val PER: 0.0933
2026-01-04 14:07:21,078: t15.2023.12.17 val PER: 0.1237
2026-01-04 14:07:21,078: t15.2023.12.29 val PER: 0.1290
2026-01-04 14:07:21,079: t15.2024.02.25 val PER: 0.1039
2026-01-04 14:07:21,079: t15.2024.03.08 val PER: 0.2304
2026-01-04 14:07:21,079: t15.2024.03.15 val PER: 0.1989
2026-01-04 14:07:21,079: t15.2024.03.17 val PER: 0.1409
2026-01-04 14:07:21,082: t15.2024.05.10 val PER: 0.1679
2026-01-04 14:07:21,082: t15.2024.06.14 val PER: 0.1625
2026-01-04 14:07:21,082: t15.2024.07.19 val PER: 0.2393
2026-01-04 14:07:21,082: t15.2024.07.21 val PER: 0.0917
2026-01-04 14:07:21,082: t15.2024.07.28 val PER: 0.1294
2026-01-04 14:07:21,082: t15.2025.01.10 val PER: 0.2948
2026-01-04 14:07:21,082: t15.2025.01.12 val PER: 0.1440
2026-01-04 14:07:21,082: t15.2025.03.14 val PER: 0.3506
2026-01-04 14:07:21,082: t15.2025.03.16 val PER: 0.1950
2026-01-04 14:07:21,082: t15.2025.03.30 val PER: 0.2851
2026-01-04 14:07:21,083: t15.2025.04.13 val PER: 0.2126
2026-01-04 14:07:21,377: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_16000
2026-01-04 14:07:39,744: Train batch 16200: loss: 6.02 grad norm: 40.29 time: 0.055
2026-01-04 14:07:57,963: Train batch 16400: loss: 10.25 grad norm: 61.22 time: 0.057
2026-01-04 14:08:06,979: Running test after training batch: 16500
2026-01-04 14:08:07,109: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:08:11,939: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:08:11,986: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:08:14,472: Val batch 16500: PER (avg): 0.1475 CTC Loss (avg): 15.2255 WER(1gram): 45.18% (n=64) time: 7.492
2026-01-04 14:08:14,472: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 14:08:14,473: t15.2023.08.13 val PER: 0.1123
2026-01-04 14:08:14,473: t15.2023.08.18 val PER: 0.1031
2026-01-04 14:08:14,473: t15.2023.08.20 val PER: 0.1064
2026-01-04 14:08:14,473: t15.2023.08.25 val PER: 0.0858
2026-01-04 14:08:14,473: t15.2023.08.27 val PER: 0.1785
2026-01-04 14:08:14,473: t15.2023.09.01 val PER: 0.0771
2026-01-04 14:08:14,473: t15.2023.09.03 val PER: 0.1556
2026-01-04 14:08:14,473: t15.2023.09.24 val PER: 0.1250
2026-01-04 14:08:14,473: t15.2023.09.29 val PER: 0.1289
2026-01-04 14:08:14,473: t15.2023.10.01 val PER: 0.1744
2026-01-04 14:08:14,473: t15.2023.10.06 val PER: 0.0872
2026-01-04 14:08:14,474: t15.2023.10.08 val PER: 0.2544
2026-01-04 14:08:14,474: t15.2023.10.13 val PER: 0.1924
2026-01-04 14:08:14,474: t15.2023.10.15 val PER: 0.1477
2026-01-04 14:08:14,474: t15.2023.10.20 val PER: 0.1812
2026-01-04 14:08:14,474: t15.2023.10.22 val PER: 0.1069
2026-01-04 14:08:14,474: t15.2023.11.03 val PER: 0.1723
2026-01-04 14:08:14,474: t15.2023.11.04 val PER: 0.0341
2026-01-04 14:08:14,475: t15.2023.11.17 val PER: 0.0311
2026-01-04 14:08:14,475: t15.2023.11.19 val PER: 0.0359
2026-01-04 14:08:14,475: t15.2023.11.26 val PER: 0.1138
2026-01-04 14:08:14,475: t15.2023.12.03 val PER: 0.1092
2026-01-04 14:08:14,475: t15.2023.12.08 val PER: 0.0992
2026-01-04 14:08:14,475: t15.2023.12.10 val PER: 0.0867
2026-01-04 14:08:14,476: t15.2023.12.17 val PER: 0.1247
2026-01-04 14:08:14,476: t15.2023.12.29 val PER: 0.1277
2026-01-04 14:08:14,476: t15.2024.02.25 val PER: 0.1067
2026-01-04 14:08:14,476: t15.2024.03.08 val PER: 0.2333
2026-01-04 14:08:14,476: t15.2024.03.15 val PER: 0.2008
2026-01-04 14:08:14,476: t15.2024.03.17 val PER: 0.1395
2026-01-04 14:08:14,476: t15.2024.05.10 val PER: 0.1486
2026-01-04 14:08:14,476: t15.2024.06.14 val PER: 0.1640
2026-01-04 14:08:14,476: t15.2024.07.19 val PER: 0.2367
2026-01-04 14:08:14,476: t15.2024.07.21 val PER: 0.0848
2026-01-04 14:08:14,476: t15.2024.07.28 val PER: 0.1346
2026-01-04 14:08:14,477: t15.2025.01.10 val PER: 0.2837
2026-01-04 14:08:14,477: t15.2025.01.12 val PER: 0.1401
2026-01-04 14:08:14,477: t15.2025.03.14 val PER: 0.3550
2026-01-04 14:08:14,477: t15.2025.03.16 val PER: 0.1924
2026-01-04 14:08:14,477: t15.2025.03.30 val PER: 0.2885
2026-01-04 14:08:14,477: t15.2025.04.13 val PER: 0.2068
2026-01-04 14:08:14,477: New best val WER(1gram) 45.43% --> 45.18%
2026-01-04 14:08:14,477: Checkpointing model
2026-01-04 14:08:15,110: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/best_checkpoint
2026-01-04 14:08:15,410: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_16500
2026-01-04 14:08:24,217: Train batch 16600: loss: 8.69 grad norm: 53.72 time: 0.052
2026-01-04 14:08:41,684: Train batch 16800: loss: 16.06 grad norm: 74.19 time: 0.061
2026-01-04 14:09:01,031: Train batch 17000: loss: 7.81 grad norm: 47.99 time: 0.082
2026-01-04 14:09:01,032: Running test after training batch: 17000
2026-01-04 14:09:01,133: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:09:05,941: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:09:05,988: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:09:08,407: Val batch 17000: PER (avg): 0.1470 CTC Loss (avg): 15.1109 WER(1gram): 47.21% (n=64) time: 7.375
2026-01-04 14:09:08,407: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 14:09:08,408: t15.2023.08.13 val PER: 0.1123
2026-01-04 14:09:08,408: t15.2023.08.18 val PER: 0.1081
2026-01-04 14:09:08,408: t15.2023.08.20 val PER: 0.1041
2026-01-04 14:09:08,408: t15.2023.08.25 val PER: 0.0889
2026-01-04 14:09:08,408: t15.2023.08.27 val PER: 0.1768
2026-01-04 14:09:08,408: t15.2023.09.01 val PER: 0.0731
2026-01-04 14:09:08,408: t15.2023.09.03 val PER: 0.1556
2026-01-04 14:09:08,408: t15.2023.09.24 val PER: 0.1262
2026-01-04 14:09:08,408: t15.2023.09.29 val PER: 0.1302
2026-01-04 14:09:08,409: t15.2023.10.01 val PER: 0.1684
2026-01-04 14:09:08,409: t15.2023.10.06 val PER: 0.0807
2026-01-04 14:09:08,409: t15.2023.10.08 val PER: 0.2476
2026-01-04 14:09:08,409: t15.2023.10.13 val PER: 0.1939
2026-01-04 14:09:08,409: t15.2023.10.15 val PER: 0.1457
2026-01-04 14:09:08,409: t15.2023.10.20 val PER: 0.1745
2026-01-04 14:09:08,409: t15.2023.10.22 val PER: 0.1024
2026-01-04 14:09:08,409: t15.2023.11.03 val PER: 0.1771
2026-01-04 14:09:08,409: t15.2023.11.04 val PER: 0.0273
2026-01-04 14:09:08,409: t15.2023.11.17 val PER: 0.0342
2026-01-04 14:09:08,410: t15.2023.11.19 val PER: 0.0379
2026-01-04 14:09:08,410: t15.2023.11.26 val PER: 0.1101
2026-01-04 14:09:08,410: t15.2023.12.03 val PER: 0.1092
2026-01-04 14:09:08,410: t15.2023.12.08 val PER: 0.0959
2026-01-04 14:09:08,410: t15.2023.12.10 val PER: 0.0867
2026-01-04 14:09:08,410: t15.2023.12.17 val PER: 0.1247
2026-01-04 14:09:08,410: t15.2023.12.29 val PER: 0.1304
2026-01-04 14:09:08,410: t15.2024.02.25 val PER: 0.1138
2026-01-04 14:09:08,410: t15.2024.03.08 val PER: 0.2276
2026-01-04 14:09:08,411: t15.2024.03.15 val PER: 0.2001
2026-01-04 14:09:08,411: t15.2024.03.17 val PER: 0.1367
2026-01-04 14:09:08,411: t15.2024.05.10 val PER: 0.1620
2026-01-04 14:09:08,411: t15.2024.06.14 val PER: 0.1609
2026-01-04 14:09:08,411: t15.2024.07.19 val PER: 0.2334
2026-01-04 14:09:08,411: t15.2024.07.21 val PER: 0.0897
2026-01-04 14:09:08,411: t15.2024.07.28 val PER: 0.1324
2026-01-04 14:09:08,411: t15.2025.01.10 val PER: 0.2906
2026-01-04 14:09:08,411: t15.2025.01.12 val PER: 0.1393
2026-01-04 14:09:08,411: t15.2025.03.14 val PER: 0.3491
2026-01-04 14:09:08,411: t15.2025.03.16 val PER: 0.1898
2026-01-04 14:09:08,411: t15.2025.03.30 val PER: 0.2862
2026-01-04 14:09:08,411: t15.2025.04.13 val PER: 0.2140
2026-01-04 14:09:08,686: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_17000
2026-01-04 14:09:26,854: Train batch 17200: loss: 9.22 grad norm: 46.98 time: 0.083
2026-01-04 14:09:44,296: Train batch 17400: loss: 11.87 grad norm: 61.84 time: 0.070
2026-01-04 14:09:52,867: Running test after training batch: 17500
2026-01-04 14:09:53,265: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:09:58,132: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:09:58,174: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:10:00,399: Val batch 17500: PER (avg): 0.1462 CTC Loss (avg): 15.1093 WER(1gram): 46.95% (n=64) time: 7.531
2026-01-04 14:10:00,400: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 14:10:00,400: t15.2023.08.13 val PER: 0.1143
2026-01-04 14:10:00,400: t15.2023.08.18 val PER: 0.1090
2026-01-04 14:10:00,400: t15.2023.08.20 val PER: 0.1056
2026-01-04 14:10:00,400: t15.2023.08.25 val PER: 0.0949
2026-01-04 14:10:00,400: t15.2023.08.27 val PER: 0.1736
2026-01-04 14:10:00,400: t15.2023.09.01 val PER: 0.0722
2026-01-04 14:10:00,401: t15.2023.09.03 val PER: 0.1556
2026-01-04 14:10:00,401: t15.2023.09.24 val PER: 0.1274
2026-01-04 14:10:00,401: t15.2023.09.29 val PER: 0.1283
2026-01-04 14:10:00,401: t15.2023.10.01 val PER: 0.1711
2026-01-04 14:10:00,401: t15.2023.10.06 val PER: 0.0807
2026-01-04 14:10:00,401: t15.2023.10.08 val PER: 0.2598
2026-01-04 14:10:00,401: t15.2023.10.13 val PER: 0.1901
2026-01-04 14:10:00,401: t15.2023.10.15 val PER: 0.1457
2026-01-04 14:10:00,401: t15.2023.10.20 val PER: 0.1913
2026-01-04 14:10:00,401: t15.2023.10.22 val PER: 0.1047
2026-01-04 14:10:00,401: t15.2023.11.03 val PER: 0.1777
2026-01-04 14:10:00,402: t15.2023.11.04 val PER: 0.0341
2026-01-04 14:10:00,402: t15.2023.11.17 val PER: 0.0327
2026-01-04 14:10:00,402: t15.2023.11.19 val PER: 0.0379
2026-01-04 14:10:00,402: t15.2023.11.26 val PER: 0.1116
2026-01-04 14:10:00,402: t15.2023.12.03 val PER: 0.1071
2026-01-04 14:10:00,402: t15.2023.12.08 val PER: 0.0952
2026-01-04 14:10:00,402: t15.2023.12.10 val PER: 0.0894
2026-01-04 14:10:00,402: t15.2023.12.17 val PER: 0.1268
2026-01-04 14:10:00,402: t15.2023.12.29 val PER: 0.1290
2026-01-04 14:10:00,402: t15.2024.02.25 val PER: 0.1053
2026-01-04 14:10:00,402: t15.2024.03.08 val PER: 0.2233
2026-01-04 14:10:00,402: t15.2024.03.15 val PER: 0.1957
2026-01-04 14:10:00,402: t15.2024.03.17 val PER: 0.1360
2026-01-04 14:10:00,402: t15.2024.05.10 val PER: 0.1516
2026-01-04 14:10:00,402: t15.2024.06.14 val PER: 0.1577
2026-01-04 14:10:00,403: t15.2024.07.19 val PER: 0.2254
2026-01-04 14:10:00,403: t15.2024.07.21 val PER: 0.0910
2026-01-04 14:10:00,403: t15.2024.07.28 val PER: 0.1279
2026-01-04 14:10:00,403: t15.2025.01.10 val PER: 0.2975
2026-01-04 14:10:00,403: t15.2025.01.12 val PER: 0.1332
2026-01-04 14:10:00,403: t15.2025.03.14 val PER: 0.3506
2026-01-04 14:10:00,404: t15.2025.03.16 val PER: 0.1911
2026-01-04 14:10:00,404: t15.2025.03.30 val PER: 0.2770
2026-01-04 14:10:00,404: t15.2025.04.13 val PER: 0.2097
2026-01-04 14:10:00,683: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_17500
2026-01-04 14:10:09,950: Train batch 17600: loss: 9.46 grad norm: 56.50 time: 0.050
2026-01-04 14:10:27,608: Train batch 17800: loss: 6.03 grad norm: 48.46 time: 0.041
2026-01-04 14:10:45,578: Train batch 18000: loss: 10.87 grad norm: 64.10 time: 0.061
2026-01-04 14:10:45,578: Running test after training batch: 18000
2026-01-04 14:10:45,776: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:10:50,512: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:10:50,548: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:10:52,480: Val batch 18000: PER (avg): 0.1457 CTC Loss (avg): 15.0782 WER(1gram): 45.18% (n=64) time: 6.902
2026-01-04 14:10:52,481: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 14:10:52,481: t15.2023.08.13 val PER: 0.1143
2026-01-04 14:10:52,481: t15.2023.08.18 val PER: 0.1123
2026-01-04 14:10:52,481: t15.2023.08.20 val PER: 0.1064
2026-01-04 14:10:52,481: t15.2023.08.25 val PER: 0.0904
2026-01-04 14:10:52,481: t15.2023.08.27 val PER: 0.1768
2026-01-04 14:10:52,482: t15.2023.09.01 val PER: 0.0706
2026-01-04 14:10:52,482: t15.2023.09.03 val PER: 0.1556
2026-01-04 14:10:52,482: t15.2023.09.24 val PER: 0.1286
2026-01-04 14:10:52,482: t15.2023.09.29 val PER: 0.1289
2026-01-04 14:10:52,482: t15.2023.10.01 val PER: 0.1697
2026-01-04 14:10:52,482: t15.2023.10.06 val PER: 0.0840
2026-01-04 14:10:52,482: t15.2023.10.08 val PER: 0.2571
2026-01-04 14:10:52,482: t15.2023.10.13 val PER: 0.1924
2026-01-04 14:10:52,482: t15.2023.10.15 val PER: 0.1503
2026-01-04 14:10:52,482: t15.2023.10.20 val PER: 0.1879
2026-01-04 14:10:52,482: t15.2023.10.22 val PER: 0.1024
2026-01-04 14:10:52,482: t15.2023.11.03 val PER: 0.1737
2026-01-04 14:10:52,482: t15.2023.11.04 val PER: 0.0273
2026-01-04 14:10:52,483: t15.2023.11.17 val PER: 0.0295
2026-01-04 14:10:52,483: t15.2023.11.19 val PER: 0.0379
2026-01-04 14:10:52,483: t15.2023.11.26 val PER: 0.1065
2026-01-04 14:10:52,483: t15.2023.12.03 val PER: 0.1082
2026-01-04 14:10:52,483: t15.2023.12.08 val PER: 0.0979
2026-01-04 14:10:52,483: t15.2023.12.10 val PER: 0.0841
2026-01-04 14:10:52,483: t15.2023.12.17 val PER: 0.1227
2026-01-04 14:10:52,483: t15.2023.12.29 val PER: 0.1249
2026-01-04 14:10:52,483: t15.2024.02.25 val PER: 0.1011
2026-01-04 14:10:52,483: t15.2024.03.08 val PER: 0.2262
2026-01-04 14:10:52,483: t15.2024.03.15 val PER: 0.1951
2026-01-04 14:10:52,483: t15.2024.03.17 val PER: 0.1374
2026-01-04 14:10:52,484: t15.2024.05.10 val PER: 0.1501
2026-01-04 14:10:52,484: t15.2024.06.14 val PER: 0.1593
2026-01-04 14:10:52,484: t15.2024.07.19 val PER: 0.2254
2026-01-04 14:10:52,484: t15.2024.07.21 val PER: 0.0910
2026-01-04 14:10:52,484: t15.2024.07.28 val PER: 0.1316
2026-01-04 14:10:52,484: t15.2025.01.10 val PER: 0.2934
2026-01-04 14:10:52,484: t15.2025.01.12 val PER: 0.1355
2026-01-04 14:10:52,484: t15.2025.03.14 val PER: 0.3299
2026-01-04 14:10:52,484: t15.2025.03.16 val PER: 0.1846
2026-01-04 14:10:52,484: t15.2025.03.30 val PER: 0.2874
2026-01-04 14:10:52,484: t15.2025.04.13 val PER: 0.2083
2026-01-04 14:10:52,755: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_18000
2026-01-04 14:11:11,533: Train batch 18200: loss: 7.20 grad norm: 45.23 time: 0.074
2026-01-04 14:11:29,640: Train batch 18400: loss: 4.37 grad norm: 38.42 time: 0.058
2026-01-04 14:11:39,268: Running test after training batch: 18500
2026-01-04 14:11:39,370: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:11:44,050: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:11:44,085: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:11:46,353: Val batch 18500: PER (avg): 0.1459 CTC Loss (avg): 15.0804 WER(1gram): 45.94% (n=64) time: 7.084
2026-01-04 14:11:46,353: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 14:11:46,354: t15.2023.08.13 val PER: 0.1102
2026-01-04 14:11:46,354: t15.2023.08.18 val PER: 0.1098
2026-01-04 14:11:46,354: t15.2023.08.20 val PER: 0.1104
2026-01-04 14:11:46,354: t15.2023.08.25 val PER: 0.0889
2026-01-04 14:11:46,354: t15.2023.08.27 val PER: 0.1768
2026-01-04 14:11:46,355: t15.2023.09.01 val PER: 0.0714
2026-01-04 14:11:46,355: t15.2023.09.03 val PER: 0.1556
2026-01-04 14:11:46,355: t15.2023.09.24 val PER: 0.1311
2026-01-04 14:11:46,355: t15.2023.09.29 val PER: 0.1264
2026-01-04 14:11:46,355: t15.2023.10.01 val PER: 0.1704
2026-01-04 14:11:46,355: t15.2023.10.06 val PER: 0.0829
2026-01-04 14:11:46,355: t15.2023.10.08 val PER: 0.2598
2026-01-04 14:11:46,355: t15.2023.10.13 val PER: 0.1947
2026-01-04 14:11:46,355: t15.2023.10.15 val PER: 0.1463
2026-01-04 14:11:46,355: t15.2023.10.20 val PER: 0.1879
2026-01-04 14:11:46,356: t15.2023.10.22 val PER: 0.1024
2026-01-04 14:11:46,356: t15.2023.11.03 val PER: 0.1716
2026-01-04 14:11:46,356: t15.2023.11.04 val PER: 0.0341
2026-01-04 14:11:46,356: t15.2023.11.17 val PER: 0.0295
2026-01-04 14:11:46,356: t15.2023.11.19 val PER: 0.0359
2026-01-04 14:11:46,356: t15.2023.11.26 val PER: 0.1109
2026-01-04 14:11:46,356: t15.2023.12.03 val PER: 0.1050
2026-01-04 14:11:46,356: t15.2023.12.08 val PER: 0.0919
2026-01-04 14:11:46,356: t15.2023.12.10 val PER: 0.0907
2026-01-04 14:11:46,356: t15.2023.12.17 val PER: 0.1247
2026-01-04 14:11:46,356: t15.2023.12.29 val PER: 0.1297
2026-01-04 14:11:46,356: t15.2024.02.25 val PER: 0.1039
2026-01-04 14:11:46,356: t15.2024.03.08 val PER: 0.2248
2026-01-04 14:11:46,356: t15.2024.03.15 val PER: 0.2001
2026-01-04 14:11:46,357: t15.2024.03.17 val PER: 0.1374
2026-01-04 14:11:46,357: t15.2024.05.10 val PER: 0.1456
2026-01-04 14:11:46,357: t15.2024.06.14 val PER: 0.1577
2026-01-04 14:11:46,357: t15.2024.07.19 val PER: 0.2314
2026-01-04 14:11:46,357: t15.2024.07.21 val PER: 0.0883
2026-01-04 14:11:46,357: t15.2024.07.28 val PER: 0.1331
2026-01-04 14:11:46,357: t15.2025.01.10 val PER: 0.2879
2026-01-04 14:11:46,357: t15.2025.01.12 val PER: 0.1370
2026-01-04 14:11:46,357: t15.2025.03.14 val PER: 0.3358
2026-01-04 14:11:46,357: t15.2025.03.16 val PER: 0.1898
2026-01-04 14:11:46,357: t15.2025.03.30 val PER: 0.2793
2026-01-04 14:11:46,357: t15.2025.04.13 val PER: 0.2054
2026-01-04 14:11:46,646: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_18500
2026-01-04 14:11:55,289: Train batch 18600: loss: 12.18 grad norm: 61.49 time: 0.067
2026-01-04 14:12:13,847: Train batch 18800: loss: 7.80 grad norm: 49.28 time: 0.065
2026-01-04 14:12:31,363: Train batch 19000: loss: 8.02 grad norm: 44.54 time: 0.064
2026-01-04 14:12:31,363: Running test after training batch: 19000
2026-01-04 14:12:31,493: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:12:36,234: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:12:36,276: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:12:38,507: Val batch 19000: PER (avg): 0.1462 CTC Loss (avg): 15.0617 WER(1gram): 45.43% (n=64) time: 7.144
2026-01-04 14:12:38,508: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 14:12:38,508: t15.2023.08.13 val PER: 0.1112
2026-01-04 14:12:38,508: t15.2023.08.18 val PER: 0.1081
2026-01-04 14:12:38,508: t15.2023.08.20 val PER: 0.1056
2026-01-04 14:12:38,508: t15.2023.08.25 val PER: 0.0949
2026-01-04 14:12:38,508: t15.2023.08.27 val PER: 0.1688
2026-01-04 14:12:38,508: t15.2023.09.01 val PER: 0.0747
2026-01-04 14:12:38,508: t15.2023.09.03 val PER: 0.1532
2026-01-04 14:12:38,508: t15.2023.09.24 val PER: 0.1311
2026-01-04 14:12:38,509: t15.2023.09.29 val PER: 0.1238
2026-01-04 14:12:38,509: t15.2023.10.01 val PER: 0.1671
2026-01-04 14:12:38,509: t15.2023.10.06 val PER: 0.0840
2026-01-04 14:12:38,509: t15.2023.10.08 val PER: 0.2530
2026-01-04 14:12:38,509: t15.2023.10.13 val PER: 0.1916
2026-01-04 14:12:38,509: t15.2023.10.15 val PER: 0.1450
2026-01-04 14:12:38,509: t15.2023.10.20 val PER: 0.1913
2026-01-04 14:12:38,509: t15.2023.10.22 val PER: 0.1036
2026-01-04 14:12:38,509: t15.2023.11.03 val PER: 0.1777
2026-01-04 14:12:38,510: t15.2023.11.04 val PER: 0.0273
2026-01-04 14:12:38,510: t15.2023.11.17 val PER: 0.0280
2026-01-04 14:12:38,510: t15.2023.11.19 val PER: 0.0359
2026-01-04 14:12:38,510: t15.2023.11.26 val PER: 0.1138
2026-01-04 14:12:38,510: t15.2023.12.03 val PER: 0.1145
2026-01-04 14:12:38,510: t15.2023.12.08 val PER: 0.0919
2026-01-04 14:12:38,510: t15.2023.12.10 val PER: 0.0880
2026-01-04 14:12:38,510: t15.2023.12.17 val PER: 0.1268
2026-01-04 14:12:38,510: t15.2023.12.29 val PER: 0.1290
2026-01-04 14:12:38,510: t15.2024.02.25 val PER: 0.1011
2026-01-04 14:12:38,510: t15.2024.03.08 val PER: 0.2290
2026-01-04 14:12:38,510: t15.2024.03.15 val PER: 0.1995
2026-01-04 14:12:38,511: t15.2024.03.17 val PER: 0.1353
2026-01-04 14:12:38,511: t15.2024.05.10 val PER: 0.1486
2026-01-04 14:12:38,511: t15.2024.06.14 val PER: 0.1735
2026-01-04 14:12:38,511: t15.2024.07.19 val PER: 0.2334
2026-01-04 14:12:38,511: t15.2024.07.21 val PER: 0.0890
2026-01-04 14:12:38,511: t15.2024.07.28 val PER: 0.1301
2026-01-04 14:12:38,511: t15.2025.01.10 val PER: 0.2851
2026-01-04 14:12:38,511: t15.2025.01.12 val PER: 0.1355
2026-01-04 14:12:38,511: t15.2025.03.14 val PER: 0.3506
2026-01-04 14:12:38,511: t15.2025.03.16 val PER: 0.1898
2026-01-04 14:12:38,511: t15.2025.03.30 val PER: 0.2793
2026-01-04 14:12:38,511: t15.2025.04.13 val PER: 0.2083
2026-01-04 14:12:38,794: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_19000
2026-01-04 14:12:56,644: Train batch 19200: loss: 5.58 grad norm: 46.81 time: 0.063
2026-01-04 14:13:15,151: Train batch 19400: loss: 4.72 grad norm: 36.08 time: 0.053
2026-01-04 14:13:24,150: Running test after training batch: 19500
2026-01-04 14:13:24,359: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:13:28,979: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:13:29,015: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:13:30,967: Val batch 19500: PER (avg): 0.1455 CTC Loss (avg): 15.0314 WER(1gram): 45.69% (n=64) time: 6.817
2026-01-04 14:13:30,967: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 14:13:30,967: t15.2023.08.13 val PER: 0.1123
2026-01-04 14:13:30,967: t15.2023.08.18 val PER: 0.1048
2026-01-04 14:13:30,968: t15.2023.08.20 val PER: 0.1025
2026-01-04 14:13:30,968: t15.2023.08.25 val PER: 0.0904
2026-01-04 14:13:30,968: t15.2023.08.27 val PER: 0.1672
2026-01-04 14:13:30,968: t15.2023.09.01 val PER: 0.0722
2026-01-04 14:13:30,968: t15.2023.09.03 val PER: 0.1508
2026-01-04 14:13:30,968: t15.2023.09.24 val PER: 0.1335
2026-01-04 14:13:30,968: t15.2023.09.29 val PER: 0.1308
2026-01-04 14:13:30,968: t15.2023.10.01 val PER: 0.1684
2026-01-04 14:13:30,968: t15.2023.10.06 val PER: 0.0829
2026-01-04 14:13:30,968: t15.2023.10.08 val PER: 0.2544
2026-01-04 14:13:30,969: t15.2023.10.13 val PER: 0.1908
2026-01-04 14:13:30,969: t15.2023.10.15 val PER: 0.1477
2026-01-04 14:13:30,969: t15.2023.10.20 val PER: 0.1946
2026-01-04 14:13:30,969: t15.2023.10.22 val PER: 0.1080
2026-01-04 14:13:30,969: t15.2023.11.03 val PER: 0.1744
2026-01-04 14:13:30,969: t15.2023.11.04 val PER: 0.0307
2026-01-04 14:13:30,969: t15.2023.11.17 val PER: 0.0280
2026-01-04 14:13:30,969: t15.2023.11.19 val PER: 0.0379
2026-01-04 14:13:30,969: t15.2023.11.26 val PER: 0.1123
2026-01-04 14:13:30,969: t15.2023.12.03 val PER: 0.1113
2026-01-04 14:13:30,969: t15.2023.12.08 val PER: 0.0925
2026-01-04 14:13:30,970: t15.2023.12.10 val PER: 0.0867
2026-01-04 14:13:30,970: t15.2023.12.17 val PER: 0.1227
2026-01-04 14:13:30,970: t15.2023.12.29 val PER: 0.1270
2026-01-04 14:13:30,970: t15.2024.02.25 val PER: 0.0997
2026-01-04 14:13:30,970: t15.2024.03.08 val PER: 0.2248
2026-01-04 14:13:30,970: t15.2024.03.15 val PER: 0.1982
2026-01-04 14:13:30,970: t15.2024.03.17 val PER: 0.1325
2026-01-04 14:13:30,970: t15.2024.05.10 val PER: 0.1560
2026-01-04 14:13:30,970: t15.2024.06.14 val PER: 0.1640
2026-01-04 14:13:30,970: t15.2024.07.19 val PER: 0.2314
2026-01-04 14:13:30,970: t15.2024.07.21 val PER: 0.0897
2026-01-04 14:13:30,970: t15.2024.07.28 val PER: 0.1301
2026-01-04 14:13:30,971: t15.2025.01.10 val PER: 0.2769
2026-01-04 14:13:30,971: t15.2025.01.12 val PER: 0.1370
2026-01-04 14:13:30,971: t15.2025.03.14 val PER: 0.3358
2026-01-04 14:13:30,971: t15.2025.03.16 val PER: 0.1832
2026-01-04 14:13:30,971: t15.2025.03.30 val PER: 0.2839
2026-01-04 14:13:30,971: t15.2025.04.13 val PER: 0.2154
2026-01-04 14:13:31,245: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_19500
2026-01-04 14:13:40,400: Train batch 19600: loss: 7.40 grad norm: 48.22 time: 0.056
2026-01-04 14:13:58,611: Train batch 19800: loss: 7.14 grad norm: 50.53 time: 0.056
2026-01-04 14:14:16,989: Running test after training batch: 19999
2026-01-04 14:14:17,126: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:14:21,891: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:14:21,936: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:14:24,258: Val batch 19999: PER (avg): 0.1450 CTC Loss (avg): 15.0150 WER(1gram): 45.18% (n=64) time: 7.268
2026-01-04 14:14:24,259: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 14:14:24,259: t15.2023.08.13 val PER: 0.1102
2026-01-04 14:14:24,259: t15.2023.08.18 val PER: 0.1073
2026-01-04 14:14:24,259: t15.2023.08.20 val PER: 0.1041
2026-01-04 14:14:24,259: t15.2023.08.25 val PER: 0.0904
2026-01-04 14:14:24,259: t15.2023.08.27 val PER: 0.1688
2026-01-04 14:14:24,260: t15.2023.09.01 val PER: 0.0747
2026-01-04 14:14:24,260: t15.2023.09.03 val PER: 0.1520
2026-01-04 14:14:24,260: t15.2023.09.24 val PER: 0.1335
2026-01-04 14:14:24,260: t15.2023.09.29 val PER: 0.1295
2026-01-04 14:14:24,260: t15.2023.10.01 val PER: 0.1671
2026-01-04 14:14:24,260: t15.2023.10.06 val PER: 0.0818
2026-01-04 14:14:24,261: t15.2023.10.08 val PER: 0.2571
2026-01-04 14:14:24,261: t15.2023.10.13 val PER: 0.1908
2026-01-04 14:14:24,261: t15.2023.10.15 val PER: 0.1457
2026-01-04 14:14:24,261: t15.2023.10.20 val PER: 0.1879
2026-01-04 14:14:24,261: t15.2023.10.22 val PER: 0.1047
2026-01-04 14:14:24,261: t15.2023.11.03 val PER: 0.1730
2026-01-04 14:14:24,261: t15.2023.11.04 val PER: 0.0239
2026-01-04 14:14:24,261: t15.2023.11.17 val PER: 0.0295
2026-01-04 14:14:24,261: t15.2023.11.19 val PER: 0.0379
2026-01-04 14:14:24,261: t15.2023.11.26 val PER: 0.1130
2026-01-04 14:14:24,261: t15.2023.12.03 val PER: 0.1082
2026-01-04 14:14:24,262: t15.2023.12.08 val PER: 0.0872
2026-01-04 14:14:24,262: t15.2023.12.10 val PER: 0.0828
2026-01-04 14:14:24,262: t15.2023.12.17 val PER: 0.1247
2026-01-04 14:14:24,262: t15.2023.12.29 val PER: 0.1263
2026-01-04 14:14:24,262: t15.2024.02.25 val PER: 0.1011
2026-01-04 14:14:24,262: t15.2024.03.08 val PER: 0.2290
2026-01-04 14:14:24,262: t15.2024.03.15 val PER: 0.1945
2026-01-04 14:14:24,262: t15.2024.03.17 val PER: 0.1311
2026-01-04 14:14:24,262: t15.2024.05.10 val PER: 0.1441
2026-01-04 14:14:24,262: t15.2024.06.14 val PER: 0.1656
2026-01-04 14:14:24,262: t15.2024.07.19 val PER: 0.2320
2026-01-04 14:14:24,263: t15.2024.07.21 val PER: 0.0924
2026-01-04 14:14:24,263: t15.2024.07.28 val PER: 0.1301
2026-01-04 14:14:24,263: t15.2025.01.10 val PER: 0.2782
2026-01-04 14:14:24,263: t15.2025.01.12 val PER: 0.1370
2026-01-04 14:14:24,263: t15.2025.03.14 val PER: 0.3417
2026-01-04 14:14:24,263: t15.2025.03.16 val PER: 0.1872
2026-01-04 14:14:24,263: t15.2025.03.30 val PER: 0.2862
2026-01-04 14:14:24,263: t15.2025.04.13 val PER: 0.2068
2026-01-04 14:14:24,534: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd1e-5/checkpoint/checkpoint_batch_19999
2026-01-04 14:14:24,568: Best avg val PER achieved: 0.14747
2026-01-04 14:14:24,569: Total training time: 38.05 minutes

=== RUN wd3e-4.yaml ===
2026-01-04 14:14:29,734: Using device: cuda:0
2026-01-04 14:14:31,429: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-04 14:14:31,452: Using 45 sessions after filtering (from 45).
2026-01-04 14:14:31,957: Using torch.compile (if available)
2026-01-04 14:14:31,958: torch.compile not available (torch<2.0). Skipping.
2026-01-04 14:14:31,958: Initialized RNN decoding model
2026-01-04 14:14:31,958: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-04 14:14:31,958: Model has 44,907,305 parameters
2026-01-04 14:14:31,959: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-04 14:14:33,234: Successfully initialized datasets
2026-01-04 14:14:33,234: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-04 14:14:34,286: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.184
2026-01-04 14:14:34,286: Running test after training batch: 0
2026-01-04 14:14:34,399: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:14:39,904: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-04 14:14:40,608: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-04 14:15:13,948: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 39.661
2026-01-04 14:15:13,948: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-04 14:15:13,949: t15.2023.08.13 val PER: 1.3056
2026-01-04 14:15:13,949: t15.2023.08.18 val PER: 1.4208
2026-01-04 14:15:13,949: t15.2023.08.20 val PER: 1.3002
2026-01-04 14:15:13,949: t15.2023.08.25 val PER: 1.3389
2026-01-04 14:15:13,949: t15.2023.08.27 val PER: 1.2460
2026-01-04 14:15:13,949: t15.2023.09.01 val PER: 1.4537
2026-01-04 14:15:13,949: t15.2023.09.03 val PER: 1.3171
2026-01-04 14:15:13,949: t15.2023.09.24 val PER: 1.5461
2026-01-04 14:15:13,949: t15.2023.09.29 val PER: 1.4671
2026-01-04 14:15:13,949: t15.2023.10.01 val PER: 1.2147
2026-01-04 14:15:13,949: t15.2023.10.06 val PER: 1.4876
2026-01-04 14:15:13,949: t15.2023.10.08 val PER: 1.1827
2026-01-04 14:15:13,949: t15.2023.10.13 val PER: 1.3964
2026-01-04 14:15:13,950: t15.2023.10.15 val PER: 1.3889
2026-01-04 14:15:13,950: t15.2023.10.20 val PER: 1.4866
2026-01-04 14:15:13,950: t15.2023.10.22 val PER: 1.3942
2026-01-04 14:15:13,950: t15.2023.11.03 val PER: 1.5923
2026-01-04 14:15:13,950: t15.2023.11.04 val PER: 2.0171
2026-01-04 14:15:13,950: t15.2023.11.17 val PER: 1.9518
2026-01-04 14:15:13,950: t15.2023.11.19 val PER: 1.6707
2026-01-04 14:15:13,950: t15.2023.11.26 val PER: 1.5413
2026-01-04 14:15:13,950: t15.2023.12.03 val PER: 1.4254
2026-01-04 14:15:13,950: t15.2023.12.08 val PER: 1.4487
2026-01-04 14:15:13,951: t15.2023.12.10 val PER: 1.6899
2026-01-04 14:15:13,951: t15.2023.12.17 val PER: 1.3077
2026-01-04 14:15:13,951: t15.2023.12.29 val PER: 1.4063
2026-01-04 14:15:13,951: t15.2024.02.25 val PER: 1.4228
2026-01-04 14:15:13,951: t15.2024.03.08 val PER: 1.3257
2026-01-04 14:15:13,951: t15.2024.03.15 val PER: 1.3196
2026-01-04 14:15:13,951: t15.2024.03.17 val PER: 1.4052
2026-01-04 14:15:13,951: t15.2024.05.10 val PER: 1.3224
2026-01-04 14:15:13,951: t15.2024.06.14 val PER: 1.5315
2026-01-04 14:15:13,951: t15.2024.07.19 val PER: 1.0817
2026-01-04 14:15:13,951: t15.2024.07.21 val PER: 1.6290
2026-01-04 14:15:13,951: t15.2024.07.28 val PER: 1.6588
2026-01-04 14:15:13,951: t15.2025.01.10 val PER: 1.0923
2026-01-04 14:15:13,952: t15.2025.01.12 val PER: 1.7629
2026-01-04 14:15:13,952: t15.2025.03.14 val PER: 1.0414
2026-01-04 14:15:13,952: t15.2025.03.16 val PER: 1.6257
2026-01-04 14:15:13,952: t15.2025.03.30 val PER: 1.2874
2026-01-04 14:15:13,952: t15.2025.04.13 val PER: 1.5949
2026-01-04 14:15:13,953: New best val WER(1gram) inf% --> 100.00%
2026-01-04 14:15:13,953: Checkpointing model
2026-01-04 14:15:14,205: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:15:14,460: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_0
2026-01-04 14:15:31,962: Train batch 200: loss: 77.60 grad norm: 106.14 time: 0.055
2026-01-04 14:15:48,970: Train batch 400: loss: 53.49 grad norm: 89.07 time: 0.063
2026-01-04 14:15:57,799: Running test after training batch: 500
2026-01-04 14:15:57,958: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:16:02,829: WER debug example
  GT : you can see the code at this point as well
  PR : used and ease thus uhde at this ide is aisles
2026-01-04 14:16:02,866: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as tides
2026-01-04 14:16:05,640: Val batch 500: PER (avg): 0.5165 CTC Loss (avg): 55.5391 WER(1gram): 89.09% (n=64) time: 7.841
2026-01-04 14:16:05,641: WER lens: avg_true_words=6.16 avg_pred_words=5.75 max_pred_words=12
2026-01-04 14:16:05,641: t15.2023.08.13 val PER: 0.4605
2026-01-04 14:16:05,641: t15.2023.08.18 val PER: 0.4459
2026-01-04 14:16:05,641: t15.2023.08.20 val PER: 0.4353
2026-01-04 14:16:05,641: t15.2023.08.25 val PER: 0.4262
2026-01-04 14:16:05,641: t15.2023.08.27 val PER: 0.5177
2026-01-04 14:16:05,641: t15.2023.09.01 val PER: 0.4107
2026-01-04 14:16:05,641: t15.2023.09.03 val PER: 0.4857
2026-01-04 14:16:05,641: t15.2023.09.24 val PER: 0.4235
2026-01-04 14:16:05,642: t15.2023.09.29 val PER: 0.4690
2026-01-04 14:16:05,642: t15.2023.10.01 val PER: 0.5139
2026-01-04 14:16:05,642: t15.2023.10.06 val PER: 0.4349
2026-01-04 14:16:05,642: t15.2023.10.08 val PER: 0.5386
2026-01-04 14:16:05,642: t15.2023.10.13 val PER: 0.5842
2026-01-04 14:16:05,642: t15.2023.10.15 val PER: 0.4891
2026-01-04 14:16:05,642: t15.2023.10.20 val PER: 0.4698
2026-01-04 14:16:05,642: t15.2023.10.22 val PER: 0.4532
2026-01-04 14:16:05,642: t15.2023.11.03 val PER: 0.5081
2026-01-04 14:16:05,642: t15.2023.11.04 val PER: 0.2560
2026-01-04 14:16:05,643: t15.2023.11.17 val PER: 0.3655
2026-01-04 14:16:05,643: t15.2023.11.19 val PER: 0.3333
2026-01-04 14:16:05,643: t15.2023.11.26 val PER: 0.5464
2026-01-04 14:16:05,643: t15.2023.12.03 val PER: 0.5063
2026-01-04 14:16:05,643: t15.2023.12.08 val PER: 0.5133
2026-01-04 14:16:05,643: t15.2023.12.10 val PER: 0.4481
2026-01-04 14:16:05,643: t15.2023.12.17 val PER: 0.5613
2026-01-04 14:16:05,643: t15.2023.12.29 val PER: 0.5305
2026-01-04 14:16:05,643: t15.2024.02.25 val PER: 0.4817
2026-01-04 14:16:05,643: t15.2024.03.08 val PER: 0.6088
2026-01-04 14:16:05,643: t15.2024.03.15 val PER: 0.5560
2026-01-04 14:16:05,643: t15.2024.03.17 val PER: 0.5042
2026-01-04 14:16:05,643: t15.2024.05.10 val PER: 0.5438
2026-01-04 14:16:05,643: t15.2024.06.14 val PER: 0.5221
2026-01-04 14:16:05,643: t15.2024.07.19 val PER: 0.6717
2026-01-04 14:16:05,643: t15.2024.07.21 val PER: 0.4759
2026-01-04 14:16:05,644: t15.2024.07.28 val PER: 0.5044
2026-01-04 14:16:05,644: t15.2025.01.10 val PER: 0.7507
2026-01-04 14:16:05,644: t15.2025.01.12 val PER: 0.5512
2026-01-04 14:16:05,644: t15.2025.03.14 val PER: 0.7811
2026-01-04 14:16:05,644: t15.2025.03.16 val PER: 0.5903
2026-01-04 14:16:05,644: t15.2025.03.30 val PER: 0.7218
2026-01-04 14:16:05,644: t15.2025.04.13 val PER: 0.5720
2026-01-04 14:16:05,645: New best val WER(1gram) 100.00% --> 89.09%
2026-01-04 14:16:05,645: Checkpointing model
2026-01-04 14:16:06,267: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:16:06,528: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_500
2026-01-04 14:16:15,642: Train batch 600: loss: 48.97 grad norm: 79.22 time: 0.078
2026-01-04 14:16:32,672: Train batch 800: loss: 40.67 grad norm: 81.43 time: 0.057
2026-01-04 14:16:49,735: Train batch 1000: loss: 42.86 grad norm: 80.77 time: 0.066
2026-01-04 14:16:49,735: Running test after training batch: 1000
2026-01-04 14:16:49,850: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:16:54,954: WER debug example
  GT : you can see the code at this point as well
  PR : used wint ease utt good it this uhde is while
2026-01-04 14:16:54,990: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus wass it
2026-01-04 14:16:57,003: Val batch 1000: PER (avg): 0.4096 CTC Loss (avg): 42.2677 WER(1gram): 81.98% (n=64) time: 7.268
2026-01-04 14:16:57,004: WER lens: avg_true_words=6.16 avg_pred_words=5.58 max_pred_words=12
2026-01-04 14:16:57,004: t15.2023.08.13 val PER: 0.3701
2026-01-04 14:16:57,004: t15.2023.08.18 val PER: 0.3378
2026-01-04 14:16:57,004: t15.2023.08.20 val PER: 0.3487
2026-01-04 14:16:57,004: t15.2023.08.25 val PER: 0.3027
2026-01-04 14:16:57,005: t15.2023.08.27 val PER: 0.4164
2026-01-04 14:16:57,005: t15.2023.09.01 val PER: 0.3052
2026-01-04 14:16:57,005: t15.2023.09.03 val PER: 0.3967
2026-01-04 14:16:57,005: t15.2023.09.24 val PER: 0.3325
2026-01-04 14:16:57,005: t15.2023.09.29 val PER: 0.3682
2026-01-04 14:16:57,005: t15.2023.10.01 val PER: 0.4148
2026-01-04 14:16:57,005: t15.2023.10.06 val PER: 0.3154
2026-01-04 14:16:57,005: t15.2023.10.08 val PER: 0.4533
2026-01-04 14:16:57,005: t15.2023.10.13 val PER: 0.4647
2026-01-04 14:16:57,005: t15.2023.10.15 val PER: 0.3764
2026-01-04 14:16:57,006: t15.2023.10.20 val PER: 0.3792
2026-01-04 14:16:57,006: t15.2023.10.22 val PER: 0.3530
2026-01-04 14:16:57,006: t15.2023.11.03 val PER: 0.3982
2026-01-04 14:16:57,006: t15.2023.11.04 val PER: 0.1638
2026-01-04 14:16:57,006: t15.2023.11.17 val PER: 0.2597
2026-01-04 14:16:57,006: t15.2023.11.19 val PER: 0.2156
2026-01-04 14:16:57,006: t15.2023.11.26 val PER: 0.4478
2026-01-04 14:16:57,006: t15.2023.12.03 val PER: 0.4013
2026-01-04 14:16:57,006: t15.2023.12.08 val PER: 0.4055
2026-01-04 14:16:57,006: t15.2023.12.10 val PER: 0.3522
2026-01-04 14:16:57,006: t15.2023.12.17 val PER: 0.4054
2026-01-04 14:16:57,006: t15.2023.12.29 val PER: 0.4091
2026-01-04 14:16:57,006: t15.2024.02.25 val PER: 0.3567
2026-01-04 14:16:57,007: t15.2024.03.08 val PER: 0.4950
2026-01-04 14:16:57,007: t15.2024.03.15 val PER: 0.4409
2026-01-04 14:16:57,007: t15.2024.03.17 val PER: 0.4093
2026-01-04 14:16:57,007: t15.2024.05.10 val PER: 0.4146
2026-01-04 14:16:57,007: t15.2024.06.14 val PER: 0.4038
2026-01-04 14:16:57,007: t15.2024.07.19 val PER: 0.5379
2026-01-04 14:16:57,007: t15.2024.07.21 val PER: 0.3821
2026-01-04 14:16:57,007: t15.2024.07.28 val PER: 0.4125
2026-01-04 14:16:57,007: t15.2025.01.10 val PER: 0.6171
2026-01-04 14:16:57,007: t15.2025.01.12 val PER: 0.4511
2026-01-04 14:16:57,007: t15.2025.03.14 val PER: 0.6361
2026-01-04 14:16:57,007: t15.2025.03.16 val PER: 0.4686
2026-01-04 14:16:57,007: t15.2025.03.30 val PER: 0.6632
2026-01-04 14:16:57,007: t15.2025.04.13 val PER: 0.4979
2026-01-04 14:16:57,009: New best val WER(1gram) 89.09% --> 81.98%
2026-01-04 14:16:57,010: Checkpointing model
2026-01-04 14:16:57,601: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:16:57,858: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_1000
2026-01-04 14:17:15,561: Train batch 1200: loss: 33.04 grad norm: 74.30 time: 0.067
2026-01-04 14:17:33,089: Train batch 1400: loss: 36.38 grad norm: 82.69 time: 0.061
2026-01-04 14:17:42,669: Running test after training batch: 1500
2026-01-04 14:17:43,047: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:17:48,079: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt e the good at this boyde is will
2026-01-04 14:17:48,109: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that cost
2026-01-04 14:17:49,610: Val batch 1500: PER (avg): 0.3800 CTC Loss (avg): 37.1873 WER(1gram): 75.63% (n=64) time: 6.941
2026-01-04 14:17:49,611: WER lens: avg_true_words=6.16 avg_pred_words=5.05 max_pred_words=11
2026-01-04 14:17:49,611: t15.2023.08.13 val PER: 0.3410
2026-01-04 14:17:49,611: t15.2023.08.18 val PER: 0.3160
2026-01-04 14:17:49,611: t15.2023.08.20 val PER: 0.3082
2026-01-04 14:17:49,611: t15.2023.08.25 val PER: 0.2545
2026-01-04 14:17:49,612: t15.2023.08.27 val PER: 0.3987
2026-01-04 14:17:49,612: t15.2023.09.01 val PER: 0.2817
2026-01-04 14:17:49,612: t15.2023.09.03 val PER: 0.3587
2026-01-04 14:17:49,612: t15.2023.09.24 val PER: 0.3058
2026-01-04 14:17:49,612: t15.2023.09.29 val PER: 0.3414
2026-01-04 14:17:49,612: t15.2023.10.01 val PER: 0.3996
2026-01-04 14:17:49,612: t15.2023.10.06 val PER: 0.2885
2026-01-04 14:17:49,612: t15.2023.10.08 val PER: 0.4411
2026-01-04 14:17:49,612: t15.2023.10.13 val PER: 0.4461
2026-01-04 14:17:49,612: t15.2023.10.15 val PER: 0.3678
2026-01-04 14:17:49,613: t15.2023.10.20 val PER: 0.3188
2026-01-04 14:17:49,613: t15.2023.10.22 val PER: 0.3096
2026-01-04 14:17:49,613: t15.2023.11.03 val PER: 0.3589
2026-01-04 14:17:49,613: t15.2023.11.04 val PER: 0.1024
2026-01-04 14:17:49,613: t15.2023.11.17 val PER: 0.2271
2026-01-04 14:17:49,613: t15.2023.11.19 val PER: 0.1697
2026-01-04 14:17:49,613: t15.2023.11.26 val PER: 0.4167
2026-01-04 14:17:49,613: t15.2023.12.03 val PER: 0.3603
2026-01-04 14:17:49,613: t15.2023.12.08 val PER: 0.3622
2026-01-04 14:17:49,613: t15.2023.12.10 val PER: 0.2983
2026-01-04 14:17:49,614: t15.2023.12.17 val PER: 0.3669
2026-01-04 14:17:49,614: t15.2023.12.29 val PER: 0.3693
2026-01-04 14:17:49,614: t15.2024.02.25 val PER: 0.3258
2026-01-04 14:17:49,614: t15.2024.03.08 val PER: 0.4566
2026-01-04 14:17:49,614: t15.2024.03.15 val PER: 0.4178
2026-01-04 14:17:49,614: t15.2024.03.17 val PER: 0.3731
2026-01-04 14:17:49,614: t15.2024.05.10 val PER: 0.3685
2026-01-04 14:17:49,614: t15.2024.06.14 val PER: 0.3975
2026-01-04 14:17:49,614: t15.2024.07.19 val PER: 0.5122
2026-01-04 14:17:49,614: t15.2024.07.21 val PER: 0.3462
2026-01-04 14:17:49,614: t15.2024.07.28 val PER: 0.3721
2026-01-04 14:17:49,614: t15.2025.01.10 val PER: 0.6226
2026-01-04 14:17:49,615: t15.2025.01.12 val PER: 0.4334
2026-01-04 14:17:49,615: t15.2025.03.14 val PER: 0.5962
2026-01-04 14:17:49,615: t15.2025.03.16 val PER: 0.4673
2026-01-04 14:17:49,615: t15.2025.03.30 val PER: 0.6391
2026-01-04 14:17:49,615: t15.2025.04.13 val PER: 0.4807
2026-01-04 14:17:49,615: New best val WER(1gram) 81.98% --> 75.63%
2026-01-04 14:17:49,615: Checkpointing model
2026-01-04 14:17:50,235: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:17:50,497: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_1500
2026-01-04 14:17:59,171: Train batch 1600: loss: 36.60 grad norm: 79.50 time: 0.063
2026-01-04 14:18:16,366: Train batch 1800: loss: 34.95 grad norm: 67.57 time: 0.087
2026-01-04 14:18:34,087: Train batch 2000: loss: 34.35 grad norm: 74.55 time: 0.066
2026-01-04 14:18:34,087: Running test after training batch: 2000
2026-01-04 14:18:34,332: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:18:39,315: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned ease the code at this and is wheel
2026-01-04 14:18:39,347: WER debug example
  GT : how does it keep the cost down
  PR : houde buice it eke thus wass it
2026-01-04 14:18:41,041: Val batch 2000: PER (avg): 0.3265 CTC Loss (avg): 32.7909 WER(1gram): 72.08% (n=64) time: 6.954
2026-01-04 14:18:41,042: WER lens: avg_true_words=6.16 avg_pred_words=5.73 max_pred_words=11
2026-01-04 14:18:41,042: t15.2023.08.13 val PER: 0.3035
2026-01-04 14:18:41,042: t15.2023.08.18 val PER: 0.2531
2026-01-04 14:18:41,042: t15.2023.08.20 val PER: 0.2550
2026-01-04 14:18:41,043: t15.2023.08.25 val PER: 0.2319
2026-01-04 14:18:41,043: t15.2023.08.27 val PER: 0.3296
2026-01-04 14:18:41,043: t15.2023.09.01 val PER: 0.2362
2026-01-04 14:18:41,043: t15.2023.09.03 val PER: 0.3171
2026-01-04 14:18:41,043: t15.2023.09.24 val PER: 0.2427
2026-01-04 14:18:41,043: t15.2023.09.29 val PER: 0.2750
2026-01-04 14:18:41,043: t15.2023.10.01 val PER: 0.3256
2026-01-04 14:18:41,043: t15.2023.10.06 val PER: 0.2293
2026-01-04 14:18:41,043: t15.2023.10.08 val PER: 0.3897
2026-01-04 14:18:41,044: t15.2023.10.13 val PER: 0.3763
2026-01-04 14:18:41,044: t15.2023.10.15 val PER: 0.2966
2026-01-04 14:18:41,044: t15.2023.10.20 val PER: 0.2852
2026-01-04 14:18:41,044: t15.2023.10.22 val PER: 0.2695
2026-01-04 14:18:41,044: t15.2023.11.03 val PER: 0.3155
2026-01-04 14:18:41,044: t15.2023.11.04 val PER: 0.0887
2026-01-04 14:18:41,044: t15.2023.11.17 val PER: 0.1742
2026-01-04 14:18:41,044: t15.2023.11.19 val PER: 0.1417
2026-01-04 14:18:41,044: t15.2023.11.26 val PER: 0.3681
2026-01-04 14:18:41,044: t15.2023.12.03 val PER: 0.3204
2026-01-04 14:18:41,044: t15.2023.12.08 val PER: 0.3123
2026-01-04 14:18:41,045: t15.2023.12.10 val PER: 0.2562
2026-01-04 14:18:41,045: t15.2023.12.17 val PER: 0.3285
2026-01-04 14:18:41,045: t15.2023.12.29 val PER: 0.3329
2026-01-04 14:18:41,045: t15.2024.02.25 val PER: 0.2851
2026-01-04 14:18:41,045: t15.2024.03.08 val PER: 0.3997
2026-01-04 14:18:41,045: t15.2024.03.15 val PER: 0.3602
2026-01-04 14:18:41,045: t15.2024.03.17 val PER: 0.3361
2026-01-04 14:18:41,045: t15.2024.05.10 val PER: 0.3254
2026-01-04 14:18:41,045: t15.2024.06.14 val PER: 0.3375
2026-01-04 14:18:41,045: t15.2024.07.19 val PER: 0.4621
2026-01-04 14:18:41,046: t15.2024.07.21 val PER: 0.3007
2026-01-04 14:18:41,046: t15.2024.07.28 val PER: 0.3265
2026-01-04 14:18:41,046: t15.2025.01.10 val PER: 0.5592
2026-01-04 14:18:41,046: t15.2025.01.12 val PER: 0.3788
2026-01-04 14:18:41,046: t15.2025.03.14 val PER: 0.5222
2026-01-04 14:18:41,046: t15.2025.03.16 val PER: 0.3874
2026-01-04 14:18:41,046: t15.2025.03.30 val PER: 0.5276
2026-01-04 14:18:41,046: t15.2025.04.13 val PER: 0.4023
2026-01-04 14:18:41,047: New best val WER(1gram) 75.63% --> 72.08%
2026-01-04 14:18:41,047: Checkpointing model
2026-01-04 14:18:41,634: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:18:41,890: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_2000
2026-01-04 14:18:59,645: Train batch 2200: loss: 28.93 grad norm: 75.58 time: 0.060
2026-01-04 14:19:17,470: Train batch 2400: loss: 29.19 grad norm: 64.36 time: 0.051
2026-01-04 14:19:26,075: Running test after training batch: 2500
2026-01-04 14:19:26,294: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:19:31,117: WER debug example
  GT : you can see the code at this point as well
  PR : yule kent e the code at this point is wheel
2026-01-04 14:19:31,146: WER debug example
  GT : how does it keep the cost down
  PR : houde des it eke the wass it
2026-01-04 14:19:32,859: Val batch 2500: PER (avg): 0.3048 CTC Loss (avg): 30.2116 WER(1gram): 68.78% (n=64) time: 6.784
2026-01-04 14:19:32,860: WER lens: avg_true_words=6.16 avg_pred_words=5.47 max_pred_words=10
2026-01-04 14:19:32,860: t15.2023.08.13 val PER: 0.2848
2026-01-04 14:19:32,860: t15.2023.08.18 val PER: 0.2439
2026-01-04 14:19:32,860: t15.2023.08.20 val PER: 0.2438
2026-01-04 14:19:32,860: t15.2023.08.25 val PER: 0.2018
2026-01-04 14:19:32,861: t15.2023.08.27 val PER: 0.3151
2026-01-04 14:19:32,861: t15.2023.09.01 val PER: 0.2110
2026-01-04 14:19:32,861: t15.2023.09.03 val PER: 0.2791
2026-01-04 14:19:32,861: t15.2023.09.24 val PER: 0.2391
2026-01-04 14:19:32,861: t15.2023.09.29 val PER: 0.2578
2026-01-04 14:19:32,861: t15.2023.10.01 val PER: 0.3065
2026-01-04 14:19:32,861: t15.2023.10.06 val PER: 0.2228
2026-01-04 14:19:32,861: t15.2023.10.08 val PER: 0.3775
2026-01-04 14:19:32,861: t15.2023.10.13 val PER: 0.3561
2026-01-04 14:19:32,861: t15.2023.10.15 val PER: 0.2821
2026-01-04 14:19:32,862: t15.2023.10.20 val PER: 0.2752
2026-01-04 14:19:32,862: t15.2023.10.22 val PER: 0.2327
2026-01-04 14:19:32,862: t15.2023.11.03 val PER: 0.2870
2026-01-04 14:19:32,862: t15.2023.11.04 val PER: 0.0990
2026-01-04 14:19:32,862: t15.2023.11.17 val PER: 0.1462
2026-01-04 14:19:32,862: t15.2023.11.19 val PER: 0.1198
2026-01-04 14:19:32,862: t15.2023.11.26 val PER: 0.3457
2026-01-04 14:19:32,862: t15.2023.12.03 val PER: 0.2868
2026-01-04 14:19:32,862: t15.2023.12.08 val PER: 0.2803
2026-01-04 14:19:32,862: t15.2023.12.10 val PER: 0.2418
2026-01-04 14:19:32,862: t15.2023.12.17 val PER: 0.3025
2026-01-04 14:19:32,862: t15.2023.12.29 val PER: 0.3013
2026-01-04 14:19:32,862: t15.2024.02.25 val PER: 0.2444
2026-01-04 14:19:32,862: t15.2024.03.08 val PER: 0.3670
2026-01-04 14:19:32,862: t15.2024.03.15 val PER: 0.3465
2026-01-04 14:19:32,863: t15.2024.03.17 val PER: 0.3124
2026-01-04 14:19:32,863: t15.2024.05.10 val PER: 0.3180
2026-01-04 14:19:32,863: t15.2024.06.14 val PER: 0.3155
2026-01-04 14:19:32,863: t15.2024.07.19 val PER: 0.4423
2026-01-04 14:19:32,863: t15.2024.07.21 val PER: 0.2703
2026-01-04 14:19:32,863: t15.2024.07.28 val PER: 0.3037
2026-01-04 14:19:32,863: t15.2025.01.10 val PER: 0.5096
2026-01-04 14:19:32,863: t15.2025.01.12 val PER: 0.3572
2026-01-04 14:19:32,863: t15.2025.03.14 val PER: 0.5059
2026-01-04 14:19:32,863: t15.2025.03.16 val PER: 0.3639
2026-01-04 14:19:32,863: t15.2025.03.30 val PER: 0.5115
2026-01-04 14:19:32,863: t15.2025.04.13 val PER: 0.3937
2026-01-04 14:19:32,865: New best val WER(1gram) 72.08% --> 68.78%
2026-01-04 14:19:32,865: Checkpointing model
2026-01-04 14:19:33,483: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:19:33,740: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_2500
2026-01-04 14:19:42,235: Train batch 2600: loss: 35.06 grad norm: 81.96 time: 0.054
2026-01-04 14:19:59,444: Train batch 2800: loss: 25.78 grad norm: 71.24 time: 0.081
2026-01-04 14:20:16,308: Train batch 3000: loss: 31.34 grad norm: 70.51 time: 0.082
2026-01-04 14:20:16,309: Running test after training batch: 3000
2026-01-04 14:20:16,405: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:20:21,239: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-04 14:20:21,273: WER debug example
  GT : how does it keep the cost down
  PR : houde does it yip the cost get
2026-01-04 14:20:23,007: Val batch 3000: PER (avg): 0.2800 CTC Loss (avg): 27.8674 WER(1gram): 65.23% (n=64) time: 6.698
2026-01-04 14:20:23,007: WER lens: avg_true_words=6.16 avg_pred_words=5.86 max_pred_words=11
2026-01-04 14:20:23,007: t15.2023.08.13 val PER: 0.2609
2026-01-04 14:20:23,007: t15.2023.08.18 val PER: 0.2196
2026-01-04 14:20:23,008: t15.2023.08.20 val PER: 0.2129
2026-01-04 14:20:23,008: t15.2023.08.25 val PER: 0.1928
2026-01-04 14:20:23,008: t15.2023.08.27 val PER: 0.2894
2026-01-04 14:20:23,008: t15.2023.09.01 val PER: 0.1834
2026-01-04 14:20:23,008: t15.2023.09.03 val PER: 0.2898
2026-01-04 14:20:23,008: t15.2023.09.24 val PER: 0.2112
2026-01-04 14:20:23,008: t15.2023.09.29 val PER: 0.2323
2026-01-04 14:20:23,008: t15.2023.10.01 val PER: 0.2933
2026-01-04 14:20:23,008: t15.2023.10.06 val PER: 0.1970
2026-01-04 14:20:23,008: t15.2023.10.08 val PER: 0.3424
2026-01-04 14:20:23,008: t15.2023.10.13 val PER: 0.3359
2026-01-04 14:20:23,008: t15.2023.10.15 val PER: 0.2663
2026-01-04 14:20:23,009: t15.2023.10.20 val PER: 0.2550
2026-01-04 14:20:23,009: t15.2023.10.22 val PER: 0.2127
2026-01-04 14:20:23,009: t15.2023.11.03 val PER: 0.2714
2026-01-04 14:20:23,009: t15.2023.11.04 val PER: 0.0717
2026-01-04 14:20:23,009: t15.2023.11.17 val PER: 0.1306
2026-01-04 14:20:23,009: t15.2023.11.19 val PER: 0.1098
2026-01-04 14:20:23,009: t15.2023.11.26 val PER: 0.2993
2026-01-04 14:20:23,009: t15.2023.12.03 val PER: 0.2731
2026-01-04 14:20:23,009: t15.2023.12.08 val PER: 0.2557
2026-01-04 14:20:23,009: t15.2023.12.10 val PER: 0.2142
2026-01-04 14:20:23,010: t15.2023.12.17 val PER: 0.2672
2026-01-04 14:20:23,010: t15.2023.12.29 val PER: 0.2780
2026-01-04 14:20:23,010: t15.2024.02.25 val PER: 0.2360
2026-01-04 14:20:23,010: t15.2024.03.08 val PER: 0.3499
2026-01-04 14:20:23,010: t15.2024.03.15 val PER: 0.3308
2026-01-04 14:20:23,010: t15.2024.03.17 val PER: 0.2943
2026-01-04 14:20:23,010: t15.2024.05.10 val PER: 0.2957
2026-01-04 14:20:23,010: t15.2024.06.14 val PER: 0.2981
2026-01-04 14:20:23,010: t15.2024.07.19 val PER: 0.3995
2026-01-04 14:20:23,010: t15.2024.07.21 val PER: 0.2317
2026-01-04 14:20:23,010: t15.2024.07.28 val PER: 0.2757
2026-01-04 14:20:23,010: t15.2025.01.10 val PER: 0.4945
2026-01-04 14:20:23,010: t15.2025.01.12 val PER: 0.3264
2026-01-04 14:20:23,010: t15.2025.03.14 val PER: 0.4601
2026-01-04 14:20:23,010: t15.2025.03.16 val PER: 0.3272
2026-01-04 14:20:23,011: t15.2025.03.30 val PER: 0.4782
2026-01-04 14:20:23,011: t15.2025.04.13 val PER: 0.3395
2026-01-04 14:20:23,012: New best val WER(1gram) 68.78% --> 65.23%
2026-01-04 14:20:23,012: Checkpointing model
2026-01-04 14:20:23,623: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:20:23,878: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_3000
2026-01-04 14:20:41,854: Train batch 3200: loss: 27.01 grad norm: 67.13 time: 0.075
2026-01-04 14:20:58,774: Train batch 3400: loss: 18.34 grad norm: 55.09 time: 0.048
2026-01-04 14:21:07,417: Running test after training batch: 3500
2026-01-04 14:21:07,820: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:21:12,625: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point will
2026-01-04 14:21:12,653: WER debug example
  GT : how does it keep the cost down
  PR : aue des it yip thus cussed get
2026-01-04 14:21:14,205: Val batch 3500: PER (avg): 0.2662 CTC Loss (avg): 26.5499 WER(1gram): 64.97% (n=64) time: 6.788
2026-01-04 14:21:14,205: WER lens: avg_true_words=6.16 avg_pred_words=5.88 max_pred_words=11
2026-01-04 14:21:14,205: t15.2023.08.13 val PER: 0.2360
2026-01-04 14:21:14,205: t15.2023.08.18 val PER: 0.2070
2026-01-04 14:21:14,205: t15.2023.08.20 val PER: 0.2089
2026-01-04 14:21:14,206: t15.2023.08.25 val PER: 0.1807
2026-01-04 14:21:14,206: t15.2023.08.27 val PER: 0.2588
2026-01-04 14:21:14,206: t15.2023.09.01 val PER: 0.1761
2026-01-04 14:21:14,206: t15.2023.09.03 val PER: 0.2648
2026-01-04 14:21:14,206: t15.2023.09.24 val PER: 0.2136
2026-01-04 14:21:14,206: t15.2023.09.29 val PER: 0.2208
2026-01-04 14:21:14,206: t15.2023.10.01 val PER: 0.2774
2026-01-04 14:21:14,206: t15.2023.10.06 val PER: 0.1819
2026-01-04 14:21:14,206: t15.2023.10.08 val PER: 0.3451
2026-01-04 14:21:14,206: t15.2023.10.13 val PER: 0.3212
2026-01-04 14:21:14,206: t15.2023.10.15 val PER: 0.2485
2026-01-04 14:21:14,206: t15.2023.10.20 val PER: 0.2349
2026-01-04 14:21:14,207: t15.2023.10.22 val PER: 0.1982
2026-01-04 14:21:14,207: t15.2023.11.03 val PER: 0.2544
2026-01-04 14:21:14,207: t15.2023.11.04 val PER: 0.0683
2026-01-04 14:21:14,207: t15.2023.11.17 val PER: 0.1073
2026-01-04 14:21:14,207: t15.2023.11.19 val PER: 0.1018
2026-01-04 14:21:14,207: t15.2023.11.26 val PER: 0.2862
2026-01-04 14:21:14,207: t15.2023.12.03 val PER: 0.2374
2026-01-04 14:21:14,207: t15.2023.12.08 val PER: 0.2463
2026-01-04 14:21:14,207: t15.2023.12.10 val PER: 0.1958
2026-01-04 14:21:14,207: t15.2023.12.17 val PER: 0.2599
2026-01-04 14:21:14,207: t15.2023.12.29 val PER: 0.2512
2026-01-04 14:21:14,207: t15.2024.02.25 val PER: 0.2121
2026-01-04 14:21:14,207: t15.2024.03.08 val PER: 0.3428
2026-01-04 14:21:14,208: t15.2024.03.15 val PER: 0.3196
2026-01-04 14:21:14,208: t15.2024.03.17 val PER: 0.2768
2026-01-04 14:21:14,208: t15.2024.05.10 val PER: 0.2660
2026-01-04 14:21:14,208: t15.2024.06.14 val PER: 0.2808
2026-01-04 14:21:14,208: t15.2024.07.19 val PER: 0.3988
2026-01-04 14:21:14,208: t15.2024.07.21 val PER: 0.2193
2026-01-04 14:21:14,208: t15.2024.07.28 val PER: 0.2794
2026-01-04 14:21:14,208: t15.2025.01.10 val PER: 0.4711
2026-01-04 14:21:14,208: t15.2025.01.12 val PER: 0.2995
2026-01-04 14:21:14,208: t15.2025.03.14 val PER: 0.4453
2026-01-04 14:21:14,208: t15.2025.03.16 val PER: 0.3259
2026-01-04 14:21:14,208: t15.2025.03.30 val PER: 0.4690
2026-01-04 14:21:14,208: t15.2025.04.13 val PER: 0.3352
2026-01-04 14:21:14,210: New best val WER(1gram) 65.23% --> 64.97%
2026-01-04 14:21:14,210: Checkpointing model
2026-01-04 14:21:14,835: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:21:15,086: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_3500
2026-01-04 14:21:23,687: Train batch 3600: loss: 22.66 grad norm: 66.67 time: 0.066
2026-01-04 14:21:40,255: Train batch 3800: loss: 25.77 grad norm: 67.91 time: 0.067
2026-01-04 14:21:57,773: Train batch 4000: loss: 20.12 grad norm: 56.44 time: 0.056
2026-01-04 14:21:57,773: Running test after training batch: 4000
2026-01-04 14:21:57,920: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:22:02,692: WER debug example
  GT : you can see the code at this point as well
  PR : yule kent sci the code at this point is will
2026-01-04 14:22:02,724: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cussed nit
2026-01-04 14:22:04,419: Val batch 4000: PER (avg): 0.2510 CTC Loss (avg): 24.5166 WER(1gram): 63.45% (n=64) time: 6.645
2026-01-04 14:22:04,419: WER lens: avg_true_words=6.16 avg_pred_words=6.02 max_pred_words=11
2026-01-04 14:22:04,420: t15.2023.08.13 val PER: 0.2204
2026-01-04 14:22:04,420: t15.2023.08.18 val PER: 0.2054
2026-01-04 14:22:04,420: t15.2023.08.20 val PER: 0.2129
2026-01-04 14:22:04,420: t15.2023.08.25 val PER: 0.1627
2026-01-04 14:22:04,420: t15.2023.08.27 val PER: 0.2814
2026-01-04 14:22:04,420: t15.2023.09.01 val PER: 0.1688
2026-01-04 14:22:04,420: t15.2023.09.03 val PER: 0.2506
2026-01-04 14:22:04,420: t15.2023.09.24 val PER: 0.1917
2026-01-04 14:22:04,420: t15.2023.09.29 val PER: 0.1985
2026-01-04 14:22:04,420: t15.2023.10.01 val PER: 0.2596
2026-01-04 14:22:04,421: t15.2023.10.06 val PER: 0.1658
2026-01-04 14:22:04,421: t15.2023.10.08 val PER: 0.3275
2026-01-04 14:22:04,421: t15.2023.10.13 val PER: 0.3057
2026-01-04 14:22:04,421: t15.2023.10.15 val PER: 0.2413
2026-01-04 14:22:04,421: t15.2023.10.20 val PER: 0.2248
2026-01-04 14:22:04,421: t15.2023.10.22 val PER: 0.2016
2026-01-04 14:22:04,421: t15.2023.11.03 val PER: 0.2422
2026-01-04 14:22:04,421: t15.2023.11.04 val PER: 0.0683
2026-01-04 14:22:04,422: t15.2023.11.17 val PER: 0.0980
2026-01-04 14:22:04,422: t15.2023.11.19 val PER: 0.0938
2026-01-04 14:22:04,422: t15.2023.11.26 val PER: 0.2797
2026-01-04 14:22:04,422: t15.2023.12.03 val PER: 0.2248
2026-01-04 14:22:04,422: t15.2023.12.08 val PER: 0.2270
2026-01-04 14:22:04,422: t15.2023.12.10 val PER: 0.1682
2026-01-04 14:22:04,422: t15.2023.12.17 val PER: 0.2536
2026-01-04 14:22:04,422: t15.2023.12.29 val PER: 0.2594
2026-01-04 14:22:04,422: t15.2024.02.25 val PER: 0.2079
2026-01-04 14:22:04,422: t15.2024.03.08 val PER: 0.3272
2026-01-04 14:22:04,423: t15.2024.03.15 val PER: 0.3021
2026-01-04 14:22:04,423: t15.2024.03.17 val PER: 0.2559
2026-01-04 14:22:04,423: t15.2024.05.10 val PER: 0.2615
2026-01-04 14:22:04,423: t15.2024.06.14 val PER: 0.2871
2026-01-04 14:22:04,423: t15.2024.07.19 val PER: 0.3652
2026-01-04 14:22:04,423: t15.2024.07.21 val PER: 0.1938
2026-01-04 14:22:04,423: t15.2024.07.28 val PER: 0.2338
2026-01-04 14:22:04,423: t15.2025.01.10 val PER: 0.4325
2026-01-04 14:22:04,423: t15.2025.01.12 val PER: 0.2802
2026-01-04 14:22:04,423: t15.2025.03.14 val PER: 0.4246
2026-01-04 14:22:04,424: t15.2025.03.16 val PER: 0.3089
2026-01-04 14:22:04,424: t15.2025.03.30 val PER: 0.4126
2026-01-04 14:22:04,424: t15.2025.04.13 val PER: 0.3124
2026-01-04 14:22:04,424: New best val WER(1gram) 64.97% --> 63.45%
2026-01-04 14:22:04,425: Checkpointing model
2026-01-04 14:22:05,024: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:22:05,285: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_4000
2026-01-04 14:22:22,588: Train batch 4200: loss: 22.19 grad norm: 65.91 time: 0.078
2026-01-04 14:22:40,361: Train batch 4400: loss: 16.92 grad norm: 54.54 time: 0.065
2026-01-04 14:22:49,213: Running test after training batch: 4500
2026-01-04 14:22:49,352: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:22:54,416: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 14:22:54,447: WER debug example
  GT : how does it keep the cost down
  PR : houde just it heap the rust get
2026-01-04 14:22:56,163: Val batch 4500: PER (avg): 0.2374 CTC Loss (avg): 23.4009 WER(1gram): 60.15% (n=64) time: 6.950
2026-01-04 14:22:56,164: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-04 14:22:56,164: t15.2023.08.13 val PER: 0.2048
2026-01-04 14:22:56,164: t15.2023.08.18 val PER: 0.1878
2026-01-04 14:22:56,164: t15.2023.08.20 val PER: 0.1859
2026-01-04 14:22:56,164: t15.2023.08.25 val PER: 0.1386
2026-01-04 14:22:56,164: t15.2023.08.27 val PER: 0.2556
2026-01-04 14:22:56,164: t15.2023.09.01 val PER: 0.1485
2026-01-04 14:22:56,165: t15.2023.09.03 val PER: 0.2387
2026-01-04 14:22:56,165: t15.2023.09.24 val PER: 0.1663
2026-01-04 14:22:56,165: t15.2023.09.29 val PER: 0.1991
2026-01-04 14:22:56,165: t15.2023.10.01 val PER: 0.2490
2026-01-04 14:22:56,165: t15.2023.10.06 val PER: 0.1582
2026-01-04 14:22:56,165: t15.2023.10.08 val PER: 0.3112
2026-01-04 14:22:56,166: t15.2023.10.13 val PER: 0.3002
2026-01-04 14:22:56,166: t15.2023.10.15 val PER: 0.2301
2026-01-04 14:22:56,166: t15.2023.10.20 val PER: 0.2248
2026-01-04 14:22:56,166: t15.2023.10.22 val PER: 0.1915
2026-01-04 14:22:56,166: t15.2023.11.03 val PER: 0.2374
2026-01-04 14:22:56,166: t15.2023.11.04 val PER: 0.0546
2026-01-04 14:22:56,167: t15.2023.11.17 val PER: 0.0918
2026-01-04 14:22:56,167: t15.2023.11.19 val PER: 0.0998
2026-01-04 14:22:56,167: t15.2023.11.26 val PER: 0.2659
2026-01-04 14:22:56,167: t15.2023.12.03 val PER: 0.2111
2026-01-04 14:22:56,167: t15.2023.12.08 val PER: 0.2210
2026-01-04 14:22:56,167: t15.2023.12.10 val PER: 0.1735
2026-01-04 14:22:56,167: t15.2023.12.17 val PER: 0.2349
2026-01-04 14:22:56,167: t15.2023.12.29 val PER: 0.2402
2026-01-04 14:22:56,167: t15.2024.02.25 val PER: 0.1910
2026-01-04 14:22:56,167: t15.2024.03.08 val PER: 0.3087
2026-01-04 14:22:56,168: t15.2024.03.15 val PER: 0.2808
2026-01-04 14:22:56,168: t15.2024.03.17 val PER: 0.2434
2026-01-04 14:22:56,168: t15.2024.05.10 val PER: 0.2467
2026-01-04 14:22:56,168: t15.2024.06.14 val PER: 0.2476
2026-01-04 14:22:56,168: t15.2024.07.19 val PER: 0.3421
2026-01-04 14:22:56,168: t15.2024.07.21 val PER: 0.1779
2026-01-04 14:22:56,168: t15.2024.07.28 val PER: 0.2404
2026-01-04 14:22:56,168: t15.2025.01.10 val PER: 0.4146
2026-01-04 14:22:56,169: t15.2025.01.12 val PER: 0.2602
2026-01-04 14:22:56,169: t15.2025.03.14 val PER: 0.4083
2026-01-04 14:22:56,169: t15.2025.03.16 val PER: 0.2919
2026-01-04 14:22:56,169: t15.2025.03.30 val PER: 0.4149
2026-01-04 14:22:56,169: t15.2025.04.13 val PER: 0.2853
2026-01-04 14:22:56,169: New best val WER(1gram) 63.45% --> 60.15%
2026-01-04 14:22:56,170: Checkpointing model
2026-01-04 14:22:56,794: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:22:57,050: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_4500
2026-01-04 14:23:05,934: Train batch 4600: loss: 19.95 grad norm: 61.02 time: 0.064
2026-01-04 14:23:23,034: Train batch 4800: loss: 13.85 grad norm: 53.55 time: 0.063
2026-01-04 14:23:40,741: Train batch 5000: loss: 32.17 grad norm: 82.24 time: 0.063
2026-01-04 14:23:40,742: Running test after training batch: 5000
2026-01-04 14:23:40,922: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:23:45,693: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 14:23:45,724: WER debug example
  GT : how does it keep the cost down
  PR : houde just it heap the cost it
2026-01-04 14:23:47,442: Val batch 5000: PER (avg): 0.2274 CTC Loss (avg): 22.1302 WER(1gram): 62.18% (n=64) time: 6.700
2026-01-04 14:23:47,443: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-04 14:23:47,443: t15.2023.08.13 val PER: 0.1933
2026-01-04 14:23:47,443: t15.2023.08.18 val PER: 0.1794
2026-01-04 14:23:47,443: t15.2023.08.20 val PER: 0.1843
2026-01-04 14:23:47,443: t15.2023.08.25 val PER: 0.1340
2026-01-04 14:23:47,443: t15.2023.08.27 val PER: 0.2508
2026-01-04 14:23:47,443: t15.2023.09.01 val PER: 0.1356
2026-01-04 14:23:47,443: t15.2023.09.03 val PER: 0.2316
2026-01-04 14:23:47,443: t15.2023.09.24 val PER: 0.1760
2026-01-04 14:23:47,444: t15.2023.09.29 val PER: 0.1844
2026-01-04 14:23:47,444: t15.2023.10.01 val PER: 0.2285
2026-01-04 14:23:47,444: t15.2023.10.06 val PER: 0.1485
2026-01-04 14:23:47,444: t15.2023.10.08 val PER: 0.3112
2026-01-04 14:23:47,444: t15.2023.10.13 val PER: 0.2847
2026-01-04 14:23:47,444: t15.2023.10.15 val PER: 0.2254
2026-01-04 14:23:47,444: t15.2023.10.20 val PER: 0.2215
2026-01-04 14:23:47,444: t15.2023.10.22 val PER: 0.1715
2026-01-04 14:23:47,444: t15.2023.11.03 val PER: 0.2347
2026-01-04 14:23:47,444: t15.2023.11.04 val PER: 0.0410
2026-01-04 14:23:47,444: t15.2023.11.17 val PER: 0.0871
2026-01-04 14:23:47,444: t15.2023.11.19 val PER: 0.0858
2026-01-04 14:23:47,445: t15.2023.11.26 val PER: 0.2355
2026-01-04 14:23:47,445: t15.2023.12.03 val PER: 0.1954
2026-01-04 14:23:47,445: t15.2023.12.08 val PER: 0.2051
2026-01-04 14:23:47,445: t15.2023.12.10 val PER: 0.1603
2026-01-04 14:23:47,445: t15.2023.12.17 val PER: 0.2214
2026-01-04 14:23:47,445: t15.2023.12.29 val PER: 0.2286
2026-01-04 14:23:47,445: t15.2024.02.25 val PER: 0.1826
2026-01-04 14:23:47,445: t15.2024.03.08 val PER: 0.3115
2026-01-04 14:23:47,445: t15.2024.03.15 val PER: 0.2789
2026-01-04 14:23:47,445: t15.2024.03.17 val PER: 0.2371
2026-01-04 14:23:47,445: t15.2024.05.10 val PER: 0.2511
2026-01-04 14:23:47,445: t15.2024.06.14 val PER: 0.2445
2026-01-04 14:23:47,445: t15.2024.07.19 val PER: 0.3322
2026-01-04 14:23:47,445: t15.2024.07.21 val PER: 0.1869
2026-01-04 14:23:47,445: t15.2024.07.28 val PER: 0.2125
2026-01-04 14:23:47,445: t15.2025.01.10 val PER: 0.3898
2026-01-04 14:23:47,446: t15.2025.01.12 val PER: 0.2433
2026-01-04 14:23:47,446: t15.2025.03.14 val PER: 0.3743
2026-01-04 14:23:47,446: t15.2025.03.16 val PER: 0.2683
2026-01-04 14:23:47,446: t15.2025.03.30 val PER: 0.4057
2026-01-04 14:23:47,446: t15.2025.04.13 val PER: 0.3110
2026-01-04 14:23:47,688: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_5000
2026-01-04 14:24:04,895: Train batch 5200: loss: 16.12 grad norm: 56.99 time: 0.051
2026-01-04 14:24:22,221: Train batch 5400: loss: 17.18 grad norm: 57.17 time: 0.068
2026-01-04 14:24:30,805: Running test after training batch: 5500
2026-01-04 14:24:30,902: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:24:35,723: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point will
2026-01-04 14:24:35,754: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-04 14:24:37,451: Val batch 5500: PER (avg): 0.2155 CTC Loss (avg): 21.1284 WER(1gram): 57.11% (n=64) time: 6.646
2026-01-04 14:24:37,452: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-04 14:24:37,452: t15.2023.08.13 val PER: 0.1736
2026-01-04 14:24:37,452: t15.2023.08.18 val PER: 0.1651
2026-01-04 14:24:37,452: t15.2023.08.20 val PER: 0.1700
2026-01-04 14:24:37,452: t15.2023.08.25 val PER: 0.1220
2026-01-04 14:24:37,452: t15.2023.08.27 val PER: 0.2460
2026-01-04 14:24:37,452: t15.2023.09.01 val PER: 0.1323
2026-01-04 14:24:37,453: t15.2023.09.03 val PER: 0.2150
2026-01-04 14:24:37,453: t15.2023.09.24 val PER: 0.1675
2026-01-04 14:24:37,453: t15.2023.09.29 val PER: 0.1838
2026-01-04 14:24:37,453: t15.2023.10.01 val PER: 0.2259
2026-01-04 14:24:37,453: t15.2023.10.06 val PER: 0.1324
2026-01-04 14:24:37,453: t15.2023.10.08 val PER: 0.2869
2026-01-04 14:24:37,453: t15.2023.10.13 val PER: 0.2731
2026-01-04 14:24:37,453: t15.2023.10.15 val PER: 0.2096
2026-01-04 14:24:37,453: t15.2023.10.20 val PER: 0.2282
2026-01-04 14:24:37,453: t15.2023.10.22 val PER: 0.1715
2026-01-04 14:24:37,453: t15.2023.11.03 val PER: 0.2225
2026-01-04 14:24:37,453: t15.2023.11.04 val PER: 0.0478
2026-01-04 14:24:37,453: t15.2023.11.17 val PER: 0.0762
2026-01-04 14:24:37,454: t15.2023.11.19 val PER: 0.0778
2026-01-04 14:24:37,454: t15.2023.11.26 val PER: 0.2283
2026-01-04 14:24:37,454: t15.2023.12.03 val PER: 0.1922
2026-01-04 14:24:37,454: t15.2023.12.08 val PER: 0.1831
2026-01-04 14:24:37,454: t15.2023.12.10 val PER: 0.1577
2026-01-04 14:24:37,454: t15.2023.12.17 val PER: 0.2193
2026-01-04 14:24:37,454: t15.2023.12.29 val PER: 0.2100
2026-01-04 14:24:37,454: t15.2024.02.25 val PER: 0.1798
2026-01-04 14:24:37,454: t15.2024.03.08 val PER: 0.2774
2026-01-04 14:24:37,454: t15.2024.03.15 val PER: 0.2539
2026-01-04 14:24:37,455: t15.2024.03.17 val PER: 0.2155
2026-01-04 14:24:37,455: t15.2024.05.10 val PER: 0.2259
2026-01-04 14:24:37,455: t15.2024.06.14 val PER: 0.2413
2026-01-04 14:24:37,455: t15.2024.07.19 val PER: 0.3197
2026-01-04 14:24:37,455: t15.2024.07.21 val PER: 0.1621
2026-01-04 14:24:37,455: t15.2024.07.28 val PER: 0.2132
2026-01-04 14:24:37,455: t15.2025.01.10 val PER: 0.3953
2026-01-04 14:24:37,455: t15.2025.01.12 val PER: 0.2379
2026-01-04 14:24:37,455: t15.2025.03.14 val PER: 0.3772
2026-01-04 14:24:37,455: t15.2025.03.16 val PER: 0.2644
2026-01-04 14:24:37,455: t15.2025.03.30 val PER: 0.3609
2026-01-04 14:24:37,455: t15.2025.04.13 val PER: 0.2953
2026-01-04 14:24:37,456: New best val WER(1gram) 60.15% --> 57.11%
2026-01-04 14:24:37,456: Checkpointing model
2026-01-04 14:24:38,091: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:24:38,345: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_5500
2026-01-04 14:24:47,270: Train batch 5600: loss: 19.59 grad norm: 68.00 time: 0.061
2026-01-04 14:25:04,558: Train batch 5800: loss: 13.63 grad norm: 55.02 time: 0.081
2026-01-04 14:25:21,641: Train batch 6000: loss: 14.18 grad norm: 56.94 time: 0.048
2026-01-04 14:25:21,642: Running test after training batch: 6000
2026-01-04 14:25:21,941: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:25:26,727: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-04 14:25:26,761: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 14:25:28,492: Val batch 6000: PER (avg): 0.2108 CTC Loss (avg): 20.8810 WER(1gram): 58.12% (n=64) time: 6.850
2026-01-04 14:25:28,493: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-04 14:25:28,493: t15.2023.08.13 val PER: 0.1694
2026-01-04 14:25:28,493: t15.2023.08.18 val PER: 0.1668
2026-01-04 14:25:28,493: t15.2023.08.20 val PER: 0.1589
2026-01-04 14:25:28,493: t15.2023.08.25 val PER: 0.1205
2026-01-04 14:25:28,493: t15.2023.08.27 val PER: 0.2379
2026-01-04 14:25:28,493: t15.2023.09.01 val PER: 0.1193
2026-01-04 14:25:28,493: t15.2023.09.03 val PER: 0.2067
2026-01-04 14:25:28,493: t15.2023.09.24 val PER: 0.1663
2026-01-04 14:25:28,493: t15.2023.09.29 val PER: 0.1634
2026-01-04 14:25:28,494: t15.2023.10.01 val PER: 0.2173
2026-01-04 14:25:28,494: t15.2023.10.06 val PER: 0.1421
2026-01-04 14:25:28,494: t15.2023.10.08 val PER: 0.2896
2026-01-04 14:25:28,494: t15.2023.10.13 val PER: 0.2723
2026-01-04 14:25:28,494: t15.2023.10.15 val PER: 0.2103
2026-01-04 14:25:28,494: t15.2023.10.20 val PER: 0.2114
2026-01-04 14:25:28,494: t15.2023.10.22 val PER: 0.1693
2026-01-04 14:25:28,494: t15.2023.11.03 val PER: 0.2280
2026-01-04 14:25:28,494: t15.2023.11.04 val PER: 0.0478
2026-01-04 14:25:28,494: t15.2023.11.17 val PER: 0.0778
2026-01-04 14:25:28,494: t15.2023.11.19 val PER: 0.0758
2026-01-04 14:25:28,495: t15.2023.11.26 val PER: 0.2275
2026-01-04 14:25:28,495: t15.2023.12.03 val PER: 0.1765
2026-01-04 14:25:28,495: t15.2023.12.08 val PER: 0.1724
2026-01-04 14:25:28,495: t15.2023.12.10 val PER: 0.1603
2026-01-04 14:25:28,495: t15.2023.12.17 val PER: 0.1975
2026-01-04 14:25:28,495: t15.2023.12.29 val PER: 0.2121
2026-01-04 14:25:28,495: t15.2024.02.25 val PER: 0.1671
2026-01-04 14:25:28,495: t15.2024.03.08 val PER: 0.2888
2026-01-04 14:25:28,495: t15.2024.03.15 val PER: 0.2689
2026-01-04 14:25:28,495: t15.2024.03.17 val PER: 0.2085
2026-01-04 14:25:28,496: t15.2024.05.10 val PER: 0.2259
2026-01-04 14:25:28,496: t15.2024.06.14 val PER: 0.2224
2026-01-04 14:25:28,496: t15.2024.07.19 val PER: 0.3098
2026-01-04 14:25:28,496: t15.2024.07.21 val PER: 0.1662
2026-01-04 14:25:28,496: t15.2024.07.28 val PER: 0.2088
2026-01-04 14:25:28,496: t15.2025.01.10 val PER: 0.3554
2026-01-04 14:25:28,496: t15.2025.01.12 val PER: 0.2186
2026-01-04 14:25:28,496: t15.2025.03.14 val PER: 0.3743
2026-01-04 14:25:28,496: t15.2025.03.16 val PER: 0.2605
2026-01-04 14:25:28,496: t15.2025.03.30 val PER: 0.3874
2026-01-04 14:25:28,496: t15.2025.04.13 val PER: 0.2725
2026-01-04 14:25:28,739: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_6000
2026-01-04 14:25:46,609: Train batch 6200: loss: 16.94 grad norm: 59.63 time: 0.070
2026-01-04 14:26:03,709: Train batch 6400: loss: 18.56 grad norm: 66.23 time: 0.061
2026-01-04 14:26:11,935: Running test after training batch: 6500
2026-01-04 14:26:12,027: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:26:16,817: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 14:26:16,850: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:26:18,550: Val batch 6500: PER (avg): 0.2038 CTC Loss (avg): 20.0718 WER(1gram): 52.28% (n=64) time: 6.615
2026-01-04 14:26:18,551: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 14:26:18,551: t15.2023.08.13 val PER: 0.1767
2026-01-04 14:26:18,551: t15.2023.08.18 val PER: 0.1425
2026-01-04 14:26:18,551: t15.2023.08.20 val PER: 0.1604
2026-01-04 14:26:18,551: t15.2023.08.25 val PER: 0.1039
2026-01-04 14:26:18,551: t15.2023.08.27 val PER: 0.2347
2026-01-04 14:26:18,551: t15.2023.09.01 val PER: 0.1234
2026-01-04 14:26:18,551: t15.2023.09.03 val PER: 0.2114
2026-01-04 14:26:18,551: t15.2023.09.24 val PER: 0.1711
2026-01-04 14:26:18,551: t15.2023.09.29 val PER: 0.1698
2026-01-04 14:26:18,552: t15.2023.10.01 val PER: 0.2127
2026-01-04 14:26:18,552: t15.2023.10.06 val PER: 0.1227
2026-01-04 14:26:18,552: t15.2023.10.08 val PER: 0.2882
2026-01-04 14:26:18,552: t15.2023.10.13 val PER: 0.2708
2026-01-04 14:26:18,552: t15.2023.10.15 val PER: 0.2182
2026-01-04 14:26:18,552: t15.2023.10.20 val PER: 0.2248
2026-01-04 14:26:18,553: t15.2023.10.22 val PER: 0.1604
2026-01-04 14:26:18,553: t15.2023.11.03 val PER: 0.2123
2026-01-04 14:26:18,553: t15.2023.11.04 val PER: 0.0546
2026-01-04 14:26:18,553: t15.2023.11.17 val PER: 0.0591
2026-01-04 14:26:18,553: t15.2023.11.19 val PER: 0.0659
2026-01-04 14:26:18,553: t15.2023.11.26 val PER: 0.2123
2026-01-04 14:26:18,553: t15.2023.12.03 val PER: 0.1849
2026-01-04 14:26:18,553: t15.2023.12.08 val PER: 0.1664
2026-01-04 14:26:18,553: t15.2023.12.10 val PER: 0.1432
2026-01-04 14:26:18,554: t15.2023.12.17 val PER: 0.1881
2026-01-04 14:26:18,554: t15.2023.12.29 val PER: 0.2004
2026-01-04 14:26:18,554: t15.2024.02.25 val PER: 0.1699
2026-01-04 14:26:18,554: t15.2024.03.08 val PER: 0.2930
2026-01-04 14:26:18,554: t15.2024.03.15 val PER: 0.2545
2026-01-04 14:26:18,554: t15.2024.03.17 val PER: 0.2071
2026-01-04 14:26:18,554: t15.2024.05.10 val PER: 0.2169
2026-01-04 14:26:18,554: t15.2024.06.14 val PER: 0.2224
2026-01-04 14:26:18,554: t15.2024.07.19 val PER: 0.2966
2026-01-04 14:26:18,554: t15.2024.07.21 val PER: 0.1448
2026-01-04 14:26:18,554: t15.2024.07.28 val PER: 0.1860
2026-01-04 14:26:18,554: t15.2025.01.10 val PER: 0.3650
2026-01-04 14:26:18,554: t15.2025.01.12 val PER: 0.2086
2026-01-04 14:26:18,554: t15.2025.03.14 val PER: 0.3728
2026-01-04 14:26:18,554: t15.2025.03.16 val PER: 0.2435
2026-01-04 14:26:18,555: t15.2025.03.30 val PER: 0.3517
2026-01-04 14:26:18,555: t15.2025.04.13 val PER: 0.2696
2026-01-04 14:26:18,556: New best val WER(1gram) 57.11% --> 52.28%
2026-01-04 14:26:18,556: Checkpointing model
2026-01-04 14:26:19,161: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:26:19,414: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_6500
2026-01-04 14:26:27,935: Train batch 6600: loss: 11.80 grad norm: 50.68 time: 0.045
2026-01-04 14:26:45,398: Train batch 6800: loss: 15.74 grad norm: 58.82 time: 0.047
2026-01-04 14:27:03,225: Train batch 7000: loss: 17.77 grad norm: 66.36 time: 0.060
2026-01-04 14:27:03,225: Running test after training batch: 7000
2026-01-04 14:27:03,378: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:27:08,392: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the cold at this point as will
2026-01-04 14:27:08,424: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-04 14:27:10,198: Val batch 7000: PER (avg): 0.1939 CTC Loss (avg): 19.2344 WER(1gram): 52.79% (n=64) time: 6.972
2026-01-04 14:27:10,198: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-04 14:27:10,198: t15.2023.08.13 val PER: 0.1622
2026-01-04 14:27:10,198: t15.2023.08.18 val PER: 0.1383
2026-01-04 14:27:10,199: t15.2023.08.20 val PER: 0.1493
2026-01-04 14:27:10,199: t15.2023.08.25 val PER: 0.1009
2026-01-04 14:27:10,199: t15.2023.08.27 val PER: 0.2203
2026-01-04 14:27:10,199: t15.2023.09.01 val PER: 0.1136
2026-01-04 14:27:10,199: t15.2023.09.03 val PER: 0.1888
2026-01-04 14:27:10,199: t15.2023.09.24 val PER: 0.1553
2026-01-04 14:27:10,199: t15.2023.09.29 val PER: 0.1634
2026-01-04 14:27:10,199: t15.2023.10.01 val PER: 0.2074
2026-01-04 14:27:10,199: t15.2023.10.06 val PER: 0.1152
2026-01-04 14:27:10,199: t15.2023.10.08 val PER: 0.2828
2026-01-04 14:27:10,199: t15.2023.10.13 val PER: 0.2545
2026-01-04 14:27:10,199: t15.2023.10.15 val PER: 0.1971
2026-01-04 14:27:10,200: t15.2023.10.20 val PER: 0.2081
2026-01-04 14:27:10,200: t15.2023.10.22 val PER: 0.1414
2026-01-04 14:27:10,200: t15.2023.11.03 val PER: 0.1995
2026-01-04 14:27:10,200: t15.2023.11.04 val PER: 0.0512
2026-01-04 14:27:10,200: t15.2023.11.17 val PER: 0.0560
2026-01-04 14:27:10,200: t15.2023.11.19 val PER: 0.0439
2026-01-04 14:27:10,200: t15.2023.11.26 val PER: 0.1993
2026-01-04 14:27:10,200: t15.2023.12.03 val PER: 0.1649
2026-01-04 14:27:10,200: t15.2023.12.08 val PER: 0.1571
2026-01-04 14:27:10,200: t15.2023.12.10 val PER: 0.1419
2026-01-04 14:27:10,200: t15.2023.12.17 val PER: 0.1715
2026-01-04 14:27:10,200: t15.2023.12.29 val PER: 0.1997
2026-01-04 14:27:10,200: t15.2024.02.25 val PER: 0.1587
2026-01-04 14:27:10,200: t15.2024.03.08 val PER: 0.2603
2026-01-04 14:27:10,201: t15.2024.03.15 val PER: 0.2370
2026-01-04 14:27:10,201: t15.2024.03.17 val PER: 0.1946
2026-01-04 14:27:10,201: t15.2024.05.10 val PER: 0.2021
2026-01-04 14:27:10,201: t15.2024.06.14 val PER: 0.2098
2026-01-04 14:27:10,201: t15.2024.07.19 val PER: 0.3059
2026-01-04 14:27:10,201: t15.2024.07.21 val PER: 0.1352
2026-01-04 14:27:10,201: t15.2024.07.28 val PER: 0.1794
2026-01-04 14:27:10,201: t15.2025.01.10 val PER: 0.3595
2026-01-04 14:27:10,201: t15.2025.01.12 val PER: 0.2125
2026-01-04 14:27:10,201: t15.2025.03.14 val PER: 0.3521
2026-01-04 14:27:10,202: t15.2025.03.16 val PER: 0.2343
2026-01-04 14:27:10,202: t15.2025.03.30 val PER: 0.3609
2026-01-04 14:27:10,202: t15.2025.04.13 val PER: 0.2710
2026-01-04 14:27:10,453: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_7000
2026-01-04 14:27:27,783: Train batch 7200: loss: 14.49 grad norm: 56.67 time: 0.079
2026-01-04 14:27:45,002: Train batch 7400: loss: 13.12 grad norm: 53.88 time: 0.075
2026-01-04 14:27:53,601: Running test after training batch: 7500
2026-01-04 14:27:53,715: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:27:58,446: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 14:27:58,477: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 14:28:00,243: Val batch 7500: PER (avg): 0.1905 CTC Loss (avg): 18.8189 WER(1gram): 55.08% (n=64) time: 6.641
2026-01-04 14:28:00,244: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-04 14:28:00,244: t15.2023.08.13 val PER: 0.1507
2026-01-04 14:28:00,244: t15.2023.08.18 val PER: 0.1425
2026-01-04 14:28:00,244: t15.2023.08.20 val PER: 0.1493
2026-01-04 14:28:00,244: t15.2023.08.25 val PER: 0.1054
2026-01-04 14:28:00,244: t15.2023.08.27 val PER: 0.2074
2026-01-04 14:28:00,244: t15.2023.09.01 val PER: 0.1161
2026-01-04 14:28:00,244: t15.2023.09.03 val PER: 0.1841
2026-01-04 14:28:00,244: t15.2023.09.24 val PER: 0.1517
2026-01-04 14:28:00,244: t15.2023.09.29 val PER: 0.1570
2026-01-04 14:28:00,244: t15.2023.10.01 val PER: 0.1988
2026-01-04 14:28:00,244: t15.2023.10.06 val PER: 0.1184
2026-01-04 14:28:00,245: t15.2023.10.08 val PER: 0.2639
2026-01-04 14:28:00,245: t15.2023.10.13 val PER: 0.2459
2026-01-04 14:28:00,245: t15.2023.10.15 val PER: 0.1905
2026-01-04 14:28:00,245: t15.2023.10.20 val PER: 0.1846
2026-01-04 14:28:00,245: t15.2023.10.22 val PER: 0.1437
2026-01-04 14:28:00,245: t15.2023.11.03 val PER: 0.2090
2026-01-04 14:28:00,245: t15.2023.11.04 val PER: 0.0478
2026-01-04 14:28:00,245: t15.2023.11.17 val PER: 0.0747
2026-01-04 14:28:00,245: t15.2023.11.19 val PER: 0.0539
2026-01-04 14:28:00,245: t15.2023.11.26 val PER: 0.1870
2026-01-04 14:28:00,245: t15.2023.12.03 val PER: 0.1691
2026-01-04 14:28:00,245: t15.2023.12.08 val PER: 0.1458
2026-01-04 14:28:00,245: t15.2023.12.10 val PER: 0.1367
2026-01-04 14:28:00,245: t15.2023.12.17 val PER: 0.1746
2026-01-04 14:28:00,245: t15.2023.12.29 val PER: 0.1846
2026-01-04 14:28:00,245: t15.2024.02.25 val PER: 0.1489
2026-01-04 14:28:00,246: t15.2024.03.08 val PER: 0.2788
2026-01-04 14:28:00,246: t15.2024.03.15 val PER: 0.2439
2026-01-04 14:28:00,246: t15.2024.03.17 val PER: 0.1799
2026-01-04 14:28:00,246: t15.2024.05.10 val PER: 0.2051
2026-01-04 14:28:00,246: t15.2024.06.14 val PER: 0.2082
2026-01-04 14:28:00,246: t15.2024.07.19 val PER: 0.3052
2026-01-04 14:28:00,246: t15.2024.07.21 val PER: 0.1428
2026-01-04 14:28:00,246: t15.2024.07.28 val PER: 0.1713
2026-01-04 14:28:00,246: t15.2025.01.10 val PER: 0.3540
2026-01-04 14:28:00,246: t15.2025.01.12 val PER: 0.1917
2026-01-04 14:28:00,247: t15.2025.03.14 val PER: 0.3669
2026-01-04 14:28:00,247: t15.2025.03.16 val PER: 0.2395
2026-01-04 14:28:00,247: t15.2025.03.30 val PER: 0.3563
2026-01-04 14:28:00,247: t15.2025.04.13 val PER: 0.2468
2026-01-04 14:28:00,486: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_7500
2026-01-04 14:28:09,458: Train batch 7600: loss: 15.92 grad norm: 57.96 time: 0.068
2026-01-04 14:28:27,630: Train batch 7800: loss: 14.25 grad norm: 61.07 time: 0.055
2026-01-04 14:28:46,297: Train batch 8000: loss: 11.44 grad norm: 52.94 time: 0.072
2026-01-04 14:28:46,297: Running test after training batch: 8000
2026-01-04 14:28:46,397: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:28:51,345: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 14:28:51,377: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-04 14:28:53,147: Val batch 8000: PER (avg): 0.1844 CTC Loss (avg): 18.1425 WER(1gram): 53.81% (n=64) time: 6.850
2026-01-04 14:28:53,148: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-04 14:28:53,148: t15.2023.08.13 val PER: 0.1476
2026-01-04 14:28:53,148: t15.2023.08.18 val PER: 0.1274
2026-01-04 14:28:53,148: t15.2023.08.20 val PER: 0.1406
2026-01-04 14:28:53,148: t15.2023.08.25 val PER: 0.1130
2026-01-04 14:28:53,148: t15.2023.08.27 val PER: 0.2170
2026-01-04 14:28:53,148: t15.2023.09.01 val PER: 0.1031
2026-01-04 14:28:53,148: t15.2023.09.03 val PER: 0.1841
2026-01-04 14:28:53,148: t15.2023.09.24 val PER: 0.1566
2026-01-04 14:28:53,148: t15.2023.09.29 val PER: 0.1506
2026-01-04 14:28:53,148: t15.2023.10.01 val PER: 0.2041
2026-01-04 14:28:53,148: t15.2023.10.06 val PER: 0.1130
2026-01-04 14:28:53,148: t15.2023.10.08 val PER: 0.2625
2026-01-04 14:28:53,149: t15.2023.10.13 val PER: 0.2428
2026-01-04 14:28:53,149: t15.2023.10.15 val PER: 0.1892
2026-01-04 14:28:53,149: t15.2023.10.20 val PER: 0.2081
2026-01-04 14:28:53,149: t15.2023.10.22 val PER: 0.1470
2026-01-04 14:28:53,149: t15.2023.11.03 val PER: 0.2049
2026-01-04 14:28:53,149: t15.2023.11.04 val PER: 0.0307
2026-01-04 14:28:53,149: t15.2023.11.17 val PER: 0.0591
2026-01-04 14:28:53,149: t15.2023.11.19 val PER: 0.0579
2026-01-04 14:28:53,149: t15.2023.11.26 val PER: 0.1797
2026-01-04 14:28:53,149: t15.2023.12.03 val PER: 0.1523
2026-01-04 14:28:53,149: t15.2023.12.08 val PER: 0.1438
2026-01-04 14:28:53,149: t15.2023.12.10 val PER: 0.1327
2026-01-04 14:28:53,149: t15.2023.12.17 val PER: 0.1726
2026-01-04 14:28:53,149: t15.2023.12.29 val PER: 0.1764
2026-01-04 14:28:53,150: t15.2024.02.25 val PER: 0.1447
2026-01-04 14:28:53,150: t15.2024.03.08 val PER: 0.2717
2026-01-04 14:28:53,150: t15.2024.03.15 val PER: 0.2414
2026-01-04 14:28:53,150: t15.2024.03.17 val PER: 0.1743
2026-01-04 14:28:53,150: t15.2024.05.10 val PER: 0.1887
2026-01-04 14:28:53,150: t15.2024.06.14 val PER: 0.2050
2026-01-04 14:28:53,150: t15.2024.07.19 val PER: 0.2881
2026-01-04 14:28:53,150: t15.2024.07.21 val PER: 0.1297
2026-01-04 14:28:53,150: t15.2024.07.28 val PER: 0.1529
2026-01-04 14:28:53,150: t15.2025.01.10 val PER: 0.3347
2026-01-04 14:28:53,150: t15.2025.01.12 val PER: 0.1824
2026-01-04 14:28:53,150: t15.2025.03.14 val PER: 0.3639
2026-01-04 14:28:53,150: t15.2025.03.16 val PER: 0.2173
2026-01-04 14:28:53,150: t15.2025.03.30 val PER: 0.3563
2026-01-04 14:28:53,150: t15.2025.04.13 val PER: 0.2439
2026-01-04 14:28:53,392: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_8000
2026-01-04 14:29:10,893: Train batch 8200: loss: 9.76 grad norm: 48.23 time: 0.054
2026-01-04 14:29:28,481: Train batch 8400: loss: 10.15 grad norm: 46.03 time: 0.066
2026-01-04 14:29:37,406: Running test after training batch: 8500
2026-01-04 14:29:37,528: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:29:42,846: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 14:29:42,878: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost et
2026-01-04 14:29:44,628: Val batch 8500: PER (avg): 0.1803 CTC Loss (avg): 17.8361 WER(1gram): 49.75% (n=64) time: 7.222
2026-01-04 14:29:44,628: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 14:29:44,629: t15.2023.08.13 val PER: 0.1445
2026-01-04 14:29:44,629: t15.2023.08.18 val PER: 0.1408
2026-01-04 14:29:44,629: t15.2023.08.20 val PER: 0.1287
2026-01-04 14:29:44,629: t15.2023.08.25 val PER: 0.1084
2026-01-04 14:29:44,629: t15.2023.08.27 val PER: 0.2074
2026-01-04 14:29:44,629: t15.2023.09.01 val PER: 0.1071
2026-01-04 14:29:44,629: t15.2023.09.03 val PER: 0.1924
2026-01-04 14:29:44,629: t15.2023.09.24 val PER: 0.1456
2026-01-04 14:29:44,629: t15.2023.09.29 val PER: 0.1544
2026-01-04 14:29:44,629: t15.2023.10.01 val PER: 0.1962
2026-01-04 14:29:44,629: t15.2023.10.06 val PER: 0.1119
2026-01-04 14:29:44,629: t15.2023.10.08 val PER: 0.2679
2026-01-04 14:29:44,630: t15.2023.10.13 val PER: 0.2382
2026-01-04 14:29:44,630: t15.2023.10.15 val PER: 0.1892
2026-01-04 14:29:44,630: t15.2023.10.20 val PER: 0.1980
2026-01-04 14:29:44,630: t15.2023.10.22 val PER: 0.1470
2026-01-04 14:29:44,630: t15.2023.11.03 val PER: 0.1940
2026-01-04 14:29:44,630: t15.2023.11.04 val PER: 0.0512
2026-01-04 14:29:44,630: t15.2023.11.17 val PER: 0.0575
2026-01-04 14:29:44,630: t15.2023.11.19 val PER: 0.0539
2026-01-04 14:29:44,631: t15.2023.11.26 val PER: 0.1681
2026-01-04 14:29:44,631: t15.2023.12.03 val PER: 0.1502
2026-01-04 14:29:44,631: t15.2023.12.08 val PER: 0.1378
2026-01-04 14:29:44,631: t15.2023.12.10 val PER: 0.1222
2026-01-04 14:29:44,631: t15.2023.12.17 val PER: 0.1611
2026-01-04 14:29:44,631: t15.2023.12.29 val PER: 0.1716
2026-01-04 14:29:44,631: t15.2024.02.25 val PER: 0.1362
2026-01-04 14:29:44,632: t15.2024.03.08 val PER: 0.2674
2026-01-04 14:29:44,632: t15.2024.03.15 val PER: 0.2314
2026-01-04 14:29:44,632: t15.2024.03.17 val PER: 0.1709
2026-01-04 14:29:44,632: t15.2024.05.10 val PER: 0.1976
2026-01-04 14:29:44,632: t15.2024.06.14 val PER: 0.1972
2026-01-04 14:29:44,632: t15.2024.07.19 val PER: 0.2736
2026-01-04 14:29:44,632: t15.2024.07.21 val PER: 0.1228
2026-01-04 14:29:44,632: t15.2024.07.28 val PER: 0.1699
2026-01-04 14:29:44,632: t15.2025.01.10 val PER: 0.3154
2026-01-04 14:29:44,632: t15.2025.01.12 val PER: 0.1878
2026-01-04 14:29:44,632: t15.2025.03.14 val PER: 0.3565
2026-01-04 14:29:44,633: t15.2025.03.16 val PER: 0.2107
2026-01-04 14:29:44,633: t15.2025.03.30 val PER: 0.3333
2026-01-04 14:29:44,633: t15.2025.04.13 val PER: 0.2268
2026-01-04 14:29:44,633: New best val WER(1gram) 52.28% --> 49.75%
2026-01-04 14:29:44,633: Checkpointing model
2026-01-04 14:29:45,270: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:29:45,524: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_8500
2026-01-04 14:29:54,771: Train batch 8600: loss: 16.10 grad norm: 64.26 time: 0.055
2026-01-04 14:30:13,140: Train batch 8800: loss: 15.07 grad norm: 58.20 time: 0.060
2026-01-04 14:30:30,516: Train batch 9000: loss: 15.72 grad norm: 64.30 time: 0.072
2026-01-04 14:30:30,516: Running test after training batch: 9000
2026-01-04 14:30:30,635: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:30:35,859: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 14:30:35,890: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-04 14:30:37,613: Val batch 9000: PER (avg): 0.1741 CTC Loss (avg): 17.2899 WER(1gram): 52.03% (n=64) time: 7.097
2026-01-04 14:30:37,614: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 14:30:37,614: t15.2023.08.13 val PER: 0.1383
2026-01-04 14:30:37,615: t15.2023.08.18 val PER: 0.1241
2026-01-04 14:30:37,615: t15.2023.08.20 val PER: 0.1334
2026-01-04 14:30:37,615: t15.2023.08.25 val PER: 0.1009
2026-01-04 14:30:37,615: t15.2023.08.27 val PER: 0.2122
2026-01-04 14:30:37,615: t15.2023.09.01 val PER: 0.0958
2026-01-04 14:30:37,615: t15.2023.09.03 val PER: 0.1888
2026-01-04 14:30:37,615: t15.2023.09.24 val PER: 0.1493
2026-01-04 14:30:37,615: t15.2023.09.29 val PER: 0.1474
2026-01-04 14:30:37,615: t15.2023.10.01 val PER: 0.1909
2026-01-04 14:30:37,615: t15.2023.10.06 val PER: 0.0969
2026-01-04 14:30:37,616: t15.2023.10.08 val PER: 0.2639
2026-01-04 14:30:37,616: t15.2023.10.13 val PER: 0.2273
2026-01-04 14:30:37,616: t15.2023.10.15 val PER: 0.1780
2026-01-04 14:30:37,616: t15.2023.10.20 val PER: 0.1779
2026-01-04 14:30:37,616: t15.2023.10.22 val PER: 0.1347
2026-01-04 14:30:37,616: t15.2023.11.03 val PER: 0.2035
2026-01-04 14:30:37,616: t15.2023.11.04 val PER: 0.0341
2026-01-04 14:30:37,616: t15.2023.11.17 val PER: 0.0498
2026-01-04 14:30:37,616: t15.2023.11.19 val PER: 0.0519
2026-01-04 14:30:37,616: t15.2023.11.26 val PER: 0.1638
2026-01-04 14:30:37,616: t15.2023.12.03 val PER: 0.1408
2026-01-04 14:30:37,617: t15.2023.12.08 val PER: 0.1292
2026-01-04 14:30:37,617: t15.2023.12.10 val PER: 0.1143
2026-01-04 14:30:37,617: t15.2023.12.17 val PER: 0.1663
2026-01-04 14:30:37,617: t15.2023.12.29 val PER: 0.1544
2026-01-04 14:30:37,617: t15.2024.02.25 val PER: 0.1419
2026-01-04 14:30:37,617: t15.2024.03.08 val PER: 0.2603
2026-01-04 14:30:37,617: t15.2024.03.15 val PER: 0.2283
2026-01-04 14:30:37,617: t15.2024.03.17 val PER: 0.1709
2026-01-04 14:30:37,617: t15.2024.05.10 val PER: 0.1828
2026-01-04 14:30:37,617: t15.2024.06.14 val PER: 0.1845
2026-01-04 14:30:37,617: t15.2024.07.19 val PER: 0.2736
2026-01-04 14:30:37,617: t15.2024.07.21 val PER: 0.1166
2026-01-04 14:30:37,617: t15.2024.07.28 val PER: 0.1566
2026-01-04 14:30:37,617: t15.2025.01.10 val PER: 0.3168
2026-01-04 14:30:37,617: t15.2025.01.12 val PER: 0.1717
2026-01-04 14:30:37,617: t15.2025.03.14 val PER: 0.3462
2026-01-04 14:30:37,618: t15.2025.03.16 val PER: 0.2042
2026-01-04 14:30:37,618: t15.2025.03.30 val PER: 0.3230
2026-01-04 14:30:37,618: t15.2025.04.13 val PER: 0.2411
2026-01-04 14:30:37,880: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_9000
2026-01-04 14:30:55,477: Train batch 9200: loss: 10.96 grad norm: 51.34 time: 0.056
2026-01-04 14:31:12,418: Train batch 9400: loss: 7.65 grad norm: 50.37 time: 0.067
2026-01-04 14:31:21,212: Running test after training batch: 9500
2026-01-04 14:31:21,383: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:31:26,186: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:31:26,217: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-04 14:31:27,927: Val batch 9500: PER (avg): 0.1741 CTC Loss (avg): 17.1869 WER(1gram): 50.76% (n=64) time: 6.714
2026-01-04 14:31:27,927: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 14:31:27,927: t15.2023.08.13 val PER: 0.1258
2026-01-04 14:31:27,927: t15.2023.08.18 val PER: 0.1241
2026-01-04 14:31:27,927: t15.2023.08.20 val PER: 0.1350
2026-01-04 14:31:27,928: t15.2023.08.25 val PER: 0.1039
2026-01-04 14:31:27,928: t15.2023.08.27 val PER: 0.1977
2026-01-04 14:31:27,928: t15.2023.09.01 val PER: 0.0942
2026-01-04 14:31:27,928: t15.2023.09.03 val PER: 0.1817
2026-01-04 14:31:27,928: t15.2023.09.24 val PER: 0.1359
2026-01-04 14:31:27,928: t15.2023.09.29 val PER: 0.1468
2026-01-04 14:31:27,928: t15.2023.10.01 val PER: 0.1942
2026-01-04 14:31:27,928: t15.2023.10.06 val PER: 0.0990
2026-01-04 14:31:27,928: t15.2023.10.08 val PER: 0.2598
2026-01-04 14:31:27,928: t15.2023.10.13 val PER: 0.2296
2026-01-04 14:31:27,928: t15.2023.10.15 val PER: 0.1833
2026-01-04 14:31:27,929: t15.2023.10.20 val PER: 0.1879
2026-01-04 14:31:27,929: t15.2023.10.22 val PER: 0.1292
2026-01-04 14:31:27,929: t15.2023.11.03 val PER: 0.1934
2026-01-04 14:31:27,929: t15.2023.11.04 val PER: 0.0341
2026-01-04 14:31:27,929: t15.2023.11.17 val PER: 0.0529
2026-01-04 14:31:27,929: t15.2023.11.19 val PER: 0.0399
2026-01-04 14:31:27,929: t15.2023.11.26 val PER: 0.1609
2026-01-04 14:31:27,929: t15.2023.12.03 val PER: 0.1502
2026-01-04 14:31:27,929: t15.2023.12.08 val PER: 0.1338
2026-01-04 14:31:27,929: t15.2023.12.10 val PER: 0.1156
2026-01-04 14:31:27,929: t15.2023.12.17 val PER: 0.1705
2026-01-04 14:31:27,929: t15.2023.12.29 val PER: 0.1572
2026-01-04 14:31:27,929: t15.2024.02.25 val PER: 0.1250
2026-01-04 14:31:27,929: t15.2024.03.08 val PER: 0.2617
2026-01-04 14:31:27,929: t15.2024.03.15 val PER: 0.2239
2026-01-04 14:31:27,929: t15.2024.03.17 val PER: 0.1722
2026-01-04 14:31:27,930: t15.2024.05.10 val PER: 0.1857
2026-01-04 14:31:27,930: t15.2024.06.14 val PER: 0.1798
2026-01-04 14:31:27,930: t15.2024.07.19 val PER: 0.2709
2026-01-04 14:31:27,930: t15.2024.07.21 val PER: 0.1241
2026-01-04 14:31:27,930: t15.2024.07.28 val PER: 0.1588
2026-01-04 14:31:27,930: t15.2025.01.10 val PER: 0.3223
2026-01-04 14:31:27,930: t15.2025.01.12 val PER: 0.1809
2026-01-04 14:31:27,930: t15.2025.03.14 val PER: 0.3802
2026-01-04 14:31:27,930: t15.2025.03.16 val PER: 0.2081
2026-01-04 14:31:27,930: t15.2025.03.30 val PER: 0.3207
2026-01-04 14:31:27,930: t15.2025.04.13 val PER: 0.2254
2026-01-04 14:31:28,196: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_9500
2026-01-04 14:31:36,837: Train batch 9600: loss: 8.32 grad norm: 46.34 time: 0.072
2026-01-04 14:31:54,007: Train batch 9800: loss: 12.44 grad norm: 56.69 time: 0.063
2026-01-04 14:32:11,207: Train batch 10000: loss: 5.82 grad norm: 35.46 time: 0.061
2026-01-04 14:32:11,208: Running test after training batch: 10000
2026-01-04 14:32:11,310: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:32:15,997: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:32:16,029: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 14:32:17,759: Val batch 10000: PER (avg): 0.1710 CTC Loss (avg): 16.9216 WER(1gram): 53.81% (n=64) time: 6.551
2026-01-04 14:32:17,759: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 14:32:17,759: t15.2023.08.13 val PER: 0.1299
2026-01-04 14:32:17,759: t15.2023.08.18 val PER: 0.1274
2026-01-04 14:32:17,759: t15.2023.08.20 val PER: 0.1255
2026-01-04 14:32:17,759: t15.2023.08.25 val PER: 0.1069
2026-01-04 14:32:17,759: t15.2023.08.27 val PER: 0.1994
2026-01-04 14:32:17,760: t15.2023.09.01 val PER: 0.0877
2026-01-04 14:32:17,760: t15.2023.09.03 val PER: 0.1960
2026-01-04 14:32:17,760: t15.2023.09.24 val PER: 0.1359
2026-01-04 14:32:17,760: t15.2023.09.29 val PER: 0.1544
2026-01-04 14:32:17,760: t15.2023.10.01 val PER: 0.1843
2026-01-04 14:32:17,760: t15.2023.10.06 val PER: 0.1023
2026-01-04 14:32:17,760: t15.2023.10.08 val PER: 0.2625
2026-01-04 14:32:17,760: t15.2023.10.13 val PER: 0.2242
2026-01-04 14:32:17,760: t15.2023.10.15 val PER: 0.1668
2026-01-04 14:32:17,760: t15.2023.10.20 val PER: 0.1946
2026-01-04 14:32:17,760: t15.2023.10.22 val PER: 0.1336
2026-01-04 14:32:17,764: t15.2023.11.03 val PER: 0.1879
2026-01-04 14:32:17,765: t15.2023.11.04 val PER: 0.0341
2026-01-04 14:32:17,765: t15.2023.11.17 val PER: 0.0513
2026-01-04 14:32:17,765: t15.2023.11.19 val PER: 0.0519
2026-01-04 14:32:17,765: t15.2023.11.26 val PER: 0.1457
2026-01-04 14:32:17,765: t15.2023.12.03 val PER: 0.1397
2026-01-04 14:32:17,765: t15.2023.12.08 val PER: 0.1378
2026-01-04 14:32:17,765: t15.2023.12.10 val PER: 0.1261
2026-01-04 14:32:17,765: t15.2023.12.17 val PER: 0.1705
2026-01-04 14:32:17,765: t15.2023.12.29 val PER: 0.1393
2026-01-04 14:32:17,765: t15.2024.02.25 val PER: 0.1475
2026-01-04 14:32:17,766: t15.2024.03.08 val PER: 0.2475
2026-01-04 14:32:17,766: t15.2024.03.15 val PER: 0.2195
2026-01-04 14:32:17,766: t15.2024.03.17 val PER: 0.1597
2026-01-04 14:32:17,766: t15.2024.05.10 val PER: 0.1872
2026-01-04 14:32:17,766: t15.2024.06.14 val PER: 0.1845
2026-01-04 14:32:17,766: t15.2024.07.19 val PER: 0.2755
2026-01-04 14:32:17,767: t15.2024.07.21 val PER: 0.1145
2026-01-04 14:32:17,767: t15.2024.07.28 val PER: 0.1551
2026-01-04 14:32:17,767: t15.2025.01.10 val PER: 0.3127
2026-01-04 14:32:17,767: t15.2025.01.12 val PER: 0.1663
2026-01-04 14:32:17,767: t15.2025.03.14 val PER: 0.3550
2026-01-04 14:32:17,767: t15.2025.03.16 val PER: 0.2120
2026-01-04 14:32:17,767: t15.2025.03.30 val PER: 0.3138
2026-01-04 14:32:17,768: t15.2025.04.13 val PER: 0.2397
2026-01-04 14:32:18,028: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_10000
2026-01-04 14:32:35,094: Train batch 10200: loss: 6.30 grad norm: 37.79 time: 0.050
2026-01-04 14:32:52,311: Train batch 10400: loss: 9.39 grad norm: 58.81 time: 0.072
2026-01-04 14:33:01,212: Running test after training batch: 10500
2026-01-04 14:33:01,351: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:33:06,132: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:33:06,162: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-04 14:33:07,888: Val batch 10500: PER (avg): 0.1666 CTC Loss (avg): 16.7060 WER(1gram): 47.97% (n=64) time: 6.677
2026-01-04 14:33:07,889: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 14:33:07,889: t15.2023.08.13 val PER: 0.1258
2026-01-04 14:33:07,889: t15.2023.08.18 val PER: 0.1199
2026-01-04 14:33:07,889: t15.2023.08.20 val PER: 0.1263
2026-01-04 14:33:07,889: t15.2023.08.25 val PER: 0.1114
2026-01-04 14:33:07,889: t15.2023.08.27 val PER: 0.1977
2026-01-04 14:33:07,889: t15.2023.09.01 val PER: 0.0917
2026-01-04 14:33:07,889: t15.2023.09.03 val PER: 0.1829
2026-01-04 14:33:07,889: t15.2023.09.24 val PER: 0.1359
2026-01-04 14:33:07,889: t15.2023.09.29 val PER: 0.1461
2026-01-04 14:33:07,890: t15.2023.10.01 val PER: 0.1896
2026-01-04 14:33:07,890: t15.2023.10.06 val PER: 0.0926
2026-01-04 14:33:07,890: t15.2023.10.08 val PER: 0.2530
2026-01-04 14:33:07,890: t15.2023.10.13 val PER: 0.2095
2026-01-04 14:33:07,890: t15.2023.10.15 val PER: 0.1734
2026-01-04 14:33:07,890: t15.2023.10.20 val PER: 0.2013
2026-01-04 14:33:07,890: t15.2023.10.22 val PER: 0.1247
2026-01-04 14:33:07,890: t15.2023.11.03 val PER: 0.1913
2026-01-04 14:33:07,890: t15.2023.11.04 val PER: 0.0444
2026-01-04 14:33:07,890: t15.2023.11.17 val PER: 0.0498
2026-01-04 14:33:07,891: t15.2023.11.19 val PER: 0.0479
2026-01-04 14:33:07,891: t15.2023.11.26 val PER: 0.1406
2026-01-04 14:33:07,891: t15.2023.12.03 val PER: 0.1387
2026-01-04 14:33:07,891: t15.2023.12.08 val PER: 0.1278
2026-01-04 14:33:07,891: t15.2023.12.10 val PER: 0.1104
2026-01-04 14:33:07,891: t15.2023.12.17 val PER: 0.1528
2026-01-04 14:33:07,891: t15.2023.12.29 val PER: 0.1510
2026-01-04 14:33:07,891: t15.2024.02.25 val PER: 0.1390
2026-01-04 14:33:07,891: t15.2024.03.08 val PER: 0.2361
2026-01-04 14:33:07,891: t15.2024.03.15 val PER: 0.2133
2026-01-04 14:33:07,891: t15.2024.03.17 val PER: 0.1667
2026-01-04 14:33:07,891: t15.2024.05.10 val PER: 0.1783
2026-01-04 14:33:07,891: t15.2024.06.14 val PER: 0.1767
2026-01-04 14:33:07,891: t15.2024.07.19 val PER: 0.2564
2026-01-04 14:33:07,891: t15.2024.07.21 val PER: 0.1138
2026-01-04 14:33:07,891: t15.2024.07.28 val PER: 0.1397
2026-01-04 14:33:07,892: t15.2025.01.10 val PER: 0.3168
2026-01-04 14:33:07,892: t15.2025.01.12 val PER: 0.1578
2026-01-04 14:33:07,892: t15.2025.03.14 val PER: 0.3550
2026-01-04 14:33:07,892: t15.2025.03.16 val PER: 0.1976
2026-01-04 14:33:07,892: t15.2025.03.30 val PER: 0.3218
2026-01-04 14:33:07,892: t15.2025.04.13 val PER: 0.2197
2026-01-04 14:33:07,893: New best val WER(1gram) 49.75% --> 47.97%
2026-01-04 14:33:07,893: Checkpointing model
2026-01-04 14:33:08,515: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:33:08,794: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_10500
2026-01-04 14:33:17,770: Train batch 10600: loss: 9.36 grad norm: 56.35 time: 0.071
2026-01-04 14:33:35,606: Train batch 10800: loss: 14.88 grad norm: 64.80 time: 0.065
2026-01-04 14:33:53,395: Train batch 11000: loss: 15.18 grad norm: 64.26 time: 0.056
2026-01-04 14:33:53,396: Running test after training batch: 11000
2026-01-04 14:33:53,802: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:33:59,576: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:33:59,608: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:34:01,350: Val batch 11000: PER (avg): 0.1644 CTC Loss (avg): 16.5037 WER(1gram): 47.46% (n=64) time: 7.954
2026-01-04 14:34:01,350: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 14:34:01,350: t15.2023.08.13 val PER: 0.1195
2026-01-04 14:34:01,350: t15.2023.08.18 val PER: 0.1249
2026-01-04 14:34:01,351: t15.2023.08.20 val PER: 0.1231
2026-01-04 14:34:01,351: t15.2023.08.25 val PER: 0.1024
2026-01-04 14:34:01,351: t15.2023.08.27 val PER: 0.1929
2026-01-04 14:34:01,351: t15.2023.09.01 val PER: 0.0779
2026-01-04 14:34:01,351: t15.2023.09.03 val PER: 0.1876
2026-01-04 14:34:01,351: t15.2023.09.24 val PER: 0.1432
2026-01-04 14:34:01,351: t15.2023.09.29 val PER: 0.1449
2026-01-04 14:34:01,351: t15.2023.10.01 val PER: 0.1915
2026-01-04 14:34:01,351: t15.2023.10.06 val PER: 0.0969
2026-01-04 14:34:01,352: t15.2023.10.08 val PER: 0.2571
2026-01-04 14:34:01,352: t15.2023.10.13 val PER: 0.2172
2026-01-04 14:34:01,352: t15.2023.10.15 val PER: 0.1655
2026-01-04 14:34:01,352: t15.2023.10.20 val PER: 0.1946
2026-01-04 14:34:01,352: t15.2023.10.22 val PER: 0.1236
2026-01-04 14:34:01,352: t15.2023.11.03 val PER: 0.1940
2026-01-04 14:34:01,352: t15.2023.11.04 val PER: 0.0307
2026-01-04 14:34:01,352: t15.2023.11.17 val PER: 0.0498
2026-01-04 14:34:01,352: t15.2023.11.19 val PER: 0.0439
2026-01-04 14:34:01,352: t15.2023.11.26 val PER: 0.1399
2026-01-04 14:34:01,352: t15.2023.12.03 val PER: 0.1355
2026-01-04 14:34:01,353: t15.2023.12.08 val PER: 0.1158
2026-01-04 14:34:01,353: t15.2023.12.10 val PER: 0.0972
2026-01-04 14:34:01,353: t15.2023.12.17 val PER: 0.1507
2026-01-04 14:34:01,353: t15.2023.12.29 val PER: 0.1421
2026-01-04 14:34:01,353: t15.2024.02.25 val PER: 0.1222
2026-01-04 14:34:01,353: t15.2024.03.08 val PER: 0.2347
2026-01-04 14:34:01,353: t15.2024.03.15 val PER: 0.2145
2026-01-04 14:34:01,353: t15.2024.03.17 val PER: 0.1590
2026-01-04 14:34:01,353: t15.2024.05.10 val PER: 0.1783
2026-01-04 14:34:01,353: t15.2024.06.14 val PER: 0.1688
2026-01-04 14:34:01,354: t15.2024.07.19 val PER: 0.2597
2026-01-04 14:34:01,354: t15.2024.07.21 val PER: 0.1117
2026-01-04 14:34:01,354: t15.2024.07.28 val PER: 0.1485
2026-01-04 14:34:01,354: t15.2025.01.10 val PER: 0.3099
2026-01-04 14:34:01,354: t15.2025.01.12 val PER: 0.1594
2026-01-04 14:34:01,354: t15.2025.03.14 val PER: 0.3462
2026-01-04 14:34:01,354: t15.2025.03.16 val PER: 0.1976
2026-01-04 14:34:01,354: t15.2025.03.30 val PER: 0.3092
2026-01-04 14:34:01,354: t15.2025.04.13 val PER: 0.2340
2026-01-04 14:34:01,355: New best val WER(1gram) 47.97% --> 47.46%
2026-01-04 14:34:01,355: Checkpointing model
2026-01-04 14:34:01,972: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:34:02,252: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_11000
2026-01-04 14:34:19,555: Train batch 11200: loss: 10.80 grad norm: 51.60 time: 0.071
2026-01-04 14:34:36,480: Train batch 11400: loss: 9.59 grad norm: 52.93 time: 0.057
2026-01-04 14:34:45,156: Running test after training batch: 11500
2026-01-04 14:34:45,399: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:34:50,115: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:34:50,145: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 14:34:51,886: Val batch 11500: PER (avg): 0.1624 CTC Loss (avg): 16.3619 WER(1gram): 48.98% (n=64) time: 6.730
2026-01-04 14:34:51,887: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 14:34:51,887: t15.2023.08.13 val PER: 0.1216
2026-01-04 14:34:51,887: t15.2023.08.18 val PER: 0.1174
2026-01-04 14:34:51,887: t15.2023.08.20 val PER: 0.1168
2026-01-04 14:34:51,887: t15.2023.08.25 val PER: 0.1039
2026-01-04 14:34:51,887: t15.2023.08.27 val PER: 0.1913
2026-01-04 14:34:51,887: t15.2023.09.01 val PER: 0.0877
2026-01-04 14:34:51,887: t15.2023.09.03 val PER: 0.1781
2026-01-04 14:34:51,887: t15.2023.09.24 val PER: 0.1396
2026-01-04 14:34:51,888: t15.2023.09.29 val PER: 0.1359
2026-01-04 14:34:51,888: t15.2023.10.01 val PER: 0.1790
2026-01-04 14:34:51,888: t15.2023.10.06 val PER: 0.0915
2026-01-04 14:34:51,888: t15.2023.10.08 val PER: 0.2585
2026-01-04 14:34:51,888: t15.2023.10.13 val PER: 0.2102
2026-01-04 14:34:51,888: t15.2023.10.15 val PER: 0.1595
2026-01-04 14:34:51,888: t15.2023.10.20 val PER: 0.1846
2026-01-04 14:34:51,888: t15.2023.10.22 val PER: 0.1336
2026-01-04 14:34:51,888: t15.2023.11.03 val PER: 0.1866
2026-01-04 14:34:51,888: t15.2023.11.04 val PER: 0.0341
2026-01-04 14:34:51,888: t15.2023.11.17 val PER: 0.0482
2026-01-04 14:34:51,888: t15.2023.11.19 val PER: 0.0519
2026-01-04 14:34:51,888: t15.2023.11.26 val PER: 0.1362
2026-01-04 14:34:51,888: t15.2023.12.03 val PER: 0.1313
2026-01-04 14:34:51,888: t15.2023.12.08 val PER: 0.1192
2026-01-04 14:34:51,888: t15.2023.12.10 val PER: 0.1117
2026-01-04 14:34:51,889: t15.2023.12.17 val PER: 0.1455
2026-01-04 14:34:51,889: t15.2023.12.29 val PER: 0.1428
2026-01-04 14:34:51,889: t15.2024.02.25 val PER: 0.1166
2026-01-04 14:34:51,889: t15.2024.03.08 val PER: 0.2319
2026-01-04 14:34:51,889: t15.2024.03.15 val PER: 0.2076
2026-01-04 14:34:51,889: t15.2024.03.17 val PER: 0.1576
2026-01-04 14:34:51,889: t15.2024.05.10 val PER: 0.1649
2026-01-04 14:34:51,889: t15.2024.06.14 val PER: 0.1814
2026-01-04 14:34:51,889: t15.2024.07.19 val PER: 0.2577
2026-01-04 14:34:51,889: t15.2024.07.21 val PER: 0.1062
2026-01-04 14:34:51,889: t15.2024.07.28 val PER: 0.1485
2026-01-04 14:34:51,889: t15.2025.01.10 val PER: 0.3168
2026-01-04 14:34:51,889: t15.2025.01.12 val PER: 0.1640
2026-01-04 14:34:51,889: t15.2025.03.14 val PER: 0.3565
2026-01-04 14:34:51,889: t15.2025.03.16 val PER: 0.2042
2026-01-04 14:34:51,889: t15.2025.03.30 val PER: 0.3069
2026-01-04 14:34:51,889: t15.2025.04.13 val PER: 0.2240
2026-01-04 14:34:52,153: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_11500
2026-01-04 14:35:00,780: Train batch 11600: loss: 11.20 grad norm: 48.32 time: 0.060
2026-01-04 14:35:17,831: Train batch 11800: loss: 6.58 grad norm: 43.37 time: 0.044
2026-01-04 14:35:34,915: Train batch 12000: loss: 13.71 grad norm: 51.99 time: 0.070
2026-01-04 14:35:34,916: Running test after training batch: 12000
2026-01-04 14:35:35,013: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:35:39,937: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:35:39,973: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 14:35:41,924: Val batch 12000: PER (avg): 0.1583 CTC Loss (avg): 16.0719 WER(1gram): 49.49% (n=64) time: 7.008
2026-01-04 14:35:41,925: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 14:35:41,925: t15.2023.08.13 val PER: 0.1185
2026-01-04 14:35:41,925: t15.2023.08.18 val PER: 0.1098
2026-01-04 14:35:41,925: t15.2023.08.20 val PER: 0.1176
2026-01-04 14:35:41,925: t15.2023.08.25 val PER: 0.0994
2026-01-04 14:35:41,925: t15.2023.08.27 val PER: 0.1865
2026-01-04 14:35:41,926: t15.2023.09.01 val PER: 0.0836
2026-01-04 14:35:41,926: t15.2023.09.03 val PER: 0.1675
2026-01-04 14:35:41,926: t15.2023.09.24 val PER: 0.1274
2026-01-04 14:35:41,926: t15.2023.09.29 val PER: 0.1417
2026-01-04 14:35:41,926: t15.2023.10.01 val PER: 0.1803
2026-01-04 14:35:41,926: t15.2023.10.06 val PER: 0.0893
2026-01-04 14:35:41,926: t15.2023.10.08 val PER: 0.2544
2026-01-04 14:35:41,926: t15.2023.10.13 val PER: 0.2087
2026-01-04 14:35:41,926: t15.2023.10.15 val PER: 0.1562
2026-01-04 14:35:41,927: t15.2023.10.20 val PER: 0.1779
2026-01-04 14:35:41,927: t15.2023.10.22 val PER: 0.1225
2026-01-04 14:35:41,927: t15.2023.11.03 val PER: 0.1839
2026-01-04 14:35:41,927: t15.2023.11.04 val PER: 0.0375
2026-01-04 14:35:41,927: t15.2023.11.17 val PER: 0.0451
2026-01-04 14:35:41,927: t15.2023.11.19 val PER: 0.0399
2026-01-04 14:35:41,927: t15.2023.11.26 val PER: 0.1217
2026-01-04 14:35:41,927: t15.2023.12.03 val PER: 0.1218
2026-01-04 14:35:41,927: t15.2023.12.08 val PER: 0.1105
2026-01-04 14:35:41,927: t15.2023.12.10 val PER: 0.0946
2026-01-04 14:35:41,928: t15.2023.12.17 val PER: 0.1466
2026-01-04 14:35:41,928: t15.2023.12.29 val PER: 0.1359
2026-01-04 14:35:41,928: t15.2024.02.25 val PER: 0.1152
2026-01-04 14:35:41,928: t15.2024.03.08 val PER: 0.2461
2026-01-04 14:35:41,928: t15.2024.03.15 val PER: 0.2058
2026-01-04 14:35:41,928: t15.2024.03.17 val PER: 0.1457
2026-01-04 14:35:41,928: t15.2024.05.10 val PER: 0.1842
2026-01-04 14:35:41,932: t15.2024.06.14 val PER: 0.1830
2026-01-04 14:35:41,933: t15.2024.07.19 val PER: 0.2498
2026-01-04 14:35:41,933: t15.2024.07.21 val PER: 0.1041
2026-01-04 14:35:41,933: t15.2024.07.28 val PER: 0.1368
2026-01-04 14:35:41,933: t15.2025.01.10 val PER: 0.3099
2026-01-04 14:35:41,933: t15.2025.01.12 val PER: 0.1578
2026-01-04 14:35:41,933: t15.2025.03.14 val PER: 0.3669
2026-01-04 14:35:41,933: t15.2025.03.16 val PER: 0.2107
2026-01-04 14:35:41,933: t15.2025.03.30 val PER: 0.2989
2026-01-04 14:35:41,933: t15.2025.04.13 val PER: 0.2183
2026-01-04 14:35:42,211: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_12000
2026-01-04 14:35:59,399: Train batch 12200: loss: 6.05 grad norm: 42.57 time: 0.065
2026-01-04 14:36:16,874: Train batch 12400: loss: 4.93 grad norm: 34.75 time: 0.040
2026-01-04 14:36:25,829: Running test after training batch: 12500
2026-01-04 14:36:25,985: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:36:30,686: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 14:36:30,719: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 14:36:32,525: Val batch 12500: PER (avg): 0.1562 CTC Loss (avg): 15.9842 WER(1gram): 47.46% (n=64) time: 6.695
2026-01-04 14:36:32,525: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 14:36:32,525: t15.2023.08.13 val PER: 0.1268
2026-01-04 14:36:32,525: t15.2023.08.18 val PER: 0.1140
2026-01-04 14:36:32,525: t15.2023.08.20 val PER: 0.1096
2026-01-04 14:36:32,526: t15.2023.08.25 val PER: 0.0979
2026-01-04 14:36:32,526: t15.2023.08.27 val PER: 0.1913
2026-01-04 14:36:32,526: t15.2023.09.01 val PER: 0.0771
2026-01-04 14:36:32,526: t15.2023.09.03 val PER: 0.1686
2026-01-04 14:36:32,526: t15.2023.09.24 val PER: 0.1262
2026-01-04 14:36:32,526: t15.2023.09.29 val PER: 0.1327
2026-01-04 14:36:32,526: t15.2023.10.01 val PER: 0.1790
2026-01-04 14:36:32,526: t15.2023.10.06 val PER: 0.0829
2026-01-04 14:36:32,526: t15.2023.10.08 val PER: 0.2436
2026-01-04 14:36:32,527: t15.2023.10.13 val PER: 0.2149
2026-01-04 14:36:32,527: t15.2023.10.15 val PER: 0.1536
2026-01-04 14:36:32,527: t15.2023.10.20 val PER: 0.1879
2026-01-04 14:36:32,527: t15.2023.10.22 val PER: 0.1180
2026-01-04 14:36:32,527: t15.2023.11.03 val PER: 0.1791
2026-01-04 14:36:32,527: t15.2023.11.04 val PER: 0.0307
2026-01-04 14:36:32,527: t15.2023.11.17 val PER: 0.0482
2026-01-04 14:36:32,527: t15.2023.11.19 val PER: 0.0439
2026-01-04 14:36:32,527: t15.2023.11.26 val PER: 0.1312
2026-01-04 14:36:32,527: t15.2023.12.03 val PER: 0.1282
2026-01-04 14:36:32,527: t15.2023.12.08 val PER: 0.1045
2026-01-04 14:36:32,527: t15.2023.12.10 val PER: 0.0972
2026-01-04 14:36:32,528: t15.2023.12.17 val PER: 0.1497
2026-01-04 14:36:32,528: t15.2023.12.29 val PER: 0.1338
2026-01-04 14:36:32,528: t15.2024.02.25 val PER: 0.1053
2026-01-04 14:36:32,528: t15.2024.03.08 val PER: 0.2176
2026-01-04 14:36:32,528: t15.2024.03.15 val PER: 0.2070
2026-01-04 14:36:32,528: t15.2024.03.17 val PER: 0.1457
2026-01-04 14:36:32,528: t15.2024.05.10 val PER: 0.1738
2026-01-04 14:36:32,528: t15.2024.06.14 val PER: 0.1703
2026-01-04 14:36:32,528: t15.2024.07.19 val PER: 0.2492
2026-01-04 14:36:32,528: t15.2024.07.21 val PER: 0.0986
2026-01-04 14:36:32,528: t15.2024.07.28 val PER: 0.1360
2026-01-04 14:36:32,528: t15.2025.01.10 val PER: 0.3140
2026-01-04 14:36:32,528: t15.2025.01.12 val PER: 0.1501
2026-01-04 14:36:32,528: t15.2025.03.14 val PER: 0.3639
2026-01-04 14:36:32,529: t15.2025.03.16 val PER: 0.1924
2026-01-04 14:36:32,529: t15.2025.03.30 val PER: 0.3069
2026-01-04 14:36:32,529: t15.2025.04.13 val PER: 0.2211
2026-01-04 14:36:32,806: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_12500
2026-01-04 14:36:41,296: Train batch 12600: loss: 7.77 grad norm: 41.76 time: 0.057
2026-01-04 14:36:59,591: Train batch 12800: loss: 5.82 grad norm: 38.77 time: 0.052
2026-01-04 14:37:17,734: Train batch 13000: loss: 6.62 grad norm: 42.06 time: 0.066
2026-01-04 14:37:17,734: Running test after training batch: 13000
2026-01-04 14:37:17,839: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:37:23,892: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:37:23,925: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:37:25,672: Val batch 13000: PER (avg): 0.1552 CTC Loss (avg): 15.7608 WER(1gram): 46.70% (n=64) time: 7.938
2026-01-04 14:37:25,673: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 14:37:25,673: t15.2023.08.13 val PER: 0.1206
2026-01-04 14:37:25,673: t15.2023.08.18 val PER: 0.1056
2026-01-04 14:37:25,673: t15.2023.08.20 val PER: 0.1088
2026-01-04 14:37:25,673: t15.2023.08.25 val PER: 0.0934
2026-01-04 14:37:25,673: t15.2023.08.27 val PER: 0.1881
2026-01-04 14:37:25,673: t15.2023.09.01 val PER: 0.0820
2026-01-04 14:37:25,673: t15.2023.09.03 val PER: 0.1770
2026-01-04 14:37:25,673: t15.2023.09.24 val PER: 0.1286
2026-01-04 14:37:25,674: t15.2023.09.29 val PER: 0.1334
2026-01-04 14:37:25,674: t15.2023.10.01 val PER: 0.1717
2026-01-04 14:37:25,674: t15.2023.10.06 val PER: 0.0904
2026-01-04 14:37:25,674: t15.2023.10.08 val PER: 0.2476
2026-01-04 14:37:25,674: t15.2023.10.13 val PER: 0.2009
2026-01-04 14:37:25,674: t15.2023.10.15 val PER: 0.1602
2026-01-04 14:37:25,674: t15.2023.10.20 val PER: 0.1846
2026-01-04 14:37:25,674: t15.2023.10.22 val PER: 0.1203
2026-01-04 14:37:25,674: t15.2023.11.03 val PER: 0.1771
2026-01-04 14:37:25,674: t15.2023.11.04 val PER: 0.0307
2026-01-04 14:37:25,674: t15.2023.11.17 val PER: 0.0451
2026-01-04 14:37:25,675: t15.2023.11.19 val PER: 0.0439
2026-01-04 14:37:25,675: t15.2023.11.26 val PER: 0.1239
2026-01-04 14:37:25,675: t15.2023.12.03 val PER: 0.1282
2026-01-04 14:37:25,675: t15.2023.12.08 val PER: 0.1072
2026-01-04 14:37:25,675: t15.2023.12.10 val PER: 0.0972
2026-01-04 14:37:25,675: t15.2023.12.17 val PER: 0.1486
2026-01-04 14:37:25,675: t15.2023.12.29 val PER: 0.1386
2026-01-04 14:37:25,675: t15.2024.02.25 val PER: 0.1138
2026-01-04 14:37:25,675: t15.2024.03.08 val PER: 0.2347
2026-01-04 14:37:25,675: t15.2024.03.15 val PER: 0.2095
2026-01-04 14:37:25,675: t15.2024.03.17 val PER: 0.1346
2026-01-04 14:37:25,675: t15.2024.05.10 val PER: 0.1605
2026-01-04 14:37:25,676: t15.2024.06.14 val PER: 0.1751
2026-01-04 14:37:25,676: t15.2024.07.19 val PER: 0.2531
2026-01-04 14:37:25,676: t15.2024.07.21 val PER: 0.0945
2026-01-04 14:37:25,676: t15.2024.07.28 val PER: 0.1434
2026-01-04 14:37:25,676: t15.2025.01.10 val PER: 0.2961
2026-01-04 14:37:25,676: t15.2025.01.12 val PER: 0.1509
2026-01-04 14:37:25,676: t15.2025.03.14 val PER: 0.3432
2026-01-04 14:37:25,676: t15.2025.03.16 val PER: 0.1832
2026-01-04 14:37:25,676: t15.2025.03.30 val PER: 0.3034
2026-01-04 14:37:25,676: t15.2025.04.13 val PER: 0.2297
2026-01-04 14:37:25,677: New best val WER(1gram) 47.46% --> 46.70%
2026-01-04 14:37:25,677: Checkpointing model
2026-01-04 14:37:26,306: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:37:26,585: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_13000
2026-01-04 14:37:43,720: Train batch 13200: loss: 12.58 grad norm: 59.42 time: 0.054
2026-01-04 14:38:01,230: Train batch 13400: loss: 8.65 grad norm: 55.87 time: 0.062
2026-01-04 14:38:09,897: Running test after training batch: 13500
2026-01-04 14:38:10,023: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:38:14,686: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 14:38:14,719: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:38:16,531: Val batch 13500: PER (avg): 0.1531 CTC Loss (avg): 15.5422 WER(1gram): 46.45% (n=64) time: 6.633
2026-01-04 14:38:16,531: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-04 14:38:16,531: t15.2023.08.13 val PER: 0.1102
2026-01-04 14:38:16,531: t15.2023.08.18 val PER: 0.1048
2026-01-04 14:38:16,531: t15.2023.08.20 val PER: 0.1096
2026-01-04 14:38:16,531: t15.2023.08.25 val PER: 0.0979
2026-01-04 14:38:16,531: t15.2023.08.27 val PER: 0.1865
2026-01-04 14:38:16,532: t15.2023.09.01 val PER: 0.0812
2026-01-04 14:38:16,532: t15.2023.09.03 val PER: 0.1675
2026-01-04 14:38:16,532: t15.2023.09.24 val PER: 0.1274
2026-01-04 14:38:16,532: t15.2023.09.29 val PER: 0.1340
2026-01-04 14:38:16,532: t15.2023.10.01 val PER: 0.1750
2026-01-04 14:38:16,532: t15.2023.10.06 val PER: 0.0936
2026-01-04 14:38:16,532: t15.2023.10.08 val PER: 0.2517
2026-01-04 14:38:16,532: t15.2023.10.13 val PER: 0.2025
2026-01-04 14:38:16,532: t15.2023.10.15 val PER: 0.1562
2026-01-04 14:38:16,533: t15.2023.10.20 val PER: 0.1745
2026-01-04 14:38:16,533: t15.2023.10.22 val PER: 0.1169
2026-01-04 14:38:16,533: t15.2023.11.03 val PER: 0.1852
2026-01-04 14:38:16,533: t15.2023.11.04 val PER: 0.0375
2026-01-04 14:38:16,533: t15.2023.11.17 val PER: 0.0404
2026-01-04 14:38:16,533: t15.2023.11.19 val PER: 0.0399
2026-01-04 14:38:16,533: t15.2023.11.26 val PER: 0.1210
2026-01-04 14:38:16,533: t15.2023.12.03 val PER: 0.1166
2026-01-04 14:38:16,533: t15.2023.12.08 val PER: 0.1099
2026-01-04 14:38:16,533: t15.2023.12.10 val PER: 0.0986
2026-01-04 14:38:16,534: t15.2023.12.17 val PER: 0.1258
2026-01-04 14:38:16,534: t15.2023.12.29 val PER: 0.1290
2026-01-04 14:38:16,534: t15.2024.02.25 val PER: 0.1053
2026-01-04 14:38:16,534: t15.2024.03.08 val PER: 0.2290
2026-01-04 14:38:16,534: t15.2024.03.15 val PER: 0.1957
2026-01-04 14:38:16,534: t15.2024.03.17 val PER: 0.1492
2026-01-04 14:38:16,534: t15.2024.05.10 val PER: 0.1620
2026-01-04 14:38:16,534: t15.2024.06.14 val PER: 0.1703
2026-01-04 14:38:16,534: t15.2024.07.19 val PER: 0.2406
2026-01-04 14:38:16,535: t15.2024.07.21 val PER: 0.0979
2026-01-04 14:38:16,535: t15.2024.07.28 val PER: 0.1375
2026-01-04 14:38:16,535: t15.2025.01.10 val PER: 0.2961
2026-01-04 14:38:16,535: t15.2025.01.12 val PER: 0.1501
2026-01-04 14:38:16,535: t15.2025.03.14 val PER: 0.3388
2026-01-04 14:38:16,535: t15.2025.03.16 val PER: 0.2016
2026-01-04 14:38:16,535: t15.2025.03.30 val PER: 0.3023
2026-01-04 14:38:16,535: t15.2025.04.13 val PER: 0.2154
2026-01-04 14:38:16,536: New best val WER(1gram) 46.70% --> 46.45%
2026-01-04 14:38:16,536: Checkpointing model
2026-01-04 14:38:17,144: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:38:17,425: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_13500
2026-01-04 14:38:26,291: Train batch 13600: loss: 12.40 grad norm: 67.34 time: 0.062
2026-01-04 14:38:43,664: Train batch 13800: loss: 9.21 grad norm: 60.70 time: 0.055
2026-01-04 14:39:01,467: Train batch 14000: loss: 11.79 grad norm: 57.62 time: 0.050
2026-01-04 14:39:01,467: Running test after training batch: 14000
2026-01-04 14:39:01,625: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:39:06,289: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:39:06,323: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:39:08,178: Val batch 14000: PER (avg): 0.1519 CTC Loss (avg): 15.6189 WER(1gram): 46.45% (n=64) time: 6.711
2026-01-04 14:39:08,179: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 14:39:08,179: t15.2023.08.13 val PER: 0.1133
2026-01-04 14:39:08,179: t15.2023.08.18 val PER: 0.1039
2026-01-04 14:39:08,179: t15.2023.08.20 val PER: 0.1112
2026-01-04 14:39:08,179: t15.2023.08.25 val PER: 0.1084
2026-01-04 14:39:08,179: t15.2023.08.27 val PER: 0.1881
2026-01-04 14:39:08,179: t15.2023.09.01 val PER: 0.0779
2026-01-04 14:39:08,179: t15.2023.09.03 val PER: 0.1651
2026-01-04 14:39:08,180: t15.2023.09.24 val PER: 0.1250
2026-01-04 14:39:08,180: t15.2023.09.29 val PER: 0.1347
2026-01-04 14:39:08,180: t15.2023.10.01 val PER: 0.1697
2026-01-04 14:39:08,180: t15.2023.10.06 val PER: 0.0893
2026-01-04 14:39:08,180: t15.2023.10.08 val PER: 0.2503
2026-01-04 14:39:08,180: t15.2023.10.13 val PER: 0.2025
2026-01-04 14:39:08,180: t15.2023.10.15 val PER: 0.1483
2026-01-04 14:39:08,180: t15.2023.10.20 val PER: 0.1846
2026-01-04 14:39:08,180: t15.2023.10.22 val PER: 0.1169
2026-01-04 14:39:08,180: t15.2023.11.03 val PER: 0.1798
2026-01-04 14:39:08,180: t15.2023.11.04 val PER: 0.0307
2026-01-04 14:39:08,180: t15.2023.11.17 val PER: 0.0404
2026-01-04 14:39:08,180: t15.2023.11.19 val PER: 0.0359
2026-01-04 14:39:08,180: t15.2023.11.26 val PER: 0.1203
2026-01-04 14:39:08,180: t15.2023.12.03 val PER: 0.1261
2026-01-04 14:39:08,181: t15.2023.12.08 val PER: 0.1052
2026-01-04 14:39:08,181: t15.2023.12.10 val PER: 0.0946
2026-01-04 14:39:08,181: t15.2023.12.17 val PER: 0.1331
2026-01-04 14:39:08,181: t15.2023.12.29 val PER: 0.1311
2026-01-04 14:39:08,181: t15.2024.02.25 val PER: 0.1110
2026-01-04 14:39:08,181: t15.2024.03.08 val PER: 0.2390
2026-01-04 14:39:08,181: t15.2024.03.15 val PER: 0.2058
2026-01-04 14:39:08,181: t15.2024.03.17 val PER: 0.1430
2026-01-04 14:39:08,181: t15.2024.05.10 val PER: 0.1530
2026-01-04 14:39:08,181: t15.2024.06.14 val PER: 0.1609
2026-01-04 14:39:08,181: t15.2024.07.19 val PER: 0.2327
2026-01-04 14:39:08,181: t15.2024.07.21 val PER: 0.0890
2026-01-04 14:39:08,181: t15.2024.07.28 val PER: 0.1471
2026-01-04 14:39:08,181: t15.2025.01.10 val PER: 0.3030
2026-01-04 14:39:08,181: t15.2025.01.12 val PER: 0.1463
2026-01-04 14:39:08,181: t15.2025.03.14 val PER: 0.3447
2026-01-04 14:39:08,182: t15.2025.03.16 val PER: 0.1832
2026-01-04 14:39:08,182: t15.2025.03.30 val PER: 0.2862
2026-01-04 14:39:08,182: t15.2025.04.13 val PER: 0.2168
2026-01-04 14:39:08,450: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_14000
2026-01-04 14:39:26,224: Train batch 14200: loss: 8.23 grad norm: 51.91 time: 0.055
2026-01-04 14:39:43,661: Train batch 14400: loss: 5.75 grad norm: 40.00 time: 0.064
2026-01-04 14:39:52,355: Running test after training batch: 14500
2026-01-04 14:39:52,464: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:39:57,110: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:39:57,143: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:39:58,975: Val batch 14500: PER (avg): 0.1517 CTC Loss (avg): 15.5718 WER(1gram): 47.97% (n=64) time: 6.619
2026-01-04 14:39:58,976: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 14:39:58,976: t15.2023.08.13 val PER: 0.1185
2026-01-04 14:39:58,976: t15.2023.08.18 val PER: 0.1073
2026-01-04 14:39:58,976: t15.2023.08.20 val PER: 0.1120
2026-01-04 14:39:58,976: t15.2023.08.25 val PER: 0.0904
2026-01-04 14:39:58,976: t15.2023.08.27 val PER: 0.1833
2026-01-04 14:39:58,976: t15.2023.09.01 val PER: 0.0731
2026-01-04 14:39:58,976: t15.2023.09.03 val PER: 0.1651
2026-01-04 14:39:58,976: t15.2023.09.24 val PER: 0.1323
2026-01-04 14:39:58,976: t15.2023.09.29 val PER: 0.1321
2026-01-04 14:39:58,976: t15.2023.10.01 val PER: 0.1816
2026-01-04 14:39:58,976: t15.2023.10.06 val PER: 0.0883
2026-01-04 14:39:58,977: t15.2023.10.08 val PER: 0.2463
2026-01-04 14:39:58,977: t15.2023.10.13 val PER: 0.2025
2026-01-04 14:39:58,977: t15.2023.10.15 val PER: 0.1569
2026-01-04 14:39:58,977: t15.2023.10.20 val PER: 0.1946
2026-01-04 14:39:58,977: t15.2023.10.22 val PER: 0.1136
2026-01-04 14:39:58,977: t15.2023.11.03 val PER: 0.1784
2026-01-04 14:39:58,977: t15.2023.11.04 val PER: 0.0307
2026-01-04 14:39:58,977: t15.2023.11.17 val PER: 0.0404
2026-01-04 14:39:58,977: t15.2023.11.19 val PER: 0.0439
2026-01-04 14:39:58,977: t15.2023.11.26 val PER: 0.1167
2026-01-04 14:39:58,977: t15.2023.12.03 val PER: 0.1124
2026-01-04 14:39:58,977: t15.2023.12.08 val PER: 0.1039
2026-01-04 14:39:58,977: t15.2023.12.10 val PER: 0.0946
2026-01-04 14:39:58,978: t15.2023.12.17 val PER: 0.1445
2026-01-04 14:39:58,978: t15.2023.12.29 val PER: 0.1338
2026-01-04 14:39:58,978: t15.2024.02.25 val PER: 0.1152
2026-01-04 14:39:58,978: t15.2024.03.08 val PER: 0.2262
2026-01-04 14:39:58,978: t15.2024.03.15 val PER: 0.2001
2026-01-04 14:39:58,978: t15.2024.03.17 val PER: 0.1381
2026-01-04 14:39:58,978: t15.2024.05.10 val PER: 0.1501
2026-01-04 14:39:58,978: t15.2024.06.14 val PER: 0.1609
2026-01-04 14:39:58,978: t15.2024.07.19 val PER: 0.2413
2026-01-04 14:39:58,978: t15.2024.07.21 val PER: 0.0924
2026-01-04 14:39:58,978: t15.2024.07.28 val PER: 0.1426
2026-01-04 14:39:58,978: t15.2025.01.10 val PER: 0.2920
2026-01-04 14:39:58,978: t15.2025.01.12 val PER: 0.1432
2026-01-04 14:39:58,978: t15.2025.03.14 val PER: 0.3462
2026-01-04 14:39:58,978: t15.2025.03.16 val PER: 0.1767
2026-01-04 14:39:58,979: t15.2025.03.30 val PER: 0.2920
2026-01-04 14:39:58,979: t15.2025.04.13 val PER: 0.2197
2026-01-04 14:39:59,241: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_14500
2026-01-04 14:40:07,768: Train batch 14600: loss: 12.04 grad norm: 59.43 time: 0.058
2026-01-04 14:40:25,086: Train batch 14800: loss: 5.47 grad norm: 40.46 time: 0.050
2026-01-04 14:40:42,301: Train batch 15000: loss: 8.66 grad norm: 48.22 time: 0.052
2026-01-04 14:40:42,301: Running test after training batch: 15000
2026-01-04 14:40:42,428: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:40:47,108: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:40:47,141: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:40:48,968: Val batch 15000: PER (avg): 0.1508 CTC Loss (avg): 15.3696 WER(1gram): 45.69% (n=64) time: 6.667
2026-01-04 14:40:48,968: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=12
2026-01-04 14:40:48,969: t15.2023.08.13 val PER: 0.1123
2026-01-04 14:40:48,969: t15.2023.08.18 val PER: 0.1006
2026-01-04 14:40:48,969: t15.2023.08.20 val PER: 0.1120
2026-01-04 14:40:48,969: t15.2023.08.25 val PER: 0.0934
2026-01-04 14:40:48,969: t15.2023.08.27 val PER: 0.1849
2026-01-04 14:40:48,969: t15.2023.09.01 val PER: 0.0714
2026-01-04 14:40:48,969: t15.2023.09.03 val PER: 0.1532
2026-01-04 14:40:48,969: t15.2023.09.24 val PER: 0.1274
2026-01-04 14:40:48,969: t15.2023.09.29 val PER: 0.1257
2026-01-04 14:40:48,969: t15.2023.10.01 val PER: 0.1770
2026-01-04 14:40:48,970: t15.2023.10.06 val PER: 0.0818
2026-01-04 14:40:48,970: t15.2023.10.08 val PER: 0.2503
2026-01-04 14:40:48,970: t15.2023.10.13 val PER: 0.2102
2026-01-04 14:40:48,970: t15.2023.10.15 val PER: 0.1496
2026-01-04 14:40:48,970: t15.2023.10.20 val PER: 0.1946
2026-01-04 14:40:48,970: t15.2023.10.22 val PER: 0.1114
2026-01-04 14:40:48,970: t15.2023.11.03 val PER: 0.1777
2026-01-04 14:40:48,970: t15.2023.11.04 val PER: 0.0375
2026-01-04 14:40:48,970: t15.2023.11.17 val PER: 0.0451
2026-01-04 14:40:48,970: t15.2023.11.19 val PER: 0.0359
2026-01-04 14:40:48,970: t15.2023.11.26 val PER: 0.1174
2026-01-04 14:40:48,970: t15.2023.12.03 val PER: 0.1208
2026-01-04 14:40:48,971: t15.2023.12.08 val PER: 0.1019
2026-01-04 14:40:48,971: t15.2023.12.10 val PER: 0.0907
2026-01-04 14:40:48,971: t15.2023.12.17 val PER: 0.1424
2026-01-04 14:40:48,971: t15.2023.12.29 val PER: 0.1407
2026-01-04 14:40:48,971: t15.2024.02.25 val PER: 0.1053
2026-01-04 14:40:48,971: t15.2024.03.08 val PER: 0.2219
2026-01-04 14:40:48,971: t15.2024.03.15 val PER: 0.2008
2026-01-04 14:40:48,971: t15.2024.03.17 val PER: 0.1325
2026-01-04 14:40:48,971: t15.2024.05.10 val PER: 0.1664
2026-01-04 14:40:48,971: t15.2024.06.14 val PER: 0.1719
2026-01-04 14:40:48,971: t15.2024.07.19 val PER: 0.2419
2026-01-04 14:40:48,972: t15.2024.07.21 val PER: 0.0917
2026-01-04 14:40:48,972: t15.2024.07.28 val PER: 0.1353
2026-01-04 14:40:48,972: t15.2025.01.10 val PER: 0.2989
2026-01-04 14:40:48,972: t15.2025.01.12 val PER: 0.1370
2026-01-04 14:40:48,972: t15.2025.03.14 val PER: 0.3462
2026-01-04 14:40:48,972: t15.2025.03.16 val PER: 0.1846
2026-01-04 14:40:48,972: t15.2025.03.30 val PER: 0.2943
2026-01-04 14:40:48,972: t15.2025.04.13 val PER: 0.2254
2026-01-04 14:40:48,973: New best val WER(1gram) 46.45% --> 45.69%
2026-01-04 14:40:48,973: Checkpointing model
2026-01-04 14:40:49,569: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:40:49,850: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_15000
2026-01-04 14:41:07,397: Train batch 15200: loss: 5.04 grad norm: 42.73 time: 0.056
2026-01-04 14:41:24,231: Train batch 15400: loss: 11.25 grad norm: 57.14 time: 0.049
2026-01-04 14:41:32,974: Running test after training batch: 15500
2026-01-04 14:41:33,079: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:41:37,707: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:41:37,741: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:41:39,602: Val batch 15500: PER (avg): 0.1500 CTC Loss (avg): 15.2893 WER(1gram): 46.19% (n=64) time: 6.628
2026-01-04 14:41:39,602: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 14:41:39,602: t15.2023.08.13 val PER: 0.1091
2026-01-04 14:41:39,603: t15.2023.08.18 val PER: 0.1065
2026-01-04 14:41:39,603: t15.2023.08.20 val PER: 0.1080
2026-01-04 14:41:39,603: t15.2023.08.25 val PER: 0.0964
2026-01-04 14:41:39,603: t15.2023.08.27 val PER: 0.1833
2026-01-04 14:41:39,603: t15.2023.09.01 val PER: 0.0787
2026-01-04 14:41:39,603: t15.2023.09.03 val PER: 0.1603
2026-01-04 14:41:39,603: t15.2023.09.24 val PER: 0.1262
2026-01-04 14:41:39,603: t15.2023.09.29 val PER: 0.1283
2026-01-04 14:41:39,603: t15.2023.10.01 val PER: 0.1704
2026-01-04 14:41:39,603: t15.2023.10.06 val PER: 0.0883
2026-01-04 14:41:39,603: t15.2023.10.08 val PER: 0.2530
2026-01-04 14:41:39,603: t15.2023.10.13 val PER: 0.1994
2026-01-04 14:41:39,603: t15.2023.10.15 val PER: 0.1477
2026-01-04 14:41:39,603: t15.2023.10.20 val PER: 0.1879
2026-01-04 14:41:39,604: t15.2023.10.22 val PER: 0.1192
2026-01-04 14:41:39,604: t15.2023.11.03 val PER: 0.1744
2026-01-04 14:41:39,604: t15.2023.11.04 val PER: 0.0341
2026-01-04 14:41:39,604: t15.2023.11.17 val PER: 0.0373
2026-01-04 14:41:39,604: t15.2023.11.19 val PER: 0.0359
2026-01-04 14:41:39,604: t15.2023.11.26 val PER: 0.1188
2026-01-04 14:41:39,604: t15.2023.12.03 val PER: 0.1155
2026-01-04 14:41:39,604: t15.2023.12.08 val PER: 0.1039
2026-01-04 14:41:39,604: t15.2023.12.10 val PER: 0.0867
2026-01-04 14:41:39,604: t15.2023.12.17 val PER: 0.1362
2026-01-04 14:41:39,604: t15.2023.12.29 val PER: 0.1359
2026-01-04 14:41:39,604: t15.2024.02.25 val PER: 0.1025
2026-01-04 14:41:39,605: t15.2024.03.08 val PER: 0.2347
2026-01-04 14:41:39,605: t15.2024.03.15 val PER: 0.2014
2026-01-04 14:41:39,605: t15.2024.03.17 val PER: 0.1388
2026-01-04 14:41:39,605: t15.2024.05.10 val PER: 0.1605
2026-01-04 14:41:39,605: t15.2024.06.14 val PER: 0.1656
2026-01-04 14:41:39,605: t15.2024.07.19 val PER: 0.2347
2026-01-04 14:41:39,605: t15.2024.07.21 val PER: 0.0938
2026-01-04 14:41:39,605: t15.2024.07.28 val PER: 0.1397
2026-01-04 14:41:39,606: t15.2025.01.10 val PER: 0.2879
2026-01-04 14:41:39,606: t15.2025.01.12 val PER: 0.1440
2026-01-04 14:41:39,606: t15.2025.03.14 val PER: 0.3417
2026-01-04 14:41:39,606: t15.2025.03.16 val PER: 0.1832
2026-01-04 14:41:39,606: t15.2025.03.30 val PER: 0.2828
2026-01-04 14:41:39,606: t15.2025.04.13 val PER: 0.2197
2026-01-04 14:41:39,873: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_15500
2026-01-04 14:41:48,536: Train batch 15600: loss: 11.87 grad norm: 57.47 time: 0.061
2026-01-04 14:42:06,050: Train batch 15800: loss: 13.61 grad norm: 62.59 time: 0.067
2026-01-04 14:42:23,623: Train batch 16000: loss: 8.20 grad norm: 43.15 time: 0.056
2026-01-04 14:42:23,624: Running test after training batch: 16000
2026-01-04 14:42:23,722: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:42:28,380: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:42:28,413: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:42:30,278: Val batch 16000: PER (avg): 0.1486 CTC Loss (avg): 15.2822 WER(1gram): 46.19% (n=64) time: 6.655
2026-01-04 14:42:30,279: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 14:42:30,279: t15.2023.08.13 val PER: 0.1185
2026-01-04 14:42:30,279: t15.2023.08.18 val PER: 0.1031
2026-01-04 14:42:30,279: t15.2023.08.20 val PER: 0.1112
2026-01-04 14:42:30,279: t15.2023.08.25 val PER: 0.0904
2026-01-04 14:42:30,279: t15.2023.08.27 val PER: 0.1849
2026-01-04 14:42:30,279: t15.2023.09.01 val PER: 0.0722
2026-01-04 14:42:30,279: t15.2023.09.03 val PER: 0.1580
2026-01-04 14:42:30,279: t15.2023.09.24 val PER: 0.1286
2026-01-04 14:42:30,280: t15.2023.09.29 val PER: 0.1308
2026-01-04 14:42:30,280: t15.2023.10.01 val PER: 0.1678
2026-01-04 14:42:30,280: t15.2023.10.06 val PER: 0.0861
2026-01-04 14:42:30,280: t15.2023.10.08 val PER: 0.2449
2026-01-04 14:42:30,280: t15.2023.10.13 val PER: 0.2002
2026-01-04 14:42:30,280: t15.2023.10.15 val PER: 0.1463
2026-01-04 14:42:30,280: t15.2023.10.20 val PER: 0.1846
2026-01-04 14:42:30,280: t15.2023.10.22 val PER: 0.1169
2026-01-04 14:42:30,280: t15.2023.11.03 val PER: 0.1716
2026-01-04 14:42:30,280: t15.2023.11.04 val PER: 0.0273
2026-01-04 14:42:30,280: t15.2023.11.17 val PER: 0.0389
2026-01-04 14:42:30,280: t15.2023.11.19 val PER: 0.0439
2026-01-04 14:42:30,280: t15.2023.11.26 val PER: 0.1145
2026-01-04 14:42:30,280: t15.2023.12.03 val PER: 0.1145
2026-01-04 14:42:30,281: t15.2023.12.08 val PER: 0.0985
2026-01-04 14:42:30,281: t15.2023.12.10 val PER: 0.0894
2026-01-04 14:42:30,281: t15.2023.12.17 val PER: 0.1403
2026-01-04 14:42:30,281: t15.2023.12.29 val PER: 0.1304
2026-01-04 14:42:30,281: t15.2024.02.25 val PER: 0.1053
2026-01-04 14:42:30,281: t15.2024.03.08 val PER: 0.2290
2026-01-04 14:42:30,281: t15.2024.03.15 val PER: 0.1964
2026-01-04 14:42:30,281: t15.2024.03.17 val PER: 0.1374
2026-01-04 14:42:30,281: t15.2024.05.10 val PER: 0.1560
2026-01-04 14:42:30,281: t15.2024.06.14 val PER: 0.1609
2026-01-04 14:42:30,281: t15.2024.07.19 val PER: 0.2327
2026-01-04 14:42:30,281: t15.2024.07.21 val PER: 0.0897
2026-01-04 14:42:30,281: t15.2024.07.28 val PER: 0.1368
2026-01-04 14:42:30,281: t15.2025.01.10 val PER: 0.2920
2026-01-04 14:42:30,281: t15.2025.01.12 val PER: 0.1447
2026-01-04 14:42:30,281: t15.2025.03.14 val PER: 0.3388
2026-01-04 14:42:30,282: t15.2025.03.16 val PER: 0.1859
2026-01-04 14:42:30,282: t15.2025.03.30 val PER: 0.2885
2026-01-04 14:42:30,282: t15.2025.04.13 val PER: 0.2140
2026-01-04 14:42:30,550: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_16000
2026-01-04 14:42:48,029: Train batch 16200: loss: 6.08 grad norm: 42.23 time: 0.054
2026-01-04 14:43:05,734: Train batch 16400: loss: 10.47 grad norm: 59.96 time: 0.057
2026-01-04 14:43:14,662: Running test after training batch: 16500
2026-01-04 14:43:14,820: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:43:19,569: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:43:19,607: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:43:21,704: Val batch 16500: PER (avg): 0.1469 CTC Loss (avg): 15.1801 WER(1gram): 45.18% (n=64) time: 7.041
2026-01-04 14:43:21,704: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 14:43:21,704: t15.2023.08.13 val PER: 0.1102
2026-01-04 14:43:21,705: t15.2023.08.18 val PER: 0.1014
2026-01-04 14:43:21,705: t15.2023.08.20 val PER: 0.1072
2026-01-04 14:43:21,705: t15.2023.08.25 val PER: 0.0889
2026-01-04 14:43:21,705: t15.2023.08.27 val PER: 0.1752
2026-01-04 14:43:21,705: t15.2023.09.01 val PER: 0.0731
2026-01-04 14:43:21,705: t15.2023.09.03 val PER: 0.1603
2026-01-04 14:43:21,705: t15.2023.09.24 val PER: 0.1274
2026-01-04 14:43:21,705: t15.2023.09.29 val PER: 0.1251
2026-01-04 14:43:21,705: t15.2023.10.01 val PER: 0.1697
2026-01-04 14:43:21,706: t15.2023.10.06 val PER: 0.0840
2026-01-04 14:43:21,706: t15.2023.10.08 val PER: 0.2503
2026-01-04 14:43:21,706: t15.2023.10.13 val PER: 0.1947
2026-01-04 14:43:21,706: t15.2023.10.15 val PER: 0.1444
2026-01-04 14:43:21,706: t15.2023.10.20 val PER: 0.1946
2026-01-04 14:43:21,706: t15.2023.10.22 val PER: 0.1169
2026-01-04 14:43:21,706: t15.2023.11.03 val PER: 0.1730
2026-01-04 14:43:21,706: t15.2023.11.04 val PER: 0.0307
2026-01-04 14:43:21,706: t15.2023.11.17 val PER: 0.0389
2026-01-04 14:43:21,707: t15.2023.11.19 val PER: 0.0399
2026-01-04 14:43:21,707: t15.2023.11.26 val PER: 0.1130
2026-01-04 14:43:21,707: t15.2023.12.03 val PER: 0.1103
2026-01-04 14:43:21,707: t15.2023.12.08 val PER: 0.0959
2026-01-04 14:43:21,707: t15.2023.12.10 val PER: 0.0815
2026-01-04 14:43:21,707: t15.2023.12.17 val PER: 0.1414
2026-01-04 14:43:21,707: t15.2023.12.29 val PER: 0.1235
2026-01-04 14:43:21,707: t15.2024.02.25 val PER: 0.0997
2026-01-04 14:43:21,708: t15.2024.03.08 val PER: 0.2276
2026-01-04 14:43:21,708: t15.2024.03.15 val PER: 0.1970
2026-01-04 14:43:21,708: t15.2024.03.17 val PER: 0.1346
2026-01-04 14:43:21,708: t15.2024.05.10 val PER: 0.1545
2026-01-04 14:43:21,708: t15.2024.06.14 val PER: 0.1656
2026-01-04 14:43:21,708: t15.2024.07.19 val PER: 0.2287
2026-01-04 14:43:21,708: t15.2024.07.21 val PER: 0.0862
2026-01-04 14:43:21,708: t15.2024.07.28 val PER: 0.1375
2026-01-04 14:43:21,708: t15.2025.01.10 val PER: 0.2906
2026-01-04 14:43:21,708: t15.2025.01.12 val PER: 0.1440
2026-01-04 14:43:21,709: t15.2025.03.14 val PER: 0.3417
2026-01-04 14:43:21,709: t15.2025.03.16 val PER: 0.1859
2026-01-04 14:43:21,709: t15.2025.03.30 val PER: 0.2816
2026-01-04 14:43:21,709: t15.2025.04.13 val PER: 0.2225
2026-01-04 14:43:21,709: New best val WER(1gram) 45.69% --> 45.18%
2026-01-04 14:43:21,709: Checkpointing model
2026-01-04 14:43:22,320: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:43:22,607: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_16500
2026-01-04 14:43:31,427: Train batch 16600: loss: 8.30 grad norm: 54.66 time: 0.052
2026-01-04 14:43:48,652: Train batch 16800: loss: 16.30 grad norm: 73.05 time: 0.061
2026-01-04 14:44:05,697: Train batch 17000: loss: 8.09 grad norm: 47.59 time: 0.081
2026-01-04 14:44:05,698: Running test after training batch: 17000
2026-01-04 14:44:05,972: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:44:11,013: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:44:11,051: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:44:13,155: Val batch 17000: PER (avg): 0.1462 CTC Loss (avg): 15.0532 WER(1gram): 44.92% (n=64) time: 7.457
2026-01-04 14:44:13,155: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 14:44:13,155: t15.2023.08.13 val PER: 0.1071
2026-01-04 14:44:13,156: t15.2023.08.18 val PER: 0.1039
2026-01-04 14:44:13,156: t15.2023.08.20 val PER: 0.1072
2026-01-04 14:44:13,156: t15.2023.08.25 val PER: 0.0904
2026-01-04 14:44:13,156: t15.2023.08.27 val PER: 0.1736
2026-01-04 14:44:13,156: t15.2023.09.01 val PER: 0.0771
2026-01-04 14:44:13,156: t15.2023.09.03 val PER: 0.1544
2026-01-04 14:44:13,156: t15.2023.09.24 val PER: 0.1311
2026-01-04 14:44:13,156: t15.2023.09.29 val PER: 0.1276
2026-01-04 14:44:13,157: t15.2023.10.01 val PER: 0.1697
2026-01-04 14:44:13,157: t15.2023.10.06 val PER: 0.0829
2026-01-04 14:44:13,157: t15.2023.10.08 val PER: 0.2463
2026-01-04 14:44:13,157: t15.2023.10.13 val PER: 0.1916
2026-01-04 14:44:13,157: t15.2023.10.15 val PER: 0.1496
2026-01-04 14:44:13,157: t15.2023.10.20 val PER: 0.1779
2026-01-04 14:44:13,157: t15.2023.10.22 val PER: 0.1069
2026-01-04 14:44:13,157: t15.2023.11.03 val PER: 0.1777
2026-01-04 14:44:13,157: t15.2023.11.04 val PER: 0.0307
2026-01-04 14:44:13,157: t15.2023.11.17 val PER: 0.0295
2026-01-04 14:44:13,158: t15.2023.11.19 val PER: 0.0359
2026-01-04 14:44:13,158: t15.2023.11.26 val PER: 0.1058
2026-01-04 14:44:13,158: t15.2023.12.03 val PER: 0.1134
2026-01-04 14:44:13,158: t15.2023.12.08 val PER: 0.0945
2026-01-04 14:44:13,158: t15.2023.12.10 val PER: 0.0828
2026-01-04 14:44:13,158: t15.2023.12.17 val PER: 0.1362
2026-01-04 14:44:13,158: t15.2023.12.29 val PER: 0.1229
2026-01-04 14:44:13,158: t15.2024.02.25 val PER: 0.1081
2026-01-04 14:44:13,158: t15.2024.03.08 val PER: 0.2333
2026-01-04 14:44:13,158: t15.2024.03.15 val PER: 0.1982
2026-01-04 14:44:13,159: t15.2024.03.17 val PER: 0.1353
2026-01-04 14:44:13,159: t15.2024.05.10 val PER: 0.1590
2026-01-04 14:44:13,159: t15.2024.06.14 val PER: 0.1672
2026-01-04 14:44:13,159: t15.2024.07.19 val PER: 0.2314
2026-01-04 14:44:13,159: t15.2024.07.21 val PER: 0.0890
2026-01-04 14:44:13,159: t15.2024.07.28 val PER: 0.1294
2026-01-04 14:44:13,159: t15.2025.01.10 val PER: 0.2879
2026-01-04 14:44:13,159: t15.2025.01.12 val PER: 0.1363
2026-01-04 14:44:13,159: t15.2025.03.14 val PER: 0.3402
2026-01-04 14:44:13,159: t15.2025.03.16 val PER: 0.1767
2026-01-04 14:44:13,160: t15.2025.03.30 val PER: 0.2839
2026-01-04 14:44:13,160: t15.2025.04.13 val PER: 0.2183
2026-01-04 14:44:13,160: New best val WER(1gram) 45.18% --> 44.92%
2026-01-04 14:44:13,160: Checkpointing model
2026-01-04 14:44:13,792: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:44:14,081: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_17000
2026-01-04 14:44:31,336: Train batch 17200: loss: 9.34 grad norm: 48.89 time: 0.082
2026-01-04 14:44:48,958: Train batch 17400: loss: 11.84 grad norm: 60.26 time: 0.071
2026-01-04 14:44:57,889: Running test after training batch: 17500
2026-01-04 14:44:58,048: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:45:02,725: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:45:02,757: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:45:04,651: Val batch 17500: PER (avg): 0.1466 CTC Loss (avg): 15.0640 WER(1gram): 45.94% (n=64) time: 6.762
2026-01-04 14:45:04,652: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 14:45:04,652: t15.2023.08.13 val PER: 0.1112
2026-01-04 14:45:04,652: t15.2023.08.18 val PER: 0.1014
2026-01-04 14:45:04,652: t15.2023.08.20 val PER: 0.1056
2026-01-04 14:45:04,653: t15.2023.08.25 val PER: 0.0934
2026-01-04 14:45:04,653: t15.2023.08.27 val PER: 0.1736
2026-01-04 14:45:04,653: t15.2023.09.01 val PER: 0.0714
2026-01-04 14:45:04,653: t15.2023.09.03 val PER: 0.1615
2026-01-04 14:45:04,653: t15.2023.09.24 val PER: 0.1323
2026-01-04 14:45:04,653: t15.2023.09.29 val PER: 0.1257
2026-01-04 14:45:04,654: t15.2023.10.01 val PER: 0.1724
2026-01-04 14:45:04,654: t15.2023.10.06 val PER: 0.0840
2026-01-04 14:45:04,654: t15.2023.10.08 val PER: 0.2544
2026-01-04 14:45:04,654: t15.2023.10.13 val PER: 0.1939
2026-01-04 14:45:04,654: t15.2023.10.15 val PER: 0.1543
2026-01-04 14:45:04,654: t15.2023.10.20 val PER: 0.1946
2026-01-04 14:45:04,654: t15.2023.10.22 val PER: 0.1036
2026-01-04 14:45:04,654: t15.2023.11.03 val PER: 0.1716
2026-01-04 14:45:04,654: t15.2023.11.04 val PER: 0.0341
2026-01-04 14:45:04,654: t15.2023.11.17 val PER: 0.0327
2026-01-04 14:45:04,654: t15.2023.11.19 val PER: 0.0359
2026-01-04 14:45:04,655: t15.2023.11.26 val PER: 0.1109
2026-01-04 14:45:04,655: t15.2023.12.03 val PER: 0.1145
2026-01-04 14:45:04,655: t15.2023.12.08 val PER: 0.0979
2026-01-04 14:45:04,655: t15.2023.12.10 val PER: 0.0841
2026-01-04 14:45:04,655: t15.2023.12.17 val PER: 0.1351
2026-01-04 14:45:04,655: t15.2023.12.29 val PER: 0.1283
2026-01-04 14:45:04,655: t15.2024.02.25 val PER: 0.1025
2026-01-04 14:45:04,655: t15.2024.03.08 val PER: 0.2376
2026-01-04 14:45:04,655: t15.2024.03.15 val PER: 0.1932
2026-01-04 14:45:04,655: t15.2024.03.17 val PER: 0.1360
2026-01-04 14:45:04,655: t15.2024.05.10 val PER: 0.1545
2026-01-04 14:45:04,655: t15.2024.06.14 val PER: 0.1577
2026-01-04 14:45:04,656: t15.2024.07.19 val PER: 0.2327
2026-01-04 14:45:04,656: t15.2024.07.21 val PER: 0.0869
2026-01-04 14:45:04,656: t15.2024.07.28 val PER: 0.1272
2026-01-04 14:45:04,656: t15.2025.01.10 val PER: 0.2906
2026-01-04 14:45:04,656: t15.2025.01.12 val PER: 0.1270
2026-01-04 14:45:04,656: t15.2025.03.14 val PER: 0.3432
2026-01-04 14:45:04,656: t15.2025.03.16 val PER: 0.1780
2026-01-04 14:45:04,656: t15.2025.03.30 val PER: 0.2920
2026-01-04 14:45:04,656: t15.2025.04.13 val PER: 0.2183
2026-01-04 14:45:05,007: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_17500
2026-01-04 14:45:13,953: Train batch 17600: loss: 9.60 grad norm: 57.26 time: 0.052
2026-01-04 14:45:31,414: Train batch 17800: loss: 6.25 grad norm: 48.22 time: 0.043
2026-01-04 14:45:48,546: Train batch 18000: loss: 11.31 grad norm: 66.55 time: 0.060
2026-01-04 14:45:48,546: Running test after training batch: 18000
2026-01-04 14:45:48,687: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:45:54,505: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:45:54,544: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:45:56,667: Val batch 18000: PER (avg): 0.1461 CTC Loss (avg): 15.0462 WER(1gram): 44.67% (n=64) time: 8.120
2026-01-04 14:45:56,667: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 14:45:56,667: t15.2023.08.13 val PER: 0.1112
2026-01-04 14:45:56,667: t15.2023.08.18 val PER: 0.0997
2026-01-04 14:45:56,667: t15.2023.08.20 val PER: 0.1033
2026-01-04 14:45:56,667: t15.2023.08.25 val PER: 0.0949
2026-01-04 14:45:56,668: t15.2023.08.27 val PER: 0.1817
2026-01-04 14:45:56,668: t15.2023.09.01 val PER: 0.0739
2026-01-04 14:45:56,668: t15.2023.09.03 val PER: 0.1532
2026-01-04 14:45:56,668: t15.2023.09.24 val PER: 0.1323
2026-01-04 14:45:56,668: t15.2023.09.29 val PER: 0.1257
2026-01-04 14:45:56,668: t15.2023.10.01 val PER: 0.1711
2026-01-04 14:45:56,668: t15.2023.10.06 val PER: 0.0829
2026-01-04 14:45:56,668: t15.2023.10.08 val PER: 0.2503
2026-01-04 14:45:56,668: t15.2023.10.13 val PER: 0.1908
2026-01-04 14:45:56,669: t15.2023.10.15 val PER: 0.1483
2026-01-04 14:45:56,669: t15.2023.10.20 val PER: 0.1980
2026-01-04 14:45:56,669: t15.2023.10.22 val PER: 0.1069
2026-01-04 14:45:56,669: t15.2023.11.03 val PER: 0.1710
2026-01-04 14:45:56,669: t15.2023.11.04 val PER: 0.0307
2026-01-04 14:45:56,669: t15.2023.11.17 val PER: 0.0311
2026-01-04 14:45:56,669: t15.2023.11.19 val PER: 0.0339
2026-01-04 14:45:56,669: t15.2023.11.26 val PER: 0.1101
2026-01-04 14:45:56,669: t15.2023.12.03 val PER: 0.1145
2026-01-04 14:45:56,669: t15.2023.12.08 val PER: 0.0952
2026-01-04 14:45:56,670: t15.2023.12.10 val PER: 0.0880
2026-01-04 14:45:56,670: t15.2023.12.17 val PER: 0.1351
2026-01-04 14:45:56,670: t15.2023.12.29 val PER: 0.1304
2026-01-04 14:45:56,671: t15.2024.02.25 val PER: 0.1053
2026-01-04 14:45:56,671: t15.2024.03.08 val PER: 0.2276
2026-01-04 14:45:56,671: t15.2024.03.15 val PER: 0.1926
2026-01-04 14:45:56,671: t15.2024.03.17 val PER: 0.1353
2026-01-04 14:45:56,671: t15.2024.05.10 val PER: 0.1486
2026-01-04 14:45:56,671: t15.2024.06.14 val PER: 0.1546
2026-01-04 14:45:56,671: t15.2024.07.19 val PER: 0.2380
2026-01-04 14:45:56,671: t15.2024.07.21 val PER: 0.0869
2026-01-04 14:45:56,671: t15.2024.07.28 val PER: 0.1353
2026-01-04 14:45:56,671: t15.2025.01.10 val PER: 0.2865
2026-01-04 14:45:56,671: t15.2025.01.12 val PER: 0.1370
2026-01-04 14:45:56,672: t15.2025.03.14 val PER: 0.3432
2026-01-04 14:45:56,672: t15.2025.03.16 val PER: 0.1741
2026-01-04 14:45:56,672: t15.2025.03.30 val PER: 0.2828
2026-01-04 14:45:56,672: t15.2025.04.13 val PER: 0.2154
2026-01-04 14:45:56,672: New best val WER(1gram) 44.92% --> 44.67%
2026-01-04 14:45:56,672: Checkpointing model
2026-01-04 14:45:57,299: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/best_checkpoint
2026-01-04 14:45:57,553: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_18000
2026-01-04 14:46:14,635: Train batch 18200: loss: 7.44 grad norm: 46.62 time: 0.073
2026-01-04 14:46:31,925: Train batch 18400: loss: 4.81 grad norm: 40.03 time: 0.057
2026-01-04 14:46:40,825: Running test after training batch: 18500
2026-01-04 14:46:40,962: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:46:45,743: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:46:45,783: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:46:47,926: Val batch 18500: PER (avg): 0.1461 CTC Loss (avg): 15.0424 WER(1gram): 44.92% (n=64) time: 7.101
2026-01-04 14:46:47,926: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 14:46:47,926: t15.2023.08.13 val PER: 0.1175
2026-01-04 14:46:47,926: t15.2023.08.18 val PER: 0.0997
2026-01-04 14:46:47,927: t15.2023.08.20 val PER: 0.1048
2026-01-04 14:46:47,927: t15.2023.08.25 val PER: 0.0904
2026-01-04 14:46:47,927: t15.2023.08.27 val PER: 0.1785
2026-01-04 14:46:47,927: t15.2023.09.01 val PER: 0.0755
2026-01-04 14:46:47,927: t15.2023.09.03 val PER: 0.1556
2026-01-04 14:46:47,927: t15.2023.09.24 val PER: 0.1299
2026-01-04 14:46:47,927: t15.2023.09.29 val PER: 0.1302
2026-01-04 14:46:47,928: t15.2023.10.01 val PER: 0.1645
2026-01-04 14:46:47,928: t15.2023.10.06 val PER: 0.0850
2026-01-04 14:46:47,928: t15.2023.10.08 val PER: 0.2422
2026-01-04 14:46:47,928: t15.2023.10.13 val PER: 0.1916
2026-01-04 14:46:47,928: t15.2023.10.15 val PER: 0.1470
2026-01-04 14:46:47,928: t15.2023.10.20 val PER: 0.1846
2026-01-04 14:46:47,928: t15.2023.10.22 val PER: 0.1091
2026-01-04 14:46:47,928: t15.2023.11.03 val PER: 0.1696
2026-01-04 14:46:47,928: t15.2023.11.04 val PER: 0.0273
2026-01-04 14:46:47,928: t15.2023.11.17 val PER: 0.0358
2026-01-04 14:46:47,928: t15.2023.11.19 val PER: 0.0339
2026-01-04 14:46:47,928: t15.2023.11.26 val PER: 0.1123
2026-01-04 14:46:47,928: t15.2023.12.03 val PER: 0.1166
2026-01-04 14:46:47,928: t15.2023.12.08 val PER: 0.0972
2026-01-04 14:46:47,928: t15.2023.12.10 val PER: 0.0880
2026-01-04 14:46:47,929: t15.2023.12.17 val PER: 0.1383
2026-01-04 14:46:47,929: t15.2023.12.29 val PER: 0.1345
2026-01-04 14:46:47,929: t15.2024.02.25 val PER: 0.1039
2026-01-04 14:46:47,929: t15.2024.03.08 val PER: 0.2191
2026-01-04 14:46:47,929: t15.2024.03.15 val PER: 0.1951
2026-01-04 14:46:47,929: t15.2024.03.17 val PER: 0.1318
2026-01-04 14:46:47,929: t15.2024.05.10 val PER: 0.1471
2026-01-04 14:46:47,929: t15.2024.06.14 val PER: 0.1546
2026-01-04 14:46:47,929: t15.2024.07.19 val PER: 0.2399
2026-01-04 14:46:47,929: t15.2024.07.21 val PER: 0.0862
2026-01-04 14:46:47,929: t15.2024.07.28 val PER: 0.1331
2026-01-04 14:46:47,929: t15.2025.01.10 val PER: 0.2879
2026-01-04 14:46:47,929: t15.2025.01.12 val PER: 0.1332
2026-01-04 14:46:47,930: t15.2025.03.14 val PER: 0.3432
2026-01-04 14:46:47,930: t15.2025.03.16 val PER: 0.1754
2026-01-04 14:46:47,930: t15.2025.03.30 val PER: 0.2839
2026-01-04 14:46:47,930: t15.2025.04.13 val PER: 0.2126
2026-01-04 14:46:48,211: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_18500
2026-01-04 14:46:56,747: Train batch 18600: loss: 12.35 grad norm: 60.00 time: 0.066
2026-01-04 14:47:14,375: Train batch 18800: loss: 7.95 grad norm: 47.64 time: 0.065
2026-01-04 14:47:31,958: Train batch 19000: loss: 8.48 grad norm: 47.60 time: 0.063
2026-01-04 14:47:31,959: Running test after training batch: 19000
2026-01-04 14:47:32,080: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:47:36,996: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:47:37,035: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:47:39,208: Val batch 19000: PER (avg): 0.1457 CTC Loss (avg): 15.0286 WER(1gram): 45.18% (n=64) time: 7.249
2026-01-04 14:47:39,208: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 14:47:39,208: t15.2023.08.13 val PER: 0.1091
2026-01-04 14:47:39,208: t15.2023.08.18 val PER: 0.0964
2026-01-04 14:47:39,209: t15.2023.08.20 val PER: 0.1056
2026-01-04 14:47:39,209: t15.2023.08.25 val PER: 0.0934
2026-01-04 14:47:39,209: t15.2023.08.27 val PER: 0.1768
2026-01-04 14:47:39,209: t15.2023.09.01 val PER: 0.0722
2026-01-04 14:47:39,209: t15.2023.09.03 val PER: 0.1520
2026-01-04 14:47:39,209: t15.2023.09.24 val PER: 0.1335
2026-01-04 14:47:39,209: t15.2023.09.29 val PER: 0.1289
2026-01-04 14:47:39,209: t15.2023.10.01 val PER: 0.1658
2026-01-04 14:47:39,210: t15.2023.10.06 val PER: 0.0786
2026-01-04 14:47:39,210: t15.2023.10.08 val PER: 0.2436
2026-01-04 14:47:39,210: t15.2023.10.13 val PER: 0.1877
2026-01-04 14:47:39,210: t15.2023.10.15 val PER: 0.1450
2026-01-04 14:47:39,210: t15.2023.10.20 val PER: 0.1946
2026-01-04 14:47:39,210: t15.2023.10.22 val PER: 0.1147
2026-01-04 14:47:39,210: t15.2023.11.03 val PER: 0.1730
2026-01-04 14:47:39,210: t15.2023.11.04 val PER: 0.0239
2026-01-04 14:47:39,210: t15.2023.11.17 val PER: 0.0327
2026-01-04 14:47:39,210: t15.2023.11.19 val PER: 0.0359
2026-01-04 14:47:39,210: t15.2023.11.26 val PER: 0.1145
2026-01-04 14:47:39,211: t15.2023.12.03 val PER: 0.1176
2026-01-04 14:47:39,211: t15.2023.12.08 val PER: 0.0939
2026-01-04 14:47:39,211: t15.2023.12.10 val PER: 0.0907
2026-01-04 14:47:39,211: t15.2023.12.17 val PER: 0.1403
2026-01-04 14:47:39,211: t15.2023.12.29 val PER: 0.1290
2026-01-04 14:47:39,211: t15.2024.02.25 val PER: 0.1053
2026-01-04 14:47:39,211: t15.2024.03.08 val PER: 0.2262
2026-01-04 14:47:39,211: t15.2024.03.15 val PER: 0.1926
2026-01-04 14:47:39,211: t15.2024.03.17 val PER: 0.1360
2026-01-04 14:47:39,212: t15.2024.05.10 val PER: 0.1471
2026-01-04 14:47:39,212: t15.2024.06.14 val PER: 0.1577
2026-01-04 14:47:39,212: t15.2024.07.19 val PER: 0.2294
2026-01-04 14:47:39,212: t15.2024.07.21 val PER: 0.0883
2026-01-04 14:47:39,212: t15.2024.07.28 val PER: 0.1316
2026-01-04 14:47:39,212: t15.2025.01.10 val PER: 0.2879
2026-01-04 14:47:39,212: t15.2025.01.12 val PER: 0.1401
2026-01-04 14:47:39,212: t15.2025.03.14 val PER: 0.3506
2026-01-04 14:47:39,212: t15.2025.03.16 val PER: 0.1767
2026-01-04 14:47:39,212: t15.2025.03.30 val PER: 0.2770
2026-01-04 14:47:39,213: t15.2025.04.13 val PER: 0.2168
2026-01-04 14:47:39,495: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_19000
2026-01-04 14:47:56,932: Train batch 19200: loss: 5.56 grad norm: 44.78 time: 0.063
2026-01-04 14:48:14,193: Train batch 19400: loss: 5.00 grad norm: 37.43 time: 0.052
2026-01-04 14:48:23,230: Running test after training batch: 19500
2026-01-04 14:48:23,392: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:48:28,069: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:48:28,103: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:48:30,045: Val batch 19500: PER (avg): 0.1446 CTC Loss (avg): 14.9969 WER(1gram): 45.18% (n=64) time: 6.814
2026-01-04 14:48:30,045: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 14:48:30,045: t15.2023.08.13 val PER: 0.1081
2026-01-04 14:48:30,045: t15.2023.08.18 val PER: 0.1006
2026-01-04 14:48:30,046: t15.2023.08.20 val PER: 0.1064
2026-01-04 14:48:30,046: t15.2023.08.25 val PER: 0.0873
2026-01-04 14:48:30,046: t15.2023.08.27 val PER: 0.1785
2026-01-04 14:48:30,046: t15.2023.09.01 val PER: 0.0739
2026-01-04 14:48:30,046: t15.2023.09.03 val PER: 0.1520
2026-01-04 14:48:30,046: t15.2023.09.24 val PER: 0.1359
2026-01-04 14:48:30,046: t15.2023.09.29 val PER: 0.1295
2026-01-04 14:48:30,046: t15.2023.10.01 val PER: 0.1664
2026-01-04 14:48:30,046: t15.2023.10.06 val PER: 0.0807
2026-01-04 14:48:30,046: t15.2023.10.08 val PER: 0.2558
2026-01-04 14:48:30,046: t15.2023.10.13 val PER: 0.1877
2026-01-04 14:48:30,046: t15.2023.10.15 val PER: 0.1483
2026-01-04 14:48:30,046: t15.2023.10.20 val PER: 0.1980
2026-01-04 14:48:30,046: t15.2023.10.22 val PER: 0.1080
2026-01-04 14:48:30,047: t15.2023.11.03 val PER: 0.1703
2026-01-04 14:48:30,047: t15.2023.11.04 val PER: 0.0341
2026-01-04 14:48:30,047: t15.2023.11.17 val PER: 0.0295
2026-01-04 14:48:30,047: t15.2023.11.19 val PER: 0.0339
2026-01-04 14:48:30,047: t15.2023.11.26 val PER: 0.1065
2026-01-04 14:48:30,047: t15.2023.12.03 val PER: 0.1103
2026-01-04 14:48:30,047: t15.2023.12.08 val PER: 0.0932
2026-01-04 14:48:30,047: t15.2023.12.10 val PER: 0.0867
2026-01-04 14:48:30,047: t15.2023.12.17 val PER: 0.1331
2026-01-04 14:48:30,047: t15.2023.12.29 val PER: 0.1290
2026-01-04 14:48:30,047: t15.2024.02.25 val PER: 0.1011
2026-01-04 14:48:30,048: t15.2024.03.08 val PER: 0.2233
2026-01-04 14:48:30,048: t15.2024.03.15 val PER: 0.1895
2026-01-04 14:48:30,048: t15.2024.03.17 val PER: 0.1339
2026-01-04 14:48:30,048: t15.2024.05.10 val PER: 0.1486
2026-01-04 14:48:30,049: t15.2024.06.14 val PER: 0.1514
2026-01-04 14:48:30,049: t15.2024.07.19 val PER: 0.2294
2026-01-04 14:48:30,049: t15.2024.07.21 val PER: 0.0848
2026-01-04 14:48:30,049: t15.2024.07.28 val PER: 0.1301
2026-01-04 14:48:30,049: t15.2025.01.10 val PER: 0.2824
2026-01-04 14:48:30,050: t15.2025.01.12 val PER: 0.1401
2026-01-04 14:48:30,050: t15.2025.03.14 val PER: 0.3506
2026-01-04 14:48:30,050: t15.2025.03.16 val PER: 0.1741
2026-01-04 14:48:30,050: t15.2025.03.30 val PER: 0.2759
2026-01-04 14:48:30,050: t15.2025.04.13 val PER: 0.2126
2026-01-04 14:48:30,325: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_19500
2026-01-04 14:48:40,122: Train batch 19600: loss: 7.53 grad norm: 48.04 time: 0.056
2026-01-04 14:48:57,300: Train batch 19800: loss: 7.31 grad norm: 53.18 time: 0.055
2026-01-04 14:49:14,369: Running test after training batch: 19999
2026-01-04 14:49:14,524: WER debug GT example: You can see the code at this point as well.
2026-01-04 14:49:20,422: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 14:49:20,456: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 14:49:22,375: Val batch 19999: PER (avg): 0.1457 CTC Loss (avg): 14.9893 WER(1gram): 45.43% (n=64) time: 8.006
2026-01-04 14:49:22,376: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-04 14:49:22,376: t15.2023.08.13 val PER: 0.1071
2026-01-04 14:49:22,376: t15.2023.08.18 val PER: 0.0997
2026-01-04 14:49:22,376: t15.2023.08.20 val PER: 0.1096
2026-01-04 14:49:22,376: t15.2023.08.25 val PER: 0.0904
2026-01-04 14:49:22,376: t15.2023.08.27 val PER: 0.1785
2026-01-04 14:49:22,376: t15.2023.09.01 val PER: 0.0747
2026-01-04 14:49:22,377: t15.2023.09.03 val PER: 0.1615
2026-01-04 14:49:22,377: t15.2023.09.24 val PER: 0.1335
2026-01-04 14:49:22,377: t15.2023.09.29 val PER: 0.1302
2026-01-04 14:49:22,377: t15.2023.10.01 val PER: 0.1684
2026-01-04 14:49:22,377: t15.2023.10.06 val PER: 0.0829
2026-01-04 14:49:22,377: t15.2023.10.08 val PER: 0.2476
2026-01-04 14:49:22,377: t15.2023.10.13 val PER: 0.1901
2026-01-04 14:49:22,377: t15.2023.10.15 val PER: 0.1463
2026-01-04 14:49:22,377: t15.2023.10.20 val PER: 0.1980
2026-01-04 14:49:22,377: t15.2023.10.22 val PER: 0.1069
2026-01-04 14:49:22,377: t15.2023.11.03 val PER: 0.1744
2026-01-04 14:49:22,377: t15.2023.11.04 val PER: 0.0273
2026-01-04 14:49:22,377: t15.2023.11.17 val PER: 0.0358
2026-01-04 14:49:22,377: t15.2023.11.19 val PER: 0.0339
2026-01-04 14:49:22,378: t15.2023.11.26 val PER: 0.1101
2026-01-04 14:49:22,378: t15.2023.12.03 val PER: 0.1082
2026-01-04 14:49:22,378: t15.2023.12.08 val PER: 0.0932
2026-01-04 14:49:22,378: t15.2023.12.10 val PER: 0.0867
2026-01-04 14:49:22,378: t15.2023.12.17 val PER: 0.1393
2026-01-04 14:49:22,378: t15.2023.12.29 val PER: 0.1256
2026-01-04 14:49:22,378: t15.2024.02.25 val PER: 0.1053
2026-01-04 14:49:22,378: t15.2024.03.08 val PER: 0.2276
2026-01-04 14:49:22,378: t15.2024.03.15 val PER: 0.1951
2026-01-04 14:49:22,378: t15.2024.03.17 val PER: 0.1290
2026-01-04 14:49:22,378: t15.2024.05.10 val PER: 0.1471
2026-01-04 14:49:22,378: t15.2024.06.14 val PER: 0.1593
2026-01-04 14:49:22,378: t15.2024.07.19 val PER: 0.2340
2026-01-04 14:49:22,378: t15.2024.07.21 val PER: 0.0910
2026-01-04 14:49:22,378: t15.2024.07.28 val PER: 0.1265
2026-01-04 14:49:22,379: t15.2025.01.10 val PER: 0.2837
2026-01-04 14:49:22,379: t15.2025.01.12 val PER: 0.1393
2026-01-04 14:49:22,379: t15.2025.03.14 val PER: 0.3447
2026-01-04 14:49:22,379: t15.2025.03.16 val PER: 0.1741
2026-01-04 14:49:22,380: t15.2025.03.30 val PER: 0.2805
2026-01-04 14:49:22,380: t15.2025.04.13 val PER: 0.2154
2026-01-04 14:49:22,667: Saved model to checkpoint: /tmp/e12511253_b2t_348736/trained_models/weight_decay/lr40/wd3e-4/checkpoint/checkpoint_batch_19999
2026-01-04 14:49:22,696: Best avg val PER achieved: 0.14612
2026-01-04 14:49:22,697: Total training time: 34.82 minutes
All runs finished. Outputs in: /tmp/e12511253_b2t_348736/trained_models
