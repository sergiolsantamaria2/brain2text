TMPDIR=/home/e12511253/tmp
JOB_TMP=/home/e12511253/tmp/e12511253_b2t_351129
TORCH_EXTENSIONS_DIR=/home/e12511253/tmp/e12511253_b2t_351129/torch_extensions
WANDB_DIR=/home/e12511253/tmp/e12511253_b2t_351129/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/home/e12511253/tmp/e12511253_b2t_351129/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  8 15:28 /home/e12511253/tmp/e12511253_b2t_351129/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
OUT_ROOT=/home/e12511253/Brain2Text/brain2text/trained_models
==============================================
Job: b2t5g  ID: 351129
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_5gram_only.yaml
Folders: configs/experiments/gru/papers/ablations
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/gru/papers/ablations ==========
Num configs: 5

=== RUN baseline.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline
2026-01-08 15:28:53,141: Using device: cuda:0
2026-01-08 15:32:41,195: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-08 15:32:41,224: Using 45 sessions after filtering (from 45).
2026-01-08 15:32:41,674: Using torch.compile (if available)
2026-01-08 15:32:41,674: torch.compile not available (torch<2.0). Skipping.
2026-01-08 15:32:41,674: Initialized RNN decoding model
2026-01-08 15:32:41,674: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-08 15:32:41,674: Model has 44,907,305 parameters
2026-01-08 15:32:41,674: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-08 15:32:42,961: Successfully initialized datasets
2026-01-08 15:32:42,961: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-08 15:32:44,451: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.182
2026-01-08 15:32:44,451: Running test after training batch: 0
2026-01-08 15:32:44,562: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:32:50,328: WER debug example
  GT : you can see the code at this point as well
  PR : she has from his
2026-01-08 15:32:51,323: WER debug example
  GT : how does it keep the cost down
  PR : money from
2026-01-08 15:36:43,945: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(5gram): 99.67% (n=256) time: 239.493
2026-01-08 15:36:43,949: WER lens: avg_true_words=5.99 avg_pred_words=2.82 max_pred_words=7
2026-01-08 15:36:43,955: t15.2023.08.13 val PER: 1.3056
2026-01-08 15:36:43,957: t15.2023.08.18 val PER: 1.4208
2026-01-08 15:36:43,958: t15.2023.08.20 val PER: 1.3002
2026-01-08 15:36:43,958: t15.2023.08.25 val PER: 1.3389
2026-01-08 15:36:43,958: t15.2023.08.27 val PER: 1.2460
2026-01-08 15:36:43,958: t15.2023.09.01 val PER: 1.4537
2026-01-08 15:36:43,958: t15.2023.09.03 val PER: 1.3171
2026-01-08 15:36:43,958: t15.2023.09.24 val PER: 1.5461
2026-01-08 15:36:43,958: t15.2023.09.29 val PER: 1.4671
2026-01-08 15:36:43,958: t15.2023.10.01 val PER: 1.2147
2026-01-08 15:36:43,958: t15.2023.10.06 val PER: 1.4876
2026-01-08 15:36:43,959: t15.2023.10.08 val PER: 1.1827
2026-01-08 15:36:43,959: t15.2023.10.13 val PER: 1.3964
2026-01-08 15:36:43,959: t15.2023.10.15 val PER: 1.3889
2026-01-08 15:36:43,959: t15.2023.10.20 val PER: 1.4866
2026-01-08 15:36:43,959: t15.2023.10.22 val PER: 1.3942
2026-01-08 15:36:43,959: t15.2023.11.03 val PER: 1.5923
2026-01-08 15:36:43,959: t15.2023.11.04 val PER: 2.0171
2026-01-08 15:36:43,959: t15.2023.11.17 val PER: 1.9518
2026-01-08 15:36:43,959: t15.2023.11.19 val PER: 1.6707
2026-01-08 15:36:43,959: t15.2023.11.26 val PER: 1.5413
2026-01-08 15:36:43,959: t15.2023.12.03 val PER: 1.4254
2026-01-08 15:36:43,959: t15.2023.12.08 val PER: 1.4487
2026-01-08 15:36:43,959: t15.2023.12.10 val PER: 1.6899
2026-01-08 15:36:43,959: t15.2023.12.17 val PER: 1.3077
2026-01-08 15:36:43,960: t15.2023.12.29 val PER: 1.4063
2026-01-08 15:36:43,960: t15.2024.02.25 val PER: 1.4228
2026-01-08 15:36:43,960: t15.2024.03.08 val PER: 1.3257
2026-01-08 15:36:43,960: t15.2024.03.15 val PER: 1.3196
2026-01-08 15:36:43,960: t15.2024.03.17 val PER: 1.4052
2026-01-08 15:36:43,960: t15.2024.05.10 val PER: 1.3224
2026-01-08 15:36:43,960: t15.2024.06.14 val PER: 1.5315
2026-01-08 15:36:43,960: t15.2024.07.19 val PER: 1.0817
2026-01-08 15:36:43,960: t15.2024.07.21 val PER: 1.6290
2026-01-08 15:36:43,960: t15.2024.07.28 val PER: 1.6588
2026-01-08 15:36:43,960: t15.2025.01.10 val PER: 1.0923
2026-01-08 15:36:43,960: t15.2025.01.12 val PER: 1.7629
2026-01-08 15:36:43,960: t15.2025.03.14 val PER: 1.0414
2026-01-08 15:36:43,960: t15.2025.03.16 val PER: 1.6257
2026-01-08 15:36:43,960: t15.2025.03.30 val PER: 1.2874
2026-01-08 15:36:43,961: t15.2025.04.13 val PER: 1.5949
2026-01-08 15:36:43,962: New best val WER(5gram) inf% --> 99.67%
2026-01-08 15:36:44,150: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_0
2026-01-08 15:37:00,396: Train batch 200: loss: 77.58 grad norm: 106.05 time: 0.054
2026-01-08 15:37:16,255: Train batch 400: loss: 53.92 grad norm: 92.24 time: 0.063
2026-01-08 15:37:24,075: Running test after training batch: 500
2026-01-08 15:37:24,192: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:37:29,237: WER debug example
  GT : you can see the code at this point as well
  PR : you'll need is the ease and at this guide is all
2026-01-08 15:37:29,382: WER debug example
  GT : how does it keep the cost down
  PR : and does it think that is tied
2026-01-08 15:38:14,915: Val batch 500: PER (avg): 0.5312 CTC Loss (avg): 55.3373 WER(5gram): 74.32% (n=256) time: 50.840
2026-01-08 15:38:14,917: WER lens: avg_true_words=5.99 avg_pred_words=5.56 max_pred_words=11
2026-01-08 15:38:14,919: t15.2023.08.13 val PER: 0.4688
2026-01-08 15:38:14,921: t15.2023.08.18 val PER: 0.4585
2026-01-08 15:38:14,923: t15.2023.08.20 val PER: 0.4424
2026-01-08 15:38:14,925: t15.2023.08.25 val PER: 0.4367
2026-01-08 15:38:14,926: t15.2023.08.27 val PER: 0.5289
2026-01-08 15:38:14,927: t15.2023.09.01 val PER: 0.4229
2026-01-08 15:38:14,929: t15.2023.09.03 val PER: 0.5131
2026-01-08 15:38:14,930: t15.2023.09.24 val PER: 0.4405
2026-01-08 15:38:14,931: t15.2023.09.29 val PER: 0.4780
2026-01-08 15:38:14,933: t15.2023.10.01 val PER: 0.5376
2026-01-08 15:38:14,934: t15.2023.10.06 val PER: 0.4381
2026-01-08 15:38:14,935: t15.2023.10.08 val PER: 0.5480
2026-01-08 15:38:14,937: t15.2023.10.13 val PER: 0.5896
2026-01-08 15:38:14,938: t15.2023.10.15 val PER: 0.5003
2026-01-08 15:38:14,939: t15.2023.10.20 val PER: 0.4564
2026-01-08 15:38:14,941: t15.2023.10.22 val PER: 0.4644
2026-01-08 15:38:14,942: t15.2023.11.03 val PER: 0.5204
2026-01-08 15:38:14,943: t15.2023.11.04 val PER: 0.2730
2026-01-08 15:38:14,945: t15.2023.11.17 val PER: 0.3624
2026-01-08 15:38:14,946: t15.2023.11.19 val PER: 0.3333
2026-01-08 15:38:14,947: t15.2023.11.26 val PER: 0.5710
2026-01-08 15:38:14,948: t15.2023.12.03 val PER: 0.5200
2026-01-08 15:38:14,950: t15.2023.12.08 val PER: 0.5333
2026-01-08 15:38:14,951: t15.2023.12.10 val PER: 0.4625
2026-01-08 15:38:14,953: t15.2023.12.17 val PER: 0.5738
2026-01-08 15:38:14,954: t15.2023.12.29 val PER: 0.5607
2026-01-08 15:38:14,955: t15.2024.02.25 val PER: 0.5042
2026-01-08 15:38:14,956: t15.2024.03.08 val PER: 0.6302
2026-01-08 15:38:14,957: t15.2024.03.15 val PER: 0.5754
2026-01-08 15:38:14,959: t15.2024.03.17 val PER: 0.5293
2026-01-08 15:38:14,960: t15.2024.05.10 val PER: 0.5676
2026-01-08 15:38:14,961: t15.2024.06.14 val PER: 0.5221
2026-01-08 15:38:14,962: t15.2024.07.19 val PER: 0.6908
2026-01-08 15:38:14,964: t15.2024.07.21 val PER: 0.4800
2026-01-08 15:38:14,965: t15.2024.07.28 val PER: 0.5243
2026-01-08 15:38:14,967: t15.2025.01.10 val PER: 0.7590
2026-01-08 15:38:14,968: t15.2025.01.12 val PER: 0.5743
2026-01-08 15:38:14,970: t15.2025.03.14 val PER: 0.7678
2026-01-08 15:38:14,971: t15.2025.03.16 val PER: 0.6165
2026-01-08 15:38:14,972: t15.2025.03.30 val PER: 0.7506
2026-01-08 15:38:14,976: t15.2025.04.13 val PER: 0.5792
2026-01-08 15:38:14,977: New best val WER(5gram) 99.67% --> 74.32%
2026-01-08 15:38:15,166: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_500
2026-01-08 15:38:23,107: Train batch 600: loss: 48.95 grad norm: 76.52 time: 0.079
2026-01-08 15:38:40,018: Train batch 800: loss: 40.49 grad norm: 83.19 time: 0.058
2026-01-08 15:38:57,228: Train batch 1000: loss: 42.57 grad norm: 79.08 time: 0.067
2026-01-08 15:38:57,232: Running test after training batch: 1000
2026-01-08 15:38:57,351: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:39:02,407: WER debug example
  GT : you can see the code at this point as well
  PR : you'd get me the code at this and is well
2026-01-08 15:39:02,551: WER debug example
  GT : how does it keep the cost down
  PR : howled as it is that it's not
2026-01-08 15:39:30,245: Val batch 1000: PER (avg): 0.4098 CTC Loss (avg): 42.4916 WER(5gram): 51.69% (n=256) time: 33.009
2026-01-08 15:39:30,247: WER lens: avg_true_words=5.99 avg_pred_words=5.61 max_pred_words=12
2026-01-08 15:39:30,250: t15.2023.08.13 val PER: 0.3794
2026-01-08 15:39:30,252: t15.2023.08.18 val PER: 0.3412
2026-01-08 15:39:30,254: t15.2023.08.20 val PER: 0.3392
2026-01-08 15:39:30,256: t15.2023.08.25 val PER: 0.3027
2026-01-08 15:39:30,257: t15.2023.08.27 val PER: 0.4164
2026-01-08 15:39:30,259: t15.2023.09.01 val PER: 0.3060
2026-01-08 15:39:30,261: t15.2023.09.03 val PER: 0.3990
2026-01-08 15:39:30,263: t15.2023.09.24 val PER: 0.3301
2026-01-08 15:39:30,265: t15.2023.09.29 val PER: 0.3650
2026-01-08 15:39:30,267: t15.2023.10.01 val PER: 0.4022
2026-01-08 15:39:30,268: t15.2023.10.06 val PER: 0.3154
2026-01-08 15:39:30,270: t15.2023.10.08 val PER: 0.4493
2026-01-08 15:39:30,272: t15.2023.10.13 val PER: 0.4694
2026-01-08 15:39:30,274: t15.2023.10.15 val PER: 0.3817
2026-01-08 15:39:30,275: t15.2023.10.20 val PER: 0.3893
2026-01-08 15:39:30,277: t15.2023.10.22 val PER: 0.3452
2026-01-08 15:39:30,279: t15.2023.11.03 val PER: 0.3976
2026-01-08 15:39:30,281: t15.2023.11.04 val PER: 0.1672
2026-01-08 15:39:30,283: t15.2023.11.17 val PER: 0.2737
2026-01-08 15:39:30,284: t15.2023.11.19 val PER: 0.2116
2026-01-08 15:39:30,286: t15.2023.11.26 val PER: 0.4449
2026-01-08 15:39:30,288: t15.2023.12.03 val PER: 0.4149
2026-01-08 15:39:30,290: t15.2023.12.08 val PER: 0.4061
2026-01-08 15:39:30,291: t15.2023.12.10 val PER: 0.3509
2026-01-08 15:39:30,293: t15.2023.12.17 val PER: 0.4106
2026-01-08 15:39:30,295: t15.2023.12.29 val PER: 0.4022
2026-01-08 15:39:30,297: t15.2024.02.25 val PER: 0.3581
2026-01-08 15:39:30,298: t15.2024.03.08 val PER: 0.4964
2026-01-08 15:39:30,302: t15.2024.03.15 val PER: 0.4415
2026-01-08 15:39:30,303: t15.2024.03.17 val PER: 0.4135
2026-01-08 15:39:30,305: t15.2024.05.10 val PER: 0.4190
2026-01-08 15:39:30,307: t15.2024.06.14 val PER: 0.4006
2026-01-08 15:39:30,309: t15.2024.07.19 val PER: 0.5333
2026-01-08 15:39:30,311: t15.2024.07.21 val PER: 0.3800
2026-01-08 15:39:30,313: t15.2024.07.28 val PER: 0.4081
2026-01-08 15:39:30,314: t15.2025.01.10 val PER: 0.6143
2026-01-08 15:39:30,317: t15.2025.01.12 val PER: 0.4511
2026-01-08 15:39:30,319: t15.2025.03.14 val PER: 0.6420
2026-01-08 15:39:30,322: t15.2025.03.16 val PER: 0.5026
2026-01-08 15:39:30,324: t15.2025.03.30 val PER: 0.6575
2026-01-08 15:39:30,326: t15.2025.04.13 val PER: 0.4893
2026-01-08 15:39:30,328: New best val WER(5gram) 74.32% --> 51.69%
2026-01-08 15:39:30,520: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_1000
2026-01-08 15:39:47,496: Train batch 1200: loss: 32.92 grad norm: 72.00 time: 0.069
2026-01-08 15:40:04,033: Train batch 1400: loss: 35.61 grad norm: 77.37 time: 0.061
2026-01-08 15:40:12,055: Running test after training batch: 1500
2026-01-08 15:40:12,195: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:40:17,148: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-08 15:40:17,247: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us in
2026-01-08 15:40:35,057: Val batch 1500: PER (avg): 0.3798 CTC Loss (avg): 37.2718 WER(5gram): 35.59% (n=256) time: 23.000
2026-01-08 15:40:35,059: WER lens: avg_true_words=5.99 avg_pred_words=5.30 max_pred_words=12
2026-01-08 15:40:35,061: t15.2023.08.13 val PER: 0.3503
2026-01-08 15:40:35,063: t15.2023.08.18 val PER: 0.3101
2026-01-08 15:40:35,066: t15.2023.08.20 val PER: 0.3082
2026-01-08 15:40:35,068: t15.2023.08.25 val PER: 0.2605
2026-01-08 15:40:35,070: t15.2023.08.27 val PER: 0.3891
2026-01-08 15:40:35,072: t15.2023.09.01 val PER: 0.2752
2026-01-08 15:40:35,074: t15.2023.09.03 val PER: 0.3860
2026-01-08 15:40:35,075: t15.2023.09.24 val PER: 0.3107
2026-01-08 15:40:35,077: t15.2023.09.29 val PER: 0.3261
2026-01-08 15:40:35,078: t15.2023.10.01 val PER: 0.3989
2026-01-08 15:40:35,080: t15.2023.10.06 val PER: 0.2896
2026-01-08 15:40:35,082: t15.2023.10.08 val PER: 0.4235
2026-01-08 15:40:35,084: t15.2023.10.13 val PER: 0.4391
2026-01-08 15:40:35,085: t15.2023.10.15 val PER: 0.3652
2026-01-08 15:40:35,087: t15.2023.10.20 val PER: 0.3322
2026-01-08 15:40:35,089: t15.2023.10.22 val PER: 0.3118
2026-01-08 15:40:35,090: t15.2023.11.03 val PER: 0.3623
2026-01-08 15:40:35,092: t15.2023.11.04 val PER: 0.1160
2026-01-08 15:40:35,094: t15.2023.11.17 val PER: 0.2364
2026-01-08 15:40:35,095: t15.2023.11.19 val PER: 0.1657
2026-01-08 15:40:35,097: t15.2023.11.26 val PER: 0.4174
2026-01-08 15:40:35,099: t15.2023.12.03 val PER: 0.3729
2026-01-08 15:40:35,100: t15.2023.12.08 val PER: 0.3429
2026-01-08 15:40:35,102: t15.2023.12.10 val PER: 0.2996
2026-01-08 15:40:35,104: t15.2023.12.17 val PER: 0.3711
2026-01-08 15:40:35,105: t15.2023.12.29 val PER: 0.3665
2026-01-08 15:40:35,107: t15.2024.02.25 val PER: 0.3104
2026-01-08 15:40:35,109: t15.2024.03.08 val PER: 0.4680
2026-01-08 15:40:35,110: t15.2024.03.15 val PER: 0.4228
2026-01-08 15:40:35,112: t15.2024.03.17 val PER: 0.3703
2026-01-08 15:40:35,113: t15.2024.05.10 val PER: 0.3848
2026-01-08 15:40:35,115: t15.2024.06.14 val PER: 0.4117
2026-01-08 15:40:35,117: t15.2024.07.19 val PER: 0.5208
2026-01-08 15:40:35,118: t15.2024.07.21 val PER: 0.3386
2026-01-08 15:40:35,119: t15.2024.07.28 val PER: 0.3735
2026-01-08 15:40:35,121: t15.2025.01.10 val PER: 0.6336
2026-01-08 15:40:35,123: t15.2025.01.12 val PER: 0.4226
2026-01-08 15:40:35,124: t15.2025.03.14 val PER: 0.6006
2026-01-08 15:40:35,126: t15.2025.03.16 val PER: 0.4529
2026-01-08 15:40:35,127: t15.2025.03.30 val PER: 0.6483
2026-01-08 15:40:35,129: t15.2025.04.13 val PER: 0.4765
2026-01-08 15:40:35,130: New best val WER(5gram) 51.69% --> 35.59%
2026-01-08 15:40:35,343: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_1500
2026-01-08 15:40:43,671: Train batch 1600: loss: 36.55 grad norm: 80.49 time: 0.065
2026-01-08 15:40:59,742: Train batch 1800: loss: 35.26 grad norm: 72.83 time: 0.091
2026-01-08 15:41:15,688: Train batch 2000: loss: 33.98 grad norm: 71.05 time: 0.067
2026-01-08 15:41:15,691: Running test after training batch: 2000
2026-01-08 15:41:15,789: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:41:20,729: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 15:41:20,818: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 15:41:38,017: Val batch 2000: PER (avg): 0.3276 CTC Loss (avg): 32.9064 WER(5gram): 29.34% (n=256) time: 22.324
2026-01-08 15:41:38,019: WER lens: avg_true_words=5.99 avg_pred_words=5.79 max_pred_words=12
2026-01-08 15:41:38,022: t15.2023.08.13 val PER: 0.3025
2026-01-08 15:41:38,023: t15.2023.08.18 val PER: 0.2607
2026-01-08 15:41:38,025: t15.2023.08.20 val PER: 0.2653
2026-01-08 15:41:38,026: t15.2023.08.25 val PER: 0.2364
2026-01-08 15:41:38,028: t15.2023.08.27 val PER: 0.3392
2026-01-08 15:41:38,029: t15.2023.09.01 val PER: 0.2386
2026-01-08 15:41:38,031: t15.2023.09.03 val PER: 0.3219
2026-01-08 15:41:38,034: t15.2023.09.24 val PER: 0.2524
2026-01-08 15:41:38,036: t15.2023.09.29 val PER: 0.2750
2026-01-08 15:41:38,038: t15.2023.10.01 val PER: 0.3250
2026-01-08 15:41:38,039: t15.2023.10.06 val PER: 0.2400
2026-01-08 15:41:38,041: t15.2023.10.08 val PER: 0.3938
2026-01-08 15:41:38,043: t15.2023.10.13 val PER: 0.3801
2026-01-08 15:41:38,044: t15.2023.10.15 val PER: 0.3032
2026-01-08 15:41:38,046: t15.2023.10.20 val PER: 0.3087
2026-01-08 15:41:38,048: t15.2023.10.22 val PER: 0.2617
2026-01-08 15:41:38,051: t15.2023.11.03 val PER: 0.3223
2026-01-08 15:41:38,052: t15.2023.11.04 val PER: 0.1024
2026-01-08 15:41:38,054: t15.2023.11.17 val PER: 0.1695
2026-01-08 15:41:38,055: t15.2023.11.19 val PER: 0.1238
2026-01-08 15:41:38,057: t15.2023.11.26 val PER: 0.3609
2026-01-08 15:41:38,059: t15.2023.12.03 val PER: 0.3151
2026-01-08 15:41:38,061: t15.2023.12.08 val PER: 0.3089
2026-01-08 15:41:38,062: t15.2023.12.10 val PER: 0.2668
2026-01-08 15:41:38,064: t15.2023.12.17 val PER: 0.3202
2026-01-08 15:41:38,065: t15.2023.12.29 val PER: 0.3240
2026-01-08 15:41:38,067: t15.2024.02.25 val PER: 0.2767
2026-01-08 15:41:38,068: t15.2024.03.08 val PER: 0.3940
2026-01-08 15:41:38,070: t15.2024.03.15 val PER: 0.3621
2026-01-08 15:41:38,071: t15.2024.03.17 val PER: 0.3396
2026-01-08 15:41:38,073: t15.2024.05.10 val PER: 0.3388
2026-01-08 15:41:38,075: t15.2024.06.14 val PER: 0.3423
2026-01-08 15:41:38,076: t15.2024.07.19 val PER: 0.4680
2026-01-08 15:41:38,078: t15.2024.07.21 val PER: 0.2828
2026-01-08 15:41:38,080: t15.2024.07.28 val PER: 0.3169
2026-01-08 15:41:38,081: t15.2025.01.10 val PER: 0.5482
2026-01-08 15:41:38,083: t15.2025.01.12 val PER: 0.3841
2026-01-08 15:41:38,085: t15.2025.03.14 val PER: 0.5222
2026-01-08 15:41:38,087: t15.2025.03.16 val PER: 0.3848
2026-01-08 15:41:38,088: t15.2025.03.30 val PER: 0.5517
2026-01-08 15:41:38,090: t15.2025.04.13 val PER: 0.4108
2026-01-08 15:41:38,092: New best val WER(5gram) 35.59% --> 29.34%
2026-01-08 15:41:38,279: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_2000
2026-01-08 15:41:54,091: Train batch 2200: loss: 28.49 grad norm: 71.16 time: 0.061
2026-01-08 15:42:09,878: Train batch 2400: loss: 29.11 grad norm: 62.86 time: 0.053
2026-01-08 15:42:17,905: Running test after training batch: 2500
2026-01-08 15:42:18,028: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:42:23,252: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 15:42:23,311: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us at
2026-01-08 15:42:38,548: Val batch 2500: PER (avg): 0.3014 CTC Loss (avg): 30.0436 WER(5gram): 27.64% (n=256) time: 20.641
2026-01-08 15:42:38,551: WER lens: avg_true_words=5.99 avg_pred_words=5.88 max_pred_words=12
2026-01-08 15:42:38,554: t15.2023.08.13 val PER: 0.2786
2026-01-08 15:42:38,555: t15.2023.08.18 val PER: 0.2381
2026-01-08 15:42:38,557: t15.2023.08.20 val PER: 0.2303
2026-01-08 15:42:38,559: t15.2023.08.25 val PER: 0.2048
2026-01-08 15:42:38,561: t15.2023.08.27 val PER: 0.3199
2026-01-08 15:42:38,563: t15.2023.09.01 val PER: 0.2037
2026-01-08 15:42:38,564: t15.2023.09.03 val PER: 0.2957
2026-01-08 15:42:38,566: t15.2023.09.24 val PER: 0.2233
2026-01-08 15:42:38,568: t15.2023.09.29 val PER: 0.2495
2026-01-08 15:42:38,569: t15.2023.10.01 val PER: 0.3085
2026-01-08 15:42:38,571: t15.2023.10.06 val PER: 0.2217
2026-01-08 15:42:38,573: t15.2023.10.08 val PER: 0.3694
2026-01-08 15:42:38,575: t15.2023.10.13 val PER: 0.3522
2026-01-08 15:42:38,577: t15.2023.10.15 val PER: 0.2868
2026-01-08 15:42:38,579: t15.2023.10.20 val PER: 0.2752
2026-01-08 15:42:38,581: t15.2023.10.22 val PER: 0.2216
2026-01-08 15:42:38,583: t15.2023.11.03 val PER: 0.2863
2026-01-08 15:42:38,584: t15.2023.11.04 val PER: 0.0683
2026-01-08 15:42:38,586: t15.2023.11.17 val PER: 0.1571
2026-01-08 15:42:38,588: t15.2023.11.19 val PER: 0.1178
2026-01-08 15:42:38,589: t15.2023.11.26 val PER: 0.3391
2026-01-08 15:42:38,591: t15.2023.12.03 val PER: 0.2920
2026-01-08 15:42:38,593: t15.2023.12.08 val PER: 0.2790
2026-01-08 15:42:38,594: t15.2023.12.10 val PER: 0.2339
2026-01-08 15:42:38,596: t15.2023.12.17 val PER: 0.2807
2026-01-08 15:42:38,598: t15.2023.12.29 val PER: 0.3082
2026-01-08 15:42:38,599: t15.2024.02.25 val PER: 0.2346
2026-01-08 15:42:38,601: t15.2024.03.08 val PER: 0.3599
2026-01-08 15:42:38,602: t15.2024.03.15 val PER: 0.3496
2026-01-08 15:42:38,604: t15.2024.03.17 val PER: 0.3131
2026-01-08 15:42:38,607: t15.2024.05.10 val PER: 0.3150
2026-01-08 15:42:38,609: t15.2024.06.14 val PER: 0.3297
2026-01-08 15:42:38,610: t15.2024.07.19 val PER: 0.4324
2026-01-08 15:42:38,612: t15.2024.07.21 val PER: 0.2552
2026-01-08 15:42:38,614: t15.2024.07.28 val PER: 0.3051
2026-01-08 15:42:38,616: t15.2025.01.10 val PER: 0.4890
2026-01-08 15:42:38,617: t15.2025.01.12 val PER: 0.3580
2026-01-08 15:42:38,619: t15.2025.03.14 val PER: 0.5015
2026-01-08 15:42:38,621: t15.2025.03.16 val PER: 0.3560
2026-01-08 15:42:38,622: t15.2025.03.30 val PER: 0.5092
2026-01-08 15:42:38,624: t15.2025.04.13 val PER: 0.3980
2026-01-08 15:42:38,625: New best val WER(5gram) 29.34% --> 27.64%
2026-01-08 15:42:38,819: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_2500
2026-01-08 15:42:46,697: Train batch 2600: loss: 35.30 grad norm: 81.40 time: 0.055
2026-01-08 15:43:02,738: Train batch 2800: loss: 25.61 grad norm: 69.83 time: 0.082
2026-01-08 15:43:19,467: Train batch 3000: loss: 30.82 grad norm: 70.28 time: 0.084
2026-01-08 15:43:19,469: Running test after training batch: 3000
2026-01-08 15:43:19,584: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:43:24,917: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 15:43:24,987: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost of
2026-01-08 15:43:39,654: Val batch 3000: PER (avg): 0.2794 CTC Loss (avg): 27.6979 WER(5gram): 26.01% (n=256) time: 20.183
2026-01-08 15:43:39,656: WER lens: avg_true_words=5.99 avg_pred_words=6.02 max_pred_words=12
2026-01-08 15:43:39,659: t15.2023.08.13 val PER: 0.2578
2026-01-08 15:43:39,661: t15.2023.08.18 val PER: 0.2179
2026-01-08 15:43:39,663: t15.2023.08.20 val PER: 0.2137
2026-01-08 15:43:39,664: t15.2023.08.25 val PER: 0.1898
2026-01-08 15:43:39,666: t15.2023.08.27 val PER: 0.2990
2026-01-08 15:43:39,667: t15.2023.09.01 val PER: 0.1916
2026-01-08 15:43:39,669: t15.2023.09.03 val PER: 0.2850
2026-01-08 15:43:39,673: t15.2023.09.24 val PER: 0.2184
2026-01-08 15:43:39,675: t15.2023.09.29 val PER: 0.2291
2026-01-08 15:43:39,676: t15.2023.10.01 val PER: 0.2827
2026-01-08 15:43:39,678: t15.2023.10.06 val PER: 0.1981
2026-01-08 15:43:39,680: t15.2023.10.08 val PER: 0.3464
2026-01-08 15:43:39,681: t15.2023.10.13 val PER: 0.3437
2026-01-08 15:43:39,683: t15.2023.10.15 val PER: 0.2630
2026-01-08 15:43:39,684: t15.2023.10.20 val PER: 0.2450
2026-01-08 15:43:39,686: t15.2023.10.22 val PER: 0.2016
2026-01-08 15:43:39,687: t15.2023.11.03 val PER: 0.2666
2026-01-08 15:43:39,688: t15.2023.11.04 val PER: 0.0819
2026-01-08 15:43:39,690: t15.2023.11.17 val PER: 0.1260
2026-01-08 15:43:39,691: t15.2023.11.19 val PER: 0.1158
2026-01-08 15:43:39,693: t15.2023.11.26 val PER: 0.2949
2026-01-08 15:43:39,695: t15.2023.12.03 val PER: 0.2500
2026-01-08 15:43:39,696: t15.2023.12.08 val PER: 0.2537
2026-01-08 15:43:39,698: t15.2023.12.10 val PER: 0.2194
2026-01-08 15:43:39,700: t15.2023.12.17 val PER: 0.2734
2026-01-08 15:43:39,701: t15.2023.12.29 val PER: 0.2711
2026-01-08 15:43:39,703: t15.2024.02.25 val PER: 0.2500
2026-01-08 15:43:39,704: t15.2024.03.08 val PER: 0.3656
2026-01-08 15:43:39,706: t15.2024.03.15 val PER: 0.3352
2026-01-08 15:43:39,707: t15.2024.03.17 val PER: 0.2901
2026-01-08 15:43:39,709: t15.2024.05.10 val PER: 0.3046
2026-01-08 15:43:39,710: t15.2024.06.14 val PER: 0.2965
2026-01-08 15:43:39,712: t15.2024.07.19 val PER: 0.4015
2026-01-08 15:43:39,713: t15.2024.07.21 val PER: 0.2276
2026-01-08 15:43:39,715: t15.2024.07.28 val PER: 0.2772
2026-01-08 15:43:39,716: t15.2025.01.10 val PER: 0.4862
2026-01-08 15:43:39,718: t15.2025.01.12 val PER: 0.3256
2026-01-08 15:43:39,719: t15.2025.03.14 val PER: 0.4393
2026-01-08 15:43:39,721: t15.2025.03.16 val PER: 0.3338
2026-01-08 15:43:39,722: t15.2025.03.30 val PER: 0.4770
2026-01-08 15:43:39,724: t15.2025.04.13 val PER: 0.3581
2026-01-08 15:43:39,725: New best val WER(5gram) 27.64% --> 26.01%
2026-01-08 15:43:39,916: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_3000
2026-01-08 15:43:56,852: Train batch 3200: loss: 26.53 grad norm: 67.67 time: 0.077
2026-01-08 15:44:12,933: Train batch 3400: loss: 17.78 grad norm: 54.41 time: 0.049
2026-01-08 15:44:20,787: Running test after training batch: 3500
2026-01-08 15:44:20,890: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:44:25,884: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 15:44:25,943: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us in
2026-01-08 15:44:40,218: Val batch 3500: PER (avg): 0.2680 CTC Loss (avg): 26.9179 WER(5gram): 22.36% (n=256) time: 19.429
2026-01-08 15:44:40,220: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-08 15:44:40,222: t15.2023.08.13 val PER: 0.2318
2026-01-08 15:44:40,224: t15.2023.08.18 val PER: 0.2070
2026-01-08 15:44:40,225: t15.2023.08.20 val PER: 0.2113
2026-01-08 15:44:40,227: t15.2023.08.25 val PER: 0.1852
2026-01-08 15:44:40,230: t15.2023.08.27 val PER: 0.2733
2026-01-08 15:44:40,231: t15.2023.09.01 val PER: 0.1810
2026-01-08 15:44:40,233: t15.2023.09.03 val PER: 0.2601
2026-01-08 15:44:40,234: t15.2023.09.24 val PER: 0.2027
2026-01-08 15:44:40,236: t15.2023.09.29 val PER: 0.2183
2026-01-08 15:44:40,237: t15.2023.10.01 val PER: 0.2748
2026-01-08 15:44:40,239: t15.2023.10.06 val PER: 0.1970
2026-01-08 15:44:40,240: t15.2023.10.08 val PER: 0.3681
2026-01-08 15:44:40,242: t15.2023.10.13 val PER: 0.3072
2026-01-08 15:44:40,243: t15.2023.10.15 val PER: 0.2512
2026-01-08 15:44:40,244: t15.2023.10.20 val PER: 0.2550
2026-01-08 15:44:40,246: t15.2023.10.22 val PER: 0.2004
2026-01-08 15:44:40,247: t15.2023.11.03 val PER: 0.2659
2026-01-08 15:44:40,249: t15.2023.11.04 val PER: 0.0887
2026-01-08 15:44:40,250: t15.2023.11.17 val PER: 0.1244
2026-01-08 15:44:40,252: t15.2023.11.19 val PER: 0.1098
2026-01-08 15:44:40,253: t15.2023.11.26 val PER: 0.2841
2026-01-08 15:44:40,255: t15.2023.12.03 val PER: 0.2342
2026-01-08 15:44:40,257: t15.2023.12.08 val PER: 0.2390
2026-01-08 15:44:40,258: t15.2023.12.10 val PER: 0.2024
2026-01-08 15:44:40,260: t15.2023.12.17 val PER: 0.2703
2026-01-08 15:44:40,261: t15.2023.12.29 val PER: 0.2642
2026-01-08 15:44:40,263: t15.2024.02.25 val PER: 0.2317
2026-01-08 15:44:40,264: t15.2024.03.08 val PER: 0.3371
2026-01-08 15:44:40,266: t15.2024.03.15 val PER: 0.3196
2026-01-08 15:44:40,267: t15.2024.03.17 val PER: 0.2817
2026-01-08 15:44:40,269: t15.2024.05.10 val PER: 0.2630
2026-01-08 15:44:40,271: t15.2024.06.14 val PER: 0.3076
2026-01-08 15:44:40,272: t15.2024.07.19 val PER: 0.3850
2026-01-08 15:44:40,273: t15.2024.07.21 val PER: 0.2124
2026-01-08 15:44:40,275: t15.2024.07.28 val PER: 0.2794
2026-01-08 15:44:40,276: t15.2025.01.10 val PER: 0.4793
2026-01-08 15:44:40,278: t15.2025.01.12 val PER: 0.2895
2026-01-08 15:44:40,279: t15.2025.03.14 val PER: 0.4630
2026-01-08 15:44:40,281: t15.2025.03.16 val PER: 0.3272
2026-01-08 15:44:40,282: t15.2025.03.30 val PER: 0.4552
2026-01-08 15:44:40,284: t15.2025.04.13 val PER: 0.3381
2026-01-08 15:44:40,286: New best val WER(5gram) 26.01% --> 22.36%
2026-01-08 15:44:40,483: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_3500
2026-01-08 15:44:48,347: Train batch 3600: loss: 22.73 grad norm: 61.11 time: 0.067
2026-01-08 15:45:04,067: Train batch 3800: loss: 25.04 grad norm: 65.38 time: 0.067
2026-01-08 15:45:20,711: Train batch 4000: loss: 19.82 grad norm: 53.68 time: 0.056
2026-01-08 15:45:20,713: Running test after training batch: 4000
2026-01-08 15:45:20,832: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:45:26,025: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-08 15:45:26,076: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost at
2026-01-08 15:45:39,280: Val batch 4000: PER (avg): 0.2478 CTC Loss (avg): 24.3555 WER(5gram): 26.34% (n=256) time: 18.565
2026-01-08 15:45:39,283: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-08 15:45:39,287: t15.2023.08.13 val PER: 0.2152
2026-01-08 15:45:39,289: t15.2023.08.18 val PER: 0.2045
2026-01-08 15:45:39,291: t15.2023.08.20 val PER: 0.2041
2026-01-08 15:45:39,293: t15.2023.08.25 val PER: 0.1596
2026-01-08 15:45:39,295: t15.2023.08.27 val PER: 0.2749
2026-01-08 15:45:39,297: t15.2023.09.01 val PER: 0.1591
2026-01-08 15:45:39,299: t15.2023.09.03 val PER: 0.2447
2026-01-08 15:45:39,301: t15.2023.09.24 val PER: 0.1893
2026-01-08 15:45:39,302: t15.2023.09.29 val PER: 0.1972
2026-01-08 15:45:39,304: t15.2023.10.01 val PER: 0.2609
2026-01-08 15:45:39,306: t15.2023.10.06 val PER: 0.1668
2026-01-08 15:45:39,308: t15.2023.10.08 val PER: 0.3194
2026-01-08 15:45:39,310: t15.2023.10.13 val PER: 0.3072
2026-01-08 15:45:39,312: t15.2023.10.15 val PER: 0.2314
2026-01-08 15:45:39,314: t15.2023.10.20 val PER: 0.2215
2026-01-08 15:45:39,316: t15.2023.10.22 val PER: 0.1927
2026-01-08 15:45:39,318: t15.2023.11.03 val PER: 0.2415
2026-01-08 15:45:39,321: t15.2023.11.04 val PER: 0.0648
2026-01-08 15:45:39,323: t15.2023.11.17 val PER: 0.1104
2026-01-08 15:45:39,324: t15.2023.11.19 val PER: 0.1158
2026-01-08 15:45:39,326: t15.2023.11.26 val PER: 0.2594
2026-01-08 15:45:39,328: t15.2023.12.03 val PER: 0.2111
2026-01-08 15:45:39,330: t15.2023.12.08 val PER: 0.2244
2026-01-08 15:45:39,333: t15.2023.12.10 val PER: 0.1892
2026-01-08 15:45:39,334: t15.2023.12.17 val PER: 0.2318
2026-01-08 15:45:39,336: t15.2023.12.29 val PER: 0.2498
2026-01-08 15:45:39,338: t15.2024.02.25 val PER: 0.2219
2026-01-08 15:45:39,340: t15.2024.03.08 val PER: 0.3314
2026-01-08 15:45:39,342: t15.2024.03.15 val PER: 0.2971
2026-01-08 15:45:39,344: t15.2024.03.17 val PER: 0.2524
2026-01-08 15:45:39,345: t15.2024.05.10 val PER: 0.2689
2026-01-08 15:45:39,347: t15.2024.06.14 val PER: 0.2666
2026-01-08 15:45:39,349: t15.2024.07.19 val PER: 0.3645
2026-01-08 15:45:39,351: t15.2024.07.21 val PER: 0.1931
2026-01-08 15:45:39,353: t15.2024.07.28 val PER: 0.2441
2026-01-08 15:45:39,355: t15.2025.01.10 val PER: 0.4215
2026-01-08 15:45:39,356: t15.2025.01.12 val PER: 0.2748
2026-01-08 15:45:39,358: t15.2025.03.14 val PER: 0.4024
2026-01-08 15:45:39,360: t15.2025.03.16 val PER: 0.3154
2026-01-08 15:45:39,362: t15.2025.03.30 val PER: 0.4103
2026-01-08 15:45:39,365: t15.2025.04.13 val PER: 0.3252
2026-01-08 15:45:39,509: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_4000
2026-01-08 15:45:55,798: Train batch 4200: loss: 22.46 grad norm: 64.67 time: 0.079
2026-01-08 15:46:12,327: Train batch 4400: loss: 16.85 grad norm: 49.90 time: 0.068
2026-01-08 15:46:20,667: Running test after training batch: 4500
2026-01-08 15:46:20,767: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:46:26,126: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 15:46:26,184: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost net
2026-01-08 15:46:38,896: Val batch 4500: PER (avg): 0.2383 CTC Loss (avg): 23.2812 WER(5gram): 22.36% (n=256) time: 18.226
2026-01-08 15:46:38,898: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-08 15:46:38,900: t15.2023.08.13 val PER: 0.2069
2026-01-08 15:46:38,902: t15.2023.08.18 val PER: 0.1785
2026-01-08 15:46:38,903: t15.2023.08.20 val PER: 0.1962
2026-01-08 15:46:38,905: t15.2023.08.25 val PER: 0.1476
2026-01-08 15:46:38,906: t15.2023.08.27 val PER: 0.2588
2026-01-08 15:46:38,908: t15.2023.09.01 val PER: 0.1518
2026-01-08 15:46:38,910: t15.2023.09.03 val PER: 0.2435
2026-01-08 15:46:38,911: t15.2023.09.24 val PER: 0.1650
2026-01-08 15:46:38,913: t15.2023.09.29 val PER: 0.1895
2026-01-08 15:46:38,915: t15.2023.10.01 val PER: 0.2517
2026-01-08 15:46:38,916: t15.2023.10.06 val PER: 0.1529
2026-01-08 15:46:38,918: t15.2023.10.08 val PER: 0.3221
2026-01-08 15:46:38,919: t15.2023.10.13 val PER: 0.3018
2026-01-08 15:46:38,921: t15.2023.10.15 val PER: 0.2360
2026-01-08 15:46:38,922: t15.2023.10.20 val PER: 0.2148
2026-01-08 15:46:38,924: t15.2023.10.22 val PER: 0.1837
2026-01-08 15:46:38,926: t15.2023.11.03 val PER: 0.2402
2026-01-08 15:46:38,927: t15.2023.11.04 val PER: 0.0614
2026-01-08 15:46:38,929: t15.2023.11.17 val PER: 0.0995
2026-01-08 15:46:38,930: t15.2023.11.19 val PER: 0.0958
2026-01-08 15:46:38,932: t15.2023.11.26 val PER: 0.2594
2026-01-08 15:46:38,934: t15.2023.12.03 val PER: 0.2069
2026-01-08 15:46:38,935: t15.2023.12.08 val PER: 0.2091
2026-01-08 15:46:38,937: t15.2023.12.10 val PER: 0.1827
2026-01-08 15:46:38,938: t15.2023.12.17 val PER: 0.2349
2026-01-08 15:46:38,940: t15.2023.12.29 val PER: 0.2457
2026-01-08 15:46:38,941: t15.2024.02.25 val PER: 0.1952
2026-01-08 15:46:38,943: t15.2024.03.08 val PER: 0.3215
2026-01-08 15:46:38,944: t15.2024.03.15 val PER: 0.2864
2026-01-08 15:46:38,946: t15.2024.03.17 val PER: 0.2420
2026-01-08 15:46:38,947: t15.2024.05.10 val PER: 0.2600
2026-01-08 15:46:38,949: t15.2024.06.14 val PER: 0.2587
2026-01-08 15:46:38,950: t15.2024.07.19 val PER: 0.3461
2026-01-08 15:46:38,952: t15.2024.07.21 val PER: 0.1814
2026-01-08 15:46:38,953: t15.2024.07.28 val PER: 0.2250
2026-01-08 15:46:38,955: t15.2025.01.10 val PER: 0.4160
2026-01-08 15:46:38,956: t15.2025.01.12 val PER: 0.2679
2026-01-08 15:46:38,958: t15.2025.03.14 val PER: 0.3935
2026-01-08 15:46:38,960: t15.2025.03.16 val PER: 0.2958
2026-01-08 15:46:38,961: t15.2025.03.30 val PER: 0.4057
2026-01-08 15:46:38,963: t15.2025.04.13 val PER: 0.2996
2026-01-08 15:46:39,104: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_4500
2026-01-08 15:46:47,398: Train batch 4600: loss: 20.14 grad norm: 69.31 time: 0.063
2026-01-08 15:47:03,906: Train batch 4800: loss: 13.60 grad norm: 54.37 time: 0.064
2026-01-08 15:47:20,028: Train batch 5000: loss: 31.87 grad norm: 86.91 time: 0.064
2026-01-08 15:47:20,031: Running test after training batch: 5000
2026-01-08 15:47:20,138: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:47:25,172: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 15:47:25,242: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost net
2026-01-08 15:47:37,934: Val batch 5000: PER (avg): 0.2259 CTC Loss (avg): 22.1423 WER(5gram): 23.27% (n=256) time: 17.897
2026-01-08 15:47:37,938: WER lens: avg_true_words=5.99 avg_pred_words=6.23 max_pred_words=12
2026-01-08 15:47:37,940: t15.2023.08.13 val PER: 0.1985
2026-01-08 15:47:37,942: t15.2023.08.18 val PER: 0.1760
2026-01-08 15:47:37,944: t15.2023.08.20 val PER: 0.1811
2026-01-08 15:47:37,946: t15.2023.08.25 val PER: 0.1295
2026-01-08 15:47:37,948: t15.2023.08.27 val PER: 0.2315
2026-01-08 15:47:37,949: t15.2023.09.01 val PER: 0.1380
2026-01-08 15:47:37,951: t15.2023.09.03 val PER: 0.2316
2026-01-08 15:47:37,952: t15.2023.09.24 val PER: 0.1772
2026-01-08 15:47:37,954: t15.2023.09.29 val PER: 0.1787
2026-01-08 15:47:37,956: t15.2023.10.01 val PER: 0.2417
2026-01-08 15:47:37,958: t15.2023.10.06 val PER: 0.1367
2026-01-08 15:47:37,961: t15.2023.10.08 val PER: 0.3058
2026-01-08 15:47:37,963: t15.2023.10.13 val PER: 0.2886
2026-01-08 15:47:37,965: t15.2023.10.15 val PER: 0.2162
2026-01-08 15:47:37,968: t15.2023.10.20 val PER: 0.2248
2026-01-08 15:47:37,970: t15.2023.10.22 val PER: 0.1670
2026-01-08 15:47:37,972: t15.2023.11.03 val PER: 0.2239
2026-01-08 15:47:37,973: t15.2023.11.04 val PER: 0.0683
2026-01-08 15:47:37,975: t15.2023.11.17 val PER: 0.0824
2026-01-08 15:47:37,977: t15.2023.11.19 val PER: 0.0778
2026-01-08 15:47:37,978: t15.2023.11.26 val PER: 0.2449
2026-01-08 15:47:37,980: t15.2023.12.03 val PER: 0.1996
2026-01-08 15:47:37,981: t15.2023.12.08 val PER: 0.1917
2026-01-08 15:47:37,983: t15.2023.12.10 val PER: 0.1669
2026-01-08 15:47:37,984: t15.2023.12.17 val PER: 0.2339
2026-01-08 15:47:37,986: t15.2023.12.29 val PER: 0.2286
2026-01-08 15:47:37,988: t15.2024.02.25 val PER: 0.1952
2026-01-08 15:47:37,989: t15.2024.03.08 val PER: 0.3073
2026-01-08 15:47:37,991: t15.2024.03.15 val PER: 0.2752
2026-01-08 15:47:37,993: t15.2024.03.17 val PER: 0.2245
2026-01-08 15:47:37,995: t15.2024.05.10 val PER: 0.2496
2026-01-08 15:47:37,996: t15.2024.06.14 val PER: 0.2382
2026-01-08 15:47:37,998: t15.2024.07.19 val PER: 0.3270
2026-01-08 15:47:37,999: t15.2024.07.21 val PER: 0.1738
2026-01-08 15:47:38,001: t15.2024.07.28 val PER: 0.2184
2026-01-08 15:47:38,002: t15.2025.01.10 val PER: 0.3760
2026-01-08 15:47:38,004: t15.2025.01.12 val PER: 0.2494
2026-01-08 15:47:38,005: t15.2025.03.14 val PER: 0.3920
2026-01-08 15:47:38,007: t15.2025.03.16 val PER: 0.2788
2026-01-08 15:47:38,009: t15.2025.03.30 val PER: 0.4126
2026-01-08 15:47:38,011: t15.2025.04.13 val PER: 0.2967
2026-01-08 15:47:38,147: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_5000
2026-01-08 15:47:55,195: Train batch 5200: loss: 16.72 grad norm: 61.89 time: 0.053
2026-01-08 15:48:12,355: Train batch 5400: loss: 17.49 grad norm: 59.99 time: 0.071
2026-01-08 15:48:20,934: Running test after training batch: 5500
2026-01-08 15:48:21,078: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:48:26,425: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 15:48:26,485: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 15:48:39,047: Val batch 5500: PER (avg): 0.2162 CTC Loss (avg): 21.1080 WER(5gram): 20.53% (n=256) time: 18.110
2026-01-08 15:48:39,049: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-08 15:48:39,052: t15.2023.08.13 val PER: 0.1913
2026-01-08 15:48:39,065: t15.2023.08.18 val PER: 0.1534
2026-01-08 15:48:39,070: t15.2023.08.20 val PER: 0.1787
2026-01-08 15:48:39,078: t15.2023.08.25 val PER: 0.1205
2026-01-08 15:48:39,080: t15.2023.08.27 val PER: 0.2444
2026-01-08 15:48:39,083: t15.2023.09.01 val PER: 0.1299
2026-01-08 15:48:39,086: t15.2023.09.03 val PER: 0.2268
2026-01-08 15:48:39,090: t15.2023.09.24 val PER: 0.1820
2026-01-08 15:48:39,093: t15.2023.09.29 val PER: 0.1780
2026-01-08 15:48:39,096: t15.2023.10.01 val PER: 0.2338
2026-01-08 15:48:39,098: t15.2023.10.06 val PER: 0.1302
2026-01-08 15:48:39,100: t15.2023.10.08 val PER: 0.3072
2026-01-08 15:48:39,101: t15.2023.10.13 val PER: 0.2723
2026-01-08 15:48:39,103: t15.2023.10.15 val PER: 0.2129
2026-01-08 15:48:39,106: t15.2023.10.20 val PER: 0.2215
2026-01-08 15:48:39,111: t15.2023.10.22 val PER: 0.1559
2026-01-08 15:48:39,113: t15.2023.11.03 val PER: 0.2286
2026-01-08 15:48:39,114: t15.2023.11.04 val PER: 0.0683
2026-01-08 15:48:39,116: t15.2023.11.17 val PER: 0.0824
2026-01-08 15:48:39,117: t15.2023.11.19 val PER: 0.0699
2026-01-08 15:48:39,119: t15.2023.11.26 val PER: 0.2246
2026-01-08 15:48:39,120: t15.2023.12.03 val PER: 0.1870
2026-01-08 15:48:39,122: t15.2023.12.08 val PER: 0.1864
2026-01-08 15:48:39,123: t15.2023.12.10 val PER: 0.1459
2026-01-08 15:48:39,125: t15.2023.12.17 val PER: 0.2235
2026-01-08 15:48:39,129: t15.2023.12.29 val PER: 0.2189
2026-01-08 15:48:39,133: t15.2024.02.25 val PER: 0.1756
2026-01-08 15:48:39,134: t15.2024.03.08 val PER: 0.2902
2026-01-08 15:48:39,136: t15.2024.03.15 val PER: 0.2658
2026-01-08 15:48:39,138: t15.2024.03.17 val PER: 0.2155
2026-01-08 15:48:39,139: t15.2024.05.10 val PER: 0.2437
2026-01-08 15:48:39,141: t15.2024.06.14 val PER: 0.2334
2026-01-08 15:48:39,142: t15.2024.07.19 val PER: 0.3138
2026-01-08 15:48:39,144: t15.2024.07.21 val PER: 0.1586
2026-01-08 15:48:39,145: t15.2024.07.28 val PER: 0.2110
2026-01-08 15:48:39,147: t15.2025.01.10 val PER: 0.3829
2026-01-08 15:48:39,149: t15.2025.01.12 val PER: 0.2340
2026-01-08 15:48:39,150: t15.2025.03.14 val PER: 0.3728
2026-01-08 15:48:39,152: t15.2025.03.16 val PER: 0.2605
2026-01-08 15:48:39,153: t15.2025.03.30 val PER: 0.3517
2026-01-08 15:48:39,155: t15.2025.04.13 val PER: 0.2853
2026-01-08 15:48:39,158: New best val WER(5gram) 22.36% --> 20.53%
2026-01-08 15:48:39,356: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_5500
2026-01-08 15:48:47,993: Train batch 5600: loss: 19.70 grad norm: 67.05 time: 0.062
2026-01-08 15:49:04,101: Train batch 5800: loss: 13.57 grad norm: 57.59 time: 0.083
2026-01-08 15:49:19,935: Train batch 6000: loss: 14.29 grad norm: 56.86 time: 0.049
2026-01-08 15:49:19,937: Running test after training batch: 6000
2026-01-08 15:49:20,099: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:49:25,644: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 15:49:25,718: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 15:49:37,450: Val batch 6000: PER (avg): 0.2122 CTC Loss (avg): 20.8454 WER(5gram): 21.90% (n=256) time: 17.510
2026-01-08 15:49:37,454: WER lens: avg_true_words=5.99 avg_pred_words=6.26 max_pred_words=12
2026-01-08 15:49:37,457: t15.2023.08.13 val PER: 0.1736
2026-01-08 15:49:37,458: t15.2023.08.18 val PER: 0.1626
2026-01-08 15:49:37,461: t15.2023.08.20 val PER: 0.1644
2026-01-08 15:49:37,463: t15.2023.08.25 val PER: 0.1295
2026-01-08 15:49:37,465: t15.2023.08.27 val PER: 0.2315
2026-01-08 15:49:37,467: t15.2023.09.01 val PER: 0.1274
2026-01-08 15:49:37,469: t15.2023.09.03 val PER: 0.2221
2026-01-08 15:49:37,470: t15.2023.09.24 val PER: 0.1699
2026-01-08 15:49:37,472: t15.2023.09.29 val PER: 0.1691
2026-01-08 15:49:37,474: t15.2023.10.01 val PER: 0.2285
2026-01-08 15:49:37,475: t15.2023.10.06 val PER: 0.1378
2026-01-08 15:49:37,477: t15.2023.10.08 val PER: 0.2936
2026-01-08 15:49:37,479: t15.2023.10.13 val PER: 0.2676
2026-01-08 15:49:37,481: t15.2023.10.15 val PER: 0.2129
2026-01-08 15:49:37,482: t15.2023.10.20 val PER: 0.2215
2026-01-08 15:49:37,484: t15.2023.10.22 val PER: 0.1748
2026-01-08 15:49:37,486: t15.2023.11.03 val PER: 0.2313
2026-01-08 15:49:37,487: t15.2023.11.04 val PER: 0.0751
2026-01-08 15:49:37,489: t15.2023.11.17 val PER: 0.0778
2026-01-08 15:49:37,491: t15.2023.11.19 val PER: 0.0798
2026-01-08 15:49:37,492: t15.2023.11.26 val PER: 0.2275
2026-01-08 15:49:37,494: t15.2023.12.03 val PER: 0.1723
2026-01-08 15:49:37,496: t15.2023.12.08 val PER: 0.1724
2026-01-08 15:49:37,497: t15.2023.12.10 val PER: 0.1445
2026-01-08 15:49:37,499: t15.2023.12.17 val PER: 0.1985
2026-01-08 15:49:37,501: t15.2023.12.29 val PER: 0.2189
2026-01-08 15:49:37,502: t15.2024.02.25 val PER: 0.1713
2026-01-08 15:49:37,504: t15.2024.03.08 val PER: 0.2859
2026-01-08 15:49:37,506: t15.2024.03.15 val PER: 0.2689
2026-01-08 15:49:37,507: t15.2024.03.17 val PER: 0.2183
2026-01-08 15:49:37,509: t15.2024.05.10 val PER: 0.2214
2026-01-08 15:49:37,511: t15.2024.06.14 val PER: 0.2192
2026-01-08 15:49:37,512: t15.2024.07.19 val PER: 0.3072
2026-01-08 15:49:37,514: t15.2024.07.21 val PER: 0.1648
2026-01-08 15:49:37,516: t15.2024.07.28 val PER: 0.2015
2026-01-08 15:49:37,517: t15.2025.01.10 val PER: 0.3691
2026-01-08 15:49:37,519: t15.2025.01.12 val PER: 0.2179
2026-01-08 15:49:37,520: t15.2025.03.14 val PER: 0.3787
2026-01-08 15:49:37,522: t15.2025.03.16 val PER: 0.2565
2026-01-08 15:49:37,524: t15.2025.03.30 val PER: 0.3678
2026-01-08 15:49:37,525: t15.2025.04.13 val PER: 0.2668
2026-01-08 15:49:37,661: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_6000
2026-01-08 15:49:54,725: Train batch 6200: loss: 16.29 grad norm: 61.48 time: 0.073
2026-01-08 15:50:11,728: Train batch 6400: loss: 18.37 grad norm: 64.38 time: 0.065
2026-01-08 15:50:20,139: Running test after training batch: 6500
2026-01-08 15:50:20,244: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:50:25,302: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 15:50:25,372: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 15:50:36,765: Val batch 6500: PER (avg): 0.2027 CTC Loss (avg): 20.1001 WER(5gram): 20.47% (n=256) time: 16.625
2026-01-08 15:50:36,767: WER lens: avg_true_words=5.99 avg_pred_words=6.23 max_pred_words=12
2026-01-08 15:50:36,769: t15.2023.08.13 val PER: 0.1694
2026-01-08 15:50:36,771: t15.2023.08.18 val PER: 0.1467
2026-01-08 15:50:36,773: t15.2023.08.20 val PER: 0.1612
2026-01-08 15:50:36,774: t15.2023.08.25 val PER: 0.1054
2026-01-08 15:50:36,775: t15.2023.08.27 val PER: 0.2412
2026-01-08 15:50:36,777: t15.2023.09.01 val PER: 0.1201
2026-01-08 15:50:36,778: t15.2023.09.03 val PER: 0.2055
2026-01-08 15:50:36,780: t15.2023.09.24 val PER: 0.1626
2026-01-08 15:50:36,782: t15.2023.09.29 val PER: 0.1640
2026-01-08 15:50:36,783: t15.2023.10.01 val PER: 0.2160
2026-01-08 15:50:36,784: t15.2023.10.06 val PER: 0.1281
2026-01-08 15:50:36,786: t15.2023.10.08 val PER: 0.2909
2026-01-08 15:50:36,788: t15.2023.10.13 val PER: 0.2746
2026-01-08 15:50:36,790: t15.2023.10.15 val PER: 0.2109
2026-01-08 15:50:36,791: t15.2023.10.20 val PER: 0.2114
2026-01-08 15:50:36,793: t15.2023.10.22 val PER: 0.1481
2026-01-08 15:50:36,794: t15.2023.11.03 val PER: 0.2144
2026-01-08 15:50:36,796: t15.2023.11.04 val PER: 0.0375
2026-01-08 15:50:36,797: t15.2023.11.17 val PER: 0.0684
2026-01-08 15:50:36,800: t15.2023.11.19 val PER: 0.0739
2026-01-08 15:50:36,801: t15.2023.11.26 val PER: 0.2007
2026-01-08 15:50:36,803: t15.2023.12.03 val PER: 0.1712
2026-01-08 15:50:36,804: t15.2023.12.08 val PER: 0.1698
2026-01-08 15:50:36,806: t15.2023.12.10 val PER: 0.1367
2026-01-08 15:50:36,808: t15.2023.12.17 val PER: 0.1965
2026-01-08 15:50:36,810: t15.2023.12.29 val PER: 0.2086
2026-01-08 15:50:36,811: t15.2024.02.25 val PER: 0.1545
2026-01-08 15:50:36,812: t15.2024.03.08 val PER: 0.2802
2026-01-08 15:50:36,814: t15.2024.03.15 val PER: 0.2589
2026-01-08 15:50:36,815: t15.2024.03.17 val PER: 0.2064
2026-01-08 15:50:36,818: t15.2024.05.10 val PER: 0.2036
2026-01-08 15:50:36,820: t15.2024.06.14 val PER: 0.2114
2026-01-08 15:50:36,821: t15.2024.07.19 val PER: 0.2900
2026-01-08 15:50:36,822: t15.2024.07.21 val PER: 0.1559
2026-01-08 15:50:36,824: t15.2024.07.28 val PER: 0.1912
2026-01-08 15:50:36,825: t15.2025.01.10 val PER: 0.3609
2026-01-08 15:50:36,826: t15.2025.01.12 val PER: 0.2125
2026-01-08 15:50:36,828: t15.2025.03.14 val PER: 0.3920
2026-01-08 15:50:36,829: t15.2025.03.16 val PER: 0.2238
2026-01-08 15:50:36,830: t15.2025.03.30 val PER: 0.3517
2026-01-08 15:50:36,832: t15.2025.04.13 val PER: 0.2739
2026-01-08 15:50:36,833: New best val WER(5gram) 20.53% --> 20.47%
2026-01-08 15:50:37,027: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_6500
2026-01-08 15:50:45,318: Train batch 6600: loss: 12.12 grad norm: 51.85 time: 0.045
2026-01-08 15:51:02,459: Train batch 6800: loss: 15.64 grad norm: 57.21 time: 0.049
2026-01-08 15:51:18,928: Train batch 7000: loss: 16.71 grad norm: 61.78 time: 0.062
2026-01-08 15:51:18,930: Running test after training batch: 7000
2026-01-08 15:51:19,061: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:51:24,037: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 15:51:24,079: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost of
2026-01-08 15:51:35,319: Val batch 7000: PER (avg): 0.1951 CTC Loss (avg): 19.2344 WER(5gram): 19.36% (n=256) time: 16.386
2026-01-08 15:51:35,321: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-08 15:51:35,324: t15.2023.08.13 val PER: 0.1559
2026-01-08 15:51:35,326: t15.2023.08.18 val PER: 0.1408
2026-01-08 15:51:35,329: t15.2023.08.20 val PER: 0.1597
2026-01-08 15:51:35,331: t15.2023.08.25 val PER: 0.1069
2026-01-08 15:51:35,332: t15.2023.08.27 val PER: 0.2106
2026-01-08 15:51:35,334: t15.2023.09.01 val PER: 0.1144
2026-01-08 15:51:35,335: t15.2023.09.03 val PER: 0.1841
2026-01-08 15:51:35,337: t15.2023.09.24 val PER: 0.1578
2026-01-08 15:51:35,338: t15.2023.09.29 val PER: 0.1634
2026-01-08 15:51:35,340: t15.2023.10.01 val PER: 0.2153
2026-01-08 15:51:35,342: t15.2023.10.06 val PER: 0.1163
2026-01-08 15:51:35,343: t15.2023.10.08 val PER: 0.2842
2026-01-08 15:51:35,345: t15.2023.10.13 val PER: 0.2645
2026-01-08 15:51:35,347: t15.2023.10.15 val PER: 0.1958
2026-01-08 15:51:35,348: t15.2023.10.20 val PER: 0.2047
2026-01-08 15:51:35,350: t15.2023.10.22 val PER: 0.1448
2026-01-08 15:51:35,352: t15.2023.11.03 val PER: 0.2035
2026-01-08 15:51:35,353: t15.2023.11.04 val PER: 0.0444
2026-01-08 15:51:35,355: t15.2023.11.17 val PER: 0.0653
2026-01-08 15:51:35,356: t15.2023.11.19 val PER: 0.0579
2026-01-08 15:51:35,359: t15.2023.11.26 val PER: 0.1913
2026-01-08 15:51:35,361: t15.2023.12.03 val PER: 0.1586
2026-01-08 15:51:35,362: t15.2023.12.08 val PER: 0.1531
2026-01-08 15:51:35,364: t15.2023.12.10 val PER: 0.1419
2026-01-08 15:51:35,365: t15.2023.12.17 val PER: 0.1767
2026-01-08 15:51:35,367: t15.2023.12.29 val PER: 0.1997
2026-01-08 15:51:35,368: t15.2024.02.25 val PER: 0.1559
2026-01-08 15:51:35,370: t15.2024.03.08 val PER: 0.2817
2026-01-08 15:51:35,371: t15.2024.03.15 val PER: 0.2439
2026-01-08 15:51:35,373: t15.2024.03.17 val PER: 0.1967
2026-01-08 15:51:35,374: t15.2024.05.10 val PER: 0.1961
2026-01-08 15:51:35,376: t15.2024.06.14 val PER: 0.2129
2026-01-08 15:51:35,378: t15.2024.07.19 val PER: 0.2966
2026-01-08 15:51:35,379: t15.2024.07.21 val PER: 0.1379
2026-01-08 15:51:35,381: t15.2024.07.28 val PER: 0.1721
2026-01-08 15:51:35,382: t15.2025.01.10 val PER: 0.3774
2026-01-08 15:51:35,384: t15.2025.01.12 val PER: 0.2009
2026-01-08 15:51:35,385: t15.2025.03.14 val PER: 0.3802
2026-01-08 15:51:35,387: t15.2025.03.16 val PER: 0.2356
2026-01-08 15:51:35,388: t15.2025.03.30 val PER: 0.3609
2026-01-08 15:51:35,390: t15.2025.04.13 val PER: 0.2639
2026-01-08 15:51:35,391: New best val WER(5gram) 20.47% --> 19.36%
2026-01-08 15:51:35,581: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_7000
2026-01-08 15:51:52,527: Train batch 7200: loss: 14.24 grad norm: 61.33 time: 0.087
2026-01-08 15:52:09,466: Train batch 7400: loss: 13.23 grad norm: 52.55 time: 0.075
2026-01-08 15:52:17,527: Running test after training batch: 7500
2026-01-08 15:52:17,689: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:52:22,676: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 15:52:22,735: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 15:52:33,878: Val batch 7500: PER (avg): 0.1880 CTC Loss (avg): 18.7028 WER(5gram): 19.10% (n=256) time: 16.348
2026-01-08 15:52:33,880: WER lens: avg_true_words=5.99 avg_pred_words=6.23 max_pred_words=12
2026-01-08 15:52:33,883: t15.2023.08.13 val PER: 0.1538
2026-01-08 15:52:33,886: t15.2023.08.18 val PER: 0.1308
2026-01-08 15:52:33,888: t15.2023.08.20 val PER: 0.1469
2026-01-08 15:52:33,889: t15.2023.08.25 val PER: 0.0994
2026-01-08 15:52:33,891: t15.2023.08.27 val PER: 0.2090
2026-01-08 15:52:33,893: t15.2023.09.01 val PER: 0.1144
2026-01-08 15:52:33,894: t15.2023.09.03 val PER: 0.1853
2026-01-08 15:52:33,896: t15.2023.09.24 val PER: 0.1517
2026-01-08 15:52:33,897: t15.2023.09.29 val PER: 0.1506
2026-01-08 15:52:33,899: t15.2023.10.01 val PER: 0.1962
2026-01-08 15:52:33,900: t15.2023.10.06 val PER: 0.1109
2026-01-08 15:52:33,902: t15.2023.10.08 val PER: 0.2747
2026-01-08 15:52:33,903: t15.2023.10.13 val PER: 0.2521
2026-01-08 15:52:33,905: t15.2023.10.15 val PER: 0.1905
2026-01-08 15:52:33,907: t15.2023.10.20 val PER: 0.1879
2026-01-08 15:52:33,908: t15.2023.10.22 val PER: 0.1370
2026-01-08 15:52:33,909: t15.2023.11.03 val PER: 0.2022
2026-01-08 15:52:33,911: t15.2023.11.04 val PER: 0.0478
2026-01-08 15:52:33,912: t15.2023.11.17 val PER: 0.0513
2026-01-08 15:52:33,914: t15.2023.11.19 val PER: 0.0539
2026-01-08 15:52:33,915: t15.2023.11.26 val PER: 0.1906
2026-01-08 15:52:33,917: t15.2023.12.03 val PER: 0.1681
2026-01-08 15:52:33,918: t15.2023.12.08 val PER: 0.1558
2026-01-08 15:52:33,920: t15.2023.12.10 val PER: 0.1301
2026-01-08 15:52:33,921: t15.2023.12.17 val PER: 0.1788
2026-01-08 15:52:33,923: t15.2023.12.29 val PER: 0.1826
2026-01-08 15:52:33,924: t15.2024.02.25 val PER: 0.1531
2026-01-08 15:52:33,926: t15.2024.03.08 val PER: 0.2688
2026-01-08 15:52:33,927: t15.2024.03.15 val PER: 0.2408
2026-01-08 15:52:33,929: t15.2024.03.17 val PER: 0.1806
2026-01-08 15:52:33,931: t15.2024.05.10 val PER: 0.2095
2026-01-08 15:52:33,933: t15.2024.06.14 val PER: 0.2019
2026-01-08 15:52:33,934: t15.2024.07.19 val PER: 0.2874
2026-01-08 15:52:33,936: t15.2024.07.21 val PER: 0.1366
2026-01-08 15:52:33,937: t15.2024.07.28 val PER: 0.1787
2026-01-08 15:52:33,939: t15.2025.01.10 val PER: 0.3333
2026-01-08 15:52:33,940: t15.2025.01.12 val PER: 0.1863
2026-01-08 15:52:33,942: t15.2025.03.14 val PER: 0.3536
2026-01-08 15:52:33,943: t15.2025.03.16 val PER: 0.2408
2026-01-08 15:52:33,945: t15.2025.03.30 val PER: 0.3540
2026-01-08 15:52:33,946: t15.2025.04.13 val PER: 0.2468
2026-01-08 15:52:33,948: New best val WER(5gram) 19.36% --> 19.10%
2026-01-08 15:52:34,137: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_7500
2026-01-08 15:52:42,642: Train batch 7600: loss: 16.07 grad norm: 58.87 time: 0.071
2026-01-08 15:52:58,656: Train batch 7800: loss: 14.80 grad norm: 63.81 time: 0.056
2026-01-08 15:53:15,241: Train batch 8000: loss: 11.25 grad norm: 52.28 time: 0.072
2026-01-08 15:53:15,243: Running test after training batch: 8000
2026-01-08 15:53:15,342: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:53:20,256: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 15:53:20,314: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost of
2026-01-08 15:53:31,521: Val batch 8000: PER (avg): 0.1840 CTC Loss (avg): 18.2067 WER(5gram): 19.36% (n=256) time: 16.275
2026-01-08 15:53:31,524: WER lens: avg_true_words=5.99 avg_pred_words=6.24 max_pred_words=13
2026-01-08 15:53:31,526: t15.2023.08.13 val PER: 0.1466
2026-01-08 15:53:31,528: t15.2023.08.18 val PER: 0.1266
2026-01-08 15:53:31,530: t15.2023.08.20 val PER: 0.1557
2026-01-08 15:53:31,531: t15.2023.08.25 val PER: 0.1130
2026-01-08 15:53:31,533: t15.2023.08.27 val PER: 0.2058
2026-01-08 15:53:31,535: t15.2023.09.01 val PER: 0.0966
2026-01-08 15:53:31,536: t15.2023.09.03 val PER: 0.1865
2026-01-08 15:53:31,538: t15.2023.09.24 val PER: 0.1432
2026-01-08 15:53:31,541: t15.2023.09.29 val PER: 0.1474
2026-01-08 15:53:31,543: t15.2023.10.01 val PER: 0.2048
2026-01-08 15:53:31,544: t15.2023.10.06 val PER: 0.1152
2026-01-08 15:53:31,546: t15.2023.10.08 val PER: 0.2693
2026-01-08 15:53:31,548: t15.2023.10.13 val PER: 0.2366
2026-01-08 15:53:31,550: t15.2023.10.15 val PER: 0.1991
2026-01-08 15:53:31,551: t15.2023.10.20 val PER: 0.2013
2026-01-08 15:53:31,553: t15.2023.10.22 val PER: 0.1414
2026-01-08 15:53:31,555: t15.2023.11.03 val PER: 0.2056
2026-01-08 15:53:31,556: t15.2023.11.04 val PER: 0.0341
2026-01-08 15:53:31,558: t15.2023.11.17 val PER: 0.0591
2026-01-08 15:53:31,560: t15.2023.11.19 val PER: 0.0679
2026-01-08 15:53:31,561: t15.2023.11.26 val PER: 0.1732
2026-01-08 15:53:31,563: t15.2023.12.03 val PER: 0.1555
2026-01-08 15:53:31,564: t15.2023.12.08 val PER: 0.1431
2026-01-08 15:53:31,566: t15.2023.12.10 val PER: 0.1170
2026-01-08 15:53:31,567: t15.2023.12.17 val PER: 0.1788
2026-01-08 15:53:31,569: t15.2023.12.29 val PER: 0.1846
2026-01-08 15:53:31,570: t15.2024.02.25 val PER: 0.1517
2026-01-08 15:53:31,572: t15.2024.03.08 val PER: 0.2688
2026-01-08 15:53:31,573: t15.2024.03.15 val PER: 0.2370
2026-01-08 15:53:31,575: t15.2024.03.17 val PER: 0.1688
2026-01-08 15:53:31,577: t15.2024.05.10 val PER: 0.1991
2026-01-08 15:53:31,578: t15.2024.06.14 val PER: 0.2050
2026-01-08 15:53:31,580: t15.2024.07.19 val PER: 0.2920
2026-01-08 15:53:31,581: t15.2024.07.21 val PER: 0.1200
2026-01-08 15:53:31,583: t15.2024.07.28 val PER: 0.1537
2026-01-08 15:53:31,584: t15.2025.01.10 val PER: 0.3333
2026-01-08 15:53:31,587: t15.2025.01.12 val PER: 0.1824
2026-01-08 15:53:31,589: t15.2025.03.14 val PER: 0.3521
2026-01-08 15:53:31,590: t15.2025.03.16 val PER: 0.2199
2026-01-08 15:53:31,591: t15.2025.03.30 val PER: 0.3471
2026-01-08 15:53:31,593: t15.2025.04.13 val PER: 0.2553
2026-01-08 15:53:31,727: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_8000
2026-01-08 15:53:47,884: Train batch 8200: loss: 9.45 grad norm: 47.66 time: 0.055
2026-01-08 15:54:03,865: Train batch 8400: loss: 10.05 grad norm: 48.26 time: 0.064
2026-01-08 15:54:12,085: Running test after training batch: 8500
2026-01-08 15:54:12,180: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:54:17,083: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 15:54:17,134: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 15:54:28,288: Val batch 8500: PER (avg): 0.1778 CTC Loss (avg): 17.6630 WER(5gram): 15.97% (n=256) time: 16.201
2026-01-08 15:54:28,290: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 15:54:28,292: t15.2023.08.13 val PER: 0.1424
2026-01-08 15:54:28,294: t15.2023.08.18 val PER: 0.1215
2026-01-08 15:54:28,295: t15.2023.08.20 val PER: 0.1326
2026-01-08 15:54:28,297: t15.2023.08.25 val PER: 0.1054
2026-01-08 15:54:28,299: t15.2023.08.27 val PER: 0.2090
2026-01-08 15:54:28,300: t15.2023.09.01 val PER: 0.0982
2026-01-08 15:54:28,302: t15.2023.09.03 val PER: 0.1888
2026-01-08 15:54:28,304: t15.2023.09.24 val PER: 0.1505
2026-01-08 15:54:28,306: t15.2023.09.29 val PER: 0.1474
2026-01-08 15:54:28,308: t15.2023.10.01 val PER: 0.1889
2026-01-08 15:54:28,310: t15.2023.10.06 val PER: 0.1001
2026-01-08 15:54:28,311: t15.2023.10.08 val PER: 0.2639
2026-01-08 15:54:28,313: t15.2023.10.13 val PER: 0.2413
2026-01-08 15:54:28,314: t15.2023.10.15 val PER: 0.1800
2026-01-08 15:54:28,315: t15.2023.10.20 val PER: 0.1879
2026-01-08 15:54:28,317: t15.2023.10.22 val PER: 0.1481
2026-01-08 15:54:28,319: t15.2023.11.03 val PER: 0.1995
2026-01-08 15:54:28,320: t15.2023.11.04 val PER: 0.0444
2026-01-08 15:54:28,321: t15.2023.11.17 val PER: 0.0544
2026-01-08 15:54:28,323: t15.2023.11.19 val PER: 0.0539
2026-01-08 15:54:28,324: t15.2023.11.26 val PER: 0.1790
2026-01-08 15:54:28,326: t15.2023.12.03 val PER: 0.1418
2026-01-08 15:54:28,327: t15.2023.12.08 val PER: 0.1325
2026-01-08 15:54:28,329: t15.2023.12.10 val PER: 0.1143
2026-01-08 15:54:28,330: t15.2023.12.17 val PER: 0.1861
2026-01-08 15:54:28,332: t15.2023.12.29 val PER: 0.1647
2026-01-08 15:54:28,333: t15.2024.02.25 val PER: 0.1390
2026-01-08 15:54:28,335: t15.2024.03.08 val PER: 0.2617
2026-01-08 15:54:28,336: t15.2024.03.15 val PER: 0.2370
2026-01-08 15:54:28,338: t15.2024.03.17 val PER: 0.1681
2026-01-08 15:54:28,339: t15.2024.05.10 val PER: 0.1887
2026-01-08 15:54:28,341: t15.2024.06.14 val PER: 0.1782
2026-01-08 15:54:28,343: t15.2024.07.19 val PER: 0.2670
2026-01-08 15:54:28,345: t15.2024.07.21 val PER: 0.1186
2026-01-08 15:54:28,346: t15.2024.07.28 val PER: 0.1603
2026-01-08 15:54:28,347: t15.2025.01.10 val PER: 0.3251
2026-01-08 15:54:28,349: t15.2025.01.12 val PER: 0.1848
2026-01-08 15:54:28,351: t15.2025.03.14 val PER: 0.3521
2026-01-08 15:54:28,352: t15.2025.03.16 val PER: 0.2081
2026-01-08 15:54:28,354: t15.2025.03.30 val PER: 0.3356
2026-01-08 15:54:28,356: t15.2025.04.13 val PER: 0.2439
2026-01-08 15:54:28,358: New best val WER(5gram) 19.10% --> 15.97%
2026-01-08 15:54:28,544: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_8500
2026-01-08 15:54:36,556: Train batch 8600: loss: 15.70 grad norm: 64.81 time: 0.056
2026-01-08 15:54:52,710: Train batch 8800: loss: 15.57 grad norm: 59.24 time: 0.062
2026-01-08 15:55:09,989: Train batch 9000: loss: 15.87 grad norm: 64.68 time: 0.074
2026-01-08 15:55:10,003: Running test after training batch: 9000
2026-01-08 15:55:10,117: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:55:14,989: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 15:55:15,047: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost get
2026-01-08 15:55:26,148: Val batch 9000: PER (avg): 0.1731 CTC Loss (avg): 17.4061 WER(5gram): 20.53% (n=256) time: 16.141
2026-01-08 15:55:26,150: WER lens: avg_true_words=5.99 avg_pred_words=6.28 max_pred_words=12
2026-01-08 15:55:26,152: t15.2023.08.13 val PER: 0.1351
2026-01-08 15:55:26,154: t15.2023.08.18 val PER: 0.1282
2026-01-08 15:55:26,156: t15.2023.08.20 val PER: 0.1303
2026-01-08 15:55:26,158: t15.2023.08.25 val PER: 0.0994
2026-01-08 15:55:26,159: t15.2023.08.27 val PER: 0.2026
2026-01-08 15:55:26,161: t15.2023.09.01 val PER: 0.0974
2026-01-08 15:55:26,162: t15.2023.09.03 val PER: 0.1746
2026-01-08 15:55:26,164: t15.2023.09.24 val PER: 0.1408
2026-01-08 15:55:26,166: t15.2023.09.29 val PER: 0.1487
2026-01-08 15:55:26,168: t15.2023.10.01 val PER: 0.1929
2026-01-08 15:55:26,169: t15.2023.10.06 val PER: 0.0969
2026-01-08 15:55:26,171: t15.2023.10.08 val PER: 0.2693
2026-01-08 15:55:26,173: t15.2023.10.13 val PER: 0.2366
2026-01-08 15:55:26,174: t15.2023.10.15 val PER: 0.1721
2026-01-08 15:55:26,176: t15.2023.10.20 val PER: 0.1879
2026-01-08 15:55:26,178: t15.2023.10.22 val PER: 0.1370
2026-01-08 15:55:26,179: t15.2023.11.03 val PER: 0.2001
2026-01-08 15:55:26,181: t15.2023.11.04 val PER: 0.0410
2026-01-08 15:55:26,183: t15.2023.11.17 val PER: 0.0544
2026-01-08 15:55:26,184: t15.2023.11.19 val PER: 0.0459
2026-01-08 15:55:26,185: t15.2023.11.26 val PER: 0.1623
2026-01-08 15:55:26,187: t15.2023.12.03 val PER: 0.1408
2026-01-08 15:55:26,189: t15.2023.12.08 val PER: 0.1232
2026-01-08 15:55:26,191: t15.2023.12.10 val PER: 0.1025
2026-01-08 15:55:26,193: t15.2023.12.17 val PER: 0.1694
2026-01-08 15:55:26,195: t15.2023.12.29 val PER: 0.1640
2026-01-08 15:55:26,196: t15.2024.02.25 val PER: 0.1404
2026-01-08 15:55:26,197: t15.2024.03.08 val PER: 0.2546
2026-01-08 15:55:26,199: t15.2024.03.15 val PER: 0.2270
2026-01-08 15:55:26,201: t15.2024.03.17 val PER: 0.1674
2026-01-08 15:55:26,205: t15.2024.05.10 val PER: 0.1872
2026-01-08 15:55:26,206: t15.2024.06.14 val PER: 0.1814
2026-01-08 15:55:26,208: t15.2024.07.19 val PER: 0.2749
2026-01-08 15:55:26,210: t15.2024.07.21 val PER: 0.1110
2026-01-08 15:55:26,211: t15.2024.07.28 val PER: 0.1515
2026-01-08 15:55:26,213: t15.2025.01.10 val PER: 0.3030
2026-01-08 15:55:26,215: t15.2025.01.12 val PER: 0.1655
2026-01-08 15:55:26,216: t15.2025.03.14 val PER: 0.3536
2026-01-08 15:55:26,218: t15.2025.03.16 val PER: 0.2264
2026-01-08 15:55:26,220: t15.2025.03.30 val PER: 0.3253
2026-01-08 15:55:26,221: t15.2025.04.13 val PER: 0.2240
2026-01-08 15:55:26,358: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_9000
2026-01-08 15:55:43,543: Train batch 9200: loss: 10.95 grad norm: 49.46 time: 0.059
2026-01-08 15:56:00,706: Train batch 9400: loss: 7.58 grad norm: 46.80 time: 0.070
2026-01-08 15:56:09,239: Running test after training batch: 9500
2026-01-08 15:56:09,370: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:56:14,233: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 15:56:14,281: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 15:56:25,375: Val batch 9500: PER (avg): 0.1755 CTC Loss (avg): 17.3088 WER(5gram): 16.82% (n=256) time: 16.134
2026-01-08 15:56:25,377: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=13
2026-01-08 15:56:25,379: t15.2023.08.13 val PER: 0.1341
2026-01-08 15:56:25,381: t15.2023.08.18 val PER: 0.1190
2026-01-08 15:56:25,383: t15.2023.08.20 val PER: 0.1326
2026-01-08 15:56:25,385: t15.2023.08.25 val PER: 0.1069
2026-01-08 15:56:25,387: t15.2023.08.27 val PER: 0.2026
2026-01-08 15:56:25,389: t15.2023.09.01 val PER: 0.0942
2026-01-08 15:56:25,391: t15.2023.09.03 val PER: 0.1770
2026-01-08 15:56:25,393: t15.2023.09.24 val PER: 0.1432
2026-01-08 15:56:25,395: t15.2023.09.29 val PER: 0.1449
2026-01-08 15:56:25,396: t15.2023.10.01 val PER: 0.1935
2026-01-08 15:56:25,398: t15.2023.10.06 val PER: 0.1141
2026-01-08 15:56:25,400: t15.2023.10.08 val PER: 0.2585
2026-01-08 15:56:25,402: t15.2023.10.13 val PER: 0.2382
2026-01-08 15:56:25,404: t15.2023.10.15 val PER: 0.1833
2026-01-08 15:56:25,406: t15.2023.10.20 val PER: 0.1779
2026-01-08 15:56:25,408: t15.2023.10.22 val PER: 0.1325
2026-01-08 15:56:25,409: t15.2023.11.03 val PER: 0.1913
2026-01-08 15:56:25,411: t15.2023.11.04 val PER: 0.0410
2026-01-08 15:56:25,413: t15.2023.11.17 val PER: 0.0544
2026-01-08 15:56:25,415: t15.2023.11.19 val PER: 0.0539
2026-01-08 15:56:25,417: t15.2023.11.26 val PER: 0.1616
2026-01-08 15:56:25,419: t15.2023.12.03 val PER: 0.1502
2026-01-08 15:56:25,421: t15.2023.12.08 val PER: 0.1365
2026-01-08 15:56:25,422: t15.2023.12.10 val PER: 0.1091
2026-01-08 15:56:25,424: t15.2023.12.17 val PER: 0.1694
2026-01-08 15:56:25,426: t15.2023.12.29 val PER: 0.1565
2026-01-08 15:56:25,427: t15.2024.02.25 val PER: 0.1503
2026-01-08 15:56:25,429: t15.2024.03.08 val PER: 0.2703
2026-01-08 15:56:25,430: t15.2024.03.15 val PER: 0.2258
2026-01-08 15:56:25,432: t15.2024.03.17 val PER: 0.1667
2026-01-08 15:56:25,434: t15.2024.05.10 val PER: 0.1976
2026-01-08 15:56:25,436: t15.2024.06.14 val PER: 0.1861
2026-01-08 15:56:25,437: t15.2024.07.19 val PER: 0.2769
2026-01-08 15:56:25,439: t15.2024.07.21 val PER: 0.1138
2026-01-08 15:56:25,441: t15.2024.07.28 val PER: 0.1574
2026-01-08 15:56:25,442: t15.2025.01.10 val PER: 0.3182
2026-01-08 15:56:25,444: t15.2025.01.12 val PER: 0.1878
2026-01-08 15:56:25,446: t15.2025.03.14 val PER: 0.3624
2026-01-08 15:56:25,447: t15.2025.03.16 val PER: 0.2055
2026-01-08 15:56:25,449: t15.2025.03.30 val PER: 0.3218
2026-01-08 15:56:25,451: t15.2025.04.13 val PER: 0.2368
2026-01-08 15:56:25,589: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_9500
2026-01-08 15:56:33,803: Train batch 9600: loss: 8.45 grad norm: 44.74 time: 0.073
2026-01-08 15:56:50,024: Train batch 9800: loss: 11.94 grad norm: 56.92 time: 0.064
2026-01-08 15:57:07,012: Train batch 10000: loss: 5.48 grad norm: 38.20 time: 0.061
2026-01-08 15:57:07,014: Running test after training batch: 10000
2026-01-08 15:57:07,123: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:57:12,037: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 15:57:12,084: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-08 15:57:23,052: Val batch 10000: PER (avg): 0.1709 CTC Loss (avg): 16.9731 WER(5gram): 17.47% (n=256) time: 16.035
2026-01-08 15:57:23,054: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=13
2026-01-08 15:57:23,056: t15.2023.08.13 val PER: 0.1299
2026-01-08 15:57:23,058: t15.2023.08.18 val PER: 0.1291
2026-01-08 15:57:23,060: t15.2023.08.20 val PER: 0.1295
2026-01-08 15:57:23,062: t15.2023.08.25 val PER: 0.1039
2026-01-08 15:57:23,064: t15.2023.08.27 val PER: 0.2026
2026-01-08 15:57:23,065: t15.2023.09.01 val PER: 0.0869
2026-01-08 15:57:23,068: t15.2023.09.03 val PER: 0.1710
2026-01-08 15:57:23,070: t15.2023.09.24 val PER: 0.1396
2026-01-08 15:57:23,072: t15.2023.09.29 val PER: 0.1442
2026-01-08 15:57:23,074: t15.2023.10.01 val PER: 0.1836
2026-01-08 15:57:23,076: t15.2023.10.06 val PER: 0.1076
2026-01-08 15:57:23,078: t15.2023.10.08 val PER: 0.2490
2026-01-08 15:57:23,079: t15.2023.10.13 val PER: 0.2289
2026-01-08 15:57:23,081: t15.2023.10.15 val PER: 0.1721
2026-01-08 15:57:23,083: t15.2023.10.20 val PER: 0.1846
2026-01-08 15:57:23,085: t15.2023.10.22 val PER: 0.1403
2026-01-08 15:57:23,086: t15.2023.11.03 val PER: 0.1913
2026-01-08 15:57:23,088: t15.2023.11.04 val PER: 0.0410
2026-01-08 15:57:23,090: t15.2023.11.17 val PER: 0.0513
2026-01-08 15:57:23,091: t15.2023.11.19 val PER: 0.0499
2026-01-08 15:57:23,093: t15.2023.11.26 val PER: 0.1551
2026-01-08 15:57:23,095: t15.2023.12.03 val PER: 0.1366
2026-01-08 15:57:23,096: t15.2023.12.08 val PER: 0.1285
2026-01-08 15:57:23,098: t15.2023.12.10 val PER: 0.1130
2026-01-08 15:57:23,099: t15.2023.12.17 val PER: 0.1663
2026-01-08 15:57:23,101: t15.2023.12.29 val PER: 0.1489
2026-01-08 15:57:23,103: t15.2024.02.25 val PER: 0.1489
2026-01-08 15:57:23,104: t15.2024.03.08 val PER: 0.2347
2026-01-08 15:57:23,106: t15.2024.03.15 val PER: 0.2264
2026-01-08 15:57:23,107: t15.2024.03.17 val PER: 0.1590
2026-01-08 15:57:23,109: t15.2024.05.10 val PER: 0.1783
2026-01-08 15:57:23,110: t15.2024.06.14 val PER: 0.1940
2026-01-08 15:57:23,114: t15.2024.07.19 val PER: 0.2709
2026-01-08 15:57:23,116: t15.2024.07.21 val PER: 0.1207
2026-01-08 15:57:23,117: t15.2024.07.28 val PER: 0.1500
2026-01-08 15:57:23,120: t15.2025.01.10 val PER: 0.3030
2026-01-08 15:57:23,121: t15.2025.01.12 val PER: 0.1724
2026-01-08 15:57:23,123: t15.2025.03.14 val PER: 0.3595
2026-01-08 15:57:23,124: t15.2025.03.16 val PER: 0.2186
2026-01-08 15:57:23,126: t15.2025.03.30 val PER: 0.3218
2026-01-08 15:57:23,127: t15.2025.04.13 val PER: 0.2382
2026-01-08 15:57:23,272: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_10000
2026-01-08 15:57:39,643: Train batch 10200: loss: 5.99 grad norm: 36.20 time: 0.053
2026-01-08 15:57:56,465: Train batch 10400: loss: 9.21 grad norm: 50.40 time: 0.075
2026-01-08 15:58:04,456: Running test after training batch: 10500
2026-01-08 15:58:04,578: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:58:09,671: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 15:58:09,713: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 15:58:20,844: Val batch 10500: PER (avg): 0.1661 CTC Loss (avg): 16.7128 WER(5gram): 16.43% (n=256) time: 16.386
2026-01-08 15:58:20,846: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=12
2026-01-08 15:58:20,849: t15.2023.08.13 val PER: 0.1331
2026-01-08 15:58:20,850: t15.2023.08.18 val PER: 0.1207
2026-01-08 15:58:20,852: t15.2023.08.20 val PER: 0.1247
2026-01-08 15:58:20,854: t15.2023.08.25 val PER: 0.1084
2026-01-08 15:58:20,856: t15.2023.08.27 val PER: 0.1961
2026-01-08 15:58:20,858: t15.2023.09.01 val PER: 0.0950
2026-01-08 15:58:20,860: t15.2023.09.03 val PER: 0.1758
2026-01-08 15:58:20,861: t15.2023.09.24 val PER: 0.1468
2026-01-08 15:58:20,863: t15.2023.09.29 val PER: 0.1429
2026-01-08 15:58:20,865: t15.2023.10.01 val PER: 0.1869
2026-01-08 15:58:20,866: t15.2023.10.06 val PER: 0.0936
2026-01-08 15:58:20,868: t15.2023.10.08 val PER: 0.2476
2026-01-08 15:58:20,870: t15.2023.10.13 val PER: 0.2133
2026-01-08 15:58:20,872: t15.2023.10.15 val PER: 0.1747
2026-01-08 15:58:20,875: t15.2023.10.20 val PER: 0.1779
2026-01-08 15:58:20,877: t15.2023.10.22 val PER: 0.1225
2026-01-08 15:58:20,879: t15.2023.11.03 val PER: 0.1893
2026-01-08 15:58:20,880: t15.2023.11.04 val PER: 0.0375
2026-01-08 15:58:20,882: t15.2023.11.17 val PER: 0.0467
2026-01-08 15:58:20,884: t15.2023.11.19 val PER: 0.0519
2026-01-08 15:58:20,886: t15.2023.11.26 val PER: 0.1442
2026-01-08 15:58:20,887: t15.2023.12.03 val PER: 0.1292
2026-01-08 15:58:20,889: t15.2023.12.08 val PER: 0.1165
2026-01-08 15:58:20,891: t15.2023.12.10 val PER: 0.1051
2026-01-08 15:58:20,892: t15.2023.12.17 val PER: 0.1403
2026-01-08 15:58:20,894: t15.2023.12.29 val PER: 0.1510
2026-01-08 15:58:20,896: t15.2024.02.25 val PER: 0.1390
2026-01-08 15:58:20,898: t15.2024.03.08 val PER: 0.2333
2026-01-08 15:58:20,899: t15.2024.03.15 val PER: 0.2176
2026-01-08 15:58:20,901: t15.2024.03.17 val PER: 0.1632
2026-01-08 15:58:20,903: t15.2024.05.10 val PER: 0.1768
2026-01-08 15:58:20,904: t15.2024.06.14 val PER: 0.1814
2026-01-08 15:58:20,906: t15.2024.07.19 val PER: 0.2676
2026-01-08 15:58:20,908: t15.2024.07.21 val PER: 0.1041
2026-01-08 15:58:20,910: t15.2024.07.28 val PER: 0.1412
2026-01-08 15:58:20,912: t15.2025.01.10 val PER: 0.3182
2026-01-08 15:58:20,913: t15.2025.01.12 val PER: 0.1701
2026-01-08 15:58:20,915: t15.2025.03.14 val PER: 0.3491
2026-01-08 15:58:20,916: t15.2025.03.16 val PER: 0.2029
2026-01-08 15:58:20,918: t15.2025.03.30 val PER: 0.3241
2026-01-08 15:58:20,919: t15.2025.04.13 val PER: 0.2211
2026-01-08 15:58:21,062: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_10500
2026-01-08 15:58:29,738: Train batch 10600: loss: 9.49 grad norm: 56.44 time: 0.073
2026-01-08 15:58:46,411: Train batch 10800: loss: 14.90 grad norm: 63.72 time: 0.065
2026-01-08 15:59:02,379: Train batch 11000: loss: 14.65 grad norm: 63.97 time: 0.057
2026-01-08 15:59:02,381: Running test after training batch: 11000
2026-01-08 15:59:02,494: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:59:07,569: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 15:59:07,615: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 15:59:18,456: Val batch 11000: PER (avg): 0.1635 CTC Loss (avg): 16.4746 WER(5gram): 18.51% (n=256) time: 16.073
2026-01-08 15:59:18,459: WER lens: avg_true_words=5.99 avg_pred_words=6.23 max_pred_words=12
2026-01-08 15:59:18,460: t15.2023.08.13 val PER: 0.1185
2026-01-08 15:59:18,462: t15.2023.08.18 val PER: 0.1174
2026-01-08 15:59:18,464: t15.2023.08.20 val PER: 0.1263
2026-01-08 15:59:18,466: t15.2023.08.25 val PER: 0.1009
2026-01-08 15:59:18,467: t15.2023.08.27 val PER: 0.2010
2026-01-08 15:59:18,469: t15.2023.09.01 val PER: 0.0804
2026-01-08 15:59:18,470: t15.2023.09.03 val PER: 0.1734
2026-01-08 15:59:18,472: t15.2023.09.24 val PER: 0.1468
2026-01-08 15:59:18,474: t15.2023.09.29 val PER: 0.1347
2026-01-08 15:59:18,475: t15.2023.10.01 val PER: 0.1909
2026-01-08 15:59:18,476: t15.2023.10.06 val PER: 0.0936
2026-01-08 15:59:18,478: t15.2023.10.08 val PER: 0.2530
2026-01-08 15:59:18,480: t15.2023.10.13 val PER: 0.2219
2026-01-08 15:59:18,481: t15.2023.10.15 val PER: 0.1727
2026-01-08 15:59:18,483: t15.2023.10.20 val PER: 0.1980
2026-01-08 15:59:18,484: t15.2023.10.22 val PER: 0.1236
2026-01-08 15:59:18,486: t15.2023.11.03 val PER: 0.1900
2026-01-08 15:59:18,487: t15.2023.11.04 val PER: 0.0375
2026-01-08 15:59:18,489: t15.2023.11.17 val PER: 0.0529
2026-01-08 15:59:18,490: t15.2023.11.19 val PER: 0.0439
2026-01-08 15:59:18,492: t15.2023.11.26 val PER: 0.1449
2026-01-08 15:59:18,493: t15.2023.12.03 val PER: 0.1313
2026-01-08 15:59:18,495: t15.2023.12.08 val PER: 0.1178
2026-01-08 15:59:18,497: t15.2023.12.10 val PER: 0.1012
2026-01-08 15:59:18,498: t15.2023.12.17 val PER: 0.1538
2026-01-08 15:59:18,499: t15.2023.12.29 val PER: 0.1386
2026-01-08 15:59:18,501: t15.2024.02.25 val PER: 0.1306
2026-01-08 15:59:18,502: t15.2024.03.08 val PER: 0.2319
2026-01-08 15:59:18,504: t15.2024.03.15 val PER: 0.2183
2026-01-08 15:59:18,505: t15.2024.03.17 val PER: 0.1506
2026-01-08 15:59:18,507: t15.2024.05.10 val PER: 0.1783
2026-01-08 15:59:18,508: t15.2024.06.14 val PER: 0.1703
2026-01-08 15:59:18,510: t15.2024.07.19 val PER: 0.2505
2026-01-08 15:59:18,511: t15.2024.07.21 val PER: 0.1055
2026-01-08 15:59:18,513: t15.2024.07.28 val PER: 0.1434
2026-01-08 15:59:18,514: t15.2025.01.10 val PER: 0.3044
2026-01-08 15:59:18,516: t15.2025.01.12 val PER: 0.1671
2026-01-08 15:59:18,517: t15.2025.03.14 val PER: 0.3491
2026-01-08 15:59:18,518: t15.2025.03.16 val PER: 0.1950
2026-01-08 15:59:18,520: t15.2025.03.30 val PER: 0.3092
2026-01-08 15:59:18,522: t15.2025.04.13 val PER: 0.2282
2026-01-08 15:59:18,661: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_11000
2026-01-08 15:59:35,591: Train batch 11200: loss: 11.00 grad norm: 54.40 time: 0.073
2026-01-08 15:59:52,617: Train batch 11400: loss: 9.72 grad norm: 55.42 time: 0.060
2026-01-08 16:00:01,305: Running test after training batch: 11500
2026-01-08 16:00:01,444: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:00:06,297: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:00:06,343: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 16:00:17,061: Val batch 11500: PER (avg): 0.1613 CTC Loss (avg): 16.4931 WER(5gram): 17.28% (n=256) time: 15.753
2026-01-08 16:00:17,063: WER lens: avg_true_words=5.99 avg_pred_words=6.22 max_pred_words=12
2026-01-08 16:00:17,065: t15.2023.08.13 val PER: 0.1185
2026-01-08 16:00:17,067: t15.2023.08.18 val PER: 0.1148
2026-01-08 16:00:17,069: t15.2023.08.20 val PER: 0.1183
2026-01-08 16:00:17,071: t15.2023.08.25 val PER: 0.1024
2026-01-08 16:00:17,072: t15.2023.08.27 val PER: 0.1929
2026-01-08 16:00:17,074: t15.2023.09.01 val PER: 0.0933
2026-01-08 16:00:17,076: t15.2023.09.03 val PER: 0.1698
2026-01-08 16:00:17,078: t15.2023.09.24 val PER: 0.1335
2026-01-08 16:00:17,080: t15.2023.09.29 val PER: 0.1385
2026-01-08 16:00:17,082: t15.2023.10.01 val PER: 0.1823
2026-01-08 16:00:17,084: t15.2023.10.06 val PER: 0.0861
2026-01-08 16:00:17,085: t15.2023.10.08 val PER: 0.2517
2026-01-08 16:00:17,087: t15.2023.10.13 val PER: 0.2056
2026-01-08 16:00:17,088: t15.2023.10.15 val PER: 0.1753
2026-01-08 16:00:17,092: t15.2023.10.20 val PER: 0.1980
2026-01-08 16:00:17,094: t15.2023.10.22 val PER: 0.1169
2026-01-08 16:00:17,096: t15.2023.11.03 val PER: 0.1825
2026-01-08 16:00:17,097: t15.2023.11.04 val PER: 0.0307
2026-01-08 16:00:17,099: t15.2023.11.17 val PER: 0.0404
2026-01-08 16:00:17,101: t15.2023.11.19 val PER: 0.0459
2026-01-08 16:00:17,103: t15.2023.11.26 val PER: 0.1319
2026-01-08 16:00:17,104: t15.2023.12.03 val PER: 0.1092
2026-01-08 16:00:17,107: t15.2023.12.08 val PER: 0.1125
2026-01-08 16:00:17,109: t15.2023.12.10 val PER: 0.1012
2026-01-08 16:00:17,111: t15.2023.12.17 val PER: 0.1528
2026-01-08 16:00:17,112: t15.2023.12.29 val PER: 0.1380
2026-01-08 16:00:17,115: t15.2024.02.25 val PER: 0.1278
2026-01-08 16:00:17,116: t15.2024.03.08 val PER: 0.2319
2026-01-08 16:00:17,119: t15.2024.03.15 val PER: 0.2195
2026-01-08 16:00:17,120: t15.2024.03.17 val PER: 0.1492
2026-01-08 16:00:17,122: t15.2024.05.10 val PER: 0.1917
2026-01-08 16:00:17,123: t15.2024.06.14 val PER: 0.1814
2026-01-08 16:00:17,125: t15.2024.07.19 val PER: 0.2492
2026-01-08 16:00:17,126: t15.2024.07.21 val PER: 0.1069
2026-01-08 16:00:17,128: t15.2024.07.28 val PER: 0.1368
2026-01-08 16:00:17,129: t15.2025.01.10 val PER: 0.3182
2026-01-08 16:00:17,131: t15.2025.01.12 val PER: 0.1640
2026-01-08 16:00:17,132: t15.2025.03.14 val PER: 0.3417
2026-01-08 16:00:17,134: t15.2025.03.16 val PER: 0.2107
2026-01-08 16:00:17,135: t15.2025.03.30 val PER: 0.3230
2026-01-08 16:00:17,137: t15.2025.04.13 val PER: 0.2268
2026-01-08 16:00:17,272: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_11500
2026-01-08 16:00:25,661: Train batch 11600: loss: 10.76 grad norm: 47.56 time: 0.062
2026-01-08 16:00:42,706: Train batch 11800: loss: 6.43 grad norm: 39.51 time: 0.045
2026-01-08 16:00:59,283: Train batch 12000: loss: 13.64 grad norm: 58.25 time: 0.072
2026-01-08 16:00:59,285: Running test after training batch: 12000
2026-01-08 16:00:59,383: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:01:04,235: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:01:04,283: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 16:01:15,059: Val batch 12000: PER (avg): 0.1597 CTC Loss (avg): 16.1637 WER(5gram): 16.95% (n=256) time: 15.773
2026-01-08 16:01:15,062: WER lens: avg_true_words=5.99 avg_pred_words=6.22 max_pred_words=12
2026-01-08 16:01:15,063: t15.2023.08.13 val PER: 0.1175
2026-01-08 16:01:15,065: t15.2023.08.18 val PER: 0.1106
2026-01-08 16:01:15,067: t15.2023.08.20 val PER: 0.1144
2026-01-08 16:01:15,068: t15.2023.08.25 val PER: 0.0964
2026-01-08 16:01:15,070: t15.2023.08.27 val PER: 0.1945
2026-01-08 16:01:15,072: t15.2023.09.01 val PER: 0.0828
2026-01-08 16:01:15,073: t15.2023.09.03 val PER: 0.1532
2026-01-08 16:01:15,075: t15.2023.09.24 val PER: 0.1299
2026-01-08 16:01:15,077: t15.2023.09.29 val PER: 0.1410
2026-01-08 16:01:15,078: t15.2023.10.01 val PER: 0.1764
2026-01-08 16:01:15,080: t15.2023.10.06 val PER: 0.0926
2026-01-08 16:01:15,082: t15.2023.10.08 val PER: 0.2476
2026-01-08 16:01:15,083: t15.2023.10.13 val PER: 0.2048
2026-01-08 16:01:15,085: t15.2023.10.15 val PER: 0.1727
2026-01-08 16:01:15,086: t15.2023.10.20 val PER: 0.2047
2026-01-08 16:01:15,088: t15.2023.10.22 val PER: 0.1203
2026-01-08 16:01:15,090: t15.2023.11.03 val PER: 0.1845
2026-01-08 16:01:15,092: t15.2023.11.04 val PER: 0.0410
2026-01-08 16:01:15,093: t15.2023.11.17 val PER: 0.0451
2026-01-08 16:01:15,095: t15.2023.11.19 val PER: 0.0459
2026-01-08 16:01:15,097: t15.2023.11.26 val PER: 0.1333
2026-01-08 16:01:15,098: t15.2023.12.03 val PER: 0.1239
2026-01-08 16:01:15,100: t15.2023.12.08 val PER: 0.1085
2026-01-08 16:01:15,101: t15.2023.12.10 val PER: 0.0959
2026-01-08 16:01:15,103: t15.2023.12.17 val PER: 0.1341
2026-01-08 16:01:15,104: t15.2023.12.29 val PER: 0.1400
2026-01-08 16:01:15,106: t15.2024.02.25 val PER: 0.1264
2026-01-08 16:01:15,108: t15.2024.03.08 val PER: 0.2361
2026-01-08 16:01:15,109: t15.2024.03.15 val PER: 0.2164
2026-01-08 16:01:15,111: t15.2024.03.17 val PER: 0.1541
2026-01-08 16:01:15,112: t15.2024.05.10 val PER: 0.1902
2026-01-08 16:01:15,114: t15.2024.06.14 val PER: 0.1924
2026-01-08 16:01:15,115: t15.2024.07.19 val PER: 0.2518
2026-01-08 16:01:15,116: t15.2024.07.21 val PER: 0.1097
2026-01-08 16:01:15,118: t15.2024.07.28 val PER: 0.1375
2026-01-08 16:01:15,119: t15.2025.01.10 val PER: 0.3017
2026-01-08 16:01:15,122: t15.2025.01.12 val PER: 0.1609
2026-01-08 16:01:15,124: t15.2025.03.14 val PER: 0.3536
2026-01-08 16:01:15,125: t15.2025.03.16 val PER: 0.1937
2026-01-08 16:01:15,127: t15.2025.03.30 val PER: 0.3115
2026-01-08 16:01:15,128: t15.2025.04.13 val PER: 0.2197
2026-01-08 16:01:15,288: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_12000
2026-01-08 16:01:31,547: Train batch 12200: loss: 5.84 grad norm: 40.28 time: 0.068
2026-01-08 16:01:47,409: Train batch 12400: loss: 4.72 grad norm: 37.36 time: 0.042
2026-01-08 16:01:55,587: Running test after training batch: 12500
2026-01-08 16:01:55,707: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:02:00,619: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:02:00,670: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost set
2026-01-08 16:02:11,814: Val batch 12500: PER (avg): 0.1567 CTC Loss (avg): 16.0567 WER(5gram): 15.78% (n=256) time: 16.225
2026-01-08 16:02:11,817: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 16:02:11,819: t15.2023.08.13 val PER: 0.1237
2026-01-08 16:02:11,821: t15.2023.08.18 val PER: 0.1106
2026-01-08 16:02:11,822: t15.2023.08.20 val PER: 0.1136
2026-01-08 16:02:11,824: t15.2023.08.25 val PER: 0.0919
2026-01-08 16:02:11,826: t15.2023.08.27 val PER: 0.1833
2026-01-08 16:02:11,827: t15.2023.09.01 val PER: 0.0836
2026-01-08 16:02:11,829: t15.2023.09.03 val PER: 0.1615
2026-01-08 16:02:11,830: t15.2023.09.24 val PER: 0.1250
2026-01-08 16:02:11,832: t15.2023.09.29 val PER: 0.1327
2026-01-08 16:02:11,833: t15.2023.10.01 val PER: 0.1704
2026-01-08 16:02:11,835: t15.2023.10.06 val PER: 0.0818
2026-01-08 16:02:11,837: t15.2023.10.08 val PER: 0.2544
2026-01-08 16:02:11,838: t15.2023.10.13 val PER: 0.2141
2026-01-08 16:02:11,840: t15.2023.10.15 val PER: 0.1582
2026-01-08 16:02:11,841: t15.2023.10.20 val PER: 0.1879
2026-01-08 16:02:11,843: t15.2023.10.22 val PER: 0.1136
2026-01-08 16:02:11,846: t15.2023.11.03 val PER: 0.1859
2026-01-08 16:02:11,847: t15.2023.11.04 val PER: 0.0375
2026-01-08 16:02:11,849: t15.2023.11.17 val PER: 0.0513
2026-01-08 16:02:11,851: t15.2023.11.19 val PER: 0.0359
2026-01-08 16:02:11,852: t15.2023.11.26 val PER: 0.1304
2026-01-08 16:02:11,854: t15.2023.12.03 val PER: 0.1176
2026-01-08 16:02:11,855: t15.2023.12.08 val PER: 0.1085
2026-01-08 16:02:11,857: t15.2023.12.10 val PER: 0.0972
2026-01-08 16:02:11,859: t15.2023.12.17 val PER: 0.1497
2026-01-08 16:02:11,861: t15.2023.12.29 val PER: 0.1400
2026-01-08 16:02:11,862: t15.2024.02.25 val PER: 0.1096
2026-01-08 16:02:11,864: t15.2024.03.08 val PER: 0.2447
2026-01-08 16:02:11,865: t15.2024.03.15 val PER: 0.2120
2026-01-08 16:02:11,867: t15.2024.03.17 val PER: 0.1457
2026-01-08 16:02:11,868: t15.2024.05.10 val PER: 0.1724
2026-01-08 16:02:11,870: t15.2024.06.14 val PER: 0.1861
2026-01-08 16:02:11,872: t15.2024.07.19 val PER: 0.2452
2026-01-08 16:02:11,873: t15.2024.07.21 val PER: 0.1021
2026-01-08 16:02:11,875: t15.2024.07.28 val PER: 0.1301
2026-01-08 16:02:11,876: t15.2025.01.10 val PER: 0.2989
2026-01-08 16:02:11,878: t15.2025.01.12 val PER: 0.1501
2026-01-08 16:02:11,881: t15.2025.03.14 val PER: 0.3654
2026-01-08 16:02:11,882: t15.2025.03.16 val PER: 0.2081
2026-01-08 16:02:11,884: t15.2025.03.30 val PER: 0.2943
2026-01-08 16:02:11,885: t15.2025.04.13 val PER: 0.2225
2026-01-08 16:02:11,887: New best val WER(5gram) 15.97% --> 15.78%
2026-01-08 16:02:12,078: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_12500
2026-01-08 16:02:20,057: Train batch 12600: loss: 7.79 grad norm: 42.12 time: 0.057
2026-01-08 16:02:36,083: Train batch 12800: loss: 5.77 grad norm: 38.36 time: 0.053
2026-01-08 16:02:53,340: Train batch 13000: loss: 6.27 grad norm: 39.88 time: 0.067
2026-01-08 16:02:53,342: Running test after training batch: 13000
2026-01-08 16:02:53,439: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:02:58,286: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:02:58,331: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost in
2026-01-08 16:03:09,237: Val batch 13000: PER (avg): 0.1563 CTC Loss (avg): 15.8198 WER(5gram): 15.65% (n=256) time: 15.893
2026-01-08 16:03:09,240: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-08 16:03:09,242: t15.2023.08.13 val PER: 0.1185
2026-01-08 16:03:09,244: t15.2023.08.18 val PER: 0.1132
2026-01-08 16:03:09,246: t15.2023.08.20 val PER: 0.1088
2026-01-08 16:03:09,248: t15.2023.08.25 val PER: 0.0949
2026-01-08 16:03:09,250: t15.2023.08.27 val PER: 0.1865
2026-01-08 16:03:09,251: t15.2023.09.01 val PER: 0.0828
2026-01-08 16:03:09,253: t15.2023.09.03 val PER: 0.1722
2026-01-08 16:03:09,257: t15.2023.09.24 val PER: 0.1286
2026-01-08 16:03:09,258: t15.2023.09.29 val PER: 0.1308
2026-01-08 16:03:09,260: t15.2023.10.01 val PER: 0.1704
2026-01-08 16:03:09,261: t15.2023.10.06 val PER: 0.0893
2026-01-08 16:03:09,263: t15.2023.10.08 val PER: 0.2517
2026-01-08 16:03:09,265: t15.2023.10.13 val PER: 0.2017
2026-01-08 16:03:09,267: t15.2023.10.15 val PER: 0.1681
2026-01-08 16:03:09,268: t15.2023.10.20 val PER: 0.1946
2026-01-08 16:03:09,271: t15.2023.10.22 val PER: 0.1136
2026-01-08 16:03:09,273: t15.2023.11.03 val PER: 0.1811
2026-01-08 16:03:09,274: t15.2023.11.04 val PER: 0.0375
2026-01-08 16:03:09,276: t15.2023.11.17 val PER: 0.0451
2026-01-08 16:03:09,277: t15.2023.11.19 val PER: 0.0419
2026-01-08 16:03:09,279: t15.2023.11.26 val PER: 0.1319
2026-01-08 16:03:09,280: t15.2023.12.03 val PER: 0.1166
2026-01-08 16:03:09,282: t15.2023.12.08 val PER: 0.1059
2026-01-08 16:03:09,284: t15.2023.12.10 val PER: 0.1025
2026-01-08 16:03:09,285: t15.2023.12.17 val PER: 0.1445
2026-01-08 16:03:09,287: t15.2023.12.29 val PER: 0.1400
2026-01-08 16:03:09,288: t15.2024.02.25 val PER: 0.1053
2026-01-08 16:03:09,290: t15.2024.03.08 val PER: 0.2418
2026-01-08 16:03:09,291: t15.2024.03.15 val PER: 0.2051
2026-01-08 16:03:09,292: t15.2024.03.17 val PER: 0.1450
2026-01-08 16:03:09,294: t15.2024.05.10 val PER: 0.1694
2026-01-08 16:03:09,295: t15.2024.06.14 val PER: 0.1656
2026-01-08 16:03:09,297: t15.2024.07.19 val PER: 0.2485
2026-01-08 16:03:09,298: t15.2024.07.21 val PER: 0.1048
2026-01-08 16:03:09,300: t15.2024.07.28 val PER: 0.1412
2026-01-08 16:03:09,301: t15.2025.01.10 val PER: 0.2934
2026-01-08 16:03:09,302: t15.2025.01.12 val PER: 0.1570
2026-01-08 16:03:09,304: t15.2025.03.14 val PER: 0.3462
2026-01-08 16:03:09,305: t15.2025.03.16 val PER: 0.1911
2026-01-08 16:03:09,307: t15.2025.03.30 val PER: 0.3103
2026-01-08 16:03:09,308: t15.2025.04.13 val PER: 0.2225
2026-01-08 16:03:09,310: New best val WER(5gram) 15.78% --> 15.65%
2026-01-08 16:03:09,501: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_13000
2026-01-08 16:03:25,802: Train batch 13200: loss: 12.19 grad norm: 59.88 time: 0.057
2026-01-08 16:03:42,820: Train batch 13400: loss: 8.62 grad norm: 50.73 time: 0.065
2026-01-08 16:03:51,314: Running test after training batch: 13500
2026-01-08 16:03:51,424: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:03:56,441: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:03:56,486: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 16:04:07,790: Val batch 13500: PER (avg): 0.1524 CTC Loss (avg): 15.5697 WER(5gram): 16.69% (n=256) time: 16.473
2026-01-08 16:04:07,792: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=13
2026-01-08 16:04:07,794: t15.2023.08.13 val PER: 0.1164
2026-01-08 16:04:07,796: t15.2023.08.18 val PER: 0.1132
2026-01-08 16:04:07,798: t15.2023.08.20 val PER: 0.1104
2026-01-08 16:04:07,800: t15.2023.08.25 val PER: 0.0873
2026-01-08 16:04:07,801: t15.2023.08.27 val PER: 0.1833
2026-01-08 16:04:07,803: t15.2023.09.01 val PER: 0.0771
2026-01-08 16:04:07,805: t15.2023.09.03 val PER: 0.1496
2026-01-08 16:04:07,806: t15.2023.09.24 val PER: 0.1226
2026-01-08 16:04:07,809: t15.2023.09.29 val PER: 0.1270
2026-01-08 16:04:07,811: t15.2023.10.01 val PER: 0.1770
2026-01-08 16:04:07,812: t15.2023.10.06 val PER: 0.0915
2026-01-08 16:04:07,814: t15.2023.10.08 val PER: 0.2585
2026-01-08 16:04:07,817: t15.2023.10.13 val PER: 0.1986
2026-01-08 16:04:07,819: t15.2023.10.15 val PER: 0.1562
2026-01-08 16:04:07,821: t15.2023.10.20 val PER: 0.1946
2026-01-08 16:04:07,822: t15.2023.10.22 val PER: 0.1158
2026-01-08 16:04:07,824: t15.2023.11.03 val PER: 0.1818
2026-01-08 16:04:07,826: t15.2023.11.04 val PER: 0.0341
2026-01-08 16:04:07,828: t15.2023.11.17 val PER: 0.0435
2026-01-08 16:04:07,829: t15.2023.11.19 val PER: 0.0319
2026-01-08 16:04:07,831: t15.2023.11.26 val PER: 0.1232
2026-01-08 16:04:07,833: t15.2023.12.03 val PER: 0.1092
2026-01-08 16:04:07,834: t15.2023.12.08 val PER: 0.1072
2026-01-08 16:04:07,836: t15.2023.12.10 val PER: 0.0920
2026-01-08 16:04:07,838: t15.2023.12.17 val PER: 0.1341
2026-01-08 16:04:07,839: t15.2023.12.29 val PER: 0.1352
2026-01-08 16:04:07,841: t15.2024.02.25 val PER: 0.1096
2026-01-08 16:04:07,843: t15.2024.03.08 val PER: 0.2390
2026-01-08 16:04:07,845: t15.2024.03.15 val PER: 0.2033
2026-01-08 16:04:07,847: t15.2024.03.17 val PER: 0.1464
2026-01-08 16:04:07,848: t15.2024.05.10 val PER: 0.1605
2026-01-08 16:04:07,850: t15.2024.06.14 val PER: 0.1656
2026-01-08 16:04:07,851: t15.2024.07.19 val PER: 0.2380
2026-01-08 16:04:07,853: t15.2024.07.21 val PER: 0.0945
2026-01-08 16:04:07,855: t15.2024.07.28 val PER: 0.1353
2026-01-08 16:04:07,856: t15.2025.01.10 val PER: 0.2879
2026-01-08 16:04:07,858: t15.2025.01.12 val PER: 0.1486
2026-01-08 16:04:07,859: t15.2025.03.14 val PER: 0.3491
2026-01-08 16:04:07,861: t15.2025.03.16 val PER: 0.1793
2026-01-08 16:04:07,863: t15.2025.03.30 val PER: 0.3138
2026-01-08 16:04:07,864: t15.2025.04.13 val PER: 0.2183
2026-01-08 16:04:08,006: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_13500
2026-01-08 16:04:16,628: Train batch 13600: loss: 12.82 grad norm: 63.08 time: 0.065
2026-01-08 16:04:32,642: Train batch 13800: loss: 9.09 grad norm: 54.07 time: 0.056
2026-01-08 16:04:48,564: Train batch 14000: loss: 11.35 grad norm: 57.91 time: 0.051
2026-01-08 16:04:48,566: Running test after training batch: 14000
2026-01-08 16:04:48,666: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:04:53,680: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:04:53,729: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 16:05:05,240: Val batch 14000: PER (avg): 0.1513 CTC Loss (avg): 15.4720 WER(5gram): 16.49% (n=256) time: 16.672
2026-01-08 16:05:05,243: WER lens: avg_true_words=5.99 avg_pred_words=6.22 max_pred_words=13
2026-01-08 16:05:05,244: t15.2023.08.13 val PER: 0.1102
2026-01-08 16:05:05,246: t15.2023.08.18 val PER: 0.1039
2026-01-08 16:05:05,247: t15.2023.08.20 val PER: 0.1025
2026-01-08 16:05:05,249: t15.2023.08.25 val PER: 0.0979
2026-01-08 16:05:05,250: t15.2023.08.27 val PER: 0.1785
2026-01-08 16:05:05,252: t15.2023.09.01 val PER: 0.0795
2026-01-08 16:05:05,253: t15.2023.09.03 val PER: 0.1615
2026-01-08 16:05:05,254: t15.2023.09.24 val PER: 0.1262
2026-01-08 16:05:05,256: t15.2023.09.29 val PER: 0.1327
2026-01-08 16:05:05,257: t15.2023.10.01 val PER: 0.1711
2026-01-08 16:05:05,259: t15.2023.10.06 val PER: 0.0936
2026-01-08 16:05:05,260: t15.2023.10.08 val PER: 0.2571
2026-01-08 16:05:05,262: t15.2023.10.13 val PER: 0.1932
2026-01-08 16:05:05,263: t15.2023.10.15 val PER: 0.1602
2026-01-08 16:05:05,264: t15.2023.10.20 val PER: 0.1879
2026-01-08 16:05:05,266: t15.2023.10.22 val PER: 0.1125
2026-01-08 16:05:05,267: t15.2023.11.03 val PER: 0.1798
2026-01-08 16:05:05,269: t15.2023.11.04 val PER: 0.0273
2026-01-08 16:05:05,270: t15.2023.11.17 val PER: 0.0467
2026-01-08 16:05:05,272: t15.2023.11.19 val PER: 0.0359
2026-01-08 16:05:05,273: t15.2023.11.26 val PER: 0.1239
2026-01-08 16:05:05,275: t15.2023.12.03 val PER: 0.1208
2026-01-08 16:05:05,277: t15.2023.12.08 val PER: 0.1052
2026-01-08 16:05:05,279: t15.2023.12.10 val PER: 0.0986
2026-01-08 16:05:05,280: t15.2023.12.17 val PER: 0.1331
2026-01-08 16:05:05,281: t15.2023.12.29 val PER: 0.1311
2026-01-08 16:05:05,283: t15.2024.02.25 val PER: 0.1152
2026-01-08 16:05:05,284: t15.2024.03.08 val PER: 0.2162
2026-01-08 16:05:05,286: t15.2024.03.15 val PER: 0.2039
2026-01-08 16:05:05,287: t15.2024.03.17 val PER: 0.1444
2026-01-08 16:05:05,289: t15.2024.05.10 val PER: 0.1694
2026-01-08 16:05:05,290: t15.2024.06.14 val PER: 0.1640
2026-01-08 16:05:05,292: t15.2024.07.19 val PER: 0.2320
2026-01-08 16:05:05,293: t15.2024.07.21 val PER: 0.0897
2026-01-08 16:05:05,295: t15.2024.07.28 val PER: 0.1324
2026-01-08 16:05:05,296: t15.2025.01.10 val PER: 0.2948
2026-01-08 16:05:05,298: t15.2025.01.12 val PER: 0.1440
2026-01-08 16:05:05,301: t15.2025.03.14 val PER: 0.3417
2026-01-08 16:05:05,302: t15.2025.03.16 val PER: 0.1911
2026-01-08 16:05:05,304: t15.2025.03.30 val PER: 0.3023
2026-01-08 16:05:05,306: t15.2025.04.13 val PER: 0.2126
2026-01-08 16:05:05,447: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_14000
2026-01-08 16:05:22,392: Train batch 14200: loss: 8.19 grad norm: 54.20 time: 0.057
2026-01-08 16:05:38,861: Train batch 14400: loss: 5.87 grad norm: 39.44 time: 0.068
2026-01-08 16:05:46,941: Running test after training batch: 14500
2026-01-08 16:05:47,038: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:05:51,979: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:05:52,031: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost get
2026-01-08 16:06:03,298: Val batch 14500: PER (avg): 0.1514 CTC Loss (avg): 15.5896 WER(5gram): 16.36% (n=256) time: 16.355
2026-01-08 16:06:03,301: WER lens: avg_true_words=5.99 avg_pred_words=6.23 max_pred_words=12
2026-01-08 16:06:03,304: t15.2023.08.13 val PER: 0.1185
2026-01-08 16:06:03,307: t15.2023.08.18 val PER: 0.1090
2026-01-08 16:06:03,309: t15.2023.08.20 val PER: 0.1025
2026-01-08 16:06:03,311: t15.2023.08.25 val PER: 0.0904
2026-01-08 16:06:03,313: t15.2023.08.27 val PER: 0.1785
2026-01-08 16:06:03,315: t15.2023.09.01 val PER: 0.0779
2026-01-08 16:06:03,317: t15.2023.09.03 val PER: 0.1568
2026-01-08 16:06:03,319: t15.2023.09.24 val PER: 0.1359
2026-01-08 16:06:03,322: t15.2023.09.29 val PER: 0.1276
2026-01-08 16:06:03,324: t15.2023.10.01 val PER: 0.1757
2026-01-08 16:06:03,326: t15.2023.10.06 val PER: 0.0818
2026-01-08 16:06:03,328: t15.2023.10.08 val PER: 0.2463
2026-01-08 16:06:03,329: t15.2023.10.13 val PER: 0.2002
2026-01-08 16:06:03,331: t15.2023.10.15 val PER: 0.1589
2026-01-08 16:06:03,333: t15.2023.10.20 val PER: 0.1812
2026-01-08 16:06:03,336: t15.2023.10.22 val PER: 0.1114
2026-01-08 16:06:03,338: t15.2023.11.03 val PER: 0.1859
2026-01-08 16:06:03,340: t15.2023.11.04 val PER: 0.0375
2026-01-08 16:06:03,342: t15.2023.11.17 val PER: 0.0498
2026-01-08 16:06:03,343: t15.2023.11.19 val PER: 0.0299
2026-01-08 16:06:03,345: t15.2023.11.26 val PER: 0.1225
2026-01-08 16:06:03,347: t15.2023.12.03 val PER: 0.1082
2026-01-08 16:06:03,349: t15.2023.12.08 val PER: 0.1012
2026-01-08 16:06:03,351: t15.2023.12.10 val PER: 0.0907
2026-01-08 16:06:03,353: t15.2023.12.17 val PER: 0.1445
2026-01-08 16:06:03,355: t15.2023.12.29 val PER: 0.1386
2026-01-08 16:06:03,357: t15.2024.02.25 val PER: 0.1152
2026-01-08 16:06:03,359: t15.2024.03.08 val PER: 0.2134
2026-01-08 16:06:03,360: t15.2024.03.15 val PER: 0.1995
2026-01-08 16:06:03,362: t15.2024.03.17 val PER: 0.1409
2026-01-08 16:06:03,364: t15.2024.05.10 val PER: 0.1471
2026-01-08 16:06:03,366: t15.2024.06.14 val PER: 0.1703
2026-01-08 16:06:03,368: t15.2024.07.19 val PER: 0.2465
2026-01-08 16:06:03,370: t15.2024.07.21 val PER: 0.0959
2026-01-08 16:06:03,372: t15.2024.07.28 val PER: 0.1338
2026-01-08 16:06:03,374: t15.2025.01.10 val PER: 0.2906
2026-01-08 16:06:03,375: t15.2025.01.12 val PER: 0.1470
2026-01-08 16:06:03,378: t15.2025.03.14 val PER: 0.3462
2026-01-08 16:06:03,380: t15.2025.03.16 val PER: 0.1819
2026-01-08 16:06:03,382: t15.2025.03.30 val PER: 0.2966
2026-01-08 16:06:03,383: t15.2025.04.13 val PER: 0.2154
2026-01-08 16:06:03,520: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_14500
2026-01-08 16:06:12,178: Train batch 14600: loss: 12.02 grad norm: 61.11 time: 0.059
2026-01-08 16:06:29,412: Train batch 14800: loss: 5.65 grad norm: 43.71 time: 0.051
2026-01-08 16:06:45,836: Train batch 15000: loss: 8.76 grad norm: 52.67 time: 0.052
2026-01-08 16:06:45,838: Running test after training batch: 15000
2026-01-08 16:06:45,947: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:06:50,853: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:06:50,906: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-08 16:07:02,202: Val batch 15000: PER (avg): 0.1488 CTC Loss (avg): 15.2603 WER(5gram): 14.41% (n=256) time: 16.361
2026-01-08 16:07:02,205: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 16:07:02,209: t15.2023.08.13 val PER: 0.1060
2026-01-08 16:07:02,211: t15.2023.08.18 val PER: 0.1031
2026-01-08 16:07:02,213: t15.2023.08.20 val PER: 0.1128
2026-01-08 16:07:02,215: t15.2023.08.25 val PER: 0.0919
2026-01-08 16:07:02,216: t15.2023.08.27 val PER: 0.1849
2026-01-08 16:07:02,218: t15.2023.09.01 val PER: 0.0722
2026-01-08 16:07:02,220: t15.2023.09.03 val PER: 0.1461
2026-01-08 16:07:02,222: t15.2023.09.24 val PER: 0.1299
2026-01-08 16:07:02,224: t15.2023.09.29 val PER: 0.1264
2026-01-08 16:07:02,229: t15.2023.10.01 val PER: 0.1664
2026-01-08 16:07:02,232: t15.2023.10.06 val PER: 0.0840
2026-01-08 16:07:02,234: t15.2023.10.08 val PER: 0.2476
2026-01-08 16:07:02,236: t15.2023.10.13 val PER: 0.1986
2026-01-08 16:07:02,238: t15.2023.10.15 val PER: 0.1589
2026-01-08 16:07:02,240: t15.2023.10.20 val PER: 0.1980
2026-01-08 16:07:02,241: t15.2023.10.22 val PER: 0.1091
2026-01-08 16:07:02,243: t15.2023.11.03 val PER: 0.1839
2026-01-08 16:07:02,244: t15.2023.11.04 val PER: 0.0410
2026-01-08 16:07:02,246: t15.2023.11.17 val PER: 0.0420
2026-01-08 16:07:02,248: t15.2023.11.19 val PER: 0.0319
2026-01-08 16:07:02,249: t15.2023.11.26 val PER: 0.1174
2026-01-08 16:07:02,251: t15.2023.12.03 val PER: 0.1050
2026-01-08 16:07:02,253: t15.2023.12.08 val PER: 0.1012
2026-01-08 16:07:02,254: t15.2023.12.10 val PER: 0.0920
2026-01-08 16:07:02,256: t15.2023.12.17 val PER: 0.1393
2026-01-08 16:07:02,257: t15.2023.12.29 val PER: 0.1304
2026-01-08 16:07:02,261: t15.2024.02.25 val PER: 0.0997
2026-01-08 16:07:02,262: t15.2024.03.08 val PER: 0.2248
2026-01-08 16:07:02,264: t15.2024.03.15 val PER: 0.1970
2026-01-08 16:07:02,265: t15.2024.03.17 val PER: 0.1367
2026-01-08 16:07:02,267: t15.2024.05.10 val PER: 0.1545
2026-01-08 16:07:02,269: t15.2024.06.14 val PER: 0.1703
2026-01-08 16:07:02,270: t15.2024.07.19 val PER: 0.2393
2026-01-08 16:07:02,272: t15.2024.07.21 val PER: 0.0862
2026-01-08 16:07:02,274: t15.2024.07.28 val PER: 0.1257
2026-01-08 16:07:02,275: t15.2025.01.10 val PER: 0.2961
2026-01-08 16:07:02,277: t15.2025.01.12 val PER: 0.1432
2026-01-08 16:07:02,278: t15.2025.03.14 val PER: 0.3476
2026-01-08 16:07:02,280: t15.2025.03.16 val PER: 0.1728
2026-01-08 16:07:02,282: t15.2025.03.30 val PER: 0.2989
2026-01-08 16:07:02,283: t15.2025.04.13 val PER: 0.2254
2026-01-08 16:07:02,285: New best val WER(5gram) 15.65% --> 14.41%
2026-01-08 16:07:02,476: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_15000
2026-01-08 16:07:18,514: Train batch 15200: loss: 4.80 grad norm: 39.77 time: 0.059
2026-01-08 16:07:35,204: Train batch 15400: loss: 11.18 grad norm: 57.13 time: 0.050
2026-01-08 16:07:44,015: Running test after training batch: 15500
2026-01-08 16:07:44,170: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:07:49,057: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:07:49,109: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-08 16:08:00,494: Val batch 15500: PER (avg): 0.1490 CTC Loss (avg): 15.2052 WER(5gram): 15.38% (n=256) time: 16.477
2026-01-08 16:08:00,497: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-08 16:08:00,501: t15.2023.08.13 val PER: 0.1112
2026-01-08 16:08:00,505: t15.2023.08.18 val PER: 0.1006
2026-01-08 16:08:00,507: t15.2023.08.20 val PER: 0.1080
2026-01-08 16:08:00,509: t15.2023.08.25 val PER: 0.0919
2026-01-08 16:08:00,511: t15.2023.08.27 val PER: 0.1817
2026-01-08 16:08:00,512: t15.2023.09.01 val PER: 0.0739
2026-01-08 16:08:00,514: t15.2023.09.03 val PER: 0.1496
2026-01-08 16:08:00,516: t15.2023.09.24 val PER: 0.1165
2026-01-08 16:08:00,518: t15.2023.09.29 val PER: 0.1238
2026-01-08 16:08:00,520: t15.2023.10.01 val PER: 0.1717
2026-01-08 16:08:00,522: t15.2023.10.06 val PER: 0.0850
2026-01-08 16:08:00,523: t15.2023.10.08 val PER: 0.2355
2026-01-08 16:08:00,525: t15.2023.10.13 val PER: 0.1877
2026-01-08 16:08:00,527: t15.2023.10.15 val PER: 0.1575
2026-01-08 16:08:00,529: t15.2023.10.20 val PER: 0.1913
2026-01-08 16:08:00,531: t15.2023.10.22 val PER: 0.1125
2026-01-08 16:08:00,533: t15.2023.11.03 val PER: 0.1764
2026-01-08 16:08:00,534: t15.2023.11.04 val PER: 0.0375
2026-01-08 16:08:00,536: t15.2023.11.17 val PER: 0.0467
2026-01-08 16:08:00,538: t15.2023.11.19 val PER: 0.0419
2026-01-08 16:08:00,539: t15.2023.11.26 val PER: 0.1188
2026-01-08 16:08:00,541: t15.2023.12.03 val PER: 0.1124
2026-01-08 16:08:00,543: t15.2023.12.08 val PER: 0.0999
2026-01-08 16:08:00,544: t15.2023.12.10 val PER: 0.0828
2026-01-08 16:08:00,546: t15.2023.12.17 val PER: 0.1435
2026-01-08 16:08:00,548: t15.2023.12.29 val PER: 0.1311
2026-01-08 16:08:00,552: t15.2024.02.25 val PER: 0.1138
2026-01-08 16:08:00,554: t15.2024.03.08 val PER: 0.2304
2026-01-08 16:08:00,555: t15.2024.03.15 val PER: 0.1982
2026-01-08 16:08:00,557: t15.2024.03.17 val PER: 0.1374
2026-01-08 16:08:00,559: t15.2024.05.10 val PER: 0.1709
2026-01-08 16:08:00,560: t15.2024.06.14 val PER: 0.1656
2026-01-08 16:08:00,562: t15.2024.07.19 val PER: 0.2347
2026-01-08 16:08:00,564: t15.2024.07.21 val PER: 0.0945
2026-01-08 16:08:00,566: t15.2024.07.28 val PER: 0.1316
2026-01-08 16:08:00,569: t15.2025.01.10 val PER: 0.2961
2026-01-08 16:08:00,571: t15.2025.01.12 val PER: 0.1478
2026-01-08 16:08:00,573: t15.2025.03.14 val PER: 0.3417
2026-01-08 16:08:00,574: t15.2025.03.16 val PER: 0.1793
2026-01-08 16:08:00,576: t15.2025.03.30 val PER: 0.2966
2026-01-08 16:08:00,577: t15.2025.04.13 val PER: 0.2183
2026-01-08 16:08:00,716: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_15500
2026-01-08 16:08:09,267: Train batch 15600: loss: 11.87 grad norm: 59.03 time: 0.065
2026-01-08 16:08:26,039: Train batch 15800: loss: 13.57 grad norm: 63.72 time: 0.067
2026-01-08 16:08:42,731: Train batch 16000: loss: 8.62 grad norm: 46.96 time: 0.055
2026-01-08 16:08:42,733: Running test after training batch: 16000
2026-01-08 16:08:42,858: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:08:47,716: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:08:47,768: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-08 16:08:59,244: Val batch 16000: PER (avg): 0.1495 CTC Loss (avg): 15.2961 WER(5gram): 15.97% (n=256) time: 16.509
2026-01-08 16:08:59,247: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=12
2026-01-08 16:08:59,249: t15.2023.08.13 val PER: 0.1060
2026-01-08 16:08:59,250: t15.2023.08.18 val PER: 0.0989
2026-01-08 16:08:59,252: t15.2023.08.20 val PER: 0.1048
2026-01-08 16:08:59,254: t15.2023.08.25 val PER: 0.1009
2026-01-08 16:08:59,255: t15.2023.08.27 val PER: 0.1881
2026-01-08 16:08:59,257: t15.2023.09.01 val PER: 0.0804
2026-01-08 16:08:59,258: t15.2023.09.03 val PER: 0.1485
2026-01-08 16:08:59,260: t15.2023.09.24 val PER: 0.1226
2026-01-08 16:08:59,262: t15.2023.09.29 val PER: 0.1295
2026-01-08 16:08:59,263: t15.2023.10.01 val PER: 0.1724
2026-01-08 16:08:59,266: t15.2023.10.06 val PER: 0.0829
2026-01-08 16:08:59,267: t15.2023.10.08 val PER: 0.2422
2026-01-08 16:08:59,269: t15.2023.10.13 val PER: 0.1978
2026-01-08 16:08:59,270: t15.2023.10.15 val PER: 0.1549
2026-01-08 16:08:59,272: t15.2023.10.20 val PER: 0.1779
2026-01-08 16:08:59,274: t15.2023.10.22 val PER: 0.1091
2026-01-08 16:08:59,275: t15.2023.11.03 val PER: 0.1818
2026-01-08 16:08:59,277: t15.2023.11.04 val PER: 0.0341
2026-01-08 16:08:59,278: t15.2023.11.17 val PER: 0.0435
2026-01-08 16:08:59,281: t15.2023.11.19 val PER: 0.0339
2026-01-08 16:08:59,283: t15.2023.11.26 val PER: 0.1217
2026-01-08 16:08:59,285: t15.2023.12.03 val PER: 0.1092
2026-01-08 16:08:59,286: t15.2023.12.08 val PER: 0.0992
2026-01-08 16:08:59,288: t15.2023.12.10 val PER: 0.0894
2026-01-08 16:08:59,289: t15.2023.12.17 val PER: 0.1372
2026-01-08 16:08:59,291: t15.2023.12.29 val PER: 0.1263
2026-01-08 16:08:59,293: t15.2024.02.25 val PER: 0.1039
2026-01-08 16:08:59,294: t15.2024.03.08 val PER: 0.2404
2026-01-08 16:08:59,296: t15.2024.03.15 val PER: 0.1976
2026-01-08 16:08:59,297: t15.2024.03.17 val PER: 0.1318
2026-01-08 16:08:59,298: t15.2024.05.10 val PER: 0.1664
2026-01-08 16:08:59,300: t15.2024.06.14 val PER: 0.1735
2026-01-08 16:08:59,301: t15.2024.07.19 val PER: 0.2432
2026-01-08 16:08:59,303: t15.2024.07.21 val PER: 0.0897
2026-01-08 16:08:59,304: t15.2024.07.28 val PER: 0.1353
2026-01-08 16:08:59,306: t15.2025.01.10 val PER: 0.2906
2026-01-08 16:08:59,308: t15.2025.01.12 val PER: 0.1386
2026-01-08 16:08:59,309: t15.2025.03.14 val PER: 0.3521
2026-01-08 16:08:59,310: t15.2025.03.16 val PER: 0.1885
2026-01-08 16:08:59,312: t15.2025.03.30 val PER: 0.2989
2026-01-08 16:08:59,314: t15.2025.04.13 val PER: 0.2154
2026-01-08 16:08:59,454: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_16000
2026-01-08 16:09:15,947: Train batch 16200: loss: 6.41 grad norm: 41.21 time: 0.055
2026-01-08 16:09:32,117: Train batch 16400: loss: 10.19 grad norm: 62.37 time: 0.057
2026-01-08 16:09:40,362: Running test after training batch: 16500
2026-01-08 16:09:40,456: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:09:45,348: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:09:45,398: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost get
2026-01-08 16:09:57,066: Val batch 16500: PER (avg): 0.1480 CTC Loss (avg): 15.1766 WER(5gram): 15.25% (n=256) time: 16.702
2026-01-08 16:09:57,074: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-08 16:09:57,077: t15.2023.08.13 val PER: 0.1102
2026-01-08 16:09:57,079: t15.2023.08.18 val PER: 0.0997
2026-01-08 16:09:57,080: t15.2023.08.20 val PER: 0.1072
2026-01-08 16:09:57,082: t15.2023.08.25 val PER: 0.0873
2026-01-08 16:09:57,084: t15.2023.08.27 val PER: 0.1897
2026-01-08 16:09:57,086: t15.2023.09.01 val PER: 0.0779
2026-01-08 16:09:57,087: t15.2023.09.03 val PER: 0.1520
2026-01-08 16:09:57,089: t15.2023.09.24 val PER: 0.1299
2026-01-08 16:09:57,091: t15.2023.09.29 val PER: 0.1270
2026-01-08 16:09:57,092: t15.2023.10.01 val PER: 0.1711
2026-01-08 16:09:57,094: t15.2023.10.06 val PER: 0.0850
2026-01-08 16:09:57,096: t15.2023.10.08 val PER: 0.2436
2026-01-08 16:09:57,097: t15.2023.10.13 val PER: 0.1939
2026-01-08 16:09:57,099: t15.2023.10.15 val PER: 0.1575
2026-01-08 16:09:57,101: t15.2023.10.20 val PER: 0.1846
2026-01-08 16:09:57,102: t15.2023.10.22 val PER: 0.1058
2026-01-08 16:09:57,104: t15.2023.11.03 val PER: 0.1784
2026-01-08 16:09:57,106: t15.2023.11.04 val PER: 0.0341
2026-01-08 16:09:57,108: t15.2023.11.17 val PER: 0.0373
2026-01-08 16:09:57,109: t15.2023.11.19 val PER: 0.0359
2026-01-08 16:09:57,111: t15.2023.11.26 val PER: 0.1094
2026-01-08 16:09:57,113: t15.2023.12.03 val PER: 0.1113
2026-01-08 16:09:57,114: t15.2023.12.08 val PER: 0.0972
2026-01-08 16:09:57,116: t15.2023.12.10 val PER: 0.0933
2026-01-08 16:09:57,117: t15.2023.12.17 val PER: 0.1403
2026-01-08 16:09:57,119: t15.2023.12.29 val PER: 0.1256
2026-01-08 16:09:57,121: t15.2024.02.25 val PER: 0.1039
2026-01-08 16:09:57,122: t15.2024.03.08 val PER: 0.2233
2026-01-08 16:09:57,123: t15.2024.03.15 val PER: 0.1932
2026-01-08 16:09:57,125: t15.2024.03.17 val PER: 0.1416
2026-01-08 16:09:57,126: t15.2024.05.10 val PER: 0.1620
2026-01-08 16:09:57,128: t15.2024.06.14 val PER: 0.1672
2026-01-08 16:09:57,129: t15.2024.07.19 val PER: 0.2353
2026-01-08 16:09:57,131: t15.2024.07.21 val PER: 0.0897
2026-01-08 16:09:57,132: t15.2024.07.28 val PER: 0.1265
2026-01-08 16:09:57,134: t15.2025.01.10 val PER: 0.2879
2026-01-08 16:09:57,135: t15.2025.01.12 val PER: 0.1440
2026-01-08 16:09:57,137: t15.2025.03.14 val PER: 0.3447
2026-01-08 16:09:57,138: t15.2025.03.16 val PER: 0.1819
2026-01-08 16:09:57,140: t15.2025.03.30 val PER: 0.2908
2026-01-08 16:09:57,141: t15.2025.04.13 val PER: 0.2225
2026-01-08 16:09:57,278: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_16500
2026-01-08 16:10:05,336: Train batch 16600: loss: 8.50 grad norm: 53.38 time: 0.053
2026-01-08 16:10:21,680: Train batch 16800: loss: 15.60 grad norm: 72.67 time: 0.062
2026-01-08 16:10:37,918: Train batch 17000: loss: 7.89 grad norm: 47.69 time: 0.081
2026-01-08 16:10:37,921: Running test after training batch: 17000
2026-01-08 16:10:38,019: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:10:42,925: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:10:42,978: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-08 16:10:54,760: Val batch 17000: PER (avg): 0.1470 CTC Loss (avg): 15.0142 WER(5gram): 15.58% (n=256) time: 16.838
2026-01-08 16:10:54,763: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-08 16:10:54,765: t15.2023.08.13 val PER: 0.1112
2026-01-08 16:10:54,766: t15.2023.08.18 val PER: 0.1006
2026-01-08 16:10:54,768: t15.2023.08.20 val PER: 0.1080
2026-01-08 16:10:54,770: t15.2023.08.25 val PER: 0.0949
2026-01-08 16:10:54,771: t15.2023.08.27 val PER: 0.1817
2026-01-08 16:10:54,773: t15.2023.09.01 val PER: 0.0787
2026-01-08 16:10:54,774: t15.2023.09.03 val PER: 0.1520
2026-01-08 16:10:54,776: t15.2023.09.24 val PER: 0.1250
2026-01-08 16:10:54,777: t15.2023.09.29 val PER: 0.1251
2026-01-08 16:10:54,779: t15.2023.10.01 val PER: 0.1717
2026-01-08 16:10:54,780: t15.2023.10.06 val PER: 0.0775
2026-01-08 16:10:54,782: t15.2023.10.08 val PER: 0.2409
2026-01-08 16:10:54,783: t15.2023.10.13 val PER: 0.1901
2026-01-08 16:10:54,785: t15.2023.10.15 val PER: 0.1529
2026-01-08 16:10:54,786: t15.2023.10.20 val PER: 0.1711
2026-01-08 16:10:54,788: t15.2023.10.22 val PER: 0.1069
2026-01-08 16:10:54,790: t15.2023.11.03 val PER: 0.1777
2026-01-08 16:10:54,791: t15.2023.11.04 val PER: 0.0307
2026-01-08 16:10:54,793: t15.2023.11.17 val PER: 0.0389
2026-01-08 16:10:54,794: t15.2023.11.19 val PER: 0.0399
2026-01-08 16:10:54,796: t15.2023.11.26 val PER: 0.1094
2026-01-08 16:10:54,797: t15.2023.12.03 val PER: 0.1071
2026-01-08 16:10:54,799: t15.2023.12.08 val PER: 0.0985
2026-01-08 16:10:54,801: t15.2023.12.10 val PER: 0.0880
2026-01-08 16:10:54,802: t15.2023.12.17 val PER: 0.1403
2026-01-08 16:10:54,804: t15.2023.12.29 val PER: 0.1256
2026-01-08 16:10:54,805: t15.2024.02.25 val PER: 0.1039
2026-01-08 16:10:54,806: t15.2024.03.08 val PER: 0.2248
2026-01-08 16:10:54,808: t15.2024.03.15 val PER: 0.1957
2026-01-08 16:10:54,811: t15.2024.03.17 val PER: 0.1360
2026-01-08 16:10:54,813: t15.2024.05.10 val PER: 0.1649
2026-01-08 16:10:54,815: t15.2024.06.14 val PER: 0.1719
2026-01-08 16:10:54,816: t15.2024.07.19 val PER: 0.2334
2026-01-08 16:10:54,818: t15.2024.07.21 val PER: 0.0855
2026-01-08 16:10:54,819: t15.2024.07.28 val PER: 0.1272
2026-01-08 16:10:54,821: t15.2025.01.10 val PER: 0.2920
2026-01-08 16:10:54,822: t15.2025.01.12 val PER: 0.1332
2026-01-08 16:10:54,824: t15.2025.03.14 val PER: 0.3536
2026-01-08 16:10:54,828: t15.2025.03.16 val PER: 0.1767
2026-01-08 16:10:54,829: t15.2025.03.30 val PER: 0.3023
2026-01-08 16:10:54,831: t15.2025.04.13 val PER: 0.2183
2026-01-08 16:10:54,971: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_17000
2026-01-08 16:11:10,978: Train batch 17200: loss: 9.25 grad norm: 51.63 time: 0.085
2026-01-08 16:11:27,310: Train batch 17400: loss: 11.52 grad norm: 59.55 time: 0.074
2026-01-08 16:11:35,104: Running test after training batch: 17500
2026-01-08 16:11:35,216: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:11:40,390: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:11:40,442: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-08 16:11:52,252: Val batch 17500: PER (avg): 0.1462 CTC Loss (avg): 15.0398 WER(5gram): 15.38% (n=256) time: 17.146
2026-01-08 16:11:52,254: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 16:11:52,257: t15.2023.08.13 val PER: 0.1112
2026-01-08 16:11:52,258: t15.2023.08.18 val PER: 0.0997
2026-01-08 16:11:52,260: t15.2023.08.20 val PER: 0.1096
2026-01-08 16:11:52,261: t15.2023.08.25 val PER: 0.0889
2026-01-08 16:11:52,263: t15.2023.08.27 val PER: 0.1881
2026-01-08 16:11:52,264: t15.2023.09.01 val PER: 0.0722
2026-01-08 16:11:52,266: t15.2023.09.03 val PER: 0.1485
2026-01-08 16:11:52,267: t15.2023.09.24 val PER: 0.1238
2026-01-08 16:11:52,269: t15.2023.09.29 val PER: 0.1238
2026-01-08 16:11:52,270: t15.2023.10.01 val PER: 0.1731
2026-01-08 16:11:52,272: t15.2023.10.06 val PER: 0.0818
2026-01-08 16:11:52,273: t15.2023.10.08 val PER: 0.2422
2026-01-08 16:11:52,274: t15.2023.10.13 val PER: 0.1862
2026-01-08 16:11:52,276: t15.2023.10.15 val PER: 0.1483
2026-01-08 16:11:52,277: t15.2023.10.20 val PER: 0.1745
2026-01-08 16:11:52,279: t15.2023.10.22 val PER: 0.1091
2026-01-08 16:11:52,280: t15.2023.11.03 val PER: 0.1791
2026-01-08 16:11:52,282: t15.2023.11.04 val PER: 0.0375
2026-01-08 16:11:52,283: t15.2023.11.17 val PER: 0.0327
2026-01-08 16:11:52,284: t15.2023.11.19 val PER: 0.0359
2026-01-08 16:11:52,286: t15.2023.11.26 val PER: 0.1123
2026-01-08 16:11:52,287: t15.2023.12.03 val PER: 0.1050
2026-01-08 16:11:52,289: t15.2023.12.08 val PER: 0.1032
2026-01-08 16:11:52,291: t15.2023.12.10 val PER: 0.0880
2026-01-08 16:11:52,292: t15.2023.12.17 val PER: 0.1351
2026-01-08 16:11:52,294: t15.2023.12.29 val PER: 0.1249
2026-01-08 16:11:52,295: t15.2024.02.25 val PER: 0.1011
2026-01-08 16:11:52,297: t15.2024.03.08 val PER: 0.2191
2026-01-08 16:11:52,299: t15.2024.03.15 val PER: 0.1907
2026-01-08 16:11:52,302: t15.2024.03.17 val PER: 0.1416
2026-01-08 16:11:52,304: t15.2024.05.10 val PER: 0.1620
2026-01-08 16:11:52,306: t15.2024.06.14 val PER: 0.1609
2026-01-08 16:11:52,308: t15.2024.07.19 val PER: 0.2287
2026-01-08 16:11:52,309: t15.2024.07.21 val PER: 0.0890
2026-01-08 16:11:52,311: t15.2024.07.28 val PER: 0.1294
2026-01-08 16:11:52,313: t15.2025.01.10 val PER: 0.2961
2026-01-08 16:11:52,314: t15.2025.01.12 val PER: 0.1355
2026-01-08 16:11:52,316: t15.2025.03.14 val PER: 0.3476
2026-01-08 16:11:52,318: t15.2025.03.16 val PER: 0.1767
2026-01-08 16:11:52,319: t15.2025.03.30 val PER: 0.2966
2026-01-08 16:11:52,321: t15.2025.04.13 val PER: 0.2140
2026-01-08 16:11:52,465: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_17500
2026-01-08 16:12:00,321: Train batch 17600: loss: 9.73 grad norm: 55.56 time: 0.051
2026-01-08 16:12:16,694: Train batch 17800: loss: 6.29 grad norm: 49.81 time: 0.042
2026-01-08 16:12:33,796: Train batch 18000: loss: 10.91 grad norm: 64.70 time: 0.062
2026-01-08 16:12:33,799: Running test after training batch: 18000
2026-01-08 16:12:33,921: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:12:39,016: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:12:39,066: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-08 16:12:50,799: Val batch 18000: PER (avg): 0.1455 CTC Loss (avg): 15.0128 WER(5gram): 15.51% (n=256) time: 16.998
2026-01-08 16:12:50,802: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-08 16:12:50,806: t15.2023.08.13 val PER: 0.1102
2026-01-08 16:12:50,808: t15.2023.08.18 val PER: 0.1031
2026-01-08 16:12:50,810: t15.2023.08.20 val PER: 0.1033
2026-01-08 16:12:50,812: t15.2023.08.25 val PER: 0.0889
2026-01-08 16:12:50,813: t15.2023.08.27 val PER: 0.1865
2026-01-08 16:12:50,815: t15.2023.09.01 val PER: 0.0763
2026-01-08 16:12:50,817: t15.2023.09.03 val PER: 0.1473
2026-01-08 16:12:50,819: t15.2023.09.24 val PER: 0.1262
2026-01-08 16:12:50,821: t15.2023.09.29 val PER: 0.1264
2026-01-08 16:12:50,822: t15.2023.10.01 val PER: 0.1717
2026-01-08 16:12:50,824: t15.2023.10.06 val PER: 0.0775
2026-01-08 16:12:50,826: t15.2023.10.08 val PER: 0.2368
2026-01-08 16:12:50,831: t15.2023.10.13 val PER: 0.1877
2026-01-08 16:12:50,833: t15.2023.10.15 val PER: 0.1529
2026-01-08 16:12:50,835: t15.2023.10.20 val PER: 0.1846
2026-01-08 16:12:50,836: t15.2023.10.22 val PER: 0.1013
2026-01-08 16:12:50,838: t15.2023.11.03 val PER: 0.1744
2026-01-08 16:12:50,840: t15.2023.11.04 val PER: 0.0307
2026-01-08 16:12:50,842: t15.2023.11.17 val PER: 0.0358
2026-01-08 16:12:50,843: t15.2023.11.19 val PER: 0.0339
2026-01-08 16:12:50,845: t15.2023.11.26 val PER: 0.1138
2026-01-08 16:12:50,846: t15.2023.12.03 val PER: 0.1061
2026-01-08 16:12:50,848: t15.2023.12.08 val PER: 0.1019
2026-01-08 16:12:50,849: t15.2023.12.10 val PER: 0.0880
2026-01-08 16:12:50,851: t15.2023.12.17 val PER: 0.1351
2026-01-08 16:12:50,853: t15.2023.12.29 val PER: 0.1270
2026-01-08 16:12:50,854: t15.2024.02.25 val PER: 0.0997
2026-01-08 16:12:50,858: t15.2024.03.08 val PER: 0.2205
2026-01-08 16:12:50,859: t15.2024.03.15 val PER: 0.1951
2026-01-08 16:12:50,861: t15.2024.03.17 val PER: 0.1367
2026-01-08 16:12:50,863: t15.2024.05.10 val PER: 0.1575
2026-01-08 16:12:50,864: t15.2024.06.14 val PER: 0.1656
2026-01-08 16:12:50,866: t15.2024.07.19 val PER: 0.2294
2026-01-08 16:12:50,867: t15.2024.07.21 val PER: 0.0869
2026-01-08 16:12:50,869: t15.2024.07.28 val PER: 0.1316
2026-01-08 16:12:50,871: t15.2025.01.10 val PER: 0.2824
2026-01-08 16:12:50,872: t15.2025.01.12 val PER: 0.1386
2026-01-08 16:12:50,874: t15.2025.03.14 val PER: 0.3328
2026-01-08 16:12:50,875: t15.2025.03.16 val PER: 0.1728
2026-01-08 16:12:50,877: t15.2025.03.30 val PER: 0.2920
2026-01-08 16:12:50,878: t15.2025.04.13 val PER: 0.2126
2026-01-08 16:12:51,016: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_18000
2026-01-08 16:13:08,073: Train batch 18200: loss: 7.45 grad norm: 46.71 time: 0.073
2026-01-08 16:13:23,910: Train batch 18400: loss: 4.41 grad norm: 43.57 time: 0.058
2026-01-08 16:13:32,318: Running test after training batch: 18500
2026-01-08 16:13:32,444: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:13:37,630: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:13:37,681: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-08 16:13:49,595: Val batch 18500: PER (avg): 0.1458 CTC Loss (avg): 14.9858 WER(5gram): 15.38% (n=256) time: 17.275
2026-01-08 16:13:49,598: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-08 16:13:49,601: t15.2023.08.13 val PER: 0.1081
2026-01-08 16:13:49,603: t15.2023.08.18 val PER: 0.1006
2026-01-08 16:13:49,605: t15.2023.08.20 val PER: 0.1033
2026-01-08 16:13:49,607: t15.2023.08.25 val PER: 0.0873
2026-01-08 16:13:49,608: t15.2023.08.27 val PER: 0.1865
2026-01-08 16:13:49,610: t15.2023.09.01 val PER: 0.0763
2026-01-08 16:13:49,612: t15.2023.09.03 val PER: 0.1449
2026-01-08 16:13:49,614: t15.2023.09.24 val PER: 0.1274
2026-01-08 16:13:49,615: t15.2023.09.29 val PER: 0.1232
2026-01-08 16:13:49,617: t15.2023.10.01 val PER: 0.1684
2026-01-08 16:13:49,619: t15.2023.10.06 val PER: 0.0840
2026-01-08 16:13:49,621: t15.2023.10.08 val PER: 0.2368
2026-01-08 16:13:49,623: t15.2023.10.13 val PER: 0.1916
2026-01-08 16:13:49,625: t15.2023.10.15 val PER: 0.1608
2026-01-08 16:13:49,627: t15.2023.10.20 val PER: 0.1812
2026-01-08 16:13:49,629: t15.2023.10.22 val PER: 0.1114
2026-01-08 16:13:49,630: t15.2023.11.03 val PER: 0.1777
2026-01-08 16:13:49,632: t15.2023.11.04 val PER: 0.0307
2026-01-08 16:13:49,634: t15.2023.11.17 val PER: 0.0358
2026-01-08 16:13:49,635: t15.2023.11.19 val PER: 0.0339
2026-01-08 16:13:49,637: t15.2023.11.26 val PER: 0.1138
2026-01-08 16:13:49,639: t15.2023.12.03 val PER: 0.1008
2026-01-08 16:13:49,640: t15.2023.12.08 val PER: 0.0999
2026-01-08 16:13:49,642: t15.2023.12.10 val PER: 0.0880
2026-01-08 16:13:49,644: t15.2023.12.17 val PER: 0.1341
2026-01-08 16:13:49,645: t15.2023.12.29 val PER: 0.1249
2026-01-08 16:13:49,647: t15.2024.02.25 val PER: 0.0983
2026-01-08 16:13:49,648: t15.2024.03.08 val PER: 0.2248
2026-01-08 16:13:49,650: t15.2024.03.15 val PER: 0.1907
2026-01-08 16:13:49,652: t15.2024.03.17 val PER: 0.1353
2026-01-08 16:13:49,653: t15.2024.05.10 val PER: 0.1590
2026-01-08 16:13:49,655: t15.2024.06.14 val PER: 0.1703
2026-01-08 16:13:49,657: t15.2024.07.19 val PER: 0.2334
2026-01-08 16:13:49,658: t15.2024.07.21 val PER: 0.0834
2026-01-08 16:13:49,660: t15.2024.07.28 val PER: 0.1272
2026-01-08 16:13:49,662: t15.2025.01.10 val PER: 0.2837
2026-01-08 16:13:49,663: t15.2025.01.12 val PER: 0.1355
2026-01-08 16:13:49,666: t15.2025.03.14 val PER: 0.3521
2026-01-08 16:13:49,668: t15.2025.03.16 val PER: 0.1780
2026-01-08 16:13:49,669: t15.2025.03.30 val PER: 0.2885
2026-01-08 16:13:49,671: t15.2025.04.13 val PER: 0.2154
2026-01-08 16:13:49,808: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_18500
2026-01-08 16:13:57,798: Train batch 18600: loss: 12.03 grad norm: 57.70 time: 0.068
2026-01-08 16:14:14,142: Train batch 18800: loss: 7.84 grad norm: 51.64 time: 0.065
2026-01-08 16:14:30,890: Train batch 19000: loss: 7.96 grad norm: 43.60 time: 0.065
2026-01-08 16:14:30,892: Running test after training batch: 19000
2026-01-08 16:14:31,001: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:14:35,915: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:14:35,967: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-08 16:14:47,650: Val batch 19000: PER (avg): 0.1453 CTC Loss (avg): 15.0147 WER(5gram): 15.38% (n=256) time: 16.755
2026-01-08 16:14:47,652: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-08 16:14:47,655: t15.2023.08.13 val PER: 0.1060
2026-01-08 16:14:47,657: t15.2023.08.18 val PER: 0.0997
2026-01-08 16:14:47,658: t15.2023.08.20 val PER: 0.1056
2026-01-08 16:14:47,660: t15.2023.08.25 val PER: 0.0873
2026-01-08 16:14:47,662: t15.2023.08.27 val PER: 0.1833
2026-01-08 16:14:47,663: t15.2023.09.01 val PER: 0.0739
2026-01-08 16:14:47,665: t15.2023.09.03 val PER: 0.1473
2026-01-08 16:14:47,666: t15.2023.09.24 val PER: 0.1250
2026-01-08 16:14:47,668: t15.2023.09.29 val PER: 0.1213
2026-01-08 16:14:47,669: t15.2023.10.01 val PER: 0.1684
2026-01-08 16:14:47,671: t15.2023.10.06 val PER: 0.0797
2026-01-08 16:14:47,672: t15.2023.10.08 val PER: 0.2341
2026-01-08 16:14:47,674: t15.2023.10.13 val PER: 0.1924
2026-01-08 16:14:47,676: t15.2023.10.15 val PER: 0.1523
2026-01-08 16:14:47,677: t15.2023.10.20 val PER: 0.1846
2026-01-08 16:14:47,679: t15.2023.10.22 val PER: 0.1047
2026-01-08 16:14:47,681: t15.2023.11.03 val PER: 0.1791
2026-01-08 16:14:47,682: t15.2023.11.04 val PER: 0.0307
2026-01-08 16:14:47,685: t15.2023.11.17 val PER: 0.0373
2026-01-08 16:14:47,687: t15.2023.11.19 val PER: 0.0319
2026-01-08 16:14:47,688: t15.2023.11.26 val PER: 0.1094
2026-01-08 16:14:47,690: t15.2023.12.03 val PER: 0.1061
2026-01-08 16:14:47,692: t15.2023.12.08 val PER: 0.0985
2026-01-08 16:14:47,693: t15.2023.12.10 val PER: 0.0894
2026-01-08 16:14:47,695: t15.2023.12.17 val PER: 0.1351
2026-01-08 16:14:47,697: t15.2023.12.29 val PER: 0.1201
2026-01-08 16:14:47,698: t15.2024.02.25 val PER: 0.1039
2026-01-08 16:14:47,700: t15.2024.03.08 val PER: 0.2233
2026-01-08 16:14:47,702: t15.2024.03.15 val PER: 0.1914
2026-01-08 16:14:47,703: t15.2024.03.17 val PER: 0.1374
2026-01-08 16:14:47,705: t15.2024.05.10 val PER: 0.1501
2026-01-08 16:14:47,707: t15.2024.06.14 val PER: 0.1656
2026-01-08 16:14:47,708: t15.2024.07.19 val PER: 0.2327
2026-01-08 16:14:47,711: t15.2024.07.21 val PER: 0.0876
2026-01-08 16:14:47,714: t15.2024.07.28 val PER: 0.1279
2026-01-08 16:14:47,716: t15.2025.01.10 val PER: 0.2948
2026-01-08 16:14:47,718: t15.2025.01.12 val PER: 0.1378
2026-01-08 16:14:47,719: t15.2025.03.14 val PER: 0.3595
2026-01-08 16:14:47,721: t15.2025.03.16 val PER: 0.1767
2026-01-08 16:14:47,723: t15.2025.03.30 val PER: 0.2862
2026-01-08 16:14:47,724: t15.2025.04.13 val PER: 0.2168
2026-01-08 16:14:47,877: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_19000
2026-01-08 16:15:04,382: Train batch 19200: loss: 5.77 grad norm: 45.23 time: 0.064
2026-01-08 16:15:21,347: Train batch 19400: loss: 4.67 grad norm: 37.02 time: 0.054
2026-01-08 16:15:30,007: Running test after training batch: 19500
2026-01-08 16:15:30,109: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:15:35,011: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:15:35,070: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-08 16:15:47,022: Val batch 19500: PER (avg): 0.1451 CTC Loss (avg): 14.9945 WER(5gram): 15.38% (n=256) time: 17.013
2026-01-08 16:15:47,024: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 16:15:47,027: t15.2023.08.13 val PER: 0.1050
2026-01-08 16:15:47,029: t15.2023.08.18 val PER: 0.0981
2026-01-08 16:15:47,031: t15.2023.08.20 val PER: 0.1072
2026-01-08 16:15:47,032: t15.2023.08.25 val PER: 0.0873
2026-01-08 16:15:47,034: t15.2023.08.27 val PER: 0.1817
2026-01-08 16:15:47,036: t15.2023.09.01 val PER: 0.0787
2026-01-08 16:15:47,037: t15.2023.09.03 val PER: 0.1449
2026-01-08 16:15:47,039: t15.2023.09.24 val PER: 0.1262
2026-01-08 16:15:47,041: t15.2023.09.29 val PER: 0.1232
2026-01-08 16:15:47,042: t15.2023.10.01 val PER: 0.1717
2026-01-08 16:15:47,044: t15.2023.10.06 val PER: 0.0797
2026-01-08 16:15:47,046: t15.2023.10.08 val PER: 0.2368
2026-01-08 16:15:47,047: t15.2023.10.13 val PER: 0.1901
2026-01-08 16:15:47,049: t15.2023.10.15 val PER: 0.1549
2026-01-08 16:15:47,051: t15.2023.10.20 val PER: 0.1913
2026-01-08 16:15:47,052: t15.2023.10.22 val PER: 0.1102
2026-01-08 16:15:47,054: t15.2023.11.03 val PER: 0.1764
2026-01-08 16:15:47,055: t15.2023.11.04 val PER: 0.0375
2026-01-08 16:15:47,057: t15.2023.11.17 val PER: 0.0327
2026-01-08 16:15:47,058: t15.2023.11.19 val PER: 0.0399
2026-01-08 16:15:47,060: t15.2023.11.26 val PER: 0.1101
2026-01-08 16:15:47,062: t15.2023.12.03 val PER: 0.1008
2026-01-08 16:15:47,063: t15.2023.12.08 val PER: 0.0952
2026-01-08 16:15:47,065: t15.2023.12.10 val PER: 0.0894
2026-01-08 16:15:47,066: t15.2023.12.17 val PER: 0.1341
2026-01-08 16:15:47,068: t15.2023.12.29 val PER: 0.1174
2026-01-08 16:15:47,069: t15.2024.02.25 val PER: 0.1039
2026-01-08 16:15:47,070: t15.2024.03.08 val PER: 0.2205
2026-01-08 16:15:47,072: t15.2024.03.15 val PER: 0.1907
2026-01-08 16:15:47,073: t15.2024.03.17 val PER: 0.1332
2026-01-08 16:15:47,075: t15.2024.05.10 val PER: 0.1516
2026-01-08 16:15:47,076: t15.2024.06.14 val PER: 0.1609
2026-01-08 16:15:47,078: t15.2024.07.19 val PER: 0.2367
2026-01-08 16:15:47,079: t15.2024.07.21 val PER: 0.0897
2026-01-08 16:15:47,080: t15.2024.07.28 val PER: 0.1301
2026-01-08 16:15:47,082: t15.2025.01.10 val PER: 0.2879
2026-01-08 16:15:47,083: t15.2025.01.12 val PER: 0.1355
2026-01-08 16:15:47,084: t15.2025.03.14 val PER: 0.3506
2026-01-08 16:15:47,086: t15.2025.03.16 val PER: 0.1780
2026-01-08 16:15:47,087: t15.2025.03.30 val PER: 0.2851
2026-01-08 16:15:47,089: t15.2025.04.13 val PER: 0.2154
2026-01-08 16:15:47,266: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_19500
2026-01-08 16:15:55,459: Train batch 19600: loss: 7.89 grad norm: 49.43 time: 0.057
2026-01-08 16:16:11,848: Train batch 19800: loss: 7.24 grad norm: 47.33 time: 0.055
2026-01-08 16:16:28,095: Running test after training batch: 19999
2026-01-08 16:16:28,196: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:16:33,033: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 16:16:33,095: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-08 16:16:45,106: Val batch 19999: PER (avg): 0.1449 CTC Loss (avg): 14.9843 WER(5gram): 15.45% (n=256) time: 17.009
2026-01-08 16:16:45,108: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-08 16:16:45,111: t15.2023.08.13 val PER: 0.1029
2026-01-08 16:16:45,112: t15.2023.08.18 val PER: 0.0989
2026-01-08 16:16:45,114: t15.2023.08.20 val PER: 0.1144
2026-01-08 16:16:45,116: t15.2023.08.25 val PER: 0.0889
2026-01-08 16:16:45,118: t15.2023.08.27 val PER: 0.1833
2026-01-08 16:16:45,120: t15.2023.09.01 val PER: 0.0747
2026-01-08 16:16:45,122: t15.2023.09.03 val PER: 0.1461
2026-01-08 16:16:45,123: t15.2023.09.24 val PER: 0.1286
2026-01-08 16:16:45,125: t15.2023.09.29 val PER: 0.1264
2026-01-08 16:16:45,127: t15.2023.10.01 val PER: 0.1651
2026-01-08 16:16:45,129: t15.2023.10.06 val PER: 0.0753
2026-01-08 16:16:45,131: t15.2023.10.08 val PER: 0.2355
2026-01-08 16:16:45,133: t15.2023.10.13 val PER: 0.1932
2026-01-08 16:16:45,136: t15.2023.10.15 val PER: 0.1569
2026-01-08 16:16:45,138: t15.2023.10.20 val PER: 0.1846
2026-01-08 16:16:45,140: t15.2023.10.22 val PER: 0.1091
2026-01-08 16:16:45,142: t15.2023.11.03 val PER: 0.1771
2026-01-08 16:16:45,143: t15.2023.11.04 val PER: 0.0341
2026-01-08 16:16:45,145: t15.2023.11.17 val PER: 0.0373
2026-01-08 16:16:45,147: t15.2023.11.19 val PER: 0.0379
2026-01-08 16:16:45,149: t15.2023.11.26 val PER: 0.1087
2026-01-08 16:16:45,150: t15.2023.12.03 val PER: 0.1071
2026-01-08 16:16:45,152: t15.2023.12.08 val PER: 0.0925
2026-01-08 16:16:45,154: t15.2023.12.10 val PER: 0.0880
2026-01-08 16:16:45,155: t15.2023.12.17 val PER: 0.1310
2026-01-08 16:16:45,157: t15.2023.12.29 val PER: 0.1222
2026-01-08 16:16:45,159: t15.2024.02.25 val PER: 0.1025
2026-01-08 16:16:45,161: t15.2024.03.08 val PER: 0.2176
2026-01-08 16:16:45,163: t15.2024.03.15 val PER: 0.1851
2026-01-08 16:16:45,164: t15.2024.03.17 val PER: 0.1339
2026-01-08 16:16:45,166: t15.2024.05.10 val PER: 0.1516
2026-01-08 16:16:45,168: t15.2024.06.14 val PER: 0.1672
2026-01-08 16:16:45,173: t15.2024.07.19 val PER: 0.2301
2026-01-08 16:16:45,175: t15.2024.07.21 val PER: 0.0890
2026-01-08 16:16:45,177: t15.2024.07.28 val PER: 0.1301
2026-01-08 16:16:45,178: t15.2025.01.10 val PER: 0.2906
2026-01-08 16:16:45,180: t15.2025.01.12 val PER: 0.1339
2026-01-08 16:16:45,182: t15.2025.03.14 val PER: 0.3506
2026-01-08 16:16:45,183: t15.2025.03.16 val PER: 0.1715
2026-01-08 16:16:45,185: t15.2025.03.30 val PER: 0.2920
2026-01-08 16:16:45,187: t15.2025.04.13 val PER: 0.2154
2026-01-08 16:16:45,329: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/baseline/checkpoint/checkpoint_batch_19999
2026-01-08 16:16:45,816: Best avg val PER achieved: 0.14877
2026-01-08 16:16:45,818: Total training time: 44.04 minutes

=== RUN head_ln_2blocks.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks
2026-01-08 16:18:41,287: Using device: cuda:0
2026-01-08 16:22:30,635: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-08 16:22:30,660: Using 45 sessions after filtering (from 45).
2026-01-08 16:22:31,104: Using torch.compile (if available)
2026-01-08 16:22:31,105: torch.compile not available (torch<2.0). Skipping.
2026-01-08 16:22:31,105: Initialized RNN decoding model
2026-01-08 16:22:31,105: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
    (1): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-08 16:22:31,105: Model has 45,499,433 parameters
2026-01-08 16:22:31,105: Model has 11,819,520 day-specific parameters | 25.98% of total parameters
2026-01-08 16:22:32,366: Successfully initialized datasets
2026-01-08 16:22:32,369: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-08 16:22:33,905: Train batch 0: loss: 600.34 grad norm: 2334.58 time: 0.181
2026-01-08 16:22:33,907: Running test after training batch: 0
2026-01-08 16:22:34,021: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:22:39,791: WER debug example
  GT : you can see the code at this point as well
  PR : xiao huang lao lao who had
2026-01-08 16:22:40,171: WER debug example
  GT : how does it keep the cost down
  PR : while hillbilly hillbilly
2026-01-08 16:24:10,026: Val batch 0: PER (avg): 1.3381 CTC Loss (avg): 626.6909 WER(5gram): 99.93% (n=256) time: 96.118
2026-01-08 16:24:10,030: WER lens: avg_true_words=5.99 avg_pred_words=2.42 max_pred_words=7
2026-01-08 16:24:10,030: t15.2023.08.13 val PER: 1.2037
2026-01-08 16:24:10,030: t15.2023.08.18 val PER: 1.2833
2026-01-08 16:24:10,031: t15.2023.08.20 val PER: 1.2740
2026-01-08 16:24:10,031: t15.2023.08.25 val PER: 1.3404
2026-01-08 16:24:10,031: t15.2023.08.27 val PER: 1.2058
2026-01-08 16:24:10,031: t15.2023.09.01 val PER: 1.3369
2026-01-08 16:24:10,031: t15.2023.09.03 val PER: 1.1746
2026-01-08 16:24:10,031: t15.2023.09.24 val PER: 1.4430
2026-01-08 16:24:10,031: t15.2023.09.29 val PER: 1.4773
2026-01-08 16:24:10,031: t15.2023.10.01 val PER: 1.1684
2026-01-08 16:24:10,032: t15.2023.10.06 val PER: 1.4273
2026-01-08 16:24:10,032: t15.2023.10.08 val PER: 1.1191
2026-01-08 16:24:10,032: t15.2023.10.13 val PER: 1.3569
2026-01-08 16:24:10,032: t15.2023.10.15 val PER: 1.3731
2026-01-08 16:24:10,032: t15.2023.10.20 val PER: 1.4866
2026-01-08 16:24:10,032: t15.2023.10.22 val PER: 1.3886
2026-01-08 16:24:10,032: t15.2023.11.03 val PER: 1.5631
2026-01-08 16:24:10,032: t15.2023.11.04 val PER: 1.5495
2026-01-08 16:24:10,032: t15.2023.11.17 val PER: 1.8398
2026-01-08 16:24:10,032: t15.2023.11.19 val PER: 1.5689
2026-01-08 16:24:10,033: t15.2023.11.26 val PER: 1.4188
2026-01-08 16:24:10,033: t15.2023.12.03 val PER: 1.3351
2026-01-08 16:24:10,033: t15.2023.12.08 val PER: 1.4068
2026-01-08 16:24:10,033: t15.2023.12.10 val PER: 1.4534
2026-01-08 16:24:10,033: t15.2023.12.17 val PER: 1.2391
2026-01-08 16:24:10,033: t15.2023.12.29 val PER: 1.2677
2026-01-08 16:24:10,033: t15.2024.02.25 val PER: 1.3174
2026-01-08 16:24:10,033: t15.2024.03.08 val PER: 1.2176
2026-01-08 16:24:10,033: t15.2024.03.15 val PER: 1.2745
2026-01-08 16:24:10,034: t15.2024.03.17 val PER: 1.3368
2026-01-08 16:24:10,034: t15.2024.05.10 val PER: 1.3744
2026-01-08 16:24:10,034: t15.2024.06.14 val PER: 1.4259
2026-01-08 16:24:10,034: t15.2024.07.19 val PER: 1.0000
2026-01-08 16:24:10,034: t15.2024.07.21 val PER: 1.4524
2026-01-08 16:24:10,034: t15.2024.07.28 val PER: 1.4728
2026-01-08 16:24:10,034: t15.2025.01.10 val PER: 0.9848
2026-01-08 16:24:10,034: t15.2025.01.12 val PER: 1.5304
2026-01-08 16:24:10,034: t15.2025.03.14 val PER: 1.0222
2026-01-08 16:24:10,035: t15.2025.03.16 val PER: 1.4476
2026-01-08 16:24:10,035: t15.2025.03.30 val PER: 1.2241
2026-01-08 16:24:10,035: t15.2025.04.13 val PER: 1.2625
2026-01-08 16:24:10,035: New best val WER(5gram) inf% --> 99.93%
2026-01-08 16:24:10,208: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_0
2026-01-08 16:24:27,707: Train batch 200: loss: 93.78 grad norm: 194.43 time: 0.055
2026-01-08 16:24:44,517: Train batch 400: loss: 63.75 grad norm: 92.17 time: 0.064
2026-01-08 16:24:53,127: Running test after training batch: 500
2026-01-08 16:24:53,251: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:24:58,532: WER debug example
  GT : you can see the code at this point as well
  PR : he had teased this handpicked this product is
2026-01-08 16:24:58,902: WER debug example
  GT : how does it keep the cost down
  PR : it had tested until the sides
2026-01-08 16:26:31,445: Val batch 500: PER (avg): 0.6194 CTC Loss (avg): 67.3444 WER(5gram): 97.13% (n=256) time: 98.316
2026-01-08 16:26:31,450: WER lens: avg_true_words=5.99 avg_pred_words=5.27 max_pred_words=13
2026-01-08 16:26:31,451: t15.2023.08.13 val PER: 0.5821
2026-01-08 16:26:31,451: t15.2023.08.18 val PER: 0.5599
2026-01-08 16:26:31,452: t15.2023.08.20 val PER: 0.5504
2026-01-08 16:26:31,452: t15.2023.08.25 val PER: 0.5542
2026-01-08 16:26:31,452: t15.2023.08.27 val PER: 0.6029
2026-01-08 16:26:31,452: t15.2023.09.01 val PER: 0.5471
2026-01-08 16:26:31,452: t15.2023.09.03 val PER: 0.6093
2026-01-08 16:26:31,452: t15.2023.09.24 val PER: 0.5510
2026-01-08 16:26:31,452: t15.2023.09.29 val PER: 0.5673
2026-01-08 16:26:31,452: t15.2023.10.01 val PER: 0.6196
2026-01-08 16:26:31,453: t15.2023.10.06 val PER: 0.5748
2026-01-08 16:26:31,453: t15.2023.10.08 val PER: 0.6035
2026-01-08 16:26:31,453: t15.2023.10.13 val PER: 0.6649
2026-01-08 16:26:31,453: t15.2023.10.15 val PER: 0.6058
2026-01-08 16:26:31,453: t15.2023.10.20 val PER: 0.5537
2026-01-08 16:26:31,453: t15.2023.10.22 val PER: 0.5969
2026-01-08 16:26:31,453: t15.2023.11.03 val PER: 0.6608
2026-01-08 16:26:31,453: t15.2023.11.04 val PER: 0.5085
2026-01-08 16:26:31,453: t15.2023.11.17 val PER: 0.5319
2026-01-08 16:26:31,453: t15.2023.11.19 val PER: 0.4611
2026-01-08 16:26:31,453: t15.2023.11.26 val PER: 0.6196
2026-01-08 16:26:31,453: t15.2023.12.03 val PER: 0.6155
2026-01-08 16:26:31,453: t15.2023.12.08 val PER: 0.5999
2026-01-08 16:26:31,453: t15.2023.12.10 val PER: 0.5742
2026-01-08 16:26:31,453: t15.2023.12.17 val PER: 0.6237
2026-01-08 16:26:31,454: t15.2023.12.29 val PER: 0.5971
2026-01-08 16:26:31,454: t15.2024.02.25 val PER: 0.5913
2026-01-08 16:26:31,454: t15.2024.03.08 val PER: 0.6700
2026-01-08 16:26:31,454: t15.2024.03.15 val PER: 0.6535
2026-01-08 16:26:31,454: t15.2024.03.17 val PER: 0.5976
2026-01-08 16:26:31,454: t15.2024.05.10 val PER: 0.6226
2026-01-08 16:26:31,454: t15.2024.06.14 val PER: 0.6735
2026-01-08 16:26:31,454: t15.2024.07.19 val PER: 0.7165
2026-01-08 16:26:31,454: t15.2024.07.21 val PER: 0.6048
2026-01-08 16:26:31,454: t15.2024.07.28 val PER: 0.6574
2026-01-08 16:26:31,454: t15.2025.01.10 val PER: 0.7865
2026-01-08 16:26:31,454: t15.2025.01.12 val PER: 0.6690
2026-01-08 16:26:31,454: t15.2025.03.14 val PER: 0.7456
2026-01-08 16:26:31,454: t15.2025.03.16 val PER: 0.6911
2026-01-08 16:26:31,455: t15.2025.03.30 val PER: 0.7770
2026-01-08 16:26:31,455: t15.2025.04.13 val PER: 0.6519
2026-01-08 16:26:31,456: New best val WER(5gram) 99.93% --> 97.13%
2026-01-08 16:26:31,630: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_500
2026-01-08 16:26:40,286: Train batch 600: loss: 59.45 grad norm: 97.99 time: 0.081
2026-01-08 16:26:57,436: Train batch 800: loss: 51.16 grad norm: 101.01 time: 0.058
2026-01-08 16:27:14,532: Train batch 1000: loss: 50.04 grad norm: 95.56 time: 0.068
2026-01-08 16:27:14,534: Running test after training batch: 1000
2026-01-08 16:27:14,648: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:27:19,860: WER debug example
  GT : you can see the code at this point as well
  PR : hudak and sees the truck advocates ponds
2026-01-08 16:27:20,112: WER debug example
  GT : how does it keep the cost down
  PR : diehards hit six thus costs
2026-01-08 16:28:18,566: Val batch 1000: PER (avg): 0.4763 CTC Loss (avg): 51.1230 WER(5gram): 99.61% (n=256) time: 64.029
2026-01-08 16:28:18,568: WER lens: avg_true_words=5.99 avg_pred_words=5.75 max_pred_words=12
2026-01-08 16:28:18,571: t15.2023.08.13 val PER: 0.4459
2026-01-08 16:28:18,573: t15.2023.08.18 val PER: 0.4275
2026-01-08 16:28:18,575: t15.2023.08.20 val PER: 0.4186
2026-01-08 16:28:18,577: t15.2023.08.25 val PER: 0.3840
2026-01-08 16:28:18,579: t15.2023.08.27 val PER: 0.5032
2026-01-08 16:28:18,582: t15.2023.09.01 val PER: 0.4002
2026-01-08 16:28:18,584: t15.2023.09.03 val PER: 0.4964
2026-01-08 16:28:18,586: t15.2023.09.24 val PER: 0.4102
2026-01-08 16:28:18,588: t15.2023.09.29 val PER: 0.4308
2026-01-08 16:28:18,590: t15.2023.10.01 val PER: 0.4643
2026-01-08 16:28:18,592: t15.2023.10.06 val PER: 0.4037
2026-01-08 16:28:18,594: t15.2023.10.08 val PER: 0.5169
2026-01-08 16:28:18,596: t15.2023.10.13 val PER: 0.5423
2026-01-08 16:28:18,598: t15.2023.10.15 val PER: 0.4450
2026-01-08 16:28:18,600: t15.2023.10.20 val PER: 0.4329
2026-01-08 16:28:18,602: t15.2023.10.22 val PER: 0.4220
2026-01-08 16:28:18,604: t15.2023.11.03 val PER: 0.4505
2026-01-08 16:28:18,606: t15.2023.11.04 val PER: 0.2253
2026-01-08 16:28:18,608: t15.2023.11.17 val PER: 0.3266
2026-01-08 16:28:18,609: t15.2023.11.19 val PER: 0.2994
2026-01-08 16:28:18,611: t15.2023.11.26 val PER: 0.5116
2026-01-08 16:28:18,613: t15.2023.12.03 val PER: 0.4643
2026-01-08 16:28:18,615: t15.2023.12.08 val PER: 0.4627
2026-01-08 16:28:18,617: t15.2023.12.10 val PER: 0.4100
2026-01-08 16:28:18,619: t15.2023.12.17 val PER: 0.4813
2026-01-08 16:28:18,621: t15.2023.12.29 val PER: 0.4749
2026-01-08 16:28:18,623: t15.2024.02.25 val PER: 0.4003
2026-01-08 16:28:18,625: t15.2024.03.08 val PER: 0.5690
2026-01-08 16:28:18,627: t15.2024.03.15 val PER: 0.4972
2026-01-08 16:28:18,629: t15.2024.03.17 val PER: 0.4784
2026-01-08 16:28:18,631: t15.2024.05.10 val PER: 0.4829
2026-01-08 16:28:18,633: t15.2024.06.14 val PER: 0.4763
2026-01-08 16:28:18,635: t15.2024.07.19 val PER: 0.6196
2026-01-08 16:28:18,637: t15.2024.07.21 val PER: 0.4338
2026-01-08 16:28:18,640: t15.2024.07.28 val PER: 0.4574
2026-01-08 16:28:18,642: t15.2025.01.10 val PER: 0.6956
2026-01-08 16:28:18,644: t15.2025.01.12 val PER: 0.5273
2026-01-08 16:28:18,646: t15.2025.03.14 val PER: 0.6982
2026-01-08 16:28:18,648: t15.2025.03.16 val PER: 0.5026
2026-01-08 16:28:18,650: t15.2025.03.30 val PER: 0.7034
2026-01-08 16:28:18,651: t15.2025.04.13 val PER: 0.5335
2026-01-08 16:28:18,786: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_1000
2026-01-08 16:28:35,772: Train batch 1200: loss: 43.72 grad norm: 90.01 time: 0.070
2026-01-08 16:28:52,809: Train batch 1400: loss: 42.97 grad norm: 86.66 time: 0.062
2026-01-08 16:29:01,529: Running test after training batch: 1500
2026-01-08 16:29:01,628: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:29:06,932: WER debug example
  GT : you can see the code at this point as well
  PR : hughley candy seized the truck had hit this point is files
2026-01-08 16:29:07,203: WER debug example
  GT : how does it keep the cost down
  PR : it holds posey hit kick that sends
2026-01-08 16:29:48,360: Val batch 1500: PER (avg): 0.4234 CTC Loss (avg): 44.6083 WER(5gram): 96.94% (n=256) time: 46.829
2026-01-08 16:29:48,363: WER lens: avg_true_words=5.99 avg_pred_words=6.03 max_pred_words=12
2026-01-08 16:29:48,365: t15.2023.08.13 val PER: 0.4106
2026-01-08 16:29:48,367: t15.2023.08.18 val PER: 0.3755
2026-01-08 16:29:48,369: t15.2023.08.20 val PER: 0.3749
2026-01-08 16:29:48,371: t15.2023.08.25 val PER: 0.3313
2026-01-08 16:29:48,372: t15.2023.08.27 val PER: 0.4582
2026-01-08 16:29:48,374: t15.2023.09.01 val PER: 0.3604
2026-01-08 16:29:48,376: t15.2023.09.03 val PER: 0.4418
2026-01-08 16:29:48,377: t15.2023.09.24 val PER: 0.3459
2026-01-08 16:29:48,379: t15.2023.09.29 val PER: 0.3676
2026-01-08 16:29:48,381: t15.2023.10.01 val PER: 0.4221
2026-01-08 16:29:48,383: t15.2023.10.06 val PER: 0.3466
2026-01-08 16:29:48,384: t15.2023.10.08 val PER: 0.4736
2026-01-08 16:29:48,386: t15.2023.10.13 val PER: 0.4756
2026-01-08 16:29:48,387: t15.2023.10.15 val PER: 0.3876
2026-01-08 16:29:48,389: t15.2023.10.20 val PER: 0.4161
2026-01-08 16:29:48,390: t15.2023.10.22 val PER: 0.3764
2026-01-08 16:29:48,392: t15.2023.11.03 val PER: 0.4261
2026-01-08 16:29:48,393: t15.2023.11.04 val PER: 0.1809
2026-01-08 16:29:48,395: t15.2023.11.17 val PER: 0.2675
2026-01-08 16:29:48,396: t15.2023.11.19 val PER: 0.2475
2026-01-08 16:29:48,398: t15.2023.11.26 val PER: 0.4659
2026-01-08 16:29:48,399: t15.2023.12.03 val PER: 0.4076
2026-01-08 16:29:48,401: t15.2023.12.08 val PER: 0.3988
2026-01-08 16:29:48,402: t15.2023.12.10 val PER: 0.3614
2026-01-08 16:29:48,404: t15.2023.12.17 val PER: 0.4179
2026-01-08 16:29:48,405: t15.2023.12.29 val PER: 0.4173
2026-01-08 16:29:48,407: t15.2024.02.25 val PER: 0.3624
2026-01-08 16:29:48,408: t15.2024.03.08 val PER: 0.4723
2026-01-08 16:29:48,410: t15.2024.03.15 val PER: 0.4340
2026-01-08 16:29:48,411: t15.2024.03.17 val PER: 0.4275
2026-01-08 16:29:48,414: t15.2024.05.10 val PER: 0.4071
2026-01-08 16:29:48,416: t15.2024.06.14 val PER: 0.4227
2026-01-08 16:29:48,417: t15.2024.07.19 val PER: 0.5597
2026-01-08 16:29:48,419: t15.2024.07.21 val PER: 0.3690
2026-01-08 16:29:48,420: t15.2024.07.28 val PER: 0.4037
2026-01-08 16:29:48,423: t15.2025.01.10 val PER: 0.6419
2026-01-08 16:29:48,424: t15.2025.01.12 val PER: 0.4611
2026-01-08 16:29:48,425: t15.2025.03.14 val PER: 0.6479
2026-01-08 16:29:48,427: t15.2025.03.16 val PER: 0.4647
2026-01-08 16:29:48,428: t15.2025.03.30 val PER: 0.6655
2026-01-08 16:29:48,430: t15.2025.04.13 val PER: 0.5036
2026-01-08 16:29:48,431: New best val WER(5gram) 97.13% --> 96.94%
2026-01-08 16:29:48,614: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_1500
2026-01-08 16:29:57,014: Train batch 1600: loss: 41.39 grad norm: 77.22 time: 0.066
2026-01-08 16:30:14,292: Train batch 1800: loss: 40.79 grad norm: 77.67 time: 0.091
2026-01-08 16:30:31,723: Train batch 2000: loss: 38.85 grad norm: 81.18 time: 0.068
2026-01-08 16:30:31,725: Running test after training batch: 2000
2026-01-08 16:30:31,849: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:30:36,902: WER debug example
  GT : you can see the code at this point as well
  PR : hudy candy sweetness that ahead hotspot has swells
2026-01-08 16:30:37,074: WER debug example
  GT : how does it keep the cost down
  PR : singh had city seeks that sends
2026-01-08 16:31:12,901: Val batch 2000: PER (avg): 0.3879 CTC Loss (avg): 40.7816 WER(5gram): 104.89% (n=256) time: 41.174
2026-01-08 16:31:12,903: WER lens: avg_true_words=5.99 avg_pred_words=6.48 max_pred_words=14
2026-01-08 16:31:12,905: t15.2023.08.13 val PER: 0.3680
2026-01-08 16:31:12,907: t15.2023.08.18 val PER: 0.3378
2026-01-08 16:31:12,908: t15.2023.08.20 val PER: 0.3272
2026-01-08 16:31:12,910: t15.2023.08.25 val PER: 0.3163
2026-01-08 16:31:12,911: t15.2023.08.27 val PER: 0.4260
2026-01-08 16:31:12,913: t15.2023.09.01 val PER: 0.3157
2026-01-08 16:31:12,914: t15.2023.09.03 val PER: 0.4097
2026-01-08 16:31:12,915: t15.2023.09.24 val PER: 0.3180
2026-01-08 16:31:12,917: t15.2023.09.29 val PER: 0.3280
2026-01-08 16:31:12,918: t15.2023.10.01 val PER: 0.3791
2026-01-08 16:31:12,920: t15.2023.10.06 val PER: 0.3122
2026-01-08 16:31:12,921: t15.2023.10.08 val PER: 0.4398
2026-01-08 16:31:12,923: t15.2023.10.13 val PER: 0.4647
2026-01-08 16:31:12,924: t15.2023.10.15 val PER: 0.3606
2026-01-08 16:31:12,925: t15.2023.10.20 val PER: 0.3859
2026-01-08 16:31:12,927: t15.2023.10.22 val PER: 0.3330
2026-01-08 16:31:12,928: t15.2023.11.03 val PER: 0.3806
2026-01-08 16:31:12,929: t15.2023.11.04 val PER: 0.1706
2026-01-08 16:31:12,931: t15.2023.11.17 val PER: 0.2255
2026-01-08 16:31:12,932: t15.2023.11.19 val PER: 0.2275
2026-01-08 16:31:12,933: t15.2023.11.26 val PER: 0.4268
2026-01-08 16:31:12,935: t15.2023.12.03 val PER: 0.3918
2026-01-08 16:31:12,936: t15.2023.12.08 val PER: 0.3775
2026-01-08 16:31:12,938: t15.2023.12.10 val PER: 0.3272
2026-01-08 16:31:12,939: t15.2023.12.17 val PER: 0.3576
2026-01-08 16:31:12,941: t15.2023.12.29 val PER: 0.3734
2026-01-08 16:31:12,942: t15.2024.02.25 val PER: 0.3610
2026-01-08 16:31:12,944: t15.2024.03.08 val PER: 0.4523
2026-01-08 16:31:12,945: t15.2024.03.15 val PER: 0.3946
2026-01-08 16:31:12,946: t15.2024.03.17 val PER: 0.4003
2026-01-08 16:31:12,948: t15.2024.05.10 val PER: 0.3878
2026-01-08 16:31:12,949: t15.2024.06.14 val PER: 0.3912
2026-01-08 16:31:12,951: t15.2024.07.19 val PER: 0.5115
2026-01-08 16:31:12,952: t15.2024.07.21 val PER: 0.3503
2026-01-08 16:31:12,953: t15.2024.07.28 val PER: 0.3831
2026-01-08 16:31:12,955: t15.2025.01.10 val PER: 0.5702
2026-01-08 16:31:12,956: t15.2025.01.12 val PER: 0.4357
2026-01-08 16:31:12,958: t15.2025.03.14 val PER: 0.5814
2026-01-08 16:31:12,959: t15.2025.03.16 val PER: 0.4450
2026-01-08 16:31:12,961: t15.2025.03.30 val PER: 0.5678
2026-01-08 16:31:12,962: t15.2025.04.13 val PER: 0.4494
2026-01-08 16:31:13,097: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_2000
2026-01-08 16:31:30,075: Train batch 2200: loss: 36.02 grad norm: 78.75 time: 0.064
2026-01-08 16:31:48,191: Train batch 2400: loss: 36.16 grad norm: 82.91 time: 0.054
2026-01-08 16:31:57,021: Running test after training batch: 2500
2026-01-08 16:31:57,128: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:32:02,147: WER debug example
  GT : you can see the code at this point as well
  PR : hughley kids seeks the toehold hits this point his swells
2026-01-08 16:32:02,292: WER debug example
  GT : how does it keep the cost down
  PR : headsets hits kicks that's what's getting
2026-01-08 16:32:31,484: Val batch 2500: PER (avg): 0.3567 CTC Loss (avg): 37.7974 WER(5gram): 101.56% (n=256) time: 34.461
2026-01-08 16:32:31,486: WER lens: avg_true_words=5.99 avg_pred_words=6.46 max_pred_words=13
2026-01-08 16:32:31,489: t15.2023.08.13 val PER: 0.3462
2026-01-08 16:32:31,491: t15.2023.08.18 val PER: 0.3277
2026-01-08 16:32:31,493: t15.2023.08.20 val PER: 0.3082
2026-01-08 16:32:31,494: t15.2023.08.25 val PER: 0.2877
2026-01-08 16:32:31,496: t15.2023.08.27 val PER: 0.3746
2026-01-08 16:32:31,498: t15.2023.09.01 val PER: 0.2849
2026-01-08 16:32:31,500: t15.2023.09.03 val PER: 0.3634
2026-01-08 16:32:31,501: t15.2023.09.24 val PER: 0.2949
2026-01-08 16:32:31,503: t15.2023.09.29 val PER: 0.3025
2026-01-08 16:32:31,505: t15.2023.10.01 val PER: 0.3441
2026-01-08 16:32:31,506: t15.2023.10.06 val PER: 0.2842
2026-01-08 16:32:31,508: t15.2023.10.08 val PER: 0.4290
2026-01-08 16:32:31,510: t15.2023.10.13 val PER: 0.4251
2026-01-08 16:32:31,511: t15.2023.10.15 val PER: 0.3415
2026-01-08 16:32:31,513: t15.2023.10.20 val PER: 0.3456
2026-01-08 16:32:31,515: t15.2023.10.22 val PER: 0.3118
2026-01-08 16:32:31,517: t15.2023.11.03 val PER: 0.3623
2026-01-08 16:32:31,520: t15.2023.11.04 val PER: 0.1570
2026-01-08 16:32:31,522: t15.2023.11.17 val PER: 0.1913
2026-01-08 16:32:31,523: t15.2023.11.19 val PER: 0.1856
2026-01-08 16:32:31,525: t15.2023.11.26 val PER: 0.3920
2026-01-08 16:32:31,526: t15.2023.12.03 val PER: 0.3403
2026-01-08 16:32:31,528: t15.2023.12.08 val PER: 0.3389
2026-01-08 16:32:31,529: t15.2023.12.10 val PER: 0.2943
2026-01-08 16:32:31,531: t15.2023.12.17 val PER: 0.3306
2026-01-08 16:32:31,533: t15.2023.12.29 val PER: 0.3644
2026-01-08 16:32:31,534: t15.2024.02.25 val PER: 0.3146
2026-01-08 16:32:31,536: t15.2024.03.08 val PER: 0.3983
2026-01-08 16:32:31,537: t15.2024.03.15 val PER: 0.3702
2026-01-08 16:32:31,539: t15.2024.03.17 val PER: 0.3612
2026-01-08 16:32:31,540: t15.2024.05.10 val PER: 0.3388
2026-01-08 16:32:31,542: t15.2024.06.14 val PER: 0.3454
2026-01-08 16:32:31,543: t15.2024.07.19 val PER: 0.4608
2026-01-08 16:32:31,545: t15.2024.07.21 val PER: 0.3055
2026-01-08 16:32:31,546: t15.2024.07.28 val PER: 0.3404
2026-01-08 16:32:31,548: t15.2025.01.10 val PER: 0.5606
2026-01-08 16:32:31,549: t15.2025.01.12 val PER: 0.4134
2026-01-08 16:32:31,550: t15.2025.03.14 val PER: 0.5577
2026-01-08 16:32:31,552: t15.2025.03.16 val PER: 0.3822
2026-01-08 16:32:31,553: t15.2025.03.30 val PER: 0.5448
2026-01-08 16:32:31,555: t15.2025.04.13 val PER: 0.4379
2026-01-08 16:32:31,687: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_2500
2026-01-08 16:32:40,063: Train batch 2600: loss: 42.15 grad norm: 96.86 time: 0.057
2026-01-08 16:32:58,114: Train batch 2800: loss: 32.14 grad norm: 78.34 time: 0.088
2026-01-08 16:33:16,212: Train batch 3000: loss: 36.09 grad norm: 72.33 time: 0.086
2026-01-08 16:33:16,215: Running test after training batch: 3000
2026-01-08 16:33:16,316: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:33:21,292: WER debug example
  GT : you can see the code at this point as well
  PR : you decant sees us the tusk daihatsu stupp and his
2026-01-08 16:33:21,432: WER debug example
  GT : how does it keep the cost down
  PR : leasehold sus hits kicks the t-shirt stands
2026-01-08 16:33:50,036: Val batch 3000: PER (avg): 0.3397 CTC Loss (avg): 35.0070 WER(5gram): 100.72% (n=256) time: 33.819
2026-01-08 16:33:50,039: WER lens: avg_true_words=5.99 avg_pred_words=6.54 max_pred_words=14
2026-01-08 16:33:50,041: t15.2023.08.13 val PER: 0.3129
2026-01-08 16:33:50,043: t15.2023.08.18 val PER: 0.2842
2026-01-08 16:33:50,045: t15.2023.08.20 val PER: 0.2939
2026-01-08 16:33:50,047: t15.2023.08.25 val PER: 0.2696
2026-01-08 16:33:50,048: t15.2023.08.27 val PER: 0.3762
2026-01-08 16:33:50,050: t15.2023.09.01 val PER: 0.2646
2026-01-08 16:33:50,052: t15.2023.09.03 val PER: 0.3610
2026-01-08 16:33:50,053: t15.2023.09.24 val PER: 0.2816
2026-01-08 16:33:50,055: t15.2023.09.29 val PER: 0.2891
2026-01-08 16:33:50,056: t15.2023.10.01 val PER: 0.3342
2026-01-08 16:33:50,058: t15.2023.10.06 val PER: 0.2756
2026-01-08 16:33:50,059: t15.2023.10.08 val PER: 0.4317
2026-01-08 16:33:50,060: t15.2023.10.13 val PER: 0.4174
2026-01-08 16:33:50,063: t15.2023.10.15 val PER: 0.3158
2026-01-08 16:33:50,065: t15.2023.10.20 val PER: 0.3423
2026-01-08 16:33:50,066: t15.2023.10.22 val PER: 0.2906
2026-01-08 16:33:50,068: t15.2023.11.03 val PER: 0.3318
2026-01-08 16:33:50,070: t15.2023.11.04 val PER: 0.1570
2026-01-08 16:33:50,072: t15.2023.11.17 val PER: 0.1960
2026-01-08 16:33:50,074: t15.2023.11.19 val PER: 0.2036
2026-01-08 16:33:50,075: t15.2023.11.26 val PER: 0.3717
2026-01-08 16:33:50,077: t15.2023.12.03 val PER: 0.3120
2026-01-08 16:33:50,078: t15.2023.12.08 val PER: 0.3189
2026-01-08 16:33:50,080: t15.2023.12.10 val PER: 0.2891
2026-01-08 16:33:50,082: t15.2023.12.17 val PER: 0.3274
2026-01-08 16:33:50,083: t15.2023.12.29 val PER: 0.3342
2026-01-08 16:33:50,085: t15.2024.02.25 val PER: 0.2992
2026-01-08 16:33:50,086: t15.2024.03.08 val PER: 0.3940
2026-01-08 16:33:50,088: t15.2024.03.15 val PER: 0.3640
2026-01-08 16:33:50,089: t15.2024.03.17 val PER: 0.3501
2026-01-08 16:33:50,091: t15.2024.05.10 val PER: 0.3061
2026-01-08 16:33:50,093: t15.2024.06.14 val PER: 0.3486
2026-01-08 16:33:50,094: t15.2024.07.19 val PER: 0.4351
2026-01-08 16:33:50,096: t15.2024.07.21 val PER: 0.2766
2026-01-08 16:33:50,097: t15.2024.07.28 val PER: 0.3287
2026-01-08 16:33:50,099: t15.2025.01.10 val PER: 0.5358
2026-01-08 16:33:50,100: t15.2025.01.12 val PER: 0.3911
2026-01-08 16:33:50,101: t15.2025.03.14 val PER: 0.5325
2026-01-08 16:33:50,103: t15.2025.03.16 val PER: 0.3678
2026-01-08 16:33:50,104: t15.2025.03.30 val PER: 0.5161
2026-01-08 16:33:50,106: t15.2025.04.13 val PER: 0.4180
2026-01-08 16:33:50,241: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_3000
2026-01-08 16:34:07,351: Train batch 3200: loss: 30.17 grad norm: 68.20 time: 0.078
2026-01-08 16:34:25,124: Train batch 3400: loss: 25.31 grad norm: 61.38 time: 0.050
2026-01-08 16:34:33,713: Running test after training batch: 3500
2026-01-08 16:34:33,834: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:34:39,216: WER debug example
  GT : you can see the code at this point as well
  PR : hudy had seeks the steelhead hats that suspends his swells
2026-01-08 16:34:39,347: WER debug example
  GT : how does it keep the cost down
  PR : eat hostess hits hex that's what sets
2026-01-08 16:35:02,752: Val batch 3500: PER (avg): 0.3305 CTC Loss (avg): 33.8023 WER(5gram): 109.26% (n=256) time: 29.038
2026-01-08 16:35:02,755: WER lens: avg_true_words=5.99 avg_pred_words=7.03 max_pred_words=14
2026-01-08 16:35:02,757: t15.2023.08.13 val PER: 0.3233
2026-01-08 16:35:02,759: t15.2023.08.18 val PER: 0.2984
2026-01-08 16:35:02,760: t15.2023.08.20 val PER: 0.2915
2026-01-08 16:35:02,762: t15.2023.08.25 val PER: 0.2605
2026-01-08 16:35:02,763: t15.2023.08.27 val PER: 0.3746
2026-01-08 16:35:02,765: t15.2023.09.01 val PER: 0.2646
2026-01-08 16:35:02,767: t15.2023.09.03 val PER: 0.3539
2026-01-08 16:35:02,768: t15.2023.09.24 val PER: 0.2791
2026-01-08 16:35:02,771: t15.2023.09.29 val PER: 0.2827
2026-01-08 16:35:02,773: t15.2023.10.01 val PER: 0.3329
2026-01-08 16:35:02,774: t15.2023.10.06 val PER: 0.2670
2026-01-08 16:35:02,776: t15.2023.10.08 val PER: 0.4154
2026-01-08 16:35:02,778: t15.2023.10.13 val PER: 0.3933
2026-01-08 16:35:02,779: t15.2023.10.15 val PER: 0.3045
2026-01-08 16:35:02,781: t15.2023.10.20 val PER: 0.2919
2026-01-08 16:35:02,782: t15.2023.10.22 val PER: 0.2806
2026-01-08 16:35:02,784: t15.2023.11.03 val PER: 0.3284
2026-01-08 16:35:02,785: t15.2023.11.04 val PER: 0.1468
2026-01-08 16:35:02,787: t15.2023.11.17 val PER: 0.1820
2026-01-08 16:35:02,789: t15.2023.11.19 val PER: 0.1836
2026-01-08 16:35:02,791: t15.2023.11.26 val PER: 0.3522
2026-01-08 16:35:02,793: t15.2023.12.03 val PER: 0.2931
2026-01-08 16:35:02,795: t15.2023.12.08 val PER: 0.3076
2026-01-08 16:35:02,797: t15.2023.12.10 val PER: 0.2733
2026-01-08 16:35:02,799: t15.2023.12.17 val PER: 0.3098
2026-01-08 16:35:02,800: t15.2023.12.29 val PER: 0.3260
2026-01-08 16:35:02,802: t15.2024.02.25 val PER: 0.2823
2026-01-08 16:35:02,804: t15.2024.03.08 val PER: 0.3954
2026-01-08 16:35:02,805: t15.2024.03.15 val PER: 0.3527
2026-01-08 16:35:02,807: t15.2024.03.17 val PER: 0.3445
2026-01-08 16:35:02,809: t15.2024.05.10 val PER: 0.3239
2026-01-08 16:35:02,811: t15.2024.06.14 val PER: 0.3297
2026-01-08 16:35:02,812: t15.2024.07.19 val PER: 0.4173
2026-01-08 16:35:02,814: t15.2024.07.21 val PER: 0.2697
2026-01-08 16:35:02,815: t15.2024.07.28 val PER: 0.3213
2026-01-08 16:35:02,817: t15.2025.01.10 val PER: 0.5014
2026-01-08 16:35:02,819: t15.2025.01.12 val PER: 0.3841
2026-01-08 16:35:02,820: t15.2025.03.14 val PER: 0.5178
2026-01-08 16:35:02,822: t15.2025.03.16 val PER: 0.3652
2026-01-08 16:35:02,824: t15.2025.03.30 val PER: 0.4977
2026-01-08 16:35:02,825: t15.2025.04.13 val PER: 0.4023
2026-01-08 16:35:02,956: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_3500
2026-01-08 16:35:11,539: Train batch 3600: loss: 28.99 grad norm: 67.25 time: 0.070
2026-01-08 16:35:28,700: Train batch 3800: loss: 33.32 grad norm: 81.00 time: 0.070
2026-01-08 16:35:46,990: Train batch 4000: loss: 24.03 grad norm: 65.89 time: 0.058
2026-01-08 16:35:46,993: Running test after training batch: 4000
2026-01-08 16:35:47,104: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:35:52,409: WER debug example
  GT : you can see the code at this point as well
  PR : you decant seats that had hats it appoints his
2026-01-08 16:35:52,519: WER debug example
  GT : how does it keep the cost down
  PR : niehaus just hits hicks that's cost
2026-01-08 16:36:15,624: Val batch 4000: PER (avg): 0.3086 CTC Loss (avg): 31.8529 WER(5gram): 105.67% (n=256) time: 28.628
2026-01-08 16:36:15,626: WER lens: avg_true_words=5.99 avg_pred_words=6.89 max_pred_words=14
2026-01-08 16:36:15,628: t15.2023.08.13 val PER: 0.2848
2026-01-08 16:36:15,630: t15.2023.08.18 val PER: 0.2766
2026-01-08 16:36:15,632: t15.2023.08.20 val PER: 0.2716
2026-01-08 16:36:15,633: t15.2023.08.25 val PER: 0.2440
2026-01-08 16:36:15,635: t15.2023.08.27 val PER: 0.3521
2026-01-08 16:36:15,637: t15.2023.09.01 val PER: 0.2492
2026-01-08 16:36:15,639: t15.2023.09.03 val PER: 0.3254
2026-01-08 16:36:15,640: t15.2023.09.24 val PER: 0.2597
2026-01-08 16:36:15,642: t15.2023.09.29 val PER: 0.2668
2026-01-08 16:36:15,644: t15.2023.10.01 val PER: 0.3131
2026-01-08 16:36:15,646: t15.2023.10.06 val PER: 0.2357
2026-01-08 16:36:15,648: t15.2023.10.08 val PER: 0.3884
2026-01-08 16:36:15,649: t15.2023.10.13 val PER: 0.3708
2026-01-08 16:36:15,651: t15.2023.10.15 val PER: 0.2999
2026-01-08 16:36:15,653: t15.2023.10.20 val PER: 0.2752
2026-01-08 16:36:15,655: t15.2023.10.22 val PER: 0.2661
2026-01-08 16:36:15,656: t15.2023.11.03 val PER: 0.3005
2026-01-08 16:36:15,658: t15.2023.11.04 val PER: 0.1433
2026-01-08 16:36:15,659: t15.2023.11.17 val PER: 0.1773
2026-01-08 16:36:15,661: t15.2023.11.19 val PER: 0.1756
2026-01-08 16:36:15,662: t15.2023.11.26 val PER: 0.3304
2026-01-08 16:36:15,664: t15.2023.12.03 val PER: 0.2710
2026-01-08 16:36:15,665: t15.2023.12.08 val PER: 0.2770
2026-01-08 16:36:15,667: t15.2023.12.10 val PER: 0.2510
2026-01-08 16:36:15,669: t15.2023.12.17 val PER: 0.2983
2026-01-08 16:36:15,670: t15.2023.12.29 val PER: 0.3130
2026-01-08 16:36:15,672: t15.2024.02.25 val PER: 0.2669
2026-01-08 16:36:15,674: t15.2024.03.08 val PER: 0.3741
2026-01-08 16:36:15,675: t15.2024.03.15 val PER: 0.3177
2026-01-08 16:36:15,677: t15.2024.03.17 val PER: 0.3110
2026-01-08 16:36:15,679: t15.2024.05.10 val PER: 0.3180
2026-01-08 16:36:15,680: t15.2024.06.14 val PER: 0.3155
2026-01-08 16:36:15,682: t15.2024.07.19 val PER: 0.4021
2026-01-08 16:36:15,684: t15.2024.07.21 val PER: 0.2359
2026-01-08 16:36:15,685: t15.2024.07.28 val PER: 0.3051
2026-01-08 16:36:15,687: t15.2025.01.10 val PER: 0.4835
2026-01-08 16:36:15,689: t15.2025.01.12 val PER: 0.3533
2026-01-08 16:36:15,691: t15.2025.03.14 val PER: 0.4704
2026-01-08 16:36:15,693: t15.2025.03.16 val PER: 0.3377
2026-01-08 16:36:15,695: t15.2025.03.30 val PER: 0.4632
2026-01-08 16:36:15,696: t15.2025.04.13 val PER: 0.3866
2026-01-08 16:36:15,831: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_4000
2026-01-08 16:36:33,331: Train batch 4200: loss: 28.50 grad norm: 67.85 time: 0.082
2026-01-08 16:36:50,554: Train batch 4400: loss: 23.63 grad norm: 59.95 time: 0.069
2026-01-08 16:36:59,307: Running test after training batch: 4500
2026-01-08 16:36:59,415: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:37:04,462: WER debug example
  GT : you can see the code at this point as well
  PR : cute kids seats the two hoods hats it's point his swells
2026-01-08 16:37:04,523: WER debug example
  GT : how does it keep the cost down
  PR : it holds sus hits hicks that's what sounds
2026-01-08 16:37:24,437: Val batch 4500: PER (avg): 0.2969 CTC Loss (avg): 30.8004 WER(5gram): 103.59% (n=256) time: 25.127
2026-01-08 16:37:24,439: WER lens: avg_true_words=5.99 avg_pred_words=6.88 max_pred_words=14
2026-01-08 16:37:24,441: t15.2023.08.13 val PER: 0.2859
2026-01-08 16:37:24,443: t15.2023.08.18 val PER: 0.2632
2026-01-08 16:37:24,445: t15.2023.08.20 val PER: 0.2597
2026-01-08 16:37:24,446: t15.2023.08.25 val PER: 0.2274
2026-01-08 16:37:24,450: t15.2023.08.27 val PER: 0.3408
2026-01-08 16:37:24,451: t15.2023.09.01 val PER: 0.2459
2026-01-08 16:37:24,453: t15.2023.09.03 val PER: 0.3124
2026-01-08 16:37:24,454: t15.2023.09.24 val PER: 0.2512
2026-01-08 16:37:24,456: t15.2023.09.29 val PER: 0.2527
2026-01-08 16:37:24,457: t15.2023.10.01 val PER: 0.2946
2026-01-08 16:37:24,459: t15.2023.10.06 val PER: 0.2239
2026-01-08 16:37:24,460: t15.2023.10.08 val PER: 0.3829
2026-01-08 16:37:24,462: t15.2023.10.13 val PER: 0.3801
2026-01-08 16:37:24,464: t15.2023.10.15 val PER: 0.2953
2026-01-08 16:37:24,466: t15.2023.10.20 val PER: 0.2987
2026-01-08 16:37:24,467: t15.2023.10.22 val PER: 0.2428
2026-01-08 16:37:24,469: t15.2023.11.03 val PER: 0.2972
2026-01-08 16:37:24,471: t15.2023.11.04 val PER: 0.1399
2026-01-08 16:37:24,472: t15.2023.11.17 val PER: 0.1617
2026-01-08 16:37:24,474: t15.2023.11.19 val PER: 0.1776
2026-01-08 16:37:24,476: t15.2023.11.26 val PER: 0.3239
2026-01-08 16:37:24,478: t15.2023.12.03 val PER: 0.2647
2026-01-08 16:37:24,480: t15.2023.12.08 val PER: 0.2703
2026-01-08 16:37:24,482: t15.2023.12.10 val PER: 0.2392
2026-01-08 16:37:24,483: t15.2023.12.17 val PER: 0.2703
2026-01-08 16:37:24,485: t15.2023.12.29 val PER: 0.2999
2026-01-08 16:37:24,487: t15.2024.02.25 val PER: 0.2486
2026-01-08 16:37:24,489: t15.2024.03.08 val PER: 0.3471
2026-01-08 16:37:24,491: t15.2024.03.15 val PER: 0.3290
2026-01-08 16:37:24,493: t15.2024.03.17 val PER: 0.2964
2026-01-08 16:37:24,494: t15.2024.05.10 val PER: 0.2972
2026-01-08 16:37:24,496: t15.2024.06.14 val PER: 0.2839
2026-01-08 16:37:24,498: t15.2024.07.19 val PER: 0.3731
2026-01-08 16:37:24,499: t15.2024.07.21 val PER: 0.2234
2026-01-08 16:37:24,501: t15.2024.07.28 val PER: 0.2838
2026-01-08 16:37:24,502: t15.2025.01.10 val PER: 0.4587
2026-01-08 16:37:24,504: t15.2025.01.12 val PER: 0.3341
2026-01-08 16:37:24,505: t15.2025.03.14 val PER: 0.4571
2026-01-08 16:37:24,507: t15.2025.03.16 val PER: 0.3442
2026-01-08 16:37:24,508: t15.2025.03.30 val PER: 0.4437
2026-01-08 16:37:24,510: t15.2025.04.13 val PER: 0.3581
2026-01-08 16:37:24,646: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_4500
2026-01-08 16:37:33,539: Train batch 4600: loss: 25.37 grad norm: 64.65 time: 0.064
2026-01-08 16:37:51,412: Train batch 4800: loss: 20.42 grad norm: 63.52 time: 0.065
2026-01-08 16:38:08,579: Train batch 5000: loss: 36.14 grad norm: 80.50 time: 0.068
2026-01-08 16:38:08,581: Running test after training batch: 5000
2026-01-08 16:38:08,694: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:38:13,655: WER debug example
  GT : you can see the code at this point as well
  PR : you can't seeks the toehold hits this point his
2026-01-08 16:38:13,707: WER debug example
  GT : how does it keep the cost down
  PR : it holds stacey hits hicks the toast setzer
2026-01-08 16:38:32,187: Val batch 5000: PER (avg): 0.2865 CTC Loss (avg): 29.4126 WER(5gram): 104.95% (n=256) time: 23.604
2026-01-08 16:38:32,189: WER lens: avg_true_words=5.99 avg_pred_words=7.02 max_pred_words=14
2026-01-08 16:38:32,193: t15.2023.08.13 val PER: 0.2651
2026-01-08 16:38:32,194: t15.2023.08.18 val PER: 0.2498
2026-01-08 16:38:32,196: t15.2023.08.20 val PER: 0.2605
2026-01-08 16:38:32,198: t15.2023.08.25 val PER: 0.2304
2026-01-08 16:38:32,199: t15.2023.08.27 val PER: 0.3296
2026-01-08 16:38:32,201: t15.2023.09.01 val PER: 0.2256
2026-01-08 16:38:32,203: t15.2023.09.03 val PER: 0.3230
2026-01-08 16:38:32,204: t15.2023.09.24 val PER: 0.2597
2026-01-08 16:38:32,206: t15.2023.09.29 val PER: 0.2412
2026-01-08 16:38:32,208: t15.2023.10.01 val PER: 0.2966
2026-01-08 16:38:32,210: t15.2023.10.06 val PER: 0.2239
2026-01-08 16:38:32,211: t15.2023.10.08 val PER: 0.3762
2026-01-08 16:38:32,213: t15.2023.10.13 val PER: 0.3600
2026-01-08 16:38:32,215: t15.2023.10.15 val PER: 0.2729
2026-01-08 16:38:32,217: t15.2023.10.20 val PER: 0.2651
2026-01-08 16:38:32,218: t15.2023.10.22 val PER: 0.2405
2026-01-08 16:38:32,220: t15.2023.11.03 val PER: 0.2877
2026-01-08 16:38:32,222: t15.2023.11.04 val PER: 0.1229
2026-01-08 16:38:32,225: t15.2023.11.17 val PER: 0.1509
2026-01-08 16:38:32,227: t15.2023.11.19 val PER: 0.1537
2026-01-08 16:38:32,229: t15.2023.11.26 val PER: 0.2993
2026-01-08 16:38:32,230: t15.2023.12.03 val PER: 0.2584
2026-01-08 16:38:32,232: t15.2023.12.08 val PER: 0.2597
2026-01-08 16:38:32,233: t15.2023.12.10 val PER: 0.2247
2026-01-08 16:38:32,235: t15.2023.12.17 val PER: 0.2672
2026-01-08 16:38:32,237: t15.2023.12.29 val PER: 0.2951
2026-01-08 16:38:32,238: t15.2024.02.25 val PER: 0.2500
2026-01-08 16:38:32,240: t15.2024.03.08 val PER: 0.3457
2026-01-08 16:38:32,241: t15.2024.03.15 val PER: 0.3014
2026-01-08 16:38:32,243: t15.2024.03.17 val PER: 0.2782
2026-01-08 16:38:32,244: t15.2024.05.10 val PER: 0.2957
2026-01-08 16:38:32,246: t15.2024.06.14 val PER: 0.2981
2026-01-08 16:38:32,247: t15.2024.07.19 val PER: 0.3507
2026-01-08 16:38:32,249: t15.2024.07.21 val PER: 0.2103
2026-01-08 16:38:32,250: t15.2024.07.28 val PER: 0.2779
2026-01-08 16:38:32,251: t15.2025.01.10 val PER: 0.4353
2026-01-08 16:38:32,253: t15.2025.01.12 val PER: 0.3210
2026-01-08 16:38:32,254: t15.2025.03.14 val PER: 0.4408
2026-01-08 16:38:32,256: t15.2025.03.16 val PER: 0.3338
2026-01-08 16:38:32,258: t15.2025.03.30 val PER: 0.4287
2026-01-08 16:38:32,259: t15.2025.04.13 val PER: 0.3752
2026-01-08 16:38:32,391: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_5000
2026-01-08 16:38:50,707: Train batch 5200: loss: 22.42 grad norm: 65.34 time: 0.055
2026-01-08 16:39:08,968: Train batch 5400: loss: 24.23 grad norm: 64.79 time: 0.070
2026-01-08 16:39:17,765: Running test after training batch: 5500
2026-01-08 16:39:17,915: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:39:23,204: WER debug example
  GT : you can see the code at this point as well
  PR : you can't seeks the stuccoed hat sits points his
2026-01-08 16:39:23,267: WER debug example
  GT : how does it keep the cost down
  PR : east holds sus hits kicks that sends
2026-01-08 16:39:40,728: Val batch 5500: PER (avg): 0.2809 CTC Loss (avg): 28.6078 WER(5gram): 102.35% (n=256) time: 22.960
2026-01-08 16:39:40,730: WER lens: avg_true_words=5.99 avg_pred_words=6.84 max_pred_words=14
2026-01-08 16:39:40,732: t15.2023.08.13 val PER: 0.2547
2026-01-08 16:39:40,734: t15.2023.08.18 val PER: 0.2498
2026-01-08 16:39:40,735: t15.2023.08.20 val PER: 0.2446
2026-01-08 16:39:40,737: t15.2023.08.25 val PER: 0.2259
2026-01-08 16:39:40,739: t15.2023.08.27 val PER: 0.3360
2026-01-08 16:39:40,740: t15.2023.09.01 val PER: 0.2127
2026-01-08 16:39:40,743: t15.2023.09.03 val PER: 0.3195
2026-01-08 16:39:40,744: t15.2023.09.24 val PER: 0.2342
2026-01-08 16:39:40,746: t15.2023.09.29 val PER: 0.2521
2026-01-08 16:39:40,747: t15.2023.10.01 val PER: 0.2913
2026-01-08 16:39:40,749: t15.2023.10.06 val PER: 0.2164
2026-01-08 16:39:40,751: t15.2023.10.08 val PER: 0.3613
2026-01-08 16:39:40,752: t15.2023.10.13 val PER: 0.3522
2026-01-08 16:39:40,754: t15.2023.10.15 val PER: 0.2670
2026-01-08 16:39:40,755: t15.2023.10.20 val PER: 0.2785
2026-01-08 16:39:40,757: t15.2023.10.22 val PER: 0.2261
2026-01-08 16:39:40,760: t15.2023.11.03 val PER: 0.2877
2026-01-08 16:39:40,761: t15.2023.11.04 val PER: 0.1229
2026-01-08 16:39:40,763: t15.2023.11.17 val PER: 0.1524
2026-01-08 16:39:40,765: t15.2023.11.19 val PER: 0.1537
2026-01-08 16:39:40,766: t15.2023.11.26 val PER: 0.2884
2026-01-08 16:39:40,768: t15.2023.12.03 val PER: 0.2426
2026-01-08 16:39:40,769: t15.2023.12.08 val PER: 0.2497
2026-01-08 16:39:40,771: t15.2023.12.10 val PER: 0.2273
2026-01-08 16:39:40,773: t15.2023.12.17 val PER: 0.2547
2026-01-08 16:39:40,774: t15.2023.12.29 val PER: 0.2883
2026-01-08 16:39:40,776: t15.2024.02.25 val PER: 0.2331
2026-01-08 16:39:40,777: t15.2024.03.08 val PER: 0.3542
2026-01-08 16:39:40,779: t15.2024.03.15 val PER: 0.2989
2026-01-08 16:39:40,780: t15.2024.03.17 val PER: 0.2957
2026-01-08 16:39:40,782: t15.2024.05.10 val PER: 0.3001
2026-01-08 16:39:40,784: t15.2024.06.14 val PER: 0.2918
2026-01-08 16:39:40,785: t15.2024.07.19 val PER: 0.3514
2026-01-08 16:39:40,787: t15.2024.07.21 val PER: 0.2034
2026-01-08 16:39:40,788: t15.2024.07.28 val PER: 0.2706
2026-01-08 16:39:40,790: t15.2025.01.10 val PER: 0.4353
2026-01-08 16:39:40,791: t15.2025.01.12 val PER: 0.3025
2026-01-08 16:39:40,793: t15.2025.03.14 val PER: 0.4320
2026-01-08 16:39:40,795: t15.2025.03.16 val PER: 0.3102
2026-01-08 16:39:40,796: t15.2025.03.30 val PER: 0.4391
2026-01-08 16:39:40,798: t15.2025.04.13 val PER: 0.3509
2026-01-08 16:39:40,934: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_5500
2026-01-08 16:39:49,612: Train batch 5600: loss: 24.60 grad norm: 71.55 time: 0.067
2026-01-08 16:40:07,630: Train batch 5800: loss: 18.43 grad norm: 63.45 time: 0.086
2026-01-08 16:40:25,125: Train batch 6000: loss: 18.84 grad norm: 60.97 time: 0.052
2026-01-08 16:40:25,127: Running test after training batch: 6000
2026-01-08 16:40:25,247: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:40:30,293: WER debug example
  GT : you can see the code at this point as well
  PR : you'd kids seeks the tusk good hats it appoints his swells
2026-01-08 16:40:30,367: WER debug example
  GT : how does it keep the cost down
  PR : eat hounds sus hits he peeks the scott stetzer
2026-01-08 16:40:48,022: Val batch 6000: PER (avg): 0.2702 CTC Loss (avg): 27.7704 WER(5gram): 103.85% (n=256) time: 22.892
2026-01-08 16:40:48,024: WER lens: avg_true_words=5.99 avg_pred_words=7.04 max_pred_words=14
2026-01-08 16:40:48,027: t15.2023.08.13 val PER: 0.2526
2026-01-08 16:40:48,028: t15.2023.08.18 val PER: 0.2590
2026-01-08 16:40:48,030: t15.2023.08.20 val PER: 0.2256
2026-01-08 16:40:48,032: t15.2023.08.25 val PER: 0.2214
2026-01-08 16:40:48,034: t15.2023.08.27 val PER: 0.3296
2026-01-08 16:40:48,035: t15.2023.09.01 val PER: 0.2086
2026-01-08 16:40:48,037: t15.2023.09.03 val PER: 0.3029
2026-01-08 16:40:48,039: t15.2023.09.24 val PER: 0.2221
2026-01-08 16:40:48,040: t15.2023.09.29 val PER: 0.2393
2026-01-08 16:40:48,042: t15.2023.10.01 val PER: 0.2840
2026-01-08 16:40:48,044: t15.2023.10.06 val PER: 0.2110
2026-01-08 16:40:48,045: t15.2023.10.08 val PER: 0.3586
2026-01-08 16:40:48,047: t15.2023.10.13 val PER: 0.3382
2026-01-08 16:40:48,049: t15.2023.10.15 val PER: 0.2657
2026-01-08 16:40:48,050: t15.2023.10.20 val PER: 0.2617
2026-01-08 16:40:48,052: t15.2023.10.22 val PER: 0.2038
2026-01-08 16:40:48,054: t15.2023.11.03 val PER: 0.2822
2026-01-08 16:40:48,055: t15.2023.11.04 val PER: 0.1365
2026-01-08 16:40:48,057: t15.2023.11.17 val PER: 0.1369
2026-01-08 16:40:48,058: t15.2023.11.19 val PER: 0.1417
2026-01-08 16:40:48,060: t15.2023.11.26 val PER: 0.2913
2026-01-08 16:40:48,062: t15.2023.12.03 val PER: 0.2458
2026-01-08 16:40:48,063: t15.2023.12.08 val PER: 0.2450
2026-01-08 16:40:48,065: t15.2023.12.10 val PER: 0.2129
2026-01-08 16:40:48,066: t15.2023.12.17 val PER: 0.2432
2026-01-08 16:40:48,068: t15.2023.12.29 val PER: 0.2793
2026-01-08 16:40:48,070: t15.2024.02.25 val PER: 0.2374
2026-01-08 16:40:48,071: t15.2024.03.08 val PER: 0.3300
2026-01-08 16:40:48,073: t15.2024.03.15 val PER: 0.2921
2026-01-08 16:40:48,074: t15.2024.03.17 val PER: 0.2748
2026-01-08 16:40:48,076: t15.2024.05.10 val PER: 0.2897
2026-01-08 16:40:48,077: t15.2024.06.14 val PER: 0.2587
2026-01-08 16:40:48,079: t15.2024.07.19 val PER: 0.3296
2026-01-08 16:40:48,080: t15.2024.07.21 val PER: 0.1938
2026-01-08 16:40:48,082: t15.2024.07.28 val PER: 0.2338
2026-01-08 16:40:48,083: t15.2025.01.10 val PER: 0.4050
2026-01-08 16:40:48,085: t15.2025.01.12 val PER: 0.2910
2026-01-08 16:40:48,086: t15.2025.03.14 val PER: 0.4246
2026-01-08 16:40:48,088: t15.2025.03.16 val PER: 0.3089
2026-01-08 16:40:48,089: t15.2025.03.30 val PER: 0.4149
2026-01-08 16:40:48,091: t15.2025.04.13 val PER: 0.3395
2026-01-08 16:40:48,224: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_6000
2026-01-08 16:41:05,362: Train batch 6200: loss: 20.82 grad norm: 66.44 time: 0.073
2026-01-08 16:41:22,821: Train batch 6400: loss: 25.42 grad norm: 72.82 time: 0.065
2026-01-08 16:41:31,433: Running test after training batch: 6500
2026-01-08 16:41:31,562: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:41:36,714: WER debug example
  GT : you can see the code at this point as well
  PR : you kids seeks the treuhand hat sits points his swells
2026-01-08 16:41:36,794: WER debug example
  GT : how does it keep the cost down
  PR : t holds sus hits keep curse that scott stetzer
2026-01-08 16:41:54,000: Val batch 6500: PER (avg): 0.2661 CTC Loss (avg): 27.1969 WER(5gram): 104.69% (n=256) time: 22.564
2026-01-08 16:41:54,002: WER lens: avg_true_words=5.99 avg_pred_words=7.06 max_pred_words=15
2026-01-08 16:41:54,005: t15.2023.08.13 val PER: 0.2391
2026-01-08 16:41:54,007: t15.2023.08.18 val PER: 0.2381
2026-01-08 16:41:54,009: t15.2023.08.20 val PER: 0.2248
2026-01-08 16:41:54,011: t15.2023.08.25 val PER: 0.2169
2026-01-08 16:41:54,013: t15.2023.08.27 val PER: 0.3119
2026-01-08 16:41:54,016: t15.2023.09.01 val PER: 0.2110
2026-01-08 16:41:54,018: t15.2023.09.03 val PER: 0.2957
2026-01-08 16:41:54,020: t15.2023.09.24 val PER: 0.2330
2026-01-08 16:41:54,022: t15.2023.09.29 val PER: 0.2336
2026-01-08 16:41:54,024: t15.2023.10.01 val PER: 0.2754
2026-01-08 16:41:54,026: t15.2023.10.06 val PER: 0.2142
2026-01-08 16:41:54,028: t15.2023.10.08 val PER: 0.3451
2026-01-08 16:41:54,030: t15.2023.10.13 val PER: 0.3398
2026-01-08 16:41:54,032: t15.2023.10.15 val PER: 0.2643
2026-01-08 16:41:54,034: t15.2023.10.20 val PER: 0.2651
2026-01-08 16:41:54,036: t15.2023.10.22 val PER: 0.2160
2026-01-08 16:41:54,038: t15.2023.11.03 val PER: 0.2768
2026-01-08 16:41:54,040: t15.2023.11.04 val PER: 0.1160
2026-01-08 16:41:54,042: t15.2023.11.17 val PER: 0.1337
2026-01-08 16:41:54,043: t15.2023.11.19 val PER: 0.1577
2026-01-08 16:41:54,045: t15.2023.11.26 val PER: 0.2703
2026-01-08 16:41:54,047: t15.2023.12.03 val PER: 0.2342
2026-01-08 16:41:54,049: t15.2023.12.08 val PER: 0.2490
2026-01-08 16:41:54,051: t15.2023.12.10 val PER: 0.1905
2026-01-08 16:41:54,053: t15.2023.12.17 val PER: 0.2464
2026-01-08 16:41:54,055: t15.2023.12.29 val PER: 0.2793
2026-01-08 16:41:54,057: t15.2024.02.25 val PER: 0.2261
2026-01-08 16:41:54,059: t15.2024.03.08 val PER: 0.3257
2026-01-08 16:41:54,061: t15.2024.03.15 val PER: 0.2896
2026-01-08 16:41:54,063: t15.2024.03.17 val PER: 0.2629
2026-01-08 16:41:54,065: t15.2024.05.10 val PER: 0.2853
2026-01-08 16:41:54,068: t15.2024.06.14 val PER: 0.2555
2026-01-08 16:41:54,070: t15.2024.07.19 val PER: 0.3303
2026-01-08 16:41:54,072: t15.2024.07.21 val PER: 0.1993
2026-01-08 16:41:54,074: t15.2024.07.28 val PER: 0.2456
2026-01-08 16:41:54,076: t15.2025.01.10 val PER: 0.4105
2026-01-08 16:41:54,078: t15.2025.01.12 val PER: 0.2910
2026-01-08 16:41:54,080: t15.2025.03.14 val PER: 0.4068
2026-01-08 16:41:54,081: t15.2025.03.16 val PER: 0.2958
2026-01-08 16:41:54,083: t15.2025.03.30 val PER: 0.3989
2026-01-08 16:41:54,085: t15.2025.04.13 val PER: 0.3295
2026-01-08 16:41:54,221: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_6500
2026-01-08 16:42:02,651: Train batch 6600: loss: 16.58 grad norm: 55.66 time: 0.047
2026-01-08 16:42:21,139: Train batch 6800: loss: 22.23 grad norm: 59.46 time: 0.050
2026-01-08 16:42:39,752: Train batch 7000: loss: 22.42 grad norm: 65.84 time: 0.066
2026-01-08 16:42:39,754: Running test after training batch: 7000
2026-01-08 16:42:39,923: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:42:44,944: WER debug example
  GT : you can see the code at this point as well
  PR : i used canned seeks the tusk and hat sits points his swells
2026-01-08 16:42:45,003: WER debug example
  GT : how does it keep the cost down
  PR : sir howard's sus hits he peaks that scott stetzer
2026-01-08 16:43:02,307: Val batch 7000: PER (avg): 0.2553 CTC Loss (avg): 26.3868 WER(5gram): 105.54% (n=256) time: 22.549
2026-01-08 16:43:02,309: WER lens: avg_true_words=5.99 avg_pred_words=7.12 max_pred_words=14
2026-01-08 16:43:02,311: t15.2023.08.13 val PER: 0.2391
2026-01-08 16:43:02,314: t15.2023.08.18 val PER: 0.2196
2026-01-08 16:43:02,315: t15.2023.08.20 val PER: 0.2240
2026-01-08 16:43:02,317: t15.2023.08.25 val PER: 0.2018
2026-01-08 16:43:02,318: t15.2023.08.27 val PER: 0.2974
2026-01-08 16:43:02,320: t15.2023.09.01 val PER: 0.1981
2026-01-08 16:43:02,322: t15.2023.09.03 val PER: 0.2827
2026-01-08 16:43:02,323: t15.2023.09.24 val PER: 0.2197
2026-01-08 16:43:02,325: t15.2023.09.29 val PER: 0.2272
2026-01-08 16:43:02,326: t15.2023.10.01 val PER: 0.2675
2026-01-08 16:43:02,328: t15.2023.10.06 val PER: 0.1916
2026-01-08 16:43:02,329: t15.2023.10.08 val PER: 0.3491
2026-01-08 16:43:02,331: t15.2023.10.13 val PER: 0.3313
2026-01-08 16:43:02,332: t15.2023.10.15 val PER: 0.2512
2026-01-08 16:43:02,334: t15.2023.10.20 val PER: 0.2617
2026-01-08 16:43:02,335: t15.2023.10.22 val PER: 0.1982
2026-01-08 16:43:02,337: t15.2023.11.03 val PER: 0.2632
2026-01-08 16:43:02,338: t15.2023.11.04 val PER: 0.1058
2026-01-08 16:43:02,340: t15.2023.11.17 val PER: 0.1213
2026-01-08 16:43:02,341: t15.2023.11.19 val PER: 0.1497
2026-01-08 16:43:02,343: t15.2023.11.26 val PER: 0.2572
2026-01-08 16:43:02,344: t15.2023.12.03 val PER: 0.2363
2026-01-08 16:43:02,346: t15.2023.12.08 val PER: 0.2237
2026-01-08 16:43:02,347: t15.2023.12.10 val PER: 0.2011
2026-01-08 16:43:02,349: t15.2023.12.17 val PER: 0.2256
2026-01-08 16:43:02,350: t15.2023.12.29 val PER: 0.2574
2026-01-08 16:43:02,352: t15.2024.02.25 val PER: 0.2107
2026-01-08 16:43:02,353: t15.2024.03.08 val PER: 0.3144
2026-01-08 16:43:02,355: t15.2024.03.15 val PER: 0.2789
2026-01-08 16:43:02,356: t15.2024.03.17 val PER: 0.2531
2026-01-08 16:43:02,358: t15.2024.05.10 val PER: 0.2689
2026-01-08 16:43:02,359: t15.2024.06.14 val PER: 0.2571
2026-01-08 16:43:02,360: t15.2024.07.19 val PER: 0.3164
2026-01-08 16:43:02,362: t15.2024.07.21 val PER: 0.1683
2026-01-08 16:43:02,365: t15.2024.07.28 val PER: 0.2250
2026-01-08 16:43:02,367: t15.2025.01.10 val PER: 0.4160
2026-01-08 16:43:02,369: t15.2025.01.12 val PER: 0.2925
2026-01-08 16:43:02,370: t15.2025.03.14 val PER: 0.4068
2026-01-08 16:43:02,372: t15.2025.03.16 val PER: 0.2984
2026-01-08 16:43:02,373: t15.2025.03.30 val PER: 0.3966
2026-01-08 16:43:02,375: t15.2025.04.13 val PER: 0.3310
2026-01-08 16:43:02,517: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_7000
2026-01-08 16:43:19,877: Train batch 7200: loss: 20.03 grad norm: 68.35 time: 0.081
2026-01-08 16:43:36,664: Train batch 7400: loss: 19.85 grad norm: 66.41 time: 0.078
2026-01-08 16:43:45,021: Running test after training batch: 7500
2026-01-08 16:43:45,140: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:43:50,195: WER debug example
  GT : you can see the code at this point as well
  PR : you candies seeks the hoods hats it appoints his swells
2026-01-08 16:43:50,236: WER debug example
  GT : how does it keep the cost down
  PR : east holmes sus hits he peaks thus costs tens
2026-01-08 16:44:04,206: Val batch 7500: PER (avg): 0.2561 CTC Loss (avg): 26.5715 WER(5gram): 108.15% (n=256) time: 19.182
2026-01-08 16:44:04,208: WER lens: avg_true_words=5.99 avg_pred_words=7.19 max_pred_words=15
2026-01-08 16:44:04,210: t15.2023.08.13 val PER: 0.2349
2026-01-08 16:44:04,212: t15.2023.08.18 val PER: 0.2313
2026-01-08 16:44:04,214: t15.2023.08.20 val PER: 0.2248
2026-01-08 16:44:04,215: t15.2023.08.25 val PER: 0.2093
2026-01-08 16:44:04,217: t15.2023.08.27 val PER: 0.2942
2026-01-08 16:44:04,218: t15.2023.09.01 val PER: 0.2029
2026-01-08 16:44:04,220: t15.2023.09.03 val PER: 0.2827
2026-01-08 16:44:04,222: t15.2023.09.24 val PER: 0.2269
2026-01-08 16:44:04,223: t15.2023.09.29 val PER: 0.2240
2026-01-08 16:44:04,225: t15.2023.10.01 val PER: 0.2668
2026-01-08 16:44:04,226: t15.2023.10.06 val PER: 0.1981
2026-01-08 16:44:04,228: t15.2023.10.08 val PER: 0.3396
2026-01-08 16:44:04,229: t15.2023.10.13 val PER: 0.3157
2026-01-08 16:44:04,231: t15.2023.10.15 val PER: 0.2512
2026-01-08 16:44:04,232: t15.2023.10.20 val PER: 0.2617
2026-01-08 16:44:04,234: t15.2023.10.22 val PER: 0.2160
2026-01-08 16:44:04,235: t15.2023.11.03 val PER: 0.2795
2026-01-08 16:44:04,237: t15.2023.11.04 val PER: 0.1331
2026-01-08 16:44:04,238: t15.2023.11.17 val PER: 0.1353
2026-01-08 16:44:04,240: t15.2023.11.19 val PER: 0.1317
2026-01-08 16:44:04,242: t15.2023.11.26 val PER: 0.2514
2026-01-08 16:44:04,244: t15.2023.12.03 val PER: 0.2227
2026-01-08 16:44:04,245: t15.2023.12.08 val PER: 0.2290
2026-01-08 16:44:04,247: t15.2023.12.10 val PER: 0.1984
2026-01-08 16:44:04,248: t15.2023.12.17 val PER: 0.2110
2026-01-08 16:44:04,250: t15.2023.12.29 val PER: 0.2615
2026-01-08 16:44:04,252: t15.2024.02.25 val PER: 0.2121
2026-01-08 16:44:04,253: t15.2024.03.08 val PER: 0.3257
2026-01-08 16:44:04,255: t15.2024.03.15 val PER: 0.2808
2026-01-08 16:44:04,256: t15.2024.03.17 val PER: 0.2517
2026-01-08 16:44:04,258: t15.2024.05.10 val PER: 0.2526
2026-01-08 16:44:04,259: t15.2024.06.14 val PER: 0.2587
2026-01-08 16:44:04,261: t15.2024.07.19 val PER: 0.3256
2026-01-08 16:44:04,262: t15.2024.07.21 val PER: 0.1793
2026-01-08 16:44:04,264: t15.2024.07.28 val PER: 0.2265
2026-01-08 16:44:04,267: t15.2025.01.10 val PER: 0.4063
2026-01-08 16:44:04,269: t15.2025.01.12 val PER: 0.2879
2026-01-08 16:44:04,270: t15.2025.03.14 val PER: 0.4024
2026-01-08 16:44:04,272: t15.2025.03.16 val PER: 0.2945
2026-01-08 16:44:04,274: t15.2025.03.30 val PER: 0.3954
2026-01-08 16:44:04,275: t15.2025.04.13 val PER: 0.3310
2026-01-08 16:44:04,419: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_7500
2026-01-08 16:44:12,876: Train batch 7600: loss: 22.17 grad norm: 70.24 time: 0.071
2026-01-08 16:44:29,944: Train batch 7800: loss: 22.09 grad norm: 77.58 time: 0.058
2026-01-08 16:44:47,264: Train batch 8000: loss: 17.87 grad norm: 57.58 time: 0.075
2026-01-08 16:44:47,266: Running test after training batch: 8000
2026-01-08 16:44:47,369: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:44:52,338: WER debug example
  GT : you can see the code at this point as well
  PR : you candies seeks the toehold hats this appointees has wheels
2026-01-08 16:44:52,389: WER debug example
  GT : how does it keep the cost down
  PR : it holds stacy hit hicks thus costs tens
2026-01-08 16:45:08,614: Val batch 8000: PER (avg): 0.2458 CTC Loss (avg): 25.6472 WER(5gram): 105.80% (n=256) time: 21.345
2026-01-08 16:45:08,617: WER lens: avg_true_words=5.99 avg_pred_words=7.30 max_pred_words=15
2026-01-08 16:45:08,619: t15.2023.08.13 val PER: 0.2287
2026-01-08 16:45:08,620: t15.2023.08.18 val PER: 0.2230
2026-01-08 16:45:08,622: t15.2023.08.20 val PER: 0.2105
2026-01-08 16:45:08,624: t15.2023.08.25 val PER: 0.2274
2026-01-08 16:45:08,625: t15.2023.08.27 val PER: 0.3119
2026-01-08 16:45:08,627: t15.2023.09.01 val PER: 0.1875
2026-01-08 16:45:08,629: t15.2023.09.03 val PER: 0.2637
2026-01-08 16:45:08,631: t15.2023.09.24 val PER: 0.2136
2026-01-08 16:45:08,632: t15.2023.09.29 val PER: 0.2202
2026-01-08 16:45:08,635: t15.2023.10.01 val PER: 0.2649
2026-01-08 16:45:08,637: t15.2023.10.06 val PER: 0.1873
2026-01-08 16:45:08,638: t15.2023.10.08 val PER: 0.3153
2026-01-08 16:45:08,640: t15.2023.10.13 val PER: 0.3289
2026-01-08 16:45:08,641: t15.2023.10.15 val PER: 0.2406
2026-01-08 16:45:08,643: t15.2023.10.20 val PER: 0.2416
2026-01-08 16:45:08,645: t15.2023.10.22 val PER: 0.1960
2026-01-08 16:45:08,646: t15.2023.11.03 val PER: 0.2605
2026-01-08 16:45:08,648: t15.2023.11.04 val PER: 0.1160
2026-01-08 16:45:08,650: t15.2023.11.17 val PER: 0.1337
2026-01-08 16:45:08,651: t15.2023.11.19 val PER: 0.1437
2026-01-08 16:45:08,653: t15.2023.11.26 val PER: 0.2500
2026-01-08 16:45:08,654: t15.2023.12.03 val PER: 0.2227
2026-01-08 16:45:08,656: t15.2023.12.08 val PER: 0.2091
2026-01-08 16:45:08,657: t15.2023.12.10 val PER: 0.1879
2026-01-08 16:45:08,659: t15.2023.12.17 val PER: 0.2079
2026-01-08 16:45:08,660: t15.2023.12.29 val PER: 0.2464
2026-01-08 16:45:08,662: t15.2024.02.25 val PER: 0.1952
2026-01-08 16:45:08,664: t15.2024.03.08 val PER: 0.2930
2026-01-08 16:45:08,665: t15.2024.03.15 val PER: 0.2645
2026-01-08 16:45:08,667: t15.2024.03.17 val PER: 0.2287
2026-01-08 16:45:08,670: t15.2024.05.10 val PER: 0.2689
2026-01-08 16:45:08,672: t15.2024.06.14 val PER: 0.2366
2026-01-08 16:45:08,674: t15.2024.07.19 val PER: 0.3045
2026-01-08 16:45:08,675: t15.2024.07.21 val PER: 0.1697
2026-01-08 16:45:08,677: t15.2024.07.28 val PER: 0.2184
2026-01-08 16:45:08,679: t15.2025.01.10 val PER: 0.3802
2026-01-08 16:45:08,680: t15.2025.01.12 val PER: 0.2771
2026-01-08 16:45:08,682: t15.2025.03.14 val PER: 0.3920
2026-01-08 16:45:08,684: t15.2025.03.16 val PER: 0.2984
2026-01-08 16:45:08,685: t15.2025.03.30 val PER: 0.3736
2026-01-08 16:45:08,687: t15.2025.04.13 val PER: 0.3210
2026-01-08 16:45:08,819: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_8000
2026-01-08 16:45:27,104: Train batch 8200: loss: 16.41 grad norm: 52.90 time: 0.057
2026-01-08 16:45:45,310: Train batch 8400: loss: 15.73 grad norm: 60.67 time: 0.066
2026-01-08 16:45:54,588: Running test after training batch: 8500
2026-01-08 16:45:54,695: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:45:59,687: WER debug example
  GT : you can see the code at this point as well
  PR : you candies seeks the toehold hats this to appointees has wheels
2026-01-08 16:45:59,738: WER debug example
  GT : how does it keep the cost down
  PR : it holds sus hits keep curse the scott stetzer
2026-01-08 16:46:16,934: Val batch 8500: PER (avg): 0.2416 CTC Loss (avg): 25.0800 WER(5gram): 107.11% (n=256) time: 22.344
2026-01-08 16:46:16,937: WER lens: avg_true_words=5.99 avg_pred_words=7.43 max_pred_words=15
2026-01-08 16:46:16,939: t15.2023.08.13 val PER: 0.2110
2026-01-08 16:46:16,941: t15.2023.08.18 val PER: 0.2188
2026-01-08 16:46:16,942: t15.2023.08.20 val PER: 0.2065
2026-01-08 16:46:16,944: t15.2023.08.25 val PER: 0.2018
2026-01-08 16:46:16,946: t15.2023.08.27 val PER: 0.2958
2026-01-08 16:46:16,947: t15.2023.09.01 val PER: 0.1834
2026-01-08 16:46:16,949: t15.2023.09.03 val PER: 0.2743
2026-01-08 16:46:16,951: t15.2023.09.24 val PER: 0.2039
2026-01-08 16:46:16,953: t15.2023.09.29 val PER: 0.2183
2026-01-08 16:46:16,955: t15.2023.10.01 val PER: 0.2616
2026-01-08 16:46:16,956: t15.2023.10.06 val PER: 0.1819
2026-01-08 16:46:16,958: t15.2023.10.08 val PER: 0.3302
2026-01-08 16:46:16,959: t15.2023.10.13 val PER: 0.3088
2026-01-08 16:46:16,960: t15.2023.10.15 val PER: 0.2419
2026-01-08 16:46:16,962: t15.2023.10.20 val PER: 0.2450
2026-01-08 16:46:16,963: t15.2023.10.22 val PER: 0.1927
2026-01-08 16:46:16,964: t15.2023.11.03 val PER: 0.2612
2026-01-08 16:46:16,966: t15.2023.11.04 val PER: 0.1092
2026-01-08 16:46:16,967: t15.2023.11.17 val PER: 0.1275
2026-01-08 16:46:16,969: t15.2023.11.19 val PER: 0.1297
2026-01-08 16:46:16,970: t15.2023.11.26 val PER: 0.2464
2026-01-08 16:46:16,971: t15.2023.12.03 val PER: 0.2185
2026-01-08 16:46:16,973: t15.2023.12.08 val PER: 0.2044
2026-01-08 16:46:16,974: t15.2023.12.10 val PER: 0.1800
2026-01-08 16:46:16,976: t15.2023.12.17 val PER: 0.1965
2026-01-08 16:46:16,977: t15.2023.12.29 val PER: 0.2457
2026-01-08 16:46:16,978: t15.2024.02.25 val PER: 0.2065
2026-01-08 16:46:16,980: t15.2024.03.08 val PER: 0.2916
2026-01-08 16:46:16,981: t15.2024.03.15 val PER: 0.2677
2026-01-08 16:46:16,982: t15.2024.03.17 val PER: 0.2364
2026-01-08 16:46:16,983: t15.2024.05.10 val PER: 0.2422
2026-01-08 16:46:16,985: t15.2024.06.14 val PER: 0.2445
2026-01-08 16:46:16,986: t15.2024.07.19 val PER: 0.2986
2026-01-08 16:46:16,987: t15.2024.07.21 val PER: 0.1586
2026-01-08 16:46:16,989: t15.2024.07.28 val PER: 0.2088
2026-01-08 16:46:16,991: t15.2025.01.10 val PER: 0.3815
2026-01-08 16:46:16,993: t15.2025.01.12 val PER: 0.2710
2026-01-08 16:46:16,994: t15.2025.03.14 val PER: 0.4112
2026-01-08 16:46:16,995: t15.2025.03.16 val PER: 0.2801
2026-01-08 16:46:16,996: t15.2025.03.30 val PER: 0.3724
2026-01-08 16:46:16,998: t15.2025.04.13 val PER: 0.3138
2026-01-08 16:46:17,136: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_8500
2026-01-08 16:46:25,947: Train batch 8600: loss: 23.46 grad norm: 66.64 time: 0.057
2026-01-08 16:46:43,984: Train batch 8800: loss: 21.02 grad norm: 67.24 time: 0.063
2026-01-08 16:47:01,681: Train batch 9000: loss: 22.35 grad norm: 69.71 time: 0.076
2026-01-08 16:47:01,689: Running test after training batch: 9000
2026-01-08 16:47:01,805: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:47:06,969: WER debug example
  GT : you can see the code at this point as well
  PR : you'd candies seeks the two hoods hats this appointees has
2026-01-08 16:47:07,020: WER debug example
  GT : how does it keep the cost down
  PR : seed heads sus hits he peaks thus cost setzer
2026-01-08 16:47:21,969: Val batch 9000: PER (avg): 0.2378 CTC Loss (avg): 24.6464 WER(5gram): 109.65% (n=256) time: 20.277
2026-01-08 16:47:21,971: WER lens: avg_true_words=5.99 avg_pred_words=7.50 max_pred_words=15
2026-01-08 16:47:21,973: t15.2023.08.13 val PER: 0.2183
2026-01-08 16:47:21,975: t15.2023.08.18 val PER: 0.2096
2026-01-08 16:47:21,976: t15.2023.08.20 val PER: 0.2010
2026-01-08 16:47:21,978: t15.2023.08.25 val PER: 0.1988
2026-01-08 16:47:21,980: t15.2023.08.27 val PER: 0.3103
2026-01-08 16:47:21,981: t15.2023.09.01 val PER: 0.1843
2026-01-08 16:47:21,983: t15.2023.09.03 val PER: 0.2625
2026-01-08 16:47:21,984: t15.2023.09.24 val PER: 0.2063
2026-01-08 16:47:21,986: t15.2023.09.29 val PER: 0.2157
2026-01-08 16:47:21,987: t15.2023.10.01 val PER: 0.2517
2026-01-08 16:47:21,988: t15.2023.10.06 val PER: 0.1862
2026-01-08 16:47:21,990: t15.2023.10.08 val PER: 0.3221
2026-01-08 16:47:21,991: t15.2023.10.13 val PER: 0.3080
2026-01-08 16:47:21,993: t15.2023.10.15 val PER: 0.2413
2026-01-08 16:47:21,994: t15.2023.10.20 val PER: 0.2584
2026-01-08 16:47:21,997: t15.2023.10.22 val PER: 0.1826
2026-01-08 16:47:21,999: t15.2023.11.03 val PER: 0.2639
2026-01-08 16:47:22,001: t15.2023.11.04 val PER: 0.1126
2026-01-08 16:47:22,003: t15.2023.11.17 val PER: 0.1151
2026-01-08 16:47:22,005: t15.2023.11.19 val PER: 0.1317
2026-01-08 16:47:22,006: t15.2023.11.26 val PER: 0.2355
2026-01-08 16:47:22,007: t15.2023.12.03 val PER: 0.2038
2026-01-08 16:47:22,009: t15.2023.12.08 val PER: 0.1944
2026-01-08 16:47:22,011: t15.2023.12.10 val PER: 0.1932
2026-01-08 16:47:22,012: t15.2023.12.17 val PER: 0.2037
2026-01-08 16:47:22,014: t15.2023.12.29 val PER: 0.2402
2026-01-08 16:47:22,015: t15.2024.02.25 val PER: 0.1910
2026-01-08 16:47:22,017: t15.2024.03.08 val PER: 0.2802
2026-01-08 16:47:22,019: t15.2024.03.15 val PER: 0.2633
2026-01-08 16:47:22,020: t15.2024.03.17 val PER: 0.2315
2026-01-08 16:47:22,021: t15.2024.05.10 val PER: 0.2511
2026-01-08 16:47:22,023: t15.2024.06.14 val PER: 0.2208
2026-01-08 16:47:22,025: t15.2024.07.19 val PER: 0.2953
2026-01-08 16:47:22,026: t15.2024.07.21 val PER: 0.1593
2026-01-08 16:47:22,028: t15.2024.07.28 val PER: 0.2103
2026-01-08 16:47:22,029: t15.2025.01.10 val PER: 0.3678
2026-01-08 16:47:22,030: t15.2025.01.12 val PER: 0.2679
2026-01-08 16:47:22,032: t15.2025.03.14 val PER: 0.3861
2026-01-08 16:47:22,033: t15.2025.03.16 val PER: 0.2827
2026-01-08 16:47:22,035: t15.2025.03.30 val PER: 0.3747
2026-01-08 16:47:22,036: t15.2025.04.13 val PER: 0.2981
2026-01-08 16:47:22,170: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_9000
2026-01-08 16:47:39,669: Train batch 9200: loss: 17.54 grad norm: 59.23 time: 0.058
2026-01-08 16:47:56,738: Train batch 9400: loss: 14.99 grad norm: 53.70 time: 0.070
2026-01-08 16:48:05,239: Running test after training batch: 9500
2026-01-08 16:48:05,344: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:48:10,621: WER debug example
  GT : you can see the code at this point as well
  PR : you'd candies seeks the two hoods hats this appointees has
2026-01-08 16:48:10,676: WER debug example
  GT : how does it keep the cost down
  PR : seed heads sus hits keep curse the scott stetzer
2026-01-08 16:48:26,640: Val batch 9500: PER (avg): 0.2331 CTC Loss (avg): 24.3010 WER(5gram): 109.39% (n=256) time: 21.399
2026-01-08 16:48:26,642: WER lens: avg_true_words=5.99 avg_pred_words=7.49 max_pred_words=14
2026-01-08 16:48:26,644: t15.2023.08.13 val PER: 0.2131
2026-01-08 16:48:26,646: t15.2023.08.18 val PER: 0.2045
2026-01-08 16:48:26,648: t15.2023.08.20 val PER: 0.2025
2026-01-08 16:48:26,650: t15.2023.08.25 val PER: 0.2078
2026-01-08 16:48:26,652: t15.2023.08.27 val PER: 0.3071
2026-01-08 16:48:26,653: t15.2023.09.01 val PER: 0.1648
2026-01-08 16:48:26,656: t15.2023.09.03 val PER: 0.2553
2026-01-08 16:48:26,659: t15.2023.09.24 val PER: 0.2148
2026-01-08 16:48:26,662: t15.2023.09.29 val PER: 0.2068
2026-01-08 16:48:26,664: t15.2023.10.01 val PER: 0.2550
2026-01-08 16:48:26,666: t15.2023.10.06 val PER: 0.1851
2026-01-08 16:48:26,668: t15.2023.10.08 val PER: 0.3221
2026-01-08 16:48:26,670: t15.2023.10.13 val PER: 0.3018
2026-01-08 16:48:26,672: t15.2023.10.15 val PER: 0.2327
2026-01-08 16:48:26,673: t15.2023.10.20 val PER: 0.2517
2026-01-08 16:48:26,675: t15.2023.10.22 val PER: 0.1748
2026-01-08 16:48:26,677: t15.2023.11.03 val PER: 0.2605
2026-01-08 16:48:26,678: t15.2023.11.04 val PER: 0.1058
2026-01-08 16:48:26,680: t15.2023.11.17 val PER: 0.1166
2026-01-08 16:48:26,682: t15.2023.11.19 val PER: 0.1257
2026-01-08 16:48:26,684: t15.2023.11.26 val PER: 0.2333
2026-01-08 16:48:26,686: t15.2023.12.03 val PER: 0.2069
2026-01-08 16:48:26,687: t15.2023.12.08 val PER: 0.1937
2026-01-08 16:48:26,689: t15.2023.12.10 val PER: 0.1761
2026-01-08 16:48:26,691: t15.2023.12.17 val PER: 0.1933
2026-01-08 16:48:26,693: t15.2023.12.29 val PER: 0.2292
2026-01-08 16:48:26,695: t15.2024.02.25 val PER: 0.1868
2026-01-08 16:48:26,697: t15.2024.03.08 val PER: 0.2987
2026-01-08 16:48:26,699: t15.2024.03.15 val PER: 0.2564
2026-01-08 16:48:26,701: t15.2024.03.17 val PER: 0.2190
2026-01-08 16:48:26,703: t15.2024.05.10 val PER: 0.2452
2026-01-08 16:48:26,705: t15.2024.06.14 val PER: 0.2145
2026-01-08 16:48:26,706: t15.2024.07.19 val PER: 0.2762
2026-01-08 16:48:26,708: t15.2024.07.21 val PER: 0.1517
2026-01-08 16:48:26,710: t15.2024.07.28 val PER: 0.2066
2026-01-08 16:48:26,712: t15.2025.01.10 val PER: 0.3623
2026-01-08 16:48:26,714: t15.2025.01.12 val PER: 0.2617
2026-01-08 16:48:26,715: t15.2025.03.14 val PER: 0.3831
2026-01-08 16:48:26,717: t15.2025.03.16 val PER: 0.2866
2026-01-08 16:48:26,719: t15.2025.03.30 val PER: 0.3736
2026-01-08 16:48:26,721: t15.2025.04.13 val PER: 0.3067
2026-01-08 16:48:26,859: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_9500
2026-01-08 16:48:35,319: Train batch 9600: loss: 15.22 grad norm: 53.18 time: 0.076
2026-01-08 16:48:52,304: Train batch 9800: loss: 14.90 grad norm: 55.42 time: 0.065
2026-01-08 16:49:10,176: Train batch 10000: loss: 10.37 grad norm: 46.54 time: 0.063
2026-01-08 16:49:10,178: Running test after training batch: 10000
2026-01-08 16:49:10,277: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:49:15,314: WER debug example
  GT : you can see the code at this point as well
  PR : a few candies seeks the hoods hats this to appointees has swells
2026-01-08 16:49:15,372: WER debug example
  GT : how does it keep the cost down
  PR : seed heads sus hits he piqued the scott stetzer
2026-01-08 16:49:30,832: Val batch 10000: PER (avg): 0.2319 CTC Loss (avg): 24.3649 WER(5gram): 110.17% (n=256) time: 20.651
2026-01-08 16:49:30,834: WER lens: avg_true_words=5.99 avg_pred_words=7.61 max_pred_words=15
2026-01-08 16:49:30,837: t15.2023.08.13 val PER: 0.2162
2026-01-08 16:49:30,838: t15.2023.08.18 val PER: 0.2079
2026-01-08 16:49:30,840: t15.2023.08.20 val PER: 0.1994
2026-01-08 16:49:30,842: t15.2023.08.25 val PER: 0.1958
2026-01-08 16:49:30,844: t15.2023.08.27 val PER: 0.2878
2026-01-08 16:49:30,846: t15.2023.09.01 val PER: 0.1826
2026-01-08 16:49:30,848: t15.2023.09.03 val PER: 0.2589
2026-01-08 16:49:30,851: t15.2023.09.24 val PER: 0.2015
2026-01-08 16:49:30,853: t15.2023.09.29 val PER: 0.2036
2026-01-08 16:49:30,854: t15.2023.10.01 val PER: 0.2338
2026-01-08 16:49:30,856: t15.2023.10.06 val PER: 0.1841
2026-01-08 16:49:30,858: t15.2023.10.08 val PER: 0.3072
2026-01-08 16:49:30,860: t15.2023.10.13 val PER: 0.2995
2026-01-08 16:49:30,861: t15.2023.10.15 val PER: 0.2281
2026-01-08 16:49:30,863: t15.2023.10.20 val PER: 0.2450
2026-01-08 16:49:30,865: t15.2023.10.22 val PER: 0.1893
2026-01-08 16:49:30,866: t15.2023.11.03 val PER: 0.2612
2026-01-08 16:49:30,868: t15.2023.11.04 val PER: 0.1024
2026-01-08 16:49:30,870: t15.2023.11.17 val PER: 0.1198
2026-01-08 16:49:30,871: t15.2023.11.19 val PER: 0.1158
2026-01-08 16:49:30,872: t15.2023.11.26 val PER: 0.2254
2026-01-08 16:49:30,874: t15.2023.12.03 val PER: 0.2038
2026-01-08 16:49:30,876: t15.2023.12.08 val PER: 0.1924
2026-01-08 16:49:30,877: t15.2023.12.10 val PER: 0.1735
2026-01-08 16:49:30,879: t15.2023.12.17 val PER: 0.2058
2026-01-08 16:49:30,880: t15.2023.12.29 val PER: 0.2361
2026-01-08 16:49:30,882: t15.2024.02.25 val PER: 0.1910
2026-01-08 16:49:30,883: t15.2024.03.08 val PER: 0.2831
2026-01-08 16:49:30,885: t15.2024.03.15 val PER: 0.2545
2026-01-08 16:49:30,886: t15.2024.03.17 val PER: 0.2266
2026-01-08 16:49:30,888: t15.2024.05.10 val PER: 0.2481
2026-01-08 16:49:30,890: t15.2024.06.14 val PER: 0.2256
2026-01-08 16:49:30,891: t15.2024.07.19 val PER: 0.2907
2026-01-08 16:49:30,893: t15.2024.07.21 val PER: 0.1545
2026-01-08 16:49:30,894: t15.2024.07.28 val PER: 0.2118
2026-01-08 16:49:30,896: t15.2025.01.10 val PER: 0.3650
2026-01-08 16:49:30,897: t15.2025.01.12 val PER: 0.2610
2026-01-08 16:49:30,899: t15.2025.03.14 val PER: 0.3817
2026-01-08 16:49:30,902: t15.2025.03.16 val PER: 0.2644
2026-01-08 16:49:30,903: t15.2025.03.30 val PER: 0.3621
2026-01-08 16:49:30,905: t15.2025.04.13 val PER: 0.2867
2026-01-08 16:49:31,043: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_10000
2026-01-08 16:49:49,300: Train batch 10200: loss: 14.72 grad norm: 55.26 time: 0.052
2026-01-08 16:50:07,823: Train batch 10400: loss: 16.52 grad norm: 54.74 time: 0.075
2026-01-08 16:50:16,469: Running test after training batch: 10500
2026-01-08 16:50:16,602: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:50:21,740: WER debug example
  GT : you can see the code at this point as well
  PR : a few candies seeks the code's hats this appointees has
2026-01-08 16:50:21,806: WER debug example
  GT : how does it keep the cost down
  PR : seed howard's stacy hit keep curse the scott stetzer
2026-01-08 16:50:38,082: Val batch 10500: PER (avg): 0.2259 CTC Loss (avg): 24.0705 WER(5gram): 107.69% (n=256) time: 21.612
2026-01-08 16:50:38,085: WER lens: avg_true_words=5.99 avg_pred_words=7.40 max_pred_words=15
2026-01-08 16:50:38,087: t15.2023.08.13 val PER: 0.2027
2026-01-08 16:50:38,089: t15.2023.08.18 val PER: 0.2112
2026-01-08 16:50:38,091: t15.2023.08.20 val PER: 0.1930
2026-01-08 16:50:38,093: t15.2023.08.25 val PER: 0.2078
2026-01-08 16:50:38,094: t15.2023.08.27 val PER: 0.2814
2026-01-08 16:50:38,096: t15.2023.09.01 val PER: 0.1721
2026-01-08 16:50:38,097: t15.2023.09.03 val PER: 0.2387
2026-01-08 16:50:38,099: t15.2023.09.24 val PER: 0.2002
2026-01-08 16:50:38,101: t15.2023.09.29 val PER: 0.2080
2026-01-08 16:50:38,102: t15.2023.10.01 val PER: 0.2365
2026-01-08 16:50:38,104: t15.2023.10.06 val PER: 0.1712
2026-01-08 16:50:38,105: t15.2023.10.08 val PER: 0.2977
2026-01-08 16:50:38,107: t15.2023.10.13 val PER: 0.2832
2026-01-08 16:50:38,109: t15.2023.10.15 val PER: 0.2373
2026-01-08 16:50:38,111: t15.2023.10.20 val PER: 0.2617
2026-01-08 16:50:38,112: t15.2023.10.22 val PER: 0.1737
2026-01-08 16:50:38,114: t15.2023.11.03 val PER: 0.2524
2026-01-08 16:50:38,115: t15.2023.11.04 val PER: 0.1126
2026-01-08 16:50:38,117: t15.2023.11.17 val PER: 0.1229
2026-01-08 16:50:38,119: t15.2023.11.19 val PER: 0.1337
2026-01-08 16:50:38,121: t15.2023.11.26 val PER: 0.2109
2026-01-08 16:50:38,122: t15.2023.12.03 val PER: 0.1933
2026-01-08 16:50:38,124: t15.2023.12.08 val PER: 0.1964
2026-01-08 16:50:38,125: t15.2023.12.10 val PER: 0.1682
2026-01-08 16:50:38,127: t15.2023.12.17 val PER: 0.1923
2026-01-08 16:50:38,129: t15.2023.12.29 val PER: 0.2299
2026-01-08 16:50:38,131: t15.2024.02.25 val PER: 0.1938
2026-01-08 16:50:38,132: t15.2024.03.08 val PER: 0.2617
2026-01-08 16:50:38,134: t15.2024.03.15 val PER: 0.2508
2026-01-08 16:50:38,135: t15.2024.03.17 val PER: 0.2266
2026-01-08 16:50:38,138: t15.2024.05.10 val PER: 0.2392
2026-01-08 16:50:38,140: t15.2024.06.14 val PER: 0.2145
2026-01-08 16:50:38,141: t15.2024.07.19 val PER: 0.2755
2026-01-08 16:50:38,143: t15.2024.07.21 val PER: 0.1414
2026-01-08 16:50:38,144: t15.2024.07.28 val PER: 0.1956
2026-01-08 16:50:38,146: t15.2025.01.10 val PER: 0.3733
2026-01-08 16:50:38,147: t15.2025.01.12 val PER: 0.2487
2026-01-08 16:50:38,149: t15.2025.03.14 val PER: 0.3743
2026-01-08 16:50:38,151: t15.2025.03.16 val PER: 0.2552
2026-01-08 16:50:38,152: t15.2025.03.30 val PER: 0.3425
2026-01-08 16:50:38,154: t15.2025.04.13 val PER: 0.2896
2026-01-08 16:50:38,296: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_10500
2026-01-08 16:50:46,889: Train batch 10600: loss: 13.71 grad norm: 59.53 time: 0.076
2026-01-08 16:51:03,974: Train batch 10800: loss: 19.78 grad norm: 68.20 time: 0.067
2026-01-08 16:51:21,731: Train batch 11000: loss: 21.05 grad norm: 73.85 time: 0.058
2026-01-08 16:51:21,733: Running test after training batch: 11000
2026-01-08 16:51:21,879: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:51:26,956: WER debug example
  GT : you can see the code at this point as well
  PR : if you'd like andy's seeks the tuck good hats this appointees has
2026-01-08 16:51:27,020: WER debug example
  GT : how does it keep the cost down
  PR : eat hounds stacy hit key piqued the scott stetzer
2026-01-08 16:51:44,124: Val batch 11000: PER (avg): 0.2262 CTC Loss (avg): 24.0120 WER(5gram): 106.32% (n=256) time: 22.389
2026-01-08 16:51:44,126: WER lens: avg_true_words=5.99 avg_pred_words=7.57 max_pred_words=14
2026-01-08 16:51:44,128: t15.2023.08.13 val PER: 0.2058
2026-01-08 16:51:44,130: t15.2023.08.18 val PER: 0.2054
2026-01-08 16:51:44,133: t15.2023.08.20 val PER: 0.1898
2026-01-08 16:51:44,134: t15.2023.08.25 val PER: 0.2078
2026-01-08 16:51:44,136: t15.2023.08.27 val PER: 0.2701
2026-01-08 16:51:44,137: t15.2023.09.01 val PER: 0.1721
2026-01-08 16:51:44,139: t15.2023.09.03 val PER: 0.2565
2026-01-08 16:51:44,140: t15.2023.09.24 val PER: 0.1942
2026-01-08 16:51:44,142: t15.2023.09.29 val PER: 0.2049
2026-01-08 16:51:44,143: t15.2023.10.01 val PER: 0.2358
2026-01-08 16:51:44,145: t15.2023.10.06 val PER: 0.1765
2026-01-08 16:51:44,146: t15.2023.10.08 val PER: 0.3045
2026-01-08 16:51:44,148: t15.2023.10.13 val PER: 0.2933
2026-01-08 16:51:44,149: t15.2023.10.15 val PER: 0.2235
2026-01-08 16:51:44,151: t15.2023.10.20 val PER: 0.2483
2026-01-08 16:51:44,153: t15.2023.10.22 val PER: 0.1682
2026-01-08 16:51:44,154: t15.2023.11.03 val PER: 0.2571
2026-01-08 16:51:44,156: t15.2023.11.04 val PER: 0.0990
2026-01-08 16:51:44,157: t15.2023.11.17 val PER: 0.1275
2026-01-08 16:51:44,159: t15.2023.11.19 val PER: 0.1317
2026-01-08 16:51:44,160: t15.2023.11.26 val PER: 0.2232
2026-01-08 16:51:44,162: t15.2023.12.03 val PER: 0.1964
2026-01-08 16:51:44,163: t15.2023.12.08 val PER: 0.1864
2026-01-08 16:51:44,165: t15.2023.12.10 val PER: 0.1643
2026-01-08 16:51:44,167: t15.2023.12.17 val PER: 0.1985
2026-01-08 16:51:44,168: t15.2023.12.29 val PER: 0.2279
2026-01-08 16:51:44,170: t15.2024.02.25 val PER: 0.1868
2026-01-08 16:51:44,171: t15.2024.03.08 val PER: 0.2817
2026-01-08 16:51:44,173: t15.2024.03.15 val PER: 0.2464
2026-01-08 16:51:44,174: t15.2024.03.17 val PER: 0.2204
2026-01-08 16:51:44,176: t15.2024.05.10 val PER: 0.2318
2026-01-08 16:51:44,177: t15.2024.06.14 val PER: 0.2098
2026-01-08 16:51:44,179: t15.2024.07.19 val PER: 0.2854
2026-01-08 16:51:44,180: t15.2024.07.21 val PER: 0.1462
2026-01-08 16:51:44,182: t15.2024.07.28 val PER: 0.1978
2026-01-08 16:51:44,183: t15.2025.01.10 val PER: 0.3540
2026-01-08 16:51:44,184: t15.2025.01.12 val PER: 0.2556
2026-01-08 16:51:44,186: t15.2025.03.14 val PER: 0.3891
2026-01-08 16:51:44,187: t15.2025.03.16 val PER: 0.2657
2026-01-08 16:51:44,189: t15.2025.03.30 val PER: 0.3483
2026-01-08 16:51:44,190: t15.2025.04.13 val PER: 0.2796
2026-01-08 16:51:44,322: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_11000
2026-01-08 16:52:01,790: Train batch 11200: loss: 15.36 grad norm: 65.10 time: 0.073
2026-01-08 16:52:19,233: Train batch 11400: loss: 14.70 grad norm: 58.51 time: 0.059
2026-01-08 16:52:27,793: Running test after training batch: 11500
2026-01-08 16:52:27,937: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:52:32,910: WER debug example
  GT : you can see the code at this point as well
  PR : a few candies seeks the code hats this appointees has
2026-01-08 16:52:32,959: WER debug example
  GT : how does it keep the cost down
  PR : east howard's sus hits keep curse the scott stetzer
2026-01-08 16:52:47,491: Val batch 11500: PER (avg): 0.2222 CTC Loss (avg): 23.7765 WER(5gram): 105.61% (n=256) time: 19.696
2026-01-08 16:52:47,494: WER lens: avg_true_words=5.99 avg_pred_words=7.43 max_pred_words=14
2026-01-08 16:52:47,497: t15.2023.08.13 val PER: 0.2048
2026-01-08 16:52:47,500: t15.2023.08.18 val PER: 0.2003
2026-01-08 16:52:47,502: t15.2023.08.20 val PER: 0.1946
2026-01-08 16:52:47,503: t15.2023.08.25 val PER: 0.1928
2026-01-08 16:52:47,505: t15.2023.08.27 val PER: 0.2669
2026-01-08 16:52:47,507: t15.2023.09.01 val PER: 0.1656
2026-01-08 16:52:47,509: t15.2023.09.03 val PER: 0.2613
2026-01-08 16:52:47,511: t15.2023.09.24 val PER: 0.1893
2026-01-08 16:52:47,513: t15.2023.09.29 val PER: 0.2125
2026-01-08 16:52:47,515: t15.2023.10.01 val PER: 0.2391
2026-01-08 16:52:47,516: t15.2023.10.06 val PER: 0.1733
2026-01-08 16:52:47,518: t15.2023.10.08 val PER: 0.3031
2026-01-08 16:52:47,520: t15.2023.10.13 val PER: 0.2847
2026-01-08 16:52:47,522: t15.2023.10.15 val PER: 0.2301
2026-01-08 16:52:47,524: t15.2023.10.20 val PER: 0.2383
2026-01-08 16:52:47,526: t15.2023.10.22 val PER: 0.1748
2026-01-08 16:52:47,528: t15.2023.11.03 val PER: 0.2483
2026-01-08 16:52:47,530: t15.2023.11.04 val PER: 0.1024
2026-01-08 16:52:47,532: t15.2023.11.17 val PER: 0.1213
2026-01-08 16:52:47,534: t15.2023.11.19 val PER: 0.1238
2026-01-08 16:52:47,536: t15.2023.11.26 val PER: 0.2065
2026-01-08 16:52:47,538: t15.2023.12.03 val PER: 0.1985
2026-01-08 16:52:47,540: t15.2023.12.08 val PER: 0.1838
2026-01-08 16:52:47,541: t15.2023.12.10 val PER: 0.1616
2026-01-08 16:52:47,543: t15.2023.12.17 val PER: 0.1684
2026-01-08 16:52:47,545: t15.2023.12.29 val PER: 0.2196
2026-01-08 16:52:47,547: t15.2024.02.25 val PER: 0.1938
2026-01-08 16:52:47,549: t15.2024.03.08 val PER: 0.2660
2026-01-08 16:52:47,551: t15.2024.03.15 val PER: 0.2439
2026-01-08 16:52:47,554: t15.2024.03.17 val PER: 0.2120
2026-01-08 16:52:47,556: t15.2024.05.10 val PER: 0.2229
2026-01-08 16:52:47,558: t15.2024.06.14 val PER: 0.2256
2026-01-08 16:52:47,559: t15.2024.07.19 val PER: 0.2835
2026-01-08 16:52:47,561: t15.2024.07.21 val PER: 0.1434
2026-01-08 16:52:47,563: t15.2024.07.28 val PER: 0.1904
2026-01-08 16:52:47,565: t15.2025.01.10 val PER: 0.3444
2026-01-08 16:52:47,567: t15.2025.01.12 val PER: 0.2479
2026-01-08 16:52:47,568: t15.2025.03.14 val PER: 0.3713
2026-01-08 16:52:47,570: t15.2025.03.16 val PER: 0.2513
2026-01-08 16:52:47,572: t15.2025.03.30 val PER: 0.3471
2026-01-08 16:52:47,574: t15.2025.04.13 val PER: 0.2867
2026-01-08 16:52:47,710: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_11500
2026-01-08 16:52:56,299: Train batch 11600: loss: 17.55 grad norm: 64.59 time: 0.062
2026-01-08 16:53:13,396: Train batch 11800: loss: 13.86 grad norm: 52.48 time: 0.045
2026-01-08 16:53:31,137: Train batch 12000: loss: 18.10 grad norm: 69.97 time: 0.076
2026-01-08 16:53:31,139: Running test after training batch: 12000
2026-01-08 16:53:31,238: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:53:36,318: WER debug example
  GT : you can see the code at this point as well
  PR : a few candies seeks the two hoods hats this two appointees has swells
2026-01-08 16:53:36,368: WER debug example
  GT : how does it keep the cost down
  PR : t howard's stacy hit key peace thus costs
2026-01-08 16:53:49,808: Val batch 12000: PER (avg): 0.2186 CTC Loss (avg): 23.3345 WER(5gram): 107.43% (n=256) time: 18.666
2026-01-08 16:53:49,810: WER lens: avg_true_words=5.99 avg_pred_words=7.47 max_pred_words=16
2026-01-08 16:53:49,812: t15.2023.08.13 val PER: 0.2079
2026-01-08 16:53:49,814: t15.2023.08.18 val PER: 0.1928
2026-01-08 16:53:49,816: t15.2023.08.20 val PER: 0.1875
2026-01-08 16:53:49,819: t15.2023.08.25 val PER: 0.1852
2026-01-08 16:53:49,821: t15.2023.08.27 val PER: 0.2765
2026-01-08 16:53:49,824: t15.2023.09.01 val PER: 0.1648
2026-01-08 16:53:49,825: t15.2023.09.03 val PER: 0.2482
2026-01-08 16:53:49,827: t15.2023.09.24 val PER: 0.1942
2026-01-08 16:53:49,829: t15.2023.09.29 val PER: 0.2023
2026-01-08 16:53:49,830: t15.2023.10.01 val PER: 0.2338
2026-01-08 16:53:49,832: t15.2023.10.06 val PER: 0.1668
2026-01-08 16:53:49,834: t15.2023.10.08 val PER: 0.3004
2026-01-08 16:53:49,835: t15.2023.10.13 val PER: 0.2847
2026-01-08 16:53:49,837: t15.2023.10.15 val PER: 0.2221
2026-01-08 16:53:49,839: t15.2023.10.20 val PER: 0.2315
2026-01-08 16:53:49,841: t15.2023.10.22 val PER: 0.1648
2026-01-08 16:53:49,842: t15.2023.11.03 val PER: 0.2463
2026-01-08 16:53:49,844: t15.2023.11.04 val PER: 0.0922
2026-01-08 16:53:49,846: t15.2023.11.17 val PER: 0.1042
2026-01-08 16:53:49,847: t15.2023.11.19 val PER: 0.1198
2026-01-08 16:53:49,849: t15.2023.11.26 val PER: 0.2101
2026-01-08 16:53:49,851: t15.2023.12.03 val PER: 0.2006
2026-01-08 16:53:49,853: t15.2023.12.08 val PER: 0.1764
2026-01-08 16:53:49,855: t15.2023.12.10 val PER: 0.1669
2026-01-08 16:53:49,858: t15.2023.12.17 val PER: 0.1705
2026-01-08 16:53:49,860: t15.2023.12.29 val PER: 0.2155
2026-01-08 16:53:49,862: t15.2024.02.25 val PER: 0.1798
2026-01-08 16:53:49,864: t15.2024.03.08 val PER: 0.2632
2026-01-08 16:53:49,865: t15.2024.03.15 val PER: 0.2376
2026-01-08 16:53:49,867: t15.2024.03.17 val PER: 0.2078
2026-01-08 16:53:49,869: t15.2024.05.10 val PER: 0.2244
2026-01-08 16:53:49,871: t15.2024.06.14 val PER: 0.2256
2026-01-08 16:53:49,872: t15.2024.07.19 val PER: 0.2690
2026-01-08 16:53:49,874: t15.2024.07.21 val PER: 0.1372
2026-01-08 16:53:49,876: t15.2024.07.28 val PER: 0.1904
2026-01-08 16:53:49,878: t15.2025.01.10 val PER: 0.3554
2026-01-08 16:53:49,880: t15.2025.01.12 val PER: 0.2440
2026-01-08 16:53:49,881: t15.2025.03.14 val PER: 0.3802
2026-01-08 16:53:49,883: t15.2025.03.16 val PER: 0.2618
2026-01-08 16:53:49,885: t15.2025.03.30 val PER: 0.3356
2026-01-08 16:53:49,886: t15.2025.04.13 val PER: 0.2924
2026-01-08 16:53:50,025: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_12000
2026-01-08 16:54:07,081: Train batch 12200: loss: 13.50 grad norm: 54.62 time: 0.069
2026-01-08 16:54:24,700: Train batch 12400: loss: 10.14 grad norm: 46.15 time: 0.042
2026-01-08 16:54:33,760: Running test after training batch: 12500
2026-01-08 16:54:33,900: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:54:39,325: WER debug example
  GT : you can see the code at this point as well
  PR : a few candies seeks the tuck good hats this appointees has swells
2026-01-08 16:54:39,367: WER debug example
  GT : how does it keep the cost down
  PR : t howard's sus hits keep curse thus cost cents
2026-01-08 16:54:52,967: Val batch 12500: PER (avg): 0.2177 CTC Loss (avg): 23.2284 WER(5gram): 105.35% (n=256) time: 19.205
2026-01-08 16:54:52,970: WER lens: avg_true_words=5.99 avg_pred_words=7.46 max_pred_words=14
2026-01-08 16:54:52,972: t15.2023.08.13 val PER: 0.2027
2026-01-08 16:54:52,974: t15.2023.08.18 val PER: 0.1995
2026-01-08 16:54:52,976: t15.2023.08.20 val PER: 0.1819
2026-01-08 16:54:52,978: t15.2023.08.25 val PER: 0.1928
2026-01-08 16:54:52,979: t15.2023.08.27 val PER: 0.2621
2026-01-08 16:54:52,981: t15.2023.09.01 val PER: 0.1583
2026-01-08 16:54:52,983: t15.2023.09.03 val PER: 0.2340
2026-01-08 16:54:52,985: t15.2023.09.24 val PER: 0.1917
2026-01-08 16:54:52,987: t15.2023.09.29 val PER: 0.1966
2026-01-08 16:54:52,988: t15.2023.10.01 val PER: 0.2305
2026-01-08 16:54:52,990: t15.2023.10.06 val PER: 0.1819
2026-01-08 16:54:52,992: t15.2023.10.08 val PER: 0.3018
2026-01-08 16:54:52,994: t15.2023.10.13 val PER: 0.2762
2026-01-08 16:54:52,996: t15.2023.10.15 val PER: 0.2162
2026-01-08 16:54:52,997: t15.2023.10.20 val PER: 0.2617
2026-01-08 16:54:52,999: t15.2023.10.22 val PER: 0.1592
2026-01-08 16:54:53,001: t15.2023.11.03 val PER: 0.2436
2026-01-08 16:54:53,002: t15.2023.11.04 val PER: 0.0990
2026-01-08 16:54:53,004: t15.2023.11.17 val PER: 0.1058
2026-01-08 16:54:53,005: t15.2023.11.19 val PER: 0.1257
2026-01-08 16:54:53,007: t15.2023.11.26 val PER: 0.2072
2026-01-08 16:54:53,009: t15.2023.12.03 val PER: 0.1901
2026-01-08 16:54:53,010: t15.2023.12.08 val PER: 0.1804
2026-01-08 16:54:53,015: t15.2023.12.10 val PER: 0.1643
2026-01-08 16:54:53,016: t15.2023.12.17 val PER: 0.1715
2026-01-08 16:54:53,018: t15.2023.12.29 val PER: 0.2128
2026-01-08 16:54:53,019: t15.2024.02.25 val PER: 0.1840
2026-01-08 16:54:53,021: t15.2024.03.08 val PER: 0.2575
2026-01-08 16:54:53,023: t15.2024.03.15 val PER: 0.2483
2026-01-08 16:54:53,025: t15.2024.03.17 val PER: 0.2050
2026-01-08 16:54:53,027: t15.2024.05.10 val PER: 0.2303
2026-01-08 16:54:53,029: t15.2024.06.14 val PER: 0.2177
2026-01-08 16:54:53,031: t15.2024.07.19 val PER: 0.2683
2026-01-08 16:54:53,032: t15.2024.07.21 val PER: 0.1386
2026-01-08 16:54:53,034: t15.2024.07.28 val PER: 0.1919
2026-01-08 16:54:53,036: t15.2025.01.10 val PER: 0.3554
2026-01-08 16:54:53,037: t15.2025.01.12 val PER: 0.2487
2026-01-08 16:54:53,039: t15.2025.03.14 val PER: 0.3772
2026-01-08 16:54:53,041: t15.2025.03.16 val PER: 0.2670
2026-01-08 16:54:53,042: t15.2025.03.30 val PER: 0.3414
2026-01-08 16:54:53,044: t15.2025.04.13 val PER: 0.2796
2026-01-08 16:54:53,182: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_12500
2026-01-08 16:55:01,699: Train batch 12600: loss: 13.07 grad norm: 54.52 time: 0.060
2026-01-08 16:55:19,466: Train batch 12800: loss: 11.80 grad norm: 51.33 time: 0.054
2026-01-08 16:55:37,400: Train batch 13000: loss: 11.79 grad norm: 52.25 time: 0.068
2026-01-08 16:55:37,402: Running test after training batch: 13000
2026-01-08 16:55:37,499: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:55:42,458: WER debug example
  GT : you can see the code at this point as well
  PR : a few candies seeks the stuccoed hats this appointees has
2026-01-08 16:55:42,504: WER debug example
  GT : how does it keep the cost down
  PR : durkheim's sus hits keep curse thus costs tens
2026-01-08 16:55:55,268: Val batch 13000: PER (avg): 0.2142 CTC Loss (avg): 23.0233 WER(5gram): 105.08% (n=256) time: 17.864
2026-01-08 16:55:55,270: WER lens: avg_true_words=5.99 avg_pred_words=7.41 max_pred_words=15
2026-01-08 16:55:55,271: t15.2023.08.13 val PER: 0.2048
2026-01-08 16:55:55,273: t15.2023.08.18 val PER: 0.1953
2026-01-08 16:55:55,275: t15.2023.08.20 val PER: 0.1843
2026-01-08 16:55:55,276: t15.2023.08.25 val PER: 0.1988
2026-01-08 16:55:55,278: t15.2023.08.27 val PER: 0.2669
2026-01-08 16:55:55,280: t15.2023.09.01 val PER: 0.1591
2026-01-08 16:55:55,282: t15.2023.09.03 val PER: 0.2328
2026-01-08 16:55:55,284: t15.2023.09.24 val PER: 0.1857
2026-01-08 16:55:55,285: t15.2023.09.29 val PER: 0.2004
2026-01-08 16:55:55,287: t15.2023.10.01 val PER: 0.2266
2026-01-08 16:55:55,288: t15.2023.10.06 val PER: 0.1658
2026-01-08 16:55:55,290: t15.2023.10.08 val PER: 0.3031
2026-01-08 16:55:55,292: t15.2023.10.13 val PER: 0.2676
2026-01-08 16:55:55,293: t15.2023.10.15 val PER: 0.2076
2026-01-08 16:55:55,295: t15.2023.10.20 val PER: 0.2450
2026-01-08 16:55:55,297: t15.2023.10.22 val PER: 0.1615
2026-01-08 16:55:55,298: t15.2023.11.03 val PER: 0.2449
2026-01-08 16:55:55,300: t15.2023.11.04 val PER: 0.0853
2026-01-08 16:55:55,302: t15.2023.11.17 val PER: 0.1151
2026-01-08 16:55:55,303: t15.2023.11.19 val PER: 0.1178
2026-01-08 16:55:55,305: t15.2023.11.26 val PER: 0.1913
2026-01-08 16:55:55,307: t15.2023.12.03 val PER: 0.1954
2026-01-08 16:55:55,309: t15.2023.12.08 val PER: 0.1718
2026-01-08 16:55:55,311: t15.2023.12.10 val PER: 0.1603
2026-01-08 16:55:55,313: t15.2023.12.17 val PER: 0.1705
2026-01-08 16:55:55,315: t15.2023.12.29 val PER: 0.2114
2026-01-08 16:55:55,317: t15.2024.02.25 val PER: 0.1657
2026-01-08 16:55:55,318: t15.2024.03.08 val PER: 0.2660
2026-01-08 16:55:55,320: t15.2024.03.15 val PER: 0.2389
2026-01-08 16:55:55,322: t15.2024.03.17 val PER: 0.2015
2026-01-08 16:55:55,324: t15.2024.05.10 val PER: 0.2259
2026-01-08 16:55:55,325: t15.2024.06.14 val PER: 0.2098
2026-01-08 16:55:55,327: t15.2024.07.19 val PER: 0.2742
2026-01-08 16:55:55,328: t15.2024.07.21 val PER: 0.1331
2026-01-08 16:55:55,330: t15.2024.07.28 val PER: 0.1831
2026-01-08 16:55:55,332: t15.2025.01.10 val PER: 0.3416
2026-01-08 16:55:55,333: t15.2025.01.12 val PER: 0.2456
2026-01-08 16:55:55,335: t15.2025.03.14 val PER: 0.3654
2026-01-08 16:55:55,337: t15.2025.03.16 val PER: 0.2474
2026-01-08 16:55:55,339: t15.2025.03.30 val PER: 0.3448
2026-01-08 16:55:55,340: t15.2025.04.13 val PER: 0.2910
2026-01-08 16:55:55,474: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_13000
2026-01-08 16:56:12,156: Train batch 13200: loss: 18.55 grad norm: 75.70 time: 0.057
2026-01-08 16:56:28,945: Train batch 13400: loss: 14.28 grad norm: 64.69 time: 0.064
2026-01-08 16:56:37,312: Running test after training batch: 13500
2026-01-08 16:56:37,428: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:56:42,398: WER debug example
  GT : you can see the code at this point as well
  PR : a few candies seeks the stuccoed hats this two appointees has
2026-01-08 16:56:42,445: WER debug example
  GT : how does it keep the cost down
  PR : t homes sus hits keep curse the scott stetzer
2026-01-08 16:56:56,346: Val batch 13500: PER (avg): 0.2109 CTC Loss (avg): 22.5407 WER(5gram): 103.26% (n=256) time: 19.032
2026-01-08 16:56:56,349: WER lens: avg_true_words=5.99 avg_pred_words=7.42 max_pred_words=16
2026-01-08 16:56:56,351: t15.2023.08.13 val PER: 0.1913
2026-01-08 16:56:56,353: t15.2023.08.18 val PER: 0.1978
2026-01-08 16:56:56,355: t15.2023.08.20 val PER: 0.1724
2026-01-08 16:56:56,356: t15.2023.08.25 val PER: 0.1837
2026-01-08 16:56:56,358: t15.2023.08.27 val PER: 0.2621
2026-01-08 16:56:56,359: t15.2023.09.01 val PER: 0.1567
2026-01-08 16:56:56,361: t15.2023.09.03 val PER: 0.2268
2026-01-08 16:56:56,363: t15.2023.09.24 val PER: 0.1796
2026-01-08 16:56:56,364: t15.2023.09.29 val PER: 0.1985
2026-01-08 16:56:56,366: t15.2023.10.01 val PER: 0.2299
2026-01-08 16:56:56,367: t15.2023.10.06 val PER: 0.1722
2026-01-08 16:56:56,369: t15.2023.10.08 val PER: 0.2801
2026-01-08 16:56:56,370: t15.2023.10.13 val PER: 0.2731
2026-01-08 16:56:56,373: t15.2023.10.15 val PER: 0.2156
2026-01-08 16:56:56,375: t15.2023.10.20 val PER: 0.2248
2026-01-08 16:56:56,378: t15.2023.10.22 val PER: 0.1693
2026-01-08 16:56:56,379: t15.2023.11.03 val PER: 0.2422
2026-01-08 16:56:56,381: t15.2023.11.04 val PER: 0.0990
2026-01-08 16:56:56,382: t15.2023.11.17 val PER: 0.1104
2026-01-08 16:56:56,384: t15.2023.11.19 val PER: 0.1198
2026-01-08 16:56:56,385: t15.2023.11.26 val PER: 0.1986
2026-01-08 16:56:56,387: t15.2023.12.03 val PER: 0.1859
2026-01-08 16:56:56,388: t15.2023.12.08 val PER: 0.1658
2026-01-08 16:56:56,390: t15.2023.12.10 val PER: 0.1511
2026-01-08 16:56:56,391: t15.2023.12.17 val PER: 0.1726
2026-01-08 16:56:56,393: t15.2023.12.29 val PER: 0.2059
2026-01-08 16:56:56,394: t15.2024.02.25 val PER: 0.1615
2026-01-08 16:56:56,395: t15.2024.03.08 val PER: 0.2575
2026-01-08 16:56:56,397: t15.2024.03.15 val PER: 0.2326
2026-01-08 16:56:56,398: t15.2024.03.17 val PER: 0.1980
2026-01-08 16:56:56,400: t15.2024.05.10 val PER: 0.2155
2026-01-08 16:56:56,401: t15.2024.06.14 val PER: 0.2082
2026-01-08 16:56:56,402: t15.2024.07.19 val PER: 0.2690
2026-01-08 16:56:56,404: t15.2024.07.21 val PER: 0.1338
2026-01-08 16:56:56,405: t15.2024.07.28 val PER: 0.1779
2026-01-08 16:56:56,407: t15.2025.01.10 val PER: 0.3471
2026-01-08 16:56:56,408: t15.2025.01.12 val PER: 0.2340
2026-01-08 16:56:56,410: t15.2025.03.14 val PER: 0.3713
2026-01-08 16:56:56,411: t15.2025.03.16 val PER: 0.2461
2026-01-08 16:56:56,412: t15.2025.03.30 val PER: 0.3264
2026-01-08 16:56:56,414: t15.2025.04.13 val PER: 0.2882
2026-01-08 16:56:56,547: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_13500
2026-01-08 16:57:05,592: Train batch 13600: loss: 17.18 grad norm: 65.49 time: 0.064
2026-01-08 16:57:23,487: Train batch 13800: loss: 11.89 grad norm: 52.57 time: 0.057
2026-01-08 16:57:41,653: Train batch 14000: loss: 17.82 grad norm: 68.28 time: 0.052
2026-01-08 16:57:41,655: Running test after training batch: 14000
2026-01-08 16:57:41,774: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:57:46,804: WER debug example
  GT : you can see the code at this point as well
  PR : the uli candies seeks the stuccoed hats this appointees has
2026-01-08 16:57:46,850: WER debug example
  GT : how does it keep the cost down
  PR : howard's sta sees hits key perks the scott stetzer
2026-01-08 16:58:00,073: Val batch 14000: PER (avg): 0.2113 CTC Loss (avg): 22.6742 WER(5gram): 99.74% (n=256) time: 18.415
2026-01-08 16:58:00,075: WER lens: avg_true_words=5.99 avg_pred_words=7.30 max_pred_words=14
2026-01-08 16:58:00,077: t15.2023.08.13 val PER: 0.1923
2026-01-08 16:58:00,079: t15.2023.08.18 val PER: 0.1936
2026-01-08 16:58:00,081: t15.2023.08.20 val PER: 0.1724
2026-01-08 16:58:00,082: t15.2023.08.25 val PER: 0.1777
2026-01-08 16:58:00,084: t15.2023.08.27 val PER: 0.2605
2026-01-08 16:58:00,085: t15.2023.09.01 val PER: 0.1599
2026-01-08 16:58:00,087: t15.2023.09.03 val PER: 0.2268
2026-01-08 16:58:00,088: t15.2023.09.24 val PER: 0.1808
2026-01-08 16:58:00,090: t15.2023.09.29 val PER: 0.1927
2026-01-08 16:58:00,091: t15.2023.10.01 val PER: 0.2180
2026-01-08 16:58:00,093: t15.2023.10.06 val PER: 0.1647
2026-01-08 16:58:00,095: t15.2023.10.08 val PER: 0.2977
2026-01-08 16:58:00,096: t15.2023.10.13 val PER: 0.2785
2026-01-08 16:58:00,098: t15.2023.10.15 val PER: 0.2195
2026-01-08 16:58:00,099: t15.2023.10.20 val PER: 0.2248
2026-01-08 16:58:00,101: t15.2023.10.22 val PER: 0.1748
2026-01-08 16:58:00,102: t15.2023.11.03 val PER: 0.2497
2026-01-08 16:58:00,104: t15.2023.11.04 val PER: 0.0887
2026-01-08 16:58:00,105: t15.2023.11.17 val PER: 0.1135
2026-01-08 16:58:00,107: t15.2023.11.19 val PER: 0.1178
2026-01-08 16:58:00,108: t15.2023.11.26 val PER: 0.1906
2026-01-08 16:58:00,110: t15.2023.12.03 val PER: 0.1880
2026-01-08 16:58:00,111: t15.2023.12.08 val PER: 0.1711
2026-01-08 16:58:00,113: t15.2023.12.10 val PER: 0.1524
2026-01-08 16:58:00,114: t15.2023.12.17 val PER: 0.1653
2026-01-08 16:58:00,116: t15.2023.12.29 val PER: 0.2059
2026-01-08 16:58:00,117: t15.2024.02.25 val PER: 0.1615
2026-01-08 16:58:00,119: t15.2024.03.08 val PER: 0.2617
2026-01-08 16:58:00,120: t15.2024.03.15 val PER: 0.2326
2026-01-08 16:58:00,122: t15.2024.03.17 val PER: 0.2057
2026-01-08 16:58:00,124: t15.2024.05.10 val PER: 0.2244
2026-01-08 16:58:00,125: t15.2024.06.14 val PER: 0.2114
2026-01-08 16:58:00,127: t15.2024.07.19 val PER: 0.2703
2026-01-08 16:58:00,128: t15.2024.07.21 val PER: 0.1324
2026-01-08 16:58:00,131: t15.2024.07.28 val PER: 0.1787
2026-01-08 16:58:00,132: t15.2025.01.10 val PER: 0.3375
2026-01-08 16:58:00,134: t15.2025.01.12 val PER: 0.2371
2026-01-08 16:58:00,135: t15.2025.03.14 val PER: 0.3654
2026-01-08 16:58:00,137: t15.2025.03.16 val PER: 0.2448
2026-01-08 16:58:00,138: t15.2025.03.30 val PER: 0.3483
2026-01-08 16:58:00,140: t15.2025.04.13 val PER: 0.2710
2026-01-08 16:58:00,278: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_14000
2026-01-08 16:58:17,262: Train batch 14200: loss: 14.24 grad norm: 58.69 time: 0.060
2026-01-08 16:58:35,765: Train batch 14400: loss: 11.72 grad norm: 58.20 time: 0.066
2026-01-08 16:58:44,812: Running test after training batch: 14500
2026-01-08 16:58:44,928: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:58:50,018: WER debug example
  GT : you can see the code at this point as well
  PR : a few candies seeks the code's hats this appointees has
2026-01-08 16:58:50,071: WER debug example
  GT : how does it keep the cost down
  PR : holmes sus hits keep curse the scott stetzer
2026-01-08 16:59:03,199: Val batch 14500: PER (avg): 0.2090 CTC Loss (avg): 22.4861 WER(5gram): 101.11% (n=256) time: 18.386
2026-01-08 16:59:03,202: WER lens: avg_true_words=5.99 avg_pred_words=7.31 max_pred_words=15
2026-01-08 16:59:03,205: t15.2023.08.13 val PER: 0.1933
2026-01-08 16:59:03,208: t15.2023.08.18 val PER: 0.1911
2026-01-08 16:59:03,211: t15.2023.08.20 val PER: 0.1755
2026-01-08 16:59:03,214: t15.2023.08.25 val PER: 0.1792
2026-01-08 16:59:03,216: t15.2023.08.27 val PER: 0.2621
2026-01-08 16:59:03,217: t15.2023.09.01 val PER: 0.1591
2026-01-08 16:59:03,219: t15.2023.09.03 val PER: 0.2245
2026-01-08 16:59:03,221: t15.2023.09.24 val PER: 0.1796
2026-01-08 16:59:03,223: t15.2023.09.29 val PER: 0.1927
2026-01-08 16:59:03,225: t15.2023.10.01 val PER: 0.2213
2026-01-08 16:59:03,226: t15.2023.10.06 val PER: 0.1722
2026-01-08 16:59:03,228: t15.2023.10.08 val PER: 0.2991
2026-01-08 16:59:03,231: t15.2023.10.13 val PER: 0.2723
2026-01-08 16:59:03,233: t15.2023.10.15 val PER: 0.2109
2026-01-08 16:59:03,234: t15.2023.10.20 val PER: 0.2349
2026-01-08 16:59:03,236: t15.2023.10.22 val PER: 0.1659
2026-01-08 16:59:03,237: t15.2023.11.03 val PER: 0.2388
2026-01-08 16:59:03,239: t15.2023.11.04 val PER: 0.0853
2026-01-08 16:59:03,241: t15.2023.11.17 val PER: 0.1120
2026-01-08 16:59:03,243: t15.2023.11.19 val PER: 0.1218
2026-01-08 16:59:03,244: t15.2023.11.26 val PER: 0.1855
2026-01-08 16:59:03,246: t15.2023.12.03 val PER: 0.1838
2026-01-08 16:59:03,248: t15.2023.12.08 val PER: 0.1691
2026-01-08 16:59:03,249: t15.2023.12.10 val PER: 0.1485
2026-01-08 16:59:03,251: t15.2023.12.17 val PER: 0.1642
2026-01-08 16:59:03,253: t15.2023.12.29 val PER: 0.2025
2026-01-08 16:59:03,254: t15.2024.02.25 val PER: 0.1699
2026-01-08 16:59:03,256: t15.2024.03.08 val PER: 0.2688
2026-01-08 16:59:03,257: t15.2024.03.15 val PER: 0.2283
2026-01-08 16:59:03,259: t15.2024.03.17 val PER: 0.1918
2026-01-08 16:59:03,260: t15.2024.05.10 val PER: 0.2140
2026-01-08 16:59:03,262: t15.2024.06.14 val PER: 0.2003
2026-01-08 16:59:03,263: t15.2024.07.19 val PER: 0.2544
2026-01-08 16:59:03,265: t15.2024.07.21 val PER: 0.1393
2026-01-08 16:59:03,266: t15.2024.07.28 val PER: 0.1816
2026-01-08 16:59:03,268: t15.2025.01.10 val PER: 0.3333
2026-01-08 16:59:03,269: t15.2025.01.12 val PER: 0.2317
2026-01-08 16:59:03,271: t15.2025.03.14 val PER: 0.3550
2026-01-08 16:59:03,272: t15.2025.03.16 val PER: 0.2552
2026-01-08 16:59:03,274: t15.2025.03.30 val PER: 0.3391
2026-01-08 16:59:03,275: t15.2025.04.13 val PER: 0.2882
2026-01-08 16:59:03,418: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_14500
2026-01-08 16:59:12,381: Train batch 14600: loss: 18.81 grad norm: 71.61 time: 0.060
2026-01-08 16:59:30,342: Train batch 14800: loss: 11.39 grad norm: 49.55 time: 0.052
2026-01-08 16:59:47,975: Train batch 15000: loss: 13.38 grad norm: 54.79 time: 0.054
2026-01-08 16:59:47,977: Running test after training batch: 15000
2026-01-08 16:59:48,106: WER debug GT example: You can see the code at this point as well.
2026-01-08 16:59:53,387: WER debug example
  GT : you can see the code at this point as well
  PR : you candies seeks the good hats this appointees has
2026-01-08 16:59:53,432: WER debug example
  GT : how does it keep the cost down
  PR : holmes sus hits keep curse the scott stetzer
2026-01-08 17:00:06,304: Val batch 15000: PER (avg): 0.2064 CTC Loss (avg): 22.1997 WER(5gram): 100.85% (n=256) time: 18.325
2026-01-08 17:00:06,306: WER lens: avg_true_words=5.99 avg_pred_words=7.26 max_pred_words=14
2026-01-08 17:00:06,309: t15.2023.08.13 val PER: 0.1965
2026-01-08 17:00:06,310: t15.2023.08.18 val PER: 0.1878
2026-01-08 17:00:06,312: t15.2023.08.20 val PER: 0.1676
2026-01-08 17:00:06,313: t15.2023.08.25 val PER: 0.1822
2026-01-08 17:00:06,315: t15.2023.08.27 val PER: 0.2637
2026-01-08 17:00:06,316: t15.2023.09.01 val PER: 0.1591
2026-01-08 17:00:06,317: t15.2023.09.03 val PER: 0.2316
2026-01-08 17:00:06,319: t15.2023.09.24 val PER: 0.1869
2026-01-08 17:00:06,320: t15.2023.09.29 val PER: 0.1914
2026-01-08 17:00:06,322: t15.2023.10.01 val PER: 0.2160
2026-01-08 17:00:06,323: t15.2023.10.06 val PER: 0.1550
2026-01-08 17:00:06,325: t15.2023.10.08 val PER: 0.2869
2026-01-08 17:00:06,326: t15.2023.10.13 val PER: 0.2731
2026-01-08 17:00:06,328: t15.2023.10.15 val PER: 0.2156
2026-01-08 17:00:06,332: t15.2023.10.20 val PER: 0.2315
2026-01-08 17:00:06,333: t15.2023.10.22 val PER: 0.1592
2026-01-08 17:00:06,335: t15.2023.11.03 val PER: 0.2395
2026-01-08 17:00:06,336: t15.2023.11.04 val PER: 0.0922
2026-01-08 17:00:06,339: t15.2023.11.17 val PER: 0.1073
2026-01-08 17:00:06,340: t15.2023.11.19 val PER: 0.1098
2026-01-08 17:00:06,342: t15.2023.11.26 val PER: 0.1790
2026-01-08 17:00:06,343: t15.2023.12.03 val PER: 0.1754
2026-01-08 17:00:06,345: t15.2023.12.08 val PER: 0.1664
2026-01-08 17:00:06,346: t15.2023.12.10 val PER: 0.1419
2026-01-08 17:00:06,348: t15.2023.12.17 val PER: 0.1684
2026-01-08 17:00:06,349: t15.2023.12.29 val PER: 0.1949
2026-01-08 17:00:06,350: t15.2024.02.25 val PER: 0.1601
2026-01-08 17:00:06,352: t15.2024.03.08 val PER: 0.2532
2026-01-08 17:00:06,353: t15.2024.03.15 val PER: 0.2245
2026-01-08 17:00:06,354: t15.2024.03.17 val PER: 0.1855
2026-01-08 17:00:06,356: t15.2024.05.10 val PER: 0.2214
2026-01-08 17:00:06,357: t15.2024.06.14 val PER: 0.2019
2026-01-08 17:00:06,359: t15.2024.07.19 val PER: 0.2604
2026-01-08 17:00:06,360: t15.2024.07.21 val PER: 0.1317
2026-01-08 17:00:06,361: t15.2024.07.28 val PER: 0.1838
2026-01-08 17:00:06,363: t15.2025.01.10 val PER: 0.3306
2026-01-08 17:00:06,364: t15.2025.01.12 val PER: 0.2325
2026-01-08 17:00:06,365: t15.2025.03.14 val PER: 0.3624
2026-01-08 17:00:06,367: t15.2025.03.16 val PER: 0.2461
2026-01-08 17:00:06,368: t15.2025.03.30 val PER: 0.3322
2026-01-08 17:00:06,369: t15.2025.04.13 val PER: 0.2853
2026-01-08 17:00:06,503: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_15000
2026-01-08 17:00:23,795: Train batch 15200: loss: 11.31 grad norm: 46.19 time: 0.060
2026-01-08 17:00:41,175: Train batch 15400: loss: 16.24 grad norm: 67.48 time: 0.052
2026-01-08 17:00:50,124: Running test after training batch: 15500
2026-01-08 17:00:50,246: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:00:55,478: WER debug example
  GT : you can see the code at this point as well
  PR : you candies seeks the stuccoed hats this appointees has swells
2026-01-08 17:00:55,524: WER debug example
  GT : how does it keep the cost down
  PR : house dust hits keep curse the scott stetzer
2026-01-08 17:01:08,479: Val batch 15500: PER (avg): 0.2074 CTC Loss (avg): 22.1719 WER(5gram): 100.13% (n=256) time: 18.353
2026-01-08 17:01:08,482: WER lens: avg_true_words=5.99 avg_pred_words=7.23 max_pred_words=15
2026-01-08 17:01:08,484: t15.2023.08.13 val PER: 0.1913
2026-01-08 17:01:08,486: t15.2023.08.18 val PER: 0.1945
2026-01-08 17:01:08,488: t15.2023.08.20 val PER: 0.1692
2026-01-08 17:01:08,489: t15.2023.08.25 val PER: 0.1807
2026-01-08 17:01:08,491: t15.2023.08.27 val PER: 0.2669
2026-01-08 17:01:08,493: t15.2023.09.01 val PER: 0.1583
2026-01-08 17:01:08,494: t15.2023.09.03 val PER: 0.2328
2026-01-08 17:01:08,496: t15.2023.09.24 val PER: 0.1784
2026-01-08 17:01:08,497: t15.2023.09.29 val PER: 0.1870
2026-01-08 17:01:08,499: t15.2023.10.01 val PER: 0.2219
2026-01-08 17:01:08,501: t15.2023.10.06 val PER: 0.1658
2026-01-08 17:01:08,503: t15.2023.10.08 val PER: 0.2842
2026-01-08 17:01:08,504: t15.2023.10.13 val PER: 0.2684
2026-01-08 17:01:08,506: t15.2023.10.15 val PER: 0.2189
2026-01-08 17:01:08,507: t15.2023.10.20 val PER: 0.2315
2026-01-08 17:01:08,509: t15.2023.10.22 val PER: 0.1503
2026-01-08 17:01:08,511: t15.2023.11.03 val PER: 0.2388
2026-01-08 17:01:08,515: t15.2023.11.04 val PER: 0.0819
2026-01-08 17:01:08,517: t15.2023.11.17 val PER: 0.1120
2026-01-08 17:01:08,519: t15.2023.11.19 val PER: 0.1078
2026-01-08 17:01:08,520: t15.2023.11.26 val PER: 0.1913
2026-01-08 17:01:08,522: t15.2023.12.03 val PER: 0.1849
2026-01-08 17:01:08,524: t15.2023.12.08 val PER: 0.1625
2026-01-08 17:01:08,526: t15.2023.12.10 val PER: 0.1472
2026-01-08 17:01:08,527: t15.2023.12.17 val PER: 0.1622
2026-01-08 17:01:08,529: t15.2023.12.29 val PER: 0.1942
2026-01-08 17:01:08,531: t15.2024.02.25 val PER: 0.1657
2026-01-08 17:01:08,532: t15.2024.03.08 val PER: 0.2461
2026-01-08 17:01:08,534: t15.2024.03.15 val PER: 0.2233
2026-01-08 17:01:08,536: t15.2024.03.17 val PER: 0.1932
2026-01-08 17:01:08,537: t15.2024.05.10 val PER: 0.2244
2026-01-08 17:01:08,539: t15.2024.06.14 val PER: 0.2019
2026-01-08 17:01:08,541: t15.2024.07.19 val PER: 0.2591
2026-01-08 17:01:08,543: t15.2024.07.21 val PER: 0.1345
2026-01-08 17:01:08,544: t15.2024.07.28 val PER: 0.1853
2026-01-08 17:01:08,546: t15.2025.01.10 val PER: 0.3375
2026-01-08 17:01:08,547: t15.2025.01.12 val PER: 0.2302
2026-01-08 17:01:08,549: t15.2025.03.14 val PER: 0.3802
2026-01-08 17:01:08,551: t15.2025.03.16 val PER: 0.2526
2026-01-08 17:01:08,553: t15.2025.03.30 val PER: 0.3345
2026-01-08 17:01:08,554: t15.2025.04.13 val PER: 0.2739
2026-01-08 17:01:08,691: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_15500
2026-01-08 17:01:17,708: Train batch 15600: loss: 19.61 grad norm: 72.31 time: 0.064
2026-01-08 17:01:34,884: Train batch 15800: loss: 19.85 grad norm: 66.50 time: 0.069
2026-01-08 17:01:52,901: Train batch 16000: loss: 14.03 grad norm: 68.47 time: 0.058
2026-01-08 17:01:52,904: Running test after training batch: 16000
2026-01-08 17:01:53,047: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:01:57,995: WER debug example
  GT : you can see the code at this point as well
  PR : a few candies seeks the good hats this appointees has swells
2026-01-08 17:01:58,039: WER debug example
  GT : how does it keep the cost down
  PR : howard's sus hits keep curse the scott stetzer
2026-01-08 17:02:11,036: Val batch 16000: PER (avg): 0.2048 CTC Loss (avg): 22.0875 WER(5gram): 99.67% (n=256) time: 18.130
2026-01-08 17:02:11,039: WER lens: avg_true_words=5.99 avg_pred_words=7.11 max_pred_words=15
2026-01-08 17:02:11,042: t15.2023.08.13 val PER: 0.1933
2026-01-08 17:02:11,044: t15.2023.08.18 val PER: 0.1928
2026-01-08 17:02:11,047: t15.2023.08.20 val PER: 0.1684
2026-01-08 17:02:11,049: t15.2023.08.25 val PER: 0.1702
2026-01-08 17:02:11,050: t15.2023.08.27 val PER: 0.2588
2026-01-08 17:02:11,052: t15.2023.09.01 val PER: 0.1542
2026-01-08 17:02:11,054: t15.2023.09.03 val PER: 0.2162
2026-01-08 17:02:11,056: t15.2023.09.24 val PER: 0.1748
2026-01-08 17:02:11,058: t15.2023.09.29 val PER: 0.1940
2026-01-08 17:02:11,062: t15.2023.10.01 val PER: 0.2186
2026-01-08 17:02:11,064: t15.2023.10.06 val PER: 0.1615
2026-01-08 17:02:11,066: t15.2023.10.08 val PER: 0.2950
2026-01-08 17:02:11,068: t15.2023.10.13 val PER: 0.2669
2026-01-08 17:02:11,070: t15.2023.10.15 val PER: 0.2044
2026-01-08 17:02:11,071: t15.2023.10.20 val PER: 0.2282
2026-01-08 17:02:11,073: t15.2023.10.22 val PER: 0.1592
2026-01-08 17:02:11,075: t15.2023.11.03 val PER: 0.2354
2026-01-08 17:02:11,077: t15.2023.11.04 val PER: 0.0819
2026-01-08 17:02:11,079: t15.2023.11.17 val PER: 0.1073
2026-01-08 17:02:11,081: t15.2023.11.19 val PER: 0.1058
2026-01-08 17:02:11,083: t15.2023.11.26 val PER: 0.1841
2026-01-08 17:02:11,084: t15.2023.12.03 val PER: 0.1817
2026-01-08 17:02:11,086: t15.2023.12.08 val PER: 0.1618
2026-01-08 17:02:11,088: t15.2023.12.10 val PER: 0.1419
2026-01-08 17:02:11,090: t15.2023.12.17 val PER: 0.1653
2026-01-08 17:02:11,091: t15.2023.12.29 val PER: 0.1935
2026-01-08 17:02:11,093: t15.2024.02.25 val PER: 0.1657
2026-01-08 17:02:11,095: t15.2024.03.08 val PER: 0.2475
2026-01-08 17:02:11,097: t15.2024.03.15 val PER: 0.2270
2026-01-08 17:02:11,098: t15.2024.03.17 val PER: 0.1855
2026-01-08 17:02:11,100: t15.2024.05.10 val PER: 0.2155
2026-01-08 17:02:11,102: t15.2024.06.14 val PER: 0.2066
2026-01-08 17:02:11,103: t15.2024.07.19 val PER: 0.2544
2026-01-08 17:02:11,105: t15.2024.07.21 val PER: 0.1372
2026-01-08 17:02:11,107: t15.2024.07.28 val PER: 0.1801
2026-01-08 17:02:11,108: t15.2025.01.10 val PER: 0.3347
2026-01-08 17:02:11,110: t15.2025.01.12 val PER: 0.2363
2026-01-08 17:02:11,112: t15.2025.03.14 val PER: 0.3476
2026-01-08 17:02:11,114: t15.2025.03.16 val PER: 0.2448
2026-01-08 17:02:11,115: t15.2025.03.30 val PER: 0.3299
2026-01-08 17:02:11,117: t15.2025.04.13 val PER: 0.2767
2026-01-08 17:02:11,258: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_16000
2026-01-08 17:02:28,373: Train batch 16200: loss: 11.12 grad norm: 121.43 time: 0.057
2026-01-08 17:02:45,357: Train batch 16400: loss: 15.14 grad norm: 66.40 time: 0.059
2026-01-08 17:02:53,848: Running test after training batch: 16500
2026-01-08 17:02:53,974: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:02:58,977: WER debug example
  GT : you can see the code at this point as well
  PR : a few candies seeks the good hats this appointees has
2026-01-08 17:02:59,023: WER debug example
  GT : how does it keep the cost down
  PR : howard's sta seas hit keep curse the scott stetzer
2026-01-08 17:03:11,869: Val batch 16500: PER (avg): 0.2043 CTC Loss (avg): 22.1580 WER(5gram): 99.09% (n=256) time: 18.019
2026-01-08 17:03:11,871: WER lens: avg_true_words=5.99 avg_pred_words=7.09 max_pred_words=14
2026-01-08 17:03:11,873: t15.2023.08.13 val PER: 0.1996
2026-01-08 17:03:11,875: t15.2023.08.18 val PER: 0.1936
2026-01-08 17:03:11,876: t15.2023.08.20 val PER: 0.1724
2026-01-08 17:03:11,878: t15.2023.08.25 val PER: 0.1807
2026-01-08 17:03:11,881: t15.2023.08.27 val PER: 0.2637
2026-01-08 17:03:11,882: t15.2023.09.01 val PER: 0.1485
2026-01-08 17:03:11,884: t15.2023.09.03 val PER: 0.2292
2026-01-08 17:03:11,885: t15.2023.09.24 val PER: 0.1748
2026-01-08 17:03:11,887: t15.2023.09.29 val PER: 0.1927
2026-01-08 17:03:11,888: t15.2023.10.01 val PER: 0.2252
2026-01-08 17:03:11,890: t15.2023.10.06 val PER: 0.1647
2026-01-08 17:03:11,892: t15.2023.10.08 val PER: 0.2869
2026-01-08 17:03:11,893: t15.2023.10.13 val PER: 0.2638
2026-01-08 17:03:11,896: t15.2023.10.15 val PER: 0.2090
2026-01-08 17:03:11,897: t15.2023.10.20 val PER: 0.2181
2026-01-08 17:03:11,899: t15.2023.10.22 val PER: 0.1503
2026-01-08 17:03:11,901: t15.2023.11.03 val PER: 0.2341
2026-01-08 17:03:11,902: t15.2023.11.04 val PER: 0.0887
2026-01-08 17:03:11,904: t15.2023.11.17 val PER: 0.1042
2026-01-08 17:03:11,906: t15.2023.11.19 val PER: 0.1038
2026-01-08 17:03:11,907: t15.2023.11.26 val PER: 0.1812
2026-01-08 17:03:11,909: t15.2023.12.03 val PER: 0.1786
2026-01-08 17:03:11,911: t15.2023.12.08 val PER: 0.1585
2026-01-08 17:03:11,912: t15.2023.12.10 val PER: 0.1406
2026-01-08 17:03:11,914: t15.2023.12.17 val PER: 0.1622
2026-01-08 17:03:11,916: t15.2023.12.29 val PER: 0.1949
2026-01-08 17:03:11,917: t15.2024.02.25 val PER: 0.1587
2026-01-08 17:03:11,919: t15.2024.03.08 val PER: 0.2447
2026-01-08 17:03:11,921: t15.2024.03.15 val PER: 0.2283
2026-01-08 17:03:11,923: t15.2024.03.17 val PER: 0.1897
2026-01-08 17:03:11,924: t15.2024.05.10 val PER: 0.2184
2026-01-08 17:03:11,926: t15.2024.06.14 val PER: 0.2035
2026-01-08 17:03:11,928: t15.2024.07.19 val PER: 0.2551
2026-01-08 17:03:11,930: t15.2024.07.21 val PER: 0.1290
2026-01-08 17:03:11,931: t15.2024.07.28 val PER: 0.1824
2026-01-08 17:03:11,933: t15.2025.01.10 val PER: 0.3251
2026-01-08 17:03:11,935: t15.2025.01.12 val PER: 0.2217
2026-01-08 17:03:11,937: t15.2025.03.14 val PER: 0.3669
2026-01-08 17:03:11,938: t15.2025.03.16 val PER: 0.2408
2026-01-08 17:03:11,940: t15.2025.03.30 val PER: 0.3299
2026-01-08 17:03:11,942: t15.2025.04.13 val PER: 0.2725
2026-01-08 17:03:12,074: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_16500
2026-01-08 17:03:21,217: Train batch 16600: loss: 15.97 grad norm: 58.64 time: 0.058
2026-01-08 17:03:39,995: Train batch 16800: loss: 20.68 grad norm: 80.04 time: 0.068
2026-01-08 17:03:58,613: Train batch 17000: loss: 14.21 grad norm: 64.60 time: 0.085
2026-01-08 17:03:58,615: Running test after training batch: 17000
2026-01-08 17:03:58,720: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:04:03,694: WER debug example
  GT : you can see the code at this point as well
  PR : you candies seeks the code hats this appointees has swells
2026-01-08 17:04:03,740: WER debug example
  GT : how does it keep the cost down
  PR : house dust hit keep curse thus costs tens
2026-01-08 17:04:16,483: Val batch 17000: PER (avg): 0.2045 CTC Loss (avg): 22.3909 WER(5gram): 100.46% (n=256) time: 17.864
2026-01-08 17:04:16,485: WER lens: avg_true_words=5.99 avg_pred_words=7.18 max_pred_words=14
2026-01-08 17:04:16,488: t15.2023.08.13 val PER: 0.1933
2026-01-08 17:04:16,490: t15.2023.08.18 val PER: 0.1869
2026-01-08 17:04:16,491: t15.2023.08.20 val PER: 0.1700
2026-01-08 17:04:16,493: t15.2023.08.25 val PER: 0.1807
2026-01-08 17:04:16,494: t15.2023.08.27 val PER: 0.2572
2026-01-08 17:04:16,496: t15.2023.09.01 val PER: 0.1542
2026-01-08 17:04:16,498: t15.2023.09.03 val PER: 0.2268
2026-01-08 17:04:16,499: t15.2023.09.24 val PER: 0.1772
2026-01-08 17:04:16,501: t15.2023.09.29 val PER: 0.1870
2026-01-08 17:04:16,503: t15.2023.10.01 val PER: 0.2219
2026-01-08 17:04:16,504: t15.2023.10.06 val PER: 0.1625
2026-01-08 17:04:16,506: t15.2023.10.08 val PER: 0.2963
2026-01-08 17:04:16,507: t15.2023.10.13 val PER: 0.2653
2026-01-08 17:04:16,509: t15.2023.10.15 val PER: 0.2149
2026-01-08 17:04:16,510: t15.2023.10.20 val PER: 0.2148
2026-01-08 17:04:16,512: t15.2023.10.22 val PER: 0.1570
2026-01-08 17:04:16,514: t15.2023.11.03 val PER: 0.2381
2026-01-08 17:04:16,515: t15.2023.11.04 val PER: 0.0819
2026-01-08 17:04:16,517: t15.2023.11.17 val PER: 0.1026
2026-01-08 17:04:16,518: t15.2023.11.19 val PER: 0.1118
2026-01-08 17:04:16,520: t15.2023.11.26 val PER: 0.1899
2026-01-08 17:04:16,521: t15.2023.12.03 val PER: 0.1765
2026-01-08 17:04:16,523: t15.2023.12.08 val PER: 0.1585
2026-01-08 17:04:16,525: t15.2023.12.10 val PER: 0.1445
2026-01-08 17:04:16,526: t15.2023.12.17 val PER: 0.1653
2026-01-08 17:04:16,528: t15.2023.12.29 val PER: 0.1929
2026-01-08 17:04:16,529: t15.2024.02.25 val PER: 0.1559
2026-01-08 17:04:16,531: t15.2024.03.08 val PER: 0.2546
2026-01-08 17:04:16,532: t15.2024.03.15 val PER: 0.2258
2026-01-08 17:04:16,534: t15.2024.03.17 val PER: 0.1876
2026-01-08 17:04:16,536: t15.2024.05.10 val PER: 0.2140
2026-01-08 17:04:16,537: t15.2024.06.14 val PER: 0.2003
2026-01-08 17:04:16,539: t15.2024.07.19 val PER: 0.2512
2026-01-08 17:04:16,540: t15.2024.07.21 val PER: 0.1345
2026-01-08 17:04:16,541: t15.2024.07.28 val PER: 0.1794
2026-01-08 17:04:16,543: t15.2025.01.10 val PER: 0.3375
2026-01-08 17:04:16,544: t15.2025.01.12 val PER: 0.2309
2026-01-08 17:04:16,548: t15.2025.03.14 val PER: 0.3506
2026-01-08 17:04:16,549: t15.2025.03.16 val PER: 0.2421
2026-01-08 17:04:16,551: t15.2025.03.30 val PER: 0.3276
2026-01-08 17:04:16,552: t15.2025.04.13 val PER: 0.2653
2026-01-08 17:04:16,690: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_17000
2026-01-08 17:04:33,941: Train batch 17200: loss: 15.63 grad norm: 63.37 time: 0.088
2026-01-08 17:04:51,651: Train batch 17400: loss: 16.98 grad norm: 66.42 time: 0.074
2026-01-08 17:05:00,432: Running test after training batch: 17500
2026-01-08 17:05:00,569: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:05:05,569: WER debug example
  GT : you can see the code at this point as well
  PR : a few candies seeks the good hats this appointees has
2026-01-08 17:05:05,617: WER debug example
  GT : how does it keep the cost down
  PR : howard's sta c hit keep curse the scott stetzer
2026-01-08 17:05:18,779: Val batch 17500: PER (avg): 0.2027 CTC Loss (avg): 21.7592 WER(5gram): 100.07% (n=256) time: 18.345
2026-01-08 17:05:18,782: WER lens: avg_true_words=5.99 avg_pred_words=7.22 max_pred_words=15
2026-01-08 17:05:18,784: t15.2023.08.13 val PER: 0.1881
2026-01-08 17:05:18,785: t15.2023.08.18 val PER: 0.1878
2026-01-08 17:05:18,787: t15.2023.08.20 val PER: 0.1636
2026-01-08 17:05:18,789: t15.2023.08.25 val PER: 0.1747
2026-01-08 17:05:18,791: t15.2023.08.27 val PER: 0.2637
2026-01-08 17:05:18,792: t15.2023.09.01 val PER: 0.1494
2026-01-08 17:05:18,794: t15.2023.09.03 val PER: 0.2233
2026-01-08 17:05:18,796: t15.2023.09.24 val PER: 0.1784
2026-01-08 17:05:18,797: t15.2023.09.29 val PER: 0.1889
2026-01-08 17:05:18,799: t15.2023.10.01 val PER: 0.2246
2026-01-08 17:05:18,801: t15.2023.10.06 val PER: 0.1615
2026-01-08 17:05:18,802: t15.2023.10.08 val PER: 0.2801
2026-01-08 17:05:18,804: t15.2023.10.13 val PER: 0.2723
2026-01-08 17:05:18,806: t15.2023.10.15 val PER: 0.2044
2026-01-08 17:05:18,807: t15.2023.10.20 val PER: 0.2248
2026-01-08 17:05:18,809: t15.2023.10.22 val PER: 0.1481
2026-01-08 17:05:18,810: t15.2023.11.03 val PER: 0.2334
2026-01-08 17:05:18,813: t15.2023.11.04 val PER: 0.0887
2026-01-08 17:05:18,814: t15.2023.11.17 val PER: 0.1042
2026-01-08 17:05:18,816: t15.2023.11.19 val PER: 0.1058
2026-01-08 17:05:18,817: t15.2023.11.26 val PER: 0.1797
2026-01-08 17:05:18,819: t15.2023.12.03 val PER: 0.1702
2026-01-08 17:05:18,820: t15.2023.12.08 val PER: 0.1605
2026-01-08 17:05:18,822: t15.2023.12.10 val PER: 0.1406
2026-01-08 17:05:18,823: t15.2023.12.17 val PER: 0.1538
2026-01-08 17:05:18,825: t15.2023.12.29 val PER: 0.1997
2026-01-08 17:05:18,826: t15.2024.02.25 val PER: 0.1587
2026-01-08 17:05:18,828: t15.2024.03.08 val PER: 0.2532
2026-01-08 17:05:18,830: t15.2024.03.15 val PER: 0.2239
2026-01-08 17:05:18,831: t15.2024.03.17 val PER: 0.1848
2026-01-08 17:05:18,833: t15.2024.05.10 val PER: 0.2155
2026-01-08 17:05:18,834: t15.2024.06.14 val PER: 0.1987
2026-01-08 17:05:18,835: t15.2024.07.19 val PER: 0.2492
2026-01-08 17:05:18,837: t15.2024.07.21 val PER: 0.1303
2026-01-08 17:05:18,838: t15.2024.07.28 val PER: 0.1787
2026-01-08 17:05:18,839: t15.2025.01.10 val PER: 0.3223
2026-01-08 17:05:18,841: t15.2025.01.12 val PER: 0.2263
2026-01-08 17:05:18,842: t15.2025.03.14 val PER: 0.3609
2026-01-08 17:05:18,844: t15.2025.03.16 val PER: 0.2513
2026-01-08 17:05:18,845: t15.2025.03.30 val PER: 0.3299
2026-01-08 17:05:18,847: t15.2025.04.13 val PER: 0.2753
2026-01-08 17:05:18,984: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_17500
2026-01-08 17:05:27,309: Train batch 17600: loss: 17.57 grad norm: 60.02 time: 0.053
2026-01-08 17:05:44,520: Train batch 17800: loss: 11.71 grad norm: 52.88 time: 0.045
2026-01-08 17:06:01,923: Train batch 18000: loss: 13.15 grad norm: 63.12 time: 0.072
2026-01-08 17:06:01,925: Running test after training batch: 18000
2026-01-08 17:06:02,049: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:06:07,079: WER debug example
  GT : you can see the code at this point as well
  PR : you candies seeks the code hats this appointees has
2026-01-08 17:06:07,122: WER debug example
  GT : how does it keep the cost down
  PR : howard's sta c hit keep curse the scott stetzer
2026-01-08 17:06:20,042: Val batch 18000: PER (avg): 0.2034 CTC Loss (avg): 22.0456 WER(5gram): 99.35% (n=256) time: 18.115
2026-01-08 17:06:20,044: WER lens: avg_true_words=5.99 avg_pred_words=7.16 max_pred_words=14
2026-01-08 17:06:20,046: t15.2023.08.13 val PER: 0.1923
2026-01-08 17:06:20,048: t15.2023.08.18 val PER: 0.1861
2026-01-08 17:06:20,050: t15.2023.08.20 val PER: 0.1676
2026-01-08 17:06:20,052: t15.2023.08.25 val PER: 0.1762
2026-01-08 17:06:20,053: t15.2023.08.27 val PER: 0.2556
2026-01-08 17:06:20,055: t15.2023.09.01 val PER: 0.1534
2026-01-08 17:06:20,057: t15.2023.09.03 val PER: 0.2268
2026-01-08 17:06:20,059: t15.2023.09.24 val PER: 0.1735
2026-01-08 17:06:20,060: t15.2023.09.29 val PER: 0.1908
2026-01-08 17:06:20,062: t15.2023.10.01 val PER: 0.2266
2026-01-08 17:06:20,064: t15.2023.10.06 val PER: 0.1604
2026-01-08 17:06:20,065: t15.2023.10.08 val PER: 0.2963
2026-01-08 17:06:20,067: t15.2023.10.13 val PER: 0.2708
2026-01-08 17:06:20,069: t15.2023.10.15 val PER: 0.2109
2026-01-08 17:06:20,071: t15.2023.10.20 val PER: 0.2148
2026-01-08 17:06:20,072: t15.2023.10.22 val PER: 0.1559
2026-01-08 17:06:20,074: t15.2023.11.03 val PER: 0.2395
2026-01-08 17:06:20,075: t15.2023.11.04 val PER: 0.0819
2026-01-08 17:06:20,077: t15.2023.11.17 val PER: 0.1026
2026-01-08 17:06:20,078: t15.2023.11.19 val PER: 0.1058
2026-01-08 17:06:20,080: t15.2023.11.26 val PER: 0.1797
2026-01-08 17:06:20,082: t15.2023.12.03 val PER: 0.1744
2026-01-08 17:06:20,083: t15.2023.12.08 val PER: 0.1631
2026-01-08 17:06:20,085: t15.2023.12.10 val PER: 0.1393
2026-01-08 17:06:20,087: t15.2023.12.17 val PER: 0.1590
2026-01-08 17:06:20,088: t15.2023.12.29 val PER: 0.1949
2026-01-08 17:06:20,090: t15.2024.02.25 val PER: 0.1601
2026-01-08 17:06:20,092: t15.2024.03.08 val PER: 0.2546
2026-01-08 17:06:20,094: t15.2024.03.15 val PER: 0.2145
2026-01-08 17:06:20,096: t15.2024.03.17 val PER: 0.1855
2026-01-08 17:06:20,098: t15.2024.05.10 val PER: 0.2125
2026-01-08 17:06:20,100: t15.2024.06.14 val PER: 0.2019
2026-01-08 17:06:20,103: t15.2024.07.19 val PER: 0.2591
2026-01-08 17:06:20,104: t15.2024.07.21 val PER: 0.1283
2026-01-08 17:06:20,106: t15.2024.07.28 val PER: 0.1743
2026-01-08 17:06:20,108: t15.2025.01.10 val PER: 0.3292
2026-01-08 17:06:20,109: t15.2025.01.12 val PER: 0.2286
2026-01-08 17:06:20,111: t15.2025.03.14 val PER: 0.3521
2026-01-08 17:06:20,112: t15.2025.03.16 val PER: 0.2435
2026-01-08 17:06:20,114: t15.2025.03.30 val PER: 0.3310
2026-01-08 17:06:20,115: t15.2025.04.13 val PER: 0.2653
2026-01-08 17:06:20,250: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_18000
2026-01-08 17:06:38,868: Train batch 18200: loss: 13.63 grad norm: 57.79 time: 0.079
2026-01-08 17:06:57,220: Train batch 18400: loss: 9.61 grad norm: 48.76 time: 0.063
2026-01-08 17:07:06,359: Running test after training batch: 18500
2026-01-08 17:07:06,506: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:07:11,809: WER debug example
  GT : you can see the code at this point as well
  PR : a few candies seeks the good hats this appointees has swells
2026-01-08 17:07:11,850: WER debug example
  GT : how does it keep the cost down
  PR : howard's sta c hit keep curse the scott stetzer
2026-01-08 17:07:24,452: Val batch 18500: PER (avg): 0.2032 CTC Loss (avg): 21.6977 WER(5gram): 99.54% (n=256) time: 18.091
2026-01-08 17:07:24,454: WER lens: avg_true_words=5.99 avg_pred_words=7.19 max_pred_words=14
2026-01-08 17:07:24,456: t15.2023.08.13 val PER: 0.1902
2026-01-08 17:07:24,458: t15.2023.08.18 val PER: 0.1886
2026-01-08 17:07:24,460: t15.2023.08.20 val PER: 0.1636
2026-01-08 17:07:24,462: t15.2023.08.25 val PER: 0.1732
2026-01-08 17:07:24,464: t15.2023.08.27 val PER: 0.2637
2026-01-08 17:07:24,465: t15.2023.09.01 val PER: 0.1534
2026-01-08 17:07:24,467: t15.2023.09.03 val PER: 0.2268
2026-01-08 17:07:24,469: t15.2023.09.24 val PER: 0.1796
2026-01-08 17:07:24,471: t15.2023.09.29 val PER: 0.1870
2026-01-08 17:07:24,473: t15.2023.10.01 val PER: 0.2186
2026-01-08 17:07:24,475: t15.2023.10.06 val PER: 0.1647
2026-01-08 17:07:24,476: t15.2023.10.08 val PER: 0.2909
2026-01-08 17:07:24,478: t15.2023.10.13 val PER: 0.2708
2026-01-08 17:07:24,480: t15.2023.10.15 val PER: 0.2090
2026-01-08 17:07:24,481: t15.2023.10.20 val PER: 0.2114
2026-01-08 17:07:24,483: t15.2023.10.22 val PER: 0.1548
2026-01-08 17:07:24,485: t15.2023.11.03 val PER: 0.2313
2026-01-08 17:07:24,487: t15.2023.11.04 val PER: 0.0785
2026-01-08 17:07:24,489: t15.2023.11.17 val PER: 0.1011
2026-01-08 17:07:24,491: t15.2023.11.19 val PER: 0.1098
2026-01-08 17:07:24,493: t15.2023.11.26 val PER: 0.1804
2026-01-08 17:07:24,494: t15.2023.12.03 val PER: 0.1723
2026-01-08 17:07:24,496: t15.2023.12.08 val PER: 0.1591
2026-01-08 17:07:24,498: t15.2023.12.10 val PER: 0.1485
2026-01-08 17:07:24,500: t15.2023.12.17 val PER: 0.1601
2026-01-08 17:07:24,501: t15.2023.12.29 val PER: 0.1984
2026-01-08 17:07:24,503: t15.2024.02.25 val PER: 0.1671
2026-01-08 17:07:24,505: t15.2024.03.08 val PER: 0.2447
2026-01-08 17:07:24,507: t15.2024.03.15 val PER: 0.2220
2026-01-08 17:07:24,509: t15.2024.03.17 val PER: 0.1855
2026-01-08 17:07:24,511: t15.2024.05.10 val PER: 0.2065
2026-01-08 17:07:24,513: t15.2024.06.14 val PER: 0.2035
2026-01-08 17:07:24,515: t15.2024.07.19 val PER: 0.2571
2026-01-08 17:07:24,516: t15.2024.07.21 val PER: 0.1331
2026-01-08 17:07:24,518: t15.2024.07.28 val PER: 0.1801
2026-01-08 17:07:24,520: t15.2025.01.10 val PER: 0.3278
2026-01-08 17:07:24,521: t15.2025.01.12 val PER: 0.2271
2026-01-08 17:07:24,523: t15.2025.03.14 val PER: 0.3462
2026-01-08 17:07:24,525: t15.2025.03.16 val PER: 0.2408
2026-01-08 17:07:24,527: t15.2025.03.30 val PER: 0.3322
2026-01-08 17:07:24,529: t15.2025.04.13 val PER: 0.2696
2026-01-08 17:07:24,661: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_18500
2026-01-08 17:07:33,613: Train batch 18600: loss: 16.07 grad norm: 63.76 time: 0.071
2026-01-08 17:07:50,938: Train batch 18800: loss: 15.15 grad norm: 63.36 time: 0.068
2026-01-08 17:08:08,937: Train batch 19000: loss: 13.59 grad norm: 60.43 time: 0.067
2026-01-08 17:08:08,939: Running test after training batch: 19000
2026-01-08 17:08:09,057: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:08:14,039: WER debug example
  GT : you can see the code at this point as well
  PR : you candies seeks the good hats this appointees has swells
2026-01-08 17:08:14,085: WER debug example
  GT : how does it keep the cost down
  PR : howard's sta c hit keep curse the scott stetzer
2026-01-08 17:08:26,646: Val batch 19000: PER (avg): 0.2025 CTC Loss (avg): 22.0974 WER(5gram): 99.80% (n=256) time: 17.704
2026-01-08 17:08:26,649: WER lens: avg_true_words=5.99 avg_pred_words=7.10 max_pred_words=15
2026-01-08 17:08:26,651: t15.2023.08.13 val PER: 0.1892
2026-01-08 17:08:26,653: t15.2023.08.18 val PER: 0.1911
2026-01-08 17:08:26,654: t15.2023.08.20 val PER: 0.1692
2026-01-08 17:08:26,656: t15.2023.08.25 val PER: 0.1762
2026-01-08 17:08:26,657: t15.2023.08.27 val PER: 0.2605
2026-01-08 17:08:26,659: t15.2023.09.01 val PER: 0.1485
2026-01-08 17:08:26,661: t15.2023.09.03 val PER: 0.2173
2026-01-08 17:08:26,662: t15.2023.09.24 val PER: 0.1796
2026-01-08 17:08:26,664: t15.2023.09.29 val PER: 0.1870
2026-01-08 17:08:26,666: t15.2023.10.01 val PER: 0.2166
2026-01-08 17:08:26,667: t15.2023.10.06 val PER: 0.1582
2026-01-08 17:08:26,669: t15.2023.10.08 val PER: 0.2936
2026-01-08 17:08:26,671: t15.2023.10.13 val PER: 0.2708
2026-01-08 17:08:26,672: t15.2023.10.15 val PER: 0.2083
2026-01-08 17:08:26,674: t15.2023.10.20 val PER: 0.2148
2026-01-08 17:08:26,676: t15.2023.10.22 val PER: 0.1526
2026-01-08 17:08:26,677: t15.2023.11.03 val PER: 0.2327
2026-01-08 17:08:26,679: t15.2023.11.04 val PER: 0.0853
2026-01-08 17:08:26,681: t15.2023.11.17 val PER: 0.1026
2026-01-08 17:08:26,682: t15.2023.11.19 val PER: 0.1138
2026-01-08 17:08:26,684: t15.2023.11.26 val PER: 0.1790
2026-01-08 17:08:26,685: t15.2023.12.03 val PER: 0.1786
2026-01-08 17:08:26,687: t15.2023.12.08 val PER: 0.1618
2026-01-08 17:08:26,688: t15.2023.12.10 val PER: 0.1406
2026-01-08 17:08:26,690: t15.2023.12.17 val PER: 0.1559
2026-01-08 17:08:26,691: t15.2023.12.29 val PER: 0.1997
2026-01-08 17:08:26,693: t15.2024.02.25 val PER: 0.1545
2026-01-08 17:08:26,695: t15.2024.03.08 val PER: 0.2447
2026-01-08 17:08:26,698: t15.2024.03.15 val PER: 0.2220
2026-01-08 17:08:26,699: t15.2024.03.17 val PER: 0.1841
2026-01-08 17:08:26,701: t15.2024.05.10 val PER: 0.2080
2026-01-08 17:08:26,702: t15.2024.06.14 val PER: 0.2082
2026-01-08 17:08:26,704: t15.2024.07.19 val PER: 0.2518
2026-01-08 17:08:26,706: t15.2024.07.21 val PER: 0.1255
2026-01-08 17:08:26,707: t15.2024.07.28 val PER: 0.1787
2026-01-08 17:08:26,709: t15.2025.01.10 val PER: 0.3251
2026-01-08 17:08:26,710: t15.2025.01.12 val PER: 0.2271
2026-01-08 17:08:26,711: t15.2025.03.14 val PER: 0.3550
2026-01-08 17:08:26,713: t15.2025.03.16 val PER: 0.2421
2026-01-08 17:08:26,714: t15.2025.03.30 val PER: 0.3322
2026-01-08 17:08:26,715: t15.2025.04.13 val PER: 0.2696
2026-01-08 17:08:26,854: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_19000
2026-01-08 17:08:44,125: Train batch 19200: loss: 11.03 grad norm: 53.59 time: 0.066
2026-01-08 17:09:01,181: Train batch 19400: loss: 12.07 grad norm: 49.51 time: 0.055
2026-01-08 17:09:09,670: Running test after training batch: 19500
2026-01-08 17:09:09,773: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:09:14,741: WER debug example
  GT : you can see the code at this point as well
  PR : you candies seeks the code hats this appointees has
2026-01-08 17:09:14,786: WER debug example
  GT : how does it keep the cost down
  PR : howard's sta c hit keep curse the scott stetzer
2026-01-08 17:09:27,559: Val batch 19500: PER (avg): 0.2016 CTC Loss (avg): 22.0766 WER(5gram): 97.46% (n=256) time: 17.886
2026-01-08 17:09:27,561: WER lens: avg_true_words=5.99 avg_pred_words=7.12 max_pred_words=14
2026-01-08 17:09:27,564: t15.2023.08.13 val PER: 0.1861
2026-01-08 17:09:27,567: t15.2023.08.18 val PER: 0.1903
2026-01-08 17:09:27,570: t15.2023.08.20 val PER: 0.1660
2026-01-08 17:09:27,572: t15.2023.08.25 val PER: 0.1747
2026-01-08 17:09:27,573: t15.2023.08.27 val PER: 0.2637
2026-01-08 17:09:27,575: t15.2023.09.01 val PER: 0.1477
2026-01-08 17:09:27,577: t15.2023.09.03 val PER: 0.2209
2026-01-08 17:09:27,579: t15.2023.09.24 val PER: 0.1735
2026-01-08 17:09:27,580: t15.2023.09.29 val PER: 0.1851
2026-01-08 17:09:27,582: t15.2023.10.01 val PER: 0.2127
2026-01-08 17:09:27,584: t15.2023.10.06 val PER: 0.1572
2026-01-08 17:09:27,586: t15.2023.10.08 val PER: 0.2950
2026-01-08 17:09:27,588: t15.2023.10.13 val PER: 0.2676
2026-01-08 17:09:27,589: t15.2023.10.15 val PER: 0.2096
2026-01-08 17:09:27,591: t15.2023.10.20 val PER: 0.2181
2026-01-08 17:09:27,593: t15.2023.10.22 val PER: 0.1537
2026-01-08 17:09:27,595: t15.2023.11.03 val PER: 0.2354
2026-01-08 17:09:27,597: t15.2023.11.04 val PER: 0.0853
2026-01-08 17:09:27,599: t15.2023.11.17 val PER: 0.1042
2026-01-08 17:09:27,601: t15.2023.11.19 val PER: 0.1098
2026-01-08 17:09:27,602: t15.2023.11.26 val PER: 0.1826
2026-01-08 17:09:27,604: t15.2023.12.03 val PER: 0.1744
2026-01-08 17:09:27,606: t15.2023.12.08 val PER: 0.1618
2026-01-08 17:09:27,608: t15.2023.12.10 val PER: 0.1393
2026-01-08 17:09:27,610: t15.2023.12.17 val PER: 0.1642
2026-01-08 17:09:27,612: t15.2023.12.29 val PER: 0.1963
2026-01-08 17:09:27,615: t15.2024.02.25 val PER: 0.1559
2026-01-08 17:09:27,617: t15.2024.03.08 val PER: 0.2475
2026-01-08 17:09:27,618: t15.2024.03.15 val PER: 0.2183
2026-01-08 17:09:27,621: t15.2024.03.17 val PER: 0.1778
2026-01-08 17:09:27,623: t15.2024.05.10 val PER: 0.2080
2026-01-08 17:09:27,624: t15.2024.06.14 val PER: 0.2035
2026-01-08 17:09:27,626: t15.2024.07.19 val PER: 0.2518
2026-01-08 17:09:27,628: t15.2024.07.21 val PER: 0.1297
2026-01-08 17:09:27,630: t15.2024.07.28 val PER: 0.1772
2026-01-08 17:09:27,632: t15.2025.01.10 val PER: 0.3154
2026-01-08 17:09:27,634: t15.2025.01.12 val PER: 0.2279
2026-01-08 17:09:27,635: t15.2025.03.14 val PER: 0.3521
2026-01-08 17:09:27,637: t15.2025.03.16 val PER: 0.2369
2026-01-08 17:09:27,639: t15.2025.03.30 val PER: 0.3276
2026-01-08 17:09:27,641: t15.2025.04.13 val PER: 0.2739
2026-01-08 17:09:27,781: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_19500
2026-01-08 17:09:36,232: Train batch 19600: loss: 15.21 grad norm: 58.64 time: 0.059
2026-01-08 17:09:53,254: Train batch 19800: loss: 11.60 grad norm: 56.87 time: 0.057
2026-01-08 17:10:10,275: Running test after training batch: 19999
2026-01-08 17:10:10,369: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:10:15,289: WER debug example
  GT : you can see the code at this point as well
  PR : you candies seeks the code hats this appointees has
2026-01-08 17:10:15,334: WER debug example
  GT : how does it keep the cost down
  PR : howard's sta c hit keep curse the scott stetzer
2026-01-08 17:10:28,230: Val batch 19999: PER (avg): 0.2003 CTC Loss (avg): 21.5927 WER(5gram): 99.28% (n=256) time: 17.953
2026-01-08 17:10:28,233: WER lens: avg_true_words=5.99 avg_pred_words=7.18 max_pred_words=14
2026-01-08 17:10:28,236: t15.2023.08.13 val PER: 0.1871
2026-01-08 17:10:28,239: t15.2023.08.18 val PER: 0.1827
2026-01-08 17:10:28,241: t15.2023.08.20 val PER: 0.1620
2026-01-08 17:10:28,243: t15.2023.08.25 val PER: 0.1777
2026-01-08 17:10:28,245: t15.2023.08.27 val PER: 0.2621
2026-01-08 17:10:28,248: t15.2023.09.01 val PER: 0.1461
2026-01-08 17:10:28,250: t15.2023.09.03 val PER: 0.2173
2026-01-08 17:10:28,251: t15.2023.09.24 val PER: 0.1748
2026-01-08 17:10:28,253: t15.2023.09.29 val PER: 0.1857
2026-01-08 17:10:28,255: t15.2023.10.01 val PER: 0.2166
2026-01-08 17:10:28,257: t15.2023.10.06 val PER: 0.1550
2026-01-08 17:10:28,259: t15.2023.10.08 val PER: 0.2896
2026-01-08 17:10:28,261: t15.2023.10.13 val PER: 0.2653
2026-01-08 17:10:28,263: t15.2023.10.15 val PER: 0.2024
2026-01-08 17:10:28,264: t15.2023.10.20 val PER: 0.2215
2026-01-08 17:10:28,266: t15.2023.10.22 val PER: 0.1526
2026-01-08 17:10:28,268: t15.2023.11.03 val PER: 0.2266
2026-01-08 17:10:28,270: t15.2023.11.04 val PER: 0.0819
2026-01-08 17:10:28,271: t15.2023.11.17 val PER: 0.1026
2026-01-08 17:10:28,273: t15.2023.11.19 val PER: 0.1078
2026-01-08 17:10:28,274: t15.2023.11.26 val PER: 0.1783
2026-01-08 17:10:28,276: t15.2023.12.03 val PER: 0.1765
2026-01-08 17:10:28,279: t15.2023.12.08 val PER: 0.1585
2026-01-08 17:10:28,280: t15.2023.12.10 val PER: 0.1419
2026-01-08 17:10:28,282: t15.2023.12.17 val PER: 0.1622
2026-01-08 17:10:28,284: t15.2023.12.29 val PER: 0.1970
2026-01-08 17:10:28,286: t15.2024.02.25 val PER: 0.1573
2026-01-08 17:10:28,287: t15.2024.03.08 val PER: 0.2546
2026-01-08 17:10:28,289: t15.2024.03.15 val PER: 0.2176
2026-01-08 17:10:28,291: t15.2024.03.17 val PER: 0.1785
2026-01-08 17:10:28,293: t15.2024.05.10 val PER: 0.2065
2026-01-08 17:10:28,294: t15.2024.06.14 val PER: 0.2003
2026-01-08 17:10:28,296: t15.2024.07.19 val PER: 0.2538
2026-01-08 17:10:28,298: t15.2024.07.21 val PER: 0.1303
2026-01-08 17:10:28,299: t15.2024.07.28 val PER: 0.1735
2026-01-08 17:10:28,301: t15.2025.01.10 val PER: 0.3140
2026-01-08 17:10:28,302: t15.2025.01.12 val PER: 0.2263
2026-01-08 17:10:28,304: t15.2025.03.14 val PER: 0.3536
2026-01-08 17:10:28,305: t15.2025.03.16 val PER: 0.2369
2026-01-08 17:10:28,307: t15.2025.03.30 val PER: 0.3287
2026-01-08 17:10:28,308: t15.2025.04.13 val PER: 0.2739
2026-01-08 17:10:28,448: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/head_ln_2blocks/checkpoint/checkpoint_batch_19999
2026-01-08 17:10:28,951: Best avg val PER achieved: 0.42339
2026-01-08 17:10:28,953: Total training time: 47.94 minutes

=== RUN patch_20.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20
2026-01-08 17:12:12,645: Using device: cuda:0
2026-01-08 17:15:59,876: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-08 17:15:59,900: Using 45 sessions after filtering (from 45).
2026-01-08 17:16:00,393: Using torch.compile (if available)
2026-01-08 17:16:00,394: torch.compile not available (torch<2.0). Skipping.
2026-01-08 17:16:00,394: Initialized RNN decoding model
2026-01-08 17:16:00,394: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(10240, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-08 17:16:00,394: Model has 51,985,193 parameters
2026-01-08 17:16:00,394: Model has 11,819,520 day-specific parameters | 22.74% of total parameters
2026-01-08 17:16:01,648: Successfully initialized datasets
2026-01-08 17:16:01,650: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-08 17:16:03,180: Train batch 0: loss: 517.58 grad norm: 2664.50 time: 0.189
2026-01-08 17:16:03,183: Running test after training batch: 0
2026-01-08 17:16:03,297: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:16:08,835: WER debug example
  GT : you can see the code at this point as well
  PR : zhao ziyang
2026-01-08 17:16:09,026: WER debug example
  GT : how does it keep the cost down
  PR : zsa zsa
2026-01-08 17:19:16,006: Val batch 0: PER (avg): 1.1610 CTC Loss (avg): 572.7795 WER(5gram): 99.80% (n=256) time: 192.822
2026-01-08 17:19:16,015: WER lens: avg_true_words=5.99 avg_pred_words=2.93 max_pred_words=6
2026-01-08 17:19:16,018: t15.2023.08.13 val PER: 1.0416
2026-01-08 17:19:16,021: t15.2023.08.18 val PER: 1.1676
2026-01-08 17:19:16,021: t15.2023.08.20 val PER: 1.0445
2026-01-08 17:19:16,021: t15.2023.08.25 val PER: 1.0678
2026-01-08 17:19:16,021: t15.2023.08.27 val PER: 1.0386
2026-01-08 17:19:16,021: t15.2023.09.01 val PER: 1.1477
2026-01-08 17:19:16,021: t15.2023.09.03 val PER: 1.0819
2026-01-08 17:19:16,022: t15.2023.09.24 val PER: 1.2549
2026-01-08 17:19:16,022: t15.2023.09.29 val PER: 1.1742
2026-01-08 17:19:16,022: t15.2023.10.01 val PER: 1.0112
2026-01-08 17:19:16,022: t15.2023.10.06 val PER: 1.2217
2026-01-08 17:19:16,022: t15.2023.10.08 val PER: 0.9675
2026-01-08 17:19:16,022: t15.2023.10.13 val PER: 1.1676
2026-01-08 17:19:16,022: t15.2023.10.15 val PER: 1.1905
2026-01-08 17:19:16,022: t15.2023.10.20 val PER: 1.2315
2026-01-08 17:19:16,022: t15.2023.10.22 val PER: 1.1849
2026-01-08 17:19:16,022: t15.2023.11.03 val PER: 1.2469
2026-01-08 17:19:16,022: t15.2023.11.04 val PER: 1.4334
2026-01-08 17:19:16,022: t15.2023.11.17 val PER: 1.5288
2026-01-08 17:19:16,022: t15.2023.11.19 val PER: 1.4172
2026-01-08 17:19:16,023: t15.2023.11.26 val PER: 1.2261
2026-01-08 17:19:16,023: t15.2023.12.03 val PER: 1.1807
2026-01-08 17:19:16,023: t15.2023.12.08 val PER: 1.2344
2026-01-08 17:19:16,023: t15.2023.12.10 val PER: 1.3180
2026-01-08 17:19:16,023: t15.2023.12.17 val PER: 1.0644
2026-01-08 17:19:16,023: t15.2023.12.29 val PER: 1.1400
2026-01-08 17:19:16,023: t15.2024.02.25 val PER: 1.2584
2026-01-08 17:19:16,023: t15.2024.03.08 val PER: 1.0569
2026-01-08 17:19:16,023: t15.2024.03.15 val PER: 1.1107
2026-01-08 17:19:16,023: t15.2024.03.17 val PER: 1.1457
2026-01-08 17:19:16,023: t15.2024.05.10 val PER: 1.1278
2026-01-08 17:19:16,023: t15.2024.06.14 val PER: 1.2886
2026-01-08 17:19:16,023: t15.2024.07.19 val PER: 0.9624
2026-01-08 17:19:16,023: t15.2024.07.21 val PER: 1.3179
2026-01-08 17:19:16,023: t15.2024.07.28 val PER: 1.3169
2026-01-08 17:19:16,023: t15.2025.01.10 val PER: 0.9408
2026-01-08 17:19:16,024: t15.2025.01.12 val PER: 1.3287
2026-01-08 17:19:16,024: t15.2025.03.14 val PER: 0.9453
2026-01-08 17:19:16,024: t15.2025.03.16 val PER: 1.1571
2026-01-08 17:19:16,024: t15.2025.03.30 val PER: 0.9609
2026-01-08 17:19:16,024: t15.2025.04.13 val PER: 1.1355
2026-01-08 17:19:16,025: New best val WER(5gram) inf% --> 99.80%
2026-01-08 17:19:16,229: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_0
2026-01-08 17:19:33,754: Train batch 200: loss: 74.63 grad norm: 72.04 time: 0.061
2026-01-08 17:19:50,726: Train batch 400: loss: 50.98 grad norm: 83.35 time: 0.071
2026-01-08 17:19:59,191: Running test after training batch: 500
2026-01-08 17:19:59,323: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:20:04,567: WER debug example
  GT : you can see the code at this point as well
  PR : e e t e d d
2026-01-08 17:20:04,744: WER debug example
  GT : how does it keep the cost down
  PR : is it to us
2026-01-08 17:20:38,650: Val batch 500: PER (avg): 0.5034 CTC Loss (avg): 52.0372 WER(5gram): 96.68% (n=256) time: 39.456
2026-01-08 17:20:38,652: WER lens: avg_true_words=5.99 avg_pred_words=3.41 max_pred_words=9
2026-01-08 17:20:38,655: t15.2023.08.13 val PER: 0.4501
2026-01-08 17:20:38,657: t15.2023.08.18 val PER: 0.4250
2026-01-08 17:20:38,659: t15.2023.08.20 val PER: 0.4504
2026-01-08 17:20:38,660: t15.2023.08.25 val PER: 0.4217
2026-01-08 17:20:38,662: t15.2023.08.27 val PER: 0.5531
2026-01-08 17:20:38,664: t15.2023.09.01 val PER: 0.4067
2026-01-08 17:20:38,666: t15.2023.09.03 val PER: 0.5048
2026-01-08 17:20:38,668: t15.2023.09.24 val PER: 0.4223
2026-01-08 17:20:38,669: t15.2023.09.29 val PER: 0.4327
2026-01-08 17:20:38,671: t15.2023.10.01 val PER: 0.4980
2026-01-08 17:20:38,673: t15.2023.10.06 val PER: 0.4252
2026-01-08 17:20:38,675: t15.2023.10.08 val PER: 0.5210
2026-01-08 17:20:38,677: t15.2023.10.13 val PER: 0.5454
2026-01-08 17:20:38,678: t15.2023.10.15 val PER: 0.4904
2026-01-08 17:20:38,680: t15.2023.10.20 val PER: 0.4396
2026-01-08 17:20:38,682: t15.2023.10.22 val PER: 0.4410
2026-01-08 17:20:38,684: t15.2023.11.03 val PER: 0.4674
2026-01-08 17:20:38,686: t15.2023.11.04 val PER: 0.2901
2026-01-08 17:20:38,688: t15.2023.11.17 val PER: 0.3468
2026-01-08 17:20:38,690: t15.2023.11.19 val PER: 0.3214
2026-01-08 17:20:38,691: t15.2023.11.26 val PER: 0.5341
2026-01-08 17:20:38,693: t15.2023.12.03 val PER: 0.4842
2026-01-08 17:20:38,695: t15.2023.12.08 val PER: 0.5033
2026-01-08 17:20:38,697: t15.2023.12.10 val PER: 0.4468
2026-01-08 17:20:38,699: t15.2023.12.17 val PER: 0.5489
2026-01-08 17:20:38,701: t15.2023.12.29 val PER: 0.5285
2026-01-08 17:20:38,703: t15.2024.02.25 val PER: 0.4551
2026-01-08 17:20:38,705: t15.2024.03.08 val PER: 0.5989
2026-01-08 17:20:38,708: t15.2024.03.15 val PER: 0.5428
2026-01-08 17:20:38,710: t15.2024.03.17 val PER: 0.4979
2026-01-08 17:20:38,712: t15.2024.05.10 val PER: 0.5052
2026-01-08 17:20:38,713: t15.2024.06.14 val PER: 0.5189
2026-01-08 17:20:38,715: t15.2024.07.19 val PER: 0.6533
2026-01-08 17:20:38,717: t15.2024.07.21 val PER: 0.4800
2026-01-08 17:20:38,718: t15.2024.07.28 val PER: 0.5029
2026-01-08 17:20:38,720: t15.2025.01.10 val PER: 0.7190
2026-01-08 17:20:38,722: t15.2025.01.12 val PER: 0.5350
2026-01-08 17:20:38,723: t15.2025.03.14 val PER: 0.7056
2026-01-08 17:20:38,725: t15.2025.03.16 val PER: 0.5628
2026-01-08 17:20:38,727: t15.2025.03.30 val PER: 0.7207
2026-01-08 17:20:38,728: t15.2025.04.13 val PER: 0.5378
2026-01-08 17:20:38,730: New best val WER(5gram) 99.80% --> 96.68%
2026-01-08 17:20:38,945: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_500
2026-01-08 17:20:47,359: Train batch 600: loss: 47.81 grad norm: 84.21 time: 0.089
2026-01-08 17:21:05,307: Train batch 800: loss: 39.90 grad norm: 82.84 time: 0.065
2026-01-08 17:21:23,207: Train batch 1000: loss: 42.47 grad norm: 73.22 time: 0.074
2026-01-08 17:21:23,210: Running test after training batch: 1000
2026-01-08 17:21:23,327: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:21:28,418: WER debug example
  GT : you can see the code at this point as well
  PR : you ou a t e t t t t y
2026-01-08 17:21:28,477: WER debug example
  GT : how does it keep the cost down
  PR : is it to the us and
2026-01-08 17:21:50,710: Val batch 1000: PER (avg): 0.4125 CTC Loss (avg): 40.8463 WER(5gram): 99.93% (n=256) time: 27.499
2026-01-08 17:21:50,713: WER lens: avg_true_words=5.99 avg_pred_words=5.71 max_pred_words=14
2026-01-08 17:21:50,715: t15.2023.08.13 val PER: 0.3680
2026-01-08 17:21:50,717: t15.2023.08.18 val PER: 0.3479
2026-01-08 17:21:50,719: t15.2023.08.20 val PER: 0.3527
2026-01-08 17:21:50,720: t15.2023.08.25 val PER: 0.3343
2026-01-08 17:21:50,722: t15.2023.08.27 val PER: 0.4277
2026-01-08 17:21:50,723: t15.2023.09.01 val PER: 0.3247
2026-01-08 17:21:50,725: t15.2023.09.03 val PER: 0.4062
2026-01-08 17:21:50,726: t15.2023.09.24 val PER: 0.3459
2026-01-08 17:21:50,728: t15.2023.09.29 val PER: 0.3599
2026-01-08 17:21:50,730: t15.2023.10.01 val PER: 0.4260
2026-01-08 17:21:50,731: t15.2023.10.06 val PER: 0.3100
2026-01-08 17:21:50,733: t15.2023.10.08 val PER: 0.4465
2026-01-08 17:21:50,734: t15.2023.10.13 val PER: 0.4701
2026-01-08 17:21:50,736: t15.2023.10.15 val PER: 0.3843
2026-01-08 17:21:50,738: t15.2023.10.20 val PER: 0.3456
2026-01-08 17:21:50,739: t15.2023.10.22 val PER: 0.3541
2026-01-08 17:21:50,741: t15.2023.11.03 val PER: 0.3982
2026-01-08 17:21:50,742: t15.2023.11.04 val PER: 0.2048
2026-01-08 17:21:50,744: t15.2023.11.17 val PER: 0.2566
2026-01-08 17:21:50,745: t15.2023.11.19 val PER: 0.2275
2026-01-08 17:21:50,747: t15.2023.11.26 val PER: 0.4362
2026-01-08 17:21:50,748: t15.2023.12.03 val PER: 0.3939
2026-01-08 17:21:50,750: t15.2023.12.08 val PER: 0.4115
2026-01-08 17:21:50,751: t15.2023.12.10 val PER: 0.3443
2026-01-08 17:21:50,753: t15.2023.12.17 val PER: 0.4127
2026-01-08 17:21:50,754: t15.2023.12.29 val PER: 0.4173
2026-01-08 17:21:50,756: t15.2024.02.25 val PER: 0.3497
2026-01-08 17:21:50,757: t15.2024.03.08 val PER: 0.4808
2026-01-08 17:21:50,759: t15.2024.03.15 val PER: 0.4515
2026-01-08 17:21:50,760: t15.2024.03.17 val PER: 0.4093
2026-01-08 17:21:50,762: t15.2024.05.10 val PER: 0.4383
2026-01-08 17:21:50,763: t15.2024.06.14 val PER: 0.4259
2026-01-08 17:21:50,765: t15.2024.07.19 val PER: 0.5544
2026-01-08 17:21:50,766: t15.2024.07.21 val PER: 0.3752
2026-01-08 17:21:50,768: t15.2024.07.28 val PER: 0.4037
2026-01-08 17:21:50,769: t15.2025.01.10 val PER: 0.6281
2026-01-08 17:21:50,771: t15.2025.01.12 val PER: 0.4380
2026-01-08 17:21:50,772: t15.2025.03.14 val PER: 0.6257
2026-01-08 17:21:50,774: t15.2025.03.16 val PER: 0.4961
2026-01-08 17:21:50,775: t15.2025.03.30 val PER: 0.6621
2026-01-08 17:21:50,777: t15.2025.04.13 val PER: 0.4608
2026-01-08 17:21:50,936: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_1000
2026-01-08 17:22:08,571: Train batch 1200: loss: 34.82 grad norm: 77.34 time: 0.077
2026-01-08 17:22:26,978: Train batch 1400: loss: 35.83 grad norm: 81.65 time: 0.068
2026-01-08 17:22:36,177: Running test after training batch: 1500
2026-01-08 17:22:36,325: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:22:41,528: WER debug example
  GT : you can see the code at this point as well
  PR : ou qui ne the a and d y
2026-01-08 17:22:41,596: WER debug example
  GT : how does it keep the cost down
  PR : see the new
2026-01-08 17:23:00,522: Val batch 1500: PER (avg): 0.3726 CTC Loss (avg): 35.9701 WER(5gram): 92.57% (n=256) time: 24.342
2026-01-08 17:23:00,524: WER lens: avg_true_words=5.99 avg_pred_words=4.56 max_pred_words=12
2026-01-08 17:23:00,526: t15.2023.08.13 val PER: 0.3565
2026-01-08 17:23:00,528: t15.2023.08.18 val PER: 0.3127
2026-01-08 17:23:00,529: t15.2023.08.20 val PER: 0.3034
2026-01-08 17:23:00,531: t15.2023.08.25 val PER: 0.2786
2026-01-08 17:23:00,532: t15.2023.08.27 val PER: 0.4019
2026-01-08 17:23:00,533: t15.2023.09.01 val PER: 0.2646
2026-01-08 17:23:00,535: t15.2023.09.03 val PER: 0.3539
2026-01-08 17:23:00,536: t15.2023.09.24 val PER: 0.3058
2026-01-08 17:23:00,538: t15.2023.09.29 val PER: 0.3248
2026-01-08 17:23:00,539: t15.2023.10.01 val PER: 0.3818
2026-01-08 17:23:00,541: t15.2023.10.06 val PER: 0.2992
2026-01-08 17:23:00,542: t15.2023.10.08 val PER: 0.4168
2026-01-08 17:23:00,543: t15.2023.10.13 val PER: 0.4344
2026-01-08 17:23:00,545: t15.2023.10.15 val PER: 0.3461
2026-01-08 17:23:00,546: t15.2023.10.20 val PER: 0.3389
2026-01-08 17:23:00,548: t15.2023.10.22 val PER: 0.3096
2026-01-08 17:23:00,549: t15.2023.11.03 val PER: 0.3616
2026-01-08 17:23:00,550: t15.2023.11.04 val PER: 0.1433
2026-01-08 17:23:00,552: t15.2023.11.17 val PER: 0.2457
2026-01-08 17:23:00,553: t15.2023.11.19 val PER: 0.2016
2026-01-08 17:23:00,555: t15.2023.11.26 val PER: 0.3826
2026-01-08 17:23:00,556: t15.2023.12.03 val PER: 0.3477
2026-01-08 17:23:00,560: t15.2023.12.08 val PER: 0.3482
2026-01-08 17:23:00,561: t15.2023.12.10 val PER: 0.2983
2026-01-08 17:23:00,562: t15.2023.12.17 val PER: 0.3649
2026-01-08 17:23:00,564: t15.2023.12.29 val PER: 0.3802
2026-01-08 17:23:00,565: t15.2024.02.25 val PER: 0.2823
2026-01-08 17:23:00,567: t15.2024.03.08 val PER: 0.4495
2026-01-08 17:23:00,568: t15.2024.03.15 val PER: 0.4021
2026-01-08 17:23:00,569: t15.2024.03.17 val PER: 0.3703
2026-01-08 17:23:00,570: t15.2024.05.10 val PER: 0.3834
2026-01-08 17:23:00,572: t15.2024.06.14 val PER: 0.3880
2026-01-08 17:23:00,573: t15.2024.07.19 val PER: 0.5135
2026-01-08 17:23:00,575: t15.2024.07.21 val PER: 0.3414
2026-01-08 17:23:00,576: t15.2024.07.28 val PER: 0.3691
2026-01-08 17:23:00,577: t15.2025.01.10 val PER: 0.5978
2026-01-08 17:23:00,579: t15.2025.01.12 val PER: 0.3995
2026-01-08 17:23:00,580: t15.2025.03.14 val PER: 0.6021
2026-01-08 17:23:00,582: t15.2025.03.16 val PER: 0.4634
2026-01-08 17:23:00,583: t15.2025.03.30 val PER: 0.6276
2026-01-08 17:23:00,584: t15.2025.04.13 val PER: 0.4251
2026-01-08 17:23:00,586: New best val WER(5gram) 96.68% --> 92.57%
2026-01-08 17:23:00,821: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_1500
2026-01-08 17:23:09,749: Train batch 1600: loss: 35.36 grad norm: 75.57 time: 0.074
2026-01-08 17:23:27,703: Train batch 1800: loss: 37.19 grad norm: 81.53 time: 0.099
2026-01-08 17:23:45,285: Train batch 2000: loss: 31.75 grad norm: 71.37 time: 0.075
2026-01-08 17:23:45,288: Running test after training batch: 2000
2026-01-08 17:23:45,399: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:23:50,336: WER debug example
  GT : you can see the code at this point as well
  PR : you know the how and why
2026-01-08 17:23:50,533: WER debug example
  GT : how does it keep the cost down
  PR : as per the new
2026-01-08 17:24:06,553: Val batch 2000: PER (avg): 0.3319 CTC Loss (avg): 32.2980 WER(5gram): 91.46% (n=256) time: 21.260
2026-01-08 17:24:06,555: WER lens: avg_true_words=5.99 avg_pred_words=5.42 max_pred_words=14
2026-01-08 17:24:06,557: t15.2023.08.13 val PER: 0.3015
2026-01-08 17:24:06,559: t15.2023.08.18 val PER: 0.2825
2026-01-08 17:24:06,561: t15.2023.08.20 val PER: 0.2637
2026-01-08 17:24:06,562: t15.2023.08.25 val PER: 0.2440
2026-01-08 17:24:06,564: t15.2023.08.27 val PER: 0.3682
2026-01-08 17:24:06,565: t15.2023.09.01 val PER: 0.2435
2026-01-08 17:24:06,567: t15.2023.09.03 val PER: 0.3337
2026-01-08 17:24:06,569: t15.2023.09.24 val PER: 0.2621
2026-01-08 17:24:06,570: t15.2023.09.29 val PER: 0.2719
2026-01-08 17:24:06,571: t15.2023.10.01 val PER: 0.3362
2026-01-08 17:24:06,573: t15.2023.10.06 val PER: 0.2271
2026-01-08 17:24:06,574: t15.2023.10.08 val PER: 0.3816
2026-01-08 17:24:06,576: t15.2023.10.13 val PER: 0.3832
2026-01-08 17:24:06,577: t15.2023.10.15 val PER: 0.3039
2026-01-08 17:24:06,578: t15.2023.10.20 val PER: 0.2718
2026-01-08 17:24:06,580: t15.2023.10.22 val PER: 0.2728
2026-01-08 17:24:06,581: t15.2023.11.03 val PER: 0.3223
2026-01-08 17:24:06,583: t15.2023.11.04 val PER: 0.1126
2026-01-08 17:24:06,584: t15.2023.11.17 val PER: 0.1835
2026-01-08 17:24:06,586: t15.2023.11.19 val PER: 0.1477
2026-01-08 17:24:06,587: t15.2023.11.26 val PER: 0.3514
2026-01-08 17:24:06,589: t15.2023.12.03 val PER: 0.3130
2026-01-08 17:24:06,591: t15.2023.12.08 val PER: 0.3169
2026-01-08 17:24:06,592: t15.2023.12.10 val PER: 0.2641
2026-01-08 17:24:06,594: t15.2023.12.17 val PER: 0.3139
2026-01-08 17:24:06,595: t15.2023.12.29 val PER: 0.3233
2026-01-08 17:24:06,597: t15.2024.02.25 val PER: 0.2809
2026-01-08 17:24:06,598: t15.2024.03.08 val PER: 0.3926
2026-01-08 17:24:06,599: t15.2024.03.15 val PER: 0.3684
2026-01-08 17:24:06,601: t15.2024.03.17 val PER: 0.3445
2026-01-08 17:24:06,602: t15.2024.05.10 val PER: 0.3447
2026-01-08 17:24:06,604: t15.2024.06.14 val PER: 0.3565
2026-01-08 17:24:06,605: t15.2024.07.19 val PER: 0.4700
2026-01-08 17:24:06,607: t15.2024.07.21 val PER: 0.3124
2026-01-08 17:24:06,608: t15.2024.07.28 val PER: 0.3368
2026-01-08 17:24:06,611: t15.2025.01.10 val PER: 0.5344
2026-01-08 17:24:06,612: t15.2025.01.12 val PER: 0.3718
2026-01-08 17:24:06,616: t15.2025.03.14 val PER: 0.5340
2026-01-08 17:24:06,617: t15.2025.03.16 val PER: 0.3979
2026-01-08 17:24:06,619: t15.2025.03.30 val PER: 0.5736
2026-01-08 17:24:06,620: t15.2025.04.13 val PER: 0.3880
2026-01-08 17:24:06,622: New best val WER(5gram) 92.57% --> 91.46%
2026-01-08 17:24:06,831: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_2000
2026-01-08 17:24:23,552: Train batch 2200: loss: 30.75 grad norm: 75.93 time: 0.069
2026-01-08 17:24:41,556: Train batch 2400: loss: 30.59 grad norm: 74.49 time: 0.059
2026-01-08 17:24:50,661: Running test after training batch: 2500
2026-01-08 17:24:50,863: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:24:55,845: WER debug example
  GT : you can see the code at this point as well
  PR : you know the way
2026-01-08 17:24:55,888: WER debug example
  GT : how does it keep the cost down
  PR : the
2026-01-08 17:25:09,312: Val batch 2500: PER (avg): 0.3159 CTC Loss (avg): 29.8249 WER(5gram): 87.81% (n=256) time: 18.649
2026-01-08 17:25:09,315: WER lens: avg_true_words=5.99 avg_pred_words=4.50 max_pred_words=12
2026-01-08 17:25:09,318: t15.2023.08.13 val PER: 0.2879
2026-01-08 17:25:09,320: t15.2023.08.18 val PER: 0.2523
2026-01-08 17:25:09,322: t15.2023.08.20 val PER: 0.2621
2026-01-08 17:25:09,324: t15.2023.08.25 val PER: 0.2184
2026-01-08 17:25:09,325: t15.2023.08.27 val PER: 0.3376
2026-01-08 17:25:09,327: t15.2023.09.01 val PER: 0.2248
2026-01-08 17:25:09,329: t15.2023.09.03 val PER: 0.3135
2026-01-08 17:25:09,331: t15.2023.09.24 val PER: 0.2536
2026-01-08 17:25:09,333: t15.2023.09.29 val PER: 0.2636
2026-01-08 17:25:09,335: t15.2023.10.01 val PER: 0.3269
2026-01-08 17:25:09,336: t15.2023.10.06 val PER: 0.2228
2026-01-08 17:25:09,338: t15.2023.10.08 val PER: 0.3667
2026-01-08 17:25:09,340: t15.2023.10.13 val PER: 0.3708
2026-01-08 17:25:09,341: t15.2023.10.15 val PER: 0.2881
2026-01-08 17:25:09,345: t15.2023.10.20 val PER: 0.2785
2026-01-08 17:25:09,347: t15.2023.10.22 val PER: 0.2561
2026-01-08 17:25:09,348: t15.2023.11.03 val PER: 0.3066
2026-01-08 17:25:09,350: t15.2023.11.04 val PER: 0.0922
2026-01-08 17:25:09,352: t15.2023.11.17 val PER: 0.1649
2026-01-08 17:25:09,354: t15.2023.11.19 val PER: 0.1417
2026-01-08 17:25:09,356: t15.2023.11.26 val PER: 0.3413
2026-01-08 17:25:09,357: t15.2023.12.03 val PER: 0.2784
2026-01-08 17:25:09,359: t15.2023.12.08 val PER: 0.3009
2026-01-08 17:25:09,361: t15.2023.12.10 val PER: 0.2457
2026-01-08 17:25:09,364: t15.2023.12.17 val PER: 0.3191
2026-01-08 17:25:09,365: t15.2023.12.29 val PER: 0.3137
2026-01-08 17:25:09,367: t15.2024.02.25 val PER: 0.2612
2026-01-08 17:25:09,368: t15.2024.03.08 val PER: 0.3670
2026-01-08 17:25:09,370: t15.2024.03.15 val PER: 0.3646
2026-01-08 17:25:09,372: t15.2024.03.17 val PER: 0.3271
2026-01-08 17:25:09,374: t15.2024.05.10 val PER: 0.3105
2026-01-08 17:25:09,375: t15.2024.06.14 val PER: 0.3438
2026-01-08 17:25:09,377: t15.2024.07.19 val PER: 0.4568
2026-01-08 17:25:09,378: t15.2024.07.21 val PER: 0.2717
2026-01-08 17:25:09,380: t15.2024.07.28 val PER: 0.3007
2026-01-08 17:25:09,381: t15.2025.01.10 val PER: 0.5152
2026-01-08 17:25:09,383: t15.2025.01.12 val PER: 0.3672
2026-01-08 17:25:09,385: t15.2025.03.14 val PER: 0.5207
2026-01-08 17:25:09,386: t15.2025.03.16 val PER: 0.3613
2026-01-08 17:25:09,388: t15.2025.03.30 val PER: 0.5483
2026-01-08 17:25:09,389: t15.2025.04.13 val PER: 0.3980
2026-01-08 17:25:09,391: New best val WER(5gram) 91.46% --> 87.81%
2026-01-08 17:25:09,600: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_2500
2026-01-08 17:25:18,645: Train batch 2600: loss: 33.45 grad norm: 87.95 time: 0.062
2026-01-08 17:25:36,744: Train batch 2800: loss: 25.57 grad norm: 68.98 time: 0.092
2026-01-08 17:25:54,396: Train batch 3000: loss: 32.89 grad norm: 83.57 time: 0.092
2026-01-08 17:25:54,399: Running test after training batch: 3000
2026-01-08 17:25:54,501: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:25:59,529: WER debug example
  GT : you can see the code at this point as well
  PR : you see the dust this is why
2026-01-08 17:25:59,569: WER debug example
  GT : how does it keep the cost down
  PR : how is it the way to
2026-01-08 17:26:11,314: Val batch 3000: PER (avg): 0.2929 CTC Loss (avg): 28.2761 WER(5gram): 86.05% (n=256) time: 16.913
2026-01-08 17:26:11,316: WER lens: avg_true_words=5.99 avg_pred_words=6.00 max_pred_words=14
2026-01-08 17:26:11,319: t15.2023.08.13 val PER: 0.2651
2026-01-08 17:26:11,320: t15.2023.08.18 val PER: 0.2422
2026-01-08 17:26:11,322: t15.2023.08.20 val PER: 0.2367
2026-01-08 17:26:11,324: t15.2023.08.25 val PER: 0.2078
2026-01-08 17:26:11,326: t15.2023.08.27 val PER: 0.3006
2026-01-08 17:26:11,328: t15.2023.09.01 val PER: 0.2062
2026-01-08 17:26:11,330: t15.2023.09.03 val PER: 0.2803
2026-01-08 17:26:11,331: t15.2023.09.24 val PER: 0.2464
2026-01-08 17:26:11,335: t15.2023.09.29 val PER: 0.2527
2026-01-08 17:26:11,337: t15.2023.10.01 val PER: 0.2959
2026-01-08 17:26:11,340: t15.2023.10.06 val PER: 0.2131
2026-01-08 17:26:11,342: t15.2023.10.08 val PER: 0.3667
2026-01-08 17:26:11,345: t15.2023.10.13 val PER: 0.3499
2026-01-08 17:26:11,346: t15.2023.10.15 val PER: 0.2663
2026-01-08 17:26:11,349: t15.2023.10.20 val PER: 0.2651
2026-01-08 17:26:11,350: t15.2023.10.22 val PER: 0.2272
2026-01-08 17:26:11,354: t15.2023.11.03 val PER: 0.2931
2026-01-08 17:26:11,356: t15.2023.11.04 val PER: 0.0853
2026-01-08 17:26:11,357: t15.2023.11.17 val PER: 0.1509
2026-01-08 17:26:11,359: t15.2023.11.19 val PER: 0.1277
2026-01-08 17:26:11,361: t15.2023.11.26 val PER: 0.3116
2026-01-08 17:26:11,363: t15.2023.12.03 val PER: 0.2679
2026-01-08 17:26:11,365: t15.2023.12.08 val PER: 0.2696
2026-01-08 17:26:11,367: t15.2023.12.10 val PER: 0.2102
2026-01-08 17:26:11,369: t15.2023.12.17 val PER: 0.3077
2026-01-08 17:26:11,370: t15.2023.12.29 val PER: 0.2951
2026-01-08 17:26:11,372: t15.2024.02.25 val PER: 0.2458
2026-01-08 17:26:11,374: t15.2024.03.08 val PER: 0.3613
2026-01-08 17:26:11,375: t15.2024.03.15 val PER: 0.3333
2026-01-08 17:26:11,377: t15.2024.03.17 val PER: 0.3020
2026-01-08 17:26:11,379: t15.2024.05.10 val PER: 0.3076
2026-01-08 17:26:11,380: t15.2024.06.14 val PER: 0.3218
2026-01-08 17:26:11,382: t15.2024.07.19 val PER: 0.4179
2026-01-08 17:26:11,384: t15.2024.07.21 val PER: 0.2503
2026-01-08 17:26:11,386: t15.2024.07.28 val PER: 0.2757
2026-01-08 17:26:11,388: t15.2025.01.10 val PER: 0.4848
2026-01-08 17:26:11,389: t15.2025.01.12 val PER: 0.3256
2026-01-08 17:26:11,391: t15.2025.03.14 val PER: 0.4719
2026-01-08 17:26:11,393: t15.2025.03.16 val PER: 0.3442
2026-01-08 17:26:11,395: t15.2025.03.30 val PER: 0.4885
2026-01-08 17:26:11,397: t15.2025.04.13 val PER: 0.3695
2026-01-08 17:26:11,399: New best val WER(5gram) 87.81% --> 86.05%
2026-01-08 17:26:11,609: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_3000
2026-01-08 17:26:29,614: Train batch 3200: loss: 25.21 grad norm: 71.42 time: 0.086
2026-01-08 17:26:47,878: Train batch 3400: loss: 19.14 grad norm: 64.23 time: 0.055
2026-01-08 17:26:57,063: Running test after training batch: 3500
2026-01-08 17:26:57,186: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:27:02,187: WER debug example
  GT : you can see the code at this point as well
  PR : you see the go and this is why
2026-01-08 17:27:02,233: WER debug example
  GT : how does it keep the cost down
  PR : how i see it the way
2026-01-08 17:27:13,185: Val batch 3500: PER (avg): 0.2772 CTC Loss (avg): 26.1579 WER(5gram): 79.34% (n=256) time: 16.120
2026-01-08 17:27:13,187: WER lens: avg_true_words=5.99 avg_pred_words=5.44 max_pred_words=13
2026-01-08 17:27:13,193: t15.2023.08.13 val PER: 0.2453
2026-01-08 17:27:13,196: t15.2023.08.18 val PER: 0.2179
2026-01-08 17:27:13,198: t15.2023.08.20 val PER: 0.2303
2026-01-08 17:27:13,200: t15.2023.08.25 val PER: 0.1928
2026-01-08 17:27:13,201: t15.2023.08.27 val PER: 0.2974
2026-01-08 17:27:13,202: t15.2023.09.01 val PER: 0.1859
2026-01-08 17:27:13,204: t15.2023.09.03 val PER: 0.2803
2026-01-08 17:27:13,205: t15.2023.09.24 val PER: 0.2063
2026-01-08 17:27:13,206: t15.2023.09.29 val PER: 0.2304
2026-01-08 17:27:13,208: t15.2023.10.01 val PER: 0.2979
2026-01-08 17:27:13,209: t15.2023.10.06 val PER: 0.1787
2026-01-08 17:27:13,211: t15.2023.10.08 val PER: 0.3572
2026-01-08 17:27:13,212: t15.2023.10.13 val PER: 0.3282
2026-01-08 17:27:13,214: t15.2023.10.15 val PER: 0.2492
2026-01-08 17:27:13,215: t15.2023.10.20 val PER: 0.2617
2026-01-08 17:27:13,216: t15.2023.10.22 val PER: 0.2160
2026-01-08 17:27:13,218: t15.2023.11.03 val PER: 0.2646
2026-01-08 17:27:13,219: t15.2023.11.04 val PER: 0.0819
2026-01-08 17:27:13,221: t15.2023.11.17 val PER: 0.1446
2026-01-08 17:27:13,222: t15.2023.11.19 val PER: 0.1198
2026-01-08 17:27:13,223: t15.2023.11.26 val PER: 0.2775
2026-01-08 17:27:13,225: t15.2023.12.03 val PER: 0.2353
2026-01-08 17:27:13,226: t15.2023.12.08 val PER: 0.2690
2026-01-08 17:27:13,228: t15.2023.12.10 val PER: 0.2142
2026-01-08 17:27:13,229: t15.2023.12.17 val PER: 0.2775
2026-01-08 17:27:13,230: t15.2023.12.29 val PER: 0.2787
2026-01-08 17:27:13,232: t15.2024.02.25 val PER: 0.2205
2026-01-08 17:27:13,233: t15.2024.03.08 val PER: 0.3400
2026-01-08 17:27:13,234: t15.2024.03.15 val PER: 0.3302
2026-01-08 17:27:13,236: t15.2024.03.17 val PER: 0.2831
2026-01-08 17:27:13,237: t15.2024.05.10 val PER: 0.3031
2026-01-08 17:27:13,238: t15.2024.06.14 val PER: 0.2981
2026-01-08 17:27:13,240: t15.2024.07.19 val PER: 0.3995
2026-01-08 17:27:13,241: t15.2024.07.21 val PER: 0.2469
2026-01-08 17:27:13,243: t15.2024.07.28 val PER: 0.2809
2026-01-08 17:27:13,244: t15.2025.01.10 val PER: 0.4449
2026-01-08 17:27:13,246: t15.2025.01.12 val PER: 0.3118
2026-01-08 17:27:13,248: t15.2025.03.14 val PER: 0.4364
2026-01-08 17:27:13,249: t15.2025.03.16 val PER: 0.3312
2026-01-08 17:27:13,250: t15.2025.03.30 val PER: 0.4828
2026-01-08 17:27:13,252: t15.2025.04.13 val PER: 0.3524
2026-01-08 17:27:13,253: New best val WER(5gram) 86.05% --> 79.34%
2026-01-08 17:27:13,465: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_3500
2026-01-08 17:27:22,053: Train batch 3600: loss: 22.69 grad norm: 64.18 time: 0.075
2026-01-08 17:27:39,126: Train batch 3800: loss: 23.46 grad norm: 67.99 time: 0.076
2026-01-08 17:27:56,232: Train batch 4000: loss: 20.24 grad norm: 63.33 time: 0.064
2026-01-08 17:27:56,234: Running test after training batch: 4000
2026-01-08 17:27:56,362: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:28:01,390: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the co due to this boy we
2026-01-08 17:28:01,425: WER debug example
  GT : how does it keep the cost down
  PR : how i see it the way to
2026-01-08 17:28:12,783: Val batch 4000: PER (avg): 0.2550 CTC Loss (avg): 24.9459 WER(5gram): 75.36% (n=256) time: 16.545
2026-01-08 17:28:12,786: WER lens: avg_true_words=5.99 avg_pred_words=6.04 max_pred_words=13
2026-01-08 17:28:12,791: t15.2023.08.13 val PER: 0.2235
2026-01-08 17:28:12,793: t15.2023.08.18 val PER: 0.2079
2026-01-08 17:28:12,795: t15.2023.08.20 val PER: 0.1898
2026-01-08 17:28:12,796: t15.2023.08.25 val PER: 0.1762
2026-01-08 17:28:12,798: t15.2023.08.27 val PER: 0.2878
2026-01-08 17:28:12,800: t15.2023.09.01 val PER: 0.1672
2026-01-08 17:28:12,801: t15.2023.09.03 val PER: 0.2613
2026-01-08 17:28:12,803: t15.2023.09.24 val PER: 0.1942
2026-01-08 17:28:12,804: t15.2023.09.29 val PER: 0.2106
2026-01-08 17:28:12,806: t15.2023.10.01 val PER: 0.2715
2026-01-08 17:28:12,807: t15.2023.10.06 val PER: 0.1733
2026-01-08 17:28:12,809: t15.2023.10.08 val PER: 0.3221
2026-01-08 17:28:12,811: t15.2023.10.13 val PER: 0.2886
2026-01-08 17:28:12,812: t15.2023.10.15 val PER: 0.2413
2026-01-08 17:28:12,814: t15.2023.10.20 val PER: 0.2047
2026-01-08 17:28:12,815: t15.2023.10.22 val PER: 0.1949
2026-01-08 17:28:12,817: t15.2023.11.03 val PER: 0.2442
2026-01-08 17:28:12,819: t15.2023.11.04 val PER: 0.0648
2026-01-08 17:28:12,820: t15.2023.11.17 val PER: 0.1089
2026-01-08 17:28:12,822: t15.2023.11.19 val PER: 0.0858
2026-01-08 17:28:12,824: t15.2023.11.26 val PER: 0.2558
2026-01-08 17:28:12,825: t15.2023.12.03 val PER: 0.2342
2026-01-08 17:28:12,827: t15.2023.12.08 val PER: 0.2350
2026-01-08 17:28:12,828: t15.2023.12.10 val PER: 0.1866
2026-01-08 17:28:12,830: t15.2023.12.17 val PER: 0.2609
2026-01-08 17:28:12,832: t15.2023.12.29 val PER: 0.2519
2026-01-08 17:28:12,833: t15.2024.02.25 val PER: 0.2107
2026-01-08 17:28:12,835: t15.2024.03.08 val PER: 0.3129
2026-01-08 17:28:12,836: t15.2024.03.15 val PER: 0.2939
2026-01-08 17:28:12,838: t15.2024.03.17 val PER: 0.2685
2026-01-08 17:28:12,839: t15.2024.05.10 val PER: 0.2883
2026-01-08 17:28:12,841: t15.2024.06.14 val PER: 0.2744
2026-01-08 17:28:12,842: t15.2024.07.19 val PER: 0.3830
2026-01-08 17:28:12,844: t15.2024.07.21 val PER: 0.2097
2026-01-08 17:28:12,845: t15.2024.07.28 val PER: 0.2610
2026-01-08 17:28:12,847: t15.2025.01.10 val PER: 0.4601
2026-01-08 17:28:12,848: t15.2025.01.12 val PER: 0.2910
2026-01-08 17:28:12,849: t15.2025.03.14 val PER: 0.4275
2026-01-08 17:28:12,851: t15.2025.03.16 val PER: 0.3194
2026-01-08 17:28:12,852: t15.2025.03.30 val PER: 0.4287
2026-01-08 17:28:12,854: t15.2025.04.13 val PER: 0.3181
2026-01-08 17:28:12,856: New best val WER(5gram) 79.34% --> 75.36%
2026-01-08 17:28:13,079: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_4000
2026-01-08 17:28:30,585: Train batch 4200: loss: 23.43 grad norm: 71.72 time: 0.088
2026-01-08 17:28:48,464: Train batch 4400: loss: 17.48 grad norm: 62.19 time: 0.080
2026-01-08 17:28:57,738: Running test after training batch: 4500
2026-01-08 17:28:57,855: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:29:02,869: WER debug example
  GT : you can see the code at this point as well
  PR : you see the go to this boy we
2026-01-08 17:29:02,907: WER debug example
  GT : how does it keep the cost down
  PR : how do it the
2026-01-08 17:29:13,071: Val batch 4500: PER (avg): 0.2429 CTC Loss (avg): 23.1810 WER(5gram): 70.66% (n=256) time: 15.330
2026-01-08 17:29:13,073: WER lens: avg_true_words=5.99 avg_pred_words=5.55 max_pred_words=12
2026-01-08 17:29:13,075: t15.2023.08.13 val PER: 0.2110
2026-01-08 17:29:13,077: t15.2023.08.18 val PER: 0.2003
2026-01-08 17:29:13,079: t15.2023.08.20 val PER: 0.1811
2026-01-08 17:29:13,080: t15.2023.08.25 val PER: 0.1551
2026-01-08 17:29:13,082: t15.2023.08.27 val PER: 0.2685
2026-01-08 17:29:13,084: t15.2023.09.01 val PER: 0.1623
2026-01-08 17:29:13,085: t15.2023.09.03 val PER: 0.2518
2026-01-08 17:29:13,087: t15.2023.09.24 val PER: 0.1990
2026-01-08 17:29:13,088: t15.2023.09.29 val PER: 0.2068
2026-01-08 17:29:13,090: t15.2023.10.01 val PER: 0.2668
2026-01-08 17:29:13,091: t15.2023.10.06 val PER: 0.1518
2026-01-08 17:29:13,093: t15.2023.10.08 val PER: 0.3207
2026-01-08 17:29:13,094: t15.2023.10.13 val PER: 0.3049
2026-01-08 17:29:13,096: t15.2023.10.15 val PER: 0.2334
2026-01-08 17:29:13,097: t15.2023.10.20 val PER: 0.2584
2026-01-08 17:29:13,099: t15.2023.10.22 val PER: 0.1938
2026-01-08 17:29:13,100: t15.2023.11.03 val PER: 0.2503
2026-01-08 17:29:13,102: t15.2023.11.04 val PER: 0.0648
2026-01-08 17:29:13,104: t15.2023.11.17 val PER: 0.0886
2026-01-08 17:29:13,105: t15.2023.11.19 val PER: 0.0978
2026-01-08 17:29:13,107: t15.2023.11.26 val PER: 0.2449
2026-01-08 17:29:13,108: t15.2023.12.03 val PER: 0.2153
2026-01-08 17:29:13,109: t15.2023.12.08 val PER: 0.2250
2026-01-08 17:29:13,111: t15.2023.12.10 val PER: 0.1813
2026-01-08 17:29:13,113: t15.2023.12.17 val PER: 0.2464
2026-01-08 17:29:13,116: t15.2023.12.29 val PER: 0.2430
2026-01-08 17:29:13,117: t15.2024.02.25 val PER: 0.2022
2026-01-08 17:29:13,119: t15.2024.03.08 val PER: 0.3144
2026-01-08 17:29:13,121: t15.2024.03.15 val PER: 0.2802
2026-01-08 17:29:13,122: t15.2024.03.17 val PER: 0.2490
2026-01-08 17:29:13,124: t15.2024.05.10 val PER: 0.2511
2026-01-08 17:29:13,125: t15.2024.06.14 val PER: 0.2634
2026-01-08 17:29:13,127: t15.2024.07.19 val PER: 0.3454
2026-01-08 17:29:13,128: t15.2024.07.21 val PER: 0.1869
2026-01-08 17:29:13,130: t15.2024.07.28 val PER: 0.2382
2026-01-08 17:29:13,131: t15.2025.01.10 val PER: 0.4270
2026-01-08 17:29:13,133: t15.2025.01.12 val PER: 0.2540
2026-01-08 17:29:13,134: t15.2025.03.14 val PER: 0.3950
2026-01-08 17:29:13,136: t15.2025.03.16 val PER: 0.2801
2026-01-08 17:29:13,138: t15.2025.03.30 val PER: 0.4149
2026-01-08 17:29:13,139: t15.2025.04.13 val PER: 0.3195
2026-01-08 17:29:13,141: New best val WER(5gram) 75.36% --> 70.66%
2026-01-08 17:29:13,353: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_4500
2026-01-08 17:29:22,415: Train batch 4600: loss: 21.30 grad norm: 70.10 time: 0.070
2026-01-08 17:29:40,833: Train batch 4800: loss: 15.63 grad norm: 55.48 time: 0.072
2026-01-08 17:29:59,106: Train batch 5000: loss: 33.36 grad norm: 78.80 time: 0.071
2026-01-08 17:29:59,108: Running test after training batch: 5000
2026-01-08 17:29:59,234: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:30:04,168: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the go to this boy will
2026-01-08 17:30:04,203: WER debug example
  GT : how does it keep the cost down
  PR : how it per the la sta
2026-01-08 17:30:13,741: Val batch 5000: PER (avg): 0.2337 CTC Loss (avg): 22.1206 WER(5gram): 67.67% (n=256) time: 14.627
2026-01-08 17:30:13,743: WER lens: avg_true_words=5.99 avg_pred_words=5.78 max_pred_words=12
2026-01-08 17:30:13,745: t15.2023.08.13 val PER: 0.2110
2026-01-08 17:30:13,748: t15.2023.08.18 val PER: 0.1836
2026-01-08 17:30:13,752: t15.2023.08.20 val PER: 0.1930
2026-01-08 17:30:13,754: t15.2023.08.25 val PER: 0.1596
2026-01-08 17:30:13,756: t15.2023.08.27 val PER: 0.2540
2026-01-08 17:30:13,758: t15.2023.09.01 val PER: 0.1526
2026-01-08 17:30:13,759: t15.2023.09.03 val PER: 0.2292
2026-01-08 17:30:13,761: t15.2023.09.24 val PER: 0.1784
2026-01-08 17:30:13,763: t15.2023.09.29 val PER: 0.1927
2026-01-08 17:30:13,765: t15.2023.10.01 val PER: 0.2477
2026-01-08 17:30:13,767: t15.2023.10.06 val PER: 0.1378
2026-01-08 17:30:13,769: t15.2023.10.08 val PER: 0.2950
2026-01-08 17:30:13,770: t15.2023.10.13 val PER: 0.2870
2026-01-08 17:30:13,772: t15.2023.10.15 val PER: 0.2241
2026-01-08 17:30:13,774: t15.2023.10.20 val PER: 0.2584
2026-01-08 17:30:13,776: t15.2023.10.22 val PER: 0.1748
2026-01-08 17:30:13,777: t15.2023.11.03 val PER: 0.2273
2026-01-08 17:30:13,779: t15.2023.11.04 val PER: 0.0580
2026-01-08 17:30:13,781: t15.2023.11.17 val PER: 0.0840
2026-01-08 17:30:13,783: t15.2023.11.19 val PER: 0.1078
2026-01-08 17:30:13,785: t15.2023.11.26 val PER: 0.2275
2026-01-08 17:30:13,787: t15.2023.12.03 val PER: 0.1996
2026-01-08 17:30:13,788: t15.2023.12.08 val PER: 0.2137
2026-01-08 17:30:13,790: t15.2023.12.10 val PER: 0.1669
2026-01-08 17:30:13,792: t15.2023.12.17 val PER: 0.2256
2026-01-08 17:30:13,794: t15.2023.12.29 val PER: 0.2388
2026-01-08 17:30:13,797: t15.2024.02.25 val PER: 0.1966
2026-01-08 17:30:13,799: t15.2024.03.08 val PER: 0.3144
2026-01-08 17:30:13,800: t15.2024.03.15 val PER: 0.2783
2026-01-08 17:30:13,802: t15.2024.03.17 val PER: 0.2392
2026-01-08 17:30:13,804: t15.2024.05.10 val PER: 0.2392
2026-01-08 17:30:13,805: t15.2024.06.14 val PER: 0.2492
2026-01-08 17:30:13,807: t15.2024.07.19 val PER: 0.3428
2026-01-08 17:30:13,809: t15.2024.07.21 val PER: 0.1759
2026-01-08 17:30:13,811: t15.2024.07.28 val PER: 0.2412
2026-01-08 17:30:13,812: t15.2025.01.10 val PER: 0.4118
2026-01-08 17:30:13,814: t15.2025.01.12 val PER: 0.2625
2026-01-08 17:30:13,816: t15.2025.03.14 val PER: 0.3831
2026-01-08 17:30:13,818: t15.2025.03.16 val PER: 0.2958
2026-01-08 17:30:13,820: t15.2025.03.30 val PER: 0.3966
2026-01-08 17:30:13,821: t15.2025.04.13 val PER: 0.3310
2026-01-08 17:30:13,823: New best val WER(5gram) 70.66% --> 67.67%
2026-01-08 17:30:14,050: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_5000
2026-01-08 17:30:32,076: Train batch 5200: loss: 18.04 grad norm: 68.50 time: 0.058
2026-01-08 17:30:49,917: Train batch 5400: loss: 19.85 grad norm: 66.50 time: 0.075
2026-01-08 17:30:58,486: Running test after training batch: 5500
2026-01-08 17:30:58,625: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:31:03,539: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold to this point as well
2026-01-08 17:31:03,577: WER debug example
  GT : how does it keep the cost down
  PR : how to see it the sea
2026-01-08 17:31:12,803: Val batch 5500: PER (avg): 0.2243 CTC Loss (avg): 21.3514 WER(5gram): 57.56% (n=256) time: 14.315
2026-01-08 17:31:12,805: WER lens: avg_true_words=5.99 avg_pred_words=5.70 max_pred_words=12
2026-01-08 17:31:12,809: t15.2023.08.13 val PER: 0.1861
2026-01-08 17:31:12,810: t15.2023.08.18 val PER: 0.1777
2026-01-08 17:31:12,812: t15.2023.08.20 val PER: 0.1747
2026-01-08 17:31:12,814: t15.2023.08.25 val PER: 0.1416
2026-01-08 17:31:12,816: t15.2023.08.27 val PER: 0.2476
2026-01-08 17:31:12,817: t15.2023.09.01 val PER: 0.1339
2026-01-08 17:31:12,819: t15.2023.09.03 val PER: 0.2209
2026-01-08 17:31:12,821: t15.2023.09.24 val PER: 0.1699
2026-01-08 17:31:12,822: t15.2023.09.29 val PER: 0.1914
2026-01-08 17:31:12,824: t15.2023.10.01 val PER: 0.2398
2026-01-08 17:31:12,826: t15.2023.10.06 val PER: 0.1453
2026-01-08 17:31:12,827: t15.2023.10.08 val PER: 0.3099
2026-01-08 17:31:12,829: t15.2023.10.13 val PER: 0.2746
2026-01-08 17:31:12,832: t15.2023.10.15 val PER: 0.2123
2026-01-08 17:31:12,834: t15.2023.10.20 val PER: 0.2181
2026-01-08 17:31:12,835: t15.2023.10.22 val PER: 0.1704
2026-01-08 17:31:12,837: t15.2023.11.03 val PER: 0.2307
2026-01-08 17:31:12,839: t15.2023.11.04 val PER: 0.0444
2026-01-08 17:31:12,840: t15.2023.11.17 val PER: 0.1011
2026-01-08 17:31:12,842: t15.2023.11.19 val PER: 0.0699
2026-01-08 17:31:12,843: t15.2023.11.26 val PER: 0.2312
2026-01-08 17:31:12,845: t15.2023.12.03 val PER: 0.1933
2026-01-08 17:31:12,847: t15.2023.12.08 val PER: 0.2031
2026-01-08 17:31:12,848: t15.2023.12.10 val PER: 0.1629
2026-01-08 17:31:12,850: t15.2023.12.17 val PER: 0.2328
2026-01-08 17:31:12,851: t15.2023.12.29 val PER: 0.2189
2026-01-08 17:31:12,853: t15.2024.02.25 val PER: 0.1910
2026-01-08 17:31:12,855: t15.2024.03.08 val PER: 0.2774
2026-01-08 17:31:12,856: t15.2024.03.15 val PER: 0.2727
2026-01-08 17:31:12,858: t15.2024.03.17 val PER: 0.2280
2026-01-08 17:31:12,860: t15.2024.05.10 val PER: 0.2407
2026-01-08 17:31:12,861: t15.2024.06.14 val PER: 0.2476
2026-01-08 17:31:12,863: t15.2024.07.19 val PER: 0.3349
2026-01-08 17:31:12,864: t15.2024.07.21 val PER: 0.1731
2026-01-08 17:31:12,866: t15.2024.07.28 val PER: 0.2265
2026-01-08 17:31:12,868: t15.2025.01.10 val PER: 0.4050
2026-01-08 17:31:12,870: t15.2025.01.12 val PER: 0.2440
2026-01-08 17:31:12,871: t15.2025.03.14 val PER: 0.3831
2026-01-08 17:31:12,873: t15.2025.03.16 val PER: 0.2631
2026-01-08 17:31:12,874: t15.2025.03.30 val PER: 0.3770
2026-01-08 17:31:12,876: t15.2025.04.13 val PER: 0.2924
2026-01-08 17:31:12,878: New best val WER(5gram) 67.67% --> 57.56%
2026-01-08 17:31:13,091: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_5500
2026-01-08 17:31:21,627: Train batch 5600: loss: 21.17 grad norm: 66.84 time: 0.069
2026-01-08 17:31:39,771: Train batch 5800: loss: 14.15 grad norm: 65.28 time: 0.093
2026-01-08 17:31:58,391: Train batch 6000: loss: 13.90 grad norm: 61.35 time: 0.056
2026-01-08 17:31:58,394: Running test after training batch: 6000
2026-01-08 17:31:58,515: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:32:03,495: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the go to this point we
2026-01-08 17:32:03,532: WER debug example
  GT : how does it keep the cost down
  PR : how do it the ka sta
2026-01-08 17:32:12,614: Val batch 6000: PER (avg): 0.2189 CTC Loss (avg): 20.7718 WER(5gram): 60.82% (n=256) time: 14.214
2026-01-08 17:32:12,616: WER lens: avg_true_words=5.99 avg_pred_words=5.84 max_pred_words=13
2026-01-08 17:32:12,621: t15.2023.08.13 val PER: 0.1892
2026-01-08 17:32:12,623: t15.2023.08.18 val PER: 0.1886
2026-01-08 17:32:12,625: t15.2023.08.20 val PER: 0.1747
2026-01-08 17:32:12,626: t15.2023.08.25 val PER: 0.1340
2026-01-08 17:32:12,628: t15.2023.08.27 val PER: 0.2331
2026-01-08 17:32:12,629: t15.2023.09.01 val PER: 0.1420
2026-01-08 17:32:12,631: t15.2023.09.03 val PER: 0.2162
2026-01-08 17:32:12,632: t15.2023.09.24 val PER: 0.1833
2026-01-08 17:32:12,634: t15.2023.09.29 val PER: 0.1812
2026-01-08 17:32:12,637: t15.2023.10.01 val PER: 0.2358
2026-01-08 17:32:12,639: t15.2023.10.06 val PER: 0.1453
2026-01-08 17:32:12,641: t15.2023.10.08 val PER: 0.3126
2026-01-08 17:32:12,642: t15.2023.10.13 val PER: 0.2770
2026-01-08 17:32:12,644: t15.2023.10.15 val PER: 0.2129
2026-01-08 17:32:12,646: t15.2023.10.20 val PER: 0.2315
2026-01-08 17:32:12,648: t15.2023.10.22 val PER: 0.1704
2026-01-08 17:32:12,649: t15.2023.11.03 val PER: 0.2252
2026-01-08 17:32:12,651: t15.2023.11.04 val PER: 0.0614
2026-01-08 17:32:12,653: t15.2023.11.17 val PER: 0.0793
2026-01-08 17:32:12,654: t15.2023.11.19 val PER: 0.0739
2026-01-08 17:32:12,656: t15.2023.11.26 val PER: 0.2232
2026-01-08 17:32:12,657: t15.2023.12.03 val PER: 0.1817
2026-01-08 17:32:12,659: t15.2023.12.08 val PER: 0.1877
2026-01-08 17:32:12,661: t15.2023.12.10 val PER: 0.1629
2026-01-08 17:32:12,662: t15.2023.12.17 val PER: 0.2183
2026-01-08 17:32:12,664: t15.2023.12.29 val PER: 0.2237
2026-01-08 17:32:12,665: t15.2024.02.25 val PER: 0.1742
2026-01-08 17:32:12,667: t15.2024.03.08 val PER: 0.2731
2026-01-08 17:32:12,668: t15.2024.03.15 val PER: 0.2614
2026-01-08 17:32:12,671: t15.2024.03.17 val PER: 0.2245
2026-01-08 17:32:12,672: t15.2024.05.10 val PER: 0.2333
2026-01-08 17:32:12,674: t15.2024.06.14 val PER: 0.2350
2026-01-08 17:32:12,676: t15.2024.07.19 val PER: 0.3118
2026-01-08 17:32:12,677: t15.2024.07.21 val PER: 0.1593
2026-01-08 17:32:12,679: t15.2024.07.28 val PER: 0.2029
2026-01-08 17:32:12,680: t15.2025.01.10 val PER: 0.3843
2026-01-08 17:32:12,682: t15.2025.01.12 val PER: 0.2433
2026-01-08 17:32:12,683: t15.2025.03.14 val PER: 0.3787
2026-01-08 17:32:12,685: t15.2025.03.16 val PER: 0.2723
2026-01-08 17:32:12,686: t15.2025.03.30 val PER: 0.3690
2026-01-08 17:32:12,687: t15.2025.04.13 val PER: 0.2839
2026-01-08 17:32:12,849: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_6000
2026-01-08 17:32:30,371: Train batch 6200: loss: 16.46 grad norm: 73.82 time: 0.079
2026-01-08 17:32:47,718: Train batch 6400: loss: 20.57 grad norm: 68.18 time: 0.069
2026-01-08 17:32:56,085: Running test after training batch: 6500
2026-01-08 17:32:56,230: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:33:01,255: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:33:01,291: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the cost
2026-01-08 17:33:10,682: Val batch 6500: PER (avg): 0.2065 CTC Loss (avg): 20.0650 WER(5gram): 53.65% (n=256) time: 14.595
2026-01-08 17:33:10,684: WER lens: avg_true_words=5.99 avg_pred_words=5.94 max_pred_words=12
2026-01-08 17:33:10,688: t15.2023.08.13 val PER: 0.1767
2026-01-08 17:33:10,690: t15.2023.08.18 val PER: 0.1635
2026-01-08 17:33:10,692: t15.2023.08.20 val PER: 0.1477
2026-01-08 17:33:10,694: t15.2023.08.25 val PER: 0.1295
2026-01-08 17:33:10,695: t15.2023.08.27 val PER: 0.2347
2026-01-08 17:33:10,697: t15.2023.09.01 val PER: 0.1088
2026-01-08 17:33:10,699: t15.2023.09.03 val PER: 0.1983
2026-01-08 17:33:10,700: t15.2023.09.24 val PER: 0.1796
2026-01-08 17:33:10,702: t15.2023.09.29 val PER: 0.1717
2026-01-08 17:33:10,704: t15.2023.10.01 val PER: 0.2133
2026-01-08 17:33:10,705: t15.2023.10.06 val PER: 0.1399
2026-01-08 17:33:10,707: t15.2023.10.08 val PER: 0.2855
2026-01-08 17:33:10,710: t15.2023.10.13 val PER: 0.2560
2026-01-08 17:33:10,711: t15.2023.10.15 val PER: 0.2057
2026-01-08 17:33:10,713: t15.2023.10.20 val PER: 0.2416
2026-01-08 17:33:10,714: t15.2023.10.22 val PER: 0.1604
2026-01-08 17:33:10,716: t15.2023.11.03 val PER: 0.2280
2026-01-08 17:33:10,717: t15.2023.11.04 val PER: 0.0683
2026-01-08 17:33:10,719: t15.2023.11.17 val PER: 0.0669
2026-01-08 17:33:10,720: t15.2023.11.19 val PER: 0.0818
2026-01-08 17:33:10,722: t15.2023.11.26 val PER: 0.2007
2026-01-08 17:33:10,723: t15.2023.12.03 val PER: 0.1618
2026-01-08 17:33:10,725: t15.2023.12.08 val PER: 0.1818
2026-01-08 17:33:10,726: t15.2023.12.10 val PER: 0.1485
2026-01-08 17:33:10,728: t15.2023.12.17 val PER: 0.1985
2026-01-08 17:33:10,729: t15.2023.12.29 val PER: 0.2011
2026-01-08 17:33:10,730: t15.2024.02.25 val PER: 0.1713
2026-01-08 17:33:10,732: t15.2024.03.08 val PER: 0.2817
2026-01-08 17:33:10,733: t15.2024.03.15 val PER: 0.2577
2026-01-08 17:33:10,735: t15.2024.03.17 val PER: 0.2022
2026-01-08 17:33:10,736: t15.2024.05.10 val PER: 0.2199
2026-01-08 17:33:10,738: t15.2024.06.14 val PER: 0.2161
2026-01-08 17:33:10,739: t15.2024.07.19 val PER: 0.3092
2026-01-08 17:33:10,741: t15.2024.07.21 val PER: 0.1538
2026-01-08 17:33:10,742: t15.2024.07.28 val PER: 0.2029
2026-01-08 17:33:10,743: t15.2025.01.10 val PER: 0.3857
2026-01-08 17:33:10,745: t15.2025.01.12 val PER: 0.2217
2026-01-08 17:33:10,746: t15.2025.03.14 val PER: 0.3609
2026-01-08 17:33:10,747: t15.2025.03.16 val PER: 0.2461
2026-01-08 17:33:10,749: t15.2025.03.30 val PER: 0.3552
2026-01-08 17:33:10,750: t15.2025.04.13 val PER: 0.2839
2026-01-08 17:33:10,752: New best val WER(5gram) 57.56% --> 53.65%
2026-01-08 17:33:10,960: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_6500
2026-01-08 17:33:19,426: Train batch 6600: loss: 11.71 grad norm: 46.07 time: 0.051
2026-01-08 17:33:37,078: Train batch 6800: loss: 14.53 grad norm: 51.48 time: 0.054
2026-01-08 17:33:54,891: Train batch 7000: loss: 17.08 grad norm: 62.96 time: 0.068
2026-01-08 17:33:54,893: Running test after training batch: 7000
2026-01-08 17:33:55,043: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:34:00,021: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point is we
2026-01-08 17:34:00,058: WER debug example
  GT : how does it keep the cost down
  PR : how i see it keep the cost
2026-01-08 17:34:08,893: Val batch 7000: PER (avg): 0.2021 CTC Loss (avg): 19.5514 WER(5gram): 51.17% (n=256) time: 13.987
2026-01-08 17:34:08,895: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-08 17:34:08,898: t15.2023.08.13 val PER: 0.1674
2026-01-08 17:34:08,900: t15.2023.08.18 val PER: 0.1551
2026-01-08 17:34:08,901: t15.2023.08.20 val PER: 0.1581
2026-01-08 17:34:08,903: t15.2023.08.25 val PER: 0.1325
2026-01-08 17:34:08,904: t15.2023.08.27 val PER: 0.2395
2026-01-08 17:34:08,906: t15.2023.09.01 val PER: 0.1088
2026-01-08 17:34:08,908: t15.2023.09.03 val PER: 0.2102
2026-01-08 17:34:08,909: t15.2023.09.24 val PER: 0.1711
2026-01-08 17:34:08,911: t15.2023.09.29 val PER: 0.1723
2026-01-08 17:34:08,912: t15.2023.10.01 val PER: 0.2193
2026-01-08 17:34:08,914: t15.2023.10.06 val PER: 0.1238
2026-01-08 17:34:08,915: t15.2023.10.08 val PER: 0.2855
2026-01-08 17:34:08,917: t15.2023.10.13 val PER: 0.2521
2026-01-08 17:34:08,919: t15.2023.10.15 val PER: 0.1885
2026-01-08 17:34:08,920: t15.2023.10.20 val PER: 0.2148
2026-01-08 17:34:08,922: t15.2023.10.22 val PER: 0.1470
2026-01-08 17:34:08,923: t15.2023.11.03 val PER: 0.2198
2026-01-08 17:34:08,925: t15.2023.11.04 val PER: 0.0444
2026-01-08 17:34:08,926: t15.2023.11.17 val PER: 0.0715
2026-01-08 17:34:08,928: t15.2023.11.19 val PER: 0.0758
2026-01-08 17:34:08,929: t15.2023.11.26 val PER: 0.1899
2026-01-08 17:34:08,931: t15.2023.12.03 val PER: 0.1534
2026-01-08 17:34:08,932: t15.2023.12.08 val PER: 0.1758
2026-01-08 17:34:08,934: t15.2023.12.10 val PER: 0.1275
2026-01-08 17:34:08,935: t15.2023.12.17 val PER: 0.1933
2026-01-08 17:34:08,937: t15.2023.12.29 val PER: 0.1977
2026-01-08 17:34:08,938: t15.2024.02.25 val PER: 0.1601
2026-01-08 17:34:08,940: t15.2024.03.08 val PER: 0.2802
2026-01-08 17:34:08,941: t15.2024.03.15 val PER: 0.2464
2026-01-08 17:34:08,943: t15.2024.03.17 val PER: 0.1967
2026-01-08 17:34:08,944: t15.2024.05.10 val PER: 0.2259
2026-01-08 17:34:08,946: t15.2024.06.14 val PER: 0.2145
2026-01-08 17:34:08,948: t15.2024.07.19 val PER: 0.3131
2026-01-08 17:34:08,949: t15.2024.07.21 val PER: 0.1503
2026-01-08 17:34:08,951: t15.2024.07.28 val PER: 0.1897
2026-01-08 17:34:08,952: t15.2025.01.10 val PER: 0.3857
2026-01-08 17:34:08,954: t15.2025.01.12 val PER: 0.2248
2026-01-08 17:34:08,956: t15.2025.03.14 val PER: 0.3802
2026-01-08 17:34:08,957: t15.2025.03.16 val PER: 0.2356
2026-01-08 17:34:08,959: t15.2025.03.30 val PER: 0.3414
2026-01-08 17:34:08,962: t15.2025.04.13 val PER: 0.2882
2026-01-08 17:34:08,964: New best val WER(5gram) 53.65% --> 51.17%
2026-01-08 17:34:09,173: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_7000
2026-01-08 17:34:27,555: Train batch 7200: loss: 15.42 grad norm: 61.65 time: 0.091
2026-01-08 17:34:45,874: Train batch 7400: loss: 15.29 grad norm: 65.25 time: 0.085
2026-01-08 17:34:54,940: Running test after training batch: 7500
2026-01-08 17:34:55,051: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:35:00,074: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:35:00,110: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the cost
2026-01-08 17:35:09,040: Val batch 7500: PER (avg): 0.1940 CTC Loss (avg): 18.6904 WER(5gram): 51.56% (n=256) time: 14.098
2026-01-08 17:35:09,042: WER lens: avg_true_words=5.99 avg_pred_words=5.95 max_pred_words=13
2026-01-08 17:35:09,048: t15.2023.08.13 val PER: 0.1559
2026-01-08 17:35:09,050: t15.2023.08.18 val PER: 0.1500
2026-01-08 17:35:09,052: t15.2023.08.20 val PER: 0.1477
2026-01-08 17:35:09,053: t15.2023.08.25 val PER: 0.1446
2026-01-08 17:35:09,055: t15.2023.08.27 val PER: 0.2138
2026-01-08 17:35:09,057: t15.2023.09.01 val PER: 0.1112
2026-01-08 17:35:09,058: t15.2023.09.03 val PER: 0.2043
2026-01-08 17:35:09,060: t15.2023.09.24 val PER: 0.1529
2026-01-08 17:35:09,061: t15.2023.09.29 val PER: 0.1589
2026-01-08 17:35:09,063: t15.2023.10.01 val PER: 0.2173
2026-01-08 17:35:09,064: t15.2023.10.06 val PER: 0.1141
2026-01-08 17:35:09,066: t15.2023.10.08 val PER: 0.2909
2026-01-08 17:35:09,068: t15.2023.10.13 val PER: 0.2475
2026-01-08 17:35:09,070: t15.2023.10.15 val PER: 0.1898
2026-01-08 17:35:09,071: t15.2023.10.20 val PER: 0.2013
2026-01-08 17:35:09,073: t15.2023.10.22 val PER: 0.1425
2026-01-08 17:35:09,074: t15.2023.11.03 val PER: 0.2130
2026-01-08 17:35:09,076: t15.2023.11.04 val PER: 0.0444
2026-01-08 17:35:09,077: t15.2023.11.17 val PER: 0.0731
2026-01-08 17:35:09,079: t15.2023.11.19 val PER: 0.0679
2026-01-08 17:35:09,081: t15.2023.11.26 val PER: 0.1848
2026-01-08 17:35:09,082: t15.2023.12.03 val PER: 0.1523
2026-01-08 17:35:09,084: t15.2023.12.08 val PER: 0.1724
2026-01-08 17:35:09,085: t15.2023.12.10 val PER: 0.1380
2026-01-08 17:35:09,087: t15.2023.12.17 val PER: 0.1746
2026-01-08 17:35:09,089: t15.2023.12.29 val PER: 0.1846
2026-01-08 17:35:09,090: t15.2024.02.25 val PER: 0.1629
2026-01-08 17:35:09,093: t15.2024.03.08 val PER: 0.2660
2026-01-08 17:35:09,095: t15.2024.03.15 val PER: 0.2420
2026-01-08 17:35:09,097: t15.2024.03.17 val PER: 0.1855
2026-01-08 17:35:09,098: t15.2024.05.10 val PER: 0.2036
2026-01-08 17:35:09,100: t15.2024.06.14 val PER: 0.2114
2026-01-08 17:35:09,102: t15.2024.07.19 val PER: 0.3006
2026-01-08 17:35:09,103: t15.2024.07.21 val PER: 0.1455
2026-01-08 17:35:09,105: t15.2024.07.28 val PER: 0.1794
2026-01-08 17:35:09,106: t15.2025.01.10 val PER: 0.3719
2026-01-08 17:35:09,108: t15.2025.01.12 val PER: 0.1994
2026-01-08 17:35:09,109: t15.2025.03.14 val PER: 0.3624
2026-01-08 17:35:09,111: t15.2025.03.16 val PER: 0.2251
2026-01-08 17:35:09,113: t15.2025.03.30 val PER: 0.3230
2026-01-08 17:35:09,114: t15.2025.04.13 val PER: 0.2582
2026-01-08 17:35:09,275: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_7500
2026-01-08 17:35:18,181: Train batch 7600: loss: 16.60 grad norm: 74.30 time: 0.076
2026-01-08 17:35:35,469: Train batch 7800: loss: 14.34 grad norm: 65.01 time: 0.063
2026-01-08 17:35:53,202: Train batch 8000: loss: 11.77 grad norm: 53.32 time: 0.080
2026-01-08 17:35:53,204: Running test after training batch: 8000
2026-01-08 17:35:53,305: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:35:58,279: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the go to this point will
2026-01-08 17:35:58,318: WER debug example
  GT : how does it keep the cost down
  PR : how i see it the ca sta
2026-01-08 17:36:07,145: Val batch 8000: PER (avg): 0.1892 CTC Loss (avg): 18.3280 WER(5gram): 58.87% (n=256) time: 13.938
2026-01-08 17:36:07,147: WER lens: avg_true_words=5.99 avg_pred_words=5.99 max_pred_words=12
2026-01-08 17:36:07,153: t15.2023.08.13 val PER: 0.1518
2026-01-08 17:36:07,155: t15.2023.08.18 val PER: 0.1391
2026-01-08 17:36:07,157: t15.2023.08.20 val PER: 0.1366
2026-01-08 17:36:07,158: t15.2023.08.25 val PER: 0.1370
2026-01-08 17:36:07,160: t15.2023.08.27 val PER: 0.2170
2026-01-08 17:36:07,161: t15.2023.09.01 val PER: 0.1031
2026-01-08 17:36:07,163: t15.2023.09.03 val PER: 0.1936
2026-01-08 17:36:07,165: t15.2023.09.24 val PER: 0.1541
2026-01-08 17:36:07,166: t15.2023.09.29 val PER: 0.1589
2026-01-08 17:36:07,168: t15.2023.10.01 val PER: 0.2054
2026-01-08 17:36:07,169: t15.2023.10.06 val PER: 0.1130
2026-01-08 17:36:07,171: t15.2023.10.08 val PER: 0.2706
2026-01-08 17:36:07,172: t15.2023.10.13 val PER: 0.2459
2026-01-08 17:36:07,174: t15.2023.10.15 val PER: 0.1885
2026-01-08 17:36:07,176: t15.2023.10.20 val PER: 0.1980
2026-01-08 17:36:07,177: t15.2023.10.22 val PER: 0.1548
2026-01-08 17:36:07,179: t15.2023.11.03 val PER: 0.2062
2026-01-08 17:36:07,180: t15.2023.11.04 val PER: 0.0478
2026-01-08 17:36:07,182: t15.2023.11.17 val PER: 0.0607
2026-01-08 17:36:07,183: t15.2023.11.19 val PER: 0.0778
2026-01-08 17:36:07,186: t15.2023.11.26 val PER: 0.1783
2026-01-08 17:36:07,188: t15.2023.12.03 val PER: 0.1502
2026-01-08 17:36:07,189: t15.2023.12.08 val PER: 0.1591
2026-01-08 17:36:07,191: t15.2023.12.10 val PER: 0.1340
2026-01-08 17:36:07,192: t15.2023.12.17 val PER: 0.1840
2026-01-08 17:36:07,194: t15.2023.12.29 val PER: 0.1764
2026-01-08 17:36:07,195: t15.2024.02.25 val PER: 0.1503
2026-01-08 17:36:07,197: t15.2024.03.08 val PER: 0.2560
2026-01-08 17:36:07,199: t15.2024.03.15 val PER: 0.2258
2026-01-08 17:36:07,200: t15.2024.03.17 val PER: 0.1869
2026-01-08 17:36:07,202: t15.2024.05.10 val PER: 0.2110
2026-01-08 17:36:07,203: t15.2024.06.14 val PER: 0.2019
2026-01-08 17:36:07,205: t15.2024.07.19 val PER: 0.2920
2026-01-08 17:36:07,206: t15.2024.07.21 val PER: 0.1290
2026-01-08 17:36:07,208: t15.2024.07.28 val PER: 0.1779
2026-01-08 17:36:07,209: t15.2025.01.10 val PER: 0.3636
2026-01-08 17:36:07,210: t15.2025.01.12 val PER: 0.2040
2026-01-08 17:36:07,212: t15.2025.03.14 val PER: 0.3432
2026-01-08 17:36:07,213: t15.2025.03.16 val PER: 0.2277
2026-01-08 17:36:07,215: t15.2025.03.30 val PER: 0.3402
2026-01-08 17:36:07,216: t15.2025.04.13 val PER: 0.2682
2026-01-08 17:36:07,377: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_8000
2026-01-08 17:36:24,353: Train batch 8200: loss: 10.30 grad norm: 53.59 time: 0.061
2026-01-08 17:36:41,354: Train batch 8400: loss: 11.53 grad norm: 53.82 time: 0.071
2026-01-08 17:36:50,363: Running test after training batch: 8500
2026-01-08 17:36:50,473: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:36:55,758: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point will
2026-01-08 17:36:55,798: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the cost
2026-01-08 17:37:04,548: Val batch 8500: PER (avg): 0.1810 CTC Loss (avg): 17.7322 WER(5gram): 46.22% (n=256) time: 14.182
2026-01-08 17:37:04,550: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-08 17:37:04,554: t15.2023.08.13 val PER: 0.1393
2026-01-08 17:37:04,556: t15.2023.08.18 val PER: 0.1433
2026-01-08 17:37:04,558: t15.2023.08.20 val PER: 0.1422
2026-01-08 17:37:04,560: t15.2023.08.25 val PER: 0.1250
2026-01-08 17:37:04,562: t15.2023.08.27 val PER: 0.1961
2026-01-08 17:37:04,564: t15.2023.09.01 val PER: 0.0966
2026-01-08 17:37:04,566: t15.2023.09.03 val PER: 0.1817
2026-01-08 17:37:04,568: t15.2023.09.24 val PER: 0.1517
2026-01-08 17:37:04,570: t15.2023.09.29 val PER: 0.1493
2026-01-08 17:37:04,571: t15.2023.10.01 val PER: 0.2021
2026-01-08 17:37:04,575: t15.2023.10.06 val PER: 0.1119
2026-01-08 17:37:04,577: t15.2023.10.08 val PER: 0.2639
2026-01-08 17:37:04,578: t15.2023.10.13 val PER: 0.2366
2026-01-08 17:37:04,581: t15.2023.10.15 val PER: 0.1786
2026-01-08 17:37:04,583: t15.2023.10.20 val PER: 0.1846
2026-01-08 17:37:04,585: t15.2023.10.22 val PER: 0.1425
2026-01-08 17:37:04,587: t15.2023.11.03 val PER: 0.2083
2026-01-08 17:37:04,589: t15.2023.11.04 val PER: 0.0444
2026-01-08 17:37:04,590: t15.2023.11.17 val PER: 0.0560
2026-01-08 17:37:04,592: t15.2023.11.19 val PER: 0.0539
2026-01-08 17:37:04,594: t15.2023.11.26 val PER: 0.1667
2026-01-08 17:37:04,596: t15.2023.12.03 val PER: 0.1418
2026-01-08 17:37:04,598: t15.2023.12.08 val PER: 0.1518
2026-01-08 17:37:04,600: t15.2023.12.10 val PER: 0.1248
2026-01-08 17:37:04,602: t15.2023.12.17 val PER: 0.1653
2026-01-08 17:37:04,604: t15.2023.12.29 val PER: 0.1853
2026-01-08 17:37:04,605: t15.2024.02.25 val PER: 0.1461
2026-01-08 17:37:04,607: t15.2024.03.08 val PER: 0.2575
2026-01-08 17:37:04,609: t15.2024.03.15 val PER: 0.2226
2026-01-08 17:37:04,611: t15.2024.03.17 val PER: 0.1778
2026-01-08 17:37:04,613: t15.2024.05.10 val PER: 0.1961
2026-01-08 17:37:04,614: t15.2024.06.14 val PER: 0.1845
2026-01-08 17:37:04,616: t15.2024.07.19 val PER: 0.2769
2026-01-08 17:37:04,618: t15.2024.07.21 val PER: 0.1145
2026-01-08 17:37:04,620: t15.2024.07.28 val PER: 0.1676
2026-01-08 17:37:04,622: t15.2025.01.10 val PER: 0.3375
2026-01-08 17:37:04,624: t15.2025.01.12 val PER: 0.1932
2026-01-08 17:37:04,625: t15.2025.03.14 val PER: 0.3609
2026-01-08 17:37:04,627: t15.2025.03.16 val PER: 0.2094
2026-01-08 17:37:04,629: t15.2025.03.30 val PER: 0.3057
2026-01-08 17:37:04,631: t15.2025.04.13 val PER: 0.2596
2026-01-08 17:37:04,633: New best val WER(5gram) 51.17% --> 46.22%
2026-01-08 17:37:04,846: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_8500
2026-01-08 17:37:13,794: Train batch 8600: loss: 16.24 grad norm: 60.83 time: 0.061
2026-01-08 17:37:31,685: Train batch 8800: loss: 16.45 grad norm: 72.99 time: 0.068
2026-01-08 17:37:49,360: Train batch 9000: loss: 16.47 grad norm: 63.71 time: 0.082
2026-01-08 17:37:49,362: Running test after training batch: 9000
2026-01-08 17:37:49,487: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:37:54,653: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the go at this point as well
2026-01-08 17:37:54,695: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the cost
2026-01-08 17:38:03,592: Val batch 9000: PER (avg): 0.1788 CTC Loss (avg): 17.3310 WER(5gram): 48.24% (n=256) time: 14.225
2026-01-08 17:38:03,594: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-08 17:38:03,602: t15.2023.08.13 val PER: 0.1435
2026-01-08 17:38:03,604: t15.2023.08.18 val PER: 0.1299
2026-01-08 17:38:03,605: t15.2023.08.20 val PER: 0.1430
2026-01-08 17:38:03,607: t15.2023.08.25 val PER: 0.1175
2026-01-08 17:38:03,609: t15.2023.08.27 val PER: 0.1913
2026-01-08 17:38:03,610: t15.2023.09.01 val PER: 0.0982
2026-01-08 17:38:03,612: t15.2023.09.03 val PER: 0.1865
2026-01-08 17:38:03,614: t15.2023.09.24 val PER: 0.1432
2026-01-08 17:38:03,615: t15.2023.09.29 val PER: 0.1563
2026-01-08 17:38:03,617: t15.2023.10.01 val PER: 0.2034
2026-01-08 17:38:03,619: t15.2023.10.06 val PER: 0.1044
2026-01-08 17:38:03,621: t15.2023.10.08 val PER: 0.2395
2026-01-08 17:38:03,622: t15.2023.10.13 val PER: 0.2358
2026-01-08 17:38:03,624: t15.2023.10.15 val PER: 0.1727
2026-01-08 17:38:03,626: t15.2023.10.20 val PER: 0.1913
2026-01-08 17:38:03,628: t15.2023.10.22 val PER: 0.1403
2026-01-08 17:38:03,629: t15.2023.11.03 val PER: 0.2035
2026-01-08 17:38:03,633: t15.2023.11.04 val PER: 0.0410
2026-01-08 17:38:03,634: t15.2023.11.17 val PER: 0.0544
2026-01-08 17:38:03,637: t15.2023.11.19 val PER: 0.0679
2026-01-08 17:38:03,639: t15.2023.11.26 val PER: 0.1667
2026-01-08 17:38:03,640: t15.2023.12.03 val PER: 0.1376
2026-01-08 17:38:03,642: t15.2023.12.08 val PER: 0.1451
2026-01-08 17:38:03,644: t15.2023.12.10 val PER: 0.1209
2026-01-08 17:38:03,646: t15.2023.12.17 val PER: 0.1663
2026-01-08 17:38:03,647: t15.2023.12.29 val PER: 0.1682
2026-01-08 17:38:03,649: t15.2024.02.25 val PER: 0.1461
2026-01-08 17:38:03,651: t15.2024.03.08 val PER: 0.2461
2026-01-08 17:38:03,653: t15.2024.03.15 val PER: 0.2283
2026-01-08 17:38:03,655: t15.2024.03.17 val PER: 0.1743
2026-01-08 17:38:03,656: t15.2024.05.10 val PER: 0.1902
2026-01-08 17:38:03,658: t15.2024.06.14 val PER: 0.1956
2026-01-08 17:38:03,660: t15.2024.07.19 val PER: 0.2749
2026-01-08 17:38:03,662: t15.2024.07.21 val PER: 0.1248
2026-01-08 17:38:03,664: t15.2024.07.28 val PER: 0.1588
2026-01-08 17:38:03,665: t15.2025.01.10 val PER: 0.3471
2026-01-08 17:38:03,667: t15.2025.01.12 val PER: 0.1840
2026-01-08 17:38:03,669: t15.2025.03.14 val PER: 0.3609
2026-01-08 17:38:03,671: t15.2025.03.16 val PER: 0.2094
2026-01-08 17:38:03,672: t15.2025.03.30 val PER: 0.3138
2026-01-08 17:38:03,674: t15.2025.04.13 val PER: 0.2525
2026-01-08 17:38:03,835: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_9000
2026-01-08 17:38:20,748: Train batch 9200: loss: 9.81 grad norm: 53.55 time: 0.063
2026-01-08 17:38:38,876: Train batch 9400: loss: 7.84 grad norm: 46.81 time: 0.078
2026-01-08 17:38:47,978: Running test after training batch: 9500
2026-01-08 17:38:48,079: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:38:53,356: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:38:53,395: WER debug example
  GT : how does it keep the cost down
  PR : how i see it keep the cost
2026-01-08 17:39:02,715: Val batch 9500: PER (avg): 0.1766 CTC Loss (avg): 17.3882 WER(5gram): 39.83% (n=256) time: 14.736
2026-01-08 17:39:02,718: WER lens: avg_true_words=5.99 avg_pred_words=6.26 max_pred_words=12
2026-01-08 17:39:02,723: t15.2023.08.13 val PER: 0.1351
2026-01-08 17:39:02,724: t15.2023.08.18 val PER: 0.1291
2026-01-08 17:39:02,726: t15.2023.08.20 val PER: 0.1279
2026-01-08 17:39:02,727: t15.2023.08.25 val PER: 0.1325
2026-01-08 17:39:02,729: t15.2023.08.27 val PER: 0.1994
2026-01-08 17:39:02,731: t15.2023.09.01 val PER: 0.0950
2026-01-08 17:39:02,734: t15.2023.09.03 val PER: 0.1805
2026-01-08 17:39:02,736: t15.2023.09.24 val PER: 0.1481
2026-01-08 17:39:02,738: t15.2023.09.29 val PER: 0.1474
2026-01-08 17:39:02,740: t15.2023.10.01 val PER: 0.1856
2026-01-08 17:39:02,741: t15.2023.10.06 val PER: 0.1184
2026-01-08 17:39:02,743: t15.2023.10.08 val PER: 0.2503
2026-01-08 17:39:02,745: t15.2023.10.13 val PER: 0.2289
2026-01-08 17:39:02,747: t15.2023.10.15 val PER: 0.1833
2026-01-08 17:39:02,748: t15.2023.10.20 val PER: 0.2114
2026-01-08 17:39:02,750: t15.2023.10.22 val PER: 0.1347
2026-01-08 17:39:02,752: t15.2023.11.03 val PER: 0.1940
2026-01-08 17:39:02,754: t15.2023.11.04 val PER: 0.0478
2026-01-08 17:39:02,756: t15.2023.11.17 val PER: 0.0529
2026-01-08 17:39:02,757: t15.2023.11.19 val PER: 0.0639
2026-01-08 17:39:02,759: t15.2023.11.26 val PER: 0.1543
2026-01-08 17:39:02,761: t15.2023.12.03 val PER: 0.1292
2026-01-08 17:39:02,762: t15.2023.12.08 val PER: 0.1438
2026-01-08 17:39:02,764: t15.2023.12.10 val PER: 0.1235
2026-01-08 17:39:02,766: t15.2023.12.17 val PER: 0.1570
2026-01-08 17:39:02,768: t15.2023.12.29 val PER: 0.1668
2026-01-08 17:39:02,770: t15.2024.02.25 val PER: 0.1531
2026-01-08 17:39:02,772: t15.2024.03.08 val PER: 0.2504
2026-01-08 17:39:02,774: t15.2024.03.15 val PER: 0.2314
2026-01-08 17:39:02,775: t15.2024.03.17 val PER: 0.1667
2026-01-08 17:39:02,777: t15.2024.05.10 val PER: 0.1947
2026-01-08 17:39:02,779: t15.2024.06.14 val PER: 0.1861
2026-01-08 17:39:02,780: t15.2024.07.19 val PER: 0.2690
2026-01-08 17:39:02,782: t15.2024.07.21 val PER: 0.1269
2026-01-08 17:39:02,784: t15.2024.07.28 val PER: 0.1610
2026-01-08 17:39:02,785: t15.2025.01.10 val PER: 0.3471
2026-01-08 17:39:02,787: t15.2025.01.12 val PER: 0.1871
2026-01-08 17:39:02,789: t15.2025.03.14 val PER: 0.3550
2026-01-08 17:39:02,790: t15.2025.03.16 val PER: 0.2081
2026-01-08 17:39:02,792: t15.2025.03.30 val PER: 0.3092
2026-01-08 17:39:02,794: t15.2025.04.13 val PER: 0.2511
2026-01-08 17:39:02,796: New best val WER(5gram) 46.22% --> 39.83%
2026-01-08 17:39:02,996: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_9500
2026-01-08 17:39:12,092: Train batch 9600: loss: 8.96 grad norm: 47.02 time: 0.082
2026-01-08 17:39:30,238: Train batch 9800: loss: 13.06 grad norm: 64.71 time: 0.070
2026-01-08 17:39:47,338: Train batch 10000: loss: 6.70 grad norm: 41.72 time: 0.068
2026-01-08 17:39:47,341: Running test after training batch: 10000
2026-01-08 17:39:47,439: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:39:52,784: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:39:52,826: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-08 17:40:01,946: Val batch 10000: PER (avg): 0.1719 CTC Loss (avg): 16.9807 WER(5gram): 37.42% (n=256) time: 14.603
2026-01-08 17:40:01,949: WER lens: avg_true_words=5.99 avg_pred_words=6.29 max_pred_words=13
2026-01-08 17:40:01,955: t15.2023.08.13 val PER: 0.1435
2026-01-08 17:40:01,957: t15.2023.08.18 val PER: 0.1324
2026-01-08 17:40:01,961: t15.2023.08.20 val PER: 0.1239
2026-01-08 17:40:01,962: t15.2023.08.25 val PER: 0.1250
2026-01-08 17:40:01,964: t15.2023.08.27 val PER: 0.2010
2026-01-08 17:40:01,966: t15.2023.09.01 val PER: 0.0836
2026-01-08 17:40:01,968: t15.2023.09.03 val PER: 0.1698
2026-01-08 17:40:01,969: t15.2023.09.24 val PER: 0.1432
2026-01-08 17:40:01,971: t15.2023.09.29 val PER: 0.1468
2026-01-08 17:40:01,972: t15.2023.10.01 val PER: 0.1876
2026-01-08 17:40:01,974: t15.2023.10.06 val PER: 0.1044
2026-01-08 17:40:01,976: t15.2023.10.08 val PER: 0.2476
2026-01-08 17:40:01,977: t15.2023.10.13 val PER: 0.2258
2026-01-08 17:40:01,979: t15.2023.10.15 val PER: 0.1734
2026-01-08 17:40:01,981: t15.2023.10.20 val PER: 0.2047
2026-01-08 17:40:01,982: t15.2023.10.22 val PER: 0.1281
2026-01-08 17:40:01,984: t15.2023.11.03 val PER: 0.1934
2026-01-08 17:40:01,986: t15.2023.11.04 val PER: 0.0375
2026-01-08 17:40:01,987: t15.2023.11.17 val PER: 0.0420
2026-01-08 17:40:01,989: t15.2023.11.19 val PER: 0.0559
2026-01-08 17:40:01,990: t15.2023.11.26 val PER: 0.1464
2026-01-08 17:40:01,992: t15.2023.12.03 val PER: 0.1376
2026-01-08 17:40:01,994: t15.2023.12.08 val PER: 0.1385
2026-01-08 17:40:01,996: t15.2023.12.10 val PER: 0.1183
2026-01-08 17:40:01,999: t15.2023.12.17 val PER: 0.1601
2026-01-08 17:40:02,002: t15.2023.12.29 val PER: 0.1654
2026-01-08 17:40:02,003: t15.2024.02.25 val PER: 0.1433
2026-01-08 17:40:02,006: t15.2024.03.08 val PER: 0.2404
2026-01-08 17:40:02,008: t15.2024.03.15 val PER: 0.2208
2026-01-08 17:40:02,010: t15.2024.03.17 val PER: 0.1492
2026-01-08 17:40:02,011: t15.2024.05.10 val PER: 0.1976
2026-01-08 17:40:02,013: t15.2024.06.14 val PER: 0.1893
2026-01-08 17:40:02,014: t15.2024.07.19 val PER: 0.2610
2026-01-08 17:40:02,016: t15.2024.07.21 val PER: 0.1248
2026-01-08 17:40:02,018: t15.2024.07.28 val PER: 0.1610
2026-01-08 17:40:02,019: t15.2025.01.10 val PER: 0.3251
2026-01-08 17:40:02,021: t15.2025.01.12 val PER: 0.1848
2026-01-08 17:40:02,022: t15.2025.03.14 val PER: 0.3462
2026-01-08 17:40:02,024: t15.2025.03.16 val PER: 0.2068
2026-01-08 17:40:02,026: t15.2025.03.30 val PER: 0.3011
2026-01-08 17:40:02,027: t15.2025.04.13 val PER: 0.2496
2026-01-08 17:40:02,029: New best val WER(5gram) 39.83% --> 37.42%
2026-01-08 17:40:02,232: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_10000
2026-01-08 17:40:20,445: Train batch 10200: loss: 7.28 grad norm: 42.51 time: 0.058
2026-01-08 17:40:39,010: Train batch 10400: loss: 9.29 grad norm: 46.20 time: 0.084
2026-01-08 17:40:48,051: Running test after training batch: 10500
2026-01-08 17:40:48,148: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:40:53,235: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:40:53,275: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-08 17:41:02,091: Val batch 10500: PER (avg): 0.1689 CTC Loss (avg): 16.8567 WER(5gram): 41.33% (n=256) time: 14.038
2026-01-08 17:41:02,093: WER lens: avg_true_words=5.99 avg_pred_words=6.23 max_pred_words=12
2026-01-08 17:41:02,096: t15.2023.08.13 val PER: 0.1383
2026-01-08 17:41:02,098: t15.2023.08.18 val PER: 0.1215
2026-01-08 17:41:02,100: t15.2023.08.20 val PER: 0.1303
2026-01-08 17:41:02,101: t15.2023.08.25 val PER: 0.1220
2026-01-08 17:41:02,103: t15.2023.08.27 val PER: 0.1929
2026-01-08 17:41:02,105: t15.2023.09.01 val PER: 0.0925
2026-01-08 17:41:02,106: t15.2023.09.03 val PER: 0.1710
2026-01-08 17:41:02,108: t15.2023.09.24 val PER: 0.1420
2026-01-08 17:41:02,109: t15.2023.09.29 val PER: 0.1461
2026-01-08 17:41:02,111: t15.2023.10.01 val PER: 0.1942
2026-01-08 17:41:02,112: t15.2023.10.06 val PER: 0.1012
2026-01-08 17:41:02,114: t15.2023.10.08 val PER: 0.2449
2026-01-08 17:41:02,115: t15.2023.10.13 val PER: 0.2242
2026-01-08 17:41:02,117: t15.2023.10.15 val PER: 0.1721
2026-01-08 17:41:02,119: t15.2023.10.20 val PER: 0.1779
2026-01-08 17:41:02,120: t15.2023.10.22 val PER: 0.1225
2026-01-08 17:41:02,121: t15.2023.11.03 val PER: 0.1879
2026-01-08 17:41:02,123: t15.2023.11.04 val PER: 0.0580
2026-01-08 17:41:02,125: t15.2023.11.17 val PER: 0.0498
2026-01-08 17:41:02,126: t15.2023.11.19 val PER: 0.0639
2026-01-08 17:41:02,128: t15.2023.11.26 val PER: 0.1514
2026-01-08 17:41:02,129: t15.2023.12.03 val PER: 0.1303
2026-01-08 17:41:02,131: t15.2023.12.08 val PER: 0.1332
2026-01-08 17:41:02,132: t15.2023.12.10 val PER: 0.1156
2026-01-08 17:41:02,134: t15.2023.12.17 val PER: 0.1538
2026-01-08 17:41:02,135: t15.2023.12.29 val PER: 0.1544
2026-01-08 17:41:02,137: t15.2024.02.25 val PER: 0.1404
2026-01-08 17:41:02,138: t15.2024.03.08 val PER: 0.2347
2026-01-08 17:41:02,140: t15.2024.03.15 val PER: 0.2220
2026-01-08 17:41:02,142: t15.2024.03.17 val PER: 0.1541
2026-01-08 17:41:02,143: t15.2024.05.10 val PER: 0.1857
2026-01-08 17:41:02,144: t15.2024.06.14 val PER: 0.1703
2026-01-08 17:41:02,146: t15.2024.07.19 val PER: 0.2472
2026-01-08 17:41:02,147: t15.2024.07.21 val PER: 0.1097
2026-01-08 17:41:02,149: t15.2024.07.28 val PER: 0.1522
2026-01-08 17:41:02,150: t15.2025.01.10 val PER: 0.3264
2026-01-08 17:41:02,152: t15.2025.01.12 val PER: 0.1701
2026-01-08 17:41:02,154: t15.2025.03.14 val PER: 0.3698
2026-01-08 17:41:02,156: t15.2025.03.16 val PER: 0.1937
2026-01-08 17:41:02,157: t15.2025.03.30 val PER: 0.3011
2026-01-08 17:41:02,159: t15.2025.04.13 val PER: 0.2525
2026-01-08 17:41:02,320: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_10500
2026-01-08 17:41:11,688: Train batch 10600: loss: 8.96 grad norm: 50.07 time: 0.084
2026-01-08 17:41:30,141: Train batch 10800: loss: 13.78 grad norm: 66.56 time: 0.075
2026-01-08 17:41:48,317: Train batch 11000: loss: 14.98 grad norm: 65.70 time: 0.065
2026-01-08 17:41:48,319: Running test after training batch: 11000
2026-01-08 17:41:48,452: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:41:53,600: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 17:41:53,645: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 17:42:02,542: Val batch 11000: PER (avg): 0.1658 CTC Loss (avg): 16.3363 WER(5gram): 42.11% (n=256) time: 14.218
2026-01-08 17:42:02,544: WER lens: avg_true_words=5.99 avg_pred_words=6.34 max_pred_words=13
2026-01-08 17:42:02,550: t15.2023.08.13 val PER: 0.1310
2026-01-08 17:42:02,552: t15.2023.08.18 val PER: 0.1207
2026-01-08 17:42:02,555: t15.2023.08.20 val PER: 0.1319
2026-01-08 17:42:02,557: t15.2023.08.25 val PER: 0.1145
2026-01-08 17:42:02,558: t15.2023.08.27 val PER: 0.2026
2026-01-08 17:42:02,560: t15.2023.09.01 val PER: 0.0877
2026-01-08 17:42:02,562: t15.2023.09.03 val PER: 0.1698
2026-01-08 17:42:02,564: t15.2023.09.24 val PER: 0.1335
2026-01-08 17:42:02,565: t15.2023.09.29 val PER: 0.1366
2026-01-08 17:42:02,567: t15.2023.10.01 val PER: 0.1823
2026-01-08 17:42:02,569: t15.2023.10.06 val PER: 0.0958
2026-01-08 17:42:02,570: t15.2023.10.08 val PER: 0.2341
2026-01-08 17:42:02,572: t15.2023.10.13 val PER: 0.2172
2026-01-08 17:42:02,573: t15.2023.10.15 val PER: 0.1701
2026-01-08 17:42:02,575: t15.2023.10.20 val PER: 0.1879
2026-01-08 17:42:02,577: t15.2023.10.22 val PER: 0.1247
2026-01-08 17:42:02,578: t15.2023.11.03 val PER: 0.1839
2026-01-08 17:42:02,580: t15.2023.11.04 val PER: 0.0444
2026-01-08 17:42:02,582: t15.2023.11.17 val PER: 0.0482
2026-01-08 17:42:02,583: t15.2023.11.19 val PER: 0.0519
2026-01-08 17:42:02,585: t15.2023.11.26 val PER: 0.1391
2026-01-08 17:42:02,588: t15.2023.12.03 val PER: 0.1176
2026-01-08 17:42:02,590: t15.2023.12.08 val PER: 0.1285
2026-01-08 17:42:02,592: t15.2023.12.10 val PER: 0.1209
2026-01-08 17:42:02,594: t15.2023.12.17 val PER: 0.1486
2026-01-08 17:42:02,595: t15.2023.12.29 val PER: 0.1544
2026-01-08 17:42:02,597: t15.2024.02.25 val PER: 0.1404
2026-01-08 17:42:02,598: t15.2024.03.08 val PER: 0.2290
2026-01-08 17:42:02,602: t15.2024.03.15 val PER: 0.2139
2026-01-08 17:42:02,604: t15.2024.03.17 val PER: 0.1618
2026-01-08 17:42:02,606: t15.2024.05.10 val PER: 0.1813
2026-01-08 17:42:02,607: t15.2024.06.14 val PER: 0.1735
2026-01-08 17:42:02,609: t15.2024.07.19 val PER: 0.2617
2026-01-08 17:42:02,611: t15.2024.07.21 val PER: 0.1076
2026-01-08 17:42:02,612: t15.2024.07.28 val PER: 0.1500
2026-01-08 17:42:02,614: t15.2025.01.10 val PER: 0.3320
2026-01-08 17:42:02,615: t15.2025.01.12 val PER: 0.1709
2026-01-08 17:42:02,617: t15.2025.03.14 val PER: 0.3343
2026-01-08 17:42:02,619: t15.2025.03.16 val PER: 0.1937
2026-01-08 17:42:02,620: t15.2025.03.30 val PER: 0.3092
2026-01-08 17:42:02,622: t15.2025.04.13 val PER: 0.2525
2026-01-08 17:42:02,783: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_11000
2026-01-08 17:42:21,176: Train batch 11200: loss: 10.54 grad norm: 59.52 time: 0.081
2026-01-08 17:42:39,495: Train batch 11400: loss: 9.56 grad norm: 51.15 time: 0.065
2026-01-08 17:42:48,744: Running test after training batch: 11500
2026-01-08 17:42:48,888: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:42:54,157: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:42:54,197: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 17:43:03,159: Val batch 11500: PER (avg): 0.1627 CTC Loss (avg): 16.2226 WER(5gram): 38.98% (n=256) time: 14.413
2026-01-08 17:43:03,161: WER lens: avg_true_words=5.99 avg_pred_words=6.39 max_pred_words=13
2026-01-08 17:43:03,164: t15.2023.08.13 val PER: 0.1268
2026-01-08 17:43:03,166: t15.2023.08.18 val PER: 0.1224
2026-01-08 17:43:03,168: t15.2023.08.20 val PER: 0.1215
2026-01-08 17:43:03,169: t15.2023.08.25 val PER: 0.1099
2026-01-08 17:43:03,171: t15.2023.08.27 val PER: 0.1961
2026-01-08 17:43:03,173: t15.2023.09.01 val PER: 0.0820
2026-01-08 17:43:03,174: t15.2023.09.03 val PER: 0.1663
2026-01-08 17:43:03,176: t15.2023.09.24 val PER: 0.1335
2026-01-08 17:43:03,178: t15.2023.09.29 val PER: 0.1398
2026-01-08 17:43:03,180: t15.2023.10.01 val PER: 0.1863
2026-01-08 17:43:03,181: t15.2023.10.06 val PER: 0.0893
2026-01-08 17:43:03,183: t15.2023.10.08 val PER: 0.2355
2026-01-08 17:43:03,184: t15.2023.10.13 val PER: 0.2133
2026-01-08 17:43:03,186: t15.2023.10.15 val PER: 0.1694
2026-01-08 17:43:03,187: t15.2023.10.20 val PER: 0.1879
2026-01-08 17:43:03,189: t15.2023.10.22 val PER: 0.1225
2026-01-08 17:43:03,191: t15.2023.11.03 val PER: 0.1764
2026-01-08 17:43:03,192: t15.2023.11.04 val PER: 0.0375
2026-01-08 17:43:03,194: t15.2023.11.17 val PER: 0.0451
2026-01-08 17:43:03,196: t15.2023.11.19 val PER: 0.0499
2026-01-08 17:43:03,197: t15.2023.11.26 val PER: 0.1333
2026-01-08 17:43:03,199: t15.2023.12.03 val PER: 0.1218
2026-01-08 17:43:03,201: t15.2023.12.08 val PER: 0.1245
2026-01-08 17:43:03,202: t15.2023.12.10 val PER: 0.0959
2026-01-08 17:43:03,204: t15.2023.12.17 val PER: 0.1580
2026-01-08 17:43:03,205: t15.2023.12.29 val PER: 0.1441
2026-01-08 17:43:03,207: t15.2024.02.25 val PER: 0.1236
2026-01-08 17:43:03,208: t15.2024.03.08 val PER: 0.2418
2026-01-08 17:43:03,210: t15.2024.03.15 val PER: 0.2189
2026-01-08 17:43:03,212: t15.2024.03.17 val PER: 0.1506
2026-01-08 17:43:03,213: t15.2024.05.10 val PER: 0.1872
2026-01-08 17:43:03,215: t15.2024.06.14 val PER: 0.1751
2026-01-08 17:43:03,217: t15.2024.07.19 val PER: 0.2571
2026-01-08 17:43:03,219: t15.2024.07.21 val PER: 0.1069
2026-01-08 17:43:03,220: t15.2024.07.28 val PER: 0.1471
2026-01-08 17:43:03,222: t15.2025.01.10 val PER: 0.3182
2026-01-08 17:43:03,224: t15.2025.01.12 val PER: 0.1686
2026-01-08 17:43:03,227: t15.2025.03.14 val PER: 0.3447
2026-01-08 17:43:03,228: t15.2025.03.16 val PER: 0.1924
2026-01-08 17:43:03,230: t15.2025.03.30 val PER: 0.2989
2026-01-08 17:43:03,231: t15.2025.04.13 val PER: 0.2411
2026-01-08 17:43:03,383: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_11500
2026-01-08 17:43:12,403: Train batch 11600: loss: 12.40 grad norm: 60.80 time: 0.068
2026-01-08 17:43:30,711: Train batch 11800: loss: 6.82 grad norm: 43.86 time: 0.051
2026-01-08 17:43:48,966: Train batch 12000: loss: 14.47 grad norm: 57.82 time: 0.082
2026-01-08 17:43:48,968: Running test after training batch: 12000
2026-01-08 17:43:49,069: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:43:54,470: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:43:54,509: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 17:44:03,479: Val batch 12000: PER (avg): 0.1614 CTC Loss (avg): 15.9875 WER(5gram): 35.53% (n=256) time: 14.509
2026-01-08 17:44:03,482: WER lens: avg_true_words=5.99 avg_pred_words=6.33 max_pred_words=14
2026-01-08 17:44:03,484: t15.2023.08.13 val PER: 0.1268
2026-01-08 17:44:03,486: t15.2023.08.18 val PER: 0.1232
2026-01-08 17:44:03,488: t15.2023.08.20 val PER: 0.1136
2026-01-08 17:44:03,490: t15.2023.08.25 val PER: 0.1160
2026-01-08 17:44:03,492: t15.2023.08.27 val PER: 0.1929
2026-01-08 17:44:03,493: t15.2023.09.01 val PER: 0.0877
2026-01-08 17:44:03,495: t15.2023.09.03 val PER: 0.1722
2026-01-08 17:44:03,497: t15.2023.09.24 val PER: 0.1359
2026-01-08 17:44:03,499: t15.2023.09.29 val PER: 0.1429
2026-01-08 17:44:03,500: t15.2023.10.01 val PER: 0.1757
2026-01-08 17:44:03,502: t15.2023.10.06 val PER: 0.0969
2026-01-08 17:44:03,504: t15.2023.10.08 val PER: 0.2341
2026-01-08 17:44:03,506: t15.2023.10.13 val PER: 0.2141
2026-01-08 17:44:03,507: t15.2023.10.15 val PER: 0.1707
2026-01-08 17:44:03,509: t15.2023.10.20 val PER: 0.1846
2026-01-08 17:44:03,510: t15.2023.10.22 val PER: 0.1214
2026-01-08 17:44:03,512: t15.2023.11.03 val PER: 0.1764
2026-01-08 17:44:03,514: t15.2023.11.04 val PER: 0.0444
2026-01-08 17:44:03,515: t15.2023.11.17 val PER: 0.0389
2026-01-08 17:44:03,517: t15.2023.11.19 val PER: 0.0479
2026-01-08 17:44:03,519: t15.2023.11.26 val PER: 0.1399
2026-01-08 17:44:03,521: t15.2023.12.03 val PER: 0.1197
2026-01-08 17:44:03,523: t15.2023.12.08 val PER: 0.1318
2026-01-08 17:44:03,524: t15.2023.12.10 val PER: 0.1091
2026-01-08 17:44:03,526: t15.2023.12.17 val PER: 0.1424
2026-01-08 17:44:03,528: t15.2023.12.29 val PER: 0.1524
2026-01-08 17:44:03,529: t15.2024.02.25 val PER: 0.1306
2026-01-08 17:44:03,531: t15.2024.03.08 val PER: 0.2248
2026-01-08 17:44:03,532: t15.2024.03.15 val PER: 0.2095
2026-01-08 17:44:03,534: t15.2024.03.17 val PER: 0.1457
2026-01-08 17:44:03,536: t15.2024.05.10 val PER: 0.1724
2026-01-08 17:44:03,537: t15.2024.06.14 val PER: 0.1672
2026-01-08 17:44:03,539: t15.2024.07.19 val PER: 0.2531
2026-01-08 17:44:03,540: t15.2024.07.21 val PER: 0.1028
2026-01-08 17:44:03,542: t15.2024.07.28 val PER: 0.1404
2026-01-08 17:44:03,543: t15.2025.01.10 val PER: 0.3168
2026-01-08 17:44:03,545: t15.2025.01.12 val PER: 0.1663
2026-01-08 17:44:03,547: t15.2025.03.14 val PER: 0.3491
2026-01-08 17:44:03,548: t15.2025.03.16 val PER: 0.1911
2026-01-08 17:44:03,550: t15.2025.03.30 val PER: 0.2931
2026-01-08 17:44:03,551: t15.2025.04.13 val PER: 0.2297
2026-01-08 17:44:03,553: New best val WER(5gram) 37.42% --> 35.53%
2026-01-08 17:44:03,764: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_12000
2026-01-08 17:44:20,926: Train batch 12200: loss: 5.92 grad norm: 42.92 time: 0.074
2026-01-08 17:44:39,156: Train batch 12400: loss: 5.27 grad norm: 37.34 time: 0.046
2026-01-08 17:44:48,650: Running test after training batch: 12500
2026-01-08 17:44:48,782: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:44:54,181: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:44:54,219: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the cost
2026-01-08 17:45:03,160: Val batch 12500: PER (avg): 0.1573 CTC Loss (avg): 15.8318 WER(5gram): 34.29% (n=256) time: 14.508
2026-01-08 17:45:03,163: WER lens: avg_true_words=5.99 avg_pred_words=6.39 max_pred_words=14
2026-01-08 17:45:03,166: t15.2023.08.13 val PER: 0.1206
2026-01-08 17:45:03,168: t15.2023.08.18 val PER: 0.1140
2026-01-08 17:45:03,170: t15.2023.08.20 val PER: 0.1152
2026-01-08 17:45:03,171: t15.2023.08.25 val PER: 0.1175
2026-01-08 17:45:03,173: t15.2023.08.27 val PER: 0.1865
2026-01-08 17:45:03,175: t15.2023.09.01 val PER: 0.0812
2026-01-08 17:45:03,177: t15.2023.09.03 val PER: 0.1591
2026-01-08 17:45:03,179: t15.2023.09.24 val PER: 0.1250
2026-01-08 17:45:03,181: t15.2023.09.29 val PER: 0.1372
2026-01-08 17:45:03,182: t15.2023.10.01 val PER: 0.1764
2026-01-08 17:45:03,184: t15.2023.10.06 val PER: 0.0990
2026-01-08 17:45:03,186: t15.2023.10.08 val PER: 0.2476
2026-01-08 17:45:03,188: t15.2023.10.13 val PER: 0.2087
2026-01-08 17:45:03,189: t15.2023.10.15 val PER: 0.1628
2026-01-08 17:45:03,191: t15.2023.10.20 val PER: 0.1913
2026-01-08 17:45:03,194: t15.2023.10.22 val PER: 0.1125
2026-01-08 17:45:03,196: t15.2023.11.03 val PER: 0.1723
2026-01-08 17:45:03,198: t15.2023.11.04 val PER: 0.0375
2026-01-08 17:45:03,200: t15.2023.11.17 val PER: 0.0467
2026-01-08 17:45:03,201: t15.2023.11.19 val PER: 0.0439
2026-01-08 17:45:03,203: t15.2023.11.26 val PER: 0.1326
2026-01-08 17:45:03,205: t15.2023.12.03 val PER: 0.1155
2026-01-08 17:45:03,207: t15.2023.12.08 val PER: 0.1192
2026-01-08 17:45:03,208: t15.2023.12.10 val PER: 0.1064
2026-01-08 17:45:03,210: t15.2023.12.17 val PER: 0.1279
2026-01-08 17:45:03,212: t15.2023.12.29 val PER: 0.1469
2026-01-08 17:45:03,214: t15.2024.02.25 val PER: 0.1278
2026-01-08 17:45:03,216: t15.2024.03.08 val PER: 0.2276
2026-01-08 17:45:03,217: t15.2024.03.15 val PER: 0.2114
2026-01-08 17:45:03,219: t15.2024.03.17 val PER: 0.1374
2026-01-08 17:45:03,221: t15.2024.05.10 val PER: 0.1887
2026-01-08 17:45:03,222: t15.2024.06.14 val PER: 0.1719
2026-01-08 17:45:03,224: t15.2024.07.19 val PER: 0.2334
2026-01-08 17:45:03,226: t15.2024.07.21 val PER: 0.1034
2026-01-08 17:45:03,227: t15.2024.07.28 val PER: 0.1441
2026-01-08 17:45:03,229: t15.2025.01.10 val PER: 0.2989
2026-01-08 17:45:03,231: t15.2025.01.12 val PER: 0.1632
2026-01-08 17:45:03,232: t15.2025.03.14 val PER: 0.3447
2026-01-08 17:45:03,234: t15.2025.03.16 val PER: 0.1872
2026-01-08 17:45:03,236: t15.2025.03.30 val PER: 0.2897
2026-01-08 17:45:03,237: t15.2025.04.13 val PER: 0.2397
2026-01-08 17:45:03,239: New best val WER(5gram) 35.53% --> 34.29%
2026-01-08 17:45:03,455: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_12500
2026-01-08 17:45:12,040: Train batch 12600: loss: 8.78 grad norm: 56.30 time: 0.064
2026-01-08 17:45:29,450: Train batch 12800: loss: 6.33 grad norm: 42.69 time: 0.059
2026-01-08 17:45:47,012: Train batch 13000: loss: 6.84 grad norm: 55.38 time: 0.074
2026-01-08 17:45:47,015: Running test after training batch: 13000
2026-01-08 17:45:47,126: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:45:52,294: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:45:52,335: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 17:46:01,351: Val batch 13000: PER (avg): 0.1560 CTC Loss (avg): 15.6645 WER(5gram): 36.70% (n=256) time: 14.334
2026-01-08 17:46:01,353: WER lens: avg_true_words=5.99 avg_pred_words=6.39 max_pred_words=12
2026-01-08 17:46:01,357: t15.2023.08.13 val PER: 0.1206
2026-01-08 17:46:01,359: t15.2023.08.18 val PER: 0.1148
2026-01-08 17:46:01,360: t15.2023.08.20 val PER: 0.1112
2026-01-08 17:46:01,363: t15.2023.08.25 val PER: 0.1099
2026-01-08 17:46:01,366: t15.2023.08.27 val PER: 0.1929
2026-01-08 17:46:01,367: t15.2023.09.01 val PER: 0.0787
2026-01-08 17:46:01,369: t15.2023.09.03 val PER: 0.1556
2026-01-08 17:46:01,371: t15.2023.09.24 val PER: 0.1274
2026-01-08 17:46:01,372: t15.2023.09.29 val PER: 0.1289
2026-01-08 17:46:01,374: t15.2023.10.01 val PER: 0.1737
2026-01-08 17:46:01,376: t15.2023.10.06 val PER: 0.1012
2026-01-08 17:46:01,377: t15.2023.10.08 val PER: 0.2382
2026-01-08 17:46:01,379: t15.2023.10.13 val PER: 0.2056
2026-01-08 17:46:01,381: t15.2023.10.15 val PER: 0.1602
2026-01-08 17:46:01,383: t15.2023.10.20 val PER: 0.1812
2026-01-08 17:46:01,385: t15.2023.10.22 val PER: 0.1180
2026-01-08 17:46:01,386: t15.2023.11.03 val PER: 0.1716
2026-01-08 17:46:01,388: t15.2023.11.04 val PER: 0.0410
2026-01-08 17:46:01,390: t15.2023.11.17 val PER: 0.0420
2026-01-08 17:46:01,392: t15.2023.11.19 val PER: 0.0439
2026-01-08 17:46:01,393: t15.2023.11.26 val PER: 0.1268
2026-01-08 17:46:01,395: t15.2023.12.03 val PER: 0.1197
2026-01-08 17:46:01,397: t15.2023.12.08 val PER: 0.1125
2026-01-08 17:46:01,399: t15.2023.12.10 val PER: 0.1078
2026-01-08 17:46:01,401: t15.2023.12.17 val PER: 0.1507
2026-01-08 17:46:01,402: t15.2023.12.29 val PER: 0.1448
2026-01-08 17:46:01,404: t15.2024.02.25 val PER: 0.1250
2026-01-08 17:46:01,406: t15.2024.03.08 val PER: 0.2319
2026-01-08 17:46:01,407: t15.2024.03.15 val PER: 0.2076
2026-01-08 17:46:01,409: t15.2024.03.17 val PER: 0.1346
2026-01-08 17:46:01,410: t15.2024.05.10 val PER: 0.1842
2026-01-08 17:46:01,412: t15.2024.06.14 val PER: 0.1609
2026-01-08 17:46:01,413: t15.2024.07.19 val PER: 0.2380
2026-01-08 17:46:01,415: t15.2024.07.21 val PER: 0.1048
2026-01-08 17:46:01,416: t15.2024.07.28 val PER: 0.1390
2026-01-08 17:46:01,419: t15.2025.01.10 val PER: 0.2989
2026-01-08 17:46:01,421: t15.2025.01.12 val PER: 0.1609
2026-01-08 17:46:01,422: t15.2025.03.14 val PER: 0.3432
2026-01-08 17:46:01,424: t15.2025.03.16 val PER: 0.1819
2026-01-08 17:46:01,425: t15.2025.03.30 val PER: 0.2989
2026-01-08 17:46:01,427: t15.2025.04.13 val PER: 0.2311
2026-01-08 17:46:01,583: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_13000
2026-01-08 17:46:18,723: Train batch 13200: loss: 14.16 grad norm: 71.76 time: 0.060
2026-01-08 17:46:35,716: Train batch 13400: loss: 10.26 grad norm: 58.53 time: 0.070
2026-01-08 17:46:44,932: Running test after training batch: 13500
2026-01-08 17:46:45,051: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:46:49,998: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:46:50,039: WER debug example
  GT : how does it keep the cost down
  PR : how i see it keep the cost
2026-01-08 17:46:59,118: Val batch 13500: PER (avg): 0.1541 CTC Loss (avg): 15.4474 WER(5gram): 37.42% (n=256) time: 14.184
2026-01-08 17:46:59,121: WER lens: avg_true_words=5.99 avg_pred_words=6.47 max_pred_words=13
2026-01-08 17:46:59,123: t15.2023.08.13 val PER: 0.1206
2026-01-08 17:46:59,125: t15.2023.08.18 val PER: 0.1073
2026-01-08 17:46:59,127: t15.2023.08.20 val PER: 0.1080
2026-01-08 17:46:59,129: t15.2023.08.25 val PER: 0.1084
2026-01-08 17:46:59,131: t15.2023.08.27 val PER: 0.1913
2026-01-08 17:46:59,132: t15.2023.09.01 val PER: 0.0779
2026-01-08 17:46:59,134: t15.2023.09.03 val PER: 0.1603
2026-01-08 17:46:59,136: t15.2023.09.24 val PER: 0.1226
2026-01-08 17:46:59,138: t15.2023.09.29 val PER: 0.1347
2026-01-08 17:46:59,140: t15.2023.10.01 val PER: 0.1737
2026-01-08 17:46:59,142: t15.2023.10.06 val PER: 0.1012
2026-01-08 17:46:59,144: t15.2023.10.08 val PER: 0.2395
2026-01-08 17:46:59,145: t15.2023.10.13 val PER: 0.2017
2026-01-08 17:46:59,147: t15.2023.10.15 val PER: 0.1536
2026-01-08 17:46:59,149: t15.2023.10.20 val PER: 0.1812
2026-01-08 17:46:59,151: t15.2023.10.22 val PER: 0.1169
2026-01-08 17:46:59,153: t15.2023.11.03 val PER: 0.1771
2026-01-08 17:46:59,155: t15.2023.11.04 val PER: 0.0410
2026-01-08 17:46:59,157: t15.2023.11.17 val PER: 0.0358
2026-01-08 17:46:59,159: t15.2023.11.19 val PER: 0.0459
2026-01-08 17:46:59,161: t15.2023.11.26 val PER: 0.1348
2026-01-08 17:46:59,163: t15.2023.12.03 val PER: 0.1124
2026-01-08 17:46:59,165: t15.2023.12.08 val PER: 0.1045
2026-01-08 17:46:59,167: t15.2023.12.10 val PER: 0.1012
2026-01-08 17:46:59,169: t15.2023.12.17 val PER: 0.1372
2026-01-08 17:46:59,171: t15.2023.12.29 val PER: 0.1366
2026-01-08 17:46:59,173: t15.2024.02.25 val PER: 0.1278
2026-01-08 17:46:59,175: t15.2024.03.08 val PER: 0.2162
2026-01-08 17:46:59,177: t15.2024.03.15 val PER: 0.2101
2026-01-08 17:46:59,179: t15.2024.03.17 val PER: 0.1388
2026-01-08 17:46:59,180: t15.2024.05.10 val PER: 0.1917
2026-01-08 17:46:59,182: t15.2024.06.14 val PER: 0.1498
2026-01-08 17:46:59,184: t15.2024.07.19 val PER: 0.2459
2026-01-08 17:46:59,186: t15.2024.07.21 val PER: 0.1014
2026-01-08 17:46:59,188: t15.2024.07.28 val PER: 0.1301
2026-01-08 17:46:59,189: t15.2025.01.10 val PER: 0.3168
2026-01-08 17:46:59,191: t15.2025.01.12 val PER: 0.1532
2026-01-08 17:46:59,193: t15.2025.03.14 val PER: 0.3388
2026-01-08 17:46:59,195: t15.2025.03.16 val PER: 0.1728
2026-01-08 17:46:59,197: t15.2025.03.30 val PER: 0.2874
2026-01-08 17:46:59,198: t15.2025.04.13 val PER: 0.2297
2026-01-08 17:46:59,356: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_13500
2026-01-08 17:47:08,700: Train batch 13600: loss: 10.79 grad norm: 57.58 time: 0.072
2026-01-08 17:47:27,283: Train batch 13800: loss: 8.27 grad norm: 50.77 time: 0.064
2026-01-08 17:47:45,695: Train batch 14000: loss: 12.52 grad norm: 66.59 time: 0.058
2026-01-08 17:47:45,698: Running test after training batch: 14000
2026-01-08 17:47:45,800: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:47:50,820: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:47:50,860: WER debug example
  GT : how does it keep the cost down
  PR : how i see it keep the cost
2026-01-08 17:47:59,931: Val batch 14000: PER (avg): 0.1537 CTC Loss (avg): 15.5467 WER(5gram): 34.88% (n=256) time: 14.231
2026-01-08 17:47:59,933: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=13
2026-01-08 17:47:59,936: t15.2023.08.13 val PER: 0.1175
2026-01-08 17:47:59,938: t15.2023.08.18 val PER: 0.1123
2026-01-08 17:47:59,940: t15.2023.08.20 val PER: 0.1088
2026-01-08 17:47:59,942: t15.2023.08.25 val PER: 0.1009
2026-01-08 17:47:59,944: t15.2023.08.27 val PER: 0.1881
2026-01-08 17:47:59,946: t15.2023.09.01 val PER: 0.0747
2026-01-08 17:47:59,948: t15.2023.09.03 val PER: 0.1663
2026-01-08 17:47:59,950: t15.2023.09.24 val PER: 0.1201
2026-01-08 17:47:59,952: t15.2023.09.29 val PER: 0.1340
2026-01-08 17:47:59,953: t15.2023.10.01 val PER: 0.1678
2026-01-08 17:47:59,955: t15.2023.10.06 val PER: 0.0980
2026-01-08 17:47:59,957: t15.2023.10.08 val PER: 0.2355
2026-01-08 17:47:59,959: t15.2023.10.13 val PER: 0.2048
2026-01-08 17:47:59,961: t15.2023.10.15 val PER: 0.1615
2026-01-08 17:47:59,963: t15.2023.10.20 val PER: 0.1711
2026-01-08 17:47:59,964: t15.2023.10.22 val PER: 0.1225
2026-01-08 17:47:59,966: t15.2023.11.03 val PER: 0.1750
2026-01-08 17:47:59,968: t15.2023.11.04 val PER: 0.0444
2026-01-08 17:47:59,970: t15.2023.11.17 val PER: 0.0373
2026-01-08 17:47:59,971: t15.2023.11.19 val PER: 0.0519
2026-01-08 17:47:59,973: t15.2023.11.26 val PER: 0.1370
2026-01-08 17:47:59,975: t15.2023.12.03 val PER: 0.1019
2026-01-08 17:47:59,977: t15.2023.12.08 val PER: 0.1165
2026-01-08 17:47:59,978: t15.2023.12.10 val PER: 0.1064
2026-01-08 17:47:59,980: t15.2023.12.17 val PER: 0.1237
2026-01-08 17:47:59,982: t15.2023.12.29 val PER: 0.1345
2026-01-08 17:47:59,984: t15.2024.02.25 val PER: 0.1180
2026-01-08 17:47:59,985: t15.2024.03.08 val PER: 0.2262
2026-01-08 17:47:59,987: t15.2024.03.15 val PER: 0.2095
2026-01-08 17:47:59,989: t15.2024.03.17 val PER: 0.1395
2026-01-08 17:47:59,990: t15.2024.05.10 val PER: 0.1872
2026-01-08 17:47:59,992: t15.2024.06.14 val PER: 0.1498
2026-01-08 17:47:59,994: t15.2024.07.19 val PER: 0.2386
2026-01-08 17:47:59,996: t15.2024.07.21 val PER: 0.0931
2026-01-08 17:47:59,997: t15.2024.07.28 val PER: 0.1368
2026-01-08 17:48:00,000: t15.2025.01.10 val PER: 0.3017
2026-01-08 17:48:00,002: t15.2025.01.12 val PER: 0.1540
2026-01-08 17:48:00,004: t15.2025.03.14 val PER: 0.3373
2026-01-08 17:48:00,006: t15.2025.03.16 val PER: 0.1806
2026-01-08 17:48:00,007: t15.2025.03.30 val PER: 0.2931
2026-01-08 17:48:00,009: t15.2025.04.13 val PER: 0.2354
2026-01-08 17:48:00,171: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_14000
2026-01-08 17:48:18,416: Train batch 14200: loss: 8.32 grad norm: 50.83 time: 0.063
2026-01-08 17:48:36,927: Train batch 14400: loss: 7.25 grad norm: 47.54 time: 0.073
2026-01-08 17:48:46,265: Running test after training batch: 14500
2026-01-08 17:48:46,365: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:48:51,349: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:48:51,392: WER debug example
  GT : how does it keep the cost down
  PR : how i see it keep the cost
2026-01-08 17:49:00,445: Val batch 14500: PER (avg): 0.1528 CTC Loss (avg): 15.3257 WER(5gram): 40.03% (n=256) time: 14.177
2026-01-08 17:49:00,448: WER lens: avg_true_words=5.99 avg_pred_words=6.51 max_pred_words=12
2026-01-08 17:49:00,451: t15.2023.08.13 val PER: 0.1143
2026-01-08 17:49:00,453: t15.2023.08.18 val PER: 0.1065
2026-01-08 17:49:00,454: t15.2023.08.20 val PER: 0.1136
2026-01-08 17:49:00,456: t15.2023.08.25 val PER: 0.1054
2026-01-08 17:49:00,458: t15.2023.08.27 val PER: 0.1897
2026-01-08 17:49:00,459: t15.2023.09.01 val PER: 0.0771
2026-01-08 17:49:00,463: t15.2023.09.03 val PER: 0.1520
2026-01-08 17:49:00,464: t15.2023.09.24 val PER: 0.1226
2026-01-08 17:49:00,466: t15.2023.09.29 val PER: 0.1327
2026-01-08 17:49:00,468: t15.2023.10.01 val PER: 0.1684
2026-01-08 17:49:00,470: t15.2023.10.06 val PER: 0.0883
2026-01-08 17:49:00,471: t15.2023.10.08 val PER: 0.2273
2026-01-08 17:49:00,473: t15.2023.10.13 val PER: 0.2095
2026-01-08 17:49:00,475: t15.2023.10.15 val PER: 0.1608
2026-01-08 17:49:00,477: t15.2023.10.20 val PER: 0.1678
2026-01-08 17:49:00,479: t15.2023.10.22 val PER: 0.1192
2026-01-08 17:49:00,480: t15.2023.11.03 val PER: 0.1757
2026-01-08 17:49:00,482: t15.2023.11.04 val PER: 0.0341
2026-01-08 17:49:00,484: t15.2023.11.17 val PER: 0.0373
2026-01-08 17:49:00,485: t15.2023.11.19 val PER: 0.0499
2026-01-08 17:49:00,487: t15.2023.11.26 val PER: 0.1261
2026-01-08 17:49:00,489: t15.2023.12.03 val PER: 0.1008
2026-01-08 17:49:00,490: t15.2023.12.08 val PER: 0.1138
2026-01-08 17:49:00,492: t15.2023.12.10 val PER: 0.0933
2026-01-08 17:49:00,493: t15.2023.12.17 val PER: 0.1424
2026-01-08 17:49:00,495: t15.2023.12.29 val PER: 0.1373
2026-01-08 17:49:00,497: t15.2024.02.25 val PER: 0.1208
2026-01-08 17:49:00,499: t15.2024.03.08 val PER: 0.2290
2026-01-08 17:49:00,500: t15.2024.03.15 val PER: 0.2058
2026-01-08 17:49:00,502: t15.2024.03.17 val PER: 0.1318
2026-01-08 17:49:00,504: t15.2024.05.10 val PER: 0.1842
2026-01-08 17:49:00,505: t15.2024.06.14 val PER: 0.1593
2026-01-08 17:49:00,507: t15.2024.07.19 val PER: 0.2399
2026-01-08 17:49:00,508: t15.2024.07.21 val PER: 0.0986
2026-01-08 17:49:00,510: t15.2024.07.28 val PER: 0.1331
2026-01-08 17:49:00,511: t15.2025.01.10 val PER: 0.3072
2026-01-08 17:49:00,513: t15.2025.01.12 val PER: 0.1486
2026-01-08 17:49:00,515: t15.2025.03.14 val PER: 0.3402
2026-01-08 17:49:00,516: t15.2025.03.16 val PER: 0.1832
2026-01-08 17:49:00,518: t15.2025.03.30 val PER: 0.2828
2026-01-08 17:49:00,519: t15.2025.04.13 val PER: 0.2439
2026-01-08 17:49:00,679: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_14500
2026-01-08 17:49:09,850: Train batch 14600: loss: 12.93 grad norm: 65.28 time: 0.066
2026-01-08 17:49:28,374: Train batch 14800: loss: 6.31 grad norm: 47.71 time: 0.058
2026-01-08 17:49:46,763: Train batch 15000: loss: 9.30 grad norm: 51.58 time: 0.060
2026-01-08 17:49:46,765: Running test after training batch: 15000
2026-01-08 17:49:46,890: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:49:51,867: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:49:51,911: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 17:50:01,200: Val batch 15000: PER (avg): 0.1518 CTC Loss (avg): 15.1940 WER(5gram): 35.07% (n=256) time: 14.432
2026-01-08 17:50:01,202: WER lens: avg_true_words=5.99 avg_pred_words=6.42 max_pred_words=14
2026-01-08 17:50:01,205: t15.2023.08.13 val PER: 0.1206
2026-01-08 17:50:01,207: t15.2023.08.18 val PER: 0.1123
2026-01-08 17:50:01,210: t15.2023.08.20 val PER: 0.1104
2026-01-08 17:50:01,212: t15.2023.08.25 val PER: 0.1024
2026-01-08 17:50:01,214: t15.2023.08.27 val PER: 0.1961
2026-01-08 17:50:01,216: t15.2023.09.01 val PER: 0.0795
2026-01-08 17:50:01,217: t15.2023.09.03 val PER: 0.1473
2026-01-08 17:50:01,219: t15.2023.09.24 val PER: 0.1177
2026-01-08 17:50:01,221: t15.2023.09.29 val PER: 0.1283
2026-01-08 17:50:01,223: t15.2023.10.01 val PER: 0.1750
2026-01-08 17:50:01,225: t15.2023.10.06 val PER: 0.0947
2026-01-08 17:50:01,226: t15.2023.10.08 val PER: 0.2287
2026-01-08 17:50:01,228: t15.2023.10.13 val PER: 0.1994
2026-01-08 17:50:01,230: t15.2023.10.15 val PER: 0.1549
2026-01-08 17:50:01,232: t15.2023.10.20 val PER: 0.1644
2026-01-08 17:50:01,233: t15.2023.10.22 val PER: 0.1192
2026-01-08 17:50:01,235: t15.2023.11.03 val PER: 0.1750
2026-01-08 17:50:01,237: t15.2023.11.04 val PER: 0.0375
2026-01-08 17:50:01,239: t15.2023.11.17 val PER: 0.0358
2026-01-08 17:50:01,240: t15.2023.11.19 val PER: 0.0419
2026-01-08 17:50:01,242: t15.2023.11.26 val PER: 0.1239
2026-01-08 17:50:01,244: t15.2023.12.03 val PER: 0.0987
2026-01-08 17:50:01,245: t15.2023.12.08 val PER: 0.1085
2026-01-08 17:50:01,247: t15.2023.12.10 val PER: 0.1038
2026-01-08 17:50:01,249: t15.2023.12.17 val PER: 0.1372
2026-01-08 17:50:01,251: t15.2023.12.29 val PER: 0.1270
2026-01-08 17:50:01,252: t15.2024.02.25 val PER: 0.1222
2026-01-08 17:50:01,254: t15.2024.03.08 val PER: 0.2248
2026-01-08 17:50:01,256: t15.2024.03.15 val PER: 0.2083
2026-01-08 17:50:01,257: t15.2024.03.17 val PER: 0.1248
2026-01-08 17:50:01,259: t15.2024.05.10 val PER: 0.1887
2026-01-08 17:50:01,261: t15.2024.06.14 val PER: 0.1514
2026-01-08 17:50:01,262: t15.2024.07.19 val PER: 0.2380
2026-01-08 17:50:01,264: t15.2024.07.21 val PER: 0.1007
2026-01-08 17:50:01,266: t15.2024.07.28 val PER: 0.1353
2026-01-08 17:50:01,267: t15.2025.01.10 val PER: 0.3017
2026-01-08 17:50:01,269: t15.2025.01.12 val PER: 0.1563
2026-01-08 17:50:01,270: t15.2025.03.14 val PER: 0.3521
2026-01-08 17:50:01,272: t15.2025.03.16 val PER: 0.1898
2026-01-08 17:50:01,274: t15.2025.03.30 val PER: 0.2747
2026-01-08 17:50:01,276: t15.2025.04.13 val PER: 0.2340
2026-01-08 17:50:01,430: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_15000
2026-01-08 17:50:19,910: Train batch 15200: loss: 5.36 grad norm: 40.33 time: 0.065
2026-01-08 17:50:37,934: Train batch 15400: loss: 11.33 grad norm: 59.28 time: 0.056
2026-01-08 17:50:46,704: Running test after training batch: 15500
2026-01-08 17:50:46,864: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:50:52,194: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:50:52,236: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 17:51:01,647: Val batch 15500: PER (avg): 0.1490 CTC Loss (avg): 15.1175 WER(5gram): 33.70% (n=256) time: 14.941
2026-01-08 17:51:01,650: WER lens: avg_true_words=5.99 avg_pred_words=6.43 max_pred_words=13
2026-01-08 17:51:01,654: t15.2023.08.13 val PER: 0.1185
2026-01-08 17:51:01,656: t15.2023.08.18 val PER: 0.1065
2026-01-08 17:51:01,658: t15.2023.08.20 val PER: 0.1056
2026-01-08 17:51:01,660: t15.2023.08.25 val PER: 0.0994
2026-01-08 17:51:01,661: t15.2023.08.27 val PER: 0.1881
2026-01-08 17:51:01,663: t15.2023.09.01 val PER: 0.0739
2026-01-08 17:51:01,665: t15.2023.09.03 val PER: 0.1544
2026-01-08 17:51:01,667: t15.2023.09.24 val PER: 0.1189
2026-01-08 17:51:01,669: t15.2023.09.29 val PER: 0.1315
2026-01-08 17:51:01,671: t15.2023.10.01 val PER: 0.1678
2026-01-08 17:51:01,672: t15.2023.10.06 val PER: 0.0915
2026-01-08 17:51:01,674: t15.2023.10.08 val PER: 0.2219
2026-01-08 17:51:01,676: t15.2023.10.13 val PER: 0.1994
2026-01-08 17:51:01,678: t15.2023.10.15 val PER: 0.1529
2026-01-08 17:51:01,679: t15.2023.10.20 val PER: 0.1812
2026-01-08 17:51:01,681: t15.2023.10.22 val PER: 0.1214
2026-01-08 17:51:01,683: t15.2023.11.03 val PER: 0.1710
2026-01-08 17:51:01,684: t15.2023.11.04 val PER: 0.0341
2026-01-08 17:51:01,686: t15.2023.11.17 val PER: 0.0404
2026-01-08 17:51:01,688: t15.2023.11.19 val PER: 0.0459
2026-01-08 17:51:01,689: t15.2023.11.26 val PER: 0.1239
2026-01-08 17:51:01,691: t15.2023.12.03 val PER: 0.1008
2026-01-08 17:51:01,692: t15.2023.12.08 val PER: 0.1065
2026-01-08 17:51:01,694: t15.2023.12.10 val PER: 0.0933
2026-01-08 17:51:01,696: t15.2023.12.17 val PER: 0.1227
2026-01-08 17:51:01,697: t15.2023.12.29 val PER: 0.1311
2026-01-08 17:51:01,699: t15.2024.02.25 val PER: 0.1194
2026-01-08 17:51:01,700: t15.2024.03.08 val PER: 0.2119
2026-01-08 17:51:01,702: t15.2024.03.15 val PER: 0.2045
2026-01-08 17:51:01,704: t15.2024.03.17 val PER: 0.1255
2026-01-08 17:51:01,705: t15.2024.05.10 val PER: 0.1798
2026-01-08 17:51:01,707: t15.2024.06.14 val PER: 0.1530
2026-01-08 17:51:01,709: t15.2024.07.19 val PER: 0.2334
2026-01-08 17:51:01,710: t15.2024.07.21 val PER: 0.0959
2026-01-08 17:51:01,713: t15.2024.07.28 val PER: 0.1221
2026-01-08 17:51:01,715: t15.2025.01.10 val PER: 0.2975
2026-01-08 17:51:01,716: t15.2025.01.12 val PER: 0.1555
2026-01-08 17:51:01,718: t15.2025.03.14 val PER: 0.3417
2026-01-08 17:51:01,720: t15.2025.03.16 val PER: 0.1780
2026-01-08 17:51:01,721: t15.2025.03.30 val PER: 0.2885
2026-01-08 17:51:01,723: t15.2025.04.13 val PER: 0.2254
2026-01-08 17:51:01,726: New best val WER(5gram) 34.29% --> 33.70%
2026-01-08 17:51:01,938: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_15500
2026-01-08 17:51:11,216: Train batch 15600: loss: 11.05 grad norm: 61.34 time: 0.071
2026-01-08 17:51:29,639: Train batch 15800: loss: 13.47 grad norm: 65.27 time: 0.076
2026-01-08 17:51:48,483: Train batch 16000: loss: 7.08 grad norm: 42.74 time: 0.062
2026-01-08 17:51:48,485: Running test after training batch: 16000
2026-01-08 17:51:48,621: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:51:53,840: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:51:53,886: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 17:52:03,354: Val batch 16000: PER (avg): 0.1494 CTC Loss (avg): 15.0224 WER(5gram): 37.22% (n=256) time: 14.865
2026-01-08 17:52:03,357: WER lens: avg_true_words=5.99 avg_pred_words=6.52 max_pred_words=13
2026-01-08 17:52:03,359: t15.2023.08.13 val PER: 0.1154
2026-01-08 17:52:03,361: t15.2023.08.18 val PER: 0.1123
2026-01-08 17:52:03,363: t15.2023.08.20 val PER: 0.1088
2026-01-08 17:52:03,365: t15.2023.08.25 val PER: 0.0964
2026-01-08 17:52:03,367: t15.2023.08.27 val PER: 0.1849
2026-01-08 17:52:03,369: t15.2023.09.01 val PER: 0.0731
2026-01-08 17:52:03,371: t15.2023.09.03 val PER: 0.1520
2026-01-08 17:52:03,373: t15.2023.09.24 val PER: 0.1153
2026-01-08 17:52:03,375: t15.2023.09.29 val PER: 0.1327
2026-01-08 17:52:03,377: t15.2023.10.01 val PER: 0.1684
2026-01-08 17:52:03,378: t15.2023.10.06 val PER: 0.0969
2026-01-08 17:52:03,380: t15.2023.10.08 val PER: 0.2260
2026-01-08 17:52:03,382: t15.2023.10.13 val PER: 0.1986
2026-01-08 17:52:03,384: t15.2023.10.15 val PER: 0.1523
2026-01-08 17:52:03,386: t15.2023.10.20 val PER: 0.1812
2026-01-08 17:52:03,388: t15.2023.10.22 val PER: 0.1247
2026-01-08 17:52:03,390: t15.2023.11.03 val PER: 0.1723
2026-01-08 17:52:03,393: t15.2023.11.04 val PER: 0.0341
2026-01-08 17:52:03,394: t15.2023.11.17 val PER: 0.0373
2026-01-08 17:52:03,396: t15.2023.11.19 val PER: 0.0479
2026-01-08 17:52:03,398: t15.2023.11.26 val PER: 0.1051
2026-01-08 17:52:03,400: t15.2023.12.03 val PER: 0.0987
2026-01-08 17:52:03,402: t15.2023.12.08 val PER: 0.1099
2026-01-08 17:52:03,404: t15.2023.12.10 val PER: 0.1025
2026-01-08 17:52:03,407: t15.2023.12.17 val PER: 0.1247
2026-01-08 17:52:03,409: t15.2023.12.29 val PER: 0.1352
2026-01-08 17:52:03,411: t15.2024.02.25 val PER: 0.1194
2026-01-08 17:52:03,413: t15.2024.03.08 val PER: 0.2219
2026-01-08 17:52:03,414: t15.2024.03.15 val PER: 0.2020
2026-01-08 17:52:03,416: t15.2024.03.17 val PER: 0.1325
2026-01-08 17:52:03,418: t15.2024.05.10 val PER: 0.1738
2026-01-08 17:52:03,420: t15.2024.06.14 val PER: 0.1514
2026-01-08 17:52:03,423: t15.2024.07.19 val PER: 0.2274
2026-01-08 17:52:03,425: t15.2024.07.21 val PER: 0.0959
2026-01-08 17:52:03,427: t15.2024.07.28 val PER: 0.1294
2026-01-08 17:52:03,428: t15.2025.01.10 val PER: 0.2934
2026-01-08 17:52:03,430: t15.2025.01.12 val PER: 0.1540
2026-01-08 17:52:03,432: t15.2025.03.14 val PER: 0.3432
2026-01-08 17:52:03,434: t15.2025.03.16 val PER: 0.1872
2026-01-08 17:52:03,436: t15.2025.03.30 val PER: 0.2793
2026-01-08 17:52:03,437: t15.2025.04.13 val PER: 0.2382
2026-01-08 17:52:03,602: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_16000
2026-01-08 17:52:22,103: Train batch 16200: loss: 7.06 grad norm: 46.13 time: 0.067
2026-01-08 17:52:40,514: Train batch 16400: loss: 9.35 grad norm: 52.77 time: 0.066
2026-01-08 17:52:49,774: Running test after training batch: 16500
2026-01-08 17:52:49,997: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:52:55,419: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 17:52:55,465: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 17:53:04,883: Val batch 16500: PER (avg): 0.1493 CTC Loss (avg): 14.9699 WER(5gram): 34.22% (n=256) time: 15.106
2026-01-08 17:53:04,885: WER lens: avg_true_words=5.99 avg_pred_words=6.45 max_pred_words=13
2026-01-08 17:53:04,888: t15.2023.08.13 val PER: 0.1164
2026-01-08 17:53:04,890: t15.2023.08.18 val PER: 0.1115
2026-01-08 17:53:04,891: t15.2023.08.20 val PER: 0.1104
2026-01-08 17:53:04,892: t15.2023.08.25 val PER: 0.0979
2026-01-08 17:53:04,894: t15.2023.08.27 val PER: 0.1817
2026-01-08 17:53:04,895: t15.2023.09.01 val PER: 0.0731
2026-01-08 17:53:04,897: t15.2023.09.03 val PER: 0.1544
2026-01-08 17:53:04,901: t15.2023.09.24 val PER: 0.1165
2026-01-08 17:53:04,902: t15.2023.09.29 val PER: 0.1283
2026-01-08 17:53:04,904: t15.2023.10.01 val PER: 0.1704
2026-01-08 17:53:04,905: t15.2023.10.06 val PER: 0.0883
2026-01-08 17:53:04,906: t15.2023.10.08 val PER: 0.2273
2026-01-08 17:53:04,908: t15.2023.10.13 val PER: 0.1971
2026-01-08 17:53:04,909: t15.2023.10.15 val PER: 0.1556
2026-01-08 17:53:04,911: t15.2023.10.20 val PER: 0.1745
2026-01-08 17:53:04,912: t15.2023.10.22 val PER: 0.1136
2026-01-08 17:53:04,913: t15.2023.11.03 val PER: 0.1723
2026-01-08 17:53:04,915: t15.2023.11.04 val PER: 0.0307
2026-01-08 17:53:04,916: t15.2023.11.17 val PER: 0.0389
2026-01-08 17:53:04,918: t15.2023.11.19 val PER: 0.0439
2026-01-08 17:53:04,919: t15.2023.11.26 val PER: 0.1145
2026-01-08 17:53:04,921: t15.2023.12.03 val PER: 0.1071
2026-01-08 17:53:04,922: t15.2023.12.08 val PER: 0.1072
2026-01-08 17:53:04,923: t15.2023.12.10 val PER: 0.0946
2026-01-08 17:53:04,925: t15.2023.12.17 val PER: 0.1310
2026-01-08 17:53:04,927: t15.2023.12.29 val PER: 0.1229
2026-01-08 17:53:04,928: t15.2024.02.25 val PER: 0.1194
2026-01-08 17:53:04,930: t15.2024.03.08 val PER: 0.2233
2026-01-08 17:53:04,932: t15.2024.03.15 val PER: 0.2089
2026-01-08 17:53:04,933: t15.2024.03.17 val PER: 0.1255
2026-01-08 17:53:04,935: t15.2024.05.10 val PER: 0.1753
2026-01-08 17:53:04,936: t15.2024.06.14 val PER: 0.1483
2026-01-08 17:53:04,938: t15.2024.07.19 val PER: 0.2380
2026-01-08 17:53:04,939: t15.2024.07.21 val PER: 0.0959
2026-01-08 17:53:04,941: t15.2024.07.28 val PER: 0.1287
2026-01-08 17:53:04,942: t15.2025.01.10 val PER: 0.2934
2026-01-08 17:53:04,944: t15.2025.01.12 val PER: 0.1478
2026-01-08 17:53:04,945: t15.2025.03.14 val PER: 0.3447
2026-01-08 17:53:04,947: t15.2025.03.16 val PER: 0.1911
2026-01-08 17:53:04,948: t15.2025.03.30 val PER: 0.2862
2026-01-08 17:53:04,950: t15.2025.04.13 val PER: 0.2325
2026-01-08 17:53:05,109: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_16500
2026-01-08 17:53:14,344: Train batch 16600: loss: 8.89 grad norm: 51.54 time: 0.062
2026-01-08 17:53:32,143: Train batch 16800: loss: 15.86 grad norm: 70.41 time: 0.069
2026-01-08 17:53:49,209: Train batch 17000: loss: 7.70 grad norm: 46.73 time: 0.092
2026-01-08 17:53:49,211: Running test after training batch: 17000
2026-01-08 17:53:49,312: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:53:54,434: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:53:54,479: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 17:54:03,970: Val batch 17000: PER (avg): 0.1476 CTC Loss (avg): 14.9663 WER(5gram): 31.36% (n=256) time: 14.756
2026-01-08 17:54:03,972: WER lens: avg_true_words=5.99 avg_pred_words=6.42 max_pred_words=13
2026-01-08 17:54:03,975: t15.2023.08.13 val PER: 0.1227
2026-01-08 17:54:03,979: t15.2023.08.18 val PER: 0.1073
2026-01-08 17:54:03,981: t15.2023.08.20 val PER: 0.1048
2026-01-08 17:54:03,983: t15.2023.08.25 val PER: 0.0994
2026-01-08 17:54:03,985: t15.2023.08.27 val PER: 0.1801
2026-01-08 17:54:03,987: t15.2023.09.01 val PER: 0.0731
2026-01-08 17:54:03,990: t15.2023.09.03 val PER: 0.1508
2026-01-08 17:54:03,992: t15.2023.09.24 val PER: 0.1141
2026-01-08 17:54:03,994: t15.2023.09.29 val PER: 0.1321
2026-01-08 17:54:03,996: t15.2023.10.01 val PER: 0.1638
2026-01-08 17:54:03,999: t15.2023.10.06 val PER: 0.0883
2026-01-08 17:54:04,001: t15.2023.10.08 val PER: 0.2314
2026-01-08 17:54:04,003: t15.2023.10.13 val PER: 0.1947
2026-01-08 17:54:04,005: t15.2023.10.15 val PER: 0.1529
2026-01-08 17:54:04,007: t15.2023.10.20 val PER: 0.1779
2026-01-08 17:54:04,009: t15.2023.10.22 val PER: 0.1136
2026-01-08 17:54:04,012: t15.2023.11.03 val PER: 0.1737
2026-01-08 17:54:04,014: t15.2023.11.04 val PER: 0.0341
2026-01-08 17:54:04,016: t15.2023.11.17 val PER: 0.0327
2026-01-08 17:54:04,018: t15.2023.11.19 val PER: 0.0399
2026-01-08 17:54:04,020: t15.2023.11.26 val PER: 0.1152
2026-01-08 17:54:04,021: t15.2023.12.03 val PER: 0.1040
2026-01-08 17:54:04,023: t15.2023.12.08 val PER: 0.1099
2026-01-08 17:54:04,025: t15.2023.12.10 val PER: 0.0946
2026-01-08 17:54:04,026: t15.2023.12.17 val PER: 0.1247
2026-01-08 17:54:04,028: t15.2023.12.29 val PER: 0.1290
2026-01-08 17:54:04,030: t15.2024.02.25 val PER: 0.1222
2026-01-08 17:54:04,032: t15.2024.03.08 val PER: 0.2162
2026-01-08 17:54:04,034: t15.2024.03.15 val PER: 0.2045
2026-01-08 17:54:04,036: t15.2024.03.17 val PER: 0.1269
2026-01-08 17:54:04,037: t15.2024.05.10 val PER: 0.1813
2026-01-08 17:54:04,039: t15.2024.06.14 val PER: 0.1435
2026-01-08 17:54:04,041: t15.2024.07.19 val PER: 0.2281
2026-01-08 17:54:04,042: t15.2024.07.21 val PER: 0.0910
2026-01-08 17:54:04,044: t15.2024.07.28 val PER: 0.1287
2026-01-08 17:54:04,046: t15.2025.01.10 val PER: 0.2879
2026-01-08 17:54:04,047: t15.2025.01.12 val PER: 0.1524
2026-01-08 17:54:04,049: t15.2025.03.14 val PER: 0.3373
2026-01-08 17:54:04,051: t15.2025.03.16 val PER: 0.1819
2026-01-08 17:54:04,052: t15.2025.03.30 val PER: 0.2713
2026-01-08 17:54:04,054: t15.2025.04.13 val PER: 0.2340
2026-01-08 17:54:04,057: New best val WER(5gram) 33.70% --> 31.36%
2026-01-08 17:54:04,258: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_17000
2026-01-08 17:54:21,795: Train batch 17200: loss: 10.09 grad norm: 56.09 time: 0.096
2026-01-08 17:54:40,459: Train batch 17400: loss: 10.70 grad norm: 59.12 time: 0.080
2026-01-08 17:54:49,552: Running test after training batch: 17500
2026-01-08 17:54:49,660: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:54:54,970: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 17:54:55,014: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 17:55:04,447: Val batch 17500: PER (avg): 0.1465 CTC Loss (avg): 14.9022 WER(5gram): 30.96% (n=256) time: 14.893
2026-01-08 17:55:04,449: WER lens: avg_true_words=5.99 avg_pred_words=6.40 max_pred_words=13
2026-01-08 17:55:04,453: t15.2023.08.13 val PER: 0.1164
2026-01-08 17:55:04,455: t15.2023.08.18 val PER: 0.1073
2026-01-08 17:55:04,456: t15.2023.08.20 val PER: 0.1009
2026-01-08 17:55:04,458: t15.2023.08.25 val PER: 0.1009
2026-01-08 17:55:04,460: t15.2023.08.27 val PER: 0.1736
2026-01-08 17:55:04,462: t15.2023.09.01 val PER: 0.0771
2026-01-08 17:55:04,463: t15.2023.09.03 val PER: 0.1520
2026-01-08 17:55:04,465: t15.2023.09.24 val PER: 0.1117
2026-01-08 17:55:04,467: t15.2023.09.29 val PER: 0.1264
2026-01-08 17:55:04,469: t15.2023.10.01 val PER: 0.1658
2026-01-08 17:55:04,470: t15.2023.10.06 val PER: 0.0797
2026-01-08 17:55:04,472: t15.2023.10.08 val PER: 0.2219
2026-01-08 17:55:04,474: t15.2023.10.13 val PER: 0.1924
2026-01-08 17:55:04,475: t15.2023.10.15 val PER: 0.1490
2026-01-08 17:55:04,477: t15.2023.10.20 val PER: 0.1812
2026-01-08 17:55:04,478: t15.2023.10.22 val PER: 0.1158
2026-01-08 17:55:04,480: t15.2023.11.03 val PER: 0.1689
2026-01-08 17:55:04,483: t15.2023.11.04 val PER: 0.0341
2026-01-08 17:55:04,485: t15.2023.11.17 val PER: 0.0327
2026-01-08 17:55:04,487: t15.2023.11.19 val PER: 0.0479
2026-01-08 17:55:04,488: t15.2023.11.26 val PER: 0.1145
2026-01-08 17:55:04,490: t15.2023.12.03 val PER: 0.1040
2026-01-08 17:55:04,492: t15.2023.12.08 val PER: 0.1059
2026-01-08 17:55:04,494: t15.2023.12.10 val PER: 0.0972
2026-01-08 17:55:04,495: t15.2023.12.17 val PER: 0.1320
2026-01-08 17:55:04,497: t15.2023.12.29 val PER: 0.1229
2026-01-08 17:55:04,499: t15.2024.02.25 val PER: 0.1053
2026-01-08 17:55:04,501: t15.2024.03.08 val PER: 0.2176
2026-01-08 17:55:04,502: t15.2024.03.15 val PER: 0.2001
2026-01-08 17:55:04,504: t15.2024.03.17 val PER: 0.1283
2026-01-08 17:55:04,506: t15.2024.05.10 val PER: 0.1902
2026-01-08 17:55:04,507: t15.2024.06.14 val PER: 0.1483
2026-01-08 17:55:04,509: t15.2024.07.19 val PER: 0.2340
2026-01-08 17:55:04,511: t15.2024.07.21 val PER: 0.0903
2026-01-08 17:55:04,512: t15.2024.07.28 val PER: 0.1324
2026-01-08 17:55:04,514: t15.2025.01.10 val PER: 0.2865
2026-01-08 17:55:04,516: t15.2025.01.12 val PER: 0.1509
2026-01-08 17:55:04,517: t15.2025.03.14 val PER: 0.3358
2026-01-08 17:55:04,519: t15.2025.03.16 val PER: 0.1819
2026-01-08 17:55:04,520: t15.2025.03.30 val PER: 0.2724
2026-01-08 17:55:04,522: t15.2025.04.13 val PER: 0.2268
2026-01-08 17:55:04,524: New best val WER(5gram) 31.36% --> 30.96%
2026-01-08 17:55:04,732: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_17500
2026-01-08 17:55:13,533: Train batch 17600: loss: 8.94 grad norm: 59.08 time: 0.057
2026-01-08 17:55:31,256: Train batch 17800: loss: 5.91 grad norm: 46.62 time: 0.047
2026-01-08 17:55:48,437: Train batch 18000: loss: 8.74 grad norm: 67.92 time: 0.069
2026-01-08 17:55:48,439: Running test after training batch: 18000
2026-01-08 17:55:48,565: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:55:53,808: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 17:55:53,852: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 17:56:03,343: Val batch 18000: PER (avg): 0.1480 CTC Loss (avg): 14.8907 WER(5gram): 32.59% (n=256) time: 14.901
2026-01-08 17:56:03,346: WER lens: avg_true_words=5.99 avg_pred_words=6.46 max_pred_words=13
2026-01-08 17:56:03,349: t15.2023.08.13 val PER: 0.1175
2026-01-08 17:56:03,351: t15.2023.08.18 val PER: 0.1031
2026-01-08 17:56:03,353: t15.2023.08.20 val PER: 0.1041
2026-01-08 17:56:03,355: t15.2023.08.25 val PER: 0.0949
2026-01-08 17:56:03,356: t15.2023.08.27 val PER: 0.1736
2026-01-08 17:56:03,358: t15.2023.09.01 val PER: 0.0763
2026-01-08 17:56:03,360: t15.2023.09.03 val PER: 0.1496
2026-01-08 17:56:03,363: t15.2023.09.24 val PER: 0.1153
2026-01-08 17:56:03,365: t15.2023.09.29 val PER: 0.1289
2026-01-08 17:56:03,367: t15.2023.10.01 val PER: 0.1638
2026-01-08 17:56:03,368: t15.2023.10.06 val PER: 0.0840
2026-01-08 17:56:03,370: t15.2023.10.08 val PER: 0.2273
2026-01-08 17:56:03,372: t15.2023.10.13 val PER: 0.1939
2026-01-08 17:56:03,374: t15.2023.10.15 val PER: 0.1496
2026-01-08 17:56:03,376: t15.2023.10.20 val PER: 0.1745
2026-01-08 17:56:03,378: t15.2023.10.22 val PER: 0.1158
2026-01-08 17:56:03,380: t15.2023.11.03 val PER: 0.1703
2026-01-08 17:56:03,381: t15.2023.11.04 val PER: 0.0341
2026-01-08 17:56:03,383: t15.2023.11.17 val PER: 0.0342
2026-01-08 17:56:03,386: t15.2023.11.19 val PER: 0.0439
2026-01-08 17:56:03,387: t15.2023.11.26 val PER: 0.1181
2026-01-08 17:56:03,389: t15.2023.12.03 val PER: 0.0966
2026-01-08 17:56:03,391: t15.2023.12.08 val PER: 0.1099
2026-01-08 17:56:03,393: t15.2023.12.10 val PER: 0.0972
2026-01-08 17:56:03,394: t15.2023.12.17 val PER: 0.1299
2026-01-08 17:56:03,396: t15.2023.12.29 val PER: 0.1297
2026-01-08 17:56:03,397: t15.2024.02.25 val PER: 0.1180
2026-01-08 17:56:03,399: t15.2024.03.08 val PER: 0.2148
2026-01-08 17:56:03,400: t15.2024.03.15 val PER: 0.2033
2026-01-08 17:56:03,402: t15.2024.03.17 val PER: 0.1311
2026-01-08 17:56:03,404: t15.2024.05.10 val PER: 0.1842
2026-01-08 17:56:03,405: t15.2024.06.14 val PER: 0.1498
2026-01-08 17:56:03,408: t15.2024.07.19 val PER: 0.2393
2026-01-08 17:56:03,410: t15.2024.07.21 val PER: 0.0903
2026-01-08 17:56:03,411: t15.2024.07.28 val PER: 0.1279
2026-01-08 17:56:03,413: t15.2025.01.10 val PER: 0.2948
2026-01-08 17:56:03,414: t15.2025.01.12 val PER: 0.1532
2026-01-08 17:56:03,422: t15.2025.03.14 val PER: 0.3417
2026-01-08 17:56:03,429: t15.2025.03.16 val PER: 0.1924
2026-01-08 17:56:03,432: t15.2025.03.30 val PER: 0.2747
2026-01-08 17:56:03,435: t15.2025.04.13 val PER: 0.2368
2026-01-08 17:56:03,599: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_18000
2026-01-08 17:56:21,368: Train batch 18200: loss: 7.67 grad norm: 46.43 time: 0.083
2026-01-08 17:56:39,020: Train batch 18400: loss: 5.45 grad norm: 42.25 time: 0.065
2026-01-08 17:56:47,987: Running test after training batch: 18500
2026-01-08 17:56:48,121: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:56:53,123: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:56:53,167: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 17:57:02,715: Val batch 18500: PER (avg): 0.1479 CTC Loss (avg): 14.8752 WER(5gram): 33.31% (n=256) time: 14.725
2026-01-08 17:57:02,717: WER lens: avg_true_words=5.99 avg_pred_words=6.48 max_pred_words=13
2026-01-08 17:57:02,720: t15.2023.08.13 val PER: 0.1143
2026-01-08 17:57:02,722: t15.2023.08.18 val PER: 0.1065
2026-01-08 17:57:02,724: t15.2023.08.20 val PER: 0.1025
2026-01-08 17:57:02,726: t15.2023.08.25 val PER: 0.0979
2026-01-08 17:57:02,728: t15.2023.08.27 val PER: 0.1865
2026-01-08 17:57:02,730: t15.2023.09.01 val PER: 0.0739
2026-01-08 17:57:02,732: t15.2023.09.03 val PER: 0.1508
2026-01-08 17:57:02,734: t15.2023.09.24 val PER: 0.1189
2026-01-08 17:57:02,736: t15.2023.09.29 val PER: 0.1327
2026-01-08 17:57:02,738: t15.2023.10.01 val PER: 0.1638
2026-01-08 17:57:02,739: t15.2023.10.06 val PER: 0.0850
2026-01-08 17:57:02,741: t15.2023.10.08 val PER: 0.2355
2026-01-08 17:57:02,743: t15.2023.10.13 val PER: 0.1908
2026-01-08 17:57:02,745: t15.2023.10.15 val PER: 0.1529
2026-01-08 17:57:02,747: t15.2023.10.20 val PER: 0.1779
2026-01-08 17:57:02,749: t15.2023.10.22 val PER: 0.1158
2026-01-08 17:57:02,751: t15.2023.11.03 val PER: 0.1710
2026-01-08 17:57:02,753: t15.2023.11.04 val PER: 0.0341
2026-01-08 17:57:02,754: t15.2023.11.17 val PER: 0.0373
2026-01-08 17:57:02,756: t15.2023.11.19 val PER: 0.0439
2026-01-08 17:57:02,758: t15.2023.11.26 val PER: 0.1159
2026-01-08 17:57:02,760: t15.2023.12.03 val PER: 0.0956
2026-01-08 17:57:02,762: t15.2023.12.08 val PER: 0.1105
2026-01-08 17:57:02,764: t15.2023.12.10 val PER: 0.0946
2026-01-08 17:57:02,765: t15.2023.12.17 val PER: 0.1258
2026-01-08 17:57:02,767: t15.2023.12.29 val PER: 0.1249
2026-01-08 17:57:02,769: t15.2024.02.25 val PER: 0.1124
2026-01-08 17:57:02,771: t15.2024.03.08 val PER: 0.2176
2026-01-08 17:57:02,773: t15.2024.03.15 val PER: 0.2008
2026-01-08 17:57:02,775: t15.2024.03.17 val PER: 0.1332
2026-01-08 17:57:02,777: t15.2024.05.10 val PER: 0.1842
2026-01-08 17:57:02,779: t15.2024.06.14 val PER: 0.1388
2026-01-08 17:57:02,781: t15.2024.07.19 val PER: 0.2334
2026-01-08 17:57:02,783: t15.2024.07.21 val PER: 0.0924
2026-01-08 17:57:02,785: t15.2024.07.28 val PER: 0.1331
2026-01-08 17:57:02,787: t15.2025.01.10 val PER: 0.2865
2026-01-08 17:57:02,789: t15.2025.01.12 val PER: 0.1517
2026-01-08 17:57:02,791: t15.2025.03.14 val PER: 0.3417
2026-01-08 17:57:02,793: t15.2025.03.16 val PER: 0.1819
2026-01-08 17:57:02,795: t15.2025.03.30 val PER: 0.2874
2026-01-08 17:57:02,797: t15.2025.04.13 val PER: 0.2354
2026-01-08 17:57:02,958: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_18500
2026-01-08 17:57:11,514: Train batch 18600: loss: 11.22 grad norm: 60.37 time: 0.076
2026-01-08 17:57:29,058: Train batch 18800: loss: 7.83 grad norm: 53.40 time: 0.073
2026-01-08 17:57:46,883: Train batch 19000: loss: 7.33 grad norm: 47.50 time: 0.071
2026-01-08 17:57:46,885: Running test after training batch: 19000
2026-01-08 17:57:47,002: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:57:52,232: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:57:52,276: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 17:58:01,951: Val batch 19000: PER (avg): 0.1469 CTC Loss (avg): 14.8463 WER(5gram): 33.77% (n=256) time: 15.062
2026-01-08 17:58:01,953: WER lens: avg_true_words=5.99 avg_pred_words=6.50 max_pred_words=13
2026-01-08 17:58:01,956: t15.2023.08.13 val PER: 0.1091
2026-01-08 17:58:01,958: t15.2023.08.18 val PER: 0.1081
2026-01-08 17:58:01,960: t15.2023.08.20 val PER: 0.1041
2026-01-08 17:58:01,961: t15.2023.08.25 val PER: 0.0979
2026-01-08 17:58:01,963: t15.2023.08.27 val PER: 0.1768
2026-01-08 17:58:01,965: t15.2023.09.01 val PER: 0.0779
2026-01-08 17:58:01,967: t15.2023.09.03 val PER: 0.1544
2026-01-08 17:58:01,968: t15.2023.09.24 val PER: 0.1117
2026-01-08 17:58:01,970: t15.2023.09.29 val PER: 0.1308
2026-01-08 17:58:01,971: t15.2023.10.01 val PER: 0.1651
2026-01-08 17:58:01,973: t15.2023.10.06 val PER: 0.0861
2026-01-08 17:58:01,975: t15.2023.10.08 val PER: 0.2382
2026-01-08 17:58:01,976: t15.2023.10.13 val PER: 0.1963
2026-01-08 17:58:01,978: t15.2023.10.15 val PER: 0.1516
2026-01-08 17:58:01,980: t15.2023.10.20 val PER: 0.1711
2026-01-08 17:58:01,981: t15.2023.10.22 val PER: 0.1114
2026-01-08 17:58:01,983: t15.2023.11.03 val PER: 0.1737
2026-01-08 17:58:01,984: t15.2023.11.04 val PER: 0.0341
2026-01-08 17:58:01,986: t15.2023.11.17 val PER: 0.0373
2026-01-08 17:58:01,987: t15.2023.11.19 val PER: 0.0439
2026-01-08 17:58:01,991: t15.2023.11.26 val PER: 0.1130
2026-01-08 17:58:01,992: t15.2023.12.03 val PER: 0.0924
2026-01-08 17:58:01,994: t15.2023.12.08 val PER: 0.1059
2026-01-08 17:58:01,996: t15.2023.12.10 val PER: 0.0920
2026-01-08 17:58:01,998: t15.2023.12.17 val PER: 0.1258
2026-01-08 17:58:02,000: t15.2023.12.29 val PER: 0.1263
2026-01-08 17:58:02,001: t15.2024.02.25 val PER: 0.1110
2026-01-08 17:58:02,003: t15.2024.03.08 val PER: 0.2020
2026-01-08 17:58:02,004: t15.2024.03.15 val PER: 0.1982
2026-01-08 17:58:02,006: t15.2024.03.17 val PER: 0.1297
2026-01-08 17:58:02,008: t15.2024.05.10 val PER: 0.1857
2026-01-08 17:58:02,010: t15.2024.06.14 val PER: 0.1483
2026-01-08 17:58:02,011: t15.2024.07.19 val PER: 0.2274
2026-01-08 17:58:02,013: t15.2024.07.21 val PER: 0.0910
2026-01-08 17:58:02,015: t15.2024.07.28 val PER: 0.1309
2026-01-08 17:58:02,017: t15.2025.01.10 val PER: 0.2879
2026-01-08 17:58:02,018: t15.2025.01.12 val PER: 0.1509
2026-01-08 17:58:02,020: t15.2025.03.14 val PER: 0.3447
2026-01-08 17:58:02,022: t15.2025.03.16 val PER: 0.1872
2026-01-08 17:58:02,023: t15.2025.03.30 val PER: 0.2770
2026-01-08 17:58:02,025: t15.2025.04.13 val PER: 0.2411
2026-01-08 17:58:02,184: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_19000
2026-01-08 17:58:19,370: Train batch 19200: loss: 6.25 grad norm: 49.14 time: 0.074
2026-01-08 17:58:36,919: Train batch 19400: loss: 5.76 grad norm: 48.37 time: 0.059
2026-01-08 17:58:45,526: Running test after training batch: 19500
2026-01-08 17:58:45,620: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:58:50,692: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 17:58:50,739: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 17:59:00,591: Val batch 19500: PER (avg): 0.1458 CTC Loss (avg): 14.8200 WER(5gram): 32.20% (n=256) time: 15.062
2026-01-08 17:59:00,594: WER lens: avg_true_words=5.99 avg_pred_words=6.45 max_pred_words=13
2026-01-08 17:59:00,596: t15.2023.08.13 val PER: 0.1112
2026-01-08 17:59:00,599: t15.2023.08.18 val PER: 0.1065
2026-01-08 17:59:00,600: t15.2023.08.20 val PER: 0.1064
2026-01-08 17:59:00,602: t15.2023.08.25 val PER: 0.0964
2026-01-08 17:59:00,604: t15.2023.08.27 val PER: 0.1817
2026-01-08 17:59:00,608: t15.2023.09.01 val PER: 0.0731
2026-01-08 17:59:00,610: t15.2023.09.03 val PER: 0.1461
2026-01-08 17:59:00,612: t15.2023.09.24 val PER: 0.1165
2026-01-08 17:59:00,614: t15.2023.09.29 val PER: 0.1270
2026-01-08 17:59:00,616: t15.2023.10.01 val PER: 0.1645
2026-01-08 17:59:00,617: t15.2023.10.06 val PER: 0.0818
2026-01-08 17:59:00,619: t15.2023.10.08 val PER: 0.2382
2026-01-08 17:59:00,621: t15.2023.10.13 val PER: 0.1877
2026-01-08 17:59:00,623: t15.2023.10.15 val PER: 0.1503
2026-01-08 17:59:00,625: t15.2023.10.20 val PER: 0.1678
2026-01-08 17:59:00,627: t15.2023.10.22 val PER: 0.1058
2026-01-08 17:59:00,629: t15.2023.11.03 val PER: 0.1710
2026-01-08 17:59:00,631: t15.2023.11.04 val PER: 0.0341
2026-01-08 17:59:00,633: t15.2023.11.17 val PER: 0.0327
2026-01-08 17:59:00,634: t15.2023.11.19 val PER: 0.0479
2026-01-08 17:59:00,636: t15.2023.11.26 val PER: 0.1101
2026-01-08 17:59:00,638: t15.2023.12.03 val PER: 0.0998
2026-01-08 17:59:00,640: t15.2023.12.08 val PER: 0.1092
2026-01-08 17:59:00,642: t15.2023.12.10 val PER: 0.0933
2026-01-08 17:59:00,643: t15.2023.12.17 val PER: 0.1237
2026-01-08 17:59:00,645: t15.2023.12.29 val PER: 0.1235
2026-01-08 17:59:00,647: t15.2024.02.25 val PER: 0.1081
2026-01-08 17:59:00,649: t15.2024.03.08 val PER: 0.2105
2026-01-08 17:59:00,651: t15.2024.03.15 val PER: 0.2008
2026-01-08 17:59:00,653: t15.2024.03.17 val PER: 0.1262
2026-01-08 17:59:00,654: t15.2024.05.10 val PER: 0.1842
2026-01-08 17:59:00,656: t15.2024.06.14 val PER: 0.1404
2026-01-08 17:59:00,658: t15.2024.07.19 val PER: 0.2248
2026-01-08 17:59:00,660: t15.2024.07.21 val PER: 0.0931
2026-01-08 17:59:00,662: t15.2024.07.28 val PER: 0.1294
2026-01-08 17:59:00,664: t15.2025.01.10 val PER: 0.2893
2026-01-08 17:59:00,666: t15.2025.01.12 val PER: 0.1524
2026-01-08 17:59:00,667: t15.2025.03.14 val PER: 0.3447
2026-01-08 17:59:00,669: t15.2025.03.16 val PER: 0.1859
2026-01-08 17:59:00,671: t15.2025.03.30 val PER: 0.2736
2026-01-08 17:59:00,673: t15.2025.04.13 val PER: 0.2340
2026-01-08 17:59:00,834: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_19500
2026-01-08 17:59:09,540: Train batch 19600: loss: 8.01 grad norm: 59.07 time: 0.064
2026-01-08 17:59:27,146: Train batch 19800: loss: 6.33 grad norm: 46.33 time: 0.063
2026-01-08 17:59:44,466: Running test after training batch: 19999
2026-01-08 17:59:44,565: WER debug GT example: You can see the code at this point as well.
2026-01-08 17:59:49,490: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point as well
2026-01-08 17:59:49,536: WER debug example
  GT : how does it keep the cost down
  PR : how i see it keep the cost
2026-01-08 17:59:59,361: Val batch 19999: PER (avg): 0.1460 CTC Loss (avg): 14.8361 WER(5gram): 34.09% (n=256) time: 14.891
2026-01-08 17:59:59,364: WER lens: avg_true_words=5.99 avg_pred_words=6.47 max_pred_words=13
2026-01-08 17:59:59,367: t15.2023.08.13 val PER: 0.1123
2026-01-08 17:59:59,369: t15.2023.08.18 val PER: 0.1056
2026-01-08 17:59:59,371: t15.2023.08.20 val PER: 0.1025
2026-01-08 17:59:59,373: t15.2023.08.25 val PER: 0.0994
2026-01-08 17:59:59,374: t15.2023.08.27 val PER: 0.1817
2026-01-08 17:59:59,376: t15.2023.09.01 val PER: 0.0763
2026-01-08 17:59:59,377: t15.2023.09.03 val PER: 0.1496
2026-01-08 17:59:59,380: t15.2023.09.24 val PER: 0.1117
2026-01-08 17:59:59,382: t15.2023.09.29 val PER: 0.1257
2026-01-08 17:59:59,384: t15.2023.10.01 val PER: 0.1645
2026-01-08 17:59:59,386: t15.2023.10.06 val PER: 0.0850
2026-01-08 17:59:59,387: t15.2023.10.08 val PER: 0.2327
2026-01-08 17:59:59,389: t15.2023.10.13 val PER: 0.1901
2026-01-08 17:59:59,390: t15.2023.10.15 val PER: 0.1523
2026-01-08 17:59:59,392: t15.2023.10.20 val PER: 0.1745
2026-01-08 17:59:59,393: t15.2023.10.22 val PER: 0.1136
2026-01-08 17:59:59,395: t15.2023.11.03 val PER: 0.1750
2026-01-08 17:59:59,397: t15.2023.11.04 val PER: 0.0341
2026-01-08 17:59:59,398: t15.2023.11.17 val PER: 0.0373
2026-01-08 17:59:59,400: t15.2023.11.19 val PER: 0.0459
2026-01-08 17:59:59,403: t15.2023.11.26 val PER: 0.1130
2026-01-08 17:59:59,404: t15.2023.12.03 val PER: 0.0966
2026-01-08 17:59:59,406: t15.2023.12.08 val PER: 0.1065
2026-01-08 17:59:59,407: t15.2023.12.10 val PER: 0.0959
2026-01-08 17:59:59,409: t15.2023.12.17 val PER: 0.1237
2026-01-08 17:59:59,411: t15.2023.12.29 val PER: 0.1249
2026-01-08 17:59:59,414: t15.2024.02.25 val PER: 0.1053
2026-01-08 17:59:59,416: t15.2024.03.08 val PER: 0.2077
2026-01-08 17:59:59,418: t15.2024.03.15 val PER: 0.1995
2026-01-08 17:59:59,419: t15.2024.03.17 val PER: 0.1290
2026-01-08 17:59:59,421: t15.2024.05.10 val PER: 0.1813
2026-01-08 17:59:59,423: t15.2024.06.14 val PER: 0.1420
2026-01-08 17:59:59,425: t15.2024.07.19 val PER: 0.2274
2026-01-08 17:59:59,427: t15.2024.07.21 val PER: 0.0910
2026-01-08 17:59:59,430: t15.2024.07.28 val PER: 0.1243
2026-01-08 17:59:59,431: t15.2025.01.10 val PER: 0.2879
2026-01-08 17:59:59,433: t15.2025.01.12 val PER: 0.1524
2026-01-08 17:59:59,435: t15.2025.03.14 val PER: 0.3432
2026-01-08 17:59:59,436: t15.2025.03.16 val PER: 0.1846
2026-01-08 17:59:59,438: t15.2025.03.30 val PER: 0.2770
2026-01-08 17:59:59,440: t15.2025.04.13 val PER: 0.2297
2026-01-08 17:59:59,600: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/patch_20/checkpoint/checkpoint_batch_19999
2026-01-08 18:00:00,118: Best avg val PER achieved: 0.14645
2026-01-08 18:00:00,123: Total training time: 43.97 minutes

=== RUN speckle_03.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03
2026-01-08 18:01:47,550: Using device: cuda:0
2026-01-08 18:05:42,246: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-08 18:05:42,274: Using 45 sessions after filtering (from 45).
2026-01-08 18:05:42,729: Using torch.compile (if available)
2026-01-08 18:05:42,731: torch.compile not available (torch<2.0). Skipping.
2026-01-08 18:05:42,734: Initialized RNN decoding model
2026-01-08 18:05:42,736: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-08 18:05:42,738: Model has 44,907,305 parameters
2026-01-08 18:05:42,740: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-08 18:05:44,007: Successfully initialized datasets
2026-01-08 18:05:44,010: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-08 18:05:45,540: Train batch 0: loss: 572.52 grad norm: 1495.05 time: 0.200
2026-01-08 18:05:45,542: Running test after training batch: 0
2026-01-08 18:05:45,661: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:05:51,787: WER debug example
  GT : you can see the code at this point as well
  PR : she has from his
2026-01-08 18:05:52,825: WER debug example
  GT : how does it keep the cost down
  PR : money from
2026-01-08 18:09:48,439: Val batch 0: PER (avg): 1.4296 CTC Loss (avg): 633.1590 WER(5gram): 99.67% (n=256) time: 242.890
2026-01-08 18:09:48,455: WER lens: avg_true_words=5.99 avg_pred_words=2.81 max_pred_words=7
2026-01-08 18:09:48,460: t15.2023.08.13 val PER: 1.3025
2026-01-08 18:09:48,463: t15.2023.08.18 val PER: 1.4258
2026-01-08 18:09:48,464: t15.2023.08.20 val PER: 1.3066
2026-01-08 18:09:48,464: t15.2023.08.25 val PER: 1.3328
2026-01-08 18:09:48,464: t15.2023.08.27 val PER: 1.2428
2026-01-08 18:09:48,464: t15.2023.09.01 val PER: 1.4529
2026-01-08 18:09:48,464: t15.2023.09.03 val PER: 1.3171
2026-01-08 18:09:48,464: t15.2023.09.24 val PER: 1.5449
2026-01-08 18:09:48,464: t15.2023.09.29 val PER: 1.4678
2026-01-08 18:09:48,465: t15.2023.10.01 val PER: 1.2140
2026-01-08 18:09:48,465: t15.2023.10.06 val PER: 1.4909
2026-01-08 18:09:48,465: t15.2023.10.08 val PER: 1.1867
2026-01-08 18:09:48,465: t15.2023.10.13 val PER: 1.3964
2026-01-08 18:09:48,465: t15.2023.10.15 val PER: 1.3902
2026-01-08 18:09:48,465: t15.2023.10.20 val PER: 1.5067
2026-01-08 18:09:48,465: t15.2023.10.22 val PER: 1.3953
2026-01-08 18:09:48,465: t15.2023.11.03 val PER: 1.5943
2026-01-08 18:09:48,465: t15.2023.11.04 val PER: 2.0410
2026-01-08 18:09:48,465: t15.2023.11.17 val PER: 1.9456
2026-01-08 18:09:48,465: t15.2023.11.19 val PER: 1.6766
2026-01-08 18:09:48,466: t15.2023.11.26 val PER: 1.5384
2026-01-08 18:09:48,466: t15.2023.12.03 val PER: 1.4254
2026-01-08 18:09:48,466: t15.2023.12.08 val PER: 1.4521
2026-01-08 18:09:48,466: t15.2023.12.10 val PER: 1.6978
2026-01-08 18:09:48,466: t15.2023.12.17 val PER: 1.3035
2026-01-08 18:09:48,466: t15.2023.12.29 val PER: 1.4070
2026-01-08 18:09:48,466: t15.2024.02.25 val PER: 1.4242
2026-01-08 18:09:48,466: t15.2024.03.08 val PER: 1.3243
2026-01-08 18:09:48,466: t15.2024.03.15 val PER: 1.3183
2026-01-08 18:09:48,466: t15.2024.03.17 val PER: 1.4031
2026-01-08 18:09:48,466: t15.2024.05.10 val PER: 1.3165
2026-01-08 18:09:48,467: t15.2024.06.14 val PER: 1.5284
2026-01-08 18:09:48,467: t15.2024.07.19 val PER: 1.0824
2026-01-08 18:09:48,467: t15.2024.07.21 val PER: 1.6290
2026-01-08 18:09:48,467: t15.2024.07.28 val PER: 1.6581
2026-01-08 18:09:48,467: t15.2025.01.10 val PER: 1.0895
2026-01-08 18:09:48,467: t15.2025.01.12 val PER: 1.7606
2026-01-08 18:09:48,467: t15.2025.03.14 val PER: 1.0370
2026-01-08 18:09:48,467: t15.2025.03.16 val PER: 1.6204
2026-01-08 18:09:48,467: t15.2025.03.30 val PER: 1.2908
2026-01-08 18:09:48,467: t15.2025.04.13 val PER: 1.5934
2026-01-08 18:09:48,468: New best val WER(5gram) inf% --> 99.67%
2026-01-08 18:09:48,678: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_0
2026-01-08 18:10:06,939: Train batch 200: loss: 82.28 grad norm: 85.74 time: 0.058
2026-01-08 18:10:25,029: Train batch 400: loss: 58.36 grad norm: 90.61 time: 0.063
2026-01-08 18:10:33,746: Running test after training batch: 500
2026-01-08 18:10:33,882: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:10:39,427: WER debug example
  GT : you can see the code at this point as well
  PR : you'll need is used and at this ride is all
2026-01-08 18:10:39,598: WER debug example
  GT : how does it keep the cost down
  PR : and does it feel this is as
2026-01-08 18:11:37,631: Val batch 500: PER (avg): 0.5456 CTC Loss (avg): 60.6912 WER(5gram): 82.33% (n=256) time: 63.883
2026-01-08 18:11:37,634: WER lens: avg_true_words=5.99 avg_pred_words=5.79 max_pred_words=12
2026-01-08 18:11:37,636: t15.2023.08.13 val PER: 0.4751
2026-01-08 18:11:37,638: t15.2023.08.18 val PER: 0.4862
2026-01-08 18:11:37,640: t15.2023.08.20 val PER: 0.4909
2026-01-08 18:11:37,641: t15.2023.08.25 val PER: 0.4473
2026-01-08 18:11:37,643: t15.2023.08.27 val PER: 0.5418
2026-01-08 18:11:37,645: t15.2023.09.01 val PER: 0.4416
2026-01-08 18:11:37,646: t15.2023.09.03 val PER: 0.5166
2026-01-08 18:11:37,648: t15.2023.09.24 val PER: 0.4612
2026-01-08 18:11:37,650: t15.2023.09.29 val PER: 0.4888
2026-01-08 18:11:37,651: t15.2023.10.01 val PER: 0.5244
2026-01-08 18:11:37,653: t15.2023.10.06 val PER: 0.4586
2026-01-08 18:11:37,655: t15.2023.10.08 val PER: 0.5616
2026-01-08 18:11:37,656: t15.2023.10.13 val PER: 0.5756
2026-01-08 18:11:37,658: t15.2023.10.15 val PER: 0.5221
2026-01-08 18:11:37,660: t15.2023.10.20 val PER: 0.5000
2026-01-08 18:11:37,661: t15.2023.10.22 val PER: 0.4655
2026-01-08 18:11:37,663: t15.2023.11.03 val PER: 0.5400
2026-01-08 18:11:37,664: t15.2023.11.04 val PER: 0.3208
2026-01-08 18:11:37,666: t15.2023.11.17 val PER: 0.3997
2026-01-08 18:11:37,668: t15.2023.11.19 val PER: 0.3932
2026-01-08 18:11:37,669: t15.2023.11.26 val PER: 0.5710
2026-01-08 18:11:37,671: t15.2023.12.03 val PER: 0.5221
2026-01-08 18:11:37,672: t15.2023.12.08 val PER: 0.5360
2026-01-08 18:11:37,674: t15.2023.12.10 val PER: 0.5007
2026-01-08 18:11:37,675: t15.2023.12.17 val PER: 0.6060
2026-01-08 18:11:37,677: t15.2023.12.29 val PER: 0.5916
2026-01-08 18:11:37,678: t15.2024.02.25 val PER: 0.5014
2026-01-08 18:11:37,680: t15.2024.03.08 val PER: 0.6543
2026-01-08 18:11:37,682: t15.2024.03.15 val PER: 0.6048
2026-01-08 18:11:37,683: t15.2024.03.17 val PER: 0.5314
2026-01-08 18:11:37,685: t15.2024.05.10 val PER: 0.5691
2026-01-08 18:11:37,687: t15.2024.06.14 val PER: 0.5379
2026-01-08 18:11:37,688: t15.2024.07.19 val PER: 0.7020
2026-01-08 18:11:37,690: t15.2024.07.21 val PER: 0.5124
2026-01-08 18:11:37,692: t15.2024.07.28 val PER: 0.5353
2026-01-08 18:11:37,693: t15.2025.01.10 val PER: 0.7493
2026-01-08 18:11:37,695: t15.2025.01.12 val PER: 0.5928
2026-01-08 18:11:37,696: t15.2025.03.14 val PER: 0.7544
2026-01-08 18:11:37,700: t15.2025.03.16 val PER: 0.6178
2026-01-08 18:11:37,701: t15.2025.03.30 val PER: 0.7379
2026-01-08 18:11:37,703: t15.2025.04.13 val PER: 0.6148
2026-01-08 18:11:37,705: New best val WER(5gram) 99.67% --> 82.33%
2026-01-08 18:11:37,887: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_500
2026-01-08 18:11:46,351: Train batch 600: loss: 52.68 grad norm: 89.13 time: 0.080
2026-01-08 18:12:03,592: Train batch 800: loss: 44.18 grad norm: 83.69 time: 0.058
2026-01-08 18:12:21,016: Train batch 1000: loss: 45.95 grad norm: 75.81 time: 0.069
2026-01-08 18:12:21,018: Running test after training batch: 1000
2026-01-08 18:12:21,134: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:12:26,609: WER debug example
  GT : you can see the code at this point as well
  PR : yield and ease the head at this and is well
2026-01-08 18:12:26,738: WER debug example
  GT : how does it keep the cost down
  PR : howled as it is that it's not
2026-01-08 18:13:01,060: Val batch 1000: PER (avg): 0.4295 CTC Loss (avg): 45.5213 WER(5gram): 58.15% (n=256) time: 40.037
2026-01-08 18:13:01,063: WER lens: avg_true_words=5.99 avg_pred_words=5.68 max_pred_words=12
2026-01-08 18:13:01,066: t15.2023.08.13 val PER: 0.3981
2026-01-08 18:13:01,068: t15.2023.08.18 val PER: 0.3722
2026-01-08 18:13:01,070: t15.2023.08.20 val PER: 0.3630
2026-01-08 18:13:01,072: t15.2023.08.25 val PER: 0.3313
2026-01-08 18:13:01,073: t15.2023.08.27 val PER: 0.4244
2026-01-08 18:13:01,075: t15.2023.09.01 val PER: 0.3263
2026-01-08 18:13:01,077: t15.2023.09.03 val PER: 0.4181
2026-01-08 18:13:01,079: t15.2023.09.24 val PER: 0.3519
2026-01-08 18:13:01,082: t15.2023.09.29 val PER: 0.3893
2026-01-08 18:13:01,083: t15.2023.10.01 val PER: 0.4287
2026-01-08 18:13:01,085: t15.2023.10.06 val PER: 0.3402
2026-01-08 18:13:01,087: t15.2023.10.08 val PER: 0.4614
2026-01-08 18:13:01,089: t15.2023.10.13 val PER: 0.4779
2026-01-08 18:13:01,091: t15.2023.10.15 val PER: 0.4054
2026-01-08 18:13:01,093: t15.2023.10.20 val PER: 0.4128
2026-01-08 18:13:01,094: t15.2023.10.22 val PER: 0.3675
2026-01-08 18:13:01,096: t15.2023.11.03 val PER: 0.4247
2026-01-08 18:13:01,099: t15.2023.11.04 val PER: 0.1911
2026-01-08 18:13:01,101: t15.2023.11.17 val PER: 0.2784
2026-01-08 18:13:01,103: t15.2023.11.19 val PER: 0.2495
2026-01-08 18:13:01,104: t15.2023.11.26 val PER: 0.4667
2026-01-08 18:13:01,106: t15.2023.12.03 val PER: 0.4286
2026-01-08 18:13:01,108: t15.2023.12.08 val PER: 0.4254
2026-01-08 18:13:01,109: t15.2023.12.10 val PER: 0.3693
2026-01-08 18:13:01,111: t15.2023.12.17 val PER: 0.4356
2026-01-08 18:13:01,113: t15.2023.12.29 val PER: 0.4248
2026-01-08 18:13:01,114: t15.2024.02.25 val PER: 0.3778
2026-01-08 18:13:01,116: t15.2024.03.08 val PER: 0.5363
2026-01-08 18:13:01,118: t15.2024.03.15 val PER: 0.4628
2026-01-08 18:13:01,119: t15.2024.03.17 val PER: 0.4198
2026-01-08 18:13:01,121: t15.2024.05.10 val PER: 0.4502
2026-01-08 18:13:01,122: t15.2024.06.14 val PER: 0.4369
2026-01-08 18:13:01,124: t15.2024.07.19 val PER: 0.5583
2026-01-08 18:13:01,126: t15.2024.07.21 val PER: 0.4007
2026-01-08 18:13:01,128: t15.2024.07.28 val PER: 0.4324
2026-01-08 18:13:01,129: t15.2025.01.10 val PER: 0.6226
2026-01-08 18:13:01,131: t15.2025.01.12 val PER: 0.4673
2026-01-08 18:13:01,133: t15.2025.03.14 val PER: 0.6361
2026-01-08 18:13:01,134: t15.2025.03.16 val PER: 0.4974
2026-01-08 18:13:01,136: t15.2025.03.30 val PER: 0.6598
2026-01-08 18:13:01,138: t15.2025.04.13 val PER: 0.4907
2026-01-08 18:13:01,139: New best val WER(5gram) 82.33% --> 58.15%
2026-01-08 18:13:01,322: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_1000
2026-01-08 18:13:18,148: Train batch 1200: loss: 36.35 grad norm: 78.50 time: 0.070
2026-01-08 18:13:35,639: Train batch 1400: loss: 38.10 grad norm: 79.61 time: 0.062
2026-01-08 18:13:44,426: Running test after training batch: 1500
2026-01-08 18:13:44,533: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:13:49,661: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-08 18:13:49,783: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep that was it
2026-01-08 18:14:09,977: Val batch 1500: PER (avg): 0.3958 CTC Loss (avg): 39.4377 WER(5gram): 38.14% (n=256) time: 25.549
2026-01-08 18:14:09,980: WER lens: avg_true_words=5.99 avg_pred_words=5.27 max_pred_words=12
2026-01-08 18:14:09,983: t15.2023.08.13 val PER: 0.3576
2026-01-08 18:14:09,985: t15.2023.08.18 val PER: 0.3336
2026-01-08 18:14:09,987: t15.2023.08.20 val PER: 0.3090
2026-01-08 18:14:09,989: t15.2023.08.25 val PER: 0.2771
2026-01-08 18:14:09,991: t15.2023.08.27 val PER: 0.4003
2026-01-08 18:14:09,993: t15.2023.09.01 val PER: 0.2995
2026-01-08 18:14:09,995: t15.2023.09.03 val PER: 0.3848
2026-01-08 18:14:09,997: t15.2023.09.24 val PER: 0.3204
2026-01-08 18:14:09,999: t15.2023.09.29 val PER: 0.3618
2026-01-08 18:14:10,001: t15.2023.10.01 val PER: 0.4095
2026-01-08 18:14:10,003: t15.2023.10.06 val PER: 0.3079
2026-01-08 18:14:10,005: t15.2023.10.08 val PER: 0.4587
2026-01-08 18:14:10,008: t15.2023.10.13 val PER: 0.4515
2026-01-08 18:14:10,010: t15.2023.10.15 val PER: 0.3777
2026-01-08 18:14:10,012: t15.2023.10.20 val PER: 0.3691
2026-01-08 18:14:10,014: t15.2023.10.22 val PER: 0.3396
2026-01-08 18:14:10,016: t15.2023.11.03 val PER: 0.3786
2026-01-08 18:14:10,018: t15.2023.11.04 val PER: 0.1433
2026-01-08 18:14:10,020: t15.2023.11.17 val PER: 0.2348
2026-01-08 18:14:10,022: t15.2023.11.19 val PER: 0.1936
2026-01-08 18:14:10,024: t15.2023.11.26 val PER: 0.4362
2026-01-08 18:14:10,026: t15.2023.12.03 val PER: 0.3897
2026-01-08 18:14:10,028: t15.2023.12.08 val PER: 0.3862
2026-01-08 18:14:10,031: t15.2023.12.10 val PER: 0.3101
2026-01-08 18:14:10,033: t15.2023.12.17 val PER: 0.3753
2026-01-08 18:14:10,035: t15.2023.12.29 val PER: 0.3830
2026-01-08 18:14:10,037: t15.2024.02.25 val PER: 0.3258
2026-01-08 18:14:10,040: t15.2024.03.08 val PER: 0.4680
2026-01-08 18:14:10,042: t15.2024.03.15 val PER: 0.4365
2026-01-08 18:14:10,044: t15.2024.03.17 val PER: 0.3905
2026-01-08 18:14:10,046: t15.2024.05.10 val PER: 0.4086
2026-01-08 18:14:10,048: t15.2024.06.14 val PER: 0.3975
2026-01-08 18:14:10,050: t15.2024.07.19 val PER: 0.5445
2026-01-08 18:14:10,052: t15.2024.07.21 val PER: 0.3545
2026-01-08 18:14:10,054: t15.2024.07.28 val PER: 0.3919
2026-01-08 18:14:10,056: t15.2025.01.10 val PER: 0.6157
2026-01-08 18:14:10,057: t15.2025.01.12 val PER: 0.4442
2026-01-08 18:14:10,059: t15.2025.03.14 val PER: 0.6109
2026-01-08 18:14:10,061: t15.2025.03.16 val PER: 0.4686
2026-01-08 18:14:10,063: t15.2025.03.30 val PER: 0.6540
2026-01-08 18:14:10,064: t15.2025.04.13 val PER: 0.4864
2026-01-08 18:14:10,066: New best val WER(5gram) 58.15% --> 38.14%
2026-01-08 18:14:10,268: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_1500
2026-01-08 18:14:19,265: Train batch 1600: loss: 38.74 grad norm: 75.64 time: 0.064
2026-01-08 18:14:37,107: Train batch 1800: loss: 36.53 grad norm: 75.09 time: 0.088
2026-01-08 18:14:55,059: Train batch 2000: loss: 35.70 grad norm: 71.90 time: 0.067
2026-01-08 18:14:55,061: Running test after training batch: 2000
2026-01-08 18:14:55,194: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:15:00,230: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 18:15:00,314: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost of
2026-01-08 18:15:19,200: Val batch 2000: PER (avg): 0.3382 CTC Loss (avg): 34.2888 WER(5gram): 31.23% (n=256) time: 24.136
2026-01-08 18:15:19,203: WER lens: avg_true_words=5.99 avg_pred_words=5.77 max_pred_words=12
2026-01-08 18:15:19,208: t15.2023.08.13 val PER: 0.3285
2026-01-08 18:15:19,210: t15.2023.08.18 val PER: 0.2674
2026-01-08 18:15:19,212: t15.2023.08.20 val PER: 0.2716
2026-01-08 18:15:19,214: t15.2023.08.25 val PER: 0.2319
2026-01-08 18:15:19,216: t15.2023.08.27 val PER: 0.3569
2026-01-08 18:15:19,218: t15.2023.09.01 val PER: 0.2443
2026-01-08 18:15:19,220: t15.2023.09.03 val PER: 0.3290
2026-01-08 18:15:19,222: t15.2023.09.24 val PER: 0.2609
2026-01-08 18:15:19,224: t15.2023.09.29 val PER: 0.2795
2026-01-08 18:15:19,226: t15.2023.10.01 val PER: 0.3375
2026-01-08 18:15:19,227: t15.2023.10.06 val PER: 0.2594
2026-01-08 18:15:19,230: t15.2023.10.08 val PER: 0.4005
2026-01-08 18:15:19,231: t15.2023.10.13 val PER: 0.3879
2026-01-08 18:15:19,233: t15.2023.10.15 val PER: 0.3052
2026-01-08 18:15:19,235: t15.2023.10.20 val PER: 0.2919
2026-01-08 18:15:19,236: t15.2023.10.22 val PER: 0.2773
2026-01-08 18:15:19,238: t15.2023.11.03 val PER: 0.3304
2026-01-08 18:15:19,239: t15.2023.11.04 val PER: 0.1092
2026-01-08 18:15:19,241: t15.2023.11.17 val PER: 0.1882
2026-01-08 18:15:19,243: t15.2023.11.19 val PER: 0.1437
2026-01-08 18:15:19,244: t15.2023.11.26 val PER: 0.3703
2026-01-08 18:15:19,246: t15.2023.12.03 val PER: 0.3351
2026-01-08 18:15:19,247: t15.2023.12.08 val PER: 0.3236
2026-01-08 18:15:19,249: t15.2023.12.10 val PER: 0.2654
2026-01-08 18:15:19,250: t15.2023.12.17 val PER: 0.3316
2026-01-08 18:15:19,252: t15.2023.12.29 val PER: 0.3404
2026-01-08 18:15:19,253: t15.2024.02.25 val PER: 0.2823
2026-01-08 18:15:19,255: t15.2024.03.08 val PER: 0.4040
2026-01-08 18:15:19,257: t15.2024.03.15 val PER: 0.3727
2026-01-08 18:15:19,258: t15.2024.03.17 val PER: 0.3563
2026-01-08 18:15:19,260: t15.2024.05.10 val PER: 0.3551
2026-01-08 18:15:19,262: t15.2024.06.14 val PER: 0.3486
2026-01-08 18:15:19,264: t15.2024.07.19 val PER: 0.4726
2026-01-08 18:15:19,265: t15.2024.07.21 val PER: 0.2945
2026-01-08 18:15:19,267: t15.2024.07.28 val PER: 0.3360
2026-01-08 18:15:19,269: t15.2025.01.10 val PER: 0.5399
2026-01-08 18:15:19,271: t15.2025.01.12 val PER: 0.3918
2026-01-08 18:15:19,273: t15.2025.03.14 val PER: 0.5533
2026-01-08 18:15:19,275: t15.2025.03.16 val PER: 0.3835
2026-01-08 18:15:19,276: t15.2025.03.30 val PER: 0.5874
2026-01-08 18:15:19,278: t15.2025.04.13 val PER: 0.4137
2026-01-08 18:15:19,280: New best val WER(5gram) 38.14% --> 31.23%
2026-01-08 18:15:19,472: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_2000
2026-01-08 18:15:36,315: Train batch 2200: loss: 30.62 grad norm: 71.50 time: 0.063
2026-01-08 18:15:53,272: Train batch 2400: loss: 30.75 grad norm: 62.20 time: 0.053
2026-01-08 18:16:01,828: Running test after training batch: 2500
2026-01-08 18:16:01,963: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:16:06,922: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 18:16:06,982: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us in
2026-01-08 18:16:23,394: Val batch 2500: PER (avg): 0.3166 CTC Loss (avg): 31.7349 WER(5gram): 28.10% (n=256) time: 21.564
2026-01-08 18:16:23,397: WER lens: avg_true_words=5.99 avg_pred_words=5.78 max_pred_words=12
2026-01-08 18:16:23,400: t15.2023.08.13 val PER: 0.3015
2026-01-08 18:16:23,402: t15.2023.08.18 val PER: 0.2649
2026-01-08 18:16:23,404: t15.2023.08.20 val PER: 0.2526
2026-01-08 18:16:23,406: t15.2023.08.25 val PER: 0.2214
2026-01-08 18:16:23,407: t15.2023.08.27 val PER: 0.3424
2026-01-08 18:16:23,409: t15.2023.09.01 val PER: 0.2240
2026-01-08 18:16:23,411: t15.2023.09.03 val PER: 0.2969
2026-01-08 18:16:23,412: t15.2023.09.24 val PER: 0.2415
2026-01-08 18:16:23,414: t15.2023.09.29 val PER: 0.2610
2026-01-08 18:16:23,415: t15.2023.10.01 val PER: 0.3296
2026-01-08 18:16:23,417: t15.2023.10.06 val PER: 0.2250
2026-01-08 18:16:23,418: t15.2023.10.08 val PER: 0.3843
2026-01-08 18:16:23,420: t15.2023.10.13 val PER: 0.3732
2026-01-08 18:16:23,422: t15.2023.10.15 val PER: 0.2940
2026-01-08 18:16:23,425: t15.2023.10.20 val PER: 0.2852
2026-01-08 18:16:23,426: t15.2023.10.22 val PER: 0.2561
2026-01-08 18:16:23,428: t15.2023.11.03 val PER: 0.3114
2026-01-08 18:16:23,430: t15.2023.11.04 val PER: 0.0922
2026-01-08 18:16:23,431: t15.2023.11.17 val PER: 0.1509
2026-01-08 18:16:23,433: t15.2023.11.19 val PER: 0.1257
2026-01-08 18:16:23,435: t15.2023.11.26 val PER: 0.3616
2026-01-08 18:16:23,437: t15.2023.12.03 val PER: 0.3025
2026-01-08 18:16:23,439: t15.2023.12.08 val PER: 0.2956
2026-01-08 18:16:23,440: t15.2023.12.10 val PER: 0.2549
2026-01-08 18:16:23,442: t15.2023.12.17 val PER: 0.3098
2026-01-08 18:16:23,443: t15.2023.12.29 val PER: 0.3219
2026-01-08 18:16:23,445: t15.2024.02.25 val PER: 0.2486
2026-01-08 18:16:23,447: t15.2024.03.08 val PER: 0.3755
2026-01-08 18:16:23,448: t15.2024.03.15 val PER: 0.3458
2026-01-08 18:16:23,450: t15.2024.03.17 val PER: 0.3180
2026-01-08 18:16:23,452: t15.2024.05.10 val PER: 0.3328
2026-01-08 18:16:23,453: t15.2024.06.14 val PER: 0.2981
2026-01-08 18:16:23,455: t15.2024.07.19 val PER: 0.4502
2026-01-08 18:16:23,457: t15.2024.07.21 val PER: 0.2683
2026-01-08 18:16:23,458: t15.2024.07.28 val PER: 0.3118
2026-01-08 18:16:23,460: t15.2025.01.10 val PER: 0.5138
2026-01-08 18:16:23,461: t15.2025.01.12 val PER: 0.3703
2026-01-08 18:16:23,463: t15.2025.03.14 val PER: 0.5311
2026-01-08 18:16:23,464: t15.2025.03.16 val PER: 0.3809
2026-01-08 18:16:23,466: t15.2025.03.30 val PER: 0.5437
2026-01-08 18:16:23,467: t15.2025.04.13 val PER: 0.3994
2026-01-08 18:16:23,469: New best val WER(5gram) 31.23% --> 28.10%
2026-01-08 18:16:23,660: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_2500
2026-01-08 18:16:32,430: Train batch 2600: loss: 37.47 grad norm: 84.37 time: 0.056
2026-01-08 18:16:49,057: Train batch 2800: loss: 26.91 grad norm: 66.44 time: 0.082
2026-01-08 18:17:05,766: Train batch 3000: loss: 33.19 grad norm: 75.09 time: 0.083
2026-01-08 18:17:05,768: Running test after training batch: 3000
2026-01-08 18:17:05,870: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:17:11,229: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 18:17:11,296: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 18:17:26,827: Val batch 3000: PER (avg): 0.2887 CTC Loss (avg): 29.0737 WER(5gram): 24.77% (n=256) time: 21.057
2026-01-08 18:17:26,830: WER lens: avg_true_words=5.99 avg_pred_words=5.96 max_pred_words=12
2026-01-08 18:17:26,834: t15.2023.08.13 val PER: 0.2578
2026-01-08 18:17:26,836: t15.2023.08.18 val PER: 0.2347
2026-01-08 18:17:26,838: t15.2023.08.20 val PER: 0.2248
2026-01-08 18:17:26,840: t15.2023.08.25 val PER: 0.2033
2026-01-08 18:17:26,841: t15.2023.08.27 val PER: 0.3119
2026-01-08 18:17:26,843: t15.2023.09.01 val PER: 0.1907
2026-01-08 18:17:26,846: t15.2023.09.03 val PER: 0.2815
2026-01-08 18:17:26,848: t15.2023.09.24 val PER: 0.2124
2026-01-08 18:17:26,849: t15.2023.09.29 val PER: 0.2361
2026-01-08 18:17:26,851: t15.2023.10.01 val PER: 0.2933
2026-01-08 18:17:26,852: t15.2023.10.06 val PER: 0.2110
2026-01-08 18:17:26,854: t15.2023.10.08 val PER: 0.3627
2026-01-08 18:17:26,856: t15.2023.10.13 val PER: 0.3476
2026-01-08 18:17:26,857: t15.2023.10.15 val PER: 0.2749
2026-01-08 18:17:26,859: t15.2023.10.20 val PER: 0.2617
2026-01-08 18:17:26,860: t15.2023.10.22 val PER: 0.2227
2026-01-08 18:17:26,862: t15.2023.11.03 val PER: 0.2856
2026-01-08 18:17:26,864: t15.2023.11.04 val PER: 0.0853
2026-01-08 18:17:26,865: t15.2023.11.17 val PER: 0.1384
2026-01-08 18:17:26,867: t15.2023.11.19 val PER: 0.1198
2026-01-08 18:17:26,868: t15.2023.11.26 val PER: 0.3116
2026-01-08 18:17:26,870: t15.2023.12.03 val PER: 0.2679
2026-01-08 18:17:26,872: t15.2023.12.08 val PER: 0.2730
2026-01-08 18:17:26,873: t15.2023.12.10 val PER: 0.2102
2026-01-08 18:17:26,875: t15.2023.12.17 val PER: 0.2817
2026-01-08 18:17:26,877: t15.2023.12.29 val PER: 0.2917
2026-01-08 18:17:26,878: t15.2024.02.25 val PER: 0.2570
2026-01-08 18:17:26,880: t15.2024.03.08 val PER: 0.3556
2026-01-08 18:17:26,882: t15.2024.03.15 val PER: 0.3265
2026-01-08 18:17:26,883: t15.2024.03.17 val PER: 0.3013
2026-01-08 18:17:26,885: t15.2024.05.10 val PER: 0.2942
2026-01-08 18:17:26,887: t15.2024.06.14 val PER: 0.2871
2026-01-08 18:17:26,888: t15.2024.07.19 val PER: 0.4087
2026-01-08 18:17:26,890: t15.2024.07.21 val PER: 0.2352
2026-01-08 18:17:26,891: t15.2024.07.28 val PER: 0.2801
2026-01-08 18:17:26,893: t15.2025.01.10 val PER: 0.4835
2026-01-08 18:17:26,895: t15.2025.01.12 val PER: 0.3457
2026-01-08 18:17:26,896: t15.2025.03.14 val PER: 0.4837
2026-01-08 18:17:26,898: t15.2025.03.16 val PER: 0.3351
2026-01-08 18:17:26,900: t15.2025.03.30 val PER: 0.5080
2026-01-08 18:17:26,901: t15.2025.04.13 val PER: 0.3723
2026-01-08 18:17:26,903: New best val WER(5gram) 28.10% --> 24.77%
2026-01-08 18:17:27,096: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_3000
2026-01-08 18:17:45,529: Train batch 3200: loss: 28.11 grad norm: 64.20 time: 0.079
2026-01-08 18:18:04,005: Train batch 3400: loss: 20.14 grad norm: 53.30 time: 0.049
2026-01-08 18:18:13,432: Running test after training batch: 3500
2026-01-08 18:18:13,566: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:18:18,598: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 18:18:18,664: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 18:18:33,404: Val batch 3500: PER (avg): 0.2789 CTC Loss (avg): 27.8953 WER(5gram): 24.64% (n=256) time: 19.969
2026-01-08 18:18:33,407: WER lens: avg_true_words=5.99 avg_pred_words=5.97 max_pred_words=12
2026-01-08 18:18:33,410: t15.2023.08.13 val PER: 0.2578
2026-01-08 18:18:33,412: t15.2023.08.18 val PER: 0.2137
2026-01-08 18:18:33,416: t15.2023.08.20 val PER: 0.2208
2026-01-08 18:18:33,418: t15.2023.08.25 val PER: 0.1943
2026-01-08 18:18:33,420: t15.2023.08.27 val PER: 0.2974
2026-01-08 18:18:33,422: t15.2023.09.01 val PER: 0.1916
2026-01-08 18:18:33,424: t15.2023.09.03 val PER: 0.2732
2026-01-08 18:18:33,426: t15.2023.09.24 val PER: 0.2245
2026-01-08 18:18:33,428: t15.2023.09.29 val PER: 0.2157
2026-01-08 18:18:33,430: t15.2023.10.01 val PER: 0.2880
2026-01-08 18:18:33,431: t15.2023.10.06 val PER: 0.2088
2026-01-08 18:18:33,433: t15.2023.10.08 val PER: 0.3478
2026-01-08 18:18:33,435: t15.2023.10.13 val PER: 0.3406
2026-01-08 18:18:33,438: t15.2023.10.15 val PER: 0.2584
2026-01-08 18:18:33,440: t15.2023.10.20 val PER: 0.2651
2026-01-08 18:18:33,442: t15.2023.10.22 val PER: 0.2205
2026-01-08 18:18:33,444: t15.2023.11.03 val PER: 0.2720
2026-01-08 18:18:33,445: t15.2023.11.04 val PER: 0.0785
2026-01-08 18:18:33,447: t15.2023.11.17 val PER: 0.1260
2026-01-08 18:18:33,449: t15.2023.11.19 val PER: 0.0998
2026-01-08 18:18:33,451: t15.2023.11.26 val PER: 0.2913
2026-01-08 18:18:33,454: t15.2023.12.03 val PER: 0.2553
2026-01-08 18:18:33,456: t15.2023.12.08 val PER: 0.2783
2026-01-08 18:18:33,458: t15.2023.12.10 val PER: 0.2142
2026-01-08 18:18:33,459: t15.2023.12.17 val PER: 0.2536
2026-01-08 18:18:33,461: t15.2023.12.29 val PER: 0.2752
2026-01-08 18:18:33,463: t15.2024.02.25 val PER: 0.2331
2026-01-08 18:18:33,464: t15.2024.03.08 val PER: 0.3485
2026-01-08 18:18:33,466: t15.2024.03.15 val PER: 0.3215
2026-01-08 18:18:33,468: t15.2024.03.17 val PER: 0.2901
2026-01-08 18:18:33,469: t15.2024.05.10 val PER: 0.2704
2026-01-08 18:18:33,471: t15.2024.06.14 val PER: 0.2950
2026-01-08 18:18:33,473: t15.2024.07.19 val PER: 0.4107
2026-01-08 18:18:33,475: t15.2024.07.21 val PER: 0.2421
2026-01-08 18:18:33,477: t15.2024.07.28 val PER: 0.2846
2026-01-08 18:18:33,478: t15.2025.01.10 val PER: 0.4725
2026-01-08 18:18:33,480: t15.2025.01.12 val PER: 0.3149
2026-01-08 18:18:33,482: t15.2025.03.14 val PER: 0.4571
2026-01-08 18:18:33,483: t15.2025.03.16 val PER: 0.3442
2026-01-08 18:18:33,485: t15.2025.03.30 val PER: 0.4586
2026-01-08 18:18:33,487: t15.2025.04.13 val PER: 0.3524
2026-01-08 18:18:33,489: New best val WER(5gram) 24.77% --> 24.64%
2026-01-08 18:18:33,677: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_3500
2026-01-08 18:18:42,858: Train batch 3600: loss: 23.59 grad norm: 58.79 time: 0.069
2026-01-08 18:19:01,120: Train batch 3800: loss: 27.45 grad norm: 69.72 time: 0.071
2026-01-08 18:19:19,590: Train batch 4000: loss: 20.74 grad norm: 54.35 time: 0.059
2026-01-08 18:19:19,592: Running test after training batch: 4000
2026-01-08 18:19:19,741: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:19:24,958: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 18:19:25,023: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost at
2026-01-08 18:19:38,763: Val batch 4000: PER (avg): 0.2614 CTC Loss (avg): 25.9346 WER(5gram): 24.58% (n=256) time: 19.168
2026-01-08 18:19:38,766: WER lens: avg_true_words=5.99 avg_pred_words=6.03 max_pred_words=12
2026-01-08 18:19:38,769: t15.2023.08.13 val PER: 0.2391
2026-01-08 18:19:38,771: t15.2023.08.18 val PER: 0.2137
2026-01-08 18:19:38,773: t15.2023.08.20 val PER: 0.2160
2026-01-08 18:19:38,774: t15.2023.08.25 val PER: 0.1596
2026-01-08 18:19:38,776: t15.2023.08.27 val PER: 0.2942
2026-01-08 18:19:38,778: t15.2023.09.01 val PER: 0.1851
2026-01-08 18:19:38,780: t15.2023.09.03 val PER: 0.2589
2026-01-08 18:19:38,782: t15.2023.09.24 val PER: 0.1966
2026-01-08 18:19:38,783: t15.2023.09.29 val PER: 0.2042
2026-01-08 18:19:38,786: t15.2023.10.01 val PER: 0.2662
2026-01-08 18:19:38,787: t15.2023.10.06 val PER: 0.1938
2026-01-08 18:19:38,789: t15.2023.10.08 val PER: 0.3369
2026-01-08 18:19:38,791: t15.2023.10.13 val PER: 0.3181
2026-01-08 18:19:38,793: t15.2023.10.15 val PER: 0.2479
2026-01-08 18:19:38,794: t15.2023.10.20 val PER: 0.2651
2026-01-08 18:19:38,795: t15.2023.10.22 val PER: 0.2160
2026-01-08 18:19:38,797: t15.2023.11.03 val PER: 0.2558
2026-01-08 18:19:38,799: t15.2023.11.04 val PER: 0.0614
2026-01-08 18:19:38,800: t15.2023.11.17 val PER: 0.1104
2026-01-08 18:19:38,802: t15.2023.11.19 val PER: 0.1018
2026-01-08 18:19:38,803: t15.2023.11.26 val PER: 0.2920
2026-01-08 18:19:38,804: t15.2023.12.03 val PER: 0.2269
2026-01-08 18:19:38,806: t15.2023.12.08 val PER: 0.2397
2026-01-08 18:19:38,807: t15.2023.12.10 val PER: 0.1879
2026-01-08 18:19:38,809: t15.2023.12.17 val PER: 0.2474
2026-01-08 18:19:38,810: t15.2023.12.29 val PER: 0.2567
2026-01-08 18:19:38,812: t15.2024.02.25 val PER: 0.2261
2026-01-08 18:19:38,813: t15.2024.03.08 val PER: 0.3215
2026-01-08 18:19:38,814: t15.2024.03.15 val PER: 0.3133
2026-01-08 18:19:38,816: t15.2024.03.17 val PER: 0.2706
2026-01-08 18:19:38,817: t15.2024.05.10 val PER: 0.2734
2026-01-08 18:19:38,819: t15.2024.06.14 val PER: 0.2681
2026-01-08 18:19:38,820: t15.2024.07.19 val PER: 0.3817
2026-01-08 18:19:38,822: t15.2024.07.21 val PER: 0.2007
2026-01-08 18:19:38,823: t15.2024.07.28 val PER: 0.2551
2026-01-08 18:19:38,825: t15.2025.01.10 val PER: 0.4256
2026-01-08 18:19:38,826: t15.2025.01.12 val PER: 0.3010
2026-01-08 18:19:38,828: t15.2025.03.14 val PER: 0.4320
2026-01-08 18:19:38,829: t15.2025.03.16 val PER: 0.3220
2026-01-08 18:19:38,831: t15.2025.03.30 val PER: 0.4333
2026-01-08 18:19:38,832: t15.2025.04.13 val PER: 0.3466
2026-01-08 18:19:38,834: New best val WER(5gram) 24.64% --> 24.58%
2026-01-08 18:19:39,018: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_4000
2026-01-08 18:19:57,169: Train batch 4200: loss: 23.84 grad norm: 66.70 time: 0.080
2026-01-08 18:20:14,324: Train batch 4400: loss: 17.86 grad norm: 56.59 time: 0.068
2026-01-08 18:20:23,094: Running test after training batch: 4500
2026-01-08 18:20:23,201: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:20:28,185: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 18:20:28,256: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the west end
2026-01-08 18:20:41,985: Val batch 4500: PER (avg): 0.2502 CTC Loss (avg): 24.4243 WER(5gram): 22.16% (n=256) time: 18.888
2026-01-08 18:20:41,988: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-08 18:20:41,991: t15.2023.08.13 val PER: 0.2193
2026-01-08 18:20:41,993: t15.2023.08.18 val PER: 0.2012
2026-01-08 18:20:41,994: t15.2023.08.20 val PER: 0.2010
2026-01-08 18:20:41,996: t15.2023.08.25 val PER: 0.1642
2026-01-08 18:20:41,998: t15.2023.08.27 val PER: 0.2765
2026-01-08 18:20:42,000: t15.2023.09.01 val PER: 0.1599
2026-01-08 18:20:42,002: t15.2023.09.03 val PER: 0.2577
2026-01-08 18:20:42,004: t15.2023.09.24 val PER: 0.1784
2026-01-08 18:20:42,005: t15.2023.09.29 val PER: 0.1997
2026-01-08 18:20:42,007: t15.2023.10.01 val PER: 0.2655
2026-01-08 18:20:42,008: t15.2023.10.06 val PER: 0.1744
2026-01-08 18:20:42,010: t15.2023.10.08 val PER: 0.3275
2026-01-08 18:20:42,012: t15.2023.10.13 val PER: 0.3095
2026-01-08 18:20:42,013: t15.2023.10.15 val PER: 0.2287
2026-01-08 18:20:42,015: t15.2023.10.20 val PER: 0.2416
2026-01-08 18:20:42,017: t15.2023.10.22 val PER: 0.1949
2026-01-08 18:20:42,019: t15.2023.11.03 val PER: 0.2578
2026-01-08 18:20:42,021: t15.2023.11.04 val PER: 0.0751
2026-01-08 18:20:42,022: t15.2023.11.17 val PER: 0.1058
2026-01-08 18:20:42,024: t15.2023.11.19 val PER: 0.0958
2026-01-08 18:20:42,026: t15.2023.11.26 val PER: 0.2783
2026-01-08 18:20:42,027: t15.2023.12.03 val PER: 0.2174
2026-01-08 18:20:42,029: t15.2023.12.08 val PER: 0.2237
2026-01-08 18:20:42,031: t15.2023.12.10 val PER: 0.1984
2026-01-08 18:20:42,032: t15.2023.12.17 val PER: 0.2412
2026-01-08 18:20:42,034: t15.2023.12.29 val PER: 0.2533
2026-01-08 18:20:42,036: t15.2024.02.25 val PER: 0.2037
2026-01-08 18:20:42,037: t15.2024.03.08 val PER: 0.3215
2026-01-08 18:20:42,039: t15.2024.03.15 val PER: 0.2971
2026-01-08 18:20:42,040: t15.2024.03.17 val PER: 0.2462
2026-01-08 18:20:42,042: t15.2024.05.10 val PER: 0.2823
2026-01-08 18:20:42,043: t15.2024.06.14 val PER: 0.2587
2026-01-08 18:20:42,045: t15.2024.07.19 val PER: 0.3579
2026-01-08 18:20:42,047: t15.2024.07.21 val PER: 0.1945
2026-01-08 18:20:42,048: t15.2024.07.28 val PER: 0.2294
2026-01-08 18:20:42,050: t15.2025.01.10 val PER: 0.4380
2026-01-08 18:20:42,051: t15.2025.01.12 val PER: 0.2779
2026-01-08 18:20:42,053: t15.2025.03.14 val PER: 0.4172
2026-01-08 18:20:42,055: t15.2025.03.16 val PER: 0.3089
2026-01-08 18:20:42,056: t15.2025.03.30 val PER: 0.4529
2026-01-08 18:20:42,058: t15.2025.04.13 val PER: 0.3153
2026-01-08 18:20:42,060: New best val WER(5gram) 24.58% --> 22.16%
2026-01-08 18:20:42,249: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_4500
2026-01-08 18:20:51,470: Train batch 4600: loss: 21.77 grad norm: 60.08 time: 0.064
2026-01-08 18:21:10,041: Train batch 4800: loss: 15.15 grad norm: 53.32 time: 0.066
2026-01-08 18:21:28,557: Train batch 5000: loss: 34.65 grad norm: 82.26 time: 0.067
2026-01-08 18:21:28,560: Running test after training batch: 5000
2026-01-08 18:21:28,698: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:21:33,717: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:21:33,772: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 18:21:46,847: Val batch 5000: PER (avg): 0.2348 CTC Loss (avg): 23.2768 WER(5gram): 19.30% (n=256) time: 18.283
2026-01-08 18:21:46,849: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-08 18:21:46,852: t15.2023.08.13 val PER: 0.2058
2026-01-08 18:21:46,854: t15.2023.08.18 val PER: 0.1861
2026-01-08 18:21:46,856: t15.2023.08.20 val PER: 0.1771
2026-01-08 18:21:46,858: t15.2023.08.25 val PER: 0.1446
2026-01-08 18:21:46,859: t15.2023.08.27 val PER: 0.2556
2026-01-08 18:21:46,861: t15.2023.09.01 val PER: 0.1429
2026-01-08 18:21:46,863: t15.2023.09.03 val PER: 0.2304
2026-01-08 18:21:46,865: t15.2023.09.24 val PER: 0.1869
2026-01-08 18:21:46,867: t15.2023.09.29 val PER: 0.1870
2026-01-08 18:21:46,868: t15.2023.10.01 val PER: 0.2312
2026-01-08 18:21:46,870: t15.2023.10.06 val PER: 0.1679
2026-01-08 18:21:46,872: t15.2023.10.08 val PER: 0.3099
2026-01-08 18:21:46,876: t15.2023.10.13 val PER: 0.2870
2026-01-08 18:21:46,878: t15.2023.10.15 val PER: 0.2142
2026-01-08 18:21:46,880: t15.2023.10.20 val PER: 0.2416
2026-01-08 18:21:46,881: t15.2023.10.22 val PER: 0.1759
2026-01-08 18:21:46,883: t15.2023.11.03 val PER: 0.2442
2026-01-08 18:21:46,885: t15.2023.11.04 val PER: 0.0478
2026-01-08 18:21:46,886: t15.2023.11.17 val PER: 0.0995
2026-01-08 18:21:46,888: t15.2023.11.19 val PER: 0.0918
2026-01-08 18:21:46,890: t15.2023.11.26 val PER: 0.2514
2026-01-08 18:21:46,891: t15.2023.12.03 val PER: 0.2038
2026-01-08 18:21:46,893: t15.2023.12.08 val PER: 0.2177
2026-01-08 18:21:46,895: t15.2023.12.10 val PER: 0.1721
2026-01-08 18:21:46,897: t15.2023.12.17 val PER: 0.2401
2026-01-08 18:21:46,898: t15.2023.12.29 val PER: 0.2382
2026-01-08 18:21:46,900: t15.2024.02.25 val PER: 0.1798
2026-01-08 18:21:46,901: t15.2024.03.08 val PER: 0.3158
2026-01-08 18:21:46,903: t15.2024.03.15 val PER: 0.2877
2026-01-08 18:21:46,905: t15.2024.03.17 val PER: 0.2371
2026-01-08 18:21:46,906: t15.2024.05.10 val PER: 0.2452
2026-01-08 18:21:46,908: t15.2024.06.14 val PER: 0.2461
2026-01-08 18:21:46,910: t15.2024.07.19 val PER: 0.3527
2026-01-08 18:21:46,911: t15.2024.07.21 val PER: 0.1862
2026-01-08 18:21:46,913: t15.2024.07.28 val PER: 0.2243
2026-01-08 18:21:46,914: t15.2025.01.10 val PER: 0.4077
2026-01-08 18:21:46,916: t15.2025.01.12 val PER: 0.2571
2026-01-08 18:21:46,917: t15.2025.03.14 val PER: 0.3994
2026-01-08 18:21:46,919: t15.2025.03.16 val PER: 0.2971
2026-01-08 18:21:46,921: t15.2025.03.30 val PER: 0.4172
2026-01-08 18:21:46,922: t15.2025.04.13 val PER: 0.3039
2026-01-08 18:21:46,924: New best val WER(5gram) 22.16% --> 19.30%
2026-01-08 18:21:47,111: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_5000
2026-01-08 18:22:04,554: Train batch 5200: loss: 18.21 grad norm: 60.45 time: 0.052
2026-01-08 18:22:22,052: Train batch 5400: loss: 18.99 grad norm: 60.49 time: 0.068
2026-01-08 18:22:31,058: Running test after training batch: 5500
2026-01-08 18:22:31,213: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:22:36,351: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 18:22:36,409: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost of
2026-01-08 18:22:49,273: Val batch 5500: PER (avg): 0.2247 CTC Loss (avg): 22.1370 WER(5gram): 20.86% (n=256) time: 18.212
2026-01-08 18:22:49,277: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-08 18:22:49,280: t15.2023.08.13 val PER: 0.1850
2026-01-08 18:22:49,282: t15.2023.08.18 val PER: 0.1760
2026-01-08 18:22:49,284: t15.2023.08.20 val PER: 0.1755
2026-01-08 18:22:49,286: t15.2023.08.25 val PER: 0.1340
2026-01-08 18:22:49,288: t15.2023.08.27 val PER: 0.2588
2026-01-08 18:22:49,297: t15.2023.09.01 val PER: 0.1364
2026-01-08 18:22:49,299: t15.2023.09.03 val PER: 0.2209
2026-01-08 18:22:49,301: t15.2023.09.24 val PER: 0.1808
2026-01-08 18:22:49,303: t15.2023.09.29 val PER: 0.1774
2026-01-08 18:22:49,305: t15.2023.10.01 val PER: 0.2424
2026-01-08 18:22:49,307: t15.2023.10.06 val PER: 0.1518
2026-01-08 18:22:49,308: t15.2023.10.08 val PER: 0.3058
2026-01-08 18:22:49,310: t15.2023.10.13 val PER: 0.2870
2026-01-08 18:22:49,312: t15.2023.10.15 val PER: 0.2149
2026-01-08 18:22:49,314: t15.2023.10.20 val PER: 0.2181
2026-01-08 18:22:49,316: t15.2023.10.22 val PER: 0.1737
2026-01-08 18:22:49,317: t15.2023.11.03 val PER: 0.2374
2026-01-08 18:22:49,319: t15.2023.11.04 val PER: 0.0512
2026-01-08 18:22:49,321: t15.2023.11.17 val PER: 0.0871
2026-01-08 18:22:49,323: t15.2023.11.19 val PER: 0.0719
2026-01-08 18:22:49,325: t15.2023.11.26 val PER: 0.2413
2026-01-08 18:22:49,327: t15.2023.12.03 val PER: 0.1870
2026-01-08 18:22:49,329: t15.2023.12.08 val PER: 0.2097
2026-01-08 18:22:49,330: t15.2023.12.10 val PER: 0.1669
2026-01-08 18:22:49,332: t15.2023.12.17 val PER: 0.2225
2026-01-08 18:22:49,334: t15.2023.12.29 val PER: 0.2286
2026-01-08 18:22:49,335: t15.2024.02.25 val PER: 0.1854
2026-01-08 18:22:49,337: t15.2024.03.08 val PER: 0.3044
2026-01-08 18:22:49,339: t15.2024.03.15 val PER: 0.2639
2026-01-08 18:22:49,340: t15.2024.03.17 val PER: 0.2266
2026-01-08 18:22:49,342: t15.2024.05.10 val PER: 0.2244
2026-01-08 18:22:49,345: t15.2024.06.14 val PER: 0.2334
2026-01-08 18:22:49,346: t15.2024.07.19 val PER: 0.3256
2026-01-08 18:22:49,348: t15.2024.07.21 val PER: 0.1759
2026-01-08 18:22:49,349: t15.2024.07.28 val PER: 0.2184
2026-01-08 18:22:49,351: t15.2025.01.10 val PER: 0.3967
2026-01-08 18:22:49,352: t15.2025.01.12 val PER: 0.2410
2026-01-08 18:22:49,354: t15.2025.03.14 val PER: 0.3787
2026-01-08 18:22:49,355: t15.2025.03.16 val PER: 0.2853
2026-01-08 18:22:49,357: t15.2025.03.30 val PER: 0.3805
2026-01-08 18:22:49,359: t15.2025.04.13 val PER: 0.2953
2026-01-08 18:22:49,503: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_5500
2026-01-08 18:22:58,066: Train batch 5600: loss: 21.38 grad norm: 64.42 time: 0.064
2026-01-08 18:23:15,203: Train batch 5800: loss: 14.30 grad norm: 56.06 time: 0.086
2026-01-08 18:23:32,286: Train batch 6000: loss: 15.62 grad norm: 61.82 time: 0.049
2026-01-08 18:23:32,288: Running test after training batch: 6000
2026-01-08 18:23:32,396: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:23:37,688: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:23:37,756: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 18:23:50,723: Val batch 6000: PER (avg): 0.2191 CTC Loss (avg): 21.6413 WER(5gram): 21.77% (n=256) time: 18.432
2026-01-08 18:23:50,726: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 18:23:50,728: t15.2023.08.13 val PER: 0.1830
2026-01-08 18:23:50,730: t15.2023.08.18 val PER: 0.1777
2026-01-08 18:23:50,732: t15.2023.08.20 val PER: 0.1747
2026-01-08 18:23:50,734: t15.2023.08.25 val PER: 0.1355
2026-01-08 18:23:50,735: t15.2023.08.27 val PER: 0.2476
2026-01-08 18:23:50,737: t15.2023.09.01 val PER: 0.1323
2026-01-08 18:23:50,739: t15.2023.09.03 val PER: 0.2352
2026-01-08 18:23:50,740: t15.2023.09.24 val PER: 0.1735
2026-01-08 18:23:50,742: t15.2023.09.29 val PER: 0.1729
2026-01-08 18:23:50,744: t15.2023.10.01 val PER: 0.2338
2026-01-08 18:23:50,746: t15.2023.10.06 val PER: 0.1378
2026-01-08 18:23:50,747: t15.2023.10.08 val PER: 0.3072
2026-01-08 18:23:50,750: t15.2023.10.13 val PER: 0.2808
2026-01-08 18:23:50,752: t15.2023.10.15 val PER: 0.2221
2026-01-08 18:23:50,753: t15.2023.10.20 val PER: 0.2282
2026-01-08 18:23:50,755: t15.2023.10.22 val PER: 0.1737
2026-01-08 18:23:50,757: t15.2023.11.03 val PER: 0.2320
2026-01-08 18:23:50,758: t15.2023.11.04 val PER: 0.0512
2026-01-08 18:23:50,760: t15.2023.11.17 val PER: 0.0809
2026-01-08 18:23:50,761: t15.2023.11.19 val PER: 0.0818
2026-01-08 18:23:50,763: t15.2023.11.26 val PER: 0.2377
2026-01-08 18:23:50,765: t15.2023.12.03 val PER: 0.1838
2026-01-08 18:23:50,766: t15.2023.12.08 val PER: 0.1824
2026-01-08 18:23:50,768: t15.2023.12.10 val PER: 0.1577
2026-01-08 18:23:50,770: t15.2023.12.17 val PER: 0.1985
2026-01-08 18:23:50,772: t15.2023.12.29 val PER: 0.2196
2026-01-08 18:23:50,773: t15.2024.02.25 val PER: 0.1629
2026-01-08 18:23:50,775: t15.2024.03.08 val PER: 0.2817
2026-01-08 18:23:50,776: t15.2024.03.15 val PER: 0.2658
2026-01-08 18:23:50,778: t15.2024.03.17 val PER: 0.2225
2026-01-08 18:23:50,779: t15.2024.05.10 val PER: 0.2333
2026-01-08 18:23:50,780: t15.2024.06.14 val PER: 0.2334
2026-01-08 18:23:50,782: t15.2024.07.19 val PER: 0.3276
2026-01-08 18:23:50,783: t15.2024.07.21 val PER: 0.1683
2026-01-08 18:23:50,785: t15.2024.07.28 val PER: 0.2081
2026-01-08 18:23:50,786: t15.2025.01.10 val PER: 0.3939
2026-01-08 18:23:50,787: t15.2025.01.12 val PER: 0.2209
2026-01-08 18:23:50,789: t15.2025.03.14 val PER: 0.3772
2026-01-08 18:23:50,790: t15.2025.03.16 val PER: 0.2762
2026-01-08 18:23:50,792: t15.2025.03.30 val PER: 0.3759
2026-01-08 18:23:50,793: t15.2025.04.13 val PER: 0.2810
2026-01-08 18:23:50,931: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_6000
2026-01-08 18:24:08,066: Train batch 6200: loss: 18.25 grad norm: 57.36 time: 0.070
2026-01-08 18:24:25,055: Train batch 6400: loss: 21.06 grad norm: 64.81 time: 0.063
2026-01-08 18:24:33,725: Running test after training batch: 6500
2026-01-08 18:24:33,830: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:24:39,071: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:24:39,132: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 18:24:50,973: Val batch 6500: PER (avg): 0.2112 CTC Loss (avg): 20.6617 WER(5gram): 17.47% (n=256) time: 17.245
2026-01-08 18:24:50,975: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-08 18:24:50,979: t15.2023.08.13 val PER: 0.1819
2026-01-08 18:24:50,981: t15.2023.08.18 val PER: 0.1576
2026-01-08 18:24:50,982: t15.2023.08.20 val PER: 0.1581
2026-01-08 18:24:50,984: t15.2023.08.25 val PER: 0.1190
2026-01-08 18:24:50,987: t15.2023.08.27 val PER: 0.2508
2026-01-08 18:24:50,989: t15.2023.09.01 val PER: 0.1274
2026-01-08 18:24:50,990: t15.2023.09.03 val PER: 0.2114
2026-01-08 18:24:50,992: t15.2023.09.24 val PER: 0.1735
2026-01-08 18:24:50,994: t15.2023.09.29 val PER: 0.1749
2026-01-08 18:24:50,996: t15.2023.10.01 val PER: 0.2199
2026-01-08 18:24:50,997: t15.2023.10.06 val PER: 0.1389
2026-01-08 18:24:50,999: t15.2023.10.08 val PER: 0.2923
2026-01-08 18:24:51,001: t15.2023.10.13 val PER: 0.2708
2026-01-08 18:24:51,003: t15.2023.10.15 val PER: 0.2129
2026-01-08 18:24:51,005: t15.2023.10.20 val PER: 0.2416
2026-01-08 18:24:51,007: t15.2023.10.22 val PER: 0.1670
2026-01-08 18:24:51,009: t15.2023.11.03 val PER: 0.2286
2026-01-08 18:24:51,010: t15.2023.11.04 val PER: 0.0546
2026-01-08 18:24:51,012: t15.2023.11.17 val PER: 0.0638
2026-01-08 18:24:51,014: t15.2023.11.19 val PER: 0.0739
2026-01-08 18:24:51,016: t15.2023.11.26 val PER: 0.2210
2026-01-08 18:24:51,017: t15.2023.12.03 val PER: 0.1702
2026-01-08 18:24:51,019: t15.2023.12.08 val PER: 0.1751
2026-01-08 18:24:51,021: t15.2023.12.10 val PER: 0.1551
2026-01-08 18:24:51,022: t15.2023.12.17 val PER: 0.1902
2026-01-08 18:24:51,024: t15.2023.12.29 val PER: 0.2196
2026-01-08 18:24:51,026: t15.2024.02.25 val PER: 0.1784
2026-01-08 18:24:51,027: t15.2024.03.08 val PER: 0.3144
2026-01-08 18:24:51,029: t15.2024.03.15 val PER: 0.2564
2026-01-08 18:24:51,031: t15.2024.03.17 val PER: 0.1980
2026-01-08 18:24:51,032: t15.2024.05.10 val PER: 0.2273
2026-01-08 18:24:51,034: t15.2024.06.14 val PER: 0.2192
2026-01-08 18:24:51,035: t15.2024.07.19 val PER: 0.3098
2026-01-08 18:24:51,037: t15.2024.07.21 val PER: 0.1655
2026-01-08 18:24:51,038: t15.2024.07.28 val PER: 0.2059
2026-01-08 18:24:51,040: t15.2025.01.10 val PER: 0.3857
2026-01-08 18:24:51,041: t15.2025.01.12 val PER: 0.2063
2026-01-08 18:24:51,043: t15.2025.03.14 val PER: 0.3802
2026-01-08 18:24:51,045: t15.2025.03.16 val PER: 0.2526
2026-01-08 18:24:51,046: t15.2025.03.30 val PER: 0.3678
2026-01-08 18:24:51,048: t15.2025.04.13 val PER: 0.2810
2026-01-08 18:24:51,050: New best val WER(5gram) 19.30% --> 17.47%
2026-01-08 18:24:51,235: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_6500
2026-01-08 18:25:00,074: Train batch 6600: loss: 13.40 grad norm: 54.00 time: 0.045
2026-01-08 18:25:17,833: Train batch 6800: loss: 16.28 grad norm: 56.19 time: 0.049
2026-01-08 18:25:35,815: Train batch 7000: loss: 18.59 grad norm: 66.51 time: 0.061
2026-01-08 18:25:35,818: Running test after training batch: 7000
2026-01-08 18:25:35,969: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:25:41,173: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:25:41,215: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost and
2026-01-08 18:25:52,733: Val batch 7000: PER (avg): 0.2016 CTC Loss (avg): 20.1397 WER(5gram): 17.99% (n=256) time: 16.912
2026-01-08 18:25:52,736: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-08 18:25:52,738: t15.2023.08.13 val PER: 0.1736
2026-01-08 18:25:52,740: t15.2023.08.18 val PER: 0.1551
2026-01-08 18:25:52,742: t15.2023.08.20 val PER: 0.1644
2026-01-08 18:25:52,744: t15.2023.08.25 val PER: 0.1145
2026-01-08 18:25:52,745: t15.2023.08.27 val PER: 0.2395
2026-01-08 18:25:52,747: t15.2023.09.01 val PER: 0.1136
2026-01-08 18:25:52,749: t15.2023.09.03 val PER: 0.2019
2026-01-08 18:25:52,751: t15.2023.09.24 val PER: 0.1614
2026-01-08 18:25:52,753: t15.2023.09.29 val PER: 0.1698
2026-01-08 18:25:52,754: t15.2023.10.01 val PER: 0.2166
2026-01-08 18:25:52,756: t15.2023.10.06 val PER: 0.1184
2026-01-08 18:25:52,757: t15.2023.10.08 val PER: 0.2747
2026-01-08 18:25:52,759: t15.2023.10.13 val PER: 0.2514
2026-01-08 18:25:52,761: t15.2023.10.15 val PER: 0.2004
2026-01-08 18:25:52,762: t15.2023.10.20 val PER: 0.2148
2026-01-08 18:25:52,764: t15.2023.10.22 val PER: 0.1492
2026-01-08 18:25:52,765: t15.2023.11.03 val PER: 0.2198
2026-01-08 18:25:52,767: t15.2023.11.04 val PER: 0.0546
2026-01-08 18:25:52,768: t15.2023.11.17 val PER: 0.0607
2026-01-08 18:25:52,770: t15.2023.11.19 val PER: 0.0559
2026-01-08 18:25:52,771: t15.2023.11.26 val PER: 0.2036
2026-01-08 18:25:52,773: t15.2023.12.03 val PER: 0.1712
2026-01-08 18:25:52,774: t15.2023.12.08 val PER: 0.1638
2026-01-08 18:25:52,776: t15.2023.12.10 val PER: 0.1353
2026-01-08 18:25:52,777: t15.2023.12.17 val PER: 0.1881
2026-01-08 18:25:52,779: t15.2023.12.29 val PER: 0.1970
2026-01-08 18:25:52,780: t15.2024.02.25 val PER: 0.1728
2026-01-08 18:25:52,782: t15.2024.03.08 val PER: 0.2774
2026-01-08 18:25:52,783: t15.2024.03.15 val PER: 0.2477
2026-01-08 18:25:52,785: t15.2024.03.17 val PER: 0.2099
2026-01-08 18:25:52,787: t15.2024.05.10 val PER: 0.2036
2026-01-08 18:25:52,788: t15.2024.06.14 val PER: 0.2114
2026-01-08 18:25:52,790: t15.2024.07.19 val PER: 0.3098
2026-01-08 18:25:52,791: t15.2024.07.21 val PER: 0.1469
2026-01-08 18:25:52,793: t15.2024.07.28 val PER: 0.1897
2026-01-08 18:25:52,794: t15.2025.01.10 val PER: 0.3554
2026-01-08 18:25:52,796: t15.2025.01.12 val PER: 0.2148
2026-01-08 18:25:52,799: t15.2025.03.14 val PER: 0.3713
2026-01-08 18:25:52,800: t15.2025.03.16 val PER: 0.2448
2026-01-08 18:25:52,802: t15.2025.03.30 val PER: 0.3690
2026-01-08 18:25:52,803: t15.2025.04.13 val PER: 0.2782
2026-01-08 18:25:52,946: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_7000
2026-01-08 18:26:10,183: Train batch 7200: loss: 15.89 grad norm: 61.82 time: 0.080
2026-01-08 18:26:27,739: Train batch 7400: loss: 14.96 grad norm: 54.93 time: 0.077
2026-01-08 18:26:36,864: Running test after training batch: 7500
2026-01-08 18:26:37,068: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:26:42,051: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:26:42,098: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost and
2026-01-08 18:26:53,846: Val batch 7500: PER (avg): 0.1934 CTC Loss (avg): 19.3236 WER(5gram): 17.21% (n=256) time: 16.979
2026-01-08 18:26:53,849: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-08 18:26:53,857: t15.2023.08.13 val PER: 0.1590
2026-01-08 18:26:53,859: t15.2023.08.18 val PER: 0.1475
2026-01-08 18:26:53,860: t15.2023.08.20 val PER: 0.1461
2026-01-08 18:26:53,862: t15.2023.08.25 val PER: 0.1160
2026-01-08 18:26:53,864: t15.2023.08.27 val PER: 0.2315
2026-01-08 18:26:53,866: t15.2023.09.01 val PER: 0.1242
2026-01-08 18:26:53,868: t15.2023.09.03 val PER: 0.1912
2026-01-08 18:26:53,870: t15.2023.09.24 val PER: 0.1650
2026-01-08 18:26:53,872: t15.2023.09.29 val PER: 0.1544
2026-01-08 18:26:53,873: t15.2023.10.01 val PER: 0.2094
2026-01-08 18:26:53,875: t15.2023.10.06 val PER: 0.1227
2026-01-08 18:26:53,877: t15.2023.10.08 val PER: 0.2652
2026-01-08 18:26:53,879: t15.2023.10.13 val PER: 0.2452
2026-01-08 18:26:53,880: t15.2023.10.15 val PER: 0.1978
2026-01-08 18:26:53,882: t15.2023.10.20 val PER: 0.2013
2026-01-08 18:26:53,884: t15.2023.10.22 val PER: 0.1548
2026-01-08 18:26:53,886: t15.2023.11.03 val PER: 0.2144
2026-01-08 18:26:53,888: t15.2023.11.04 val PER: 0.0444
2026-01-08 18:26:53,890: t15.2023.11.17 val PER: 0.0560
2026-01-08 18:26:53,892: t15.2023.11.19 val PER: 0.0539
2026-01-08 18:26:53,894: t15.2023.11.26 val PER: 0.1957
2026-01-08 18:26:53,896: t15.2023.12.03 val PER: 0.1555
2026-01-08 18:26:53,898: t15.2023.12.08 val PER: 0.1518
2026-01-08 18:26:53,900: t15.2023.12.10 val PER: 0.1380
2026-01-08 18:26:53,902: t15.2023.12.17 val PER: 0.1757
2026-01-08 18:26:53,904: t15.2023.12.29 val PER: 0.1956
2026-01-08 18:26:53,905: t15.2024.02.25 val PER: 0.1685
2026-01-08 18:26:53,907: t15.2024.03.08 val PER: 0.2774
2026-01-08 18:26:53,909: t15.2024.03.15 val PER: 0.2483
2026-01-08 18:26:53,911: t15.2024.03.17 val PER: 0.1792
2026-01-08 18:26:53,913: t15.2024.05.10 val PER: 0.2051
2026-01-08 18:26:53,914: t15.2024.06.14 val PER: 0.1972
2026-01-08 18:26:53,916: t15.2024.07.19 val PER: 0.2920
2026-01-08 18:26:53,918: t15.2024.07.21 val PER: 0.1379
2026-01-08 18:26:53,919: t15.2024.07.28 val PER: 0.1787
2026-01-08 18:26:53,921: t15.2025.01.10 val PER: 0.3609
2026-01-08 18:26:53,923: t15.2025.01.12 val PER: 0.1925
2026-01-08 18:26:53,924: t15.2025.03.14 val PER: 0.3550
2026-01-08 18:26:53,926: t15.2025.03.16 val PER: 0.2421
2026-01-08 18:26:53,928: t15.2025.03.30 val PER: 0.3494
2026-01-08 18:26:53,930: t15.2025.04.13 val PER: 0.2596
2026-01-08 18:26:53,932: New best val WER(5gram) 17.47% --> 17.21%
2026-01-08 18:26:54,118: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_7500
2026-01-08 18:27:03,210: Train batch 7600: loss: 16.34 grad norm: 57.38 time: 0.071
2026-01-08 18:27:21,601: Train batch 7800: loss: 15.94 grad norm: 61.15 time: 0.061
2026-01-08 18:27:40,170: Train batch 8000: loss: 12.89 grad norm: 54.12 time: 0.076
2026-01-08 18:27:40,172: Running test after training batch: 8000
2026-01-08 18:27:40,282: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:27:45,283: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:27:45,343: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost and
2026-01-08 18:27:56,878: Val batch 8000: PER (avg): 0.1909 CTC Loss (avg): 18.6955 WER(5gram): 19.17% (n=256) time: 16.703
2026-01-08 18:27:56,881: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-08 18:27:56,884: t15.2023.08.13 val PER: 0.1570
2026-01-08 18:27:56,886: t15.2023.08.18 val PER: 0.1375
2026-01-08 18:27:56,888: t15.2023.08.20 val PER: 0.1454
2026-01-08 18:27:56,890: t15.2023.08.25 val PER: 0.1190
2026-01-08 18:27:56,892: t15.2023.08.27 val PER: 0.2058
2026-01-08 18:27:56,894: t15.2023.09.01 val PER: 0.1209
2026-01-08 18:27:56,896: t15.2023.09.03 val PER: 0.1948
2026-01-08 18:27:56,898: t15.2023.09.24 val PER: 0.1602
2026-01-08 18:27:56,901: t15.2023.09.29 val PER: 0.1512
2026-01-08 18:27:56,903: t15.2023.10.01 val PER: 0.2081
2026-01-08 18:27:56,905: t15.2023.10.06 val PER: 0.1227
2026-01-08 18:27:56,907: t15.2023.10.08 val PER: 0.2666
2026-01-08 18:27:56,909: t15.2023.10.13 val PER: 0.2436
2026-01-08 18:27:56,911: t15.2023.10.15 val PER: 0.1991
2026-01-08 18:27:56,913: t15.2023.10.20 val PER: 0.2181
2026-01-08 18:27:56,914: t15.2023.10.22 val PER: 0.1592
2026-01-08 18:27:56,916: t15.2023.11.03 val PER: 0.2042
2026-01-08 18:27:56,918: t15.2023.11.04 val PER: 0.0410
2026-01-08 18:27:56,920: t15.2023.11.17 val PER: 0.0669
2026-01-08 18:27:56,922: t15.2023.11.19 val PER: 0.0659
2026-01-08 18:27:56,925: t15.2023.11.26 val PER: 0.1906
2026-01-08 18:27:56,927: t15.2023.12.03 val PER: 0.1523
2026-01-08 18:27:56,929: t15.2023.12.08 val PER: 0.1525
2026-01-08 18:27:56,931: t15.2023.12.10 val PER: 0.1380
2026-01-08 18:27:56,932: t15.2023.12.17 val PER: 0.1798
2026-01-08 18:27:56,934: t15.2023.12.29 val PER: 0.1819
2026-01-08 18:27:56,936: t15.2024.02.25 val PER: 0.1489
2026-01-08 18:27:56,938: t15.2024.03.08 val PER: 0.2817
2026-01-08 18:27:56,940: t15.2024.03.15 val PER: 0.2420
2026-01-08 18:27:56,941: t15.2024.03.17 val PER: 0.1799
2026-01-08 18:27:56,943: t15.2024.05.10 val PER: 0.1976
2026-01-08 18:27:56,945: t15.2024.06.14 val PER: 0.2066
2026-01-08 18:27:56,947: t15.2024.07.19 val PER: 0.3013
2026-01-08 18:27:56,949: t15.2024.07.21 val PER: 0.1276
2026-01-08 18:27:56,950: t15.2024.07.28 val PER: 0.1632
2026-01-08 18:27:56,952: t15.2025.01.10 val PER: 0.3526
2026-01-08 18:27:56,954: t15.2025.01.12 val PER: 0.1863
2026-01-08 18:27:56,956: t15.2025.03.14 val PER: 0.3476
2026-01-08 18:27:56,957: t15.2025.03.16 val PER: 0.2356
2026-01-08 18:27:56,960: t15.2025.03.30 val PER: 0.3609
2026-01-08 18:27:56,962: t15.2025.04.13 val PER: 0.2725
2026-01-08 18:27:57,106: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_8000
2026-01-08 18:28:14,616: Train batch 8200: loss: 10.63 grad norm: 49.84 time: 0.054
2026-01-08 18:28:32,020: Train batch 8400: loss: 10.53 grad norm: 44.49 time: 0.065
2026-01-08 18:28:40,937: Running test after training batch: 8500
2026-01-08 18:28:41,056: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:28:46,094: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:28:46,148: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the west end
2026-01-08 18:28:57,522: Val batch 8500: PER (avg): 0.1874 CTC Loss (avg): 18.3336 WER(5gram): 17.54% (n=256) time: 16.583
2026-01-08 18:28:57,525: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-08 18:28:57,528: t15.2023.08.13 val PER: 0.1393
2026-01-08 18:28:57,530: t15.2023.08.18 val PER: 0.1534
2026-01-08 18:28:57,532: t15.2023.08.20 val PER: 0.1454
2026-01-08 18:28:57,534: t15.2023.08.25 val PER: 0.1190
2026-01-08 18:28:57,535: t15.2023.08.27 val PER: 0.2283
2026-01-08 18:28:57,537: t15.2023.09.01 val PER: 0.1128
2026-01-08 18:28:57,539: t15.2023.09.03 val PER: 0.1912
2026-01-08 18:28:57,541: t15.2023.09.24 val PER: 0.1578
2026-01-08 18:28:57,542: t15.2023.09.29 val PER: 0.1525
2026-01-08 18:28:57,544: t15.2023.10.01 val PER: 0.1988
2026-01-08 18:28:57,546: t15.2023.10.06 val PER: 0.1098
2026-01-08 18:28:57,548: t15.2023.10.08 val PER: 0.2679
2026-01-08 18:28:57,551: t15.2023.10.13 val PER: 0.2521
2026-01-08 18:28:57,553: t15.2023.10.15 val PER: 0.1892
2026-01-08 18:28:57,555: t15.2023.10.20 val PER: 0.1913
2026-01-08 18:28:57,556: t15.2023.10.22 val PER: 0.1559
2026-01-08 18:28:57,558: t15.2023.11.03 val PER: 0.1974
2026-01-08 18:28:57,560: t15.2023.11.04 val PER: 0.0410
2026-01-08 18:28:57,562: t15.2023.11.17 val PER: 0.0607
2026-01-08 18:28:57,563: t15.2023.11.19 val PER: 0.0539
2026-01-08 18:28:57,565: t15.2023.11.26 val PER: 0.1862
2026-01-08 18:28:57,567: t15.2023.12.03 val PER: 0.1502
2026-01-08 18:28:57,568: t15.2023.12.08 val PER: 0.1531
2026-01-08 18:28:57,570: t15.2023.12.10 val PER: 0.1170
2026-01-08 18:28:57,572: t15.2023.12.17 val PER: 0.1778
2026-01-08 18:28:57,574: t15.2023.12.29 val PER: 0.1853
2026-01-08 18:28:57,575: t15.2024.02.25 val PER: 0.1306
2026-01-08 18:28:57,577: t15.2024.03.08 val PER: 0.2731
2026-01-08 18:28:57,579: t15.2024.03.15 val PER: 0.2383
2026-01-08 18:28:57,581: t15.2024.03.17 val PER: 0.1750
2026-01-08 18:28:57,583: t15.2024.05.10 val PER: 0.1857
2026-01-08 18:28:57,584: t15.2024.06.14 val PER: 0.1956
2026-01-08 18:28:57,586: t15.2024.07.19 val PER: 0.2874
2026-01-08 18:28:57,588: t15.2024.07.21 val PER: 0.1290
2026-01-08 18:28:57,589: t15.2024.07.28 val PER: 0.1632
2026-01-08 18:28:57,591: t15.2025.01.10 val PER: 0.3499
2026-01-08 18:28:57,593: t15.2025.01.12 val PER: 0.1963
2026-01-08 18:28:57,595: t15.2025.03.14 val PER: 0.3565
2026-01-08 18:28:57,596: t15.2025.03.16 val PER: 0.2356
2026-01-08 18:28:57,603: t15.2025.03.30 val PER: 0.3506
2026-01-08 18:28:57,607: t15.2025.04.13 val PER: 0.2582
2026-01-08 18:28:57,747: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_8500
2026-01-08 18:29:06,292: Train batch 8600: loss: 16.97 grad norm: 59.38 time: 0.056
2026-01-08 18:29:23,183: Train batch 8800: loss: 16.23 grad norm: 58.19 time: 0.063
2026-01-08 18:29:40,380: Train batch 9000: loss: 16.83 grad norm: 63.29 time: 0.072
2026-01-08 18:29:40,382: Running test after training batch: 9000
2026-01-08 18:29:40,494: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:29:45,457: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:29:45,502: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost in
2026-01-08 18:29:57,099: Val batch 9000: PER (avg): 0.1814 CTC Loss (avg): 18.0031 WER(5gram): 17.99% (n=256) time: 16.713
2026-01-08 18:29:57,101: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=13
2026-01-08 18:29:57,109: t15.2023.08.13 val PER: 0.1445
2026-01-08 18:29:57,112: t15.2023.08.18 val PER: 0.1417
2026-01-08 18:29:57,113: t15.2023.08.20 val PER: 0.1334
2026-01-08 18:29:57,115: t15.2023.08.25 val PER: 0.1039
2026-01-08 18:29:57,117: t15.2023.08.27 val PER: 0.2219
2026-01-08 18:29:57,118: t15.2023.09.01 val PER: 0.1047
2026-01-08 18:29:57,120: t15.2023.09.03 val PER: 0.1734
2026-01-08 18:29:57,122: t15.2023.09.24 val PER: 0.1541
2026-01-08 18:29:57,123: t15.2023.09.29 val PER: 0.1487
2026-01-08 18:29:57,125: t15.2023.10.01 val PER: 0.1995
2026-01-08 18:29:57,128: t15.2023.10.06 val PER: 0.1055
2026-01-08 18:29:57,130: t15.2023.10.08 val PER: 0.2612
2026-01-08 18:29:57,131: t15.2023.10.13 val PER: 0.2444
2026-01-08 18:29:57,133: t15.2023.10.15 val PER: 0.1931
2026-01-08 18:29:57,135: t15.2023.10.20 val PER: 0.2047
2026-01-08 18:29:57,136: t15.2023.10.22 val PER: 0.1459
2026-01-08 18:29:57,138: t15.2023.11.03 val PER: 0.2028
2026-01-08 18:29:57,139: t15.2023.11.04 val PER: 0.0375
2026-01-08 18:29:57,141: t15.2023.11.17 val PER: 0.0560
2026-01-08 18:29:57,143: t15.2023.11.19 val PER: 0.0479
2026-01-08 18:29:57,144: t15.2023.11.26 val PER: 0.1746
2026-01-08 18:29:57,147: t15.2023.12.03 val PER: 0.1408
2026-01-08 18:29:57,149: t15.2023.12.08 val PER: 0.1398
2026-01-08 18:29:57,150: t15.2023.12.10 val PER: 0.1156
2026-01-08 18:29:57,152: t15.2023.12.17 val PER: 0.1726
2026-01-08 18:29:57,153: t15.2023.12.29 val PER: 0.1778
2026-01-08 18:29:57,155: t15.2024.02.25 val PER: 0.1404
2026-01-08 18:29:57,156: t15.2024.03.08 val PER: 0.2617
2026-01-08 18:29:57,158: t15.2024.03.15 val PER: 0.2295
2026-01-08 18:29:57,159: t15.2024.03.17 val PER: 0.1729
2026-01-08 18:29:57,161: t15.2024.05.10 val PER: 0.1842
2026-01-08 18:29:57,162: t15.2024.06.14 val PER: 0.1814
2026-01-08 18:29:57,164: t15.2024.07.19 val PER: 0.2821
2026-01-08 18:29:57,166: t15.2024.07.21 val PER: 0.1159
2026-01-08 18:29:57,167: t15.2024.07.28 val PER: 0.1699
2026-01-08 18:29:57,169: t15.2025.01.10 val PER: 0.3512
2026-01-08 18:29:57,170: t15.2025.01.12 val PER: 0.1778
2026-01-08 18:29:57,172: t15.2025.03.14 val PER: 0.3432
2026-01-08 18:29:57,173: t15.2025.03.16 val PER: 0.2277
2026-01-08 18:29:57,175: t15.2025.03.30 val PER: 0.3345
2026-01-08 18:29:57,176: t15.2025.04.13 val PER: 0.2582
2026-01-08 18:29:57,316: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_9000
2026-01-08 18:30:14,437: Train batch 9200: loss: 11.24 grad norm: 47.46 time: 0.057
2026-01-08 18:30:32,142: Train batch 9400: loss: 8.48 grad norm: 46.31 time: 0.070
2026-01-08 18:30:40,737: Running test after training batch: 9500
2026-01-08 18:30:40,883: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:30:45,882: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:30:45,931: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost in
2026-01-08 18:30:57,304: Val batch 9500: PER (avg): 0.1790 CTC Loss (avg): 17.7954 WER(5gram): 17.28% (n=256) time: 16.565
2026-01-08 18:30:57,307: WER lens: avg_true_words=5.99 avg_pred_words=6.15 max_pred_words=12
2026-01-08 18:30:57,310: t15.2023.08.13 val PER: 0.1362
2026-01-08 18:30:57,313: t15.2023.08.18 val PER: 0.1358
2026-01-08 18:30:57,315: t15.2023.08.20 val PER: 0.1430
2026-01-08 18:30:57,320: t15.2023.08.25 val PER: 0.1160
2026-01-08 18:30:57,322: t15.2023.08.27 val PER: 0.2090
2026-01-08 18:30:57,324: t15.2023.09.01 val PER: 0.1047
2026-01-08 18:30:57,325: t15.2023.09.03 val PER: 0.1734
2026-01-08 18:30:57,327: t15.2023.09.24 val PER: 0.1432
2026-01-08 18:30:57,329: t15.2023.09.29 val PER: 0.1487
2026-01-08 18:30:57,330: t15.2023.10.01 val PER: 0.1922
2026-01-08 18:30:57,332: t15.2023.10.06 val PER: 0.1098
2026-01-08 18:30:57,334: t15.2023.10.08 val PER: 0.2801
2026-01-08 18:30:57,336: t15.2023.10.13 val PER: 0.2374
2026-01-08 18:30:57,337: t15.2023.10.15 val PER: 0.1852
2026-01-08 18:30:57,339: t15.2023.10.20 val PER: 0.2148
2026-01-08 18:30:57,343: t15.2023.10.22 val PER: 0.1370
2026-01-08 18:30:57,345: t15.2023.11.03 val PER: 0.1920
2026-01-08 18:30:57,347: t15.2023.11.04 val PER: 0.0375
2026-01-08 18:30:57,349: t15.2023.11.17 val PER: 0.0529
2026-01-08 18:30:57,350: t15.2023.11.19 val PER: 0.0419
2026-01-08 18:30:57,352: t15.2023.11.26 val PER: 0.1638
2026-01-08 18:30:57,354: t15.2023.12.03 val PER: 0.1345
2026-01-08 18:30:57,355: t15.2023.12.08 val PER: 0.1398
2026-01-08 18:30:57,357: t15.2023.12.10 val PER: 0.1143
2026-01-08 18:30:57,358: t15.2023.12.17 val PER: 0.1840
2026-01-08 18:30:57,360: t15.2023.12.29 val PER: 0.1640
2026-01-08 18:30:57,361: t15.2024.02.25 val PER: 0.1306
2026-01-08 18:30:57,363: t15.2024.03.08 val PER: 0.2703
2026-01-08 18:30:57,364: t15.2024.03.15 val PER: 0.2358
2026-01-08 18:30:57,366: t15.2024.03.17 val PER: 0.1764
2026-01-08 18:30:57,367: t15.2024.05.10 val PER: 0.1961
2026-01-08 18:30:57,369: t15.2024.06.14 val PER: 0.1861
2026-01-08 18:30:57,371: t15.2024.07.19 val PER: 0.2749
2026-01-08 18:30:57,372: t15.2024.07.21 val PER: 0.1172
2026-01-08 18:30:57,374: t15.2024.07.28 val PER: 0.1596
2026-01-08 18:30:57,375: t15.2025.01.10 val PER: 0.3168
2026-01-08 18:30:57,377: t15.2025.01.12 val PER: 0.1801
2026-01-08 18:30:57,378: t15.2025.03.14 val PER: 0.3580
2026-01-08 18:30:57,380: t15.2025.03.16 val PER: 0.2251
2026-01-08 18:30:57,381: t15.2025.03.30 val PER: 0.3345
2026-01-08 18:30:57,383: t15.2025.04.13 val PER: 0.2525
2026-01-08 18:30:57,521: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_9500
2026-01-08 18:31:06,073: Train batch 9600: loss: 9.59 grad norm: 50.50 time: 0.075
2026-01-08 18:31:23,655: Train batch 9800: loss: 13.11 grad norm: 55.67 time: 0.065
2026-01-08 18:31:42,180: Train batch 10000: loss: 6.28 grad norm: 37.94 time: 0.065
2026-01-08 18:31:42,182: Running test after training batch: 10000
2026-01-08 18:31:42,305: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:31:47,253: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:31:47,306: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost in
2026-01-08 18:31:58,666: Val batch 10000: PER (avg): 0.1739 CTC Loss (avg): 17.3597 WER(5gram): 15.97% (n=256) time: 16.481
2026-01-08 18:31:58,669: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-08 18:31:58,675: t15.2023.08.13 val PER: 0.1310
2026-01-08 18:31:58,677: t15.2023.08.18 val PER: 0.1366
2026-01-08 18:31:58,679: t15.2023.08.20 val PER: 0.1295
2026-01-08 18:31:58,680: t15.2023.08.25 val PER: 0.1114
2026-01-08 18:31:58,682: t15.2023.08.27 val PER: 0.2042
2026-01-08 18:31:58,684: t15.2023.09.01 val PER: 0.0950
2026-01-08 18:31:58,686: t15.2023.09.03 val PER: 0.1734
2026-01-08 18:31:58,687: t15.2023.09.24 val PER: 0.1420
2026-01-08 18:31:58,690: t15.2023.09.29 val PER: 0.1449
2026-01-08 18:31:58,694: t15.2023.10.01 val PER: 0.1869
2026-01-08 18:31:58,695: t15.2023.10.06 val PER: 0.1195
2026-01-08 18:31:58,697: t15.2023.10.08 val PER: 0.2490
2026-01-08 18:31:58,699: t15.2023.10.13 val PER: 0.2343
2026-01-08 18:31:58,701: t15.2023.10.15 val PER: 0.1780
2026-01-08 18:31:58,703: t15.2023.10.20 val PER: 0.2047
2026-01-08 18:31:58,705: t15.2023.10.22 val PER: 0.1325
2026-01-08 18:31:58,707: t15.2023.11.03 val PER: 0.1913
2026-01-08 18:31:58,709: t15.2023.11.04 val PER: 0.0444
2026-01-08 18:31:58,711: t15.2023.11.17 val PER: 0.0591
2026-01-08 18:31:58,713: t15.2023.11.19 val PER: 0.0559
2026-01-08 18:31:58,715: t15.2023.11.26 val PER: 0.1594
2026-01-08 18:31:58,716: t15.2023.12.03 val PER: 0.1355
2026-01-08 18:31:58,718: t15.2023.12.08 val PER: 0.1332
2026-01-08 18:31:58,720: t15.2023.12.10 val PER: 0.1156
2026-01-08 18:31:58,722: t15.2023.12.17 val PER: 0.1674
2026-01-08 18:31:58,725: t15.2023.12.29 val PER: 0.1585
2026-01-08 18:31:58,728: t15.2024.02.25 val PER: 0.1306
2026-01-08 18:31:58,730: t15.2024.03.08 val PER: 0.2674
2026-01-08 18:31:58,732: t15.2024.03.15 val PER: 0.2233
2026-01-08 18:31:58,734: t15.2024.03.17 val PER: 0.1681
2026-01-08 18:31:58,735: t15.2024.05.10 val PER: 0.1724
2026-01-08 18:31:58,737: t15.2024.06.14 val PER: 0.1798
2026-01-08 18:31:58,739: t15.2024.07.19 val PER: 0.2709
2026-01-08 18:31:58,741: t15.2024.07.21 val PER: 0.1110
2026-01-08 18:31:58,743: t15.2024.07.28 val PER: 0.1544
2026-01-08 18:31:58,745: t15.2025.01.10 val PER: 0.3430
2026-01-08 18:31:58,747: t15.2025.01.12 val PER: 0.1732
2026-01-08 18:31:58,749: t15.2025.03.14 val PER: 0.3447
2026-01-08 18:31:58,750: t15.2025.03.16 val PER: 0.2186
2026-01-08 18:31:58,752: t15.2025.03.30 val PER: 0.3253
2026-01-08 18:31:58,754: t15.2025.04.13 val PER: 0.2439
2026-01-08 18:31:58,757: New best val WER(5gram) 17.21% --> 15.97%
2026-01-08 18:31:58,940: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_10000
2026-01-08 18:32:17,049: Train batch 10200: loss: 6.47 grad norm: 35.00 time: 0.050
2026-01-08 18:32:34,528: Train batch 10400: loss: 10.46 grad norm: 50.58 time: 0.072
2026-01-08 18:32:43,240: Running test after training batch: 10500
2026-01-08 18:32:43,364: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:32:48,307: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:32:48,363: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 18:32:59,597: Val batch 10500: PER (avg): 0.1737 CTC Loss (avg): 17.3046 WER(5gram): 15.97% (n=256) time: 16.354
2026-01-08 18:32:59,599: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-08 18:32:59,603: t15.2023.08.13 val PER: 0.1310
2026-01-08 18:32:59,604: t15.2023.08.18 val PER: 0.1400
2026-01-08 18:32:59,606: t15.2023.08.20 val PER: 0.1430
2026-01-08 18:32:59,608: t15.2023.08.25 val PER: 0.1160
2026-01-08 18:32:59,610: t15.2023.08.27 val PER: 0.2042
2026-01-08 18:32:59,611: t15.2023.09.01 val PER: 0.0998
2026-01-08 18:32:59,613: t15.2023.09.03 val PER: 0.1722
2026-01-08 18:32:59,615: t15.2023.09.24 val PER: 0.1456
2026-01-08 18:32:59,617: t15.2023.09.29 val PER: 0.1500
2026-01-08 18:32:59,618: t15.2023.10.01 val PER: 0.1882
2026-01-08 18:32:59,620: t15.2023.10.06 val PER: 0.1076
2026-01-08 18:32:59,622: t15.2023.10.08 val PER: 0.2490
2026-01-08 18:32:59,624: t15.2023.10.13 val PER: 0.2265
2026-01-08 18:32:59,625: t15.2023.10.15 val PER: 0.1826
2026-01-08 18:32:59,627: t15.2023.10.20 val PER: 0.1812
2026-01-08 18:32:59,629: t15.2023.10.22 val PER: 0.1281
2026-01-08 18:32:59,630: t15.2023.11.03 val PER: 0.1906
2026-01-08 18:32:59,632: t15.2023.11.04 val PER: 0.0444
2026-01-08 18:32:59,633: t15.2023.11.17 val PER: 0.0467
2026-01-08 18:32:59,635: t15.2023.11.19 val PER: 0.0519
2026-01-08 18:32:59,637: t15.2023.11.26 val PER: 0.1486
2026-01-08 18:32:59,638: t15.2023.12.03 val PER: 0.1334
2026-01-08 18:32:59,640: t15.2023.12.08 val PER: 0.1292
2026-01-08 18:32:59,642: t15.2023.12.10 val PER: 0.1130
2026-01-08 18:32:59,644: t15.2023.12.17 val PER: 0.1549
2026-01-08 18:32:59,645: t15.2023.12.29 val PER: 0.1633
2026-01-08 18:32:59,647: t15.2024.02.25 val PER: 0.1419
2026-01-08 18:32:59,649: t15.2024.03.08 val PER: 0.2603
2026-01-08 18:32:59,650: t15.2024.03.15 val PER: 0.2239
2026-01-08 18:32:59,652: t15.2024.03.17 val PER: 0.1688
2026-01-08 18:32:59,654: t15.2024.05.10 val PER: 0.1709
2026-01-08 18:32:59,655: t15.2024.06.14 val PER: 0.1909
2026-01-08 18:32:59,657: t15.2024.07.19 val PER: 0.2775
2026-01-08 18:32:59,659: t15.2024.07.21 val PER: 0.1138
2026-01-08 18:32:59,660: t15.2024.07.28 val PER: 0.1559
2026-01-08 18:32:59,663: t15.2025.01.10 val PER: 0.3375
2026-01-08 18:32:59,665: t15.2025.01.12 val PER: 0.1717
2026-01-08 18:32:59,667: t15.2025.03.14 val PER: 0.3609
2026-01-08 18:32:59,669: t15.2025.03.16 val PER: 0.2081
2026-01-08 18:32:59,671: t15.2025.03.30 val PER: 0.3172
2026-01-08 18:32:59,673: t15.2025.04.13 val PER: 0.2425
2026-01-08 18:32:59,810: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_10500
2026-01-08 18:33:08,489: Train batch 10600: loss: 10.47 grad norm: 54.89 time: 0.073
2026-01-08 18:33:25,545: Train batch 10800: loss: 15.61 grad norm: 62.95 time: 0.065
2026-01-08 18:33:42,516: Train batch 11000: loss: 15.90 grad norm: 59.88 time: 0.058
2026-01-08 18:33:42,518: Running test after training batch: 11000
2026-01-08 18:33:42,659: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:33:47,629: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:33:47,688: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost in
2026-01-08 18:33:58,932: Val batch 11000: PER (avg): 0.1689 CTC Loss (avg): 17.1062 WER(5gram): 16.30% (n=256) time: 16.411
2026-01-08 18:33:58,935: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-08 18:33:58,947: t15.2023.08.13 val PER: 0.1393
2026-01-08 18:33:58,950: t15.2023.08.18 val PER: 0.1282
2026-01-08 18:33:58,952: t15.2023.08.20 val PER: 0.1287
2026-01-08 18:33:58,954: t15.2023.08.25 val PER: 0.1024
2026-01-08 18:33:58,956: t15.2023.08.27 val PER: 0.2042
2026-01-08 18:33:58,958: t15.2023.09.01 val PER: 0.0909
2026-01-08 18:33:58,959: t15.2023.09.03 val PER: 0.1770
2026-01-08 18:33:58,961: t15.2023.09.24 val PER: 0.1432
2026-01-08 18:33:58,963: t15.2023.09.29 val PER: 0.1417
2026-01-08 18:33:58,965: t15.2023.10.01 val PER: 0.1830
2026-01-08 18:33:58,966: t15.2023.10.06 val PER: 0.0915
2026-01-08 18:33:58,968: t15.2023.10.08 val PER: 0.2530
2026-01-08 18:33:58,969: t15.2023.10.13 val PER: 0.2250
2026-01-08 18:33:58,971: t15.2023.10.15 val PER: 0.1753
2026-01-08 18:33:58,972: t15.2023.10.20 val PER: 0.1946
2026-01-08 18:33:58,974: t15.2023.10.22 val PER: 0.1325
2026-01-08 18:33:58,976: t15.2023.11.03 val PER: 0.1920
2026-01-08 18:33:58,977: t15.2023.11.04 val PER: 0.0478
2026-01-08 18:33:58,979: t15.2023.11.17 val PER: 0.0529
2026-01-08 18:33:58,981: t15.2023.11.19 val PER: 0.0419
2026-01-08 18:33:58,982: t15.2023.11.26 val PER: 0.1464
2026-01-08 18:33:58,984: t15.2023.12.03 val PER: 0.1250
2026-01-08 18:33:58,986: t15.2023.12.08 val PER: 0.1252
2026-01-08 18:33:58,988: t15.2023.12.10 val PER: 0.1130
2026-01-08 18:33:58,989: t15.2023.12.17 val PER: 0.1445
2026-01-08 18:33:58,991: t15.2023.12.29 val PER: 0.1592
2026-01-08 18:33:58,993: t15.2024.02.25 val PER: 0.1236
2026-01-08 18:33:58,995: t15.2024.03.08 val PER: 0.2432
2026-01-08 18:33:58,996: t15.2024.03.15 val PER: 0.2183
2026-01-08 18:33:58,998: t15.2024.03.17 val PER: 0.1674
2026-01-08 18:33:59,000: t15.2024.05.10 val PER: 0.1753
2026-01-08 18:33:59,003: t15.2024.06.14 val PER: 0.1735
2026-01-08 18:33:59,004: t15.2024.07.19 val PER: 0.2512
2026-01-08 18:33:59,006: t15.2024.07.21 val PER: 0.1159
2026-01-08 18:33:59,008: t15.2024.07.28 val PER: 0.1610
2026-01-08 18:33:59,010: t15.2025.01.10 val PER: 0.3278
2026-01-08 18:33:59,011: t15.2025.01.12 val PER: 0.1709
2026-01-08 18:33:59,013: t15.2025.03.14 val PER: 0.3565
2026-01-08 18:33:59,015: t15.2025.03.16 val PER: 0.2107
2026-01-08 18:33:59,017: t15.2025.03.30 val PER: 0.3184
2026-01-08 18:33:59,018: t15.2025.04.13 val PER: 0.2354
2026-01-08 18:33:59,160: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_11000
2026-01-08 18:34:17,424: Train batch 11200: loss: 12.01 grad norm: 56.85 time: 0.072
2026-01-08 18:34:34,742: Train batch 11400: loss: 10.56 grad norm: 54.35 time: 0.057
2026-01-08 18:34:43,523: Running test after training batch: 11500
2026-01-08 18:34:43,632: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:34:48,659: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:34:48,718: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost said
2026-01-08 18:35:00,266: Val batch 11500: PER (avg): 0.1654 CTC Loss (avg): 16.8192 WER(5gram): 15.65% (n=256) time: 16.741
2026-01-08 18:35:00,268: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-08 18:35:00,271: t15.2023.08.13 val PER: 0.1310
2026-01-08 18:35:00,273: t15.2023.08.18 val PER: 0.1207
2026-01-08 18:35:00,275: t15.2023.08.20 val PER: 0.1279
2026-01-08 18:35:00,276: t15.2023.08.25 val PER: 0.1084
2026-01-08 18:35:00,278: t15.2023.08.27 val PER: 0.2010
2026-01-08 18:35:00,280: t15.2023.09.01 val PER: 0.0917
2026-01-08 18:35:00,281: t15.2023.09.03 val PER: 0.1639
2026-01-08 18:35:00,283: t15.2023.09.24 val PER: 0.1299
2026-01-08 18:35:00,285: t15.2023.09.29 val PER: 0.1455
2026-01-08 18:35:00,286: t15.2023.10.01 val PER: 0.1869
2026-01-08 18:35:00,288: t15.2023.10.06 val PER: 0.1001
2026-01-08 18:35:00,290: t15.2023.10.08 val PER: 0.2476
2026-01-08 18:35:00,291: t15.2023.10.13 val PER: 0.2172
2026-01-08 18:35:00,293: t15.2023.10.15 val PER: 0.1688
2026-01-08 18:35:00,295: t15.2023.10.20 val PER: 0.2047
2026-01-08 18:35:00,297: t15.2023.10.22 val PER: 0.1269
2026-01-08 18:35:00,299: t15.2023.11.03 val PER: 0.1798
2026-01-08 18:35:00,300: t15.2023.11.04 val PER: 0.0410
2026-01-08 18:35:00,302: t15.2023.11.17 val PER: 0.0482
2026-01-08 18:35:00,303: t15.2023.11.19 val PER: 0.0439
2026-01-08 18:35:00,305: t15.2023.11.26 val PER: 0.1428
2026-01-08 18:35:00,307: t15.2023.12.03 val PER: 0.1145
2026-01-08 18:35:00,308: t15.2023.12.08 val PER: 0.1152
2026-01-08 18:35:00,309: t15.2023.12.10 val PER: 0.1078
2026-01-08 18:35:00,311: t15.2023.12.17 val PER: 0.1528
2026-01-08 18:35:00,313: t15.2023.12.29 val PER: 0.1510
2026-01-08 18:35:00,314: t15.2024.02.25 val PER: 0.1264
2026-01-08 18:35:00,317: t15.2024.03.08 val PER: 0.2489
2026-01-08 18:35:00,319: t15.2024.03.15 val PER: 0.2214
2026-01-08 18:35:00,320: t15.2024.03.17 val PER: 0.1569
2026-01-08 18:35:00,322: t15.2024.05.10 val PER: 0.1738
2026-01-08 18:35:00,323: t15.2024.06.14 val PER: 0.1719
2026-01-08 18:35:00,325: t15.2024.07.19 val PER: 0.2637
2026-01-08 18:35:00,326: t15.2024.07.21 val PER: 0.1103
2026-01-08 18:35:00,328: t15.2024.07.28 val PER: 0.1566
2026-01-08 18:35:00,329: t15.2025.01.10 val PER: 0.3072
2026-01-08 18:35:00,331: t15.2025.01.12 val PER: 0.1624
2026-01-08 18:35:00,332: t15.2025.03.14 val PER: 0.3536
2026-01-08 18:35:00,334: t15.2025.03.16 val PER: 0.2134
2026-01-08 18:35:00,335: t15.2025.03.30 val PER: 0.3115
2026-01-08 18:35:00,337: t15.2025.04.13 val PER: 0.2211
2026-01-08 18:35:00,338: New best val WER(5gram) 15.97% --> 15.65%
2026-01-08 18:35:00,530: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_11500
2026-01-08 18:35:08,927: Train batch 11600: loss: 11.38 grad norm: 47.07 time: 0.063
2026-01-08 18:35:26,272: Train batch 11800: loss: 7.49 grad norm: 42.33 time: 0.045
2026-01-08 18:35:43,803: Train batch 12000: loss: 14.36 grad norm: 54.31 time: 0.072
2026-01-08 18:35:43,805: Running test after training batch: 12000
2026-01-08 18:35:43,906: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:35:48,907: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:35:48,958: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost set
2026-01-08 18:36:00,562: Val batch 12000: PER (avg): 0.1638 CTC Loss (avg): 16.7490 WER(5gram): 15.78% (n=256) time: 16.754
2026-01-08 18:36:00,564: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-08 18:36:00,567: t15.2023.08.13 val PER: 0.1258
2026-01-08 18:36:00,569: t15.2023.08.18 val PER: 0.1157
2026-01-08 18:36:00,571: t15.2023.08.20 val PER: 0.1176
2026-01-08 18:36:00,573: t15.2023.08.25 val PER: 0.1069
2026-01-08 18:36:00,574: t15.2023.08.27 val PER: 0.2090
2026-01-08 18:36:00,576: t15.2023.09.01 val PER: 0.0893
2026-01-08 18:36:00,578: t15.2023.09.03 val PER: 0.1639
2026-01-08 18:36:00,579: t15.2023.09.24 val PER: 0.1408
2026-01-08 18:36:00,581: t15.2023.09.29 val PER: 0.1461
2026-01-08 18:36:00,583: t15.2023.10.01 val PER: 0.1869
2026-01-08 18:36:00,585: t15.2023.10.06 val PER: 0.0915
2026-01-08 18:36:00,586: t15.2023.10.08 val PER: 0.2463
2026-01-08 18:36:00,588: t15.2023.10.13 val PER: 0.2242
2026-01-08 18:36:00,590: t15.2023.10.15 val PER: 0.1635
2026-01-08 18:36:00,591: t15.2023.10.20 val PER: 0.1846
2026-01-08 18:36:00,593: t15.2023.10.22 val PER: 0.1347
2026-01-08 18:36:00,595: t15.2023.11.03 val PER: 0.1879
2026-01-08 18:36:00,596: t15.2023.11.04 val PER: 0.0341
2026-01-08 18:36:00,598: t15.2023.11.17 val PER: 0.0482
2026-01-08 18:36:00,600: t15.2023.11.19 val PER: 0.0439
2026-01-08 18:36:00,601: t15.2023.11.26 val PER: 0.1442
2026-01-08 18:36:00,603: t15.2023.12.03 val PER: 0.1103
2026-01-08 18:36:00,604: t15.2023.12.08 val PER: 0.1119
2026-01-08 18:36:00,606: t15.2023.12.10 val PER: 0.1078
2026-01-08 18:36:00,608: t15.2023.12.17 val PER: 0.1403
2026-01-08 18:36:00,609: t15.2023.12.29 val PER: 0.1537
2026-01-08 18:36:00,611: t15.2024.02.25 val PER: 0.1250
2026-01-08 18:36:00,612: t15.2024.03.08 val PER: 0.2418
2026-01-08 18:36:00,614: t15.2024.03.15 val PER: 0.2233
2026-01-08 18:36:00,615: t15.2024.03.17 val PER: 0.1562
2026-01-08 18:36:00,617: t15.2024.05.10 val PER: 0.1753
2026-01-08 18:36:00,618: t15.2024.06.14 val PER: 0.1830
2026-01-08 18:36:00,620: t15.2024.07.19 val PER: 0.2498
2026-01-08 18:36:00,621: t15.2024.07.21 val PER: 0.1062
2026-01-08 18:36:00,623: t15.2024.07.28 val PER: 0.1588
2026-01-08 18:36:00,624: t15.2025.01.10 val PER: 0.3017
2026-01-08 18:36:00,626: t15.2025.01.12 val PER: 0.1617
2026-01-08 18:36:00,627: t15.2025.03.14 val PER: 0.3506
2026-01-08 18:36:00,629: t15.2025.03.16 val PER: 0.2029
2026-01-08 18:36:00,630: t15.2025.03.30 val PER: 0.3034
2026-01-08 18:36:00,632: t15.2025.04.13 val PER: 0.2240
2026-01-08 18:36:00,775: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_12000
2026-01-08 18:36:18,640: Train batch 12200: loss: 6.71 grad norm: 45.90 time: 0.068
2026-01-08 18:36:36,070: Train batch 12400: loss: 5.28 grad norm: 33.53 time: 0.041
2026-01-08 18:36:44,791: Running test after training batch: 12500
2026-01-08 18:36:44,931: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:36:49,888: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:36:49,942: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost is
2026-01-08 18:37:01,250: Val batch 12500: PER (avg): 0.1608 CTC Loss (avg): 16.3027 WER(5gram): 14.15% (n=256) time: 16.456
2026-01-08 18:37:01,252: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-08 18:37:01,255: t15.2023.08.13 val PER: 0.1320
2026-01-08 18:37:01,257: t15.2023.08.18 val PER: 0.1140
2026-01-08 18:37:01,259: t15.2023.08.20 val PER: 0.1128
2026-01-08 18:37:01,261: t15.2023.08.25 val PER: 0.0949
2026-01-08 18:37:01,263: t15.2023.08.27 val PER: 0.2042
2026-01-08 18:37:01,264: t15.2023.09.01 val PER: 0.0869
2026-01-08 18:37:01,266: t15.2023.09.03 val PER: 0.1698
2026-01-08 18:37:01,268: t15.2023.09.24 val PER: 0.1347
2026-01-08 18:37:01,270: t15.2023.09.29 val PER: 0.1359
2026-01-08 18:37:01,272: t15.2023.10.01 val PER: 0.1764
2026-01-08 18:37:01,273: t15.2023.10.06 val PER: 0.0947
2026-01-08 18:37:01,275: t15.2023.10.08 val PER: 0.2503
2026-01-08 18:37:01,277: t15.2023.10.13 val PER: 0.2196
2026-01-08 18:37:01,279: t15.2023.10.15 val PER: 0.1641
2026-01-08 18:37:01,281: t15.2023.10.20 val PER: 0.1879
2026-01-08 18:37:01,283: t15.2023.10.22 val PER: 0.1269
2026-01-08 18:37:01,284: t15.2023.11.03 val PER: 0.1805
2026-01-08 18:37:01,286: t15.2023.11.04 val PER: 0.0375
2026-01-08 18:37:01,288: t15.2023.11.17 val PER: 0.0513
2026-01-08 18:37:01,289: t15.2023.11.19 val PER: 0.0399
2026-01-08 18:37:01,291: t15.2023.11.26 val PER: 0.1399
2026-01-08 18:37:01,293: t15.2023.12.03 val PER: 0.1145
2026-01-08 18:37:01,295: t15.2023.12.08 val PER: 0.1138
2026-01-08 18:37:01,296: t15.2023.12.10 val PER: 0.0959
2026-01-08 18:37:01,299: t15.2023.12.17 val PER: 0.1424
2026-01-08 18:37:01,301: t15.2023.12.29 val PER: 0.1510
2026-01-08 18:37:01,303: t15.2024.02.25 val PER: 0.1067
2026-01-08 18:37:01,305: t15.2024.03.08 val PER: 0.2304
2026-01-08 18:37:01,306: t15.2024.03.15 val PER: 0.2189
2026-01-08 18:37:01,308: t15.2024.03.17 val PER: 0.1583
2026-01-08 18:37:01,309: t15.2024.05.10 val PER: 0.1709
2026-01-08 18:37:01,313: t15.2024.06.14 val PER: 0.1703
2026-01-08 18:37:01,315: t15.2024.07.19 val PER: 0.2551
2026-01-08 18:37:01,316: t15.2024.07.21 val PER: 0.1000
2026-01-08 18:37:01,318: t15.2024.07.28 val PER: 0.1515
2026-01-08 18:37:01,319: t15.2025.01.10 val PER: 0.3140
2026-01-08 18:37:01,321: t15.2025.01.12 val PER: 0.1509
2026-01-08 18:37:01,323: t15.2025.03.14 val PER: 0.3388
2026-01-08 18:37:01,324: t15.2025.03.16 val PER: 0.2068
2026-01-08 18:37:01,326: t15.2025.03.30 val PER: 0.3057
2026-01-08 18:37:01,327: t15.2025.04.13 val PER: 0.2240
2026-01-08 18:37:01,329: New best val WER(5gram) 15.65% --> 14.15%
2026-01-08 18:37:01,510: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_12500
2026-01-08 18:37:10,655: Train batch 12600: loss: 8.13 grad norm: 42.99 time: 0.058
2026-01-08 18:37:29,090: Train batch 12800: loss: 6.92 grad norm: 41.05 time: 0.053
2026-01-08 18:37:47,072: Train batch 13000: loss: 7.32 grad norm: 42.62 time: 0.066
2026-01-08 18:37:47,074: Running test after training batch: 13000
2026-01-08 18:37:47,189: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:37:52,132: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:37:52,185: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 18:38:03,461: Val batch 13000: PER (avg): 0.1584 CTC Loss (avg): 16.0947 WER(5gram): 15.71% (n=256) time: 16.383
2026-01-08 18:38:03,463: WER lens: avg_true_words=5.99 avg_pred_words=6.15 max_pred_words=12
2026-01-08 18:38:03,466: t15.2023.08.13 val PER: 0.1143
2026-01-08 18:38:03,469: t15.2023.08.18 val PER: 0.1224
2026-01-08 18:38:03,470: t15.2023.08.20 val PER: 0.1112
2026-01-08 18:38:03,472: t15.2023.08.25 val PER: 0.0994
2026-01-08 18:38:03,474: t15.2023.08.27 val PER: 0.1913
2026-01-08 18:38:03,476: t15.2023.09.01 val PER: 0.0893
2026-01-08 18:38:03,478: t15.2023.09.03 val PER: 0.1686
2026-01-08 18:38:03,480: t15.2023.09.24 val PER: 0.1347
2026-01-08 18:38:03,482: t15.2023.09.29 val PER: 0.1378
2026-01-08 18:38:03,484: t15.2023.10.01 val PER: 0.1777
2026-01-08 18:38:03,486: t15.2023.10.06 val PER: 0.0936
2026-01-08 18:38:03,487: t15.2023.10.08 val PER: 0.2395
2026-01-08 18:38:03,489: t15.2023.10.13 val PER: 0.2180
2026-01-08 18:38:03,491: t15.2023.10.15 val PER: 0.1602
2026-01-08 18:38:03,493: t15.2023.10.20 val PER: 0.1913
2026-01-08 18:38:03,494: t15.2023.10.22 val PER: 0.1236
2026-01-08 18:38:03,496: t15.2023.11.03 val PER: 0.1825
2026-01-08 18:38:03,498: t15.2023.11.04 val PER: 0.0341
2026-01-08 18:38:03,499: t15.2023.11.17 val PER: 0.0482
2026-01-08 18:38:03,501: t15.2023.11.19 val PER: 0.0399
2026-01-08 18:38:03,503: t15.2023.11.26 val PER: 0.1370
2026-01-08 18:38:03,505: t15.2023.12.03 val PER: 0.1197
2026-01-08 18:38:03,506: t15.2023.12.08 val PER: 0.1105
2026-01-08 18:38:03,508: t15.2023.12.10 val PER: 0.1051
2026-01-08 18:38:03,510: t15.2023.12.17 val PER: 0.1362
2026-01-08 18:38:03,512: t15.2023.12.29 val PER: 0.1503
2026-01-08 18:38:03,513: t15.2024.02.25 val PER: 0.1152
2026-01-08 18:38:03,515: t15.2024.03.08 val PER: 0.2461
2026-01-08 18:38:03,516: t15.2024.03.15 val PER: 0.2083
2026-01-08 18:38:03,518: t15.2024.03.17 val PER: 0.1576
2026-01-08 18:38:03,520: t15.2024.05.10 val PER: 0.1634
2026-01-08 18:38:03,522: t15.2024.06.14 val PER: 0.1703
2026-01-08 18:38:03,523: t15.2024.07.19 val PER: 0.2446
2026-01-08 18:38:03,525: t15.2024.07.21 val PER: 0.0945
2026-01-08 18:38:03,526: t15.2024.07.28 val PER: 0.1463
2026-01-08 18:38:03,528: t15.2025.01.10 val PER: 0.2934
2026-01-08 18:38:03,529: t15.2025.01.12 val PER: 0.1455
2026-01-08 18:38:03,531: t15.2025.03.14 val PER: 0.3506
2026-01-08 18:38:03,532: t15.2025.03.16 val PER: 0.1963
2026-01-08 18:38:03,534: t15.2025.03.30 val PER: 0.2954
2026-01-08 18:38:03,536: t15.2025.04.13 val PER: 0.2268
2026-01-08 18:38:03,675: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_13000
2026-01-08 18:38:20,708: Train batch 13200: loss: 14.48 grad norm: 61.81 time: 0.055
2026-01-08 18:38:37,610: Train batch 13400: loss: 9.27 grad norm: 47.97 time: 0.064
2026-01-08 18:38:46,058: Running test after training batch: 13500
2026-01-08 18:38:46,164: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:38:51,160: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:38:51,210: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost set
2026-01-08 18:39:02,658: Val batch 13500: PER (avg): 0.1571 CTC Loss (avg): 16.0123 WER(5gram): 15.32% (n=256) time: 16.598
2026-01-08 18:39:02,661: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-08 18:39:02,663: t15.2023.08.13 val PER: 0.1195
2026-01-08 18:39:02,665: t15.2023.08.18 val PER: 0.1140
2026-01-08 18:39:02,667: t15.2023.08.20 val PER: 0.1120
2026-01-08 18:39:02,669: t15.2023.08.25 val PER: 0.1009
2026-01-08 18:39:02,671: t15.2023.08.27 val PER: 0.1929
2026-01-08 18:39:02,673: t15.2023.09.01 val PER: 0.0909
2026-01-08 18:39:02,675: t15.2023.09.03 val PER: 0.1651
2026-01-08 18:39:02,677: t15.2023.09.24 val PER: 0.1274
2026-01-08 18:39:02,678: t15.2023.09.29 val PER: 0.1372
2026-01-08 18:39:02,680: t15.2023.10.01 val PER: 0.1843
2026-01-08 18:39:02,682: t15.2023.10.06 val PER: 0.0969
2026-01-08 18:39:02,684: t15.2023.10.08 val PER: 0.2395
2026-01-08 18:39:02,686: t15.2023.10.13 val PER: 0.2087
2026-01-08 18:39:02,688: t15.2023.10.15 val PER: 0.1582
2026-01-08 18:39:02,689: t15.2023.10.20 val PER: 0.1745
2026-01-08 18:39:02,691: t15.2023.10.22 val PER: 0.1303
2026-01-08 18:39:02,693: t15.2023.11.03 val PER: 0.1927
2026-01-08 18:39:02,696: t15.2023.11.04 val PER: 0.0410
2026-01-08 18:39:02,698: t15.2023.11.17 val PER: 0.0451
2026-01-08 18:39:02,700: t15.2023.11.19 val PER: 0.0299
2026-01-08 18:39:02,702: t15.2023.11.26 val PER: 0.1362
2026-01-08 18:39:02,703: t15.2023.12.03 val PER: 0.1092
2026-01-08 18:39:02,705: t15.2023.12.08 val PER: 0.1025
2026-01-08 18:39:02,707: t15.2023.12.10 val PER: 0.0972
2026-01-08 18:39:02,709: t15.2023.12.17 val PER: 0.1331
2026-01-08 18:39:02,711: t15.2023.12.29 val PER: 0.1352
2026-01-08 18:39:02,712: t15.2024.02.25 val PER: 0.1124
2026-01-08 18:39:02,714: t15.2024.03.08 val PER: 0.2361
2026-01-08 18:39:02,716: t15.2024.03.15 val PER: 0.2064
2026-01-08 18:39:02,718: t15.2024.03.17 val PER: 0.1464
2026-01-08 18:39:02,720: t15.2024.05.10 val PER: 0.1605
2026-01-08 18:39:02,721: t15.2024.06.14 val PER: 0.1751
2026-01-08 18:39:02,723: t15.2024.07.19 val PER: 0.2419
2026-01-08 18:39:02,725: t15.2024.07.21 val PER: 0.0986
2026-01-08 18:39:02,727: t15.2024.07.28 val PER: 0.1551
2026-01-08 18:39:02,728: t15.2025.01.10 val PER: 0.3044
2026-01-08 18:39:02,730: t15.2025.01.12 val PER: 0.1478
2026-01-08 18:39:02,732: t15.2025.03.14 val PER: 0.3521
2026-01-08 18:39:02,735: t15.2025.03.16 val PER: 0.1898
2026-01-08 18:39:02,736: t15.2025.03.30 val PER: 0.3034
2026-01-08 18:39:02,738: t15.2025.04.13 val PER: 0.2225
2026-01-08 18:39:02,877: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_13500
2026-01-08 18:39:11,376: Train batch 13600: loss: 13.33 grad norm: 62.16 time: 0.065
2026-01-08 18:39:28,471: Train batch 13800: loss: 10.19 grad norm: 55.94 time: 0.056
2026-01-08 18:39:45,627: Train batch 14000: loss: 12.81 grad norm: 57.52 time: 0.051
2026-01-08 18:39:45,630: Running test after training batch: 14000
2026-01-08 18:39:45,741: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:39:50,653: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:39:50,808: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost in
2026-01-08 18:40:02,322: Val batch 14000: PER (avg): 0.1562 CTC Loss (avg): 15.8898 WER(5gram): 15.58% (n=256) time: 16.690
2026-01-08 18:40:02,324: WER lens: avg_true_words=5.99 avg_pred_words=6.15 max_pred_words=12
2026-01-08 18:40:02,327: t15.2023.08.13 val PER: 0.1175
2026-01-08 18:40:02,328: t15.2023.08.18 val PER: 0.1090
2026-01-08 18:40:02,330: t15.2023.08.20 val PER: 0.1160
2026-01-08 18:40:02,331: t15.2023.08.25 val PER: 0.1054
2026-01-08 18:40:02,333: t15.2023.08.27 val PER: 0.1897
2026-01-08 18:40:02,334: t15.2023.09.01 val PER: 0.0836
2026-01-08 18:40:02,337: t15.2023.09.03 val PER: 0.1746
2026-01-08 18:40:02,338: t15.2023.09.24 val PER: 0.1274
2026-01-08 18:40:02,340: t15.2023.09.29 val PER: 0.1385
2026-01-08 18:40:02,341: t15.2023.10.01 val PER: 0.1764
2026-01-08 18:40:02,343: t15.2023.10.06 val PER: 0.0904
2026-01-08 18:40:02,344: t15.2023.10.08 val PER: 0.2273
2026-01-08 18:40:02,346: t15.2023.10.13 val PER: 0.2079
2026-01-08 18:40:02,347: t15.2023.10.15 val PER: 0.1589
2026-01-08 18:40:02,349: t15.2023.10.20 val PER: 0.1812
2026-01-08 18:40:02,351: t15.2023.10.22 val PER: 0.1236
2026-01-08 18:40:02,352: t15.2023.11.03 val PER: 0.1811
2026-01-08 18:40:02,354: t15.2023.11.04 val PER: 0.0375
2026-01-08 18:40:02,355: t15.2023.11.17 val PER: 0.0451
2026-01-08 18:40:02,357: t15.2023.11.19 val PER: 0.0359
2026-01-08 18:40:02,358: t15.2023.11.26 val PER: 0.1341
2026-01-08 18:40:02,359: t15.2023.12.03 val PER: 0.1092
2026-01-08 18:40:02,361: t15.2023.12.08 val PER: 0.1045
2026-01-08 18:40:02,362: t15.2023.12.10 val PER: 0.1012
2026-01-08 18:40:02,364: t15.2023.12.17 val PER: 0.1341
2026-01-08 18:40:02,367: t15.2023.12.29 val PER: 0.1448
2026-01-08 18:40:02,369: t15.2024.02.25 val PER: 0.1138
2026-01-08 18:40:02,370: t15.2024.03.08 val PER: 0.2489
2026-01-08 18:40:02,372: t15.2024.03.15 val PER: 0.2126
2026-01-08 18:40:02,373: t15.2024.03.17 val PER: 0.1444
2026-01-08 18:40:02,375: t15.2024.05.10 val PER: 0.1694
2026-01-08 18:40:02,376: t15.2024.06.14 val PER: 0.1735
2026-01-08 18:40:02,378: t15.2024.07.19 val PER: 0.2432
2026-01-08 18:40:02,379: t15.2024.07.21 val PER: 0.0931
2026-01-08 18:40:02,380: t15.2024.07.28 val PER: 0.1456
2026-01-08 18:40:02,382: t15.2025.01.10 val PER: 0.3168
2026-01-08 18:40:02,383: t15.2025.01.12 val PER: 0.1470
2026-01-08 18:40:02,385: t15.2025.03.14 val PER: 0.3402
2026-01-08 18:40:02,387: t15.2025.03.16 val PER: 0.1911
2026-01-08 18:40:02,388: t15.2025.03.30 val PER: 0.2885
2026-01-08 18:40:02,389: t15.2025.04.13 val PER: 0.2240
2026-01-08 18:40:02,533: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_14000
2026-01-08 18:40:20,773: Train batch 14200: loss: 8.80 grad norm: 51.68 time: 0.057
2026-01-08 18:40:39,384: Train batch 14400: loss: 6.18 grad norm: 38.19 time: 0.067
2026-01-08 18:40:48,667: Running test after training batch: 14500
2026-01-08 18:40:48,771: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:40:53,993: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:40:54,045: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost set
2026-01-08 18:41:05,586: Val batch 14500: PER (avg): 0.1550 CTC Loss (avg): 15.9451 WER(5gram): 16.17% (n=256) time: 16.916
2026-01-08 18:41:05,588: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 18:41:05,590: t15.2023.08.13 val PER: 0.1133
2026-01-08 18:41:05,593: t15.2023.08.18 val PER: 0.1106
2026-01-08 18:41:05,594: t15.2023.08.20 val PER: 0.1152
2026-01-08 18:41:05,596: t15.2023.08.25 val PER: 0.0964
2026-01-08 18:41:05,598: t15.2023.08.27 val PER: 0.1833
2026-01-08 18:41:05,600: t15.2023.09.01 val PER: 0.0836
2026-01-08 18:41:05,601: t15.2023.09.03 val PER: 0.1651
2026-01-08 18:41:05,603: t15.2023.09.24 val PER: 0.1250
2026-01-08 18:41:05,604: t15.2023.09.29 val PER: 0.1340
2026-01-08 18:41:05,606: t15.2023.10.01 val PER: 0.1777
2026-01-08 18:41:05,608: t15.2023.10.06 val PER: 0.0883
2026-01-08 18:41:05,610: t15.2023.10.08 val PER: 0.2463
2026-01-08 18:41:05,612: t15.2023.10.13 val PER: 0.2126
2026-01-08 18:41:05,615: t15.2023.10.15 val PER: 0.1589
2026-01-08 18:41:05,617: t15.2023.10.20 val PER: 0.1913
2026-01-08 18:41:05,619: t15.2023.10.22 val PER: 0.1192
2026-01-08 18:41:05,621: t15.2023.11.03 val PER: 0.1818
2026-01-08 18:41:05,623: t15.2023.11.04 val PER: 0.0375
2026-01-08 18:41:05,625: t15.2023.11.17 val PER: 0.0467
2026-01-08 18:41:05,627: t15.2023.11.19 val PER: 0.0319
2026-01-08 18:41:05,629: t15.2023.11.26 val PER: 0.1261
2026-01-08 18:41:05,630: t15.2023.12.03 val PER: 0.1071
2026-01-08 18:41:05,632: t15.2023.12.08 val PER: 0.1039
2026-01-08 18:41:05,634: t15.2023.12.10 val PER: 0.0867
2026-01-08 18:41:05,636: t15.2023.12.17 val PER: 0.1403
2026-01-08 18:41:05,638: t15.2023.12.29 val PER: 0.1366
2026-01-08 18:41:05,639: t15.2024.02.25 val PER: 0.1067
2026-01-08 18:41:05,641: t15.2024.03.08 val PER: 0.2319
2026-01-08 18:41:05,643: t15.2024.03.15 val PER: 0.2114
2026-01-08 18:41:05,645: t15.2024.03.17 val PER: 0.1471
2026-01-08 18:41:05,647: t15.2024.05.10 val PER: 0.1560
2026-01-08 18:41:05,649: t15.2024.06.14 val PER: 0.1719
2026-01-08 18:41:05,650: t15.2024.07.19 val PER: 0.2446
2026-01-08 18:41:05,652: t15.2024.07.21 val PER: 0.0966
2026-01-08 18:41:05,654: t15.2024.07.28 val PER: 0.1471
2026-01-08 18:41:05,656: t15.2025.01.10 val PER: 0.3085
2026-01-08 18:41:05,658: t15.2025.01.12 val PER: 0.1463
2026-01-08 18:41:05,660: t15.2025.03.14 val PER: 0.3580
2026-01-08 18:41:05,661: t15.2025.03.16 val PER: 0.1911
2026-01-08 18:41:05,663: t15.2025.03.30 val PER: 0.2839
2026-01-08 18:41:05,665: t15.2025.04.13 val PER: 0.2340
2026-01-08 18:41:05,810: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_14500
2026-01-08 18:41:14,989: Train batch 14600: loss: 14.28 grad norm: 60.14 time: 0.061
2026-01-08 18:41:33,544: Train batch 14800: loss: 6.38 grad norm: 43.04 time: 0.053
2026-01-08 18:41:51,952: Train batch 15000: loss: 9.21 grad norm: 52.26 time: 0.053
2026-01-08 18:41:51,955: Running test after training batch: 15000
2026-01-08 18:41:52,093: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:41:56,934: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:41:56,984: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost set
2026-01-08 18:42:08,390: Val batch 15000: PER (avg): 0.1536 CTC Loss (avg): 15.7064 WER(5gram): 15.32% (n=256) time: 16.432
2026-01-08 18:42:08,392: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 18:42:08,395: t15.2023.08.13 val PER: 0.1133
2026-01-08 18:42:08,397: t15.2023.08.18 val PER: 0.1115
2026-01-08 18:42:08,398: t15.2023.08.20 val PER: 0.1199
2026-01-08 18:42:08,400: t15.2023.08.25 val PER: 0.1099
2026-01-08 18:42:08,402: t15.2023.08.27 val PER: 0.1833
2026-01-08 18:42:08,404: t15.2023.09.01 val PER: 0.0812
2026-01-08 18:42:08,405: t15.2023.09.03 val PER: 0.1580
2026-01-08 18:42:08,407: t15.2023.09.24 val PER: 0.1299
2026-01-08 18:42:08,409: t15.2023.09.29 val PER: 0.1385
2026-01-08 18:42:08,411: t15.2023.10.01 val PER: 0.1724
2026-01-08 18:42:08,412: t15.2023.10.06 val PER: 0.0829
2026-01-08 18:42:08,414: t15.2023.10.08 val PER: 0.2260
2026-01-08 18:42:08,416: t15.2023.10.13 val PER: 0.2095
2026-01-08 18:42:08,417: t15.2023.10.15 val PER: 0.1589
2026-01-08 18:42:08,419: t15.2023.10.20 val PER: 0.1644
2026-01-08 18:42:08,421: t15.2023.10.22 val PER: 0.1225
2026-01-08 18:42:08,422: t15.2023.11.03 val PER: 0.1784
2026-01-08 18:42:08,424: t15.2023.11.04 val PER: 0.0410
2026-01-08 18:42:08,426: t15.2023.11.17 val PER: 0.0373
2026-01-08 18:42:08,427: t15.2023.11.19 val PER: 0.0359
2026-01-08 18:42:08,429: t15.2023.11.26 val PER: 0.1232
2026-01-08 18:42:08,430: t15.2023.12.03 val PER: 0.1103
2026-01-08 18:42:08,432: t15.2023.12.08 val PER: 0.1025
2026-01-08 18:42:08,433: t15.2023.12.10 val PER: 0.0946
2026-01-08 18:42:08,435: t15.2023.12.17 val PER: 0.1372
2026-01-08 18:42:08,436: t15.2023.12.29 val PER: 0.1393
2026-01-08 18:42:08,438: t15.2024.02.25 val PER: 0.1194
2026-01-08 18:42:08,441: t15.2024.03.08 val PER: 0.2418
2026-01-08 18:42:08,442: t15.2024.03.15 val PER: 0.2101
2026-01-08 18:42:08,444: t15.2024.03.17 val PER: 0.1416
2026-01-08 18:42:08,446: t15.2024.05.10 val PER: 0.1738
2026-01-08 18:42:08,447: t15.2024.06.14 val PER: 0.1577
2026-01-08 18:42:08,449: t15.2024.07.19 val PER: 0.2373
2026-01-08 18:42:08,451: t15.2024.07.21 val PER: 0.0966
2026-01-08 18:42:08,452: t15.2024.07.28 val PER: 0.1382
2026-01-08 18:42:08,454: t15.2025.01.10 val PER: 0.3127
2026-01-08 18:42:08,455: t15.2025.01.12 val PER: 0.1455
2026-01-08 18:42:08,457: t15.2025.03.14 val PER: 0.3447
2026-01-08 18:42:08,458: t15.2025.03.16 val PER: 0.1832
2026-01-08 18:42:08,460: t15.2025.03.30 val PER: 0.2805
2026-01-08 18:42:08,461: t15.2025.04.13 val PER: 0.2268
2026-01-08 18:42:08,604: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_15000
2026-01-08 18:42:27,221: Train batch 15200: loss: 5.14 grad norm: 40.96 time: 0.059
2026-01-08 18:42:45,302: Train batch 15400: loss: 11.79 grad norm: 55.79 time: 0.051
2026-01-08 18:42:53,935: Running test after training batch: 15500
2026-01-08 18:42:54,097: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:42:59,047: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:42:59,099: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost said
2026-01-08 18:43:10,673: Val batch 15500: PER (avg): 0.1535 CTC Loss (avg): 15.6901 WER(5gram): 16.04% (n=256) time: 16.735
2026-01-08 18:43:10,675: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 18:43:10,678: t15.2023.08.13 val PER: 0.1133
2026-01-08 18:43:10,680: t15.2023.08.18 val PER: 0.1106
2026-01-08 18:43:10,681: t15.2023.08.20 val PER: 0.1112
2026-01-08 18:43:10,683: t15.2023.08.25 val PER: 0.1099
2026-01-08 18:43:10,685: t15.2023.08.27 val PER: 0.1897
2026-01-08 18:43:10,687: t15.2023.09.01 val PER: 0.0804
2026-01-08 18:43:10,689: t15.2023.09.03 val PER: 0.1698
2026-01-08 18:43:10,692: t15.2023.09.24 val PER: 0.1286
2026-01-08 18:43:10,694: t15.2023.09.29 val PER: 0.1353
2026-01-08 18:43:10,696: t15.2023.10.01 val PER: 0.1717
2026-01-08 18:43:10,698: t15.2023.10.06 val PER: 0.0850
2026-01-08 18:43:10,699: t15.2023.10.08 val PER: 0.2355
2026-01-08 18:43:10,702: t15.2023.10.13 val PER: 0.2079
2026-01-08 18:43:10,704: t15.2023.10.15 val PER: 0.1549
2026-01-08 18:43:10,705: t15.2023.10.20 val PER: 0.1812
2026-01-08 18:43:10,707: t15.2023.10.22 val PER: 0.1192
2026-01-08 18:43:10,709: t15.2023.11.03 val PER: 0.1845
2026-01-08 18:43:10,710: t15.2023.11.04 val PER: 0.0410
2026-01-08 18:43:10,712: t15.2023.11.17 val PER: 0.0311
2026-01-08 18:43:10,714: t15.2023.11.19 val PER: 0.0419
2026-01-08 18:43:10,715: t15.2023.11.26 val PER: 0.1254
2026-01-08 18:43:10,717: t15.2023.12.03 val PER: 0.1071
2026-01-08 18:43:10,719: t15.2023.12.08 val PER: 0.1019
2026-01-08 18:43:10,720: t15.2023.12.10 val PER: 0.0894
2026-01-08 18:43:10,722: t15.2023.12.17 val PER: 0.1341
2026-01-08 18:43:10,724: t15.2023.12.29 val PER: 0.1380
2026-01-08 18:43:10,725: t15.2024.02.25 val PER: 0.1194
2026-01-08 18:43:10,727: t15.2024.03.08 val PER: 0.2390
2026-01-08 18:43:10,728: t15.2024.03.15 val PER: 0.2045
2026-01-08 18:43:10,730: t15.2024.03.17 val PER: 0.1444
2026-01-08 18:43:10,732: t15.2024.05.10 val PER: 0.1694
2026-01-08 18:43:10,733: t15.2024.06.14 val PER: 0.1577
2026-01-08 18:43:10,735: t15.2024.07.19 val PER: 0.2380
2026-01-08 18:43:10,737: t15.2024.07.21 val PER: 0.0979
2026-01-08 18:43:10,738: t15.2024.07.28 val PER: 0.1397
2026-01-08 18:43:10,740: t15.2025.01.10 val PER: 0.3058
2026-01-08 18:43:10,742: t15.2025.01.12 val PER: 0.1493
2026-01-08 18:43:10,743: t15.2025.03.14 val PER: 0.3580
2026-01-08 18:43:10,745: t15.2025.03.16 val PER: 0.1872
2026-01-08 18:43:10,746: t15.2025.03.30 val PER: 0.2862
2026-01-08 18:43:10,748: t15.2025.04.13 val PER: 0.2126
2026-01-08 18:43:10,888: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_15500
2026-01-08 18:43:19,538: Train batch 15600: loss: 12.53 grad norm: 58.33 time: 0.063
2026-01-08 18:43:37,105: Train batch 15800: loss: 14.43 grad norm: 63.74 time: 0.072
2026-01-08 18:43:54,756: Train batch 16000: loss: 9.43 grad norm: 48.28 time: 0.056
2026-01-08 18:43:54,759: Running test after training batch: 16000
2026-01-08 18:43:54,863: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:43:59,764: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:43:59,817: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost set
2026-01-08 18:44:11,547: Val batch 16000: PER (avg): 0.1518 CTC Loss (avg): 15.6852 WER(5gram): 16.04% (n=256) time: 16.786
2026-01-08 18:44:11,550: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 18:44:11,552: t15.2023.08.13 val PER: 0.1091
2026-01-08 18:44:11,554: t15.2023.08.18 val PER: 0.1098
2026-01-08 18:44:11,556: t15.2023.08.20 val PER: 0.1104
2026-01-08 18:44:11,558: t15.2023.08.25 val PER: 0.1039
2026-01-08 18:44:11,559: t15.2023.08.27 val PER: 0.1929
2026-01-08 18:44:11,562: t15.2023.09.01 val PER: 0.0836
2026-01-08 18:44:11,564: t15.2023.09.03 val PER: 0.1615
2026-01-08 18:44:11,566: t15.2023.09.24 val PER: 0.1286
2026-01-08 18:44:11,567: t15.2023.09.29 val PER: 0.1372
2026-01-08 18:44:11,569: t15.2023.10.01 val PER: 0.1711
2026-01-08 18:44:11,571: t15.2023.10.06 val PER: 0.0904
2026-01-08 18:44:11,573: t15.2023.10.08 val PER: 0.2382
2026-01-08 18:44:11,575: t15.2023.10.13 val PER: 0.2071
2026-01-08 18:44:11,577: t15.2023.10.15 val PER: 0.1562
2026-01-08 18:44:11,578: t15.2023.10.20 val PER: 0.1812
2026-01-08 18:44:11,581: t15.2023.10.22 val PER: 0.1203
2026-01-08 18:44:11,583: t15.2023.11.03 val PER: 0.1811
2026-01-08 18:44:11,584: t15.2023.11.04 val PER: 0.0341
2026-01-08 18:44:11,586: t15.2023.11.17 val PER: 0.0373
2026-01-08 18:44:11,588: t15.2023.11.19 val PER: 0.0339
2026-01-08 18:44:11,590: t15.2023.11.26 val PER: 0.1246
2026-01-08 18:44:11,591: t15.2023.12.03 val PER: 0.1071
2026-01-08 18:44:11,593: t15.2023.12.08 val PER: 0.0912
2026-01-08 18:44:11,595: t15.2023.12.10 val PER: 0.0841
2026-01-08 18:44:11,597: t15.2023.12.17 val PER: 0.1403
2026-01-08 18:44:11,598: t15.2023.12.29 val PER: 0.1352
2026-01-08 18:44:11,600: t15.2024.02.25 val PER: 0.1124
2026-01-08 18:44:11,602: t15.2024.03.08 val PER: 0.2347
2026-01-08 18:44:11,604: t15.2024.03.15 val PER: 0.2064
2026-01-08 18:44:11,605: t15.2024.03.17 val PER: 0.1360
2026-01-08 18:44:11,607: t15.2024.05.10 val PER: 0.1620
2026-01-08 18:44:11,609: t15.2024.06.14 val PER: 0.1640
2026-01-08 18:44:11,611: t15.2024.07.19 val PER: 0.2347
2026-01-08 18:44:11,612: t15.2024.07.21 val PER: 0.0938
2026-01-08 18:44:11,614: t15.2024.07.28 val PER: 0.1397
2026-01-08 18:44:11,616: t15.2025.01.10 val PER: 0.3085
2026-01-08 18:44:11,617: t15.2025.01.12 val PER: 0.1386
2026-01-08 18:44:11,619: t15.2025.03.14 val PER: 0.3476
2026-01-08 18:44:11,621: t15.2025.03.16 val PER: 0.1898
2026-01-08 18:44:11,623: t15.2025.03.30 val PER: 0.2816
2026-01-08 18:44:11,625: t15.2025.04.13 val PER: 0.2211
2026-01-08 18:44:11,763: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_16000
2026-01-08 18:44:28,979: Train batch 16200: loss: 7.01 grad norm: 43.08 time: 0.057
2026-01-08 18:44:46,320: Train batch 16400: loss: 12.02 grad norm: 60.56 time: 0.059
2026-01-08 18:44:55,444: Running test after training batch: 16500
2026-01-08 18:44:55,541: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:45:00,455: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:45:00,520: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost cents
2026-01-08 18:45:12,250: Val batch 16500: PER (avg): 0.1523 CTC Loss (avg): 15.5137 WER(5gram): 15.91% (n=256) time: 16.804
2026-01-08 18:45:12,253: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 18:45:12,255: t15.2023.08.13 val PER: 0.1081
2026-01-08 18:45:12,258: t15.2023.08.18 val PER: 0.1123
2026-01-08 18:45:12,260: t15.2023.08.20 val PER: 0.1096
2026-01-08 18:45:12,261: t15.2023.08.25 val PER: 0.1024
2026-01-08 18:45:12,263: t15.2023.08.27 val PER: 0.1977
2026-01-08 18:45:12,267: t15.2023.09.01 val PER: 0.0828
2026-01-08 18:45:12,269: t15.2023.09.03 val PER: 0.1663
2026-01-08 18:45:12,271: t15.2023.09.24 val PER: 0.1201
2026-01-08 18:45:12,272: t15.2023.09.29 val PER: 0.1340
2026-01-08 18:45:12,274: t15.2023.10.01 val PER: 0.1764
2026-01-08 18:45:12,275: t15.2023.10.06 val PER: 0.0915
2026-01-08 18:45:12,277: t15.2023.10.08 val PER: 0.2327
2026-01-08 18:45:12,279: t15.2023.10.13 val PER: 0.2079
2026-01-08 18:45:12,280: t15.2023.10.15 val PER: 0.1543
2026-01-08 18:45:12,282: t15.2023.10.20 val PER: 0.1812
2026-01-08 18:45:12,283: t15.2023.10.22 val PER: 0.1203
2026-01-08 18:45:12,285: t15.2023.11.03 val PER: 0.1811
2026-01-08 18:45:12,287: t15.2023.11.04 val PER: 0.0341
2026-01-08 18:45:12,288: t15.2023.11.17 val PER: 0.0327
2026-01-08 18:45:12,290: t15.2023.11.19 val PER: 0.0299
2026-01-08 18:45:12,292: t15.2023.11.26 val PER: 0.1254
2026-01-08 18:45:12,293: t15.2023.12.03 val PER: 0.1082
2026-01-08 18:45:12,294: t15.2023.12.08 val PER: 0.0992
2026-01-08 18:45:12,296: t15.2023.12.10 val PER: 0.0867
2026-01-08 18:45:12,298: t15.2023.12.17 val PER: 0.1268
2026-01-08 18:45:12,299: t15.2023.12.29 val PER: 0.1366
2026-01-08 18:45:12,300: t15.2024.02.25 val PER: 0.1124
2026-01-08 18:45:12,302: t15.2024.03.08 val PER: 0.2432
2026-01-08 18:45:12,303: t15.2024.03.15 val PER: 0.2139
2026-01-08 18:45:12,305: t15.2024.03.17 val PER: 0.1416
2026-01-08 18:45:12,306: t15.2024.05.10 val PER: 0.1545
2026-01-08 18:45:12,308: t15.2024.06.14 val PER: 0.1672
2026-01-08 18:45:12,309: t15.2024.07.19 val PER: 0.2399
2026-01-08 18:45:12,312: t15.2024.07.21 val PER: 0.0924
2026-01-08 18:45:12,313: t15.2024.07.28 val PER: 0.1404
2026-01-08 18:45:12,315: t15.2025.01.10 val PER: 0.3030
2026-01-08 18:45:12,316: t15.2025.01.12 val PER: 0.1424
2026-01-08 18:45:12,318: t15.2025.03.14 val PER: 0.3388
2026-01-08 18:45:12,319: t15.2025.03.16 val PER: 0.1950
2026-01-08 18:45:12,320: t15.2025.03.30 val PER: 0.2747
2026-01-08 18:45:12,321: t15.2025.04.13 val PER: 0.2211
2026-01-08 18:45:12,461: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_16500
2026-01-08 18:45:21,534: Train batch 16600: loss: 9.35 grad norm: 51.50 time: 0.054
2026-01-08 18:45:40,085: Train batch 16800: loss: 17.81 grad norm: 68.40 time: 0.063
2026-01-08 18:45:58,592: Train batch 17000: loss: 9.34 grad norm: 48.49 time: 0.082
2026-01-08 18:45:58,594: Running test after training batch: 17000
2026-01-08 18:45:58,698: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:46:03,671: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:46:03,732: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost cents
2026-01-08 18:46:15,506: Val batch 17000: PER (avg): 0.1505 CTC Loss (avg): 15.4608 WER(5gram): 15.91% (n=256) time: 16.909
2026-01-08 18:46:15,508: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 18:46:15,514: t15.2023.08.13 val PER: 0.1060
2026-01-08 18:46:15,516: t15.2023.08.18 val PER: 0.1090
2026-01-08 18:46:15,518: t15.2023.08.20 val PER: 0.1072
2026-01-08 18:46:15,520: t15.2023.08.25 val PER: 0.0949
2026-01-08 18:46:15,522: t15.2023.08.27 val PER: 0.1929
2026-01-08 18:46:15,524: t15.2023.09.01 val PER: 0.0747
2026-01-08 18:46:15,526: t15.2023.09.03 val PER: 0.1627
2026-01-08 18:46:15,528: t15.2023.09.24 val PER: 0.1274
2026-01-08 18:46:15,529: t15.2023.09.29 val PER: 0.1372
2026-01-08 18:46:15,531: t15.2023.10.01 val PER: 0.1704
2026-01-08 18:46:15,533: t15.2023.10.06 val PER: 0.0872
2026-01-08 18:46:15,535: t15.2023.10.08 val PER: 0.2260
2026-01-08 18:46:15,536: t15.2023.10.13 val PER: 0.2141
2026-01-08 18:46:15,538: t15.2023.10.15 val PER: 0.1556
2026-01-08 18:46:15,540: t15.2023.10.20 val PER: 0.1779
2026-01-08 18:46:15,542: t15.2023.10.22 val PER: 0.1203
2026-01-08 18:46:15,544: t15.2023.11.03 val PER: 0.1805
2026-01-08 18:46:15,546: t15.2023.11.04 val PER: 0.0307
2026-01-08 18:46:15,548: t15.2023.11.17 val PER: 0.0358
2026-01-08 18:46:15,550: t15.2023.11.19 val PER: 0.0339
2026-01-08 18:46:15,551: t15.2023.11.26 val PER: 0.1188
2026-01-08 18:46:15,553: t15.2023.12.03 val PER: 0.1061
2026-01-08 18:46:15,555: t15.2023.12.08 val PER: 0.0939
2026-01-08 18:46:15,557: t15.2023.12.10 val PER: 0.0867
2026-01-08 18:46:15,559: t15.2023.12.17 val PER: 0.1310
2026-01-08 18:46:15,561: t15.2023.12.29 val PER: 0.1304
2026-01-08 18:46:15,562: t15.2024.02.25 val PER: 0.1166
2026-01-08 18:46:15,564: t15.2024.03.08 val PER: 0.2376
2026-01-08 18:46:15,566: t15.2024.03.15 val PER: 0.2051
2026-01-08 18:46:15,568: t15.2024.03.17 val PER: 0.1430
2026-01-08 18:46:15,569: t15.2024.05.10 val PER: 0.1634
2026-01-08 18:46:15,571: t15.2024.06.14 val PER: 0.1577
2026-01-08 18:46:15,574: t15.2024.07.19 val PER: 0.2334
2026-01-08 18:46:15,575: t15.2024.07.21 val PER: 0.0931
2026-01-08 18:46:15,577: t15.2024.07.28 val PER: 0.1404
2026-01-08 18:46:15,579: t15.2025.01.10 val PER: 0.3044
2026-01-08 18:46:15,581: t15.2025.01.12 val PER: 0.1393
2026-01-08 18:46:15,582: t15.2025.03.14 val PER: 0.3521
2026-01-08 18:46:15,584: t15.2025.03.16 val PER: 0.1911
2026-01-08 18:46:15,586: t15.2025.03.30 val PER: 0.2736
2026-01-08 18:46:15,589: t15.2025.04.13 val PER: 0.2154
2026-01-08 18:46:15,734: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_17000
2026-01-08 18:46:33,190: Train batch 17200: loss: 10.70 grad norm: 50.05 time: 0.085
2026-01-08 18:46:50,852: Train batch 17400: loss: 13.89 grad norm: 60.89 time: 0.071
2026-01-08 18:46:59,641: Running test after training batch: 17500
2026-01-08 18:46:59,778: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:47:04,682: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:47:04,738: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost cents
2026-01-08 18:47:16,700: Val batch 17500: PER (avg): 0.1511 CTC Loss (avg): 15.4353 WER(5gram): 15.58% (n=256) time: 17.057
2026-01-08 18:47:16,705: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 18:47:16,709: t15.2023.08.13 val PER: 0.1175
2026-01-08 18:47:16,711: t15.2023.08.18 val PER: 0.1081
2026-01-08 18:47:16,712: t15.2023.08.20 val PER: 0.1033
2026-01-08 18:47:16,714: t15.2023.08.25 val PER: 0.0994
2026-01-08 18:47:16,715: t15.2023.08.27 val PER: 0.1881
2026-01-08 18:47:16,717: t15.2023.09.01 val PER: 0.0795
2026-01-08 18:47:16,718: t15.2023.09.03 val PER: 0.1651
2026-01-08 18:47:16,720: t15.2023.09.24 val PER: 0.1238
2026-01-08 18:47:16,721: t15.2023.09.29 val PER: 0.1327
2026-01-08 18:47:16,723: t15.2023.10.01 val PER: 0.1750
2026-01-08 18:47:16,724: t15.2023.10.06 val PER: 0.0904
2026-01-08 18:47:16,726: t15.2023.10.08 val PER: 0.2314
2026-01-08 18:47:16,728: t15.2023.10.13 val PER: 0.2126
2026-01-08 18:47:16,729: t15.2023.10.15 val PER: 0.1523
2026-01-08 18:47:16,731: t15.2023.10.20 val PER: 0.1812
2026-01-08 18:47:16,733: t15.2023.10.22 val PER: 0.1192
2026-01-08 18:47:16,734: t15.2023.11.03 val PER: 0.1737
2026-01-08 18:47:16,736: t15.2023.11.04 val PER: 0.0375
2026-01-08 18:47:16,738: t15.2023.11.17 val PER: 0.0342
2026-01-08 18:47:16,739: t15.2023.11.19 val PER: 0.0299
2026-01-08 18:47:16,741: t15.2023.11.26 val PER: 0.1167
2026-01-08 18:47:16,742: t15.2023.12.03 val PER: 0.1050
2026-01-08 18:47:16,744: t15.2023.12.08 val PER: 0.0965
2026-01-08 18:47:16,746: t15.2023.12.10 val PER: 0.0920
2026-01-08 18:47:16,747: t15.2023.12.17 val PER: 0.1320
2026-01-08 18:47:16,749: t15.2023.12.29 val PER: 0.1318
2026-01-08 18:47:16,751: t15.2024.02.25 val PER: 0.1124
2026-01-08 18:47:16,753: t15.2024.03.08 val PER: 0.2376
2026-01-08 18:47:16,754: t15.2024.03.15 val PER: 0.2070
2026-01-08 18:47:16,756: t15.2024.03.17 val PER: 0.1360
2026-01-08 18:47:16,758: t15.2024.05.10 val PER: 0.1605
2026-01-08 18:47:16,760: t15.2024.06.14 val PER: 0.1656
2026-01-08 18:47:16,762: t15.2024.07.19 val PER: 0.2353
2026-01-08 18:47:16,763: t15.2024.07.21 val PER: 0.0917
2026-01-08 18:47:16,765: t15.2024.07.28 val PER: 0.1434
2026-01-08 18:47:16,766: t15.2025.01.10 val PER: 0.3072
2026-01-08 18:47:16,768: t15.2025.01.12 val PER: 0.1432
2026-01-08 18:47:16,769: t15.2025.03.14 val PER: 0.3580
2026-01-08 18:47:16,771: t15.2025.03.16 val PER: 0.1911
2026-01-08 18:47:16,773: t15.2025.03.30 val PER: 0.2805
2026-01-08 18:47:16,774: t15.2025.04.13 val PER: 0.2197
2026-01-08 18:47:16,913: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_17500
2026-01-08 18:47:25,658: Train batch 17600: loss: 10.88 grad norm: 54.39 time: 0.051
2026-01-08 18:47:43,543: Train batch 17800: loss: 6.69 grad norm: 48.14 time: 0.042
2026-01-08 18:48:00,929: Train batch 18000: loss: 11.83 grad norm: 60.05 time: 0.068
2026-01-08 18:48:00,931: Running test after training batch: 18000
2026-01-08 18:48:01,055: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:48:05,987: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:48:06,044: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost cents
2026-01-08 18:48:17,966: Val batch 18000: PER (avg): 0.1496 CTC Loss (avg): 15.4013 WER(5gram): 15.65% (n=256) time: 17.032
2026-01-08 18:48:17,968: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 18:48:17,977: t15.2023.08.13 val PER: 0.1071
2026-01-08 18:48:17,979: t15.2023.08.18 val PER: 0.1073
2026-01-08 18:48:17,981: t15.2023.08.20 val PER: 0.1056
2026-01-08 18:48:17,982: t15.2023.08.25 val PER: 0.0979
2026-01-08 18:48:17,984: t15.2023.08.27 val PER: 0.1913
2026-01-08 18:48:17,986: t15.2023.09.01 val PER: 0.0755
2026-01-08 18:48:17,987: t15.2023.09.03 val PER: 0.1627
2026-01-08 18:48:17,989: t15.2023.09.24 val PER: 0.1226
2026-01-08 18:48:17,990: t15.2023.09.29 val PER: 0.1353
2026-01-08 18:48:17,992: t15.2023.10.01 val PER: 0.1717
2026-01-08 18:48:17,994: t15.2023.10.06 val PER: 0.0893
2026-01-08 18:48:17,995: t15.2023.10.08 val PER: 0.2300
2026-01-08 18:48:17,997: t15.2023.10.13 val PER: 0.2056
2026-01-08 18:48:17,999: t15.2023.10.15 val PER: 0.1549
2026-01-08 18:48:18,000: t15.2023.10.20 val PER: 0.1779
2026-01-08 18:48:18,002: t15.2023.10.22 val PER: 0.1180
2026-01-08 18:48:18,003: t15.2023.11.03 val PER: 0.1805
2026-01-08 18:48:18,005: t15.2023.11.04 val PER: 0.0375
2026-01-08 18:48:18,006: t15.2023.11.17 val PER: 0.0358
2026-01-08 18:48:18,008: t15.2023.11.19 val PER: 0.0319
2026-01-08 18:48:18,012: t15.2023.11.26 val PER: 0.1188
2026-01-08 18:48:18,014: t15.2023.12.03 val PER: 0.1029
2026-01-08 18:48:18,015: t15.2023.12.08 val PER: 0.0972
2026-01-08 18:48:18,017: t15.2023.12.10 val PER: 0.0854
2026-01-08 18:48:18,019: t15.2023.12.17 val PER: 0.1320
2026-01-08 18:48:18,020: t15.2023.12.29 val PER: 0.1311
2026-01-08 18:48:18,022: t15.2024.02.25 val PER: 0.1110
2026-01-08 18:48:18,023: t15.2024.03.08 val PER: 0.2447
2026-01-08 18:48:18,025: t15.2024.03.15 val PER: 0.1995
2026-01-08 18:48:18,027: t15.2024.03.17 val PER: 0.1353
2026-01-08 18:48:18,028: t15.2024.05.10 val PER: 0.1634
2026-01-08 18:48:18,031: t15.2024.06.14 val PER: 0.1577
2026-01-08 18:48:18,033: t15.2024.07.19 val PER: 0.2320
2026-01-08 18:48:18,034: t15.2024.07.21 val PER: 0.0910
2026-01-08 18:48:18,037: t15.2024.07.28 val PER: 0.1368
2026-01-08 18:48:18,039: t15.2025.01.10 val PER: 0.3044
2026-01-08 18:48:18,041: t15.2025.01.12 val PER: 0.1440
2026-01-08 18:48:18,043: t15.2025.03.14 val PER: 0.3506
2026-01-08 18:48:18,044: t15.2025.03.16 val PER: 0.1924
2026-01-08 18:48:18,046: t15.2025.03.30 val PER: 0.2678
2026-01-08 18:48:18,048: t15.2025.04.13 val PER: 0.2168
2026-01-08 18:48:18,182: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_18000
2026-01-08 18:48:35,979: Train batch 18200: loss: 8.27 grad norm: 48.64 time: 0.074
2026-01-08 18:48:53,207: Train batch 18400: loss: 5.91 grad norm: 44.30 time: 0.059
2026-01-08 18:49:01,943: Running test after training batch: 18500
2026-01-08 18:49:02,080: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:49:07,146: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:49:07,208: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost cents
2026-01-08 18:49:19,232: Val batch 18500: PER (avg): 0.1498 CTC Loss (avg): 15.4194 WER(5gram): 15.91% (n=256) time: 17.287
2026-01-08 18:49:19,234: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 18:49:19,237: t15.2023.08.13 val PER: 0.1060
2026-01-08 18:49:19,238: t15.2023.08.18 val PER: 0.1081
2026-01-08 18:49:19,240: t15.2023.08.20 val PER: 0.1048
2026-01-08 18:49:19,242: t15.2023.08.25 val PER: 0.0919
2026-01-08 18:49:19,244: t15.2023.08.27 val PER: 0.1961
2026-01-08 18:49:19,246: t15.2023.09.01 val PER: 0.0747
2026-01-08 18:49:19,248: t15.2023.09.03 val PER: 0.1591
2026-01-08 18:49:19,249: t15.2023.09.24 val PER: 0.1226
2026-01-08 18:49:19,251: t15.2023.09.29 val PER: 0.1398
2026-01-08 18:49:19,253: t15.2023.10.01 val PER: 0.1671
2026-01-08 18:49:19,255: t15.2023.10.06 val PER: 0.0904
2026-01-08 18:49:19,256: t15.2023.10.08 val PER: 0.2314
2026-01-08 18:49:19,258: t15.2023.10.13 val PER: 0.2079
2026-01-08 18:49:19,260: t15.2023.10.15 val PER: 0.1575
2026-01-08 18:49:19,262: t15.2023.10.20 val PER: 0.1812
2026-01-08 18:49:19,263: t15.2023.10.22 val PER: 0.1069
2026-01-08 18:49:19,265: t15.2023.11.03 val PER: 0.1825
2026-01-08 18:49:19,266: t15.2023.11.04 val PER: 0.0375
2026-01-08 18:49:19,268: t15.2023.11.17 val PER: 0.0342
2026-01-08 18:49:19,270: t15.2023.11.19 val PER: 0.0339
2026-01-08 18:49:19,271: t15.2023.11.26 val PER: 0.1159
2026-01-08 18:49:19,273: t15.2023.12.03 val PER: 0.0966
2026-01-08 18:49:19,275: t15.2023.12.08 val PER: 0.0979
2026-01-08 18:49:19,277: t15.2023.12.10 val PER: 0.0854
2026-01-08 18:49:19,279: t15.2023.12.17 val PER: 0.1279
2026-01-08 18:49:19,281: t15.2023.12.29 val PER: 0.1283
2026-01-08 18:49:19,282: t15.2024.02.25 val PER: 0.1194
2026-01-08 18:49:19,284: t15.2024.03.08 val PER: 0.2432
2026-01-08 18:49:19,286: t15.2024.03.15 val PER: 0.2014
2026-01-08 18:49:19,287: t15.2024.03.17 val PER: 0.1388
2026-01-08 18:49:19,289: t15.2024.05.10 val PER: 0.1634
2026-01-08 18:49:19,291: t15.2024.06.14 val PER: 0.1577
2026-01-08 18:49:19,292: t15.2024.07.19 val PER: 0.2367
2026-01-08 18:49:19,295: t15.2024.07.21 val PER: 0.0945
2026-01-08 18:49:19,297: t15.2024.07.28 val PER: 0.1397
2026-01-08 18:49:19,299: t15.2025.01.10 val PER: 0.3044
2026-01-08 18:49:19,300: t15.2025.01.12 val PER: 0.1370
2026-01-08 18:49:19,302: t15.2025.03.14 val PER: 0.3506
2026-01-08 18:49:19,303: t15.2025.03.16 val PER: 0.1924
2026-01-08 18:49:19,305: t15.2025.03.30 val PER: 0.2759
2026-01-08 18:49:19,307: t15.2025.04.13 val PER: 0.2183
2026-01-08 18:49:19,450: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_18500
2026-01-08 18:49:28,163: Train batch 18600: loss: 13.34 grad norm: 59.02 time: 0.070
2026-01-08 18:49:45,534: Train batch 18800: loss: 9.24 grad norm: 50.73 time: 0.066
2026-01-08 18:50:02,940: Train batch 19000: loss: 9.20 grad norm: 45.60 time: 0.067
2026-01-08 18:50:02,943: Running test after training batch: 19000
2026-01-08 18:50:03,054: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:50:08,104: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:50:08,167: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost cents
2026-01-08 18:50:20,170: Val batch 19000: PER (avg): 0.1491 CTC Loss (avg): 15.3419 WER(5gram): 15.58% (n=256) time: 17.217
2026-01-08 18:50:20,172: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 18:50:20,175: t15.2023.08.13 val PER: 0.1040
2026-01-08 18:50:20,177: t15.2023.08.18 val PER: 0.1065
2026-01-08 18:50:20,179: t15.2023.08.20 val PER: 0.1080
2026-01-08 18:50:20,181: t15.2023.08.25 val PER: 0.0934
2026-01-08 18:50:20,182: t15.2023.08.27 val PER: 0.1929
2026-01-08 18:50:20,184: t15.2023.09.01 val PER: 0.0731
2026-01-08 18:50:20,186: t15.2023.09.03 val PER: 0.1603
2026-01-08 18:50:20,188: t15.2023.09.24 val PER: 0.1262
2026-01-08 18:50:20,190: t15.2023.09.29 val PER: 0.1385
2026-01-08 18:50:20,192: t15.2023.10.01 val PER: 0.1678
2026-01-08 18:50:20,193: t15.2023.10.06 val PER: 0.0904
2026-01-08 18:50:20,195: t15.2023.10.08 val PER: 0.2300
2026-01-08 18:50:20,197: t15.2023.10.13 val PER: 0.2056
2026-01-08 18:50:20,199: t15.2023.10.15 val PER: 0.1543
2026-01-08 18:50:20,201: t15.2023.10.20 val PER: 0.1846
2026-01-08 18:50:20,203: t15.2023.10.22 val PER: 0.1114
2026-01-08 18:50:20,204: t15.2023.11.03 val PER: 0.1832
2026-01-08 18:50:20,206: t15.2023.11.04 val PER: 0.0375
2026-01-08 18:50:20,208: t15.2023.11.17 val PER: 0.0327
2026-01-08 18:50:20,210: t15.2023.11.19 val PER: 0.0339
2026-01-08 18:50:20,211: t15.2023.11.26 val PER: 0.1152
2026-01-08 18:50:20,213: t15.2023.12.03 val PER: 0.0987
2026-01-08 18:50:20,215: t15.2023.12.08 val PER: 0.0919
2026-01-08 18:50:20,216: t15.2023.12.10 val PER: 0.0894
2026-01-08 18:50:20,218: t15.2023.12.17 val PER: 0.1258
2026-01-08 18:50:20,220: t15.2023.12.29 val PER: 0.1318
2026-01-08 18:50:20,221: t15.2024.02.25 val PER: 0.1222
2026-01-08 18:50:20,223: t15.2024.03.08 val PER: 0.2333
2026-01-08 18:50:20,225: t15.2024.03.15 val PER: 0.2014
2026-01-08 18:50:20,226: t15.2024.03.17 val PER: 0.1402
2026-01-08 18:50:20,228: t15.2024.05.10 val PER: 0.1575
2026-01-08 18:50:20,230: t15.2024.06.14 val PER: 0.1593
2026-01-08 18:50:20,231: t15.2024.07.19 val PER: 0.2301
2026-01-08 18:50:20,233: t15.2024.07.21 val PER: 0.0938
2026-01-08 18:50:20,235: t15.2024.07.28 val PER: 0.1397
2026-01-08 18:50:20,236: t15.2025.01.10 val PER: 0.3044
2026-01-08 18:50:20,240: t15.2025.01.12 val PER: 0.1432
2026-01-08 18:50:20,241: t15.2025.03.14 val PER: 0.3506
2026-01-08 18:50:20,243: t15.2025.03.16 val PER: 0.1885
2026-01-08 18:50:20,244: t15.2025.03.30 val PER: 0.2644
2026-01-08 18:50:20,246: t15.2025.04.13 val PER: 0.2168
2026-01-08 18:50:20,384: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_19000
2026-01-08 18:50:37,337: Train batch 19200: loss: 6.85 grad norm: 46.60 time: 0.065
2026-01-08 18:50:55,320: Train batch 19400: loss: 5.31 grad norm: 37.92 time: 0.054
2026-01-08 18:51:04,677: Running test after training batch: 19500
2026-01-08 18:51:04,817: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:51:10,281: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:51:10,340: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost cents
2026-01-08 18:51:22,626: Val batch 19500: PER (avg): 0.1491 CTC Loss (avg): 15.3806 WER(5gram): 15.71% (n=256) time: 17.946
2026-01-08 18:51:22,628: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 18:51:22,631: t15.2023.08.13 val PER: 0.1102
2026-01-08 18:51:22,633: t15.2023.08.18 val PER: 0.1115
2026-01-08 18:51:22,635: t15.2023.08.20 val PER: 0.1056
2026-01-08 18:51:22,637: t15.2023.08.25 val PER: 0.0934
2026-01-08 18:51:22,638: t15.2023.08.27 val PER: 0.1913
2026-01-08 18:51:22,640: t15.2023.09.01 val PER: 0.0731
2026-01-08 18:51:22,642: t15.2023.09.03 val PER: 0.1603
2026-01-08 18:51:22,643: t15.2023.09.24 val PER: 0.1262
2026-01-08 18:51:22,645: t15.2023.09.29 val PER: 0.1334
2026-01-08 18:51:22,647: t15.2023.10.01 val PER: 0.1671
2026-01-08 18:51:22,648: t15.2023.10.06 val PER: 0.0840
2026-01-08 18:51:22,650: t15.2023.10.08 val PER: 0.2287
2026-01-08 18:51:22,652: t15.2023.10.13 val PER: 0.2064
2026-01-08 18:51:22,653: t15.2023.10.15 val PER: 0.1549
2026-01-08 18:51:22,655: t15.2023.10.20 val PER: 0.1745
2026-01-08 18:51:22,657: t15.2023.10.22 val PER: 0.1125
2026-01-08 18:51:22,659: t15.2023.11.03 val PER: 0.1825
2026-01-08 18:51:22,661: t15.2023.11.04 val PER: 0.0375
2026-01-08 18:51:22,662: t15.2023.11.17 val PER: 0.0358
2026-01-08 18:51:22,664: t15.2023.11.19 val PER: 0.0379
2026-01-08 18:51:22,666: t15.2023.11.26 val PER: 0.1145
2026-01-08 18:51:22,668: t15.2023.12.03 val PER: 0.0987
2026-01-08 18:51:22,670: t15.2023.12.08 val PER: 0.0952
2026-01-08 18:51:22,671: t15.2023.12.10 val PER: 0.0854
2026-01-08 18:51:22,673: t15.2023.12.17 val PER: 0.1268
2026-01-08 18:51:22,675: t15.2023.12.29 val PER: 0.1270
2026-01-08 18:51:22,676: t15.2024.02.25 val PER: 0.1180
2026-01-08 18:51:22,678: t15.2024.03.08 val PER: 0.2376
2026-01-08 18:51:22,679: t15.2024.03.15 val PER: 0.2039
2026-01-08 18:51:22,681: t15.2024.03.17 val PER: 0.1374
2026-01-08 18:51:22,682: t15.2024.05.10 val PER: 0.1664
2026-01-08 18:51:22,684: t15.2024.06.14 val PER: 0.1562
2026-01-08 18:51:22,686: t15.2024.07.19 val PER: 0.2314
2026-01-08 18:51:22,687: t15.2024.07.21 val PER: 0.0924
2026-01-08 18:51:22,689: t15.2024.07.28 val PER: 0.1382
2026-01-08 18:51:22,690: t15.2025.01.10 val PER: 0.2961
2026-01-08 18:51:22,692: t15.2025.01.12 val PER: 0.1447
2026-01-08 18:51:22,694: t15.2025.03.14 val PER: 0.3491
2026-01-08 18:51:22,695: t15.2025.03.16 val PER: 0.1924
2026-01-08 18:51:22,697: t15.2025.03.30 val PER: 0.2701
2026-01-08 18:51:22,698: t15.2025.04.13 val PER: 0.2240
2026-01-08 18:51:22,841: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_19500
2026-01-08 18:51:31,920: Train batch 19600: loss: 8.57 grad norm: 48.60 time: 0.058
2026-01-08 18:51:50,431: Train batch 19800: loss: 8.12 grad norm: 49.21 time: 0.056
2026-01-08 18:52:08,848: Running test after training batch: 19999
2026-01-08 18:52:08,941: WER debug GT example: You can see the code at this point as well.
2026-01-08 18:52:13,819: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 18:52:13,879: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost cents
2026-01-08 18:52:26,413: Val batch 19999: PER (avg): 0.1491 CTC Loss (avg): 15.3055 WER(5gram): 15.97% (n=256) time: 17.562
2026-01-08 18:52:26,415: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 18:52:26,418: t15.2023.08.13 val PER: 0.1071
2026-01-08 18:52:26,419: t15.2023.08.18 val PER: 0.1073
2026-01-08 18:52:26,421: t15.2023.08.20 val PER: 0.1080
2026-01-08 18:52:26,423: t15.2023.08.25 val PER: 0.0964
2026-01-08 18:52:26,424: t15.2023.08.27 val PER: 0.1865
2026-01-08 18:52:26,426: t15.2023.09.01 val PER: 0.0714
2026-01-08 18:52:26,428: t15.2023.09.03 val PER: 0.1651
2026-01-08 18:52:26,430: t15.2023.09.24 val PER: 0.1262
2026-01-08 18:52:26,432: t15.2023.09.29 val PER: 0.1347
2026-01-08 18:52:26,433: t15.2023.10.01 val PER: 0.1684
2026-01-08 18:52:26,435: t15.2023.10.06 val PER: 0.0936
2026-01-08 18:52:26,436: t15.2023.10.08 val PER: 0.2260
2026-01-08 18:52:26,438: t15.2023.10.13 val PER: 0.2087
2026-01-08 18:52:26,440: t15.2023.10.15 val PER: 0.1575
2026-01-08 18:52:26,441: t15.2023.10.20 val PER: 0.1779
2026-01-08 18:52:26,443: t15.2023.10.22 val PER: 0.1102
2026-01-08 18:52:26,447: t15.2023.11.03 val PER: 0.1818
2026-01-08 18:52:26,449: t15.2023.11.04 val PER: 0.0341
2026-01-08 18:52:26,451: t15.2023.11.17 val PER: 0.0373
2026-01-08 18:52:26,453: t15.2023.11.19 val PER: 0.0339
2026-01-08 18:52:26,454: t15.2023.11.26 val PER: 0.1109
2026-01-08 18:52:26,456: t15.2023.12.03 val PER: 0.0998
2026-01-08 18:52:26,457: t15.2023.12.08 val PER: 0.0905
2026-01-08 18:52:26,459: t15.2023.12.10 val PER: 0.0880
2026-01-08 18:52:26,461: t15.2023.12.17 val PER: 0.1268
2026-01-08 18:52:26,462: t15.2023.12.29 val PER: 0.1242
2026-01-08 18:52:26,464: t15.2024.02.25 val PER: 0.1180
2026-01-08 18:52:26,465: t15.2024.03.08 val PER: 0.2390
2026-01-08 18:52:26,467: t15.2024.03.15 val PER: 0.2039
2026-01-08 18:52:26,468: t15.2024.03.17 val PER: 0.1409
2026-01-08 18:52:26,470: t15.2024.05.10 val PER: 0.1590
2026-01-08 18:52:26,471: t15.2024.06.14 val PER: 0.1577
2026-01-08 18:52:26,473: t15.2024.07.19 val PER: 0.2314
2026-01-08 18:52:26,474: t15.2024.07.21 val PER: 0.0917
2026-01-08 18:52:26,476: t15.2024.07.28 val PER: 0.1382
2026-01-08 18:52:26,477: t15.2025.01.10 val PER: 0.3113
2026-01-08 18:52:26,479: t15.2025.01.12 val PER: 0.1424
2026-01-08 18:52:26,481: t15.2025.03.14 val PER: 0.3491
2026-01-08 18:52:26,483: t15.2025.03.16 val PER: 0.1898
2026-01-08 18:52:26,484: t15.2025.03.30 val PER: 0.2713
2026-01-08 18:52:26,486: t15.2025.04.13 val PER: 0.2183
2026-01-08 18:52:26,625: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_19999
2026-01-08 18:52:27,168: Best avg val PER achieved: 0.16076
2026-01-08 18:52:27,170: Total training time: 46.71 minutes

=== RUN stepdrop_15k.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k
2026-01-08 18:54:22,137: Using device: cuda:0
