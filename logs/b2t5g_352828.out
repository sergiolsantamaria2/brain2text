TMPDIR=/home/e12511253/tmp
JOB_TMP=/home/e12511253/tmp/e12511253_b2t_352828
TORCH_EXTENSIONS_DIR=/home/e12511253/tmp/e12511253_b2t_352828/torch_extensions
WANDB_DIR=/home/e12511253/tmp/e12511253_b2t_352828/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/home/e12511253/tmp/e12511253_b2t_352828/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan 10 17:14 /home/e12511253/tmp/e12511253_b2t_352828/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
OUT_ROOT=/home/e12511253/Brain2Text/brain2text/trained_models
==============================================
Job: b2t5g  ID: 352828
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_5gram_only.yaml
Folders: configs/experiments/diphones
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/diphones ==========
Num configs: 4

=== RUN diphone_base.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base
2026-01-10 17:14:51,850: Using device: cuda:0
2026-01-10 17:18:38,503: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-10 17:18:38,507: Diphone mode ENABLED: n_classes changed from 41 to 1601
2026-01-10 17:18:38,535: Using 45 sessions after filtering (from 45).
2026-01-10 17:18:38,932: Using torch.compile (if available)
2026-01-10 17:18:38,933: torch.compile not available (torch<2.0). Skipping.
2026-01-10 17:18:38,933: Initialized RNN decoding model
2026-01-10 17:18:38,933: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Identity()
  (out): Linear(in_features=768, out_features=1601, bias=True)
)
2026-01-10 17:18:38,933: Model has 45,514,817 parameters
2026-01-10 17:18:38,934: Model has 11,819,520 day-specific parameters | 25.97% of total parameters
2026-01-10 17:18:40,221: Successfully initialized datasets
2026-01-10 17:18:40,224: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-10 17:18:41,693: Train batch 0: loss: 1387.52 grad norm: 203.28 time: 0.166
2026-01-10 17:18:41,694: Running test after training batch: 0
2026-01-10 17:18:41,809: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:18:48,587: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-10 17:18:49,678: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-10 17:23:09,771: Val batch 0: PER (avg): 4.1038 CTC Loss (avg): 1560.2442 WER(5gram): 100.00% (n=256) time: 268.074
2026-01-10 17:23:09,776: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=0
2026-01-10 17:23:09,776: t15.2023.08.13 val PER: 3.3836
2026-01-10 17:23:09,776: t15.2023.08.18 val PER: 3.6588
2026-01-10 17:23:09,776: t15.2023.08.20 val PER: 3.6402
2026-01-10 17:23:09,776: t15.2023.08.25 val PER: 3.6958
2026-01-10 17:23:09,776: t15.2023.08.27 val PER: 3.4630
2026-01-10 17:23:09,776: t15.2023.09.01 val PER: 3.7679
2026-01-10 17:23:09,776: t15.2023.09.03 val PER: 3.6413
2026-01-10 17:23:09,776: t15.2023.09.24 val PER: 4.2791
2026-01-10 17:23:09,777: t15.2023.09.29 val PER: 4.3095
2026-01-10 17:23:09,777: t15.2023.10.01 val PER: 3.3144
2026-01-10 17:23:09,777: t15.2023.10.06 val PER: 4.1130
2026-01-10 17:23:09,777: t15.2023.10.08 val PER: 3.0920
2026-01-10 17:23:09,777: t15.2023.10.13 val PER: 3.8976
2026-01-10 17:23:09,777: t15.2023.10.15 val PER: 4.3441
2026-01-10 17:23:09,777: t15.2023.10.20 val PER: 4.5671
2026-01-10 17:23:09,777: t15.2023.10.22 val PER: 4.3497
2026-01-10 17:23:09,777: t15.2023.11.03 val PER: 4.6208
2026-01-10 17:23:09,777: t15.2023.11.04 val PER: 5.5631
2026-01-10 17:23:09,777: t15.2023.11.17 val PER: 5.9020
2026-01-10 17:23:09,777: t15.2023.11.19 val PER: 4.5768
2026-01-10 17:23:09,777: t15.2023.11.26 val PER: 4.6058
2026-01-10 17:23:09,777: t15.2023.12.03 val PER: 4.3340
2026-01-10 17:23:09,777: t15.2023.12.08 val PER: 4.5972
2026-01-10 17:23:09,778: t15.2023.12.10 val PER: 5.0696
2026-01-10 17:23:09,778: t15.2023.12.17 val PER: 3.7464
2026-01-10 17:23:09,778: t15.2023.12.29 val PER: 4.1050
2026-01-10 17:23:09,778: t15.2024.02.25 val PER: 3.8483
2026-01-10 17:23:09,778: t15.2024.03.08 val PER: 3.8321
2026-01-10 17:23:09,778: t15.2024.03.15 val PER: 3.7323
2026-01-10 17:23:09,778: t15.2024.03.17 val PER: 3.9700
2026-01-10 17:23:09,778: t15.2024.05.10 val PER: 3.8663
2026-01-10 17:23:09,778: t15.2024.06.14 val PER: 4.3991
2026-01-10 17:23:09,778: t15.2024.07.19 val PER: 3.0310
2026-01-10 17:23:09,778: t15.2024.07.21 val PER: 4.6076
2026-01-10 17:23:09,778: t15.2024.07.28 val PER: 4.8051
2026-01-10 17:23:09,778: t15.2025.01.10 val PER: 2.8072
2026-01-10 17:23:09,778: t15.2025.01.12 val PER: 5.3426
2026-01-10 17:23:09,778: t15.2025.03.14 val PER: 2.8580
2026-01-10 17:23:09,778: t15.2025.03.16 val PER: 5.0432
2026-01-10 17:23:09,778: t15.2025.03.30 val PER: 4.0264
2026-01-10 17:23:09,779: t15.2025.04.13 val PER: 4.4736
2026-01-10 17:23:09,780: New best val WER(5gram) inf% --> 100.00%
2026-01-10 17:23:10,830: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_0
2026-01-10 17:23:27,621: Train batch 200: loss: 169.35 grad norm: 40.32 time: 0.060
2026-01-10 17:23:44,293: Train batch 400: loss: 138.72 grad norm: 49.28 time: 0.068
2026-01-10 17:23:52,613: Running test after training batch: 500
2026-01-10 17:23:52,734: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:23:58,614: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-10 17:23:58,640: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-10 17:24:04,029: Val batch 500: PER (avg): 0.9926 CTC Loss (avg): 160.5410 WER(5gram): 100.00% (n=256) time: 11.413
2026-01-10 17:24:04,031: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=0
2026-01-10 17:24:04,034: t15.2023.08.13 val PER: 0.9938
2026-01-10 17:24:04,035: t15.2023.08.18 val PER: 0.9925
2026-01-10 17:24:04,037: t15.2023.08.20 val PER: 0.9968
2026-01-10 17:24:04,039: t15.2023.08.25 val PER: 0.9925
2026-01-10 17:24:04,041: t15.2023.08.27 val PER: 0.9936
2026-01-10 17:24:04,043: t15.2023.09.01 val PER: 0.9927
2026-01-10 17:24:04,044: t15.2023.09.03 val PER: 0.9905
2026-01-10 17:24:04,046: t15.2023.09.24 val PER: 0.9964
2026-01-10 17:24:04,048: t15.2023.09.29 val PER: 0.9987
2026-01-10 17:24:04,049: t15.2023.10.01 val PER: 0.9987
2026-01-10 17:24:04,051: t15.2023.10.06 val PER: 0.9914
2026-01-10 17:24:04,053: t15.2023.10.08 val PER: 1.0000
2026-01-10 17:24:04,055: t15.2023.10.13 val PER: 0.9984
2026-01-10 17:24:04,057: t15.2023.10.15 val PER: 0.9954
2026-01-10 17:24:04,058: t15.2023.10.20 val PER: 0.9899
2026-01-10 17:24:04,060: t15.2023.10.22 val PER: 0.9900
2026-01-10 17:24:04,062: t15.2023.11.03 val PER: 0.9885
2026-01-10 17:24:04,063: t15.2023.11.04 val PER: 0.9898
2026-01-10 17:24:04,065: t15.2023.11.17 val PER: 0.9829
2026-01-10 17:24:04,066: t15.2023.11.19 val PER: 0.9900
2026-01-10 17:24:04,068: t15.2023.11.26 val PER: 0.9928
2026-01-10 17:24:04,069: t15.2023.12.03 val PER: 0.9916
2026-01-10 17:24:04,070: t15.2023.12.08 val PER: 0.9933
2026-01-10 17:24:04,072: t15.2023.12.10 val PER: 0.9947
2026-01-10 17:24:04,073: t15.2023.12.17 val PER: 0.9917
2026-01-10 17:24:04,075: t15.2023.12.29 val PER: 0.9911
2026-01-10 17:24:04,076: t15.2024.02.25 val PER: 0.9958
2026-01-10 17:24:04,078: t15.2024.03.08 val PER: 0.9900
2026-01-10 17:24:04,079: t15.2024.03.15 val PER: 0.9962
2026-01-10 17:24:04,080: t15.2024.03.17 val PER: 0.9972
2026-01-10 17:24:04,082: t15.2024.05.10 val PER: 0.9911
2026-01-10 17:24:04,083: t15.2024.06.14 val PER: 0.9905
2026-01-10 17:24:04,085: t15.2024.07.19 val PER: 0.9921
2026-01-10 17:24:04,088: t15.2024.07.21 val PER: 0.9945
2026-01-10 17:24:04,089: t15.2024.07.28 val PER: 0.9912
2026-01-10 17:24:04,091: t15.2025.01.10 val PER: 0.9835
2026-01-10 17:24:04,093: t15.2025.01.12 val PER: 0.9892
2026-01-10 17:24:04,094: t15.2025.03.14 val PER: 0.9822
2026-01-10 17:24:04,095: t15.2025.03.16 val PER: 0.9882
2026-01-10 17:24:04,097: t15.2025.03.30 val PER: 0.9828
2026-01-10 17:24:04,098: t15.2025.04.13 val PER: 0.9829
2026-01-10 17:24:04,235: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_500
2026-01-10 17:24:12,664: Train batch 600: loss: 148.00 grad norm: 120.75 time: 0.083
2026-01-10 17:24:29,284: Train batch 800: loss: 143.07 grad norm: 76.05 time: 0.062
2026-01-10 17:24:46,290: Train batch 1000: loss: 127.08 grad norm: 33.12 time: 0.071
2026-01-10 17:24:46,292: Running test after training batch: 1000
2026-01-10 17:24:46,398: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:24:52,210: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-10 17:24:52,238: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-10 17:24:57,163: Val batch 1000: PER (avg): 0.9998 CTC Loss (avg): 153.0103 WER(5gram): 100.00% (n=256) time: 10.868
2026-01-10 17:24:57,166: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=0
2026-01-10 17:24:57,169: t15.2023.08.13 val PER: 1.0000
2026-01-10 17:24:57,171: t15.2023.08.18 val PER: 1.0000
2026-01-10 17:24:57,173: t15.2023.08.20 val PER: 0.9992
2026-01-10 17:24:57,175: t15.2023.08.25 val PER: 0.9955
2026-01-10 17:24:57,177: t15.2023.08.27 val PER: 0.9984
2026-01-10 17:24:57,179: t15.2023.09.01 val PER: 0.9992
2026-01-10 17:24:57,181: t15.2023.09.03 val PER: 1.0000
2026-01-10 17:24:57,183: t15.2023.09.24 val PER: 1.0000
2026-01-10 17:24:57,184: t15.2023.09.29 val PER: 1.0000
2026-01-10 17:24:57,186: t15.2023.10.01 val PER: 0.9993
2026-01-10 17:24:57,188: t15.2023.10.06 val PER: 1.0000
2026-01-10 17:24:57,190: t15.2023.10.08 val PER: 0.9986
2026-01-10 17:24:57,191: t15.2023.10.13 val PER: 1.0000
2026-01-10 17:24:57,193: t15.2023.10.15 val PER: 0.9993
2026-01-10 17:24:57,194: t15.2023.10.20 val PER: 1.0000
2026-01-10 17:24:57,196: t15.2023.10.22 val PER: 1.0000
2026-01-10 17:24:57,197: t15.2023.11.03 val PER: 1.0000
2026-01-10 17:24:57,199: t15.2023.11.04 val PER: 1.0000
2026-01-10 17:24:57,200: t15.2023.11.17 val PER: 1.0000
2026-01-10 17:24:57,202: t15.2023.11.19 val PER: 1.0000
2026-01-10 17:24:57,203: t15.2023.11.26 val PER: 1.0000
2026-01-10 17:24:57,205: t15.2023.12.03 val PER: 1.0000
2026-01-10 17:24:57,206: t15.2023.12.08 val PER: 1.0000
2026-01-10 17:24:57,208: t15.2023.12.10 val PER: 1.0000
2026-01-10 17:24:57,209: t15.2023.12.17 val PER: 1.0000
2026-01-10 17:24:57,210: t15.2023.12.29 val PER: 1.0000
2026-01-10 17:24:57,212: t15.2024.02.25 val PER: 1.0000
2026-01-10 17:24:57,213: t15.2024.03.08 val PER: 1.0000
2026-01-10 17:24:57,215: t15.2024.03.15 val PER: 1.0000
2026-01-10 17:24:57,216: t15.2024.03.17 val PER: 1.0000
2026-01-10 17:24:57,219: t15.2024.05.10 val PER: 1.0000
2026-01-10 17:24:57,220: t15.2024.06.14 val PER: 1.0000
2026-01-10 17:24:57,222: t15.2024.07.19 val PER: 1.0000
2026-01-10 17:24:57,224: t15.2024.07.21 val PER: 1.0000
2026-01-10 17:24:57,225: t15.2024.07.28 val PER: 1.0000
2026-01-10 17:24:57,226: t15.2025.01.10 val PER: 1.0000
2026-01-10 17:24:57,228: t15.2025.01.12 val PER: 1.0000
2026-01-10 17:24:57,229: t15.2025.03.14 val PER: 1.0000
2026-01-10 17:24:57,230: t15.2025.03.16 val PER: 1.0000
2026-01-10 17:24:57,232: t15.2025.03.30 val PER: 1.0000
2026-01-10 17:24:57,233: t15.2025.04.13 val PER: 1.0000
2026-01-10 17:24:57,370: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_1000
2026-01-10 17:25:13,923: Train batch 1200: loss: 139.57 grad norm: 53.80 time: 0.073
2026-01-10 17:25:31,047: Train batch 1400: loss: 123.24 grad norm: 58.18 time: 0.065
2026-01-10 17:25:39,370: Running test after training batch: 1500
2026-01-10 17:25:39,509: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:25:45,300: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-10 17:25:45,324: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-10 17:25:50,595: Val batch 1500: PER (avg): 0.7724 CTC Loss (avg): 136.7754 WER(5gram): 100.00% (n=256) time: 11.222
2026-01-10 17:25:50,597: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=0
2026-01-10 17:25:50,600: t15.2023.08.13 val PER: 0.7609
2026-01-10 17:25:50,601: t15.2023.08.18 val PER: 0.7561
2026-01-10 17:25:50,603: t15.2023.08.20 val PER: 0.7609
2026-01-10 17:25:50,604: t15.2023.08.25 val PER: 0.7530
2026-01-10 17:25:50,606: t15.2023.08.27 val PER: 0.7797
2026-01-10 17:25:50,608: t15.2023.09.01 val PER: 0.7549
2026-01-10 17:25:50,609: t15.2023.09.03 val PER: 0.7755
2026-01-10 17:25:50,611: t15.2023.09.24 val PER: 0.7585
2026-01-10 17:25:50,612: t15.2023.09.29 val PER: 0.7690
2026-01-10 17:25:50,614: t15.2023.10.01 val PER: 0.7867
2026-01-10 17:25:50,615: t15.2023.10.06 val PER: 0.7578
2026-01-10 17:25:50,617: t15.2023.10.08 val PER: 0.7984
2026-01-10 17:25:50,618: t15.2023.10.13 val PER: 0.8099
2026-01-10 17:25:50,620: t15.2023.10.15 val PER: 0.7805
2026-01-10 17:25:50,621: t15.2023.10.20 val PER: 0.7584
2026-01-10 17:25:50,623: t15.2023.10.22 val PER: 0.7595
2026-01-10 17:25:50,624: t15.2023.11.03 val PER: 0.7693
2026-01-10 17:25:50,626: t15.2023.11.04 val PER: 0.7099
2026-01-10 17:25:50,627: t15.2023.11.17 val PER: 0.7247
2026-01-10 17:25:50,629: t15.2023.11.19 val PER: 0.7305
2026-01-10 17:25:50,630: t15.2023.11.26 val PER: 0.7942
2026-01-10 17:25:50,632: t15.2023.12.03 val PER: 0.7773
2026-01-10 17:25:50,633: t15.2023.12.08 val PER: 0.7716
2026-01-10 17:25:50,635: t15.2023.12.10 val PER: 0.7740
2026-01-10 17:25:50,636: t15.2023.12.17 val PER: 0.7807
2026-01-10 17:25:50,638: t15.2023.12.29 val PER: 0.7680
2026-01-10 17:25:50,639: t15.2024.02.25 val PER: 0.7669
2026-01-10 17:25:50,641: t15.2024.03.08 val PER: 0.7809
2026-01-10 17:25:50,643: t15.2024.03.15 val PER: 0.7799
2026-01-10 17:25:50,644: t15.2024.03.17 val PER: 0.7601
2026-01-10 17:25:50,646: t15.2024.05.10 val PER: 0.7608
2026-01-10 17:25:50,648: t15.2024.06.14 val PER: 0.7492
2026-01-10 17:25:50,649: t15.2024.07.19 val PER: 0.7950
2026-01-10 17:25:50,651: t15.2024.07.21 val PER: 0.7524
2026-01-10 17:25:50,653: t15.2024.07.28 val PER: 0.7676
2026-01-10 17:25:50,655: t15.2025.01.10 val PER: 0.8154
2026-01-10 17:25:50,657: t15.2025.01.12 val PER: 0.7644
2026-01-10 17:25:50,659: t15.2025.03.14 val PER: 0.8092
2026-01-10 17:25:50,660: t15.2025.03.16 val PER: 0.7723
2026-01-10 17:25:50,662: t15.2025.03.30 val PER: 0.8161
2026-01-10 17:25:50,663: t15.2025.04.13 val PER: 0.7689
2026-01-10 17:25:50,803: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_1500
2026-01-10 17:25:58,954: Train batch 1600: loss: 126.70 grad norm: 32.66 time: 0.069
2026-01-10 17:26:15,718: Train batch 1800: loss: 119.56 grad norm: 97.98 time: 0.092
2026-01-10 17:26:32,611: Train batch 2000: loss: 114.71 grad norm: 41.09 time: 0.072
2026-01-10 17:26:32,613: Running test after training batch: 2000
2026-01-10 17:26:32,715: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:26:38,574: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-10 17:26:38,646: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-10 17:26:50,816: Val batch 2000: PER (avg): 0.7265 CTC Loss (avg): 121.0605 WER(5gram): 99.80% (n=256) time: 18.200
2026-01-10 17:26:50,818: WER lens: avg_true_words=5.99 avg_pred_words=0.01 max_pred_words=1
2026-01-10 17:26:50,821: t15.2023.08.13 val PER: 0.7214
2026-01-10 17:26:50,823: t15.2023.08.18 val PER: 0.7091
2026-01-10 17:26:50,825: t15.2023.08.20 val PER: 0.7014
2026-01-10 17:26:50,827: t15.2023.08.25 val PER: 0.7033
2026-01-10 17:26:50,828: t15.2023.08.27 val PER: 0.7508
2026-01-10 17:26:50,830: t15.2023.09.01 val PER: 0.7062
2026-01-10 17:26:50,832: t15.2023.09.03 val PER: 0.7257
2026-01-10 17:26:50,834: t15.2023.09.24 val PER: 0.7233
2026-01-10 17:26:50,835: t15.2023.09.29 val PER: 0.7122
2026-01-10 17:26:50,837: t15.2023.10.01 val PER: 0.7358
2026-01-10 17:26:50,838: t15.2023.10.06 val PER: 0.7158
2026-01-10 17:26:50,840: t15.2023.10.08 val PER: 0.7456
2026-01-10 17:26:50,842: t15.2023.10.13 val PER: 0.7665
2026-01-10 17:26:50,843: t15.2023.10.15 val PER: 0.7238
2026-01-10 17:26:50,845: t15.2023.10.20 val PER: 0.6980
2026-01-10 17:26:50,847: t15.2023.10.22 val PER: 0.7238
2026-01-10 17:26:50,850: t15.2023.11.03 val PER: 0.7232
2026-01-10 17:26:50,852: t15.2023.11.04 val PER: 0.6689
2026-01-10 17:26:50,854: t15.2023.11.17 val PER: 0.6812
2026-01-10 17:26:50,855: t15.2023.11.19 val PER: 0.6926
2026-01-10 17:26:50,857: t15.2023.11.26 val PER: 0.7565
2026-01-10 17:26:50,858: t15.2023.12.03 val PER: 0.7353
2026-01-10 17:26:50,860: t15.2023.12.08 val PER: 0.7277
2026-01-10 17:26:50,862: t15.2023.12.10 val PER: 0.7175
2026-01-10 17:26:50,864: t15.2023.12.17 val PER: 0.7235
2026-01-10 17:26:50,865: t15.2023.12.29 val PER: 0.7200
2026-01-10 17:26:50,867: t15.2024.02.25 val PER: 0.7205
2026-01-10 17:26:50,868: t15.2024.03.08 val PER: 0.7340
2026-01-10 17:26:50,871: t15.2024.03.15 val PER: 0.7361
2026-01-10 17:26:50,872: t15.2024.03.17 val PER: 0.7197
2026-01-10 17:26:50,874: t15.2024.05.10 val PER: 0.7132
2026-01-10 17:26:50,875: t15.2024.06.14 val PER: 0.7050
2026-01-10 17:26:50,877: t15.2024.07.19 val PER: 0.7436
2026-01-10 17:26:50,878: t15.2024.07.21 val PER: 0.7090
2026-01-10 17:26:50,879: t15.2024.07.28 val PER: 0.7324
2026-01-10 17:26:50,881: t15.2025.01.10 val PER: 0.7603
2026-01-10 17:26:50,882: t15.2025.01.12 val PER: 0.7244
2026-01-10 17:26:50,884: t15.2025.03.14 val PER: 0.7678
2026-01-10 17:26:50,885: t15.2025.03.16 val PER: 0.7382
2026-01-10 17:26:50,887: t15.2025.03.30 val PER: 0.7644
2026-01-10 17:26:50,888: t15.2025.04.13 val PER: 0.7347
2026-01-10 17:26:50,890: New best val WER(5gram) 100.00% --> 99.80%
2026-01-10 17:26:51,990: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_2000
2026-01-10 17:27:08,646: Train batch 2200: loss: 106.91 grad norm: 40.48 time: 0.065
2026-01-10 17:27:25,364: Train batch 2400: loss: 99.58 grad norm: 39.80 time: 0.057
2026-01-10 17:27:33,876: Running test after training batch: 2500
2026-01-10 17:27:34,004: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:27:40,019: WER debug example
  GT : you can see the code at this point as well
  PR : a
2026-01-10 17:27:40,201: WER debug example
  GT : how does it keep the cost down
  PR : a
2026-01-10 17:28:19,459: Val batch 2500: PER (avg): 0.5862 CTC Loss (avg): 108.7600 WER(5gram): 98.37% (n=256) time: 45.581
2026-01-10 17:28:19,461: WER lens: avg_true_words=5.99 avg_pred_words=0.56 max_pred_words=6
2026-01-10 17:28:19,464: t15.2023.08.13 val PER: 0.5832
2026-01-10 17:28:19,466: t15.2023.08.18 val PER: 0.5574
2026-01-10 17:28:19,468: t15.2023.08.20 val PER: 0.5473
2026-01-10 17:28:19,470: t15.2023.08.25 val PER: 0.5602
2026-01-10 17:28:19,471: t15.2023.08.27 val PER: 0.6367
2026-01-10 17:28:19,473: t15.2023.09.01 val PER: 0.5390
2026-01-10 17:28:19,474: t15.2023.09.03 val PER: 0.5843
2026-01-10 17:28:19,476: t15.2023.09.24 val PER: 0.5728
2026-01-10 17:28:19,477: t15.2023.09.29 val PER: 0.5743
2026-01-10 17:28:19,479: t15.2023.10.01 val PER: 0.5971
2026-01-10 17:28:19,480: t15.2023.10.06 val PER: 0.5673
2026-01-10 17:28:19,482: t15.2023.10.08 val PER: 0.6089
2026-01-10 17:28:19,483: t15.2023.10.13 val PER: 0.6524
2026-01-10 17:28:19,485: t15.2023.10.15 val PER: 0.5801
2026-01-10 17:28:19,487: t15.2023.10.20 val PER: 0.5638
2026-01-10 17:28:19,488: t15.2023.10.22 val PER: 0.5791
2026-01-10 17:28:19,490: t15.2023.11.03 val PER: 0.5787
2026-01-10 17:28:19,491: t15.2023.11.04 val PER: 0.4778
2026-01-10 17:28:19,493: t15.2023.11.17 val PER: 0.4946
2026-01-10 17:28:19,494: t15.2023.11.19 val PER: 0.4930
2026-01-10 17:28:19,496: t15.2023.11.26 val PER: 0.6413
2026-01-10 17:28:19,497: t15.2023.12.03 val PER: 0.5851
2026-01-10 17:28:19,499: t15.2023.12.08 val PER: 0.5819
2026-01-10 17:28:19,500: t15.2023.12.10 val PER: 0.5821
2026-01-10 17:28:19,502: t15.2023.12.17 val PER: 0.5728
2026-01-10 17:28:19,503: t15.2023.12.29 val PER: 0.5951
2026-01-10 17:28:19,505: t15.2024.02.25 val PER: 0.5604
2026-01-10 17:28:19,506: t15.2024.03.08 val PER: 0.5974
2026-01-10 17:28:19,508: t15.2024.03.15 val PER: 0.5910
2026-01-10 17:28:19,510: t15.2024.03.17 val PER: 0.5774
2026-01-10 17:28:19,511: t15.2024.05.10 val PER: 0.5750
2026-01-10 17:28:19,513: t15.2024.06.14 val PER: 0.5536
2026-01-10 17:28:19,514: t15.2024.07.19 val PER: 0.6348
2026-01-10 17:28:19,516: t15.2024.07.21 val PER: 0.5579
2026-01-10 17:28:19,517: t15.2024.07.28 val PER: 0.5787
2026-01-10 17:28:19,519: t15.2025.01.10 val PER: 0.6680
2026-01-10 17:28:19,520: t15.2025.01.12 val PER: 0.5820
2026-01-10 17:28:19,522: t15.2025.03.14 val PER: 0.6435
2026-01-10 17:28:19,524: t15.2025.03.16 val PER: 0.6270
2026-01-10 17:28:19,525: t15.2025.03.30 val PER: 0.6483
2026-01-10 17:28:19,527: t15.2025.04.13 val PER: 0.6020
2026-01-10 17:28:19,528: New best val WER(5gram) 99.80% --> 98.37%
2026-01-10 17:28:20,658: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_2500
2026-01-10 17:28:28,913: Train batch 2600: loss: 104.20 grad norm: 56.74 time: 0.060
2026-01-10 17:28:45,749: Train batch 2800: loss: 88.27 grad norm: 45.87 time: 0.085
2026-01-10 17:29:02,749: Train batch 3000: loss: 94.39 grad norm: 81.41 time: 0.086
2026-01-10 17:29:02,752: Running test after training batch: 3000
2026-01-10 17:29:02,851: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:29:09,307: WER debug example
  GT : you can see the code at this point as well
  PR : a
2026-01-10 17:29:09,511: WER debug example
  GT : how does it keep the cost down
  PR : a
2026-01-10 17:29:51,578: Val batch 3000: PER (avg): 0.5425 CTC Loss (avg): 100.5619 WER(5gram): 97.98% (n=256) time: 48.824
2026-01-10 17:29:51,580: WER lens: avg_true_words=5.99 avg_pred_words=1.21 max_pred_words=8
2026-01-10 17:29:51,583: t15.2023.08.13 val PER: 0.5322
2026-01-10 17:29:51,585: t15.2023.08.18 val PER: 0.5197
2026-01-10 17:29:51,587: t15.2023.08.20 val PER: 0.5044
2026-01-10 17:29:51,588: t15.2023.08.25 val PER: 0.4849
2026-01-10 17:29:51,590: t15.2023.08.27 val PER: 0.5868
2026-01-10 17:29:51,592: t15.2023.09.01 val PER: 0.4854
2026-01-10 17:29:51,593: t15.2023.09.03 val PER: 0.5487
2026-01-10 17:29:51,595: t15.2023.09.24 val PER: 0.5170
2026-01-10 17:29:51,596: t15.2023.09.29 val PER: 0.5137
2026-01-10 17:29:51,598: t15.2023.10.01 val PER: 0.5733
2026-01-10 17:29:51,599: t15.2023.10.06 val PER: 0.5102
2026-01-10 17:29:51,601: t15.2023.10.08 val PER: 0.5656
2026-01-10 17:29:51,602: t15.2023.10.13 val PER: 0.6206
2026-01-10 17:29:51,604: t15.2023.10.15 val PER: 0.5399
2026-01-10 17:29:51,605: t15.2023.10.20 val PER: 0.5436
2026-01-10 17:29:51,607: t15.2023.10.22 val PER: 0.5212
2026-01-10 17:29:51,609: t15.2023.11.03 val PER: 0.5258
2026-01-10 17:29:51,611: t15.2023.11.04 val PER: 0.4164
2026-01-10 17:29:51,613: t15.2023.11.17 val PER: 0.4495
2026-01-10 17:29:51,614: t15.2023.11.19 val PER: 0.4271
2026-01-10 17:29:51,616: t15.2023.11.26 val PER: 0.5906
2026-01-10 17:29:51,618: t15.2023.12.03 val PER: 0.5368
2026-01-10 17:29:51,619: t15.2023.12.08 val PER: 0.5393
2026-01-10 17:29:51,621: t15.2023.12.10 val PER: 0.5480
2026-01-10 17:29:51,622: t15.2023.12.17 val PER: 0.5416
2026-01-10 17:29:51,627: t15.2023.12.29 val PER: 0.5655
2026-01-10 17:29:51,629: t15.2024.02.25 val PER: 0.5070
2026-01-10 17:29:51,630: t15.2024.03.08 val PER: 0.5505
2026-01-10 17:29:51,632: t15.2024.03.15 val PER: 0.5578
2026-01-10 17:29:51,633: t15.2024.03.17 val PER: 0.5446
2026-01-10 17:29:51,635: t15.2024.05.10 val PER: 0.5364
2026-01-10 17:29:51,637: t15.2024.06.14 val PER: 0.5189
2026-01-10 17:29:51,638: t15.2024.07.19 val PER: 0.5887
2026-01-10 17:29:51,640: t15.2024.07.21 val PER: 0.5069
2026-01-10 17:29:51,641: t15.2024.07.28 val PER: 0.5287
2026-01-10 17:29:51,643: t15.2025.01.10 val PER: 0.6129
2026-01-10 17:29:51,645: t15.2025.01.12 val PER: 0.5466
2026-01-10 17:29:51,646: t15.2025.03.14 val PER: 0.6021
2026-01-10 17:29:51,648: t15.2025.03.16 val PER: 0.5890
2026-01-10 17:29:51,650: t15.2025.03.30 val PER: 0.6287
2026-01-10 17:29:51,651: t15.2025.04.13 val PER: 0.5521
2026-01-10 17:29:51,653: New best val WER(5gram) 98.37% --> 97.98%
2026-01-10 17:29:52,786: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_3000
2026-01-10 17:30:09,557: Train batch 3200: loss: 88.78 grad norm: 59.04 time: 0.080
2026-01-10 17:30:26,485: Train batch 3400: loss: 76.57 grad norm: 52.51 time: 0.053
2026-01-10 17:30:34,959: Running test after training batch: 3500
2026-01-10 17:30:35,083: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:30:41,355: WER debug example
  GT : you can see the code at this point as well
  PR : a
2026-01-10 17:30:41,573: WER debug example
  GT : how does it keep the cost down
  PR : a a a a
2026-01-10 17:31:25,337: Val batch 3500: PER (avg): 0.5071 CTC Loss (avg): 94.5977 WER(5gram): 97.39% (n=256) time: 50.376
2026-01-10 17:31:25,340: WER lens: avg_true_words=5.99 avg_pred_words=2.11 max_pred_words=11
2026-01-10 17:31:25,343: t15.2023.08.13 val PER: 0.4896
2026-01-10 17:31:25,344: t15.2023.08.18 val PER: 0.4769
2026-01-10 17:31:25,346: t15.2023.08.20 val PER: 0.4615
2026-01-10 17:31:25,348: t15.2023.08.25 val PER: 0.4639
2026-01-10 17:31:25,350: t15.2023.08.27 val PER: 0.5627
2026-01-10 17:31:25,352: t15.2023.09.01 val PER: 0.4570
2026-01-10 17:31:25,354: t15.2023.09.03 val PER: 0.5083
2026-01-10 17:31:25,356: t15.2023.09.24 val PER: 0.4903
2026-01-10 17:31:25,357: t15.2023.09.29 val PER: 0.4837
2026-01-10 17:31:25,359: t15.2023.10.01 val PER: 0.5376
2026-01-10 17:31:25,361: t15.2023.10.06 val PER: 0.4704
2026-01-10 17:31:25,363: t15.2023.10.08 val PER: 0.5589
2026-01-10 17:31:25,365: t15.2023.10.13 val PER: 0.5826
2026-01-10 17:31:25,367: t15.2023.10.15 val PER: 0.5089
2026-01-10 17:31:25,369: t15.2023.10.20 val PER: 0.4799
2026-01-10 17:31:25,371: t15.2023.10.22 val PER: 0.4844
2026-01-10 17:31:25,373: t15.2023.11.03 val PER: 0.4912
2026-01-10 17:31:25,374: t15.2023.11.04 val PER: 0.3208
2026-01-10 17:31:25,376: t15.2023.11.17 val PER: 0.3795
2026-01-10 17:31:25,378: t15.2023.11.19 val PER: 0.3872
2026-01-10 17:31:25,380: t15.2023.11.26 val PER: 0.5630
2026-01-10 17:31:25,381: t15.2023.12.03 val PER: 0.5021
2026-01-10 17:31:25,383: t15.2023.12.08 val PER: 0.4973
2026-01-10 17:31:25,385: t15.2023.12.10 val PER: 0.5020
2026-01-10 17:31:25,387: t15.2023.12.17 val PER: 0.4927
2026-01-10 17:31:25,388: t15.2023.12.29 val PER: 0.5360
2026-01-10 17:31:25,390: t15.2024.02.25 val PER: 0.4621
2026-01-10 17:31:25,392: t15.2024.03.08 val PER: 0.5363
2026-01-10 17:31:25,393: t15.2024.03.15 val PER: 0.4997
2026-01-10 17:31:25,395: t15.2024.03.17 val PER: 0.5007
2026-01-10 17:31:25,397: t15.2024.05.10 val PER: 0.5111
2026-01-10 17:31:25,398: t15.2024.06.14 val PER: 0.4842
2026-01-10 17:31:25,400: t15.2024.07.19 val PER: 0.5676
2026-01-10 17:31:25,402: t15.2024.07.21 val PER: 0.4621
2026-01-10 17:31:25,403: t15.2024.07.28 val PER: 0.5015
2026-01-10 17:31:25,405: t15.2025.01.10 val PER: 0.5826
2026-01-10 17:31:25,406: t15.2025.01.12 val PER: 0.5266
2026-01-10 17:31:25,408: t15.2025.03.14 val PER: 0.5651
2026-01-10 17:31:25,410: t15.2025.03.16 val PER: 0.5641
2026-01-10 17:31:25,411: t15.2025.03.30 val PER: 0.5966
2026-01-10 17:31:25,413: t15.2025.04.13 val PER: 0.5292
2026-01-10 17:31:25,415: New best val WER(5gram) 97.98% --> 97.39%
2026-01-10 17:31:26,565: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_3500
2026-01-10 17:31:35,062: Train batch 3600: loss: 82.32 grad norm: 59.76 time: 0.071
2026-01-10 17:31:51,698: Train batch 3800: loss: 90.79 grad norm: 61.89 time: 0.072
2026-01-10 17:32:08,427: Train batch 4000: loss: 70.83 grad norm: 53.90 time: 0.061
2026-01-10 17:32:08,430: Running test after training batch: 4000
2026-01-10 17:32:08,536: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:32:14,543: WER debug example
  GT : you can see the code at this point as well
  PR : a this is a
2026-01-10 17:32:14,774: WER debug example
  GT : how does it keep the cost down
  PR : a a a a a a
2026-01-10 17:33:00,532: Val batch 4000: PER (avg): 0.4794 CTC Loss (avg): 89.4806 WER(5gram): 96.09% (n=256) time: 52.100
2026-01-10 17:33:00,535: WER lens: avg_true_words=5.99 avg_pred_words=3.65 max_pred_words=13
2026-01-10 17:33:00,538: t15.2023.08.13 val PER: 0.4751
2026-01-10 17:33:00,540: t15.2023.08.18 val PER: 0.4543
2026-01-10 17:33:00,542: t15.2023.08.20 val PER: 0.4321
2026-01-10 17:33:00,544: t15.2023.08.25 val PER: 0.4157
2026-01-10 17:33:00,545: t15.2023.08.27 val PER: 0.5161
2026-01-10 17:33:00,547: t15.2023.09.01 val PER: 0.4172
2026-01-10 17:33:00,549: t15.2023.09.03 val PER: 0.4739
2026-01-10 17:33:00,550: t15.2023.09.24 val PER: 0.4466
2026-01-10 17:33:00,552: t15.2023.09.29 val PER: 0.4378
2026-01-10 17:33:00,554: t15.2023.10.01 val PER: 0.5046
2026-01-10 17:33:00,555: t15.2023.10.06 val PER: 0.4295
2026-01-10 17:33:00,557: t15.2023.10.08 val PER: 0.5359
2026-01-10 17:33:00,558: t15.2023.10.13 val PER: 0.5656
2026-01-10 17:33:00,560: t15.2023.10.15 val PER: 0.4918
2026-01-10 17:33:00,561: t15.2023.10.20 val PER: 0.4866
2026-01-10 17:33:00,563: t15.2023.10.22 val PER: 0.4543
2026-01-10 17:33:00,564: t15.2023.11.03 val PER: 0.4681
2026-01-10 17:33:00,566: t15.2023.11.04 val PER: 0.2730
2026-01-10 17:33:00,568: t15.2023.11.17 val PER: 0.3670
2026-01-10 17:33:00,569: t15.2023.11.19 val PER: 0.3573
2026-01-10 17:33:00,571: t15.2023.11.26 val PER: 0.5370
2026-01-10 17:33:00,572: t15.2023.12.03 val PER: 0.4842
2026-01-10 17:33:00,574: t15.2023.12.08 val PER: 0.4780
2026-01-10 17:33:00,576: t15.2023.12.10 val PER: 0.4652
2026-01-10 17:33:00,578: t15.2023.12.17 val PER: 0.4657
2026-01-10 17:33:00,580: t15.2023.12.29 val PER: 0.5038
2026-01-10 17:33:00,581: t15.2024.02.25 val PER: 0.4410
2026-01-10 17:33:00,583: t15.2024.03.08 val PER: 0.5121
2026-01-10 17:33:00,585: t15.2024.03.15 val PER: 0.4859
2026-01-10 17:33:00,586: t15.2024.03.17 val PER: 0.4951
2026-01-10 17:33:00,588: t15.2024.05.10 val PER: 0.4844
2026-01-10 17:33:00,589: t15.2024.06.14 val PER: 0.4448
2026-01-10 17:33:00,591: t15.2024.07.19 val PER: 0.5438
2026-01-10 17:33:00,592: t15.2024.07.21 val PER: 0.4297
2026-01-10 17:33:00,594: t15.2024.07.28 val PER: 0.4706
2026-01-10 17:33:00,595: t15.2025.01.10 val PER: 0.5647
2026-01-10 17:33:00,597: t15.2025.01.12 val PER: 0.4865
2026-01-10 17:33:00,598: t15.2025.03.14 val PER: 0.5651
2026-01-10 17:33:00,600: t15.2025.03.16 val PER: 0.5157
2026-01-10 17:33:00,601: t15.2025.03.30 val PER: 0.5471
2026-01-10 17:33:00,603: t15.2025.04.13 val PER: 0.5078
2026-01-10 17:33:00,605: New best val WER(5gram) 97.39% --> 96.09%
2026-01-10 17:33:01,749: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_4000
2026-01-10 17:33:18,473: Train batch 4200: loss: 73.39 grad norm: 72.39 time: 0.083
2026-01-10 17:33:35,269: Train batch 4400: loss: 67.77 grad norm: 62.73 time: 0.070
2026-01-10 17:33:43,644: Running test after training batch: 4500
2026-01-10 17:33:43,766: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:33:49,738: WER debug example
  GT : you can see the code at this point as well
  PR : a this is why
2026-01-10 17:33:49,947: WER debug example
  GT : how does it keep the cost down
  PR : a a a a a
2026-01-10 17:34:32,567: Val batch 4500: PER (avg): 0.4672 CTC Loss (avg): 85.2046 WER(5gram): 94.85% (n=256) time: 48.921
2026-01-10 17:34:32,570: WER lens: avg_true_words=5.99 avg_pred_words=3.27 max_pred_words=13
2026-01-10 17:34:32,573: t15.2023.08.13 val PER: 0.4491
2026-01-10 17:34:32,574: t15.2023.08.18 val PER: 0.4451
2026-01-10 17:34:32,576: t15.2023.08.20 val PER: 0.4210
2026-01-10 17:34:32,577: t15.2023.08.25 val PER: 0.4051
2026-01-10 17:34:32,579: t15.2023.08.27 val PER: 0.5080
2026-01-10 17:34:32,581: t15.2023.09.01 val PER: 0.4123
2026-01-10 17:34:32,582: t15.2023.09.03 val PER: 0.4608
2026-01-10 17:34:32,584: t15.2023.09.24 val PER: 0.4260
2026-01-10 17:34:32,585: t15.2023.09.29 val PER: 0.4352
2026-01-10 17:34:32,587: t15.2023.10.01 val PER: 0.4888
2026-01-10 17:34:32,588: t15.2023.10.06 val PER: 0.4177
2026-01-10 17:34:32,590: t15.2023.10.08 val PER: 0.5237
2026-01-10 17:34:32,591: t15.2023.10.13 val PER: 0.5539
2026-01-10 17:34:32,593: t15.2023.10.15 val PER: 0.4700
2026-01-10 17:34:32,594: t15.2023.10.20 val PER: 0.4866
2026-01-10 17:34:32,596: t15.2023.10.22 val PER: 0.4477
2026-01-10 17:34:32,598: t15.2023.11.03 val PER: 0.4518
2026-01-10 17:34:32,599: t15.2023.11.04 val PER: 0.2628
2026-01-10 17:34:32,601: t15.2023.11.17 val PER: 0.3375
2026-01-10 17:34:32,603: t15.2023.11.19 val PER: 0.3433
2026-01-10 17:34:32,604: t15.2023.11.26 val PER: 0.5341
2026-01-10 17:34:32,606: t15.2023.12.03 val PER: 0.4601
2026-01-10 17:34:32,608: t15.2023.12.08 val PER: 0.4581
2026-01-10 17:34:32,610: t15.2023.12.10 val PER: 0.4560
2026-01-10 17:34:32,611: t15.2023.12.17 val PER: 0.4543
2026-01-10 17:34:32,613: t15.2023.12.29 val PER: 0.4887
2026-01-10 17:34:32,615: t15.2024.02.25 val PER: 0.4284
2026-01-10 17:34:32,616: t15.2024.03.08 val PER: 0.4879
2026-01-10 17:34:32,617: t15.2024.03.15 val PER: 0.4697
2026-01-10 17:34:32,619: t15.2024.03.17 val PER: 0.4603
2026-01-10 17:34:32,620: t15.2024.05.10 val PER: 0.4814
2026-01-10 17:34:32,622: t15.2024.06.14 val PER: 0.4322
2026-01-10 17:34:32,623: t15.2024.07.19 val PER: 0.5339
2026-01-10 17:34:32,625: t15.2024.07.21 val PER: 0.4186
2026-01-10 17:34:32,626: t15.2024.07.28 val PER: 0.4669
2026-01-10 17:34:32,628: t15.2025.01.10 val PER: 0.5579
2026-01-10 17:34:32,629: t15.2025.01.12 val PER: 0.4842
2026-01-10 17:34:32,630: t15.2025.03.14 val PER: 0.5680
2026-01-10 17:34:32,632: t15.2025.03.16 val PER: 0.5026
2026-01-10 17:34:32,633: t15.2025.03.30 val PER: 0.5552
2026-01-10 17:34:32,635: t15.2025.04.13 val PER: 0.5093
2026-01-10 17:34:32,636: New best val WER(5gram) 96.09% --> 94.85%
2026-01-10 17:34:33,782: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_4500
2026-01-10 17:34:42,107: Train batch 4600: loss: 76.18 grad norm: 75.10 time: 0.068
2026-01-10 17:34:58,875: Train batch 4800: loss: 62.04 grad norm: 60.79 time: 0.071
2026-01-10 17:35:15,602: Train batch 5000: loss: 95.99 grad norm: 85.91 time: 0.070
2026-01-10 17:35:15,605: Running test after training batch: 5000
2026-01-10 17:35:15,715: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:35:21,693: WER debug example
  GT : you can see the code at this point as well
  PR : a this is why
2026-01-10 17:35:21,902: WER debug example
  GT : how does it keep the cost down
  PR : a a a a a a
2026-01-10 17:36:01,490: Val batch 5000: PER (avg): 0.4518 CTC Loss (avg): 81.6064 WER(5gram): 94.26% (n=256) time: 45.883
2026-01-10 17:36:01,493: WER lens: avg_true_words=5.99 avg_pred_words=3.62 max_pred_words=14
2026-01-10 17:36:01,495: t15.2023.08.13 val PER: 0.4293
2026-01-10 17:36:01,497: t15.2023.08.18 val PER: 0.4233
2026-01-10 17:36:01,499: t15.2023.08.20 val PER: 0.4019
2026-01-10 17:36:01,500: t15.2023.08.25 val PER: 0.3901
2026-01-10 17:36:01,501: t15.2023.08.27 val PER: 0.4984
2026-01-10 17:36:01,503: t15.2023.09.01 val PER: 0.3912
2026-01-10 17:36:01,504: t15.2023.09.03 val PER: 0.4549
2026-01-10 17:36:01,506: t15.2023.09.24 val PER: 0.4223
2026-01-10 17:36:01,507: t15.2023.09.29 val PER: 0.4161
2026-01-10 17:36:01,509: t15.2023.10.01 val PER: 0.4808
2026-01-10 17:36:01,510: t15.2023.10.06 val PER: 0.4026
2026-01-10 17:36:01,511: t15.2023.10.08 val PER: 0.5142
2026-01-10 17:36:01,513: t15.2023.10.13 val PER: 0.5407
2026-01-10 17:36:01,514: t15.2023.10.15 val PER: 0.4601
2026-01-10 17:36:01,515: t15.2023.10.20 val PER: 0.4631
2026-01-10 17:36:01,517: t15.2023.10.22 val PER: 0.4276
2026-01-10 17:36:01,518: t15.2023.11.03 val PER: 0.4383
2026-01-10 17:36:01,520: t15.2023.11.04 val PER: 0.2253
2026-01-10 17:36:01,521: t15.2023.11.17 val PER: 0.3095
2026-01-10 17:36:01,523: t15.2023.11.19 val PER: 0.3054
2026-01-10 17:36:01,525: t15.2023.11.26 val PER: 0.5145
2026-01-10 17:36:01,526: t15.2023.12.03 val PER: 0.4527
2026-01-10 17:36:01,528: t15.2023.12.08 val PER: 0.4581
2026-01-10 17:36:01,529: t15.2023.12.10 val PER: 0.4350
2026-01-10 17:36:01,530: t15.2023.12.17 val PER: 0.4356
2026-01-10 17:36:01,532: t15.2023.12.29 val PER: 0.4839
2026-01-10 17:36:01,533: t15.2024.02.25 val PER: 0.3961
2026-01-10 17:36:01,534: t15.2024.03.08 val PER: 0.4893
2026-01-10 17:36:01,536: t15.2024.03.15 val PER: 0.4565
2026-01-10 17:36:01,537: t15.2024.03.17 val PER: 0.4540
2026-01-10 17:36:01,538: t15.2024.05.10 val PER: 0.4681
2026-01-10 17:36:01,540: t15.2024.06.14 val PER: 0.4211
2026-01-10 17:36:01,541: t15.2024.07.19 val PER: 0.5148
2026-01-10 17:36:01,542: t15.2024.07.21 val PER: 0.3917
2026-01-10 17:36:01,544: t15.2024.07.28 val PER: 0.4559
2026-01-10 17:36:01,545: t15.2025.01.10 val PER: 0.5468
2026-01-10 17:36:01,546: t15.2025.01.12 val PER: 0.4604
2026-01-10 17:36:01,547: t15.2025.03.14 val PER: 0.5370
2026-01-10 17:36:01,549: t15.2025.03.16 val PER: 0.4817
2026-01-10 17:36:01,550: t15.2025.03.30 val PER: 0.5322
2026-01-10 17:36:01,551: t15.2025.04.13 val PER: 0.4964
2026-01-10 17:36:01,553: New best val WER(5gram) 94.85% --> 94.26%
2026-01-10 17:36:02,701: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_5000
2026-01-10 17:36:19,298: Train batch 5200: loss: 68.03 grad norm: 68.26 time: 0.057
2026-01-10 17:36:36,133: Train batch 5400: loss: 74.71 grad norm: 62.53 time: 0.074
2026-01-10 17:36:44,417: Running test after training batch: 5500
2026-01-10 17:36:44,540: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:36:50,554: WER debug example
  GT : you can see the code at this point as well
  PR : it is a go at it this is why
2026-01-10 17:36:50,748: WER debug example
  GT : how does it keep the cost down
  PR : a a a a a
2026-01-10 17:37:29,866: Val batch 5500: PER (avg): 0.4368 CTC Loss (avg): 77.9553 WER(5gram): 94.72% (n=256) time: 45.447
2026-01-10 17:37:29,868: WER lens: avg_true_words=5.99 avg_pred_words=4.15 max_pred_words=12
2026-01-10 17:37:29,871: t15.2023.08.13 val PER: 0.4075
2026-01-10 17:37:29,873: t15.2023.08.18 val PER: 0.4082
2026-01-10 17:37:29,875: t15.2023.08.20 val PER: 0.3908
2026-01-10 17:37:29,877: t15.2023.08.25 val PER: 0.3630
2026-01-10 17:37:29,878: t15.2023.08.27 val PER: 0.4759
2026-01-10 17:37:29,880: t15.2023.09.01 val PER: 0.3880
2026-01-10 17:37:29,881: t15.2023.09.03 val PER: 0.4359
2026-01-10 17:37:29,883: t15.2023.09.24 val PER: 0.4090
2026-01-10 17:37:29,885: t15.2023.09.29 val PER: 0.4174
2026-01-10 17:37:29,886: t15.2023.10.01 val PER: 0.4690
2026-01-10 17:37:29,888: t15.2023.10.06 val PER: 0.3843
2026-01-10 17:37:29,889: t15.2023.10.08 val PER: 0.4993
2026-01-10 17:37:29,891: t15.2023.10.13 val PER: 0.5345
2026-01-10 17:37:29,893: t15.2023.10.15 val PER: 0.4483
2026-01-10 17:37:29,894: t15.2023.10.20 val PER: 0.4664
2026-01-10 17:37:29,896: t15.2023.10.22 val PER: 0.4276
2026-01-10 17:37:29,897: t15.2023.11.03 val PER: 0.4261
2026-01-10 17:37:29,899: t15.2023.11.04 val PER: 0.2150
2026-01-10 17:37:29,900: t15.2023.11.17 val PER: 0.3079
2026-01-10 17:37:29,901: t15.2023.11.19 val PER: 0.2814
2026-01-10 17:37:29,903: t15.2023.11.26 val PER: 0.4949
2026-01-10 17:37:29,904: t15.2023.12.03 val PER: 0.4349
2026-01-10 17:37:29,906: t15.2023.12.08 val PER: 0.4374
2026-01-10 17:37:29,907: t15.2023.12.10 val PER: 0.4113
2026-01-10 17:37:29,908: t15.2023.12.17 val PER: 0.4210
2026-01-10 17:37:29,910: t15.2023.12.29 val PER: 0.4578
2026-01-10 17:37:29,911: t15.2024.02.25 val PER: 0.4017
2026-01-10 17:37:29,913: t15.2024.03.08 val PER: 0.4595
2026-01-10 17:37:29,914: t15.2024.03.15 val PER: 0.4503
2026-01-10 17:37:29,916: t15.2024.03.17 val PER: 0.4324
2026-01-10 17:37:29,917: t15.2024.05.10 val PER: 0.4502
2026-01-10 17:37:29,919: t15.2024.06.14 val PER: 0.4038
2026-01-10 17:37:29,920: t15.2024.07.19 val PER: 0.4871
2026-01-10 17:37:29,921: t15.2024.07.21 val PER: 0.3800
2026-01-10 17:37:29,923: t15.2024.07.28 val PER: 0.4331
2026-01-10 17:37:29,924: t15.2025.01.10 val PER: 0.5358
2026-01-10 17:37:29,926: t15.2025.01.12 val PER: 0.4326
2026-01-10 17:37:29,927: t15.2025.03.14 val PER: 0.5133
2026-01-10 17:37:29,928: t15.2025.03.16 val PER: 0.4777
2026-01-10 17:37:29,930: t15.2025.03.30 val PER: 0.5368
2026-01-10 17:37:29,931: t15.2025.04.13 val PER: 0.4622
2026-01-10 17:37:30,071: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_5500
2026-01-10 17:37:38,757: Train batch 5600: loss: 72.71 grad norm: 80.97 time: 0.069
2026-01-10 17:37:55,508: Train batch 5800: loss: 61.55 grad norm: 81.25 time: 0.086
2026-01-10 17:38:12,134: Train batch 6000: loss: 56.34 grad norm: 65.85 time: 0.054
2026-01-10 17:38:12,137: Running test after training batch: 6000
2026-01-10 17:38:12,302: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:38:18,223: WER debug example
  GT : you can see the code at this point as well
  PR : i see a guy and this is why
2026-01-10 17:38:18,432: WER debug example
  GT : how does it keep the cost down
  PR : a a a a a
2026-01-10 17:38:55,482: Val batch 6000: PER (avg): 0.4298 CTC Loss (avg): 75.1679 WER(5gram): 94.46% (n=256) time: 43.343
2026-01-10 17:38:55,485: WER lens: avg_true_words=5.99 avg_pred_words=3.97 max_pred_words=12
2026-01-10 17:38:55,487: t15.2023.08.13 val PER: 0.4033
2026-01-10 17:38:55,489: t15.2023.08.18 val PER: 0.4007
2026-01-10 17:38:55,491: t15.2023.08.20 val PER: 0.3749
2026-01-10 17:38:55,493: t15.2023.08.25 val PER: 0.3599
2026-01-10 17:38:55,494: t15.2023.08.27 val PER: 0.4646
2026-01-10 17:38:55,496: t15.2023.09.01 val PER: 0.3709
2026-01-10 17:38:55,497: t15.2023.09.03 val PER: 0.4287
2026-01-10 17:38:55,498: t15.2023.09.24 val PER: 0.3883
2026-01-10 17:38:55,500: t15.2023.09.29 val PER: 0.4046
2026-01-10 17:38:55,501: t15.2023.10.01 val PER: 0.4577
2026-01-10 17:38:55,503: t15.2023.10.06 val PER: 0.3649
2026-01-10 17:38:55,504: t15.2023.10.08 val PER: 0.4966
2026-01-10 17:38:55,506: t15.2023.10.13 val PER: 0.5291
2026-01-10 17:38:55,507: t15.2023.10.15 val PER: 0.4496
2026-01-10 17:38:55,509: t15.2023.10.20 val PER: 0.4497
2026-01-10 17:38:55,511: t15.2023.10.22 val PER: 0.4131
2026-01-10 17:38:55,513: t15.2023.11.03 val PER: 0.4240
2026-01-10 17:38:55,514: t15.2023.11.04 val PER: 0.1980
2026-01-10 17:38:55,516: t15.2023.11.17 val PER: 0.2877
2026-01-10 17:38:55,517: t15.2023.11.19 val PER: 0.2854
2026-01-10 17:38:55,519: t15.2023.11.26 val PER: 0.4855
2026-01-10 17:38:55,520: t15.2023.12.03 val PER: 0.4296
2026-01-10 17:38:55,521: t15.2023.12.08 val PER: 0.4308
2026-01-10 17:38:55,523: t15.2023.12.10 val PER: 0.4139
2026-01-10 17:38:55,524: t15.2023.12.17 val PER: 0.4137
2026-01-10 17:38:55,525: t15.2023.12.29 val PER: 0.4447
2026-01-10 17:38:55,527: t15.2024.02.25 val PER: 0.3792
2026-01-10 17:38:55,529: t15.2024.03.08 val PER: 0.4637
2026-01-10 17:38:55,531: t15.2024.03.15 val PER: 0.4490
2026-01-10 17:38:55,532: t15.2024.03.17 val PER: 0.4135
2026-01-10 17:38:55,534: t15.2024.05.10 val PER: 0.4577
2026-01-10 17:38:55,535: t15.2024.06.14 val PER: 0.4022
2026-01-10 17:38:55,537: t15.2024.07.19 val PER: 0.4759
2026-01-10 17:38:55,538: t15.2024.07.21 val PER: 0.3800
2026-01-10 17:38:55,539: t15.2024.07.28 val PER: 0.4397
2026-01-10 17:38:55,541: t15.2025.01.10 val PER: 0.5317
2026-01-10 17:38:55,542: t15.2025.01.12 val PER: 0.4365
2026-01-10 17:38:55,543: t15.2025.03.14 val PER: 0.5074
2026-01-10 17:38:55,545: t15.2025.03.16 val PER: 0.4817
2026-01-10 17:38:55,546: t15.2025.03.30 val PER: 0.5241
2026-01-10 17:38:55,548: t15.2025.04.13 val PER: 0.4650
2026-01-10 17:38:55,687: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_6000
2026-01-10 17:39:12,308: Train batch 6200: loss: 64.67 grad norm: 98.33 time: 0.075
2026-01-10 17:39:28,939: Train batch 6400: loss: 76.19 grad norm: 72.25 time: 0.068
2026-01-10 17:39:37,115: Running test after training batch: 6500
2026-01-10 17:39:37,240: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:39:43,127: WER debug example
  GT : you can see the code at this point as well
  PR : you can see a guy and this is why
2026-01-10 17:39:43,286: WER debug example
  GT : how does it keep the cost down
  PR : a a a a a
2026-01-10 17:40:18,516: Val batch 6500: PER (avg): 0.4185 CTC Loss (avg): 72.7278 WER(5gram): 89.83% (n=256) time: 41.399
2026-01-10 17:40:18,519: WER lens: avg_true_words=5.99 avg_pred_words=4.30 max_pred_words=12
2026-01-10 17:40:18,521: t15.2023.08.13 val PER: 0.4033
2026-01-10 17:40:18,523: t15.2023.08.18 val PER: 0.3898
2026-01-10 17:40:18,525: t15.2023.08.20 val PER: 0.3709
2026-01-10 17:40:18,526: t15.2023.08.25 val PER: 0.3494
2026-01-10 17:40:18,527: t15.2023.08.27 val PER: 0.4662
2026-01-10 17:40:18,529: t15.2023.09.01 val PER: 0.3628
2026-01-10 17:40:18,531: t15.2023.09.03 val PER: 0.4216
2026-01-10 17:40:18,532: t15.2023.09.24 val PER: 0.3799
2026-01-10 17:40:18,533: t15.2023.09.29 val PER: 0.3835
2026-01-10 17:40:18,535: t15.2023.10.01 val PER: 0.4399
2026-01-10 17:40:18,536: t15.2023.10.06 val PER: 0.3606
2026-01-10 17:40:18,538: t15.2023.10.08 val PER: 0.4844
2026-01-10 17:40:18,539: t15.2023.10.13 val PER: 0.5136
2026-01-10 17:40:18,541: t15.2023.10.15 val PER: 0.4252
2026-01-10 17:40:18,542: t15.2023.10.20 val PER: 0.4463
2026-01-10 17:40:18,543: t15.2023.10.22 val PER: 0.4031
2026-01-10 17:40:18,545: t15.2023.11.03 val PER: 0.4152
2026-01-10 17:40:18,546: t15.2023.11.04 val PER: 0.1877
2026-01-10 17:40:18,548: t15.2023.11.17 val PER: 0.2753
2026-01-10 17:40:18,549: t15.2023.11.19 val PER: 0.2695
2026-01-10 17:40:18,550: t15.2023.11.26 val PER: 0.4768
2026-01-10 17:40:18,552: t15.2023.12.03 val PER: 0.4076
2026-01-10 17:40:18,553: t15.2023.12.08 val PER: 0.4154
2026-01-10 17:40:18,554: t15.2023.12.10 val PER: 0.3929
2026-01-10 17:40:18,556: t15.2023.12.17 val PER: 0.4168
2026-01-10 17:40:18,557: t15.2023.12.29 val PER: 0.4393
2026-01-10 17:40:18,558: t15.2024.02.25 val PER: 0.3736
2026-01-10 17:40:18,560: t15.2024.03.08 val PER: 0.4495
2026-01-10 17:40:18,561: t15.2024.03.15 val PER: 0.4371
2026-01-10 17:40:18,563: t15.2024.03.17 val PER: 0.4066
2026-01-10 17:40:18,564: t15.2024.05.10 val PER: 0.4487
2026-01-10 17:40:18,565: t15.2024.06.14 val PER: 0.3975
2026-01-10 17:40:18,567: t15.2024.07.19 val PER: 0.4667
2026-01-10 17:40:18,568: t15.2024.07.21 val PER: 0.3703
2026-01-10 17:40:18,569: t15.2024.07.28 val PER: 0.4125
2026-01-10 17:40:18,571: t15.2025.01.10 val PER: 0.5207
2026-01-10 17:40:18,572: t15.2025.01.12 val PER: 0.4242
2026-01-10 17:40:18,574: t15.2025.03.14 val PER: 0.4970
2026-01-10 17:40:18,575: t15.2025.03.16 val PER: 0.4594
2026-01-10 17:40:18,576: t15.2025.03.30 val PER: 0.5184
2026-01-10 17:40:18,578: t15.2025.04.13 val PER: 0.4622
2026-01-10 17:40:18,579: New best val WER(5gram) 94.26% --> 89.83%
2026-01-10 17:40:19,714: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_6500
2026-01-10 17:40:27,818: Train batch 6600: loss: 50.94 grad norm: 55.10 time: 0.049
2026-01-10 17:40:45,105: Train batch 6800: loss: 59.43 grad norm: 62.21 time: 0.054
2026-01-10 17:41:02,249: Train batch 7000: loss: 63.41 grad norm: 67.85 time: 0.068
2026-01-10 17:41:02,251: Running test after training batch: 7000
2026-01-10 17:41:02,386: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:41:08,465: WER debug example
  GT : you can see the code at this point as well
  PR : you can see a guy and this is why
2026-01-10 17:41:08,616: WER debug example
  GT : how does it keep the cost down
  PR : is it a go at a
2026-01-10 17:41:41,919: Val batch 7000: PER (avg): 0.4090 CTC Loss (avg): 69.9531 WER(5gram): 91.59% (n=256) time: 39.665
2026-01-10 17:41:41,921: WER lens: avg_true_words=5.99 avg_pred_words=4.18 max_pred_words=13
2026-01-10 17:41:41,924: t15.2023.08.13 val PER: 0.3929
2026-01-10 17:41:41,926: t15.2023.08.18 val PER: 0.3747
2026-01-10 17:41:41,928: t15.2023.08.20 val PER: 0.3463
2026-01-10 17:41:41,930: t15.2023.08.25 val PER: 0.3434
2026-01-10 17:41:41,932: t15.2023.08.27 val PER: 0.4518
2026-01-10 17:41:41,933: t15.2023.09.01 val PER: 0.3515
2026-01-10 17:41:41,935: t15.2023.09.03 val PER: 0.4204
2026-01-10 17:41:41,936: t15.2023.09.24 val PER: 0.3701
2026-01-10 17:41:41,938: t15.2023.09.29 val PER: 0.3778
2026-01-10 17:41:41,939: t15.2023.10.01 val PER: 0.4571
2026-01-10 17:41:41,941: t15.2023.10.06 val PER: 0.3455
2026-01-10 17:41:41,942: t15.2023.10.08 val PER: 0.4750
2026-01-10 17:41:41,944: t15.2023.10.13 val PER: 0.5027
2026-01-10 17:41:41,947: t15.2023.10.15 val PER: 0.4113
2026-01-10 17:41:41,948: t15.2023.10.20 val PER: 0.4228
2026-01-10 17:41:41,950: t15.2023.10.22 val PER: 0.3953
2026-01-10 17:41:41,951: t15.2023.11.03 val PER: 0.4057
2026-01-10 17:41:41,953: t15.2023.11.04 val PER: 0.1809
2026-01-10 17:41:41,954: t15.2023.11.17 val PER: 0.2691
2026-01-10 17:41:41,956: t15.2023.11.19 val PER: 0.2395
2026-01-10 17:41:41,957: t15.2023.11.26 val PER: 0.4674
2026-01-10 17:41:41,959: t15.2023.12.03 val PER: 0.3981
2026-01-10 17:41:41,960: t15.2023.12.08 val PER: 0.4068
2026-01-10 17:41:41,962: t15.2023.12.10 val PER: 0.3863
2026-01-10 17:41:41,965: t15.2023.12.17 val PER: 0.4064
2026-01-10 17:41:41,967: t15.2023.12.29 val PER: 0.4351
2026-01-10 17:41:41,969: t15.2024.02.25 val PER: 0.3553
2026-01-10 17:41:41,971: t15.2024.03.08 val PER: 0.4452
2026-01-10 17:41:41,974: t15.2024.03.15 val PER: 0.4190
2026-01-10 17:41:41,975: t15.2024.03.17 val PER: 0.4045
2026-01-10 17:41:41,977: t15.2024.05.10 val PER: 0.4175
2026-01-10 17:41:41,978: t15.2024.06.14 val PER: 0.3896
2026-01-10 17:41:41,980: t15.2024.07.19 val PER: 0.4542
2026-01-10 17:41:41,981: t15.2024.07.21 val PER: 0.3586
2026-01-10 17:41:41,983: t15.2024.07.28 val PER: 0.4140
2026-01-10 17:41:41,984: t15.2025.01.10 val PER: 0.5110
2026-01-10 17:41:41,986: t15.2025.01.12 val PER: 0.4126
2026-01-10 17:41:41,988: t15.2025.03.14 val PER: 0.4882
2026-01-10 17:41:41,989: t15.2025.03.16 val PER: 0.4385
2026-01-10 17:41:41,991: t15.2025.03.30 val PER: 0.5172
2026-01-10 17:41:41,993: t15.2025.04.13 val PER: 0.4579
2026-01-10 17:41:42,131: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_7000
2026-01-10 17:41:59,703: Train batch 7200: loss: 59.97 grad norm: 76.53 time: 0.083
2026-01-10 17:42:16,353: Train batch 7400: loss: 61.60 grad norm: 68.72 time: 0.080
2026-01-10 17:42:24,757: Running test after training batch: 7500
2026-01-10 17:42:24,880: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:42:30,855: WER debug example
  GT : you can see the code at this point as well
  PR : i know it a go at it this is why
2026-01-10 17:42:31,046: WER debug example
  GT : how does it keep the cost down
  PR : i know a guy at the
2026-01-10 17:43:05,442: Val batch 7500: PER (avg): 0.3994 CTC Loss (avg): 68.1910 WER(5gram): 91.13% (n=256) time: 40.683
2026-01-10 17:43:05,444: WER lens: avg_true_words=5.99 avg_pred_words=4.52 max_pred_words=14
2026-01-10 17:43:05,447: t15.2023.08.13 val PER: 0.3815
2026-01-10 17:43:05,449: t15.2023.08.18 val PER: 0.3747
2026-01-10 17:43:05,450: t15.2023.08.20 val PER: 0.3511
2026-01-10 17:43:05,452: t15.2023.08.25 val PER: 0.3358
2026-01-10 17:43:05,453: t15.2023.08.27 val PER: 0.4453
2026-01-10 17:43:05,455: t15.2023.09.01 val PER: 0.3287
2026-01-10 17:43:05,456: t15.2023.09.03 val PER: 0.4062
2026-01-10 17:43:05,458: t15.2023.09.24 val PER: 0.3556
2026-01-10 17:43:05,459: t15.2023.09.29 val PER: 0.3682
2026-01-10 17:43:05,461: t15.2023.10.01 val PER: 0.4386
2026-01-10 17:43:05,462: t15.2023.10.06 val PER: 0.3402
2026-01-10 17:43:05,464: t15.2023.10.08 val PER: 0.4723
2026-01-10 17:43:05,465: t15.2023.10.13 val PER: 0.4934
2026-01-10 17:43:05,467: t15.2023.10.15 val PER: 0.4047
2026-01-10 17:43:05,468: t15.2023.10.20 val PER: 0.4195
2026-01-10 17:43:05,469: t15.2023.10.22 val PER: 0.3864
2026-01-10 17:43:05,471: t15.2023.11.03 val PER: 0.3887
2026-01-10 17:43:05,472: t15.2023.11.04 val PER: 0.1741
2026-01-10 17:43:05,474: t15.2023.11.17 val PER: 0.2613
2026-01-10 17:43:05,475: t15.2023.11.19 val PER: 0.2275
2026-01-10 17:43:05,477: t15.2023.11.26 val PER: 0.4652
2026-01-10 17:43:05,478: t15.2023.12.03 val PER: 0.3939
2026-01-10 17:43:05,479: t15.2023.12.08 val PER: 0.3935
2026-01-10 17:43:05,481: t15.2023.12.10 val PER: 0.3666
2026-01-10 17:43:05,483: t15.2023.12.17 val PER: 0.4033
2026-01-10 17:43:05,484: t15.2023.12.29 val PER: 0.4104
2026-01-10 17:43:05,485: t15.2024.02.25 val PER: 0.3413
2026-01-10 17:43:05,487: t15.2024.03.08 val PER: 0.4367
2026-01-10 17:43:05,488: t15.2024.03.15 val PER: 0.4134
2026-01-10 17:43:05,490: t15.2024.03.17 val PER: 0.3828
2026-01-10 17:43:05,491: t15.2024.05.10 val PER: 0.4235
2026-01-10 17:43:05,493: t15.2024.06.14 val PER: 0.3927
2026-01-10 17:43:05,494: t15.2024.07.19 val PER: 0.4463
2026-01-10 17:43:05,495: t15.2024.07.21 val PER: 0.3407
2026-01-10 17:43:05,497: t15.2024.07.28 val PER: 0.3985
2026-01-10 17:43:05,498: t15.2025.01.10 val PER: 0.5014
2026-01-10 17:43:05,500: t15.2025.01.12 val PER: 0.4065
2026-01-10 17:43:05,501: t15.2025.03.14 val PER: 0.4926
2026-01-10 17:43:05,502: t15.2025.03.16 val PER: 0.4398
2026-01-10 17:43:05,504: t15.2025.03.30 val PER: 0.5023
2026-01-10 17:43:05,505: t15.2025.04.13 val PER: 0.4622
2026-01-10 17:43:05,643: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_7500
2026-01-10 17:43:13,997: Train batch 7600: loss: 61.36 grad norm: 70.09 time: 0.075
2026-01-10 17:43:31,055: Train batch 7800: loss: 54.67 grad norm: 76.86 time: 0.060
2026-01-10 17:43:48,098: Train batch 8000: loss: 51.72 grad norm: 68.61 time: 0.077
2026-01-10 17:43:48,100: Running test after training batch: 8000
2026-01-10 17:43:48,202: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:43:54,308: WER debug example
  GT : you can see the code at this point as well
  PR : you can see a guy at this point we
2026-01-10 17:43:54,472: WER debug example
  GT : how does it keep the cost down
  PR : i know it to be a sign at the
2026-01-10 17:44:26,886: Val batch 8000: PER (avg): 0.3903 CTC Loss (avg): 66.0719 WER(5gram): 89.83% (n=256) time: 38.783
2026-01-10 17:44:26,888: WER lens: avg_true_words=5.99 avg_pred_words=4.68 max_pred_words=13
2026-01-10 17:44:26,891: t15.2023.08.13 val PER: 0.3659
2026-01-10 17:44:26,893: t15.2023.08.18 val PER: 0.3588
2026-01-10 17:44:26,895: t15.2023.08.20 val PER: 0.3328
2026-01-10 17:44:26,896: t15.2023.08.25 val PER: 0.3223
2026-01-10 17:44:26,898: t15.2023.08.27 val PER: 0.4293
2026-01-10 17:44:26,900: t15.2023.09.01 val PER: 0.3263
2026-01-10 17:44:26,901: t15.2023.09.03 val PER: 0.3789
2026-01-10 17:44:26,903: t15.2023.09.24 val PER: 0.3617
2026-01-10 17:44:26,905: t15.2023.09.29 val PER: 0.3599
2026-01-10 17:44:26,907: t15.2023.10.01 val PER: 0.4293
2026-01-10 17:44:26,909: t15.2023.10.06 val PER: 0.3294
2026-01-10 17:44:26,912: t15.2023.10.08 val PER: 0.4750
2026-01-10 17:44:26,915: t15.2023.10.13 val PER: 0.4895
2026-01-10 17:44:26,917: t15.2023.10.15 val PER: 0.3982
2026-01-10 17:44:26,919: t15.2023.10.20 val PER: 0.4060
2026-01-10 17:44:26,921: t15.2023.10.22 val PER: 0.3708
2026-01-10 17:44:26,923: t15.2023.11.03 val PER: 0.3874
2026-01-10 17:44:26,925: t15.2023.11.04 val PER: 0.1604
2026-01-10 17:44:26,926: t15.2023.11.17 val PER: 0.2286
2026-01-10 17:44:26,928: t15.2023.11.19 val PER: 0.2355
2026-01-10 17:44:26,930: t15.2023.11.26 val PER: 0.4572
2026-01-10 17:44:26,932: t15.2023.12.03 val PER: 0.4013
2026-01-10 17:44:26,933: t15.2023.12.08 val PER: 0.3862
2026-01-10 17:44:26,935: t15.2023.12.10 val PER: 0.3693
2026-01-10 17:44:26,937: t15.2023.12.17 val PER: 0.3909
2026-01-10 17:44:26,939: t15.2023.12.29 val PER: 0.4056
2026-01-10 17:44:26,940: t15.2024.02.25 val PER: 0.3399
2026-01-10 17:44:26,942: t15.2024.03.08 val PER: 0.4253
2026-01-10 17:44:26,943: t15.2024.03.15 val PER: 0.3959
2026-01-10 17:44:26,945: t15.2024.03.17 val PER: 0.3912
2026-01-10 17:44:26,946: t15.2024.05.10 val PER: 0.4175
2026-01-10 17:44:26,948: t15.2024.06.14 val PER: 0.3770
2026-01-10 17:44:26,949: t15.2024.07.19 val PER: 0.4377
2026-01-10 17:44:26,951: t15.2024.07.21 val PER: 0.3255
2026-01-10 17:44:26,952: t15.2024.07.28 val PER: 0.3890
2026-01-10 17:44:26,954: t15.2025.01.10 val PER: 0.4945
2026-01-10 17:44:26,955: t15.2025.01.12 val PER: 0.3957
2026-01-10 17:44:26,956: t15.2025.03.14 val PER: 0.4793
2026-01-10 17:44:26,958: t15.2025.03.16 val PER: 0.4280
2026-01-10 17:44:26,959: t15.2025.03.30 val PER: 0.4874
2026-01-10 17:44:26,960: t15.2025.04.13 val PER: 0.4351
2026-01-10 17:44:27,096: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_8000
2026-01-10 17:44:43,813: Train batch 8200: loss: 43.53 grad norm: 53.84 time: 0.059
2026-01-10 17:45:00,617: Train batch 8400: loss: 47.25 grad norm: 56.40 time: 0.068
2026-01-10 17:45:09,031: Running test after training batch: 8500
2026-01-10 17:45:09,130: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:45:15,038: WER debug example
  GT : you can see the code at this point as well
  PR : i see a guy and this is why
2026-01-10 17:45:15,214: WER debug example
  GT : how does it keep the cost down
  PR : i know it in a row at the
2026-01-10 17:45:45,098: Val batch 8500: PER (avg): 0.3851 CTC Loss (avg): 64.3232 WER(5gram): 87.48% (n=256) time: 36.065
2026-01-10 17:45:45,100: WER lens: avg_true_words=5.99 avg_pred_words=4.26 max_pred_words=11
2026-01-10 17:45:45,103: t15.2023.08.13 val PER: 0.3690
2026-01-10 17:45:45,104: t15.2023.08.18 val PER: 0.3546
2026-01-10 17:45:45,106: t15.2023.08.20 val PER: 0.3288
2026-01-10 17:45:45,108: t15.2023.08.25 val PER: 0.3133
2026-01-10 17:45:45,109: t15.2023.08.27 val PER: 0.4244
2026-01-10 17:45:45,111: t15.2023.09.01 val PER: 0.3247
2026-01-10 17:45:45,113: t15.2023.09.03 val PER: 0.3824
2026-01-10 17:45:45,114: t15.2023.09.24 val PER: 0.3519
2026-01-10 17:45:45,116: t15.2023.09.29 val PER: 0.3574
2026-01-10 17:45:45,117: t15.2023.10.01 val PER: 0.4194
2026-01-10 17:45:45,119: t15.2023.10.06 val PER: 0.3305
2026-01-10 17:45:45,120: t15.2023.10.08 val PER: 0.4777
2026-01-10 17:45:45,122: t15.2023.10.13 val PER: 0.4856
2026-01-10 17:45:45,123: t15.2023.10.15 val PER: 0.3955
2026-01-10 17:45:45,125: t15.2023.10.20 val PER: 0.3960
2026-01-10 17:45:45,127: t15.2023.10.22 val PER: 0.3641
2026-01-10 17:45:45,129: t15.2023.11.03 val PER: 0.3786
2026-01-10 17:45:45,131: t15.2023.11.04 val PER: 0.1638
2026-01-10 17:45:45,132: t15.2023.11.17 val PER: 0.2379
2026-01-10 17:45:45,134: t15.2023.11.19 val PER: 0.2295
2026-01-10 17:45:45,135: t15.2023.11.26 val PER: 0.4478
2026-01-10 17:45:45,137: t15.2023.12.03 val PER: 0.3834
2026-01-10 17:45:45,138: t15.2023.12.08 val PER: 0.3828
2026-01-10 17:45:45,140: t15.2023.12.10 val PER: 0.3482
2026-01-10 17:45:45,141: t15.2023.12.17 val PER: 0.3898
2026-01-10 17:45:45,143: t15.2023.12.29 val PER: 0.3919
2026-01-10 17:45:45,144: t15.2024.02.25 val PER: 0.3357
2026-01-10 17:45:45,146: t15.2024.03.08 val PER: 0.4267
2026-01-10 17:45:45,147: t15.2024.03.15 val PER: 0.4021
2026-01-10 17:45:45,149: t15.2024.03.17 val PER: 0.3842
2026-01-10 17:45:45,151: t15.2024.05.10 val PER: 0.3997
2026-01-10 17:45:45,152: t15.2024.06.14 val PER: 0.3628
2026-01-10 17:45:45,153: t15.2024.07.19 val PER: 0.4357
2026-01-10 17:45:45,155: t15.2024.07.21 val PER: 0.3172
2026-01-10 17:45:45,156: t15.2024.07.28 val PER: 0.3831
2026-01-10 17:45:45,158: t15.2025.01.10 val PER: 0.5000
2026-01-10 17:45:45,159: t15.2025.01.12 val PER: 0.3841
2026-01-10 17:45:45,160: t15.2025.03.14 val PER: 0.4749
2026-01-10 17:45:45,162: t15.2025.03.16 val PER: 0.4280
2026-01-10 17:45:45,163: t15.2025.03.30 val PER: 0.4713
2026-01-10 17:45:45,164: t15.2025.04.13 val PER: 0.4322
2026-01-10 17:45:45,166: New best val WER(5gram) 89.83% --> 87.48%
2026-01-10 17:45:46,286: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_8500
2026-01-10 17:45:55,429: Train batch 8600: loss: 57.12 grad norm: 69.14 time: 0.059
2026-01-10 17:46:12,129: Train batch 8800: loss: 63.89 grad norm: 90.74 time: 0.066
2026-01-10 17:46:28,902: Train batch 9000: loss: 58.94 grad norm: 85.06 time: 0.077
2026-01-10 17:46:28,904: Running test after training batch: 9000
2026-01-10 17:46:29,009: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:46:34,886: WER debug example
  GT : you can see the code at this point as well
  PR : you can see a guy and this is why
2026-01-10 17:46:35,032: WER debug example
  GT : how does it keep the cost down
  PR : i see it in a row at the end
2026-01-10 17:47:02,877: Val batch 9000: PER (avg): 0.3808 CTC Loss (avg): 63.1779 WER(5gram): 88.01% (n=256) time: 33.971
2026-01-10 17:47:02,880: WER lens: avg_true_words=5.99 avg_pred_words=4.67 max_pred_words=12
2026-01-10 17:47:02,882: t15.2023.08.13 val PER: 0.3628
2026-01-10 17:47:02,884: t15.2023.08.18 val PER: 0.3453
2026-01-10 17:47:02,886: t15.2023.08.20 val PER: 0.3193
2026-01-10 17:47:02,887: t15.2023.08.25 val PER: 0.3072
2026-01-10 17:47:02,889: t15.2023.08.27 val PER: 0.4132
2026-01-10 17:47:02,890: t15.2023.09.01 val PER: 0.3166
2026-01-10 17:47:02,892: t15.2023.09.03 val PER: 0.3670
2026-01-10 17:47:02,893: t15.2023.09.24 val PER: 0.3507
2026-01-10 17:47:02,895: t15.2023.09.29 val PER: 0.3427
2026-01-10 17:47:02,896: t15.2023.10.01 val PER: 0.4260
2026-01-10 17:47:02,899: t15.2023.10.06 val PER: 0.3165
2026-01-10 17:47:02,901: t15.2023.10.08 val PER: 0.4628
2026-01-10 17:47:02,902: t15.2023.10.13 val PER: 0.4818
2026-01-10 17:47:02,904: t15.2023.10.15 val PER: 0.3995
2026-01-10 17:47:02,905: t15.2023.10.20 val PER: 0.3691
2026-01-10 17:47:02,907: t15.2023.10.22 val PER: 0.3630
2026-01-10 17:47:02,909: t15.2023.11.03 val PER: 0.3786
2026-01-10 17:47:02,910: t15.2023.11.04 val PER: 0.1502
2026-01-10 17:47:02,911: t15.2023.11.17 val PER: 0.2302
2026-01-10 17:47:02,913: t15.2023.11.19 val PER: 0.2176
2026-01-10 17:47:02,914: t15.2023.11.26 val PER: 0.4486
2026-01-10 17:47:02,916: t15.2023.12.03 val PER: 0.3834
2026-01-10 17:47:02,918: t15.2023.12.08 val PER: 0.3828
2026-01-10 17:47:02,919: t15.2023.12.10 val PER: 0.3495
2026-01-10 17:47:02,920: t15.2023.12.17 val PER: 0.3825
2026-01-10 17:47:02,922: t15.2023.12.29 val PER: 0.3953
2026-01-10 17:47:02,923: t15.2024.02.25 val PER: 0.3174
2026-01-10 17:47:02,925: t15.2024.03.08 val PER: 0.4139
2026-01-10 17:47:02,927: t15.2024.03.15 val PER: 0.4046
2026-01-10 17:47:02,928: t15.2024.03.17 val PER: 0.3773
2026-01-10 17:47:02,930: t15.2024.05.10 val PER: 0.3997
2026-01-10 17:47:02,931: t15.2024.06.14 val PER: 0.3644
2026-01-10 17:47:02,933: t15.2024.07.19 val PER: 0.4291
2026-01-10 17:47:02,934: t15.2024.07.21 val PER: 0.3193
2026-01-10 17:47:02,935: t15.2024.07.28 val PER: 0.3831
2026-01-10 17:47:02,937: t15.2025.01.10 val PER: 0.4931
2026-01-10 17:47:02,938: t15.2025.01.12 val PER: 0.3865
2026-01-10 17:47:02,940: t15.2025.03.14 val PER: 0.4763
2026-01-10 17:47:02,941: t15.2025.03.16 val PER: 0.4149
2026-01-10 17:47:02,943: t15.2025.03.30 val PER: 0.4736
2026-01-10 17:47:02,944: t15.2025.04.13 val PER: 0.4151
2026-01-10 17:47:03,081: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_9000
2026-01-10 17:47:19,767: Train batch 9200: loss: 46.99 grad norm: 63.83 time: 0.061
2026-01-10 17:47:36,364: Train batch 9400: loss: 44.76 grad norm: 73.90 time: 0.073
2026-01-10 17:47:44,831: Running test after training batch: 9500
2026-01-10 17:47:44,946: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:47:50,796: WER debug example
  GT : you can see the code at this point as well
  PR : you can see a guy and this is why
2026-01-10 17:47:50,950: WER debug example
  GT : how does it keep the cost down
  PR : i know it in a row at the
2026-01-10 17:48:19,106: Val batch 9500: PER (avg): 0.3691 CTC Loss (avg): 61.6665 WER(5gram): 85.66% (n=256) time: 34.273
2026-01-10 17:48:19,109: WER lens: avg_true_words=5.99 avg_pred_words=4.86 max_pred_words=13
2026-01-10 17:48:19,111: t15.2023.08.13 val PER: 0.3524
2026-01-10 17:48:19,113: t15.2023.08.18 val PER: 0.3353
2026-01-10 17:48:19,115: t15.2023.08.20 val PER: 0.3129
2026-01-10 17:48:19,116: t15.2023.08.25 val PER: 0.2997
2026-01-10 17:48:19,117: t15.2023.08.27 val PER: 0.4116
2026-01-10 17:48:19,119: t15.2023.09.01 val PER: 0.3044
2026-01-10 17:48:19,120: t15.2023.09.03 val PER: 0.3658
2026-01-10 17:48:19,122: t15.2023.09.24 val PER: 0.3374
2026-01-10 17:48:19,123: t15.2023.09.29 val PER: 0.3369
2026-01-10 17:48:19,125: t15.2023.10.01 val PER: 0.4016
2026-01-10 17:48:19,127: t15.2023.10.06 val PER: 0.3003
2026-01-10 17:48:19,128: t15.2023.10.08 val PER: 0.4587
2026-01-10 17:48:19,130: t15.2023.10.13 val PER: 0.4631
2026-01-10 17:48:19,131: t15.2023.10.15 val PER: 0.3771
2026-01-10 17:48:19,133: t15.2023.10.20 val PER: 0.3960
2026-01-10 17:48:19,134: t15.2023.10.22 val PER: 0.3463
2026-01-10 17:48:19,135: t15.2023.11.03 val PER: 0.3643
2026-01-10 17:48:19,136: t15.2023.11.04 val PER: 0.1433
2026-01-10 17:48:19,138: t15.2023.11.17 val PER: 0.2131
2026-01-10 17:48:19,139: t15.2023.11.19 val PER: 0.2116
2026-01-10 17:48:19,140: t15.2023.11.26 val PER: 0.4391
2026-01-10 17:48:19,142: t15.2023.12.03 val PER: 0.3666
2026-01-10 17:48:19,143: t15.2023.12.08 val PER: 0.3675
2026-01-10 17:48:19,144: t15.2023.12.10 val PER: 0.3377
2026-01-10 17:48:19,146: t15.2023.12.17 val PER: 0.3659
2026-01-10 17:48:19,147: t15.2023.12.29 val PER: 0.3782
2026-01-10 17:48:19,148: t15.2024.02.25 val PER: 0.3118
2026-01-10 17:48:19,150: t15.2024.03.08 val PER: 0.4239
2026-01-10 17:48:19,151: t15.2024.03.15 val PER: 0.3921
2026-01-10 17:48:19,152: t15.2024.03.17 val PER: 0.3675
2026-01-10 17:48:19,153: t15.2024.05.10 val PER: 0.3759
2026-01-10 17:48:19,155: t15.2024.06.14 val PER: 0.3470
2026-01-10 17:48:19,156: t15.2024.07.19 val PER: 0.4252
2026-01-10 17:48:19,157: t15.2024.07.21 val PER: 0.3207
2026-01-10 17:48:19,159: t15.2024.07.28 val PER: 0.3610
2026-01-10 17:48:19,160: t15.2025.01.10 val PER: 0.4669
2026-01-10 17:48:19,161: t15.2025.01.12 val PER: 0.3703
2026-01-10 17:48:19,163: t15.2025.03.14 val PER: 0.4689
2026-01-10 17:48:19,164: t15.2025.03.16 val PER: 0.4136
2026-01-10 17:48:19,166: t15.2025.03.30 val PER: 0.4586
2026-01-10 17:48:19,168: t15.2025.04.13 val PER: 0.4080
2026-01-10 17:48:19,169: New best val WER(5gram) 87.48% --> 85.66%
2026-01-10 17:48:20,313: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_9500
2026-01-10 17:48:28,554: Train batch 9600: loss: 39.73 grad norm: 59.61 time: 0.078
2026-01-10 17:48:45,281: Train batch 9800: loss: 55.76 grad norm: 91.96 time: 0.068
2026-01-10 17:49:02,000: Train batch 10000: loss: 34.40 grad norm: 58.63 time: 0.065
2026-01-10 17:49:02,002: Running test after training batch: 10000
2026-01-10 17:49:02,116: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:49:08,063: WER debug example
  GT : you can see the code at this point as well
  PR : you can see a guy at this point is we
2026-01-10 17:49:08,206: WER debug example
  GT : how does it keep the cost down
  PR : it is a guy at the
2026-01-10 17:49:35,954: Val batch 10000: PER (avg): 0.3644 CTC Loss (avg): 60.3256 WER(5gram): 85.85% (n=256) time: 33.949
2026-01-10 17:49:35,957: WER lens: avg_true_words=5.99 avg_pred_words=4.75 max_pred_words=12
2026-01-10 17:49:35,959: t15.2023.08.13 val PER: 0.3462
2026-01-10 17:49:35,962: t15.2023.08.18 val PER: 0.3244
2026-01-10 17:49:35,963: t15.2023.08.20 val PER: 0.3010
2026-01-10 17:49:35,965: t15.2023.08.25 val PER: 0.2952
2026-01-10 17:49:35,967: t15.2023.08.27 val PER: 0.3955
2026-01-10 17:49:35,969: t15.2023.09.01 val PER: 0.2987
2026-01-10 17:49:35,971: t15.2023.09.03 val PER: 0.3646
2026-01-10 17:49:35,974: t15.2023.09.24 val PER: 0.3289
2026-01-10 17:49:35,976: t15.2023.09.29 val PER: 0.3338
2026-01-10 17:49:35,978: t15.2023.10.01 val PER: 0.4009
2026-01-10 17:49:35,980: t15.2023.10.06 val PER: 0.2939
2026-01-10 17:49:35,982: t15.2023.10.08 val PER: 0.4465
2026-01-10 17:49:35,984: t15.2023.10.13 val PER: 0.4577
2026-01-10 17:49:35,986: t15.2023.10.15 val PER: 0.3804
2026-01-10 17:49:35,987: t15.2023.10.20 val PER: 0.4094
2026-01-10 17:49:35,989: t15.2023.10.22 val PER: 0.3385
2026-01-10 17:49:35,991: t15.2023.11.03 val PER: 0.3562
2026-01-10 17:49:35,993: t15.2023.11.04 val PER: 0.1433
2026-01-10 17:49:35,995: t15.2023.11.17 val PER: 0.2208
2026-01-10 17:49:35,996: t15.2023.11.19 val PER: 0.2016
2026-01-10 17:49:35,998: t15.2023.11.26 val PER: 0.4326
2026-01-10 17:49:36,000: t15.2023.12.03 val PER: 0.3571
2026-01-10 17:49:36,002: t15.2023.12.08 val PER: 0.3688
2026-01-10 17:49:36,005: t15.2023.12.10 val PER: 0.3285
2026-01-10 17:49:36,006: t15.2023.12.17 val PER: 0.3721
2026-01-10 17:49:36,008: t15.2023.12.29 val PER: 0.3720
2026-01-10 17:49:36,010: t15.2024.02.25 val PER: 0.3090
2026-01-10 17:49:36,011: t15.2024.03.08 val PER: 0.4083
2026-01-10 17:49:36,013: t15.2024.03.15 val PER: 0.3821
2026-01-10 17:49:36,015: t15.2024.03.17 val PER: 0.3584
2026-01-10 17:49:36,016: t15.2024.05.10 val PER: 0.3834
2026-01-10 17:49:36,018: t15.2024.06.14 val PER: 0.3502
2026-01-10 17:49:36,020: t15.2024.07.19 val PER: 0.4127
2026-01-10 17:49:36,021: t15.2024.07.21 val PER: 0.3090
2026-01-10 17:49:36,024: t15.2024.07.28 val PER: 0.3662
2026-01-10 17:49:36,025: t15.2025.01.10 val PER: 0.4821
2026-01-10 17:49:36,027: t15.2025.01.12 val PER: 0.3649
2026-01-10 17:49:36,029: t15.2025.03.14 val PER: 0.4586
2026-01-10 17:49:36,030: t15.2025.03.16 val PER: 0.4175
2026-01-10 17:49:36,032: t15.2025.03.30 val PER: 0.4483
2026-01-10 17:49:36,033: t15.2025.04.13 val PER: 0.4151
2026-01-10 17:49:36,177: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_10000
2026-01-10 17:49:53,393: Train batch 10200: loss: 37.77 grad norm: 63.58 time: 0.054
2026-01-10 17:50:10,442: Train batch 10400: loss: 42.13 grad norm: 89.11 time: 0.076
2026-01-10 17:50:18,831: Running test after training batch: 10500
2026-01-10 17:50:18,932: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:50:24,886: WER debug example
  GT : you can see the code at this point as well
  PR : i see a guy at this point is we
2026-01-10 17:50:25,015: WER debug example
  GT : how does it keep the cost down
  PR : i know is he a guy at the
2026-01-10 17:50:52,359: Val batch 10500: PER (avg): 0.3589 CTC Loss (avg): 59.1531 WER(5gram): 87.48% (n=256) time: 33.525
2026-01-10 17:50:52,361: WER lens: avg_true_words=5.99 avg_pred_words=4.75 max_pred_words=13
2026-01-10 17:50:52,365: t15.2023.08.13 val PER: 0.3368
2026-01-10 17:50:52,366: t15.2023.08.18 val PER: 0.3353
2026-01-10 17:50:52,368: t15.2023.08.20 val PER: 0.2923
2026-01-10 17:50:52,369: t15.2023.08.25 val PER: 0.2861
2026-01-10 17:50:52,371: t15.2023.08.27 val PER: 0.3955
2026-01-10 17:50:52,373: t15.2023.09.01 val PER: 0.2971
2026-01-10 17:50:52,374: t15.2023.09.03 val PER: 0.3599
2026-01-10 17:50:52,376: t15.2023.09.24 val PER: 0.3228
2026-01-10 17:50:52,377: t15.2023.09.29 val PER: 0.3274
2026-01-10 17:50:52,379: t15.2023.10.01 val PER: 0.3983
2026-01-10 17:50:52,380: t15.2023.10.06 val PER: 0.2874
2026-01-10 17:50:52,382: t15.2023.10.08 val PER: 0.4411
2026-01-10 17:50:52,383: t15.2023.10.13 val PER: 0.4538
2026-01-10 17:50:52,385: t15.2023.10.15 val PER: 0.3777
2026-01-10 17:50:52,386: t15.2023.10.20 val PER: 0.3993
2026-01-10 17:50:52,388: t15.2023.10.22 val PER: 0.3318
2026-01-10 17:50:52,389: t15.2023.11.03 val PER: 0.3548
2026-01-10 17:50:52,391: t15.2023.11.04 val PER: 0.1502
2026-01-10 17:50:52,392: t15.2023.11.17 val PER: 0.2146
2026-01-10 17:50:52,393: t15.2023.11.19 val PER: 0.2056
2026-01-10 17:50:52,394: t15.2023.11.26 val PER: 0.4246
2026-01-10 17:50:52,396: t15.2023.12.03 val PER: 0.3456
2026-01-10 17:50:52,397: t15.2023.12.08 val PER: 0.3589
2026-01-10 17:50:52,398: t15.2023.12.10 val PER: 0.3246
2026-01-10 17:50:52,400: t15.2023.12.17 val PER: 0.3555
2026-01-10 17:50:52,401: t15.2023.12.29 val PER: 0.3658
2026-01-10 17:50:52,402: t15.2024.02.25 val PER: 0.3020
2026-01-10 17:50:52,404: t15.2024.03.08 val PER: 0.3969
2026-01-10 17:50:52,405: t15.2024.03.15 val PER: 0.3821
2026-01-10 17:50:52,406: t15.2024.03.17 val PER: 0.3543
2026-01-10 17:50:52,408: t15.2024.05.10 val PER: 0.3566
2026-01-10 17:50:52,409: t15.2024.06.14 val PER: 0.3580
2026-01-10 17:50:52,410: t15.2024.07.19 val PER: 0.4087
2026-01-10 17:50:52,412: t15.2024.07.21 val PER: 0.3000
2026-01-10 17:50:52,413: t15.2024.07.28 val PER: 0.3559
2026-01-10 17:50:52,415: t15.2025.01.10 val PER: 0.4587
2026-01-10 17:50:52,416: t15.2025.01.12 val PER: 0.3734
2026-01-10 17:50:52,417: t15.2025.03.14 val PER: 0.4615
2026-01-10 17:50:52,419: t15.2025.03.16 val PER: 0.3953
2026-01-10 17:50:52,420: t15.2025.03.30 val PER: 0.4552
2026-01-10 17:50:52,421: t15.2025.04.13 val PER: 0.3980
2026-01-10 17:50:52,560: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_10500
2026-01-10 17:51:00,986: Train batch 10600: loss: 41.65 grad norm: 87.11 time: 0.077
2026-01-10 17:51:17,574: Train batch 10800: loss: 57.34 grad norm: 75.45 time: 0.071
2026-01-10 17:51:34,262: Train batch 11000: loss: 58.70 grad norm: 78.10 time: 0.063
2026-01-10 17:51:34,265: Running test after training batch: 11000
2026-01-10 17:51:34,389: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:51:40,383: WER debug example
  GT : you can see the code at this point as well
  PR : you can see a guy at this point is we
2026-01-10 17:51:40,518: WER debug example
  GT : how does it keep the cost down
  PR : i know it in a row at the
2026-01-10 17:52:07,015: Val batch 11000: PER (avg): 0.3559 CTC Loss (avg): 58.1990 WER(5gram): 84.55% (n=256) time: 32.747
2026-01-10 17:52:07,017: WER lens: avg_true_words=5.99 avg_pred_words=4.76 max_pred_words=13
2026-01-10 17:52:07,020: t15.2023.08.13 val PER: 0.3326
2026-01-10 17:52:07,021: t15.2023.08.18 val PER: 0.3118
2026-01-10 17:52:07,023: t15.2023.08.20 val PER: 0.2923
2026-01-10 17:52:07,025: t15.2023.08.25 val PER: 0.2892
2026-01-10 17:52:07,027: t15.2023.08.27 val PER: 0.3939
2026-01-10 17:52:07,029: t15.2023.09.01 val PER: 0.2946
2026-01-10 17:52:07,031: t15.2023.09.03 val PER: 0.3575
2026-01-10 17:52:07,033: t15.2023.09.24 val PER: 0.3216
2026-01-10 17:52:07,034: t15.2023.09.29 val PER: 0.3146
2026-01-10 17:52:07,036: t15.2023.10.01 val PER: 0.3904
2026-01-10 17:52:07,037: t15.2023.10.06 val PER: 0.2906
2026-01-10 17:52:07,039: t15.2023.10.08 val PER: 0.4465
2026-01-10 17:52:07,041: t15.2023.10.13 val PER: 0.4562
2026-01-10 17:52:07,042: t15.2023.10.15 val PER: 0.3764
2026-01-10 17:52:07,044: t15.2023.10.20 val PER: 0.3826
2026-01-10 17:52:07,045: t15.2023.10.22 val PER: 0.3341
2026-01-10 17:52:07,047: t15.2023.11.03 val PER: 0.3501
2026-01-10 17:52:07,048: t15.2023.11.04 val PER: 0.1468
2026-01-10 17:52:07,050: t15.2023.11.17 val PER: 0.2053
2026-01-10 17:52:07,051: t15.2023.11.19 val PER: 0.1936
2026-01-10 17:52:07,052: t15.2023.11.26 val PER: 0.4174
2026-01-10 17:52:07,054: t15.2023.12.03 val PER: 0.3571
2026-01-10 17:52:07,056: t15.2023.12.08 val PER: 0.3582
2026-01-10 17:52:07,057: t15.2023.12.10 val PER: 0.3075
2026-01-10 17:52:07,059: t15.2023.12.17 val PER: 0.3638
2026-01-10 17:52:07,060: t15.2023.12.29 val PER: 0.3590
2026-01-10 17:52:07,062: t15.2024.02.25 val PER: 0.3048
2026-01-10 17:52:07,063: t15.2024.03.08 val PER: 0.4011
2026-01-10 17:52:07,065: t15.2024.03.15 val PER: 0.3752
2026-01-10 17:52:07,066: t15.2024.03.17 val PER: 0.3473
2026-01-10 17:52:07,068: t15.2024.05.10 val PER: 0.3655
2026-01-10 17:52:07,069: t15.2024.06.14 val PER: 0.3517
2026-01-10 17:52:07,071: t15.2024.07.19 val PER: 0.4047
2026-01-10 17:52:07,072: t15.2024.07.21 val PER: 0.2959
2026-01-10 17:52:07,074: t15.2024.07.28 val PER: 0.3551
2026-01-10 17:52:07,075: t15.2025.01.10 val PER: 0.4807
2026-01-10 17:52:07,076: t15.2025.01.12 val PER: 0.3618
2026-01-10 17:52:07,078: t15.2025.03.14 val PER: 0.4645
2026-01-10 17:52:07,079: t15.2025.03.16 val PER: 0.4005
2026-01-10 17:52:07,081: t15.2025.03.30 val PER: 0.4460
2026-01-10 17:52:07,082: t15.2025.04.13 val PER: 0.4009
2026-01-10 17:52:07,084: New best val WER(5gram) 85.66% --> 84.55%
2026-01-10 17:52:08,214: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_11000
2026-01-10 17:52:25,291: Train batch 11200: loss: 48.38 grad norm: 68.29 time: 0.076
2026-01-10 17:52:42,272: Train batch 11400: loss: 44.29 grad norm: 73.21 time: 0.061
2026-01-10 17:52:50,747: Running test after training batch: 11500
2026-01-10 17:52:50,890: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:52:56,918: WER debug example
  GT : you can see the code at this point as well
  PR : you can see a guy at this point is
2026-01-10 17:52:57,039: WER debug example
  GT : how does it keep the cost down
  PR : i see it in a row at the end
2026-01-10 17:53:22,923: Val batch 11500: PER (avg): 0.3465 CTC Loss (avg): 57.0845 WER(5gram): 85.07% (n=256) time: 32.173
2026-01-10 17:53:22,931: WER lens: avg_true_words=5.99 avg_pred_words=5.06 max_pred_words=13
2026-01-10 17:53:22,935: t15.2023.08.13 val PER: 0.3316
2026-01-10 17:53:22,939: t15.2023.08.18 val PER: 0.3168
2026-01-10 17:53:22,941: t15.2023.08.20 val PER: 0.2828
2026-01-10 17:53:22,943: t15.2023.08.25 val PER: 0.2741
2026-01-10 17:53:22,946: t15.2023.08.27 val PER: 0.3907
2026-01-10 17:53:22,952: t15.2023.09.01 val PER: 0.2833
2026-01-10 17:53:22,957: t15.2023.09.03 val PER: 0.3527
2026-01-10 17:53:22,960: t15.2023.09.24 val PER: 0.3083
2026-01-10 17:53:22,964: t15.2023.09.29 val PER: 0.3108
2026-01-10 17:53:22,968: t15.2023.10.01 val PER: 0.3791
2026-01-10 17:53:22,970: t15.2023.10.06 val PER: 0.2745
2026-01-10 17:53:22,972: t15.2023.10.08 val PER: 0.4317
2026-01-10 17:53:22,974: t15.2023.10.13 val PER: 0.4414
2026-01-10 17:53:22,976: t15.2023.10.15 val PER: 0.3566
2026-01-10 17:53:22,977: t15.2023.10.20 val PER: 0.3591
2026-01-10 17:53:22,979: t15.2023.10.22 val PER: 0.3163
2026-01-10 17:53:22,981: t15.2023.11.03 val PER: 0.3406
2026-01-10 17:53:22,983: t15.2023.11.04 val PER: 0.1331
2026-01-10 17:53:22,984: t15.2023.11.17 val PER: 0.1944
2026-01-10 17:53:22,986: t15.2023.11.19 val PER: 0.1896
2026-01-10 17:53:22,988: t15.2023.11.26 val PER: 0.4101
2026-01-10 17:53:22,990: t15.2023.12.03 val PER: 0.3487
2026-01-10 17:53:22,991: t15.2023.12.08 val PER: 0.3422
2026-01-10 17:53:22,993: t15.2023.12.10 val PER: 0.3049
2026-01-10 17:53:22,994: t15.2023.12.17 val PER: 0.3493
2026-01-10 17:53:22,995: t15.2023.12.29 val PER: 0.3507
2026-01-10 17:53:22,997: t15.2024.02.25 val PER: 0.2992
2026-01-10 17:53:22,999: t15.2024.03.08 val PER: 0.3969
2026-01-10 17:53:23,000: t15.2024.03.15 val PER: 0.3677
2026-01-10 17:53:23,002: t15.2024.03.17 val PER: 0.3473
2026-01-10 17:53:23,004: t15.2024.05.10 val PER: 0.3522
2026-01-10 17:53:23,005: t15.2024.06.14 val PER: 0.3502
2026-01-10 17:53:23,007: t15.2024.07.19 val PER: 0.4034
2026-01-10 17:53:23,009: t15.2024.07.21 val PER: 0.2731
2026-01-10 17:53:23,010: t15.2024.07.28 val PER: 0.3566
2026-01-10 17:53:23,012: t15.2025.01.10 val PER: 0.4614
2026-01-10 17:53:23,014: t15.2025.01.12 val PER: 0.3495
2026-01-10 17:53:23,015: t15.2025.03.14 val PER: 0.4408
2026-01-10 17:53:23,017: t15.2025.03.16 val PER: 0.4031
2026-01-10 17:53:23,019: t15.2025.03.30 val PER: 0.4322
2026-01-10 17:53:23,020: t15.2025.04.13 val PER: 0.3909
2026-01-10 17:53:23,160: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_11500
2026-01-10 17:53:31,232: Train batch 11600: loss: 48.74 grad norm: 69.59 time: 0.066
2026-01-10 17:53:47,684: Train batch 11800: loss: 36.63 grad norm: 70.62 time: 0.049
2026-01-10 17:54:04,319: Train batch 12000: loss: 49.08 grad norm: 75.77 time: 0.076
2026-01-10 17:54:04,321: Running test after training batch: 12000
2026-01-10 17:54:04,424: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:54:10,409: WER debug example
  GT : you can see the code at this point as well
  PR : you can see a go at this point is
2026-01-10 17:54:10,519: WER debug example
  GT : how does it keep the cost down
  PR : a s t u v a ra at the
2026-01-10 17:54:36,669: Val batch 12000: PER (avg): 0.3412 CTC Loss (avg): 56.2400 WER(5gram): 82.33% (n=256) time: 32.345
2026-01-10 17:54:36,671: WER lens: avg_true_words=5.99 avg_pred_words=5.14 max_pred_words=13
2026-01-10 17:54:36,674: t15.2023.08.13 val PER: 0.3202
2026-01-10 17:54:36,677: t15.2023.08.18 val PER: 0.3068
2026-01-10 17:54:36,683: t15.2023.08.20 val PER: 0.2724
2026-01-10 17:54:36,685: t15.2023.08.25 val PER: 0.2696
2026-01-10 17:54:36,686: t15.2023.08.27 val PER: 0.3778
2026-01-10 17:54:36,688: t15.2023.09.01 val PER: 0.2776
2026-01-10 17:54:36,689: t15.2023.09.03 val PER: 0.3361
2026-01-10 17:54:36,691: t15.2023.09.24 val PER: 0.3058
2026-01-10 17:54:36,693: t15.2023.09.29 val PER: 0.3038
2026-01-10 17:54:36,695: t15.2023.10.01 val PER: 0.3712
2026-01-10 17:54:36,696: t15.2023.10.06 val PER: 0.2777
2026-01-10 17:54:36,698: t15.2023.10.08 val PER: 0.4208
2026-01-10 17:54:36,700: t15.2023.10.13 val PER: 0.4352
2026-01-10 17:54:36,701: t15.2023.10.15 val PER: 0.3612
2026-01-10 17:54:36,703: t15.2023.10.20 val PER: 0.3557
2026-01-10 17:54:36,704: t15.2023.10.22 val PER: 0.3285
2026-01-10 17:54:36,706: t15.2023.11.03 val PER: 0.3521
2026-01-10 17:54:36,707: t15.2023.11.04 val PER: 0.1331
2026-01-10 17:54:36,709: t15.2023.11.17 val PER: 0.2006
2026-01-10 17:54:36,710: t15.2023.11.19 val PER: 0.1816
2026-01-10 17:54:36,712: t15.2023.11.26 val PER: 0.3993
2026-01-10 17:54:36,713: t15.2023.12.03 val PER: 0.3414
2026-01-10 17:54:36,715: t15.2023.12.08 val PER: 0.3402
2026-01-10 17:54:36,716: t15.2023.12.10 val PER: 0.3009
2026-01-10 17:54:36,718: t15.2023.12.17 val PER: 0.3462
2026-01-10 17:54:36,719: t15.2023.12.29 val PER: 0.3445
2026-01-10 17:54:36,721: t15.2024.02.25 val PER: 0.2935
2026-01-10 17:54:36,723: t15.2024.03.08 val PER: 0.3898
2026-01-10 17:54:36,725: t15.2024.03.15 val PER: 0.3558
2026-01-10 17:54:36,726: t15.2024.03.17 val PER: 0.3312
2026-01-10 17:54:36,728: t15.2024.05.10 val PER: 0.3626
2026-01-10 17:54:36,729: t15.2024.06.14 val PER: 0.3423
2026-01-10 17:54:36,731: t15.2024.07.19 val PER: 0.3929
2026-01-10 17:54:36,732: t15.2024.07.21 val PER: 0.2793
2026-01-10 17:54:36,733: t15.2024.07.28 val PER: 0.3449
2026-01-10 17:54:36,735: t15.2025.01.10 val PER: 0.4449
2026-01-10 17:54:36,736: t15.2025.01.12 val PER: 0.3380
2026-01-10 17:54:36,738: t15.2025.03.14 val PER: 0.4482
2026-01-10 17:54:36,739: t15.2025.03.16 val PER: 0.3783
2026-01-10 17:54:36,741: t15.2025.03.30 val PER: 0.4299
2026-01-10 17:54:36,742: t15.2025.04.13 val PER: 0.4023
2026-01-10 17:54:36,744: New best val WER(5gram) 84.55% --> 82.33%
2026-01-10 17:54:37,878: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_12000
2026-01-10 17:54:54,430: Train batch 12200: loss: 39.54 grad norm: 72.98 time: 0.071
2026-01-10 17:55:10,960: Train batch 12400: loss: 29.87 grad norm: 58.22 time: 0.045
2026-01-10 17:55:19,410: Running test after training batch: 12500
2026-01-10 17:55:19,534: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:55:25,744: WER debug example
  GT : you can see the code at this point as well
  PR : you can see a go at this point is
2026-01-10 17:55:25,868: WER debug example
  GT : how does it keep the cost down
  PR : it is a guy at a
2026-01-10 17:55:50,979: Val batch 12500: PER (avg): 0.3368 CTC Loss (avg): 55.8369 WER(5gram): 81.42% (n=256) time: 31.567
2026-01-10 17:55:50,981: WER lens: avg_true_words=5.99 avg_pred_words=5.14 max_pred_words=13
2026-01-10 17:55:50,984: t15.2023.08.13 val PER: 0.3160
2026-01-10 17:55:50,985: t15.2023.08.18 val PER: 0.2925
2026-01-10 17:55:50,987: t15.2023.08.20 val PER: 0.2701
2026-01-10 17:55:50,989: t15.2023.08.25 val PER: 0.2636
2026-01-10 17:55:50,990: t15.2023.08.27 val PER: 0.3698
2026-01-10 17:55:50,992: t15.2023.09.01 val PER: 0.2703
2026-01-10 17:55:50,994: t15.2023.09.03 val PER: 0.3266
2026-01-10 17:55:50,995: t15.2023.09.24 val PER: 0.3083
2026-01-10 17:55:50,997: t15.2023.09.29 val PER: 0.3012
2026-01-10 17:55:50,998: t15.2023.10.01 val PER: 0.3692
2026-01-10 17:55:51,000: t15.2023.10.06 val PER: 0.2713
2026-01-10 17:55:51,002: t15.2023.10.08 val PER: 0.4127
2026-01-10 17:55:51,003: t15.2023.10.13 val PER: 0.4399
2026-01-10 17:55:51,005: t15.2023.10.15 val PER: 0.3448
2026-01-10 17:55:51,007: t15.2023.10.20 val PER: 0.3591
2026-01-10 17:55:51,009: t15.2023.10.22 val PER: 0.3174
2026-01-10 17:55:51,010: t15.2023.11.03 val PER: 0.3412
2026-01-10 17:55:51,012: t15.2023.11.04 val PER: 0.1297
2026-01-10 17:55:51,014: t15.2023.11.17 val PER: 0.1773
2026-01-10 17:55:51,016: t15.2023.11.19 val PER: 0.1856
2026-01-10 17:55:51,018: t15.2023.11.26 val PER: 0.4022
2026-01-10 17:55:51,020: t15.2023.12.03 val PER: 0.3235
2026-01-10 17:55:51,021: t15.2023.12.08 val PER: 0.3389
2026-01-10 17:55:51,023: t15.2023.12.10 val PER: 0.2852
2026-01-10 17:55:51,024: t15.2023.12.17 val PER: 0.3482
2026-01-10 17:55:51,026: t15.2023.12.29 val PER: 0.3432
2026-01-10 17:55:51,027: t15.2024.02.25 val PER: 0.2921
2026-01-10 17:55:51,029: t15.2024.03.08 val PER: 0.3926
2026-01-10 17:55:51,030: t15.2024.03.15 val PER: 0.3502
2026-01-10 17:55:51,032: t15.2024.03.17 val PER: 0.3347
2026-01-10 17:55:51,033: t15.2024.05.10 val PER: 0.3507
2026-01-10 17:55:51,035: t15.2024.06.14 val PER: 0.3312
2026-01-10 17:55:51,036: t15.2024.07.19 val PER: 0.3929
2026-01-10 17:55:51,038: t15.2024.07.21 val PER: 0.2793
2026-01-10 17:55:51,039: t15.2024.07.28 val PER: 0.3426
2026-01-10 17:55:51,041: t15.2025.01.10 val PER: 0.4477
2026-01-10 17:55:51,042: t15.2025.01.12 val PER: 0.3395
2026-01-10 17:55:51,044: t15.2025.03.14 val PER: 0.4320
2026-01-10 17:55:51,046: t15.2025.03.16 val PER: 0.3835
2026-01-10 17:55:51,048: t15.2025.03.30 val PER: 0.4310
2026-01-10 17:55:51,049: t15.2025.04.13 val PER: 0.3937
2026-01-10 17:55:51,051: New best val WER(5gram) 82.33% --> 81.42%
2026-01-10 17:55:52,201: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_12500
2026-01-10 17:56:00,452: Train batch 12600: loss: 37.57 grad norm: 80.12 time: 0.062
2026-01-10 17:56:17,166: Train batch 12800: loss: 35.38 grad norm: 69.83 time: 0.057
2026-01-10 17:56:34,002: Train batch 13000: loss: 35.74 grad norm: 69.50 time: 0.073
2026-01-10 17:56:34,004: Running test after training batch: 13000
2026-01-10 17:56:34,108: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:56:40,128: WER debug example
  GT : you can see the code at this point as well
  PR : go see a go at this point it's
2026-01-10 17:56:40,235: WER debug example
  GT : how does it keep the cost down
  PR : is it a go at the
2026-01-10 17:57:05,737: Val batch 13000: PER (avg): 0.3348 CTC Loss (avg): 54.9588 WER(5gram): 83.83% (n=256) time: 31.730
2026-01-10 17:57:05,740: WER lens: avg_true_words=5.99 avg_pred_words=4.77 max_pred_words=13
2026-01-10 17:57:05,742: t15.2023.08.13 val PER: 0.3129
2026-01-10 17:57:05,744: t15.2023.08.18 val PER: 0.2900
2026-01-10 17:57:05,746: t15.2023.08.20 val PER: 0.2693
2026-01-10 17:57:05,747: t15.2023.08.25 val PER: 0.2620
2026-01-10 17:57:05,749: t15.2023.08.27 val PER: 0.3810
2026-01-10 17:57:05,750: t15.2023.09.01 val PER: 0.2679
2026-01-10 17:57:05,754: t15.2023.09.03 val PER: 0.3468
2026-01-10 17:57:05,756: t15.2023.09.24 val PER: 0.3022
2026-01-10 17:57:05,757: t15.2023.09.29 val PER: 0.2993
2026-01-10 17:57:05,759: t15.2023.10.01 val PER: 0.3633
2026-01-10 17:57:05,760: t15.2023.10.06 val PER: 0.2702
2026-01-10 17:57:05,762: t15.2023.10.08 val PER: 0.4154
2026-01-10 17:57:05,764: t15.2023.10.13 val PER: 0.4313
2026-01-10 17:57:05,765: t15.2023.10.15 val PER: 0.3467
2026-01-10 17:57:05,767: t15.2023.10.20 val PER: 0.3557
2026-01-10 17:57:05,768: t15.2023.10.22 val PER: 0.3174
2026-01-10 17:57:05,770: t15.2023.11.03 val PER: 0.3372
2026-01-10 17:57:05,772: t15.2023.11.04 val PER: 0.1297
2026-01-10 17:57:05,774: t15.2023.11.17 val PER: 0.1851
2026-01-10 17:57:05,776: t15.2023.11.19 val PER: 0.1796
2026-01-10 17:57:05,778: t15.2023.11.26 val PER: 0.3935
2026-01-10 17:57:05,779: t15.2023.12.03 val PER: 0.3193
2026-01-10 17:57:05,781: t15.2023.12.08 val PER: 0.3362
2026-01-10 17:57:05,783: t15.2023.12.10 val PER: 0.2838
2026-01-10 17:57:05,785: t15.2023.12.17 val PER: 0.3441
2026-01-10 17:57:05,787: t15.2023.12.29 val PER: 0.3363
2026-01-10 17:57:05,789: t15.2024.02.25 val PER: 0.2921
2026-01-10 17:57:05,791: t15.2024.03.08 val PER: 0.3798
2026-01-10 17:57:05,792: t15.2024.03.15 val PER: 0.3546
2026-01-10 17:57:05,794: t15.2024.03.17 val PER: 0.3333
2026-01-10 17:57:05,795: t15.2024.05.10 val PER: 0.3566
2026-01-10 17:57:05,797: t15.2024.06.14 val PER: 0.3297
2026-01-10 17:57:05,799: t15.2024.07.19 val PER: 0.3935
2026-01-10 17:57:05,801: t15.2024.07.21 val PER: 0.2786
2026-01-10 17:57:05,802: t15.2024.07.28 val PER: 0.3331
2026-01-10 17:57:05,804: t15.2025.01.10 val PER: 0.4311
2026-01-10 17:57:05,805: t15.2025.01.12 val PER: 0.3410
2026-01-10 17:57:05,807: t15.2025.03.14 val PER: 0.4527
2026-01-10 17:57:05,809: t15.2025.03.16 val PER: 0.3665
2026-01-10 17:57:05,810: t15.2025.03.30 val PER: 0.4195
2026-01-10 17:57:05,812: t15.2025.04.13 val PER: 0.3951
2026-01-10 17:57:05,954: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_13000
2026-01-10 17:57:22,589: Train batch 13200: loss: 54.88 grad norm: 82.05 time: 0.060
2026-01-10 17:57:39,381: Train batch 13400: loss: 39.40 grad norm: 83.93 time: 0.068
2026-01-10 17:57:47,649: Running test after training batch: 13500
2026-01-10 17:57:47,759: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:57:53,655: WER debug example
  GT : you can see the code at this point as well
  PR : you may see a go at this point is
2026-01-10 17:57:53,779: WER debug example
  GT : how does it keep the cost down
  PR : i know it to be a guy at the
2026-01-10 17:58:18,547: Val batch 13500: PER (avg): 0.3300 CTC Loss (avg): 54.4473 WER(5gram): 82.53% (n=256) time: 30.896
2026-01-10 17:58:18,550: WER lens: avg_true_words=5.99 avg_pred_words=5.05 max_pred_words=12
2026-01-10 17:58:18,553: t15.2023.08.13 val PER: 0.3129
2026-01-10 17:58:18,555: t15.2023.08.18 val PER: 0.2867
2026-01-10 17:58:18,557: t15.2023.08.20 val PER: 0.2597
2026-01-10 17:58:18,558: t15.2023.08.25 val PER: 0.2560
2026-01-10 17:58:18,560: t15.2023.08.27 val PER: 0.3746
2026-01-10 17:58:18,562: t15.2023.09.01 val PER: 0.2606
2026-01-10 17:58:18,564: t15.2023.09.03 val PER: 0.3278
2026-01-10 17:58:18,565: t15.2023.09.24 val PER: 0.2937
2026-01-10 17:58:18,567: t15.2023.09.29 val PER: 0.2910
2026-01-10 17:58:18,568: t15.2023.10.01 val PER: 0.3560
2026-01-10 17:58:18,570: t15.2023.10.06 val PER: 0.2659
2026-01-10 17:58:18,572: t15.2023.10.08 val PER: 0.4114
2026-01-10 17:58:18,573: t15.2023.10.13 val PER: 0.4267
2026-01-10 17:58:18,575: t15.2023.10.15 val PER: 0.3401
2026-01-10 17:58:18,576: t15.2023.10.20 val PER: 0.3624
2026-01-10 17:58:18,578: t15.2023.10.22 val PER: 0.3096
2026-01-10 17:58:18,579: t15.2023.11.03 val PER: 0.3297
2026-01-10 17:58:18,581: t15.2023.11.04 val PER: 0.1365
2026-01-10 17:58:18,583: t15.2023.11.17 val PER: 0.1711
2026-01-10 17:58:18,584: t15.2023.11.19 val PER: 0.1856
2026-01-10 17:58:18,586: t15.2023.11.26 val PER: 0.3848
2026-01-10 17:58:18,587: t15.2023.12.03 val PER: 0.3214
2026-01-10 17:58:18,589: t15.2023.12.08 val PER: 0.3289
2026-01-10 17:58:18,590: t15.2023.12.10 val PER: 0.2825
2026-01-10 17:58:18,592: t15.2023.12.17 val PER: 0.3347
2026-01-10 17:58:18,594: t15.2023.12.29 val PER: 0.3425
2026-01-10 17:58:18,595: t15.2024.02.25 val PER: 0.2907
2026-01-10 17:58:18,597: t15.2024.03.08 val PER: 0.3812
2026-01-10 17:58:18,600: t15.2024.03.15 val PER: 0.3446
2026-01-10 17:58:18,602: t15.2024.03.17 val PER: 0.3319
2026-01-10 17:58:18,604: t15.2024.05.10 val PER: 0.3522
2026-01-10 17:58:18,605: t15.2024.06.14 val PER: 0.3233
2026-01-10 17:58:18,606: t15.2024.07.19 val PER: 0.3876
2026-01-10 17:58:18,608: t15.2024.07.21 val PER: 0.2683
2026-01-10 17:58:18,609: t15.2024.07.28 val PER: 0.3331
2026-01-10 17:58:18,611: t15.2025.01.10 val PER: 0.4394
2026-01-10 17:58:18,612: t15.2025.01.12 val PER: 0.3395
2026-01-10 17:58:18,614: t15.2025.03.14 val PER: 0.4453
2026-01-10 17:58:18,615: t15.2025.03.16 val PER: 0.3704
2026-01-10 17:58:18,617: t15.2025.03.30 val PER: 0.4149
2026-01-10 17:58:18,619: t15.2025.04.13 val PER: 0.3894
2026-01-10 17:58:18,754: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_13500
2026-01-10 17:58:27,452: Train batch 13600: loss: 49.59 grad norm: 74.02 time: 0.068
2026-01-10 17:58:44,120: Train batch 13800: loss: 40.87 grad norm: 83.43 time: 0.062
2026-01-10 17:59:00,813: Train batch 14000: loss: 52.05 grad norm: 101.61 time: 0.056
2026-01-10 17:59:00,815: Running test after training batch: 14000
2026-01-10 17:59:00,955: WER debug GT example: You can see the code at this point as well.
2026-01-10 17:59:07,694: WER debug example
  GT : you can see the code at this point as well
  PR : you may be a good at this point so we
2026-01-10 17:59:07,815: WER debug example
  GT : how does it keep the cost down
  PR : i know it to be a guy at the
2026-01-10 17:59:32,612: Val batch 14000: PER (avg): 0.3287 CTC Loss (avg): 54.1508 WER(5gram): 82.79% (n=256) time: 31.794
2026-01-10 17:59:32,614: WER lens: avg_true_words=5.99 avg_pred_words=5.11 max_pred_words=14
2026-01-10 17:59:32,617: t15.2023.08.13 val PER: 0.3077
2026-01-10 17:59:32,618: t15.2023.08.18 val PER: 0.2842
2026-01-10 17:59:32,620: t15.2023.08.20 val PER: 0.2661
2026-01-10 17:59:32,622: t15.2023.08.25 val PER: 0.2500
2026-01-10 17:59:32,623: t15.2023.08.27 val PER: 0.3730
2026-01-10 17:59:32,625: t15.2023.09.01 val PER: 0.2606
2026-01-10 17:59:32,627: t15.2023.09.03 val PER: 0.3314
2026-01-10 17:59:32,628: t15.2023.09.24 val PER: 0.2779
2026-01-10 17:59:32,630: t15.2023.09.29 val PER: 0.2904
2026-01-10 17:59:32,631: t15.2023.10.01 val PER: 0.3554
2026-01-10 17:59:32,633: t15.2023.10.06 val PER: 0.2626
2026-01-10 17:59:32,634: t15.2023.10.08 val PER: 0.4168
2026-01-10 17:59:32,636: t15.2023.10.13 val PER: 0.4244
2026-01-10 17:59:32,637: t15.2023.10.15 val PER: 0.3494
2026-01-10 17:59:32,639: t15.2023.10.20 val PER: 0.3792
2026-01-10 17:59:32,640: t15.2023.10.22 val PER: 0.3018
2026-01-10 17:59:32,642: t15.2023.11.03 val PER: 0.3331
2026-01-10 17:59:32,643: t15.2023.11.04 val PER: 0.1263
2026-01-10 17:59:32,644: t15.2023.11.17 val PER: 0.1726
2026-01-10 17:59:32,646: t15.2023.11.19 val PER: 0.1776
2026-01-10 17:59:32,647: t15.2023.11.26 val PER: 0.3884
2026-01-10 17:59:32,649: t15.2023.12.03 val PER: 0.3235
2026-01-10 17:59:32,650: t15.2023.12.08 val PER: 0.3262
2026-01-10 17:59:32,652: t15.2023.12.10 val PER: 0.2799
2026-01-10 17:59:32,653: t15.2023.12.17 val PER: 0.3274
2026-01-10 17:59:32,654: t15.2023.12.29 val PER: 0.3397
2026-01-10 17:59:32,656: t15.2024.02.25 val PER: 0.2879
2026-01-10 17:59:32,657: t15.2024.03.08 val PER: 0.3798
2026-01-10 17:59:32,659: t15.2024.03.15 val PER: 0.3502
2026-01-10 17:59:32,660: t15.2024.03.17 val PER: 0.3298
2026-01-10 17:59:32,662: t15.2024.05.10 val PER: 0.3462
2026-01-10 17:59:32,663: t15.2024.06.14 val PER: 0.3139
2026-01-10 17:59:32,665: t15.2024.07.19 val PER: 0.3817
2026-01-10 17:59:32,666: t15.2024.07.21 val PER: 0.2655
2026-01-10 17:59:32,667: t15.2024.07.28 val PER: 0.3309
2026-01-10 17:59:32,669: t15.2025.01.10 val PER: 0.4187
2026-01-10 17:59:32,670: t15.2025.01.12 val PER: 0.3433
2026-01-10 17:59:32,671: t15.2025.03.14 val PER: 0.4423
2026-01-10 17:59:32,673: t15.2025.03.16 val PER: 0.3665
2026-01-10 17:59:32,674: t15.2025.03.30 val PER: 0.4207
2026-01-10 17:59:32,675: t15.2025.04.13 val PER: 0.3852
2026-01-10 17:59:32,815: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_14000
2026-01-10 17:59:49,385: Train batch 14200: loss: 43.95 grad norm: 64.60 time: 0.061
2026-01-10 18:00:06,279: Train batch 14400: loss: 33.33 grad norm: 64.58 time: 0.068
2026-01-10 18:00:14,732: Running test after training batch: 14500
2026-01-10 18:00:14,834: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:00:20,699: WER debug example
  GT : you can see the code at this point as well
  PR : go and see a go at this point as we
2026-01-10 18:00:20,822: WER debug example
  GT : how does it keep the cost down
  PR : i know it to be a guy at the
2026-01-10 18:00:45,483: Val batch 14500: PER (avg): 0.3242 CTC Loss (avg): 53.6140 WER(5gram): 82.72% (n=256) time: 30.748
2026-01-10 18:00:45,485: WER lens: avg_true_words=5.99 avg_pred_words=5.22 max_pred_words=13
2026-01-10 18:00:45,487: t15.2023.08.13 val PER: 0.3087
2026-01-10 18:00:45,489: t15.2023.08.18 val PER: 0.2808
2026-01-10 18:00:45,491: t15.2023.08.20 val PER: 0.2637
2026-01-10 18:00:45,493: t15.2023.08.25 val PER: 0.2515
2026-01-10 18:00:45,495: t15.2023.08.27 val PER: 0.3714
2026-01-10 18:00:45,496: t15.2023.09.01 val PER: 0.2541
2026-01-10 18:00:45,498: t15.2023.09.03 val PER: 0.3207
2026-01-10 18:00:45,500: t15.2023.09.24 val PER: 0.2852
2026-01-10 18:00:45,502: t15.2023.09.29 val PER: 0.2884
2026-01-10 18:00:45,504: t15.2023.10.01 val PER: 0.3540
2026-01-10 18:00:45,505: t15.2023.10.06 val PER: 0.2530
2026-01-10 18:00:45,507: t15.2023.10.08 val PER: 0.4168
2026-01-10 18:00:45,509: t15.2023.10.13 val PER: 0.4127
2026-01-10 18:00:45,510: t15.2023.10.15 val PER: 0.3401
2026-01-10 18:00:45,512: t15.2023.10.20 val PER: 0.3658
2026-01-10 18:00:45,514: t15.2023.10.22 val PER: 0.3051
2026-01-10 18:00:45,516: t15.2023.11.03 val PER: 0.3284
2026-01-10 18:00:45,517: t15.2023.11.04 val PER: 0.1331
2026-01-10 18:00:45,519: t15.2023.11.17 val PER: 0.1695
2026-01-10 18:00:45,522: t15.2023.11.19 val PER: 0.1876
2026-01-10 18:00:45,523: t15.2023.11.26 val PER: 0.3848
2026-01-10 18:00:45,525: t15.2023.12.03 val PER: 0.2973
2026-01-10 18:00:45,526: t15.2023.12.08 val PER: 0.3296
2026-01-10 18:00:45,528: t15.2023.12.10 val PER: 0.2720
2026-01-10 18:00:45,529: t15.2023.12.17 val PER: 0.3264
2026-01-10 18:00:45,531: t15.2023.12.29 val PER: 0.3315
2026-01-10 18:00:45,533: t15.2024.02.25 val PER: 0.2823
2026-01-10 18:00:45,534: t15.2024.03.08 val PER: 0.3798
2026-01-10 18:00:45,536: t15.2024.03.15 val PER: 0.3396
2026-01-10 18:00:45,537: t15.2024.03.17 val PER: 0.3215
2026-01-10 18:00:45,539: t15.2024.05.10 val PER: 0.3432
2026-01-10 18:00:45,540: t15.2024.06.14 val PER: 0.3123
2026-01-10 18:00:45,542: t15.2024.07.19 val PER: 0.3757
2026-01-10 18:00:45,544: t15.2024.07.21 val PER: 0.2628
2026-01-10 18:00:45,545: t15.2024.07.28 val PER: 0.3228
2026-01-10 18:00:45,547: t15.2025.01.10 val PER: 0.4256
2026-01-10 18:00:45,548: t15.2025.01.12 val PER: 0.3333
2026-01-10 18:00:45,550: t15.2025.03.14 val PER: 0.4497
2026-01-10 18:00:45,552: t15.2025.03.16 val PER: 0.3599
2026-01-10 18:00:45,553: t15.2025.03.30 val PER: 0.4184
2026-01-10 18:00:45,555: t15.2025.04.13 val PER: 0.3723
2026-01-10 18:00:45,691: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_14500
2026-01-10 18:00:53,991: Train batch 14600: loss: 54.18 grad norm: 78.66 time: 0.064
2026-01-10 18:01:10,669: Train batch 14800: loss: 34.45 grad norm: 71.66 time: 0.055
2026-01-10 18:01:27,496: Train batch 15000: loss: 37.34 grad norm: 60.61 time: 0.056
2026-01-10 18:01:27,498: Running test after training batch: 15000
2026-01-10 18:01:27,609: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:01:33,495: WER debug example
  GT : you can see the code at this point as well
  PR : go and see a go at this point so we
2026-01-10 18:01:33,616: WER debug example
  GT : how does it keep the cost down
  PR : a s t u v a ra at the
2026-01-10 18:01:58,290: Val batch 15000: PER (avg): 0.3264 CTC Loss (avg): 53.2737 WER(5gram): 84.68% (n=256) time: 30.789
2026-01-10 18:01:58,292: WER lens: avg_true_words=5.99 avg_pred_words=5.01 max_pred_words=14
2026-01-10 18:01:58,295: t15.2023.08.13 val PER: 0.3098
2026-01-10 18:01:58,297: t15.2023.08.18 val PER: 0.2883
2026-01-10 18:01:58,299: t15.2023.08.20 val PER: 0.2645
2026-01-10 18:01:58,300: t15.2023.08.25 val PER: 0.2500
2026-01-10 18:01:58,302: t15.2023.08.27 val PER: 0.3714
2026-01-10 18:01:58,304: t15.2023.09.01 val PER: 0.2516
2026-01-10 18:01:58,306: t15.2023.09.03 val PER: 0.3230
2026-01-10 18:01:58,308: t15.2023.09.24 val PER: 0.2937
2026-01-10 18:01:58,310: t15.2023.09.29 val PER: 0.2936
2026-01-10 18:01:58,311: t15.2023.10.01 val PER: 0.3593
2026-01-10 18:01:58,313: t15.2023.10.06 val PER: 0.2443
2026-01-10 18:01:58,314: t15.2023.10.08 val PER: 0.4060
2026-01-10 18:01:58,316: t15.2023.10.13 val PER: 0.4182
2026-01-10 18:01:58,318: t15.2023.10.15 val PER: 0.3421
2026-01-10 18:01:58,319: t15.2023.10.20 val PER: 0.3490
2026-01-10 18:01:58,321: t15.2023.10.22 val PER: 0.3107
2026-01-10 18:01:58,322: t15.2023.11.03 val PER: 0.3277
2026-01-10 18:01:58,324: t15.2023.11.04 val PER: 0.1195
2026-01-10 18:01:58,325: t15.2023.11.17 val PER: 0.1711
2026-01-10 18:01:58,327: t15.2023.11.19 val PER: 0.1717
2026-01-10 18:01:58,328: t15.2023.11.26 val PER: 0.3775
2026-01-10 18:01:58,330: t15.2023.12.03 val PER: 0.3057
2026-01-10 18:01:58,332: t15.2023.12.08 val PER: 0.3329
2026-01-10 18:01:58,333: t15.2023.12.10 val PER: 0.2694
2026-01-10 18:01:58,335: t15.2023.12.17 val PER: 0.3326
2026-01-10 18:01:58,337: t15.2023.12.29 val PER: 0.3322
2026-01-10 18:01:58,339: t15.2024.02.25 val PER: 0.2823
2026-01-10 18:01:58,341: t15.2024.03.08 val PER: 0.3855
2026-01-10 18:01:58,342: t15.2024.03.15 val PER: 0.3508
2026-01-10 18:01:58,344: t15.2024.03.17 val PER: 0.3285
2026-01-10 18:01:58,346: t15.2024.05.10 val PER: 0.3492
2026-01-10 18:01:58,347: t15.2024.06.14 val PER: 0.3170
2026-01-10 18:01:58,349: t15.2024.07.19 val PER: 0.3804
2026-01-10 18:01:58,351: t15.2024.07.21 val PER: 0.2662
2026-01-10 18:01:58,352: t15.2024.07.28 val PER: 0.3331
2026-01-10 18:01:58,354: t15.2025.01.10 val PER: 0.4353
2026-01-10 18:01:58,355: t15.2025.01.12 val PER: 0.3372
2026-01-10 18:01:58,357: t15.2025.03.14 val PER: 0.4467
2026-01-10 18:01:58,358: t15.2025.03.16 val PER: 0.3573
2026-01-10 18:01:58,360: t15.2025.03.30 val PER: 0.4184
2026-01-10 18:01:58,361: t15.2025.04.13 val PER: 0.3695
2026-01-10 18:01:58,500: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_15000
2026-01-10 18:02:15,635: Train batch 15200: loss: 31.02 grad norm: 59.72 time: 0.062
2026-01-10 18:02:32,221: Train batch 15400: loss: 45.59 grad norm: 88.26 time: 0.055
2026-01-10 18:02:40,532: Running test after training batch: 15500
2026-01-10 18:02:40,671: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:02:46,556: WER debug example
  GT : you can see the code at this point as well
  PR : go see a go at this point as we
2026-01-10 18:02:46,680: WER debug example
  GT : how does it keep the cost down
  PR : i know it to be a guy at the
2026-01-10 18:03:10,983: Val batch 15500: PER (avg): 0.3216 CTC Loss (avg): 53.0798 WER(5gram): 83.90% (n=256) time: 30.449
2026-01-10 18:03:10,985: WER lens: avg_true_words=5.99 avg_pred_words=5.20 max_pred_words=13
2026-01-10 18:03:10,988: t15.2023.08.13 val PER: 0.2963
2026-01-10 18:03:10,990: t15.2023.08.18 val PER: 0.2842
2026-01-10 18:03:10,992: t15.2023.08.20 val PER: 0.2542
2026-01-10 18:03:10,994: t15.2023.08.25 val PER: 0.2440
2026-01-10 18:03:10,995: t15.2023.08.27 val PER: 0.3762
2026-01-10 18:03:10,997: t15.2023.09.01 val PER: 0.2492
2026-01-10 18:03:10,998: t15.2023.09.03 val PER: 0.3195
2026-01-10 18:03:11,000: t15.2023.09.24 val PER: 0.2767
2026-01-10 18:03:11,002: t15.2023.09.29 val PER: 0.2821
2026-01-10 18:03:11,003: t15.2023.10.01 val PER: 0.3474
2026-01-10 18:03:11,005: t15.2023.10.06 val PER: 0.2551
2026-01-10 18:03:11,008: t15.2023.10.08 val PER: 0.4073
2026-01-10 18:03:11,009: t15.2023.10.13 val PER: 0.4151
2026-01-10 18:03:11,011: t15.2023.10.15 val PER: 0.3355
2026-01-10 18:03:11,013: t15.2023.10.20 val PER: 0.3456
2026-01-10 18:03:11,014: t15.2023.10.22 val PER: 0.2984
2026-01-10 18:03:11,016: t15.2023.11.03 val PER: 0.3250
2026-01-10 18:03:11,018: t15.2023.11.04 val PER: 0.1365
2026-01-10 18:03:11,020: t15.2023.11.17 val PER: 0.1680
2026-01-10 18:03:11,021: t15.2023.11.19 val PER: 0.1737
2026-01-10 18:03:11,023: t15.2023.11.26 val PER: 0.3783
2026-01-10 18:03:11,025: t15.2023.12.03 val PER: 0.3067
2026-01-10 18:03:11,027: t15.2023.12.08 val PER: 0.3262
2026-01-10 18:03:11,029: t15.2023.12.10 val PER: 0.2733
2026-01-10 18:03:11,031: t15.2023.12.17 val PER: 0.3337
2026-01-10 18:03:11,032: t15.2023.12.29 val PER: 0.3315
2026-01-10 18:03:11,034: t15.2024.02.25 val PER: 0.2795
2026-01-10 18:03:11,036: t15.2024.03.08 val PER: 0.3784
2026-01-10 18:03:11,037: t15.2024.03.15 val PER: 0.3396
2026-01-10 18:03:11,040: t15.2024.03.17 val PER: 0.3215
2026-01-10 18:03:11,042: t15.2024.05.10 val PER: 0.3418
2026-01-10 18:03:11,044: t15.2024.06.14 val PER: 0.3060
2026-01-10 18:03:11,046: t15.2024.07.19 val PER: 0.3784
2026-01-10 18:03:11,047: t15.2024.07.21 val PER: 0.2600
2026-01-10 18:03:11,049: t15.2024.07.28 val PER: 0.3250
2026-01-10 18:03:11,051: t15.2025.01.10 val PER: 0.4284
2026-01-10 18:03:11,053: t15.2025.01.12 val PER: 0.3295
2026-01-10 18:03:11,054: t15.2025.03.14 val PER: 0.4408
2026-01-10 18:03:11,056: t15.2025.03.16 val PER: 0.3665
2026-01-10 18:03:11,058: t15.2025.03.30 val PER: 0.4069
2026-01-10 18:03:11,059: t15.2025.04.13 val PER: 0.3695
2026-01-10 18:03:11,198: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_15500
2026-01-10 18:03:19,494: Train batch 15600: loss: 55.38 grad norm: 103.19 time: 0.067
2026-01-10 18:03:35,931: Train batch 15800: loss: 60.16 grad norm: 79.24 time: 0.072
2026-01-10 18:03:52,879: Train batch 16000: loss: 38.49 grad norm: 103.60 time: 0.060
2026-01-10 18:03:52,881: Running test after training batch: 16000
2026-01-10 18:03:53,005: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:03:58,857: WER debug example
  GT : you can see the code at this point as well
  PR : you can see a go at this point it's
2026-01-10 18:03:58,979: WER debug example
  GT : how does it keep the cost down
  PR : i know it to be a guy at the
2026-01-10 18:04:23,615: Val batch 16000: PER (avg): 0.3217 CTC Loss (avg): 52.8123 WER(5gram): 81.88% (n=256) time: 30.730
2026-01-10 18:04:23,617: WER lens: avg_true_words=5.99 avg_pred_words=5.22 max_pred_words=13
2026-01-10 18:04:23,619: t15.2023.08.13 val PER: 0.3046
2026-01-10 18:04:23,621: t15.2023.08.18 val PER: 0.2758
2026-01-10 18:04:23,623: t15.2023.08.20 val PER: 0.2597
2026-01-10 18:04:23,625: t15.2023.08.25 val PER: 0.2545
2026-01-10 18:04:23,627: t15.2023.08.27 val PER: 0.3714
2026-01-10 18:04:23,628: t15.2023.09.01 val PER: 0.2532
2026-01-10 18:04:23,630: t15.2023.09.03 val PER: 0.3171
2026-01-10 18:04:23,631: t15.2023.09.24 val PER: 0.2718
2026-01-10 18:04:23,633: t15.2023.09.29 val PER: 0.2872
2026-01-10 18:04:23,637: t15.2023.10.01 val PER: 0.3527
2026-01-10 18:04:23,638: t15.2023.10.06 val PER: 0.2551
2026-01-10 18:04:23,640: t15.2023.10.08 val PER: 0.4060
2026-01-10 18:04:23,642: t15.2023.10.13 val PER: 0.4166
2026-01-10 18:04:23,643: t15.2023.10.15 val PER: 0.3408
2026-01-10 18:04:23,645: t15.2023.10.20 val PER: 0.3591
2026-01-10 18:04:23,647: t15.2023.10.22 val PER: 0.3007
2026-01-10 18:04:23,648: t15.2023.11.03 val PER: 0.3256
2026-01-10 18:04:23,650: t15.2023.11.04 val PER: 0.1297
2026-01-10 18:04:23,652: t15.2023.11.17 val PER: 0.1773
2026-01-10 18:04:23,653: t15.2023.11.19 val PER: 0.1756
2026-01-10 18:04:23,655: t15.2023.11.26 val PER: 0.3848
2026-01-10 18:04:23,656: t15.2023.12.03 val PER: 0.3067
2026-01-10 18:04:23,658: t15.2023.12.08 val PER: 0.3262
2026-01-10 18:04:23,660: t15.2023.12.10 val PER: 0.2668
2026-01-10 18:04:23,661: t15.2023.12.17 val PER: 0.3233
2026-01-10 18:04:23,663: t15.2023.12.29 val PER: 0.3281
2026-01-10 18:04:23,664: t15.2024.02.25 val PER: 0.2781
2026-01-10 18:04:23,665: t15.2024.03.08 val PER: 0.3670
2026-01-10 18:04:23,667: t15.2024.03.15 val PER: 0.3452
2026-01-10 18:04:23,668: t15.2024.03.17 val PER: 0.3173
2026-01-10 18:04:23,670: t15.2024.05.10 val PER: 0.3403
2026-01-10 18:04:23,671: t15.2024.06.14 val PER: 0.3170
2026-01-10 18:04:23,673: t15.2024.07.19 val PER: 0.3718
2026-01-10 18:04:23,674: t15.2024.07.21 val PER: 0.2497
2026-01-10 18:04:23,676: t15.2024.07.28 val PER: 0.3294
2026-01-10 18:04:23,677: t15.2025.01.10 val PER: 0.4270
2026-01-10 18:04:23,679: t15.2025.01.12 val PER: 0.3295
2026-01-10 18:04:23,680: t15.2025.03.14 val PER: 0.4364
2026-01-10 18:04:23,682: t15.2025.03.16 val PER: 0.3534
2026-01-10 18:04:23,683: t15.2025.03.30 val PER: 0.4057
2026-01-10 18:04:23,685: t15.2025.04.13 val PER: 0.3738
2026-01-10 18:04:23,822: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_16000
2026-01-10 18:04:40,502: Train batch 16200: loss: 36.26 grad norm: 61.62 time: 0.060
2026-01-10 18:04:57,173: Train batch 16400: loss: 43.75 grad norm: 73.37 time: 0.062
2026-01-10 18:05:05,514: Running test after training batch: 16500
2026-01-10 18:05:05,629: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:05:11,461: WER debug example
  GT : you can see the code at this point as well
  PR : go and see a go at this point as we
2026-01-10 18:05:11,576: WER debug example
  GT : how does it keep the cost down
  PR : i know it to be a guy at the
2026-01-10 18:05:35,999: Val batch 16500: PER (avg): 0.3190 CTC Loss (avg): 52.4388 WER(5gram): 83.70% (n=256) time: 30.482
2026-01-10 18:05:36,001: WER lens: avg_true_words=5.99 avg_pred_words=5.19 max_pred_words=13
2026-01-10 18:05:36,004: t15.2023.08.13 val PER: 0.2983
2026-01-10 18:05:36,005: t15.2023.08.18 val PER: 0.2775
2026-01-10 18:05:36,007: t15.2023.08.20 val PER: 0.2526
2026-01-10 18:05:36,009: t15.2023.08.25 val PER: 0.2470
2026-01-10 18:05:36,010: t15.2023.08.27 val PER: 0.3633
2026-01-10 18:05:36,012: t15.2023.09.01 val PER: 0.2435
2026-01-10 18:05:36,014: t15.2023.09.03 val PER: 0.3112
2026-01-10 18:05:36,016: t15.2023.09.24 val PER: 0.2767
2026-01-10 18:05:36,018: t15.2023.09.29 val PER: 0.2808
2026-01-10 18:05:36,019: t15.2023.10.01 val PER: 0.3435
2026-01-10 18:05:36,021: t15.2023.10.06 val PER: 0.2454
2026-01-10 18:05:36,022: t15.2023.10.08 val PER: 0.4073
2026-01-10 18:05:36,024: t15.2023.10.13 val PER: 0.4104
2026-01-10 18:05:36,026: t15.2023.10.15 val PER: 0.3336
2026-01-10 18:05:36,028: t15.2023.10.20 val PER: 0.3423
2026-01-10 18:05:36,030: t15.2023.10.22 val PER: 0.3018
2026-01-10 18:05:36,032: t15.2023.11.03 val PER: 0.3243
2026-01-10 18:05:36,033: t15.2023.11.04 val PER: 0.1297
2026-01-10 18:05:36,035: t15.2023.11.17 val PER: 0.1649
2026-01-10 18:05:36,037: t15.2023.11.19 val PER: 0.1717
2026-01-10 18:05:36,039: t15.2023.11.26 val PER: 0.3761
2026-01-10 18:05:36,042: t15.2023.12.03 val PER: 0.3025
2026-01-10 18:05:36,044: t15.2023.12.08 val PER: 0.3229
2026-01-10 18:05:36,046: t15.2023.12.10 val PER: 0.2615
2026-01-10 18:05:36,048: t15.2023.12.17 val PER: 0.3233
2026-01-10 18:05:36,050: t15.2023.12.29 val PER: 0.3233
2026-01-10 18:05:36,051: t15.2024.02.25 val PER: 0.2795
2026-01-10 18:05:36,053: t15.2024.03.08 val PER: 0.3826
2026-01-10 18:05:36,055: t15.2024.03.15 val PER: 0.3440
2026-01-10 18:05:36,057: t15.2024.03.17 val PER: 0.3215
2026-01-10 18:05:36,059: t15.2024.05.10 val PER: 0.3328
2026-01-10 18:05:36,060: t15.2024.06.14 val PER: 0.3076
2026-01-10 18:05:36,062: t15.2024.07.19 val PER: 0.3784
2026-01-10 18:05:36,063: t15.2024.07.21 val PER: 0.2607
2026-01-10 18:05:36,065: t15.2024.07.28 val PER: 0.3235
2026-01-10 18:05:36,066: t15.2025.01.10 val PER: 0.4256
2026-01-10 18:05:36,068: t15.2025.01.12 val PER: 0.3287
2026-01-10 18:05:36,069: t15.2025.03.14 val PER: 0.4438
2026-01-10 18:05:36,071: t15.2025.03.16 val PER: 0.3521
2026-01-10 18:05:36,072: t15.2025.03.30 val PER: 0.4126
2026-01-10 18:05:36,074: t15.2025.04.13 val PER: 0.3666
2026-01-10 18:05:36,211: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_16500
2026-01-10 18:05:44,788: Train batch 16600: loss: 41.90 grad norm: 74.03 time: 0.058
2026-01-10 18:06:01,585: Train batch 16800: loss: 56.87 grad norm: 106.68 time: 0.067
2026-01-10 18:06:18,239: Train batch 17000: loss: 37.93 grad norm: 62.26 time: 0.086
2026-01-10 18:06:18,243: Running test after training batch: 17000
2026-01-10 18:06:18,344: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:06:24,465: WER debug example
  GT : you can see the code at this point as well
  PR : go see a go at this point as we
2026-01-10 18:06:24,584: WER debug example
  GT : how does it keep the cost down
  PR : i know it to be a guy at a
2026-01-10 18:06:48,631: Val batch 17000: PER (avg): 0.3191 CTC Loss (avg): 52.3455 WER(5gram): 82.33% (n=256) time: 30.385
2026-01-10 18:06:48,633: WER lens: avg_true_words=5.99 avg_pred_words=5.20 max_pred_words=13
2026-01-10 18:06:48,636: t15.2023.08.13 val PER: 0.3015
2026-01-10 18:06:48,638: t15.2023.08.18 val PER: 0.2791
2026-01-10 18:06:48,640: t15.2023.08.20 val PER: 0.2510
2026-01-10 18:06:48,642: t15.2023.08.25 val PER: 0.2470
2026-01-10 18:06:48,643: t15.2023.08.27 val PER: 0.3617
2026-01-10 18:06:48,645: t15.2023.09.01 val PER: 0.2476
2026-01-10 18:06:48,647: t15.2023.09.03 val PER: 0.3100
2026-01-10 18:06:48,648: t15.2023.09.24 val PER: 0.2706
2026-01-10 18:06:48,650: t15.2023.09.29 val PER: 0.2859
2026-01-10 18:06:48,652: t15.2023.10.01 val PER: 0.3415
2026-01-10 18:06:48,653: t15.2023.10.06 val PER: 0.2497
2026-01-10 18:06:48,655: t15.2023.10.08 val PER: 0.4046
2026-01-10 18:06:48,656: t15.2023.10.13 val PER: 0.4096
2026-01-10 18:06:48,658: t15.2023.10.15 val PER: 0.3316
2026-01-10 18:06:48,659: t15.2023.10.20 val PER: 0.3523
2026-01-10 18:06:48,661: t15.2023.10.22 val PER: 0.2973
2026-01-10 18:06:48,662: t15.2023.11.03 val PER: 0.3250
2026-01-10 18:06:48,664: t15.2023.11.04 val PER: 0.1297
2026-01-10 18:06:48,665: t15.2023.11.17 val PER: 0.1680
2026-01-10 18:06:48,667: t15.2023.11.19 val PER: 0.1796
2026-01-10 18:06:48,668: t15.2023.11.26 val PER: 0.3775
2026-01-10 18:06:48,670: t15.2023.12.03 val PER: 0.3036
2026-01-10 18:06:48,671: t15.2023.12.08 val PER: 0.3209
2026-01-10 18:06:48,673: t15.2023.12.10 val PER: 0.2628
2026-01-10 18:06:48,674: t15.2023.12.17 val PER: 0.3212
2026-01-10 18:06:48,676: t15.2023.12.29 val PER: 0.3191
2026-01-10 18:06:48,677: t15.2024.02.25 val PER: 0.2767
2026-01-10 18:06:48,679: t15.2024.03.08 val PER: 0.3798
2026-01-10 18:06:48,682: t15.2024.03.15 val PER: 0.3477
2026-01-10 18:06:48,684: t15.2024.03.17 val PER: 0.3208
2026-01-10 18:06:48,686: t15.2024.05.10 val PER: 0.3358
2026-01-10 18:06:48,687: t15.2024.06.14 val PER: 0.3076
2026-01-10 18:06:48,689: t15.2024.07.19 val PER: 0.3698
2026-01-10 18:06:48,690: t15.2024.07.21 val PER: 0.2648
2026-01-10 18:06:48,692: t15.2024.07.28 val PER: 0.3250
2026-01-10 18:06:48,693: t15.2025.01.10 val PER: 0.4353
2026-01-10 18:06:48,695: t15.2025.01.12 val PER: 0.3226
2026-01-10 18:06:48,696: t15.2025.03.14 val PER: 0.4482
2026-01-10 18:06:48,698: t15.2025.03.16 val PER: 0.3495
2026-01-10 18:06:48,699: t15.2025.03.30 val PER: 0.4138
2026-01-10 18:06:48,701: t15.2025.04.13 val PER: 0.3695
2026-01-10 18:06:48,838: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_17000
2026-01-10 18:07:05,506: Train batch 17200: loss: 46.96 grad norm: 68.40 time: 0.088
2026-01-10 18:07:22,499: Train batch 17400: loss: 50.24 grad norm: 71.78 time: 0.075
2026-01-10 18:07:30,725: Running test after training batch: 17500
2026-01-10 18:07:30,847: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:07:36,741: WER debug example
  GT : you can see the code at this point as well
  PR : you may see a go at this point as we
2026-01-10 18:07:36,865: WER debug example
  GT : how does it keep the cost down
  PR : i know it to be a guy at the
2026-01-10 18:08:00,977: Val batch 17500: PER (avg): 0.3178 CTC Loss (avg): 52.2048 WER(5gram): 81.68% (n=256) time: 30.250
2026-01-10 18:08:00,980: WER lens: avg_true_words=5.99 avg_pred_words=5.26 max_pred_words=13
2026-01-10 18:08:00,983: t15.2023.08.13 val PER: 0.2994
2026-01-10 18:08:00,985: t15.2023.08.18 val PER: 0.2749
2026-01-10 18:08:00,987: t15.2023.08.20 val PER: 0.2526
2026-01-10 18:08:00,989: t15.2023.08.25 val PER: 0.2440
2026-01-10 18:08:00,990: t15.2023.08.27 val PER: 0.3521
2026-01-10 18:08:00,992: t15.2023.09.01 val PER: 0.2451
2026-01-10 18:08:00,994: t15.2023.09.03 val PER: 0.3100
2026-01-10 18:08:00,997: t15.2023.09.24 val PER: 0.2706
2026-01-10 18:08:00,998: t15.2023.09.29 val PER: 0.2789
2026-01-10 18:08:01,000: t15.2023.10.01 val PER: 0.3454
2026-01-10 18:08:01,002: t15.2023.10.06 val PER: 0.2476
2026-01-10 18:08:01,004: t15.2023.10.08 val PER: 0.3951
2026-01-10 18:08:01,005: t15.2023.10.13 val PER: 0.4073
2026-01-10 18:08:01,007: t15.2023.10.15 val PER: 0.3283
2026-01-10 18:08:01,009: t15.2023.10.20 val PER: 0.3456
2026-01-10 18:08:01,011: t15.2023.10.22 val PER: 0.2996
2026-01-10 18:08:01,012: t15.2023.11.03 val PER: 0.3277
2026-01-10 18:08:01,014: t15.2023.11.04 val PER: 0.1297
2026-01-10 18:08:01,016: t15.2023.11.17 val PER: 0.1695
2026-01-10 18:08:01,018: t15.2023.11.19 val PER: 0.1737
2026-01-10 18:08:01,019: t15.2023.11.26 val PER: 0.3739
2026-01-10 18:08:01,021: t15.2023.12.03 val PER: 0.3036
2026-01-10 18:08:01,023: t15.2023.12.08 val PER: 0.3236
2026-01-10 18:08:01,024: t15.2023.12.10 val PER: 0.2628
2026-01-10 18:08:01,026: t15.2023.12.17 val PER: 0.3212
2026-01-10 18:08:01,028: t15.2023.12.29 val PER: 0.3171
2026-01-10 18:08:01,029: t15.2024.02.25 val PER: 0.2739
2026-01-10 18:08:01,031: t15.2024.03.08 val PER: 0.3741
2026-01-10 18:08:01,033: t15.2024.03.15 val PER: 0.3452
2026-01-10 18:08:01,035: t15.2024.03.17 val PER: 0.3166
2026-01-10 18:08:01,036: t15.2024.05.10 val PER: 0.3477
2026-01-10 18:08:01,038: t15.2024.06.14 val PER: 0.3091
2026-01-10 18:08:01,040: t15.2024.07.19 val PER: 0.3751
2026-01-10 18:08:01,042: t15.2024.07.21 val PER: 0.2614
2026-01-10 18:08:01,043: t15.2024.07.28 val PER: 0.3169
2026-01-10 18:08:01,045: t15.2025.01.10 val PER: 0.4298
2026-01-10 18:08:01,047: t15.2025.01.12 val PER: 0.3272
2026-01-10 18:08:01,048: t15.2025.03.14 val PER: 0.4379
2026-01-10 18:08:01,050: t15.2025.03.16 val PER: 0.3573
2026-01-10 18:08:01,052: t15.2025.03.30 val PER: 0.4126
2026-01-10 18:08:01,054: t15.2025.04.13 val PER: 0.3680
2026-01-10 18:08:01,191: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_17500
2026-01-10 18:08:09,814: Train batch 17600: loss: 51.12 grad norm: 94.85 time: 0.056
2026-01-10 18:08:26,611: Train batch 17800: loss: 31.80 grad norm: 62.04 time: 0.045
2026-01-10 18:08:43,927: Train batch 18000: loss: 44.41 grad norm: 88.00 time: 0.066
2026-01-10 18:08:43,929: Running test after training batch: 18000
2026-01-10 18:08:44,046: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:08:49,910: WER debug example
  GT : you can see the code at this point as well
  PR : you may see a go at this point as we
2026-01-10 18:08:50,057: WER debug example
  GT : how does it keep the cost down
  PR : i know it to be a guy at the
2026-01-10 18:09:14,563: Val batch 18000: PER (avg): 0.3179 CTC Loss (avg): 51.9964 WER(5gram): 82.46% (n=256) time: 30.631
2026-01-10 18:09:14,566: WER lens: avg_true_words=5.99 avg_pred_words=5.26 max_pred_words=13
2026-01-10 18:09:14,569: t15.2023.08.13 val PER: 0.2952
2026-01-10 18:09:14,571: t15.2023.08.18 val PER: 0.2749
2026-01-10 18:09:14,573: t15.2023.08.20 val PER: 0.2534
2026-01-10 18:09:14,574: t15.2023.08.25 val PER: 0.2440
2026-01-10 18:09:14,576: t15.2023.08.27 val PER: 0.3585
2026-01-10 18:09:14,578: t15.2023.09.01 val PER: 0.2435
2026-01-10 18:09:14,580: t15.2023.09.03 val PER: 0.3088
2026-01-10 18:09:14,581: t15.2023.09.24 val PER: 0.2718
2026-01-10 18:09:14,583: t15.2023.09.29 val PER: 0.2814
2026-01-10 18:09:14,584: t15.2023.10.01 val PER: 0.3435
2026-01-10 18:09:14,586: t15.2023.10.06 val PER: 0.2540
2026-01-10 18:09:14,588: t15.2023.10.08 val PER: 0.4073
2026-01-10 18:09:14,590: t15.2023.10.13 val PER: 0.4096
2026-01-10 18:09:14,592: t15.2023.10.15 val PER: 0.3296
2026-01-10 18:09:14,593: t15.2023.10.20 val PER: 0.3456
2026-01-10 18:09:14,595: t15.2023.10.22 val PER: 0.2951
2026-01-10 18:09:14,597: t15.2023.11.03 val PER: 0.3256
2026-01-10 18:09:14,598: t15.2023.11.04 val PER: 0.1297
2026-01-10 18:09:14,601: t15.2023.11.17 val PER: 0.1726
2026-01-10 18:09:14,603: t15.2023.11.19 val PER: 0.1796
2026-01-10 18:09:14,605: t15.2023.11.26 val PER: 0.3761
2026-01-10 18:09:14,606: t15.2023.12.03 val PER: 0.2994
2026-01-10 18:09:14,608: t15.2023.12.08 val PER: 0.3216
2026-01-10 18:09:14,610: t15.2023.12.10 val PER: 0.2602
2026-01-10 18:09:14,611: t15.2023.12.17 val PER: 0.3191
2026-01-10 18:09:14,613: t15.2023.12.29 val PER: 0.3226
2026-01-10 18:09:14,615: t15.2024.02.25 val PER: 0.2795
2026-01-10 18:09:14,616: t15.2024.03.08 val PER: 0.3812
2026-01-10 18:09:14,618: t15.2024.03.15 val PER: 0.3427
2026-01-10 18:09:14,620: t15.2024.03.17 val PER: 0.3145
2026-01-10 18:09:14,621: t15.2024.05.10 val PER: 0.3432
2026-01-10 18:09:14,623: t15.2024.06.14 val PER: 0.3028
2026-01-10 18:09:14,625: t15.2024.07.19 val PER: 0.3744
2026-01-10 18:09:14,626: t15.2024.07.21 val PER: 0.2538
2026-01-10 18:09:14,628: t15.2024.07.28 val PER: 0.3228
2026-01-10 18:09:14,629: t15.2025.01.10 val PER: 0.4229
2026-01-10 18:09:14,631: t15.2025.01.12 val PER: 0.3333
2026-01-10 18:09:14,633: t15.2025.03.14 val PER: 0.4408
2026-01-10 18:09:14,634: t15.2025.03.16 val PER: 0.3521
2026-01-10 18:09:14,636: t15.2025.03.30 val PER: 0.4092
2026-01-10 18:09:14,637: t15.2025.04.13 val PER: 0.3652
2026-01-10 18:09:14,777: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_18000
2026-01-10 18:09:31,615: Train batch 18200: loss: 44.18 grad norm: 75.14 time: 0.078
2026-01-10 18:09:48,250: Train batch 18400: loss: 31.65 grad norm: 91.03 time: 0.062
2026-01-10 18:09:56,677: Running test after training batch: 18500
2026-01-10 18:09:56,803: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:10:02,682: WER debug example
  GT : you can see the code at this point as well
  PR : you may see a go at this point it's
2026-01-10 18:10:02,813: WER debug example
  GT : how does it keep the cost down
  PR : i know it to be a guy at the
2026-01-10 18:10:26,950: Val batch 18500: PER (avg): 0.3153 CTC Loss (avg): 51.9190 WER(5gram): 81.81% (n=256) time: 30.270
2026-01-10 18:10:26,952: WER lens: avg_true_words=5.99 avg_pred_words=5.25 max_pred_words=13
2026-01-10 18:10:26,955: t15.2023.08.13 val PER: 0.2921
2026-01-10 18:10:26,956: t15.2023.08.18 val PER: 0.2699
2026-01-10 18:10:26,958: t15.2023.08.20 val PER: 0.2486
2026-01-10 18:10:26,959: t15.2023.08.25 val PER: 0.2364
2026-01-10 18:10:26,961: t15.2023.08.27 val PER: 0.3601
2026-01-10 18:10:26,962: t15.2023.09.01 val PER: 0.2435
2026-01-10 18:10:26,964: t15.2023.09.03 val PER: 0.3100
2026-01-10 18:10:26,966: t15.2023.09.24 val PER: 0.2706
2026-01-10 18:10:26,967: t15.2023.09.29 val PER: 0.2776
2026-01-10 18:10:26,969: t15.2023.10.01 val PER: 0.3395
2026-01-10 18:10:26,971: t15.2023.10.06 val PER: 0.2487
2026-01-10 18:10:26,972: t15.2023.10.08 val PER: 0.4046
2026-01-10 18:10:26,974: t15.2023.10.13 val PER: 0.4135
2026-01-10 18:10:26,976: t15.2023.10.15 val PER: 0.3303
2026-01-10 18:10:26,977: t15.2023.10.20 val PER: 0.3389
2026-01-10 18:10:26,979: t15.2023.10.22 val PER: 0.2951
2026-01-10 18:10:26,980: t15.2023.11.03 val PER: 0.3250
2026-01-10 18:10:26,982: t15.2023.11.04 val PER: 0.1195
2026-01-10 18:10:26,983: t15.2023.11.17 val PER: 0.1664
2026-01-10 18:10:26,985: t15.2023.11.19 val PER: 0.1776
2026-01-10 18:10:26,986: t15.2023.11.26 val PER: 0.3703
2026-01-10 18:10:26,988: t15.2023.12.03 val PER: 0.2994
2026-01-10 18:10:26,990: t15.2023.12.08 val PER: 0.3149
2026-01-10 18:10:26,991: t15.2023.12.10 val PER: 0.2615
2026-01-10 18:10:26,993: t15.2023.12.17 val PER: 0.3212
2026-01-10 18:10:26,994: t15.2023.12.29 val PER: 0.3205
2026-01-10 18:10:26,996: t15.2024.02.25 val PER: 0.2739
2026-01-10 18:10:26,997: t15.2024.03.08 val PER: 0.3741
2026-01-10 18:10:26,999: t15.2024.03.15 val PER: 0.3402
2026-01-10 18:10:27,000: t15.2024.03.17 val PER: 0.3117
2026-01-10 18:10:27,002: t15.2024.05.10 val PER: 0.3403
2026-01-10 18:10:27,003: t15.2024.06.14 val PER: 0.3107
2026-01-10 18:10:27,005: t15.2024.07.19 val PER: 0.3678
2026-01-10 18:10:27,006: t15.2024.07.21 val PER: 0.2545
2026-01-10 18:10:27,007: t15.2024.07.28 val PER: 0.3176
2026-01-10 18:10:27,009: t15.2025.01.10 val PER: 0.4242
2026-01-10 18:10:27,010: t15.2025.01.12 val PER: 0.3241
2026-01-10 18:10:27,012: t15.2025.03.14 val PER: 0.4423
2026-01-10 18:10:27,013: t15.2025.03.16 val PER: 0.3521
2026-01-10 18:10:27,015: t15.2025.03.30 val PER: 0.4000
2026-01-10 18:10:27,016: t15.2025.04.13 val PER: 0.3695
2026-01-10 18:10:27,154: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_18500
2026-01-10 18:10:35,484: Train batch 18600: loss: 44.49 grad norm: 72.88 time: 0.072
2026-01-10 18:10:52,105: Train batch 18800: loss: 48.68 grad norm: 81.18 time: 0.070
2026-01-10 18:11:08,836: Train batch 19000: loss: 40.24 grad norm: 66.18 time: 0.069
2026-01-10 18:11:08,839: Running test after training batch: 19000
2026-01-10 18:11:08,951: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:11:14,767: WER debug example
  GT : you can see the code at this point as well
  PR : you may be a good at this point it's
2026-01-10 18:11:14,894: WER debug example
  GT : how does it keep the cost down
  PR : a at t k a k at the
2026-01-10 18:11:39,321: Val batch 19000: PER (avg): 0.3151 CTC Loss (avg): 51.8767 WER(5gram): 81.29% (n=256) time: 30.480
2026-01-10 18:11:39,324: WER lens: avg_true_words=5.99 avg_pred_words=5.23 max_pred_words=14
2026-01-10 18:11:39,326: t15.2023.08.13 val PER: 0.2931
2026-01-10 18:11:39,328: t15.2023.08.18 val PER: 0.2733
2026-01-10 18:11:39,329: t15.2023.08.20 val PER: 0.2470
2026-01-10 18:11:39,331: t15.2023.08.25 val PER: 0.2395
2026-01-10 18:11:39,332: t15.2023.08.27 val PER: 0.3585
2026-01-10 18:11:39,334: t15.2023.09.01 val PER: 0.2516
2026-01-10 18:11:39,335: t15.2023.09.03 val PER: 0.3064
2026-01-10 18:11:39,337: t15.2023.09.24 val PER: 0.2767
2026-01-10 18:11:39,338: t15.2023.09.29 val PER: 0.2750
2026-01-10 18:11:39,339: t15.2023.10.01 val PER: 0.3448
2026-01-10 18:11:39,341: t15.2023.10.06 val PER: 0.2519
2026-01-10 18:11:39,342: t15.2023.10.08 val PER: 0.4032
2026-01-10 18:11:39,344: t15.2023.10.13 val PER: 0.4088
2026-01-10 18:11:39,345: t15.2023.10.15 val PER: 0.3289
2026-01-10 18:11:39,346: t15.2023.10.20 val PER: 0.3389
2026-01-10 18:11:39,348: t15.2023.10.22 val PER: 0.2929
2026-01-10 18:11:39,349: t15.2023.11.03 val PER: 0.3236
2026-01-10 18:11:39,351: t15.2023.11.04 val PER: 0.1229
2026-01-10 18:11:39,354: t15.2023.11.17 val PER: 0.1633
2026-01-10 18:11:39,356: t15.2023.11.19 val PER: 0.1776
2026-01-10 18:11:39,357: t15.2023.11.26 val PER: 0.3674
2026-01-10 18:11:39,359: t15.2023.12.03 val PER: 0.2962
2026-01-10 18:11:39,360: t15.2023.12.08 val PER: 0.3176
2026-01-10 18:11:39,362: t15.2023.12.10 val PER: 0.2602
2026-01-10 18:11:39,363: t15.2023.12.17 val PER: 0.3202
2026-01-10 18:11:39,364: t15.2023.12.29 val PER: 0.3205
2026-01-10 18:11:39,366: t15.2024.02.25 val PER: 0.2739
2026-01-10 18:11:39,367: t15.2024.03.08 val PER: 0.3698
2026-01-10 18:11:39,369: t15.2024.03.15 val PER: 0.3383
2026-01-10 18:11:39,370: t15.2024.03.17 val PER: 0.3110
2026-01-10 18:11:39,371: t15.2024.05.10 val PER: 0.3432
2026-01-10 18:11:39,373: t15.2024.06.14 val PER: 0.3170
2026-01-10 18:11:39,375: t15.2024.07.19 val PER: 0.3619
2026-01-10 18:11:39,376: t15.2024.07.21 val PER: 0.2524
2026-01-10 18:11:39,378: t15.2024.07.28 val PER: 0.3206
2026-01-10 18:11:39,379: t15.2025.01.10 val PER: 0.4229
2026-01-10 18:11:39,381: t15.2025.01.12 val PER: 0.3264
2026-01-10 18:11:39,382: t15.2025.03.14 val PER: 0.4408
2026-01-10 18:11:39,384: t15.2025.03.16 val PER: 0.3586
2026-01-10 18:11:39,385: t15.2025.03.30 val PER: 0.3954
2026-01-10 18:11:39,386: t15.2025.04.13 val PER: 0.3623
2026-01-10 18:11:39,388: New best val WER(5gram) 81.42% --> 81.29%
2026-01-10 18:11:40,474: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_19000
2026-01-10 18:11:57,313: Train batch 19200: loss: 35.26 grad norm: 61.76 time: 0.070
2026-01-10 18:12:14,005: Train batch 19400: loss: 34.49 grad norm: 60.61 time: 0.057
2026-01-10 18:12:22,446: Running test after training batch: 19500
2026-01-10 18:12:22,569: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:12:28,431: WER debug example
  GT : you can see the code at this point as well
  PR : you may be a good at this point it's
2026-01-10 18:12:28,557: WER debug example
  GT : how does it keep the cost down
  PR : a at t k a k at the
2026-01-10 18:12:53,223: Val batch 19500: PER (avg): 0.3151 CTC Loss (avg): 51.7892 WER(5gram): 81.23% (n=256) time: 30.775
2026-01-10 18:12:53,226: WER lens: avg_true_words=5.99 avg_pred_words=5.23 max_pred_words=14
2026-01-10 18:12:53,229: t15.2023.08.13 val PER: 0.2952
2026-01-10 18:12:53,231: t15.2023.08.18 val PER: 0.2724
2026-01-10 18:12:53,233: t15.2023.08.20 val PER: 0.2486
2026-01-10 18:12:53,235: t15.2023.08.25 val PER: 0.2410
2026-01-10 18:12:53,236: t15.2023.08.27 val PER: 0.3553
2026-01-10 18:12:53,238: t15.2023.09.01 val PER: 0.2516
2026-01-10 18:12:53,240: t15.2023.09.03 val PER: 0.3076
2026-01-10 18:12:53,242: t15.2023.09.24 val PER: 0.2718
2026-01-10 18:12:53,244: t15.2023.09.29 val PER: 0.2750
2026-01-10 18:12:53,245: t15.2023.10.01 val PER: 0.3441
2026-01-10 18:12:53,247: t15.2023.10.06 val PER: 0.2497
2026-01-10 18:12:53,249: t15.2023.10.08 val PER: 0.3965
2026-01-10 18:12:53,251: t15.2023.10.13 val PER: 0.4104
2026-01-10 18:12:53,252: t15.2023.10.15 val PER: 0.3289
2026-01-10 18:12:53,254: t15.2023.10.20 val PER: 0.3490
2026-01-10 18:12:53,256: t15.2023.10.22 val PER: 0.2929
2026-01-10 18:12:53,257: t15.2023.11.03 val PER: 0.3250
2026-01-10 18:12:53,259: t15.2023.11.04 val PER: 0.1195
2026-01-10 18:12:53,261: t15.2023.11.17 val PER: 0.1680
2026-01-10 18:12:53,262: t15.2023.11.19 val PER: 0.1776
2026-01-10 18:12:53,264: t15.2023.11.26 val PER: 0.3696
2026-01-10 18:12:53,265: t15.2023.12.03 val PER: 0.2994
2026-01-10 18:12:53,267: t15.2023.12.08 val PER: 0.3162
2026-01-10 18:12:53,269: t15.2023.12.10 val PER: 0.2576
2026-01-10 18:12:53,270: t15.2023.12.17 val PER: 0.3191
2026-01-10 18:12:53,272: t15.2023.12.29 val PER: 0.3185
2026-01-10 18:12:53,273: t15.2024.02.25 val PER: 0.2753
2026-01-10 18:12:53,275: t15.2024.03.08 val PER: 0.3698
2026-01-10 18:12:53,277: t15.2024.03.15 val PER: 0.3415
2026-01-10 18:12:53,278: t15.2024.03.17 val PER: 0.3047
2026-01-10 18:12:53,280: t15.2024.05.10 val PER: 0.3403
2026-01-10 18:12:53,282: t15.2024.06.14 val PER: 0.3076
2026-01-10 18:12:53,283: t15.2024.07.19 val PER: 0.3665
2026-01-10 18:12:53,285: t15.2024.07.21 val PER: 0.2552
2026-01-10 18:12:53,286: t15.2024.07.28 val PER: 0.3221
2026-01-10 18:12:53,288: t15.2025.01.10 val PER: 0.4215
2026-01-10 18:12:53,289: t15.2025.01.12 val PER: 0.3226
2026-01-10 18:12:53,290: t15.2025.03.14 val PER: 0.4408
2026-01-10 18:12:53,292: t15.2025.03.16 val PER: 0.3547
2026-01-10 18:12:53,293: t15.2025.03.30 val PER: 0.4011
2026-01-10 18:12:53,296: t15.2025.04.13 val PER: 0.3666
2026-01-10 18:12:53,298: New best val WER(5gram) 81.29% --> 81.23%
2026-01-10 18:12:54,450: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_19500
2026-01-10 18:13:02,651: Train batch 19600: loss: 36.92 grad norm: 71.94 time: 0.062
2026-01-10 18:13:19,296: Train batch 19800: loss: 38.16 grad norm: 69.93 time: 0.060
2026-01-10 18:13:36,222: Running test after training batch: 19999
2026-01-10 18:13:36,318: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:13:42,148: WER debug example
  GT : you can see the code at this point as well
  PR : you may be a good at this point as we
2026-01-10 18:13:42,274: WER debug example
  GT : how does it keep the cost down
  PR : i know it to be a go at
2026-01-10 18:14:06,790: Val batch 19999: PER (avg): 0.3142 CTC Loss (avg): 51.7133 WER(5gram): 80.44% (n=256) time: 30.566
2026-01-10 18:14:06,793: WER lens: avg_true_words=5.99 avg_pred_words=5.28 max_pred_words=12
2026-01-10 18:14:06,797: t15.2023.08.13 val PER: 0.2963
2026-01-10 18:14:06,799: t15.2023.08.18 val PER: 0.2674
2026-01-10 18:14:06,800: t15.2023.08.20 val PER: 0.2518
2026-01-10 18:14:06,802: t15.2023.08.25 val PER: 0.2425
2026-01-10 18:14:06,804: t15.2023.08.27 val PER: 0.3457
2026-01-10 18:14:06,805: t15.2023.09.01 val PER: 0.2451
2026-01-10 18:14:06,807: t15.2023.09.03 val PER: 0.3147
2026-01-10 18:14:06,809: t15.2023.09.24 val PER: 0.2731
2026-01-10 18:14:06,810: t15.2023.09.29 val PER: 0.2725
2026-01-10 18:14:06,812: t15.2023.10.01 val PER: 0.3402
2026-01-10 18:14:06,813: t15.2023.10.06 val PER: 0.2519
2026-01-10 18:14:06,815: t15.2023.10.08 val PER: 0.3978
2026-01-10 18:14:06,816: t15.2023.10.13 val PER: 0.4112
2026-01-10 18:14:06,818: t15.2023.10.15 val PER: 0.3289
2026-01-10 18:14:06,819: t15.2023.10.20 val PER: 0.3490
2026-01-10 18:14:06,821: t15.2023.10.22 val PER: 0.2895
2026-01-10 18:14:06,822: t15.2023.11.03 val PER: 0.3236
2026-01-10 18:14:06,824: t15.2023.11.04 val PER: 0.1195
2026-01-10 18:14:06,825: t15.2023.11.17 val PER: 0.1633
2026-01-10 18:14:06,826: t15.2023.11.19 val PER: 0.1796
2026-01-10 18:14:06,828: t15.2023.11.26 val PER: 0.3681
2026-01-10 18:14:06,829: t15.2023.12.03 val PER: 0.2973
2026-01-10 18:14:06,831: t15.2023.12.08 val PER: 0.3142
2026-01-10 18:14:06,832: t15.2023.12.10 val PER: 0.2589
2026-01-10 18:14:06,834: t15.2023.12.17 val PER: 0.3233
2026-01-10 18:14:06,835: t15.2023.12.29 val PER: 0.3164
2026-01-10 18:14:06,836: t15.2024.02.25 val PER: 0.2669
2026-01-10 18:14:06,838: t15.2024.03.08 val PER: 0.3741
2026-01-10 18:14:06,839: t15.2024.03.15 val PER: 0.3396
2026-01-10 18:14:06,840: t15.2024.03.17 val PER: 0.3110
2026-01-10 18:14:06,842: t15.2024.05.10 val PER: 0.3373
2026-01-10 18:14:06,843: t15.2024.06.14 val PER: 0.3013
2026-01-10 18:14:06,845: t15.2024.07.19 val PER: 0.3626
2026-01-10 18:14:06,846: t15.2024.07.21 val PER: 0.2586
2026-01-10 18:14:06,847: t15.2024.07.28 val PER: 0.3213
2026-01-10 18:14:06,849: t15.2025.01.10 val PER: 0.4187
2026-01-10 18:14:06,850: t15.2025.01.12 val PER: 0.3233
2026-01-10 18:14:06,852: t15.2025.03.14 val PER: 0.4305
2026-01-10 18:14:06,853: t15.2025.03.16 val PER: 0.3586
2026-01-10 18:14:06,855: t15.2025.03.30 val PER: 0.4011
2026-01-10 18:14:06,856: t15.2025.04.13 val PER: 0.3652
2026-01-10 18:14:06,857: New best val WER(5gram) 81.23% --> 80.44%
2026-01-10 18:14:08,010: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_base/checkpoint/checkpoint_batch_19999
2026-01-10 18:14:08,455: Best avg val PER achieved: 0.31424
2026-01-10 18:14:08,459: Total training time: 55.46 minutes

=== RUN diphone_long.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long
2026-01-10 18:16:08,771: Using device: cuda:0
2026-01-10 18:19:58,655: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-10 18:19:58,660: Diphone mode ENABLED: n_classes changed from 41 to 1601
2026-01-10 18:19:58,687: Using 45 sessions after filtering (from 45).
2026-01-10 18:19:59,076: Using torch.compile (if available)
2026-01-10 18:19:59,079: torch.compile not available (torch<2.0). Skipping.
2026-01-10 18:19:59,081: Initialized RNN decoding model
2026-01-10 18:19:59,082: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Identity()
  (out): Linear(in_features=768, out_features=1601, bias=True)
)
2026-01-10 18:19:59,084: Model has 45,514,817 parameters
2026-01-10 18:19:59,086: Model has 11,819,520 day-specific parameters | 25.97% of total parameters
2026-01-10 18:20:01,152: Successfully initialized datasets
2026-01-10 18:20:01,155: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-10 18:20:02,598: Train batch 0: loss: 1387.52 grad norm: 203.28 time: 0.175
2026-01-10 18:20:02,600: Running test after training batch: 0
2026-01-10 18:20:02,715: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:20:09,630: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-10 18:20:10,722: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-10 18:24:31,624: Val batch 0: PER (avg): 4.1038 CTC Loss (avg): 1560.2442 WER(5gram): 100.00% (n=256) time: 269.021
2026-01-10 18:24:31,630: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=0
2026-01-10 18:24:31,630: t15.2023.08.13 val PER: 3.3836
2026-01-10 18:24:31,630: t15.2023.08.18 val PER: 3.6588
2026-01-10 18:24:31,630: t15.2023.08.20 val PER: 3.6402
2026-01-10 18:24:31,630: t15.2023.08.25 val PER: 3.6958
2026-01-10 18:24:31,630: t15.2023.08.27 val PER: 3.4630
2026-01-10 18:24:31,630: t15.2023.09.01 val PER: 3.7679
2026-01-10 18:24:31,630: t15.2023.09.03 val PER: 3.6413
2026-01-10 18:24:31,630: t15.2023.09.24 val PER: 4.2791
2026-01-10 18:24:31,630: t15.2023.09.29 val PER: 4.3095
2026-01-10 18:24:31,631: t15.2023.10.01 val PER: 3.3144
2026-01-10 18:24:31,631: t15.2023.10.06 val PER: 4.1130
2026-01-10 18:24:31,631: t15.2023.10.08 val PER: 3.0920
2026-01-10 18:24:31,631: t15.2023.10.13 val PER: 3.8976
2026-01-10 18:24:31,631: t15.2023.10.15 val PER: 4.3441
2026-01-10 18:24:31,631: t15.2023.10.20 val PER: 4.5671
2026-01-10 18:24:31,631: t15.2023.10.22 val PER: 4.3497
2026-01-10 18:24:31,631: t15.2023.11.03 val PER: 4.6208
2026-01-10 18:24:31,631: t15.2023.11.04 val PER: 5.5631
2026-01-10 18:24:31,631: t15.2023.11.17 val PER: 5.9020
2026-01-10 18:24:31,631: t15.2023.11.19 val PER: 4.5768
2026-01-10 18:24:31,631: t15.2023.11.26 val PER: 4.6058
2026-01-10 18:24:31,631: t15.2023.12.03 val PER: 4.3340
2026-01-10 18:24:31,631: t15.2023.12.08 val PER: 4.5972
2026-01-10 18:24:31,631: t15.2023.12.10 val PER: 5.0696
2026-01-10 18:24:31,631: t15.2023.12.17 val PER: 3.7464
2026-01-10 18:24:31,632: t15.2023.12.29 val PER: 4.1050
2026-01-10 18:24:31,632: t15.2024.02.25 val PER: 3.8483
2026-01-10 18:24:31,632: t15.2024.03.08 val PER: 3.8321
2026-01-10 18:24:31,632: t15.2024.03.15 val PER: 3.7323
2026-01-10 18:24:31,632: t15.2024.03.17 val PER: 3.9700
2026-01-10 18:24:31,632: t15.2024.05.10 val PER: 3.8663
2026-01-10 18:24:31,632: t15.2024.06.14 val PER: 4.3991
2026-01-10 18:24:31,632: t15.2024.07.19 val PER: 3.0310
2026-01-10 18:24:31,632: t15.2024.07.21 val PER: 4.6076
2026-01-10 18:24:31,632: t15.2024.07.28 val PER: 4.8051
2026-01-10 18:24:31,632: t15.2025.01.10 val PER: 2.8072
2026-01-10 18:24:31,632: t15.2025.01.12 val PER: 5.3426
2026-01-10 18:24:31,632: t15.2025.03.14 val PER: 2.8580
2026-01-10 18:24:31,632: t15.2025.03.16 val PER: 5.0432
2026-01-10 18:24:31,632: t15.2025.03.30 val PER: 4.0264
2026-01-10 18:24:31,633: t15.2025.04.13 val PER: 4.4736
2026-01-10 18:24:31,634: New best val WER(5gram) inf% --> 100.00%
2026-01-10 18:24:32,659: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_0
2026-01-10 18:24:49,687: Train batch 200: loss: 169.35 grad norm: 40.14 time: 0.060
2026-01-10 18:25:06,409: Train batch 400: loss: 138.48 grad norm: 20.74 time: 0.069
2026-01-10 18:25:14,793: Running test after training batch: 500
2026-01-10 18:25:14,912: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:25:20,910: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-10 18:25:20,931: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-10 18:25:25,567: Val batch 500: PER (avg): 0.9953 CTC Loss (avg): 159.9344 WER(5gram): 100.00% (n=256) time: 10.771
2026-01-10 18:25:25,569: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=0
2026-01-10 18:25:25,571: t15.2023.08.13 val PER: 0.9979
2026-01-10 18:25:25,573: t15.2023.08.18 val PER: 0.9941
2026-01-10 18:25:25,575: t15.2023.08.20 val PER: 0.9984
2026-01-10 18:25:25,576: t15.2023.08.25 val PER: 0.9970
2026-01-10 18:25:25,578: t15.2023.08.27 val PER: 0.9968
2026-01-10 18:25:25,579: t15.2023.09.01 val PER: 0.9959
2026-01-10 18:25:25,581: t15.2023.09.03 val PER: 0.9964
2026-01-10 18:25:25,583: t15.2023.09.24 val PER: 0.9951
2026-01-10 18:25:25,584: t15.2023.09.29 val PER: 0.9987
2026-01-10 18:25:25,586: t15.2023.10.01 val PER: 1.0000
2026-01-10 18:25:25,588: t15.2023.10.06 val PER: 0.9925
2026-01-10 18:25:25,590: t15.2023.10.08 val PER: 1.0000
2026-01-10 18:25:25,591: t15.2023.10.13 val PER: 0.9992
2026-01-10 18:25:25,594: t15.2023.10.15 val PER: 0.9967
2026-01-10 18:25:25,596: t15.2023.10.20 val PER: 0.9966
2026-01-10 18:25:25,598: t15.2023.10.22 val PER: 0.9944
2026-01-10 18:25:25,599: t15.2023.11.03 val PER: 0.9925
2026-01-10 18:25:25,601: t15.2023.11.04 val PER: 0.9932
2026-01-10 18:25:25,604: t15.2023.11.17 val PER: 0.9907
2026-01-10 18:25:25,606: t15.2023.11.19 val PER: 0.9900
2026-01-10 18:25:25,607: t15.2023.11.26 val PER: 0.9964
2026-01-10 18:25:25,608: t15.2023.12.03 val PER: 0.9926
2026-01-10 18:25:25,610: t15.2023.12.08 val PER: 0.9987
2026-01-10 18:25:25,612: t15.2023.12.10 val PER: 0.9974
2026-01-10 18:25:25,614: t15.2023.12.17 val PER: 0.9938
2026-01-10 18:25:25,616: t15.2023.12.29 val PER: 0.9945
2026-01-10 18:25:25,617: t15.2024.02.25 val PER: 0.9972
2026-01-10 18:25:25,619: t15.2024.03.08 val PER: 0.9972
2026-01-10 18:25:25,620: t15.2024.03.15 val PER: 0.9981
2026-01-10 18:25:25,622: t15.2024.03.17 val PER: 0.9979
2026-01-10 18:25:25,623: t15.2024.05.10 val PER: 0.9955
2026-01-10 18:25:25,625: t15.2024.06.14 val PER: 0.9968
2026-01-10 18:25:25,626: t15.2024.07.19 val PER: 0.9941
2026-01-10 18:25:25,627: t15.2024.07.21 val PER: 0.9966
2026-01-10 18:25:25,629: t15.2024.07.28 val PER: 0.9971
2026-01-10 18:25:25,630: t15.2025.01.10 val PER: 0.9862
2026-01-10 18:25:25,632: t15.2025.01.12 val PER: 0.9938
2026-01-10 18:25:25,633: t15.2025.03.14 val PER: 0.9837
2026-01-10 18:25:25,634: t15.2025.03.16 val PER: 0.9908
2026-01-10 18:25:25,636: t15.2025.03.30 val PER: 0.9851
2026-01-10 18:25:25,637: t15.2025.04.13 val PER: 0.9872
2026-01-10 18:25:25,773: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_500
2026-01-10 18:25:34,124: Train batch 600: loss: 149.80 grad norm: 227.19 time: 0.083
2026-01-10 18:25:50,781: Train batch 800: loss: 143.05 grad norm: 82.76 time: 0.063
2026-01-10 18:26:07,564: Train batch 1000: loss: 126.96 grad norm: 76.79 time: 0.070
2026-01-10 18:26:07,566: Running test after training batch: 1000
2026-01-10 18:26:07,674: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:26:13,530: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-10 18:26:13,559: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-10 18:26:18,532: Val batch 1000: PER (avg): 0.9998 CTC Loss (avg): 152.6927 WER(5gram): 100.00% (n=256) time: 10.963
2026-01-10 18:26:18,534: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=1
2026-01-10 18:26:18,537: t15.2023.08.13 val PER: 1.0000
2026-01-10 18:26:18,538: t15.2023.08.18 val PER: 1.0000
2026-01-10 18:26:18,540: t15.2023.08.20 val PER: 1.0000
2026-01-10 18:26:18,542: t15.2023.08.25 val PER: 0.9970
2026-01-10 18:26:18,544: t15.2023.08.27 val PER: 0.9984
2026-01-10 18:26:18,545: t15.2023.09.01 val PER: 0.9992
2026-01-10 18:26:18,547: t15.2023.09.03 val PER: 1.0000
2026-01-10 18:26:18,549: t15.2023.09.24 val PER: 1.0000
2026-01-10 18:26:18,550: t15.2023.09.29 val PER: 1.0000
2026-01-10 18:26:18,552: t15.2023.10.01 val PER: 1.0000
2026-01-10 18:26:18,554: t15.2023.10.06 val PER: 1.0000
2026-01-10 18:26:18,555: t15.2023.10.08 val PER: 0.9986
2026-01-10 18:26:18,556: t15.2023.10.13 val PER: 1.0000
2026-01-10 18:26:18,559: t15.2023.10.15 val PER: 1.0000
2026-01-10 18:26:18,561: t15.2023.10.20 val PER: 1.0000
2026-01-10 18:26:18,563: t15.2023.10.22 val PER: 1.0000
2026-01-10 18:26:18,564: t15.2023.11.03 val PER: 1.0000
2026-01-10 18:26:18,566: t15.2023.11.04 val PER: 1.0000
2026-01-10 18:26:18,567: t15.2023.11.17 val PER: 0.9984
2026-01-10 18:26:18,569: t15.2023.11.19 val PER: 1.0000
2026-01-10 18:26:18,570: t15.2023.11.26 val PER: 1.0000
2026-01-10 18:26:18,572: t15.2023.12.03 val PER: 1.0000
2026-01-10 18:26:18,573: t15.2023.12.08 val PER: 1.0000
2026-01-10 18:26:18,574: t15.2023.12.10 val PER: 1.0000
2026-01-10 18:26:18,576: t15.2023.12.17 val PER: 1.0000
2026-01-10 18:26:18,577: t15.2023.12.29 val PER: 1.0000
2026-01-10 18:26:18,578: t15.2024.02.25 val PER: 0.9986
2026-01-10 18:26:18,580: t15.2024.03.08 val PER: 1.0000
2026-01-10 18:26:18,582: t15.2024.03.15 val PER: 1.0000
2026-01-10 18:26:18,583: t15.2024.03.17 val PER: 1.0000
2026-01-10 18:26:18,585: t15.2024.05.10 val PER: 1.0000
2026-01-10 18:26:18,587: t15.2024.06.14 val PER: 1.0000
2026-01-10 18:26:18,588: t15.2024.07.19 val PER: 1.0000
2026-01-10 18:26:18,589: t15.2024.07.21 val PER: 1.0000
2026-01-10 18:26:18,591: t15.2024.07.28 val PER: 1.0000
2026-01-10 18:26:18,592: t15.2025.01.10 val PER: 1.0000
2026-01-10 18:26:18,594: t15.2025.01.12 val PER: 1.0000
2026-01-10 18:26:18,595: t15.2025.03.14 val PER: 1.0000
2026-01-10 18:26:18,598: t15.2025.03.16 val PER: 1.0000
2026-01-10 18:26:18,599: t15.2025.03.30 val PER: 1.0000
2026-01-10 18:26:18,601: t15.2025.04.13 val PER: 1.0000
2026-01-10 18:26:18,735: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_1000
2026-01-10 18:26:35,959: Train batch 1200: loss: 139.62 grad norm: 70.29 time: 0.073
2026-01-10 18:26:52,734: Train batch 1400: loss: 123.24 grad norm: 28.81 time: 0.069
2026-01-10 18:27:01,109: Running test after training batch: 1500
2026-01-10 18:27:01,216: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:27:07,218: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-10 18:27:07,245: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-10 18:27:12,355: Val batch 1500: PER (avg): 0.7726 CTC Loss (avg): 136.5320 WER(5gram): 100.00% (n=256) time: 11.244
2026-01-10 18:27:12,357: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=0
2026-01-10 18:27:12,360: t15.2023.08.13 val PER: 0.7630
2026-01-10 18:27:12,362: t15.2023.08.18 val PER: 0.7561
2026-01-10 18:27:12,363: t15.2023.08.20 val PER: 0.7609
2026-01-10 18:27:12,365: t15.2023.08.25 val PER: 0.7530
2026-01-10 18:27:12,366: t15.2023.08.27 val PER: 0.7814
2026-01-10 18:27:12,368: t15.2023.09.01 val PER: 0.7549
2026-01-10 18:27:12,369: t15.2023.09.03 val PER: 0.7779
2026-01-10 18:27:12,371: t15.2023.09.24 val PER: 0.7585
2026-01-10 18:27:12,373: t15.2023.09.29 val PER: 0.7696
2026-01-10 18:27:12,374: t15.2023.10.01 val PER: 0.7867
2026-01-10 18:27:12,376: t15.2023.10.06 val PER: 0.7578
2026-01-10 18:27:12,378: t15.2023.10.08 val PER: 0.7984
2026-01-10 18:27:12,380: t15.2023.10.13 val PER: 0.8099
2026-01-10 18:27:12,382: t15.2023.10.15 val PER: 0.7805
2026-01-10 18:27:12,383: t15.2023.10.20 val PER: 0.7584
2026-01-10 18:27:12,385: t15.2023.10.22 val PER: 0.7595
2026-01-10 18:27:12,386: t15.2023.11.03 val PER: 0.7693
2026-01-10 18:27:12,388: t15.2023.11.04 val PER: 0.7099
2026-01-10 18:27:12,389: t15.2023.11.17 val PER: 0.7247
2026-01-10 18:27:12,390: t15.2023.11.19 val PER: 0.7305
2026-01-10 18:27:12,392: t15.2023.11.26 val PER: 0.7942
2026-01-10 18:27:12,393: t15.2023.12.03 val PER: 0.7773
2026-01-10 18:27:12,395: t15.2023.12.08 val PER: 0.7716
2026-01-10 18:27:12,396: t15.2023.12.10 val PER: 0.7753
2026-01-10 18:27:12,398: t15.2023.12.17 val PER: 0.7796
2026-01-10 18:27:12,399: t15.2023.12.29 val PER: 0.7680
2026-01-10 18:27:12,400: t15.2024.02.25 val PER: 0.7654
2026-01-10 18:27:12,402: t15.2024.03.08 val PER: 0.7809
2026-01-10 18:27:12,403: t15.2024.03.15 val PER: 0.7799
2026-01-10 18:27:12,405: t15.2024.03.17 val PER: 0.7608
2026-01-10 18:27:12,406: t15.2024.05.10 val PER: 0.7608
2026-01-10 18:27:12,407: t15.2024.06.14 val PER: 0.7476
2026-01-10 18:27:12,409: t15.2024.07.19 val PER: 0.7976
2026-01-10 18:27:12,410: t15.2024.07.21 val PER: 0.7524
2026-01-10 18:27:12,412: t15.2024.07.28 val PER: 0.7676
2026-01-10 18:27:12,413: t15.2025.01.10 val PER: 0.8127
2026-01-10 18:27:12,414: t15.2025.01.12 val PER: 0.7644
2026-01-10 18:27:12,416: t15.2025.03.14 val PER: 0.8121
2026-01-10 18:27:12,417: t15.2025.03.16 val PER: 0.7723
2026-01-10 18:27:12,419: t15.2025.03.30 val PER: 0.8161
2026-01-10 18:27:12,420: t15.2025.04.13 val PER: 0.7703
2026-01-10 18:27:12,555: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_1500
2026-01-10 18:27:20,759: Train batch 1600: loss: 126.95 grad norm: 32.47 time: 0.069
2026-01-10 18:27:37,759: Train batch 1800: loss: 119.76 grad norm: 101.59 time: 0.093
2026-01-10 18:27:55,021: Train batch 2000: loss: 114.81 grad norm: 41.39 time: 0.072
2026-01-10 18:27:55,023: Running test after training batch: 2000
2026-01-10 18:27:55,135: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:28:01,025: WER debug example
  GT : you can see the code at this point as well
  PR : a
2026-01-10 18:28:01,097: WER debug example
  GT : how does it keep the cost down
  PR : a
2026-01-10 18:28:15,939: Val batch 2000: PER (avg): 0.7217 CTC Loss (avg): 121.2337 WER(5gram): 99.41% (n=256) time: 20.913
2026-01-10 18:28:15,942: WER lens: avg_true_words=5.99 avg_pred_words=0.06 max_pred_words=1
2026-01-10 18:28:15,945: t15.2023.08.13 val PER: 0.7141
2026-01-10 18:28:15,947: t15.2023.08.18 val PER: 0.7041
2026-01-10 18:28:15,948: t15.2023.08.20 val PER: 0.6982
2026-01-10 18:28:15,950: t15.2023.08.25 val PER: 0.6913
2026-01-10 18:28:15,952: t15.2023.08.27 val PER: 0.7540
2026-01-10 18:28:15,953: t15.2023.09.01 val PER: 0.7005
2026-01-10 18:28:15,955: t15.2023.09.03 val PER: 0.7138
2026-01-10 18:28:15,957: t15.2023.09.24 val PER: 0.7136
2026-01-10 18:28:15,959: t15.2023.09.29 val PER: 0.7179
2026-01-10 18:28:15,960: t15.2023.10.01 val PER: 0.7358
2026-01-10 18:28:15,962: t15.2023.10.06 val PER: 0.7061
2026-01-10 18:28:15,963: t15.2023.10.08 val PER: 0.7470
2026-01-10 18:28:15,965: t15.2023.10.13 val PER: 0.7564
2026-01-10 18:28:15,967: t15.2023.10.15 val PER: 0.7126
2026-01-10 18:28:15,968: t15.2023.10.20 val PER: 0.7148
2026-01-10 18:28:15,970: t15.2023.10.22 val PER: 0.7082
2026-01-10 18:28:15,972: t15.2023.11.03 val PER: 0.7144
2026-01-10 18:28:15,973: t15.2023.11.04 val PER: 0.6587
2026-01-10 18:28:15,975: t15.2023.11.17 val PER: 0.6703
2026-01-10 18:28:15,976: t15.2023.11.19 val PER: 0.6707
2026-01-10 18:28:15,978: t15.2023.11.26 val PER: 0.7507
2026-01-10 18:28:15,980: t15.2023.12.03 val PER: 0.7321
2026-01-10 18:28:15,981: t15.2023.12.08 val PER: 0.7264
2026-01-10 18:28:15,982: t15.2023.12.10 val PER: 0.7109
2026-01-10 18:28:15,984: t15.2023.12.17 val PER: 0.7225
2026-01-10 18:28:15,986: t15.2023.12.29 val PER: 0.7227
2026-01-10 18:28:15,987: t15.2024.02.25 val PER: 0.7205
2026-01-10 18:28:15,989: t15.2024.03.08 val PER: 0.7212
2026-01-10 18:28:15,991: t15.2024.03.15 val PER: 0.7286
2026-01-10 18:28:15,992: t15.2024.03.17 val PER: 0.7120
2026-01-10 18:28:15,994: t15.2024.05.10 val PER: 0.7117
2026-01-10 18:28:15,995: t15.2024.06.14 val PER: 0.7050
2026-01-10 18:28:15,997: t15.2024.07.19 val PER: 0.7429
2026-01-10 18:28:15,998: t15.2024.07.21 val PER: 0.7090
2026-01-10 18:28:16,000: t15.2024.07.28 val PER: 0.7243
2026-01-10 18:28:16,001: t15.2025.01.10 val PER: 0.7617
2026-01-10 18:28:16,003: t15.2025.01.12 val PER: 0.7182
2026-01-10 18:28:16,004: t15.2025.03.14 val PER: 0.7663
2026-01-10 18:28:16,006: t15.2025.03.16 val PER: 0.7369
2026-01-10 18:28:16,007: t15.2025.03.30 val PER: 0.7586
2026-01-10 18:28:16,009: t15.2025.04.13 val PER: 0.7261
2026-01-10 18:28:16,010: New best val WER(5gram) 100.00% --> 99.41%
2026-01-10 18:28:17,127: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_2000
2026-01-10 18:28:33,631: Train batch 2200: loss: 106.80 grad norm: 42.52 time: 0.065
2026-01-10 18:28:50,355: Train batch 2400: loss: 99.61 grad norm: 41.83 time: 0.057
2026-01-10 18:28:58,905: Running test after training batch: 2500
2026-01-10 18:28:59,035: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:29:04,956: WER debug example
  GT : you can see the code at this point as well
  PR : a
2026-01-10 18:29:05,127: WER debug example
  GT : how does it keep the cost down
  PR : it's a
2026-01-10 18:29:48,388: Val batch 2500: PER (avg): 0.5828 CTC Loss (avg): 108.5172 WER(5gram): 97.98% (n=256) time: 49.481
2026-01-10 18:29:48,391: WER lens: avg_true_words=5.99 avg_pred_words=1.17 max_pred_words=7
2026-01-10 18:29:48,393: t15.2023.08.13 val PER: 0.5665
2026-01-10 18:29:48,395: t15.2023.08.18 val PER: 0.5507
2026-01-10 18:29:48,397: t15.2023.08.20 val PER: 0.5552
2026-01-10 18:29:48,399: t15.2023.08.25 val PER: 0.5587
2026-01-10 18:29:48,401: t15.2023.08.27 val PER: 0.6383
2026-01-10 18:29:48,403: t15.2023.09.01 val PER: 0.5252
2026-01-10 18:29:48,405: t15.2023.09.03 val PER: 0.5606
2026-01-10 18:29:48,406: t15.2023.09.24 val PER: 0.5595
2026-01-10 18:29:48,408: t15.2023.09.29 val PER: 0.5718
2026-01-10 18:29:48,410: t15.2023.10.01 val PER: 0.5978
2026-01-10 18:29:48,411: t15.2023.10.06 val PER: 0.5716
2026-01-10 18:29:48,413: t15.2023.10.08 val PER: 0.6008
2026-01-10 18:29:48,414: t15.2023.10.13 val PER: 0.6431
2026-01-10 18:29:48,416: t15.2023.10.15 val PER: 0.5742
2026-01-10 18:29:48,418: t15.2023.10.20 val PER: 0.5906
2026-01-10 18:29:48,419: t15.2023.10.22 val PER: 0.5679
2026-01-10 18:29:48,421: t15.2023.11.03 val PER: 0.5719
2026-01-10 18:29:48,422: t15.2023.11.04 val PER: 0.4812
2026-01-10 18:29:48,424: t15.2023.11.17 val PER: 0.5163
2026-01-10 18:29:48,425: t15.2023.11.19 val PER: 0.4950
2026-01-10 18:29:48,427: t15.2023.11.26 val PER: 0.6333
2026-01-10 18:29:48,428: t15.2023.12.03 val PER: 0.5809
2026-01-10 18:29:48,430: t15.2023.12.08 val PER: 0.5812
2026-01-10 18:29:48,431: t15.2023.12.10 val PER: 0.5808
2026-01-10 18:29:48,432: t15.2023.12.17 val PER: 0.5863
2026-01-10 18:29:48,434: t15.2023.12.29 val PER: 0.5923
2026-01-10 18:29:48,435: t15.2024.02.25 val PER: 0.5829
2026-01-10 18:29:48,437: t15.2024.03.08 val PER: 0.6060
2026-01-10 18:29:48,438: t15.2024.03.15 val PER: 0.5785
2026-01-10 18:29:48,440: t15.2024.03.17 val PER: 0.5823
2026-01-10 18:29:48,441: t15.2024.05.10 val PER: 0.5706
2026-01-10 18:29:48,443: t15.2024.06.14 val PER: 0.5505
2026-01-10 18:29:48,444: t15.2024.07.19 val PER: 0.6190
2026-01-10 18:29:48,446: t15.2024.07.21 val PER: 0.5503
2026-01-10 18:29:48,447: t15.2024.07.28 val PER: 0.5684
2026-01-10 18:29:48,449: t15.2025.01.10 val PER: 0.6515
2026-01-10 18:29:48,451: t15.2025.01.12 val PER: 0.5789
2026-01-10 18:29:48,452: t15.2025.03.14 val PER: 0.6464
2026-01-10 18:29:48,454: t15.2025.03.16 val PER: 0.6348
2026-01-10 18:29:48,455: t15.2025.03.30 val PER: 0.6494
2026-01-10 18:29:48,457: t15.2025.04.13 val PER: 0.6049
2026-01-10 18:29:48,458: New best val WER(5gram) 99.41% --> 97.98%
2026-01-10 18:29:49,584: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_2500
2026-01-10 18:29:58,122: Train batch 2600: loss: 103.90 grad norm: 59.97 time: 0.070
2026-01-10 18:30:14,751: Train batch 2800: loss: 88.34 grad norm: 50.41 time: 0.087
2026-01-10 18:30:31,699: Train batch 3000: loss: 94.61 grad norm: 79.50 time: 0.086
2026-01-10 18:30:31,701: Running test after training batch: 3000
2026-01-10 18:30:31,802: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:30:37,756: WER debug example
  GT : you can see the code at this point as well
  PR : in a
2026-01-10 18:30:37,930: WER debug example
  GT : how does it keep the cost down
  PR : it is a
2026-01-10 18:31:23,923: Val batch 3000: PER (avg): 0.5396 CTC Loss (avg): 100.4424 WER(5gram): 97.26% (n=256) time: 52.219
2026-01-10 18:31:23,925: WER lens: avg_true_words=5.99 avg_pred_words=2.31 max_pred_words=10
2026-01-10 18:31:23,929: t15.2023.08.13 val PER: 0.5249
2026-01-10 18:31:23,930: t15.2023.08.18 val PER: 0.5105
2026-01-10 18:31:23,932: t15.2023.08.20 val PER: 0.4988
2026-01-10 18:31:23,934: t15.2023.08.25 val PER: 0.5015
2026-01-10 18:31:23,936: t15.2023.08.27 val PER: 0.5965
2026-01-10 18:31:23,938: t15.2023.09.01 val PER: 0.4886
2026-01-10 18:31:23,940: t15.2023.09.03 val PER: 0.5392
2026-01-10 18:31:23,941: t15.2023.09.24 val PER: 0.5255
2026-01-10 18:31:23,943: t15.2023.09.29 val PER: 0.5258
2026-01-10 18:31:23,945: t15.2023.10.01 val PER: 0.5667
2026-01-10 18:31:23,946: t15.2023.10.06 val PER: 0.5027
2026-01-10 18:31:23,948: t15.2023.10.08 val PER: 0.5792
2026-01-10 18:31:23,949: t15.2023.10.13 val PER: 0.6144
2026-01-10 18:31:23,951: t15.2023.10.15 val PER: 0.5339
2026-01-10 18:31:23,953: t15.2023.10.20 val PER: 0.5369
2026-01-10 18:31:23,954: t15.2023.10.22 val PER: 0.5234
2026-01-10 18:31:23,956: t15.2023.11.03 val PER: 0.5088
2026-01-10 18:31:23,957: t15.2023.11.04 val PER: 0.3925
2026-01-10 18:31:23,959: t15.2023.11.17 val PER: 0.4572
2026-01-10 18:31:23,961: t15.2023.11.19 val PER: 0.4411
2026-01-10 18:31:23,962: t15.2023.11.26 val PER: 0.5877
2026-01-10 18:31:23,964: t15.2023.12.03 val PER: 0.5357
2026-01-10 18:31:23,965: t15.2023.12.08 val PER: 0.5253
2026-01-10 18:31:23,967: t15.2023.12.10 val PER: 0.5466
2026-01-10 18:31:23,969: t15.2023.12.17 val PER: 0.5281
2026-01-10 18:31:23,971: t15.2023.12.29 val PER: 0.5566
2026-01-10 18:31:23,972: t15.2024.02.25 val PER: 0.5112
2026-01-10 18:31:23,974: t15.2024.03.08 val PER: 0.5548
2026-01-10 18:31:23,975: t15.2024.03.15 val PER: 0.5460
2026-01-10 18:31:23,977: t15.2024.03.17 val PER: 0.5474
2026-01-10 18:31:23,978: t15.2024.05.10 val PER: 0.5275
2026-01-10 18:31:23,980: t15.2024.06.14 val PER: 0.5079
2026-01-10 18:31:23,981: t15.2024.07.19 val PER: 0.5979
2026-01-10 18:31:23,983: t15.2024.07.21 val PER: 0.4979
2026-01-10 18:31:23,984: t15.2024.07.28 val PER: 0.5316
2026-01-10 18:31:23,986: t15.2025.01.10 val PER: 0.5978
2026-01-10 18:31:23,987: t15.2025.01.12 val PER: 0.5343
2026-01-10 18:31:23,989: t15.2025.03.14 val PER: 0.6095
2026-01-10 18:31:23,990: t15.2025.03.16 val PER: 0.5969
2026-01-10 18:31:23,992: t15.2025.03.30 val PER: 0.6218
2026-01-10 18:31:23,993: t15.2025.04.13 val PER: 0.5521
2026-01-10 18:31:23,995: New best val WER(5gram) 97.98% --> 97.26%
2026-01-10 18:31:25,100: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_3000
2026-01-10 18:31:41,690: Train batch 3200: loss: 88.58 grad norm: 59.02 time: 0.080
2026-01-10 18:31:58,203: Train batch 3400: loss: 75.95 grad norm: 47.16 time: 0.053
2026-01-10 18:32:06,687: Running test after training batch: 3500
2026-01-10 18:32:06,812: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:32:12,791: WER debug example
  GT : you can see the code at this point as well
  PR : a this is why
2026-01-10 18:32:12,967: WER debug example
  GT : how does it keep the cost down
  PR : it is a
2026-01-10 18:32:57,564: Val batch 3500: PER (avg): 0.5066 CTC Loss (avg): 94.5274 WER(5gram): 96.68% (n=256) time: 50.875
2026-01-10 18:32:57,567: WER lens: avg_true_words=5.99 avg_pred_words=3.15 max_pred_words=11
2026-01-10 18:32:57,570: t15.2023.08.13 val PER: 0.4948
2026-01-10 18:32:57,571: t15.2023.08.18 val PER: 0.4820
2026-01-10 18:32:57,573: t15.2023.08.20 val PER: 0.4567
2026-01-10 18:32:57,575: t15.2023.08.25 val PER: 0.4623
2026-01-10 18:32:57,576: t15.2023.08.27 val PER: 0.5740
2026-01-10 18:32:57,578: t15.2023.09.01 val PER: 0.4505
2026-01-10 18:32:57,579: t15.2023.09.03 val PER: 0.5166
2026-01-10 18:32:57,581: t15.2023.09.24 val PER: 0.4818
2026-01-10 18:32:57,583: t15.2023.09.29 val PER: 0.4773
2026-01-10 18:32:57,585: t15.2023.10.01 val PER: 0.5218
2026-01-10 18:32:57,586: t15.2023.10.06 val PER: 0.4758
2026-01-10 18:32:57,588: t15.2023.10.08 val PER: 0.5629
2026-01-10 18:32:57,589: t15.2023.10.13 val PER: 0.5881
2026-01-10 18:32:57,591: t15.2023.10.15 val PER: 0.4990
2026-01-10 18:32:57,592: t15.2023.10.20 val PER: 0.4966
2026-01-10 18:32:57,594: t15.2023.10.22 val PER: 0.5011
2026-01-10 18:32:57,596: t15.2023.11.03 val PER: 0.4905
2026-01-10 18:32:57,597: t15.2023.11.04 val PER: 0.3174
2026-01-10 18:32:57,598: t15.2023.11.17 val PER: 0.3795
2026-01-10 18:32:57,601: t15.2023.11.19 val PER: 0.3872
2026-01-10 18:32:57,603: t15.2023.11.26 val PER: 0.5746
2026-01-10 18:32:57,604: t15.2023.12.03 val PER: 0.5189
2026-01-10 18:32:57,605: t15.2023.12.08 val PER: 0.5007
2026-01-10 18:32:57,607: t15.2023.12.10 val PER: 0.4954
2026-01-10 18:32:57,608: t15.2023.12.17 val PER: 0.4969
2026-01-10 18:32:57,610: t15.2023.12.29 val PER: 0.5237
2026-01-10 18:32:57,612: t15.2024.02.25 val PER: 0.4663
2026-01-10 18:32:57,614: t15.2024.03.08 val PER: 0.5349
2026-01-10 18:32:57,615: t15.2024.03.15 val PER: 0.5103
2026-01-10 18:32:57,617: t15.2024.03.17 val PER: 0.4972
2026-01-10 18:32:57,618: t15.2024.05.10 val PER: 0.5022
2026-01-10 18:32:57,620: t15.2024.06.14 val PER: 0.4811
2026-01-10 18:32:57,621: t15.2024.07.19 val PER: 0.5669
2026-01-10 18:32:57,623: t15.2024.07.21 val PER: 0.4566
2026-01-10 18:32:57,624: t15.2024.07.28 val PER: 0.4890
2026-01-10 18:32:57,626: t15.2025.01.10 val PER: 0.6061
2026-01-10 18:32:57,627: t15.2025.01.12 val PER: 0.5081
2026-01-10 18:32:57,629: t15.2025.03.14 val PER: 0.5814
2026-01-10 18:32:57,630: t15.2025.03.16 val PER: 0.5550
2026-01-10 18:32:57,631: t15.2025.03.30 val PER: 0.5920
2026-01-10 18:32:57,633: t15.2025.04.13 val PER: 0.5335
2026-01-10 18:32:57,635: New best val WER(5gram) 97.26% --> 96.68%
2026-01-10 18:32:58,756: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_3500
2026-01-10 18:33:07,102: Train batch 3600: loss: 81.53 grad norm: 56.84 time: 0.072
2026-01-10 18:33:23,798: Train batch 3800: loss: 90.51 grad norm: 59.09 time: 0.073
2026-01-10 18:33:40,735: Train batch 4000: loss: 70.61 grad norm: 57.38 time: 0.061
2026-01-10 18:33:40,737: Running test after training batch: 4000
2026-01-10 18:33:40,876: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:33:46,903: WER debug example
  GT : you can see the code at this point as well
  PR : it is a need to see it as a
2026-01-10 18:33:47,106: WER debug example
  GT : how does it keep the cost down
  PR : t t t t t a r s t
2026-01-10 18:34:31,070: Val batch 4000: PER (avg): 0.4787 CTC Loss (avg): 89.1947 WER(5gram): 98.37% (n=256) time: 50.330
2026-01-10 18:34:31,072: WER lens: avg_true_words=5.99 avg_pred_words=4.39 max_pred_words=12
2026-01-10 18:34:31,075: t15.2023.08.13 val PER: 0.4563
2026-01-10 18:34:31,077: t15.2023.08.18 val PER: 0.4468
2026-01-10 18:34:31,079: t15.2023.08.20 val PER: 0.4313
2026-01-10 18:34:31,081: t15.2023.08.25 val PER: 0.4202
2026-01-10 18:34:31,082: t15.2023.08.27 val PER: 0.5305
2026-01-10 18:34:31,084: t15.2023.09.01 val PER: 0.4261
2026-01-10 18:34:31,086: t15.2023.09.03 val PER: 0.4751
2026-01-10 18:34:31,087: t15.2023.09.24 val PER: 0.4405
2026-01-10 18:34:31,089: t15.2023.09.29 val PER: 0.4493
2026-01-10 18:34:31,090: t15.2023.10.01 val PER: 0.4980
2026-01-10 18:34:31,092: t15.2023.10.06 val PER: 0.4360
2026-01-10 18:34:31,093: t15.2023.10.08 val PER: 0.5386
2026-01-10 18:34:31,095: t15.2023.10.13 val PER: 0.5679
2026-01-10 18:34:31,097: t15.2023.10.15 val PER: 0.4858
2026-01-10 18:34:31,098: t15.2023.10.20 val PER: 0.4899
2026-01-10 18:34:31,100: t15.2023.10.22 val PER: 0.4655
2026-01-10 18:34:31,102: t15.2023.11.03 val PER: 0.4607
2026-01-10 18:34:31,103: t15.2023.11.04 val PER: 0.2799
2026-01-10 18:34:31,105: t15.2023.11.17 val PER: 0.3546
2026-01-10 18:34:31,106: t15.2023.11.19 val PER: 0.3613
2026-01-10 18:34:31,108: t15.2023.11.26 val PER: 0.5304
2026-01-10 18:34:31,110: t15.2023.12.03 val PER: 0.4737
2026-01-10 18:34:31,111: t15.2023.12.08 val PER: 0.4734
2026-01-10 18:34:31,113: t15.2023.12.10 val PER: 0.4691
2026-01-10 18:34:31,115: t15.2023.12.17 val PER: 0.4813
2026-01-10 18:34:31,117: t15.2023.12.29 val PER: 0.5079
2026-01-10 18:34:31,118: t15.2024.02.25 val PER: 0.4396
2026-01-10 18:34:31,120: t15.2024.03.08 val PER: 0.5050
2026-01-10 18:34:31,121: t15.2024.03.15 val PER: 0.4809
2026-01-10 18:34:31,123: t15.2024.03.17 val PER: 0.4798
2026-01-10 18:34:31,124: t15.2024.05.10 val PER: 0.4844
2026-01-10 18:34:31,126: t15.2024.06.14 val PER: 0.4432
2026-01-10 18:34:31,127: t15.2024.07.19 val PER: 0.5465
2026-01-10 18:34:31,129: t15.2024.07.21 val PER: 0.4207
2026-01-10 18:34:31,130: t15.2024.07.28 val PER: 0.4713
2026-01-10 18:34:31,131: t15.2025.01.10 val PER: 0.5744
2026-01-10 18:34:31,134: t15.2025.01.12 val PER: 0.4935
2026-01-10 18:34:31,136: t15.2025.03.14 val PER: 0.5710
2026-01-10 18:34:31,137: t15.2025.03.16 val PER: 0.5065
2026-01-10 18:34:31,138: t15.2025.03.30 val PER: 0.5540
2026-01-10 18:34:31,140: t15.2025.04.13 val PER: 0.5136
2026-01-10 18:34:31,271: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_4000
2026-01-10 18:34:48,328: Train batch 4200: loss: 73.91 grad norm: 71.60 time: 0.082
2026-01-10 18:35:05,511: Train batch 4400: loss: 66.43 grad norm: 57.71 time: 0.070
2026-01-10 18:35:13,845: Running test after training batch: 4500
2026-01-10 18:35:13,950: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:35:19,929: WER debug example
  GT : you can see the code at this point as well
  PR : a this is why
2026-01-10 18:35:20,097: WER debug example
  GT : how does it keep the cost down
  PR : it is a way that
2026-01-10 18:36:00,321: Val batch 4500: PER (avg): 0.4628 CTC Loss (avg): 84.7481 WER(5gram): 95.37% (n=256) time: 46.473
2026-01-10 18:36:00,323: WER lens: avg_true_words=5.99 avg_pred_words=4.26 max_pred_words=14
2026-01-10 18:36:00,326: t15.2023.08.13 val PER: 0.4449
2026-01-10 18:36:00,327: t15.2023.08.18 val PER: 0.4434
2026-01-10 18:36:00,329: t15.2023.08.20 val PER: 0.4067
2026-01-10 18:36:00,330: t15.2023.08.25 val PER: 0.4066
2026-01-10 18:36:00,332: t15.2023.08.27 val PER: 0.5129
2026-01-10 18:36:00,333: t15.2023.09.01 val PER: 0.4067
2026-01-10 18:36:00,335: t15.2023.09.03 val PER: 0.4489
2026-01-10 18:36:00,336: t15.2023.09.24 val PER: 0.4187
2026-01-10 18:36:00,338: t15.2023.09.29 val PER: 0.4308
2026-01-10 18:36:00,340: t15.2023.10.01 val PER: 0.4835
2026-01-10 18:36:00,341: t15.2023.10.06 val PER: 0.4101
2026-01-10 18:36:00,343: t15.2023.10.08 val PER: 0.5237
2026-01-10 18:36:00,345: t15.2023.10.13 val PER: 0.5601
2026-01-10 18:36:00,346: t15.2023.10.15 val PER: 0.4746
2026-01-10 18:36:00,348: t15.2023.10.20 val PER: 0.4832
2026-01-10 18:36:00,350: t15.2023.10.22 val PER: 0.4477
2026-01-10 18:36:00,351: t15.2023.11.03 val PER: 0.4437
2026-01-10 18:36:00,353: t15.2023.11.04 val PER: 0.2150
2026-01-10 18:36:00,356: t15.2023.11.17 val PER: 0.3266
2026-01-10 18:36:00,358: t15.2023.11.19 val PER: 0.3234
2026-01-10 18:36:00,360: t15.2023.11.26 val PER: 0.5261
2026-01-10 18:36:00,361: t15.2023.12.03 val PER: 0.4569
2026-01-10 18:36:00,363: t15.2023.12.08 val PER: 0.4660
2026-01-10 18:36:00,365: t15.2023.12.10 val PER: 0.4415
2026-01-10 18:36:00,366: t15.2023.12.17 val PER: 0.4543
2026-01-10 18:36:00,368: t15.2023.12.29 val PER: 0.4852
2026-01-10 18:36:00,369: t15.2024.02.25 val PER: 0.4298
2026-01-10 18:36:00,371: t15.2024.03.08 val PER: 0.4964
2026-01-10 18:36:00,372: t15.2024.03.15 val PER: 0.4728
2026-01-10 18:36:00,374: t15.2024.03.17 val PER: 0.4589
2026-01-10 18:36:00,375: t15.2024.05.10 val PER: 0.4621
2026-01-10 18:36:00,376: t15.2024.06.14 val PER: 0.4274
2026-01-10 18:36:00,378: t15.2024.07.19 val PER: 0.5300
2026-01-10 18:36:00,379: t15.2024.07.21 val PER: 0.3986
2026-01-10 18:36:00,381: t15.2024.07.28 val PER: 0.4632
2026-01-10 18:36:00,382: t15.2025.01.10 val PER: 0.5744
2026-01-10 18:36:00,383: t15.2025.01.12 val PER: 0.4773
2026-01-10 18:36:00,385: t15.2025.03.14 val PER: 0.5399
2026-01-10 18:36:00,386: t15.2025.03.16 val PER: 0.5170
2026-01-10 18:36:00,388: t15.2025.03.30 val PER: 0.5517
2026-01-10 18:36:00,389: t15.2025.04.13 val PER: 0.4864
2026-01-10 18:36:00,391: New best val WER(5gram) 96.68% --> 95.37%
2026-01-10 18:36:01,510: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_4500
2026-01-10 18:36:09,855: Train batch 4600: loss: 75.39 grad norm: 67.48 time: 0.068
2026-01-10 18:36:26,671: Train batch 4800: loss: 61.12 grad norm: 56.86 time: 0.068
2026-01-10 18:36:43,322: Train batch 5000: loss: 95.74 grad norm: 91.63 time: 0.070
2026-01-10 18:36:43,324: Running test after training batch: 5000
2026-01-10 18:36:43,440: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:36:49,412: WER debug example
  GT : you can see the code at this point as well
  PR : it is a go at it this way
2026-01-10 18:36:49,564: WER debug example
  GT : how does it keep the cost down
  PR : is it a go at the
2026-01-10 18:37:25,442: Val batch 5000: PER (avg): 0.4483 CTC Loss (avg): 80.6847 WER(5gram): 95.63% (n=256) time: 42.115
2026-01-10 18:37:25,444: WER lens: avg_true_words=5.99 avg_pred_words=4.31 max_pred_words=14
2026-01-10 18:37:25,447: t15.2023.08.13 val PER: 0.4324
2026-01-10 18:37:25,449: t15.2023.08.18 val PER: 0.4275
2026-01-10 18:37:25,451: t15.2023.08.20 val PER: 0.3948
2026-01-10 18:37:25,453: t15.2023.08.25 val PER: 0.3810
2026-01-10 18:37:25,454: t15.2023.08.27 val PER: 0.4823
2026-01-10 18:37:25,456: t15.2023.09.01 val PER: 0.3985
2026-01-10 18:37:25,458: t15.2023.09.03 val PER: 0.4454
2026-01-10 18:37:25,459: t15.2023.09.24 val PER: 0.4199
2026-01-10 18:37:25,461: t15.2023.09.29 val PER: 0.4180
2026-01-10 18:37:25,462: t15.2023.10.01 val PER: 0.4815
2026-01-10 18:37:25,464: t15.2023.10.06 val PER: 0.3994
2026-01-10 18:37:25,466: t15.2023.10.08 val PER: 0.5074
2026-01-10 18:37:25,467: t15.2023.10.13 val PER: 0.5392
2026-01-10 18:37:25,469: t15.2023.10.15 val PER: 0.4548
2026-01-10 18:37:25,470: t15.2023.10.20 val PER: 0.4664
2026-01-10 18:37:25,472: t15.2023.10.22 val PER: 0.4388
2026-01-10 18:37:25,474: t15.2023.11.03 val PER: 0.4294
2026-01-10 18:37:25,475: t15.2023.11.04 val PER: 0.2321
2026-01-10 18:37:25,477: t15.2023.11.17 val PER: 0.3250
2026-01-10 18:37:25,478: t15.2023.11.19 val PER: 0.3034
2026-01-10 18:37:25,480: t15.2023.11.26 val PER: 0.5065
2026-01-10 18:37:25,481: t15.2023.12.03 val PER: 0.4391
2026-01-10 18:37:25,483: t15.2023.12.08 val PER: 0.4481
2026-01-10 18:37:25,485: t15.2023.12.10 val PER: 0.4258
2026-01-10 18:37:25,486: t15.2023.12.17 val PER: 0.4231
2026-01-10 18:37:25,488: t15.2023.12.29 val PER: 0.4715
2026-01-10 18:37:25,490: t15.2024.02.25 val PER: 0.4087
2026-01-10 18:37:25,491: t15.2024.03.08 val PER: 0.4723
2026-01-10 18:37:25,493: t15.2024.03.15 val PER: 0.4528
2026-01-10 18:37:25,494: t15.2024.03.17 val PER: 0.4463
2026-01-10 18:37:25,497: t15.2024.05.10 val PER: 0.4681
2026-01-10 18:37:25,499: t15.2024.06.14 val PER: 0.4211
2026-01-10 18:37:25,500: t15.2024.07.19 val PER: 0.5109
2026-01-10 18:37:25,502: t15.2024.07.21 val PER: 0.3938
2026-01-10 18:37:25,503: t15.2024.07.28 val PER: 0.4507
2026-01-10 18:37:25,505: t15.2025.01.10 val PER: 0.5413
2026-01-10 18:37:25,506: t15.2025.01.12 val PER: 0.4465
2026-01-10 18:37:25,508: t15.2025.03.14 val PER: 0.5281
2026-01-10 18:37:25,510: t15.2025.03.16 val PER: 0.4961
2026-01-10 18:37:25,511: t15.2025.03.30 val PER: 0.5414
2026-01-10 18:37:25,513: t15.2025.04.13 val PER: 0.4907
2026-01-10 18:37:25,647: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_5000
2026-01-10 18:37:42,411: Train batch 5200: loss: 67.03 grad norm: 65.20 time: 0.059
2026-01-10 18:37:59,007: Train batch 5400: loss: 73.42 grad norm: 62.21 time: 0.073
2026-01-10 18:38:07,309: Running test after training batch: 5500
2026-01-10 18:38:07,446: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:38:13,417: WER debug example
  GT : you can see the code at this point as well
  PR : it is a go at it this is why
2026-01-10 18:38:13,568: WER debug example
  GT : how does it keep the cost down
  PR : so it is a guy at the
2026-01-10 18:38:49,586: Val batch 5500: PER (avg): 0.4329 CTC Loss (avg): 77.2363 WER(5gram): 94.13% (n=256) time: 42.275
2026-01-10 18:38:49,588: WER lens: avg_true_words=5.99 avg_pred_words=4.70 max_pred_words=14
2026-01-10 18:38:49,591: t15.2023.08.13 val PER: 0.4168
2026-01-10 18:38:49,593: t15.2023.08.18 val PER: 0.4158
2026-01-10 18:38:49,595: t15.2023.08.20 val PER: 0.3852
2026-01-10 18:38:49,597: t15.2023.08.25 val PER: 0.3630
2026-01-10 18:38:49,598: t15.2023.08.27 val PER: 0.4743
2026-01-10 18:38:49,600: t15.2023.09.01 val PER: 0.3847
2026-01-10 18:38:49,602: t15.2023.09.03 val PER: 0.4347
2026-01-10 18:38:49,603: t15.2023.09.24 val PER: 0.3896
2026-01-10 18:38:49,605: t15.2023.09.29 val PER: 0.4052
2026-01-10 18:38:49,606: t15.2023.10.01 val PER: 0.4716
2026-01-10 18:38:49,608: t15.2023.10.06 val PER: 0.3864
2026-01-10 18:38:49,609: t15.2023.10.08 val PER: 0.5007
2026-01-10 18:38:49,611: t15.2023.10.13 val PER: 0.5353
2026-01-10 18:38:49,612: t15.2023.10.15 val PER: 0.4423
2026-01-10 18:38:49,614: t15.2023.10.20 val PER: 0.4564
2026-01-10 18:38:49,615: t15.2023.10.22 val PER: 0.4087
2026-01-10 18:38:49,617: t15.2023.11.03 val PER: 0.4166
2026-01-10 18:38:49,618: t15.2023.11.04 val PER: 0.1911
2026-01-10 18:38:49,620: t15.2023.11.17 val PER: 0.2815
2026-01-10 18:38:49,621: t15.2023.11.19 val PER: 0.2715
2026-01-10 18:38:49,623: t15.2023.11.26 val PER: 0.4920
2026-01-10 18:38:49,624: t15.2023.12.03 val PER: 0.4286
2026-01-10 18:38:49,626: t15.2023.12.08 val PER: 0.4288
2026-01-10 18:38:49,627: t15.2023.12.10 val PER: 0.4034
2026-01-10 18:38:49,628: t15.2023.12.17 val PER: 0.4158
2026-01-10 18:38:49,630: t15.2023.12.29 val PER: 0.4578
2026-01-10 18:38:49,631: t15.2024.02.25 val PER: 0.3947
2026-01-10 18:38:49,633: t15.2024.03.08 val PER: 0.4495
2026-01-10 18:38:49,634: t15.2024.03.15 val PER: 0.4447
2026-01-10 18:38:49,636: t15.2024.03.17 val PER: 0.4198
2026-01-10 18:38:49,637: t15.2024.05.10 val PER: 0.4487
2026-01-10 18:38:49,639: t15.2024.06.14 val PER: 0.3991
2026-01-10 18:38:49,640: t15.2024.07.19 val PER: 0.4918
2026-01-10 18:38:49,642: t15.2024.07.21 val PER: 0.3841
2026-01-10 18:38:49,643: t15.2024.07.28 val PER: 0.4360
2026-01-10 18:38:49,644: t15.2025.01.10 val PER: 0.5427
2026-01-10 18:38:49,646: t15.2025.01.12 val PER: 0.4388
2026-01-10 18:38:49,647: t15.2025.03.14 val PER: 0.5178
2026-01-10 18:38:49,649: t15.2025.03.16 val PER: 0.4699
2026-01-10 18:38:49,650: t15.2025.03.30 val PER: 0.5161
2026-01-10 18:38:49,651: t15.2025.04.13 val PER: 0.4593
2026-01-10 18:38:49,653: New best val WER(5gram) 95.37% --> 94.13%
2026-01-10 18:38:50,769: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_5500
2026-01-10 18:38:59,185: Train batch 5600: loss: 71.39 grad norm: 81.43 time: 0.067
2026-01-10 18:39:16,016: Train batch 5800: loss: 59.16 grad norm: 68.61 time: 0.087
2026-01-10 18:39:32,671: Train batch 6000: loss: 55.18 grad norm: 72.81 time: 0.053
2026-01-10 18:39:32,673: Running test after training batch: 6000
2026-01-10 18:39:32,839: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:39:38,870: WER debug example
  GT : you can see the code at this point as well
  PR : i know it a go at it this is why
2026-01-10 18:39:39,042: WER debug example
  GT : how does it keep the cost down
  PR : is it a go at the
2026-01-10 18:40:14,088: Val batch 6000: PER (avg): 0.4252 CTC Loss (avg): 74.1662 WER(5gram): 92.57% (n=256) time: 41.411
2026-01-10 18:40:14,090: WER lens: avg_true_words=5.99 avg_pred_words=4.39 max_pred_words=14
2026-01-10 18:40:14,093: t15.2023.08.13 val PER: 0.4075
2026-01-10 18:40:14,095: t15.2023.08.18 val PER: 0.4040
2026-01-10 18:40:14,096: t15.2023.08.20 val PER: 0.3678
2026-01-10 18:40:14,098: t15.2023.08.25 val PER: 0.3554
2026-01-10 18:40:14,100: t15.2023.08.27 val PER: 0.4775
2026-01-10 18:40:14,101: t15.2023.09.01 val PER: 0.3718
2026-01-10 18:40:14,103: t15.2023.09.03 val PER: 0.4311
2026-01-10 18:40:14,105: t15.2023.09.24 val PER: 0.3823
2026-01-10 18:40:14,107: t15.2023.09.29 val PER: 0.4071
2026-01-10 18:40:14,108: t15.2023.10.01 val PER: 0.4571
2026-01-10 18:40:14,110: t15.2023.10.06 val PER: 0.3757
2026-01-10 18:40:14,112: t15.2023.10.08 val PER: 0.4953
2026-01-10 18:40:14,114: t15.2023.10.13 val PER: 0.5291
2026-01-10 18:40:14,116: t15.2023.10.15 val PER: 0.4417
2026-01-10 18:40:14,117: t15.2023.10.20 val PER: 0.4530
2026-01-10 18:40:14,119: t15.2023.10.22 val PER: 0.3998
2026-01-10 18:40:14,121: t15.2023.11.03 val PER: 0.4166
2026-01-10 18:40:14,123: t15.2023.11.04 val PER: 0.2116
2026-01-10 18:40:14,124: t15.2023.11.17 val PER: 0.2830
2026-01-10 18:40:14,126: t15.2023.11.19 val PER: 0.2735
2026-01-10 18:40:14,128: t15.2023.11.26 val PER: 0.4920
2026-01-10 18:40:14,130: t15.2023.12.03 val PER: 0.4181
2026-01-10 18:40:14,131: t15.2023.12.08 val PER: 0.4148
2026-01-10 18:40:14,133: t15.2023.12.10 val PER: 0.3929
2026-01-10 18:40:14,136: t15.2023.12.17 val PER: 0.4168
2026-01-10 18:40:14,138: t15.2023.12.29 val PER: 0.4427
2026-01-10 18:40:14,139: t15.2024.02.25 val PER: 0.3792
2026-01-10 18:40:14,141: t15.2024.03.08 val PER: 0.4580
2026-01-10 18:40:14,143: t15.2024.03.15 val PER: 0.4447
2026-01-10 18:40:14,144: t15.2024.03.17 val PER: 0.4059
2026-01-10 18:40:14,146: t15.2024.05.10 val PER: 0.4264
2026-01-10 18:40:14,147: t15.2024.06.14 val PER: 0.3927
2026-01-10 18:40:14,149: t15.2024.07.19 val PER: 0.4779
2026-01-10 18:40:14,150: t15.2024.07.21 val PER: 0.3717
2026-01-10 18:40:14,152: t15.2024.07.28 val PER: 0.4272
2026-01-10 18:40:14,153: t15.2025.01.10 val PER: 0.5014
2026-01-10 18:40:14,155: t15.2025.01.12 val PER: 0.4319
2026-01-10 18:40:14,157: t15.2025.03.14 val PER: 0.5104
2026-01-10 18:40:14,158: t15.2025.03.16 val PER: 0.4686
2026-01-10 18:40:14,160: t15.2025.03.30 val PER: 0.5080
2026-01-10 18:40:14,161: t15.2025.04.13 val PER: 0.4650
2026-01-10 18:40:14,163: New best val WER(5gram) 94.13% --> 92.57%
2026-01-10 18:40:15,304: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_6000
2026-01-10 18:40:32,559: Train batch 6200: loss: 64.01 grad norm: 91.75 time: 0.075
2026-01-10 18:40:49,395: Train batch 6400: loss: 75.11 grad norm: 67.75 time: 0.068
2026-01-10 18:40:57,566: Running test after training batch: 6500
2026-01-10 18:40:57,683: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:41:03,611: WER debug example
  GT : you can see the code at this point as well
  PR : be it a go at it this is why
2026-01-10 18:41:03,756: WER debug example
  GT : how does it keep the cost down
  PR : it is a guy at the
2026-01-10 18:41:37,013: Val batch 6500: PER (avg): 0.4145 CTC Loss (avg): 71.1601 WER(5gram): 92.50% (n=256) time: 39.445
2026-01-10 18:41:37,015: WER lens: avg_true_words=5.99 avg_pred_words=4.70 max_pred_words=13
2026-01-10 18:41:37,018: t15.2023.08.13 val PER: 0.3981
2026-01-10 18:41:37,020: t15.2023.08.18 val PER: 0.3864
2026-01-10 18:41:37,021: t15.2023.08.20 val PER: 0.3511
2026-01-10 18:41:37,023: t15.2023.08.25 val PER: 0.3404
2026-01-10 18:41:37,024: t15.2023.08.27 val PER: 0.4534
2026-01-10 18:41:37,026: t15.2023.09.01 val PER: 0.3580
2026-01-10 18:41:37,027: t15.2023.09.03 val PER: 0.4109
2026-01-10 18:41:37,029: t15.2023.09.24 val PER: 0.3714
2026-01-10 18:41:37,031: t15.2023.09.29 val PER: 0.3835
2026-01-10 18:41:37,032: t15.2023.10.01 val PER: 0.4472
2026-01-10 18:41:37,034: t15.2023.10.06 val PER: 0.3520
2026-01-10 18:41:37,035: t15.2023.10.08 val PER: 0.4912
2026-01-10 18:41:37,037: t15.2023.10.13 val PER: 0.5182
2026-01-10 18:41:37,038: t15.2023.10.15 val PER: 0.4252
2026-01-10 18:41:37,040: t15.2023.10.20 val PER: 0.4295
2026-01-10 18:41:37,041: t15.2023.10.22 val PER: 0.3942
2026-01-10 18:41:37,043: t15.2023.11.03 val PER: 0.4030
2026-01-10 18:41:37,044: t15.2023.11.04 val PER: 0.1775
2026-01-10 18:41:37,046: t15.2023.11.17 val PER: 0.2706
2026-01-10 18:41:37,047: t15.2023.11.19 val PER: 0.2695
2026-01-10 18:41:37,049: t15.2023.11.26 val PER: 0.4717
2026-01-10 18:41:37,050: t15.2023.12.03 val PER: 0.4128
2026-01-10 18:41:37,052: t15.2023.12.08 val PER: 0.4308
2026-01-10 18:41:37,053: t15.2023.12.10 val PER: 0.3798
2026-01-10 18:41:37,054: t15.2023.12.17 val PER: 0.3981
2026-01-10 18:41:37,056: t15.2023.12.29 val PER: 0.4420
2026-01-10 18:41:37,057: t15.2024.02.25 val PER: 0.3596
2026-01-10 18:41:37,059: t15.2024.03.08 val PER: 0.4367
2026-01-10 18:41:37,060: t15.2024.03.15 val PER: 0.4296
2026-01-10 18:41:37,062: t15.2024.03.17 val PER: 0.4073
2026-01-10 18:41:37,063: t15.2024.05.10 val PER: 0.4339
2026-01-10 18:41:37,065: t15.2024.06.14 val PER: 0.3896
2026-01-10 18:41:37,066: t15.2024.07.19 val PER: 0.4647
2026-01-10 18:41:37,067: t15.2024.07.21 val PER: 0.3600
2026-01-10 18:41:37,069: t15.2024.07.28 val PER: 0.4140
2026-01-10 18:41:37,070: t15.2025.01.10 val PER: 0.5096
2026-01-10 18:41:37,072: t15.2025.01.12 val PER: 0.4226
2026-01-10 18:41:37,074: t15.2025.03.14 val PER: 0.5015
2026-01-10 18:41:37,075: t15.2025.03.16 val PER: 0.4581
2026-01-10 18:41:37,077: t15.2025.03.30 val PER: 0.5195
2026-01-10 18:41:37,078: t15.2025.04.13 val PER: 0.4565
2026-01-10 18:41:37,080: New best val WER(5gram) 92.57% --> 92.50%
2026-01-10 18:41:38,208: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_6500
2026-01-10 18:41:46,448: Train batch 6600: loss: 49.28 grad norm: 54.57 time: 0.050
2026-01-10 18:42:03,086: Train batch 6800: loss: 58.02 grad norm: 61.90 time: 0.054
2026-01-10 18:42:19,774: Train batch 7000: loss: 61.03 grad norm: 69.05 time: 0.066
2026-01-10 18:42:19,776: Running test after training batch: 7000
2026-01-10 18:42:19,967: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:42:26,390: WER debug example
  GT : you can see the code at this point as well
  PR : i know it a go at it this is why
2026-01-10 18:42:26,550: WER debug example
  GT : how does it keep the cost down
  PR : at a cost
2026-01-10 18:42:57,624: Val batch 7000: PER (avg): 0.4037 CTC Loss (avg): 68.2448 WER(5gram): 91.46% (n=256) time: 37.845
2026-01-10 18:42:57,626: WER lens: avg_true_words=5.99 avg_pred_words=4.61 max_pred_words=13
2026-01-10 18:42:57,629: t15.2023.08.13 val PER: 0.3784
2026-01-10 18:42:57,631: t15.2023.08.18 val PER: 0.3705
2026-01-10 18:42:57,632: t15.2023.08.20 val PER: 0.3336
2026-01-10 18:42:57,634: t15.2023.08.25 val PER: 0.3208
2026-01-10 18:42:57,635: t15.2023.08.27 val PER: 0.4421
2026-01-10 18:42:57,637: t15.2023.09.01 val PER: 0.3360
2026-01-10 18:42:57,639: t15.2023.09.03 val PER: 0.4014
2026-01-10 18:42:57,640: t15.2023.09.24 val PER: 0.3726
2026-01-10 18:42:57,642: t15.2023.09.29 val PER: 0.3874
2026-01-10 18:42:57,643: t15.2023.10.01 val PER: 0.4452
2026-01-10 18:42:57,645: t15.2023.10.06 val PER: 0.3348
2026-01-10 18:42:57,646: t15.2023.10.08 val PER: 0.4790
2026-01-10 18:42:57,648: t15.2023.10.13 val PER: 0.5035
2026-01-10 18:42:57,649: t15.2023.10.15 val PER: 0.4120
2026-01-10 18:42:57,652: t15.2023.10.20 val PER: 0.4228
2026-01-10 18:42:57,654: t15.2023.10.22 val PER: 0.3808
2026-01-10 18:42:57,655: t15.2023.11.03 val PER: 0.4030
2026-01-10 18:42:57,657: t15.2023.11.04 val PER: 0.1672
2026-01-10 18:42:57,658: t15.2023.11.17 val PER: 0.2535
2026-01-10 18:42:57,660: t15.2023.11.19 val PER: 0.2555
2026-01-10 18:42:57,661: t15.2023.11.26 val PER: 0.4609
2026-01-10 18:42:57,663: t15.2023.12.03 val PER: 0.3950
2026-01-10 18:42:57,664: t15.2023.12.08 val PER: 0.4101
2026-01-10 18:42:57,665: t15.2023.12.10 val PER: 0.3719
2026-01-10 18:42:57,667: t15.2023.12.17 val PER: 0.4064
2026-01-10 18:42:57,668: t15.2023.12.29 val PER: 0.4221
2026-01-10 18:42:57,670: t15.2024.02.25 val PER: 0.3497
2026-01-10 18:42:57,671: t15.2024.03.08 val PER: 0.4310
2026-01-10 18:42:57,673: t15.2024.03.15 val PER: 0.4209
2026-01-10 18:42:57,674: t15.2024.03.17 val PER: 0.3996
2026-01-10 18:42:57,676: t15.2024.05.10 val PER: 0.4116
2026-01-10 18:42:57,677: t15.2024.06.14 val PER: 0.3864
2026-01-10 18:42:57,679: t15.2024.07.19 val PER: 0.4595
2026-01-10 18:42:57,680: t15.2024.07.21 val PER: 0.3552
2026-01-10 18:42:57,681: t15.2024.07.28 val PER: 0.3949
2026-01-10 18:42:57,683: t15.2025.01.10 val PER: 0.5041
2026-01-10 18:42:57,684: t15.2025.01.12 val PER: 0.4142
2026-01-10 18:42:57,686: t15.2025.03.14 val PER: 0.5000
2026-01-10 18:42:57,687: t15.2025.03.16 val PER: 0.4476
2026-01-10 18:42:57,689: t15.2025.03.30 val PER: 0.5023
2026-01-10 18:42:57,690: t15.2025.04.13 val PER: 0.4408
2026-01-10 18:42:57,692: New best val WER(5gram) 92.50% --> 91.46%
2026-01-10 18:42:58,830: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_7000
2026-01-10 18:43:15,633: Train batch 7200: loss: 56.61 grad norm: 93.42 time: 0.083
2026-01-10 18:43:32,189: Train batch 7400: loss: 59.52 grad norm: 71.97 time: 0.080
2026-01-10 18:43:40,599: Running test after training batch: 7500
2026-01-10 18:43:40,713: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:43:46,697: WER debug example
  GT : you can see the code at this point as well
  PR : you can add a card at this point we
2026-01-10 18:43:46,849: WER debug example
  GT : how does it keep the cost down
  PR : at a cost and
2026-01-10 18:44:17,480: Val batch 7500: PER (avg): 0.3891 CTC Loss (avg): 65.7118 WER(5gram): 86.38% (n=256) time: 36.879
2026-01-10 18:44:17,483: WER lens: avg_true_words=5.99 avg_pred_words=4.82 max_pred_words=13
2026-01-10 18:44:17,486: t15.2023.08.13 val PER: 0.3742
2026-01-10 18:44:17,488: t15.2023.08.18 val PER: 0.3621
2026-01-10 18:44:17,489: t15.2023.08.20 val PER: 0.3296
2026-01-10 18:44:17,491: t15.2023.08.25 val PER: 0.3163
2026-01-10 18:44:17,493: t15.2023.08.27 val PER: 0.4293
2026-01-10 18:44:17,494: t15.2023.09.01 val PER: 0.3206
2026-01-10 18:44:17,496: t15.2023.09.03 val PER: 0.4026
2026-01-10 18:44:17,497: t15.2023.09.24 val PER: 0.3532
2026-01-10 18:44:17,499: t15.2023.09.29 val PER: 0.3580
2026-01-10 18:44:17,500: t15.2023.10.01 val PER: 0.4313
2026-01-10 18:44:17,502: t15.2023.10.06 val PER: 0.3326
2026-01-10 18:44:17,503: t15.2023.10.08 val PER: 0.4614
2026-01-10 18:44:17,505: t15.2023.10.13 val PER: 0.4965
2026-01-10 18:44:17,506: t15.2023.10.15 val PER: 0.4015
2026-01-10 18:44:17,508: t15.2023.10.20 val PER: 0.4094
2026-01-10 18:44:17,509: t15.2023.10.22 val PER: 0.3719
2026-01-10 18:44:17,511: t15.2023.11.03 val PER: 0.3840
2026-01-10 18:44:17,512: t15.2023.11.04 val PER: 0.1638
2026-01-10 18:44:17,514: t15.2023.11.17 val PER: 0.2473
2026-01-10 18:44:17,516: t15.2023.11.19 val PER: 0.2455
2026-01-10 18:44:17,517: t15.2023.11.26 val PER: 0.4580
2026-01-10 18:44:17,519: t15.2023.12.03 val PER: 0.3866
2026-01-10 18:44:17,520: t15.2023.12.08 val PER: 0.3881
2026-01-10 18:44:17,522: t15.2023.12.10 val PER: 0.3574
2026-01-10 18:44:17,523: t15.2023.12.17 val PER: 0.3794
2026-01-10 18:44:17,525: t15.2023.12.29 val PER: 0.4049
2026-01-10 18:44:17,527: t15.2024.02.25 val PER: 0.3427
2026-01-10 18:44:17,528: t15.2024.03.08 val PER: 0.4125
2026-01-10 18:44:17,530: t15.2024.03.15 val PER: 0.4034
2026-01-10 18:44:17,531: t15.2024.03.17 val PER: 0.3766
2026-01-10 18:44:17,533: t15.2024.05.10 val PER: 0.4071
2026-01-10 18:44:17,534: t15.2024.06.14 val PER: 0.3722
2026-01-10 18:44:17,537: t15.2024.07.19 val PER: 0.4357
2026-01-10 18:44:17,539: t15.2024.07.21 val PER: 0.3359
2026-01-10 18:44:17,540: t15.2024.07.28 val PER: 0.3882
2026-01-10 18:44:17,542: t15.2025.01.10 val PER: 0.4848
2026-01-10 18:44:17,543: t15.2025.01.12 val PER: 0.3918
2026-01-10 18:44:17,545: t15.2025.03.14 val PER: 0.4763
2026-01-10 18:44:17,546: t15.2025.03.16 val PER: 0.4097
2026-01-10 18:44:17,548: t15.2025.03.30 val PER: 0.4828
2026-01-10 18:44:17,549: t15.2025.04.13 val PER: 0.4265
2026-01-10 18:44:17,551: New best val WER(5gram) 91.46% --> 86.38%
2026-01-10 18:44:18,687: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_7500
2026-01-10 18:44:27,188: Train batch 7600: loss: 58.46 grad norm: 70.38 time: 0.074
2026-01-10 18:44:43,789: Train batch 7800: loss: 51.59 grad norm: 74.45 time: 0.060
2026-01-10 18:45:00,919: Train batch 8000: loss: 48.54 grad norm: 67.05 time: 0.078
2026-01-10 18:45:00,920: Running test after training batch: 8000
2026-01-10 18:45:01,022: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:45:06,910: WER debug example
  GT : you can see the code at this point as well
  PR : you can add a card at this point is we
2026-01-10 18:45:07,043: WER debug example
  GT : how does it keep the cost down
  PR : a s t u v a ra at the end
2026-01-10 18:45:35,136: Val batch 8000: PER (avg): 0.3796 CTC Loss (avg): 63.4454 WER(5gram): 86.11% (n=256) time: 34.214
2026-01-10 18:45:35,139: WER lens: avg_true_words=5.99 avg_pred_words=5.07 max_pred_words=12
2026-01-10 18:45:35,141: t15.2023.08.13 val PER: 0.3586
2026-01-10 18:45:35,143: t15.2023.08.18 val PER: 0.3437
2026-01-10 18:45:35,145: t15.2023.08.20 val PER: 0.3209
2026-01-10 18:45:35,146: t15.2023.08.25 val PER: 0.3072
2026-01-10 18:45:35,149: t15.2023.08.27 val PER: 0.4196
2026-01-10 18:45:35,152: t15.2023.09.01 val PER: 0.3182
2026-01-10 18:45:35,154: t15.2023.09.03 val PER: 0.3789
2026-01-10 18:45:35,155: t15.2023.09.24 val PER: 0.3507
2026-01-10 18:45:35,157: t15.2023.09.29 val PER: 0.3427
2026-01-10 18:45:35,158: t15.2023.10.01 val PER: 0.4207
2026-01-10 18:45:35,160: t15.2023.10.06 val PER: 0.3197
2026-01-10 18:45:35,161: t15.2023.10.08 val PER: 0.4736
2026-01-10 18:45:35,163: t15.2023.10.13 val PER: 0.4849
2026-01-10 18:45:35,165: t15.2023.10.15 val PER: 0.3942
2026-01-10 18:45:35,166: t15.2023.10.20 val PER: 0.3893
2026-01-10 18:45:35,168: t15.2023.10.22 val PER: 0.3530
2026-01-10 18:45:35,169: t15.2023.11.03 val PER: 0.3779
2026-01-10 18:45:35,171: t15.2023.11.04 val PER: 0.1536
2026-01-10 18:45:35,172: t15.2023.11.17 val PER: 0.2348
2026-01-10 18:45:35,174: t15.2023.11.19 val PER: 0.2176
2026-01-10 18:45:35,175: t15.2023.11.26 val PER: 0.4435
2026-01-10 18:45:35,177: t15.2023.12.03 val PER: 0.3761
2026-01-10 18:45:35,178: t15.2023.12.08 val PER: 0.3835
2026-01-10 18:45:35,180: t15.2023.12.10 val PER: 0.3509
2026-01-10 18:45:35,181: t15.2023.12.17 val PER: 0.3773
2026-01-10 18:45:35,183: t15.2023.12.29 val PER: 0.3892
2026-01-10 18:45:35,184: t15.2024.02.25 val PER: 0.3399
2026-01-10 18:45:35,186: t15.2024.03.08 val PER: 0.4111
2026-01-10 18:45:35,187: t15.2024.03.15 val PER: 0.3996
2026-01-10 18:45:35,189: t15.2024.03.17 val PER: 0.3794
2026-01-10 18:45:35,190: t15.2024.05.10 val PER: 0.3878
2026-01-10 18:45:35,192: t15.2024.06.14 val PER: 0.3644
2026-01-10 18:45:35,193: t15.2024.07.19 val PER: 0.4278
2026-01-10 18:45:35,195: t15.2024.07.21 val PER: 0.3248
2026-01-10 18:45:35,196: t15.2024.07.28 val PER: 0.3581
2026-01-10 18:45:35,198: t15.2025.01.10 val PER: 0.4807
2026-01-10 18:45:35,199: t15.2025.01.12 val PER: 0.3880
2026-01-10 18:45:35,201: t15.2025.03.14 val PER: 0.4778
2026-01-10 18:45:35,202: t15.2025.03.16 val PER: 0.4071
2026-01-10 18:45:35,204: t15.2025.03.30 val PER: 0.4793
2026-01-10 18:45:35,205: t15.2025.04.13 val PER: 0.4151
2026-01-10 18:45:35,207: New best val WER(5gram) 86.38% --> 86.11%
2026-01-10 18:45:36,351: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_8000
2026-01-10 18:45:52,996: Train batch 8200: loss: 40.40 grad norm: 55.82 time: 0.058
2026-01-10 18:46:09,740: Train batch 8400: loss: 44.70 grad norm: 63.92 time: 0.068
2026-01-10 18:46:18,291: Running test after training batch: 8500
2026-01-10 18:46:18,395: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:46:24,688: WER debug example
  GT : you can see the code at this point as well
  PR : get a go at this point is we
2026-01-10 18:46:24,827: WER debug example
  GT : how does it keep the cost down
  PR : so is it a go at the
2026-01-10 18:46:52,135: Val batch 8500: PER (avg): 0.3680 CTC Loss (avg): 61.8134 WER(5gram): 83.31% (n=256) time: 33.842
2026-01-10 18:46:52,137: WER lens: avg_true_words=5.99 avg_pred_words=4.89 max_pred_words=13
2026-01-10 18:46:52,140: t15.2023.08.13 val PER: 0.3576
2026-01-10 18:46:52,143: t15.2023.08.18 val PER: 0.3294
2026-01-10 18:46:52,145: t15.2023.08.20 val PER: 0.3106
2026-01-10 18:46:52,146: t15.2023.08.25 val PER: 0.2846
2026-01-10 18:46:52,148: t15.2023.08.27 val PER: 0.4132
2026-01-10 18:46:52,150: t15.2023.09.01 val PER: 0.3109
2026-01-10 18:46:52,151: t15.2023.09.03 val PER: 0.3587
2026-01-10 18:46:52,153: t15.2023.09.24 val PER: 0.3313
2026-01-10 18:46:52,154: t15.2023.09.29 val PER: 0.3414
2026-01-10 18:46:52,156: t15.2023.10.01 val PER: 0.3910
2026-01-10 18:46:52,158: t15.2023.10.06 val PER: 0.2917
2026-01-10 18:46:52,161: t15.2023.10.08 val PER: 0.4506
2026-01-10 18:46:52,162: t15.2023.10.13 val PER: 0.4701
2026-01-10 18:46:52,164: t15.2023.10.15 val PER: 0.3843
2026-01-10 18:46:52,165: t15.2023.10.20 val PER: 0.4060
2026-01-10 18:46:52,167: t15.2023.10.22 val PER: 0.3497
2026-01-10 18:46:52,169: t15.2023.11.03 val PER: 0.3569
2026-01-10 18:46:52,171: t15.2023.11.04 val PER: 0.1502
2026-01-10 18:46:52,173: t15.2023.11.17 val PER: 0.2177
2026-01-10 18:46:52,175: t15.2023.11.19 val PER: 0.1996
2026-01-10 18:46:52,176: t15.2023.11.26 val PER: 0.4391
2026-01-10 18:46:52,178: t15.2023.12.03 val PER: 0.3540
2026-01-10 18:46:52,180: t15.2023.12.08 val PER: 0.3595
2026-01-10 18:46:52,182: t15.2023.12.10 val PER: 0.3233
2026-01-10 18:46:52,184: t15.2023.12.17 val PER: 0.3669
2026-01-10 18:46:52,185: t15.2023.12.29 val PER: 0.3857
2026-01-10 18:46:52,187: t15.2024.02.25 val PER: 0.3343
2026-01-10 18:46:52,188: t15.2024.03.08 val PER: 0.4125
2026-01-10 18:46:52,190: t15.2024.03.15 val PER: 0.3927
2026-01-10 18:46:52,192: t15.2024.03.17 val PER: 0.3605
2026-01-10 18:46:52,193: t15.2024.05.10 val PER: 0.3908
2026-01-10 18:46:52,195: t15.2024.06.14 val PER: 0.3486
2026-01-10 18:46:52,197: t15.2024.07.19 val PER: 0.4219
2026-01-10 18:46:52,198: t15.2024.07.21 val PER: 0.3062
2026-01-10 18:46:52,200: t15.2024.07.28 val PER: 0.3618
2026-01-10 18:46:52,201: t15.2025.01.10 val PER: 0.4738
2026-01-10 18:46:52,202: t15.2025.01.12 val PER: 0.3780
2026-01-10 18:46:52,204: t15.2025.03.14 val PER: 0.4852
2026-01-10 18:46:52,206: t15.2025.03.16 val PER: 0.3953
2026-01-10 18:46:52,207: t15.2025.03.30 val PER: 0.4678
2026-01-10 18:46:52,209: t15.2025.04.13 val PER: 0.4066
2026-01-10 18:46:52,210: New best val WER(5gram) 86.11% --> 83.31%
2026-01-10 18:46:53,349: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_8500
2026-01-10 18:47:01,756: Train batch 8600: loss: 54.59 grad norm: 81.93 time: 0.059
2026-01-10 18:47:18,454: Train batch 8800: loss: 59.89 grad norm: 81.64 time: 0.066
2026-01-10 18:47:35,536: Train batch 9000: loss: 55.83 grad norm: 79.01 time: 0.077
2026-01-10 18:47:35,539: Running test after training batch: 9000
2026-01-10 18:47:35,642: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:47:41,757: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point is
2026-01-10 18:47:41,880: WER debug example
  GT : how does it keep the cost down
  PR : a s t u v a ra at the
2026-01-10 18:48:07,574: Val batch 9000: PER (avg): 0.3626 CTC Loss (avg): 59.9112 WER(5gram): 87.81% (n=256) time: 32.033
2026-01-10 18:48:07,577: WER lens: avg_true_words=5.99 avg_pred_words=5.10 max_pred_words=15
2026-01-10 18:48:07,579: t15.2023.08.13 val PER: 0.3430
2026-01-10 18:48:07,581: t15.2023.08.18 val PER: 0.3236
2026-01-10 18:48:07,583: t15.2023.08.20 val PER: 0.2994
2026-01-10 18:48:07,584: t15.2023.08.25 val PER: 0.2892
2026-01-10 18:48:07,586: t15.2023.08.27 val PER: 0.4019
2026-01-10 18:48:07,588: t15.2023.09.01 val PER: 0.2971
2026-01-10 18:48:07,589: t15.2023.09.03 val PER: 0.3622
2026-01-10 18:48:07,591: t15.2023.09.24 val PER: 0.3362
2026-01-10 18:48:07,593: t15.2023.09.29 val PER: 0.3312
2026-01-10 18:48:07,594: t15.2023.10.01 val PER: 0.3996
2026-01-10 18:48:07,596: t15.2023.10.06 val PER: 0.2906
2026-01-10 18:48:07,599: t15.2023.10.08 val PER: 0.4425
2026-01-10 18:48:07,601: t15.2023.10.13 val PER: 0.4663
2026-01-10 18:48:07,602: t15.2023.10.15 val PER: 0.3843
2026-01-10 18:48:07,604: t15.2023.10.20 val PER: 0.3859
2026-01-10 18:48:07,606: t15.2023.10.22 val PER: 0.3330
2026-01-10 18:48:07,607: t15.2023.11.03 val PER: 0.3643
2026-01-10 18:48:07,609: t15.2023.11.04 val PER: 0.1433
2026-01-10 18:48:07,610: t15.2023.11.17 val PER: 0.2100
2026-01-10 18:48:07,612: t15.2023.11.19 val PER: 0.2176
2026-01-10 18:48:07,613: t15.2023.11.26 val PER: 0.4297
2026-01-10 18:48:07,616: t15.2023.12.03 val PER: 0.3613
2026-01-10 18:48:07,617: t15.2023.12.08 val PER: 0.3635
2026-01-10 18:48:07,619: t15.2023.12.10 val PER: 0.3154
2026-01-10 18:48:07,621: t15.2023.12.17 val PER: 0.3617
2026-01-10 18:48:07,622: t15.2023.12.29 val PER: 0.3741
2026-01-10 18:48:07,624: t15.2024.02.25 val PER: 0.3202
2026-01-10 18:48:07,625: t15.2024.03.08 val PER: 0.3855
2026-01-10 18:48:07,627: t15.2024.03.15 val PER: 0.3934
2026-01-10 18:48:07,628: t15.2024.03.17 val PER: 0.3543
2026-01-10 18:48:07,630: t15.2024.05.10 val PER: 0.3848
2026-01-10 18:48:07,631: t15.2024.06.14 val PER: 0.3360
2026-01-10 18:48:07,633: t15.2024.07.19 val PER: 0.3968
2026-01-10 18:48:07,634: t15.2024.07.21 val PER: 0.3048
2026-01-10 18:48:07,636: t15.2024.07.28 val PER: 0.3581
2026-01-10 18:48:07,637: t15.2025.01.10 val PER: 0.4711
2026-01-10 18:48:07,638: t15.2025.01.12 val PER: 0.3780
2026-01-10 18:48:07,640: t15.2025.03.14 val PER: 0.4719
2026-01-10 18:48:07,641: t15.2025.03.16 val PER: 0.3874
2026-01-10 18:48:07,643: t15.2025.03.30 val PER: 0.4586
2026-01-10 18:48:07,644: t15.2025.04.13 val PER: 0.4009
2026-01-10 18:48:07,778: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_9000
2026-01-10 18:48:24,581: Train batch 9200: loss: 44.00 grad norm: 53.76 time: 0.061
2026-01-10 18:48:41,261: Train batch 9400: loss: 40.25 grad norm: 66.37 time: 0.073
2026-01-10 18:48:49,567: Running test after training batch: 9500
2026-01-10 18:48:49,701: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:48:55,972: WER debug example
  GT : you can see the code at this point as well
  PR : i know it a go at it this is why
2026-01-10 18:48:56,088: WER debug example
  GT : how does it keep the cost down
  PR : a s t u v a ra at the end
2026-01-10 18:49:21,799: Val batch 9500: PER (avg): 0.3506 CTC Loss (avg): 57.7772 WER(5gram): 88.33% (n=256) time: 32.230
2026-01-10 18:49:21,802: WER lens: avg_true_words=5.99 avg_pred_words=5.27 max_pred_words=13
2026-01-10 18:49:21,804: t15.2023.08.13 val PER: 0.3212
2026-01-10 18:49:21,806: t15.2023.08.18 val PER: 0.3143
2026-01-10 18:49:21,807: t15.2023.08.20 val PER: 0.2867
2026-01-10 18:49:21,809: t15.2023.08.25 val PER: 0.2741
2026-01-10 18:49:21,810: t15.2023.08.27 val PER: 0.3987
2026-01-10 18:49:21,812: t15.2023.09.01 val PER: 0.2881
2026-01-10 18:49:21,814: t15.2023.09.03 val PER: 0.3492
2026-01-10 18:49:21,815: t15.2023.09.24 val PER: 0.3119
2026-01-10 18:49:21,817: t15.2023.09.29 val PER: 0.3178
2026-01-10 18:49:21,818: t15.2023.10.01 val PER: 0.3791
2026-01-10 18:49:21,820: t15.2023.10.06 val PER: 0.2777
2026-01-10 18:49:21,821: t15.2023.10.08 val PER: 0.4276
2026-01-10 18:49:21,824: t15.2023.10.13 val PER: 0.4593
2026-01-10 18:49:21,825: t15.2023.10.15 val PER: 0.3619
2026-01-10 18:49:21,827: t15.2023.10.20 val PER: 0.3792
2026-01-10 18:49:21,828: t15.2023.10.22 val PER: 0.3285
2026-01-10 18:49:21,830: t15.2023.11.03 val PER: 0.3440
2026-01-10 18:49:21,831: t15.2023.11.04 val PER: 0.1297
2026-01-10 18:49:21,833: t15.2023.11.17 val PER: 0.2006
2026-01-10 18:49:21,835: t15.2023.11.19 val PER: 0.1976
2026-01-10 18:49:21,836: t15.2023.11.26 val PER: 0.4210
2026-01-10 18:49:21,838: t15.2023.12.03 val PER: 0.3624
2026-01-10 18:49:21,839: t15.2023.12.08 val PER: 0.3442
2026-01-10 18:49:21,841: t15.2023.12.10 val PER: 0.3035
2026-01-10 18:49:21,842: t15.2023.12.17 val PER: 0.3378
2026-01-10 18:49:21,844: t15.2023.12.29 val PER: 0.3617
2026-01-10 18:49:21,845: t15.2024.02.25 val PER: 0.3020
2026-01-10 18:49:21,847: t15.2024.03.08 val PER: 0.4011
2026-01-10 18:49:21,849: t15.2024.03.15 val PER: 0.3821
2026-01-10 18:49:21,850: t15.2024.03.17 val PER: 0.3326
2026-01-10 18:49:21,852: t15.2024.05.10 val PER: 0.3596
2026-01-10 18:49:21,853: t15.2024.06.14 val PER: 0.3438
2026-01-10 18:49:21,855: t15.2024.07.19 val PER: 0.4021
2026-01-10 18:49:21,857: t15.2024.07.21 val PER: 0.2938
2026-01-10 18:49:21,858: t15.2024.07.28 val PER: 0.3544
2026-01-10 18:49:21,861: t15.2025.01.10 val PER: 0.4614
2026-01-10 18:49:21,863: t15.2025.01.12 val PER: 0.3672
2026-01-10 18:49:21,864: t15.2025.03.14 val PER: 0.4467
2026-01-10 18:49:21,866: t15.2025.03.16 val PER: 0.3953
2026-01-10 18:49:21,867: t15.2025.03.30 val PER: 0.4425
2026-01-10 18:49:21,869: t15.2025.04.13 val PER: 0.3909
2026-01-10 18:49:22,002: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_9500
2026-01-10 18:49:30,316: Train batch 9600: loss: 36.52 grad norm: 62.52 time: 0.078
2026-01-10 18:49:47,060: Train batch 9800: loss: 51.54 grad norm: 87.14 time: 0.068
2026-01-10 18:50:03,904: Train batch 10000: loss: 30.57 grad norm: 71.80 time: 0.065
2026-01-10 18:50:03,906: Running test after training batch: 10000
2026-01-10 18:50:04,020: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:50:10,105: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point is we
2026-01-10 18:50:10,223: WER debug example
  GT : how does it keep the cost down
  PR : so is it a go at the end
2026-01-10 18:50:33,743: Val batch 10000: PER (avg): 0.3458 CTC Loss (avg): 56.2552 WER(5gram): 80.57% (n=256) time: 29.834
2026-01-10 18:50:33,745: WER lens: avg_true_words=5.99 avg_pred_words=4.98 max_pred_words=12
2026-01-10 18:50:33,748: t15.2023.08.13 val PER: 0.3295
2026-01-10 18:50:33,750: t15.2023.08.18 val PER: 0.3068
2026-01-10 18:50:33,752: t15.2023.08.20 val PER: 0.2724
2026-01-10 18:50:33,753: t15.2023.08.25 val PER: 0.2726
2026-01-10 18:50:33,755: t15.2023.08.27 val PER: 0.3682
2026-01-10 18:50:33,757: t15.2023.09.01 val PER: 0.2776
2026-01-10 18:50:33,758: t15.2023.09.03 val PER: 0.3432
2026-01-10 18:50:33,760: t15.2023.09.24 val PER: 0.3046
2026-01-10 18:50:33,761: t15.2023.09.29 val PER: 0.3101
2026-01-10 18:50:33,763: t15.2023.10.01 val PER: 0.3765
2026-01-10 18:50:33,766: t15.2023.10.06 val PER: 0.2788
2026-01-10 18:50:33,769: t15.2023.10.08 val PER: 0.4249
2026-01-10 18:50:33,771: t15.2023.10.13 val PER: 0.4430
2026-01-10 18:50:33,773: t15.2023.10.15 val PER: 0.3711
2026-01-10 18:50:33,774: t15.2023.10.20 val PER: 0.3658
2026-01-10 18:50:33,776: t15.2023.10.22 val PER: 0.3229
2026-01-10 18:50:33,778: t15.2023.11.03 val PER: 0.3467
2026-01-10 18:50:33,780: t15.2023.11.04 val PER: 0.1365
2026-01-10 18:50:33,782: t15.2023.11.17 val PER: 0.1991
2026-01-10 18:50:33,784: t15.2023.11.19 val PER: 0.1876
2026-01-10 18:50:33,785: t15.2023.11.26 val PER: 0.4196
2026-01-10 18:50:33,787: t15.2023.12.03 val PER: 0.3435
2026-01-10 18:50:33,789: t15.2023.12.08 val PER: 0.3402
2026-01-10 18:50:33,791: t15.2023.12.10 val PER: 0.2996
2026-01-10 18:50:33,792: t15.2023.12.17 val PER: 0.3410
2026-01-10 18:50:33,794: t15.2023.12.29 val PER: 0.3535
2026-01-10 18:50:33,796: t15.2024.02.25 val PER: 0.3132
2026-01-10 18:50:33,798: t15.2024.03.08 val PER: 0.3841
2026-01-10 18:50:33,799: t15.2024.03.15 val PER: 0.3665
2026-01-10 18:50:33,801: t15.2024.03.17 val PER: 0.3452
2026-01-10 18:50:33,803: t15.2024.05.10 val PER: 0.3596
2026-01-10 18:50:33,804: t15.2024.06.14 val PER: 0.3407
2026-01-10 18:50:33,806: t15.2024.07.19 val PER: 0.3935
2026-01-10 18:50:33,807: t15.2024.07.21 val PER: 0.2883
2026-01-10 18:50:33,808: t15.2024.07.28 val PER: 0.3515
2026-01-10 18:50:33,810: t15.2025.01.10 val PER: 0.4683
2026-01-10 18:50:33,811: t15.2025.01.12 val PER: 0.3587
2026-01-10 18:50:33,813: t15.2025.03.14 val PER: 0.4601
2026-01-10 18:50:33,814: t15.2025.03.16 val PER: 0.3678
2026-01-10 18:50:33,816: t15.2025.03.30 val PER: 0.4345
2026-01-10 18:50:33,817: t15.2025.04.13 val PER: 0.3894
2026-01-10 18:50:33,819: New best val WER(5gram) 83.31% --> 80.57%
2026-01-10 18:50:34,958: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_10000
2026-01-10 18:50:51,609: Train batch 10200: loss: 33.93 grad norm: 55.71 time: 0.054
2026-01-10 18:51:08,561: Train batch 10400: loss: 38.16 grad norm: 90.98 time: 0.076
2026-01-10 18:51:17,038: Running test after training batch: 10500
2026-01-10 18:51:17,138: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:51:23,278: WER debug example
  GT : you can see the code at this point as well
  PR : i need a good at this point is we
2026-01-10 18:51:23,384: WER debug example
  GT : how does it keep the cost down
  PR : i know is it a go at it
2026-01-10 18:51:46,883: Val batch 10500: PER (avg): 0.3300 CTC Loss (avg): 54.9298 WER(5gram): 80.96% (n=256) time: 29.843
2026-01-10 18:51:46,886: WER lens: avg_true_words=5.99 avg_pred_words=5.41 max_pred_words=13
2026-01-10 18:51:46,888: t15.2023.08.13 val PER: 0.2994
2026-01-10 18:51:46,890: t15.2023.08.18 val PER: 0.2900
2026-01-10 18:51:46,892: t15.2023.08.20 val PER: 0.2621
2026-01-10 18:51:46,894: t15.2023.08.25 val PER: 0.2515
2026-01-10 18:51:46,896: t15.2023.08.27 val PER: 0.3633
2026-01-10 18:51:46,898: t15.2023.09.01 val PER: 0.2630
2026-01-10 18:51:46,900: t15.2023.09.03 val PER: 0.3242
2026-01-10 18:51:46,902: t15.2023.09.24 val PER: 0.3046
2026-01-10 18:51:46,904: t15.2023.09.29 val PER: 0.2891
2026-01-10 18:51:46,906: t15.2023.10.01 val PER: 0.3540
2026-01-10 18:51:46,907: t15.2023.10.06 val PER: 0.2573
2026-01-10 18:51:46,909: t15.2023.10.08 val PER: 0.4181
2026-01-10 18:51:46,911: t15.2023.10.13 val PER: 0.4290
2026-01-10 18:51:46,912: t15.2023.10.15 val PER: 0.3467
2026-01-10 18:51:46,914: t15.2023.10.20 val PER: 0.3591
2026-01-10 18:51:46,916: t15.2023.10.22 val PER: 0.3085
2026-01-10 18:51:46,917: t15.2023.11.03 val PER: 0.3365
2026-01-10 18:51:46,919: t15.2023.11.04 val PER: 0.1195
2026-01-10 18:51:46,921: t15.2023.11.17 val PER: 0.1788
2026-01-10 18:51:46,923: t15.2023.11.19 val PER: 0.1756
2026-01-10 18:51:46,926: t15.2023.11.26 val PER: 0.3870
2026-01-10 18:51:46,927: t15.2023.12.03 val PER: 0.3235
2026-01-10 18:51:46,929: t15.2023.12.08 val PER: 0.3289
2026-01-10 18:51:46,930: t15.2023.12.10 val PER: 0.2654
2026-01-10 18:51:46,932: t15.2023.12.17 val PER: 0.3212
2026-01-10 18:51:46,933: t15.2023.12.29 val PER: 0.3418
2026-01-10 18:51:46,935: t15.2024.02.25 val PER: 0.2823
2026-01-10 18:51:46,937: t15.2024.03.08 val PER: 0.3755
2026-01-10 18:51:46,938: t15.2024.03.15 val PER: 0.3709
2026-01-10 18:51:46,940: t15.2024.03.17 val PER: 0.3215
2026-01-10 18:51:46,942: t15.2024.05.10 val PER: 0.3299
2026-01-10 18:51:46,943: t15.2024.06.14 val PER: 0.3265
2026-01-10 18:51:46,945: t15.2024.07.19 val PER: 0.3797
2026-01-10 18:51:46,946: t15.2024.07.21 val PER: 0.2759
2026-01-10 18:51:46,948: t15.2024.07.28 val PER: 0.3346
2026-01-10 18:51:46,950: t15.2025.01.10 val PER: 0.4311
2026-01-10 18:51:46,951: t15.2025.01.12 val PER: 0.3349
2026-01-10 18:51:46,953: t15.2025.03.14 val PER: 0.4630
2026-01-10 18:51:46,956: t15.2025.03.16 val PER: 0.3757
2026-01-10 18:51:46,958: t15.2025.03.30 val PER: 0.4460
2026-01-10 18:51:46,959: t15.2025.04.13 val PER: 0.3695
2026-01-10 18:51:47,094: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_10500
2026-01-10 18:51:55,593: Train batch 10600: loss: 35.94 grad norm: 75.28 time: 0.076
2026-01-10 18:52:12,167: Train batch 10800: loss: 55.19 grad norm: 135.43 time: 0.070
2026-01-10 18:52:29,120: Train batch 11000: loss: 52.61 grad norm: 82.45 time: 0.063
2026-01-10 18:52:29,122: Running test after training batch: 11000
2026-01-10 18:52:29,249: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:52:35,122: WER debug example
  GT : you can see the code at this point as well
  PR : you may need a go at this point is we
2026-01-10 18:52:35,225: WER debug example
  GT : how does it keep the cost down
  PR : the us to go to a guy at the end
2026-01-10 18:52:57,859: Val batch 11000: PER (avg): 0.3232 CTC Loss (avg): 53.4864 WER(5gram): 81.03% (n=256) time: 28.734
2026-01-10 18:52:57,862: WER lens: avg_true_words=5.99 avg_pred_words=5.40 max_pred_words=14
2026-01-10 18:52:57,865: t15.2023.08.13 val PER: 0.3067
2026-01-10 18:52:57,867: t15.2023.08.18 val PER: 0.2749
2026-01-10 18:52:57,868: t15.2023.08.20 val PER: 0.2621
2026-01-10 18:52:57,870: t15.2023.08.25 val PER: 0.2530
2026-01-10 18:52:57,872: t15.2023.08.27 val PER: 0.3408
2026-01-10 18:52:57,873: t15.2023.09.01 val PER: 0.2589
2026-01-10 18:52:57,875: t15.2023.09.03 val PER: 0.3183
2026-01-10 18:52:57,877: t15.2023.09.24 val PER: 0.2743
2026-01-10 18:52:57,879: t15.2023.09.29 val PER: 0.2853
2026-01-10 18:52:57,880: t15.2023.10.01 val PER: 0.3534
2026-01-10 18:52:57,882: t15.2023.10.06 val PER: 0.2583
2026-01-10 18:52:57,884: t15.2023.10.08 val PER: 0.4141
2026-01-10 18:52:57,885: t15.2023.10.13 val PER: 0.4220
2026-01-10 18:52:57,887: t15.2023.10.15 val PER: 0.3388
2026-01-10 18:52:57,889: t15.2023.10.20 val PER: 0.3557
2026-01-10 18:52:57,890: t15.2023.10.22 val PER: 0.3062
2026-01-10 18:52:57,892: t15.2023.11.03 val PER: 0.3223
2026-01-10 18:52:57,894: t15.2023.11.04 val PER: 0.1229
2026-01-10 18:52:57,898: t15.2023.11.17 val PER: 0.1726
2026-01-10 18:52:57,900: t15.2023.11.19 val PER: 0.1776
2026-01-10 18:52:57,901: t15.2023.11.26 val PER: 0.3862
2026-01-10 18:52:57,903: t15.2023.12.03 val PER: 0.3193
2026-01-10 18:52:57,904: t15.2023.12.08 val PER: 0.3169
2026-01-10 18:52:57,906: t15.2023.12.10 val PER: 0.2786
2026-01-10 18:52:57,908: t15.2023.12.17 val PER: 0.3264
2026-01-10 18:52:57,909: t15.2023.12.29 val PER: 0.3233
2026-01-10 18:52:57,911: t15.2024.02.25 val PER: 0.2725
2026-01-10 18:52:57,912: t15.2024.03.08 val PER: 0.3656
2026-01-10 18:52:57,914: t15.2024.03.15 val PER: 0.3652
2026-01-10 18:52:57,915: t15.2024.03.17 val PER: 0.3131
2026-01-10 18:52:57,917: t15.2024.05.10 val PER: 0.3373
2026-01-10 18:52:57,918: t15.2024.06.14 val PER: 0.3202
2026-01-10 18:52:57,920: t15.2024.07.19 val PER: 0.3652
2026-01-10 18:52:57,921: t15.2024.07.21 val PER: 0.2614
2026-01-10 18:52:57,923: t15.2024.07.28 val PER: 0.3243
2026-01-10 18:52:57,927: t15.2025.01.10 val PER: 0.4311
2026-01-10 18:52:57,929: t15.2025.01.12 val PER: 0.3364
2026-01-10 18:52:57,931: t15.2025.03.14 val PER: 0.4601
2026-01-10 18:52:57,932: t15.2025.03.16 val PER: 0.3626
2026-01-10 18:52:57,934: t15.2025.03.30 val PER: 0.4103
2026-01-10 18:52:57,935: t15.2025.04.13 val PER: 0.3723
2026-01-10 18:52:58,070: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_11000
2026-01-10 18:53:14,753: Train batch 11200: loss: 43.06 grad norm: 65.67 time: 0.077
2026-01-10 18:53:31,982: Train batch 11400: loss: 38.98 grad norm: 77.94 time: 0.061
2026-01-10 18:53:40,648: Running test after training batch: 11500
2026-01-10 18:53:40,789: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:53:46,646: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point is we
2026-01-10 18:53:46,746: WER debug example
  GT : how does it keep the cost down
  PR : the us to be a go at it
2026-01-10 18:54:08,414: Val batch 11500: PER (avg): 0.3151 CTC Loss (avg): 51.8492 WER(5gram): 79.14% (n=256) time: 27.764
2026-01-10 18:54:08,418: WER lens: avg_true_words=5.99 avg_pred_words=5.53 max_pred_words=14
2026-01-10 18:54:08,421: t15.2023.08.13 val PER: 0.3004
2026-01-10 18:54:08,423: t15.2023.08.18 val PER: 0.2615
2026-01-10 18:54:08,424: t15.2023.08.20 val PER: 0.2518
2026-01-10 18:54:08,426: t15.2023.08.25 val PER: 0.2485
2026-01-10 18:54:08,428: t15.2023.08.27 val PER: 0.3424
2026-01-10 18:54:08,429: t15.2023.09.01 val PER: 0.2443
2026-01-10 18:54:08,431: t15.2023.09.03 val PER: 0.3183
2026-01-10 18:54:08,433: t15.2023.09.24 val PER: 0.2803
2026-01-10 18:54:08,434: t15.2023.09.29 val PER: 0.2750
2026-01-10 18:54:08,436: t15.2023.10.01 val PER: 0.3362
2026-01-10 18:54:08,437: t15.2023.10.06 val PER: 0.2519
2026-01-10 18:54:08,439: t15.2023.10.08 val PER: 0.3951
2026-01-10 18:54:08,440: t15.2023.10.13 val PER: 0.4088
2026-01-10 18:54:08,442: t15.2023.10.15 val PER: 0.3349
2026-01-10 18:54:08,444: t15.2023.10.20 val PER: 0.3423
2026-01-10 18:54:08,445: t15.2023.10.22 val PER: 0.2951
2026-01-10 18:54:08,447: t15.2023.11.03 val PER: 0.3161
2026-01-10 18:54:08,449: t15.2023.11.04 val PER: 0.1126
2026-01-10 18:54:08,450: t15.2023.11.17 val PER: 0.1633
2026-01-10 18:54:08,452: t15.2023.11.19 val PER: 0.1657
2026-01-10 18:54:08,454: t15.2023.11.26 val PER: 0.3623
2026-01-10 18:54:08,455: t15.2023.12.03 val PER: 0.3015
2026-01-10 18:54:08,457: t15.2023.12.08 val PER: 0.2976
2026-01-10 18:54:08,458: t15.2023.12.10 val PER: 0.2654
2026-01-10 18:54:08,459: t15.2023.12.17 val PER: 0.3150
2026-01-10 18:54:08,461: t15.2023.12.29 val PER: 0.3253
2026-01-10 18:54:08,462: t15.2024.02.25 val PER: 0.2753
2026-01-10 18:54:08,466: t15.2024.03.08 val PER: 0.3627
2026-01-10 18:54:08,467: t15.2024.03.15 val PER: 0.3515
2026-01-10 18:54:08,469: t15.2024.03.17 val PER: 0.3138
2026-01-10 18:54:08,470: t15.2024.05.10 val PER: 0.3388
2026-01-10 18:54:08,472: t15.2024.06.14 val PER: 0.3218
2026-01-10 18:54:08,473: t15.2024.07.19 val PER: 0.3659
2026-01-10 18:54:08,475: t15.2024.07.21 val PER: 0.2503
2026-01-10 18:54:08,476: t15.2024.07.28 val PER: 0.3221
2026-01-10 18:54:08,478: t15.2025.01.10 val PER: 0.4339
2026-01-10 18:54:08,480: t15.2025.01.12 val PER: 0.3272
2026-01-10 18:54:08,481: t15.2025.03.14 val PER: 0.4438
2026-01-10 18:54:08,482: t15.2025.03.16 val PER: 0.3613
2026-01-10 18:54:08,484: t15.2025.03.30 val PER: 0.4069
2026-01-10 18:54:08,486: t15.2025.04.13 val PER: 0.3666
2026-01-10 18:54:08,488: New best val WER(5gram) 80.57% --> 79.14%
2026-01-10 18:54:09,604: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_11500
2026-01-10 18:54:17,866: Train batch 11600: loss: 43.36 grad norm: 66.27 time: 0.066
2026-01-10 18:54:34,408: Train batch 11800: loss: 31.60 grad norm: 68.24 time: 0.050
2026-01-10 18:54:51,096: Train batch 12000: loss: 42.99 grad norm: 81.08 time: 0.078
2026-01-10 18:54:51,098: Running test after training batch: 12000
2026-01-10 18:54:51,199: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:54:57,279: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point is we
2026-01-10 18:54:57,391: WER debug example
  GT : how does it keep the cost down
  PR : the us to keep a guy out and
2026-01-10 18:55:18,074: Val batch 12000: PER (avg): 0.3106 CTC Loss (avg): 50.7866 WER(5gram): 79.40% (n=256) time: 26.973
2026-01-10 18:55:18,076: WER lens: avg_true_words=5.99 avg_pred_words=5.53 max_pred_words=14
2026-01-10 18:55:18,079: t15.2023.08.13 val PER: 0.2931
2026-01-10 18:55:18,081: t15.2023.08.18 val PER: 0.2640
2026-01-10 18:55:18,082: t15.2023.08.20 val PER: 0.2494
2026-01-10 18:55:18,084: t15.2023.08.25 val PER: 0.2455
2026-01-10 18:55:18,086: t15.2023.08.27 val PER: 0.3473
2026-01-10 18:55:18,087: t15.2023.09.01 val PER: 0.2354
2026-01-10 18:55:18,089: t15.2023.09.03 val PER: 0.3052
2026-01-10 18:55:18,091: t15.2023.09.24 val PER: 0.2731
2026-01-10 18:55:18,092: t15.2023.09.29 val PER: 0.2757
2026-01-10 18:55:18,094: t15.2023.10.01 val PER: 0.3355
2026-01-10 18:55:18,096: t15.2023.10.06 val PER: 0.2476
2026-01-10 18:55:18,098: t15.2023.10.08 val PER: 0.3951
2026-01-10 18:55:18,100: t15.2023.10.13 val PER: 0.4151
2026-01-10 18:55:18,101: t15.2023.10.15 val PER: 0.3309
2026-01-10 18:55:18,103: t15.2023.10.20 val PER: 0.3322
2026-01-10 18:55:18,104: t15.2023.10.22 val PER: 0.2962
2026-01-10 18:55:18,106: t15.2023.11.03 val PER: 0.3189
2026-01-10 18:55:18,108: t15.2023.11.04 val PER: 0.1058
2026-01-10 18:55:18,110: t15.2023.11.17 val PER: 0.1493
2026-01-10 18:55:18,111: t15.2023.11.19 val PER: 0.1617
2026-01-10 18:55:18,113: t15.2023.11.26 val PER: 0.3667
2026-01-10 18:55:18,114: t15.2023.12.03 val PER: 0.2973
2026-01-10 18:55:18,115: t15.2023.12.08 val PER: 0.3036
2026-01-10 18:55:18,117: t15.2023.12.10 val PER: 0.2602
2026-01-10 18:55:18,118: t15.2023.12.17 val PER: 0.2921
2026-01-10 18:55:18,120: t15.2023.12.29 val PER: 0.3123
2026-01-10 18:55:18,122: t15.2024.02.25 val PER: 0.2612
2026-01-10 18:55:18,124: t15.2024.03.08 val PER: 0.3570
2026-01-10 18:55:18,128: t15.2024.03.15 val PER: 0.3452
2026-01-10 18:55:18,130: t15.2024.03.17 val PER: 0.3096
2026-01-10 18:55:18,131: t15.2024.05.10 val PER: 0.3195
2026-01-10 18:55:18,133: t15.2024.06.14 val PER: 0.3076
2026-01-10 18:55:18,135: t15.2024.07.19 val PER: 0.3606
2026-01-10 18:55:18,136: t15.2024.07.21 val PER: 0.2469
2026-01-10 18:55:18,137: t15.2024.07.28 val PER: 0.3081
2026-01-10 18:55:18,139: t15.2025.01.10 val PER: 0.4050
2026-01-10 18:55:18,140: t15.2025.01.12 val PER: 0.3380
2026-01-10 18:55:18,142: t15.2025.03.14 val PER: 0.4290
2026-01-10 18:55:18,143: t15.2025.03.16 val PER: 0.3495
2026-01-10 18:55:18,145: t15.2025.03.30 val PER: 0.4138
2026-01-10 18:55:18,146: t15.2025.04.13 val PER: 0.3652
2026-01-10 18:55:18,279: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_12000
2026-01-10 18:55:34,940: Train batch 12200: loss: 32.89 grad norm: 87.24 time: 0.070
2026-01-10 18:55:51,354: Train batch 12400: loss: 25.66 grad norm: 55.29 time: 0.045
2026-01-10 18:55:59,785: Running test after training batch: 12500
2026-01-10 18:55:59,910: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:56:05,814: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point is we
2026-01-10 18:56:05,915: WER debug example
  GT : how does it keep the cost down
  PR : i see it in a row at the end
2026-01-10 18:56:25,837: Val batch 12500: PER (avg): 0.3019 CTC Loss (avg): 49.8945 WER(5gram): 75.03% (n=256) time: 26.050
2026-01-10 18:56:25,840: WER lens: avg_true_words=5.99 avg_pred_words=5.68 max_pred_words=14
2026-01-10 18:56:25,842: t15.2023.08.13 val PER: 0.2755
2026-01-10 18:56:25,843: t15.2023.08.18 val PER: 0.2615
2026-01-10 18:56:25,845: t15.2023.08.20 val PER: 0.2415
2026-01-10 18:56:25,847: t15.2023.08.25 val PER: 0.2334
2026-01-10 18:56:25,848: t15.2023.08.27 val PER: 0.3521
2026-01-10 18:56:25,850: t15.2023.09.01 val PER: 0.2240
2026-01-10 18:56:25,852: t15.2023.09.03 val PER: 0.2993
2026-01-10 18:56:25,853: t15.2023.09.24 val PER: 0.2646
2026-01-10 18:56:25,855: t15.2023.09.29 val PER: 0.2636
2026-01-10 18:56:25,856: t15.2023.10.01 val PER: 0.3170
2026-01-10 18:56:25,858: t15.2023.10.06 val PER: 0.2368
2026-01-10 18:56:25,859: t15.2023.10.08 val PER: 0.3911
2026-01-10 18:56:25,861: t15.2023.10.13 val PER: 0.3910
2026-01-10 18:56:25,862: t15.2023.10.15 val PER: 0.3184
2026-01-10 18:56:25,864: t15.2023.10.20 val PER: 0.3255
2026-01-10 18:56:25,865: t15.2023.10.22 val PER: 0.2884
2026-01-10 18:56:25,867: t15.2023.11.03 val PER: 0.3134
2026-01-10 18:56:25,868: t15.2023.11.04 val PER: 0.1058
2026-01-10 18:56:25,870: t15.2023.11.17 val PER: 0.1462
2026-01-10 18:56:25,872: t15.2023.11.19 val PER: 0.1557
2026-01-10 18:56:25,873: t15.2023.11.26 val PER: 0.3638
2026-01-10 18:56:25,876: t15.2023.12.03 val PER: 0.2815
2026-01-10 18:56:25,878: t15.2023.12.08 val PER: 0.2883
2026-01-10 18:56:25,879: t15.2023.12.10 val PER: 0.2484
2026-01-10 18:56:25,881: t15.2023.12.17 val PER: 0.2931
2026-01-10 18:56:25,883: t15.2023.12.29 val PER: 0.3109
2026-01-10 18:56:25,885: t15.2024.02.25 val PER: 0.2598
2026-01-10 18:56:25,886: t15.2024.03.08 val PER: 0.3642
2026-01-10 18:56:25,888: t15.2024.03.15 val PER: 0.3340
2026-01-10 18:56:25,891: t15.2024.03.17 val PER: 0.2978
2026-01-10 18:56:25,892: t15.2024.05.10 val PER: 0.3165
2026-01-10 18:56:25,894: t15.2024.06.14 val PER: 0.2981
2026-01-10 18:56:25,896: t15.2024.07.19 val PER: 0.3612
2026-01-10 18:56:25,897: t15.2024.07.21 val PER: 0.2379
2026-01-10 18:56:25,899: t15.2024.07.28 val PER: 0.3059
2026-01-10 18:56:25,900: t15.2025.01.10 val PER: 0.4132
2026-01-10 18:56:25,902: t15.2025.01.12 val PER: 0.3102
2026-01-10 18:56:25,904: t15.2025.03.14 val PER: 0.4290
2026-01-10 18:56:25,905: t15.2025.03.16 val PER: 0.3416
2026-01-10 18:56:25,907: t15.2025.03.30 val PER: 0.3920
2026-01-10 18:56:25,908: t15.2025.04.13 val PER: 0.3566
2026-01-10 18:56:25,910: New best val WER(5gram) 79.14% --> 75.03%
2026-01-10 18:56:27,051: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_12500
2026-01-10 18:56:35,447: Train batch 12600: loss: 31.36 grad norm: 88.62 time: 0.061
2026-01-10 18:56:52,490: Train batch 12800: loss: 29.19 grad norm: 65.39 time: 0.057
2026-01-10 18:57:09,372: Train batch 13000: loss: 29.64 grad norm: 73.81 time: 0.071
2026-01-10 18:57:09,374: Running test after training batch: 13000
2026-01-10 18:57:09,475: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:57:15,332: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point we
2026-01-10 18:57:15,418: WER debug example
  GT : how does it keep the cost down
  PR : is it a go at the
2026-01-10 18:57:35,794: Val batch 13000: PER (avg): 0.2963 CTC Loss (avg): 48.4142 WER(5gram): 75.49% (n=256) time: 26.417
2026-01-10 18:57:35,796: WER lens: avg_true_words=5.99 avg_pred_words=5.38 max_pred_words=13
2026-01-10 18:57:35,798: t15.2023.08.13 val PER: 0.2827
2026-01-10 18:57:35,800: t15.2023.08.18 val PER: 0.2590
2026-01-10 18:57:35,802: t15.2023.08.20 val PER: 0.2343
2026-01-10 18:57:35,803: t15.2023.08.25 val PER: 0.2304
2026-01-10 18:57:35,805: t15.2023.08.27 val PER: 0.3376
2026-01-10 18:57:35,807: t15.2023.09.01 val PER: 0.2281
2026-01-10 18:57:35,809: t15.2023.09.03 val PER: 0.2969
2026-01-10 18:57:35,810: t15.2023.09.24 val PER: 0.2476
2026-01-10 18:57:35,812: t15.2023.09.29 val PER: 0.2559
2026-01-10 18:57:35,813: t15.2023.10.01 val PER: 0.3091
2026-01-10 18:57:35,815: t15.2023.10.06 val PER: 0.2347
2026-01-10 18:57:35,817: t15.2023.10.08 val PER: 0.3843
2026-01-10 18:57:35,818: t15.2023.10.13 val PER: 0.3871
2026-01-10 18:57:35,820: t15.2023.10.15 val PER: 0.3125
2026-01-10 18:57:35,821: t15.2023.10.20 val PER: 0.3322
2026-01-10 18:57:35,823: t15.2023.10.22 val PER: 0.2840
2026-01-10 18:57:35,824: t15.2023.11.03 val PER: 0.3182
2026-01-10 18:57:35,826: t15.2023.11.04 val PER: 0.0990
2026-01-10 18:57:35,827: t15.2023.11.17 val PER: 0.1524
2026-01-10 18:57:35,828: t15.2023.11.19 val PER: 0.1557
2026-01-10 18:57:35,830: t15.2023.11.26 val PER: 0.3464
2026-01-10 18:57:35,831: t15.2023.12.03 val PER: 0.2847
2026-01-10 18:57:35,832: t15.2023.12.08 val PER: 0.2790
2026-01-10 18:57:35,834: t15.2023.12.10 val PER: 0.2497
2026-01-10 18:57:35,835: t15.2023.12.17 val PER: 0.2796
2026-01-10 18:57:35,836: t15.2023.12.29 val PER: 0.2992
2026-01-10 18:57:35,837: t15.2024.02.25 val PER: 0.2542
2026-01-10 18:57:35,840: t15.2024.03.08 val PER: 0.3471
2026-01-10 18:57:35,841: t15.2024.03.15 val PER: 0.3221
2026-01-10 18:57:35,843: t15.2024.03.17 val PER: 0.2915
2026-01-10 18:57:35,844: t15.2024.05.10 val PER: 0.3135
2026-01-10 18:57:35,846: t15.2024.06.14 val PER: 0.2902
2026-01-10 18:57:35,848: t15.2024.07.19 val PER: 0.3507
2026-01-10 18:57:35,850: t15.2024.07.21 val PER: 0.2317
2026-01-10 18:57:35,852: t15.2024.07.28 val PER: 0.3022
2026-01-10 18:57:35,854: t15.2025.01.10 val PER: 0.4022
2026-01-10 18:57:35,855: t15.2025.01.12 val PER: 0.3133
2026-01-10 18:57:35,857: t15.2025.03.14 val PER: 0.4231
2026-01-10 18:57:35,858: t15.2025.03.16 val PER: 0.3325
2026-01-10 18:57:35,860: t15.2025.03.30 val PER: 0.3943
2026-01-10 18:57:35,861: t15.2025.04.13 val PER: 0.3452
2026-01-10 18:57:35,991: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_13000
2026-01-10 18:57:52,515: Train batch 13200: loss: 45.61 grad norm: 79.78 time: 0.060
2026-01-10 18:58:09,110: Train batch 13400: loss: 32.46 grad norm: 87.51 time: 0.067
2026-01-10 18:58:17,434: Running test after training batch: 13500
2026-01-10 18:58:17,540: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:58:23,427: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point so we
2026-01-10 18:58:23,533: WER debug example
  GT : how does it keep the cost down
  PR : i see it in a row at the end
2026-01-10 18:58:43,872: Val batch 13500: PER (avg): 0.2875 CTC Loss (avg): 47.8313 WER(5gram): 78.68% (n=256) time: 26.436
2026-01-10 18:58:43,874: WER lens: avg_true_words=5.99 avg_pred_words=5.81 max_pred_words=14
2026-01-10 18:58:43,877: t15.2023.08.13 val PER: 0.2651
2026-01-10 18:58:43,879: t15.2023.08.18 val PER: 0.2490
2026-01-10 18:58:43,880: t15.2023.08.20 val PER: 0.2184
2026-01-10 18:58:43,882: t15.2023.08.25 val PER: 0.2199
2026-01-10 18:58:43,883: t15.2023.08.27 val PER: 0.3424
2026-01-10 18:58:43,885: t15.2023.09.01 val PER: 0.2119
2026-01-10 18:58:43,887: t15.2023.09.03 val PER: 0.2779
2026-01-10 18:58:43,889: t15.2023.09.24 val PER: 0.2549
2026-01-10 18:58:43,890: t15.2023.09.29 val PER: 0.2508
2026-01-10 18:58:43,892: t15.2023.10.01 val PER: 0.3078
2026-01-10 18:58:43,893: t15.2023.10.06 val PER: 0.2347
2026-01-10 18:58:43,895: t15.2023.10.08 val PER: 0.3708
2026-01-10 18:58:43,896: t15.2023.10.13 val PER: 0.3825
2026-01-10 18:58:43,898: t15.2023.10.15 val PER: 0.3026
2026-01-10 18:58:43,899: t15.2023.10.20 val PER: 0.3356
2026-01-10 18:58:43,901: t15.2023.10.22 val PER: 0.2639
2026-01-10 18:58:43,903: t15.2023.11.03 val PER: 0.3053
2026-01-10 18:58:43,905: t15.2023.11.04 val PER: 0.0819
2026-01-10 18:58:43,907: t15.2023.11.17 val PER: 0.1337
2026-01-10 18:58:43,908: t15.2023.11.19 val PER: 0.1537
2026-01-10 18:58:43,910: t15.2023.11.26 val PER: 0.3377
2026-01-10 18:58:43,911: t15.2023.12.03 val PER: 0.2742
2026-01-10 18:58:43,913: t15.2023.12.08 val PER: 0.2643
2026-01-10 18:58:43,914: t15.2023.12.10 val PER: 0.2339
2026-01-10 18:58:43,916: t15.2023.12.17 val PER: 0.2703
2026-01-10 18:58:43,917: t15.2023.12.29 val PER: 0.2958
2026-01-10 18:58:43,920: t15.2024.02.25 val PER: 0.2444
2026-01-10 18:58:43,921: t15.2024.03.08 val PER: 0.3371
2026-01-10 18:58:43,923: t15.2024.03.15 val PER: 0.3189
2026-01-10 18:58:43,925: t15.2024.03.17 val PER: 0.2775
2026-01-10 18:58:43,927: t15.2024.05.10 val PER: 0.2942
2026-01-10 18:58:43,929: t15.2024.06.14 val PER: 0.2965
2026-01-10 18:58:43,930: t15.2024.07.19 val PER: 0.3467
2026-01-10 18:58:43,932: t15.2024.07.21 val PER: 0.2207
2026-01-10 18:58:43,933: t15.2024.07.28 val PER: 0.2919
2026-01-10 18:58:43,934: t15.2025.01.10 val PER: 0.3981
2026-01-10 18:58:43,936: t15.2025.01.12 val PER: 0.3072
2026-01-10 18:58:43,937: t15.2025.03.14 val PER: 0.4275
2026-01-10 18:58:43,939: t15.2025.03.16 val PER: 0.3220
2026-01-10 18:58:43,940: t15.2025.03.30 val PER: 0.3874
2026-01-10 18:58:43,941: t15.2025.04.13 val PER: 0.3295
2026-01-10 18:58:44,071: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_13500
2026-01-10 18:58:52,460: Train batch 13600: loss: 42.39 grad norm: 81.77 time: 0.067
2026-01-10 18:59:09,247: Train batch 13800: loss: 33.97 grad norm: 93.49 time: 0.061
2026-01-10 18:59:26,025: Train batch 14000: loss: 44.23 grad norm: 76.02 time: 0.056
2026-01-10 18:59:26,028: Running test after training batch: 14000
2026-01-10 18:59:26,134: WER debug GT example: You can see the code at this point as well.
2026-01-10 18:59:32,043: WER debug example
  GT : you can see the code at this point as well
  PR : i need a go at this point it's
2026-01-10 18:59:32,132: WER debug example
  GT : how does it keep the cost down
  PR : a s t e p a go at the end
2026-01-10 18:59:52,022: Val batch 14000: PER (avg): 0.2843 CTC Loss (avg): 46.8083 WER(5gram): 83.05% (n=256) time: 25.991
2026-01-10 18:59:52,024: WER lens: avg_true_words=5.99 avg_pred_words=5.80 max_pred_words=16
2026-01-10 18:59:52,027: t15.2023.08.13 val PER: 0.2620
2026-01-10 18:59:52,028: t15.2023.08.18 val PER: 0.2397
2026-01-10 18:59:52,030: t15.2023.08.20 val PER: 0.2216
2026-01-10 18:59:52,032: t15.2023.08.25 val PER: 0.2244
2026-01-10 18:59:52,034: t15.2023.08.27 val PER: 0.3248
2026-01-10 18:59:52,036: t15.2023.09.01 val PER: 0.2094
2026-01-10 18:59:52,037: t15.2023.09.03 val PER: 0.2755
2026-01-10 18:59:52,039: t15.2023.09.24 val PER: 0.2476
2026-01-10 18:59:52,041: t15.2023.09.29 val PER: 0.2470
2026-01-10 18:59:52,042: t15.2023.10.01 val PER: 0.3071
2026-01-10 18:59:52,044: t15.2023.10.06 val PER: 0.2153
2026-01-10 18:59:52,045: t15.2023.10.08 val PER: 0.3545
2026-01-10 18:59:52,047: t15.2023.10.13 val PER: 0.3848
2026-01-10 18:59:52,049: t15.2023.10.15 val PER: 0.2927
2026-01-10 18:59:52,050: t15.2023.10.20 val PER: 0.3221
2026-01-10 18:59:52,052: t15.2023.10.22 val PER: 0.2695
2026-01-10 18:59:52,053: t15.2023.11.03 val PER: 0.3094
2026-01-10 18:59:52,055: t15.2023.11.04 val PER: 0.0853
2026-01-10 18:59:52,057: t15.2023.11.17 val PER: 0.1306
2026-01-10 18:59:52,058: t15.2023.11.19 val PER: 0.1537
2026-01-10 18:59:52,060: t15.2023.11.26 val PER: 0.3348
2026-01-10 18:59:52,061: t15.2023.12.03 val PER: 0.2647
2026-01-10 18:59:52,063: t15.2023.12.08 val PER: 0.2663
2026-01-10 18:59:52,064: t15.2023.12.10 val PER: 0.2457
2026-01-10 18:59:52,066: t15.2023.12.17 val PER: 0.2557
2026-01-10 18:59:52,067: t15.2023.12.29 val PER: 0.2800
2026-01-10 18:59:52,069: t15.2024.02.25 val PER: 0.2472
2026-01-10 18:59:52,071: t15.2024.03.08 val PER: 0.3528
2026-01-10 18:59:52,072: t15.2024.03.15 val PER: 0.3227
2026-01-10 18:59:52,074: t15.2024.03.17 val PER: 0.2803
2026-01-10 18:59:52,076: t15.2024.05.10 val PER: 0.2942
2026-01-10 18:59:52,077: t15.2024.06.14 val PER: 0.2808
2026-01-10 18:59:52,079: t15.2024.07.19 val PER: 0.3296
2026-01-10 18:59:52,080: t15.2024.07.21 val PER: 0.2200
2026-01-10 18:59:52,082: t15.2024.07.28 val PER: 0.2926
2026-01-10 18:59:52,083: t15.2025.01.10 val PER: 0.3815
2026-01-10 18:59:52,085: t15.2025.01.12 val PER: 0.3010
2026-01-10 18:59:52,086: t15.2025.03.14 val PER: 0.4186
2026-01-10 18:59:52,088: t15.2025.03.16 val PER: 0.3298
2026-01-10 18:59:52,089: t15.2025.03.30 val PER: 0.3897
2026-01-10 18:59:52,091: t15.2025.04.13 val PER: 0.3409
2026-01-10 18:59:52,227: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_14000
2026-01-10 19:00:08,836: Train batch 14200: loss: 35.44 grad norm: 68.76 time: 0.061
2026-01-10 19:00:25,527: Train batch 14400: loss: 27.15 grad norm: 51.00 time: 0.068
2026-01-10 19:00:33,906: Running test after training batch: 14500
2026-01-10 19:00:34,006: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:00:39,906: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point is we
2026-01-10 19:00:40,012: WER debug example
  GT : how does it keep the cost down
  PR : just to be a cost and
2026-01-10 19:00:59,393: Val batch 14500: PER (avg): 0.2789 CTC Loss (avg): 46.4219 WER(5gram): 73.66% (n=256) time: 25.485
2026-01-10 19:00:59,396: WER lens: avg_true_words=5.99 avg_pred_words=5.59 max_pred_words=15
2026-01-10 19:00:59,398: t15.2023.08.13 val PER: 0.2588
2026-01-10 19:00:59,400: t15.2023.08.18 val PER: 0.2322
2026-01-10 19:00:59,402: t15.2023.08.20 val PER: 0.2200
2026-01-10 19:00:59,404: t15.2023.08.25 val PER: 0.2259
2026-01-10 19:00:59,406: t15.2023.08.27 val PER: 0.3248
2026-01-10 19:00:59,409: t15.2023.09.01 val PER: 0.2029
2026-01-10 19:00:59,412: t15.2023.09.03 val PER: 0.2708
2026-01-10 19:00:59,414: t15.2023.09.24 val PER: 0.2633
2026-01-10 19:00:59,415: t15.2023.09.29 val PER: 0.2406
2026-01-10 19:00:59,417: t15.2023.10.01 val PER: 0.3032
2026-01-10 19:00:59,419: t15.2023.10.06 val PER: 0.2142
2026-01-10 19:00:59,421: t15.2023.10.08 val PER: 0.3735
2026-01-10 19:00:59,422: t15.2023.10.13 val PER: 0.3716
2026-01-10 19:00:59,424: t15.2023.10.15 val PER: 0.2973
2026-01-10 19:00:59,425: t15.2023.10.20 val PER: 0.3389
2026-01-10 19:00:59,427: t15.2023.10.22 val PER: 0.2628
2026-01-10 19:00:59,429: t15.2023.11.03 val PER: 0.2938
2026-01-10 19:00:59,430: t15.2023.11.04 val PER: 0.0853
2026-01-10 19:00:59,432: t15.2023.11.17 val PER: 0.1353
2026-01-10 19:00:59,433: t15.2023.11.19 val PER: 0.1377
2026-01-10 19:00:59,435: t15.2023.11.26 val PER: 0.3304
2026-01-10 19:00:59,437: t15.2023.12.03 val PER: 0.2532
2026-01-10 19:00:59,438: t15.2023.12.08 val PER: 0.2483
2026-01-10 19:00:59,440: t15.2023.12.10 val PER: 0.2273
2026-01-10 19:00:59,441: t15.2023.12.17 val PER: 0.2516
2026-01-10 19:00:59,443: t15.2023.12.29 val PER: 0.2697
2026-01-10 19:00:59,444: t15.2024.02.25 val PER: 0.2430
2026-01-10 19:00:59,446: t15.2024.03.08 val PER: 0.3385
2026-01-10 19:00:59,447: t15.2024.03.15 val PER: 0.3171
2026-01-10 19:00:59,449: t15.2024.03.17 val PER: 0.2762
2026-01-10 19:00:59,450: t15.2024.05.10 val PER: 0.2883
2026-01-10 19:00:59,452: t15.2024.06.14 val PER: 0.2729
2026-01-10 19:00:59,453: t15.2024.07.19 val PER: 0.3362
2026-01-10 19:00:59,454: t15.2024.07.21 val PER: 0.2207
2026-01-10 19:00:59,456: t15.2024.07.28 val PER: 0.2824
2026-01-10 19:00:59,457: t15.2025.01.10 val PER: 0.3788
2026-01-10 19:00:59,459: t15.2025.01.12 val PER: 0.2848
2026-01-10 19:00:59,460: t15.2025.03.14 val PER: 0.4157
2026-01-10 19:00:59,462: t15.2025.03.16 val PER: 0.3246
2026-01-10 19:00:59,463: t15.2025.03.30 val PER: 0.3770
2026-01-10 19:00:59,465: t15.2025.04.13 val PER: 0.3267
2026-01-10 19:00:59,466: New best val WER(5gram) 75.03% --> 73.66%
2026-01-10 19:01:00,585: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_14500
2026-01-10 19:01:08,911: Train batch 14600: loss: 45.07 grad norm: 92.02 time: 0.064
2026-01-10 19:01:26,120: Train batch 14800: loss: 26.89 grad norm: 62.89 time: 0.055
2026-01-10 19:01:43,009: Train batch 15000: loss: 30.63 grad norm: 59.73 time: 0.057
2026-01-10 19:01:43,011: Running test after training batch: 15000
2026-01-10 19:01:43,120: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:01:49,031: WER debug example
  GT : you can see the code at this point as well
  PR : k and a good at this point so we
2026-01-10 19:01:49,124: WER debug example
  GT : how does it keep the cost down
  PR : the us to keep a count and
2026-01-10 19:02:08,431: Val batch 15000: PER (avg): 0.2756 CTC Loss (avg): 45.2652 WER(5gram): 76.66% (n=256) time: 25.417
2026-01-10 19:02:08,433: WER lens: avg_true_words=5.99 avg_pred_words=5.69 max_pred_words=15
2026-01-10 19:02:08,436: t15.2023.08.13 val PER: 0.2568
2026-01-10 19:02:08,437: t15.2023.08.18 val PER: 0.2330
2026-01-10 19:02:08,439: t15.2023.08.20 val PER: 0.2160
2026-01-10 19:02:08,441: t15.2023.08.25 val PER: 0.2199
2026-01-10 19:02:08,443: t15.2023.08.27 val PER: 0.3215
2026-01-10 19:02:08,444: t15.2023.09.01 val PER: 0.1964
2026-01-10 19:02:08,446: t15.2023.09.03 val PER: 0.2660
2026-01-10 19:02:08,448: t15.2023.09.24 val PER: 0.2524
2026-01-10 19:02:08,449: t15.2023.09.29 val PER: 0.2393
2026-01-10 19:02:08,451: t15.2023.10.01 val PER: 0.2966
2026-01-10 19:02:08,453: t15.2023.10.06 val PER: 0.2099
2026-01-10 19:02:08,454: t15.2023.10.08 val PER: 0.3518
2026-01-10 19:02:08,456: t15.2023.10.13 val PER: 0.3724
2026-01-10 19:02:08,458: t15.2023.10.15 val PER: 0.2914
2026-01-10 19:02:08,459: t15.2023.10.20 val PER: 0.3389
2026-01-10 19:02:08,461: t15.2023.10.22 val PER: 0.2639
2026-01-10 19:02:08,463: t15.2023.11.03 val PER: 0.3019
2026-01-10 19:02:08,465: t15.2023.11.04 val PER: 0.0887
2026-01-10 19:02:08,467: t15.2023.11.17 val PER: 0.1182
2026-01-10 19:02:08,469: t15.2023.11.19 val PER: 0.1437
2026-01-10 19:02:08,470: t15.2023.11.26 val PER: 0.3239
2026-01-10 19:02:08,472: t15.2023.12.03 val PER: 0.2489
2026-01-10 19:02:08,474: t15.2023.12.08 val PER: 0.2410
2026-01-10 19:02:08,475: t15.2023.12.10 val PER: 0.2300
2026-01-10 19:02:08,477: t15.2023.12.17 val PER: 0.2568
2026-01-10 19:02:08,479: t15.2023.12.29 val PER: 0.2711
2026-01-10 19:02:08,481: t15.2024.02.25 val PER: 0.2430
2026-01-10 19:02:08,483: t15.2024.03.08 val PER: 0.3215
2026-01-10 19:02:08,484: t15.2024.03.15 val PER: 0.3071
2026-01-10 19:02:08,486: t15.2024.03.17 val PER: 0.2692
2026-01-10 19:02:08,487: t15.2024.05.10 val PER: 0.2897
2026-01-10 19:02:08,489: t15.2024.06.14 val PER: 0.2855
2026-01-10 19:02:08,491: t15.2024.07.19 val PER: 0.3336
2026-01-10 19:02:08,493: t15.2024.07.21 val PER: 0.2131
2026-01-10 19:02:08,494: t15.2024.07.28 val PER: 0.2735
2026-01-10 19:02:08,496: t15.2025.01.10 val PER: 0.3871
2026-01-10 19:02:08,497: t15.2025.01.12 val PER: 0.2848
2026-01-10 19:02:08,499: t15.2025.03.14 val PER: 0.4305
2026-01-10 19:02:08,500: t15.2025.03.16 val PER: 0.3076
2026-01-10 19:02:08,502: t15.2025.03.30 val PER: 0.3782
2026-01-10 19:02:08,503: t15.2025.04.13 val PER: 0.3224
2026-01-10 19:02:08,640: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_15000
2026-01-10 19:02:25,875: Train batch 15200: loss: 24.76 grad norm: 76.09 time: 0.061
2026-01-10 19:02:42,311: Train batch 15400: loss: 37.38 grad norm: 76.21 time: 0.054
2026-01-10 19:02:50,710: Running test after training batch: 15500
2026-01-10 19:02:50,853: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:02:56,960: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point it's why
2026-01-10 19:02:57,036: WER debug example
  GT : how does it keep the cost down
  PR : the us to keep a guy at the end
2026-01-10 19:03:15,994: Val batch 15500: PER (avg): 0.2674 CTC Loss (avg): 44.5641 WER(5gram): 76.53% (n=256) time: 25.280
2026-01-10 19:03:15,996: WER lens: avg_true_words=5.99 avg_pred_words=5.82 max_pred_words=14
2026-01-10 19:03:15,999: t15.2023.08.13 val PER: 0.2443
2026-01-10 19:03:16,001: t15.2023.08.18 val PER: 0.2297
2026-01-10 19:03:16,004: t15.2023.08.20 val PER: 0.2057
2026-01-10 19:03:16,006: t15.2023.08.25 val PER: 0.2093
2026-01-10 19:03:16,008: t15.2023.08.27 val PER: 0.3280
2026-01-10 19:03:16,010: t15.2023.09.01 val PER: 0.1916
2026-01-10 19:03:16,011: t15.2023.09.03 val PER: 0.2601
2026-01-10 19:03:16,013: t15.2023.09.24 val PER: 0.2318
2026-01-10 19:03:16,014: t15.2023.09.29 val PER: 0.2227
2026-01-10 19:03:16,016: t15.2023.10.01 val PER: 0.2827
2026-01-10 19:03:16,018: t15.2023.10.06 val PER: 0.2045
2026-01-10 19:03:16,019: t15.2023.10.08 val PER: 0.3505
2026-01-10 19:03:16,021: t15.2023.10.13 val PER: 0.3600
2026-01-10 19:03:16,022: t15.2023.10.15 val PER: 0.2762
2026-01-10 19:03:16,024: t15.2023.10.20 val PER: 0.3154
2026-01-10 19:03:16,025: t15.2023.10.22 val PER: 0.2628
2026-01-10 19:03:16,027: t15.2023.11.03 val PER: 0.2965
2026-01-10 19:03:16,028: t15.2023.11.04 val PER: 0.0819
2026-01-10 19:03:16,029: t15.2023.11.17 val PER: 0.1151
2026-01-10 19:03:16,031: t15.2023.11.19 val PER: 0.1457
2026-01-10 19:03:16,032: t15.2023.11.26 val PER: 0.3014
2026-01-10 19:03:16,034: t15.2023.12.03 val PER: 0.2405
2026-01-10 19:03:16,035: t15.2023.12.08 val PER: 0.2357
2026-01-10 19:03:16,037: t15.2023.12.10 val PER: 0.2194
2026-01-10 19:03:16,038: t15.2023.12.17 val PER: 0.2557
2026-01-10 19:03:16,039: t15.2023.12.29 val PER: 0.2636
2026-01-10 19:03:16,041: t15.2024.02.25 val PER: 0.2261
2026-01-10 19:03:16,042: t15.2024.03.08 val PER: 0.3257
2026-01-10 19:03:16,044: t15.2024.03.15 val PER: 0.2989
2026-01-10 19:03:16,045: t15.2024.03.17 val PER: 0.2650
2026-01-10 19:03:16,046: t15.2024.05.10 val PER: 0.2808
2026-01-10 19:03:16,048: t15.2024.06.14 val PER: 0.2681
2026-01-10 19:03:16,049: t15.2024.07.19 val PER: 0.3237
2026-01-10 19:03:16,051: t15.2024.07.21 val PER: 0.2055
2026-01-10 19:03:16,052: t15.2024.07.28 val PER: 0.2684
2026-01-10 19:03:16,053: t15.2025.01.10 val PER: 0.3843
2026-01-10 19:03:16,055: t15.2025.01.12 val PER: 0.2810
2026-01-10 19:03:16,056: t15.2025.03.14 val PER: 0.4201
2026-01-10 19:03:16,058: t15.2025.03.16 val PER: 0.3063
2026-01-10 19:03:16,059: t15.2025.03.30 val PER: 0.3805
2026-01-10 19:03:16,060: t15.2025.04.13 val PER: 0.3039
2026-01-10 19:03:16,195: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_15500
2026-01-10 19:03:24,518: Train batch 15600: loss: 43.95 grad norm: 98.16 time: 0.067
2026-01-10 19:03:41,367: Train batch 15800: loss: 48.56 grad norm: 98.21 time: 0.072
2026-01-10 19:03:58,207: Train batch 16000: loss: 31.03 grad norm: 97.36 time: 0.060
2026-01-10 19:03:58,209: Running test after training batch: 16000
2026-01-10 19:03:58,330: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:04:04,207: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point it's why
2026-01-10 19:04:04,293: WER debug example
  GT : how does it keep the cost down
  PR : the us to go to a guy at the end
2026-01-10 19:04:23,192: Val batch 16000: PER (avg): 0.2648 CTC Loss (avg): 44.0674 WER(5gram): 80.77% (n=256) time: 24.980
2026-01-10 19:04:23,194: WER lens: avg_true_words=5.99 avg_pred_words=5.89 max_pred_words=14
2026-01-10 19:04:23,196: t15.2023.08.13 val PER: 0.2339
2026-01-10 19:04:23,198: t15.2023.08.18 val PER: 0.2246
2026-01-10 19:04:23,200: t15.2023.08.20 val PER: 0.2033
2026-01-10 19:04:23,201: t15.2023.08.25 val PER: 0.2093
2026-01-10 19:04:23,203: t15.2023.08.27 val PER: 0.3360
2026-01-10 19:04:23,204: t15.2023.09.01 val PER: 0.1932
2026-01-10 19:04:23,206: t15.2023.09.03 val PER: 0.2577
2026-01-10 19:04:23,207: t15.2023.09.24 val PER: 0.2427
2026-01-10 19:04:23,209: t15.2023.09.29 val PER: 0.2246
2026-01-10 19:04:23,211: t15.2023.10.01 val PER: 0.2787
2026-01-10 19:04:23,212: t15.2023.10.06 val PER: 0.2045
2026-01-10 19:04:23,214: t15.2023.10.08 val PER: 0.3505
2026-01-10 19:04:23,216: t15.2023.10.13 val PER: 0.3569
2026-01-10 19:04:23,217: t15.2023.10.15 val PER: 0.2815
2026-01-10 19:04:23,219: t15.2023.10.20 val PER: 0.3221
2026-01-10 19:04:23,221: t15.2023.10.22 val PER: 0.2506
2026-01-10 19:04:23,222: t15.2023.11.03 val PER: 0.2849
2026-01-10 19:04:23,224: t15.2023.11.04 val PER: 0.0785
2026-01-10 19:04:23,226: t15.2023.11.17 val PER: 0.1275
2026-01-10 19:04:23,227: t15.2023.11.19 val PER: 0.1437
2026-01-10 19:04:23,230: t15.2023.11.26 val PER: 0.3007
2026-01-10 19:04:23,232: t15.2023.12.03 val PER: 0.2353
2026-01-10 19:04:23,233: t15.2023.12.08 val PER: 0.2277
2026-01-10 19:04:23,235: t15.2023.12.10 val PER: 0.2378
2026-01-10 19:04:23,236: t15.2023.12.17 val PER: 0.2380
2026-01-10 19:04:23,237: t15.2023.12.29 val PER: 0.2567
2026-01-10 19:04:23,239: t15.2024.02.25 val PER: 0.2205
2026-01-10 19:04:23,240: t15.2024.03.08 val PER: 0.3201
2026-01-10 19:04:23,242: t15.2024.03.15 val PER: 0.3008
2026-01-10 19:04:23,243: t15.2024.03.17 val PER: 0.2538
2026-01-10 19:04:23,245: t15.2024.05.10 val PER: 0.2779
2026-01-10 19:04:23,246: t15.2024.06.14 val PER: 0.2729
2026-01-10 19:04:23,248: t15.2024.07.19 val PER: 0.3184
2026-01-10 19:04:23,249: t15.2024.07.21 val PER: 0.2041
2026-01-10 19:04:23,250: t15.2024.07.28 val PER: 0.2647
2026-01-10 19:04:23,254: t15.2025.01.10 val PER: 0.3747
2026-01-10 19:04:23,256: t15.2025.01.12 val PER: 0.2794
2026-01-10 19:04:23,258: t15.2025.03.14 val PER: 0.4024
2026-01-10 19:04:23,259: t15.2025.03.16 val PER: 0.3024
2026-01-10 19:04:23,261: t15.2025.03.30 val PER: 0.3759
2026-01-10 19:04:23,262: t15.2025.04.13 val PER: 0.3181
2026-01-10 19:04:23,396: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_16000
2026-01-10 19:04:40,078: Train batch 16200: loss: 27.69 grad norm: 65.54 time: 0.060
2026-01-10 19:04:56,865: Train batch 16400: loss: 35.44 grad norm: 116.90 time: 0.063
2026-01-10 19:05:05,235: Running test after training batch: 16500
2026-01-10 19:05:05,343: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:05:11,353: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point a way
2026-01-10 19:05:11,430: WER debug example
  GT : how does it keep the cost down
  PR : just to keep a guy at the end
2026-01-10 19:05:29,905: Val batch 16500: PER (avg): 0.2621 CTC Loss (avg): 43.3499 WER(5gram): 70.08% (n=256) time: 24.667
2026-01-10 19:05:29,907: WER lens: avg_true_words=5.99 avg_pred_words=5.82 max_pred_words=13
2026-01-10 19:05:29,910: t15.2023.08.13 val PER: 0.2277
2026-01-10 19:05:29,912: t15.2023.08.18 val PER: 0.2171
2026-01-10 19:05:29,913: t15.2023.08.20 val PER: 0.2049
2026-01-10 19:05:29,915: t15.2023.08.25 val PER: 0.2078
2026-01-10 19:05:29,917: t15.2023.08.27 val PER: 0.3055
2026-01-10 19:05:29,919: t15.2023.09.01 val PER: 0.1956
2026-01-10 19:05:29,920: t15.2023.09.03 val PER: 0.2589
2026-01-10 19:05:29,922: t15.2023.09.24 val PER: 0.2379
2026-01-10 19:05:29,924: t15.2023.09.29 val PER: 0.2208
2026-01-10 19:05:29,925: t15.2023.10.01 val PER: 0.2787
2026-01-10 19:05:29,927: t15.2023.10.06 val PER: 0.2045
2026-01-10 19:05:29,929: t15.2023.10.08 val PER: 0.3410
2026-01-10 19:05:29,930: t15.2023.10.13 val PER: 0.3507
2026-01-10 19:05:29,932: t15.2023.10.15 val PER: 0.2755
2026-01-10 19:05:29,934: t15.2023.10.20 val PER: 0.3188
2026-01-10 19:05:29,936: t15.2023.10.22 val PER: 0.2539
2026-01-10 19:05:29,937: t15.2023.11.03 val PER: 0.2910
2026-01-10 19:05:29,939: t15.2023.11.04 val PER: 0.0785
2026-01-10 19:05:29,941: t15.2023.11.17 val PER: 0.1213
2026-01-10 19:05:29,942: t15.2023.11.19 val PER: 0.1397
2026-01-10 19:05:29,944: t15.2023.11.26 val PER: 0.3036
2026-01-10 19:05:29,945: t15.2023.12.03 val PER: 0.2164
2026-01-10 19:05:29,947: t15.2023.12.08 val PER: 0.2250
2026-01-10 19:05:29,948: t15.2023.12.10 val PER: 0.2142
2026-01-10 19:05:29,950: t15.2023.12.17 val PER: 0.2412
2026-01-10 19:05:29,951: t15.2023.12.29 val PER: 0.2567
2026-01-10 19:05:29,953: t15.2024.02.25 val PER: 0.2107
2026-01-10 19:05:29,954: t15.2024.03.08 val PER: 0.3201
2026-01-10 19:05:29,956: t15.2024.03.15 val PER: 0.2996
2026-01-10 19:05:29,957: t15.2024.03.17 val PER: 0.2573
2026-01-10 19:05:29,959: t15.2024.05.10 val PER: 0.2779
2026-01-10 19:05:29,960: t15.2024.06.14 val PER: 0.2587
2026-01-10 19:05:29,962: t15.2024.07.19 val PER: 0.3144
2026-01-10 19:05:29,964: t15.2024.07.21 val PER: 0.2110
2026-01-10 19:05:29,966: t15.2024.07.28 val PER: 0.2566
2026-01-10 19:05:29,967: t15.2025.01.10 val PER: 0.3774
2026-01-10 19:05:29,969: t15.2025.01.12 val PER: 0.2802
2026-01-10 19:05:29,970: t15.2025.03.14 val PER: 0.4068
2026-01-10 19:05:29,971: t15.2025.03.16 val PER: 0.2997
2026-01-10 19:05:29,973: t15.2025.03.30 val PER: 0.3701
2026-01-10 19:05:29,974: t15.2025.04.13 val PER: 0.3210
2026-01-10 19:05:29,976: New best val WER(5gram) 73.66% --> 70.08%
2026-01-10 19:05:31,090: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_16500
2026-01-10 19:05:39,463: Train batch 16600: loss: 32.31 grad norm: 70.89 time: 0.057
2026-01-10 19:05:56,253: Train batch 16800: loss: 46.35 grad norm: 94.04 time: 0.067
2026-01-10 19:06:12,974: Train batch 17000: loss: 27.88 grad norm: 63.93 time: 0.088
2026-01-10 19:06:12,976: Running test after training batch: 17000
2026-01-10 19:06:13,079: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:06:18,927: WER debug example
  GT : you can see the code at this point as well
  PR : ke no e a go at this point as we
2026-01-10 19:06:19,016: WER debug example
  GT : how does it keep the cost down
  PR : a us it may be a guy at the
2026-01-10 19:06:36,589: Val batch 17000: PER (avg): 0.2581 CTC Loss (avg): 43.0240 WER(5gram): 78.10% (n=256) time: 23.610
2026-01-10 19:06:36,591: WER lens: avg_true_words=5.99 avg_pred_words=6.06 max_pred_words=14
2026-01-10 19:06:36,593: t15.2023.08.13 val PER: 0.2349
2026-01-10 19:06:36,595: t15.2023.08.18 val PER: 0.2163
2026-01-10 19:06:36,596: t15.2023.08.20 val PER: 0.1986
2026-01-10 19:06:36,598: t15.2023.08.25 val PER: 0.2063
2026-01-10 19:06:36,599: t15.2023.08.27 val PER: 0.3006
2026-01-10 19:06:36,601: t15.2023.09.01 val PER: 0.1924
2026-01-10 19:06:36,603: t15.2023.09.03 val PER: 0.2542
2026-01-10 19:06:36,604: t15.2023.09.24 val PER: 0.2354
2026-01-10 19:06:36,606: t15.2023.09.29 val PER: 0.2151
2026-01-10 19:06:36,607: t15.2023.10.01 val PER: 0.2794
2026-01-10 19:06:36,609: t15.2023.10.06 val PER: 0.1884
2026-01-10 19:06:36,610: t15.2023.10.08 val PER: 0.3424
2026-01-10 19:06:36,612: t15.2023.10.13 val PER: 0.3437
2026-01-10 19:06:36,613: t15.2023.10.15 val PER: 0.2690
2026-01-10 19:06:36,614: t15.2023.10.20 val PER: 0.3121
2026-01-10 19:06:36,616: t15.2023.10.22 val PER: 0.2572
2026-01-10 19:06:36,617: t15.2023.11.03 val PER: 0.2843
2026-01-10 19:06:36,619: t15.2023.11.04 val PER: 0.0751
2026-01-10 19:06:36,620: t15.2023.11.17 val PER: 0.1244
2026-01-10 19:06:36,622: t15.2023.11.19 val PER: 0.1337
2026-01-10 19:06:36,624: t15.2023.11.26 val PER: 0.2913
2026-01-10 19:06:36,626: t15.2023.12.03 val PER: 0.2258
2026-01-10 19:06:36,627: t15.2023.12.08 val PER: 0.2157
2026-01-10 19:06:36,630: t15.2023.12.10 val PER: 0.2116
2026-01-10 19:06:36,631: t15.2023.12.17 val PER: 0.2349
2026-01-10 19:06:36,633: t15.2023.12.29 val PER: 0.2546
2026-01-10 19:06:36,634: t15.2024.02.25 val PER: 0.2205
2026-01-10 19:06:36,635: t15.2024.03.08 val PER: 0.3129
2026-01-10 19:06:36,637: t15.2024.03.15 val PER: 0.3089
2026-01-10 19:06:36,638: t15.2024.03.17 val PER: 0.2455
2026-01-10 19:06:36,639: t15.2024.05.10 val PER: 0.2719
2026-01-10 19:06:36,641: t15.2024.06.14 val PER: 0.2618
2026-01-10 19:06:36,642: t15.2024.07.19 val PER: 0.3111
2026-01-10 19:06:36,643: t15.2024.07.21 val PER: 0.1945
2026-01-10 19:06:36,645: t15.2024.07.28 val PER: 0.2603
2026-01-10 19:06:36,646: t15.2025.01.10 val PER: 0.3802
2026-01-10 19:06:36,648: t15.2025.01.12 val PER: 0.2717
2026-01-10 19:06:36,649: t15.2025.03.14 val PER: 0.4112
2026-01-10 19:06:36,650: t15.2025.03.16 val PER: 0.2893
2026-01-10 19:06:36,652: t15.2025.03.30 val PER: 0.3575
2026-01-10 19:06:36,653: t15.2025.04.13 val PER: 0.2996
2026-01-10 19:06:36,787: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_17000
2026-01-10 19:06:53,948: Train batch 17200: loss: 35.60 grad norm: 81.08 time: 0.088
2026-01-10 19:07:10,771: Train batch 17400: loss: 38.66 grad norm: 81.82 time: 0.078
2026-01-10 19:07:19,005: Running test after training batch: 17500
2026-01-10 19:07:19,129: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:07:25,057: WER debug example
  GT : you can see the code at this point as well
  PR : we need a go at this point is why
2026-01-10 19:07:25,134: WER debug example
  GT : how does it keep the cost down
  PR : just to keep a guy at the end
2026-01-10 19:07:42,984: Val batch 17500: PER (avg): 0.2545 CTC Loss (avg): 42.2502 WER(5gram): 74.12% (n=256) time: 23.977
2026-01-10 19:07:42,987: WER lens: avg_true_words=5.99 avg_pred_words=5.91 max_pred_words=14
2026-01-10 19:07:42,989: t15.2023.08.13 val PER: 0.2360
2026-01-10 19:07:42,991: t15.2023.08.18 val PER: 0.2146
2026-01-10 19:07:42,993: t15.2023.08.20 val PER: 0.1994
2026-01-10 19:07:42,995: t15.2023.08.25 val PER: 0.2033
2026-01-10 19:07:42,996: t15.2023.08.27 val PER: 0.3103
2026-01-10 19:07:42,998: t15.2023.09.01 val PER: 0.1843
2026-01-10 19:07:43,000: t15.2023.09.03 val PER: 0.2530
2026-01-10 19:07:43,002: t15.2023.09.24 val PER: 0.2233
2026-01-10 19:07:43,004: t15.2023.09.29 val PER: 0.2138
2026-01-10 19:07:43,006: t15.2023.10.01 val PER: 0.2741
2026-01-10 19:07:43,008: t15.2023.10.06 val PER: 0.1905
2026-01-10 19:07:43,010: t15.2023.10.08 val PER: 0.3356
2026-01-10 19:07:43,012: t15.2023.10.13 val PER: 0.3460
2026-01-10 19:07:43,014: t15.2023.10.15 val PER: 0.2690
2026-01-10 19:07:43,016: t15.2023.10.20 val PER: 0.3020
2026-01-10 19:07:43,018: t15.2023.10.22 val PER: 0.2517
2026-01-10 19:07:43,020: t15.2023.11.03 val PER: 0.2768
2026-01-10 19:07:43,022: t15.2023.11.04 val PER: 0.0819
2026-01-10 19:07:43,024: t15.2023.11.17 val PER: 0.1104
2026-01-10 19:07:43,025: t15.2023.11.19 val PER: 0.1337
2026-01-10 19:07:43,027: t15.2023.11.26 val PER: 0.2848
2026-01-10 19:07:43,028: t15.2023.12.03 val PER: 0.2237
2026-01-10 19:07:43,030: t15.2023.12.08 val PER: 0.2124
2026-01-10 19:07:43,032: t15.2023.12.10 val PER: 0.2116
2026-01-10 19:07:43,033: t15.2023.12.17 val PER: 0.2235
2026-01-10 19:07:43,035: t15.2023.12.29 val PER: 0.2478
2026-01-10 19:07:43,036: t15.2024.02.25 val PER: 0.2149
2026-01-10 19:07:43,038: t15.2024.03.08 val PER: 0.3058
2026-01-10 19:07:43,039: t15.2024.03.15 val PER: 0.3021
2026-01-10 19:07:43,041: t15.2024.03.17 val PER: 0.2420
2026-01-10 19:07:43,042: t15.2024.05.10 val PER: 0.2541
2026-01-10 19:07:43,044: t15.2024.06.14 val PER: 0.2587
2026-01-10 19:07:43,046: t15.2024.07.19 val PER: 0.3131
2026-01-10 19:07:43,047: t15.2024.07.21 val PER: 0.1903
2026-01-10 19:07:43,049: t15.2024.07.28 val PER: 0.2522
2026-01-10 19:07:43,050: t15.2025.01.10 val PER: 0.3609
2026-01-10 19:07:43,052: t15.2025.01.12 val PER: 0.2725
2026-01-10 19:07:43,053: t15.2025.03.14 val PER: 0.4038
2026-01-10 19:07:43,055: t15.2025.03.16 val PER: 0.2945
2026-01-10 19:07:43,056: t15.2025.03.30 val PER: 0.3678
2026-01-10 19:07:43,058: t15.2025.04.13 val PER: 0.2981
2026-01-10 19:07:43,193: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_17500
2026-01-10 19:07:51,386: Train batch 17600: loss: 38.15 grad norm: 95.34 time: 0.056
2026-01-10 19:08:08,180: Train batch 17800: loss: 23.11 grad norm: 70.56 time: 0.045
2026-01-10 19:08:25,094: Train batch 18000: loss: 31.44 grad norm: 82.72 time: 0.067
2026-01-10 19:08:25,097: Running test after training batch: 18000
2026-01-10 19:08:25,208: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:08:31,090: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point it's
2026-01-10 19:08:31,149: WER debug example
  GT : how does it keep the cost down
  PR : just to keep a guy at the end
2026-01-10 19:08:49,156: Val batch 18000: PER (avg): 0.2490 CTC Loss (avg): 41.8343 WER(5gram): 69.56% (n=256) time: 24.056
2026-01-10 19:08:49,158: WER lens: avg_true_words=5.99 avg_pred_words=6.06 max_pred_words=15
2026-01-10 19:08:49,161: t15.2023.08.13 val PER: 0.2256
2026-01-10 19:08:49,163: t15.2023.08.18 val PER: 0.2087
2026-01-10 19:08:49,165: t15.2023.08.20 val PER: 0.1867
2026-01-10 19:08:49,167: t15.2023.08.25 val PER: 0.2048
2026-01-10 19:08:49,169: t15.2023.08.27 val PER: 0.2942
2026-01-10 19:08:49,171: t15.2023.09.01 val PER: 0.1867
2026-01-10 19:08:49,172: t15.2023.09.03 val PER: 0.2435
2026-01-10 19:08:49,174: t15.2023.09.24 val PER: 0.2221
2026-01-10 19:08:49,176: t15.2023.09.29 val PER: 0.2055
2026-01-10 19:08:49,178: t15.2023.10.01 val PER: 0.2642
2026-01-10 19:08:49,179: t15.2023.10.06 val PER: 0.1895
2026-01-10 19:08:49,181: t15.2023.10.08 val PER: 0.3383
2026-01-10 19:08:49,183: t15.2023.10.13 val PER: 0.3289
2026-01-10 19:08:49,184: t15.2023.10.15 val PER: 0.2604
2026-01-10 19:08:49,186: t15.2023.10.20 val PER: 0.3188
2026-01-10 19:08:49,188: t15.2023.10.22 val PER: 0.2450
2026-01-10 19:08:49,191: t15.2023.11.03 val PER: 0.2727
2026-01-10 19:08:49,192: t15.2023.11.04 val PER: 0.0785
2026-01-10 19:08:49,194: t15.2023.11.17 val PER: 0.1166
2026-01-10 19:08:49,195: t15.2023.11.19 val PER: 0.1238
2026-01-10 19:08:49,197: t15.2023.11.26 val PER: 0.2819
2026-01-10 19:08:49,198: t15.2023.12.03 val PER: 0.2269
2026-01-10 19:08:49,200: t15.2023.12.08 val PER: 0.2071
2026-01-10 19:08:49,201: t15.2023.12.10 val PER: 0.2063
2026-01-10 19:08:49,203: t15.2023.12.17 val PER: 0.2297
2026-01-10 19:08:49,205: t15.2023.12.29 val PER: 0.2471
2026-01-10 19:08:49,206: t15.2024.02.25 val PER: 0.2051
2026-01-10 19:08:49,208: t15.2024.03.08 val PER: 0.3087
2026-01-10 19:08:49,210: t15.2024.03.15 val PER: 0.2827
2026-01-10 19:08:49,211: t15.2024.03.17 val PER: 0.2315
2026-01-10 19:08:49,213: t15.2024.05.10 val PER: 0.2511
2026-01-10 19:08:49,215: t15.2024.06.14 val PER: 0.2429
2026-01-10 19:08:49,216: t15.2024.07.19 val PER: 0.3098
2026-01-10 19:08:49,218: t15.2024.07.21 val PER: 0.1972
2026-01-10 19:08:49,219: t15.2024.07.28 val PER: 0.2456
2026-01-10 19:08:49,221: t15.2025.01.10 val PER: 0.3554
2026-01-10 19:08:49,222: t15.2025.01.12 val PER: 0.2687
2026-01-10 19:08:49,224: t15.2025.03.14 val PER: 0.3964
2026-01-10 19:08:49,225: t15.2025.03.16 val PER: 0.2814
2026-01-10 19:08:49,226: t15.2025.03.30 val PER: 0.3575
2026-01-10 19:08:49,228: t15.2025.04.13 val PER: 0.2967
2026-01-10 19:08:49,230: New best val WER(5gram) 70.08% --> 69.56%
2026-01-10 19:08:50,661: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_18000
2026-01-10 19:09:08,017: Train batch 18200: loss: 32.80 grad norm: 60.78 time: 0.079
2026-01-10 19:09:24,916: Train batch 18400: loss: 21.41 grad norm: 119.82 time: 0.062
2026-01-10 19:09:33,238: Running test after training batch: 18500
2026-01-10 19:09:33,365: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:09:39,538: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point it's why
2026-01-10 19:09:39,605: WER debug example
  GT : how does it keep the cost down
  PR : just to keep the cost and
2026-01-10 19:09:56,846: Val batch 18500: PER (avg): 0.2477 CTC Loss (avg): 41.2982 WER(5gram): 67.54% (n=256) time: 23.605
2026-01-10 19:09:56,848: WER lens: avg_true_words=5.99 avg_pred_words=6.06 max_pred_words=14
2026-01-10 19:09:56,851: t15.2023.08.13 val PER: 0.2225
2026-01-10 19:09:56,852: t15.2023.08.18 val PER: 0.2020
2026-01-10 19:09:56,854: t15.2023.08.20 val PER: 0.1946
2026-01-10 19:09:56,856: t15.2023.08.25 val PER: 0.1958
2026-01-10 19:09:56,857: t15.2023.08.27 val PER: 0.3023
2026-01-10 19:09:56,858: t15.2023.09.01 val PER: 0.1843
2026-01-10 19:09:56,860: t15.2023.09.03 val PER: 0.2375
2026-01-10 19:09:56,862: t15.2023.09.24 val PER: 0.2197
2026-01-10 19:09:56,866: t15.2023.09.29 val PER: 0.2074
2026-01-10 19:09:56,868: t15.2023.10.01 val PER: 0.2695
2026-01-10 19:09:56,870: t15.2023.10.06 val PER: 0.1905
2026-01-10 19:09:56,871: t15.2023.10.08 val PER: 0.3329
2026-01-10 19:09:56,872: t15.2023.10.13 val PER: 0.3413
2026-01-10 19:09:56,874: t15.2023.10.15 val PER: 0.2630
2026-01-10 19:09:56,875: t15.2023.10.20 val PER: 0.3020
2026-01-10 19:09:56,877: t15.2023.10.22 val PER: 0.2550
2026-01-10 19:09:56,878: t15.2023.11.03 val PER: 0.2707
2026-01-10 19:09:56,880: t15.2023.11.04 val PER: 0.0751
2026-01-10 19:09:56,881: t15.2023.11.17 val PER: 0.1120
2026-01-10 19:09:56,882: t15.2023.11.19 val PER: 0.1178
2026-01-10 19:09:56,884: t15.2023.11.26 val PER: 0.2790
2026-01-10 19:09:56,885: t15.2023.12.03 val PER: 0.2101
2026-01-10 19:09:56,887: t15.2023.12.08 val PER: 0.2037
2026-01-10 19:09:56,888: t15.2023.12.10 val PER: 0.1997
2026-01-10 19:09:56,890: t15.2023.12.17 val PER: 0.2183
2026-01-10 19:09:56,891: t15.2023.12.29 val PER: 0.2313
2026-01-10 19:09:56,893: t15.2024.02.25 val PER: 0.1952
2026-01-10 19:09:56,894: t15.2024.03.08 val PER: 0.3058
2026-01-10 19:09:56,896: t15.2024.03.15 val PER: 0.2883
2026-01-10 19:09:56,897: t15.2024.03.17 val PER: 0.2371
2026-01-10 19:09:56,899: t15.2024.05.10 val PER: 0.2496
2026-01-10 19:09:56,900: t15.2024.06.14 val PER: 0.2397
2026-01-10 19:09:56,902: t15.2024.07.19 val PER: 0.3105
2026-01-10 19:09:56,903: t15.2024.07.21 val PER: 0.1924
2026-01-10 19:09:56,904: t15.2024.07.28 val PER: 0.2456
2026-01-10 19:09:56,906: t15.2025.01.10 val PER: 0.3733
2026-01-10 19:09:56,907: t15.2025.01.12 val PER: 0.2571
2026-01-10 19:09:56,909: t15.2025.03.14 val PER: 0.3905
2026-01-10 19:09:56,910: t15.2025.03.16 val PER: 0.2840
2026-01-10 19:09:56,912: t15.2025.03.30 val PER: 0.3598
2026-01-10 19:09:56,913: t15.2025.04.13 val PER: 0.3010
2026-01-10 19:09:56,915: New best val WER(5gram) 69.56% --> 67.54%
2026-01-10 19:09:58,229: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_18500
2026-01-10 19:10:06,573: Train batch 18600: loss: 34.07 grad norm: 105.80 time: 0.072
2026-01-10 19:10:23,713: Train batch 18800: loss: 32.94 grad norm: 86.97 time: 0.070
2026-01-10 19:10:40,547: Train batch 19000: loss: 29.52 grad norm: 62.38 time: 0.069
2026-01-10 19:10:40,549: Running test after training batch: 19000
2026-01-10 19:10:40,660: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:10:46,570: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point it's
2026-01-10 19:10:46,643: WER debug example
  GT : how does it keep the cost down
  PR : just to keep a close aide
2026-01-10 19:11:03,491: Val batch 19000: PER (avg): 0.2473 CTC Loss (avg): 40.7300 WER(5gram): 68.38% (n=256) time: 22.940
2026-01-10 19:11:03,494: WER lens: avg_true_words=5.99 avg_pred_words=6.03 max_pred_words=14
2026-01-10 19:11:03,497: t15.2023.08.13 val PER: 0.2225
2026-01-10 19:11:03,499: t15.2023.08.18 val PER: 0.2087
2026-01-10 19:11:03,501: t15.2023.08.20 val PER: 0.1938
2026-01-10 19:11:03,504: t15.2023.08.25 val PER: 0.1898
2026-01-10 19:11:03,507: t15.2023.08.27 val PER: 0.2958
2026-01-10 19:11:03,512: t15.2023.09.01 val PER: 0.1802
2026-01-10 19:11:03,514: t15.2023.09.03 val PER: 0.2423
2026-01-10 19:11:03,516: t15.2023.09.24 val PER: 0.2184
2026-01-10 19:11:03,518: t15.2023.09.29 val PER: 0.2023
2026-01-10 19:11:03,520: t15.2023.10.01 val PER: 0.2715
2026-01-10 19:11:03,522: t15.2023.10.06 val PER: 0.1873
2026-01-10 19:11:03,524: t15.2023.10.08 val PER: 0.3329
2026-01-10 19:11:03,525: t15.2023.10.13 val PER: 0.3328
2026-01-10 19:11:03,527: t15.2023.10.15 val PER: 0.2571
2026-01-10 19:11:03,529: t15.2023.10.20 val PER: 0.2987
2026-01-10 19:11:03,531: t15.2023.10.22 val PER: 0.2517
2026-01-10 19:11:03,533: t15.2023.11.03 val PER: 0.2727
2026-01-10 19:11:03,534: t15.2023.11.04 val PER: 0.0683
2026-01-10 19:11:03,539: t15.2023.11.17 val PER: 0.1104
2026-01-10 19:11:03,541: t15.2023.11.19 val PER: 0.1297
2026-01-10 19:11:03,543: t15.2023.11.26 val PER: 0.2696
2026-01-10 19:11:03,545: t15.2023.12.03 val PER: 0.2143
2026-01-10 19:11:03,547: t15.2023.12.08 val PER: 0.2117
2026-01-10 19:11:03,549: t15.2023.12.10 val PER: 0.2037
2026-01-10 19:11:03,552: t15.2023.12.17 val PER: 0.2110
2026-01-10 19:11:03,554: t15.2023.12.29 val PER: 0.2361
2026-01-10 19:11:03,555: t15.2024.02.25 val PER: 0.1952
2026-01-10 19:11:03,558: t15.2024.03.08 val PER: 0.3101
2026-01-10 19:11:03,560: t15.2024.03.15 val PER: 0.2908
2026-01-10 19:11:03,561: t15.2024.03.17 val PER: 0.2371
2026-01-10 19:11:03,566: t15.2024.05.10 val PER: 0.2481
2026-01-10 19:11:03,568: t15.2024.06.14 val PER: 0.2587
2026-01-10 19:11:03,570: t15.2024.07.19 val PER: 0.3032
2026-01-10 19:11:03,572: t15.2024.07.21 val PER: 0.1848
2026-01-10 19:11:03,574: t15.2024.07.28 val PER: 0.2478
2026-01-10 19:11:03,576: t15.2025.01.10 val PER: 0.3843
2026-01-10 19:11:03,578: t15.2025.01.12 val PER: 0.2633
2026-01-10 19:11:03,579: t15.2025.03.14 val PER: 0.4024
2026-01-10 19:11:03,581: t15.2025.03.16 val PER: 0.2749
2026-01-10 19:11:03,583: t15.2025.03.30 val PER: 0.3644
2026-01-10 19:11:03,585: t15.2025.04.13 val PER: 0.2853
2026-01-10 19:11:03,744: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_19000
2026-01-10 19:11:20,427: Train batch 19200: loss: 23.42 grad norm: 86.80 time: 0.068
2026-01-10 19:11:37,853: Train batch 19400: loss: 22.33 grad norm: 66.16 time: 0.057
2026-01-10 19:11:46,200: Running test after training batch: 19500
2026-01-10 19:11:46,328: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:11:52,386: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point it's why
2026-01-10 19:11:52,461: WER debug example
  GT : how does it keep the cost down
  PR : just to keep a current day and
2026-01-10 19:12:09,731: Val batch 19500: PER (avg): 0.2434 CTC Loss (avg): 40.2558 WER(5gram): 70.60% (n=256) time: 23.528
2026-01-10 19:12:09,733: WER lens: avg_true_words=5.99 avg_pred_words=5.97 max_pred_words=16
2026-01-10 19:12:09,736: t15.2023.08.13 val PER: 0.2287
2026-01-10 19:12:09,738: t15.2023.08.18 val PER: 0.1987
2026-01-10 19:12:09,739: t15.2023.08.20 val PER: 0.1930
2026-01-10 19:12:09,741: t15.2023.08.25 val PER: 0.1958
2026-01-10 19:12:09,742: t15.2023.08.27 val PER: 0.2862
2026-01-10 19:12:09,744: t15.2023.09.01 val PER: 0.1810
2026-01-10 19:12:09,745: t15.2023.09.03 val PER: 0.2423
2026-01-10 19:12:09,747: t15.2023.09.24 val PER: 0.2136
2026-01-10 19:12:09,748: t15.2023.09.29 val PER: 0.2112
2026-01-10 19:12:09,750: t15.2023.10.01 val PER: 0.2688
2026-01-10 19:12:09,751: t15.2023.10.06 val PER: 0.1841
2026-01-10 19:12:09,753: t15.2023.10.08 val PER: 0.3261
2026-01-10 19:12:09,755: t15.2023.10.13 val PER: 0.3243
2026-01-10 19:12:09,756: t15.2023.10.15 val PER: 0.2624
2026-01-10 19:12:09,758: t15.2023.10.20 val PER: 0.3054
2026-01-10 19:12:09,759: t15.2023.10.22 val PER: 0.2439
2026-01-10 19:12:09,761: t15.2023.11.03 val PER: 0.2626
2026-01-10 19:12:09,762: t15.2023.11.04 val PER: 0.0785
2026-01-10 19:12:09,764: t15.2023.11.17 val PER: 0.0995
2026-01-10 19:12:09,765: t15.2023.11.19 val PER: 0.1218
2026-01-10 19:12:09,767: t15.2023.11.26 val PER: 0.2681
2026-01-10 19:12:09,768: t15.2023.12.03 val PER: 0.2185
2026-01-10 19:12:09,769: t15.2023.12.08 val PER: 0.1997
2026-01-10 19:12:09,771: t15.2023.12.10 val PER: 0.1984
2026-01-10 19:12:09,772: t15.2023.12.17 val PER: 0.2225
2026-01-10 19:12:09,774: t15.2023.12.29 val PER: 0.2382
2026-01-10 19:12:09,776: t15.2024.02.25 val PER: 0.2008
2026-01-10 19:12:09,777: t15.2024.03.08 val PER: 0.2873
2026-01-10 19:12:09,779: t15.2024.03.15 val PER: 0.2758
2026-01-10 19:12:09,780: t15.2024.03.17 val PER: 0.2232
2026-01-10 19:12:09,781: t15.2024.05.10 val PER: 0.2496
2026-01-10 19:12:09,783: t15.2024.06.14 val PER: 0.2476
2026-01-10 19:12:09,785: t15.2024.07.19 val PER: 0.2940
2026-01-10 19:12:09,786: t15.2024.07.21 val PER: 0.1786
2026-01-10 19:12:09,787: t15.2024.07.28 val PER: 0.2434
2026-01-10 19:12:09,789: t15.2025.01.10 val PER: 0.3581
2026-01-10 19:12:09,790: t15.2025.01.12 val PER: 0.2602
2026-01-10 19:12:09,792: t15.2025.03.14 val PER: 0.4009
2026-01-10 19:12:09,793: t15.2025.03.16 val PER: 0.2670
2026-01-10 19:12:09,795: t15.2025.03.30 val PER: 0.3621
2026-01-10 19:12:09,797: t15.2025.04.13 val PER: 0.2981
2026-01-10 19:12:09,956: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_19500
2026-01-10 19:12:18,244: Train batch 19600: loss: 25.17 grad norm: 80.52 time: 0.062
2026-01-10 19:12:34,844: Train batch 19800: loss: 26.30 grad norm: 68.54 time: 0.060
2026-01-10 19:12:52,110: Train batch 20000: loss: 26.87 grad norm: 64.62 time: 0.073
2026-01-10 19:12:52,114: Running test after training batch: 20000
2026-01-10 19:12:52,219: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:12:58,228: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point it's why
2026-01-10 19:12:58,325: WER debug example
  GT : how does it keep the cost down
  PR : just to keep a current day and
2026-01-10 19:13:15,016: Val batch 20000: PER (avg): 0.2432 CTC Loss (avg): 40.2505 WER(5gram): 69.17% (n=256) time: 22.899
2026-01-10 19:13:15,019: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=14
2026-01-10 19:13:15,021: t15.2023.08.13 val PER: 0.2214
2026-01-10 19:13:15,024: t15.2023.08.18 val PER: 0.1970
2026-01-10 19:13:15,026: t15.2023.08.20 val PER: 0.1851
2026-01-10 19:13:15,029: t15.2023.08.25 val PER: 0.1973
2026-01-10 19:13:15,031: t15.2023.08.27 val PER: 0.2878
2026-01-10 19:13:15,033: t15.2023.09.01 val PER: 0.1769
2026-01-10 19:13:15,034: t15.2023.09.03 val PER: 0.2435
2026-01-10 19:13:15,036: t15.2023.09.24 val PER: 0.2257
2026-01-10 19:13:15,047: t15.2023.09.29 val PER: 0.2068
2026-01-10 19:13:15,049: t15.2023.10.01 val PER: 0.2629
2026-01-10 19:13:15,051: t15.2023.10.06 val PER: 0.1862
2026-01-10 19:13:15,052: t15.2023.10.08 val PER: 0.3180
2026-01-10 19:13:15,054: t15.2023.10.13 val PER: 0.3313
2026-01-10 19:13:15,055: t15.2023.10.15 val PER: 0.2538
2026-01-10 19:13:15,057: t15.2023.10.20 val PER: 0.3221
2026-01-10 19:13:15,058: t15.2023.10.22 val PER: 0.2472
2026-01-10 19:13:15,060: t15.2023.11.03 val PER: 0.2727
2026-01-10 19:13:15,061: t15.2023.11.04 val PER: 0.0853
2026-01-10 19:13:15,063: t15.2023.11.17 val PER: 0.1089
2026-01-10 19:13:15,064: t15.2023.11.19 val PER: 0.1198
2026-01-10 19:13:15,066: t15.2023.11.26 val PER: 0.2826
2026-01-10 19:13:15,067: t15.2023.12.03 val PER: 0.2122
2026-01-10 19:13:15,069: t15.2023.12.08 val PER: 0.2031
2026-01-10 19:13:15,070: t15.2023.12.10 val PER: 0.2011
2026-01-10 19:13:15,072: t15.2023.12.17 val PER: 0.2131
2026-01-10 19:13:15,073: t15.2023.12.29 val PER: 0.2313
2026-01-10 19:13:15,074: t15.2024.02.25 val PER: 0.1840
2026-01-10 19:13:15,076: t15.2024.03.08 val PER: 0.2987
2026-01-10 19:13:15,078: t15.2024.03.15 val PER: 0.2789
2026-01-10 19:13:15,079: t15.2024.03.17 val PER: 0.2176
2026-01-10 19:13:15,081: t15.2024.05.10 val PER: 0.2541
2026-01-10 19:13:15,082: t15.2024.06.14 val PER: 0.2366
2026-01-10 19:13:15,083: t15.2024.07.19 val PER: 0.3039
2026-01-10 19:13:15,085: t15.2024.07.21 val PER: 0.1869
2026-01-10 19:13:15,087: t15.2024.07.28 val PER: 0.2441
2026-01-10 19:13:15,088: t15.2025.01.10 val PER: 0.3609
2026-01-10 19:13:15,089: t15.2025.01.12 val PER: 0.2502
2026-01-10 19:13:15,091: t15.2025.03.14 val PER: 0.3994
2026-01-10 19:13:15,092: t15.2025.03.16 val PER: 0.2723
2026-01-10 19:13:15,094: t15.2025.03.30 val PER: 0.3609
2026-01-10 19:13:15,095: t15.2025.04.13 val PER: 0.2767
2026-01-10 19:13:15,258: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_20000
2026-01-10 19:13:31,927: Train batch 20200: loss: 23.73 grad norm: 75.28 time: 0.066
2026-01-10 19:13:48,742: Train batch 20400: loss: 24.81 grad norm: 92.27 time: 0.068
2026-01-10 19:13:56,963: Running test after training batch: 20500
2026-01-10 19:13:57,081: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:14:03,099: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point it's why
2026-01-10 19:14:03,175: WER debug example
  GT : how does it keep the cost down
  PR : just to keep a cost to
2026-01-10 19:14:20,238: Val batch 20500: PER (avg): 0.2403 CTC Loss (avg): 39.7414 WER(5gram): 67.60% (n=256) time: 23.272
2026-01-10 19:14:20,240: WER lens: avg_true_words=5.99 avg_pred_words=5.91 max_pred_words=13
2026-01-10 19:14:20,243: t15.2023.08.13 val PER: 0.2277
2026-01-10 19:14:20,245: t15.2023.08.18 val PER: 0.1995
2026-01-10 19:14:20,247: t15.2023.08.20 val PER: 0.1811
2026-01-10 19:14:20,248: t15.2023.08.25 val PER: 0.1883
2026-01-10 19:14:20,250: t15.2023.08.27 val PER: 0.2830
2026-01-10 19:14:20,253: t15.2023.09.01 val PER: 0.1778
2026-01-10 19:14:20,255: t15.2023.09.03 val PER: 0.2363
2026-01-10 19:14:20,257: t15.2023.09.24 val PER: 0.2148
2026-01-10 19:14:20,259: t15.2023.09.29 val PER: 0.2010
2026-01-10 19:14:20,260: t15.2023.10.01 val PER: 0.2576
2026-01-10 19:14:20,262: t15.2023.10.06 val PER: 0.1884
2026-01-10 19:14:20,264: t15.2023.10.08 val PER: 0.3275
2026-01-10 19:14:20,265: t15.2023.10.13 val PER: 0.3289
2026-01-10 19:14:20,267: t15.2023.10.15 val PER: 0.2544
2026-01-10 19:14:20,269: t15.2023.10.20 val PER: 0.3221
2026-01-10 19:14:20,270: t15.2023.10.22 val PER: 0.2327
2026-01-10 19:14:20,272: t15.2023.11.03 val PER: 0.2659
2026-01-10 19:14:20,274: t15.2023.11.04 val PER: 0.0751
2026-01-10 19:14:20,275: t15.2023.11.17 val PER: 0.1135
2026-01-10 19:14:20,277: t15.2023.11.19 val PER: 0.1118
2026-01-10 19:14:20,278: t15.2023.11.26 val PER: 0.2754
2026-01-10 19:14:20,280: t15.2023.12.03 val PER: 0.2132
2026-01-10 19:14:20,281: t15.2023.12.08 val PER: 0.1997
2026-01-10 19:14:20,283: t15.2023.12.10 val PER: 0.1932
2026-01-10 19:14:20,284: t15.2023.12.17 val PER: 0.2173
2026-01-10 19:14:20,286: t15.2023.12.29 val PER: 0.2382
2026-01-10 19:14:20,287: t15.2024.02.25 val PER: 0.1938
2026-01-10 19:14:20,289: t15.2024.03.08 val PER: 0.2888
2026-01-10 19:14:20,290: t15.2024.03.15 val PER: 0.2733
2026-01-10 19:14:20,292: t15.2024.03.17 val PER: 0.2329
2026-01-10 19:14:20,293: t15.2024.05.10 val PER: 0.2363
2026-01-10 19:14:20,295: t15.2024.06.14 val PER: 0.2445
2026-01-10 19:14:20,296: t15.2024.07.19 val PER: 0.2927
2026-01-10 19:14:20,298: t15.2024.07.21 val PER: 0.1876
2026-01-10 19:14:20,299: t15.2024.07.28 val PER: 0.2346
2026-01-10 19:14:20,301: t15.2025.01.10 val PER: 0.3499
2026-01-10 19:14:20,302: t15.2025.01.12 val PER: 0.2479
2026-01-10 19:14:20,304: t15.2025.03.14 val PER: 0.3979
2026-01-10 19:14:20,305: t15.2025.03.16 val PER: 0.2605
2026-01-10 19:14:20,307: t15.2025.03.30 val PER: 0.3448
2026-01-10 19:14:20,308: t15.2025.04.13 val PER: 0.2796
2026-01-10 19:14:20,474: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_20500
2026-01-10 19:14:28,845: Train batch 20600: loss: 28.55 grad norm: 71.70 time: 0.062
2026-01-10 19:14:46,316: Train batch 20800: loss: 27.77 grad norm: 88.76 time: 0.059
2026-01-10 19:15:03,151: Train batch 21000: loss: 31.66 grad norm: 93.55 time: 0.060
2026-01-10 19:15:03,153: Running test after training batch: 21000
2026-01-10 19:15:03,264: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:15:09,480: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point as we
2026-01-10 19:15:09,558: WER debug example
  GT : how does it keep the cost down
  PR : just to keep the costs they may
2026-01-10 19:15:26,166: Val batch 21000: PER (avg): 0.2378 CTC Loss (avg): 39.3138 WER(5gram): 69.23% (n=256) time: 23.009
2026-01-10 19:15:26,168: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=15
2026-01-10 19:15:26,172: t15.2023.08.13 val PER: 0.2183
2026-01-10 19:15:26,175: t15.2023.08.18 val PER: 0.1961
2026-01-10 19:15:26,176: t15.2023.08.20 val PER: 0.1835
2026-01-10 19:15:26,178: t15.2023.08.25 val PER: 0.1883
2026-01-10 19:15:26,180: t15.2023.08.27 val PER: 0.2781
2026-01-10 19:15:26,182: t15.2023.09.01 val PER: 0.1769
2026-01-10 19:15:26,184: t15.2023.09.03 val PER: 0.2316
2026-01-10 19:15:26,185: t15.2023.09.24 val PER: 0.2112
2026-01-10 19:15:26,187: t15.2023.09.29 val PER: 0.1934
2026-01-10 19:15:26,189: t15.2023.10.01 val PER: 0.2596
2026-01-10 19:15:26,192: t15.2023.10.06 val PER: 0.1798
2026-01-10 19:15:26,193: t15.2023.10.08 val PER: 0.3275
2026-01-10 19:15:26,195: t15.2023.10.13 val PER: 0.3173
2026-01-10 19:15:26,196: t15.2023.10.15 val PER: 0.2564
2026-01-10 19:15:26,198: t15.2023.10.20 val PER: 0.3154
2026-01-10 19:15:26,200: t15.2023.10.22 val PER: 0.2339
2026-01-10 19:15:26,201: t15.2023.11.03 val PER: 0.2598
2026-01-10 19:15:26,203: t15.2023.11.04 val PER: 0.0785
2026-01-10 19:15:26,205: t15.2023.11.17 val PER: 0.1120
2026-01-10 19:15:26,206: t15.2023.11.19 val PER: 0.1138
2026-01-10 19:15:26,207: t15.2023.11.26 val PER: 0.2674
2026-01-10 19:15:26,209: t15.2023.12.03 val PER: 0.2153
2026-01-10 19:15:26,210: t15.2023.12.08 val PER: 0.1984
2026-01-10 19:15:26,212: t15.2023.12.10 val PER: 0.1853
2026-01-10 19:15:26,213: t15.2023.12.17 val PER: 0.2131
2026-01-10 19:15:26,215: t15.2023.12.29 val PER: 0.2361
2026-01-10 19:15:26,216: t15.2024.02.25 val PER: 0.1896
2026-01-10 19:15:26,218: t15.2024.03.08 val PER: 0.2902
2026-01-10 19:15:26,219: t15.2024.03.15 val PER: 0.2720
2026-01-10 19:15:26,221: t15.2024.03.17 val PER: 0.2225
2026-01-10 19:15:26,223: t15.2024.05.10 val PER: 0.2452
2026-01-10 19:15:26,224: t15.2024.06.14 val PER: 0.2350
2026-01-10 19:15:26,226: t15.2024.07.19 val PER: 0.2940
2026-01-10 19:15:26,227: t15.2024.07.21 val PER: 0.1779
2026-01-10 19:15:26,228: t15.2024.07.28 val PER: 0.2360
2026-01-10 19:15:26,230: t15.2025.01.10 val PER: 0.3402
2026-01-10 19:15:26,231: t15.2025.01.12 val PER: 0.2525
2026-01-10 19:15:26,233: t15.2025.03.14 val PER: 0.4053
2026-01-10 19:15:26,234: t15.2025.03.16 val PER: 0.2552
2026-01-10 19:15:26,236: t15.2025.03.30 val PER: 0.3529
2026-01-10 19:15:26,237: t15.2025.04.13 val PER: 0.2782
2026-01-10 19:15:26,401: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_21000
2026-01-10 19:15:43,314: Train batch 21200: loss: 23.96 grad norm: 88.38 time: 0.082
2026-01-10 19:16:00,396: Train batch 21400: loss: 32.97 grad norm: 80.24 time: 0.066
2026-01-10 19:16:08,744: Running test after training batch: 21500
2026-01-10 19:16:08,846: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:16:14,912: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point as we
2026-01-10 19:16:14,994: WER debug example
  GT : how does it keep the cost down
  PR : the us to keep a cost k per
2026-01-10 19:16:31,545: Val batch 21500: PER (avg): 0.2347 CTC Loss (avg): 39.0610 WER(5gram): 70.47% (n=256) time: 22.799
2026-01-10 19:16:31,548: WER lens: avg_true_words=5.99 avg_pred_words=6.24 max_pred_words=15
2026-01-10 19:16:31,550: t15.2023.08.13 val PER: 0.2193
2026-01-10 19:16:31,553: t15.2023.08.18 val PER: 0.1945
2026-01-10 19:16:31,554: t15.2023.08.20 val PER: 0.1843
2026-01-10 19:16:31,556: t15.2023.08.25 val PER: 0.1883
2026-01-10 19:16:31,558: t15.2023.08.27 val PER: 0.2669
2026-01-10 19:16:31,559: t15.2023.09.01 val PER: 0.1778
2026-01-10 19:16:31,561: t15.2023.09.03 val PER: 0.2328
2026-01-10 19:16:31,563: t15.2023.09.24 val PER: 0.2100
2026-01-10 19:16:31,565: t15.2023.09.29 val PER: 0.1895
2026-01-10 19:16:31,566: t15.2023.10.01 val PER: 0.2563
2026-01-10 19:16:31,568: t15.2023.10.06 val PER: 0.1830
2026-01-10 19:16:31,569: t15.2023.10.08 val PER: 0.3207
2026-01-10 19:16:31,571: t15.2023.10.13 val PER: 0.3064
2026-01-10 19:16:31,573: t15.2023.10.15 val PER: 0.2459
2026-01-10 19:16:31,575: t15.2023.10.20 val PER: 0.3054
2026-01-10 19:16:31,576: t15.2023.10.22 val PER: 0.2327
2026-01-10 19:16:31,578: t15.2023.11.03 val PER: 0.2619
2026-01-10 19:16:31,580: t15.2023.11.04 val PER: 0.0751
2026-01-10 19:16:31,581: t15.2023.11.17 val PER: 0.1026
2026-01-10 19:16:31,583: t15.2023.11.19 val PER: 0.1098
2026-01-10 19:16:31,585: t15.2023.11.26 val PER: 0.2667
2026-01-10 19:16:31,586: t15.2023.12.03 val PER: 0.2006
2026-01-10 19:16:31,587: t15.2023.12.08 val PER: 0.2037
2026-01-10 19:16:31,589: t15.2023.12.10 val PER: 0.1840
2026-01-10 19:16:31,590: t15.2023.12.17 val PER: 0.2110
2026-01-10 19:16:31,592: t15.2023.12.29 val PER: 0.2279
2026-01-10 19:16:31,595: t15.2024.02.25 val PER: 0.1798
2026-01-10 19:16:31,597: t15.2024.03.08 val PER: 0.2945
2026-01-10 19:16:31,599: t15.2024.03.15 val PER: 0.2608
2026-01-10 19:16:31,601: t15.2024.03.17 val PER: 0.2176
2026-01-10 19:16:31,603: t15.2024.05.10 val PER: 0.2467
2026-01-10 19:16:31,604: t15.2024.06.14 val PER: 0.2192
2026-01-10 19:16:31,606: t15.2024.07.19 val PER: 0.2940
2026-01-10 19:16:31,607: t15.2024.07.21 val PER: 0.1821
2026-01-10 19:16:31,609: t15.2024.07.28 val PER: 0.2272
2026-01-10 19:16:31,610: t15.2025.01.10 val PER: 0.3512
2026-01-10 19:16:31,612: t15.2025.01.12 val PER: 0.2402
2026-01-10 19:16:31,613: t15.2025.03.14 val PER: 0.3950
2026-01-10 19:16:31,615: t15.2025.03.16 val PER: 0.2670
2026-01-10 19:16:31,616: t15.2025.03.30 val PER: 0.3563
2026-01-10 19:16:31,618: t15.2025.04.13 val PER: 0.2796
2026-01-10 19:16:31,783: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_21500
2026-01-10 19:16:40,087: Train batch 21600: loss: 33.16 grad norm: 98.03 time: 0.080
2026-01-10 19:16:56,851: Train batch 21800: loss: 28.48 grad norm: 95.88 time: 0.087
2026-01-10 19:17:13,503: Train batch 22000: loss: 39.17 grad norm: 104.39 time: 0.058
2026-01-10 19:17:13,505: Running test after training batch: 22000
2026-01-10 19:17:13,648: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:17:19,687: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point as we
2026-01-10 19:17:19,754: WER debug example
  GT : how does it keep the cost down
  PR : just to keep a car at the
2026-01-10 19:17:36,718: Val batch 22000: PER (avg): 0.2329 CTC Loss (avg): 38.6760 WER(5gram): 75.03% (n=256) time: 23.210
2026-01-10 19:17:36,721: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=13
2026-01-10 19:17:36,723: t15.2023.08.13 val PER: 0.2131
2026-01-10 19:17:36,725: t15.2023.08.18 val PER: 0.1987
2026-01-10 19:17:36,726: t15.2023.08.20 val PER: 0.1803
2026-01-10 19:17:36,728: t15.2023.08.25 val PER: 0.1913
2026-01-10 19:17:36,729: t15.2023.08.27 val PER: 0.2669
2026-01-10 19:17:36,731: t15.2023.09.01 val PER: 0.1745
2026-01-10 19:17:36,732: t15.2023.09.03 val PER: 0.2316
2026-01-10 19:17:36,733: t15.2023.09.24 val PER: 0.2136
2026-01-10 19:17:36,735: t15.2023.09.29 val PER: 0.1908
2026-01-10 19:17:36,736: t15.2023.10.01 val PER: 0.2490
2026-01-10 19:17:36,738: t15.2023.10.06 val PER: 0.1819
2026-01-10 19:17:36,739: t15.2023.10.08 val PER: 0.3234
2026-01-10 19:17:36,741: t15.2023.10.13 val PER: 0.3033
2026-01-10 19:17:36,742: t15.2023.10.15 val PER: 0.2505
2026-01-10 19:17:36,744: t15.2023.10.20 val PER: 0.2987
2026-01-10 19:17:36,745: t15.2023.10.22 val PER: 0.2305
2026-01-10 19:17:36,746: t15.2023.11.03 val PER: 0.2578
2026-01-10 19:17:36,748: t15.2023.11.04 val PER: 0.0717
2026-01-10 19:17:36,749: t15.2023.11.17 val PER: 0.0980
2026-01-10 19:17:36,751: t15.2023.11.19 val PER: 0.1078
2026-01-10 19:17:36,752: t15.2023.11.26 val PER: 0.2594
2026-01-10 19:17:36,754: t15.2023.12.03 val PER: 0.2059
2026-01-10 19:17:36,756: t15.2023.12.08 val PER: 0.1897
2026-01-10 19:17:36,757: t15.2023.12.10 val PER: 0.1840
2026-01-10 19:17:36,759: t15.2023.12.17 val PER: 0.2089
2026-01-10 19:17:36,760: t15.2023.12.29 val PER: 0.2107
2026-01-10 19:17:36,762: t15.2024.02.25 val PER: 0.1770
2026-01-10 19:17:36,763: t15.2024.03.08 val PER: 0.2930
2026-01-10 19:17:36,764: t15.2024.03.15 val PER: 0.2639
2026-01-10 19:17:36,766: t15.2024.03.17 val PER: 0.2204
2026-01-10 19:17:36,767: t15.2024.05.10 val PER: 0.2526
2026-01-10 19:17:36,769: t15.2024.06.14 val PER: 0.2334
2026-01-10 19:17:36,770: t15.2024.07.19 val PER: 0.2960
2026-01-10 19:17:36,771: t15.2024.07.21 val PER: 0.1772
2026-01-10 19:17:36,773: t15.2024.07.28 val PER: 0.2294
2026-01-10 19:17:36,774: t15.2025.01.10 val PER: 0.3375
2026-01-10 19:17:36,776: t15.2025.01.12 val PER: 0.2456
2026-01-10 19:17:36,777: t15.2025.03.14 val PER: 0.3831
2026-01-10 19:17:36,779: t15.2025.03.16 val PER: 0.2709
2026-01-10 19:17:36,780: t15.2025.03.30 val PER: 0.3517
2026-01-10 19:17:36,782: t15.2025.04.13 val PER: 0.2782
2026-01-10 19:17:36,952: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_22000
2026-01-10 19:17:53,588: Train batch 22200: loss: 29.69 grad norm: 95.29 time: 0.064
2026-01-10 19:18:10,023: Train batch 22400: loss: 27.36 grad norm: 78.00 time: 0.059
2026-01-10 19:18:18,476: Running test after training batch: 22500
2026-01-10 19:18:18,596: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:18:24,601: WER debug example
  GT : you can see the code at this point as well
  PR : k and a good eye at this point as we
2026-01-10 19:18:24,673: WER debug example
  GT : how does it keep the cost down
  PR : just to keep the costs they may
2026-01-10 19:18:41,422: Val batch 22500: PER (avg): 0.2303 CTC Loss (avg): 38.3752 WER(5gram): 72.82% (n=256) time: 22.944
2026-01-10 19:18:41,424: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=15
2026-01-10 19:18:41,427: t15.2023.08.13 val PER: 0.2141
2026-01-10 19:18:41,429: t15.2023.08.18 val PER: 0.1861
2026-01-10 19:18:41,430: t15.2023.08.20 val PER: 0.1763
2026-01-10 19:18:41,432: t15.2023.08.25 val PER: 0.1913
2026-01-10 19:18:41,434: t15.2023.08.27 val PER: 0.2701
2026-01-10 19:18:41,435: t15.2023.09.01 val PER: 0.1753
2026-01-10 19:18:41,437: t15.2023.09.03 val PER: 0.2209
2026-01-10 19:18:41,438: t15.2023.09.24 val PER: 0.2100
2026-01-10 19:18:41,440: t15.2023.09.29 val PER: 0.1883
2026-01-10 19:18:41,442: t15.2023.10.01 val PER: 0.2424
2026-01-10 19:18:41,443: t15.2023.10.06 val PER: 0.1744
2026-01-10 19:18:41,445: t15.2023.10.08 val PER: 0.3099
2026-01-10 19:18:41,446: t15.2023.10.13 val PER: 0.2971
2026-01-10 19:18:41,448: t15.2023.10.15 val PER: 0.2465
2026-01-10 19:18:41,450: t15.2023.10.20 val PER: 0.2953
2026-01-10 19:18:41,451: t15.2023.10.22 val PER: 0.2283
2026-01-10 19:18:41,453: t15.2023.11.03 val PER: 0.2680
2026-01-10 19:18:41,454: t15.2023.11.04 val PER: 0.0717
2026-01-10 19:18:41,456: t15.2023.11.17 val PER: 0.0995
2026-01-10 19:18:41,457: t15.2023.11.19 val PER: 0.0958
2026-01-10 19:18:41,459: t15.2023.11.26 val PER: 0.2572
2026-01-10 19:18:41,461: t15.2023.12.03 val PER: 0.1985
2026-01-10 19:18:41,462: t15.2023.12.08 val PER: 0.1951
2026-01-10 19:18:41,464: t15.2023.12.10 val PER: 0.1787
2026-01-10 19:18:41,465: t15.2023.12.17 val PER: 0.2069
2026-01-10 19:18:41,467: t15.2023.12.29 val PER: 0.2155
2026-01-10 19:18:41,468: t15.2024.02.25 val PER: 0.1840
2026-01-10 19:18:41,470: t15.2024.03.08 val PER: 0.2802
2026-01-10 19:18:41,471: t15.2024.03.15 val PER: 0.2639
2026-01-10 19:18:41,473: t15.2024.03.17 val PER: 0.2169
2026-01-10 19:18:41,474: t15.2024.05.10 val PER: 0.2437
2026-01-10 19:18:41,476: t15.2024.06.14 val PER: 0.2319
2026-01-10 19:18:41,479: t15.2024.07.19 val PER: 0.2933
2026-01-10 19:18:41,480: t15.2024.07.21 val PER: 0.1752
2026-01-10 19:18:41,482: t15.2024.07.28 val PER: 0.2235
2026-01-10 19:18:41,483: t15.2025.01.10 val PER: 0.3430
2026-01-10 19:18:41,485: t15.2025.01.12 val PER: 0.2317
2026-01-10 19:18:41,489: t15.2025.03.14 val PER: 0.4053
2026-01-10 19:18:41,491: t15.2025.03.16 val PER: 0.2644
2026-01-10 19:18:41,492: t15.2025.03.30 val PER: 0.3448
2026-01-10 19:18:41,494: t15.2025.04.13 val PER: 0.2796
2026-01-10 19:18:41,667: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_22500
2026-01-10 19:18:49,963: Train batch 22600: loss: 37.94 grad norm: 91.69 time: 0.065
2026-01-10 19:19:07,101: Train batch 22800: loss: 28.98 grad norm: 128.15 time: 0.063
2026-01-10 19:19:23,848: Train batch 23000: loss: 34.40 grad norm: 115.69 time: 0.067
2026-01-10 19:19:23,850: Running test after training batch: 23000
2026-01-10 19:19:23,962: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:19:30,060: WER debug example
  GT : you can see the code at this point as well
  PR : we need a good at this point is why
2026-01-10 19:19:30,138: WER debug example
  GT : how does it keep the cost down
  PR : just to keep the cost and
2026-01-10 19:19:46,995: Val batch 23000: PER (avg): 0.2288 CTC Loss (avg): 38.5162 WER(5gram): 67.28% (n=256) time: 23.143
2026-01-10 19:19:46,998: WER lens: avg_true_words=5.99 avg_pred_words=6.25 max_pred_words=14
2026-01-10 19:19:47,000: t15.2023.08.13 val PER: 0.2235
2026-01-10 19:19:47,003: t15.2023.08.18 val PER: 0.1878
2026-01-10 19:19:47,004: t15.2023.08.20 val PER: 0.1755
2026-01-10 19:19:47,006: t15.2023.08.25 val PER: 0.1777
2026-01-10 19:19:47,008: t15.2023.08.27 val PER: 0.2749
2026-01-10 19:19:47,009: t15.2023.09.01 val PER: 0.1705
2026-01-10 19:19:47,011: t15.2023.09.03 val PER: 0.2257
2026-01-10 19:19:47,013: t15.2023.09.24 val PER: 0.2075
2026-01-10 19:19:47,014: t15.2023.09.29 val PER: 0.1959
2026-01-10 19:19:47,016: t15.2023.10.01 val PER: 0.2464
2026-01-10 19:19:47,018: t15.2023.10.06 val PER: 0.1776
2026-01-10 19:19:47,019: t15.2023.10.08 val PER: 0.3112
2026-01-10 19:19:47,021: t15.2023.10.13 val PER: 0.3049
2026-01-10 19:19:47,022: t15.2023.10.15 val PER: 0.2367
2026-01-10 19:19:47,024: t15.2023.10.20 val PER: 0.3121
2026-01-10 19:19:47,026: t15.2023.10.22 val PER: 0.2339
2026-01-10 19:19:47,027: t15.2023.11.03 val PER: 0.2564
2026-01-10 19:19:47,029: t15.2023.11.04 val PER: 0.0683
2026-01-10 19:19:47,031: t15.2023.11.17 val PER: 0.0995
2026-01-10 19:19:47,032: t15.2023.11.19 val PER: 0.1078
2026-01-10 19:19:47,034: t15.2023.11.26 val PER: 0.2536
2026-01-10 19:19:47,035: t15.2023.12.03 val PER: 0.2048
2026-01-10 19:19:47,037: t15.2023.12.08 val PER: 0.1897
2026-01-10 19:19:47,038: t15.2023.12.10 val PER: 0.1761
2026-01-10 19:19:47,040: t15.2023.12.17 val PER: 0.2110
2026-01-10 19:19:47,042: t15.2023.12.29 val PER: 0.2189
2026-01-10 19:19:47,043: t15.2024.02.25 val PER: 0.1798
2026-01-10 19:19:47,045: t15.2024.03.08 val PER: 0.2831
2026-01-10 19:19:47,047: t15.2024.03.15 val PER: 0.2645
2026-01-10 19:19:47,048: t15.2024.03.17 val PER: 0.2176
2026-01-10 19:19:47,050: t15.2024.05.10 val PER: 0.2244
2026-01-10 19:19:47,051: t15.2024.06.14 val PER: 0.2319
2026-01-10 19:19:47,053: t15.2024.07.19 val PER: 0.2828
2026-01-10 19:19:47,055: t15.2024.07.21 val PER: 0.1710
2026-01-10 19:19:47,057: t15.2024.07.28 val PER: 0.2162
2026-01-10 19:19:47,058: t15.2025.01.10 val PER: 0.3499
2026-01-10 19:19:47,060: t15.2025.01.12 val PER: 0.2394
2026-01-10 19:19:47,062: t15.2025.03.14 val PER: 0.3891
2026-01-10 19:19:47,063: t15.2025.03.16 val PER: 0.2435
2026-01-10 19:19:47,065: t15.2025.03.30 val PER: 0.3310
2026-01-10 19:19:47,066: t15.2025.04.13 val PER: 0.2639
2026-01-10 19:19:47,068: New best val WER(5gram) 67.54% --> 67.28%
2026-01-10 19:19:48,635: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_23000
2026-01-10 19:20:05,589: Train batch 23200: loss: 32.84 grad norm: 108.08 time: 0.066
2026-01-10 19:20:22,524: Train batch 23400: loss: 23.94 grad norm: 105.83 time: 0.076
2026-01-10 19:20:30,906: Running test after training batch: 23500
2026-01-10 19:20:31,006: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:20:37,225: WER debug example
  GT : you can see the code at this point as well
  PR : i know a good at this point is why
2026-01-10 19:20:37,310: WER debug example
  GT : how does it keep the cost down
  PR : just to keep a close game
2026-01-10 19:20:53,885: Val batch 23500: PER (avg): 0.2264 CTC Loss (avg): 37.7147 WER(5gram): 76.53% (n=256) time: 22.977
2026-01-10 19:20:53,888: WER lens: avg_true_words=5.99 avg_pred_words=6.25 max_pred_words=15
2026-01-10 19:20:53,890: t15.2023.08.13 val PER: 0.2058
2026-01-10 19:20:53,892: t15.2023.08.18 val PER: 0.1903
2026-01-10 19:20:53,894: t15.2023.08.20 val PER: 0.1747
2026-01-10 19:20:53,896: t15.2023.08.25 val PER: 0.1762
2026-01-10 19:20:53,897: t15.2023.08.27 val PER: 0.2508
2026-01-10 19:20:53,899: t15.2023.09.01 val PER: 0.1705
2026-01-10 19:20:53,900: t15.2023.09.03 val PER: 0.2316
2026-01-10 19:20:53,902: t15.2023.09.24 val PER: 0.2051
2026-01-10 19:20:53,903: t15.2023.09.29 val PER: 0.1934
2026-01-10 19:20:53,905: t15.2023.10.01 val PER: 0.2483
2026-01-10 19:20:53,906: t15.2023.10.06 val PER: 0.1733
2026-01-10 19:20:53,908: t15.2023.10.08 val PER: 0.3099
2026-01-10 19:20:53,909: t15.2023.10.13 val PER: 0.2979
2026-01-10 19:20:53,911: t15.2023.10.15 val PER: 0.2413
2026-01-10 19:20:53,912: t15.2023.10.20 val PER: 0.2987
2026-01-10 19:20:53,914: t15.2023.10.22 val PER: 0.2272
2026-01-10 19:20:53,915: t15.2023.11.03 val PER: 0.2510
2026-01-10 19:20:53,917: t15.2023.11.04 val PER: 0.0648
2026-01-10 19:20:53,918: t15.2023.11.17 val PER: 0.1073
2026-01-10 19:20:53,920: t15.2023.11.19 val PER: 0.1058
2026-01-10 19:20:53,921: t15.2023.11.26 val PER: 0.2529
2026-01-10 19:20:53,923: t15.2023.12.03 val PER: 0.1985
2026-01-10 19:20:53,924: t15.2023.12.08 val PER: 0.1791
2026-01-10 19:20:53,926: t15.2023.12.10 val PER: 0.1748
2026-01-10 19:20:53,928: t15.2023.12.17 val PER: 0.1985
2026-01-10 19:20:53,929: t15.2023.12.29 val PER: 0.2066
2026-01-10 19:20:53,931: t15.2024.02.25 val PER: 0.1910
2026-01-10 19:20:53,933: t15.2024.03.08 val PER: 0.2802
2026-01-10 19:20:53,935: t15.2024.03.15 val PER: 0.2558
2026-01-10 19:20:53,936: t15.2024.03.17 val PER: 0.2155
2026-01-10 19:20:53,938: t15.2024.05.10 val PER: 0.2273
2026-01-10 19:20:53,940: t15.2024.06.14 val PER: 0.2145
2026-01-10 19:20:53,941: t15.2024.07.19 val PER: 0.2828
2026-01-10 19:20:53,943: t15.2024.07.21 val PER: 0.1772
2026-01-10 19:20:53,944: t15.2024.07.28 val PER: 0.2191
2026-01-10 19:20:53,946: t15.2025.01.10 val PER: 0.3444
2026-01-10 19:20:53,947: t15.2025.01.12 val PER: 0.2356
2026-01-10 19:20:53,949: t15.2025.03.14 val PER: 0.3846
2026-01-10 19:20:53,951: t15.2025.03.16 val PER: 0.2526
2026-01-10 19:20:53,952: t15.2025.03.30 val PER: 0.3483
2026-01-10 19:20:53,954: t15.2025.04.13 val PER: 0.2710
2026-01-10 19:20:54,626: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_23500
2026-01-10 19:21:03,029: Train batch 23600: loss: 15.44 grad norm: 79.11 time: 0.063
2026-01-10 19:21:19,777: Train batch 23800: loss: 23.65 grad norm: 68.37 time: 0.061
2026-01-10 19:21:36,696: Train batch 24000: loss: 27.71 grad norm: 103.59 time: 0.084
2026-01-10 19:21:36,698: Running test after training batch: 24000
2026-01-10 19:21:36,798: WER debug GT example: You can see the code at this point as well.
2026-01-10 19:21:43,576: WER debug example
  GT : you can see the code at this point as well
  PR : you can see a good at this point so we
2026-01-10 19:21:43,652: WER debug example
  GT : how does it keep the cost down
  PR : just to keep the cost and
2026-01-10 19:21:59,711: Val batch 24000: PER (avg): 0.2238 CTC Loss (avg): 37.6351 WER(5gram): 69.88% (n=256) time: 23.010
2026-01-10 19:21:59,713: WER lens: avg_true_words=5.99 avg_pred_words=6.42 max_pred_words=17
2026-01-10 19:21:59,716: t15.2023.08.13 val PER: 0.2131
2026-01-10 19:21:59,717: t15.2023.08.18 val PER: 0.1827
2026-01-10 19:21:59,719: t15.2023.08.20 val PER: 0.1716
2026-01-10 19:21:59,720: t15.2023.08.25 val PER: 0.1792
2026-01-10 19:21:59,722: t15.2023.08.27 val PER: 0.2637
2026-01-10 19:21:59,723: t15.2023.09.01 val PER: 0.1648
2026-01-10 19:21:59,725: t15.2023.09.03 val PER: 0.2245
2026-01-10 19:21:59,726: t15.2023.09.24 val PER: 0.1966
2026-01-10 19:21:59,728: t15.2023.09.29 val PER: 0.1927
2026-01-10 19:21:59,729: t15.2023.10.01 val PER: 0.2464
2026-01-10 19:21:59,731: t15.2023.10.06 val PER: 0.1668
2026-01-10 19:21:59,732: t15.2023.10.08 val PER: 0.3018
2026-01-10 19:21:59,734: t15.2023.10.13 val PER: 0.2956
2026-01-10 19:21:59,735: t15.2023.10.15 val PER: 0.2432
2026-01-10 19:21:59,736: t15.2023.10.20 val PER: 0.2953
2026-01-10 19:21:59,738: t15.2023.10.22 val PER: 0.2283
2026-01-10 19:21:59,739: t15.2023.11.03 val PER: 0.2497
2026-01-10 19:21:59,741: t15.2023.11.04 val PER: 0.0717
2026-01-10 19:21:59,742: t15.2023.11.17 val PER: 0.0995
2026-01-10 19:21:59,743: t15.2023.11.19 val PER: 0.1058
2026-01-10 19:21:59,745: t15.2023.11.26 val PER: 0.2478
2026-01-10 19:21:59,746: t15.2023.12.03 val PER: 0.1933
2026-01-10 19:21:59,748: t15.2023.12.08 val PER: 0.1758
2026-01-10 19:21:59,749: t15.2023.12.10 val PER: 0.1827
2026-01-10 19:21:59,750: t15.2023.12.17 val PER: 0.1965
2026-01-10 19:21:59,752: t15.2023.12.29 val PER: 0.2121
2026-01-10 19:21:59,753: t15.2024.02.25 val PER: 0.1840
2026-01-10 19:21:59,755: t15.2024.03.08 val PER: 0.2688
2026-01-10 19:21:59,756: t15.2024.03.15 val PER: 0.2552
2026-01-10 19:21:59,757: t15.2024.03.17 val PER: 0.2050
2026-01-10 19:21:59,759: t15.2024.05.10 val PER: 0.2229
2026-01-10 19:21:59,761: t15.2024.06.14 val PER: 0.2271
2026-01-10 19:21:59,763: t15.2024.07.19 val PER: 0.2828
2026-01-10 19:21:59,764: t15.2024.07.21 val PER: 0.1655
2026-01-10 19:21:59,765: t15.2024.07.28 val PER: 0.2213
2026-01-10 19:21:59,767: t15.2025.01.10 val PER: 0.3388
2026-01-10 19:21:59,768: t15.2025.01.12 val PER: 0.2279
2026-01-10 19:21:59,770: t15.2025.03.14 val PER: 0.3802
2026-01-10 19:21:59,771: t15.2025.03.16 val PER: 0.2487
2026-01-10 19:21:59,772: t15.2025.03.30 val PER: 0.3310
2026-01-10 19:21:59,774: t15.2025.04.13 val PER: 0.2782
2026-01-10 19:22:00,091: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_long/checkpoint/checkpoint_batch_24000
