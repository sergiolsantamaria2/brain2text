TMPDIR=/home/e12511253/tmp
JOB_TMP=/home/e12511253/tmp/e12511253_b2t_355548
TORCH_EXTENSIONS_DIR=/home/e12511253/tmp/e12511253_b2t_355548/torch_extensions
WANDB_DIR=/home/e12511253/tmp/e12511253_b2t_355548/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/home/e12511253/tmp/e12511253_b2t_355548/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan 13 21:39 /home/e12511253/tmp/e12511253_b2t_355548/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
OUT_ROOT=/home/e12511253/Brain2Text/brain2text/trained_models
==============================================
Job: b2t5g  ID: 355548
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_5gram_only.yaml
Folders: configs/experiments/diphones/
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/diphones/ ==========
Num configs: 2

=== RUN base.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/diphones/base
2026-01-13 21:39:05,440: Using device: cuda:0
2026-01-13 21:42:47,604: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-13 21:42:47,629: Using 45 sessions after filtering (from 45).
2026-01-13 21:42:48,049: Using torch.compile (if available)
2026-01-13 21:42:48,049: torch.compile not available (torch<2.0). Skipping.
2026-01-13 21:42:48,049: Initialized RNN decoding model
2026-01-13 21:42:48,049: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-13 21:42:48,050: Model has 44,907,305 parameters
2026-01-13 21:42:48,050: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-13 21:42:52,496: Successfully initialized datasets
2026-01-13 21:42:52,496: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-13 21:42:54,078: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.192
2026-01-13 21:42:54,079: Running test after training batch: 0
2026-01-13 21:42:54,186: WER debug GT example: You can see the code at this point as well.
2026-01-13 21:43:00,000: WER debug example
  GT : you can see the code at this point as well
  PR : she has from his
2026-01-13 21:43:01,127: WER debug example
  GT : how does it keep the cost down
  PR : money from
2026-01-13 21:47:21,726: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(5gram): 99.67% (n=256) time: 267.647
2026-01-13 21:47:21,731: WER lens: avg_true_words=5.99 avg_pred_words=2.82 max_pred_words=7
2026-01-13 21:47:21,737: t15.2023.08.13 val PER: 1.3056
2026-01-13 21:47:21,744: t15.2023.08.18 val PER: 1.4208
2026-01-13 21:47:21,747: t15.2023.08.20 val PER: 1.3002
2026-01-13 21:47:21,747: t15.2023.08.25 val PER: 1.3389
2026-01-13 21:47:21,747: t15.2023.08.27 val PER: 1.2460
2026-01-13 21:47:21,747: t15.2023.09.01 val PER: 1.4537
2026-01-13 21:47:21,747: t15.2023.09.03 val PER: 1.3171
2026-01-13 21:47:21,747: t15.2023.09.24 val PER: 1.5461
2026-01-13 21:47:21,747: t15.2023.09.29 val PER: 1.4671
2026-01-13 21:47:21,747: t15.2023.10.01 val PER: 1.2147
2026-01-13 21:47:21,747: t15.2023.10.06 val PER: 1.4876
2026-01-13 21:47:21,748: t15.2023.10.08 val PER: 1.1827
2026-01-13 21:47:21,748: t15.2023.10.13 val PER: 1.3964
2026-01-13 21:47:21,748: t15.2023.10.15 val PER: 1.3889
2026-01-13 21:47:21,748: t15.2023.10.20 val PER: 1.4866
2026-01-13 21:47:21,748: t15.2023.10.22 val PER: 1.3942
2026-01-13 21:47:21,748: t15.2023.11.03 val PER: 1.5923
2026-01-13 21:47:21,748: t15.2023.11.04 val PER: 2.0171
2026-01-13 21:47:21,748: t15.2023.11.17 val PER: 1.9518
2026-01-13 21:47:21,748: t15.2023.11.19 val PER: 1.6707
2026-01-13 21:47:21,748: t15.2023.11.26 val PER: 1.5413
2026-01-13 21:47:21,748: t15.2023.12.03 val PER: 1.4254
2026-01-13 21:47:21,748: t15.2023.12.08 val PER: 1.4487
2026-01-13 21:47:21,748: t15.2023.12.10 val PER: 1.6899
2026-01-13 21:47:21,748: t15.2023.12.17 val PER: 1.3077
2026-01-13 21:47:21,749: t15.2023.12.29 val PER: 1.4063
2026-01-13 21:47:21,749: t15.2024.02.25 val PER: 1.4228
2026-01-13 21:47:21,749: t15.2024.03.08 val PER: 1.3257
2026-01-13 21:47:21,749: t15.2024.03.15 val PER: 1.3196
2026-01-13 21:47:21,749: t15.2024.03.17 val PER: 1.4052
2026-01-13 21:47:21,749: t15.2024.05.10 val PER: 1.3224
2026-01-13 21:47:21,749: t15.2024.06.14 val PER: 1.5315
2026-01-13 21:47:21,749: t15.2024.07.19 val PER: 1.0817
2026-01-13 21:47:21,749: t15.2024.07.21 val PER: 1.6290
2026-01-13 21:47:21,749: t15.2024.07.28 val PER: 1.6588
2026-01-13 21:47:21,749: t15.2025.01.10 val PER: 1.0923
2026-01-13 21:47:21,749: t15.2025.01.12 val PER: 1.7629
2026-01-13 21:47:21,749: t15.2025.03.14 val PER: 1.0414
2026-01-13 21:47:21,749: t15.2025.03.16 val PER: 1.6257
2026-01-13 21:47:21,749: t15.2025.03.30 val PER: 1.2874
2026-01-13 21:47:21,750: t15.2025.04.13 val PER: 1.5949
2026-01-13 21:47:21,751: New best val WER(5gram) inf% --> 99.67%
2026-01-13 21:47:21,905: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_0
2026-01-13 21:47:39,677: Train batch 200: loss: 77.59 grad norm: 106.24 time: 0.055
2026-01-13 21:47:57,300: Train batch 400: loss: 54.11 grad norm: 89.04 time: 0.062
2026-01-13 21:48:06,275: Running test after training batch: 500
2026-01-13 21:48:06,416: WER debug GT example: You can see the code at this point as well.
2026-01-13 21:48:11,522: WER debug example
  GT : you can see the code at this point as well
  PR : you'll need is the ease and at this ride is all
2026-01-13 21:48:11,688: WER debug example
  GT : how does it keep the cost down
  PR : and does it think this is as
2026-01-13 21:48:59,032: Val batch 500: PER (avg): 0.5177 CTC Loss (avg): 55.5955 WER(5gram): 75.55% (n=256) time: 52.756
2026-01-13 21:48:59,032: WER lens: avg_true_words=5.99 avg_pred_words=5.71 max_pred_words=12
2026-01-13 21:48:59,032: t15.2023.08.13 val PER: 0.4605
2026-01-13 21:48:59,033: t15.2023.08.18 val PER: 0.4510
2026-01-13 21:48:59,033: t15.2023.08.20 val PER: 0.4369
2026-01-13 21:48:59,033: t15.2023.08.25 val PER: 0.4292
2026-01-13 21:48:59,033: t15.2023.08.27 val PER: 0.5273
2026-01-13 21:48:59,033: t15.2023.09.01 val PER: 0.4123
2026-01-13 21:48:59,033: t15.2023.09.03 val PER: 0.4917
2026-01-13 21:48:59,033: t15.2023.09.24 val PER: 0.4260
2026-01-13 21:48:59,033: t15.2023.09.29 val PER: 0.4678
2026-01-13 21:48:59,033: t15.2023.10.01 val PER: 0.5112
2026-01-13 21:48:59,033: t15.2023.10.06 val PER: 0.4241
2026-01-13 21:48:59,033: t15.2023.10.08 val PER: 0.5440
2026-01-13 21:48:59,033: t15.2023.10.13 val PER: 0.5702
2026-01-13 21:48:59,034: t15.2023.10.15 val PER: 0.4944
2026-01-13 21:48:59,034: t15.2023.10.20 val PER: 0.4463
2026-01-13 21:48:59,034: t15.2023.10.22 val PER: 0.4454
2026-01-13 21:48:59,034: t15.2023.11.03 val PER: 0.4980
2026-01-13 21:48:59,034: t15.2023.11.04 val PER: 0.2730
2026-01-13 21:48:59,034: t15.2023.11.17 val PER: 0.3655
2026-01-13 21:48:59,034: t15.2023.11.19 val PER: 0.3313
2026-01-13 21:48:59,034: t15.2023.11.26 val PER: 0.5493
2026-01-13 21:48:59,034: t15.2023.12.03 val PER: 0.5011
2026-01-13 21:48:59,035: t15.2023.12.08 val PER: 0.5193
2026-01-13 21:48:59,035: t15.2023.12.10 val PER: 0.4573
2026-01-13 21:48:59,035: t15.2023.12.17 val PER: 0.5728
2026-01-13 21:48:59,035: t15.2023.12.29 val PER: 0.5395
2026-01-13 21:48:59,035: t15.2024.02.25 val PER: 0.4747
2026-01-13 21:48:59,035: t15.2024.03.08 val PER: 0.6145
2026-01-13 21:48:59,035: t15.2024.03.15 val PER: 0.5510
2026-01-13 21:48:59,035: t15.2024.03.17 val PER: 0.5084
2026-01-13 21:48:59,035: t15.2024.05.10 val PER: 0.5438
2026-01-13 21:48:59,035: t15.2024.06.14 val PER: 0.5063
2026-01-13 21:48:59,035: t15.2024.07.19 val PER: 0.6638
2026-01-13 21:48:59,035: t15.2024.07.21 val PER: 0.4800
2026-01-13 21:48:59,035: t15.2024.07.28 val PER: 0.5213
2026-01-13 21:48:59,035: t15.2025.01.10 val PER: 0.7521
2026-01-13 21:48:59,035: t15.2025.01.12 val PER: 0.5674
2026-01-13 21:48:59,035: t15.2025.03.14 val PER: 0.7574
2026-01-13 21:48:59,036: t15.2025.03.16 val PER: 0.5982
2026-01-13 21:48:59,036: t15.2025.03.30 val PER: 0.7345
2026-01-13 21:48:59,036: t15.2025.04.13 val PER: 0.5792
2026-01-13 21:48:59,037: New best val WER(5gram) 99.67% --> 75.55%
2026-01-13 21:48:59,189: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_500
2026-01-13 21:49:08,288: Train batch 600: loss: 48.73 grad norm: 79.76 time: 0.078
2026-01-13 21:49:25,724: Train batch 800: loss: 41.36 grad norm: 86.79 time: 0.057
2026-01-13 21:49:43,338: Train batch 1000: loss: 42.31 grad norm: 74.93 time: 0.066
2026-01-13 21:49:43,339: Running test after training batch: 1000
2026-01-13 21:49:43,468: WER debug GT example: You can see the code at this point as well.
2026-01-13 21:49:48,672: WER debug example
  GT : you can see the code at this point as well
  PR : you'd get she's that good at it and is well
2026-01-13 21:49:48,896: WER debug example
  GT : how does it keep the cost down
  PR : howled as it is that what it
2026-01-13 21:50:18,276: Val batch 1000: PER (avg): 0.4099 CTC Loss (avg): 42.3272 WER(5gram): 53.59% (n=256) time: 34.936
2026-01-13 21:50:18,276: WER lens: avg_true_words=5.99 avg_pred_words=5.57 max_pred_words=12
2026-01-13 21:50:18,276: t15.2023.08.13 val PER: 0.3815
2026-01-13 21:50:18,277: t15.2023.08.18 val PER: 0.3412
2026-01-13 21:50:18,277: t15.2023.08.20 val PER: 0.3439
2026-01-13 21:50:18,277: t15.2023.08.25 val PER: 0.3117
2026-01-13 21:50:18,277: t15.2023.08.27 val PER: 0.4244
2026-01-13 21:50:18,277: t15.2023.09.01 val PER: 0.3003
2026-01-13 21:50:18,277: t15.2023.09.03 val PER: 0.3943
2026-01-13 21:50:18,277: t15.2023.09.24 val PER: 0.3350
2026-01-13 21:50:18,277: t15.2023.09.29 val PER: 0.3593
2026-01-13 21:50:18,278: t15.2023.10.01 val PER: 0.4003
2026-01-13 21:50:18,278: t15.2023.10.06 val PER: 0.3208
2026-01-13 21:50:18,278: t15.2023.10.08 val PER: 0.4520
2026-01-13 21:50:18,278: t15.2023.10.13 val PER: 0.4647
2026-01-13 21:50:18,278: t15.2023.10.15 val PER: 0.3850
2026-01-13 21:50:18,278: t15.2023.10.20 val PER: 0.3725
2026-01-13 21:50:18,278: t15.2023.10.22 val PER: 0.3563
2026-01-13 21:50:18,278: t15.2023.11.03 val PER: 0.4043
2026-01-13 21:50:18,278: t15.2023.11.04 val PER: 0.1570
2026-01-13 21:50:18,278: t15.2023.11.17 val PER: 0.2566
2026-01-13 21:50:18,278: t15.2023.11.19 val PER: 0.2255
2026-01-13 21:50:18,278: t15.2023.11.26 val PER: 0.4464
2026-01-13 21:50:18,278: t15.2023.12.03 val PER: 0.4055
2026-01-13 21:50:18,279: t15.2023.12.08 val PER: 0.3995
2026-01-13 21:50:18,279: t15.2023.12.10 val PER: 0.3535
2026-01-13 21:50:18,279: t15.2023.12.17 val PER: 0.4096
2026-01-13 21:50:18,279: t15.2023.12.29 val PER: 0.4043
2026-01-13 21:50:18,279: t15.2024.02.25 val PER: 0.3399
2026-01-13 21:50:18,279: t15.2024.03.08 val PER: 0.5064
2026-01-13 21:50:18,279: t15.2024.03.15 val PER: 0.4472
2026-01-13 21:50:18,279: t15.2024.03.17 val PER: 0.4059
2026-01-13 21:50:18,280: t15.2024.05.10 val PER: 0.4264
2026-01-13 21:50:18,280: t15.2024.06.14 val PER: 0.4006
2026-01-13 21:50:18,280: t15.2024.07.19 val PER: 0.5432
2026-01-13 21:50:18,280: t15.2024.07.21 val PER: 0.3766
2026-01-13 21:50:18,280: t15.2024.07.28 val PER: 0.4147
2026-01-13 21:50:18,280: t15.2025.01.10 val PER: 0.6212
2026-01-13 21:50:18,280: t15.2025.01.12 val PER: 0.4496
2026-01-13 21:50:18,280: t15.2025.03.14 val PER: 0.6391
2026-01-13 21:50:18,280: t15.2025.03.16 val PER: 0.4804
2026-01-13 21:50:18,280: t15.2025.03.30 val PER: 0.6552
2026-01-13 21:50:18,280: t15.2025.04.13 val PER: 0.4979
2026-01-13 21:50:18,281: New best val WER(5gram) 75.55% --> 53.59%
2026-01-13 21:50:18,429: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_1000
2026-01-13 21:50:35,973: Train batch 1200: loss: 32.78 grad norm: 75.52 time: 0.068
2026-01-13 21:50:53,603: Train batch 1400: loss: 36.04 grad norm: 76.20 time: 0.060
2026-01-13 21:51:02,397: Running test after training batch: 1500
2026-01-13 21:51:02,499: WER debug GT example: You can see the code at this point as well.
2026-01-13 21:51:07,688: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-13 21:51:07,781: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us in
2026-01-13 21:51:27,568: Val batch 1500: PER (avg): 0.3811 CTC Loss (avg): 37.2576 WER(5gram): 35.27% (n=256) time: 25.170
2026-01-13 21:51:27,568: WER lens: avg_true_words=5.99 avg_pred_words=5.35 max_pred_words=12
2026-01-13 21:51:27,568: t15.2023.08.13 val PER: 0.3493
2026-01-13 21:51:27,569: t15.2023.08.18 val PER: 0.3236
2026-01-13 21:51:27,569: t15.2023.08.20 val PER: 0.3098
2026-01-13 21:51:27,569: t15.2023.08.25 val PER: 0.2605
2026-01-13 21:51:27,569: t15.2023.08.27 val PER: 0.3955
2026-01-13 21:51:27,569: t15.2023.09.01 val PER: 0.2735
2026-01-13 21:51:27,569: t15.2023.09.03 val PER: 0.3741
2026-01-13 21:51:27,569: t15.2023.09.24 val PER: 0.3155
2026-01-13 21:51:27,569: t15.2023.09.29 val PER: 0.3350
2026-01-13 21:51:27,569: t15.2023.10.01 val PER: 0.3923
2026-01-13 21:51:27,570: t15.2023.10.06 val PER: 0.2853
2026-01-13 21:51:27,570: t15.2023.10.08 val PER: 0.4371
2026-01-13 21:51:27,570: t15.2023.10.13 val PER: 0.4445
2026-01-13 21:51:27,570: t15.2023.10.15 val PER: 0.3665
2026-01-13 21:51:27,570: t15.2023.10.20 val PER: 0.3389
2026-01-13 21:51:27,570: t15.2023.10.22 val PER: 0.3118
2026-01-13 21:51:27,570: t15.2023.11.03 val PER: 0.3616
2026-01-13 21:51:27,570: t15.2023.11.04 val PER: 0.1229
2026-01-13 21:51:27,570: t15.2023.11.17 val PER: 0.2208
2026-01-13 21:51:27,570: t15.2023.11.19 val PER: 0.1657
2026-01-13 21:51:27,570: t15.2023.11.26 val PER: 0.4109
2026-01-13 21:51:27,571: t15.2023.12.03 val PER: 0.3645
2026-01-13 21:51:27,571: t15.2023.12.08 val PER: 0.3542
2026-01-13 21:51:27,571: t15.2023.12.10 val PER: 0.3075
2026-01-13 21:51:27,571: t15.2023.12.17 val PER: 0.3836
2026-01-13 21:51:27,571: t15.2023.12.29 val PER: 0.3727
2026-01-13 21:51:27,571: t15.2024.02.25 val PER: 0.3076
2026-01-13 21:51:27,571: t15.2024.03.08 val PER: 0.4481
2026-01-13 21:51:27,571: t15.2024.03.15 val PER: 0.4178
2026-01-13 21:51:27,571: t15.2024.03.17 val PER: 0.3877
2026-01-13 21:51:27,571: t15.2024.05.10 val PER: 0.3893
2026-01-13 21:51:27,571: t15.2024.06.14 val PER: 0.3991
2026-01-13 21:51:27,572: t15.2024.07.19 val PER: 0.5333
2026-01-13 21:51:27,572: t15.2024.07.21 val PER: 0.3510
2026-01-13 21:51:27,572: t15.2024.07.28 val PER: 0.3743
2026-01-13 21:51:27,572: t15.2025.01.10 val PER: 0.6171
2026-01-13 21:51:27,572: t15.2025.01.12 val PER: 0.4234
2026-01-13 21:51:27,572: t15.2025.03.14 val PER: 0.6080
2026-01-13 21:51:27,572: t15.2025.03.16 val PER: 0.4581
2026-01-13 21:51:27,572: t15.2025.03.30 val PER: 0.6218
2026-01-13 21:51:27,572: t15.2025.04.13 val PER: 0.4693
2026-01-13 21:51:27,573: New best val WER(5gram) 53.59% --> 35.27%
2026-01-13 21:51:27,722: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_1500
2026-01-13 21:51:36,708: Train batch 1600: loss: 36.96 grad norm: 81.41 time: 0.065
2026-01-13 21:51:54,975: Train batch 1800: loss: 35.61 grad norm: 73.03 time: 0.089
2026-01-13 21:52:13,292: Train batch 2000: loss: 33.36 grad norm: 69.07 time: 0.066
2026-01-13 21:52:13,292: Running test after training batch: 2000
2026-01-13 21:52:13,425: WER debug GT example: You can see the code at this point as well.
2026-01-13 21:52:18,331: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-13 21:52:18,404: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us and
2026-01-13 21:52:36,805: Val batch 2000: PER (avg): 0.3271 CTC Loss (avg): 32.8513 WER(5gram): 29.92% (n=256) time: 23.513
2026-01-13 21:52:36,806: WER lens: avg_true_words=5.99 avg_pred_words=5.80 max_pred_words=12
2026-01-13 21:52:36,806: t15.2023.08.13 val PER: 0.2911
2026-01-13 21:52:36,806: t15.2023.08.18 val PER: 0.2632
2026-01-13 21:52:36,806: t15.2023.08.20 val PER: 0.2462
2026-01-13 21:52:36,806: t15.2023.08.25 val PER: 0.2380
2026-01-13 21:52:36,806: t15.2023.08.27 val PER: 0.3264
2026-01-13 21:52:36,806: t15.2023.09.01 val PER: 0.2265
2026-01-13 21:52:36,806: t15.2023.09.03 val PER: 0.3325
2026-01-13 21:52:36,806: t15.2023.09.24 val PER: 0.2585
2026-01-13 21:52:36,806: t15.2023.09.29 val PER: 0.2680
2026-01-13 21:52:36,807: t15.2023.10.01 val PER: 0.3250
2026-01-13 21:52:36,807: t15.2023.10.06 val PER: 0.2379
2026-01-13 21:52:36,807: t15.2023.10.08 val PER: 0.3897
2026-01-13 21:52:36,807: t15.2023.10.13 val PER: 0.3670
2026-01-13 21:52:36,807: t15.2023.10.15 val PER: 0.3085
2026-01-13 21:52:36,807: t15.2023.10.20 val PER: 0.2919
2026-01-13 21:52:36,807: t15.2023.10.22 val PER: 0.2695
2026-01-13 21:52:36,807: t15.2023.11.03 val PER: 0.3121
2026-01-13 21:52:36,807: t15.2023.11.04 val PER: 0.0853
2026-01-13 21:52:36,808: t15.2023.11.17 val PER: 0.1695
2026-01-13 21:52:36,808: t15.2023.11.19 val PER: 0.1377
2026-01-13 21:52:36,808: t15.2023.11.26 val PER: 0.3674
2026-01-13 21:52:36,808: t15.2023.12.03 val PER: 0.3130
2026-01-13 21:52:36,808: t15.2023.12.08 val PER: 0.3169
2026-01-13 21:52:36,808: t15.2023.12.10 val PER: 0.2654
2026-01-13 21:52:36,808: t15.2023.12.17 val PER: 0.3212
2026-01-13 21:52:36,808: t15.2023.12.29 val PER: 0.3342
2026-01-13 21:52:36,808: t15.2024.02.25 val PER: 0.2851
2026-01-13 21:52:36,808: t15.2024.03.08 val PER: 0.3855
2026-01-13 21:52:36,808: t15.2024.03.15 val PER: 0.3609
2026-01-13 21:52:36,808: t15.2024.03.17 val PER: 0.3389
2026-01-13 21:52:36,809: t15.2024.05.10 val PER: 0.3358
2026-01-13 21:52:36,809: t15.2024.06.14 val PER: 0.3612
2026-01-13 21:52:36,809: t15.2024.07.19 val PER: 0.4588
2026-01-13 21:52:36,809: t15.2024.07.21 val PER: 0.2876
2026-01-13 21:52:36,809: t15.2024.07.28 val PER: 0.3213
2026-01-13 21:52:36,809: t15.2025.01.10 val PER: 0.5399
2026-01-13 21:52:36,809: t15.2025.01.12 val PER: 0.3872
2026-01-13 21:52:36,809: t15.2025.03.14 val PER: 0.5399
2026-01-13 21:52:36,809: t15.2025.03.16 val PER: 0.3953
2026-01-13 21:52:36,809: t15.2025.03.30 val PER: 0.5414
2026-01-13 21:52:36,809: t15.2025.04.13 val PER: 0.4180
2026-01-13 21:52:36,810: New best val WER(5gram) 35.27% --> 29.92%
2026-01-13 21:52:36,959: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_2000
2026-01-13 21:52:54,898: Train batch 2200: loss: 28.66 grad norm: 74.75 time: 0.060
2026-01-13 21:53:13,311: Train batch 2400: loss: 29.05 grad norm: 64.94 time: 0.051
2026-01-13 21:53:22,338: Running test after training batch: 2500
2026-01-13 21:53:22,503: WER debug GT example: You can see the code at this point as well.
2026-01-13 21:53:27,368: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-13 21:53:27,424: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-13 21:53:43,697: Val batch 2500: PER (avg): 0.3033 CTC Loss (avg): 30.2183 WER(5gram): 26.21% (n=256) time: 21.359
2026-01-13 21:53:43,698: WER lens: avg_true_words=5.99 avg_pred_words=5.86 max_pred_words=12
2026-01-13 21:53:43,698: t15.2023.08.13 val PER: 0.2786
2026-01-13 21:53:43,698: t15.2023.08.18 val PER: 0.2414
2026-01-13 21:53:43,698: t15.2023.08.20 val PER: 0.2248
2026-01-13 21:53:43,699: t15.2023.08.25 val PER: 0.1988
2026-01-13 21:53:43,699: t15.2023.08.27 val PER: 0.2990
2026-01-13 21:53:43,699: t15.2023.09.01 val PER: 0.2054
2026-01-13 21:53:43,699: t15.2023.09.03 val PER: 0.3017
2026-01-13 21:53:43,699: t15.2023.09.24 val PER: 0.2379
2026-01-13 21:53:43,699: t15.2023.09.29 val PER: 0.2565
2026-01-13 21:53:43,699: t15.2023.10.01 val PER: 0.3131
2026-01-13 21:53:43,699: t15.2023.10.06 val PER: 0.2174
2026-01-13 21:53:43,699: t15.2023.10.08 val PER: 0.3735
2026-01-13 21:53:43,699: t15.2023.10.13 val PER: 0.3701
2026-01-13 21:53:43,700: t15.2023.10.15 val PER: 0.2854
2026-01-13 21:53:43,700: t15.2023.10.20 val PER: 0.2852
2026-01-13 21:53:43,700: t15.2023.10.22 val PER: 0.2283
2026-01-13 21:53:43,700: t15.2023.11.03 val PER: 0.2897
2026-01-13 21:53:43,700: t15.2023.11.04 val PER: 0.0751
2026-01-13 21:53:43,700: t15.2023.11.17 val PER: 0.1555
2026-01-13 21:53:43,700: t15.2023.11.19 val PER: 0.1138
2026-01-13 21:53:43,700: t15.2023.11.26 val PER: 0.3486
2026-01-13 21:53:43,700: t15.2023.12.03 val PER: 0.2826
2026-01-13 21:53:43,700: t15.2023.12.08 val PER: 0.2816
2026-01-13 21:53:43,700: t15.2023.12.10 val PER: 0.2339
2026-01-13 21:53:43,700: t15.2023.12.17 val PER: 0.2838
2026-01-13 21:53:43,700: t15.2023.12.29 val PER: 0.3109
2026-01-13 21:53:43,700: t15.2024.02.25 val PER: 0.2388
2026-01-13 21:53:43,701: t15.2024.03.08 val PER: 0.3670
2026-01-13 21:53:43,701: t15.2024.03.15 val PER: 0.3546
2026-01-13 21:53:43,701: t15.2024.03.17 val PER: 0.3033
2026-01-13 21:53:43,701: t15.2024.05.10 val PER: 0.3150
2026-01-13 21:53:43,701: t15.2024.06.14 val PER: 0.3233
2026-01-13 21:53:43,701: t15.2024.07.19 val PER: 0.4344
2026-01-13 21:53:43,701: t15.2024.07.21 val PER: 0.2648
2026-01-13 21:53:43,702: t15.2024.07.28 val PER: 0.2993
2026-01-13 21:53:43,702: t15.2025.01.10 val PER: 0.5014
2026-01-13 21:53:43,702: t15.2025.01.12 val PER: 0.3533
2026-01-13 21:53:43,702: t15.2025.03.14 val PER: 0.5059
2026-01-13 21:53:43,702: t15.2025.03.16 val PER: 0.3626
2026-01-13 21:53:43,702: t15.2025.03.30 val PER: 0.5103
2026-01-13 21:53:43,702: t15.2025.04.13 val PER: 0.3923
2026-01-13 21:53:43,703: New best val WER(5gram) 29.92% --> 26.21%
2026-01-13 21:53:43,853: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_2500
2026-01-13 21:53:52,666: Train batch 2600: loss: 34.77 grad norm: 82.89 time: 0.055
2026-01-13 21:54:10,842: Train batch 2800: loss: 26.01 grad norm: 71.43 time: 0.081
2026-01-13 21:54:28,372: Train batch 3000: loss: 31.16 grad norm: 71.96 time: 0.082
2026-01-13 21:54:28,372: Running test after training batch: 3000
2026-01-13 21:54:28,484: WER debug GT example: You can see the code at this point as well.
2026-01-13 21:54:33,370: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-13 21:54:33,435: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost of
2026-01-13 21:54:48,969: Val batch 3000: PER (avg): 0.2811 CTC Loss (avg): 27.8260 WER(5gram): 25.23% (n=256) time: 20.596
2026-01-13 21:54:48,969: WER lens: avg_true_words=5.99 avg_pred_words=5.97 max_pred_words=12
2026-01-13 21:54:48,969: t15.2023.08.13 val PER: 0.2547
2026-01-13 21:54:48,970: t15.2023.08.18 val PER: 0.2347
2026-01-13 21:54:48,970: t15.2023.08.20 val PER: 0.2200
2026-01-13 21:54:48,970: t15.2023.08.25 val PER: 0.1973
2026-01-13 21:54:48,970: t15.2023.08.27 val PER: 0.2910
2026-01-13 21:54:48,970: t15.2023.09.01 val PER: 0.1875
2026-01-13 21:54:48,970: t15.2023.09.03 val PER: 0.2755
2026-01-13 21:54:48,970: t15.2023.09.24 val PER: 0.2112
2026-01-13 21:54:48,970: t15.2023.09.29 val PER: 0.2323
2026-01-13 21:54:48,970: t15.2023.10.01 val PER: 0.2939
2026-01-13 21:54:48,971: t15.2023.10.06 val PER: 0.1916
2026-01-13 21:54:48,971: t15.2023.10.08 val PER: 0.3451
2026-01-13 21:54:48,971: t15.2023.10.13 val PER: 0.3367
2026-01-13 21:54:48,971: t15.2023.10.15 val PER: 0.2696
2026-01-13 21:54:48,971: t15.2023.10.20 val PER: 0.2450
2026-01-13 21:54:48,971: t15.2023.10.22 val PER: 0.1949
2026-01-13 21:54:48,971: t15.2023.11.03 val PER: 0.2795
2026-01-13 21:54:48,971: t15.2023.11.04 val PER: 0.0819
2026-01-13 21:54:48,971: t15.2023.11.17 val PER: 0.1322
2026-01-13 21:54:48,971: t15.2023.11.19 val PER: 0.1218
2026-01-13 21:54:48,971: t15.2023.11.26 val PER: 0.3000
2026-01-13 21:54:48,971: t15.2023.12.03 val PER: 0.2605
2026-01-13 21:54:48,971: t15.2023.12.08 val PER: 0.2583
2026-01-13 21:54:48,971: t15.2023.12.10 val PER: 0.2076
2026-01-13 21:54:48,971: t15.2023.12.17 val PER: 0.2827
2026-01-13 21:54:48,972: t15.2023.12.29 val PER: 0.2855
2026-01-13 21:54:48,972: t15.2024.02.25 val PER: 0.2360
2026-01-13 21:54:48,972: t15.2024.03.08 val PER: 0.3514
2026-01-13 21:54:48,972: t15.2024.03.15 val PER: 0.3271
2026-01-13 21:54:48,972: t15.2024.03.17 val PER: 0.2887
2026-01-13 21:54:48,972: t15.2024.05.10 val PER: 0.3031
2026-01-13 21:54:48,972: t15.2024.06.14 val PER: 0.3028
2026-01-13 21:54:48,972: t15.2024.07.19 val PER: 0.4015
2026-01-13 21:54:48,972: t15.2024.07.21 val PER: 0.2366
2026-01-13 21:54:48,972: t15.2024.07.28 val PER: 0.2787
2026-01-13 21:54:48,972: t15.2025.01.10 val PER: 0.4752
2026-01-13 21:54:48,972: t15.2025.01.12 val PER: 0.3249
2026-01-13 21:54:48,972: t15.2025.03.14 val PER: 0.4586
2026-01-13 21:54:48,972: t15.2025.03.16 val PER: 0.3154
2026-01-13 21:54:48,972: t15.2025.03.30 val PER: 0.4862
2026-01-13 21:54:48,972: t15.2025.04.13 val PER: 0.3623
2026-01-13 21:54:48,974: New best val WER(5gram) 26.21% --> 25.23%
2026-01-13 21:54:49,126: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_3000
2026-01-13 21:55:07,512: Train batch 3200: loss: 26.38 grad norm: 65.55 time: 0.076
2026-01-13 21:55:25,907: Train batch 3400: loss: 17.91 grad norm: 53.51 time: 0.048
2026-01-13 21:55:34,874: Running test after training batch: 3500
2026-01-13 21:55:35,012: WER debug GT example: You can see the code at this point as well.
2026-01-13 21:55:40,162: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-13 21:55:40,223: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us in
2026-01-13 21:55:55,178: Val batch 3500: PER (avg): 0.2671 CTC Loss (avg): 26.6437 WER(5gram): 23.08% (n=256) time: 20.304
2026-01-13 21:55:55,179: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-13 21:55:55,179: t15.2023.08.13 val PER: 0.2328
2026-01-13 21:55:55,179: t15.2023.08.18 val PER: 0.2104
2026-01-13 21:55:55,179: t15.2023.08.20 val PER: 0.2129
2026-01-13 21:55:55,180: t15.2023.08.25 val PER: 0.1883
2026-01-13 21:55:55,180: t15.2023.08.27 val PER: 0.2621
2026-01-13 21:55:55,180: t15.2023.09.01 val PER: 0.1834
2026-01-13 21:55:55,180: t15.2023.09.03 val PER: 0.2601
2026-01-13 21:55:55,180: t15.2023.09.24 val PER: 0.2087
2026-01-13 21:55:55,180: t15.2023.09.29 val PER: 0.2234
2026-01-13 21:55:55,180: t15.2023.10.01 val PER: 0.2715
2026-01-13 21:55:55,180: t15.2023.10.06 val PER: 0.1798
2026-01-13 21:55:55,180: t15.2023.10.08 val PER: 0.3356
2026-01-13 21:55:55,180: t15.2023.10.13 val PER: 0.3227
2026-01-13 21:55:55,180: t15.2023.10.15 val PER: 0.2551
2026-01-13 21:55:55,180: t15.2023.10.20 val PER: 0.2483
2026-01-13 21:55:55,181: t15.2023.10.22 val PER: 0.2194
2026-01-13 21:55:55,181: t15.2023.11.03 val PER: 0.2626
2026-01-13 21:55:55,181: t15.2023.11.04 val PER: 0.0853
2026-01-13 21:55:55,181: t15.2023.11.17 val PER: 0.1213
2026-01-13 21:55:55,181: t15.2023.11.19 val PER: 0.1158
2026-01-13 21:55:55,181: t15.2023.11.26 val PER: 0.2942
2026-01-13 21:55:55,181: t15.2023.12.03 val PER: 0.2395
2026-01-13 21:55:55,181: t15.2023.12.08 val PER: 0.2383
2026-01-13 21:55:55,181: t15.2023.12.10 val PER: 0.2050
2026-01-13 21:55:55,181: t15.2023.12.17 val PER: 0.2516
2026-01-13 21:55:55,181: t15.2023.12.29 val PER: 0.2629
2026-01-13 21:55:55,182: t15.2024.02.25 val PER: 0.2093
2026-01-13 21:55:55,182: t15.2024.03.08 val PER: 0.3414
2026-01-13 21:55:55,182: t15.2024.03.15 val PER: 0.3164
2026-01-13 21:55:55,182: t15.2024.03.17 val PER: 0.2734
2026-01-13 21:55:55,182: t15.2024.05.10 val PER: 0.2689
2026-01-13 21:55:55,182: t15.2024.06.14 val PER: 0.2823
2026-01-13 21:55:55,182: t15.2024.07.19 val PER: 0.3968
2026-01-13 21:55:55,182: t15.2024.07.21 val PER: 0.2345
2026-01-13 21:55:55,182: t15.2024.07.28 val PER: 0.2713
2026-01-13 21:55:55,182: t15.2025.01.10 val PER: 0.4656
2026-01-13 21:55:55,182: t15.2025.01.12 val PER: 0.2910
2026-01-13 21:55:55,182: t15.2025.03.14 val PER: 0.4231
2026-01-13 21:55:55,182: t15.2025.03.16 val PER: 0.3377
2026-01-13 21:55:55,182: t15.2025.03.30 val PER: 0.4333
2026-01-13 21:55:55,183: t15.2025.04.13 val PER: 0.3424
2026-01-13 21:55:55,184: New best val WER(5gram) 25.23% --> 23.08%
2026-01-13 21:55:55,329: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_3500
2026-01-13 21:56:04,223: Train batch 3600: loss: 22.15 grad norm: 62.16 time: 0.066
2026-01-13 21:56:21,701: Train batch 3800: loss: 25.81 grad norm: 71.87 time: 0.066
2026-01-13 21:56:39,450: Train batch 4000: loss: 19.20 grad norm: 51.99 time: 0.056
2026-01-13 21:56:39,450: Running test after training batch: 4000
2026-01-13 21:56:39,599: WER debug GT example: You can see the code at this point as well.
2026-01-13 21:56:44,444: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-13 21:56:44,499: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-13 21:56:58,565: Val batch 4000: PER (avg): 0.2467 CTC Loss (avg): 24.2462 WER(5gram): 24.38% (n=256) time: 19.115
2026-01-13 21:56:58,566: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-13 21:56:58,566: t15.2023.08.13 val PER: 0.2235
2026-01-13 21:56:58,566: t15.2023.08.18 val PER: 0.2054
2026-01-13 21:56:58,567: t15.2023.08.20 val PER: 0.2033
2026-01-13 21:56:58,567: t15.2023.08.25 val PER: 0.1551
2026-01-13 21:56:58,567: t15.2023.08.27 val PER: 0.2830
2026-01-13 21:56:58,567: t15.2023.09.01 val PER: 0.1607
2026-01-13 21:56:58,567: t15.2023.09.03 val PER: 0.2435
2026-01-13 21:56:58,567: t15.2023.09.24 val PER: 0.1845
2026-01-13 21:56:58,567: t15.2023.09.29 val PER: 0.1978
2026-01-13 21:56:58,567: t15.2023.10.01 val PER: 0.2510
2026-01-13 21:56:58,567: t15.2023.10.06 val PER: 0.1464
2026-01-13 21:56:58,568: t15.2023.10.08 val PER: 0.3261
2026-01-13 21:56:58,568: t15.2023.10.13 val PER: 0.2979
2026-01-13 21:56:58,568: t15.2023.10.15 val PER: 0.2413
2026-01-13 21:56:58,568: t15.2023.10.20 val PER: 0.2550
2026-01-13 21:56:58,568: t15.2023.10.22 val PER: 0.1949
2026-01-13 21:56:58,568: t15.2023.11.03 val PER: 0.2381
2026-01-13 21:56:58,568: t15.2023.11.04 val PER: 0.0819
2026-01-13 21:56:58,568: t15.2023.11.17 val PER: 0.1011
2026-01-13 21:56:58,568: t15.2023.11.19 val PER: 0.0978
2026-01-13 21:56:58,568: t15.2023.11.26 val PER: 0.2609
2026-01-13 21:56:58,569: t15.2023.12.03 val PER: 0.2111
2026-01-13 21:56:58,569: t15.2023.12.08 val PER: 0.2157
2026-01-13 21:56:58,569: t15.2023.12.10 val PER: 0.1958
2026-01-13 21:56:58,569: t15.2023.12.17 val PER: 0.2370
2026-01-13 21:56:58,569: t15.2023.12.29 val PER: 0.2533
2026-01-13 21:56:58,569: t15.2024.02.25 val PER: 0.2149
2026-01-13 21:56:58,569: t15.2024.03.08 val PER: 0.3215
2026-01-13 21:56:58,569: t15.2024.03.15 val PER: 0.3008
2026-01-13 21:56:58,569: t15.2024.03.17 val PER: 0.2566
2026-01-13 21:56:58,569: t15.2024.05.10 val PER: 0.2615
2026-01-13 21:56:58,569: t15.2024.06.14 val PER: 0.2539
2026-01-13 21:56:58,569: t15.2024.07.19 val PER: 0.3520
2026-01-13 21:56:58,570: t15.2024.07.21 val PER: 0.1903
2026-01-13 21:56:58,570: t15.2024.07.28 val PER: 0.2331
2026-01-13 21:56:58,570: t15.2025.01.10 val PER: 0.4256
2026-01-13 21:56:58,570: t15.2025.01.12 val PER: 0.2741
2026-01-13 21:56:58,570: t15.2025.03.14 val PER: 0.4216
2026-01-13 21:56:58,570: t15.2025.03.16 val PER: 0.3010
2026-01-13 21:56:58,570: t15.2025.03.30 val PER: 0.4172
2026-01-13 21:56:58,570: t15.2025.04.13 val PER: 0.3381
2026-01-13 21:56:58,708: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_4000
2026-01-13 21:57:16,951: Train batch 4200: loss: 22.44 grad norm: 66.05 time: 0.079
2026-01-13 21:57:35,463: Train batch 4400: loss: 16.55 grad norm: 53.96 time: 0.066
2026-01-13 21:57:44,540: Running test after training batch: 4500
2026-01-13 21:57:44,683: WER debug GT example: You can see the code at this point as well.
2026-01-13 21:57:49,500: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 21:57:49,547: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost at
2026-01-13 21:58:02,950: Val batch 4500: PER (avg): 0.2354 CTC Loss (avg): 23.0839 WER(5gram): 23.27% (n=256) time: 18.410
2026-01-13 21:58:02,951: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-13 21:58:02,951: t15.2023.08.13 val PER: 0.2079
2026-01-13 21:58:02,952: t15.2023.08.18 val PER: 0.1769
2026-01-13 21:58:02,952: t15.2023.08.20 val PER: 0.1994
2026-01-13 21:58:02,952: t15.2023.08.25 val PER: 0.1446
2026-01-13 21:58:02,952: t15.2023.08.27 val PER: 0.2572
2026-01-13 21:58:02,952: t15.2023.09.01 val PER: 0.1445
2026-01-13 21:58:02,952: t15.2023.09.03 val PER: 0.2387
2026-01-13 21:58:02,952: t15.2023.09.24 val PER: 0.1820
2026-01-13 21:58:02,952: t15.2023.09.29 val PER: 0.1921
2026-01-13 21:58:02,952: t15.2023.10.01 val PER: 0.2609
2026-01-13 21:58:02,952: t15.2023.10.06 val PER: 0.1550
2026-01-13 21:58:02,952: t15.2023.10.08 val PER: 0.3112
2026-01-13 21:58:02,952: t15.2023.10.13 val PER: 0.2940
2026-01-13 21:58:02,952: t15.2023.10.15 val PER: 0.2235
2026-01-13 21:58:02,953: t15.2023.10.20 val PER: 0.2416
2026-01-13 21:58:02,953: t15.2023.10.22 val PER: 0.1793
2026-01-13 21:58:02,953: t15.2023.11.03 val PER: 0.2408
2026-01-13 21:58:02,953: t15.2023.11.04 val PER: 0.0717
2026-01-13 21:58:02,953: t15.2023.11.17 val PER: 0.0995
2026-01-13 21:58:02,953: t15.2023.11.19 val PER: 0.0838
2026-01-13 21:58:02,953: t15.2023.11.26 val PER: 0.2703
2026-01-13 21:58:02,953: t15.2023.12.03 val PER: 0.2122
2026-01-13 21:58:02,953: t15.2023.12.08 val PER: 0.2017
2026-01-13 21:58:02,953: t15.2023.12.10 val PER: 0.1813
2026-01-13 21:58:02,953: t15.2023.12.17 val PER: 0.2380
2026-01-13 21:58:02,953: t15.2023.12.29 val PER: 0.2443
2026-01-13 21:58:02,953: t15.2024.02.25 val PER: 0.1938
2026-01-13 21:58:02,954: t15.2024.03.08 val PER: 0.3129
2026-01-13 21:58:02,954: t15.2024.03.15 val PER: 0.2827
2026-01-13 21:58:02,954: t15.2024.03.17 val PER: 0.2413
2026-01-13 21:58:02,954: t15.2024.05.10 val PER: 0.2481
2026-01-13 21:58:02,954: t15.2024.06.14 val PER: 0.2397
2026-01-13 21:58:02,955: t15.2024.07.19 val PER: 0.3375
2026-01-13 21:58:02,955: t15.2024.07.21 val PER: 0.1621
2026-01-13 21:58:02,955: t15.2024.07.28 val PER: 0.2191
2026-01-13 21:58:02,955: t15.2025.01.10 val PER: 0.4160
2026-01-13 21:58:02,955: t15.2025.01.12 val PER: 0.2540
2026-01-13 21:58:02,955: t15.2025.03.14 val PER: 0.4112
2026-01-13 21:58:02,955: t15.2025.03.16 val PER: 0.2880
2026-01-13 21:58:02,955: t15.2025.03.30 val PER: 0.3805
2026-01-13 21:58:02,955: t15.2025.04.13 val PER: 0.2996
2026-01-13 21:58:03,106: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_4500
2026-01-13 21:58:12,230: Train batch 4600: loss: 20.35 grad norm: 68.72 time: 0.063
2026-01-13 21:58:30,614: Train batch 4800: loss: 12.79 grad norm: 52.65 time: 0.063
2026-01-13 21:58:49,142: Train batch 5000: loss: 31.04 grad norm: 84.74 time: 0.065
2026-01-13 21:58:49,142: Running test after training batch: 5000
2026-01-13 21:58:49,293: WER debug GT example: You can see the code at this point as well.
2026-01-13 21:58:54,584: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 21:58:54,656: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost net
2026-01-13 21:59:08,980: Val batch 5000: PER (avg): 0.2244 CTC Loss (avg): 21.6817 WER(5gram): 23.73% (n=256) time: 19.838
2026-01-13 21:59:08,981: WER lens: avg_true_words=5.99 avg_pred_words=6.23 max_pred_words=12
2026-01-13 21:59:08,981: t15.2023.08.13 val PER: 0.1923
2026-01-13 21:59:08,981: t15.2023.08.18 val PER: 0.1651
2026-01-13 21:59:08,981: t15.2023.08.20 val PER: 0.1692
2026-01-13 21:59:08,981: t15.2023.08.25 val PER: 0.1340
2026-01-13 21:59:08,981: t15.2023.08.27 val PER: 0.2428
2026-01-13 21:59:08,981: t15.2023.09.01 val PER: 0.1347
2026-01-13 21:59:08,981: t15.2023.09.03 val PER: 0.2257
2026-01-13 21:59:08,981: t15.2023.09.24 val PER: 0.1735
2026-01-13 21:59:08,981: t15.2023.09.29 val PER: 0.1755
2026-01-13 21:59:08,982: t15.2023.10.01 val PER: 0.2371
2026-01-13 21:59:08,982: t15.2023.10.06 val PER: 0.1442
2026-01-13 21:59:08,982: t15.2023.10.08 val PER: 0.2977
2026-01-13 21:59:08,982: t15.2023.10.13 val PER: 0.2816
2026-01-13 21:59:08,982: t15.2023.10.15 val PER: 0.2274
2026-01-13 21:59:08,982: t15.2023.10.20 val PER: 0.2282
2026-01-13 21:59:08,982: t15.2023.10.22 val PER: 0.1659
2026-01-13 21:59:08,982: t15.2023.11.03 val PER: 0.2218
2026-01-13 21:59:08,982: t15.2023.11.04 val PER: 0.0444
2026-01-13 21:59:08,982: t15.2023.11.17 val PER: 0.0809
2026-01-13 21:59:08,982: t15.2023.11.19 val PER: 0.0778
2026-01-13 21:59:08,982: t15.2023.11.26 val PER: 0.2268
2026-01-13 21:59:08,982: t15.2023.12.03 val PER: 0.1891
2026-01-13 21:59:08,982: t15.2023.12.08 val PER: 0.1844
2026-01-13 21:59:08,982: t15.2023.12.10 val PER: 0.1551
2026-01-13 21:59:08,983: t15.2023.12.17 val PER: 0.2328
2026-01-13 21:59:08,983: t15.2023.12.29 val PER: 0.2141
2026-01-13 21:59:08,983: t15.2024.02.25 val PER: 0.1938
2026-01-13 21:59:08,983: t15.2024.03.08 val PER: 0.3215
2026-01-13 21:59:08,983: t15.2024.03.15 val PER: 0.2814
2026-01-13 21:59:08,983: t15.2024.03.17 val PER: 0.2329
2026-01-13 21:59:08,983: t15.2024.05.10 val PER: 0.2348
2026-01-13 21:59:08,983: t15.2024.06.14 val PER: 0.2413
2026-01-13 21:59:08,983: t15.2024.07.19 val PER: 0.3362
2026-01-13 21:59:08,984: t15.2024.07.21 val PER: 0.1786
2026-01-13 21:59:08,984: t15.2024.07.28 val PER: 0.2206
2026-01-13 21:59:08,984: t15.2025.01.10 val PER: 0.3953
2026-01-13 21:59:08,984: t15.2025.01.12 val PER: 0.2510
2026-01-13 21:59:08,984: t15.2025.03.14 val PER: 0.4038
2026-01-13 21:59:08,984: t15.2025.03.16 val PER: 0.2592
2026-01-13 21:59:08,984: t15.2025.03.30 val PER: 0.4115
2026-01-13 21:59:08,984: t15.2025.04.13 val PER: 0.3081
2026-01-13 21:59:09,135: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_5000
2026-01-13 21:59:27,512: Train batch 5200: loss: 16.00 grad norm: 56.98 time: 0.051
2026-01-13 21:59:45,404: Train batch 5400: loss: 17.20 grad norm: 57.88 time: 0.068
2026-01-13 21:59:54,487: Running test after training batch: 5500
2026-01-13 21:59:54,588: WER debug GT example: You can see the code at this point as well.
2026-01-13 21:59:59,597: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-13 21:59:59,647: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-13 22:00:14,842: Val batch 5500: PER (avg): 0.2125 CTC Loss (avg): 20.6720 WER(5gram): 21.90% (n=256) time: 20.354
2026-01-13 22:00:14,842: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=12
2026-01-13 22:00:14,842: t15.2023.08.13 val PER: 0.1861
2026-01-13 22:00:14,842: t15.2023.08.18 val PER: 0.1643
2026-01-13 22:00:14,843: t15.2023.08.20 val PER: 0.1620
2026-01-13 22:00:14,843: t15.2023.08.25 val PER: 0.1355
2026-01-13 22:00:14,843: t15.2023.08.27 val PER: 0.2347
2026-01-13 22:00:14,843: t15.2023.09.01 val PER: 0.1364
2026-01-13 22:00:14,843: t15.2023.09.03 val PER: 0.2209
2026-01-13 22:00:14,843: t15.2023.09.24 val PER: 0.1723
2026-01-13 22:00:14,843: t15.2023.09.29 val PER: 0.1704
2026-01-13 22:00:14,843: t15.2023.10.01 val PER: 0.2252
2026-01-13 22:00:14,843: t15.2023.10.06 val PER: 0.1324
2026-01-13 22:00:14,843: t15.2023.10.08 val PER: 0.2882
2026-01-13 22:00:14,843: t15.2023.10.13 val PER: 0.2770
2026-01-13 22:00:14,844: t15.2023.10.15 val PER: 0.2083
2026-01-13 22:00:14,844: t15.2023.10.20 val PER: 0.2383
2026-01-13 22:00:14,844: t15.2023.10.22 val PER: 0.1559
2026-01-13 22:00:14,844: t15.2023.11.03 val PER: 0.2218
2026-01-13 22:00:14,844: t15.2023.11.04 val PER: 0.0683
2026-01-13 22:00:14,844: t15.2023.11.17 val PER: 0.0747
2026-01-13 22:00:14,844: t15.2023.11.19 val PER: 0.0599
2026-01-13 22:00:14,844: t15.2023.11.26 val PER: 0.2145
2026-01-13 22:00:14,844: t15.2023.12.03 val PER: 0.1723
2026-01-13 22:00:14,844: t15.2023.12.08 val PER: 0.1771
2026-01-13 22:00:14,844: t15.2023.12.10 val PER: 0.1524
2026-01-13 22:00:14,844: t15.2023.12.17 val PER: 0.2183
2026-01-13 22:00:14,844: t15.2023.12.29 val PER: 0.2100
2026-01-13 22:00:14,845: t15.2024.02.25 val PER: 0.1826
2026-01-13 22:00:14,845: t15.2024.03.08 val PER: 0.2859
2026-01-13 22:00:14,845: t15.2024.03.15 val PER: 0.2489
2026-01-13 22:00:14,845: t15.2024.03.17 val PER: 0.2225
2026-01-13 22:00:14,845: t15.2024.05.10 val PER: 0.2288
2026-01-13 22:00:14,845: t15.2024.06.14 val PER: 0.2271
2026-01-13 22:00:14,845: t15.2024.07.19 val PER: 0.3204
2026-01-13 22:00:14,845: t15.2024.07.21 val PER: 0.1607
2026-01-13 22:00:14,845: t15.2024.07.28 val PER: 0.2059
2026-01-13 22:00:14,845: t15.2025.01.10 val PER: 0.3912
2026-01-13 22:00:14,845: t15.2025.01.12 val PER: 0.2240
2026-01-13 22:00:14,845: t15.2025.03.14 val PER: 0.3536
2026-01-13 22:00:14,845: t15.2025.03.16 val PER: 0.2683
2026-01-13 22:00:14,845: t15.2025.03.30 val PER: 0.3598
2026-01-13 22:00:14,845: t15.2025.04.13 val PER: 0.2825
2026-01-13 22:00:14,847: New best val WER(5gram) 23.08% --> 21.90%
2026-01-13 22:00:14,997: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_5500
2026-01-13 22:00:24,357: Train batch 5600: loss: 18.92 grad norm: 67.46 time: 0.061
2026-01-13 22:00:43,042: Train batch 5800: loss: 13.76 grad norm: 57.26 time: 0.081
2026-01-13 22:01:01,164: Train batch 6000: loss: 14.07 grad norm: 56.93 time: 0.049
2026-01-13 22:01:01,165: Running test after training batch: 6000
2026-01-13 22:01:01,344: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:01:06,182: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:01:06,241: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-13 22:01:18,528: Val batch 6000: PER (avg): 0.2096 CTC Loss (avg): 20.3396 WER(5gram): 22.82% (n=256) time: 17.363
2026-01-13 22:01:18,528: WER lens: avg_true_words=5.99 avg_pred_words=6.26 max_pred_words=12
2026-01-13 22:01:18,528: t15.2023.08.13 val PER: 0.1694
2026-01-13 22:01:18,528: t15.2023.08.18 val PER: 0.1635
2026-01-13 22:01:18,529: t15.2023.08.20 val PER: 0.1668
2026-01-13 22:01:18,529: t15.2023.08.25 val PER: 0.1205
2026-01-13 22:01:18,529: t15.2023.08.27 val PER: 0.2395
2026-01-13 22:01:18,529: t15.2023.09.01 val PER: 0.1299
2026-01-13 22:01:18,529: t15.2023.09.03 val PER: 0.2138
2026-01-13 22:01:18,529: t15.2023.09.24 val PER: 0.1638
2026-01-13 22:01:18,529: t15.2023.09.29 val PER: 0.1685
2026-01-13 22:01:18,529: t15.2023.10.01 val PER: 0.2246
2026-01-13 22:01:18,529: t15.2023.10.06 val PER: 0.1216
2026-01-13 22:01:18,530: t15.2023.10.08 val PER: 0.2950
2026-01-13 22:01:18,530: t15.2023.10.13 val PER: 0.2692
2026-01-13 22:01:18,530: t15.2023.10.15 val PER: 0.2090
2026-01-13 22:01:18,530: t15.2023.10.20 val PER: 0.2215
2026-01-13 22:01:18,530: t15.2023.10.22 val PER: 0.1670
2026-01-13 22:01:18,530: t15.2023.11.03 val PER: 0.2225
2026-01-13 22:01:18,530: t15.2023.11.04 val PER: 0.0580
2026-01-13 22:01:18,531: t15.2023.11.17 val PER: 0.0731
2026-01-13 22:01:18,531: t15.2023.11.19 val PER: 0.0798
2026-01-13 22:01:18,531: t15.2023.11.26 val PER: 0.2181
2026-01-13 22:01:18,531: t15.2023.12.03 val PER: 0.1691
2026-01-13 22:01:18,531: t15.2023.12.08 val PER: 0.1658
2026-01-13 22:01:18,531: t15.2023.12.10 val PER: 0.1353
2026-01-13 22:01:18,531: t15.2023.12.17 val PER: 0.2006
2026-01-13 22:01:18,531: t15.2023.12.29 val PER: 0.2231
2026-01-13 22:01:18,531: t15.2024.02.25 val PER: 0.1699
2026-01-13 22:01:18,531: t15.2024.03.08 val PER: 0.2902
2026-01-13 22:01:18,531: t15.2024.03.15 val PER: 0.2620
2026-01-13 22:01:18,532: t15.2024.03.17 val PER: 0.2022
2026-01-13 22:01:18,532: t15.2024.05.10 val PER: 0.2140
2026-01-13 22:01:18,532: t15.2024.06.14 val PER: 0.2240
2026-01-13 22:01:18,532: t15.2024.07.19 val PER: 0.3039
2026-01-13 22:01:18,532: t15.2024.07.21 val PER: 0.1614
2026-01-13 22:01:18,532: t15.2024.07.28 val PER: 0.1993
2026-01-13 22:01:18,532: t15.2025.01.10 val PER: 0.3829
2026-01-13 22:01:18,533: t15.2025.01.12 val PER: 0.2256
2026-01-13 22:01:18,533: t15.2025.03.14 val PER: 0.3920
2026-01-13 22:01:18,533: t15.2025.03.16 val PER: 0.2461
2026-01-13 22:01:18,533: t15.2025.03.30 val PER: 0.3759
2026-01-13 22:01:18,533: t15.2025.04.13 val PER: 0.2682
2026-01-13 22:01:18,673: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_6000
2026-01-13 22:01:36,454: Train batch 6200: loss: 15.90 grad norm: 59.61 time: 0.069
2026-01-13 22:01:54,242: Train batch 6400: loss: 18.45 grad norm: 66.80 time: 0.062
2026-01-13 22:02:02,986: Running test after training batch: 6500
2026-01-13 22:02:03,122: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:02:08,223: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:02:08,274: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-13 22:02:20,054: Val batch 6500: PER (avg): 0.2005 CTC Loss (avg): 19.8877 WER(5gram): 20.21% (n=256) time: 17.067
2026-01-13 22:02:20,054: WER lens: avg_true_words=5.99 avg_pred_words=6.22 max_pred_words=12
2026-01-13 22:02:20,054: t15.2023.08.13 val PER: 0.1653
2026-01-13 22:02:20,054: t15.2023.08.18 val PER: 0.1450
2026-01-13 22:02:20,055: t15.2023.08.20 val PER: 0.1541
2026-01-13 22:02:20,055: t15.2023.08.25 val PER: 0.1084
2026-01-13 22:02:20,055: t15.2023.08.27 val PER: 0.2331
2026-01-13 22:02:20,055: t15.2023.09.01 val PER: 0.1128
2026-01-13 22:02:20,055: t15.2023.09.03 val PER: 0.1960
2026-01-13 22:02:20,055: t15.2023.09.24 val PER: 0.1614
2026-01-13 22:02:20,055: t15.2023.09.29 val PER: 0.1595
2026-01-13 22:02:20,055: t15.2023.10.01 val PER: 0.2173
2026-01-13 22:02:20,055: t15.2023.10.06 val PER: 0.1152
2026-01-13 22:02:20,056: t15.2023.10.08 val PER: 0.2936
2026-01-13 22:02:20,056: t15.2023.10.13 val PER: 0.2599
2026-01-13 22:02:20,056: t15.2023.10.15 val PER: 0.2083
2026-01-13 22:02:20,056: t15.2023.10.20 val PER: 0.2148
2026-01-13 22:02:20,056: t15.2023.10.22 val PER: 0.1437
2026-01-13 22:02:20,056: t15.2023.11.03 val PER: 0.2144
2026-01-13 22:02:20,056: t15.2023.11.04 val PER: 0.0444
2026-01-13 22:02:20,056: t15.2023.11.17 val PER: 0.0700
2026-01-13 22:02:20,056: t15.2023.11.19 val PER: 0.0699
2026-01-13 22:02:20,057: t15.2023.11.26 val PER: 0.1949
2026-01-13 22:02:20,057: t15.2023.12.03 val PER: 0.1765
2026-01-13 22:02:20,057: t15.2023.12.08 val PER: 0.1658
2026-01-13 22:02:20,057: t15.2023.12.10 val PER: 0.1459
2026-01-13 22:02:20,057: t15.2023.12.17 val PER: 0.1902
2026-01-13 22:02:20,057: t15.2023.12.29 val PER: 0.1949
2026-01-13 22:02:20,057: t15.2024.02.25 val PER: 0.1629
2026-01-13 22:02:20,057: t15.2024.03.08 val PER: 0.2888
2026-01-13 22:02:20,057: t15.2024.03.15 val PER: 0.2633
2026-01-13 22:02:20,057: t15.2024.03.17 val PER: 0.1925
2026-01-13 22:02:20,057: t15.2024.05.10 val PER: 0.2125
2026-01-13 22:02:20,057: t15.2024.06.14 val PER: 0.2161
2026-01-13 22:02:20,058: t15.2024.07.19 val PER: 0.2927
2026-01-13 22:02:20,058: t15.2024.07.21 val PER: 0.1469
2026-01-13 22:02:20,058: t15.2024.07.28 val PER: 0.1956
2026-01-13 22:02:20,058: t15.2025.01.10 val PER: 0.3540
2026-01-13 22:02:20,058: t15.2025.01.12 val PER: 0.2163
2026-01-13 22:02:20,058: t15.2025.03.14 val PER: 0.3802
2026-01-13 22:02:20,058: t15.2025.03.16 val PER: 0.2421
2026-01-13 22:02:20,058: t15.2025.03.30 val PER: 0.3540
2026-01-13 22:02:20,058: t15.2025.04.13 val PER: 0.2753
2026-01-13 22:02:20,059: New best val WER(5gram) 21.90% --> 20.21%
2026-01-13 22:02:20,206: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_6500
2026-01-13 22:02:28,764: Train batch 6600: loss: 12.16 grad norm: 52.25 time: 0.044
2026-01-13 22:02:46,767: Train batch 6800: loss: 14.81 grad norm: 54.15 time: 0.049
2026-01-13 22:03:04,506: Train batch 7000: loss: 16.34 grad norm: 61.20 time: 0.060
2026-01-13 22:03:04,506: Running test after training batch: 7000
2026-01-13 22:03:04,663: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:03:10,128: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:03:10,175: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost net
2026-01-13 22:03:25,744: Val batch 7000: PER (avg): 0.1905 CTC Loss (avg): 18.7358 WER(5gram): 19.43% (n=256) time: 21.237
2026-01-13 22:03:25,744: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-13 22:03:25,744: t15.2023.08.13 val PER: 0.1435
2026-01-13 22:03:25,745: t15.2023.08.18 val PER: 0.1375
2026-01-13 22:03:25,745: t15.2023.08.20 val PER: 0.1469
2026-01-13 22:03:25,745: t15.2023.08.25 val PER: 0.1039
2026-01-13 22:03:25,745: t15.2023.08.27 val PER: 0.2122
2026-01-13 22:03:25,745: t15.2023.09.01 val PER: 0.1055
2026-01-13 22:03:25,745: t15.2023.09.03 val PER: 0.1853
2026-01-13 22:03:25,745: t15.2023.09.24 val PER: 0.1505
2026-01-13 22:03:25,746: t15.2023.09.29 val PER: 0.1608
2026-01-13 22:03:25,746: t15.2023.10.01 val PER: 0.2074
2026-01-13 22:03:25,746: t15.2023.10.06 val PER: 0.1066
2026-01-13 22:03:25,746: t15.2023.10.08 val PER: 0.2720
2026-01-13 22:03:25,746: t15.2023.10.13 val PER: 0.2537
2026-01-13 22:03:25,746: t15.2023.10.15 val PER: 0.1846
2026-01-13 22:03:25,746: t15.2023.10.20 val PER: 0.2181
2026-01-13 22:03:25,746: t15.2023.10.22 val PER: 0.1526
2026-01-13 22:03:25,747: t15.2023.11.03 val PER: 0.2035
2026-01-13 22:03:25,747: t15.2023.11.04 val PER: 0.0478
2026-01-13 22:03:25,747: t15.2023.11.17 val PER: 0.0653
2026-01-13 22:03:25,747: t15.2023.11.19 val PER: 0.0599
2026-01-13 22:03:25,747: t15.2023.11.26 val PER: 0.1812
2026-01-13 22:03:25,747: t15.2023.12.03 val PER: 0.1534
2026-01-13 22:03:25,747: t15.2023.12.08 val PER: 0.1478
2026-01-13 22:03:25,747: t15.2023.12.10 val PER: 0.1301
2026-01-13 22:03:25,747: t15.2023.12.17 val PER: 0.1840
2026-01-13 22:03:25,747: t15.2023.12.29 val PER: 0.1915
2026-01-13 22:03:25,747: t15.2024.02.25 val PER: 0.1545
2026-01-13 22:03:25,747: t15.2024.03.08 val PER: 0.2575
2026-01-13 22:03:25,747: t15.2024.03.15 val PER: 0.2452
2026-01-13 22:03:25,747: t15.2024.03.17 val PER: 0.1994
2026-01-13 22:03:25,747: t15.2024.05.10 val PER: 0.1961
2026-01-13 22:03:25,747: t15.2024.06.14 val PER: 0.2082
2026-01-13 22:03:25,748: t15.2024.07.19 val PER: 0.3019
2026-01-13 22:03:25,748: t15.2024.07.21 val PER: 0.1379
2026-01-13 22:03:25,748: t15.2024.07.28 val PER: 0.1669
2026-01-13 22:03:25,748: t15.2025.01.10 val PER: 0.3540
2026-01-13 22:03:25,748: t15.2025.01.12 val PER: 0.2048
2026-01-13 22:03:25,748: t15.2025.03.14 val PER: 0.3565
2026-01-13 22:03:25,748: t15.2025.03.16 val PER: 0.2421
2026-01-13 22:03:25,748: t15.2025.03.30 val PER: 0.3598
2026-01-13 22:03:25,748: t15.2025.04.13 val PER: 0.2496
2026-01-13 22:03:25,749: New best val WER(5gram) 20.21% --> 19.43%
2026-01-13 22:03:25,909: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_7000
2026-01-13 22:03:44,186: Train batch 7200: loss: 13.65 grad norm: 55.76 time: 0.078
2026-01-13 22:04:02,385: Train batch 7400: loss: 12.46 grad norm: 52.41 time: 0.075
2026-01-13 22:04:11,437: Running test after training batch: 7500
2026-01-13 22:04:11,539: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:04:16,555: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:04:16,624: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-13 22:04:31,659: Val batch 7500: PER (avg): 0.1869 CTC Loss (avg): 18.5995 WER(5gram): 18.25% (n=256) time: 20.222
2026-01-13 22:04:31,660: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-13 22:04:31,660: t15.2023.08.13 val PER: 0.1528
2026-01-13 22:04:31,660: t15.2023.08.18 val PER: 0.1383
2026-01-13 22:04:31,660: t15.2023.08.20 val PER: 0.1422
2026-01-13 22:04:31,660: t15.2023.08.25 val PER: 0.1084
2026-01-13 22:04:31,660: t15.2023.08.27 val PER: 0.2058
2026-01-13 22:04:31,660: t15.2023.09.01 val PER: 0.1104
2026-01-13 22:04:31,660: t15.2023.09.03 val PER: 0.1876
2026-01-13 22:04:31,661: t15.2023.09.24 val PER: 0.1590
2026-01-13 22:04:31,661: t15.2023.09.29 val PER: 0.1563
2026-01-13 22:04:31,661: t15.2023.10.01 val PER: 0.1935
2026-01-13 22:04:31,661: t15.2023.10.06 val PER: 0.1076
2026-01-13 22:04:31,661: t15.2023.10.08 val PER: 0.2733
2026-01-13 22:04:31,661: t15.2023.10.13 val PER: 0.2444
2026-01-13 22:04:31,661: t15.2023.10.15 val PER: 0.1951
2026-01-13 22:04:31,661: t15.2023.10.20 val PER: 0.1913
2026-01-13 22:04:31,661: t15.2023.10.22 val PER: 0.1526
2026-01-13 22:04:31,661: t15.2023.11.03 val PER: 0.2015
2026-01-13 22:04:31,661: t15.2023.11.04 val PER: 0.0546
2026-01-13 22:04:31,661: t15.2023.11.17 val PER: 0.0591
2026-01-13 22:04:31,661: t15.2023.11.19 val PER: 0.0519
2026-01-13 22:04:31,661: t15.2023.11.26 val PER: 0.1746
2026-01-13 22:04:31,661: t15.2023.12.03 val PER: 0.1544
2026-01-13 22:04:31,662: t15.2023.12.08 val PER: 0.1445
2026-01-13 22:04:31,662: t15.2023.12.10 val PER: 0.1235
2026-01-13 22:04:31,662: t15.2023.12.17 val PER: 0.1788
2026-01-13 22:04:31,662: t15.2023.12.29 val PER: 0.1894
2026-01-13 22:04:31,662: t15.2024.02.25 val PER: 0.1475
2026-01-13 22:04:31,662: t15.2024.03.08 val PER: 0.2802
2026-01-13 22:04:31,662: t15.2024.03.15 val PER: 0.2427
2026-01-13 22:04:31,662: t15.2024.03.17 val PER: 0.1792
2026-01-13 22:04:31,662: t15.2024.05.10 val PER: 0.2036
2026-01-13 22:04:31,662: t15.2024.06.14 val PER: 0.1987
2026-01-13 22:04:31,662: t15.2024.07.19 val PER: 0.2775
2026-01-13 22:04:31,662: t15.2024.07.21 val PER: 0.1359
2026-01-13 22:04:31,663: t15.2024.07.28 val PER: 0.1632
2026-01-13 22:04:31,663: t15.2025.01.10 val PER: 0.3361
2026-01-13 22:04:31,663: t15.2025.01.12 val PER: 0.1886
2026-01-13 22:04:31,663: t15.2025.03.14 val PER: 0.3743
2026-01-13 22:04:31,663: t15.2025.03.16 val PER: 0.2461
2026-01-13 22:04:31,663: t15.2025.03.30 val PER: 0.3425
2026-01-13 22:04:31,663: t15.2025.04.13 val PER: 0.2496
2026-01-13 22:04:31,665: New best val WER(5gram) 19.43% --> 18.25%
2026-01-13 22:04:31,814: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_7500
2026-01-13 22:04:40,528: Train batch 7600: loss: 15.43 grad norm: 61.20 time: 0.068
2026-01-13 22:04:58,039: Train batch 7800: loss: 13.30 grad norm: 59.24 time: 0.056
2026-01-13 22:05:16,082: Train batch 8000: loss: 10.82 grad norm: 49.90 time: 0.071
2026-01-13 22:05:16,083: Running test after training batch: 8000
2026-01-13 22:05:16,186: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:05:20,946: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:05:20,998: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost and
2026-01-13 22:05:32,126: Val batch 8000: PER (avg): 0.1782 CTC Loss (avg): 17.6072 WER(5gram): 18.45% (n=256) time: 16.043
2026-01-13 22:05:32,127: WER lens: avg_true_words=5.99 avg_pred_words=6.25 max_pred_words=13
2026-01-13 22:05:32,127: t15.2023.08.13 val PER: 0.1383
2026-01-13 22:05:32,127: t15.2023.08.18 val PER: 0.1123
2026-01-13 22:05:32,127: t15.2023.08.20 val PER: 0.1406
2026-01-13 22:05:32,127: t15.2023.08.25 val PER: 0.1175
2026-01-13 22:05:32,128: t15.2023.08.27 val PER: 0.1994
2026-01-13 22:05:32,128: t15.2023.09.01 val PER: 0.0966
2026-01-13 22:05:32,128: t15.2023.09.03 val PER: 0.1817
2026-01-13 22:05:32,128: t15.2023.09.24 val PER: 0.1590
2026-01-13 22:05:32,128: t15.2023.09.29 val PER: 0.1506
2026-01-13 22:05:32,128: t15.2023.10.01 val PER: 0.1975
2026-01-13 22:05:32,128: t15.2023.10.06 val PER: 0.1098
2026-01-13 22:05:32,128: t15.2023.10.08 val PER: 0.2517
2026-01-13 22:05:32,128: t15.2023.10.13 val PER: 0.2343
2026-01-13 22:05:32,128: t15.2023.10.15 val PER: 0.1846
2026-01-13 22:05:32,128: t15.2023.10.20 val PER: 0.2013
2026-01-13 22:05:32,128: t15.2023.10.22 val PER: 0.1359
2026-01-13 22:05:32,128: t15.2023.11.03 val PER: 0.1967
2026-01-13 22:05:32,129: t15.2023.11.04 val PER: 0.0273
2026-01-13 22:05:32,129: t15.2023.11.17 val PER: 0.0575
2026-01-13 22:05:32,129: t15.2023.11.19 val PER: 0.0599
2026-01-13 22:05:32,129: t15.2023.11.26 val PER: 0.1681
2026-01-13 22:05:32,129: t15.2023.12.03 val PER: 0.1628
2026-01-13 22:05:32,129: t15.2023.12.08 val PER: 0.1358
2026-01-13 22:05:32,129: t15.2023.12.10 val PER: 0.1156
2026-01-13 22:05:32,129: t15.2023.12.17 val PER: 0.1746
2026-01-13 22:05:32,129: t15.2023.12.29 val PER: 0.1647
2026-01-13 22:05:32,129: t15.2024.02.25 val PER: 0.1404
2026-01-13 22:05:32,129: t15.2024.03.08 val PER: 0.2518
2026-01-13 22:05:32,129: t15.2024.03.15 val PER: 0.2276
2026-01-13 22:05:32,129: t15.2024.03.17 val PER: 0.1611
2026-01-13 22:05:32,129: t15.2024.05.10 val PER: 0.1768
2026-01-13 22:05:32,129: t15.2024.06.14 val PER: 0.2050
2026-01-13 22:05:32,129: t15.2024.07.19 val PER: 0.2742
2026-01-13 22:05:32,130: t15.2024.07.21 val PER: 0.1138
2026-01-13 22:05:32,130: t15.2024.07.28 val PER: 0.1566
2026-01-13 22:05:32,130: t15.2025.01.10 val PER: 0.3223
2026-01-13 22:05:32,131: t15.2025.01.12 val PER: 0.1894
2026-01-13 22:05:32,131: t15.2025.03.14 val PER: 0.3654
2026-01-13 22:05:32,131: t15.2025.03.16 val PER: 0.2186
2026-01-13 22:05:32,131: t15.2025.03.30 val PER: 0.3460
2026-01-13 22:05:32,131: t15.2025.04.13 val PER: 0.2568
2026-01-13 22:05:32,275: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_8000
2026-01-13 22:05:50,339: Train batch 8200: loss: 9.00 grad norm: 45.27 time: 0.054
2026-01-13 22:06:07,942: Train batch 8400: loss: 9.67 grad norm: 48.45 time: 0.063
2026-01-13 22:06:16,908: Running test after training batch: 8500
2026-01-13 22:06:17,047: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:06:21,961: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:06:22,006: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-13 22:06:33,029: Val batch 8500: PER (avg): 0.1741 CTC Loss (avg): 17.3466 WER(5gram): 17.60% (n=256) time: 16.121
2026-01-13 22:06:33,030: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-13 22:06:33,030: t15.2023.08.13 val PER: 0.1268
2026-01-13 22:06:33,030: t15.2023.08.18 val PER: 0.1266
2026-01-13 22:06:33,030: t15.2023.08.20 val PER: 0.1350
2026-01-13 22:06:33,030: t15.2023.08.25 val PER: 0.1039
2026-01-13 22:06:33,030: t15.2023.08.27 val PER: 0.2090
2026-01-13 22:06:33,030: t15.2023.09.01 val PER: 0.0958
2026-01-13 22:06:33,031: t15.2023.09.03 val PER: 0.1936
2026-01-13 22:06:33,031: t15.2023.09.24 val PER: 0.1432
2026-01-13 22:06:33,031: t15.2023.09.29 val PER: 0.1417
2026-01-13 22:06:33,031: t15.2023.10.01 val PER: 0.1922
2026-01-13 22:06:33,031: t15.2023.10.06 val PER: 0.1076
2026-01-13 22:06:33,031: t15.2023.10.08 val PER: 0.2679
2026-01-13 22:06:33,031: t15.2023.10.13 val PER: 0.2366
2026-01-13 22:06:33,031: t15.2023.10.15 val PER: 0.1780
2026-01-13 22:06:33,031: t15.2023.10.20 val PER: 0.2013
2026-01-13 22:06:33,031: t15.2023.10.22 val PER: 0.1425
2026-01-13 22:06:33,031: t15.2023.11.03 val PER: 0.1988
2026-01-13 22:06:33,031: t15.2023.11.04 val PER: 0.0512
2026-01-13 22:06:33,031: t15.2023.11.17 val PER: 0.0544
2026-01-13 22:06:33,032: t15.2023.11.19 val PER: 0.0419
2026-01-13 22:06:33,032: t15.2023.11.26 val PER: 0.1681
2026-01-13 22:06:33,032: t15.2023.12.03 val PER: 0.1450
2026-01-13 22:06:33,032: t15.2023.12.08 val PER: 0.1352
2026-01-13 22:06:33,032: t15.2023.12.10 val PER: 0.1051
2026-01-13 22:06:33,032: t15.2023.12.17 val PER: 0.1653
2026-01-13 22:06:33,032: t15.2023.12.29 val PER: 0.1599
2026-01-13 22:06:33,032: t15.2024.02.25 val PER: 0.1362
2026-01-13 22:06:33,032: t15.2024.03.08 val PER: 0.2461
2026-01-13 22:06:33,032: t15.2024.03.15 val PER: 0.2245
2026-01-13 22:06:33,032: t15.2024.03.17 val PER: 0.1625
2026-01-13 22:06:33,032: t15.2024.05.10 val PER: 0.1783
2026-01-13 22:06:33,032: t15.2024.06.14 val PER: 0.1830
2026-01-13 22:06:33,032: t15.2024.07.19 val PER: 0.2617
2026-01-13 22:06:33,032: t15.2024.07.21 val PER: 0.1152
2026-01-13 22:06:33,032: t15.2024.07.28 val PER: 0.1551
2026-01-13 22:06:33,033: t15.2025.01.10 val PER: 0.3140
2026-01-13 22:06:33,033: t15.2025.01.12 val PER: 0.1771
2026-01-13 22:06:33,033: t15.2025.03.14 val PER: 0.3521
2026-01-13 22:06:33,033: t15.2025.03.16 val PER: 0.2120
2026-01-13 22:06:33,033: t15.2025.03.30 val PER: 0.3322
2026-01-13 22:06:33,033: t15.2025.04.13 val PER: 0.2311
2026-01-13 22:06:33,035: New best val WER(5gram) 18.25% --> 17.60%
2026-01-13 22:06:33,181: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_8500
2026-01-13 22:06:41,928: Train batch 8600: loss: 15.07 grad norm: 58.65 time: 0.054
2026-01-13 22:06:59,423: Train batch 8800: loss: 13.73 grad norm: 56.46 time: 0.060
2026-01-13 22:07:17,172: Train batch 9000: loss: 14.91 grad norm: 65.78 time: 0.071
2026-01-13 22:07:17,172: Running test after training batch: 9000
2026-01-13 22:07:17,267: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:07:22,140: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:07:22,187: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-13 22:07:33,289: Val batch 9000: PER (avg): 0.1667 CTC Loss (avg): 16.7855 WER(5gram): 18.19% (n=256) time: 16.117
2026-01-13 22:07:33,290: WER lens: avg_true_words=5.99 avg_pred_words=6.23 max_pred_words=12
2026-01-13 22:07:33,290: t15.2023.08.13 val PER: 0.1143
2026-01-13 22:07:33,290: t15.2023.08.18 val PER: 0.1207
2026-01-13 22:07:33,290: t15.2023.08.20 val PER: 0.1326
2026-01-13 22:07:33,291: t15.2023.08.25 val PER: 0.1069
2026-01-13 22:07:33,291: t15.2023.08.27 val PER: 0.1977
2026-01-13 22:07:33,291: t15.2023.09.01 val PER: 0.0909
2026-01-13 22:07:33,291: t15.2023.09.03 val PER: 0.1829
2026-01-13 22:07:33,291: t15.2023.09.24 val PER: 0.1323
2026-01-13 22:07:33,291: t15.2023.09.29 val PER: 0.1315
2026-01-13 22:07:33,291: t15.2023.10.01 val PER: 0.1869
2026-01-13 22:07:33,291: t15.2023.10.06 val PER: 0.0947
2026-01-13 22:07:33,292: t15.2023.10.08 val PER: 0.2652
2026-01-13 22:07:33,292: t15.2023.10.13 val PER: 0.2304
2026-01-13 22:07:33,292: t15.2023.10.15 val PER: 0.1668
2026-01-13 22:07:33,292: t15.2023.10.20 val PER: 0.2013
2026-01-13 22:07:33,292: t15.2023.10.22 val PER: 0.1325
2026-01-13 22:07:33,292: t15.2023.11.03 val PER: 0.1988
2026-01-13 22:07:33,292: t15.2023.11.04 val PER: 0.0341
2026-01-13 22:07:33,292: t15.2023.11.17 val PER: 0.0482
2026-01-13 22:07:33,292: t15.2023.11.19 val PER: 0.0419
2026-01-13 22:07:33,292: t15.2023.11.26 val PER: 0.1580
2026-01-13 22:07:33,293: t15.2023.12.03 val PER: 0.1376
2026-01-13 22:07:33,293: t15.2023.12.08 val PER: 0.1178
2026-01-13 22:07:33,293: t15.2023.12.10 val PER: 0.0999
2026-01-13 22:07:33,293: t15.2023.12.17 val PER: 0.1528
2026-01-13 22:07:33,293: t15.2023.12.29 val PER: 0.1510
2026-01-13 22:07:33,293: t15.2024.02.25 val PER: 0.1362
2026-01-13 22:07:33,293: t15.2024.03.08 val PER: 0.2432
2026-01-13 22:07:33,293: t15.2024.03.15 val PER: 0.2158
2026-01-13 22:07:33,293: t15.2024.03.17 val PER: 0.1590
2026-01-13 22:07:33,293: t15.2024.05.10 val PER: 0.1902
2026-01-13 22:07:33,293: t15.2024.06.14 val PER: 0.1672
2026-01-13 22:07:33,294: t15.2024.07.19 val PER: 0.2577
2026-01-13 22:07:33,294: t15.2024.07.21 val PER: 0.1055
2026-01-13 22:07:33,294: t15.2024.07.28 val PER: 0.1507
2026-01-13 22:07:33,294: t15.2025.01.10 val PER: 0.3017
2026-01-13 22:07:33,294: t15.2025.01.12 val PER: 0.1647
2026-01-13 22:07:33,294: t15.2025.03.14 val PER: 0.3328
2026-01-13 22:07:33,294: t15.2025.03.16 val PER: 0.2094
2026-01-13 22:07:33,294: t15.2025.03.30 val PER: 0.3138
2026-01-13 22:07:33,294: t15.2025.04.13 val PER: 0.2411
2026-01-13 22:07:33,433: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_9000
2026-01-13 22:07:50,997: Train batch 9200: loss: 9.82 grad norm: 49.66 time: 0.057
2026-01-13 22:08:08,588: Train batch 9400: loss: 6.95 grad norm: 43.66 time: 0.067
2026-01-13 22:08:17,440: Running test after training batch: 9500
2026-01-13 22:08:17,594: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:08:22,596: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:08:22,643: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-13 22:08:33,520: Val batch 9500: PER (avg): 0.1668 CTC Loss (avg): 16.6430 WER(5gram): 17.93% (n=256) time: 16.079
2026-01-13 22:08:33,520: WER lens: avg_true_words=5.99 avg_pred_words=6.23 max_pred_words=13
2026-01-13 22:08:33,521: t15.2023.08.13 val PER: 0.1310
2026-01-13 22:08:33,521: t15.2023.08.18 val PER: 0.1190
2026-01-13 22:08:33,521: t15.2023.08.20 val PER: 0.1215
2026-01-13 22:08:33,521: t15.2023.08.25 val PER: 0.0919
2026-01-13 22:08:33,521: t15.2023.08.27 val PER: 0.1833
2026-01-13 22:08:33,522: t15.2023.09.01 val PER: 0.0925
2026-01-13 22:08:33,522: t15.2023.09.03 val PER: 0.1698
2026-01-13 22:08:33,522: t15.2023.09.24 val PER: 0.1359
2026-01-13 22:08:33,522: t15.2023.09.29 val PER: 0.1372
2026-01-13 22:08:33,522: t15.2023.10.01 val PER: 0.1843
2026-01-13 22:08:33,522: t15.2023.10.06 val PER: 0.0947
2026-01-13 22:08:33,522: t15.2023.10.08 val PER: 0.2558
2026-01-13 22:08:33,522: t15.2023.10.13 val PER: 0.2196
2026-01-13 22:08:33,522: t15.2023.10.15 val PER: 0.1734
2026-01-13 22:08:33,522: t15.2023.10.20 val PER: 0.1980
2026-01-13 22:08:33,522: t15.2023.10.22 val PER: 0.1236
2026-01-13 22:08:33,522: t15.2023.11.03 val PER: 0.1974
2026-01-13 22:08:33,522: t15.2023.11.04 val PER: 0.0273
2026-01-13 22:08:33,522: t15.2023.11.17 val PER: 0.0607
2026-01-13 22:08:33,522: t15.2023.11.19 val PER: 0.0479
2026-01-13 22:08:33,522: t15.2023.11.26 val PER: 0.1413
2026-01-13 22:08:33,523: t15.2023.12.03 val PER: 0.1345
2026-01-13 22:08:33,523: t15.2023.12.08 val PER: 0.1198
2026-01-13 22:08:33,523: t15.2023.12.10 val PER: 0.1078
2026-01-13 22:08:33,523: t15.2023.12.17 val PER: 0.1497
2026-01-13 22:08:33,523: t15.2023.12.29 val PER: 0.1482
2026-01-13 22:08:33,523: t15.2024.02.25 val PER: 0.1292
2026-01-13 22:08:33,523: t15.2024.03.08 val PER: 0.2461
2026-01-13 22:08:33,523: t15.2024.03.15 val PER: 0.2208
2026-01-13 22:08:33,523: t15.2024.03.17 val PER: 0.1548
2026-01-13 22:08:33,523: t15.2024.05.10 val PER: 0.1902
2026-01-13 22:08:33,523: t15.2024.06.14 val PER: 0.1751
2026-01-13 22:08:33,524: t15.2024.07.19 val PER: 0.2663
2026-01-13 22:08:33,524: t15.2024.07.21 val PER: 0.1076
2026-01-13 22:08:33,524: t15.2024.07.28 val PER: 0.1551
2026-01-13 22:08:33,524: t15.2025.01.10 val PER: 0.3058
2026-01-13 22:08:33,524: t15.2025.01.12 val PER: 0.1809
2026-01-13 22:08:33,524: t15.2025.03.14 val PER: 0.3728
2026-01-13 22:08:33,524: t15.2025.03.16 val PER: 0.2055
2026-01-13 22:08:33,524: t15.2025.03.30 val PER: 0.3115
2026-01-13 22:08:33,524: t15.2025.04.13 val PER: 0.2282
2026-01-13 22:08:33,665: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_9500
2026-01-13 22:08:42,470: Train batch 9600: loss: 7.85 grad norm: 45.60 time: 0.072
2026-01-13 22:09:00,313: Train batch 9800: loss: 11.23 grad norm: 61.14 time: 0.062
2026-01-13 22:09:18,205: Train batch 10000: loss: 4.44 grad norm: 37.16 time: 0.060
2026-01-13 22:09:18,206: Running test after training batch: 10000
2026-01-13 22:09:18,302: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:09:23,195: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:09:23,239: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-13 22:09:34,302: Val batch 10000: PER (avg): 0.1638 CTC Loss (avg): 16.5399 WER(5gram): 16.82% (n=256) time: 16.096
2026-01-13 22:09:34,303: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-13 22:09:34,303: t15.2023.08.13 val PER: 0.1195
2026-01-13 22:09:34,303: t15.2023.08.18 val PER: 0.1224
2026-01-13 22:09:34,303: t15.2023.08.20 val PER: 0.1231
2026-01-13 22:09:34,303: t15.2023.08.25 val PER: 0.1054
2026-01-13 22:09:34,303: t15.2023.08.27 val PER: 0.1817
2026-01-13 22:09:34,304: t15.2023.09.01 val PER: 0.0795
2026-01-13 22:09:34,304: t15.2023.09.03 val PER: 0.1639
2026-01-13 22:09:34,304: t15.2023.09.24 val PER: 0.1335
2026-01-13 22:09:34,304: t15.2023.09.29 val PER: 0.1442
2026-01-13 22:09:34,304: t15.2023.10.01 val PER: 0.1915
2026-01-13 22:09:34,304: t15.2023.10.06 val PER: 0.0926
2026-01-13 22:09:34,304: t15.2023.10.08 val PER: 0.2503
2026-01-13 22:09:34,304: t15.2023.10.13 val PER: 0.2102
2026-01-13 22:09:34,304: t15.2023.10.15 val PER: 0.1674
2026-01-13 22:09:34,305: t15.2023.10.20 val PER: 0.2013
2026-01-13 22:09:34,305: t15.2023.10.22 val PER: 0.1303
2026-01-13 22:09:34,305: t15.2023.11.03 val PER: 0.1927
2026-01-13 22:09:34,305: t15.2023.11.04 val PER: 0.0410
2026-01-13 22:09:34,305: t15.2023.11.17 val PER: 0.0467
2026-01-13 22:09:34,305: t15.2023.11.19 val PER: 0.0399
2026-01-13 22:09:34,305: t15.2023.11.26 val PER: 0.1377
2026-01-13 22:09:34,305: t15.2023.12.03 val PER: 0.1261
2026-01-13 22:09:34,305: t15.2023.12.08 val PER: 0.1298
2026-01-13 22:09:34,306: t15.2023.12.10 val PER: 0.1025
2026-01-13 22:09:34,306: t15.2023.12.17 val PER: 0.1486
2026-01-13 22:09:34,306: t15.2023.12.29 val PER: 0.1393
2026-01-13 22:09:34,306: t15.2024.02.25 val PER: 0.1334
2026-01-13 22:09:34,306: t15.2024.03.08 val PER: 0.2447
2026-01-13 22:09:34,306: t15.2024.03.15 val PER: 0.2145
2026-01-13 22:09:34,306: t15.2024.03.17 val PER: 0.1541
2026-01-13 22:09:34,306: t15.2024.05.10 val PER: 0.1575
2026-01-13 22:09:34,306: t15.2024.06.14 val PER: 0.1656
2026-01-13 22:09:34,307: t15.2024.07.19 val PER: 0.2584
2026-01-13 22:09:34,307: t15.2024.07.21 val PER: 0.1048
2026-01-13 22:09:34,307: t15.2024.07.28 val PER: 0.1522
2026-01-13 22:09:34,307: t15.2025.01.10 val PER: 0.3209
2026-01-13 22:09:34,307: t15.2025.01.12 val PER: 0.1709
2026-01-13 22:09:34,307: t15.2025.03.14 val PER: 0.3402
2026-01-13 22:09:34,307: t15.2025.03.16 val PER: 0.2029
2026-01-13 22:09:34,307: t15.2025.03.30 val PER: 0.3092
2026-01-13 22:09:34,307: t15.2025.04.13 val PER: 0.2382
2026-01-13 22:09:34,308: New best val WER(5gram) 17.60% --> 16.82%
2026-01-13 22:09:34,457: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_10000
2026-01-13 22:09:52,751: Train batch 10200: loss: 5.74 grad norm: 37.46 time: 0.049
2026-01-13 22:10:11,486: Train batch 10400: loss: 8.11 grad norm: 54.41 time: 0.071
2026-01-13 22:10:20,858: Running test after training batch: 10500
2026-01-13 22:10:21,012: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:10:26,017: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:10:26,060: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-13 22:10:37,199: Val batch 10500: PER (avg): 0.1579 CTC Loss (avg): 15.9564 WER(5gram): 16.10% (n=256) time: 16.341
2026-01-13 22:10:37,200: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-13 22:10:37,200: t15.2023.08.13 val PER: 0.1216
2026-01-13 22:10:37,200: t15.2023.08.18 val PER: 0.1098
2026-01-13 22:10:37,200: t15.2023.08.20 val PER: 0.1168
2026-01-13 22:10:37,200: t15.2023.08.25 val PER: 0.1084
2026-01-13 22:10:37,200: t15.2023.08.27 val PER: 0.1849
2026-01-13 22:10:37,200: t15.2023.09.01 val PER: 0.0885
2026-01-13 22:10:37,200: t15.2023.09.03 val PER: 0.1651
2026-01-13 22:10:37,200: t15.2023.09.24 val PER: 0.1347
2026-01-13 22:10:37,201: t15.2023.09.29 val PER: 0.1436
2026-01-13 22:10:37,201: t15.2023.10.01 val PER: 0.1856
2026-01-13 22:10:37,201: t15.2023.10.06 val PER: 0.0861
2026-01-13 22:10:37,201: t15.2023.10.08 val PER: 0.2422
2026-01-13 22:10:37,201: t15.2023.10.13 val PER: 0.2079
2026-01-13 22:10:37,201: t15.2023.10.15 val PER: 0.1608
2026-01-13 22:10:37,201: t15.2023.10.20 val PER: 0.1913
2026-01-13 22:10:37,201: t15.2023.10.22 val PER: 0.1214
2026-01-13 22:10:37,201: t15.2023.11.03 val PER: 0.1859
2026-01-13 22:10:37,201: t15.2023.11.04 val PER: 0.0307
2026-01-13 22:10:37,201: t15.2023.11.17 val PER: 0.0513
2026-01-13 22:10:37,201: t15.2023.11.19 val PER: 0.0499
2026-01-13 22:10:37,201: t15.2023.11.26 val PER: 0.1225
2026-01-13 22:10:37,201: t15.2023.12.03 val PER: 0.1166
2026-01-13 22:10:37,201: t15.2023.12.08 val PER: 0.1132
2026-01-13 22:10:37,201: t15.2023.12.10 val PER: 0.0959
2026-01-13 22:10:37,201: t15.2023.12.17 val PER: 0.1351
2026-01-13 22:10:37,202: t15.2023.12.29 val PER: 0.1434
2026-01-13 22:10:37,202: t15.2024.02.25 val PER: 0.1278
2026-01-13 22:10:37,202: t15.2024.03.08 val PER: 0.2432
2026-01-13 22:10:37,202: t15.2024.03.15 val PER: 0.2095
2026-01-13 22:10:37,202: t15.2024.03.17 val PER: 0.1464
2026-01-13 22:10:37,202: t15.2024.05.10 val PER: 0.1590
2026-01-13 22:10:37,202: t15.2024.06.14 val PER: 0.1688
2026-01-13 22:10:37,202: t15.2024.07.19 val PER: 0.2439
2026-01-13 22:10:37,202: t15.2024.07.21 val PER: 0.0945
2026-01-13 22:10:37,203: t15.2024.07.28 val PER: 0.1419
2026-01-13 22:10:37,203: t15.2025.01.10 val PER: 0.3113
2026-01-13 22:10:37,203: t15.2025.01.12 val PER: 0.1570
2026-01-13 22:10:37,203: t15.2025.03.14 val PER: 0.3550
2026-01-13 22:10:37,203: t15.2025.03.16 val PER: 0.1793
2026-01-13 22:10:37,203: t15.2025.03.30 val PER: 0.3023
2026-01-13 22:10:37,203: t15.2025.04.13 val PER: 0.2254
2026-01-13 22:10:37,204: New best val WER(5gram) 16.82% --> 16.10%
2026-01-13 22:10:37,359: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_10500
2026-01-13 22:10:46,860: Train batch 10600: loss: 8.18 grad norm: 66.52 time: 0.072
2026-01-13 22:11:04,806: Train batch 10800: loss: 13.35 grad norm: 65.43 time: 0.064
2026-01-13 22:11:22,840: Train batch 11000: loss: 11.45 grad norm: 55.54 time: 0.057
2026-01-13 22:11:22,840: Running test after training batch: 11000
2026-01-13 22:11:22,985: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:11:27,762: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:11:27,812: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost get
2026-01-13 22:11:39,117: Val batch 11000: PER (avg): 0.1546 CTC Loss (avg): 15.9069 WER(5gram): 18.97% (n=256) time: 16.277
2026-01-13 22:11:39,117: WER lens: avg_true_words=5.99 avg_pred_words=6.27 max_pred_words=13
2026-01-13 22:11:39,118: t15.2023.08.13 val PER: 0.1206
2026-01-13 22:11:39,118: t15.2023.08.18 val PER: 0.1090
2026-01-13 22:11:39,118: t15.2023.08.20 val PER: 0.1088
2026-01-13 22:11:39,118: t15.2023.08.25 val PER: 0.0828
2026-01-13 22:11:39,118: t15.2023.08.27 val PER: 0.1913
2026-01-13 22:11:39,118: t15.2023.09.01 val PER: 0.0869
2026-01-13 22:11:39,118: t15.2023.09.03 val PER: 0.1675
2026-01-13 22:11:39,118: t15.2023.09.24 val PER: 0.1286
2026-01-13 22:11:39,118: t15.2023.09.29 val PER: 0.1353
2026-01-13 22:11:39,118: t15.2023.10.01 val PER: 0.1783
2026-01-13 22:11:39,118: t15.2023.10.06 val PER: 0.0818
2026-01-13 22:11:39,119: t15.2023.10.08 val PER: 0.2422
2026-01-13 22:11:39,119: t15.2023.10.13 val PER: 0.2025
2026-01-13 22:11:39,119: t15.2023.10.15 val PER: 0.1575
2026-01-13 22:11:39,119: t15.2023.10.20 val PER: 0.2215
2026-01-13 22:11:39,119: t15.2023.10.22 val PER: 0.1236
2026-01-13 22:11:39,119: t15.2023.11.03 val PER: 0.1872
2026-01-13 22:11:39,119: t15.2023.11.04 val PER: 0.0307
2026-01-13 22:11:39,119: t15.2023.11.17 val PER: 0.0467
2026-01-13 22:11:39,119: t15.2023.11.19 val PER: 0.0379
2026-01-13 22:11:39,119: t15.2023.11.26 val PER: 0.1232
2026-01-13 22:11:39,119: t15.2023.12.03 val PER: 0.1103
2026-01-13 22:11:39,119: t15.2023.12.08 val PER: 0.1059
2026-01-13 22:11:39,119: t15.2023.12.10 val PER: 0.0933
2026-01-13 22:11:39,119: t15.2023.12.17 val PER: 0.1497
2026-01-13 22:11:39,119: t15.2023.12.29 val PER: 0.1338
2026-01-13 22:11:39,119: t15.2024.02.25 val PER: 0.1292
2026-01-13 22:11:39,119: t15.2024.03.08 val PER: 0.2205
2026-01-13 22:11:39,120: t15.2024.03.15 val PER: 0.2145
2026-01-13 22:11:39,120: t15.2024.03.17 val PER: 0.1304
2026-01-13 22:11:39,120: t15.2024.05.10 val PER: 0.1649
2026-01-13 22:11:39,120: t15.2024.06.14 val PER: 0.1577
2026-01-13 22:11:39,120: t15.2024.07.19 val PER: 0.2327
2026-01-13 22:11:39,120: t15.2024.07.21 val PER: 0.1007
2026-01-13 22:11:39,120: t15.2024.07.28 val PER: 0.1338
2026-01-13 22:11:39,120: t15.2025.01.10 val PER: 0.3003
2026-01-13 22:11:39,120: t15.2025.01.12 val PER: 0.1563
2026-01-13 22:11:39,120: t15.2025.03.14 val PER: 0.3491
2026-01-13 22:11:39,120: t15.2025.03.16 val PER: 0.2042
2026-01-13 22:11:39,120: t15.2025.03.30 val PER: 0.2931
2026-01-13 22:11:39,120: t15.2025.04.13 val PER: 0.2268
2026-01-13 22:11:39,278: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_11000
2026-01-13 22:11:57,074: Train batch 11200: loss: 9.24 grad norm: 51.11 time: 0.071
2026-01-13 22:12:14,601: Train batch 11400: loss: 8.11 grad norm: 48.04 time: 0.057
2026-01-13 22:12:23,510: Running test after training batch: 11500
2026-01-13 22:12:23,613: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:12:28,432: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:12:28,480: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-13 22:12:39,777: Val batch 11500: PER (avg): 0.1521 CTC Loss (avg): 15.9748 WER(5gram): 17.60% (n=256) time: 16.267
2026-01-13 22:12:39,778: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=12
2026-01-13 22:12:39,778: t15.2023.08.13 val PER: 0.1091
2026-01-13 22:12:39,778: t15.2023.08.18 val PER: 0.1140
2026-01-13 22:12:39,778: t15.2023.08.20 val PER: 0.1120
2026-01-13 22:12:39,778: t15.2023.08.25 val PER: 0.1039
2026-01-13 22:12:39,779: t15.2023.08.27 val PER: 0.1801
2026-01-13 22:12:39,779: t15.2023.09.01 val PER: 0.0812
2026-01-13 22:12:39,779: t15.2023.09.03 val PER: 0.1568
2026-01-13 22:12:39,779: t15.2023.09.24 val PER: 0.1335
2026-01-13 22:12:39,779: t15.2023.09.29 val PER: 0.1359
2026-01-13 22:12:39,779: t15.2023.10.01 val PER: 0.1783
2026-01-13 22:12:39,779: t15.2023.10.06 val PER: 0.0883
2026-01-13 22:12:39,779: t15.2023.10.08 val PER: 0.2490
2026-01-13 22:12:39,779: t15.2023.10.13 val PER: 0.2025
2026-01-13 22:12:39,779: t15.2023.10.15 val PER: 0.1430
2026-01-13 22:12:39,779: t15.2023.10.20 val PER: 0.1946
2026-01-13 22:12:39,779: t15.2023.10.22 val PER: 0.1269
2026-01-13 22:12:39,779: t15.2023.11.03 val PER: 0.1750
2026-01-13 22:12:39,779: t15.2023.11.04 val PER: 0.0307
2026-01-13 22:12:39,779: t15.2023.11.17 val PER: 0.0435
2026-01-13 22:12:39,780: t15.2023.11.19 val PER: 0.0439
2026-01-13 22:12:39,780: t15.2023.11.26 val PER: 0.1152
2026-01-13 22:12:39,780: t15.2023.12.03 val PER: 0.1061
2026-01-13 22:12:39,780: t15.2023.12.08 val PER: 0.0979
2026-01-13 22:12:39,780: t15.2023.12.10 val PER: 0.0920
2026-01-13 22:12:39,780: t15.2023.12.17 val PER: 0.1497
2026-01-13 22:12:39,780: t15.2023.12.29 val PER: 0.1242
2026-01-13 22:12:39,780: t15.2024.02.25 val PER: 0.1081
2026-01-13 22:12:39,780: t15.2024.03.08 val PER: 0.2276
2026-01-13 22:12:39,780: t15.2024.03.15 val PER: 0.2001
2026-01-13 22:12:39,780: t15.2024.03.17 val PER: 0.1430
2026-01-13 22:12:39,780: t15.2024.05.10 val PER: 0.1664
2026-01-13 22:12:39,780: t15.2024.06.14 val PER: 0.1767
2026-01-13 22:12:39,780: t15.2024.07.19 val PER: 0.2373
2026-01-13 22:12:39,781: t15.2024.07.21 val PER: 0.0945
2026-01-13 22:12:39,781: t15.2024.07.28 val PER: 0.1338
2026-01-13 22:12:39,781: t15.2025.01.10 val PER: 0.3099
2026-01-13 22:12:39,781: t15.2025.01.12 val PER: 0.1509
2026-01-13 22:12:39,781: t15.2025.03.14 val PER: 0.3343
2026-01-13 22:12:39,781: t15.2025.03.16 val PER: 0.1898
2026-01-13 22:12:39,781: t15.2025.03.30 val PER: 0.2943
2026-01-13 22:12:39,781: t15.2025.04.13 val PER: 0.2268
2026-01-13 22:12:39,928: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_11500
2026-01-13 22:12:48,446: Train batch 11600: loss: 8.49 grad norm: 44.71 time: 0.060
2026-01-13 22:13:06,170: Train batch 11800: loss: 5.72 grad norm: 39.83 time: 0.044
2026-01-13 22:13:23,902: Train batch 12000: loss: 11.13 grad norm: 54.28 time: 0.071
2026-01-13 22:13:23,902: Running test after training batch: 12000
2026-01-13 22:13:24,009: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:13:28,791: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:13:28,840: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-13 22:13:40,181: Val batch 12000: PER (avg): 0.1508 CTC Loss (avg): 16.0273 WER(5gram): 17.73% (n=256) time: 16.278
2026-01-13 22:13:40,181: WER lens: avg_true_words=5.99 avg_pred_words=6.25 max_pred_words=13
2026-01-13 22:13:40,181: t15.2023.08.13 val PER: 0.1112
2026-01-13 22:13:40,181: t15.2023.08.18 val PER: 0.1039
2026-01-13 22:13:40,181: t15.2023.08.20 val PER: 0.1056
2026-01-13 22:13:40,182: t15.2023.08.25 val PER: 0.1039
2026-01-13 22:13:40,182: t15.2023.08.27 val PER: 0.1801
2026-01-13 22:13:40,182: t15.2023.09.01 val PER: 0.0739
2026-01-13 22:13:40,182: t15.2023.09.03 val PER: 0.1651
2026-01-13 22:13:40,182: t15.2023.09.24 val PER: 0.1214
2026-01-13 22:13:40,182: t15.2023.09.29 val PER: 0.1308
2026-01-13 22:13:40,182: t15.2023.10.01 val PER: 0.1757
2026-01-13 22:13:40,182: t15.2023.10.06 val PER: 0.0861
2026-01-13 22:13:40,182: t15.2023.10.08 val PER: 0.2409
2026-01-13 22:13:40,182: t15.2023.10.13 val PER: 0.2017
2026-01-13 22:13:40,182: t15.2023.10.15 val PER: 0.1569
2026-01-13 22:13:40,182: t15.2023.10.20 val PER: 0.1812
2026-01-13 22:13:40,182: t15.2023.10.22 val PER: 0.1158
2026-01-13 22:13:40,182: t15.2023.11.03 val PER: 0.1852
2026-01-13 22:13:40,182: t15.2023.11.04 val PER: 0.0273
2026-01-13 22:13:40,183: t15.2023.11.17 val PER: 0.0389
2026-01-13 22:13:40,183: t15.2023.11.19 val PER: 0.0240
2026-01-13 22:13:40,183: t15.2023.11.26 val PER: 0.1232
2026-01-13 22:13:40,183: t15.2023.12.03 val PER: 0.1145
2026-01-13 22:13:40,183: t15.2023.12.08 val PER: 0.1019
2026-01-13 22:13:40,183: t15.2023.12.10 val PER: 0.0959
2026-01-13 22:13:40,183: t15.2023.12.17 val PER: 0.1403
2026-01-13 22:13:40,183: t15.2023.12.29 val PER: 0.1304
2026-01-13 22:13:40,183: t15.2024.02.25 val PER: 0.1096
2026-01-13 22:13:40,183: t15.2024.03.08 val PER: 0.2304
2026-01-13 22:13:40,183: t15.2024.03.15 val PER: 0.1945
2026-01-13 22:13:40,183: t15.2024.03.17 val PER: 0.1374
2026-01-13 22:13:40,184: t15.2024.05.10 val PER: 0.1560
2026-01-13 22:13:40,184: t15.2024.06.14 val PER: 0.1703
2026-01-13 22:13:40,184: t15.2024.07.19 val PER: 0.2254
2026-01-13 22:13:40,184: t15.2024.07.21 val PER: 0.0883
2026-01-13 22:13:40,184: t15.2024.07.28 val PER: 0.1419
2026-01-13 22:13:40,184: t15.2025.01.10 val PER: 0.2989
2026-01-13 22:13:40,184: t15.2025.01.12 val PER: 0.1509
2026-01-13 22:13:40,184: t15.2025.03.14 val PER: 0.3491
2026-01-13 22:13:40,184: t15.2025.03.16 val PER: 0.1976
2026-01-13 22:13:40,184: t15.2025.03.30 val PER: 0.2908
2026-01-13 22:13:40,184: t15.2025.04.13 val PER: 0.2240
2026-01-13 22:13:40,325: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_12000
2026-01-13 22:13:58,144: Train batch 12200: loss: 4.71 grad norm: 36.78 time: 0.065
2026-01-13 22:14:15,326: Train batch 12400: loss: 4.18 grad norm: 37.31 time: 0.041
2026-01-13 22:14:24,415: Running test after training batch: 12500
2026-01-13 22:14:24,557: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:14:29,494: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:14:29,562: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-13 22:14:45,521: Val batch 12500: PER (avg): 0.1470 CTC Loss (avg): 15.5483 WER(5gram): 16.69% (n=256) time: 21.105
2026-01-13 22:14:45,521: WER lens: avg_true_words=5.99 avg_pred_words=6.22 max_pred_words=13
2026-01-13 22:14:45,521: t15.2023.08.13 val PER: 0.1112
2026-01-13 22:14:45,521: t15.2023.08.18 val PER: 0.1090
2026-01-13 22:14:45,522: t15.2023.08.20 val PER: 0.1033
2026-01-13 22:14:45,522: t15.2023.08.25 val PER: 0.0889
2026-01-13 22:14:45,522: t15.2023.08.27 val PER: 0.1897
2026-01-13 22:14:45,522: t15.2023.09.01 val PER: 0.0706
2026-01-13 22:14:45,522: t15.2023.09.03 val PER: 0.1473
2026-01-13 22:14:45,522: t15.2023.09.24 val PER: 0.1117
2026-01-13 22:14:45,522: t15.2023.09.29 val PER: 0.1289
2026-01-13 22:14:45,522: t15.2023.10.01 val PER: 0.1671
2026-01-13 22:14:45,522: t15.2023.10.06 val PER: 0.0850
2026-01-13 22:14:45,523: t15.2023.10.08 val PER: 0.2490
2026-01-13 22:14:45,523: t15.2023.10.13 val PER: 0.2009
2026-01-13 22:14:45,523: t15.2023.10.15 val PER: 0.1444
2026-01-13 22:14:45,523: t15.2023.10.20 val PER: 0.1980
2026-01-13 22:14:45,523: t15.2023.10.22 val PER: 0.1091
2026-01-13 22:14:45,523: t15.2023.11.03 val PER: 0.1791
2026-01-13 22:14:45,523: t15.2023.11.04 val PER: 0.0239
2026-01-13 22:14:45,523: t15.2023.11.17 val PER: 0.0498
2026-01-13 22:14:45,524: t15.2023.11.19 val PER: 0.0259
2026-01-13 22:14:45,524: t15.2023.11.26 val PER: 0.1145
2026-01-13 22:14:45,524: t15.2023.12.03 val PER: 0.0851
2026-01-13 22:14:45,524: t15.2023.12.08 val PER: 0.0939
2026-01-13 22:14:45,524: t15.2023.12.10 val PER: 0.0867
2026-01-13 22:14:45,524: t15.2023.12.17 val PER: 0.1299
2026-01-13 22:14:45,524: t15.2023.12.29 val PER: 0.1332
2026-01-13 22:14:45,524: t15.2024.02.25 val PER: 0.1053
2026-01-13 22:14:45,524: t15.2024.03.08 val PER: 0.2134
2026-01-13 22:14:45,524: t15.2024.03.15 val PER: 0.1926
2026-01-13 22:14:45,524: t15.2024.03.17 val PER: 0.1395
2026-01-13 22:14:45,524: t15.2024.05.10 val PER: 0.1575
2026-01-13 22:14:45,524: t15.2024.06.14 val PER: 0.1735
2026-01-13 22:14:45,524: t15.2024.07.19 val PER: 0.2340
2026-01-13 22:14:45,524: t15.2024.07.21 val PER: 0.0834
2026-01-13 22:14:45,524: t15.2024.07.28 val PER: 0.1397
2026-01-13 22:14:45,524: t15.2025.01.10 val PER: 0.2961
2026-01-13 22:14:45,525: t15.2025.01.12 val PER: 0.1501
2026-01-13 22:14:45,525: t15.2025.03.14 val PER: 0.3521
2026-01-13 22:14:45,525: t15.2025.03.16 val PER: 0.1702
2026-01-13 22:14:45,525: t15.2025.03.30 val PER: 0.2943
2026-01-13 22:14:45,525: t15.2025.04.13 val PER: 0.2225
2026-01-13 22:14:45,669: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_12500
2026-01-13 22:14:54,468: Train batch 12600: loss: 6.38 grad norm: 41.31 time: 0.057
2026-01-13 22:15:12,911: Train batch 12800: loss: 4.24 grad norm: 36.20 time: 0.052
2026-01-13 22:15:30,680: Train batch 13000: loss: 4.78 grad norm: 40.41 time: 0.065
2026-01-13 22:15:30,680: Running test after training batch: 13000
2026-01-13 22:15:30,795: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:15:35,700: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:15:35,782: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost is
2026-01-13 22:15:51,688: Val batch 13000: PER (avg): 0.1443 CTC Loss (avg): 15.5726 WER(5gram): 16.88% (n=256) time: 21.007
2026-01-13 22:15:51,688: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-13 22:15:51,689: t15.2023.08.13 val PER: 0.1081
2026-01-13 22:15:51,689: t15.2023.08.18 val PER: 0.1048
2026-01-13 22:15:51,689: t15.2023.08.20 val PER: 0.1001
2026-01-13 22:15:51,689: t15.2023.08.25 val PER: 0.0964
2026-01-13 22:15:51,689: t15.2023.08.27 val PER: 0.1865
2026-01-13 22:15:51,689: t15.2023.09.01 val PER: 0.0698
2026-01-13 22:15:51,689: t15.2023.09.03 val PER: 0.1568
2026-01-13 22:15:51,689: t15.2023.09.24 val PER: 0.1189
2026-01-13 22:15:51,689: t15.2023.09.29 val PER: 0.1315
2026-01-13 22:15:51,689: t15.2023.10.01 val PER: 0.1638
2026-01-13 22:15:51,690: t15.2023.10.06 val PER: 0.0850
2026-01-13 22:15:51,690: t15.2023.10.08 val PER: 0.2463
2026-01-13 22:15:51,690: t15.2023.10.13 val PER: 0.2025
2026-01-13 22:15:51,690: t15.2023.10.15 val PER: 0.1397
2026-01-13 22:15:51,690: t15.2023.10.20 val PER: 0.1913
2026-01-13 22:15:51,690: t15.2023.10.22 val PER: 0.1024
2026-01-13 22:15:51,690: t15.2023.11.03 val PER: 0.1710
2026-01-13 22:15:51,690: t15.2023.11.04 val PER: 0.0273
2026-01-13 22:15:51,690: t15.2023.11.17 val PER: 0.0327
2026-01-13 22:15:51,690: t15.2023.11.19 val PER: 0.0359
2026-01-13 22:15:51,690: t15.2023.11.26 val PER: 0.1109
2026-01-13 22:15:51,690: t15.2023.12.03 val PER: 0.1061
2026-01-13 22:15:51,690: t15.2023.12.08 val PER: 0.0925
2026-01-13 22:15:51,690: t15.2023.12.10 val PER: 0.0788
2026-01-13 22:15:51,690: t15.2023.12.17 val PER: 0.1362
2026-01-13 22:15:51,690: t15.2023.12.29 val PER: 0.1325
2026-01-13 22:15:51,691: t15.2024.02.25 val PER: 0.1110
2026-01-13 22:15:51,691: t15.2024.03.08 val PER: 0.2390
2026-01-13 22:15:51,691: t15.2024.03.15 val PER: 0.1970
2026-01-13 22:15:51,692: t15.2024.03.17 val PER: 0.1220
2026-01-13 22:15:51,692: t15.2024.05.10 val PER: 0.1486
2026-01-13 22:15:51,692: t15.2024.06.14 val PER: 0.1546
2026-01-13 22:15:51,692: t15.2024.07.19 val PER: 0.2287
2026-01-13 22:15:51,692: t15.2024.07.21 val PER: 0.0855
2026-01-13 22:15:51,692: t15.2024.07.28 val PER: 0.1309
2026-01-13 22:15:51,692: t15.2025.01.10 val PER: 0.2713
2026-01-13 22:15:51,692: t15.2025.01.12 val PER: 0.1355
2026-01-13 22:15:51,692: t15.2025.03.14 val PER: 0.3432
2026-01-13 22:15:51,692: t15.2025.03.16 val PER: 0.1675
2026-01-13 22:15:51,692: t15.2025.03.30 val PER: 0.2724
2026-01-13 22:15:51,692: t15.2025.04.13 val PER: 0.2225
2026-01-13 22:15:51,834: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_13000
2026-01-13 22:16:09,382: Train batch 13200: loss: 9.18 grad norm: 56.06 time: 0.054
2026-01-13 22:16:26,803: Train batch 13400: loss: 6.39 grad norm: 48.07 time: 0.062
2026-01-13 22:16:35,519: Running test after training batch: 13500
2026-01-13 22:16:35,614: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:16:41,063: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:16:41,124: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-13 22:16:57,562: Val batch 13500: PER (avg): 0.1408 CTC Loss (avg): 15.0917 WER(5gram): 15.65% (n=256) time: 22.043
2026-01-13 22:16:57,562: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-13 22:16:57,563: t15.2023.08.13 val PER: 0.1102
2026-01-13 22:16:57,563: t15.2023.08.18 val PER: 0.0972
2026-01-13 22:16:57,563: t15.2023.08.20 val PER: 0.0866
2026-01-13 22:16:57,563: t15.2023.08.25 val PER: 0.0964
2026-01-13 22:16:57,563: t15.2023.08.27 val PER: 0.1768
2026-01-13 22:16:57,563: t15.2023.09.01 val PER: 0.0722
2026-01-13 22:16:57,563: t15.2023.09.03 val PER: 0.1461
2026-01-13 22:16:57,563: t15.2023.09.24 val PER: 0.1189
2026-01-13 22:16:57,563: t15.2023.09.29 val PER: 0.1283
2026-01-13 22:16:57,563: t15.2023.10.01 val PER: 0.1658
2026-01-13 22:16:57,563: t15.2023.10.06 val PER: 0.0829
2026-01-13 22:16:57,563: t15.2023.10.08 val PER: 0.2355
2026-01-13 22:16:57,563: t15.2023.10.13 val PER: 0.1932
2026-01-13 22:16:57,564: t15.2023.10.15 val PER: 0.1490
2026-01-13 22:16:57,564: t15.2023.10.20 val PER: 0.1745
2026-01-13 22:16:57,564: t15.2023.10.22 val PER: 0.1102
2026-01-13 22:16:57,564: t15.2023.11.03 val PER: 0.1635
2026-01-13 22:16:57,564: t15.2023.11.04 val PER: 0.0341
2026-01-13 22:16:57,564: t15.2023.11.17 val PER: 0.0373
2026-01-13 22:16:57,564: t15.2023.11.19 val PER: 0.0200
2026-01-13 22:16:57,564: t15.2023.11.26 val PER: 0.1043
2026-01-13 22:16:57,564: t15.2023.12.03 val PER: 0.0956
2026-01-13 22:16:57,564: t15.2023.12.08 val PER: 0.0839
2026-01-13 22:16:57,564: t15.2023.12.10 val PER: 0.0788
2026-01-13 22:16:57,564: t15.2023.12.17 val PER: 0.1279
2026-01-13 22:16:57,564: t15.2023.12.29 val PER: 0.1249
2026-01-13 22:16:57,564: t15.2024.02.25 val PER: 0.1053
2026-01-13 22:16:57,565: t15.2024.03.08 val PER: 0.2176
2026-01-13 22:16:57,565: t15.2024.03.15 val PER: 0.1889
2026-01-13 22:16:57,565: t15.2024.03.17 val PER: 0.1241
2026-01-13 22:16:57,565: t15.2024.05.10 val PER: 0.1367
2026-01-13 22:16:57,565: t15.2024.06.14 val PER: 0.1577
2026-01-13 22:16:57,565: t15.2024.07.19 val PER: 0.2228
2026-01-13 22:16:57,565: t15.2024.07.21 val PER: 0.0841
2026-01-13 22:16:57,565: t15.2024.07.28 val PER: 0.1316
2026-01-13 22:16:57,565: t15.2025.01.10 val PER: 0.2824
2026-01-13 22:16:57,565: t15.2025.01.12 val PER: 0.1355
2026-01-13 22:16:57,565: t15.2025.03.14 val PER: 0.3506
2026-01-13 22:16:57,565: t15.2025.03.16 val PER: 0.1649
2026-01-13 22:16:57,565: t15.2025.03.30 val PER: 0.2885
2026-01-13 22:16:57,565: t15.2025.04.13 val PER: 0.1997
2026-01-13 22:16:57,567: New best val WER(5gram) 16.10% --> 15.65%
2026-01-13 22:16:57,714: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_13500
2026-01-13 22:17:06,576: Train batch 13600: loss: 10.38 grad norm: 63.60 time: 0.061
2026-01-13 22:17:24,150: Train batch 13800: loss: 6.57 grad norm: 50.20 time: 0.056
2026-01-13 22:17:41,816: Train batch 14000: loss: 8.57 grad norm: 54.95 time: 0.050
2026-01-13 22:17:41,816: Running test after training batch: 14000
2026-01-13 22:17:41,914: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:17:46,636: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:17:46,685: WER debug example
  GT : how does it keep the cost down
  PR : how dusty it keep the cost net
2026-01-13 22:17:57,877: Val batch 14000: PER (avg): 0.1433 CTC Loss (avg): 15.6041 WER(5gram): 17.73% (n=256) time: 16.061
2026-01-13 22:17:57,878: WER lens: avg_true_words=5.99 avg_pred_words=6.24 max_pred_words=12
2026-01-13 22:17:57,878: t15.2023.08.13 val PER: 0.1091
2026-01-13 22:17:57,878: t15.2023.08.18 val PER: 0.0997
2026-01-13 22:17:57,878: t15.2023.08.20 val PER: 0.0993
2026-01-13 22:17:57,878: t15.2023.08.25 val PER: 0.0949
2026-01-13 22:17:57,878: t15.2023.08.27 val PER: 0.1752
2026-01-13 22:17:57,878: t15.2023.09.01 val PER: 0.0722
2026-01-13 22:17:57,878: t15.2023.09.03 val PER: 0.1746
2026-01-13 22:17:57,878: t15.2023.09.24 val PER: 0.1129
2026-01-13 22:17:57,879: t15.2023.09.29 val PER: 0.1219
2026-01-13 22:17:57,879: t15.2023.10.01 val PER: 0.1658
2026-01-13 22:17:57,879: t15.2023.10.06 val PER: 0.0861
2026-01-13 22:17:57,879: t15.2023.10.08 val PER: 0.2368
2026-01-13 22:17:57,879: t15.2023.10.13 val PER: 0.1994
2026-01-13 22:17:57,879: t15.2023.10.15 val PER: 0.1411
2026-01-13 22:17:57,879: t15.2023.10.20 val PER: 0.1846
2026-01-13 22:17:57,879: t15.2023.10.22 val PER: 0.1080
2026-01-13 22:17:57,880: t15.2023.11.03 val PER: 0.1757
2026-01-13 22:17:57,880: t15.2023.11.04 val PER: 0.0239
2026-01-13 22:17:57,880: t15.2023.11.17 val PER: 0.0389
2026-01-13 22:17:57,880: t15.2023.11.19 val PER: 0.0200
2026-01-13 22:17:57,880: t15.2023.11.26 val PER: 0.1123
2026-01-13 22:17:57,880: t15.2023.12.03 val PER: 0.1082
2026-01-13 22:17:57,880: t15.2023.12.08 val PER: 0.1019
2026-01-13 22:17:57,880: t15.2023.12.10 val PER: 0.0788
2026-01-13 22:17:57,880: t15.2023.12.17 val PER: 0.1279
2026-01-13 22:17:57,880: t15.2023.12.29 val PER: 0.1249
2026-01-13 22:17:57,880: t15.2024.02.25 val PER: 0.1053
2026-01-13 22:17:57,880: t15.2024.03.08 val PER: 0.2162
2026-01-13 22:17:57,880: t15.2024.03.15 val PER: 0.1907
2026-01-13 22:17:57,880: t15.2024.03.17 val PER: 0.1297
2026-01-13 22:17:57,880: t15.2024.05.10 val PER: 0.1530
2026-01-13 22:17:57,880: t15.2024.06.14 val PER: 0.1593
2026-01-13 22:17:57,880: t15.2024.07.19 val PER: 0.2235
2026-01-13 22:17:57,881: t15.2024.07.21 val PER: 0.0821
2026-01-13 22:17:57,881: t15.2024.07.28 val PER: 0.1228
2026-01-13 22:17:57,881: t15.2025.01.10 val PER: 0.2906
2026-01-13 22:17:57,881: t15.2025.01.12 val PER: 0.1424
2026-01-13 22:17:57,881: t15.2025.03.14 val PER: 0.3402
2026-01-13 22:17:57,881: t15.2025.03.16 val PER: 0.1793
2026-01-13 22:17:57,881: t15.2025.03.30 val PER: 0.2770
2026-01-13 22:17:57,881: t15.2025.04.13 val PER: 0.1912
2026-01-13 22:17:58,025: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_14000
2026-01-13 22:18:15,667: Train batch 14200: loss: 6.05 grad norm: 47.34 time: 0.056
2026-01-13 22:18:33,674: Train batch 14400: loss: 4.55 grad norm: 39.21 time: 0.064
2026-01-13 22:18:42,625: Running test after training batch: 14500
2026-01-13 22:18:42,757: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:18:47,479: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:18:47,531: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-13 22:18:58,597: Val batch 14500: PER (avg): 0.1400 CTC Loss (avg): 15.4244 WER(5gram): 16.56% (n=256) time: 15.972
2026-01-13 22:18:58,598: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=13
2026-01-13 22:18:58,598: t15.2023.08.13 val PER: 0.1060
2026-01-13 22:18:58,598: t15.2023.08.18 val PER: 0.0939
2026-01-13 22:18:58,598: t15.2023.08.20 val PER: 0.0969
2026-01-13 22:18:58,598: t15.2023.08.25 val PER: 0.0934
2026-01-13 22:18:58,598: t15.2023.08.27 val PER: 0.1865
2026-01-13 22:18:58,598: t15.2023.09.01 val PER: 0.0649
2026-01-13 22:18:58,599: t15.2023.09.03 val PER: 0.1580
2026-01-13 22:18:58,599: t15.2023.09.24 val PER: 0.1165
2026-01-13 22:18:58,599: t15.2023.09.29 val PER: 0.1174
2026-01-13 22:18:58,599: t15.2023.10.01 val PER: 0.1691
2026-01-13 22:18:58,599: t15.2023.10.06 val PER: 0.0840
2026-01-13 22:18:58,599: t15.2023.10.08 val PER: 0.2422
2026-01-13 22:18:58,599: t15.2023.10.13 val PER: 0.1885
2026-01-13 22:18:58,599: t15.2023.10.15 val PER: 0.1444
2026-01-13 22:18:58,599: t15.2023.10.20 val PER: 0.1711
2026-01-13 22:18:58,599: t15.2023.10.22 val PER: 0.1080
2026-01-13 22:18:58,599: t15.2023.11.03 val PER: 0.1757
2026-01-13 22:18:58,599: t15.2023.11.04 val PER: 0.0239
2026-01-13 22:18:58,599: t15.2023.11.17 val PER: 0.0358
2026-01-13 22:18:58,600: t15.2023.11.19 val PER: 0.0299
2026-01-13 22:18:58,600: t15.2023.11.26 val PER: 0.1029
2026-01-13 22:18:58,600: t15.2023.12.03 val PER: 0.0840
2026-01-13 22:18:58,600: t15.2023.12.08 val PER: 0.0826
2026-01-13 22:18:58,600: t15.2023.12.10 val PER: 0.0762
2026-01-13 22:18:58,600: t15.2023.12.17 val PER: 0.1331
2026-01-13 22:18:58,600: t15.2023.12.29 val PER: 0.1194
2026-01-13 22:18:58,600: t15.2024.02.25 val PER: 0.1096
2026-01-13 22:18:58,600: t15.2024.03.08 val PER: 0.2119
2026-01-13 22:18:58,600: t15.2024.03.15 val PER: 0.1889
2026-01-13 22:18:58,600: t15.2024.03.17 val PER: 0.1220
2026-01-13 22:18:58,600: t15.2024.05.10 val PER: 0.1412
2026-01-13 22:18:58,600: t15.2024.06.14 val PER: 0.1719
2026-01-13 22:18:58,601: t15.2024.07.19 val PER: 0.2182
2026-01-13 22:18:58,601: t15.2024.07.21 val PER: 0.0786
2026-01-13 22:18:58,601: t15.2024.07.28 val PER: 0.1257
2026-01-13 22:18:58,601: t15.2025.01.10 val PER: 0.2645
2026-01-13 22:18:58,601: t15.2025.01.12 val PER: 0.1432
2026-01-13 22:18:58,601: t15.2025.03.14 val PER: 0.3447
2026-01-13 22:18:58,601: t15.2025.03.16 val PER: 0.1819
2026-01-13 22:18:58,601: t15.2025.03.30 val PER: 0.2793
2026-01-13 22:18:58,602: t15.2025.04.13 val PER: 0.1954
2026-01-13 22:18:58,745: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_14500
2026-01-13 22:19:08,290: Train batch 14600: loss: 8.61 grad norm: 56.82 time: 0.059
2026-01-13 22:19:26,193: Train batch 14800: loss: 3.80 grad norm: 36.27 time: 0.050
2026-01-13 22:19:43,892: Train batch 15000: loss: 4.46 grad norm: 42.13 time: 0.052
2026-01-13 22:19:43,892: Running test after training batch: 15000
2026-01-13 22:19:44,019: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:19:48,813: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:19:48,864: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost net
2026-01-13 22:20:00,436: Val batch 15000: PER (avg): 0.1394 CTC Loss (avg): 15.2470 WER(5gram): 17.28% (n=256) time: 16.543
2026-01-13 22:20:00,436: WER lens: avg_true_words=5.99 avg_pred_words=6.26 max_pred_words=13
2026-01-13 22:20:00,436: t15.2023.08.13 val PER: 0.1071
2026-01-13 22:20:00,436: t15.2023.08.18 val PER: 0.0939
2026-01-13 22:20:00,437: t15.2023.08.20 val PER: 0.0890
2026-01-13 22:20:00,437: t15.2023.08.25 val PER: 0.0858
2026-01-13 22:20:00,437: t15.2023.08.27 val PER: 0.1801
2026-01-13 22:20:00,437: t15.2023.09.01 val PER: 0.0747
2026-01-13 22:20:00,437: t15.2023.09.03 val PER: 0.1544
2026-01-13 22:20:00,437: t15.2023.09.24 val PER: 0.1214
2026-01-13 22:20:00,437: t15.2023.09.29 val PER: 0.1181
2026-01-13 22:20:00,437: t15.2023.10.01 val PER: 0.1559
2026-01-13 22:20:00,437: t15.2023.10.06 val PER: 0.0829
2026-01-13 22:20:00,437: t15.2023.10.08 val PER: 0.2382
2026-01-13 22:20:00,437: t15.2023.10.13 val PER: 0.1815
2026-01-13 22:20:00,437: t15.2023.10.15 val PER: 0.1397
2026-01-13 22:20:00,437: t15.2023.10.20 val PER: 0.1644
2026-01-13 22:20:00,437: t15.2023.10.22 val PER: 0.1158
2026-01-13 22:20:00,437: t15.2023.11.03 val PER: 0.1703
2026-01-13 22:20:00,438: t15.2023.11.04 val PER: 0.0273
2026-01-13 22:20:00,438: t15.2023.11.17 val PER: 0.0311
2026-01-13 22:20:00,438: t15.2023.11.19 val PER: 0.0259
2026-01-13 22:20:00,438: t15.2023.11.26 val PER: 0.1058
2026-01-13 22:20:00,438: t15.2023.12.03 val PER: 0.0998
2026-01-13 22:20:00,438: t15.2023.12.08 val PER: 0.0772
2026-01-13 22:20:00,438: t15.2023.12.10 val PER: 0.0749
2026-01-13 22:20:00,438: t15.2023.12.17 val PER: 0.1435
2026-01-13 22:20:00,438: t15.2023.12.29 val PER: 0.1235
2026-01-13 22:20:00,438: t15.2024.02.25 val PER: 0.0955
2026-01-13 22:20:00,438: t15.2024.03.08 val PER: 0.2162
2026-01-13 22:20:00,438: t15.2024.03.15 val PER: 0.1914
2026-01-13 22:20:00,438: t15.2024.03.17 val PER: 0.1137
2026-01-13 22:20:00,438: t15.2024.05.10 val PER: 0.1382
2026-01-13 22:20:00,439: t15.2024.06.14 val PER: 0.1703
2026-01-13 22:20:00,439: t15.2024.07.19 val PER: 0.2030
2026-01-13 22:20:00,439: t15.2024.07.21 val PER: 0.0834
2026-01-13 22:20:00,439: t15.2024.07.28 val PER: 0.1221
2026-01-13 22:20:00,439: t15.2025.01.10 val PER: 0.3058
2026-01-13 22:20:00,439: t15.2025.01.12 val PER: 0.1463
2026-01-13 22:20:00,439: t15.2025.03.14 val PER: 0.3476
2026-01-13 22:20:00,439: t15.2025.03.16 val PER: 0.1846
2026-01-13 22:20:00,439: t15.2025.03.30 val PER: 0.2747
2026-01-13 22:20:00,439: t15.2025.04.13 val PER: 0.2054
2026-01-13 22:20:00,581: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_15000
2026-01-13 22:20:18,971: Train batch 15200: loss: 3.41 grad norm: 38.68 time: 0.057
2026-01-13 22:20:37,055: Train batch 15400: loss: 7.37 grad norm: 50.91 time: 0.049
2026-01-13 22:20:46,280: Running test after training batch: 15500
2026-01-13 22:20:46,456: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:20:51,368: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:20:51,430: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-13 22:21:02,503: Val batch 15500: PER (avg): 0.1352 CTC Loss (avg): 15.1416 WER(5gram): 18.38% (n=256) time: 16.223
2026-01-13 22:21:02,504: WER lens: avg_true_words=5.99 avg_pred_words=6.25 max_pred_words=12
2026-01-13 22:21:02,504: t15.2023.08.13 val PER: 0.1081
2026-01-13 22:21:02,504: t15.2023.08.18 val PER: 0.0889
2026-01-13 22:21:02,504: t15.2023.08.20 val PER: 0.0913
2026-01-13 22:21:02,504: t15.2023.08.25 val PER: 0.0858
2026-01-13 22:21:02,505: t15.2023.08.27 val PER: 0.1785
2026-01-13 22:21:02,505: t15.2023.09.01 val PER: 0.0601
2026-01-13 22:21:02,505: t15.2023.09.03 val PER: 0.1615
2026-01-13 22:21:02,505: t15.2023.09.24 val PER: 0.1117
2026-01-13 22:21:02,505: t15.2023.09.29 val PER: 0.1130
2026-01-13 22:21:02,505: t15.2023.10.01 val PER: 0.1612
2026-01-13 22:21:02,505: t15.2023.10.06 val PER: 0.0775
2026-01-13 22:21:02,505: t15.2023.10.08 val PER: 0.2206
2026-01-13 22:21:02,505: t15.2023.10.13 val PER: 0.1901
2026-01-13 22:21:02,506: t15.2023.10.15 val PER: 0.1351
2026-01-13 22:21:02,506: t15.2023.10.20 val PER: 0.1745
2026-01-13 22:21:02,506: t15.2023.10.22 val PER: 0.1169
2026-01-13 22:21:02,506: t15.2023.11.03 val PER: 0.1649
2026-01-13 22:21:02,506: t15.2023.11.04 val PER: 0.0273
2026-01-13 22:21:02,506: t15.2023.11.17 val PER: 0.0327
2026-01-13 22:21:02,506: t15.2023.11.19 val PER: 0.0299
2026-01-13 22:21:02,506: t15.2023.11.26 val PER: 0.0870
2026-01-13 22:21:02,506: t15.2023.12.03 val PER: 0.0966
2026-01-13 22:21:02,506: t15.2023.12.08 val PER: 0.0752
2026-01-13 22:21:02,506: t15.2023.12.10 val PER: 0.0696
2026-01-13 22:21:02,507: t15.2023.12.17 val PER: 0.1299
2026-01-13 22:21:02,507: t15.2023.12.29 val PER: 0.1146
2026-01-13 22:21:02,507: t15.2024.02.25 val PER: 0.0983
2026-01-13 22:21:02,507: t15.2024.03.08 val PER: 0.2119
2026-01-13 22:21:02,507: t15.2024.03.15 val PER: 0.1932
2026-01-13 22:21:02,507: t15.2024.03.17 val PER: 0.1137
2026-01-13 22:21:02,507: t15.2024.05.10 val PER: 0.1382
2026-01-13 22:21:02,508: t15.2024.06.14 val PER: 0.1498
2026-01-13 22:21:02,508: t15.2024.07.19 val PER: 0.2057
2026-01-13 22:21:02,508: t15.2024.07.21 val PER: 0.0786
2026-01-13 22:21:02,508: t15.2024.07.28 val PER: 0.1243
2026-01-13 22:21:02,508: t15.2025.01.10 val PER: 0.2796
2026-01-13 22:21:02,508: t15.2025.01.12 val PER: 0.1424
2026-01-13 22:21:02,508: t15.2025.03.14 val PER: 0.3358
2026-01-13 22:21:02,508: t15.2025.03.16 val PER: 0.1649
2026-01-13 22:21:02,508: t15.2025.03.30 val PER: 0.2540
2026-01-13 22:21:02,509: t15.2025.04.13 val PER: 0.2054
2026-01-13 22:21:02,652: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_15500
2026-01-13 22:21:11,392: Train batch 15600: loss: 7.24 grad norm: 51.98 time: 0.061
2026-01-13 22:21:29,501: Train batch 15800: loss: 8.68 grad norm: 57.25 time: 0.069
2026-01-13 22:21:47,234: Train batch 16000: loss: 5.11 grad norm: 40.96 time: 0.055
2026-01-13 22:21:47,234: Running test after training batch: 16000
2026-01-13 22:21:47,383: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:21:52,614: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:21:52,707: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-13 22:22:11,521: Val batch 16000: PER (avg): 0.1357 CTC Loss (avg): 15.2090 WER(5gram): 15.91% (n=256) time: 24.287
2026-01-13 22:22:11,522: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-13 22:22:11,522: t15.2023.08.13 val PER: 0.1019
2026-01-13 22:22:11,522: t15.2023.08.18 val PER: 0.0989
2026-01-13 22:22:11,522: t15.2023.08.20 val PER: 0.0913
2026-01-13 22:22:11,522: t15.2023.08.25 val PER: 0.0783
2026-01-13 22:22:11,522: t15.2023.08.27 val PER: 0.1704
2026-01-13 22:22:11,522: t15.2023.09.01 val PER: 0.0706
2026-01-13 22:22:11,522: t15.2023.09.03 val PER: 0.1461
2026-01-13 22:22:11,523: t15.2023.09.24 val PER: 0.1068
2026-01-13 22:22:11,523: t15.2023.09.29 val PER: 0.1168
2026-01-13 22:22:11,523: t15.2023.10.01 val PER: 0.1612
2026-01-13 22:22:11,523: t15.2023.10.06 val PER: 0.0840
2026-01-13 22:22:11,524: t15.2023.10.08 val PER: 0.2219
2026-01-13 22:22:11,524: t15.2023.10.13 val PER: 0.1916
2026-01-13 22:22:11,524: t15.2023.10.15 val PER: 0.1404
2026-01-13 22:22:11,524: t15.2023.10.20 val PER: 0.1644
2026-01-13 22:22:11,524: t15.2023.10.22 val PER: 0.1203
2026-01-13 22:22:11,524: t15.2023.11.03 val PER: 0.1737
2026-01-13 22:22:11,524: t15.2023.11.04 val PER: 0.0307
2026-01-13 22:22:11,524: t15.2023.11.17 val PER: 0.0358
2026-01-13 22:22:11,524: t15.2023.11.19 val PER: 0.0339
2026-01-13 22:22:11,524: t15.2023.11.26 val PER: 0.0906
2026-01-13 22:22:11,524: t15.2023.12.03 val PER: 0.0872
2026-01-13 22:22:11,524: t15.2023.12.08 val PER: 0.0739
2026-01-13 22:22:11,524: t15.2023.12.10 val PER: 0.0696
2026-01-13 22:22:11,524: t15.2023.12.17 val PER: 0.1164
2026-01-13 22:22:11,525: t15.2023.12.29 val PER: 0.1078
2026-01-13 22:22:11,525: t15.2024.02.25 val PER: 0.1039
2026-01-13 22:22:11,525: t15.2024.03.08 val PER: 0.2063
2026-01-13 22:22:11,525: t15.2024.03.15 val PER: 0.1851
2026-01-13 22:22:11,525: t15.2024.03.17 val PER: 0.1123
2026-01-13 22:22:11,525: t15.2024.05.10 val PER: 0.1426
2026-01-13 22:22:11,525: t15.2024.06.14 val PER: 0.1562
2026-01-13 22:22:11,525: t15.2024.07.19 val PER: 0.2129
2026-01-13 22:22:11,525: t15.2024.07.21 val PER: 0.0848
2026-01-13 22:22:11,525: t15.2024.07.28 val PER: 0.1235
2026-01-13 22:22:11,525: t15.2025.01.10 val PER: 0.2906
2026-01-13 22:22:11,525: t15.2025.01.12 val PER: 0.1393
2026-01-13 22:22:11,525: t15.2025.03.14 val PER: 0.3284
2026-01-13 22:22:11,525: t15.2025.03.16 val PER: 0.1636
2026-01-13 22:22:11,525: t15.2025.03.30 val PER: 0.2713
2026-01-13 22:22:11,525: t15.2025.04.13 val PER: 0.1940
2026-01-13 22:22:11,668: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_16000
2026-01-13 22:22:29,792: Train batch 16200: loss: 3.90 grad norm: 45.21 time: 0.055
2026-01-13 22:22:48,087: Train batch 16400: loss: 6.90 grad norm: 52.12 time: 0.057
2026-01-13 22:22:57,355: Running test after training batch: 16500
2026-01-13 22:22:57,497: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:23:02,270: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:23:02,317: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost it
2026-01-13 22:23:13,845: Val batch 16500: PER (avg): 0.1314 CTC Loss (avg): 14.9595 WER(5gram): 17.28% (n=256) time: 16.489
2026-01-13 22:23:13,845: WER lens: avg_true_words=5.99 avg_pred_words=6.28 max_pred_words=13
2026-01-13 22:23:13,845: t15.2023.08.13 val PER: 0.1050
2026-01-13 22:23:13,845: t15.2023.08.18 val PER: 0.0863
2026-01-13 22:23:13,845: t15.2023.08.20 val PER: 0.0913
2026-01-13 22:23:13,846: t15.2023.08.25 val PER: 0.0828
2026-01-13 22:23:13,846: t15.2023.08.27 val PER: 0.1768
2026-01-13 22:23:13,846: t15.2023.09.01 val PER: 0.0666
2026-01-13 22:23:13,846: t15.2023.09.03 val PER: 0.1366
2026-01-13 22:23:13,846: t15.2023.09.24 val PER: 0.1129
2026-01-13 22:23:13,846: t15.2023.09.29 val PER: 0.1047
2026-01-13 22:23:13,846: t15.2023.10.01 val PER: 0.1552
2026-01-13 22:23:13,846: t15.2023.10.06 val PER: 0.0818
2026-01-13 22:23:13,846: t15.2023.10.08 val PER: 0.2246
2026-01-13 22:23:13,846: t15.2023.10.13 val PER: 0.1792
2026-01-13 22:23:13,846: t15.2023.10.15 val PER: 0.1285
2026-01-13 22:23:13,846: t15.2023.10.20 val PER: 0.1846
2026-01-13 22:23:13,847: t15.2023.10.22 val PER: 0.1002
2026-01-13 22:23:13,847: t15.2023.11.03 val PER: 0.1601
2026-01-13 22:23:13,847: t15.2023.11.04 val PER: 0.0239
2026-01-13 22:23:13,847: t15.2023.11.17 val PER: 0.0373
2026-01-13 22:23:13,847: t15.2023.11.19 val PER: 0.0240
2026-01-13 22:23:13,847: t15.2023.11.26 val PER: 0.0862
2026-01-13 22:23:13,847: t15.2023.12.03 val PER: 0.0840
2026-01-13 22:23:13,847: t15.2023.12.08 val PER: 0.0712
2026-01-13 22:23:13,847: t15.2023.12.10 val PER: 0.0618
2026-01-13 22:23:13,847: t15.2023.12.17 val PER: 0.1112
2026-01-13 22:23:13,847: t15.2023.12.29 val PER: 0.1023
2026-01-13 22:23:13,847: t15.2024.02.25 val PER: 0.0983
2026-01-13 22:23:13,847: t15.2024.03.08 val PER: 0.2134
2026-01-13 22:23:13,847: t15.2024.03.15 val PER: 0.1882
2026-01-13 22:23:13,847: t15.2024.03.17 val PER: 0.1116
2026-01-13 22:23:13,847: t15.2024.05.10 val PER: 0.1337
2026-01-13 22:23:13,848: t15.2024.06.14 val PER: 0.1498
2026-01-13 22:23:13,848: t15.2024.07.19 val PER: 0.2096
2026-01-13 22:23:13,848: t15.2024.07.21 val PER: 0.0821
2026-01-13 22:23:13,848: t15.2024.07.28 val PER: 0.1184
2026-01-13 22:23:13,848: t15.2025.01.10 val PER: 0.2893
2026-01-13 22:23:13,848: t15.2025.01.12 val PER: 0.1378
2026-01-13 22:23:13,848: t15.2025.03.14 val PER: 0.3136
2026-01-13 22:23:13,848: t15.2025.03.16 val PER: 0.1623
2026-01-13 22:23:13,848: t15.2025.03.30 val PER: 0.2655
2026-01-13 22:23:13,848: t15.2025.04.13 val PER: 0.2054
2026-01-13 22:23:13,992: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_16500
2026-01-13 22:23:23,130: Train batch 16600: loss: 5.06 grad norm: 45.64 time: 0.051
2026-01-13 22:23:40,885: Train batch 16800: loss: 10.51 grad norm: 66.38 time: 0.061
2026-01-13 22:23:58,805: Train batch 17000: loss: 4.43 grad norm: 39.68 time: 0.080
2026-01-13 22:23:58,806: Running test after training batch: 17000
2026-01-13 22:23:58,988: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:24:03,752: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:24:03,804: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 22:24:15,202: Val batch 17000: PER (avg): 0.1327 CTC Loss (avg): 15.0939 WER(5gram): 18.32% (n=256) time: 16.396
2026-01-13 22:24:15,202: WER lens: avg_true_words=5.99 avg_pred_words=6.25 max_pred_words=13
2026-01-13 22:24:15,203: t15.2023.08.13 val PER: 0.0977
2026-01-13 22:24:15,203: t15.2023.08.18 val PER: 0.0872
2026-01-13 22:24:15,203: t15.2023.08.20 val PER: 0.0882
2026-01-13 22:24:15,203: t15.2023.08.25 val PER: 0.0889
2026-01-13 22:24:15,203: t15.2023.08.27 val PER: 0.1736
2026-01-13 22:24:15,203: t15.2023.09.01 val PER: 0.0674
2026-01-13 22:24:15,203: t15.2023.09.03 val PER: 0.1449
2026-01-13 22:24:15,203: t15.2023.09.24 val PER: 0.1068
2026-01-13 22:24:15,203: t15.2023.09.29 val PER: 0.1168
2026-01-13 22:24:15,203: t15.2023.10.01 val PER: 0.1618
2026-01-13 22:24:15,204: t15.2023.10.06 val PER: 0.0753
2026-01-13 22:24:15,204: t15.2023.10.08 val PER: 0.2260
2026-01-13 22:24:15,204: t15.2023.10.13 val PER: 0.1908
2026-01-13 22:24:15,204: t15.2023.10.15 val PER: 0.1430
2026-01-13 22:24:15,204: t15.2023.10.20 val PER: 0.1577
2026-01-13 22:24:15,204: t15.2023.10.22 val PER: 0.1002
2026-01-13 22:24:15,204: t15.2023.11.03 val PER: 0.1642
2026-01-13 22:24:15,204: t15.2023.11.04 val PER: 0.0307
2026-01-13 22:24:15,204: t15.2023.11.17 val PER: 0.0342
2026-01-13 22:24:15,204: t15.2023.11.19 val PER: 0.0259
2026-01-13 22:24:15,204: t15.2023.11.26 val PER: 0.0862
2026-01-13 22:24:15,205: t15.2023.12.03 val PER: 0.0767
2026-01-13 22:24:15,205: t15.2023.12.08 val PER: 0.0666
2026-01-13 22:24:15,205: t15.2023.12.10 val PER: 0.0618
2026-01-13 22:24:15,205: t15.2023.12.17 val PER: 0.1206
2026-01-13 22:24:15,205: t15.2023.12.29 val PER: 0.1016
2026-01-13 22:24:15,205: t15.2024.02.25 val PER: 0.0941
2026-01-13 22:24:15,205: t15.2024.03.08 val PER: 0.1991
2026-01-13 22:24:15,205: t15.2024.03.15 val PER: 0.1864
2026-01-13 22:24:15,205: t15.2024.03.17 val PER: 0.1158
2026-01-13 22:24:15,205: t15.2024.05.10 val PER: 0.1501
2026-01-13 22:24:15,205: t15.2024.06.14 val PER: 0.1372
2026-01-13 22:24:15,205: t15.2024.07.19 val PER: 0.2116
2026-01-13 22:24:15,205: t15.2024.07.21 val PER: 0.0745
2026-01-13 22:24:15,205: t15.2024.07.28 val PER: 0.1213
2026-01-13 22:24:15,205: t15.2025.01.10 val PER: 0.2865
2026-01-13 22:24:15,206: t15.2025.01.12 val PER: 0.1401
2026-01-13 22:24:15,206: t15.2025.03.14 val PER: 0.3402
2026-01-13 22:24:15,206: t15.2025.03.16 val PER: 0.1623
2026-01-13 22:24:15,206: t15.2025.03.30 val PER: 0.2632
2026-01-13 22:24:15,206: t15.2025.04.13 val PER: 0.2026
2026-01-13 22:24:15,350: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_17000
2026-01-13 22:24:33,044: Train batch 17200: loss: 5.47 grad norm: 39.89 time: 0.084
2026-01-13 22:24:50,771: Train batch 17400: loss: 6.69 grad norm: 54.23 time: 0.071
2026-01-13 22:24:59,595: Running test after training batch: 17500
2026-01-13 22:24:59,699: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:25:04,442: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:25:04,494: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 22:25:15,794: Val batch 17500: PER (avg): 0.1309 CTC Loss (avg): 15.3002 WER(5gram): 17.21% (n=256) time: 16.198
2026-01-13 22:25:15,795: WER lens: avg_true_words=5.99 avg_pred_words=6.24 max_pred_words=13
2026-01-13 22:25:15,795: t15.2023.08.13 val PER: 0.0977
2026-01-13 22:25:15,795: t15.2023.08.18 val PER: 0.0947
2026-01-13 22:25:15,796: t15.2023.08.20 val PER: 0.0834
2026-01-13 22:25:15,796: t15.2023.08.25 val PER: 0.0843
2026-01-13 22:25:15,796: t15.2023.08.27 val PER: 0.1720
2026-01-13 22:25:15,796: t15.2023.09.01 val PER: 0.0682
2026-01-13 22:25:15,796: t15.2023.09.03 val PER: 0.1354
2026-01-13 22:25:15,796: t15.2023.09.24 val PER: 0.1117
2026-01-13 22:25:15,796: t15.2023.09.29 val PER: 0.1098
2026-01-13 22:25:15,796: t15.2023.10.01 val PER: 0.1625
2026-01-13 22:25:15,796: t15.2023.10.06 val PER: 0.0721
2026-01-13 22:25:15,796: t15.2023.10.08 val PER: 0.2138
2026-01-13 22:25:15,796: t15.2023.10.13 val PER: 0.1676
2026-01-13 22:25:15,796: t15.2023.10.15 val PER: 0.1312
2026-01-13 22:25:15,796: t15.2023.10.20 val PER: 0.1879
2026-01-13 22:25:15,796: t15.2023.10.22 val PER: 0.1069
2026-01-13 22:25:15,797: t15.2023.11.03 val PER: 0.1689
2026-01-13 22:25:15,797: t15.2023.11.04 val PER: 0.0273
2026-01-13 22:25:15,797: t15.2023.11.17 val PER: 0.0327
2026-01-13 22:25:15,797: t15.2023.11.19 val PER: 0.0279
2026-01-13 22:25:15,797: t15.2023.11.26 val PER: 0.0957
2026-01-13 22:25:15,797: t15.2023.12.03 val PER: 0.0819
2026-01-13 22:25:15,797: t15.2023.12.08 val PER: 0.0719
2026-01-13 22:25:15,797: t15.2023.12.10 val PER: 0.0723
2026-01-13 22:25:15,798: t15.2023.12.17 val PER: 0.1123
2026-01-13 22:25:15,798: t15.2023.12.29 val PER: 0.1126
2026-01-13 22:25:15,798: t15.2024.02.25 val PER: 0.0927
2026-01-13 22:25:15,798: t15.2024.03.08 val PER: 0.2091
2026-01-13 22:25:15,798: t15.2024.03.15 val PER: 0.1789
2026-01-13 22:25:15,798: t15.2024.03.17 val PER: 0.1102
2026-01-13 22:25:15,798: t15.2024.05.10 val PER: 0.1367
2026-01-13 22:25:15,798: t15.2024.06.14 val PER: 0.1451
2026-01-13 22:25:15,798: t15.2024.07.19 val PER: 0.2129
2026-01-13 22:25:15,798: t15.2024.07.21 val PER: 0.0800
2026-01-13 22:25:15,798: t15.2024.07.28 val PER: 0.1103
2026-01-13 22:25:15,798: t15.2025.01.10 val PER: 0.2700
2026-01-13 22:25:15,798: t15.2025.01.12 val PER: 0.1324
2026-01-13 22:25:15,798: t15.2025.03.14 val PER: 0.3314
2026-01-13 22:25:15,798: t15.2025.03.16 val PER: 0.1597
2026-01-13 22:25:15,798: t15.2025.03.30 val PER: 0.2598
2026-01-13 22:25:15,799: t15.2025.04.13 val PER: 0.1969
2026-01-13 22:25:15,939: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_17500
2026-01-13 22:25:24,993: Train batch 17600: loss: 4.79 grad norm: 40.39 time: 0.050
2026-01-13 22:25:43,283: Train batch 17800: loss: 3.60 grad norm: 40.31 time: 0.040
2026-01-13 22:26:01,356: Train batch 18000: loss: 4.98 grad norm: 47.76 time: 0.060
2026-01-13 22:26:01,357: Running test after training batch: 18000
2026-01-13 22:26:01,489: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:26:06,286: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:26:06,342: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 22:26:18,075: Val batch 18000: PER (avg): 0.1274 CTC Loss (avg): 14.9607 WER(5gram): 15.58% (n=256) time: 16.718
2026-01-13 22:26:18,076: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=13
2026-01-13 22:26:18,076: t15.2023.08.13 val PER: 0.0988
2026-01-13 22:26:18,076: t15.2023.08.18 val PER: 0.0796
2026-01-13 22:26:18,076: t15.2023.08.20 val PER: 0.0802
2026-01-13 22:26:18,076: t15.2023.08.25 val PER: 0.0964
2026-01-13 22:26:18,077: t15.2023.08.27 val PER: 0.1817
2026-01-13 22:26:18,077: t15.2023.09.01 val PER: 0.0593
2026-01-13 22:26:18,077: t15.2023.09.03 val PER: 0.1306
2026-01-13 22:26:18,077: t15.2023.09.24 val PER: 0.1129
2026-01-13 22:26:18,077: t15.2023.09.29 val PER: 0.1155
2026-01-13 22:26:18,077: t15.2023.10.01 val PER: 0.1565
2026-01-13 22:26:18,077: t15.2023.10.06 val PER: 0.0721
2026-01-13 22:26:18,077: t15.2023.10.08 val PER: 0.2233
2026-01-13 22:26:18,077: t15.2023.10.13 val PER: 0.1815
2026-01-13 22:26:18,078: t15.2023.10.15 val PER: 0.1338
2026-01-13 22:26:18,078: t15.2023.10.20 val PER: 0.1611
2026-01-13 22:26:18,078: t15.2023.10.22 val PER: 0.0969
2026-01-13 22:26:18,078: t15.2023.11.03 val PER: 0.1635
2026-01-13 22:26:18,078: t15.2023.11.04 val PER: 0.0273
2026-01-13 22:26:18,078: t15.2023.11.17 val PER: 0.0358
2026-01-13 22:26:18,078: t15.2023.11.19 val PER: 0.0220
2026-01-13 22:26:18,078: t15.2023.11.26 val PER: 0.0833
2026-01-13 22:26:18,078: t15.2023.12.03 val PER: 0.0767
2026-01-13 22:26:18,078: t15.2023.12.08 val PER: 0.0626
2026-01-13 22:26:18,079: t15.2023.12.10 val PER: 0.0578
2026-01-13 22:26:18,079: t15.2023.12.17 val PER: 0.1154
2026-01-13 22:26:18,079: t15.2023.12.29 val PER: 0.0988
2026-01-13 22:26:18,079: t15.2024.02.25 val PER: 0.0815
2026-01-13 22:26:18,079: t15.2024.03.08 val PER: 0.1991
2026-01-13 22:26:18,079: t15.2024.03.15 val PER: 0.1845
2026-01-13 22:26:18,079: t15.2024.03.17 val PER: 0.1074
2026-01-13 22:26:18,079: t15.2024.05.10 val PER: 0.1352
2026-01-13 22:26:18,079: t15.2024.06.14 val PER: 0.1325
2026-01-13 22:26:18,079: t15.2024.07.19 val PER: 0.1898
2026-01-13 22:26:18,079: t15.2024.07.21 val PER: 0.0766
2026-01-13 22:26:18,080: t15.2024.07.28 val PER: 0.1088
2026-01-13 22:26:18,080: t15.2025.01.10 val PER: 0.2658
2026-01-13 22:26:18,080: t15.2025.01.12 val PER: 0.1286
2026-01-13 22:26:18,080: t15.2025.03.14 val PER: 0.3314
2026-01-13 22:26:18,080: t15.2025.03.16 val PER: 0.1584
2026-01-13 22:26:18,080: t15.2025.03.30 val PER: 0.2598
2026-01-13 22:26:18,080: t15.2025.04.13 val PER: 0.2026
2026-01-13 22:26:18,080: New best val WER(5gram) 15.65% --> 15.58%
2026-01-13 22:26:18,235: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_18000
2026-01-13 22:26:35,880: Train batch 18200: loss: 4.14 grad norm: 43.21 time: 0.073
2026-01-13 22:26:53,330: Train batch 18400: loss: 2.27 grad norm: 34.69 time: 0.057
2026-01-13 22:27:02,101: Running test after training batch: 18500
2026-01-13 22:27:02,197: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:27:07,357: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:27:07,440: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cents
2026-01-13 22:27:27,293: Val batch 18500: PER (avg): 0.1285 CTC Loss (avg): 15.3066 WER(5gram): 17.73% (n=256) time: 25.192
2026-01-13 22:27:27,294: WER lens: avg_true_words=5.99 avg_pred_words=6.25 max_pred_words=13
2026-01-13 22:27:27,294: t15.2023.08.13 val PER: 0.1008
2026-01-13 22:27:27,294: t15.2023.08.18 val PER: 0.0922
2026-01-13 22:27:27,294: t15.2023.08.20 val PER: 0.0890
2026-01-13 22:27:27,295: t15.2023.08.25 val PER: 0.0843
2026-01-13 22:27:27,295: t15.2023.08.27 val PER: 0.1704
2026-01-13 22:27:27,295: t15.2023.09.01 val PER: 0.0617
2026-01-13 22:27:27,295: t15.2023.09.03 val PER: 0.1342
2026-01-13 22:27:27,295: t15.2023.09.24 val PER: 0.0934
2026-01-13 22:27:27,295: t15.2023.09.29 val PER: 0.1072
2026-01-13 22:27:27,295: t15.2023.10.01 val PER: 0.1532
2026-01-13 22:27:27,295: t15.2023.10.06 val PER: 0.0764
2026-01-13 22:27:27,295: t15.2023.10.08 val PER: 0.2327
2026-01-13 22:27:27,295: t15.2023.10.13 val PER: 0.1707
2026-01-13 22:27:27,295: t15.2023.10.15 val PER: 0.1292
2026-01-13 22:27:27,295: t15.2023.10.20 val PER: 0.1611
2026-01-13 22:27:27,295: t15.2023.10.22 val PER: 0.1047
2026-01-13 22:27:27,295: t15.2023.11.03 val PER: 0.1703
2026-01-13 22:27:27,295: t15.2023.11.04 val PER: 0.0239
2026-01-13 22:27:27,295: t15.2023.11.17 val PER: 0.0311
2026-01-13 22:27:27,296: t15.2023.11.19 val PER: 0.0299
2026-01-13 22:27:27,296: t15.2023.11.26 val PER: 0.0935
2026-01-13 22:27:27,296: t15.2023.12.03 val PER: 0.0819
2026-01-13 22:27:27,296: t15.2023.12.08 val PER: 0.0712
2026-01-13 22:27:27,296: t15.2023.12.10 val PER: 0.0526
2026-01-13 22:27:27,296: t15.2023.12.17 val PER: 0.1071
2026-01-13 22:27:27,296: t15.2023.12.29 val PER: 0.1016
2026-01-13 22:27:27,296: t15.2024.02.25 val PER: 0.0899
2026-01-13 22:27:27,296: t15.2024.03.08 val PER: 0.1977
2026-01-13 22:27:27,296: t15.2024.03.15 val PER: 0.1757
2026-01-13 22:27:27,296: t15.2024.03.17 val PER: 0.0997
2026-01-13 22:27:27,296: t15.2024.05.10 val PER: 0.1337
2026-01-13 22:27:27,296: t15.2024.06.14 val PER: 0.1262
2026-01-13 22:27:27,296: t15.2024.07.19 val PER: 0.2076
2026-01-13 22:27:27,296: t15.2024.07.21 val PER: 0.0752
2026-01-13 22:27:27,297: t15.2024.07.28 val PER: 0.1257
2026-01-13 22:27:27,297: t15.2025.01.10 val PER: 0.2603
2026-01-13 22:27:27,297: t15.2025.01.12 val PER: 0.1324
2026-01-13 22:27:27,297: t15.2025.03.14 val PER: 0.3299
2026-01-13 22:27:27,297: t15.2025.03.16 val PER: 0.1571
2026-01-13 22:27:27,297: t15.2025.03.30 val PER: 0.2713
2026-01-13 22:27:27,297: t15.2025.04.13 val PER: 0.1983
2026-01-13 22:27:27,437: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_18500
2026-01-13 22:27:36,076: Train batch 18600: loss: 7.11 grad norm: 64.78 time: 0.067
2026-01-13 22:27:53,737: Train batch 18800: loss: 4.03 grad norm: 43.17 time: 0.064
2026-01-13 22:28:11,476: Train batch 19000: loss: 4.43 grad norm: 42.84 time: 0.064
2026-01-13 22:28:11,477: Running test after training batch: 19000
2026-01-13 22:28:11,607: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:28:16,634: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:28:16,733: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 22:28:38,319: Val batch 19000: PER (avg): 0.1271 CTC Loss (avg): 15.0556 WER(5gram): 18.64% (n=256) time: 26.842
2026-01-13 22:28:38,320: WER lens: avg_true_words=5.99 avg_pred_words=6.29 max_pred_words=13
2026-01-13 22:28:38,320: t15.2023.08.13 val PER: 0.0925
2026-01-13 22:28:38,320: t15.2023.08.18 val PER: 0.0821
2026-01-13 22:28:38,320: t15.2023.08.20 val PER: 0.0794
2026-01-13 22:28:38,320: t15.2023.08.25 val PER: 0.0813
2026-01-13 22:28:38,320: t15.2023.08.27 val PER: 0.1511
2026-01-13 22:28:38,320: t15.2023.09.01 val PER: 0.0641
2026-01-13 22:28:38,320: t15.2023.09.03 val PER: 0.1390
2026-01-13 22:28:38,321: t15.2023.09.24 val PER: 0.1007
2026-01-13 22:28:38,321: t15.2023.09.29 val PER: 0.1072
2026-01-13 22:28:38,321: t15.2023.10.01 val PER: 0.1598
2026-01-13 22:28:38,321: t15.2023.10.06 val PER: 0.0807
2026-01-13 22:28:38,321: t15.2023.10.08 val PER: 0.2152
2026-01-13 22:28:38,321: t15.2023.10.13 val PER: 0.1777
2026-01-13 22:28:38,321: t15.2023.10.15 val PER: 0.1365
2026-01-13 22:28:38,322: t15.2023.10.20 val PER: 0.2013
2026-01-13 22:28:38,322: t15.2023.10.22 val PER: 0.0991
2026-01-13 22:28:38,322: t15.2023.11.03 val PER: 0.1635
2026-01-13 22:28:38,322: t15.2023.11.04 val PER: 0.0273
2026-01-13 22:28:38,322: t15.2023.11.17 val PER: 0.0280
2026-01-13 22:28:38,322: t15.2023.11.19 val PER: 0.0180
2026-01-13 22:28:38,322: t15.2023.11.26 val PER: 0.0855
2026-01-13 22:28:38,322: t15.2023.12.03 val PER: 0.0809
2026-01-13 22:28:38,322: t15.2023.12.08 val PER: 0.0672
2026-01-13 22:28:38,322: t15.2023.12.10 val PER: 0.0526
2026-01-13 22:28:38,322: t15.2023.12.17 val PER: 0.1040
2026-01-13 22:28:38,322: t15.2023.12.29 val PER: 0.1057
2026-01-13 22:28:38,322: t15.2024.02.25 val PER: 0.0857
2026-01-13 22:28:38,322: t15.2024.03.08 val PER: 0.1892
2026-01-13 22:28:38,323: t15.2024.03.15 val PER: 0.1726
2026-01-13 22:28:38,323: t15.2024.03.17 val PER: 0.1018
2026-01-13 22:28:38,323: t15.2024.05.10 val PER: 0.1322
2026-01-13 22:28:38,323: t15.2024.06.14 val PER: 0.1593
2026-01-13 22:28:38,323: t15.2024.07.19 val PER: 0.2030
2026-01-13 22:28:38,323: t15.2024.07.21 val PER: 0.0731
2026-01-13 22:28:38,323: t15.2024.07.28 val PER: 0.1125
2026-01-13 22:28:38,323: t15.2025.01.10 val PER: 0.2769
2026-01-13 22:28:38,323: t15.2025.01.12 val PER: 0.1247
2026-01-13 22:28:38,323: t15.2025.03.14 val PER: 0.3432
2026-01-13 22:28:38,323: t15.2025.03.16 val PER: 0.1505
2026-01-13 22:28:38,323: t15.2025.03.30 val PER: 0.2517
2026-01-13 22:28:38,323: t15.2025.04.13 val PER: 0.2111
2026-01-13 22:28:38,463: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_19000
2026-01-13 22:28:56,825: Train batch 19200: loss: 3.19 grad norm: 41.51 time: 0.064
2026-01-13 22:29:14,348: Train batch 19400: loss: 1.99 grad norm: 28.96 time: 0.053
2026-01-13 22:29:23,117: Running test after training batch: 19500
2026-01-13 22:29:23,247: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:29:28,066: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:29:28,115: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 22:29:40,132: Val batch 19500: PER (avg): 0.1277 CTC Loss (avg): 15.2894 WER(5gram): 17.54% (n=256) time: 17.014
2026-01-13 22:29:40,132: WER lens: avg_true_words=5.99 avg_pred_words=6.25 max_pred_words=13
2026-01-13 22:29:40,133: t15.2023.08.13 val PER: 0.0988
2026-01-13 22:29:40,133: t15.2023.08.18 val PER: 0.0989
2026-01-13 22:29:40,133: t15.2023.08.20 val PER: 0.0874
2026-01-13 22:29:40,133: t15.2023.08.25 val PER: 0.0768
2026-01-13 22:29:40,133: t15.2023.08.27 val PER: 0.1656
2026-01-13 22:29:40,133: t15.2023.09.01 val PER: 0.0617
2026-01-13 22:29:40,133: t15.2023.09.03 val PER: 0.1390
2026-01-13 22:29:40,133: t15.2023.09.24 val PER: 0.1056
2026-01-13 22:29:40,133: t15.2023.09.29 val PER: 0.1110
2026-01-13 22:29:40,133: t15.2023.10.01 val PER: 0.1552
2026-01-13 22:29:40,134: t15.2023.10.06 val PER: 0.0743
2026-01-13 22:29:40,134: t15.2023.10.08 val PER: 0.2246
2026-01-13 22:29:40,134: t15.2023.10.13 val PER: 0.1761
2026-01-13 22:29:40,134: t15.2023.10.15 val PER: 0.1345
2026-01-13 22:29:40,134: t15.2023.10.20 val PER: 0.1745
2026-01-13 22:29:40,134: t15.2023.10.22 val PER: 0.1058
2026-01-13 22:29:40,134: t15.2023.11.03 val PER: 0.1642
2026-01-13 22:29:40,134: t15.2023.11.04 val PER: 0.0341
2026-01-13 22:29:40,134: t15.2023.11.17 val PER: 0.0249
2026-01-13 22:29:40,134: t15.2023.11.19 val PER: 0.0259
2026-01-13 22:29:40,134: t15.2023.11.26 val PER: 0.0790
2026-01-13 22:29:40,134: t15.2023.12.03 val PER: 0.0788
2026-01-13 22:29:40,134: t15.2023.12.08 val PER: 0.0626
2026-01-13 22:29:40,134: t15.2023.12.10 val PER: 0.0591
2026-01-13 22:29:40,134: t15.2023.12.17 val PER: 0.1175
2026-01-13 22:29:40,135: t15.2023.12.29 val PER: 0.1016
2026-01-13 22:29:40,135: t15.2024.02.25 val PER: 0.0885
2026-01-13 22:29:40,135: t15.2024.03.08 val PER: 0.1935
2026-01-13 22:29:40,135: t15.2024.03.15 val PER: 0.1770
2026-01-13 22:29:40,135: t15.2024.03.17 val PER: 0.1011
2026-01-13 22:29:40,135: t15.2024.05.10 val PER: 0.1352
2026-01-13 22:29:40,135: t15.2024.06.14 val PER: 0.1372
2026-01-13 22:29:40,135: t15.2024.07.19 val PER: 0.1997
2026-01-13 22:29:40,135: t15.2024.07.21 val PER: 0.0690
2026-01-13 22:29:40,135: t15.2024.07.28 val PER: 0.1118
2026-01-13 22:29:40,135: t15.2025.01.10 val PER: 0.2672
2026-01-13 22:29:40,135: t15.2025.01.12 val PER: 0.1401
2026-01-13 22:29:40,135: t15.2025.03.14 val PER: 0.3151
2026-01-13 22:29:40,135: t15.2025.03.16 val PER: 0.1545
2026-01-13 22:29:40,135: t15.2025.03.30 val PER: 0.2598
2026-01-13 22:29:40,136: t15.2025.04.13 val PER: 0.2140
2026-01-13 22:29:40,279: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_19500
2026-01-13 22:29:49,158: Train batch 19600: loss: 3.54 grad norm: 41.10 time: 0.057
2026-01-13 22:30:07,441: Train batch 19800: loss: 3.46 grad norm: 41.22 time: 0.055
2026-01-13 22:30:25,967: Train batch 20000: loss: 3.04 grad norm: 44.92 time: 0.067
2026-01-13 22:30:25,967: Running test after training batch: 20000
2026-01-13 22:30:26,087: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:30:30,846: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:30:30,895: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost get
2026-01-13 22:30:42,362: Val batch 20000: PER (avg): 0.1246 CTC Loss (avg): 15.2862 WER(5gram): 16.69% (n=256) time: 16.395
2026-01-13 22:30:42,363: WER lens: avg_true_words=5.99 avg_pred_words=6.24 max_pred_words=13
2026-01-13 22:30:42,363: t15.2023.08.13 val PER: 0.1008
2026-01-13 22:30:42,363: t15.2023.08.18 val PER: 0.0939
2026-01-13 22:30:42,363: t15.2023.08.20 val PER: 0.0794
2026-01-13 22:30:42,363: t15.2023.08.25 val PER: 0.0873
2026-01-13 22:30:42,363: t15.2023.08.27 val PER: 0.1656
2026-01-13 22:30:42,364: t15.2023.09.01 val PER: 0.0568
2026-01-13 22:30:42,364: t15.2023.09.03 val PER: 0.1390
2026-01-13 22:30:42,364: t15.2023.09.24 val PER: 0.0971
2026-01-13 22:30:42,364: t15.2023.09.29 val PER: 0.1072
2026-01-13 22:30:42,364: t15.2023.10.01 val PER: 0.1592
2026-01-13 22:30:42,364: t15.2023.10.06 val PER: 0.0689
2026-01-13 22:30:42,364: t15.2023.10.08 val PER: 0.2057
2026-01-13 22:30:42,364: t15.2023.10.13 val PER: 0.1746
2026-01-13 22:30:42,364: t15.2023.10.15 val PER: 0.1266
2026-01-13 22:30:42,364: t15.2023.10.20 val PER: 0.1779
2026-01-13 22:30:42,364: t15.2023.10.22 val PER: 0.1036
2026-01-13 22:30:42,365: t15.2023.11.03 val PER: 0.1601
2026-01-13 22:30:42,365: t15.2023.11.04 val PER: 0.0307
2026-01-13 22:30:42,365: t15.2023.11.17 val PER: 0.0233
2026-01-13 22:30:42,365: t15.2023.11.19 val PER: 0.0240
2026-01-13 22:30:42,365: t15.2023.11.26 val PER: 0.0790
2026-01-13 22:30:42,365: t15.2023.12.03 val PER: 0.0756
2026-01-13 22:30:42,365: t15.2023.12.08 val PER: 0.0586
2026-01-13 22:30:42,365: t15.2023.12.10 val PER: 0.0499
2026-01-13 22:30:42,365: t15.2023.12.17 val PER: 0.1133
2026-01-13 22:30:42,365: t15.2023.12.29 val PER: 0.0995
2026-01-13 22:30:42,365: t15.2024.02.25 val PER: 0.0843
2026-01-13 22:30:42,365: t15.2024.03.08 val PER: 0.2148
2026-01-13 22:30:42,365: t15.2024.03.15 val PER: 0.1776
2026-01-13 22:30:42,365: t15.2024.03.17 val PER: 0.0955
2026-01-13 22:30:42,365: t15.2024.05.10 val PER: 0.1308
2026-01-13 22:30:42,365: t15.2024.06.14 val PER: 0.1498
2026-01-13 22:30:42,366: t15.2024.07.19 val PER: 0.1984
2026-01-13 22:30:42,366: t15.2024.07.21 val PER: 0.0662
2026-01-13 22:30:42,366: t15.2024.07.28 val PER: 0.1081
2026-01-13 22:30:42,366: t15.2025.01.10 val PER: 0.2796
2026-01-13 22:30:42,366: t15.2025.01.12 val PER: 0.1170
2026-01-13 22:30:42,366: t15.2025.03.14 val PER: 0.3180
2026-01-13 22:30:42,366: t15.2025.03.16 val PER: 0.1623
2026-01-13 22:30:42,366: t15.2025.03.30 val PER: 0.2333
2026-01-13 22:30:42,366: t15.2025.04.13 val PER: 0.2054
2026-01-13 22:30:42,516: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_20000
2026-01-13 22:31:00,066: Train batch 20200: loss: 2.26 grad norm: 37.62 time: 0.061
2026-01-13 22:31:17,946: Train batch 20400: loss: 3.50 grad norm: 45.31 time: 0.063
2026-01-13 22:31:26,682: Running test after training batch: 20500
2026-01-13 22:31:26,824: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:31:31,572: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:31:31,624: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cents
2026-01-13 22:31:43,506: Val batch 20500: PER (avg): 0.1242 CTC Loss (avg): 15.1970 WER(5gram): 17.73% (n=256) time: 16.823
2026-01-13 22:31:43,507: WER lens: avg_true_words=5.99 avg_pred_words=6.26 max_pred_words=13
2026-01-13 22:31:43,507: t15.2023.08.13 val PER: 0.0988
2026-01-13 22:31:43,507: t15.2023.08.18 val PER: 0.0805
2026-01-13 22:31:43,507: t15.2023.08.20 val PER: 0.0818
2026-01-13 22:31:43,507: t15.2023.08.25 val PER: 0.0678
2026-01-13 22:31:43,508: t15.2023.08.27 val PER: 0.1624
2026-01-13 22:31:43,508: t15.2023.09.01 val PER: 0.0584
2026-01-13 22:31:43,508: t15.2023.09.03 val PER: 0.1425
2026-01-13 22:31:43,508: t15.2023.09.24 val PER: 0.1007
2026-01-13 22:31:43,508: t15.2023.09.29 val PER: 0.1027
2026-01-13 22:31:43,508: t15.2023.10.01 val PER: 0.1473
2026-01-13 22:31:43,508: t15.2023.10.06 val PER: 0.0743
2026-01-13 22:31:43,508: t15.2023.10.08 val PER: 0.2233
2026-01-13 22:31:43,508: t15.2023.10.13 val PER: 0.1792
2026-01-13 22:31:43,508: t15.2023.10.15 val PER: 0.1259
2026-01-13 22:31:43,508: t15.2023.10.20 val PER: 0.1510
2026-01-13 22:31:43,509: t15.2023.10.22 val PER: 0.0980
2026-01-13 22:31:43,509: t15.2023.11.03 val PER: 0.1588
2026-01-13 22:31:43,509: t15.2023.11.04 val PER: 0.0341
2026-01-13 22:31:43,509: t15.2023.11.17 val PER: 0.0311
2026-01-13 22:31:43,509: t15.2023.11.19 val PER: 0.0160
2026-01-13 22:31:43,509: t15.2023.11.26 val PER: 0.0761
2026-01-13 22:31:43,509: t15.2023.12.03 val PER: 0.0872
2026-01-13 22:31:43,509: t15.2023.12.08 val PER: 0.0639
2026-01-13 22:31:43,509: t15.2023.12.10 val PER: 0.0512
2026-01-13 22:31:43,509: t15.2023.12.17 val PER: 0.1123
2026-01-13 22:31:43,510: t15.2023.12.29 val PER: 0.0975
2026-01-13 22:31:43,510: t15.2024.02.25 val PER: 0.0871
2026-01-13 22:31:43,510: t15.2024.03.08 val PER: 0.2148
2026-01-13 22:31:43,510: t15.2024.03.15 val PER: 0.1764
2026-01-13 22:31:43,510: t15.2024.03.17 val PER: 0.1039
2026-01-13 22:31:43,510: t15.2024.05.10 val PER: 0.1382
2026-01-13 22:31:43,510: t15.2024.06.14 val PER: 0.1498
2026-01-13 22:31:43,510: t15.2024.07.19 val PER: 0.2063
2026-01-13 22:31:43,510: t15.2024.07.21 val PER: 0.0662
2026-01-13 22:31:43,510: t15.2024.07.28 val PER: 0.1096
2026-01-13 22:31:43,510: t15.2025.01.10 val PER: 0.2590
2026-01-13 22:31:43,511: t15.2025.01.12 val PER: 0.1132
2026-01-13 22:31:43,511: t15.2025.03.14 val PER: 0.3151
2026-01-13 22:31:43,511: t15.2025.03.16 val PER: 0.1361
2026-01-13 22:31:43,511: t15.2025.03.30 val PER: 0.2621
2026-01-13 22:31:43,511: t15.2025.04.13 val PER: 0.1969
2026-01-13 22:31:43,656: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_20500
2026-01-13 22:31:52,248: Train batch 20600: loss: 2.99 grad norm: 36.09 time: 0.058
2026-01-13 22:32:10,121: Train batch 20800: loss: 2.89 grad norm: 38.65 time: 0.055
2026-01-13 22:32:28,306: Train batch 21000: loss: 2.51 grad norm: 33.97 time: 0.052
2026-01-13 22:32:28,307: Running test after training batch: 21000
2026-01-13 22:32:28,404: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:32:33,193: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:32:33,246: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 22:32:44,835: Val batch 21000: PER (avg): 0.1219 CTC Loss (avg): 15.0484 WER(5gram): 18.06% (n=256) time: 16.529
2026-01-13 22:32:44,836: WER lens: avg_true_words=5.99 avg_pred_words=6.29 max_pred_words=13
2026-01-13 22:32:44,836: t15.2023.08.13 val PER: 0.0956
2026-01-13 22:32:44,836: t15.2023.08.18 val PER: 0.0905
2026-01-13 22:32:44,836: t15.2023.08.20 val PER: 0.0874
2026-01-13 22:32:44,837: t15.2023.08.25 val PER: 0.0798
2026-01-13 22:32:44,837: t15.2023.08.27 val PER: 0.1543
2026-01-13 22:32:44,837: t15.2023.09.01 val PER: 0.0609
2026-01-13 22:32:44,837: t15.2023.09.03 val PER: 0.1306
2026-01-13 22:32:44,837: t15.2023.09.24 val PER: 0.0971
2026-01-13 22:32:44,837: t15.2023.09.29 val PER: 0.1098
2026-01-13 22:32:44,837: t15.2023.10.01 val PER: 0.1526
2026-01-13 22:32:44,837: t15.2023.10.06 val PER: 0.0624
2026-01-13 22:32:44,837: t15.2023.10.08 val PER: 0.2246
2026-01-13 22:32:44,837: t15.2023.10.13 val PER: 0.1645
2026-01-13 22:32:44,838: t15.2023.10.15 val PER: 0.1193
2026-01-13 22:32:44,838: t15.2023.10.20 val PER: 0.1611
2026-01-13 22:32:44,838: t15.2023.10.22 val PER: 0.0902
2026-01-13 22:32:44,838: t15.2023.11.03 val PER: 0.1567
2026-01-13 22:32:44,838: t15.2023.11.04 val PER: 0.0205
2026-01-13 22:32:44,838: t15.2023.11.17 val PER: 0.0295
2026-01-13 22:32:44,838: t15.2023.11.19 val PER: 0.0200
2026-01-13 22:32:44,838: t15.2023.11.26 val PER: 0.0674
2026-01-13 22:32:44,839: t15.2023.12.03 val PER: 0.0714
2026-01-13 22:32:44,839: t15.2023.12.08 val PER: 0.0566
2026-01-13 22:32:44,839: t15.2023.12.10 val PER: 0.0486
2026-01-13 22:32:44,839: t15.2023.12.17 val PER: 0.1008
2026-01-13 22:32:44,839: t15.2023.12.29 val PER: 0.0995
2026-01-13 22:32:44,839: t15.2024.02.25 val PER: 0.0871
2026-01-13 22:32:44,839: t15.2024.03.08 val PER: 0.2119
2026-01-13 22:32:44,839: t15.2024.03.15 val PER: 0.1645
2026-01-13 22:32:44,839: t15.2024.03.17 val PER: 0.0976
2026-01-13 22:32:44,839: t15.2024.05.10 val PER: 0.1337
2026-01-13 22:32:44,839: t15.2024.06.14 val PER: 0.1278
2026-01-13 22:32:44,839: t15.2024.07.19 val PER: 0.2044
2026-01-13 22:32:44,839: t15.2024.07.21 val PER: 0.0683
2026-01-13 22:32:44,840: t15.2024.07.28 val PER: 0.1154
2026-01-13 22:32:44,840: t15.2025.01.10 val PER: 0.2658
2026-01-13 22:32:44,840: t15.2025.01.12 val PER: 0.1309
2026-01-13 22:32:44,840: t15.2025.03.14 val PER: 0.3121
2026-01-13 22:32:44,840: t15.2025.03.16 val PER: 0.1479
2026-01-13 22:32:44,840: t15.2025.03.30 val PER: 0.2379
2026-01-13 22:32:44,840: t15.2025.04.13 val PER: 0.1940
2026-01-13 22:32:44,981: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_21000
2026-01-13 22:33:02,845: Train batch 21200: loss: 2.10 grad norm: 30.24 time: 0.078
2026-01-13 22:33:20,786: Train batch 21400: loss: 3.91 grad norm: 39.09 time: 0.060
2026-01-13 22:33:29,730: Running test after training batch: 21500
2026-01-13 22:33:29,835: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:33:34,628: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:33:34,676: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost said
2026-01-13 22:33:46,948: Val batch 21500: PER (avg): 0.1237 CTC Loss (avg): 15.4006 WER(5gram): 17.99% (n=256) time: 17.217
2026-01-13 22:33:46,948: WER lens: avg_true_words=5.99 avg_pred_words=6.29 max_pred_words=13
2026-01-13 22:33:46,949: t15.2023.08.13 val PER: 0.0946
2026-01-13 22:33:46,949: t15.2023.08.18 val PER: 0.0872
2026-01-13 22:33:46,949: t15.2023.08.20 val PER: 0.0858
2026-01-13 22:33:46,949: t15.2023.08.25 val PER: 0.0768
2026-01-13 22:33:46,949: t15.2023.08.27 val PER: 0.1688
2026-01-13 22:33:46,949: t15.2023.09.01 val PER: 0.0584
2026-01-13 22:33:46,949: t15.2023.09.03 val PER: 0.1390
2026-01-13 22:33:46,949: t15.2023.09.24 val PER: 0.1032
2026-01-13 22:33:46,950: t15.2023.09.29 val PER: 0.1034
2026-01-13 22:33:46,950: t15.2023.10.01 val PER: 0.1579
2026-01-13 22:33:46,950: t15.2023.10.06 val PER: 0.0732
2026-01-13 22:33:46,950: t15.2023.10.08 val PER: 0.2179
2026-01-13 22:33:46,950: t15.2023.10.13 val PER: 0.1730
2026-01-13 22:33:46,950: t15.2023.10.15 val PER: 0.1305
2026-01-13 22:33:46,950: t15.2023.10.20 val PER: 0.1779
2026-01-13 22:33:46,950: t15.2023.10.22 val PER: 0.0969
2026-01-13 22:33:46,950: t15.2023.11.03 val PER: 0.1635
2026-01-13 22:33:46,950: t15.2023.11.04 val PER: 0.0307
2026-01-13 22:33:46,951: t15.2023.11.17 val PER: 0.0327
2026-01-13 22:33:46,951: t15.2023.11.19 val PER: 0.0220
2026-01-13 22:33:46,951: t15.2023.11.26 val PER: 0.0761
2026-01-13 22:33:46,951: t15.2023.12.03 val PER: 0.0830
2026-01-13 22:33:46,951: t15.2023.12.08 val PER: 0.0606
2026-01-13 22:33:46,951: t15.2023.12.10 val PER: 0.0460
2026-01-13 22:33:46,951: t15.2023.12.17 val PER: 0.0998
2026-01-13 22:33:46,951: t15.2023.12.29 val PER: 0.0920
2026-01-13 22:33:46,951: t15.2024.02.25 val PER: 0.0744
2026-01-13 22:33:46,951: t15.2024.03.08 val PER: 0.1906
2026-01-13 22:33:46,951: t15.2024.03.15 val PER: 0.1657
2026-01-13 22:33:46,951: t15.2024.03.17 val PER: 0.1095
2026-01-13 22:33:46,952: t15.2024.05.10 val PER: 0.1412
2026-01-13 22:33:46,952: t15.2024.06.14 val PER: 0.1546
2026-01-13 22:33:46,952: t15.2024.07.19 val PER: 0.1978
2026-01-13 22:33:46,952: t15.2024.07.21 val PER: 0.0731
2026-01-13 22:33:46,952: t15.2024.07.28 val PER: 0.1066
2026-01-13 22:33:46,952: t15.2025.01.10 val PER: 0.2686
2026-01-13 22:33:46,952: t15.2025.01.12 val PER: 0.1270
2026-01-13 22:33:46,952: t15.2025.03.14 val PER: 0.3062
2026-01-13 22:33:46,952: t15.2025.03.16 val PER: 0.1414
2026-01-13 22:33:46,952: t15.2025.03.30 val PER: 0.2356
2026-01-13 22:33:46,952: t15.2025.04.13 val PER: 0.2026
2026-01-13 22:33:47,090: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_21500
2026-01-13 22:33:56,096: Train batch 21600: loss: 4.08 grad norm: 52.44 time: 0.075
2026-01-13 22:34:14,033: Train batch 21800: loss: 2.45 grad norm: 37.02 time: 0.082
2026-01-13 22:34:32,070: Train batch 22000: loss: 4.95 grad norm: 46.92 time: 0.057
2026-01-13 22:34:32,071: Running test after training batch: 22000
2026-01-13 22:34:32,229: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:34:37,381: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:34:37,464: WER debug example
  GT : how does it keep the cost down
  PR : how dusty it keep the cost cents
2026-01-13 22:35:00,480: Val batch 22000: PER (avg): 0.1226 CTC Loss (avg): 15.3881 WER(5gram): 18.51% (n=256) time: 28.409
2026-01-13 22:35:00,480: WER lens: avg_true_words=5.99 avg_pred_words=6.27 max_pred_words=13
2026-01-13 22:35:00,480: t15.2023.08.13 val PER: 0.0956
2026-01-13 22:35:00,480: t15.2023.08.18 val PER: 0.0889
2026-01-13 22:35:00,481: t15.2023.08.20 val PER: 0.0755
2026-01-13 22:35:00,481: t15.2023.08.25 val PER: 0.0843
2026-01-13 22:35:00,481: t15.2023.08.27 val PER: 0.1640
2026-01-13 22:35:00,481: t15.2023.09.01 val PER: 0.0641
2026-01-13 22:35:00,481: t15.2023.09.03 val PER: 0.1378
2026-01-13 22:35:00,481: t15.2023.09.24 val PER: 0.1007
2026-01-13 22:35:00,482: t15.2023.09.29 val PER: 0.1027
2026-01-13 22:35:00,482: t15.2023.10.01 val PER: 0.1407
2026-01-13 22:35:00,482: t15.2023.10.06 val PER: 0.0732
2026-01-13 22:35:00,482: t15.2023.10.08 val PER: 0.2111
2026-01-13 22:35:00,482: t15.2023.10.13 val PER: 0.1691
2026-01-13 22:35:00,482: t15.2023.10.15 val PER: 0.1252
2026-01-13 22:35:00,482: t15.2023.10.20 val PER: 0.1443
2026-01-13 22:35:00,482: t15.2023.10.22 val PER: 0.1047
2026-01-13 22:35:00,482: t15.2023.11.03 val PER: 0.1655
2026-01-13 22:35:00,483: t15.2023.11.04 val PER: 0.0375
2026-01-13 22:35:00,483: t15.2023.11.17 val PER: 0.0311
2026-01-13 22:35:00,483: t15.2023.11.19 val PER: 0.0200
2026-01-13 22:35:00,483: t15.2023.11.26 val PER: 0.0754
2026-01-13 22:35:00,483: t15.2023.12.03 val PER: 0.0683
2026-01-13 22:35:00,483: t15.2023.12.08 val PER: 0.0586
2026-01-13 22:35:00,483: t15.2023.12.10 val PER: 0.0552
2026-01-13 22:35:00,483: t15.2023.12.17 val PER: 0.1071
2026-01-13 22:35:00,483: t15.2023.12.29 val PER: 0.0947
2026-01-13 22:35:00,483: t15.2024.02.25 val PER: 0.0885
2026-01-13 22:35:00,483: t15.2024.03.08 val PER: 0.1949
2026-01-13 22:35:00,484: t15.2024.03.15 val PER: 0.1676
2026-01-13 22:35:00,484: t15.2024.03.17 val PER: 0.0900
2026-01-13 22:35:00,484: t15.2024.05.10 val PER: 0.1204
2026-01-13 22:35:00,484: t15.2024.06.14 val PER: 0.1420
2026-01-13 22:35:00,484: t15.2024.07.19 val PER: 0.2044
2026-01-13 22:35:00,484: t15.2024.07.21 val PER: 0.0655
2026-01-13 22:35:00,484: t15.2024.07.28 val PER: 0.1147
2026-01-13 22:35:00,484: t15.2025.01.10 val PER: 0.2631
2026-01-13 22:35:00,484: t15.2025.01.12 val PER: 0.1386
2026-01-13 22:35:00,484: t15.2025.03.14 val PER: 0.3166
2026-01-13 22:35:00,484: t15.2025.03.16 val PER: 0.1466
2026-01-13 22:35:00,484: t15.2025.03.30 val PER: 0.2460
2026-01-13 22:35:00,485: t15.2025.04.13 val PER: 0.1940
2026-01-13 22:35:00,622: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_22000
2026-01-13 22:35:18,130: Train batch 22200: loss: 2.84 grad norm: 35.27 time: 0.060
2026-01-13 22:35:35,305: Train batch 22400: loss: 2.10 grad norm: 30.21 time: 0.054
2026-01-13 22:35:44,288: Running test after training batch: 22500
2026-01-13 22:35:44,433: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:35:49,246: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:35:49,301: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 22:36:01,548: Val batch 22500: PER (avg): 0.1210 CTC Loss (avg): 15.2917 WER(5gram): 18.12% (n=256) time: 17.259
2026-01-13 22:36:01,548: WER lens: avg_true_words=5.99 avg_pred_words=6.28 max_pred_words=13
2026-01-13 22:36:01,549: t15.2023.08.13 val PER: 0.0904
2026-01-13 22:36:01,549: t15.2023.08.18 val PER: 0.0905
2026-01-13 22:36:01,549: t15.2023.08.20 val PER: 0.0786
2026-01-13 22:36:01,549: t15.2023.08.25 val PER: 0.0798
2026-01-13 22:36:01,549: t15.2023.08.27 val PER: 0.1672
2026-01-13 22:36:01,549: t15.2023.09.01 val PER: 0.0601
2026-01-13 22:36:01,549: t15.2023.09.03 val PER: 0.1330
2026-01-13 22:36:01,549: t15.2023.09.24 val PER: 0.1019
2026-01-13 22:36:01,549: t15.2023.09.29 val PER: 0.1072
2026-01-13 22:36:01,549: t15.2023.10.01 val PER: 0.1460
2026-01-13 22:36:01,550: t15.2023.10.06 val PER: 0.0646
2026-01-13 22:36:01,550: t15.2023.10.08 val PER: 0.2165
2026-01-13 22:36:01,550: t15.2023.10.13 val PER: 0.1738
2026-01-13 22:36:01,550: t15.2023.10.15 val PER: 0.1213
2026-01-13 22:36:01,550: t15.2023.10.20 val PER: 0.1678
2026-01-13 22:36:01,550: t15.2023.10.22 val PER: 0.0947
2026-01-13 22:36:01,550: t15.2023.11.03 val PER: 0.1560
2026-01-13 22:36:01,550: t15.2023.11.04 val PER: 0.0205
2026-01-13 22:36:01,550: t15.2023.11.17 val PER: 0.0249
2026-01-13 22:36:01,550: t15.2023.11.19 val PER: 0.0160
2026-01-13 22:36:01,550: t15.2023.11.26 val PER: 0.0739
2026-01-13 22:36:01,550: t15.2023.12.03 val PER: 0.0683
2026-01-13 22:36:01,550: t15.2023.12.08 val PER: 0.0593
2026-01-13 22:36:01,550: t15.2023.12.10 val PER: 0.0539
2026-01-13 22:36:01,550: t15.2023.12.17 val PER: 0.0967
2026-01-13 22:36:01,550: t15.2023.12.29 val PER: 0.0940
2026-01-13 22:36:01,551: t15.2024.02.25 val PER: 0.0772
2026-01-13 22:36:01,551: t15.2024.03.08 val PER: 0.2006
2026-01-13 22:36:01,551: t15.2024.03.15 val PER: 0.1745
2026-01-13 22:36:01,551: t15.2024.03.17 val PER: 0.1039
2026-01-13 22:36:01,551: t15.2024.05.10 val PER: 0.1293
2026-01-13 22:36:01,551: t15.2024.06.14 val PER: 0.1325
2026-01-13 22:36:01,551: t15.2024.07.19 val PER: 0.1964
2026-01-13 22:36:01,551: t15.2024.07.21 val PER: 0.0690
2026-01-13 22:36:01,551: t15.2024.07.28 val PER: 0.1088
2026-01-13 22:36:01,551: t15.2025.01.10 val PER: 0.2755
2026-01-13 22:36:01,551: t15.2025.01.12 val PER: 0.1178
2026-01-13 22:36:01,551: t15.2025.03.14 val PER: 0.3018
2026-01-13 22:36:01,551: t15.2025.03.16 val PER: 0.1427
2026-01-13 22:36:01,551: t15.2025.03.30 val PER: 0.2425
2026-01-13 22:36:01,551: t15.2025.04.13 val PER: 0.1883
2026-01-13 22:36:01,693: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_22500
2026-01-13 22:36:10,667: Train batch 22600: loss: 4.57 grad norm: 47.68 time: 0.060
2026-01-13 22:36:29,077: Train batch 22800: loss: 4.61 grad norm: 48.27 time: 0.059
2026-01-13 22:36:47,159: Train batch 23000: loss: 3.84 grad norm: 48.86 time: 0.061
2026-01-13 22:36:47,159: Running test after training batch: 23000
2026-01-13 22:36:47,310: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:36:52,102: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:36:52,156: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost set
2026-01-13 22:37:04,254: Val batch 23000: PER (avg): 0.1195 CTC Loss (avg): 15.1196 WER(5gram): 17.60% (n=256) time: 17.095
2026-01-13 22:37:04,255: WER lens: avg_true_words=5.99 avg_pred_words=6.27 max_pred_words=13
2026-01-13 22:37:04,255: t15.2023.08.13 val PER: 0.0904
2026-01-13 22:37:04,255: t15.2023.08.18 val PER: 0.0796
2026-01-13 22:37:04,255: t15.2023.08.20 val PER: 0.0675
2026-01-13 22:37:04,256: t15.2023.08.25 val PER: 0.0828
2026-01-13 22:37:04,256: t15.2023.08.27 val PER: 0.1511
2026-01-13 22:37:04,256: t15.2023.09.01 val PER: 0.0519
2026-01-13 22:37:04,256: t15.2023.09.03 val PER: 0.1437
2026-01-13 22:37:04,256: t15.2023.09.24 val PER: 0.1032
2026-01-13 22:37:04,256: t15.2023.09.29 val PER: 0.1059
2026-01-13 22:37:04,256: t15.2023.10.01 val PER: 0.1427
2026-01-13 22:37:04,256: t15.2023.10.06 val PER: 0.0657
2026-01-13 22:37:04,257: t15.2023.10.08 val PER: 0.2057
2026-01-13 22:37:04,257: t15.2023.10.13 val PER: 0.1652
2026-01-13 22:37:04,257: t15.2023.10.15 val PER: 0.1252
2026-01-13 22:37:04,257: t15.2023.10.20 val PER: 0.1678
2026-01-13 22:37:04,257: t15.2023.10.22 val PER: 0.0980
2026-01-13 22:37:04,257: t15.2023.11.03 val PER: 0.1581
2026-01-13 22:37:04,257: t15.2023.11.04 val PER: 0.0273
2026-01-13 22:37:04,257: t15.2023.11.17 val PER: 0.0249
2026-01-13 22:37:04,257: t15.2023.11.19 val PER: 0.0200
2026-01-13 22:37:04,257: t15.2023.11.26 val PER: 0.0659
2026-01-13 22:37:04,257: t15.2023.12.03 val PER: 0.0777
2026-01-13 22:37:04,257: t15.2023.12.08 val PER: 0.0632
2026-01-13 22:37:04,257: t15.2023.12.10 val PER: 0.0512
2026-01-13 22:37:04,257: t15.2023.12.17 val PER: 0.1019
2026-01-13 22:37:04,258: t15.2023.12.29 val PER: 0.0961
2026-01-13 22:37:04,258: t15.2024.02.25 val PER: 0.0801
2026-01-13 22:37:04,258: t15.2024.03.08 val PER: 0.1920
2026-01-13 22:37:04,258: t15.2024.03.15 val PER: 0.1670
2026-01-13 22:37:04,258: t15.2024.03.17 val PER: 0.0872
2026-01-13 22:37:04,258: t15.2024.05.10 val PER: 0.1367
2026-01-13 22:37:04,258: t15.2024.06.14 val PER: 0.1356
2026-01-13 22:37:04,258: t15.2024.07.19 val PER: 0.2004
2026-01-13 22:37:04,258: t15.2024.07.21 val PER: 0.0628
2026-01-13 22:37:04,258: t15.2024.07.28 val PER: 0.1022
2026-01-13 22:37:04,258: t15.2025.01.10 val PER: 0.2617
2026-01-13 22:37:04,258: t15.2025.01.12 val PER: 0.1201
2026-01-13 22:37:04,258: t15.2025.03.14 val PER: 0.3195
2026-01-13 22:37:04,258: t15.2025.03.16 val PER: 0.1571
2026-01-13 22:37:04,258: t15.2025.03.30 val PER: 0.2310
2026-01-13 22:37:04,258: t15.2025.04.13 val PER: 0.2026
2026-01-13 22:37:04,399: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_23000
2026-01-13 22:37:22,573: Train batch 23200: loss: 3.11 grad norm: 52.22 time: 0.062
2026-01-13 22:37:40,591: Train batch 23400: loss: 1.23 grad norm: 22.39 time: 0.070
2026-01-13 22:37:49,442: Running test after training batch: 23500
2026-01-13 22:37:49,540: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:37:54,897: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:37:54,994: WER debug example
  GT : how does it keep the cost down
  PR : how dusty it keep the cost set
2026-01-13 22:38:09,759: Val batch 23500: PER (avg): 0.1210 CTC Loss (avg): 15.5378 WER(5gram): 17.01% (n=256) time: 20.317
2026-01-13 22:38:09,760: WER lens: avg_true_words=5.99 avg_pred_words=6.23 max_pred_words=13
2026-01-13 22:38:09,760: t15.2023.08.13 val PER: 0.1008
2026-01-13 22:38:09,760: t15.2023.08.18 val PER: 0.0872
2026-01-13 22:38:09,760: t15.2023.08.20 val PER: 0.0763
2026-01-13 22:38:09,760: t15.2023.08.25 val PER: 0.0828
2026-01-13 22:38:09,760: t15.2023.08.27 val PER: 0.1543
2026-01-13 22:38:09,761: t15.2023.09.01 val PER: 0.0584
2026-01-13 22:38:09,761: t15.2023.09.03 val PER: 0.1354
2026-01-13 22:38:09,761: t15.2023.09.24 val PER: 0.1044
2026-01-13 22:38:09,761: t15.2023.09.29 val PER: 0.1085
2026-01-13 22:38:09,761: t15.2023.10.01 val PER: 0.1453
2026-01-13 22:38:09,761: t15.2023.10.06 val PER: 0.0657
2026-01-13 22:38:09,761: t15.2023.10.08 val PER: 0.2219
2026-01-13 22:38:09,761: t15.2023.10.13 val PER: 0.1660
2026-01-13 22:38:09,761: t15.2023.10.15 val PER: 0.1233
2026-01-13 22:38:09,761: t15.2023.10.20 val PER: 0.1678
2026-01-13 22:38:09,761: t15.2023.10.22 val PER: 0.0980
2026-01-13 22:38:09,762: t15.2023.11.03 val PER: 0.1560
2026-01-13 22:38:09,762: t15.2023.11.04 val PER: 0.0239
2026-01-13 22:38:09,762: t15.2023.11.17 val PER: 0.0280
2026-01-13 22:38:09,762: t15.2023.11.19 val PER: 0.0180
2026-01-13 22:38:09,762: t15.2023.11.26 val PER: 0.0783
2026-01-13 22:38:09,762: t15.2023.12.03 val PER: 0.0798
2026-01-13 22:38:09,762: t15.2023.12.08 val PER: 0.0613
2026-01-13 22:38:09,762: t15.2023.12.10 val PER: 0.0486
2026-01-13 22:38:09,762: t15.2023.12.17 val PER: 0.0936
2026-01-13 22:38:09,762: t15.2023.12.29 val PER: 0.0940
2026-01-13 22:38:09,762: t15.2024.02.25 val PER: 0.0857
2026-01-13 22:38:09,762: t15.2024.03.08 val PER: 0.1849
2026-01-13 22:38:09,762: t15.2024.03.15 val PER: 0.1676
2026-01-13 22:38:09,763: t15.2024.03.17 val PER: 0.0941
2026-01-13 22:38:09,763: t15.2024.05.10 val PER: 0.1367
2026-01-13 22:38:09,763: t15.2024.06.14 val PER: 0.1325
2026-01-13 22:38:09,763: t15.2024.07.19 val PER: 0.1931
2026-01-13 22:38:09,763: t15.2024.07.21 val PER: 0.0634
2026-01-13 22:38:09,763: t15.2024.07.28 val PER: 0.1140
2026-01-13 22:38:09,763: t15.2025.01.10 val PER: 0.2672
2026-01-13 22:38:09,763: t15.2025.01.12 val PER: 0.1170
2026-01-13 22:38:09,763: t15.2025.03.14 val PER: 0.3121
2026-01-13 22:38:09,763: t15.2025.03.16 val PER: 0.1584
2026-01-13 22:38:09,763: t15.2025.03.30 val PER: 0.2287
2026-01-13 22:38:09,763: t15.2025.04.13 val PER: 0.2026
2026-01-13 22:38:09,908: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_23500
2026-01-13 22:38:18,777: Train batch 23600: loss: 0.69 grad norm: 19.90 time: 0.059
2026-01-13 22:38:36,498: Train batch 23800: loss: 2.49 grad norm: 39.94 time: 0.058
2026-01-13 22:38:54,166: Train batch 24000: loss: 2.14 grad norm: 41.83 time: 0.079
2026-01-13 22:38:54,167: Running test after training batch: 24000
2026-01-13 22:38:54,271: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:38:59,335: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:38:59,438: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 22:39:16,585: Val batch 24000: PER (avg): 0.1197 CTC Loss (avg): 15.6433 WER(5gram): 17.80% (n=256) time: 22.418
2026-01-13 22:39:16,585: WER lens: avg_true_words=5.99 avg_pred_words=6.27 max_pred_words=13
2026-01-13 22:39:16,586: t15.2023.08.13 val PER: 0.0988
2026-01-13 22:39:16,586: t15.2023.08.18 val PER: 0.0754
2026-01-13 22:39:16,586: t15.2023.08.20 val PER: 0.0794
2026-01-13 22:39:16,586: t15.2023.08.25 val PER: 0.0738
2026-01-13 22:39:16,586: t15.2023.08.27 val PER: 0.1559
2026-01-13 22:39:16,586: t15.2023.09.01 val PER: 0.0568
2026-01-13 22:39:16,586: t15.2023.09.03 val PER: 0.1271
2026-01-13 22:39:16,586: t15.2023.09.24 val PER: 0.0959
2026-01-13 22:39:16,586: t15.2023.09.29 val PER: 0.1053
2026-01-13 22:39:16,586: t15.2023.10.01 val PER: 0.1480
2026-01-13 22:39:16,587: t15.2023.10.06 val PER: 0.0614
2026-01-13 22:39:16,587: t15.2023.10.08 val PER: 0.2111
2026-01-13 22:39:16,587: t15.2023.10.13 val PER: 0.1722
2026-01-13 22:39:16,587: t15.2023.10.15 val PER: 0.1193
2026-01-13 22:39:16,587: t15.2023.10.20 val PER: 0.1577
2026-01-13 22:39:16,587: t15.2023.10.22 val PER: 0.0924
2026-01-13 22:39:16,587: t15.2023.11.03 val PER: 0.1472
2026-01-13 22:39:16,587: t15.2023.11.04 val PER: 0.0239
2026-01-13 22:39:16,587: t15.2023.11.17 val PER: 0.0249
2026-01-13 22:39:16,587: t15.2023.11.19 val PER: 0.0160
2026-01-13 22:39:16,587: t15.2023.11.26 val PER: 0.0688
2026-01-13 22:39:16,587: t15.2023.12.03 val PER: 0.0725
2026-01-13 22:39:16,588: t15.2023.12.08 val PER: 0.0526
2026-01-13 22:39:16,588: t15.2023.12.10 val PER: 0.0499
2026-01-13 22:39:16,588: t15.2023.12.17 val PER: 0.1050
2026-01-13 22:39:16,588: t15.2023.12.29 val PER: 0.0879
2026-01-13 22:39:16,588: t15.2024.02.25 val PER: 0.0871
2026-01-13 22:39:16,588: t15.2024.03.08 val PER: 0.2134
2026-01-13 22:39:16,588: t15.2024.03.15 val PER: 0.1701
2026-01-13 22:39:16,588: t15.2024.03.17 val PER: 0.0976
2026-01-13 22:39:16,588: t15.2024.05.10 val PER: 0.1308
2026-01-13 22:39:16,588: t15.2024.06.14 val PER: 0.1372
2026-01-13 22:39:16,588: t15.2024.07.19 val PER: 0.1885
2026-01-13 22:39:16,588: t15.2024.07.21 val PER: 0.0724
2026-01-13 22:39:16,588: t15.2024.07.28 val PER: 0.1000
2026-01-13 22:39:16,588: t15.2025.01.10 val PER: 0.2590
2026-01-13 22:39:16,588: t15.2025.01.12 val PER: 0.1270
2026-01-13 22:39:16,588: t15.2025.03.14 val PER: 0.3151
2026-01-13 22:39:16,589: t15.2025.03.16 val PER: 0.1597
2026-01-13 22:39:16,589: t15.2025.03.30 val PER: 0.2552
2026-01-13 22:39:16,589: t15.2025.04.13 val PER: 0.1969
2026-01-13 22:39:16,734: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_24000
2026-01-13 22:39:34,863: Train batch 24200: loss: 1.70 grad norm: 35.60 time: 0.056
2026-01-13 22:39:52,883: Train batch 24400: loss: 3.97 grad norm: 43.56 time: 0.053
2026-01-13 22:40:01,841: Running test after training batch: 24500
2026-01-13 22:40:01,990: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:40:07,002: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:40:07,052: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cents
2026-01-13 22:40:19,406: Val batch 24500: PER (avg): 0.1187 CTC Loss (avg): 15.6115 WER(5gram): 18.77% (n=256) time: 17.565
2026-01-13 22:40:19,407: WER lens: avg_true_words=5.99 avg_pred_words=6.29 max_pred_words=13
2026-01-13 22:40:19,407: t15.2023.08.13 val PER: 0.0936
2026-01-13 22:40:19,407: t15.2023.08.18 val PER: 0.0830
2026-01-13 22:40:19,407: t15.2023.08.20 val PER: 0.0715
2026-01-13 22:40:19,408: t15.2023.08.25 val PER: 0.0783
2026-01-13 22:40:19,408: t15.2023.08.27 val PER: 0.1656
2026-01-13 22:40:19,408: t15.2023.09.01 val PER: 0.0560
2026-01-13 22:40:19,408: t15.2023.09.03 val PER: 0.1330
2026-01-13 22:40:19,408: t15.2023.09.24 val PER: 0.0995
2026-01-13 22:40:19,408: t15.2023.09.29 val PER: 0.1181
2026-01-13 22:40:19,408: t15.2023.10.01 val PER: 0.1387
2026-01-13 22:40:19,408: t15.2023.10.06 val PER: 0.0689
2026-01-13 22:40:19,408: t15.2023.10.08 val PER: 0.2192
2026-01-13 22:40:19,408: t15.2023.10.13 val PER: 0.1699
2026-01-13 22:40:19,409: t15.2023.10.15 val PER: 0.1121
2026-01-13 22:40:19,409: t15.2023.10.20 val PER: 0.1711
2026-01-13 22:40:19,409: t15.2023.10.22 val PER: 0.0935
2026-01-13 22:40:19,409: t15.2023.11.03 val PER: 0.1554
2026-01-13 22:40:19,409: t15.2023.11.04 val PER: 0.0239
2026-01-13 22:40:19,409: t15.2023.11.17 val PER: 0.0311
2026-01-13 22:40:19,409: t15.2023.11.19 val PER: 0.0140
2026-01-13 22:40:19,409: t15.2023.11.26 val PER: 0.0725
2026-01-13 22:40:19,409: t15.2023.12.03 val PER: 0.0620
2026-01-13 22:40:19,409: t15.2023.12.08 val PER: 0.0533
2026-01-13 22:40:19,410: t15.2023.12.10 val PER: 0.0486
2026-01-13 22:40:19,410: t15.2023.12.17 val PER: 0.0956
2026-01-13 22:40:19,410: t15.2023.12.29 val PER: 0.0940
2026-01-13 22:40:19,410: t15.2024.02.25 val PER: 0.0843
2026-01-13 22:40:19,410: t15.2024.03.08 val PER: 0.2077
2026-01-13 22:40:19,410: t15.2024.03.15 val PER: 0.1620
2026-01-13 22:40:19,410: t15.2024.03.17 val PER: 0.0886
2026-01-13 22:40:19,410: t15.2024.05.10 val PER: 0.1456
2026-01-13 22:40:19,410: t15.2024.06.14 val PER: 0.1293
2026-01-13 22:40:19,410: t15.2024.07.19 val PER: 0.1859
2026-01-13 22:40:19,410: t15.2024.07.21 val PER: 0.0655
2026-01-13 22:40:19,410: t15.2024.07.28 val PER: 0.1103
2026-01-13 22:40:19,411: t15.2025.01.10 val PER: 0.2534
2026-01-13 22:40:19,411: t15.2025.01.12 val PER: 0.1178
2026-01-13 22:40:19,411: t15.2025.03.14 val PER: 0.3018
2026-01-13 22:40:19,411: t15.2025.03.16 val PER: 0.1505
2026-01-13 22:40:19,411: t15.2025.03.30 val PER: 0.2345
2026-01-13 22:40:19,411: t15.2025.04.13 val PER: 0.2040
2026-01-13 22:40:19,549: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_24500
2026-01-13 22:40:29,079: Train batch 24600: loss: 2.51 grad norm: 35.35 time: 0.059
2026-01-13 22:40:47,040: Train batch 24800: loss: 2.62 grad norm: 45.88 time: 0.079
2026-01-13 22:41:04,805: Train batch 25000: loss: 2.97 grad norm: 45.08 time: 0.066
2026-01-13 22:41:04,806: Running test after training batch: 25000
2026-01-13 22:41:04,905: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:41:09,854: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:41:09,905: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 22:41:22,318: Val batch 25000: PER (avg): 0.1194 CTC Loss (avg): 16.3011 WER(5gram): 18.25% (n=256) time: 17.512
2026-01-13 22:41:22,319: WER lens: avg_true_words=5.99 avg_pred_words=6.26 max_pred_words=13
2026-01-13 22:41:22,319: t15.2023.08.13 val PER: 0.0936
2026-01-13 22:41:22,319: t15.2023.08.18 val PER: 0.0805
2026-01-13 22:41:22,319: t15.2023.08.20 val PER: 0.0755
2026-01-13 22:41:22,319: t15.2023.08.25 val PER: 0.0949
2026-01-13 22:41:22,320: t15.2023.08.27 val PER: 0.1640
2026-01-13 22:41:22,320: t15.2023.09.01 val PER: 0.0593
2026-01-13 22:41:22,320: t15.2023.09.03 val PER: 0.1271
2026-01-13 22:41:22,320: t15.2023.09.24 val PER: 0.0959
2026-01-13 22:41:22,320: t15.2023.09.29 val PER: 0.1040
2026-01-13 22:41:22,320: t15.2023.10.01 val PER: 0.1460
2026-01-13 22:41:22,320: t15.2023.10.06 val PER: 0.0657
2026-01-13 22:41:22,321: t15.2023.10.08 val PER: 0.2111
2026-01-13 22:41:22,321: t15.2023.10.13 val PER: 0.1567
2026-01-13 22:41:22,321: t15.2023.10.15 val PER: 0.1160
2026-01-13 22:41:22,321: t15.2023.10.20 val PER: 0.1544
2026-01-13 22:41:22,321: t15.2023.10.22 val PER: 0.0846
2026-01-13 22:41:22,321: t15.2023.11.03 val PER: 0.1520
2026-01-13 22:41:22,321: t15.2023.11.04 val PER: 0.0239
2026-01-13 22:41:22,321: t15.2023.11.17 val PER: 0.0295
2026-01-13 22:41:22,321: t15.2023.11.19 val PER: 0.0240
2026-01-13 22:41:22,321: t15.2023.11.26 val PER: 0.0739
2026-01-13 22:41:22,321: t15.2023.12.03 val PER: 0.0693
2026-01-13 22:41:22,321: t15.2023.12.08 val PER: 0.0559
2026-01-13 22:41:22,321: t15.2023.12.10 val PER: 0.0526
2026-01-13 22:41:22,322: t15.2023.12.17 val PER: 0.1050
2026-01-13 22:41:22,322: t15.2023.12.29 val PER: 0.0954
2026-01-13 22:41:22,322: t15.2024.02.25 val PER: 0.0758
2026-01-13 22:41:22,322: t15.2024.03.08 val PER: 0.1949
2026-01-13 22:41:22,322: t15.2024.03.15 val PER: 0.1714
2026-01-13 22:41:22,322: t15.2024.03.17 val PER: 0.1032
2026-01-13 22:41:22,322: t15.2024.05.10 val PER: 0.1426
2026-01-13 22:41:22,322: t15.2024.06.14 val PER: 0.1404
2026-01-13 22:41:22,322: t15.2024.07.19 val PER: 0.1905
2026-01-13 22:41:22,322: t15.2024.07.21 val PER: 0.0703
2026-01-13 22:41:22,322: t15.2024.07.28 val PER: 0.1125
2026-01-13 22:41:22,322: t15.2025.01.10 val PER: 0.2617
2026-01-13 22:41:22,322: t15.2025.01.12 val PER: 0.1209
2026-01-13 22:41:22,322: t15.2025.03.14 val PER: 0.3077
2026-01-13 22:41:22,323: t15.2025.03.16 val PER: 0.1453
2026-01-13 22:41:22,323: t15.2025.03.30 val PER: 0.2322
2026-01-13 22:41:22,323: t15.2025.04.13 val PER: 0.1912
2026-01-13 22:41:22,467: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_25000
2026-01-13 22:41:40,171: Train batch 25200: loss: 1.58 grad norm: 33.58 time: 0.057
2026-01-13 22:41:58,615: Train batch 25400: loss: 2.12 grad norm: 37.23 time: 0.055
2026-01-13 22:42:07,757: Running test after training batch: 25500
2026-01-13 22:42:07,874: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:42:12,913: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:42:13,030: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost said
2026-01-13 22:42:28,842: Val batch 25500: PER (avg): 0.1184 CTC Loss (avg): 15.7611 WER(5gram): 19.43% (n=256) time: 21.085
2026-01-13 22:42:28,842: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-13 22:42:28,842: t15.2023.08.13 val PER: 0.0915
2026-01-13 22:42:28,843: t15.2023.08.18 val PER: 0.0788
2026-01-13 22:42:28,843: t15.2023.08.20 val PER: 0.0739
2026-01-13 22:42:28,843: t15.2023.08.25 val PER: 0.0783
2026-01-13 22:42:28,843: t15.2023.08.27 val PER: 0.1495
2026-01-13 22:42:28,843: t15.2023.09.01 val PER: 0.0576
2026-01-13 22:42:28,843: t15.2023.09.03 val PER: 0.1271
2026-01-13 22:42:28,843: t15.2023.09.24 val PER: 0.0983
2026-01-13 22:42:28,843: t15.2023.09.29 val PER: 0.0983
2026-01-13 22:42:28,843: t15.2023.10.01 val PER: 0.1473
2026-01-13 22:42:28,843: t15.2023.10.06 val PER: 0.0721
2026-01-13 22:42:28,843: t15.2023.10.08 val PER: 0.2030
2026-01-13 22:42:28,844: t15.2023.10.13 val PER: 0.1559
2026-01-13 22:42:28,844: t15.2023.10.15 val PER: 0.1173
2026-01-13 22:42:28,844: t15.2023.10.20 val PER: 0.1644
2026-01-13 22:42:28,844: t15.2023.10.22 val PER: 0.0947
2026-01-13 22:42:28,844: t15.2023.11.03 val PER: 0.1594
2026-01-13 22:42:28,844: t15.2023.11.04 val PER: 0.0307
2026-01-13 22:42:28,844: t15.2023.11.17 val PER: 0.0233
2026-01-13 22:42:28,844: t15.2023.11.19 val PER: 0.0279
2026-01-13 22:42:28,844: t15.2023.11.26 val PER: 0.0630
2026-01-13 22:42:28,844: t15.2023.12.03 val PER: 0.0683
2026-01-13 22:42:28,844: t15.2023.12.08 val PER: 0.0533
2026-01-13 22:42:28,844: t15.2023.12.10 val PER: 0.0552
2026-01-13 22:42:28,845: t15.2023.12.17 val PER: 0.1081
2026-01-13 22:42:28,845: t15.2023.12.29 val PER: 0.0968
2026-01-13 22:42:28,845: t15.2024.02.25 val PER: 0.0801
2026-01-13 22:42:28,845: t15.2024.03.08 val PER: 0.1977
2026-01-13 22:42:28,845: t15.2024.03.15 val PER: 0.1664
2026-01-13 22:42:28,845: t15.2024.03.17 val PER: 0.1018
2026-01-13 22:42:28,845: t15.2024.05.10 val PER: 0.1233
2026-01-13 22:42:28,845: t15.2024.06.14 val PER: 0.1388
2026-01-13 22:42:28,845: t15.2024.07.19 val PER: 0.1912
2026-01-13 22:42:28,845: t15.2024.07.21 val PER: 0.0676
2026-01-13 22:42:28,845: t15.2024.07.28 val PER: 0.1081
2026-01-13 22:42:28,845: t15.2025.01.10 val PER: 0.2645
2026-01-13 22:42:28,845: t15.2025.01.12 val PER: 0.1201
2026-01-13 22:42:28,845: t15.2025.03.14 val PER: 0.3136
2026-01-13 22:42:28,845: t15.2025.03.16 val PER: 0.1440
2026-01-13 22:42:28,845: t15.2025.03.30 val PER: 0.2425
2026-01-13 22:42:28,845: t15.2025.04.13 val PER: 0.1840
2026-01-13 22:42:28,994: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_25500
2026-01-13 22:42:37,780: Train batch 25600: loss: 2.69 grad norm: 39.80 time: 0.058
2026-01-13 22:42:55,301: Train batch 25800: loss: 1.40 grad norm: 30.02 time: 0.061
2026-01-13 22:43:13,818: Train batch 26000: loss: 1.69 grad norm: 34.11 time: 0.063
2026-01-13 22:43:13,818: Running test after training batch: 26000
2026-01-13 22:43:13,973: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:43:18,777: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:43:18,827: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost needs
2026-01-13 22:43:30,943: Val batch 26000: PER (avg): 0.1172 CTC Loss (avg): 15.6352 WER(5gram): 19.10% (n=256) time: 17.125
2026-01-13 22:43:30,943: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=13
2026-01-13 22:43:30,944: t15.2023.08.13 val PER: 0.0894
2026-01-13 22:43:30,944: t15.2023.08.18 val PER: 0.0813
2026-01-13 22:43:30,944: t15.2023.08.20 val PER: 0.0651
2026-01-13 22:43:30,944: t15.2023.08.25 val PER: 0.0904
2026-01-13 22:43:30,944: t15.2023.08.27 val PER: 0.1511
2026-01-13 22:43:30,944: t15.2023.09.01 val PER: 0.0519
2026-01-13 22:43:30,944: t15.2023.09.03 val PER: 0.1330
2026-01-13 22:43:30,944: t15.2023.09.24 val PER: 0.0947
2026-01-13 22:43:30,944: t15.2023.09.29 val PER: 0.1027
2026-01-13 22:43:30,944: t15.2023.10.01 val PER: 0.1519
2026-01-13 22:43:30,944: t15.2023.10.06 val PER: 0.0667
2026-01-13 22:43:30,944: t15.2023.10.08 val PER: 0.2124
2026-01-13 22:43:30,945: t15.2023.10.13 val PER: 0.1513
2026-01-13 22:43:30,945: t15.2023.10.15 val PER: 0.1121
2026-01-13 22:43:30,945: t15.2023.10.20 val PER: 0.1644
2026-01-13 22:43:30,945: t15.2023.10.22 val PER: 0.0824
2026-01-13 22:43:30,945: t15.2023.11.03 val PER: 0.1554
2026-01-13 22:43:30,945: t15.2023.11.04 val PER: 0.0239
2026-01-13 22:43:30,949: t15.2023.11.17 val PER: 0.0249
2026-01-13 22:43:30,949: t15.2023.11.19 val PER: 0.0180
2026-01-13 22:43:30,949: t15.2023.11.26 val PER: 0.0645
2026-01-13 22:43:30,949: t15.2023.12.03 val PER: 0.0546
2026-01-13 22:43:30,950: t15.2023.12.08 val PER: 0.0659
2026-01-13 22:43:30,950: t15.2023.12.10 val PER: 0.0591
2026-01-13 22:43:30,950: t15.2023.12.17 val PER: 0.0967
2026-01-13 22:43:30,950: t15.2023.12.29 val PER: 0.0940
2026-01-13 22:43:30,950: t15.2024.02.25 val PER: 0.0787
2026-01-13 22:43:30,950: t15.2024.03.08 val PER: 0.1935
2026-01-13 22:43:30,950: t15.2024.03.15 val PER: 0.1645
2026-01-13 22:43:30,950: t15.2024.03.17 val PER: 0.0886
2026-01-13 22:43:30,950: t15.2024.05.10 val PER: 0.1397
2026-01-13 22:43:30,950: t15.2024.06.14 val PER: 0.1262
2026-01-13 22:43:30,951: t15.2024.07.19 val PER: 0.1866
2026-01-13 22:43:30,951: t15.2024.07.21 val PER: 0.0690
2026-01-13 22:43:30,951: t15.2024.07.28 val PER: 0.1199
2026-01-13 22:43:30,951: t15.2025.01.10 val PER: 0.2658
2026-01-13 22:43:30,951: t15.2025.01.12 val PER: 0.1116
2026-01-13 22:43:30,951: t15.2025.03.14 val PER: 0.3121
2026-01-13 22:43:30,951: t15.2025.03.16 val PER: 0.1518
2026-01-13 22:43:30,951: t15.2025.03.30 val PER: 0.2276
2026-01-13 22:43:30,951: t15.2025.04.13 val PER: 0.2011
2026-01-13 22:43:31,099: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_26000
2026-01-13 22:43:48,850: Train batch 26200: loss: 0.81 grad norm: 20.16 time: 0.063
2026-01-13 22:44:06,832: Train batch 26400: loss: 2.22 grad norm: 33.49 time: 0.077
2026-01-13 22:44:15,680: Running test after training batch: 26500
2026-01-13 22:44:15,771: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:44:20,664: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:44:20,715: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cents
2026-01-13 22:44:32,849: Val batch 26500: PER (avg): 0.1160 CTC Loss (avg): 15.8268 WER(5gram): 18.97% (n=256) time: 17.169
2026-01-13 22:44:32,850: WER lens: avg_true_words=5.99 avg_pred_words=6.31 max_pred_words=13
2026-01-13 22:44:32,850: t15.2023.08.13 val PER: 0.0884
2026-01-13 22:44:32,850: t15.2023.08.18 val PER: 0.0738
2026-01-13 22:44:32,850: t15.2023.08.20 val PER: 0.0675
2026-01-13 22:44:32,850: t15.2023.08.25 val PER: 0.0768
2026-01-13 22:44:32,851: t15.2023.08.27 val PER: 0.1527
2026-01-13 22:44:32,851: t15.2023.09.01 val PER: 0.0519
2026-01-13 22:44:32,851: t15.2023.09.03 val PER: 0.1342
2026-01-13 22:44:32,851: t15.2023.09.24 val PER: 0.0971
2026-01-13 22:44:32,851: t15.2023.09.29 val PER: 0.0964
2026-01-13 22:44:32,851: t15.2023.10.01 val PER: 0.1420
2026-01-13 22:44:32,851: t15.2023.10.06 val PER: 0.0614
2026-01-13 22:44:32,851: t15.2023.10.08 val PER: 0.2206
2026-01-13 22:44:32,851: t15.2023.10.13 val PER: 0.1567
2026-01-13 22:44:32,851: t15.2023.10.15 val PER: 0.1187
2026-01-13 22:44:32,851: t15.2023.10.20 val PER: 0.1477
2026-01-13 22:44:32,851: t15.2023.10.22 val PER: 0.0902
2026-01-13 22:44:32,852: t15.2023.11.03 val PER: 0.1520
2026-01-13 22:44:32,852: t15.2023.11.04 val PER: 0.0307
2026-01-13 22:44:32,852: t15.2023.11.17 val PER: 0.0218
2026-01-13 22:44:32,852: t15.2023.11.19 val PER: 0.0200
2026-01-13 22:44:32,852: t15.2023.11.26 val PER: 0.0659
2026-01-13 22:44:32,852: t15.2023.12.03 val PER: 0.0630
2026-01-13 22:44:32,852: t15.2023.12.08 val PER: 0.0526
2026-01-13 22:44:32,852: t15.2023.12.10 val PER: 0.0565
2026-01-13 22:44:32,852: t15.2023.12.17 val PER: 0.1112
2026-01-13 22:44:32,852: t15.2023.12.29 val PER: 0.0933
2026-01-13 22:44:32,852: t15.2024.02.25 val PER: 0.0716
2026-01-13 22:44:32,852: t15.2024.03.08 val PER: 0.2077
2026-01-13 22:44:32,852: t15.2024.03.15 val PER: 0.1707
2026-01-13 22:44:32,852: t15.2024.03.17 val PER: 0.0872
2026-01-13 22:44:32,852: t15.2024.05.10 val PER: 0.1278
2026-01-13 22:44:32,852: t15.2024.06.14 val PER: 0.1278
2026-01-13 22:44:32,852: t15.2024.07.19 val PER: 0.1806
2026-01-13 22:44:32,853: t15.2024.07.21 val PER: 0.0683
2026-01-13 22:44:32,853: t15.2024.07.28 val PER: 0.1081
2026-01-13 22:44:32,853: t15.2025.01.10 val PER: 0.2534
2026-01-13 22:44:32,853: t15.2025.01.12 val PER: 0.1147
2026-01-13 22:44:32,853: t15.2025.03.14 val PER: 0.3062
2026-01-13 22:44:32,853: t15.2025.03.16 val PER: 0.1518
2026-01-13 22:44:32,853: t15.2025.03.30 val PER: 0.2310
2026-01-13 22:44:32,853: t15.2025.04.13 val PER: 0.1940
2026-01-13 22:44:32,999: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_26500
2026-01-13 22:44:41,807: Train batch 26600: loss: 1.07 grad norm: 25.93 time: 0.058
2026-01-13 22:44:59,294: Train batch 26800: loss: 3.44 grad norm: 39.63 time: 0.079
2026-01-13 22:45:16,433: Train batch 27000: loss: 1.91 grad norm: 35.20 time: 0.063
2026-01-13 22:45:16,434: Running test after training batch: 27000
2026-01-13 22:45:16,574: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:45:21,713: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:45:21,770: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 22:45:35,172: Val batch 27000: PER (avg): 0.1198 CTC Loss (avg): 16.3610 WER(5gram): 19.30% (n=256) time: 18.738
2026-01-13 22:45:35,172: WER lens: avg_true_words=5.99 avg_pred_words=6.30 max_pred_words=13
2026-01-13 22:45:35,172: t15.2023.08.13 val PER: 0.0956
2026-01-13 22:45:35,173: t15.2023.08.18 val PER: 0.0780
2026-01-13 22:45:35,173: t15.2023.08.20 val PER: 0.0707
2026-01-13 22:45:35,173: t15.2023.08.25 val PER: 0.0828
2026-01-13 22:45:35,173: t15.2023.08.27 val PER: 0.1672
2026-01-13 22:45:35,173: t15.2023.09.01 val PER: 0.0576
2026-01-13 22:45:35,173: t15.2023.09.03 val PER: 0.1413
2026-01-13 22:45:35,173: t15.2023.09.24 val PER: 0.1007
2026-01-13 22:45:35,173: t15.2023.09.29 val PER: 0.1059
2026-01-13 22:45:35,173: t15.2023.10.01 val PER: 0.1453
2026-01-13 22:45:35,174: t15.2023.10.06 val PER: 0.0614
2026-01-13 22:45:35,174: t15.2023.10.08 val PER: 0.2152
2026-01-13 22:45:35,174: t15.2023.10.13 val PER: 0.1699
2026-01-13 22:45:35,174: t15.2023.10.15 val PER: 0.1180
2026-01-13 22:45:35,174: t15.2023.10.20 val PER: 0.1611
2026-01-13 22:45:35,174: t15.2023.10.22 val PER: 0.0958
2026-01-13 22:45:35,174: t15.2023.11.03 val PER: 0.1560
2026-01-13 22:45:35,174: t15.2023.11.04 val PER: 0.0307
2026-01-13 22:45:35,174: t15.2023.11.17 val PER: 0.0218
2026-01-13 22:45:35,174: t15.2023.11.19 val PER: 0.0200
2026-01-13 22:45:35,174: t15.2023.11.26 val PER: 0.0630
2026-01-13 22:45:35,175: t15.2023.12.03 val PER: 0.0609
2026-01-13 22:45:35,175: t15.2023.12.08 val PER: 0.0539
2026-01-13 22:45:35,175: t15.2023.12.10 val PER: 0.0578
2026-01-13 22:45:35,175: t15.2023.12.17 val PER: 0.1060
2026-01-13 22:45:35,175: t15.2023.12.29 val PER: 0.0844
2026-01-13 22:45:35,175: t15.2024.02.25 val PER: 0.0815
2026-01-13 22:45:35,175: t15.2024.03.08 val PER: 0.2134
2026-01-13 22:45:35,175: t15.2024.03.15 val PER: 0.1651
2026-01-13 22:45:35,175: t15.2024.03.17 val PER: 0.0851
2026-01-13 22:45:35,175: t15.2024.05.10 val PER: 0.1397
2026-01-13 22:45:35,175: t15.2024.06.14 val PER: 0.1356
2026-01-13 22:45:35,175: t15.2024.07.19 val PER: 0.1872
2026-01-13 22:45:35,175: t15.2024.07.21 val PER: 0.0669
2026-01-13 22:45:35,176: t15.2024.07.28 val PER: 0.1103
2026-01-13 22:45:35,176: t15.2025.01.10 val PER: 0.2645
2026-01-13 22:45:35,176: t15.2025.01.12 val PER: 0.1278
2026-01-13 22:45:35,176: t15.2025.03.14 val PER: 0.3151
2026-01-13 22:45:35,176: t15.2025.03.16 val PER: 0.1649
2026-01-13 22:45:35,176: t15.2025.03.30 val PER: 0.2517
2026-01-13 22:45:35,176: t15.2025.04.13 val PER: 0.1983
2026-01-13 22:45:35,331: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_27000
2026-01-13 22:45:53,245: Train batch 27200: loss: 1.76 grad norm: 27.89 time: 0.054
2026-01-13 22:46:11,278: Train batch 27400: loss: 2.61 grad norm: 41.35 time: 0.055
2026-01-13 22:46:20,402: Running test after training batch: 27500
2026-01-13 22:46:20,604: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:46:25,951: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:46:26,002: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 22:46:43,645: Val batch 27500: PER (avg): 0.1201 CTC Loss (avg): 16.4398 WER(5gram): 19.17% (n=256) time: 23.242
2026-01-13 22:46:43,645: WER lens: avg_true_words=5.99 avg_pred_words=6.33 max_pred_words=13
2026-01-13 22:46:43,646: t15.2023.08.13 val PER: 0.0946
2026-01-13 22:46:43,646: t15.2023.08.18 val PER: 0.0855
2026-01-13 22:46:43,646: t15.2023.08.20 val PER: 0.0699
2026-01-13 22:46:43,646: t15.2023.08.25 val PER: 0.0828
2026-01-13 22:46:43,646: t15.2023.08.27 val PER: 0.1624
2026-01-13 22:46:43,646: t15.2023.09.01 val PER: 0.0536
2026-01-13 22:46:43,646: t15.2023.09.03 val PER: 0.1366
2026-01-13 22:46:43,647: t15.2023.09.24 val PER: 0.0995
2026-01-13 22:46:43,647: t15.2023.09.29 val PER: 0.1021
2026-01-13 22:46:43,647: t15.2023.10.01 val PER: 0.1585
2026-01-13 22:46:43,647: t15.2023.10.06 val PER: 0.0667
2026-01-13 22:46:43,647: t15.2023.10.08 val PER: 0.2219
2026-01-13 22:46:43,647: t15.2023.10.13 val PER: 0.1691
2026-01-13 22:46:43,647: t15.2023.10.15 val PER: 0.1226
2026-01-13 22:46:43,647: t15.2023.10.20 val PER: 0.1644
2026-01-13 22:46:43,647: t15.2023.10.22 val PER: 0.0880
2026-01-13 22:46:43,647: t15.2023.11.03 val PER: 0.1533
2026-01-13 22:46:43,648: t15.2023.11.04 val PER: 0.0239
2026-01-13 22:46:43,648: t15.2023.11.17 val PER: 0.0218
2026-01-13 22:46:43,648: t15.2023.11.19 val PER: 0.0200
2026-01-13 22:46:43,648: t15.2023.11.26 val PER: 0.0616
2026-01-13 22:46:43,648: t15.2023.12.03 val PER: 0.0567
2026-01-13 22:46:43,648: t15.2023.12.08 val PER: 0.0526
2026-01-13 22:46:43,648: t15.2023.12.10 val PER: 0.0499
2026-01-13 22:46:43,648: t15.2023.12.17 val PER: 0.1102
2026-01-13 22:46:43,648: t15.2023.12.29 val PER: 0.0906
2026-01-13 22:46:43,648: t15.2024.02.25 val PER: 0.0843
2026-01-13 22:46:43,648: t15.2024.03.08 val PER: 0.1935
2026-01-13 22:46:43,649: t15.2024.03.15 val PER: 0.1764
2026-01-13 22:46:43,649: t15.2024.03.17 val PER: 0.0997
2026-01-13 22:46:43,649: t15.2024.05.10 val PER: 0.1278
2026-01-13 22:46:43,649: t15.2024.06.14 val PER: 0.1309
2026-01-13 22:46:43,649: t15.2024.07.19 val PER: 0.1931
2026-01-13 22:46:43,649: t15.2024.07.21 val PER: 0.0662
2026-01-13 22:46:43,649: t15.2024.07.28 val PER: 0.1103
2026-01-13 22:46:43,649: t15.2025.01.10 val PER: 0.2727
2026-01-13 22:46:43,649: t15.2025.01.12 val PER: 0.1132
2026-01-13 22:46:43,649: t15.2025.03.14 val PER: 0.3284
2026-01-13 22:46:43,649: t15.2025.03.16 val PER: 0.1387
2026-01-13 22:46:43,650: t15.2025.03.30 val PER: 0.2494
2026-01-13 22:46:43,650: t15.2025.04.13 val PER: 0.2026
2026-01-13 22:46:43,805: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_27500
2026-01-13 22:46:53,106: Train batch 27600: loss: 4.74 grad norm: 37.04 time: 0.062
2026-01-13 22:47:11,180: Train batch 27800: loss: 2.28 grad norm: 35.02 time: 0.042
2026-01-13 22:47:29,098: Train batch 28000: loss: 0.95 grad norm: 21.54 time: 0.054
2026-01-13 22:47:29,098: Running test after training batch: 28000
2026-01-13 22:47:29,272: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:47:34,117: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:47:34,170: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cents
2026-01-13 22:47:47,103: Val batch 28000: PER (avg): 0.1184 CTC Loss (avg): 16.4442 WER(5gram): 18.58% (n=256) time: 18.004
2026-01-13 22:47:47,103: WER lens: avg_true_words=5.99 avg_pred_words=6.30 max_pred_words=13
2026-01-13 22:47:47,103: t15.2023.08.13 val PER: 0.0894
2026-01-13 22:47:47,103: t15.2023.08.18 val PER: 0.0788
2026-01-13 22:47:47,103: t15.2023.08.20 val PER: 0.0699
2026-01-13 22:47:47,104: t15.2023.08.25 val PER: 0.0753
2026-01-13 22:47:47,104: t15.2023.08.27 val PER: 0.1479
2026-01-13 22:47:47,104: t15.2023.09.01 val PER: 0.0519
2026-01-13 22:47:47,104: t15.2023.09.03 val PER: 0.1342
2026-01-13 22:47:47,104: t15.2023.09.24 val PER: 0.0983
2026-01-13 22:47:47,104: t15.2023.09.29 val PER: 0.1066
2026-01-13 22:47:47,104: t15.2023.10.01 val PER: 0.1493
2026-01-13 22:47:47,104: t15.2023.10.06 val PER: 0.0592
2026-01-13 22:47:47,104: t15.2023.10.08 val PER: 0.2124
2026-01-13 22:47:47,104: t15.2023.10.13 val PER: 0.1606
2026-01-13 22:47:47,104: t15.2023.10.15 val PER: 0.1160
2026-01-13 22:47:47,104: t15.2023.10.20 val PER: 0.1745
2026-01-13 22:47:47,104: t15.2023.10.22 val PER: 0.0891
2026-01-13 22:47:47,105: t15.2023.11.03 val PER: 0.1601
2026-01-13 22:47:47,105: t15.2023.11.04 val PER: 0.0273
2026-01-13 22:47:47,105: t15.2023.11.17 val PER: 0.0264
2026-01-13 22:47:47,105: t15.2023.11.19 val PER: 0.0220
2026-01-13 22:47:47,105: t15.2023.11.26 val PER: 0.0688
2026-01-13 22:47:47,105: t15.2023.12.03 val PER: 0.0672
2026-01-13 22:47:47,105: t15.2023.12.08 val PER: 0.0519
2026-01-13 22:47:47,105: t15.2023.12.10 val PER: 0.0539
2026-01-13 22:47:47,105: t15.2023.12.17 val PER: 0.0915
2026-01-13 22:47:47,105: t15.2023.12.29 val PER: 0.0913
2026-01-13 22:47:47,105: t15.2024.02.25 val PER: 0.0801
2026-01-13 22:47:47,105: t15.2024.03.08 val PER: 0.1935
2026-01-13 22:47:47,105: t15.2024.03.15 val PER: 0.1764
2026-01-13 22:47:47,105: t15.2024.03.17 val PER: 0.1032
2026-01-13 22:47:47,106: t15.2024.05.10 val PER: 0.1352
2026-01-13 22:47:47,106: t15.2024.06.14 val PER: 0.1262
2026-01-13 22:47:47,106: t15.2024.07.19 val PER: 0.1892
2026-01-13 22:47:47,106: t15.2024.07.21 val PER: 0.0703
2026-01-13 22:47:47,106: t15.2024.07.28 val PER: 0.1066
2026-01-13 22:47:47,106: t15.2025.01.10 val PER: 0.2672
2026-01-13 22:47:47,106: t15.2025.01.12 val PER: 0.1162
2026-01-13 22:47:47,106: t15.2025.03.14 val PER: 0.3077
2026-01-13 22:47:47,106: t15.2025.03.16 val PER: 0.1505
2026-01-13 22:47:47,106: t15.2025.03.30 val PER: 0.2299
2026-01-13 22:47:47,106: t15.2025.04.13 val PER: 0.2083
2026-01-13 22:47:47,265: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_28000
2026-01-13 22:48:05,231: Train batch 28200: loss: 1.45 grad norm: 27.77 time: 0.062
2026-01-13 22:48:23,375: Train batch 28400: loss: 1.95 grad norm: 37.98 time: 0.060
2026-01-13 22:48:32,404: Running test after training batch: 28500
2026-01-13 22:48:32,533: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:48:37,562: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:48:37,613: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cents
2026-01-13 22:48:50,444: Val batch 28500: PER (avg): 0.1189 CTC Loss (avg): 16.3075 WER(5gram): 19.04% (n=256) time: 18.040
2026-01-13 22:48:50,444: WER lens: avg_true_words=5.99 avg_pred_words=6.31 max_pred_words=13
2026-01-13 22:48:50,444: t15.2023.08.13 val PER: 0.0977
2026-01-13 22:48:50,444: t15.2023.08.18 val PER: 0.0905
2026-01-13 22:48:50,445: t15.2023.08.20 val PER: 0.0731
2026-01-13 22:48:50,445: t15.2023.08.25 val PER: 0.0828
2026-01-13 22:48:50,445: t15.2023.08.27 val PER: 0.1543
2026-01-13 22:48:50,445: t15.2023.09.01 val PER: 0.0528
2026-01-13 22:48:50,445: t15.2023.09.03 val PER: 0.1378
2026-01-13 22:48:50,445: t15.2023.09.24 val PER: 0.0983
2026-01-13 22:48:50,445: t15.2023.09.29 val PER: 0.0951
2026-01-13 22:48:50,445: t15.2023.10.01 val PER: 0.1420
2026-01-13 22:48:50,445: t15.2023.10.06 val PER: 0.0667
2026-01-13 22:48:50,445: t15.2023.10.08 val PER: 0.2070
2026-01-13 22:48:50,445: t15.2023.10.13 val PER: 0.1676
2026-01-13 22:48:50,446: t15.2023.10.15 val PER: 0.1226
2026-01-13 22:48:50,446: t15.2023.10.20 val PER: 0.1711
2026-01-13 22:48:50,446: t15.2023.10.22 val PER: 0.0913
2026-01-13 22:48:50,446: t15.2023.11.03 val PER: 0.1554
2026-01-13 22:48:50,446: t15.2023.11.04 val PER: 0.0307
2026-01-13 22:48:50,446: t15.2023.11.17 val PER: 0.0358
2026-01-13 22:48:50,446: t15.2023.11.19 val PER: 0.0259
2026-01-13 22:48:50,446: t15.2023.11.26 val PER: 0.0645
2026-01-13 22:48:50,446: t15.2023.12.03 val PER: 0.0515
2026-01-13 22:48:50,446: t15.2023.12.08 val PER: 0.0573
2026-01-13 22:48:50,446: t15.2023.12.10 val PER: 0.0526
2026-01-13 22:48:50,446: t15.2023.12.17 val PER: 0.1091
2026-01-13 22:48:50,446: t15.2023.12.29 val PER: 0.0947
2026-01-13 22:48:50,446: t15.2024.02.25 val PER: 0.0772
2026-01-13 22:48:50,446: t15.2024.03.08 val PER: 0.1863
2026-01-13 22:48:50,446: t15.2024.03.15 val PER: 0.1701
2026-01-13 22:48:50,447: t15.2024.03.17 val PER: 0.0983
2026-01-13 22:48:50,447: t15.2024.05.10 val PER: 0.1278
2026-01-13 22:48:50,447: t15.2024.06.14 val PER: 0.1262
2026-01-13 22:48:50,447: t15.2024.07.19 val PER: 0.1964
2026-01-13 22:48:50,447: t15.2024.07.21 val PER: 0.0655
2026-01-13 22:48:50,447: t15.2024.07.28 val PER: 0.1140
2026-01-13 22:48:50,447: t15.2025.01.10 val PER: 0.2645
2026-01-13 22:48:50,447: t15.2025.01.12 val PER: 0.1186
2026-01-13 22:48:50,447: t15.2025.03.14 val PER: 0.3047
2026-01-13 22:48:50,447: t15.2025.03.16 val PER: 0.1518
2026-01-13 22:48:50,447: t15.2025.03.30 val PER: 0.2379
2026-01-13 22:48:50,447: t15.2025.04.13 val PER: 0.1883
2026-01-13 22:48:50,591: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_28500
2026-01-13 22:48:59,749: Train batch 28600: loss: 1.51 grad norm: 24.02 time: 0.064
2026-01-13 22:49:18,086: Train batch 28800: loss: 1.31 grad norm: 29.65 time: 0.047
2026-01-13 22:49:36,200: Train batch 29000: loss: 1.00 grad norm: 24.53 time: 0.050
2026-01-13 22:49:36,201: Running test after training batch: 29000
2026-01-13 22:49:36,302: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:49:41,471: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:49:41,525: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cents
2026-01-13 22:49:54,579: Val batch 29000: PER (avg): 0.1159 CTC Loss (avg): 16.2497 WER(5gram): 16.95% (n=256) time: 18.378
2026-01-13 22:49:54,579: WER lens: avg_true_words=5.99 avg_pred_words=6.27 max_pred_words=13
2026-01-13 22:49:54,579: t15.2023.08.13 val PER: 0.0873
2026-01-13 22:49:54,579: t15.2023.08.18 val PER: 0.0830
2026-01-13 22:49:54,579: t15.2023.08.20 val PER: 0.0715
2026-01-13 22:49:54,580: t15.2023.08.25 val PER: 0.0768
2026-01-13 22:49:54,580: t15.2023.08.27 val PER: 0.1495
2026-01-13 22:49:54,580: t15.2023.09.01 val PER: 0.0560
2026-01-13 22:49:54,580: t15.2023.09.03 val PER: 0.1271
2026-01-13 22:49:54,580: t15.2023.09.24 val PER: 0.0947
2026-01-13 22:49:54,580: t15.2023.09.29 val PER: 0.0989
2026-01-13 22:49:54,580: t15.2023.10.01 val PER: 0.1407
2026-01-13 22:49:54,580: t15.2023.10.06 val PER: 0.0603
2026-01-13 22:49:54,580: t15.2023.10.08 val PER: 0.2043
2026-01-13 22:49:54,580: t15.2023.10.13 val PER: 0.1660
2026-01-13 22:49:54,580: t15.2023.10.15 val PER: 0.1239
2026-01-13 22:49:54,580: t15.2023.10.20 val PER: 0.1678
2026-01-13 22:49:54,581: t15.2023.10.22 val PER: 0.0947
2026-01-13 22:49:54,581: t15.2023.11.03 val PER: 0.1554
2026-01-13 22:49:54,581: t15.2023.11.04 val PER: 0.0239
2026-01-13 22:49:54,581: t15.2023.11.17 val PER: 0.0249
2026-01-13 22:49:54,581: t15.2023.11.19 val PER: 0.0240
2026-01-13 22:49:54,581: t15.2023.11.26 val PER: 0.0717
2026-01-13 22:49:54,581: t15.2023.12.03 val PER: 0.0588
2026-01-13 22:49:54,581: t15.2023.12.08 val PER: 0.0426
2026-01-13 22:49:54,581: t15.2023.12.10 val PER: 0.0604
2026-01-13 22:49:54,581: t15.2023.12.17 val PER: 0.1060
2026-01-13 22:49:54,581: t15.2023.12.29 val PER: 0.0885
2026-01-13 22:49:54,581: t15.2024.02.25 val PER: 0.0744
2026-01-13 22:49:54,581: t15.2024.03.08 val PER: 0.1650
2026-01-13 22:49:54,581: t15.2024.03.15 val PER: 0.1795
2026-01-13 22:49:54,581: t15.2024.03.17 val PER: 0.0983
2026-01-13 22:49:54,581: t15.2024.05.10 val PER: 0.1233
2026-01-13 22:49:54,581: t15.2024.06.14 val PER: 0.1293
2026-01-13 22:49:54,582: t15.2024.07.19 val PER: 0.1786
2026-01-13 22:49:54,582: t15.2024.07.21 val PER: 0.0614
2026-01-13 22:49:54,582: t15.2024.07.28 val PER: 0.0985
2026-01-13 22:49:54,582: t15.2025.01.10 val PER: 0.2713
2026-01-13 22:49:54,582: t15.2025.01.12 val PER: 0.1147
2026-01-13 22:49:54,582: t15.2025.03.14 val PER: 0.2988
2026-01-13 22:49:54,582: t15.2025.03.16 val PER: 0.1505
2026-01-13 22:49:54,582: t15.2025.03.30 val PER: 0.2356
2026-01-13 22:49:54,582: t15.2025.04.13 val PER: 0.1883
2026-01-13 22:49:54,723: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_29000
2026-01-13 22:50:12,348: Train batch 29200: loss: 1.58 grad norm: 38.81 time: 0.067
2026-01-13 22:50:30,283: Train batch 29400: loss: 1.27 grad norm: 27.51 time: 0.060
2026-01-13 22:50:39,208: Running test after training batch: 29500
2026-01-13 22:50:39,353: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:50:44,220: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:50:44,292: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 22:50:58,083: Val batch 29500: PER (avg): 0.1146 CTC Loss (avg): 15.9009 WER(5gram): 18.71% (n=256) time: 18.875
2026-01-13 22:50:58,083: WER lens: avg_true_words=5.99 avg_pred_words=6.32 max_pred_words=13
2026-01-13 22:50:58,084: t15.2023.08.13 val PER: 0.0863
2026-01-13 22:50:58,084: t15.2023.08.18 val PER: 0.0939
2026-01-13 22:50:58,084: t15.2023.08.20 val PER: 0.0651
2026-01-13 22:50:58,084: t15.2023.08.25 val PER: 0.0738
2026-01-13 22:50:58,084: t15.2023.08.27 val PER: 0.1608
2026-01-13 22:50:58,084: t15.2023.09.01 val PER: 0.0552
2026-01-13 22:50:58,084: t15.2023.09.03 val PER: 0.1247
2026-01-13 22:50:58,084: t15.2023.09.24 val PER: 0.1019
2026-01-13 22:50:58,084: t15.2023.09.29 val PER: 0.1027
2026-01-13 22:50:58,084: t15.2023.10.01 val PER: 0.1526
2026-01-13 22:50:58,084: t15.2023.10.06 val PER: 0.0667
2026-01-13 22:50:58,084: t15.2023.10.08 val PER: 0.2016
2026-01-13 22:50:58,085: t15.2023.10.13 val PER: 0.1544
2026-01-13 22:50:58,085: t15.2023.10.15 val PER: 0.1061
2026-01-13 22:50:58,085: t15.2023.10.20 val PER: 0.1846
2026-01-13 22:50:58,085: t15.2023.10.22 val PER: 0.0824
2026-01-13 22:50:58,085: t15.2023.11.03 val PER: 0.1431
2026-01-13 22:50:58,085: t15.2023.11.04 val PER: 0.0273
2026-01-13 22:50:58,085: t15.2023.11.17 val PER: 0.0171
2026-01-13 22:50:58,085: t15.2023.11.19 val PER: 0.0180
2026-01-13 22:50:58,085: t15.2023.11.26 val PER: 0.0565
2026-01-13 22:50:58,085: t15.2023.12.03 val PER: 0.0609
2026-01-13 22:50:58,085: t15.2023.12.08 val PER: 0.0466
2026-01-13 22:50:58,085: t15.2023.12.10 val PER: 0.0486
2026-01-13 22:50:58,086: t15.2023.12.17 val PER: 0.1029
2026-01-13 22:50:58,086: t15.2023.12.29 val PER: 0.0906
2026-01-13 22:50:58,086: t15.2024.02.25 val PER: 0.0758
2026-01-13 22:50:58,086: t15.2024.03.08 val PER: 0.1764
2026-01-13 22:50:58,086: t15.2024.03.15 val PER: 0.1689
2026-01-13 22:50:58,086: t15.2024.03.17 val PER: 0.0830
2026-01-13 22:50:58,086: t15.2024.05.10 val PER: 0.1322
2026-01-13 22:50:58,086: t15.2024.06.14 val PER: 0.1246
2026-01-13 22:50:58,086: t15.2024.07.19 val PER: 0.1760
2026-01-13 22:50:58,086: t15.2024.07.21 val PER: 0.0607
2026-01-13 22:50:58,086: t15.2024.07.28 val PER: 0.1110
2026-01-13 22:50:58,086: t15.2025.01.10 val PER: 0.2548
2026-01-13 22:50:58,086: t15.2025.01.12 val PER: 0.1186
2026-01-13 22:50:58,086: t15.2025.03.14 val PER: 0.3047
2026-01-13 22:50:58,086: t15.2025.03.16 val PER: 0.1453
2026-01-13 22:50:58,086: t15.2025.03.30 val PER: 0.2563
2026-01-13 22:50:58,087: t15.2025.04.13 val PER: 0.1926
2026-01-13 22:50:58,230: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_29500
2026-01-13 22:51:07,351: Train batch 29600: loss: 1.18 grad norm: 35.06 time: 0.050
2026-01-13 22:51:25,355: Train batch 29800: loss: 1.39 grad norm: 31.76 time: 0.079
2026-01-13 22:51:42,757: Train batch 30000: loss: 1.75 grad norm: 37.28 time: 0.067
2026-01-13 22:51:42,757: Running test after training batch: 30000
2026-01-13 22:51:42,903: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:51:47,794: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:51:47,856: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 22:52:04,557: Val batch 30000: PER (avg): 0.1133 CTC Loss (avg): 16.2248 WER(5gram): 20.53% (n=256) time: 21.800
2026-01-13 22:52:04,558: WER lens: avg_true_words=5.99 avg_pred_words=6.39 max_pred_words=13
2026-01-13 22:52:04,558: t15.2023.08.13 val PER: 0.0852
2026-01-13 22:52:04,558: t15.2023.08.18 val PER: 0.0780
2026-01-13 22:52:04,558: t15.2023.08.20 val PER: 0.0675
2026-01-13 22:52:04,559: t15.2023.08.25 val PER: 0.0813
2026-01-13 22:52:04,559: t15.2023.08.27 val PER: 0.1495
2026-01-13 22:52:04,559: t15.2023.09.01 val PER: 0.0511
2026-01-13 22:52:04,559: t15.2023.09.03 val PER: 0.1271
2026-01-13 22:52:04,559: t15.2023.09.24 val PER: 0.1019
2026-01-13 22:52:04,559: t15.2023.09.29 val PER: 0.1015
2026-01-13 22:52:04,559: t15.2023.10.01 val PER: 0.1400
2026-01-13 22:52:04,559: t15.2023.10.06 val PER: 0.0635
2026-01-13 22:52:04,559: t15.2023.10.08 val PER: 0.1989
2026-01-13 22:52:04,560: t15.2023.10.13 val PER: 0.1497
2026-01-13 22:52:04,560: t15.2023.10.15 val PER: 0.1167
2026-01-13 22:52:04,561: t15.2023.10.20 val PER: 0.1745
2026-01-13 22:52:04,561: t15.2023.10.22 val PER: 0.0869
2026-01-13 22:52:04,561: t15.2023.11.03 val PER: 0.1418
2026-01-13 22:52:04,561: t15.2023.11.04 val PER: 0.0307
2026-01-13 22:52:04,561: t15.2023.11.17 val PER: 0.0295
2026-01-13 22:52:04,561: t15.2023.11.19 val PER: 0.0259
2026-01-13 22:52:04,561: t15.2023.11.26 val PER: 0.0601
2026-01-13 22:52:04,561: t15.2023.12.03 val PER: 0.0567
2026-01-13 22:52:04,561: t15.2023.12.08 val PER: 0.0473
2026-01-13 22:52:04,561: t15.2023.12.10 val PER: 0.0460
2026-01-13 22:52:04,561: t15.2023.12.17 val PER: 0.1216
2026-01-13 22:52:04,561: t15.2023.12.29 val PER: 0.0885
2026-01-13 22:52:04,561: t15.2024.02.25 val PER: 0.0730
2026-01-13 22:52:04,561: t15.2024.03.08 val PER: 0.1821
2026-01-13 22:52:04,561: t15.2024.03.15 val PER: 0.1657
2026-01-13 22:52:04,562: t15.2024.03.17 val PER: 0.0900
2026-01-13 22:52:04,562: t15.2024.05.10 val PER: 0.1397
2026-01-13 22:52:04,562: t15.2024.06.14 val PER: 0.1262
2026-01-13 22:52:04,562: t15.2024.07.19 val PER: 0.1859
2026-01-13 22:52:04,562: t15.2024.07.21 val PER: 0.0655
2026-01-13 22:52:04,562: t15.2024.07.28 val PER: 0.0971
2026-01-13 22:52:04,562: t15.2025.01.10 val PER: 0.2658
2026-01-13 22:52:04,562: t15.2025.01.12 val PER: 0.1139
2026-01-13 22:52:04,562: t15.2025.03.14 val PER: 0.2914
2026-01-13 22:52:04,562: t15.2025.03.16 val PER: 0.1466
2026-01-13 22:52:04,562: t15.2025.03.30 val PER: 0.2092
2026-01-13 22:52:04,562: t15.2025.04.13 val PER: 0.1740
2026-01-13 22:52:04,708: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_30000
2026-01-13 22:52:22,784: Train batch 30200: loss: 1.93 grad norm: 31.85 time: 0.069
2026-01-13 22:52:40,197: Train batch 30400: loss: 0.81 grad norm: 21.54 time: 0.062
2026-01-13 22:52:49,034: Running test after training batch: 30500
2026-01-13 22:52:49,167: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:52:54,246: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:52:54,369: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost out
2026-01-13 22:53:15,374: Val batch 30500: PER (avg): 0.1143 CTC Loss (avg): 16.0872 WER(5gram): 19.69% (n=256) time: 26.339
2026-01-13 22:53:15,374: WER lens: avg_true_words=5.99 avg_pred_words=6.39 max_pred_words=13
2026-01-13 22:53:15,374: t15.2023.08.13 val PER: 0.0780
2026-01-13 22:53:15,374: t15.2023.08.18 val PER: 0.0805
2026-01-13 22:53:15,375: t15.2023.08.20 val PER: 0.0810
2026-01-13 22:53:15,375: t15.2023.08.25 val PER: 0.0738
2026-01-13 22:53:15,375: t15.2023.08.27 val PER: 0.1415
2026-01-13 22:53:15,375: t15.2023.09.01 val PER: 0.0503
2026-01-13 22:53:15,375: t15.2023.09.03 val PER: 0.1176
2026-01-13 22:53:15,375: t15.2023.09.24 val PER: 0.0850
2026-01-13 22:53:15,375: t15.2023.09.29 val PER: 0.1015
2026-01-13 22:53:15,375: t15.2023.10.01 val PER: 0.1413
2026-01-13 22:53:15,375: t15.2023.10.06 val PER: 0.0635
2026-01-13 22:53:15,375: t15.2023.10.08 val PER: 0.2070
2026-01-13 22:53:15,375: t15.2023.10.13 val PER: 0.1621
2026-01-13 22:53:15,375: t15.2023.10.15 val PER: 0.1173
2026-01-13 22:53:15,375: t15.2023.10.20 val PER: 0.1510
2026-01-13 22:53:15,375: t15.2023.10.22 val PER: 0.0947
2026-01-13 22:53:15,376: t15.2023.11.03 val PER: 0.1520
2026-01-13 22:53:15,376: t15.2023.11.04 val PER: 0.0171
2026-01-13 22:53:15,376: t15.2023.11.17 val PER: 0.0233
2026-01-13 22:53:15,376: t15.2023.11.19 val PER: 0.0160
2026-01-13 22:53:15,376: t15.2023.11.26 val PER: 0.0623
2026-01-13 22:53:15,376: t15.2023.12.03 val PER: 0.0693
2026-01-13 22:53:15,376: t15.2023.12.08 val PER: 0.0506
2026-01-13 22:53:15,376: t15.2023.12.10 val PER: 0.0460
2026-01-13 22:53:15,376: t15.2023.12.17 val PER: 0.1019
2026-01-13 22:53:15,376: t15.2023.12.29 val PER: 0.0913
2026-01-13 22:53:15,377: t15.2024.02.25 val PER: 0.0772
2026-01-13 22:53:15,377: t15.2024.03.08 val PER: 0.1878
2026-01-13 22:53:15,377: t15.2024.03.15 val PER: 0.1582
2026-01-13 22:53:15,377: t15.2024.03.17 val PER: 0.0837
2026-01-13 22:53:15,377: t15.2024.05.10 val PER: 0.1322
2026-01-13 22:53:15,377: t15.2024.06.14 val PER: 0.1451
2026-01-13 22:53:15,377: t15.2024.07.19 val PER: 0.1813
2026-01-13 22:53:15,377: t15.2024.07.21 val PER: 0.0669
2026-01-13 22:53:15,377: t15.2024.07.28 val PER: 0.1110
2026-01-13 22:53:15,377: t15.2025.01.10 val PER: 0.2548
2026-01-13 22:53:15,377: t15.2025.01.12 val PER: 0.1070
2026-01-13 22:53:15,377: t15.2025.03.14 val PER: 0.3047
2026-01-13 22:53:15,377: t15.2025.03.16 val PER: 0.1414
2026-01-13 22:53:15,377: t15.2025.03.30 val PER: 0.2391
2026-01-13 22:53:15,377: t15.2025.04.13 val PER: 0.1912
2026-01-13 22:53:15,519: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_30500
2026-01-13 22:53:24,260: Train batch 30600: loss: 1.83 grad norm: 41.65 time: 0.066
2026-01-13 22:53:42,010: Train batch 30800: loss: 0.85 grad norm: 23.69 time: 0.061
2026-01-13 22:53:59,560: Train batch 31000: loss: 1.42 grad norm: 43.82 time: 0.063
2026-01-13 22:53:59,561: Running test after training batch: 31000
2026-01-13 22:53:59,658: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:54:04,705: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:54:04,838: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cents
2026-01-13 22:54:26,597: Val batch 31000: PER (avg): 0.1121 CTC Loss (avg): 16.0335 WER(5gram): 18.90% (n=256) time: 27.036
2026-01-13 22:54:26,598: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=13
2026-01-13 22:54:26,598: t15.2023.08.13 val PER: 0.0800
2026-01-13 22:54:26,598: t15.2023.08.18 val PER: 0.0763
2026-01-13 22:54:26,598: t15.2023.08.20 val PER: 0.0739
2026-01-13 22:54:26,598: t15.2023.08.25 val PER: 0.0708
2026-01-13 22:54:26,598: t15.2023.08.27 val PER: 0.1479
2026-01-13 22:54:26,598: t15.2023.09.01 val PER: 0.0503
2026-01-13 22:54:26,598: t15.2023.09.03 val PER: 0.1271
2026-01-13 22:54:26,598: t15.2023.09.24 val PER: 0.0850
2026-01-13 22:54:26,598: t15.2023.09.29 val PER: 0.0989
2026-01-13 22:54:26,599: t15.2023.10.01 val PER: 0.1334
2026-01-13 22:54:26,599: t15.2023.10.06 val PER: 0.0635
2026-01-13 22:54:26,599: t15.2023.10.08 val PER: 0.2179
2026-01-13 22:54:26,599: t15.2023.10.13 val PER: 0.1451
2026-01-13 22:54:26,599: t15.2023.10.15 val PER: 0.1121
2026-01-13 22:54:26,599: t15.2023.10.20 val PER: 0.1611
2026-01-13 22:54:26,599: t15.2023.10.22 val PER: 0.0835
2026-01-13 22:54:26,599: t15.2023.11.03 val PER: 0.1384
2026-01-13 22:54:26,599: t15.2023.11.04 val PER: 0.0273
2026-01-13 22:54:26,599: t15.2023.11.17 val PER: 0.0233
2026-01-13 22:54:26,599: t15.2023.11.19 val PER: 0.0240
2026-01-13 22:54:26,599: t15.2023.11.26 val PER: 0.0580
2026-01-13 22:54:26,599: t15.2023.12.03 val PER: 0.0599
2026-01-13 22:54:26,599: t15.2023.12.08 val PER: 0.0539
2026-01-13 22:54:26,599: t15.2023.12.10 val PER: 0.0394
2026-01-13 22:54:26,599: t15.2023.12.17 val PER: 0.0915
2026-01-13 22:54:26,600: t15.2023.12.29 val PER: 0.0858
2026-01-13 22:54:26,600: t15.2024.02.25 val PER: 0.0716
2026-01-13 22:54:26,600: t15.2024.03.08 val PER: 0.1778
2026-01-13 22:54:26,600: t15.2024.03.15 val PER: 0.1632
2026-01-13 22:54:26,600: t15.2024.03.17 val PER: 0.0858
2026-01-13 22:54:26,600: t15.2024.05.10 val PER: 0.1382
2026-01-13 22:54:26,600: t15.2024.06.14 val PER: 0.1262
2026-01-13 22:54:26,600: t15.2024.07.19 val PER: 0.1839
2026-01-13 22:54:26,600: t15.2024.07.21 val PER: 0.0628
2026-01-13 22:54:26,600: t15.2024.07.28 val PER: 0.1118
2026-01-13 22:54:26,600: t15.2025.01.10 val PER: 0.2521
2026-01-13 22:54:26,600: t15.2025.01.12 val PER: 0.1209
2026-01-13 22:54:26,600: t15.2025.03.14 val PER: 0.3077
2026-01-13 22:54:26,600: t15.2025.03.16 val PER: 0.1401
2026-01-13 22:54:26,600: t15.2025.03.30 val PER: 0.2333
2026-01-13 22:54:26,601: t15.2025.04.13 val PER: 0.1854
2026-01-13 22:54:26,747: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_31000
2026-01-13 22:54:44,475: Train batch 31200: loss: 0.71 grad norm: 19.37 time: 0.070
2026-01-13 22:55:02,224: Train batch 31400: loss: 1.88 grad norm: 31.90 time: 0.056
2026-01-13 22:55:11,189: Running test after training batch: 31500
2026-01-13 22:55:11,292: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:55:16,126: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:55:16,182: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cent
2026-01-13 22:55:29,344: Val batch 31500: PER (avg): 0.1145 CTC Loss (avg): 16.8654 WER(5gram): 18.90% (n=256) time: 18.155
2026-01-13 22:55:29,345: WER lens: avg_true_words=5.99 avg_pred_words=6.30 max_pred_words=13
2026-01-13 22:55:29,345: t15.2023.08.13 val PER: 0.0852
2026-01-13 22:55:29,345: t15.2023.08.18 val PER: 0.0805
2026-01-13 22:55:29,345: t15.2023.08.20 val PER: 0.0778
2026-01-13 22:55:29,345: t15.2023.08.25 val PER: 0.0768
2026-01-13 22:55:29,345: t15.2023.08.27 val PER: 0.1527
2026-01-13 22:55:29,345: t15.2023.09.01 val PER: 0.0528
2026-01-13 22:55:29,345: t15.2023.09.03 val PER: 0.1283
2026-01-13 22:55:29,345: t15.2023.09.24 val PER: 0.0959
2026-01-13 22:55:29,346: t15.2023.09.29 val PER: 0.0989
2026-01-13 22:55:29,346: t15.2023.10.01 val PER: 0.1466
2026-01-13 22:55:29,346: t15.2023.10.06 val PER: 0.0624
2026-01-13 22:55:29,346: t15.2023.10.08 val PER: 0.1962
2026-01-13 22:55:29,346: t15.2023.10.13 val PER: 0.1606
2026-01-13 22:55:29,346: t15.2023.10.15 val PER: 0.1134
2026-01-13 22:55:29,346: t15.2023.10.20 val PER: 0.1544
2026-01-13 22:55:29,346: t15.2023.10.22 val PER: 0.0880
2026-01-13 22:55:29,346: t15.2023.11.03 val PER: 0.1493
2026-01-13 22:55:29,346: t15.2023.11.04 val PER: 0.0239
2026-01-13 22:55:29,346: t15.2023.11.17 val PER: 0.0295
2026-01-13 22:55:29,346: t15.2023.11.19 val PER: 0.0240
2026-01-13 22:55:29,346: t15.2023.11.26 val PER: 0.0638
2026-01-13 22:55:29,346: t15.2023.12.03 val PER: 0.0683
2026-01-13 22:55:29,346: t15.2023.12.08 val PER: 0.0499
2026-01-13 22:55:29,347: t15.2023.12.10 val PER: 0.0512
2026-01-13 22:55:29,347: t15.2023.12.17 val PER: 0.0852
2026-01-13 22:55:29,347: t15.2023.12.29 val PER: 0.0824
2026-01-13 22:55:29,347: t15.2024.02.25 val PER: 0.0716
2026-01-13 22:55:29,347: t15.2024.03.08 val PER: 0.1821
2026-01-13 22:55:29,347: t15.2024.03.15 val PER: 0.1595
2026-01-13 22:55:29,347: t15.2024.03.17 val PER: 0.0865
2026-01-13 22:55:29,347: t15.2024.05.10 val PER: 0.1412
2026-01-13 22:55:29,347: t15.2024.06.14 val PER: 0.1498
2026-01-13 22:55:29,347: t15.2024.07.19 val PER: 0.1806
2026-01-13 22:55:29,347: t15.2024.07.21 val PER: 0.0676
2026-01-13 22:55:29,347: t15.2024.07.28 val PER: 0.1029
2026-01-13 22:55:29,347: t15.2025.01.10 val PER: 0.2562
2026-01-13 22:55:29,347: t15.2025.01.12 val PER: 0.1101
2026-01-13 22:55:29,348: t15.2025.03.14 val PER: 0.3018
2026-01-13 22:55:29,348: t15.2025.03.16 val PER: 0.1466
2026-01-13 22:55:29,348: t15.2025.03.30 val PER: 0.2368
2026-01-13 22:55:29,348: t15.2025.04.13 val PER: 0.2026
2026-01-13 22:55:29,504: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_31500
2026-01-13 22:55:38,315: Train batch 31600: loss: 0.79 grad norm: 23.97 time: 0.069
2026-01-13 22:55:55,767: Train batch 31800: loss: 1.64 grad norm: 38.66 time: 0.054
2026-01-13 22:56:13,505: Train batch 32000: loss: 1.52 grad norm: 29.18 time: 0.072
2026-01-13 22:56:13,505: Running test after training batch: 32000
2026-01-13 22:56:13,630: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:56:18,483: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:56:18,535: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cents
2026-01-13 22:56:32,859: Val batch 32000: PER (avg): 0.1120 CTC Loss (avg): 16.3226 WER(5gram): 19.43% (n=256) time: 19.354
2026-01-13 22:56:32,859: WER lens: avg_true_words=5.99 avg_pred_words=6.32 max_pred_words=13
2026-01-13 22:56:32,859: t15.2023.08.13 val PER: 0.0842
2026-01-13 22:56:32,859: t15.2023.08.18 val PER: 0.0729
2026-01-13 22:56:32,860: t15.2023.08.20 val PER: 0.0739
2026-01-13 22:56:32,860: t15.2023.08.25 val PER: 0.0723
2026-01-13 22:56:32,860: t15.2023.08.27 val PER: 0.1463
2026-01-13 22:56:32,860: t15.2023.09.01 val PER: 0.0519
2026-01-13 22:56:32,860: t15.2023.09.03 val PER: 0.1247
2026-01-13 22:56:32,860: t15.2023.09.24 val PER: 0.0934
2026-01-13 22:56:32,860: t15.2023.09.29 val PER: 0.1008
2026-01-13 22:56:32,860: t15.2023.10.01 val PER: 0.1453
2026-01-13 22:56:32,860: t15.2023.10.06 val PER: 0.0603
2026-01-13 22:56:32,860: t15.2023.10.08 val PER: 0.1976
2026-01-13 22:56:32,860: t15.2023.10.13 val PER: 0.1606
2026-01-13 22:56:32,860: t15.2023.10.15 val PER: 0.1048
2026-01-13 22:56:32,861: t15.2023.10.20 val PER: 0.1678
2026-01-13 22:56:32,861: t15.2023.10.22 val PER: 0.0824
2026-01-13 22:56:32,861: t15.2023.11.03 val PER: 0.1357
2026-01-13 22:56:32,861: t15.2023.11.04 val PER: 0.0273
2026-01-13 22:56:32,861: t15.2023.11.17 val PER: 0.0249
2026-01-13 22:56:32,861: t15.2023.11.19 val PER: 0.0259
2026-01-13 22:56:32,861: t15.2023.11.26 val PER: 0.0630
2026-01-13 22:56:32,861: t15.2023.12.03 val PER: 0.0567
2026-01-13 22:56:32,861: t15.2023.12.08 val PER: 0.0439
2026-01-13 22:56:32,862: t15.2023.12.10 val PER: 0.0473
2026-01-13 22:56:32,862: t15.2023.12.17 val PER: 0.0936
2026-01-13 22:56:32,862: t15.2023.12.29 val PER: 0.0858
2026-01-13 22:56:32,862: t15.2024.02.25 val PER: 0.0702
2026-01-13 22:56:32,862: t15.2024.03.08 val PER: 0.1892
2026-01-13 22:56:32,862: t15.2024.03.15 val PER: 0.1582
2026-01-13 22:56:32,862: t15.2024.03.17 val PER: 0.0858
2026-01-13 22:56:32,862: t15.2024.05.10 val PER: 0.1293
2026-01-13 22:56:32,862: t15.2024.06.14 val PER: 0.1278
2026-01-13 22:56:32,862: t15.2024.07.19 val PER: 0.1701
2026-01-13 22:56:32,862: t15.2024.07.21 val PER: 0.0593
2026-01-13 22:56:32,862: t15.2024.07.28 val PER: 0.1140
2026-01-13 22:56:32,862: t15.2025.01.10 val PER: 0.2645
2026-01-13 22:56:32,862: t15.2025.01.12 val PER: 0.1124
2026-01-13 22:56:32,862: t15.2025.03.14 val PER: 0.3047
2026-01-13 22:56:32,862: t15.2025.03.16 val PER: 0.1492
2026-01-13 22:56:32,863: t15.2025.03.30 val PER: 0.2299
2026-01-13 22:56:32,863: t15.2025.04.13 val PER: 0.2026
2026-01-13 22:56:33,008: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_32000
2026-01-13 22:56:50,789: Train batch 32200: loss: 0.77 grad norm: 21.97 time: 0.084
2026-01-13 22:57:08,637: Train batch 32400: loss: 5.78 grad norm: 31.19 time: 0.063
2026-01-13 22:57:17,540: Running test after training batch: 32500
2026-01-13 22:57:17,673: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:57:22,644: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:57:22,701: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cents
2026-01-13 22:57:35,885: Val batch 32500: PER (avg): 0.1149 CTC Loss (avg): 16.3876 WER(5gram): 18.71% (n=256) time: 18.344
2026-01-13 22:57:35,885: WER lens: avg_true_words=5.99 avg_pred_words=6.30 max_pred_words=13
2026-01-13 22:57:35,885: t15.2023.08.13 val PER: 0.0863
2026-01-13 22:57:35,886: t15.2023.08.18 val PER: 0.0813
2026-01-13 22:57:35,886: t15.2023.08.20 val PER: 0.0715
2026-01-13 22:57:35,886: t15.2023.08.25 val PER: 0.0768
2026-01-13 22:57:35,886: t15.2023.08.27 val PER: 0.1592
2026-01-13 22:57:35,886: t15.2023.09.01 val PER: 0.0560
2026-01-13 22:57:35,886: t15.2023.09.03 val PER: 0.1366
2026-01-13 22:57:35,886: t15.2023.09.24 val PER: 0.0959
2026-01-13 22:57:35,887: t15.2023.09.29 val PER: 0.1002
2026-01-13 22:57:35,887: t15.2023.10.01 val PER: 0.1453
2026-01-13 22:57:35,887: t15.2023.10.06 val PER: 0.0646
2026-01-13 22:57:35,887: t15.2023.10.08 val PER: 0.2057
2026-01-13 22:57:35,887: t15.2023.10.13 val PER: 0.1544
2026-01-13 22:57:35,887: t15.2023.10.15 val PER: 0.1140
2026-01-13 22:57:35,887: t15.2023.10.20 val PER: 0.1577
2026-01-13 22:57:35,887: t15.2023.10.22 val PER: 0.0935
2026-01-13 22:57:35,887: t15.2023.11.03 val PER: 0.1547
2026-01-13 22:57:35,887: t15.2023.11.04 val PER: 0.0273
2026-01-13 22:57:35,887: t15.2023.11.17 val PER: 0.0264
2026-01-13 22:57:35,887: t15.2023.11.19 val PER: 0.0180
2026-01-13 22:57:35,888: t15.2023.11.26 val PER: 0.0572
2026-01-13 22:57:35,888: t15.2023.12.03 val PER: 0.0630
2026-01-13 22:57:35,888: t15.2023.12.08 val PER: 0.0493
2026-01-13 22:57:35,888: t15.2023.12.10 val PER: 0.0539
2026-01-13 22:57:35,888: t15.2023.12.17 val PER: 0.1040
2026-01-13 22:57:35,888: t15.2023.12.29 val PER: 0.0913
2026-01-13 22:57:35,888: t15.2024.02.25 val PER: 0.0688
2026-01-13 22:57:35,888: t15.2024.03.08 val PER: 0.1721
2026-01-13 22:57:35,888: t15.2024.03.15 val PER: 0.1601
2026-01-13 22:57:35,888: t15.2024.03.17 val PER: 0.0907
2026-01-13 22:57:35,888: t15.2024.05.10 val PER: 0.1322
2026-01-13 22:57:35,888: t15.2024.06.14 val PER: 0.1293
2026-01-13 22:57:35,888: t15.2024.07.19 val PER: 0.1740
2026-01-13 22:57:35,888: t15.2024.07.21 val PER: 0.0593
2026-01-13 22:57:35,888: t15.2024.07.28 val PER: 0.1118
2026-01-13 22:57:35,888: t15.2025.01.10 val PER: 0.2645
2026-01-13 22:57:35,889: t15.2025.01.12 val PER: 0.1178
2026-01-13 22:57:35,889: t15.2025.03.14 val PER: 0.3018
2026-01-13 22:57:35,889: t15.2025.03.16 val PER: 0.1335
2026-01-13 22:57:35,889: t15.2025.03.30 val PER: 0.2471
2026-01-13 22:57:35,889: t15.2025.04.13 val PER: 0.1954
2026-01-13 22:57:36,031: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_32500
2026-01-13 22:57:44,741: Train batch 32600: loss: 1.11 grad norm: 26.55 time: 0.088
2026-01-13 22:58:02,514: Train batch 32800: loss: 1.33 grad norm: 24.99 time: 0.059
2026-01-13 22:58:20,092: Train batch 33000: loss: 0.84 grad norm: 26.84 time: 0.065
2026-01-13 22:58:20,092: Running test after training batch: 33000
2026-01-13 22:58:20,342: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:58:25,293: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:58:25,343: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 22:58:37,581: Val batch 33000: PER (avg): 0.1113 CTC Loss (avg): 16.3339 WER(5gram): 17.54% (n=256) time: 17.489
2026-01-13 22:58:37,582: WER lens: avg_true_words=5.99 avg_pred_words=6.26 max_pred_words=13
2026-01-13 22:58:37,582: t15.2023.08.13 val PER: 0.0821
2026-01-13 22:58:37,582: t15.2023.08.18 val PER: 0.0754
2026-01-13 22:58:37,582: t15.2023.08.20 val PER: 0.0707
2026-01-13 22:58:37,583: t15.2023.08.25 val PER: 0.0678
2026-01-13 22:58:37,583: t15.2023.08.27 val PER: 0.1431
2026-01-13 22:58:37,583: t15.2023.09.01 val PER: 0.0479
2026-01-13 22:58:37,583: t15.2023.09.03 val PER: 0.1211
2026-01-13 22:58:37,583: t15.2023.09.24 val PER: 0.0934
2026-01-13 22:58:37,583: t15.2023.09.29 val PER: 0.0976
2026-01-13 22:58:37,583: t15.2023.10.01 val PER: 0.1354
2026-01-13 22:58:37,583: t15.2023.10.06 val PER: 0.0689
2026-01-13 22:58:37,583: t15.2023.10.08 val PER: 0.2003
2026-01-13 22:58:37,584: t15.2023.10.13 val PER: 0.1458
2026-01-13 22:58:37,584: t15.2023.10.15 val PER: 0.1107
2026-01-13 22:58:37,584: t15.2023.10.20 val PER: 0.1544
2026-01-13 22:58:37,584: t15.2023.10.22 val PER: 0.0768
2026-01-13 22:58:37,584: t15.2023.11.03 val PER: 0.1465
2026-01-13 22:58:37,584: t15.2023.11.04 val PER: 0.0273
2026-01-13 22:58:37,584: t15.2023.11.17 val PER: 0.0187
2026-01-13 22:58:37,584: t15.2023.11.19 val PER: 0.0200
2026-01-13 22:58:37,584: t15.2023.11.26 val PER: 0.0529
2026-01-13 22:58:37,584: t15.2023.12.03 val PER: 0.0546
2026-01-13 22:58:37,584: t15.2023.12.08 val PER: 0.0446
2026-01-13 22:58:37,585: t15.2023.12.10 val PER: 0.0473
2026-01-13 22:58:37,585: t15.2023.12.17 val PER: 0.0936
2026-01-13 22:58:37,585: t15.2023.12.29 val PER: 0.0885
2026-01-13 22:58:37,585: t15.2024.02.25 val PER: 0.0772
2026-01-13 22:58:37,585: t15.2024.03.08 val PER: 0.1721
2026-01-13 22:58:37,585: t15.2024.03.15 val PER: 0.1588
2026-01-13 22:58:37,585: t15.2024.03.17 val PER: 0.0858
2026-01-13 22:58:37,585: t15.2024.05.10 val PER: 0.1367
2026-01-13 22:58:37,586: t15.2024.06.14 val PER: 0.1341
2026-01-13 22:58:37,586: t15.2024.07.19 val PER: 0.1806
2026-01-13 22:58:37,586: t15.2024.07.21 val PER: 0.0669
2026-01-13 22:58:37,586: t15.2024.07.28 val PER: 0.1029
2026-01-13 22:58:37,586: t15.2025.01.10 val PER: 0.2631
2026-01-13 22:58:37,586: t15.2025.01.12 val PER: 0.1193
2026-01-13 22:58:37,586: t15.2025.03.14 val PER: 0.3092
2026-01-13 22:58:37,586: t15.2025.03.16 val PER: 0.1518
2026-01-13 22:58:37,586: t15.2025.03.30 val PER: 0.2310
2026-01-13 22:58:37,586: t15.2025.04.13 val PER: 0.1840
2026-01-13 22:58:37,734: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_33000
2026-01-13 22:58:54,992: Train batch 33200: loss: 1.30 grad norm: 32.02 time: 0.064
2026-01-13 22:59:12,431: Train batch 33400: loss: 0.24 grad norm: 10.95 time: 0.054
2026-01-13 22:59:20,982: Running test after training batch: 33500
2026-01-13 22:59:21,078: WER debug GT example: You can see the code at this point as well.
2026-01-13 22:59:25,941: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 22:59:25,997: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 22:59:39,604: Val batch 33500: PER (avg): 0.1137 CTC Loss (avg): 16.7996 WER(5gram): 15.84% (n=256) time: 18.622
2026-01-13 22:59:39,605: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=13
2026-01-13 22:59:39,605: t15.2023.08.13 val PER: 0.0842
2026-01-13 22:59:39,605: t15.2023.08.18 val PER: 0.0788
2026-01-13 22:59:39,605: t15.2023.08.20 val PER: 0.0747
2026-01-13 22:59:39,605: t15.2023.08.25 val PER: 0.0813
2026-01-13 22:59:39,605: t15.2023.08.27 val PER: 0.1672
2026-01-13 22:59:39,605: t15.2023.09.01 val PER: 0.0536
2026-01-13 22:59:39,605: t15.2023.09.03 val PER: 0.1176
2026-01-13 22:59:39,605: t15.2023.09.24 val PER: 0.0983
2026-01-13 22:59:39,605: t15.2023.09.29 val PER: 0.0970
2026-01-13 22:59:39,606: t15.2023.10.01 val PER: 0.1486
2026-01-13 22:59:39,606: t15.2023.10.06 val PER: 0.0667
2026-01-13 22:59:39,606: t15.2023.10.08 val PER: 0.2030
2026-01-13 22:59:39,606: t15.2023.10.13 val PER: 0.1497
2026-01-13 22:59:39,606: t15.2023.10.15 val PER: 0.1154
2026-01-13 22:59:39,606: t15.2023.10.20 val PER: 0.1611
2026-01-13 22:59:39,606: t15.2023.10.22 val PER: 0.0869
2026-01-13 22:59:39,606: t15.2023.11.03 val PER: 0.1506
2026-01-13 22:59:39,606: t15.2023.11.04 val PER: 0.0239
2026-01-13 22:59:39,606: t15.2023.11.17 val PER: 0.0264
2026-01-13 22:59:39,606: t15.2023.11.19 val PER: 0.0160
2026-01-13 22:59:39,606: t15.2023.11.26 val PER: 0.0572
2026-01-13 22:59:39,606: t15.2023.12.03 val PER: 0.0704
2026-01-13 22:59:39,606: t15.2023.12.08 val PER: 0.0519
2026-01-13 22:59:39,606: t15.2023.12.10 val PER: 0.0394
2026-01-13 22:59:39,606: t15.2023.12.17 val PER: 0.0873
2026-01-13 22:59:39,607: t15.2023.12.29 val PER: 0.0899
2026-01-13 22:59:39,607: t15.2024.02.25 val PER: 0.0758
2026-01-13 22:59:39,607: t15.2024.03.08 val PER: 0.1650
2026-01-13 22:59:39,607: t15.2024.03.15 val PER: 0.1639
2026-01-13 22:59:39,607: t15.2024.03.17 val PER: 0.0914
2026-01-13 22:59:39,607: t15.2024.05.10 val PER: 0.1322
2026-01-13 22:59:39,607: t15.2024.06.14 val PER: 0.1372
2026-01-13 22:59:39,607: t15.2024.07.19 val PER: 0.1714
2026-01-13 22:59:39,608: t15.2024.07.21 val PER: 0.0634
2026-01-13 22:59:39,608: t15.2024.07.28 val PER: 0.1059
2026-01-13 22:59:39,608: t15.2025.01.10 val PER: 0.2438
2026-01-13 22:59:39,608: t15.2025.01.12 val PER: 0.1170
2026-01-13 22:59:39,608: t15.2025.03.14 val PER: 0.3018
2026-01-13 22:59:39,608: t15.2025.03.16 val PER: 0.1387
2026-01-13 22:59:39,608: t15.2025.03.30 val PER: 0.2391
2026-01-13 22:59:39,608: t15.2025.04.13 val PER: 0.2097
2026-01-13 22:59:39,754: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_33500
2026-01-13 22:59:48,755: Train batch 33600: loss: 1.19 grad norm: 38.72 time: 0.054
2026-01-13 23:00:06,723: Train batch 33800: loss: 0.89 grad norm: 27.90 time: 0.068
2026-01-13 23:00:24,797: Train batch 34000: loss: 2.32 grad norm: 37.49 time: 0.070
2026-01-13 23:00:24,798: Running test after training batch: 34000
2026-01-13 23:00:24,967: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:00:29,807: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:00:29,864: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cents
2026-01-13 23:00:43,236: Val batch 34000: PER (avg): 0.1125 CTC Loss (avg): 16.4718 WER(5gram): 17.93% (n=256) time: 18.438
2026-01-13 23:00:43,236: WER lens: avg_true_words=5.99 avg_pred_words=6.29 max_pred_words=13
2026-01-13 23:00:43,237: t15.2023.08.13 val PER: 0.0811
2026-01-13 23:00:43,237: t15.2023.08.18 val PER: 0.0763
2026-01-13 23:00:43,237: t15.2023.08.20 val PER: 0.0667
2026-01-13 23:00:43,237: t15.2023.08.25 val PER: 0.0708
2026-01-13 23:00:43,237: t15.2023.08.27 val PER: 0.1527
2026-01-13 23:00:43,237: t15.2023.09.01 val PER: 0.0479
2026-01-13 23:00:43,237: t15.2023.09.03 val PER: 0.1235
2026-01-13 23:00:43,237: t15.2023.09.24 val PER: 0.0934
2026-01-13 23:00:43,237: t15.2023.09.29 val PER: 0.1015
2026-01-13 23:00:43,237: t15.2023.10.01 val PER: 0.1367
2026-01-13 23:00:43,238: t15.2023.10.06 val PER: 0.0614
2026-01-13 23:00:43,238: t15.2023.10.08 val PER: 0.2152
2026-01-13 23:00:43,238: t15.2023.10.13 val PER: 0.1505
2026-01-13 23:00:43,238: t15.2023.10.15 val PER: 0.1042
2026-01-13 23:00:43,238: t15.2023.10.20 val PER: 0.1611
2026-01-13 23:00:43,238: t15.2023.10.22 val PER: 0.0880
2026-01-13 23:00:43,238: t15.2023.11.03 val PER: 0.1486
2026-01-13 23:00:43,238: t15.2023.11.04 val PER: 0.0341
2026-01-13 23:00:43,238: t15.2023.11.17 val PER: 0.0233
2026-01-13 23:00:43,238: t15.2023.11.19 val PER: 0.0279
2026-01-13 23:00:43,238: t15.2023.11.26 val PER: 0.0529
2026-01-13 23:00:43,238: t15.2023.12.03 val PER: 0.0641
2026-01-13 23:00:43,239: t15.2023.12.08 val PER: 0.0553
2026-01-13 23:00:43,239: t15.2023.12.10 val PER: 0.0473
2026-01-13 23:00:43,239: t15.2023.12.17 val PER: 0.1040
2026-01-13 23:00:43,239: t15.2023.12.29 val PER: 0.0872
2026-01-13 23:00:43,239: t15.2024.02.25 val PER: 0.0758
2026-01-13 23:00:43,239: t15.2024.03.08 val PER: 0.1807
2026-01-13 23:00:43,239: t15.2024.03.15 val PER: 0.1576
2026-01-13 23:00:43,239: t15.2024.03.17 val PER: 0.0934
2026-01-13 23:00:43,239: t15.2024.05.10 val PER: 0.1441
2026-01-13 23:00:43,239: t15.2024.06.14 val PER: 0.1309
2026-01-13 23:00:43,239: t15.2024.07.19 val PER: 0.1806
2026-01-13 23:00:43,239: t15.2024.07.21 val PER: 0.0662
2026-01-13 23:00:43,239: t15.2024.07.28 val PER: 0.1059
2026-01-13 23:00:43,239: t15.2025.01.10 val PER: 0.2493
2026-01-13 23:00:43,239: t15.2025.01.12 val PER: 0.1078
2026-01-13 23:00:43,239: t15.2025.03.14 val PER: 0.3018
2026-01-13 23:00:43,240: t15.2025.03.16 val PER: 0.1401
2026-01-13 23:00:43,240: t15.2025.03.30 val PER: 0.2253
2026-01-13 23:00:43,240: t15.2025.04.13 val PER: 0.1883
2026-01-13 23:00:43,385: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_34000
2026-01-13 23:01:01,046: Train batch 34200: loss: 1.36 grad norm: 29.98 time: 0.045
2026-01-13 23:01:18,608: Train batch 34400: loss: 0.76 grad norm: 18.72 time: 0.058
2026-01-13 23:01:27,419: Running test after training batch: 34500
2026-01-13 23:01:27,706: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:01:32,490: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:01:32,542: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:01:44,927: Val batch 34500: PER (avg): 0.1119 CTC Loss (avg): 16.5424 WER(5gram): 21.32% (n=256) time: 17.508
2026-01-13 23:01:44,928: WER lens: avg_true_words=5.99 avg_pred_words=6.39 max_pred_words=13
2026-01-13 23:01:44,928: t15.2023.08.13 val PER: 0.0863
2026-01-13 23:01:44,928: t15.2023.08.18 val PER: 0.0754
2026-01-13 23:01:44,928: t15.2023.08.20 val PER: 0.0675
2026-01-13 23:01:44,928: t15.2023.08.25 val PER: 0.0753
2026-01-13 23:01:44,928: t15.2023.08.27 val PER: 0.1592
2026-01-13 23:01:44,928: t15.2023.09.01 val PER: 0.0544
2026-01-13 23:01:44,928: t15.2023.09.03 val PER: 0.1354
2026-01-13 23:01:44,928: t15.2023.09.24 val PER: 0.0934
2026-01-13 23:01:44,928: t15.2023.09.29 val PER: 0.1008
2026-01-13 23:01:44,928: t15.2023.10.01 val PER: 0.1380
2026-01-13 23:01:44,928: t15.2023.10.06 val PER: 0.0657
2026-01-13 23:01:44,928: t15.2023.10.08 val PER: 0.2070
2026-01-13 23:01:44,929: t15.2023.10.13 val PER: 0.1521
2026-01-13 23:01:44,929: t15.2023.10.15 val PER: 0.1035
2026-01-13 23:01:44,929: t15.2023.10.20 val PER: 0.1477
2026-01-13 23:01:44,929: t15.2023.10.22 val PER: 0.0746
2026-01-13 23:01:44,929: t15.2023.11.03 val PER: 0.1472
2026-01-13 23:01:44,929: t15.2023.11.04 val PER: 0.0205
2026-01-13 23:01:44,929: t15.2023.11.17 val PER: 0.0171
2026-01-13 23:01:44,929: t15.2023.11.19 val PER: 0.0240
2026-01-13 23:01:44,929: t15.2023.11.26 val PER: 0.0572
2026-01-13 23:01:44,929: t15.2023.12.03 val PER: 0.0557
2026-01-13 23:01:44,930: t15.2023.12.08 val PER: 0.0499
2026-01-13 23:01:44,930: t15.2023.12.10 val PER: 0.0447
2026-01-13 23:01:44,930: t15.2023.12.17 val PER: 0.0998
2026-01-13 23:01:44,930: t15.2023.12.29 val PER: 0.0830
2026-01-13 23:01:44,930: t15.2024.02.25 val PER: 0.0730
2026-01-13 23:01:44,930: t15.2024.03.08 val PER: 0.1764
2026-01-13 23:01:44,930: t15.2024.03.15 val PER: 0.1607
2026-01-13 23:01:44,930: t15.2024.03.17 val PER: 0.0927
2026-01-13 23:01:44,930: t15.2024.05.10 val PER: 0.1426
2026-01-13 23:01:44,930: t15.2024.06.14 val PER: 0.1325
2026-01-13 23:01:44,930: t15.2024.07.19 val PER: 0.1819
2026-01-13 23:01:44,930: t15.2024.07.21 val PER: 0.0634
2026-01-13 23:01:44,930: t15.2024.07.28 val PER: 0.1022
2026-01-13 23:01:44,930: t15.2025.01.10 val PER: 0.2397
2026-01-13 23:01:44,930: t15.2025.01.12 val PER: 0.1147
2026-01-13 23:01:44,930: t15.2025.03.14 val PER: 0.2944
2026-01-13 23:01:44,931: t15.2025.03.16 val PER: 0.1492
2026-01-13 23:01:44,931: t15.2025.03.30 val PER: 0.2299
2026-01-13 23:01:44,931: t15.2025.04.13 val PER: 0.1840
2026-01-13 23:01:45,081: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_34500
2026-01-13 23:01:53,796: Train batch 34600: loss: 0.96 grad norm: 25.22 time: 0.060
2026-01-13 23:02:11,296: Train batch 34800: loss: 0.91 grad norm: 24.55 time: 0.054
2026-01-13 23:02:28,804: Train batch 35000: loss: 1.69 grad norm: 34.66 time: 0.056
2026-01-13 23:02:28,804: Running test after training batch: 35000
2026-01-13 23:02:28,949: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:02:33,786: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:02:33,843: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:02:47,569: Val batch 35000: PER (avg): 0.1114 CTC Loss (avg): 16.4410 WER(5gram): 19.62% (n=256) time: 18.765
2026-01-13 23:02:47,569: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-13 23:02:47,570: t15.2023.08.13 val PER: 0.0894
2026-01-13 23:02:47,570: t15.2023.08.18 val PER: 0.0754
2026-01-13 23:02:47,570: t15.2023.08.20 val PER: 0.0707
2026-01-13 23:02:47,570: t15.2023.08.25 val PER: 0.0753
2026-01-13 23:02:47,570: t15.2023.08.27 val PER: 0.1495
2026-01-13 23:02:47,570: t15.2023.09.01 val PER: 0.0495
2026-01-13 23:02:47,570: t15.2023.09.03 val PER: 0.1306
2026-01-13 23:02:47,570: t15.2023.09.24 val PER: 0.0947
2026-01-13 23:02:47,570: t15.2023.09.29 val PER: 0.0996
2026-01-13 23:02:47,570: t15.2023.10.01 val PER: 0.1387
2026-01-13 23:02:47,570: t15.2023.10.06 val PER: 0.0667
2026-01-13 23:02:47,571: t15.2023.10.08 val PER: 0.1935
2026-01-13 23:02:47,571: t15.2023.10.13 val PER: 0.1528
2026-01-13 23:02:47,571: t15.2023.10.15 val PER: 0.1101
2026-01-13 23:02:47,571: t15.2023.10.20 val PER: 0.1510
2026-01-13 23:02:47,571: t15.2023.10.22 val PER: 0.0791
2026-01-13 23:02:47,571: t15.2023.11.03 val PER: 0.1425
2026-01-13 23:02:47,571: t15.2023.11.04 val PER: 0.0273
2026-01-13 23:02:47,571: t15.2023.11.17 val PER: 0.0264
2026-01-13 23:02:47,571: t15.2023.11.19 val PER: 0.0200
2026-01-13 23:02:47,571: t15.2023.11.26 val PER: 0.0543
2026-01-13 23:02:47,571: t15.2023.12.03 val PER: 0.0525
2026-01-13 23:02:47,571: t15.2023.12.08 val PER: 0.0433
2026-01-13 23:02:47,571: t15.2023.12.10 val PER: 0.0473
2026-01-13 23:02:47,571: t15.2023.12.17 val PER: 0.0915
2026-01-13 23:02:47,571: t15.2023.12.29 val PER: 0.0837
2026-01-13 23:02:47,571: t15.2024.02.25 val PER: 0.0730
2026-01-13 23:02:47,572: t15.2024.03.08 val PER: 0.1920
2026-01-13 23:02:47,572: t15.2024.03.15 val PER: 0.1732
2026-01-13 23:02:47,572: t15.2024.03.17 val PER: 0.0934
2026-01-13 23:02:47,572: t15.2024.05.10 val PER: 0.1397
2026-01-13 23:02:47,572: t15.2024.06.14 val PER: 0.1262
2026-01-13 23:02:47,572: t15.2024.07.19 val PER: 0.1747
2026-01-13 23:02:47,572: t15.2024.07.21 val PER: 0.0621
2026-01-13 23:02:47,572: t15.2024.07.28 val PER: 0.1022
2026-01-13 23:02:47,573: t15.2025.01.10 val PER: 0.2562
2026-01-13 23:02:47,573: t15.2025.01.12 val PER: 0.1016
2026-01-13 23:02:47,573: t15.2025.03.14 val PER: 0.2899
2026-01-13 23:02:47,573: t15.2025.03.16 val PER: 0.1374
2026-01-13 23:02:47,573: t15.2025.03.30 val PER: 0.2310
2026-01-13 23:02:47,573: t15.2025.04.13 val PER: 0.1969
2026-01-13 23:02:47,718: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_35000
2026-01-13 23:03:05,707: Train batch 35200: loss: 0.77 grad norm: 20.41 time: 0.064
2026-01-13 23:03:23,140: Train batch 35400: loss: 1.23 grad norm: 27.11 time: 0.082
2026-01-13 23:03:31,881: Running test after training batch: 35500
2026-01-13 23:03:32,047: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:03:37,006: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:03:37,059: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the costs set
2026-01-13 23:03:50,684: Val batch 35500: PER (avg): 0.1131 CTC Loss (avg): 16.6294 WER(5gram): 18.97% (n=256) time: 18.803
2026-01-13 23:03:50,685: WER lens: avg_true_words=5.99 avg_pred_words=6.32 max_pred_words=13
2026-01-13 23:03:50,685: t15.2023.08.13 val PER: 0.0894
2026-01-13 23:03:50,685: t15.2023.08.18 val PER: 0.0813
2026-01-13 23:03:50,685: t15.2023.08.20 val PER: 0.0707
2026-01-13 23:03:50,685: t15.2023.08.25 val PER: 0.0753
2026-01-13 23:03:50,685: t15.2023.08.27 val PER: 0.1511
2026-01-13 23:03:50,685: t15.2023.09.01 val PER: 0.0511
2026-01-13 23:03:50,685: t15.2023.09.03 val PER: 0.1223
2026-01-13 23:03:50,686: t15.2023.09.24 val PER: 0.0971
2026-01-13 23:03:50,686: t15.2023.09.29 val PER: 0.1072
2026-01-13 23:03:50,686: t15.2023.10.01 val PER: 0.1486
2026-01-13 23:03:50,686: t15.2023.10.06 val PER: 0.0624
2026-01-13 23:03:50,686: t15.2023.10.08 val PER: 0.2043
2026-01-13 23:03:50,686: t15.2023.10.13 val PER: 0.1466
2026-01-13 23:03:50,686: t15.2023.10.15 val PER: 0.1107
2026-01-13 23:03:50,686: t15.2023.10.20 val PER: 0.1678
2026-01-13 23:03:50,686: t15.2023.10.22 val PER: 0.0824
2026-01-13 23:03:50,686: t15.2023.11.03 val PER: 0.1533
2026-01-13 23:03:50,686: t15.2023.11.04 val PER: 0.0171
2026-01-13 23:03:50,686: t15.2023.11.17 val PER: 0.0202
2026-01-13 23:03:50,686: t15.2023.11.19 val PER: 0.0240
2026-01-13 23:03:50,687: t15.2023.11.26 val PER: 0.0630
2026-01-13 23:03:50,687: t15.2023.12.03 val PER: 0.0557
2026-01-13 23:03:50,687: t15.2023.12.08 val PER: 0.0479
2026-01-13 23:03:50,687: t15.2023.12.10 val PER: 0.0473
2026-01-13 23:03:50,687: t15.2023.12.17 val PER: 0.0915
2026-01-13 23:03:50,687: t15.2023.12.29 val PER: 0.0782
2026-01-13 23:03:50,687: t15.2024.02.25 val PER: 0.0829
2026-01-13 23:03:50,687: t15.2024.03.08 val PER: 0.1707
2026-01-13 23:03:50,687: t15.2024.03.15 val PER: 0.1620
2026-01-13 23:03:50,687: t15.2024.03.17 val PER: 0.0851
2026-01-13 23:03:50,687: t15.2024.05.10 val PER: 0.1382
2026-01-13 23:03:50,687: t15.2024.06.14 val PER: 0.1372
2026-01-13 23:03:50,688: t15.2024.07.19 val PER: 0.1839
2026-01-13 23:03:50,688: t15.2024.07.21 val PER: 0.0586
2026-01-13 23:03:50,688: t15.2024.07.28 val PER: 0.1088
2026-01-13 23:03:50,688: t15.2025.01.10 val PER: 0.2548
2026-01-13 23:03:50,688: t15.2025.01.12 val PER: 0.1170
2026-01-13 23:03:50,688: t15.2025.03.14 val PER: 0.2855
2026-01-13 23:03:50,688: t15.2025.03.16 val PER: 0.1453
2026-01-13 23:03:50,688: t15.2025.03.30 val PER: 0.2310
2026-01-13 23:03:50,688: t15.2025.04.13 val PER: 0.1926
2026-01-13 23:03:50,830: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_35500
2026-01-13 23:03:59,562: Train batch 35600: loss: 0.67 grad norm: 34.00 time: 0.082
2026-01-13 23:04:17,415: Train batch 35800: loss: 0.66 grad norm: 26.13 time: 0.067
2026-01-13 23:04:34,953: Train batch 36000: loss: 0.84 grad norm: 34.47 time: 0.056
2026-01-13 23:04:34,953: Running test after training batch: 36000
2026-01-13 23:04:35,058: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:04:40,041: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:04:40,098: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:04:52,664: Val batch 36000: PER (avg): 0.1111 CTC Loss (avg): 16.7032 WER(5gram): 18.25% (n=256) time: 17.710
2026-01-13 23:04:52,664: WER lens: avg_true_words=5.99 avg_pred_words=6.31 max_pred_words=13
2026-01-13 23:04:52,664: t15.2023.08.13 val PER: 0.0811
2026-01-13 23:04:52,665: t15.2023.08.18 val PER: 0.0763
2026-01-13 23:04:52,665: t15.2023.08.20 val PER: 0.0699
2026-01-13 23:04:52,665: t15.2023.08.25 val PER: 0.0783
2026-01-13 23:04:52,665: t15.2023.08.27 val PER: 0.1511
2026-01-13 23:04:52,665: t15.2023.09.01 val PER: 0.0495
2026-01-13 23:04:52,665: t15.2023.09.03 val PER: 0.1295
2026-01-13 23:04:52,665: t15.2023.09.24 val PER: 0.0862
2026-01-13 23:04:52,665: t15.2023.09.29 val PER: 0.0996
2026-01-13 23:04:52,665: t15.2023.10.01 val PER: 0.1374
2026-01-13 23:04:52,665: t15.2023.10.06 val PER: 0.0646
2026-01-13 23:04:52,666: t15.2023.10.08 val PER: 0.2124
2026-01-13 23:04:52,666: t15.2023.10.13 val PER: 0.1559
2026-01-13 23:04:52,666: t15.2023.10.15 val PER: 0.1259
2026-01-13 23:04:52,666: t15.2023.10.20 val PER: 0.1544
2026-01-13 23:04:52,666: t15.2023.10.22 val PER: 0.0857
2026-01-13 23:04:52,666: t15.2023.11.03 val PER: 0.1520
2026-01-13 23:04:52,666: t15.2023.11.04 val PER: 0.0273
2026-01-13 23:04:52,666: t15.2023.11.17 val PER: 0.0358
2026-01-13 23:04:52,667: t15.2023.11.19 val PER: 0.0120
2026-01-13 23:04:52,667: t15.2023.11.26 val PER: 0.0558
2026-01-13 23:04:52,667: t15.2023.12.03 val PER: 0.0567
2026-01-13 23:04:52,667: t15.2023.12.08 val PER: 0.0419
2026-01-13 23:04:52,667: t15.2023.12.10 val PER: 0.0486
2026-01-13 23:04:52,667: t15.2023.12.17 val PER: 0.0842
2026-01-13 23:04:52,667: t15.2023.12.29 val PER: 0.0769
2026-01-13 23:04:52,667: t15.2024.02.25 val PER: 0.0787
2026-01-13 23:04:52,667: t15.2024.03.08 val PER: 0.1565
2026-01-13 23:04:52,667: t15.2024.03.15 val PER: 0.1576
2026-01-13 23:04:52,668: t15.2024.03.17 val PER: 0.0962
2026-01-13 23:04:52,668: t15.2024.05.10 val PER: 0.1382
2026-01-13 23:04:52,668: t15.2024.06.14 val PER: 0.1388
2026-01-13 23:04:52,668: t15.2024.07.19 val PER: 0.1734
2026-01-13 23:04:52,668: t15.2024.07.21 val PER: 0.0621
2026-01-13 23:04:52,668: t15.2024.07.28 val PER: 0.1015
2026-01-13 23:04:52,668: t15.2025.01.10 val PER: 0.2452
2026-01-13 23:04:52,668: t15.2025.01.12 val PER: 0.1085
2026-01-13 23:04:52,668: t15.2025.03.14 val PER: 0.2870
2026-01-13 23:04:52,668: t15.2025.03.16 val PER: 0.1401
2026-01-13 23:04:52,668: t15.2025.03.30 val PER: 0.2276
2026-01-13 23:04:52,668: t15.2025.04.13 val PER: 0.1840
2026-01-13 23:04:52,826: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_36000
2026-01-13 23:05:10,153: Train batch 36200: loss: 0.77 grad norm: 26.08 time: 0.068
2026-01-13 23:05:27,596: Train batch 36400: loss: 0.31 grad norm: 13.92 time: 0.086
2026-01-13 23:05:36,215: Running test after training batch: 36500
2026-01-13 23:05:36,406: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:05:41,334: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:05:41,400: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:05:54,690: Val batch 36500: PER (avg): 0.1092 CTC Loss (avg): 16.5183 WER(5gram): 17.54% (n=256) time: 18.474
2026-01-13 23:05:54,690: WER lens: avg_true_words=5.99 avg_pred_words=6.31 max_pred_words=12
2026-01-13 23:05:54,690: t15.2023.08.13 val PER: 0.0800
2026-01-13 23:05:54,690: t15.2023.08.18 val PER: 0.0687
2026-01-13 23:05:54,690: t15.2023.08.20 val PER: 0.0675
2026-01-13 23:05:54,690: t15.2023.08.25 val PER: 0.0753
2026-01-13 23:05:54,691: t15.2023.08.27 val PER: 0.1367
2026-01-13 23:05:54,691: t15.2023.09.01 val PER: 0.0487
2026-01-13 23:05:54,691: t15.2023.09.03 val PER: 0.1223
2026-01-13 23:05:54,691: t15.2023.09.24 val PER: 0.0922
2026-01-13 23:05:54,691: t15.2023.09.29 val PER: 0.1021
2026-01-13 23:05:54,691: t15.2023.10.01 val PER: 0.1420
2026-01-13 23:05:54,691: t15.2023.10.06 val PER: 0.0592
2026-01-13 23:05:54,692: t15.2023.10.08 val PER: 0.2084
2026-01-13 23:05:54,692: t15.2023.10.13 val PER: 0.1466
2026-01-13 23:05:54,693: t15.2023.10.15 val PER: 0.1074
2026-01-13 23:05:54,693: t15.2023.10.20 val PER: 0.1611
2026-01-13 23:05:54,693: t15.2023.10.22 val PER: 0.0913
2026-01-13 23:05:54,693: t15.2023.11.03 val PER: 0.1499
2026-01-13 23:05:54,693: t15.2023.11.04 val PER: 0.0273
2026-01-13 23:05:54,693: t15.2023.11.17 val PER: 0.0218
2026-01-13 23:05:54,693: t15.2023.11.19 val PER: 0.0160
2026-01-13 23:05:54,693: t15.2023.11.26 val PER: 0.0464
2026-01-13 23:05:54,694: t15.2023.12.03 val PER: 0.0515
2026-01-13 23:05:54,694: t15.2023.12.08 val PER: 0.0473
2026-01-13 23:05:54,694: t15.2023.12.10 val PER: 0.0447
2026-01-13 23:05:54,694: t15.2023.12.17 val PER: 0.0936
2026-01-13 23:05:54,694: t15.2023.12.29 val PER: 0.0899
2026-01-13 23:05:54,694: t15.2024.02.25 val PER: 0.0660
2026-01-13 23:05:54,694: t15.2024.03.08 val PER: 0.1821
2026-01-13 23:05:54,694: t15.2024.03.15 val PER: 0.1726
2026-01-13 23:05:54,694: t15.2024.03.17 val PER: 0.0879
2026-01-13 23:05:54,694: t15.2024.05.10 val PER: 0.1278
2026-01-13 23:05:54,694: t15.2024.06.14 val PER: 0.1309
2026-01-13 23:05:54,694: t15.2024.07.19 val PER: 0.1655
2026-01-13 23:05:54,694: t15.2024.07.21 val PER: 0.0566
2026-01-13 23:05:54,694: t15.2024.07.28 val PER: 0.0934
2026-01-13 23:05:54,694: t15.2025.01.10 val PER: 0.2383
2026-01-13 23:05:54,694: t15.2025.01.12 val PER: 0.1024
2026-01-13 23:05:54,694: t15.2025.03.14 val PER: 0.3003
2026-01-13 23:05:54,695: t15.2025.03.16 val PER: 0.1414
2026-01-13 23:05:54,695: t15.2025.03.30 val PER: 0.2195
2026-01-13 23:05:54,695: t15.2025.04.13 val PER: 0.1897
2026-01-13 23:05:54,844: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_36500
2026-01-13 23:06:03,668: Train batch 36600: loss: 0.92 grad norm: 23.71 time: 0.078
2026-01-13 23:06:21,281: Train batch 36800: loss: 1.26 grad norm: 31.40 time: 0.054
2026-01-13 23:06:38,926: Train batch 37000: loss: 0.70 grad norm: 19.71 time: 0.067
2026-01-13 23:06:38,926: Running test after training batch: 37000
2026-01-13 23:06:39,057: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:06:43,835: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:06:43,890: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:06:57,001: Val batch 37000: PER (avg): 0.1103 CTC Loss (avg): 16.7959 WER(5gram): 17.28% (n=256) time: 18.074
2026-01-13 23:06:57,001: WER lens: avg_true_words=5.99 avg_pred_words=6.31 max_pred_words=13
2026-01-13 23:06:57,001: t15.2023.08.13 val PER: 0.0884
2026-01-13 23:06:57,001: t15.2023.08.18 val PER: 0.0679
2026-01-13 23:06:57,002: t15.2023.08.20 val PER: 0.0667
2026-01-13 23:06:57,002: t15.2023.08.25 val PER: 0.0708
2026-01-13 23:06:57,002: t15.2023.08.27 val PER: 0.1479
2026-01-13 23:06:57,002: t15.2023.09.01 val PER: 0.0495
2026-01-13 23:06:57,002: t15.2023.09.03 val PER: 0.1354
2026-01-13 23:06:57,002: t15.2023.09.24 val PER: 0.0898
2026-01-13 23:06:57,002: t15.2023.09.29 val PER: 0.1034
2026-01-13 23:06:57,002: t15.2023.10.01 val PER: 0.1433
2026-01-13 23:06:57,002: t15.2023.10.06 val PER: 0.0646
2026-01-13 23:06:57,002: t15.2023.10.08 val PER: 0.1989
2026-01-13 23:06:57,003: t15.2023.10.13 val PER: 0.1466
2026-01-13 23:06:57,003: t15.2023.10.15 val PER: 0.1028
2026-01-13 23:06:57,003: t15.2023.10.20 val PER: 0.1577
2026-01-13 23:06:57,003: t15.2023.10.22 val PER: 0.0757
2026-01-13 23:06:57,003: t15.2023.11.03 val PER: 0.1445
2026-01-13 23:06:57,003: t15.2023.11.04 val PER: 0.0273
2026-01-13 23:06:57,003: t15.2023.11.17 val PER: 0.0264
2026-01-13 23:06:57,003: t15.2023.11.19 val PER: 0.0180
2026-01-13 23:06:57,004: t15.2023.11.26 val PER: 0.0514
2026-01-13 23:06:57,004: t15.2023.12.03 val PER: 0.0588
2026-01-13 23:06:57,004: t15.2023.12.08 val PER: 0.0493
2026-01-13 23:06:57,004: t15.2023.12.10 val PER: 0.0486
2026-01-13 23:06:57,004: t15.2023.12.17 val PER: 0.0852
2026-01-13 23:06:57,004: t15.2023.12.29 val PER: 0.0789
2026-01-13 23:06:57,004: t15.2024.02.25 val PER: 0.0801
2026-01-13 23:06:57,004: t15.2024.03.08 val PER: 0.1807
2026-01-13 23:06:57,004: t15.2024.03.15 val PER: 0.1607
2026-01-13 23:06:57,004: t15.2024.03.17 val PER: 0.0865
2026-01-13 23:06:57,004: t15.2024.05.10 val PER: 0.1293
2026-01-13 23:06:57,004: t15.2024.06.14 val PER: 0.1388
2026-01-13 23:06:57,004: t15.2024.07.19 val PER: 0.1694
2026-01-13 23:06:57,004: t15.2024.07.21 val PER: 0.0593
2026-01-13 23:06:57,004: t15.2024.07.28 val PER: 0.0949
2026-01-13 23:06:57,004: t15.2025.01.10 val PER: 0.2534
2026-01-13 23:06:57,005: t15.2025.01.12 val PER: 0.1062
2026-01-13 23:06:57,005: t15.2025.03.14 val PER: 0.3151
2026-01-13 23:06:57,005: t15.2025.03.16 val PER: 0.1414
2026-01-13 23:06:57,005: t15.2025.03.30 val PER: 0.2322
2026-01-13 23:06:57,005: t15.2025.04.13 val PER: 0.2040
2026-01-13 23:06:57,147: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_37000
2026-01-13 23:07:14,661: Train batch 37200: loss: 0.67 grad norm: 21.63 time: 0.063
2026-01-13 23:07:32,296: Train batch 37400: loss: 0.42 grad norm: 15.69 time: 0.081
2026-01-13 23:07:40,870: Running test after training batch: 37500
2026-01-13 23:07:41,026: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:07:45,822: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:07:45,876: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:07:58,376: Val batch 37500: PER (avg): 0.1103 CTC Loss (avg): 16.9611 WER(5gram): 17.80% (n=256) time: 17.506
2026-01-13 23:07:58,377: WER lens: avg_true_words=5.99 avg_pred_words=6.30 max_pred_words=13
2026-01-13 23:07:58,377: t15.2023.08.13 val PER: 0.0728
2026-01-13 23:07:58,377: t15.2023.08.18 val PER: 0.0763
2026-01-13 23:07:58,377: t15.2023.08.20 val PER: 0.0635
2026-01-13 23:07:58,377: t15.2023.08.25 val PER: 0.0723
2026-01-13 23:07:58,377: t15.2023.08.27 val PER: 0.1447
2026-01-13 23:07:58,378: t15.2023.09.01 val PER: 0.0503
2026-01-13 23:07:58,378: t15.2023.09.03 val PER: 0.1259
2026-01-13 23:07:58,378: t15.2023.09.24 val PER: 0.0934
2026-01-13 23:07:58,378: t15.2023.09.29 val PER: 0.1002
2026-01-13 23:07:58,378: t15.2023.10.01 val PER: 0.1361
2026-01-13 23:07:58,378: t15.2023.10.06 val PER: 0.0527
2026-01-13 23:07:58,378: t15.2023.10.08 val PER: 0.2003
2026-01-13 23:07:58,378: t15.2023.10.13 val PER: 0.1427
2026-01-13 23:07:58,378: t15.2023.10.15 val PER: 0.1074
2026-01-13 23:07:58,378: t15.2023.10.20 val PER: 0.1544
2026-01-13 23:07:58,378: t15.2023.10.22 val PER: 0.0813
2026-01-13 23:07:58,378: t15.2023.11.03 val PER: 0.1513
2026-01-13 23:07:58,378: t15.2023.11.04 val PER: 0.0205
2026-01-13 23:07:58,378: t15.2023.11.17 val PER: 0.0218
2026-01-13 23:07:58,378: t15.2023.11.19 val PER: 0.0180
2026-01-13 23:07:58,378: t15.2023.11.26 val PER: 0.0551
2026-01-13 23:07:58,379: t15.2023.12.03 val PER: 0.0525
2026-01-13 23:07:58,379: t15.2023.12.08 val PER: 0.0433
2026-01-13 23:07:58,379: t15.2023.12.10 val PER: 0.0447
2026-01-13 23:07:58,379: t15.2023.12.17 val PER: 0.0925
2026-01-13 23:07:58,379: t15.2023.12.29 val PER: 0.0858
2026-01-13 23:07:58,379: t15.2024.02.25 val PER: 0.0716
2026-01-13 23:07:58,379: t15.2024.03.08 val PER: 0.1778
2026-01-13 23:07:58,379: t15.2024.03.15 val PER: 0.1689
2026-01-13 23:07:58,379: t15.2024.03.17 val PER: 0.0865
2026-01-13 23:07:58,379: t15.2024.05.10 val PER: 0.1352
2026-01-13 23:07:58,379: t15.2024.06.14 val PER: 0.1278
2026-01-13 23:07:58,379: t15.2024.07.19 val PER: 0.1780
2026-01-13 23:07:58,380: t15.2024.07.21 val PER: 0.0669
2026-01-13 23:07:58,380: t15.2024.07.28 val PER: 0.0985
2026-01-13 23:07:58,380: t15.2025.01.10 val PER: 0.2521
2026-01-13 23:07:58,380: t15.2025.01.12 val PER: 0.1078
2026-01-13 23:07:58,380: t15.2025.03.14 val PER: 0.3180
2026-01-13 23:07:58,380: t15.2025.03.16 val PER: 0.1414
2026-01-13 23:07:58,380: t15.2025.03.30 val PER: 0.2276
2026-01-13 23:07:58,380: t15.2025.04.13 val PER: 0.1954
2026-01-13 23:07:58,523: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_37500
2026-01-13 23:08:07,195: Train batch 37600: loss: 0.29 grad norm: 13.09 time: 0.054
2026-01-13 23:08:24,951: Train batch 37800: loss: 1.19 grad norm: 29.26 time: 0.093
2026-01-13 23:08:42,333: Train batch 38000: loss: 1.23 grad norm: 27.73 time: 0.079
2026-01-13 23:08:42,333: Running test after training batch: 38000
2026-01-13 23:08:42,469: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:08:47,231: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:08:47,283: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cents
2026-01-13 23:08:59,897: Val batch 38000: PER (avg): 0.1108 CTC Loss (avg): 16.8965 WER(5gram): 19.23% (n=256) time: 17.563
2026-01-13 23:08:59,897: WER lens: avg_true_words=5.99 avg_pred_words=6.31 max_pred_words=13
2026-01-13 23:08:59,898: t15.2023.08.13 val PER: 0.0717
2026-01-13 23:08:59,898: t15.2023.08.18 val PER: 0.0754
2026-01-13 23:08:59,898: t15.2023.08.20 val PER: 0.0739
2026-01-13 23:08:59,898: t15.2023.08.25 val PER: 0.0738
2026-01-13 23:08:59,898: t15.2023.08.27 val PER: 0.1431
2026-01-13 23:08:59,898: t15.2023.09.01 val PER: 0.0511
2026-01-13 23:08:59,898: t15.2023.09.03 val PER: 0.1271
2026-01-13 23:08:59,899: t15.2023.09.24 val PER: 0.1019
2026-01-13 23:08:59,899: t15.2023.09.29 val PER: 0.1008
2026-01-13 23:08:59,899: t15.2023.10.01 val PER: 0.1367
2026-01-13 23:08:59,899: t15.2023.10.06 val PER: 0.0581
2026-01-13 23:08:59,899: t15.2023.10.08 val PER: 0.1962
2026-01-13 23:08:59,899: t15.2023.10.13 val PER: 0.1443
2026-01-13 23:08:59,899: t15.2023.10.15 val PER: 0.0989
2026-01-13 23:08:59,899: t15.2023.10.20 val PER: 0.1477
2026-01-13 23:08:59,899: t15.2023.10.22 val PER: 0.0969
2026-01-13 23:08:59,899: t15.2023.11.03 val PER: 0.1594
2026-01-13 23:08:59,900: t15.2023.11.04 val PER: 0.0239
2026-01-13 23:08:59,900: t15.2023.11.17 val PER: 0.0218
2026-01-13 23:08:59,900: t15.2023.11.19 val PER: 0.0220
2026-01-13 23:08:59,900: t15.2023.11.26 val PER: 0.0551
2026-01-13 23:08:59,900: t15.2023.12.03 val PER: 0.0599
2026-01-13 23:08:59,900: t15.2023.12.08 val PER: 0.0453
2026-01-13 23:08:59,900: t15.2023.12.10 val PER: 0.0447
2026-01-13 23:08:59,900: t15.2023.12.17 val PER: 0.0925
2026-01-13 23:08:59,900: t15.2023.12.29 val PER: 0.0844
2026-01-13 23:08:59,900: t15.2024.02.25 val PER: 0.0758
2026-01-13 23:08:59,900: t15.2024.03.08 val PER: 0.1764
2026-01-13 23:08:59,900: t15.2024.03.15 val PER: 0.1695
2026-01-13 23:08:59,900: t15.2024.03.17 val PER: 0.0886
2026-01-13 23:08:59,900: t15.2024.05.10 val PER: 0.1293
2026-01-13 23:08:59,900: t15.2024.06.14 val PER: 0.1325
2026-01-13 23:08:59,901: t15.2024.07.19 val PER: 0.1773
2026-01-13 23:08:59,901: t15.2024.07.21 val PER: 0.0586
2026-01-13 23:08:59,901: t15.2024.07.28 val PER: 0.1044
2026-01-13 23:08:59,901: t15.2025.01.10 val PER: 0.2479
2026-01-13 23:08:59,901: t15.2025.01.12 val PER: 0.1070
2026-01-13 23:08:59,901: t15.2025.03.14 val PER: 0.3003
2026-01-13 23:08:59,901: t15.2025.03.16 val PER: 0.1361
2026-01-13 23:08:59,901: t15.2025.03.30 val PER: 0.2241
2026-01-13 23:08:59,901: t15.2025.04.13 val PER: 0.1883
2026-01-13 23:09:00,045: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_38000
2026-01-13 23:09:17,705: Train batch 38200: loss: 1.14 grad norm: 30.62 time: 0.091
2026-01-13 23:09:35,073: Train batch 38400: loss: 2.45 grad norm: 30.36 time: 0.078
2026-01-13 23:09:43,837: Running test after training batch: 38500
2026-01-13 23:09:44,007: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:09:48,791: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:09:48,842: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the costs set
2026-01-13 23:10:01,277: Val batch 38500: PER (avg): 0.1113 CTC Loss (avg): 17.2739 WER(5gram): 17.60% (n=256) time: 17.439
2026-01-13 23:10:01,277: WER lens: avg_true_words=5.99 avg_pred_words=6.29 max_pred_words=13
2026-01-13 23:10:01,278: t15.2023.08.13 val PER: 0.0852
2026-01-13 23:10:01,278: t15.2023.08.18 val PER: 0.0788
2026-01-13 23:10:01,278: t15.2023.08.20 val PER: 0.0651
2026-01-13 23:10:01,278: t15.2023.08.25 val PER: 0.0723
2026-01-13 23:10:01,278: t15.2023.08.27 val PER: 0.1415
2026-01-13 23:10:01,278: t15.2023.09.01 val PER: 0.0511
2026-01-13 23:10:01,278: t15.2023.09.03 val PER: 0.1247
2026-01-13 23:10:01,279: t15.2023.09.24 val PER: 0.0898
2026-01-13 23:10:01,279: t15.2023.09.29 val PER: 0.1002
2026-01-13 23:10:01,279: t15.2023.10.01 val PER: 0.1460
2026-01-13 23:10:01,279: t15.2023.10.06 val PER: 0.0603
2026-01-13 23:10:01,279: t15.2023.10.08 val PER: 0.2003
2026-01-13 23:10:01,279: t15.2023.10.13 val PER: 0.1528
2026-01-13 23:10:01,279: t15.2023.10.15 val PER: 0.1035
2026-01-13 23:10:01,279: t15.2023.10.20 val PER: 0.1611
2026-01-13 23:10:01,279: t15.2023.10.22 val PER: 0.0991
2026-01-13 23:10:01,279: t15.2023.11.03 val PER: 0.1513
2026-01-13 23:10:01,279: t15.2023.11.04 val PER: 0.0307
2026-01-13 23:10:01,279: t15.2023.11.17 val PER: 0.0233
2026-01-13 23:10:01,279: t15.2023.11.19 val PER: 0.0259
2026-01-13 23:10:01,279: t15.2023.11.26 val PER: 0.0529
2026-01-13 23:10:01,279: t15.2023.12.03 val PER: 0.0630
2026-01-13 23:10:01,280: t15.2023.12.08 val PER: 0.0413
2026-01-13 23:10:01,280: t15.2023.12.10 val PER: 0.0434
2026-01-13 23:10:01,280: t15.2023.12.17 val PER: 0.0811
2026-01-13 23:10:01,280: t15.2023.12.29 val PER: 0.0837
2026-01-13 23:10:01,280: t15.2024.02.25 val PER: 0.0801
2026-01-13 23:10:01,280: t15.2024.03.08 val PER: 0.1664
2026-01-13 23:10:01,280: t15.2024.03.15 val PER: 0.1701
2026-01-13 23:10:01,280: t15.2024.03.17 val PER: 0.0921
2026-01-13 23:10:01,281: t15.2024.05.10 val PER: 0.1293
2026-01-13 23:10:01,281: t15.2024.06.14 val PER: 0.1451
2026-01-13 23:10:01,281: t15.2024.07.19 val PER: 0.1734
2026-01-13 23:10:01,281: t15.2024.07.21 val PER: 0.0662
2026-01-13 23:10:01,281: t15.2024.07.28 val PER: 0.1066
2026-01-13 23:10:01,281: t15.2025.01.10 val PER: 0.2534
2026-01-13 23:10:01,281: t15.2025.01.12 val PER: 0.0939
2026-01-13 23:10:01,281: t15.2025.03.14 val PER: 0.2973
2026-01-13 23:10:01,281: t15.2025.03.16 val PER: 0.1361
2026-01-13 23:10:01,281: t15.2025.03.30 val PER: 0.2287
2026-01-13 23:10:01,281: t15.2025.04.13 val PER: 0.1969
2026-01-13 23:10:01,424: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_38500
2026-01-13 23:10:10,089: Train batch 38600: loss: 0.74 grad norm: 19.12 time: 0.069
2026-01-13 23:10:27,784: Train batch 38800: loss: 0.73 grad norm: 23.13 time: 0.056
2026-01-13 23:10:45,513: Train batch 39000: loss: 0.45 grad norm: 25.07 time: 0.082
2026-01-13 23:10:45,513: Running test after training batch: 39000
2026-01-13 23:10:45,619: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:10:50,399: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:10:50,464: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:11:02,818: Val batch 39000: PER (avg): 0.1100 CTC Loss (avg): 17.3322 WER(5gram): 16.56% (n=256) time: 17.304
2026-01-13 23:11:02,818: WER lens: avg_true_words=5.99 avg_pred_words=6.27 max_pred_words=13
2026-01-13 23:11:02,818: t15.2023.08.13 val PER: 0.0738
2026-01-13 23:11:02,818: t15.2023.08.18 val PER: 0.0813
2026-01-13 23:11:02,818: t15.2023.08.20 val PER: 0.0667
2026-01-13 23:11:02,818: t15.2023.08.25 val PER: 0.0753
2026-01-13 23:11:02,819: t15.2023.08.27 val PER: 0.1463
2026-01-13 23:11:02,819: t15.2023.09.01 val PER: 0.0528
2026-01-13 23:11:02,819: t15.2023.09.03 val PER: 0.1259
2026-01-13 23:11:02,819: t15.2023.09.24 val PER: 0.0922
2026-01-13 23:11:02,819: t15.2023.09.29 val PER: 0.1040
2026-01-13 23:11:02,819: t15.2023.10.01 val PER: 0.1387
2026-01-13 23:11:02,819: t15.2023.10.06 val PER: 0.0560
2026-01-13 23:11:02,820: t15.2023.10.08 val PER: 0.1989
2026-01-13 23:11:02,820: t15.2023.10.13 val PER: 0.1443
2026-01-13 23:11:02,820: t15.2023.10.15 val PER: 0.1055
2026-01-13 23:11:02,820: t15.2023.10.20 val PER: 0.1577
2026-01-13 23:11:02,820: t15.2023.10.22 val PER: 0.0924
2026-01-13 23:11:02,820: t15.2023.11.03 val PER: 0.1513
2026-01-13 23:11:02,820: t15.2023.11.04 val PER: 0.0239
2026-01-13 23:11:02,820: t15.2023.11.17 val PER: 0.0156
2026-01-13 23:11:02,820: t15.2023.11.19 val PER: 0.0140
2026-01-13 23:11:02,820: t15.2023.11.26 val PER: 0.0522
2026-01-13 23:11:02,820: t15.2023.12.03 val PER: 0.0546
2026-01-13 23:11:02,821: t15.2023.12.08 val PER: 0.0446
2026-01-13 23:11:02,821: t15.2023.12.10 val PER: 0.0434
2026-01-13 23:11:02,821: t15.2023.12.17 val PER: 0.0956
2026-01-13 23:11:02,821: t15.2023.12.29 val PER: 0.0789
2026-01-13 23:11:02,821: t15.2024.02.25 val PER: 0.0744
2026-01-13 23:11:02,821: t15.2024.03.08 val PER: 0.1807
2026-01-13 23:11:02,821: t15.2024.03.15 val PER: 0.1757
2026-01-13 23:11:02,821: t15.2024.03.17 val PER: 0.0900
2026-01-13 23:11:02,821: t15.2024.05.10 val PER: 0.1278
2026-01-13 23:11:02,821: t15.2024.06.14 val PER: 0.1420
2026-01-13 23:11:02,821: t15.2024.07.19 val PER: 0.1734
2026-01-13 23:11:02,821: t15.2024.07.21 val PER: 0.0552
2026-01-13 23:11:02,821: t15.2024.07.28 val PER: 0.1029
2026-01-13 23:11:02,821: t15.2025.01.10 val PER: 0.2534
2026-01-13 23:11:02,821: t15.2025.01.12 val PER: 0.1008
2026-01-13 23:11:02,822: t15.2025.03.14 val PER: 0.2988
2026-01-13 23:11:02,822: t15.2025.03.16 val PER: 0.1230
2026-01-13 23:11:02,822: t15.2025.03.30 val PER: 0.2218
2026-01-13 23:11:02,822: t15.2025.04.13 val PER: 0.1897
2026-01-13 23:11:02,962: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_39000
2026-01-13 23:11:20,450: Train batch 39200: loss: 0.47 grad norm: 21.71 time: 0.061
2026-01-13 23:11:37,815: Train batch 39400: loss: 0.98 grad norm: 25.06 time: 0.062
2026-01-13 23:11:46,700: Running test after training batch: 39500
2026-01-13 23:11:46,807: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:11:51,693: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:11:51,747: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:12:04,749: Val batch 39500: PER (avg): 0.1099 CTC Loss (avg): 17.5827 WER(5gram): 17.73% (n=256) time: 18.049
2026-01-13 23:12:04,750: WER lens: avg_true_words=5.99 avg_pred_words=6.30 max_pred_words=13
2026-01-13 23:12:04,750: t15.2023.08.13 val PER: 0.0780
2026-01-13 23:12:04,750: t15.2023.08.18 val PER: 0.0796
2026-01-13 23:12:04,750: t15.2023.08.20 val PER: 0.0683
2026-01-13 23:12:04,750: t15.2023.08.25 val PER: 0.0678
2026-01-13 23:12:04,750: t15.2023.08.27 val PER: 0.1511
2026-01-13 23:12:04,751: t15.2023.09.01 val PER: 0.0511
2026-01-13 23:12:04,751: t15.2023.09.03 val PER: 0.1295
2026-01-13 23:12:04,751: t15.2023.09.24 val PER: 0.0971
2026-01-13 23:12:04,751: t15.2023.09.29 val PER: 0.1008
2026-01-13 23:12:04,751: t15.2023.10.01 val PER: 0.1387
2026-01-13 23:12:04,751: t15.2023.10.06 val PER: 0.0549
2026-01-13 23:12:04,751: t15.2023.10.08 val PER: 0.1989
2026-01-13 23:12:04,752: t15.2023.10.13 val PER: 0.1490
2026-01-13 23:12:04,752: t15.2023.10.15 val PER: 0.1094
2026-01-13 23:12:04,752: t15.2023.10.20 val PER: 0.1678
2026-01-13 23:12:04,752: t15.2023.10.22 val PER: 0.0913
2026-01-13 23:12:04,752: t15.2023.11.03 val PER: 0.1479
2026-01-13 23:12:04,752: t15.2023.11.04 val PER: 0.0171
2026-01-13 23:12:04,752: t15.2023.11.17 val PER: 0.0233
2026-01-13 23:12:04,752: t15.2023.11.19 val PER: 0.0180
2026-01-13 23:12:04,752: t15.2023.11.26 val PER: 0.0551
2026-01-13 23:12:04,752: t15.2023.12.03 val PER: 0.0599
2026-01-13 23:12:04,753: t15.2023.12.08 val PER: 0.0439
2026-01-13 23:12:04,753: t15.2023.12.10 val PER: 0.0394
2026-01-13 23:12:04,753: t15.2023.12.17 val PER: 0.0915
2026-01-13 23:12:04,753: t15.2023.12.29 val PER: 0.0810
2026-01-13 23:12:04,753: t15.2024.02.25 val PER: 0.0632
2026-01-13 23:12:04,753: t15.2024.03.08 val PER: 0.1735
2026-01-13 23:12:04,753: t15.2024.03.15 val PER: 0.1532
2026-01-13 23:12:04,753: t15.2024.03.17 val PER: 0.0872
2026-01-13 23:12:04,753: t15.2024.05.10 val PER: 0.1263
2026-01-13 23:12:04,754: t15.2024.06.14 val PER: 0.1262
2026-01-13 23:12:04,754: t15.2024.07.19 val PER: 0.1806
2026-01-13 23:12:04,754: t15.2024.07.21 val PER: 0.0586
2026-01-13 23:12:04,754: t15.2024.07.28 val PER: 0.1022
2026-01-13 23:12:04,754: t15.2025.01.10 val PER: 0.2617
2026-01-13 23:12:04,754: t15.2025.01.12 val PER: 0.1101
2026-01-13 23:12:04,754: t15.2025.03.14 val PER: 0.3018
2026-01-13 23:12:04,754: t15.2025.03.16 val PER: 0.1243
2026-01-13 23:12:04,754: t15.2025.03.30 val PER: 0.2241
2026-01-13 23:12:04,754: t15.2025.04.13 val PER: 0.1983
2026-01-13 23:12:04,896: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_39500
2026-01-13 23:12:13,790: Train batch 39600: loss: 0.58 grad norm: 24.88 time: 0.059
2026-01-13 23:12:31,266: Train batch 39800: loss: 0.83 grad norm: 31.43 time: 0.051
2026-01-13 23:12:48,630: Train batch 40000: loss: 0.50 grad norm: 19.90 time: 0.062
2026-01-13 23:12:48,630: Running test after training batch: 40000
2026-01-13 23:12:48,814: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:12:53,896: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:12:53,950: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cent
2026-01-13 23:13:06,756: Val batch 40000: PER (avg): 0.1116 CTC Loss (avg): 17.7329 WER(5gram): 17.01% (n=256) time: 18.126
2026-01-13 23:13:06,757: WER lens: avg_true_words=5.99 avg_pred_words=6.28 max_pred_words=13
2026-01-13 23:13:06,757: t15.2023.08.13 val PER: 0.0821
2026-01-13 23:13:06,757: t15.2023.08.18 val PER: 0.0729
2026-01-13 23:13:06,757: t15.2023.08.20 val PER: 0.0715
2026-01-13 23:13:06,757: t15.2023.08.25 val PER: 0.0633
2026-01-13 23:13:06,757: t15.2023.08.27 val PER: 0.1640
2026-01-13 23:13:06,757: t15.2023.09.01 val PER: 0.0487
2026-01-13 23:13:06,758: t15.2023.09.03 val PER: 0.1306
2026-01-13 23:13:06,758: t15.2023.09.24 val PER: 0.0922
2026-01-13 23:13:06,758: t15.2023.09.29 val PER: 0.1059
2026-01-13 23:13:06,758: t15.2023.10.01 val PER: 0.1519
2026-01-13 23:13:06,758: t15.2023.10.06 val PER: 0.0571
2026-01-13 23:13:06,758: t15.2023.10.08 val PER: 0.1908
2026-01-13 23:13:06,758: t15.2023.10.13 val PER: 0.1552
2026-01-13 23:13:06,758: t15.2023.10.15 val PER: 0.1081
2026-01-13 23:13:06,758: t15.2023.10.20 val PER: 0.1711
2026-01-13 23:13:06,758: t15.2023.10.22 val PER: 0.0935
2026-01-13 23:13:06,758: t15.2023.11.03 val PER: 0.1452
2026-01-13 23:13:06,758: t15.2023.11.04 val PER: 0.0205
2026-01-13 23:13:06,758: t15.2023.11.17 val PER: 0.0280
2026-01-13 23:13:06,759: t15.2023.11.19 val PER: 0.0220
2026-01-13 23:13:06,759: t15.2023.11.26 val PER: 0.0486
2026-01-13 23:13:06,759: t15.2023.12.03 val PER: 0.0536
2026-01-13 23:13:06,759: t15.2023.12.08 val PER: 0.0479
2026-01-13 23:13:06,759: t15.2023.12.10 val PER: 0.0342
2026-01-13 23:13:06,759: t15.2023.12.17 val PER: 0.0894
2026-01-13 23:13:06,759: t15.2023.12.29 val PER: 0.0830
2026-01-13 23:13:06,759: t15.2024.02.25 val PER: 0.0730
2026-01-13 23:13:06,759: t15.2024.03.08 val PER: 0.1863
2026-01-13 23:13:06,759: t15.2024.03.15 val PER: 0.1588
2026-01-13 23:13:06,759: t15.2024.03.17 val PER: 0.0879
2026-01-13 23:13:06,759: t15.2024.05.10 val PER: 0.1337
2026-01-13 23:13:06,759: t15.2024.06.14 val PER: 0.1278
2026-01-13 23:13:06,759: t15.2024.07.19 val PER: 0.1773
2026-01-13 23:13:06,759: t15.2024.07.21 val PER: 0.0614
2026-01-13 23:13:06,760: t15.2024.07.28 val PER: 0.1000
2026-01-13 23:13:06,760: t15.2025.01.10 val PER: 0.2562
2026-01-13 23:13:06,760: t15.2025.01.12 val PER: 0.1132
2026-01-13 23:13:06,760: t15.2025.03.14 val PER: 0.3166
2026-01-13 23:13:06,760: t15.2025.03.16 val PER: 0.1361
2026-01-13 23:13:06,760: t15.2025.03.30 val PER: 0.2253
2026-01-13 23:13:06,760: t15.2025.04.13 val PER: 0.1954
2026-01-13 23:13:06,907: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_40000
2026-01-13 23:13:24,492: Train batch 40200: loss: 0.30 grad norm: 12.96 time: 0.062
2026-01-13 23:13:42,286: Train batch 40400: loss: 0.68 grad norm: 29.16 time: 0.057
2026-01-13 23:13:51,074: Running test after training batch: 40500
2026-01-13 23:13:51,171: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:13:56,537: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:13:56,590: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cent
2026-01-13 23:14:09,392: Val batch 40500: PER (avg): 0.1081 CTC Loss (avg): 17.1276 WER(5gram): 16.62% (n=256) time: 18.318
2026-01-13 23:14:09,393: WER lens: avg_true_words=5.99 avg_pred_words=6.28 max_pred_words=13
2026-01-13 23:14:09,393: t15.2023.08.13 val PER: 0.0759
2026-01-13 23:14:09,393: t15.2023.08.18 val PER: 0.0763
2026-01-13 23:14:09,393: t15.2023.08.20 val PER: 0.0659
2026-01-13 23:14:09,393: t15.2023.08.25 val PER: 0.0753
2026-01-13 23:14:09,394: t15.2023.08.27 val PER: 0.1367
2026-01-13 23:14:09,394: t15.2023.09.01 val PER: 0.0430
2026-01-13 23:14:09,394: t15.2023.09.03 val PER: 0.1200
2026-01-13 23:14:09,394: t15.2023.09.24 val PER: 0.0971
2026-01-13 23:14:09,394: t15.2023.09.29 val PER: 0.0970
2026-01-13 23:14:09,394: t15.2023.10.01 val PER: 0.1453
2026-01-13 23:14:09,394: t15.2023.10.06 val PER: 0.0527
2026-01-13 23:14:09,394: t15.2023.10.08 val PER: 0.1935
2026-01-13 23:14:09,395: t15.2023.10.13 val PER: 0.1435
2026-01-13 23:14:09,395: t15.2023.10.15 val PER: 0.1121
2026-01-13 23:14:09,395: t15.2023.10.20 val PER: 0.1678
2026-01-13 23:14:09,395: t15.2023.10.22 val PER: 0.0902
2026-01-13 23:14:09,395: t15.2023.11.03 val PER: 0.1445
2026-01-13 23:14:09,395: t15.2023.11.04 val PER: 0.0307
2026-01-13 23:14:09,395: t15.2023.11.17 val PER: 0.0218
2026-01-13 23:14:09,395: t15.2023.11.19 val PER: 0.0200
2026-01-13 23:14:09,395: t15.2023.11.26 val PER: 0.0464
2026-01-13 23:14:09,395: t15.2023.12.03 val PER: 0.0567
2026-01-13 23:14:09,395: t15.2023.12.08 val PER: 0.0413
2026-01-13 23:14:09,395: t15.2023.12.10 val PER: 0.0342
2026-01-13 23:14:09,395: t15.2023.12.17 val PER: 0.0946
2026-01-13 23:14:09,395: t15.2023.12.29 val PER: 0.0769
2026-01-13 23:14:09,395: t15.2024.02.25 val PER: 0.0674
2026-01-13 23:14:09,396: t15.2024.03.08 val PER: 0.1735
2026-01-13 23:14:09,396: t15.2024.03.15 val PER: 0.1639
2026-01-13 23:14:09,396: t15.2024.03.17 val PER: 0.0900
2026-01-13 23:14:09,396: t15.2024.05.10 val PER: 0.1263
2026-01-13 23:14:09,396: t15.2024.06.14 val PER: 0.1199
2026-01-13 23:14:09,396: t15.2024.07.19 val PER: 0.1661
2026-01-13 23:14:09,396: t15.2024.07.21 val PER: 0.0607
2026-01-13 23:14:09,396: t15.2024.07.28 val PER: 0.0993
2026-01-13 23:14:09,396: t15.2025.01.10 val PER: 0.2603
2026-01-13 23:14:09,396: t15.2025.01.12 val PER: 0.1093
2026-01-13 23:14:09,396: t15.2025.03.14 val PER: 0.2929
2026-01-13 23:14:09,396: t15.2025.03.16 val PER: 0.1361
2026-01-13 23:14:09,396: t15.2025.03.30 val PER: 0.2207
2026-01-13 23:14:09,396: t15.2025.04.13 val PER: 0.1897
2026-01-13 23:14:09,537: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_40500
2026-01-13 23:14:18,049: Train batch 40600: loss: 1.33 grad norm: 34.56 time: 0.059
2026-01-13 23:14:35,569: Train batch 40800: loss: 0.59 grad norm: 24.31 time: 0.058
2026-01-13 23:14:53,225: Train batch 41000: loss: 0.87 grad norm: 24.85 time: 0.061
2026-01-13 23:14:53,226: Running test after training batch: 41000
2026-01-13 23:14:53,368: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:14:58,154: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:14:58,205: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-13 23:15:11,157: Val batch 41000: PER (avg): 0.1100 CTC Loss (avg): 17.1546 WER(5gram): 17.67% (n=256) time: 17.931
2026-01-13 23:15:11,158: WER lens: avg_true_words=5.99 avg_pred_words=6.31 max_pred_words=13
2026-01-13 23:15:11,158: t15.2023.08.13 val PER: 0.0748
2026-01-13 23:15:11,158: t15.2023.08.18 val PER: 0.0746
2026-01-13 23:15:11,158: t15.2023.08.20 val PER: 0.0627
2026-01-13 23:15:11,158: t15.2023.08.25 val PER: 0.0708
2026-01-13 23:15:11,158: t15.2023.08.27 val PER: 0.1495
2026-01-13 23:15:11,158: t15.2023.09.01 val PER: 0.0536
2026-01-13 23:15:11,158: t15.2023.09.03 val PER: 0.1390
2026-01-13 23:15:11,158: t15.2023.09.24 val PER: 0.0959
2026-01-13 23:15:11,158: t15.2023.09.29 val PER: 0.1021
2026-01-13 23:15:11,159: t15.2023.10.01 val PER: 0.1532
2026-01-13 23:15:11,159: t15.2023.10.06 val PER: 0.0657
2026-01-13 23:15:11,159: t15.2023.10.08 val PER: 0.1962
2026-01-13 23:15:11,159: t15.2023.10.13 val PER: 0.1420
2026-01-13 23:15:11,159: t15.2023.10.15 val PER: 0.1127
2026-01-13 23:15:11,159: t15.2023.10.20 val PER: 0.1544
2026-01-13 23:15:11,159: t15.2023.10.22 val PER: 0.0913
2026-01-13 23:15:11,160: t15.2023.11.03 val PER: 0.1445
2026-01-13 23:15:11,160: t15.2023.11.04 val PER: 0.0273
2026-01-13 23:15:11,160: t15.2023.11.17 val PER: 0.0140
2026-01-13 23:15:11,160: t15.2023.11.19 val PER: 0.0160
2026-01-13 23:15:11,160: t15.2023.11.26 val PER: 0.0486
2026-01-13 23:15:11,160: t15.2023.12.03 val PER: 0.0599
2026-01-13 23:15:11,160: t15.2023.12.08 val PER: 0.0433
2026-01-13 23:15:11,160: t15.2023.12.10 val PER: 0.0447
2026-01-13 23:15:11,160: t15.2023.12.17 val PER: 0.0936
2026-01-13 23:15:11,160: t15.2023.12.29 val PER: 0.0837
2026-01-13 23:15:11,160: t15.2024.02.25 val PER: 0.0688
2026-01-13 23:15:11,161: t15.2024.03.08 val PER: 0.1807
2026-01-13 23:15:11,161: t15.2024.03.15 val PER: 0.1595
2026-01-13 23:15:11,161: t15.2024.03.17 val PER: 0.0865
2026-01-13 23:15:11,161: t15.2024.05.10 val PER: 0.1278
2026-01-13 23:15:11,161: t15.2024.06.14 val PER: 0.1230
2026-01-13 23:15:11,161: t15.2024.07.19 val PER: 0.1714
2026-01-13 23:15:11,161: t15.2024.07.21 val PER: 0.0614
2026-01-13 23:15:11,161: t15.2024.07.28 val PER: 0.0949
2026-01-13 23:15:11,161: t15.2025.01.10 val PER: 0.2507
2026-01-13 23:15:11,161: t15.2025.01.12 val PER: 0.1024
2026-01-13 23:15:11,161: t15.2025.03.14 val PER: 0.3210
2026-01-13 23:15:11,162: t15.2025.03.16 val PER: 0.1283
2026-01-13 23:15:11,162: t15.2025.03.30 val PER: 0.2287
2026-01-13 23:15:11,162: t15.2025.04.13 val PER: 0.1897
2026-01-13 23:15:11,304: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_41000
2026-01-13 23:15:28,629: Train batch 41200: loss: 0.59 grad norm: 19.81 time: 0.073
2026-01-13 23:15:46,055: Train batch 41400: loss: 0.94 grad norm: 24.24 time: 0.061
2026-01-13 23:15:54,989: Running test after training batch: 41500
2026-01-13 23:15:55,087: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:15:59,865: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:15:59,918: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost said
2026-01-13 23:16:13,408: Val batch 41500: PER (avg): 0.1087 CTC Loss (avg): 17.1330 WER(5gram): 17.80% (n=256) time: 18.418
2026-01-13 23:16:13,408: WER lens: avg_true_words=5.99 avg_pred_words=6.33 max_pred_words=14
2026-01-13 23:16:13,408: t15.2023.08.13 val PER: 0.0832
2026-01-13 23:16:13,409: t15.2023.08.18 val PER: 0.0813
2026-01-13 23:16:13,409: t15.2023.08.20 val PER: 0.0643
2026-01-13 23:16:13,409: t15.2023.08.25 val PER: 0.0768
2026-01-13 23:16:13,409: t15.2023.08.27 val PER: 0.1399
2026-01-13 23:16:13,409: t15.2023.09.01 val PER: 0.0495
2026-01-13 23:16:13,409: t15.2023.09.03 val PER: 0.1176
2026-01-13 23:16:13,409: t15.2023.09.24 val PER: 0.0922
2026-01-13 23:16:13,409: t15.2023.09.29 val PER: 0.0996
2026-01-13 23:16:13,409: t15.2023.10.01 val PER: 0.1433
2026-01-13 23:16:13,409: t15.2023.10.06 val PER: 0.0592
2026-01-13 23:16:13,409: t15.2023.10.08 val PER: 0.2030
2026-01-13 23:16:13,410: t15.2023.10.13 val PER: 0.1544
2026-01-13 23:16:13,410: t15.2023.10.15 val PER: 0.1154
2026-01-13 23:16:13,410: t15.2023.10.20 val PER: 0.1644
2026-01-13 23:16:13,410: t15.2023.10.22 val PER: 0.0835
2026-01-13 23:16:13,410: t15.2023.11.03 val PER: 0.1459
2026-01-13 23:16:13,410: t15.2023.11.04 val PER: 0.0239
2026-01-13 23:16:13,410: t15.2023.11.17 val PER: 0.0202
2026-01-13 23:16:13,410: t15.2023.11.19 val PER: 0.0160
2026-01-13 23:16:13,410: t15.2023.11.26 val PER: 0.0478
2026-01-13 23:16:13,410: t15.2023.12.03 val PER: 0.0546
2026-01-13 23:16:13,410: t15.2023.12.08 val PER: 0.0446
2026-01-13 23:16:13,410: t15.2023.12.10 val PER: 0.0434
2026-01-13 23:16:13,410: t15.2023.12.17 val PER: 0.0936
2026-01-13 23:16:13,411: t15.2023.12.29 val PER: 0.0810
2026-01-13 23:16:13,411: t15.2024.02.25 val PER: 0.0688
2026-01-13 23:16:13,411: t15.2024.03.08 val PER: 0.1721
2026-01-13 23:16:13,411: t15.2024.03.15 val PER: 0.1632
2026-01-13 23:16:13,411: t15.2024.03.17 val PER: 0.0802
2026-01-13 23:16:13,411: t15.2024.05.10 val PER: 0.1248
2026-01-13 23:16:13,411: t15.2024.06.14 val PER: 0.1120
2026-01-13 23:16:13,411: t15.2024.07.19 val PER: 0.1701
2026-01-13 23:16:13,411: t15.2024.07.21 val PER: 0.0566
2026-01-13 23:16:13,411: t15.2024.07.28 val PER: 0.0971
2026-01-13 23:16:13,411: t15.2025.01.10 val PER: 0.2548
2026-01-13 23:16:13,411: t15.2025.01.12 val PER: 0.1039
2026-01-13 23:16:13,411: t15.2025.03.14 val PER: 0.2988
2026-01-13 23:16:13,411: t15.2025.03.16 val PER: 0.1296
2026-01-13 23:16:13,411: t15.2025.03.30 val PER: 0.2276
2026-01-13 23:16:13,411: t15.2025.04.13 val PER: 0.1883
2026-01-13 23:16:13,557: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_41500
2026-01-13 23:16:22,197: Train batch 41600: loss: 0.87 grad norm: 23.26 time: 0.066
2026-01-13 23:16:39,755: Train batch 41800: loss: 0.65 grad norm: 22.69 time: 0.075
2026-01-13 23:16:57,937: Train batch 42000: loss: 0.62 grad norm: 22.22 time: 0.062
2026-01-13 23:16:57,937: Running test after training batch: 42000
2026-01-13 23:16:58,106: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:17:02,929: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:17:02,985: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:17:17,405: Val batch 42000: PER (avg): 0.1106 CTC Loss (avg): 17.6708 WER(5gram): 18.58% (n=256) time: 19.468
2026-01-13 23:17:17,406: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-13 23:17:17,406: t15.2023.08.13 val PER: 0.0842
2026-01-13 23:17:17,406: t15.2023.08.18 val PER: 0.0780
2026-01-13 23:17:17,406: t15.2023.08.20 val PER: 0.0659
2026-01-13 23:17:17,406: t15.2023.08.25 val PER: 0.0617
2026-01-13 23:17:17,406: t15.2023.08.27 val PER: 0.1415
2026-01-13 23:17:17,406: t15.2023.09.01 val PER: 0.0503
2026-01-13 23:17:17,406: t15.2023.09.03 val PER: 0.1283
2026-01-13 23:17:17,406: t15.2023.09.24 val PER: 0.0922
2026-01-13 23:17:17,406: t15.2023.09.29 val PER: 0.1053
2026-01-13 23:17:17,407: t15.2023.10.01 val PER: 0.1446
2026-01-13 23:17:17,407: t15.2023.10.06 val PER: 0.0581
2026-01-13 23:17:17,407: t15.2023.10.08 val PER: 0.2003
2026-01-13 23:17:17,407: t15.2023.10.13 val PER: 0.1482
2026-01-13 23:17:17,407: t15.2023.10.15 val PER: 0.1081
2026-01-13 23:17:17,407: t15.2023.10.20 val PER: 0.1611
2026-01-13 23:17:17,408: t15.2023.10.22 val PER: 0.0880
2026-01-13 23:17:17,408: t15.2023.11.03 val PER: 0.1479
2026-01-13 23:17:17,408: t15.2023.11.04 val PER: 0.0205
2026-01-13 23:17:17,408: t15.2023.11.17 val PER: 0.0202
2026-01-13 23:17:17,408: t15.2023.11.19 val PER: 0.0140
2026-01-13 23:17:17,408: t15.2023.11.26 val PER: 0.0551
2026-01-13 23:17:17,408: t15.2023.12.03 val PER: 0.0620
2026-01-13 23:17:17,408: t15.2023.12.08 val PER: 0.0473
2026-01-13 23:17:17,409: t15.2023.12.10 val PER: 0.0381
2026-01-13 23:17:17,409: t15.2023.12.17 val PER: 0.0936
2026-01-13 23:17:17,409: t15.2023.12.29 val PER: 0.0844
2026-01-13 23:17:17,409: t15.2024.02.25 val PER: 0.0702
2026-01-13 23:17:17,409: t15.2024.03.08 val PER: 0.1892
2026-01-13 23:17:17,409: t15.2024.03.15 val PER: 0.1645
2026-01-13 23:17:17,409: t15.2024.03.17 val PER: 0.0774
2026-01-13 23:17:17,409: t15.2024.05.10 val PER: 0.1352
2026-01-13 23:17:17,409: t15.2024.06.14 val PER: 0.1246
2026-01-13 23:17:17,409: t15.2024.07.19 val PER: 0.1688
2026-01-13 23:17:17,409: t15.2024.07.21 val PER: 0.0634
2026-01-13 23:17:17,409: t15.2024.07.28 val PER: 0.0956
2026-01-13 23:17:17,409: t15.2025.01.10 val PER: 0.2741
2026-01-13 23:17:17,409: t15.2025.01.12 val PER: 0.1062
2026-01-13 23:17:17,409: t15.2025.03.14 val PER: 0.3210
2026-01-13 23:17:17,410: t15.2025.03.16 val PER: 0.1309
2026-01-13 23:17:17,410: t15.2025.03.30 val PER: 0.2230
2026-01-13 23:17:17,410: t15.2025.04.13 val PER: 0.1883
2026-01-13 23:17:17,557: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_42000
2026-01-13 23:17:36,905: Train batch 42200: loss: 0.89 grad norm: 28.98 time: 0.062
2026-01-13 23:17:55,032: Train batch 42400: loss: 0.88 grad norm: 36.10 time: 0.048
2026-01-13 23:18:03,897: Running test after training batch: 42500
2026-01-13 23:18:04,066: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:18:09,105: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:18:09,163: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:18:23,437: Val batch 42500: PER (avg): 0.1082 CTC Loss (avg): 17.6698 WER(5gram): 18.19% (n=256) time: 19.540
2026-01-13 23:18:23,438: WER lens: avg_true_words=5.99 avg_pred_words=6.32 max_pred_words=14
2026-01-13 23:18:23,438: t15.2023.08.13 val PER: 0.0769
2026-01-13 23:18:23,438: t15.2023.08.18 val PER: 0.0813
2026-01-13 23:18:23,438: t15.2023.08.20 val PER: 0.0659
2026-01-13 23:18:23,438: t15.2023.08.25 val PER: 0.0723
2026-01-13 23:18:23,438: t15.2023.08.27 val PER: 0.1447
2026-01-13 23:18:23,439: t15.2023.09.01 val PER: 0.0479
2026-01-13 23:18:23,439: t15.2023.09.03 val PER: 0.1295
2026-01-13 23:18:23,439: t15.2023.09.24 val PER: 0.1007
2026-01-13 23:18:23,439: t15.2023.09.29 val PER: 0.0944
2026-01-13 23:18:23,439: t15.2023.10.01 val PER: 0.1407
2026-01-13 23:18:23,439: t15.2023.10.06 val PER: 0.0603
2026-01-13 23:18:23,439: t15.2023.10.08 val PER: 0.2043
2026-01-13 23:18:23,439: t15.2023.10.13 val PER: 0.1474
2026-01-13 23:18:23,439: t15.2023.10.15 val PER: 0.1055
2026-01-13 23:18:23,439: t15.2023.10.20 val PER: 0.1477
2026-01-13 23:18:23,439: t15.2023.10.22 val PER: 0.0935
2026-01-13 23:18:23,439: t15.2023.11.03 val PER: 0.1465
2026-01-13 23:18:23,439: t15.2023.11.04 val PER: 0.0273
2026-01-13 23:18:23,439: t15.2023.11.17 val PER: 0.0218
2026-01-13 23:18:23,439: t15.2023.11.19 val PER: 0.0180
2026-01-13 23:18:23,439: t15.2023.11.26 val PER: 0.0507
2026-01-13 23:18:23,440: t15.2023.12.03 val PER: 0.0567
2026-01-13 23:18:23,440: t15.2023.12.08 val PER: 0.0386
2026-01-13 23:18:23,440: t15.2023.12.10 val PER: 0.0447
2026-01-13 23:18:23,440: t15.2023.12.17 val PER: 0.0967
2026-01-13 23:18:23,440: t15.2023.12.29 val PER: 0.0803
2026-01-13 23:18:23,440: t15.2024.02.25 val PER: 0.0674
2026-01-13 23:18:23,440: t15.2024.03.08 val PER: 0.1721
2026-01-13 23:18:23,440: t15.2024.03.15 val PER: 0.1570
2026-01-13 23:18:23,440: t15.2024.03.17 val PER: 0.0872
2026-01-13 23:18:23,440: t15.2024.05.10 val PER: 0.1040
2026-01-13 23:18:23,440: t15.2024.06.14 val PER: 0.1199
2026-01-13 23:18:23,440: t15.2024.07.19 val PER: 0.1681
2026-01-13 23:18:23,440: t15.2024.07.21 val PER: 0.0621
2026-01-13 23:18:23,441: t15.2024.07.28 val PER: 0.0926
2026-01-13 23:18:23,441: t15.2025.01.10 val PER: 0.2562
2026-01-13 23:18:23,441: t15.2025.01.12 val PER: 0.0931
2026-01-13 23:18:23,441: t15.2025.03.14 val PER: 0.3047
2026-01-13 23:18:23,441: t15.2025.03.16 val PER: 0.1335
2026-01-13 23:18:23,441: t15.2025.03.30 val PER: 0.2379
2026-01-13 23:18:23,441: t15.2025.04.13 val PER: 0.1869
2026-01-13 23:18:23,583: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_42500
2026-01-13 23:18:32,323: Train batch 42600: loss: 0.27 grad norm: 12.47 time: 0.055
2026-01-13 23:18:49,757: Train batch 42800: loss: 0.84 grad norm: 22.86 time: 0.055
2026-01-13 23:19:07,268: Train batch 43000: loss: 0.61 grad norm: 19.76 time: 0.064
2026-01-13 23:19:07,269: Running test after training batch: 43000
2026-01-13 23:19:07,404: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:19:12,224: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:19:12,288: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-13 23:19:26,497: Val batch 43000: PER (avg): 0.1111 CTC Loss (avg): 17.7421 WER(5gram): 17.47% (n=256) time: 19.228
2026-01-13 23:19:26,497: WER lens: avg_true_words=5.99 avg_pred_words=6.32 max_pred_words=14
2026-01-13 23:19:26,497: t15.2023.08.13 val PER: 0.0780
2026-01-13 23:19:26,497: t15.2023.08.18 val PER: 0.0721
2026-01-13 23:19:26,498: t15.2023.08.20 val PER: 0.0723
2026-01-13 23:19:26,498: t15.2023.08.25 val PER: 0.0768
2026-01-13 23:19:26,498: t15.2023.08.27 val PER: 0.1367
2026-01-13 23:19:26,498: t15.2023.09.01 val PER: 0.0463
2026-01-13 23:19:26,498: t15.2023.09.03 val PER: 0.1211
2026-01-13 23:19:26,498: t15.2023.09.24 val PER: 0.0874
2026-01-13 23:19:26,498: t15.2023.09.29 val PER: 0.0976
2026-01-13 23:19:26,498: t15.2023.10.01 val PER: 0.1427
2026-01-13 23:19:26,498: t15.2023.10.06 val PER: 0.0538
2026-01-13 23:19:26,498: t15.2023.10.08 val PER: 0.2016
2026-01-13 23:19:26,499: t15.2023.10.13 val PER: 0.1443
2026-01-13 23:19:26,499: t15.2023.10.15 val PER: 0.1068
2026-01-13 23:19:26,499: t15.2023.10.20 val PER: 0.1577
2026-01-13 23:19:26,499: t15.2023.10.22 val PER: 0.0891
2026-01-13 23:19:26,499: t15.2023.11.03 val PER: 0.1459
2026-01-13 23:19:26,499: t15.2023.11.04 val PER: 0.0307
2026-01-13 23:19:26,499: t15.2023.11.17 val PER: 0.0264
2026-01-13 23:19:26,499: t15.2023.11.19 val PER: 0.0200
2026-01-13 23:19:26,499: t15.2023.11.26 val PER: 0.0609
2026-01-13 23:19:26,499: t15.2023.12.03 val PER: 0.0536
2026-01-13 23:19:26,499: t15.2023.12.08 val PER: 0.0413
2026-01-13 23:19:26,499: t15.2023.12.10 val PER: 0.0460
2026-01-13 23:19:26,499: t15.2023.12.17 val PER: 0.0967
2026-01-13 23:19:26,500: t15.2023.12.29 val PER: 0.0947
2026-01-13 23:19:26,500: t15.2024.02.25 val PER: 0.0815
2026-01-13 23:19:26,500: t15.2024.03.08 val PER: 0.1721
2026-01-13 23:19:26,500: t15.2024.03.15 val PER: 0.1682
2026-01-13 23:19:26,500: t15.2024.03.17 val PER: 0.0837
2026-01-13 23:19:26,500: t15.2024.05.10 val PER: 0.1293
2026-01-13 23:19:26,500: t15.2024.06.14 val PER: 0.1262
2026-01-13 23:19:26,500: t15.2024.07.19 val PER: 0.1694
2026-01-13 23:19:26,500: t15.2024.07.21 val PER: 0.0655
2026-01-13 23:19:26,500: t15.2024.07.28 val PER: 0.1000
2026-01-13 23:19:26,500: t15.2025.01.10 val PER: 0.2548
2026-01-13 23:19:26,500: t15.2025.01.12 val PER: 0.1078
2026-01-13 23:19:26,501: t15.2025.03.14 val PER: 0.3092
2026-01-13 23:19:26,501: t15.2025.03.16 val PER: 0.1414
2026-01-13 23:19:26,501: t15.2025.03.30 val PER: 0.2529
2026-01-13 23:19:26,501: t15.2025.04.13 val PER: 0.1969
2026-01-13 23:19:26,643: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_43000
2026-01-13 23:19:44,125: Train batch 43200: loss: 0.70 grad norm: 33.74 time: 0.065
2026-01-13 23:20:01,422: Train batch 43400: loss: 0.52 grad norm: 17.73 time: 0.065
2026-01-13 23:20:10,214: Running test after training batch: 43500
2026-01-13 23:20:10,408: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:20:15,223: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:20:15,287: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:20:28,650: Val batch 43500: PER (avg): 0.1082 CTC Loss (avg): 17.5333 WER(5gram): 17.34% (n=256) time: 18.436
2026-01-13 23:20:28,651: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=13
2026-01-13 23:20:28,651: t15.2023.08.13 val PER: 0.0686
2026-01-13 23:20:28,651: t15.2023.08.18 val PER: 0.0754
2026-01-13 23:20:28,651: t15.2023.08.20 val PER: 0.0643
2026-01-13 23:20:28,651: t15.2023.08.25 val PER: 0.0633
2026-01-13 23:20:28,652: t15.2023.08.27 val PER: 0.1415
2026-01-13 23:20:28,652: t15.2023.09.01 val PER: 0.0495
2026-01-13 23:20:28,652: t15.2023.09.03 val PER: 0.1152
2026-01-13 23:20:28,652: t15.2023.09.24 val PER: 0.0898
2026-01-13 23:20:28,652: t15.2023.09.29 val PER: 0.0989
2026-01-13 23:20:28,652: t15.2023.10.01 val PER: 0.1473
2026-01-13 23:20:28,652: t15.2023.10.06 val PER: 0.0560
2026-01-13 23:20:28,652: t15.2023.10.08 val PER: 0.2016
2026-01-13 23:20:28,652: t15.2023.10.13 val PER: 0.1583
2026-01-13 23:20:28,653: t15.2023.10.15 val PER: 0.1035
2026-01-13 23:20:28,653: t15.2023.10.20 val PER: 0.1477
2026-01-13 23:20:28,653: t15.2023.10.22 val PER: 0.0813
2026-01-13 23:20:28,653: t15.2023.11.03 val PER: 0.1425
2026-01-13 23:20:28,653: t15.2023.11.04 val PER: 0.0205
2026-01-13 23:20:28,653: t15.2023.11.17 val PER: 0.0202
2026-01-13 23:20:28,653: t15.2023.11.19 val PER: 0.0200
2026-01-13 23:20:28,653: t15.2023.11.26 val PER: 0.0478
2026-01-13 23:20:28,653: t15.2023.12.03 val PER: 0.0546
2026-01-13 23:20:28,653: t15.2023.12.08 val PER: 0.0413
2026-01-13 23:20:28,653: t15.2023.12.10 val PER: 0.0407
2026-01-13 23:20:28,654: t15.2023.12.17 val PER: 0.0967
2026-01-13 23:20:28,654: t15.2023.12.29 val PER: 0.0721
2026-01-13 23:20:28,654: t15.2024.02.25 val PER: 0.0632
2026-01-13 23:20:28,654: t15.2024.03.08 val PER: 0.1693
2026-01-13 23:20:28,654: t15.2024.03.15 val PER: 0.1532
2026-01-13 23:20:28,654: t15.2024.03.17 val PER: 0.0844
2026-01-13 23:20:28,654: t15.2024.05.10 val PER: 0.1308
2026-01-13 23:20:28,654: t15.2024.06.14 val PER: 0.1293
2026-01-13 23:20:28,654: t15.2024.07.19 val PER: 0.1786
2026-01-13 23:20:28,654: t15.2024.07.21 val PER: 0.0593
2026-01-13 23:20:28,654: t15.2024.07.28 val PER: 0.0963
2026-01-13 23:20:28,654: t15.2025.01.10 val PER: 0.2631
2026-01-13 23:20:28,655: t15.2025.01.12 val PER: 0.0931
2026-01-13 23:20:28,655: t15.2025.03.14 val PER: 0.3299
2026-01-13 23:20:28,655: t15.2025.03.16 val PER: 0.1309
2026-01-13 23:20:28,655: t15.2025.03.30 val PER: 0.2368
2026-01-13 23:20:28,655: t15.2025.04.13 val PER: 0.2068
2026-01-13 23:20:28,795: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_43500
2026-01-13 23:20:37,576: Train batch 43600: loss: 0.99 grad norm: 28.74 time: 0.048
2026-01-13 23:20:54,758: Train batch 43800: loss: 0.27 grad norm: 21.45 time: 0.065
2026-01-13 23:21:11,993: Train batch 44000: loss: 0.70 grad norm: 25.03 time: 0.057
2026-01-13 23:21:11,993: Running test after training batch: 44000
2026-01-13 23:21:12,088: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:21:16,909: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:21:16,966: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:21:31,451: Val batch 44000: PER (avg): 0.1078 CTC Loss (avg): 17.7015 WER(5gram): 19.23% (n=256) time: 19.458
2026-01-13 23:21:31,452: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=14
2026-01-13 23:21:31,452: t15.2023.08.13 val PER: 0.0728
2026-01-13 23:21:31,452: t15.2023.08.18 val PER: 0.0763
2026-01-13 23:21:31,452: t15.2023.08.20 val PER: 0.0643
2026-01-13 23:21:31,452: t15.2023.08.25 val PER: 0.0798
2026-01-13 23:21:31,452: t15.2023.08.27 val PER: 0.1399
2026-01-13 23:21:31,452: t15.2023.09.01 val PER: 0.0479
2026-01-13 23:21:31,452: t15.2023.09.03 val PER: 0.1283
2026-01-13 23:21:31,453: t15.2023.09.24 val PER: 0.0983
2026-01-13 23:21:31,453: t15.2023.09.29 val PER: 0.1047
2026-01-13 23:21:31,453: t15.2023.10.01 val PER: 0.1334
2026-01-13 23:21:31,453: t15.2023.10.06 val PER: 0.0571
2026-01-13 23:21:31,453: t15.2023.10.08 val PER: 0.1935
2026-01-13 23:21:31,453: t15.2023.10.13 val PER: 0.1513
2026-01-13 23:21:31,453: t15.2023.10.15 val PER: 0.1055
2026-01-13 23:21:31,453: t15.2023.10.20 val PER: 0.1644
2026-01-13 23:21:31,453: t15.2023.10.22 val PER: 0.0824
2026-01-13 23:21:31,454: t15.2023.11.03 val PER: 0.1438
2026-01-13 23:21:31,454: t15.2023.11.04 val PER: 0.0239
2026-01-13 23:21:31,454: t15.2023.11.17 val PER: 0.0187
2026-01-13 23:21:31,454: t15.2023.11.19 val PER: 0.0259
2026-01-13 23:21:31,454: t15.2023.11.26 val PER: 0.0486
2026-01-13 23:21:31,454: t15.2023.12.03 val PER: 0.0483
2026-01-13 23:21:31,454: t15.2023.12.08 val PER: 0.0453
2026-01-13 23:21:31,454: t15.2023.12.10 val PER: 0.0420
2026-01-13 23:21:31,454: t15.2023.12.17 val PER: 0.0925
2026-01-13 23:21:31,454: t15.2023.12.29 val PER: 0.0728
2026-01-13 23:21:31,454: t15.2024.02.25 val PER: 0.0646
2026-01-13 23:21:31,454: t15.2024.03.08 val PER: 0.1593
2026-01-13 23:21:31,454: t15.2024.03.15 val PER: 0.1532
2026-01-13 23:21:31,455: t15.2024.03.17 val PER: 0.0837
2026-01-13 23:21:31,455: t15.2024.05.10 val PER: 0.1367
2026-01-13 23:21:31,455: t15.2024.06.14 val PER: 0.1372
2026-01-13 23:21:31,455: t15.2024.07.19 val PER: 0.1688
2026-01-13 23:21:31,455: t15.2024.07.21 val PER: 0.0552
2026-01-13 23:21:31,455: t15.2024.07.28 val PER: 0.0926
2026-01-13 23:21:31,455: t15.2025.01.10 val PER: 0.2521
2026-01-13 23:21:31,455: t15.2025.01.12 val PER: 0.1055
2026-01-13 23:21:31,455: t15.2025.03.14 val PER: 0.3062
2026-01-13 23:21:31,455: t15.2025.03.16 val PER: 0.1466
2026-01-13 23:21:31,455: t15.2025.03.30 val PER: 0.2356
2026-01-13 23:21:31,455: t15.2025.04.13 val PER: 0.1912
2026-01-13 23:21:31,593: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_44000
2026-01-13 23:21:48,933: Train batch 44200: loss: 0.58 grad norm: 22.71 time: 0.059
2026-01-13 23:22:06,467: Train batch 44400: loss: 0.26 grad norm: 12.40 time: 0.055
2026-01-13 23:22:15,223: Running test after training batch: 44500
2026-01-13 23:22:15,363: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:22:20,210: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:22:20,267: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:22:34,290: Val batch 44500: PER (avg): 0.1079 CTC Loss (avg): 17.7881 WER(5gram): 18.90% (n=256) time: 19.067
2026-01-13 23:22:34,290: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=13
2026-01-13 23:22:34,290: t15.2023.08.13 val PER: 0.0780
2026-01-13 23:22:34,290: t15.2023.08.18 val PER: 0.0754
2026-01-13 23:22:34,290: t15.2023.08.20 val PER: 0.0675
2026-01-13 23:22:34,291: t15.2023.08.25 val PER: 0.0708
2026-01-13 23:22:34,291: t15.2023.08.27 val PER: 0.1431
2026-01-13 23:22:34,291: t15.2023.09.01 val PER: 0.0446
2026-01-13 23:22:34,291: t15.2023.09.03 val PER: 0.1295
2026-01-13 23:22:34,291: t15.2023.09.24 val PER: 0.1007
2026-01-13 23:22:34,291: t15.2023.09.29 val PER: 0.1021
2026-01-13 23:22:34,291: t15.2023.10.01 val PER: 0.1367
2026-01-13 23:22:34,291: t15.2023.10.06 val PER: 0.0581
2026-01-13 23:22:34,292: t15.2023.10.08 val PER: 0.1908
2026-01-13 23:22:34,292: t15.2023.10.13 val PER: 0.1435
2026-01-13 23:22:34,292: t15.2023.10.15 val PER: 0.1088
2026-01-13 23:22:34,292: t15.2023.10.20 val PER: 0.1544
2026-01-13 23:22:34,292: t15.2023.10.22 val PER: 0.0913
2026-01-13 23:22:34,292: t15.2023.11.03 val PER: 0.1438
2026-01-13 23:22:34,292: t15.2023.11.04 val PER: 0.0273
2026-01-13 23:22:34,292: t15.2023.11.17 val PER: 0.0171
2026-01-13 23:22:34,292: t15.2023.11.19 val PER: 0.0220
2026-01-13 23:22:34,292: t15.2023.11.26 val PER: 0.0457
2026-01-13 23:22:34,292: t15.2023.12.03 val PER: 0.0536
2026-01-13 23:22:34,292: t15.2023.12.08 val PER: 0.0406
2026-01-13 23:22:34,293: t15.2023.12.10 val PER: 0.0420
2026-01-13 23:22:34,293: t15.2023.12.17 val PER: 0.0884
2026-01-13 23:22:34,293: t15.2023.12.29 val PER: 0.0817
2026-01-13 23:22:34,293: t15.2024.02.25 val PER: 0.0744
2026-01-13 23:22:34,293: t15.2024.03.08 val PER: 0.1721
2026-01-13 23:22:34,293: t15.2024.03.15 val PER: 0.1526
2026-01-13 23:22:34,293: t15.2024.03.17 val PER: 0.0788
2026-01-13 23:22:34,293: t15.2024.05.10 val PER: 0.1218
2026-01-13 23:22:34,293: t15.2024.06.14 val PER: 0.1293
2026-01-13 23:22:34,293: t15.2024.07.19 val PER: 0.1760
2026-01-13 23:22:34,293: t15.2024.07.21 val PER: 0.0586
2026-01-13 23:22:34,293: t15.2024.07.28 val PER: 0.0949
2026-01-13 23:22:34,294: t15.2025.01.10 val PER: 0.2452
2026-01-13 23:22:34,294: t15.2025.01.12 val PER: 0.1078
2026-01-13 23:22:34,294: t15.2025.03.14 val PER: 0.3151
2026-01-13 23:22:34,294: t15.2025.03.16 val PER: 0.1374
2026-01-13 23:22:34,294: t15.2025.03.30 val PER: 0.2310
2026-01-13 23:22:34,294: t15.2025.04.13 val PER: 0.1826
2026-01-13 23:22:34,430: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_44500
2026-01-13 23:22:43,160: Train batch 44600: loss: 0.19 grad norm: 10.31 time: 0.053
2026-01-13 23:23:00,756: Train batch 44800: loss: 1.56 grad norm: 74.79 time: 0.048
2026-01-13 23:23:18,496: Train batch 45000: loss: 0.64 grad norm: 23.40 time: 0.070
2026-01-13 23:23:18,497: Running test after training batch: 45000
2026-01-13 23:23:18,634: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:23:23,895: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:23:23,951: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:23:37,281: Val batch 45000: PER (avg): 0.1094 CTC Loss (avg): 18.2357 WER(5gram): 16.88% (n=256) time: 18.784
2026-01-13 23:23:37,281: WER lens: avg_true_words=5.99 avg_pred_words=6.25 max_pred_words=13
2026-01-13 23:23:37,282: t15.2023.08.13 val PER: 0.0676
2026-01-13 23:23:37,282: t15.2023.08.18 val PER: 0.0780
2026-01-13 23:23:37,282: t15.2023.08.20 val PER: 0.0699
2026-01-13 23:23:37,282: t15.2023.08.25 val PER: 0.0678
2026-01-13 23:23:37,282: t15.2023.08.27 val PER: 0.1495
2026-01-13 23:23:37,282: t15.2023.09.01 val PER: 0.0446
2026-01-13 23:23:37,282: t15.2023.09.03 val PER: 0.1283
2026-01-13 23:23:37,282: t15.2023.09.24 val PER: 0.0898
2026-01-13 23:23:37,282: t15.2023.09.29 val PER: 0.1066
2026-01-13 23:23:37,283: t15.2023.10.01 val PER: 0.1427
2026-01-13 23:23:37,283: t15.2023.10.06 val PER: 0.0571
2026-01-13 23:23:37,283: t15.2023.10.08 val PER: 0.2111
2026-01-13 23:23:37,283: t15.2023.10.13 val PER: 0.1505
2026-01-13 23:23:37,283: t15.2023.10.15 val PER: 0.1061
2026-01-13 23:23:37,283: t15.2023.10.20 val PER: 0.1577
2026-01-13 23:23:37,283: t15.2023.10.22 val PER: 0.0880
2026-01-13 23:23:37,283: t15.2023.11.03 val PER: 0.1465
2026-01-13 23:23:37,283: t15.2023.11.04 val PER: 0.0307
2026-01-13 23:23:37,283: t15.2023.11.17 val PER: 0.0187
2026-01-13 23:23:37,283: t15.2023.11.19 val PER: 0.0160
2026-01-13 23:23:37,283: t15.2023.11.26 val PER: 0.0486
2026-01-13 23:23:37,283: t15.2023.12.03 val PER: 0.0567
2026-01-13 23:23:37,284: t15.2023.12.08 val PER: 0.0419
2026-01-13 23:23:37,284: t15.2023.12.10 val PER: 0.0460
2026-01-13 23:23:37,284: t15.2023.12.17 val PER: 0.0873
2026-01-13 23:23:37,284: t15.2023.12.29 val PER: 0.0810
2026-01-13 23:23:37,284: t15.2024.02.25 val PER: 0.0702
2026-01-13 23:23:37,284: t15.2024.03.08 val PER: 0.1792
2026-01-13 23:23:37,284: t15.2024.03.15 val PER: 0.1614
2026-01-13 23:23:37,284: t15.2024.03.17 val PER: 0.0844
2026-01-13 23:23:37,284: t15.2024.05.10 val PER: 0.1218
2026-01-13 23:23:37,284: t15.2024.06.14 val PER: 0.1278
2026-01-13 23:23:37,284: t15.2024.07.19 val PER: 0.1760
2026-01-13 23:23:37,284: t15.2024.07.21 val PER: 0.0572
2026-01-13 23:23:37,284: t15.2024.07.28 val PER: 0.0985
2026-01-13 23:23:37,284: t15.2025.01.10 val PER: 0.2507
2026-01-13 23:23:37,285: t15.2025.01.12 val PER: 0.1147
2026-01-13 23:23:37,285: t15.2025.03.14 val PER: 0.3033
2026-01-13 23:23:37,285: t15.2025.03.16 val PER: 0.1348
2026-01-13 23:23:37,285: t15.2025.03.30 val PER: 0.2276
2026-01-13 23:23:37,285: t15.2025.04.13 val PER: 0.1826
2026-01-13 23:23:37,421: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_45000
2026-01-13 23:23:55,140: Train batch 45200: loss: 0.81 grad norm: 30.83 time: 0.061
2026-01-13 23:24:12,371: Train batch 45400: loss: 0.78 grad norm: 31.60 time: 0.052
2026-01-13 23:24:21,241: Running test after training batch: 45500
2026-01-13 23:24:21,363: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:24:26,321: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:24:26,379: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost out
2026-01-13 23:24:39,522: Val batch 45500: PER (avg): 0.1084 CTC Loss (avg): 18.1257 WER(5gram): 17.01% (n=256) time: 18.281
2026-01-13 23:24:39,523: WER lens: avg_true_words=5.99 avg_pred_words=6.29 max_pred_words=13
2026-01-13 23:24:39,523: t15.2023.08.13 val PER: 0.0748
2026-01-13 23:24:39,523: t15.2023.08.18 val PER: 0.0763
2026-01-13 23:24:39,523: t15.2023.08.20 val PER: 0.0635
2026-01-13 23:24:39,524: t15.2023.08.25 val PER: 0.0617
2026-01-13 23:24:39,524: t15.2023.08.27 val PER: 0.1495
2026-01-13 23:24:39,524: t15.2023.09.01 val PER: 0.0406
2026-01-13 23:24:39,524: t15.2023.09.03 val PER: 0.1247
2026-01-13 23:24:39,524: t15.2023.09.24 val PER: 0.0947
2026-01-13 23:24:39,524: t15.2023.09.29 val PER: 0.0996
2026-01-13 23:24:39,524: t15.2023.10.01 val PER: 0.1354
2026-01-13 23:24:39,524: t15.2023.10.06 val PER: 0.0517
2026-01-13 23:24:39,524: t15.2023.10.08 val PER: 0.2070
2026-01-13 23:24:39,524: t15.2023.10.13 val PER: 0.1482
2026-01-13 23:24:39,525: t15.2023.10.15 val PER: 0.1068
2026-01-13 23:24:39,525: t15.2023.10.20 val PER: 0.1611
2026-01-13 23:24:39,525: t15.2023.10.22 val PER: 0.0869
2026-01-13 23:24:39,525: t15.2023.11.03 val PER: 0.1547
2026-01-13 23:24:39,525: t15.2023.11.04 val PER: 0.0273
2026-01-13 23:24:39,525: t15.2023.11.17 val PER: 0.0187
2026-01-13 23:24:39,525: t15.2023.11.19 val PER: 0.0160
2026-01-13 23:24:39,525: t15.2023.11.26 val PER: 0.0457
2026-01-13 23:24:39,525: t15.2023.12.03 val PER: 0.0588
2026-01-13 23:24:39,525: t15.2023.12.08 val PER: 0.0426
2026-01-13 23:24:39,525: t15.2023.12.10 val PER: 0.0407
2026-01-13 23:24:39,525: t15.2023.12.17 val PER: 0.0936
2026-01-13 23:24:39,525: t15.2023.12.29 val PER: 0.0741
2026-01-13 23:24:39,525: t15.2024.02.25 val PER: 0.0801
2026-01-13 23:24:39,526: t15.2024.03.08 val PER: 0.1963
2026-01-13 23:24:39,526: t15.2024.03.15 val PER: 0.1632
2026-01-13 23:24:39,526: t15.2024.03.17 val PER: 0.0865
2026-01-13 23:24:39,526: t15.2024.05.10 val PER: 0.1129
2026-01-13 23:24:39,526: t15.2024.06.14 val PER: 0.1120
2026-01-13 23:24:39,526: t15.2024.07.19 val PER: 0.1694
2026-01-13 23:24:39,526: t15.2024.07.21 val PER: 0.0566
2026-01-13 23:24:39,526: t15.2024.07.28 val PER: 0.0985
2026-01-13 23:24:39,526: t15.2025.01.10 val PER: 0.2507
2026-01-13 23:24:39,526: t15.2025.01.12 val PER: 0.1024
2026-01-13 23:24:39,526: t15.2025.03.14 val PER: 0.3136
2026-01-13 23:24:39,526: t15.2025.03.16 val PER: 0.1361
2026-01-13 23:24:39,526: t15.2025.03.30 val PER: 0.2345
2026-01-13 23:24:39,526: t15.2025.04.13 val PER: 0.1897
2026-01-13 23:24:39,665: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_45500
2026-01-13 23:24:48,158: Train batch 45600: loss: 0.26 grad norm: 12.13 time: 0.048
2026-01-13 23:25:05,528: Train batch 45800: loss: 0.28 grad norm: 12.40 time: 0.059
2026-01-13 23:25:23,414: Train batch 46000: loss: 0.50 grad norm: 15.04 time: 0.059
2026-01-13 23:25:23,414: Running test after training batch: 46000
2026-01-13 23:25:23,591: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:25:28,389: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:25:28,446: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:25:41,551: Val batch 46000: PER (avg): 0.1098 CTC Loss (avg): 17.9204 WER(5gram): 19.69% (n=256) time: 18.136
2026-01-13 23:25:41,551: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=14
2026-01-13 23:25:41,551: t15.2023.08.13 val PER: 0.0717
2026-01-13 23:25:41,551: t15.2023.08.18 val PER: 0.0855
2026-01-13 23:25:41,551: t15.2023.08.20 val PER: 0.0651
2026-01-13 23:25:41,552: t15.2023.08.25 val PER: 0.0678
2026-01-13 23:25:41,552: t15.2023.08.27 val PER: 0.1367
2026-01-13 23:25:41,552: t15.2023.09.01 val PER: 0.0471
2026-01-13 23:25:41,552: t15.2023.09.03 val PER: 0.1306
2026-01-13 23:25:41,552: t15.2023.09.24 val PER: 0.0947
2026-01-13 23:25:41,552: t15.2023.09.29 val PER: 0.0970
2026-01-13 23:25:41,552: t15.2023.10.01 val PER: 0.1361
2026-01-13 23:25:41,552: t15.2023.10.06 val PER: 0.0560
2026-01-13 23:25:41,552: t15.2023.10.08 val PER: 0.2138
2026-01-13 23:25:41,552: t15.2023.10.13 val PER: 0.1497
2026-01-13 23:25:41,553: t15.2023.10.15 val PER: 0.1101
2026-01-13 23:25:41,553: t15.2023.10.20 val PER: 0.1510
2026-01-13 23:25:41,553: t15.2023.10.22 val PER: 0.0857
2026-01-13 23:25:41,553: t15.2023.11.03 val PER: 0.1533
2026-01-13 23:25:41,553: t15.2023.11.04 val PER: 0.0273
2026-01-13 23:25:41,553: t15.2023.11.17 val PER: 0.0171
2026-01-13 23:25:41,553: t15.2023.11.19 val PER: 0.0180
2026-01-13 23:25:41,553: t15.2023.11.26 val PER: 0.0543
2026-01-13 23:25:41,553: t15.2023.12.03 val PER: 0.0525
2026-01-13 23:25:41,553: t15.2023.12.08 val PER: 0.0406
2026-01-13 23:25:41,553: t15.2023.12.10 val PER: 0.0499
2026-01-13 23:25:41,553: t15.2023.12.17 val PER: 0.0904
2026-01-13 23:25:41,554: t15.2023.12.29 val PER: 0.0837
2026-01-13 23:25:41,554: t15.2024.02.25 val PER: 0.0758
2026-01-13 23:25:41,554: t15.2024.03.08 val PER: 0.1807
2026-01-13 23:25:41,554: t15.2024.03.15 val PER: 0.1588
2026-01-13 23:25:41,554: t15.2024.03.17 val PER: 0.0844
2026-01-13 23:25:41,554: t15.2024.05.10 val PER: 0.1293
2026-01-13 23:25:41,554: t15.2024.06.14 val PER: 0.1136
2026-01-13 23:25:41,554: t15.2024.07.19 val PER: 0.1727
2026-01-13 23:25:41,554: t15.2024.07.21 val PER: 0.0600
2026-01-13 23:25:41,554: t15.2024.07.28 val PER: 0.1015
2026-01-13 23:25:41,554: t15.2025.01.10 val PER: 0.2686
2026-01-13 23:25:41,554: t15.2025.01.12 val PER: 0.1085
2026-01-13 23:25:41,554: t15.2025.03.14 val PER: 0.2959
2026-01-13 23:25:41,554: t15.2025.03.16 val PER: 0.1492
2026-01-13 23:25:41,554: t15.2025.03.30 val PER: 0.2253
2026-01-13 23:25:41,554: t15.2025.04.13 val PER: 0.1883
2026-01-13 23:25:41,694: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_46000
2026-01-13 23:25:58,819: Train batch 46200: loss: 0.44 grad norm: 18.82 time: 0.051
2026-01-13 23:26:16,366: Train batch 46400: loss: 0.68 grad norm: 22.50 time: 0.055
2026-01-13 23:26:25,025: Running test after training batch: 46500
2026-01-13 23:26:25,137: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:26:29,957: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:26:30,037: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cent
2026-01-13 23:26:45,578: Val batch 46500: PER (avg): 0.1099 CTC Loss (avg): 17.9712 WER(5gram): 16.88% (n=256) time: 20.553
2026-01-13 23:26:45,579: WER lens: avg_true_words=5.99 avg_pred_words=6.27 max_pred_words=13
2026-01-13 23:26:45,579: t15.2023.08.13 val PER: 0.0800
2026-01-13 23:26:45,579: t15.2023.08.18 val PER: 0.0788
2026-01-13 23:26:45,579: t15.2023.08.20 val PER: 0.0659
2026-01-13 23:26:45,579: t15.2023.08.25 val PER: 0.0663
2026-01-13 23:26:45,579: t15.2023.08.27 val PER: 0.1495
2026-01-13 23:26:45,579: t15.2023.09.01 val PER: 0.0463
2026-01-13 23:26:45,580: t15.2023.09.03 val PER: 0.1235
2026-01-13 23:26:45,580: t15.2023.09.24 val PER: 0.0850
2026-01-13 23:26:45,580: t15.2023.09.29 val PER: 0.1059
2026-01-13 23:26:45,580: t15.2023.10.01 val PER: 0.1413
2026-01-13 23:26:45,580: t15.2023.10.06 val PER: 0.0592
2026-01-13 23:26:45,580: t15.2023.10.08 val PER: 0.1949
2026-01-13 23:26:45,580: t15.2023.10.13 val PER: 0.1536
2026-01-13 23:26:45,580: t15.2023.10.15 val PER: 0.1002
2026-01-13 23:26:45,580: t15.2023.10.20 val PER: 0.1611
2026-01-13 23:26:45,580: t15.2023.10.22 val PER: 0.0880
2026-01-13 23:26:45,580: t15.2023.11.03 val PER: 0.1560
2026-01-13 23:26:45,580: t15.2023.11.04 val PER: 0.0239
2026-01-13 23:26:45,580: t15.2023.11.17 val PER: 0.0202
2026-01-13 23:26:45,580: t15.2023.11.19 val PER: 0.0220
2026-01-13 23:26:45,580: t15.2023.11.26 val PER: 0.0500
2026-01-13 23:26:45,580: t15.2023.12.03 val PER: 0.0515
2026-01-13 23:26:45,581: t15.2023.12.08 val PER: 0.0386
2026-01-13 23:26:45,581: t15.2023.12.10 val PER: 0.0460
2026-01-13 23:26:45,581: t15.2023.12.17 val PER: 0.0946
2026-01-13 23:26:45,581: t15.2023.12.29 val PER: 0.0789
2026-01-13 23:26:45,581: t15.2024.02.25 val PER: 0.0744
2026-01-13 23:26:45,581: t15.2024.03.08 val PER: 0.1835
2026-01-13 23:26:45,581: t15.2024.03.15 val PER: 0.1614
2026-01-13 23:26:45,581: t15.2024.03.17 val PER: 0.0858
2026-01-13 23:26:45,581: t15.2024.05.10 val PER: 0.1174
2026-01-13 23:26:45,581: t15.2024.06.14 val PER: 0.1230
2026-01-13 23:26:45,581: t15.2024.07.19 val PER: 0.1760
2026-01-13 23:26:45,581: t15.2024.07.21 val PER: 0.0634
2026-01-13 23:26:45,581: t15.2024.07.28 val PER: 0.0985
2026-01-13 23:26:45,582: t15.2025.01.10 val PER: 0.2576
2026-01-13 23:26:45,582: t15.2025.01.12 val PER: 0.1085
2026-01-13 23:26:45,582: t15.2025.03.14 val PER: 0.2973
2026-01-13 23:26:45,582: t15.2025.03.16 val PER: 0.1453
2026-01-13 23:26:45,582: t15.2025.03.30 val PER: 0.2299
2026-01-13 23:26:45,583: t15.2025.04.13 val PER: 0.1969
2026-01-13 23:26:45,718: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_46500
2026-01-13 23:26:54,425: Train batch 46600: loss: 0.67 grad norm: 21.17 time: 0.083
2026-01-13 23:27:11,746: Train batch 46800: loss: 0.80 grad norm: 23.09 time: 0.060
2026-01-13 23:27:29,150: Train batch 47000: loss: 0.51 grad norm: 17.02 time: 0.046
2026-01-13 23:27:29,150: Running test after training batch: 47000
2026-01-13 23:27:29,301: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:27:34,147: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:27:34,205: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost out
2026-01-13 23:27:48,855: Val batch 47000: PER (avg): 0.1097 CTC Loss (avg): 18.1125 WER(5gram): 17.67% (n=256) time: 19.705
2026-01-13 23:27:48,856: WER lens: avg_true_words=5.99 avg_pred_words=6.32 max_pred_words=13
2026-01-13 23:27:48,856: t15.2023.08.13 val PER: 0.0769
2026-01-13 23:27:48,856: t15.2023.08.18 val PER: 0.0796
2026-01-13 23:27:48,856: t15.2023.08.20 val PER: 0.0604
2026-01-13 23:27:48,857: t15.2023.08.25 val PER: 0.0648
2026-01-13 23:27:48,857: t15.2023.08.27 val PER: 0.1447
2026-01-13 23:27:48,857: t15.2023.09.01 val PER: 0.0438
2026-01-13 23:27:48,857: t15.2023.09.03 val PER: 0.1283
2026-01-13 23:27:48,857: t15.2023.09.24 val PER: 0.0874
2026-01-13 23:27:48,857: t15.2023.09.29 val PER: 0.1040
2026-01-13 23:27:48,857: t15.2023.10.01 val PER: 0.1361
2026-01-13 23:27:48,857: t15.2023.10.06 val PER: 0.0603
2026-01-13 23:27:48,857: t15.2023.10.08 val PER: 0.1894
2026-01-13 23:27:48,857: t15.2023.10.13 val PER: 0.1521
2026-01-13 23:27:48,857: t15.2023.10.15 val PER: 0.1061
2026-01-13 23:27:48,857: t15.2023.10.20 val PER: 0.1611
2026-01-13 23:27:48,858: t15.2023.10.22 val PER: 0.0813
2026-01-13 23:27:48,858: t15.2023.11.03 val PER: 0.1479
2026-01-13 23:27:48,858: t15.2023.11.04 val PER: 0.0171
2026-01-13 23:27:48,858: t15.2023.11.17 val PER: 0.0187
2026-01-13 23:27:48,858: t15.2023.11.19 val PER: 0.0200
2026-01-13 23:27:48,858: t15.2023.11.26 val PER: 0.0572
2026-01-13 23:27:48,858: t15.2023.12.03 val PER: 0.0588
2026-01-13 23:27:48,858: t15.2023.12.08 val PER: 0.0379
2026-01-13 23:27:48,858: t15.2023.12.10 val PER: 0.0407
2026-01-13 23:27:48,858: t15.2023.12.17 val PER: 0.0936
2026-01-13 23:27:48,858: t15.2023.12.29 val PER: 0.0817
2026-01-13 23:27:48,858: t15.2024.02.25 val PER: 0.0801
2026-01-13 23:27:48,858: t15.2024.03.08 val PER: 0.1878
2026-01-13 23:27:48,858: t15.2024.03.15 val PER: 0.1545
2026-01-13 23:27:48,858: t15.2024.03.17 val PER: 0.0886
2026-01-13 23:27:48,858: t15.2024.05.10 val PER: 0.1456
2026-01-13 23:27:48,858: t15.2024.06.14 val PER: 0.1215
2026-01-13 23:27:48,859: t15.2024.07.19 val PER: 0.1839
2026-01-13 23:27:48,859: t15.2024.07.21 val PER: 0.0593
2026-01-13 23:27:48,859: t15.2024.07.28 val PER: 0.0926
2026-01-13 23:27:48,859: t15.2025.01.10 val PER: 0.2727
2026-01-13 23:27:48,859: t15.2025.01.12 val PER: 0.1078
2026-01-13 23:27:48,859: t15.2025.03.14 val PER: 0.3047
2026-01-13 23:27:48,859: t15.2025.03.16 val PER: 0.1322
2026-01-13 23:27:48,859: t15.2025.03.30 val PER: 0.2368
2026-01-13 23:27:48,859: t15.2025.04.13 val PER: 0.1840
2026-01-13 23:27:48,997: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_47000
2026-01-13 23:28:06,317: Train batch 47200: loss: 0.30 grad norm: 15.12 time: 0.046
2026-01-13 23:28:23,673: Train batch 47400: loss: 0.44 grad norm: 24.83 time: 0.067
2026-01-13 23:28:32,277: Running test after training batch: 47500
2026-01-13 23:28:32,367: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:28:37,385: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:28:37,442: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost out
2026-01-13 23:28:52,236: Val batch 47500: PER (avg): 0.1088 CTC Loss (avg): 18.0861 WER(5gram): 18.51% (n=256) time: 19.959
2026-01-13 23:28:52,237: WER lens: avg_true_words=5.99 avg_pred_words=6.32 max_pred_words=13
2026-01-13 23:28:52,237: t15.2023.08.13 val PER: 0.0842
2026-01-13 23:28:52,237: t15.2023.08.18 val PER: 0.0780
2026-01-13 23:28:52,237: t15.2023.08.20 val PER: 0.0699
2026-01-13 23:28:52,237: t15.2023.08.25 val PER: 0.0723
2026-01-13 23:28:52,238: t15.2023.08.27 val PER: 0.1672
2026-01-13 23:28:52,238: t15.2023.09.01 val PER: 0.0422
2026-01-13 23:28:52,238: t15.2023.09.03 val PER: 0.1330
2026-01-13 23:28:52,238: t15.2023.09.24 val PER: 0.0910
2026-01-13 23:28:52,238: t15.2023.09.29 val PER: 0.1034
2026-01-13 23:28:52,238: t15.2023.10.01 val PER: 0.1427
2026-01-13 23:28:52,238: t15.2023.10.06 val PER: 0.0506
2026-01-13 23:28:52,238: t15.2023.10.08 val PER: 0.1962
2026-01-13 23:28:52,239: t15.2023.10.13 val PER: 0.1528
2026-01-13 23:28:52,239: t15.2023.10.15 val PER: 0.1160
2026-01-13 23:28:52,239: t15.2023.10.20 val PER: 0.1443
2026-01-13 23:28:52,239: t15.2023.10.22 val PER: 0.0891
2026-01-13 23:28:52,239: t15.2023.11.03 val PER: 0.1479
2026-01-13 23:28:52,239: t15.2023.11.04 val PER: 0.0171
2026-01-13 23:28:52,239: t15.2023.11.17 val PER: 0.0171
2026-01-13 23:28:52,239: t15.2023.11.19 val PER: 0.0200
2026-01-13 23:28:52,239: t15.2023.11.26 val PER: 0.0500
2026-01-13 23:28:52,240: t15.2023.12.03 val PER: 0.0536
2026-01-13 23:28:52,240: t15.2023.12.08 val PER: 0.0386
2026-01-13 23:28:52,240: t15.2023.12.10 val PER: 0.0381
2026-01-13 23:28:52,240: t15.2023.12.17 val PER: 0.0915
2026-01-13 23:28:52,240: t15.2023.12.29 val PER: 0.0769
2026-01-13 23:28:52,240: t15.2024.02.25 val PER: 0.0772
2026-01-13 23:28:52,240: t15.2024.03.08 val PER: 0.1721
2026-01-13 23:28:52,240: t15.2024.03.15 val PER: 0.1507
2026-01-13 23:28:52,241: t15.2024.03.17 val PER: 0.0851
2026-01-13 23:28:52,241: t15.2024.05.10 val PER: 0.1382
2026-01-13 23:28:52,241: t15.2024.06.14 val PER: 0.1246
2026-01-13 23:28:52,241: t15.2024.07.19 val PER: 0.1714
2026-01-13 23:28:52,241: t15.2024.07.21 val PER: 0.0545
2026-01-13 23:28:52,241: t15.2024.07.28 val PER: 0.0956
2026-01-13 23:28:52,241: t15.2025.01.10 val PER: 0.2534
2026-01-13 23:28:52,241: t15.2025.01.12 val PER: 0.1008
2026-01-13 23:28:52,241: t15.2025.03.14 val PER: 0.3018
2026-01-13 23:28:52,241: t15.2025.03.16 val PER: 0.1361
2026-01-13 23:28:52,242: t15.2025.03.30 val PER: 0.2333
2026-01-13 23:28:52,242: t15.2025.04.13 val PER: 0.1840
2026-01-13 23:28:52,375: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_47500
2026-01-13 23:29:01,207: Train batch 47600: loss: 0.63 grad norm: 27.24 time: 0.081
2026-01-13 23:29:18,476: Train batch 47800: loss: 0.30 grad norm: 16.04 time: 0.056
2026-01-13 23:29:35,954: Train batch 48000: loss: 0.26 grad norm: 13.28 time: 0.047
2026-01-13 23:29:35,954: Running test after training batch: 48000
2026-01-13 23:29:36,113: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:29:41,221: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:29:41,279: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cents
2026-01-13 23:29:55,615: Val batch 48000: PER (avg): 0.1072 CTC Loss (avg): 17.6193 WER(5gram): 17.80% (n=256) time: 19.660
2026-01-13 23:29:55,616: WER lens: avg_true_words=5.99 avg_pred_words=6.32 max_pred_words=13
2026-01-13 23:29:55,616: t15.2023.08.13 val PER: 0.0800
2026-01-13 23:29:55,616: t15.2023.08.18 val PER: 0.0813
2026-01-13 23:29:55,616: t15.2023.08.20 val PER: 0.0635
2026-01-13 23:29:55,616: t15.2023.08.25 val PER: 0.0633
2026-01-13 23:29:55,616: t15.2023.08.27 val PER: 0.1383
2026-01-13 23:29:55,616: t15.2023.09.01 val PER: 0.0487
2026-01-13 23:29:55,616: t15.2023.09.03 val PER: 0.1259
2026-01-13 23:29:55,617: t15.2023.09.24 val PER: 0.0874
2026-01-13 23:29:55,617: t15.2023.09.29 val PER: 0.1021
2026-01-13 23:29:55,617: t15.2023.10.01 val PER: 0.1341
2026-01-13 23:29:55,617: t15.2023.10.06 val PER: 0.0603
2026-01-13 23:29:55,617: t15.2023.10.08 val PER: 0.1894
2026-01-13 23:29:55,617: t15.2023.10.13 val PER: 0.1583
2026-01-13 23:29:55,617: t15.2023.10.15 val PER: 0.1061
2026-01-13 23:29:55,617: t15.2023.10.20 val PER: 0.1611
2026-01-13 23:29:55,617: t15.2023.10.22 val PER: 0.0869
2026-01-13 23:29:55,617: t15.2023.11.03 val PER: 0.1479
2026-01-13 23:29:55,617: t15.2023.11.04 val PER: 0.0205
2026-01-13 23:29:55,617: t15.2023.11.17 val PER: 0.0140
2026-01-13 23:29:55,617: t15.2023.11.19 val PER: 0.0240
2026-01-13 23:29:55,618: t15.2023.11.26 val PER: 0.0464
2026-01-13 23:29:55,618: t15.2023.12.03 val PER: 0.0504
2026-01-13 23:29:55,618: t15.2023.12.08 val PER: 0.0373
2026-01-13 23:29:55,618: t15.2023.12.10 val PER: 0.0368
2026-01-13 23:29:55,618: t15.2023.12.17 val PER: 0.0925
2026-01-13 23:29:55,618: t15.2023.12.29 val PER: 0.0707
2026-01-13 23:29:55,618: t15.2024.02.25 val PER: 0.0702
2026-01-13 23:29:55,618: t15.2024.03.08 val PER: 0.1721
2026-01-13 23:29:55,618: t15.2024.03.15 val PER: 0.1513
2026-01-13 23:29:55,618: t15.2024.03.17 val PER: 0.0795
2026-01-13 23:29:55,618: t15.2024.05.10 val PER: 0.1352
2026-01-13 23:29:55,618: t15.2024.06.14 val PER: 0.1278
2026-01-13 23:29:55,618: t15.2024.07.19 val PER: 0.1747
2026-01-13 23:29:55,618: t15.2024.07.21 val PER: 0.0566
2026-01-13 23:29:55,619: t15.2024.07.28 val PER: 0.0993
2026-01-13 23:29:55,619: t15.2025.01.10 val PER: 0.2590
2026-01-13 23:29:55,619: t15.2025.01.12 val PER: 0.0947
2026-01-13 23:29:55,619: t15.2025.03.14 val PER: 0.3003
2026-01-13 23:29:55,619: t15.2025.03.16 val PER: 0.1335
2026-01-13 23:29:55,619: t15.2025.03.30 val PER: 0.2379
2026-01-13 23:29:55,619: t15.2025.04.13 val PER: 0.1869
2026-01-13 23:29:55,755: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_48000
2026-01-13 23:30:13,154: Train batch 48200: loss: 0.06 grad norm: 5.53 time: 0.046
2026-01-13 23:30:30,852: Train batch 48400: loss: 0.54 grad norm: 24.87 time: 0.075
2026-01-13 23:30:39,519: Running test after training batch: 48500
2026-01-13 23:30:39,616: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:30:44,442: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:30:44,511: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost out
2026-01-13 23:30:59,260: Val batch 48500: PER (avg): 0.1071 CTC Loss (avg): 17.9139 WER(5gram): 17.67% (n=256) time: 19.741
2026-01-13 23:30:59,261: WER lens: avg_true_words=5.99 avg_pred_words=6.29 max_pred_words=13
2026-01-13 23:30:59,261: t15.2023.08.13 val PER: 0.0821
2026-01-13 23:30:59,261: t15.2023.08.18 val PER: 0.0738
2026-01-13 23:30:59,261: t15.2023.08.20 val PER: 0.0659
2026-01-13 23:30:59,261: t15.2023.08.25 val PER: 0.0768
2026-01-13 23:30:59,261: t15.2023.08.27 val PER: 0.1463
2026-01-13 23:30:59,261: t15.2023.09.01 val PER: 0.0463
2026-01-13 23:30:59,262: t15.2023.09.03 val PER: 0.1283
2026-01-13 23:30:59,262: t15.2023.09.24 val PER: 0.0971
2026-01-13 23:30:59,262: t15.2023.09.29 val PER: 0.1078
2026-01-13 23:30:59,262: t15.2023.10.01 val PER: 0.1427
2026-01-13 23:30:59,262: t15.2023.10.06 val PER: 0.0549
2026-01-13 23:30:59,262: t15.2023.10.08 val PER: 0.1908
2026-01-13 23:30:59,262: t15.2023.10.13 val PER: 0.1451
2026-01-13 23:30:59,262: t15.2023.10.15 val PER: 0.1094
2026-01-13 23:30:59,262: t15.2023.10.20 val PER: 0.1611
2026-01-13 23:30:59,263: t15.2023.10.22 val PER: 0.0757
2026-01-13 23:30:59,263: t15.2023.11.03 val PER: 0.1493
2026-01-13 23:30:59,263: t15.2023.11.04 val PER: 0.0205
2026-01-13 23:30:59,263: t15.2023.11.17 val PER: 0.0218
2026-01-13 23:30:59,263: t15.2023.11.19 val PER: 0.0180
2026-01-13 23:30:59,263: t15.2023.11.26 val PER: 0.0514
2026-01-13 23:30:59,263: t15.2023.12.03 val PER: 0.0504
2026-01-13 23:30:59,263: t15.2023.12.08 val PER: 0.0386
2026-01-13 23:30:59,263: t15.2023.12.10 val PER: 0.0329
2026-01-13 23:30:59,263: t15.2023.12.17 val PER: 0.0811
2026-01-13 23:30:59,263: t15.2023.12.29 val PER: 0.0700
2026-01-13 23:30:59,263: t15.2024.02.25 val PER: 0.0730
2026-01-13 23:30:59,263: t15.2024.03.08 val PER: 0.1878
2026-01-13 23:30:59,263: t15.2024.03.15 val PER: 0.1557
2026-01-13 23:30:59,263: t15.2024.03.17 val PER: 0.0781
2026-01-13 23:30:59,263: t15.2024.05.10 val PER: 0.1100
2026-01-13 23:30:59,264: t15.2024.06.14 val PER: 0.1262
2026-01-13 23:30:59,264: t15.2024.07.19 val PER: 0.1668
2026-01-13 23:30:59,264: t15.2024.07.21 val PER: 0.0572
2026-01-13 23:30:59,264: t15.2024.07.28 val PER: 0.0993
2026-01-13 23:30:59,264: t15.2025.01.10 val PER: 0.2521
2026-01-13 23:30:59,264: t15.2025.01.12 val PER: 0.0978
2026-01-13 23:30:59,264: t15.2025.03.14 val PER: 0.3077
2026-01-13 23:30:59,264: t15.2025.03.16 val PER: 0.1335
2026-01-13 23:30:59,264: t15.2025.03.30 val PER: 0.2391
2026-01-13 23:30:59,264: t15.2025.04.13 val PER: 0.1712
2026-01-13 23:30:59,408: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_48500
2026-01-13 23:31:08,470: Train batch 48600: loss: 1.30 grad norm: 32.65 time: 0.078
2026-01-13 23:31:26,066: Train batch 48800: loss: 0.90 grad norm: 29.16 time: 0.061
2026-01-13 23:31:43,836: Train batch 49000: loss: 0.60 grad norm: 23.62 time: 0.056
2026-01-13 23:31:43,836: Running test after training batch: 49000
2026-01-13 23:31:43,976: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:31:48,722: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:31:48,778: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:32:02,398: Val batch 49000: PER (avg): 0.1084 CTC Loss (avg): 18.3434 WER(5gram): 18.90% (n=256) time: 18.561
2026-01-13 23:32:02,398: WER lens: avg_true_words=5.99 avg_pred_words=6.32 max_pred_words=13
2026-01-13 23:32:02,398: t15.2023.08.13 val PER: 0.0769
2026-01-13 23:32:02,398: t15.2023.08.18 val PER: 0.0738
2026-01-13 23:32:02,398: t15.2023.08.20 val PER: 0.0620
2026-01-13 23:32:02,398: t15.2023.08.25 val PER: 0.0723
2026-01-13 23:32:02,399: t15.2023.08.27 val PER: 0.1576
2026-01-13 23:32:02,399: t15.2023.09.01 val PER: 0.0446
2026-01-13 23:32:02,399: t15.2023.09.03 val PER: 0.1283
2026-01-13 23:32:02,399: t15.2023.09.24 val PER: 0.0874
2026-01-13 23:32:02,399: t15.2023.09.29 val PER: 0.1104
2026-01-13 23:32:02,399: t15.2023.10.01 val PER: 0.1321
2026-01-13 23:32:02,399: t15.2023.10.06 val PER: 0.0517
2026-01-13 23:32:02,400: t15.2023.10.08 val PER: 0.2097
2026-01-13 23:32:02,400: t15.2023.10.13 val PER: 0.1427
2026-01-13 23:32:02,400: t15.2023.10.15 val PER: 0.1074
2026-01-13 23:32:02,400: t15.2023.10.20 val PER: 0.1477
2026-01-13 23:32:02,400: t15.2023.10.22 val PER: 0.0846
2026-01-13 23:32:02,400: t15.2023.11.03 val PER: 0.1520
2026-01-13 23:32:02,400: t15.2023.11.04 val PER: 0.0307
2026-01-13 23:32:02,400: t15.2023.11.17 val PER: 0.0171
2026-01-13 23:32:02,400: t15.2023.11.19 val PER: 0.0180
2026-01-13 23:32:02,400: t15.2023.11.26 val PER: 0.0486
2026-01-13 23:32:02,400: t15.2023.12.03 val PER: 0.0546
2026-01-13 23:32:02,400: t15.2023.12.08 val PER: 0.0406
2026-01-13 23:32:02,400: t15.2023.12.10 val PER: 0.0342
2026-01-13 23:32:02,400: t15.2023.12.17 val PER: 0.0915
2026-01-13 23:32:02,401: t15.2023.12.29 val PER: 0.0830
2026-01-13 23:32:02,401: t15.2024.02.25 val PER: 0.0730
2026-01-13 23:32:02,401: t15.2024.03.08 val PER: 0.1764
2026-01-13 23:32:02,401: t15.2024.03.15 val PER: 0.1470
2026-01-13 23:32:02,401: t15.2024.03.17 val PER: 0.0823
2026-01-13 23:32:02,401: t15.2024.05.10 val PER: 0.1293
2026-01-13 23:32:02,401: t15.2024.06.14 val PER: 0.1262
2026-01-13 23:32:02,401: t15.2024.07.19 val PER: 0.1694
2026-01-13 23:32:02,402: t15.2024.07.21 val PER: 0.0614
2026-01-13 23:32:02,402: t15.2024.07.28 val PER: 0.1015
2026-01-13 23:32:02,402: t15.2025.01.10 val PER: 0.2686
2026-01-13 23:32:02,402: t15.2025.01.12 val PER: 0.1078
2026-01-13 23:32:02,402: t15.2025.03.14 val PER: 0.2959
2026-01-13 23:32:02,402: t15.2025.03.16 val PER: 0.1322
2026-01-13 23:32:02,402: t15.2025.03.30 val PER: 0.2368
2026-01-13 23:32:02,402: t15.2025.04.13 val PER: 0.1883
2026-01-13 23:32:02,552: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_49000
2026-01-13 23:32:20,395: Train batch 49200: loss: 0.24 grad norm: 19.33 time: 0.056
2026-01-13 23:32:37,860: Train batch 49400: loss: 0.44 grad norm: 17.21 time: 0.062
2026-01-13 23:32:46,656: Running test after training batch: 49500
2026-01-13 23:32:46,751: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:32:51,522: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:32:51,576: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-13 23:33:05,292: Val batch 49500: PER (avg): 0.1058 CTC Loss (avg): 17.8884 WER(5gram): 17.14% (n=256) time: 18.636
2026-01-13 23:33:05,293: WER lens: avg_true_words=5.99 avg_pred_words=6.30 max_pred_words=13
2026-01-13 23:33:05,293: t15.2023.08.13 val PER: 0.0686
2026-01-13 23:33:05,293: t15.2023.08.18 val PER: 0.0721
2026-01-13 23:33:05,293: t15.2023.08.20 val PER: 0.0540
2026-01-13 23:33:05,293: t15.2023.08.25 val PER: 0.0738
2026-01-13 23:33:05,293: t15.2023.08.27 val PER: 0.1334
2026-01-13 23:33:05,293: t15.2023.09.01 val PER: 0.0479
2026-01-13 23:33:05,294: t15.2023.09.03 val PER: 0.1295
2026-01-13 23:33:05,294: t15.2023.09.24 val PER: 0.0922
2026-01-13 23:33:05,294: t15.2023.09.29 val PER: 0.1015
2026-01-13 23:33:05,294: t15.2023.10.01 val PER: 0.1367
2026-01-13 23:33:05,294: t15.2023.10.06 val PER: 0.0527
2026-01-13 23:33:05,294: t15.2023.10.08 val PER: 0.1935
2026-01-13 23:33:05,294: t15.2023.10.13 val PER: 0.1474
2026-01-13 23:33:05,294: t15.2023.10.15 val PER: 0.1081
2026-01-13 23:33:05,294: t15.2023.10.20 val PER: 0.1544
2026-01-13 23:33:05,294: t15.2023.10.22 val PER: 0.0813
2026-01-13 23:33:05,294: t15.2023.11.03 val PER: 0.1472
2026-01-13 23:33:05,294: t15.2023.11.04 val PER: 0.0171
2026-01-13 23:33:05,294: t15.2023.11.17 val PER: 0.0140
2026-01-13 23:33:05,294: t15.2023.11.19 val PER: 0.0200
2026-01-13 23:33:05,294: t15.2023.11.26 val PER: 0.0464
2026-01-13 23:33:05,294: t15.2023.12.03 val PER: 0.0588
2026-01-13 23:33:05,295: t15.2023.12.08 val PER: 0.0340
2026-01-13 23:33:05,295: t15.2023.12.10 val PER: 0.0289
2026-01-13 23:33:05,295: t15.2023.12.17 val PER: 0.0863
2026-01-13 23:33:05,295: t15.2023.12.29 val PER: 0.0741
2026-01-13 23:33:05,295: t15.2024.02.25 val PER: 0.0730
2026-01-13 23:33:05,295: t15.2024.03.08 val PER: 0.1650
2026-01-13 23:33:05,295: t15.2024.03.15 val PER: 0.1563
2026-01-13 23:33:05,295: t15.2024.03.17 val PER: 0.0802
2026-01-13 23:33:05,295: t15.2024.05.10 val PER: 0.1189
2026-01-13 23:33:05,295: t15.2024.06.14 val PER: 0.1215
2026-01-13 23:33:05,295: t15.2024.07.19 val PER: 0.1707
2026-01-13 23:33:05,295: t15.2024.07.21 val PER: 0.0593
2026-01-13 23:33:05,295: t15.2024.07.28 val PER: 0.0993
2026-01-13 23:33:05,295: t15.2025.01.10 val PER: 0.2631
2026-01-13 23:33:05,296: t15.2025.01.12 val PER: 0.1055
2026-01-13 23:33:05,296: t15.2025.03.14 val PER: 0.3033
2026-01-13 23:33:05,296: t15.2025.03.16 val PER: 0.1361
2026-01-13 23:33:05,296: t15.2025.03.30 val PER: 0.2322
2026-01-13 23:33:05,296: t15.2025.04.13 val PER: 0.1698
2026-01-13 23:33:05,434: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_49500
2026-01-13 23:33:14,051: Train batch 49600: loss: 0.24 grad norm: 15.76 time: 0.066
2026-01-13 23:33:31,306: Train batch 49800: loss: 0.76 grad norm: 21.62 time: 0.049
2026-01-13 23:33:48,688: Train batch 50000: loss: 0.36 grad norm: 12.16 time: 0.060
2026-01-13 23:33:48,688: Running test after training batch: 50000
2026-01-13 23:33:48,786: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:33:53,778: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:33:53,833: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:34:07,398: Val batch 50000: PER (avg): 0.1073 CTC Loss (avg): 18.3168 WER(5gram): 18.64% (n=256) time: 18.709
2026-01-13 23:34:07,398: WER lens: avg_true_words=5.99 avg_pred_words=6.29 max_pred_words=13
2026-01-13 23:34:07,399: t15.2023.08.13 val PER: 0.0738
2026-01-13 23:34:07,399: t15.2023.08.18 val PER: 0.0721
2026-01-13 23:34:07,399: t15.2023.08.20 val PER: 0.0627
2026-01-13 23:34:07,399: t15.2023.08.25 val PER: 0.0678
2026-01-13 23:34:07,399: t15.2023.08.27 val PER: 0.1479
2026-01-13 23:34:07,399: t15.2023.09.01 val PER: 0.0438
2026-01-13 23:34:07,399: t15.2023.09.03 val PER: 0.1342
2026-01-13 23:34:07,399: t15.2023.09.24 val PER: 0.0910
2026-01-13 23:34:07,399: t15.2023.09.29 val PER: 0.1015
2026-01-13 23:34:07,399: t15.2023.10.01 val PER: 0.1413
2026-01-13 23:34:07,400: t15.2023.10.06 val PER: 0.0527
2026-01-13 23:34:07,400: t15.2023.10.08 val PER: 0.1989
2026-01-13 23:34:07,400: t15.2023.10.13 val PER: 0.1420
2026-01-13 23:34:07,400: t15.2023.10.15 val PER: 0.1088
2026-01-13 23:34:07,400: t15.2023.10.20 val PER: 0.1611
2026-01-13 23:34:07,400: t15.2023.10.22 val PER: 0.0802
2026-01-13 23:34:07,400: t15.2023.11.03 val PER: 0.1472
2026-01-13 23:34:07,400: t15.2023.11.04 val PER: 0.0239
2026-01-13 23:34:07,401: t15.2023.11.17 val PER: 0.0156
2026-01-13 23:34:07,401: t15.2023.11.19 val PER: 0.0160
2026-01-13 23:34:07,401: t15.2023.11.26 val PER: 0.0486
2026-01-13 23:34:07,401: t15.2023.12.03 val PER: 0.0515
2026-01-13 23:34:07,401: t15.2023.12.08 val PER: 0.0406
2026-01-13 23:34:07,401: t15.2023.12.10 val PER: 0.0420
2026-01-13 23:34:07,401: t15.2023.12.17 val PER: 0.0894
2026-01-13 23:34:07,401: t15.2023.12.29 val PER: 0.0789
2026-01-13 23:34:07,401: t15.2024.02.25 val PER: 0.0716
2026-01-13 23:34:07,401: t15.2024.03.08 val PER: 0.1849
2026-01-13 23:34:07,401: t15.2024.03.15 val PER: 0.1488
2026-01-13 23:34:07,401: t15.2024.03.17 val PER: 0.0851
2026-01-13 23:34:07,401: t15.2024.05.10 val PER: 0.1233
2026-01-13 23:34:07,401: t15.2024.06.14 val PER: 0.1199
2026-01-13 23:34:07,402: t15.2024.07.19 val PER: 0.1740
2026-01-13 23:34:07,402: t15.2024.07.21 val PER: 0.0586
2026-01-13 23:34:07,402: t15.2024.07.28 val PER: 0.0978
2026-01-13 23:34:07,402: t15.2025.01.10 val PER: 0.2603
2026-01-13 23:34:07,402: t15.2025.01.12 val PER: 0.0978
2026-01-13 23:34:07,402: t15.2025.03.14 val PER: 0.2959
2026-01-13 23:34:07,402: t15.2025.03.16 val PER: 0.1361
2026-01-13 23:34:07,402: t15.2025.03.30 val PER: 0.2345
2026-01-13 23:34:07,402: t15.2025.04.13 val PER: 0.1883
2026-01-13 23:34:07,538: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_50000
2026-01-13 23:34:24,916: Train batch 50200: loss: 1.07 grad norm: 36.17 time: 0.062
2026-01-13 23:34:42,209: Train batch 50400: loss: 0.35 grad norm: 14.63 time: 0.052
2026-01-13 23:34:51,057: Running test after training batch: 50500
2026-01-13 23:34:51,224: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:34:56,311: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:34:56,379: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:35:10,328: Val batch 50500: PER (avg): 0.1065 CTC Loss (avg): 18.2586 WER(5gram): 18.64% (n=256) time: 19.271
2026-01-13 23:35:10,328: WER lens: avg_true_words=5.99 avg_pred_words=6.29 max_pred_words=13
2026-01-13 23:35:10,328: t15.2023.08.13 val PER: 0.0748
2026-01-13 23:35:10,328: t15.2023.08.18 val PER: 0.0754
2026-01-13 23:35:10,329: t15.2023.08.20 val PER: 0.0612
2026-01-13 23:35:10,329: t15.2023.08.25 val PER: 0.0738
2026-01-13 23:35:10,329: t15.2023.08.27 val PER: 0.1479
2026-01-13 23:35:10,329: t15.2023.09.01 val PER: 0.0487
2026-01-13 23:35:10,329: t15.2023.09.03 val PER: 0.1318
2026-01-13 23:35:10,329: t15.2023.09.24 val PER: 0.0886
2026-01-13 23:35:10,329: t15.2023.09.29 val PER: 0.0996
2026-01-13 23:35:10,329: t15.2023.10.01 val PER: 0.1387
2026-01-13 23:35:10,329: t15.2023.10.06 val PER: 0.0549
2026-01-13 23:35:10,329: t15.2023.10.08 val PER: 0.1976
2026-01-13 23:35:10,329: t15.2023.10.13 val PER: 0.1412
2026-01-13 23:35:10,330: t15.2023.10.15 val PER: 0.1009
2026-01-13 23:35:10,330: t15.2023.10.20 val PER: 0.1544
2026-01-13 23:35:10,330: t15.2023.10.22 val PER: 0.0857
2026-01-13 23:35:10,330: t15.2023.11.03 val PER: 0.1526
2026-01-13 23:35:10,330: t15.2023.11.04 val PER: 0.0171
2026-01-13 23:35:10,330: t15.2023.11.17 val PER: 0.0202
2026-01-13 23:35:10,330: t15.2023.11.19 val PER: 0.0240
2026-01-13 23:35:10,330: t15.2023.11.26 val PER: 0.0486
2026-01-13 23:35:10,330: t15.2023.12.03 val PER: 0.0483
2026-01-13 23:35:10,330: t15.2023.12.08 val PER: 0.0340
2026-01-13 23:35:10,330: t15.2023.12.10 val PER: 0.0355
2026-01-13 23:35:10,330: t15.2023.12.17 val PER: 0.0894
2026-01-13 23:35:10,330: t15.2023.12.29 val PER: 0.0762
2026-01-13 23:35:10,330: t15.2024.02.25 val PER: 0.0660
2026-01-13 23:35:10,330: t15.2024.03.08 val PER: 0.1735
2026-01-13 23:35:10,330: t15.2024.03.15 val PER: 0.1595
2026-01-13 23:35:10,331: t15.2024.03.17 val PER: 0.0851
2026-01-13 23:35:10,331: t15.2024.05.10 val PER: 0.1278
2026-01-13 23:35:10,331: t15.2024.06.14 val PER: 0.1183
2026-01-13 23:35:10,331: t15.2024.07.19 val PER: 0.1694
2026-01-13 23:35:10,331: t15.2024.07.21 val PER: 0.0559
2026-01-13 23:35:10,331: t15.2024.07.28 val PER: 0.0926
2026-01-13 23:35:10,331: t15.2025.01.10 val PER: 0.2521
2026-01-13 23:35:10,331: t15.2025.01.12 val PER: 0.0947
2026-01-13 23:35:10,331: t15.2025.03.14 val PER: 0.2899
2026-01-13 23:35:10,331: t15.2025.03.16 val PER: 0.1427
2026-01-13 23:35:10,331: t15.2025.03.30 val PER: 0.2299
2026-01-13 23:35:10,331: t15.2025.04.13 val PER: 0.1954
2026-01-13 23:35:10,468: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_50500
2026-01-13 23:35:18,791: Train batch 50600: loss: 0.86 grad norm: 29.14 time: 0.053
2026-01-13 23:35:36,199: Train batch 50800: loss: 0.57 grad norm: 21.60 time: 0.075
2026-01-13 23:35:53,784: Train batch 51000: loss: 1.23 grad norm: 35.15 time: 0.056
2026-01-13 23:35:53,785: Running test after training batch: 51000
2026-01-13 23:35:53,936: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:35:58,694: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:35:58,748: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-13 23:36:12,718: Val batch 51000: PER (avg): 0.1074 CTC Loss (avg): 18.4064 WER(5gram): 19.82% (n=256) time: 18.933
2026-01-13 23:36:12,718: WER lens: avg_true_words=5.99 avg_pred_words=6.34 max_pred_words=13
2026-01-13 23:36:12,718: t15.2023.08.13 val PER: 0.0800
2026-01-13 23:36:12,718: t15.2023.08.18 val PER: 0.0780
2026-01-13 23:36:12,718: t15.2023.08.20 val PER: 0.0620
2026-01-13 23:36:12,718: t15.2023.08.25 val PER: 0.0768
2026-01-13 23:36:12,719: t15.2023.08.27 val PER: 0.1367
2026-01-13 23:36:12,719: t15.2023.09.01 val PER: 0.0455
2026-01-13 23:36:12,719: t15.2023.09.03 val PER: 0.1318
2026-01-13 23:36:12,719: t15.2023.09.24 val PER: 0.0910
2026-01-13 23:36:12,719: t15.2023.09.29 val PER: 0.1059
2026-01-13 23:36:12,719: t15.2023.10.01 val PER: 0.1413
2026-01-13 23:36:12,719: t15.2023.10.06 val PER: 0.0506
2026-01-13 23:36:12,719: t15.2023.10.08 val PER: 0.2111
2026-01-13 23:36:12,719: t15.2023.10.13 val PER: 0.1505
2026-01-13 23:36:12,719: t15.2023.10.15 val PER: 0.1055
2026-01-13 23:36:12,719: t15.2023.10.20 val PER: 0.1409
2026-01-13 23:36:12,719: t15.2023.10.22 val PER: 0.0869
2026-01-13 23:36:12,720: t15.2023.11.03 val PER: 0.1418
2026-01-13 23:36:12,720: t15.2023.11.04 val PER: 0.0205
2026-01-13 23:36:12,720: t15.2023.11.17 val PER: 0.0156
2026-01-13 23:36:12,720: t15.2023.11.19 val PER: 0.0200
2026-01-13 23:36:12,720: t15.2023.11.26 val PER: 0.0449
2026-01-13 23:36:12,720: t15.2023.12.03 val PER: 0.0494
2026-01-13 23:36:12,720: t15.2023.12.08 val PER: 0.0320
2026-01-13 23:36:12,720: t15.2023.12.10 val PER: 0.0355
2026-01-13 23:36:12,720: t15.2023.12.17 val PER: 0.0852
2026-01-13 23:36:12,720: t15.2023.12.29 val PER: 0.0734
2026-01-13 23:36:12,720: t15.2024.02.25 val PER: 0.0660
2026-01-13 23:36:12,720: t15.2024.03.08 val PER: 0.1821
2026-01-13 23:36:12,720: t15.2024.03.15 val PER: 0.1507
2026-01-13 23:36:12,720: t15.2024.03.17 val PER: 0.0851
2026-01-13 23:36:12,720: t15.2024.05.10 val PER: 0.1293
2026-01-13 23:36:12,721: t15.2024.06.14 val PER: 0.1246
2026-01-13 23:36:12,721: t15.2024.07.19 val PER: 0.1688
2026-01-13 23:36:12,721: t15.2024.07.21 val PER: 0.0634
2026-01-13 23:36:12,721: t15.2024.07.28 val PER: 0.0985
2026-01-13 23:36:12,721: t15.2025.01.10 val PER: 0.2548
2026-01-13 23:36:12,721: t15.2025.01.12 val PER: 0.1070
2026-01-13 23:36:12,721: t15.2025.03.14 val PER: 0.3107
2026-01-13 23:36:12,721: t15.2025.03.16 val PER: 0.1374
2026-01-13 23:36:12,721: t15.2025.03.30 val PER: 0.2356
2026-01-13 23:36:12,721: t15.2025.04.13 val PER: 0.1797
2026-01-13 23:36:12,860: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_51000
2026-01-13 23:36:30,232: Train batch 51200: loss: 0.19 grad norm: 11.59 time: 0.079
2026-01-13 23:36:47,505: Train batch 51400: loss: 0.61 grad norm: 19.59 time: 0.053
2026-01-13 23:36:56,280: Running test after training batch: 51500
2026-01-13 23:36:56,383: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:37:01,201: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:37:01,260: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-13 23:37:16,119: Val batch 51500: PER (avg): 0.1052 CTC Loss (avg): 18.1346 WER(5gram): 17.54% (n=256) time: 19.839
2026-01-13 23:37:16,120: WER lens: avg_true_words=5.99 avg_pred_words=6.30 max_pred_words=13
2026-01-13 23:37:16,120: t15.2023.08.13 val PER: 0.0790
2026-01-13 23:37:16,120: t15.2023.08.18 val PER: 0.0704
2026-01-13 23:37:16,120: t15.2023.08.20 val PER: 0.0612
2026-01-13 23:37:16,120: t15.2023.08.25 val PER: 0.0738
2026-01-13 23:37:16,121: t15.2023.08.27 val PER: 0.1383
2026-01-13 23:37:16,121: t15.2023.09.01 val PER: 0.0422
2026-01-13 23:37:16,121: t15.2023.09.03 val PER: 0.1200
2026-01-13 23:37:16,121: t15.2023.09.24 val PER: 0.0922
2026-01-13 23:37:16,121: t15.2023.09.29 val PER: 0.0996
2026-01-13 23:37:16,121: t15.2023.10.01 val PER: 0.1420
2026-01-13 23:37:16,121: t15.2023.10.06 val PER: 0.0538
2026-01-13 23:37:16,121: t15.2023.10.08 val PER: 0.1867
2026-01-13 23:37:16,121: t15.2023.10.13 val PER: 0.1466
2026-01-13 23:37:16,121: t15.2023.10.15 val PER: 0.1074
2026-01-13 23:37:16,122: t15.2023.10.20 val PER: 0.1711
2026-01-13 23:37:16,122: t15.2023.10.22 val PER: 0.0835
2026-01-13 23:37:16,122: t15.2023.11.03 val PER: 0.1431
2026-01-13 23:37:16,122: t15.2023.11.04 val PER: 0.0205
2026-01-13 23:37:16,122: t15.2023.11.17 val PER: 0.0187
2026-01-13 23:37:16,122: t15.2023.11.19 val PER: 0.0220
2026-01-13 23:37:16,122: t15.2023.11.26 val PER: 0.0449
2026-01-13 23:37:16,122: t15.2023.12.03 val PER: 0.0483
2026-01-13 23:37:16,122: t15.2023.12.08 val PER: 0.0340
2026-01-13 23:37:16,122: t15.2023.12.10 val PER: 0.0381
2026-01-13 23:37:16,122: t15.2023.12.17 val PER: 0.0842
2026-01-13 23:37:16,122: t15.2023.12.29 val PER: 0.0755
2026-01-13 23:37:16,123: t15.2024.02.25 val PER: 0.0618
2026-01-13 23:37:16,123: t15.2024.03.08 val PER: 0.1693
2026-01-13 23:37:16,123: t15.2024.03.15 val PER: 0.1445
2026-01-13 23:37:16,123: t15.2024.03.17 val PER: 0.0844
2026-01-13 23:37:16,123: t15.2024.05.10 val PER: 0.1263
2026-01-13 23:37:16,123: t15.2024.06.14 val PER: 0.1293
2026-01-13 23:37:16,123: t15.2024.07.19 val PER: 0.1648
2026-01-13 23:37:16,123: t15.2024.07.21 val PER: 0.0566
2026-01-13 23:37:16,123: t15.2024.07.28 val PER: 0.1007
2026-01-13 23:37:16,123: t15.2025.01.10 val PER: 0.2507
2026-01-13 23:37:16,123: t15.2025.01.12 val PER: 0.1062
2026-01-13 23:37:16,124: t15.2025.03.14 val PER: 0.3018
2026-01-13 23:37:16,124: t15.2025.03.16 val PER: 0.1283
2026-01-13 23:37:16,124: t15.2025.03.30 val PER: 0.2230
2026-01-13 23:37:16,124: t15.2025.04.13 val PER: 0.1840
2026-01-13 23:37:16,260: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_51500
2026-01-13 23:37:25,325: Train batch 51600: loss: 0.68 grad norm: 25.72 time: 0.081
2026-01-13 23:37:42,792: Train batch 51800: loss: 0.32 grad norm: 14.47 time: 0.060
2026-01-13 23:38:00,358: Train batch 52000: loss: 0.69 grad norm: 26.87 time: 0.066
2026-01-13 23:38:00,359: Running test after training batch: 52000
2026-01-13 23:38:00,457: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:38:05,429: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:38:05,502: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cents
2026-01-13 23:38:20,443: Val batch 52000: PER (avg): 0.1057 CTC Loss (avg): 18.4500 WER(5gram): 18.64% (n=256) time: 20.084
2026-01-13 23:38:20,444: WER lens: avg_true_words=5.99 avg_pred_words=6.32 max_pred_words=13
2026-01-13 23:38:20,444: t15.2023.08.13 val PER: 0.0759
2026-01-13 23:38:20,444: t15.2023.08.18 val PER: 0.0771
2026-01-13 23:38:20,444: t15.2023.08.20 val PER: 0.0635
2026-01-13 23:38:20,444: t15.2023.08.25 val PER: 0.0738
2026-01-13 23:38:20,444: t15.2023.08.27 val PER: 0.1415
2026-01-13 23:38:20,444: t15.2023.09.01 val PER: 0.0414
2026-01-13 23:38:20,444: t15.2023.09.03 val PER: 0.1259
2026-01-13 23:38:20,444: t15.2023.09.24 val PER: 0.0947
2026-01-13 23:38:20,444: t15.2023.09.29 val PER: 0.0964
2026-01-13 23:38:20,445: t15.2023.10.01 val PER: 0.1387
2026-01-13 23:38:20,445: t15.2023.10.06 val PER: 0.0560
2026-01-13 23:38:20,445: t15.2023.10.08 val PER: 0.1935
2026-01-13 23:38:20,445: t15.2023.10.13 val PER: 0.1435
2026-01-13 23:38:20,445: t15.2023.10.15 val PER: 0.1055
2026-01-13 23:38:20,445: t15.2023.10.20 val PER: 0.1477
2026-01-13 23:38:20,445: t15.2023.10.22 val PER: 0.0813
2026-01-13 23:38:20,445: t15.2023.11.03 val PER: 0.1459
2026-01-13 23:38:20,445: t15.2023.11.04 val PER: 0.0307
2026-01-13 23:38:20,445: t15.2023.11.17 val PER: 0.0171
2026-01-13 23:38:20,445: t15.2023.11.19 val PER: 0.0180
2026-01-13 23:38:20,445: t15.2023.11.26 val PER: 0.0493
2026-01-13 23:38:20,446: t15.2023.12.03 val PER: 0.0494
2026-01-13 23:38:20,446: t15.2023.12.08 val PER: 0.0346
2026-01-13 23:38:20,446: t15.2023.12.10 val PER: 0.0355
2026-01-13 23:38:20,446: t15.2023.12.17 val PER: 0.0728
2026-01-13 23:38:20,446: t15.2023.12.29 val PER: 0.0734
2026-01-13 23:38:20,446: t15.2024.02.25 val PER: 0.0618
2026-01-13 23:38:20,446: t15.2024.03.08 val PER: 0.1707
2026-01-13 23:38:20,446: t15.2024.03.15 val PER: 0.1582
2026-01-13 23:38:20,446: t15.2024.03.17 val PER: 0.0872
2026-01-13 23:38:20,446: t15.2024.05.10 val PER: 0.1278
2026-01-13 23:38:20,446: t15.2024.06.14 val PER: 0.1230
2026-01-13 23:38:20,446: t15.2024.07.19 val PER: 0.1615
2026-01-13 23:38:20,446: t15.2024.07.21 val PER: 0.0614
2026-01-13 23:38:20,446: t15.2024.07.28 val PER: 0.0978
2026-01-13 23:38:20,446: t15.2025.01.10 val PER: 0.2507
2026-01-13 23:38:20,446: t15.2025.01.12 val PER: 0.1008
2026-01-13 23:38:20,446: t15.2025.03.14 val PER: 0.3018
2026-01-13 23:38:20,447: t15.2025.03.16 val PER: 0.1414
2026-01-13 23:38:20,447: t15.2025.03.30 val PER: 0.2264
2026-01-13 23:38:20,447: t15.2025.04.13 val PER: 0.1883
2026-01-13 23:38:20,582: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_52000
2026-01-13 23:38:38,025: Train batch 52200: loss: 0.95 grad norm: 32.16 time: 0.069
2026-01-13 23:38:55,420: Train batch 52400: loss: 0.68 grad norm: 27.77 time: 0.054
2026-01-13 23:39:04,297: Running test after training batch: 52500
2026-01-13 23:39:04,436: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:39:09,691: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:39:09,746: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:39:26,438: Val batch 52500: PER (avg): 0.1042 CTC Loss (avg): 17.9393 WER(5gram): 20.01% (n=256) time: 22.141
2026-01-13 23:39:26,439: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=13
2026-01-13 23:39:26,439: t15.2023.08.13 val PER: 0.0811
2026-01-13 23:39:26,439: t15.2023.08.18 val PER: 0.0645
2026-01-13 23:39:26,439: t15.2023.08.20 val PER: 0.0620
2026-01-13 23:39:26,439: t15.2023.08.25 val PER: 0.0678
2026-01-13 23:39:26,440: t15.2023.08.27 val PER: 0.1383
2026-01-13 23:39:26,440: t15.2023.09.01 val PER: 0.0455
2026-01-13 23:39:26,440: t15.2023.09.03 val PER: 0.1247
2026-01-13 23:39:26,440: t15.2023.09.24 val PER: 0.0898
2026-01-13 23:39:26,440: t15.2023.09.29 val PER: 0.0983
2026-01-13 23:39:26,440: t15.2023.10.01 val PER: 0.1400
2026-01-13 23:39:26,440: t15.2023.10.06 val PER: 0.0527
2026-01-13 23:39:26,440: t15.2023.10.08 val PER: 0.1935
2026-01-13 23:39:26,440: t15.2023.10.13 val PER: 0.1373
2026-01-13 23:39:26,440: t15.2023.10.15 val PER: 0.0995
2026-01-13 23:39:26,440: t15.2023.10.20 val PER: 0.1443
2026-01-13 23:39:26,441: t15.2023.10.22 val PER: 0.0780
2026-01-13 23:39:26,441: t15.2023.11.03 val PER: 0.1493
2026-01-13 23:39:26,441: t15.2023.11.04 val PER: 0.0239
2026-01-13 23:39:26,441: t15.2023.11.17 val PER: 0.0218
2026-01-13 23:39:26,441: t15.2023.11.19 val PER: 0.0160
2026-01-13 23:39:26,441: t15.2023.11.26 val PER: 0.0449
2026-01-13 23:39:26,441: t15.2023.12.03 val PER: 0.0525
2026-01-13 23:39:26,441: t15.2023.12.08 val PER: 0.0379
2026-01-13 23:39:26,441: t15.2023.12.10 val PER: 0.0315
2026-01-13 23:39:26,441: t15.2023.12.17 val PER: 0.0780
2026-01-13 23:39:26,442: t15.2023.12.29 val PER: 0.0693
2026-01-13 23:39:26,442: t15.2024.02.25 val PER: 0.0688
2026-01-13 23:39:26,442: t15.2024.03.08 val PER: 0.1679
2026-01-13 23:39:26,442: t15.2024.03.15 val PER: 0.1538
2026-01-13 23:39:26,442: t15.2024.03.17 val PER: 0.0802
2026-01-13 23:39:26,442: t15.2024.05.10 val PER: 0.1144
2026-01-13 23:39:26,442: t15.2024.06.14 val PER: 0.1262
2026-01-13 23:39:26,442: t15.2024.07.19 val PER: 0.1641
2026-01-13 23:39:26,443: t15.2024.07.21 val PER: 0.0655
2026-01-13 23:39:26,443: t15.2024.07.28 val PER: 0.0971
2026-01-13 23:39:26,443: t15.2025.01.10 val PER: 0.2493
2026-01-13 23:39:26,443: t15.2025.01.12 val PER: 0.0955
2026-01-13 23:39:26,443: t15.2025.03.14 val PER: 0.3018
2026-01-13 23:39:26,443: t15.2025.03.16 val PER: 0.1348
2026-01-13 23:39:26,443: t15.2025.03.30 val PER: 0.2264
2026-01-13 23:39:26,443: t15.2025.04.13 val PER: 0.1812
2026-01-13 23:39:26,579: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_52500
2026-01-13 23:39:35,658: Train batch 52600: loss: 0.32 grad norm: 21.75 time: 0.055
2026-01-13 23:39:53,431: Train batch 52800: loss: 0.54 grad norm: 21.43 time: 0.061
2026-01-13 23:40:10,707: Train batch 53000: loss: 0.55 grad norm: 26.60 time: 0.056
2026-01-13 23:40:10,707: Running test after training batch: 53000
2026-01-13 23:40:10,976: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:40:15,783: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:40:15,838: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cent
2026-01-13 23:40:30,118: Val batch 53000: PER (avg): 0.1041 CTC Loss (avg): 18.1571 WER(5gram): 18.19% (n=256) time: 19.410
2026-01-13 23:40:30,118: WER lens: avg_true_words=5.99 avg_pred_words=6.34 max_pred_words=13
2026-01-13 23:40:30,118: t15.2023.08.13 val PER: 0.0696
2026-01-13 23:40:30,119: t15.2023.08.18 val PER: 0.0754
2026-01-13 23:40:30,119: t15.2023.08.20 val PER: 0.0612
2026-01-13 23:40:30,119: t15.2023.08.25 val PER: 0.0753
2026-01-13 23:40:30,119: t15.2023.08.27 val PER: 0.1399
2026-01-13 23:40:30,119: t15.2023.09.01 val PER: 0.0455
2026-01-13 23:40:30,119: t15.2023.09.03 val PER: 0.1211
2026-01-13 23:40:30,119: t15.2023.09.24 val PER: 0.0910
2026-01-13 23:40:30,119: t15.2023.09.29 val PER: 0.0938
2026-01-13 23:40:30,119: t15.2023.10.01 val PER: 0.1341
2026-01-13 23:40:30,119: t15.2023.10.06 val PER: 0.0549
2026-01-13 23:40:30,119: t15.2023.10.08 val PER: 0.1922
2026-01-13 23:40:30,119: t15.2023.10.13 val PER: 0.1451
2026-01-13 23:40:30,119: t15.2023.10.15 val PER: 0.1028
2026-01-13 23:40:30,120: t15.2023.10.20 val PER: 0.1443
2026-01-13 23:40:30,120: t15.2023.10.22 val PER: 0.0802
2026-01-13 23:40:30,120: t15.2023.11.03 val PER: 0.1431
2026-01-13 23:40:30,120: t15.2023.11.04 val PER: 0.0239
2026-01-13 23:40:30,120: t15.2023.11.17 val PER: 0.0187
2026-01-13 23:40:30,120: t15.2023.11.19 val PER: 0.0160
2026-01-13 23:40:30,120: t15.2023.11.26 val PER: 0.0384
2026-01-13 23:40:30,120: t15.2023.12.03 val PER: 0.0536
2026-01-13 23:40:30,120: t15.2023.12.08 val PER: 0.0360
2026-01-13 23:40:30,120: t15.2023.12.10 val PER: 0.0368
2026-01-13 23:40:30,120: t15.2023.12.17 val PER: 0.0873
2026-01-13 23:40:30,120: t15.2023.12.29 val PER: 0.0707
2026-01-13 23:40:30,120: t15.2024.02.25 val PER: 0.0674
2026-01-13 23:40:30,120: t15.2024.03.08 val PER: 0.1636
2026-01-13 23:40:30,120: t15.2024.03.15 val PER: 0.1432
2026-01-13 23:40:30,121: t15.2024.03.17 val PER: 0.0767
2026-01-13 23:40:30,121: t15.2024.05.10 val PER: 0.1278
2026-01-13 23:40:30,121: t15.2024.06.14 val PER: 0.1167
2026-01-13 23:40:30,121: t15.2024.07.19 val PER: 0.1648
2026-01-13 23:40:30,121: t15.2024.07.21 val PER: 0.0607
2026-01-13 23:40:30,121: t15.2024.07.28 val PER: 0.0956
2026-01-13 23:40:30,121: t15.2025.01.10 val PER: 0.2548
2026-01-13 23:40:30,121: t15.2025.01.12 val PER: 0.1055
2026-01-13 23:40:30,121: t15.2025.03.14 val PER: 0.3092
2026-01-13 23:40:30,121: t15.2025.03.16 val PER: 0.1348
2026-01-13 23:40:30,121: t15.2025.03.30 val PER: 0.2264
2026-01-13 23:40:30,121: t15.2025.04.13 val PER: 0.1897
2026-01-13 23:40:30,260: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_53000
2026-01-13 23:40:47,603: Train batch 53200: loss: 0.35 grad norm: 17.45 time: 0.064
2026-01-13 23:41:04,781: Train batch 53400: loss: 0.28 grad norm: 21.53 time: 0.058
2026-01-13 23:41:13,643: Running test after training batch: 53500
2026-01-13 23:41:13,790: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:41:18,868: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:41:18,924: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost out
2026-01-13 23:41:33,087: Val batch 53500: PER (avg): 0.1047 CTC Loss (avg): 18.6076 WER(5gram): 16.88% (n=256) time: 19.443
2026-01-13 23:41:33,087: WER lens: avg_true_words=5.99 avg_pred_words=6.29 max_pred_words=13
2026-01-13 23:41:33,087: t15.2023.08.13 val PER: 0.0707
2026-01-13 23:41:33,088: t15.2023.08.18 val PER: 0.0721
2026-01-13 23:41:33,088: t15.2023.08.20 val PER: 0.0612
2026-01-13 23:41:33,088: t15.2023.08.25 val PER: 0.0768
2026-01-13 23:41:33,088: t15.2023.08.27 val PER: 0.1463
2026-01-13 23:41:33,088: t15.2023.09.01 val PER: 0.0430
2026-01-13 23:41:33,088: t15.2023.09.03 val PER: 0.1235
2026-01-13 23:41:33,088: t15.2023.09.24 val PER: 0.0910
2026-01-13 23:41:33,088: t15.2023.09.29 val PER: 0.0970
2026-01-13 23:41:33,088: t15.2023.10.01 val PER: 0.1367
2026-01-13 23:41:33,088: t15.2023.10.06 val PER: 0.0517
2026-01-13 23:41:33,088: t15.2023.10.08 val PER: 0.1949
2026-01-13 23:41:33,089: t15.2023.10.13 val PER: 0.1427
2026-01-13 23:41:33,089: t15.2023.10.15 val PER: 0.1094
2026-01-13 23:41:33,089: t15.2023.10.20 val PER: 0.1477
2026-01-13 23:41:33,089: t15.2023.10.22 val PER: 0.0768
2026-01-13 23:41:33,089: t15.2023.11.03 val PER: 0.1357
2026-01-13 23:41:33,089: t15.2023.11.04 val PER: 0.0239
2026-01-13 23:41:33,089: t15.2023.11.17 val PER: 0.0187
2026-01-13 23:41:33,089: t15.2023.11.19 val PER: 0.0180
2026-01-13 23:41:33,089: t15.2023.11.26 val PER: 0.0449
2026-01-13 23:41:33,089: t15.2023.12.03 val PER: 0.0588
2026-01-13 23:41:33,089: t15.2023.12.08 val PER: 0.0353
2026-01-13 23:41:33,089: t15.2023.12.10 val PER: 0.0355
2026-01-13 23:41:33,089: t15.2023.12.17 val PER: 0.0852
2026-01-13 23:41:33,089: t15.2023.12.29 val PER: 0.0707
2026-01-13 23:41:33,090: t15.2024.02.25 val PER: 0.0618
2026-01-13 23:41:33,090: t15.2024.03.08 val PER: 0.1650
2026-01-13 23:41:33,090: t15.2024.03.15 val PER: 0.1545
2026-01-13 23:41:33,090: t15.2024.03.17 val PER: 0.0781
2026-01-13 23:41:33,090: t15.2024.05.10 val PER: 0.1189
2026-01-13 23:41:33,090: t15.2024.06.14 val PER: 0.1262
2026-01-13 23:41:33,090: t15.2024.07.19 val PER: 0.1701
2026-01-13 23:41:33,090: t15.2024.07.21 val PER: 0.0628
2026-01-13 23:41:33,090: t15.2024.07.28 val PER: 0.0956
2026-01-13 23:41:33,090: t15.2025.01.10 val PER: 0.2521
2026-01-13 23:41:33,090: t15.2025.01.12 val PER: 0.1109
2026-01-13 23:41:33,090: t15.2025.03.14 val PER: 0.2914
2026-01-13 23:41:33,090: t15.2025.03.16 val PER: 0.1257
2026-01-13 23:41:33,090: t15.2025.03.30 val PER: 0.2218
2026-01-13 23:41:33,090: t15.2025.04.13 val PER: 0.1869
2026-01-13 23:41:33,234: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_53500
2026-01-13 23:41:42,242: Train batch 53600: loss: 0.55 grad norm: 21.14 time: 0.062
2026-01-13 23:41:59,787: Train batch 53800: loss: 0.34 grad norm: 17.09 time: 0.066
2026-01-13 23:42:17,327: Train batch 54000: loss: 0.29 grad norm: 13.73 time: 0.066
2026-01-13 23:42:17,327: Running test after training batch: 54000
2026-01-13 23:42:17,460: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:42:22,257: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:42:22,311: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:42:36,210: Val batch 54000: PER (avg): 0.1052 CTC Loss (avg): 18.3504 WER(5gram): 18.12% (n=256) time: 18.882
2026-01-13 23:42:36,210: WER lens: avg_true_words=5.99 avg_pred_words=6.33 max_pred_words=13
2026-01-13 23:42:36,211: t15.2023.08.13 val PER: 0.0707
2026-01-13 23:42:36,211: t15.2023.08.18 val PER: 0.0721
2026-01-13 23:42:36,211: t15.2023.08.20 val PER: 0.0604
2026-01-13 23:42:36,211: t15.2023.08.25 val PER: 0.0828
2026-01-13 23:42:36,211: t15.2023.08.27 val PER: 0.1383
2026-01-13 23:42:36,211: t15.2023.09.01 val PER: 0.0430
2026-01-13 23:42:36,211: t15.2023.09.03 val PER: 0.1152
2026-01-13 23:42:36,211: t15.2023.09.24 val PER: 0.0934
2026-01-13 23:42:36,211: t15.2023.09.29 val PER: 0.0932
2026-01-13 23:42:36,212: t15.2023.10.01 val PER: 0.1341
2026-01-13 23:42:36,212: t15.2023.10.06 val PER: 0.0506
2026-01-13 23:42:36,212: t15.2023.10.08 val PER: 0.1840
2026-01-13 23:42:36,212: t15.2023.10.13 val PER: 0.1427
2026-01-13 23:42:36,212: t15.2023.10.15 val PER: 0.1068
2026-01-13 23:42:36,212: t15.2023.10.20 val PER: 0.1577
2026-01-13 23:42:36,212: t15.2023.10.22 val PER: 0.0857
2026-01-13 23:42:36,212: t15.2023.11.03 val PER: 0.1391
2026-01-13 23:42:36,212: t15.2023.11.04 val PER: 0.0205
2026-01-13 23:42:36,213: t15.2023.11.17 val PER: 0.0140
2026-01-13 23:42:36,213: t15.2023.11.19 val PER: 0.0200
2026-01-13 23:42:36,213: t15.2023.11.26 val PER: 0.0457
2026-01-13 23:42:36,213: t15.2023.12.03 val PER: 0.0494
2026-01-13 23:42:36,213: t15.2023.12.08 val PER: 0.0366
2026-01-13 23:42:36,213: t15.2023.12.10 val PER: 0.0315
2026-01-13 23:42:36,213: t15.2023.12.17 val PER: 0.0800
2026-01-13 23:42:36,213: t15.2023.12.29 val PER: 0.0803
2026-01-13 23:42:36,213: t15.2024.02.25 val PER: 0.0674
2026-01-13 23:42:36,213: t15.2024.03.08 val PER: 0.1636
2026-01-13 23:42:36,213: t15.2024.03.15 val PER: 0.1595
2026-01-13 23:42:36,214: t15.2024.03.17 val PER: 0.0746
2026-01-13 23:42:36,214: t15.2024.05.10 val PER: 0.1308
2026-01-13 23:42:36,214: t15.2024.06.14 val PER: 0.1262
2026-01-13 23:42:36,214: t15.2024.07.19 val PER: 0.1793
2026-01-13 23:42:36,214: t15.2024.07.21 val PER: 0.0648
2026-01-13 23:42:36,214: t15.2024.07.28 val PER: 0.0956
2026-01-13 23:42:36,214: t15.2025.01.10 val PER: 0.2686
2026-01-13 23:42:36,215: t15.2025.01.12 val PER: 0.1070
2026-01-13 23:42:36,215: t15.2025.03.14 val PER: 0.2914
2026-01-13 23:42:36,215: t15.2025.03.16 val PER: 0.1257
2026-01-13 23:42:36,215: t15.2025.03.30 val PER: 0.2310
2026-01-13 23:42:36,215: t15.2025.04.13 val PER: 0.1755
2026-01-13 23:42:36,358: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_54000
2026-01-13 23:42:53,471: Train batch 54200: loss: 0.21 grad norm: 11.41 time: 0.084
2026-01-13 23:43:10,803: Train batch 54400: loss: 0.18 grad norm: 9.50 time: 0.071
2026-01-13 23:43:19,471: Running test after training batch: 54500
2026-01-13 23:43:19,631: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:43:24,443: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:43:24,502: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:43:38,515: Val batch 54500: PER (avg): 0.1048 CTC Loss (avg): 18.4016 WER(5gram): 18.32% (n=256) time: 19.043
2026-01-13 23:43:38,515: WER lens: avg_true_words=5.99 avg_pred_words=6.30 max_pred_words=13
2026-01-13 23:43:38,515: t15.2023.08.13 val PER: 0.0728
2026-01-13 23:43:38,515: t15.2023.08.18 val PER: 0.0679
2026-01-13 23:43:38,516: t15.2023.08.20 val PER: 0.0540
2026-01-13 23:43:38,516: t15.2023.08.25 val PER: 0.0783
2026-01-13 23:43:38,516: t15.2023.08.27 val PER: 0.1431
2026-01-13 23:43:38,516: t15.2023.09.01 val PER: 0.0446
2026-01-13 23:43:38,516: t15.2023.09.03 val PER: 0.1235
2026-01-13 23:43:38,516: t15.2023.09.24 val PER: 0.0995
2026-01-13 23:43:38,516: t15.2023.09.29 val PER: 0.0957
2026-01-13 23:43:38,516: t15.2023.10.01 val PER: 0.1367
2026-01-13 23:43:38,516: t15.2023.10.06 val PER: 0.0527
2026-01-13 23:43:38,517: t15.2023.10.08 val PER: 0.1935
2026-01-13 23:43:38,517: t15.2023.10.13 val PER: 0.1474
2026-01-13 23:43:38,517: t15.2023.10.15 val PER: 0.1088
2026-01-13 23:43:38,517: t15.2023.10.20 val PER: 0.1644
2026-01-13 23:43:38,517: t15.2023.10.22 val PER: 0.0869
2026-01-13 23:43:38,517: t15.2023.11.03 val PER: 0.1493
2026-01-13 23:43:38,517: t15.2023.11.04 val PER: 0.0205
2026-01-13 23:43:38,517: t15.2023.11.17 val PER: 0.0140
2026-01-13 23:43:38,517: t15.2023.11.19 val PER: 0.0180
2026-01-13 23:43:38,517: t15.2023.11.26 val PER: 0.0486
2026-01-13 23:43:38,517: t15.2023.12.03 val PER: 0.0473
2026-01-13 23:43:38,517: t15.2023.12.08 val PER: 0.0360
2026-01-13 23:43:38,518: t15.2023.12.10 val PER: 0.0368
2026-01-13 23:43:38,518: t15.2023.12.17 val PER: 0.0800
2026-01-13 23:43:38,518: t15.2023.12.29 val PER: 0.0776
2026-01-13 23:43:38,518: t15.2024.02.25 val PER: 0.0660
2026-01-13 23:43:38,518: t15.2024.03.08 val PER: 0.1636
2026-01-13 23:43:38,518: t15.2024.03.15 val PER: 0.1451
2026-01-13 23:43:38,518: t15.2024.03.17 val PER: 0.0669
2026-01-13 23:43:38,518: t15.2024.05.10 val PER: 0.1278
2026-01-13 23:43:38,518: t15.2024.06.14 val PER: 0.1262
2026-01-13 23:43:38,518: t15.2024.07.19 val PER: 0.1701
2026-01-13 23:43:38,518: t15.2024.07.21 val PER: 0.0621
2026-01-13 23:43:38,518: t15.2024.07.28 val PER: 0.0934
2026-01-13 23:43:38,518: t15.2025.01.10 val PER: 0.2576
2026-01-13 23:43:38,518: t15.2025.01.12 val PER: 0.0985
2026-01-13 23:43:38,518: t15.2025.03.14 val PER: 0.3033
2026-01-13 23:43:38,518: t15.2025.03.16 val PER: 0.1296
2026-01-13 23:43:38,519: t15.2025.03.30 val PER: 0.2230
2026-01-13 23:43:38,519: t15.2025.04.13 val PER: 0.1883
2026-01-13 23:43:38,662: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_54500
2026-01-13 23:43:47,541: Train batch 54600: loss: 0.23 grad norm: 16.98 time: 0.054
2026-01-13 23:44:05,037: Train batch 54800: loss: 0.35 grad norm: 21.31 time: 0.072
2026-01-13 23:44:22,658: Train batch 55000: loss: 0.06 grad norm: 6.05 time: 0.063
2026-01-13 23:44:22,658: Running test after training batch: 55000
2026-01-13 23:44:22,974: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:44:27,803: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:44:27,854: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:44:42,043: Val batch 55000: PER (avg): 0.1071 CTC Loss (avg): 18.8509 WER(5gram): 19.36% (n=256) time: 19.385
2026-01-13 23:44:42,044: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=13
2026-01-13 23:44:42,044: t15.2023.08.13 val PER: 0.0748
2026-01-13 23:44:42,044: t15.2023.08.18 val PER: 0.0721
2026-01-13 23:44:42,044: t15.2023.08.20 val PER: 0.0643
2026-01-13 23:44:42,045: t15.2023.08.25 val PER: 0.0753
2026-01-13 23:44:42,045: t15.2023.08.27 val PER: 0.1399
2026-01-13 23:44:42,045: t15.2023.09.01 val PER: 0.0463
2026-01-13 23:44:42,045: t15.2023.09.03 val PER: 0.1366
2026-01-13 23:44:42,045: t15.2023.09.24 val PER: 0.0995
2026-01-13 23:44:42,045: t15.2023.09.29 val PER: 0.0932
2026-01-13 23:44:42,045: t15.2023.10.01 val PER: 0.1413
2026-01-13 23:44:42,045: t15.2023.10.06 val PER: 0.0538
2026-01-13 23:44:42,045: t15.2023.10.08 val PER: 0.2070
2026-01-13 23:44:42,045: t15.2023.10.13 val PER: 0.1482
2026-01-13 23:44:42,045: t15.2023.10.15 val PER: 0.1114
2026-01-13 23:44:42,045: t15.2023.10.20 val PER: 0.1611
2026-01-13 23:44:42,046: t15.2023.10.22 val PER: 0.0924
2026-01-13 23:44:42,046: t15.2023.11.03 val PER: 0.1506
2026-01-13 23:44:42,046: t15.2023.11.04 val PER: 0.0205
2026-01-13 23:44:42,046: t15.2023.11.17 val PER: 0.0140
2026-01-13 23:44:42,046: t15.2023.11.19 val PER: 0.0200
2026-01-13 23:44:42,046: t15.2023.11.26 val PER: 0.0522
2026-01-13 23:44:42,046: t15.2023.12.03 val PER: 0.0504
2026-01-13 23:44:42,046: t15.2023.12.08 val PER: 0.0379
2026-01-13 23:44:42,046: t15.2023.12.10 val PER: 0.0342
2026-01-13 23:44:42,046: t15.2023.12.17 val PER: 0.0873
2026-01-13 23:44:42,046: t15.2023.12.29 val PER: 0.0728
2026-01-13 23:44:42,046: t15.2024.02.25 val PER: 0.0730
2026-01-13 23:44:42,046: t15.2024.03.08 val PER: 0.1679
2026-01-13 23:44:42,046: t15.2024.03.15 val PER: 0.1526
2026-01-13 23:44:42,047: t15.2024.03.17 val PER: 0.0746
2026-01-13 23:44:42,047: t15.2024.05.10 val PER: 0.1278
2026-01-13 23:44:42,047: t15.2024.06.14 val PER: 0.1341
2026-01-13 23:44:42,047: t15.2024.07.19 val PER: 0.1641
2026-01-13 23:44:42,047: t15.2024.07.21 val PER: 0.0628
2026-01-13 23:44:42,047: t15.2024.07.28 val PER: 0.0985
2026-01-13 23:44:42,047: t15.2025.01.10 val PER: 0.2631
2026-01-13 23:44:42,047: t15.2025.01.12 val PER: 0.0924
2026-01-13 23:44:42,047: t15.2025.03.14 val PER: 0.3018
2026-01-13 23:44:42,047: t15.2025.03.16 val PER: 0.1257
2026-01-13 23:44:42,047: t15.2025.03.30 val PER: 0.2322
2026-01-13 23:44:42,047: t15.2025.04.13 val PER: 0.1869
2026-01-13 23:44:42,192: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_55000
2026-01-13 23:45:00,016: Train batch 55200: loss: 0.33 grad norm: 16.47 time: 0.066
2026-01-13 23:45:17,322: Train batch 55400: loss: 0.30 grad norm: 21.33 time: 0.051
2026-01-13 23:45:25,822: Running test after training batch: 55500
2026-01-13 23:45:25,930: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:45:30,750: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:45:30,811: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost out
2026-01-13 23:45:45,985: Val batch 55500: PER (avg): 0.1059 CTC Loss (avg): 18.6858 WER(5gram): 20.14% (n=256) time: 20.163
2026-01-13 23:45:45,986: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=13
2026-01-13 23:45:45,986: t15.2023.08.13 val PER: 0.0759
2026-01-13 23:45:45,986: t15.2023.08.18 val PER: 0.0738
2026-01-13 23:45:45,986: t15.2023.08.20 val PER: 0.0612
2026-01-13 23:45:45,986: t15.2023.08.25 val PER: 0.0753
2026-01-13 23:45:45,986: t15.2023.08.27 val PER: 0.1463
2026-01-13 23:45:45,986: t15.2023.09.01 val PER: 0.0511
2026-01-13 23:45:45,986: t15.2023.09.03 val PER: 0.1188
2026-01-13 23:45:45,986: t15.2023.09.24 val PER: 0.0910
2026-01-13 23:45:45,986: t15.2023.09.29 val PER: 0.0951
2026-01-13 23:45:45,987: t15.2023.10.01 val PER: 0.1387
2026-01-13 23:45:45,987: t15.2023.10.06 val PER: 0.0484
2026-01-13 23:45:45,987: t15.2023.10.08 val PER: 0.2084
2026-01-13 23:45:45,987: t15.2023.10.13 val PER: 0.1458
2026-01-13 23:45:45,987: t15.2023.10.15 val PER: 0.1081
2026-01-13 23:45:45,987: t15.2023.10.20 val PER: 0.1477
2026-01-13 23:45:45,987: t15.2023.10.22 val PER: 0.0869
2026-01-13 23:45:45,987: t15.2023.11.03 val PER: 0.1452
2026-01-13 23:45:45,987: t15.2023.11.04 val PER: 0.0137
2026-01-13 23:45:45,987: t15.2023.11.17 val PER: 0.0171
2026-01-13 23:45:45,987: t15.2023.11.19 val PER: 0.0200
2026-01-13 23:45:45,988: t15.2023.11.26 val PER: 0.0478
2026-01-13 23:45:45,988: t15.2023.12.03 val PER: 0.0504
2026-01-13 23:45:45,988: t15.2023.12.08 val PER: 0.0413
2026-01-13 23:45:45,988: t15.2023.12.10 val PER: 0.0381
2026-01-13 23:45:45,988: t15.2023.12.17 val PER: 0.0967
2026-01-13 23:45:45,988: t15.2023.12.29 val PER: 0.0734
2026-01-13 23:45:45,988: t15.2024.02.25 val PER: 0.0702
2026-01-13 23:45:45,988: t15.2024.03.08 val PER: 0.1679
2026-01-13 23:45:45,988: t15.2024.03.15 val PER: 0.1551
2026-01-13 23:45:45,988: t15.2024.03.17 val PER: 0.0746
2026-01-13 23:45:45,988: t15.2024.05.10 val PER: 0.1204
2026-01-13 23:45:45,988: t15.2024.06.14 val PER: 0.1341
2026-01-13 23:45:45,988: t15.2024.07.19 val PER: 0.1668
2026-01-13 23:45:45,989: t15.2024.07.21 val PER: 0.0614
2026-01-13 23:45:45,989: t15.2024.07.28 val PER: 0.0919
2026-01-13 23:45:45,989: t15.2025.01.10 val PER: 0.2521
2026-01-13 23:45:45,989: t15.2025.01.12 val PER: 0.0931
2026-01-13 23:45:45,989: t15.2025.03.14 val PER: 0.3047
2026-01-13 23:45:45,989: t15.2025.03.16 val PER: 0.1361
2026-01-13 23:45:45,989: t15.2025.03.30 val PER: 0.2230
2026-01-13 23:45:45,989: t15.2025.04.13 val PER: 0.1826
2026-01-13 23:45:46,130: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_55500
2026-01-13 23:45:54,862: Train batch 55600: loss: 0.73 grad norm: 55.40 time: 0.056
2026-01-13 23:46:12,542: Train batch 55800: loss: 0.17 grad norm: 9.63 time: 0.058
2026-01-13 23:46:30,162: Train batch 56000: loss: 0.19 grad norm: 10.77 time: 0.053
2026-01-13 23:46:30,162: Running test after training batch: 56000
2026-01-13 23:46:30,269: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:46:35,059: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:46:35,119: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost out
2026-01-13 23:46:49,250: Val batch 56000: PER (avg): 0.1056 CTC Loss (avg): 18.7802 WER(5gram): 19.17% (n=256) time: 19.088
2026-01-13 23:46:49,251: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=13
2026-01-13 23:46:49,251: t15.2023.08.13 val PER: 0.0780
2026-01-13 23:46:49,251: t15.2023.08.18 val PER: 0.0729
2026-01-13 23:46:49,251: t15.2023.08.20 val PER: 0.0548
2026-01-13 23:46:49,251: t15.2023.08.25 val PER: 0.0693
2026-01-13 23:46:49,252: t15.2023.08.27 val PER: 0.1318
2026-01-13 23:46:49,252: t15.2023.09.01 val PER: 0.0503
2026-01-13 23:46:49,252: t15.2023.09.03 val PER: 0.1259
2026-01-13 23:46:49,252: t15.2023.09.24 val PER: 0.0934
2026-01-13 23:46:49,252: t15.2023.09.29 val PER: 0.0957
2026-01-13 23:46:49,252: t15.2023.10.01 val PER: 0.1347
2026-01-13 23:46:49,252: t15.2023.10.06 val PER: 0.0463
2026-01-13 23:46:49,252: t15.2023.10.08 val PER: 0.2016
2026-01-13 23:46:49,252: t15.2023.10.13 val PER: 0.1404
2026-01-13 23:46:49,253: t15.2023.10.15 val PER: 0.1048
2026-01-13 23:46:49,253: t15.2023.10.20 val PER: 0.1510
2026-01-13 23:46:49,253: t15.2023.10.22 val PER: 0.0835
2026-01-13 23:46:49,253: t15.2023.11.03 val PER: 0.1418
2026-01-13 23:46:49,253: t15.2023.11.04 val PER: 0.0205
2026-01-13 23:46:49,253: t15.2023.11.17 val PER: 0.0140
2026-01-13 23:46:49,253: t15.2023.11.19 val PER: 0.0180
2026-01-13 23:46:49,253: t15.2023.11.26 val PER: 0.0471
2026-01-13 23:46:49,253: t15.2023.12.03 val PER: 0.0494
2026-01-13 23:46:49,253: t15.2023.12.08 val PER: 0.0353
2026-01-13 23:46:49,253: t15.2023.12.10 val PER: 0.0394
2026-01-13 23:46:49,254: t15.2023.12.17 val PER: 0.0894
2026-01-13 23:46:49,254: t15.2023.12.29 val PER: 0.0755
2026-01-13 23:46:49,254: t15.2024.02.25 val PER: 0.0646
2026-01-13 23:46:49,254: t15.2024.03.08 val PER: 0.1607
2026-01-13 23:46:49,254: t15.2024.03.15 val PER: 0.1626
2026-01-13 23:46:49,254: t15.2024.03.17 val PER: 0.0788
2026-01-13 23:46:49,254: t15.2024.05.10 val PER: 0.1263
2026-01-13 23:46:49,254: t15.2024.06.14 val PER: 0.1215
2026-01-13 23:46:49,254: t15.2024.07.19 val PER: 0.1701
2026-01-13 23:46:49,254: t15.2024.07.21 val PER: 0.0572
2026-01-13 23:46:49,254: t15.2024.07.28 val PER: 0.1066
2026-01-13 23:46:49,254: t15.2025.01.10 val PER: 0.2479
2026-01-13 23:46:49,254: t15.2025.01.12 val PER: 0.0970
2026-01-13 23:46:49,254: t15.2025.03.14 val PER: 0.3166
2026-01-13 23:46:49,254: t15.2025.03.16 val PER: 0.1361
2026-01-13 23:46:49,254: t15.2025.03.30 val PER: 0.2333
2026-01-13 23:46:49,254: t15.2025.04.13 val PER: 0.1869
2026-01-13 23:46:49,395: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_56000
2026-01-13 23:47:06,883: Train batch 56200: loss: 0.31 grad norm: 17.18 time: 0.071
2026-01-13 23:47:24,104: Train batch 56400: loss: 0.56 grad norm: 21.33 time: 0.056
2026-01-13 23:47:32,657: Running test after training batch: 56500
2026-01-13 23:47:32,755: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:47:37,867: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:47:37,924: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-13 23:47:51,958: Val batch 56500: PER (avg): 0.1073 CTC Loss (avg): 19.0878 WER(5gram): 19.30% (n=256) time: 19.300
2026-01-13 23:47:51,958: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=14
2026-01-13 23:47:51,958: t15.2023.08.13 val PER: 0.0769
2026-01-13 23:47:51,958: t15.2023.08.18 val PER: 0.0746
2026-01-13 23:47:51,959: t15.2023.08.20 val PER: 0.0627
2026-01-13 23:47:51,959: t15.2023.08.25 val PER: 0.0798
2026-01-13 23:47:51,959: t15.2023.08.27 val PER: 0.1415
2026-01-13 23:47:51,959: t15.2023.09.01 val PER: 0.0487
2026-01-13 23:47:51,959: t15.2023.09.03 val PER: 0.1223
2026-01-13 23:47:51,959: t15.2023.09.24 val PER: 0.0959
2026-01-13 23:47:51,959: t15.2023.09.29 val PER: 0.1021
2026-01-13 23:47:51,959: t15.2023.10.01 val PER: 0.1413
2026-01-13 23:47:51,960: t15.2023.10.06 val PER: 0.0495
2026-01-13 23:47:51,960: t15.2023.10.08 val PER: 0.1881
2026-01-13 23:47:51,960: t15.2023.10.13 val PER: 0.1497
2026-01-13 23:47:51,960: t15.2023.10.15 val PER: 0.1088
2026-01-13 23:47:51,960: t15.2023.10.20 val PER: 0.1409
2026-01-13 23:47:51,960: t15.2023.10.22 val PER: 0.0835
2026-01-13 23:47:51,960: t15.2023.11.03 val PER: 0.1465
2026-01-13 23:47:51,960: t15.2023.11.04 val PER: 0.0239
2026-01-13 23:47:51,960: t15.2023.11.17 val PER: 0.0171
2026-01-13 23:47:51,961: t15.2023.11.19 val PER: 0.0140
2026-01-13 23:47:51,961: t15.2023.11.26 val PER: 0.0464
2026-01-13 23:47:51,961: t15.2023.12.03 val PER: 0.0546
2026-01-13 23:47:51,961: t15.2023.12.08 val PER: 0.0379
2026-01-13 23:47:51,961: t15.2023.12.10 val PER: 0.0434
2026-01-13 23:47:51,961: t15.2023.12.17 val PER: 0.0925
2026-01-13 23:47:51,961: t15.2023.12.29 val PER: 0.0865
2026-01-13 23:47:51,961: t15.2024.02.25 val PER: 0.0674
2026-01-13 23:47:51,961: t15.2024.03.08 val PER: 0.1778
2026-01-13 23:47:51,961: t15.2024.03.15 val PER: 0.1551
2026-01-13 23:47:51,962: t15.2024.03.17 val PER: 0.0809
2026-01-13 23:47:51,962: t15.2024.05.10 val PER: 0.1174
2026-01-13 23:47:51,962: t15.2024.06.14 val PER: 0.1230
2026-01-13 23:47:51,962: t15.2024.07.19 val PER: 0.1641
2026-01-13 23:47:51,962: t15.2024.07.21 val PER: 0.0607
2026-01-13 23:47:51,962: t15.2024.07.28 val PER: 0.1000
2026-01-13 23:47:51,962: t15.2025.01.10 val PER: 0.2466
2026-01-13 23:47:51,962: t15.2025.01.12 val PER: 0.0978
2026-01-13 23:47:51,962: t15.2025.03.14 val PER: 0.2944
2026-01-13 23:47:51,962: t15.2025.03.16 val PER: 0.1374
2026-01-13 23:47:51,963: t15.2025.03.30 val PER: 0.2448
2026-01-13 23:47:51,963: t15.2025.04.13 val PER: 0.1869
2026-01-13 23:47:52,106: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_56500
2026-01-13 23:48:00,924: Train batch 56600: loss: 0.30 grad norm: 17.25 time: 0.051
2026-01-13 23:48:18,112: Train batch 56800: loss: 0.15 grad norm: 92.43 time: 0.046
2026-01-13 23:48:35,976: Train batch 57000: loss: 0.26 grad norm: 14.37 time: 0.066
2026-01-13 23:48:35,976: Running test after training batch: 57000
2026-01-13 23:48:36,099: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:48:41,195: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:48:41,261: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost out
2026-01-13 23:48:56,587: Val batch 57000: PER (avg): 0.1062 CTC Loss (avg): 18.5900 WER(5gram): 19.43% (n=256) time: 20.610
2026-01-13 23:48:56,588: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=14
2026-01-13 23:48:56,588: t15.2023.08.13 val PER: 0.0759
2026-01-13 23:48:56,588: t15.2023.08.18 val PER: 0.0788
2026-01-13 23:48:56,588: t15.2023.08.20 val PER: 0.0635
2026-01-13 23:48:56,588: t15.2023.08.25 val PER: 0.0753
2026-01-13 23:48:56,588: t15.2023.08.27 val PER: 0.1399
2026-01-13 23:48:56,588: t15.2023.09.01 val PER: 0.0455
2026-01-13 23:48:56,588: t15.2023.09.03 val PER: 0.1223
2026-01-13 23:48:56,588: t15.2023.09.24 val PER: 0.0886
2026-01-13 23:48:56,588: t15.2023.09.29 val PER: 0.1002
2026-01-13 23:48:56,589: t15.2023.10.01 val PER: 0.1420
2026-01-13 23:48:56,589: t15.2023.10.06 val PER: 0.0527
2026-01-13 23:48:56,589: t15.2023.10.08 val PER: 0.1989
2026-01-13 23:48:56,589: t15.2023.10.13 val PER: 0.1451
2026-01-13 23:48:56,589: t15.2023.10.15 val PER: 0.1127
2026-01-13 23:48:56,589: t15.2023.10.20 val PER: 0.1544
2026-01-13 23:48:56,589: t15.2023.10.22 val PER: 0.0846
2026-01-13 23:48:56,589: t15.2023.11.03 val PER: 0.1418
2026-01-13 23:48:56,589: t15.2023.11.04 val PER: 0.0239
2026-01-13 23:48:56,589: t15.2023.11.17 val PER: 0.0202
2026-01-13 23:48:56,589: t15.2023.11.19 val PER: 0.0160
2026-01-13 23:48:56,589: t15.2023.11.26 val PER: 0.0442
2026-01-13 23:48:56,590: t15.2023.12.03 val PER: 0.0462
2026-01-13 23:48:56,590: t15.2023.12.08 val PER: 0.0379
2026-01-13 23:48:56,590: t15.2023.12.10 val PER: 0.0394
2026-01-13 23:48:56,590: t15.2023.12.17 val PER: 0.0873
2026-01-13 23:48:56,590: t15.2023.12.29 val PER: 0.0755
2026-01-13 23:48:56,590: t15.2024.02.25 val PER: 0.0702
2026-01-13 23:48:56,590: t15.2024.03.08 val PER: 0.1664
2026-01-13 23:48:56,590: t15.2024.03.15 val PER: 0.1588
2026-01-13 23:48:56,590: t15.2024.03.17 val PER: 0.0809
2026-01-13 23:48:56,590: t15.2024.05.10 val PER: 0.1263
2026-01-13 23:48:56,590: t15.2024.06.14 val PER: 0.1309
2026-01-13 23:48:56,590: t15.2024.07.19 val PER: 0.1608
2026-01-13 23:48:56,590: t15.2024.07.21 val PER: 0.0572
2026-01-13 23:48:56,590: t15.2024.07.28 val PER: 0.1015
2026-01-13 23:48:56,590: t15.2025.01.10 val PER: 0.2576
2026-01-13 23:48:56,591: t15.2025.01.12 val PER: 0.0955
2026-01-13 23:48:56,591: t15.2025.03.14 val PER: 0.2959
2026-01-13 23:48:56,591: t15.2025.03.16 val PER: 0.1348
2026-01-13 23:48:56,591: t15.2025.03.30 val PER: 0.2310
2026-01-13 23:48:56,591: t15.2025.04.13 val PER: 0.1783
2026-01-13 23:48:56,733: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_57000
2026-01-13 23:49:14,269: Train batch 57200: loss: 0.86 grad norm: 35.60 time: 0.056
2026-01-13 23:49:31,410: Train batch 57400: loss: 0.17 grad norm: 20.27 time: 0.067
2026-01-13 23:49:40,144: Running test after training batch: 57500
2026-01-13 23:49:40,237: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:49:45,055: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:49:45,125: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:50:00,302: Val batch 57500: PER (avg): 0.1051 CTC Loss (avg): 18.4922 WER(5gram): 19.75% (n=256) time: 20.158
2026-01-13 23:50:00,303: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=13
2026-01-13 23:50:00,303: t15.2023.08.13 val PER: 0.0738
2026-01-13 23:50:00,303: t15.2023.08.18 val PER: 0.0721
2026-01-13 23:50:00,303: t15.2023.08.20 val PER: 0.0635
2026-01-13 23:50:00,303: t15.2023.08.25 val PER: 0.0723
2026-01-13 23:50:00,303: t15.2023.08.27 val PER: 0.1559
2026-01-13 23:50:00,303: t15.2023.09.01 val PER: 0.0455
2026-01-13 23:50:00,304: t15.2023.09.03 val PER: 0.1354
2026-01-13 23:50:00,304: t15.2023.09.24 val PER: 0.0874
2026-01-13 23:50:00,304: t15.2023.09.29 val PER: 0.0996
2026-01-13 23:50:00,304: t15.2023.10.01 val PER: 0.1407
2026-01-13 23:50:00,304: t15.2023.10.06 val PER: 0.0484
2026-01-13 23:50:00,304: t15.2023.10.08 val PER: 0.1976
2026-01-13 23:50:00,304: t15.2023.10.13 val PER: 0.1482
2026-01-13 23:50:00,304: t15.2023.10.15 val PER: 0.1134
2026-01-13 23:50:00,304: t15.2023.10.20 val PER: 0.1443
2026-01-13 23:50:00,304: t15.2023.10.22 val PER: 0.0857
2026-01-13 23:50:00,304: t15.2023.11.03 val PER: 0.1411
2026-01-13 23:50:00,305: t15.2023.11.04 val PER: 0.0171
2026-01-13 23:50:00,305: t15.2023.11.17 val PER: 0.0140
2026-01-13 23:50:00,305: t15.2023.11.19 val PER: 0.0160
2026-01-13 23:50:00,305: t15.2023.11.26 val PER: 0.0478
2026-01-13 23:50:00,305: t15.2023.12.03 val PER: 0.0483
2026-01-13 23:50:00,305: t15.2023.12.08 val PER: 0.0360
2026-01-13 23:50:00,305: t15.2023.12.10 val PER: 0.0289
2026-01-13 23:50:00,305: t15.2023.12.17 val PER: 0.0894
2026-01-13 23:50:00,305: t15.2023.12.29 val PER: 0.0734
2026-01-13 23:50:00,305: t15.2024.02.25 val PER: 0.0590
2026-01-13 23:50:00,305: t15.2024.03.08 val PER: 0.1721
2026-01-13 23:50:00,306: t15.2024.03.15 val PER: 0.1526
2026-01-13 23:50:00,306: t15.2024.03.17 val PER: 0.0718
2026-01-13 23:50:00,306: t15.2024.05.10 val PER: 0.1174
2026-01-13 23:50:00,306: t15.2024.06.14 val PER: 0.1293
2026-01-13 23:50:00,306: t15.2024.07.19 val PER: 0.1641
2026-01-13 23:50:00,306: t15.2024.07.21 val PER: 0.0586
2026-01-13 23:50:00,306: t15.2024.07.28 val PER: 0.0971
2026-01-13 23:50:00,306: t15.2025.01.10 val PER: 0.2466
2026-01-13 23:50:00,306: t15.2025.01.12 val PER: 0.0962
2026-01-13 23:50:00,306: t15.2025.03.14 val PER: 0.3003
2026-01-13 23:50:00,306: t15.2025.03.16 val PER: 0.1296
2026-01-13 23:50:00,307: t15.2025.03.30 val PER: 0.2310
2026-01-13 23:50:00,307: t15.2025.04.13 val PER: 0.1840
2026-01-13 23:50:00,444: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_57500
2026-01-13 23:50:08,998: Train batch 57600: loss: 0.29 grad norm: 13.80 time: 0.055
2026-01-13 23:50:26,430: Train batch 57800: loss: 0.25 grad norm: 16.23 time: 0.052
2026-01-13 23:50:43,982: Train batch 58000: loss: 0.52 grad norm: 21.47 time: 0.064
2026-01-13 23:50:43,983: Running test after training batch: 58000
2026-01-13 23:50:44,150: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:50:48,998: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:50:49,050: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-13 23:51:03,033: Val batch 58000: PER (avg): 0.1059 CTC Loss (avg): 18.7729 WER(5gram): 19.95% (n=256) time: 19.050
2026-01-13 23:51:03,034: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=13
2026-01-13 23:51:03,034: t15.2023.08.13 val PER: 0.0790
2026-01-13 23:51:03,034: t15.2023.08.18 val PER: 0.0746
2026-01-13 23:51:03,034: t15.2023.08.20 val PER: 0.0643
2026-01-13 23:51:03,035: t15.2023.08.25 val PER: 0.0738
2026-01-13 23:51:03,035: t15.2023.08.27 val PER: 0.1431
2026-01-13 23:51:03,035: t15.2023.09.01 val PER: 0.0446
2026-01-13 23:51:03,035: t15.2023.09.03 val PER: 0.1354
2026-01-13 23:51:03,035: t15.2023.09.24 val PER: 0.0910
2026-01-13 23:51:03,035: t15.2023.09.29 val PER: 0.1021
2026-01-13 23:51:03,035: t15.2023.10.01 val PER: 0.1361
2026-01-13 23:51:03,035: t15.2023.10.06 val PER: 0.0527
2026-01-13 23:51:03,035: t15.2023.10.08 val PER: 0.2016
2026-01-13 23:51:03,035: t15.2023.10.13 val PER: 0.1490
2026-01-13 23:51:03,035: t15.2023.10.15 val PER: 0.1074
2026-01-13 23:51:03,035: t15.2023.10.20 val PER: 0.1443
2026-01-13 23:51:03,036: t15.2023.10.22 val PER: 0.0780
2026-01-13 23:51:03,036: t15.2023.11.03 val PER: 0.1438
2026-01-13 23:51:03,036: t15.2023.11.04 val PER: 0.0239
2026-01-13 23:51:03,036: t15.2023.11.17 val PER: 0.0140
2026-01-13 23:51:03,036: t15.2023.11.19 val PER: 0.0220
2026-01-13 23:51:03,036: t15.2023.11.26 val PER: 0.0464
2026-01-13 23:51:03,036: t15.2023.12.03 val PER: 0.0452
2026-01-13 23:51:03,036: t15.2023.12.08 val PER: 0.0379
2026-01-13 23:51:03,036: t15.2023.12.10 val PER: 0.0276
2026-01-13 23:51:03,036: t15.2023.12.17 val PER: 0.0852
2026-01-13 23:51:03,036: t15.2023.12.29 val PER: 0.0748
2026-01-13 23:51:03,036: t15.2024.02.25 val PER: 0.0702
2026-01-13 23:51:03,036: t15.2024.03.08 val PER: 0.1750
2026-01-13 23:51:03,037: t15.2024.03.15 val PER: 0.1538
2026-01-13 23:51:03,037: t15.2024.03.17 val PER: 0.0788
2026-01-13 23:51:03,037: t15.2024.05.10 val PER: 0.1248
2026-01-13 23:51:03,037: t15.2024.06.14 val PER: 0.1183
2026-01-13 23:51:03,037: t15.2024.07.19 val PER: 0.1674
2026-01-13 23:51:03,037: t15.2024.07.21 val PER: 0.0600
2026-01-13 23:51:03,037: t15.2024.07.28 val PER: 0.0949
2026-01-13 23:51:03,037: t15.2025.01.10 val PER: 0.2507
2026-01-13 23:51:03,037: t15.2025.01.12 val PER: 0.1008
2026-01-13 23:51:03,038: t15.2025.03.14 val PER: 0.3003
2026-01-13 23:51:03,038: t15.2025.03.16 val PER: 0.1335
2026-01-13 23:51:03,038: t15.2025.03.30 val PER: 0.2299
2026-01-13 23:51:03,038: t15.2025.04.13 val PER: 0.1869
2026-01-13 23:51:03,175: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_58000
2026-01-13 23:51:20,331: Train batch 58200: loss: 0.36 grad norm: 15.76 time: 0.070
2026-01-13 23:51:37,530: Train batch 58400: loss: 0.08 grad norm: 5.35 time: 0.055
2026-01-13 23:51:46,263: Running test after training batch: 58500
2026-01-13 23:51:46,397: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:51:51,466: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:51:51,535: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-13 23:52:06,842: Val batch 58500: PER (avg): 0.1054 CTC Loss (avg): 18.9083 WER(5gram): 20.40% (n=256) time: 20.579
2026-01-13 23:52:06,843: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-13 23:52:06,843: t15.2023.08.13 val PER: 0.0811
2026-01-13 23:52:06,843: t15.2023.08.18 val PER: 0.0738
2026-01-13 23:52:06,843: t15.2023.08.20 val PER: 0.0635
2026-01-13 23:52:06,843: t15.2023.08.25 val PER: 0.0663
2026-01-13 23:52:06,843: t15.2023.08.27 val PER: 0.1511
2026-01-13 23:52:06,843: t15.2023.09.01 val PER: 0.0446
2026-01-13 23:52:06,843: t15.2023.09.03 val PER: 0.1342
2026-01-13 23:52:06,843: t15.2023.09.24 val PER: 0.0862
2026-01-13 23:52:06,843: t15.2023.09.29 val PER: 0.0944
2026-01-13 23:52:06,844: t15.2023.10.01 val PER: 0.1334
2026-01-13 23:52:06,844: t15.2023.10.06 val PER: 0.0527
2026-01-13 23:52:06,846: t15.2023.10.08 val PER: 0.1976
2026-01-13 23:52:06,846: t15.2023.10.13 val PER: 0.1412
2026-01-13 23:52:06,846: t15.2023.10.15 val PER: 0.1022
2026-01-13 23:52:06,846: t15.2023.10.20 val PER: 0.1644
2026-01-13 23:52:06,846: t15.2023.10.22 val PER: 0.0768
2026-01-13 23:52:06,846: t15.2023.11.03 val PER: 0.1499
2026-01-13 23:52:06,846: t15.2023.11.04 val PER: 0.0239
2026-01-13 23:52:06,846: t15.2023.11.17 val PER: 0.0171
2026-01-13 23:52:06,846: t15.2023.11.19 val PER: 0.0200
2026-01-13 23:52:06,846: t15.2023.11.26 val PER: 0.0449
2026-01-13 23:52:06,846: t15.2023.12.03 val PER: 0.0504
2026-01-13 23:52:06,846: t15.2023.12.08 val PER: 0.0353
2026-01-13 23:52:06,847: t15.2023.12.10 val PER: 0.0302
2026-01-13 23:52:06,847: t15.2023.12.17 val PER: 0.0821
2026-01-13 23:52:06,847: t15.2023.12.29 val PER: 0.0776
2026-01-13 23:52:06,847: t15.2024.02.25 val PER: 0.0604
2026-01-13 23:52:06,847: t15.2024.03.08 val PER: 0.1636
2026-01-13 23:52:06,847: t15.2024.03.15 val PER: 0.1551
2026-01-13 23:52:06,847: t15.2024.03.17 val PER: 0.0739
2026-01-13 23:52:06,847: t15.2024.05.10 val PER: 0.1204
2026-01-13 23:52:06,847: t15.2024.06.14 val PER: 0.1088
2026-01-13 23:52:06,847: t15.2024.07.19 val PER: 0.1615
2026-01-13 23:52:06,847: t15.2024.07.21 val PER: 0.0607
2026-01-13 23:52:06,847: t15.2024.07.28 val PER: 0.1007
2026-01-13 23:52:06,847: t15.2025.01.10 val PER: 0.2603
2026-01-13 23:52:06,847: t15.2025.01.12 val PER: 0.1024
2026-01-13 23:52:06,847: t15.2025.03.14 val PER: 0.3151
2026-01-13 23:52:06,848: t15.2025.03.16 val PER: 0.1374
2026-01-13 23:52:06,848: t15.2025.03.30 val PER: 0.2402
2026-01-13 23:52:06,848: t15.2025.04.13 val PER: 0.1869
2026-01-13 23:52:06,983: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_58500
2026-01-13 23:52:15,763: Train batch 58600: loss: 0.66 grad norm: 22.00 time: 0.065
2026-01-13 23:52:33,192: Train batch 58800: loss: 0.37 grad norm: 19.25 time: 0.062
2026-01-13 23:52:50,674: Train batch 59000: loss: 0.36 grad norm: 17.37 time: 0.052
2026-01-13 23:52:50,674: Running test after training batch: 59000
2026-01-13 23:52:50,859: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:52:56,080: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:52:56,147: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-13 23:53:09,688: Val batch 59000: PER (avg): 0.1045 CTC Loss (avg): 18.6752 WER(5gram): 20.01% (n=256) time: 19.014
2026-01-13 23:53:09,688: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-13 23:53:09,688: t15.2023.08.13 val PER: 0.0759
2026-01-13 23:53:09,689: t15.2023.08.18 val PER: 0.0729
2026-01-13 23:53:09,689: t15.2023.08.20 val PER: 0.0643
2026-01-13 23:53:09,689: t15.2023.08.25 val PER: 0.0648
2026-01-13 23:53:09,689: t15.2023.08.27 val PER: 0.1543
2026-01-13 23:53:09,689: t15.2023.09.01 val PER: 0.0446
2026-01-13 23:53:09,689: t15.2023.09.03 val PER: 0.1271
2026-01-13 23:53:09,689: t15.2023.09.24 val PER: 0.0886
2026-01-13 23:53:09,689: t15.2023.09.29 val PER: 0.0996
2026-01-13 23:53:09,689: t15.2023.10.01 val PER: 0.1334
2026-01-13 23:53:09,690: t15.2023.10.06 val PER: 0.0452
2026-01-13 23:53:09,690: t15.2023.10.08 val PER: 0.1894
2026-01-13 23:53:09,690: t15.2023.10.13 val PER: 0.1521
2026-01-13 23:53:09,690: t15.2023.10.15 val PER: 0.1022
2026-01-13 23:53:09,690: t15.2023.10.20 val PER: 0.1611
2026-01-13 23:53:09,690: t15.2023.10.22 val PER: 0.0791
2026-01-13 23:53:09,690: t15.2023.11.03 val PER: 0.1445
2026-01-13 23:53:09,690: t15.2023.11.04 val PER: 0.0205
2026-01-13 23:53:09,690: t15.2023.11.17 val PER: 0.0140
2026-01-13 23:53:09,690: t15.2023.11.19 val PER: 0.0180
2026-01-13 23:53:09,690: t15.2023.11.26 val PER: 0.0420
2026-01-13 23:53:09,690: t15.2023.12.03 val PER: 0.0515
2026-01-13 23:53:09,690: t15.2023.12.08 val PER: 0.0399
2026-01-13 23:53:09,690: t15.2023.12.10 val PER: 0.0342
2026-01-13 23:53:09,690: t15.2023.12.17 val PER: 0.0863
2026-01-13 23:53:09,690: t15.2023.12.29 val PER: 0.0700
2026-01-13 23:53:09,691: t15.2024.02.25 val PER: 0.0730
2026-01-13 23:53:09,691: t15.2024.03.08 val PER: 0.1593
2026-01-13 23:53:09,691: t15.2024.03.15 val PER: 0.1513
2026-01-13 23:53:09,691: t15.2024.03.17 val PER: 0.0802
2026-01-13 23:53:09,691: t15.2024.05.10 val PER: 0.1218
2026-01-13 23:53:09,691: t15.2024.06.14 val PER: 0.1215
2026-01-13 23:53:09,691: t15.2024.07.19 val PER: 0.1668
2026-01-13 23:53:09,691: t15.2024.07.21 val PER: 0.0614
2026-01-13 23:53:09,691: t15.2024.07.28 val PER: 0.0963
2026-01-13 23:53:09,691: t15.2025.01.10 val PER: 0.2562
2026-01-13 23:53:09,691: t15.2025.01.12 val PER: 0.1024
2026-01-13 23:53:09,691: t15.2025.03.14 val PER: 0.3107
2026-01-13 23:53:09,691: t15.2025.03.16 val PER: 0.1178
2026-01-13 23:53:09,692: t15.2025.03.30 val PER: 0.2184
2026-01-13 23:53:09,692: t15.2025.04.13 val PER: 0.1769
2026-01-13 23:53:09,833: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_59000
2026-01-13 23:53:27,449: Train batch 59200: loss: 0.87 grad norm: 37.89 time: 0.085
2026-01-13 23:53:44,954: Train batch 59400: loss: 0.26 grad norm: 14.61 time: 0.082
2026-01-13 23:53:53,768: Running test after training batch: 59500
2026-01-13 23:53:53,915: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:53:58,716: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:53:58,779: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-13 23:54:13,502: Val batch 59500: PER (avg): 0.1034 CTC Loss (avg): 18.8349 WER(5gram): 19.10% (n=256) time: 19.734
2026-01-13 23:54:13,503: WER lens: avg_true_words=5.99 avg_pred_words=6.34 max_pred_words=13
2026-01-13 23:54:13,503: t15.2023.08.13 val PER: 0.0759
2026-01-13 23:54:13,503: t15.2023.08.18 val PER: 0.0729
2026-01-13 23:54:13,503: t15.2023.08.20 val PER: 0.0643
2026-01-13 23:54:13,503: t15.2023.08.25 val PER: 0.0738
2026-01-13 23:54:13,504: t15.2023.08.27 val PER: 0.1399
2026-01-13 23:54:13,504: t15.2023.09.01 val PER: 0.0430
2026-01-13 23:54:13,504: t15.2023.09.03 val PER: 0.1318
2026-01-13 23:54:13,504: t15.2023.09.24 val PER: 0.0898
2026-01-13 23:54:13,504: t15.2023.09.29 val PER: 0.0944
2026-01-13 23:54:13,504: t15.2023.10.01 val PER: 0.1394
2026-01-13 23:54:13,504: t15.2023.10.06 val PER: 0.0463
2026-01-13 23:54:13,504: t15.2023.10.08 val PER: 0.1827
2026-01-13 23:54:13,504: t15.2023.10.13 val PER: 0.1404
2026-01-13 23:54:13,504: t15.2023.10.15 val PER: 0.1035
2026-01-13 23:54:13,504: t15.2023.10.20 val PER: 0.1577
2026-01-13 23:54:13,504: t15.2023.10.22 val PER: 0.0802
2026-01-13 23:54:13,504: t15.2023.11.03 val PER: 0.1411
2026-01-13 23:54:13,504: t15.2023.11.04 val PER: 0.0239
2026-01-13 23:54:13,504: t15.2023.11.17 val PER: 0.0140
2026-01-13 23:54:13,504: t15.2023.11.19 val PER: 0.0160
2026-01-13 23:54:13,505: t15.2023.11.26 val PER: 0.0478
2026-01-13 23:54:13,505: t15.2023.12.03 val PER: 0.0494
2026-01-13 23:54:13,505: t15.2023.12.08 val PER: 0.0366
2026-01-13 23:54:13,505: t15.2023.12.10 val PER: 0.0342
2026-01-13 23:54:13,505: t15.2023.12.17 val PER: 0.0780
2026-01-13 23:54:13,505: t15.2023.12.29 val PER: 0.0748
2026-01-13 23:54:13,505: t15.2024.02.25 val PER: 0.0618
2026-01-13 23:54:13,505: t15.2024.03.08 val PER: 0.1707
2026-01-13 23:54:13,505: t15.2024.03.15 val PER: 0.1563
2026-01-13 23:54:13,505: t15.2024.03.17 val PER: 0.0739
2026-01-13 23:54:13,505: t15.2024.05.10 val PER: 0.1233
2026-01-13 23:54:13,505: t15.2024.06.14 val PER: 0.1025
2026-01-13 23:54:13,505: t15.2024.07.19 val PER: 0.1661
2026-01-13 23:54:13,505: t15.2024.07.21 val PER: 0.0586
2026-01-13 23:54:13,506: t15.2024.07.28 val PER: 0.0934
2026-01-13 23:54:13,506: t15.2025.01.10 val PER: 0.2466
2026-01-13 23:54:13,506: t15.2025.01.12 val PER: 0.0916
2026-01-13 23:54:13,506: t15.2025.03.14 val PER: 0.3107
2026-01-13 23:54:13,506: t15.2025.03.16 val PER: 0.1270
2026-01-13 23:54:13,506: t15.2025.03.30 val PER: 0.2322
2026-01-13 23:54:13,506: t15.2025.04.13 val PER: 0.1755
2026-01-13 23:54:13,650: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_59500
2026-01-13 23:54:22,390: Train batch 59600: loss: 0.45 grad norm: 18.40 time: 0.048
2026-01-13 23:54:40,202: Train batch 59800: loss: 0.69 grad norm: 21.87 time: 0.062
2026-01-13 23:54:57,951: Train batch 60000: loss: 0.20 grad norm: 12.36 time: 0.085
2026-01-13 23:54:57,951: Running test after training batch: 60000
2026-01-13 23:54:58,050: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:55:02,842: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:55:02,908: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:55:17,810: Val batch 60000: PER (avg): 0.1072 CTC Loss (avg): 19.1293 WER(5gram): 20.01% (n=256) time: 19.859
2026-01-13 23:55:17,810: WER lens: avg_true_words=5.99 avg_pred_words=6.34 max_pred_words=13
2026-01-13 23:55:17,811: t15.2023.08.13 val PER: 0.0790
2026-01-13 23:55:17,811: t15.2023.08.18 val PER: 0.0712
2026-01-13 23:55:17,811: t15.2023.08.20 val PER: 0.0635
2026-01-13 23:55:17,811: t15.2023.08.25 val PER: 0.0678
2026-01-13 23:55:17,811: t15.2023.08.27 val PER: 0.1479
2026-01-13 23:55:17,811: t15.2023.09.01 val PER: 0.0430
2026-01-13 23:55:17,811: t15.2023.09.03 val PER: 0.1330
2026-01-13 23:55:17,811: t15.2023.09.24 val PER: 0.0934
2026-01-13 23:55:17,811: t15.2023.09.29 val PER: 0.1002
2026-01-13 23:55:17,811: t15.2023.10.01 val PER: 0.1387
2026-01-13 23:55:17,811: t15.2023.10.06 val PER: 0.0527
2026-01-13 23:55:17,811: t15.2023.10.08 val PER: 0.2057
2026-01-13 23:55:17,811: t15.2023.10.13 val PER: 0.1482
2026-01-13 23:55:17,812: t15.2023.10.15 val PER: 0.1088
2026-01-13 23:55:17,812: t15.2023.10.20 val PER: 0.1678
2026-01-13 23:55:17,812: t15.2023.10.22 val PER: 0.0824
2026-01-13 23:55:17,812: t15.2023.11.03 val PER: 0.1472
2026-01-13 23:55:17,812: t15.2023.11.04 val PER: 0.0205
2026-01-13 23:55:17,812: t15.2023.11.17 val PER: 0.0187
2026-01-13 23:55:17,812: t15.2023.11.19 val PER: 0.0180
2026-01-13 23:55:17,812: t15.2023.11.26 val PER: 0.0478
2026-01-13 23:55:17,813: t15.2023.12.03 val PER: 0.0494
2026-01-13 23:55:17,813: t15.2023.12.08 val PER: 0.0366
2026-01-13 23:55:17,813: t15.2023.12.10 val PER: 0.0302
2026-01-13 23:55:17,813: t15.2023.12.17 val PER: 0.0915
2026-01-13 23:55:17,813: t15.2023.12.29 val PER: 0.0741
2026-01-13 23:55:17,813: t15.2024.02.25 val PER: 0.0646
2026-01-13 23:55:17,813: t15.2024.03.08 val PER: 0.1778
2026-01-13 23:55:17,813: t15.2024.03.15 val PER: 0.1513
2026-01-13 23:55:17,813: t15.2024.03.17 val PER: 0.0823
2026-01-13 23:55:17,813: t15.2024.05.10 val PER: 0.1174
2026-01-13 23:55:17,813: t15.2024.06.14 val PER: 0.1199
2026-01-13 23:55:17,813: t15.2024.07.19 val PER: 0.1674
2026-01-13 23:55:17,813: t15.2024.07.21 val PER: 0.0621
2026-01-13 23:55:17,813: t15.2024.07.28 val PER: 0.1015
2026-01-13 23:55:17,813: t15.2025.01.10 val PER: 0.2590
2026-01-13 23:55:17,813: t15.2025.01.12 val PER: 0.1024
2026-01-13 23:55:17,814: t15.2025.03.14 val PER: 0.3062
2026-01-13 23:55:17,814: t15.2025.03.16 val PER: 0.1270
2026-01-13 23:55:17,814: t15.2025.03.30 val PER: 0.2448
2026-01-13 23:55:17,814: t15.2025.04.13 val PER: 0.1883
2026-01-13 23:55:17,961: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_60000
2026-01-13 23:55:35,410: Train batch 60200: loss: 0.20 grad norm: 15.84 time: 0.067
2026-01-13 23:55:52,461: Train batch 60400: loss: 0.31 grad norm: 17.25 time: 0.052
2026-01-13 23:56:00,928: Running test after training batch: 60500
2026-01-13 23:56:01,090: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:56:05,951: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:56:06,008: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:56:21,274: Val batch 60500: PER (avg): 0.1037 CTC Loss (avg): 18.6874 WER(5gram): 18.25% (n=256) time: 20.346
2026-01-13 23:56:21,275: WER lens: avg_true_words=5.99 avg_pred_words=6.34 max_pred_words=13
2026-01-13 23:56:21,275: t15.2023.08.13 val PER: 0.0759
2026-01-13 23:56:21,275: t15.2023.08.18 val PER: 0.0679
2026-01-13 23:56:21,275: t15.2023.08.20 val PER: 0.0588
2026-01-13 23:56:21,275: t15.2023.08.25 val PER: 0.0633
2026-01-13 23:56:21,275: t15.2023.08.27 val PER: 0.1447
2026-01-13 23:56:21,275: t15.2023.09.01 val PER: 0.0398
2026-01-13 23:56:21,275: t15.2023.09.03 val PER: 0.1235
2026-01-13 23:56:21,276: t15.2023.09.24 val PER: 0.0862
2026-01-13 23:56:21,276: t15.2023.09.29 val PER: 0.0976
2026-01-13 23:56:21,276: t15.2023.10.01 val PER: 0.1347
2026-01-13 23:56:21,276: t15.2023.10.06 val PER: 0.0506
2026-01-13 23:56:21,276: t15.2023.10.08 val PER: 0.1989
2026-01-13 23:56:21,276: t15.2023.10.13 val PER: 0.1427
2026-01-13 23:56:21,276: t15.2023.10.15 val PER: 0.1055
2026-01-13 23:56:21,276: t15.2023.10.20 val PER: 0.1510
2026-01-13 23:56:21,276: t15.2023.10.22 val PER: 0.0746
2026-01-13 23:56:21,276: t15.2023.11.03 val PER: 0.1418
2026-01-13 23:56:21,276: t15.2023.11.04 val PER: 0.0205
2026-01-13 23:56:21,276: t15.2023.11.17 val PER: 0.0171
2026-01-13 23:56:21,276: t15.2023.11.19 val PER: 0.0180
2026-01-13 23:56:21,276: t15.2023.11.26 val PER: 0.0420
2026-01-13 23:56:21,276: t15.2023.12.03 val PER: 0.0494
2026-01-13 23:56:21,277: t15.2023.12.08 val PER: 0.0360
2026-01-13 23:56:21,277: t15.2023.12.10 val PER: 0.0342
2026-01-13 23:56:21,277: t15.2023.12.17 val PER: 0.0790
2026-01-13 23:56:21,277: t15.2023.12.29 val PER: 0.0686
2026-01-13 23:56:21,277: t15.2024.02.25 val PER: 0.0702
2026-01-13 23:56:21,277: t15.2024.03.08 val PER: 0.1792
2026-01-13 23:56:21,277: t15.2024.03.15 val PER: 0.1463
2026-01-13 23:56:21,277: t15.2024.03.17 val PER: 0.0767
2026-01-13 23:56:21,277: t15.2024.05.10 val PER: 0.1218
2026-01-13 23:56:21,277: t15.2024.06.14 val PER: 0.1183
2026-01-13 23:56:21,277: t15.2024.07.19 val PER: 0.1734
2026-01-13 23:56:21,277: t15.2024.07.21 val PER: 0.0607
2026-01-13 23:56:21,277: t15.2024.07.28 val PER: 0.0956
2026-01-13 23:56:21,277: t15.2025.01.10 val PER: 0.2645
2026-01-13 23:56:21,277: t15.2025.01.12 val PER: 0.1039
2026-01-13 23:56:21,278: t15.2025.03.14 val PER: 0.3062
2026-01-13 23:56:21,278: t15.2025.03.16 val PER: 0.1243
2026-01-13 23:56:21,278: t15.2025.03.30 val PER: 0.2207
2026-01-13 23:56:21,278: t15.2025.04.13 val PER: 0.1797
2026-01-13 23:56:21,419: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_60500
2026-01-13 23:56:30,241: Train batch 60600: loss: 0.31 grad norm: 15.44 time: 0.086
2026-01-13 23:56:47,517: Train batch 60800: loss: 0.36 grad norm: 15.61 time: 0.081
2026-01-13 23:57:05,059: Train batch 61000: loss: 0.23 grad norm: 14.76 time: 0.067
2026-01-13 23:57:05,060: Running test after training batch: 61000
2026-01-13 23:57:05,196: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:57:10,103: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:57:10,157: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:57:23,854: Val batch 61000: PER (avg): 0.1052 CTC Loss (avg): 18.9706 WER(5gram): 19.17% (n=256) time: 18.794
2026-01-13 23:57:23,855: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-13 23:57:23,855: t15.2023.08.13 val PER: 0.0759
2026-01-13 23:57:23,855: t15.2023.08.18 val PER: 0.0662
2026-01-13 23:57:23,855: t15.2023.08.20 val PER: 0.0651
2026-01-13 23:57:23,855: t15.2023.08.25 val PER: 0.0663
2026-01-13 23:57:23,855: t15.2023.08.27 val PER: 0.1399
2026-01-13 23:57:23,857: t15.2023.09.01 val PER: 0.0438
2026-01-13 23:57:23,857: t15.2023.09.03 val PER: 0.1295
2026-01-13 23:57:23,857: t15.2023.09.24 val PER: 0.0910
2026-01-13 23:57:23,857: t15.2023.09.29 val PER: 0.0944
2026-01-13 23:57:23,857: t15.2023.10.01 val PER: 0.1374
2026-01-13 23:57:23,857: t15.2023.10.06 val PER: 0.0495
2026-01-13 23:57:23,857: t15.2023.10.08 val PER: 0.1922
2026-01-13 23:57:23,858: t15.2023.10.13 val PER: 0.1552
2026-01-13 23:57:23,858: t15.2023.10.15 val PER: 0.1094
2026-01-13 23:57:23,858: t15.2023.10.20 val PER: 0.1443
2026-01-13 23:57:23,858: t15.2023.10.22 val PER: 0.0780
2026-01-13 23:57:23,858: t15.2023.11.03 val PER: 0.1445
2026-01-13 23:57:23,858: t15.2023.11.04 val PER: 0.0239
2026-01-13 23:57:23,858: t15.2023.11.17 val PER: 0.0171
2026-01-13 23:57:23,858: t15.2023.11.19 val PER: 0.0100
2026-01-13 23:57:23,858: t15.2023.11.26 val PER: 0.0449
2026-01-13 23:57:23,858: t15.2023.12.03 val PER: 0.0536
2026-01-13 23:57:23,858: t15.2023.12.08 val PER: 0.0386
2026-01-13 23:57:23,858: t15.2023.12.10 val PER: 0.0315
2026-01-13 23:57:23,859: t15.2023.12.17 val PER: 0.0894
2026-01-13 23:57:23,859: t15.2023.12.29 val PER: 0.0741
2026-01-13 23:57:23,859: t15.2024.02.25 val PER: 0.0702
2026-01-13 23:57:23,859: t15.2024.03.08 val PER: 0.1750
2026-01-13 23:57:23,859: t15.2024.03.15 val PER: 0.1520
2026-01-13 23:57:23,859: t15.2024.03.17 val PER: 0.0816
2026-01-13 23:57:23,859: t15.2024.05.10 val PER: 0.1263
2026-01-13 23:57:23,859: t15.2024.06.14 val PER: 0.1183
2026-01-13 23:57:23,859: t15.2024.07.19 val PER: 0.1608
2026-01-13 23:57:23,859: t15.2024.07.21 val PER: 0.0607
2026-01-13 23:57:23,859: t15.2024.07.28 val PER: 0.0838
2026-01-13 23:57:23,859: t15.2025.01.10 val PER: 0.2576
2026-01-13 23:57:23,859: t15.2025.01.12 val PER: 0.1016
2026-01-13 23:57:23,859: t15.2025.03.14 val PER: 0.3062
2026-01-13 23:57:23,859: t15.2025.03.16 val PER: 0.1309
2026-01-13 23:57:23,860: t15.2025.03.30 val PER: 0.2310
2026-01-13 23:57:23,860: t15.2025.04.13 val PER: 0.1969
2026-01-13 23:57:24,004: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_61000
2026-01-13 23:57:41,547: Train batch 61200: loss: 0.18 grad norm: 13.91 time: 0.060
2026-01-13 23:57:58,923: Train batch 61400: loss: 0.22 grad norm: 13.71 time: 0.062
2026-01-13 23:58:07,607: Running test after training batch: 61500
2026-01-13 23:58:07,740: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:58:12,557: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:58:12,619: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-13 23:58:27,915: Val batch 61500: PER (avg): 0.1048 CTC Loss (avg): 19.0090 WER(5gram): 19.49% (n=256) time: 20.308
2026-01-13 23:58:27,916: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=13
2026-01-13 23:58:27,916: t15.2023.08.13 val PER: 0.0759
2026-01-13 23:58:27,916: t15.2023.08.18 val PER: 0.0662
2026-01-13 23:58:27,916: t15.2023.08.20 val PER: 0.0548
2026-01-13 23:58:27,916: t15.2023.08.25 val PER: 0.0753
2026-01-13 23:58:27,916: t15.2023.08.27 val PER: 0.1463
2026-01-13 23:58:27,916: t15.2023.09.01 val PER: 0.0446
2026-01-13 23:58:27,916: t15.2023.09.03 val PER: 0.1200
2026-01-13 23:58:27,917: t15.2023.09.24 val PER: 0.0850
2026-01-13 23:58:27,917: t15.2023.09.29 val PER: 0.0944
2026-01-13 23:58:27,917: t15.2023.10.01 val PER: 0.1367
2026-01-13 23:58:27,917: t15.2023.10.06 val PER: 0.0495
2026-01-13 23:58:27,917: t15.2023.10.08 val PER: 0.1989
2026-01-13 23:58:27,917: t15.2023.10.13 val PER: 0.1412
2026-01-13 23:58:27,917: t15.2023.10.15 val PER: 0.1048
2026-01-13 23:58:27,917: t15.2023.10.20 val PER: 0.1510
2026-01-13 23:58:27,918: t15.2023.10.22 val PER: 0.0824
2026-01-13 23:58:27,918: t15.2023.11.03 val PER: 0.1452
2026-01-13 23:58:27,918: t15.2023.11.04 val PER: 0.0239
2026-01-13 23:58:27,918: t15.2023.11.17 val PER: 0.0202
2026-01-13 23:58:27,918: t15.2023.11.19 val PER: 0.0180
2026-01-13 23:58:27,918: t15.2023.11.26 val PER: 0.0500
2026-01-13 23:58:27,918: t15.2023.12.03 val PER: 0.0504
2026-01-13 23:58:27,918: t15.2023.12.08 val PER: 0.0386
2026-01-13 23:58:27,918: t15.2023.12.10 val PER: 0.0329
2026-01-13 23:58:27,918: t15.2023.12.17 val PER: 0.0780
2026-01-13 23:58:27,919: t15.2023.12.29 val PER: 0.0721
2026-01-13 23:58:27,919: t15.2024.02.25 val PER: 0.0604
2026-01-13 23:58:27,919: t15.2024.03.08 val PER: 0.1750
2026-01-13 23:58:27,919: t15.2024.03.15 val PER: 0.1526
2026-01-13 23:58:27,919: t15.2024.03.17 val PER: 0.0830
2026-01-13 23:58:27,919: t15.2024.05.10 val PER: 0.1293
2026-01-13 23:58:27,919: t15.2024.06.14 val PER: 0.1104
2026-01-13 23:58:27,919: t15.2024.07.19 val PER: 0.1635
2026-01-13 23:58:27,919: t15.2024.07.21 val PER: 0.0572
2026-01-13 23:58:27,920: t15.2024.07.28 val PER: 0.0919
2026-01-13 23:58:27,920: t15.2025.01.10 val PER: 0.2700
2026-01-13 23:58:27,920: t15.2025.01.12 val PER: 0.1047
2026-01-13 23:58:27,920: t15.2025.03.14 val PER: 0.3107
2026-01-13 23:58:27,920: t15.2025.03.16 val PER: 0.1296
2026-01-13 23:58:27,920: t15.2025.03.30 val PER: 0.2322
2026-01-13 23:58:27,920: t15.2025.04.13 val PER: 0.1897
2026-01-13 23:58:28,062: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_61500
2026-01-13 23:58:37,139: Train batch 61600: loss: 0.61 grad norm: 24.75 time: 0.060
2026-01-13 23:58:55,011: Train batch 61800: loss: 0.28 grad norm: 17.49 time: 0.078
2026-01-13 23:59:12,619: Train batch 62000: loss: 0.37 grad norm: 18.93 time: 0.065
2026-01-13 23:59:12,619: Running test after training batch: 62000
2026-01-13 23:59:12,710: WER debug GT example: You can see the code at this point as well.
2026-01-13 23:59:17,521: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-13 23:59:17,583: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cent
2026-01-13 23:59:32,581: Val batch 62000: PER (avg): 0.1051 CTC Loss (avg): 18.8686 WER(5gram): 19.95% (n=256) time: 19.961
2026-01-13 23:59:32,582: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=13
2026-01-13 23:59:32,582: t15.2023.08.13 val PER: 0.0728
2026-01-13 23:59:32,582: t15.2023.08.18 val PER: 0.0679
2026-01-13 23:59:32,582: t15.2023.08.20 val PER: 0.0627
2026-01-13 23:59:32,582: t15.2023.08.25 val PER: 0.0738
2026-01-13 23:59:32,582: t15.2023.08.27 val PER: 0.1559
2026-01-13 23:59:32,582: t15.2023.09.01 val PER: 0.0446
2026-01-13 23:59:32,583: t15.2023.09.03 val PER: 0.1271
2026-01-13 23:59:32,583: t15.2023.09.24 val PER: 0.0886
2026-01-13 23:59:32,583: t15.2023.09.29 val PER: 0.0970
2026-01-13 23:59:32,583: t15.2023.10.01 val PER: 0.1321
2026-01-13 23:59:32,583: t15.2023.10.06 val PER: 0.0495
2026-01-13 23:59:32,583: t15.2023.10.08 val PER: 0.1881
2026-01-13 23:59:32,583: t15.2023.10.13 val PER: 0.1482
2026-01-13 23:59:32,583: t15.2023.10.15 val PER: 0.1022
2026-01-13 23:59:32,583: t15.2023.10.20 val PER: 0.1577
2026-01-13 23:59:32,583: t15.2023.10.22 val PER: 0.0780
2026-01-13 23:59:32,583: t15.2023.11.03 val PER: 0.1472
2026-01-13 23:59:32,584: t15.2023.11.04 val PER: 0.0205
2026-01-13 23:59:32,584: t15.2023.11.17 val PER: 0.0218
2026-01-13 23:59:32,584: t15.2023.11.19 val PER: 0.0220
2026-01-13 23:59:32,584: t15.2023.11.26 val PER: 0.0420
2026-01-13 23:59:32,584: t15.2023.12.03 val PER: 0.0504
2026-01-13 23:59:32,584: t15.2023.12.08 val PER: 0.0393
2026-01-13 23:59:32,584: t15.2023.12.10 val PER: 0.0342
2026-01-13 23:59:32,584: t15.2023.12.17 val PER: 0.0925
2026-01-13 23:59:32,584: t15.2023.12.29 val PER: 0.0714
2026-01-13 23:59:32,584: t15.2024.02.25 val PER: 0.0688
2026-01-13 23:59:32,584: t15.2024.03.08 val PER: 0.1821
2026-01-13 23:59:32,585: t15.2024.03.15 val PER: 0.1532
2026-01-13 23:59:32,585: t15.2024.03.17 val PER: 0.0767
2026-01-13 23:59:32,585: t15.2024.05.10 val PER: 0.1293
2026-01-13 23:59:32,585: t15.2024.06.14 val PER: 0.1183
2026-01-13 23:59:32,585: t15.2024.07.19 val PER: 0.1688
2026-01-13 23:59:32,585: t15.2024.07.21 val PER: 0.0586
2026-01-13 23:59:32,585: t15.2024.07.28 val PER: 0.0956
2026-01-13 23:59:32,585: t15.2025.01.10 val PER: 0.2521
2026-01-13 23:59:32,585: t15.2025.01.12 val PER: 0.0939
2026-01-13 23:59:32,586: t15.2025.03.14 val PER: 0.3077
2026-01-13 23:59:32,586: t15.2025.03.16 val PER: 0.1322
2026-01-13 23:59:32,586: t15.2025.03.30 val PER: 0.2276
2026-01-13 23:59:32,586: t15.2025.04.13 val PER: 0.1883
2026-01-13 23:59:32,731: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_62000
2026-01-13 23:59:50,188: Train batch 62200: loss: 0.23 grad norm: 19.37 time: 0.062
2026-01-14 00:00:07,710: Train batch 62400: loss: 0.16 grad norm: 13.80 time: 0.068
2026-01-14 00:00:16,503: Running test after training batch: 62500
2026-01-14 00:00:16,593: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:00:21,369: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:00:21,437: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cent
2026-01-14 00:00:36,311: Val batch 62500: PER (avg): 0.1049 CTC Loss (avg): 19.1705 WER(5gram): 19.17% (n=256) time: 19.808
2026-01-14 00:00:36,312: WER lens: avg_true_words=5.99 avg_pred_words=6.32 max_pred_words=14
2026-01-14 00:00:36,312: t15.2023.08.13 val PER: 0.0738
2026-01-14 00:00:36,312: t15.2023.08.18 val PER: 0.0704
2026-01-14 00:00:36,312: t15.2023.08.20 val PER: 0.0627
2026-01-14 00:00:36,313: t15.2023.08.25 val PER: 0.0753
2026-01-14 00:00:36,313: t15.2023.08.27 val PER: 0.1495
2026-01-14 00:00:36,313: t15.2023.09.01 val PER: 0.0422
2026-01-14 00:00:36,313: t15.2023.09.03 val PER: 0.1128
2026-01-14 00:00:36,313: t15.2023.09.24 val PER: 0.0947
2026-01-14 00:00:36,313: t15.2023.09.29 val PER: 0.0996
2026-01-14 00:00:36,313: t15.2023.10.01 val PER: 0.1334
2026-01-14 00:00:36,313: t15.2023.10.06 val PER: 0.0506
2026-01-14 00:00:36,313: t15.2023.10.08 val PER: 0.1962
2026-01-14 00:00:36,313: t15.2023.10.13 val PER: 0.1404
2026-01-14 00:00:36,313: t15.2023.10.15 val PER: 0.1042
2026-01-14 00:00:36,313: t15.2023.10.20 val PER: 0.1544
2026-01-14 00:00:36,314: t15.2023.10.22 val PER: 0.0857
2026-01-14 00:00:36,314: t15.2023.11.03 val PER: 0.1431
2026-01-14 00:00:36,314: t15.2023.11.04 val PER: 0.0239
2026-01-14 00:00:36,314: t15.2023.11.17 val PER: 0.0202
2026-01-14 00:00:36,314: t15.2023.11.19 val PER: 0.0259
2026-01-14 00:00:36,314: t15.2023.11.26 val PER: 0.0420
2026-01-14 00:00:36,314: t15.2023.12.03 val PER: 0.0567
2026-01-14 00:00:36,314: t15.2023.12.08 val PER: 0.0340
2026-01-14 00:00:36,314: t15.2023.12.10 val PER: 0.0394
2026-01-14 00:00:36,314: t15.2023.12.17 val PER: 0.0936
2026-01-14 00:00:36,314: t15.2023.12.29 val PER: 0.0728
2026-01-14 00:00:36,314: t15.2024.02.25 val PER: 0.0660
2026-01-14 00:00:36,314: t15.2024.03.08 val PER: 0.1792
2026-01-14 00:00:36,314: t15.2024.03.15 val PER: 0.1570
2026-01-14 00:00:36,315: t15.2024.03.17 val PER: 0.0837
2026-01-14 00:00:36,315: t15.2024.05.10 val PER: 0.1129
2026-01-14 00:00:36,315: t15.2024.06.14 val PER: 0.1325
2026-01-14 00:00:36,315: t15.2024.07.19 val PER: 0.1701
2026-01-14 00:00:36,315: t15.2024.07.21 val PER: 0.0579
2026-01-14 00:00:36,315: t15.2024.07.28 val PER: 0.0904
2026-01-14 00:00:36,315: t15.2025.01.10 val PER: 0.2397
2026-01-14 00:00:36,315: t15.2025.01.12 val PER: 0.0924
2026-01-14 00:00:36,315: t15.2025.03.14 val PER: 0.3033
2026-01-14 00:00:36,315: t15.2025.03.16 val PER: 0.1309
2026-01-14 00:00:36,315: t15.2025.03.30 val PER: 0.2253
2026-01-14 00:00:36,315: t15.2025.04.13 val PER: 0.1854
2026-01-14 00:00:36,459: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_62500
2026-01-14 00:00:45,306: Train batch 62600: loss: 0.38 grad norm: 21.97 time: 0.048
2026-01-14 00:01:02,994: Train batch 62800: loss: 0.70 grad norm: 33.56 time: 0.069
2026-01-14 00:01:20,674: Train batch 63000: loss: 0.23 grad norm: 13.21 time: 0.055
2026-01-14 00:01:20,674: Running test after training batch: 63000
2026-01-14 00:01:20,814: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:01:26,195: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:01:26,259: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-14 00:01:41,268: Val batch 63000: PER (avg): 0.1036 CTC Loss (avg): 18.9276 WER(5gram): 19.49% (n=256) time: 20.594
2026-01-14 00:01:41,269: WER lens: avg_true_words=5.99 avg_pred_words=6.34 max_pred_words=13
2026-01-14 00:01:41,269: t15.2023.08.13 val PER: 0.0790
2026-01-14 00:01:41,269: t15.2023.08.18 val PER: 0.0645
2026-01-14 00:01:41,269: t15.2023.08.20 val PER: 0.0596
2026-01-14 00:01:41,269: t15.2023.08.25 val PER: 0.0633
2026-01-14 00:01:41,269: t15.2023.08.27 val PER: 0.1447
2026-01-14 00:01:41,269: t15.2023.09.01 val PER: 0.0455
2026-01-14 00:01:41,269: t15.2023.09.03 val PER: 0.1116
2026-01-14 00:01:41,269: t15.2023.09.24 val PER: 0.0850
2026-01-14 00:01:41,270: t15.2023.09.29 val PER: 0.0970
2026-01-14 00:01:41,270: t15.2023.10.01 val PER: 0.1347
2026-01-14 00:01:41,270: t15.2023.10.06 val PER: 0.0549
2026-01-14 00:01:41,270: t15.2023.10.08 val PER: 0.1908
2026-01-14 00:01:41,270: t15.2023.10.13 val PER: 0.1334
2026-01-14 00:01:41,270: t15.2023.10.15 val PER: 0.1042
2026-01-14 00:01:41,270: t15.2023.10.20 val PER: 0.1678
2026-01-14 00:01:41,270: t15.2023.10.22 val PER: 0.0824
2026-01-14 00:01:41,270: t15.2023.11.03 val PER: 0.1391
2026-01-14 00:01:41,270: t15.2023.11.04 val PER: 0.0239
2026-01-14 00:01:41,270: t15.2023.11.17 val PER: 0.0140
2026-01-14 00:01:41,270: t15.2023.11.19 val PER: 0.0220
2026-01-14 00:01:41,270: t15.2023.11.26 val PER: 0.0449
2026-01-14 00:01:41,270: t15.2023.12.03 val PER: 0.0441
2026-01-14 00:01:41,271: t15.2023.12.08 val PER: 0.0379
2026-01-14 00:01:41,271: t15.2023.12.10 val PER: 0.0315
2026-01-14 00:01:41,271: t15.2023.12.17 val PER: 0.0873
2026-01-14 00:01:41,271: t15.2023.12.29 val PER: 0.0782
2026-01-14 00:01:41,271: t15.2024.02.25 val PER: 0.0688
2026-01-14 00:01:41,271: t15.2024.03.08 val PER: 0.1693
2026-01-14 00:01:41,271: t15.2024.03.15 val PER: 0.1513
2026-01-14 00:01:41,271: t15.2024.03.17 val PER: 0.0760
2026-01-14 00:01:41,272: t15.2024.05.10 val PER: 0.1189
2026-01-14 00:01:41,272: t15.2024.06.14 val PER: 0.1309
2026-01-14 00:01:41,272: t15.2024.07.19 val PER: 0.1648
2026-01-14 00:01:41,272: t15.2024.07.21 val PER: 0.0586
2026-01-14 00:01:41,272: t15.2024.07.28 val PER: 0.1037
2026-01-14 00:01:41,272: t15.2025.01.10 val PER: 0.2438
2026-01-14 00:01:41,272: t15.2025.01.12 val PER: 0.1001
2026-01-14 00:01:41,272: t15.2025.03.14 val PER: 0.3033
2026-01-14 00:01:41,272: t15.2025.03.16 val PER: 0.1270
2026-01-14 00:01:41,272: t15.2025.03.30 val PER: 0.2230
2026-01-14 00:01:41,272: t15.2025.04.13 val PER: 0.1840
2026-01-14 00:01:41,411: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_63000
2026-01-14 00:01:58,784: Train batch 63200: loss: 0.24 grad norm: 13.78 time: 0.061
2026-01-14 00:02:16,557: Train batch 63400: loss: 0.50 grad norm: 20.95 time: 0.069
2026-01-14 00:02:25,420: Running test after training batch: 63500
2026-01-14 00:02:25,565: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:02:30,386: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:02:30,452: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-14 00:02:44,670: Val batch 63500: PER (avg): 0.1037 CTC Loss (avg): 19.3315 WER(5gram): 18.25% (n=256) time: 19.250
2026-01-14 00:02:44,671: WER lens: avg_true_words=5.99 avg_pred_words=6.30 max_pred_words=13
2026-01-14 00:02:44,671: t15.2023.08.13 val PER: 0.0821
2026-01-14 00:02:44,671: t15.2023.08.18 val PER: 0.0721
2026-01-14 00:02:44,671: t15.2023.08.20 val PER: 0.0588
2026-01-14 00:02:44,671: t15.2023.08.25 val PER: 0.0648
2026-01-14 00:02:44,672: t15.2023.08.27 val PER: 0.1527
2026-01-14 00:02:44,672: t15.2023.09.01 val PER: 0.0422
2026-01-14 00:02:44,672: t15.2023.09.03 val PER: 0.1200
2026-01-14 00:02:44,672: t15.2023.09.24 val PER: 0.0922
2026-01-14 00:02:44,672: t15.2023.09.29 val PER: 0.0944
2026-01-14 00:02:44,672: t15.2023.10.01 val PER: 0.1328
2026-01-14 00:02:44,672: t15.2023.10.06 val PER: 0.0495
2026-01-14 00:02:44,672: t15.2023.10.08 val PER: 0.1976
2026-01-14 00:02:44,672: t15.2023.10.13 val PER: 0.1458
2026-01-14 00:02:44,672: t15.2023.10.15 val PER: 0.1035
2026-01-14 00:02:44,672: t15.2023.10.20 val PER: 0.1577
2026-01-14 00:02:44,672: t15.2023.10.22 val PER: 0.0802
2026-01-14 00:02:44,672: t15.2023.11.03 val PER: 0.1404
2026-01-14 00:02:44,672: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:02:44,673: t15.2023.11.17 val PER: 0.0187
2026-01-14 00:02:44,673: t15.2023.11.19 val PER: 0.0180
2026-01-14 00:02:44,673: t15.2023.11.26 val PER: 0.0457
2026-01-14 00:02:44,673: t15.2023.12.03 val PER: 0.0494
2026-01-14 00:02:44,673: t15.2023.12.08 val PER: 0.0393
2026-01-14 00:02:44,673: t15.2023.12.10 val PER: 0.0342
2026-01-14 00:02:44,673: t15.2023.12.17 val PER: 0.0894
2026-01-14 00:02:44,673: t15.2023.12.29 val PER: 0.0734
2026-01-14 00:02:44,673: t15.2024.02.25 val PER: 0.0646
2026-01-14 00:02:44,673: t15.2024.03.08 val PER: 0.1735
2026-01-14 00:02:44,673: t15.2024.03.15 val PER: 0.1470
2026-01-14 00:02:44,673: t15.2024.03.17 val PER: 0.0704
2026-01-14 00:02:44,673: t15.2024.05.10 val PER: 0.1174
2026-01-14 00:02:44,673: t15.2024.06.14 val PER: 0.1341
2026-01-14 00:02:44,674: t15.2024.07.19 val PER: 0.1628
2026-01-14 00:02:44,674: t15.2024.07.21 val PER: 0.0593
2026-01-14 00:02:44,674: t15.2024.07.28 val PER: 0.0949
2026-01-14 00:02:44,674: t15.2025.01.10 val PER: 0.2548
2026-01-14 00:02:44,674: t15.2025.01.12 val PER: 0.0955
2026-01-14 00:02:44,674: t15.2025.03.14 val PER: 0.2959
2026-01-14 00:02:44,674: t15.2025.03.16 val PER: 0.1322
2026-01-14 00:02:44,674: t15.2025.03.30 val PER: 0.2253
2026-01-14 00:02:44,675: t15.2025.04.13 val PER: 0.1769
2026-01-14 00:02:44,815: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_63500
2026-01-14 00:02:53,427: Train batch 63600: loss: 0.09 grad norm: 5.05 time: 0.067
2026-01-14 00:03:10,642: Train batch 63800: loss: 0.25 grad norm: 16.60 time: 0.061
2026-01-14 00:03:28,135: Train batch 64000: loss: 0.25 grad norm: 14.03 time: 0.067
2026-01-14 00:03:28,135: Running test after training batch: 64000
2026-01-14 00:03:28,273: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:03:33,076: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:03:33,135: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-14 00:03:47,582: Val batch 64000: PER (avg): 0.1031 CTC Loss (avg): 18.8725 WER(5gram): 20.47% (n=256) time: 19.446
2026-01-14 00:03:47,582: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=13
2026-01-14 00:03:47,582: t15.2023.08.13 val PER: 0.0842
2026-01-14 00:03:47,582: t15.2023.08.18 val PER: 0.0696
2026-01-14 00:03:47,583: t15.2023.08.20 val PER: 0.0620
2026-01-14 00:03:47,583: t15.2023.08.25 val PER: 0.0678
2026-01-14 00:03:47,583: t15.2023.08.27 val PER: 0.1479
2026-01-14 00:03:47,583: t15.2023.09.01 val PER: 0.0463
2026-01-14 00:03:47,583: t15.2023.09.03 val PER: 0.1211
2026-01-14 00:03:47,583: t15.2023.09.24 val PER: 0.0886
2026-01-14 00:03:47,583: t15.2023.09.29 val PER: 0.0964
2026-01-14 00:03:47,583: t15.2023.10.01 val PER: 0.1295
2026-01-14 00:03:47,584: t15.2023.10.06 val PER: 0.0484
2026-01-14 00:03:47,584: t15.2023.10.08 val PER: 0.1935
2026-01-14 00:03:47,584: t15.2023.10.13 val PER: 0.1389
2026-01-14 00:03:47,584: t15.2023.10.15 val PER: 0.1061
2026-01-14 00:03:47,584: t15.2023.10.20 val PER: 0.1510
2026-01-14 00:03:47,584: t15.2023.10.22 val PER: 0.0813
2026-01-14 00:03:47,584: t15.2023.11.03 val PER: 0.1438
2026-01-14 00:03:47,584: t15.2023.11.04 val PER: 0.0273
2026-01-14 00:03:47,585: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:03:47,585: t15.2023.11.19 val PER: 0.0200
2026-01-14 00:03:47,585: t15.2023.11.26 val PER: 0.0442
2026-01-14 00:03:47,585: t15.2023.12.03 val PER: 0.0515
2026-01-14 00:03:47,585: t15.2023.12.08 val PER: 0.0306
2026-01-14 00:03:47,585: t15.2023.12.10 val PER: 0.0289
2026-01-14 00:03:47,585: t15.2023.12.17 val PER: 0.0821
2026-01-14 00:03:47,585: t15.2023.12.29 val PER: 0.0700
2026-01-14 00:03:47,585: t15.2024.02.25 val PER: 0.0618
2026-01-14 00:03:47,585: t15.2024.03.08 val PER: 0.1622
2026-01-14 00:03:47,586: t15.2024.03.15 val PER: 0.1488
2026-01-14 00:03:47,586: t15.2024.03.17 val PER: 0.0725
2026-01-14 00:03:47,586: t15.2024.05.10 val PER: 0.1159
2026-01-14 00:03:47,586: t15.2024.06.14 val PER: 0.1199
2026-01-14 00:03:47,586: t15.2024.07.19 val PER: 0.1668
2026-01-14 00:03:47,586: t15.2024.07.21 val PER: 0.0566
2026-01-14 00:03:47,586: t15.2024.07.28 val PER: 0.0904
2026-01-14 00:03:47,586: t15.2025.01.10 val PER: 0.2548
2026-01-14 00:03:47,586: t15.2025.01.12 val PER: 0.0993
2026-01-14 00:03:47,586: t15.2025.03.14 val PER: 0.3003
2026-01-14 00:03:47,587: t15.2025.03.16 val PER: 0.1322
2026-01-14 00:03:47,587: t15.2025.03.30 val PER: 0.2356
2026-01-14 00:03:47,587: t15.2025.04.13 val PER: 0.1854
2026-01-14 00:03:47,726: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_64000
2026-01-14 00:04:05,087: Train batch 64200: loss: 0.04 grad norm: 2.68 time: 0.069
2026-01-14 00:04:22,736: Train batch 64400: loss: 0.32 grad norm: 21.62 time: 0.056
2026-01-14 00:04:31,411: Running test after training batch: 64500
2026-01-14 00:04:31,511: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:04:36,594: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:04:36,651: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:04:50,621: Val batch 64500: PER (avg): 0.1034 CTC Loss (avg): 19.0260 WER(5gram): 20.01% (n=256) time: 19.209
2026-01-14 00:04:50,621: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=13
2026-01-14 00:04:50,621: t15.2023.08.13 val PER: 0.0728
2026-01-14 00:04:50,621: t15.2023.08.18 val PER: 0.0696
2026-01-14 00:04:50,622: t15.2023.08.20 val PER: 0.0627
2026-01-14 00:04:50,622: t15.2023.08.25 val PER: 0.0693
2026-01-14 00:04:50,622: t15.2023.08.27 val PER: 0.1511
2026-01-14 00:04:50,622: t15.2023.09.01 val PER: 0.0430
2026-01-14 00:04:50,622: t15.2023.09.03 val PER: 0.1318
2026-01-14 00:04:50,622: t15.2023.09.24 val PER: 0.0862
2026-01-14 00:04:50,622: t15.2023.09.29 val PER: 0.0944
2026-01-14 00:04:50,622: t15.2023.10.01 val PER: 0.1347
2026-01-14 00:04:50,622: t15.2023.10.06 val PER: 0.0495
2026-01-14 00:04:50,623: t15.2023.10.08 val PER: 0.1922
2026-01-14 00:04:50,623: t15.2023.10.13 val PER: 0.1412
2026-01-14 00:04:50,623: t15.2023.10.15 val PER: 0.0989
2026-01-14 00:04:50,623: t15.2023.10.20 val PER: 0.1510
2026-01-14 00:04:50,623: t15.2023.10.22 val PER: 0.0835
2026-01-14 00:04:50,623: t15.2023.11.03 val PER: 0.1384
2026-01-14 00:04:50,623: t15.2023.11.04 val PER: 0.0273
2026-01-14 00:04:50,623: t15.2023.11.17 val PER: 0.0156
2026-01-14 00:04:50,623: t15.2023.11.19 val PER: 0.0220
2026-01-14 00:04:50,623: t15.2023.11.26 val PER: 0.0435
2026-01-14 00:04:50,623: t15.2023.12.03 val PER: 0.0588
2026-01-14 00:04:50,623: t15.2023.12.08 val PER: 0.0353
2026-01-14 00:04:50,623: t15.2023.12.10 val PER: 0.0329
2026-01-14 00:04:50,623: t15.2023.12.17 val PER: 0.0863
2026-01-14 00:04:50,623: t15.2023.12.29 val PER: 0.0707
2026-01-14 00:04:50,623: t15.2024.02.25 val PER: 0.0716
2026-01-14 00:04:50,623: t15.2024.03.08 val PER: 0.1792
2026-01-14 00:04:50,624: t15.2024.03.15 val PER: 0.1501
2026-01-14 00:04:50,624: t15.2024.03.17 val PER: 0.0725
2026-01-14 00:04:50,624: t15.2024.05.10 val PER: 0.1218
2026-01-14 00:04:50,624: t15.2024.06.14 val PER: 0.1183
2026-01-14 00:04:50,624: t15.2024.07.19 val PER: 0.1602
2026-01-14 00:04:50,624: t15.2024.07.21 val PER: 0.0552
2026-01-14 00:04:50,624: t15.2024.07.28 val PER: 0.0941
2026-01-14 00:04:50,624: t15.2025.01.10 val PER: 0.2562
2026-01-14 00:04:50,624: t15.2025.01.12 val PER: 0.0970
2026-01-14 00:04:50,624: t15.2025.03.14 val PER: 0.3047
2026-01-14 00:04:50,624: t15.2025.03.16 val PER: 0.1335
2026-01-14 00:04:50,624: t15.2025.03.30 val PER: 0.2287
2026-01-14 00:04:50,624: t15.2025.04.13 val PER: 0.1712
2026-01-14 00:04:50,766: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_64500
2026-01-14 00:04:59,554: Train batch 64600: loss: 0.59 grad norm: 30.25 time: 0.073
2026-01-14 00:05:16,896: Train batch 64800: loss: 0.35 grad norm: 16.69 time: 0.059
2026-01-14 00:05:34,074: Train batch 65000: loss: 0.08 grad norm: 8.54 time: 0.054
2026-01-14 00:05:34,075: Running test after training batch: 65000
2026-01-14 00:05:34,217: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:05:39,013: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:05:39,084: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:05:54,617: Val batch 65000: PER (avg): 0.1036 CTC Loss (avg): 19.1191 WER(5gram): 19.23% (n=256) time: 20.542
2026-01-14 00:05:54,617: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=13
2026-01-14 00:05:54,617: t15.2023.08.13 val PER: 0.0759
2026-01-14 00:05:54,617: t15.2023.08.18 val PER: 0.0629
2026-01-14 00:05:54,617: t15.2023.08.20 val PER: 0.0612
2026-01-14 00:05:54,617: t15.2023.08.25 val PER: 0.0783
2026-01-14 00:05:54,618: t15.2023.08.27 val PER: 0.1399
2026-01-14 00:05:54,618: t15.2023.09.01 val PER: 0.0430
2026-01-14 00:05:54,618: t15.2023.09.03 val PER: 0.1176
2026-01-14 00:05:54,618: t15.2023.09.24 val PER: 0.0898
2026-01-14 00:05:54,618: t15.2023.09.29 val PER: 0.0970
2026-01-14 00:05:54,618: t15.2023.10.01 val PER: 0.1347
2026-01-14 00:05:54,618: t15.2023.10.06 val PER: 0.0484
2026-01-14 00:05:54,618: t15.2023.10.08 val PER: 0.1976
2026-01-14 00:05:54,618: t15.2023.10.13 val PER: 0.1497
2026-01-14 00:05:54,618: t15.2023.10.15 val PER: 0.0989
2026-01-14 00:05:54,619: t15.2023.10.20 val PER: 0.1510
2026-01-14 00:05:54,619: t15.2023.10.22 val PER: 0.0846
2026-01-14 00:05:54,619: t15.2023.11.03 val PER: 0.1336
2026-01-14 00:05:54,619: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:05:54,619: t15.2023.11.17 val PER: 0.0124
2026-01-14 00:05:54,619: t15.2023.11.19 val PER: 0.0180
2026-01-14 00:05:54,619: t15.2023.11.26 val PER: 0.0464
2026-01-14 00:05:54,619: t15.2023.12.03 val PER: 0.0504
2026-01-14 00:05:54,619: t15.2023.12.08 val PER: 0.0386
2026-01-14 00:05:54,619: t15.2023.12.10 val PER: 0.0368
2026-01-14 00:05:54,619: t15.2023.12.17 val PER: 0.0790
2026-01-14 00:05:54,619: t15.2023.12.29 val PER: 0.0734
2026-01-14 00:05:54,619: t15.2024.02.25 val PER: 0.0688
2026-01-14 00:05:54,619: t15.2024.03.08 val PER: 0.1664
2026-01-14 00:05:54,619: t15.2024.03.15 val PER: 0.1476
2026-01-14 00:05:54,619: t15.2024.03.17 val PER: 0.0844
2026-01-14 00:05:54,620: t15.2024.05.10 val PER: 0.1248
2026-01-14 00:05:54,620: t15.2024.06.14 val PER: 0.1151
2026-01-14 00:05:54,620: t15.2024.07.19 val PER: 0.1688
2026-01-14 00:05:54,620: t15.2024.07.21 val PER: 0.0579
2026-01-14 00:05:54,620: t15.2024.07.28 val PER: 0.0926
2026-01-14 00:05:54,620: t15.2025.01.10 val PER: 0.2562
2026-01-14 00:05:54,620: t15.2025.01.12 val PER: 0.0939
2026-01-14 00:05:54,620: t15.2025.03.14 val PER: 0.3077
2026-01-14 00:05:54,620: t15.2025.03.16 val PER: 0.1322
2026-01-14 00:05:54,620: t15.2025.03.30 val PER: 0.2184
2026-01-14 00:05:54,620: t15.2025.04.13 val PER: 0.1883
2026-01-14 00:05:54,768: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_65000
2026-01-14 00:06:12,049: Train batch 65200: loss: 0.08 grad norm: 4.98 time: 0.062
2026-01-14 00:06:29,673: Train batch 65400: loss: 0.15 grad norm: 10.16 time: 0.062
2026-01-14 00:06:38,408: Running test after training batch: 65500
2026-01-14 00:06:38,691: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:06:43,504: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:06:43,569: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost sounds
2026-01-14 00:06:58,324: Val batch 65500: PER (avg): 0.1020 CTC Loss (avg): 18.7911 WER(5gram): 18.71% (n=256) time: 19.916
2026-01-14 00:06:58,324: WER lens: avg_true_words=5.99 avg_pred_words=6.33 max_pred_words=13
2026-01-14 00:06:58,325: t15.2023.08.13 val PER: 0.0738
2026-01-14 00:06:58,325: t15.2023.08.18 val PER: 0.0645
2026-01-14 00:06:58,325: t15.2023.08.20 val PER: 0.0604
2026-01-14 00:06:58,325: t15.2023.08.25 val PER: 0.0753
2026-01-14 00:06:58,325: t15.2023.08.27 val PER: 0.1350
2026-01-14 00:06:58,325: t15.2023.09.01 val PER: 0.0446
2026-01-14 00:06:58,325: t15.2023.09.03 val PER: 0.1093
2026-01-14 00:06:58,325: t15.2023.09.24 val PER: 0.0886
2026-01-14 00:06:58,325: t15.2023.09.29 val PER: 0.0964
2026-01-14 00:06:58,325: t15.2023.10.01 val PER: 0.1334
2026-01-14 00:06:58,325: t15.2023.10.06 val PER: 0.0506
2026-01-14 00:06:58,325: t15.2023.10.08 val PER: 0.1935
2026-01-14 00:06:58,325: t15.2023.10.13 val PER: 0.1427
2026-01-14 00:06:58,325: t15.2023.10.15 val PER: 0.1035
2026-01-14 00:06:58,326: t15.2023.10.20 val PER: 0.1477
2026-01-14 00:06:58,326: t15.2023.10.22 val PER: 0.0780
2026-01-14 00:06:58,326: t15.2023.11.03 val PER: 0.1398
2026-01-14 00:06:58,326: t15.2023.11.04 val PER: 0.0273
2026-01-14 00:06:58,326: t15.2023.11.17 val PER: 0.0187
2026-01-14 00:06:58,326: t15.2023.11.19 val PER: 0.0160
2026-01-14 00:06:58,326: t15.2023.11.26 val PER: 0.0428
2026-01-14 00:06:58,326: t15.2023.12.03 val PER: 0.0441
2026-01-14 00:06:58,326: t15.2023.12.08 val PER: 0.0340
2026-01-14 00:06:58,326: t15.2023.12.10 val PER: 0.0342
2026-01-14 00:06:58,326: t15.2023.12.17 val PER: 0.0842
2026-01-14 00:06:58,326: t15.2023.12.29 val PER: 0.0679
2026-01-14 00:06:58,326: t15.2024.02.25 val PER: 0.0646
2026-01-14 00:06:58,327: t15.2024.03.08 val PER: 0.1750
2026-01-14 00:06:58,327: t15.2024.03.15 val PER: 0.1476
2026-01-14 00:06:58,327: t15.2024.03.17 val PER: 0.0753
2026-01-14 00:06:58,327: t15.2024.05.10 val PER: 0.1278
2026-01-14 00:06:58,327: t15.2024.06.14 val PER: 0.1199
2026-01-14 00:06:58,327: t15.2024.07.19 val PER: 0.1589
2026-01-14 00:06:58,327: t15.2024.07.21 val PER: 0.0593
2026-01-14 00:06:58,327: t15.2024.07.28 val PER: 0.0934
2026-01-14 00:06:58,327: t15.2025.01.10 val PER: 0.2576
2026-01-14 00:06:58,327: t15.2025.01.12 val PER: 0.0893
2026-01-14 00:06:58,327: t15.2025.03.14 val PER: 0.3018
2026-01-14 00:06:58,327: t15.2025.03.16 val PER: 0.1204
2026-01-14 00:06:58,327: t15.2025.03.30 val PER: 0.2287
2026-01-14 00:06:58,327: t15.2025.04.13 val PER: 0.1826
2026-01-14 00:06:58,464: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_65500
2026-01-14 00:07:07,511: Train batch 65600: loss: 0.23 grad norm: 14.98 time: 0.090
2026-01-14 00:07:25,474: Train batch 65800: loss: 0.24 grad norm: 26.98 time: 0.073
2026-01-14 00:07:43,533: Train batch 66000: loss: 0.13 grad norm: 8.65 time: 0.071
2026-01-14 00:07:43,533: Running test after training batch: 66000
2026-01-14 00:07:43,626: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:07:48,420: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:07:48,479: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost doubt
2026-01-14 00:08:03,350: Val batch 66000: PER (avg): 0.1032 CTC Loss (avg): 19.0631 WER(5gram): 19.56% (n=256) time: 19.817
2026-01-14 00:08:03,351: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-14 00:08:03,351: t15.2023.08.13 val PER: 0.0748
2026-01-14 00:08:03,351: t15.2023.08.18 val PER: 0.0637
2026-01-14 00:08:03,351: t15.2023.08.20 val PER: 0.0596
2026-01-14 00:08:03,351: t15.2023.08.25 val PER: 0.0753
2026-01-14 00:08:03,351: t15.2023.08.27 val PER: 0.1334
2026-01-14 00:08:03,352: t15.2023.09.01 val PER: 0.0446
2026-01-14 00:08:03,352: t15.2023.09.03 val PER: 0.1152
2026-01-14 00:08:03,352: t15.2023.09.24 val PER: 0.0898
2026-01-14 00:08:03,352: t15.2023.09.29 val PER: 0.0996
2026-01-14 00:08:03,352: t15.2023.10.01 val PER: 0.1341
2026-01-14 00:08:03,352: t15.2023.10.06 val PER: 0.0484
2026-01-14 00:08:03,352: t15.2023.10.08 val PER: 0.2003
2026-01-14 00:08:03,352: t15.2023.10.13 val PER: 0.1458
2026-01-14 00:08:03,352: t15.2023.10.15 val PER: 0.1009
2026-01-14 00:08:03,353: t15.2023.10.20 val PER: 0.1611
2026-01-14 00:08:03,353: t15.2023.10.22 val PER: 0.0835
2026-01-14 00:08:03,353: t15.2023.11.03 val PER: 0.1370
2026-01-14 00:08:03,353: t15.2023.11.04 val PER: 0.0273
2026-01-14 00:08:03,353: t15.2023.11.17 val PER: 0.0187
2026-01-14 00:08:03,353: t15.2023.11.19 val PER: 0.0200
2026-01-14 00:08:03,353: t15.2023.11.26 val PER: 0.0435
2026-01-14 00:08:03,353: t15.2023.12.03 val PER: 0.0483
2026-01-14 00:08:03,353: t15.2023.12.08 val PER: 0.0366
2026-01-14 00:08:03,354: t15.2023.12.10 val PER: 0.0381
2026-01-14 00:08:03,354: t15.2023.12.17 val PER: 0.0852
2026-01-14 00:08:03,354: t15.2023.12.29 val PER: 0.0693
2026-01-14 00:08:03,354: t15.2024.02.25 val PER: 0.0716
2026-01-14 00:08:03,354: t15.2024.03.08 val PER: 0.1792
2026-01-14 00:08:03,354: t15.2024.03.15 val PER: 0.1495
2026-01-14 00:08:03,354: t15.2024.03.17 val PER: 0.0753
2026-01-14 00:08:03,354: t15.2024.05.10 val PER: 0.1204
2026-01-14 00:08:03,354: t15.2024.06.14 val PER: 0.1167
2026-01-14 00:08:03,354: t15.2024.07.19 val PER: 0.1582
2026-01-14 00:08:03,355: t15.2024.07.21 val PER: 0.0579
2026-01-14 00:08:03,355: t15.2024.07.28 val PER: 0.0993
2026-01-14 00:08:03,355: t15.2025.01.10 val PER: 0.2466
2026-01-14 00:08:03,355: t15.2025.01.12 val PER: 0.1001
2026-01-14 00:08:03,355: t15.2025.03.14 val PER: 0.2959
2026-01-14 00:08:03,355: t15.2025.03.16 val PER: 0.1348
2026-01-14 00:08:03,355: t15.2025.03.30 val PER: 0.2218
2026-01-14 00:08:03,355: t15.2025.04.13 val PER: 0.1812
2026-01-14 00:08:03,492: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_66000
2026-01-14 00:08:21,125: Train batch 66200: loss: 0.35 grad norm: 26.93 time: 0.062
2026-01-14 00:08:38,435: Train batch 66400: loss: 0.26 grad norm: 16.16 time: 0.060
2026-01-14 00:08:47,207: Running test after training batch: 66500
2026-01-14 00:08:47,296: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:08:52,522: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:08:52,581: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:09:07,665: Val batch 66500: PER (avg): 0.1025 CTC Loss (avg): 19.0047 WER(5gram): 18.45% (n=256) time: 20.458
2026-01-14 00:09:07,666: WER lens: avg_true_words=5.99 avg_pred_words=6.33 max_pred_words=13
2026-01-14 00:09:07,666: t15.2023.08.13 val PER: 0.0748
2026-01-14 00:09:07,666: t15.2023.08.18 val PER: 0.0696
2026-01-14 00:09:07,666: t15.2023.08.20 val PER: 0.0548
2026-01-14 00:09:07,666: t15.2023.08.25 val PER: 0.0678
2026-01-14 00:09:07,667: t15.2023.08.27 val PER: 0.1431
2026-01-14 00:09:07,667: t15.2023.09.01 val PER: 0.0406
2026-01-14 00:09:07,667: t15.2023.09.03 val PER: 0.1176
2026-01-14 00:09:07,667: t15.2023.09.24 val PER: 0.0922
2026-01-14 00:09:07,667: t15.2023.09.29 val PER: 0.0957
2026-01-14 00:09:07,667: t15.2023.10.01 val PER: 0.1347
2026-01-14 00:09:07,667: t15.2023.10.06 val PER: 0.0484
2026-01-14 00:09:07,667: t15.2023.10.08 val PER: 0.1962
2026-01-14 00:09:07,667: t15.2023.10.13 val PER: 0.1427
2026-01-14 00:09:07,667: t15.2023.10.15 val PER: 0.0969
2026-01-14 00:09:07,668: t15.2023.10.20 val PER: 0.1477
2026-01-14 00:09:07,668: t15.2023.10.22 val PER: 0.0846
2026-01-14 00:09:07,668: t15.2023.11.03 val PER: 0.1404
2026-01-14 00:09:07,668: t15.2023.11.04 val PER: 0.0239
2026-01-14 00:09:07,668: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:09:07,668: t15.2023.11.19 val PER: 0.0180
2026-01-14 00:09:07,668: t15.2023.11.26 val PER: 0.0449
2026-01-14 00:09:07,668: t15.2023.12.03 val PER: 0.0441
2026-01-14 00:09:07,669: t15.2023.12.08 val PER: 0.0360
2026-01-14 00:09:07,669: t15.2023.12.10 val PER: 0.0407
2026-01-14 00:09:07,669: t15.2023.12.17 val PER: 0.0863
2026-01-14 00:09:07,669: t15.2023.12.29 val PER: 0.0666
2026-01-14 00:09:07,669: t15.2024.02.25 val PER: 0.0688
2026-01-14 00:09:07,669: t15.2024.03.08 val PER: 0.1650
2026-01-14 00:09:07,669: t15.2024.03.15 val PER: 0.1538
2026-01-14 00:09:07,669: t15.2024.03.17 val PER: 0.0760
2026-01-14 00:09:07,669: t15.2024.05.10 val PER: 0.1174
2026-01-14 00:09:07,669: t15.2024.06.14 val PER: 0.1230
2026-01-14 00:09:07,670: t15.2024.07.19 val PER: 0.1628
2026-01-14 00:09:07,670: t15.2024.07.21 val PER: 0.0586
2026-01-14 00:09:07,670: t15.2024.07.28 val PER: 0.0963
2026-01-14 00:09:07,670: t15.2025.01.10 val PER: 0.2479
2026-01-14 00:09:07,670: t15.2025.01.12 val PER: 0.0916
2026-01-14 00:09:07,670: t15.2025.03.14 val PER: 0.3062
2026-01-14 00:09:07,670: t15.2025.03.16 val PER: 0.1283
2026-01-14 00:09:07,670: t15.2025.03.30 val PER: 0.2184
2026-01-14 00:09:07,670: t15.2025.04.13 val PER: 0.1797
2026-01-14 00:09:07,806: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_66500
2026-01-14 00:09:17,271: Train batch 66600: loss: 0.18 grad norm: 13.38 time: 0.076
2026-01-14 00:09:35,079: Train batch 66800: loss: 0.31 grad norm: 28.98 time: 0.051
2026-01-14 00:09:52,854: Train batch 67000: loss: 0.34 grad norm: 17.02 time: 0.078
2026-01-14 00:09:52,855: Running test after training batch: 67000
2026-01-14 00:09:52,997: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:09:57,817: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:09:57,876: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:10:12,783: Val batch 67000: PER (avg): 0.1022 CTC Loss (avg): 18.9580 WER(5gram): 19.95% (n=256) time: 19.928
2026-01-14 00:10:12,783: WER lens: avg_true_words=5.99 avg_pred_words=6.39 max_pred_words=13
2026-01-14 00:10:12,783: t15.2023.08.13 val PER: 0.0717
2026-01-14 00:10:12,784: t15.2023.08.18 val PER: 0.0654
2026-01-14 00:10:12,784: t15.2023.08.20 val PER: 0.0532
2026-01-14 00:10:12,784: t15.2023.08.25 val PER: 0.0738
2026-01-14 00:10:12,784: t15.2023.08.27 val PER: 0.1479
2026-01-14 00:10:12,784: t15.2023.09.01 val PER: 0.0406
2026-01-14 00:10:12,784: t15.2023.09.03 val PER: 0.1128
2026-01-14 00:10:12,784: t15.2023.09.24 val PER: 0.0850
2026-01-14 00:10:12,784: t15.2023.09.29 val PER: 0.0976
2026-01-14 00:10:12,784: t15.2023.10.01 val PER: 0.1361
2026-01-14 00:10:12,784: t15.2023.10.06 val PER: 0.0474
2026-01-14 00:10:12,784: t15.2023.10.08 val PER: 0.1962
2026-01-14 00:10:12,784: t15.2023.10.13 val PER: 0.1427
2026-01-14 00:10:12,785: t15.2023.10.15 val PER: 0.1035
2026-01-14 00:10:12,785: t15.2023.10.20 val PER: 0.1510
2026-01-14 00:10:12,785: t15.2023.10.22 val PER: 0.0869
2026-01-14 00:10:12,785: t15.2023.11.03 val PER: 0.1404
2026-01-14 00:10:12,785: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:10:12,785: t15.2023.11.17 val PER: 0.0156
2026-01-14 00:10:12,785: t15.2023.11.19 val PER: 0.0200
2026-01-14 00:10:12,785: t15.2023.11.26 val PER: 0.0457
2026-01-14 00:10:12,785: t15.2023.12.03 val PER: 0.0462
2026-01-14 00:10:12,785: t15.2023.12.08 val PER: 0.0340
2026-01-14 00:10:12,785: t15.2023.12.10 val PER: 0.0329
2026-01-14 00:10:12,785: t15.2023.12.17 val PER: 0.0780
2026-01-14 00:10:12,785: t15.2023.12.29 val PER: 0.0728
2026-01-14 00:10:12,785: t15.2024.02.25 val PER: 0.0702
2026-01-14 00:10:12,785: t15.2024.03.08 val PER: 0.1622
2026-01-14 00:10:12,786: t15.2024.03.15 val PER: 0.1488
2026-01-14 00:10:12,786: t15.2024.03.17 val PER: 0.0732
2026-01-14 00:10:12,786: t15.2024.05.10 val PER: 0.1189
2026-01-14 00:10:12,786: t15.2024.06.14 val PER: 0.1167
2026-01-14 00:10:12,786: t15.2024.07.19 val PER: 0.1608
2026-01-14 00:10:12,786: t15.2024.07.21 val PER: 0.0607
2026-01-14 00:10:12,786: t15.2024.07.28 val PER: 0.0926
2026-01-14 00:10:12,786: t15.2025.01.10 val PER: 0.2548
2026-01-14 00:10:12,786: t15.2025.01.12 val PER: 0.1016
2026-01-14 00:10:12,786: t15.2025.03.14 val PER: 0.3077
2026-01-14 00:10:12,786: t15.2025.03.16 val PER: 0.1165
2026-01-14 00:10:12,786: t15.2025.03.30 val PER: 0.2299
2026-01-14 00:10:12,786: t15.2025.04.13 val PER: 0.1683
2026-01-14 00:10:12,928: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_67000
2026-01-14 00:10:30,358: Train batch 67200: loss: 0.19 grad norm: 14.50 time: 0.084
2026-01-14 00:10:48,554: Train batch 67400: loss: 0.15 grad norm: 12.33 time: 0.072
2026-01-14 00:10:57,577: Running test after training batch: 67500
2026-01-14 00:10:57,734: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:11:02,569: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:11:02,626: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost doubt
2026-01-14 00:11:17,520: Val batch 67500: PER (avg): 0.1021 CTC Loss (avg): 18.8523 WER(5gram): 20.34% (n=256) time: 19.942
2026-01-14 00:11:17,520: WER lens: avg_true_words=5.99 avg_pred_words=6.39 max_pred_words=13
2026-01-14 00:11:17,520: t15.2023.08.13 val PER: 0.0655
2026-01-14 00:11:17,520: t15.2023.08.18 val PER: 0.0687
2026-01-14 00:11:17,520: t15.2023.08.20 val PER: 0.0580
2026-01-14 00:11:17,520: t15.2023.08.25 val PER: 0.0663
2026-01-14 00:11:17,520: t15.2023.08.27 val PER: 0.1383
2026-01-14 00:11:17,520: t15.2023.09.01 val PER: 0.0390
2026-01-14 00:11:17,521: t15.2023.09.03 val PER: 0.1081
2026-01-14 00:11:17,521: t15.2023.09.24 val PER: 0.0874
2026-01-14 00:11:17,521: t15.2023.09.29 val PER: 0.0996
2026-01-14 00:11:17,521: t15.2023.10.01 val PER: 0.1361
2026-01-14 00:11:17,521: t15.2023.10.06 val PER: 0.0441
2026-01-14 00:11:17,521: t15.2023.10.08 val PER: 0.1894
2026-01-14 00:11:17,521: t15.2023.10.13 val PER: 0.1435
2026-01-14 00:11:17,521: t15.2023.10.15 val PER: 0.1068
2026-01-14 00:11:17,521: t15.2023.10.20 val PER: 0.1443
2026-01-14 00:11:17,521: t15.2023.10.22 val PER: 0.0791
2026-01-14 00:11:17,521: t15.2023.11.03 val PER: 0.1404
2026-01-14 00:11:17,521: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:11:17,522: t15.2023.11.17 val PER: 0.0187
2026-01-14 00:11:17,522: t15.2023.11.19 val PER: 0.0200
2026-01-14 00:11:17,522: t15.2023.11.26 val PER: 0.0435
2026-01-14 00:11:17,522: t15.2023.12.03 val PER: 0.0452
2026-01-14 00:11:17,522: t15.2023.12.08 val PER: 0.0346
2026-01-14 00:11:17,522: t15.2023.12.10 val PER: 0.0289
2026-01-14 00:11:17,522: t15.2023.12.17 val PER: 0.0832
2026-01-14 00:11:17,522: t15.2023.12.29 val PER: 0.0693
2026-01-14 00:11:17,522: t15.2024.02.25 val PER: 0.0646
2026-01-14 00:11:17,522: t15.2024.03.08 val PER: 0.1721
2026-01-14 00:11:17,522: t15.2024.03.15 val PER: 0.1501
2026-01-14 00:11:17,522: t15.2024.03.17 val PER: 0.0753
2026-01-14 00:11:17,523: t15.2024.05.10 val PER: 0.1248
2026-01-14 00:11:17,523: t15.2024.06.14 val PER: 0.1057
2026-01-14 00:11:17,523: t15.2024.07.19 val PER: 0.1575
2026-01-14 00:11:17,523: t15.2024.07.21 val PER: 0.0621
2026-01-14 00:11:17,523: t15.2024.07.28 val PER: 0.0993
2026-01-14 00:11:17,523: t15.2025.01.10 val PER: 0.2521
2026-01-14 00:11:17,523: t15.2025.01.12 val PER: 0.1008
2026-01-14 00:11:17,523: t15.2025.03.14 val PER: 0.2988
2026-01-14 00:11:17,523: t15.2025.03.16 val PER: 0.1283
2026-01-14 00:11:17,523: t15.2025.03.30 val PER: 0.2322
2026-01-14 00:11:17,523: t15.2025.04.13 val PER: 0.1755
2026-01-14 00:11:17,663: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_67500
2026-01-14 00:11:26,611: Train batch 67600: loss: 0.82 grad norm: 59.84 time: 0.089
2026-01-14 00:11:44,137: Train batch 67800: loss: 0.07 grad norm: 6.40 time: 0.064
2026-01-14 00:12:01,715: Train batch 68000: loss: 0.39 grad norm: 20.50 time: 0.063
2026-01-14 00:12:01,715: Running test after training batch: 68000
2026-01-14 00:12:01,809: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:12:06,849: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:12:06,906: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:12:21,554: Val batch 68000: PER (avg): 0.1029 CTC Loss (avg): 19.0903 WER(5gram): 20.08% (n=256) time: 19.838
2026-01-14 00:12:21,554: WER lens: avg_true_words=5.99 avg_pred_words=6.39 max_pred_words=13
2026-01-14 00:12:21,554: t15.2023.08.13 val PER: 0.0686
2026-01-14 00:12:21,555: t15.2023.08.18 val PER: 0.0712
2026-01-14 00:12:21,555: t15.2023.08.20 val PER: 0.0540
2026-01-14 00:12:21,555: t15.2023.08.25 val PER: 0.0753
2026-01-14 00:12:21,555: t15.2023.08.27 val PER: 0.1318
2026-01-14 00:12:21,555: t15.2023.09.01 val PER: 0.0390
2026-01-14 00:12:21,555: t15.2023.09.03 val PER: 0.1069
2026-01-14 00:12:21,555: t15.2023.09.24 val PER: 0.0934
2026-01-14 00:12:21,555: t15.2023.09.29 val PER: 0.0989
2026-01-14 00:12:21,555: t15.2023.10.01 val PER: 0.1374
2026-01-14 00:12:21,555: t15.2023.10.06 val PER: 0.0463
2026-01-14 00:12:21,555: t15.2023.10.08 val PER: 0.1949
2026-01-14 00:12:21,555: t15.2023.10.13 val PER: 0.1451
2026-01-14 00:12:21,556: t15.2023.10.15 val PER: 0.1022
2026-01-14 00:12:21,556: t15.2023.10.20 val PER: 0.1477
2026-01-14 00:12:21,556: t15.2023.10.22 val PER: 0.0846
2026-01-14 00:12:21,556: t15.2023.11.03 val PER: 0.1377
2026-01-14 00:12:21,556: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:12:21,556: t15.2023.11.17 val PER: 0.0156
2026-01-14 00:12:21,556: t15.2023.11.19 val PER: 0.0160
2026-01-14 00:12:21,556: t15.2023.11.26 val PER: 0.0442
2026-01-14 00:12:21,556: t15.2023.12.03 val PER: 0.0431
2026-01-14 00:12:21,557: t15.2023.12.08 val PER: 0.0373
2026-01-14 00:12:21,557: t15.2023.12.10 val PER: 0.0342
2026-01-14 00:12:21,557: t15.2023.12.17 val PER: 0.0904
2026-01-14 00:12:21,557: t15.2023.12.29 val PER: 0.0707
2026-01-14 00:12:21,557: t15.2024.02.25 val PER: 0.0632
2026-01-14 00:12:21,557: t15.2024.03.08 val PER: 0.1721
2026-01-14 00:12:21,557: t15.2024.03.15 val PER: 0.1538
2026-01-14 00:12:21,557: t15.2024.03.17 val PER: 0.0746
2026-01-14 00:12:21,558: t15.2024.05.10 val PER: 0.1218
2026-01-14 00:12:21,558: t15.2024.06.14 val PER: 0.1167
2026-01-14 00:12:21,558: t15.2024.07.19 val PER: 0.1589
2026-01-14 00:12:21,558: t15.2024.07.21 val PER: 0.0628
2026-01-14 00:12:21,558: t15.2024.07.28 val PER: 0.0934
2026-01-14 00:12:21,558: t15.2025.01.10 val PER: 0.2576
2026-01-14 00:12:21,558: t15.2025.01.12 val PER: 0.0955
2026-01-14 00:12:21,558: t15.2025.03.14 val PER: 0.3062
2026-01-14 00:12:21,558: t15.2025.03.16 val PER: 0.1296
2026-01-14 00:12:21,558: t15.2025.03.30 val PER: 0.2299
2026-01-14 00:12:21,558: t15.2025.04.13 val PER: 0.1869
2026-01-14 00:12:21,700: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_68000
2026-01-14 00:12:39,282: Train batch 68200: loss: 0.26 grad norm: 25.68 time: 0.055
2026-01-14 00:12:56,882: Train batch 68400: loss: 0.06 grad norm: 6.38 time: 0.055
2026-01-14 00:13:05,648: Running test after training batch: 68500
2026-01-14 00:13:05,797: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:13:10,989: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:13:11,053: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:13:25,262: Val batch 68500: PER (avg): 0.1027 CTC Loss (avg): 19.3551 WER(5gram): 20.66% (n=256) time: 19.613
2026-01-14 00:13:25,262: WER lens: avg_true_words=5.99 avg_pred_words=6.41 max_pred_words=13
2026-01-14 00:13:25,262: t15.2023.08.13 val PER: 0.0676
2026-01-14 00:13:25,262: t15.2023.08.18 val PER: 0.0729
2026-01-14 00:13:25,263: t15.2023.08.20 val PER: 0.0564
2026-01-14 00:13:25,263: t15.2023.08.25 val PER: 0.0783
2026-01-14 00:13:25,263: t15.2023.08.27 val PER: 0.1495
2026-01-14 00:13:25,263: t15.2023.09.01 val PER: 0.0430
2026-01-14 00:13:25,263: t15.2023.09.03 val PER: 0.1247
2026-01-14 00:13:25,263: t15.2023.09.24 val PER: 0.0971
2026-01-14 00:13:25,263: t15.2023.09.29 val PER: 0.0989
2026-01-14 00:13:25,263: t15.2023.10.01 val PER: 0.1334
2026-01-14 00:13:25,263: t15.2023.10.06 val PER: 0.0495
2026-01-14 00:13:25,264: t15.2023.10.08 val PER: 0.1922
2026-01-14 00:13:25,264: t15.2023.10.13 val PER: 0.1443
2026-01-14 00:13:25,264: t15.2023.10.15 val PER: 0.1035
2026-01-14 00:13:25,264: t15.2023.10.20 val PER: 0.1544
2026-01-14 00:13:25,264: t15.2023.10.22 val PER: 0.0824
2026-01-14 00:13:25,264: t15.2023.11.03 val PER: 0.1404
2026-01-14 00:13:25,264: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:13:25,264: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:13:25,264: t15.2023.11.19 val PER: 0.0240
2026-01-14 00:13:25,264: t15.2023.11.26 val PER: 0.0457
2026-01-14 00:13:25,264: t15.2023.12.03 val PER: 0.0441
2026-01-14 00:13:25,264: t15.2023.12.08 val PER: 0.0393
2026-01-14 00:13:25,264: t15.2023.12.10 val PER: 0.0329
2026-01-14 00:13:25,264: t15.2023.12.17 val PER: 0.0811
2026-01-14 00:13:25,265: t15.2023.12.29 val PER: 0.0693
2026-01-14 00:13:25,265: t15.2024.02.25 val PER: 0.0632
2026-01-14 00:13:25,265: t15.2024.03.08 val PER: 0.1579
2026-01-14 00:13:25,265: t15.2024.03.15 val PER: 0.1476
2026-01-14 00:13:25,265: t15.2024.03.17 val PER: 0.0739
2026-01-14 00:13:25,265: t15.2024.05.10 val PER: 0.1129
2026-01-14 00:13:25,265: t15.2024.06.14 val PER: 0.1136
2026-01-14 00:13:25,265: t15.2024.07.19 val PER: 0.1549
2026-01-14 00:13:25,265: t15.2024.07.21 val PER: 0.0600
2026-01-14 00:13:25,265: t15.2024.07.28 val PER: 0.0875
2026-01-14 00:13:25,265: t15.2025.01.10 val PER: 0.2479
2026-01-14 00:13:25,265: t15.2025.01.12 val PER: 0.0924
2026-01-14 00:13:25,265: t15.2025.03.14 val PER: 0.3062
2026-01-14 00:13:25,265: t15.2025.03.16 val PER: 0.1387
2026-01-14 00:13:25,266: t15.2025.03.30 val PER: 0.2345
2026-01-14 00:13:25,266: t15.2025.04.13 val PER: 0.1854
2026-01-14 00:13:25,404: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_68500
2026-01-14 00:13:34,099: Train batch 68600: loss: 0.09 grad norm: 4.67 time: 0.048
2026-01-14 00:13:51,805: Train batch 68800: loss: 0.16 grad norm: 14.40 time: 0.078
2026-01-14 00:14:09,701: Train batch 69000: loss: 0.22 grad norm: 10.97 time: 0.059
2026-01-14 00:14:09,701: Running test after training batch: 69000
2026-01-14 00:14:09,807: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:14:14,627: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:14:14,686: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-14 00:14:30,269: Val batch 69000: PER (avg): 0.1036 CTC Loss (avg): 19.3980 WER(5gram): 20.80% (n=256) time: 20.568
2026-01-14 00:14:30,270: WER lens: avg_true_words=5.99 avg_pred_words=6.40 max_pred_words=13
2026-01-14 00:14:30,270: t15.2023.08.13 val PER: 0.0738
2026-01-14 00:14:30,270: t15.2023.08.18 val PER: 0.0704
2026-01-14 00:14:30,270: t15.2023.08.20 val PER: 0.0580
2026-01-14 00:14:30,270: t15.2023.08.25 val PER: 0.0813
2026-01-14 00:14:30,270: t15.2023.08.27 val PER: 0.1511
2026-01-14 00:14:30,270: t15.2023.09.01 val PER: 0.0406
2026-01-14 00:14:30,270: t15.2023.09.03 val PER: 0.1247
2026-01-14 00:14:30,270: t15.2023.09.24 val PER: 0.0910
2026-01-14 00:14:30,271: t15.2023.09.29 val PER: 0.1021
2026-01-14 00:14:30,271: t15.2023.10.01 val PER: 0.1295
2026-01-14 00:14:30,271: t15.2023.10.06 val PER: 0.0538
2026-01-14 00:14:30,271: t15.2023.10.08 val PER: 0.2016
2026-01-14 00:14:30,271: t15.2023.10.13 val PER: 0.1466
2026-01-14 00:14:30,271: t15.2023.10.15 val PER: 0.1028
2026-01-14 00:14:30,271: t15.2023.10.20 val PER: 0.1409
2026-01-14 00:14:30,271: t15.2023.10.22 val PER: 0.0869
2026-01-14 00:14:30,272: t15.2023.11.03 val PER: 0.1377
2026-01-14 00:14:30,272: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:14:30,272: t15.2023.11.17 val PER: 0.0140
2026-01-14 00:14:30,272: t15.2023.11.19 val PER: 0.0220
2026-01-14 00:14:30,272: t15.2023.11.26 val PER: 0.0435
2026-01-14 00:14:30,272: t15.2023.12.03 val PER: 0.0441
2026-01-14 00:14:30,272: t15.2023.12.08 val PER: 0.0360
2026-01-14 00:14:30,272: t15.2023.12.10 val PER: 0.0368
2026-01-14 00:14:30,272: t15.2023.12.17 val PER: 0.0821
2026-01-14 00:14:30,272: t15.2023.12.29 val PER: 0.0673
2026-01-14 00:14:30,273: t15.2024.02.25 val PER: 0.0646
2026-01-14 00:14:30,273: t15.2024.03.08 val PER: 0.1721
2026-01-14 00:14:30,273: t15.2024.03.15 val PER: 0.1470
2026-01-14 00:14:30,273: t15.2024.03.17 val PER: 0.0774
2026-01-14 00:14:30,273: t15.2024.05.10 val PER: 0.1218
2026-01-14 00:14:30,273: t15.2024.06.14 val PER: 0.1183
2026-01-14 00:14:30,273: t15.2024.07.19 val PER: 0.1622
2026-01-14 00:14:30,273: t15.2024.07.21 val PER: 0.0559
2026-01-14 00:14:30,273: t15.2024.07.28 val PER: 0.0919
2026-01-14 00:14:30,273: t15.2025.01.10 val PER: 0.2493
2026-01-14 00:14:30,273: t15.2025.01.12 val PER: 0.1001
2026-01-14 00:14:30,273: t15.2025.03.14 val PER: 0.2988
2026-01-14 00:14:30,273: t15.2025.03.16 val PER: 0.1309
2026-01-14 00:14:30,273: t15.2025.03.30 val PER: 0.2402
2026-01-14 00:14:30,273: t15.2025.04.13 val PER: 0.1854
2026-01-14 00:14:30,414: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_69000
2026-01-14 00:14:47,921: Train batch 69200: loss: 0.62 grad norm: 25.47 time: 0.071
2026-01-14 00:15:05,384: Train batch 69400: loss: 0.07 grad norm: 6.37 time: 0.052
2026-01-14 00:15:14,424: Running test after training batch: 69500
2026-01-14 00:15:14,516: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:15:19,333: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:15:19,404: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:15:35,439: Val batch 69500: PER (avg): 0.1039 CTC Loss (avg): 19.2977 WER(5gram): 20.53% (n=256) time: 21.014
2026-01-14 00:15:35,439: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=13
2026-01-14 00:15:35,439: t15.2023.08.13 val PER: 0.0707
2026-01-14 00:15:35,439: t15.2023.08.18 val PER: 0.0712
2026-01-14 00:15:35,440: t15.2023.08.20 val PER: 0.0532
2026-01-14 00:15:35,440: t15.2023.08.25 val PER: 0.0768
2026-01-14 00:15:35,440: t15.2023.08.27 val PER: 0.1511
2026-01-14 00:15:35,440: t15.2023.09.01 val PER: 0.0390
2026-01-14 00:15:35,440: t15.2023.09.03 val PER: 0.1235
2026-01-14 00:15:35,440: t15.2023.09.24 val PER: 0.0825
2026-01-14 00:15:35,441: t15.2023.09.29 val PER: 0.0996
2026-01-14 00:15:35,441: t15.2023.10.01 val PER: 0.1314
2026-01-14 00:15:35,441: t15.2023.10.06 val PER: 0.0495
2026-01-14 00:15:35,441: t15.2023.10.08 val PER: 0.1989
2026-01-14 00:15:35,441: t15.2023.10.13 val PER: 0.1505
2026-01-14 00:15:35,441: t15.2023.10.15 val PER: 0.1035
2026-01-14 00:15:35,441: t15.2023.10.20 val PER: 0.1611
2026-01-14 00:15:35,441: t15.2023.10.22 val PER: 0.0857
2026-01-14 00:15:35,441: t15.2023.11.03 val PER: 0.1404
2026-01-14 00:15:35,441: t15.2023.11.04 val PER: 0.0273
2026-01-14 00:15:35,442: t15.2023.11.17 val PER: 0.0156
2026-01-14 00:15:35,442: t15.2023.11.19 val PER: 0.0220
2026-01-14 00:15:35,442: t15.2023.11.26 val PER: 0.0442
2026-01-14 00:15:35,442: t15.2023.12.03 val PER: 0.0462
2026-01-14 00:15:35,442: t15.2023.12.08 val PER: 0.0379
2026-01-14 00:15:35,442: t15.2023.12.10 val PER: 0.0342
2026-01-14 00:15:35,442: t15.2023.12.17 val PER: 0.0863
2026-01-14 00:15:35,442: t15.2023.12.29 val PER: 0.0776
2026-01-14 00:15:35,442: t15.2024.02.25 val PER: 0.0618
2026-01-14 00:15:35,442: t15.2024.03.08 val PER: 0.1622
2026-01-14 00:15:35,442: t15.2024.03.15 val PER: 0.1538
2026-01-14 00:15:35,442: t15.2024.03.17 val PER: 0.0753
2026-01-14 00:15:35,443: t15.2024.05.10 val PER: 0.1174
2026-01-14 00:15:35,443: t15.2024.06.14 val PER: 0.1151
2026-01-14 00:15:35,443: t15.2024.07.19 val PER: 0.1575
2026-01-14 00:15:35,443: t15.2024.07.21 val PER: 0.0634
2026-01-14 00:15:35,443: t15.2024.07.28 val PER: 0.0912
2026-01-14 00:15:35,443: t15.2025.01.10 val PER: 0.2548
2026-01-14 00:15:35,443: t15.2025.01.12 val PER: 0.0993
2026-01-14 00:15:35,443: t15.2025.03.14 val PER: 0.2944
2026-01-14 00:15:35,444: t15.2025.03.16 val PER: 0.1322
2026-01-14 00:15:35,444: t15.2025.03.30 val PER: 0.2356
2026-01-14 00:15:35,444: t15.2025.04.13 val PER: 0.1883
2026-01-14 00:15:35,588: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_69500
2026-01-14 00:15:44,341: Train batch 69600: loss: 0.28 grad norm: 19.48 time: 0.068
2026-01-14 00:16:02,104: Train batch 69800: loss: 0.12 grad norm: 6.74 time: 0.088
2026-01-14 00:16:19,616: Train batch 70000: loss: 0.10 grad norm: 8.10 time: 0.052
2026-01-14 00:16:19,616: Running test after training batch: 70000
2026-01-14 00:16:19,706: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:16:25,105: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:16:25,173: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:16:41,511: Val batch 70000: PER (avg): 0.1029 CTC Loss (avg): 19.0119 WER(5gram): 19.95% (n=256) time: 21.895
2026-01-14 00:16:41,512: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=13
2026-01-14 00:16:41,512: t15.2023.08.13 val PER: 0.0707
2026-01-14 00:16:41,512: t15.2023.08.18 val PER: 0.0729
2026-01-14 00:16:41,512: t15.2023.08.20 val PER: 0.0580
2026-01-14 00:16:41,512: t15.2023.08.25 val PER: 0.0753
2026-01-14 00:16:41,512: t15.2023.08.27 val PER: 0.1415
2026-01-14 00:16:41,512: t15.2023.09.01 val PER: 0.0390
2026-01-14 00:16:41,513: t15.2023.09.03 val PER: 0.1247
2026-01-14 00:16:41,513: t15.2023.09.24 val PER: 0.0837
2026-01-14 00:16:41,513: t15.2023.09.29 val PER: 0.0970
2026-01-14 00:16:41,513: t15.2023.10.01 val PER: 0.1281
2026-01-14 00:16:41,513: t15.2023.10.06 val PER: 0.0474
2026-01-14 00:16:41,513: t15.2023.10.08 val PER: 0.2003
2026-01-14 00:16:41,513: t15.2023.10.13 val PER: 0.1528
2026-01-14 00:16:41,513: t15.2023.10.15 val PER: 0.1055
2026-01-14 00:16:41,514: t15.2023.10.20 val PER: 0.1477
2026-01-14 00:16:41,514: t15.2023.10.22 val PER: 0.0824
2026-01-14 00:16:41,514: t15.2023.11.03 val PER: 0.1357
2026-01-14 00:16:41,514: t15.2023.11.04 val PER: 0.0273
2026-01-14 00:16:41,514: t15.2023.11.17 val PER: 0.0156
2026-01-14 00:16:41,514: t15.2023.11.19 val PER: 0.0160
2026-01-14 00:16:41,514: t15.2023.11.26 val PER: 0.0435
2026-01-14 00:16:41,514: t15.2023.12.03 val PER: 0.0494
2026-01-14 00:16:41,514: t15.2023.12.08 val PER: 0.0373
2026-01-14 00:16:41,514: t15.2023.12.10 val PER: 0.0315
2026-01-14 00:16:41,514: t15.2023.12.17 val PER: 0.0863
2026-01-14 00:16:41,514: t15.2023.12.29 val PER: 0.0673
2026-01-14 00:16:41,514: t15.2024.02.25 val PER: 0.0716
2026-01-14 00:16:41,514: t15.2024.03.08 val PER: 0.1593
2026-01-14 00:16:41,514: t15.2024.03.15 val PER: 0.1476
2026-01-14 00:16:41,514: t15.2024.03.17 val PER: 0.0739
2026-01-14 00:16:41,514: t15.2024.05.10 val PER: 0.1144
2026-01-14 00:16:41,515: t15.2024.06.14 val PER: 0.1215
2026-01-14 00:16:41,515: t15.2024.07.19 val PER: 0.1681
2026-01-14 00:16:41,515: t15.2024.07.21 val PER: 0.0614
2026-01-14 00:16:41,515: t15.2024.07.28 val PER: 0.0890
2026-01-14 00:16:41,515: t15.2025.01.10 val PER: 0.2562
2026-01-14 00:16:41,515: t15.2025.01.12 val PER: 0.0962
2026-01-14 00:16:41,515: t15.2025.03.14 val PER: 0.3033
2026-01-14 00:16:41,515: t15.2025.03.16 val PER: 0.1322
2026-01-14 00:16:41,515: t15.2025.03.30 val PER: 0.2287
2026-01-14 00:16:41,515: t15.2025.04.13 val PER: 0.1769
2026-01-14 00:16:41,665: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_70000
2026-01-14 00:16:59,401: Train batch 70200: loss: 0.22 grad norm: 10.98 time: 0.066
2026-01-14 00:17:17,193: Train batch 70400: loss: 0.10 grad norm: 7.73 time: 0.062
2026-01-14 00:17:26,096: Running test after training batch: 70500
2026-01-14 00:17:26,201: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:17:31,046: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:17:31,121: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:17:47,098: Val batch 70500: PER (avg): 0.1030 CTC Loss (avg): 19.0086 WER(5gram): 20.60% (n=256) time: 21.001
2026-01-14 00:17:47,099: WER lens: avg_true_words=5.99 avg_pred_words=6.39 max_pred_words=13
2026-01-14 00:17:47,099: t15.2023.08.13 val PER: 0.0707
2026-01-14 00:17:47,099: t15.2023.08.18 val PER: 0.0645
2026-01-14 00:17:47,100: t15.2023.08.20 val PER: 0.0564
2026-01-14 00:17:47,100: t15.2023.08.25 val PER: 0.0813
2026-01-14 00:17:47,100: t15.2023.08.27 val PER: 0.1415
2026-01-14 00:17:47,100: t15.2023.09.01 val PER: 0.0398
2026-01-14 00:17:47,100: t15.2023.09.03 val PER: 0.1176
2026-01-14 00:17:47,100: t15.2023.09.24 val PER: 0.0886
2026-01-14 00:17:47,100: t15.2023.09.29 val PER: 0.0989
2026-01-14 00:17:47,100: t15.2023.10.01 val PER: 0.1347
2026-01-14 00:17:47,100: t15.2023.10.06 val PER: 0.0506
2026-01-14 00:17:47,100: t15.2023.10.08 val PER: 0.1867
2026-01-14 00:17:47,101: t15.2023.10.13 val PER: 0.1513
2026-01-14 00:17:47,101: t15.2023.10.15 val PER: 0.1068
2026-01-14 00:17:47,101: t15.2023.10.20 val PER: 0.1510
2026-01-14 00:17:47,101: t15.2023.10.22 val PER: 0.0869
2026-01-14 00:17:47,101: t15.2023.11.03 val PER: 0.1431
2026-01-14 00:17:47,101: t15.2023.11.04 val PER: 0.0273
2026-01-14 00:17:47,101: t15.2023.11.17 val PER: 0.0218
2026-01-14 00:17:47,101: t15.2023.11.19 val PER: 0.0180
2026-01-14 00:17:47,101: t15.2023.11.26 val PER: 0.0486
2026-01-14 00:17:47,101: t15.2023.12.03 val PER: 0.0473
2026-01-14 00:17:47,102: t15.2023.12.08 val PER: 0.0346
2026-01-14 00:17:47,102: t15.2023.12.10 val PER: 0.0368
2026-01-14 00:17:47,102: t15.2023.12.17 val PER: 0.0852
2026-01-14 00:17:47,102: t15.2023.12.29 val PER: 0.0721
2026-01-14 00:17:47,102: t15.2024.02.25 val PER: 0.0660
2026-01-14 00:17:47,102: t15.2024.03.08 val PER: 0.1593
2026-01-14 00:17:47,102: t15.2024.03.15 val PER: 0.1513
2026-01-14 00:17:47,102: t15.2024.03.17 val PER: 0.0753
2026-01-14 00:17:47,102: t15.2024.05.10 val PER: 0.1129
2026-01-14 00:17:47,102: t15.2024.06.14 val PER: 0.1183
2026-01-14 00:17:47,102: t15.2024.07.19 val PER: 0.1483
2026-01-14 00:17:47,103: t15.2024.07.21 val PER: 0.0593
2026-01-14 00:17:47,103: t15.2024.07.28 val PER: 0.0897
2026-01-14 00:17:47,103: t15.2025.01.10 val PER: 0.2603
2026-01-14 00:17:47,103: t15.2025.01.12 val PER: 0.0931
2026-01-14 00:17:47,103: t15.2025.03.14 val PER: 0.3018
2026-01-14 00:17:47,103: t15.2025.03.16 val PER: 0.1374
2026-01-14 00:17:47,103: t15.2025.03.30 val PER: 0.2391
2026-01-14 00:17:47,103: t15.2025.04.13 val PER: 0.1712
2026-01-14 00:17:47,245: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_70500
2026-01-14 00:17:56,181: Train batch 70600: loss: 0.18 grad norm: 18.61 time: 0.066
2026-01-14 00:18:13,890: Train batch 70800: loss: 0.16 grad norm: 11.02 time: 0.063
2026-01-14 00:18:32,019: Train batch 71000: loss: 0.09 grad norm: 9.70 time: 0.060
2026-01-14 00:18:32,019: Running test after training batch: 71000
2026-01-14 00:18:32,125: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:18:36,986: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:18:37,058: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-14 00:18:52,982: Val batch 71000: PER (avg): 0.1021 CTC Loss (avg): 19.0245 WER(5gram): 19.49% (n=256) time: 20.963
2026-01-14 00:18:52,983: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=13
2026-01-14 00:18:52,983: t15.2023.08.13 val PER: 0.0696
2026-01-14 00:18:52,983: t15.2023.08.18 val PER: 0.0721
2026-01-14 00:18:52,983: t15.2023.08.20 val PER: 0.0532
2026-01-14 00:18:52,983: t15.2023.08.25 val PER: 0.0738
2026-01-14 00:18:52,984: t15.2023.08.27 val PER: 0.1463
2026-01-14 00:18:52,984: t15.2023.09.01 val PER: 0.0414
2026-01-14 00:18:52,984: t15.2023.09.03 val PER: 0.1200
2026-01-14 00:18:52,984: t15.2023.09.24 val PER: 0.0850
2026-01-14 00:18:52,984: t15.2023.09.29 val PER: 0.0989
2026-01-14 00:18:52,984: t15.2023.10.01 val PER: 0.1361
2026-01-14 00:18:52,984: t15.2023.10.06 val PER: 0.0495
2026-01-14 00:18:52,984: t15.2023.10.08 val PER: 0.1976
2026-01-14 00:18:52,984: t15.2023.10.13 val PER: 0.1404
2026-01-14 00:18:52,984: t15.2023.10.15 val PER: 0.1061
2026-01-14 00:18:52,984: t15.2023.10.20 val PER: 0.1477
2026-01-14 00:18:52,984: t15.2023.10.22 val PER: 0.0757
2026-01-14 00:18:52,984: t15.2023.11.03 val PER: 0.1418
2026-01-14 00:18:52,985: t15.2023.11.04 val PER: 0.0239
2026-01-14 00:18:52,985: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:18:52,985: t15.2023.11.19 val PER: 0.0200
2026-01-14 00:18:52,985: t15.2023.11.26 val PER: 0.0442
2026-01-14 00:18:52,985: t15.2023.12.03 val PER: 0.0452
2026-01-14 00:18:52,985: t15.2023.12.08 val PER: 0.0379
2026-01-14 00:18:52,985: t15.2023.12.10 val PER: 0.0329
2026-01-14 00:18:52,985: t15.2023.12.17 val PER: 0.0800
2026-01-14 00:18:52,985: t15.2023.12.29 val PER: 0.0707
2026-01-14 00:18:52,985: t15.2024.02.25 val PER: 0.0674
2026-01-14 00:18:52,985: t15.2024.03.08 val PER: 0.1622
2026-01-14 00:18:52,985: t15.2024.03.15 val PER: 0.1463
2026-01-14 00:18:52,985: t15.2024.03.17 val PER: 0.0690
2026-01-14 00:18:52,985: t15.2024.05.10 val PER: 0.1174
2026-01-14 00:18:52,985: t15.2024.06.14 val PER: 0.1183
2026-01-14 00:18:52,986: t15.2024.07.19 val PER: 0.1569
2026-01-14 00:18:52,986: t15.2024.07.21 val PER: 0.0614
2026-01-14 00:18:52,986: t15.2024.07.28 val PER: 0.0934
2026-01-14 00:18:52,986: t15.2025.01.10 val PER: 0.2590
2026-01-14 00:18:52,986: t15.2025.01.12 val PER: 0.0962
2026-01-14 00:18:52,986: t15.2025.03.14 val PER: 0.2973
2026-01-14 00:18:52,986: t15.2025.03.16 val PER: 0.1296
2026-01-14 00:18:52,986: t15.2025.03.30 val PER: 0.2230
2026-01-14 00:18:52,986: t15.2025.04.13 val PER: 0.1797
2026-01-14 00:18:53,135: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_71000
2026-01-14 00:19:10,226: Train batch 71200: loss: 0.20 grad norm: 12.04 time: 0.075
2026-01-14 00:19:27,575: Train batch 71400: loss: 0.06 grad norm: 4.60 time: 0.078
2026-01-14 00:19:36,177: Running test after training batch: 71500
2026-01-14 00:19:36,297: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:19:41,337: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:19:41,401: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cent
2026-01-14 00:19:57,177: Val batch 71500: PER (avg): 0.1022 CTC Loss (avg): 18.9603 WER(5gram): 19.56% (n=256) time: 20.999
2026-01-14 00:19:57,177: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=13
2026-01-14 00:19:57,178: t15.2023.08.13 val PER: 0.0748
2026-01-14 00:19:57,178: t15.2023.08.18 val PER: 0.0654
2026-01-14 00:19:57,178: t15.2023.08.20 val PER: 0.0572
2026-01-14 00:19:57,178: t15.2023.08.25 val PER: 0.0693
2026-01-14 00:19:57,178: t15.2023.08.27 val PER: 0.1559
2026-01-14 00:19:57,178: t15.2023.09.01 val PER: 0.0414
2026-01-14 00:19:57,178: t15.2023.09.03 val PER: 0.1211
2026-01-14 00:19:57,178: t15.2023.09.24 val PER: 0.0850
2026-01-14 00:19:57,178: t15.2023.09.29 val PER: 0.0976
2026-01-14 00:19:57,178: t15.2023.10.01 val PER: 0.1334
2026-01-14 00:19:57,178: t15.2023.10.06 val PER: 0.0517
2026-01-14 00:19:57,179: t15.2023.10.08 val PER: 0.1800
2026-01-14 00:19:57,179: t15.2023.10.13 val PER: 0.1412
2026-01-14 00:19:57,179: t15.2023.10.15 val PER: 0.1042
2026-01-14 00:19:57,179: t15.2023.10.20 val PER: 0.1644
2026-01-14 00:19:57,179: t15.2023.10.22 val PER: 0.0813
2026-01-14 00:19:57,179: t15.2023.11.03 val PER: 0.1438
2026-01-14 00:19:57,179: t15.2023.11.04 val PER: 0.0239
2026-01-14 00:19:57,179: t15.2023.11.17 val PER: 0.0187
2026-01-14 00:19:57,179: t15.2023.11.19 val PER: 0.0180
2026-01-14 00:19:57,179: t15.2023.11.26 val PER: 0.0413
2026-01-14 00:19:57,179: t15.2023.12.03 val PER: 0.0452
2026-01-14 00:19:57,179: t15.2023.12.08 val PER: 0.0346
2026-01-14 00:19:57,180: t15.2023.12.10 val PER: 0.0381
2026-01-14 00:19:57,180: t15.2023.12.17 val PER: 0.0852
2026-01-14 00:19:57,180: t15.2023.12.29 val PER: 0.0748
2026-01-14 00:19:57,180: t15.2024.02.25 val PER: 0.0660
2026-01-14 00:19:57,180: t15.2024.03.08 val PER: 0.1550
2026-01-14 00:19:57,180: t15.2024.03.15 val PER: 0.1451
2026-01-14 00:19:57,180: t15.2024.03.17 val PER: 0.0732
2026-01-14 00:19:57,180: t15.2024.05.10 val PER: 0.1159
2026-01-14 00:19:57,180: t15.2024.06.14 val PER: 0.1230
2026-01-14 00:19:57,180: t15.2024.07.19 val PER: 0.1668
2026-01-14 00:19:57,180: t15.2024.07.21 val PER: 0.0628
2026-01-14 00:19:57,180: t15.2024.07.28 val PER: 0.0890
2026-01-14 00:19:57,181: t15.2025.01.10 val PER: 0.2534
2026-01-14 00:19:57,181: t15.2025.01.12 val PER: 0.0893
2026-01-14 00:19:57,181: t15.2025.03.14 val PER: 0.2944
2026-01-14 00:19:57,181: t15.2025.03.16 val PER: 0.1230
2026-01-14 00:19:57,181: t15.2025.03.30 val PER: 0.2322
2026-01-14 00:19:57,181: t15.2025.04.13 val PER: 0.1769
2026-01-14 00:19:57,318: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_71500
2026-01-14 00:20:05,885: Train batch 71600: loss: 0.21 grad norm: 18.09 time: 0.061
2026-01-14 00:20:23,549: Train batch 71800: loss: 0.36 grad norm: 18.58 time: 0.063
2026-01-14 00:20:40,885: Train batch 72000: loss: 0.39 grad norm: 15.43 time: 0.056
2026-01-14 00:20:40,885: Running test after training batch: 72000
2026-01-14 00:20:41,034: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:20:45,892: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:20:45,959: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:21:01,788: Val batch 72000: PER (avg): 0.1018 CTC Loss (avg): 18.9997 WER(5gram): 19.75% (n=256) time: 20.902
2026-01-14 00:21:01,788: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=13
2026-01-14 00:21:01,788: t15.2023.08.13 val PER: 0.0676
2026-01-14 00:21:01,789: t15.2023.08.18 val PER: 0.0696
2026-01-14 00:21:01,789: t15.2023.08.20 val PER: 0.0556
2026-01-14 00:21:01,789: t15.2023.08.25 val PER: 0.0708
2026-01-14 00:21:01,789: t15.2023.08.27 val PER: 0.1479
2026-01-14 00:21:01,789: t15.2023.09.01 val PER: 0.0381
2026-01-14 00:21:01,789: t15.2023.09.03 val PER: 0.1330
2026-01-14 00:21:01,789: t15.2023.09.24 val PER: 0.0874
2026-01-14 00:21:01,789: t15.2023.09.29 val PER: 0.0970
2026-01-14 00:21:01,789: t15.2023.10.01 val PER: 0.1354
2026-01-14 00:21:01,789: t15.2023.10.06 val PER: 0.0474
2026-01-14 00:21:01,790: t15.2023.10.08 val PER: 0.1827
2026-01-14 00:21:01,790: t15.2023.10.13 val PER: 0.1389
2026-01-14 00:21:01,790: t15.2023.10.15 val PER: 0.1035
2026-01-14 00:21:01,790: t15.2023.10.20 val PER: 0.1510
2026-01-14 00:21:01,790: t15.2023.10.22 val PER: 0.0813
2026-01-14 00:21:01,790: t15.2023.11.03 val PER: 0.1384
2026-01-14 00:21:01,790: t15.2023.11.04 val PER: 0.0273
2026-01-14 00:21:01,790: t15.2023.11.17 val PER: 0.0124
2026-01-14 00:21:01,790: t15.2023.11.19 val PER: 0.0220
2026-01-14 00:21:01,790: t15.2023.11.26 val PER: 0.0420
2026-01-14 00:21:01,790: t15.2023.12.03 val PER: 0.0515
2026-01-14 00:21:01,790: t15.2023.12.08 val PER: 0.0379
2026-01-14 00:21:01,791: t15.2023.12.10 val PER: 0.0355
2026-01-14 00:21:01,791: t15.2023.12.17 val PER: 0.0894
2026-01-14 00:21:01,791: t15.2023.12.29 val PER: 0.0734
2026-01-14 00:21:01,791: t15.2024.02.25 val PER: 0.0646
2026-01-14 00:21:01,791: t15.2024.03.08 val PER: 0.1679
2026-01-14 00:21:01,791: t15.2024.03.15 val PER: 0.1451
2026-01-14 00:21:01,791: t15.2024.03.17 val PER: 0.0704
2026-01-14 00:21:01,791: t15.2024.05.10 val PER: 0.1189
2026-01-14 00:21:01,791: t15.2024.06.14 val PER: 0.1199
2026-01-14 00:21:01,791: t15.2024.07.19 val PER: 0.1556
2026-01-14 00:21:01,791: t15.2024.07.21 val PER: 0.0614
2026-01-14 00:21:01,791: t15.2024.07.28 val PER: 0.0875
2026-01-14 00:21:01,791: t15.2025.01.10 val PER: 0.2507
2026-01-14 00:21:01,791: t15.2025.01.12 val PER: 0.0939
2026-01-14 00:21:01,791: t15.2025.03.14 val PER: 0.2944
2026-01-14 00:21:01,792: t15.2025.03.16 val PER: 0.1283
2026-01-14 00:21:01,792: t15.2025.03.30 val PER: 0.2333
2026-01-14 00:21:01,792: t15.2025.04.13 val PER: 0.1726
2026-01-14 00:21:01,930: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_72000
2026-01-14 00:21:19,373: Train batch 72200: loss: 0.24 grad norm: 14.65 time: 0.054
2026-01-14 00:21:36,763: Train batch 72400: loss: 0.06 grad norm: 4.47 time: 0.052
2026-01-14 00:21:45,542: Running test after training batch: 72500
2026-01-14 00:21:45,722: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:21:50,537: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:21:50,599: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:22:04,851: Val batch 72500: PER (avg): 0.1030 CTC Loss (avg): 19.4295 WER(5gram): 19.10% (n=256) time: 19.309
2026-01-14 00:22:04,852: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=13
2026-01-14 00:22:04,852: t15.2023.08.13 val PER: 0.0665
2026-01-14 00:22:04,852: t15.2023.08.18 val PER: 0.0687
2026-01-14 00:22:04,852: t15.2023.08.20 val PER: 0.0564
2026-01-14 00:22:04,852: t15.2023.08.25 val PER: 0.0768
2026-01-14 00:22:04,853: t15.2023.08.27 val PER: 0.1463
2026-01-14 00:22:04,853: t15.2023.09.01 val PER: 0.0381
2026-01-14 00:22:04,853: t15.2023.09.03 val PER: 0.1223
2026-01-14 00:22:04,853: t15.2023.09.24 val PER: 0.0886
2026-01-14 00:22:04,853: t15.2023.09.29 val PER: 0.0983
2026-01-14 00:22:04,853: t15.2023.10.01 val PER: 0.1347
2026-01-14 00:22:04,853: t15.2023.10.06 val PER: 0.0441
2026-01-14 00:22:04,853: t15.2023.10.08 val PER: 0.1922
2026-01-14 00:22:04,853: t15.2023.10.13 val PER: 0.1381
2026-01-14 00:22:04,854: t15.2023.10.15 val PER: 0.1081
2026-01-14 00:22:04,854: t15.2023.10.20 val PER: 0.1611
2026-01-14 00:22:04,854: t15.2023.10.22 val PER: 0.0813
2026-01-14 00:22:04,854: t15.2023.11.03 val PER: 0.1404
2026-01-14 00:22:04,854: t15.2023.11.04 val PER: 0.0273
2026-01-14 00:22:04,854: t15.2023.11.17 val PER: 0.0140
2026-01-14 00:22:04,854: t15.2023.11.19 val PER: 0.0220
2026-01-14 00:22:04,854: t15.2023.11.26 val PER: 0.0442
2026-01-14 00:22:04,854: t15.2023.12.03 val PER: 0.0504
2026-01-14 00:22:04,854: t15.2023.12.08 val PER: 0.0379
2026-01-14 00:22:04,855: t15.2023.12.10 val PER: 0.0342
2026-01-14 00:22:04,855: t15.2023.12.17 val PER: 0.0863
2026-01-14 00:22:04,855: t15.2023.12.29 val PER: 0.0748
2026-01-14 00:22:04,855: t15.2024.02.25 val PER: 0.0590
2026-01-14 00:22:04,855: t15.2024.03.08 val PER: 0.1593
2026-01-14 00:22:04,855: t15.2024.03.15 val PER: 0.1488
2026-01-14 00:22:04,855: t15.2024.03.17 val PER: 0.0746
2026-01-14 00:22:04,855: t15.2024.05.10 val PER: 0.1263
2026-01-14 00:22:04,855: t15.2024.06.14 val PER: 0.1183
2026-01-14 00:22:04,855: t15.2024.07.19 val PER: 0.1549
2026-01-14 00:22:04,855: t15.2024.07.21 val PER: 0.0607
2026-01-14 00:22:04,856: t15.2024.07.28 val PER: 0.0949
2026-01-14 00:22:04,856: t15.2025.01.10 val PER: 0.2631
2026-01-14 00:22:04,856: t15.2025.01.12 val PER: 0.1024
2026-01-14 00:22:04,856: t15.2025.03.14 val PER: 0.3047
2026-01-14 00:22:04,856: t15.2025.03.16 val PER: 0.1230
2026-01-14 00:22:04,856: t15.2025.03.30 val PER: 0.2241
2026-01-14 00:22:04,856: t15.2025.04.13 val PER: 0.1840
2026-01-14 00:22:04,993: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_72500
2026-01-14 00:22:13,655: Train batch 72600: loss: 0.16 grad norm: 10.20 time: 0.060
2026-01-14 00:22:30,986: Train batch 72800: loss: 0.12 grad norm: 9.40 time: 0.080
2026-01-14 00:22:48,169: Train batch 73000: loss: 0.06 grad norm: 6.99 time: 0.081
2026-01-14 00:22:48,170: Running test after training batch: 73000
2026-01-14 00:22:48,265: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:22:53,418: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:22:53,475: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:23:07,713: Val batch 73000: PER (avg): 0.1006 CTC Loss (avg): 19.5632 WER(5gram): 20.21% (n=256) time: 19.544
2026-01-14 00:23:07,714: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=13
2026-01-14 00:23:07,714: t15.2023.08.13 val PER: 0.0613
2026-01-14 00:23:07,714: t15.2023.08.18 val PER: 0.0662
2026-01-14 00:23:07,715: t15.2023.08.20 val PER: 0.0516
2026-01-14 00:23:07,715: t15.2023.08.25 val PER: 0.0678
2026-01-14 00:23:07,715: t15.2023.08.27 val PER: 0.1463
2026-01-14 00:23:07,715: t15.2023.09.01 val PER: 0.0349
2026-01-14 00:23:07,715: t15.2023.09.03 val PER: 0.1188
2026-01-14 00:23:07,715: t15.2023.09.24 val PER: 0.0910
2026-01-14 00:23:07,715: t15.2023.09.29 val PER: 0.0989
2026-01-14 00:23:07,715: t15.2023.10.01 val PER: 0.1308
2026-01-14 00:23:07,715: t15.2023.10.06 val PER: 0.0463
2026-01-14 00:23:07,715: t15.2023.10.08 val PER: 0.1867
2026-01-14 00:23:07,715: t15.2023.10.13 val PER: 0.1327
2026-01-14 00:23:07,715: t15.2023.10.15 val PER: 0.1061
2026-01-14 00:23:07,715: t15.2023.10.20 val PER: 0.1409
2026-01-14 00:23:07,715: t15.2023.10.22 val PER: 0.0857
2026-01-14 00:23:07,716: t15.2023.11.03 val PER: 0.1350
2026-01-14 00:23:07,716: t15.2023.11.04 val PER: 0.0307
2026-01-14 00:23:07,716: t15.2023.11.17 val PER: 0.0156
2026-01-14 00:23:07,716: t15.2023.11.19 val PER: 0.0240
2026-01-14 00:23:07,716: t15.2023.11.26 val PER: 0.0449
2026-01-14 00:23:07,716: t15.2023.12.03 val PER: 0.0494
2026-01-14 00:23:07,717: t15.2023.12.08 val PER: 0.0346
2026-01-14 00:23:07,717: t15.2023.12.10 val PER: 0.0276
2026-01-14 00:23:07,717: t15.2023.12.17 val PER: 0.0925
2026-01-14 00:23:07,717: t15.2023.12.29 val PER: 0.0686
2026-01-14 00:23:07,717: t15.2024.02.25 val PER: 0.0604
2026-01-14 00:23:07,717: t15.2024.03.08 val PER: 0.1679
2026-01-14 00:23:07,717: t15.2024.03.15 val PER: 0.1520
2026-01-14 00:23:07,717: t15.2024.03.17 val PER: 0.0725
2026-01-14 00:23:07,717: t15.2024.05.10 val PER: 0.1114
2026-01-14 00:23:07,717: t15.2024.06.14 val PER: 0.1199
2026-01-14 00:23:07,717: t15.2024.07.19 val PER: 0.1549
2026-01-14 00:23:07,717: t15.2024.07.21 val PER: 0.0586
2026-01-14 00:23:07,718: t15.2024.07.28 val PER: 0.0897
2026-01-14 00:23:07,718: t15.2025.01.10 val PER: 0.2479
2026-01-14 00:23:07,718: t15.2025.01.12 val PER: 0.0955
2026-01-14 00:23:07,718: t15.2025.03.14 val PER: 0.3077
2026-01-14 00:23:07,718: t15.2025.03.16 val PER: 0.1243
2026-01-14 00:23:07,718: t15.2025.03.30 val PER: 0.2149
2026-01-14 00:23:07,718: t15.2025.04.13 val PER: 0.1797
2026-01-14 00:23:07,856: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_73000
2026-01-14 00:23:25,158: Train batch 73200: loss: 0.12 grad norm: 7.83 time: 0.070
2026-01-14 00:23:42,701: Train batch 73400: loss: 0.04 grad norm: 4.73 time: 0.050
2026-01-14 00:23:51,420: Running test after training batch: 73500
2026-01-14 00:23:51,562: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:23:56,481: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:23:56,552: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:24:11,907: Val batch 73500: PER (avg): 0.1017 CTC Loss (avg): 19.3231 WER(5gram): 19.23% (n=256) time: 20.486
2026-01-14 00:24:11,907: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=13
2026-01-14 00:24:11,907: t15.2023.08.13 val PER: 0.0728
2026-01-14 00:24:11,907: t15.2023.08.18 val PER: 0.0696
2026-01-14 00:24:11,907: t15.2023.08.20 val PER: 0.0564
2026-01-14 00:24:11,908: t15.2023.08.25 val PER: 0.0738
2026-01-14 00:24:11,908: t15.2023.08.27 val PER: 0.1495
2026-01-14 00:24:11,908: t15.2023.09.01 val PER: 0.0406
2026-01-14 00:24:11,908: t15.2023.09.03 val PER: 0.1176
2026-01-14 00:24:11,908: t15.2023.09.24 val PER: 0.0874
2026-01-14 00:24:11,908: t15.2023.09.29 val PER: 0.0964
2026-01-14 00:24:11,908: t15.2023.10.01 val PER: 0.1281
2026-01-14 00:24:11,908: t15.2023.10.06 val PER: 0.0441
2026-01-14 00:24:11,908: t15.2023.10.08 val PER: 0.1935
2026-01-14 00:24:11,909: t15.2023.10.13 val PER: 0.1311
2026-01-14 00:24:11,909: t15.2023.10.15 val PER: 0.1061
2026-01-14 00:24:11,909: t15.2023.10.20 val PER: 0.1577
2026-01-14 00:24:11,909: t15.2023.10.22 val PER: 0.0780
2026-01-14 00:24:11,909: t15.2023.11.03 val PER: 0.1431
2026-01-14 00:24:11,909: t15.2023.11.04 val PER: 0.0273
2026-01-14 00:24:11,909: t15.2023.11.17 val PER: 0.0140
2026-01-14 00:24:11,909: t15.2023.11.19 val PER: 0.0220
2026-01-14 00:24:11,909: t15.2023.11.26 val PER: 0.0457
2026-01-14 00:24:11,909: t15.2023.12.03 val PER: 0.0483
2026-01-14 00:24:11,909: t15.2023.12.08 val PER: 0.0373
2026-01-14 00:24:11,910: t15.2023.12.10 val PER: 0.0302
2026-01-14 00:24:11,910: t15.2023.12.17 val PER: 0.0842
2026-01-14 00:24:11,910: t15.2023.12.29 val PER: 0.0679
2026-01-14 00:24:11,910: t15.2024.02.25 val PER: 0.0604
2026-01-14 00:24:11,910: t15.2024.03.08 val PER: 0.1636
2026-01-14 00:24:11,910: t15.2024.03.15 val PER: 0.1545
2026-01-14 00:24:11,910: t15.2024.03.17 val PER: 0.0704
2026-01-14 00:24:11,910: t15.2024.05.10 val PER: 0.1129
2026-01-14 00:24:11,910: t15.2024.06.14 val PER: 0.1215
2026-01-14 00:24:11,910: t15.2024.07.19 val PER: 0.1608
2026-01-14 00:24:11,910: t15.2024.07.21 val PER: 0.0586
2026-01-14 00:24:11,910: t15.2024.07.28 val PER: 0.0897
2026-01-14 00:24:11,911: t15.2025.01.10 val PER: 0.2410
2026-01-14 00:24:11,911: t15.2025.01.12 val PER: 0.0916
2026-01-14 00:24:11,911: t15.2025.03.14 val PER: 0.2988
2026-01-14 00:24:11,911: t15.2025.03.16 val PER: 0.1309
2026-01-14 00:24:11,911: t15.2025.03.30 val PER: 0.2391
2026-01-14 00:24:11,911: t15.2025.04.13 val PER: 0.1812
2026-01-14 00:24:12,051: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_73500
2026-01-14 00:24:20,687: Train batch 73600: loss: 0.18 grad norm: 16.59 time: 0.057
2026-01-14 00:24:38,269: Train batch 73800: loss: 0.11 grad norm: 7.23 time: 0.072
2026-01-14 00:24:55,612: Train batch 74000: loss: 0.02 grad norm: 1.75 time: 0.058
2026-01-14 00:24:55,612: Running test after training batch: 74000
2026-01-14 00:24:55,753: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:25:00,547: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:25:00,602: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:25:14,814: Val batch 74000: PER (avg): 0.1020 CTC Loss (avg): 19.4526 WER(5gram): 19.75% (n=256) time: 19.201
2026-01-14 00:25:14,815: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-14 00:25:14,815: t15.2023.08.13 val PER: 0.0696
2026-01-14 00:25:14,815: t15.2023.08.18 val PER: 0.0679
2026-01-14 00:25:14,815: t15.2023.08.20 val PER: 0.0596
2026-01-14 00:25:14,815: t15.2023.08.25 val PER: 0.0768
2026-01-14 00:25:14,815: t15.2023.08.27 val PER: 0.1399
2026-01-14 00:25:14,815: t15.2023.09.01 val PER: 0.0414
2026-01-14 00:25:14,815: t15.2023.09.03 val PER: 0.1128
2026-01-14 00:25:14,815: t15.2023.09.24 val PER: 0.0825
2026-01-14 00:25:14,815: t15.2023.09.29 val PER: 0.0925
2026-01-14 00:25:14,816: t15.2023.10.01 val PER: 0.1328
2026-01-14 00:25:14,816: t15.2023.10.06 val PER: 0.0495
2026-01-14 00:25:14,816: t15.2023.10.08 val PER: 0.1976
2026-01-14 00:25:14,816: t15.2023.10.13 val PER: 0.1412
2026-01-14 00:25:14,816: t15.2023.10.15 val PER: 0.1042
2026-01-14 00:25:14,816: t15.2023.10.20 val PER: 0.1577
2026-01-14 00:25:14,816: t15.2023.10.22 val PER: 0.0802
2026-01-14 00:25:14,816: t15.2023.11.03 val PER: 0.1377
2026-01-14 00:25:14,816: t15.2023.11.04 val PER: 0.0273
2026-01-14 00:25:14,816: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:25:14,816: t15.2023.11.19 val PER: 0.0220
2026-01-14 00:25:14,817: t15.2023.11.26 val PER: 0.0435
2026-01-14 00:25:14,817: t15.2023.12.03 val PER: 0.0431
2026-01-14 00:25:14,817: t15.2023.12.08 val PER: 0.0379
2026-01-14 00:25:14,817: t15.2023.12.10 val PER: 0.0342
2026-01-14 00:25:14,817: t15.2023.12.17 val PER: 0.0884
2026-01-14 00:25:14,817: t15.2023.12.29 val PER: 0.0700
2026-01-14 00:25:14,817: t15.2024.02.25 val PER: 0.0604
2026-01-14 00:25:14,818: t15.2024.03.08 val PER: 0.1693
2026-01-14 00:25:14,818: t15.2024.03.15 val PER: 0.1538
2026-01-14 00:25:14,818: t15.2024.03.17 val PER: 0.0683
2026-01-14 00:25:14,818: t15.2024.05.10 val PER: 0.1218
2026-01-14 00:25:14,818: t15.2024.06.14 val PER: 0.1293
2026-01-14 00:25:14,818: t15.2024.07.19 val PER: 0.1615
2026-01-14 00:25:14,818: t15.2024.07.21 val PER: 0.0621
2026-01-14 00:25:14,818: t15.2024.07.28 val PER: 0.0904
2026-01-14 00:25:14,818: t15.2025.01.10 val PER: 0.2479
2026-01-14 00:25:14,818: t15.2025.01.12 val PER: 0.0955
2026-01-14 00:25:14,818: t15.2025.03.14 val PER: 0.2973
2026-01-14 00:25:14,819: t15.2025.03.16 val PER: 0.1204
2026-01-14 00:25:14,819: t15.2025.03.30 val PER: 0.2241
2026-01-14 00:25:14,819: t15.2025.04.13 val PER: 0.1797
2026-01-14 00:25:14,961: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_74000
2026-01-14 00:25:32,511: Train batch 74200: loss: 0.12 grad norm: 10.77 time: 0.056
2026-01-14 00:25:50,044: Train batch 74400: loss: 0.49 grad norm: 21.21 time: 0.081
2026-01-14 00:25:58,938: Running test after training batch: 74500
2026-01-14 00:25:59,028: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:26:03,846: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:26:03,909: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:26:19,153: Val batch 74500: PER (avg): 0.1010 CTC Loss (avg): 19.1003 WER(5gram): 19.36% (n=256) time: 20.215
2026-01-14 00:26:19,154: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=13
2026-01-14 00:26:19,154: t15.2023.08.13 val PER: 0.0717
2026-01-14 00:26:19,154: t15.2023.08.18 val PER: 0.0637
2026-01-14 00:26:19,154: t15.2023.08.20 val PER: 0.0635
2026-01-14 00:26:19,154: t15.2023.08.25 val PER: 0.0723
2026-01-14 00:26:19,154: t15.2023.08.27 val PER: 0.1447
2026-01-14 00:26:19,154: t15.2023.09.01 val PER: 0.0430
2026-01-14 00:26:19,154: t15.2023.09.03 val PER: 0.1152
2026-01-14 00:26:19,154: t15.2023.09.24 val PER: 0.0910
2026-01-14 00:26:19,154: t15.2023.09.29 val PER: 0.0964
2026-01-14 00:26:19,154: t15.2023.10.01 val PER: 0.1248
2026-01-14 00:26:19,155: t15.2023.10.06 val PER: 0.0452
2026-01-14 00:26:19,155: t15.2023.10.08 val PER: 0.1894
2026-01-14 00:26:19,155: t15.2023.10.13 val PER: 0.1342
2026-01-14 00:26:19,155: t15.2023.10.15 val PER: 0.1028
2026-01-14 00:26:19,155: t15.2023.10.20 val PER: 0.1443
2026-01-14 00:26:19,155: t15.2023.10.22 val PER: 0.0791
2026-01-14 00:26:19,155: t15.2023.11.03 val PER: 0.1391
2026-01-14 00:26:19,155: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:26:19,155: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:26:19,155: t15.2023.11.19 val PER: 0.0160
2026-01-14 00:26:19,155: t15.2023.11.26 val PER: 0.0428
2026-01-14 00:26:19,155: t15.2023.12.03 val PER: 0.0431
2026-01-14 00:26:19,156: t15.2023.12.08 val PER: 0.0353
2026-01-14 00:26:19,156: t15.2023.12.10 val PER: 0.0355
2026-01-14 00:26:19,156: t15.2023.12.17 val PER: 0.0863
2026-01-14 00:26:19,156: t15.2023.12.29 val PER: 0.0693
2026-01-14 00:26:19,156: t15.2024.02.25 val PER: 0.0646
2026-01-14 00:26:19,156: t15.2024.03.08 val PER: 0.1636
2026-01-14 00:26:19,156: t15.2024.03.15 val PER: 0.1507
2026-01-14 00:26:19,156: t15.2024.03.17 val PER: 0.0676
2026-01-14 00:26:19,156: t15.2024.05.10 val PER: 0.1144
2026-01-14 00:26:19,156: t15.2024.06.14 val PER: 0.1230
2026-01-14 00:26:19,157: t15.2024.07.19 val PER: 0.1543
2026-01-14 00:26:19,157: t15.2024.07.21 val PER: 0.0621
2026-01-14 00:26:19,157: t15.2024.07.28 val PER: 0.0971
2026-01-14 00:26:19,157: t15.2025.01.10 val PER: 0.2479
2026-01-14 00:26:19,157: t15.2025.01.12 val PER: 0.0939
2026-01-14 00:26:19,157: t15.2025.03.14 val PER: 0.2988
2026-01-14 00:26:19,157: t15.2025.03.16 val PER: 0.1283
2026-01-14 00:26:19,157: t15.2025.03.30 val PER: 0.2287
2026-01-14 00:26:19,157: t15.2025.04.13 val PER: 0.1740
2026-01-14 00:26:19,299: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_74500
2026-01-14 00:26:28,008: Train batch 74600: loss: 0.20 grad norm: 12.82 time: 0.079
2026-01-14 00:26:45,454: Train batch 74800: loss: 0.05 grad norm: 3.27 time: 0.061
2026-01-14 00:27:03,075: Train batch 75000: loss: 0.03 grad norm: 4.00 time: 0.059
2026-01-14 00:27:03,076: Running test after training batch: 75000
2026-01-14 00:27:03,191: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:27:08,221: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:27:08,282: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cent
2026-01-14 00:27:23,132: Val batch 75000: PER (avg): 0.1013 CTC Loss (avg): 19.3567 WER(5gram): 19.30% (n=256) time: 20.056
2026-01-14 00:27:23,133: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-14 00:27:23,133: t15.2023.08.13 val PER: 0.0696
2026-01-14 00:27:23,133: t15.2023.08.18 val PER: 0.0687
2026-01-14 00:27:23,133: t15.2023.08.20 val PER: 0.0588
2026-01-14 00:27:23,133: t15.2023.08.25 val PER: 0.0693
2026-01-14 00:27:23,133: t15.2023.08.27 val PER: 0.1447
2026-01-14 00:27:23,134: t15.2023.09.01 val PER: 0.0414
2026-01-14 00:27:23,134: t15.2023.09.03 val PER: 0.1176
2026-01-14 00:27:23,134: t15.2023.09.24 val PER: 0.0886
2026-01-14 00:27:23,134: t15.2023.09.29 val PER: 0.0964
2026-01-14 00:27:23,134: t15.2023.10.01 val PER: 0.1255
2026-01-14 00:27:23,134: t15.2023.10.06 val PER: 0.0474
2026-01-14 00:27:23,134: t15.2023.10.08 val PER: 0.1867
2026-01-14 00:27:23,134: t15.2023.10.13 val PER: 0.1358
2026-01-14 00:27:23,134: t15.2023.10.15 val PER: 0.1028
2026-01-14 00:27:23,134: t15.2023.10.20 val PER: 0.1477
2026-01-14 00:27:23,134: t15.2023.10.22 val PER: 0.0824
2026-01-14 00:27:23,135: t15.2023.11.03 val PER: 0.1418
2026-01-14 00:27:23,135: t15.2023.11.04 val PER: 0.0273
2026-01-14 00:27:23,135: t15.2023.11.17 val PER: 0.0156
2026-01-14 00:27:23,135: t15.2023.11.19 val PER: 0.0140
2026-01-14 00:27:23,135: t15.2023.11.26 val PER: 0.0420
2026-01-14 00:27:23,135: t15.2023.12.03 val PER: 0.0441
2026-01-14 00:27:23,135: t15.2023.12.08 val PER: 0.0353
2026-01-14 00:27:23,135: t15.2023.12.10 val PER: 0.0355
2026-01-14 00:27:23,135: t15.2023.12.17 val PER: 0.0873
2026-01-14 00:27:23,135: t15.2023.12.29 val PER: 0.0734
2026-01-14 00:27:23,135: t15.2024.02.25 val PER: 0.0618
2026-01-14 00:27:23,135: t15.2024.03.08 val PER: 0.1693
2026-01-14 00:27:23,135: t15.2024.03.15 val PER: 0.1470
2026-01-14 00:27:23,135: t15.2024.03.17 val PER: 0.0697
2026-01-14 00:27:23,135: t15.2024.05.10 val PER: 0.1233
2026-01-14 00:27:23,135: t15.2024.06.14 val PER: 0.1167
2026-01-14 00:27:23,136: t15.2024.07.19 val PER: 0.1582
2026-01-14 00:27:23,136: t15.2024.07.21 val PER: 0.0607
2026-01-14 00:27:23,136: t15.2024.07.28 val PER: 0.0934
2026-01-14 00:27:23,136: t15.2025.01.10 val PER: 0.2631
2026-01-14 00:27:23,136: t15.2025.01.12 val PER: 0.0878
2026-01-14 00:27:23,136: t15.2025.03.14 val PER: 0.2959
2026-01-14 00:27:23,136: t15.2025.03.16 val PER: 0.1335
2026-01-14 00:27:23,136: t15.2025.03.30 val PER: 0.2230
2026-01-14 00:27:23,136: t15.2025.04.13 val PER: 0.1740
2026-01-14 00:27:23,275: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_75000
2026-01-14 00:27:40,863: Train batch 75200: loss: 0.40 grad norm: 16.94 time: 0.055
2026-01-14 00:27:58,436: Train batch 75400: loss: 0.08 grad norm: 5.10 time: 0.049
2026-01-14 00:28:07,406: Running test after training batch: 75500
2026-01-14 00:28:07,663: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:28:12,522: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:28:12,596: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost doubt
2026-01-14 00:28:28,294: Val batch 75500: PER (avg): 0.1010 CTC Loss (avg): 19.4130 WER(5gram): 20.34% (n=256) time: 20.887
2026-01-14 00:28:28,294: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=13
2026-01-14 00:28:28,294: t15.2023.08.13 val PER: 0.0655
2026-01-14 00:28:28,294: t15.2023.08.18 val PER: 0.0645
2026-01-14 00:28:28,294: t15.2023.08.20 val PER: 0.0580
2026-01-14 00:28:28,295: t15.2023.08.25 val PER: 0.0708
2026-01-14 00:28:28,295: t15.2023.08.27 val PER: 0.1431
2026-01-14 00:28:28,295: t15.2023.09.01 val PER: 0.0398
2026-01-14 00:28:28,295: t15.2023.09.03 val PER: 0.1152
2026-01-14 00:28:28,295: t15.2023.09.24 val PER: 0.0886
2026-01-14 00:28:28,295: t15.2023.09.29 val PER: 0.0951
2026-01-14 00:28:28,295: t15.2023.10.01 val PER: 0.1255
2026-01-14 00:28:28,295: t15.2023.10.06 val PER: 0.0431
2026-01-14 00:28:28,295: t15.2023.10.08 val PER: 0.1949
2026-01-14 00:28:28,295: t15.2023.10.13 val PER: 0.1365
2026-01-14 00:28:28,295: t15.2023.10.15 val PER: 0.1035
2026-01-14 00:28:28,295: t15.2023.10.20 val PER: 0.1577
2026-01-14 00:28:28,295: t15.2023.10.22 val PER: 0.0824
2026-01-14 00:28:28,295: t15.2023.11.03 val PER: 0.1411
2026-01-14 00:28:28,295: t15.2023.11.04 val PER: 0.0239
2026-01-14 00:28:28,296: t15.2023.11.17 val PER: 0.0156
2026-01-14 00:28:28,296: t15.2023.11.19 val PER: 0.0200
2026-01-14 00:28:28,296: t15.2023.11.26 val PER: 0.0428
2026-01-14 00:28:28,296: t15.2023.12.03 val PER: 0.0441
2026-01-14 00:28:28,296: t15.2023.12.08 val PER: 0.0340
2026-01-14 00:28:28,296: t15.2023.12.10 val PER: 0.0329
2026-01-14 00:28:28,296: t15.2023.12.17 val PER: 0.0821
2026-01-14 00:28:28,296: t15.2023.12.29 val PER: 0.0755
2026-01-14 00:28:28,296: t15.2024.02.25 val PER: 0.0632
2026-01-14 00:28:28,296: t15.2024.03.08 val PER: 0.1536
2026-01-14 00:28:28,296: t15.2024.03.15 val PER: 0.1545
2026-01-14 00:28:28,296: t15.2024.03.17 val PER: 0.0690
2026-01-14 00:28:28,296: t15.2024.05.10 val PER: 0.1248
2026-01-14 00:28:28,296: t15.2024.06.14 val PER: 0.1183
2026-01-14 00:28:28,297: t15.2024.07.19 val PER: 0.1569
2026-01-14 00:28:28,297: t15.2024.07.21 val PER: 0.0572
2026-01-14 00:28:28,297: t15.2024.07.28 val PER: 0.0926
2026-01-14 00:28:28,297: t15.2025.01.10 val PER: 0.2562
2026-01-14 00:28:28,297: t15.2025.01.12 val PER: 0.0908
2026-01-14 00:28:28,297: t15.2025.03.14 val PER: 0.3047
2026-01-14 00:28:28,297: t15.2025.03.16 val PER: 0.1230
2026-01-14 00:28:28,297: t15.2025.03.30 val PER: 0.2345
2026-01-14 00:28:28,297: t15.2025.04.13 val PER: 0.1740
2026-01-14 00:28:28,438: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_75500
2026-01-14 00:28:37,132: Train batch 75600: loss: 0.07 grad norm: 6.65 time: 0.069
2026-01-14 00:28:54,399: Train batch 75800: loss: 0.14 grad norm: 13.64 time: 0.059
2026-01-14 00:29:12,027: Train batch 76000: loss: 0.19 grad norm: 16.73 time: 0.080
2026-01-14 00:29:12,027: Running test after training batch: 76000
2026-01-14 00:29:12,133: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:29:16,941: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:29:17,003: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost doubt
2026-01-14 00:29:31,838: Val batch 76000: PER (avg): 0.1010 CTC Loss (avg): 19.2679 WER(5gram): 19.88% (n=256) time: 19.811
2026-01-14 00:29:31,838: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-14 00:29:31,838: t15.2023.08.13 val PER: 0.0707
2026-01-14 00:29:31,839: t15.2023.08.18 val PER: 0.0662
2026-01-14 00:29:31,839: t15.2023.08.20 val PER: 0.0596
2026-01-14 00:29:31,839: t15.2023.08.25 val PER: 0.0738
2026-01-14 00:29:31,839: t15.2023.08.27 val PER: 0.1447
2026-01-14 00:29:31,839: t15.2023.09.01 val PER: 0.0398
2026-01-14 00:29:31,839: t15.2023.09.03 val PER: 0.1140
2026-01-14 00:29:31,839: t15.2023.09.24 val PER: 0.0874
2026-01-14 00:29:31,839: t15.2023.09.29 val PER: 0.0964
2026-01-14 00:29:31,839: t15.2023.10.01 val PER: 0.1301
2026-01-14 00:29:31,839: t15.2023.10.06 val PER: 0.0463
2026-01-14 00:29:31,839: t15.2023.10.08 val PER: 0.1922
2026-01-14 00:29:31,839: t15.2023.10.13 val PER: 0.1404
2026-01-14 00:29:31,840: t15.2023.10.15 val PER: 0.1015
2026-01-14 00:29:31,840: t15.2023.10.20 val PER: 0.1477
2026-01-14 00:29:31,840: t15.2023.10.22 val PER: 0.0813
2026-01-14 00:29:31,840: t15.2023.11.03 val PER: 0.1384
2026-01-14 00:29:31,840: t15.2023.11.04 val PER: 0.0137
2026-01-14 00:29:31,840: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:29:31,840: t15.2023.11.19 val PER: 0.0220
2026-01-14 00:29:31,840: t15.2023.11.26 val PER: 0.0428
2026-01-14 00:29:31,840: t15.2023.12.03 val PER: 0.0431
2026-01-14 00:29:31,840: t15.2023.12.08 val PER: 0.0340
2026-01-14 00:29:31,840: t15.2023.12.10 val PER: 0.0381
2026-01-14 00:29:31,840: t15.2023.12.17 val PER: 0.0821
2026-01-14 00:29:31,841: t15.2023.12.29 val PER: 0.0686
2026-01-14 00:29:31,841: t15.2024.02.25 val PER: 0.0674
2026-01-14 00:29:31,841: t15.2024.03.08 val PER: 0.1650
2026-01-14 00:29:31,841: t15.2024.03.15 val PER: 0.1532
2026-01-14 00:29:31,841: t15.2024.03.17 val PER: 0.0676
2026-01-14 00:29:31,841: t15.2024.05.10 val PER: 0.1144
2026-01-14 00:29:31,841: t15.2024.06.14 val PER: 0.1136
2026-01-14 00:29:31,841: t15.2024.07.19 val PER: 0.1510
2026-01-14 00:29:31,841: t15.2024.07.21 val PER: 0.0579
2026-01-14 00:29:31,841: t15.2024.07.28 val PER: 0.0912
2026-01-14 00:29:31,841: t15.2025.01.10 val PER: 0.2617
2026-01-14 00:29:31,841: t15.2025.01.12 val PER: 0.0931
2026-01-14 00:29:31,841: t15.2025.03.14 val PER: 0.2973
2026-01-14 00:29:31,841: t15.2025.03.16 val PER: 0.1283
2026-01-14 00:29:31,841: t15.2025.03.30 val PER: 0.2276
2026-01-14 00:29:31,841: t15.2025.04.13 val PER: 0.1840
2026-01-14 00:29:31,981: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_76000
2026-01-14 00:29:49,434: Train batch 76200: loss: 0.22 grad norm: 12.46 time: 0.056
2026-01-14 00:30:06,844: Train batch 76400: loss: 0.35 grad norm: 14.90 time: 0.054
2026-01-14 00:30:15,753: Running test after training batch: 76500
2026-01-14 00:30:15,863: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:30:20,709: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:30:20,789: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost doubt
2026-01-14 00:30:36,710: Val batch 76500: PER (avg): 0.1011 CTC Loss (avg): 19.2977 WER(5gram): 20.34% (n=256) time: 20.957
2026-01-14 00:30:36,711: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=13
2026-01-14 00:30:36,711: t15.2023.08.13 val PER: 0.0696
2026-01-14 00:30:36,711: t15.2023.08.18 val PER: 0.0671
2026-01-14 00:30:36,711: t15.2023.08.20 val PER: 0.0612
2026-01-14 00:30:36,711: t15.2023.08.25 val PER: 0.0708
2026-01-14 00:30:36,711: t15.2023.08.27 val PER: 0.1383
2026-01-14 00:30:36,711: t15.2023.09.01 val PER: 0.0430
2026-01-14 00:30:36,711: t15.2023.09.03 val PER: 0.1164
2026-01-14 00:30:36,711: t15.2023.09.24 val PER: 0.0850
2026-01-14 00:30:36,711: t15.2023.09.29 val PER: 0.0964
2026-01-14 00:30:36,711: t15.2023.10.01 val PER: 0.1308
2026-01-14 00:30:36,711: t15.2023.10.06 val PER: 0.0463
2026-01-14 00:30:36,712: t15.2023.10.08 val PER: 0.1935
2026-01-14 00:30:36,712: t15.2023.10.13 val PER: 0.1396
2026-01-14 00:30:36,712: t15.2023.10.15 val PER: 0.1009
2026-01-14 00:30:36,712: t15.2023.10.20 val PER: 0.1309
2026-01-14 00:30:36,712: t15.2023.10.22 val PER: 0.0802
2026-01-14 00:30:36,712: t15.2023.11.03 val PER: 0.1404
2026-01-14 00:30:36,712: t15.2023.11.04 val PER: 0.0171
2026-01-14 00:30:36,712: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:30:36,712: t15.2023.11.19 val PER: 0.0160
2026-01-14 00:30:36,712: t15.2023.11.26 val PER: 0.0442
2026-01-14 00:30:36,712: t15.2023.12.03 val PER: 0.0462
2026-01-14 00:30:36,713: t15.2023.12.08 val PER: 0.0320
2026-01-14 00:30:36,713: t15.2023.12.10 val PER: 0.0315
2026-01-14 00:30:36,713: t15.2023.12.17 val PER: 0.0811
2026-01-14 00:30:36,713: t15.2023.12.29 val PER: 0.0714
2026-01-14 00:30:36,713: t15.2024.02.25 val PER: 0.0646
2026-01-14 00:30:36,713: t15.2024.03.08 val PER: 0.1650
2026-01-14 00:30:36,713: t15.2024.03.15 val PER: 0.1476
2026-01-14 00:30:36,713: t15.2024.03.17 val PER: 0.0676
2026-01-14 00:30:36,713: t15.2024.05.10 val PER: 0.1144
2026-01-14 00:30:36,713: t15.2024.06.14 val PER: 0.1167
2026-01-14 00:30:36,713: t15.2024.07.19 val PER: 0.1543
2026-01-14 00:30:36,713: t15.2024.07.21 val PER: 0.0600
2026-01-14 00:30:36,713: t15.2024.07.28 val PER: 0.0971
2026-01-14 00:30:36,713: t15.2025.01.10 val PER: 0.2562
2026-01-14 00:30:36,714: t15.2025.01.12 val PER: 0.0978
2026-01-14 00:30:36,714: t15.2025.03.14 val PER: 0.3092
2026-01-14 00:30:36,714: t15.2025.03.16 val PER: 0.1283
2026-01-14 00:30:36,714: t15.2025.03.30 val PER: 0.2218
2026-01-14 00:30:36,714: t15.2025.04.13 val PER: 0.1812
2026-01-14 00:30:36,854: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_76500
2026-01-14 00:30:45,460: Train batch 76600: loss: 0.07 grad norm: 5.20 time: 0.056
2026-01-14 00:31:02,879: Train batch 76800: loss: 0.24 grad norm: 17.58 time: 0.053
2026-01-14 00:31:20,389: Train batch 77000: loss: 0.13 grad norm: 12.82 time: 0.060
2026-01-14 00:31:20,389: Running test after training batch: 77000
2026-01-14 00:31:20,571: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:31:25,896: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:31:25,958: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-14 00:31:41,917: Val batch 77000: PER (avg): 0.1014 CTC Loss (avg): 19.3340 WER(5gram): 19.82% (n=256) time: 21.528
2026-01-14 00:31:41,917: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=13
2026-01-14 00:31:41,918: t15.2023.08.13 val PER: 0.0717
2026-01-14 00:31:41,918: t15.2023.08.18 val PER: 0.0679
2026-01-14 00:31:41,918: t15.2023.08.20 val PER: 0.0572
2026-01-14 00:31:41,918: t15.2023.08.25 val PER: 0.0708
2026-01-14 00:31:41,918: t15.2023.08.27 val PER: 0.1463
2026-01-14 00:31:41,918: t15.2023.09.01 val PER: 0.0398
2026-01-14 00:31:41,918: t15.2023.09.03 val PER: 0.1223
2026-01-14 00:31:41,918: t15.2023.09.24 val PER: 0.0910
2026-01-14 00:31:41,918: t15.2023.09.29 val PER: 0.0957
2026-01-14 00:31:41,918: t15.2023.10.01 val PER: 0.1301
2026-01-14 00:31:41,919: t15.2023.10.06 val PER: 0.0452
2026-01-14 00:31:41,919: t15.2023.10.08 val PER: 0.1976
2026-01-14 00:31:41,919: t15.2023.10.13 val PER: 0.1420
2026-01-14 00:31:41,919: t15.2023.10.15 val PER: 0.1022
2026-01-14 00:31:41,919: t15.2023.10.20 val PER: 0.1477
2026-01-14 00:31:41,919: t15.2023.10.22 val PER: 0.0857
2026-01-14 00:31:41,919: t15.2023.11.03 val PER: 0.1377
2026-01-14 00:31:41,919: t15.2023.11.04 val PER: 0.0239
2026-01-14 00:31:41,919: t15.2023.11.17 val PER: 0.0156
2026-01-14 00:31:41,919: t15.2023.11.19 val PER: 0.0140
2026-01-14 00:31:41,919: t15.2023.11.26 val PER: 0.0406
2026-01-14 00:31:41,919: t15.2023.12.03 val PER: 0.0452
2026-01-14 00:31:41,919: t15.2023.12.08 val PER: 0.0346
2026-01-14 00:31:41,919: t15.2023.12.10 val PER: 0.0315
2026-01-14 00:31:41,919: t15.2023.12.17 val PER: 0.0790
2026-01-14 00:31:41,920: t15.2023.12.29 val PER: 0.0700
2026-01-14 00:31:41,920: t15.2024.02.25 val PER: 0.0632
2026-01-14 00:31:41,920: t15.2024.03.08 val PER: 0.1593
2026-01-14 00:31:41,920: t15.2024.03.15 val PER: 0.1526
2026-01-14 00:31:41,920: t15.2024.03.17 val PER: 0.0676
2026-01-14 00:31:41,920: t15.2024.05.10 val PER: 0.1263
2026-01-14 00:31:41,920: t15.2024.06.14 val PER: 0.1088
2026-01-14 00:31:41,920: t15.2024.07.19 val PER: 0.1556
2026-01-14 00:31:41,920: t15.2024.07.21 val PER: 0.0607
2026-01-14 00:31:41,920: t15.2024.07.28 val PER: 0.0934
2026-01-14 00:31:41,920: t15.2025.01.10 val PER: 0.2562
2026-01-14 00:31:41,920: t15.2025.01.12 val PER: 0.0916
2026-01-14 00:31:41,920: t15.2025.03.14 val PER: 0.3018
2026-01-14 00:31:41,920: t15.2025.03.16 val PER: 0.1270
2026-01-14 00:31:41,921: t15.2025.03.30 val PER: 0.2287
2026-01-14 00:31:41,921: t15.2025.04.13 val PER: 0.1854
2026-01-14 00:31:42,059: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_77000
2026-01-14 00:31:59,275: Train batch 77200: loss: 0.15 grad norm: 14.57 time: 0.054
2026-01-14 00:32:16,802: Train batch 77400: loss: 0.10 grad norm: 7.89 time: 0.060
2026-01-14 00:32:25,587: Running test after training batch: 77500
2026-01-14 00:32:25,703: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:32:30,538: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:32:30,606: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cent
2026-01-14 00:32:46,410: Val batch 77500: PER (avg): 0.1027 CTC Loss (avg): 19.4837 WER(5gram): 20.40% (n=256) time: 20.822
2026-01-14 00:32:46,410: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=13
2026-01-14 00:32:46,410: t15.2023.08.13 val PER: 0.0686
2026-01-14 00:32:46,410: t15.2023.08.18 val PER: 0.0637
2026-01-14 00:32:46,411: t15.2023.08.20 val PER: 0.0596
2026-01-14 00:32:46,411: t15.2023.08.25 val PER: 0.0768
2026-01-14 00:32:46,411: t15.2023.08.27 val PER: 0.1447
2026-01-14 00:32:46,411: t15.2023.09.01 val PER: 0.0422
2026-01-14 00:32:46,411: t15.2023.09.03 val PER: 0.1188
2026-01-14 00:32:46,411: t15.2023.09.24 val PER: 0.0874
2026-01-14 00:32:46,411: t15.2023.09.29 val PER: 0.0970
2026-01-14 00:32:46,411: t15.2023.10.01 val PER: 0.1321
2026-01-14 00:32:46,411: t15.2023.10.06 val PER: 0.0452
2026-01-14 00:32:46,411: t15.2023.10.08 val PER: 0.1935
2026-01-14 00:32:46,411: t15.2023.10.13 val PER: 0.1404
2026-01-14 00:32:46,411: t15.2023.10.15 val PER: 0.1015
2026-01-14 00:32:46,411: t15.2023.10.20 val PER: 0.1376
2026-01-14 00:32:46,411: t15.2023.10.22 val PER: 0.0835
2026-01-14 00:32:46,412: t15.2023.11.03 val PER: 0.1445
2026-01-14 00:32:46,412: t15.2023.11.04 val PER: 0.0239
2026-01-14 00:32:46,412: t15.2023.11.17 val PER: 0.0140
2026-01-14 00:32:46,412: t15.2023.11.19 val PER: 0.0160
2026-01-14 00:32:46,412: t15.2023.11.26 val PER: 0.0464
2026-01-14 00:32:46,412: t15.2023.12.03 val PER: 0.0452
2026-01-14 00:32:46,412: t15.2023.12.08 val PER: 0.0333
2026-01-14 00:32:46,412: t15.2023.12.10 val PER: 0.0394
2026-01-14 00:32:46,412: t15.2023.12.17 val PER: 0.0811
2026-01-14 00:32:46,412: t15.2023.12.29 val PER: 0.0734
2026-01-14 00:32:46,412: t15.2024.02.25 val PER: 0.0674
2026-01-14 00:32:46,412: t15.2024.03.08 val PER: 0.1607
2026-01-14 00:32:46,412: t15.2024.03.15 val PER: 0.1513
2026-01-14 00:32:46,413: t15.2024.03.17 val PER: 0.0739
2026-01-14 00:32:46,413: t15.2024.05.10 val PER: 0.1278
2026-01-14 00:32:46,413: t15.2024.06.14 val PER: 0.1246
2026-01-14 00:32:46,413: t15.2024.07.19 val PER: 0.1595
2026-01-14 00:32:46,413: t15.2024.07.21 val PER: 0.0593
2026-01-14 00:32:46,413: t15.2024.07.28 val PER: 0.0963
2026-01-14 00:32:46,413: t15.2025.01.10 val PER: 0.2576
2026-01-14 00:32:46,413: t15.2025.01.12 val PER: 0.0970
2026-01-14 00:32:46,413: t15.2025.03.14 val PER: 0.3121
2026-01-14 00:32:46,413: t15.2025.03.16 val PER: 0.1283
2026-01-14 00:32:46,413: t15.2025.03.30 val PER: 0.2230
2026-01-14 00:32:46,413: t15.2025.04.13 val PER: 0.1769
2026-01-14 00:32:46,548: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_77500
2026-01-14 00:32:55,170: Train batch 77600: loss: 0.23 grad norm: 20.46 time: 0.065
2026-01-14 00:33:12,470: Train batch 77800: loss: 0.42 grad norm: 17.15 time: 0.053
2026-01-14 00:33:29,992: Train batch 78000: loss: 0.01 grad norm: 0.88 time: 0.071
2026-01-14 00:33:29,992: Running test after training batch: 78000
2026-01-14 00:33:30,106: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:33:34,966: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:33:35,023: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cent
2026-01-14 00:33:49,187: Val batch 78000: PER (avg): 0.1016 CTC Loss (avg): 19.3759 WER(5gram): 20.53% (n=256) time: 19.194
2026-01-14 00:33:49,188: WER lens: avg_true_words=5.99 avg_pred_words=6.39 max_pred_words=13
2026-01-14 00:33:49,188: t15.2023.08.13 val PER: 0.0707
2026-01-14 00:33:49,188: t15.2023.08.18 val PER: 0.0679
2026-01-14 00:33:49,188: t15.2023.08.20 val PER: 0.0588
2026-01-14 00:33:49,188: t15.2023.08.25 val PER: 0.0678
2026-01-14 00:33:49,188: t15.2023.08.27 val PER: 0.1367
2026-01-14 00:33:49,188: t15.2023.09.01 val PER: 0.0430
2026-01-14 00:33:49,188: t15.2023.09.03 val PER: 0.1200
2026-01-14 00:33:49,189: t15.2023.09.24 val PER: 0.0922
2026-01-14 00:33:49,189: t15.2023.09.29 val PER: 0.0976
2026-01-14 00:33:49,189: t15.2023.10.01 val PER: 0.1281
2026-01-14 00:33:49,189: t15.2023.10.06 val PER: 0.0431
2026-01-14 00:33:49,189: t15.2023.10.08 val PER: 0.1989
2026-01-14 00:33:49,189: t15.2023.10.13 val PER: 0.1412
2026-01-14 00:33:49,189: t15.2023.10.15 val PER: 0.1081
2026-01-14 00:33:49,189: t15.2023.10.20 val PER: 0.1544
2026-01-14 00:33:49,189: t15.2023.10.22 val PER: 0.0891
2026-01-14 00:33:49,189: t15.2023.11.03 val PER: 0.1431
2026-01-14 00:33:49,190: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:33:49,190: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:33:49,190: t15.2023.11.19 val PER: 0.0180
2026-01-14 00:33:49,190: t15.2023.11.26 val PER: 0.0413
2026-01-14 00:33:49,190: t15.2023.12.03 val PER: 0.0473
2026-01-14 00:33:49,190: t15.2023.12.08 val PER: 0.0346
2026-01-14 00:33:49,190: t15.2023.12.10 val PER: 0.0368
2026-01-14 00:33:49,190: t15.2023.12.17 val PER: 0.0821
2026-01-14 00:33:49,190: t15.2023.12.29 val PER: 0.0673
2026-01-14 00:33:49,190: t15.2024.02.25 val PER: 0.0674
2026-01-14 00:33:49,190: t15.2024.03.08 val PER: 0.1622
2026-01-14 00:33:49,190: t15.2024.03.15 val PER: 0.1507
2026-01-14 00:33:49,190: t15.2024.03.17 val PER: 0.0781
2026-01-14 00:33:49,190: t15.2024.05.10 val PER: 0.1174
2026-01-14 00:33:49,190: t15.2024.06.14 val PER: 0.1183
2026-01-14 00:33:49,190: t15.2024.07.19 val PER: 0.1543
2026-01-14 00:33:49,190: t15.2024.07.21 val PER: 0.0566
2026-01-14 00:33:49,191: t15.2024.07.28 val PER: 0.0941
2026-01-14 00:33:49,191: t15.2025.01.10 val PER: 0.2452
2026-01-14 00:33:49,191: t15.2025.01.12 val PER: 0.0878
2026-01-14 00:33:49,191: t15.2025.03.14 val PER: 0.2959
2026-01-14 00:33:49,191: t15.2025.03.16 val PER: 0.1296
2026-01-14 00:33:49,191: t15.2025.03.30 val PER: 0.2230
2026-01-14 00:33:49,191: t15.2025.04.13 val PER: 0.1740
2026-01-14 00:33:49,336: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_78000
2026-01-14 00:34:06,604: Train batch 78200: loss: 0.19 grad norm: 11.97 time: 0.057
2026-01-14 00:34:24,192: Train batch 78400: loss: 0.06 grad norm: 4.54 time: 0.061
2026-01-14 00:34:32,893: Running test after training batch: 78500
2026-01-14 00:34:33,003: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:34:38,200: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:34:38,270: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cents
2026-01-14 00:34:53,138: Val batch 78500: PER (avg): 0.1027 CTC Loss (avg): 19.6889 WER(5gram): 19.49% (n=256) time: 20.245
2026-01-14 00:34:53,139: WER lens: avg_true_words=5.99 avg_pred_words=6.34 max_pred_words=13
2026-01-14 00:34:53,139: t15.2023.08.13 val PER: 0.0759
2026-01-14 00:34:53,139: t15.2023.08.18 val PER: 0.0687
2026-01-14 00:34:53,139: t15.2023.08.20 val PER: 0.0635
2026-01-14 00:34:53,139: t15.2023.08.25 val PER: 0.0663
2026-01-14 00:34:53,139: t15.2023.08.27 val PER: 0.1495
2026-01-14 00:34:53,139: t15.2023.09.01 val PER: 0.0398
2026-01-14 00:34:53,139: t15.2023.09.03 val PER: 0.1188
2026-01-14 00:34:53,139: t15.2023.09.24 val PER: 0.0850
2026-01-14 00:34:53,140: t15.2023.09.29 val PER: 0.0964
2026-01-14 00:34:53,140: t15.2023.10.01 val PER: 0.1301
2026-01-14 00:34:53,140: t15.2023.10.06 val PER: 0.0420
2026-01-14 00:34:53,140: t15.2023.10.08 val PER: 0.2016
2026-01-14 00:34:53,140: t15.2023.10.13 val PER: 0.1451
2026-01-14 00:34:53,140: t15.2023.10.15 val PER: 0.1061
2026-01-14 00:34:53,140: t15.2023.10.20 val PER: 0.1477
2026-01-14 00:34:53,140: t15.2023.10.22 val PER: 0.0869
2026-01-14 00:34:53,140: t15.2023.11.03 val PER: 0.1431
2026-01-14 00:34:53,140: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:34:53,140: t15.2023.11.17 val PER: 0.0202
2026-01-14 00:34:53,141: t15.2023.11.19 val PER: 0.0160
2026-01-14 00:34:53,141: t15.2023.11.26 val PER: 0.0442
2026-01-14 00:34:53,141: t15.2023.12.03 val PER: 0.0462
2026-01-14 00:34:53,141: t15.2023.12.08 val PER: 0.0306
2026-01-14 00:34:53,141: t15.2023.12.10 val PER: 0.0355
2026-01-14 00:34:53,141: t15.2023.12.17 val PER: 0.0811
2026-01-14 00:34:53,141: t15.2023.12.29 val PER: 0.0721
2026-01-14 00:34:53,141: t15.2024.02.25 val PER: 0.0674
2026-01-14 00:34:53,141: t15.2024.03.08 val PER: 0.1636
2026-01-14 00:34:53,141: t15.2024.03.15 val PER: 0.1513
2026-01-14 00:34:53,141: t15.2024.03.17 val PER: 0.0746
2026-01-14 00:34:53,141: t15.2024.05.10 val PER: 0.1263
2026-01-14 00:34:53,141: t15.2024.06.14 val PER: 0.1199
2026-01-14 00:34:53,141: t15.2024.07.19 val PER: 0.1556
2026-01-14 00:34:53,141: t15.2024.07.21 val PER: 0.0559
2026-01-14 00:34:53,142: t15.2024.07.28 val PER: 0.0934
2026-01-14 00:34:53,142: t15.2025.01.10 val PER: 0.2534
2026-01-14 00:34:53,142: t15.2025.01.12 val PER: 0.0901
2026-01-14 00:34:53,142: t15.2025.03.14 val PER: 0.3003
2026-01-14 00:34:53,142: t15.2025.03.16 val PER: 0.1296
2026-01-14 00:34:53,142: t15.2025.03.30 val PER: 0.2345
2026-01-14 00:34:53,142: t15.2025.04.13 val PER: 0.1883
2026-01-14 00:34:53,281: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_78500
2026-01-14 00:35:02,166: Train batch 78600: loss: 0.36 grad norm: 43.81 time: 0.054
2026-01-14 00:35:19,531: Train batch 78800: loss: 0.11 grad norm: 11.25 time: 0.078
2026-01-14 00:35:36,951: Train batch 79000: loss: 0.11 grad norm: 7.19 time: 0.049
2026-01-14 00:35:36,952: Running test after training batch: 79000
2026-01-14 00:35:37,183: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:35:42,018: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:35:42,083: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost out
2026-01-14 00:35:56,756: Val batch 79000: PER (avg): 0.1022 CTC Loss (avg): 19.8777 WER(5gram): 20.60% (n=256) time: 19.805
2026-01-14 00:35:56,757: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=13
2026-01-14 00:35:56,757: t15.2023.08.13 val PER: 0.0696
2026-01-14 00:35:56,757: t15.2023.08.18 val PER: 0.0721
2026-01-14 00:35:56,757: t15.2023.08.20 val PER: 0.0604
2026-01-14 00:35:56,757: t15.2023.08.25 val PER: 0.0738
2026-01-14 00:35:56,758: t15.2023.08.27 val PER: 0.1431
2026-01-14 00:35:56,758: t15.2023.09.01 val PER: 0.0414
2026-01-14 00:35:56,758: t15.2023.09.03 val PER: 0.1200
2026-01-14 00:35:56,758: t15.2023.09.24 val PER: 0.0813
2026-01-14 00:35:56,758: t15.2023.09.29 val PER: 0.0957
2026-01-14 00:35:56,758: t15.2023.10.01 val PER: 0.1361
2026-01-14 00:35:56,758: t15.2023.10.06 val PER: 0.0463
2026-01-14 00:35:56,758: t15.2023.10.08 val PER: 0.1867
2026-01-14 00:35:56,758: t15.2023.10.13 val PER: 0.1381
2026-01-14 00:35:56,758: t15.2023.10.15 val PER: 0.1048
2026-01-14 00:35:56,759: t15.2023.10.20 val PER: 0.1544
2026-01-14 00:35:56,759: t15.2023.10.22 val PER: 0.0869
2026-01-14 00:35:56,759: t15.2023.11.03 val PER: 0.1411
2026-01-14 00:35:56,759: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:35:56,759: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:35:56,759: t15.2023.11.19 val PER: 0.0160
2026-01-14 00:35:56,759: t15.2023.11.26 val PER: 0.0406
2026-01-14 00:35:56,759: t15.2023.12.03 val PER: 0.0410
2026-01-14 00:35:56,759: t15.2023.12.08 val PER: 0.0326
2026-01-14 00:35:56,759: t15.2023.12.10 val PER: 0.0394
2026-01-14 00:35:56,760: t15.2023.12.17 val PER: 0.0800
2026-01-14 00:35:56,760: t15.2023.12.29 val PER: 0.0734
2026-01-14 00:35:56,760: t15.2024.02.25 val PER: 0.0618
2026-01-14 00:35:56,760: t15.2024.03.08 val PER: 0.1579
2026-01-14 00:35:56,760: t15.2024.03.15 val PER: 0.1526
2026-01-14 00:35:56,760: t15.2024.03.17 val PER: 0.0795
2026-01-14 00:35:56,760: t15.2024.05.10 val PER: 0.1233
2026-01-14 00:35:56,760: t15.2024.06.14 val PER: 0.1151
2026-01-14 00:35:56,760: t15.2024.07.19 val PER: 0.1569
2026-01-14 00:35:56,760: t15.2024.07.21 val PER: 0.0593
2026-01-14 00:35:56,761: t15.2024.07.28 val PER: 0.0956
2026-01-14 00:35:56,761: t15.2025.01.10 val PER: 0.2548
2026-01-14 00:35:56,761: t15.2025.01.12 val PER: 0.0931
2026-01-14 00:35:56,761: t15.2025.03.14 val PER: 0.3018
2026-01-14 00:35:56,761: t15.2025.03.16 val PER: 0.1270
2026-01-14 00:35:56,761: t15.2025.03.30 val PER: 0.2241
2026-01-14 00:35:56,761: t15.2025.04.13 val PER: 0.1883
2026-01-14 00:35:56,901: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_79000
2026-01-14 00:36:14,473: Train batch 79200: loss: 0.16 grad norm: 13.60 time: 0.061
2026-01-14 00:36:31,797: Train batch 79400: loss: 0.02 grad norm: 2.51 time: 0.066
2026-01-14 00:36:40,577: Running test after training batch: 79500
2026-01-14 00:36:40,699: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:36:45,600: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:36:45,668: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:37:01,587: Val batch 79500: PER (avg): 0.1011 CTC Loss (avg): 19.5050 WER(5gram): 19.49% (n=256) time: 21.010
2026-01-14 00:37:01,587: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-14 00:37:01,588: t15.2023.08.13 val PER: 0.0696
2026-01-14 00:37:01,588: t15.2023.08.18 val PER: 0.0687
2026-01-14 00:37:01,588: t15.2023.08.20 val PER: 0.0580
2026-01-14 00:37:01,588: t15.2023.08.25 val PER: 0.0693
2026-01-14 00:37:01,588: t15.2023.08.27 val PER: 0.1415
2026-01-14 00:37:01,588: t15.2023.09.01 val PER: 0.0373
2026-01-14 00:37:01,588: t15.2023.09.03 val PER: 0.1152
2026-01-14 00:37:01,588: t15.2023.09.24 val PER: 0.0862
2026-01-14 00:37:01,588: t15.2023.09.29 val PER: 0.0970
2026-01-14 00:37:01,588: t15.2023.10.01 val PER: 0.1334
2026-01-14 00:37:01,589: t15.2023.10.06 val PER: 0.0463
2026-01-14 00:37:01,589: t15.2023.10.08 val PER: 0.1867
2026-01-14 00:37:01,589: t15.2023.10.13 val PER: 0.1350
2026-01-14 00:37:01,589: t15.2023.10.15 val PER: 0.1035
2026-01-14 00:37:01,589: t15.2023.10.20 val PER: 0.1544
2026-01-14 00:37:01,589: t15.2023.10.22 val PER: 0.0935
2026-01-14 00:37:01,589: t15.2023.11.03 val PER: 0.1404
2026-01-14 00:37:01,589: t15.2023.11.04 val PER: 0.0171
2026-01-14 00:37:01,589: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:37:01,589: t15.2023.11.19 val PER: 0.0140
2026-01-14 00:37:01,589: t15.2023.11.26 val PER: 0.0442
2026-01-14 00:37:01,589: t15.2023.12.03 val PER: 0.0473
2026-01-14 00:37:01,589: t15.2023.12.08 val PER: 0.0300
2026-01-14 00:37:01,590: t15.2023.12.10 val PER: 0.0329
2026-01-14 00:37:01,590: t15.2023.12.17 val PER: 0.0780
2026-01-14 00:37:01,590: t15.2023.12.29 val PER: 0.0728
2026-01-14 00:37:01,590: t15.2024.02.25 val PER: 0.0632
2026-01-14 00:37:01,590: t15.2024.03.08 val PER: 0.1579
2026-01-14 00:37:01,590: t15.2024.03.15 val PER: 0.1507
2026-01-14 00:37:01,590: t15.2024.03.17 val PER: 0.0725
2026-01-14 00:37:01,590: t15.2024.05.10 val PER: 0.1204
2026-01-14 00:37:01,590: t15.2024.06.14 val PER: 0.1167
2026-01-14 00:37:01,590: t15.2024.07.19 val PER: 0.1595
2026-01-14 00:37:01,590: t15.2024.07.21 val PER: 0.0572
2026-01-14 00:37:01,590: t15.2024.07.28 val PER: 0.0941
2026-01-14 00:37:01,590: t15.2025.01.10 val PER: 0.2562
2026-01-14 00:37:01,591: t15.2025.01.12 val PER: 0.0901
2026-01-14 00:37:01,591: t15.2025.03.14 val PER: 0.2944
2026-01-14 00:37:01,591: t15.2025.03.16 val PER: 0.1257
2026-01-14 00:37:01,591: t15.2025.03.30 val PER: 0.2299
2026-01-14 00:37:01,591: t15.2025.04.13 val PER: 0.1769
2026-01-14 00:37:01,731: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_79500
2026-01-14 00:37:10,368: Train batch 79600: loss: 0.07 grad norm: 7.64 time: 0.069
2026-01-14 00:37:27,843: Train batch 79800: loss: 0.24 grad norm: 14.15 time: 0.055
2026-01-14 00:37:45,334: Train batch 80000: loss: 0.12 grad norm: 12.30 time: 0.066
2026-01-14 00:37:45,335: Running test after training batch: 80000
2026-01-14 00:37:45,438: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:37:50,283: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:37:50,361: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cent
2026-01-14 00:38:06,281: Val batch 80000: PER (avg): 0.1012 CTC Loss (avg): 19.4991 WER(5gram): 19.04% (n=256) time: 20.946
2026-01-14 00:38:06,281: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=13
2026-01-14 00:38:06,282: t15.2023.08.13 val PER: 0.0665
2026-01-14 00:38:06,282: t15.2023.08.18 val PER: 0.0662
2026-01-14 00:38:06,282: t15.2023.08.20 val PER: 0.0572
2026-01-14 00:38:06,282: t15.2023.08.25 val PER: 0.0723
2026-01-14 00:38:06,282: t15.2023.08.27 val PER: 0.1495
2026-01-14 00:38:06,282: t15.2023.09.01 val PER: 0.0390
2026-01-14 00:38:06,282: t15.2023.09.03 val PER: 0.1140
2026-01-14 00:38:06,282: t15.2023.09.24 val PER: 0.0898
2026-01-14 00:38:06,282: t15.2023.09.29 val PER: 0.0976
2026-01-14 00:38:06,282: t15.2023.10.01 val PER: 0.1308
2026-01-14 00:38:06,282: t15.2023.10.06 val PER: 0.0452
2026-01-14 00:38:06,283: t15.2023.10.08 val PER: 0.1922
2026-01-14 00:38:06,283: t15.2023.10.13 val PER: 0.1381
2026-01-14 00:38:06,283: t15.2023.10.15 val PER: 0.1022
2026-01-14 00:38:06,283: t15.2023.10.20 val PER: 0.1544
2026-01-14 00:38:06,283: t15.2023.10.22 val PER: 0.0869
2026-01-14 00:38:06,283: t15.2023.11.03 val PER: 0.1445
2026-01-14 00:38:06,283: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:38:06,283: t15.2023.11.17 val PER: 0.0140
2026-01-14 00:38:06,283: t15.2023.11.19 val PER: 0.0160
2026-01-14 00:38:06,283: t15.2023.11.26 val PER: 0.0457
2026-01-14 00:38:06,283: t15.2023.12.03 val PER: 0.0410
2026-01-14 00:38:06,283: t15.2023.12.08 val PER: 0.0326
2026-01-14 00:38:06,283: t15.2023.12.10 val PER: 0.0315
2026-01-14 00:38:06,284: t15.2023.12.17 val PER: 0.0811
2026-01-14 00:38:06,284: t15.2023.12.29 val PER: 0.0693
2026-01-14 00:38:06,284: t15.2024.02.25 val PER: 0.0604
2026-01-14 00:38:06,284: t15.2024.03.08 val PER: 0.1479
2026-01-14 00:38:06,284: t15.2024.03.15 val PER: 0.1451
2026-01-14 00:38:06,284: t15.2024.03.17 val PER: 0.0725
2026-01-14 00:38:06,284: t15.2024.05.10 val PER: 0.1248
2026-01-14 00:38:06,284: t15.2024.06.14 val PER: 0.1199
2026-01-14 00:38:06,284: t15.2024.07.19 val PER: 0.1661
2026-01-14 00:38:06,284: t15.2024.07.21 val PER: 0.0607
2026-01-14 00:38:06,284: t15.2024.07.28 val PER: 0.0897
2026-01-14 00:38:06,284: t15.2025.01.10 val PER: 0.2603
2026-01-14 00:38:06,284: t15.2025.01.12 val PER: 0.0931
2026-01-14 00:38:06,284: t15.2025.03.14 val PER: 0.2944
2026-01-14 00:38:06,284: t15.2025.03.16 val PER: 0.1270
2026-01-14 00:38:06,284: t15.2025.03.30 val PER: 0.2218
2026-01-14 00:38:06,285: t15.2025.04.13 val PER: 0.1854
2026-01-14 00:38:06,427: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_80000
2026-01-14 00:38:24,179: Train batch 80200: loss: 0.27 grad norm: 38.44 time: 0.068
2026-01-14 00:38:41,412: Train batch 80400: loss: 0.08 grad norm: 10.18 time: 0.060
2026-01-14 00:38:50,192: Running test after training batch: 80500
2026-01-14 00:38:50,310: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:38:55,421: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:38:55,483: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:39:11,450: Val batch 80500: PER (avg): 0.1008 CTC Loss (avg): 19.4119 WER(5gram): 19.10% (n=256) time: 21.258
2026-01-14 00:39:11,451: WER lens: avg_true_words=5.99 avg_pred_words=6.34 max_pred_words=13
2026-01-14 00:39:11,451: t15.2023.08.13 val PER: 0.0665
2026-01-14 00:39:11,451: t15.2023.08.18 val PER: 0.0654
2026-01-14 00:39:11,451: t15.2023.08.20 val PER: 0.0556
2026-01-14 00:39:11,452: t15.2023.08.25 val PER: 0.0678
2026-01-14 00:39:11,452: t15.2023.08.27 val PER: 0.1415
2026-01-14 00:39:11,452: t15.2023.09.01 val PER: 0.0414
2026-01-14 00:39:11,452: t15.2023.09.03 val PER: 0.1152
2026-01-14 00:39:11,452: t15.2023.09.24 val PER: 0.0922
2026-01-14 00:39:11,452: t15.2023.09.29 val PER: 0.0976
2026-01-14 00:39:11,452: t15.2023.10.01 val PER: 0.1314
2026-01-14 00:39:11,452: t15.2023.10.06 val PER: 0.0409
2026-01-14 00:39:11,452: t15.2023.10.08 val PER: 0.1840
2026-01-14 00:39:11,452: t15.2023.10.13 val PER: 0.1350
2026-01-14 00:39:11,453: t15.2023.10.15 val PER: 0.1061
2026-01-14 00:39:11,453: t15.2023.10.20 val PER: 0.1544
2026-01-14 00:39:11,453: t15.2023.10.22 val PER: 0.0913
2026-01-14 00:39:11,453: t15.2023.11.03 val PER: 0.1398
2026-01-14 00:39:11,453: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:39:11,453: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:39:11,453: t15.2023.11.19 val PER: 0.0180
2026-01-14 00:39:11,453: t15.2023.11.26 val PER: 0.0457
2026-01-14 00:39:11,453: t15.2023.12.03 val PER: 0.0441
2026-01-14 00:39:11,453: t15.2023.12.08 val PER: 0.0326
2026-01-14 00:39:11,453: t15.2023.12.10 val PER: 0.0302
2026-01-14 00:39:11,454: t15.2023.12.17 val PER: 0.0769
2026-01-14 00:39:11,454: t15.2023.12.29 val PER: 0.0700
2026-01-14 00:39:11,454: t15.2024.02.25 val PER: 0.0562
2026-01-14 00:39:11,454: t15.2024.03.08 val PER: 0.1522
2026-01-14 00:39:11,454: t15.2024.03.15 val PER: 0.1501
2026-01-14 00:39:11,454: t15.2024.03.17 val PER: 0.0718
2026-01-14 00:39:11,454: t15.2024.05.10 val PER: 0.1204
2026-01-14 00:39:11,454: t15.2024.06.14 val PER: 0.1215
2026-01-14 00:39:11,454: t15.2024.07.19 val PER: 0.1562
2026-01-14 00:39:11,454: t15.2024.07.21 val PER: 0.0600
2026-01-14 00:39:11,454: t15.2024.07.28 val PER: 0.0912
2026-01-14 00:39:11,455: t15.2025.01.10 val PER: 0.2548
2026-01-14 00:39:11,455: t15.2025.01.12 val PER: 0.0924
2026-01-14 00:39:11,455: t15.2025.03.14 val PER: 0.2973
2026-01-14 00:39:11,455: t15.2025.03.16 val PER: 0.1243
2026-01-14 00:39:11,455: t15.2025.03.30 val PER: 0.2310
2026-01-14 00:39:11,455: t15.2025.04.13 val PER: 0.1883
2026-01-14 00:39:11,592: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_80500
2026-01-14 00:39:20,693: Train batch 80600: loss: 0.21 grad norm: 14.02 time: 0.055
2026-01-14 00:39:38,382: Train batch 80800: loss: 0.16 grad norm: 21.70 time: 0.055
2026-01-14 00:39:55,741: Train batch 81000: loss: 0.10 grad norm: 10.24 time: 0.058
2026-01-14 00:39:55,741: Running test after training batch: 81000
2026-01-14 00:39:55,901: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:40:00,770: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:40:00,831: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-14 00:40:17,040: Val batch 81000: PER (avg): 0.1015 CTC Loss (avg): 19.3461 WER(5gram): 18.97% (n=256) time: 21.299
2026-01-14 00:40:17,040: WER lens: avg_true_words=5.99 avg_pred_words=6.34 max_pred_words=13
2026-01-14 00:40:17,040: t15.2023.08.13 val PER: 0.0696
2026-01-14 00:40:17,040: t15.2023.08.18 val PER: 0.0696
2026-01-14 00:40:17,041: t15.2023.08.20 val PER: 0.0572
2026-01-14 00:40:17,041: t15.2023.08.25 val PER: 0.0678
2026-01-14 00:40:17,041: t15.2023.08.27 val PER: 0.1576
2026-01-14 00:40:17,041: t15.2023.09.01 val PER: 0.0422
2026-01-14 00:40:17,041: t15.2023.09.03 val PER: 0.1164
2026-01-14 00:40:17,041: t15.2023.09.24 val PER: 0.0898
2026-01-14 00:40:17,041: t15.2023.09.29 val PER: 0.1002
2026-01-14 00:40:17,041: t15.2023.10.01 val PER: 0.1341
2026-01-14 00:40:17,041: t15.2023.10.06 val PER: 0.0409
2026-01-14 00:40:17,041: t15.2023.10.08 val PER: 0.1840
2026-01-14 00:40:17,041: t15.2023.10.13 val PER: 0.1280
2026-01-14 00:40:17,041: t15.2023.10.15 val PER: 0.1048
2026-01-14 00:40:17,042: t15.2023.10.20 val PER: 0.1577
2026-01-14 00:40:17,042: t15.2023.10.22 val PER: 0.0846
2026-01-14 00:40:17,042: t15.2023.11.03 val PER: 0.1431
2026-01-14 00:40:17,042: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:40:17,042: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:40:17,042: t15.2023.11.19 val PER: 0.0160
2026-01-14 00:40:17,042: t15.2023.11.26 val PER: 0.0442
2026-01-14 00:40:17,042: t15.2023.12.03 val PER: 0.0410
2026-01-14 00:40:17,042: t15.2023.12.08 val PER: 0.0340
2026-01-14 00:40:17,042: t15.2023.12.10 val PER: 0.0315
2026-01-14 00:40:17,042: t15.2023.12.17 val PER: 0.0800
2026-01-14 00:40:17,042: t15.2023.12.29 val PER: 0.0707
2026-01-14 00:40:17,042: t15.2024.02.25 val PER: 0.0632
2026-01-14 00:40:17,042: t15.2024.03.08 val PER: 0.1550
2026-01-14 00:40:17,042: t15.2024.03.15 val PER: 0.1470
2026-01-14 00:40:17,043: t15.2024.03.17 val PER: 0.0690
2026-01-14 00:40:17,043: t15.2024.05.10 val PER: 0.1204
2026-01-14 00:40:17,043: t15.2024.06.14 val PER: 0.1183
2026-01-14 00:40:17,043: t15.2024.07.19 val PER: 0.1562
2026-01-14 00:40:17,043: t15.2024.07.21 val PER: 0.0614
2026-01-14 00:40:17,043: t15.2024.07.28 val PER: 0.0926
2026-01-14 00:40:17,043: t15.2025.01.10 val PER: 0.2658
2026-01-14 00:40:17,043: t15.2025.01.12 val PER: 0.0993
2026-01-14 00:40:17,043: t15.2025.03.14 val PER: 0.3003
2026-01-14 00:40:17,043: t15.2025.03.16 val PER: 0.1270
2026-01-14 00:40:17,043: t15.2025.03.30 val PER: 0.2253
2026-01-14 00:40:17,043: t15.2025.04.13 val PER: 0.1826
2026-01-14 00:40:17,181: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_81000
2026-01-14 00:40:34,714: Train batch 81200: loss: 0.10 grad norm: 6.49 time: 0.060
2026-01-14 00:40:52,353: Train batch 81400: loss: 0.02 grad norm: 1.92 time: 0.063
2026-01-14 00:41:01,112: Running test after training batch: 81500
2026-01-14 00:41:01,242: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:41:06,097: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:41:06,157: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:41:22,287: Val batch 81500: PER (avg): 0.1008 CTC Loss (avg): 19.2673 WER(5gram): 18.90% (n=256) time: 21.175
2026-01-14 00:41:22,288: WER lens: avg_true_words=5.99 avg_pred_words=6.34 max_pred_words=13
2026-01-14 00:41:22,288: t15.2023.08.13 val PER: 0.0717
2026-01-14 00:41:22,288: t15.2023.08.18 val PER: 0.0662
2026-01-14 00:41:22,288: t15.2023.08.20 val PER: 0.0548
2026-01-14 00:41:22,288: t15.2023.08.25 val PER: 0.0678
2026-01-14 00:41:22,288: t15.2023.08.27 val PER: 0.1415
2026-01-14 00:41:22,289: t15.2023.09.01 val PER: 0.0414
2026-01-14 00:41:22,289: t15.2023.09.03 val PER: 0.1188
2026-01-14 00:41:22,289: t15.2023.09.24 val PER: 0.0850
2026-01-14 00:41:22,289: t15.2023.09.29 val PER: 0.0951
2026-01-14 00:41:22,289: t15.2023.10.01 val PER: 0.1314
2026-01-14 00:41:22,289: t15.2023.10.06 val PER: 0.0420
2026-01-14 00:41:22,292: t15.2023.10.08 val PER: 0.1840
2026-01-14 00:41:22,292: t15.2023.10.13 val PER: 0.1327
2026-01-14 00:41:22,293: t15.2023.10.15 val PER: 0.1035
2026-01-14 00:41:22,293: t15.2023.10.20 val PER: 0.1477
2026-01-14 00:41:22,293: t15.2023.10.22 val PER: 0.0891
2026-01-14 00:41:22,293: t15.2023.11.03 val PER: 0.1404
2026-01-14 00:41:22,293: t15.2023.11.04 val PER: 0.0273
2026-01-14 00:41:22,293: t15.2023.11.17 val PER: 0.0156
2026-01-14 00:41:22,293: t15.2023.11.19 val PER: 0.0140
2026-01-14 00:41:22,293: t15.2023.11.26 val PER: 0.0449
2026-01-14 00:41:22,293: t15.2023.12.03 val PER: 0.0483
2026-01-14 00:41:22,293: t15.2023.12.08 val PER: 0.0333
2026-01-14 00:41:22,293: t15.2023.12.10 val PER: 0.0394
2026-01-14 00:41:22,293: t15.2023.12.17 val PER: 0.0780
2026-01-14 00:41:22,293: t15.2023.12.29 val PER: 0.0679
2026-01-14 00:41:22,294: t15.2024.02.25 val PER: 0.0632
2026-01-14 00:41:22,294: t15.2024.03.08 val PER: 0.1593
2026-01-14 00:41:22,294: t15.2024.03.15 val PER: 0.1426
2026-01-14 00:41:22,294: t15.2024.03.17 val PER: 0.0718
2026-01-14 00:41:22,294: t15.2024.05.10 val PER: 0.1248
2026-01-14 00:41:22,294: t15.2024.06.14 val PER: 0.1215
2026-01-14 00:41:22,294: t15.2024.07.19 val PER: 0.1602
2026-01-14 00:41:22,294: t15.2024.07.21 val PER: 0.0579
2026-01-14 00:41:22,294: t15.2024.07.28 val PER: 0.0971
2026-01-14 00:41:22,294: t15.2025.01.10 val PER: 0.2507
2026-01-14 00:41:22,294: t15.2025.01.12 val PER: 0.0962
2026-01-14 00:41:22,294: t15.2025.03.14 val PER: 0.2973
2026-01-14 00:41:22,294: t15.2025.03.16 val PER: 0.1230
2026-01-14 00:41:22,294: t15.2025.03.30 val PER: 0.2253
2026-01-14 00:41:22,294: t15.2025.04.13 val PER: 0.1826
2026-01-14 00:41:22,434: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_81500
2026-01-14 00:41:31,121: Train batch 81600: loss: 0.40 grad norm: 38.14 time: 0.070
2026-01-14 00:41:48,687: Train batch 81800: loss: 0.10 grad norm: 8.45 time: 0.054
2026-01-14 00:42:06,353: Train batch 82000: loss: 0.05 grad norm: 7.47 time: 0.059
2026-01-14 00:42:06,354: Running test after training batch: 82000
2026-01-14 00:42:06,456: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:42:11,332: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:42:11,394: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-14 00:42:26,178: Val batch 82000: PER (avg): 0.1016 CTC Loss (avg): 19.4065 WER(5gram): 18.90% (n=256) time: 19.825
2026-01-14 00:42:26,179: WER lens: avg_true_words=5.99 avg_pred_words=6.33 max_pred_words=13
2026-01-14 00:42:26,179: t15.2023.08.13 val PER: 0.0686
2026-01-14 00:42:26,179: t15.2023.08.18 val PER: 0.0671
2026-01-14 00:42:26,179: t15.2023.08.20 val PER: 0.0564
2026-01-14 00:42:26,179: t15.2023.08.25 val PER: 0.0663
2026-01-14 00:42:26,179: t15.2023.08.27 val PER: 0.1511
2026-01-14 00:42:26,179: t15.2023.09.01 val PER: 0.0390
2026-01-14 00:42:26,179: t15.2023.09.03 val PER: 0.1176
2026-01-14 00:42:26,179: t15.2023.09.24 val PER: 0.0922
2026-01-14 00:42:26,180: t15.2023.09.29 val PER: 0.0983
2026-01-14 00:42:26,180: t15.2023.10.01 val PER: 0.1314
2026-01-14 00:42:26,180: t15.2023.10.06 val PER: 0.0388
2026-01-14 00:42:26,180: t15.2023.10.08 val PER: 0.1949
2026-01-14 00:42:26,180: t15.2023.10.13 val PER: 0.1404
2026-01-14 00:42:26,180: t15.2023.10.15 val PER: 0.1028
2026-01-14 00:42:26,180: t15.2023.10.20 val PER: 0.1510
2026-01-14 00:42:26,180: t15.2023.10.22 val PER: 0.0880
2026-01-14 00:42:26,181: t15.2023.11.03 val PER: 0.1431
2026-01-14 00:42:26,181: t15.2023.11.04 val PER: 0.0273
2026-01-14 00:42:26,181: t15.2023.11.17 val PER: 0.0156
2026-01-14 00:42:26,181: t15.2023.11.19 val PER: 0.0180
2026-01-14 00:42:26,181: t15.2023.11.26 val PER: 0.0449
2026-01-14 00:42:26,181: t15.2023.12.03 val PER: 0.0515
2026-01-14 00:42:26,181: t15.2023.12.08 val PER: 0.0340
2026-01-14 00:42:26,181: t15.2023.12.10 val PER: 0.0381
2026-01-14 00:42:26,181: t15.2023.12.17 val PER: 0.0759
2026-01-14 00:42:26,181: t15.2023.12.29 val PER: 0.0666
2026-01-14 00:42:26,181: t15.2024.02.25 val PER: 0.0604
2026-01-14 00:42:26,181: t15.2024.03.08 val PER: 0.1593
2026-01-14 00:42:26,181: t15.2024.03.15 val PER: 0.1432
2026-01-14 00:42:26,181: t15.2024.03.17 val PER: 0.0753
2026-01-14 00:42:26,181: t15.2024.05.10 val PER: 0.1233
2026-01-14 00:42:26,182: t15.2024.06.14 val PER: 0.1167
2026-01-14 00:42:26,182: t15.2024.07.19 val PER: 0.1582
2026-01-14 00:42:26,182: t15.2024.07.21 val PER: 0.0607
2026-01-14 00:42:26,182: t15.2024.07.28 val PER: 0.0934
2026-01-14 00:42:26,182: t15.2025.01.10 val PER: 0.2534
2026-01-14 00:42:26,182: t15.2025.01.12 val PER: 0.0947
2026-01-14 00:42:26,182: t15.2025.03.14 val PER: 0.3003
2026-01-14 00:42:26,182: t15.2025.03.16 val PER: 0.1230
2026-01-14 00:42:26,182: t15.2025.03.30 val PER: 0.2264
2026-01-14 00:42:26,182: t15.2025.04.13 val PER: 0.1840
2026-01-14 00:42:26,326: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_82000
2026-01-14 00:42:43,727: Train batch 82200: loss: 0.26 grad norm: 10.61 time: 0.061
2026-01-14 00:43:01,280: Train batch 82400: loss: 0.19 grad norm: 43.11 time: 0.085
2026-01-14 00:43:10,083: Running test after training batch: 82500
2026-01-14 00:43:10,225: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:43:15,077: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:43:15,148: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-14 00:43:31,315: Val batch 82500: PER (avg): 0.1010 CTC Loss (avg): 19.5288 WER(5gram): 18.84% (n=256) time: 21.232
2026-01-14 00:43:31,315: WER lens: avg_true_words=5.99 avg_pred_words=6.32 max_pred_words=14
2026-01-14 00:43:31,316: t15.2023.08.13 val PER: 0.0665
2026-01-14 00:43:31,316: t15.2023.08.18 val PER: 0.0662
2026-01-14 00:43:31,316: t15.2023.08.20 val PER: 0.0580
2026-01-14 00:43:31,316: t15.2023.08.25 val PER: 0.0708
2026-01-14 00:43:31,316: t15.2023.08.27 val PER: 0.1543
2026-01-14 00:43:31,316: t15.2023.09.01 val PER: 0.0390
2026-01-14 00:43:31,316: t15.2023.09.03 val PER: 0.1176
2026-01-14 00:43:31,316: t15.2023.09.24 val PER: 0.0850
2026-01-14 00:43:31,316: t15.2023.09.29 val PER: 0.0989
2026-01-14 00:43:31,316: t15.2023.10.01 val PER: 0.1321
2026-01-14 00:43:31,316: t15.2023.10.06 val PER: 0.0398
2026-01-14 00:43:31,317: t15.2023.10.08 val PER: 0.1908
2026-01-14 00:43:31,317: t15.2023.10.13 val PER: 0.1435
2026-01-14 00:43:31,317: t15.2023.10.15 val PER: 0.1055
2026-01-14 00:43:31,317: t15.2023.10.20 val PER: 0.1477
2026-01-14 00:43:31,317: t15.2023.10.22 val PER: 0.0835
2026-01-14 00:43:31,317: t15.2023.11.03 val PER: 0.1343
2026-01-14 00:43:31,317: t15.2023.11.04 val PER: 0.0273
2026-01-14 00:43:31,317: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:43:31,317: t15.2023.11.19 val PER: 0.0140
2026-01-14 00:43:31,317: t15.2023.11.26 val PER: 0.0435
2026-01-14 00:43:31,317: t15.2023.12.03 val PER: 0.0431
2026-01-14 00:43:31,317: t15.2023.12.08 val PER: 0.0333
2026-01-14 00:43:31,317: t15.2023.12.10 val PER: 0.0381
2026-01-14 00:43:31,317: t15.2023.12.17 val PER: 0.0780
2026-01-14 00:43:31,317: t15.2023.12.29 val PER: 0.0707
2026-01-14 00:43:31,317: t15.2024.02.25 val PER: 0.0576
2026-01-14 00:43:31,318: t15.2024.03.08 val PER: 0.1607
2026-01-14 00:43:31,318: t15.2024.03.15 val PER: 0.1476
2026-01-14 00:43:31,318: t15.2024.03.17 val PER: 0.0732
2026-01-14 00:43:31,318: t15.2024.05.10 val PER: 0.1174
2026-01-14 00:43:31,318: t15.2024.06.14 val PER: 0.1183
2026-01-14 00:43:31,318: t15.2024.07.19 val PER: 0.1510
2026-01-14 00:43:31,318: t15.2024.07.21 val PER: 0.0600
2026-01-14 00:43:31,318: t15.2024.07.28 val PER: 0.0941
2026-01-14 00:43:31,318: t15.2025.01.10 val PER: 0.2590
2026-01-14 00:43:31,318: t15.2025.01.12 val PER: 0.0924
2026-01-14 00:43:31,318: t15.2025.03.14 val PER: 0.3033
2026-01-14 00:43:31,319: t15.2025.03.16 val PER: 0.1243
2026-01-14 00:43:31,319: t15.2025.03.30 val PER: 0.2230
2026-01-14 00:43:31,319: t15.2025.04.13 val PER: 0.1883
2026-01-14 00:43:31,456: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_82500
2026-01-14 00:43:40,191: Train batch 82600: loss: 0.10 grad norm: 12.22 time: 0.068
2026-01-14 00:43:57,977: Train batch 82800: loss: 0.18 grad norm: 18.54 time: 0.062
2026-01-14 00:44:15,783: Train batch 83000: loss: 0.06 grad norm: 10.72 time: 0.047
2026-01-14 00:44:15,783: Running test after training batch: 83000
2026-01-14 00:44:15,937: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:44:20,907: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:44:20,983: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-14 00:44:37,316: Val batch 83000: PER (avg): 0.1014 CTC Loss (avg): 19.4804 WER(5gram): 19.17% (n=256) time: 21.532
2026-01-14 00:44:37,316: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=14
2026-01-14 00:44:37,316: t15.2023.08.13 val PER: 0.0686
2026-01-14 00:44:37,316: t15.2023.08.18 val PER: 0.0662
2026-01-14 00:44:37,316: t15.2023.08.20 val PER: 0.0580
2026-01-14 00:44:37,317: t15.2023.08.25 val PER: 0.0648
2026-01-14 00:44:37,317: t15.2023.08.27 val PER: 0.1495
2026-01-14 00:44:37,317: t15.2023.09.01 val PER: 0.0398
2026-01-14 00:44:37,317: t15.2023.09.03 val PER: 0.1116
2026-01-14 00:44:37,317: t15.2023.09.24 val PER: 0.0886
2026-01-14 00:44:37,317: t15.2023.09.29 val PER: 0.0983
2026-01-14 00:44:37,317: t15.2023.10.01 val PER: 0.1275
2026-01-14 00:44:37,317: t15.2023.10.06 val PER: 0.0431
2026-01-14 00:44:37,317: t15.2023.10.08 val PER: 0.1881
2026-01-14 00:44:37,317: t15.2023.10.13 val PER: 0.1412
2026-01-14 00:44:37,317: t15.2023.10.15 val PER: 0.1061
2026-01-14 00:44:37,318: t15.2023.10.20 val PER: 0.1510
2026-01-14 00:44:37,318: t15.2023.10.22 val PER: 0.0802
2026-01-14 00:44:37,318: t15.2023.11.03 val PER: 0.1404
2026-01-14 00:44:37,318: t15.2023.11.04 val PER: 0.0239
2026-01-14 00:44:37,318: t15.2023.11.17 val PER: 0.0156
2026-01-14 00:44:37,318: t15.2023.11.19 val PER: 0.0160
2026-01-14 00:44:37,318: t15.2023.11.26 val PER: 0.0435
2026-01-14 00:44:37,318: t15.2023.12.03 val PER: 0.0431
2026-01-14 00:44:37,318: t15.2023.12.08 val PER: 0.0346
2026-01-14 00:44:37,318: t15.2023.12.10 val PER: 0.0342
2026-01-14 00:44:37,318: t15.2023.12.17 val PER: 0.0811
2026-01-14 00:44:37,318: t15.2023.12.29 val PER: 0.0666
2026-01-14 00:44:37,318: t15.2024.02.25 val PER: 0.0604
2026-01-14 00:44:37,318: t15.2024.03.08 val PER: 0.1622
2026-01-14 00:44:37,318: t15.2024.03.15 val PER: 0.1507
2026-01-14 00:44:37,319: t15.2024.03.17 val PER: 0.0690
2026-01-14 00:44:37,319: t15.2024.05.10 val PER: 0.1204
2026-01-14 00:44:37,319: t15.2024.06.14 val PER: 0.1230
2026-01-14 00:44:37,319: t15.2024.07.19 val PER: 0.1569
2026-01-14 00:44:37,319: t15.2024.07.21 val PER: 0.0621
2026-01-14 00:44:37,319: t15.2024.07.28 val PER: 0.0934
2026-01-14 00:44:37,319: t15.2025.01.10 val PER: 0.2534
2026-01-14 00:44:37,319: t15.2025.01.12 val PER: 0.0947
2026-01-14 00:44:37,319: t15.2025.03.14 val PER: 0.3151
2026-01-14 00:44:37,319: t15.2025.03.16 val PER: 0.1335
2026-01-14 00:44:37,319: t15.2025.03.30 val PER: 0.2207
2026-01-14 00:44:37,319: t15.2025.04.13 val PER: 0.1854
2026-01-14 00:44:37,472: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_83000
2026-01-14 00:44:54,843: Train batch 83200: loss: 0.32 grad norm: 32.43 time: 0.066
2026-01-14 00:45:12,261: Train batch 83400: loss: 0.01 grad norm: 1.32 time: 0.043
2026-01-14 00:45:21,053: Running test after training batch: 83500
2026-01-14 00:45:21,166: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:45:26,091: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:45:26,154: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:45:42,225: Val batch 83500: PER (avg): 0.1013 CTC Loss (avg): 19.5375 WER(5gram): 19.62% (n=256) time: 21.172
2026-01-14 00:45:42,226: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=14
2026-01-14 00:45:42,226: t15.2023.08.13 val PER: 0.0655
2026-01-14 00:45:42,226: t15.2023.08.18 val PER: 0.0654
2026-01-14 00:45:42,226: t15.2023.08.20 val PER: 0.0556
2026-01-14 00:45:42,226: t15.2023.08.25 val PER: 0.0678
2026-01-14 00:45:42,226: t15.2023.08.27 val PER: 0.1415
2026-01-14 00:45:42,226: t15.2023.09.01 val PER: 0.0406
2026-01-14 00:45:42,227: t15.2023.09.03 val PER: 0.1069
2026-01-14 00:45:42,227: t15.2023.09.24 val PER: 0.0910
2026-01-14 00:45:42,227: t15.2023.09.29 val PER: 0.0932
2026-01-14 00:45:42,227: t15.2023.10.01 val PER: 0.1301
2026-01-14 00:45:42,227: t15.2023.10.06 val PER: 0.0431
2026-01-14 00:45:42,227: t15.2023.10.08 val PER: 0.1854
2026-01-14 00:45:42,227: t15.2023.10.13 val PER: 0.1412
2026-01-14 00:45:42,227: t15.2023.10.15 val PER: 0.1015
2026-01-14 00:45:42,227: t15.2023.10.20 val PER: 0.1443
2026-01-14 00:45:42,227: t15.2023.10.22 val PER: 0.0891
2026-01-14 00:45:42,227: t15.2023.11.03 val PER: 0.1425
2026-01-14 00:45:42,227: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:45:42,227: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:45:42,227: t15.2023.11.19 val PER: 0.0180
2026-01-14 00:45:42,227: t15.2023.11.26 val PER: 0.0464
2026-01-14 00:45:42,228: t15.2023.12.03 val PER: 0.0452
2026-01-14 00:45:42,228: t15.2023.12.08 val PER: 0.0333
2026-01-14 00:45:42,228: t15.2023.12.10 val PER: 0.0355
2026-01-14 00:45:42,228: t15.2023.12.17 val PER: 0.0759
2026-01-14 00:45:42,228: t15.2023.12.29 val PER: 0.0721
2026-01-14 00:45:42,228: t15.2024.02.25 val PER: 0.0632
2026-01-14 00:45:42,228: t15.2024.03.08 val PER: 0.1622
2026-01-14 00:45:42,228: t15.2024.03.15 val PER: 0.1495
2026-01-14 00:45:42,228: t15.2024.03.17 val PER: 0.0725
2026-01-14 00:45:42,228: t15.2024.05.10 val PER: 0.1159
2026-01-14 00:45:42,232: t15.2024.06.14 val PER: 0.1183
2026-01-14 00:45:42,232: t15.2024.07.19 val PER: 0.1602
2026-01-14 00:45:42,232: t15.2024.07.21 val PER: 0.0600
2026-01-14 00:45:42,232: t15.2024.07.28 val PER: 0.0934
2026-01-14 00:45:42,233: t15.2025.01.10 val PER: 0.2479
2026-01-14 00:45:42,233: t15.2025.01.12 val PER: 0.0970
2026-01-14 00:45:42,233: t15.2025.03.14 val PER: 0.3047
2026-01-14 00:45:42,233: t15.2025.03.16 val PER: 0.1322
2026-01-14 00:45:42,233: t15.2025.03.30 val PER: 0.2276
2026-01-14 00:45:42,233: t15.2025.04.13 val PER: 0.1897
2026-01-14 00:45:42,384: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_83500
2026-01-14 00:45:51,156: Train batch 83600: loss: 0.22 grad norm: 11.63 time: 0.053
2026-01-14 00:46:08,973: Train batch 83800: loss: 0.11 grad norm: 7.65 time: 0.060
2026-01-14 00:46:26,682: Train batch 84000: loss: 0.30 grad norm: 23.51 time: 0.060
2026-01-14 00:46:26,682: Running test after training batch: 84000
2026-01-14 00:46:26,914: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:46:31,804: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:46:31,881: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:46:48,006: Val batch 84000: PER (avg): 0.1013 CTC Loss (avg): 19.6676 WER(5gram): 18.25% (n=256) time: 21.324
2026-01-14 00:46:48,007: WER lens: avg_true_words=5.99 avg_pred_words=6.33 max_pred_words=13
2026-01-14 00:46:48,007: t15.2023.08.13 val PER: 0.0696
2026-01-14 00:46:48,007: t15.2023.08.18 val PER: 0.0671
2026-01-14 00:46:48,007: t15.2023.08.20 val PER: 0.0572
2026-01-14 00:46:48,007: t15.2023.08.25 val PER: 0.0693
2026-01-14 00:46:48,007: t15.2023.08.27 val PER: 0.1399
2026-01-14 00:46:48,007: t15.2023.09.01 val PER: 0.0406
2026-01-14 00:46:48,007: t15.2023.09.03 val PER: 0.1152
2026-01-14 00:46:48,008: t15.2023.09.24 val PER: 0.0898
2026-01-14 00:46:48,008: t15.2023.09.29 val PER: 0.0957
2026-01-14 00:46:48,008: t15.2023.10.01 val PER: 0.1281
2026-01-14 00:46:48,008: t15.2023.10.06 val PER: 0.0420
2026-01-14 00:46:48,008: t15.2023.10.08 val PER: 0.1827
2026-01-14 00:46:48,008: t15.2023.10.13 val PER: 0.1427
2026-01-14 00:46:48,008: t15.2023.10.15 val PER: 0.1048
2026-01-14 00:46:48,008: t15.2023.10.20 val PER: 0.1376
2026-01-14 00:46:48,008: t15.2023.10.22 val PER: 0.0824
2026-01-14 00:46:48,008: t15.2023.11.03 val PER: 0.1398
2026-01-14 00:46:48,008: t15.2023.11.04 val PER: 0.0239
2026-01-14 00:46:48,008: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:46:48,009: t15.2023.11.19 val PER: 0.0160
2026-01-14 00:46:48,009: t15.2023.11.26 val PER: 0.0435
2026-01-14 00:46:48,009: t15.2023.12.03 val PER: 0.0483
2026-01-14 00:46:48,009: t15.2023.12.08 val PER: 0.0320
2026-01-14 00:46:48,009: t15.2023.12.10 val PER: 0.0342
2026-01-14 00:46:48,009: t15.2023.12.17 val PER: 0.0821
2026-01-14 00:46:48,009: t15.2023.12.29 val PER: 0.0728
2026-01-14 00:46:48,009: t15.2024.02.25 val PER: 0.0618
2026-01-14 00:46:48,009: t15.2024.03.08 val PER: 0.1579
2026-01-14 00:46:48,009: t15.2024.03.15 val PER: 0.1501
2026-01-14 00:46:48,009: t15.2024.03.17 val PER: 0.0732
2026-01-14 00:46:48,009: t15.2024.05.10 val PER: 0.1129
2026-01-14 00:46:48,009: t15.2024.06.14 val PER: 0.1136
2026-01-14 00:46:48,009: t15.2024.07.19 val PER: 0.1602
2026-01-14 00:46:48,009: t15.2024.07.21 val PER: 0.0600
2026-01-14 00:46:48,009: t15.2024.07.28 val PER: 0.0919
2026-01-14 00:46:48,010: t15.2025.01.10 val PER: 0.2548
2026-01-14 00:46:48,010: t15.2025.01.12 val PER: 0.0931
2026-01-14 00:46:48,010: t15.2025.03.14 val PER: 0.3121
2026-01-14 00:46:48,010: t15.2025.03.16 val PER: 0.1309
2026-01-14 00:46:48,010: t15.2025.03.30 val PER: 0.2276
2026-01-14 00:46:48,010: t15.2025.04.13 val PER: 0.1840
2026-01-14 00:46:48,168: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_84000
2026-01-14 00:47:05,647: Train batch 84200: loss: 0.08 grad norm: 8.17 time: 0.071
2026-01-14 00:47:23,512: Train batch 84400: loss: 0.12 grad norm: 10.17 time: 0.065
2026-01-14 00:47:32,353: Running test after training batch: 84500
2026-01-14 00:47:32,724: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:47:37,810: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:47:37,880: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:47:52,491: Val batch 84500: PER (avg): 0.1006 CTC Loss (avg): 19.4652 WER(5gram): 18.97% (n=256) time: 20.137
2026-01-14 00:47:52,491: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=13
2026-01-14 00:47:52,491: t15.2023.08.13 val PER: 0.0676
2026-01-14 00:47:52,492: t15.2023.08.18 val PER: 0.0671
2026-01-14 00:47:52,492: t15.2023.08.20 val PER: 0.0572
2026-01-14 00:47:52,492: t15.2023.08.25 val PER: 0.0708
2026-01-14 00:47:52,492: t15.2023.08.27 val PER: 0.1447
2026-01-14 00:47:52,492: t15.2023.09.01 val PER: 0.0422
2026-01-14 00:47:52,492: t15.2023.09.03 val PER: 0.1176
2026-01-14 00:47:52,492: t15.2023.09.24 val PER: 0.0801
2026-01-14 00:47:52,492: t15.2023.09.29 val PER: 0.0976
2026-01-14 00:47:52,492: t15.2023.10.01 val PER: 0.1255
2026-01-14 00:47:52,493: t15.2023.10.06 val PER: 0.0452
2026-01-14 00:47:52,493: t15.2023.10.08 val PER: 0.1867
2026-01-14 00:47:52,493: t15.2023.10.13 val PER: 0.1404
2026-01-14 00:47:52,493: t15.2023.10.15 val PER: 0.1015
2026-01-14 00:47:52,493: t15.2023.10.20 val PER: 0.1409
2026-01-14 00:47:52,493: t15.2023.10.22 val PER: 0.0835
2026-01-14 00:47:52,493: t15.2023.11.03 val PER: 0.1418
2026-01-14 00:47:52,493: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:47:52,493: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:47:52,493: t15.2023.11.19 val PER: 0.0160
2026-01-14 00:47:52,494: t15.2023.11.26 val PER: 0.0413
2026-01-14 00:47:52,494: t15.2023.12.03 val PER: 0.0452
2026-01-14 00:47:52,494: t15.2023.12.08 val PER: 0.0333
2026-01-14 00:47:52,494: t15.2023.12.10 val PER: 0.0355
2026-01-14 00:47:52,494: t15.2023.12.17 val PER: 0.0800
2026-01-14 00:47:52,494: t15.2023.12.29 val PER: 0.0755
2026-01-14 00:47:52,494: t15.2024.02.25 val PER: 0.0632
2026-01-14 00:47:52,494: t15.2024.03.08 val PER: 0.1536
2026-01-14 00:47:52,494: t15.2024.03.15 val PER: 0.1520
2026-01-14 00:47:52,494: t15.2024.03.17 val PER: 0.0697
2026-01-14 00:47:52,494: t15.2024.05.10 val PER: 0.1159
2026-01-14 00:47:52,494: t15.2024.06.14 val PER: 0.1120
2026-01-14 00:47:52,495: t15.2024.07.19 val PER: 0.1569
2026-01-14 00:47:52,495: t15.2024.07.21 val PER: 0.0579
2026-01-14 00:47:52,495: t15.2024.07.28 val PER: 0.0926
2026-01-14 00:47:52,495: t15.2025.01.10 val PER: 0.2521
2026-01-14 00:47:52,495: t15.2025.01.12 val PER: 0.0908
2026-01-14 00:47:52,495: t15.2025.03.14 val PER: 0.3062
2026-01-14 00:47:52,495: t15.2025.03.16 val PER: 0.1230
2026-01-14 00:47:52,495: t15.2025.03.30 val PER: 0.2253
2026-01-14 00:47:52,495: t15.2025.04.13 val PER: 0.1826
2026-01-14 00:47:52,651: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_84500
2026-01-14 00:48:01,209: Train batch 84600: loss: 0.10 grad norm: 7.98 time: 0.061
2026-01-14 00:48:18,709: Train batch 84800: loss: 0.25 grad norm: 11.65 time: 0.074
2026-01-14 00:48:35,976: Train batch 85000: loss: 0.01 grad norm: 1.35 time: 0.051
2026-01-14 00:48:35,976: Running test after training batch: 85000
2026-01-14 00:48:36,118: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:48:41,153: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:48:41,221: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-14 00:48:55,988: Val batch 85000: PER (avg): 0.1008 CTC Loss (avg): 19.5964 WER(5gram): 18.32% (n=256) time: 20.011
2026-01-14 00:48:55,988: WER lens: avg_true_words=5.99 avg_pred_words=6.31 max_pred_words=13
2026-01-14 00:48:55,989: t15.2023.08.13 val PER: 0.0686
2026-01-14 00:48:55,989: t15.2023.08.18 val PER: 0.0654
2026-01-14 00:48:55,989: t15.2023.08.20 val PER: 0.0564
2026-01-14 00:48:55,989: t15.2023.08.25 val PER: 0.0723
2026-01-14 00:48:55,989: t15.2023.08.27 val PER: 0.1415
2026-01-14 00:48:55,989: t15.2023.09.01 val PER: 0.0430
2026-01-14 00:48:55,989: t15.2023.09.03 val PER: 0.1128
2026-01-14 00:48:55,989: t15.2023.09.24 val PER: 0.0850
2026-01-14 00:48:55,989: t15.2023.09.29 val PER: 0.0970
2026-01-14 00:48:55,989: t15.2023.10.01 val PER: 0.1262
2026-01-14 00:48:55,989: t15.2023.10.06 val PER: 0.0420
2026-01-14 00:48:55,990: t15.2023.10.08 val PER: 0.1922
2026-01-14 00:48:55,990: t15.2023.10.13 val PER: 0.1365
2026-01-14 00:48:55,990: t15.2023.10.15 val PER: 0.1048
2026-01-14 00:48:55,990: t15.2023.10.20 val PER: 0.1510
2026-01-14 00:48:55,990: t15.2023.10.22 val PER: 0.0824
2026-01-14 00:48:55,990: t15.2023.11.03 val PER: 0.1336
2026-01-14 00:48:55,990: t15.2023.11.04 val PER: 0.0239
2026-01-14 00:48:55,990: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:48:55,990: t15.2023.11.19 val PER: 0.0200
2026-01-14 00:48:55,990: t15.2023.11.26 val PER: 0.0449
2026-01-14 00:48:55,990: t15.2023.12.03 val PER: 0.0473
2026-01-14 00:48:55,990: t15.2023.12.08 val PER: 0.0306
2026-01-14 00:48:55,990: t15.2023.12.10 val PER: 0.0342
2026-01-14 00:48:55,990: t15.2023.12.17 val PER: 0.0863
2026-01-14 00:48:55,990: t15.2023.12.29 val PER: 0.0741
2026-01-14 00:48:55,991: t15.2024.02.25 val PER: 0.0562
2026-01-14 00:48:55,991: t15.2024.03.08 val PER: 0.1622
2026-01-14 00:48:55,991: t15.2024.03.15 val PER: 0.1520
2026-01-14 00:48:55,991: t15.2024.03.17 val PER: 0.0725
2026-01-14 00:48:55,991: t15.2024.05.10 val PER: 0.1144
2026-01-14 00:48:55,991: t15.2024.06.14 val PER: 0.1167
2026-01-14 00:48:55,991: t15.2024.07.19 val PER: 0.1523
2026-01-14 00:48:55,991: t15.2024.07.21 val PER: 0.0579
2026-01-14 00:48:55,991: t15.2024.07.28 val PER: 0.0971
2026-01-14 00:48:55,991: t15.2025.01.10 val PER: 0.2534
2026-01-14 00:48:55,991: t15.2025.01.12 val PER: 0.0970
2026-01-14 00:48:55,991: t15.2025.03.14 val PER: 0.3018
2026-01-14 00:48:55,991: t15.2025.03.16 val PER: 0.1257
2026-01-14 00:48:55,991: t15.2025.03.30 val PER: 0.2218
2026-01-14 00:48:55,991: t15.2025.04.13 val PER: 0.1812
2026-01-14 00:48:56,146: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_85000
2026-01-14 00:49:13,476: Train batch 85200: loss: 0.07 grad norm: 6.19 time: 0.052
2026-01-14 00:49:30,937: Train batch 85400: loss: 0.23 grad norm: 13.74 time: 0.052
2026-01-14 00:49:39,719: Running test after training batch: 85500
2026-01-14 00:49:39,868: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:49:44,707: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:49:44,766: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-14 00:49:59,400: Val batch 85500: PER (avg): 0.1007 CTC Loss (avg): 19.7536 WER(5gram): 18.84% (n=256) time: 19.681
2026-01-14 00:49:59,401: WER lens: avg_true_words=5.99 avg_pred_words=6.34 max_pred_words=13
2026-01-14 00:49:59,401: t15.2023.08.13 val PER: 0.0696
2026-01-14 00:49:59,401: t15.2023.08.18 val PER: 0.0662
2026-01-14 00:49:59,401: t15.2023.08.20 val PER: 0.0580
2026-01-14 00:49:59,401: t15.2023.08.25 val PER: 0.0738
2026-01-14 00:49:59,401: t15.2023.08.27 val PER: 0.1431
2026-01-14 00:49:59,402: t15.2023.09.01 val PER: 0.0422
2026-01-14 00:49:59,402: t15.2023.09.03 val PER: 0.1140
2026-01-14 00:49:59,402: t15.2023.09.24 val PER: 0.0850
2026-01-14 00:49:59,402: t15.2023.09.29 val PER: 0.0970
2026-01-14 00:49:59,402: t15.2023.10.01 val PER: 0.1268
2026-01-14 00:49:59,402: t15.2023.10.06 val PER: 0.0420
2026-01-14 00:49:59,402: t15.2023.10.08 val PER: 0.1989
2026-01-14 00:49:59,402: t15.2023.10.13 val PER: 0.1358
2026-01-14 00:49:59,402: t15.2023.10.15 val PER: 0.1015
2026-01-14 00:49:59,403: t15.2023.10.20 val PER: 0.1477
2026-01-14 00:49:59,403: t15.2023.10.22 val PER: 0.0813
2026-01-14 00:49:59,403: t15.2023.11.03 val PER: 0.1377
2026-01-14 00:49:59,403: t15.2023.11.04 val PER: 0.0239
2026-01-14 00:49:59,403: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:49:59,403: t15.2023.11.19 val PER: 0.0200
2026-01-14 00:49:59,403: t15.2023.11.26 val PER: 0.0478
2026-01-14 00:49:59,403: t15.2023.12.03 val PER: 0.0441
2026-01-14 00:49:59,403: t15.2023.12.08 val PER: 0.0313
2026-01-14 00:49:59,403: t15.2023.12.10 val PER: 0.0355
2026-01-14 00:49:59,404: t15.2023.12.17 val PER: 0.0842
2026-01-14 00:49:59,404: t15.2023.12.29 val PER: 0.0714
2026-01-14 00:49:59,404: t15.2024.02.25 val PER: 0.0576
2026-01-14 00:49:59,404: t15.2024.03.08 val PER: 0.1593
2026-01-14 00:49:59,404: t15.2024.03.15 val PER: 0.1538
2026-01-14 00:49:59,404: t15.2024.03.17 val PER: 0.0676
2026-01-14 00:49:59,404: t15.2024.05.10 val PER: 0.1144
2026-01-14 00:49:59,404: t15.2024.06.14 val PER: 0.1120
2026-01-14 00:49:59,404: t15.2024.07.19 val PER: 0.1595
2026-01-14 00:49:59,404: t15.2024.07.21 val PER: 0.0572
2026-01-14 00:49:59,404: t15.2024.07.28 val PER: 0.0949
2026-01-14 00:49:59,404: t15.2025.01.10 val PER: 0.2521
2026-01-14 00:49:59,404: t15.2025.01.12 val PER: 0.0916
2026-01-14 00:49:59,405: t15.2025.03.14 val PER: 0.3003
2026-01-14 00:49:59,407: t15.2025.03.16 val PER: 0.1243
2026-01-14 00:49:59,407: t15.2025.03.30 val PER: 0.2253
2026-01-14 00:49:59,407: t15.2025.04.13 val PER: 0.1812
2026-01-14 00:49:59,555: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_85500
2026-01-14 00:50:08,151: Train batch 85600: loss: 0.24 grad norm: 23.08 time: 0.059
2026-01-14 00:50:25,706: Train batch 85800: loss: 0.43 grad norm: 19.17 time: 0.060
2026-01-14 00:50:43,407: Train batch 86000: loss: 0.25 grad norm: 14.58 time: 0.056
2026-01-14 00:50:43,407: Running test after training batch: 86000
2026-01-14 00:50:43,509: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:50:48,394: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:50:48,454: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:51:03,357: Val batch 86000: PER (avg): 0.1004 CTC Loss (avg): 19.5399 WER(5gram): 18.97% (n=256) time: 19.950
2026-01-14 00:51:03,358: WER lens: avg_true_words=5.99 avg_pred_words=6.34 max_pred_words=13
2026-01-14 00:51:03,358: t15.2023.08.13 val PER: 0.0665
2026-01-14 00:51:03,358: t15.2023.08.18 val PER: 0.0671
2026-01-14 00:51:03,358: t15.2023.08.20 val PER: 0.0572
2026-01-14 00:51:03,359: t15.2023.08.25 val PER: 0.0708
2026-01-14 00:51:03,359: t15.2023.08.27 val PER: 0.1431
2026-01-14 00:51:03,359: t15.2023.09.01 val PER: 0.0398
2026-01-14 00:51:03,359: t15.2023.09.03 val PER: 0.1152
2026-01-14 00:51:03,359: t15.2023.09.24 val PER: 0.0837
2026-01-14 00:51:03,359: t15.2023.09.29 val PER: 0.0976
2026-01-14 00:51:03,359: t15.2023.10.01 val PER: 0.1268
2026-01-14 00:51:03,359: t15.2023.10.06 val PER: 0.0441
2026-01-14 00:51:03,359: t15.2023.10.08 val PER: 0.1976
2026-01-14 00:51:03,360: t15.2023.10.13 val PER: 0.1365
2026-01-14 00:51:03,360: t15.2023.10.15 val PER: 0.1048
2026-01-14 00:51:03,360: t15.2023.10.20 val PER: 0.1544
2026-01-14 00:51:03,360: t15.2023.10.22 val PER: 0.0791
2026-01-14 00:51:03,360: t15.2023.11.03 val PER: 0.1391
2026-01-14 00:51:03,360: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:51:03,360: t15.2023.11.17 val PER: 0.0156
2026-01-14 00:51:03,360: t15.2023.11.19 val PER: 0.0180
2026-01-14 00:51:03,360: t15.2023.11.26 val PER: 0.0471
2026-01-14 00:51:03,360: t15.2023.12.03 val PER: 0.0420
2026-01-14 00:51:03,360: t15.2023.12.08 val PER: 0.0313
2026-01-14 00:51:03,361: t15.2023.12.10 val PER: 0.0329
2026-01-14 00:51:03,361: t15.2023.12.17 val PER: 0.0884
2026-01-14 00:51:03,361: t15.2023.12.29 val PER: 0.0728
2026-01-14 00:51:03,361: t15.2024.02.25 val PER: 0.0604
2026-01-14 00:51:03,361: t15.2024.03.08 val PER: 0.1593
2026-01-14 00:51:03,361: t15.2024.03.15 val PER: 0.1470
2026-01-14 00:51:03,361: t15.2024.03.17 val PER: 0.0711
2026-01-14 00:51:03,361: t15.2024.05.10 val PER: 0.1189
2026-01-14 00:51:03,361: t15.2024.06.14 val PER: 0.1246
2026-01-14 00:51:03,361: t15.2024.07.19 val PER: 0.1496
2026-01-14 00:51:03,361: t15.2024.07.21 val PER: 0.0559
2026-01-14 00:51:03,361: t15.2024.07.28 val PER: 0.0934
2026-01-14 00:51:03,361: t15.2025.01.10 val PER: 0.2410
2026-01-14 00:51:03,361: t15.2025.01.12 val PER: 0.0970
2026-01-14 00:51:03,361: t15.2025.03.14 val PER: 0.2929
2026-01-14 00:51:03,361: t15.2025.03.16 val PER: 0.1257
2026-01-14 00:51:03,362: t15.2025.03.30 val PER: 0.2276
2026-01-14 00:51:03,362: t15.2025.04.13 val PER: 0.1812
2026-01-14 00:51:03,516: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_86000
2026-01-14 00:51:21,032: Train batch 86200: loss: 0.08 grad norm: 8.58 time: 0.070
2026-01-14 00:51:38,787: Train batch 86400: loss: 0.04 grad norm: 3.67 time: 0.065
2026-01-14 00:51:47,600: Running test after training batch: 86500
2026-01-14 00:51:47,787: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:51:52,710: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:51:52,770: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:52:07,199: Val batch 86500: PER (avg): 0.0997 CTC Loss (avg): 19.5306 WER(5gram): 19.30% (n=256) time: 19.599
2026-01-14 00:52:07,200: WER lens: avg_true_words=5.99 avg_pred_words=6.34 max_pred_words=13
2026-01-14 00:52:07,200: t15.2023.08.13 val PER: 0.0655
2026-01-14 00:52:07,200: t15.2023.08.18 val PER: 0.0645
2026-01-14 00:52:07,200: t15.2023.08.20 val PER: 0.0564
2026-01-14 00:52:07,200: t15.2023.08.25 val PER: 0.0663
2026-01-14 00:52:07,200: t15.2023.08.27 val PER: 0.1415
2026-01-14 00:52:07,200: t15.2023.09.01 val PER: 0.0406
2026-01-14 00:52:07,201: t15.2023.09.03 val PER: 0.1140
2026-01-14 00:52:07,201: t15.2023.09.24 val PER: 0.0862
2026-01-14 00:52:07,201: t15.2023.09.29 val PER: 0.0976
2026-01-14 00:52:07,201: t15.2023.10.01 val PER: 0.1308
2026-01-14 00:52:07,201: t15.2023.10.06 val PER: 0.0452
2026-01-14 00:52:07,201: t15.2023.10.08 val PER: 0.1935
2026-01-14 00:52:07,201: t15.2023.10.13 val PER: 0.1389
2026-01-14 00:52:07,201: t15.2023.10.15 val PER: 0.1028
2026-01-14 00:52:07,202: t15.2023.10.20 val PER: 0.1577
2026-01-14 00:52:07,202: t15.2023.10.22 val PER: 0.0824
2026-01-14 00:52:07,202: t15.2023.11.03 val PER: 0.1384
2026-01-14 00:52:07,202: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:52:07,202: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:52:07,202: t15.2023.11.19 val PER: 0.0180
2026-01-14 00:52:07,202: t15.2023.11.26 val PER: 0.0428
2026-01-14 00:52:07,202: t15.2023.12.03 val PER: 0.0410
2026-01-14 00:52:07,202: t15.2023.12.08 val PER: 0.0320
2026-01-14 00:52:07,202: t15.2023.12.10 val PER: 0.0329
2026-01-14 00:52:07,202: t15.2023.12.17 val PER: 0.0852
2026-01-14 00:52:07,203: t15.2023.12.29 val PER: 0.0728
2026-01-14 00:52:07,203: t15.2024.02.25 val PER: 0.0646
2026-01-14 00:52:07,203: t15.2024.03.08 val PER: 0.1494
2026-01-14 00:52:07,203: t15.2024.03.15 val PER: 0.1470
2026-01-14 00:52:07,203: t15.2024.03.17 val PER: 0.0656
2026-01-14 00:52:07,203: t15.2024.05.10 val PER: 0.1204
2026-01-14 00:52:07,203: t15.2024.06.14 val PER: 0.1167
2026-01-14 00:52:07,203: t15.2024.07.19 val PER: 0.1523
2026-01-14 00:52:07,203: t15.2024.07.21 val PER: 0.0552
2026-01-14 00:52:07,203: t15.2024.07.28 val PER: 0.0904
2026-01-14 00:52:07,203: t15.2025.01.10 val PER: 0.2479
2026-01-14 00:52:07,204: t15.2025.01.12 val PER: 0.0939
2026-01-14 00:52:07,204: t15.2025.03.14 val PER: 0.2973
2026-01-14 00:52:07,204: t15.2025.03.16 val PER: 0.1217
2026-01-14 00:52:07,204: t15.2025.03.30 val PER: 0.2207
2026-01-14 00:52:07,204: t15.2025.04.13 val PER: 0.1854
2026-01-14 00:52:07,359: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_86500
2026-01-14 00:52:16,114: Train batch 86600: loss: 0.12 grad norm: 16.64 time: 0.070
2026-01-14 00:52:33,620: Train batch 86800: loss: 0.11 grad norm: 11.87 time: 0.061
2026-01-14 00:52:51,112: Train batch 87000: loss: 0.11 grad norm: 10.55 time: 0.061
2026-01-14 00:52:51,112: Running test after training batch: 87000
2026-01-14 00:52:51,215: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:52:56,332: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:52:56,398: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:53:12,504: Val batch 87000: PER (avg): 0.0993 CTC Loss (avg): 19.4873 WER(5gram): 18.71% (n=256) time: 21.392
2026-01-14 00:53:12,505: WER lens: avg_true_words=5.99 avg_pred_words=6.34 max_pred_words=13
2026-01-14 00:53:12,505: t15.2023.08.13 val PER: 0.0655
2026-01-14 00:53:12,505: t15.2023.08.18 val PER: 0.0620
2026-01-14 00:53:12,505: t15.2023.08.20 val PER: 0.0580
2026-01-14 00:53:12,506: t15.2023.08.25 val PER: 0.0678
2026-01-14 00:53:12,506: t15.2023.08.27 val PER: 0.1399
2026-01-14 00:53:12,506: t15.2023.09.01 val PER: 0.0414
2026-01-14 00:53:12,506: t15.2023.09.03 val PER: 0.1105
2026-01-14 00:53:12,506: t15.2023.09.24 val PER: 0.0874
2026-01-14 00:53:12,506: t15.2023.09.29 val PER: 0.0938
2026-01-14 00:53:12,506: t15.2023.10.01 val PER: 0.1295
2026-01-14 00:53:12,506: t15.2023.10.06 val PER: 0.0441
2026-01-14 00:53:12,506: t15.2023.10.08 val PER: 0.1949
2026-01-14 00:53:12,506: t15.2023.10.13 val PER: 0.1381
2026-01-14 00:53:12,506: t15.2023.10.15 val PER: 0.1002
2026-01-14 00:53:12,506: t15.2023.10.20 val PER: 0.1577
2026-01-14 00:53:12,506: t15.2023.10.22 val PER: 0.0846
2026-01-14 00:53:12,507: t15.2023.11.03 val PER: 0.1425
2026-01-14 00:53:12,507: t15.2023.11.04 val PER: 0.0239
2026-01-14 00:53:12,507: t15.2023.11.17 val PER: 0.0187
2026-01-14 00:53:12,507: t15.2023.11.19 val PER: 0.0180
2026-01-14 00:53:12,507: t15.2023.11.26 val PER: 0.0428
2026-01-14 00:53:12,507: t15.2023.12.03 val PER: 0.0441
2026-01-14 00:53:12,507: t15.2023.12.08 val PER: 0.0326
2026-01-14 00:53:12,507: t15.2023.12.10 val PER: 0.0276
2026-01-14 00:53:12,507: t15.2023.12.17 val PER: 0.0780
2026-01-14 00:53:12,507: t15.2023.12.29 val PER: 0.0686
2026-01-14 00:53:12,508: t15.2024.02.25 val PER: 0.0632
2026-01-14 00:53:12,508: t15.2024.03.08 val PER: 0.1508
2026-01-14 00:53:12,508: t15.2024.03.15 val PER: 0.1476
2026-01-14 00:53:12,508: t15.2024.03.17 val PER: 0.0676
2026-01-14 00:53:12,508: t15.2024.05.10 val PER: 0.1218
2026-01-14 00:53:12,508: t15.2024.06.14 val PER: 0.1167
2026-01-14 00:53:12,508: t15.2024.07.19 val PER: 0.1483
2026-01-14 00:53:12,508: t15.2024.07.21 val PER: 0.0600
2026-01-14 00:53:12,508: t15.2024.07.28 val PER: 0.0926
2026-01-14 00:53:12,508: t15.2025.01.10 val PER: 0.2466
2026-01-14 00:53:12,508: t15.2025.01.12 val PER: 0.0924
2026-01-14 00:53:12,508: t15.2025.03.14 val PER: 0.2973
2026-01-14 00:53:12,508: t15.2025.03.16 val PER: 0.1191
2026-01-14 00:53:12,508: t15.2025.03.30 val PER: 0.2184
2026-01-14 00:53:12,509: t15.2025.04.13 val PER: 0.1854
2026-01-14 00:53:12,661: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_87000
2026-01-14 00:53:30,129: Train batch 87200: loss: 0.21 grad norm: 19.56 time: 0.064
2026-01-14 00:53:47,570: Train batch 87400: loss: 0.09 grad norm: 7.78 time: 0.061
2026-01-14 00:53:56,097: Running test after training batch: 87500
2026-01-14 00:53:56,189: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:54:01,051: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:54:01,137: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:54:16,908: Val batch 87500: PER (avg): 0.0995 CTC Loss (avg): 19.4328 WER(5gram): 18.90% (n=256) time: 20.811
2026-01-14 00:54:16,909: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-14 00:54:16,909: t15.2023.08.13 val PER: 0.0655
2026-01-14 00:54:16,909: t15.2023.08.18 val PER: 0.0671
2026-01-14 00:54:16,909: t15.2023.08.20 val PER: 0.0580
2026-01-14 00:54:16,909: t15.2023.08.25 val PER: 0.0708
2026-01-14 00:54:16,909: t15.2023.08.27 val PER: 0.1399
2026-01-14 00:54:16,910: t15.2023.09.01 val PER: 0.0390
2026-01-14 00:54:16,910: t15.2023.09.03 val PER: 0.1152
2026-01-14 00:54:16,910: t15.2023.09.24 val PER: 0.0825
2026-01-14 00:54:16,910: t15.2023.09.29 val PER: 0.0932
2026-01-14 00:54:16,910: t15.2023.10.01 val PER: 0.1281
2026-01-14 00:54:16,910: t15.2023.10.06 val PER: 0.0452
2026-01-14 00:54:16,910: t15.2023.10.08 val PER: 0.1908
2026-01-14 00:54:16,910: t15.2023.10.13 val PER: 0.1404
2026-01-14 00:54:16,910: t15.2023.10.15 val PER: 0.1015
2026-01-14 00:54:16,911: t15.2023.10.20 val PER: 0.1544
2026-01-14 00:54:16,911: t15.2023.10.22 val PER: 0.0824
2026-01-14 00:54:16,911: t15.2023.11.03 val PER: 0.1411
2026-01-14 00:54:16,911: t15.2023.11.04 val PER: 0.0171
2026-01-14 00:54:16,911: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:54:16,911: t15.2023.11.19 val PER: 0.0180
2026-01-14 00:54:16,911: t15.2023.11.26 val PER: 0.0399
2026-01-14 00:54:16,911: t15.2023.12.03 val PER: 0.0420
2026-01-14 00:54:16,911: t15.2023.12.08 val PER: 0.0320
2026-01-14 00:54:16,912: t15.2023.12.10 val PER: 0.0315
2026-01-14 00:54:16,912: t15.2023.12.17 val PER: 0.0842
2026-01-14 00:54:16,912: t15.2023.12.29 val PER: 0.0707
2026-01-14 00:54:16,912: t15.2024.02.25 val PER: 0.0562
2026-01-14 00:54:16,912: t15.2024.03.08 val PER: 0.1550
2026-01-14 00:54:16,912: t15.2024.03.15 val PER: 0.1476
2026-01-14 00:54:16,912: t15.2024.03.17 val PER: 0.0697
2026-01-14 00:54:16,912: t15.2024.05.10 val PER: 0.1114
2026-01-14 00:54:16,912: t15.2024.06.14 val PER: 0.1183
2026-01-14 00:54:16,912: t15.2024.07.19 val PER: 0.1523
2026-01-14 00:54:16,913: t15.2024.07.21 val PER: 0.0579
2026-01-14 00:54:16,913: t15.2024.07.28 val PER: 0.0949
2026-01-14 00:54:16,913: t15.2025.01.10 val PER: 0.2507
2026-01-14 00:54:16,913: t15.2025.01.12 val PER: 0.0893
2026-01-14 00:54:16,913: t15.2025.03.14 val PER: 0.3003
2026-01-14 00:54:16,913: t15.2025.03.16 val PER: 0.1230
2026-01-14 00:54:16,913: t15.2025.03.30 val PER: 0.2218
2026-01-14 00:54:16,913: t15.2025.04.13 val PER: 0.1826
2026-01-14 00:54:17,070: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_87500
2026-01-14 00:54:25,814: Train batch 87600: loss: 0.17 grad norm: 10.28 time: 0.071
2026-01-14 00:54:43,530: Train batch 87800: loss: 0.12 grad norm: 9.31 time: 0.062
2026-01-14 00:55:01,392: Train batch 88000: loss: 0.04 grad norm: 3.80 time: 0.065
2026-01-14 00:55:01,392: Running test after training batch: 88000
2026-01-14 00:55:01,520: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:55:06,423: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:55:06,495: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:55:21,501: Val batch 88000: PER (avg): 0.0998 CTC Loss (avg): 19.3703 WER(5gram): 18.51% (n=256) time: 20.108
2026-01-14 00:55:21,501: WER lens: avg_true_words=5.99 avg_pred_words=6.34 max_pred_words=13
2026-01-14 00:55:21,501: t15.2023.08.13 val PER: 0.0696
2026-01-14 00:55:21,501: t15.2023.08.18 val PER: 0.0679
2026-01-14 00:55:21,502: t15.2023.08.20 val PER: 0.0572
2026-01-14 00:55:21,502: t15.2023.08.25 val PER: 0.0693
2026-01-14 00:55:21,502: t15.2023.08.27 val PER: 0.1399
2026-01-14 00:55:21,502: t15.2023.09.01 val PER: 0.0414
2026-01-14 00:55:21,502: t15.2023.09.03 val PER: 0.1128
2026-01-14 00:55:21,502: t15.2023.09.24 val PER: 0.0862
2026-01-14 00:55:21,502: t15.2023.09.29 val PER: 0.0944
2026-01-14 00:55:21,502: t15.2023.10.01 val PER: 0.1275
2026-01-14 00:55:21,502: t15.2023.10.06 val PER: 0.0431
2026-01-14 00:55:21,502: t15.2023.10.08 val PER: 0.1976
2026-01-14 00:55:21,503: t15.2023.10.13 val PER: 0.1358
2026-01-14 00:55:21,503: t15.2023.10.15 val PER: 0.1022
2026-01-14 00:55:21,503: t15.2023.10.20 val PER: 0.1510
2026-01-14 00:55:21,503: t15.2023.10.22 val PER: 0.0846
2026-01-14 00:55:21,503: t15.2023.11.03 val PER: 0.1384
2026-01-14 00:55:21,503: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:55:21,503: t15.2023.11.17 val PER: 0.0187
2026-01-14 00:55:21,503: t15.2023.11.19 val PER: 0.0220
2026-01-14 00:55:21,503: t15.2023.11.26 val PER: 0.0384
2026-01-14 00:55:21,503: t15.2023.12.03 val PER: 0.0441
2026-01-14 00:55:21,503: t15.2023.12.08 val PER: 0.0313
2026-01-14 00:55:21,503: t15.2023.12.10 val PER: 0.0355
2026-01-14 00:55:21,504: t15.2023.12.17 val PER: 0.0821
2026-01-14 00:55:21,504: t15.2023.12.29 val PER: 0.0686
2026-01-14 00:55:21,504: t15.2024.02.25 val PER: 0.0646
2026-01-14 00:55:21,504: t15.2024.03.08 val PER: 0.1550
2026-01-14 00:55:21,504: t15.2024.03.15 val PER: 0.1507
2026-01-14 00:55:21,504: t15.2024.03.17 val PER: 0.0690
2026-01-14 00:55:21,504: t15.2024.05.10 val PER: 0.1159
2026-01-14 00:55:21,504: t15.2024.06.14 val PER: 0.1167
2026-01-14 00:55:21,504: t15.2024.07.19 val PER: 0.1490
2026-01-14 00:55:21,504: t15.2024.07.21 val PER: 0.0572
2026-01-14 00:55:21,504: t15.2024.07.28 val PER: 0.0926
2026-01-14 00:55:21,504: t15.2025.01.10 val PER: 0.2479
2026-01-14 00:55:21,504: t15.2025.01.12 val PER: 0.0916
2026-01-14 00:55:21,504: t15.2025.03.14 val PER: 0.2959
2026-01-14 00:55:21,504: t15.2025.03.16 val PER: 0.1243
2026-01-14 00:55:21,504: t15.2025.03.30 val PER: 0.2230
2026-01-14 00:55:21,505: t15.2025.04.13 val PER: 0.1854
2026-01-14 00:55:21,660: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_88000
2026-01-14 00:55:39,034: Train batch 88200: loss: 0.04 grad norm: 3.67 time: 0.067
2026-01-14 00:55:56,362: Train batch 88400: loss: 0.15 grad norm: 9.23 time: 0.070
2026-01-14 00:56:05,176: Running test after training batch: 88500
2026-01-14 00:56:05,303: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:56:10,623: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:56:10,702: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:56:27,386: Val batch 88500: PER (avg): 0.1006 CTC Loss (avg): 19.4660 WER(5gram): 18.90% (n=256) time: 22.209
2026-01-14 00:56:27,387: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=13
2026-01-14 00:56:27,387: t15.2023.08.13 val PER: 0.0634
2026-01-14 00:56:27,387: t15.2023.08.18 val PER: 0.0671
2026-01-14 00:56:27,387: t15.2023.08.20 val PER: 0.0604
2026-01-14 00:56:27,387: t15.2023.08.25 val PER: 0.0723
2026-01-14 00:56:27,388: t15.2023.08.27 val PER: 0.1399
2026-01-14 00:56:27,388: t15.2023.09.01 val PER: 0.0398
2026-01-14 00:56:27,388: t15.2023.09.03 val PER: 0.1176
2026-01-14 00:56:27,388: t15.2023.09.24 val PER: 0.0874
2026-01-14 00:56:27,388: t15.2023.09.29 val PER: 0.0989
2026-01-14 00:56:27,388: t15.2023.10.01 val PER: 0.1288
2026-01-14 00:56:27,388: t15.2023.10.06 val PER: 0.0420
2026-01-14 00:56:27,388: t15.2023.10.08 val PER: 0.2003
2026-01-14 00:56:27,388: t15.2023.10.13 val PER: 0.1319
2026-01-14 00:56:27,388: t15.2023.10.15 val PER: 0.1015
2026-01-14 00:56:27,389: t15.2023.10.20 val PER: 0.1544
2026-01-14 00:56:27,389: t15.2023.10.22 val PER: 0.0880
2026-01-14 00:56:27,389: t15.2023.11.03 val PER: 0.1445
2026-01-14 00:56:27,389: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:56:27,389: t15.2023.11.17 val PER: 0.0187
2026-01-14 00:56:27,389: t15.2023.11.19 val PER: 0.0220
2026-01-14 00:56:27,389: t15.2023.11.26 val PER: 0.0435
2026-01-14 00:56:27,389: t15.2023.12.03 val PER: 0.0473
2026-01-14 00:56:27,389: t15.2023.12.08 val PER: 0.0293
2026-01-14 00:56:27,389: t15.2023.12.10 val PER: 0.0355
2026-01-14 00:56:27,389: t15.2023.12.17 val PER: 0.0811
2026-01-14 00:56:27,390: t15.2023.12.29 val PER: 0.0673
2026-01-14 00:56:27,390: t15.2024.02.25 val PER: 0.0590
2026-01-14 00:56:27,390: t15.2024.03.08 val PER: 0.1536
2026-01-14 00:56:27,390: t15.2024.03.15 val PER: 0.1545
2026-01-14 00:56:27,390: t15.2024.03.17 val PER: 0.0711
2026-01-14 00:56:27,390: t15.2024.05.10 val PER: 0.1159
2026-01-14 00:56:27,390: t15.2024.06.14 val PER: 0.1136
2026-01-14 00:56:27,390: t15.2024.07.19 val PER: 0.1556
2026-01-14 00:56:27,390: t15.2024.07.21 val PER: 0.0566
2026-01-14 00:56:27,390: t15.2024.07.28 val PER: 0.0919
2026-01-14 00:56:27,391: t15.2025.01.10 val PER: 0.2507
2026-01-14 00:56:27,391: t15.2025.01.12 val PER: 0.0931
2026-01-14 00:56:27,391: t15.2025.03.14 val PER: 0.3018
2026-01-14 00:56:27,391: t15.2025.03.16 val PER: 0.1204
2026-01-14 00:56:27,391: t15.2025.03.30 val PER: 0.2172
2026-01-14 00:56:27,391: t15.2025.04.13 val PER: 0.1854
2026-01-14 00:56:27,546: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_88500
2026-01-14 00:56:36,367: Train batch 88600: loss: 0.10 grad norm: 9.39 time: 0.062
2026-01-14 00:56:53,701: Train batch 88800: loss: 0.20 grad norm: 8.04 time: 0.063
2026-01-14 00:57:11,217: Train batch 89000: loss: 0.52 grad norm: 22.19 time: 0.055
2026-01-14 00:57:11,218: Running test after training batch: 89000
2026-01-14 00:57:11,377: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:57:16,375: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:57:16,442: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:57:32,864: Val batch 89000: PER (avg): 0.1005 CTC Loss (avg): 19.5492 WER(5gram): 18.97% (n=256) time: 21.646
2026-01-14 00:57:32,864: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=13
2026-01-14 00:57:32,864: t15.2023.08.13 val PER: 0.0676
2026-01-14 00:57:32,864: t15.2023.08.18 val PER: 0.0662
2026-01-14 00:57:32,865: t15.2023.08.20 val PER: 0.0572
2026-01-14 00:57:32,865: t15.2023.08.25 val PER: 0.0738
2026-01-14 00:57:32,865: t15.2023.08.27 val PER: 0.1399
2026-01-14 00:57:32,865: t15.2023.09.01 val PER: 0.0398
2026-01-14 00:57:32,865: t15.2023.09.03 val PER: 0.1105
2026-01-14 00:57:32,865: t15.2023.09.24 val PER: 0.0837
2026-01-14 00:57:32,865: t15.2023.09.29 val PER: 0.0957
2026-01-14 00:57:32,865: t15.2023.10.01 val PER: 0.1281
2026-01-14 00:57:32,865: t15.2023.10.06 val PER: 0.0420
2026-01-14 00:57:32,865: t15.2023.10.08 val PER: 0.2043
2026-01-14 00:57:32,865: t15.2023.10.13 val PER: 0.1342
2026-01-14 00:57:32,865: t15.2023.10.15 val PER: 0.1028
2026-01-14 00:57:32,866: t15.2023.10.20 val PER: 0.1510
2026-01-14 00:57:32,866: t15.2023.10.22 val PER: 0.0846
2026-01-14 00:57:32,866: t15.2023.11.03 val PER: 0.1445
2026-01-14 00:57:32,866: t15.2023.11.04 val PER: 0.0239
2026-01-14 00:57:32,866: t15.2023.11.17 val PER: 0.0156
2026-01-14 00:57:32,866: t15.2023.11.19 val PER: 0.0220
2026-01-14 00:57:32,866: t15.2023.11.26 val PER: 0.0413
2026-01-14 00:57:32,866: t15.2023.12.03 val PER: 0.0452
2026-01-14 00:57:32,866: t15.2023.12.08 val PER: 0.0326
2026-01-14 00:57:32,866: t15.2023.12.10 val PER: 0.0355
2026-01-14 00:57:32,866: t15.2023.12.17 val PER: 0.0842
2026-01-14 00:57:32,866: t15.2023.12.29 val PER: 0.0707
2026-01-14 00:57:32,866: t15.2024.02.25 val PER: 0.0562
2026-01-14 00:57:32,866: t15.2024.03.08 val PER: 0.1607
2026-01-14 00:57:32,866: t15.2024.03.15 val PER: 0.1482
2026-01-14 00:57:32,866: t15.2024.03.17 val PER: 0.0676
2026-01-14 00:57:32,867: t15.2024.05.10 val PER: 0.1189
2026-01-14 00:57:32,867: t15.2024.06.14 val PER: 0.1183
2026-01-14 00:57:32,867: t15.2024.07.19 val PER: 0.1575
2026-01-14 00:57:32,867: t15.2024.07.21 val PER: 0.0572
2026-01-14 00:57:32,867: t15.2024.07.28 val PER: 0.0941
2026-01-14 00:57:32,867: t15.2025.01.10 val PER: 0.2534
2026-01-14 00:57:32,867: t15.2025.01.12 val PER: 0.0893
2026-01-14 00:57:32,867: t15.2025.03.14 val PER: 0.2959
2026-01-14 00:57:32,867: t15.2025.03.16 val PER: 0.1230
2026-01-14 00:57:32,867: t15.2025.03.30 val PER: 0.2241
2026-01-14 00:57:32,867: t15.2025.04.13 val PER: 0.1883
2026-01-14 00:57:33,024: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_89000
2026-01-14 00:57:50,556: Train batch 89200: loss: 0.14 grad norm: 11.23 time: 0.060
2026-01-14 00:58:08,230: Train batch 89400: loss: 0.31 grad norm: 18.06 time: 0.084
2026-01-14 00:58:16,906: Running test after training batch: 89500
2026-01-14 00:58:17,030: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:58:22,216: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:58:22,289: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:58:37,350: Val batch 89500: PER (avg): 0.1001 CTC Loss (avg): 19.4806 WER(5gram): 18.58% (n=256) time: 20.444
2026-01-14 00:58:37,351: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-14 00:58:37,351: t15.2023.08.13 val PER: 0.0665
2026-01-14 00:58:37,351: t15.2023.08.18 val PER: 0.0662
2026-01-14 00:58:37,351: t15.2023.08.20 val PER: 0.0580
2026-01-14 00:58:37,351: t15.2023.08.25 val PER: 0.0708
2026-01-14 00:58:37,351: t15.2023.08.27 val PER: 0.1463
2026-01-14 00:58:37,351: t15.2023.09.01 val PER: 0.0406
2026-01-14 00:58:37,352: t15.2023.09.03 val PER: 0.1081
2026-01-14 00:58:37,352: t15.2023.09.24 val PER: 0.0850
2026-01-14 00:58:37,352: t15.2023.09.29 val PER: 0.0989
2026-01-14 00:58:37,352: t15.2023.10.01 val PER: 0.1275
2026-01-14 00:58:37,352: t15.2023.10.06 val PER: 0.0420
2026-01-14 00:58:37,352: t15.2023.10.08 val PER: 0.2043
2026-01-14 00:58:37,352: t15.2023.10.13 val PER: 0.1350
2026-01-14 00:58:37,352: t15.2023.10.15 val PER: 0.1015
2026-01-14 00:58:37,352: t15.2023.10.20 val PER: 0.1544
2026-01-14 00:58:37,352: t15.2023.10.22 val PER: 0.0857
2026-01-14 00:58:37,352: t15.2023.11.03 val PER: 0.1384
2026-01-14 00:58:37,352: t15.2023.11.04 val PER: 0.0205
2026-01-14 00:58:37,352: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:58:37,352: t15.2023.11.19 val PER: 0.0180
2026-01-14 00:58:37,353: t15.2023.11.26 val PER: 0.0406
2026-01-14 00:58:37,353: t15.2023.12.03 val PER: 0.0483
2026-01-14 00:58:37,353: t15.2023.12.08 val PER: 0.0313
2026-01-14 00:58:37,354: t15.2023.12.10 val PER: 0.0342
2026-01-14 00:58:37,354: t15.2023.12.17 val PER: 0.0769
2026-01-14 00:58:37,354: t15.2023.12.29 val PER: 0.0748
2026-01-14 00:58:37,354: t15.2024.02.25 val PER: 0.0576
2026-01-14 00:58:37,354: t15.2024.03.08 val PER: 0.1550
2026-01-14 00:58:37,354: t15.2024.03.15 val PER: 0.1501
2026-01-14 00:58:37,354: t15.2024.03.17 val PER: 0.0676
2026-01-14 00:58:37,354: t15.2024.05.10 val PER: 0.1218
2026-01-14 00:58:37,354: t15.2024.06.14 val PER: 0.1120
2026-01-14 00:58:37,354: t15.2024.07.19 val PER: 0.1556
2026-01-14 00:58:37,354: t15.2024.07.21 val PER: 0.0593
2026-01-14 00:58:37,355: t15.2024.07.28 val PER: 0.0904
2026-01-14 00:58:37,355: t15.2025.01.10 val PER: 0.2548
2026-01-14 00:58:37,355: t15.2025.01.12 val PER: 0.0847
2026-01-14 00:58:37,355: t15.2025.03.14 val PER: 0.3033
2026-01-14 00:58:37,355: t15.2025.03.16 val PER: 0.1230
2026-01-14 00:58:37,355: t15.2025.03.30 val PER: 0.2207
2026-01-14 00:58:37,355: t15.2025.04.13 val PER: 0.1854
2026-01-14 00:58:37,508: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_89500
2026-01-14 00:58:46,332: Train batch 89600: loss: 0.53 grad norm: 32.04 time: 0.066
2026-01-14 00:59:03,630: Train batch 89800: loss: 0.13 grad norm: 8.22 time: 0.074
2026-01-14 00:59:21,084: Train batch 90000: loss: 0.08 grad norm: 10.80 time: 0.056
2026-01-14 00:59:21,084: Running test after training batch: 90000
2026-01-14 00:59:21,193: WER debug GT example: You can see the code at this point as well.
2026-01-14 00:59:26,284: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 00:59:26,347: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 00:59:41,396: Val batch 90000: PER (avg): 0.1002 CTC Loss (avg): 19.5992 WER(5gram): 18.71% (n=256) time: 20.312
2026-01-14 00:59:41,397: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-14 00:59:41,397: t15.2023.08.13 val PER: 0.0665
2026-01-14 00:59:41,397: t15.2023.08.18 val PER: 0.0662
2026-01-14 00:59:41,398: t15.2023.08.20 val PER: 0.0556
2026-01-14 00:59:41,398: t15.2023.08.25 val PER: 0.0693
2026-01-14 00:59:41,398: t15.2023.08.27 val PER: 0.1383
2026-01-14 00:59:41,398: t15.2023.09.01 val PER: 0.0414
2026-01-14 00:59:41,398: t15.2023.09.03 val PER: 0.1093
2026-01-14 00:59:41,398: t15.2023.09.24 val PER: 0.0874
2026-01-14 00:59:41,398: t15.2023.09.29 val PER: 0.0996
2026-01-14 00:59:41,398: t15.2023.10.01 val PER: 0.1295
2026-01-14 00:59:41,398: t15.2023.10.06 val PER: 0.0441
2026-01-14 00:59:41,398: t15.2023.10.08 val PER: 0.1962
2026-01-14 00:59:41,398: t15.2023.10.13 val PER: 0.1389
2026-01-14 00:59:41,398: t15.2023.10.15 val PER: 0.1028
2026-01-14 00:59:41,398: t15.2023.10.20 val PER: 0.1577
2026-01-14 00:59:41,399: t15.2023.10.22 val PER: 0.0846
2026-01-14 00:59:41,399: t15.2023.11.03 val PER: 0.1350
2026-01-14 00:59:41,399: t15.2023.11.04 val PER: 0.0239
2026-01-14 00:59:41,399: t15.2023.11.17 val PER: 0.0171
2026-01-14 00:59:41,399: t15.2023.11.19 val PER: 0.0180
2026-01-14 00:59:41,399: t15.2023.11.26 val PER: 0.0449
2026-01-14 00:59:41,399: t15.2023.12.03 val PER: 0.0431
2026-01-14 00:59:41,399: t15.2023.12.08 val PER: 0.0326
2026-01-14 00:59:41,399: t15.2023.12.10 val PER: 0.0355
2026-01-14 00:59:41,399: t15.2023.12.17 val PER: 0.0800
2026-01-14 00:59:41,399: t15.2023.12.29 val PER: 0.0707
2026-01-14 00:59:41,399: t15.2024.02.25 val PER: 0.0576
2026-01-14 00:59:41,399: t15.2024.03.08 val PER: 0.1565
2026-01-14 00:59:41,399: t15.2024.03.15 val PER: 0.1488
2026-01-14 00:59:41,400: t15.2024.03.17 val PER: 0.0662
2026-01-14 00:59:41,400: t15.2024.05.10 val PER: 0.1233
2026-01-14 00:59:41,400: t15.2024.06.14 val PER: 0.1151
2026-01-14 00:59:41,400: t15.2024.07.19 val PER: 0.1569
2026-01-14 00:59:41,400: t15.2024.07.21 val PER: 0.0559
2026-01-14 00:59:41,400: t15.2024.07.28 val PER: 0.0904
2026-01-14 00:59:41,400: t15.2025.01.10 val PER: 0.2576
2026-01-14 00:59:41,400: t15.2025.01.12 val PER: 0.0885
2026-01-14 00:59:41,400: t15.2025.03.14 val PER: 0.3033
2026-01-14 00:59:41,400: t15.2025.03.16 val PER: 0.1243
2026-01-14 00:59:41,400: t15.2025.03.30 val PER: 0.2241
2026-01-14 00:59:41,400: t15.2025.04.13 val PER: 0.1826
2026-01-14 00:59:41,576: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_90000
2026-01-14 00:59:58,866: Train batch 90200: loss: 0.04 grad norm: 2.83 time: 0.055
2026-01-14 01:00:16,454: Train batch 90400: loss: 0.03 grad norm: 2.99 time: 0.063
2026-01-14 01:00:25,122: Running test after training batch: 90500
2026-01-14 01:00:25,280: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:00:30,092: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:00:30,155: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:00:45,312: Val batch 90500: PER (avg): 0.1003 CTC Loss (avg): 19.4075 WER(5gram): 18.71% (n=256) time: 20.189
2026-01-14 01:00:45,312: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=13
2026-01-14 01:00:45,312: t15.2023.08.13 val PER: 0.0696
2026-01-14 01:00:45,313: t15.2023.08.18 val PER: 0.0662
2026-01-14 01:00:45,313: t15.2023.08.20 val PER: 0.0564
2026-01-14 01:00:45,313: t15.2023.08.25 val PER: 0.0738
2026-01-14 01:00:45,313: t15.2023.08.27 val PER: 0.1383
2026-01-14 01:00:45,313: t15.2023.09.01 val PER: 0.0414
2026-01-14 01:00:45,313: t15.2023.09.03 val PER: 0.1128
2026-01-14 01:00:45,313: t15.2023.09.24 val PER: 0.0850
2026-01-14 01:00:45,313: t15.2023.09.29 val PER: 0.0970
2026-01-14 01:00:45,314: t15.2023.10.01 val PER: 0.1248
2026-01-14 01:00:45,314: t15.2023.10.06 val PER: 0.0452
2026-01-14 01:00:45,314: t15.2023.10.08 val PER: 0.1881
2026-01-14 01:00:45,314: t15.2023.10.13 val PER: 0.1350
2026-01-14 01:00:45,314: t15.2023.10.15 val PER: 0.1028
2026-01-14 01:00:45,314: t15.2023.10.20 val PER: 0.1577
2026-01-14 01:00:45,314: t15.2023.10.22 val PER: 0.0891
2026-01-14 01:00:45,314: t15.2023.11.03 val PER: 0.1398
2026-01-14 01:00:45,314: t15.2023.11.04 val PER: 0.0205
2026-01-14 01:00:45,315: t15.2023.11.17 val PER: 0.0156
2026-01-14 01:00:45,315: t15.2023.11.19 val PER: 0.0220
2026-01-14 01:00:45,315: t15.2023.11.26 val PER: 0.0435
2026-01-14 01:00:45,315: t15.2023.12.03 val PER: 0.0452
2026-01-14 01:00:45,315: t15.2023.12.08 val PER: 0.0333
2026-01-14 01:00:45,315: t15.2023.12.10 val PER: 0.0302
2026-01-14 01:00:45,315: t15.2023.12.17 val PER: 0.0748
2026-01-14 01:00:45,315: t15.2023.12.29 val PER: 0.0707
2026-01-14 01:00:45,315: t15.2024.02.25 val PER: 0.0548
2026-01-14 01:00:45,315: t15.2024.03.08 val PER: 0.1579
2026-01-14 01:00:45,315: t15.2024.03.15 val PER: 0.1520
2026-01-14 01:00:45,316: t15.2024.03.17 val PER: 0.0676
2026-01-14 01:00:45,316: t15.2024.05.10 val PER: 0.1248
2026-01-14 01:00:45,316: t15.2024.06.14 val PER: 0.1230
2026-01-14 01:00:45,316: t15.2024.07.19 val PER: 0.1608
2026-01-14 01:00:45,316: t15.2024.07.21 val PER: 0.0600
2026-01-14 01:00:45,316: t15.2024.07.28 val PER: 0.0897
2026-01-14 01:00:45,316: t15.2025.01.10 val PER: 0.2521
2026-01-14 01:00:45,316: t15.2025.01.12 val PER: 0.0847
2026-01-14 01:00:45,316: t15.2025.03.14 val PER: 0.3033
2026-01-14 01:00:45,316: t15.2025.03.16 val PER: 0.1243
2026-01-14 01:00:45,316: t15.2025.03.30 val PER: 0.2241
2026-01-14 01:00:45,317: t15.2025.04.13 val PER: 0.1826
2026-01-14 01:00:45,469: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_90500
2026-01-14 01:00:54,170: Train batch 90600: loss: 0.10 grad norm: 11.91 time: 0.078
2026-01-14 01:01:11,147: Train batch 90800: loss: 0.04 grad norm: 4.99 time: 0.051
2026-01-14 01:01:28,471: Train batch 91000: loss: 0.07 grad norm: 12.33 time: 0.059
2026-01-14 01:01:28,471: Running test after training batch: 91000
2026-01-14 01:01:28,606: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:01:33,478: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:01:33,547: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:01:50,250: Val batch 91000: PER (avg): 0.1000 CTC Loss (avg): 19.5052 WER(5gram): 19.49% (n=256) time: 21.779
2026-01-14 01:01:50,251: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=13
2026-01-14 01:01:50,251: t15.2023.08.13 val PER: 0.0665
2026-01-14 01:01:50,251: t15.2023.08.18 val PER: 0.0637
2026-01-14 01:01:50,251: t15.2023.08.20 val PER: 0.0564
2026-01-14 01:01:50,251: t15.2023.08.25 val PER: 0.0663
2026-01-14 01:01:50,251: t15.2023.08.27 val PER: 0.1399
2026-01-14 01:01:50,251: t15.2023.09.01 val PER: 0.0414
2026-01-14 01:01:50,251: t15.2023.09.03 val PER: 0.1128
2026-01-14 01:01:50,251: t15.2023.09.24 val PER: 0.0862
2026-01-14 01:01:50,252: t15.2023.09.29 val PER: 0.0976
2026-01-14 01:01:50,252: t15.2023.10.01 val PER: 0.1314
2026-01-14 01:01:50,252: t15.2023.10.06 val PER: 0.0420
2026-01-14 01:01:50,252: t15.2023.10.08 val PER: 0.1989
2026-01-14 01:01:50,252: t15.2023.10.13 val PER: 0.1396
2026-01-14 01:01:50,252: t15.2023.10.15 val PER: 0.0969
2026-01-14 01:01:50,252: t15.2023.10.20 val PER: 0.1477
2026-01-14 01:01:50,252: t15.2023.10.22 val PER: 0.0846
2026-01-14 01:01:50,252: t15.2023.11.03 val PER: 0.1411
2026-01-14 01:01:50,252: t15.2023.11.04 val PER: 0.0205
2026-01-14 01:01:50,252: t15.2023.11.17 val PER: 0.0187
2026-01-14 01:01:50,252: t15.2023.11.19 val PER: 0.0180
2026-01-14 01:01:50,253: t15.2023.11.26 val PER: 0.0428
2026-01-14 01:01:50,253: t15.2023.12.03 val PER: 0.0431
2026-01-14 01:01:50,253: t15.2023.12.08 val PER: 0.0340
2026-01-14 01:01:50,253: t15.2023.12.10 val PER: 0.0329
2026-01-14 01:01:50,253: t15.2023.12.17 val PER: 0.0769
2026-01-14 01:01:50,253: t15.2023.12.29 val PER: 0.0721
2026-01-14 01:01:50,253: t15.2024.02.25 val PER: 0.0604
2026-01-14 01:01:50,253: t15.2024.03.08 val PER: 0.1607
2026-01-14 01:01:50,253: t15.2024.03.15 val PER: 0.1513
2026-01-14 01:01:50,253: t15.2024.03.17 val PER: 0.0711
2026-01-14 01:01:50,253: t15.2024.05.10 val PER: 0.1263
2026-01-14 01:01:50,253: t15.2024.06.14 val PER: 0.1167
2026-01-14 01:01:50,253: t15.2024.07.19 val PER: 0.1562
2026-01-14 01:01:50,253: t15.2024.07.21 val PER: 0.0559
2026-01-14 01:01:50,253: t15.2024.07.28 val PER: 0.0912
2026-01-14 01:01:50,254: t15.2025.01.10 val PER: 0.2521
2026-01-14 01:01:50,254: t15.2025.01.12 val PER: 0.0870
2026-01-14 01:01:50,254: t15.2025.03.14 val PER: 0.3003
2026-01-14 01:01:50,254: t15.2025.03.16 val PER: 0.1243
2026-01-14 01:01:50,254: t15.2025.03.30 val PER: 0.2115
2026-01-14 01:01:50,254: t15.2025.04.13 val PER: 0.1797
2026-01-14 01:01:50,407: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_91000
2026-01-14 01:02:08,538: Train batch 91200: loss: 0.12 grad norm: 9.50 time: 0.064
2026-01-14 01:02:26,569: Train batch 91400: loss: 0.41 grad norm: 22.26 time: 0.093
2026-01-14 01:02:35,244: Running test after training batch: 91500
2026-01-14 01:02:35,374: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:02:40,487: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:02:40,550: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:02:56,871: Val batch 91500: PER (avg): 0.0999 CTC Loss (avg): 19.5963 WER(5gram): 18.84% (n=256) time: 21.627
2026-01-14 01:02:56,872: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-14 01:02:56,872: t15.2023.08.13 val PER: 0.0686
2026-01-14 01:02:56,872: t15.2023.08.18 val PER: 0.0662
2026-01-14 01:02:56,872: t15.2023.08.20 val PER: 0.0580
2026-01-14 01:02:56,872: t15.2023.08.25 val PER: 0.0723
2026-01-14 01:02:56,873: t15.2023.08.27 val PER: 0.1367
2026-01-14 01:02:56,873: t15.2023.09.01 val PER: 0.0414
2026-01-14 01:02:56,873: t15.2023.09.03 val PER: 0.1116
2026-01-14 01:02:56,873: t15.2023.09.24 val PER: 0.0850
2026-01-14 01:02:56,873: t15.2023.09.29 val PER: 0.0951
2026-01-14 01:02:56,873: t15.2023.10.01 val PER: 0.1314
2026-01-14 01:02:56,873: t15.2023.10.06 val PER: 0.0431
2026-01-14 01:02:56,873: t15.2023.10.08 val PER: 0.1935
2026-01-14 01:02:56,873: t15.2023.10.13 val PER: 0.1350
2026-01-14 01:02:56,873: t15.2023.10.15 val PER: 0.1002
2026-01-14 01:02:56,873: t15.2023.10.20 val PER: 0.1611
2026-01-14 01:02:56,873: t15.2023.10.22 val PER: 0.0835
2026-01-14 01:02:56,874: t15.2023.11.03 val PER: 0.1370
2026-01-14 01:02:56,874: t15.2023.11.04 val PER: 0.0171
2026-01-14 01:02:56,874: t15.2023.11.17 val PER: 0.0171
2026-01-14 01:02:56,874: t15.2023.11.19 val PER: 0.0200
2026-01-14 01:02:56,874: t15.2023.11.26 val PER: 0.0435
2026-01-14 01:02:56,874: t15.2023.12.03 val PER: 0.0420
2026-01-14 01:02:56,874: t15.2023.12.08 val PER: 0.0333
2026-01-14 01:02:56,874: t15.2023.12.10 val PER: 0.0329
2026-01-14 01:02:56,874: t15.2023.12.17 val PER: 0.0790
2026-01-14 01:02:56,874: t15.2023.12.29 val PER: 0.0755
2026-01-14 01:02:56,874: t15.2024.02.25 val PER: 0.0618
2026-01-14 01:02:56,874: t15.2024.03.08 val PER: 0.1550
2026-01-14 01:02:56,875: t15.2024.03.15 val PER: 0.1507
2026-01-14 01:02:56,875: t15.2024.03.17 val PER: 0.0697
2026-01-14 01:02:56,875: t15.2024.05.10 val PER: 0.1248
2026-01-14 01:02:56,875: t15.2024.06.14 val PER: 0.1104
2026-01-14 01:02:56,875: t15.2024.07.19 val PER: 0.1543
2026-01-14 01:02:56,875: t15.2024.07.21 val PER: 0.0572
2026-01-14 01:02:56,875: t15.2024.07.28 val PER: 0.0926
2026-01-14 01:02:56,875: t15.2025.01.10 val PER: 0.2590
2026-01-14 01:02:56,875: t15.2025.01.12 val PER: 0.0862
2026-01-14 01:02:56,875: t15.2025.03.14 val PER: 0.2959
2026-01-14 01:02:56,875: t15.2025.03.16 val PER: 0.1243
2026-01-14 01:02:56,875: t15.2025.03.30 val PER: 0.2149
2026-01-14 01:02:56,875: t15.2025.04.13 val PER: 0.1826
2026-01-14 01:02:57,028: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_91500
2026-01-14 01:03:05,910: Train batch 91600: loss: 0.09 grad norm: 10.67 time: 0.077
2026-01-14 01:03:23,419: Train batch 91800: loss: 0.07 grad norm: 6.72 time: 0.061
2026-01-14 01:03:41,186: Train batch 92000: loss: 0.05 grad norm: 5.33 time: 0.064
2026-01-14 01:03:41,187: Running test after training batch: 92000
2026-01-14 01:03:41,296: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:03:46,195: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:03:46,266: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:04:02,705: Val batch 92000: PER (avg): 0.1000 CTC Loss (avg): 19.6059 WER(5gram): 18.77% (n=256) time: 21.518
2026-01-14 01:04:02,705: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=13
2026-01-14 01:04:02,705: t15.2023.08.13 val PER: 0.0686
2026-01-14 01:04:02,705: t15.2023.08.18 val PER: 0.0654
2026-01-14 01:04:02,705: t15.2023.08.20 val PER: 0.0588
2026-01-14 01:04:02,705: t15.2023.08.25 val PER: 0.0708
2026-01-14 01:04:02,706: t15.2023.08.27 val PER: 0.1415
2026-01-14 01:04:02,706: t15.2023.09.01 val PER: 0.0406
2026-01-14 01:04:02,706: t15.2023.09.03 val PER: 0.1128
2026-01-14 01:04:02,706: t15.2023.09.24 val PER: 0.0850
2026-01-14 01:04:02,706: t15.2023.09.29 val PER: 0.0964
2026-01-14 01:04:02,706: t15.2023.10.01 val PER: 0.1275
2026-01-14 01:04:02,706: t15.2023.10.06 val PER: 0.0398
2026-01-14 01:04:02,706: t15.2023.10.08 val PER: 0.1922
2026-01-14 01:04:02,706: t15.2023.10.13 val PER: 0.1381
2026-01-14 01:04:02,706: t15.2023.10.15 val PER: 0.1002
2026-01-14 01:04:02,706: t15.2023.10.20 val PER: 0.1510
2026-01-14 01:04:02,706: t15.2023.10.22 val PER: 0.0824
2026-01-14 01:04:02,707: t15.2023.11.03 val PER: 0.1364
2026-01-14 01:04:02,707: t15.2023.11.04 val PER: 0.0205
2026-01-14 01:04:02,707: t15.2023.11.17 val PER: 0.0156
2026-01-14 01:04:02,707: t15.2023.11.19 val PER: 0.0160
2026-01-14 01:04:02,707: t15.2023.11.26 val PER: 0.0442
2026-01-14 01:04:02,707: t15.2023.12.03 val PER: 0.0420
2026-01-14 01:04:02,707: t15.2023.12.08 val PER: 0.0326
2026-01-14 01:04:02,707: t15.2023.12.10 val PER: 0.0329
2026-01-14 01:04:02,707: t15.2023.12.17 val PER: 0.0759
2026-01-14 01:04:02,707: t15.2023.12.29 val PER: 0.0728
2026-01-14 01:04:02,707: t15.2024.02.25 val PER: 0.0632
2026-01-14 01:04:02,708: t15.2024.03.08 val PER: 0.1579
2026-01-14 01:04:02,708: t15.2024.03.15 val PER: 0.1495
2026-01-14 01:04:02,708: t15.2024.03.17 val PER: 0.0697
2026-01-14 01:04:02,708: t15.2024.05.10 val PER: 0.1248
2026-01-14 01:04:02,708: t15.2024.06.14 val PER: 0.1136
2026-01-14 01:04:02,708: t15.2024.07.19 val PER: 0.1589
2026-01-14 01:04:02,708: t15.2024.07.21 val PER: 0.0566
2026-01-14 01:04:02,708: t15.2024.07.28 val PER: 0.0949
2026-01-14 01:04:02,708: t15.2025.01.10 val PER: 0.2534
2026-01-14 01:04:02,708: t15.2025.01.12 val PER: 0.0885
2026-01-14 01:04:02,708: t15.2025.03.14 val PER: 0.3003
2026-01-14 01:04:02,708: t15.2025.03.16 val PER: 0.1243
2026-01-14 01:04:02,708: t15.2025.03.30 val PER: 0.2218
2026-01-14 01:04:02,708: t15.2025.04.13 val PER: 0.1826
2026-01-14 01:04:02,861: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_92000
2026-01-14 01:04:20,165: Train batch 92200: loss: 0.08 grad norm: 7.30 time: 0.081
2026-01-14 01:04:37,724: Train batch 92400: loss: 0.07 grad norm: 8.33 time: 0.081
2026-01-14 01:04:46,341: Running test after training batch: 92500
2026-01-14 01:04:46,449: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:04:51,350: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:04:51,432: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:05:07,837: Val batch 92500: PER (avg): 0.0993 CTC Loss (avg): 19.5279 WER(5gram): 19.17% (n=256) time: 21.496
2026-01-14 01:05:07,837: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=13
2026-01-14 01:05:07,838: t15.2023.08.13 val PER: 0.0686
2026-01-14 01:05:07,838: t15.2023.08.18 val PER: 0.0687
2026-01-14 01:05:07,838: t15.2023.08.20 val PER: 0.0572
2026-01-14 01:05:07,838: t15.2023.08.25 val PER: 0.0738
2026-01-14 01:05:07,838: t15.2023.08.27 val PER: 0.1383
2026-01-14 01:05:07,838: t15.2023.09.01 val PER: 0.0390
2026-01-14 01:05:07,838: t15.2023.09.03 val PER: 0.1128
2026-01-14 01:05:07,838: t15.2023.09.24 val PER: 0.0874
2026-01-14 01:05:07,838: t15.2023.09.29 val PER: 0.0964
2026-01-14 01:05:07,839: t15.2023.10.01 val PER: 0.1301
2026-01-14 01:05:07,839: t15.2023.10.06 val PER: 0.0420
2026-01-14 01:05:07,839: t15.2023.10.08 val PER: 0.1894
2026-01-14 01:05:07,839: t15.2023.10.13 val PER: 0.1373
2026-01-14 01:05:07,839: t15.2023.10.15 val PER: 0.1002
2026-01-14 01:05:07,839: t15.2023.10.20 val PER: 0.1477
2026-01-14 01:05:07,839: t15.2023.10.22 val PER: 0.0780
2026-01-14 01:05:07,839: t15.2023.11.03 val PER: 0.1370
2026-01-14 01:05:07,839: t15.2023.11.04 val PER: 0.0205
2026-01-14 01:05:07,839: t15.2023.11.17 val PER: 0.0171
2026-01-14 01:05:07,839: t15.2023.11.19 val PER: 0.0160
2026-01-14 01:05:07,839: t15.2023.11.26 val PER: 0.0428
2026-01-14 01:05:07,839: t15.2023.12.03 val PER: 0.0431
2026-01-14 01:05:07,839: t15.2023.12.08 val PER: 0.0326
2026-01-14 01:05:07,839: t15.2023.12.10 val PER: 0.0329
2026-01-14 01:05:07,839: t15.2023.12.17 val PER: 0.0780
2026-01-14 01:05:07,840: t15.2023.12.29 val PER: 0.0693
2026-01-14 01:05:07,840: t15.2024.02.25 val PER: 0.0632
2026-01-14 01:05:07,840: t15.2024.03.08 val PER: 0.1550
2026-01-14 01:05:07,840: t15.2024.03.15 val PER: 0.1488
2026-01-14 01:05:07,840: t15.2024.03.17 val PER: 0.0690
2026-01-14 01:05:07,840: t15.2024.05.10 val PER: 0.1218
2026-01-14 01:05:07,840: t15.2024.06.14 val PER: 0.1120
2026-01-14 01:05:07,840: t15.2024.07.19 val PER: 0.1556
2026-01-14 01:05:07,840: t15.2024.07.21 val PER: 0.0572
2026-01-14 01:05:07,841: t15.2024.07.28 val PER: 0.0949
2026-01-14 01:05:07,841: t15.2025.01.10 val PER: 0.2534
2026-01-14 01:05:07,841: t15.2025.01.12 val PER: 0.0847
2026-01-14 01:05:07,841: t15.2025.03.14 val PER: 0.3003
2026-01-14 01:05:07,841: t15.2025.03.16 val PER: 0.1191
2026-01-14 01:05:07,841: t15.2025.03.30 val PER: 0.2149
2026-01-14 01:05:07,841: t15.2025.04.13 val PER: 0.1826
2026-01-14 01:05:07,993: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_92500
2026-01-14 01:05:16,676: Train batch 92600: loss: 1.00 grad norm: 13.66 time: 0.069
2026-01-14 01:05:33,949: Train batch 92800: loss: 0.33 grad norm: 16.47 time: 0.063
2026-01-14 01:05:51,467: Train batch 93000: loss: 0.04 grad norm: 3.36 time: 0.065
2026-01-14 01:05:51,467: Running test after training batch: 93000
2026-01-14 01:05:51,609: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:05:56,588: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:05:56,650: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:06:11,357: Val batch 93000: PER (avg): 0.0999 CTC Loss (avg): 19.6429 WER(5gram): 19.36% (n=256) time: 19.890
2026-01-14 01:06:11,358: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=13
2026-01-14 01:06:11,358: t15.2023.08.13 val PER: 0.0655
2026-01-14 01:06:11,358: t15.2023.08.18 val PER: 0.0645
2026-01-14 01:06:11,358: t15.2023.08.20 val PER: 0.0556
2026-01-14 01:06:11,358: t15.2023.08.25 val PER: 0.0738
2026-01-14 01:06:11,358: t15.2023.08.27 val PER: 0.1479
2026-01-14 01:06:11,359: t15.2023.09.01 val PER: 0.0390
2026-01-14 01:06:11,359: t15.2023.09.03 val PER: 0.1152
2026-01-14 01:06:11,359: t15.2023.09.24 val PER: 0.0850
2026-01-14 01:06:11,359: t15.2023.09.29 val PER: 0.0951
2026-01-14 01:06:11,359: t15.2023.10.01 val PER: 0.1314
2026-01-14 01:06:11,359: t15.2023.10.06 val PER: 0.0431
2026-01-14 01:06:11,359: t15.2023.10.08 val PER: 0.1962
2026-01-14 01:06:11,359: t15.2023.10.13 val PER: 0.1365
2026-01-14 01:06:11,359: t15.2023.10.15 val PER: 0.1015
2026-01-14 01:06:11,359: t15.2023.10.20 val PER: 0.1477
2026-01-14 01:06:11,360: t15.2023.10.22 val PER: 0.0802
2026-01-14 01:06:11,360: t15.2023.11.03 val PER: 0.1398
2026-01-14 01:06:11,360: t15.2023.11.04 val PER: 0.0171
2026-01-14 01:06:11,360: t15.2023.11.17 val PER: 0.0156
2026-01-14 01:06:11,360: t15.2023.11.19 val PER: 0.0180
2026-01-14 01:06:11,360: t15.2023.11.26 val PER: 0.0428
2026-01-14 01:06:11,360: t15.2023.12.03 val PER: 0.0431
2026-01-14 01:06:11,360: t15.2023.12.08 val PER: 0.0333
2026-01-14 01:06:11,360: t15.2023.12.10 val PER: 0.0302
2026-01-14 01:06:11,360: t15.2023.12.17 val PER: 0.0821
2026-01-14 01:06:11,360: t15.2023.12.29 val PER: 0.0666
2026-01-14 01:06:11,360: t15.2024.02.25 val PER: 0.0562
2026-01-14 01:06:11,360: t15.2024.03.08 val PER: 0.1607
2026-01-14 01:06:11,361: t15.2024.03.15 val PER: 0.1520
2026-01-14 01:06:11,361: t15.2024.03.17 val PER: 0.0683
2026-01-14 01:06:11,361: t15.2024.05.10 val PER: 0.1159
2026-01-14 01:06:11,361: t15.2024.06.14 val PER: 0.1151
2026-01-14 01:06:11,361: t15.2024.07.19 val PER: 0.1608
2026-01-14 01:06:11,361: t15.2024.07.21 val PER: 0.0579
2026-01-14 01:06:11,361: t15.2024.07.28 val PER: 0.0941
2026-01-14 01:06:11,361: t15.2025.01.10 val PER: 0.2562
2026-01-14 01:06:11,361: t15.2025.01.12 val PER: 0.0847
2026-01-14 01:06:11,361: t15.2025.03.14 val PER: 0.2973
2026-01-14 01:06:11,361: t15.2025.03.16 val PER: 0.1230
2026-01-14 01:06:11,361: t15.2025.03.30 val PER: 0.2184
2026-01-14 01:06:11,361: t15.2025.04.13 val PER: 0.1812
2026-01-14 01:06:11,515: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_93000
2026-01-14 01:06:29,145: Train batch 93200: loss: 0.03 grad norm: 3.29 time: 0.060
2026-01-14 01:06:46,750: Train batch 93400: loss: 0.15 grad norm: 9.48 time: 0.051
2026-01-14 01:06:55,502: Running test after training batch: 93500
2026-01-14 01:06:55,601: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:07:00,454: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:07:00,515: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:07:15,239: Val batch 93500: PER (avg): 0.1000 CTC Loss (avg): 19.6388 WER(5gram): 19.75% (n=256) time: 19.737
2026-01-14 01:07:15,240: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=13
2026-01-14 01:07:15,240: t15.2023.08.13 val PER: 0.0686
2026-01-14 01:07:15,241: t15.2023.08.18 val PER: 0.0662
2026-01-14 01:07:15,241: t15.2023.08.20 val PER: 0.0572
2026-01-14 01:07:15,241: t15.2023.08.25 val PER: 0.0723
2026-01-14 01:07:15,241: t15.2023.08.27 val PER: 0.1399
2026-01-14 01:07:15,241: t15.2023.09.01 val PER: 0.0406
2026-01-14 01:07:15,241: t15.2023.09.03 val PER: 0.1105
2026-01-14 01:07:15,241: t15.2023.09.24 val PER: 0.0825
2026-01-14 01:07:15,241: t15.2023.09.29 val PER: 0.0976
2026-01-14 01:07:15,241: t15.2023.10.01 val PER: 0.1321
2026-01-14 01:07:15,242: t15.2023.10.06 val PER: 0.0463
2026-01-14 01:07:15,242: t15.2023.10.08 val PER: 0.1935
2026-01-14 01:07:15,242: t15.2023.10.13 val PER: 0.1381
2026-01-14 01:07:15,242: t15.2023.10.15 val PER: 0.1002
2026-01-14 01:07:15,242: t15.2023.10.20 val PER: 0.1443
2026-01-14 01:07:15,242: t15.2023.10.22 val PER: 0.0780
2026-01-14 01:07:15,242: t15.2023.11.03 val PER: 0.1459
2026-01-14 01:07:15,242: t15.2023.11.04 val PER: 0.0171
2026-01-14 01:07:15,242: t15.2023.11.17 val PER: 0.0171
2026-01-14 01:07:15,242: t15.2023.11.19 val PER: 0.0180
2026-01-14 01:07:15,242: t15.2023.11.26 val PER: 0.0449
2026-01-14 01:07:15,243: t15.2023.12.03 val PER: 0.0410
2026-01-14 01:07:15,243: t15.2023.12.08 val PER: 0.0326
2026-01-14 01:07:15,243: t15.2023.12.10 val PER: 0.0329
2026-01-14 01:07:15,243: t15.2023.12.17 val PER: 0.0800
2026-01-14 01:07:15,243: t15.2023.12.29 val PER: 0.0707
2026-01-14 01:07:15,243: t15.2024.02.25 val PER: 0.0590
2026-01-14 01:07:15,243: t15.2024.03.08 val PER: 0.1593
2026-01-14 01:07:15,243: t15.2024.03.15 val PER: 0.1513
2026-01-14 01:07:15,243: t15.2024.03.17 val PER: 0.0690
2026-01-14 01:07:15,243: t15.2024.05.10 val PER: 0.1189
2026-01-14 01:07:15,243: t15.2024.06.14 val PER: 0.1120
2026-01-14 01:07:15,244: t15.2024.07.19 val PER: 0.1622
2026-01-14 01:07:15,244: t15.2024.07.21 val PER: 0.0559
2026-01-14 01:07:15,244: t15.2024.07.28 val PER: 0.0926
2026-01-14 01:07:15,244: t15.2025.01.10 val PER: 0.2507
2026-01-14 01:07:15,244: t15.2025.01.12 val PER: 0.0847
2026-01-14 01:07:15,244: t15.2025.03.14 val PER: 0.2929
2026-01-14 01:07:15,244: t15.2025.03.16 val PER: 0.1243
2026-01-14 01:07:15,244: t15.2025.03.30 val PER: 0.2161
2026-01-14 01:07:15,244: t15.2025.04.13 val PER: 0.1812
2026-01-14 01:07:15,398: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_93500
2026-01-14 01:07:24,054: Train batch 93600: loss: 0.19 grad norm: 13.77 time: 0.072
2026-01-14 01:07:41,274: Train batch 93800: loss: 0.09 grad norm: 17.86 time: 0.054
2026-01-14 01:07:59,023: Train batch 94000: loss: 0.15 grad norm: 11.73 time: 0.058
2026-01-14 01:07:59,023: Running test after training batch: 94000
2026-01-14 01:07:59,231: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:08:04,096: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:08:04,154: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:08:18,725: Val batch 94000: PER (avg): 0.0996 CTC Loss (avg): 19.7003 WER(5gram): 19.43% (n=256) time: 19.702
2026-01-14 01:08:18,726: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=13
2026-01-14 01:08:18,726: t15.2023.08.13 val PER: 0.0655
2026-01-14 01:08:18,726: t15.2023.08.18 val PER: 0.0637
2026-01-14 01:08:18,726: t15.2023.08.20 val PER: 0.0572
2026-01-14 01:08:18,726: t15.2023.08.25 val PER: 0.0738
2026-01-14 01:08:18,726: t15.2023.08.27 val PER: 0.1431
2026-01-14 01:08:18,726: t15.2023.09.01 val PER: 0.0422
2026-01-14 01:08:18,727: t15.2023.09.03 val PER: 0.1128
2026-01-14 01:08:18,727: t15.2023.09.24 val PER: 0.0862
2026-01-14 01:08:18,727: t15.2023.09.29 val PER: 0.0964
2026-01-14 01:08:18,727: t15.2023.10.01 val PER: 0.1295
2026-01-14 01:08:18,727: t15.2023.10.06 val PER: 0.0431
2026-01-14 01:08:18,727: t15.2023.10.08 val PER: 0.1881
2026-01-14 01:08:18,727: t15.2023.10.13 val PER: 0.1350
2026-01-14 01:08:18,727: t15.2023.10.15 val PER: 0.1009
2026-01-14 01:08:18,727: t15.2023.10.20 val PER: 0.1577
2026-01-14 01:08:18,727: t15.2023.10.22 val PER: 0.0780
2026-01-14 01:08:18,728: t15.2023.11.03 val PER: 0.1377
2026-01-14 01:08:18,728: t15.2023.11.04 val PER: 0.0171
2026-01-14 01:08:18,728: t15.2023.11.17 val PER: 0.0156
2026-01-14 01:08:18,728: t15.2023.11.19 val PER: 0.0180
2026-01-14 01:08:18,728: t15.2023.11.26 val PER: 0.0413
2026-01-14 01:08:18,728: t15.2023.12.03 val PER: 0.0420
2026-01-14 01:08:18,728: t15.2023.12.08 val PER: 0.0333
2026-01-14 01:08:18,728: t15.2023.12.10 val PER: 0.0329
2026-01-14 01:08:18,728: t15.2023.12.17 val PER: 0.0821
2026-01-14 01:08:18,728: t15.2023.12.29 val PER: 0.0673
2026-01-14 01:08:18,728: t15.2024.02.25 val PER: 0.0604
2026-01-14 01:08:18,729: t15.2024.03.08 val PER: 0.1593
2026-01-14 01:08:18,729: t15.2024.03.15 val PER: 0.1545
2026-01-14 01:08:18,729: t15.2024.03.17 val PER: 0.0711
2026-01-14 01:08:18,729: t15.2024.05.10 val PER: 0.1218
2026-01-14 01:08:18,729: t15.2024.06.14 val PER: 0.1104
2026-01-14 01:08:18,729: t15.2024.07.19 val PER: 0.1589
2026-01-14 01:08:18,729: t15.2024.07.21 val PER: 0.0559
2026-01-14 01:08:18,729: t15.2024.07.28 val PER: 0.0919
2026-01-14 01:08:18,729: t15.2025.01.10 val PER: 0.2521
2026-01-14 01:08:18,729: t15.2025.01.12 val PER: 0.0855
2026-01-14 01:08:18,729: t15.2025.03.14 val PER: 0.2988
2026-01-14 01:08:18,729: t15.2025.03.16 val PER: 0.1204
2026-01-14 01:08:18,729: t15.2025.03.30 val PER: 0.2195
2026-01-14 01:08:18,729: t15.2025.04.13 val PER: 0.1812
2026-01-14 01:08:18,879: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_94000
2026-01-14 01:08:36,165: Train batch 94200: loss: 0.06 grad norm: 6.09 time: 0.074
2026-01-14 01:08:53,923: Train batch 94400: loss: 0.11 grad norm: 12.16 time: 0.062
2026-01-14 01:09:02,778: Running test after training batch: 94500
2026-01-14 01:09:02,939: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:09:07,883: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:09:07,948: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:09:22,479: Val batch 94500: PER (avg): 0.1007 CTC Loss (avg): 19.8084 WER(5gram): 19.17% (n=256) time: 19.700
2026-01-14 01:09:22,479: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-14 01:09:22,479: t15.2023.08.13 val PER: 0.0696
2026-01-14 01:09:22,480: t15.2023.08.18 val PER: 0.0629
2026-01-14 01:09:22,480: t15.2023.08.20 val PER: 0.0564
2026-01-14 01:09:22,480: t15.2023.08.25 val PER: 0.0738
2026-01-14 01:09:22,480: t15.2023.08.27 val PER: 0.1447
2026-01-14 01:09:22,480: t15.2023.09.01 val PER: 0.0398
2026-01-14 01:09:22,480: t15.2023.09.03 val PER: 0.1140
2026-01-14 01:09:22,480: t15.2023.09.24 val PER: 0.0886
2026-01-14 01:09:22,480: t15.2023.09.29 val PER: 0.0957
2026-01-14 01:09:22,480: t15.2023.10.01 val PER: 0.1281
2026-01-14 01:09:22,480: t15.2023.10.06 val PER: 0.0398
2026-01-14 01:09:22,481: t15.2023.10.08 val PER: 0.1962
2026-01-14 01:09:22,481: t15.2023.10.13 val PER: 0.1389
2026-01-14 01:09:22,481: t15.2023.10.15 val PER: 0.1028
2026-01-14 01:09:22,481: t15.2023.10.20 val PER: 0.1577
2026-01-14 01:09:22,481: t15.2023.10.22 val PER: 0.0813
2026-01-14 01:09:22,481: t15.2023.11.03 val PER: 0.1384
2026-01-14 01:09:22,481: t15.2023.11.04 val PER: 0.0205
2026-01-14 01:09:22,481: t15.2023.11.17 val PER: 0.0171
2026-01-14 01:09:22,481: t15.2023.11.19 val PER: 0.0180
2026-01-14 01:09:22,481: t15.2023.11.26 val PER: 0.0449
2026-01-14 01:09:22,481: t15.2023.12.03 val PER: 0.0441
2026-01-14 01:09:22,482: t15.2023.12.08 val PER: 0.0333
2026-01-14 01:09:22,482: t15.2023.12.10 val PER: 0.0329
2026-01-14 01:09:22,482: t15.2023.12.17 val PER: 0.0780
2026-01-14 01:09:22,482: t15.2023.12.29 val PER: 0.0721
2026-01-14 01:09:22,482: t15.2024.02.25 val PER: 0.0674
2026-01-14 01:09:22,482: t15.2024.03.08 val PER: 0.1650
2026-01-14 01:09:22,482: t15.2024.03.15 val PER: 0.1576
2026-01-14 01:09:22,482: t15.2024.03.17 val PER: 0.0697
2026-01-14 01:09:22,482: t15.2024.05.10 val PER: 0.1204
2026-01-14 01:09:22,482: t15.2024.06.14 val PER: 0.1104
2026-01-14 01:09:22,482: t15.2024.07.19 val PER: 0.1595
2026-01-14 01:09:22,482: t15.2024.07.21 val PER: 0.0572
2026-01-14 01:09:22,482: t15.2024.07.28 val PER: 0.0897
2026-01-14 01:09:22,482: t15.2025.01.10 val PER: 0.2548
2026-01-14 01:09:22,483: t15.2025.01.12 val PER: 0.0878
2026-01-14 01:09:22,483: t15.2025.03.14 val PER: 0.2988
2026-01-14 01:09:22,483: t15.2025.03.16 val PER: 0.1257
2026-01-14 01:09:22,483: t15.2025.03.30 val PER: 0.2172
2026-01-14 01:09:22,483: t15.2025.04.13 val PER: 0.1854
2026-01-14 01:09:22,638: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_94500
2026-01-14 01:09:31,294: Train batch 94600: loss: 0.21 grad norm: 16.81 time: 0.053
2026-01-14 01:09:48,528: Train batch 94800: loss: 0.15 grad norm: 11.14 time: 0.051
2026-01-14 01:10:06,053: Train batch 95000: loss: 0.06 grad norm: 5.75 time: 0.058
2026-01-14 01:10:06,053: Running test after training batch: 95000
2026-01-14 01:10:06,206: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:10:11,204: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:10:11,266: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:10:25,946: Val batch 95000: PER (avg): 0.1002 CTC Loss (avg): 19.5837 WER(5gram): 19.49% (n=256) time: 19.893
2026-01-14 01:10:25,947: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=14
2026-01-14 01:10:25,947: t15.2023.08.13 val PER: 0.0686
2026-01-14 01:10:25,947: t15.2023.08.18 val PER: 0.0612
2026-01-14 01:10:25,947: t15.2023.08.20 val PER: 0.0572
2026-01-14 01:10:25,947: t15.2023.08.25 val PER: 0.0723
2026-01-14 01:10:25,947: t15.2023.08.27 val PER: 0.1495
2026-01-14 01:10:25,947: t15.2023.09.01 val PER: 0.0390
2026-01-14 01:10:25,947: t15.2023.09.03 val PER: 0.1176
2026-01-14 01:10:25,947: t15.2023.09.24 val PER: 0.0837
2026-01-14 01:10:25,948: t15.2023.09.29 val PER: 0.0970
2026-01-14 01:10:25,948: t15.2023.10.01 val PER: 0.1281
2026-01-14 01:10:25,948: t15.2023.10.06 val PER: 0.0409
2026-01-14 01:10:25,948: t15.2023.10.08 val PER: 0.1962
2026-01-14 01:10:25,948: t15.2023.10.13 val PER: 0.1373
2026-01-14 01:10:25,948: t15.2023.10.15 val PER: 0.1002
2026-01-14 01:10:25,948: t15.2023.10.20 val PER: 0.1544
2026-01-14 01:10:25,948: t15.2023.10.22 val PER: 0.0802
2026-01-14 01:10:25,948: t15.2023.11.03 val PER: 0.1398
2026-01-14 01:10:25,948: t15.2023.11.04 val PER: 0.0171
2026-01-14 01:10:25,948: t15.2023.11.17 val PER: 0.0187
2026-01-14 01:10:25,948: t15.2023.11.19 val PER: 0.0200
2026-01-14 01:10:25,949: t15.2023.11.26 val PER: 0.0442
2026-01-14 01:10:25,949: t15.2023.12.03 val PER: 0.0441
2026-01-14 01:10:25,949: t15.2023.12.08 val PER: 0.0333
2026-01-14 01:10:25,949: t15.2023.12.10 val PER: 0.0342
2026-01-14 01:10:25,949: t15.2023.12.17 val PER: 0.0863
2026-01-14 01:10:25,949: t15.2023.12.29 val PER: 0.0693
2026-01-14 01:10:25,949: t15.2024.02.25 val PER: 0.0604
2026-01-14 01:10:25,949: t15.2024.03.08 val PER: 0.1607
2026-01-14 01:10:25,949: t15.2024.03.15 val PER: 0.1532
2026-01-14 01:10:25,949: t15.2024.03.17 val PER: 0.0669
2026-01-14 01:10:25,949: t15.2024.05.10 val PER: 0.1204
2026-01-14 01:10:25,949: t15.2024.06.14 val PER: 0.1120
2026-01-14 01:10:25,949: t15.2024.07.19 val PER: 0.1562
2026-01-14 01:10:25,949: t15.2024.07.21 val PER: 0.0572
2026-01-14 01:10:25,950: t15.2024.07.28 val PER: 0.0890
2026-01-14 01:10:25,950: t15.2025.01.10 val PER: 0.2576
2026-01-14 01:10:25,950: t15.2025.01.12 val PER: 0.0870
2026-01-14 01:10:25,950: t15.2025.03.14 val PER: 0.3003
2026-01-14 01:10:25,950: t15.2025.03.16 val PER: 0.1243
2026-01-14 01:10:25,950: t15.2025.03.30 val PER: 0.2207
2026-01-14 01:10:25,950: t15.2025.04.13 val PER: 0.1797
2026-01-14 01:10:26,105: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_95000
2026-01-14 01:10:43,848: Train batch 95200: loss: 0.48 grad norm: 27.38 time: 0.060
2026-01-14 01:11:01,699: Train batch 95400: loss: 0.03 grad norm: 4.63 time: 0.055
2026-01-14 01:11:10,427: Running test after training batch: 95500
2026-01-14 01:11:10,568: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:11:15,464: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:11:15,531: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:11:32,096: Val batch 95500: PER (avg): 0.1010 CTC Loss (avg): 19.5697 WER(5gram): 19.69% (n=256) time: 21.668
2026-01-14 01:11:32,096: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=14
2026-01-14 01:11:32,096: t15.2023.08.13 val PER: 0.0676
2026-01-14 01:11:32,096: t15.2023.08.18 val PER: 0.0671
2026-01-14 01:11:32,096: t15.2023.08.20 val PER: 0.0588
2026-01-14 01:11:32,097: t15.2023.08.25 val PER: 0.0693
2026-01-14 01:11:32,097: t15.2023.08.27 val PER: 0.1495
2026-01-14 01:11:32,097: t15.2023.09.01 val PER: 0.0390
2026-01-14 01:11:32,097: t15.2023.09.03 val PER: 0.1152
2026-01-14 01:11:32,097: t15.2023.09.24 val PER: 0.0825
2026-01-14 01:11:32,097: t15.2023.09.29 val PER: 0.0976
2026-01-14 01:11:32,097: t15.2023.10.01 val PER: 0.1301
2026-01-14 01:11:32,097: t15.2023.10.06 val PER: 0.0441
2026-01-14 01:11:32,097: t15.2023.10.08 val PER: 0.1989
2026-01-14 01:11:32,097: t15.2023.10.13 val PER: 0.1412
2026-01-14 01:11:32,097: t15.2023.10.15 val PER: 0.1028
2026-01-14 01:11:32,097: t15.2023.10.20 val PER: 0.1544
2026-01-14 01:11:32,097: t15.2023.10.22 val PER: 0.0813
2026-01-14 01:11:32,097: t15.2023.11.03 val PER: 0.1411
2026-01-14 01:11:32,097: t15.2023.11.04 val PER: 0.0171
2026-01-14 01:11:32,098: t15.2023.11.17 val PER: 0.0156
2026-01-14 01:11:32,098: t15.2023.11.19 val PER: 0.0180
2026-01-14 01:11:32,098: t15.2023.11.26 val PER: 0.0449
2026-01-14 01:11:32,098: t15.2023.12.03 val PER: 0.0452
2026-01-14 01:11:32,098: t15.2023.12.08 val PER: 0.0360
2026-01-14 01:11:32,098: t15.2023.12.10 val PER: 0.0329
2026-01-14 01:11:32,098: t15.2023.12.17 val PER: 0.0842
2026-01-14 01:11:32,098: t15.2023.12.29 val PER: 0.0659
2026-01-14 01:11:32,099: t15.2024.02.25 val PER: 0.0604
2026-01-14 01:11:32,099: t15.2024.03.08 val PER: 0.1622
2026-01-14 01:11:32,099: t15.2024.03.15 val PER: 0.1532
2026-01-14 01:11:32,099: t15.2024.03.17 val PER: 0.0697
2026-01-14 01:11:32,099: t15.2024.05.10 val PER: 0.1263
2026-01-14 01:11:32,099: t15.2024.06.14 val PER: 0.1120
2026-01-14 01:11:32,099: t15.2024.07.19 val PER: 0.1543
2026-01-14 01:11:32,099: t15.2024.07.21 val PER: 0.0559
2026-01-14 01:11:32,099: t15.2024.07.28 val PER: 0.0897
2026-01-14 01:11:32,099: t15.2025.01.10 val PER: 0.2603
2026-01-14 01:11:32,099: t15.2025.01.12 val PER: 0.0931
2026-01-14 01:11:32,099: t15.2025.03.14 val PER: 0.3018
2026-01-14 01:11:32,099: t15.2025.03.16 val PER: 0.1243
2026-01-14 01:11:32,099: t15.2025.03.30 val PER: 0.2172
2026-01-14 01:11:32,099: t15.2025.04.13 val PER: 0.1840
2026-01-14 01:11:32,252: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_95500
2026-01-14 01:11:41,559: Train batch 95600: loss: 0.25 grad norm: 15.10 time: 0.074
2026-01-14 01:11:58,988: Train batch 95800: loss: 0.35 grad norm: 18.48 time: 0.054
2026-01-14 01:12:16,361: Train batch 96000: loss: 0.09 grad norm: 6.44 time: 0.070
2026-01-14 01:12:16,361: Running test after training batch: 96000
2026-01-14 01:12:16,486: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:12:21,353: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:12:21,416: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:12:36,607: Val batch 96000: PER (avg): 0.1007 CTC Loss (avg): 19.5703 WER(5gram): 19.36% (n=256) time: 20.245
2026-01-14 01:12:36,607: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=14
2026-01-14 01:12:36,607: t15.2023.08.13 val PER: 0.0686
2026-01-14 01:12:36,607: t15.2023.08.18 val PER: 0.0645
2026-01-14 01:12:36,607: t15.2023.08.20 val PER: 0.0572
2026-01-14 01:12:36,607: t15.2023.08.25 val PER: 0.0693
2026-01-14 01:12:36,608: t15.2023.08.27 val PER: 0.1431
2026-01-14 01:12:36,608: t15.2023.09.01 val PER: 0.0381
2026-01-14 01:12:36,608: t15.2023.09.03 val PER: 0.1164
2026-01-14 01:12:36,608: t15.2023.09.24 val PER: 0.0825
2026-01-14 01:12:36,608: t15.2023.09.29 val PER: 0.0957
2026-01-14 01:12:36,608: t15.2023.10.01 val PER: 0.1301
2026-01-14 01:12:36,608: t15.2023.10.06 val PER: 0.0441
2026-01-14 01:12:36,608: t15.2023.10.08 val PER: 0.1949
2026-01-14 01:12:36,608: t15.2023.10.13 val PER: 0.1381
2026-01-14 01:12:36,608: t15.2023.10.15 val PER: 0.1015
2026-01-14 01:12:36,608: t15.2023.10.20 val PER: 0.1577
2026-01-14 01:12:36,609: t15.2023.10.22 val PER: 0.0802
2026-01-14 01:12:36,609: t15.2023.11.03 val PER: 0.1445
2026-01-14 01:12:36,609: t15.2023.11.04 val PER: 0.0171
2026-01-14 01:12:36,609: t15.2023.11.17 val PER: 0.0171
2026-01-14 01:12:36,609: t15.2023.11.19 val PER: 0.0180
2026-01-14 01:12:36,609: t15.2023.11.26 val PER: 0.0435
2026-01-14 01:12:36,609: t15.2023.12.03 val PER: 0.0431
2026-01-14 01:12:36,609: t15.2023.12.08 val PER: 0.0320
2026-01-14 01:12:36,609: t15.2023.12.10 val PER: 0.0329
2026-01-14 01:12:36,609: t15.2023.12.17 val PER: 0.0832
2026-01-14 01:12:36,609: t15.2023.12.29 val PER: 0.0673
2026-01-14 01:12:36,609: t15.2024.02.25 val PER: 0.0618
2026-01-14 01:12:36,609: t15.2024.03.08 val PER: 0.1593
2026-01-14 01:12:36,609: t15.2024.03.15 val PER: 0.1538
2026-01-14 01:12:36,609: t15.2024.03.17 val PER: 0.0683
2026-01-14 01:12:36,609: t15.2024.05.10 val PER: 0.1248
2026-01-14 01:12:36,609: t15.2024.06.14 val PER: 0.1088
2026-01-14 01:12:36,610: t15.2024.07.19 val PER: 0.1602
2026-01-14 01:12:36,610: t15.2024.07.21 val PER: 0.0593
2026-01-14 01:12:36,610: t15.2024.07.28 val PER: 0.0941
2026-01-14 01:12:36,610: t15.2025.01.10 val PER: 0.2521
2026-01-14 01:12:36,610: t15.2025.01.12 val PER: 0.0916
2026-01-14 01:12:36,610: t15.2025.03.14 val PER: 0.2988
2026-01-14 01:12:36,610: t15.2025.03.16 val PER: 0.1230
2026-01-14 01:12:36,610: t15.2025.03.30 val PER: 0.2230
2026-01-14 01:12:36,610: t15.2025.04.13 val PER: 0.1840
2026-01-14 01:12:36,764: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_96000
2026-01-14 01:12:54,369: Train batch 96200: loss: 0.14 grad norm: 12.85 time: 0.055
2026-01-14 01:13:11,613: Train batch 96400: loss: 0.01 grad norm: 1.69 time: 0.065
2026-01-14 01:13:20,242: Running test after training batch: 96500
2026-01-14 01:13:20,413: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:13:25,685: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:13:25,757: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-14 01:13:42,677: Val batch 96500: PER (avg): 0.1005 CTC Loss (avg): 19.6288 WER(5gram): 19.49% (n=256) time: 22.434
2026-01-14 01:13:42,677: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=14
2026-01-14 01:13:42,677: t15.2023.08.13 val PER: 0.0676
2026-01-14 01:13:42,677: t15.2023.08.18 val PER: 0.0662
2026-01-14 01:13:42,677: t15.2023.08.20 val PER: 0.0572
2026-01-14 01:13:42,677: t15.2023.08.25 val PER: 0.0753
2026-01-14 01:13:42,677: t15.2023.08.27 val PER: 0.1415
2026-01-14 01:13:42,678: t15.2023.09.01 val PER: 0.0406
2026-01-14 01:13:42,678: t15.2023.09.03 val PER: 0.1152
2026-01-14 01:13:42,678: t15.2023.09.24 val PER: 0.0862
2026-01-14 01:13:42,678: t15.2023.09.29 val PER: 0.0964
2026-01-14 01:13:42,678: t15.2023.10.01 val PER: 0.1314
2026-01-14 01:13:42,678: t15.2023.10.06 val PER: 0.0452
2026-01-14 01:13:42,678: t15.2023.10.08 val PER: 0.1935
2026-01-14 01:13:42,678: t15.2023.10.13 val PER: 0.1381
2026-01-14 01:13:42,679: t15.2023.10.15 val PER: 0.1002
2026-01-14 01:13:42,679: t15.2023.10.20 val PER: 0.1611
2026-01-14 01:13:42,679: t15.2023.10.22 val PER: 0.0791
2026-01-14 01:13:42,679: t15.2023.11.03 val PER: 0.1425
2026-01-14 01:13:42,679: t15.2023.11.04 val PER: 0.0171
2026-01-14 01:13:42,679: t15.2023.11.17 val PER: 0.0187
2026-01-14 01:13:42,679: t15.2023.11.19 val PER: 0.0180
2026-01-14 01:13:42,679: t15.2023.11.26 val PER: 0.0420
2026-01-14 01:13:42,679: t15.2023.12.03 val PER: 0.0431
2026-01-14 01:13:42,679: t15.2023.12.08 val PER: 0.0340
2026-01-14 01:13:42,679: t15.2023.12.10 val PER: 0.0302
2026-01-14 01:13:42,679: t15.2023.12.17 val PER: 0.0832
2026-01-14 01:13:42,679: t15.2023.12.29 val PER: 0.0679
2026-01-14 01:13:42,679: t15.2024.02.25 val PER: 0.0604
2026-01-14 01:13:42,680: t15.2024.03.08 val PER: 0.1536
2026-01-14 01:13:42,680: t15.2024.03.15 val PER: 0.1570
2026-01-14 01:13:42,680: t15.2024.03.17 val PER: 0.0683
2026-01-14 01:13:42,680: t15.2024.05.10 val PER: 0.1233
2026-01-14 01:13:42,680: t15.2024.06.14 val PER: 0.1120
2026-01-14 01:13:42,680: t15.2024.07.19 val PER: 0.1582
2026-01-14 01:13:42,680: t15.2024.07.21 val PER: 0.0572
2026-01-14 01:13:42,680: t15.2024.07.28 val PER: 0.0897
2026-01-14 01:13:42,680: t15.2025.01.10 val PER: 0.2521
2026-01-14 01:13:42,680: t15.2025.01.12 val PER: 0.0908
2026-01-14 01:13:42,680: t15.2025.03.14 val PER: 0.3003
2026-01-14 01:13:42,680: t15.2025.03.16 val PER: 0.1217
2026-01-14 01:13:42,680: t15.2025.03.30 val PER: 0.2207
2026-01-14 01:13:42,680: t15.2025.04.13 val PER: 0.1826
2026-01-14 01:13:42,836: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_96500
2026-01-14 01:13:51,385: Train batch 96600: loss: 0.08 grad norm: 5.78 time: 0.075
2026-01-14 01:14:08,573: Train batch 96800: loss: 0.06 grad norm: 6.16 time: 0.062
2026-01-14 01:14:26,157: Train batch 97000: loss: 0.30 grad norm: 15.53 time: 0.060
2026-01-14 01:14:26,157: Running test after training batch: 97000
2026-01-14 01:14:26,252: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:14:31,147: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:14:31,227: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:14:48,065: Val batch 97000: PER (avg): 0.1009 CTC Loss (avg): 19.7028 WER(5gram): 19.43% (n=256) time: 21.908
2026-01-14 01:14:48,065: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=13
2026-01-14 01:14:48,065: t15.2023.08.13 val PER: 0.0665
2026-01-14 01:14:48,066: t15.2023.08.18 val PER: 0.0662
2026-01-14 01:14:48,066: t15.2023.08.20 val PER: 0.0564
2026-01-14 01:14:48,066: t15.2023.08.25 val PER: 0.0738
2026-01-14 01:14:48,066: t15.2023.08.27 val PER: 0.1479
2026-01-14 01:14:48,066: t15.2023.09.01 val PER: 0.0390
2026-01-14 01:14:48,066: t15.2023.09.03 val PER: 0.1152
2026-01-14 01:14:48,066: t15.2023.09.24 val PER: 0.0862
2026-01-14 01:14:48,066: t15.2023.09.29 val PER: 0.0976
2026-01-14 01:14:48,067: t15.2023.10.01 val PER: 0.1288
2026-01-14 01:14:48,067: t15.2023.10.06 val PER: 0.0452
2026-01-14 01:14:48,067: t15.2023.10.08 val PER: 0.1922
2026-01-14 01:14:48,067: t15.2023.10.13 val PER: 0.1381
2026-01-14 01:14:48,067: t15.2023.10.15 val PER: 0.1002
2026-01-14 01:14:48,067: t15.2023.10.20 val PER: 0.1577
2026-01-14 01:14:48,067: t15.2023.10.22 val PER: 0.0791
2026-01-14 01:14:48,067: t15.2023.11.03 val PER: 0.1411
2026-01-14 01:14:48,067: t15.2023.11.04 val PER: 0.0205
2026-01-14 01:14:48,067: t15.2023.11.17 val PER: 0.0171
2026-01-14 01:14:48,068: t15.2023.11.19 val PER: 0.0220
2026-01-14 01:14:48,068: t15.2023.11.26 val PER: 0.0406
2026-01-14 01:14:48,068: t15.2023.12.03 val PER: 0.0473
2026-01-14 01:14:48,068: t15.2023.12.08 val PER: 0.0346
2026-01-14 01:14:48,068: t15.2023.12.10 val PER: 0.0329
2026-01-14 01:14:48,068: t15.2023.12.17 val PER: 0.0832
2026-01-14 01:14:48,068: t15.2023.12.29 val PER: 0.0707
2026-01-14 01:14:48,068: t15.2024.02.25 val PER: 0.0604
2026-01-14 01:14:48,068: t15.2024.03.08 val PER: 0.1565
2026-01-14 01:14:48,068: t15.2024.03.15 val PER: 0.1557
2026-01-14 01:14:48,069: t15.2024.03.17 val PER: 0.0669
2026-01-14 01:14:48,069: t15.2024.05.10 val PER: 0.1293
2026-01-14 01:14:48,069: t15.2024.06.14 val PER: 0.1104
2026-01-14 01:14:48,069: t15.2024.07.19 val PER: 0.1582
2026-01-14 01:14:48,069: t15.2024.07.21 val PER: 0.0586
2026-01-14 01:14:48,069: t15.2024.07.28 val PER: 0.0934
2026-01-14 01:14:48,069: t15.2025.01.10 val PER: 0.2562
2026-01-14 01:14:48,069: t15.2025.01.12 val PER: 0.0901
2026-01-14 01:14:48,069: t15.2025.03.14 val PER: 0.2988
2026-01-14 01:14:48,069: t15.2025.03.16 val PER: 0.1257
2026-01-14 01:14:48,069: t15.2025.03.30 val PER: 0.2207
2026-01-14 01:14:48,070: t15.2025.04.13 val PER: 0.1812
2026-01-14 01:14:48,221: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_97000
2026-01-14 01:15:05,850: Train batch 97200: loss: 0.36 grad norm: 26.22 time: 0.066
2026-01-14 01:15:23,268: Train batch 97400: loss: 0.07 grad norm: 8.09 time: 0.079
2026-01-14 01:15:31,951: Running test after training batch: 97500
2026-01-14 01:15:32,054: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:15:36,978: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:15:37,052: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:15:53,714: Val batch 97500: PER (avg): 0.0998 CTC Loss (avg): 19.5555 WER(5gram): 19.36% (n=256) time: 21.763
2026-01-14 01:15:53,715: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=13
2026-01-14 01:15:53,715: t15.2023.08.13 val PER: 0.0676
2026-01-14 01:15:53,715: t15.2023.08.18 val PER: 0.0645
2026-01-14 01:15:53,715: t15.2023.08.20 val PER: 0.0540
2026-01-14 01:15:53,716: t15.2023.08.25 val PER: 0.0753
2026-01-14 01:15:53,716: t15.2023.08.27 val PER: 0.1447
2026-01-14 01:15:53,716: t15.2023.09.01 val PER: 0.0414
2026-01-14 01:15:53,716: t15.2023.09.03 val PER: 0.1128
2026-01-14 01:15:53,716: t15.2023.09.24 val PER: 0.0813
2026-01-14 01:15:53,716: t15.2023.09.29 val PER: 0.0957
2026-01-14 01:15:53,716: t15.2023.10.01 val PER: 0.1301
2026-01-14 01:15:53,716: t15.2023.10.06 val PER: 0.0420
2026-01-14 01:15:53,716: t15.2023.10.08 val PER: 0.1935
2026-01-14 01:15:53,716: t15.2023.10.13 val PER: 0.1350
2026-01-14 01:15:53,717: t15.2023.10.15 val PER: 0.1009
2026-01-14 01:15:53,717: t15.2023.10.20 val PER: 0.1611
2026-01-14 01:15:53,717: t15.2023.10.22 val PER: 0.0791
2026-01-14 01:15:53,717: t15.2023.11.03 val PER: 0.1384
2026-01-14 01:15:53,717: t15.2023.11.04 val PER: 0.0205
2026-01-14 01:15:53,717: t15.2023.11.17 val PER: 0.0156
2026-01-14 01:15:53,717: t15.2023.11.19 val PER: 0.0200
2026-01-14 01:15:53,717: t15.2023.11.26 val PER: 0.0420
2026-01-14 01:15:53,717: t15.2023.12.03 val PER: 0.0441
2026-01-14 01:15:53,717: t15.2023.12.08 val PER: 0.0346
2026-01-14 01:15:53,717: t15.2023.12.10 val PER: 0.0342
2026-01-14 01:15:53,717: t15.2023.12.17 val PER: 0.0800
2026-01-14 01:15:53,718: t15.2023.12.29 val PER: 0.0693
2026-01-14 01:15:53,718: t15.2024.02.25 val PER: 0.0618
2026-01-14 01:15:53,718: t15.2024.03.08 val PER: 0.1550
2026-01-14 01:15:53,718: t15.2024.03.15 val PER: 0.1532
2026-01-14 01:15:53,718: t15.2024.03.17 val PER: 0.0669
2026-01-14 01:15:53,718: t15.2024.05.10 val PER: 0.1218
2026-01-14 01:15:53,718: t15.2024.06.14 val PER: 0.1120
2026-01-14 01:15:53,718: t15.2024.07.19 val PER: 0.1582
2026-01-14 01:15:53,718: t15.2024.07.21 val PER: 0.0593
2026-01-14 01:15:53,718: t15.2024.07.28 val PER: 0.0904
2026-01-14 01:15:53,718: t15.2025.01.10 val PER: 0.2562
2026-01-14 01:15:53,718: t15.2025.01.12 val PER: 0.0901
2026-01-14 01:15:53,718: t15.2025.03.14 val PER: 0.2929
2026-01-14 01:15:53,718: t15.2025.03.16 val PER: 0.1243
2026-01-14 01:15:53,718: t15.2025.03.30 val PER: 0.2172
2026-01-14 01:15:53,718: t15.2025.04.13 val PER: 0.1783
2026-01-14 01:15:53,885: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_97500
2026-01-14 01:16:02,556: Train batch 97600: loss: 0.06 grad norm: 3.74 time: 0.060
2026-01-14 01:16:20,061: Train batch 97800: loss: 0.10 grad norm: 11.42 time: 0.046
2026-01-14 01:16:37,763: Train batch 98000: loss: 0.05 grad norm: 4.72 time: 0.043
2026-01-14 01:16:37,763: Running test after training batch: 98000
2026-01-14 01:16:37,889: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:16:42,779: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:16:42,860: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:16:59,589: Val batch 98000: PER (avg): 0.0997 CTC Loss (avg): 19.4827 WER(5gram): 18.97% (n=256) time: 21.825
2026-01-14 01:16:59,589: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=14
2026-01-14 01:16:59,590: t15.2023.08.13 val PER: 0.0686
2026-01-14 01:16:59,590: t15.2023.08.18 val PER: 0.0645
2026-01-14 01:16:59,590: t15.2023.08.20 val PER: 0.0564
2026-01-14 01:16:59,590: t15.2023.08.25 val PER: 0.0723
2026-01-14 01:16:59,590: t15.2023.08.27 val PER: 0.1383
2026-01-14 01:16:59,590: t15.2023.09.01 val PER: 0.0390
2026-01-14 01:16:59,590: t15.2023.09.03 val PER: 0.1140
2026-01-14 01:16:59,590: t15.2023.09.24 val PER: 0.0837
2026-01-14 01:16:59,590: t15.2023.09.29 val PER: 0.0970
2026-01-14 01:16:59,590: t15.2023.10.01 val PER: 0.1281
2026-01-14 01:16:59,590: t15.2023.10.06 val PER: 0.0420
2026-01-14 01:16:59,591: t15.2023.10.08 val PER: 0.1908
2026-01-14 01:16:59,591: t15.2023.10.13 val PER: 0.1342
2026-01-14 01:16:59,591: t15.2023.10.15 val PER: 0.1015
2026-01-14 01:16:59,591: t15.2023.10.20 val PER: 0.1577
2026-01-14 01:16:59,591: t15.2023.10.22 val PER: 0.0780
2026-01-14 01:16:59,591: t15.2023.11.03 val PER: 0.1404
2026-01-14 01:16:59,591: t15.2023.11.04 val PER: 0.0205
2026-01-14 01:16:59,591: t15.2023.11.17 val PER: 0.0156
2026-01-14 01:16:59,591: t15.2023.11.19 val PER: 0.0180
2026-01-14 01:16:59,591: t15.2023.11.26 val PER: 0.0406
2026-01-14 01:16:59,591: t15.2023.12.03 val PER: 0.0462
2026-01-14 01:16:59,591: t15.2023.12.08 val PER: 0.0360
2026-01-14 01:16:59,591: t15.2023.12.10 val PER: 0.0315
2026-01-14 01:16:59,592: t15.2023.12.17 val PER: 0.0811
2026-01-14 01:16:59,592: t15.2023.12.29 val PER: 0.0673
2026-01-14 01:16:59,592: t15.2024.02.25 val PER: 0.0604
2026-01-14 01:16:59,592: t15.2024.03.08 val PER: 0.1522
2026-01-14 01:16:59,592: t15.2024.03.15 val PER: 0.1513
2026-01-14 01:16:59,592: t15.2024.03.17 val PER: 0.0642
2026-01-14 01:16:59,592: t15.2024.05.10 val PER: 0.1233
2026-01-14 01:16:59,592: t15.2024.06.14 val PER: 0.1120
2026-01-14 01:16:59,592: t15.2024.07.19 val PER: 0.1575
2026-01-14 01:16:59,592: t15.2024.07.21 val PER: 0.0579
2026-01-14 01:16:59,592: t15.2024.07.28 val PER: 0.0912
2026-01-14 01:16:59,592: t15.2025.01.10 val PER: 0.2548
2026-01-14 01:16:59,592: t15.2025.01.12 val PER: 0.0924
2026-01-14 01:16:59,592: t15.2025.03.14 val PER: 0.3033
2026-01-14 01:16:59,592: t15.2025.03.16 val PER: 0.1217
2026-01-14 01:16:59,593: t15.2025.03.30 val PER: 0.2253
2026-01-14 01:16:59,593: t15.2025.04.13 val PER: 0.1783
2026-01-14 01:16:59,748: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_98000
2026-01-14 01:17:17,508: Train batch 98200: loss: 1.07 grad norm: 10.79 time: 0.069
2026-01-14 01:17:35,440: Train batch 98400: loss: 0.12 grad norm: 7.99 time: 0.053
2026-01-14 01:17:44,551: Running test after training batch: 98500
2026-01-14 01:17:44,666: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:17:49,518: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:17:49,579: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:18:04,952: Val batch 98500: PER (avg): 0.0999 CTC Loss (avg): 19.4295 WER(5gram): 18.97% (n=256) time: 20.401
2026-01-14 01:18:04,952: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-14 01:18:04,953: t15.2023.08.13 val PER: 0.0665
2026-01-14 01:18:04,953: t15.2023.08.18 val PER: 0.0687
2026-01-14 01:18:04,953: t15.2023.08.20 val PER: 0.0564
2026-01-14 01:18:04,953: t15.2023.08.25 val PER: 0.0723
2026-01-14 01:18:04,953: t15.2023.08.27 val PER: 0.1479
2026-01-14 01:18:04,953: t15.2023.09.01 val PER: 0.0381
2026-01-14 01:18:04,953: t15.2023.09.03 val PER: 0.1128
2026-01-14 01:18:04,953: t15.2023.09.24 val PER: 0.0837
2026-01-14 01:18:04,953: t15.2023.09.29 val PER: 0.0944
2026-01-14 01:18:04,953: t15.2023.10.01 val PER: 0.1275
2026-01-14 01:18:04,954: t15.2023.10.06 val PER: 0.0409
2026-01-14 01:18:04,954: t15.2023.10.08 val PER: 0.1989
2026-01-14 01:18:04,954: t15.2023.10.13 val PER: 0.1334
2026-01-14 01:18:04,954: t15.2023.10.15 val PER: 0.0995
2026-01-14 01:18:04,954: t15.2023.10.20 val PER: 0.1611
2026-01-14 01:18:04,954: t15.2023.10.22 val PER: 0.0791
2026-01-14 01:18:04,954: t15.2023.11.03 val PER: 0.1364
2026-01-14 01:18:04,954: t15.2023.11.04 val PER: 0.0205
2026-01-14 01:18:04,954: t15.2023.11.17 val PER: 0.0187
2026-01-14 01:18:04,954: t15.2023.11.19 val PER: 0.0200
2026-01-14 01:18:04,954: t15.2023.11.26 val PER: 0.0442
2026-01-14 01:18:04,954: t15.2023.12.03 val PER: 0.0473
2026-01-14 01:18:04,955: t15.2023.12.08 val PER: 0.0346
2026-01-14 01:18:04,955: t15.2023.12.10 val PER: 0.0342
2026-01-14 01:18:04,955: t15.2023.12.17 val PER: 0.0821
2026-01-14 01:18:04,955: t15.2023.12.29 val PER: 0.0652
2026-01-14 01:18:04,955: t15.2024.02.25 val PER: 0.0604
2026-01-14 01:18:04,955: t15.2024.03.08 val PER: 0.1579
2026-01-14 01:18:04,955: t15.2024.03.15 val PER: 0.1545
2026-01-14 01:18:04,955: t15.2024.03.17 val PER: 0.0676
2026-01-14 01:18:04,955: t15.2024.05.10 val PER: 0.1204
2026-01-14 01:18:04,955: t15.2024.06.14 val PER: 0.1151
2026-01-14 01:18:04,955: t15.2024.07.19 val PER: 0.1543
2026-01-14 01:18:04,955: t15.2024.07.21 val PER: 0.0593
2026-01-14 01:18:04,955: t15.2024.07.28 val PER: 0.0897
2026-01-14 01:18:04,955: t15.2025.01.10 val PER: 0.2548
2026-01-14 01:18:04,956: t15.2025.01.12 val PER: 0.0908
2026-01-14 01:18:04,956: t15.2025.03.14 val PER: 0.2973
2026-01-14 01:18:04,956: t15.2025.03.16 val PER: 0.1217
2026-01-14 01:18:04,956: t15.2025.03.30 val PER: 0.2253
2026-01-14 01:18:04,956: t15.2025.04.13 val PER: 0.1797
2026-01-14 01:18:05,111: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_98500
2026-01-14 01:18:14,122: Train batch 98600: loss: 0.27 grad norm: 18.27 time: 0.078
2026-01-14 01:18:31,859: Train batch 98800: loss: 0.06 grad norm: 4.49 time: 0.060
2026-01-14 01:18:49,233: Train batch 99000: loss: 0.03 grad norm: 2.89 time: 0.060
2026-01-14 01:18:49,233: Running test after training batch: 99000
2026-01-14 01:18:49,356: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:18:54,797: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:18:54,858: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:19:10,246: Val batch 99000: PER (avg): 0.1003 CTC Loss (avg): 19.4858 WER(5gram): 19.62% (n=256) time: 21.013
2026-01-14 01:19:10,246: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=13
2026-01-14 01:19:10,246: t15.2023.08.13 val PER: 0.0696
2026-01-14 01:19:10,246: t15.2023.08.18 val PER: 0.0645
2026-01-14 01:19:10,246: t15.2023.08.20 val PER: 0.0580
2026-01-14 01:19:10,247: t15.2023.08.25 val PER: 0.0723
2026-01-14 01:19:10,247: t15.2023.08.27 val PER: 0.1447
2026-01-14 01:19:10,247: t15.2023.09.01 val PER: 0.0373
2026-01-14 01:19:10,247: t15.2023.09.03 val PER: 0.1152
2026-01-14 01:19:10,247: t15.2023.09.24 val PER: 0.0813
2026-01-14 01:19:10,247: t15.2023.09.29 val PER: 0.0970
2026-01-14 01:19:10,247: t15.2023.10.01 val PER: 0.1268
2026-01-14 01:19:10,247: t15.2023.10.06 val PER: 0.0409
2026-01-14 01:19:10,247: t15.2023.10.08 val PER: 0.1949
2026-01-14 01:19:10,247: t15.2023.10.13 val PER: 0.1373
2026-01-14 01:19:10,247: t15.2023.10.15 val PER: 0.1028
2026-01-14 01:19:10,248: t15.2023.10.20 val PER: 0.1577
2026-01-14 01:19:10,248: t15.2023.10.22 val PER: 0.0791
2026-01-14 01:19:10,248: t15.2023.11.03 val PER: 0.1377
2026-01-14 01:19:10,248: t15.2023.11.04 val PER: 0.0171
2026-01-14 01:19:10,248: t15.2023.11.17 val PER: 0.0156
2026-01-14 01:19:10,248: t15.2023.11.19 val PER: 0.0200
2026-01-14 01:19:10,248: t15.2023.11.26 val PER: 0.0442
2026-01-14 01:19:10,248: t15.2023.12.03 val PER: 0.0483
2026-01-14 01:19:10,248: t15.2023.12.08 val PER: 0.0346
2026-01-14 01:19:10,248: t15.2023.12.10 val PER: 0.0342
2026-01-14 01:19:10,248: t15.2023.12.17 val PER: 0.0873
2026-01-14 01:19:10,248: t15.2023.12.29 val PER: 0.0652
2026-01-14 01:19:10,248: t15.2024.02.25 val PER: 0.0576
2026-01-14 01:19:10,248: t15.2024.03.08 val PER: 0.1565
2026-01-14 01:19:10,248: t15.2024.03.15 val PER: 0.1538
2026-01-14 01:19:10,248: t15.2024.03.17 val PER: 0.0662
2026-01-14 01:19:10,248: t15.2024.05.10 val PER: 0.1233
2026-01-14 01:19:10,249: t15.2024.06.14 val PER: 0.1151
2026-01-14 01:19:10,249: t15.2024.07.19 val PER: 0.1569
2026-01-14 01:19:10,249: t15.2024.07.21 val PER: 0.0586
2026-01-14 01:19:10,249: t15.2024.07.28 val PER: 0.0904
2026-01-14 01:19:10,249: t15.2025.01.10 val PER: 0.2507
2026-01-14 01:19:10,249: t15.2025.01.12 val PER: 0.0955
2026-01-14 01:19:10,249: t15.2025.03.14 val PER: 0.2973
2026-01-14 01:19:10,249: t15.2025.03.16 val PER: 0.1204
2026-01-14 01:19:10,249: t15.2025.03.30 val PER: 0.2230
2026-01-14 01:19:10,249: t15.2025.04.13 val PER: 0.1840
2026-01-14 01:19:10,406: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_99000
2026-01-14 01:19:27,505: Train batch 99200: loss: 0.13 grad norm: 10.52 time: 0.066
2026-01-14 01:19:44,798: Train batch 99400: loss: 0.05 grad norm: 4.73 time: 0.069
2026-01-14 01:19:53,465: Running test after training batch: 99500
2026-01-14 01:19:53,672: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:19:58,477: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:19:58,537: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-14 01:20:13,908: Val batch 99500: PER (avg): 0.0997 CTC Loss (avg): 19.5079 WER(5gram): 19.56% (n=256) time: 20.443
2026-01-14 01:20:13,909: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-14 01:20:13,909: t15.2023.08.13 val PER: 0.0696
2026-01-14 01:20:13,909: t15.2023.08.18 val PER: 0.0654
2026-01-14 01:20:13,909: t15.2023.08.20 val PER: 0.0580
2026-01-14 01:20:13,909: t15.2023.08.25 val PER: 0.0753
2026-01-14 01:20:13,909: t15.2023.08.27 val PER: 0.1399
2026-01-14 01:20:13,909: t15.2023.09.01 val PER: 0.0390
2026-01-14 01:20:13,909: t15.2023.09.03 val PER: 0.1140
2026-01-14 01:20:13,909: t15.2023.09.24 val PER: 0.0801
2026-01-14 01:20:13,910: t15.2023.09.29 val PER: 0.0957
2026-01-14 01:20:13,910: t15.2023.10.01 val PER: 0.1262
2026-01-14 01:20:13,910: t15.2023.10.06 val PER: 0.0398
2026-01-14 01:20:13,910: t15.2023.10.08 val PER: 0.1935
2026-01-14 01:20:13,910: t15.2023.10.13 val PER: 0.1358
2026-01-14 01:20:13,910: t15.2023.10.15 val PER: 0.1002
2026-01-14 01:20:13,910: t15.2023.10.20 val PER: 0.1577
2026-01-14 01:20:13,910: t15.2023.10.22 val PER: 0.0802
2026-01-14 01:20:13,910: t15.2023.11.03 val PER: 0.1364
2026-01-14 01:20:13,910: t15.2023.11.04 val PER: 0.0171
2026-01-14 01:20:13,910: t15.2023.11.17 val PER: 0.0187
2026-01-14 01:20:13,910: t15.2023.11.19 val PER: 0.0180
2026-01-14 01:20:13,911: t15.2023.11.26 val PER: 0.0442
2026-01-14 01:20:13,911: t15.2023.12.03 val PER: 0.0462
2026-01-14 01:20:13,911: t15.2023.12.08 val PER: 0.0333
2026-01-14 01:20:13,911: t15.2023.12.10 val PER: 0.0329
2026-01-14 01:20:13,911: t15.2023.12.17 val PER: 0.0821
2026-01-14 01:20:13,911: t15.2023.12.29 val PER: 0.0659
2026-01-14 01:20:13,911: t15.2024.02.25 val PER: 0.0590
2026-01-14 01:20:13,911: t15.2024.03.08 val PER: 0.1579
2026-01-14 01:20:13,911: t15.2024.03.15 val PER: 0.1507
2026-01-14 01:20:13,911: t15.2024.03.17 val PER: 0.0669
2026-01-14 01:20:13,911: t15.2024.05.10 val PER: 0.1218
2026-01-14 01:20:13,911: t15.2024.06.14 val PER: 0.1136
2026-01-14 01:20:13,911: t15.2024.07.19 val PER: 0.1575
2026-01-14 01:20:13,911: t15.2024.07.21 val PER: 0.0572
2026-01-14 01:20:13,912: t15.2024.07.28 val PER: 0.0934
2026-01-14 01:20:13,912: t15.2025.01.10 val PER: 0.2521
2026-01-14 01:20:13,912: t15.2025.01.12 val PER: 0.0939
2026-01-14 01:20:13,912: t15.2025.03.14 val PER: 0.2959
2026-01-14 01:20:13,912: t15.2025.03.16 val PER: 0.1204
2026-01-14 01:20:13,912: t15.2025.03.30 val PER: 0.2230
2026-01-14 01:20:13,912: t15.2025.04.13 val PER: 0.1854
2026-01-14 01:20:14,063: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_99500
2026-01-14 01:20:22,871: Train batch 99600: loss: 0.25 grad norm: 20.65 time: 0.056
2026-01-14 01:20:40,440: Train batch 99800: loss: 0.03 grad norm: 5.37 time: 0.051
2026-01-14 01:20:57,747: Running test after training batch: 99999
2026-01-14 01:20:57,868: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:21:02,635: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:21:02,697: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost set
2026-01-14 01:21:17,767: Val batch 99999: PER (avg): 0.1001 CTC Loss (avg): 19.6168 WER(5gram): 19.62% (n=256) time: 20.019
2026-01-14 01:21:17,767: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=13
2026-01-14 01:21:17,767: t15.2023.08.13 val PER: 0.0686
2026-01-14 01:21:17,768: t15.2023.08.18 val PER: 0.0654
2026-01-14 01:21:17,768: t15.2023.08.20 val PER: 0.0564
2026-01-14 01:21:17,768: t15.2023.08.25 val PER: 0.0708
2026-01-14 01:21:17,768: t15.2023.08.27 val PER: 0.1463
2026-01-14 01:21:17,768: t15.2023.09.01 val PER: 0.0398
2026-01-14 01:21:17,768: t15.2023.09.03 val PER: 0.1105
2026-01-14 01:21:17,768: t15.2023.09.24 val PER: 0.0813
2026-01-14 01:21:17,768: t15.2023.09.29 val PER: 0.0970
2026-01-14 01:21:17,768: t15.2023.10.01 val PER: 0.1281
2026-01-14 01:21:17,768: t15.2023.10.06 val PER: 0.0409
2026-01-14 01:21:17,768: t15.2023.10.08 val PER: 0.1962
2026-01-14 01:21:17,769: t15.2023.10.13 val PER: 0.1365
2026-01-14 01:21:17,769: t15.2023.10.15 val PER: 0.0982
2026-01-14 01:21:17,769: t15.2023.10.20 val PER: 0.1577
2026-01-14 01:21:17,769: t15.2023.10.22 val PER: 0.0802
2026-01-14 01:21:17,769: t15.2023.11.03 val PER: 0.1384
2026-01-14 01:21:17,769: t15.2023.11.04 val PER: 0.0171
2026-01-14 01:21:17,769: t15.2023.11.17 val PER: 0.0187
2026-01-14 01:21:17,769: t15.2023.11.19 val PER: 0.0180
2026-01-14 01:21:17,769: t15.2023.11.26 val PER: 0.0420
2026-01-14 01:21:17,769: t15.2023.12.03 val PER: 0.0462
2026-01-14 01:21:17,769: t15.2023.12.08 val PER: 0.0346
2026-01-14 01:21:17,769: t15.2023.12.10 val PER: 0.0302
2026-01-14 01:21:17,769: t15.2023.12.17 val PER: 0.0832
2026-01-14 01:21:17,769: t15.2023.12.29 val PER: 0.0666
2026-01-14 01:21:17,770: t15.2024.02.25 val PER: 0.0562
2026-01-14 01:21:17,770: t15.2024.03.08 val PER: 0.1565
2026-01-14 01:21:17,770: t15.2024.03.15 val PER: 0.1520
2026-01-14 01:21:17,770: t15.2024.03.17 val PER: 0.0683
2026-01-14 01:21:17,770: t15.2024.05.10 val PER: 0.1204
2026-01-14 01:21:17,770: t15.2024.06.14 val PER: 0.1183
2026-01-14 01:21:17,770: t15.2024.07.19 val PER: 0.1602
2026-01-14 01:21:17,770: t15.2024.07.21 val PER: 0.0572
2026-01-14 01:21:17,770: t15.2024.07.28 val PER: 0.0941
2026-01-14 01:21:17,771: t15.2025.01.10 val PER: 0.2548
2026-01-14 01:21:17,771: t15.2025.01.12 val PER: 0.0955
2026-01-14 01:21:17,771: t15.2025.03.14 val PER: 0.2988
2026-01-14 01:21:17,771: t15.2025.03.16 val PER: 0.1217
2026-01-14 01:21:17,771: t15.2025.03.30 val PER: 0.2218
2026-01-14 01:21:17,771: t15.2025.04.13 val PER: 0.1812
2026-01-14 01:21:17,927: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/base/checkpoint/checkpoint_batch_99999
2026-01-14 01:21:18,399: Best avg val PER achieved: 0.12737
2026-01-14 01:21:18,399: Total training time: 218.42 minutes

=== RUN diphone_100k.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k
2026-01-14 01:23:19,051: Using device: cuda:0
2026-01-14 01:27:13,507: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-14 01:27:13,508: Diphone mode ENABLED: n_classes changed from 41 to 1601
2026-01-14 01:27:13,535: Using 45 sessions after filtering (from 45).
2026-01-14 01:27:13,985: Using torch.compile (if available)
2026-01-14 01:27:13,986: torch.compile not available (torch<2.0). Skipping.
2026-01-14 01:27:13,986: Initialized RNN decoding model
2026-01-14 01:27:13,986: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=1601, bias=True)
)
2026-01-14 01:27:13,986: Model has 46,106,945 parameters
2026-01-14 01:27:13,986: Model has 11,819,520 day-specific parameters | 25.64% of total parameters
2026-01-14 01:27:18,415: Successfully initialized datasets
2026-01-14 01:27:18,415: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-14 01:27:20,133: Train batch 0: loss: 1341.10 grad norm: 955.07 time: 0.450
2026-01-14 01:27:20,133: Running test after training batch: 0
2026-01-14 01:27:20,346: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:27:26,934: WER debug example
  GT : you can see the code at this point as well
  PR : as
2026-01-14 01:27:28,108: WER debug example
  GT : how does it keep the cost down
  PR : here's
2026-01-14 01:31:54,777: Val batch 0: PER (avg): 1.5858 CTC Loss (avg): 1498.2145 WER(5gram): 99.87% (n=256) time: 274.643
2026-01-14 01:31:54,780: WER lens: avg_true_words=5.99 avg_pred_words=0.20 max_pred_words=1
2026-01-14 01:31:54,784: t15.2023.08.13 val PER: 1.4324
2026-01-14 01:31:54,787: t15.2023.08.18 val PER: 1.5105
2026-01-14 01:31:54,788: t15.2023.08.20 val PER: 1.4234
2026-01-14 01:31:54,788: t15.2023.08.25 val PER: 1.5919
2026-01-14 01:31:54,788: t15.2023.08.27 val PER: 1.4180
2026-01-14 01:31:54,788: t15.2023.09.01 val PER: 1.5544
2026-01-14 01:31:54,788: t15.2023.09.03 val PER: 1.4976
2026-01-14 01:31:54,788: t15.2023.09.24 val PER: 1.7269
2026-01-14 01:31:54,788: t15.2023.09.29 val PER: 1.6943
2026-01-14 01:31:54,788: t15.2023.10.01 val PER: 1.3032
2026-01-14 01:31:54,788: t15.2023.10.06 val PER: 1.6136
2026-01-14 01:31:54,789: t15.2023.10.08 val PER: 1.1894
2026-01-14 01:31:54,789: t15.2023.10.13 val PER: 1.4810
2026-01-14 01:31:54,789: t15.2023.10.15 val PER: 1.5313
2026-01-14 01:31:54,789: t15.2023.10.20 val PER: 1.6879
2026-01-14 01:31:54,789: t15.2023.10.22 val PER: 1.7227
2026-01-14 01:31:54,789: t15.2023.11.03 val PER: 1.7795
2026-01-14 01:31:54,789: t15.2023.11.04 val PER: 2.0478
2026-01-14 01:31:54,789: t15.2023.11.17 val PER: 2.1820
2026-01-14 01:31:54,789: t15.2023.11.19 val PER: 1.9242
2026-01-14 01:31:54,789: t15.2023.11.26 val PER: 1.7457
2026-01-14 01:31:54,789: t15.2023.12.03 val PER: 1.5557
2026-01-14 01:31:54,789: t15.2023.12.08 val PER: 1.6884
2026-01-14 01:31:54,789: t15.2023.12.10 val PER: 1.8817
2026-01-14 01:31:54,790: t15.2023.12.17 val PER: 1.4168
2026-01-14 01:31:54,790: t15.2023.12.29 val PER: 1.5278
2026-01-14 01:31:54,790: t15.2024.02.25 val PER: 1.5899
2026-01-14 01:31:54,790: t15.2024.03.08 val PER: 1.6117
2026-01-14 01:31:54,790: t15.2024.03.15 val PER: 1.4878
2026-01-14 01:31:54,790: t15.2024.03.17 val PER: 1.5167
2026-01-14 01:31:54,790: t15.2024.05.10 val PER: 1.4978
2026-01-14 01:31:54,790: t15.2024.06.14 val PER: 1.7035
2026-01-14 01:31:54,790: t15.2024.07.19 val PER: 1.1780
2026-01-14 01:31:54,790: t15.2024.07.21 val PER: 1.7931
2026-01-14 01:31:54,790: t15.2024.07.28 val PER: 1.7868
2026-01-14 01:31:54,790: t15.2025.01.10 val PER: 1.2617
2026-01-14 01:31:54,790: t15.2025.01.12 val PER: 1.8961
2026-01-14 01:31:54,790: t15.2025.03.14 val PER: 1.1612
2026-01-14 01:31:54,790: t15.2025.03.16 val PER: 1.8521
2026-01-14 01:31:54,791: t15.2025.03.30 val PER: 1.4839
2026-01-14 01:31:54,791: t15.2025.04.13 val PER: 1.7247
2026-01-14 01:31:54,792: New best val WER(5gram) inf% --> 99.87%
2026-01-14 01:31:54,940: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_0
2026-01-14 01:32:12,245: Train batch 200: loss: 151.60 grad norm: 56.06 time: 0.064
2026-01-14 01:32:29,816: Train batch 400: loss: 113.70 grad norm: 110.03 time: 0.073
2026-01-14 01:32:38,665: Running test after training batch: 500
2026-01-14 01:32:38,803: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:32:44,239: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-14 01:32:44,267: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-14 01:32:50,654: Val batch 500: PER (avg): 0.9405 CTC Loss (avg): 119.2270 WER(5gram): 99.93% (n=256) time: 11.989
2026-01-14 01:32:50,654: WER lens: avg_true_words=5.99 avg_pred_words=0.00 max_pred_words=1
2026-01-14 01:32:50,654: t15.2023.08.13 val PER: 0.9387
2026-01-14 01:32:50,655: t15.2023.08.18 val PER: 0.9195
2026-01-14 01:32:50,655: t15.2023.08.20 val PER: 0.9245
2026-01-14 01:32:50,655: t15.2023.08.25 val PER: 0.9111
2026-01-14 01:32:50,655: t15.2023.08.27 val PER: 0.9486
2026-01-14 01:32:50,655: t15.2023.09.01 val PER: 0.9115
2026-01-14 01:32:50,655: t15.2023.09.03 val PER: 0.9667
2026-01-14 01:32:50,655: t15.2023.09.24 val PER: 0.9381
2026-01-14 01:32:50,655: t15.2023.09.29 val PER: 0.9030
2026-01-14 01:32:50,655: t15.2023.10.01 val PER: 0.9346
2026-01-14 01:32:50,656: t15.2023.10.06 val PER: 0.9128
2026-01-14 01:32:50,656: t15.2023.10.08 val PER: 0.9337
2026-01-14 01:32:50,656: t15.2023.10.13 val PER: 0.9317
2026-01-14 01:32:50,656: t15.2023.10.15 val PER: 0.9090
2026-01-14 01:32:50,656: t15.2023.10.20 val PER: 0.9094
2026-01-14 01:32:50,656: t15.2023.10.22 val PER: 0.9087
2026-01-14 01:32:50,656: t15.2023.11.03 val PER: 0.9206
2026-01-14 01:32:50,657: t15.2023.11.04 val PER: 0.9147
2026-01-14 01:32:50,657: t15.2023.11.17 val PER: 0.9145
2026-01-14 01:32:50,657: t15.2023.11.19 val PER: 0.9361
2026-01-14 01:32:50,657: t15.2023.11.26 val PER: 0.9435
2026-01-14 01:32:50,657: t15.2023.12.03 val PER: 0.9254
2026-01-14 01:32:50,657: t15.2023.12.08 val PER: 0.9414
2026-01-14 01:32:50,657: t15.2023.12.10 val PER: 0.9264
2026-01-14 01:32:50,657: t15.2023.12.17 val PER: 0.9740
2026-01-14 01:32:50,657: t15.2023.12.29 val PER: 0.9643
2026-01-14 01:32:50,657: t15.2024.02.25 val PER: 0.9382
2026-01-14 01:32:50,657: t15.2024.03.08 val PER: 0.9730
2026-01-14 01:32:50,657: t15.2024.03.15 val PER: 0.9562
2026-01-14 01:32:50,657: t15.2024.03.17 val PER: 0.9435
2026-01-14 01:32:50,657: t15.2024.05.10 val PER: 0.9480
2026-01-14 01:32:50,657: t15.2024.06.14 val PER: 0.9558
2026-01-14 01:32:50,657: t15.2024.07.19 val PER: 0.9868
2026-01-14 01:32:50,657: t15.2024.07.21 val PER: 0.9476
2026-01-14 01:32:50,658: t15.2024.07.28 val PER: 0.9441
2026-01-14 01:32:50,658: t15.2025.01.10 val PER: 0.9821
2026-01-14 01:32:50,658: t15.2025.01.12 val PER: 0.9523
2026-01-14 01:32:50,658: t15.2025.03.14 val PER: 0.9956
2026-01-14 01:32:50,658: t15.2025.03.16 val PER: 0.9555
2026-01-14 01:32:50,658: t15.2025.03.30 val PER: 0.9805
2026-01-14 01:32:50,658: t15.2025.04.13 val PER: 0.9429
2026-01-14 01:32:50,802: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_500
2026-01-14 01:32:59,740: Train batch 600: loss: 100.06 grad norm: 90.99 time: 0.090
2026-01-14 01:33:17,822: Train batch 800: loss: 79.13 grad norm: 100.76 time: 0.067
2026-01-14 01:33:35,552: Train batch 1000: loss: 78.36 grad norm: 100.01 time: 0.075
2026-01-14 01:33:35,553: Running test after training batch: 1000
2026-01-14 01:33:35,672: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:33:41,700: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the
2026-01-14 01:33:41,851: WER debug example
  GT : how does it keep the cost down
  PR : in the
2026-01-14 01:34:24,234: Val batch 1000: PER (avg): 0.4654 CTC Loss (avg): 81.0859 WER(5gram): 81.55% (n=256) time: 48.681
2026-01-14 01:34:24,234: WER lens: avg_true_words=5.99 avg_pred_words=2.83 max_pred_words=10
2026-01-14 01:34:24,234: t15.2023.08.13 val PER: 0.4366
2026-01-14 01:34:24,234: t15.2023.08.18 val PER: 0.4065
2026-01-14 01:34:24,235: t15.2023.08.20 val PER: 0.4067
2026-01-14 01:34:24,235: t15.2023.08.25 val PER: 0.3870
2026-01-14 01:34:24,235: t15.2023.08.27 val PER: 0.5080
2026-01-14 01:34:24,235: t15.2023.09.01 val PER: 0.3726
2026-01-14 01:34:24,235: t15.2023.09.03 val PER: 0.4608
2026-01-14 01:34:24,235: t15.2023.09.24 val PER: 0.4223
2026-01-14 01:34:24,235: t15.2023.09.29 val PER: 0.4352
2026-01-14 01:34:24,235: t15.2023.10.01 val PER: 0.4954
2026-01-14 01:34:24,235: t15.2023.10.06 val PER: 0.4080
2026-01-14 01:34:24,235: t15.2023.10.08 val PER: 0.5142
2026-01-14 01:34:24,236: t15.2023.10.13 val PER: 0.5244
2026-01-14 01:34:24,236: t15.2023.10.15 val PER: 0.4456
2026-01-14 01:34:24,236: t15.2023.10.20 val PER: 0.4396
2026-01-14 01:34:24,236: t15.2023.10.22 val PER: 0.4209
2026-01-14 01:34:24,236: t15.2023.11.03 val PER: 0.4444
2026-01-14 01:34:24,236: t15.2023.11.04 val PER: 0.2321
2026-01-14 01:34:24,236: t15.2023.11.17 val PER: 0.3344
2026-01-14 01:34:24,236: t15.2023.11.19 val PER: 0.2914
2026-01-14 01:34:24,236: t15.2023.11.26 val PER: 0.5014
2026-01-14 01:34:24,236: t15.2023.12.03 val PER: 0.4559
2026-01-14 01:34:24,236: t15.2023.12.08 val PER: 0.4640
2026-01-14 01:34:24,236: t15.2023.12.10 val PER: 0.4205
2026-01-14 01:34:24,236: t15.2023.12.17 val PER: 0.4595
2026-01-14 01:34:24,237: t15.2023.12.29 val PER: 0.4688
2026-01-14 01:34:24,237: t15.2024.02.25 val PER: 0.3933
2026-01-14 01:34:24,237: t15.2024.03.08 val PER: 0.5377
2026-01-14 01:34:24,237: t15.2024.03.15 val PER: 0.4972
2026-01-14 01:34:24,237: t15.2024.03.17 val PER: 0.4616
2026-01-14 01:34:24,237: t15.2024.05.10 val PER: 0.4606
2026-01-14 01:34:24,237: t15.2024.06.14 val PER: 0.4338
2026-01-14 01:34:24,237: t15.2024.07.19 val PER: 0.5616
2026-01-14 01:34:24,237: t15.2024.07.21 val PER: 0.4324
2026-01-14 01:34:24,237: t15.2024.07.28 val PER: 0.4669
2026-01-14 01:34:24,237: t15.2025.01.10 val PER: 0.6350
2026-01-14 01:34:24,238: t15.2025.01.12 val PER: 0.4850
2026-01-14 01:34:24,238: t15.2025.03.14 val PER: 0.6376
2026-01-14 01:34:24,238: t15.2025.03.16 val PER: 0.5301
2026-01-14 01:34:24,238: t15.2025.03.30 val PER: 0.6506
2026-01-14 01:34:24,238: t15.2025.04.13 val PER: 0.5250
2026-01-14 01:34:24,239: New best val WER(5gram) 99.87% --> 81.55%
2026-01-14 01:34:24,385: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_1000
2026-01-14 01:34:41,801: Train batch 1200: loss: 63.23 grad norm: 112.25 time: 0.078
2026-01-14 01:34:59,284: Train batch 1400: loss: 64.93 grad norm: 129.01 time: 0.070
2026-01-14 01:35:08,001: Running test after training batch: 1500
2026-01-14 01:35:08,142: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:35:13,743: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the way this is why
2026-01-14 01:35:13,931: WER debug example
  GT : how does it keep the cost down
  PR : to see it in the sa
2026-01-14 01:35:49,126: Val batch 1500: PER (avg): 0.4029 CTC Loss (avg): 69.0371 WER(5gram): 70.27% (n=256) time: 41.124
2026-01-14 01:35:49,127: WER lens: avg_true_words=5.99 avg_pred_words=4.76 max_pred_words=13
2026-01-14 01:35:49,127: t15.2023.08.13 val PER: 0.3669
2026-01-14 01:35:49,127: t15.2023.08.18 val PER: 0.3420
2026-01-14 01:35:49,127: t15.2023.08.20 val PER: 0.3487
2026-01-14 01:35:49,127: t15.2023.08.25 val PER: 0.3313
2026-01-14 01:35:49,127: t15.2023.08.27 val PER: 0.4309
2026-01-14 01:35:49,127: t15.2023.09.01 val PER: 0.2995
2026-01-14 01:35:49,127: t15.2023.09.03 val PER: 0.3979
2026-01-14 01:35:49,128: t15.2023.09.24 val PER: 0.3313
2026-01-14 01:35:49,128: t15.2023.09.29 val PER: 0.3733
2026-01-14 01:35:49,128: t15.2023.10.01 val PER: 0.4148
2026-01-14 01:35:49,128: t15.2023.10.06 val PER: 0.3272
2026-01-14 01:35:49,128: t15.2023.10.08 val PER: 0.4560
2026-01-14 01:35:49,128: t15.2023.10.13 val PER: 0.4856
2026-01-14 01:35:49,128: t15.2023.10.15 val PER: 0.3995
2026-01-14 01:35:49,128: t15.2023.10.20 val PER: 0.3993
2026-01-14 01:35:49,128: t15.2023.10.22 val PER: 0.3541
2026-01-14 01:35:49,128: t15.2023.11.03 val PER: 0.3847
2026-01-14 01:35:49,129: t15.2023.11.04 val PER: 0.1604
2026-01-14 01:35:49,129: t15.2023.11.17 val PER: 0.2395
2026-01-14 01:35:49,129: t15.2023.11.19 val PER: 0.2236
2026-01-14 01:35:49,129: t15.2023.11.26 val PER: 0.4471
2026-01-14 01:35:49,130: t15.2023.12.03 val PER: 0.3834
2026-01-14 01:35:49,130: t15.2023.12.08 val PER: 0.4001
2026-01-14 01:35:49,130: t15.2023.12.10 val PER: 0.3561
2026-01-14 01:35:49,130: t15.2023.12.17 val PER: 0.4012
2026-01-14 01:35:49,130: t15.2023.12.29 val PER: 0.4036
2026-01-14 01:35:49,130: t15.2024.02.25 val PER: 0.3455
2026-01-14 01:35:49,130: t15.2024.03.08 val PER: 0.4694
2026-01-14 01:35:49,130: t15.2024.03.15 val PER: 0.4303
2026-01-14 01:35:49,130: t15.2024.03.17 val PER: 0.3975
2026-01-14 01:35:49,130: t15.2024.05.10 val PER: 0.4012
2026-01-14 01:35:49,130: t15.2024.06.14 val PER: 0.3975
2026-01-14 01:35:49,130: t15.2024.07.19 val PER: 0.5260
2026-01-14 01:35:49,130: t15.2024.07.21 val PER: 0.3614
2026-01-14 01:35:49,130: t15.2024.07.28 val PER: 0.3978
2026-01-14 01:35:49,130: t15.2025.01.10 val PER: 0.5620
2026-01-14 01:35:49,131: t15.2025.01.12 val PER: 0.4288
2026-01-14 01:35:49,131: t15.2025.03.14 val PER: 0.5680
2026-01-14 01:35:49,131: t15.2025.03.16 val PER: 0.4751
2026-01-14 01:35:49,131: t15.2025.03.30 val PER: 0.6046
2026-01-14 01:35:49,131: t15.2025.04.13 val PER: 0.4636
2026-01-14 01:35:49,132: New best val WER(5gram) 81.55% --> 70.27%
2026-01-14 01:35:49,287: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_1500
2026-01-14 01:35:58,048: Train batch 1600: loss: 64.75 grad norm: 105.40 time: 0.074
2026-01-14 01:36:15,690: Train batch 1800: loss: 63.13 grad norm: 130.83 time: 0.100
2026-01-14 01:36:33,466: Train batch 2000: loss: 60.78 grad norm: 108.17 time: 0.077
2026-01-14 01:36:33,466: Running test after training batch: 2000
2026-01-14 01:36:33,746: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:36:39,677: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the way this is why
2026-01-14 01:36:39,816: WER debug example
  GT : how does it keep the cost down
  PR : i see it in the sa
2026-01-14 01:37:04,991: Val batch 2000: PER (avg): 0.3635 CTC Loss (avg): 61.1221 WER(5gram): 56.19% (n=256) time: 31.524
2026-01-14 01:37:04,991: WER lens: avg_true_words=5.99 avg_pred_words=5.46 max_pred_words=12
2026-01-14 01:37:04,991: t15.2023.08.13 val PER: 0.3316
2026-01-14 01:37:04,992: t15.2023.08.18 val PER: 0.3110
2026-01-14 01:37:04,992: t15.2023.08.20 val PER: 0.2994
2026-01-14 01:37:04,992: t15.2023.08.25 val PER: 0.2741
2026-01-14 01:37:04,992: t15.2023.08.27 val PER: 0.3923
2026-01-14 01:37:04,992: t15.2023.09.01 val PER: 0.2703
2026-01-14 01:37:04,992: t15.2023.09.03 val PER: 0.3753
2026-01-14 01:37:04,992: t15.2023.09.24 val PER: 0.2998
2026-01-14 01:37:04,992: t15.2023.09.29 val PER: 0.3242
2026-01-14 01:37:04,992: t15.2023.10.01 val PER: 0.3785
2026-01-14 01:37:04,992: t15.2023.10.06 val PER: 0.2896
2026-01-14 01:37:04,992: t15.2023.10.08 val PER: 0.4290
2026-01-14 01:37:04,993: t15.2023.10.13 val PER: 0.4476
2026-01-14 01:37:04,993: t15.2023.10.15 val PER: 0.3448
2026-01-14 01:37:04,993: t15.2023.10.20 val PER: 0.3423
2026-01-14 01:37:04,993: t15.2023.10.22 val PER: 0.3073
2026-01-14 01:37:04,993: t15.2023.11.03 val PER: 0.3575
2026-01-14 01:37:04,993: t15.2023.11.04 val PER: 0.1229
2026-01-14 01:37:04,993: t15.2023.11.17 val PER: 0.1897
2026-01-14 01:37:04,993: t15.2023.11.19 val PER: 0.1617
2026-01-14 01:37:04,993: t15.2023.11.26 val PER: 0.4210
2026-01-14 01:37:04,993: t15.2023.12.03 val PER: 0.3571
2026-01-14 01:37:04,993: t15.2023.12.08 val PER: 0.3642
2026-01-14 01:37:04,993: t15.2023.12.10 val PER: 0.3154
2026-01-14 01:37:04,993: t15.2023.12.17 val PER: 0.3472
2026-01-14 01:37:04,994: t15.2023.12.29 val PER: 0.3624
2026-01-14 01:37:04,994: t15.2024.02.25 val PER: 0.3062
2026-01-14 01:37:04,994: t15.2024.03.08 val PER: 0.4395
2026-01-14 01:37:04,994: t15.2024.03.15 val PER: 0.3821
2026-01-14 01:37:04,994: t15.2024.03.17 val PER: 0.3619
2026-01-14 01:37:04,994: t15.2024.05.10 val PER: 0.3611
2026-01-14 01:37:04,994: t15.2024.06.14 val PER: 0.3470
2026-01-14 01:37:04,994: t15.2024.07.19 val PER: 0.4852
2026-01-14 01:37:04,994: t15.2024.07.21 val PER: 0.3248
2026-01-14 01:37:04,994: t15.2024.07.28 val PER: 0.3441
2026-01-14 01:37:04,995: t15.2025.01.10 val PER: 0.5303
2026-01-14 01:37:04,995: t15.2025.01.12 val PER: 0.3895
2026-01-14 01:37:04,995: t15.2025.03.14 val PER: 0.5562
2026-01-14 01:37:04,995: t15.2025.03.16 val PER: 0.4293
2026-01-14 01:37:04,995: t15.2025.03.30 val PER: 0.5621
2026-01-14 01:37:04,995: t15.2025.04.13 val PER: 0.4223
2026-01-14 01:37:04,996: New best val WER(5gram) 70.27% --> 56.19%
2026-01-14 01:37:05,158: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_2000
2026-01-14 01:37:22,711: Train batch 2200: loss: 53.47 grad norm: 98.32 time: 0.070
2026-01-14 01:37:40,451: Train batch 2400: loss: 50.89 grad norm: 93.84 time: 0.061
2026-01-14 01:37:49,272: Running test after training batch: 2500
2026-01-14 01:37:49,366: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:37:55,368: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the guy at this point is why
2026-01-14 01:37:55,461: WER debug example
  GT : how does it keep the cost down
  PR : to see it in the sa
2026-01-14 01:38:17,490: Val batch 2500: PER (avg): 0.3383 CTC Loss (avg): 55.3905 WER(5gram): 57.37% (n=256) time: 28.217
2026-01-14 01:38:17,491: WER lens: avg_true_words=5.99 avg_pred_words=5.75 max_pred_words=14
2026-01-14 01:38:17,491: t15.2023.08.13 val PER: 0.3067
2026-01-14 01:38:17,491: t15.2023.08.18 val PER: 0.2892
2026-01-14 01:38:17,491: t15.2023.08.20 val PER: 0.2788
2026-01-14 01:38:17,491: t15.2023.08.25 val PER: 0.2410
2026-01-14 01:38:17,491: t15.2023.08.27 val PER: 0.3714
2026-01-14 01:38:17,491: t15.2023.09.01 val PER: 0.2386
2026-01-14 01:38:17,491: t15.2023.09.03 val PER: 0.3432
2026-01-14 01:38:17,491: t15.2023.09.24 val PER: 0.2779
2026-01-14 01:38:17,491: t15.2023.09.29 val PER: 0.3076
2026-01-14 01:38:17,491: t15.2023.10.01 val PER: 0.3580
2026-01-14 01:38:17,491: t15.2023.10.06 val PER: 0.2487
2026-01-14 01:38:17,491: t15.2023.10.08 val PER: 0.4195
2026-01-14 01:38:17,492: t15.2023.10.13 val PER: 0.4383
2026-01-14 01:38:17,492: t15.2023.10.15 val PER: 0.3322
2026-01-14 01:38:17,492: t15.2023.10.20 val PER: 0.3154
2026-01-14 01:38:17,492: t15.2023.10.22 val PER: 0.2895
2026-01-14 01:38:17,492: t15.2023.11.03 val PER: 0.3236
2026-01-14 01:38:17,492: t15.2023.11.04 val PER: 0.0990
2026-01-14 01:38:17,492: t15.2023.11.17 val PER: 0.1695
2026-01-14 01:38:17,492: t15.2023.11.19 val PER: 0.1517
2026-01-14 01:38:17,492: t15.2023.11.26 val PER: 0.3775
2026-01-14 01:38:17,492: t15.2023.12.03 val PER: 0.3372
2026-01-14 01:38:17,492: t15.2023.12.08 val PER: 0.3455
2026-01-14 01:38:17,492: t15.2023.12.10 val PER: 0.2825
2026-01-14 01:38:17,492: t15.2023.12.17 val PER: 0.3181
2026-01-14 01:38:17,492: t15.2023.12.29 val PER: 0.3370
2026-01-14 01:38:17,493: t15.2024.02.25 val PER: 0.2893
2026-01-14 01:38:17,493: t15.2024.03.08 val PER: 0.3883
2026-01-14 01:38:17,493: t15.2024.03.15 val PER: 0.3652
2026-01-14 01:38:17,493: t15.2024.03.17 val PER: 0.3333
2026-01-14 01:38:17,493: t15.2024.05.10 val PER: 0.3343
2026-01-14 01:38:17,493: t15.2024.06.14 val PER: 0.3312
2026-01-14 01:38:17,493: t15.2024.07.19 val PER: 0.4562
2026-01-14 01:38:17,493: t15.2024.07.21 val PER: 0.2938
2026-01-14 01:38:17,493: t15.2024.07.28 val PER: 0.3066
2026-01-14 01:38:17,493: t15.2025.01.10 val PER: 0.4917
2026-01-14 01:38:17,494: t15.2025.01.12 val PER: 0.3541
2026-01-14 01:38:17,494: t15.2025.03.14 val PER: 0.5133
2026-01-14 01:38:17,494: t15.2025.03.16 val PER: 0.3992
2026-01-14 01:38:17,494: t15.2025.03.30 val PER: 0.5747
2026-01-14 01:38:17,494: t15.2025.04.13 val PER: 0.3923
2026-01-14 01:38:17,635: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_2500
2026-01-14 01:38:26,403: Train batch 2600: loss: 59.02 grad norm: 116.08 time: 0.064
2026-01-14 01:38:43,850: Train batch 2800: loss: 45.26 grad norm: 102.82 time: 0.091
2026-01-14 01:39:01,216: Train batch 3000: loss: 56.08 grad norm: 118.94 time: 0.093
2026-01-14 01:39:01,216: Running test after training batch: 3000
2026-01-14 01:39:01,351: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:39:06,834: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the ca this is why
2026-01-14 01:39:06,911: WER debug example
  GT : how does it keep the cost down
  PR : to see it in the cost
2026-01-14 01:39:22,734: Val batch 3000: PER (avg): 0.3216 CTC Loss (avg): 51.2737 WER(5gram): 50.85% (n=256) time: 21.517
2026-01-14 01:39:22,734: WER lens: avg_true_words=5.99 avg_pred_words=5.49 max_pred_words=12
2026-01-14 01:39:22,735: t15.2023.08.13 val PER: 0.2744
2026-01-14 01:39:22,735: t15.2023.08.18 val PER: 0.2791
2026-01-14 01:39:22,735: t15.2023.08.20 val PER: 0.2589
2026-01-14 01:39:22,735: t15.2023.08.25 val PER: 0.2334
2026-01-14 01:39:22,735: t15.2023.08.27 val PER: 0.3392
2026-01-14 01:39:22,735: t15.2023.09.01 val PER: 0.2224
2026-01-14 01:39:22,735: t15.2023.09.03 val PER: 0.3230
2026-01-14 01:39:22,735: t15.2023.09.24 val PER: 0.2682
2026-01-14 01:39:22,735: t15.2023.09.29 val PER: 0.2884
2026-01-14 01:39:22,735: t15.2023.10.01 val PER: 0.3514
2026-01-14 01:39:22,735: t15.2023.10.06 val PER: 0.2443
2026-01-14 01:39:22,736: t15.2023.10.08 val PER: 0.4019
2026-01-14 01:39:22,736: t15.2023.10.13 val PER: 0.4189
2026-01-14 01:39:22,736: t15.2023.10.15 val PER: 0.3171
2026-01-14 01:39:22,736: t15.2023.10.20 val PER: 0.3020
2026-01-14 01:39:22,736: t15.2023.10.22 val PER: 0.2784
2026-01-14 01:39:22,736: t15.2023.11.03 val PER: 0.3100
2026-01-14 01:39:22,736: t15.2023.11.04 val PER: 0.1024
2026-01-14 01:39:22,736: t15.2023.11.17 val PER: 0.1571
2026-01-14 01:39:22,736: t15.2023.11.19 val PER: 0.1497
2026-01-14 01:39:22,736: t15.2023.11.26 val PER: 0.3659
2026-01-14 01:39:22,736: t15.2023.12.03 val PER: 0.3067
2026-01-14 01:39:22,736: t15.2023.12.08 val PER: 0.3089
2026-01-14 01:39:22,736: t15.2023.12.10 val PER: 0.2799
2026-01-14 01:39:22,737: t15.2023.12.17 val PER: 0.2879
2026-01-14 01:39:22,737: t15.2023.12.29 val PER: 0.3240
2026-01-14 01:39:22,737: t15.2024.02.25 val PER: 0.2992
2026-01-14 01:39:22,737: t15.2024.03.08 val PER: 0.3883
2026-01-14 01:39:22,737: t15.2024.03.15 val PER: 0.3659
2026-01-14 01:39:22,737: t15.2024.03.17 val PER: 0.3117
2026-01-14 01:39:22,737: t15.2024.05.10 val PER: 0.3314
2026-01-14 01:39:22,737: t15.2024.06.14 val PER: 0.3044
2026-01-14 01:39:22,737: t15.2024.07.19 val PER: 0.4225
2026-01-14 01:39:22,737: t15.2024.07.21 val PER: 0.2752
2026-01-14 01:39:22,737: t15.2024.07.28 val PER: 0.3221
2026-01-14 01:39:22,737: t15.2025.01.10 val PER: 0.4711
2026-01-14 01:39:22,737: t15.2025.01.12 val PER: 0.3418
2026-01-14 01:39:22,737: t15.2025.03.14 val PER: 0.4763
2026-01-14 01:39:22,737: t15.2025.03.16 val PER: 0.3691
2026-01-14 01:39:22,738: t15.2025.03.30 val PER: 0.5069
2026-01-14 01:39:22,738: t15.2025.04.13 val PER: 0.3666
2026-01-14 01:39:22,739: New best val WER(5gram) 56.19% --> 50.85%
2026-01-14 01:39:22,889: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_3000
2026-01-14 01:39:40,584: Train batch 3200: loss: 44.19 grad norm: 98.16 time: 0.087
2026-01-14 01:39:58,439: Train batch 3400: loss: 33.18 grad norm: 78.87 time: 0.057
2026-01-14 01:40:07,264: Running test after training batch: 3500
2026-01-14 01:40:07,399: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:40:12,928: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point is why
2026-01-14 01:40:13,018: WER debug example
  GT : how does it keep the cost down
  PR : how to see it in the first day
2026-01-14 01:40:28,738: Val batch 3500: PER (avg): 0.3075 CTC Loss (avg): 48.3971 WER(5gram): 44.39% (n=256) time: 21.474
2026-01-14 01:40:28,739: WER lens: avg_true_words=5.99 avg_pred_words=5.84 max_pred_words=13
2026-01-14 01:40:28,739: t15.2023.08.13 val PER: 0.2692
2026-01-14 01:40:28,739: t15.2023.08.18 val PER: 0.2540
2026-01-14 01:40:28,739: t15.2023.08.20 val PER: 0.2375
2026-01-14 01:40:28,740: t15.2023.08.25 val PER: 0.2093
2026-01-14 01:40:28,740: t15.2023.08.27 val PER: 0.3248
2026-01-14 01:40:28,740: t15.2023.09.01 val PER: 0.2078
2026-01-14 01:40:28,740: t15.2023.09.03 val PER: 0.3135
2026-01-14 01:40:28,740: t15.2023.09.24 val PER: 0.2354
2026-01-14 01:40:28,740: t15.2023.09.29 val PER: 0.2642
2026-01-14 01:40:28,740: t15.2023.10.01 val PER: 0.3316
2026-01-14 01:40:28,740: t15.2023.10.06 val PER: 0.2153
2026-01-14 01:40:28,740: t15.2023.10.08 val PER: 0.4100
2026-01-14 01:40:28,740: t15.2023.10.13 val PER: 0.4057
2026-01-14 01:40:28,740: t15.2023.10.15 val PER: 0.3144
2026-01-14 01:40:28,740: t15.2023.10.20 val PER: 0.2953
2026-01-14 01:40:28,740: t15.2023.10.22 val PER: 0.2673
2026-01-14 01:40:28,740: t15.2023.11.03 val PER: 0.3039
2026-01-14 01:40:28,740: t15.2023.11.04 val PER: 0.0785
2026-01-14 01:40:28,741: t15.2023.11.17 val PER: 0.1291
2026-01-14 01:40:28,741: t15.2023.11.19 val PER: 0.1297
2026-01-14 01:40:28,741: t15.2023.11.26 val PER: 0.3442
2026-01-14 01:40:28,741: t15.2023.12.03 val PER: 0.2836
2026-01-14 01:40:28,741: t15.2023.12.08 val PER: 0.3063
2026-01-14 01:40:28,741: t15.2023.12.10 val PER: 0.2615
2026-01-14 01:40:28,741: t15.2023.12.17 val PER: 0.2838
2026-01-14 01:40:28,741: t15.2023.12.29 val PER: 0.3027
2026-01-14 01:40:28,741: t15.2024.02.25 val PER: 0.2612
2026-01-14 01:40:28,742: t15.2024.03.08 val PER: 0.3983
2026-01-14 01:40:28,742: t15.2024.03.15 val PER: 0.3440
2026-01-14 01:40:28,742: t15.2024.03.17 val PER: 0.3026
2026-01-14 01:40:28,742: t15.2024.05.10 val PER: 0.3165
2026-01-14 01:40:28,742: t15.2024.06.14 val PER: 0.3186
2026-01-14 01:40:28,742: t15.2024.07.19 val PER: 0.4074
2026-01-14 01:40:28,742: t15.2024.07.21 val PER: 0.2593
2026-01-14 01:40:28,742: t15.2024.07.28 val PER: 0.3022
2026-01-14 01:40:28,742: t15.2025.01.10 val PER: 0.4766
2026-01-14 01:40:28,742: t15.2025.01.12 val PER: 0.3218
2026-01-14 01:40:28,742: t15.2025.03.14 val PER: 0.4837
2026-01-14 01:40:28,742: t15.2025.03.16 val PER: 0.3626
2026-01-14 01:40:28,742: t15.2025.03.30 val PER: 0.5092
2026-01-14 01:40:28,743: t15.2025.04.13 val PER: 0.3538
2026-01-14 01:40:28,744: New best val WER(5gram) 50.85% --> 44.39%
2026-01-14 01:40:28,902: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_3500
2026-01-14 01:40:37,828: Train batch 3600: loss: 39.08 grad norm: 91.33 time: 0.077
2026-01-14 01:40:55,119: Train batch 3800: loss: 43.05 grad norm: 94.23 time: 0.077
2026-01-14 01:41:12,681: Train batch 4000: loss: 37.35 grad norm: 76.78 time: 0.065
2026-01-14 01:41:12,681: Running test after training batch: 4000
2026-01-14 01:41:12,839: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:41:18,335: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is why
2026-01-14 01:41:18,389: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-14 01:41:31,533: Val batch 4000: PER (avg): 0.2900 CTC Loss (avg): 44.7763 WER(5gram): 35.66% (n=256) time: 18.852
2026-01-14 01:41:31,534: WER lens: avg_true_words=5.99 avg_pred_words=5.72 max_pred_words=11
2026-01-14 01:41:31,534: t15.2023.08.13 val PER: 0.2412
2026-01-14 01:41:31,534: t15.2023.08.18 val PER: 0.2364
2026-01-14 01:41:31,534: t15.2023.08.20 val PER: 0.2303
2026-01-14 01:41:31,535: t15.2023.08.25 val PER: 0.1913
2026-01-14 01:41:31,535: t15.2023.08.27 val PER: 0.3199
2026-01-14 01:41:31,535: t15.2023.09.01 val PER: 0.1916
2026-01-14 01:41:31,535: t15.2023.09.03 val PER: 0.3100
2026-01-14 01:41:31,535: t15.2023.09.24 val PER: 0.2269
2026-01-14 01:41:31,535: t15.2023.09.29 val PER: 0.2514
2026-01-14 01:41:31,535: t15.2023.10.01 val PER: 0.3243
2026-01-14 01:41:31,535: t15.2023.10.06 val PER: 0.2260
2026-01-14 01:41:31,535: t15.2023.10.08 val PER: 0.3897
2026-01-14 01:41:31,535: t15.2023.10.13 val PER: 0.3856
2026-01-14 01:41:31,535: t15.2023.10.15 val PER: 0.2821
2026-01-14 01:41:31,535: t15.2023.10.20 val PER: 0.3054
2026-01-14 01:41:31,535: t15.2023.10.22 val PER: 0.2383
2026-01-14 01:41:31,536: t15.2023.11.03 val PER: 0.2843
2026-01-14 01:41:31,536: t15.2023.11.04 val PER: 0.0751
2026-01-14 01:41:31,536: t15.2023.11.17 val PER: 0.1213
2026-01-14 01:41:31,536: t15.2023.11.19 val PER: 0.1058
2026-01-14 01:41:31,536: t15.2023.11.26 val PER: 0.3239
2026-01-14 01:41:31,536: t15.2023.12.03 val PER: 0.2679
2026-01-14 01:41:31,536: t15.2023.12.08 val PER: 0.2836
2026-01-14 01:41:31,536: t15.2023.12.10 val PER: 0.2326
2026-01-14 01:41:31,536: t15.2023.12.17 val PER: 0.2568
2026-01-14 01:41:31,536: t15.2023.12.29 val PER: 0.2876
2026-01-14 01:41:31,536: t15.2024.02.25 val PER: 0.2514
2026-01-14 01:41:31,536: t15.2024.03.08 val PER: 0.3585
2026-01-14 01:41:31,536: t15.2024.03.15 val PER: 0.3371
2026-01-14 01:41:31,536: t15.2024.03.17 val PER: 0.2894
2026-01-14 01:41:31,536: t15.2024.05.10 val PER: 0.3031
2026-01-14 01:41:31,537: t15.2024.06.14 val PER: 0.2965
2026-01-14 01:41:31,537: t15.2024.07.19 val PER: 0.4028
2026-01-14 01:41:31,537: t15.2024.07.21 val PER: 0.2262
2026-01-14 01:41:31,537: t15.2024.07.28 val PER: 0.2728
2026-01-14 01:41:31,537: t15.2025.01.10 val PER: 0.4325
2026-01-14 01:41:31,537: t15.2025.01.12 val PER: 0.3164
2026-01-14 01:41:31,537: t15.2025.03.14 val PER: 0.4630
2026-01-14 01:41:31,537: t15.2025.03.16 val PER: 0.3482
2026-01-14 01:41:31,537: t15.2025.03.30 val PER: 0.4598
2026-01-14 01:41:31,537: t15.2025.04.13 val PER: 0.3466
2026-01-14 01:41:31,539: New best val WER(5gram) 44.39% --> 35.66%
2026-01-14 01:41:31,686: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_4000
2026-01-14 01:41:49,502: Train batch 4200: loss: 38.75 grad norm: 95.79 time: 0.090
2026-01-14 01:42:06,849: Train batch 4400: loss: 27.76 grad norm: 75.88 time: 0.076
2026-01-14 01:42:15,991: Running test after training batch: 4500
2026-01-14 01:42:16,100: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:42:21,615: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point as well
2026-01-14 01:42:21,681: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-14 01:42:35,756: Val batch 4500: PER (avg): 0.2645 CTC Loss (avg): 42.2797 WER(5gram): 28.49% (n=256) time: 19.765
2026-01-14 01:42:35,757: WER lens: avg_true_words=5.99 avg_pred_words=5.97 max_pred_words=12
2026-01-14 01:42:35,757: t15.2023.08.13 val PER: 0.2245
2026-01-14 01:42:35,757: t15.2023.08.18 val PER: 0.2129
2026-01-14 01:42:35,757: t15.2023.08.20 val PER: 0.2168
2026-01-14 01:42:35,757: t15.2023.08.25 val PER: 0.1747
2026-01-14 01:42:35,757: t15.2023.08.27 val PER: 0.2846
2026-01-14 01:42:35,757: t15.2023.09.01 val PER: 0.1753
2026-01-14 01:42:35,757: t15.2023.09.03 val PER: 0.2732
2026-01-14 01:42:35,758: t15.2023.09.24 val PER: 0.2051
2026-01-14 01:42:35,758: t15.2023.09.29 val PER: 0.2195
2026-01-14 01:42:35,758: t15.2023.10.01 val PER: 0.2913
2026-01-14 01:42:35,758: t15.2023.10.06 val PER: 0.1841
2026-01-14 01:42:35,758: t15.2023.10.08 val PER: 0.3505
2026-01-14 01:42:35,758: t15.2023.10.13 val PER: 0.3646
2026-01-14 01:42:35,758: t15.2023.10.15 val PER: 0.2676
2026-01-14 01:42:35,758: t15.2023.10.20 val PER: 0.2953
2026-01-14 01:42:35,758: t15.2023.10.22 val PER: 0.2305
2026-01-14 01:42:35,759: t15.2023.11.03 val PER: 0.2809
2026-01-14 01:42:35,759: t15.2023.11.04 val PER: 0.0546
2026-01-14 01:42:35,759: t15.2023.11.17 val PER: 0.0824
2026-01-14 01:42:35,759: t15.2023.11.19 val PER: 0.0898
2026-01-14 01:42:35,759: t15.2023.11.26 val PER: 0.2877
2026-01-14 01:42:35,759: t15.2023.12.03 val PER: 0.2542
2026-01-14 01:42:35,759: t15.2023.12.08 val PER: 0.2590
2026-01-14 01:42:35,759: t15.2023.12.10 val PER: 0.1958
2026-01-14 01:42:35,759: t15.2023.12.17 val PER: 0.2370
2026-01-14 01:42:35,760: t15.2023.12.29 val PER: 0.2656
2026-01-14 01:42:35,760: t15.2024.02.25 val PER: 0.2430
2026-01-14 01:42:35,760: t15.2024.03.08 val PER: 0.3243
2026-01-14 01:42:35,760: t15.2024.03.15 val PER: 0.3014
2026-01-14 01:42:35,760: t15.2024.03.17 val PER: 0.2566
2026-01-14 01:42:35,760: t15.2024.05.10 val PER: 0.2883
2026-01-14 01:42:35,760: t15.2024.06.14 val PER: 0.2666
2026-01-14 01:42:35,760: t15.2024.07.19 val PER: 0.3626
2026-01-14 01:42:35,760: t15.2024.07.21 val PER: 0.2055
2026-01-14 01:42:35,761: t15.2024.07.28 val PER: 0.2515
2026-01-14 01:42:35,761: t15.2025.01.10 val PER: 0.3871
2026-01-14 01:42:35,761: t15.2025.01.12 val PER: 0.2856
2026-01-14 01:42:35,761: t15.2025.03.14 val PER: 0.4408
2026-01-14 01:42:35,761: t15.2025.03.16 val PER: 0.3115
2026-01-14 01:42:35,761: t15.2025.03.30 val PER: 0.4414
2026-01-14 01:42:35,761: t15.2025.04.13 val PER: 0.3053
2026-01-14 01:42:35,762: New best val WER(5gram) 35.66% --> 28.49%
2026-01-14 01:42:35,910: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_4500
2026-01-14 01:42:44,824: Train batch 4600: loss: 33.32 grad norm: 93.85 time: 0.072
2026-01-14 01:43:02,861: Train batch 4800: loss: 25.31 grad norm: 81.29 time: 0.072
2026-01-14 01:43:20,542: Train batch 5000: loss: 53.29 grad norm: 119.97 time: 0.075
2026-01-14 01:43:20,542: Running test after training batch: 5000
2026-01-14 01:43:20,653: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:43:26,464: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:43:26,519: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-14 01:43:38,194: Val batch 5000: PER (avg): 0.2535 CTC Loss (avg): 39.9999 WER(5gram): 28.10% (n=256) time: 17.652
2026-01-14 01:43:38,194: WER lens: avg_true_words=5.99 avg_pred_words=5.95 max_pred_words=11
2026-01-14 01:43:38,195: t15.2023.08.13 val PER: 0.2173
2026-01-14 01:43:38,195: t15.2023.08.18 val PER: 0.2079
2026-01-14 01:43:38,195: t15.2023.08.20 val PER: 0.2121
2026-01-14 01:43:38,195: t15.2023.08.25 val PER: 0.1747
2026-01-14 01:43:38,195: t15.2023.08.27 val PER: 0.2717
2026-01-14 01:43:38,195: t15.2023.09.01 val PER: 0.1615
2026-01-14 01:43:38,195: t15.2023.09.03 val PER: 0.2803
2026-01-14 01:43:38,195: t15.2023.09.24 val PER: 0.1978
2026-01-14 01:43:38,195: t15.2023.09.29 val PER: 0.2246
2026-01-14 01:43:38,195: t15.2023.10.01 val PER: 0.2867
2026-01-14 01:43:38,195: t15.2023.10.06 val PER: 0.1776
2026-01-14 01:43:38,195: t15.2023.10.08 val PER: 0.3424
2026-01-14 01:43:38,195: t15.2023.10.13 val PER: 0.3406
2026-01-14 01:43:38,196: t15.2023.10.15 val PER: 0.2630
2026-01-14 01:43:38,196: t15.2023.10.20 val PER: 0.2852
2026-01-14 01:43:38,196: t15.2023.10.22 val PER: 0.2038
2026-01-14 01:43:38,196: t15.2023.11.03 val PER: 0.2544
2026-01-14 01:43:38,196: t15.2023.11.04 val PER: 0.0512
2026-01-14 01:43:38,196: t15.2023.11.17 val PER: 0.0793
2026-01-14 01:43:38,196: t15.2023.11.19 val PER: 0.0739
2026-01-14 01:43:38,196: t15.2023.11.26 val PER: 0.2696
2026-01-14 01:43:38,196: t15.2023.12.03 val PER: 0.2374
2026-01-14 01:43:38,196: t15.2023.12.08 val PER: 0.2397
2026-01-14 01:43:38,196: t15.2023.12.10 val PER: 0.1932
2026-01-14 01:43:38,197: t15.2023.12.17 val PER: 0.2162
2026-01-14 01:43:38,197: t15.2023.12.29 val PER: 0.2402
2026-01-14 01:43:38,197: t15.2024.02.25 val PER: 0.2261
2026-01-14 01:43:38,197: t15.2024.03.08 val PER: 0.3272
2026-01-14 01:43:38,197: t15.2024.03.15 val PER: 0.2977
2026-01-14 01:43:38,197: t15.2024.03.17 val PER: 0.2552
2026-01-14 01:43:38,197: t15.2024.05.10 val PER: 0.2630
2026-01-14 01:43:38,197: t15.2024.06.14 val PER: 0.2760
2026-01-14 01:43:38,197: t15.2024.07.19 val PER: 0.3421
2026-01-14 01:43:38,197: t15.2024.07.21 val PER: 0.2007
2026-01-14 01:43:38,197: t15.2024.07.28 val PER: 0.2419
2026-01-14 01:43:38,197: t15.2025.01.10 val PER: 0.4105
2026-01-14 01:43:38,197: t15.2025.01.12 val PER: 0.2694
2026-01-14 01:43:38,197: t15.2025.03.14 val PER: 0.4038
2026-01-14 01:43:38,198: t15.2025.03.16 val PER: 0.3050
2026-01-14 01:43:38,198: t15.2025.03.30 val PER: 0.3851
2026-01-14 01:43:38,198: t15.2025.04.13 val PER: 0.3167
2026-01-14 01:43:38,199: New best val WER(5gram) 28.49% --> 28.10%
2026-01-14 01:43:38,365: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_5000
2026-01-14 01:43:55,852: Train batch 5200: loss: 26.82 grad norm: 84.75 time: 0.060
2026-01-14 01:44:13,698: Train batch 5400: loss: 33.29 grad norm: 91.49 time: 0.081
2026-01-14 01:44:22,743: Running test after training batch: 5500
2026-01-14 01:44:22,890: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:44:28,372: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:44:28,409: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-14 01:44:40,252: Val batch 5500: PER (avg): 0.2389 CTC Loss (avg): 37.8952 WER(5gram): 24.84% (n=256) time: 17.509
2026-01-14 01:44:40,253: WER lens: avg_true_words=5.99 avg_pred_words=5.91 max_pred_words=11
2026-01-14 01:44:40,253: t15.2023.08.13 val PER: 0.2027
2026-01-14 01:44:40,253: t15.2023.08.18 val PER: 0.1945
2026-01-14 01:44:40,254: t15.2023.08.20 val PER: 0.1819
2026-01-14 01:44:40,254: t15.2023.08.25 val PER: 0.1702
2026-01-14 01:44:40,254: t15.2023.08.27 val PER: 0.2653
2026-01-14 01:44:40,254: t15.2023.09.01 val PER: 0.1575
2026-01-14 01:44:40,254: t15.2023.09.03 val PER: 0.2530
2026-01-14 01:44:40,254: t15.2023.09.24 val PER: 0.1857
2026-01-14 01:44:40,254: t15.2023.09.29 val PER: 0.2087
2026-01-14 01:44:40,254: t15.2023.10.01 val PER: 0.2734
2026-01-14 01:44:40,254: t15.2023.10.06 val PER: 0.1690
2026-01-14 01:44:40,254: t15.2023.10.08 val PER: 0.3275
2026-01-14 01:44:40,254: t15.2023.10.13 val PER: 0.3336
2026-01-14 01:44:40,255: t15.2023.10.15 val PER: 0.2406
2026-01-14 01:44:40,255: t15.2023.10.20 val PER: 0.2987
2026-01-14 01:44:40,255: t15.2023.10.22 val PER: 0.1826
2026-01-14 01:44:40,255: t15.2023.11.03 val PER: 0.2531
2026-01-14 01:44:40,255: t15.2023.11.04 val PER: 0.0546
2026-01-14 01:44:40,255: t15.2023.11.17 val PER: 0.0886
2026-01-14 01:44:40,255: t15.2023.11.19 val PER: 0.0758
2026-01-14 01:44:40,255: t15.2023.11.26 val PER: 0.2449
2026-01-14 01:44:40,255: t15.2023.12.03 val PER: 0.2143
2026-01-14 01:44:40,255: t15.2023.12.08 val PER: 0.2230
2026-01-14 01:44:40,255: t15.2023.12.10 val PER: 0.1813
2026-01-14 01:44:40,255: t15.2023.12.17 val PER: 0.2048
2026-01-14 01:44:40,255: t15.2023.12.29 val PER: 0.2361
2026-01-14 01:44:40,255: t15.2024.02.25 val PER: 0.2163
2026-01-14 01:44:40,255: t15.2024.03.08 val PER: 0.3001
2026-01-14 01:44:40,255: t15.2024.03.15 val PER: 0.2833
2026-01-14 01:44:40,256: t15.2024.03.17 val PER: 0.2266
2026-01-14 01:44:40,256: t15.2024.05.10 val PER: 0.2422
2026-01-14 01:44:40,256: t15.2024.06.14 val PER: 0.2429
2026-01-14 01:44:40,256: t15.2024.07.19 val PER: 0.3289
2026-01-14 01:44:40,256: t15.2024.07.21 val PER: 0.1807
2026-01-14 01:44:40,256: t15.2024.07.28 val PER: 0.2397
2026-01-14 01:44:40,256: t15.2025.01.10 val PER: 0.3788
2026-01-14 01:44:40,256: t15.2025.01.12 val PER: 0.2525
2026-01-14 01:44:40,256: t15.2025.03.14 val PER: 0.4068
2026-01-14 01:44:40,256: t15.2025.03.16 val PER: 0.2775
2026-01-14 01:44:40,256: t15.2025.03.30 val PER: 0.3747
2026-01-14 01:44:40,256: t15.2025.04.13 val PER: 0.2896
2026-01-14 01:44:40,258: New best val WER(5gram) 28.10% --> 24.84%
2026-01-14 01:44:40,407: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_5500
2026-01-14 01:44:49,348: Train batch 5600: loss: 31.78 grad norm: 89.82 time: 0.072
2026-01-14 01:45:07,166: Train batch 5800: loss: 22.88 grad norm: 88.33 time: 0.095
2026-01-14 01:45:24,897: Train batch 6000: loss: 21.87 grad norm: 77.75 time: 0.058
2026-01-14 01:45:24,897: Running test after training batch: 6000
2026-01-14 01:45:25,087: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:45:30,514: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point as well
2026-01-14 01:45:30,578: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-14 01:45:41,733: Val batch 6000: PER (avg): 0.2322 CTC Loss (avg): 36.9945 WER(5gram): 22.23% (n=256) time: 16.836
2026-01-14 01:45:41,734: WER lens: avg_true_words=5.99 avg_pred_words=5.92 max_pred_words=11
2026-01-14 01:45:41,734: t15.2023.08.13 val PER: 0.2048
2026-01-14 01:45:41,734: t15.2023.08.18 val PER: 0.1953
2026-01-14 01:45:41,734: t15.2023.08.20 val PER: 0.1819
2026-01-14 01:45:41,734: t15.2023.08.25 val PER: 0.1627
2026-01-14 01:45:41,734: t15.2023.08.27 val PER: 0.2765
2026-01-14 01:45:41,734: t15.2023.09.01 val PER: 0.1461
2026-01-14 01:45:41,734: t15.2023.09.03 val PER: 0.2518
2026-01-14 01:45:41,734: t15.2023.09.24 val PER: 0.1748
2026-01-14 01:45:41,734: t15.2023.09.29 val PER: 0.2055
2026-01-14 01:45:41,734: t15.2023.10.01 val PER: 0.2734
2026-01-14 01:45:41,734: t15.2023.10.06 val PER: 0.1658
2026-01-14 01:45:41,735: t15.2023.10.08 val PER: 0.3518
2026-01-14 01:45:41,735: t15.2023.10.13 val PER: 0.3266
2026-01-14 01:45:41,735: t15.2023.10.15 val PER: 0.2393
2026-01-14 01:45:41,735: t15.2023.10.20 val PER: 0.2550
2026-01-14 01:45:41,735: t15.2023.10.22 val PER: 0.2071
2026-01-14 01:45:41,735: t15.2023.11.03 val PER: 0.2510
2026-01-14 01:45:41,735: t15.2023.11.04 val PER: 0.0580
2026-01-14 01:45:41,736: t15.2023.11.17 val PER: 0.0747
2026-01-14 01:45:41,736: t15.2023.11.19 val PER: 0.0719
2026-01-14 01:45:41,736: t15.2023.11.26 val PER: 0.2449
2026-01-14 01:45:41,736: t15.2023.12.03 val PER: 0.1901
2026-01-14 01:45:41,736: t15.2023.12.08 val PER: 0.2011
2026-01-14 01:45:41,736: t15.2023.12.10 val PER: 0.1708
2026-01-14 01:45:41,736: t15.2023.12.17 val PER: 0.1840
2026-01-14 01:45:41,736: t15.2023.12.29 val PER: 0.2286
2026-01-14 01:45:41,736: t15.2024.02.25 val PER: 0.2149
2026-01-14 01:45:41,736: t15.2024.03.08 val PER: 0.2973
2026-01-14 01:45:41,736: t15.2024.03.15 val PER: 0.2714
2026-01-14 01:45:41,736: t15.2024.03.17 val PER: 0.2329
2026-01-14 01:45:41,736: t15.2024.05.10 val PER: 0.2377
2026-01-14 01:45:41,736: t15.2024.06.14 val PER: 0.2413
2026-01-14 01:45:41,736: t15.2024.07.19 val PER: 0.3039
2026-01-14 01:45:41,737: t15.2024.07.21 val PER: 0.1752
2026-01-14 01:45:41,737: t15.2024.07.28 val PER: 0.2081
2026-01-14 01:45:41,737: t15.2025.01.10 val PER: 0.3719
2026-01-14 01:45:41,737: t15.2025.01.12 val PER: 0.2579
2026-01-14 01:45:41,737: t15.2025.03.14 val PER: 0.3891
2026-01-14 01:45:41,737: t15.2025.03.16 val PER: 0.2631
2026-01-14 01:45:41,737: t15.2025.03.30 val PER: 0.3506
2026-01-14 01:45:41,738: t15.2025.04.13 val PER: 0.2782
2026-01-14 01:45:41,739: New best val WER(5gram) 24.84% --> 22.23%
2026-01-14 01:45:41,887: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_6000
2026-01-14 01:45:59,112: Train batch 6200: loss: 25.44 grad norm: 79.83 time: 0.080
2026-01-14 01:46:16,376: Train batch 6400: loss: 35.11 grad norm: 96.50 time: 0.074
2026-01-14 01:46:25,578: Running test after training batch: 6500
2026-01-14 01:46:25,711: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:46:31,164: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the good at this point as well
2026-01-14 01:46:31,205: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-14 01:46:42,379: Val batch 6500: PER (avg): 0.2189 CTC Loss (avg): 35.6268 WER(5gram): 20.21% (n=256) time: 16.800
2026-01-14 01:46:42,379: WER lens: avg_true_words=5.99 avg_pred_words=6.00 max_pred_words=12
2026-01-14 01:46:42,379: t15.2023.08.13 val PER: 0.1819
2026-01-14 01:46:42,380: t15.2023.08.18 val PER: 0.1785
2026-01-14 01:46:42,380: t15.2023.08.20 val PER: 0.1668
2026-01-14 01:46:42,380: t15.2023.08.25 val PER: 0.1370
2026-01-14 01:46:42,380: t15.2023.08.27 val PER: 0.2588
2026-01-14 01:46:42,380: t15.2023.09.01 val PER: 0.1323
2026-01-14 01:46:42,380: t15.2023.09.03 val PER: 0.2185
2026-01-14 01:46:42,380: t15.2023.09.24 val PER: 0.1784
2026-01-14 01:46:42,381: t15.2023.09.29 val PER: 0.1946
2026-01-14 01:46:42,381: t15.2023.10.01 val PER: 0.2517
2026-01-14 01:46:42,381: t15.2023.10.06 val PER: 0.1496
2026-01-14 01:46:42,381: t15.2023.10.08 val PER: 0.3383
2026-01-14 01:46:42,382: t15.2023.10.13 val PER: 0.3041
2026-01-14 01:46:42,382: t15.2023.10.15 val PER: 0.2281
2026-01-14 01:46:42,382: t15.2023.10.20 val PER: 0.2785
2026-01-14 01:46:42,382: t15.2023.10.22 val PER: 0.1882
2026-01-14 01:46:42,382: t15.2023.11.03 val PER: 0.2374
2026-01-14 01:46:42,382: t15.2023.11.04 val PER: 0.0512
2026-01-14 01:46:42,382: t15.2023.11.17 val PER: 0.0700
2026-01-14 01:46:42,382: t15.2023.11.19 val PER: 0.0619
2026-01-14 01:46:42,382: t15.2023.11.26 val PER: 0.2203
2026-01-14 01:46:42,382: t15.2023.12.03 val PER: 0.1922
2026-01-14 01:46:42,382: t15.2023.12.08 val PER: 0.1818
2026-01-14 01:46:42,383: t15.2023.12.10 val PER: 0.1616
2026-01-14 01:46:42,383: t15.2023.12.17 val PER: 0.1933
2026-01-14 01:46:42,383: t15.2023.12.29 val PER: 0.2121
2026-01-14 01:46:42,383: t15.2024.02.25 val PER: 0.2022
2026-01-14 01:46:42,383: t15.2024.03.08 val PER: 0.2774
2026-01-14 01:46:42,383: t15.2024.03.15 val PER: 0.2570
2026-01-14 01:46:42,383: t15.2024.03.17 val PER: 0.2183
2026-01-14 01:46:42,383: t15.2024.05.10 val PER: 0.2363
2026-01-14 01:46:42,383: t15.2024.06.14 val PER: 0.2319
2026-01-14 01:46:42,383: t15.2024.07.19 val PER: 0.2980
2026-01-14 01:46:42,383: t15.2024.07.21 val PER: 0.1559
2026-01-14 01:46:42,383: t15.2024.07.28 val PER: 0.2007
2026-01-14 01:46:42,383: t15.2025.01.10 val PER: 0.3636
2026-01-14 01:46:42,383: t15.2025.01.12 val PER: 0.2294
2026-01-14 01:46:42,384: t15.2025.03.14 val PER: 0.3964
2026-01-14 01:46:42,384: t15.2025.03.16 val PER: 0.2513
2026-01-14 01:46:42,384: t15.2025.03.30 val PER: 0.3425
2026-01-14 01:46:42,384: t15.2025.04.13 val PER: 0.2739
2026-01-14 01:46:42,385: New best val WER(5gram) 22.23% --> 20.21%
2026-01-14 01:46:42,531: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_6500
2026-01-14 01:46:51,114: Train batch 6600: loss: 19.93 grad norm: 66.23 time: 0.052
2026-01-14 01:47:09,131: Train batch 6800: loss: 27.98 grad norm: 82.60 time: 0.057
2026-01-14 01:47:27,131: Train batch 7000: loss: 25.85 grad norm: 86.36 time: 0.071
2026-01-14 01:47:27,131: Running test after training batch: 7000
2026-01-14 01:47:27,408: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:47:32,825: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:47:32,863: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-14 01:47:43,565: Val batch 7000: PER (avg): 0.2067 CTC Loss (avg): 33.7636 WER(5gram): 19.17% (n=256) time: 16.434
2026-01-14 01:47:43,566: WER lens: avg_true_words=5.99 avg_pred_words=5.91 max_pred_words=11
2026-01-14 01:47:43,566: t15.2023.08.13 val PER: 0.1767
2026-01-14 01:47:43,566: t15.2023.08.18 val PER: 0.1735
2026-01-14 01:47:43,566: t15.2023.08.20 val PER: 0.1517
2026-01-14 01:47:43,566: t15.2023.08.25 val PER: 0.1461
2026-01-14 01:47:43,566: t15.2023.08.27 val PER: 0.2395
2026-01-14 01:47:43,567: t15.2023.09.01 val PER: 0.1169
2026-01-14 01:47:43,567: t15.2023.09.03 val PER: 0.2185
2026-01-14 01:47:43,567: t15.2023.09.24 val PER: 0.1614
2026-01-14 01:47:43,567: t15.2023.09.29 val PER: 0.1883
2026-01-14 01:47:43,567: t15.2023.10.01 val PER: 0.2483
2026-01-14 01:47:43,567: t15.2023.10.06 val PER: 0.1281
2026-01-14 01:47:43,567: t15.2023.10.08 val PER: 0.3153
2026-01-14 01:47:43,567: t15.2023.10.13 val PER: 0.3002
2026-01-14 01:47:43,567: t15.2023.10.15 val PER: 0.2116
2026-01-14 01:47:43,567: t15.2023.10.20 val PER: 0.2282
2026-01-14 01:47:43,567: t15.2023.10.22 val PER: 0.1637
2026-01-14 01:47:43,568: t15.2023.11.03 val PER: 0.2259
2026-01-14 01:47:43,568: t15.2023.11.04 val PER: 0.0444
2026-01-14 01:47:43,568: t15.2023.11.17 val PER: 0.0653
2026-01-14 01:47:43,568: t15.2023.11.19 val PER: 0.0599
2026-01-14 01:47:43,568: t15.2023.11.26 val PER: 0.1884
2026-01-14 01:47:43,568: t15.2023.12.03 val PER: 0.1744
2026-01-14 01:47:43,568: t15.2023.12.08 val PER: 0.1585
2026-01-14 01:47:43,568: t15.2023.12.10 val PER: 0.1472
2026-01-14 01:47:43,569: t15.2023.12.17 val PER: 0.1528
2026-01-14 01:47:43,569: t15.2023.12.29 val PER: 0.1887
2026-01-14 01:47:43,569: t15.2024.02.25 val PER: 0.1784
2026-01-14 01:47:43,569: t15.2024.03.08 val PER: 0.2802
2026-01-14 01:47:43,569: t15.2024.03.15 val PER: 0.2595
2026-01-14 01:47:43,569: t15.2024.03.17 val PER: 0.2050
2026-01-14 01:47:43,569: t15.2024.05.10 val PER: 0.2184
2026-01-14 01:47:43,569: t15.2024.06.14 val PER: 0.2129
2026-01-14 01:47:43,569: t15.2024.07.19 val PER: 0.2973
2026-01-14 01:47:43,570: t15.2024.07.21 val PER: 0.1441
2026-01-14 01:47:43,570: t15.2024.07.28 val PER: 0.2074
2026-01-14 01:47:43,570: t15.2025.01.10 val PER: 0.3691
2026-01-14 01:47:43,570: t15.2025.01.12 val PER: 0.2294
2026-01-14 01:47:43,570: t15.2025.03.14 val PER: 0.3698
2026-01-14 01:47:43,570: t15.2025.03.16 val PER: 0.2291
2026-01-14 01:47:43,570: t15.2025.03.30 val PER: 0.3356
2026-01-14 01:47:43,570: t15.2025.04.13 val PER: 0.2568
2026-01-14 01:47:43,571: New best val WER(5gram) 20.21% --> 19.17%
2026-01-14 01:47:43,718: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_7000
2026-01-14 01:48:01,944: Train batch 7200: loss: 22.74 grad norm: 80.16 time: 0.090
2026-01-14 01:48:20,040: Train batch 7400: loss: 24.16 grad norm: 79.38 time: 0.087
2026-01-14 01:48:28,748: Running test after training batch: 7500
2026-01-14 01:48:28,863: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:48:34,373: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point it will
2026-01-14 01:48:34,429: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-14 01:48:44,507: Val batch 7500: PER (avg): 0.1997 CTC Loss (avg): 32.4915 WER(5gram): 19.10% (n=256) time: 15.759
2026-01-14 01:48:44,507: WER lens: avg_true_words=5.99 avg_pred_words=5.99 max_pred_words=11
2026-01-14 01:48:44,508: t15.2023.08.13 val PER: 0.1705
2026-01-14 01:48:44,508: t15.2023.08.18 val PER: 0.1660
2026-01-14 01:48:44,508: t15.2023.08.20 val PER: 0.1430
2026-01-14 01:48:44,508: t15.2023.08.25 val PER: 0.1250
2026-01-14 01:48:44,508: t15.2023.08.27 val PER: 0.2186
2026-01-14 01:48:44,508: t15.2023.09.01 val PER: 0.1185
2026-01-14 01:48:44,508: t15.2023.09.03 val PER: 0.2043
2026-01-14 01:48:44,508: t15.2023.09.24 val PER: 0.1347
2026-01-14 01:48:44,508: t15.2023.09.29 val PER: 0.1717
2026-01-14 01:48:44,508: t15.2023.10.01 val PER: 0.2318
2026-01-14 01:48:44,508: t15.2023.10.06 val PER: 0.1335
2026-01-14 01:48:44,509: t15.2023.10.08 val PER: 0.3045
2026-01-14 01:48:44,509: t15.2023.10.13 val PER: 0.2886
2026-01-14 01:48:44,509: t15.2023.10.15 val PER: 0.2123
2026-01-14 01:48:44,509: t15.2023.10.20 val PER: 0.2517
2026-01-14 01:48:44,509: t15.2023.10.22 val PER: 0.1604
2026-01-14 01:48:44,509: t15.2023.11.03 val PER: 0.2164
2026-01-14 01:48:44,509: t15.2023.11.04 val PER: 0.0307
2026-01-14 01:48:44,509: t15.2023.11.17 val PER: 0.0529
2026-01-14 01:48:44,509: t15.2023.11.19 val PER: 0.0459
2026-01-14 01:48:44,509: t15.2023.11.26 val PER: 0.1870
2026-01-14 01:48:44,509: t15.2023.12.03 val PER: 0.1639
2026-01-14 01:48:44,509: t15.2023.12.08 val PER: 0.1531
2026-01-14 01:48:44,509: t15.2023.12.10 val PER: 0.1393
2026-01-14 01:48:44,509: t15.2023.12.17 val PER: 0.1726
2026-01-14 01:48:44,510: t15.2023.12.29 val PER: 0.1771
2026-01-14 01:48:44,510: t15.2024.02.25 val PER: 0.1924
2026-01-14 01:48:44,510: t15.2024.03.08 val PER: 0.2575
2026-01-14 01:48:44,510: t15.2024.03.15 val PER: 0.2489
2026-01-14 01:48:44,510: t15.2024.03.17 val PER: 0.1967
2026-01-14 01:48:44,510: t15.2024.05.10 val PER: 0.2273
2026-01-14 01:48:44,510: t15.2024.06.14 val PER: 0.2066
2026-01-14 01:48:44,510: t15.2024.07.19 val PER: 0.2874
2026-01-14 01:48:44,510: t15.2024.07.21 val PER: 0.1490
2026-01-14 01:48:44,510: t15.2024.07.28 val PER: 0.1934
2026-01-14 01:48:44,510: t15.2025.01.10 val PER: 0.3540
2026-01-14 01:48:44,510: t15.2025.01.12 val PER: 0.2186
2026-01-14 01:48:44,510: t15.2025.03.14 val PER: 0.3654
2026-01-14 01:48:44,510: t15.2025.03.16 val PER: 0.2421
2026-01-14 01:48:44,510: t15.2025.03.30 val PER: 0.3264
2026-01-14 01:48:44,511: t15.2025.04.13 val PER: 0.2596
2026-01-14 01:48:44,512: New best val WER(5gram) 19.17% --> 19.10%
2026-01-14 01:48:44,663: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_7500
2026-01-14 01:48:53,457: Train batch 7600: loss: 24.96 grad norm: 84.36 time: 0.079
2026-01-14 01:49:11,035: Train batch 7800: loss: 22.19 grad norm: 81.80 time: 0.064
2026-01-14 01:49:28,780: Train batch 8000: loss: 18.19 grad norm: 75.37 time: 0.082
2026-01-14 01:49:28,780: Running test after training batch: 8000
2026-01-14 01:49:28,961: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:49:34,377: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:49:34,435: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-14 01:49:45,385: Val batch 8000: PER (avg): 0.1908 CTC Loss (avg): 31.7441 WER(5gram): 18.58% (n=256) time: 16.605
2026-01-14 01:49:45,386: WER lens: avg_true_words=5.99 avg_pred_words=5.99 max_pred_words=12
2026-01-14 01:49:45,386: t15.2023.08.13 val PER: 0.1611
2026-01-14 01:49:45,386: t15.2023.08.18 val PER: 0.1584
2026-01-14 01:49:45,386: t15.2023.08.20 val PER: 0.1477
2026-01-14 01:49:45,386: t15.2023.08.25 val PER: 0.1220
2026-01-14 01:49:45,386: t15.2023.08.27 val PER: 0.2106
2026-01-14 01:49:45,386: t15.2023.09.01 val PER: 0.1047
2026-01-14 01:49:45,386: t15.2023.09.03 val PER: 0.1912
2026-01-14 01:49:45,386: t15.2023.09.24 val PER: 0.1456
2026-01-14 01:49:45,386: t15.2023.09.29 val PER: 0.1646
2026-01-14 01:49:45,386: t15.2023.10.01 val PER: 0.2299
2026-01-14 01:49:45,387: t15.2023.10.06 val PER: 0.1259
2026-01-14 01:49:45,387: t15.2023.10.08 val PER: 0.3099
2026-01-14 01:49:45,387: t15.2023.10.13 val PER: 0.2777
2026-01-14 01:49:45,387: t15.2023.10.15 val PER: 0.1945
2026-01-14 01:49:45,387: t15.2023.10.20 val PER: 0.2685
2026-01-14 01:49:45,387: t15.2023.10.22 val PER: 0.1737
2026-01-14 01:49:45,387: t15.2023.11.03 val PER: 0.2185
2026-01-14 01:49:45,387: t15.2023.11.04 val PER: 0.0307
2026-01-14 01:49:45,387: t15.2023.11.17 val PER: 0.0451
2026-01-14 01:49:45,387: t15.2023.11.19 val PER: 0.0519
2026-01-14 01:49:45,387: t15.2023.11.26 val PER: 0.1696
2026-01-14 01:49:45,387: t15.2023.12.03 val PER: 0.1744
2026-01-14 01:49:45,387: t15.2023.12.08 val PER: 0.1372
2026-01-14 01:49:45,387: t15.2023.12.10 val PER: 0.1327
2026-01-14 01:49:45,388: t15.2023.12.17 val PER: 0.1497
2026-01-14 01:49:45,388: t15.2023.12.29 val PER: 0.1633
2026-01-14 01:49:45,388: t15.2024.02.25 val PER: 0.1657
2026-01-14 01:49:45,388: t15.2024.03.08 val PER: 0.2617
2026-01-14 01:49:45,388: t15.2024.03.15 val PER: 0.2445
2026-01-14 01:49:45,388: t15.2024.03.17 val PER: 0.1848
2026-01-14 01:49:45,388: t15.2024.05.10 val PER: 0.2051
2026-01-14 01:49:45,388: t15.2024.06.14 val PER: 0.2161
2026-01-14 01:49:45,388: t15.2024.07.19 val PER: 0.2690
2026-01-14 01:49:45,388: t15.2024.07.21 val PER: 0.1303
2026-01-14 01:49:45,388: t15.2024.07.28 val PER: 0.1787
2026-01-14 01:49:45,388: t15.2025.01.10 val PER: 0.3320
2026-01-14 01:49:45,388: t15.2025.01.12 val PER: 0.1955
2026-01-14 01:49:45,388: t15.2025.03.14 val PER: 0.3609
2026-01-14 01:49:45,388: t15.2025.03.16 val PER: 0.2225
2026-01-14 01:49:45,388: t15.2025.03.30 val PER: 0.3069
2026-01-14 01:49:45,388: t15.2025.04.13 val PER: 0.2553
2026-01-14 01:49:45,390: New best val WER(5gram) 19.10% --> 18.58%
2026-01-14 01:49:45,541: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_8000
2026-01-14 01:50:02,898: Train batch 8200: loss: 14.96 grad norm: 62.51 time: 0.063
2026-01-14 01:50:20,335: Train batch 8400: loss: 16.02 grad norm: 69.60 time: 0.075
2026-01-14 01:50:29,157: Running test after training batch: 8500
2026-01-14 01:50:29,266: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:50:34,745: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:50:34,802: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-14 01:50:45,907: Val batch 8500: PER (avg): 0.1831 CTC Loss (avg): 30.4054 WER(5gram): 15.84% (n=256) time: 16.750
2026-01-14 01:50:45,908: WER lens: avg_true_words=5.99 avg_pred_words=5.99 max_pred_words=11
2026-01-14 01:50:45,908: t15.2023.08.13 val PER: 0.1580
2026-01-14 01:50:45,908: t15.2023.08.18 val PER: 0.1517
2026-01-14 01:50:45,908: t15.2023.08.20 val PER: 0.1366
2026-01-14 01:50:45,908: t15.2023.08.25 val PER: 0.1114
2026-01-14 01:50:45,908: t15.2023.08.27 val PER: 0.2074
2026-01-14 01:50:45,908: t15.2023.09.01 val PER: 0.0990
2026-01-14 01:50:45,909: t15.2023.09.03 val PER: 0.1924
2026-01-14 01:50:45,909: t15.2023.09.24 val PER: 0.1408
2026-01-14 01:50:45,909: t15.2023.09.29 val PER: 0.1640
2026-01-14 01:50:45,909: t15.2023.10.01 val PER: 0.2166
2026-01-14 01:50:45,909: t15.2023.10.06 val PER: 0.1216
2026-01-14 01:50:45,909: t15.2023.10.08 val PER: 0.3139
2026-01-14 01:50:45,909: t15.2023.10.13 val PER: 0.2622
2026-01-14 01:50:45,909: t15.2023.10.15 val PER: 0.1931
2026-01-14 01:50:45,909: t15.2023.10.20 val PER: 0.2450
2026-01-14 01:50:45,909: t15.2023.10.22 val PER: 0.1659
2026-01-14 01:50:45,909: t15.2023.11.03 val PER: 0.2056
2026-01-14 01:50:45,910: t15.2023.11.04 val PER: 0.0273
2026-01-14 01:50:45,910: t15.2023.11.17 val PER: 0.0404
2026-01-14 01:50:45,910: t15.2023.11.19 val PER: 0.0419
2026-01-14 01:50:45,910: t15.2023.11.26 val PER: 0.1601
2026-01-14 01:50:45,910: t15.2023.12.03 val PER: 0.1313
2026-01-14 01:50:45,910: t15.2023.12.08 val PER: 0.1292
2026-01-14 01:50:45,910: t15.2023.12.10 val PER: 0.1196
2026-01-14 01:50:45,910: t15.2023.12.17 val PER: 0.1570
2026-01-14 01:50:45,910: t15.2023.12.29 val PER: 0.1565
2026-01-14 01:50:45,910: t15.2024.02.25 val PER: 0.1601
2026-01-14 01:50:45,910: t15.2024.03.08 val PER: 0.2603
2026-01-14 01:50:45,910: t15.2024.03.15 val PER: 0.2276
2026-01-14 01:50:45,910: t15.2024.03.17 val PER: 0.1736
2026-01-14 01:50:45,910: t15.2024.05.10 val PER: 0.1917
2026-01-14 01:50:45,910: t15.2024.06.14 val PER: 0.2003
2026-01-14 01:50:45,910: t15.2024.07.19 val PER: 0.2584
2026-01-14 01:50:45,910: t15.2024.07.21 val PER: 0.1345
2026-01-14 01:50:45,911: t15.2024.07.28 val PER: 0.1787
2026-01-14 01:50:45,911: t15.2025.01.10 val PER: 0.3347
2026-01-14 01:50:45,911: t15.2025.01.12 val PER: 0.1886
2026-01-14 01:50:45,911: t15.2025.03.14 val PER: 0.3595
2026-01-14 01:50:45,911: t15.2025.03.16 val PER: 0.2016
2026-01-14 01:50:45,911: t15.2025.03.30 val PER: 0.3103
2026-01-14 01:50:45,911: t15.2025.04.13 val PER: 0.2397
2026-01-14 01:50:45,912: New best val WER(5gram) 18.58% --> 15.84%
2026-01-14 01:50:46,059: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_8500
2026-01-14 01:50:54,836: Train batch 8600: loss: 22.49 grad norm: 78.31 time: 0.064
2026-01-14 01:51:12,840: Train batch 8800: loss: 23.98 grad norm: 83.29 time: 0.073
2026-01-14 01:51:31,139: Train batch 9000: loss: 24.51 grad norm: 93.17 time: 0.083
2026-01-14 01:51:31,139: Running test after training batch: 9000
2026-01-14 01:51:31,243: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:51:36,807: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:51:36,869: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-14 01:51:47,983: Val batch 9000: PER (avg): 0.1772 CTC Loss (avg): 29.6720 WER(5gram): 16.88% (n=256) time: 16.844
2026-01-14 01:51:47,984: WER lens: avg_true_words=5.99 avg_pred_words=5.98 max_pred_words=11
2026-01-14 01:51:47,984: t15.2023.08.13 val PER: 0.1538
2026-01-14 01:51:47,984: t15.2023.08.18 val PER: 0.1492
2026-01-14 01:51:47,984: t15.2023.08.20 val PER: 0.1287
2026-01-14 01:51:47,984: t15.2023.08.25 val PER: 0.1160
2026-01-14 01:51:47,985: t15.2023.08.27 val PER: 0.2090
2026-01-14 01:51:47,985: t15.2023.09.01 val PER: 0.0998
2026-01-14 01:51:47,985: t15.2023.09.03 val PER: 0.1805
2026-01-14 01:51:47,985: t15.2023.09.24 val PER: 0.1456
2026-01-14 01:51:47,985: t15.2023.09.29 val PER: 0.1646
2026-01-14 01:51:47,985: t15.2023.10.01 val PER: 0.2259
2026-01-14 01:51:47,985: t15.2023.10.06 val PER: 0.1087
2026-01-14 01:51:47,985: t15.2023.10.08 val PER: 0.3018
2026-01-14 01:51:47,986: t15.2023.10.13 val PER: 0.2576
2026-01-14 01:51:47,986: t15.2023.10.15 val PER: 0.1866
2026-01-14 01:51:47,986: t15.2023.10.20 val PER: 0.2450
2026-01-14 01:51:47,986: t15.2023.10.22 val PER: 0.1459
2026-01-14 01:51:47,986: t15.2023.11.03 val PER: 0.1967
2026-01-14 01:51:47,986: t15.2023.11.04 val PER: 0.0273
2026-01-14 01:51:47,986: t15.2023.11.17 val PER: 0.0451
2026-01-14 01:51:47,986: t15.2023.11.19 val PER: 0.0439
2026-01-14 01:51:47,986: t15.2023.11.26 val PER: 0.1529
2026-01-14 01:51:47,987: t15.2023.12.03 val PER: 0.1250
2026-01-14 01:51:47,987: t15.2023.12.08 val PER: 0.1258
2026-01-14 01:51:47,987: t15.2023.12.10 val PER: 0.1156
2026-01-14 01:51:47,987: t15.2023.12.17 val PER: 0.1258
2026-01-14 01:51:47,987: t15.2023.12.29 val PER: 0.1503
2026-01-14 01:51:47,987: t15.2024.02.25 val PER: 0.1615
2026-01-14 01:51:47,987: t15.2024.03.08 val PER: 0.2504
2026-01-14 01:51:47,987: t15.2024.03.15 val PER: 0.2226
2026-01-14 01:51:47,987: t15.2024.03.17 val PER: 0.1702
2026-01-14 01:51:47,987: t15.2024.05.10 val PER: 0.1768
2026-01-14 01:51:47,987: t15.2024.06.14 val PER: 0.1814
2026-01-14 01:51:47,988: t15.2024.07.19 val PER: 0.2676
2026-01-14 01:51:47,988: t15.2024.07.21 val PER: 0.1186
2026-01-14 01:51:47,988: t15.2024.07.28 val PER: 0.1765
2026-01-14 01:51:47,988: t15.2025.01.10 val PER: 0.3030
2026-01-14 01:51:47,988: t15.2025.01.12 val PER: 0.1878
2026-01-14 01:51:47,988: t15.2025.03.14 val PER: 0.3254
2026-01-14 01:51:47,988: t15.2025.03.16 val PER: 0.2029
2026-01-14 01:51:47,988: t15.2025.03.30 val PER: 0.2885
2026-01-14 01:51:47,988: t15.2025.04.13 val PER: 0.2468
2026-01-14 01:51:48,129: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_9000
2026-01-14 01:52:05,676: Train batch 9200: loss: 15.49 grad norm: 68.74 time: 0.065
2026-01-14 01:52:23,078: Train batch 9400: loss: 9.71 grad norm: 54.39 time: 0.078
2026-01-14 01:52:31,916: Running test after training batch: 9500
2026-01-14 01:52:32,063: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:52:37,542: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:52:37,592: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-14 01:52:48,545: Val batch 9500: PER (avg): 0.1708 CTC Loss (avg): 29.1826 WER(5gram): 15.19% (n=256) time: 16.629
2026-01-14 01:52:48,545: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-14 01:52:48,546: t15.2023.08.13 val PER: 0.1476
2026-01-14 01:52:48,546: t15.2023.08.18 val PER: 0.1341
2026-01-14 01:52:48,546: t15.2023.08.20 val PER: 0.1152
2026-01-14 01:52:48,546: t15.2023.08.25 val PER: 0.1069
2026-01-14 01:52:48,546: t15.2023.08.27 val PER: 0.2010
2026-01-14 01:52:48,546: t15.2023.09.01 val PER: 0.0933
2026-01-14 01:52:48,546: t15.2023.09.03 val PER: 0.1793
2026-01-14 01:52:48,546: t15.2023.09.24 val PER: 0.1274
2026-01-14 01:52:48,546: t15.2023.09.29 val PER: 0.1627
2026-01-14 01:52:48,546: t15.2023.10.01 val PER: 0.2094
2026-01-14 01:52:48,546: t15.2023.10.06 val PER: 0.1066
2026-01-14 01:52:48,546: t15.2023.10.08 val PER: 0.2963
2026-01-14 01:52:48,546: t15.2023.10.13 val PER: 0.2483
2026-01-14 01:52:48,546: t15.2023.10.15 val PER: 0.1846
2026-01-14 01:52:48,547: t15.2023.10.20 val PER: 0.2215
2026-01-14 01:52:48,547: t15.2023.10.22 val PER: 0.1492
2026-01-14 01:52:48,547: t15.2023.11.03 val PER: 0.1906
2026-01-14 01:52:48,547: t15.2023.11.04 val PER: 0.0375
2026-01-14 01:52:48,547: t15.2023.11.17 val PER: 0.0513
2026-01-14 01:52:48,547: t15.2023.11.19 val PER: 0.0459
2026-01-14 01:52:48,547: t15.2023.11.26 val PER: 0.1384
2026-01-14 01:52:48,547: t15.2023.12.03 val PER: 0.1355
2026-01-14 01:52:48,547: t15.2023.12.08 val PER: 0.1225
2026-01-14 01:52:48,547: t15.2023.12.10 val PER: 0.1170
2026-01-14 01:52:48,547: t15.2023.12.17 val PER: 0.1320
2026-01-14 01:52:48,547: t15.2023.12.29 val PER: 0.1448
2026-01-14 01:52:48,547: t15.2024.02.25 val PER: 0.1461
2026-01-14 01:52:48,547: t15.2024.03.08 val PER: 0.2418
2026-01-14 01:52:48,548: t15.2024.03.15 val PER: 0.2176
2026-01-14 01:52:48,548: t15.2024.03.17 val PER: 0.1681
2026-01-14 01:52:48,548: t15.2024.05.10 val PER: 0.1679
2026-01-14 01:52:48,548: t15.2024.06.14 val PER: 0.1845
2026-01-14 01:52:48,548: t15.2024.07.19 val PER: 0.2406
2026-01-14 01:52:48,548: t15.2024.07.21 val PER: 0.1221
2026-01-14 01:52:48,548: t15.2024.07.28 val PER: 0.1618
2026-01-14 01:52:48,548: t15.2025.01.10 val PER: 0.2934
2026-01-14 01:52:48,548: t15.2025.01.12 val PER: 0.1763
2026-01-14 01:52:48,548: t15.2025.03.14 val PER: 0.3447
2026-01-14 01:52:48,548: t15.2025.03.16 val PER: 0.1950
2026-01-14 01:52:48,548: t15.2025.03.30 val PER: 0.2839
2026-01-14 01:52:48,548: t15.2025.04.13 val PER: 0.2282
2026-01-14 01:52:48,550: New best val WER(5gram) 15.84% --> 15.19%
2026-01-14 01:52:48,698: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_9500
2026-01-14 01:52:57,789: Train batch 9600: loss: 13.22 grad norm: 75.44 time: 0.083
2026-01-14 01:53:15,420: Train batch 9800: loss: 17.09 grad norm: 81.34 time: 0.073
2026-01-14 01:53:33,060: Train batch 10000: loss: 6.80 grad norm: 45.19 time: 0.069
2026-01-14 01:53:33,061: Running test after training batch: 10000
2026-01-14 01:53:33,183: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:53:38,862: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:53:38,909: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-14 01:53:49,525: Val batch 10000: PER (avg): 0.1713 CTC Loss (avg): 28.4232 WER(5gram): 14.93% (n=256) time: 16.464
2026-01-14 01:53:49,526: WER lens: avg_true_words=5.99 avg_pred_words=6.00 max_pred_words=12
2026-01-14 01:53:49,526: t15.2023.08.13 val PER: 0.1362
2026-01-14 01:53:49,526: t15.2023.08.18 val PER: 0.1241
2026-01-14 01:53:49,526: t15.2023.08.20 val PER: 0.1207
2026-01-14 01:53:49,526: t15.2023.08.25 val PER: 0.1069
2026-01-14 01:53:49,526: t15.2023.08.27 val PER: 0.2074
2026-01-14 01:53:49,526: t15.2023.09.01 val PER: 0.0990
2026-01-14 01:53:49,526: t15.2023.09.03 val PER: 0.1793
2026-01-14 01:53:49,526: t15.2023.09.24 val PER: 0.1408
2026-01-14 01:53:49,527: t15.2023.09.29 val PER: 0.1487
2026-01-14 01:53:49,527: t15.2023.10.01 val PER: 0.2127
2026-01-14 01:53:49,527: t15.2023.10.06 val PER: 0.1163
2026-01-14 01:53:49,527: t15.2023.10.08 val PER: 0.2963
2026-01-14 01:53:49,527: t15.2023.10.13 val PER: 0.2483
2026-01-14 01:53:49,527: t15.2023.10.15 val PER: 0.1760
2026-01-14 01:53:49,527: t15.2023.10.20 val PER: 0.2282
2026-01-14 01:53:49,527: t15.2023.10.22 val PER: 0.1459
2026-01-14 01:53:49,527: t15.2023.11.03 val PER: 0.1967
2026-01-14 01:53:49,527: t15.2023.11.04 val PER: 0.0341
2026-01-14 01:53:49,527: t15.2023.11.17 val PER: 0.0327
2026-01-14 01:53:49,527: t15.2023.11.19 val PER: 0.0319
2026-01-14 01:53:49,527: t15.2023.11.26 val PER: 0.1196
2026-01-14 01:53:49,528: t15.2023.12.03 val PER: 0.1250
2026-01-14 01:53:49,528: t15.2023.12.08 val PER: 0.1099
2026-01-14 01:53:49,528: t15.2023.12.10 val PER: 0.1222
2026-01-14 01:53:49,528: t15.2023.12.17 val PER: 0.1351
2026-01-14 01:53:49,528: t15.2023.12.29 val PER: 0.1407
2026-01-14 01:53:49,528: t15.2024.02.25 val PER: 0.1587
2026-01-14 01:53:49,528: t15.2024.03.08 val PER: 0.2304
2026-01-14 01:53:49,528: t15.2024.03.15 val PER: 0.2170
2026-01-14 01:53:49,528: t15.2024.03.17 val PER: 0.1736
2026-01-14 01:53:49,528: t15.2024.05.10 val PER: 0.1783
2026-01-14 01:53:49,528: t15.2024.06.14 val PER: 0.1830
2026-01-14 01:53:49,528: t15.2024.07.19 val PER: 0.2512
2026-01-14 01:53:49,528: t15.2024.07.21 val PER: 0.1255
2026-01-14 01:53:49,528: t15.2024.07.28 val PER: 0.1735
2026-01-14 01:53:49,528: t15.2025.01.10 val PER: 0.3044
2026-01-14 01:53:49,529: t15.2025.01.12 val PER: 0.1794
2026-01-14 01:53:49,529: t15.2025.03.14 val PER: 0.3565
2026-01-14 01:53:49,529: t15.2025.03.16 val PER: 0.2186
2026-01-14 01:53:49,529: t15.2025.03.30 val PER: 0.2851
2026-01-14 01:53:49,529: t15.2025.04.13 val PER: 0.2482
2026-01-14 01:53:49,530: New best val WER(5gram) 15.19% --> 14.93%
2026-01-14 01:53:49,677: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_10000
2026-01-14 01:54:07,298: Train batch 10200: loss: 10.66 grad norm: 57.88 time: 0.058
2026-01-14 01:54:25,180: Train batch 10400: loss: 12.04 grad norm: 61.55 time: 0.082
2026-01-14 01:54:34,420: Running test after training batch: 10500
2026-01-14 01:54:34,552: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:54:40,160: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:54:40,212: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-14 01:54:50,565: Val batch 10500: PER (avg): 0.1641 CTC Loss (avg): 27.8392 WER(5gram): 14.73% (n=256) time: 16.144
2026-01-14 01:54:50,565: WER lens: avg_true_words=5.99 avg_pred_words=6.02 max_pred_words=12
2026-01-14 01:54:50,565: t15.2023.08.13 val PER: 0.1341
2026-01-14 01:54:50,565: t15.2023.08.18 val PER: 0.1299
2026-01-14 01:54:50,566: t15.2023.08.20 val PER: 0.1152
2026-01-14 01:54:50,566: t15.2023.08.25 val PER: 0.1130
2026-01-14 01:54:50,566: t15.2023.08.27 val PER: 0.2010
2026-01-14 01:54:50,566: t15.2023.09.01 val PER: 0.0885
2026-01-14 01:54:50,566: t15.2023.09.03 val PER: 0.1829
2026-01-14 01:54:50,566: t15.2023.09.24 val PER: 0.1274
2026-01-14 01:54:50,566: t15.2023.09.29 val PER: 0.1468
2026-01-14 01:54:50,566: t15.2023.10.01 val PER: 0.2074
2026-01-14 01:54:50,566: t15.2023.10.06 val PER: 0.0990
2026-01-14 01:54:50,566: t15.2023.10.08 val PER: 0.2855
2026-01-14 01:54:50,566: t15.2023.10.13 val PER: 0.2304
2026-01-14 01:54:50,566: t15.2023.10.15 val PER: 0.1800
2026-01-14 01:54:50,567: t15.2023.10.20 val PER: 0.2047
2026-01-14 01:54:50,567: t15.2023.10.22 val PER: 0.1336
2026-01-14 01:54:50,567: t15.2023.11.03 val PER: 0.1906
2026-01-14 01:54:50,567: t15.2023.11.04 val PER: 0.0273
2026-01-14 01:54:50,567: t15.2023.11.17 val PER: 0.0420
2026-01-14 01:54:50,567: t15.2023.11.19 val PER: 0.0479
2026-01-14 01:54:50,567: t15.2023.11.26 val PER: 0.1203
2026-01-14 01:54:50,567: t15.2023.12.03 val PER: 0.1155
2026-01-14 01:54:50,567: t15.2023.12.08 val PER: 0.1059
2026-01-14 01:54:50,567: t15.2023.12.10 val PER: 0.0933
2026-01-14 01:54:50,567: t15.2023.12.17 val PER: 0.1320
2026-01-14 01:54:50,567: t15.2023.12.29 val PER: 0.1359
2026-01-14 01:54:50,568: t15.2024.02.25 val PER: 0.1419
2026-01-14 01:54:50,568: t15.2024.03.08 val PER: 0.2248
2026-01-14 01:54:50,568: t15.2024.03.15 val PER: 0.2170
2026-01-14 01:54:50,568: t15.2024.03.17 val PER: 0.1646
2026-01-14 01:54:50,568: t15.2024.05.10 val PER: 0.1768
2026-01-14 01:54:50,568: t15.2024.06.14 val PER: 0.1814
2026-01-14 01:54:50,568: t15.2024.07.19 val PER: 0.2347
2026-01-14 01:54:50,568: t15.2024.07.21 val PER: 0.1117
2026-01-14 01:54:50,568: t15.2024.07.28 val PER: 0.1463
2026-01-14 01:54:50,568: t15.2025.01.10 val PER: 0.3140
2026-01-14 01:54:50,568: t15.2025.01.12 val PER: 0.1678
2026-01-14 01:54:50,568: t15.2025.03.14 val PER: 0.3565
2026-01-14 01:54:50,568: t15.2025.03.16 val PER: 0.1780
2026-01-14 01:54:50,568: t15.2025.03.30 val PER: 0.2874
2026-01-14 01:54:50,569: t15.2025.04.13 val PER: 0.2297
2026-01-14 01:54:50,570: New best val WER(5gram) 14.93% --> 14.73%
2026-01-14 01:54:50,717: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_10500
2026-01-14 01:55:00,145: Train batch 10600: loss: 12.39 grad norm: 73.16 time: 0.082
2026-01-14 01:55:17,900: Train batch 10800: loss: 21.05 grad norm: 86.33 time: 0.075
2026-01-14 01:55:35,724: Train batch 11000: loss: 18.02 grad norm: 77.27 time: 0.067
2026-01-14 01:55:35,724: Running test after training batch: 11000
2026-01-14 01:55:35,865: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:55:41,890: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:55:41,941: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost now
2026-01-14 01:55:52,319: Val batch 11000: PER (avg): 0.1607 CTC Loss (avg): 27.7593 WER(5gram): 14.67% (n=256) time: 16.595
2026-01-14 01:55:52,319: WER lens: avg_true_words=5.99 avg_pred_words=6.04 max_pred_words=12
2026-01-14 01:55:52,319: t15.2023.08.13 val PER: 0.1289
2026-01-14 01:55:52,319: t15.2023.08.18 val PER: 0.1324
2026-01-14 01:55:52,320: t15.2023.08.20 val PER: 0.1041
2026-01-14 01:55:52,320: t15.2023.08.25 val PER: 0.0979
2026-01-14 01:55:52,320: t15.2023.08.27 val PER: 0.1961
2026-01-14 01:55:52,320: t15.2023.09.01 val PER: 0.0877
2026-01-14 01:55:52,320: t15.2023.09.03 val PER: 0.1698
2026-01-14 01:55:52,320: t15.2023.09.24 val PER: 0.1250
2026-01-14 01:55:52,320: t15.2023.09.29 val PER: 0.1506
2026-01-14 01:55:52,320: t15.2023.10.01 val PER: 0.1988
2026-01-14 01:55:52,320: t15.2023.10.06 val PER: 0.0958
2026-01-14 01:55:52,320: t15.2023.10.08 val PER: 0.2747
2026-01-14 01:55:52,320: t15.2023.10.13 val PER: 0.2335
2026-01-14 01:55:52,321: t15.2023.10.15 val PER: 0.1740
2026-01-14 01:55:52,321: t15.2023.10.20 val PER: 0.2148
2026-01-14 01:55:52,321: t15.2023.10.22 val PER: 0.1347
2026-01-14 01:55:52,321: t15.2023.11.03 val PER: 0.1852
2026-01-14 01:55:52,321: t15.2023.11.04 val PER: 0.0307
2026-01-14 01:55:52,321: t15.2023.11.17 val PER: 0.0373
2026-01-14 01:55:52,321: t15.2023.11.19 val PER: 0.0379
2026-01-14 01:55:52,321: t15.2023.11.26 val PER: 0.1014
2026-01-14 01:55:52,321: t15.2023.12.03 val PER: 0.1103
2026-01-14 01:55:52,321: t15.2023.12.08 val PER: 0.1092
2026-01-14 01:55:52,321: t15.2023.12.10 val PER: 0.0920
2026-01-14 01:55:52,321: t15.2023.12.17 val PER: 0.1216
2026-01-14 01:55:52,321: t15.2023.12.29 val PER: 0.1421
2026-01-14 01:55:52,321: t15.2024.02.25 val PER: 0.1362
2026-01-14 01:55:52,321: t15.2024.03.08 val PER: 0.2447
2026-01-14 01:55:52,321: t15.2024.03.15 val PER: 0.2239
2026-01-14 01:55:52,321: t15.2024.03.17 val PER: 0.1555
2026-01-14 01:55:52,322: t15.2024.05.10 val PER: 0.1605
2026-01-14 01:55:52,322: t15.2024.06.14 val PER: 0.1609
2026-01-14 01:55:52,322: t15.2024.07.19 val PER: 0.2406
2026-01-14 01:55:52,322: t15.2024.07.21 val PER: 0.1090
2026-01-14 01:55:52,322: t15.2024.07.28 val PER: 0.1537
2026-01-14 01:55:52,322: t15.2025.01.10 val PER: 0.2906
2026-01-14 01:55:52,322: t15.2025.01.12 val PER: 0.1586
2026-01-14 01:55:52,322: t15.2025.03.14 val PER: 0.3299
2026-01-14 01:55:52,322: t15.2025.03.16 val PER: 0.2003
2026-01-14 01:55:52,322: t15.2025.03.30 val PER: 0.2931
2026-01-14 01:55:52,322: t15.2025.04.13 val PER: 0.2168
2026-01-14 01:55:52,324: New best val WER(5gram) 14.73% --> 14.67%
2026-01-14 01:55:52,480: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_11000
2026-01-14 01:56:10,271: Train batch 11200: loss: 16.52 grad norm: 79.22 time: 0.082
2026-01-14 01:56:27,846: Train batch 11400: loss: 14.42 grad norm: 71.45 time: 0.066
2026-01-14 01:56:36,994: Running test after training batch: 11500
2026-01-14 01:56:37,158: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:56:42,586: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:56:42,634: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost now
2026-01-14 01:56:53,002: Val batch 11500: PER (avg): 0.1589 CTC Loss (avg): 27.6266 WER(5gram): 15.32% (n=256) time: 16.007
2026-01-14 01:56:53,003: WER lens: avg_true_words=5.99 avg_pred_words=6.04 max_pred_words=11
2026-01-14 01:56:53,003: t15.2023.08.13 val PER: 0.1237
2026-01-14 01:56:53,003: t15.2023.08.18 val PER: 0.1291
2026-01-14 01:56:53,003: t15.2023.08.20 val PER: 0.1104
2026-01-14 01:56:53,003: t15.2023.08.25 val PER: 0.1054
2026-01-14 01:56:53,003: t15.2023.08.27 val PER: 0.1994
2026-01-14 01:56:53,003: t15.2023.09.01 val PER: 0.0836
2026-01-14 01:56:53,003: t15.2023.09.03 val PER: 0.1627
2026-01-14 01:56:53,003: t15.2023.09.24 val PER: 0.1177
2026-01-14 01:56:53,003: t15.2023.09.29 val PER: 0.1506
2026-01-14 01:56:53,003: t15.2023.10.01 val PER: 0.1962
2026-01-14 01:56:53,003: t15.2023.10.06 val PER: 0.1033
2026-01-14 01:56:53,003: t15.2023.10.08 val PER: 0.2788
2026-01-14 01:56:53,003: t15.2023.10.13 val PER: 0.2304
2026-01-14 01:56:53,004: t15.2023.10.15 val PER: 0.1688
2026-01-14 01:56:53,004: t15.2023.10.20 val PER: 0.2181
2026-01-14 01:56:53,004: t15.2023.10.22 val PER: 0.1247
2026-01-14 01:56:53,004: t15.2023.11.03 val PER: 0.1879
2026-01-14 01:56:53,004: t15.2023.11.04 val PER: 0.0307
2026-01-14 01:56:53,004: t15.2023.11.17 val PER: 0.0404
2026-01-14 01:56:53,004: t15.2023.11.19 val PER: 0.0399
2026-01-14 01:56:53,004: t15.2023.11.26 val PER: 0.1072
2026-01-14 01:56:53,004: t15.2023.12.03 val PER: 0.0935
2026-01-14 01:56:53,005: t15.2023.12.08 val PER: 0.0919
2026-01-14 01:56:53,005: t15.2023.12.10 val PER: 0.0933
2026-01-14 01:56:53,005: t15.2023.12.17 val PER: 0.1258
2026-01-14 01:56:53,005: t15.2023.12.29 val PER: 0.1283
2026-01-14 01:56:53,005: t15.2024.02.25 val PER: 0.1433
2026-01-14 01:56:53,005: t15.2024.03.08 val PER: 0.2319
2026-01-14 01:56:53,005: t15.2024.03.15 val PER: 0.2120
2026-01-14 01:56:53,005: t15.2024.03.17 val PER: 0.1513
2026-01-14 01:56:53,005: t15.2024.05.10 val PER: 0.1738
2026-01-14 01:56:53,005: t15.2024.06.14 val PER: 0.1751
2026-01-14 01:56:53,005: t15.2024.07.19 val PER: 0.2446
2026-01-14 01:56:53,005: t15.2024.07.21 val PER: 0.1034
2026-01-14 01:56:53,005: t15.2024.07.28 val PER: 0.1485
2026-01-14 01:56:53,005: t15.2025.01.10 val PER: 0.3127
2026-01-14 01:56:53,006: t15.2025.01.12 val PER: 0.1671
2026-01-14 01:56:53,006: t15.2025.03.14 val PER: 0.3328
2026-01-14 01:56:53,006: t15.2025.03.16 val PER: 0.1990
2026-01-14 01:56:53,006: t15.2025.03.30 val PER: 0.2678
2026-01-14 01:56:53,006: t15.2025.04.13 val PER: 0.2240
2026-01-14 01:56:53,147: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_11500
2026-01-14 01:57:01,741: Train batch 11600: loss: 14.54 grad norm: 68.37 time: 0.071
2026-01-14 01:57:19,109: Train batch 11800: loss: 9.17 grad norm: 63.19 time: 0.052
2026-01-14 01:57:36,528: Train batch 12000: loss: 19.20 grad norm: 80.58 time: 0.082
2026-01-14 01:57:36,528: Running test after training batch: 12000
2026-01-14 01:57:36,631: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:57:42,273: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:57:42,328: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost to
2026-01-14 01:57:52,784: Val batch 12000: PER (avg): 0.1510 CTC Loss (avg): 26.7363 WER(5gram): 13.36% (n=256) time: 16.256
2026-01-14 01:57:52,785: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-14 01:57:52,785: t15.2023.08.13 val PER: 0.1331
2026-01-14 01:57:52,785: t15.2023.08.18 val PER: 0.1065
2026-01-14 01:57:52,785: t15.2023.08.20 val PER: 0.1064
2026-01-14 01:57:52,785: t15.2023.08.25 val PER: 0.1009
2026-01-14 01:57:52,785: t15.2023.08.27 val PER: 0.1785
2026-01-14 01:57:52,785: t15.2023.09.01 val PER: 0.0860
2026-01-14 01:57:52,785: t15.2023.09.03 val PER: 0.1520
2026-01-14 01:57:52,786: t15.2023.09.24 val PER: 0.1165
2026-01-14 01:57:52,786: t15.2023.09.29 val PER: 0.1442
2026-01-14 01:57:52,786: t15.2023.10.01 val PER: 0.1816
2026-01-14 01:57:52,786: t15.2023.10.06 val PER: 0.0861
2026-01-14 01:57:52,786: t15.2023.10.08 val PER: 0.2625
2026-01-14 01:57:52,786: t15.2023.10.13 val PER: 0.2227
2026-01-14 01:57:52,786: t15.2023.10.15 val PER: 0.1549
2026-01-14 01:57:52,786: t15.2023.10.20 val PER: 0.2181
2026-01-14 01:57:52,787: t15.2023.10.22 val PER: 0.1303
2026-01-14 01:57:52,787: t15.2023.11.03 val PER: 0.1825
2026-01-14 01:57:52,787: t15.2023.11.04 val PER: 0.0307
2026-01-14 01:57:52,787: t15.2023.11.17 val PER: 0.0404
2026-01-14 01:57:52,787: t15.2023.11.19 val PER: 0.0180
2026-01-14 01:57:52,787: t15.2023.11.26 val PER: 0.0942
2026-01-14 01:57:52,787: t15.2023.12.03 val PER: 0.0945
2026-01-14 01:57:52,787: t15.2023.12.08 val PER: 0.0999
2026-01-14 01:57:52,788: t15.2023.12.10 val PER: 0.0775
2026-01-14 01:57:52,788: t15.2023.12.17 val PER: 0.1237
2026-01-14 01:57:52,788: t15.2023.12.29 val PER: 0.1263
2026-01-14 01:57:52,788: t15.2024.02.25 val PER: 0.1250
2026-01-14 01:57:52,788: t15.2024.03.08 val PER: 0.2148
2026-01-14 01:57:52,788: t15.2024.03.15 val PER: 0.2045
2026-01-14 01:57:52,788: t15.2024.03.17 val PER: 0.1478
2026-01-14 01:57:52,788: t15.2024.05.10 val PER: 0.1590
2026-01-14 01:57:52,788: t15.2024.06.14 val PER: 0.1672
2026-01-14 01:57:52,789: t15.2024.07.19 val PER: 0.2274
2026-01-14 01:57:52,789: t15.2024.07.21 val PER: 0.1055
2026-01-14 01:57:52,789: t15.2024.07.28 val PER: 0.1449
2026-01-14 01:57:52,789: t15.2025.01.10 val PER: 0.2810
2026-01-14 01:57:52,789: t15.2025.01.12 val PER: 0.1517
2026-01-14 01:57:52,789: t15.2025.03.14 val PER: 0.3210
2026-01-14 01:57:52,789: t15.2025.03.16 val PER: 0.1924
2026-01-14 01:57:52,789: t15.2025.03.30 val PER: 0.2655
2026-01-14 01:57:52,789: t15.2025.04.13 val PER: 0.2083
2026-01-14 01:57:52,790: New best val WER(5gram) 14.67% --> 13.36%
2026-01-14 01:57:52,940: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_12000
2026-01-14 01:58:10,542: Train batch 12200: loss: 6.61 grad norm: 50.86 time: 0.076
2026-01-14 01:58:27,958: Train batch 12400: loss: 6.92 grad norm: 51.38 time: 0.048
2026-01-14 01:58:36,856: Running test after training batch: 12500
2026-01-14 01:58:37,059: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:58:42,474: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:58:42,520: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost
2026-01-14 01:58:52,894: Val batch 12500: PER (avg): 0.1484 CTC Loss (avg): 26.5665 WER(5gram): 14.02% (n=256) time: 16.038
2026-01-14 01:58:52,895: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-14 01:58:52,895: t15.2023.08.13 val PER: 0.1154
2026-01-14 01:58:52,895: t15.2023.08.18 val PER: 0.1140
2026-01-14 01:58:52,895: t15.2023.08.20 val PER: 0.1001
2026-01-14 01:58:52,895: t15.2023.08.25 val PER: 0.0873
2026-01-14 01:58:52,895: t15.2023.08.27 val PER: 0.1833
2026-01-14 01:58:52,895: t15.2023.09.01 val PER: 0.0755
2026-01-14 01:58:52,895: t15.2023.09.03 val PER: 0.1485
2026-01-14 01:58:52,896: t15.2023.09.24 val PER: 0.1129
2026-01-14 01:58:52,896: t15.2023.09.29 val PER: 0.1359
2026-01-14 01:58:52,896: t15.2023.10.01 val PER: 0.1704
2026-01-14 01:58:52,896: t15.2023.10.06 val PER: 0.0840
2026-01-14 01:58:52,896: t15.2023.10.08 val PER: 0.2571
2026-01-14 01:58:52,896: t15.2023.10.13 val PER: 0.2250
2026-01-14 01:58:52,896: t15.2023.10.15 val PER: 0.1536
2026-01-14 01:58:52,896: t15.2023.10.20 val PER: 0.2081
2026-01-14 01:58:52,896: t15.2023.10.22 val PER: 0.1269
2026-01-14 01:58:52,897: t15.2023.11.03 val PER: 0.1852
2026-01-14 01:58:52,897: t15.2023.11.04 val PER: 0.0341
2026-01-14 01:58:52,897: t15.2023.11.17 val PER: 0.0358
2026-01-14 01:58:52,897: t15.2023.11.19 val PER: 0.0339
2026-01-14 01:58:52,897: t15.2023.11.26 val PER: 0.1000
2026-01-14 01:58:52,897: t15.2023.12.03 val PER: 0.1008
2026-01-14 01:58:52,897: t15.2023.12.08 val PER: 0.0872
2026-01-14 01:58:52,897: t15.2023.12.10 val PER: 0.0762
2026-01-14 01:58:52,898: t15.2023.12.17 val PER: 0.0988
2026-01-14 01:58:52,898: t15.2023.12.29 val PER: 0.1304
2026-01-14 01:58:52,898: t15.2024.02.25 val PER: 0.1362
2026-01-14 01:58:52,898: t15.2024.03.08 val PER: 0.2262
2026-01-14 01:58:52,898: t15.2024.03.15 val PER: 0.2014
2026-01-14 01:58:52,898: t15.2024.03.17 val PER: 0.1444
2026-01-14 01:58:52,898: t15.2024.05.10 val PER: 0.1471
2026-01-14 01:58:52,898: t15.2024.06.14 val PER: 0.1688
2026-01-14 01:58:52,898: t15.2024.07.19 val PER: 0.2307
2026-01-14 01:58:52,898: t15.2024.07.21 val PER: 0.1007
2026-01-14 01:58:52,898: t15.2024.07.28 val PER: 0.1346
2026-01-14 01:58:52,899: t15.2025.01.10 val PER: 0.3017
2026-01-14 01:58:52,899: t15.2025.01.12 val PER: 0.1517
2026-01-14 01:58:52,899: t15.2025.03.14 val PER: 0.3240
2026-01-14 01:58:52,899: t15.2025.03.16 val PER: 0.1597
2026-01-14 01:58:52,899: t15.2025.03.30 val PER: 0.2713
2026-01-14 01:58:52,899: t15.2025.04.13 val PER: 0.2197
2026-01-14 01:58:53,040: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_12500
2026-01-14 01:59:01,792: Train batch 12600: loss: 10.45 grad norm: 59.05 time: 0.065
2026-01-14 01:59:20,142: Train batch 12800: loss: 6.25 grad norm: 46.41 time: 0.060
2026-01-14 01:59:37,833: Train batch 13000: loss: 7.16 grad norm: 56.90 time: 0.078
2026-01-14 01:59:37,834: Running test after training batch: 13000
2026-01-14 01:59:37,951: WER debug GT example: You can see the code at this point as well.
2026-01-14 01:59:43,338: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 01:59:43,390: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost now
2026-01-14 01:59:53,628: Val batch 13000: PER (avg): 0.1451 CTC Loss (avg): 26.3888 WER(5gram): 13.36% (n=256) time: 15.794
2026-01-14 01:59:53,628: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-14 01:59:53,628: t15.2023.08.13 val PER: 0.1154
2026-01-14 01:59:53,629: t15.2023.08.18 val PER: 0.1081
2026-01-14 01:59:53,629: t15.2023.08.20 val PER: 0.1001
2026-01-14 01:59:53,629: t15.2023.08.25 val PER: 0.0964
2026-01-14 01:59:53,629: t15.2023.08.27 val PER: 0.1768
2026-01-14 01:59:53,629: t15.2023.09.01 val PER: 0.0755
2026-01-14 01:59:53,629: t15.2023.09.03 val PER: 0.1698
2026-01-14 01:59:53,629: t15.2023.09.24 val PER: 0.1056
2026-01-14 01:59:53,629: t15.2023.09.29 val PER: 0.1276
2026-01-14 01:59:53,629: t15.2023.10.01 val PER: 0.1764
2026-01-14 01:59:53,629: t15.2023.10.06 val PER: 0.0904
2026-01-14 01:59:53,629: t15.2023.10.08 val PER: 0.2598
2026-01-14 01:59:53,629: t15.2023.10.13 val PER: 0.2133
2026-01-14 01:59:53,630: t15.2023.10.15 val PER: 0.1589
2026-01-14 01:59:53,630: t15.2023.10.20 val PER: 0.2248
2026-01-14 01:59:53,630: t15.2023.10.22 val PER: 0.1281
2026-01-14 01:59:53,630: t15.2023.11.03 val PER: 0.1845
2026-01-14 01:59:53,630: t15.2023.11.04 val PER: 0.0307
2026-01-14 01:59:53,630: t15.2023.11.17 val PER: 0.0311
2026-01-14 01:59:53,630: t15.2023.11.19 val PER: 0.0339
2026-01-14 01:59:53,630: t15.2023.11.26 val PER: 0.0906
2026-01-14 01:59:53,631: t15.2023.12.03 val PER: 0.0903
2026-01-14 01:59:53,631: t15.2023.12.08 val PER: 0.0786
2026-01-14 01:59:53,631: t15.2023.12.10 val PER: 0.0696
2026-01-14 01:59:53,631: t15.2023.12.17 val PER: 0.1164
2026-01-14 01:59:53,631: t15.2023.12.29 val PER: 0.1215
2026-01-14 01:59:53,631: t15.2024.02.25 val PER: 0.1250
2026-01-14 01:59:53,631: t15.2024.03.08 val PER: 0.2333
2026-01-14 01:59:53,631: t15.2024.03.15 val PER: 0.2070
2026-01-14 01:59:53,631: t15.2024.03.17 val PER: 0.1353
2026-01-14 01:59:53,632: t15.2024.05.10 val PER: 0.1486
2026-01-14 01:59:53,632: t15.2024.06.14 val PER: 0.1420
2026-01-14 01:59:53,632: t15.2024.07.19 val PER: 0.2182
2026-01-14 01:59:53,632: t15.2024.07.21 val PER: 0.0903
2026-01-14 01:59:53,632: t15.2024.07.28 val PER: 0.1368
2026-01-14 01:59:53,632: t15.2025.01.10 val PER: 0.2796
2026-01-14 01:59:53,632: t15.2025.01.12 val PER: 0.1370
2026-01-14 01:59:53,632: t15.2025.03.14 val PER: 0.3047
2026-01-14 01:59:53,632: t15.2025.03.16 val PER: 0.1623
2026-01-14 01:59:53,632: t15.2025.03.30 val PER: 0.2609
2026-01-14 01:59:53,632: t15.2025.04.13 val PER: 0.2282
2026-01-14 01:59:53,774: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_13000
2026-01-14 02:00:11,348: Train batch 13200: loss: 13.77 grad norm: 74.06 time: 0.064
2026-01-14 02:00:28,898: Train batch 13400: loss: 9.36 grad norm: 64.68 time: 0.075
2026-01-14 02:00:38,270: Running test after training batch: 13500
2026-01-14 02:00:38,375: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:00:43,800: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:00:43,850: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-14 02:00:54,051: Val batch 13500: PER (avg): 0.1423 CTC Loss (avg): 26.1210 WER(5gram): 11.08% (n=256) time: 15.781
2026-01-14 02:00:54,052: WER lens: avg_true_words=5.99 avg_pred_words=6.04 max_pred_words=11
2026-01-14 02:00:54,052: t15.2023.08.13 val PER: 0.1164
2026-01-14 02:00:54,053: t15.2023.08.18 val PER: 0.1039
2026-01-14 02:00:54,053: t15.2023.08.20 val PER: 0.0921
2026-01-14 02:00:54,053: t15.2023.08.25 val PER: 0.0979
2026-01-14 02:00:54,053: t15.2023.08.27 val PER: 0.1801
2026-01-14 02:00:54,053: t15.2023.09.01 val PER: 0.0714
2026-01-14 02:00:54,053: t15.2023.09.03 val PER: 0.1461
2026-01-14 02:00:54,053: t15.2023.09.24 val PER: 0.1092
2026-01-14 02:00:54,053: t15.2023.09.29 val PER: 0.1359
2026-01-14 02:00:54,053: t15.2023.10.01 val PER: 0.1823
2026-01-14 02:00:54,053: t15.2023.10.06 val PER: 0.0840
2026-01-14 02:00:54,053: t15.2023.10.08 val PER: 0.2544
2026-01-14 02:00:54,053: t15.2023.10.13 val PER: 0.2141
2026-01-14 02:00:54,053: t15.2023.10.15 val PER: 0.1496
2026-01-14 02:00:54,054: t15.2023.10.20 val PER: 0.1980
2026-01-14 02:00:54,054: t15.2023.10.22 val PER: 0.1180
2026-01-14 02:00:54,054: t15.2023.11.03 val PER: 0.1737
2026-01-14 02:00:54,054: t15.2023.11.04 val PER: 0.0341
2026-01-14 02:00:54,054: t15.2023.11.17 val PER: 0.0327
2026-01-14 02:00:54,054: t15.2023.11.19 val PER: 0.0220
2026-01-14 02:00:54,054: t15.2023.11.26 val PER: 0.0862
2026-01-14 02:00:54,054: t15.2023.12.03 val PER: 0.0819
2026-01-14 02:00:54,054: t15.2023.12.08 val PER: 0.0832
2026-01-14 02:00:54,054: t15.2023.12.10 val PER: 0.0775
2026-01-14 02:00:54,054: t15.2023.12.17 val PER: 0.1102
2026-01-14 02:00:54,054: t15.2023.12.29 val PER: 0.1290
2026-01-14 02:00:54,054: t15.2024.02.25 val PER: 0.1222
2026-01-14 02:00:54,055: t15.2024.03.08 val PER: 0.2176
2026-01-14 02:00:54,055: t15.2024.03.15 val PER: 0.2001
2026-01-14 02:00:54,055: t15.2024.03.17 val PER: 0.1234
2026-01-14 02:00:54,055: t15.2024.05.10 val PER: 0.1471
2026-01-14 02:00:54,055: t15.2024.06.14 val PER: 0.1514
2026-01-14 02:00:54,055: t15.2024.07.19 val PER: 0.2274
2026-01-14 02:00:54,055: t15.2024.07.21 val PER: 0.0890
2026-01-14 02:00:54,055: t15.2024.07.28 val PER: 0.1360
2026-01-14 02:00:54,055: t15.2025.01.10 val PER: 0.2658
2026-01-14 02:00:54,055: t15.2025.01.12 val PER: 0.1386
2026-01-14 02:00:54,056: t15.2025.03.14 val PER: 0.3121
2026-01-14 02:00:54,056: t15.2025.03.16 val PER: 0.1584
2026-01-14 02:00:54,056: t15.2025.03.30 val PER: 0.2586
2026-01-14 02:00:54,056: t15.2025.04.13 val PER: 0.2068
2026-01-14 02:00:54,057: New best val WER(5gram) 13.36% --> 11.08%
2026-01-14 02:00:54,210: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_13500
2026-01-14 02:01:03,106: Train batch 13600: loss: 16.06 grad norm: 87.95 time: 0.073
2026-01-14 02:01:20,663: Train batch 13800: loss: 8.74 grad norm: 69.29 time: 0.065
2026-01-14 02:01:38,297: Train batch 14000: loss: 15.27 grad norm: 78.76 time: 0.059
2026-01-14 02:01:38,298: Running test after training batch: 14000
2026-01-14 02:01:38,401: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:01:43,926: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:01:43,986: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost now
2026-01-14 02:01:55,145: Val batch 14000: PER (avg): 0.1390 CTC Loss (avg): 25.5146 WER(5gram): 13.10% (n=256) time: 16.847
2026-01-14 02:01:55,145: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-14 02:01:55,145: t15.2023.08.13 val PER: 0.1040
2026-01-14 02:01:55,146: t15.2023.08.18 val PER: 0.0981
2026-01-14 02:01:55,146: t15.2023.08.20 val PER: 0.0913
2026-01-14 02:01:55,146: t15.2023.08.25 val PER: 0.1009
2026-01-14 02:01:55,146: t15.2023.08.27 val PER: 0.1688
2026-01-14 02:01:55,146: t15.2023.09.01 val PER: 0.0682
2026-01-14 02:01:55,146: t15.2023.09.03 val PER: 0.1520
2026-01-14 02:01:55,146: t15.2023.09.24 val PER: 0.1044
2026-01-14 02:01:55,146: t15.2023.09.29 val PER: 0.1295
2026-01-14 02:01:55,146: t15.2023.10.01 val PER: 0.1697
2026-01-14 02:01:55,146: t15.2023.10.06 val PER: 0.0883
2026-01-14 02:01:55,147: t15.2023.10.08 val PER: 0.2517
2026-01-14 02:01:55,147: t15.2023.10.13 val PER: 0.2079
2026-01-14 02:01:55,147: t15.2023.10.15 val PER: 0.1516
2026-01-14 02:01:55,147: t15.2023.10.20 val PER: 0.2181
2026-01-14 02:01:55,147: t15.2023.10.22 val PER: 0.1180
2026-01-14 02:01:55,147: t15.2023.11.03 val PER: 0.1784
2026-01-14 02:01:55,147: t15.2023.11.04 val PER: 0.0307
2026-01-14 02:01:55,147: t15.2023.11.17 val PER: 0.0342
2026-01-14 02:01:55,148: t15.2023.11.19 val PER: 0.0359
2026-01-14 02:01:55,148: t15.2023.11.26 val PER: 0.0891
2026-01-14 02:01:55,148: t15.2023.12.03 val PER: 0.0840
2026-01-14 02:01:55,148: t15.2023.12.08 val PER: 0.0779
2026-01-14 02:01:55,148: t15.2023.12.10 val PER: 0.0644
2026-01-14 02:01:55,148: t15.2023.12.17 val PER: 0.1133
2026-01-14 02:01:55,148: t15.2023.12.29 val PER: 0.1098
2026-01-14 02:01:55,148: t15.2024.02.25 val PER: 0.1236
2026-01-14 02:01:55,148: t15.2024.03.08 val PER: 0.2205
2026-01-14 02:01:55,149: t15.2024.03.15 val PER: 0.1876
2026-01-14 02:01:55,149: t15.2024.03.17 val PER: 0.1339
2026-01-14 02:01:55,149: t15.2024.05.10 val PER: 0.1367
2026-01-14 02:01:55,149: t15.2024.06.14 val PER: 0.1420
2026-01-14 02:01:55,149: t15.2024.07.19 val PER: 0.2103
2026-01-14 02:01:55,149: t15.2024.07.21 val PER: 0.0834
2026-01-14 02:01:55,149: t15.2024.07.28 val PER: 0.1169
2026-01-14 02:01:55,149: t15.2025.01.10 val PER: 0.2590
2026-01-14 02:01:55,150: t15.2025.01.12 val PER: 0.1470
2026-01-14 02:01:55,150: t15.2025.03.14 val PER: 0.3166
2026-01-14 02:01:55,150: t15.2025.03.16 val PER: 0.1597
2026-01-14 02:01:55,150: t15.2025.03.30 val PER: 0.2529
2026-01-14 02:01:55,150: t15.2025.04.13 val PER: 0.2168
2026-01-14 02:01:55,294: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_14000
2026-01-14 02:02:12,885: Train batch 14200: loss: 10.66 grad norm: 67.05 time: 0.066
2026-01-14 02:02:30,406: Train batch 14400: loss: 8.66 grad norm: 55.88 time: 0.073
2026-01-14 02:02:39,209: Running test after training batch: 14500
2026-01-14 02:02:39,332: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:02:44,775: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:02:44,827: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:02:55,929: Val batch 14500: PER (avg): 0.1407 CTC Loss (avg): 25.9442 WER(5gram): 13.23% (n=256) time: 16.719
2026-01-14 02:02:55,929: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-14 02:02:55,929: t15.2023.08.13 val PER: 0.1123
2026-01-14 02:02:55,929: t15.2023.08.18 val PER: 0.1123
2026-01-14 02:02:55,929: t15.2023.08.20 val PER: 0.0977
2026-01-14 02:02:55,929: t15.2023.08.25 val PER: 0.0889
2026-01-14 02:02:55,930: t15.2023.08.27 val PER: 0.1688
2026-01-14 02:02:55,930: t15.2023.09.01 val PER: 0.0698
2026-01-14 02:02:55,930: t15.2023.09.03 val PER: 0.1425
2026-01-14 02:02:55,930: t15.2023.09.24 val PER: 0.1092
2026-01-14 02:02:55,930: t15.2023.09.29 val PER: 0.1372
2026-01-14 02:02:55,930: t15.2023.10.01 val PER: 0.1724
2026-01-14 02:02:55,930: t15.2023.10.06 val PER: 0.0829
2026-01-14 02:02:55,930: t15.2023.10.08 val PER: 0.2558
2026-01-14 02:02:55,930: t15.2023.10.13 val PER: 0.2064
2026-01-14 02:02:55,930: t15.2023.10.15 val PER: 0.1411
2026-01-14 02:02:55,931: t15.2023.10.20 val PER: 0.2047
2026-01-14 02:02:55,931: t15.2023.10.22 val PER: 0.1381
2026-01-14 02:02:55,931: t15.2023.11.03 val PER: 0.1744
2026-01-14 02:02:55,931: t15.2023.11.04 val PER: 0.0307
2026-01-14 02:02:55,931: t15.2023.11.17 val PER: 0.0373
2026-01-14 02:02:55,931: t15.2023.11.19 val PER: 0.0259
2026-01-14 02:02:55,931: t15.2023.11.26 val PER: 0.0884
2026-01-14 02:02:55,931: t15.2023.12.03 val PER: 0.0914
2026-01-14 02:02:55,931: t15.2023.12.08 val PER: 0.0766
2026-01-14 02:02:55,931: t15.2023.12.10 val PER: 0.0710
2026-01-14 02:02:55,931: t15.2023.12.17 val PER: 0.1154
2026-01-14 02:02:55,932: t15.2023.12.29 val PER: 0.1078
2026-01-14 02:02:55,932: t15.2024.02.25 val PER: 0.1110
2026-01-14 02:02:55,932: t15.2024.03.08 val PER: 0.2418
2026-01-14 02:02:55,932: t15.2024.03.15 val PER: 0.1920
2026-01-14 02:02:55,932: t15.2024.03.17 val PER: 0.1262
2026-01-14 02:02:55,932: t15.2024.05.10 val PER: 0.1456
2026-01-14 02:02:55,932: t15.2024.06.14 val PER: 0.1420
2026-01-14 02:02:55,932: t15.2024.07.19 val PER: 0.2129
2026-01-14 02:02:55,932: t15.2024.07.21 val PER: 0.0972
2026-01-14 02:02:55,932: t15.2024.07.28 val PER: 0.1324
2026-01-14 02:02:55,932: t15.2025.01.10 val PER: 0.2658
2026-01-14 02:02:55,932: t15.2025.01.12 val PER: 0.1409
2026-01-14 02:02:55,932: t15.2025.03.14 val PER: 0.3121
2026-01-14 02:02:55,932: t15.2025.03.16 val PER: 0.1675
2026-01-14 02:02:55,932: t15.2025.03.30 val PER: 0.2379
2026-01-14 02:02:55,932: t15.2025.04.13 val PER: 0.2154
2026-01-14 02:02:56,076: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_14500
2026-01-14 02:03:05,216: Train batch 14600: loss: 12.99 grad norm: 77.28 time: 0.068
2026-01-14 02:03:23,015: Train batch 14800: loss: 7.17 grad norm: 53.88 time: 0.059
2026-01-14 02:03:40,969: Train batch 15000: loss: 10.18 grad norm: 58.92 time: 0.060
2026-01-14 02:03:40,969: Running test after training batch: 15000
2026-01-14 02:03:41,069: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:03:46,499: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:03:46,551: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost now
2026-01-14 02:03:57,718: Val batch 15000: PER (avg): 0.1365 CTC Loss (avg): 25.4248 WER(5gram): 12.19% (n=256) time: 16.748
2026-01-14 02:03:57,718: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-14 02:03:57,718: t15.2023.08.13 val PER: 0.1029
2026-01-14 02:03:57,718: t15.2023.08.18 val PER: 0.0947
2026-01-14 02:03:57,719: t15.2023.08.20 val PER: 0.0874
2026-01-14 02:03:57,719: t15.2023.08.25 val PER: 0.0843
2026-01-14 02:03:57,719: t15.2023.08.27 val PER: 0.1624
2026-01-14 02:03:57,719: t15.2023.09.01 val PER: 0.0568
2026-01-14 02:03:57,719: t15.2023.09.03 val PER: 0.1425
2026-01-14 02:03:57,719: t15.2023.09.24 val PER: 0.1117
2026-01-14 02:03:57,719: t15.2023.09.29 val PER: 0.1327
2026-01-14 02:03:57,719: t15.2023.10.01 val PER: 0.1724
2026-01-14 02:03:57,719: t15.2023.10.06 val PER: 0.0807
2026-01-14 02:03:57,719: t15.2023.10.08 val PER: 0.2530
2026-01-14 02:03:57,719: t15.2023.10.13 val PER: 0.1971
2026-01-14 02:03:57,719: t15.2023.10.15 val PER: 0.1510
2026-01-14 02:03:57,719: t15.2023.10.20 val PER: 0.2047
2026-01-14 02:03:57,719: t15.2023.10.22 val PER: 0.1125
2026-01-14 02:03:57,719: t15.2023.11.03 val PER: 0.1723
2026-01-14 02:03:57,720: t15.2023.11.04 val PER: 0.0273
2026-01-14 02:03:57,720: t15.2023.11.17 val PER: 0.0295
2026-01-14 02:03:57,720: t15.2023.11.19 val PER: 0.0240
2026-01-14 02:03:57,720: t15.2023.11.26 val PER: 0.0833
2026-01-14 02:03:57,720: t15.2023.12.03 val PER: 0.0914
2026-01-14 02:03:57,720: t15.2023.12.08 val PER: 0.0719
2026-01-14 02:03:57,720: t15.2023.12.10 val PER: 0.0565
2026-01-14 02:03:57,720: t15.2023.12.17 val PER: 0.1081
2026-01-14 02:03:57,720: t15.2023.12.29 val PER: 0.1112
2026-01-14 02:03:57,720: t15.2024.02.25 val PER: 0.1053
2026-01-14 02:03:57,720: t15.2024.03.08 val PER: 0.2034
2026-01-14 02:03:57,720: t15.2024.03.15 val PER: 0.1864
2026-01-14 02:03:57,720: t15.2024.03.17 val PER: 0.1318
2026-01-14 02:03:57,721: t15.2024.05.10 val PER: 0.1352
2026-01-14 02:03:57,721: t15.2024.06.14 val PER: 0.1356
2026-01-14 02:03:57,721: t15.2024.07.19 val PER: 0.2162
2026-01-14 02:03:57,721: t15.2024.07.21 val PER: 0.0883
2026-01-14 02:03:57,721: t15.2024.07.28 val PER: 0.1250
2026-01-14 02:03:57,721: t15.2025.01.10 val PER: 0.2562
2026-01-14 02:03:57,721: t15.2025.01.12 val PER: 0.1355
2026-01-14 02:03:57,721: t15.2025.03.14 val PER: 0.3284
2026-01-14 02:03:57,721: t15.2025.03.16 val PER: 0.1505
2026-01-14 02:03:57,721: t15.2025.03.30 val PER: 0.2736
2026-01-14 02:03:57,721: t15.2025.04.13 val PER: 0.2211
2026-01-14 02:03:57,859: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_15000
2026-01-14 02:04:15,522: Train batch 15200: loss: 5.64 grad norm: 43.74 time: 0.066
2026-01-14 02:04:32,842: Train batch 15400: loss: 13.02 grad norm: 77.61 time: 0.058
2026-01-14 02:04:42,081: Running test after training batch: 15500
2026-01-14 02:04:42,269: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:04:47,671: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:04:47,721: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost now
2026-01-14 02:04:58,386: Val batch 15500: PER (avg): 0.1341 CTC Loss (avg): 25.0533 WER(5gram): 13.10% (n=256) time: 16.305
2026-01-14 02:04:58,386: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-14 02:04:58,387: t15.2023.08.13 val PER: 0.1050
2026-01-14 02:04:58,387: t15.2023.08.18 val PER: 0.0939
2026-01-14 02:04:58,387: t15.2023.08.20 val PER: 0.0898
2026-01-14 02:04:58,387: t15.2023.08.25 val PER: 0.0964
2026-01-14 02:04:58,387: t15.2023.08.27 val PER: 0.1752
2026-01-14 02:04:58,387: t15.2023.09.01 val PER: 0.0601
2026-01-14 02:04:58,388: t15.2023.09.03 val PER: 0.1508
2026-01-14 02:04:58,388: t15.2023.09.24 val PER: 0.1068
2026-01-14 02:04:58,388: t15.2023.09.29 val PER: 0.1251
2026-01-14 02:04:58,388: t15.2023.10.01 val PER: 0.1684
2026-01-14 02:04:58,388: t15.2023.10.06 val PER: 0.0829
2026-01-14 02:04:58,388: t15.2023.10.08 val PER: 0.2422
2026-01-14 02:04:58,388: t15.2023.10.13 val PER: 0.2033
2026-01-14 02:04:58,388: t15.2023.10.15 val PER: 0.1437
2026-01-14 02:04:58,389: t15.2023.10.20 val PER: 0.2081
2026-01-14 02:04:58,389: t15.2023.10.22 val PER: 0.1314
2026-01-14 02:04:58,389: t15.2023.11.03 val PER: 0.1696
2026-01-14 02:04:58,389: t15.2023.11.04 val PER: 0.0375
2026-01-14 02:04:58,389: t15.2023.11.17 val PER: 0.0342
2026-01-14 02:04:58,389: t15.2023.11.19 val PER: 0.0259
2026-01-14 02:04:58,389: t15.2023.11.26 val PER: 0.0768
2026-01-14 02:04:58,389: t15.2023.12.03 val PER: 0.0851
2026-01-14 02:04:58,389: t15.2023.12.08 val PER: 0.0599
2026-01-14 02:04:58,389: t15.2023.12.10 val PER: 0.0526
2026-01-14 02:04:58,389: t15.2023.12.17 val PER: 0.1102
2026-01-14 02:04:58,389: t15.2023.12.29 val PER: 0.1132
2026-01-14 02:04:58,389: t15.2024.02.25 val PER: 0.0969
2026-01-14 02:04:58,389: t15.2024.03.08 val PER: 0.2048
2026-01-14 02:04:58,389: t15.2024.03.15 val PER: 0.1939
2026-01-14 02:04:58,389: t15.2024.03.17 val PER: 0.1248
2026-01-14 02:04:58,390: t15.2024.05.10 val PER: 0.1367
2026-01-14 02:04:58,390: t15.2024.06.14 val PER: 0.1183
2026-01-14 02:04:58,390: t15.2024.07.19 val PER: 0.2096
2026-01-14 02:04:58,390: t15.2024.07.21 val PER: 0.0834
2026-01-14 02:04:58,390: t15.2024.07.28 val PER: 0.1257
2026-01-14 02:04:58,390: t15.2025.01.10 val PER: 0.2452
2026-01-14 02:04:58,390: t15.2025.01.12 val PER: 0.1201
2026-01-14 02:04:58,390: t15.2025.03.14 val PER: 0.3136
2026-01-14 02:04:58,390: t15.2025.03.16 val PER: 0.1571
2026-01-14 02:04:58,390: t15.2025.03.30 val PER: 0.2586
2026-01-14 02:04:58,390: t15.2025.04.13 val PER: 0.2040
2026-01-14 02:04:58,533: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_15500
2026-01-14 02:05:07,285: Train batch 15600: loss: 10.87 grad norm: 74.63 time: 0.074
2026-01-14 02:05:25,047: Train batch 15800: loss: 13.65 grad norm: 81.47 time: 0.077
2026-01-14 02:05:42,709: Train batch 16000: loss: 9.86 grad norm: 112.01 time: 0.064
2026-01-14 02:05:42,710: Running test after training batch: 16000
2026-01-14 02:05:42,839: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:05:48,250: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:05:48,301: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:05:58,878: Val batch 16000: PER (avg): 0.1352 CTC Loss (avg): 25.1835 WER(5gram): 13.49% (n=256) time: 16.168
2026-01-14 02:05:58,879: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-14 02:05:58,879: t15.2023.08.13 val PER: 0.1008
2026-01-14 02:05:58,879: t15.2023.08.18 val PER: 0.0956
2026-01-14 02:05:58,879: t15.2023.08.20 val PER: 0.0834
2026-01-14 02:05:58,879: t15.2023.08.25 val PER: 0.0843
2026-01-14 02:05:58,879: t15.2023.08.27 val PER: 0.1688
2026-01-14 02:05:58,879: t15.2023.09.01 val PER: 0.0657
2026-01-14 02:05:58,879: t15.2023.09.03 val PER: 0.1556
2026-01-14 02:05:58,879: t15.2023.09.24 val PER: 0.1068
2026-01-14 02:05:58,880: t15.2023.09.29 val PER: 0.1232
2026-01-14 02:05:58,880: t15.2023.10.01 val PER: 0.1697
2026-01-14 02:05:58,880: t15.2023.10.06 val PER: 0.0753
2026-01-14 02:05:58,880: t15.2023.10.08 val PER: 0.2544
2026-01-14 02:05:58,880: t15.2023.10.13 val PER: 0.1971
2026-01-14 02:05:58,880: t15.2023.10.15 val PER: 0.1430
2026-01-14 02:05:58,880: t15.2023.10.20 val PER: 0.2148
2026-01-14 02:05:58,880: t15.2023.10.22 val PER: 0.1169
2026-01-14 02:05:58,881: t15.2023.11.03 val PER: 0.1682
2026-01-14 02:05:58,881: t15.2023.11.04 val PER: 0.0341
2026-01-14 02:05:58,881: t15.2023.11.17 val PER: 0.0295
2026-01-14 02:05:58,881: t15.2023.11.19 val PER: 0.0259
2026-01-14 02:05:58,881: t15.2023.11.26 val PER: 0.0739
2026-01-14 02:05:58,881: t15.2023.12.03 val PER: 0.0914
2026-01-14 02:05:58,881: t15.2023.12.08 val PER: 0.0553
2026-01-14 02:05:58,881: t15.2023.12.10 val PER: 0.0565
2026-01-14 02:05:58,881: t15.2023.12.17 val PER: 0.1102
2026-01-14 02:05:58,881: t15.2023.12.29 val PER: 0.1071
2026-01-14 02:05:58,881: t15.2024.02.25 val PER: 0.1180
2026-01-14 02:05:58,882: t15.2024.03.08 val PER: 0.2063
2026-01-14 02:05:58,882: t15.2024.03.15 val PER: 0.1951
2026-01-14 02:05:58,882: t15.2024.03.17 val PER: 0.1185
2026-01-14 02:05:58,882: t15.2024.05.10 val PER: 0.1441
2026-01-14 02:05:58,882: t15.2024.06.14 val PER: 0.1530
2026-01-14 02:05:58,882: t15.2024.07.19 val PER: 0.2116
2026-01-14 02:05:58,882: t15.2024.07.21 val PER: 0.0848
2026-01-14 02:05:58,882: t15.2024.07.28 val PER: 0.1294
2026-01-14 02:05:58,882: t15.2025.01.10 val PER: 0.2782
2026-01-14 02:05:58,882: t15.2025.01.12 val PER: 0.1255
2026-01-14 02:05:58,882: t15.2025.03.14 val PER: 0.3151
2026-01-14 02:05:58,882: t15.2025.03.16 val PER: 0.1571
2026-01-14 02:05:58,883: t15.2025.03.30 val PER: 0.2678
2026-01-14 02:05:58,883: t15.2025.04.13 val PER: 0.2111
2026-01-14 02:05:59,022: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_16000
2026-01-14 02:06:16,662: Train batch 16200: loss: 7.25 grad norm: 63.25 time: 0.067
2026-01-14 02:06:34,523: Train batch 16400: loss: 12.84 grad norm: 78.87 time: 0.066
2026-01-14 02:06:43,503: Running test after training batch: 16500
2026-01-14 02:06:43,622: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:06:49,020: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:06:49,067: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost now
2026-01-14 02:06:59,745: Val batch 16500: PER (avg): 0.1313 CTC Loss (avg): 24.7744 WER(5gram): 12.52% (n=256) time: 16.242
2026-01-14 02:06:59,745: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-14 02:06:59,745: t15.2023.08.13 val PER: 0.0998
2026-01-14 02:06:59,746: t15.2023.08.18 val PER: 0.0939
2026-01-14 02:06:59,746: t15.2023.08.20 val PER: 0.0929
2026-01-14 02:06:59,746: t15.2023.08.25 val PER: 0.0858
2026-01-14 02:06:59,746: t15.2023.08.27 val PER: 0.1736
2026-01-14 02:06:59,746: t15.2023.09.01 val PER: 0.0674
2026-01-14 02:06:59,746: t15.2023.09.03 val PER: 0.1378
2026-01-14 02:06:59,746: t15.2023.09.24 val PER: 0.1214
2026-01-14 02:06:59,746: t15.2023.09.29 val PER: 0.1264
2026-01-14 02:06:59,746: t15.2023.10.01 val PER: 0.1651
2026-01-14 02:06:59,746: t15.2023.10.06 val PER: 0.0829
2026-01-14 02:06:59,746: t15.2023.10.08 val PER: 0.2233
2026-01-14 02:06:59,746: t15.2023.10.13 val PER: 0.1893
2026-01-14 02:06:59,747: t15.2023.10.15 val PER: 0.1358
2026-01-14 02:06:59,747: t15.2023.10.20 val PER: 0.1946
2026-01-14 02:06:59,747: t15.2023.10.22 val PER: 0.1114
2026-01-14 02:06:59,747: t15.2023.11.03 val PER: 0.1601
2026-01-14 02:06:59,747: t15.2023.11.04 val PER: 0.0239
2026-01-14 02:06:59,747: t15.2023.11.17 val PER: 0.0295
2026-01-14 02:06:59,747: t15.2023.11.19 val PER: 0.0279
2026-01-14 02:06:59,747: t15.2023.11.26 val PER: 0.0732
2026-01-14 02:06:59,747: t15.2023.12.03 val PER: 0.0746
2026-01-14 02:06:59,747: t15.2023.12.08 val PER: 0.0672
2026-01-14 02:06:59,747: t15.2023.12.10 val PER: 0.0526
2026-01-14 02:06:59,747: t15.2023.12.17 val PER: 0.1091
2026-01-14 02:06:59,747: t15.2023.12.29 val PER: 0.1112
2026-01-14 02:06:59,748: t15.2024.02.25 val PER: 0.1081
2026-01-14 02:06:59,748: t15.2024.03.08 val PER: 0.1906
2026-01-14 02:06:59,748: t15.2024.03.15 val PER: 0.1814
2026-01-14 02:06:59,748: t15.2024.03.17 val PER: 0.1185
2026-01-14 02:06:59,748: t15.2024.05.10 val PER: 0.1426
2026-01-14 02:06:59,748: t15.2024.06.14 val PER: 0.1341
2026-01-14 02:06:59,748: t15.2024.07.19 val PER: 0.2050
2026-01-14 02:06:59,748: t15.2024.07.21 val PER: 0.0772
2026-01-14 02:06:59,748: t15.2024.07.28 val PER: 0.1221
2026-01-14 02:06:59,748: t15.2025.01.10 val PER: 0.2645
2026-01-14 02:06:59,748: t15.2025.01.12 val PER: 0.1201
2026-01-14 02:06:59,748: t15.2025.03.14 val PER: 0.3299
2026-01-14 02:06:59,748: t15.2025.03.16 val PER: 0.1518
2026-01-14 02:06:59,748: t15.2025.03.30 val PER: 0.2471
2026-01-14 02:06:59,748: t15.2025.04.13 val PER: 0.2126
2026-01-14 02:06:59,889: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_16500
2026-01-14 02:07:08,858: Train batch 16600: loss: 9.07 grad norm: 60.02 time: 0.061
2026-01-14 02:07:26,313: Train batch 16800: loss: 16.04 grad norm: 89.87 time: 0.072
2026-01-14 02:07:43,967: Train batch 17000: loss: 8.51 grad norm: 58.47 time: 0.093
2026-01-14 02:07:43,967: Running test after training batch: 17000
2026-01-14 02:07:44,092: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:07:49,549: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:07:49,602: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:08:01,173: Val batch 17000: PER (avg): 0.1335 CTC Loss (avg): 25.5316 WER(5gram): 12.39% (n=256) time: 17.206
2026-01-14 02:08:01,173: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-14 02:08:01,174: t15.2023.08.13 val PER: 0.0988
2026-01-14 02:08:01,174: t15.2023.08.18 val PER: 0.0956
2026-01-14 02:08:01,174: t15.2023.08.20 val PER: 0.0882
2026-01-14 02:08:01,174: t15.2023.08.25 val PER: 0.0904
2026-01-14 02:08:01,174: t15.2023.08.27 val PER: 0.1608
2026-01-14 02:08:01,174: t15.2023.09.01 val PER: 0.0617
2026-01-14 02:08:01,174: t15.2023.09.03 val PER: 0.1473
2026-01-14 02:08:01,175: t15.2023.09.24 val PER: 0.1080
2026-01-14 02:08:01,175: t15.2023.09.29 val PER: 0.1276
2026-01-14 02:08:01,175: t15.2023.10.01 val PER: 0.1671
2026-01-14 02:08:01,175: t15.2023.10.06 val PER: 0.0743
2026-01-14 02:08:01,175: t15.2023.10.08 val PER: 0.2300
2026-01-14 02:08:01,175: t15.2023.10.13 val PER: 0.1947
2026-01-14 02:08:01,175: t15.2023.10.15 val PER: 0.1378
2026-01-14 02:08:01,175: t15.2023.10.20 val PER: 0.2114
2026-01-14 02:08:01,176: t15.2023.10.22 val PER: 0.1091
2026-01-14 02:08:01,176: t15.2023.11.03 val PER: 0.1642
2026-01-14 02:08:01,176: t15.2023.11.04 val PER: 0.0375
2026-01-14 02:08:01,176: t15.2023.11.17 val PER: 0.0327
2026-01-14 02:08:01,176: t15.2023.11.19 val PER: 0.0220
2026-01-14 02:08:01,176: t15.2023.11.26 val PER: 0.0696
2026-01-14 02:08:01,176: t15.2023.12.03 val PER: 0.0746
2026-01-14 02:08:01,176: t15.2023.12.08 val PER: 0.0686
2026-01-14 02:08:01,176: t15.2023.12.10 val PER: 0.0526
2026-01-14 02:08:01,177: t15.2023.12.17 val PER: 0.1081
2026-01-14 02:08:01,177: t15.2023.12.29 val PER: 0.0995
2026-01-14 02:08:01,177: t15.2024.02.25 val PER: 0.1124
2026-01-14 02:08:01,177: t15.2024.03.08 val PER: 0.2148
2026-01-14 02:08:01,177: t15.2024.03.15 val PER: 0.1876
2026-01-14 02:08:01,177: t15.2024.03.17 val PER: 0.1213
2026-01-14 02:08:01,177: t15.2024.05.10 val PER: 0.1590
2026-01-14 02:08:01,177: t15.2024.06.14 val PER: 0.1420
2026-01-14 02:08:01,177: t15.2024.07.19 val PER: 0.2116
2026-01-14 02:08:01,177: t15.2024.07.21 val PER: 0.0945
2026-01-14 02:08:01,178: t15.2024.07.28 val PER: 0.1287
2026-01-14 02:08:01,178: t15.2025.01.10 val PER: 0.2672
2026-01-14 02:08:01,178: t15.2025.01.12 val PER: 0.1286
2026-01-14 02:08:01,178: t15.2025.03.14 val PER: 0.3254
2026-01-14 02:08:01,178: t15.2025.03.16 val PER: 0.1453
2026-01-14 02:08:01,178: t15.2025.03.30 val PER: 0.2506
2026-01-14 02:08:01,178: t15.2025.04.13 val PER: 0.2211
2026-01-14 02:08:01,317: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_17000
2026-01-14 02:08:18,628: Train batch 17200: loss: 8.70 grad norm: 60.81 time: 0.096
2026-01-14 02:08:36,173: Train batch 17400: loss: 10.51 grad norm: 67.09 time: 0.081
2026-01-14 02:08:45,011: Running test after training batch: 17500
2026-01-14 02:08:45,164: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:08:50,581: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:08:50,632: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:09:02,172: Val batch 17500: PER (avg): 0.1307 CTC Loss (avg): 25.0064 WER(5gram): 11.99% (n=256) time: 17.161
2026-01-14 02:09:02,172: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-14 02:09:02,172: t15.2023.08.13 val PER: 0.0925
2026-01-14 02:09:02,173: t15.2023.08.18 val PER: 0.0855
2026-01-14 02:09:02,173: t15.2023.08.20 val PER: 0.0850
2026-01-14 02:09:02,173: t15.2023.08.25 val PER: 0.0798
2026-01-14 02:09:02,173: t15.2023.08.27 val PER: 0.1688
2026-01-14 02:09:02,173: t15.2023.09.01 val PER: 0.0568
2026-01-14 02:09:02,173: t15.2023.09.03 val PER: 0.1520
2026-01-14 02:09:02,173: t15.2023.09.24 val PER: 0.1117
2026-01-14 02:09:02,173: t15.2023.09.29 val PER: 0.1206
2026-01-14 02:09:02,173: t15.2023.10.01 val PER: 0.1612
2026-01-14 02:09:02,173: t15.2023.10.06 val PER: 0.0721
2026-01-14 02:09:02,173: t15.2023.10.08 val PER: 0.2355
2026-01-14 02:09:02,173: t15.2023.10.13 val PER: 0.1901
2026-01-14 02:09:02,173: t15.2023.10.15 val PER: 0.1351
2026-01-14 02:09:02,173: t15.2023.10.20 val PER: 0.1846
2026-01-14 02:09:02,174: t15.2023.10.22 val PER: 0.1080
2026-01-14 02:09:02,174: t15.2023.11.03 val PER: 0.1642
2026-01-14 02:09:02,174: t15.2023.11.04 val PER: 0.0341
2026-01-14 02:09:02,174: t15.2023.11.17 val PER: 0.0311
2026-01-14 02:09:02,174: t15.2023.11.19 val PER: 0.0200
2026-01-14 02:09:02,174: t15.2023.11.26 val PER: 0.0725
2026-01-14 02:09:02,174: t15.2023.12.03 val PER: 0.0746
2026-01-14 02:09:02,174: t15.2023.12.08 val PER: 0.0659
2026-01-14 02:09:02,174: t15.2023.12.10 val PER: 0.0618
2026-01-14 02:09:02,174: t15.2023.12.17 val PER: 0.1164
2026-01-14 02:09:02,174: t15.2023.12.29 val PER: 0.1084
2026-01-14 02:09:02,174: t15.2024.02.25 val PER: 0.0941
2026-01-14 02:09:02,174: t15.2024.03.08 val PER: 0.2205
2026-01-14 02:09:02,174: t15.2024.03.15 val PER: 0.1857
2026-01-14 02:09:02,175: t15.2024.03.17 val PER: 0.1185
2026-01-14 02:09:02,175: t15.2024.05.10 val PER: 0.1426
2026-01-14 02:09:02,175: t15.2024.06.14 val PER: 0.1262
2026-01-14 02:09:02,175: t15.2024.07.19 val PER: 0.2083
2026-01-14 02:09:02,175: t15.2024.07.21 val PER: 0.0876
2026-01-14 02:09:02,175: t15.2024.07.28 val PER: 0.1228
2026-01-14 02:09:02,175: t15.2025.01.10 val PER: 0.2782
2026-01-14 02:09:02,175: t15.2025.01.12 val PER: 0.1186
2026-01-14 02:09:02,175: t15.2025.03.14 val PER: 0.3047
2026-01-14 02:09:02,175: t15.2025.03.16 val PER: 0.1571
2026-01-14 02:09:02,175: t15.2025.03.30 val PER: 0.2483
2026-01-14 02:09:02,175: t15.2025.04.13 val PER: 0.2111
2026-01-14 02:09:02,316: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_17500
2026-01-14 02:09:10,870: Train batch 17600: loss: 7.73 grad norm: 60.69 time: 0.061
2026-01-14 02:09:28,772: Train batch 17800: loss: 6.64 grad norm: 63.51 time: 0.053
2026-01-14 02:09:47,028: Train batch 18000: loss: 7.77 grad norm: 67.41 time: 0.071
2026-01-14 02:09:47,028: Running test after training batch: 18000
2026-01-14 02:09:47,162: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:09:52,823: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:09:52,868: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:10:03,768: Val batch 18000: PER (avg): 0.1256 CTC Loss (avg): 24.7887 WER(5gram): 11.99% (n=256) time: 16.740
2026-01-14 02:10:03,769: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-14 02:10:03,769: t15.2023.08.13 val PER: 0.0884
2026-01-14 02:10:03,769: t15.2023.08.18 val PER: 0.0813
2026-01-14 02:10:03,769: t15.2023.08.20 val PER: 0.0842
2026-01-14 02:10:03,769: t15.2023.08.25 val PER: 0.0723
2026-01-14 02:10:03,769: t15.2023.08.27 val PER: 0.1559
2026-01-14 02:10:03,769: t15.2023.09.01 val PER: 0.0617
2026-01-14 02:10:03,769: t15.2023.09.03 val PER: 0.1259
2026-01-14 02:10:03,770: t15.2023.09.24 val PER: 0.1092
2026-01-14 02:10:03,770: t15.2023.09.29 val PER: 0.1181
2026-01-14 02:10:03,770: t15.2023.10.01 val PER: 0.1638
2026-01-14 02:10:03,770: t15.2023.10.06 val PER: 0.0689
2026-01-14 02:10:03,770: t15.2023.10.08 val PER: 0.2219
2026-01-14 02:10:03,771: t15.2023.10.13 val PER: 0.1877
2026-01-14 02:10:03,771: t15.2023.10.15 val PER: 0.1299
2026-01-14 02:10:03,771: t15.2023.10.20 val PER: 0.1913
2026-01-14 02:10:03,771: t15.2023.10.22 val PER: 0.0980
2026-01-14 02:10:03,771: t15.2023.11.03 val PER: 0.1723
2026-01-14 02:10:03,771: t15.2023.11.04 val PER: 0.0239
2026-01-14 02:10:03,771: t15.2023.11.17 val PER: 0.0358
2026-01-14 02:10:03,771: t15.2023.11.19 val PER: 0.0120
2026-01-14 02:10:03,771: t15.2023.11.26 val PER: 0.0725
2026-01-14 02:10:03,771: t15.2023.12.03 val PER: 0.0809
2026-01-14 02:10:03,772: t15.2023.12.08 val PER: 0.0586
2026-01-14 02:10:03,772: t15.2023.12.10 val PER: 0.0486
2026-01-14 02:10:03,772: t15.2023.12.17 val PER: 0.1050
2026-01-14 02:10:03,772: t15.2023.12.29 val PER: 0.0995
2026-01-14 02:10:03,772: t15.2024.02.25 val PER: 0.1039
2026-01-14 02:10:03,772: t15.2024.03.08 val PER: 0.2105
2026-01-14 02:10:03,772: t15.2024.03.15 val PER: 0.1739
2026-01-14 02:10:03,772: t15.2024.03.17 val PER: 0.1081
2026-01-14 02:10:03,772: t15.2024.05.10 val PER: 0.1367
2026-01-14 02:10:03,773: t15.2024.06.14 val PER: 0.1341
2026-01-14 02:10:03,773: t15.2024.07.19 val PER: 0.1978
2026-01-14 02:10:03,773: t15.2024.07.21 val PER: 0.0821
2026-01-14 02:10:03,773: t15.2024.07.28 val PER: 0.1191
2026-01-14 02:10:03,773: t15.2025.01.10 val PER: 0.2479
2026-01-14 02:10:03,773: t15.2025.01.12 val PER: 0.1055
2026-01-14 02:10:03,773: t15.2025.03.14 val PER: 0.3107
2026-01-14 02:10:03,773: t15.2025.03.16 val PER: 0.1518
2026-01-14 02:10:03,773: t15.2025.03.30 val PER: 0.2483
2026-01-14 02:10:03,773: t15.2025.04.13 val PER: 0.1983
2026-01-14 02:10:03,911: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_18000
2026-01-14 02:10:21,845: Train batch 18200: loss: 7.53 grad norm: 58.25 time: 0.088
2026-01-14 02:10:39,833: Train batch 18400: loss: 3.33 grad norm: 44.86 time: 0.066
2026-01-14 02:10:48,485: Running test after training batch: 18500
2026-01-14 02:10:48,672: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:10:54,324: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:10:54,375: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:11:05,594: Val batch 18500: PER (avg): 0.1258 CTC Loss (avg): 24.9276 WER(5gram): 12.45% (n=256) time: 17.109
2026-01-14 02:11:05,595: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-14 02:11:05,595: t15.2023.08.13 val PER: 0.0915
2026-01-14 02:11:05,595: t15.2023.08.18 val PER: 0.0805
2026-01-14 02:11:05,595: t15.2023.08.20 val PER: 0.0826
2026-01-14 02:11:05,595: t15.2023.08.25 val PER: 0.0768
2026-01-14 02:11:05,595: t15.2023.08.27 val PER: 0.1720
2026-01-14 02:11:05,595: t15.2023.09.01 val PER: 0.0617
2026-01-14 02:11:05,596: t15.2023.09.03 val PER: 0.1449
2026-01-14 02:11:05,596: t15.2023.09.24 val PER: 0.0983
2026-01-14 02:11:05,596: t15.2023.09.29 val PER: 0.1206
2026-01-14 02:11:05,596: t15.2023.10.01 val PER: 0.1565
2026-01-14 02:11:05,596: t15.2023.10.06 val PER: 0.0667
2026-01-14 02:11:05,596: t15.2023.10.08 val PER: 0.2260
2026-01-14 02:11:05,596: t15.2023.10.13 val PER: 0.1870
2026-01-14 02:11:05,596: t15.2023.10.15 val PER: 0.1272
2026-01-14 02:11:05,596: t15.2023.10.20 val PER: 0.1879
2026-01-14 02:11:05,596: t15.2023.10.22 val PER: 0.1158
2026-01-14 02:11:05,596: t15.2023.11.03 val PER: 0.1621
2026-01-14 02:11:05,597: t15.2023.11.04 val PER: 0.0239
2026-01-14 02:11:05,597: t15.2023.11.17 val PER: 0.0264
2026-01-14 02:11:05,597: t15.2023.11.19 val PER: 0.0220
2026-01-14 02:11:05,597: t15.2023.11.26 val PER: 0.0703
2026-01-14 02:11:05,597: t15.2023.12.03 val PER: 0.0756
2026-01-14 02:11:05,597: t15.2023.12.08 val PER: 0.0499
2026-01-14 02:11:05,597: t15.2023.12.10 val PER: 0.0447
2026-01-14 02:11:05,597: t15.2023.12.17 val PER: 0.0936
2026-01-14 02:11:05,597: t15.2023.12.29 val PER: 0.0933
2026-01-14 02:11:05,597: t15.2024.02.25 val PER: 0.0927
2026-01-14 02:11:05,598: t15.2024.03.08 val PER: 0.1963
2026-01-14 02:11:05,598: t15.2024.03.15 val PER: 0.1864
2026-01-14 02:11:05,598: t15.2024.03.17 val PER: 0.1123
2026-01-14 02:11:05,598: t15.2024.05.10 val PER: 0.1426
2026-01-14 02:11:05,598: t15.2024.06.14 val PER: 0.1372
2026-01-14 02:11:05,598: t15.2024.07.19 val PER: 0.1997
2026-01-14 02:11:05,598: t15.2024.07.21 val PER: 0.0793
2026-01-14 02:11:05,598: t15.2024.07.28 val PER: 0.1221
2026-01-14 02:11:05,598: t15.2025.01.10 val PER: 0.2727
2026-01-14 02:11:05,598: t15.2025.01.12 val PER: 0.1109
2026-01-14 02:11:05,598: t15.2025.03.14 val PER: 0.3092
2026-01-14 02:11:05,599: t15.2025.03.16 val PER: 0.1440
2026-01-14 02:11:05,599: t15.2025.03.30 val PER: 0.2483
2026-01-14 02:11:05,599: t15.2025.04.13 val PER: 0.2111
2026-01-14 02:11:05,742: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_18500
2026-01-14 02:11:14,498: Train batch 18600: loss: 9.60 grad norm: 75.82 time: 0.077
2026-01-14 02:11:32,761: Train batch 18800: loss: 5.31 grad norm: 51.74 time: 0.075
2026-01-14 02:11:50,543: Train batch 19000: loss: 7.33 grad norm: 50.48 time: 0.076
2026-01-14 02:11:50,544: Running test after training batch: 19000
2026-01-14 02:11:50,660: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:11:56,409: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:11:56,460: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:12:07,609: Val batch 19000: PER (avg): 0.1273 CTC Loss (avg): 24.6258 WER(5gram): 13.04% (n=256) time: 17.065
2026-01-14 02:12:07,609: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-14 02:12:07,609: t15.2023.08.13 val PER: 0.0946
2026-01-14 02:12:07,609: t15.2023.08.18 val PER: 0.0947
2026-01-14 02:12:07,610: t15.2023.08.20 val PER: 0.0802
2026-01-14 02:12:07,610: t15.2023.08.25 val PER: 0.0813
2026-01-14 02:12:07,610: t15.2023.08.27 val PER: 0.1592
2026-01-14 02:12:07,610: t15.2023.09.01 val PER: 0.0633
2026-01-14 02:12:07,610: t15.2023.09.03 val PER: 0.1401
2026-01-14 02:12:07,610: t15.2023.09.24 val PER: 0.1007
2026-01-14 02:12:07,610: t15.2023.09.29 val PER: 0.1283
2026-01-14 02:12:07,610: t15.2023.10.01 val PER: 0.1651
2026-01-14 02:12:07,611: t15.2023.10.06 val PER: 0.0700
2026-01-14 02:12:07,611: t15.2023.10.08 val PER: 0.2179
2026-01-14 02:12:07,611: t15.2023.10.13 val PER: 0.1963
2026-01-14 02:12:07,611: t15.2023.10.15 val PER: 0.1338
2026-01-14 02:12:07,611: t15.2023.10.20 val PER: 0.1913
2026-01-14 02:12:07,611: t15.2023.10.22 val PER: 0.1069
2026-01-14 02:12:07,611: t15.2023.11.03 val PER: 0.1608
2026-01-14 02:12:07,611: t15.2023.11.04 val PER: 0.0205
2026-01-14 02:12:07,611: t15.2023.11.17 val PER: 0.0233
2026-01-14 02:12:07,611: t15.2023.11.19 val PER: 0.0200
2026-01-14 02:12:07,611: t15.2023.11.26 val PER: 0.0739
2026-01-14 02:12:07,611: t15.2023.12.03 val PER: 0.0798
2026-01-14 02:12:07,611: t15.2023.12.08 val PER: 0.0499
2026-01-14 02:12:07,611: t15.2023.12.10 val PER: 0.0460
2026-01-14 02:12:07,611: t15.2023.12.17 val PER: 0.1050
2026-01-14 02:12:07,612: t15.2023.12.29 val PER: 0.0981
2026-01-14 02:12:07,612: t15.2024.02.25 val PER: 0.0969
2026-01-14 02:12:07,612: t15.2024.03.08 val PER: 0.2006
2026-01-14 02:12:07,612: t15.2024.03.15 val PER: 0.1870
2026-01-14 02:12:07,612: t15.2024.03.17 val PER: 0.1158
2026-01-14 02:12:07,612: t15.2024.05.10 val PER: 0.1248
2026-01-14 02:12:07,612: t15.2024.06.14 val PER: 0.1546
2026-01-14 02:12:07,612: t15.2024.07.19 val PER: 0.1964
2026-01-14 02:12:07,612: t15.2024.07.21 val PER: 0.0814
2026-01-14 02:12:07,612: t15.2024.07.28 val PER: 0.1154
2026-01-14 02:12:07,612: t15.2025.01.10 val PER: 0.2741
2026-01-14 02:12:07,612: t15.2025.01.12 val PER: 0.1155
2026-01-14 02:12:07,612: t15.2025.03.14 val PER: 0.3033
2026-01-14 02:12:07,612: t15.2025.03.16 val PER: 0.1374
2026-01-14 02:12:07,612: t15.2025.03.30 val PER: 0.2425
2026-01-14 02:12:07,612: t15.2025.04.13 val PER: 0.2068
2026-01-14 02:12:07,752: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_19000
2026-01-14 02:12:25,005: Train batch 19200: loss: 4.58 grad norm: 50.09 time: 0.073
2026-01-14 02:12:42,878: Train batch 19400: loss: 3.11 grad norm: 47.50 time: 0.061
2026-01-14 02:12:51,758: Running test after training batch: 19500
2026-01-14 02:12:51,850: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:12:57,333: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:12:57,384: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:13:08,404: Val batch 19500: PER (avg): 0.1243 CTC Loss (avg): 24.9618 WER(5gram): 12.52% (n=256) time: 16.645
2026-01-14 02:13:08,404: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-14 02:13:08,404: t15.2023.08.13 val PER: 0.1008
2026-01-14 02:13:08,404: t15.2023.08.18 val PER: 0.0805
2026-01-14 02:13:08,405: t15.2023.08.20 val PER: 0.0842
2026-01-14 02:13:08,405: t15.2023.08.25 val PER: 0.0828
2026-01-14 02:13:08,405: t15.2023.08.27 val PER: 0.1592
2026-01-14 02:13:08,405: t15.2023.09.01 val PER: 0.0528
2026-01-14 02:13:08,405: t15.2023.09.03 val PER: 0.1390
2026-01-14 02:13:08,405: t15.2023.09.24 val PER: 0.0898
2026-01-14 02:13:08,405: t15.2023.09.29 val PER: 0.1251
2026-01-14 02:13:08,405: t15.2023.10.01 val PER: 0.1704
2026-01-14 02:13:08,405: t15.2023.10.06 val PER: 0.0581
2026-01-14 02:13:08,405: t15.2023.10.08 val PER: 0.2246
2026-01-14 02:13:08,405: t15.2023.10.13 val PER: 0.1932
2026-01-14 02:13:08,405: t15.2023.10.15 val PER: 0.1384
2026-01-14 02:13:08,405: t15.2023.10.20 val PER: 0.1913
2026-01-14 02:13:08,405: t15.2023.10.22 val PER: 0.1047
2026-01-14 02:13:08,406: t15.2023.11.03 val PER: 0.1533
2026-01-14 02:13:08,406: t15.2023.11.04 val PER: 0.0239
2026-01-14 02:13:08,406: t15.2023.11.17 val PER: 0.0327
2026-01-14 02:13:08,406: t15.2023.11.19 val PER: 0.0160
2026-01-14 02:13:08,406: t15.2023.11.26 val PER: 0.0594
2026-01-14 02:13:08,406: t15.2023.12.03 val PER: 0.0809
2026-01-14 02:13:08,406: t15.2023.12.08 val PER: 0.0539
2026-01-14 02:13:08,406: t15.2023.12.10 val PER: 0.0473
2026-01-14 02:13:08,406: t15.2023.12.17 val PER: 0.0998
2026-01-14 02:13:08,406: t15.2023.12.29 val PER: 0.1030
2026-01-14 02:13:08,406: t15.2024.02.25 val PER: 0.0969
2026-01-14 02:13:08,406: t15.2024.03.08 val PER: 0.1906
2026-01-14 02:13:08,406: t15.2024.03.15 val PER: 0.1757
2026-01-14 02:13:08,406: t15.2024.03.17 val PER: 0.1032
2026-01-14 02:13:08,407: t15.2024.05.10 val PER: 0.1218
2026-01-14 02:13:08,407: t15.2024.06.14 val PER: 0.1278
2026-01-14 02:13:08,407: t15.2024.07.19 val PER: 0.2037
2026-01-14 02:13:08,407: t15.2024.07.21 val PER: 0.0766
2026-01-14 02:13:08,407: t15.2024.07.28 val PER: 0.1059
2026-01-14 02:13:08,407: t15.2025.01.10 val PER: 0.2548
2026-01-14 02:13:08,407: t15.2025.01.12 val PER: 0.1139
2026-01-14 02:13:08,407: t15.2025.03.14 val PER: 0.3092
2026-01-14 02:13:08,407: t15.2025.03.16 val PER: 0.1518
2026-01-14 02:13:08,407: t15.2025.03.30 val PER: 0.2471
2026-01-14 02:13:08,407: t15.2025.04.13 val PER: 0.1954
2026-01-14 02:13:08,548: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_19500
2026-01-14 02:13:17,226: Train batch 19600: loss: 5.43 grad norm: 43.56 time: 0.065
2026-01-14 02:13:34,513: Train batch 19800: loss: 4.83 grad norm: 51.87 time: 0.066
2026-01-14 02:13:51,887: Train batch 20000: loss: 3.63 grad norm: 44.96 time: 0.078
2026-01-14 02:13:51,887: Running test after training batch: 20000
2026-01-14 02:13:52,120: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:13:57,489: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:13:57,539: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:14:08,719: Val batch 20000: PER (avg): 0.1230 CTC Loss (avg): 25.6201 WER(5gram): 11.34% (n=256) time: 16.832
2026-01-14 02:14:08,720: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-14 02:14:08,720: t15.2023.08.13 val PER: 0.0977
2026-01-14 02:14:08,720: t15.2023.08.18 val PER: 0.0780
2026-01-14 02:14:08,721: t15.2023.08.20 val PER: 0.0731
2026-01-14 02:14:08,721: t15.2023.08.25 val PER: 0.0813
2026-01-14 02:14:08,721: t15.2023.08.27 val PER: 0.1608
2026-01-14 02:14:08,721: t15.2023.09.01 val PER: 0.0568
2026-01-14 02:14:08,721: t15.2023.09.03 val PER: 0.1342
2026-01-14 02:14:08,721: t15.2023.09.24 val PER: 0.1056
2026-01-14 02:14:08,721: t15.2023.09.29 val PER: 0.1244
2026-01-14 02:14:08,721: t15.2023.10.01 val PER: 0.1592
2026-01-14 02:14:08,721: t15.2023.10.06 val PER: 0.0667
2026-01-14 02:14:08,721: t15.2023.10.08 val PER: 0.2152
2026-01-14 02:14:08,722: t15.2023.10.13 val PER: 0.1761
2026-01-14 02:14:08,722: t15.2023.10.15 val PER: 0.1305
2026-01-14 02:14:08,722: t15.2023.10.20 val PER: 0.2013
2026-01-14 02:14:08,722: t15.2023.10.22 val PER: 0.1102
2026-01-14 02:14:08,722: t15.2023.11.03 val PER: 0.1649
2026-01-14 02:14:08,722: t15.2023.11.04 val PER: 0.0273
2026-01-14 02:14:08,722: t15.2023.11.17 val PER: 0.0342
2026-01-14 02:14:08,722: t15.2023.11.19 val PER: 0.0180
2026-01-14 02:14:08,722: t15.2023.11.26 val PER: 0.0652
2026-01-14 02:14:08,722: t15.2023.12.03 val PER: 0.0756
2026-01-14 02:14:08,723: t15.2023.12.08 val PER: 0.0573
2026-01-14 02:14:08,723: t15.2023.12.10 val PER: 0.0460
2026-01-14 02:14:08,723: t15.2023.12.17 val PER: 0.0946
2026-01-14 02:14:08,723: t15.2023.12.29 val PER: 0.0968
2026-01-14 02:14:08,723: t15.2024.02.25 val PER: 0.0871
2026-01-14 02:14:08,723: t15.2024.03.08 val PER: 0.2034
2026-01-14 02:14:08,723: t15.2024.03.15 val PER: 0.1720
2026-01-14 02:14:08,723: t15.2024.03.17 val PER: 0.1074
2026-01-14 02:14:08,723: t15.2024.05.10 val PER: 0.1382
2026-01-14 02:14:08,723: t15.2024.06.14 val PER: 0.1356
2026-01-14 02:14:08,723: t15.2024.07.19 val PER: 0.1945
2026-01-14 02:14:08,723: t15.2024.07.21 val PER: 0.0731
2026-01-14 02:14:08,723: t15.2024.07.28 val PER: 0.1059
2026-01-14 02:14:08,724: t15.2025.01.10 val PER: 0.2466
2026-01-14 02:14:08,724: t15.2025.01.12 val PER: 0.1155
2026-01-14 02:14:08,724: t15.2025.03.14 val PER: 0.3136
2026-01-14 02:14:08,724: t15.2025.03.16 val PER: 0.1479
2026-01-14 02:14:08,724: t15.2025.03.30 val PER: 0.2368
2026-01-14 02:14:08,724: t15.2025.04.13 val PER: 0.1883
2026-01-14 02:14:08,864: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_20000
2026-01-14 02:14:26,478: Train batch 20200: loss: 3.18 grad norm: 46.91 time: 0.070
2026-01-14 02:14:44,102: Train batch 20400: loss: 3.78 grad norm: 47.42 time: 0.072
2026-01-14 02:14:52,716: Running test after training batch: 20500
2026-01-14 02:14:52,878: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:14:58,242: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:14:58,292: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost now
2026-01-14 02:15:09,146: Val batch 20500: PER (avg): 0.1249 CTC Loss (avg): 25.3624 WER(5gram): 12.26% (n=256) time: 16.430
2026-01-14 02:15:09,146: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-14 02:15:09,147: t15.2023.08.13 val PER: 0.0894
2026-01-14 02:15:09,147: t15.2023.08.18 val PER: 0.0763
2026-01-14 02:15:09,147: t15.2023.08.20 val PER: 0.0802
2026-01-14 02:15:09,147: t15.2023.08.25 val PER: 0.0738
2026-01-14 02:15:09,147: t15.2023.08.27 val PER: 0.1704
2026-01-14 02:15:09,147: t15.2023.09.01 val PER: 0.0633
2026-01-14 02:15:09,147: t15.2023.09.03 val PER: 0.1401
2026-01-14 02:15:09,147: t15.2023.09.24 val PER: 0.1007
2026-01-14 02:15:09,147: t15.2023.09.29 val PER: 0.1187
2026-01-14 02:15:09,147: t15.2023.10.01 val PER: 0.1598
2026-01-14 02:15:09,147: t15.2023.10.06 val PER: 0.0624
2026-01-14 02:15:09,147: t15.2023.10.08 val PER: 0.2368
2026-01-14 02:15:09,148: t15.2023.10.13 val PER: 0.1784
2026-01-14 02:15:09,148: t15.2023.10.15 val PER: 0.1358
2026-01-14 02:15:09,148: t15.2023.10.20 val PER: 0.1913
2026-01-14 02:15:09,148: t15.2023.10.22 val PER: 0.1102
2026-01-14 02:15:09,148: t15.2023.11.03 val PER: 0.1628
2026-01-14 02:15:09,148: t15.2023.11.04 val PER: 0.0239
2026-01-14 02:15:09,148: t15.2023.11.17 val PER: 0.0327
2026-01-14 02:15:09,148: t15.2023.11.19 val PER: 0.0200
2026-01-14 02:15:09,148: t15.2023.11.26 val PER: 0.0630
2026-01-14 02:15:09,148: t15.2023.12.03 val PER: 0.0809
2026-01-14 02:15:09,148: t15.2023.12.08 val PER: 0.0499
2026-01-14 02:15:09,148: t15.2023.12.10 val PER: 0.0473
2026-01-14 02:15:09,148: t15.2023.12.17 val PER: 0.1102
2026-01-14 02:15:09,149: t15.2023.12.29 val PER: 0.0940
2026-01-14 02:15:09,149: t15.2024.02.25 val PER: 0.0927
2026-01-14 02:15:09,149: t15.2024.03.08 val PER: 0.2048
2026-01-14 02:15:09,149: t15.2024.03.15 val PER: 0.1795
2026-01-14 02:15:09,149: t15.2024.03.17 val PER: 0.1130
2026-01-14 02:15:09,149: t15.2024.05.10 val PER: 0.1337
2026-01-14 02:15:09,149: t15.2024.06.14 val PER: 0.1293
2026-01-14 02:15:09,149: t15.2024.07.19 val PER: 0.2050
2026-01-14 02:15:09,149: t15.2024.07.21 val PER: 0.0779
2026-01-14 02:15:09,149: t15.2024.07.28 val PER: 0.1140
2026-01-14 02:15:09,149: t15.2025.01.10 val PER: 0.2617
2026-01-14 02:15:09,150: t15.2025.01.12 val PER: 0.1078
2026-01-14 02:15:09,150: t15.2025.03.14 val PER: 0.3107
2026-01-14 02:15:09,150: t15.2025.03.16 val PER: 0.1427
2026-01-14 02:15:09,150: t15.2025.03.30 val PER: 0.2402
2026-01-14 02:15:09,150: t15.2025.04.13 val PER: 0.1983
2026-01-14 02:15:09,288: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_20500
2026-01-14 02:15:18,036: Train batch 20600: loss: 4.02 grad norm: 49.82 time: 0.066
2026-01-14 02:15:35,960: Train batch 20800: loss: 5.26 grad norm: 69.37 time: 0.064
2026-01-14 02:15:53,789: Train batch 21000: loss: 3.79 grad norm: 44.20 time: 0.061
2026-01-14 02:15:53,789: Running test after training batch: 21000
2026-01-14 02:15:53,958: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:15:59,332: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:15:59,382: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost now
2026-01-14 02:16:10,557: Val batch 21000: PER (avg): 0.1212 CTC Loss (avg): 25.1444 WER(5gram): 13.04% (n=256) time: 16.768
2026-01-14 02:16:10,557: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-14 02:16:10,557: t15.2023.08.13 val PER: 0.0967
2026-01-14 02:16:10,557: t15.2023.08.18 val PER: 0.0821
2026-01-14 02:16:10,558: t15.2023.08.20 val PER: 0.0818
2026-01-14 02:16:10,558: t15.2023.08.25 val PER: 0.0768
2026-01-14 02:16:10,558: t15.2023.08.27 val PER: 0.1576
2026-01-14 02:16:10,558: t15.2023.09.01 val PER: 0.0609
2026-01-14 02:16:10,558: t15.2023.09.03 val PER: 0.1378
2026-01-14 02:16:10,558: t15.2023.09.24 val PER: 0.0995
2026-01-14 02:16:10,558: t15.2023.09.29 val PER: 0.1187
2026-01-14 02:16:10,558: t15.2023.10.01 val PER: 0.1565
2026-01-14 02:16:10,558: t15.2023.10.06 val PER: 0.0775
2026-01-14 02:16:10,558: t15.2023.10.08 val PER: 0.2233
2026-01-14 02:16:10,558: t15.2023.10.13 val PER: 0.1870
2026-01-14 02:16:10,559: t15.2023.10.15 val PER: 0.1246
2026-01-14 02:16:10,559: t15.2023.10.20 val PER: 0.2148
2026-01-14 02:16:10,559: t15.2023.10.22 val PER: 0.0958
2026-01-14 02:16:10,559: t15.2023.11.03 val PER: 0.1662
2026-01-14 02:16:10,559: t15.2023.11.04 val PER: 0.0205
2026-01-14 02:16:10,559: t15.2023.11.17 val PER: 0.0342
2026-01-14 02:16:10,559: t15.2023.11.19 val PER: 0.0180
2026-01-14 02:16:10,559: t15.2023.11.26 val PER: 0.0601
2026-01-14 02:16:10,559: t15.2023.12.03 val PER: 0.0704
2026-01-14 02:16:10,559: t15.2023.12.08 val PER: 0.0486
2026-01-14 02:16:10,559: t15.2023.12.10 val PER: 0.0420
2026-01-14 02:16:10,559: t15.2023.12.17 val PER: 0.0988
2026-01-14 02:16:10,559: t15.2023.12.29 val PER: 0.0892
2026-01-14 02:16:10,559: t15.2024.02.25 val PER: 0.0927
2026-01-14 02:16:10,559: t15.2024.03.08 val PER: 0.1821
2026-01-14 02:16:10,559: t15.2024.03.15 val PER: 0.1751
2026-01-14 02:16:10,560: t15.2024.03.17 val PER: 0.0997
2026-01-14 02:16:10,560: t15.2024.05.10 val PER: 0.1278
2026-01-14 02:16:10,560: t15.2024.06.14 val PER: 0.1136
2026-01-14 02:16:10,560: t15.2024.07.19 val PER: 0.1945
2026-01-14 02:16:10,560: t15.2024.07.21 val PER: 0.0772
2026-01-14 02:16:10,560: t15.2024.07.28 val PER: 0.1103
2026-01-14 02:16:10,560: t15.2025.01.10 val PER: 0.2493
2026-01-14 02:16:10,560: t15.2025.01.12 val PER: 0.1078
2026-01-14 02:16:10,560: t15.2025.03.14 val PER: 0.2959
2026-01-14 02:16:10,560: t15.2025.03.16 val PER: 0.1348
2026-01-14 02:16:10,560: t15.2025.03.30 val PER: 0.2414
2026-01-14 02:16:10,560: t15.2025.04.13 val PER: 0.1883
2026-01-14 02:16:10,700: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_21000
2026-01-14 02:16:28,291: Train batch 21200: loss: 3.72 grad norm: 39.72 time: 0.089
2026-01-14 02:16:46,837: Train batch 21400: loss: 6.68 grad norm: 57.98 time: 0.074
2026-01-14 02:16:55,785: Running test after training batch: 21500
2026-01-14 02:16:55,904: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:17:01,295: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:17:01,349: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost said
2026-01-14 02:17:12,343: Val batch 21500: PER (avg): 0.1216 CTC Loss (avg): 24.9858 WER(5gram): 12.91% (n=256) time: 16.558
2026-01-14 02:17:12,344: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-14 02:17:12,344: t15.2023.08.13 val PER: 0.0977
2026-01-14 02:17:12,344: t15.2023.08.18 val PER: 0.0780
2026-01-14 02:17:12,344: t15.2023.08.20 val PER: 0.0731
2026-01-14 02:17:12,344: t15.2023.08.25 val PER: 0.0633
2026-01-14 02:17:12,344: t15.2023.08.27 val PER: 0.1624
2026-01-14 02:17:12,345: t15.2023.09.01 val PER: 0.0617
2026-01-14 02:17:12,345: t15.2023.09.03 val PER: 0.1330
2026-01-14 02:17:12,345: t15.2023.09.24 val PER: 0.1032
2026-01-14 02:17:12,345: t15.2023.09.29 val PER: 0.1200
2026-01-14 02:17:12,345: t15.2023.10.01 val PER: 0.1664
2026-01-14 02:17:12,345: t15.2023.10.06 val PER: 0.0753
2026-01-14 02:17:12,345: t15.2023.10.08 val PER: 0.2124
2026-01-14 02:17:12,345: t15.2023.10.13 val PER: 0.1839
2026-01-14 02:17:12,345: t15.2023.10.15 val PER: 0.1279
2026-01-14 02:17:12,345: t15.2023.10.20 val PER: 0.1946
2026-01-14 02:17:12,346: t15.2023.10.22 val PER: 0.0991
2026-01-14 02:17:12,346: t15.2023.11.03 val PER: 0.1662
2026-01-14 02:17:12,346: t15.2023.11.04 val PER: 0.0307
2026-01-14 02:17:12,346: t15.2023.11.17 val PER: 0.0280
2026-01-14 02:17:12,346: t15.2023.11.19 val PER: 0.0220
2026-01-14 02:17:12,346: t15.2023.11.26 val PER: 0.0609
2026-01-14 02:17:12,346: t15.2023.12.03 val PER: 0.0704
2026-01-14 02:17:12,346: t15.2023.12.08 val PER: 0.0553
2026-01-14 02:17:12,346: t15.2023.12.10 val PER: 0.0460
2026-01-14 02:17:12,346: t15.2023.12.17 val PER: 0.0998
2026-01-14 02:17:12,347: t15.2023.12.29 val PER: 0.0954
2026-01-14 02:17:12,347: t15.2024.02.25 val PER: 0.0955
2026-01-14 02:17:12,347: t15.2024.03.08 val PER: 0.1821
2026-01-14 02:17:12,347: t15.2024.03.15 val PER: 0.1751
2026-01-14 02:17:12,347: t15.2024.03.17 val PER: 0.0962
2026-01-14 02:17:12,347: t15.2024.05.10 val PER: 0.1352
2026-01-14 02:17:12,347: t15.2024.06.14 val PER: 0.1120
2026-01-14 02:17:12,347: t15.2024.07.19 val PER: 0.1958
2026-01-14 02:17:12,347: t15.2024.07.21 val PER: 0.0759
2026-01-14 02:17:12,347: t15.2024.07.28 val PER: 0.1037
2026-01-14 02:17:12,347: t15.2025.01.10 val PER: 0.2576
2026-01-14 02:17:12,347: t15.2025.01.12 val PER: 0.1024
2026-01-14 02:17:12,348: t15.2025.03.14 val PER: 0.3062
2026-01-14 02:17:12,348: t15.2025.03.16 val PER: 0.1309
2026-01-14 02:17:12,348: t15.2025.03.30 val PER: 0.2471
2026-01-14 02:17:12,348: t15.2025.04.13 val PER: 0.1997
2026-01-14 02:17:12,486: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_21500
2026-01-14 02:17:21,194: Train batch 21600: loss: 7.37 grad norm: 65.78 time: 0.086
2026-01-14 02:17:39,348: Train batch 21800: loss: 3.57 grad norm: 46.54 time: 0.094
2026-01-14 02:17:56,706: Train batch 22000: loss: 7.54 grad norm: 59.74 time: 0.062
2026-01-14 02:17:56,706: Running test after training batch: 22000
2026-01-14 02:17:56,806: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:18:02,196: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:18:02,257: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:18:13,414: Val batch 22000: PER (avg): 0.1205 CTC Loss (avg): 25.0745 WER(5gram): 12.19% (n=256) time: 16.708
2026-01-14 02:18:13,415: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-14 02:18:13,415: t15.2023.08.13 val PER: 0.0800
2026-01-14 02:18:13,415: t15.2023.08.18 val PER: 0.0796
2026-01-14 02:18:13,416: t15.2023.08.20 val PER: 0.0786
2026-01-14 02:18:13,416: t15.2023.08.25 val PER: 0.0693
2026-01-14 02:18:13,416: t15.2023.08.27 val PER: 0.1704
2026-01-14 02:18:13,416: t15.2023.09.01 val PER: 0.0495
2026-01-14 02:18:13,416: t15.2023.09.03 val PER: 0.1413
2026-01-14 02:18:13,416: t15.2023.09.24 val PER: 0.0922
2026-01-14 02:18:13,416: t15.2023.09.29 val PER: 0.1219
2026-01-14 02:18:13,416: t15.2023.10.01 val PER: 0.1440
2026-01-14 02:18:13,416: t15.2023.10.06 val PER: 0.0743
2026-01-14 02:18:13,416: t15.2023.10.08 val PER: 0.2273
2026-01-14 02:18:13,417: t15.2023.10.13 val PER: 0.1831
2026-01-14 02:18:13,417: t15.2023.10.15 val PER: 0.1272
2026-01-14 02:18:13,417: t15.2023.10.20 val PER: 0.1846
2026-01-14 02:18:13,417: t15.2023.10.22 val PER: 0.1024
2026-01-14 02:18:13,417: t15.2023.11.03 val PER: 0.1621
2026-01-14 02:18:13,417: t15.2023.11.04 val PER: 0.0171
2026-01-14 02:18:13,417: t15.2023.11.17 val PER: 0.0264
2026-01-14 02:18:13,417: t15.2023.11.19 val PER: 0.0240
2026-01-14 02:18:13,417: t15.2023.11.26 val PER: 0.0638
2026-01-14 02:18:13,417: t15.2023.12.03 val PER: 0.0630
2026-01-14 02:18:13,417: t15.2023.12.08 val PER: 0.0539
2026-01-14 02:18:13,418: t15.2023.12.10 val PER: 0.0407
2026-01-14 02:18:13,418: t15.2023.12.17 val PER: 0.0988
2026-01-14 02:18:13,418: t15.2023.12.29 val PER: 0.0954
2026-01-14 02:18:13,418: t15.2024.02.25 val PER: 0.0913
2026-01-14 02:18:13,418: t15.2024.03.08 val PER: 0.1906
2026-01-14 02:18:13,418: t15.2024.03.15 val PER: 0.1720
2026-01-14 02:18:13,418: t15.2024.03.17 val PER: 0.1032
2026-01-14 02:18:13,418: t15.2024.05.10 val PER: 0.1412
2026-01-14 02:18:13,418: t15.2024.06.14 val PER: 0.1293
2026-01-14 02:18:13,418: t15.2024.07.19 val PER: 0.1866
2026-01-14 02:18:13,418: t15.2024.07.21 val PER: 0.0766
2026-01-14 02:18:13,419: t15.2024.07.28 val PER: 0.1162
2026-01-14 02:18:13,419: t15.2025.01.10 val PER: 0.2534
2026-01-14 02:18:13,419: t15.2025.01.12 val PER: 0.1055
2026-01-14 02:18:13,419: t15.2025.03.14 val PER: 0.3033
2026-01-14 02:18:13,419: t15.2025.03.16 val PER: 0.1322
2026-01-14 02:18:13,419: t15.2025.03.30 val PER: 0.2345
2026-01-14 02:18:13,419: t15.2025.04.13 val PER: 0.1983
2026-01-14 02:18:13,558: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_22000
2026-01-14 02:18:31,301: Train batch 22200: loss: 4.41 grad norm: 55.37 time: 0.069
2026-01-14 02:18:48,649: Train batch 22400: loss: 3.78 grad norm: 50.44 time: 0.063
2026-01-14 02:18:57,737: Running test after training batch: 22500
2026-01-14 02:18:57,876: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:19:03,232: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:19:03,285: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:19:14,571: Val batch 22500: PER (avg): 0.1194 CTC Loss (avg): 25.3469 WER(5gram): 13.04% (n=256) time: 16.833
2026-01-14 02:19:14,571: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-14 02:19:14,571: t15.2023.08.13 val PER: 0.0904
2026-01-14 02:19:14,571: t15.2023.08.18 val PER: 0.0830
2026-01-14 02:19:14,571: t15.2023.08.20 val PER: 0.0826
2026-01-14 02:19:14,572: t15.2023.08.25 val PER: 0.0693
2026-01-14 02:19:14,572: t15.2023.08.27 val PER: 0.1752
2026-01-14 02:19:14,572: t15.2023.09.01 val PER: 0.0519
2026-01-14 02:19:14,572: t15.2023.09.03 val PER: 0.1283
2026-01-14 02:19:14,572: t15.2023.09.24 val PER: 0.0922
2026-01-14 02:19:14,572: t15.2023.09.29 val PER: 0.1193
2026-01-14 02:19:14,572: t15.2023.10.01 val PER: 0.1526
2026-01-14 02:19:14,572: t15.2023.10.06 val PER: 0.0635
2026-01-14 02:19:14,572: t15.2023.10.08 val PER: 0.2206
2026-01-14 02:19:14,572: t15.2023.10.13 val PER: 0.1885
2026-01-14 02:19:14,572: t15.2023.10.15 val PER: 0.1220
2026-01-14 02:19:14,572: t15.2023.10.20 val PER: 0.1644
2026-01-14 02:19:14,573: t15.2023.10.22 val PER: 0.1002
2026-01-14 02:19:14,573: t15.2023.11.03 val PER: 0.1655
2026-01-14 02:19:14,573: t15.2023.11.04 val PER: 0.0239
2026-01-14 02:19:14,573: t15.2023.11.17 val PER: 0.0327
2026-01-14 02:19:14,573: t15.2023.11.19 val PER: 0.0160
2026-01-14 02:19:14,573: t15.2023.11.26 val PER: 0.0587
2026-01-14 02:19:14,573: t15.2023.12.03 val PER: 0.0683
2026-01-14 02:19:14,573: t15.2023.12.08 val PER: 0.0499
2026-01-14 02:19:14,573: t15.2023.12.10 val PER: 0.0434
2026-01-14 02:19:14,573: t15.2023.12.17 val PER: 0.0904
2026-01-14 02:19:14,573: t15.2023.12.29 val PER: 0.0858
2026-01-14 02:19:14,573: t15.2024.02.25 val PER: 0.0885
2026-01-14 02:19:14,573: t15.2024.03.08 val PER: 0.1778
2026-01-14 02:19:14,574: t15.2024.03.15 val PER: 0.1701
2026-01-14 02:19:14,574: t15.2024.03.17 val PER: 0.0927
2026-01-14 02:19:14,574: t15.2024.05.10 val PER: 0.1293
2026-01-14 02:19:14,574: t15.2024.06.14 val PER: 0.1278
2026-01-14 02:19:14,574: t15.2024.07.19 val PER: 0.1872
2026-01-14 02:19:14,574: t15.2024.07.21 val PER: 0.0766
2026-01-14 02:19:14,574: t15.2024.07.28 val PER: 0.1044
2026-01-14 02:19:14,574: t15.2025.01.10 val PER: 0.2700
2026-01-14 02:19:14,574: t15.2025.01.12 val PER: 0.1101
2026-01-14 02:19:14,574: t15.2025.03.14 val PER: 0.3062
2026-01-14 02:19:14,574: t15.2025.03.16 val PER: 0.1374
2026-01-14 02:19:14,574: t15.2025.03.30 val PER: 0.2299
2026-01-14 02:19:14,574: t15.2025.04.13 val PER: 0.2111
2026-01-14 02:19:14,710: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_22500
2026-01-14 02:19:23,566: Train batch 22600: loss: 5.98 grad norm: 56.33 time: 0.071
2026-01-14 02:19:41,438: Train batch 22800: loss: 4.99 grad norm: 57.33 time: 0.067
2026-01-14 02:19:59,148: Train batch 23000: loss: 4.60 grad norm: 59.55 time: 0.072
2026-01-14 02:19:59,148: Running test after training batch: 23000
2026-01-14 02:19:59,240: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:20:04,610: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:20:04,659: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:20:15,922: Val batch 23000: PER (avg): 0.1191 CTC Loss (avg): 25.4671 WER(5gram): 12.39% (n=256) time: 16.773
2026-01-14 02:20:15,922: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-14 02:20:15,922: t15.2023.08.13 val PER: 0.0769
2026-01-14 02:20:15,922: t15.2023.08.18 val PER: 0.0805
2026-01-14 02:20:15,923: t15.2023.08.20 val PER: 0.0723
2026-01-14 02:20:15,923: t15.2023.08.25 val PER: 0.0768
2026-01-14 02:20:15,923: t15.2023.08.27 val PER: 0.1688
2026-01-14 02:20:15,923: t15.2023.09.01 val PER: 0.0641
2026-01-14 02:20:15,923: t15.2023.09.03 val PER: 0.1390
2026-01-14 02:20:15,923: t15.2023.09.24 val PER: 0.0934
2026-01-14 02:20:15,923: t15.2023.09.29 val PER: 0.1181
2026-01-14 02:20:15,923: t15.2023.10.01 val PER: 0.1559
2026-01-14 02:20:15,923: t15.2023.10.06 val PER: 0.0667
2026-01-14 02:20:15,924: t15.2023.10.08 val PER: 0.2192
2026-01-14 02:20:15,924: t15.2023.10.13 val PER: 0.1753
2026-01-14 02:20:15,924: t15.2023.10.15 val PER: 0.1279
2026-01-14 02:20:15,924: t15.2023.10.20 val PER: 0.1879
2026-01-14 02:20:15,924: t15.2023.10.22 val PER: 0.1002
2026-01-14 02:20:15,924: t15.2023.11.03 val PER: 0.1621
2026-01-14 02:20:15,924: t15.2023.11.04 val PER: 0.0171
2026-01-14 02:20:15,924: t15.2023.11.17 val PER: 0.0280
2026-01-14 02:20:15,924: t15.2023.11.19 val PER: 0.0220
2026-01-14 02:20:15,924: t15.2023.11.26 val PER: 0.0594
2026-01-14 02:20:15,924: t15.2023.12.03 val PER: 0.0609
2026-01-14 02:20:15,924: t15.2023.12.08 val PER: 0.0539
2026-01-14 02:20:15,924: t15.2023.12.10 val PER: 0.0381
2026-01-14 02:20:15,925: t15.2023.12.17 val PER: 0.1008
2026-01-14 02:20:15,925: t15.2023.12.29 val PER: 0.0913
2026-01-14 02:20:15,925: t15.2024.02.25 val PER: 0.0941
2026-01-14 02:20:15,925: t15.2024.03.08 val PER: 0.1792
2026-01-14 02:20:15,925: t15.2024.03.15 val PER: 0.1632
2026-01-14 02:20:15,925: t15.2024.03.17 val PER: 0.1004
2026-01-14 02:20:15,925: t15.2024.05.10 val PER: 0.1337
2026-01-14 02:20:15,925: t15.2024.06.14 val PER: 0.1325
2026-01-14 02:20:15,925: t15.2024.07.19 val PER: 0.1912
2026-01-14 02:20:15,925: t15.2024.07.21 val PER: 0.0759
2026-01-14 02:20:15,925: t15.2024.07.28 val PER: 0.0956
2026-01-14 02:20:15,925: t15.2025.01.10 val PER: 0.2466
2026-01-14 02:20:15,925: t15.2025.01.12 val PER: 0.1055
2026-01-14 02:20:15,925: t15.2025.03.14 val PER: 0.3121
2026-01-14 02:20:15,925: t15.2025.03.16 val PER: 0.1374
2026-01-14 02:20:15,925: t15.2025.03.30 val PER: 0.2356
2026-01-14 02:20:15,926: t15.2025.04.13 val PER: 0.1954
2026-01-14 02:20:16,062: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_23000
2026-01-14 02:20:33,815: Train batch 23200: loss: 4.13 grad norm: 54.29 time: 0.071
2026-01-14 02:20:51,624: Train batch 23400: loss: 2.24 grad norm: 33.96 time: 0.081
2026-01-14 02:21:00,664: Running test after training batch: 23500
2026-01-14 02:21:00,894: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:21:06,259: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:21:06,312: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:21:17,474: Val batch 23500: PER (avg): 0.1202 CTC Loss (avg): 25.5396 WER(5gram): 12.97% (n=256) time: 16.810
2026-01-14 02:21:17,475: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-14 02:21:17,475: t15.2023.08.13 val PER: 0.0936
2026-01-14 02:21:17,475: t15.2023.08.18 val PER: 0.0746
2026-01-14 02:21:17,475: t15.2023.08.20 val PER: 0.0739
2026-01-14 02:21:17,475: t15.2023.08.25 val PER: 0.0798
2026-01-14 02:21:17,475: t15.2023.08.27 val PER: 0.1608
2026-01-14 02:21:17,475: t15.2023.09.01 val PER: 0.0536
2026-01-14 02:21:17,476: t15.2023.09.03 val PER: 0.1390
2026-01-14 02:21:17,476: t15.2023.09.24 val PER: 0.0983
2026-01-14 02:21:17,476: t15.2023.09.29 val PER: 0.1155
2026-01-14 02:21:17,476: t15.2023.10.01 val PER: 0.1513
2026-01-14 02:21:17,476: t15.2023.10.06 val PER: 0.0678
2026-01-14 02:21:17,476: t15.2023.10.08 val PER: 0.2192
2026-01-14 02:21:17,476: t15.2023.10.13 val PER: 0.1784
2026-01-14 02:21:17,476: t15.2023.10.15 val PER: 0.1325
2026-01-14 02:21:17,476: t15.2023.10.20 val PER: 0.1812
2026-01-14 02:21:17,476: t15.2023.10.22 val PER: 0.1013
2026-01-14 02:21:17,476: t15.2023.11.03 val PER: 0.1655
2026-01-14 02:21:17,477: t15.2023.11.04 val PER: 0.0239
2026-01-14 02:21:17,477: t15.2023.11.17 val PER: 0.0311
2026-01-14 02:21:17,477: t15.2023.11.19 val PER: 0.0200
2026-01-14 02:21:17,477: t15.2023.11.26 val PER: 0.0587
2026-01-14 02:21:17,477: t15.2023.12.03 val PER: 0.0693
2026-01-14 02:21:17,477: t15.2023.12.08 val PER: 0.0506
2026-01-14 02:21:17,477: t15.2023.12.10 val PER: 0.0434
2026-01-14 02:21:17,477: t15.2023.12.17 val PER: 0.1008
2026-01-14 02:21:17,478: t15.2023.12.29 val PER: 0.0872
2026-01-14 02:21:17,478: t15.2024.02.25 val PER: 0.0941
2026-01-14 02:21:17,478: t15.2024.03.08 val PER: 0.1721
2026-01-14 02:21:17,478: t15.2024.03.15 val PER: 0.1776
2026-01-14 02:21:17,478: t15.2024.03.17 val PER: 0.1039
2026-01-14 02:21:17,478: t15.2024.05.10 val PER: 0.1233
2026-01-14 02:21:17,478: t15.2024.06.14 val PER: 0.1230
2026-01-14 02:21:17,478: t15.2024.07.19 val PER: 0.1931
2026-01-14 02:21:17,478: t15.2024.07.21 val PER: 0.0759
2026-01-14 02:21:17,478: t15.2024.07.28 val PER: 0.1162
2026-01-14 02:21:17,478: t15.2025.01.10 val PER: 0.2479
2026-01-14 02:21:17,478: t15.2025.01.12 val PER: 0.1055
2026-01-14 02:21:17,478: t15.2025.03.14 val PER: 0.2988
2026-01-14 02:21:17,478: t15.2025.03.16 val PER: 0.1479
2026-01-14 02:21:17,478: t15.2025.03.30 val PER: 0.2287
2026-01-14 02:21:17,479: t15.2025.04.13 val PER: 0.1997
2026-01-14 02:21:17,618: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_23500
2026-01-14 02:21:27,178: Train batch 23600: loss: 1.07 grad norm: 23.35 time: 0.068
2026-01-14 02:21:44,681: Train batch 23800: loss: 3.58 grad norm: 79.77 time: 0.066
2026-01-14 02:22:02,218: Train batch 24000: loss: 3.20 grad norm: 45.89 time: 0.093
2026-01-14 02:22:02,218: Running test after training batch: 24000
2026-01-14 02:22:02,319: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:22:07,818: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:22:07,875: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:22:19,296: Val batch 24000: PER (avg): 0.1181 CTC Loss (avg): 25.6119 WER(5gram): 13.23% (n=256) time: 17.078
2026-01-14 02:22:19,296: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-14 02:22:19,297: t15.2023.08.13 val PER: 0.0956
2026-01-14 02:22:19,297: t15.2023.08.18 val PER: 0.0847
2026-01-14 02:22:19,297: t15.2023.08.20 val PER: 0.0810
2026-01-14 02:22:19,297: t15.2023.08.25 val PER: 0.0798
2026-01-14 02:22:19,297: t15.2023.08.27 val PER: 0.1576
2026-01-14 02:22:19,297: t15.2023.09.01 val PER: 0.0544
2026-01-14 02:22:19,297: t15.2023.09.03 val PER: 0.1318
2026-01-14 02:22:19,297: t15.2023.09.24 val PER: 0.0983
2026-01-14 02:22:19,297: t15.2023.09.29 val PER: 0.1142
2026-01-14 02:22:19,297: t15.2023.10.01 val PER: 0.1480
2026-01-14 02:22:19,297: t15.2023.10.06 val PER: 0.0614
2026-01-14 02:22:19,297: t15.2023.10.08 val PER: 0.2233
2026-01-14 02:22:19,298: t15.2023.10.13 val PER: 0.1885
2026-01-14 02:22:19,298: t15.2023.10.15 val PER: 0.1285
2026-01-14 02:22:19,298: t15.2023.10.20 val PER: 0.1846
2026-01-14 02:22:19,298: t15.2023.10.22 val PER: 0.1047
2026-01-14 02:22:19,298: t15.2023.11.03 val PER: 0.1662
2026-01-14 02:22:19,298: t15.2023.11.04 val PER: 0.0239
2026-01-14 02:22:19,299: t15.2023.11.17 val PER: 0.0264
2026-01-14 02:22:19,300: t15.2023.11.19 val PER: 0.0200
2026-01-14 02:22:19,300: t15.2023.11.26 val PER: 0.0609
2026-01-14 02:22:19,300: t15.2023.12.03 val PER: 0.0662
2026-01-14 02:22:19,300: t15.2023.12.08 val PER: 0.0426
2026-01-14 02:22:19,300: t15.2023.12.10 val PER: 0.0420
2026-01-14 02:22:19,300: t15.2023.12.17 val PER: 0.0915
2026-01-14 02:22:19,300: t15.2023.12.29 val PER: 0.0830
2026-01-14 02:22:19,300: t15.2024.02.25 val PER: 0.0857
2026-01-14 02:22:19,300: t15.2024.03.08 val PER: 0.1721
2026-01-14 02:22:19,300: t15.2024.03.15 val PER: 0.1651
2026-01-14 02:22:19,300: t15.2024.03.17 val PER: 0.0990
2026-01-14 02:22:19,300: t15.2024.05.10 val PER: 0.1322
2026-01-14 02:22:19,301: t15.2024.06.14 val PER: 0.1309
2026-01-14 02:22:19,301: t15.2024.07.19 val PER: 0.1813
2026-01-14 02:22:19,301: t15.2024.07.21 val PER: 0.0724
2026-01-14 02:22:19,301: t15.2024.07.28 val PER: 0.1051
2026-01-14 02:22:19,301: t15.2025.01.10 val PER: 0.2424
2026-01-14 02:22:19,301: t15.2025.01.12 val PER: 0.1008
2026-01-14 02:22:19,301: t15.2025.03.14 val PER: 0.3033
2026-01-14 02:22:19,301: t15.2025.03.16 val PER: 0.1374
2026-01-14 02:22:19,301: t15.2025.03.30 val PER: 0.2356
2026-01-14 02:22:19,301: t15.2025.04.13 val PER: 0.1954
2026-01-14 02:22:19,444: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_24000
2026-01-14 02:22:36,805: Train batch 24200: loss: 3.22 grad norm: 56.66 time: 0.065
2026-01-14 02:22:53,945: Train batch 24400: loss: 6.46 grad norm: 61.95 time: 0.063
2026-01-14 02:23:02,432: Running test after training batch: 24500
2026-01-14 02:23:02,551: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:23:08,018: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:23:08,070: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cent
2026-01-14 02:23:19,203: Val batch 24500: PER (avg): 0.1178 CTC Loss (avg): 25.6690 WER(5gram): 14.28% (n=256) time: 16.771
2026-01-14 02:23:19,203: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-14 02:23:19,204: t15.2023.08.13 val PER: 0.0956
2026-01-14 02:23:19,204: t15.2023.08.18 val PER: 0.0796
2026-01-14 02:23:19,204: t15.2023.08.20 val PER: 0.0763
2026-01-14 02:23:19,204: t15.2023.08.25 val PER: 0.0753
2026-01-14 02:23:19,204: t15.2023.08.27 val PER: 0.1527
2026-01-14 02:23:19,204: t15.2023.09.01 val PER: 0.0495
2026-01-14 02:23:19,204: t15.2023.09.03 val PER: 0.1390
2026-01-14 02:23:19,204: t15.2023.09.24 val PER: 0.0874
2026-01-14 02:23:19,204: t15.2023.09.29 val PER: 0.1174
2026-01-14 02:23:19,204: t15.2023.10.01 val PER: 0.1453
2026-01-14 02:23:19,204: t15.2023.10.06 val PER: 0.0624
2026-01-14 02:23:19,205: t15.2023.10.08 val PER: 0.2070
2026-01-14 02:23:19,205: t15.2023.10.13 val PER: 0.1753
2026-01-14 02:23:19,205: t15.2023.10.15 val PER: 0.1239
2026-01-14 02:23:19,205: t15.2023.10.20 val PER: 0.1879
2026-01-14 02:23:19,205: t15.2023.10.22 val PER: 0.1069
2026-01-14 02:23:19,205: t15.2023.11.03 val PER: 0.1567
2026-01-14 02:23:19,205: t15.2023.11.04 val PER: 0.0171
2026-01-14 02:23:19,205: t15.2023.11.17 val PER: 0.0249
2026-01-14 02:23:19,205: t15.2023.11.19 val PER: 0.0120
2026-01-14 02:23:19,205: t15.2023.11.26 val PER: 0.0623
2026-01-14 02:23:19,205: t15.2023.12.03 val PER: 0.0525
2026-01-14 02:23:19,205: t15.2023.12.08 val PER: 0.0433
2026-01-14 02:23:19,206: t15.2023.12.10 val PER: 0.0394
2026-01-14 02:23:19,206: t15.2023.12.17 val PER: 0.1050
2026-01-14 02:23:19,206: t15.2023.12.29 val PER: 0.0851
2026-01-14 02:23:19,206: t15.2024.02.25 val PER: 0.0899
2026-01-14 02:23:19,206: t15.2024.03.08 val PER: 0.1878
2026-01-14 02:23:19,206: t15.2024.03.15 val PER: 0.1807
2026-01-14 02:23:19,206: t15.2024.03.17 val PER: 0.0976
2026-01-14 02:23:19,206: t15.2024.05.10 val PER: 0.1352
2026-01-14 02:23:19,206: t15.2024.06.14 val PER: 0.1293
2026-01-14 02:23:19,206: t15.2024.07.19 val PER: 0.1931
2026-01-14 02:23:19,206: t15.2024.07.21 val PER: 0.0731
2026-01-14 02:23:19,206: t15.2024.07.28 val PER: 0.1044
2026-01-14 02:23:19,206: t15.2025.01.10 val PER: 0.2493
2026-01-14 02:23:19,206: t15.2025.01.12 val PER: 0.1024
2026-01-14 02:23:19,206: t15.2025.03.14 val PER: 0.3077
2026-01-14 02:23:19,206: t15.2025.03.16 val PER: 0.1401
2026-01-14 02:23:19,207: t15.2025.03.30 val PER: 0.2287
2026-01-14 02:23:19,207: t15.2025.04.13 val PER: 0.1926
2026-01-14 02:23:19,346: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_24500
2026-01-14 02:23:28,042: Train batch 24600: loss: 3.56 grad norm: 48.86 time: 0.068
2026-01-14 02:23:45,682: Train batch 24800: loss: 4.79 grad norm: 65.98 time: 0.090
2026-01-14 02:24:03,076: Train batch 25000: loss: 4.11 grad norm: 54.24 time: 0.077
2026-01-14 02:24:03,076: Running test after training batch: 25000
2026-01-14 02:24:03,197: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:24:09,054: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:24:09,111: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost now
2026-01-14 02:24:20,722: Val batch 25000: PER (avg): 0.1155 CTC Loss (avg): 26.4585 WER(5gram): 11.54% (n=256) time: 17.645
2026-01-14 02:24:20,722: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-14 02:24:20,722: t15.2023.08.13 val PER: 0.0946
2026-01-14 02:24:20,723: t15.2023.08.18 val PER: 0.0679
2026-01-14 02:24:20,723: t15.2023.08.20 val PER: 0.0691
2026-01-14 02:24:20,723: t15.2023.08.25 val PER: 0.0798
2026-01-14 02:24:20,723: t15.2023.08.27 val PER: 0.1672
2026-01-14 02:24:20,723: t15.2023.09.01 val PER: 0.0544
2026-01-14 02:24:20,723: t15.2023.09.03 val PER: 0.1140
2026-01-14 02:24:20,723: t15.2023.09.24 val PER: 0.0922
2026-01-14 02:24:20,724: t15.2023.09.29 val PER: 0.1168
2026-01-14 02:24:20,724: t15.2023.10.01 val PER: 0.1480
2026-01-14 02:24:20,724: t15.2023.10.06 val PER: 0.0635
2026-01-14 02:24:20,724: t15.2023.10.08 val PER: 0.2097
2026-01-14 02:24:20,724: t15.2023.10.13 val PER: 0.1738
2026-01-14 02:24:20,724: t15.2023.10.15 val PER: 0.1239
2026-01-14 02:24:20,724: t15.2023.10.20 val PER: 0.1678
2026-01-14 02:24:20,724: t15.2023.10.22 val PER: 0.1024
2026-01-14 02:24:20,724: t15.2023.11.03 val PER: 0.1567
2026-01-14 02:24:20,724: t15.2023.11.04 val PER: 0.0171
2026-01-14 02:24:20,724: t15.2023.11.17 val PER: 0.0280
2026-01-14 02:24:20,725: t15.2023.11.19 val PER: 0.0180
2026-01-14 02:24:20,725: t15.2023.11.26 val PER: 0.0580
2026-01-14 02:24:20,725: t15.2023.12.03 val PER: 0.0662
2026-01-14 02:24:20,725: t15.2023.12.08 val PER: 0.0426
2026-01-14 02:24:20,725: t15.2023.12.10 val PER: 0.0355
2026-01-14 02:24:20,725: t15.2023.12.17 val PER: 0.0884
2026-01-14 02:24:20,725: t15.2023.12.29 val PER: 0.0872
2026-01-14 02:24:20,725: t15.2024.02.25 val PER: 0.0927
2026-01-14 02:24:20,725: t15.2024.03.08 val PER: 0.1593
2026-01-14 02:24:20,725: t15.2024.03.15 val PER: 0.1726
2026-01-14 02:24:20,726: t15.2024.03.17 val PER: 0.1074
2026-01-14 02:24:20,726: t15.2024.05.10 val PER: 0.1248
2026-01-14 02:24:20,726: t15.2024.06.14 val PER: 0.1199
2026-01-14 02:24:20,726: t15.2024.07.19 val PER: 0.1800
2026-01-14 02:24:20,726: t15.2024.07.21 val PER: 0.0738
2026-01-14 02:24:20,726: t15.2024.07.28 val PER: 0.0963
2026-01-14 02:24:20,726: t15.2025.01.10 val PER: 0.2438
2026-01-14 02:24:20,726: t15.2025.01.12 val PER: 0.1186
2026-01-14 02:24:20,726: t15.2025.03.14 val PER: 0.2914
2026-01-14 02:24:20,726: t15.2025.03.16 val PER: 0.1387
2026-01-14 02:24:20,726: t15.2025.03.30 val PER: 0.2241
2026-01-14 02:24:20,727: t15.2025.04.13 val PER: 0.1912
2026-01-14 02:24:20,864: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_25000
2026-01-14 02:24:38,695: Train batch 25200: loss: 2.39 grad norm: 81.25 time: 0.066
2026-01-14 02:24:56,394: Train batch 25400: loss: 2.60 grad norm: 44.00 time: 0.064
2026-01-14 02:25:05,116: Running test after training batch: 25500
2026-01-14 02:25:05,270: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:25:10,988: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:25:11,042: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-14 02:25:23,455: Val batch 25500: PER (avg): 0.1166 CTC Loss (avg): 26.0399 WER(5gram): 11.28% (n=256) time: 18.338
2026-01-14 02:25:23,456: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-14 02:25:23,456: t15.2023.08.13 val PER: 0.0873
2026-01-14 02:25:23,456: t15.2023.08.18 val PER: 0.0872
2026-01-14 02:25:23,456: t15.2023.08.20 val PER: 0.0731
2026-01-14 02:25:23,456: t15.2023.08.25 val PER: 0.0723
2026-01-14 02:25:23,456: t15.2023.08.27 val PER: 0.1543
2026-01-14 02:25:23,456: t15.2023.09.01 val PER: 0.0528
2026-01-14 02:25:23,457: t15.2023.09.03 val PER: 0.1259
2026-01-14 02:25:23,457: t15.2023.09.24 val PER: 0.0959
2026-01-14 02:25:23,457: t15.2023.09.29 val PER: 0.1110
2026-01-14 02:25:23,457: t15.2023.10.01 val PER: 0.1453
2026-01-14 02:25:23,457: t15.2023.10.06 val PER: 0.0657
2026-01-14 02:25:23,457: t15.2023.10.08 val PER: 0.2084
2026-01-14 02:25:23,457: t15.2023.10.13 val PER: 0.1854
2026-01-14 02:25:23,458: t15.2023.10.15 val PER: 0.1226
2026-01-14 02:25:23,458: t15.2023.10.20 val PER: 0.1678
2026-01-14 02:25:23,458: t15.2023.10.22 val PER: 0.1002
2026-01-14 02:25:23,458: t15.2023.11.03 val PER: 0.1655
2026-01-14 02:25:23,458: t15.2023.11.04 val PER: 0.0239
2026-01-14 02:25:23,458: t15.2023.11.17 val PER: 0.0187
2026-01-14 02:25:23,458: t15.2023.11.19 val PER: 0.0200
2026-01-14 02:25:23,458: t15.2023.11.26 val PER: 0.0565
2026-01-14 02:25:23,458: t15.2023.12.03 val PER: 0.0662
2026-01-14 02:25:23,459: t15.2023.12.08 val PER: 0.0459
2026-01-14 02:25:23,459: t15.2023.12.10 val PER: 0.0381
2026-01-14 02:25:23,459: t15.2023.12.17 val PER: 0.0946
2026-01-14 02:25:23,459: t15.2023.12.29 val PER: 0.0830
2026-01-14 02:25:23,459: t15.2024.02.25 val PER: 0.0829
2026-01-14 02:25:23,459: t15.2024.03.08 val PER: 0.1750
2026-01-14 02:25:23,459: t15.2024.03.15 val PER: 0.1689
2026-01-14 02:25:23,459: t15.2024.03.17 val PER: 0.1081
2026-01-14 02:25:23,459: t15.2024.05.10 val PER: 0.1263
2026-01-14 02:25:23,459: t15.2024.06.14 val PER: 0.1230
2026-01-14 02:25:23,460: t15.2024.07.19 val PER: 0.1912
2026-01-14 02:25:23,460: t15.2024.07.21 val PER: 0.0710
2026-01-14 02:25:23,460: t15.2024.07.28 val PER: 0.1059
2026-01-14 02:25:23,460: t15.2025.01.10 val PER: 0.2507
2026-01-14 02:25:23,460: t15.2025.01.12 val PER: 0.0970
2026-01-14 02:25:23,460: t15.2025.03.14 val PER: 0.3047
2026-01-14 02:25:23,460: t15.2025.03.16 val PER: 0.1466
2026-01-14 02:25:23,460: t15.2025.03.30 val PER: 0.2149
2026-01-14 02:25:23,460: t15.2025.04.13 val PER: 0.1912
2026-01-14 02:25:23,596: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_25500
2026-01-14 02:25:32,288: Train batch 25600: loss: 3.40 grad norm: 49.65 time: 0.068
2026-01-14 02:25:49,789: Train batch 25800: loss: 2.07 grad norm: 38.12 time: 0.073
2026-01-14 02:26:07,290: Train batch 26000: loss: 2.23 grad norm: 39.86 time: 0.072
2026-01-14 02:26:07,290: Running test after training batch: 26000
2026-01-14 02:26:07,414: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:26:12,819: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:26:12,874: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-14 02:26:25,099: Val batch 26000: PER (avg): 0.1159 CTC Loss (avg): 26.2848 WER(5gram): 11.73% (n=256) time: 17.809
2026-01-14 02:26:25,100: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-14 02:26:25,100: t15.2023.08.13 val PER: 0.0863
2026-01-14 02:26:25,100: t15.2023.08.18 val PER: 0.0704
2026-01-14 02:26:25,100: t15.2023.08.20 val PER: 0.0723
2026-01-14 02:26:25,100: t15.2023.08.25 val PER: 0.0663
2026-01-14 02:26:25,100: t15.2023.08.27 val PER: 0.1511
2026-01-14 02:26:25,100: t15.2023.09.01 val PER: 0.0544
2026-01-14 02:26:25,100: t15.2023.09.03 val PER: 0.1211
2026-01-14 02:26:25,101: t15.2023.09.24 val PER: 0.0959
2026-01-14 02:26:25,101: t15.2023.09.29 val PER: 0.1155
2026-01-14 02:26:25,101: t15.2023.10.01 val PER: 0.1513
2026-01-14 02:26:25,101: t15.2023.10.06 val PER: 0.0538
2026-01-14 02:26:25,101: t15.2023.10.08 val PER: 0.2070
2026-01-14 02:26:25,101: t15.2023.10.13 val PER: 0.1777
2026-01-14 02:26:25,101: t15.2023.10.15 val PER: 0.1200
2026-01-14 02:26:25,101: t15.2023.10.20 val PER: 0.1779
2026-01-14 02:26:25,101: t15.2023.10.22 val PER: 0.0924
2026-01-14 02:26:25,116: t15.2023.11.03 val PER: 0.1669
2026-01-14 02:26:25,116: t15.2023.11.04 val PER: 0.0307
2026-01-14 02:26:25,116: t15.2023.11.17 val PER: 0.0233
2026-01-14 02:26:25,116: t15.2023.11.19 val PER: 0.0120
2026-01-14 02:26:25,117: t15.2023.11.26 val PER: 0.0558
2026-01-14 02:26:25,117: t15.2023.12.03 val PER: 0.0630
2026-01-14 02:26:25,117: t15.2023.12.08 val PER: 0.0453
2026-01-14 02:26:25,117: t15.2023.12.10 val PER: 0.0368
2026-01-14 02:26:25,117: t15.2023.12.17 val PER: 0.0956
2026-01-14 02:26:25,117: t15.2023.12.29 val PER: 0.0810
2026-01-14 02:26:25,117: t15.2024.02.25 val PER: 0.0843
2026-01-14 02:26:25,117: t15.2024.03.08 val PER: 0.1778
2026-01-14 02:26:25,117: t15.2024.03.15 val PER: 0.1764
2026-01-14 02:26:25,117: t15.2024.03.17 val PER: 0.1074
2026-01-14 02:26:25,117: t15.2024.05.10 val PER: 0.1218
2026-01-14 02:26:25,117: t15.2024.06.14 val PER: 0.1246
2026-01-14 02:26:25,117: t15.2024.07.19 val PER: 0.1846
2026-01-14 02:26:25,117: t15.2024.07.21 val PER: 0.0697
2026-01-14 02:26:25,118: t15.2024.07.28 val PER: 0.1015
2026-01-14 02:26:25,118: t15.2025.01.10 val PER: 0.2576
2026-01-14 02:26:25,118: t15.2025.01.12 val PER: 0.1039
2026-01-14 02:26:25,118: t15.2025.03.14 val PER: 0.3092
2026-01-14 02:26:25,118: t15.2025.03.16 val PER: 0.1348
2026-01-14 02:26:25,118: t15.2025.03.30 val PER: 0.2299
2026-01-14 02:26:25,118: t15.2025.04.13 val PER: 0.1969
2026-01-14 02:26:25,269: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_26000
2026-01-14 02:26:43,793: Train batch 26200: loss: 1.24 grad norm: 33.07 time: 0.073
2026-01-14 02:27:01,744: Train batch 26400: loss: 3.45 grad norm: 43.32 time: 0.089
2026-01-14 02:27:10,630: Running test after training batch: 26500
2026-01-14 02:27:10,741: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:27:16,171: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:27:16,226: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:27:28,700: Val batch 26500: PER (avg): 0.1144 CTC Loss (avg): 25.8844 WER(5gram): 10.95% (n=256) time: 18.069
2026-01-14 02:27:28,701: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-14 02:27:28,701: t15.2023.08.13 val PER: 0.0915
2026-01-14 02:27:28,701: t15.2023.08.18 val PER: 0.0729
2026-01-14 02:27:28,701: t15.2023.08.20 val PER: 0.0683
2026-01-14 02:27:28,701: t15.2023.08.25 val PER: 0.0798
2026-01-14 02:27:28,701: t15.2023.08.27 val PER: 0.1656
2026-01-14 02:27:28,701: t15.2023.09.01 val PER: 0.0528
2026-01-14 02:27:28,701: t15.2023.09.03 val PER: 0.1259
2026-01-14 02:27:28,701: t15.2023.09.24 val PER: 0.0922
2026-01-14 02:27:28,701: t15.2023.09.29 val PER: 0.1072
2026-01-14 02:27:28,701: t15.2023.10.01 val PER: 0.1440
2026-01-14 02:27:28,702: t15.2023.10.06 val PER: 0.0657
2026-01-14 02:27:28,702: t15.2023.10.08 val PER: 0.2233
2026-01-14 02:27:28,702: t15.2023.10.13 val PER: 0.1753
2026-01-14 02:27:28,702: t15.2023.10.15 val PER: 0.1134
2026-01-14 02:27:28,702: t15.2023.10.20 val PER: 0.1644
2026-01-14 02:27:28,702: t15.2023.10.22 val PER: 0.1013
2026-01-14 02:27:28,702: t15.2023.11.03 val PER: 0.1594
2026-01-14 02:27:28,702: t15.2023.11.04 val PER: 0.0307
2026-01-14 02:27:28,702: t15.2023.11.17 val PER: 0.0264
2026-01-14 02:27:28,702: t15.2023.11.19 val PER: 0.0140
2026-01-14 02:27:28,702: t15.2023.11.26 val PER: 0.0638
2026-01-14 02:27:28,703: t15.2023.12.03 val PER: 0.0672
2026-01-14 02:27:28,703: t15.2023.12.08 val PER: 0.0399
2026-01-14 02:27:28,703: t15.2023.12.10 val PER: 0.0473
2026-01-14 02:27:28,703: t15.2023.12.17 val PER: 0.0946
2026-01-14 02:27:28,703: t15.2023.12.29 val PER: 0.0789
2026-01-14 02:27:28,703: t15.2024.02.25 val PER: 0.0801
2026-01-14 02:27:28,703: t15.2024.03.08 val PER: 0.1792
2026-01-14 02:27:28,703: t15.2024.03.15 val PER: 0.1701
2026-01-14 02:27:28,703: t15.2024.03.17 val PER: 0.0927
2026-01-14 02:27:28,703: t15.2024.05.10 val PER: 0.1174
2026-01-14 02:27:28,703: t15.2024.06.14 val PER: 0.1262
2026-01-14 02:27:28,703: t15.2024.07.19 val PER: 0.1800
2026-01-14 02:27:28,703: t15.2024.07.21 val PER: 0.0683
2026-01-14 02:27:28,703: t15.2024.07.28 val PER: 0.1015
2026-01-14 02:27:28,703: t15.2025.01.10 val PER: 0.2631
2026-01-14 02:27:28,704: t15.2025.01.12 val PER: 0.1001
2026-01-14 02:27:28,704: t15.2025.03.14 val PER: 0.3018
2026-01-14 02:27:28,704: t15.2025.03.16 val PER: 0.1374
2026-01-14 02:27:28,704: t15.2025.03.30 val PER: 0.2172
2026-01-14 02:27:28,704: t15.2025.04.13 val PER: 0.1783
2026-01-14 02:27:28,705: New best val WER(5gram) 11.08% --> 10.95%
2026-01-14 02:27:28,847: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_26500
2026-01-14 02:27:37,734: Train batch 26600: loss: 1.30 grad norm: 34.97 time: 0.068
2026-01-14 02:27:55,249: Train batch 26800: loss: 4.16 grad norm: 53.13 time: 0.091
2026-01-14 02:28:12,593: Train batch 27000: loss: 3.25 grad norm: 46.49 time: 0.073
2026-01-14 02:28:12,593: Running test after training batch: 27000
2026-01-14 02:28:12,689: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:28:18,160: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:28:18,214: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost said
2026-01-14 02:28:30,610: Val batch 27000: PER (avg): 0.1158 CTC Loss (avg): 26.6380 WER(5gram): 12.13% (n=256) time: 18.017
2026-01-14 02:28:30,611: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-14 02:28:30,611: t15.2023.08.13 val PER: 0.0915
2026-01-14 02:28:30,611: t15.2023.08.18 val PER: 0.0771
2026-01-14 02:28:30,611: t15.2023.08.20 val PER: 0.0739
2026-01-14 02:28:30,611: t15.2023.08.25 val PER: 0.0753
2026-01-14 02:28:30,611: t15.2023.08.27 val PER: 0.1559
2026-01-14 02:28:30,611: t15.2023.09.01 val PER: 0.0576
2026-01-14 02:28:30,611: t15.2023.09.03 val PER: 0.1235
2026-01-14 02:28:30,611: t15.2023.09.24 val PER: 0.0947
2026-01-14 02:28:30,611: t15.2023.09.29 val PER: 0.1104
2026-01-14 02:28:30,611: t15.2023.10.01 val PER: 0.1532
2026-01-14 02:28:30,611: t15.2023.10.06 val PER: 0.0538
2026-01-14 02:28:30,611: t15.2023.10.08 val PER: 0.2219
2026-01-14 02:28:30,612: t15.2023.10.13 val PER: 0.1707
2026-01-14 02:28:30,612: t15.2023.10.15 val PER: 0.1252
2026-01-14 02:28:30,612: t15.2023.10.20 val PER: 0.1577
2026-01-14 02:28:30,612: t15.2023.10.22 val PER: 0.0980
2026-01-14 02:28:30,612: t15.2023.11.03 val PER: 0.1662
2026-01-14 02:28:30,612: t15.2023.11.04 val PER: 0.0205
2026-01-14 02:28:30,612: t15.2023.11.17 val PER: 0.0311
2026-01-14 02:28:30,612: t15.2023.11.19 val PER: 0.0140
2026-01-14 02:28:30,612: t15.2023.11.26 val PER: 0.0659
2026-01-14 02:28:30,612: t15.2023.12.03 val PER: 0.0609
2026-01-14 02:28:30,612: t15.2023.12.08 val PER: 0.0399
2026-01-14 02:28:30,612: t15.2023.12.10 val PER: 0.0381
2026-01-14 02:28:30,612: t15.2023.12.17 val PER: 0.0915
2026-01-14 02:28:30,612: t15.2023.12.29 val PER: 0.0851
2026-01-14 02:28:30,613: t15.2024.02.25 val PER: 0.0829
2026-01-14 02:28:30,613: t15.2024.03.08 val PER: 0.1878
2026-01-14 02:28:30,613: t15.2024.03.15 val PER: 0.1682
2026-01-14 02:28:30,613: t15.2024.03.17 val PER: 0.0990
2026-01-14 02:28:30,613: t15.2024.05.10 val PER: 0.1189
2026-01-14 02:28:30,613: t15.2024.06.14 val PER: 0.1215
2026-01-14 02:28:30,613: t15.2024.07.19 val PER: 0.1813
2026-01-14 02:28:30,613: t15.2024.07.21 val PER: 0.0752
2026-01-14 02:28:30,613: t15.2024.07.28 val PER: 0.0978
2026-01-14 02:28:30,613: t15.2025.01.10 val PER: 0.2590
2026-01-14 02:28:30,613: t15.2025.01.12 val PER: 0.0993
2026-01-14 02:28:30,613: t15.2025.03.14 val PER: 0.3033
2026-01-14 02:28:30,613: t15.2025.03.16 val PER: 0.1322
2026-01-14 02:28:30,613: t15.2025.03.30 val PER: 0.2241
2026-01-14 02:28:30,613: t15.2025.04.13 val PER: 0.1897
2026-01-14 02:28:30,753: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_27000
2026-01-14 02:28:48,272: Train batch 27200: loss: 3.74 grad norm: 63.28 time: 0.066
2026-01-14 02:29:06,176: Train batch 27400: loss: 4.24 grad norm: 55.16 time: 0.068
2026-01-14 02:29:15,127: Running test after training batch: 27500
2026-01-14 02:29:15,234: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:29:20,655: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:29:20,719: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:29:32,899: Val batch 27500: PER (avg): 0.1144 CTC Loss (avg): 26.3159 WER(5gram): 11.80% (n=256) time: 17.772
2026-01-14 02:29:32,900: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-14 02:29:32,900: t15.2023.08.13 val PER: 0.0915
2026-01-14 02:29:32,900: t15.2023.08.18 val PER: 0.0788
2026-01-14 02:29:32,900: t15.2023.08.20 val PER: 0.0723
2026-01-14 02:29:32,900: t15.2023.08.25 val PER: 0.0783
2026-01-14 02:29:32,900: t15.2023.08.27 val PER: 0.1576
2026-01-14 02:29:32,900: t15.2023.09.01 val PER: 0.0487
2026-01-14 02:29:32,900: t15.2023.09.03 val PER: 0.1283
2026-01-14 02:29:32,901: t15.2023.09.24 val PER: 0.0922
2026-01-14 02:29:32,901: t15.2023.09.29 val PER: 0.1142
2026-01-14 02:29:32,901: t15.2023.10.01 val PER: 0.1513
2026-01-14 02:29:32,901: t15.2023.10.06 val PER: 0.0603
2026-01-14 02:29:32,901: t15.2023.10.08 val PER: 0.2260
2026-01-14 02:29:32,901: t15.2023.10.13 val PER: 0.1746
2026-01-14 02:29:32,901: t15.2023.10.15 val PER: 0.1213
2026-01-14 02:29:32,901: t15.2023.10.20 val PER: 0.1711
2026-01-14 02:29:32,901: t15.2023.10.22 val PER: 0.0935
2026-01-14 02:29:32,901: t15.2023.11.03 val PER: 0.1615
2026-01-14 02:29:32,901: t15.2023.11.04 val PER: 0.0137
2026-01-14 02:29:32,901: t15.2023.11.17 val PER: 0.0249
2026-01-14 02:29:32,901: t15.2023.11.19 val PER: 0.0120
2026-01-14 02:29:32,901: t15.2023.11.26 val PER: 0.0565
2026-01-14 02:29:32,901: t15.2023.12.03 val PER: 0.0578
2026-01-14 02:29:32,902: t15.2023.12.08 val PER: 0.0413
2026-01-14 02:29:32,902: t15.2023.12.10 val PER: 0.0355
2026-01-14 02:29:32,902: t15.2023.12.17 val PER: 0.0915
2026-01-14 02:29:32,902: t15.2023.12.29 val PER: 0.0776
2026-01-14 02:29:32,902: t15.2024.02.25 val PER: 0.0885
2026-01-14 02:29:32,902: t15.2024.03.08 val PER: 0.1735
2026-01-14 02:29:32,902: t15.2024.03.15 val PER: 0.1682
2026-01-14 02:29:32,902: t15.2024.03.17 val PER: 0.0914
2026-01-14 02:29:32,903: t15.2024.05.10 val PER: 0.1293
2026-01-14 02:29:32,903: t15.2024.06.14 val PER: 0.1262
2026-01-14 02:29:32,903: t15.2024.07.19 val PER: 0.1833
2026-01-14 02:29:32,903: t15.2024.07.21 val PER: 0.0676
2026-01-14 02:29:32,903: t15.2024.07.28 val PER: 0.1015
2026-01-14 02:29:32,903: t15.2025.01.10 val PER: 0.2466
2026-01-14 02:29:32,903: t15.2025.01.12 val PER: 0.0947
2026-01-14 02:29:32,903: t15.2025.03.14 val PER: 0.3151
2026-01-14 02:29:32,903: t15.2025.03.16 val PER: 0.1440
2026-01-14 02:29:32,903: t15.2025.03.30 val PER: 0.2103
2026-01-14 02:29:32,903: t15.2025.04.13 val PER: 0.1912
2026-01-14 02:29:33,038: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_27500
2026-01-14 02:29:42,141: Train batch 27600: loss: 7.56 grad norm: 83.57 time: 0.072
2026-01-14 02:30:00,066: Train batch 27800: loss: 3.32 grad norm: 45.51 time: 0.049
2026-01-14 02:30:17,870: Train batch 28000: loss: 2.07 grad norm: 51.97 time: 0.063
2026-01-14 02:30:17,871: Running test after training batch: 28000
2026-01-14 02:30:18,024: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:30:23,563: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:30:23,615: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:30:35,494: Val batch 28000: PER (avg): 0.1166 CTC Loss (avg): 27.1179 WER(5gram): 12.91% (n=256) time: 17.624
2026-01-14 02:30:35,495: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-14 02:30:35,495: t15.2023.08.13 val PER: 0.0821
2026-01-14 02:30:35,496: t15.2023.08.18 val PER: 0.0712
2026-01-14 02:30:35,496: t15.2023.08.20 val PER: 0.0691
2026-01-14 02:30:35,496: t15.2023.08.25 val PER: 0.0813
2026-01-14 02:30:35,496: t15.2023.08.27 val PER: 0.1704
2026-01-14 02:30:35,496: t15.2023.09.01 val PER: 0.0487
2026-01-14 02:30:35,496: t15.2023.09.03 val PER: 0.1283
2026-01-14 02:30:35,496: t15.2023.09.24 val PER: 0.0934
2026-01-14 02:30:35,497: t15.2023.09.29 val PER: 0.1130
2026-01-14 02:30:35,497: t15.2023.10.01 val PER: 0.1413
2026-01-14 02:30:35,497: t15.2023.10.06 val PER: 0.0614
2026-01-14 02:30:35,497: t15.2023.10.08 val PER: 0.2260
2026-01-14 02:30:35,497: t15.2023.10.13 val PER: 0.1846
2026-01-14 02:30:35,497: t15.2023.10.15 val PER: 0.1233
2026-01-14 02:30:35,497: t15.2023.10.20 val PER: 0.1711
2026-01-14 02:30:35,498: t15.2023.10.22 val PER: 0.0969
2026-01-14 02:30:35,498: t15.2023.11.03 val PER: 0.1601
2026-01-14 02:30:35,498: t15.2023.11.04 val PER: 0.0102
2026-01-14 02:30:35,498: t15.2023.11.17 val PER: 0.0280
2026-01-14 02:30:35,498: t15.2023.11.19 val PER: 0.0220
2026-01-14 02:30:35,498: t15.2023.11.26 val PER: 0.0652
2026-01-14 02:30:35,498: t15.2023.12.03 val PER: 0.0567
2026-01-14 02:30:35,499: t15.2023.12.08 val PER: 0.0546
2026-01-14 02:30:35,511: t15.2023.12.10 val PER: 0.0355
2026-01-14 02:30:35,511: t15.2023.12.17 val PER: 0.1040
2026-01-14 02:30:35,511: t15.2023.12.29 val PER: 0.0830
2026-01-14 02:30:35,512: t15.2024.02.25 val PER: 0.0758
2026-01-14 02:30:35,512: t15.2024.03.08 val PER: 0.1721
2026-01-14 02:30:35,512: t15.2024.03.15 val PER: 0.1739
2026-01-14 02:30:35,512: t15.2024.03.17 val PER: 0.0969
2026-01-14 02:30:35,512: t15.2024.05.10 val PER: 0.1204
2026-01-14 02:30:35,512: t15.2024.06.14 val PER: 0.1230
2026-01-14 02:30:35,512: t15.2024.07.19 val PER: 0.1859
2026-01-14 02:30:35,512: t15.2024.07.21 val PER: 0.0752
2026-01-14 02:30:35,512: t15.2024.07.28 val PER: 0.1037
2026-01-14 02:30:35,512: t15.2025.01.10 val PER: 0.2397
2026-01-14 02:30:35,512: t15.2025.01.12 val PER: 0.1085
2026-01-14 02:30:35,513: t15.2025.03.14 val PER: 0.3062
2026-01-14 02:30:35,513: t15.2025.03.16 val PER: 0.1414
2026-01-14 02:30:35,513: t15.2025.03.30 val PER: 0.2310
2026-01-14 02:30:35,513: t15.2025.04.13 val PER: 0.1940
2026-01-14 02:30:35,661: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_28000
2026-01-14 02:30:53,682: Train batch 28200: loss: 1.80 grad norm: 32.91 time: 0.076
2026-01-14 02:31:11,271: Train batch 28400: loss: 2.71 grad norm: 40.71 time: 0.071
2026-01-14 02:31:20,014: Running test after training batch: 28500
2026-01-14 02:31:20,106: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:31:25,685: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:31:25,740: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:31:37,104: Val batch 28500: PER (avg): 0.1153 CTC Loss (avg): 27.2442 WER(5gram): 12.65% (n=256) time: 17.089
2026-01-14 02:31:37,104: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-14 02:31:37,104: t15.2023.08.13 val PER: 0.0956
2026-01-14 02:31:37,105: t15.2023.08.18 val PER: 0.0763
2026-01-14 02:31:37,105: t15.2023.08.20 val PER: 0.0778
2026-01-14 02:31:37,105: t15.2023.08.25 val PER: 0.0693
2026-01-14 02:31:37,105: t15.2023.08.27 val PER: 0.1624
2026-01-14 02:31:37,105: t15.2023.09.01 val PER: 0.0528
2026-01-14 02:31:37,105: t15.2023.09.03 val PER: 0.1318
2026-01-14 02:31:37,105: t15.2023.09.24 val PER: 0.0959
2026-01-14 02:31:37,105: t15.2023.09.29 val PER: 0.1130
2026-01-14 02:31:37,105: t15.2023.10.01 val PER: 0.1499
2026-01-14 02:31:37,105: t15.2023.10.06 val PER: 0.0527
2026-01-14 02:31:37,106: t15.2023.10.08 val PER: 0.2206
2026-01-14 02:31:37,106: t15.2023.10.13 val PER: 0.1699
2026-01-14 02:31:37,106: t15.2023.10.15 val PER: 0.1206
2026-01-14 02:31:37,106: t15.2023.10.20 val PER: 0.1644
2026-01-14 02:31:37,106: t15.2023.10.22 val PER: 0.0947
2026-01-14 02:31:37,106: t15.2023.11.03 val PER: 0.1615
2026-01-14 02:31:37,106: t15.2023.11.04 val PER: 0.0205
2026-01-14 02:31:37,106: t15.2023.11.17 val PER: 0.0311
2026-01-14 02:31:37,106: t15.2023.11.19 val PER: 0.0180
2026-01-14 02:31:37,107: t15.2023.11.26 val PER: 0.0551
2026-01-14 02:31:37,107: t15.2023.12.03 val PER: 0.0651
2026-01-14 02:31:37,107: t15.2023.12.08 val PER: 0.0499
2026-01-14 02:31:37,108: t15.2023.12.10 val PER: 0.0394
2026-01-14 02:31:37,108: t15.2023.12.17 val PER: 0.1008
2026-01-14 02:31:37,108: t15.2023.12.29 val PER: 0.0728
2026-01-14 02:31:37,108: t15.2024.02.25 val PER: 0.0857
2026-01-14 02:31:37,108: t15.2024.03.08 val PER: 0.1735
2026-01-14 02:31:37,108: t15.2024.03.15 val PER: 0.1632
2026-01-14 02:31:37,108: t15.2024.03.17 val PER: 0.0900
2026-01-14 02:31:37,108: t15.2024.05.10 val PER: 0.1233
2026-01-14 02:31:37,108: t15.2024.06.14 val PER: 0.1246
2026-01-14 02:31:37,108: t15.2024.07.19 val PER: 0.1964
2026-01-14 02:31:37,109: t15.2024.07.21 val PER: 0.0634
2026-01-14 02:31:37,109: t15.2024.07.28 val PER: 0.1096
2026-01-14 02:31:37,109: t15.2025.01.10 val PER: 0.2466
2026-01-14 02:31:37,109: t15.2025.01.12 val PER: 0.1016
2026-01-14 02:31:37,109: t15.2025.03.14 val PER: 0.2899
2026-01-14 02:31:37,109: t15.2025.03.16 val PER: 0.1296
2026-01-14 02:31:37,109: t15.2025.03.30 val PER: 0.2299
2026-01-14 02:31:37,109: t15.2025.04.13 val PER: 0.1969
2026-01-14 02:31:37,247: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_28500
2026-01-14 02:31:46,113: Train batch 28600: loss: 2.83 grad norm: 41.41 time: 0.071
2026-01-14 02:32:03,625: Train batch 28800: loss: 1.72 grad norm: 51.97 time: 0.056
2026-01-14 02:32:21,117: Train batch 29000: loss: 2.11 grad norm: 44.20 time: 0.059
2026-01-14 02:32:21,117: Running test after training batch: 29000
2026-01-14 02:32:21,256: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:32:26,842: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:32:26,896: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost said
2026-01-14 02:32:38,707: Val batch 29000: PER (avg): 0.1131 CTC Loss (avg): 26.8599 WER(5gram): 12.65% (n=256) time: 17.590
2026-01-14 02:32:38,708: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-14 02:32:38,708: t15.2023.08.13 val PER: 0.0894
2026-01-14 02:32:38,708: t15.2023.08.18 val PER: 0.0679
2026-01-14 02:32:38,708: t15.2023.08.20 val PER: 0.0691
2026-01-14 02:32:38,708: t15.2023.08.25 val PER: 0.0708
2026-01-14 02:32:38,708: t15.2023.08.27 val PER: 0.1543
2026-01-14 02:32:38,708: t15.2023.09.01 val PER: 0.0511
2026-01-14 02:32:38,708: t15.2023.09.03 val PER: 0.1401
2026-01-14 02:32:38,708: t15.2023.09.24 val PER: 0.0910
2026-01-14 02:32:38,708: t15.2023.09.29 val PER: 0.1117
2026-01-14 02:32:38,709: t15.2023.10.01 val PER: 0.1361
2026-01-14 02:32:38,709: t15.2023.10.06 val PER: 0.0549
2026-01-14 02:32:38,709: t15.2023.10.08 val PER: 0.2246
2026-01-14 02:32:38,709: t15.2023.10.13 val PER: 0.1792
2026-01-14 02:32:38,709: t15.2023.10.15 val PER: 0.1200
2026-01-14 02:32:38,709: t15.2023.10.20 val PER: 0.1745
2026-01-14 02:32:38,709: t15.2023.10.22 val PER: 0.0913
2026-01-14 02:32:38,709: t15.2023.11.03 val PER: 0.1547
2026-01-14 02:32:38,709: t15.2023.11.04 val PER: 0.0205
2026-01-14 02:32:38,709: t15.2023.11.17 val PER: 0.0249
2026-01-14 02:32:38,709: t15.2023.11.19 val PER: 0.0160
2026-01-14 02:32:38,709: t15.2023.11.26 val PER: 0.0543
2026-01-14 02:32:38,710: t15.2023.12.03 val PER: 0.0536
2026-01-14 02:32:38,710: t15.2023.12.08 val PER: 0.0386
2026-01-14 02:32:38,710: t15.2023.12.10 val PER: 0.0355
2026-01-14 02:32:38,710: t15.2023.12.17 val PER: 0.0884
2026-01-14 02:32:38,710: t15.2023.12.29 val PER: 0.0803
2026-01-14 02:32:38,710: t15.2024.02.25 val PER: 0.0772
2026-01-14 02:32:38,710: t15.2024.03.08 val PER: 0.1664
2026-01-14 02:32:38,710: t15.2024.03.15 val PER: 0.1776
2026-01-14 02:32:38,710: t15.2024.03.17 val PER: 0.0907
2026-01-14 02:32:38,710: t15.2024.05.10 val PER: 0.1263
2026-01-14 02:32:38,710: t15.2024.06.14 val PER: 0.1246
2026-01-14 02:32:38,711: t15.2024.07.19 val PER: 0.1740
2026-01-14 02:32:38,711: t15.2024.07.21 val PER: 0.0710
2026-01-14 02:32:38,711: t15.2024.07.28 val PER: 0.1000
2026-01-14 02:32:38,711: t15.2025.01.10 val PER: 0.2603
2026-01-14 02:32:38,711: t15.2025.01.12 val PER: 0.0978
2026-01-14 02:32:38,711: t15.2025.03.14 val PER: 0.3092
2026-01-14 02:32:38,711: t15.2025.03.16 val PER: 0.1322
2026-01-14 02:32:38,711: t15.2025.03.30 val PER: 0.2207
2026-01-14 02:32:38,711: t15.2025.04.13 val PER: 0.2011
2026-01-14 02:32:38,850: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_29000
2026-01-14 02:32:56,454: Train batch 29200: loss: 2.39 grad norm: 40.47 time: 0.077
2026-01-14 02:33:14,461: Train batch 29400: loss: 1.54 grad norm: 40.18 time: 0.070
2026-01-14 02:33:23,331: Running test after training batch: 29500
2026-01-14 02:33:23,459: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:33:28,843: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:33:28,895: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:33:40,817: Val batch 29500: PER (avg): 0.1148 CTC Loss (avg): 27.0459 WER(5gram): 12.97% (n=256) time: 17.485
2026-01-14 02:33:40,817: WER lens: avg_true_words=5.99 avg_pred_words=6.15 max_pred_words=12
2026-01-14 02:33:40,818: t15.2023.08.13 val PER: 0.0904
2026-01-14 02:33:40,818: t15.2023.08.18 val PER: 0.0788
2026-01-14 02:33:40,818: t15.2023.08.20 val PER: 0.0683
2026-01-14 02:33:40,818: t15.2023.08.25 val PER: 0.0678
2026-01-14 02:33:40,818: t15.2023.08.27 val PER: 0.1640
2026-01-14 02:33:40,818: t15.2023.09.01 val PER: 0.0487
2026-01-14 02:33:40,818: t15.2023.09.03 val PER: 0.1295
2026-01-14 02:33:40,818: t15.2023.09.24 val PER: 0.0922
2026-01-14 02:33:40,818: t15.2023.09.29 val PER: 0.1072
2026-01-14 02:33:40,818: t15.2023.10.01 val PER: 0.1519
2026-01-14 02:33:40,818: t15.2023.10.06 val PER: 0.0646
2026-01-14 02:33:40,818: t15.2023.10.08 val PER: 0.2192
2026-01-14 02:33:40,818: t15.2023.10.13 val PER: 0.1699
2026-01-14 02:33:40,819: t15.2023.10.15 val PER: 0.1154
2026-01-14 02:33:40,819: t15.2023.10.20 val PER: 0.1779
2026-01-14 02:33:40,819: t15.2023.10.22 val PER: 0.0913
2026-01-14 02:33:40,819: t15.2023.11.03 val PER: 0.1621
2026-01-14 02:33:40,819: t15.2023.11.04 val PER: 0.0239
2026-01-14 02:33:40,819: t15.2023.11.17 val PER: 0.0280
2026-01-14 02:33:40,819: t15.2023.11.19 val PER: 0.0140
2026-01-14 02:33:40,819: t15.2023.11.26 val PER: 0.0587
2026-01-14 02:33:40,819: t15.2023.12.03 val PER: 0.0620
2026-01-14 02:33:40,819: t15.2023.12.08 val PER: 0.0459
2026-01-14 02:33:40,819: t15.2023.12.10 val PER: 0.0420
2026-01-14 02:33:40,819: t15.2023.12.17 val PER: 0.0988
2026-01-14 02:33:40,820: t15.2023.12.29 val PER: 0.0762
2026-01-14 02:33:40,820: t15.2024.02.25 val PER: 0.0815
2026-01-14 02:33:40,820: t15.2024.03.08 val PER: 0.1664
2026-01-14 02:33:40,820: t15.2024.03.15 val PER: 0.1695
2026-01-14 02:33:40,820: t15.2024.03.17 val PER: 0.1032
2026-01-14 02:33:40,820: t15.2024.05.10 val PER: 0.1278
2026-01-14 02:33:40,820: t15.2024.06.14 val PER: 0.1356
2026-01-14 02:33:40,820: t15.2024.07.19 val PER: 0.1833
2026-01-14 02:33:40,820: t15.2024.07.21 val PER: 0.0724
2026-01-14 02:33:40,820: t15.2024.07.28 val PER: 0.1037
2026-01-14 02:33:40,820: t15.2025.01.10 val PER: 0.2548
2026-01-14 02:33:40,820: t15.2025.01.12 val PER: 0.0862
2026-01-14 02:33:40,820: t15.2025.03.14 val PER: 0.3062
2026-01-14 02:33:40,820: t15.2025.03.16 val PER: 0.1348
2026-01-14 02:33:40,821: t15.2025.03.30 val PER: 0.2241
2026-01-14 02:33:40,821: t15.2025.04.13 val PER: 0.1897
2026-01-14 02:33:40,961: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_29500
2026-01-14 02:33:50,423: Train batch 29600: loss: 2.42 grad norm: 58.25 time: 0.059
2026-01-14 02:34:08,299: Train batch 29800: loss: 2.30 grad norm: 42.34 time: 0.090
2026-01-14 02:34:25,618: Train batch 30000: loss: 2.23 grad norm: 43.48 time: 0.077
2026-01-14 02:34:25,618: Running test after training batch: 30000
2026-01-14 02:34:25,763: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:34:31,197: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:34:31,253: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cent
2026-01-14 02:34:44,279: Val batch 30000: PER (avg): 0.1138 CTC Loss (avg): 26.8821 WER(5gram): 11.99% (n=256) time: 18.661
2026-01-14 02:34:44,280: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-14 02:34:44,280: t15.2023.08.13 val PER: 0.0946
2026-01-14 02:34:44,280: t15.2023.08.18 val PER: 0.0788
2026-01-14 02:34:44,280: t15.2023.08.20 val PER: 0.0612
2026-01-14 02:34:44,281: t15.2023.08.25 val PER: 0.0678
2026-01-14 02:34:44,281: t15.2023.08.27 val PER: 0.1608
2026-01-14 02:34:44,281: t15.2023.09.01 val PER: 0.0463
2026-01-14 02:34:44,281: t15.2023.09.03 val PER: 0.1283
2026-01-14 02:34:44,281: t15.2023.09.24 val PER: 0.0922
2026-01-14 02:34:44,281: t15.2023.09.29 val PER: 0.1123
2026-01-14 02:34:44,281: t15.2023.10.01 val PER: 0.1440
2026-01-14 02:34:44,281: t15.2023.10.06 val PER: 0.0700
2026-01-14 02:34:44,281: t15.2023.10.08 val PER: 0.2124
2026-01-14 02:34:44,281: t15.2023.10.13 val PER: 0.1715
2026-01-14 02:34:44,282: t15.2023.10.15 val PER: 0.1220
2026-01-14 02:34:44,282: t15.2023.10.20 val PER: 0.1577
2026-01-14 02:34:44,282: t15.2023.10.22 val PER: 0.0913
2026-01-14 02:34:44,282: t15.2023.11.03 val PER: 0.1588
2026-01-14 02:34:44,282: t15.2023.11.04 val PER: 0.0171
2026-01-14 02:34:44,282: t15.2023.11.17 val PER: 0.0264
2026-01-14 02:34:44,282: t15.2023.11.19 val PER: 0.0160
2026-01-14 02:34:44,282: t15.2023.11.26 val PER: 0.0587
2026-01-14 02:34:44,282: t15.2023.12.03 val PER: 0.0651
2026-01-14 02:34:44,282: t15.2023.12.08 val PER: 0.0419
2026-01-14 02:34:44,283: t15.2023.12.10 val PER: 0.0355
2026-01-14 02:34:44,283: t15.2023.12.17 val PER: 0.0925
2026-01-14 02:34:44,283: t15.2023.12.29 val PER: 0.0776
2026-01-14 02:34:44,283: t15.2024.02.25 val PER: 0.0815
2026-01-14 02:34:44,283: t15.2024.03.08 val PER: 0.1721
2026-01-14 02:34:44,283: t15.2024.03.15 val PER: 0.1732
2026-01-14 02:34:44,283: t15.2024.03.17 val PER: 0.0893
2026-01-14 02:34:44,283: t15.2024.05.10 val PER: 0.1189
2026-01-14 02:34:44,283: t15.2024.06.14 val PER: 0.1262
2026-01-14 02:34:44,283: t15.2024.07.19 val PER: 0.1859
2026-01-14 02:34:44,284: t15.2024.07.21 val PER: 0.0745
2026-01-14 02:34:44,284: t15.2024.07.28 val PER: 0.1051
2026-01-14 02:34:44,284: t15.2025.01.10 val PER: 0.2479
2026-01-14 02:34:44,284: t15.2025.01.12 val PER: 0.1001
2026-01-14 02:34:44,284: t15.2025.03.14 val PER: 0.2973
2026-01-14 02:34:44,293: t15.2025.03.16 val PER: 0.1348
2026-01-14 02:34:44,293: t15.2025.03.30 val PER: 0.2126
2026-01-14 02:34:44,294: t15.2025.04.13 val PER: 0.1969
2026-01-14 02:34:44,442: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_30000
2026-01-14 02:35:01,960: Train batch 30200: loss: 3.10 grad norm: 49.86 time: 0.081
2026-01-14 02:35:19,368: Train batch 30400: loss: 1.60 grad norm: 43.04 time: 0.070
2026-01-14 02:35:28,181: Running test after training batch: 30500
2026-01-14 02:35:28,276: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:35:33,754: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:35:33,807: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs said
2026-01-14 02:35:46,455: Val batch 30500: PER (avg): 0.1134 CTC Loss (avg): 28.1435 WER(5gram): 12.71% (n=256) time: 18.274
2026-01-14 02:35:46,456: WER lens: avg_true_words=5.99 avg_pred_words=6.15 max_pred_words=12
2026-01-14 02:35:46,456: t15.2023.08.13 val PER: 0.0925
2026-01-14 02:35:46,456: t15.2023.08.18 val PER: 0.0704
2026-01-14 02:35:46,456: t15.2023.08.20 val PER: 0.0763
2026-01-14 02:35:46,456: t15.2023.08.25 val PER: 0.0708
2026-01-14 02:35:46,456: t15.2023.08.27 val PER: 0.1592
2026-01-14 02:35:46,456: t15.2023.09.01 val PER: 0.0471
2026-01-14 02:35:46,456: t15.2023.09.03 val PER: 0.1235
2026-01-14 02:35:46,456: t15.2023.09.24 val PER: 0.0922
2026-01-14 02:35:46,456: t15.2023.09.29 val PER: 0.1136
2026-01-14 02:35:46,457: t15.2023.10.01 val PER: 0.1466
2026-01-14 02:35:46,457: t15.2023.10.06 val PER: 0.0657
2026-01-14 02:35:46,457: t15.2023.10.08 val PER: 0.2097
2026-01-14 02:35:46,457: t15.2023.10.13 val PER: 0.1730
2026-01-14 02:35:46,457: t15.2023.10.15 val PER: 0.1246
2026-01-14 02:35:46,457: t15.2023.10.20 val PER: 0.1678
2026-01-14 02:35:46,457: t15.2023.10.22 val PER: 0.0924
2026-01-14 02:35:46,458: t15.2023.11.03 val PER: 0.1682
2026-01-14 02:35:46,458: t15.2023.11.04 val PER: 0.0239
2026-01-14 02:35:46,458: t15.2023.11.17 val PER: 0.0280
2026-01-14 02:35:46,458: t15.2023.11.19 val PER: 0.0140
2026-01-14 02:35:46,458: t15.2023.11.26 val PER: 0.0514
2026-01-14 02:35:46,458: t15.2023.12.03 val PER: 0.0588
2026-01-14 02:35:46,458: t15.2023.12.08 val PER: 0.0459
2026-01-14 02:35:46,458: t15.2023.12.10 val PER: 0.0355
2026-01-14 02:35:46,458: t15.2023.12.17 val PER: 0.0863
2026-01-14 02:35:46,458: t15.2023.12.29 val PER: 0.0734
2026-01-14 02:35:46,458: t15.2024.02.25 val PER: 0.0772
2026-01-14 02:35:46,458: t15.2024.03.08 val PER: 0.1565
2026-01-14 02:35:46,458: t15.2024.03.15 val PER: 0.1670
2026-01-14 02:35:46,458: t15.2024.03.17 val PER: 0.0893
2026-01-14 02:35:46,458: t15.2024.05.10 val PER: 0.1263
2026-01-14 02:35:46,458: t15.2024.06.14 val PER: 0.1230
2026-01-14 02:35:46,459: t15.2024.07.19 val PER: 0.1800
2026-01-14 02:35:46,459: t15.2024.07.21 val PER: 0.0683
2026-01-14 02:35:46,459: t15.2024.07.28 val PER: 0.0993
2026-01-14 02:35:46,459: t15.2025.01.10 val PER: 0.2507
2026-01-14 02:35:46,459: t15.2025.01.12 val PER: 0.1024
2026-01-14 02:35:46,459: t15.2025.03.14 val PER: 0.2885
2026-01-14 02:35:46,459: t15.2025.03.16 val PER: 0.1374
2026-01-14 02:35:46,459: t15.2025.03.30 val PER: 0.2184
2026-01-14 02:35:46,459: t15.2025.04.13 val PER: 0.2140
2026-01-14 02:35:46,632: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_30500
2026-01-14 02:35:55,790: Train batch 30600: loss: 2.92 grad norm: 53.76 time: 0.076
2026-01-14 02:36:13,301: Train batch 30800: loss: 1.18 grad norm: 35.74 time: 0.070
2026-01-14 02:36:30,580: Train batch 31000: loss: 2.34 grad norm: 62.55 time: 0.075
2026-01-14 02:36:30,580: Running test after training batch: 31000
2026-01-14 02:36:30,742: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:36:36,339: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:36:36,398: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cent
2026-01-14 02:36:48,364: Val batch 31000: PER (avg): 0.1125 CTC Loss (avg): 27.3293 WER(5gram): 13.17% (n=256) time: 17.784
2026-01-14 02:36:48,365: WER lens: avg_true_words=5.99 avg_pred_words=6.15 max_pred_words=12
2026-01-14 02:36:48,365: t15.2023.08.13 val PER: 0.0863
2026-01-14 02:36:48,365: t15.2023.08.18 val PER: 0.0771
2026-01-14 02:36:48,365: t15.2023.08.20 val PER: 0.0675
2026-01-14 02:36:48,365: t15.2023.08.25 val PER: 0.0783
2026-01-14 02:36:48,365: t15.2023.08.27 val PER: 0.1608
2026-01-14 02:36:48,365: t15.2023.09.01 val PER: 0.0479
2026-01-14 02:36:48,365: t15.2023.09.03 val PER: 0.1306
2026-01-14 02:36:48,366: t15.2023.09.24 val PER: 0.0947
2026-01-14 02:36:48,366: t15.2023.09.29 val PER: 0.1066
2026-01-14 02:36:48,366: t15.2023.10.01 val PER: 0.1427
2026-01-14 02:36:48,366: t15.2023.10.06 val PER: 0.0657
2026-01-14 02:36:48,366: t15.2023.10.08 val PER: 0.2260
2026-01-14 02:36:48,366: t15.2023.10.13 val PER: 0.1668
2026-01-14 02:36:48,366: t15.2023.10.15 val PER: 0.1259
2026-01-14 02:36:48,366: t15.2023.10.20 val PER: 0.1644
2026-01-14 02:36:48,366: t15.2023.10.22 val PER: 0.0969
2026-01-14 02:36:48,366: t15.2023.11.03 val PER: 0.1574
2026-01-14 02:36:48,366: t15.2023.11.04 val PER: 0.0102
2026-01-14 02:36:48,366: t15.2023.11.17 val PER: 0.0249
2026-01-14 02:36:48,367: t15.2023.11.19 val PER: 0.0100
2026-01-14 02:36:48,367: t15.2023.11.26 val PER: 0.0471
2026-01-14 02:36:48,367: t15.2023.12.03 val PER: 0.0630
2026-01-14 02:36:48,367: t15.2023.12.08 val PER: 0.0433
2026-01-14 02:36:48,367: t15.2023.12.10 val PER: 0.0355
2026-01-14 02:36:48,367: t15.2023.12.17 val PER: 0.0873
2026-01-14 02:36:48,367: t15.2023.12.29 val PER: 0.0789
2026-01-14 02:36:48,367: t15.2024.02.25 val PER: 0.0772
2026-01-14 02:36:48,367: t15.2024.03.08 val PER: 0.1650
2026-01-14 02:36:48,367: t15.2024.03.15 val PER: 0.1682
2026-01-14 02:36:48,367: t15.2024.03.17 val PER: 0.0969
2026-01-14 02:36:48,367: t15.2024.05.10 val PER: 0.1070
2026-01-14 02:36:48,367: t15.2024.06.14 val PER: 0.1199
2026-01-14 02:36:48,367: t15.2024.07.19 val PER: 0.1800
2026-01-14 02:36:48,367: t15.2024.07.21 val PER: 0.0697
2026-01-14 02:36:48,367: t15.2024.07.28 val PER: 0.1051
2026-01-14 02:36:48,367: t15.2025.01.10 val PER: 0.2438
2026-01-14 02:36:48,368: t15.2025.01.12 val PER: 0.1039
2026-01-14 02:36:48,368: t15.2025.03.14 val PER: 0.2988
2026-01-14 02:36:48,368: t15.2025.03.16 val PER: 0.1387
2026-01-14 02:36:48,368: t15.2025.03.30 val PER: 0.2080
2026-01-14 02:36:48,368: t15.2025.04.13 val PER: 0.1812
2026-01-14 02:36:48,511: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_31000
2026-01-14 02:37:05,988: Train batch 31200: loss: 1.32 grad norm: 26.79 time: 0.078
2026-01-14 02:37:23,549: Train batch 31400: loss: 2.08 grad norm: 41.09 time: 0.066
2026-01-14 02:37:32,301: Running test after training batch: 31500
2026-01-14 02:37:32,560: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:37:38,145: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:37:38,193: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:37:50,036: Val batch 31500: PER (avg): 0.1124 CTC Loss (avg): 28.2457 WER(5gram): 11.47% (n=256) time: 17.735
2026-01-14 02:37:50,037: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-14 02:37:50,037: t15.2023.08.13 val PER: 0.0842
2026-01-14 02:37:50,037: t15.2023.08.18 val PER: 0.0746
2026-01-14 02:37:50,037: t15.2023.08.20 val PER: 0.0691
2026-01-14 02:37:50,038: t15.2023.08.25 val PER: 0.0708
2026-01-14 02:37:50,038: t15.2023.08.27 val PER: 0.1511
2026-01-14 02:37:50,038: t15.2023.09.01 val PER: 0.0495
2026-01-14 02:37:50,038: t15.2023.09.03 val PER: 0.1295
2026-01-14 02:37:50,038: t15.2023.09.24 val PER: 0.1032
2026-01-14 02:37:50,038: t15.2023.09.29 val PER: 0.1047
2026-01-14 02:37:50,038: t15.2023.10.01 val PER: 0.1493
2026-01-14 02:37:50,038: t15.2023.10.06 val PER: 0.0635
2026-01-14 02:37:50,038: t15.2023.10.08 val PER: 0.2111
2026-01-14 02:37:50,038: t15.2023.10.13 val PER: 0.1753
2026-01-14 02:37:50,038: t15.2023.10.15 val PER: 0.1246
2026-01-14 02:37:50,038: t15.2023.10.20 val PER: 0.1745
2026-01-14 02:37:50,038: t15.2023.10.22 val PER: 0.0991
2026-01-14 02:37:50,038: t15.2023.11.03 val PER: 0.1608
2026-01-14 02:37:50,038: t15.2023.11.04 val PER: 0.0171
2026-01-14 02:37:50,038: t15.2023.11.17 val PER: 0.0295
2026-01-14 02:37:50,038: t15.2023.11.19 val PER: 0.0060
2026-01-14 02:37:50,039: t15.2023.11.26 val PER: 0.0587
2026-01-14 02:37:50,039: t15.2023.12.03 val PER: 0.0578
2026-01-14 02:37:50,039: t15.2023.12.08 val PER: 0.0433
2026-01-14 02:37:50,039: t15.2023.12.10 val PER: 0.0420
2026-01-14 02:37:50,039: t15.2023.12.17 val PER: 0.0904
2026-01-14 02:37:50,039: t15.2023.12.29 val PER: 0.0721
2026-01-14 02:37:50,039: t15.2024.02.25 val PER: 0.0646
2026-01-14 02:37:50,039: t15.2024.03.08 val PER: 0.1693
2026-01-14 02:37:50,039: t15.2024.03.15 val PER: 0.1632
2026-01-14 02:37:50,039: t15.2024.03.17 val PER: 0.0900
2026-01-14 02:37:50,039: t15.2024.05.10 val PER: 0.1174
2026-01-14 02:37:50,039: t15.2024.06.14 val PER: 0.1215
2026-01-14 02:37:50,040: t15.2024.07.19 val PER: 0.1859
2026-01-14 02:37:50,040: t15.2024.07.21 val PER: 0.0662
2026-01-14 02:37:50,040: t15.2024.07.28 val PER: 0.0890
2026-01-14 02:37:50,040: t15.2025.01.10 val PER: 0.2424
2026-01-14 02:37:50,040: t15.2025.01.12 val PER: 0.1032
2026-01-14 02:37:50,040: t15.2025.03.14 val PER: 0.3018
2026-01-14 02:37:50,040: t15.2025.03.16 val PER: 0.1348
2026-01-14 02:37:50,040: t15.2025.03.30 val PER: 0.2195
2026-01-14 02:37:50,040: t15.2025.04.13 val PER: 0.1897
2026-01-14 02:37:50,189: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_31500
2026-01-14 02:37:58,944: Train batch 31600: loss: 1.45 grad norm: 41.51 time: 0.080
2026-01-14 02:38:16,123: Train batch 31800: loss: 2.37 grad norm: 46.88 time: 0.062
2026-01-14 02:38:33,723: Train batch 32000: loss: 2.15 grad norm: 35.26 time: 0.083
2026-01-14 02:38:33,723: Running test after training batch: 32000
2026-01-14 02:38:33,824: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:38:39,563: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:38:39,614: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost said
2026-01-14 02:38:51,670: Val batch 32000: PER (avg): 0.1120 CTC Loss (avg): 27.5121 WER(5gram): 13.75% (n=256) time: 17.946
2026-01-14 02:38:51,670: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-14 02:38:51,670: t15.2023.08.13 val PER: 0.0852
2026-01-14 02:38:51,670: t15.2023.08.18 val PER: 0.0721
2026-01-14 02:38:51,670: t15.2023.08.20 val PER: 0.0731
2026-01-14 02:38:51,671: t15.2023.08.25 val PER: 0.0693
2026-01-14 02:38:51,671: t15.2023.08.27 val PER: 0.1608
2026-01-14 02:38:51,671: t15.2023.09.01 val PER: 0.0511
2026-01-14 02:38:51,671: t15.2023.09.03 val PER: 0.1223
2026-01-14 02:38:51,671: t15.2023.09.24 val PER: 0.0898
2026-01-14 02:38:51,671: t15.2023.09.29 val PER: 0.1117
2026-01-14 02:38:51,671: t15.2023.10.01 val PER: 0.1493
2026-01-14 02:38:51,671: t15.2023.10.06 val PER: 0.0603
2026-01-14 02:38:51,671: t15.2023.10.08 val PER: 0.2138
2026-01-14 02:38:51,671: t15.2023.10.13 val PER: 0.1645
2026-01-14 02:38:51,672: t15.2023.10.15 val PER: 0.1094
2026-01-14 02:38:51,672: t15.2023.10.20 val PER: 0.1678
2026-01-14 02:38:51,672: t15.2023.10.22 val PER: 0.0958
2026-01-14 02:38:51,672: t15.2023.11.03 val PER: 0.1642
2026-01-14 02:38:51,672: t15.2023.11.04 val PER: 0.0205
2026-01-14 02:38:51,672: t15.2023.11.17 val PER: 0.0264
2026-01-14 02:38:51,672: t15.2023.11.19 val PER: 0.0200
2026-01-14 02:38:51,672: t15.2023.11.26 val PER: 0.0558
2026-01-14 02:38:51,672: t15.2023.12.03 val PER: 0.0641
2026-01-14 02:38:51,672: t15.2023.12.08 val PER: 0.0393
2026-01-14 02:38:51,672: t15.2023.12.10 val PER: 0.0368
2026-01-14 02:38:51,672: t15.2023.12.17 val PER: 0.0925
2026-01-14 02:38:51,673: t15.2023.12.29 val PER: 0.0714
2026-01-14 02:38:51,673: t15.2024.02.25 val PER: 0.0801
2026-01-14 02:38:51,673: t15.2024.03.08 val PER: 0.1778
2026-01-14 02:38:51,673: t15.2024.03.15 val PER: 0.1682
2026-01-14 02:38:51,673: t15.2024.03.17 val PER: 0.0907
2026-01-14 02:38:51,673: t15.2024.05.10 val PER: 0.1114
2026-01-14 02:38:51,673: t15.2024.06.14 val PER: 0.1136
2026-01-14 02:38:51,673: t15.2024.07.19 val PER: 0.1767
2026-01-14 02:38:51,673: t15.2024.07.21 val PER: 0.0607
2026-01-14 02:38:51,673: t15.2024.07.28 val PER: 0.0971
2026-01-14 02:38:51,673: t15.2025.01.10 val PER: 0.2562
2026-01-14 02:38:51,673: t15.2025.01.12 val PER: 0.0970
2026-01-14 02:38:51,673: t15.2025.03.14 val PER: 0.2929
2026-01-14 02:38:51,673: t15.2025.03.16 val PER: 0.1309
2026-01-14 02:38:51,673: t15.2025.03.30 val PER: 0.2310
2026-01-14 02:38:51,673: t15.2025.04.13 val PER: 0.1983
2026-01-14 02:38:51,840: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_32000
2026-01-14 02:39:09,751: Train batch 32200: loss: 1.24 grad norm: 29.37 time: 0.096
2026-01-14 02:39:28,317: Train batch 32400: loss: 7.33 grad norm: 42.47 time: 0.078
2026-01-14 02:39:37,453: Running test after training batch: 32500
2026-01-14 02:39:37,579: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:39:43,047: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:39:43,100: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-14 02:39:54,939: Val batch 32500: PER (avg): 0.1127 CTC Loss (avg): 27.9779 WER(5gram): 12.58% (n=256) time: 17.485
2026-01-14 02:39:54,940: WER lens: avg_true_words=5.99 avg_pred_words=6.15 max_pred_words=12
2026-01-14 02:39:54,940: t15.2023.08.13 val PER: 0.0873
2026-01-14 02:39:54,940: t15.2023.08.18 val PER: 0.0704
2026-01-14 02:39:54,940: t15.2023.08.20 val PER: 0.0675
2026-01-14 02:39:54,940: t15.2023.08.25 val PER: 0.0738
2026-01-14 02:39:54,940: t15.2023.08.27 val PER: 0.1559
2026-01-14 02:39:54,940: t15.2023.09.01 val PER: 0.0519
2026-01-14 02:39:54,940: t15.2023.09.03 val PER: 0.1259
2026-01-14 02:39:54,940: t15.2023.09.24 val PER: 0.1019
2026-01-14 02:39:54,941: t15.2023.09.29 val PER: 0.1123
2026-01-14 02:39:54,941: t15.2023.10.01 val PER: 0.1513
2026-01-14 02:39:54,941: t15.2023.10.06 val PER: 0.0592
2026-01-14 02:39:54,941: t15.2023.10.08 val PER: 0.2206
2026-01-14 02:39:54,941: t15.2023.10.13 val PER: 0.1722
2026-01-14 02:39:54,941: t15.2023.10.15 val PER: 0.1193
2026-01-14 02:39:54,941: t15.2023.10.20 val PER: 0.1443
2026-01-14 02:39:54,942: t15.2023.10.22 val PER: 0.0924
2026-01-14 02:39:54,942: t15.2023.11.03 val PER: 0.1642
2026-01-14 02:39:54,942: t15.2023.11.04 val PER: 0.0171
2026-01-14 02:39:54,942: t15.2023.11.17 val PER: 0.0264
2026-01-14 02:39:54,942: t15.2023.11.19 val PER: 0.0140
2026-01-14 02:39:54,942: t15.2023.11.26 val PER: 0.0464
2026-01-14 02:39:54,942: t15.2023.12.03 val PER: 0.0588
2026-01-14 02:39:54,942: t15.2023.12.08 val PER: 0.0486
2026-01-14 02:39:54,942: t15.2023.12.10 val PER: 0.0355
2026-01-14 02:39:54,942: t15.2023.12.17 val PER: 0.0884
2026-01-14 02:39:54,942: t15.2023.12.29 val PER: 0.0789
2026-01-14 02:39:54,942: t15.2024.02.25 val PER: 0.0744
2026-01-14 02:39:54,942: t15.2024.03.08 val PER: 0.1707
2026-01-14 02:39:54,942: t15.2024.03.15 val PER: 0.1645
2026-01-14 02:39:54,942: t15.2024.03.17 val PER: 0.0914
2026-01-14 02:39:54,942: t15.2024.05.10 val PER: 0.1233
2026-01-14 02:39:54,942: t15.2024.06.14 val PER: 0.1230
2026-01-14 02:39:54,943: t15.2024.07.19 val PER: 0.1866
2026-01-14 02:39:54,943: t15.2024.07.21 val PER: 0.0683
2026-01-14 02:39:54,943: t15.2024.07.28 val PER: 0.0882
2026-01-14 02:39:54,943: t15.2025.01.10 val PER: 0.2534
2026-01-14 02:39:54,943: t15.2025.01.12 val PER: 0.1008
2026-01-14 02:39:54,943: t15.2025.03.14 val PER: 0.3003
2026-01-14 02:39:54,943: t15.2025.03.16 val PER: 0.1230
2026-01-14 02:39:54,943: t15.2025.03.30 val PER: 0.2184
2026-01-14 02:39:54,943: t15.2025.04.13 val PER: 0.1969
2026-01-14 02:39:55,105: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_32500
2026-01-14 02:40:04,282: Train batch 32600: loss: 1.97 grad norm: 36.92 time: 0.102
2026-01-14 02:40:22,256: Train batch 32800: loss: 1.79 grad norm: 34.52 time: 0.071
2026-01-14 02:40:39,645: Train batch 33000: loss: 1.18 grad norm: 36.37 time: 0.076
2026-01-14 02:40:39,645: Running test after training batch: 33000
2026-01-14 02:40:39,840: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:40:45,252: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:40:45,314: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:40:57,099: Val batch 33000: PER (avg): 0.1130 CTC Loss (avg): 28.0611 WER(5gram): 12.13% (n=256) time: 17.454
2026-01-14 02:40:57,099: WER lens: avg_true_words=5.99 avg_pred_words=6.15 max_pred_words=12
2026-01-14 02:40:57,100: t15.2023.08.13 val PER: 0.0873
2026-01-14 02:40:57,100: t15.2023.08.18 val PER: 0.0729
2026-01-14 02:40:57,100: t15.2023.08.20 val PER: 0.0715
2026-01-14 02:40:57,100: t15.2023.08.25 val PER: 0.0633
2026-01-14 02:40:57,100: t15.2023.08.27 val PER: 0.1527
2026-01-14 02:40:57,100: t15.2023.09.01 val PER: 0.0479
2026-01-14 02:40:57,100: t15.2023.09.03 val PER: 0.1176
2026-01-14 02:40:57,100: t15.2023.09.24 val PER: 0.0983
2026-01-14 02:40:57,100: t15.2023.09.29 val PER: 0.1161
2026-01-14 02:40:57,101: t15.2023.10.01 val PER: 0.1466
2026-01-14 02:40:57,101: t15.2023.10.06 val PER: 0.0560
2026-01-14 02:40:57,101: t15.2023.10.08 val PER: 0.2124
2026-01-14 02:40:57,101: t15.2023.10.13 val PER: 0.1738
2026-01-14 02:40:57,101: t15.2023.10.15 val PER: 0.1246
2026-01-14 02:40:57,101: t15.2023.10.20 val PER: 0.1745
2026-01-14 02:40:57,101: t15.2023.10.22 val PER: 0.0924
2026-01-14 02:40:57,101: t15.2023.11.03 val PER: 0.1581
2026-01-14 02:40:57,101: t15.2023.11.04 val PER: 0.0137
2026-01-14 02:40:57,101: t15.2023.11.17 val PER: 0.0280
2026-01-14 02:40:57,101: t15.2023.11.19 val PER: 0.0120
2026-01-14 02:40:57,102: t15.2023.11.26 val PER: 0.0529
2026-01-14 02:40:57,102: t15.2023.12.03 val PER: 0.0672
2026-01-14 02:40:57,102: t15.2023.12.08 val PER: 0.0433
2026-01-14 02:40:57,102: t15.2023.12.10 val PER: 0.0342
2026-01-14 02:40:57,102: t15.2023.12.17 val PER: 0.0884
2026-01-14 02:40:57,102: t15.2023.12.29 val PER: 0.0762
2026-01-14 02:40:57,102: t15.2024.02.25 val PER: 0.0772
2026-01-14 02:40:57,102: t15.2024.03.08 val PER: 0.1750
2026-01-14 02:40:57,102: t15.2024.03.15 val PER: 0.1676
2026-01-14 02:40:57,103: t15.2024.03.17 val PER: 0.0934
2026-01-14 02:40:57,103: t15.2024.05.10 val PER: 0.1263
2026-01-14 02:40:57,103: t15.2024.06.14 val PER: 0.1057
2026-01-14 02:40:57,103: t15.2024.07.19 val PER: 0.1872
2026-01-14 02:40:57,103: t15.2024.07.21 val PER: 0.0676
2026-01-14 02:40:57,103: t15.2024.07.28 val PER: 0.0853
2026-01-14 02:40:57,103: t15.2025.01.10 val PER: 0.2521
2026-01-14 02:40:57,103: t15.2025.01.12 val PER: 0.1093
2026-01-14 02:40:57,103: t15.2025.03.14 val PER: 0.3047
2026-01-14 02:40:57,103: t15.2025.03.16 val PER: 0.1309
2026-01-14 02:40:57,103: t15.2025.03.30 val PER: 0.2276
2026-01-14 02:40:57,103: t15.2025.04.13 val PER: 0.1912
2026-01-14 02:40:57,267: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_33000
2026-01-14 02:41:15,170: Train batch 33200: loss: 1.88 grad norm: 37.01 time: 0.075
2026-01-14 02:41:32,931: Train batch 33400: loss: 0.31 grad norm: 13.08 time: 0.063
2026-01-14 02:41:41,862: Running test after training batch: 33500
2026-01-14 02:41:42,040: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:41:47,457: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:41:47,512: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:41:59,445: Val batch 33500: PER (avg): 0.1112 CTC Loss (avg): 28.3585 WER(5gram): 12.45% (n=256) time: 17.582
2026-01-14 02:41:59,445: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-14 02:41:59,445: t15.2023.08.13 val PER: 0.0811
2026-01-14 02:41:59,446: t15.2023.08.18 val PER: 0.0746
2026-01-14 02:41:59,446: t15.2023.08.20 val PER: 0.0747
2026-01-14 02:41:59,446: t15.2023.08.25 val PER: 0.0693
2026-01-14 02:41:59,446: t15.2023.08.27 val PER: 0.1608
2026-01-14 02:41:59,446: t15.2023.09.01 val PER: 0.0487
2026-01-14 02:41:59,446: t15.2023.09.03 val PER: 0.1188
2026-01-14 02:41:59,446: t15.2023.09.24 val PER: 0.0959
2026-01-14 02:41:59,446: t15.2023.09.29 val PER: 0.1053
2026-01-14 02:41:59,446: t15.2023.10.01 val PER: 0.1532
2026-01-14 02:41:59,446: t15.2023.10.06 val PER: 0.0657
2026-01-14 02:41:59,446: t15.2023.10.08 val PER: 0.2165
2026-01-14 02:41:59,446: t15.2023.10.13 val PER: 0.1590
2026-01-14 02:41:59,446: t15.2023.10.15 val PER: 0.1127
2026-01-14 02:41:59,447: t15.2023.10.20 val PER: 0.1443
2026-01-14 02:41:59,447: t15.2023.10.22 val PER: 0.0980
2026-01-14 02:41:59,447: t15.2023.11.03 val PER: 0.1642
2026-01-14 02:41:59,447: t15.2023.11.04 val PER: 0.0137
2026-01-14 02:41:59,447: t15.2023.11.17 val PER: 0.0233
2026-01-14 02:41:59,447: t15.2023.11.19 val PER: 0.0120
2026-01-14 02:41:59,447: t15.2023.11.26 val PER: 0.0493
2026-01-14 02:41:59,447: t15.2023.12.03 val PER: 0.0672
2026-01-14 02:41:59,447: t15.2023.12.08 val PER: 0.0413
2026-01-14 02:41:59,447: t15.2023.12.10 val PER: 0.0394
2026-01-14 02:41:59,447: t15.2023.12.17 val PER: 0.0842
2026-01-14 02:41:59,447: t15.2023.12.29 val PER: 0.0762
2026-01-14 02:41:59,447: t15.2024.02.25 val PER: 0.0787
2026-01-14 02:41:59,448: t15.2024.03.08 val PER: 0.1721
2026-01-14 02:41:59,448: t15.2024.03.15 val PER: 0.1726
2026-01-14 02:41:59,448: t15.2024.03.17 val PER: 0.0914
2026-01-14 02:41:59,448: t15.2024.05.10 val PER: 0.1129
2026-01-14 02:41:59,448: t15.2024.06.14 val PER: 0.1151
2026-01-14 02:41:59,448: t15.2024.07.19 val PER: 0.1786
2026-01-14 02:41:59,448: t15.2024.07.21 val PER: 0.0683
2026-01-14 02:41:59,448: t15.2024.07.28 val PER: 0.0956
2026-01-14 02:41:59,448: t15.2025.01.10 val PER: 0.2548
2026-01-14 02:41:59,448: t15.2025.01.12 val PER: 0.0878
2026-01-14 02:41:59,448: t15.2025.03.14 val PER: 0.2885
2026-01-14 02:41:59,448: t15.2025.03.16 val PER: 0.1243
2026-01-14 02:41:59,448: t15.2025.03.30 val PER: 0.2172
2026-01-14 02:41:59,448: t15.2025.04.13 val PER: 0.1940
2026-01-14 02:41:59,612: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_33500
2026-01-14 02:42:08,898: Train batch 33600: loss: 1.63 grad norm: 32.32 time: 0.066
2026-01-14 02:42:27,425: Train batch 33800: loss: 0.97 grad norm: 37.32 time: 0.079
2026-01-14 02:42:45,479: Train batch 34000: loss: 2.49 grad norm: 53.02 time: 0.081
2026-01-14 02:42:45,480: Running test after training batch: 34000
2026-01-14 02:42:45,590: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:42:51,226: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:42:51,280: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost sen
2026-01-14 02:43:03,141: Val batch 34000: PER (avg): 0.1104 CTC Loss (avg): 27.9660 WER(5gram): 11.93% (n=256) time: 17.661
2026-01-14 02:43:03,142: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-14 02:43:03,142: t15.2023.08.13 val PER: 0.0884
2026-01-14 02:43:03,142: t15.2023.08.18 val PER: 0.0712
2026-01-14 02:43:03,142: t15.2023.08.20 val PER: 0.0715
2026-01-14 02:43:03,142: t15.2023.08.25 val PER: 0.0693
2026-01-14 02:43:03,142: t15.2023.08.27 val PER: 0.1543
2026-01-14 02:43:03,142: t15.2023.09.01 val PER: 0.0495
2026-01-14 02:43:03,142: t15.2023.09.03 val PER: 0.1188
2026-01-14 02:43:03,142: t15.2023.09.24 val PER: 0.0910
2026-01-14 02:43:03,142: t15.2023.09.29 val PER: 0.1110
2026-01-14 02:43:03,142: t15.2023.10.01 val PER: 0.1446
2026-01-14 02:43:03,143: t15.2023.10.06 val PER: 0.0538
2026-01-14 02:43:03,143: t15.2023.10.08 val PER: 0.2057
2026-01-14 02:43:03,143: t15.2023.10.13 val PER: 0.1722
2026-01-14 02:43:03,143: t15.2023.10.15 val PER: 0.1147
2026-01-14 02:43:03,143: t15.2023.10.20 val PER: 0.1846
2026-01-14 02:43:03,143: t15.2023.10.22 val PER: 0.0902
2026-01-14 02:43:03,143: t15.2023.11.03 val PER: 0.1655
2026-01-14 02:43:03,143: t15.2023.11.04 val PER: 0.0239
2026-01-14 02:43:03,143: t15.2023.11.17 val PER: 0.0218
2026-01-14 02:43:03,143: t15.2023.11.19 val PER: 0.0160
2026-01-14 02:43:03,143: t15.2023.11.26 val PER: 0.0522
2026-01-14 02:43:03,143: t15.2023.12.03 val PER: 0.0609
2026-01-14 02:43:03,143: t15.2023.12.08 val PER: 0.0386
2026-01-14 02:43:03,144: t15.2023.12.10 val PER: 0.0381
2026-01-14 02:43:03,144: t15.2023.12.17 val PER: 0.0769
2026-01-14 02:43:03,144: t15.2023.12.29 val PER: 0.0728
2026-01-14 02:43:03,144: t15.2024.02.25 val PER: 0.0772
2026-01-14 02:43:03,144: t15.2024.03.08 val PER: 0.1650
2026-01-14 02:43:03,144: t15.2024.03.15 val PER: 0.1676
2026-01-14 02:43:03,144: t15.2024.03.17 val PER: 0.0927
2026-01-14 02:43:03,144: t15.2024.05.10 val PER: 0.1278
2026-01-14 02:43:03,144: t15.2024.06.14 val PER: 0.1120
2026-01-14 02:43:03,144: t15.2024.07.19 val PER: 0.1879
2026-01-14 02:43:03,145: t15.2024.07.21 val PER: 0.0690
2026-01-14 02:43:03,145: t15.2024.07.28 val PER: 0.0853
2026-01-14 02:43:03,145: t15.2025.01.10 val PER: 0.2507
2026-01-14 02:43:03,145: t15.2025.01.12 val PER: 0.0908
2026-01-14 02:43:03,145: t15.2025.03.14 val PER: 0.2870
2026-01-14 02:43:03,145: t15.2025.03.16 val PER: 0.1283
2026-01-14 02:43:03,145: t15.2025.03.30 val PER: 0.2138
2026-01-14 02:43:03,145: t15.2025.04.13 val PER: 0.1826
2026-01-14 02:43:03,306: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_34000
2026-01-14 02:43:21,509: Train batch 34200: loss: 1.15 grad norm: 31.57 time: 0.053
2026-01-14 02:43:40,020: Train batch 34400: loss: 1.35 grad norm: 32.28 time: 0.068
2026-01-14 02:43:49,269: Running test after training batch: 34500
2026-01-14 02:43:49,384: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:43:54,932: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:43:54,993: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:44:07,072: Val batch 34500: PER (avg): 0.1099 CTC Loss (avg): 28.3451 WER(5gram): 14.28% (n=256) time: 17.803
2026-01-14 02:44:07,073: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-14 02:44:07,073: t15.2023.08.13 val PER: 0.0759
2026-01-14 02:44:07,073: t15.2023.08.18 val PER: 0.0696
2026-01-14 02:44:07,073: t15.2023.08.20 val PER: 0.0651
2026-01-14 02:44:07,074: t15.2023.08.25 val PER: 0.0693
2026-01-14 02:44:07,074: t15.2023.08.27 val PER: 0.1656
2026-01-14 02:44:07,074: t15.2023.09.01 val PER: 0.0471
2026-01-14 02:44:07,074: t15.2023.09.03 val PER: 0.1247
2026-01-14 02:44:07,074: t15.2023.09.24 val PER: 0.0886
2026-01-14 02:44:07,074: t15.2023.09.29 val PER: 0.1078
2026-01-14 02:44:07,074: t15.2023.10.01 val PER: 0.1387
2026-01-14 02:44:07,074: t15.2023.10.06 val PER: 0.0571
2026-01-14 02:44:07,074: t15.2023.10.08 val PER: 0.2043
2026-01-14 02:44:07,074: t15.2023.10.13 val PER: 0.1683
2026-01-14 02:44:07,074: t15.2023.10.15 val PER: 0.1167
2026-01-14 02:44:07,074: t15.2023.10.20 val PER: 0.1544
2026-01-14 02:44:07,074: t15.2023.10.22 val PER: 0.0813
2026-01-14 02:44:07,074: t15.2023.11.03 val PER: 0.1520
2026-01-14 02:44:07,074: t15.2023.11.04 val PER: 0.0102
2026-01-14 02:44:07,074: t15.2023.11.17 val PER: 0.0233
2026-01-14 02:44:07,075: t15.2023.11.19 val PER: 0.0140
2026-01-14 02:44:07,075: t15.2023.11.26 val PER: 0.0522
2026-01-14 02:44:07,075: t15.2023.12.03 val PER: 0.0609
2026-01-14 02:44:07,075: t15.2023.12.08 val PER: 0.0413
2026-01-14 02:44:07,075: t15.2023.12.10 val PER: 0.0355
2026-01-14 02:44:07,075: t15.2023.12.17 val PER: 0.0842
2026-01-14 02:44:07,075: t15.2023.12.29 val PER: 0.0659
2026-01-14 02:44:07,075: t15.2024.02.25 val PER: 0.0871
2026-01-14 02:44:07,075: t15.2024.03.08 val PER: 0.1664
2026-01-14 02:44:07,075: t15.2024.03.15 val PER: 0.1714
2026-01-14 02:44:07,075: t15.2024.03.17 val PER: 0.0872
2026-01-14 02:44:07,075: t15.2024.05.10 val PER: 0.1159
2026-01-14 02:44:07,075: t15.2024.06.14 val PER: 0.1183
2026-01-14 02:44:07,075: t15.2024.07.19 val PER: 0.1905
2026-01-14 02:44:07,075: t15.2024.07.21 val PER: 0.0697
2026-01-14 02:44:07,076: t15.2024.07.28 val PER: 0.0941
2026-01-14 02:44:07,076: t15.2025.01.10 val PER: 0.2590
2026-01-14 02:44:07,076: t15.2025.01.12 val PER: 0.0931
2026-01-14 02:44:07,076: t15.2025.03.14 val PER: 0.3180
2026-01-14 02:44:07,076: t15.2025.03.16 val PER: 0.1361
2026-01-14 02:44:07,076: t15.2025.03.30 val PER: 0.2149
2026-01-14 02:44:07,076: t15.2025.04.13 val PER: 0.1755
2026-01-14 02:44:07,241: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_34500
2026-01-14 02:44:16,299: Train batch 34600: loss: 1.28 grad norm: 29.39 time: 0.071
2026-01-14 02:44:33,986: Train batch 34800: loss: 0.92 grad norm: 23.77 time: 0.065
2026-01-14 02:44:52,145: Train batch 35000: loss: 1.96 grad norm: 42.58 time: 0.066
2026-01-14 02:44:52,145: Running test after training batch: 35000
2026-01-14 02:44:52,284: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:44:57,671: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:44:57,724: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs down
2026-01-14 02:45:09,718: Val batch 35000: PER (avg): 0.1103 CTC Loss (avg): 28.6787 WER(5gram): 11.34% (n=256) time: 17.572
2026-01-14 02:45:09,718: WER lens: avg_true_words=5.99 avg_pred_words=6.07 max_pred_words=12
2026-01-14 02:45:09,718: t15.2023.08.13 val PER: 0.0863
2026-01-14 02:45:09,719: t15.2023.08.18 val PER: 0.0679
2026-01-14 02:45:09,719: t15.2023.08.20 val PER: 0.0612
2026-01-14 02:45:09,719: t15.2023.08.25 val PER: 0.0738
2026-01-14 02:45:09,719: t15.2023.08.27 val PER: 0.1672
2026-01-14 02:45:09,719: t15.2023.09.01 val PER: 0.0446
2026-01-14 02:45:09,719: t15.2023.09.03 val PER: 0.1188
2026-01-14 02:45:09,719: t15.2023.09.24 val PER: 0.0850
2026-01-14 02:45:09,719: t15.2023.09.29 val PER: 0.1066
2026-01-14 02:45:09,719: t15.2023.10.01 val PER: 0.1499
2026-01-14 02:45:09,719: t15.2023.10.06 val PER: 0.0624
2026-01-14 02:45:09,719: t15.2023.10.08 val PER: 0.2070
2026-01-14 02:45:09,719: t15.2023.10.13 val PER: 0.1645
2026-01-14 02:45:09,720: t15.2023.10.15 val PER: 0.1154
2026-01-14 02:45:09,720: t15.2023.10.20 val PER: 0.1745
2026-01-14 02:45:09,720: t15.2023.10.22 val PER: 0.0891
2026-01-14 02:45:09,720: t15.2023.11.03 val PER: 0.1567
2026-01-14 02:45:09,720: t15.2023.11.04 val PER: 0.0205
2026-01-14 02:45:09,720: t15.2023.11.17 val PER: 0.0249
2026-01-14 02:45:09,720: t15.2023.11.19 val PER: 0.0100
2026-01-14 02:45:09,720: t15.2023.11.26 val PER: 0.0471
2026-01-14 02:45:09,720: t15.2023.12.03 val PER: 0.0557
2026-01-14 02:45:09,720: t15.2023.12.08 val PER: 0.0373
2026-01-14 02:45:09,720: t15.2023.12.10 val PER: 0.0381
2026-01-14 02:45:09,720: t15.2023.12.17 val PER: 0.0946
2026-01-14 02:45:09,720: t15.2023.12.29 val PER: 0.0645
2026-01-14 02:45:09,720: t15.2024.02.25 val PER: 0.0702
2026-01-14 02:45:09,720: t15.2024.03.08 val PER: 0.1679
2026-01-14 02:45:09,721: t15.2024.03.15 val PER: 0.1664
2026-01-14 02:45:09,721: t15.2024.03.17 val PER: 0.0907
2026-01-14 02:45:09,721: t15.2024.05.10 val PER: 0.1218
2026-01-14 02:45:09,721: t15.2024.06.14 val PER: 0.1262
2026-01-14 02:45:09,721: t15.2024.07.19 val PER: 0.1839
2026-01-14 02:45:09,721: t15.2024.07.21 val PER: 0.0669
2026-01-14 02:45:09,721: t15.2024.07.28 val PER: 0.0926
2026-01-14 02:45:09,721: t15.2025.01.10 val PER: 0.2548
2026-01-14 02:45:09,721: t15.2025.01.12 val PER: 0.0939
2026-01-14 02:45:09,721: t15.2025.03.14 val PER: 0.3195
2026-01-14 02:45:09,721: t15.2025.03.16 val PER: 0.1230
2026-01-14 02:45:09,721: t15.2025.03.30 val PER: 0.2322
2026-01-14 02:45:09,722: t15.2025.04.13 val PER: 0.1840
2026-01-14 02:45:09,910: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_35000
2026-01-14 02:45:27,739: Train batch 35200: loss: 1.47 grad norm: 34.50 time: 0.076
2026-01-14 02:45:45,231: Train batch 35400: loss: 1.35 grad norm: 35.20 time: 0.094
2026-01-14 02:45:54,026: Running test after training batch: 35500
2026-01-14 02:45:54,127: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:45:59,625: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:45:59,678: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:46:12,509: Val batch 35500: PER (avg): 0.1084 CTC Loss (avg): 28.2278 WER(5gram): 13.49% (n=256) time: 18.483
2026-01-14 02:46:12,510: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-14 02:46:12,510: t15.2023.08.13 val PER: 0.0717
2026-01-14 02:46:12,510: t15.2023.08.18 val PER: 0.0645
2026-01-14 02:46:12,510: t15.2023.08.20 val PER: 0.0596
2026-01-14 02:46:12,510: t15.2023.08.25 val PER: 0.0633
2026-01-14 02:46:12,510: t15.2023.08.27 val PER: 0.1688
2026-01-14 02:46:12,511: t15.2023.09.01 val PER: 0.0455
2026-01-14 02:46:12,511: t15.2023.09.03 val PER: 0.1211
2026-01-14 02:46:12,511: t15.2023.09.24 val PER: 0.0947
2026-01-14 02:46:12,511: t15.2023.09.29 val PER: 0.1085
2026-01-14 02:46:12,511: t15.2023.10.01 val PER: 0.1460
2026-01-14 02:46:12,511: t15.2023.10.06 val PER: 0.0603
2026-01-14 02:46:12,511: t15.2023.10.08 val PER: 0.2260
2026-01-14 02:46:12,511: t15.2023.10.13 val PER: 0.1614
2026-01-14 02:46:12,511: t15.2023.10.15 val PER: 0.1114
2026-01-14 02:46:12,511: t15.2023.10.20 val PER: 0.1611
2026-01-14 02:46:12,511: t15.2023.10.22 val PER: 0.0913
2026-01-14 02:46:12,511: t15.2023.11.03 val PER: 0.1526
2026-01-14 02:46:12,511: t15.2023.11.04 val PER: 0.0102
2026-01-14 02:46:12,511: t15.2023.11.17 val PER: 0.0233
2026-01-14 02:46:12,511: t15.2023.11.19 val PER: 0.0180
2026-01-14 02:46:12,512: t15.2023.11.26 val PER: 0.0435
2026-01-14 02:46:12,512: t15.2023.12.03 val PER: 0.0536
2026-01-14 02:46:12,512: t15.2023.12.08 val PER: 0.0286
2026-01-14 02:46:12,512: t15.2023.12.10 val PER: 0.0315
2026-01-14 02:46:12,512: t15.2023.12.17 val PER: 0.0821
2026-01-14 02:46:12,512: t15.2023.12.29 val PER: 0.0734
2026-01-14 02:46:12,512: t15.2024.02.25 val PER: 0.0688
2026-01-14 02:46:12,512: t15.2024.03.08 val PER: 0.1778
2026-01-14 02:46:12,512: t15.2024.03.15 val PER: 0.1626
2026-01-14 02:46:12,512: t15.2024.03.17 val PER: 0.0948
2026-01-14 02:46:12,512: t15.2024.05.10 val PER: 0.1337
2026-01-14 02:46:12,512: t15.2024.06.14 val PER: 0.1041
2026-01-14 02:46:12,513: t15.2024.07.19 val PER: 0.1800
2026-01-14 02:46:12,513: t15.2024.07.21 val PER: 0.0710
2026-01-14 02:46:12,513: t15.2024.07.28 val PER: 0.0985
2026-01-14 02:46:12,513: t15.2025.01.10 val PER: 0.2631
2026-01-14 02:46:12,513: t15.2025.01.12 val PER: 0.0885
2026-01-14 02:46:12,513: t15.2025.03.14 val PER: 0.2870
2026-01-14 02:46:12,513: t15.2025.03.16 val PER: 0.1217
2026-01-14 02:46:12,513: t15.2025.03.30 val PER: 0.2080
2026-01-14 02:46:12,513: t15.2025.04.13 val PER: 0.1912
2026-01-14 02:46:12,673: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_35500
2026-01-14 02:46:21,565: Train batch 35600: loss: 0.92 grad norm: 28.57 time: 0.092
2026-01-14 02:46:39,520: Train batch 35800: loss: 0.42 grad norm: 17.38 time: 0.080
2026-01-14 02:46:56,886: Train batch 36000: loss: 1.38 grad norm: 33.72 time: 0.066
2026-01-14 02:46:56,886: Running test after training batch: 36000
2026-01-14 02:46:57,050: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:47:02,448: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:47:02,507: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:47:14,968: Val batch 36000: PER (avg): 0.1103 CTC Loss (avg): 28.3487 WER(5gram): 11.60% (n=256) time: 18.081
2026-01-14 02:47:14,968: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-14 02:47:14,969: t15.2023.08.13 val PER: 0.0790
2026-01-14 02:47:14,969: t15.2023.08.18 val PER: 0.0746
2026-01-14 02:47:14,969: t15.2023.08.20 val PER: 0.0659
2026-01-14 02:47:14,969: t15.2023.08.25 val PER: 0.0708
2026-01-14 02:47:14,969: t15.2023.08.27 val PER: 0.1527
2026-01-14 02:47:14,969: t15.2023.09.01 val PER: 0.0503
2026-01-14 02:47:14,969: t15.2023.09.03 val PER: 0.1176
2026-01-14 02:47:14,969: t15.2023.09.24 val PER: 0.0886
2026-01-14 02:47:14,969: t15.2023.09.29 val PER: 0.1078
2026-01-14 02:47:14,969: t15.2023.10.01 val PER: 0.1433
2026-01-14 02:47:14,970: t15.2023.10.06 val PER: 0.0635
2026-01-14 02:47:14,970: t15.2023.10.08 val PER: 0.2260
2026-01-14 02:47:14,970: t15.2023.10.13 val PER: 0.1691
2026-01-14 02:47:14,970: t15.2023.10.15 val PER: 0.1114
2026-01-14 02:47:14,970: t15.2023.10.20 val PER: 0.1745
2026-01-14 02:47:14,970: t15.2023.10.22 val PER: 0.0991
2026-01-14 02:47:14,970: t15.2023.11.03 val PER: 0.1689
2026-01-14 02:47:14,970: t15.2023.11.04 val PER: 0.0137
2026-01-14 02:47:14,970: t15.2023.11.17 val PER: 0.0264
2026-01-14 02:47:14,971: t15.2023.11.19 val PER: 0.0140
2026-01-14 02:47:14,971: t15.2023.11.26 val PER: 0.0522
2026-01-14 02:47:14,971: t15.2023.12.03 val PER: 0.0609
2026-01-14 02:47:14,971: t15.2023.12.08 val PER: 0.0366
2026-01-14 02:47:14,971: t15.2023.12.10 val PER: 0.0329
2026-01-14 02:47:14,971: t15.2023.12.17 val PER: 0.0863
2026-01-14 02:47:14,971: t15.2023.12.29 val PER: 0.0728
2026-01-14 02:47:14,971: t15.2024.02.25 val PER: 0.0758
2026-01-14 02:47:14,971: t15.2024.03.08 val PER: 0.1849
2026-01-14 02:47:14,971: t15.2024.03.15 val PER: 0.1664
2026-01-14 02:47:14,971: t15.2024.03.17 val PER: 0.0955
2026-01-14 02:47:14,971: t15.2024.05.10 val PER: 0.1218
2026-01-14 02:47:14,971: t15.2024.06.14 val PER: 0.1230
2026-01-14 02:47:14,971: t15.2024.07.19 val PER: 0.1734
2026-01-14 02:47:14,971: t15.2024.07.21 val PER: 0.0697
2026-01-14 02:47:14,971: t15.2024.07.28 val PER: 0.0897
2026-01-14 02:47:14,972: t15.2025.01.10 val PER: 0.2466
2026-01-14 02:47:14,972: t15.2025.01.12 val PER: 0.0824
2026-01-14 02:47:14,972: t15.2025.03.14 val PER: 0.2781
2026-01-14 02:47:14,972: t15.2025.03.16 val PER: 0.1309
2026-01-14 02:47:14,972: t15.2025.03.30 val PER: 0.2092
2026-01-14 02:47:14,972: t15.2025.04.13 val PER: 0.1969
2026-01-14 02:47:15,128: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_36000
2026-01-14 02:47:32,945: Train batch 36200: loss: 1.39 grad norm: 34.04 time: 0.080
2026-01-14 02:47:50,872: Train batch 36400: loss: 0.83 grad norm: 30.18 time: 0.099
2026-01-14 02:47:59,425: Running test after training batch: 36500
2026-01-14 02:47:59,591: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:48:05,051: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:48:05,108: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:48:18,172: Val batch 36500: PER (avg): 0.1097 CTC Loss (avg): 28.7477 WER(5gram): 14.15% (n=256) time: 18.746
2026-01-14 02:48:18,172: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-14 02:48:18,173: t15.2023.08.13 val PER: 0.0759
2026-01-14 02:48:18,173: t15.2023.08.18 val PER: 0.0662
2026-01-14 02:48:18,173: t15.2023.08.20 val PER: 0.0604
2026-01-14 02:48:18,173: t15.2023.08.25 val PER: 0.0738
2026-01-14 02:48:18,173: t15.2023.08.27 val PER: 0.1527
2026-01-14 02:48:18,173: t15.2023.09.01 val PER: 0.0495
2026-01-14 02:48:18,173: t15.2023.09.03 val PER: 0.1223
2026-01-14 02:48:18,173: t15.2023.09.24 val PER: 0.0862
2026-01-14 02:48:18,173: t15.2023.09.29 val PER: 0.1015
2026-01-14 02:48:18,173: t15.2023.10.01 val PER: 0.1453
2026-01-14 02:48:18,174: t15.2023.10.06 val PER: 0.0646
2026-01-14 02:48:18,174: t15.2023.10.08 val PER: 0.2192
2026-01-14 02:48:18,174: t15.2023.10.13 val PER: 0.1598
2026-01-14 02:48:18,174: t15.2023.10.15 val PER: 0.1180
2026-01-14 02:48:18,174: t15.2023.10.20 val PER: 0.1812
2026-01-14 02:48:18,174: t15.2023.10.22 val PER: 0.0958
2026-01-14 02:48:18,174: t15.2023.11.03 val PER: 0.1628
2026-01-14 02:48:18,174: t15.2023.11.04 val PER: 0.0205
2026-01-14 02:48:18,174: t15.2023.11.17 val PER: 0.0264
2026-01-14 02:48:18,175: t15.2023.11.19 val PER: 0.0160
2026-01-14 02:48:18,175: t15.2023.11.26 val PER: 0.0558
2026-01-14 02:48:18,175: t15.2023.12.03 val PER: 0.0588
2026-01-14 02:48:18,175: t15.2023.12.08 val PER: 0.0379
2026-01-14 02:48:18,175: t15.2023.12.10 val PER: 0.0355
2026-01-14 02:48:18,175: t15.2023.12.17 val PER: 0.0904
2026-01-14 02:48:18,175: t15.2023.12.29 val PER: 0.0776
2026-01-14 02:48:18,175: t15.2024.02.25 val PER: 0.0660
2026-01-14 02:48:18,175: t15.2024.03.08 val PER: 0.1565
2026-01-14 02:48:18,175: t15.2024.03.15 val PER: 0.1639
2026-01-14 02:48:18,176: t15.2024.03.17 val PER: 0.0851
2026-01-14 02:48:18,176: t15.2024.05.10 val PER: 0.1218
2026-01-14 02:48:18,176: t15.2024.06.14 val PER: 0.1372
2026-01-14 02:48:18,176: t15.2024.07.19 val PER: 0.1826
2026-01-14 02:48:18,176: t15.2024.07.21 val PER: 0.0697
2026-01-14 02:48:18,176: t15.2024.07.28 val PER: 0.0956
2026-01-14 02:48:18,176: t15.2025.01.10 val PER: 0.2424
2026-01-14 02:48:18,176: t15.2025.01.12 val PER: 0.0816
2026-01-14 02:48:18,176: t15.2025.03.14 val PER: 0.2840
2026-01-14 02:48:18,176: t15.2025.03.16 val PER: 0.1204
2026-01-14 02:48:18,176: t15.2025.03.30 val PER: 0.2322
2026-01-14 02:48:18,176: t15.2025.04.13 val PER: 0.1883
2026-01-14 02:48:18,338: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_36500
2026-01-14 02:48:27,145: Train batch 36600: loss: 1.37 grad norm: 39.58 time: 0.090
2026-01-14 02:48:45,064: Train batch 36800: loss: 1.75 grad norm: 36.46 time: 0.064
2026-01-14 02:49:02,740: Train batch 37000: loss: 0.75 grad norm: 24.60 time: 0.077
2026-01-14 02:49:02,740: Running test after training batch: 37000
2026-01-14 02:49:02,852: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:49:08,496: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:49:08,557: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cent
2026-01-14 02:49:20,752: Val batch 37000: PER (avg): 0.1110 CTC Loss (avg): 29.2739 WER(5gram): 11.73% (n=256) time: 18.011
2026-01-14 02:49:20,752: WER lens: avg_true_words=5.99 avg_pred_words=6.09 max_pred_words=12
2026-01-14 02:49:20,752: t15.2023.08.13 val PER: 0.0790
2026-01-14 02:49:20,752: t15.2023.08.18 val PER: 0.0721
2026-01-14 02:49:20,753: t15.2023.08.20 val PER: 0.0556
2026-01-14 02:49:20,753: t15.2023.08.25 val PER: 0.0723
2026-01-14 02:49:20,753: t15.2023.08.27 val PER: 0.1527
2026-01-14 02:49:20,753: t15.2023.09.01 val PER: 0.0463
2026-01-14 02:49:20,753: t15.2023.09.03 val PER: 0.1188
2026-01-14 02:49:20,753: t15.2023.09.24 val PER: 0.0862
2026-01-14 02:49:20,753: t15.2023.09.29 val PER: 0.1104
2026-01-14 02:49:20,753: t15.2023.10.01 val PER: 0.1446
2026-01-14 02:49:20,753: t15.2023.10.06 val PER: 0.0646
2026-01-14 02:49:20,753: t15.2023.10.08 val PER: 0.2179
2026-01-14 02:49:20,754: t15.2023.10.13 val PER: 0.1730
2026-01-14 02:49:20,754: t15.2023.10.15 val PER: 0.1107
2026-01-14 02:49:20,754: t15.2023.10.20 val PER: 0.1510
2026-01-14 02:49:20,754: t15.2023.10.22 val PER: 0.0991
2026-01-14 02:49:20,754: t15.2023.11.03 val PER: 0.1669
2026-01-14 02:49:20,754: t15.2023.11.04 val PER: 0.0137
2026-01-14 02:49:20,754: t15.2023.11.17 val PER: 0.0280
2026-01-14 02:49:20,754: t15.2023.11.19 val PER: 0.0140
2026-01-14 02:49:20,754: t15.2023.11.26 val PER: 0.0464
2026-01-14 02:49:20,754: t15.2023.12.03 val PER: 0.0620
2026-01-14 02:49:20,754: t15.2023.12.08 val PER: 0.0366
2026-01-14 02:49:20,754: t15.2023.12.10 val PER: 0.0381
2026-01-14 02:49:20,755: t15.2023.12.17 val PER: 0.0915
2026-01-14 02:49:20,755: t15.2023.12.29 val PER: 0.0707
2026-01-14 02:49:20,755: t15.2024.02.25 val PER: 0.0772
2026-01-14 02:49:20,755: t15.2024.03.08 val PER: 0.1550
2026-01-14 02:49:20,755: t15.2024.03.15 val PER: 0.1695
2026-01-14 02:49:20,755: t15.2024.03.17 val PER: 0.0893
2026-01-14 02:49:20,755: t15.2024.05.10 val PER: 0.1322
2026-01-14 02:49:20,755: t15.2024.06.14 val PER: 0.1293
2026-01-14 02:49:20,755: t15.2024.07.19 val PER: 0.1846
2026-01-14 02:49:20,755: t15.2024.07.21 val PER: 0.0690
2026-01-14 02:49:20,755: t15.2024.07.28 val PER: 0.0919
2026-01-14 02:49:20,755: t15.2025.01.10 val PER: 0.2534
2026-01-14 02:49:20,755: t15.2025.01.12 val PER: 0.0939
2026-01-14 02:49:20,755: t15.2025.03.14 val PER: 0.2899
2026-01-14 02:49:20,755: t15.2025.03.16 val PER: 0.1257
2026-01-14 02:49:20,756: t15.2025.03.30 val PER: 0.2414
2026-01-14 02:49:20,756: t15.2025.04.13 val PER: 0.1883
2026-01-14 02:49:20,917: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_37000
2026-01-14 02:49:38,713: Train batch 37200: loss: 0.69 grad norm: 17.83 time: 0.073
2026-01-14 02:49:56,581: Train batch 37400: loss: 1.15 grad norm: 31.06 time: 0.093
2026-01-14 02:50:05,403: Running test after training batch: 37500
2026-01-14 02:50:05,585: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:50:11,089: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:50:11,154: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:50:23,176: Val batch 37500: PER (avg): 0.1082 CTC Loss (avg): 28.0904 WER(5gram): 12.78% (n=256) time: 17.772
2026-01-14 02:50:23,176: WER lens: avg_true_words=5.99 avg_pred_words=6.15 max_pred_words=12
2026-01-14 02:50:23,177: t15.2023.08.13 val PER: 0.0852
2026-01-14 02:50:23,177: t15.2023.08.18 val PER: 0.0662
2026-01-14 02:50:23,177: t15.2023.08.20 val PER: 0.0580
2026-01-14 02:50:23,177: t15.2023.08.25 val PER: 0.0783
2026-01-14 02:50:23,177: t15.2023.08.27 val PER: 0.1592
2026-01-14 02:50:23,177: t15.2023.09.01 val PER: 0.0438
2026-01-14 02:50:23,177: t15.2023.09.03 val PER: 0.1271
2026-01-14 02:50:23,177: t15.2023.09.24 val PER: 0.0886
2026-01-14 02:50:23,177: t15.2023.09.29 val PER: 0.1059
2026-01-14 02:50:23,177: t15.2023.10.01 val PER: 0.1413
2026-01-14 02:50:23,177: t15.2023.10.06 val PER: 0.0592
2026-01-14 02:50:23,177: t15.2023.10.08 val PER: 0.2192
2026-01-14 02:50:23,177: t15.2023.10.13 val PER: 0.1660
2026-01-14 02:50:23,177: t15.2023.10.15 val PER: 0.1127
2026-01-14 02:50:23,178: t15.2023.10.20 val PER: 0.1711
2026-01-14 02:50:23,178: t15.2023.10.22 val PER: 0.0935
2026-01-14 02:50:23,178: t15.2023.11.03 val PER: 0.1554
2026-01-14 02:50:23,178: t15.2023.11.04 val PER: 0.0137
2026-01-14 02:50:23,178: t15.2023.11.17 val PER: 0.0202
2026-01-14 02:50:23,178: t15.2023.11.19 val PER: 0.0140
2026-01-14 02:50:23,178: t15.2023.11.26 val PER: 0.0471
2026-01-14 02:50:23,178: t15.2023.12.03 val PER: 0.0546
2026-01-14 02:50:23,178: t15.2023.12.08 val PER: 0.0393
2026-01-14 02:50:23,178: t15.2023.12.10 val PER: 0.0302
2026-01-14 02:50:23,178: t15.2023.12.17 val PER: 0.0852
2026-01-14 02:50:23,178: t15.2023.12.29 val PER: 0.0734
2026-01-14 02:50:23,178: t15.2024.02.25 val PER: 0.0730
2026-01-14 02:50:23,178: t15.2024.03.08 val PER: 0.1664
2026-01-14 02:50:23,179: t15.2024.03.15 val PER: 0.1651
2026-01-14 02:50:23,179: t15.2024.03.17 val PER: 0.0879
2026-01-14 02:50:23,179: t15.2024.05.10 val PER: 0.1322
2026-01-14 02:50:23,179: t15.2024.06.14 val PER: 0.1262
2026-01-14 02:50:23,179: t15.2024.07.19 val PER: 0.1740
2026-01-14 02:50:23,179: t15.2024.07.21 val PER: 0.0628
2026-01-14 02:50:23,179: t15.2024.07.28 val PER: 0.0904
2026-01-14 02:50:23,179: t15.2025.01.10 val PER: 0.2424
2026-01-14 02:50:23,179: t15.2025.01.12 val PER: 0.0908
2026-01-14 02:50:23,180: t15.2025.03.14 val PER: 0.2825
2026-01-14 02:50:23,180: t15.2025.03.16 val PER: 0.1178
2026-01-14 02:50:23,180: t15.2025.03.30 val PER: 0.2195
2026-01-14 02:50:23,180: t15.2025.04.13 val PER: 0.1854
2026-01-14 02:50:23,335: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_37500
2026-01-14 02:50:32,152: Train batch 37600: loss: 0.56 grad norm: 23.67 time: 0.064
2026-01-14 02:50:50,010: Train batch 37800: loss: 1.85 grad norm: 40.54 time: 0.108
2026-01-14 02:51:08,400: Train batch 38000: loss: 1.88 grad norm: 44.69 time: 0.091
2026-01-14 02:51:08,400: Running test after training batch: 38000
2026-01-14 02:51:08,541: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:51:14,042: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:51:14,096: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:51:26,361: Val batch 38000: PER (avg): 0.1089 CTC Loss (avg): 29.1211 WER(5gram): 10.56% (n=256) time: 17.961
2026-01-14 02:51:26,361: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-14 02:51:26,362: t15.2023.08.13 val PER: 0.0769
2026-01-14 02:51:26,362: t15.2023.08.18 val PER: 0.0721
2026-01-14 02:51:26,362: t15.2023.08.20 val PER: 0.0588
2026-01-14 02:51:26,362: t15.2023.08.25 val PER: 0.0678
2026-01-14 02:51:26,362: t15.2023.08.27 val PER: 0.1511
2026-01-14 02:51:26,362: t15.2023.09.01 val PER: 0.0422
2026-01-14 02:51:26,362: t15.2023.09.03 val PER: 0.1093
2026-01-14 02:51:26,362: t15.2023.09.24 val PER: 0.0898
2026-01-14 02:51:26,362: t15.2023.09.29 val PER: 0.1034
2026-01-14 02:51:26,362: t15.2023.10.01 val PER: 0.1486
2026-01-14 02:51:26,362: t15.2023.10.06 val PER: 0.0560
2026-01-14 02:51:26,363: t15.2023.10.08 val PER: 0.2070
2026-01-14 02:51:26,363: t15.2023.10.13 val PER: 0.1606
2026-01-14 02:51:26,363: t15.2023.10.15 val PER: 0.1173
2026-01-14 02:51:26,363: t15.2023.10.20 val PER: 0.1678
2026-01-14 02:51:26,363: t15.2023.10.22 val PER: 0.0969
2026-01-14 02:51:26,363: t15.2023.11.03 val PER: 0.1615
2026-01-14 02:51:26,363: t15.2023.11.04 val PER: 0.0102
2026-01-14 02:51:26,363: t15.2023.11.17 val PER: 0.0171
2026-01-14 02:51:26,363: t15.2023.11.19 val PER: 0.0160
2026-01-14 02:51:26,364: t15.2023.11.26 val PER: 0.0435
2026-01-14 02:51:26,364: t15.2023.12.03 val PER: 0.0557
2026-01-14 02:51:26,364: t15.2023.12.08 val PER: 0.0393
2026-01-14 02:51:26,364: t15.2023.12.10 val PER: 0.0342
2026-01-14 02:51:26,364: t15.2023.12.17 val PER: 0.0863
2026-01-14 02:51:26,364: t15.2023.12.29 val PER: 0.0789
2026-01-14 02:51:26,364: t15.2024.02.25 val PER: 0.0801
2026-01-14 02:51:26,364: t15.2024.03.08 val PER: 0.1835
2026-01-14 02:51:26,364: t15.2024.03.15 val PER: 0.1682
2026-01-14 02:51:26,364: t15.2024.03.17 val PER: 0.0948
2026-01-14 02:51:26,364: t15.2024.05.10 val PER: 0.1144
2026-01-14 02:51:26,364: t15.2024.06.14 val PER: 0.1278
2026-01-14 02:51:26,365: t15.2024.07.19 val PER: 0.1866
2026-01-14 02:51:26,365: t15.2024.07.21 val PER: 0.0669
2026-01-14 02:51:26,365: t15.2024.07.28 val PER: 0.0897
2026-01-14 02:51:26,365: t15.2025.01.10 val PER: 0.2562
2026-01-14 02:51:26,365: t15.2025.01.12 val PER: 0.0831
2026-01-14 02:51:26,365: t15.2025.03.14 val PER: 0.2796
2026-01-14 02:51:26,365: t15.2025.03.16 val PER: 0.1152
2026-01-14 02:51:26,365: t15.2025.03.30 val PER: 0.2287
2026-01-14 02:51:26,365: t15.2025.04.13 val PER: 0.1769
2026-01-14 02:51:26,366: New best val WER(5gram) 10.95% --> 10.56%
2026-01-14 02:51:26,531: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_38000
2026-01-14 02:51:44,884: Train batch 38200: loss: 1.53 grad norm: 33.98 time: 0.106
2026-01-14 02:52:02,852: Train batch 38400: loss: 3.82 grad norm: 40.72 time: 0.092
2026-01-14 02:52:12,048: Running test after training batch: 38500
2026-01-14 02:52:12,242: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:52:17,698: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:52:17,755: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the costs sent
2026-01-14 02:52:30,872: Val batch 38500: PER (avg): 0.1104 CTC Loss (avg): 29.3429 WER(5gram): 13.30% (n=256) time: 18.824
2026-01-14 02:52:30,873: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-14 02:52:30,873: t15.2023.08.13 val PER: 0.0832
2026-01-14 02:52:30,873: t15.2023.08.18 val PER: 0.0704
2026-01-14 02:52:30,873: t15.2023.08.20 val PER: 0.0635
2026-01-14 02:52:30,873: t15.2023.08.25 val PER: 0.0693
2026-01-14 02:52:30,873: t15.2023.08.27 val PER: 0.1495
2026-01-14 02:52:30,873: t15.2023.09.01 val PER: 0.0479
2026-01-14 02:52:30,873: t15.2023.09.03 val PER: 0.1211
2026-01-14 02:52:30,873: t15.2023.09.24 val PER: 0.0837
2026-01-14 02:52:30,874: t15.2023.09.29 val PER: 0.1015
2026-01-14 02:52:30,874: t15.2023.10.01 val PER: 0.1466
2026-01-14 02:52:30,874: t15.2023.10.06 val PER: 0.0560
2026-01-14 02:52:30,874: t15.2023.10.08 val PER: 0.2179
2026-01-14 02:52:30,874: t15.2023.10.13 val PER: 0.1645
2026-01-14 02:52:30,874: t15.2023.10.15 val PER: 0.1200
2026-01-14 02:52:30,874: t15.2023.10.20 val PER: 0.1544
2026-01-14 02:52:30,874: t15.2023.10.22 val PER: 0.0958
2026-01-14 02:52:30,874: t15.2023.11.03 val PER: 0.1676
2026-01-14 02:52:30,874: t15.2023.11.04 val PER: 0.0137
2026-01-14 02:52:30,875: t15.2023.11.17 val PER: 0.0249
2026-01-14 02:52:30,875: t15.2023.11.19 val PER: 0.0140
2026-01-14 02:52:30,875: t15.2023.11.26 val PER: 0.0529
2026-01-14 02:52:30,875: t15.2023.12.03 val PER: 0.0504
2026-01-14 02:52:30,875: t15.2023.12.08 val PER: 0.0413
2026-01-14 02:52:30,875: t15.2023.12.10 val PER: 0.0289
2026-01-14 02:52:30,875: t15.2023.12.17 val PER: 0.0884
2026-01-14 02:52:30,875: t15.2023.12.29 val PER: 0.0755
2026-01-14 02:52:30,875: t15.2024.02.25 val PER: 0.0744
2026-01-14 02:52:30,875: t15.2024.03.08 val PER: 0.1650
2026-01-14 02:52:30,875: t15.2024.03.15 val PER: 0.1726
2026-01-14 02:52:30,875: t15.2024.03.17 val PER: 0.0844
2026-01-14 02:52:30,875: t15.2024.05.10 val PER: 0.1322
2026-01-14 02:52:30,875: t15.2024.06.14 val PER: 0.1388
2026-01-14 02:52:30,875: t15.2024.07.19 val PER: 0.1839
2026-01-14 02:52:30,875: t15.2024.07.21 val PER: 0.0634
2026-01-14 02:52:30,876: t15.2024.07.28 val PER: 0.0904
2026-01-14 02:52:30,876: t15.2025.01.10 val PER: 0.2686
2026-01-14 02:52:30,876: t15.2025.01.12 val PER: 0.0931
2026-01-14 02:52:30,876: t15.2025.03.14 val PER: 0.2855
2026-01-14 02:52:30,876: t15.2025.03.16 val PER: 0.1152
2026-01-14 02:52:30,876: t15.2025.03.30 val PER: 0.2195
2026-01-14 02:52:30,876: t15.2025.04.13 val PER: 0.1969
2026-01-14 02:52:31,053: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_38500
2026-01-14 02:52:39,634: Train batch 38600: loss: 1.25 grad norm: 28.30 time: 0.081
2026-01-14 02:52:57,427: Train batch 38800: loss: 1.13 grad norm: 33.91 time: 0.068
2026-01-14 02:53:15,693: Train batch 39000: loss: 0.48 grad norm: 29.43 time: 0.095
2026-01-14 02:53:15,693: Running test after training batch: 39000
2026-01-14 02:53:15,798: WER debug GT example: You can see the code at this point as well.
2026-01-14 02:53:21,189: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-14 02:53:21,244: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost down
2026-01-14 02:53:33,245: Val batch 39000: PER (avg): 0.1086 CTC Loss (avg): 29.0487 WER(5gram): 11.34% (n=256) time: 17.552
2026-01-14 02:53:33,246: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-14 02:53:33,246: t15.2023.08.13 val PER: 0.0769
2026-01-14 02:53:33,246: t15.2023.08.18 val PER: 0.0662
2026-01-14 02:53:33,246: t15.2023.08.20 val PER: 0.0635
2026-01-14 02:53:33,246: t15.2023.08.25 val PER: 0.0693
2026-01-14 02:53:33,246: t15.2023.08.27 val PER: 0.1495
2026-01-14 02:53:33,246: t15.2023.09.01 val PER: 0.0463
2026-01-14 02:53:33,246: t15.2023.09.03 val PER: 0.1152
2026-01-14 02:53:33,247: t15.2023.09.24 val PER: 0.0801
2026-01-14 02:53:33,247: t15.2023.09.29 val PER: 0.1047
2026-01-14 02:53:33,247: t15.2023.10.01 val PER: 0.1427
2026-01-14 02:53:33,247: t15.2023.10.06 val PER: 0.0549
2026-01-14 02:53:33,247: t15.2023.10.08 val PER: 0.2192
2026-01-14 02:53:33,247: t15.2023.10.13 val PER: 0.1637
2026-01-14 02:53:33,247: t15.2023.10.15 val PER: 0.1220
2026-01-14 02:53:33,247: t15.2023.10.20 val PER: 0.1544
2026-01-14 02:53:33,247: t15.2023.10.22 val PER: 0.0991
2026-01-14 02:53:33,247: t15.2023.11.03 val PER: 0.1608
2026-01-14 02:53:33,248: t15.2023.11.04 val PER: 0.0137
2026-01-14 02:53:33,248: t15.2023.11.17 val PER: 0.0202
2026-01-14 02:53:33,248: t15.2023.11.19 val PER: 0.0120
2026-01-14 02:53:33,248: t15.2023.11.26 val PER: 0.0514
2026-01-14 02:53:33,248: t15.2023.12.03 val PER: 0.0609
2026-01-14 02:53:33,248: t15.2023.12.08 val PER: 0.0353
2026-01-14 02:53:33,248: t15.2023.12.10 val PER: 0.0302
2026-01-14 02:53:33,248: t15.2023.12.17 val PER: 0.0852
2026-01-14 02:53:33,248: t15.2023.12.29 val PER: 0.0755
2026-01-14 02:53:33,248: t15.2024.02.25 val PER: 0.0787
2026-01-14 02:53:33,248: t15.2024.03.08 val PER: 0.1707
2026-01-14 02:53:33,249: t15.2024.03.15 val PER: 0.1620
2026-01-14 02:53:33,249: t15.2024.03.17 val PER: 0.0900
2026-01-14 02:53:33,249: t15.2024.05.10 val PER: 0.1263
2026-01-14 02:53:33,249: t15.2024.06.14 val PER: 0.1120
2026-01-14 02:53:33,249: t15.2024.07.19 val PER: 0.1852
2026-01-14 02:53:33,249: t15.2024.07.21 val PER: 0.0655
2026-01-14 02:53:33,249: t15.2024.07.28 val PER: 0.0838
2026-01-14 02:53:33,249: t15.2025.01.10 val PER: 0.2479
2026-01-14 02:53:33,249: t15.2025.01.12 val PER: 0.0839
2026-01-14 02:53:33,249: t15.2025.03.14 val PER: 0.2840
2026-01-14 02:53:33,249: t15.2025.03.16 val PER: 0.1296
2026-01-14 02:53:33,249: t15.2025.03.30 val PER: 0.2345
2026-01-14 02:53:33,249: t15.2025.04.13 val PER: 0.1812
2026-01-14 02:53:33,414: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/diphones/diphone_100k/checkpoint/checkpoint_batch_39000
