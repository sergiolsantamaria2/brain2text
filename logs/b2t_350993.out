TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_350993
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_350993/torch_extensions
WANDB_DIR=/tmp/e12511253_b2t_350993/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/e12511253_b2t_350993/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  8 10:57 /tmp/e12511253_b2t_350993/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
OUT_ROOT=/home/e12511253/Brain2Text/brain2text/trained_models
==============================================
Job: b2t  ID: 350993
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/gru/high_priority
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/gru/high_priority ==========
Num configs: 6

=== RUN combined_30k.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_30k
2026-01-08 10:57:55,729: Using device: cuda:0
2026-01-08 10:57:57,633: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-08 10:58:07,293: Using 45 sessions after filtering (from 45).
2026-01-08 10:58:07,736: Using torch.compile (if available)
2026-01-08 10:58:07,736: torch.compile not available (torch<2.0). Skipping.
2026-01-08 10:58:07,736: Initialized RNN decoding model
2026-01-08 10:58:07,736: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
    (1): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-08 10:58:07,737: Model has 45,499,433 parameters
2026-01-08 10:58:07,737: Model has 11,819,520 day-specific parameters | 25.98% of total parameters
2026-01-08 10:58:13,684: Successfully initialized datasets
2026-01-08 10:58:13,685: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-08 10:58:15,175: Train batch 0: loss: 601.28 grad norm: 2274.55 time: 0.329
2026-01-08 10:58:15,176: Running test after training batch: 0
2026-01-08 10:58:15,282: WER debug GT example: You can see the code at this point as well.
2026-01-08 10:58:19,932: WER debug example
  GT : you can see the code at this point as well
  PR : gilles hwang towle shout hetu
2026-01-08 10:58:19,991: WER debug example
  GT : how does it keep the cost down
  PR : willhelm ballyhoo
2026-01-08 10:58:24,923: Val batch 0: PER (avg): 1.3384 CTC Loss (avg): 626.7153 WER(1gram): 100.00% (n=64) time: 9.747
2026-01-08 10:58:24,923: WER lens: avg_true_words=6.16 avg_pred_words=1.55 max_pred_words=5
2026-01-08 10:58:24,924: t15.2023.08.13 val PER: 1.1913
2026-01-08 10:58:24,924: t15.2023.08.18 val PER: 1.2775
2026-01-08 10:58:24,924: t15.2023.08.20 val PER: 1.2780
2026-01-08 10:58:24,924: t15.2023.08.25 val PER: 1.3419
2026-01-08 10:58:24,924: t15.2023.08.27 val PER: 1.1977
2026-01-08 10:58:24,924: t15.2023.09.01 val PER: 1.3393
2026-01-08 10:58:24,924: t15.2023.09.03 val PER: 1.1805
2026-01-08 10:58:24,924: t15.2023.09.24 val PER: 1.4417
2026-01-08 10:58:24,924: t15.2023.09.29 val PER: 1.4812
2026-01-08 10:58:24,924: t15.2023.10.01 val PER: 1.1704
2026-01-08 10:58:24,924: t15.2023.10.06 val PER: 1.4295
2026-01-08 10:58:24,924: t15.2023.10.08 val PER: 1.1191
2026-01-08 10:58:24,924: t15.2023.10.13 val PER: 1.3530
2026-01-08 10:58:24,924: t15.2023.10.15 val PER: 1.3724
2026-01-08 10:58:24,924: t15.2023.10.20 val PER: 1.4866
2026-01-08 10:58:24,924: t15.2023.10.22 val PER: 1.3898
2026-01-08 10:58:24,925: t15.2023.11.03 val PER: 1.5597
2026-01-08 10:58:24,925: t15.2023.11.04 val PER: 1.5700
2026-01-08 10:58:24,925: t15.2023.11.17 val PER: 1.8569
2026-01-08 10:58:24,925: t15.2023.11.19 val PER: 1.5888
2026-01-08 10:58:24,925: t15.2023.11.26 val PER: 1.4145
2026-01-08 10:58:24,925: t15.2023.12.03 val PER: 1.3393
2026-01-08 10:58:24,925: t15.2023.12.08 val PER: 1.4095
2026-01-08 10:58:24,925: t15.2023.12.10 val PER: 1.4586
2026-01-08 10:58:24,925: t15.2023.12.17 val PER: 1.2380
2026-01-08 10:58:24,925: t15.2023.12.29 val PER: 1.2690
2026-01-08 10:58:24,925: t15.2024.02.25 val PER: 1.3132
2026-01-08 10:58:24,925: t15.2024.03.08 val PER: 1.2191
2026-01-08 10:58:24,925: t15.2024.03.15 val PER: 1.2739
2026-01-08 10:58:24,926: t15.2024.03.17 val PER: 1.3298
2026-01-08 10:58:24,926: t15.2024.05.10 val PER: 1.3789
2026-01-08 10:58:24,926: t15.2024.06.14 val PER: 1.4401
2026-01-08 10:58:24,926: t15.2024.07.19 val PER: 0.9993
2026-01-08 10:58:24,926: t15.2024.07.21 val PER: 1.4559
2026-01-08 10:58:24,926: t15.2024.07.28 val PER: 1.4706
2026-01-08 10:58:24,926: t15.2025.01.10 val PER: 0.9848
2026-01-08 10:58:24,926: t15.2025.01.12 val PER: 1.5358
2026-01-08 10:58:24,926: t15.2025.03.14 val PER: 1.0163
2026-01-08 10:58:24,926: t15.2025.03.16 val PER: 1.4306
2026-01-08 10:58:24,927: t15.2025.03.30 val PER: 1.2241
2026-01-08 10:58:24,927: t15.2025.04.13 val PER: 1.2611
2026-01-08 10:58:24,927: New best val WER(1gram) inf% --> 100.00%
2026-01-08 10:58:24,927: Checkpointing model
2026-01-08 10:58:25,064: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_30k/checkpoint/best_checkpoint
2026-01-08 10:58:43,343: Train batch 200: loss: 93.93 grad norm: 112.35 time: 0.057
2026-01-08 10:59:01,022: Train batch 400: loss: 65.43 grad norm: 85.45 time: 0.066
2026-01-08 10:59:09,798: Running test after training batch: 500
2026-01-08 10:59:09,939: WER debug GT example: You can see the code at this point as well.
2026-01-08 10:59:14,609: WER debug example
  GT : you can see the code at this point as well
  PR : hollyhead stees mothershead hoses reproduces
2026-01-08 10:59:14,691: WER debug example
  GT : how does it keep the cost down
  PR : knighthood citisteel this hasids
2026-01-08 10:59:19,913: Val batch 500: PER (avg): 0.6282 CTC Loss (avg): 70.6203 WER(1gram): 99.75% (n=64) time: 10.115
2026-01-08 10:59:19,914: WER lens: avg_true_words=6.16 avg_pred_words=3.38 max_pred_words=7
2026-01-08 10:59:19,914: t15.2023.08.13 val PER: 0.5904
2026-01-08 10:59:19,914: t15.2023.08.18 val PER: 0.5675
2026-01-08 10:59:19,914: t15.2023.08.20 val PER: 0.5631
2026-01-08 10:59:19,914: t15.2023.08.25 val PER: 0.5527
2026-01-08 10:59:19,914: t15.2023.08.27 val PER: 0.6254
2026-01-08 10:59:19,914: t15.2023.09.01 val PER: 0.5820
2026-01-08 10:59:19,914: t15.2023.09.03 val PER: 0.6164
2026-01-08 10:59:19,915: t15.2023.09.24 val PER: 0.5704
2026-01-08 10:59:19,915: t15.2023.09.29 val PER: 0.5718
2026-01-08 10:59:19,915: t15.2023.10.01 val PER: 0.6347
2026-01-08 10:59:19,915: t15.2023.10.06 val PER: 0.5953
2026-01-08 10:59:19,915: t15.2023.10.08 val PER: 0.6198
2026-01-08 10:59:19,915: t15.2023.10.13 val PER: 0.6703
2026-01-08 10:59:19,915: t15.2023.10.15 val PER: 0.6282
2026-01-08 10:59:19,915: t15.2023.10.20 val PER: 0.5738
2026-01-08 10:59:19,915: t15.2023.10.22 val PER: 0.6002
2026-01-08 10:59:19,915: t15.2023.11.03 val PER: 0.6608
2026-01-08 10:59:19,915: t15.2023.11.04 val PER: 0.5154
2026-01-08 10:59:19,915: t15.2023.11.17 val PER: 0.5412
2026-01-08 10:59:19,915: t15.2023.11.19 val PER: 0.4731
2026-01-08 10:59:19,915: t15.2023.11.26 val PER: 0.6362
2026-01-08 10:59:19,916: t15.2023.12.03 val PER: 0.6176
2026-01-08 10:59:19,916: t15.2023.12.08 val PER: 0.6005
2026-01-08 10:59:19,916: t15.2023.12.10 val PER: 0.5874
2026-01-08 10:59:19,916: t15.2023.12.17 val PER: 0.6299
2026-01-08 10:59:19,916: t15.2023.12.29 val PER: 0.6040
2026-01-08 10:59:19,916: t15.2024.02.25 val PER: 0.6166
2026-01-08 10:59:19,916: t15.2024.03.08 val PER: 0.6714
2026-01-08 10:59:19,916: t15.2024.03.15 val PER: 0.6535
2026-01-08 10:59:19,916: t15.2024.03.17 val PER: 0.6130
2026-01-08 10:59:19,916: t15.2024.05.10 val PER: 0.6270
2026-01-08 10:59:19,916: t15.2024.06.14 val PER: 0.6893
2026-01-08 10:59:19,916: t15.2024.07.19 val PER: 0.7264
2026-01-08 10:59:19,917: t15.2024.07.21 val PER: 0.6331
2026-01-08 10:59:19,917: t15.2024.07.28 val PER: 0.6610
2026-01-08 10:59:19,917: t15.2025.01.10 val PER: 0.7672
2026-01-08 10:59:19,917: t15.2025.01.12 val PER: 0.6644
2026-01-08 10:59:19,917: t15.2025.03.14 val PER: 0.7441
2026-01-08 10:59:19,917: t15.2025.03.16 val PER: 0.6976
2026-01-08 10:59:19,917: t15.2025.03.30 val PER: 0.7448
2026-01-08 10:59:19,917: t15.2025.04.13 val PER: 0.6619
2026-01-08 10:59:19,918: New best val WER(1gram) 100.00% --> 99.75%
2026-01-08 10:59:19,918: Checkpointing model
2026-01-08 10:59:20,054: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_30k/checkpoint/best_checkpoint
2026-01-08 10:59:29,187: Train batch 600: loss: 62.88 grad norm: 113.51 time: 0.081
2026-01-08 10:59:47,190: Train batch 800: loss: 53.32 grad norm: 98.47 time: 0.059
2026-01-08 11:00:05,198: Train batch 1000: loss: 51.41 grad norm: 89.48 time: 0.068
2026-01-08 11:00:05,198: Running test after training batch: 1000
2026-01-08 11:00:05,294: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:00:10,312: WER debug example
  GT : you can see the code at this point as well
  PR : hudak indices suk advocates spuds swells
2026-01-08 11:00:10,360: WER debug example
  GT : how does it keep the cost down
  PR : diehards hitz kicks thus
2026-01-08 11:00:13,582: Val batch 1000: PER (avg): 0.4885 CTC Loss (avg): 52.9792 WER(1gram): 99.24% (n=64) time: 8.384
2026-01-08 11:00:13,582: WER lens: avg_true_words=6.16 avg_pred_words=4.09 max_pred_words=7
2026-01-08 11:00:13,583: t15.2023.08.13 val PER: 0.4532
2026-01-08 11:00:13,583: t15.2023.08.18 val PER: 0.4434
2026-01-08 11:00:13,583: t15.2023.08.20 val PER: 0.4519
2026-01-08 11:00:13,583: t15.2023.08.25 val PER: 0.3886
2026-01-08 11:00:13,583: t15.2023.08.27 val PER: 0.5338
2026-01-08 11:00:13,583: t15.2023.09.01 val PER: 0.4115
2026-01-08 11:00:13,583: t15.2023.09.03 val PER: 0.5095
2026-01-08 11:00:13,583: t15.2023.09.24 val PER: 0.4235
2026-01-08 11:00:13,583: t15.2023.09.29 val PER: 0.4467
2026-01-08 11:00:13,583: t15.2023.10.01 val PER: 0.4828
2026-01-08 11:00:13,583: t15.2023.10.06 val PER: 0.3994
2026-01-08 11:00:13,583: t15.2023.10.08 val PER: 0.5277
2026-01-08 11:00:13,583: t15.2023.10.13 val PER: 0.5477
2026-01-08 11:00:13,583: t15.2023.10.15 val PER: 0.4529
2026-01-08 11:00:13,584: t15.2023.10.20 val PER: 0.4295
2026-01-08 11:00:13,584: t15.2023.10.22 val PER: 0.4310
2026-01-08 11:00:13,584: t15.2023.11.03 val PER: 0.4681
2026-01-08 11:00:13,584: t15.2023.11.04 val PER: 0.2730
2026-01-08 11:00:13,584: t15.2023.11.17 val PER: 0.3390
2026-01-08 11:00:13,584: t15.2023.11.19 val PER: 0.3034
2026-01-08 11:00:13,584: t15.2023.11.26 val PER: 0.5319
2026-01-08 11:00:13,584: t15.2023.12.03 val PER: 0.4758
2026-01-08 11:00:13,584: t15.2023.12.08 val PER: 0.4787
2026-01-08 11:00:13,584: t15.2023.12.10 val PER: 0.4205
2026-01-08 11:00:13,584: t15.2023.12.17 val PER: 0.4948
2026-01-08 11:00:13,584: t15.2023.12.29 val PER: 0.4907
2026-01-08 11:00:13,584: t15.2024.02.25 val PER: 0.4101
2026-01-08 11:00:13,585: t15.2024.03.08 val PER: 0.5775
2026-01-08 11:00:13,585: t15.2024.03.15 val PER: 0.5078
2026-01-08 11:00:13,585: t15.2024.03.17 val PER: 0.4819
2026-01-08 11:00:13,585: t15.2024.05.10 val PER: 0.4903
2026-01-08 11:00:13,585: t15.2024.06.14 val PER: 0.4874
2026-01-08 11:00:13,585: t15.2024.07.19 val PER: 0.6236
2026-01-08 11:00:13,585: t15.2024.07.21 val PER: 0.4366
2026-01-08 11:00:13,585: t15.2024.07.28 val PER: 0.4779
2026-01-08 11:00:13,585: t15.2025.01.10 val PER: 0.7218
2026-01-08 11:00:13,585: t15.2025.01.12 val PER: 0.5335
2026-01-08 11:00:13,585: t15.2025.03.14 val PER: 0.6953
2026-01-08 11:00:13,585: t15.2025.03.16 val PER: 0.5118
2026-01-08 11:00:13,586: t15.2025.03.30 val PER: 0.7080
2026-01-08 11:00:13,586: t15.2025.04.13 val PER: 0.5606
2026-01-08 11:00:13,586: New best val WER(1gram) 99.75% --> 99.24%
2026-01-08 11:00:13,587: Checkpointing model
2026-01-08 11:00:13,728: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_30k/checkpoint/best_checkpoint
2026-01-08 11:00:31,586: Train batch 1200: loss: 44.76 grad norm: 90.54 time: 0.071
2026-01-08 11:00:49,632: Train batch 1400: loss: 44.69 grad norm: 84.34 time: 0.063
2026-01-08 11:00:58,628: Running test after training batch: 1500
2026-01-08 11:00:58,727: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:01:03,454: WER debug example
  GT : you can see the code at this point as well
  PR : helke decease the quixote hatz appointees
2026-01-08 11:01:03,498: WER debug example
  GT : how does it keep the cost down
  PR : holds pizzi hitty skinks thus
2026-01-08 11:01:05,841: Val batch 1500: PER (avg): 0.4325 CTC Loss (avg): 46.0576 WER(1gram): 97.21% (n=64) time: 7.212
2026-01-08 11:01:05,841: WER lens: avg_true_words=6.16 avg_pred_words=4.50 max_pred_words=9
2026-01-08 11:01:05,841: t15.2023.08.13 val PER: 0.4096
2026-01-08 11:01:05,841: t15.2023.08.18 val PER: 0.3814
2026-01-08 11:01:05,842: t15.2023.08.20 val PER: 0.3836
2026-01-08 11:01:05,842: t15.2023.08.25 val PER: 0.3373
2026-01-08 11:01:05,842: t15.2023.08.27 val PER: 0.4711
2026-01-08 11:01:05,842: t15.2023.09.01 val PER: 0.3766
2026-01-08 11:01:05,842: t15.2023.09.03 val PER: 0.4406
2026-01-08 11:01:05,842: t15.2023.09.24 val PER: 0.3507
2026-01-08 11:01:05,842: t15.2023.09.29 val PER: 0.3861
2026-01-08 11:01:05,842: t15.2023.10.01 val PER: 0.4333
2026-01-08 11:01:05,842: t15.2023.10.06 val PER: 0.3617
2026-01-08 11:01:05,842: t15.2023.10.08 val PER: 0.4790
2026-01-08 11:01:05,842: t15.2023.10.13 val PER: 0.4825
2026-01-08 11:01:05,842: t15.2023.10.15 val PER: 0.4087
2026-01-08 11:01:05,842: t15.2023.10.20 val PER: 0.4027
2026-01-08 11:01:05,842: t15.2023.10.22 val PER: 0.3719
2026-01-08 11:01:05,843: t15.2023.11.03 val PER: 0.4213
2026-01-08 11:01:05,843: t15.2023.11.04 val PER: 0.2048
2026-01-08 11:01:05,843: t15.2023.11.17 val PER: 0.2846
2026-01-08 11:01:05,843: t15.2023.11.19 val PER: 0.2575
2026-01-08 11:01:05,843: t15.2023.11.26 val PER: 0.4746
2026-01-08 11:01:05,843: t15.2023.12.03 val PER: 0.4286
2026-01-08 11:01:05,843: t15.2023.12.08 val PER: 0.4174
2026-01-08 11:01:05,843: t15.2023.12.10 val PER: 0.3666
2026-01-08 11:01:05,843: t15.2023.12.17 val PER: 0.4200
2026-01-08 11:01:05,843: t15.2023.12.29 val PER: 0.4283
2026-01-08 11:01:05,843: t15.2024.02.25 val PER: 0.3820
2026-01-08 11:01:05,843: t15.2024.03.08 val PER: 0.5007
2026-01-08 11:01:05,843: t15.2024.03.15 val PER: 0.4522
2026-01-08 11:01:05,843: t15.2024.03.17 val PER: 0.4331
2026-01-08 11:01:05,843: t15.2024.05.10 val PER: 0.4309
2026-01-08 11:01:05,843: t15.2024.06.14 val PER: 0.4432
2026-01-08 11:01:05,843: t15.2024.07.19 val PER: 0.5695
2026-01-08 11:01:05,844: t15.2024.07.21 val PER: 0.3717
2026-01-08 11:01:05,844: t15.2024.07.28 val PER: 0.4051
2026-01-08 11:01:05,844: t15.2025.01.10 val PER: 0.6584
2026-01-08 11:01:05,844: t15.2025.01.12 val PER: 0.4704
2026-01-08 11:01:05,844: t15.2025.03.14 val PER: 0.6376
2026-01-08 11:01:05,844: t15.2025.03.16 val PER: 0.4686
2026-01-08 11:01:05,844: t15.2025.03.30 val PER: 0.6483
2026-01-08 11:01:05,844: t15.2025.04.13 val PER: 0.5093
2026-01-08 11:01:05,846: New best val WER(1gram) 99.24% --> 97.21%
2026-01-08 11:01:05,846: Checkpointing model
2026-01-08 11:01:05,981: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_30k/checkpoint/best_checkpoint
2026-01-08 11:01:14,933: Train batch 1600: loss: 43.63 grad norm: 76.69 time: 0.067
2026-01-08 11:01:33,075: Train batch 1800: loss: 42.80 grad norm: 85.12 time: 0.091
2026-01-08 11:01:51,154: Train batch 2000: loss: 41.11 grad norm: 79.05 time: 0.071
2026-01-08 11:01:51,155: Running test after training batch: 2000
2026-01-08 11:01:51,293: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:01:56,117: WER debug example
  GT : you can see the code at this point as well
  PR : hudy candy cetus butthead hotspot his swells
2026-01-08 11:01:56,154: WER debug example
  GT : how does it keep the cost down
  PR : diehards city sikhs that squats
2026-01-08 11:01:58,232: Val batch 2000: PER (avg): 0.3970 CTC Loss (avg): 41.8153 WER(1gram): 98.73% (n=64) time: 7.077
2026-01-08 11:01:58,232: WER lens: avg_true_words=6.16 avg_pred_words=4.75 max_pred_words=11
2026-01-08 11:01:58,232: t15.2023.08.13 val PER: 0.3711
2026-01-08 11:01:58,232: t15.2023.08.18 val PER: 0.3453
2026-01-08 11:01:58,232: t15.2023.08.20 val PER: 0.3479
2026-01-08 11:01:58,233: t15.2023.08.25 val PER: 0.3223
2026-01-08 11:01:58,233: t15.2023.08.27 val PER: 0.4148
2026-01-08 11:01:58,233: t15.2023.09.01 val PER: 0.3263
2026-01-08 11:01:58,233: t15.2023.09.03 val PER: 0.4228
2026-01-08 11:01:58,233: t15.2023.09.24 val PER: 0.3240
2026-01-08 11:01:58,233: t15.2023.09.29 val PER: 0.3312
2026-01-08 11:01:58,233: t15.2023.10.01 val PER: 0.3745
2026-01-08 11:01:58,233: t15.2023.10.06 val PER: 0.3272
2026-01-08 11:01:58,233: t15.2023.10.08 val PER: 0.4533
2026-01-08 11:01:58,233: t15.2023.10.13 val PER: 0.4717
2026-01-08 11:01:58,233: t15.2023.10.15 val PER: 0.3744
2026-01-08 11:01:58,234: t15.2023.10.20 val PER: 0.3792
2026-01-08 11:01:58,234: t15.2023.10.22 val PER: 0.3408
2026-01-08 11:01:58,234: t15.2023.11.03 val PER: 0.3874
2026-01-08 11:01:58,234: t15.2023.11.04 val PER: 0.1809
2026-01-08 11:01:58,234: t15.2023.11.17 val PER: 0.2426
2026-01-08 11:01:58,234: t15.2023.11.19 val PER: 0.2395
2026-01-08 11:01:58,234: t15.2023.11.26 val PER: 0.4406
2026-01-08 11:01:58,234: t15.2023.12.03 val PER: 0.3929
2026-01-08 11:01:58,234: t15.2023.12.08 val PER: 0.3888
2026-01-08 11:01:58,234: t15.2023.12.10 val PER: 0.3311
2026-01-08 11:01:58,234: t15.2023.12.17 val PER: 0.3742
2026-01-08 11:01:58,234: t15.2023.12.29 val PER: 0.3898
2026-01-08 11:01:58,234: t15.2024.02.25 val PER: 0.3610
2026-01-08 11:01:58,234: t15.2024.03.08 val PER: 0.4523
2026-01-08 11:01:58,235: t15.2024.03.15 val PER: 0.4084
2026-01-08 11:01:58,235: t15.2024.03.17 val PER: 0.4149
2026-01-08 11:01:58,235: t15.2024.05.10 val PER: 0.4056
2026-01-08 11:01:58,235: t15.2024.06.14 val PER: 0.4117
2026-01-08 11:01:58,235: t15.2024.07.19 val PER: 0.5115
2026-01-08 11:01:58,235: t15.2024.07.21 val PER: 0.3497
2026-01-08 11:01:58,235: t15.2024.07.28 val PER: 0.3853
2026-01-08 11:01:58,235: t15.2025.01.10 val PER: 0.5813
2026-01-08 11:01:58,235: t15.2025.01.12 val PER: 0.4527
2026-01-08 11:01:58,235: t15.2025.03.14 val PER: 0.5962
2026-01-08 11:01:58,235: t15.2025.03.16 val PER: 0.4529
2026-01-08 11:01:58,235: t15.2025.03.30 val PER: 0.5862
2026-01-08 11:01:58,235: t15.2025.04.13 val PER: 0.4722
2026-01-08 11:02:16,063: Train batch 2200: loss: 38.08 grad norm: 74.11 time: 0.062
2026-01-08 11:02:33,316: Train batch 2400: loss: 37.48 grad norm: 82.64 time: 0.054
2026-01-08 11:02:41,872: Running test after training batch: 2500
2026-01-08 11:02:41,968: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:02:46,598: WER debug example
  GT : you can see the code at this point as well
  PR : hughley candidacies smutty headhunter cysts ponds swells
2026-01-08 11:02:46,630: WER debug example
  GT : how does it keep the cost down
  PR : diehards skits kicks that squats
2026-01-08 11:02:48,339: Val batch 2500: PER (avg): 0.3662 CTC Loss (avg): 39.0770 WER(1gram): 97.97% (n=64) time: 6.467
2026-01-08 11:02:48,340: WER lens: avg_true_words=6.16 avg_pred_words=4.84 max_pred_words=10
2026-01-08 11:02:48,340: t15.2023.08.13 val PER: 0.3534
2026-01-08 11:02:48,340: t15.2023.08.18 val PER: 0.3395
2026-01-08 11:02:48,340: t15.2023.08.20 val PER: 0.3145
2026-01-08 11:02:48,340: t15.2023.08.25 val PER: 0.2907
2026-01-08 11:02:48,340: t15.2023.08.27 val PER: 0.3939
2026-01-08 11:02:48,340: t15.2023.09.01 val PER: 0.3076
2026-01-08 11:02:48,340: t15.2023.09.03 val PER: 0.3717
2026-01-08 11:02:48,341: t15.2023.09.24 val PER: 0.2913
2026-01-08 11:02:48,341: t15.2023.09.29 val PER: 0.3057
2026-01-08 11:02:48,341: t15.2023.10.01 val PER: 0.3547
2026-01-08 11:02:48,341: t15.2023.10.06 val PER: 0.2820
2026-01-08 11:02:48,341: t15.2023.10.08 val PER: 0.4465
2026-01-08 11:02:48,341: t15.2023.10.13 val PER: 0.4236
2026-01-08 11:02:48,341: t15.2023.10.15 val PER: 0.3514
2026-01-08 11:02:48,341: t15.2023.10.20 val PER: 0.3523
2026-01-08 11:02:48,341: t15.2023.10.22 val PER: 0.3296
2026-01-08 11:02:48,341: t15.2023.11.03 val PER: 0.3745
2026-01-08 11:02:48,341: t15.2023.11.04 val PER: 0.1502
2026-01-08 11:02:48,341: t15.2023.11.17 val PER: 0.2084
2026-01-08 11:02:48,341: t15.2023.11.19 val PER: 0.2176
2026-01-08 11:02:48,342: t15.2023.11.26 val PER: 0.4072
2026-01-08 11:02:48,342: t15.2023.12.03 val PER: 0.3393
2026-01-08 11:02:48,342: t15.2023.12.08 val PER: 0.3455
2026-01-08 11:02:48,342: t15.2023.12.10 val PER: 0.2983
2026-01-08 11:02:48,342: t15.2023.12.17 val PER: 0.3524
2026-01-08 11:02:48,342: t15.2023.12.29 val PER: 0.3638
2026-01-08 11:02:48,342: t15.2024.02.25 val PER: 0.3174
2026-01-08 11:02:48,342: t15.2024.03.08 val PER: 0.4168
2026-01-08 11:02:48,342: t15.2024.03.15 val PER: 0.3765
2026-01-08 11:02:48,342: t15.2024.03.17 val PER: 0.3828
2026-01-08 11:02:48,342: t15.2024.05.10 val PER: 0.3477
2026-01-08 11:02:48,342: t15.2024.06.14 val PER: 0.3533
2026-01-08 11:02:48,342: t15.2024.07.19 val PER: 0.4647
2026-01-08 11:02:48,342: t15.2024.07.21 val PER: 0.3103
2026-01-08 11:02:48,342: t15.2024.07.28 val PER: 0.3463
2026-01-08 11:02:48,342: t15.2025.01.10 val PER: 0.5758
2026-01-08 11:02:48,342: t15.2025.01.12 val PER: 0.4226
2026-01-08 11:02:48,343: t15.2025.03.14 val PER: 0.5636
2026-01-08 11:02:48,343: t15.2025.03.16 val PER: 0.4018
2026-01-08 11:02:48,343: t15.2025.03.30 val PER: 0.5644
2026-01-08 11:02:48,343: t15.2025.04.13 val PER: 0.4579
2026-01-08 11:02:56,765: Train batch 2600: loss: 44.13 grad norm: 87.46 time: 0.057
2026-01-08 11:03:13,581: Train batch 2800: loss: 34.10 grad norm: 83.52 time: 0.084
2026-01-08 11:03:31,197: Train batch 3000: loss: 37.07 grad norm: 76.48 time: 0.087
2026-01-08 11:03:31,198: Running test after training batch: 3000
2026-01-08 11:03:31,294: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:03:35,946: WER debug example
  GT : you can see the code at this point as well
  PR : hudy canty seixas the steelhead hatlestad pandas
2026-01-08 11:03:35,974: WER debug example
  GT : how does it keep the cost down
  PR : shead spacey hitty skinks the teast
2026-01-08 11:03:37,634: Val batch 3000: PER (avg): 0.3513 CTC Loss (avg): 36.3123 WER(1gram): 97.21% (n=64) time: 6.436
2026-01-08 11:03:37,635: WER lens: avg_true_words=6.16 avg_pred_words=5.22 max_pred_words=10
2026-01-08 11:03:37,635: t15.2023.08.13 val PER: 0.3254
2026-01-08 11:03:37,635: t15.2023.08.18 val PER: 0.3043
2026-01-08 11:03:37,635: t15.2023.08.20 val PER: 0.3082
2026-01-08 11:03:37,635: t15.2023.08.25 val PER: 0.2771
2026-01-08 11:03:37,635: t15.2023.08.27 val PER: 0.3698
2026-01-08 11:03:37,635: t15.2023.09.01 val PER: 0.2914
2026-01-08 11:03:37,635: t15.2023.09.03 val PER: 0.3670
2026-01-08 11:03:37,635: t15.2023.09.24 val PER: 0.2743
2026-01-08 11:03:37,636: t15.2023.09.29 val PER: 0.3019
2026-01-08 11:03:37,636: t15.2023.10.01 val PER: 0.3441
2026-01-08 11:03:37,636: t15.2023.10.06 val PER: 0.2863
2026-01-08 11:03:37,636: t15.2023.10.08 val PER: 0.4249
2026-01-08 11:03:37,636: t15.2023.10.13 val PER: 0.4174
2026-01-08 11:03:37,636: t15.2023.10.15 val PER: 0.3349
2026-01-08 11:03:37,636: t15.2023.10.20 val PER: 0.3557
2026-01-08 11:03:37,637: t15.2023.10.22 val PER: 0.2973
2026-01-08 11:03:37,637: t15.2023.11.03 val PER: 0.3412
2026-01-08 11:03:37,637: t15.2023.11.04 val PER: 0.1604
2026-01-08 11:03:37,637: t15.2023.11.17 val PER: 0.1960
2026-01-08 11:03:37,637: t15.2023.11.19 val PER: 0.2136
2026-01-08 11:03:37,637: t15.2023.11.26 val PER: 0.3848
2026-01-08 11:03:37,637: t15.2023.12.03 val PER: 0.3393
2026-01-08 11:03:37,637: t15.2023.12.08 val PER: 0.3142
2026-01-08 11:03:37,637: t15.2023.12.10 val PER: 0.2943
2026-01-08 11:03:37,638: t15.2023.12.17 val PER: 0.3399
2026-01-08 11:03:37,638: t15.2023.12.29 val PER: 0.3432
2026-01-08 11:03:37,638: t15.2024.02.25 val PER: 0.3132
2026-01-08 11:03:37,638: t15.2024.03.08 val PER: 0.4054
2026-01-08 11:03:37,638: t15.2024.03.15 val PER: 0.3734
2026-01-08 11:03:37,638: t15.2024.03.17 val PER: 0.3647
2026-01-08 11:03:37,638: t15.2024.05.10 val PER: 0.3418
2026-01-08 11:03:37,638: t15.2024.06.14 val PER: 0.3675
2026-01-08 11:03:37,638: t15.2024.07.19 val PER: 0.4608
2026-01-08 11:03:37,638: t15.2024.07.21 val PER: 0.2931
2026-01-08 11:03:37,638: t15.2024.07.28 val PER: 0.3382
2026-01-08 11:03:37,638: t15.2025.01.10 val PER: 0.5551
2026-01-08 11:03:37,638: t15.2025.01.12 val PER: 0.4011
2026-01-08 11:03:37,639: t15.2025.03.14 val PER: 0.5370
2026-01-08 11:03:37,639: t15.2025.03.16 val PER: 0.3730
2026-01-08 11:03:37,639: t15.2025.03.30 val PER: 0.5448
2026-01-08 11:03:37,639: t15.2025.04.13 val PER: 0.4194
2026-01-08 11:03:54,326: Train batch 3200: loss: 31.22 grad norm: 64.24 time: 0.079
2026-01-08 11:04:11,169: Train batch 3400: loss: 26.98 grad norm: 59.96 time: 0.051
2026-01-08 11:04:19,704: Running test after training batch: 3500
2026-01-08 11:04:19,825: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:04:24,820: WER debug example
  GT : you can see the code at this point as well
  PR : euclea candy seixas the steelhead hatz suspends his swells
2026-01-08 11:04:24,850: WER debug example
  GT : how does it keep the cost down
  PR : leasehold surcease hitty skinks the tsetse
2026-01-08 11:04:26,382: Val batch 3500: PER (avg): 0.3354 CTC Loss (avg): 34.5577 WER(1gram): 98.98% (n=64) time: 6.678
2026-01-08 11:04:26,383: WER lens: avg_true_words=6.16 avg_pred_words=5.59 max_pred_words=11
2026-01-08 11:04:26,383: t15.2023.08.13 val PER: 0.3212
2026-01-08 11:04:26,383: t15.2023.08.18 val PER: 0.3051
2026-01-08 11:04:26,383: t15.2023.08.20 val PER: 0.3042
2026-01-08 11:04:26,383: t15.2023.08.25 val PER: 0.2560
2026-01-08 11:04:26,383: t15.2023.08.27 val PER: 0.3810
2026-01-08 11:04:26,383: t15.2023.09.01 val PER: 0.2727
2026-01-08 11:04:26,384: t15.2023.09.03 val PER: 0.3575
2026-01-08 11:04:26,384: t15.2023.09.24 val PER: 0.2718
2026-01-08 11:04:26,384: t15.2023.09.29 val PER: 0.2859
2026-01-08 11:04:26,384: t15.2023.10.01 val PER: 0.3190
2026-01-08 11:04:26,384: t15.2023.10.06 val PER: 0.2809
2026-01-08 11:04:26,384: t15.2023.10.08 val PER: 0.4154
2026-01-08 11:04:26,384: t15.2023.10.13 val PER: 0.3964
2026-01-08 11:04:26,384: t15.2023.10.15 val PER: 0.3138
2026-01-08 11:04:26,384: t15.2023.10.20 val PER: 0.2953
2026-01-08 11:04:26,384: t15.2023.10.22 val PER: 0.2751
2026-01-08 11:04:26,384: t15.2023.11.03 val PER: 0.3385
2026-01-08 11:04:26,385: t15.2023.11.04 val PER: 0.1399
2026-01-08 11:04:26,385: t15.2023.11.17 val PER: 0.1835
2026-01-08 11:04:26,385: t15.2023.11.19 val PER: 0.1816
2026-01-08 11:04:26,385: t15.2023.11.26 val PER: 0.3500
2026-01-08 11:04:26,385: t15.2023.12.03 val PER: 0.3109
2026-01-08 11:04:26,385: t15.2023.12.08 val PER: 0.3156
2026-01-08 11:04:26,385: t15.2023.12.10 val PER: 0.2602
2026-01-08 11:04:26,385: t15.2023.12.17 val PER: 0.3243
2026-01-08 11:04:26,385: t15.2023.12.29 val PER: 0.3356
2026-01-08 11:04:26,385: t15.2024.02.25 val PER: 0.2963
2026-01-08 11:04:26,385: t15.2024.03.08 val PER: 0.4154
2026-01-08 11:04:26,385: t15.2024.03.15 val PER: 0.3565
2026-01-08 11:04:26,385: t15.2024.03.17 val PER: 0.3529
2026-01-08 11:04:26,385: t15.2024.05.10 val PER: 0.3165
2026-01-08 11:04:26,385: t15.2024.06.14 val PER: 0.3502
2026-01-08 11:04:26,385: t15.2024.07.19 val PER: 0.4272
2026-01-08 11:04:26,386: t15.2024.07.21 val PER: 0.2717
2026-01-08 11:04:26,386: t15.2024.07.28 val PER: 0.3257
2026-01-08 11:04:26,386: t15.2025.01.10 val PER: 0.4972
2026-01-08 11:04:26,386: t15.2025.01.12 val PER: 0.3826
2026-01-08 11:04:26,386: t15.2025.03.14 val PER: 0.5104
2026-01-08 11:04:26,386: t15.2025.03.16 val PER: 0.3848
2026-01-08 11:04:26,386: t15.2025.03.30 val PER: 0.5103
2026-01-08 11:04:26,386: t15.2025.04.13 val PER: 0.4208
2026-01-08 11:04:35,123: Train batch 3600: loss: 30.93 grad norm: 71.08 time: 0.069
2026-01-08 11:04:52,269: Train batch 3800: loss: 35.08 grad norm: 86.19 time: 0.069
2026-01-08 11:05:09,842: Train batch 4000: loss: 25.44 grad norm: 60.04 time: 0.059
2026-01-08 11:05:09,843: Running test after training batch: 4000
2026-01-08 11:05:09,982: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:05:14,639: WER debug example
  GT : you can see the code at this point as well
  PR : utica decease the stuccoed hatz this appoints his swells
2026-01-08 11:05:14,664: WER debug example
  GT : how does it keep the cost down
  PR : shouse osiecki coos the tsetse
2026-01-08 11:05:16,030: Val batch 4000: PER (avg): 0.3199 CTC Loss (avg): 32.5369 WER(1gram): 97.72% (n=64) time: 6.188
2026-01-08 11:05:16,031: WER lens: avg_true_words=6.16 avg_pred_words=5.19 max_pred_words=11
2026-01-08 11:05:16,031: t15.2023.08.13 val PER: 0.3108
2026-01-08 11:05:16,031: t15.2023.08.18 val PER: 0.2892
2026-01-08 11:05:16,031: t15.2023.08.20 val PER: 0.2859
2026-01-08 11:05:16,031: t15.2023.08.25 val PER: 0.2395
2026-01-08 11:05:16,031: t15.2023.08.27 val PER: 0.3617
2026-01-08 11:05:16,031: t15.2023.09.01 val PER: 0.2687
2026-01-08 11:05:16,031: t15.2023.09.03 val PER: 0.3337
2026-01-08 11:05:16,031: t15.2023.09.24 val PER: 0.2682
2026-01-08 11:05:16,031: t15.2023.09.29 val PER: 0.2750
2026-01-08 11:05:16,032: t15.2023.10.01 val PER: 0.3203
2026-01-08 11:05:16,032: t15.2023.10.06 val PER: 0.2691
2026-01-08 11:05:16,032: t15.2023.10.08 val PER: 0.3965
2026-01-08 11:05:16,032: t15.2023.10.13 val PER: 0.3786
2026-01-08 11:05:16,032: t15.2023.10.15 val PER: 0.3072
2026-01-08 11:05:16,032: t15.2023.10.20 val PER: 0.3255
2026-01-08 11:05:16,032: t15.2023.10.22 val PER: 0.2728
2026-01-08 11:05:16,032: t15.2023.11.03 val PER: 0.3161
2026-01-08 11:05:16,032: t15.2023.11.04 val PER: 0.1468
2026-01-08 11:05:16,032: t15.2023.11.17 val PER: 0.2006
2026-01-08 11:05:16,032: t15.2023.11.19 val PER: 0.1836
2026-01-08 11:05:16,032: t15.2023.11.26 val PER: 0.3420
2026-01-08 11:05:16,032: t15.2023.12.03 val PER: 0.2878
2026-01-08 11:05:16,032: t15.2023.12.08 val PER: 0.2736
2026-01-08 11:05:16,033: t15.2023.12.10 val PER: 0.2562
2026-01-08 11:05:16,033: t15.2023.12.17 val PER: 0.2994
2026-01-08 11:05:16,033: t15.2023.12.29 val PER: 0.3226
2026-01-08 11:05:16,033: t15.2024.02.25 val PER: 0.2767
2026-01-08 11:05:16,033: t15.2024.03.08 val PER: 0.3983
2026-01-08 11:05:16,033: t15.2024.03.15 val PER: 0.3340
2026-01-08 11:05:16,033: t15.2024.03.17 val PER: 0.3215
2026-01-08 11:05:16,033: t15.2024.05.10 val PER: 0.3120
2026-01-08 11:05:16,033: t15.2024.06.14 val PER: 0.3360
2026-01-08 11:05:16,033: t15.2024.07.19 val PER: 0.4173
2026-01-08 11:05:16,033: t15.2024.07.21 val PER: 0.2517
2026-01-08 11:05:16,033: t15.2024.07.28 val PER: 0.3015
2026-01-08 11:05:16,033: t15.2025.01.10 val PER: 0.4821
2026-01-08 11:05:16,033: t15.2025.01.12 val PER: 0.3741
2026-01-08 11:05:16,034: t15.2025.03.14 val PER: 0.4630
2026-01-08 11:05:16,034: t15.2025.03.16 val PER: 0.3560
2026-01-08 11:05:16,034: t15.2025.03.30 val PER: 0.4828
2026-01-08 11:05:16,034: t15.2025.04.13 val PER: 0.4009
2026-01-08 11:05:32,971: Train batch 4200: loss: 30.09 grad norm: 67.35 time: 0.082
2026-01-08 11:05:50,032: Train batch 4400: loss: 24.43 grad norm: 61.24 time: 0.069
2026-01-08 11:05:58,771: Running test after training batch: 4500
2026-01-08 11:05:58,887: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:06:03,519: WER debug example
  GT : you can see the code at this point as well
  PR : utica decease the toehold tat cysts point his swells
2026-01-08 11:06:03,540: WER debug example
  GT : how does it keep the cost down
  PR : toehold suss hitz geeks the teast setzer
2026-01-08 11:06:04,861: Val batch 4500: PER (avg): 0.3071 CTC Loss (avg): 31.5106 WER(1gram): 97.21% (n=64) time: 6.090
2026-01-08 11:06:04,862: WER lens: avg_true_words=6.16 avg_pred_words=5.78 max_pred_words=11
2026-01-08 11:06:04,862: t15.2023.08.13 val PER: 0.2796
2026-01-08 11:06:04,862: t15.2023.08.18 val PER: 0.2875
2026-01-08 11:06:04,862: t15.2023.08.20 val PER: 0.2788
2026-01-08 11:06:04,862: t15.2023.08.25 val PER: 0.2304
2026-01-08 11:06:04,862: t15.2023.08.27 val PER: 0.3553
2026-01-08 11:06:04,862: t15.2023.09.01 val PER: 0.2468
2026-01-08 11:06:04,862: t15.2023.09.03 val PER: 0.3230
2026-01-08 11:06:04,862: t15.2023.09.24 val PER: 0.2561
2026-01-08 11:06:04,863: t15.2023.09.29 val PER: 0.2814
2026-01-08 11:06:04,863: t15.2023.10.01 val PER: 0.3104
2026-01-08 11:06:04,863: t15.2023.10.06 val PER: 0.2400
2026-01-08 11:06:04,863: t15.2023.10.08 val PER: 0.3978
2026-01-08 11:06:04,863: t15.2023.10.13 val PER: 0.3755
2026-01-08 11:06:04,863: t15.2023.10.15 val PER: 0.3059
2026-01-08 11:06:04,863: t15.2023.10.20 val PER: 0.2852
2026-01-08 11:06:04,863: t15.2023.10.22 val PER: 0.2528
2026-01-08 11:06:04,863: t15.2023.11.03 val PER: 0.3175
2026-01-08 11:06:04,863: t15.2023.11.04 val PER: 0.1365
2026-01-08 11:06:04,863: t15.2023.11.17 val PER: 0.1726
2026-01-08 11:06:04,863: t15.2023.11.19 val PER: 0.1816
2026-01-08 11:06:04,863: t15.2023.11.26 val PER: 0.3333
2026-01-08 11:06:04,863: t15.2023.12.03 val PER: 0.2794
2026-01-08 11:06:04,864: t15.2023.12.08 val PER: 0.2736
2026-01-08 11:06:04,864: t15.2023.12.10 val PER: 0.2378
2026-01-08 11:06:04,864: t15.2023.12.17 val PER: 0.2796
2026-01-08 11:06:04,864: t15.2023.12.29 val PER: 0.3054
2026-01-08 11:06:04,864: t15.2024.02.25 val PER: 0.2584
2026-01-08 11:06:04,864: t15.2024.03.08 val PER: 0.3613
2026-01-08 11:06:04,864: t15.2024.03.15 val PER: 0.3352
2026-01-08 11:06:04,864: t15.2024.03.17 val PER: 0.2992
2026-01-08 11:06:04,864: t15.2024.05.10 val PER: 0.3076
2026-01-08 11:06:04,864: t15.2024.06.14 val PER: 0.3060
2026-01-08 11:06:04,864: t15.2024.07.19 val PER: 0.4001
2026-01-08 11:06:04,864: t15.2024.07.21 val PER: 0.2186
2026-01-08 11:06:04,864: t15.2024.07.28 val PER: 0.2926
2026-01-08 11:06:04,864: t15.2025.01.10 val PER: 0.4628
2026-01-08 11:06:04,864: t15.2025.01.12 val PER: 0.3441
2026-01-08 11:06:04,864: t15.2025.03.14 val PER: 0.4453
2026-01-08 11:06:04,865: t15.2025.03.16 val PER: 0.3482
2026-01-08 11:06:04,865: t15.2025.03.30 val PER: 0.4759
2026-01-08 11:06:04,865: t15.2025.04.13 val PER: 0.3823
2026-01-08 11:06:14,230: Train batch 4600: loss: 27.20 grad norm: 65.91 time: 0.067
2026-01-08 11:06:32,612: Train batch 4800: loss: 21.61 grad norm: 62.53 time: 0.066
2026-01-08 11:06:50,503: Train batch 5000: loss: 38.93 grad norm: 87.26 time: 0.066
2026-01-08 11:06:50,503: Running test after training batch: 5000
2026-01-08 11:06:50,621: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:06:55,630: WER debug example
  GT : you can see the code at this point as well
  PR : cute kantz eats the tung helt hatt cysts point his swells
2026-01-08 11:06:55,652: WER debug example
  GT : how does it keep the cost down
  PR : toehold stutz hitz hix the teast
2026-01-08 11:06:56,993: Val batch 5000: PER (avg): 0.2953 CTC Loss (avg): 30.4170 WER(1gram): 98.73% (n=64) time: 6.490
2026-01-08 11:06:56,993: WER lens: avg_true_words=6.16 avg_pred_words=5.95 max_pred_words=11
2026-01-08 11:06:56,993: t15.2023.08.13 val PER: 0.2848
2026-01-08 11:06:56,994: t15.2023.08.18 val PER: 0.2699
2026-01-08 11:06:56,994: t15.2023.08.20 val PER: 0.2708
2026-01-08 11:06:56,994: t15.2023.08.25 val PER: 0.2244
2026-01-08 11:06:56,994: t15.2023.08.27 val PER: 0.3232
2026-01-08 11:06:56,994: t15.2023.09.01 val PER: 0.2403
2026-01-08 11:06:56,994: t15.2023.09.03 val PER: 0.3171
2026-01-08 11:06:56,994: t15.2023.09.24 val PER: 0.2451
2026-01-08 11:06:56,994: t15.2023.09.29 val PER: 0.2591
2026-01-08 11:06:56,994: t15.2023.10.01 val PER: 0.2979
2026-01-08 11:06:56,994: t15.2023.10.06 val PER: 0.2304
2026-01-08 11:06:56,994: t15.2023.10.08 val PER: 0.3843
2026-01-08 11:06:56,994: t15.2023.10.13 val PER: 0.3693
2026-01-08 11:06:56,995: t15.2023.10.15 val PER: 0.2861
2026-01-08 11:06:56,995: t15.2023.10.20 val PER: 0.2651
2026-01-08 11:06:56,995: t15.2023.10.22 val PER: 0.2327
2026-01-08 11:06:56,995: t15.2023.11.03 val PER: 0.2985
2026-01-08 11:06:56,995: t15.2023.11.04 val PER: 0.1229
2026-01-08 11:06:56,995: t15.2023.11.17 val PER: 0.1571
2026-01-08 11:06:56,996: t15.2023.11.19 val PER: 0.1637
2026-01-08 11:06:56,996: t15.2023.11.26 val PER: 0.3080
2026-01-08 11:06:56,996: t15.2023.12.03 val PER: 0.2700
2026-01-08 11:06:56,996: t15.2023.12.08 val PER: 0.2583
2026-01-08 11:06:56,996: t15.2023.12.10 val PER: 0.2339
2026-01-08 11:06:56,996: t15.2023.12.17 val PER: 0.2713
2026-01-08 11:06:56,996: t15.2023.12.29 val PER: 0.3095
2026-01-08 11:06:56,996: t15.2024.02.25 val PER: 0.2528
2026-01-08 11:06:56,996: t15.2024.03.08 val PER: 0.3741
2026-01-08 11:06:56,996: t15.2024.03.15 val PER: 0.3202
2026-01-08 11:06:56,996: t15.2024.03.17 val PER: 0.3096
2026-01-08 11:06:56,996: t15.2024.05.10 val PER: 0.2779
2026-01-08 11:06:56,996: t15.2024.06.14 val PER: 0.3060
2026-01-08 11:06:56,996: t15.2024.07.19 val PER: 0.3757
2026-01-08 11:06:56,996: t15.2024.07.21 val PER: 0.2172
2026-01-08 11:06:56,996: t15.2024.07.28 val PER: 0.2882
2026-01-08 11:06:56,996: t15.2025.01.10 val PER: 0.4463
2026-01-08 11:06:56,997: t15.2025.01.12 val PER: 0.3226
2026-01-08 11:06:56,997: t15.2025.03.14 val PER: 0.4467
2026-01-08 11:06:56,997: t15.2025.03.16 val PER: 0.3364
2026-01-08 11:06:56,997: t15.2025.03.30 val PER: 0.4276
2026-01-08 11:06:56,997: t15.2025.04.13 val PER: 0.3823
2026-01-08 11:07:14,227: Train batch 5200: loss: 23.51 grad norm: 64.90 time: 0.053
2026-01-08 11:07:31,163: Train batch 5400: loss: 25.42 grad norm: 68.71 time: 0.071
2026-01-08 11:07:39,926: Running test after training batch: 5500
2026-01-08 11:07:40,030: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:07:44,661: WER debug example
  GT : you can see the code at this point as well
  PR : euclea kant cetus the tuckett hatt cyst appoints his
2026-01-08 11:07:44,683: WER debug example
  GT : how does it keep the cost down
  PR : toehold sitz heap coos the tsetse
2026-01-08 11:07:46,050: Val batch 5500: PER (avg): 0.2865 CTC Loss (avg): 29.3429 WER(1gram): 99.49% (n=64) time: 6.124
2026-01-08 11:07:46,051: WER lens: avg_true_words=6.16 avg_pred_words=5.73 max_pred_words=12
2026-01-08 11:07:46,051: t15.2023.08.13 val PER: 0.2661
2026-01-08 11:07:46,051: t15.2023.08.18 val PER: 0.2607
2026-01-08 11:07:46,051: t15.2023.08.20 val PER: 0.2550
2026-01-08 11:07:46,051: t15.2023.08.25 val PER: 0.2274
2026-01-08 11:07:46,051: t15.2023.08.27 val PER: 0.3183
2026-01-08 11:07:46,051: t15.2023.09.01 val PER: 0.2321
2026-01-08 11:07:46,051: t15.2023.09.03 val PER: 0.3135
2026-01-08 11:07:46,052: t15.2023.09.24 val PER: 0.2415
2026-01-08 11:07:46,052: t15.2023.09.29 val PER: 0.2591
2026-01-08 11:07:46,052: t15.2023.10.01 val PER: 0.2939
2026-01-08 11:07:46,052: t15.2023.10.06 val PER: 0.2282
2026-01-08 11:07:46,052: t15.2023.10.08 val PER: 0.3775
2026-01-08 11:07:46,052: t15.2023.10.13 val PER: 0.3460
2026-01-08 11:07:46,052: t15.2023.10.15 val PER: 0.2828
2026-01-08 11:07:46,052: t15.2023.10.20 val PER: 0.2819
2026-01-08 11:07:46,052: t15.2023.10.22 val PER: 0.2283
2026-01-08 11:07:46,052: t15.2023.11.03 val PER: 0.3026
2026-01-08 11:07:46,052: t15.2023.11.04 val PER: 0.1399
2026-01-08 11:07:46,052: t15.2023.11.17 val PER: 0.1477
2026-01-08 11:07:46,052: t15.2023.11.19 val PER: 0.1697
2026-01-08 11:07:46,053: t15.2023.11.26 val PER: 0.3007
2026-01-08 11:07:46,053: t15.2023.12.03 val PER: 0.2637
2026-01-08 11:07:46,053: t15.2023.12.08 val PER: 0.2617
2026-01-08 11:07:46,053: t15.2023.12.10 val PER: 0.2181
2026-01-08 11:07:46,053: t15.2023.12.17 val PER: 0.2526
2026-01-08 11:07:46,053: t15.2023.12.29 val PER: 0.2924
2026-01-08 11:07:46,053: t15.2024.02.25 val PER: 0.2514
2026-01-08 11:07:46,053: t15.2024.03.08 val PER: 0.3471
2026-01-08 11:07:46,053: t15.2024.03.15 val PER: 0.3121
2026-01-08 11:07:46,053: t15.2024.03.17 val PER: 0.3013
2026-01-08 11:07:46,053: t15.2024.05.10 val PER: 0.2808
2026-01-08 11:07:46,053: t15.2024.06.14 val PER: 0.2965
2026-01-08 11:07:46,053: t15.2024.07.19 val PER: 0.3645
2026-01-08 11:07:46,053: t15.2024.07.21 val PER: 0.2062
2026-01-08 11:07:46,054: t15.2024.07.28 val PER: 0.2794
2026-01-08 11:07:46,054: t15.2025.01.10 val PER: 0.4298
2026-01-08 11:07:46,054: t15.2025.01.12 val PER: 0.2987
2026-01-08 11:07:46,054: t15.2025.03.14 val PER: 0.4186
2026-01-08 11:07:46,054: t15.2025.03.16 val PER: 0.3246
2026-01-08 11:07:46,054: t15.2025.03.30 val PER: 0.4218
2026-01-08 11:07:46,054: t15.2025.04.13 val PER: 0.3481
2026-01-08 11:07:55,336: Train batch 5600: loss: 25.78 grad norm: 73.41 time: 0.066
2026-01-08 11:08:12,981: Train batch 5800: loss: 19.66 grad norm: 67.97 time: 0.085
2026-01-08 11:08:31,013: Train batch 6000: loss: 19.75 grad norm: 57.32 time: 0.051
2026-01-08 11:08:31,014: Running test after training batch: 6000
2026-01-08 11:08:31,139: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:08:35,833: WER debug example
  GT : you can see the code at this point as well
  PR : utica deceits the tuckett hatt cyst appoint his
2026-01-08 11:08:35,856: WER debug example
  GT : how does it keep the cost down
  PR : toehold sta sitz keep coos the teast setzer
2026-01-08 11:08:37,182: Val batch 6000: PER (avg): 0.2772 CTC Loss (avg): 28.6611 WER(1gram): 99.49% (n=64) time: 6.168
2026-01-08 11:08:37,182: WER lens: avg_true_words=6.16 avg_pred_words=5.81 max_pred_words=11
2026-01-08 11:08:37,182: t15.2023.08.13 val PER: 0.2578
2026-01-08 11:08:37,183: t15.2023.08.18 val PER: 0.2607
2026-01-08 11:08:37,183: t15.2023.08.20 val PER: 0.2550
2026-01-08 11:08:37,183: t15.2023.08.25 val PER: 0.2319
2026-01-08 11:08:37,183: t15.2023.08.27 val PER: 0.3199
2026-01-08 11:08:37,183: t15.2023.09.01 val PER: 0.2232
2026-01-08 11:08:37,183: t15.2023.09.03 val PER: 0.3052
2026-01-08 11:08:37,183: t15.2023.09.24 val PER: 0.2269
2026-01-08 11:08:37,183: t15.2023.09.29 val PER: 0.2470
2026-01-08 11:08:37,183: t15.2023.10.01 val PER: 0.2820
2026-01-08 11:08:37,183: t15.2023.10.06 val PER: 0.2131
2026-01-08 11:08:37,183: t15.2023.10.08 val PER: 0.3613
2026-01-08 11:08:37,183: t15.2023.10.13 val PER: 0.3437
2026-01-08 11:08:37,183: t15.2023.10.15 val PER: 0.2854
2026-01-08 11:08:37,183: t15.2023.10.20 val PER: 0.2752
2026-01-08 11:08:37,185: t15.2023.10.22 val PER: 0.2238
2026-01-08 11:08:37,186: t15.2023.11.03 val PER: 0.2890
2026-01-08 11:08:37,186: t15.2023.11.04 val PER: 0.1468
2026-01-08 11:08:37,186: t15.2023.11.17 val PER: 0.1400
2026-01-08 11:08:37,186: t15.2023.11.19 val PER: 0.1577
2026-01-08 11:08:37,186: t15.2023.11.26 val PER: 0.2913
2026-01-08 11:08:37,186: t15.2023.12.03 val PER: 0.2584
2026-01-08 11:08:37,186: t15.2023.12.08 val PER: 0.2517
2026-01-08 11:08:37,186: t15.2023.12.10 val PER: 0.2116
2026-01-08 11:08:37,186: t15.2023.12.17 val PER: 0.2547
2026-01-08 11:08:37,186: t15.2023.12.29 val PER: 0.2642
2026-01-08 11:08:37,186: t15.2024.02.25 val PER: 0.2346
2026-01-08 11:08:37,186: t15.2024.03.08 val PER: 0.3371
2026-01-08 11:08:37,186: t15.2024.03.15 val PER: 0.2908
2026-01-08 11:08:37,187: t15.2024.03.17 val PER: 0.2880
2026-01-08 11:08:37,187: t15.2024.05.10 val PER: 0.2704
2026-01-08 11:08:37,187: t15.2024.06.14 val PER: 0.2713
2026-01-08 11:08:37,187: t15.2024.07.19 val PER: 0.3434
2026-01-08 11:08:37,187: t15.2024.07.21 val PER: 0.1966
2026-01-08 11:08:37,187: t15.2024.07.28 val PER: 0.2544
2026-01-08 11:08:37,187: t15.2025.01.10 val PER: 0.4105
2026-01-08 11:08:37,187: t15.2025.01.12 val PER: 0.3187
2026-01-08 11:08:37,187: t15.2025.03.14 val PER: 0.4172
2026-01-08 11:08:37,187: t15.2025.03.16 val PER: 0.3207
2026-01-08 11:08:37,187: t15.2025.03.30 val PER: 0.4115
2026-01-08 11:08:37,187: t15.2025.04.13 val PER: 0.3452
2026-01-08 11:08:55,217: Train batch 6200: loss: 22.34 grad norm: 65.97 time: 0.073
2026-01-08 11:09:13,347: Train batch 6400: loss: 27.02 grad norm: 77.35 time: 0.066
2026-01-08 11:09:22,011: Running test after training batch: 6500
2026-01-08 11:09:22,116: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:09:26,769: WER debug example
  GT : you can see the code at this point as well
  PR : euclea candies seats the tuckett gatt cysts points his swells
2026-01-08 11:09:26,794: WER debug example
  GT : how does it keep the cost down
  PR : shouse dusty hitz keep kerce the teast
2026-01-08 11:09:28,106: Val batch 6500: PER (avg): 0.2698 CTC Loss (avg): 28.0236 WER(1gram): 98.22% (n=64) time: 6.095
2026-01-08 11:09:28,107: WER lens: avg_true_words=6.16 avg_pred_words=5.75 max_pred_words=11
2026-01-08 11:09:28,107: t15.2023.08.13 val PER: 0.2453
2026-01-08 11:09:28,107: t15.2023.08.18 val PER: 0.2364
2026-01-08 11:09:28,107: t15.2023.08.20 val PER: 0.2264
2026-01-08 11:09:28,107: t15.2023.08.25 val PER: 0.2139
2026-01-08 11:09:28,107: t15.2023.08.27 val PER: 0.3167
2026-01-08 11:09:28,107: t15.2023.09.01 val PER: 0.2135
2026-01-08 11:09:28,107: t15.2023.09.03 val PER: 0.2886
2026-01-08 11:09:28,107: t15.2023.09.24 val PER: 0.2294
2026-01-08 11:09:28,108: t15.2023.09.29 val PER: 0.2374
2026-01-08 11:09:28,108: t15.2023.10.01 val PER: 0.2741
2026-01-08 11:09:28,108: t15.2023.10.06 val PER: 0.2078
2026-01-08 11:09:28,108: t15.2023.10.08 val PER: 0.3518
2026-01-08 11:09:28,108: t15.2023.10.13 val PER: 0.3382
2026-01-08 11:09:28,108: t15.2023.10.15 val PER: 0.2683
2026-01-08 11:09:28,108: t15.2023.10.20 val PER: 0.2785
2026-01-08 11:09:28,108: t15.2023.10.22 val PER: 0.2283
2026-01-08 11:09:28,108: t15.2023.11.03 val PER: 0.2890
2026-01-08 11:09:28,109: t15.2023.11.04 val PER: 0.1195
2026-01-08 11:09:28,109: t15.2023.11.17 val PER: 0.1431
2026-01-08 11:09:28,109: t15.2023.11.19 val PER: 0.1617
2026-01-08 11:09:28,109: t15.2023.11.26 val PER: 0.2812
2026-01-08 11:09:28,109: t15.2023.12.03 val PER: 0.2511
2026-01-08 11:09:28,109: t15.2023.12.08 val PER: 0.2463
2026-01-08 11:09:28,109: t15.2023.12.10 val PER: 0.2102
2026-01-08 11:09:28,109: t15.2023.12.17 val PER: 0.2391
2026-01-08 11:09:28,109: t15.2023.12.29 val PER: 0.2697
2026-01-08 11:09:28,109: t15.2024.02.25 val PER: 0.2079
2026-01-08 11:09:28,110: t15.2024.03.08 val PER: 0.3257
2026-01-08 11:09:28,110: t15.2024.03.15 val PER: 0.2989
2026-01-08 11:09:28,110: t15.2024.03.17 val PER: 0.2699
2026-01-08 11:09:28,110: t15.2024.05.10 val PER: 0.2897
2026-01-08 11:09:28,110: t15.2024.06.14 val PER: 0.2571
2026-01-08 11:09:28,110: t15.2024.07.19 val PER: 0.3415
2026-01-08 11:09:28,110: t15.2024.07.21 val PER: 0.1869
2026-01-08 11:09:28,110: t15.2024.07.28 val PER: 0.2544
2026-01-08 11:09:28,111: t15.2025.01.10 val PER: 0.4339
2026-01-08 11:09:28,111: t15.2025.01.12 val PER: 0.3079
2026-01-08 11:09:28,111: t15.2025.03.14 val PER: 0.4083
2026-01-08 11:09:28,111: t15.2025.03.16 val PER: 0.2984
2026-01-08 11:09:28,111: t15.2025.03.30 val PER: 0.4023
2026-01-08 11:09:28,111: t15.2025.04.13 val PER: 0.3452
2026-01-08 11:09:37,093: Train batch 6600: loss: 17.27 grad norm: 52.24 time: 0.047
2026-01-08 11:09:55,486: Train batch 6800: loss: 23.06 grad norm: 57.94 time: 0.050
2026-01-08 11:10:13,818: Train batch 7000: loss: 23.59 grad norm: 68.11 time: 0.066
2026-01-08 11:10:13,819: Running test after training batch: 7000
2026-01-08 11:10:13,964: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:10:18,618: WER debug example
  GT : you can see the code at this point as well
  PR : euclea canned sikhs the tuck diskette cysts points his swells
2026-01-08 11:10:18,642: WER debug example
  GT : how does it keep the cost down
  PR : cederholm stutz hitz kicks the tso tsetse
2026-01-08 11:10:19,979: Val batch 7000: PER (avg): 0.2623 CTC Loss (avg): 27.2998 WER(1gram): 97.21% (n=64) time: 6.160
2026-01-08 11:10:19,979: WER lens: avg_true_words=6.16 avg_pred_words=5.69 max_pred_words=11
2026-01-08 11:10:19,980: t15.2023.08.13 val PER: 0.2484
2026-01-08 11:10:19,980: t15.2023.08.18 val PER: 0.2263
2026-01-08 11:10:19,980: t15.2023.08.20 val PER: 0.2343
2026-01-08 11:10:19,980: t15.2023.08.25 val PER: 0.2154
2026-01-08 11:10:19,980: t15.2023.08.27 val PER: 0.3039
2026-01-08 11:10:19,980: t15.2023.09.01 val PER: 0.2029
2026-01-08 11:10:19,980: t15.2023.09.03 val PER: 0.2791
2026-01-08 11:10:19,981: t15.2023.09.24 val PER: 0.2221
2026-01-08 11:10:19,981: t15.2023.09.29 val PER: 0.2348
2026-01-08 11:10:19,981: t15.2023.10.01 val PER: 0.2629
2026-01-08 11:10:19,981: t15.2023.10.06 val PER: 0.2110
2026-01-08 11:10:19,981: t15.2023.10.08 val PER: 0.3545
2026-01-08 11:10:19,981: t15.2023.10.13 val PER: 0.3274
2026-01-08 11:10:19,981: t15.2023.10.15 val PER: 0.2577
2026-01-08 11:10:19,981: t15.2023.10.20 val PER: 0.2718
2026-01-08 11:10:19,981: t15.2023.10.22 val PER: 0.2105
2026-01-08 11:10:19,981: t15.2023.11.03 val PER: 0.2714
2026-01-08 11:10:19,981: t15.2023.11.04 val PER: 0.1195
2026-01-08 11:10:19,981: t15.2023.11.17 val PER: 0.1493
2026-01-08 11:10:19,981: t15.2023.11.19 val PER: 0.1497
2026-01-08 11:10:19,982: t15.2023.11.26 val PER: 0.2674
2026-01-08 11:10:19,982: t15.2023.12.03 val PER: 0.2521
2026-01-08 11:10:19,982: t15.2023.12.08 val PER: 0.2344
2026-01-08 11:10:19,982: t15.2023.12.10 val PER: 0.2011
2026-01-08 11:10:19,982: t15.2023.12.17 val PER: 0.2432
2026-01-08 11:10:19,982: t15.2023.12.29 val PER: 0.2588
2026-01-08 11:10:19,982: t15.2024.02.25 val PER: 0.2317
2026-01-08 11:10:19,982: t15.2024.03.08 val PER: 0.3158
2026-01-08 11:10:19,982: t15.2024.03.15 val PER: 0.2833
2026-01-08 11:10:19,982: t15.2024.03.17 val PER: 0.2734
2026-01-08 11:10:19,982: t15.2024.05.10 val PER: 0.2526
2026-01-08 11:10:19,982: t15.2024.06.14 val PER: 0.2650
2026-01-08 11:10:19,983: t15.2024.07.19 val PER: 0.3263
2026-01-08 11:10:19,983: t15.2024.07.21 val PER: 0.1834
2026-01-08 11:10:19,983: t15.2024.07.28 val PER: 0.2360
2026-01-08 11:10:19,983: t15.2025.01.10 val PER: 0.4022
2026-01-08 11:10:19,983: t15.2025.01.12 val PER: 0.2856
2026-01-08 11:10:19,983: t15.2025.03.14 val PER: 0.4112
2026-01-08 11:10:19,983: t15.2025.03.16 val PER: 0.2997
2026-01-08 11:10:19,983: t15.2025.03.30 val PER: 0.4034
2026-01-08 11:10:19,983: t15.2025.04.13 val PER: 0.3481
2026-01-08 11:10:37,490: Train batch 7200: loss: 21.35 grad norm: 66.20 time: 0.081
2026-01-08 11:10:54,668: Train batch 7400: loss: 20.94 grad norm: 61.75 time: 0.077
2026-01-08 11:11:03,214: Running test after training batch: 7500
2026-01-08 11:11:03,325: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:11:08,189: WER debug example
  GT : you can see the code at this point as well
  PR : euclea candies sikhs the colds scat cyst appointees his swells
2026-01-08 11:11:08,215: WER debug example
  GT : how does it keep the cost down
  PR : seed hymns stacy hitz heap coos thus costs
2026-01-08 11:11:09,638: Val batch 7500: PER (avg): 0.2627 CTC Loss (avg): 27.5053 WER(1gram): 99.75% (n=64) time: 6.424
2026-01-08 11:11:09,638: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-08 11:11:09,639: t15.2023.08.13 val PER: 0.2370
2026-01-08 11:11:09,639: t15.2023.08.18 val PER: 0.2330
2026-01-08 11:11:09,639: t15.2023.08.20 val PER: 0.2264
2026-01-08 11:11:09,639: t15.2023.08.25 val PER: 0.2184
2026-01-08 11:11:09,639: t15.2023.08.27 val PER: 0.3006
2026-01-08 11:11:09,639: t15.2023.09.01 val PER: 0.2110
2026-01-08 11:11:09,639: t15.2023.09.03 val PER: 0.2886
2026-01-08 11:11:09,639: t15.2023.09.24 val PER: 0.2379
2026-01-08 11:11:09,639: t15.2023.09.29 val PER: 0.2317
2026-01-08 11:11:09,639: t15.2023.10.01 val PER: 0.2668
2026-01-08 11:11:09,640: t15.2023.10.06 val PER: 0.2034
2026-01-08 11:11:09,640: t15.2023.10.08 val PER: 0.3532
2026-01-08 11:11:09,640: t15.2023.10.13 val PER: 0.3313
2026-01-08 11:11:09,640: t15.2023.10.15 val PER: 0.2544
2026-01-08 11:11:09,640: t15.2023.10.20 val PER: 0.2584
2026-01-08 11:11:09,640: t15.2023.10.22 val PER: 0.2105
2026-01-08 11:11:09,640: t15.2023.11.03 val PER: 0.2788
2026-01-08 11:11:09,640: t15.2023.11.04 val PER: 0.1263
2026-01-08 11:11:09,640: t15.2023.11.17 val PER: 0.1524
2026-01-08 11:11:09,640: t15.2023.11.19 val PER: 0.1457
2026-01-08 11:11:09,640: t15.2023.11.26 val PER: 0.2703
2026-01-08 11:11:09,640: t15.2023.12.03 val PER: 0.2384
2026-01-08 11:11:09,640: t15.2023.12.08 val PER: 0.2304
2026-01-08 11:11:09,640: t15.2023.12.10 val PER: 0.1945
2026-01-08 11:11:09,640: t15.2023.12.17 val PER: 0.2193
2026-01-08 11:11:09,641: t15.2023.12.29 val PER: 0.2560
2026-01-08 11:11:09,641: t15.2024.02.25 val PER: 0.2107
2026-01-08 11:11:09,641: t15.2024.03.08 val PER: 0.3343
2026-01-08 11:11:09,641: t15.2024.03.15 val PER: 0.2896
2026-01-08 11:11:09,641: t15.2024.03.17 val PER: 0.2615
2026-01-08 11:11:09,641: t15.2024.05.10 val PER: 0.2793
2026-01-08 11:11:09,641: t15.2024.06.14 val PER: 0.2666
2026-01-08 11:11:09,641: t15.2024.07.19 val PER: 0.3401
2026-01-08 11:11:09,641: t15.2024.07.21 val PER: 0.1828
2026-01-08 11:11:09,641: t15.2024.07.28 val PER: 0.2390
2026-01-08 11:11:09,641: t15.2025.01.10 val PER: 0.4008
2026-01-08 11:11:09,641: t15.2025.01.12 val PER: 0.2918
2026-01-08 11:11:09,641: t15.2025.03.14 val PER: 0.4053
2026-01-08 11:11:09,641: t15.2025.03.16 val PER: 0.3102
2026-01-08 11:11:09,642: t15.2025.03.30 val PER: 0.4069
2026-01-08 11:11:09,642: t15.2025.04.13 val PER: 0.3381
2026-01-08 11:11:18,379: Train batch 7600: loss: 22.22 grad norm: 62.77 time: 0.071
2026-01-08 11:11:35,271: Train batch 7800: loss: 22.31 grad norm: 67.65 time: 0.057
2026-01-08 11:11:52,719: Train batch 8000: loss: 19.31 grad norm: 55.70 time: 0.074
2026-01-08 11:11:52,719: Running test after training batch: 8000
2026-01-08 11:11:52,838: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:11:57,762: WER debug example
  GT : you can see the code at this point as well
  PR : euclea candies sikhs the helds gatz this appointees his
2026-01-08 11:11:57,786: WER debug example
  GT : how does it keep the cost down
  PR : cederholm decency hitz keep kerce thus costs
2026-01-08 11:11:59,110: Val batch 8000: PER (avg): 0.2498 CTC Loss (avg): 26.2861 WER(1gram): 96.95% (n=64) time: 6.391
2026-01-08 11:11:59,110: WER lens: avg_true_words=6.16 avg_pred_words=5.77 max_pred_words=12
2026-01-08 11:11:59,110: t15.2023.08.13 val PER: 0.2308
2026-01-08 11:11:59,110: t15.2023.08.18 val PER: 0.2196
2026-01-08 11:11:59,110: t15.2023.08.20 val PER: 0.2224
2026-01-08 11:11:59,110: t15.2023.08.25 val PER: 0.2289
2026-01-08 11:11:59,111: t15.2023.08.27 val PER: 0.3055
2026-01-08 11:11:59,111: t15.2023.09.01 val PER: 0.1851
2026-01-08 11:11:59,111: t15.2023.09.03 val PER: 0.2779
2026-01-08 11:11:59,111: t15.2023.09.24 val PER: 0.2209
2026-01-08 11:11:59,111: t15.2023.09.29 val PER: 0.2163
2026-01-08 11:11:59,111: t15.2023.10.01 val PER: 0.2609
2026-01-08 11:11:59,111: t15.2023.10.06 val PER: 0.2024
2026-01-08 11:11:59,111: t15.2023.10.08 val PER: 0.3410
2026-01-08 11:11:59,111: t15.2023.10.13 val PER: 0.3243
2026-01-08 11:11:59,111: t15.2023.10.15 val PER: 0.2544
2026-01-08 11:11:59,111: t15.2023.10.20 val PER: 0.2584
2026-01-08 11:11:59,111: t15.2023.10.22 val PER: 0.1982
2026-01-08 11:11:59,111: t15.2023.11.03 val PER: 0.2700
2026-01-08 11:11:59,111: t15.2023.11.04 val PER: 0.1195
2026-01-08 11:11:59,111: t15.2023.11.17 val PER: 0.1369
2026-01-08 11:11:59,112: t15.2023.11.19 val PER: 0.1457
2026-01-08 11:11:59,112: t15.2023.11.26 val PER: 0.2529
2026-01-08 11:11:59,112: t15.2023.12.03 val PER: 0.2258
2026-01-08 11:11:59,112: t15.2023.12.08 val PER: 0.2284
2026-01-08 11:11:59,112: t15.2023.12.10 val PER: 0.1892
2026-01-08 11:11:59,112: t15.2023.12.17 val PER: 0.2058
2026-01-08 11:11:59,112: t15.2023.12.29 val PER: 0.2471
2026-01-08 11:11:59,113: t15.2024.02.25 val PER: 0.1924
2026-01-08 11:11:59,113: t15.2024.03.08 val PER: 0.3073
2026-01-08 11:11:59,113: t15.2024.03.15 val PER: 0.2720
2026-01-08 11:11:59,113: t15.2024.03.17 val PER: 0.2406
2026-01-08 11:11:59,113: t15.2024.05.10 val PER: 0.2407
2026-01-08 11:11:59,113: t15.2024.06.14 val PER: 0.2445
2026-01-08 11:11:59,113: t15.2024.07.19 val PER: 0.3125
2026-01-08 11:11:59,113: t15.2024.07.21 val PER: 0.1566
2026-01-08 11:11:59,113: t15.2024.07.28 val PER: 0.2206
2026-01-08 11:11:59,113: t15.2025.01.10 val PER: 0.3994
2026-01-08 11:11:59,113: t15.2025.01.12 val PER: 0.2725
2026-01-08 11:11:59,113: t15.2025.03.14 val PER: 0.3950
2026-01-08 11:11:59,113: t15.2025.03.16 val PER: 0.3115
2026-01-08 11:11:59,113: t15.2025.03.30 val PER: 0.3828
2026-01-08 11:11:59,113: t15.2025.04.13 val PER: 0.3138
2026-01-08 11:11:59,117: New best val WER(1gram) 97.21% --> 96.95%
2026-01-08 11:11:59,117: Checkpointing model
2026-01-08 11:11:59,259: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_30k/checkpoint/best_checkpoint
2026-01-08 11:12:18,256: Train batch 8200: loss: 16.54 grad norm: 55.82 time: 0.059
2026-01-08 11:12:36,842: Train batch 8400: loss: 16.42 grad norm: 57.97 time: 0.068
2026-01-08 11:12:46,130: Running test after training batch: 8500
2026-01-08 11:12:46,234: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:12:51,106: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies sikhs the tuck dietz gatt cyst appointees his
2026-01-08 11:12:51,132: WER debug example
  GT : how does it keep the cost down
  PR : cederholm decency hitz keep kerce thus costs
2026-01-08 11:12:52,502: Val batch 8500: PER (avg): 0.2492 CTC Loss (avg): 25.6877 WER(1gram): 99.75% (n=64) time: 6.372
2026-01-08 11:12:52,503: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=12
2026-01-08 11:12:52,503: t15.2023.08.13 val PER: 0.2401
2026-01-08 11:12:52,503: t15.2023.08.18 val PER: 0.2297
2026-01-08 11:12:52,503: t15.2023.08.20 val PER: 0.2176
2026-01-08 11:12:52,503: t15.2023.08.25 val PER: 0.2259
2026-01-08 11:12:52,503: t15.2023.08.27 val PER: 0.3023
2026-01-08 11:12:52,503: t15.2023.09.01 val PER: 0.1940
2026-01-08 11:12:52,504: t15.2023.09.03 val PER: 0.2743
2026-01-08 11:12:52,504: t15.2023.09.24 val PER: 0.2233
2026-01-08 11:12:52,504: t15.2023.09.29 val PER: 0.2246
2026-01-08 11:12:52,504: t15.2023.10.01 val PER: 0.2483
2026-01-08 11:12:52,504: t15.2023.10.06 val PER: 0.2056
2026-01-08 11:12:52,504: t15.2023.10.08 val PER: 0.3383
2026-01-08 11:12:52,504: t15.2023.10.13 val PER: 0.3150
2026-01-08 11:12:52,504: t15.2023.10.15 val PER: 0.2558
2026-01-08 11:12:52,504: t15.2023.10.20 val PER: 0.2483
2026-01-08 11:12:52,504: t15.2023.10.22 val PER: 0.2127
2026-01-08 11:12:52,504: t15.2023.11.03 val PER: 0.2741
2026-01-08 11:12:52,504: t15.2023.11.04 val PER: 0.1126
2026-01-08 11:12:52,504: t15.2023.11.17 val PER: 0.1353
2026-01-08 11:12:52,504: t15.2023.11.19 val PER: 0.1357
2026-01-08 11:12:52,505: t15.2023.11.26 val PER: 0.2536
2026-01-08 11:12:52,505: t15.2023.12.03 val PER: 0.2164
2026-01-08 11:12:52,505: t15.2023.12.08 val PER: 0.2104
2026-01-08 11:12:52,505: t15.2023.12.10 val PER: 0.1879
2026-01-08 11:12:52,505: t15.2023.12.17 val PER: 0.2121
2026-01-08 11:12:52,505: t15.2023.12.29 val PER: 0.2423
2026-01-08 11:12:52,505: t15.2024.02.25 val PER: 0.2079
2026-01-08 11:12:52,505: t15.2024.03.08 val PER: 0.3058
2026-01-08 11:12:52,505: t15.2024.03.15 val PER: 0.2714
2026-01-08 11:12:52,505: t15.2024.03.17 val PER: 0.2490
2026-01-08 11:12:52,505: t15.2024.05.10 val PER: 0.2585
2026-01-08 11:12:52,505: t15.2024.06.14 val PER: 0.2445
2026-01-08 11:12:52,505: t15.2024.07.19 val PER: 0.3098
2026-01-08 11:12:52,505: t15.2024.07.21 val PER: 0.1586
2026-01-08 11:12:52,505: t15.2024.07.28 val PER: 0.2140
2026-01-08 11:12:52,506: t15.2025.01.10 val PER: 0.4008
2026-01-08 11:12:52,506: t15.2025.01.12 val PER: 0.2679
2026-01-08 11:12:52,506: t15.2025.03.14 val PER: 0.3935
2026-01-08 11:12:52,506: t15.2025.03.16 val PER: 0.2866
2026-01-08 11:12:52,506: t15.2025.03.30 val PER: 0.3805
2026-01-08 11:12:52,506: t15.2025.04.13 val PER: 0.3181
2026-01-08 11:13:01,513: Train batch 8600: loss: 25.05 grad norm: 64.93 time: 0.057
2026-01-08 11:13:19,563: Train batch 8800: loss: 22.20 grad norm: 70.71 time: 0.063
2026-01-08 11:13:38,776: Train batch 9000: loss: 24.26 grad norm: 70.35 time: 0.079
2026-01-08 11:13:38,777: Running test after training batch: 9000
2026-01-08 11:13:38,915: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:13:43,910: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies cetus the stuccoed hatt cyst appointees his
2026-01-08 11:13:43,934: WER debug example
  GT : how does it keep the cost down
  PR : cederholm dease dusty hitz heap kerce the teast setzer
2026-01-08 11:13:45,267: Val batch 9000: PER (avg): 0.2421 CTC Loss (avg): 25.2701 WER(1gram): 98.98% (n=64) time: 6.489
2026-01-08 11:13:45,267: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=12
2026-01-08 11:13:45,268: t15.2023.08.13 val PER: 0.2225
2026-01-08 11:13:45,268: t15.2023.08.18 val PER: 0.2087
2026-01-08 11:13:45,268: t15.2023.08.20 val PER: 0.2041
2026-01-08 11:13:45,268: t15.2023.08.25 val PER: 0.1988
2026-01-08 11:13:45,268: t15.2023.08.27 val PER: 0.2990
2026-01-08 11:13:45,268: t15.2023.09.01 val PER: 0.1859
2026-01-08 11:13:45,268: t15.2023.09.03 val PER: 0.2637
2026-01-08 11:13:45,268: t15.2023.09.24 val PER: 0.2269
2026-01-08 11:13:45,268: t15.2023.09.29 val PER: 0.2183
2026-01-08 11:13:45,268: t15.2023.10.01 val PER: 0.2536
2026-01-08 11:13:45,268: t15.2023.10.06 val PER: 0.1841
2026-01-08 11:13:45,269: t15.2023.10.08 val PER: 0.3491
2026-01-08 11:13:45,269: t15.2023.10.13 val PER: 0.3157
2026-01-08 11:13:45,269: t15.2023.10.15 val PER: 0.2498
2026-01-08 11:13:45,269: t15.2023.10.20 val PER: 0.2450
2026-01-08 11:13:45,269: t15.2023.10.22 val PER: 0.2071
2026-01-08 11:13:45,269: t15.2023.11.03 val PER: 0.2761
2026-01-08 11:13:45,269: t15.2023.11.04 val PER: 0.1160
2026-01-08 11:13:45,269: t15.2023.11.17 val PER: 0.1306
2026-01-08 11:13:45,270: t15.2023.11.19 val PER: 0.1437
2026-01-08 11:13:45,270: t15.2023.11.26 val PER: 0.2457
2026-01-08 11:13:45,270: t15.2023.12.03 val PER: 0.2111
2026-01-08 11:13:45,270: t15.2023.12.08 val PER: 0.2084
2026-01-08 11:13:45,270: t15.2023.12.10 val PER: 0.1840
2026-01-08 11:13:45,270: t15.2023.12.17 val PER: 0.2017
2026-01-08 11:13:45,270: t15.2023.12.29 val PER: 0.2423
2026-01-08 11:13:45,270: t15.2024.02.25 val PER: 0.1966
2026-01-08 11:13:45,270: t15.2024.03.08 val PER: 0.2973
2026-01-08 11:13:45,270: t15.2024.03.15 val PER: 0.2583
2026-01-08 11:13:45,270: t15.2024.03.17 val PER: 0.2357
2026-01-08 11:13:45,270: t15.2024.05.10 val PER: 0.2496
2026-01-08 11:13:45,270: t15.2024.06.14 val PER: 0.2303
2026-01-08 11:13:45,270: t15.2024.07.19 val PER: 0.2953
2026-01-08 11:13:45,270: t15.2024.07.21 val PER: 0.1510
2026-01-08 11:13:45,271: t15.2024.07.28 val PER: 0.2110
2026-01-08 11:13:45,271: t15.2025.01.10 val PER: 0.3898
2026-01-08 11:13:45,271: t15.2025.01.12 val PER: 0.2617
2026-01-08 11:13:45,271: t15.2025.03.14 val PER: 0.3728
2026-01-08 11:13:45,271: t15.2025.03.16 val PER: 0.2880
2026-01-08 11:13:45,271: t15.2025.03.30 val PER: 0.3724
2026-01-08 11:13:45,271: t15.2025.04.13 val PER: 0.3138
2026-01-08 11:14:04,444: Train batch 9200: loss: 17.38 grad norm: 59.11 time: 0.059
2026-01-08 11:14:23,347: Train batch 9400: loss: 14.90 grad norm: 55.35 time: 0.074
2026-01-08 11:14:32,737: Running test after training batch: 9500
2026-01-08 11:14:32,893: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:14:37,645: WER debug example
  GT : you can see the code at this point as well
  PR : euchre candies cetus the kantz gatz this appointees has
2026-01-08 11:14:37,669: WER debug example
  GT : how does it keep the cost down
  PR : derham decency hitz keep kerce the teast setzer
2026-01-08 11:14:39,072: Val batch 9500: PER (avg): 0.2378 CTC Loss (avg): 24.7633 WER(1gram): 96.95% (n=64) time: 6.335
2026-01-08 11:14:39,073: WER lens: avg_true_words=6.16 avg_pred_words=5.89 max_pred_words=11
2026-01-08 11:14:39,073: t15.2023.08.13 val PER: 0.2152
2026-01-08 11:14:39,073: t15.2023.08.18 val PER: 0.2070
2026-01-08 11:14:39,073: t15.2023.08.20 val PER: 0.2057
2026-01-08 11:14:39,073: t15.2023.08.25 val PER: 0.2048
2026-01-08 11:14:39,073: t15.2023.08.27 val PER: 0.2974
2026-01-08 11:14:39,073: t15.2023.09.01 val PER: 0.1761
2026-01-08 11:14:39,073: t15.2023.09.03 val PER: 0.2684
2026-01-08 11:14:39,073: t15.2023.09.24 val PER: 0.2136
2026-01-08 11:14:39,073: t15.2023.09.29 val PER: 0.2080
2026-01-08 11:14:39,074: t15.2023.10.01 val PER: 0.2464
2026-01-08 11:14:39,074: t15.2023.10.06 val PER: 0.1970
2026-01-08 11:14:39,074: t15.2023.10.08 val PER: 0.3248
2026-01-08 11:14:39,074: t15.2023.10.13 val PER: 0.3103
2026-01-08 11:14:39,074: t15.2023.10.15 val PER: 0.2340
2026-01-08 11:14:39,074: t15.2023.10.20 val PER: 0.2483
2026-01-08 11:14:39,074: t15.2023.10.22 val PER: 0.1826
2026-01-08 11:14:39,074: t15.2023.11.03 val PER: 0.2619
2026-01-08 11:14:39,074: t15.2023.11.04 val PER: 0.1024
2026-01-08 11:14:39,074: t15.2023.11.17 val PER: 0.1166
2026-01-08 11:14:39,075: t15.2023.11.19 val PER: 0.1317
2026-01-08 11:14:39,075: t15.2023.11.26 val PER: 0.2435
2026-01-08 11:14:39,075: t15.2023.12.03 val PER: 0.2290
2026-01-08 11:14:39,075: t15.2023.12.08 val PER: 0.1964
2026-01-08 11:14:39,075: t15.2023.12.10 val PER: 0.1761
2026-01-08 11:14:39,075: t15.2023.12.17 val PER: 0.1965
2026-01-08 11:14:39,075: t15.2023.12.29 val PER: 0.2327
2026-01-08 11:14:39,075: t15.2024.02.25 val PER: 0.2008
2026-01-08 11:14:39,075: t15.2024.03.08 val PER: 0.2973
2026-01-08 11:14:39,075: t15.2024.03.15 val PER: 0.2558
2026-01-08 11:14:39,075: t15.2024.03.17 val PER: 0.2329
2026-01-08 11:14:39,075: t15.2024.05.10 val PER: 0.2511
2026-01-08 11:14:39,076: t15.2024.06.14 val PER: 0.2240
2026-01-08 11:14:39,076: t15.2024.07.19 val PER: 0.3045
2026-01-08 11:14:39,076: t15.2024.07.21 val PER: 0.1628
2026-01-08 11:14:39,076: t15.2024.07.28 val PER: 0.2059
2026-01-08 11:14:39,076: t15.2025.01.10 val PER: 0.3705
2026-01-08 11:14:39,076: t15.2025.01.12 val PER: 0.2679
2026-01-08 11:14:39,076: t15.2025.03.14 val PER: 0.3728
2026-01-08 11:14:39,076: t15.2025.03.16 val PER: 0.2749
2026-01-08 11:14:39,077: t15.2025.03.30 val PER: 0.3851
2026-01-08 11:14:39,077: t15.2025.04.13 val PER: 0.2953
2026-01-08 11:14:47,410: Train batch 9600: loss: 15.29 grad norm: 83.55 time: 0.076
2026-01-08 11:15:04,672: Train batch 9800: loss: 16.21 grad norm: 61.98 time: 0.066
2026-01-08 11:15:22,453: Train batch 10000: loss: 10.63 grad norm: 46.71 time: 0.063
2026-01-08 11:15:22,453: Running test after training batch: 10000
2026-01-08 11:15:22,571: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:15:27,193: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies seats the cads gatz this appointees has
2026-01-08 11:15:27,216: WER debug example
  GT : how does it keep the cost down
  PR : derham dietz dusts hitz heaped thus costs
2026-01-08 11:15:28,556: Val batch 10000: PER (avg): 0.2334 CTC Loss (avg): 24.6265 WER(1gram): 96.70% (n=64) time: 6.102
2026-01-08 11:15:28,556: WER lens: avg_true_words=6.16 avg_pred_words=5.84 max_pred_words=11
2026-01-08 11:15:28,556: t15.2023.08.13 val PER: 0.2204
2026-01-08 11:15:28,556: t15.2023.08.18 val PER: 0.1928
2026-01-08 11:15:28,556: t15.2023.08.20 val PER: 0.2033
2026-01-08 11:15:28,556: t15.2023.08.25 val PER: 0.1973
2026-01-08 11:15:28,557: t15.2023.08.27 val PER: 0.3023
2026-01-08 11:15:28,557: t15.2023.09.01 val PER: 0.1737
2026-01-08 11:15:28,557: t15.2023.09.03 val PER: 0.2589
2026-01-08 11:15:28,557: t15.2023.09.24 val PER: 0.2100
2026-01-08 11:15:28,557: t15.2023.09.29 val PER: 0.1978
2026-01-08 11:15:28,557: t15.2023.10.01 val PER: 0.2345
2026-01-08 11:15:28,557: t15.2023.10.06 val PER: 0.2013
2026-01-08 11:15:28,557: t15.2023.10.08 val PER: 0.3288
2026-01-08 11:15:28,557: t15.2023.10.13 val PER: 0.3041
2026-01-08 11:15:28,557: t15.2023.10.15 val PER: 0.2274
2026-01-08 11:15:28,557: t15.2023.10.20 val PER: 0.2483
2026-01-08 11:15:28,557: t15.2023.10.22 val PER: 0.1915
2026-01-08 11:15:28,557: t15.2023.11.03 val PER: 0.2653
2026-01-08 11:15:28,557: t15.2023.11.04 val PER: 0.0853
2026-01-08 11:15:28,557: t15.2023.11.17 val PER: 0.1244
2026-01-08 11:15:28,557: t15.2023.11.19 val PER: 0.1297
2026-01-08 11:15:28,558: t15.2023.11.26 val PER: 0.2261
2026-01-08 11:15:28,558: t15.2023.12.03 val PER: 0.2017
2026-01-08 11:15:28,558: t15.2023.12.08 val PER: 0.1897
2026-01-08 11:15:28,558: t15.2023.12.10 val PER: 0.1669
2026-01-08 11:15:28,558: t15.2023.12.17 val PER: 0.2079
2026-01-08 11:15:28,558: t15.2023.12.29 val PER: 0.2313
2026-01-08 11:15:28,558: t15.2024.02.25 val PER: 0.1910
2026-01-08 11:15:28,558: t15.2024.03.08 val PER: 0.2959
2026-01-08 11:15:28,558: t15.2024.03.15 val PER: 0.2527
2026-01-08 11:15:28,558: t15.2024.03.17 val PER: 0.2259
2026-01-08 11:15:28,558: t15.2024.05.10 val PER: 0.2437
2026-01-08 11:15:28,558: t15.2024.06.14 val PER: 0.2192
2026-01-08 11:15:28,558: t15.2024.07.19 val PER: 0.3019
2026-01-08 11:15:28,559: t15.2024.07.21 val PER: 0.1566
2026-01-08 11:15:28,559: t15.2024.07.28 val PER: 0.2132
2026-01-08 11:15:28,559: t15.2025.01.10 val PER: 0.3691
2026-01-08 11:15:28,559: t15.2025.01.12 val PER: 0.2610
2026-01-08 11:15:28,559: t15.2025.03.14 val PER: 0.3698
2026-01-08 11:15:28,559: t15.2025.03.16 val PER: 0.2762
2026-01-08 11:15:28,559: t15.2025.03.30 val PER: 0.3678
2026-01-08 11:15:28,559: t15.2025.04.13 val PER: 0.2996
2026-01-08 11:15:28,561: New best val WER(1gram) 96.95% --> 96.70%
2026-01-08 11:15:28,561: Checkpointing model
2026-01-08 11:15:28,698: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_30k/checkpoint/best_checkpoint
2026-01-08 11:15:46,060: Train batch 10200: loss: 14.57 grad norm: 51.04 time: 0.051
2026-01-08 11:16:03,496: Train batch 10400: loss: 16.26 grad norm: 54.68 time: 0.074
2026-01-08 11:16:12,404: Running test after training batch: 10500
2026-01-08 11:16:12,524: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:16:17,198: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa canned seats the kent gatz this appointees has
2026-01-08 11:16:17,220: WER debug example
  GT : how does it keep the cost down
  PR : dowds stutz hitz keep coos thus tsetse
2026-01-08 11:16:18,589: Val batch 10500: PER (avg): 0.2259 CTC Loss (avg): 25.2043 WER(1gram): 95.43% (n=64) time: 6.184
2026-01-08 11:16:18,589: WER lens: avg_true_words=6.16 avg_pred_words=5.56 max_pred_words=11
2026-01-08 11:16:18,590: t15.2023.08.13 val PER: 0.2079
2026-01-08 11:16:18,590: t15.2023.08.18 val PER: 0.2028
2026-01-08 11:16:18,590: t15.2023.08.20 val PER: 0.1898
2026-01-08 11:16:18,590: t15.2023.08.25 val PER: 0.2018
2026-01-08 11:16:18,590: t15.2023.08.27 val PER: 0.2862
2026-01-08 11:16:18,590: t15.2023.09.01 val PER: 0.1672
2026-01-08 11:16:18,590: t15.2023.09.03 val PER: 0.2411
2026-01-08 11:16:18,590: t15.2023.09.24 val PER: 0.2015
2026-01-08 11:16:18,590: t15.2023.09.29 val PER: 0.1966
2026-01-08 11:16:18,590: t15.2023.10.01 val PER: 0.2404
2026-01-08 11:16:18,590: t15.2023.10.06 val PER: 0.1808
2026-01-08 11:16:18,590: t15.2023.10.08 val PER: 0.3234
2026-01-08 11:16:18,591: t15.2023.10.13 val PER: 0.2901
2026-01-08 11:16:18,591: t15.2023.10.15 val PER: 0.2287
2026-01-08 11:16:18,591: t15.2023.10.20 val PER: 0.2550
2026-01-08 11:16:18,591: t15.2023.10.22 val PER: 0.1849
2026-01-08 11:16:18,591: t15.2023.11.03 val PER: 0.2429
2026-01-08 11:16:18,591: t15.2023.11.04 val PER: 0.1024
2026-01-08 11:16:18,591: t15.2023.11.17 val PER: 0.1198
2026-01-08 11:16:18,591: t15.2023.11.19 val PER: 0.1118
2026-01-08 11:16:18,591: t15.2023.11.26 val PER: 0.2232
2026-01-08 11:16:18,591: t15.2023.12.03 val PER: 0.1943
2026-01-08 11:16:18,591: t15.2023.12.08 val PER: 0.1791
2026-01-08 11:16:18,591: t15.2023.12.10 val PER: 0.1564
2026-01-08 11:16:18,592: t15.2023.12.17 val PER: 0.1965
2026-01-08 11:16:18,592: t15.2023.12.29 val PER: 0.2258
2026-01-08 11:16:18,592: t15.2024.02.25 val PER: 0.1784
2026-01-08 11:16:18,592: t15.2024.03.08 val PER: 0.2817
2026-01-08 11:16:18,592: t15.2024.03.15 val PER: 0.2539
2026-01-08 11:16:18,592: t15.2024.03.17 val PER: 0.2245
2026-01-08 11:16:18,592: t15.2024.05.10 val PER: 0.2140
2026-01-08 11:16:18,592: t15.2024.06.14 val PER: 0.2192
2026-01-08 11:16:18,592: t15.2024.07.19 val PER: 0.2848
2026-01-08 11:16:18,592: t15.2024.07.21 val PER: 0.1372
2026-01-08 11:16:18,593: t15.2024.07.28 val PER: 0.1971
2026-01-08 11:16:18,593: t15.2025.01.10 val PER: 0.3788
2026-01-08 11:16:18,593: t15.2025.01.12 val PER: 0.2610
2026-01-08 11:16:18,593: t15.2025.03.14 val PER: 0.3713
2026-01-08 11:16:18,593: t15.2025.03.16 val PER: 0.2474
2026-01-08 11:16:18,593: t15.2025.03.30 val PER: 0.3736
2026-01-08 11:16:18,593: t15.2025.04.13 val PER: 0.2924
2026-01-08 11:16:18,594: New best val WER(1gram) 96.70% --> 95.43%
2026-01-08 11:16:18,594: Checkpointing model
2026-01-08 11:16:18,738: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_30k/checkpoint/best_checkpoint
2026-01-08 11:16:27,678: Train batch 10600: loss: 13.73 grad norm: 59.78 time: 0.075
2026-01-08 11:16:45,177: Train batch 10800: loss: 19.22 grad norm: 68.05 time: 0.068
2026-01-08 11:17:02,879: Train batch 11000: loss: 21.40 grad norm: 67.85 time: 0.059
2026-01-08 11:17:02,880: Running test after training batch: 11000
2026-01-08 11:17:02,976: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:17:07,798: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa canned seats the keds gatz this appointees has
2026-01-08 11:17:07,824: WER debug example
  GT : how does it keep the cost down
  PR : dowds sta sitz keeps thus costs
2026-01-08 11:17:09,219: Val batch 11000: PER (avg): 0.2226 CTC Loss (avg): 23.7571 WER(1gram): 94.92% (n=64) time: 6.340
2026-01-08 11:17:09,220: WER lens: avg_true_words=6.16 avg_pred_words=5.70 max_pred_words=10
2026-01-08 11:17:09,220: t15.2023.08.13 val PER: 0.2100
2026-01-08 11:17:09,220: t15.2023.08.18 val PER: 0.1928
2026-01-08 11:17:09,220: t15.2023.08.20 val PER: 0.1938
2026-01-08 11:17:09,220: t15.2023.08.25 val PER: 0.2018
2026-01-08 11:17:09,220: t15.2023.08.27 val PER: 0.2781
2026-01-08 11:17:09,220: t15.2023.09.01 val PER: 0.1599
2026-01-08 11:17:09,221: t15.2023.09.03 val PER: 0.2352
2026-01-08 11:17:09,221: t15.2023.09.24 val PER: 0.1942
2026-01-08 11:17:09,221: t15.2023.09.29 val PER: 0.2017
2026-01-08 11:17:09,221: t15.2023.10.01 val PER: 0.2318
2026-01-08 11:17:09,221: t15.2023.10.06 val PER: 0.1733
2026-01-08 11:17:09,221: t15.2023.10.08 val PER: 0.3221
2026-01-08 11:17:09,221: t15.2023.10.13 val PER: 0.2933
2026-01-08 11:17:09,221: t15.2023.10.15 val PER: 0.2208
2026-01-08 11:17:09,221: t15.2023.10.20 val PER: 0.2215
2026-01-08 11:17:09,221: t15.2023.10.22 val PER: 0.1682
2026-01-08 11:17:09,221: t15.2023.11.03 val PER: 0.2503
2026-01-08 11:17:09,221: t15.2023.11.04 val PER: 0.0887
2026-01-08 11:17:09,221: t15.2023.11.17 val PER: 0.1120
2026-01-08 11:17:09,221: t15.2023.11.19 val PER: 0.1158
2026-01-08 11:17:09,222: t15.2023.11.26 val PER: 0.2109
2026-01-08 11:17:09,222: t15.2023.12.03 val PER: 0.1996
2026-01-08 11:17:09,222: t15.2023.12.08 val PER: 0.1744
2026-01-08 11:17:09,222: t15.2023.12.10 val PER: 0.1537
2026-01-08 11:17:09,222: t15.2023.12.17 val PER: 0.1954
2026-01-08 11:17:09,222: t15.2023.12.29 val PER: 0.2183
2026-01-08 11:17:09,222: t15.2024.02.25 val PER: 0.1742
2026-01-08 11:17:09,222: t15.2024.03.08 val PER: 0.2731
2026-01-08 11:17:09,222: t15.2024.03.15 val PER: 0.2495
2026-01-08 11:17:09,222: t15.2024.03.17 val PER: 0.2183
2026-01-08 11:17:09,222: t15.2024.05.10 val PER: 0.2184
2026-01-08 11:17:09,222: t15.2024.06.14 val PER: 0.2271
2026-01-08 11:17:09,222: t15.2024.07.19 val PER: 0.2848
2026-01-08 11:17:09,222: t15.2024.07.21 val PER: 0.1400
2026-01-08 11:17:09,222: t15.2024.07.28 val PER: 0.1956
2026-01-08 11:17:09,222: t15.2025.01.10 val PER: 0.3829
2026-01-08 11:17:09,223: t15.2025.01.12 val PER: 0.2540
2026-01-08 11:17:09,223: t15.2025.03.14 val PER: 0.3817
2026-01-08 11:17:09,223: t15.2025.03.16 val PER: 0.2618
2026-01-08 11:17:09,223: t15.2025.03.30 val PER: 0.3517
2026-01-08 11:17:09,223: t15.2025.04.13 val PER: 0.2782
2026-01-08 11:17:09,224: New best val WER(1gram) 95.43% --> 94.92%
2026-01-08 11:17:09,224: Checkpointing model
2026-01-08 11:17:09,367: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_30k/checkpoint/best_checkpoint
2026-01-08 11:17:27,762: Train batch 11200: loss: 15.68 grad norm: 64.61 time: 0.073
2026-01-08 11:17:44,975: Train batch 11400: loss: 14.74 grad norm: 56.75 time: 0.058
2026-01-08 11:17:53,702: Running test after training batch: 11500
2026-01-08 11:17:53,801: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:17:58,511: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies cetus the kundert gatz this appointees his
2026-01-08 11:17:58,534: WER debug example
  GT : how does it keep the cost down
  PR : modus hitz heaped thus cost
2026-01-08 11:17:59,912: Val batch 11500: PER (avg): 0.2172 CTC Loss (avg): 22.8333 WER(1gram): 93.91% (n=64) time: 6.210
2026-01-08 11:17:59,912: WER lens: avg_true_words=6.16 avg_pred_words=5.45 max_pred_words=11
2026-01-08 11:17:59,913: t15.2023.08.13 val PER: 0.2027
2026-01-08 11:17:59,913: t15.2023.08.18 val PER: 0.1861
2026-01-08 11:17:59,913: t15.2023.08.20 val PER: 0.1827
2026-01-08 11:17:59,913: t15.2023.08.25 val PER: 0.1928
2026-01-08 11:17:59,913: t15.2023.08.27 val PER: 0.2749
2026-01-08 11:17:59,913: t15.2023.09.01 val PER: 0.1567
2026-01-08 11:17:59,913: t15.2023.09.03 val PER: 0.2387
2026-01-08 11:17:59,913: t15.2023.09.24 val PER: 0.1917
2026-01-08 11:17:59,913: t15.2023.09.29 val PER: 0.1934
2026-01-08 11:17:59,914: t15.2023.10.01 val PER: 0.2259
2026-01-08 11:17:59,914: t15.2023.10.06 val PER: 0.1701
2026-01-08 11:17:59,914: t15.2023.10.08 val PER: 0.3126
2026-01-08 11:17:59,914: t15.2023.10.13 val PER: 0.2886
2026-01-08 11:17:59,914: t15.2023.10.15 val PER: 0.2254
2026-01-08 11:17:59,914: t15.2023.10.20 val PER: 0.2114
2026-01-08 11:17:59,914: t15.2023.10.22 val PER: 0.1815
2026-01-08 11:17:59,914: t15.2023.11.03 val PER: 0.2449
2026-01-08 11:17:59,914: t15.2023.11.04 val PER: 0.0853
2026-01-08 11:17:59,914: t15.2023.11.17 val PER: 0.1058
2026-01-08 11:17:59,914: t15.2023.11.19 val PER: 0.1118
2026-01-08 11:17:59,914: t15.2023.11.26 val PER: 0.2159
2026-01-08 11:17:59,914: t15.2023.12.03 val PER: 0.1838
2026-01-08 11:17:59,914: t15.2023.12.08 val PER: 0.1718
2026-01-08 11:17:59,914: t15.2023.12.10 val PER: 0.1485
2026-01-08 11:17:59,915: t15.2023.12.17 val PER: 0.1840
2026-01-08 11:17:59,915: t15.2023.12.29 val PER: 0.2114
2026-01-08 11:17:59,915: t15.2024.02.25 val PER: 0.1868
2026-01-08 11:17:59,915: t15.2024.03.08 val PER: 0.2774
2026-01-08 11:17:59,915: t15.2024.03.15 val PER: 0.2383
2026-01-08 11:17:59,915: t15.2024.03.17 val PER: 0.2134
2026-01-08 11:17:59,915: t15.2024.05.10 val PER: 0.2199
2026-01-08 11:17:59,915: t15.2024.06.14 val PER: 0.2019
2026-01-08 11:17:59,915: t15.2024.07.19 val PER: 0.2788
2026-01-08 11:17:59,915: t15.2024.07.21 val PER: 0.1366
2026-01-08 11:17:59,916: t15.2024.07.28 val PER: 0.1971
2026-01-08 11:17:59,916: t15.2025.01.10 val PER: 0.3567
2026-01-08 11:17:59,916: t15.2025.01.12 val PER: 0.2471
2026-01-08 11:17:59,916: t15.2025.03.14 val PER: 0.3432
2026-01-08 11:17:59,916: t15.2025.03.16 val PER: 0.2618
2026-01-08 11:17:59,916: t15.2025.03.30 val PER: 0.3368
2026-01-08 11:17:59,916: t15.2025.04.13 val PER: 0.2796
2026-01-08 11:17:59,917: New best val WER(1gram) 94.92% --> 93.91%
2026-01-08 11:17:59,917: Checkpointing model
2026-01-08 11:18:00,059: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_30k/checkpoint/best_checkpoint
2026-01-08 11:18:08,402: Train batch 11600: loss: 17.27 grad norm: 63.61 time: 0.063
2026-01-08 11:18:26,239: Train batch 11800: loss: 13.76 grad norm: 56.87 time: 0.047
2026-01-08 11:18:44,056: Train batch 12000: loss: 18.56 grad norm: 61.47 time: 0.074
2026-01-08 11:18:44,057: Running test after training batch: 12000
2026-01-08 11:18:44,150: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:18:48,776: WER debug example
  GT : you can see the code at this point as well
  PR : likened cetus the colds hatz this appointees has
2026-01-08 11:18:48,805: WER debug example
  GT : how does it keep the cost down
  PR : miesse dusty hitz hepker thus cost
2026-01-08 11:18:50,203: Val batch 12000: PER (avg): 0.2018 CTC Loss (avg): 21.1795 WER(1gram): 96.45% (n=64) time: 6.146
2026-01-08 11:18:50,204: WER lens: avg_true_words=6.16 avg_pred_words=5.61 max_pred_words=12
2026-01-08 11:18:50,204: t15.2023.08.13 val PER: 0.1830
2026-01-08 11:18:50,204: t15.2023.08.18 val PER: 0.1727
2026-01-08 11:18:50,204: t15.2023.08.20 val PER: 0.1604
2026-01-08 11:18:50,204: t15.2023.08.25 val PER: 0.1611
2026-01-08 11:18:50,204: t15.2023.08.27 val PER: 0.2460
2026-01-08 11:18:50,204: t15.2023.09.01 val PER: 0.1461
2026-01-08 11:18:50,204: t15.2023.09.03 val PER: 0.2221
2026-01-08 11:18:50,204: t15.2023.09.24 val PER: 0.1711
2026-01-08 11:18:50,204: t15.2023.09.29 val PER: 0.1717
2026-01-08 11:18:50,205: t15.2023.10.01 val PER: 0.2147
2026-01-08 11:18:50,205: t15.2023.10.06 val PER: 0.1615
2026-01-08 11:18:50,205: t15.2023.10.08 val PER: 0.2950
2026-01-08 11:18:50,205: t15.2023.10.13 val PER: 0.2583
2026-01-08 11:18:50,205: t15.2023.10.15 val PER: 0.1971
2026-01-08 11:18:50,205: t15.2023.10.20 val PER: 0.2081
2026-01-08 11:18:50,205: t15.2023.10.22 val PER: 0.1771
2026-01-08 11:18:50,205: t15.2023.11.03 val PER: 0.2280
2026-01-08 11:18:50,205: t15.2023.11.04 val PER: 0.0922
2026-01-08 11:18:50,205: t15.2023.11.17 val PER: 0.0855
2026-01-08 11:18:50,205: t15.2023.11.19 val PER: 0.0978
2026-01-08 11:18:50,205: t15.2023.11.26 val PER: 0.1993
2026-01-08 11:18:50,205: t15.2023.12.03 val PER: 0.1607
2026-01-08 11:18:50,205: t15.2023.12.08 val PER: 0.1525
2026-01-08 11:18:50,206: t15.2023.12.10 val PER: 0.1288
2026-01-08 11:18:50,206: t15.2023.12.17 val PER: 0.1830
2026-01-08 11:18:50,206: t15.2023.12.29 val PER: 0.1915
2026-01-08 11:18:50,206: t15.2024.02.25 val PER: 0.1699
2026-01-08 11:18:50,206: t15.2024.03.08 val PER: 0.2589
2026-01-08 11:18:50,206: t15.2024.03.15 val PER: 0.2339
2026-01-08 11:18:50,206: t15.2024.03.17 val PER: 0.1960
2026-01-08 11:18:50,206: t15.2024.05.10 val PER: 0.2051
2026-01-08 11:18:50,206: t15.2024.06.14 val PER: 0.2035
2026-01-08 11:18:50,206: t15.2024.07.19 val PER: 0.2670
2026-01-08 11:18:50,207: t15.2024.07.21 val PER: 0.1331
2026-01-08 11:18:50,207: t15.2024.07.28 val PER: 0.1875
2026-01-08 11:18:50,207: t15.2025.01.10 val PER: 0.3430
2026-01-08 11:18:50,207: t15.2025.01.12 val PER: 0.2086
2026-01-08 11:18:50,207: t15.2025.03.14 val PER: 0.3506
2026-01-08 11:18:50,207: t15.2025.03.16 val PER: 0.2526
2026-01-08 11:18:50,207: t15.2025.03.30 val PER: 0.3437
2026-01-08 11:18:50,207: t15.2025.04.13 val PER: 0.2611
2026-01-08 11:19:06,977: Train batch 12200: loss: 10.93 grad norm: 50.93 time: 0.068
2026-01-08 11:19:24,261: Train batch 12400: loss: 8.74 grad norm: 42.73 time: 0.042
2026-01-08 11:19:33,246: Running test after training batch: 12500
2026-01-08 11:19:33,385: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:19:38,327: WER debug example
  GT : you can see the code at this point as well
  PR : likened cetus the kundert gatz this appointees his
2026-01-08 11:19:38,353: WER debug example
  GT : how does it keep the cost down
  PR : dease dusty hitty hepker thus cost
2026-01-08 11:19:39,804: Val batch 12500: PER (avg): 0.2000 CTC Loss (avg): 21.1413 WER(1gram): 95.69% (n=64) time: 6.557
2026-01-08 11:19:39,804: WER lens: avg_true_words=6.16 avg_pred_words=5.62 max_pred_words=12
2026-01-08 11:19:39,804: t15.2023.08.13 val PER: 0.1830
2026-01-08 11:19:39,804: t15.2023.08.18 val PER: 0.1710
2026-01-08 11:19:39,804: t15.2023.08.20 val PER: 0.1469
2026-01-08 11:19:39,804: t15.2023.08.25 val PER: 0.1642
2026-01-08 11:19:39,804: t15.2023.08.27 val PER: 0.2379
2026-01-08 11:19:39,805: t15.2023.09.01 val PER: 0.1453
2026-01-08 11:19:39,805: t15.2023.09.03 val PER: 0.2257
2026-01-08 11:19:39,805: t15.2023.09.24 val PER: 0.1687
2026-01-08 11:19:39,805: t15.2023.09.29 val PER: 0.1710
2026-01-08 11:19:39,805: t15.2023.10.01 val PER: 0.2094
2026-01-08 11:19:39,805: t15.2023.10.06 val PER: 0.1561
2026-01-08 11:19:39,805: t15.2023.10.08 val PER: 0.2936
2026-01-08 11:19:39,805: t15.2023.10.13 val PER: 0.2521
2026-01-08 11:19:39,805: t15.2023.10.15 val PER: 0.2044
2026-01-08 11:19:39,805: t15.2023.10.20 val PER: 0.2047
2026-01-08 11:19:39,805: t15.2023.10.22 val PER: 0.1514
2026-01-08 11:19:39,805: t15.2023.11.03 val PER: 0.2327
2026-01-08 11:19:39,805: t15.2023.11.04 val PER: 0.0819
2026-01-08 11:19:39,805: t15.2023.11.17 val PER: 0.0918
2026-01-08 11:19:39,806: t15.2023.11.19 val PER: 0.1018
2026-01-08 11:19:39,806: t15.2023.11.26 val PER: 0.1732
2026-01-08 11:19:39,806: t15.2023.12.03 val PER: 0.1670
2026-01-08 11:19:39,806: t15.2023.12.08 val PER: 0.1478
2026-01-08 11:19:39,806: t15.2023.12.10 val PER: 0.1275
2026-01-08 11:19:39,806: t15.2023.12.17 val PER: 0.1726
2026-01-08 11:19:39,806: t15.2023.12.29 val PER: 0.1784
2026-01-08 11:19:39,806: t15.2024.02.25 val PER: 0.1671
2026-01-08 11:19:39,806: t15.2024.03.08 val PER: 0.2632
2026-01-08 11:19:39,806: t15.2024.03.15 val PER: 0.2427
2026-01-08 11:19:39,806: t15.2024.03.17 val PER: 0.1987
2026-01-08 11:19:39,806: t15.2024.05.10 val PER: 0.2080
2026-01-08 11:19:39,806: t15.2024.06.14 val PER: 0.2240
2026-01-08 11:19:39,806: t15.2024.07.19 val PER: 0.2709
2026-01-08 11:19:39,806: t15.2024.07.21 val PER: 0.1317
2026-01-08 11:19:39,806: t15.2024.07.28 val PER: 0.1853
2026-01-08 11:19:39,806: t15.2025.01.10 val PER: 0.3416
2026-01-08 11:19:39,807: t15.2025.01.12 val PER: 0.2163
2026-01-08 11:19:39,807: t15.2025.03.14 val PER: 0.3624
2026-01-08 11:19:39,807: t15.2025.03.16 val PER: 0.2408
2026-01-08 11:19:39,807: t15.2025.03.30 val PER: 0.3299
2026-01-08 11:19:39,807: t15.2025.04.13 val PER: 0.2739
2026-01-08 11:19:49,002: Train batch 12600: loss: 11.52 grad norm: 51.35 time: 0.060
2026-01-08 11:20:07,384: Train batch 12800: loss: 9.04 grad norm: 39.83 time: 0.054
2026-01-08 11:20:25,835: Train batch 13000: loss: 10.17 grad norm: 49.58 time: 0.068
2026-01-08 11:20:25,835: Running test after training batch: 13000
2026-01-08 11:20:25,944: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:20:30,583: WER debug example
  GT : you can see the code at this point as well
  PR : likened cetus the kundert gatz this appointees has
2026-01-08 11:20:30,608: WER debug example
  GT : how does it keep the cost down
  PR : miesse dusty hitz kieper thus costs
2026-01-08 11:20:32,020: Val batch 13000: PER (avg): 0.1957 CTC Loss (avg): 20.7460 WER(1gram): 95.43% (n=64) time: 6.185
2026-01-08 11:20:32,020: WER lens: avg_true_words=6.16 avg_pred_words=5.48 max_pred_words=11
2026-01-08 11:20:32,021: t15.2023.08.13 val PER: 0.1726
2026-01-08 11:20:32,021: t15.2023.08.18 val PER: 0.1794
2026-01-08 11:20:32,021: t15.2023.08.20 val PER: 0.1477
2026-01-08 11:20:32,021: t15.2023.08.25 val PER: 0.1627
2026-01-08 11:20:32,021: t15.2023.08.27 val PER: 0.2492
2026-01-08 11:20:32,021: t15.2023.09.01 val PER: 0.1356
2026-01-08 11:20:32,021: t15.2023.09.03 val PER: 0.2078
2026-01-08 11:20:32,021: t15.2023.09.24 val PER: 0.1723
2026-01-08 11:20:32,021: t15.2023.09.29 val PER: 0.1698
2026-01-08 11:20:32,021: t15.2023.10.01 val PER: 0.2114
2026-01-08 11:20:32,021: t15.2023.10.06 val PER: 0.1421
2026-01-08 11:20:32,021: t15.2023.10.08 val PER: 0.2882
2026-01-08 11:20:32,021: t15.2023.10.13 val PER: 0.2607
2026-01-08 11:20:32,021: t15.2023.10.15 val PER: 0.1938
2026-01-08 11:20:32,022: t15.2023.10.20 val PER: 0.2081
2026-01-08 11:20:32,022: t15.2023.10.22 val PER: 0.1581
2026-01-08 11:20:32,022: t15.2023.11.03 val PER: 0.2320
2026-01-08 11:20:32,022: t15.2023.11.04 val PER: 0.0785
2026-01-08 11:20:32,022: t15.2023.11.17 val PER: 0.0855
2026-01-08 11:20:32,022: t15.2023.11.19 val PER: 0.0958
2026-01-08 11:20:32,022: t15.2023.11.26 val PER: 0.1783
2026-01-08 11:20:32,022: t15.2023.12.03 val PER: 0.1639
2026-01-08 11:20:32,022: t15.2023.12.08 val PER: 0.1325
2026-01-08 11:20:32,022: t15.2023.12.10 val PER: 0.1275
2026-01-08 11:20:32,022: t15.2023.12.17 val PER: 0.1881
2026-01-08 11:20:32,022: t15.2023.12.29 val PER: 0.1839
2026-01-08 11:20:32,022: t15.2024.02.25 val PER: 0.1559
2026-01-08 11:20:32,022: t15.2024.03.08 val PER: 0.2532
2026-01-08 11:20:32,023: t15.2024.03.15 val PER: 0.2339
2026-01-08 11:20:32,023: t15.2024.03.17 val PER: 0.1890
2026-01-08 11:20:32,023: t15.2024.05.10 val PER: 0.2021
2026-01-08 11:20:32,023: t15.2024.06.14 val PER: 0.1924
2026-01-08 11:20:32,023: t15.2024.07.19 val PER: 0.2610
2026-01-08 11:20:32,023: t15.2024.07.21 val PER: 0.1241
2026-01-08 11:20:32,023: t15.2024.07.28 val PER: 0.1647
2026-01-08 11:20:32,023: t15.2025.01.10 val PER: 0.3430
2026-01-08 11:20:32,023: t15.2025.01.12 val PER: 0.2071
2026-01-08 11:20:32,023: t15.2025.03.14 val PER: 0.3639
2026-01-08 11:20:32,023: t15.2025.03.16 val PER: 0.2369
2026-01-08 11:20:32,024: t15.2025.03.30 val PER: 0.3184
2026-01-08 11:20:32,024: t15.2025.04.13 val PER: 0.2767
2026-01-08 11:20:50,423: Train batch 13200: loss: 16.95 grad norm: 70.40 time: 0.056
2026-01-08 11:21:08,579: Train batch 13400: loss: 13.81 grad norm: 79.24 time: 0.064
2026-01-08 11:21:17,660: Running test after training batch: 13500
2026-01-08 11:21:17,776: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:21:22,614: WER debug example
  GT : you can see the code at this point as well
  PR : pyke antecedents the candies gatz this appointees has
2026-01-08 11:21:22,641: WER debug example
  GT : how does it keep the cost down
  PR : miesse dusty hitty hepker thus costs dentzer
2026-01-08 11:21:24,076: Val batch 13500: PER (avg): 0.1893 CTC Loss (avg): 20.1104 WER(1gram): 98.98% (n=64) time: 6.416
2026-01-08 11:21:24,077: WER lens: avg_true_words=6.16 avg_pred_words=5.81 max_pred_words=11
2026-01-08 11:21:24,077: t15.2023.08.13 val PER: 0.1601
2026-01-08 11:21:24,077: t15.2023.08.18 val PER: 0.1618
2026-01-08 11:21:24,077: t15.2023.08.20 val PER: 0.1493
2026-01-08 11:21:24,077: t15.2023.08.25 val PER: 0.1657
2026-01-08 11:21:24,077: t15.2023.08.27 val PER: 0.2315
2026-01-08 11:21:24,077: t15.2023.09.01 val PER: 0.1388
2026-01-08 11:21:24,077: t15.2023.09.03 val PER: 0.2007
2026-01-08 11:21:24,078: t15.2023.09.24 val PER: 0.1626
2026-01-08 11:21:24,078: t15.2023.09.29 val PER: 0.1621
2026-01-08 11:21:24,078: t15.2023.10.01 val PER: 0.2100
2026-01-08 11:21:24,078: t15.2023.10.06 val PER: 0.1464
2026-01-08 11:21:24,078: t15.2023.10.08 val PER: 0.2774
2026-01-08 11:21:24,078: t15.2023.10.13 val PER: 0.2459
2026-01-08 11:21:24,078: t15.2023.10.15 val PER: 0.1951
2026-01-08 11:21:24,078: t15.2023.10.20 val PER: 0.1980
2026-01-08 11:21:24,078: t15.2023.10.22 val PER: 0.1604
2026-01-08 11:21:24,078: t15.2023.11.03 val PER: 0.2280
2026-01-08 11:21:24,078: t15.2023.11.04 val PER: 0.0717
2026-01-08 11:21:24,078: t15.2023.11.17 val PER: 0.0778
2026-01-08 11:21:24,078: t15.2023.11.19 val PER: 0.0898
2026-01-08 11:21:24,079: t15.2023.11.26 val PER: 0.1623
2026-01-08 11:21:24,079: t15.2023.12.03 val PER: 0.1481
2026-01-08 11:21:24,079: t15.2023.12.08 val PER: 0.1298
2026-01-08 11:21:24,079: t15.2023.12.10 val PER: 0.1209
2026-01-08 11:21:24,079: t15.2023.12.17 val PER: 0.1549
2026-01-08 11:21:24,079: t15.2023.12.29 val PER: 0.1750
2026-01-08 11:21:24,079: t15.2024.02.25 val PER: 0.1531
2026-01-08 11:21:24,079: t15.2024.03.08 val PER: 0.2390
2026-01-08 11:21:24,079: t15.2024.03.15 val PER: 0.2295
2026-01-08 11:21:24,079: t15.2024.03.17 val PER: 0.1904
2026-01-08 11:21:24,079: t15.2024.05.10 val PER: 0.1887
2026-01-08 11:21:24,079: t15.2024.06.14 val PER: 0.2098
2026-01-08 11:21:24,080: t15.2024.07.19 val PER: 0.2525
2026-01-08 11:21:24,080: t15.2024.07.21 val PER: 0.1193
2026-01-08 11:21:24,080: t15.2024.07.28 val PER: 0.1706
2026-01-08 11:21:24,080: t15.2025.01.10 val PER: 0.3457
2026-01-08 11:21:24,080: t15.2025.01.12 val PER: 0.1971
2026-01-08 11:21:24,080: t15.2025.03.14 val PER: 0.3491
2026-01-08 11:21:24,080: t15.2025.03.16 val PER: 0.2382
2026-01-08 11:21:24,080: t15.2025.03.30 val PER: 0.3103
2026-01-08 11:21:24,081: t15.2025.04.13 val PER: 0.2496
2026-01-08 11:21:32,910: Train batch 13600: loss: 16.40 grad norm: 70.93 time: 0.063
2026-01-08 11:21:50,072: Train batch 13800: loss: 10.70 grad norm: 52.91 time: 0.059
2026-01-08 11:22:07,709: Train batch 14000: loss: 15.16 grad norm: 64.01 time: 0.052
2026-01-08 11:22:07,710: Running test after training batch: 14000
2026-01-08 11:22:07,811: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:22:12,407: WER debug example
  GT : you can see the code at this point as well
  PR : dyke ands cetus the colds gatz this appointees has
2026-01-08 11:22:12,435: WER debug example
  GT : how does it keep the cost down
  PR : moses dusty hitt heap coos thus costs tends
2026-01-08 11:22:13,865: Val batch 14000: PER (avg): 0.1931 CTC Loss (avg): 20.4269 WER(1gram): 95.43% (n=64) time: 6.156
2026-01-08 11:22:13,866: WER lens: avg_true_words=6.16 avg_pred_words=5.69 max_pred_words=12
2026-01-08 11:22:13,866: t15.2023.08.13 val PER: 0.1736
2026-01-08 11:22:13,866: t15.2023.08.18 val PER: 0.1517
2026-01-08 11:22:13,866: t15.2023.08.20 val PER: 0.1461
2026-01-08 11:22:13,866: t15.2023.08.25 val PER: 0.1521
2026-01-08 11:22:13,866: t15.2023.08.27 val PER: 0.2363
2026-01-08 11:22:13,866: t15.2023.09.01 val PER: 0.1445
2026-01-08 11:22:13,866: t15.2023.09.03 val PER: 0.2067
2026-01-08 11:22:13,866: t15.2023.09.24 val PER: 0.1578
2026-01-08 11:22:13,867: t15.2023.09.29 val PER: 0.1608
2026-01-08 11:22:13,867: t15.2023.10.01 val PER: 0.2094
2026-01-08 11:22:13,867: t15.2023.10.06 val PER: 0.1421
2026-01-08 11:22:13,867: t15.2023.10.08 val PER: 0.2747
2026-01-08 11:22:13,867: t15.2023.10.13 val PER: 0.2483
2026-01-08 11:22:13,867: t15.2023.10.15 val PER: 0.1912
2026-01-08 11:22:13,867: t15.2023.10.20 val PER: 0.2114
2026-01-08 11:22:13,867: t15.2023.10.22 val PER: 0.1637
2026-01-08 11:22:13,867: t15.2023.11.03 val PER: 0.2300
2026-01-08 11:22:13,867: t15.2023.11.04 val PER: 0.0819
2026-01-08 11:22:13,867: t15.2023.11.17 val PER: 0.0949
2026-01-08 11:22:13,867: t15.2023.11.19 val PER: 0.1038
2026-01-08 11:22:13,868: t15.2023.11.26 val PER: 0.1696
2026-01-08 11:22:13,868: t15.2023.12.03 val PER: 0.1670
2026-01-08 11:22:13,868: t15.2023.12.08 val PER: 0.1451
2026-01-08 11:22:13,868: t15.2023.12.10 val PER: 0.1327
2026-01-08 11:22:13,868: t15.2023.12.17 val PER: 0.1705
2026-01-08 11:22:13,868: t15.2023.12.29 val PER: 0.1757
2026-01-08 11:22:13,868: t15.2024.02.25 val PER: 0.1615
2026-01-08 11:22:13,868: t15.2024.03.08 val PER: 0.2632
2026-01-08 11:22:13,868: t15.2024.03.15 val PER: 0.2270
2026-01-08 11:22:13,868: t15.2024.03.17 val PER: 0.1974
2026-01-08 11:22:13,868: t15.2024.05.10 val PER: 0.1976
2026-01-08 11:22:13,868: t15.2024.06.14 val PER: 0.2019
2026-01-08 11:22:13,868: t15.2024.07.19 val PER: 0.2591
2026-01-08 11:22:13,868: t15.2024.07.21 val PER: 0.1241
2026-01-08 11:22:13,869: t15.2024.07.28 val PER: 0.1618
2026-01-08 11:22:13,869: t15.2025.01.10 val PER: 0.3361
2026-01-08 11:22:13,869: t15.2025.01.12 val PER: 0.2094
2026-01-08 11:22:13,869: t15.2025.03.14 val PER: 0.3609
2026-01-08 11:22:13,869: t15.2025.03.16 val PER: 0.2291
2026-01-08 11:22:13,869: t15.2025.03.30 val PER: 0.3322
2026-01-08 11:22:13,869: t15.2025.04.13 val PER: 0.2496
2026-01-08 11:22:30,892: Train batch 14200: loss: 11.22 grad norm: 52.14 time: 0.058
2026-01-08 11:22:48,372: Train batch 14400: loss: 9.29 grad norm: 53.54 time: 0.067
2026-01-08 11:22:57,426: Running test after training batch: 14500
2026-01-08 11:22:57,522: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:23:02,138: WER debug example
  GT : you can see the code at this point as well
  PR : likened siess the kundert gatz this appointees has
2026-01-08 11:23:02,163: WER debug example
  GT : how does it keep the cost down
  PR : miesse dusty hitz kieper thus costs dentzer
2026-01-08 11:23:03,563: Val batch 14500: PER (avg): 0.1850 CTC Loss (avg): 19.5643 WER(1gram): 97.21% (n=64) time: 6.137
2026-01-08 11:23:03,564: WER lens: avg_true_words=6.16 avg_pred_words=5.86 max_pred_words=12
2026-01-08 11:23:03,564: t15.2023.08.13 val PER: 0.1674
2026-01-08 11:23:03,564: t15.2023.08.18 val PER: 0.1609
2026-01-08 11:23:03,564: t15.2023.08.20 val PER: 0.1366
2026-01-08 11:23:03,564: t15.2023.08.25 val PER: 0.1476
2026-01-08 11:23:03,564: t15.2023.08.27 val PER: 0.2235
2026-01-08 11:23:03,564: t15.2023.09.01 val PER: 0.1299
2026-01-08 11:23:03,565: t15.2023.09.03 val PER: 0.2078
2026-01-08 11:23:03,565: t15.2023.09.24 val PER: 0.1517
2026-01-08 11:23:03,565: t15.2023.09.29 val PER: 0.1583
2026-01-08 11:23:03,565: t15.2023.10.01 val PER: 0.1995
2026-01-08 11:23:03,565: t15.2023.10.06 val PER: 0.1313
2026-01-08 11:23:03,565: t15.2023.10.08 val PER: 0.2666
2026-01-08 11:23:03,565: t15.2023.10.13 val PER: 0.2428
2026-01-08 11:23:03,565: t15.2023.10.15 val PER: 0.1879
2026-01-08 11:23:03,565: t15.2023.10.20 val PER: 0.1913
2026-01-08 11:23:03,565: t15.2023.10.22 val PER: 0.1570
2026-01-08 11:23:03,565: t15.2023.11.03 val PER: 0.2225
2026-01-08 11:23:03,565: t15.2023.11.04 val PER: 0.0922
2026-01-08 11:23:03,565: t15.2023.11.17 val PER: 0.0793
2026-01-08 11:23:03,565: t15.2023.11.19 val PER: 0.0938
2026-01-08 11:23:03,566: t15.2023.11.26 val PER: 0.1464
2026-01-08 11:23:03,566: t15.2023.12.03 val PER: 0.1397
2026-01-08 11:23:03,566: t15.2023.12.08 val PER: 0.1265
2026-01-08 11:23:03,566: t15.2023.12.10 val PER: 0.1183
2026-01-08 11:23:03,566: t15.2023.12.17 val PER: 0.1705
2026-01-08 11:23:03,566: t15.2023.12.29 val PER: 0.1709
2026-01-08 11:23:03,566: t15.2024.02.25 val PER: 0.1643
2026-01-08 11:23:03,566: t15.2024.03.08 val PER: 0.2589
2026-01-08 11:23:03,566: t15.2024.03.15 val PER: 0.2364
2026-01-08 11:23:03,566: t15.2024.03.17 val PER: 0.1834
2026-01-08 11:23:03,566: t15.2024.05.10 val PER: 0.1783
2026-01-08 11:23:03,566: t15.2024.06.14 val PER: 0.1956
2026-01-08 11:23:03,566: t15.2024.07.19 val PER: 0.2531
2026-01-08 11:23:03,566: t15.2024.07.21 val PER: 0.1200
2026-01-08 11:23:03,566: t15.2024.07.28 val PER: 0.1529
2026-01-08 11:23:03,566: t15.2025.01.10 val PER: 0.3306
2026-01-08 11:23:03,566: t15.2025.01.12 val PER: 0.1925
2026-01-08 11:23:03,567: t15.2025.03.14 val PER: 0.3476
2026-01-08 11:23:03,567: t15.2025.03.16 val PER: 0.2160
2026-01-08 11:23:03,567: t15.2025.03.30 val PER: 0.3172
2026-01-08 11:23:03,567: t15.2025.04.13 val PER: 0.2496
2026-01-08 11:23:12,397: Train batch 14600: loss: 14.84 grad norm: 76.41 time: 0.060
2026-01-08 11:23:29,978: Train batch 14800: loss: 6.43 grad norm: 48.60 time: 0.052
2026-01-08 11:23:48,223: Train batch 15000: loss: 7.51 grad norm: 50.73 time: 0.054
2026-01-08 11:23:48,223: Running test after training batch: 15000
2026-01-08 11:23:48,353: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:23:53,016: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide canned seed the kundert gatz this pointers has
2026-01-08 11:23:53,045: WER debug example
  GT : how does it keep the cost down
  PR : ouzts dusty it kieper thus cussed
2026-01-08 11:23:54,521: Val batch 15000: PER (avg): 0.1538 CTC Loss (avg): 16.8206 WER(1gram): 90.61% (n=64) time: 6.298
2026-01-08 11:23:54,522: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=11
2026-01-08 11:23:54,522: t15.2023.08.13 val PER: 0.1279
2026-01-08 11:23:54,522: t15.2023.08.18 val PER: 0.1174
2026-01-08 11:23:54,522: t15.2023.08.20 val PER: 0.1025
2026-01-08 11:23:54,522: t15.2023.08.25 val PER: 0.1024
2026-01-08 11:23:54,522: t15.2023.08.27 val PER: 0.2058
2026-01-08 11:23:54,522: t15.2023.09.01 val PER: 0.0820
2026-01-08 11:23:54,522: t15.2023.09.03 val PER: 0.1580
2026-01-08 11:23:54,522: t15.2023.09.24 val PER: 0.1274
2026-01-08 11:23:54,522: t15.2023.09.29 val PER: 0.1289
2026-01-08 11:23:54,522: t15.2023.10.01 val PER: 0.1724
2026-01-08 11:23:54,522: t15.2023.10.06 val PER: 0.0936
2026-01-08 11:23:54,522: t15.2023.10.08 val PER: 0.2463
2026-01-08 11:23:54,523: t15.2023.10.13 val PER: 0.2133
2026-01-08 11:23:54,523: t15.2023.10.15 val PER: 0.1457
2026-01-08 11:23:54,523: t15.2023.10.20 val PER: 0.1711
2026-01-08 11:23:54,523: t15.2023.10.22 val PER: 0.1236
2026-01-08 11:23:54,523: t15.2023.11.03 val PER: 0.1906
2026-01-08 11:23:54,523: t15.2023.11.04 val PER: 0.0273
2026-01-08 11:23:54,523: t15.2023.11.17 val PER: 0.0358
2026-01-08 11:23:54,523: t15.2023.11.19 val PER: 0.0419
2026-01-08 11:23:54,524: t15.2023.11.26 val PER: 0.1188
2026-01-08 11:23:54,524: t15.2023.12.03 val PER: 0.1061
2026-01-08 11:23:54,524: t15.2023.12.08 val PER: 0.1052
2026-01-08 11:23:54,524: t15.2023.12.10 val PER: 0.0841
2026-01-08 11:23:54,524: t15.2023.12.17 val PER: 0.1216
2026-01-08 11:23:54,524: t15.2023.12.29 val PER: 0.1434
2026-01-08 11:23:54,524: t15.2024.02.25 val PER: 0.1166
2026-01-08 11:23:54,524: t15.2024.03.08 val PER: 0.2176
2026-01-08 11:23:54,524: t15.2024.03.15 val PER: 0.2001
2026-01-08 11:23:54,524: t15.2024.03.17 val PER: 0.1374
2026-01-08 11:23:54,524: t15.2024.05.10 val PER: 0.1530
2026-01-08 11:23:54,524: t15.2024.06.14 val PER: 0.1798
2026-01-08 11:23:54,524: t15.2024.07.19 val PER: 0.2446
2026-01-08 11:23:54,524: t15.2024.07.21 val PER: 0.0979
2026-01-08 11:23:54,524: t15.2024.07.28 val PER: 0.1471
2026-01-08 11:23:54,524: t15.2025.01.10 val PER: 0.3099
2026-01-08 11:23:54,524: t15.2025.01.12 val PER: 0.1632
2026-01-08 11:23:54,525: t15.2025.03.14 val PER: 0.3669
2026-01-08 11:23:54,525: t15.2025.03.16 val PER: 0.1806
2026-01-08 11:23:54,525: t15.2025.03.30 val PER: 0.2816
2026-01-08 11:23:54,525: t15.2025.04.13 val PER: 0.2183
2026-01-08 11:23:54,526: New best val WER(1gram) 93.91% --> 90.61%
2026-01-08 11:23:54,526: Checkpointing model
2026-01-08 11:23:54,666: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_30k/checkpoint/best_checkpoint
2026-01-08 11:24:13,345: Train batch 15200: loss: 5.34 grad norm: 43.80 time: 0.061
2026-01-08 11:24:30,892: Train batch 15400: loss: 12.74 grad norm: 65.84 time: 0.051
2026-01-08 11:24:39,537: Running test after training batch: 15500
2026-01-08 11:24:39,632: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:24:44,239: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide canned seed the candies gatz this appointees has
2026-01-08 11:24:44,269: WER debug example
  GT : how does it keep the cost down
  PR : hounds dusty hitt kieper thus costs dentzer
2026-01-08 11:24:45,818: Val batch 15500: PER (avg): 0.1497 CTC Loss (avg): 16.2412 WER(1gram): 90.61% (n=64) time: 6.281
2026-01-08 11:24:45,819: WER lens: avg_true_words=6.16 avg_pred_words=5.89 max_pred_words=12
2026-01-08 11:24:45,819: t15.2023.08.13 val PER: 0.1237
2026-01-08 11:24:45,819: t15.2023.08.18 val PER: 0.1190
2026-01-08 11:24:45,819: t15.2023.08.20 val PER: 0.1096
2026-01-08 11:24:45,819: t15.2023.08.25 val PER: 0.0964
2026-01-08 11:24:45,819: t15.2023.08.27 val PER: 0.1865
2026-01-08 11:24:45,819: t15.2023.09.01 val PER: 0.0787
2026-01-08 11:24:45,819: t15.2023.09.03 val PER: 0.1520
2026-01-08 11:24:45,819: t15.2023.09.24 val PER: 0.1117
2026-01-08 11:24:45,820: t15.2023.09.29 val PER: 0.1225
2026-01-08 11:24:45,820: t15.2023.10.01 val PER: 0.1671
2026-01-08 11:24:45,820: t15.2023.10.06 val PER: 0.0958
2026-01-08 11:24:45,820: t15.2023.10.08 val PER: 0.2422
2026-01-08 11:24:45,820: t15.2023.10.13 val PER: 0.2033
2026-01-08 11:24:45,820: t15.2023.10.15 val PER: 0.1543
2026-01-08 11:24:45,820: t15.2023.10.20 val PER: 0.1913
2026-01-08 11:24:45,820: t15.2023.10.22 val PER: 0.1180
2026-01-08 11:24:45,821: t15.2023.11.03 val PER: 0.1920
2026-01-08 11:24:45,821: t15.2023.11.04 val PER: 0.0341
2026-01-08 11:24:45,821: t15.2023.11.17 val PER: 0.0373
2026-01-08 11:24:45,821: t15.2023.11.19 val PER: 0.0399
2026-01-08 11:24:45,821: t15.2023.11.26 val PER: 0.1123
2026-01-08 11:24:45,821: t15.2023.12.03 val PER: 0.0977
2026-01-08 11:24:45,821: t15.2023.12.08 val PER: 0.0999
2026-01-08 11:24:45,821: t15.2023.12.10 val PER: 0.0657
2026-01-08 11:24:45,821: t15.2023.12.17 val PER: 0.1185
2026-01-08 11:24:45,821: t15.2023.12.29 val PER: 0.1338
2026-01-08 11:24:45,821: t15.2024.02.25 val PER: 0.1222
2026-01-08 11:24:45,821: t15.2024.03.08 val PER: 0.2077
2026-01-08 11:24:45,821: t15.2024.03.15 val PER: 0.1976
2026-01-08 11:24:45,822: t15.2024.03.17 val PER: 0.1423
2026-01-08 11:24:45,822: t15.2024.05.10 val PER: 0.1590
2026-01-08 11:24:45,822: t15.2024.06.14 val PER: 0.1751
2026-01-08 11:24:45,822: t15.2024.07.19 val PER: 0.2320
2026-01-08 11:24:45,822: t15.2024.07.21 val PER: 0.0966
2026-01-08 11:24:45,822: t15.2024.07.28 val PER: 0.1412
2026-01-08 11:24:45,822: t15.2025.01.10 val PER: 0.3072
2026-01-08 11:24:45,822: t15.2025.01.12 val PER: 0.1524
2026-01-08 11:24:45,822: t15.2025.03.14 val PER: 0.3417
2026-01-08 11:24:45,822: t15.2025.03.16 val PER: 0.1754
2026-01-08 11:24:45,822: t15.2025.03.30 val PER: 0.2897
2026-01-08 11:24:45,822: t15.2025.04.13 val PER: 0.2040
2026-01-08 11:24:55,029: Train batch 15600: loss: 12.68 grad norm: 66.30 time: 0.065
2026-01-08 11:25:13,342: Train batch 15800: loss: 13.58 grad norm: 70.39 time: 0.070
2026-01-08 11:25:31,835: Train batch 16000: loss: 8.76 grad norm: 60.45 time: 0.057
2026-01-08 11:25:31,836: Running test after training batch: 16000
2026-01-08 11:25:31,932: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:25:36,758: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the kundert gatz this appointees has
2026-01-08 11:25:36,788: WER debug example
  GT : how does it keep the cost down
  PR : hounds dusty hitt kieper thus costs dentzer
2026-01-08 11:25:38,383: Val batch 16000: PER (avg): 0.1499 CTC Loss (avg): 16.1122 WER(1gram): 90.10% (n=64) time: 6.547
2026-01-08 11:25:38,383: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=13
2026-01-08 11:25:38,383: t15.2023.08.13 val PER: 0.1268
2026-01-08 11:25:38,383: t15.2023.08.18 val PER: 0.1132
2026-01-08 11:25:38,384: t15.2023.08.20 val PER: 0.0985
2026-01-08 11:25:38,384: t15.2023.08.25 val PER: 0.1099
2026-01-08 11:25:38,384: t15.2023.08.27 val PER: 0.1785
2026-01-08 11:25:38,384: t15.2023.09.01 val PER: 0.0747
2026-01-08 11:25:38,384: t15.2023.09.03 val PER: 0.1508
2026-01-08 11:25:38,384: t15.2023.09.24 val PER: 0.1092
2026-01-08 11:25:38,384: t15.2023.09.29 val PER: 0.1257
2026-01-08 11:25:38,385: t15.2023.10.01 val PER: 0.1704
2026-01-08 11:25:38,385: t15.2023.10.06 val PER: 0.1023
2026-01-08 11:25:38,385: t15.2023.10.08 val PER: 0.2517
2026-01-08 11:25:38,385: t15.2023.10.13 val PER: 0.1971
2026-01-08 11:25:38,385: t15.2023.10.15 val PER: 0.1503
2026-01-08 11:25:38,385: t15.2023.10.20 val PER: 0.1745
2026-01-08 11:25:38,385: t15.2023.10.22 val PER: 0.1136
2026-01-08 11:25:38,385: t15.2023.11.03 val PER: 0.1859
2026-01-08 11:25:38,385: t15.2023.11.04 val PER: 0.0307
2026-01-08 11:25:38,385: t15.2023.11.17 val PER: 0.0342
2026-01-08 11:25:38,385: t15.2023.11.19 val PER: 0.0419
2026-01-08 11:25:38,385: t15.2023.11.26 val PER: 0.1123
2026-01-08 11:25:38,385: t15.2023.12.03 val PER: 0.1050
2026-01-08 11:25:38,385: t15.2023.12.08 val PER: 0.0959
2026-01-08 11:25:38,386: t15.2023.12.10 val PER: 0.0775
2026-01-08 11:25:38,386: t15.2023.12.17 val PER: 0.1310
2026-01-08 11:25:38,386: t15.2023.12.29 val PER: 0.1325
2026-01-08 11:25:38,386: t15.2024.02.25 val PER: 0.1194
2026-01-08 11:25:38,386: t15.2024.03.08 val PER: 0.2205
2026-01-08 11:25:38,386: t15.2024.03.15 val PER: 0.1989
2026-01-08 11:25:38,387: t15.2024.03.17 val PER: 0.1562
2026-01-08 11:25:38,387: t15.2024.05.10 val PER: 0.1560
2026-01-08 11:25:38,387: t15.2024.06.14 val PER: 0.1688
2026-01-08 11:25:38,387: t15.2024.07.19 val PER: 0.2367
2026-01-08 11:25:38,387: t15.2024.07.21 val PER: 0.0945
2026-01-08 11:25:38,387: t15.2024.07.28 val PER: 0.1419
2026-01-08 11:25:38,387: t15.2025.01.10 val PER: 0.3085
2026-01-08 11:25:38,388: t15.2025.01.12 val PER: 0.1601
2026-01-08 11:25:38,388: t15.2025.03.14 val PER: 0.3136
2026-01-08 11:25:38,388: t15.2025.03.16 val PER: 0.1819
2026-01-08 11:25:38,388: t15.2025.03.30 val PER: 0.2736
2026-01-08 11:25:38,388: t15.2025.04.13 val PER: 0.2183
2026-01-08 11:25:38,388: New best val WER(1gram) 90.61% --> 90.10%
2026-01-08 11:25:38,388: Checkpointing model
2026-01-08 11:25:38,533: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_30k/checkpoint/best_checkpoint
2026-01-08 11:25:57,052: Train batch 16200: loss: 5.40 grad norm: 50.08 time: 0.058
2026-01-08 11:26:14,538: Train batch 16400: loss: 10.96 grad norm: 65.61 time: 0.059
2026-01-08 11:26:23,123: Running test after training batch: 16500
2026-01-08 11:26:23,242: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:26:27,916: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the candies gatz this pointers has
2026-01-08 11:26:27,947: WER debug example
  GT : how does it keep the cost down
  PR : hounds dusty itty kieper thus cost setzer
2026-01-08 11:26:29,593: Val batch 16500: PER (avg): 0.1455 CTC Loss (avg): 15.6154 WER(1gram): 90.10% (n=64) time: 6.469
2026-01-08 11:26:29,593: WER lens: avg_true_words=6.16 avg_pred_words=6.00 max_pred_words=14
2026-01-08 11:26:29,593: t15.2023.08.13 val PER: 0.1227
2026-01-08 11:26:29,593: t15.2023.08.18 val PER: 0.1106
2026-01-08 11:26:29,594: t15.2023.08.20 val PER: 0.1072
2026-01-08 11:26:29,594: t15.2023.08.25 val PER: 0.0979
2026-01-08 11:26:29,594: t15.2023.08.27 val PER: 0.1929
2026-01-08 11:26:29,594: t15.2023.09.01 val PER: 0.0763
2026-01-08 11:26:29,594: t15.2023.09.03 val PER: 0.1556
2026-01-08 11:26:29,594: t15.2023.09.24 val PER: 0.1238
2026-01-08 11:26:29,594: t15.2023.09.29 val PER: 0.1244
2026-01-08 11:26:29,594: t15.2023.10.01 val PER: 0.1704
2026-01-08 11:26:29,594: t15.2023.10.06 val PER: 0.0926
2026-01-08 11:26:29,594: t15.2023.10.08 val PER: 0.2314
2026-01-08 11:26:29,594: t15.2023.10.13 val PER: 0.1978
2026-01-08 11:26:29,594: t15.2023.10.15 val PER: 0.1549
2026-01-08 11:26:29,594: t15.2023.10.20 val PER: 0.1611
2026-01-08 11:26:29,595: t15.2023.10.22 val PER: 0.1069
2026-01-08 11:26:29,595: t15.2023.11.03 val PER: 0.1859
2026-01-08 11:26:29,595: t15.2023.11.04 val PER: 0.0273
2026-01-08 11:26:29,595: t15.2023.11.17 val PER: 0.0358
2026-01-08 11:26:29,595: t15.2023.11.19 val PER: 0.0299
2026-01-08 11:26:29,595: t15.2023.11.26 val PER: 0.1000
2026-01-08 11:26:29,595: t15.2023.12.03 val PER: 0.1019
2026-01-08 11:26:29,595: t15.2023.12.08 val PER: 0.0859
2026-01-08 11:26:29,595: t15.2023.12.10 val PER: 0.0788
2026-01-08 11:26:29,595: t15.2023.12.17 val PER: 0.1279
2026-01-08 11:26:29,595: t15.2023.12.29 val PER: 0.1290
2026-01-08 11:26:29,595: t15.2024.02.25 val PER: 0.1180
2026-01-08 11:26:29,595: t15.2024.03.08 val PER: 0.1920
2026-01-08 11:26:29,595: t15.2024.03.15 val PER: 0.1964
2026-01-08 11:26:29,595: t15.2024.03.17 val PER: 0.1374
2026-01-08 11:26:29,596: t15.2024.05.10 val PER: 0.1590
2026-01-08 11:26:29,596: t15.2024.06.14 val PER: 0.1688
2026-01-08 11:26:29,596: t15.2024.07.19 val PER: 0.2235
2026-01-08 11:26:29,596: t15.2024.07.21 val PER: 0.0924
2026-01-08 11:26:29,596: t15.2024.07.28 val PER: 0.1316
2026-01-08 11:26:29,596: t15.2025.01.10 val PER: 0.2893
2026-01-08 11:26:29,596: t15.2025.01.12 val PER: 0.1540
2026-01-08 11:26:29,596: t15.2025.03.14 val PER: 0.3210
2026-01-08 11:26:29,596: t15.2025.03.16 val PER: 0.1846
2026-01-08 11:26:29,596: t15.2025.03.30 val PER: 0.2598
2026-01-08 11:26:29,597: t15.2025.04.13 val PER: 0.1954
2026-01-08 11:26:41,332: Train batch 16600: loss: 9.05 grad norm: 59.20 time: 0.054
2026-01-08 11:26:58,785: Train batch 16800: loss: 14.32 grad norm: 73.55 time: 0.063
2026-01-08 11:27:16,172: Train batch 17000: loss: 6.77 grad norm: 50.39 time: 0.084
2026-01-08 11:27:16,173: Running test after training batch: 17000
2026-01-08 11:27:16,286: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:27:21,646: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the candies gatz this tutt pointers has
2026-01-08 11:27:21,677: WER debug example
  GT : how does it keep the cost down
  PR : hounds dusty it kieper thus costs dentzer
2026-01-08 11:27:23,296: Val batch 17000: PER (avg): 0.1449 CTC Loss (avg): 15.4633 WER(1gram): 90.10% (n=64) time: 7.123
2026-01-08 11:27:23,296: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=12
2026-01-08 11:27:23,296: t15.2023.08.13 val PER: 0.1143
2026-01-08 11:27:23,296: t15.2023.08.18 val PER: 0.1115
2026-01-08 11:27:23,296: t15.2023.08.20 val PER: 0.1001
2026-01-08 11:27:23,296: t15.2023.08.25 val PER: 0.1054
2026-01-08 11:27:23,296: t15.2023.08.27 val PER: 0.1849
2026-01-08 11:27:23,296: t15.2023.09.01 val PER: 0.0771
2026-01-08 11:27:23,296: t15.2023.09.03 val PER: 0.1544
2026-01-08 11:27:23,296: t15.2023.09.24 val PER: 0.1092
2026-01-08 11:27:23,297: t15.2023.09.29 val PER: 0.1225
2026-01-08 11:27:23,297: t15.2023.10.01 val PER: 0.1671
2026-01-08 11:27:23,297: t15.2023.10.06 val PER: 0.0850
2026-01-08 11:27:23,297: t15.2023.10.08 val PER: 0.2476
2026-01-08 11:27:23,297: t15.2023.10.13 val PER: 0.1877
2026-01-08 11:27:23,297: t15.2023.10.15 val PER: 0.1496
2026-01-08 11:27:23,297: t15.2023.10.20 val PER: 0.1644
2026-01-08 11:27:23,297: t15.2023.10.22 val PER: 0.1058
2026-01-08 11:27:23,297: t15.2023.11.03 val PER: 0.1859
2026-01-08 11:27:23,297: t15.2023.11.04 val PER: 0.0341
2026-01-08 11:27:23,297: t15.2023.11.17 val PER: 0.0280
2026-01-08 11:27:23,298: t15.2023.11.19 val PER: 0.0359
2026-01-08 11:27:23,298: t15.2023.11.26 val PER: 0.1043
2026-01-08 11:27:23,298: t15.2023.12.03 val PER: 0.0893
2026-01-08 11:27:23,298: t15.2023.12.08 val PER: 0.0832
2026-01-08 11:27:23,298: t15.2023.12.10 val PER: 0.0749
2026-01-08 11:27:23,298: t15.2023.12.17 val PER: 0.1154
2026-01-08 11:27:23,298: t15.2023.12.29 val PER: 0.1304
2026-01-08 11:27:23,298: t15.2024.02.25 val PER: 0.1180
2026-01-08 11:27:23,298: t15.2024.03.08 val PER: 0.2119
2026-01-08 11:27:23,298: t15.2024.03.15 val PER: 0.1951
2026-01-08 11:27:23,299: t15.2024.03.17 val PER: 0.1423
2026-01-08 11:27:23,299: t15.2024.05.10 val PER: 0.1501
2026-01-08 11:27:23,299: t15.2024.06.14 val PER: 0.1703
2026-01-08 11:27:23,299: t15.2024.07.19 val PER: 0.2215
2026-01-08 11:27:23,299: t15.2024.07.21 val PER: 0.0897
2026-01-08 11:27:23,299: t15.2024.07.28 val PER: 0.1360
2026-01-08 11:27:23,299: t15.2025.01.10 val PER: 0.3072
2026-01-08 11:27:23,299: t15.2025.01.12 val PER: 0.1601
2026-01-08 11:27:23,300: t15.2025.03.14 val PER: 0.3328
2026-01-08 11:27:23,300: t15.2025.03.16 val PER: 0.1793
2026-01-08 11:27:23,300: t15.2025.03.30 val PER: 0.2655
2026-01-08 11:27:23,300: t15.2025.04.13 val PER: 0.2054
2026-01-08 11:27:40,415: Train batch 17200: loss: 9.09 grad norm: 59.02 time: 0.087
2026-01-08 11:27:57,422: Train batch 17400: loss: 9.55 grad norm: 59.86 time: 0.073
2026-01-08 11:28:05,819: Running test after training batch: 17500
2026-01-08 11:28:05,959: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:28:10,792: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide kantz seed the codes gatz this pointers has
2026-01-08 11:28:10,823: WER debug example
  GT : how does it keep the cost down
  PR : hounds dusty hitty kieper thus cussed setzer
2026-01-08 11:28:12,466: Val batch 17500: PER (avg): 0.1429 CTC Loss (avg): 15.5329 WER(1gram): 91.12% (n=64) time: 6.647
2026-01-08 11:28:12,466: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-08 11:28:12,467: t15.2023.08.13 val PER: 0.1185
2026-01-08 11:28:12,467: t15.2023.08.18 val PER: 0.1123
2026-01-08 11:28:12,467: t15.2023.08.20 val PER: 0.0953
2026-01-08 11:28:12,467: t15.2023.08.25 val PER: 0.0994
2026-01-08 11:28:12,467: t15.2023.08.27 val PER: 0.1881
2026-01-08 11:28:12,467: t15.2023.09.01 val PER: 0.0771
2026-01-08 11:28:12,467: t15.2023.09.03 val PER: 0.1401
2026-01-08 11:28:12,467: t15.2023.09.24 val PER: 0.1032
2026-01-08 11:28:12,467: t15.2023.09.29 val PER: 0.1264
2026-01-08 11:28:12,467: t15.2023.10.01 val PER: 0.1539
2026-01-08 11:28:12,468: t15.2023.10.06 val PER: 0.0807
2026-01-08 11:28:12,468: t15.2023.10.08 val PER: 0.2314
2026-01-08 11:28:12,468: t15.2023.10.13 val PER: 0.1901
2026-01-08 11:28:12,468: t15.2023.10.15 val PER: 0.1516
2026-01-08 11:28:12,468: t15.2023.10.20 val PER: 0.1611
2026-01-08 11:28:12,468: t15.2023.10.22 val PER: 0.1102
2026-01-08 11:28:12,468: t15.2023.11.03 val PER: 0.1913
2026-01-08 11:28:12,468: t15.2023.11.04 val PER: 0.0239
2026-01-08 11:28:12,468: t15.2023.11.17 val PER: 0.0327
2026-01-08 11:28:12,468: t15.2023.11.19 val PER: 0.0479
2026-01-08 11:28:12,468: t15.2023.11.26 val PER: 0.0978
2026-01-08 11:28:12,468: t15.2023.12.03 val PER: 0.0882
2026-01-08 11:28:12,468: t15.2023.12.08 val PER: 0.0852
2026-01-08 11:28:12,468: t15.2023.12.10 val PER: 0.0723
2026-01-08 11:28:12,468: t15.2023.12.17 val PER: 0.1237
2026-01-08 11:28:12,468: t15.2023.12.29 val PER: 0.1167
2026-01-08 11:28:12,469: t15.2024.02.25 val PER: 0.1096
2026-01-08 11:28:12,469: t15.2024.03.08 val PER: 0.2162
2026-01-08 11:28:12,469: t15.2024.03.15 val PER: 0.1901
2026-01-08 11:28:12,469: t15.2024.03.17 val PER: 0.1444
2026-01-08 11:28:12,469: t15.2024.05.10 val PER: 0.1694
2026-01-08 11:28:12,469: t15.2024.06.14 val PER: 0.1656
2026-01-08 11:28:12,469: t15.2024.07.19 val PER: 0.2215
2026-01-08 11:28:12,469: t15.2024.07.21 val PER: 0.0924
2026-01-08 11:28:12,469: t15.2024.07.28 val PER: 0.1221
2026-01-08 11:28:12,469: t15.2025.01.10 val PER: 0.2975
2026-01-08 11:28:12,469: t15.2025.01.12 val PER: 0.1463
2026-01-08 11:28:12,469: t15.2025.03.14 val PER: 0.3210
2026-01-08 11:28:12,469: t15.2025.03.16 val PER: 0.1702
2026-01-08 11:28:12,469: t15.2025.03.30 val PER: 0.2805
2026-01-08 11:28:12,469: t15.2025.04.13 val PER: 0.2126
2026-01-08 11:28:21,633: Train batch 17600: loss: 8.27 grad norm: 47.86 time: 0.053
2026-01-08 11:28:40,678: Train batch 17800: loss: 5.28 grad norm: 40.55 time: 0.044
2026-01-08 11:28:58,454: Train batch 18000: loss: 7.08 grad norm: 55.41 time: 0.064
2026-01-08 11:28:58,454: Running test after training batch: 18000
2026-01-08 11:28:58,583: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:29:03,393: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the candies gatz this tutt pointers has
2026-01-08 11:29:03,425: WER debug example
  GT : how does it keep the cost down
  PR : hounds dusty it kieper thus cussed
2026-01-08 11:29:05,046: Val batch 18000: PER (avg): 0.1423 CTC Loss (avg): 15.4016 WER(1gram): 89.09% (n=64) time: 6.592
2026-01-08 11:29:05,046: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=11
2026-01-08 11:29:05,047: t15.2023.08.13 val PER: 0.1091
2026-01-08 11:29:05,047: t15.2023.08.18 val PER: 0.1081
2026-01-08 11:29:05,047: t15.2023.08.20 val PER: 0.0993
2026-01-08 11:29:05,047: t15.2023.08.25 val PER: 0.0873
2026-01-08 11:29:05,047: t15.2023.08.27 val PER: 0.1736
2026-01-08 11:29:05,047: t15.2023.09.01 val PER: 0.0779
2026-01-08 11:29:05,048: t15.2023.09.03 val PER: 0.1461
2026-01-08 11:29:05,048: t15.2023.09.24 val PER: 0.1177
2026-01-08 11:29:05,048: t15.2023.09.29 val PER: 0.1244
2026-01-08 11:29:05,048: t15.2023.10.01 val PER: 0.1625
2026-01-08 11:29:05,048: t15.2023.10.06 val PER: 0.0893
2026-01-08 11:29:05,048: t15.2023.10.08 val PER: 0.2449
2026-01-08 11:29:05,048: t15.2023.10.13 val PER: 0.1947
2026-01-08 11:29:05,048: t15.2023.10.15 val PER: 0.1430
2026-01-08 11:29:05,049: t15.2023.10.20 val PER: 0.1678
2026-01-08 11:29:05,049: t15.2023.10.22 val PER: 0.1047
2026-01-08 11:29:05,049: t15.2023.11.03 val PER: 0.1825
2026-01-08 11:29:05,049: t15.2023.11.04 val PER: 0.0307
2026-01-08 11:29:05,049: t15.2023.11.17 val PER: 0.0373
2026-01-08 11:29:05,049: t15.2023.11.19 val PER: 0.0359
2026-01-08 11:29:05,049: t15.2023.11.26 val PER: 0.0899
2026-01-08 11:29:05,049: t15.2023.12.03 val PER: 0.0851
2026-01-08 11:29:05,050: t15.2023.12.08 val PER: 0.0859
2026-01-08 11:29:05,050: t15.2023.12.10 val PER: 0.0710
2026-01-08 11:29:05,050: t15.2023.12.17 val PER: 0.1154
2026-01-08 11:29:05,050: t15.2023.12.29 val PER: 0.1187
2026-01-08 11:29:05,050: t15.2024.02.25 val PER: 0.1194
2026-01-08 11:29:05,050: t15.2024.03.08 val PER: 0.2034
2026-01-08 11:29:05,050: t15.2024.03.15 val PER: 0.1957
2026-01-08 11:29:05,050: t15.2024.03.17 val PER: 0.1353
2026-01-08 11:29:05,051: t15.2024.05.10 val PER: 0.1590
2026-01-08 11:29:05,051: t15.2024.06.14 val PER: 0.1625
2026-01-08 11:29:05,051: t15.2024.07.19 val PER: 0.2261
2026-01-08 11:29:05,051: t15.2024.07.21 val PER: 0.0876
2026-01-08 11:29:05,051: t15.2024.07.28 val PER: 0.1294
2026-01-08 11:29:05,051: t15.2025.01.10 val PER: 0.3058
2026-01-08 11:29:05,051: t15.2025.01.12 val PER: 0.1532
2026-01-08 11:29:05,051: t15.2025.03.14 val PER: 0.3166
2026-01-08 11:29:05,052: t15.2025.03.16 val PER: 0.1688
2026-01-08 11:29:05,052: t15.2025.03.30 val PER: 0.2782
2026-01-08 11:29:05,052: t15.2025.04.13 val PER: 0.2068
2026-01-08 11:29:05,052: New best val WER(1gram) 90.10% --> 89.09%
2026-01-08 11:29:05,052: Checkpointing model
2026-01-08 11:29:05,248: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_30k/checkpoint/best_checkpoint
2026-01-08 11:29:23,814: Train batch 18200: loss: 7.35 grad norm: 55.04 time: 0.077
2026-01-08 11:29:41,213: Train batch 18400: loss: 4.90 grad norm: 44.75 time: 0.061
2026-01-08 11:29:50,533: Running test after training batch: 18500
2026-01-08 11:29:50,685: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:29:56,460: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the candies gatz this tutt pointers has
2026-01-08 11:29:56,492: WER debug example
  GT : how does it keep the cost down
  PR : howdy dusty itty kieper thus costs
2026-01-08 11:29:58,152: Val batch 18500: PER (avg): 0.1423 CTC Loss (avg): 15.3389 WER(1gram): 91.12% (n=64) time: 7.618
2026-01-08 11:29:58,152: WER lens: avg_true_words=6.16 avg_pred_words=6.02 max_pred_words=12
2026-01-08 11:29:58,153: t15.2023.08.13 val PER: 0.1175
2026-01-08 11:29:58,153: t15.2023.08.18 val PER: 0.1182
2026-01-08 11:29:58,153: t15.2023.08.20 val PER: 0.1017
2026-01-08 11:29:58,153: t15.2023.08.25 val PER: 0.0904
2026-01-08 11:29:58,153: t15.2023.08.27 val PER: 0.1785
2026-01-08 11:29:58,153: t15.2023.09.01 val PER: 0.0812
2026-01-08 11:29:58,153: t15.2023.09.03 val PER: 0.1556
2026-01-08 11:29:58,153: t15.2023.09.24 val PER: 0.1044
2026-01-08 11:29:58,153: t15.2023.09.29 val PER: 0.1264
2026-01-08 11:29:58,153: t15.2023.10.01 val PER: 0.1552
2026-01-08 11:29:58,153: t15.2023.10.06 val PER: 0.0861
2026-01-08 11:29:58,153: t15.2023.10.08 val PER: 0.2355
2026-01-08 11:29:58,153: t15.2023.10.13 val PER: 0.1994
2026-01-08 11:29:58,154: t15.2023.10.15 val PER: 0.1457
2026-01-08 11:29:58,154: t15.2023.10.20 val PER: 0.1678
2026-01-08 11:29:58,154: t15.2023.10.22 val PER: 0.1180
2026-01-08 11:29:58,154: t15.2023.11.03 val PER: 0.1818
2026-01-08 11:29:58,154: t15.2023.11.04 val PER: 0.0273
2026-01-08 11:29:58,154: t15.2023.11.17 val PER: 0.0249
2026-01-08 11:29:58,154: t15.2023.11.19 val PER: 0.0359
2026-01-08 11:29:58,154: t15.2023.11.26 val PER: 0.0957
2026-01-08 11:29:58,154: t15.2023.12.03 val PER: 0.0893
2026-01-08 11:29:58,154: t15.2023.12.08 val PER: 0.0759
2026-01-08 11:29:58,154: t15.2023.12.10 val PER: 0.0683
2026-01-08 11:29:58,154: t15.2023.12.17 val PER: 0.1123
2026-01-08 11:29:58,154: t15.2023.12.29 val PER: 0.1167
2026-01-08 11:29:58,154: t15.2024.02.25 val PER: 0.0997
2026-01-08 11:29:58,154: t15.2024.03.08 val PER: 0.2191
2026-01-08 11:29:58,155: t15.2024.03.15 val PER: 0.1907
2026-01-08 11:29:58,155: t15.2024.03.17 val PER: 0.1374
2026-01-08 11:29:58,155: t15.2024.05.10 val PER: 0.1456
2026-01-08 11:29:58,155: t15.2024.06.14 val PER: 0.1577
2026-01-08 11:29:58,155: t15.2024.07.19 val PER: 0.2281
2026-01-08 11:29:58,155: t15.2024.07.21 val PER: 0.0897
2026-01-08 11:29:58,155: t15.2024.07.28 val PER: 0.1301
2026-01-08 11:29:58,155: t15.2025.01.10 val PER: 0.2893
2026-01-08 11:29:58,155: t15.2025.01.12 val PER: 0.1463
2026-01-08 11:29:58,155: t15.2025.03.14 val PER: 0.3225
2026-01-08 11:29:58,155: t15.2025.03.16 val PER: 0.1780
2026-01-08 11:29:58,155: t15.2025.03.30 val PER: 0.2839
2026-01-08 11:29:58,155: t15.2025.04.13 val PER: 0.2126
2026-01-08 11:30:07,958: Train batch 18600: loss: 9.52 grad norm: 58.35 time: 0.070
2026-01-08 11:30:27,184: Train batch 18800: loss: 7.39 grad norm: 55.98 time: 0.066
2026-01-08 11:30:45,749: Train batch 19000: loss: 6.47 grad norm: 52.87 time: 0.067
2026-01-08 11:30:45,749: Running test after training batch: 19000
2026-01-08 11:30:45,891: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:30:50,709: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies seed the colds gatz this pointers has
2026-01-08 11:30:50,741: WER debug example
  GT : how does it keep the cost down
  PR : hounds dusty itty kieper thus cost
2026-01-08 11:30:52,429: Val batch 19000: PER (avg): 0.1402 CTC Loss (avg): 15.5160 WER(1gram): 86.80% (n=64) time: 6.679
2026-01-08 11:30:52,429: WER lens: avg_true_words=6.16 avg_pred_words=5.83 max_pred_words=12
2026-01-08 11:30:52,429: t15.2023.08.13 val PER: 0.1060
2026-01-08 11:30:52,429: t15.2023.08.18 val PER: 0.1190
2026-01-08 11:30:52,429: t15.2023.08.20 val PER: 0.0985
2026-01-08 11:30:52,429: t15.2023.08.25 val PER: 0.0979
2026-01-08 11:30:52,430: t15.2023.08.27 val PER: 0.1672
2026-01-08 11:30:52,430: t15.2023.09.01 val PER: 0.0706
2026-01-08 11:30:52,430: t15.2023.09.03 val PER: 0.1354
2026-01-08 11:30:52,430: t15.2023.09.24 val PER: 0.1080
2026-01-08 11:30:52,430: t15.2023.09.29 val PER: 0.1155
2026-01-08 11:30:52,430: t15.2023.10.01 val PER: 0.1572
2026-01-08 11:30:52,430: t15.2023.10.06 val PER: 0.0850
2026-01-08 11:30:52,430: t15.2023.10.08 val PER: 0.2422
2026-01-08 11:30:52,430: t15.2023.10.13 val PER: 0.1839
2026-01-08 11:30:52,430: t15.2023.10.15 val PER: 0.1457
2026-01-08 11:30:52,430: t15.2023.10.20 val PER: 0.1711
2026-01-08 11:30:52,430: t15.2023.10.22 val PER: 0.1125
2026-01-08 11:30:52,430: t15.2023.11.03 val PER: 0.1805
2026-01-08 11:30:52,430: t15.2023.11.04 val PER: 0.0239
2026-01-08 11:30:52,431: t15.2023.11.17 val PER: 0.0280
2026-01-08 11:30:52,431: t15.2023.11.19 val PER: 0.0359
2026-01-08 11:30:52,431: t15.2023.11.26 val PER: 0.0877
2026-01-08 11:30:52,431: t15.2023.12.03 val PER: 0.0872
2026-01-08 11:30:52,431: t15.2023.12.08 val PER: 0.0839
2026-01-08 11:30:52,431: t15.2023.12.10 val PER: 0.0657
2026-01-08 11:30:52,431: t15.2023.12.17 val PER: 0.1071
2026-01-08 11:30:52,431: t15.2023.12.29 val PER: 0.1187
2026-01-08 11:30:52,431: t15.2024.02.25 val PER: 0.1067
2026-01-08 11:30:52,431: t15.2024.03.08 val PER: 0.2077
2026-01-08 11:30:52,431: t15.2024.03.15 val PER: 0.1845
2026-01-08 11:30:52,432: t15.2024.03.17 val PER: 0.1388
2026-01-08 11:30:52,432: t15.2024.05.10 val PER: 0.1590
2026-01-08 11:30:52,432: t15.2024.06.14 val PER: 0.1814
2026-01-08 11:30:52,432: t15.2024.07.19 val PER: 0.2254
2026-01-08 11:30:52,432: t15.2024.07.21 val PER: 0.0841
2026-01-08 11:30:52,432: t15.2024.07.28 val PER: 0.1228
2026-01-08 11:30:52,432: t15.2025.01.10 val PER: 0.2920
2026-01-08 11:30:52,432: t15.2025.01.12 val PER: 0.1586
2026-01-08 11:30:52,432: t15.2025.03.14 val PER: 0.3314
2026-01-08 11:30:52,432: t15.2025.03.16 val PER: 0.1728
2026-01-08 11:30:52,432: t15.2025.03.30 val PER: 0.2759
2026-01-08 11:30:52,432: t15.2025.04.13 val PER: 0.2040
2026-01-08 11:30:52,434: New best val WER(1gram) 89.09% --> 86.80%
2026-01-08 11:30:52,434: Checkpointing model
2026-01-08 11:30:52,620: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_30k/checkpoint/best_checkpoint
2026-01-08 11:31:11,610: Train batch 19200: loss: 4.64 grad norm: 46.29 time: 0.068
2026-01-08 11:31:29,961: Train batch 19400: loss: 3.65 grad norm: 41.92 time: 0.055
2026-01-08 11:31:39,542: Running test after training batch: 19500
2026-01-08 11:31:39,685: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:31:45,100: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies seed the keds gatz this tutt pointers has
2026-01-08 11:31:45,132: WER debug example
  GT : how does it keep the cost down
  PR : hounds dusty it kieper thus cussed sends
2026-01-08 11:31:46,835: Val batch 19500: PER (avg): 0.1383 CTC Loss (avg): 15.0703 WER(1gram): 90.61% (n=64) time: 7.293
2026-01-08 11:31:46,835: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=12
2026-01-08 11:31:46,835: t15.2023.08.13 val PER: 0.1081
2026-01-08 11:31:46,835: t15.2023.08.18 val PER: 0.1123
2026-01-08 11:31:46,835: t15.2023.08.20 val PER: 0.0993
2026-01-08 11:31:46,836: t15.2023.08.25 val PER: 0.0919
2026-01-08 11:31:46,836: t15.2023.08.27 val PER: 0.1736
2026-01-08 11:31:46,836: t15.2023.09.01 val PER: 0.0666
2026-01-08 11:31:46,836: t15.2023.09.03 val PER: 0.1425
2026-01-08 11:31:46,836: t15.2023.09.24 val PER: 0.1068
2026-01-08 11:31:46,836: t15.2023.09.29 val PER: 0.1161
2026-01-08 11:31:46,837: t15.2023.10.01 val PER: 0.1493
2026-01-08 11:31:46,837: t15.2023.10.06 val PER: 0.0764
2026-01-08 11:31:46,837: t15.2023.10.08 val PER: 0.2395
2026-01-08 11:31:46,837: t15.2023.10.13 val PER: 0.1916
2026-01-08 11:31:46,837: t15.2023.10.15 val PER: 0.1490
2026-01-08 11:31:46,837: t15.2023.10.20 val PER: 0.1678
2026-01-08 11:31:46,837: t15.2023.10.22 val PER: 0.1058
2026-01-08 11:31:46,837: t15.2023.11.03 val PER: 0.1859
2026-01-08 11:31:46,837: t15.2023.11.04 val PER: 0.0273
2026-01-08 11:31:46,837: t15.2023.11.17 val PER: 0.0264
2026-01-08 11:31:46,837: t15.2023.11.19 val PER: 0.0379
2026-01-08 11:31:46,837: t15.2023.11.26 val PER: 0.0957
2026-01-08 11:31:46,837: t15.2023.12.03 val PER: 0.0788
2026-01-08 11:31:46,837: t15.2023.12.08 val PER: 0.0779
2026-01-08 11:31:46,837: t15.2023.12.10 val PER: 0.0683
2026-01-08 11:31:46,837: t15.2023.12.17 val PER: 0.1060
2026-01-08 11:31:46,838: t15.2023.12.29 val PER: 0.1132
2026-01-08 11:31:46,838: t15.2024.02.25 val PER: 0.1096
2026-01-08 11:31:46,838: t15.2024.03.08 val PER: 0.2162
2026-01-08 11:31:46,838: t15.2024.03.15 val PER: 0.1826
2026-01-08 11:31:46,838: t15.2024.03.17 val PER: 0.1304
2026-01-08 11:31:46,838: t15.2024.05.10 val PER: 0.1501
2026-01-08 11:31:46,838: t15.2024.06.14 val PER: 0.1593
2026-01-08 11:31:46,838: t15.2024.07.19 val PER: 0.2208
2026-01-08 11:31:46,838: t15.2024.07.21 val PER: 0.0869
2026-01-08 11:31:46,838: t15.2024.07.28 val PER: 0.1235
2026-01-08 11:31:46,838: t15.2025.01.10 val PER: 0.3003
2026-01-08 11:31:46,838: t15.2025.01.12 val PER: 0.1486
2026-01-08 11:31:46,944: t15.2025.03.14 val PER: 0.3166
2026-01-08 11:31:46,944: t15.2025.03.16 val PER: 0.1715
2026-01-08 11:31:46,944: t15.2025.03.30 val PER: 0.2644
2026-01-08 11:31:46,945: t15.2025.04.13 val PER: 0.2040
2026-01-08 11:31:56,791: Train batch 19600: loss: 7.63 grad norm: 60.21 time: 0.060
2026-01-08 11:32:15,356: Train batch 19800: loss: 6.06 grad norm: 53.37 time: 0.058
2026-01-08 11:32:33,941: Train batch 20000: loss: 5.76 grad norm: 49.28 time: 0.070
2026-01-08 11:32:33,942: Running test after training batch: 20000
2026-01-08 11:32:34,098: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:32:39,660: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the kundert hatz this tutt pointers has
2026-01-08 11:32:39,736: WER debug example
  GT : how does it keep the cost down
  PR : hounds dusty it kieper thus cost
2026-01-08 11:32:41,936: Val batch 20000: PER (avg): 0.1367 CTC Loss (avg): 15.1761 WER(1gram): 89.59% (n=64) time: 7.994
2026-01-08 11:32:41,936: WER lens: avg_true_words=6.16 avg_pred_words=5.94 max_pred_words=11
2026-01-08 11:32:41,936: t15.2023.08.13 val PER: 0.1091
2026-01-08 11:32:41,936: t15.2023.08.18 val PER: 0.1132
2026-01-08 11:32:41,937: t15.2023.08.20 val PER: 0.0898
2026-01-08 11:32:41,937: t15.2023.08.25 val PER: 0.0964
2026-01-08 11:32:41,937: t15.2023.08.27 val PER: 0.1704
2026-01-08 11:32:41,937: t15.2023.09.01 val PER: 0.0625
2026-01-08 11:32:41,937: t15.2023.09.03 val PER: 0.1366
2026-01-08 11:32:41,937: t15.2023.09.24 val PER: 0.1104
2026-01-08 11:32:41,937: t15.2023.09.29 val PER: 0.1232
2026-01-08 11:32:41,937: t15.2023.10.01 val PER: 0.1453
2026-01-08 11:32:41,937: t15.2023.10.06 val PER: 0.0807
2026-01-08 11:32:41,937: t15.2023.10.08 val PER: 0.2219
2026-01-08 11:32:41,937: t15.2023.10.13 val PER: 0.1792
2026-01-08 11:32:41,938: t15.2023.10.15 val PER: 0.1477
2026-01-08 11:32:41,938: t15.2023.10.20 val PER: 0.1611
2026-01-08 11:32:41,938: t15.2023.10.22 val PER: 0.1069
2026-01-08 11:32:41,938: t15.2023.11.03 val PER: 0.1750
2026-01-08 11:32:41,938: t15.2023.11.04 val PER: 0.0239
2026-01-08 11:32:41,938: t15.2023.11.17 val PER: 0.0358
2026-01-08 11:32:41,938: t15.2023.11.19 val PER: 0.0299
2026-01-08 11:32:41,939: t15.2023.11.26 val PER: 0.0906
2026-01-08 11:32:41,939: t15.2023.12.03 val PER: 0.0819
2026-01-08 11:32:41,939: t15.2023.12.08 val PER: 0.0806
2026-01-08 11:32:41,939: t15.2023.12.10 val PER: 0.0644
2026-01-08 11:32:41,939: t15.2023.12.17 val PER: 0.0936
2026-01-08 11:32:41,939: t15.2023.12.29 val PER: 0.1132
2026-01-08 11:32:41,939: t15.2024.02.25 val PER: 0.1138
2026-01-08 11:32:41,939: t15.2024.03.08 val PER: 0.2119
2026-01-08 11:32:41,939: t15.2024.03.15 val PER: 0.1876
2026-01-08 11:32:41,939: t15.2024.03.17 val PER: 0.1269
2026-01-08 11:32:41,939: t15.2024.05.10 val PER: 0.1605
2026-01-08 11:32:41,939: t15.2024.06.14 val PER: 0.1625
2026-01-08 11:32:41,939: t15.2024.07.19 val PER: 0.2215
2026-01-08 11:32:41,939: t15.2024.07.21 val PER: 0.0834
2026-01-08 11:32:41,939: t15.2024.07.28 val PER: 0.1191
2026-01-08 11:32:41,939: t15.2025.01.10 val PER: 0.2948
2026-01-08 11:32:41,939: t15.2025.01.12 val PER: 0.1463
2026-01-08 11:32:41,940: t15.2025.03.14 val PER: 0.3136
2026-01-08 11:32:41,940: t15.2025.03.16 val PER: 0.1623
2026-01-08 11:32:41,940: t15.2025.03.30 val PER: 0.2667
2026-01-08 11:32:41,940: t15.2025.04.13 val PER: 0.2211
2026-01-08 11:32:59,518: Train batch 20200: loss: 4.72 grad norm: 49.07 time: 0.063
2026-01-08 11:33:16,643: Train batch 20400: loss: 4.71 grad norm: 41.59 time: 0.065
2026-01-08 11:33:25,296: Running test after training batch: 20500
2026-01-08 11:33:25,440: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:33:30,462: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the kunde gatz this tutt pointers has
2026-01-08 11:33:30,493: WER debug example
  GT : how does it keep the cost down
  PR : howdy dusty itty kieper the cost
2026-01-08 11:33:32,199: Val batch 20500: PER (avg): 0.1356 CTC Loss (avg): 14.9634 WER(1gram): 88.07% (n=64) time: 6.903
2026-01-08 11:33:32,200: WER lens: avg_true_words=6.16 avg_pred_words=5.97 max_pred_words=11
2026-01-08 11:33:32,200: t15.2023.08.13 val PER: 0.1071
2026-01-08 11:33:32,200: t15.2023.08.18 val PER: 0.1073
2026-01-08 11:33:32,200: t15.2023.08.20 val PER: 0.0937
2026-01-08 11:33:32,200: t15.2023.08.25 val PER: 0.0949
2026-01-08 11:33:32,200: t15.2023.08.27 val PER: 0.1768
2026-01-08 11:33:32,200: t15.2023.09.01 val PER: 0.0706
2026-01-08 11:33:32,200: t15.2023.09.03 val PER: 0.1318
2026-01-08 11:33:32,201: t15.2023.09.24 val PER: 0.1032
2026-01-08 11:33:32,201: t15.2023.09.29 val PER: 0.1200
2026-01-08 11:33:32,201: t15.2023.10.01 val PER: 0.1473
2026-01-08 11:33:32,201: t15.2023.10.06 val PER: 0.0872
2026-01-08 11:33:32,201: t15.2023.10.08 val PER: 0.2219
2026-01-08 11:33:32,201: t15.2023.10.13 val PER: 0.1777
2026-01-08 11:33:32,202: t15.2023.10.15 val PER: 0.1444
2026-01-08 11:33:32,202: t15.2023.10.20 val PER: 0.1577
2026-01-08 11:33:32,202: t15.2023.10.22 val PER: 0.1002
2026-01-08 11:33:32,202: t15.2023.11.03 val PER: 0.1805
2026-01-08 11:33:32,202: t15.2023.11.04 val PER: 0.0239
2026-01-08 11:33:32,202: t15.2023.11.17 val PER: 0.0280
2026-01-08 11:33:32,202: t15.2023.11.19 val PER: 0.0200
2026-01-08 11:33:32,202: t15.2023.11.26 val PER: 0.0928
2026-01-08 11:33:32,202: t15.2023.12.03 val PER: 0.0756
2026-01-08 11:33:32,202: t15.2023.12.08 val PER: 0.0812
2026-01-08 11:33:32,202: t15.2023.12.10 val PER: 0.0578
2026-01-08 11:33:32,202: t15.2023.12.17 val PER: 0.1050
2026-01-08 11:33:32,202: t15.2023.12.29 val PER: 0.1105
2026-01-08 11:33:32,202: t15.2024.02.25 val PER: 0.1110
2026-01-08 11:33:32,202: t15.2024.03.08 val PER: 0.2205
2026-01-08 11:33:32,203: t15.2024.03.15 val PER: 0.1832
2026-01-08 11:33:32,203: t15.2024.03.17 val PER: 0.1262
2026-01-08 11:33:32,203: t15.2024.05.10 val PER: 0.1560
2026-01-08 11:33:32,203: t15.2024.06.14 val PER: 0.1562
2026-01-08 11:33:32,203: t15.2024.07.19 val PER: 0.2116
2026-01-08 11:33:32,203: t15.2024.07.21 val PER: 0.0814
2026-01-08 11:33:32,203: t15.2024.07.28 val PER: 0.1191
2026-01-08 11:33:32,203: t15.2025.01.10 val PER: 0.2865
2026-01-08 11:33:32,203: t15.2025.01.12 val PER: 0.1524
2026-01-08 11:33:32,203: t15.2025.03.14 val PER: 0.3210
2026-01-08 11:33:32,203: t15.2025.03.16 val PER: 0.1688
2026-01-08 11:33:32,203: t15.2025.03.30 val PER: 0.2644
2026-01-08 11:33:32,203: t15.2025.04.13 val PER: 0.2054
2026-01-08 11:33:41,843: Train batch 20600: loss: 6.29 grad norm: 49.09 time: 0.059
2026-01-08 11:34:00,946: Train batch 20800: loss: 7.45 grad norm: 58.36 time: 0.056
2026-01-08 11:34:18,088: Train batch 21000: loss: 6.74 grad norm: 56.05 time: 0.054
2026-01-08 11:34:18,088: Running test after training batch: 21000
2026-01-08 11:34:18,221: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:34:23,444: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the kundert gatz this tutt pointers has
2026-01-08 11:34:23,475: WER debug example
  GT : how does it keep the cost down
  PR : hounds dusty it kieper thus cussed
2026-01-08 11:34:25,135: Val batch 21000: PER (avg): 0.1366 CTC Loss (avg): 15.0949 WER(1gram): 89.59% (n=64) time: 7.047
2026-01-08 11:34:25,135: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=12
2026-01-08 11:34:25,135: t15.2023.08.13 val PER: 0.1133
2026-01-08 11:34:25,136: t15.2023.08.18 val PER: 0.1081
2026-01-08 11:34:25,136: t15.2023.08.20 val PER: 0.0969
2026-01-08 11:34:25,136: t15.2023.08.25 val PER: 0.0964
2026-01-08 11:34:25,136: t15.2023.08.27 val PER: 0.1720
2026-01-08 11:34:25,136: t15.2023.09.01 val PER: 0.0698
2026-01-08 11:34:25,136: t15.2023.09.03 val PER: 0.1342
2026-01-08 11:34:25,136: t15.2023.09.24 val PER: 0.1080
2026-01-08 11:34:25,136: t15.2023.09.29 val PER: 0.1187
2026-01-08 11:34:25,136: t15.2023.10.01 val PER: 0.1499
2026-01-08 11:34:25,136: t15.2023.10.06 val PER: 0.0840
2026-01-08 11:34:25,136: t15.2023.10.08 val PER: 0.2409
2026-01-08 11:34:25,136: t15.2023.10.13 val PER: 0.1815
2026-01-08 11:34:25,136: t15.2023.10.15 val PER: 0.1444
2026-01-08 11:34:25,137: t15.2023.10.20 val PER: 0.1644
2026-01-08 11:34:25,137: t15.2023.10.22 val PER: 0.1058
2026-01-08 11:34:25,137: t15.2023.11.03 val PER: 0.1852
2026-01-08 11:34:25,137: t15.2023.11.04 val PER: 0.0205
2026-01-08 11:34:25,137: t15.2023.11.17 val PER: 0.0342
2026-01-08 11:34:25,137: t15.2023.11.19 val PER: 0.0379
2026-01-08 11:34:25,137: t15.2023.11.26 val PER: 0.0884
2026-01-08 11:34:25,137: t15.2023.12.03 val PER: 0.0861
2026-01-08 11:34:25,137: t15.2023.12.08 val PER: 0.0752
2026-01-08 11:34:25,137: t15.2023.12.10 val PER: 0.0644
2026-01-08 11:34:25,137: t15.2023.12.17 val PER: 0.0998
2026-01-08 11:34:25,137: t15.2023.12.29 val PER: 0.1235
2026-01-08 11:34:25,137: t15.2024.02.25 val PER: 0.1025
2026-01-08 11:34:25,137: t15.2024.03.08 val PER: 0.2034
2026-01-08 11:34:25,138: t15.2024.03.15 val PER: 0.1839
2026-01-08 11:34:25,138: t15.2024.03.17 val PER: 0.1290
2026-01-08 11:34:25,138: t15.2024.05.10 val PER: 0.1456
2026-01-08 11:34:25,138: t15.2024.06.14 val PER: 0.1577
2026-01-08 11:34:25,138: t15.2024.07.19 val PER: 0.2076
2026-01-08 11:34:25,138: t15.2024.07.21 val PER: 0.0883
2026-01-08 11:34:25,138: t15.2024.07.28 val PER: 0.1206
2026-01-08 11:34:25,138: t15.2025.01.10 val PER: 0.2727
2026-01-08 11:34:25,138: t15.2025.01.12 val PER: 0.1463
2026-01-08 11:34:25,138: t15.2025.03.14 val PER: 0.3225
2026-01-08 11:34:25,138: t15.2025.03.16 val PER: 0.1649
2026-01-08 11:34:25,138: t15.2025.03.30 val PER: 0.2690
2026-01-08 11:34:25,138: t15.2025.04.13 val PER: 0.2040
2026-01-08 11:34:43,308: Train batch 21200: loss: 4.54 grad norm: 42.48 time: 0.082
2026-01-08 11:35:01,808: Train batch 21400: loss: 8.12 grad norm: 60.75 time: 0.063
2026-01-08 11:35:11,781: Running test after training batch: 21500
2026-01-08 11:35:11,882: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:35:17,154: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the codes gatz this tutt pointers has
2026-01-08 11:35:17,188: WER debug example
  GT : how does it keep the cost down
  PR : hounds dusty it kieper thus costs dentzer
2026-01-08 11:35:18,995: Val batch 21500: PER (avg): 0.1350 CTC Loss (avg): 14.8553 WER(1gram): 87.82% (n=64) time: 7.213
2026-01-08 11:35:18,995: WER lens: avg_true_words=6.16 avg_pred_words=5.95 max_pred_words=11
2026-01-08 11:35:18,995: t15.2023.08.13 val PER: 0.1029
2026-01-08 11:35:18,995: t15.2023.08.18 val PER: 0.1039
2026-01-08 11:35:18,995: t15.2023.08.20 val PER: 0.0913
2026-01-08 11:35:18,995: t15.2023.08.25 val PER: 0.0904
2026-01-08 11:35:18,995: t15.2023.08.27 val PER: 0.1720
2026-01-08 11:35:18,995: t15.2023.09.01 val PER: 0.0706
2026-01-08 11:35:18,995: t15.2023.09.03 val PER: 0.1366
2026-01-08 11:35:18,996: t15.2023.09.24 val PER: 0.1104
2026-01-08 11:35:18,996: t15.2023.09.29 val PER: 0.1238
2026-01-08 11:35:18,996: t15.2023.10.01 val PER: 0.1473
2026-01-08 11:35:18,996: t15.2023.10.06 val PER: 0.0764
2026-01-08 11:35:18,996: t15.2023.10.08 val PER: 0.2314
2026-01-08 11:35:18,996: t15.2023.10.13 val PER: 0.1901
2026-01-08 11:35:18,996: t15.2023.10.15 val PER: 0.1463
2026-01-08 11:35:18,996: t15.2023.10.20 val PER: 0.1544
2026-01-08 11:35:18,996: t15.2023.10.22 val PER: 0.0991
2026-01-08 11:35:18,996: t15.2023.11.03 val PER: 0.1859
2026-01-08 11:35:18,996: t15.2023.11.04 val PER: 0.0273
2026-01-08 11:35:18,996: t15.2023.11.17 val PER: 0.0295
2026-01-08 11:35:18,996: t15.2023.11.19 val PER: 0.0240
2026-01-08 11:35:18,996: t15.2023.11.26 val PER: 0.0906
2026-01-08 11:35:18,997: t15.2023.12.03 val PER: 0.0819
2026-01-08 11:35:18,997: t15.2023.12.08 val PER: 0.0732
2026-01-08 11:35:18,997: t15.2023.12.10 val PER: 0.0552
2026-01-08 11:35:18,997: t15.2023.12.17 val PER: 0.0998
2026-01-08 11:35:18,997: t15.2023.12.29 val PER: 0.1187
2026-01-08 11:35:18,997: t15.2024.02.25 val PER: 0.1081
2026-01-08 11:35:18,997: t15.2024.03.08 val PER: 0.2119
2026-01-08 11:35:18,997: t15.2024.03.15 val PER: 0.1814
2026-01-08 11:35:18,997: t15.2024.03.17 val PER: 0.1241
2026-01-08 11:35:18,997: t15.2024.05.10 val PER: 0.1575
2026-01-08 11:35:18,997: t15.2024.06.14 val PER: 0.1514
2026-01-08 11:35:18,998: t15.2024.07.19 val PER: 0.2123
2026-01-08 11:35:18,998: t15.2024.07.21 val PER: 0.0793
2026-01-08 11:35:18,998: t15.2024.07.28 val PER: 0.1154
2026-01-08 11:35:18,998: t15.2025.01.10 val PER: 0.2851
2026-01-08 11:35:18,998: t15.2025.01.12 val PER: 0.1470
2026-01-08 11:35:18,998: t15.2025.03.14 val PER: 0.3151
2026-01-08 11:35:18,998: t15.2025.03.16 val PER: 0.1571
2026-01-08 11:35:18,998: t15.2025.03.30 val PER: 0.2609
2026-01-08 11:35:18,998: t15.2025.04.13 val PER: 0.2054
2026-01-08 11:35:29,018: Train batch 21600: loss: 9.32 grad norm: 63.16 time: 0.078
2026-01-08 11:35:48,490: Train batch 21800: loss: 5.01 grad norm: 44.19 time: 0.085
2026-01-08 11:36:07,184: Train batch 22000: loss: 12.24 grad norm: 72.36 time: 0.055
2026-01-08 11:36:07,184: Running test after training batch: 22000
2026-01-08 11:36:07,303: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:36:12,538: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the colds hatz this pointers has
2026-01-08 11:36:12,573: WER debug example
  GT : how does it keep the cost down
  PR : hounds dusty itty kieper thus cost
2026-01-08 11:36:14,307: Val batch 22000: PER (avg): 0.1347 CTC Loss (avg): 14.8127 WER(1gram): 92.13% (n=64) time: 7.122
2026-01-08 11:36:14,307: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=12
2026-01-08 11:36:14,307: t15.2023.08.13 val PER: 0.1133
2026-01-08 11:36:14,307: t15.2023.08.18 val PER: 0.1106
2026-01-08 11:36:14,307: t15.2023.08.20 val PER: 0.0937
2026-01-08 11:36:14,307: t15.2023.08.25 val PER: 0.0934
2026-01-08 11:36:14,307: t15.2023.08.27 val PER: 0.1801
2026-01-08 11:36:14,307: t15.2023.09.01 val PER: 0.0649
2026-01-08 11:36:14,308: t15.2023.09.03 val PER: 0.1306
2026-01-08 11:36:14,308: t15.2023.09.24 val PER: 0.1092
2026-01-08 11:36:14,308: t15.2023.09.29 val PER: 0.1193
2026-01-08 11:36:14,308: t15.2023.10.01 val PER: 0.1526
2026-01-08 11:36:14,308: t15.2023.10.06 val PER: 0.0840
2026-01-08 11:36:14,308: t15.2023.10.08 val PER: 0.2327
2026-01-08 11:36:14,308: t15.2023.10.13 val PER: 0.1924
2026-01-08 11:36:14,308: t15.2023.10.15 val PER: 0.1391
2026-01-08 11:36:14,308: t15.2023.10.20 val PER: 0.1544
2026-01-08 11:36:14,308: t15.2023.10.22 val PER: 0.0980
2026-01-08 11:36:14,308: t15.2023.11.03 val PER: 0.1825
2026-01-08 11:36:14,308: t15.2023.11.04 val PER: 0.0273
2026-01-08 11:36:14,308: t15.2023.11.17 val PER: 0.0295
2026-01-08 11:36:14,308: t15.2023.11.19 val PER: 0.0319
2026-01-08 11:36:14,309: t15.2023.11.26 val PER: 0.0833
2026-01-08 11:36:14,309: t15.2023.12.03 val PER: 0.0809
2026-01-08 11:36:14,309: t15.2023.12.08 val PER: 0.0732
2026-01-08 11:36:14,309: t15.2023.12.10 val PER: 0.0683
2026-01-08 11:36:14,309: t15.2023.12.17 val PER: 0.0956
2026-01-08 11:36:14,309: t15.2023.12.29 val PER: 0.1132
2026-01-08 11:36:14,309: t15.2024.02.25 val PER: 0.1081
2026-01-08 11:36:14,309: t15.2024.03.08 val PER: 0.2176
2026-01-08 11:36:14,309: t15.2024.03.15 val PER: 0.1839
2026-01-08 11:36:14,309: t15.2024.03.17 val PER: 0.1185
2026-01-08 11:36:14,309: t15.2024.05.10 val PER: 0.1545
2026-01-08 11:36:14,309: t15.2024.06.14 val PER: 0.1546
2026-01-08 11:36:14,309: t15.2024.07.19 val PER: 0.2103
2026-01-08 11:36:14,309: t15.2024.07.21 val PER: 0.0793
2026-01-08 11:36:14,309: t15.2024.07.28 val PER: 0.1199
2026-01-08 11:36:14,310: t15.2025.01.10 val PER: 0.2824
2026-01-08 11:36:14,310: t15.2025.01.12 val PER: 0.1355
2026-01-08 11:36:14,310: t15.2025.03.14 val PER: 0.3180
2026-01-08 11:36:14,310: t15.2025.03.16 val PER: 0.1610
2026-01-08 11:36:14,310: t15.2025.03.30 val PER: 0.2632
2026-01-08 11:36:14,310: t15.2025.04.13 val PER: 0.1983
2026-01-08 11:36:33,092: Train batch 22200: loss: 6.82 grad norm: 47.51 time: 0.064
2026-01-08 11:36:51,907: Train batch 22400: loss: 6.40 grad norm: 53.12 time: 0.056
2026-01-08 11:37:01,530: Running test after training batch: 22500
2026-01-08 11:37:01,675: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:37:07,225: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the kundert gatz this tutt pointers has
2026-01-08 11:37:07,257: WER debug example
  GT : how does it keep the cost down
  PR : hounds dusty itty kieper thus cost
2026-01-08 11:37:09,007: Val batch 22500: PER (avg): 0.1322 CTC Loss (avg): 14.7880 WER(1gram): 89.09% (n=64) time: 7.477
2026-01-08 11:37:09,007: WER lens: avg_true_words=6.16 avg_pred_words=5.91 max_pred_words=12
2026-01-08 11:37:09,008: t15.2023.08.13 val PER: 0.1091
2026-01-08 11:37:09,008: t15.2023.08.18 val PER: 0.1106
2026-01-08 11:37:09,008: t15.2023.08.20 val PER: 0.0905
2026-01-08 11:37:09,008: t15.2023.08.25 val PER: 0.1009
2026-01-08 11:37:09,008: t15.2023.08.27 val PER: 0.1656
2026-01-08 11:37:09,008: t15.2023.09.01 val PER: 0.0617
2026-01-08 11:37:09,008: t15.2023.09.03 val PER: 0.1283
2026-01-08 11:37:09,008: t15.2023.09.24 val PER: 0.1104
2026-01-08 11:37:09,008: t15.2023.09.29 val PER: 0.1142
2026-01-08 11:37:09,008: t15.2023.10.01 val PER: 0.1546
2026-01-08 11:37:09,008: t15.2023.10.06 val PER: 0.0721
2026-01-08 11:37:09,008: t15.2023.10.08 val PER: 0.2327
2026-01-08 11:37:09,008: t15.2023.10.13 val PER: 0.1877
2026-01-08 11:37:09,008: t15.2023.10.15 val PER: 0.1384
2026-01-08 11:37:09,009: t15.2023.10.20 val PER: 0.1577
2026-01-08 11:37:09,009: t15.2023.10.22 val PER: 0.1024
2026-01-08 11:37:09,009: t15.2023.11.03 val PER: 0.1764
2026-01-08 11:37:09,009: t15.2023.11.04 val PER: 0.0273
2026-01-08 11:37:09,009: t15.2023.11.17 val PER: 0.0218
2026-01-08 11:37:09,009: t15.2023.11.19 val PER: 0.0240
2026-01-08 11:37:09,009: t15.2023.11.26 val PER: 0.0848
2026-01-08 11:37:09,009: t15.2023.12.03 val PER: 0.0788
2026-01-08 11:37:09,009: t15.2023.12.08 val PER: 0.0732
2026-01-08 11:37:09,009: t15.2023.12.10 val PER: 0.0591
2026-01-08 11:37:09,010: t15.2023.12.17 val PER: 0.0936
2026-01-08 11:37:09,010: t15.2023.12.29 val PER: 0.1057
2026-01-08 11:37:09,010: t15.2024.02.25 val PER: 0.1025
2026-01-08 11:37:09,010: t15.2024.03.08 val PER: 0.2077
2026-01-08 11:37:09,010: t15.2024.03.15 val PER: 0.1757
2026-01-08 11:37:09,010: t15.2024.03.17 val PER: 0.1199
2026-01-08 11:37:09,010: t15.2024.05.10 val PER: 0.1620
2026-01-08 11:37:09,010: t15.2024.06.14 val PER: 0.1483
2026-01-08 11:37:09,010: t15.2024.07.19 val PER: 0.2156
2026-01-08 11:37:09,010: t15.2024.07.21 val PER: 0.0807
2026-01-08 11:37:09,010: t15.2024.07.28 val PER: 0.1147
2026-01-08 11:37:09,010: t15.2025.01.10 val PER: 0.2851
2026-01-08 11:37:09,010: t15.2025.01.12 val PER: 0.1355
2026-01-08 11:37:09,010: t15.2025.03.14 val PER: 0.3136
2026-01-08 11:37:09,011: t15.2025.03.16 val PER: 0.1518
2026-01-08 11:37:09,011: t15.2025.03.30 val PER: 0.2598
2026-01-08 11:37:09,011: t15.2025.04.13 val PER: 0.1940
2026-01-08 11:37:18,833: Train batch 22600: loss: 10.12 grad norm: 63.37 time: 0.063
2026-01-08 11:37:38,065: Train batch 22800: loss: 7.51 grad norm: 53.47 time: 0.059
2026-01-08 11:37:56,061: Train batch 23000: loss: 6.51 grad norm: 53.36 time: 0.063
2026-01-08 11:37:56,061: Running test after training batch: 23000
2026-01-08 11:37:56,196: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:38:01,427: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the keds gatz this tutt pointers has
2026-01-08 11:38:01,459: WER debug example
  GT : how does it keep the cost down
  PR : hounds dusty itty kieper thus costs
2026-01-08 11:38:03,209: Val batch 23000: PER (avg): 0.1338 CTC Loss (avg): 15.1232 WER(1gram): 88.07% (n=64) time: 7.148
2026-01-08 11:38:03,209: WER lens: avg_true_words=6.16 avg_pred_words=5.97 max_pred_words=11
2026-01-08 11:38:03,209: t15.2023.08.13 val PER: 0.1071
2026-01-08 11:38:03,210: t15.2023.08.18 val PER: 0.1073
2026-01-08 11:38:03,210: t15.2023.08.20 val PER: 0.0874
2026-01-08 11:38:03,210: t15.2023.08.25 val PER: 0.0979
2026-01-08 11:38:03,210: t15.2023.08.27 val PER: 0.1736
2026-01-08 11:38:03,210: t15.2023.09.01 val PER: 0.0593
2026-01-08 11:38:03,210: t15.2023.09.03 val PER: 0.1330
2026-01-08 11:38:03,210: t15.2023.09.24 val PER: 0.1092
2026-01-08 11:38:03,210: t15.2023.09.29 val PER: 0.1187
2026-01-08 11:38:03,210: t15.2023.10.01 val PER: 0.1513
2026-01-08 11:38:03,210: t15.2023.10.06 val PER: 0.0797
2026-01-08 11:38:03,210: t15.2023.10.08 val PER: 0.2273
2026-01-08 11:38:03,211: t15.2023.10.13 val PER: 0.1815
2026-01-08 11:38:03,211: t15.2023.10.15 val PER: 0.1430
2026-01-08 11:38:03,211: t15.2023.10.20 val PER: 0.1510
2026-01-08 11:38:03,211: t15.2023.10.22 val PER: 0.1036
2026-01-08 11:38:03,211: t15.2023.11.03 val PER: 0.1730
2026-01-08 11:38:03,211: t15.2023.11.04 val PER: 0.0273
2026-01-08 11:38:03,211: t15.2023.11.17 val PER: 0.0280
2026-01-08 11:38:03,211: t15.2023.11.19 val PER: 0.0339
2026-01-08 11:38:03,211: t15.2023.11.26 val PER: 0.0884
2026-01-08 11:38:03,211: t15.2023.12.03 val PER: 0.0840
2026-01-08 11:38:03,211: t15.2023.12.08 val PER: 0.0766
2026-01-08 11:38:03,211: t15.2023.12.10 val PER: 0.0591
2026-01-08 11:38:03,211: t15.2023.12.17 val PER: 0.1071
2026-01-08 11:38:03,212: t15.2023.12.29 val PER: 0.1119
2026-01-08 11:38:03,212: t15.2024.02.25 val PER: 0.0997
2026-01-08 11:38:03,212: t15.2024.03.08 val PER: 0.2006
2026-01-08 11:38:03,212: t15.2024.03.15 val PER: 0.1795
2026-01-08 11:38:03,212: t15.2024.03.17 val PER: 0.1227
2026-01-08 11:38:03,212: t15.2024.05.10 val PER: 0.1575
2026-01-08 11:38:03,212: t15.2024.06.14 val PER: 0.1625
2026-01-08 11:38:03,213: t15.2024.07.19 val PER: 0.2156
2026-01-08 11:38:03,213: t15.2024.07.21 val PER: 0.0800
2026-01-08 11:38:03,213: t15.2024.07.28 val PER: 0.1169
2026-01-08 11:38:03,213: t15.2025.01.10 val PER: 0.2824
2026-01-08 11:38:03,213: t15.2025.01.12 val PER: 0.1416
2026-01-08 11:38:03,213: t15.2025.03.14 val PER: 0.3195
2026-01-08 11:38:03,213: t15.2025.03.16 val PER: 0.1597
2026-01-08 11:38:03,213: t15.2025.03.30 val PER: 0.2609
2026-01-08 11:38:03,214: t15.2025.04.13 val PER: 0.1969
2026-01-08 11:38:22,667: Train batch 23200: loss: 7.11 grad norm: 58.38 time: 0.062
2026-01-08 11:38:41,320: Train batch 23400: loss: 3.46 grad norm: 39.56 time: 0.072
2026-01-08 11:38:50,609: Running test after training batch: 23500
2026-01-08 11:38:50,714: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:38:56,151: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the colds gatz this appointees has
2026-01-08 11:38:56,185: WER debug example
  GT : how does it keep the cost down
  PR : hounds dusty itty kieper thus cost setzer
2026-01-08 11:38:57,963: Val batch 23500: PER (avg): 0.1316 CTC Loss (avg): 14.8230 WER(1gram): 90.61% (n=64) time: 7.353
2026-01-08 11:38:57,963: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=12
2026-01-08 11:38:57,963: t15.2023.08.13 val PER: 0.1029
2026-01-08 11:38:57,963: t15.2023.08.18 val PER: 0.1081
2026-01-08 11:38:57,964: t15.2023.08.20 val PER: 0.0882
2026-01-08 11:38:57,964: t15.2023.08.25 val PER: 0.0934
2026-01-08 11:38:57,964: t15.2023.08.27 val PER: 0.1752
2026-01-08 11:38:57,964: t15.2023.09.01 val PER: 0.0609
2026-01-08 11:38:57,964: t15.2023.09.03 val PER: 0.1306
2026-01-08 11:38:57,964: t15.2023.09.24 val PER: 0.1104
2026-01-08 11:38:57,964: t15.2023.09.29 val PER: 0.1130
2026-01-08 11:38:57,964: t15.2023.10.01 val PER: 0.1499
2026-01-08 11:38:57,965: t15.2023.10.06 val PER: 0.0807
2026-01-08 11:38:57,965: t15.2023.10.08 val PER: 0.2327
2026-01-08 11:38:57,965: t15.2023.10.13 val PER: 0.1777
2026-01-08 11:38:57,965: t15.2023.10.15 val PER: 0.1404
2026-01-08 11:38:57,965: t15.2023.10.20 val PER: 0.1644
2026-01-08 11:38:57,965: t15.2023.10.22 val PER: 0.1058
2026-01-08 11:38:57,965: t15.2023.11.03 val PER: 0.1764
2026-01-08 11:38:57,965: t15.2023.11.04 val PER: 0.0205
2026-01-08 11:38:57,965: t15.2023.11.17 val PER: 0.0295
2026-01-08 11:38:57,965: t15.2023.11.19 val PER: 0.0220
2026-01-08 11:38:57,966: t15.2023.11.26 val PER: 0.0841
2026-01-08 11:38:57,966: t15.2023.12.03 val PER: 0.0683
2026-01-08 11:38:57,966: t15.2023.12.08 val PER: 0.0746
2026-01-08 11:38:57,966: t15.2023.12.10 val PER: 0.0565
2026-01-08 11:38:57,966: t15.2023.12.17 val PER: 0.0988
2026-01-08 11:38:57,966: t15.2023.12.29 val PER: 0.1098
2026-01-08 11:38:57,966: t15.2024.02.25 val PER: 0.0997
2026-01-08 11:38:57,966: t15.2024.03.08 val PER: 0.1991
2026-01-08 11:38:57,966: t15.2024.03.15 val PER: 0.1795
2026-01-08 11:38:57,966: t15.2024.03.17 val PER: 0.1234
2026-01-08 11:38:57,966: t15.2024.05.10 val PER: 0.1530
2026-01-08 11:38:57,966: t15.2024.06.14 val PER: 0.1609
2026-01-08 11:38:57,967: t15.2024.07.19 val PER: 0.2024
2026-01-08 11:38:57,967: t15.2024.07.21 val PER: 0.0766
2026-01-08 11:38:57,967: t15.2024.07.28 val PER: 0.1221
2026-01-08 11:38:57,967: t15.2025.01.10 val PER: 0.2700
2026-01-08 11:38:57,967: t15.2025.01.12 val PER: 0.1440
2026-01-08 11:38:57,967: t15.2025.03.14 val PER: 0.3107
2026-01-08 11:38:57,967: t15.2025.03.16 val PER: 0.1466
2026-01-08 11:38:57,967: t15.2025.03.30 val PER: 0.2644
2026-01-08 11:38:57,967: t15.2025.04.13 val PER: 0.1954
2026-01-08 11:39:07,599: Train batch 23600: loss: 2.30 grad norm: 33.62 time: 0.059
2026-01-08 11:39:26,381: Train batch 23800: loss: 5.42 grad norm: 49.10 time: 0.058
2026-01-08 11:39:45,675: Train batch 24000: loss: 5.15 grad norm: 49.71 time: 0.082
2026-01-08 11:39:45,676: Running test after training batch: 24000
2026-01-08 11:39:45,785: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:39:51,083: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the candies gatz this tutt pointers as
2026-01-08 11:39:51,117: WER debug example
  GT : how does it keep the cost down
  PR : howdy dusty itty kieper thus costs
2026-01-08 11:39:52,895: Val batch 24000: PER (avg): 0.1315 CTC Loss (avg): 14.7517 WER(1gram): 89.59% (n=64) time: 7.219
2026-01-08 11:39:52,896: WER lens: avg_true_words=6.16 avg_pred_words=5.97 max_pred_words=11
2026-01-08 11:39:52,896: t15.2023.08.13 val PER: 0.1071
2026-01-08 11:39:52,896: t15.2023.08.18 val PER: 0.1031
2026-01-08 11:39:52,896: t15.2023.08.20 val PER: 0.0898
2026-01-08 11:39:52,896: t15.2023.08.25 val PER: 0.0949
2026-01-08 11:39:52,896: t15.2023.08.27 val PER: 0.1704
2026-01-08 11:39:52,896: t15.2023.09.01 val PER: 0.0609
2026-01-08 11:39:52,896: t15.2023.09.03 val PER: 0.1295
2026-01-08 11:39:52,896: t15.2023.09.24 val PER: 0.1092
2026-01-08 11:39:52,897: t15.2023.09.29 val PER: 0.1130
2026-01-08 11:39:52,897: t15.2023.10.01 val PER: 0.1486
2026-01-08 11:39:52,897: t15.2023.10.06 val PER: 0.0807
2026-01-08 11:39:52,897: t15.2023.10.08 val PER: 0.2233
2026-01-08 11:39:52,897: t15.2023.10.13 val PER: 0.1738
2026-01-08 11:39:52,897: t15.2023.10.15 val PER: 0.1391
2026-01-08 11:39:52,897: t15.2023.10.20 val PER: 0.1745
2026-01-08 11:39:52,897: t15.2023.10.22 val PER: 0.1036
2026-01-08 11:39:52,897: t15.2023.11.03 val PER: 0.1784
2026-01-08 11:39:52,897: t15.2023.11.04 val PER: 0.0171
2026-01-08 11:39:52,897: t15.2023.11.17 val PER: 0.0249
2026-01-08 11:39:52,897: t15.2023.11.19 val PER: 0.0299
2026-01-08 11:39:52,897: t15.2023.11.26 val PER: 0.0826
2026-01-08 11:39:52,897: t15.2023.12.03 val PER: 0.0725
2026-01-08 11:39:52,897: t15.2023.12.08 val PER: 0.0752
2026-01-08 11:39:52,898: t15.2023.12.10 val PER: 0.0591
2026-01-08 11:39:52,898: t15.2023.12.17 val PER: 0.0936
2026-01-08 11:39:52,898: t15.2023.12.29 val PER: 0.1105
2026-01-08 11:39:52,898: t15.2024.02.25 val PER: 0.1039
2026-01-08 11:39:52,898: t15.2024.03.08 val PER: 0.2063
2026-01-08 11:39:52,898: t15.2024.03.15 val PER: 0.1776
2026-01-08 11:39:52,898: t15.2024.03.17 val PER: 0.1199
2026-01-08 11:39:52,898: t15.2024.05.10 val PER: 0.1605
2026-01-08 11:39:52,898: t15.2024.06.14 val PER: 0.1577
2026-01-08 11:39:52,898: t15.2024.07.19 val PER: 0.2096
2026-01-08 11:39:52,898: t15.2024.07.21 val PER: 0.0738
2026-01-08 11:39:52,898: t15.2024.07.28 val PER: 0.1147
2026-01-08 11:39:52,898: t15.2025.01.10 val PER: 0.2893
2026-01-08 11:39:52,898: t15.2025.01.12 val PER: 0.1363
2026-01-08 11:39:52,898: t15.2025.03.14 val PER: 0.3210
2026-01-08 11:39:52,898: t15.2025.03.16 val PER: 0.1427
2026-01-08 11:39:52,899: t15.2025.03.30 val PER: 0.2632
2026-01-08 11:39:52,899: t15.2025.04.13 val PER: 0.2040
2026-01-08 11:40:12,493: Train batch 24200: loss: 3.97 grad norm: 48.60 time: 0.059
2026-01-08 11:40:32,073: Train batch 24400: loss: 7.28 grad norm: 53.95 time: 0.056
2026-01-08 11:40:41,841: Running test after training batch: 24500
2026-01-08 11:40:41,999: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:40:47,195: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the candies gatz this tutt pointers has
2026-01-08 11:40:47,228: WER debug example
  GT : how does it keep the cost down
  PR : howdy dusty itty kieper thus costs
2026-01-08 11:40:48,994: Val batch 24500: PER (avg): 0.1325 CTC Loss (avg): 14.8342 WER(1gram): 90.61% (n=64) time: 7.152
2026-01-08 11:40:48,994: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=12
2026-01-08 11:40:48,994: t15.2023.08.13 val PER: 0.1050
2026-01-08 11:40:48,994: t15.2023.08.18 val PER: 0.1123
2026-01-08 11:40:48,994: t15.2023.08.20 val PER: 0.0850
2026-01-08 11:40:48,995: t15.2023.08.25 val PER: 0.0934
2026-01-08 11:40:48,995: t15.2023.08.27 val PER: 0.1768
2026-01-08 11:40:48,995: t15.2023.09.01 val PER: 0.0528
2026-01-08 11:40:48,995: t15.2023.09.03 val PER: 0.1259
2026-01-08 11:40:48,995: t15.2023.09.24 val PER: 0.1104
2026-01-08 11:40:48,995: t15.2023.09.29 val PER: 0.1181
2026-01-08 11:40:48,995: t15.2023.10.01 val PER: 0.1473
2026-01-08 11:40:48,995: t15.2023.10.06 val PER: 0.0829
2026-01-08 11:40:48,995: t15.2023.10.08 val PER: 0.2314
2026-01-08 11:40:48,995: t15.2023.10.13 val PER: 0.1784
2026-01-08 11:40:48,995: t15.2023.10.15 val PER: 0.1444
2026-01-08 11:40:48,995: t15.2023.10.20 val PER: 0.1577
2026-01-08 11:40:48,996: t15.2023.10.22 val PER: 0.1069
2026-01-08 11:40:48,996: t15.2023.11.03 val PER: 0.1798
2026-01-08 11:40:48,996: t15.2023.11.04 val PER: 0.0307
2026-01-08 11:40:48,996: t15.2023.11.17 val PER: 0.0264
2026-01-08 11:40:48,996: t15.2023.11.19 val PER: 0.0279
2026-01-08 11:40:48,996: t15.2023.11.26 val PER: 0.0848
2026-01-08 11:40:48,996: t15.2023.12.03 val PER: 0.0830
2026-01-08 11:40:48,997: t15.2023.12.08 val PER: 0.0692
2026-01-08 11:40:48,997: t15.2023.12.10 val PER: 0.0644
2026-01-08 11:40:48,997: t15.2023.12.17 val PER: 0.0977
2026-01-08 11:40:48,997: t15.2023.12.29 val PER: 0.1146
2026-01-08 11:40:48,997: t15.2024.02.25 val PER: 0.1025
2026-01-08 11:40:48,997: t15.2024.03.08 val PER: 0.1991
2026-01-08 11:40:48,997: t15.2024.03.15 val PER: 0.1845
2026-01-08 11:40:48,997: t15.2024.03.17 val PER: 0.1227
2026-01-08 11:40:48,997: t15.2024.05.10 val PER: 0.1516
2026-01-08 11:40:48,997: t15.2024.06.14 val PER: 0.1546
2026-01-08 11:40:48,997: t15.2024.07.19 val PER: 0.2037
2026-01-08 11:40:48,997: t15.2024.07.21 val PER: 0.0759
2026-01-08 11:40:48,997: t15.2024.07.28 val PER: 0.1191
2026-01-08 11:40:48,998: t15.2025.01.10 val PER: 0.2741
2026-01-08 11:40:48,998: t15.2025.01.12 val PER: 0.1363
2026-01-08 11:40:48,998: t15.2025.03.14 val PER: 0.3180
2026-01-08 11:40:48,998: t15.2025.03.16 val PER: 0.1505
2026-01-08 11:40:48,998: t15.2025.03.30 val PER: 0.2667
2026-01-08 11:40:48,998: t15.2025.04.13 val PER: 0.2040
2026-01-08 11:40:59,755: Train batch 24600: loss: 6.71 grad norm: 53.06 time: 0.061
2026-01-08 11:41:19,065: Train batch 24800: loss: 8.86 grad norm: 72.05 time: 0.082
2026-01-08 11:41:38,286: Train batch 25000: loss: 7.10 grad norm: 57.37 time: 0.068
2026-01-08 11:41:38,286: Running test after training batch: 25000
2026-01-08 11:41:38,446: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:41:43,678: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the candies gatz this tutt pointers has
2026-01-08 11:41:43,711: WER debug example
  GT : how does it keep the cost down
  PR : howdy dusty itty kieper thus cost
2026-01-08 11:41:45,446: Val batch 25000: PER (avg): 0.1314 CTC Loss (avg): 14.7213 WER(1gram): 89.34% (n=64) time: 7.160
2026-01-08 11:41:45,447: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=12
2026-01-08 11:41:45,447: t15.2023.08.13 val PER: 0.1050
2026-01-08 11:41:45,447: t15.2023.08.18 val PER: 0.1048
2026-01-08 11:41:45,447: t15.2023.08.20 val PER: 0.0953
2026-01-08 11:41:45,447: t15.2023.08.25 val PER: 0.0964
2026-01-08 11:41:45,447: t15.2023.08.27 val PER: 0.1704
2026-01-08 11:41:45,448: t15.2023.09.01 val PER: 0.0552
2026-01-08 11:41:45,448: t15.2023.09.03 val PER: 0.1211
2026-01-08 11:41:45,448: t15.2023.09.24 val PER: 0.1056
2026-01-08 11:41:45,448: t15.2023.09.29 val PER: 0.1206
2026-01-08 11:41:45,448: t15.2023.10.01 val PER: 0.1473
2026-01-08 11:41:45,448: t15.2023.10.06 val PER: 0.0818
2026-01-08 11:41:45,448: t15.2023.10.08 val PER: 0.2368
2026-01-08 11:41:45,448: t15.2023.10.13 val PER: 0.1761
2026-01-08 11:41:45,448: t15.2023.10.15 val PER: 0.1411
2026-01-08 11:41:45,448: t15.2023.10.20 val PER: 0.1611
2026-01-08 11:41:45,448: t15.2023.10.22 val PER: 0.1013
2026-01-08 11:41:45,448: t15.2023.11.03 val PER: 0.1839
2026-01-08 11:41:45,448: t15.2023.11.04 val PER: 0.0239
2026-01-08 11:41:45,448: t15.2023.11.17 val PER: 0.0249
2026-01-08 11:41:45,449: t15.2023.11.19 val PER: 0.0319
2026-01-08 11:41:45,449: t15.2023.11.26 val PER: 0.0848
2026-01-08 11:41:45,449: t15.2023.12.03 val PER: 0.0725
2026-01-08 11:41:45,449: t15.2023.12.08 val PER: 0.0679
2026-01-08 11:41:45,449: t15.2023.12.10 val PER: 0.0565
2026-01-08 11:41:45,449: t15.2023.12.17 val PER: 0.0967
2026-01-08 11:41:45,449: t15.2023.12.29 val PER: 0.1098
2026-01-08 11:41:45,449: t15.2024.02.25 val PER: 0.1053
2026-01-08 11:41:45,449: t15.2024.03.08 val PER: 0.2006
2026-01-08 11:41:45,449: t15.2024.03.15 val PER: 0.1814
2026-01-08 11:41:45,449: t15.2024.03.17 val PER: 0.1151
2026-01-08 11:41:45,449: t15.2024.05.10 val PER: 0.1575
2026-01-08 11:41:45,449: t15.2024.06.14 val PER: 0.1609
2026-01-08 11:41:45,449: t15.2024.07.19 val PER: 0.2070
2026-01-08 11:41:45,449: t15.2024.07.21 val PER: 0.0724
2026-01-08 11:41:45,449: t15.2024.07.28 val PER: 0.1140
2026-01-08 11:41:45,450: t15.2025.01.10 val PER: 0.2851
2026-01-08 11:41:45,450: t15.2025.01.12 val PER: 0.1386
2026-01-08 11:41:45,450: t15.2025.03.14 val PER: 0.3107
2026-01-08 11:41:45,450: t15.2025.03.16 val PER: 0.1584
2026-01-08 11:41:45,450: t15.2025.03.30 val PER: 0.2540
2026-01-08 11:41:45,450: t15.2025.04.13 val PER: 0.1969
2026-01-08 11:42:06,895: Train batch 25200: loss: 3.95 grad norm: 51.89 time: 0.059
2026-01-08 11:42:27,491: Train batch 25400: loss: 4.41 grad norm: 43.59 time: 0.058
2026-01-08 11:42:37,558: Running test after training batch: 25500
2026-01-08 11:42:37,691: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:42:42,974: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the colds gatz this pointers has
2026-01-08 11:42:43,009: WER debug example
  GT : how does it keep the cost down
  PR : howdy dusty itty kieper thus cussed
2026-01-08 11:42:44,772: Val batch 25500: PER (avg): 0.1309 CTC Loss (avg): 14.6766 WER(1gram): 91.37% (n=64) time: 7.214
2026-01-08 11:42:44,772: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=12
2026-01-08 11:42:44,773: t15.2023.08.13 val PER: 0.1050
2026-01-08 11:42:44,773: t15.2023.08.18 val PER: 0.1065
2026-01-08 11:42:44,773: t15.2023.08.20 val PER: 0.0937
2026-01-08 11:42:44,773: t15.2023.08.25 val PER: 0.0934
2026-01-08 11:42:44,773: t15.2023.08.27 val PER: 0.1752
2026-01-08 11:42:44,773: t15.2023.09.01 val PER: 0.0560
2026-01-08 11:42:44,773: t15.2023.09.03 val PER: 0.1223
2026-01-08 11:42:44,773: t15.2023.09.24 val PER: 0.1104
2026-01-08 11:42:44,773: t15.2023.09.29 val PER: 0.1168
2026-01-08 11:42:44,773: t15.2023.10.01 val PER: 0.1453
2026-01-08 11:42:44,774: t15.2023.10.06 val PER: 0.0764
2026-01-08 11:42:44,774: t15.2023.10.08 val PER: 0.2314
2026-01-08 11:42:44,774: t15.2023.10.13 val PER: 0.1769
2026-01-08 11:42:44,774: t15.2023.10.15 val PER: 0.1424
2026-01-08 11:42:44,774: t15.2023.10.20 val PER: 0.1544
2026-01-08 11:42:44,774: t15.2023.10.22 val PER: 0.1036
2026-01-08 11:42:44,774: t15.2023.11.03 val PER: 0.1811
2026-01-08 11:42:44,774: t15.2023.11.04 val PER: 0.0273
2026-01-08 11:42:44,774: t15.2023.11.17 val PER: 0.0233
2026-01-08 11:42:44,775: t15.2023.11.19 val PER: 0.0220
2026-01-08 11:42:44,775: t15.2023.11.26 val PER: 0.0826
2026-01-08 11:42:44,775: t15.2023.12.03 val PER: 0.0767
2026-01-08 11:42:44,775: t15.2023.12.08 val PER: 0.0752
2026-01-08 11:42:44,775: t15.2023.12.10 val PER: 0.0657
2026-01-08 11:42:44,775: t15.2023.12.17 val PER: 0.0936
2026-01-08 11:42:44,775: t15.2023.12.29 val PER: 0.1091
2026-01-08 11:42:44,775: t15.2024.02.25 val PER: 0.1053
2026-01-08 11:42:44,775: t15.2024.03.08 val PER: 0.1977
2026-01-08 11:42:44,775: t15.2024.03.15 val PER: 0.1807
2026-01-08 11:42:44,775: t15.2024.03.17 val PER: 0.1213
2026-01-08 11:42:44,775: t15.2024.05.10 val PER: 0.1486
2026-01-08 11:42:44,775: t15.2024.06.14 val PER: 0.1546
2026-01-08 11:42:44,775: t15.2024.07.19 val PER: 0.2017
2026-01-08 11:42:44,775: t15.2024.07.21 val PER: 0.0738
2026-01-08 11:42:44,776: t15.2024.07.28 val PER: 0.1066
2026-01-08 11:42:44,776: t15.2025.01.10 val PER: 0.2700
2026-01-08 11:42:44,776: t15.2025.01.12 val PER: 0.1401
2026-01-08 11:42:44,776: t15.2025.03.14 val PER: 0.3136
2026-01-08 11:42:44,776: t15.2025.03.16 val PER: 0.1492
2026-01-08 11:42:44,776: t15.2025.03.30 val PER: 0.2713
2026-01-08 11:42:44,776: t15.2025.04.13 val PER: 0.1983
2026-01-08 11:42:54,340: Train batch 25600: loss: 6.63 grad norm: 53.89 time: 0.060
2026-01-08 11:43:14,175: Train batch 25800: loss: 4.52 grad norm: 45.78 time: 0.064
2026-01-08 11:43:34,084: Train batch 26000: loss: 3.26 grad norm: 40.46 time: 0.065
2026-01-08 11:43:34,086: Running test after training batch: 26000
2026-01-08 11:43:34,193: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:43:39,475: WER debug example
  GT : you can see the code at this point as well
  PR : euchre candies seed the colds gatz this pointers has
2026-01-08 11:43:39,510: WER debug example
  GT : how does it keep the cost down
  PR : howdy dusty itty kieper thus cost sends
2026-01-08 11:43:41,301: Val batch 26000: PER (avg): 0.1307 CTC Loss (avg): 14.6234 WER(1gram): 90.86% (n=64) time: 7.214
2026-01-08 11:43:41,301: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=12
2026-01-08 11:43:41,301: t15.2023.08.13 val PER: 0.1112
2026-01-08 11:43:41,301: t15.2023.08.18 val PER: 0.1006
2026-01-08 11:43:41,301: t15.2023.08.20 val PER: 0.0937
2026-01-08 11:43:41,301: t15.2023.08.25 val PER: 0.0949
2026-01-08 11:43:41,302: t15.2023.08.27 val PER: 0.1688
2026-01-08 11:43:41,302: t15.2023.09.01 val PER: 0.0593
2026-01-08 11:43:41,302: t15.2023.09.03 val PER: 0.1283
2026-01-08 11:43:41,302: t15.2023.09.24 val PER: 0.1092
2026-01-08 11:43:41,302: t15.2023.09.29 val PER: 0.1168
2026-01-08 11:43:41,302: t15.2023.10.01 val PER: 0.1506
2026-01-08 11:43:41,302: t15.2023.10.06 val PER: 0.0775
2026-01-08 11:43:41,302: t15.2023.10.08 val PER: 0.2287
2026-01-08 11:43:41,302: t15.2023.10.13 val PER: 0.1753
2026-01-08 11:43:41,302: t15.2023.10.15 val PER: 0.1457
2026-01-08 11:43:41,303: t15.2023.10.20 val PER: 0.1611
2026-01-08 11:43:41,303: t15.2023.10.22 val PER: 0.0969
2026-01-08 11:43:41,303: t15.2023.11.03 val PER: 0.1845
2026-01-08 11:43:41,303: t15.2023.11.04 val PER: 0.0307
2026-01-08 11:43:41,303: t15.2023.11.17 val PER: 0.0249
2026-01-08 11:43:41,303: t15.2023.11.19 val PER: 0.0259
2026-01-08 11:43:41,303: t15.2023.11.26 val PER: 0.0804
2026-01-08 11:43:41,303: t15.2023.12.03 val PER: 0.0788
2026-01-08 11:43:41,303: t15.2023.12.08 val PER: 0.0719
2026-01-08 11:43:41,303: t15.2023.12.10 val PER: 0.0631
2026-01-08 11:43:41,303: t15.2023.12.17 val PER: 0.0977
2026-01-08 11:43:41,303: t15.2023.12.29 val PER: 0.1057
2026-01-08 11:43:41,303: t15.2024.02.25 val PER: 0.1081
2026-01-08 11:43:41,303: t15.2024.03.08 val PER: 0.2006
2026-01-08 11:43:41,303: t15.2024.03.15 val PER: 0.1745
2026-01-08 11:43:41,303: t15.2024.03.17 val PER: 0.1179
2026-01-08 11:43:41,304: t15.2024.05.10 val PER: 0.1560
2026-01-08 11:43:41,304: t15.2024.06.14 val PER: 0.1498
2026-01-08 11:43:41,304: t15.2024.07.19 val PER: 0.2030
2026-01-08 11:43:41,304: t15.2024.07.21 val PER: 0.0724
2026-01-08 11:43:41,304: t15.2024.07.28 val PER: 0.1132
2026-01-08 11:43:41,304: t15.2025.01.10 val PER: 0.2700
2026-01-08 11:43:41,304: t15.2025.01.12 val PER: 0.1378
2026-01-08 11:43:41,304: t15.2025.03.14 val PER: 0.3107
2026-01-08 11:43:41,304: t15.2025.03.16 val PER: 0.1505
2026-01-08 11:43:41,304: t15.2025.03.30 val PER: 0.2483
2026-01-08 11:43:41,304: t15.2025.04.13 val PER: 0.2040
2026-01-08 11:44:01,736: Train batch 26200: loss: 3.09 grad norm: 38.35 time: 0.065
2026-01-08 11:44:22,316: Train batch 26400: loss: 5.59 grad norm: 49.53 time: 0.080
2026-01-08 11:44:32,260: Running test after training batch: 26500
2026-01-08 11:44:32,368: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:44:37,806: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seized the codes gatz this tutt pointers as
2026-01-08 11:44:37,839: WER debug example
  GT : how does it keep the cost down
  PR : howdy dusty itty kieper thus cussed setzer
2026-01-08 11:44:39,615: Val batch 26500: PER (avg): 0.1302 CTC Loss (avg): 14.6508 WER(1gram): 91.88% (n=64) time: 7.355
2026-01-08 11:44:39,616: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=12
2026-01-08 11:44:39,616: t15.2023.08.13 val PER: 0.1050
2026-01-08 11:44:39,616: t15.2023.08.18 val PER: 0.1056
2026-01-08 11:44:39,616: t15.2023.08.20 val PER: 0.0945
2026-01-08 11:44:39,616: t15.2023.08.25 val PER: 0.0949
2026-01-08 11:44:39,616: t15.2023.08.27 val PER: 0.1704
2026-01-08 11:44:39,616: t15.2023.09.01 val PER: 0.0601
2026-01-08 11:44:39,616: t15.2023.09.03 val PER: 0.1330
2026-01-08 11:44:39,616: t15.2023.09.24 val PER: 0.1068
2026-01-08 11:44:39,616: t15.2023.09.29 val PER: 0.1130
2026-01-08 11:44:39,616: t15.2023.10.01 val PER: 0.1480
2026-01-08 11:44:39,617: t15.2023.10.06 val PER: 0.0775
2026-01-08 11:44:39,617: t15.2023.10.08 val PER: 0.2273
2026-01-08 11:44:39,617: t15.2023.10.13 val PER: 0.1777
2026-01-08 11:44:39,617: t15.2023.10.15 val PER: 0.1404
2026-01-08 11:44:39,617: t15.2023.10.20 val PER: 0.1611
2026-01-08 11:44:39,617: t15.2023.10.22 val PER: 0.0947
2026-01-08 11:44:39,617: t15.2023.11.03 val PER: 0.1798
2026-01-08 11:44:39,617: t15.2023.11.04 val PER: 0.0239
2026-01-08 11:44:39,618: t15.2023.11.17 val PER: 0.0233
2026-01-08 11:44:39,618: t15.2023.11.19 val PER: 0.0279
2026-01-08 11:44:39,618: t15.2023.11.26 val PER: 0.0841
2026-01-08 11:44:39,618: t15.2023.12.03 val PER: 0.0756
2026-01-08 11:44:39,618: t15.2023.12.08 val PER: 0.0712
2026-01-08 11:44:39,618: t15.2023.12.10 val PER: 0.0618
2026-01-08 11:44:39,618: t15.2023.12.17 val PER: 0.0936
2026-01-08 11:44:39,618: t15.2023.12.29 val PER: 0.1078
2026-01-08 11:44:39,618: t15.2024.02.25 val PER: 0.1053
2026-01-08 11:44:39,618: t15.2024.03.08 val PER: 0.2048
2026-01-08 11:44:39,618: t15.2024.03.15 val PER: 0.1745
2026-01-08 11:44:39,618: t15.2024.03.17 val PER: 0.1158
2026-01-08 11:44:39,618: t15.2024.05.10 val PER: 0.1486
2026-01-08 11:44:39,618: t15.2024.06.14 val PER: 0.1530
2026-01-08 11:44:39,618: t15.2024.07.19 val PER: 0.1997
2026-01-08 11:44:39,618: t15.2024.07.21 val PER: 0.0779
2026-01-08 11:44:39,619: t15.2024.07.28 val PER: 0.1110
2026-01-08 11:44:39,619: t15.2025.01.10 val PER: 0.2727
2026-01-08 11:44:39,619: t15.2025.01.12 val PER: 0.1363
2026-01-08 11:44:39,619: t15.2025.03.14 val PER: 0.3047
2026-01-08 11:44:39,619: t15.2025.03.16 val PER: 0.1545
2026-01-08 11:44:39,619: t15.2025.03.30 val PER: 0.2586
2026-01-08 11:44:39,619: t15.2025.04.13 val PER: 0.2054
2026-01-08 11:44:50,098: Train batch 26600: loss: 3.13 grad norm: 40.19 time: 0.061
2026-01-08 11:45:10,379: Train batch 26800: loss: 9.36 grad norm: 66.50 time: 0.082
2026-01-08 11:45:30,273: Train batch 27000: loss: 4.23 grad norm: 40.69 time: 0.066
2026-01-08 11:45:30,273: Running test after training batch: 27000
2026-01-08 11:45:30,384: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:45:35,579: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the codes gatz this tutt pointers has
2026-01-08 11:45:35,612: WER debug example
  GT : how does it keep the cost down
  PR : howdy dusty itty kieper thus cost setzer
2026-01-08 11:45:37,406: Val batch 27000: PER (avg): 0.1302 CTC Loss (avg): 14.6224 WER(1gram): 90.36% (n=64) time: 7.133
2026-01-08 11:45:37,407: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-08 11:45:37,407: t15.2023.08.13 val PER: 0.1050
2026-01-08 11:45:37,407: t15.2023.08.18 val PER: 0.1039
2026-01-08 11:45:37,407: t15.2023.08.20 val PER: 0.0937
2026-01-08 11:45:37,407: t15.2023.08.25 val PER: 0.0873
2026-01-08 11:45:37,407: t15.2023.08.27 val PER: 0.1704
2026-01-08 11:45:37,407: t15.2023.09.01 val PER: 0.0609
2026-01-08 11:45:37,407: t15.2023.09.03 val PER: 0.1342
2026-01-08 11:45:37,407: t15.2023.09.24 val PER: 0.1104
2026-01-08 11:45:37,408: t15.2023.09.29 val PER: 0.1123
2026-01-08 11:45:37,408: t15.2023.10.01 val PER: 0.1473
2026-01-08 11:45:37,408: t15.2023.10.06 val PER: 0.0732
2026-01-08 11:45:37,408: t15.2023.10.08 val PER: 0.2273
2026-01-08 11:45:37,408: t15.2023.10.13 val PER: 0.1808
2026-01-08 11:45:37,408: t15.2023.10.15 val PER: 0.1378
2026-01-08 11:45:37,408: t15.2023.10.20 val PER: 0.1611
2026-01-08 11:45:37,408: t15.2023.10.22 val PER: 0.0991
2026-01-08 11:45:37,408: t15.2023.11.03 val PER: 0.1811
2026-01-08 11:45:37,408: t15.2023.11.04 val PER: 0.0273
2026-01-08 11:45:37,408: t15.2023.11.17 val PER: 0.0202
2026-01-08 11:45:37,408: t15.2023.11.19 val PER: 0.0319
2026-01-08 11:45:37,408: t15.2023.11.26 val PER: 0.0783
2026-01-08 11:45:37,408: t15.2023.12.03 val PER: 0.0777
2026-01-08 11:45:37,409: t15.2023.12.08 val PER: 0.0712
2026-01-08 11:45:37,409: t15.2023.12.10 val PER: 0.0618
2026-01-08 11:45:37,409: t15.2023.12.17 val PER: 0.0967
2026-01-08 11:45:37,409: t15.2023.12.29 val PER: 0.1119
2026-01-08 11:45:37,409: t15.2024.02.25 val PER: 0.1039
2026-01-08 11:45:37,409: t15.2024.03.08 val PER: 0.2048
2026-01-08 11:45:37,409: t15.2024.03.15 val PER: 0.1739
2026-01-08 11:45:37,409: t15.2024.03.17 val PER: 0.1144
2026-01-08 11:45:37,409: t15.2024.05.10 val PER: 0.1501
2026-01-08 11:45:37,409: t15.2024.06.14 val PER: 0.1562
2026-01-08 11:45:37,409: t15.2024.07.19 val PER: 0.2037
2026-01-08 11:45:37,409: t15.2024.07.21 val PER: 0.0759
2026-01-08 11:45:37,409: t15.2024.07.28 val PER: 0.1110
2026-01-08 11:45:37,409: t15.2025.01.10 val PER: 0.2782
2026-01-08 11:45:37,409: t15.2025.01.12 val PER: 0.1386
2026-01-08 11:45:37,409: t15.2025.03.14 val PER: 0.3077
2026-01-08 11:45:37,409: t15.2025.03.16 val PER: 0.1466
2026-01-08 11:45:37,410: t15.2025.03.30 val PER: 0.2517
2026-01-08 11:45:37,410: t15.2025.04.13 val PER: 0.2040
2026-01-08 11:45:57,807: Train batch 27200: loss: 4.24 grad norm: 38.75 time: 0.056
2026-01-08 11:46:18,253: Train batch 27400: loss: 7.07 grad norm: 51.55 time: 0.056
2026-01-08 11:46:28,939: Running test after training batch: 27500
2026-01-08 11:46:29,043: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:46:34,284: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the colds gatz this tutt pointers has
2026-01-08 11:46:34,318: WER debug example
  GT : how does it keep the cost down
  PR : howdy dusty itty kieper thus cost setzer
2026-01-08 11:46:36,121: Val batch 27500: PER (avg): 0.1301 CTC Loss (avg): 14.6237 WER(1gram): 90.61% (n=64) time: 7.181
2026-01-08 11:46:36,121: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-08 11:46:36,121: t15.2023.08.13 val PER: 0.1081
2026-01-08 11:46:36,121: t15.2023.08.18 val PER: 0.1023
2026-01-08 11:46:36,121: t15.2023.08.20 val PER: 0.0929
2026-01-08 11:46:36,121: t15.2023.08.25 val PER: 0.0979
2026-01-08 11:46:36,122: t15.2023.08.27 val PER: 0.1736
2026-01-08 11:46:36,122: t15.2023.09.01 val PER: 0.0601
2026-01-08 11:46:36,122: t15.2023.09.03 val PER: 0.1318
2026-01-08 11:46:36,122: t15.2023.09.24 val PER: 0.1032
2026-01-08 11:46:36,122: t15.2023.09.29 val PER: 0.1168
2026-01-08 11:46:36,122: t15.2023.10.01 val PER: 0.1473
2026-01-08 11:46:36,122: t15.2023.10.06 val PER: 0.0764
2026-01-08 11:46:36,122: t15.2023.10.08 val PER: 0.2300
2026-01-08 11:46:36,123: t15.2023.10.13 val PER: 0.1769
2026-01-08 11:46:36,123: t15.2023.10.15 val PER: 0.1430
2026-01-08 11:46:36,123: t15.2023.10.20 val PER: 0.1644
2026-01-08 11:46:36,123: t15.2023.10.22 val PER: 0.0980
2026-01-08 11:46:36,123: t15.2023.11.03 val PER: 0.1757
2026-01-08 11:46:36,123: t15.2023.11.04 val PER: 0.0239
2026-01-08 11:46:36,123: t15.2023.11.17 val PER: 0.0233
2026-01-08 11:46:36,123: t15.2023.11.19 val PER: 0.0240
2026-01-08 11:46:36,123: t15.2023.11.26 val PER: 0.0812
2026-01-08 11:46:36,123: t15.2023.12.03 val PER: 0.0767
2026-01-08 11:46:36,123: t15.2023.12.08 val PER: 0.0732
2026-01-08 11:46:36,123: t15.2023.12.10 val PER: 0.0578
2026-01-08 11:46:36,123: t15.2023.12.17 val PER: 0.0936
2026-01-08 11:46:36,123: t15.2023.12.29 val PER: 0.1050
2026-01-08 11:46:36,123: t15.2024.02.25 val PER: 0.1053
2026-01-08 11:46:36,124: t15.2024.03.08 val PER: 0.2020
2026-01-08 11:46:36,124: t15.2024.03.15 val PER: 0.1764
2026-01-08 11:46:36,124: t15.2024.03.17 val PER: 0.1158
2026-01-08 11:46:36,124: t15.2024.05.10 val PER: 0.1530
2026-01-08 11:46:36,124: t15.2024.06.14 val PER: 0.1577
2026-01-08 11:46:36,124: t15.2024.07.19 val PER: 0.1997
2026-01-08 11:46:36,124: t15.2024.07.21 val PER: 0.0752
2026-01-08 11:46:36,124: t15.2024.07.28 val PER: 0.1103
2026-01-08 11:46:36,124: t15.2025.01.10 val PER: 0.2686
2026-01-08 11:46:36,124: t15.2025.01.12 val PER: 0.1386
2026-01-08 11:46:36,124: t15.2025.03.14 val PER: 0.3062
2026-01-08 11:46:36,124: t15.2025.03.16 val PER: 0.1518
2026-01-08 11:46:36,124: t15.2025.03.30 val PER: 0.2586
2026-01-08 11:46:36,125: t15.2025.04.13 val PER: 0.2026
2026-01-08 11:46:46,150: Train batch 27600: loss: 7.26 grad norm: 43.66 time: 0.065
2026-01-08 11:47:06,990: Train batch 27800: loss: 5.04 grad norm: 46.50 time: 0.043
2026-01-08 11:47:27,818: Train batch 28000: loss: 4.82 grad norm: 50.30 time: 0.056
2026-01-08 11:47:27,819: Running test after training batch: 28000
2026-01-08 11:47:27,921: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:47:33,175: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the codes gatz this tutt pointers has
2026-01-08 11:47:33,212: WER debug example
  GT : how does it keep the cost down
  PR : howdy dusty itty kieper thus costs
2026-01-08 11:47:35,054: Val batch 28000: PER (avg): 0.1299 CTC Loss (avg): 14.6212 WER(1gram): 90.36% (n=64) time: 7.234
2026-01-08 11:47:35,054: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=12
2026-01-08 11:47:35,054: t15.2023.08.13 val PER: 0.1071
2026-01-08 11:47:35,054: t15.2023.08.18 val PER: 0.1031
2026-01-08 11:47:35,055: t15.2023.08.20 val PER: 0.0913
2026-01-08 11:47:35,055: t15.2023.08.25 val PER: 0.0934
2026-01-08 11:47:35,055: t15.2023.08.27 val PER: 0.1720
2026-01-08 11:47:35,055: t15.2023.09.01 val PER: 0.0609
2026-01-08 11:47:35,055: t15.2023.09.03 val PER: 0.1306
2026-01-08 11:47:35,055: t15.2023.09.24 val PER: 0.0995
2026-01-08 11:47:35,055: t15.2023.09.29 val PER: 0.1155
2026-01-08 11:47:35,055: t15.2023.10.01 val PER: 0.1453
2026-01-08 11:47:35,055: t15.2023.10.06 val PER: 0.0775
2026-01-08 11:47:35,055: t15.2023.10.08 val PER: 0.2260
2026-01-08 11:47:35,055: t15.2023.10.13 val PER: 0.1761
2026-01-08 11:47:35,055: t15.2023.10.15 val PER: 0.1391
2026-01-08 11:47:35,055: t15.2023.10.20 val PER: 0.1510
2026-01-08 11:47:35,055: t15.2023.10.22 val PER: 0.0958
2026-01-08 11:47:35,055: t15.2023.11.03 val PER: 0.1757
2026-01-08 11:47:35,055: t15.2023.11.04 val PER: 0.0239
2026-01-08 11:47:35,056: t15.2023.11.17 val PER: 0.0218
2026-01-08 11:47:35,056: t15.2023.11.19 val PER: 0.0279
2026-01-08 11:47:35,056: t15.2023.11.26 val PER: 0.0819
2026-01-08 11:47:35,056: t15.2023.12.03 val PER: 0.0735
2026-01-08 11:47:35,056: t15.2023.12.08 val PER: 0.0712
2026-01-08 11:47:35,056: t15.2023.12.10 val PER: 0.0591
2026-01-08 11:47:35,056: t15.2023.12.17 val PER: 0.1008
2026-01-08 11:47:35,056: t15.2023.12.29 val PER: 0.1084
2026-01-08 11:47:35,056: t15.2024.02.25 val PER: 0.0997
2026-01-08 11:47:35,057: t15.2024.03.08 val PER: 0.2006
2026-01-08 11:47:35,057: t15.2024.03.15 val PER: 0.1789
2026-01-08 11:47:35,057: t15.2024.03.17 val PER: 0.1172
2026-01-08 11:47:35,057: t15.2024.05.10 val PER: 0.1530
2026-01-08 11:47:35,057: t15.2024.06.14 val PER: 0.1546
2026-01-08 11:47:35,057: t15.2024.07.19 val PER: 0.2044
2026-01-08 11:47:35,057: t15.2024.07.21 val PER: 0.0759
2026-01-08 11:47:35,057: t15.2024.07.28 val PER: 0.1132
2026-01-08 11:47:35,057: t15.2025.01.10 val PER: 0.2672
2026-01-08 11:47:35,057: t15.2025.01.12 val PER: 0.1332
2026-01-08 11:47:35,057: t15.2025.03.14 val PER: 0.3136
2026-01-08 11:47:35,057: t15.2025.03.16 val PER: 0.1531
2026-01-08 11:47:35,057: t15.2025.03.30 val PER: 0.2552
2026-01-08 11:47:35,058: t15.2025.04.13 val PER: 0.2097
2026-01-08 11:47:56,932: Train batch 28200: loss: 3.68 grad norm: 41.89 time: 0.065
2026-01-08 11:48:18,251: Train batch 28400: loss: 4.41 grad norm: 45.70 time: 0.063
2026-01-08 11:48:29,038: Running test after training batch: 28500
2026-01-08 11:48:29,194: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:48:34,522: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the colds gatz this tutt pointers has
2026-01-08 11:48:34,555: WER debug example
  GT : how does it keep the cost down
  PR : howdy dusty itty kieper thus cost setzer
2026-01-08 11:48:36,405: Val batch 28500: PER (avg): 0.1303 CTC Loss (avg): 14.6444 WER(1gram): 91.88% (n=64) time: 7.367
2026-01-08 11:48:36,406: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-08 11:48:36,406: t15.2023.08.13 val PER: 0.1050
2026-01-08 11:48:36,406: t15.2023.08.18 val PER: 0.1023
2026-01-08 11:48:36,406: t15.2023.08.20 val PER: 0.0921
2026-01-08 11:48:36,406: t15.2023.08.25 val PER: 0.0979
2026-01-08 11:48:36,406: t15.2023.08.27 val PER: 0.1720
2026-01-08 11:48:36,406: t15.2023.09.01 val PER: 0.0601
2026-01-08 11:48:36,406: t15.2023.09.03 val PER: 0.1306
2026-01-08 11:48:36,407: t15.2023.09.24 val PER: 0.1056
2026-01-08 11:48:36,407: t15.2023.09.29 val PER: 0.1181
2026-01-08 11:48:36,407: t15.2023.10.01 val PER: 0.1466
2026-01-08 11:48:36,407: t15.2023.10.06 val PER: 0.0732
2026-01-08 11:48:36,407: t15.2023.10.08 val PER: 0.2233
2026-01-08 11:48:36,407: t15.2023.10.13 val PER: 0.1761
2026-01-08 11:48:36,407: t15.2023.10.15 val PER: 0.1430
2026-01-08 11:48:36,407: t15.2023.10.20 val PER: 0.1577
2026-01-08 11:48:36,407: t15.2023.10.22 val PER: 0.0969
2026-01-08 11:48:36,408: t15.2023.11.03 val PER: 0.1777
2026-01-08 11:48:36,408: t15.2023.11.04 val PER: 0.0239
2026-01-08 11:48:36,408: t15.2023.11.17 val PER: 0.0280
2026-01-08 11:48:36,408: t15.2023.11.19 val PER: 0.0259
2026-01-08 11:48:36,408: t15.2023.11.26 val PER: 0.0804
2026-01-08 11:48:36,408: t15.2023.12.03 val PER: 0.0788
2026-01-08 11:48:36,408: t15.2023.12.08 val PER: 0.0719
2026-01-08 11:48:36,408: t15.2023.12.10 val PER: 0.0604
2026-01-08 11:48:36,408: t15.2023.12.17 val PER: 0.0977
2026-01-08 11:48:36,408: t15.2023.12.29 val PER: 0.1057
2026-01-08 11:48:36,408: t15.2024.02.25 val PER: 0.0997
2026-01-08 11:48:36,408: t15.2024.03.08 val PER: 0.1963
2026-01-08 11:48:36,408: t15.2024.03.15 val PER: 0.1776
2026-01-08 11:48:36,408: t15.2024.03.17 val PER: 0.1206
2026-01-08 11:48:36,408: t15.2024.05.10 val PER: 0.1516
2026-01-08 11:48:36,408: t15.2024.06.14 val PER: 0.1609
2026-01-08 11:48:36,408: t15.2024.07.19 val PER: 0.2050
2026-01-08 11:48:36,409: t15.2024.07.21 val PER: 0.0800
2026-01-08 11:48:36,409: t15.2024.07.28 val PER: 0.1103
2026-01-08 11:48:36,409: t15.2025.01.10 val PER: 0.2686
2026-01-08 11:48:36,409: t15.2025.01.12 val PER: 0.1355
2026-01-08 11:48:36,409: t15.2025.03.14 val PER: 0.3003
2026-01-08 11:48:36,409: t15.2025.03.16 val PER: 0.1479
2026-01-08 11:48:36,409: t15.2025.03.30 val PER: 0.2621
2026-01-08 11:48:36,409: t15.2025.04.13 val PER: 0.2040
2026-01-08 11:48:47,384: Train batch 28600: loss: 7.26 grad norm: 59.44 time: 0.063
2026-01-08 11:49:10,630: Train batch 28800: loss: 5.46 grad norm: 50.53 time: 0.049
2026-01-08 11:49:31,964: Train batch 29000: loss: 6.23 grad norm: 56.63 time: 0.052
2026-01-08 11:49:31,965: Running test after training batch: 29000
2026-01-08 11:49:32,112: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:49:37,527: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the codes gatz this pointers has
2026-01-08 11:49:37,562: WER debug example
  GT : how does it keep the cost down
  PR : howdy dusty itty kieper thus cost sends
2026-01-08 11:49:39,385: Val batch 29000: PER (avg): 0.1297 CTC Loss (avg): 14.6067 WER(1gram): 89.34% (n=64) time: 7.420
2026-01-08 11:49:39,385: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=12
2026-01-08 11:49:39,385: t15.2023.08.13 val PER: 0.1071
2026-01-08 11:49:39,386: t15.2023.08.18 val PER: 0.1006
2026-01-08 11:49:39,386: t15.2023.08.20 val PER: 0.0890
2026-01-08 11:49:39,386: t15.2023.08.25 val PER: 0.0964
2026-01-08 11:49:39,386: t15.2023.08.27 val PER: 0.1704
2026-01-08 11:49:39,386: t15.2023.09.01 val PER: 0.0576
2026-01-08 11:49:39,386: t15.2023.09.03 val PER: 0.1330
2026-01-08 11:49:39,386: t15.2023.09.24 val PER: 0.1044
2026-01-08 11:49:39,386: t15.2023.09.29 val PER: 0.1181
2026-01-08 11:49:39,386: t15.2023.10.01 val PER: 0.1440
2026-01-08 11:49:39,386: t15.2023.10.06 val PER: 0.0775
2026-01-08 11:49:39,386: t15.2023.10.08 val PER: 0.2273
2026-01-08 11:49:39,386: t15.2023.10.13 val PER: 0.1800
2026-01-08 11:49:39,386: t15.2023.10.15 val PER: 0.1404
2026-01-08 11:49:39,387: t15.2023.10.20 val PER: 0.1409
2026-01-08 11:49:39,387: t15.2023.10.22 val PER: 0.0958
2026-01-08 11:49:39,387: t15.2023.11.03 val PER: 0.1791
2026-01-08 11:49:39,387: t15.2023.11.04 val PER: 0.0239
2026-01-08 11:49:39,387: t15.2023.11.17 val PER: 0.0249
2026-01-08 11:49:39,387: t15.2023.11.19 val PER: 0.0240
2026-01-08 11:49:39,387: t15.2023.11.26 val PER: 0.0790
2026-01-08 11:49:39,387: t15.2023.12.03 val PER: 0.0777
2026-01-08 11:49:39,387: t15.2023.12.08 val PER: 0.0699
2026-01-08 11:49:39,387: t15.2023.12.10 val PER: 0.0644
2026-01-08 11:49:39,387: t15.2023.12.17 val PER: 0.0946
2026-01-08 11:49:39,387: t15.2023.12.29 val PER: 0.1036
2026-01-08 11:49:39,387: t15.2024.02.25 val PER: 0.1039
2026-01-08 11:49:39,388: t15.2024.03.08 val PER: 0.2077
2026-01-08 11:49:39,388: t15.2024.03.15 val PER: 0.1789
2026-01-08 11:49:39,388: t15.2024.03.17 val PER: 0.1151
2026-01-08 11:49:39,388: t15.2024.05.10 val PER: 0.1545
2026-01-08 11:49:39,388: t15.2024.06.14 val PER: 0.1514
2026-01-08 11:49:39,388: t15.2024.07.19 val PER: 0.2024
2026-01-08 11:49:39,388: t15.2024.07.21 val PER: 0.0766
2026-01-08 11:49:39,388: t15.2024.07.28 val PER: 0.1154
2026-01-08 11:49:39,388: t15.2025.01.10 val PER: 0.2672
2026-01-08 11:49:39,388: t15.2025.01.12 val PER: 0.1363
2026-01-08 11:49:39,388: t15.2025.03.14 val PER: 0.3033
2026-01-08 11:49:39,388: t15.2025.03.16 val PER: 0.1505
2026-01-08 11:49:39,388: t15.2025.03.30 val PER: 0.2552
2026-01-08 11:49:39,388: t15.2025.04.13 val PER: 0.2011
2026-01-08 11:50:01,675: Train batch 29200: loss: 4.61 grad norm: 44.58 time: 0.069
2026-01-08 11:50:24,667: Train batch 29400: loss: 3.48 grad norm: 38.08 time: 0.063
2026-01-08 11:50:35,725: Running test after training batch: 29500
2026-01-08 11:50:35,831: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:50:41,348: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the codes gatz this pointers has
2026-01-08 11:50:41,381: WER debug example
  GT : how does it keep the cost down
  PR : howdy dusty itty kieper thus cost setzer
2026-01-08 11:50:43,207: Val batch 29500: PER (avg): 0.1294 CTC Loss (avg): 14.5657 WER(1gram): 89.34% (n=64) time: 7.481
2026-01-08 11:50:43,207: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=12
2026-01-08 11:50:43,207: t15.2023.08.13 val PER: 0.1050
2026-01-08 11:50:43,207: t15.2023.08.18 val PER: 0.1006
2026-01-08 11:50:43,207: t15.2023.08.20 val PER: 0.0905
2026-01-08 11:50:43,208: t15.2023.08.25 val PER: 0.0979
2026-01-08 11:50:43,208: t15.2023.08.27 val PER: 0.1656
2026-01-08 11:50:43,208: t15.2023.09.01 val PER: 0.0584
2026-01-08 11:50:43,208: t15.2023.09.03 val PER: 0.1271
2026-01-08 11:50:43,208: t15.2023.09.24 val PER: 0.1032
2026-01-08 11:50:43,208: t15.2023.09.29 val PER: 0.1187
2026-01-08 11:50:43,208: t15.2023.10.01 val PER: 0.1440
2026-01-08 11:50:43,208: t15.2023.10.06 val PER: 0.0775
2026-01-08 11:50:43,208: t15.2023.10.08 val PER: 0.2287
2026-01-08 11:50:43,209: t15.2023.10.13 val PER: 0.1792
2026-01-08 11:50:43,209: t15.2023.10.15 val PER: 0.1430
2026-01-08 11:50:43,209: t15.2023.10.20 val PER: 0.1477
2026-01-08 11:50:43,209: t15.2023.10.22 val PER: 0.0980
2026-01-08 11:50:43,209: t15.2023.11.03 val PER: 0.1791
2026-01-08 11:50:43,209: t15.2023.11.04 val PER: 0.0239
2026-01-08 11:50:43,209: t15.2023.11.17 val PER: 0.0218
2026-01-08 11:50:43,209: t15.2023.11.19 val PER: 0.0299
2026-01-08 11:50:43,209: t15.2023.11.26 val PER: 0.0790
2026-01-08 11:50:43,209: t15.2023.12.03 val PER: 0.0756
2026-01-08 11:50:43,209: t15.2023.12.08 val PER: 0.0692
2026-01-08 11:50:43,209: t15.2023.12.10 val PER: 0.0644
2026-01-08 11:50:43,210: t15.2023.12.17 val PER: 0.0967
2026-01-08 11:50:43,210: t15.2023.12.29 val PER: 0.1043
2026-01-08 11:50:43,210: t15.2024.02.25 val PER: 0.1025
2026-01-08 11:50:43,210: t15.2024.03.08 val PER: 0.1991
2026-01-08 11:50:43,210: t15.2024.03.15 val PER: 0.1770
2026-01-08 11:50:43,210: t15.2024.03.17 val PER: 0.1165
2026-01-08 11:50:43,210: t15.2024.05.10 val PER: 0.1501
2026-01-08 11:50:43,210: t15.2024.06.14 val PER: 0.1514
2026-01-08 11:50:43,210: t15.2024.07.19 val PER: 0.2057
2026-01-08 11:50:43,210: t15.2024.07.21 val PER: 0.0800
2026-01-08 11:50:43,210: t15.2024.07.28 val PER: 0.1125
2026-01-08 11:50:43,210: t15.2025.01.10 val PER: 0.2713
2026-01-08 11:50:43,210: t15.2025.01.12 val PER: 0.1309
2026-01-08 11:50:43,210: t15.2025.03.14 val PER: 0.3047
2026-01-08 11:50:43,210: t15.2025.03.16 val PER: 0.1479
2026-01-08 11:50:43,210: t15.2025.03.30 val PER: 0.2517
2026-01-08 11:50:43,211: t15.2025.04.13 val PER: 0.2011
2026-01-08 11:50:55,489: Train batch 29600: loss: 4.30 grad norm: 43.06 time: 0.053
2026-01-08 11:51:18,343: Train batch 29800: loss: 4.12 grad norm: 41.87 time: 0.082
2026-01-08 11:51:41,557: Running test after training batch: 29999
2026-01-08 11:51:41,662: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:51:47,079: WER debug example
  GT : you can see the code at this point as well
  PR : yuletide candies seed the colds gatz this tutt pointers has
2026-01-08 11:51:47,113: WER debug example
  GT : how does it keep the cost down
  PR : howdy dusty itty kieper thus costs
2026-01-08 11:51:48,892: Val batch 29999: PER (avg): 0.1295 CTC Loss (avg): 14.6569 WER(1gram): 91.12% (n=64) time: 7.333
2026-01-08 11:51:48,893: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-08 11:51:48,893: t15.2023.08.13 val PER: 0.1040
2026-01-08 11:51:48,893: t15.2023.08.18 val PER: 0.1073
2026-01-08 11:51:48,893: t15.2023.08.20 val PER: 0.0874
2026-01-08 11:51:48,893: t15.2023.08.25 val PER: 0.0994
2026-01-08 11:51:48,893: t15.2023.08.27 val PER: 0.1688
2026-01-08 11:51:48,894: t15.2023.09.01 val PER: 0.0593
2026-01-08 11:51:48,894: t15.2023.09.03 val PER: 0.1271
2026-01-08 11:51:48,894: t15.2023.09.24 val PER: 0.1056
2026-01-08 11:51:48,894: t15.2023.09.29 val PER: 0.1187
2026-01-08 11:51:48,894: t15.2023.10.01 val PER: 0.1446
2026-01-08 11:51:48,894: t15.2023.10.06 val PER: 0.0732
2026-01-08 11:51:48,894: t15.2023.10.08 val PER: 0.2219
2026-01-08 11:51:48,894: t15.2023.10.13 val PER: 0.1761
2026-01-08 11:51:48,894: t15.2023.10.15 val PER: 0.1404
2026-01-08 11:51:48,894: t15.2023.10.20 val PER: 0.1544
2026-01-08 11:51:48,894: t15.2023.10.22 val PER: 0.0969
2026-01-08 11:51:48,895: t15.2023.11.03 val PER: 0.1805
2026-01-08 11:51:48,895: t15.2023.11.04 val PER: 0.0239
2026-01-08 11:51:48,895: t15.2023.11.17 val PER: 0.0218
2026-01-08 11:51:48,895: t15.2023.11.19 val PER: 0.0259
2026-01-08 11:51:48,895: t15.2023.11.26 val PER: 0.0804
2026-01-08 11:51:48,895: t15.2023.12.03 val PER: 0.0725
2026-01-08 11:51:48,895: t15.2023.12.08 val PER: 0.0706
2026-01-08 11:51:48,895: t15.2023.12.10 val PER: 0.0604
2026-01-08 11:51:48,895: t15.2023.12.17 val PER: 0.0967
2026-01-08 11:51:48,895: t15.2023.12.29 val PER: 0.1078
2026-01-08 11:51:48,895: t15.2024.02.25 val PER: 0.1039
2026-01-08 11:51:48,896: t15.2024.03.08 val PER: 0.2020
2026-01-08 11:51:48,896: t15.2024.03.15 val PER: 0.1739
2026-01-08 11:51:48,896: t15.2024.03.17 val PER: 0.1172
2026-01-08 11:51:48,896: t15.2024.05.10 val PER: 0.1530
2026-01-08 11:51:48,896: t15.2024.06.14 val PER: 0.1498
2026-01-08 11:51:48,896: t15.2024.07.19 val PER: 0.2057
2026-01-08 11:51:48,896: t15.2024.07.21 val PER: 0.0752
2026-01-08 11:51:48,896: t15.2024.07.28 val PER: 0.1140
2026-01-08 11:51:48,896: t15.2025.01.10 val PER: 0.2645
2026-01-08 11:51:48,896: t15.2025.01.12 val PER: 0.1316
2026-01-08 11:51:48,897: t15.2025.03.14 val PER: 0.3033
2026-01-08 11:51:48,897: t15.2025.03.16 val PER: 0.1518
2026-01-08 11:51:48,897: t15.2025.03.30 val PER: 0.2621
2026-01-08 11:51:48,897: t15.2025.04.13 val PER: 0.2068
2026-01-08 11:51:48,968: Best avg val PER achieved: 0.14024
2026-01-08 11:51:48,969: Total training time: 53.57 minutes

=== RUN combined_linderman.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_linderman
2026-01-08 11:51:56,876: Using device: cuda:0
2026-01-08 11:51:58,677: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-08 11:51:58,769: Using 45 sessions after filtering (from 45).
2026-01-08 11:51:59,252: Using torch.compile (if available)
2026-01-08 11:51:59,252: torch.compile not available (torch<2.0). Skipping.
2026-01-08 11:51:59,253: Initialized RNN decoding model
2026-01-08 11:51:59,253: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
    (1): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-08 11:51:59,253: Model has 45,499,433 parameters
2026-01-08 11:51:59,253: Model has 11,819,520 day-specific parameters | 25.98% of total parameters
2026-01-08 11:52:01,316: Successfully initialized datasets
2026-01-08 11:52:01,317: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-08 11:52:02,867: Train batch 0: loss: 601.28 grad norm: 2274.55 time: 0.334
2026-01-08 11:52:02,868: Running test after training batch: 0
2026-01-08 11:52:02,980: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:52:07,952: WER debug example
  GT : you can see the code at this point as well
  PR : gilles hwang towle shout hetu
2026-01-08 11:52:08,012: WER debug example
  GT : how does it keep the cost down
  PR : willhelm ballyhoo
2026-01-08 11:52:13,206: Val batch 0: PER (avg): 1.3384 CTC Loss (avg): 626.7153 WER(1gram): 100.00% (n=64) time: 10.338
2026-01-08 11:52:13,206: WER lens: avg_true_words=6.16 avg_pred_words=1.55 max_pred_words=5
2026-01-08 11:52:13,207: t15.2023.08.13 val PER: 1.1913
2026-01-08 11:52:13,207: t15.2023.08.18 val PER: 1.2775
2026-01-08 11:52:13,207: t15.2023.08.20 val PER: 1.2780
2026-01-08 11:52:13,207: t15.2023.08.25 val PER: 1.3419
2026-01-08 11:52:13,207: t15.2023.08.27 val PER: 1.1977
2026-01-08 11:52:13,207: t15.2023.09.01 val PER: 1.3393
2026-01-08 11:52:13,208: t15.2023.09.03 val PER: 1.1805
2026-01-08 11:52:13,208: t15.2023.09.24 val PER: 1.4417
2026-01-08 11:52:13,208: t15.2023.09.29 val PER: 1.4812
2026-01-08 11:52:13,208: t15.2023.10.01 val PER: 1.1704
2026-01-08 11:52:13,208: t15.2023.10.06 val PER: 1.4295
2026-01-08 11:52:13,208: t15.2023.10.08 val PER: 1.1191
2026-01-08 11:52:13,208: t15.2023.10.13 val PER: 1.3530
2026-01-08 11:52:13,208: t15.2023.10.15 val PER: 1.3724
2026-01-08 11:52:13,208: t15.2023.10.20 val PER: 1.4866
2026-01-08 11:52:13,208: t15.2023.10.22 val PER: 1.3898
2026-01-08 11:52:13,208: t15.2023.11.03 val PER: 1.5597
2026-01-08 11:52:13,208: t15.2023.11.04 val PER: 1.5700
2026-01-08 11:52:13,208: t15.2023.11.17 val PER: 1.8569
2026-01-08 11:52:13,208: t15.2023.11.19 val PER: 1.5888
2026-01-08 11:52:13,208: t15.2023.11.26 val PER: 1.4145
2026-01-08 11:52:13,208: t15.2023.12.03 val PER: 1.3393
2026-01-08 11:52:13,208: t15.2023.12.08 val PER: 1.4095
2026-01-08 11:52:13,208: t15.2023.12.10 val PER: 1.4586
2026-01-08 11:52:13,209: t15.2023.12.17 val PER: 1.2380
2026-01-08 11:52:13,209: t15.2023.12.29 val PER: 1.2690
2026-01-08 11:52:13,209: t15.2024.02.25 val PER: 1.3132
2026-01-08 11:52:13,209: t15.2024.03.08 val PER: 1.2191
2026-01-08 11:52:13,209: t15.2024.03.15 val PER: 1.2739
2026-01-08 11:52:13,209: t15.2024.03.17 val PER: 1.3298
2026-01-08 11:52:13,209: t15.2024.05.10 val PER: 1.3789
2026-01-08 11:52:13,209: t15.2024.06.14 val PER: 1.4401
2026-01-08 11:52:13,209: t15.2024.07.19 val PER: 0.9993
2026-01-08 11:52:13,209: t15.2024.07.21 val PER: 1.4559
2026-01-08 11:52:13,209: t15.2024.07.28 val PER: 1.4706
2026-01-08 11:52:13,210: t15.2025.01.10 val PER: 0.9848
2026-01-08 11:52:13,210: t15.2025.01.12 val PER: 1.5358
2026-01-08 11:52:13,210: t15.2025.03.14 val PER: 1.0163
2026-01-08 11:52:13,210: t15.2025.03.16 val PER: 1.4306
2026-01-08 11:52:13,210: t15.2025.03.30 val PER: 1.2241
2026-01-08 11:52:13,210: t15.2025.04.13 val PER: 1.2611
2026-01-08 11:52:13,211: New best val WER(1gram) inf% --> 100.00%
2026-01-08 11:52:13,211: Checkpointing model
2026-01-08 11:52:13,350: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_linderman/checkpoint/best_checkpoint
2026-01-08 11:52:31,488: Train batch 200: loss: 93.93 grad norm: 112.33 time: 0.056
2026-01-08 11:52:47,714: Train batch 400: loss: 65.43 grad norm: 85.10 time: 0.065
2026-01-08 11:52:56,117: Running test after training batch: 500
2026-01-08 11:52:56,238: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:53:01,008: WER debug example
  GT : you can see the code at this point as well
  PR : hollyhead stees mothershead hoses reproduces
2026-01-08 11:53:01,093: WER debug example
  GT : how does it keep the cost down
  PR : knighthood citisteel this hasids
2026-01-08 11:53:06,366: Val batch 500: PER (avg): 0.6288 CTC Loss (avg): 70.6802 WER(1gram): 99.75% (n=64) time: 10.249
2026-01-08 11:53:06,367: WER lens: avg_true_words=6.16 avg_pred_words=3.42 max_pred_words=7
2026-01-08 11:53:06,367: t15.2023.08.13 val PER: 0.5988
2026-01-08 11:53:06,367: t15.2023.08.18 val PER: 0.5683
2026-01-08 11:53:06,367: t15.2023.08.20 val PER: 0.5719
2026-01-08 11:53:06,367: t15.2023.08.25 val PER: 0.5572
2026-01-08 11:53:06,367: t15.2023.08.27 val PER: 0.6238
2026-01-08 11:53:06,367: t15.2023.09.01 val PER: 0.5779
2026-01-08 11:53:06,367: t15.2023.09.03 val PER: 0.6164
2026-01-08 11:53:06,367: t15.2023.09.24 val PER: 0.5765
2026-01-08 11:53:06,368: t15.2023.09.29 val PER: 0.5718
2026-01-08 11:53:06,368: t15.2023.10.01 val PER: 0.6354
2026-01-08 11:53:06,368: t15.2023.10.06 val PER: 0.5953
2026-01-08 11:53:06,368: t15.2023.10.08 val PER: 0.6225
2026-01-08 11:53:06,368: t15.2023.10.13 val PER: 0.6703
2026-01-08 11:53:06,368: t15.2023.10.15 val PER: 0.6269
2026-01-08 11:53:06,368: t15.2023.10.20 val PER: 0.5705
2026-01-08 11:53:06,368: t15.2023.10.22 val PER: 0.5969
2026-01-08 11:53:06,368: t15.2023.11.03 val PER: 0.6594
2026-01-08 11:53:06,368: t15.2023.11.04 val PER: 0.5154
2026-01-08 11:53:06,368: t15.2023.11.17 val PER: 0.5459
2026-01-08 11:53:06,368: t15.2023.11.19 val PER: 0.4731
2026-01-08 11:53:06,368: t15.2023.11.26 val PER: 0.6304
2026-01-08 11:53:06,368: t15.2023.12.03 val PER: 0.6166
2026-01-08 11:53:06,369: t15.2023.12.08 val PER: 0.5985
2026-01-08 11:53:06,369: t15.2023.12.10 val PER: 0.5848
2026-01-08 11:53:06,369: t15.2023.12.17 val PER: 0.6279
2026-01-08 11:53:06,369: t15.2023.12.29 val PER: 0.6026
2026-01-08 11:53:06,369: t15.2024.02.25 val PER: 0.6180
2026-01-08 11:53:06,369: t15.2024.03.08 val PER: 0.6743
2026-01-08 11:53:06,369: t15.2024.03.15 val PER: 0.6579
2026-01-08 11:53:06,369: t15.2024.03.17 val PER: 0.6109
2026-01-08 11:53:06,369: t15.2024.05.10 val PER: 0.6389
2026-01-08 11:53:06,369: t15.2024.06.14 val PER: 0.6987
2026-01-08 11:53:06,369: t15.2024.07.19 val PER: 0.7231
2026-01-08 11:53:06,369: t15.2024.07.21 val PER: 0.6324
2026-01-08 11:53:06,369: t15.2024.07.28 val PER: 0.6596
2026-01-08 11:53:06,369: t15.2025.01.10 val PER: 0.7603
2026-01-08 11:53:06,369: t15.2025.01.12 val PER: 0.6651
2026-01-08 11:53:06,370: t15.2025.03.14 val PER: 0.7544
2026-01-08 11:53:06,370: t15.2025.03.16 val PER: 0.6990
2026-01-08 11:53:06,370: t15.2025.03.30 val PER: 0.7471
2026-01-08 11:53:06,370: t15.2025.04.13 val PER: 0.6662
2026-01-08 11:53:06,371: New best val WER(1gram) 100.00% --> 99.75%
2026-01-08 11:53:06,371: Checkpointing model
2026-01-08 11:53:06,508: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_linderman/checkpoint/best_checkpoint
2026-01-08 11:53:15,205: Train batch 600: loss: 62.48 grad norm: 105.46 time: 0.080
2026-01-08 11:53:32,064: Train batch 800: loss: 52.92 grad norm: 92.17 time: 0.060
2026-01-08 11:53:49,100: Train batch 1000: loss: 51.64 grad norm: 96.07 time: 0.067
2026-01-08 11:53:49,101: Running test after training batch: 1000
2026-01-08 11:53:49,221: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:53:54,090: WER debug example
  GT : you can see the code at this point as well
  PR : hudak indices sutnick advocates appendages
2026-01-08 11:53:54,141: WER debug example
  GT : how does it keep the cost down
  PR : noteholders hitty skinks thus
2026-01-08 11:53:57,531: Val batch 1000: PER (avg): 0.4907 CTC Loss (avg): 52.8471 WER(1gram): 99.24% (n=64) time: 8.431
2026-01-08 11:53:57,532: WER lens: avg_true_words=6.16 avg_pred_words=3.98 max_pred_words=8
2026-01-08 11:53:57,532: t15.2023.08.13 val PER: 0.4418
2026-01-08 11:53:57,532: t15.2023.08.18 val PER: 0.4401
2026-01-08 11:53:57,532: t15.2023.08.20 val PER: 0.4456
2026-01-08 11:53:57,532: t15.2023.08.25 val PER: 0.3946
2026-01-08 11:53:57,532: t15.2023.08.27 val PER: 0.5386
2026-01-08 11:53:57,533: t15.2023.09.01 val PER: 0.4278
2026-01-08 11:53:57,533: t15.2023.09.03 val PER: 0.5107
2026-01-08 11:53:57,533: t15.2023.09.24 val PER: 0.4187
2026-01-08 11:53:57,533: t15.2023.09.29 val PER: 0.4384
2026-01-08 11:53:57,533: t15.2023.10.01 val PER: 0.4888
2026-01-08 11:53:57,533: t15.2023.10.06 val PER: 0.4123
2026-01-08 11:53:57,533: t15.2023.10.08 val PER: 0.5413
2026-01-08 11:53:57,533: t15.2023.10.13 val PER: 0.5601
2026-01-08 11:53:57,533: t15.2023.10.15 val PER: 0.4647
2026-01-08 11:53:57,533: t15.2023.10.20 val PER: 0.4262
2026-01-08 11:53:57,533: t15.2023.10.22 val PER: 0.4265
2026-01-08 11:53:57,533: t15.2023.11.03 val PER: 0.4681
2026-01-08 11:53:57,533: t15.2023.11.04 val PER: 0.2594
2026-01-08 11:53:57,534: t15.2023.11.17 val PER: 0.3453
2026-01-08 11:53:57,534: t15.2023.11.19 val PER: 0.3234
2026-01-08 11:53:57,534: t15.2023.11.26 val PER: 0.5275
2026-01-08 11:53:57,534: t15.2023.12.03 val PER: 0.4800
2026-01-08 11:53:57,534: t15.2023.12.08 val PER: 0.4867
2026-01-08 11:53:57,534: t15.2023.12.10 val PER: 0.4205
2026-01-08 11:53:57,534: t15.2023.12.17 val PER: 0.4979
2026-01-08 11:53:57,534: t15.2023.12.29 val PER: 0.4852
2026-01-08 11:53:57,534: t15.2024.02.25 val PER: 0.4284
2026-01-08 11:53:57,534: t15.2024.03.08 val PER: 0.5761
2026-01-08 11:53:57,534: t15.2024.03.15 val PER: 0.5128
2026-01-08 11:53:57,534: t15.2024.03.17 val PER: 0.4895
2026-01-08 11:53:57,534: t15.2024.05.10 val PER: 0.4844
2026-01-08 11:53:57,534: t15.2024.06.14 val PER: 0.4763
2026-01-08 11:53:57,534: t15.2024.07.19 val PER: 0.6295
2026-01-08 11:53:57,535: t15.2024.07.21 val PER: 0.4303
2026-01-08 11:53:57,535: t15.2024.07.28 val PER: 0.4846
2026-01-08 11:53:57,535: t15.2025.01.10 val PER: 0.7094
2026-01-08 11:53:57,535: t15.2025.01.12 val PER: 0.5358
2026-01-08 11:53:57,535: t15.2025.03.14 val PER: 0.6997
2026-01-08 11:53:57,535: t15.2025.03.16 val PER: 0.5157
2026-01-08 11:53:57,535: t15.2025.03.30 val PER: 0.7138
2026-01-08 11:53:57,535: t15.2025.04.13 val PER: 0.5521
2026-01-08 11:53:57,537: New best val WER(1gram) 99.75% --> 99.24%
2026-01-08 11:53:57,537: Checkpointing model
2026-01-08 11:53:57,677: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_linderman/checkpoint/best_checkpoint
2026-01-08 11:54:14,211: Train batch 1200: loss: 44.92 grad norm: 87.34 time: 0.070
2026-01-08 11:54:30,875: Train batch 1400: loss: 44.38 grad norm: 83.80 time: 0.063
2026-01-08 11:54:39,099: Running test after training batch: 1500
2026-01-08 11:54:39,236: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:54:43,973: WER debug example
  GT : you can see the code at this point as well
  PR : hughley scantiness the quixote hottest pointers
2026-01-08 11:54:44,015: WER debug example
  GT : how does it keep the cost down
  PR : leasehold spicy hitz kothe
2026-01-08 11:54:46,401: Val batch 1500: PER (avg): 0.4352 CTC Loss (avg): 46.1668 WER(1gram): 97.97% (n=64) time: 7.302
2026-01-08 11:54:46,401: WER lens: avg_true_words=6.16 avg_pred_words=4.44 max_pred_words=10
2026-01-08 11:54:46,402: t15.2023.08.13 val PER: 0.4096
2026-01-08 11:54:46,402: t15.2023.08.18 val PER: 0.3814
2026-01-08 11:54:46,402: t15.2023.08.20 val PER: 0.3900
2026-01-08 11:54:46,402: t15.2023.08.25 val PER: 0.3389
2026-01-08 11:54:46,402: t15.2023.08.27 val PER: 0.4646
2026-01-08 11:54:46,402: t15.2023.09.01 val PER: 0.3661
2026-01-08 11:54:46,402: t15.2023.09.03 val PER: 0.4430
2026-01-08 11:54:46,402: t15.2023.09.24 val PER: 0.3544
2026-01-08 11:54:46,402: t15.2023.09.29 val PER: 0.3842
2026-01-08 11:54:46,402: t15.2023.10.01 val PER: 0.4333
2026-01-08 11:54:46,403: t15.2023.10.06 val PER: 0.3520
2026-01-08 11:54:46,403: t15.2023.10.08 val PER: 0.4763
2026-01-08 11:54:46,403: t15.2023.10.13 val PER: 0.4856
2026-01-08 11:54:46,403: t15.2023.10.15 val PER: 0.4232
2026-01-08 11:54:46,403: t15.2023.10.20 val PER: 0.4161
2026-01-08 11:54:46,403: t15.2023.10.22 val PER: 0.3775
2026-01-08 11:54:46,403: t15.2023.11.03 val PER: 0.4186
2026-01-08 11:54:46,403: t15.2023.11.04 val PER: 0.1877
2026-01-08 11:54:46,403: t15.2023.11.17 val PER: 0.2846
2026-01-08 11:54:46,403: t15.2023.11.19 val PER: 0.2435
2026-01-08 11:54:46,403: t15.2023.11.26 val PER: 0.4746
2026-01-08 11:54:46,403: t15.2023.12.03 val PER: 0.4149
2026-01-08 11:54:46,403: t15.2023.12.08 val PER: 0.4328
2026-01-08 11:54:46,403: t15.2023.12.10 val PER: 0.3653
2026-01-08 11:54:46,403: t15.2023.12.17 val PER: 0.4262
2026-01-08 11:54:46,403: t15.2023.12.29 val PER: 0.4372
2026-01-08 11:54:46,403: t15.2024.02.25 val PER: 0.3919
2026-01-08 11:54:46,404: t15.2024.03.08 val PER: 0.5021
2026-01-08 11:54:46,404: t15.2024.03.15 val PER: 0.4540
2026-01-08 11:54:46,404: t15.2024.03.17 val PER: 0.4407
2026-01-08 11:54:46,404: t15.2024.05.10 val PER: 0.4324
2026-01-08 11:54:46,404: t15.2024.06.14 val PER: 0.4369
2026-01-08 11:54:46,404: t15.2024.07.19 val PER: 0.5781
2026-01-08 11:54:46,404: t15.2024.07.21 val PER: 0.3779
2026-01-08 11:54:46,404: t15.2024.07.28 val PER: 0.4199
2026-01-08 11:54:46,404: t15.2025.01.10 val PER: 0.6570
2026-01-08 11:54:46,404: t15.2025.01.12 val PER: 0.4750
2026-01-08 11:54:46,404: t15.2025.03.14 val PER: 0.6464
2026-01-08 11:54:46,404: t15.2025.03.16 val PER: 0.4647
2026-01-08 11:54:46,404: t15.2025.03.30 val PER: 0.6678
2026-01-08 11:54:46,404: t15.2025.04.13 val PER: 0.5050
2026-01-08 11:54:46,405: New best val WER(1gram) 99.24% --> 97.97%
2026-01-08 11:54:46,406: Checkpointing model
2026-01-08 11:54:46,543: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_linderman/checkpoint/best_checkpoint
2026-01-08 11:54:54,728: Train batch 1600: loss: 43.80 grad norm: 75.46 time: 0.066
2026-01-08 11:55:11,441: Train batch 1800: loss: 42.59 grad norm: 82.60 time: 0.092
2026-01-08 11:55:28,450: Train batch 2000: loss: 40.60 grad norm: 78.39 time: 0.069
2026-01-08 11:55:28,450: Running test after training batch: 2000
2026-01-08 11:55:28,579: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:55:33,316: WER debug example
  GT : you can see the code at this point as well
  PR : hudy caddy cetus buttonholed hotspot his swells
2026-01-08 11:55:33,356: WER debug example
  GT : how does it keep the cost down
  PR : tiede headstarts hitty sikhs that squats
2026-01-08 11:55:35,438: Val batch 2000: PER (avg): 0.3999 CTC Loss (avg): 41.9381 WER(1gram): 99.49% (n=64) time: 6.988
2026-01-08 11:55:35,439: WER lens: avg_true_words=6.16 avg_pred_words=4.84 max_pred_words=11
2026-01-08 11:55:35,439: t15.2023.08.13 val PER: 0.3794
2026-01-08 11:55:35,439: t15.2023.08.18 val PER: 0.3453
2026-01-08 11:55:35,439: t15.2023.08.20 val PER: 0.3550
2026-01-08 11:55:35,439: t15.2023.08.25 val PER: 0.3148
2026-01-08 11:55:35,439: t15.2023.08.27 val PER: 0.4180
2026-01-08 11:55:35,439: t15.2023.09.01 val PER: 0.3360
2026-01-08 11:55:35,439: t15.2023.09.03 val PER: 0.4228
2026-01-08 11:55:35,439: t15.2023.09.24 val PER: 0.3313
2026-01-08 11:55:35,439: t15.2023.09.29 val PER: 0.3510
2026-01-08 11:55:35,439: t15.2023.10.01 val PER: 0.3831
2026-01-08 11:55:35,440: t15.2023.10.06 val PER: 0.3208
2026-01-08 11:55:35,440: t15.2023.10.08 val PER: 0.4614
2026-01-08 11:55:35,440: t15.2023.10.13 val PER: 0.4577
2026-01-08 11:55:35,440: t15.2023.10.15 val PER: 0.3797
2026-01-08 11:55:35,440: t15.2023.10.20 val PER: 0.3725
2026-01-08 11:55:35,440: t15.2023.10.22 val PER: 0.3508
2026-01-08 11:55:35,440: t15.2023.11.03 val PER: 0.3908
2026-01-08 11:55:35,440: t15.2023.11.04 val PER: 0.1843
2026-01-08 11:55:35,440: t15.2023.11.17 val PER: 0.2364
2026-01-08 11:55:35,440: t15.2023.11.19 val PER: 0.2395
2026-01-08 11:55:35,440: t15.2023.11.26 val PER: 0.4435
2026-01-08 11:55:35,440: t15.2023.12.03 val PER: 0.4034
2026-01-08 11:55:35,440: t15.2023.12.08 val PER: 0.3862
2026-01-08 11:55:35,440: t15.2023.12.10 val PER: 0.3351
2026-01-08 11:55:35,441: t15.2023.12.17 val PER: 0.3742
2026-01-08 11:55:35,441: t15.2023.12.29 val PER: 0.3898
2026-01-08 11:55:35,441: t15.2024.02.25 val PER: 0.3539
2026-01-08 11:55:35,441: t15.2024.03.08 val PER: 0.4723
2026-01-08 11:55:35,441: t15.2024.03.15 val PER: 0.4103
2026-01-08 11:55:35,441: t15.2024.03.17 val PER: 0.4059
2026-01-08 11:55:35,441: t15.2024.05.10 val PER: 0.3967
2026-01-08 11:55:35,441: t15.2024.06.14 val PER: 0.4148
2026-01-08 11:55:35,441: t15.2024.07.19 val PER: 0.5175
2026-01-08 11:55:35,441: t15.2024.07.21 val PER: 0.3545
2026-01-08 11:55:35,441: t15.2024.07.28 val PER: 0.3853
2026-01-08 11:55:35,441: t15.2025.01.10 val PER: 0.5978
2026-01-08 11:55:35,441: t15.2025.01.12 val PER: 0.4450
2026-01-08 11:55:35,441: t15.2025.03.14 val PER: 0.6006
2026-01-08 11:55:35,441: t15.2025.03.16 val PER: 0.4529
2026-01-08 11:55:35,441: t15.2025.03.30 val PER: 0.6057
2026-01-08 11:55:35,442: t15.2025.04.13 val PER: 0.4708
2026-01-08 11:55:52,275: Train batch 2200: loss: 38.45 grad norm: 76.78 time: 0.062
2026-01-08 11:56:09,649: Train batch 2400: loss: 37.79 grad norm: 88.04 time: 0.054
2026-01-08 11:56:18,337: Running test after training batch: 2500
2026-01-08 11:56:18,481: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:56:23,387: WER debug example
  GT : you can see the code at this point as well
  PR : hughley candy siess the steelhead hatt cysts ponds swells
2026-01-08 11:56:23,420: WER debug example
  GT : how does it keep the cost down
  PR : headstarts hitty skinks that squats
2026-01-08 11:56:25,076: Val batch 2500: PER (avg): 0.3689 CTC Loss (avg): 39.0847 WER(1gram): 97.46% (n=64) time: 6.738
2026-01-08 11:56:25,076: WER lens: avg_true_words=6.16 avg_pred_words=4.91 max_pred_words=10
2026-01-08 11:56:25,076: t15.2023.08.13 val PER: 0.3628
2026-01-08 11:56:25,076: t15.2023.08.18 val PER: 0.3277
2026-01-08 11:56:25,076: t15.2023.08.20 val PER: 0.3137
2026-01-08 11:56:25,077: t15.2023.08.25 val PER: 0.2922
2026-01-08 11:56:25,077: t15.2023.08.27 val PER: 0.4003
2026-01-08 11:56:25,077: t15.2023.09.01 val PER: 0.3109
2026-01-08 11:56:25,077: t15.2023.09.03 val PER: 0.3717
2026-01-08 11:56:25,077: t15.2023.09.24 val PER: 0.2876
2026-01-08 11:56:25,077: t15.2023.09.29 val PER: 0.3159
2026-01-08 11:56:25,077: t15.2023.10.01 val PER: 0.3527
2026-01-08 11:56:25,077: t15.2023.10.06 val PER: 0.2939
2026-01-08 11:56:25,077: t15.2023.10.08 val PER: 0.4452
2026-01-08 11:56:25,077: t15.2023.10.13 val PER: 0.4329
2026-01-08 11:56:25,077: t15.2023.10.15 val PER: 0.3540
2026-01-08 11:56:25,077: t15.2023.10.20 val PER: 0.3389
2026-01-08 11:56:25,078: t15.2023.10.22 val PER: 0.3318
2026-01-08 11:56:25,078: t15.2023.11.03 val PER: 0.3765
2026-01-08 11:56:25,078: t15.2023.11.04 val PER: 0.1536
2026-01-08 11:56:25,078: t15.2023.11.17 val PER: 0.2131
2026-01-08 11:56:25,078: t15.2023.11.19 val PER: 0.2295
2026-01-08 11:56:25,078: t15.2023.11.26 val PER: 0.4087
2026-01-08 11:56:25,078: t15.2023.12.03 val PER: 0.3372
2026-01-08 11:56:25,078: t15.2023.12.08 val PER: 0.3509
2026-01-08 11:56:25,078: t15.2023.12.10 val PER: 0.2943
2026-01-08 11:56:25,078: t15.2023.12.17 val PER: 0.3399
2026-01-08 11:56:25,078: t15.2023.12.29 val PER: 0.3665
2026-01-08 11:56:25,078: t15.2024.02.25 val PER: 0.3202
2026-01-08 11:56:25,078: t15.2024.03.08 val PER: 0.4097
2026-01-08 11:56:25,078: t15.2024.03.15 val PER: 0.3796
2026-01-08 11:56:25,078: t15.2024.03.17 val PER: 0.3780
2026-01-08 11:56:25,078: t15.2024.05.10 val PER: 0.3507
2026-01-08 11:56:25,079: t15.2024.06.14 val PER: 0.3707
2026-01-08 11:56:25,079: t15.2024.07.19 val PER: 0.4647
2026-01-08 11:56:25,079: t15.2024.07.21 val PER: 0.3207
2026-01-08 11:56:25,079: t15.2024.07.28 val PER: 0.3456
2026-01-08 11:56:25,079: t15.2025.01.10 val PER: 0.5620
2026-01-08 11:56:25,079: t15.2025.01.12 val PER: 0.4326
2026-01-08 11:56:25,079: t15.2025.03.14 val PER: 0.5621
2026-01-08 11:56:25,079: t15.2025.03.16 val PER: 0.4175
2026-01-08 11:56:25,079: t15.2025.03.30 val PER: 0.5862
2026-01-08 11:56:25,079: t15.2025.04.13 val PER: 0.4679
2026-01-08 11:56:25,081: New best val WER(1gram) 97.97% --> 97.46%
2026-01-08 11:56:25,081: Checkpointing model
2026-01-08 11:56:25,221: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_linderman/checkpoint/best_checkpoint
2026-01-08 11:56:33,580: Train batch 2600: loss: 44.21 grad norm: 87.94 time: 0.058
2026-01-08 11:56:51,005: Train batch 2800: loss: 34.59 grad norm: 82.54 time: 0.085
2026-01-08 11:57:07,581: Train batch 3000: loss: 38.38 grad norm: 78.39 time: 0.086
2026-01-08 11:57:07,581: Running test after training batch: 3000
2026-01-08 11:57:07,674: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:57:12,381: WER debug example
  GT : you can see the code at this point as well
  PR : udy candidacies smutty hood hatlestad pandas swells
2026-01-08 11:57:12,407: WER debug example
  GT : how does it keep the cost down
  PR : toehold city six the teast setzer
2026-01-08 11:57:14,270: Val batch 3000: PER (avg): 0.3569 CTC Loss (avg): 36.9709 WER(1gram): 97.97% (n=64) time: 6.688
2026-01-08 11:57:14,270: WER lens: avg_true_words=6.16 avg_pred_words=5.30 max_pred_words=11
2026-01-08 11:57:14,270: t15.2023.08.13 val PER: 0.3285
2026-01-08 11:57:14,270: t15.2023.08.18 val PER: 0.3026
2026-01-08 11:57:14,270: t15.2023.08.20 val PER: 0.3066
2026-01-08 11:57:14,270: t15.2023.08.25 val PER: 0.2831
2026-01-08 11:57:14,271: t15.2023.08.27 val PER: 0.3810
2026-01-08 11:57:14,271: t15.2023.09.01 val PER: 0.2914
2026-01-08 11:57:14,271: t15.2023.09.03 val PER: 0.3824
2026-01-08 11:57:14,271: t15.2023.09.24 val PER: 0.2816
2026-01-08 11:57:14,271: t15.2023.09.29 val PER: 0.3089
2026-01-08 11:57:14,271: t15.2023.10.01 val PER: 0.3408
2026-01-08 11:57:14,271: t15.2023.10.06 val PER: 0.2820
2026-01-08 11:57:14,271: t15.2023.10.08 val PER: 0.4263
2026-01-08 11:57:14,271: t15.2023.10.13 val PER: 0.4267
2026-01-08 11:57:14,271: t15.2023.10.15 val PER: 0.3401
2026-01-08 11:57:14,272: t15.2023.10.20 val PER: 0.3389
2026-01-08 11:57:14,272: t15.2023.10.22 val PER: 0.3029
2026-01-08 11:57:14,272: t15.2023.11.03 val PER: 0.3467
2026-01-08 11:57:14,272: t15.2023.11.04 val PER: 0.1672
2026-01-08 11:57:14,272: t15.2023.11.17 val PER: 0.2208
2026-01-08 11:57:14,272: t15.2023.11.19 val PER: 0.2216
2026-01-08 11:57:14,272: t15.2023.11.26 val PER: 0.3862
2026-01-08 11:57:14,272: t15.2023.12.03 val PER: 0.3508
2026-01-08 11:57:14,272: t15.2023.12.08 val PER: 0.3395
2026-01-08 11:57:14,272: t15.2023.12.10 val PER: 0.2852
2026-01-08 11:57:14,272: t15.2023.12.17 val PER: 0.3524
2026-01-08 11:57:14,272: t15.2023.12.29 val PER: 0.3548
2026-01-08 11:57:14,272: t15.2024.02.25 val PER: 0.3188
2026-01-08 11:57:14,272: t15.2024.03.08 val PER: 0.4011
2026-01-08 11:57:14,272: t15.2024.03.15 val PER: 0.3671
2026-01-08 11:57:14,272: t15.2024.03.17 val PER: 0.3752
2026-01-08 11:57:14,273: t15.2024.05.10 val PER: 0.3447
2026-01-08 11:57:14,273: t15.2024.06.14 val PER: 0.3580
2026-01-08 11:57:14,273: t15.2024.07.19 val PER: 0.4661
2026-01-08 11:57:14,273: t15.2024.07.21 val PER: 0.3048
2026-01-08 11:57:14,273: t15.2024.07.28 val PER: 0.3390
2026-01-08 11:57:14,273: t15.2025.01.10 val PER: 0.5496
2026-01-08 11:57:14,273: t15.2025.01.12 val PER: 0.4188
2026-01-08 11:57:14,273: t15.2025.03.14 val PER: 0.5577
2026-01-08 11:57:14,273: t15.2025.03.16 val PER: 0.3848
2026-01-08 11:57:14,273: t15.2025.03.30 val PER: 0.5379
2026-01-08 11:57:14,273: t15.2025.04.13 val PER: 0.4379
2026-01-08 11:57:32,359: Train batch 3200: loss: 31.07 grad norm: 66.73 time: 0.079
2026-01-08 11:57:50,299: Train batch 3400: loss: 26.56 grad norm: 58.70 time: 0.051
2026-01-08 11:57:59,120: Running test after training batch: 3500
2026-01-08 11:57:59,240: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:58:03,977: WER debug example
  GT : you can see the code at this point as well
  PR : euclea candy sikhs the steelhead hatz points his swells
2026-01-08 11:58:04,007: WER debug example
  GT : how does it keep the cost down
  PR : leasehold surcease hitty skinks thus cassettes
2026-01-08 11:58:05,497: Val batch 3500: PER (avg): 0.3413 CTC Loss (avg): 34.8407 WER(1gram): 100.00% (n=64) time: 6.377
2026-01-08 11:58:05,497: WER lens: avg_true_words=6.16 avg_pred_words=5.61 max_pred_words=11
2026-01-08 11:58:05,497: t15.2023.08.13 val PER: 0.3222
2026-01-08 11:58:05,498: t15.2023.08.18 val PER: 0.3068
2026-01-08 11:58:05,498: t15.2023.08.20 val PER: 0.3010
2026-01-08 11:58:05,498: t15.2023.08.25 val PER: 0.2620
2026-01-08 11:58:05,498: t15.2023.08.27 val PER: 0.3762
2026-01-08 11:58:05,498: t15.2023.09.01 val PER: 0.2825
2026-01-08 11:58:05,498: t15.2023.09.03 val PER: 0.3551
2026-01-08 11:58:05,498: t15.2023.09.24 val PER: 0.2852
2026-01-08 11:58:05,498: t15.2023.09.29 val PER: 0.2904
2026-01-08 11:58:05,498: t15.2023.10.01 val PER: 0.3210
2026-01-08 11:58:05,498: t15.2023.10.06 val PER: 0.2766
2026-01-08 11:58:05,498: t15.2023.10.08 val PER: 0.4181
2026-01-08 11:58:05,499: t15.2023.10.13 val PER: 0.4088
2026-01-08 11:58:05,499: t15.2023.10.15 val PER: 0.3256
2026-01-08 11:58:05,499: t15.2023.10.20 val PER: 0.3121
2026-01-08 11:58:05,499: t15.2023.10.22 val PER: 0.2984
2026-01-08 11:58:05,499: t15.2023.11.03 val PER: 0.3385
2026-01-08 11:58:05,499: t15.2023.11.04 val PER: 0.1502
2026-01-08 11:58:05,499: t15.2023.11.17 val PER: 0.1913
2026-01-08 11:58:05,500: t15.2023.11.19 val PER: 0.1916
2026-01-08 11:58:05,500: t15.2023.11.26 val PER: 0.3536
2026-01-08 11:58:05,500: t15.2023.12.03 val PER: 0.3141
2026-01-08 11:58:05,500: t15.2023.12.08 val PER: 0.3162
2026-01-08 11:58:05,500: t15.2023.12.10 val PER: 0.2720
2026-01-08 11:58:05,500: t15.2023.12.17 val PER: 0.3181
2026-01-08 11:58:05,501: t15.2023.12.29 val PER: 0.3384
2026-01-08 11:58:05,501: t15.2024.02.25 val PER: 0.3132
2026-01-08 11:58:05,501: t15.2024.03.08 val PER: 0.4182
2026-01-08 11:58:05,501: t15.2024.03.15 val PER: 0.3627
2026-01-08 11:58:05,501: t15.2024.03.17 val PER: 0.3584
2026-01-08 11:58:05,501: t15.2024.05.10 val PER: 0.3314
2026-01-08 11:58:05,501: t15.2024.06.14 val PER: 0.3486
2026-01-08 11:58:05,501: t15.2024.07.19 val PER: 0.4423
2026-01-08 11:58:05,501: t15.2024.07.21 val PER: 0.2848
2026-01-08 11:58:05,502: t15.2024.07.28 val PER: 0.3324
2026-01-08 11:58:05,502: t15.2025.01.10 val PER: 0.5000
2026-01-08 11:58:05,502: t15.2025.01.12 val PER: 0.3918
2026-01-08 11:58:05,502: t15.2025.03.14 val PER: 0.5074
2026-01-08 11:58:05,502: t15.2025.03.16 val PER: 0.3874
2026-01-08 11:58:05,502: t15.2025.03.30 val PER: 0.5299
2026-01-08 11:58:05,502: t15.2025.04.13 val PER: 0.4308
2026-01-08 11:58:14,065: Train batch 3600: loss: 30.65 grad norm: 71.75 time: 0.068
2026-01-08 11:58:31,320: Train batch 3800: loss: 35.62 grad norm: 82.08 time: 0.068
2026-01-08 11:58:48,429: Train batch 4000: loss: 25.47 grad norm: 60.24 time: 0.058
2026-01-08 11:58:48,429: Running test after training batch: 4000
2026-01-08 11:58:48,574: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:58:53,587: WER debug example
  GT : you can see the code at this point as well
  PR : utica deceits the toehold hatz disappoints his swells
2026-01-08 11:58:53,613: WER debug example
  GT : how does it keep the cost down
  PR : shead sitz hix the tsetse
2026-01-08 11:58:54,979: Val batch 4000: PER (avg): 0.3217 CTC Loss (avg): 33.0308 WER(1gram): 97.72% (n=64) time: 6.550
2026-01-08 11:58:54,980: WER lens: avg_true_words=6.16 avg_pred_words=5.33 max_pred_words=11
2026-01-08 11:58:54,980: t15.2023.08.13 val PER: 0.3077
2026-01-08 11:58:54,980: t15.2023.08.18 val PER: 0.2942
2026-01-08 11:58:54,980: t15.2023.08.20 val PER: 0.2828
2026-01-08 11:58:54,980: t15.2023.08.25 val PER: 0.2485
2026-01-08 11:58:54,980: t15.2023.08.27 val PER: 0.3810
2026-01-08 11:58:54,980: t15.2023.09.01 val PER: 0.2630
2026-01-08 11:58:54,980: t15.2023.09.03 val PER: 0.3349
2026-01-08 11:58:54,981: t15.2023.09.24 val PER: 0.2706
2026-01-08 11:58:54,981: t15.2023.09.29 val PER: 0.2744
2026-01-08 11:58:54,981: t15.2023.10.01 val PER: 0.3131
2026-01-08 11:58:54,981: t15.2023.10.06 val PER: 0.2616
2026-01-08 11:58:54,981: t15.2023.10.08 val PER: 0.4019
2026-01-08 11:58:54,981: t15.2023.10.13 val PER: 0.3685
2026-01-08 11:58:54,981: t15.2023.10.15 val PER: 0.3065
2026-01-08 11:58:54,981: t15.2023.10.20 val PER: 0.2785
2026-01-08 11:58:54,981: t15.2023.10.22 val PER: 0.2739
2026-01-08 11:58:54,981: t15.2023.11.03 val PER: 0.3229
2026-01-08 11:58:54,981: t15.2023.11.04 val PER: 0.1468
2026-01-08 11:58:54,981: t15.2023.11.17 val PER: 0.1851
2026-01-08 11:58:54,981: t15.2023.11.19 val PER: 0.1816
2026-01-08 11:58:54,981: t15.2023.11.26 val PER: 0.3413
2026-01-08 11:58:54,982: t15.2023.12.03 val PER: 0.2952
2026-01-08 11:58:54,982: t15.2023.12.08 val PER: 0.2830
2026-01-08 11:58:54,982: t15.2023.12.10 val PER: 0.2654
2026-01-08 11:58:54,982: t15.2023.12.17 val PER: 0.3077
2026-01-08 11:58:54,982: t15.2023.12.29 val PER: 0.3198
2026-01-08 11:58:54,982: t15.2024.02.25 val PER: 0.2781
2026-01-08 11:58:54,982: t15.2024.03.08 val PER: 0.4026
2026-01-08 11:58:54,982: t15.2024.03.15 val PER: 0.3421
2026-01-08 11:58:54,982: t15.2024.03.17 val PER: 0.3368
2026-01-08 11:58:54,982: t15.2024.05.10 val PER: 0.3195
2026-01-08 11:58:54,982: t15.2024.06.14 val PER: 0.3344
2026-01-08 11:58:54,982: t15.2024.07.19 val PER: 0.4192
2026-01-08 11:58:54,982: t15.2024.07.21 val PER: 0.2510
2026-01-08 11:58:54,982: t15.2024.07.28 val PER: 0.3059
2026-01-08 11:58:54,982: t15.2025.01.10 val PER: 0.4738
2026-01-08 11:58:54,982: t15.2025.01.12 val PER: 0.3764
2026-01-08 11:58:54,983: t15.2025.03.14 val PER: 0.4660
2026-01-08 11:58:54,983: t15.2025.03.16 val PER: 0.3678
2026-01-08 11:58:54,983: t15.2025.03.30 val PER: 0.5023
2026-01-08 11:58:54,983: t15.2025.04.13 val PER: 0.3951
2026-01-08 11:59:12,330: Train batch 4200: loss: 30.39 grad norm: 67.98 time: 0.081
2026-01-08 11:59:29,917: Train batch 4400: loss: 24.45 grad norm: 57.82 time: 0.068
2026-01-08 11:59:38,857: Running test after training batch: 4500
2026-01-08 11:59:38,985: WER debug GT example: You can see the code at this point as well.
2026-01-08 11:59:43,711: WER debug example
  GT : you can see the code at this point as well
  PR : euclea candies sikhs the toehold hatt cyst appointees his swells
2026-01-08 11:59:43,734: WER debug example
  GT : how does it keep the cost down
  PR : toehold stacy hitz hix the teast setzer
2026-01-08 11:59:45,109: Val batch 4500: PER (avg): 0.3100 CTC Loss (avg): 32.1668 WER(1gram): 100.00% (n=64) time: 6.251
2026-01-08 11:59:45,109: WER lens: avg_true_words=6.16 avg_pred_words=5.97 max_pred_words=11
2026-01-08 11:59:45,109: t15.2023.08.13 val PER: 0.2859
2026-01-08 11:59:45,109: t15.2023.08.18 val PER: 0.2833
2026-01-08 11:59:45,109: t15.2023.08.20 val PER: 0.2740
2026-01-08 11:59:45,109: t15.2023.08.25 val PER: 0.2515
2026-01-08 11:59:45,109: t15.2023.08.27 val PER: 0.3505
2026-01-08 11:59:45,110: t15.2023.09.01 val PER: 0.2630
2026-01-08 11:59:45,110: t15.2023.09.03 val PER: 0.3230
2026-01-08 11:59:45,110: t15.2023.09.24 val PER: 0.2476
2026-01-08 11:59:45,110: t15.2023.09.29 val PER: 0.2731
2026-01-08 11:59:45,110: t15.2023.10.01 val PER: 0.3058
2026-01-08 11:59:45,110: t15.2023.10.06 val PER: 0.2390
2026-01-08 11:59:45,110: t15.2023.10.08 val PER: 0.4073
2026-01-08 11:59:45,110: t15.2023.10.13 val PER: 0.3732
2026-01-08 11:59:45,110: t15.2023.10.15 val PER: 0.3013
2026-01-08 11:59:45,111: t15.2023.10.20 val PER: 0.2886
2026-01-08 11:59:45,111: t15.2023.10.22 val PER: 0.2506
2026-01-08 11:59:45,111: t15.2023.11.03 val PER: 0.3148
2026-01-08 11:59:45,111: t15.2023.11.04 val PER: 0.1365
2026-01-08 11:59:45,111: t15.2023.11.17 val PER: 0.1680
2026-01-08 11:59:45,111: t15.2023.11.19 val PER: 0.1856
2026-01-08 11:59:45,111: t15.2023.11.26 val PER: 0.3333
2026-01-08 11:59:45,111: t15.2023.12.03 val PER: 0.2868
2026-01-08 11:59:45,111: t15.2023.12.08 val PER: 0.2763
2026-01-08 11:59:45,112: t15.2023.12.10 val PER: 0.2497
2026-01-08 11:59:45,112: t15.2023.12.17 val PER: 0.2921
2026-01-08 11:59:45,112: t15.2023.12.29 val PER: 0.3116
2026-01-08 11:59:45,112: t15.2024.02.25 val PER: 0.2612
2026-01-08 11:59:45,112: t15.2024.03.08 val PER: 0.3841
2026-01-08 11:59:45,112: t15.2024.03.15 val PER: 0.3383
2026-01-08 11:59:45,112: t15.2024.03.17 val PER: 0.3159
2026-01-08 11:59:45,112: t15.2024.05.10 val PER: 0.3210
2026-01-08 11:59:45,113: t15.2024.06.14 val PER: 0.3155
2026-01-08 11:59:45,113: t15.2024.07.19 val PER: 0.3975
2026-01-08 11:59:45,113: t15.2024.07.21 val PER: 0.2331
2026-01-08 11:59:45,113: t15.2024.07.28 val PER: 0.2897
2026-01-08 11:59:45,113: t15.2025.01.10 val PER: 0.4725
2026-01-08 11:59:45,113: t15.2025.01.12 val PER: 0.3495
2026-01-08 11:59:45,113: t15.2025.03.14 val PER: 0.4541
2026-01-08 11:59:45,113: t15.2025.03.16 val PER: 0.3560
2026-01-08 11:59:45,114: t15.2025.03.30 val PER: 0.4644
2026-01-08 11:59:45,114: t15.2025.04.13 val PER: 0.3809
2026-01-08 11:59:53,743: Train batch 4600: loss: 27.05 grad norm: 70.14 time: 0.065
2026-01-08 12:00:12,036: Train batch 4800: loss: 21.83 grad norm: 59.82 time: 0.065
2026-01-08 12:00:29,239: Train batch 5000: loss: 38.47 grad norm: 84.35 time: 0.067
2026-01-08 12:00:29,239: Running test after training batch: 5000
2026-01-08 12:00:29,361: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:00:34,083: WER debug example
  GT : you can see the code at this point as well
  PR : cute kantz eats the tuckett hatt cysts points his
2026-01-08 12:00:34,106: WER debug example
  GT : how does it keep the cost down
  PR : toehold stacy hitz hix the teast setzer
2026-01-08 12:00:35,454: Val batch 5000: PER (avg): 0.2961 CTC Loss (avg): 30.4069 WER(1gram): 99.49% (n=64) time: 6.214
2026-01-08 12:00:35,454: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=11
2026-01-08 12:00:35,455: t15.2023.08.13 val PER: 0.2765
2026-01-08 12:00:35,455: t15.2023.08.18 val PER: 0.2615
2026-01-08 12:00:35,455: t15.2023.08.20 val PER: 0.2677
2026-01-08 12:00:35,455: t15.2023.08.25 val PER: 0.2395
2026-01-08 12:00:35,455: t15.2023.08.27 val PER: 0.3328
2026-01-08 12:00:35,455: t15.2023.09.01 val PER: 0.2403
2026-01-08 12:00:35,455: t15.2023.09.03 val PER: 0.3183
2026-01-08 12:00:35,455: t15.2023.09.24 val PER: 0.2549
2026-01-08 12:00:35,455: t15.2023.09.29 val PER: 0.2540
2026-01-08 12:00:35,455: t15.2023.10.01 val PER: 0.2893
2026-01-08 12:00:35,455: t15.2023.10.06 val PER: 0.2250
2026-01-08 12:00:35,455: t15.2023.10.08 val PER: 0.3857
2026-01-08 12:00:35,455: t15.2023.10.13 val PER: 0.3716
2026-01-08 12:00:35,456: t15.2023.10.15 val PER: 0.2986
2026-01-08 12:00:35,456: t15.2023.10.20 val PER: 0.2685
2026-01-08 12:00:35,456: t15.2023.10.22 val PER: 0.2528
2026-01-08 12:00:35,456: t15.2023.11.03 val PER: 0.3100
2026-01-08 12:00:35,456: t15.2023.11.04 val PER: 0.1433
2026-01-08 12:00:35,456: t15.2023.11.17 val PER: 0.1664
2026-01-08 12:00:35,456: t15.2023.11.19 val PER: 0.1617
2026-01-08 12:00:35,456: t15.2023.11.26 val PER: 0.3167
2026-01-08 12:00:35,456: t15.2023.12.03 val PER: 0.2658
2026-01-08 12:00:35,456: t15.2023.12.08 val PER: 0.2636
2026-01-08 12:00:35,456: t15.2023.12.10 val PER: 0.2352
2026-01-08 12:00:35,456: t15.2023.12.17 val PER: 0.2682
2026-01-08 12:00:35,456: t15.2023.12.29 val PER: 0.3040
2026-01-08 12:00:35,456: t15.2024.02.25 val PER: 0.2360
2026-01-08 12:00:35,456: t15.2024.03.08 val PER: 0.3798
2026-01-08 12:00:35,456: t15.2024.03.15 val PER: 0.3064
2026-01-08 12:00:35,457: t15.2024.03.17 val PER: 0.3180
2026-01-08 12:00:35,457: t15.2024.05.10 val PER: 0.2868
2026-01-08 12:00:35,457: t15.2024.06.14 val PER: 0.2950
2026-01-08 12:00:35,457: t15.2024.07.19 val PER: 0.3790
2026-01-08 12:00:35,457: t15.2024.07.21 val PER: 0.2159
2026-01-08 12:00:35,457: t15.2024.07.28 val PER: 0.2779
2026-01-08 12:00:35,457: t15.2025.01.10 val PER: 0.4490
2026-01-08 12:00:35,457: t15.2025.01.12 val PER: 0.3264
2026-01-08 12:00:35,457: t15.2025.03.14 val PER: 0.4305
2026-01-08 12:00:35,457: t15.2025.03.16 val PER: 0.3495
2026-01-08 12:00:35,457: t15.2025.03.30 val PER: 0.4345
2026-01-08 12:00:35,457: t15.2025.04.13 val PER: 0.3780
2026-01-08 12:00:53,541: Train batch 5200: loss: 23.60 grad norm: 64.42 time: 0.054
2026-01-08 12:01:11,054: Train batch 5400: loss: 25.22 grad norm: 63.51 time: 0.071
2026-01-08 12:01:19,541: Running test after training batch: 5500
2026-01-08 12:01:19,697: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:01:24,572: WER debug example
  GT : you can see the code at this point as well
  PR : euclea canned siess the stuccoed hatt cyst appoints his swells
2026-01-08 12:01:24,595: WER debug example
  GT : how does it keep the cost down
  PR : shead sta sitz hix the tsetse
2026-01-08 12:01:25,935: Val batch 5500: PER (avg): 0.2905 CTC Loss (avg): 29.3440 WER(1gram): 97.97% (n=64) time: 6.393
2026-01-08 12:01:25,935: WER lens: avg_true_words=6.16 avg_pred_words=5.64 max_pred_words=11
2026-01-08 12:01:25,935: t15.2023.08.13 val PER: 0.2568
2026-01-08 12:01:25,935: t15.2023.08.18 val PER: 0.2565
2026-01-08 12:01:25,935: t15.2023.08.20 val PER: 0.2581
2026-01-08 12:01:25,935: t15.2023.08.25 val PER: 0.2410
2026-01-08 12:01:25,936: t15.2023.08.27 val PER: 0.3392
2026-01-08 12:01:25,936: t15.2023.09.01 val PER: 0.2362
2026-01-08 12:01:25,936: t15.2023.09.03 val PER: 0.3290
2026-01-08 12:01:25,936: t15.2023.09.24 val PER: 0.2403
2026-01-08 12:01:25,936: t15.2023.09.29 val PER: 0.2591
2026-01-08 12:01:25,936: t15.2023.10.01 val PER: 0.2926
2026-01-08 12:01:25,936: t15.2023.10.06 val PER: 0.2293
2026-01-08 12:01:25,936: t15.2023.10.08 val PER: 0.3829
2026-01-08 12:01:25,936: t15.2023.10.13 val PER: 0.3638
2026-01-08 12:01:25,936: t15.2023.10.15 val PER: 0.2861
2026-01-08 12:01:25,936: t15.2023.10.20 val PER: 0.2919
2026-01-08 12:01:25,936: t15.2023.10.22 val PER: 0.2372
2026-01-08 12:01:25,937: t15.2023.11.03 val PER: 0.2978
2026-01-08 12:01:25,937: t15.2023.11.04 val PER: 0.1229
2026-01-08 12:01:25,937: t15.2023.11.17 val PER: 0.1633
2026-01-08 12:01:25,937: t15.2023.11.19 val PER: 0.1816
2026-01-08 12:01:25,937: t15.2023.11.26 val PER: 0.3007
2026-01-08 12:01:25,937: t15.2023.12.03 val PER: 0.2595
2026-01-08 12:01:25,937: t15.2023.12.08 val PER: 0.2570
2026-01-08 12:01:25,937: t15.2023.12.10 val PER: 0.2247
2026-01-08 12:01:25,937: t15.2023.12.17 val PER: 0.2609
2026-01-08 12:01:25,937: t15.2023.12.29 val PER: 0.2931
2026-01-08 12:01:25,937: t15.2024.02.25 val PER: 0.2472
2026-01-08 12:01:25,937: t15.2024.03.08 val PER: 0.3656
2026-01-08 12:01:25,937: t15.2024.03.15 val PER: 0.3158
2026-01-08 12:01:25,937: t15.2024.03.17 val PER: 0.3013
2026-01-08 12:01:25,937: t15.2024.05.10 val PER: 0.2883
2026-01-08 12:01:25,937: t15.2024.06.14 val PER: 0.2855
2026-01-08 12:01:25,938: t15.2024.07.19 val PER: 0.3560
2026-01-08 12:01:25,938: t15.2024.07.21 val PER: 0.2145
2026-01-08 12:01:25,938: t15.2024.07.28 val PER: 0.2824
2026-01-08 12:01:25,938: t15.2025.01.10 val PER: 0.4490
2026-01-08 12:01:25,938: t15.2025.01.12 val PER: 0.3125
2026-01-08 12:01:25,938: t15.2025.03.14 val PER: 0.4186
2026-01-08 12:01:25,938: t15.2025.03.16 val PER: 0.3442
2026-01-08 12:01:25,938: t15.2025.03.30 val PER: 0.4368
2026-01-08 12:01:25,938: t15.2025.04.13 val PER: 0.3666
2026-01-08 12:01:34,719: Train batch 5600: loss: 26.59 grad norm: 74.62 time: 0.063
2026-01-08 12:01:51,822: Train batch 5800: loss: 19.52 grad norm: 61.89 time: 0.084
2026-01-08 12:02:08,586: Train batch 6000: loss: 20.29 grad norm: 57.46 time: 0.050
2026-01-08 12:02:08,586: Running test after training batch: 6000
2026-01-08 12:02:08,690: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:02:13,398: WER debug example
  GT : you can see the code at this point as well
  PR : utica deceits the stuccoed scat cyst appoints his swells
2026-01-08 12:02:13,421: WER debug example
  GT : how does it keep the cost down
  PR : shead sta sitz keep coos thus tsetse
2026-01-08 12:02:14,720: Val batch 6000: PER (avg): 0.2815 CTC Loss (avg): 28.8561 WER(1gram): 99.49% (n=64) time: 6.133
2026-01-08 12:02:14,720: WER lens: avg_true_words=6.16 avg_pred_words=5.83 max_pred_words=12
2026-01-08 12:02:14,720: t15.2023.08.13 val PER: 0.2651
2026-01-08 12:02:14,720: t15.2023.08.18 val PER: 0.2573
2026-01-08 12:02:14,720: t15.2023.08.20 val PER: 0.2581
2026-01-08 12:02:14,720: t15.2023.08.25 val PER: 0.2334
2026-01-08 12:02:14,720: t15.2023.08.27 val PER: 0.3296
2026-01-08 12:02:14,720: t15.2023.09.01 val PER: 0.2354
2026-01-08 12:02:14,721: t15.2023.09.03 val PER: 0.2922
2026-01-08 12:02:14,721: t15.2023.09.24 val PER: 0.2330
2026-01-08 12:02:14,721: t15.2023.09.29 val PER: 0.2534
2026-01-08 12:02:14,721: t15.2023.10.01 val PER: 0.2900
2026-01-08 12:02:14,721: t15.2023.10.06 val PER: 0.2164
2026-01-08 12:02:14,721: t15.2023.10.08 val PER: 0.3721
2026-01-08 12:02:14,721: t15.2023.10.13 val PER: 0.3382
2026-01-08 12:02:14,721: t15.2023.10.15 val PER: 0.2821
2026-01-08 12:02:14,721: t15.2023.10.20 val PER: 0.2819
2026-01-08 12:02:14,721: t15.2023.10.22 val PER: 0.2327
2026-01-08 12:02:14,727: t15.2023.11.03 val PER: 0.2856
2026-01-08 12:02:14,727: t15.2023.11.04 val PER: 0.1433
2026-01-08 12:02:14,727: t15.2023.11.17 val PER: 0.1602
2026-01-08 12:02:14,727: t15.2023.11.19 val PER: 0.1617
2026-01-08 12:02:14,727: t15.2023.11.26 val PER: 0.2957
2026-01-08 12:02:14,727: t15.2023.12.03 val PER: 0.2647
2026-01-08 12:02:14,728: t15.2023.12.08 val PER: 0.2583
2026-01-08 12:02:14,728: t15.2023.12.10 val PER: 0.2208
2026-01-08 12:02:14,728: t15.2023.12.17 val PER: 0.2526
2026-01-08 12:02:14,728: t15.2023.12.29 val PER: 0.2807
2026-01-08 12:02:14,728: t15.2024.02.25 val PER: 0.2233
2026-01-08 12:02:14,728: t15.2024.03.08 val PER: 0.3400
2026-01-08 12:02:14,728: t15.2024.03.15 val PER: 0.2977
2026-01-08 12:02:14,728: t15.2024.03.17 val PER: 0.2908
2026-01-08 12:02:14,728: t15.2024.05.10 val PER: 0.3001
2026-01-08 12:02:14,728: t15.2024.06.14 val PER: 0.2744
2026-01-08 12:02:14,728: t15.2024.07.19 val PER: 0.3500
2026-01-08 12:02:14,728: t15.2024.07.21 val PER: 0.1959
2026-01-08 12:02:14,728: t15.2024.07.28 val PER: 0.2654
2026-01-08 12:02:14,728: t15.2025.01.10 val PER: 0.4187
2026-01-08 12:02:14,728: t15.2025.01.12 val PER: 0.3118
2026-01-08 12:02:14,728: t15.2025.03.14 val PER: 0.4216
2026-01-08 12:02:14,729: t15.2025.03.16 val PER: 0.3312
2026-01-08 12:02:14,729: t15.2025.03.30 val PER: 0.4057
2026-01-08 12:02:14,729: t15.2025.04.13 val PER: 0.3566
2026-01-08 12:02:32,012: Train batch 6200: loss: 22.25 grad norm: 66.18 time: 0.072
2026-01-08 12:02:49,462: Train batch 6400: loss: 27.35 grad norm: 76.28 time: 0.064
2026-01-08 12:02:58,068: Running test after training batch: 6500
2026-01-08 12:02:58,188: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:03:02,877: WER debug example
  GT : you can see the code at this point as well
  PR : euclea candies seats the tuckett hatt cysts points his swells
2026-01-08 12:03:02,902: WER debug example
  GT : how does it keep the cost down
  PR : seaholm desma sitz keep kiess the teast
2026-01-08 12:03:04,217: Val batch 6500: PER (avg): 0.2741 CTC Loss (avg): 28.2436 WER(1gram): 97.72% (n=64) time: 6.148
2026-01-08 12:03:04,217: WER lens: avg_true_words=6.16 avg_pred_words=5.81 max_pred_words=12
2026-01-08 12:03:04,217: t15.2023.08.13 val PER: 0.2568
2026-01-08 12:03:04,217: t15.2023.08.18 val PER: 0.2448
2026-01-08 12:03:04,217: t15.2023.08.20 val PER: 0.2319
2026-01-08 12:03:04,217: t15.2023.08.25 val PER: 0.2139
2026-01-08 12:03:04,217: t15.2023.08.27 val PER: 0.3103
2026-01-08 12:03:04,217: t15.2023.09.01 val PER: 0.2256
2026-01-08 12:03:04,218: t15.2023.09.03 val PER: 0.2898
2026-01-08 12:03:04,218: t15.2023.09.24 val PER: 0.2245
2026-01-08 12:03:04,218: t15.2023.09.29 val PER: 0.2399
2026-01-08 12:03:04,218: t15.2023.10.01 val PER: 0.2734
2026-01-08 12:03:04,218: t15.2023.10.06 val PER: 0.2239
2026-01-08 12:03:04,218: t15.2023.10.08 val PER: 0.3667
2026-01-08 12:03:04,218: t15.2023.10.13 val PER: 0.3406
2026-01-08 12:03:04,218: t15.2023.10.15 val PER: 0.2637
2026-01-08 12:03:04,218: t15.2023.10.20 val PER: 0.2651
2026-01-08 12:03:04,218: t15.2023.10.22 val PER: 0.2316
2026-01-08 12:03:04,218: t15.2023.11.03 val PER: 0.2815
2026-01-08 12:03:04,218: t15.2023.11.04 val PER: 0.1297
2026-01-08 12:03:04,218: t15.2023.11.17 val PER: 0.1540
2026-01-08 12:03:04,218: t15.2023.11.19 val PER: 0.1657
2026-01-08 12:03:04,218: t15.2023.11.26 val PER: 0.2804
2026-01-08 12:03:04,219: t15.2023.12.03 val PER: 0.2500
2026-01-08 12:03:04,219: t15.2023.12.08 val PER: 0.2563
2026-01-08 12:03:04,219: t15.2023.12.10 val PER: 0.2129
2026-01-08 12:03:04,219: t15.2023.12.17 val PER: 0.2422
2026-01-08 12:03:04,219: t15.2023.12.29 val PER: 0.2739
2026-01-08 12:03:04,219: t15.2024.02.25 val PER: 0.2205
2026-01-08 12:03:04,219: t15.2024.03.08 val PER: 0.3400
2026-01-08 12:03:04,219: t15.2024.03.15 val PER: 0.3008
2026-01-08 12:03:04,219: t15.2024.03.17 val PER: 0.2866
2026-01-08 12:03:04,219: t15.2024.05.10 val PER: 0.3001
2026-01-08 12:03:04,219: t15.2024.06.14 val PER: 0.2650
2026-01-08 12:03:04,219: t15.2024.07.19 val PER: 0.3428
2026-01-08 12:03:04,219: t15.2024.07.21 val PER: 0.1924
2026-01-08 12:03:04,219: t15.2024.07.28 val PER: 0.2581
2026-01-08 12:03:04,219: t15.2025.01.10 val PER: 0.4284
2026-01-08 12:03:04,219: t15.2025.01.12 val PER: 0.3056
2026-01-08 12:03:04,220: t15.2025.03.14 val PER: 0.4112
2026-01-08 12:03:04,220: t15.2025.03.16 val PER: 0.3168
2026-01-08 12:03:04,220: t15.2025.03.30 val PER: 0.4103
2026-01-08 12:03:04,220: t15.2025.04.13 val PER: 0.3524
2026-01-08 12:03:12,901: Train batch 6600: loss: 17.69 grad norm: 53.02 time: 0.046
2026-01-08 12:03:30,082: Train batch 6800: loss: 23.42 grad norm: 58.38 time: 0.050
2026-01-08 12:03:47,636: Train batch 7000: loss: 23.53 grad norm: 67.87 time: 0.063
2026-01-08 12:03:47,636: Running test after training batch: 7000
2026-01-08 12:03:47,731: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:03:52,724: WER debug example
  GT : you can see the code at this point as well
  PR : euclea canned seats the stuccoed scat cysts points his swells
2026-01-08 12:03:52,749: WER debug example
  GT : how does it keep the cost down
  PR : cederholm desma sitz kicks the teast
2026-01-08 12:03:54,058: Val batch 7000: PER (avg): 0.2668 CTC Loss (avg): 27.1708 WER(1gram): 96.19% (n=64) time: 6.422
2026-01-08 12:03:54,059: WER lens: avg_true_words=6.16 avg_pred_words=5.67 max_pred_words=11
2026-01-08 12:03:54,059: t15.2023.08.13 val PER: 0.2464
2026-01-08 12:03:54,059: t15.2023.08.18 val PER: 0.2397
2026-01-08 12:03:54,059: t15.2023.08.20 val PER: 0.2335
2026-01-08 12:03:54,059: t15.2023.08.25 val PER: 0.2259
2026-01-08 12:03:54,059: t15.2023.08.27 val PER: 0.3215
2026-01-08 12:03:54,059: t15.2023.09.01 val PER: 0.2151
2026-01-08 12:03:54,059: t15.2023.09.03 val PER: 0.2696
2026-01-08 12:03:54,059: t15.2023.09.24 val PER: 0.2257
2026-01-08 12:03:54,059: t15.2023.09.29 val PER: 0.2412
2026-01-08 12:03:54,059: t15.2023.10.01 val PER: 0.2649
2026-01-08 12:03:54,060: t15.2023.10.06 val PER: 0.2034
2026-01-08 12:03:54,060: t15.2023.10.08 val PER: 0.3532
2026-01-08 12:03:54,060: t15.2023.10.13 val PER: 0.3375
2026-01-08 12:03:54,060: t15.2023.10.15 val PER: 0.2683
2026-01-08 12:03:54,060: t15.2023.10.20 val PER: 0.2718
2026-01-08 12:03:54,060: t15.2023.10.22 val PER: 0.2105
2026-01-08 12:03:54,060: t15.2023.11.03 val PER: 0.2720
2026-01-08 12:03:54,061: t15.2023.11.04 val PER: 0.1160
2026-01-08 12:03:54,061: t15.2023.11.17 val PER: 0.1337
2026-01-08 12:03:54,061: t15.2023.11.19 val PER: 0.1557
2026-01-08 12:03:54,061: t15.2023.11.26 val PER: 0.2703
2026-01-08 12:03:54,061: t15.2023.12.03 val PER: 0.2489
2026-01-08 12:03:54,061: t15.2023.12.08 val PER: 0.2450
2026-01-08 12:03:54,061: t15.2023.12.10 val PER: 0.2076
2026-01-08 12:03:54,061: t15.2023.12.17 val PER: 0.2464
2026-01-08 12:03:54,061: t15.2023.12.29 val PER: 0.2732
2026-01-08 12:03:54,061: t15.2024.02.25 val PER: 0.2149
2026-01-08 12:03:54,061: t15.2024.03.08 val PER: 0.3343
2026-01-08 12:03:54,061: t15.2024.03.15 val PER: 0.2933
2026-01-08 12:03:54,061: t15.2024.03.17 val PER: 0.2671
2026-01-08 12:03:54,061: t15.2024.05.10 val PER: 0.2868
2026-01-08 12:03:54,062: t15.2024.06.14 val PER: 0.2587
2026-01-08 12:03:54,062: t15.2024.07.19 val PER: 0.3336
2026-01-08 12:03:54,062: t15.2024.07.21 val PER: 0.1821
2026-01-08 12:03:54,062: t15.2024.07.28 val PER: 0.2324
2026-01-08 12:03:54,062: t15.2025.01.10 val PER: 0.4284
2026-01-08 12:03:54,062: t15.2025.01.12 val PER: 0.2856
2026-01-08 12:03:54,062: t15.2025.03.14 val PER: 0.4305
2026-01-08 12:03:54,062: t15.2025.03.16 val PER: 0.2971
2026-01-08 12:03:54,062: t15.2025.03.30 val PER: 0.4276
2026-01-08 12:03:54,062: t15.2025.04.13 val PER: 0.3310
2026-01-08 12:03:54,063: New best val WER(1gram) 97.46% --> 96.19%
2026-01-08 12:03:54,063: Checkpointing model
2026-01-08 12:03:54,206: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_linderman/checkpoint/best_checkpoint
2026-01-08 12:04:12,265: Train batch 7200: loss: 21.90 grad norm: 65.51 time: 0.081
2026-01-08 12:04:30,458: Train batch 7400: loss: 20.69 grad norm: 63.93 time: 0.077
2026-01-08 12:04:38,990: Running test after training batch: 7500
2026-01-08 12:04:39,101: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:04:43,779: WER debug example
  GT : you can see the code at this point as well
  PR : euclea candies seixas the colds hatt cyst appointees his swells
2026-01-08 12:04:43,802: WER debug example
  GT : how does it keep the cost down
  PR : cederholm decency hitz heap coos the teast setzer
2026-01-08 12:04:45,122: Val batch 7500: PER (avg): 0.2647 CTC Loss (avg): 27.3901 WER(1gram): 100.00% (n=64) time: 6.131
2026-01-08 12:04:45,122: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=12
2026-01-08 12:04:45,122: t15.2023.08.13 val PER: 0.2453
2026-01-08 12:04:45,122: t15.2023.08.18 val PER: 0.2297
2026-01-08 12:04:45,122: t15.2023.08.20 val PER: 0.2351
2026-01-08 12:04:45,123: t15.2023.08.25 val PER: 0.2184
2026-01-08 12:04:45,123: t15.2023.08.27 val PER: 0.3071
2026-01-08 12:04:45,123: t15.2023.09.01 val PER: 0.2094
2026-01-08 12:04:45,123: t15.2023.09.03 val PER: 0.2850
2026-01-08 12:04:45,123: t15.2023.09.24 val PER: 0.2282
2026-01-08 12:04:45,123: t15.2023.09.29 val PER: 0.2221
2026-01-08 12:04:45,123: t15.2023.10.01 val PER: 0.2701
2026-01-08 12:04:45,123: t15.2023.10.06 val PER: 0.2121
2026-01-08 12:04:45,123: t15.2023.10.08 val PER: 0.3572
2026-01-08 12:04:45,123: t15.2023.10.13 val PER: 0.3266
2026-01-08 12:04:45,123: t15.2023.10.15 val PER: 0.2617
2026-01-08 12:04:45,123: t15.2023.10.20 val PER: 0.2617
2026-01-08 12:04:45,123: t15.2023.10.22 val PER: 0.2216
2026-01-08 12:04:45,124: t15.2023.11.03 val PER: 0.2795
2026-01-08 12:04:45,124: t15.2023.11.04 val PER: 0.1092
2026-01-08 12:04:45,124: t15.2023.11.17 val PER: 0.1477
2026-01-08 12:04:45,124: t15.2023.11.19 val PER: 0.1537
2026-01-08 12:04:45,124: t15.2023.11.26 val PER: 0.2703
2026-01-08 12:04:45,124: t15.2023.12.03 val PER: 0.2416
2026-01-08 12:04:45,124: t15.2023.12.08 val PER: 0.2364
2026-01-08 12:04:45,124: t15.2023.12.10 val PER: 0.1919
2026-01-08 12:04:45,124: t15.2023.12.17 val PER: 0.2401
2026-01-08 12:04:45,124: t15.2023.12.29 val PER: 0.2636
2026-01-08 12:04:45,124: t15.2024.02.25 val PER: 0.2121
2026-01-08 12:04:45,124: t15.2024.03.08 val PER: 0.3570
2026-01-08 12:04:45,124: t15.2024.03.15 val PER: 0.2877
2026-01-08 12:04:45,124: t15.2024.03.17 val PER: 0.2643
2026-01-08 12:04:45,124: t15.2024.05.10 val PER: 0.2779
2026-01-08 12:04:45,124: t15.2024.06.14 val PER: 0.2650
2026-01-08 12:04:45,124: t15.2024.07.19 val PER: 0.3368
2026-01-08 12:04:45,125: t15.2024.07.21 val PER: 0.1903
2026-01-08 12:04:45,125: t15.2024.07.28 val PER: 0.2382
2026-01-08 12:04:45,125: t15.2025.01.10 val PER: 0.4105
2026-01-08 12:04:45,125: t15.2025.01.12 val PER: 0.2956
2026-01-08 12:04:45,125: t15.2025.03.14 val PER: 0.4142
2026-01-08 12:04:45,125: t15.2025.03.16 val PER: 0.3024
2026-01-08 12:04:45,125: t15.2025.03.30 val PER: 0.4034
2026-01-08 12:04:45,126: t15.2025.04.13 val PER: 0.3267
2026-01-08 12:04:53,884: Train batch 7600: loss: 22.53 grad norm: 65.22 time: 0.072
2026-01-08 12:05:12,129: Train batch 7800: loss: 22.72 grad norm: 68.37 time: 0.058
2026-01-08 12:05:30,739: Train batch 8000: loss: 19.59 grad norm: 56.88 time: 0.075
2026-01-08 12:05:30,739: Running test after training batch: 8000
2026-01-08 12:05:30,840: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:05:35,545: WER debug example
  GT : you can see the code at this point as well
  PR : euclea candies seats the stuccoed hatz this appointees his
2026-01-08 12:05:35,569: WER debug example
  GT : how does it keep the cost down
  PR : turnham decency hitz heap kerce thus cussed
2026-01-08 12:05:36,925: Val batch 8000: PER (avg): 0.2568 CTC Loss (avg): 26.4251 WER(1gram): 97.46% (n=64) time: 6.185
2026-01-08 12:05:36,925: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=13
2026-01-08 12:05:36,925: t15.2023.08.13 val PER: 0.2516
2026-01-08 12:05:36,925: t15.2023.08.18 val PER: 0.2313
2026-01-08 12:05:36,925: t15.2023.08.20 val PER: 0.2264
2026-01-08 12:05:36,925: t15.2023.08.25 val PER: 0.2274
2026-01-08 12:05:36,925: t15.2023.08.27 val PER: 0.2942
2026-01-08 12:05:36,926: t15.2023.09.01 val PER: 0.1997
2026-01-08 12:05:36,926: t15.2023.09.03 val PER: 0.2862
2026-01-08 12:05:36,926: t15.2023.09.24 val PER: 0.2172
2026-01-08 12:05:36,926: t15.2023.09.29 val PER: 0.2202
2026-01-08 12:05:36,926: t15.2023.10.01 val PER: 0.2662
2026-01-08 12:05:36,926: t15.2023.10.06 val PER: 0.1991
2026-01-08 12:05:36,926: t15.2023.10.08 val PER: 0.3545
2026-01-08 12:05:36,926: t15.2023.10.13 val PER: 0.3266
2026-01-08 12:05:36,926: t15.2023.10.15 val PER: 0.2624
2026-01-08 12:05:36,926: t15.2023.10.20 val PER: 0.2617
2026-01-08 12:05:36,927: t15.2023.10.22 val PER: 0.2138
2026-01-08 12:05:36,927: t15.2023.11.03 val PER: 0.2748
2026-01-08 12:05:36,927: t15.2023.11.04 val PER: 0.1195
2026-01-08 12:05:36,927: t15.2023.11.17 val PER: 0.1369
2026-01-08 12:05:36,927: t15.2023.11.19 val PER: 0.1457
2026-01-08 12:05:36,927: t15.2023.11.26 val PER: 0.2572
2026-01-08 12:05:36,927: t15.2023.12.03 val PER: 0.2290
2026-01-08 12:05:36,927: t15.2023.12.08 val PER: 0.2284
2026-01-08 12:05:36,927: t15.2023.12.10 val PER: 0.2037
2026-01-08 12:05:36,927: t15.2023.12.17 val PER: 0.2141
2026-01-08 12:05:36,927: t15.2023.12.29 val PER: 0.2526
2026-01-08 12:05:36,928: t15.2024.02.25 val PER: 0.2191
2026-01-08 12:05:36,928: t15.2024.03.08 val PER: 0.3257
2026-01-08 12:05:36,928: t15.2024.03.15 val PER: 0.2770
2026-01-08 12:05:36,928: t15.2024.03.17 val PER: 0.2531
2026-01-08 12:05:36,928: t15.2024.05.10 val PER: 0.2556
2026-01-08 12:05:36,928: t15.2024.06.14 val PER: 0.2524
2026-01-08 12:05:36,928: t15.2024.07.19 val PER: 0.3223
2026-01-08 12:05:36,928: t15.2024.07.21 val PER: 0.1703
2026-01-08 12:05:36,928: t15.2024.07.28 val PER: 0.2206
2026-01-08 12:05:36,928: t15.2025.01.10 val PER: 0.4008
2026-01-08 12:05:36,928: t15.2025.01.12 val PER: 0.2741
2026-01-08 12:05:36,929: t15.2025.03.14 val PER: 0.3920
2026-01-08 12:05:36,929: t15.2025.03.16 val PER: 0.3128
2026-01-08 12:05:36,932: t15.2025.03.30 val PER: 0.4011
2026-01-08 12:05:36,932: t15.2025.04.13 val PER: 0.3381
2026-01-08 12:05:55,308: Train batch 8200: loss: 16.76 grad norm: 53.35 time: 0.056
2026-01-08 12:06:13,202: Train batch 8400: loss: 16.67 grad norm: 57.85 time: 0.066
2026-01-08 12:06:22,193: Running test after training batch: 8500
2026-01-08 12:06:22,294: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:06:26,986: WER debug example
  GT : you can see the code at this point as well
  PR : euclea canned sikhs the stuccoed hatt cyst appointees his
2026-01-08 12:06:27,010: WER debug example
  GT : how does it keep the cost down
  PR : cederholm decency hitz heap coots thus costs dentzer
2026-01-08 12:06:28,331: Val batch 8500: PER (avg): 0.2542 CTC Loss (avg): 26.1048 WER(1gram): 99.75% (n=64) time: 6.138
2026-01-08 12:06:28,331: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-08 12:06:28,331: t15.2023.08.13 val PER: 0.2422
2026-01-08 12:06:28,331: t15.2023.08.18 val PER: 0.2263
2026-01-08 12:06:28,331: t15.2023.08.20 val PER: 0.2160
2026-01-08 12:06:28,331: t15.2023.08.25 val PER: 0.2244
2026-01-08 12:06:28,331: t15.2023.08.27 val PER: 0.2990
2026-01-08 12:06:28,332: t15.2023.09.01 val PER: 0.1997
2026-01-08 12:06:28,332: t15.2023.09.03 val PER: 0.2815
2026-01-08 12:06:28,332: t15.2023.09.24 val PER: 0.2245
2026-01-08 12:06:28,332: t15.2023.09.29 val PER: 0.2259
2026-01-08 12:06:28,332: t15.2023.10.01 val PER: 0.2635
2026-01-08 12:06:28,332: t15.2023.10.06 val PER: 0.2099
2026-01-08 12:06:28,333: t15.2023.10.08 val PER: 0.3572
2026-01-08 12:06:28,333: t15.2023.10.13 val PER: 0.3220
2026-01-08 12:06:28,333: t15.2023.10.15 val PER: 0.2459
2026-01-08 12:06:28,333: t15.2023.10.20 val PER: 0.2383
2026-01-08 12:06:28,333: t15.2023.10.22 val PER: 0.2294
2026-01-08 12:06:28,333: t15.2023.11.03 val PER: 0.2775
2026-01-08 12:06:28,333: t15.2023.11.04 val PER: 0.1092
2026-01-08 12:06:28,333: t15.2023.11.17 val PER: 0.1400
2026-01-08 12:06:28,333: t15.2023.11.19 val PER: 0.1397
2026-01-08 12:06:28,333: t15.2023.11.26 val PER: 0.2565
2026-01-08 12:06:28,333: t15.2023.12.03 val PER: 0.2258
2026-01-08 12:06:28,333: t15.2023.12.08 val PER: 0.2324
2026-01-08 12:06:28,333: t15.2023.12.10 val PER: 0.1932
2026-01-08 12:06:28,333: t15.2023.12.17 val PER: 0.2308
2026-01-08 12:06:28,333: t15.2023.12.29 val PER: 0.2519
2026-01-08 12:06:28,333: t15.2024.02.25 val PER: 0.2093
2026-01-08 12:06:28,334: t15.2024.03.08 val PER: 0.3115
2026-01-08 12:06:28,334: t15.2024.03.15 val PER: 0.2770
2026-01-08 12:06:28,334: t15.2024.03.17 val PER: 0.2455
2026-01-08 12:06:28,334: t15.2024.05.10 val PER: 0.2556
2026-01-08 12:06:28,334: t15.2024.06.14 val PER: 0.2539
2026-01-08 12:06:28,334: t15.2024.07.19 val PER: 0.3184
2026-01-08 12:06:28,334: t15.2024.07.21 val PER: 0.1676
2026-01-08 12:06:28,334: t15.2024.07.28 val PER: 0.2228
2026-01-08 12:06:28,334: t15.2025.01.10 val PER: 0.3953
2026-01-08 12:06:28,334: t15.2025.01.12 val PER: 0.2741
2026-01-08 12:06:28,334: t15.2025.03.14 val PER: 0.3994
2026-01-08 12:06:28,334: t15.2025.03.16 val PER: 0.2827
2026-01-08 12:06:28,334: t15.2025.03.30 val PER: 0.3782
2026-01-08 12:06:28,334: t15.2025.04.13 val PER: 0.3252
2026-01-08 12:06:37,293: Train batch 8600: loss: 25.68 grad norm: 65.13 time: 0.058
2026-01-08 12:06:54,534: Train batch 8800: loss: 22.97 grad norm: 70.93 time: 0.063
2026-01-08 12:07:12,455: Train batch 9000: loss: 24.99 grad norm: 68.85 time: 0.074
2026-01-08 12:07:12,455: Running test after training batch: 9000
2026-01-08 12:07:12,571: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:07:17,259: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies seats the stuccoed hatt cyst appointees his
2026-01-08 12:07:17,283: WER debug example
  GT : how does it keep the cost down
  PR : cederholm decency hitz heap keitz the teast setzer
2026-01-08 12:07:18,592: Val batch 9000: PER (avg): 0.2472 CTC Loss (avg): 25.7927 WER(1gram): 100.00% (n=64) time: 6.136
2026-01-08 12:07:18,592: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=13
2026-01-08 12:07:18,592: t15.2023.08.13 val PER: 0.2308
2026-01-08 12:07:18,592: t15.2023.08.18 val PER: 0.2154
2026-01-08 12:07:18,593: t15.2023.08.20 val PER: 0.2089
2026-01-08 12:07:18,593: t15.2023.08.25 val PER: 0.2169
2026-01-08 12:07:18,593: t15.2023.08.27 val PER: 0.3023
2026-01-08 12:07:18,593: t15.2023.09.01 val PER: 0.1883
2026-01-08 12:07:18,593: t15.2023.09.03 val PER: 0.2696
2026-01-08 12:07:18,593: t15.2023.09.24 val PER: 0.2124
2026-01-08 12:07:18,593: t15.2023.09.29 val PER: 0.2163
2026-01-08 12:07:18,593: t15.2023.10.01 val PER: 0.2569
2026-01-08 12:07:18,593: t15.2023.10.06 val PER: 0.1970
2026-01-08 12:07:18,593: t15.2023.10.08 val PER: 0.3478
2026-01-08 12:07:18,593: t15.2023.10.13 val PER: 0.3157
2026-01-08 12:07:18,594: t15.2023.10.15 val PER: 0.2459
2026-01-08 12:07:18,594: t15.2023.10.20 val PER: 0.2483
2026-01-08 12:07:18,594: t15.2023.10.22 val PER: 0.2071
2026-01-08 12:07:18,594: t15.2023.11.03 val PER: 0.2693
2026-01-08 12:07:18,594: t15.2023.11.04 val PER: 0.1297
2026-01-08 12:07:18,594: t15.2023.11.17 val PER: 0.1244
2026-01-08 12:07:18,594: t15.2023.11.19 val PER: 0.1377
2026-01-08 12:07:18,594: t15.2023.11.26 val PER: 0.2514
2026-01-08 12:07:18,594: t15.2023.12.03 val PER: 0.2290
2026-01-08 12:07:18,594: t15.2023.12.08 val PER: 0.2204
2026-01-08 12:07:18,594: t15.2023.12.10 val PER: 0.1905
2026-01-08 12:07:18,594: t15.2023.12.17 val PER: 0.2162
2026-01-08 12:07:18,594: t15.2023.12.29 val PER: 0.2457
2026-01-08 12:07:18,594: t15.2024.02.25 val PER: 0.1994
2026-01-08 12:07:18,594: t15.2024.03.08 val PER: 0.3087
2026-01-08 12:07:18,595: t15.2024.03.15 val PER: 0.2664
2026-01-08 12:07:18,595: t15.2024.03.17 val PER: 0.2406
2026-01-08 12:07:18,595: t15.2024.05.10 val PER: 0.2377
2026-01-08 12:07:18,595: t15.2024.06.14 val PER: 0.2350
2026-01-08 12:07:18,595: t15.2024.07.19 val PER: 0.3092
2026-01-08 12:07:18,595: t15.2024.07.21 val PER: 0.1669
2026-01-08 12:07:18,595: t15.2024.07.28 val PER: 0.2147
2026-01-08 12:07:18,595: t15.2025.01.10 val PER: 0.3994
2026-01-08 12:07:18,595: t15.2025.01.12 val PER: 0.2879
2026-01-08 12:07:18,595: t15.2025.03.14 val PER: 0.3891
2026-01-08 12:07:18,595: t15.2025.03.16 val PER: 0.2919
2026-01-08 12:07:18,596: t15.2025.03.30 val PER: 0.3690
2026-01-08 12:07:18,596: t15.2025.04.13 val PER: 0.3024
2026-01-08 12:07:36,645: Train batch 9200: loss: 17.59 grad norm: 60.09 time: 0.058
2026-01-08 12:07:53,766: Train batch 9400: loss: 16.08 grad norm: 59.96 time: 0.070
2026-01-08 12:08:02,521: Running test after training batch: 9500
2026-01-08 12:08:02,652: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:08:07,470: WER debug example
  GT : you can see the code at this point as well
  PR : euclea candies seats the scolds hatt cyst appointees his
2026-01-08 12:08:07,494: WER debug example
  GT : how does it keep the cost down
  PR : cederholm decency hitz heaps the teast
2026-01-08 12:08:08,813: Val batch 9500: PER (avg): 0.2408 CTC Loss (avg): 24.9787 WER(1gram): 99.75% (n=64) time: 6.292
2026-01-08 12:08:08,814: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=12
2026-01-08 12:08:08,814: t15.2023.08.13 val PER: 0.2131
2026-01-08 12:08:08,814: t15.2023.08.18 val PER: 0.2129
2026-01-08 12:08:08,814: t15.2023.08.20 val PER: 0.2073
2026-01-08 12:08:08,814: t15.2023.08.25 val PER: 0.2214
2026-01-08 12:08:08,814: t15.2023.08.27 val PER: 0.2910
2026-01-08 12:08:08,814: t15.2023.09.01 val PER: 0.1843
2026-01-08 12:08:08,814: t15.2023.09.03 val PER: 0.2708
2026-01-08 12:08:08,815: t15.2023.09.24 val PER: 0.2245
2026-01-08 12:08:08,815: t15.2023.09.29 val PER: 0.2189
2026-01-08 12:08:08,815: t15.2023.10.01 val PER: 0.2483
2026-01-08 12:08:08,815: t15.2023.10.06 val PER: 0.1948
2026-01-08 12:08:08,815: t15.2023.10.08 val PER: 0.3396
2026-01-08 12:08:08,815: t15.2023.10.13 val PER: 0.3173
2026-01-08 12:08:08,815: t15.2023.10.15 val PER: 0.2367
2026-01-08 12:08:08,815: t15.2023.10.20 val PER: 0.2416
2026-01-08 12:08:08,815: t15.2023.10.22 val PER: 0.1860
2026-01-08 12:08:08,815: t15.2023.11.03 val PER: 0.2626
2026-01-08 12:08:08,816: t15.2023.11.04 val PER: 0.1126
2026-01-08 12:08:08,816: t15.2023.11.17 val PER: 0.1135
2026-01-08 12:08:08,816: t15.2023.11.19 val PER: 0.1377
2026-01-08 12:08:08,816: t15.2023.11.26 val PER: 0.2319
2026-01-08 12:08:08,816: t15.2023.12.03 val PER: 0.2269
2026-01-08 12:08:08,816: t15.2023.12.08 val PER: 0.2031
2026-01-08 12:08:08,816: t15.2023.12.10 val PER: 0.1748
2026-01-08 12:08:08,816: t15.2023.12.17 val PER: 0.2183
2026-01-08 12:08:08,816: t15.2023.12.29 val PER: 0.2361
2026-01-08 12:08:08,816: t15.2024.02.25 val PER: 0.1896
2026-01-08 12:08:08,816: t15.2024.03.08 val PER: 0.2902
2026-01-08 12:08:08,816: t15.2024.03.15 val PER: 0.2639
2026-01-08 12:08:08,816: t15.2024.03.17 val PER: 0.2343
2026-01-08 12:08:08,816: t15.2024.05.10 val PER: 0.2377
2026-01-08 12:08:08,816: t15.2024.06.14 val PER: 0.2334
2026-01-08 12:08:08,817: t15.2024.07.19 val PER: 0.3013
2026-01-08 12:08:08,817: t15.2024.07.21 val PER: 0.1648
2026-01-08 12:08:08,817: t15.2024.07.28 val PER: 0.2118
2026-01-08 12:08:08,817: t15.2025.01.10 val PER: 0.3788
2026-01-08 12:08:08,817: t15.2025.01.12 val PER: 0.2702
2026-01-08 12:08:08,817: t15.2025.03.14 val PER: 0.3846
2026-01-08 12:08:08,817: t15.2025.03.16 val PER: 0.2880
2026-01-08 12:08:08,817: t15.2025.03.30 val PER: 0.3690
2026-01-08 12:08:08,818: t15.2025.04.13 val PER: 0.3096
2026-01-08 12:08:17,566: Train batch 9600: loss: 15.75 grad norm: 50.76 time: 0.075
2026-01-08 12:08:35,009: Train batch 9800: loss: 17.00 grad norm: 63.89 time: 0.066
2026-01-08 12:08:52,340: Train batch 10000: loss: 11.10 grad norm: 48.34 time: 0.063
2026-01-08 12:08:52,340: Running test after training batch: 10000
2026-01-08 12:08:52,463: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:08:57,148: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa canned seats the scolds hatt cyst appointees has
2026-01-08 12:08:57,172: WER debug example
  GT : how does it keep the cost down
  PR : cederholm dease dusts hitty heap kurtz thus cussed setzer
2026-01-08 12:08:58,483: Val batch 10000: PER (avg): 0.2417 CTC Loss (avg): 25.2019 WER(1gram): 99.24% (n=64) time: 6.142
2026-01-08 12:08:58,483: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=12
2026-01-08 12:08:58,483: t15.2023.08.13 val PER: 0.2225
2026-01-08 12:08:58,483: t15.2023.08.18 val PER: 0.2129
2026-01-08 12:08:58,483: t15.2023.08.20 val PER: 0.2113
2026-01-08 12:08:58,483: t15.2023.08.25 val PER: 0.2063
2026-01-08 12:08:58,483: t15.2023.08.27 val PER: 0.2974
2026-01-08 12:08:58,483: t15.2023.09.01 val PER: 0.1851
2026-01-08 12:08:58,484: t15.2023.09.03 val PER: 0.2696
2026-01-08 12:08:58,484: t15.2023.09.24 val PER: 0.2087
2026-01-08 12:08:58,484: t15.2023.09.29 val PER: 0.2119
2026-01-08 12:08:58,484: t15.2023.10.01 val PER: 0.2457
2026-01-08 12:08:58,484: t15.2023.10.06 val PER: 0.2045
2026-01-08 12:08:58,484: t15.2023.10.08 val PER: 0.3356
2026-01-08 12:08:58,484: t15.2023.10.13 val PER: 0.3204
2026-01-08 12:08:58,484: t15.2023.10.15 val PER: 0.2327
2026-01-08 12:08:58,484: t15.2023.10.20 val PER: 0.2584
2026-01-08 12:08:58,484: t15.2023.10.22 val PER: 0.2049
2026-01-08 12:08:58,484: t15.2023.11.03 val PER: 0.2666
2026-01-08 12:08:58,485: t15.2023.11.04 val PER: 0.1024
2026-01-08 12:08:58,485: t15.2023.11.17 val PER: 0.1244
2026-01-08 12:08:58,485: t15.2023.11.19 val PER: 0.1357
2026-01-08 12:08:58,485: t15.2023.11.26 val PER: 0.2399
2026-01-08 12:08:58,485: t15.2023.12.03 val PER: 0.2122
2026-01-08 12:08:58,485: t15.2023.12.08 val PER: 0.2077
2026-01-08 12:08:58,485: t15.2023.12.10 val PER: 0.1827
2026-01-08 12:08:58,485: t15.2023.12.17 val PER: 0.2162
2026-01-08 12:08:58,485: t15.2023.12.29 val PER: 0.2388
2026-01-08 12:08:58,485: t15.2024.02.25 val PER: 0.1994
2026-01-08 12:08:58,485: t15.2024.03.08 val PER: 0.2888
2026-01-08 12:08:58,485: t15.2024.03.15 val PER: 0.2602
2026-01-08 12:08:58,485: t15.2024.03.17 val PER: 0.2315
2026-01-08 12:08:58,485: t15.2024.05.10 val PER: 0.2600
2026-01-08 12:08:58,485: t15.2024.06.14 val PER: 0.2319
2026-01-08 12:08:58,485: t15.2024.07.19 val PER: 0.3078
2026-01-08 12:08:58,486: t15.2024.07.21 val PER: 0.1614
2026-01-08 12:08:58,486: t15.2024.07.28 val PER: 0.2235
2026-01-08 12:08:58,486: t15.2025.01.10 val PER: 0.3829
2026-01-08 12:08:58,486: t15.2025.01.12 val PER: 0.2640
2026-01-08 12:08:58,486: t15.2025.03.14 val PER: 0.3743
2026-01-08 12:08:58,486: t15.2025.03.16 val PER: 0.2827
2026-01-08 12:08:58,486: t15.2025.03.30 val PER: 0.3678
2026-01-08 12:08:58,486: t15.2025.04.13 val PER: 0.3010
2026-01-08 12:09:15,498: Train batch 10200: loss: 15.21 grad norm: 50.95 time: 0.052
2026-01-08 12:09:32,508: Train batch 10400: loss: 17.45 grad norm: 55.30 time: 0.075
2026-01-08 12:09:40,848: Running test after training batch: 10500
2026-01-08 12:09:40,955: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:09:45,636: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies seats the colds hatt cyst appointees has
2026-01-08 12:09:45,661: WER debug example
  GT : how does it keep the cost down
  PR : cederholm decency hitz keeps thus costs dentzer
2026-01-08 12:09:46,987: Val batch 10500: PER (avg): 0.2334 CTC Loss (avg): 24.6504 WER(1gram): 100.76% (n=64) time: 6.138
2026-01-08 12:09:46,988: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=12
2026-01-08 12:09:46,988: t15.2023.08.13 val PER: 0.2162
2026-01-08 12:09:46,988: t15.2023.08.18 val PER: 0.2054
2026-01-08 12:09:46,988: t15.2023.08.20 val PER: 0.1994
2026-01-08 12:09:46,988: t15.2023.08.25 val PER: 0.2048
2026-01-08 12:09:46,988: t15.2023.08.27 val PER: 0.2765
2026-01-08 12:09:46,988: t15.2023.09.01 val PER: 0.1818
2026-01-08 12:09:46,988: t15.2023.09.03 val PER: 0.2660
2026-01-08 12:09:46,988: t15.2023.09.24 val PER: 0.2063
2026-01-08 12:09:46,988: t15.2023.09.29 val PER: 0.2074
2026-01-08 12:09:46,988: t15.2023.10.01 val PER: 0.2391
2026-01-08 12:09:46,989: t15.2023.10.06 val PER: 0.1798
2026-01-08 12:09:46,989: t15.2023.10.08 val PER: 0.3248
2026-01-08 12:09:46,989: t15.2023.10.13 val PER: 0.2909
2026-01-08 12:09:46,989: t15.2023.10.15 val PER: 0.2373
2026-01-08 12:09:46,989: t15.2023.10.20 val PER: 0.2383
2026-01-08 12:09:46,989: t15.2023.10.22 val PER: 0.1804
2026-01-08 12:09:46,989: t15.2023.11.03 val PER: 0.2592
2026-01-08 12:09:46,989: t15.2023.11.04 val PER: 0.1058
2026-01-08 12:09:46,989: t15.2023.11.17 val PER: 0.1260
2026-01-08 12:09:46,989: t15.2023.11.19 val PER: 0.1337
2026-01-08 12:09:46,989: t15.2023.11.26 val PER: 0.2377
2026-01-08 12:09:46,989: t15.2023.12.03 val PER: 0.2080
2026-01-08 12:09:46,990: t15.2023.12.08 val PER: 0.2044
2026-01-08 12:09:46,990: t15.2023.12.10 val PER: 0.1813
2026-01-08 12:09:46,990: t15.2023.12.17 val PER: 0.1913
2026-01-08 12:09:46,990: t15.2023.12.29 val PER: 0.2327
2026-01-08 12:09:46,990: t15.2024.02.25 val PER: 0.1868
2026-01-08 12:09:46,990: t15.2024.03.08 val PER: 0.2945
2026-01-08 12:09:46,990: t15.2024.03.15 val PER: 0.2658
2026-01-08 12:09:46,990: t15.2024.03.17 val PER: 0.2371
2026-01-08 12:09:46,990: t15.2024.05.10 val PER: 0.2259
2026-01-08 12:09:46,990: t15.2024.06.14 val PER: 0.2177
2026-01-08 12:09:46,990: t15.2024.07.19 val PER: 0.2927
2026-01-08 12:09:46,990: t15.2024.07.21 val PER: 0.1448
2026-01-08 12:09:46,990: t15.2024.07.28 val PER: 0.2015
2026-01-08 12:09:46,990: t15.2025.01.10 val PER: 0.3760
2026-01-08 12:09:46,990: t15.2025.01.12 val PER: 0.2656
2026-01-08 12:09:46,990: t15.2025.03.14 val PER: 0.3846
2026-01-08 12:09:46,991: t15.2025.03.16 val PER: 0.2657
2026-01-08 12:09:46,991: t15.2025.03.30 val PER: 0.3494
2026-01-08 12:09:46,991: t15.2025.04.13 val PER: 0.2939
2026-01-08 12:09:55,822: Train batch 10600: loss: 14.44 grad norm: 58.00 time: 0.074
2026-01-08 12:10:13,112: Train batch 10800: loss: 20.89 grad norm: 71.61 time: 0.067
2026-01-08 12:10:30,378: Train batch 11000: loss: 23.91 grad norm: 78.82 time: 0.058
2026-01-08 12:10:30,378: Running test after training batch: 11000
2026-01-08 12:10:30,517: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:10:35,160: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies seats the keds hatz this appointees has
2026-01-08 12:10:35,185: WER debug example
  GT : how does it keep the cost down
  PR : derham decency hitty ski peeks thus costs
2026-01-08 12:10:36,509: Val batch 11000: PER (avg): 0.2339 CTC Loss (avg): 24.6966 WER(1gram): 99.24% (n=64) time: 6.131
2026-01-08 12:10:36,510: WER lens: avg_true_words=6.16 avg_pred_words=5.94 max_pred_words=12
2026-01-08 12:10:36,510: t15.2023.08.13 val PER: 0.2204
2026-01-08 12:10:36,510: t15.2023.08.18 val PER: 0.2062
2026-01-08 12:10:36,510: t15.2023.08.20 val PER: 0.2002
2026-01-08 12:10:36,510: t15.2023.08.25 val PER: 0.2063
2026-01-08 12:10:36,510: t15.2023.08.27 val PER: 0.2846
2026-01-08 12:10:36,510: t15.2023.09.01 val PER: 0.1786
2026-01-08 12:10:36,511: t15.2023.09.03 val PER: 0.2648
2026-01-08 12:10:36,511: t15.2023.09.24 val PER: 0.2087
2026-01-08 12:10:36,511: t15.2023.09.29 val PER: 0.2068
2026-01-08 12:10:36,511: t15.2023.10.01 val PER: 0.2437
2026-01-08 12:10:36,511: t15.2023.10.06 val PER: 0.1808
2026-01-08 12:10:36,511: t15.2023.10.08 val PER: 0.3275
2026-01-08 12:10:36,511: t15.2023.10.13 val PER: 0.3111
2026-01-08 12:10:36,511: t15.2023.10.15 val PER: 0.2241
2026-01-08 12:10:36,511: t15.2023.10.20 val PER: 0.2282
2026-01-08 12:10:36,511: t15.2023.10.22 val PER: 0.1771
2026-01-08 12:10:36,511: t15.2023.11.03 val PER: 0.2578
2026-01-08 12:10:36,511: t15.2023.11.04 val PER: 0.1160
2026-01-08 12:10:36,511: t15.2023.11.17 val PER: 0.1244
2026-01-08 12:10:36,511: t15.2023.11.19 val PER: 0.1377
2026-01-08 12:10:36,512: t15.2023.11.26 val PER: 0.2319
2026-01-08 12:10:36,512: t15.2023.12.03 val PER: 0.2069
2026-01-08 12:10:36,512: t15.2023.12.08 val PER: 0.2064
2026-01-08 12:10:36,512: t15.2023.12.10 val PER: 0.1629
2026-01-08 12:10:36,512: t15.2023.12.17 val PER: 0.2027
2026-01-08 12:10:36,512: t15.2023.12.29 val PER: 0.2313
2026-01-08 12:10:36,512: t15.2024.02.25 val PER: 0.1882
2026-01-08 12:10:36,512: t15.2024.03.08 val PER: 0.2930
2026-01-08 12:10:36,512: t15.2024.03.15 val PER: 0.2639
2026-01-08 12:10:36,512: t15.2024.03.17 val PER: 0.2322
2026-01-08 12:10:36,512: t15.2024.05.10 val PER: 0.2303
2026-01-08 12:10:36,512: t15.2024.06.14 val PER: 0.2208
2026-01-08 12:10:36,512: t15.2024.07.19 val PER: 0.3026
2026-01-08 12:10:36,512: t15.2024.07.21 val PER: 0.1503
2026-01-08 12:10:36,512: t15.2024.07.28 val PER: 0.2037
2026-01-08 12:10:36,512: t15.2025.01.10 val PER: 0.3774
2026-01-08 12:10:36,512: t15.2025.01.12 val PER: 0.2594
2026-01-08 12:10:36,513: t15.2025.03.14 val PER: 0.3743
2026-01-08 12:10:36,513: t15.2025.03.16 val PER: 0.2762
2026-01-08 12:10:36,513: t15.2025.03.30 val PER: 0.3575
2026-01-08 12:10:36,513: t15.2025.04.13 val PER: 0.2782
2026-01-08 12:10:53,760: Train batch 11200: loss: 16.69 grad norm: 63.60 time: 0.073
2026-01-08 12:11:10,817: Train batch 11400: loss: 16.33 grad norm: 59.20 time: 0.058
2026-01-08 12:11:19,360: Running test after training batch: 11500
2026-01-08 12:11:19,466: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:11:24,445: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies seats the canned hatz this appointees his
2026-01-08 12:11:24,470: WER debug example
  GT : how does it keep the cost down
  PR : derham decency hitz keepers thus costs dentzer
2026-01-08 12:11:25,833: Val batch 11500: PER (avg): 0.2302 CTC Loss (avg): 24.4870 WER(1gram): 97.72% (n=64) time: 6.473
2026-01-08 12:11:25,833: WER lens: avg_true_words=6.16 avg_pred_words=5.73 max_pred_words=12
2026-01-08 12:11:25,834: t15.2023.08.13 val PER: 0.2162
2026-01-08 12:11:25,834: t15.2023.08.18 val PER: 0.1995
2026-01-08 12:11:25,834: t15.2023.08.20 val PER: 0.2002
2026-01-08 12:11:25,834: t15.2023.08.25 val PER: 0.2018
2026-01-08 12:11:25,834: t15.2023.08.27 val PER: 0.2814
2026-01-08 12:11:25,834: t15.2023.09.01 val PER: 0.1705
2026-01-08 12:11:25,834: t15.2023.09.03 val PER: 0.2601
2026-01-08 12:11:25,834: t15.2023.09.24 val PER: 0.2100
2026-01-08 12:11:25,834: t15.2023.09.29 val PER: 0.2042
2026-01-08 12:11:25,834: t15.2023.10.01 val PER: 0.2450
2026-01-08 12:11:25,834: t15.2023.10.06 val PER: 0.1830
2026-01-08 12:11:25,834: t15.2023.10.08 val PER: 0.3315
2026-01-08 12:11:25,835: t15.2023.10.13 val PER: 0.3018
2026-01-08 12:11:25,835: t15.2023.10.15 val PER: 0.2301
2026-01-08 12:11:25,835: t15.2023.10.20 val PER: 0.2215
2026-01-08 12:11:25,835: t15.2023.10.22 val PER: 0.1882
2026-01-08 12:11:25,835: t15.2023.11.03 val PER: 0.2585
2026-01-08 12:11:25,835: t15.2023.11.04 val PER: 0.0990
2026-01-08 12:11:25,835: t15.2023.11.17 val PER: 0.1244
2026-01-08 12:11:25,835: t15.2023.11.19 val PER: 0.1317
2026-01-08 12:11:25,835: t15.2023.11.26 val PER: 0.2254
2026-01-08 12:11:25,835: t15.2023.12.03 val PER: 0.2080
2026-01-08 12:11:25,835: t15.2023.12.08 val PER: 0.1917
2026-01-08 12:11:25,835: t15.2023.12.10 val PER: 0.1656
2026-01-08 12:11:25,836: t15.2023.12.17 val PER: 0.1861
2026-01-08 12:11:25,836: t15.2023.12.29 val PER: 0.2203
2026-01-08 12:11:25,836: t15.2024.02.25 val PER: 0.1840
2026-01-08 12:11:25,836: t15.2024.03.08 val PER: 0.3001
2026-01-08 12:11:25,836: t15.2024.03.15 val PER: 0.2539
2026-01-08 12:11:25,836: t15.2024.03.17 val PER: 0.2211
2026-01-08 12:11:25,836: t15.2024.05.10 val PER: 0.2259
2026-01-08 12:11:25,836: t15.2024.06.14 val PER: 0.2145
2026-01-08 12:11:25,836: t15.2024.07.19 val PER: 0.2808
2026-01-08 12:11:25,837: t15.2024.07.21 val PER: 0.1483
2026-01-08 12:11:25,837: t15.2024.07.28 val PER: 0.2059
2026-01-08 12:11:25,837: t15.2025.01.10 val PER: 0.3788
2026-01-08 12:11:25,837: t15.2025.01.12 val PER: 0.2579
2026-01-08 12:11:25,837: t15.2025.03.14 val PER: 0.3639
2026-01-08 12:11:25,837: t15.2025.03.16 val PER: 0.2696
2026-01-08 12:11:25,837: t15.2025.03.30 val PER: 0.3632
2026-01-08 12:11:25,837: t15.2025.04.13 val PER: 0.2981
2026-01-08 12:11:34,099: Train batch 11600: loss: 19.71 grad norm: 64.71 time: 0.062
2026-01-08 12:11:51,271: Train batch 11800: loss: 15.27 grad norm: 52.47 time: 0.046
2026-01-08 12:12:08,485: Train batch 12000: loss: 19.69 grad norm: 67.45 time: 0.073
2026-01-08 12:12:08,486: Running test after training batch: 12000
2026-01-08 12:12:08,579: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:12:13,260: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies seats the kent hatt cyst appointees his swells
2026-01-08 12:12:13,285: WER debug example
  GT : how does it keep the cost down
  PR : turnham dease dusty hitz keeps thus costs
2026-01-08 12:12:14,646: Val batch 12000: PER (avg): 0.2241 CTC Loss (avg): 23.7890 WER(1gram): 96.95% (n=64) time: 6.161
2026-01-08 12:12:14,647: WER lens: avg_true_words=6.16 avg_pred_words=5.83 max_pred_words=12
2026-01-08 12:12:14,647: t15.2023.08.13 val PER: 0.2121
2026-01-08 12:12:14,647: t15.2023.08.18 val PER: 0.1928
2026-01-08 12:12:14,647: t15.2023.08.20 val PER: 0.1954
2026-01-08 12:12:14,647: t15.2023.08.25 val PER: 0.1973
2026-01-08 12:12:14,648: t15.2023.08.27 val PER: 0.2733
2026-01-08 12:12:14,648: t15.2023.09.01 val PER: 0.1575
2026-01-08 12:12:14,648: t15.2023.09.03 val PER: 0.2458
2026-01-08 12:12:14,648: t15.2023.09.24 val PER: 0.1954
2026-01-08 12:12:14,648: t15.2023.09.29 val PER: 0.1940
2026-01-08 12:12:14,649: t15.2023.10.01 val PER: 0.2338
2026-01-08 12:12:14,649: t15.2023.10.06 val PER: 0.1776
2026-01-08 12:12:14,649: t15.2023.10.08 val PER: 0.3207
2026-01-08 12:12:14,649: t15.2023.10.13 val PER: 0.2948
2026-01-08 12:12:14,649: t15.2023.10.15 val PER: 0.2202
2026-01-08 12:12:14,649: t15.2023.10.20 val PER: 0.2383
2026-01-08 12:12:14,649: t15.2023.10.22 val PER: 0.1871
2026-01-08 12:12:14,649: t15.2023.11.03 val PER: 0.2469
2026-01-08 12:12:14,650: t15.2023.11.04 val PER: 0.0990
2026-01-08 12:12:14,650: t15.2023.11.17 val PER: 0.1135
2026-01-08 12:12:14,650: t15.2023.11.19 val PER: 0.1218
2026-01-08 12:12:14,650: t15.2023.11.26 val PER: 0.2326
2026-01-08 12:12:14,650: t15.2023.12.03 val PER: 0.1975
2026-01-08 12:12:14,650: t15.2023.12.08 val PER: 0.1824
2026-01-08 12:12:14,650: t15.2023.12.10 val PER: 0.1616
2026-01-08 12:12:14,650: t15.2023.12.17 val PER: 0.1819
2026-01-08 12:12:14,650: t15.2023.12.29 val PER: 0.2121
2026-01-08 12:12:14,650: t15.2024.02.25 val PER: 0.1812
2026-01-08 12:12:14,651: t15.2024.03.08 val PER: 0.2703
2026-01-08 12:12:14,651: t15.2024.03.15 val PER: 0.2489
2026-01-08 12:12:14,651: t15.2024.03.17 val PER: 0.2218
2026-01-08 12:12:14,651: t15.2024.05.10 val PER: 0.2303
2026-01-08 12:12:14,651: t15.2024.06.14 val PER: 0.2208
2026-01-08 12:12:14,651: t15.2024.07.19 val PER: 0.2788
2026-01-08 12:12:14,651: t15.2024.07.21 val PER: 0.1483
2026-01-08 12:12:14,651: t15.2024.07.28 val PER: 0.2029
2026-01-08 12:12:14,651: t15.2025.01.10 val PER: 0.3678
2026-01-08 12:12:14,651: t15.2025.01.12 val PER: 0.2533
2026-01-08 12:12:14,652: t15.2025.03.14 val PER: 0.3639
2026-01-08 12:12:14,652: t15.2025.03.16 val PER: 0.2709
2026-01-08 12:12:14,652: t15.2025.03.30 val PER: 0.3552
2026-01-08 12:12:14,653: t15.2025.04.13 val PER: 0.2867
2026-01-08 12:12:31,545: Train batch 12200: loss: 14.52 grad norm: 55.48 time: 0.067
2026-01-08 12:12:48,174: Train batch 12400: loss: 10.60 grad norm: 44.93 time: 0.042
2026-01-08 12:12:57,084: Running test after training batch: 12500
2026-01-08 12:12:57,216: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:13:01,862: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa canned seats the kundert gatt cyst appointees has
2026-01-08 12:13:01,887: WER debug example
  GT : how does it keep the cost down
  PR : dowds sta sitz keep coots thus cost
2026-01-08 12:13:03,269: Val batch 12500: PER (avg): 0.2242 CTC Loss (avg): 23.4772 WER(1gram): 96.45% (n=64) time: 6.184
2026-01-08 12:13:03,269: WER lens: avg_true_words=6.16 avg_pred_words=5.88 max_pred_words=12
2026-01-08 12:13:03,270: t15.2023.08.13 val PER: 0.2079
2026-01-08 12:13:03,270: t15.2023.08.18 val PER: 0.1928
2026-01-08 12:13:03,270: t15.2023.08.20 val PER: 0.1811
2026-01-08 12:13:03,270: t15.2023.08.25 val PER: 0.1913
2026-01-08 12:13:03,270: t15.2023.08.27 val PER: 0.2781
2026-01-08 12:13:03,270: t15.2023.09.01 val PER: 0.1640
2026-01-08 12:13:03,270: t15.2023.09.03 val PER: 0.2435
2026-01-08 12:13:03,270: t15.2023.09.24 val PER: 0.1942
2026-01-08 12:13:03,270: t15.2023.09.29 val PER: 0.2017
2026-01-08 12:13:03,270: t15.2023.10.01 val PER: 0.2365
2026-01-08 12:13:03,271: t15.2023.10.06 val PER: 0.1755
2026-01-08 12:13:03,271: t15.2023.10.08 val PER: 0.3342
2026-01-08 12:13:03,271: t15.2023.10.13 val PER: 0.2956
2026-01-08 12:13:03,271: t15.2023.10.15 val PER: 0.2228
2026-01-08 12:13:03,271: t15.2023.10.20 val PER: 0.2282
2026-01-08 12:13:03,271: t15.2023.10.22 val PER: 0.1715
2026-01-08 12:13:03,271: t15.2023.11.03 val PER: 0.2564
2026-01-08 12:13:03,271: t15.2023.11.04 val PER: 0.0990
2026-01-08 12:13:03,271: t15.2023.11.17 val PER: 0.1026
2026-01-08 12:13:03,271: t15.2023.11.19 val PER: 0.1178
2026-01-08 12:13:03,271: t15.2023.11.26 val PER: 0.2203
2026-01-08 12:13:03,271: t15.2023.12.03 val PER: 0.1985
2026-01-08 12:13:03,271: t15.2023.12.08 val PER: 0.1818
2026-01-08 12:13:03,271: t15.2023.12.10 val PER: 0.1603
2026-01-08 12:13:03,271: t15.2023.12.17 val PER: 0.1902
2026-01-08 12:13:03,271: t15.2023.12.29 val PER: 0.2107
2026-01-08 12:13:03,271: t15.2024.02.25 val PER: 0.1798
2026-01-08 12:13:03,272: t15.2024.03.08 val PER: 0.2945
2026-01-08 12:13:03,272: t15.2024.03.15 val PER: 0.2433
2026-01-08 12:13:03,272: t15.2024.03.17 val PER: 0.2155
2026-01-08 12:13:03,272: t15.2024.05.10 val PER: 0.2288
2026-01-08 12:13:03,272: t15.2024.06.14 val PER: 0.2208
2026-01-08 12:13:03,272: t15.2024.07.19 val PER: 0.2900
2026-01-08 12:13:03,272: t15.2024.07.21 val PER: 0.1448
2026-01-08 12:13:03,272: t15.2024.07.28 val PER: 0.1934
2026-01-08 12:13:03,272: t15.2025.01.10 val PER: 0.3829
2026-01-08 12:13:03,272: t15.2025.01.12 val PER: 0.2556
2026-01-08 12:13:03,272: t15.2025.03.14 val PER: 0.3861
2026-01-08 12:13:03,272: t15.2025.03.16 val PER: 0.2749
2026-01-08 12:13:03,272: t15.2025.03.30 val PER: 0.3425
2026-01-08 12:13:03,272: t15.2025.04.13 val PER: 0.2867
2026-01-08 12:13:11,692: Train batch 12600: loss: 13.53 grad norm: 54.42 time: 0.060
2026-01-08 12:13:29,137: Train batch 12800: loss: 11.68 grad norm: 46.44 time: 0.055
2026-01-08 12:13:47,250: Train batch 13000: loss: 12.13 grad norm: 50.03 time: 0.069
2026-01-08 12:13:47,251: Running test after training batch: 13000
2026-01-08 12:13:47,358: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:13:52,046: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies seats the could hatz this appointees has
2026-01-08 12:13:52,069: WER debug example
  GT : how does it keep the cost down
  PR : turnham decency sitz kieper thus costs
2026-01-08 12:13:53,445: Val batch 13000: PER (avg): 0.2217 CTC Loss (avg): 23.7368 WER(1gram): 96.19% (n=64) time: 6.194
2026-01-08 12:13:53,445: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=11
2026-01-08 12:13:53,445: t15.2023.08.13 val PER: 0.2037
2026-01-08 12:13:53,445: t15.2023.08.18 val PER: 0.2003
2026-01-08 12:13:53,446: t15.2023.08.20 val PER: 0.1851
2026-01-08 12:13:53,446: t15.2023.08.25 val PER: 0.2003
2026-01-08 12:13:53,446: t15.2023.08.27 val PER: 0.2733
2026-01-08 12:13:53,446: t15.2023.09.01 val PER: 0.1664
2026-01-08 12:13:53,446: t15.2023.09.03 val PER: 0.2447
2026-01-08 12:13:53,446: t15.2023.09.24 val PER: 0.1966
2026-01-08 12:13:53,446: t15.2023.09.29 val PER: 0.1959
2026-01-08 12:13:53,446: t15.2023.10.01 val PER: 0.2431
2026-01-08 12:13:53,446: t15.2023.10.06 val PER: 0.1733
2026-01-08 12:13:53,446: t15.2023.10.08 val PER: 0.3315
2026-01-08 12:13:53,446: t15.2023.10.13 val PER: 0.2925
2026-01-08 12:13:53,446: t15.2023.10.15 val PER: 0.2142
2026-01-08 12:13:53,446: t15.2023.10.20 val PER: 0.2248
2026-01-08 12:13:53,446: t15.2023.10.22 val PER: 0.1715
2026-01-08 12:13:53,447: t15.2023.11.03 val PER: 0.2598
2026-01-08 12:13:53,447: t15.2023.11.04 val PER: 0.0956
2026-01-08 12:13:53,447: t15.2023.11.17 val PER: 0.1135
2026-01-08 12:13:53,447: t15.2023.11.19 val PER: 0.1118
2026-01-08 12:13:53,447: t15.2023.11.26 val PER: 0.2109
2026-01-08 12:13:53,447: t15.2023.12.03 val PER: 0.1996
2026-01-08 12:13:53,447: t15.2023.12.08 val PER: 0.1778
2026-01-08 12:13:53,447: t15.2023.12.10 val PER: 0.1629
2026-01-08 12:13:53,447: t15.2023.12.17 val PER: 0.1933
2026-01-08 12:13:53,447: t15.2023.12.29 val PER: 0.2086
2026-01-08 12:13:53,447: t15.2024.02.25 val PER: 0.1685
2026-01-08 12:13:53,447: t15.2024.03.08 val PER: 0.2731
2026-01-08 12:13:53,447: t15.2024.03.15 val PER: 0.2445
2026-01-08 12:13:53,447: t15.2024.03.17 val PER: 0.2092
2026-01-08 12:13:53,447: t15.2024.05.10 val PER: 0.2169
2026-01-08 12:13:53,447: t15.2024.06.14 val PER: 0.2224
2026-01-08 12:13:53,447: t15.2024.07.19 val PER: 0.2736
2026-01-08 12:13:53,448: t15.2024.07.21 val PER: 0.1428
2026-01-08 12:13:53,448: t15.2024.07.28 val PER: 0.1904
2026-01-08 12:13:53,448: t15.2025.01.10 val PER: 0.3705
2026-01-08 12:13:53,448: t15.2025.01.12 val PER: 0.2610
2026-01-08 12:13:53,448: t15.2025.03.14 val PER: 0.3698
2026-01-08 12:13:53,448: t15.2025.03.16 val PER: 0.2565
2026-01-08 12:13:53,448: t15.2025.03.30 val PER: 0.3379
2026-01-08 12:13:53,448: t15.2025.04.13 val PER: 0.2953
2026-01-08 12:14:10,799: Train batch 13200: loss: 20.67 grad norm: 78.92 time: 0.055
2026-01-08 12:14:27,977: Train batch 13400: loss: 16.19 grad norm: 87.68 time: 0.064
2026-01-08 12:14:36,616: Running test after training batch: 13500
2026-01-08 12:14:36,730: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:14:41,404: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies seats the canned gatt cyst appointees has
2026-01-08 12:14:41,429: WER debug example
  GT : how does it keep the cost down
  PR : hougham decency hitty skips thus costs dentzer
2026-01-08 12:14:42,834: Val batch 13500: PER (avg): 0.2160 CTC Loss (avg): 23.3360 WER(1gram): 96.45% (n=64) time: 6.217
2026-01-08 12:14:42,834: WER lens: avg_true_words=6.16 avg_pred_words=5.75 max_pred_words=11
2026-01-08 12:14:42,834: t15.2023.08.13 val PER: 0.1996
2026-01-08 12:14:42,834: t15.2023.08.18 val PER: 0.1911
2026-01-08 12:14:42,835: t15.2023.08.20 val PER: 0.1803
2026-01-08 12:14:42,835: t15.2023.08.25 val PER: 0.1822
2026-01-08 12:14:42,835: t15.2023.08.27 val PER: 0.2605
2026-01-08 12:14:42,835: t15.2023.09.01 val PER: 0.1550
2026-01-08 12:14:42,835: t15.2023.09.03 val PER: 0.2399
2026-01-08 12:14:42,835: t15.2023.09.24 val PER: 0.1917
2026-01-08 12:14:42,835: t15.2023.09.29 val PER: 0.1921
2026-01-08 12:14:42,835: t15.2023.10.01 val PER: 0.2411
2026-01-08 12:14:42,835: t15.2023.10.06 val PER: 0.1701
2026-01-08 12:14:42,835: t15.2023.10.08 val PER: 0.3072
2026-01-08 12:14:42,835: t15.2023.10.13 val PER: 0.2855
2026-01-08 12:14:42,835: t15.2023.10.15 val PER: 0.2123
2026-01-08 12:14:42,835: t15.2023.10.20 val PER: 0.2181
2026-01-08 12:14:42,836: t15.2023.10.22 val PER: 0.1682
2026-01-08 12:14:42,836: t15.2023.11.03 val PER: 0.2503
2026-01-08 12:14:42,836: t15.2023.11.04 val PER: 0.0956
2026-01-08 12:14:42,836: t15.2023.11.17 val PER: 0.1073
2026-01-08 12:14:42,836: t15.2023.11.19 val PER: 0.1118
2026-01-08 12:14:42,836: t15.2023.11.26 val PER: 0.2138
2026-01-08 12:14:42,836: t15.2023.12.03 val PER: 0.1807
2026-01-08 12:14:42,836: t15.2023.12.08 val PER: 0.1818
2026-01-08 12:14:42,836: t15.2023.12.10 val PER: 0.1498
2026-01-08 12:14:42,836: t15.2023.12.17 val PER: 0.1663
2026-01-08 12:14:42,836: t15.2023.12.29 val PER: 0.2045
2026-01-08 12:14:42,837: t15.2024.02.25 val PER: 0.1713
2026-01-08 12:14:42,837: t15.2024.03.08 val PER: 0.2660
2026-01-08 12:14:42,837: t15.2024.03.15 val PER: 0.2420
2026-01-08 12:14:42,837: t15.2024.03.17 val PER: 0.2120
2026-01-08 12:14:42,837: t15.2024.05.10 val PER: 0.2125
2026-01-08 12:14:42,837: t15.2024.06.14 val PER: 0.2066
2026-01-08 12:14:42,837: t15.2024.07.19 val PER: 0.2755
2026-01-08 12:14:42,837: t15.2024.07.21 val PER: 0.1297
2026-01-08 12:14:42,838: t15.2024.07.28 val PER: 0.1875
2026-01-08 12:14:42,838: t15.2025.01.10 val PER: 0.3815
2026-01-08 12:14:42,838: t15.2025.01.12 val PER: 0.2433
2026-01-08 12:14:42,838: t15.2025.03.14 val PER: 0.3757
2026-01-08 12:14:42,838: t15.2025.03.16 val PER: 0.2539
2026-01-08 12:14:42,838: t15.2025.03.30 val PER: 0.3448
2026-01-08 12:14:42,838: t15.2025.04.13 val PER: 0.2739
2026-01-08 12:14:51,467: Train batch 13600: loss: 20.40 grad norm: 76.16 time: 0.063
2026-01-08 12:15:09,137: Train batch 13800: loss: 13.18 grad norm: 61.92 time: 0.058
2026-01-08 12:15:26,813: Train batch 14000: loss: 19.71 grad norm: 67.60 time: 0.052
2026-01-08 12:15:26,813: Running test after training batch: 14000
2026-01-08 12:15:26,903: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:15:31,540: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies seats the could gatz this appointees his
2026-01-08 12:15:31,567: WER debug example
  GT : how does it keep the cost down
  PR : home decency hitty ski peeks thus costs
2026-01-08 12:15:32,940: Val batch 14000: PER (avg): 0.2180 CTC Loss (avg): 23.2437 WER(1gram): 97.21% (n=64) time: 6.127
2026-01-08 12:15:32,941: WER lens: avg_true_words=6.16 avg_pred_words=5.91 max_pred_words=11
2026-01-08 12:15:32,941: t15.2023.08.13 val PER: 0.2027
2026-01-08 12:15:32,941: t15.2023.08.18 val PER: 0.1928
2026-01-08 12:15:32,941: t15.2023.08.20 val PER: 0.1739
2026-01-08 12:15:32,941: t15.2023.08.25 val PER: 0.1762
2026-01-08 12:15:32,941: t15.2023.08.27 val PER: 0.2685
2026-01-08 12:15:32,941: t15.2023.09.01 val PER: 0.1591
2026-01-08 12:15:32,942: t15.2023.09.03 val PER: 0.2340
2026-01-08 12:15:32,942: t15.2023.09.24 val PER: 0.1917
2026-01-08 12:15:32,942: t15.2023.09.29 val PER: 0.1940
2026-01-08 12:15:32,942: t15.2023.10.01 val PER: 0.2318
2026-01-08 12:15:32,942: t15.2023.10.06 val PER: 0.1819
2026-01-08 12:15:32,942: t15.2023.10.08 val PER: 0.3126
2026-01-08 12:15:32,942: t15.2023.10.13 val PER: 0.2909
2026-01-08 12:15:32,942: t15.2023.10.15 val PER: 0.2149
2026-01-08 12:15:32,942: t15.2023.10.20 val PER: 0.2215
2026-01-08 12:15:32,942: t15.2023.10.22 val PER: 0.1693
2026-01-08 12:15:32,942: t15.2023.11.03 val PER: 0.2469
2026-01-08 12:15:32,943: t15.2023.11.04 val PER: 0.0887
2026-01-08 12:15:32,943: t15.2023.11.17 val PER: 0.1120
2026-01-08 12:15:32,943: t15.2023.11.19 val PER: 0.1158
2026-01-08 12:15:32,943: t15.2023.11.26 val PER: 0.2159
2026-01-08 12:15:32,943: t15.2023.12.03 val PER: 0.1880
2026-01-08 12:15:32,943: t15.2023.12.08 val PER: 0.1851
2026-01-08 12:15:32,943: t15.2023.12.10 val PER: 0.1603
2026-01-08 12:15:32,943: t15.2023.12.17 val PER: 0.1798
2026-01-08 12:15:32,943: t15.2023.12.29 val PER: 0.2141
2026-01-08 12:15:32,943: t15.2024.02.25 val PER: 0.1671
2026-01-08 12:15:32,943: t15.2024.03.08 val PER: 0.2788
2026-01-08 12:15:32,943: t15.2024.03.15 val PER: 0.2289
2026-01-08 12:15:32,943: t15.2024.03.17 val PER: 0.2099
2026-01-08 12:15:32,943: t15.2024.05.10 val PER: 0.2229
2026-01-08 12:15:32,944: t15.2024.06.14 val PER: 0.2161
2026-01-08 12:15:32,944: t15.2024.07.19 val PER: 0.2729
2026-01-08 12:15:32,944: t15.2024.07.21 val PER: 0.1393
2026-01-08 12:15:32,944: t15.2024.07.28 val PER: 0.1926
2026-01-08 12:15:32,944: t15.2025.01.10 val PER: 0.3650
2026-01-08 12:15:32,944: t15.2025.01.12 val PER: 0.2487
2026-01-08 12:15:32,944: t15.2025.03.14 val PER: 0.3713
2026-01-08 12:15:32,944: t15.2025.03.16 val PER: 0.2644
2026-01-08 12:15:32,944: t15.2025.03.30 val PER: 0.3494
2026-01-08 12:15:32,944: t15.2025.04.13 val PER: 0.2782
2026-01-08 12:15:49,536: Train batch 14200: loss: 14.44 grad norm: 60.84 time: 0.058
2026-01-08 12:16:06,345: Train batch 14400: loss: 11.82 grad norm: 49.55 time: 0.066
2026-01-08 12:16:15,144: Running test after training batch: 14500
2026-01-08 12:16:15,239: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:16:19,903: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa canned seats the keds gatt cyst appointees has
2026-01-08 12:16:19,929: WER debug example
  GT : how does it keep the cost down
  PR : mounds stutz hitz keep coots thus cost
2026-01-08 12:16:21,362: Val batch 14500: PER (avg): 0.2147 CTC Loss (avg): 23.1131 WER(1gram): 95.43% (n=64) time: 6.218
2026-01-08 12:16:21,363: WER lens: avg_true_words=6.16 avg_pred_words=5.72 max_pred_words=11
2026-01-08 12:16:21,363: t15.2023.08.13 val PER: 0.1944
2026-01-08 12:16:21,363: t15.2023.08.18 val PER: 0.1886
2026-01-08 12:16:21,363: t15.2023.08.20 val PER: 0.1716
2026-01-08 12:16:21,363: t15.2023.08.25 val PER: 0.1822
2026-01-08 12:16:21,363: t15.2023.08.27 val PER: 0.2717
2026-01-08 12:16:21,363: t15.2023.09.01 val PER: 0.1615
2026-01-08 12:16:21,363: t15.2023.09.03 val PER: 0.2375
2026-01-08 12:16:21,363: t15.2023.09.24 val PER: 0.1966
2026-01-08 12:16:21,363: t15.2023.09.29 val PER: 0.1972
2026-01-08 12:16:21,364: t15.2023.10.01 val PER: 0.2272
2026-01-08 12:16:21,364: t15.2023.10.06 val PER: 0.1701
2026-01-08 12:16:21,364: t15.2023.10.08 val PER: 0.3234
2026-01-08 12:16:21,364: t15.2023.10.13 val PER: 0.2878
2026-01-08 12:16:21,364: t15.2023.10.15 val PER: 0.2123
2026-01-08 12:16:21,364: t15.2023.10.20 val PER: 0.2114
2026-01-08 12:16:21,364: t15.2023.10.22 val PER: 0.1771
2026-01-08 12:16:21,364: t15.2023.11.03 val PER: 0.2456
2026-01-08 12:16:21,364: t15.2023.11.04 val PER: 0.0853
2026-01-08 12:16:21,364: t15.2023.11.17 val PER: 0.1104
2026-01-08 12:16:21,365: t15.2023.11.19 val PER: 0.1058
2026-01-08 12:16:21,365: t15.2023.11.26 val PER: 0.2051
2026-01-08 12:16:21,365: t15.2023.12.03 val PER: 0.1817
2026-01-08 12:16:21,365: t15.2023.12.08 val PER: 0.1731
2026-01-08 12:16:21,365: t15.2023.12.10 val PER: 0.1524
2026-01-08 12:16:21,365: t15.2023.12.17 val PER: 0.1809
2026-01-08 12:16:21,365: t15.2023.12.29 val PER: 0.2059
2026-01-08 12:16:21,365: t15.2024.02.25 val PER: 0.1699
2026-01-08 12:16:21,365: t15.2024.03.08 val PER: 0.2717
2026-01-08 12:16:21,365: t15.2024.03.15 val PER: 0.2358
2026-01-08 12:16:21,366: t15.2024.03.17 val PER: 0.2029
2026-01-08 12:16:21,366: t15.2024.05.10 val PER: 0.2184
2026-01-08 12:16:21,366: t15.2024.06.14 val PER: 0.2019
2026-01-08 12:16:21,366: t15.2024.07.19 val PER: 0.2676
2026-01-08 12:16:21,366: t15.2024.07.21 val PER: 0.1428
2026-01-08 12:16:21,366: t15.2024.07.28 val PER: 0.1934
2026-01-08 12:16:21,366: t15.2025.01.10 val PER: 0.3512
2026-01-08 12:16:21,366: t15.2025.01.12 val PER: 0.2479
2026-01-08 12:16:21,366: t15.2025.03.14 val PER: 0.3536
2026-01-08 12:16:21,366: t15.2025.03.16 val PER: 0.2565
2026-01-08 12:16:21,366: t15.2025.03.30 val PER: 0.3299
2026-01-08 12:16:21,366: t15.2025.04.13 val PER: 0.2767
2026-01-08 12:16:21,367: New best val WER(1gram) 96.19% --> 95.43%
2026-01-08 12:16:21,367: Checkpointing model
2026-01-08 12:16:21,508: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_linderman/checkpoint/best_checkpoint
2026-01-08 12:16:30,316: Train batch 14600: loss: 20.17 grad norm: 76.99 time: 0.062
2026-01-08 12:16:47,475: Train batch 14800: loss: 11.48 grad norm: 50.19 time: 0.052
2026-01-08 12:17:04,798: Train batch 15000: loss: 13.57 grad norm: 59.13 time: 0.054
2026-01-08 12:17:04,799: Running test after training batch: 15000
2026-01-08 12:17:04,917: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:17:09,793: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa canned seats the could gatz this appointees has swells
2026-01-08 12:17:09,819: WER debug example
  GT : how does it keep the cost down
  PR : dowds stutz hitz keep coots thus costs
2026-01-08 12:17:11,247: Val batch 15000: PER (avg): 0.2112 CTC Loss (avg): 22.5652 WER(1gram): 95.94% (n=64) time: 6.448
2026-01-08 12:17:11,247: WER lens: avg_true_words=6.16 avg_pred_words=5.88 max_pred_words=12
2026-01-08 12:17:11,248: t15.2023.08.13 val PER: 0.1892
2026-01-08 12:17:11,248: t15.2023.08.18 val PER: 0.1886
2026-01-08 12:17:11,248: t15.2023.08.20 val PER: 0.1708
2026-01-08 12:17:11,248: t15.2023.08.25 val PER: 0.1837
2026-01-08 12:17:11,248: t15.2023.08.27 val PER: 0.2637
2026-01-08 12:17:11,248: t15.2023.09.01 val PER: 0.1567
2026-01-08 12:17:11,248: t15.2023.09.03 val PER: 0.2292
2026-01-08 12:17:11,248: t15.2023.09.24 val PER: 0.1917
2026-01-08 12:17:11,248: t15.2023.09.29 val PER: 0.1870
2026-01-08 12:17:11,248: t15.2023.10.01 val PER: 0.2272
2026-01-08 12:17:11,248: t15.2023.10.06 val PER: 0.1712
2026-01-08 12:17:11,248: t15.2023.10.08 val PER: 0.3194
2026-01-08 12:17:11,249: t15.2023.10.13 val PER: 0.2878
2026-01-08 12:17:11,249: t15.2023.10.15 val PER: 0.2030
2026-01-08 12:17:11,249: t15.2023.10.20 val PER: 0.2181
2026-01-08 12:17:11,249: t15.2023.10.22 val PER: 0.1670
2026-01-08 12:17:11,249: t15.2023.11.03 val PER: 0.2456
2026-01-08 12:17:11,249: t15.2023.11.04 val PER: 0.0819
2026-01-08 12:17:11,249: t15.2023.11.17 val PER: 0.1026
2026-01-08 12:17:11,249: t15.2023.11.19 val PER: 0.1038
2026-01-08 12:17:11,249: t15.2023.11.26 val PER: 0.2022
2026-01-08 12:17:11,249: t15.2023.12.03 val PER: 0.1702
2026-01-08 12:17:11,249: t15.2023.12.08 val PER: 0.1758
2026-01-08 12:17:11,249: t15.2023.12.10 val PER: 0.1472
2026-01-08 12:17:11,249: t15.2023.12.17 val PER: 0.1736
2026-01-08 12:17:11,249: t15.2023.12.29 val PER: 0.2032
2026-01-08 12:17:11,250: t15.2024.02.25 val PER: 0.1573
2026-01-08 12:17:11,250: t15.2024.03.08 val PER: 0.2646
2026-01-08 12:17:11,250: t15.2024.03.15 val PER: 0.2301
2026-01-08 12:17:11,250: t15.2024.03.17 val PER: 0.2022
2026-01-08 12:17:11,250: t15.2024.05.10 val PER: 0.2184
2026-01-08 12:17:11,250: t15.2024.06.14 val PER: 0.2019
2026-01-08 12:17:11,250: t15.2024.07.19 val PER: 0.2670
2026-01-08 12:17:11,250: t15.2024.07.21 val PER: 0.1386
2026-01-08 12:17:11,250: t15.2024.07.28 val PER: 0.1824
2026-01-08 12:17:11,250: t15.2025.01.10 val PER: 0.3416
2026-01-08 12:17:11,250: t15.2025.01.12 val PER: 0.2379
2026-01-08 12:17:11,250: t15.2025.03.14 val PER: 0.3772
2026-01-08 12:17:11,251: t15.2025.03.16 val PER: 0.2592
2026-01-08 12:17:11,251: t15.2025.03.30 val PER: 0.3391
2026-01-08 12:17:11,251: t15.2025.04.13 val PER: 0.2710
2026-01-08 12:17:29,210: Train batch 15200: loss: 12.29 grad norm: 381.19 time: 0.059
2026-01-08 12:17:46,551: Train batch 15400: loss: 17.97 grad norm: 65.36 time: 0.055
2026-01-08 12:17:55,110: Running test after training batch: 15500
2026-01-08 12:17:55,211: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:17:59,873: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies seats the could hatz this appointees has
2026-01-08 12:17:59,900: WER debug example
  GT : how does it keep the cost down
  PR : mounds dusty hitz kieper thus costs
2026-01-08 12:18:01,340: Val batch 15500: PER (avg): 0.2108 CTC Loss (avg): 22.4780 WER(1gram): 95.94% (n=64) time: 6.229
2026-01-08 12:18:01,340: WER lens: avg_true_words=6.16 avg_pred_words=5.75 max_pred_words=11
2026-01-08 12:18:01,340: t15.2023.08.13 val PER: 0.1892
2026-01-08 12:18:01,340: t15.2023.08.18 val PER: 0.1827
2026-01-08 12:18:01,340: t15.2023.08.20 val PER: 0.1676
2026-01-08 12:18:01,341: t15.2023.08.25 val PER: 0.1883
2026-01-08 12:18:01,341: t15.2023.08.27 val PER: 0.2524
2026-01-08 12:18:01,341: t15.2023.09.01 val PER: 0.1526
2026-01-08 12:18:01,341: t15.2023.09.03 val PER: 0.2233
2026-01-08 12:18:01,341: t15.2023.09.24 val PER: 0.1845
2026-01-08 12:18:01,341: t15.2023.09.29 val PER: 0.1908
2026-01-08 12:18:01,342: t15.2023.10.01 val PER: 0.2232
2026-01-08 12:18:01,342: t15.2023.10.06 val PER: 0.1701
2026-01-08 12:18:01,342: t15.2023.10.08 val PER: 0.3139
2026-01-08 12:18:01,342: t15.2023.10.13 val PER: 0.2832
2026-01-08 12:18:01,342: t15.2023.10.15 val PER: 0.2076
2026-01-08 12:18:01,342: t15.2023.10.20 val PER: 0.2248
2026-01-08 12:18:01,342: t15.2023.10.22 val PER: 0.1682
2026-01-08 12:18:01,342: t15.2023.11.03 val PER: 0.2388
2026-01-08 12:18:01,342: t15.2023.11.04 val PER: 0.0751
2026-01-08 12:18:01,342: t15.2023.11.17 val PER: 0.1042
2026-01-08 12:18:01,342: t15.2023.11.19 val PER: 0.1118
2026-01-08 12:18:01,342: t15.2023.11.26 val PER: 0.2043
2026-01-08 12:18:01,342: t15.2023.12.03 val PER: 0.1807
2026-01-08 12:18:01,343: t15.2023.12.08 val PER: 0.1684
2026-01-08 12:18:01,343: t15.2023.12.10 val PER: 0.1577
2026-01-08 12:18:01,343: t15.2023.12.17 val PER: 0.1653
2026-01-08 12:18:01,343: t15.2023.12.29 val PER: 0.2038
2026-01-08 12:18:01,343: t15.2024.02.25 val PER: 0.1601
2026-01-08 12:18:01,343: t15.2024.03.08 val PER: 0.2703
2026-01-08 12:18:01,343: t15.2024.03.15 val PER: 0.2395
2026-01-08 12:18:01,343: t15.2024.03.17 val PER: 0.1974
2026-01-08 12:18:01,343: t15.2024.05.10 val PER: 0.2080
2026-01-08 12:18:01,343: t15.2024.06.14 val PER: 0.2050
2026-01-08 12:18:01,343: t15.2024.07.19 val PER: 0.2663
2026-01-08 12:18:01,344: t15.2024.07.21 val PER: 0.1324
2026-01-08 12:18:01,344: t15.2024.07.28 val PER: 0.1890
2026-01-08 12:18:01,344: t15.2025.01.10 val PER: 0.3554
2026-01-08 12:18:01,344: t15.2025.01.12 val PER: 0.2440
2026-01-08 12:18:01,344: t15.2025.03.14 val PER: 0.3757
2026-01-08 12:18:01,344: t15.2025.03.16 val PER: 0.2526
2026-01-08 12:18:01,344: t15.2025.03.30 val PER: 0.3368
2026-01-08 12:18:01,344: t15.2025.04.13 val PER: 0.2725
2026-01-08 12:18:09,953: Train batch 15600: loss: 20.07 grad norm: 71.56 time: 0.063
2026-01-08 12:18:27,479: Train batch 15800: loss: 21.12 grad norm: 65.57 time: 0.070
2026-01-08 12:18:44,431: Train batch 16000: loss: 16.21 grad norm: 68.23 time: 0.058
2026-01-08 12:18:44,431: Running test after training batch: 16000
2026-01-08 12:18:44,565: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:18:49,234: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies seats the could gatt cyst appointees has
2026-01-08 12:18:49,260: WER debug example
  GT : how does it keep the cost down
  PR : moudy dusty hitz kieper thus costs
2026-01-08 12:18:50,682: Val batch 16000: PER (avg): 0.2101 CTC Loss (avg): 22.7634 WER(1gram): 96.19% (n=64) time: 6.250
2026-01-08 12:18:50,682: WER lens: avg_true_words=6.16 avg_pred_words=5.78 max_pred_words=12
2026-01-08 12:18:50,682: t15.2023.08.13 val PER: 0.1902
2026-01-08 12:18:50,682: t15.2023.08.18 val PER: 0.1903
2026-01-08 12:18:50,682: t15.2023.08.20 val PER: 0.1708
2026-01-08 12:18:50,682: t15.2023.08.25 val PER: 0.1852
2026-01-08 12:18:50,682: t15.2023.08.27 val PER: 0.2540
2026-01-08 12:18:50,682: t15.2023.09.01 val PER: 0.1502
2026-01-08 12:18:50,682: t15.2023.09.03 val PER: 0.2316
2026-01-08 12:18:50,683: t15.2023.09.24 val PER: 0.1857
2026-01-08 12:18:50,683: t15.2023.09.29 val PER: 0.1914
2026-01-08 12:18:50,683: t15.2023.10.01 val PER: 0.2252
2026-01-08 12:18:50,683: t15.2023.10.06 val PER: 0.1658
2026-01-08 12:18:50,683: t15.2023.10.08 val PER: 0.3234
2026-01-08 12:18:50,683: t15.2023.10.13 val PER: 0.2894
2026-01-08 12:18:50,683: t15.2023.10.15 val PER: 0.2076
2026-01-08 12:18:50,683: t15.2023.10.20 val PER: 0.2248
2026-01-08 12:18:50,684: t15.2023.10.22 val PER: 0.1670
2026-01-08 12:18:50,684: t15.2023.11.03 val PER: 0.2395
2026-01-08 12:18:50,684: t15.2023.11.04 val PER: 0.0853
2026-01-08 12:18:50,684: t15.2023.11.17 val PER: 0.1089
2026-01-08 12:18:50,684: t15.2023.11.19 val PER: 0.1078
2026-01-08 12:18:50,684: t15.2023.11.26 val PER: 0.2000
2026-01-08 12:18:50,684: t15.2023.12.03 val PER: 0.1723
2026-01-08 12:18:50,684: t15.2023.12.08 val PER: 0.1664
2026-01-08 12:18:50,684: t15.2023.12.10 val PER: 0.1498
2026-01-08 12:18:50,684: t15.2023.12.17 val PER: 0.1674
2026-01-08 12:18:50,684: t15.2023.12.29 val PER: 0.1956
2026-01-08 12:18:50,685: t15.2024.02.25 val PER: 0.1601
2026-01-08 12:18:50,685: t15.2024.03.08 val PER: 0.2632
2026-01-08 12:18:50,685: t15.2024.03.15 val PER: 0.2351
2026-01-08 12:18:50,685: t15.2024.03.17 val PER: 0.1953
2026-01-08 12:18:50,685: t15.2024.05.10 val PER: 0.2199
2026-01-08 12:18:50,685: t15.2024.06.14 val PER: 0.2035
2026-01-08 12:18:50,685: t15.2024.07.19 val PER: 0.2591
2026-01-08 12:18:50,685: t15.2024.07.21 val PER: 0.1352
2026-01-08 12:18:50,685: t15.2024.07.28 val PER: 0.1882
2026-01-08 12:18:50,685: t15.2025.01.10 val PER: 0.3540
2026-01-08 12:18:50,685: t15.2025.01.12 val PER: 0.2394
2026-01-08 12:18:50,685: t15.2025.03.14 val PER: 0.3698
2026-01-08 12:18:50,685: t15.2025.03.16 val PER: 0.2526
2026-01-08 12:18:50,685: t15.2025.03.30 val PER: 0.3310
2026-01-08 12:18:50,685: t15.2025.04.13 val PER: 0.2725
2026-01-08 12:19:08,520: Train batch 16200: loss: 10.95 grad norm: 49.98 time: 0.057
2026-01-08 12:19:25,659: Train batch 16400: loss: 15.17 grad norm: 61.35 time: 0.059
2026-01-08 12:19:34,111: Running test after training batch: 16500
2026-01-08 12:19:34,215: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:19:39,010: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies cetus the could gatt cyst appointees has
2026-01-08 12:19:39,037: WER debug example
  GT : how does it keep the cost down
  PR : meitz dusty hitty ski piqued thus cost
2026-01-08 12:19:40,456: Val batch 16500: PER (avg): 0.2090 CTC Loss (avg): 23.0820 WER(1gram): 95.69% (n=64) time: 6.344
2026-01-08 12:19:40,456: WER lens: avg_true_words=6.16 avg_pred_words=5.59 max_pred_words=10
2026-01-08 12:19:40,457: t15.2023.08.13 val PER: 0.1902
2026-01-08 12:19:40,457: t15.2023.08.18 val PER: 0.1852
2026-01-08 12:19:40,457: t15.2023.08.20 val PER: 0.1739
2026-01-08 12:19:40,457: t15.2023.08.25 val PER: 0.1807
2026-01-08 12:19:40,457: t15.2023.08.27 val PER: 0.2524
2026-01-08 12:19:40,457: t15.2023.09.01 val PER: 0.1534
2026-01-08 12:19:40,457: t15.2023.09.03 val PER: 0.2316
2026-01-08 12:19:40,457: t15.2023.09.24 val PER: 0.1869
2026-01-08 12:19:40,457: t15.2023.09.29 val PER: 0.1927
2026-01-08 12:19:40,458: t15.2023.10.01 val PER: 0.2279
2026-01-08 12:19:40,458: t15.2023.10.06 val PER: 0.1712
2026-01-08 12:19:40,458: t15.2023.10.08 val PER: 0.3248
2026-01-08 12:19:40,458: t15.2023.10.13 val PER: 0.2793
2026-01-08 12:19:40,458: t15.2023.10.15 val PER: 0.2030
2026-01-08 12:19:40,458: t15.2023.10.20 val PER: 0.2181
2026-01-08 12:19:40,458: t15.2023.10.22 val PER: 0.1659
2026-01-08 12:19:40,458: t15.2023.11.03 val PER: 0.2381
2026-01-08 12:19:40,458: t15.2023.11.04 val PER: 0.0751
2026-01-08 12:19:40,458: t15.2023.11.17 val PER: 0.1011
2026-01-08 12:19:40,458: t15.2023.11.19 val PER: 0.1018
2026-01-08 12:19:40,459: t15.2023.11.26 val PER: 0.2065
2026-01-08 12:19:40,459: t15.2023.12.03 val PER: 0.1807
2026-01-08 12:19:40,459: t15.2023.12.08 val PER: 0.1638
2026-01-08 12:19:40,459: t15.2023.12.10 val PER: 0.1498
2026-01-08 12:19:40,459: t15.2023.12.17 val PER: 0.1653
2026-01-08 12:19:40,459: t15.2023.12.29 val PER: 0.1956
2026-01-08 12:19:40,459: t15.2024.02.25 val PER: 0.1587
2026-01-08 12:19:40,459: t15.2024.03.08 val PER: 0.2674
2026-01-08 12:19:40,459: t15.2024.03.15 val PER: 0.2370
2026-01-08 12:19:40,459: t15.2024.03.17 val PER: 0.2001
2026-01-08 12:19:40,459: t15.2024.05.10 val PER: 0.2021
2026-01-08 12:19:40,459: t15.2024.06.14 val PER: 0.1956
2026-01-08 12:19:40,460: t15.2024.07.19 val PER: 0.2630
2026-01-08 12:19:40,460: t15.2024.07.21 val PER: 0.1262
2026-01-08 12:19:40,460: t15.2024.07.28 val PER: 0.1875
2026-01-08 12:19:40,460: t15.2025.01.10 val PER: 0.3471
2026-01-08 12:19:40,460: t15.2025.01.12 val PER: 0.2348
2026-01-08 12:19:40,460: t15.2025.03.14 val PER: 0.3639
2026-01-08 12:19:40,460: t15.2025.03.16 val PER: 0.2526
2026-01-08 12:19:40,460: t15.2025.03.30 val PER: 0.3322
2026-01-08 12:19:40,461: t15.2025.04.13 val PER: 0.2625
2026-01-08 12:19:49,631: Train batch 16600: loss: 16.85 grad norm: 63.36 time: 0.055
2026-01-08 12:20:07,985: Train batch 16800: loss: 22.20 grad norm: 74.28 time: 0.064
2026-01-08 12:20:26,158: Train batch 17000: loss: 14.58 grad norm: 206.58 time: 0.083
2026-01-08 12:20:26,158: Running test after training batch: 17000
2026-01-08 12:20:26,251: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:20:30,948: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies cetus the could gatz this appointees has swells
2026-01-08 12:20:30,975: WER debug example
  GT : how does it keep the cost down
  PR : dietz dusty hitz kieper thus costs
2026-01-08 12:20:32,442: Val batch 17000: PER (avg): 0.2068 CTC Loss (avg): 23.0202 WER(1gram): 95.18% (n=64) time: 6.284
2026-01-08 12:20:32,443: WER lens: avg_true_words=6.16 avg_pred_words=5.70 max_pred_words=10
2026-01-08 12:20:32,443: t15.2023.08.13 val PER: 0.1913
2026-01-08 12:20:32,443: t15.2023.08.18 val PER: 0.1819
2026-01-08 12:20:32,443: t15.2023.08.20 val PER: 0.1644
2026-01-08 12:20:32,443: t15.2023.08.25 val PER: 0.1867
2026-01-08 12:20:32,443: t15.2023.08.27 val PER: 0.2444
2026-01-08 12:20:32,443: t15.2023.09.01 val PER: 0.1494
2026-01-08 12:20:32,443: t15.2023.09.03 val PER: 0.2162
2026-01-08 12:20:32,443: t15.2023.09.24 val PER: 0.1760
2026-01-08 12:20:32,444: t15.2023.09.29 val PER: 0.1883
2026-01-08 12:20:32,444: t15.2023.10.01 val PER: 0.2226
2026-01-08 12:20:32,444: t15.2023.10.06 val PER: 0.1658
2026-01-08 12:20:32,444: t15.2023.10.08 val PER: 0.3139
2026-01-08 12:20:32,444: t15.2023.10.13 val PER: 0.2801
2026-01-08 12:20:32,444: t15.2023.10.15 val PER: 0.2011
2026-01-08 12:20:32,444: t15.2023.10.20 val PER: 0.2248
2026-01-08 12:20:32,444: t15.2023.10.22 val PER: 0.1648
2026-01-08 12:20:32,444: t15.2023.11.03 val PER: 0.2381
2026-01-08 12:20:32,444: t15.2023.11.04 val PER: 0.0819
2026-01-08 12:20:32,444: t15.2023.11.17 val PER: 0.0980
2026-01-08 12:20:32,444: t15.2023.11.19 val PER: 0.1038
2026-01-08 12:20:32,444: t15.2023.11.26 val PER: 0.2029
2026-01-08 12:20:32,444: t15.2023.12.03 val PER: 0.1681
2026-01-08 12:20:32,444: t15.2023.12.08 val PER: 0.1664
2026-01-08 12:20:32,444: t15.2023.12.10 val PER: 0.1498
2026-01-08 12:20:32,445: t15.2023.12.17 val PER: 0.1715
2026-01-08 12:20:32,445: t15.2023.12.29 val PER: 0.1956
2026-01-08 12:20:32,445: t15.2024.02.25 val PER: 0.1671
2026-01-08 12:20:32,445: t15.2024.03.08 val PER: 0.2703
2026-01-08 12:20:32,445: t15.2024.03.15 val PER: 0.2339
2026-01-08 12:20:32,445: t15.2024.03.17 val PER: 0.1974
2026-01-08 12:20:32,445: t15.2024.05.10 val PER: 0.1991
2026-01-08 12:20:32,445: t15.2024.06.14 val PER: 0.2066
2026-01-08 12:20:32,445: t15.2024.07.19 val PER: 0.2657
2026-01-08 12:20:32,445: t15.2024.07.21 val PER: 0.1255
2026-01-08 12:20:32,445: t15.2024.07.28 val PER: 0.1787
2026-01-08 12:20:32,445: t15.2025.01.10 val PER: 0.3485
2026-01-08 12:20:32,446: t15.2025.01.12 val PER: 0.2379
2026-01-08 12:20:32,446: t15.2025.03.14 val PER: 0.3669
2026-01-08 12:20:32,446: t15.2025.03.16 val PER: 0.2421
2026-01-08 12:20:32,446: t15.2025.03.30 val PER: 0.3230
2026-01-08 12:20:32,446: t15.2025.04.13 val PER: 0.2668
2026-01-08 12:20:32,447: New best val WER(1gram) 95.43% --> 95.18%
2026-01-08 12:20:32,447: Checkpointing model
2026-01-08 12:20:32,589: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/combined_linderman/checkpoint/best_checkpoint
2026-01-08 12:20:49,993: Train batch 17200: loss: 18.78 grad norm: 781.82 time: 0.086
2026-01-08 12:21:07,630: Train batch 17400: loss: 17.54 grad norm: 65.93 time: 0.072
2026-01-08 12:21:16,361: Running test after training batch: 17500
2026-01-08 12:21:16,495: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:21:21,278: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies cetus the could gatt cyst appointees has
2026-01-08 12:21:21,306: WER debug example
  GT : how does it keep the cost down
  PR : miesse dusty hitty skipper thus cost
2026-01-08 12:21:22,769: Val batch 17500: PER (avg): 0.2062 CTC Loss (avg): 21.9923 WER(1gram): 96.70% (n=64) time: 6.407
2026-01-08 12:21:22,769: WER lens: avg_true_words=6.16 avg_pred_words=5.83 max_pred_words=11
2026-01-08 12:21:22,769: t15.2023.08.13 val PER: 0.1778
2026-01-08 12:21:22,770: t15.2023.08.18 val PER: 0.1785
2026-01-08 12:21:22,770: t15.2023.08.20 val PER: 0.1644
2026-01-08 12:21:22,770: t15.2023.08.25 val PER: 0.1792
2026-01-08 12:21:22,770: t15.2023.08.27 val PER: 0.2572
2026-01-08 12:21:22,770: t15.2023.09.01 val PER: 0.1437
2026-01-08 12:21:22,770: t15.2023.09.03 val PER: 0.2221
2026-01-08 12:21:22,770: t15.2023.09.24 val PER: 0.1845
2026-01-08 12:21:22,770: t15.2023.09.29 val PER: 0.1857
2026-01-08 12:21:22,770: t15.2023.10.01 val PER: 0.2226
2026-01-08 12:21:22,770: t15.2023.10.06 val PER: 0.1679
2026-01-08 12:21:22,770: t15.2023.10.08 val PER: 0.3139
2026-01-08 12:21:22,770: t15.2023.10.13 val PER: 0.2847
2026-01-08 12:21:22,770: t15.2023.10.15 val PER: 0.2024
2026-01-08 12:21:22,770: t15.2023.10.20 val PER: 0.2181
2026-01-08 12:21:22,771: t15.2023.10.22 val PER: 0.1592
2026-01-08 12:21:22,771: t15.2023.11.03 val PER: 0.2395
2026-01-08 12:21:22,771: t15.2023.11.04 val PER: 0.0785
2026-01-08 12:21:22,771: t15.2023.11.17 val PER: 0.1026
2026-01-08 12:21:22,771: t15.2023.11.19 val PER: 0.1038
2026-01-08 12:21:22,771: t15.2023.11.26 val PER: 0.1978
2026-01-08 12:21:22,771: t15.2023.12.03 val PER: 0.1628
2026-01-08 12:21:22,771: t15.2023.12.08 val PER: 0.1671
2026-01-08 12:21:22,771: t15.2023.12.10 val PER: 0.1485
2026-01-08 12:21:22,771: t15.2023.12.17 val PER: 0.1684
2026-01-08 12:21:22,771: t15.2023.12.29 val PER: 0.1956
2026-01-08 12:21:22,771: t15.2024.02.25 val PER: 0.1559
2026-01-08 12:21:22,771: t15.2024.03.08 val PER: 0.2617
2026-01-08 12:21:22,771: t15.2024.03.15 val PER: 0.2333
2026-01-08 12:21:22,771: t15.2024.03.17 val PER: 0.2008
2026-01-08 12:21:22,771: t15.2024.05.10 val PER: 0.1961
2026-01-08 12:21:22,772: t15.2024.06.14 val PER: 0.2019
2026-01-08 12:21:22,772: t15.2024.07.19 val PER: 0.2643
2026-01-08 12:21:22,772: t15.2024.07.21 val PER: 0.1276
2026-01-08 12:21:22,772: t15.2024.07.28 val PER: 0.1809
2026-01-08 12:21:22,772: t15.2025.01.10 val PER: 0.3430
2026-01-08 12:21:22,772: t15.2025.01.12 val PER: 0.2363
2026-01-08 12:21:22,772: t15.2025.03.14 val PER: 0.3698
2026-01-08 12:21:22,772: t15.2025.03.16 val PER: 0.2539
2026-01-08 12:21:22,772: t15.2025.03.30 val PER: 0.3264
2026-01-08 12:21:22,772: t15.2025.04.13 val PER: 0.2668
2026-01-08 12:21:38,922: Train batch 17600: loss: 18.33 grad norm: 57.66 time: 0.052
2026-01-08 12:21:56,557: Train batch 17800: loss: 11.22 grad norm: 42.11 time: 0.043
2026-01-08 12:22:14,059: Train batch 18000: loss: 14.44 grad norm: 248.96 time: 0.063
2026-01-08 12:22:14,060: Running test after training batch: 18000
2026-01-08 12:22:14,187: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:22:19,262: WER debug example
  GT : you can see the code at this point as well
  PR : yucaipa candies cetus the could gatt cyst appointees has
2026-01-08 12:22:19,289: WER debug example
  GT : how does it keep the cost down
  PR : dietz dusty hitz kieper thus cost
2026-01-08 12:22:20,731: Val batch 18000: PER (avg): 0.2059 CTC Loss (avg): 22.6558 WER(1gram): 95.43% (n=64) time: 6.671
2026-01-08 12:22:20,731: WER lens: avg_true_words=6.16 avg_pred_words=5.73 max_pred_words=11
2026-01-08 12:22:20,732: t15.2023.08.13 val PER: 0.1840
2026-01-08 12:22:20,732: t15.2023.08.18 val PER: 0.1752
2026-01-08 12:22:20,732: t15.2023.08.20 val PER: 0.1692
2026-01-08 12:22:20,732: t15.2023.08.25 val PER: 0.1822
2026-01-08 12:22:20,732: t15.2023.08.27 val PER: 0.2476
2026-01-08 12:22:20,732: t15.2023.09.01 val PER: 0.1429
2026-01-08 12:22:20,732: t15.2023.09.03 val PER: 0.2233
2026-01-08 12:22:20,732: t15.2023.09.24 val PER: 0.1784
2026-01-08 12:22:20,732: t15.2023.09.29 val PER: 0.1870
2026-01-08 12:22:20,732: t15.2023.10.01 val PER: 0.2219
2026-01-08 12:22:20,732: t15.2023.10.06 val PER: 0.1668
2026-01-08 12:22:20,732: t15.2023.10.08 val PER: 0.3139
2026-01-08 12:22:20,732: t15.2023.10.13 val PER: 0.2839
2026-01-08 12:22:20,732: t15.2023.10.15 val PER: 0.2057
2026-01-08 12:22:20,732: t15.2023.10.20 val PER: 0.2148
2026-01-08 12:22:20,733: t15.2023.10.22 val PER: 0.1581
2026-01-08 12:22:20,733: t15.2023.11.03 val PER: 0.2415
2026-01-08 12:22:20,733: t15.2023.11.04 val PER: 0.0717
2026-01-08 12:22:20,733: t15.2023.11.17 val PER: 0.0964
2026-01-08 12:22:20,733: t15.2023.11.19 val PER: 0.1078
2026-01-08 12:22:20,733: t15.2023.11.26 val PER: 0.2000
2026-01-08 12:22:20,733: t15.2023.12.03 val PER: 0.1702
2026-01-08 12:22:20,733: t15.2023.12.08 val PER: 0.1618
2026-01-08 12:22:20,733: t15.2023.12.10 val PER: 0.1432
2026-01-08 12:22:20,733: t15.2023.12.17 val PER: 0.1653
2026-01-08 12:22:20,733: t15.2023.12.29 val PER: 0.1963
2026-01-08 12:22:20,733: t15.2024.02.25 val PER: 0.1615
2026-01-08 12:22:20,733: t15.2024.03.08 val PER: 0.2674
2026-01-08 12:22:20,734: t15.2024.03.15 val PER: 0.2283
2026-01-08 12:22:20,734: t15.2024.03.17 val PER: 0.2001
2026-01-08 12:22:20,734: t15.2024.05.10 val PER: 0.1976
2026-01-08 12:22:20,734: t15.2024.06.14 val PER: 0.2019
2026-01-08 12:22:20,734: t15.2024.07.19 val PER: 0.2630
2026-01-08 12:22:20,734: t15.2024.07.21 val PER: 0.1221
2026-01-08 12:22:20,734: t15.2024.07.28 val PER: 0.1816
2026-01-08 12:22:20,734: t15.2025.01.10 val PER: 0.3416
2026-01-08 12:22:20,734: t15.2025.01.12 val PER: 0.2356
2026-01-08 12:22:20,734: t15.2025.03.14 val PER: 0.3639
2026-01-08 12:22:20,734: t15.2025.03.16 val PER: 0.2500
2026-01-08 12:22:20,734: t15.2025.03.30 val PER: 0.3345
2026-01-08 12:22:20,734: t15.2025.04.13 val PER: 0.2668
2026-01-08 12:22:39,170: Train batch 18200: loss: 17.11 grad norm: 997.26 time: 0.076
2026-01-08 12:22:57,903: Train batch 18400: loss: 11.11 grad norm: 52.46 time: 0.060
2026-01-08 12:23:07,043: Running test after training batch: 18500
2026-01-08 12:23:07,177: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:23:11,867: WER debug example
  GT : you can see the code at this point as well
  PR : nuclei candies cetus the could gatt cyst appointees has
2026-01-08 12:23:11,894: WER debug example
  GT : how does it keep the cost down
  PR : dease dusty hitty skipper thus cost setzer
2026-01-08 12:23:13,326: Val batch 18500: PER (avg): 0.2044 CTC Loss (avg): 22.1800 WER(1gram): 96.19% (n=64) time: 6.282
2026-01-08 12:23:13,326: WER lens: avg_true_words=6.16 avg_pred_words=5.78 max_pred_words=12
2026-01-08 12:23:13,326: t15.2023.08.13 val PER: 0.1819
2026-01-08 12:23:13,326: t15.2023.08.18 val PER: 0.1769
2026-01-08 12:23:13,326: t15.2023.08.20 val PER: 0.1676
2026-01-08 12:23:13,326: t15.2023.08.25 val PER: 0.1747
2026-01-08 12:23:13,326: t15.2023.08.27 val PER: 0.2556
2026-01-08 12:23:13,327: t15.2023.09.01 val PER: 0.1437
2026-01-08 12:23:13,327: t15.2023.09.03 val PER: 0.2173
2026-01-08 12:23:13,327: t15.2023.09.24 val PER: 0.1748
2026-01-08 12:23:13,327: t15.2023.09.29 val PER: 0.1889
2026-01-08 12:23:13,327: t15.2023.10.01 val PER: 0.2199
2026-01-08 12:23:13,327: t15.2023.10.06 val PER: 0.1604
2026-01-08 12:23:13,327: t15.2023.10.08 val PER: 0.3072
2026-01-08 12:23:13,327: t15.2023.10.13 val PER: 0.2855
2026-01-08 12:23:13,327: t15.2023.10.15 val PER: 0.1971
2026-01-08 12:23:13,327: t15.2023.10.20 val PER: 0.2148
2026-01-08 12:23:13,327: t15.2023.10.22 val PER: 0.1604
2026-01-08 12:23:13,327: t15.2023.11.03 val PER: 0.2408
2026-01-08 12:23:13,327: t15.2023.11.04 val PER: 0.0717
2026-01-08 12:23:13,327: t15.2023.11.17 val PER: 0.0995
2026-01-08 12:23:13,328: t15.2023.11.19 val PER: 0.1038
2026-01-08 12:23:13,328: t15.2023.11.26 val PER: 0.1964
2026-01-08 12:23:13,328: t15.2023.12.03 val PER: 0.1649
2026-01-08 12:23:13,328: t15.2023.12.08 val PER: 0.1658
2026-01-08 12:23:13,328: t15.2023.12.10 val PER: 0.1485
2026-01-08 12:23:13,328: t15.2023.12.17 val PER: 0.1715
2026-01-08 12:23:13,328: t15.2023.12.29 val PER: 0.1935
2026-01-08 12:23:13,328: t15.2024.02.25 val PER: 0.1503
2026-01-08 12:23:13,328: t15.2024.03.08 val PER: 0.2603
2026-01-08 12:23:13,328: t15.2024.03.15 val PER: 0.2295
2026-01-08 12:23:13,328: t15.2024.03.17 val PER: 0.1960
2026-01-08 12:23:13,328: t15.2024.05.10 val PER: 0.2006
2026-01-08 12:23:13,328: t15.2024.06.14 val PER: 0.2019
2026-01-08 12:23:13,328: t15.2024.07.19 val PER: 0.2591
2026-01-08 12:23:13,328: t15.2024.07.21 val PER: 0.1228
2026-01-08 12:23:13,328: t15.2024.07.28 val PER: 0.1772
2026-01-08 12:23:13,329: t15.2025.01.10 val PER: 0.3540
2026-01-08 12:23:13,329: t15.2025.01.12 val PER: 0.2325
2026-01-08 12:23:13,329: t15.2025.03.14 val PER: 0.3565
2026-01-08 12:23:13,329: t15.2025.03.16 val PER: 0.2461
2026-01-08 12:23:13,329: t15.2025.03.30 val PER: 0.3322
2026-01-08 12:23:13,329: t15.2025.04.13 val PER: 0.2668
2026-01-08 12:23:22,624: Train batch 18600: loss: 17.30 grad norm: 70.99 time: 0.071
2026-01-08 12:23:40,366: Train batch 18800: loss: 17.16 grad norm: 67.67 time: 0.066
2026-01-08 12:23:58,016: Train batch 19000: loss: 13.64 grad norm: 57.88 time: 0.069
2026-01-08 12:23:58,016: Running test after training batch: 19000
2026-01-08 12:23:58,113: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:24:02,873: WER debug example
  GT : you can see the code at this point as well
  PR : nuclei candies cetus the could gatz this appointees has
2026-01-08 12:24:02,904: WER debug example
  GT : how does it keep the cost down
  PR : dietz dusty hitty skipper thus costs
2026-01-08 12:24:04,357: Val batch 19000: PER (avg): 0.2039 CTC Loss (avg): 22.1654 WER(1gram): 95.69% (n=64) time: 6.341
2026-01-08 12:24:04,357: WER lens: avg_true_words=6.16 avg_pred_words=5.77 max_pred_words=11
2026-01-08 12:24:04,357: t15.2023.08.13 val PER: 0.1809
2026-01-08 12:24:04,358: t15.2023.08.18 val PER: 0.1811
2026-01-08 12:24:04,358: t15.2023.08.20 val PER: 0.1620
2026-01-08 12:24:04,358: t15.2023.08.25 val PER: 0.1777
2026-01-08 12:24:04,358: t15.2023.08.27 val PER: 0.2460
2026-01-08 12:24:04,358: t15.2023.09.01 val PER: 0.1420
2026-01-08 12:24:04,358: t15.2023.09.03 val PER: 0.2221
2026-01-08 12:24:04,358: t15.2023.09.24 val PER: 0.1772
2026-01-08 12:24:04,358: t15.2023.09.29 val PER: 0.1863
2026-01-08 12:24:04,358: t15.2023.10.01 val PER: 0.2147
2026-01-08 12:24:04,358: t15.2023.10.06 val PER: 0.1593
2026-01-08 12:24:04,358: t15.2023.10.08 val PER: 0.3085
2026-01-08 12:24:04,358: t15.2023.10.13 val PER: 0.2801
2026-01-08 12:24:04,359: t15.2023.10.15 val PER: 0.1978
2026-01-08 12:24:04,359: t15.2023.10.20 val PER: 0.2181
2026-01-08 12:24:04,359: t15.2023.10.22 val PER: 0.1592
2026-01-08 12:24:04,359: t15.2023.11.03 val PER: 0.2408
2026-01-08 12:24:04,359: t15.2023.11.04 val PER: 0.0785
2026-01-08 12:24:04,359: t15.2023.11.17 val PER: 0.0964
2026-01-08 12:24:04,359: t15.2023.11.19 val PER: 0.1078
2026-01-08 12:24:04,359: t15.2023.11.26 val PER: 0.1993
2026-01-08 12:24:04,359: t15.2023.12.03 val PER: 0.1649
2026-01-08 12:24:04,359: t15.2023.12.08 val PER: 0.1678
2026-01-08 12:24:04,359: t15.2023.12.10 val PER: 0.1472
2026-01-08 12:24:04,359: t15.2023.12.17 val PER: 0.1590
2026-01-08 12:24:04,359: t15.2023.12.29 val PER: 0.1922
2026-01-08 12:24:04,359: t15.2024.02.25 val PER: 0.1545
2026-01-08 12:24:04,359: t15.2024.03.08 val PER: 0.2589
2026-01-08 12:24:04,360: t15.2024.03.15 val PER: 0.2308
2026-01-08 12:24:04,360: t15.2024.03.17 val PER: 0.1974
2026-01-08 12:24:04,360: t15.2024.05.10 val PER: 0.1961
2026-01-08 12:24:04,360: t15.2024.06.14 val PER: 0.2050
2026-01-08 12:24:04,360: t15.2024.07.19 val PER: 0.2630
2026-01-08 12:24:04,360: t15.2024.07.21 val PER: 0.1248
2026-01-08 12:24:04,360: t15.2024.07.28 val PER: 0.1787
2026-01-08 12:24:04,360: t15.2025.01.10 val PER: 0.3402
2026-01-08 12:24:04,360: t15.2025.01.12 val PER: 0.2263
2026-01-08 12:24:04,360: t15.2025.03.14 val PER: 0.3639
2026-01-08 12:24:04,360: t15.2025.03.16 val PER: 0.2487
2026-01-08 12:24:04,360: t15.2025.03.30 val PER: 0.3310
2026-01-08 12:24:04,360: t15.2025.04.13 val PER: 0.2668
2026-01-08 12:24:21,606: Train batch 19200: loss: 11.93 grad norm: 55.45 time: 0.065
2026-01-08 12:24:39,238: Train batch 19400: loss: 12.24 grad norm: 51.71 time: 0.054
2026-01-08 12:24:48,023: Running test after training batch: 19500
2026-01-08 12:24:48,119: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:24:53,334: WER debug example
  GT : you can see the code at this point as well
  PR : nuclei candies cetus the could gatt cyst appointees has
2026-01-08 12:24:53,361: WER debug example
  GT : how does it keep the cost down
  PR : dietz dusty hitz kieper thus costs
2026-01-08 12:24:54,824: Val batch 19500: PER (avg): 0.2046 CTC Loss (avg): 22.0983 WER(1gram): 96.45% (n=64) time: 6.801
2026-01-08 12:24:54,825: WER lens: avg_true_words=6.16 avg_pred_words=5.73 max_pred_words=11
2026-01-08 12:24:54,825: t15.2023.08.13 val PER: 0.1913
2026-01-08 12:24:54,825: t15.2023.08.18 val PER: 0.1760
2026-01-08 12:24:54,825: t15.2023.08.20 val PER: 0.1668
2026-01-08 12:24:54,825: t15.2023.08.25 val PER: 0.1807
2026-01-08 12:24:54,825: t15.2023.08.27 val PER: 0.2508
2026-01-08 12:24:54,825: t15.2023.09.01 val PER: 0.1453
2026-01-08 12:24:54,825: t15.2023.09.03 val PER: 0.2185
2026-01-08 12:24:54,825: t15.2023.09.24 val PER: 0.1808
2026-01-08 12:24:54,825: t15.2023.09.29 val PER: 0.1851
2026-01-08 12:24:54,825: t15.2023.10.01 val PER: 0.2166
2026-01-08 12:24:54,826: t15.2023.10.06 val PER: 0.1593
2026-01-08 12:24:54,826: t15.2023.10.08 val PER: 0.3099
2026-01-08 12:24:54,826: t15.2023.10.13 val PER: 0.2824
2026-01-08 12:24:54,826: t15.2023.10.15 val PER: 0.1964
2026-01-08 12:24:54,826: t15.2023.10.20 val PER: 0.2315
2026-01-08 12:24:54,826: t15.2023.10.22 val PER: 0.1648
2026-01-08 12:24:54,826: t15.2023.11.03 val PER: 0.2429
2026-01-08 12:24:54,826: t15.2023.11.04 val PER: 0.0785
2026-01-08 12:24:54,826: t15.2023.11.17 val PER: 0.0980
2026-01-08 12:24:54,826: t15.2023.11.19 val PER: 0.1038
2026-01-08 12:24:54,826: t15.2023.11.26 val PER: 0.1964
2026-01-08 12:24:54,826: t15.2023.12.03 val PER: 0.1576
2026-01-08 12:24:54,826: t15.2023.12.08 val PER: 0.1671
2026-01-08 12:24:54,826: t15.2023.12.10 val PER: 0.1445
2026-01-08 12:24:54,827: t15.2023.12.17 val PER: 0.1726
2026-01-08 12:24:54,827: t15.2023.12.29 val PER: 0.1942
2026-01-08 12:24:54,827: t15.2024.02.25 val PER: 0.1587
2026-01-08 12:24:54,827: t15.2024.03.08 val PER: 0.2617
2026-01-08 12:24:54,827: t15.2024.03.15 val PER: 0.2308
2026-01-08 12:24:54,827: t15.2024.03.17 val PER: 0.1925
2026-01-08 12:24:54,827: t15.2024.05.10 val PER: 0.1991
2026-01-08 12:24:54,827: t15.2024.06.14 val PER: 0.1987
2026-01-08 12:24:54,827: t15.2024.07.19 val PER: 0.2604
2026-01-08 12:24:54,827: t15.2024.07.21 val PER: 0.1297
2026-01-08 12:24:54,827: t15.2024.07.28 val PER: 0.1801
2026-01-08 12:24:54,827: t15.2025.01.10 val PER: 0.3416
2026-01-08 12:24:54,827: t15.2025.01.12 val PER: 0.2263
2026-01-08 12:24:54,827: t15.2025.03.14 val PER: 0.3595
2026-01-08 12:24:54,827: t15.2025.03.16 val PER: 0.2421
2026-01-08 12:24:54,827: t15.2025.03.30 val PER: 0.3310
2026-01-08 12:24:54,827: t15.2025.04.13 val PER: 0.2668
2026-01-08 12:25:03,315: Train batch 19600: loss: 15.70 grad norm: 54.40 time: 0.058
2026-01-08 12:25:20,804: Train batch 19800: loss: 12.10 grad norm: 56.50 time: 0.056
2026-01-08 12:25:38,499: Running test after training batch: 19999
2026-01-08 12:25:38,597: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:25:43,448: WER debug example
  GT : you can see the code at this point as well
  PR : nuclei candies seats the could gatt cyst appointees has
2026-01-08 12:25:43,475: WER debug example
  GT : how does it keep the cost down
  PR : dietz dusty hitty skipper thus cost
2026-01-08 12:25:44,974: Val batch 19999: PER (avg): 0.2027 CTC Loss (avg): 21.8747 WER(1gram): 95.69% (n=64) time: 6.475
2026-01-08 12:25:44,974: WER lens: avg_true_words=6.16 avg_pred_words=5.73 max_pred_words=11
2026-01-08 12:25:44,974: t15.2023.08.13 val PER: 0.1830
2026-01-08 12:25:44,974: t15.2023.08.18 val PER: 0.1827
2026-01-08 12:25:44,974: t15.2023.08.20 val PER: 0.1612
2026-01-08 12:25:44,975: t15.2023.08.25 val PER: 0.1702
2026-01-08 12:25:44,975: t15.2023.08.27 val PER: 0.2476
2026-01-08 12:25:44,975: t15.2023.09.01 val PER: 0.1453
2026-01-08 12:25:44,975: t15.2023.09.03 val PER: 0.2114
2026-01-08 12:25:44,975: t15.2023.09.24 val PER: 0.1760
2026-01-08 12:25:44,975: t15.2023.09.29 val PER: 0.1832
2026-01-08 12:25:44,975: t15.2023.10.01 val PER: 0.2120
2026-01-08 12:25:44,975: t15.2023.10.06 val PER: 0.1636
2026-01-08 12:25:44,975: t15.2023.10.08 val PER: 0.3153
2026-01-08 12:25:44,975: t15.2023.10.13 val PER: 0.2801
2026-01-08 12:25:44,975: t15.2023.10.15 val PER: 0.1984
2026-01-08 12:25:44,975: t15.2023.10.20 val PER: 0.2215
2026-01-08 12:25:44,975: t15.2023.10.22 val PER: 0.1581
2026-01-08 12:25:44,975: t15.2023.11.03 val PER: 0.2415
2026-01-08 12:25:44,975: t15.2023.11.04 val PER: 0.0717
2026-01-08 12:25:44,975: t15.2023.11.17 val PER: 0.0949
2026-01-08 12:25:44,976: t15.2023.11.19 val PER: 0.0998
2026-01-08 12:25:44,976: t15.2023.11.26 val PER: 0.2000
2026-01-08 12:25:44,976: t15.2023.12.03 val PER: 0.1576
2026-01-08 12:25:44,976: t15.2023.12.08 val PER: 0.1671
2026-01-08 12:25:44,976: t15.2023.12.10 val PER: 0.1432
2026-01-08 12:25:44,976: t15.2023.12.17 val PER: 0.1694
2026-01-08 12:25:44,976: t15.2023.12.29 val PER: 0.1860
2026-01-08 12:25:44,976: t15.2024.02.25 val PER: 0.1601
2026-01-08 12:25:44,976: t15.2024.03.08 val PER: 0.2646
2026-01-08 12:25:44,976: t15.2024.03.15 val PER: 0.2251
2026-01-08 12:25:44,976: t15.2024.03.17 val PER: 0.1925
2026-01-08 12:25:44,976: t15.2024.05.10 val PER: 0.1947
2026-01-08 12:25:44,976: t15.2024.06.14 val PER: 0.2035
2026-01-08 12:25:44,976: t15.2024.07.19 val PER: 0.2624
2026-01-08 12:25:44,976: t15.2024.07.21 val PER: 0.1234
2026-01-08 12:25:44,976: t15.2024.07.28 val PER: 0.1787
2026-01-08 12:25:44,977: t15.2025.01.10 val PER: 0.3430
2026-01-08 12:25:44,977: t15.2025.01.12 val PER: 0.2263
2026-01-08 12:25:44,977: t15.2025.03.14 val PER: 0.3595
2026-01-08 12:25:44,977: t15.2025.03.16 val PER: 0.2395
2026-01-08 12:25:44,977: t15.2025.03.30 val PER: 0.3276
2026-01-08 12:25:44,977: t15.2025.04.13 val PER: 0.2639
2026-01-08 12:25:45,002: Best avg val PER achieved: 0.20680
2026-01-08 12:25:45,002: Total training time: 33.71 minutes

=== RUN head_2blocks_ln.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_2blocks_ln
