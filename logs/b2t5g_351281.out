TMPDIR=/home/e12511253/tmp
JOB_TMP=/home/e12511253/tmp/e12511253_b2t_351281
TORCH_EXTENSIONS_DIR=/home/e12511253/tmp/e12511253_b2t_351281/torch_extensions
WANDB_DIR=/home/e12511253/tmp/e12511253_b2t_351281/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/home/e12511253/tmp/e12511253_b2t_351281/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  8 18:56 /home/e12511253/tmp/e12511253_b2t_351281/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
OUT_ROOT=/home/e12511253/Brain2Text/brain2text/trained_models
==============================================
Job: b2t5g  ID: 351281
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_5gram_only.yaml
Folders: configs/experiments/gru/papers/ablations
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/gru/papers/ablations ==========
Num configs: 2

=== RUN speckle_03.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03
2026-01-08 18:56:42,900: Using device: cuda:0
2026-01-08 19:00:34,057: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-08 19:00:34,086: Using 45 sessions after filtering (from 45).
2026-01-08 19:00:34,539: Using torch.compile (if available)
2026-01-08 19:00:34,539: torch.compile not available (torch<2.0). Skipping.
2026-01-08 19:00:34,539: Initialized RNN decoding model
2026-01-08 19:00:34,539: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-08 19:00:34,539: Model has 44,907,305 parameters
2026-01-08 19:00:34,539: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-08 19:00:35,810: Successfully initialized datasets
2026-01-08 19:00:35,811: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-08 19:00:38,343: Train batch 0: loss: 572.52 grad norm: 1495.05 time: 0.192
2026-01-08 19:00:38,343: Running test after training batch: 0
2026-01-08 19:00:38,454: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:00:44,324: WER debug example
  GT : you can see the code at this point as well
  PR : she has from his
2026-01-08 19:00:45,345: WER debug example
  GT : how does it keep the cost down
  PR : money from
2026-01-08 19:04:39,230: Val batch 0: PER (avg): 1.4296 CTC Loss (avg): 633.1590 WER(5gram): 99.67% (n=256) time: 240.887
2026-01-08 19:04:39,234: WER lens: avg_true_words=5.99 avg_pred_words=2.81 max_pred_words=7
2026-01-08 19:04:39,239: t15.2023.08.13 val PER: 1.3025
2026-01-08 19:04:39,242: t15.2023.08.18 val PER: 1.4258
2026-01-08 19:04:39,243: t15.2023.08.20 val PER: 1.3066
2026-01-08 19:04:39,243: t15.2023.08.25 val PER: 1.3328
2026-01-08 19:04:39,243: t15.2023.08.27 val PER: 1.2428
2026-01-08 19:04:39,243: t15.2023.09.01 val PER: 1.4529
2026-01-08 19:04:39,243: t15.2023.09.03 val PER: 1.3171
2026-01-08 19:04:39,243: t15.2023.09.24 val PER: 1.5449
2026-01-08 19:04:39,244: t15.2023.09.29 val PER: 1.4678
2026-01-08 19:04:39,244: t15.2023.10.01 val PER: 1.2140
2026-01-08 19:04:39,244: t15.2023.10.06 val PER: 1.4909
2026-01-08 19:04:39,244: t15.2023.10.08 val PER: 1.1867
2026-01-08 19:04:39,244: t15.2023.10.13 val PER: 1.3964
2026-01-08 19:04:39,244: t15.2023.10.15 val PER: 1.3902
2026-01-08 19:04:39,244: t15.2023.10.20 val PER: 1.5067
2026-01-08 19:04:39,244: t15.2023.10.22 val PER: 1.3953
2026-01-08 19:04:39,244: t15.2023.11.03 val PER: 1.5943
2026-01-08 19:04:39,244: t15.2023.11.04 val PER: 2.0410
2026-01-08 19:04:39,244: t15.2023.11.17 val PER: 1.9456
2026-01-08 19:04:39,244: t15.2023.11.19 val PER: 1.6766
2026-01-08 19:04:39,244: t15.2023.11.26 val PER: 1.5384
2026-01-08 19:04:39,244: t15.2023.12.03 val PER: 1.4254
2026-01-08 19:04:39,244: t15.2023.12.08 val PER: 1.4521
2026-01-08 19:04:39,244: t15.2023.12.10 val PER: 1.6978
2026-01-08 19:04:39,245: t15.2023.12.17 val PER: 1.3035
2026-01-08 19:04:39,245: t15.2023.12.29 val PER: 1.4070
2026-01-08 19:04:39,245: t15.2024.02.25 val PER: 1.4242
2026-01-08 19:04:39,245: t15.2024.03.08 val PER: 1.3243
2026-01-08 19:04:39,245: t15.2024.03.15 val PER: 1.3183
2026-01-08 19:04:39,245: t15.2024.03.17 val PER: 1.4031
2026-01-08 19:04:39,246: t15.2024.05.10 val PER: 1.3165
2026-01-08 19:04:39,246: t15.2024.06.14 val PER: 1.5284
2026-01-08 19:04:39,246: t15.2024.07.19 val PER: 1.0824
2026-01-08 19:04:39,246: t15.2024.07.21 val PER: 1.6290
2026-01-08 19:04:39,246: t15.2024.07.28 val PER: 1.6581
2026-01-08 19:04:39,246: t15.2025.01.10 val PER: 1.0895
2026-01-08 19:04:39,246: t15.2025.01.12 val PER: 1.7606
2026-01-08 19:04:39,246: t15.2025.03.14 val PER: 1.0370
2026-01-08 19:04:39,246: t15.2025.03.16 val PER: 1.6204
2026-01-08 19:04:39,246: t15.2025.03.30 val PER: 1.2908
2026-01-08 19:04:39,246: t15.2025.04.13 val PER: 1.5934
2026-01-08 19:04:39,247: New best val WER(5gram) inf% --> 99.67%
2026-01-08 19:04:39,433: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_0
2026-01-08 19:04:57,760: Train batch 200: loss: 82.28 grad norm: 85.60 time: 0.057
2026-01-08 19:05:15,550: Train batch 400: loss: 59.03 grad norm: 100.28 time: 0.067
2026-01-08 19:05:24,591: Running test after training batch: 500
2026-01-08 19:05:24,722: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:05:29,841: WER debug example
  GT : you can see the code at this point as well
  PR : you'll need these days and at this guide is all
2026-01-08 19:05:30,001: WER debug example
  GT : how does it keep the cost down
  PR : and does it think this is as
2026-01-08 19:06:24,487: Val batch 500: PER (avg): 0.5487 CTC Loss (avg): 60.5110 WER(5gram): 82.72% (n=256) time: 59.895
2026-01-08 19:06:24,487: WER lens: avg_true_words=5.99 avg_pred_words=5.80 max_pred_words=12
2026-01-08 19:06:24,487: t15.2023.08.13 val PER: 0.4834
2026-01-08 19:06:24,487: t15.2023.08.18 val PER: 0.4920
2026-01-08 19:06:24,488: t15.2023.08.20 val PER: 0.4909
2026-01-08 19:06:24,488: t15.2023.08.25 val PER: 0.4563
2026-01-08 19:06:24,488: t15.2023.08.27 val PER: 0.5354
2026-01-08 19:06:24,488: t15.2023.09.01 val PER: 0.4570
2026-01-08 19:06:24,488: t15.2023.09.03 val PER: 0.5143
2026-01-08 19:06:24,488: t15.2023.09.24 val PER: 0.4660
2026-01-08 19:06:24,488: t15.2023.09.29 val PER: 0.4888
2026-01-08 19:06:24,488: t15.2023.10.01 val PER: 0.5429
2026-01-08 19:06:24,488: t15.2023.10.06 val PER: 0.4639
2026-01-08 19:06:24,488: t15.2023.10.08 val PER: 0.5670
2026-01-08 19:06:24,488: t15.2023.10.13 val PER: 0.5725
2026-01-08 19:06:24,488: t15.2023.10.15 val PER: 0.5300
2026-01-08 19:06:24,489: t15.2023.10.20 val PER: 0.5000
2026-01-08 19:06:24,489: t15.2023.10.22 val PER: 0.4855
2026-01-08 19:06:24,489: t15.2023.11.03 val PER: 0.5461
2026-01-08 19:06:24,489: t15.2023.11.04 val PER: 0.3345
2026-01-08 19:06:24,489: t15.2023.11.17 val PER: 0.4059
2026-01-08 19:06:24,489: t15.2023.11.19 val PER: 0.3892
2026-01-08 19:06:24,489: t15.2023.11.26 val PER: 0.5746
2026-01-08 19:06:24,489: t15.2023.12.03 val PER: 0.5263
2026-01-08 19:06:24,489: t15.2023.12.08 val PER: 0.5413
2026-01-08 19:06:24,489: t15.2023.12.10 val PER: 0.5020
2026-01-08 19:06:24,490: t15.2023.12.17 val PER: 0.6029
2026-01-08 19:06:24,490: t15.2023.12.29 val PER: 0.5951
2026-01-08 19:06:24,490: t15.2024.02.25 val PER: 0.5014
2026-01-08 19:06:24,490: t15.2024.03.08 val PER: 0.6529
2026-01-08 19:06:24,490: t15.2024.03.15 val PER: 0.5985
2026-01-08 19:06:24,490: t15.2024.03.17 val PER: 0.5342
2026-01-08 19:06:24,490: t15.2024.05.10 val PER: 0.5765
2026-01-08 19:06:24,490: t15.2024.06.14 val PER: 0.5394
2026-01-08 19:06:24,490: t15.2024.07.19 val PER: 0.7034
2026-01-08 19:06:24,490: t15.2024.07.21 val PER: 0.5172
2026-01-08 19:06:24,491: t15.2024.07.28 val PER: 0.5529
2026-01-08 19:06:24,491: t15.2025.01.10 val PER: 0.7424
2026-01-08 19:06:24,491: t15.2025.01.12 val PER: 0.5889
2026-01-08 19:06:24,491: t15.2025.03.14 val PER: 0.7278
2026-01-08 19:06:24,491: t15.2025.03.16 val PER: 0.6165
2026-01-08 19:06:24,491: t15.2025.03.30 val PER: 0.7402
2026-01-08 19:06:24,491: t15.2025.04.13 val PER: 0.5934
2026-01-08 19:06:24,492: New best val WER(5gram) 99.67% --> 82.72%
2026-01-08 19:06:24,692: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_500
2026-01-08 19:06:33,148: Train batch 600: loss: 51.89 grad norm: 79.04 time: 0.080
2026-01-08 19:06:49,959: Train batch 800: loss: 44.44 grad norm: 87.27 time: 0.058
2026-01-08 19:07:06,924: Train batch 1000: loss: 46.39 grad norm: 84.65 time: 0.066
2026-01-08 19:07:06,925: Running test after training batch: 1000
2026-01-08 19:07:07,047: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:07:12,182: WER debug example
  GT : you can see the code at this point as well
  PR : you'd end she's that good at it and is well
2026-01-08 19:07:12,334: WER debug example
  GT : how does it keep the cost down
  PR : and as it is that it had
2026-01-08 19:07:44,319: Val batch 1000: PER (avg): 0.4275 CTC Loss (avg): 45.0460 WER(5gram): 57.63% (n=256) time: 37.394
2026-01-08 19:07:44,319: WER lens: avg_true_words=5.99 avg_pred_words=5.64 max_pred_words=12
2026-01-08 19:07:44,320: t15.2023.08.13 val PER: 0.3846
2026-01-08 19:07:44,320: t15.2023.08.18 val PER: 0.3638
2026-01-08 19:07:44,320: t15.2023.08.20 val PER: 0.3670
2026-01-08 19:07:44,320: t15.2023.08.25 val PER: 0.3072
2026-01-08 19:07:44,320: t15.2023.08.27 val PER: 0.4309
2026-01-08 19:07:44,320: t15.2023.09.01 val PER: 0.3239
2026-01-08 19:07:44,320: t15.2023.09.03 val PER: 0.4169
2026-01-08 19:07:44,320: t15.2023.09.24 val PER: 0.3422
2026-01-08 19:07:44,321: t15.2023.09.29 val PER: 0.3912
2026-01-08 19:07:44,321: t15.2023.10.01 val PER: 0.4260
2026-01-08 19:07:44,321: t15.2023.10.06 val PER: 0.3402
2026-01-08 19:07:44,321: t15.2023.10.08 val PER: 0.4668
2026-01-08 19:07:44,321: t15.2023.10.13 val PER: 0.4787
2026-01-08 19:07:44,321: t15.2023.10.15 val PER: 0.4094
2026-01-08 19:07:44,321: t15.2023.10.20 val PER: 0.3960
2026-01-08 19:07:44,321: t15.2023.10.22 val PER: 0.3808
2026-01-08 19:07:44,321: t15.2023.11.03 val PER: 0.4254
2026-01-08 19:07:44,321: t15.2023.11.04 val PER: 0.1809
2026-01-08 19:07:44,321: t15.2023.11.17 val PER: 0.2706
2026-01-08 19:07:44,322: t15.2023.11.19 val PER: 0.2355
2026-01-08 19:07:44,322: t15.2023.11.26 val PER: 0.4696
2026-01-08 19:07:44,322: t15.2023.12.03 val PER: 0.4275
2026-01-08 19:07:44,322: t15.2023.12.08 val PER: 0.4221
2026-01-08 19:07:44,322: t15.2023.12.10 val PER: 0.3811
2026-01-08 19:07:44,322: t15.2023.12.17 val PER: 0.4210
2026-01-08 19:07:44,322: t15.2023.12.29 val PER: 0.4200
2026-01-08 19:07:44,322: t15.2024.02.25 val PER: 0.3806
2026-01-08 19:07:44,322: t15.2024.03.08 val PER: 0.5306
2026-01-08 19:07:44,322: t15.2024.03.15 val PER: 0.4728
2026-01-08 19:07:44,322: t15.2024.03.17 val PER: 0.4184
2026-01-08 19:07:44,323: t15.2024.05.10 val PER: 0.4413
2026-01-08 19:07:44,323: t15.2024.06.14 val PER: 0.4211
2026-01-08 19:07:44,323: t15.2024.07.19 val PER: 0.5583
2026-01-08 19:07:44,323: t15.2024.07.21 val PER: 0.3966
2026-01-08 19:07:44,323: t15.2024.07.28 val PER: 0.4390
2026-01-08 19:07:44,323: t15.2025.01.10 val PER: 0.6129
2026-01-08 19:07:44,323: t15.2025.01.12 val PER: 0.4580
2026-01-08 19:07:44,323: t15.2025.03.14 val PER: 0.6346
2026-01-08 19:07:44,323: t15.2025.03.16 val PER: 0.4830
2026-01-08 19:07:44,323: t15.2025.03.30 val PER: 0.6552
2026-01-08 19:07:44,323: t15.2025.04.13 val PER: 0.4993
2026-01-08 19:07:44,324: New best val WER(5gram) 82.72% --> 57.63%
2026-01-08 19:07:44,514: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_1000
2026-01-08 19:08:02,202: Train batch 1200: loss: 36.19 grad norm: 80.24 time: 0.070
2026-01-08 19:08:19,984: Train batch 1400: loss: 37.81 grad norm: 75.62 time: 0.061
2026-01-08 19:08:28,721: Running test after training batch: 1500
2026-01-08 19:08:28,876: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:08:33,938: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-08 19:08:34,049: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us and
2026-01-08 19:08:53,622: Val batch 1500: PER (avg): 0.3963 CTC Loss (avg): 39.3657 WER(5gram): 35.79% (n=256) time: 24.900
2026-01-08 19:08:53,623: WER lens: avg_true_words=5.99 avg_pred_words=5.33 max_pred_words=12
2026-01-08 19:08:53,623: t15.2023.08.13 val PER: 0.3514
2026-01-08 19:08:53,623: t15.2023.08.18 val PER: 0.3378
2026-01-08 19:08:53,623: t15.2023.08.20 val PER: 0.3145
2026-01-08 19:08:53,623: t15.2023.08.25 val PER: 0.2696
2026-01-08 19:08:53,623: t15.2023.08.27 val PER: 0.3955
2026-01-08 19:08:53,623: t15.2023.09.01 val PER: 0.3060
2026-01-08 19:08:53,623: t15.2023.09.03 val PER: 0.3800
2026-01-08 19:08:53,623: t15.2023.09.24 val PER: 0.3216
2026-01-08 19:08:53,624: t15.2023.09.29 val PER: 0.3625
2026-01-08 19:08:53,624: t15.2023.10.01 val PER: 0.4042
2026-01-08 19:08:53,624: t15.2023.10.06 val PER: 0.3068
2026-01-08 19:08:53,624: t15.2023.10.08 val PER: 0.4533
2026-01-08 19:08:53,624: t15.2023.10.13 val PER: 0.4469
2026-01-08 19:08:53,624: t15.2023.10.15 val PER: 0.3724
2026-01-08 19:08:53,624: t15.2023.10.20 val PER: 0.3490
2026-01-08 19:08:53,624: t15.2023.10.22 val PER: 0.3263
2026-01-08 19:08:53,624: t15.2023.11.03 val PER: 0.3867
2026-01-08 19:08:53,624: t15.2023.11.04 val PER: 0.1331
2026-01-08 19:08:53,624: t15.2023.11.17 val PER: 0.2379
2026-01-08 19:08:53,624: t15.2023.11.19 val PER: 0.1896
2026-01-08 19:08:53,625: t15.2023.11.26 val PER: 0.4290
2026-01-08 19:08:53,625: t15.2023.12.03 val PER: 0.3950
2026-01-08 19:08:53,625: t15.2023.12.08 val PER: 0.3862
2026-01-08 19:08:53,625: t15.2023.12.10 val PER: 0.3167
2026-01-08 19:08:53,625: t15.2023.12.17 val PER: 0.3950
2026-01-08 19:08:53,625: t15.2023.12.29 val PER: 0.3871
2026-01-08 19:08:53,625: t15.2024.02.25 val PER: 0.3076
2026-01-08 19:08:53,625: t15.2024.03.08 val PER: 0.4708
2026-01-08 19:08:53,625: t15.2024.03.15 val PER: 0.4278
2026-01-08 19:08:53,625: t15.2024.03.17 val PER: 0.3996
2026-01-08 19:08:53,625: t15.2024.05.10 val PER: 0.4086
2026-01-08 19:08:53,625: t15.2024.06.14 val PER: 0.3959
2026-01-08 19:08:53,625: t15.2024.07.19 val PER: 0.5419
2026-01-08 19:08:53,625: t15.2024.07.21 val PER: 0.3621
2026-01-08 19:08:53,626: t15.2024.07.28 val PER: 0.3897
2026-01-08 19:08:53,626: t15.2025.01.10 val PER: 0.6391
2026-01-08 19:08:53,626: t15.2025.01.12 val PER: 0.4465
2026-01-08 19:08:53,626: t15.2025.03.14 val PER: 0.6139
2026-01-08 19:08:53,626: t15.2025.03.16 val PER: 0.4843
2026-01-08 19:08:53,626: t15.2025.03.30 val PER: 0.6552
2026-01-08 19:08:53,626: t15.2025.04.13 val PER: 0.4879
2026-01-08 19:08:53,627: New best val WER(5gram) 57.63% --> 35.79%
2026-01-08 19:08:53,816: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_1500
2026-01-08 19:09:02,262: Train batch 1600: loss: 39.51 grad norm: 81.12 time: 0.065
2026-01-08 19:09:19,560: Train batch 1800: loss: 36.32 grad norm: 71.33 time: 0.091
2026-01-08 19:09:37,099: Train batch 2000: loss: 36.67 grad norm: 70.25 time: 0.067
2026-01-08 19:09:37,099: Running test after training batch: 2000
2026-01-08 19:09:37,226: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:09:42,265: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 19:09:42,344: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us and
2026-01-08 19:10:00,653: Val batch 2000: PER (avg): 0.3411 CTC Loss (avg): 34.4556 WER(5gram): 30.96% (n=256) time: 23.554
2026-01-08 19:10:00,654: WER lens: avg_true_words=5.99 avg_pred_words=5.75 max_pred_words=12
2026-01-08 19:10:00,654: t15.2023.08.13 val PER: 0.3160
2026-01-08 19:10:00,654: t15.2023.08.18 val PER: 0.2758
2026-01-08 19:10:00,654: t15.2023.08.20 val PER: 0.2708
2026-01-08 19:10:00,654: t15.2023.08.25 val PER: 0.2455
2026-01-08 19:10:00,654: t15.2023.08.27 val PER: 0.3505
2026-01-08 19:10:00,654: t15.2023.09.01 val PER: 0.2541
2026-01-08 19:10:00,655: t15.2023.09.03 val PER: 0.3314
2026-01-08 19:10:00,655: t15.2023.09.24 val PER: 0.2718
2026-01-08 19:10:00,655: t15.2023.09.29 val PER: 0.2833
2026-01-08 19:10:00,655: t15.2023.10.01 val PER: 0.3369
2026-01-08 19:10:00,655: t15.2023.10.06 val PER: 0.2530
2026-01-08 19:10:00,655: t15.2023.10.08 val PER: 0.4127
2026-01-08 19:10:00,655: t15.2023.10.13 val PER: 0.3887
2026-01-08 19:10:00,655: t15.2023.10.15 val PER: 0.3039
2026-01-08 19:10:00,655: t15.2023.10.20 val PER: 0.3154
2026-01-08 19:10:00,655: t15.2023.10.22 val PER: 0.2817
2026-01-08 19:10:00,655: t15.2023.11.03 val PER: 0.3358
2026-01-08 19:10:00,655: t15.2023.11.04 val PER: 0.1263
2026-01-08 19:10:00,655: t15.2023.11.17 val PER: 0.1944
2026-01-08 19:10:00,655: t15.2023.11.19 val PER: 0.1497
2026-01-08 19:10:00,655: t15.2023.11.26 val PER: 0.3833
2026-01-08 19:10:00,655: t15.2023.12.03 val PER: 0.3204
2026-01-08 19:10:00,656: t15.2023.12.08 val PER: 0.3282
2026-01-08 19:10:00,656: t15.2023.12.10 val PER: 0.2707
2026-01-08 19:10:00,656: t15.2023.12.17 val PER: 0.3399
2026-01-08 19:10:00,656: t15.2023.12.29 val PER: 0.3411
2026-01-08 19:10:00,656: t15.2024.02.25 val PER: 0.2837
2026-01-08 19:10:00,656: t15.2024.03.08 val PER: 0.4267
2026-01-08 19:10:00,656: t15.2024.03.15 val PER: 0.3671
2026-01-08 19:10:00,656: t15.2024.03.17 val PER: 0.3480
2026-01-08 19:10:00,656: t15.2024.05.10 val PER: 0.3596
2026-01-08 19:10:00,656: t15.2024.06.14 val PER: 0.3565
2026-01-08 19:10:00,656: t15.2024.07.19 val PER: 0.4865
2026-01-08 19:10:00,656: t15.2024.07.21 val PER: 0.3041
2026-01-08 19:10:00,656: t15.2024.07.28 val PER: 0.3316
2026-01-08 19:10:00,656: t15.2025.01.10 val PER: 0.5455
2026-01-08 19:10:00,657: t15.2025.01.12 val PER: 0.3911
2026-01-08 19:10:00,657: t15.2025.03.14 val PER: 0.5429
2026-01-08 19:10:00,657: t15.2025.03.16 val PER: 0.3992
2026-01-08 19:10:00,657: t15.2025.03.30 val PER: 0.5724
2026-01-08 19:10:00,657: t15.2025.04.13 val PER: 0.4223
2026-01-08 19:10:00,658: New best val WER(5gram) 35.79% --> 30.96%
2026-01-08 19:10:00,847: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_2000
2026-01-08 19:10:18,194: Train batch 2200: loss: 30.62 grad norm: 71.22 time: 0.063
2026-01-08 19:10:35,095: Train batch 2400: loss: 30.38 grad norm: 61.61 time: 0.055
2026-01-08 19:10:43,767: Running test after training batch: 2500
2026-01-08 19:10:43,874: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:10:48,936: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 19:10:49,006: WER debug example
  GT : how does it keep the cost down
  PR : how does it in the s it
2026-01-08 19:11:05,102: Val batch 2500: PER (avg): 0.3161 CTC Loss (avg): 31.5596 WER(5gram): 28.03% (n=256) time: 21.335
2026-01-08 19:11:05,103: WER lens: avg_true_words=5.99 avg_pred_words=5.82 max_pred_words=12
2026-01-08 19:11:05,103: t15.2023.08.13 val PER: 0.3108
2026-01-08 19:11:05,103: t15.2023.08.18 val PER: 0.2548
2026-01-08 19:11:05,103: t15.2023.08.20 val PER: 0.2423
2026-01-08 19:11:05,103: t15.2023.08.25 val PER: 0.2274
2026-01-08 19:11:05,103: t15.2023.08.27 val PER: 0.3408
2026-01-08 19:11:05,103: t15.2023.09.01 val PER: 0.2305
2026-01-08 19:11:05,103: t15.2023.09.03 val PER: 0.3052
2026-01-08 19:11:05,103: t15.2023.09.24 val PER: 0.2415
2026-01-08 19:11:05,103: t15.2023.09.29 val PER: 0.2687
2026-01-08 19:11:05,103: t15.2023.10.01 val PER: 0.3217
2026-01-08 19:11:05,103: t15.2023.10.06 val PER: 0.2142
2026-01-08 19:11:05,104: t15.2023.10.08 val PER: 0.3843
2026-01-08 19:11:05,104: t15.2023.10.13 val PER: 0.3778
2026-01-08 19:11:05,104: t15.2023.10.15 val PER: 0.2874
2026-01-08 19:11:05,104: t15.2023.10.20 val PER: 0.3087
2026-01-08 19:11:05,104: t15.2023.10.22 val PER: 0.2550
2026-01-08 19:11:05,104: t15.2023.11.03 val PER: 0.3114
2026-01-08 19:11:05,104: t15.2023.11.04 val PER: 0.0922
2026-01-08 19:11:05,104: t15.2023.11.17 val PER: 0.1524
2026-01-08 19:11:05,104: t15.2023.11.19 val PER: 0.1178
2026-01-08 19:11:05,104: t15.2023.11.26 val PER: 0.3609
2026-01-08 19:11:05,104: t15.2023.12.03 val PER: 0.3015
2026-01-08 19:11:05,104: t15.2023.12.08 val PER: 0.3003
2026-01-08 19:11:05,104: t15.2023.12.10 val PER: 0.2470
2026-01-08 19:11:05,104: t15.2023.12.17 val PER: 0.3004
2026-01-08 19:11:05,104: t15.2023.12.29 val PER: 0.3171
2026-01-08 19:11:05,104: t15.2024.02.25 val PER: 0.2570
2026-01-08 19:11:05,105: t15.2024.03.08 val PER: 0.3784
2026-01-08 19:11:05,105: t15.2024.03.15 val PER: 0.3477
2026-01-08 19:11:05,105: t15.2024.03.17 val PER: 0.3194
2026-01-08 19:11:05,105: t15.2024.05.10 val PER: 0.3254
2026-01-08 19:11:05,105: t15.2024.06.14 val PER: 0.3013
2026-01-08 19:11:05,105: t15.2024.07.19 val PER: 0.4535
2026-01-08 19:11:05,105: t15.2024.07.21 val PER: 0.2731
2026-01-08 19:11:05,105: t15.2024.07.28 val PER: 0.3059
2026-01-08 19:11:05,105: t15.2025.01.10 val PER: 0.5110
2026-01-08 19:11:05,105: t15.2025.01.12 val PER: 0.3772
2026-01-08 19:11:05,105: t15.2025.03.14 val PER: 0.5192
2026-01-08 19:11:05,106: t15.2025.03.16 val PER: 0.3704
2026-01-08 19:11:05,106: t15.2025.03.30 val PER: 0.5494
2026-01-08 19:11:05,106: t15.2025.04.13 val PER: 0.3937
2026-01-08 19:11:05,107: New best val WER(5gram) 30.96% --> 28.03%
2026-01-08 19:11:05,294: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_2500
2026-01-08 19:11:14,316: Train batch 2600: loss: 36.51 grad norm: 80.25 time: 0.057
2026-01-08 19:11:32,656: Train batch 2800: loss: 27.36 grad norm: 67.10 time: 0.084
2026-01-08 19:11:51,024: Train batch 3000: loss: 33.23 grad norm: 70.37 time: 0.088
2026-01-08 19:11:51,024: Running test after training batch: 3000
2026-01-08 19:11:51,129: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:11:56,372: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 19:11:56,437: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost at
2026-01-08 19:12:11,581: Val batch 3000: PER (avg): 0.2913 CTC Loss (avg): 29.0050 WER(5gram): 25.16% (n=256) time: 20.557
2026-01-08 19:12:11,582: WER lens: avg_true_words=5.99 avg_pred_words=5.97 max_pred_words=12
2026-01-08 19:12:11,582: t15.2023.08.13 val PER: 0.2620
2026-01-08 19:12:11,582: t15.2023.08.18 val PER: 0.2288
2026-01-08 19:12:11,582: t15.2023.08.20 val PER: 0.2160
2026-01-08 19:12:11,582: t15.2023.08.25 val PER: 0.2108
2026-01-08 19:12:11,583: t15.2023.08.27 val PER: 0.3119
2026-01-08 19:12:11,583: t15.2023.09.01 val PER: 0.2021
2026-01-08 19:12:11,583: t15.2023.09.03 val PER: 0.2922
2026-01-08 19:12:11,583: t15.2023.09.24 val PER: 0.2257
2026-01-08 19:12:11,583: t15.2023.09.29 val PER: 0.2451
2026-01-08 19:12:11,583: t15.2023.10.01 val PER: 0.2972
2026-01-08 19:12:11,583: t15.2023.10.06 val PER: 0.1959
2026-01-08 19:12:11,583: t15.2023.10.08 val PER: 0.3572
2026-01-08 19:12:11,583: t15.2023.10.13 val PER: 0.3452
2026-01-08 19:12:11,583: t15.2023.10.15 val PER: 0.2802
2026-01-08 19:12:11,583: t15.2023.10.20 val PER: 0.2819
2026-01-08 19:12:11,583: t15.2023.10.22 val PER: 0.2305
2026-01-08 19:12:11,583: t15.2023.11.03 val PER: 0.2924
2026-01-08 19:12:11,583: t15.2023.11.04 val PER: 0.0785
2026-01-08 19:12:11,583: t15.2023.11.17 val PER: 0.1337
2026-01-08 19:12:11,584: t15.2023.11.19 val PER: 0.1257
2026-01-08 19:12:11,584: t15.2023.11.26 val PER: 0.3138
2026-01-08 19:12:11,584: t15.2023.12.03 val PER: 0.2878
2026-01-08 19:12:11,584: t15.2023.12.08 val PER: 0.2750
2026-01-08 19:12:11,584: t15.2023.12.10 val PER: 0.2247
2026-01-08 19:12:11,584: t15.2023.12.17 val PER: 0.2723
2026-01-08 19:12:11,584: t15.2023.12.29 val PER: 0.2814
2026-01-08 19:12:11,584: t15.2024.02.25 val PER: 0.2556
2026-01-08 19:12:11,584: t15.2024.03.08 val PER: 0.3670
2026-01-08 19:12:11,584: t15.2024.03.15 val PER: 0.3365
2026-01-08 19:12:11,584: t15.2024.03.17 val PER: 0.3013
2026-01-08 19:12:11,584: t15.2024.05.10 val PER: 0.2868
2026-01-08 19:12:11,584: t15.2024.06.14 val PER: 0.2981
2026-01-08 19:12:11,584: t15.2024.07.19 val PER: 0.4087
2026-01-08 19:12:11,585: t15.2024.07.21 val PER: 0.2386
2026-01-08 19:12:11,585: t15.2024.07.28 val PER: 0.2721
2026-01-08 19:12:11,585: t15.2025.01.10 val PER: 0.4986
2026-01-08 19:12:11,585: t15.2025.01.12 val PER: 0.3457
2026-01-08 19:12:11,585: t15.2025.03.14 val PER: 0.4763
2026-01-08 19:12:11,585: t15.2025.03.16 val PER: 0.3469
2026-01-08 19:12:11,585: t15.2025.03.30 val PER: 0.5207
2026-01-08 19:12:11,585: t15.2025.04.13 val PER: 0.3680
2026-01-08 19:12:11,587: New best val WER(5gram) 28.03% --> 25.16%
2026-01-08 19:12:11,769: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_3000
2026-01-08 19:12:30,177: Train batch 3200: loss: 28.39 grad norm: 65.66 time: 0.076
2026-01-08 19:12:47,695: Train batch 3400: loss: 19.62 grad norm: 53.89 time: 0.051
2026-01-08 19:12:56,528: Running test after training batch: 3500
2026-01-08 19:12:56,667: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:13:01,730: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point it will
2026-01-08 19:13:01,793: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost at
2026-01-08 19:13:16,721: Val batch 3500: PER (avg): 0.2788 CTC Loss (avg): 27.8405 WER(5gram): 24.51% (n=256) time: 20.192
2026-01-08 19:13:16,722: WER lens: avg_true_words=5.99 avg_pred_words=5.98 max_pred_words=12
2026-01-08 19:13:16,722: t15.2023.08.13 val PER: 0.2443
2026-01-08 19:13:16,722: t15.2023.08.18 val PER: 0.2230
2026-01-08 19:13:16,722: t15.2023.08.20 val PER: 0.2256
2026-01-08 19:13:16,722: t15.2023.08.25 val PER: 0.1988
2026-01-08 19:13:16,722: t15.2023.08.27 val PER: 0.2878
2026-01-08 19:13:16,722: t15.2023.09.01 val PER: 0.1940
2026-01-08 19:13:16,722: t15.2023.09.03 val PER: 0.2720
2026-01-08 19:13:16,722: t15.2023.09.24 val PER: 0.2197
2026-01-08 19:13:16,722: t15.2023.09.29 val PER: 0.2265
2026-01-08 19:13:16,722: t15.2023.10.01 val PER: 0.2880
2026-01-08 19:13:16,723: t15.2023.10.06 val PER: 0.2002
2026-01-08 19:13:16,723: t15.2023.10.08 val PER: 0.3491
2026-01-08 19:13:16,723: t15.2023.10.13 val PER: 0.3367
2026-01-08 19:13:16,723: t15.2023.10.15 val PER: 0.2558
2026-01-08 19:13:16,723: t15.2023.10.20 val PER: 0.2752
2026-01-08 19:13:16,723: t15.2023.10.22 val PER: 0.2261
2026-01-08 19:13:16,723: t15.2023.11.03 val PER: 0.2788
2026-01-08 19:13:16,723: t15.2023.11.04 val PER: 0.0751
2026-01-08 19:13:16,723: t15.2023.11.17 val PER: 0.1275
2026-01-08 19:13:16,723: t15.2023.11.19 val PER: 0.1158
2026-01-08 19:13:16,723: t15.2023.11.26 val PER: 0.3000
2026-01-08 19:13:16,723: t15.2023.12.03 val PER: 0.2521
2026-01-08 19:13:16,723: t15.2023.12.08 val PER: 0.2623
2026-01-08 19:13:16,723: t15.2023.12.10 val PER: 0.2089
2026-01-08 19:13:16,724: t15.2023.12.17 val PER: 0.2630
2026-01-08 19:13:16,724: t15.2023.12.29 val PER: 0.2739
2026-01-08 19:13:16,724: t15.2024.02.25 val PER: 0.2205
2026-01-08 19:13:16,724: t15.2024.03.08 val PER: 0.3570
2026-01-08 19:13:16,724: t15.2024.03.15 val PER: 0.3183
2026-01-08 19:13:16,724: t15.2024.03.17 val PER: 0.2922
2026-01-08 19:13:16,724: t15.2024.05.10 val PER: 0.2719
2026-01-08 19:13:16,724: t15.2024.06.14 val PER: 0.2744
2026-01-08 19:13:16,724: t15.2024.07.19 val PER: 0.4008
2026-01-08 19:13:16,724: t15.2024.07.21 val PER: 0.2372
2026-01-08 19:13:16,724: t15.2024.07.28 val PER: 0.2904
2026-01-08 19:13:16,724: t15.2025.01.10 val PER: 0.4614
2026-01-08 19:13:16,724: t15.2025.01.12 val PER: 0.3195
2026-01-08 19:13:16,724: t15.2025.03.14 val PER: 0.4556
2026-01-08 19:13:16,724: t15.2025.03.16 val PER: 0.3285
2026-01-08 19:13:16,724: t15.2025.03.30 val PER: 0.4874
2026-01-08 19:13:16,724: t15.2025.04.13 val PER: 0.3566
2026-01-08 19:13:16,726: New best val WER(5gram) 25.16% --> 24.51%
2026-01-08 19:13:16,923: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_3500
2026-01-08 19:13:25,494: Train batch 3600: loss: 23.99 grad norm: 63.68 time: 0.069
2026-01-08 19:13:42,935: Train batch 3800: loss: 26.64 grad norm: 74.58 time: 0.068
2026-01-08 19:14:01,184: Train batch 4000: loss: 20.46 grad norm: 52.92 time: 0.057
2026-01-08 19:14:01,185: Running test after training batch: 4000
2026-01-08 19:14:01,340: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:14:06,382: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 19:14:06,443: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost at
2026-01-08 19:14:20,056: Val batch 4000: PER (avg): 0.2592 CTC Loss (avg): 25.7917 WER(5gram): 24.51% (n=256) time: 18.871
2026-01-08 19:14:20,057: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-08 19:14:20,057: t15.2023.08.13 val PER: 0.2266
2026-01-08 19:14:20,057: t15.2023.08.18 val PER: 0.2054
2026-01-08 19:14:20,057: t15.2023.08.20 val PER: 0.2121
2026-01-08 19:14:20,057: t15.2023.08.25 val PER: 0.1702
2026-01-08 19:14:20,057: t15.2023.08.27 val PER: 0.2910
2026-01-08 19:14:20,058: t15.2023.09.01 val PER: 0.1769
2026-01-08 19:14:20,058: t15.2023.09.03 val PER: 0.2625
2026-01-08 19:14:20,058: t15.2023.09.24 val PER: 0.2039
2026-01-08 19:14:20,058: t15.2023.09.29 val PER: 0.2023
2026-01-08 19:14:20,058: t15.2023.10.01 val PER: 0.2642
2026-01-08 19:14:20,058: t15.2023.10.06 val PER: 0.1895
2026-01-08 19:14:20,058: t15.2023.10.08 val PER: 0.3424
2026-01-08 19:14:20,058: t15.2023.10.13 val PER: 0.3274
2026-01-08 19:14:20,058: t15.2023.10.15 val PER: 0.2406
2026-01-08 19:14:20,058: t15.2023.10.20 val PER: 0.2584
2026-01-08 19:14:20,058: t15.2023.10.22 val PER: 0.2116
2026-01-08 19:14:20,058: t15.2023.11.03 val PER: 0.2626
2026-01-08 19:14:20,058: t15.2023.11.04 val PER: 0.0683
2026-01-08 19:14:20,059: t15.2023.11.17 val PER: 0.1073
2026-01-08 19:14:20,059: t15.2023.11.19 val PER: 0.1118
2026-01-08 19:14:20,059: t15.2023.11.26 val PER: 0.2870
2026-01-08 19:14:20,059: t15.2023.12.03 val PER: 0.2500
2026-01-08 19:14:20,059: t15.2023.12.08 val PER: 0.2264
2026-01-08 19:14:20,059: t15.2023.12.10 val PER: 0.1932
2026-01-08 19:14:20,059: t15.2023.12.17 val PER: 0.2422
2026-01-08 19:14:20,059: t15.2023.12.29 val PER: 0.2546
2026-01-08 19:14:20,060: t15.2024.02.25 val PER: 0.2163
2026-01-08 19:14:20,060: t15.2024.03.08 val PER: 0.3272
2026-01-08 19:14:20,060: t15.2024.03.15 val PER: 0.3083
2026-01-08 19:14:20,060: t15.2024.03.17 val PER: 0.2650
2026-01-08 19:14:20,060: t15.2024.05.10 val PER: 0.2645
2026-01-08 19:14:20,060: t15.2024.06.14 val PER: 0.2618
2026-01-08 19:14:20,060: t15.2024.07.19 val PER: 0.3830
2026-01-08 19:14:20,060: t15.2024.07.21 val PER: 0.2041
2026-01-08 19:14:20,061: t15.2024.07.28 val PER: 0.2478
2026-01-08 19:14:20,061: t15.2025.01.10 val PER: 0.4270
2026-01-08 19:14:20,061: t15.2025.01.12 val PER: 0.2925
2026-01-08 19:14:20,061: t15.2025.03.14 val PER: 0.4275
2026-01-08 19:14:20,061: t15.2025.03.16 val PER: 0.3207
2026-01-08 19:14:20,061: t15.2025.03.30 val PER: 0.4299
2026-01-08 19:14:20,061: t15.2025.04.13 val PER: 0.3267
2026-01-08 19:14:20,201: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_4000
2026-01-08 19:14:38,091: Train batch 4200: loss: 23.52 grad norm: 62.68 time: 0.079
2026-01-08 19:14:55,843: Train batch 4400: loss: 17.91 grad norm: 55.47 time: 0.067
2026-01-08 19:15:04,789: Running test after training batch: 4500
2026-01-08 19:15:04,921: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:15:10,141: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:15:10,199: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost net
2026-01-08 19:15:23,613: Val batch 4500: PER (avg): 0.2475 CTC Loss (avg): 24.3052 WER(5gram): 22.62% (n=256) time: 18.823
2026-01-08 19:15:23,614: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-08 19:15:23,614: t15.2023.08.13 val PER: 0.2079
2026-01-08 19:15:23,614: t15.2023.08.18 val PER: 0.1995
2026-01-08 19:15:23,614: t15.2023.08.20 val PER: 0.1938
2026-01-08 19:15:23,614: t15.2023.08.25 val PER: 0.1687
2026-01-08 19:15:23,614: t15.2023.08.27 val PER: 0.2605
2026-01-08 19:15:23,614: t15.2023.09.01 val PER: 0.1672
2026-01-08 19:15:23,614: t15.2023.09.03 val PER: 0.2589
2026-01-08 19:15:23,614: t15.2023.09.24 val PER: 0.1881
2026-01-08 19:15:23,615: t15.2023.09.29 val PER: 0.1966
2026-01-08 19:15:23,615: t15.2023.10.01 val PER: 0.2616
2026-01-08 19:15:23,615: t15.2023.10.06 val PER: 0.1690
2026-01-08 19:15:23,615: t15.2023.10.08 val PER: 0.3275
2026-01-08 19:15:23,615: t15.2023.10.13 val PER: 0.3064
2026-01-08 19:15:23,615: t15.2023.10.15 val PER: 0.2320
2026-01-08 19:15:23,615: t15.2023.10.20 val PER: 0.2483
2026-01-08 19:15:23,615: t15.2023.10.22 val PER: 0.2082
2026-01-08 19:15:23,615: t15.2023.11.03 val PER: 0.2707
2026-01-08 19:15:23,615: t15.2023.11.04 val PER: 0.0751
2026-01-08 19:15:23,615: t15.2023.11.17 val PER: 0.1073
2026-01-08 19:15:23,615: t15.2023.11.19 val PER: 0.0918
2026-01-08 19:15:23,615: t15.2023.11.26 val PER: 0.2804
2026-01-08 19:15:23,615: t15.2023.12.03 val PER: 0.2132
2026-01-08 19:15:23,615: t15.2023.12.08 val PER: 0.2264
2026-01-08 19:15:23,615: t15.2023.12.10 val PER: 0.1800
2026-01-08 19:15:23,616: t15.2023.12.17 val PER: 0.2328
2026-01-08 19:15:23,616: t15.2023.12.29 val PER: 0.2437
2026-01-08 19:15:23,616: t15.2024.02.25 val PER: 0.2008
2026-01-08 19:15:23,616: t15.2024.03.08 val PER: 0.3243
2026-01-08 19:15:23,616: t15.2024.03.15 val PER: 0.2989
2026-01-08 19:15:23,616: t15.2024.03.17 val PER: 0.2427
2026-01-08 19:15:23,616: t15.2024.05.10 val PER: 0.2689
2026-01-08 19:15:23,616: t15.2024.06.14 val PER: 0.2429
2026-01-08 19:15:23,616: t15.2024.07.19 val PER: 0.3560
2026-01-08 19:15:23,616: t15.2024.07.21 val PER: 0.1841
2026-01-08 19:15:23,616: t15.2024.07.28 val PER: 0.2324
2026-01-08 19:15:23,616: t15.2025.01.10 val PER: 0.4201
2026-01-08 19:15:23,616: t15.2025.01.12 val PER: 0.2664
2026-01-08 19:15:23,616: t15.2025.03.14 val PER: 0.4098
2026-01-08 19:15:23,617: t15.2025.03.16 val PER: 0.3128
2026-01-08 19:15:23,617: t15.2025.03.30 val PER: 0.4241
2026-01-08 19:15:23,617: t15.2025.04.13 val PER: 0.3224
2026-01-08 19:15:23,618: New best val WER(5gram) 24.51% --> 22.62%
2026-01-08 19:15:23,801: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_4500
2026-01-08 19:15:32,338: Train batch 4600: loss: 21.73 grad norm: 61.69 time: 0.065
2026-01-08 19:15:49,429: Train batch 4800: loss: 15.42 grad norm: 53.73 time: 0.064
2026-01-08 19:16:06,875: Train batch 5000: loss: 35.45 grad norm: 90.47 time: 0.066
2026-01-08 19:16:06,876: Running test after training batch: 5000
2026-01-08 19:16:07,012: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:16:11,957: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:16:12,026: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost net
2026-01-08 19:16:25,046: Val batch 5000: PER (avg): 0.2342 CTC Loss (avg): 23.0519 WER(5gram): 21.90% (n=256) time: 18.170
2026-01-08 19:16:25,046: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-08 19:16:25,046: t15.2023.08.13 val PER: 0.2069
2026-01-08 19:16:25,047: t15.2023.08.18 val PER: 0.1802
2026-01-08 19:16:25,047: t15.2023.08.20 val PER: 0.1811
2026-01-08 19:16:25,047: t15.2023.08.25 val PER: 0.1370
2026-01-08 19:16:25,047: t15.2023.08.27 val PER: 0.2556
2026-01-08 19:16:25,047: t15.2023.09.01 val PER: 0.1388
2026-01-08 19:16:25,047: t15.2023.09.03 val PER: 0.2387
2026-01-08 19:16:25,047: t15.2023.09.24 val PER: 0.1845
2026-01-08 19:16:25,047: t15.2023.09.29 val PER: 0.1927
2026-01-08 19:16:25,047: t15.2023.10.01 val PER: 0.2371
2026-01-08 19:16:25,047: t15.2023.10.06 val PER: 0.1518
2026-01-08 19:16:25,047: t15.2023.10.08 val PER: 0.3207
2026-01-08 19:16:25,047: t15.2023.10.13 val PER: 0.2917
2026-01-08 19:16:25,047: t15.2023.10.15 val PER: 0.2281
2026-01-08 19:16:25,047: t15.2023.10.20 val PER: 0.2349
2026-01-08 19:16:25,048: t15.2023.10.22 val PER: 0.1793
2026-01-08 19:16:25,048: t15.2023.11.03 val PER: 0.2388
2026-01-08 19:16:25,048: t15.2023.11.04 val PER: 0.0444
2026-01-08 19:16:25,048: t15.2023.11.17 val PER: 0.0824
2026-01-08 19:16:25,048: t15.2023.11.19 val PER: 0.0938
2026-01-08 19:16:25,048: t15.2023.11.26 val PER: 0.2543
2026-01-08 19:16:25,048: t15.2023.12.03 val PER: 0.2111
2026-01-08 19:16:25,048: t15.2023.12.08 val PER: 0.2137
2026-01-08 19:16:25,048: t15.2023.12.10 val PER: 0.1564
2026-01-08 19:16:25,048: t15.2023.12.17 val PER: 0.2370
2026-01-08 19:16:25,048: t15.2023.12.29 val PER: 0.2334
2026-01-08 19:16:25,048: t15.2024.02.25 val PER: 0.1840
2026-01-08 19:16:25,049: t15.2024.03.08 val PER: 0.3229
2026-01-08 19:16:25,049: t15.2024.03.15 val PER: 0.2827
2026-01-08 19:16:25,049: t15.2024.03.17 val PER: 0.2420
2026-01-08 19:16:25,049: t15.2024.05.10 val PER: 0.2481
2026-01-08 19:16:25,049: t15.2024.06.14 val PER: 0.2555
2026-01-08 19:16:25,049: t15.2024.07.19 val PER: 0.3500
2026-01-08 19:16:25,049: t15.2024.07.21 val PER: 0.1855
2026-01-08 19:16:25,049: t15.2024.07.28 val PER: 0.2206
2026-01-08 19:16:25,049: t15.2025.01.10 val PER: 0.4008
2026-01-08 19:16:25,049: t15.2025.01.12 val PER: 0.2471
2026-01-08 19:16:25,049: t15.2025.03.14 val PER: 0.4068
2026-01-08 19:16:25,049: t15.2025.03.16 val PER: 0.2827
2026-01-08 19:16:25,049: t15.2025.03.30 val PER: 0.3989
2026-01-08 19:16:25,049: t15.2025.04.13 val PER: 0.3181
2026-01-08 19:16:25,050: New best val WER(5gram) 22.62% --> 21.90%
2026-01-08 19:16:25,235: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_5000
2026-01-08 19:16:42,576: Train batch 5200: loss: 18.38 grad norm: 65.30 time: 0.053
2026-01-08 19:17:00,208: Train batch 5400: loss: 18.95 grad norm: 61.22 time: 0.070
2026-01-08 19:17:08,996: Running test after training batch: 5500
2026-01-08 19:17:09,151: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:17:14,143: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 19:17:14,199: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 19:17:26,785: Val batch 5500: PER (avg): 0.2213 CTC Loss (avg): 21.9122 WER(5gram): 20.47% (n=256) time: 17.788
2026-01-08 19:17:26,785: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 19:17:26,785: t15.2023.08.13 val PER: 0.1944
2026-01-08 19:17:26,785: t15.2023.08.18 val PER: 0.1744
2026-01-08 19:17:26,786: t15.2023.08.20 val PER: 0.1747
2026-01-08 19:17:26,786: t15.2023.08.25 val PER: 0.1476
2026-01-08 19:17:26,786: t15.2023.08.27 val PER: 0.2524
2026-01-08 19:17:26,786: t15.2023.09.01 val PER: 0.1274
2026-01-08 19:17:26,786: t15.2023.09.03 val PER: 0.2162
2026-01-08 19:17:26,786: t15.2023.09.24 val PER: 0.1796
2026-01-08 19:17:26,787: t15.2023.09.29 val PER: 0.1857
2026-01-08 19:17:26,787: t15.2023.10.01 val PER: 0.2338
2026-01-08 19:17:26,787: t15.2023.10.06 val PER: 0.1378
2026-01-08 19:17:26,787: t15.2023.10.08 val PER: 0.3045
2026-01-08 19:17:26,787: t15.2023.10.13 val PER: 0.2839
2026-01-08 19:17:26,787: t15.2023.10.15 val PER: 0.2123
2026-01-08 19:17:26,787: t15.2023.10.20 val PER: 0.2517
2026-01-08 19:17:26,787: t15.2023.10.22 val PER: 0.1604
2026-01-08 19:17:26,787: t15.2023.11.03 val PER: 0.2313
2026-01-08 19:17:26,787: t15.2023.11.04 val PER: 0.0580
2026-01-08 19:17:26,787: t15.2023.11.17 val PER: 0.0855
2026-01-08 19:17:26,787: t15.2023.11.19 val PER: 0.0838
2026-01-08 19:17:26,787: t15.2023.11.26 val PER: 0.2319
2026-01-08 19:17:26,787: t15.2023.12.03 val PER: 0.1765
2026-01-08 19:17:26,787: t15.2023.12.08 val PER: 0.1971
2026-01-08 19:17:26,788: t15.2023.12.10 val PER: 0.1551
2026-01-08 19:17:26,788: t15.2023.12.17 val PER: 0.2235
2026-01-08 19:17:26,788: t15.2023.12.29 val PER: 0.2121
2026-01-08 19:17:26,788: t15.2024.02.25 val PER: 0.1798
2026-01-08 19:17:26,788: t15.2024.03.08 val PER: 0.3087
2026-01-08 19:17:26,788: t15.2024.03.15 val PER: 0.2633
2026-01-08 19:17:26,788: t15.2024.03.17 val PER: 0.2197
2026-01-08 19:17:26,788: t15.2024.05.10 val PER: 0.2318
2026-01-08 19:17:26,788: t15.2024.06.14 val PER: 0.2366
2026-01-08 19:17:26,788: t15.2024.07.19 val PER: 0.3289
2026-01-08 19:17:26,788: t15.2024.07.21 val PER: 0.1752
2026-01-08 19:17:26,788: t15.2024.07.28 val PER: 0.2176
2026-01-08 19:17:26,788: t15.2025.01.10 val PER: 0.3898
2026-01-08 19:17:26,789: t15.2025.01.12 val PER: 0.2286
2026-01-08 19:17:26,789: t15.2025.03.14 val PER: 0.3802
2026-01-08 19:17:26,789: t15.2025.03.16 val PER: 0.2893
2026-01-08 19:17:26,789: t15.2025.03.30 val PER: 0.3632
2026-01-08 19:17:26,789: t15.2025.04.13 val PER: 0.2939
2026-01-08 19:17:26,790: New best val WER(5gram) 21.90% --> 20.47%
2026-01-08 19:17:26,978: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_5500
2026-01-08 19:17:35,705: Train batch 5600: loss: 21.07 grad norm: 62.80 time: 0.066
2026-01-08 19:17:52,701: Train batch 5800: loss: 14.50 grad norm: 54.86 time: 0.084
2026-01-08 19:18:09,404: Train batch 6000: loss: 15.65 grad norm: 57.59 time: 0.050
2026-01-08 19:18:09,404: Running test after training batch: 6000
2026-01-08 19:18:09,519: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:18:14,510: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:18:14,569: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost at
2026-01-08 19:18:26,978: Val batch 6000: PER (avg): 0.2225 CTC Loss (avg): 22.0375 WER(5gram): 22.10% (n=256) time: 17.573
2026-01-08 19:18:26,978: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 19:18:26,979: t15.2023.08.13 val PER: 0.1871
2026-01-08 19:18:26,979: t15.2023.08.18 val PER: 0.1852
2026-01-08 19:18:26,979: t15.2023.08.20 val PER: 0.1819
2026-01-08 19:18:26,979: t15.2023.08.25 val PER: 0.1461
2026-01-08 19:18:26,979: t15.2023.08.27 val PER: 0.2460
2026-01-08 19:18:26,979: t15.2023.09.01 val PER: 0.1404
2026-01-08 19:18:26,979: t15.2023.09.03 val PER: 0.2352
2026-01-08 19:18:26,979: t15.2023.09.24 val PER: 0.1784
2026-01-08 19:18:26,979: t15.2023.09.29 val PER: 0.1800
2026-01-08 19:18:26,979: t15.2023.10.01 val PER: 0.2272
2026-01-08 19:18:26,980: t15.2023.10.06 val PER: 0.1432
2026-01-08 19:18:26,980: t15.2023.10.08 val PER: 0.3085
2026-01-08 19:18:26,980: t15.2023.10.13 val PER: 0.2839
2026-01-08 19:18:26,980: t15.2023.10.15 val PER: 0.2162
2026-01-08 19:18:26,980: t15.2023.10.20 val PER: 0.2148
2026-01-08 19:18:26,980: t15.2023.10.22 val PER: 0.1659
2026-01-08 19:18:26,980: t15.2023.11.03 val PER: 0.2388
2026-01-08 19:18:26,980: t15.2023.11.04 val PER: 0.0580
2026-01-08 19:18:26,980: t15.2023.11.17 val PER: 0.0949
2026-01-08 19:18:26,980: t15.2023.11.19 val PER: 0.0798
2026-01-08 19:18:26,980: t15.2023.11.26 val PER: 0.2348
2026-01-08 19:18:26,981: t15.2023.12.03 val PER: 0.1775
2026-01-08 19:18:26,981: t15.2023.12.08 val PER: 0.1891
2026-01-08 19:18:26,981: t15.2023.12.10 val PER: 0.1551
2026-01-08 19:18:26,981: t15.2023.12.17 val PER: 0.2037
2026-01-08 19:18:26,981: t15.2023.12.29 val PER: 0.2224
2026-01-08 19:18:26,981: t15.2024.02.25 val PER: 0.1629
2026-01-08 19:18:26,981: t15.2024.03.08 val PER: 0.2959
2026-01-08 19:18:26,981: t15.2024.03.15 val PER: 0.2758
2026-01-08 19:18:26,981: t15.2024.03.17 val PER: 0.2183
2026-01-08 19:18:26,981: t15.2024.05.10 val PER: 0.2348
2026-01-08 19:18:26,981: t15.2024.06.14 val PER: 0.2334
2026-01-08 19:18:26,981: t15.2024.07.19 val PER: 0.3223
2026-01-08 19:18:26,981: t15.2024.07.21 val PER: 0.1841
2026-01-08 19:18:26,981: t15.2024.07.28 val PER: 0.2132
2026-01-08 19:18:26,981: t15.2025.01.10 val PER: 0.3912
2026-01-08 19:18:26,982: t15.2025.01.12 val PER: 0.2240
2026-01-08 19:18:26,982: t15.2025.03.14 val PER: 0.4098
2026-01-08 19:18:26,982: t15.2025.03.16 val PER: 0.2801
2026-01-08 19:18:26,982: t15.2025.03.30 val PER: 0.3862
2026-01-08 19:18:26,982: t15.2025.04.13 val PER: 0.2853
2026-01-08 19:18:27,150: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_6000
2026-01-08 19:18:44,310: Train batch 6200: loss: 17.63 grad norm: 61.26 time: 0.071
2026-01-08 19:19:01,330: Train batch 6400: loss: 21.42 grad norm: 67.73 time: 0.063
2026-01-08 19:19:09,878: Running test after training batch: 6500
2026-01-08 19:19:10,008: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:19:15,036: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:19:15,096: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 19:19:27,084: Val batch 6500: PER (avg): 0.2107 CTC Loss (avg): 20.9037 WER(5gram): 19.30% (n=256) time: 17.205
2026-01-08 19:19:27,084: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 19:19:27,084: t15.2023.08.13 val PER: 0.1715
2026-01-08 19:19:27,085: t15.2023.08.18 val PER: 0.1584
2026-01-08 19:19:27,085: t15.2023.08.20 val PER: 0.1533
2026-01-08 19:19:27,085: t15.2023.08.25 val PER: 0.1325
2026-01-08 19:19:27,085: t15.2023.08.27 val PER: 0.2524
2026-01-08 19:19:27,085: t15.2023.09.01 val PER: 0.1161
2026-01-08 19:19:27,085: t15.2023.09.03 val PER: 0.2209
2026-01-08 19:19:27,085: t15.2023.09.24 val PER: 0.1650
2026-01-08 19:19:27,086: t15.2023.09.29 val PER: 0.1806
2026-01-08 19:19:27,086: t15.2023.10.01 val PER: 0.2246
2026-01-08 19:19:27,086: t15.2023.10.06 val PER: 0.1378
2026-01-08 19:19:27,086: t15.2023.10.08 val PER: 0.2950
2026-01-08 19:19:27,086: t15.2023.10.13 val PER: 0.2754
2026-01-08 19:19:27,086: t15.2023.10.15 val PER: 0.2076
2026-01-08 19:19:27,086: t15.2023.10.20 val PER: 0.2248
2026-01-08 19:19:27,086: t15.2023.10.22 val PER: 0.1748
2026-01-08 19:19:27,086: t15.2023.11.03 val PER: 0.2293
2026-01-08 19:19:27,087: t15.2023.11.04 val PER: 0.0478
2026-01-08 19:19:27,087: t15.2023.11.17 val PER: 0.0715
2026-01-08 19:19:27,087: t15.2023.11.19 val PER: 0.0758
2026-01-08 19:19:27,087: t15.2023.11.26 val PER: 0.2210
2026-01-08 19:19:27,087: t15.2023.12.03 val PER: 0.1754
2026-01-08 19:19:27,087: t15.2023.12.08 val PER: 0.1718
2026-01-08 19:19:27,087: t15.2023.12.10 val PER: 0.1511
2026-01-08 19:19:27,087: t15.2023.12.17 val PER: 0.1954
2026-01-08 19:19:27,088: t15.2023.12.29 val PER: 0.2066
2026-01-08 19:19:27,088: t15.2024.02.25 val PER: 0.1756
2026-01-08 19:19:27,088: t15.2024.03.08 val PER: 0.3044
2026-01-08 19:19:27,088: t15.2024.03.15 val PER: 0.2520
2026-01-08 19:19:27,088: t15.2024.03.17 val PER: 0.2050
2026-01-08 19:19:27,088: t15.2024.05.10 val PER: 0.2318
2026-01-08 19:19:27,088: t15.2024.06.14 val PER: 0.2240
2026-01-08 19:19:27,089: t15.2024.07.19 val PER: 0.3144
2026-01-08 19:19:27,089: t15.2024.07.21 val PER: 0.1593
2026-01-08 19:19:27,089: t15.2024.07.28 val PER: 0.1875
2026-01-08 19:19:27,089: t15.2025.01.10 val PER: 0.3871
2026-01-08 19:19:27,089: t15.2025.01.12 val PER: 0.2156
2026-01-08 19:19:27,089: t15.2025.03.14 val PER: 0.3728
2026-01-08 19:19:27,089: t15.2025.03.16 val PER: 0.2605
2026-01-08 19:19:27,089: t15.2025.03.30 val PER: 0.3851
2026-01-08 19:19:27,089: t15.2025.04.13 val PER: 0.2668
2026-01-08 19:19:27,090: New best val WER(5gram) 20.47% --> 19.30%
2026-01-08 19:19:27,287: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_6500
2026-01-08 19:19:36,462: Train batch 6600: loss: 13.12 grad norm: 50.42 time: 0.046
2026-01-08 19:19:54,918: Train batch 6800: loss: 15.77 grad norm: 54.08 time: 0.049
2026-01-08 19:20:13,562: Train batch 7000: loss: 19.16 grad norm: 69.50 time: 0.063
2026-01-08 19:20:13,563: Running test after training batch: 7000
2026-01-08 19:20:13,724: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:20:18,720: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:20:18,760: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost and
2026-01-08 19:20:30,334: Val batch 7000: PER (avg): 0.2001 CTC Loss (avg): 19.9024 WER(5gram): 19.04% (n=256) time: 16.770
2026-01-08 19:20:30,334: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-08 19:20:30,334: t15.2023.08.13 val PER: 0.1528
2026-01-08 19:20:30,334: t15.2023.08.18 val PER: 0.1567
2026-01-08 19:20:30,334: t15.2023.08.20 val PER: 0.1477
2026-01-08 19:20:30,335: t15.2023.08.25 val PER: 0.1190
2026-01-08 19:20:30,335: t15.2023.08.27 val PER: 0.2251
2026-01-08 19:20:30,335: t15.2023.09.01 val PER: 0.1161
2026-01-08 19:20:30,335: t15.2023.09.03 val PER: 0.2019
2026-01-08 19:20:30,335: t15.2023.09.24 val PER: 0.1711
2026-01-08 19:20:30,335: t15.2023.09.29 val PER: 0.1729
2026-01-08 19:20:30,335: t15.2023.10.01 val PER: 0.2074
2026-01-08 19:20:30,335: t15.2023.10.06 val PER: 0.1216
2026-01-08 19:20:30,335: t15.2023.10.08 val PER: 0.2842
2026-01-08 19:20:30,335: t15.2023.10.13 val PER: 0.2614
2026-01-08 19:20:30,335: t15.2023.10.15 val PER: 0.2050
2026-01-08 19:20:30,335: t15.2023.10.20 val PER: 0.2081
2026-01-08 19:20:30,335: t15.2023.10.22 val PER: 0.1425
2026-01-08 19:20:30,335: t15.2023.11.03 val PER: 0.2083
2026-01-08 19:20:30,335: t15.2023.11.04 val PER: 0.0478
2026-01-08 19:20:30,336: t15.2023.11.17 val PER: 0.0653
2026-01-08 19:20:30,336: t15.2023.11.19 val PER: 0.0619
2026-01-08 19:20:30,336: t15.2023.11.26 val PER: 0.2036
2026-01-08 19:20:30,336: t15.2023.12.03 val PER: 0.1723
2026-01-08 19:20:30,336: t15.2023.12.08 val PER: 0.1664
2026-01-08 19:20:30,336: t15.2023.12.10 val PER: 0.1472
2026-01-08 19:20:30,336: t15.2023.12.17 val PER: 0.1923
2026-01-08 19:20:30,336: t15.2023.12.29 val PER: 0.1977
2026-01-08 19:20:30,336: t15.2024.02.25 val PER: 0.1643
2026-01-08 19:20:30,336: t15.2024.03.08 val PER: 0.2802
2026-01-08 19:20:30,336: t15.2024.03.15 val PER: 0.2508
2026-01-08 19:20:30,336: t15.2024.03.17 val PER: 0.2001
2026-01-08 19:20:30,336: t15.2024.05.10 val PER: 0.2065
2026-01-08 19:20:30,336: t15.2024.06.14 val PER: 0.2192
2026-01-08 19:20:30,337: t15.2024.07.19 val PER: 0.3078
2026-01-08 19:20:30,337: t15.2024.07.21 val PER: 0.1497
2026-01-08 19:20:30,337: t15.2024.07.28 val PER: 0.1794
2026-01-08 19:20:30,337: t15.2025.01.10 val PER: 0.3581
2026-01-08 19:20:30,337: t15.2025.01.12 val PER: 0.2040
2026-01-08 19:20:30,337: t15.2025.03.14 val PER: 0.3757
2026-01-08 19:20:30,337: t15.2025.03.16 val PER: 0.2421
2026-01-08 19:20:30,337: t15.2025.03.30 val PER: 0.3494
2026-01-08 19:20:30,337: t15.2025.04.13 val PER: 0.2782
2026-01-08 19:20:30,339: New best val WER(5gram) 19.30% --> 19.04%
2026-01-08 19:20:30,532: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_7000
2026-01-08 19:20:48,802: Train batch 7200: loss: 15.54 grad norm: 57.64 time: 0.080
2026-01-08 19:21:06,339: Train batch 7400: loss: 15.04 grad norm: 57.26 time: 0.075
2026-01-08 19:21:15,126: Running test after training batch: 7500
2026-01-08 19:21:15,246: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:21:20,252: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:21:20,308: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-08 19:21:32,394: Val batch 7500: PER (avg): 0.1962 CTC Loss (avg): 19.4332 WER(5gram): 18.51% (n=256) time: 17.267
2026-01-08 19:21:32,395: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-08 19:21:32,395: t15.2023.08.13 val PER: 0.1653
2026-01-08 19:21:32,395: t15.2023.08.18 val PER: 0.1567
2026-01-08 19:21:32,395: t15.2023.08.20 val PER: 0.1509
2026-01-08 19:21:32,395: t15.2023.08.25 val PER: 0.1235
2026-01-08 19:21:32,395: t15.2023.08.27 val PER: 0.2122
2026-01-08 19:21:32,395: t15.2023.09.01 val PER: 0.1185
2026-01-08 19:21:32,395: t15.2023.09.03 val PER: 0.2007
2026-01-08 19:21:32,395: t15.2023.09.24 val PER: 0.1638
2026-01-08 19:21:32,396: t15.2023.09.29 val PER: 0.1685
2026-01-08 19:21:32,396: t15.2023.10.01 val PER: 0.2087
2026-01-08 19:21:32,396: t15.2023.10.06 val PER: 0.1313
2026-01-08 19:21:32,396: t15.2023.10.08 val PER: 0.2706
2026-01-08 19:21:32,396: t15.2023.10.13 val PER: 0.2545
2026-01-08 19:21:32,396: t15.2023.10.15 val PER: 0.2070
2026-01-08 19:21:32,396: t15.2023.10.20 val PER: 0.2114
2026-01-08 19:21:32,396: t15.2023.10.22 val PER: 0.1526
2026-01-08 19:21:32,396: t15.2023.11.03 val PER: 0.2056
2026-01-08 19:21:32,396: t15.2023.11.04 val PER: 0.0444
2026-01-08 19:21:32,396: t15.2023.11.17 val PER: 0.0622
2026-01-08 19:21:32,396: t15.2023.11.19 val PER: 0.0559
2026-01-08 19:21:32,396: t15.2023.11.26 val PER: 0.1964
2026-01-08 19:21:32,396: t15.2023.12.03 val PER: 0.1607
2026-01-08 19:21:32,396: t15.2023.12.08 val PER: 0.1531
2026-01-08 19:21:32,396: t15.2023.12.10 val PER: 0.1419
2026-01-08 19:21:32,397: t15.2023.12.17 val PER: 0.1871
2026-01-08 19:21:32,397: t15.2023.12.29 val PER: 0.1970
2026-01-08 19:21:32,397: t15.2024.02.25 val PER: 0.1489
2026-01-08 19:21:32,397: t15.2024.03.08 val PER: 0.2745
2026-01-08 19:21:32,397: t15.2024.03.15 val PER: 0.2433
2026-01-08 19:21:32,397: t15.2024.03.17 val PER: 0.1960
2026-01-08 19:21:32,397: t15.2024.05.10 val PER: 0.2021
2026-01-08 19:21:32,397: t15.2024.06.14 val PER: 0.2003
2026-01-08 19:21:32,397: t15.2024.07.19 val PER: 0.2993
2026-01-08 19:21:32,397: t15.2024.07.21 val PER: 0.1434
2026-01-08 19:21:32,397: t15.2024.07.28 val PER: 0.1750
2026-01-08 19:21:32,397: t15.2025.01.10 val PER: 0.3650
2026-01-08 19:21:32,397: t15.2025.01.12 val PER: 0.1940
2026-01-08 19:21:32,398: t15.2025.03.14 val PER: 0.3669
2026-01-08 19:21:32,398: t15.2025.03.16 val PER: 0.2435
2026-01-08 19:21:32,398: t15.2025.03.30 val PER: 0.3448
2026-01-08 19:21:32,398: t15.2025.04.13 val PER: 0.2582
2026-01-08 19:21:32,399: New best val WER(5gram) 19.04% --> 18.51%
2026-01-08 19:21:32,582: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_7500
2026-01-08 19:21:41,974: Train batch 7600: loss: 16.29 grad norm: 59.84 time: 0.074
2026-01-08 19:22:00,620: Train batch 7800: loss: 15.95 grad norm: 66.03 time: 0.058
2026-01-08 19:22:19,662: Train batch 8000: loss: 12.79 grad norm: 52.64 time: 0.073
2026-01-08 19:22:19,663: Running test after training batch: 8000
2026-01-08 19:22:19,775: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:22:25,101: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:22:25,149: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost and
2026-01-08 19:22:36,541: Val batch 8000: PER (avg): 0.1899 CTC Loss (avg): 18.7975 WER(5gram): 17.93% (n=256) time: 16.878
2026-01-08 19:22:36,541: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 19:22:36,541: t15.2023.08.13 val PER: 0.1590
2026-01-08 19:22:36,542: t15.2023.08.18 val PER: 0.1442
2026-01-08 19:22:36,542: t15.2023.08.20 val PER: 0.1438
2026-01-08 19:22:36,542: t15.2023.08.25 val PER: 0.1175
2026-01-08 19:22:36,542: t15.2023.08.27 val PER: 0.2219
2026-01-08 19:22:36,542: t15.2023.09.01 val PER: 0.1120
2026-01-08 19:22:36,542: t15.2023.09.03 val PER: 0.1971
2026-01-08 19:22:36,542: t15.2023.09.24 val PER: 0.1602
2026-01-08 19:22:36,542: t15.2023.09.29 val PER: 0.1532
2026-01-08 19:22:36,542: t15.2023.10.01 val PER: 0.2041
2026-01-08 19:22:36,543: t15.2023.10.06 val PER: 0.1216
2026-01-08 19:22:36,543: t15.2023.10.08 val PER: 0.2801
2026-01-08 19:22:36,543: t15.2023.10.13 val PER: 0.2514
2026-01-08 19:22:36,543: t15.2023.10.15 val PER: 0.1931
2026-01-08 19:22:36,543: t15.2023.10.20 val PER: 0.2148
2026-01-08 19:22:36,543: t15.2023.10.22 val PER: 0.1459
2026-01-08 19:22:36,543: t15.2023.11.03 val PER: 0.2103
2026-01-08 19:22:36,543: t15.2023.11.04 val PER: 0.0410
2026-01-08 19:22:36,543: t15.2023.11.17 val PER: 0.0560
2026-01-08 19:22:36,543: t15.2023.11.19 val PER: 0.0679
2026-01-08 19:22:36,543: t15.2023.11.26 val PER: 0.1848
2026-01-08 19:22:36,543: t15.2023.12.03 val PER: 0.1639
2026-01-08 19:22:36,543: t15.2023.12.08 val PER: 0.1458
2026-01-08 19:22:36,543: t15.2023.12.10 val PER: 0.1314
2026-01-08 19:22:36,543: t15.2023.12.17 val PER: 0.1736
2026-01-08 19:22:36,543: t15.2023.12.29 val PER: 0.1716
2026-01-08 19:22:36,544: t15.2024.02.25 val PER: 0.1461
2026-01-08 19:22:36,544: t15.2024.03.08 val PER: 0.2831
2026-01-08 19:22:36,544: t15.2024.03.15 val PER: 0.2351
2026-01-08 19:22:36,544: t15.2024.03.17 val PER: 0.1834
2026-01-08 19:22:36,544: t15.2024.05.10 val PER: 0.2006
2026-01-08 19:22:36,544: t15.2024.06.14 val PER: 0.2035
2026-01-08 19:22:36,544: t15.2024.07.19 val PER: 0.2986
2026-01-08 19:22:36,544: t15.2024.07.21 val PER: 0.1276
2026-01-08 19:22:36,544: t15.2024.07.28 val PER: 0.1713
2026-01-08 19:22:36,544: t15.2025.01.10 val PER: 0.3292
2026-01-08 19:22:36,544: t15.2025.01.12 val PER: 0.1932
2026-01-08 19:22:36,544: t15.2025.03.14 val PER: 0.3536
2026-01-08 19:22:36,544: t15.2025.03.16 val PER: 0.2330
2026-01-08 19:22:36,544: t15.2025.03.30 val PER: 0.3575
2026-01-08 19:22:36,544: t15.2025.04.13 val PER: 0.2668
2026-01-08 19:22:36,546: New best val WER(5gram) 18.51% --> 17.93%
2026-01-08 19:22:36,739: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_8000
2026-01-08 19:22:55,521: Train batch 8200: loss: 10.25 grad norm: 46.76 time: 0.060
2026-01-08 19:23:14,277: Train batch 8400: loss: 10.89 grad norm: 45.78 time: 0.064
2026-01-08 19:23:23,704: Running test after training batch: 8500
2026-01-08 19:23:23,815: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:23:28,762: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:23:28,803: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 19:23:40,223: Val batch 8500: PER (avg): 0.1865 CTC Loss (avg): 18.3899 WER(5gram): 16.56% (n=256) time: 16.518
2026-01-08 19:23:40,223: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-08 19:23:40,224: t15.2023.08.13 val PER: 0.1538
2026-01-08 19:23:40,224: t15.2023.08.18 val PER: 0.1492
2026-01-08 19:23:40,224: t15.2023.08.20 val PER: 0.1374
2026-01-08 19:23:40,224: t15.2023.08.25 val PER: 0.1265
2026-01-08 19:23:40,224: t15.2023.08.27 val PER: 0.2138
2026-01-08 19:23:40,224: t15.2023.09.01 val PER: 0.1096
2026-01-08 19:23:40,224: t15.2023.09.03 val PER: 0.2007
2026-01-08 19:23:40,225: t15.2023.09.24 val PER: 0.1590
2026-01-08 19:23:40,225: t15.2023.09.29 val PER: 0.1525
2026-01-08 19:23:40,225: t15.2023.10.01 val PER: 0.1982
2026-01-08 19:23:40,225: t15.2023.10.06 val PER: 0.1141
2026-01-08 19:23:40,225: t15.2023.10.08 val PER: 0.2815
2026-01-08 19:23:40,225: t15.2023.10.13 val PER: 0.2444
2026-01-08 19:23:40,225: t15.2023.10.15 val PER: 0.1872
2026-01-08 19:23:40,225: t15.2023.10.20 val PER: 0.2081
2026-01-08 19:23:40,225: t15.2023.10.22 val PER: 0.1459
2026-01-08 19:23:40,225: t15.2023.11.03 val PER: 0.1954
2026-01-08 19:23:40,225: t15.2023.11.04 val PER: 0.0375
2026-01-08 19:23:40,225: t15.2023.11.17 val PER: 0.0607
2026-01-08 19:23:40,225: t15.2023.11.19 val PER: 0.0559
2026-01-08 19:23:40,225: t15.2023.11.26 val PER: 0.1746
2026-01-08 19:23:40,225: t15.2023.12.03 val PER: 0.1513
2026-01-08 19:23:40,225: t15.2023.12.08 val PER: 0.1458
2026-01-08 19:23:40,226: t15.2023.12.10 val PER: 0.1248
2026-01-08 19:23:40,226: t15.2023.12.17 val PER: 0.1767
2026-01-08 19:23:40,226: t15.2023.12.29 val PER: 0.1682
2026-01-08 19:23:40,226: t15.2024.02.25 val PER: 0.1433
2026-01-08 19:23:40,226: t15.2024.03.08 val PER: 0.2617
2026-01-08 19:23:40,227: t15.2024.03.15 val PER: 0.2358
2026-01-08 19:23:40,227: t15.2024.03.17 val PER: 0.1876
2026-01-08 19:23:40,227: t15.2024.05.10 val PER: 0.2021
2026-01-08 19:23:40,227: t15.2024.06.14 val PER: 0.1956
2026-01-08 19:23:40,227: t15.2024.07.19 val PER: 0.2854
2026-01-08 19:23:40,227: t15.2024.07.21 val PER: 0.1241
2026-01-08 19:23:40,227: t15.2024.07.28 val PER: 0.1662
2026-01-08 19:23:40,227: t15.2025.01.10 val PER: 0.3457
2026-01-08 19:23:40,227: t15.2025.01.12 val PER: 0.1917
2026-01-08 19:23:40,227: t15.2025.03.14 val PER: 0.3669
2026-01-08 19:23:40,227: t15.2025.03.16 val PER: 0.2369
2026-01-08 19:23:40,227: t15.2025.03.30 val PER: 0.3598
2026-01-08 19:23:40,227: t15.2025.04.13 val PER: 0.2454
2026-01-08 19:23:40,229: New best val WER(5gram) 17.93% --> 16.56%
2026-01-08 19:23:40,426: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_8500
2026-01-08 19:23:49,587: Train batch 8600: loss: 17.34 grad norm: 59.74 time: 0.057
2026-01-08 19:24:07,871: Train batch 8800: loss: 16.40 grad norm: 57.91 time: 0.068
2026-01-08 19:24:26,301: Train batch 9000: loss: 16.83 grad norm: 62.60 time: 0.081
2026-01-08 19:24:26,301: Running test after training batch: 9000
2026-01-08 19:24:26,415: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:24:31,372: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:24:31,416: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 19:24:42,682: Val batch 9000: PER (avg): 0.1799 CTC Loss (avg): 17.9858 WER(5gram): 17.28% (n=256) time: 16.381
2026-01-08 19:24:42,683: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 19:24:42,683: t15.2023.08.13 val PER: 0.1476
2026-01-08 19:24:42,683: t15.2023.08.18 val PER: 0.1358
2026-01-08 19:24:42,683: t15.2023.08.20 val PER: 0.1311
2026-01-08 19:24:42,684: t15.2023.08.25 val PER: 0.1024
2026-01-08 19:24:42,684: t15.2023.08.27 val PER: 0.2138
2026-01-08 19:24:42,684: t15.2023.09.01 val PER: 0.0982
2026-01-08 19:24:42,684: t15.2023.09.03 val PER: 0.1793
2026-01-08 19:24:42,684: t15.2023.09.24 val PER: 0.1602
2026-01-08 19:24:42,684: t15.2023.09.29 val PER: 0.1525
2026-01-08 19:24:42,684: t15.2023.10.01 val PER: 0.1995
2026-01-08 19:24:42,684: t15.2023.10.06 val PER: 0.1044
2026-01-08 19:24:42,684: t15.2023.10.08 val PER: 0.2544
2026-01-08 19:24:42,684: t15.2023.10.13 val PER: 0.2335
2026-01-08 19:24:42,684: t15.2023.10.15 val PER: 0.1839
2026-01-08 19:24:42,684: t15.2023.10.20 val PER: 0.1946
2026-01-08 19:24:42,684: t15.2023.10.22 val PER: 0.1414
2026-01-08 19:24:42,684: t15.2023.11.03 val PER: 0.2056
2026-01-08 19:24:42,684: t15.2023.11.04 val PER: 0.0478
2026-01-08 19:24:42,684: t15.2023.11.17 val PER: 0.0638
2026-01-08 19:24:42,685: t15.2023.11.19 val PER: 0.0539
2026-01-08 19:24:42,685: t15.2023.11.26 val PER: 0.1703
2026-01-08 19:24:42,685: t15.2023.12.03 val PER: 0.1460
2026-01-08 19:24:42,685: t15.2023.12.08 val PER: 0.1391
2026-01-08 19:24:42,685: t15.2023.12.10 val PER: 0.1170
2026-01-08 19:24:42,685: t15.2023.12.17 val PER: 0.1590
2026-01-08 19:24:42,685: t15.2023.12.29 val PER: 0.1791
2026-01-08 19:24:42,685: t15.2024.02.25 val PER: 0.1404
2026-01-08 19:24:42,685: t15.2024.03.08 val PER: 0.2817
2026-01-08 19:24:42,685: t15.2024.03.15 val PER: 0.2320
2026-01-08 19:24:42,685: t15.2024.03.17 val PER: 0.1785
2026-01-08 19:24:42,685: t15.2024.05.10 val PER: 0.1828
2026-01-08 19:24:42,685: t15.2024.06.14 val PER: 0.1987
2026-01-08 19:24:42,685: t15.2024.07.19 val PER: 0.2709
2026-01-08 19:24:42,686: t15.2024.07.21 val PER: 0.1138
2026-01-08 19:24:42,686: t15.2024.07.28 val PER: 0.1632
2026-01-08 19:24:42,686: t15.2025.01.10 val PER: 0.3237
2026-01-08 19:24:42,686: t15.2025.01.12 val PER: 0.1801
2026-01-08 19:24:42,686: t15.2025.03.14 val PER: 0.3506
2026-01-08 19:24:42,686: t15.2025.03.16 val PER: 0.2317
2026-01-08 19:24:42,686: t15.2025.03.30 val PER: 0.3195
2026-01-08 19:24:42,686: t15.2025.04.13 val PER: 0.2539
2026-01-08 19:24:42,827: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_9000
2026-01-08 19:25:01,195: Train batch 9200: loss: 11.16 grad norm: 46.95 time: 0.057
2026-01-08 19:25:19,468: Train batch 9400: loss: 8.50 grad norm: 44.17 time: 0.075
2026-01-08 19:25:28,633: Running test after training batch: 9500
2026-01-08 19:25:28,773: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:25:33,743: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:25:33,784: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 19:25:44,943: Val batch 9500: PER (avg): 0.1798 CTC Loss (avg): 17.6071 WER(5gram): 17.41% (n=256) time: 16.310
2026-01-08 19:25:44,943: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 19:25:44,943: t15.2023.08.13 val PER: 0.1476
2026-01-08 19:25:44,943: t15.2023.08.18 val PER: 0.1333
2026-01-08 19:25:44,944: t15.2023.08.20 val PER: 0.1287
2026-01-08 19:25:44,944: t15.2023.08.25 val PER: 0.1160
2026-01-08 19:25:44,944: t15.2023.08.27 val PER: 0.1994
2026-01-08 19:25:44,945: t15.2023.09.01 val PER: 0.1031
2026-01-08 19:25:44,945: t15.2023.09.03 val PER: 0.1793
2026-01-08 19:25:44,945: t15.2023.09.24 val PER: 0.1529
2026-01-08 19:25:44,945: t15.2023.09.29 val PER: 0.1481
2026-01-08 19:25:44,945: t15.2023.10.01 val PER: 0.1856
2026-01-08 19:25:44,945: t15.2023.10.06 val PER: 0.1141
2026-01-08 19:25:44,946: t15.2023.10.08 val PER: 0.2625
2026-01-08 19:25:44,946: t15.2023.10.13 val PER: 0.2312
2026-01-08 19:25:44,946: t15.2023.10.15 val PER: 0.1846
2026-01-08 19:25:44,946: t15.2023.10.20 val PER: 0.2114
2026-01-08 19:25:44,946: t15.2023.10.22 val PER: 0.1392
2026-01-08 19:25:44,946: t15.2023.11.03 val PER: 0.2015
2026-01-08 19:25:44,946: t15.2023.11.04 val PER: 0.0375
2026-01-08 19:25:44,946: t15.2023.11.17 val PER: 0.0622
2026-01-08 19:25:44,947: t15.2023.11.19 val PER: 0.0519
2026-01-08 19:25:44,947: t15.2023.11.26 val PER: 0.1746
2026-01-08 19:25:44,947: t15.2023.12.03 val PER: 0.1471
2026-01-08 19:25:44,947: t15.2023.12.08 val PER: 0.1352
2026-01-08 19:25:44,947: t15.2023.12.10 val PER: 0.1209
2026-01-08 19:25:44,947: t15.2023.12.17 val PER: 0.1705
2026-01-08 19:25:44,947: t15.2023.12.29 val PER: 0.1599
2026-01-08 19:25:44,947: t15.2024.02.25 val PER: 0.1419
2026-01-08 19:25:44,947: t15.2024.03.08 val PER: 0.2603
2026-01-08 19:25:44,947: t15.2024.03.15 val PER: 0.2276
2026-01-08 19:25:44,948: t15.2024.03.17 val PER: 0.1771
2026-01-08 19:25:44,948: t15.2024.05.10 val PER: 0.2021
2026-01-08 19:25:44,948: t15.2024.06.14 val PER: 0.1956
2026-01-08 19:25:44,948: t15.2024.07.19 val PER: 0.2755
2026-01-08 19:25:44,948: t15.2024.07.21 val PER: 0.1303
2026-01-08 19:25:44,948: t15.2024.07.28 val PER: 0.1684
2026-01-08 19:25:44,948: t15.2025.01.10 val PER: 0.3168
2026-01-08 19:25:44,948: t15.2025.01.12 val PER: 0.1809
2026-01-08 19:25:44,948: t15.2025.03.14 val PER: 0.3669
2026-01-08 19:25:44,949: t15.2025.03.16 val PER: 0.2225
2026-01-08 19:25:44,949: t15.2025.03.30 val PER: 0.3253
2026-01-08 19:25:44,949: t15.2025.04.13 val PER: 0.2625
2026-01-08 19:25:45,086: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_9500
2026-01-08 19:25:53,850: Train batch 9600: loss: 9.61 grad norm: 47.98 time: 0.074
2026-01-08 19:26:11,662: Train batch 9800: loss: 13.34 grad norm: 61.80 time: 0.064
2026-01-08 19:26:29,809: Train batch 10000: loss: 6.62 grad norm: 40.11 time: 0.062
2026-01-08 19:26:29,809: Running test after training batch: 10000
2026-01-08 19:26:29,944: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:26:34,908: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:26:34,954: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 19:26:45,901: Val batch 10000: PER (avg): 0.1757 CTC Loss (avg): 17.6933 WER(5gram): 16.23% (n=256) time: 16.091
2026-01-08 19:26:45,901: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-08 19:26:45,901: t15.2023.08.13 val PER: 0.1393
2026-01-08 19:26:45,902: t15.2023.08.18 val PER: 0.1282
2026-01-08 19:26:45,902: t15.2023.08.20 val PER: 0.1279
2026-01-08 19:26:45,902: t15.2023.08.25 val PER: 0.1220
2026-01-08 19:26:45,902: t15.2023.08.27 val PER: 0.1977
2026-01-08 19:26:45,902: t15.2023.09.01 val PER: 0.0950
2026-01-08 19:26:45,902: t15.2023.09.03 val PER: 0.1805
2026-01-08 19:26:45,902: t15.2023.09.24 val PER: 0.1541
2026-01-08 19:26:45,902: t15.2023.09.29 val PER: 0.1474
2026-01-08 19:26:45,902: t15.2023.10.01 val PER: 0.1876
2026-01-08 19:26:45,902: t15.2023.10.06 val PER: 0.1141
2026-01-08 19:26:45,902: t15.2023.10.08 val PER: 0.2544
2026-01-08 19:26:45,902: t15.2023.10.13 val PER: 0.2358
2026-01-08 19:26:45,903: t15.2023.10.15 val PER: 0.1800
2026-01-08 19:26:45,903: t15.2023.10.20 val PER: 0.1980
2026-01-08 19:26:45,903: t15.2023.10.22 val PER: 0.1359
2026-01-08 19:26:45,903: t15.2023.11.03 val PER: 0.1961
2026-01-08 19:26:45,903: t15.2023.11.04 val PER: 0.0375
2026-01-08 19:26:45,903: t15.2023.11.17 val PER: 0.0482
2026-01-08 19:26:45,903: t15.2023.11.19 val PER: 0.0559
2026-01-08 19:26:45,903: t15.2023.11.26 val PER: 0.1645
2026-01-08 19:26:45,903: t15.2023.12.03 val PER: 0.1439
2026-01-08 19:26:45,903: t15.2023.12.08 val PER: 0.1312
2026-01-08 19:26:45,903: t15.2023.12.10 val PER: 0.1117
2026-01-08 19:26:45,903: t15.2023.12.17 val PER: 0.1549
2026-01-08 19:26:45,903: t15.2023.12.29 val PER: 0.1633
2026-01-08 19:26:45,903: t15.2024.02.25 val PER: 0.1320
2026-01-08 19:26:45,903: t15.2024.03.08 val PER: 0.2461
2026-01-08 19:26:45,903: t15.2024.03.15 val PER: 0.2170
2026-01-08 19:26:45,903: t15.2024.03.17 val PER: 0.1736
2026-01-08 19:26:45,904: t15.2024.05.10 val PER: 0.1768
2026-01-08 19:26:45,904: t15.2024.06.14 val PER: 0.1861
2026-01-08 19:26:45,904: t15.2024.07.19 val PER: 0.2788
2026-01-08 19:26:45,904: t15.2024.07.21 val PER: 0.1159
2026-01-08 19:26:45,904: t15.2024.07.28 val PER: 0.1669
2026-01-08 19:26:45,904: t15.2025.01.10 val PER: 0.3361
2026-01-08 19:26:45,904: t15.2025.01.12 val PER: 0.1778
2026-01-08 19:26:45,904: t15.2025.03.14 val PER: 0.3550
2026-01-08 19:26:45,904: t15.2025.03.16 val PER: 0.2186
2026-01-08 19:26:45,904: t15.2025.03.30 val PER: 0.3322
2026-01-08 19:26:45,904: t15.2025.04.13 val PER: 0.2511
2026-01-08 19:26:45,905: New best val WER(5gram) 16.56% --> 16.23%
2026-01-08 19:26:46,096: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_10000
2026-01-08 19:27:04,060: Train batch 10200: loss: 6.39 grad norm: 35.28 time: 0.051
2026-01-08 19:27:21,265: Train batch 10400: loss: 10.35 grad norm: 50.21 time: 0.073
2026-01-08 19:27:30,642: Running test after training batch: 10500
2026-01-08 19:27:30,798: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:27:35,758: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:27:35,799: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 19:27:47,159: Val batch 10500: PER (avg): 0.1732 CTC Loss (avg): 17.4017 WER(5gram): 16.36% (n=256) time: 16.516
2026-01-08 19:27:47,160: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 19:27:47,160: t15.2023.08.13 val PER: 0.1341
2026-01-08 19:27:47,160: t15.2023.08.18 val PER: 0.1282
2026-01-08 19:27:47,160: t15.2023.08.20 val PER: 0.1334
2026-01-08 19:27:47,161: t15.2023.08.25 val PER: 0.1099
2026-01-08 19:27:47,161: t15.2023.08.27 val PER: 0.2154
2026-01-08 19:27:47,161: t15.2023.09.01 val PER: 0.0966
2026-01-08 19:27:47,161: t15.2023.09.03 val PER: 0.1841
2026-01-08 19:27:47,161: t15.2023.09.24 val PER: 0.1553
2026-01-08 19:27:47,161: t15.2023.09.29 val PER: 0.1442
2026-01-08 19:27:47,161: t15.2023.10.01 val PER: 0.1962
2026-01-08 19:27:47,161: t15.2023.10.06 val PER: 0.1087
2026-01-08 19:27:47,161: t15.2023.10.08 val PER: 0.2544
2026-01-08 19:27:47,162: t15.2023.10.13 val PER: 0.2180
2026-01-08 19:27:47,162: t15.2023.10.15 val PER: 0.1786
2026-01-08 19:27:47,162: t15.2023.10.20 val PER: 0.1980
2026-01-08 19:27:47,162: t15.2023.10.22 val PER: 0.1336
2026-01-08 19:27:47,162: t15.2023.11.03 val PER: 0.2049
2026-01-08 19:27:47,162: t15.2023.11.04 val PER: 0.0307
2026-01-08 19:27:47,162: t15.2023.11.17 val PER: 0.0482
2026-01-08 19:27:47,162: t15.2023.11.19 val PER: 0.0559
2026-01-08 19:27:47,162: t15.2023.11.26 val PER: 0.1601
2026-01-08 19:27:47,162: t15.2023.12.03 val PER: 0.1460
2026-01-08 19:27:47,163: t15.2023.12.08 val PER: 0.1285
2026-01-08 19:27:47,163: t15.2023.12.10 val PER: 0.1104
2026-01-08 19:27:47,163: t15.2023.12.17 val PER: 0.1476
2026-01-08 19:27:47,163: t15.2023.12.29 val PER: 0.1647
2026-01-08 19:27:47,163: t15.2024.02.25 val PER: 0.1334
2026-01-08 19:27:47,163: t15.2024.03.08 val PER: 0.2617
2026-01-08 19:27:47,163: t15.2024.03.15 val PER: 0.2158
2026-01-08 19:27:47,163: t15.2024.03.17 val PER: 0.1632
2026-01-08 19:27:47,163: t15.2024.05.10 val PER: 0.1709
2026-01-08 19:27:47,163: t15.2024.06.14 val PER: 0.1830
2026-01-08 19:27:47,163: t15.2024.07.19 val PER: 0.2709
2026-01-08 19:27:47,164: t15.2024.07.21 val PER: 0.1083
2026-01-08 19:27:47,164: t15.2024.07.28 val PER: 0.1529
2026-01-08 19:27:47,164: t15.2025.01.10 val PER: 0.3361
2026-01-08 19:27:47,164: t15.2025.01.12 val PER: 0.1686
2026-01-08 19:27:47,164: t15.2025.03.14 val PER: 0.3550
2026-01-08 19:27:47,164: t15.2025.03.16 val PER: 0.2147
2026-01-08 19:27:47,164: t15.2025.03.30 val PER: 0.3276
2026-01-08 19:27:47,164: t15.2025.04.13 val PER: 0.2311
2026-01-08 19:27:47,305: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_10500
2026-01-08 19:27:56,602: Train batch 10600: loss: 10.33 grad norm: 54.81 time: 0.073
2026-01-08 19:28:14,201: Train batch 10800: loss: 15.61 grad norm: 66.16 time: 0.065
2026-01-08 19:28:31,626: Train batch 11000: loss: 16.18 grad norm: 64.13 time: 0.065
2026-01-08 19:28:31,626: Running test after training batch: 11000
2026-01-08 19:28:31,762: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:28:36,935: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:28:36,982: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost in
2026-01-08 19:28:48,486: Val batch 11000: PER (avg): 0.1682 CTC Loss (avg): 17.1028 WER(5gram): 16.88% (n=256) time: 16.860
2026-01-08 19:28:48,486: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=13
2026-01-08 19:28:48,487: t15.2023.08.13 val PER: 0.1320
2026-01-08 19:28:48,487: t15.2023.08.18 val PER: 0.1282
2026-01-08 19:28:48,487: t15.2023.08.20 val PER: 0.1255
2026-01-08 19:28:48,487: t15.2023.08.25 val PER: 0.1084
2026-01-08 19:28:48,487: t15.2023.08.27 val PER: 0.1994
2026-01-08 19:28:48,487: t15.2023.09.01 val PER: 0.0893
2026-01-08 19:28:48,487: t15.2023.09.03 val PER: 0.1829
2026-01-08 19:28:48,487: t15.2023.09.24 val PER: 0.1505
2026-01-08 19:28:48,487: t15.2023.09.29 val PER: 0.1474
2026-01-08 19:28:48,487: t15.2023.10.01 val PER: 0.1836
2026-01-08 19:28:48,487: t15.2023.10.06 val PER: 0.0958
2026-01-08 19:28:48,488: t15.2023.10.08 val PER: 0.2517
2026-01-08 19:28:48,488: t15.2023.10.13 val PER: 0.2157
2026-01-08 19:28:48,488: t15.2023.10.15 val PER: 0.1714
2026-01-08 19:28:48,488: t15.2023.10.20 val PER: 0.1879
2026-01-08 19:28:48,488: t15.2023.10.22 val PER: 0.1303
2026-01-08 19:28:48,488: t15.2023.11.03 val PER: 0.1934
2026-01-08 19:28:48,488: t15.2023.11.04 val PER: 0.0375
2026-01-08 19:28:48,488: t15.2023.11.17 val PER: 0.0544
2026-01-08 19:28:48,488: t15.2023.11.19 val PER: 0.0499
2026-01-08 19:28:48,488: t15.2023.11.26 val PER: 0.1514
2026-01-08 19:28:48,488: t15.2023.12.03 val PER: 0.1303
2026-01-08 19:28:48,488: t15.2023.12.08 val PER: 0.1198
2026-01-08 19:28:48,488: t15.2023.12.10 val PER: 0.1051
2026-01-08 19:28:48,488: t15.2023.12.17 val PER: 0.1528
2026-01-08 19:28:48,488: t15.2023.12.29 val PER: 0.1510
2026-01-08 19:28:48,489: t15.2024.02.25 val PER: 0.1278
2026-01-08 19:28:48,489: t15.2024.03.08 val PER: 0.2404
2026-01-08 19:28:48,489: t15.2024.03.15 val PER: 0.2120
2026-01-08 19:28:48,489: t15.2024.03.17 val PER: 0.1590
2026-01-08 19:28:48,489: t15.2024.05.10 val PER: 0.1753
2026-01-08 19:28:48,489: t15.2024.06.14 val PER: 0.1735
2026-01-08 19:28:48,489: t15.2024.07.19 val PER: 0.2591
2026-01-08 19:28:48,489: t15.2024.07.21 val PER: 0.1041
2026-01-08 19:28:48,489: t15.2024.07.28 val PER: 0.1640
2026-01-08 19:28:48,489: t15.2025.01.10 val PER: 0.3140
2026-01-08 19:28:48,489: t15.2025.01.12 val PER: 0.1794
2026-01-08 19:28:48,490: t15.2025.03.14 val PER: 0.3624
2026-01-08 19:28:48,490: t15.2025.03.16 val PER: 0.2173
2026-01-08 19:28:48,490: t15.2025.03.30 val PER: 0.3161
2026-01-08 19:28:48,490: t15.2025.04.13 val PER: 0.2297
2026-01-08 19:28:48,632: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_11000
2026-01-08 19:29:05,588: Train batch 11200: loss: 12.18 grad norm: 53.29 time: 0.072
2026-01-08 19:29:23,585: Train batch 11400: loss: 10.70 grad norm: 53.35 time: 0.058
2026-01-08 19:29:32,762: Running test after training batch: 11500
2026-01-08 19:29:32,878: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:29:38,059: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:29:38,107: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost cents
2026-01-08 19:29:49,384: Val batch 11500: PER (avg): 0.1674 CTC Loss (avg): 16.7686 WER(5gram): 17.47% (n=256) time: 16.622
2026-01-08 19:29:49,384: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-08 19:29:49,385: t15.2023.08.13 val PER: 0.1351
2026-01-08 19:29:49,385: t15.2023.08.18 val PER: 0.1274
2026-01-08 19:29:49,385: t15.2023.08.20 val PER: 0.1303
2026-01-08 19:29:49,385: t15.2023.08.25 val PER: 0.1130
2026-01-08 19:29:49,385: t15.2023.08.27 val PER: 0.2106
2026-01-08 19:29:49,385: t15.2023.09.01 val PER: 0.0901
2026-01-08 19:29:49,385: t15.2023.09.03 val PER: 0.1722
2026-01-08 19:29:49,385: t15.2023.09.24 val PER: 0.1311
2026-01-08 19:29:49,385: t15.2023.09.29 val PER: 0.1525
2026-01-08 19:29:49,385: t15.2023.10.01 val PER: 0.1777
2026-01-08 19:29:49,386: t15.2023.10.06 val PER: 0.1012
2026-01-08 19:29:49,386: t15.2023.10.08 val PER: 0.2476
2026-01-08 19:29:49,386: t15.2023.10.13 val PER: 0.2180
2026-01-08 19:29:49,386: t15.2023.10.15 val PER: 0.1615
2026-01-08 19:29:49,386: t15.2023.10.20 val PER: 0.1846
2026-01-08 19:29:49,386: t15.2023.10.22 val PER: 0.1392
2026-01-08 19:29:49,386: t15.2023.11.03 val PER: 0.1893
2026-01-08 19:29:49,386: t15.2023.11.04 val PER: 0.0341
2026-01-08 19:29:49,386: t15.2023.11.17 val PER: 0.0498
2026-01-08 19:29:49,386: t15.2023.11.19 val PER: 0.0519
2026-01-08 19:29:49,386: t15.2023.11.26 val PER: 0.1478
2026-01-08 19:29:49,386: t15.2023.12.03 val PER: 0.1303
2026-01-08 19:29:49,386: t15.2023.12.08 val PER: 0.1178
2026-01-08 19:29:49,386: t15.2023.12.10 val PER: 0.0999
2026-01-08 19:29:49,386: t15.2023.12.17 val PER: 0.1435
2026-01-08 19:29:49,387: t15.2023.12.29 val PER: 0.1537
2026-01-08 19:29:49,387: t15.2024.02.25 val PER: 0.1348
2026-01-08 19:29:49,387: t15.2024.03.08 val PER: 0.2418
2026-01-08 19:29:49,387: t15.2024.03.15 val PER: 0.2201
2026-01-08 19:29:49,387: t15.2024.03.17 val PER: 0.1569
2026-01-08 19:29:49,387: t15.2024.05.10 val PER: 0.1857
2026-01-08 19:29:49,387: t15.2024.06.14 val PER: 0.1877
2026-01-08 19:29:49,387: t15.2024.07.19 val PER: 0.2551
2026-01-08 19:29:49,387: t15.2024.07.21 val PER: 0.1062
2026-01-08 19:29:49,387: t15.2024.07.28 val PER: 0.1500
2026-01-08 19:29:49,387: t15.2025.01.10 val PER: 0.3196
2026-01-08 19:29:49,387: t15.2025.01.12 val PER: 0.1740
2026-01-08 19:29:49,387: t15.2025.03.14 val PER: 0.3609
2026-01-08 19:29:49,387: t15.2025.03.16 val PER: 0.2160
2026-01-08 19:29:49,387: t15.2025.03.30 val PER: 0.3103
2026-01-08 19:29:49,387: t15.2025.04.13 val PER: 0.2311
2026-01-08 19:29:49,531: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_11500
2026-01-08 19:29:58,436: Train batch 11600: loss: 11.60 grad norm: 48.09 time: 0.063
2026-01-08 19:30:16,509: Train batch 11800: loss: 7.23 grad norm: 44.05 time: 0.045
2026-01-08 19:30:34,619: Train batch 12000: loss: 14.54 grad norm: 54.98 time: 0.073
2026-01-08 19:30:34,619: Running test after training batch: 12000
2026-01-08 19:30:34,723: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:30:40,261: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:30:40,311: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost set
2026-01-08 19:30:51,475: Val batch 12000: PER (avg): 0.1657 CTC Loss (avg): 16.8859 WER(5gram): 17.08% (n=256) time: 16.856
2026-01-08 19:30:51,475: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 19:30:51,475: t15.2023.08.13 val PER: 0.1185
2026-01-08 19:30:51,475: t15.2023.08.18 val PER: 0.1182
2026-01-08 19:30:51,475: t15.2023.08.20 val PER: 0.1183
2026-01-08 19:30:51,475: t15.2023.08.25 val PER: 0.1130
2026-01-08 19:30:51,476: t15.2023.08.27 val PER: 0.1945
2026-01-08 19:30:51,476: t15.2023.09.01 val PER: 0.0820
2026-01-08 19:30:51,476: t15.2023.09.03 val PER: 0.1710
2026-01-08 19:30:51,476: t15.2023.09.24 val PER: 0.1432
2026-01-08 19:30:51,476: t15.2023.09.29 val PER: 0.1442
2026-01-08 19:30:51,476: t15.2023.10.01 val PER: 0.1744
2026-01-08 19:30:51,476: t15.2023.10.06 val PER: 0.1001
2026-01-08 19:30:51,477: t15.2023.10.08 val PER: 0.2463
2026-01-08 19:30:51,477: t15.2023.10.13 val PER: 0.2250
2026-01-08 19:30:51,477: t15.2023.10.15 val PER: 0.1668
2026-01-08 19:30:51,477: t15.2023.10.20 val PER: 0.1812
2026-01-08 19:30:51,477: t15.2023.10.22 val PER: 0.1336
2026-01-08 19:30:51,477: t15.2023.11.03 val PER: 0.1906
2026-01-08 19:30:51,477: t15.2023.11.04 val PER: 0.0410
2026-01-08 19:30:51,477: t15.2023.11.17 val PER: 0.0467
2026-01-08 19:30:51,477: t15.2023.11.19 val PER: 0.0319
2026-01-08 19:30:51,477: t15.2023.11.26 val PER: 0.1507
2026-01-08 19:30:51,478: t15.2023.12.03 val PER: 0.1397
2026-01-08 19:30:51,478: t15.2023.12.08 val PER: 0.1125
2026-01-08 19:30:51,478: t15.2023.12.10 val PER: 0.1038
2026-01-08 19:30:51,478: t15.2023.12.17 val PER: 0.1414
2026-01-08 19:30:51,478: t15.2023.12.29 val PER: 0.1503
2026-01-08 19:30:51,478: t15.2024.02.25 val PER: 0.1208
2026-01-08 19:30:51,478: t15.2024.03.08 val PER: 0.2432
2026-01-08 19:30:51,478: t15.2024.03.15 val PER: 0.2189
2026-01-08 19:30:51,478: t15.2024.03.17 val PER: 0.1548
2026-01-08 19:30:51,478: t15.2024.05.10 val PER: 0.1947
2026-01-08 19:30:51,479: t15.2024.06.14 val PER: 0.1893
2026-01-08 19:30:51,479: t15.2024.07.19 val PER: 0.2465
2026-01-08 19:30:51,479: t15.2024.07.21 val PER: 0.1097
2026-01-08 19:30:51,479: t15.2024.07.28 val PER: 0.1618
2026-01-08 19:30:51,479: t15.2025.01.10 val PER: 0.3127
2026-01-08 19:30:51,479: t15.2025.01.12 val PER: 0.1647
2026-01-08 19:30:51,479: t15.2025.03.14 val PER: 0.3550
2026-01-08 19:30:51,479: t15.2025.03.16 val PER: 0.2251
2026-01-08 19:30:51,479: t15.2025.03.30 val PER: 0.3207
2026-01-08 19:30:51,479: t15.2025.04.13 val PER: 0.2340
2026-01-08 19:30:51,616: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_12000
2026-01-08 19:31:10,004: Train batch 12200: loss: 6.88 grad norm: 42.02 time: 0.067
2026-01-08 19:31:28,055: Train batch 12400: loss: 5.07 grad norm: 34.34 time: 0.041
2026-01-08 19:31:37,058: Running test after training batch: 12500
2026-01-08 19:31:37,194: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:31:42,110: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:31:42,161: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost is
2026-01-08 19:31:53,295: Val batch 12500: PER (avg): 0.1603 CTC Loss (avg): 16.3097 WER(5gram): 15.12% (n=256) time: 16.236
2026-01-08 19:31:53,295: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-08 19:31:53,295: t15.2023.08.13 val PER: 0.1237
2026-01-08 19:31:53,295: t15.2023.08.18 val PER: 0.1140
2026-01-08 19:31:53,296: t15.2023.08.20 val PER: 0.1128
2026-01-08 19:31:53,296: t15.2023.08.25 val PER: 0.1039
2026-01-08 19:31:53,296: t15.2023.08.27 val PER: 0.1833
2026-01-08 19:31:53,296: t15.2023.09.01 val PER: 0.0860
2026-01-08 19:31:53,296: t15.2023.09.03 val PER: 0.1722
2026-01-08 19:31:53,296: t15.2023.09.24 val PER: 0.1311
2026-01-08 19:31:53,296: t15.2023.09.29 val PER: 0.1391
2026-01-08 19:31:53,296: t15.2023.10.01 val PER: 0.1724
2026-01-08 19:31:53,296: t15.2023.10.06 val PER: 0.0947
2026-01-08 19:31:53,296: t15.2023.10.08 val PER: 0.2558
2026-01-08 19:31:53,296: t15.2023.10.13 val PER: 0.2149
2026-01-08 19:31:53,296: t15.2023.10.15 val PER: 0.1648
2026-01-08 19:31:53,296: t15.2023.10.20 val PER: 0.1678
2026-01-08 19:31:53,296: t15.2023.10.22 val PER: 0.1269
2026-01-08 19:31:53,296: t15.2023.11.03 val PER: 0.1784
2026-01-08 19:31:53,297: t15.2023.11.04 val PER: 0.0307
2026-01-08 19:31:53,297: t15.2023.11.17 val PER: 0.0467
2026-01-08 19:31:53,297: t15.2023.11.19 val PER: 0.0399
2026-01-08 19:31:53,297: t15.2023.11.26 val PER: 0.1435
2026-01-08 19:31:53,297: t15.2023.12.03 val PER: 0.1229
2026-01-08 19:31:53,297: t15.2023.12.08 val PER: 0.1025
2026-01-08 19:31:53,297: t15.2023.12.10 val PER: 0.0986
2026-01-08 19:31:53,297: t15.2023.12.17 val PER: 0.1403
2026-01-08 19:31:53,297: t15.2023.12.29 val PER: 0.1434
2026-01-08 19:31:53,297: t15.2024.02.25 val PER: 0.1096
2026-01-08 19:31:53,297: t15.2024.03.08 val PER: 0.2418
2026-01-08 19:31:53,297: t15.2024.03.15 val PER: 0.2108
2026-01-08 19:31:53,297: t15.2024.03.17 val PER: 0.1548
2026-01-08 19:31:53,297: t15.2024.05.10 val PER: 0.1738
2026-01-08 19:31:53,297: t15.2024.06.14 val PER: 0.1814
2026-01-08 19:31:53,298: t15.2024.07.19 val PER: 0.2492
2026-01-08 19:31:53,298: t15.2024.07.21 val PER: 0.1014
2026-01-08 19:31:53,298: t15.2024.07.28 val PER: 0.1537
2026-01-08 19:31:53,298: t15.2025.01.10 val PER: 0.3168
2026-01-08 19:31:53,298: t15.2025.01.12 val PER: 0.1547
2026-01-08 19:31:53,298: t15.2025.03.14 val PER: 0.3565
2026-01-08 19:31:53,298: t15.2025.03.16 val PER: 0.2094
2026-01-08 19:31:53,298: t15.2025.03.30 val PER: 0.3103
2026-01-08 19:31:53,299: t15.2025.04.13 val PER: 0.2354
2026-01-08 19:31:53,300: New best val WER(5gram) 16.23% --> 15.12%
2026-01-08 19:31:53,491: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_12500
2026-01-08 19:32:02,115: Train batch 12600: loss: 7.89 grad norm: 40.49 time: 0.058
2026-01-08 19:32:19,602: Train batch 12800: loss: 6.97 grad norm: 40.74 time: 0.052
2026-01-08 19:32:37,505: Train batch 13000: loss: 7.19 grad norm: 43.22 time: 0.067
2026-01-08 19:32:37,505: Running test after training batch: 13000
2026-01-08 19:32:37,606: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:32:42,528: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:32:42,575: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 19:32:53,763: Val batch 13000: PER (avg): 0.1581 CTC Loss (avg): 16.0661 WER(5gram): 16.17% (n=256) time: 16.258
2026-01-08 19:32:53,763: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 19:32:53,763: t15.2023.08.13 val PER: 0.1164
2026-01-08 19:32:53,763: t15.2023.08.18 val PER: 0.1157
2026-01-08 19:32:53,763: t15.2023.08.20 val PER: 0.1096
2026-01-08 19:32:53,763: t15.2023.08.25 val PER: 0.0994
2026-01-08 19:32:53,764: t15.2023.08.27 val PER: 0.1961
2026-01-08 19:32:53,764: t15.2023.09.01 val PER: 0.0763
2026-01-08 19:32:53,764: t15.2023.09.03 val PER: 0.1651
2026-01-08 19:32:53,764: t15.2023.09.24 val PER: 0.1359
2026-01-08 19:32:53,764: t15.2023.09.29 val PER: 0.1334
2026-01-08 19:32:53,764: t15.2023.10.01 val PER: 0.1711
2026-01-08 19:32:53,764: t15.2023.10.06 val PER: 0.0904
2026-01-08 19:32:53,764: t15.2023.10.08 val PER: 0.2463
2026-01-08 19:32:53,764: t15.2023.10.13 val PER: 0.2188
2026-01-08 19:32:53,764: t15.2023.10.15 val PER: 0.1556
2026-01-08 19:32:53,764: t15.2023.10.20 val PER: 0.1913
2026-01-08 19:32:53,764: t15.2023.10.22 val PER: 0.1214
2026-01-08 19:32:53,764: t15.2023.11.03 val PER: 0.1825
2026-01-08 19:32:53,764: t15.2023.11.04 val PER: 0.0375
2026-01-08 19:32:53,764: t15.2023.11.17 val PER: 0.0389
2026-01-08 19:32:53,765: t15.2023.11.19 val PER: 0.0399
2026-01-08 19:32:53,765: t15.2023.11.26 val PER: 0.1355
2026-01-08 19:32:53,765: t15.2023.12.03 val PER: 0.1282
2026-01-08 19:32:53,765: t15.2023.12.08 val PER: 0.1132
2026-01-08 19:32:53,765: t15.2023.12.10 val PER: 0.1012
2026-01-08 19:32:53,765: t15.2023.12.17 val PER: 0.1414
2026-01-08 19:32:53,765: t15.2023.12.29 val PER: 0.1510
2026-01-08 19:32:53,765: t15.2024.02.25 val PER: 0.1110
2026-01-08 19:32:53,765: t15.2024.03.08 val PER: 0.2276
2026-01-08 19:32:53,765: t15.2024.03.15 val PER: 0.2070
2026-01-08 19:32:53,765: t15.2024.03.17 val PER: 0.1471
2026-01-08 19:32:53,765: t15.2024.05.10 val PER: 0.1783
2026-01-08 19:32:53,765: t15.2024.06.14 val PER: 0.1877
2026-01-08 19:32:53,765: t15.2024.07.19 val PER: 0.2498
2026-01-08 19:32:53,765: t15.2024.07.21 val PER: 0.0959
2026-01-08 19:32:53,766: t15.2024.07.28 val PER: 0.1529
2026-01-08 19:32:53,766: t15.2025.01.10 val PER: 0.3003
2026-01-08 19:32:53,766: t15.2025.01.12 val PER: 0.1540
2026-01-08 19:32:53,766: t15.2025.03.14 val PER: 0.3491
2026-01-08 19:32:53,766: t15.2025.03.16 val PER: 0.2003
2026-01-08 19:32:53,766: t15.2025.03.30 val PER: 0.2943
2026-01-08 19:32:53,766: t15.2025.04.13 val PER: 0.2297
2026-01-08 19:32:53,905: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_13000
2026-01-08 19:33:11,157: Train batch 13200: loss: 14.43 grad norm: 62.20 time: 0.055
2026-01-08 19:33:28,672: Train batch 13400: loss: 9.77 grad norm: 53.57 time: 0.063
2026-01-08 19:33:37,259: Running test after training batch: 13500
2026-01-08 19:33:37,358: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:33:42,314: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:33:42,364: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 19:33:53,827: Val batch 13500: PER (avg): 0.1571 CTC Loss (avg): 16.0260 WER(5gram): 15.58% (n=256) time: 16.568
2026-01-08 19:33:53,827: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 19:33:53,828: t15.2023.08.13 val PER: 0.1175
2026-01-08 19:33:53,828: t15.2023.08.18 val PER: 0.1090
2026-01-08 19:33:53,828: t15.2023.08.20 val PER: 0.1136
2026-01-08 19:33:53,828: t15.2023.08.25 val PER: 0.0949
2026-01-08 19:33:53,828: t15.2023.08.27 val PER: 0.1865
2026-01-08 19:33:53,828: t15.2023.09.01 val PER: 0.0787
2026-01-08 19:33:53,828: t15.2023.09.03 val PER: 0.1615
2026-01-08 19:33:53,828: t15.2023.09.24 val PER: 0.1323
2026-01-08 19:33:53,828: t15.2023.09.29 val PER: 0.1378
2026-01-08 19:33:53,828: t15.2023.10.01 val PER: 0.1783
2026-01-08 19:33:53,828: t15.2023.10.06 val PER: 0.0980
2026-01-08 19:33:53,829: t15.2023.10.08 val PER: 0.2422
2026-01-08 19:33:53,829: t15.2023.10.13 val PER: 0.2126
2026-01-08 19:33:53,829: t15.2023.10.15 val PER: 0.1635
2026-01-08 19:33:53,829: t15.2023.10.20 val PER: 0.1745
2026-01-08 19:33:53,829: t15.2023.10.22 val PER: 0.1214
2026-01-08 19:33:53,829: t15.2023.11.03 val PER: 0.1832
2026-01-08 19:33:53,829: t15.2023.11.04 val PER: 0.0341
2026-01-08 19:33:53,829: t15.2023.11.17 val PER: 0.0498
2026-01-08 19:33:53,829: t15.2023.11.19 val PER: 0.0339
2026-01-08 19:33:53,829: t15.2023.11.26 val PER: 0.1377
2026-01-08 19:33:53,829: t15.2023.12.03 val PER: 0.1218
2026-01-08 19:33:53,830: t15.2023.12.08 val PER: 0.1112
2026-01-08 19:33:53,830: t15.2023.12.10 val PER: 0.1025
2026-01-08 19:33:53,830: t15.2023.12.17 val PER: 0.1351
2026-01-08 19:33:53,830: t15.2023.12.29 val PER: 0.1359
2026-01-08 19:33:53,830: t15.2024.02.25 val PER: 0.1096
2026-01-08 19:33:53,830: t15.2024.03.08 val PER: 0.2432
2026-01-08 19:33:53,830: t15.2024.03.15 val PER: 0.2039
2026-01-08 19:33:53,830: t15.2024.03.17 val PER: 0.1395
2026-01-08 19:33:53,830: t15.2024.05.10 val PER: 0.1620
2026-01-08 19:33:53,830: t15.2024.06.14 val PER: 0.1703
2026-01-08 19:33:53,830: t15.2024.07.19 val PER: 0.2459
2026-01-08 19:33:53,831: t15.2024.07.21 val PER: 0.0966
2026-01-08 19:33:53,831: t15.2024.07.28 val PER: 0.1493
2026-01-08 19:33:53,831: t15.2025.01.10 val PER: 0.3168
2026-01-08 19:33:53,831: t15.2025.01.12 val PER: 0.1493
2026-01-08 19:33:53,831: t15.2025.03.14 val PER: 0.3536
2026-01-08 19:33:53,831: t15.2025.03.16 val PER: 0.1990
2026-01-08 19:33:53,831: t15.2025.03.30 val PER: 0.3023
2026-01-08 19:33:53,831: t15.2025.04.13 val PER: 0.2311
2026-01-08 19:33:53,969: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_13500
2026-01-08 19:34:02,878: Train batch 13600: loss: 13.45 grad norm: 59.92 time: 0.062
2026-01-08 19:34:20,263: Train batch 13800: loss: 10.17 grad norm: 58.22 time: 0.059
2026-01-08 19:34:37,910: Train batch 14000: loss: 13.07 grad norm: 60.41 time: 0.052
2026-01-08 19:34:37,910: Running test after training batch: 14000
2026-01-08 19:34:38,022: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:34:42,944: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:34:42,994: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost set
2026-01-08 19:34:54,442: Val batch 14000: PER (avg): 0.1565 CTC Loss (avg): 15.9935 WER(5gram): 15.71% (n=256) time: 16.531
2026-01-08 19:34:54,442: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 19:34:54,443: t15.2023.08.13 val PER: 0.1164
2026-01-08 19:34:54,443: t15.2023.08.18 val PER: 0.1081
2026-01-08 19:34:54,443: t15.2023.08.20 val PER: 0.1104
2026-01-08 19:34:54,443: t15.2023.08.25 val PER: 0.1024
2026-01-08 19:34:54,443: t15.2023.08.27 val PER: 0.1849
2026-01-08 19:34:54,443: t15.2023.09.01 val PER: 0.0755
2026-01-08 19:34:54,443: t15.2023.09.03 val PER: 0.1591
2026-01-08 19:34:54,443: t15.2023.09.24 val PER: 0.1274
2026-01-08 19:34:54,443: t15.2023.09.29 val PER: 0.1378
2026-01-08 19:34:54,443: t15.2023.10.01 val PER: 0.1744
2026-01-08 19:34:54,443: t15.2023.10.06 val PER: 0.0936
2026-01-08 19:34:54,444: t15.2023.10.08 val PER: 0.2368
2026-01-08 19:34:54,444: t15.2023.10.13 val PER: 0.2126
2026-01-08 19:34:54,444: t15.2023.10.15 val PER: 0.1602
2026-01-08 19:34:54,444: t15.2023.10.20 val PER: 0.1812
2026-01-08 19:34:54,444: t15.2023.10.22 val PER: 0.1180
2026-01-08 19:34:54,444: t15.2023.11.03 val PER: 0.1872
2026-01-08 19:34:54,444: t15.2023.11.04 val PER: 0.0341
2026-01-08 19:34:54,444: t15.2023.11.17 val PER: 0.0451
2026-01-08 19:34:54,444: t15.2023.11.19 val PER: 0.0359
2026-01-08 19:34:54,444: t15.2023.11.26 val PER: 0.1297
2026-01-08 19:34:54,444: t15.2023.12.03 val PER: 0.1176
2026-01-08 19:34:54,444: t15.2023.12.08 val PER: 0.1112
2026-01-08 19:34:54,444: t15.2023.12.10 val PER: 0.1064
2026-01-08 19:34:54,444: t15.2023.12.17 val PER: 0.1331
2026-01-08 19:34:54,445: t15.2023.12.29 val PER: 0.1366
2026-01-08 19:34:54,445: t15.2024.02.25 val PER: 0.1236
2026-01-08 19:34:54,445: t15.2024.03.08 val PER: 0.2432
2026-01-08 19:34:54,445: t15.2024.03.15 val PER: 0.2045
2026-01-08 19:34:54,445: t15.2024.03.17 val PER: 0.1471
2026-01-08 19:34:54,445: t15.2024.05.10 val PER: 0.1634
2026-01-08 19:34:54,445: t15.2024.06.14 val PER: 0.1719
2026-01-08 19:34:54,445: t15.2024.07.19 val PER: 0.2498
2026-01-08 19:34:54,445: t15.2024.07.21 val PER: 0.1000
2026-01-08 19:34:54,445: t15.2024.07.28 val PER: 0.1500
2026-01-08 19:34:54,445: t15.2025.01.10 val PER: 0.3030
2026-01-08 19:34:54,445: t15.2025.01.12 val PER: 0.1447
2026-01-08 19:34:54,445: t15.2025.03.14 val PER: 0.3373
2026-01-08 19:34:54,445: t15.2025.03.16 val PER: 0.2107
2026-01-08 19:34:54,445: t15.2025.03.30 val PER: 0.3023
2026-01-08 19:34:54,445: t15.2025.04.13 val PER: 0.2297
2026-01-08 19:34:54,580: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_14000
2026-01-08 19:35:11,439: Train batch 14200: loss: 8.98 grad norm: 53.44 time: 0.059
2026-01-08 19:35:29,327: Train batch 14400: loss: 6.42 grad norm: 40.87 time: 0.065
2026-01-08 19:35:38,115: Running test after training batch: 14500
2026-01-08 19:35:38,219: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:35:43,178: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:35:43,225: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost set
2026-01-08 19:35:54,291: Val batch 14500: PER (avg): 0.1545 CTC Loss (avg): 15.8866 WER(5gram): 16.56% (n=256) time: 16.175
2026-01-08 19:35:54,291: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-08 19:35:54,292: t15.2023.08.13 val PER: 0.1206
2026-01-08 19:35:54,292: t15.2023.08.18 val PER: 0.1073
2026-01-08 19:35:54,292: t15.2023.08.20 val PER: 0.1104
2026-01-08 19:35:54,292: t15.2023.08.25 val PER: 0.0964
2026-01-08 19:35:54,292: t15.2023.08.27 val PER: 0.1817
2026-01-08 19:35:54,292: t15.2023.09.01 val PER: 0.0787
2026-01-08 19:35:54,293: t15.2023.09.03 val PER: 0.1710
2026-01-08 19:35:54,293: t15.2023.09.24 val PER: 0.1226
2026-01-08 19:35:54,293: t15.2023.09.29 val PER: 0.1321
2026-01-08 19:35:54,293: t15.2023.10.01 val PER: 0.1671
2026-01-08 19:35:54,293: t15.2023.10.06 val PER: 0.0861
2026-01-08 19:35:54,293: t15.2023.10.08 val PER: 0.2463
2026-01-08 19:35:54,293: t15.2023.10.13 val PER: 0.2102
2026-01-08 19:35:54,293: t15.2023.10.15 val PER: 0.1549
2026-01-08 19:35:54,293: t15.2023.10.20 val PER: 0.1745
2026-01-08 19:35:54,293: t15.2023.10.22 val PER: 0.1214
2026-01-08 19:35:54,293: t15.2023.11.03 val PER: 0.1798
2026-01-08 19:35:54,293: t15.2023.11.04 val PER: 0.0307
2026-01-08 19:35:54,293: t15.2023.11.17 val PER: 0.0420
2026-01-08 19:35:54,293: t15.2023.11.19 val PER: 0.0379
2026-01-08 19:35:54,293: t15.2023.11.26 val PER: 0.1275
2026-01-08 19:35:54,294: t15.2023.12.03 val PER: 0.1113
2026-01-08 19:35:54,294: t15.2023.12.08 val PER: 0.0999
2026-01-08 19:35:54,294: t15.2023.12.10 val PER: 0.0907
2026-01-08 19:35:54,294: t15.2023.12.17 val PER: 0.1331
2026-01-08 19:35:54,294: t15.2023.12.29 val PER: 0.1345
2026-01-08 19:35:54,294: t15.2024.02.25 val PER: 0.1138
2026-01-08 19:35:54,295: t15.2024.03.08 val PER: 0.2376
2026-01-08 19:35:54,295: t15.2024.03.15 val PER: 0.2051
2026-01-08 19:35:54,295: t15.2024.03.17 val PER: 0.1464
2026-01-08 19:35:54,295: t15.2024.05.10 val PER: 0.1441
2026-01-08 19:35:54,295: t15.2024.06.14 val PER: 0.1893
2026-01-08 19:35:54,295: t15.2024.07.19 val PER: 0.2505
2026-01-08 19:35:54,295: t15.2024.07.21 val PER: 0.0952
2026-01-08 19:35:54,295: t15.2024.07.28 val PER: 0.1551
2026-01-08 19:35:54,295: t15.2025.01.10 val PER: 0.3017
2026-01-08 19:35:54,295: t15.2025.01.12 val PER: 0.1501
2026-01-08 19:35:54,295: t15.2025.03.14 val PER: 0.3536
2026-01-08 19:35:54,295: t15.2025.03.16 val PER: 0.1990
2026-01-08 19:35:54,295: t15.2025.03.30 val PER: 0.3000
2026-01-08 19:35:54,295: t15.2025.04.13 val PER: 0.2297
2026-01-08 19:35:54,434: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_14500
2026-01-08 19:36:02,908: Train batch 14600: loss: 14.30 grad norm: 63.90 time: 0.059
2026-01-08 19:36:20,658: Train batch 14800: loss: 6.10 grad norm: 41.94 time: 0.051
2026-01-08 19:36:38,557: Train batch 15000: loss: 9.39 grad norm: 51.94 time: 0.052
2026-01-08 19:36:38,557: Running test after training batch: 15000
2026-01-08 19:36:38,651: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:36:43,598: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:36:43,644: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost set
2026-01-08 19:36:55,280: Val batch 15000: PER (avg): 0.1525 CTC Loss (avg): 15.6336 WER(5gram): 15.78% (n=256) time: 16.723
2026-01-08 19:36:55,280: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 19:36:55,281: t15.2023.08.13 val PER: 0.1133
2026-01-08 19:36:55,281: t15.2023.08.18 val PER: 0.1132
2026-01-08 19:36:55,281: t15.2023.08.20 val PER: 0.1183
2026-01-08 19:36:55,281: t15.2023.08.25 val PER: 0.0994
2026-01-08 19:36:55,281: t15.2023.08.27 val PER: 0.1817
2026-01-08 19:36:55,281: t15.2023.09.01 val PER: 0.0787
2026-01-08 19:36:55,281: t15.2023.09.03 val PER: 0.1615
2026-01-08 19:36:55,281: t15.2023.09.24 val PER: 0.1250
2026-01-08 19:36:55,282: t15.2023.09.29 val PER: 0.1398
2026-01-08 19:36:55,282: t15.2023.10.01 val PER: 0.1638
2026-01-08 19:36:55,282: t15.2023.10.06 val PER: 0.0915
2026-01-08 19:36:55,282: t15.2023.10.08 val PER: 0.2355
2026-01-08 19:36:55,282: t15.2023.10.13 val PER: 0.2056
2026-01-08 19:36:55,282: t15.2023.10.15 val PER: 0.1589
2026-01-08 19:36:55,282: t15.2023.10.20 val PER: 0.1745
2026-01-08 19:36:55,282: t15.2023.10.22 val PER: 0.1236
2026-01-08 19:36:55,282: t15.2023.11.03 val PER: 0.1791
2026-01-08 19:36:55,282: t15.2023.11.04 val PER: 0.0341
2026-01-08 19:36:55,282: t15.2023.11.17 val PER: 0.0420
2026-01-08 19:36:55,282: t15.2023.11.19 val PER: 0.0339
2026-01-08 19:36:55,282: t15.2023.11.26 val PER: 0.1239
2026-01-08 19:36:55,282: t15.2023.12.03 val PER: 0.1155
2026-01-08 19:36:55,282: t15.2023.12.08 val PER: 0.1012
2026-01-08 19:36:55,283: t15.2023.12.10 val PER: 0.0894
2026-01-08 19:36:55,283: t15.2023.12.17 val PER: 0.1351
2026-01-08 19:36:55,283: t15.2023.12.29 val PER: 0.1428
2026-01-08 19:36:55,283: t15.2024.02.25 val PER: 0.1096
2026-01-08 19:36:55,283: t15.2024.03.08 val PER: 0.2219
2026-01-08 19:36:55,283: t15.2024.03.15 val PER: 0.2058
2026-01-08 19:36:55,283: t15.2024.03.17 val PER: 0.1374
2026-01-08 19:36:55,283: t15.2024.05.10 val PER: 0.1605
2026-01-08 19:36:55,283: t15.2024.06.14 val PER: 0.1703
2026-01-08 19:36:55,283: t15.2024.07.19 val PER: 0.2413
2026-01-08 19:36:55,283: t15.2024.07.21 val PER: 0.0972
2026-01-08 19:36:55,283: t15.2024.07.28 val PER: 0.1441
2026-01-08 19:36:55,284: t15.2025.01.10 val PER: 0.3072
2026-01-08 19:36:55,284: t15.2025.01.12 val PER: 0.1409
2026-01-08 19:36:55,284: t15.2025.03.14 val PER: 0.3299
2026-01-08 19:36:55,284: t15.2025.03.16 val PER: 0.1859
2026-01-08 19:36:55,284: t15.2025.03.30 val PER: 0.2851
2026-01-08 19:36:55,284: t15.2025.04.13 val PER: 0.2197
2026-01-08 19:36:55,421: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_15000
2026-01-08 19:37:13,340: Train batch 15200: loss: 5.66 grad norm: 39.43 time: 0.060
2026-01-08 19:37:31,471: Train batch 15400: loss: 11.98 grad norm: 57.91 time: 0.051
2026-01-08 19:37:40,755: Running test after training batch: 15500
2026-01-08 19:37:40,863: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:37:45,903: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:37:45,950: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost cents
2026-01-08 19:37:57,385: Val batch 15500: PER (avg): 0.1509 CTC Loss (avg): 15.6079 WER(5gram): 15.25% (n=256) time: 16.630
2026-01-08 19:37:57,386: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 19:37:57,386: t15.2023.08.13 val PER: 0.1133
2026-01-08 19:37:57,386: t15.2023.08.18 val PER: 0.1048
2026-01-08 19:37:57,386: t15.2023.08.20 val PER: 0.1160
2026-01-08 19:37:57,386: t15.2023.08.25 val PER: 0.1009
2026-01-08 19:37:57,386: t15.2023.08.27 val PER: 0.1929
2026-01-08 19:37:57,387: t15.2023.09.01 val PER: 0.0779
2026-01-08 19:37:57,387: t15.2023.09.03 val PER: 0.1698
2026-01-08 19:37:57,387: t15.2023.09.24 val PER: 0.1262
2026-01-08 19:37:57,387: t15.2023.09.29 val PER: 0.1308
2026-01-08 19:37:57,387: t15.2023.10.01 val PER: 0.1664
2026-01-08 19:37:57,387: t15.2023.10.06 val PER: 0.0818
2026-01-08 19:37:57,387: t15.2023.10.08 val PER: 0.2382
2026-01-08 19:37:57,387: t15.2023.10.13 val PER: 0.2033
2026-01-08 19:37:57,387: t15.2023.10.15 val PER: 0.1470
2026-01-08 19:37:57,387: t15.2023.10.20 val PER: 0.1745
2026-01-08 19:37:57,387: t15.2023.10.22 val PER: 0.1236
2026-01-08 19:37:57,388: t15.2023.11.03 val PER: 0.1771
2026-01-08 19:37:57,388: t15.2023.11.04 val PER: 0.0341
2026-01-08 19:37:57,388: t15.2023.11.17 val PER: 0.0342
2026-01-08 19:37:57,388: t15.2023.11.19 val PER: 0.0339
2026-01-08 19:37:57,388: t15.2023.11.26 val PER: 0.1232
2026-01-08 19:37:57,388: t15.2023.12.03 val PER: 0.1082
2026-01-08 19:37:57,388: t15.2023.12.08 val PER: 0.0992
2026-01-08 19:37:57,388: t15.2023.12.10 val PER: 0.0933
2026-01-08 19:37:57,388: t15.2023.12.17 val PER: 0.1258
2026-01-08 19:37:57,388: t15.2023.12.29 val PER: 0.1332
2026-01-08 19:37:57,388: t15.2024.02.25 val PER: 0.1152
2026-01-08 19:37:57,388: t15.2024.03.08 val PER: 0.2290
2026-01-08 19:37:57,388: t15.2024.03.15 val PER: 0.1970
2026-01-08 19:37:57,389: t15.2024.03.17 val PER: 0.1450
2026-01-08 19:37:57,389: t15.2024.05.10 val PER: 0.1516
2026-01-08 19:37:57,389: t15.2024.06.14 val PER: 0.1609
2026-01-08 19:37:57,389: t15.2024.07.19 val PER: 0.2340
2026-01-08 19:37:57,389: t15.2024.07.21 val PER: 0.0966
2026-01-08 19:37:57,389: t15.2024.07.28 val PER: 0.1412
2026-01-08 19:37:57,389: t15.2025.01.10 val PER: 0.2906
2026-01-08 19:37:57,389: t15.2025.01.12 val PER: 0.1486
2026-01-08 19:37:57,389: t15.2025.03.14 val PER: 0.3432
2026-01-08 19:37:57,389: t15.2025.03.16 val PER: 0.1976
2026-01-08 19:37:57,389: t15.2025.03.30 val PER: 0.3069
2026-01-08 19:37:57,389: t15.2025.04.13 val PER: 0.2111
2026-01-08 19:37:57,529: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_15500
2026-01-08 19:38:06,026: Train batch 15600: loss: 12.86 grad norm: 53.10 time: 0.062
2026-01-08 19:38:23,083: Train batch 15800: loss: 13.97 grad norm: 58.76 time: 0.067
2026-01-08 19:38:40,341: Train batch 16000: loss: 9.32 grad norm: 48.35 time: 0.056
2026-01-08 19:38:40,341: Running test after training batch: 16000
2026-01-08 19:38:40,478: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:38:45,440: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:38:45,495: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost set
2026-01-08 19:38:57,200: Val batch 16000: PER (avg): 0.1519 CTC Loss (avg): 15.5750 WER(5gram): 15.19% (n=256) time: 16.859
2026-01-08 19:38:57,201: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 19:38:57,201: t15.2023.08.13 val PER: 0.1112
2026-01-08 19:38:57,201: t15.2023.08.18 val PER: 0.1014
2026-01-08 19:38:57,201: t15.2023.08.20 val PER: 0.1168
2026-01-08 19:38:57,201: t15.2023.08.25 val PER: 0.1024
2026-01-08 19:38:57,201: t15.2023.08.27 val PER: 0.1865
2026-01-08 19:38:57,201: t15.2023.09.01 val PER: 0.0739
2026-01-08 19:38:57,201: t15.2023.09.03 val PER: 0.1627
2026-01-08 19:38:57,201: t15.2023.09.24 val PER: 0.1262
2026-01-08 19:38:57,201: t15.2023.09.29 val PER: 0.1410
2026-01-08 19:38:57,201: t15.2023.10.01 val PER: 0.1658
2026-01-08 19:38:57,201: t15.2023.10.06 val PER: 0.0840
2026-01-08 19:38:57,202: t15.2023.10.08 val PER: 0.2382
2026-01-08 19:38:57,202: t15.2023.10.13 val PER: 0.2071
2026-01-08 19:38:57,202: t15.2023.10.15 val PER: 0.1490
2026-01-08 19:38:57,202: t15.2023.10.20 val PER: 0.1745
2026-01-08 19:38:57,202: t15.2023.10.22 val PER: 0.1203
2026-01-08 19:38:57,202: t15.2023.11.03 val PER: 0.1764
2026-01-08 19:38:57,202: t15.2023.11.04 val PER: 0.0341
2026-01-08 19:38:57,202: t15.2023.11.17 val PER: 0.0420
2026-01-08 19:38:57,203: t15.2023.11.19 val PER: 0.0299
2026-01-08 19:38:57,203: t15.2023.11.26 val PER: 0.1217
2026-01-08 19:38:57,203: t15.2023.12.03 val PER: 0.1176
2026-01-08 19:38:57,203: t15.2023.12.08 val PER: 0.0965
2026-01-08 19:38:57,203: t15.2023.12.10 val PER: 0.0920
2026-01-08 19:38:57,203: t15.2023.12.17 val PER: 0.1351
2026-01-08 19:38:57,203: t15.2023.12.29 val PER: 0.1338
2026-01-08 19:38:57,203: t15.2024.02.25 val PER: 0.1138
2026-01-08 19:38:57,203: t15.2024.03.08 val PER: 0.2404
2026-01-08 19:38:57,203: t15.2024.03.15 val PER: 0.2058
2026-01-08 19:38:57,203: t15.2024.03.17 val PER: 0.1430
2026-01-08 19:38:57,203: t15.2024.05.10 val PER: 0.1634
2026-01-08 19:38:57,203: t15.2024.06.14 val PER: 0.1609
2026-01-08 19:38:57,203: t15.2024.07.19 val PER: 0.2419
2026-01-08 19:38:57,203: t15.2024.07.21 val PER: 0.0910
2026-01-08 19:38:57,204: t15.2024.07.28 val PER: 0.1441
2026-01-08 19:38:57,204: t15.2025.01.10 val PER: 0.2948
2026-01-08 19:38:57,204: t15.2025.01.12 val PER: 0.1393
2026-01-08 19:38:57,204: t15.2025.03.14 val PER: 0.3447
2026-01-08 19:38:57,204: t15.2025.03.16 val PER: 0.1976
2026-01-08 19:38:57,204: t15.2025.03.30 val PER: 0.3011
2026-01-08 19:38:57,204: t15.2025.04.13 val PER: 0.2211
2026-01-08 19:38:57,341: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_16000
2026-01-08 19:39:15,004: Train batch 16200: loss: 6.92 grad norm: 41.64 time: 0.056
2026-01-08 19:39:32,851: Train batch 16400: loss: 12.03 grad norm: 63.89 time: 0.059
2026-01-08 19:39:41,905: Running test after training batch: 16500
2026-01-08 19:39:42,028: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:39:47,030: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:39:47,080: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost cents
2026-01-08 19:39:58,636: Val batch 16500: PER (avg): 0.1505 CTC Loss (avg): 15.5869 WER(5gram): 15.45% (n=256) time: 16.730
2026-01-08 19:39:58,636: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 19:39:58,636: t15.2023.08.13 val PER: 0.1091
2026-01-08 19:39:58,636: t15.2023.08.18 val PER: 0.1014
2026-01-08 19:39:58,636: t15.2023.08.20 val PER: 0.1144
2026-01-08 19:39:58,637: t15.2023.08.25 val PER: 0.0979
2026-01-08 19:39:58,637: t15.2023.08.27 val PER: 0.1961
2026-01-08 19:39:58,637: t15.2023.09.01 val PER: 0.0690
2026-01-08 19:39:58,637: t15.2023.09.03 val PER: 0.1663
2026-01-08 19:39:58,637: t15.2023.09.24 val PER: 0.1226
2026-01-08 19:39:58,637: t15.2023.09.29 val PER: 0.1321
2026-01-08 19:39:58,637: t15.2023.10.01 val PER: 0.1684
2026-01-08 19:39:58,637: t15.2023.10.06 val PER: 0.0872
2026-01-08 19:39:58,637: t15.2023.10.08 val PER: 0.2300
2026-01-08 19:39:58,637: t15.2023.10.13 val PER: 0.2064
2026-01-08 19:39:58,637: t15.2023.10.15 val PER: 0.1543
2026-01-08 19:39:58,637: t15.2023.10.20 val PER: 0.1678
2026-01-08 19:39:58,637: t15.2023.10.22 val PER: 0.1136
2026-01-08 19:39:58,637: t15.2023.11.03 val PER: 0.1866
2026-01-08 19:39:58,637: t15.2023.11.04 val PER: 0.0341
2026-01-08 19:39:58,638: t15.2023.11.17 val PER: 0.0373
2026-01-08 19:39:58,638: t15.2023.11.19 val PER: 0.0319
2026-01-08 19:39:58,638: t15.2023.11.26 val PER: 0.1268
2026-01-08 19:39:58,638: t15.2023.12.03 val PER: 0.1082
2026-01-08 19:39:58,638: t15.2023.12.08 val PER: 0.0932
2026-01-08 19:39:58,638: t15.2023.12.10 val PER: 0.0854
2026-01-08 19:39:58,638: t15.2023.12.17 val PER: 0.1247
2026-01-08 19:39:58,638: t15.2023.12.29 val PER: 0.1373
2026-01-08 19:39:58,638: t15.2024.02.25 val PER: 0.1194
2026-01-08 19:39:58,638: t15.2024.03.08 val PER: 0.2333
2026-01-08 19:39:58,638: t15.2024.03.15 val PER: 0.1982
2026-01-08 19:39:58,639: t15.2024.03.17 val PER: 0.1402
2026-01-08 19:39:58,639: t15.2024.05.10 val PER: 0.1471
2026-01-08 19:39:58,639: t15.2024.06.14 val PER: 0.1640
2026-01-08 19:39:58,639: t15.2024.07.19 val PER: 0.2413
2026-01-08 19:39:58,639: t15.2024.07.21 val PER: 0.0903
2026-01-08 19:39:58,639: t15.2024.07.28 val PER: 0.1390
2026-01-08 19:39:58,639: t15.2025.01.10 val PER: 0.3030
2026-01-08 19:39:58,639: t15.2025.01.12 val PER: 0.1416
2026-01-08 19:39:58,639: t15.2025.03.14 val PER: 0.3388
2026-01-08 19:39:58,639: t15.2025.03.16 val PER: 0.1950
2026-01-08 19:39:58,639: t15.2025.03.30 val PER: 0.3034
2026-01-08 19:39:58,639: t15.2025.04.13 val PER: 0.2168
2026-01-08 19:39:58,778: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_16500
2026-01-08 19:40:07,286: Train batch 16600: loss: 9.32 grad norm: 52.84 time: 0.053
2026-01-08 19:40:25,341: Train batch 16800: loss: 17.94 grad norm: 72.80 time: 0.062
2026-01-08 19:40:43,362: Train batch 17000: loss: 9.06 grad norm: 48.49 time: 0.083
2026-01-08 19:40:43,362: Running test after training batch: 17000
2026-01-08 19:40:43,462: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:40:48,397: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:40:48,445: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost set
2026-01-08 19:41:00,113: Val batch 17000: PER (avg): 0.1497 CTC Loss (avg): 15.4343 WER(5gram): 16.04% (n=256) time: 16.750
2026-01-08 19:41:00,113: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 19:41:00,113: t15.2023.08.13 val PER: 0.1071
2026-01-08 19:41:00,113: t15.2023.08.18 val PER: 0.1115
2026-01-08 19:41:00,113: t15.2023.08.20 val PER: 0.1096
2026-01-08 19:41:00,114: t15.2023.08.25 val PER: 0.0964
2026-01-08 19:41:00,114: t15.2023.08.27 val PER: 0.1881
2026-01-08 19:41:00,114: t15.2023.09.01 val PER: 0.0714
2026-01-08 19:41:00,114: t15.2023.09.03 val PER: 0.1568
2026-01-08 19:41:00,114: t15.2023.09.24 val PER: 0.1262
2026-01-08 19:41:00,114: t15.2023.09.29 val PER: 0.1327
2026-01-08 19:41:00,114: t15.2023.10.01 val PER: 0.1651
2026-01-08 19:41:00,114: t15.2023.10.06 val PER: 0.0893
2026-01-08 19:41:00,114: t15.2023.10.08 val PER: 0.2341
2026-01-08 19:41:00,114: t15.2023.10.13 val PER: 0.2048
2026-01-08 19:41:00,114: t15.2023.10.15 val PER: 0.1595
2026-01-08 19:41:00,114: t15.2023.10.20 val PER: 0.1745
2026-01-08 19:41:00,114: t15.2023.10.22 val PER: 0.1203
2026-01-08 19:41:00,114: t15.2023.11.03 val PER: 0.1798
2026-01-08 19:41:00,114: t15.2023.11.04 val PER: 0.0307
2026-01-08 19:41:00,115: t15.2023.11.17 val PER: 0.0389
2026-01-08 19:41:00,115: t15.2023.11.19 val PER: 0.0359
2026-01-08 19:41:00,115: t15.2023.11.26 val PER: 0.1181
2026-01-08 19:41:00,115: t15.2023.12.03 val PER: 0.1050
2026-01-08 19:41:00,115: t15.2023.12.08 val PER: 0.0919
2026-01-08 19:41:00,115: t15.2023.12.10 val PER: 0.0894
2026-01-08 19:41:00,115: t15.2023.12.17 val PER: 0.1237
2026-01-08 19:41:00,115: t15.2023.12.29 val PER: 0.1345
2026-01-08 19:41:00,115: t15.2024.02.25 val PER: 0.1081
2026-01-08 19:41:00,115: t15.2024.03.08 val PER: 0.2219
2026-01-08 19:41:00,115: t15.2024.03.15 val PER: 0.2033
2026-01-08 19:41:00,115: t15.2024.03.17 val PER: 0.1395
2026-01-08 19:41:00,116: t15.2024.05.10 val PER: 0.1575
2026-01-08 19:41:00,116: t15.2024.06.14 val PER: 0.1546
2026-01-08 19:41:00,116: t15.2024.07.19 val PER: 0.2360
2026-01-08 19:41:00,116: t15.2024.07.21 val PER: 0.0924
2026-01-08 19:41:00,116: t15.2024.07.28 val PER: 0.1449
2026-01-08 19:41:00,116: t15.2025.01.10 val PER: 0.3003
2026-01-08 19:41:00,116: t15.2025.01.12 val PER: 0.1386
2026-01-08 19:41:00,116: t15.2025.03.14 val PER: 0.3314
2026-01-08 19:41:00,116: t15.2025.03.16 val PER: 0.2016
2026-01-08 19:41:00,116: t15.2025.03.30 val PER: 0.2908
2026-01-08 19:41:00,116: t15.2025.04.13 val PER: 0.2240
2026-01-08 19:41:00,252: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_17000
2026-01-08 19:41:17,536: Train batch 17200: loss: 10.77 grad norm: 49.60 time: 0.085
2026-01-08 19:41:34,732: Train batch 17400: loss: 13.51 grad norm: 58.81 time: 0.071
2026-01-08 19:41:43,204: Running test after training batch: 17500
2026-01-08 19:41:43,346: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:41:48,332: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:41:48,388: WER debug example
  GT : how does it keep the cost down
  PR : how dusty it keep the cost cents
2026-01-08 19:42:00,143: Val batch 17500: PER (avg): 0.1503 CTC Loss (avg): 15.4713 WER(5gram): 15.65% (n=256) time: 16.939
2026-01-08 19:42:00,144: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 19:42:00,144: t15.2023.08.13 val PER: 0.1071
2026-01-08 19:42:00,144: t15.2023.08.18 val PER: 0.0972
2026-01-08 19:42:00,145: t15.2023.08.20 val PER: 0.1128
2026-01-08 19:42:00,145: t15.2023.08.25 val PER: 0.0949
2026-01-08 19:42:00,145: t15.2023.08.27 val PER: 0.1945
2026-01-08 19:42:00,145: t15.2023.09.01 val PER: 0.0714
2026-01-08 19:42:00,145: t15.2023.09.03 val PER: 0.1627
2026-01-08 19:42:00,145: t15.2023.09.24 val PER: 0.1226
2026-01-08 19:42:00,145: t15.2023.09.29 val PER: 0.1353
2026-01-08 19:42:00,145: t15.2023.10.01 val PER: 0.1671
2026-01-08 19:42:00,145: t15.2023.10.06 val PER: 0.0872
2026-01-08 19:42:00,145: t15.2023.10.08 val PER: 0.2395
2026-01-08 19:42:00,146: t15.2023.10.13 val PER: 0.2087
2026-01-08 19:42:00,146: t15.2023.10.15 val PER: 0.1569
2026-01-08 19:42:00,146: t15.2023.10.20 val PER: 0.1644
2026-01-08 19:42:00,146: t15.2023.10.22 val PER: 0.1180
2026-01-08 19:42:00,146: t15.2023.11.03 val PER: 0.1845
2026-01-08 19:42:00,146: t15.2023.11.04 val PER: 0.0341
2026-01-08 19:42:00,146: t15.2023.11.17 val PER: 0.0389
2026-01-08 19:42:00,146: t15.2023.11.19 val PER: 0.0279
2026-01-08 19:42:00,146: t15.2023.11.26 val PER: 0.1188
2026-01-08 19:42:00,146: t15.2023.12.03 val PER: 0.1092
2026-01-08 19:42:00,146: t15.2023.12.08 val PER: 0.0959
2026-01-08 19:42:00,147: t15.2023.12.10 val PER: 0.0933
2026-01-08 19:42:00,147: t15.2023.12.17 val PER: 0.1258
2026-01-08 19:42:00,147: t15.2023.12.29 val PER: 0.1359
2026-01-08 19:42:00,147: t15.2024.02.25 val PER: 0.1096
2026-01-08 19:42:00,147: t15.2024.03.08 val PER: 0.2347
2026-01-08 19:42:00,147: t15.2024.03.15 val PER: 0.2020
2026-01-08 19:42:00,147: t15.2024.03.17 val PER: 0.1353
2026-01-08 19:42:00,147: t15.2024.05.10 val PER: 0.1590
2026-01-08 19:42:00,147: t15.2024.06.14 val PER: 0.1656
2026-01-08 19:42:00,147: t15.2024.07.19 val PER: 0.2334
2026-01-08 19:42:00,147: t15.2024.07.21 val PER: 0.0945
2026-01-08 19:42:00,148: t15.2024.07.28 val PER: 0.1404
2026-01-08 19:42:00,148: t15.2025.01.10 val PER: 0.2920
2026-01-08 19:42:00,148: t15.2025.01.12 val PER: 0.1386
2026-01-08 19:42:00,148: t15.2025.03.14 val PER: 0.3417
2026-01-08 19:42:00,148: t15.2025.03.16 val PER: 0.1924
2026-01-08 19:42:00,148: t15.2025.03.30 val PER: 0.2989
2026-01-08 19:42:00,148: t15.2025.04.13 val PER: 0.2240
2026-01-08 19:42:00,285: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_17500
2026-01-08 19:42:08,849: Train batch 17600: loss: 10.92 grad norm: 55.83 time: 0.052
2026-01-08 19:42:26,104: Train batch 17800: loss: 6.83 grad norm: 52.97 time: 0.043
2026-01-08 19:42:43,715: Train batch 18000: loss: 12.02 grad norm: 58.69 time: 0.062
2026-01-08 19:42:43,715: Running test after training batch: 18000
2026-01-08 19:42:43,854: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:42:48,830: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:42:48,879: WER debug example
  GT : how does it keep the cost down
  PR : how dusty it keep the cost set
2026-01-08 19:43:00,543: Val batch 18000: PER (avg): 0.1491 CTC Loss (avg): 15.3776 WER(5gram): 15.91% (n=256) time: 16.828
2026-01-08 19:43:00,543: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 19:43:00,544: t15.2023.08.13 val PER: 0.1050
2026-01-08 19:43:00,544: t15.2023.08.18 val PER: 0.1056
2026-01-08 19:43:00,544: t15.2023.08.20 val PER: 0.1136
2026-01-08 19:43:00,544: t15.2023.08.25 val PER: 0.0964
2026-01-08 19:43:00,544: t15.2023.08.27 val PER: 0.1913
2026-01-08 19:43:00,544: t15.2023.09.01 val PER: 0.0731
2026-01-08 19:43:00,544: t15.2023.09.03 val PER: 0.1639
2026-01-08 19:43:00,544: t15.2023.09.24 val PER: 0.1189
2026-01-08 19:43:00,544: t15.2023.09.29 val PER: 0.1321
2026-01-08 19:43:00,544: t15.2023.10.01 val PER: 0.1585
2026-01-08 19:43:00,545: t15.2023.10.06 val PER: 0.0850
2026-01-08 19:43:00,545: t15.2023.10.08 val PER: 0.2287
2026-01-08 19:43:00,545: t15.2023.10.13 val PER: 0.2040
2026-01-08 19:43:00,545: t15.2023.10.15 val PER: 0.1595
2026-01-08 19:43:00,545: t15.2023.10.20 val PER: 0.1711
2026-01-08 19:43:00,545: t15.2023.10.22 val PER: 0.1136
2026-01-08 19:43:00,545: t15.2023.11.03 val PER: 0.1777
2026-01-08 19:43:00,545: t15.2023.11.04 val PER: 0.0375
2026-01-08 19:43:00,545: t15.2023.11.17 val PER: 0.0420
2026-01-08 19:43:00,545: t15.2023.11.19 val PER: 0.0339
2026-01-08 19:43:00,545: t15.2023.11.26 val PER: 0.1130
2026-01-08 19:43:00,545: t15.2023.12.03 val PER: 0.1103
2026-01-08 19:43:00,545: t15.2023.12.08 val PER: 0.0965
2026-01-08 19:43:00,545: t15.2023.12.10 val PER: 0.0933
2026-01-08 19:43:00,545: t15.2023.12.17 val PER: 0.1216
2026-01-08 19:43:00,546: t15.2023.12.29 val PER: 0.1283
2026-01-08 19:43:00,546: t15.2024.02.25 val PER: 0.1138
2026-01-08 19:43:00,546: t15.2024.03.08 val PER: 0.2276
2026-01-08 19:43:00,546: t15.2024.03.15 val PER: 0.2045
2026-01-08 19:43:00,546: t15.2024.03.17 val PER: 0.1374
2026-01-08 19:43:00,546: t15.2024.05.10 val PER: 0.1575
2026-01-08 19:43:00,546: t15.2024.06.14 val PER: 0.1688
2026-01-08 19:43:00,546: t15.2024.07.19 val PER: 0.2386
2026-01-08 19:43:00,546: t15.2024.07.21 val PER: 0.0938
2026-01-08 19:43:00,546: t15.2024.07.28 val PER: 0.1404
2026-01-08 19:43:00,546: t15.2025.01.10 val PER: 0.2879
2026-01-08 19:43:00,546: t15.2025.01.12 val PER: 0.1424
2026-01-08 19:43:00,546: t15.2025.03.14 val PER: 0.3402
2026-01-08 19:43:00,546: t15.2025.03.16 val PER: 0.1898
2026-01-08 19:43:00,546: t15.2025.03.30 val PER: 0.2897
2026-01-08 19:43:00,546: t15.2025.04.13 val PER: 0.2183
2026-01-08 19:43:00,689: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_18000
2026-01-08 19:43:18,250: Train batch 18200: loss: 8.68 grad norm: 48.93 time: 0.077
2026-01-08 19:43:35,284: Train batch 18400: loss: 5.54 grad norm: 41.87 time: 0.059
2026-01-08 19:43:44,344: Running test after training batch: 18500
2026-01-08 19:43:44,493: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:43:49,471: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:43:49,521: WER debug example
  GT : how does it keep the cost down
  PR : how dusty it keep the cost set
2026-01-08 19:44:01,375: Val batch 18500: PER (avg): 0.1494 CTC Loss (avg): 15.4207 WER(5gram): 16.10% (n=256) time: 17.030
2026-01-08 19:44:01,375: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-08 19:44:01,375: t15.2023.08.13 val PER: 0.1040
2026-01-08 19:44:01,375: t15.2023.08.18 val PER: 0.1014
2026-01-08 19:44:01,375: t15.2023.08.20 val PER: 0.1144
2026-01-08 19:44:01,375: t15.2023.08.25 val PER: 0.0919
2026-01-08 19:44:01,375: t15.2023.08.27 val PER: 0.1929
2026-01-08 19:44:01,376: t15.2023.09.01 val PER: 0.0706
2026-01-08 19:44:01,376: t15.2023.09.03 val PER: 0.1627
2026-01-08 19:44:01,376: t15.2023.09.24 val PER: 0.1177
2026-01-08 19:44:01,376: t15.2023.09.29 val PER: 0.1372
2026-01-08 19:44:01,376: t15.2023.10.01 val PER: 0.1651
2026-01-08 19:44:01,376: t15.2023.10.06 val PER: 0.0861
2026-01-08 19:44:01,376: t15.2023.10.08 val PER: 0.2327
2026-01-08 19:44:01,376: t15.2023.10.13 val PER: 0.2017
2026-01-08 19:44:01,376: t15.2023.10.15 val PER: 0.1549
2026-01-08 19:44:01,376: t15.2023.10.20 val PER: 0.1711
2026-01-08 19:44:01,376: t15.2023.10.22 val PER: 0.1203
2026-01-08 19:44:01,376: t15.2023.11.03 val PER: 0.1852
2026-01-08 19:44:01,376: t15.2023.11.04 val PER: 0.0341
2026-01-08 19:44:01,376: t15.2023.11.17 val PER: 0.0404
2026-01-08 19:44:01,376: t15.2023.11.19 val PER: 0.0339
2026-01-08 19:44:01,376: t15.2023.11.26 val PER: 0.1188
2026-01-08 19:44:01,377: t15.2023.12.03 val PER: 0.1061
2026-01-08 19:44:01,377: t15.2023.12.08 val PER: 0.0945
2026-01-08 19:44:01,377: t15.2023.12.10 val PER: 0.0894
2026-01-08 19:44:01,377: t15.2023.12.17 val PER: 0.1247
2026-01-08 19:44:01,377: t15.2023.12.29 val PER: 0.1277
2026-01-08 19:44:01,377: t15.2024.02.25 val PER: 0.1081
2026-01-08 19:44:01,377: t15.2024.03.08 val PER: 0.2347
2026-01-08 19:44:01,377: t15.2024.03.15 val PER: 0.2026
2026-01-08 19:44:01,377: t15.2024.03.17 val PER: 0.1353
2026-01-08 19:44:01,377: t15.2024.05.10 val PER: 0.1560
2026-01-08 19:44:01,377: t15.2024.06.14 val PER: 0.1640
2026-01-08 19:44:01,377: t15.2024.07.19 val PER: 0.2353
2026-01-08 19:44:01,377: t15.2024.07.21 val PER: 0.0966
2026-01-08 19:44:01,377: t15.2024.07.28 val PER: 0.1353
2026-01-08 19:44:01,377: t15.2025.01.10 val PER: 0.2920
2026-01-08 19:44:01,378: t15.2025.01.12 val PER: 0.1393
2026-01-08 19:44:01,378: t15.2025.03.14 val PER: 0.3402
2026-01-08 19:44:01,378: t15.2025.03.16 val PER: 0.1976
2026-01-08 19:44:01,378: t15.2025.03.30 val PER: 0.2989
2026-01-08 19:44:01,378: t15.2025.04.13 val PER: 0.2225
2026-01-08 19:44:01,521: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_18500
2026-01-08 19:44:10,558: Train batch 18600: loss: 13.55 grad norm: 57.87 time: 0.068
2026-01-08 19:44:28,496: Train batch 18800: loss: 9.01 grad norm: 50.93 time: 0.066
2026-01-08 19:44:46,289: Train batch 19000: loss: 8.90 grad norm: 45.77 time: 0.067
2026-01-08 19:44:46,289: Running test after training batch: 19000
2026-01-08 19:44:46,404: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:44:51,544: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:44:51,595: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost cents
2026-01-08 19:45:03,487: Val batch 19000: PER (avg): 0.1479 CTC Loss (avg): 15.3317 WER(5gram): 15.45% (n=256) time: 17.198
2026-01-08 19:45:03,487: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 19:45:03,488: t15.2023.08.13 val PER: 0.1029
2026-01-08 19:45:03,488: t15.2023.08.18 val PER: 0.0972
2026-01-08 19:45:03,488: t15.2023.08.20 val PER: 0.1096
2026-01-08 19:45:03,488: t15.2023.08.25 val PER: 0.0904
2026-01-08 19:45:03,488: t15.2023.08.27 val PER: 0.1817
2026-01-08 19:45:03,489: t15.2023.09.01 val PER: 0.0739
2026-01-08 19:45:03,489: t15.2023.09.03 val PER: 0.1568
2026-01-08 19:45:03,489: t15.2023.09.24 val PER: 0.1226
2026-01-08 19:45:03,489: t15.2023.09.29 val PER: 0.1315
2026-01-08 19:45:03,489: t15.2023.10.01 val PER: 0.1631
2026-01-08 19:45:03,489: t15.2023.10.06 val PER: 0.0840
2026-01-08 19:45:03,489: t15.2023.10.08 val PER: 0.2368
2026-01-08 19:45:03,489: t15.2023.10.13 val PER: 0.2009
2026-01-08 19:45:03,489: t15.2023.10.15 val PER: 0.1556
2026-01-08 19:45:03,490: t15.2023.10.20 val PER: 0.1678
2026-01-08 19:45:03,490: t15.2023.10.22 val PER: 0.1158
2026-01-08 19:45:03,490: t15.2023.11.03 val PER: 0.1798
2026-01-08 19:45:03,490: t15.2023.11.04 val PER: 0.0341
2026-01-08 19:45:03,490: t15.2023.11.17 val PER: 0.0435
2026-01-08 19:45:03,490: t15.2023.11.19 val PER: 0.0339
2026-01-08 19:45:03,490: t15.2023.11.26 val PER: 0.1152
2026-01-08 19:45:03,490: t15.2023.12.03 val PER: 0.1050
2026-01-08 19:45:03,490: t15.2023.12.08 val PER: 0.0959
2026-01-08 19:45:03,490: t15.2023.12.10 val PER: 0.0880
2026-01-08 19:45:03,490: t15.2023.12.17 val PER: 0.1247
2026-01-08 19:45:03,491: t15.2023.12.29 val PER: 0.1352
2026-01-08 19:45:03,491: t15.2024.02.25 val PER: 0.1096
2026-01-08 19:45:03,491: t15.2024.03.08 val PER: 0.2276
2026-01-08 19:45:03,491: t15.2024.03.15 val PER: 0.2020
2026-01-08 19:45:03,491: t15.2024.03.17 val PER: 0.1381
2026-01-08 19:45:03,491: t15.2024.05.10 val PER: 0.1471
2026-01-08 19:45:03,491: t15.2024.06.14 val PER: 0.1703
2026-01-08 19:45:03,491: t15.2024.07.19 val PER: 0.2367
2026-01-08 19:45:03,491: t15.2024.07.21 val PER: 0.0897
2026-01-08 19:45:03,491: t15.2024.07.28 val PER: 0.1390
2026-01-08 19:45:03,491: t15.2025.01.10 val PER: 0.2879
2026-01-08 19:45:03,491: t15.2025.01.12 val PER: 0.1301
2026-01-08 19:45:03,492: t15.2025.03.14 val PER: 0.3491
2026-01-08 19:45:03,492: t15.2025.03.16 val PER: 0.1937
2026-01-08 19:45:03,492: t15.2025.03.30 val PER: 0.2828
2026-01-08 19:45:03,492: t15.2025.04.13 val PER: 0.2225
2026-01-08 19:45:03,628: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_19000
2026-01-08 19:45:21,397: Train batch 19200: loss: 6.71 grad norm: 45.52 time: 0.063
2026-01-08 19:45:39,284: Train batch 19400: loss: 5.47 grad norm: 39.51 time: 0.053
2026-01-08 19:45:48,253: Running test after training batch: 19500
2026-01-08 19:45:48,389: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:45:53,630: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:45:53,681: WER debug example
  GT : how does it keep the cost down
  PR : how dusty it keep the cost cents
2026-01-08 19:46:05,750: Val batch 19500: PER (avg): 0.1477 CTC Loss (avg): 15.3318 WER(5gram): 15.38% (n=256) time: 17.496
2026-01-08 19:46:05,751: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 19:46:05,751: t15.2023.08.13 val PER: 0.1029
2026-01-08 19:46:05,751: t15.2023.08.18 val PER: 0.1056
2026-01-08 19:46:05,751: t15.2023.08.20 val PER: 0.1112
2026-01-08 19:46:05,751: t15.2023.08.25 val PER: 0.0889
2026-01-08 19:46:05,751: t15.2023.08.27 val PER: 0.1881
2026-01-08 19:46:05,751: t15.2023.09.01 val PER: 0.0714
2026-01-08 19:46:05,751: t15.2023.09.03 val PER: 0.1591
2026-01-08 19:46:05,751: t15.2023.09.24 val PER: 0.1177
2026-01-08 19:46:05,751: t15.2023.09.29 val PER: 0.1340
2026-01-08 19:46:05,751: t15.2023.10.01 val PER: 0.1612
2026-01-08 19:46:05,751: t15.2023.10.06 val PER: 0.0840
2026-01-08 19:46:05,751: t15.2023.10.08 val PER: 0.2341
2026-01-08 19:46:05,751: t15.2023.10.13 val PER: 0.2040
2026-01-08 19:46:05,751: t15.2023.10.15 val PER: 0.1549
2026-01-08 19:46:05,752: t15.2023.10.20 val PER: 0.1644
2026-01-08 19:46:05,752: t15.2023.10.22 val PER: 0.1180
2026-01-08 19:46:05,752: t15.2023.11.03 val PER: 0.1818
2026-01-08 19:46:05,752: t15.2023.11.04 val PER: 0.0341
2026-01-08 19:46:05,752: t15.2023.11.17 val PER: 0.0420
2026-01-08 19:46:05,752: t15.2023.11.19 val PER: 0.0319
2026-01-08 19:46:05,752: t15.2023.11.26 val PER: 0.1145
2026-01-08 19:46:05,752: t15.2023.12.03 val PER: 0.1092
2026-01-08 19:46:05,752: t15.2023.12.08 val PER: 0.0912
2026-01-08 19:46:05,753: t15.2023.12.10 val PER: 0.0880
2026-01-08 19:46:05,753: t15.2023.12.17 val PER: 0.1195
2026-01-08 19:46:05,753: t15.2023.12.29 val PER: 0.1325
2026-01-08 19:46:05,753: t15.2024.02.25 val PER: 0.1124
2026-01-08 19:46:05,753: t15.2024.03.08 val PER: 0.2290
2026-01-08 19:46:05,753: t15.2024.03.15 val PER: 0.1995
2026-01-08 19:46:05,753: t15.2024.03.17 val PER: 0.1374
2026-01-08 19:46:05,753: t15.2024.05.10 val PER: 0.1516
2026-01-08 19:46:05,753: t15.2024.06.14 val PER: 0.1640
2026-01-08 19:46:05,753: t15.2024.07.19 val PER: 0.2320
2026-01-08 19:46:05,753: t15.2024.07.21 val PER: 0.0876
2026-01-08 19:46:05,753: t15.2024.07.28 val PER: 0.1360
2026-01-08 19:46:05,753: t15.2025.01.10 val PER: 0.2920
2026-01-08 19:46:05,753: t15.2025.01.12 val PER: 0.1355
2026-01-08 19:46:05,753: t15.2025.03.14 val PER: 0.3388
2026-01-08 19:46:05,754: t15.2025.03.16 val PER: 0.1898
2026-01-08 19:46:05,754: t15.2025.03.30 val PER: 0.2943
2026-01-08 19:46:05,754: t15.2025.04.13 val PER: 0.2211
2026-01-08 19:46:05,892: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_19500
2026-01-08 19:46:14,268: Train batch 19600: loss: 8.59 grad norm: 51.49 time: 0.058
2026-01-08 19:46:31,210: Train batch 19800: loss: 8.23 grad norm: 49.09 time: 0.056
2026-01-08 19:46:48,510: Running test after training batch: 19999
2026-01-08 19:46:48,609: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:46:53,691: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 19:46:53,745: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost set
2026-01-08 19:47:05,739: Val batch 19999: PER (avg): 0.1480 CTC Loss (avg): 15.3413 WER(5gram): 15.51% (n=256) time: 17.229
2026-01-08 19:47:05,739: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 19:47:05,739: t15.2023.08.13 val PER: 0.1029
2026-01-08 19:47:05,739: t15.2023.08.18 val PER: 0.1039
2026-01-08 19:47:05,739: t15.2023.08.20 val PER: 0.1104
2026-01-08 19:47:05,739: t15.2023.08.25 val PER: 0.0919
2026-01-08 19:47:05,739: t15.2023.08.27 val PER: 0.1849
2026-01-08 19:47:05,740: t15.2023.09.01 val PER: 0.0714
2026-01-08 19:47:05,740: t15.2023.09.03 val PER: 0.1615
2026-01-08 19:47:05,740: t15.2023.09.24 val PER: 0.1214
2026-01-08 19:47:05,740: t15.2023.09.29 val PER: 0.1359
2026-01-08 19:47:05,740: t15.2023.10.01 val PER: 0.1625
2026-01-08 19:47:05,740: t15.2023.10.06 val PER: 0.0861
2026-01-08 19:47:05,740: t15.2023.10.08 val PER: 0.2341
2026-01-08 19:47:05,740: t15.2023.10.13 val PER: 0.2056
2026-01-08 19:47:05,740: t15.2023.10.15 val PER: 0.1523
2026-01-08 19:47:05,740: t15.2023.10.20 val PER: 0.1577
2026-01-08 19:47:05,740: t15.2023.10.22 val PER: 0.1169
2026-01-08 19:47:05,740: t15.2023.11.03 val PER: 0.1771
2026-01-08 19:47:05,740: t15.2023.11.04 val PER: 0.0307
2026-01-08 19:47:05,740: t15.2023.11.17 val PER: 0.0389
2026-01-08 19:47:05,741: t15.2023.11.19 val PER: 0.0339
2026-01-08 19:47:05,741: t15.2023.11.26 val PER: 0.1138
2026-01-08 19:47:05,741: t15.2023.12.03 val PER: 0.1124
2026-01-08 19:47:05,741: t15.2023.12.08 val PER: 0.0945
2026-01-08 19:47:05,741: t15.2023.12.10 val PER: 0.0894
2026-01-08 19:47:05,741: t15.2023.12.17 val PER: 0.1216
2026-01-08 19:47:05,741: t15.2023.12.29 val PER: 0.1290
2026-01-08 19:47:05,741: t15.2024.02.25 val PER: 0.1081
2026-01-08 19:47:05,741: t15.2024.03.08 val PER: 0.2304
2026-01-08 19:47:05,741: t15.2024.03.15 val PER: 0.2026
2026-01-08 19:47:05,741: t15.2024.03.17 val PER: 0.1374
2026-01-08 19:47:05,741: t15.2024.05.10 val PER: 0.1471
2026-01-08 19:47:05,742: t15.2024.06.14 val PER: 0.1625
2026-01-08 19:47:05,742: t15.2024.07.19 val PER: 0.2399
2026-01-08 19:47:05,742: t15.2024.07.21 val PER: 0.0903
2026-01-08 19:47:05,742: t15.2024.07.28 val PER: 0.1375
2026-01-08 19:47:05,742: t15.2025.01.10 val PER: 0.2934
2026-01-08 19:47:05,742: t15.2025.01.12 val PER: 0.1363
2026-01-08 19:47:05,742: t15.2025.03.14 val PER: 0.3314
2026-01-08 19:47:05,743: t15.2025.03.16 val PER: 0.1976
2026-01-08 19:47:05,743: t15.2025.03.30 val PER: 0.2908
2026-01-08 19:47:05,743: t15.2025.04.13 val PER: 0.2140
2026-01-08 19:47:05,879: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/speckle_03/checkpoint/checkpoint_batch_19999
2026-01-08 19:47:06,448: Best avg val PER achieved: 0.16032
2026-01-08 19:47:06,448: Total training time: 46.49 minutes

=== RUN stepdrop_15k.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k
2026-01-08 19:48:58,339: Using device: cuda:0
2026-01-08 19:52:55,048: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-08 19:52:55,076: Using 45 sessions after filtering (from 45).
2026-01-08 19:52:55,527: Using torch.compile (if available)
2026-01-08 19:52:55,528: torch.compile not available (torch<2.0). Skipping.
2026-01-08 19:52:55,528: Initialized RNN decoding model
2026-01-08 19:52:55,528: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-08 19:52:55,528: Model has 44,907,305 parameters
2026-01-08 19:52:55,528: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-08 19:52:56,804: Successfully initialized datasets
2026-01-08 19:52:56,805: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-08 19:52:58,410: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.185
2026-01-08 19:52:58,410: Running test after training batch: 0
2026-01-08 19:52:58,523: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:53:04,410: WER debug example
  GT : you can see the code at this point as well
  PR : she has from his
2026-01-08 19:53:05,443: WER debug example
  GT : how does it keep the cost down
  PR : money from
2026-01-08 19:56:59,619: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(5gram): 99.67% (n=256) time: 241.208
2026-01-08 19:56:59,627: WER lens: avg_true_words=5.99 avg_pred_words=2.82 max_pred_words=7
2026-01-08 19:56:59,631: t15.2023.08.13 val PER: 1.3056
2026-01-08 19:56:59,636: t15.2023.08.18 val PER: 1.4208
2026-01-08 19:56:59,637: t15.2023.08.20 val PER: 1.3002
2026-01-08 19:56:59,637: t15.2023.08.25 val PER: 1.3389
2026-01-08 19:56:59,637: t15.2023.08.27 val PER: 1.2460
2026-01-08 19:56:59,637: t15.2023.09.01 val PER: 1.4537
2026-01-08 19:56:59,637: t15.2023.09.03 val PER: 1.3171
2026-01-08 19:56:59,637: t15.2023.09.24 val PER: 1.5461
2026-01-08 19:56:59,637: t15.2023.09.29 val PER: 1.4671
2026-01-08 19:56:59,637: t15.2023.10.01 val PER: 1.2147
2026-01-08 19:56:59,637: t15.2023.10.06 val PER: 1.4876
2026-01-08 19:56:59,637: t15.2023.10.08 val PER: 1.1827
2026-01-08 19:56:59,637: t15.2023.10.13 val PER: 1.3964
2026-01-08 19:56:59,638: t15.2023.10.15 val PER: 1.3889
2026-01-08 19:56:59,638: t15.2023.10.20 val PER: 1.4866
2026-01-08 19:56:59,638: t15.2023.10.22 val PER: 1.3942
2026-01-08 19:56:59,638: t15.2023.11.03 val PER: 1.5923
2026-01-08 19:56:59,638: t15.2023.11.04 val PER: 2.0171
2026-01-08 19:56:59,638: t15.2023.11.17 val PER: 1.9518
2026-01-08 19:56:59,638: t15.2023.11.19 val PER: 1.6707
2026-01-08 19:56:59,638: t15.2023.11.26 val PER: 1.5413
2026-01-08 19:56:59,638: t15.2023.12.03 val PER: 1.4254
2026-01-08 19:56:59,638: t15.2023.12.08 val PER: 1.4487
2026-01-08 19:56:59,638: t15.2023.12.10 val PER: 1.6899
2026-01-08 19:56:59,638: t15.2023.12.17 val PER: 1.3077
2026-01-08 19:56:59,638: t15.2023.12.29 val PER: 1.4063
2026-01-08 19:56:59,639: t15.2024.02.25 val PER: 1.4228
2026-01-08 19:56:59,639: t15.2024.03.08 val PER: 1.3257
2026-01-08 19:56:59,639: t15.2024.03.15 val PER: 1.3196
2026-01-08 19:56:59,639: t15.2024.03.17 val PER: 1.4052
2026-01-08 19:56:59,639: t15.2024.05.10 val PER: 1.3224
2026-01-08 19:56:59,639: t15.2024.06.14 val PER: 1.5315
2026-01-08 19:56:59,639: t15.2024.07.19 val PER: 1.0817
2026-01-08 19:56:59,639: t15.2024.07.21 val PER: 1.6290
2026-01-08 19:56:59,639: t15.2024.07.28 val PER: 1.6588
2026-01-08 19:56:59,639: t15.2025.01.10 val PER: 1.0923
2026-01-08 19:56:59,639: t15.2025.01.12 val PER: 1.7629
2026-01-08 19:56:59,639: t15.2025.03.14 val PER: 1.0414
2026-01-08 19:56:59,639: t15.2025.03.16 val PER: 1.6257
2026-01-08 19:56:59,639: t15.2025.03.30 val PER: 1.2874
2026-01-08 19:56:59,639: t15.2025.04.13 val PER: 1.5949
2026-01-08 19:56:59,640: New best val WER(5gram) inf% --> 99.67%
2026-01-08 19:56:59,848: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_0
2026-01-08 19:57:16,747: Train batch 200: loss: 77.59 grad norm: 106.09 time: 0.055
2026-01-08 19:57:33,460: Train batch 400: loss: 54.26 grad norm: 112.66 time: 0.064
2026-01-08 19:57:42,743: Running test after training batch: 500
2026-01-08 19:57:42,878: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:57:47,961: WER debug example
  GT : you can see the code at this point as well
  PR : you'll need is the ease and at this guide is all
2026-01-08 19:57:48,107: WER debug example
  GT : how does it keep the cost down
  PR : and does it think this is as
2026-01-08 19:58:35,739: Val batch 500: PER (avg): 0.5148 CTC Loss (avg): 55.1697 WER(5gram): 74.58% (n=256) time: 52.996
2026-01-08 19:58:35,740: WER lens: avg_true_words=5.99 avg_pred_words=5.72 max_pred_words=12
2026-01-08 19:58:35,740: t15.2023.08.13 val PER: 0.4667
2026-01-08 19:58:35,740: t15.2023.08.18 val PER: 0.4526
2026-01-08 19:58:35,740: t15.2023.08.20 val PER: 0.4376
2026-01-08 19:58:35,740: t15.2023.08.25 val PER: 0.4292
2026-01-08 19:58:35,740: t15.2023.08.27 val PER: 0.5257
2026-01-08 19:58:35,741: t15.2023.09.01 val PER: 0.4075
2026-01-08 19:58:35,741: t15.2023.09.03 val PER: 0.4846
2026-01-08 19:58:35,741: t15.2023.09.24 val PER: 0.4248
2026-01-08 19:58:35,741: t15.2023.09.29 val PER: 0.4671
2026-01-08 19:58:35,741: t15.2023.10.01 val PER: 0.5165
2026-01-08 19:58:35,741: t15.2023.10.06 val PER: 0.4220
2026-01-08 19:58:35,741: t15.2023.10.08 val PER: 0.5291
2026-01-08 19:58:35,741: t15.2023.10.13 val PER: 0.5756
2026-01-08 19:58:35,741: t15.2023.10.15 val PER: 0.4918
2026-01-08 19:58:35,741: t15.2023.10.20 val PER: 0.4597
2026-01-08 19:58:35,741: t15.2023.10.22 val PER: 0.4566
2026-01-08 19:58:35,741: t15.2023.11.03 val PER: 0.5095
2026-01-08 19:58:35,741: t15.2023.11.04 val PER: 0.2526
2026-01-08 19:58:35,742: t15.2023.11.17 val PER: 0.3686
2026-01-08 19:58:35,742: t15.2023.11.19 val PER: 0.3353
2026-01-08 19:58:35,742: t15.2023.11.26 val PER: 0.5449
2026-01-08 19:58:35,742: t15.2023.12.03 val PER: 0.4958
2026-01-08 19:58:35,742: t15.2023.12.08 val PER: 0.5067
2026-01-08 19:58:35,742: t15.2023.12.10 val PER: 0.4428
2026-01-08 19:58:35,742: t15.2023.12.17 val PER: 0.5572
2026-01-08 19:58:35,742: t15.2023.12.29 val PER: 0.5340
2026-01-08 19:58:35,742: t15.2024.02.25 val PER: 0.4775
2026-01-08 19:58:35,742: t15.2024.03.08 val PER: 0.5974
2026-01-08 19:58:35,742: t15.2024.03.15 val PER: 0.5503
2026-01-08 19:58:35,742: t15.2024.03.17 val PER: 0.5077
2026-01-08 19:58:35,743: t15.2024.05.10 val PER: 0.5409
2026-01-08 19:58:35,743: t15.2024.06.14 val PER: 0.5095
2026-01-08 19:58:35,743: t15.2024.07.19 val PER: 0.6658
2026-01-08 19:58:35,743: t15.2024.07.21 val PER: 0.4807
2026-01-08 19:58:35,743: t15.2024.07.28 val PER: 0.5096
2026-01-08 19:58:35,743: t15.2025.01.10 val PER: 0.7424
2026-01-08 19:58:35,743: t15.2025.01.12 val PER: 0.5504
2026-01-08 19:58:35,743: t15.2025.03.14 val PER: 0.7604
2026-01-08 19:58:35,743: t15.2025.03.16 val PER: 0.5838
2026-01-08 19:58:35,743: t15.2025.03.30 val PER: 0.7264
2026-01-08 19:58:35,743: t15.2025.04.13 val PER: 0.5678
2026-01-08 19:58:35,744: New best val WER(5gram) 99.67% --> 74.58%
2026-01-08 19:58:35,926: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_500
2026-01-08 19:58:44,525: Train batch 600: loss: 49.19 grad norm: 78.26 time: 0.081
2026-01-08 19:59:01,523: Train batch 800: loss: 41.54 grad norm: 90.70 time: 0.058
2026-01-08 19:59:19,014: Train batch 1000: loss: 42.88 grad norm: 80.03 time: 0.066
2026-01-08 19:59:19,015: Running test after training batch: 1000
2026-01-08 19:59:19,136: WER debug GT example: You can see the code at this point as well.
2026-01-08 19:59:24,581: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this and is well
2026-01-08 19:59:24,746: WER debug example
  GT : how does it keep the cost down
  PR : howled as it is that what's at
2026-01-08 19:59:54,164: Val batch 1000: PER (avg): 0.4056 CTC Loss (avg): 42.5044 WER(5gram): 52.61% (n=256) time: 35.149
2026-01-08 19:59:54,165: WER lens: avg_true_words=5.99 avg_pred_words=5.71 max_pred_words=12
2026-01-08 19:59:54,165: t15.2023.08.13 val PER: 0.3701
2026-01-08 19:59:54,165: t15.2023.08.18 val PER: 0.3378
2026-01-08 19:59:54,165: t15.2023.08.20 val PER: 0.3455
2026-01-08 19:59:54,166: t15.2023.08.25 val PER: 0.2982
2026-01-08 19:59:54,166: t15.2023.08.27 val PER: 0.4180
2026-01-08 19:59:54,166: t15.2023.09.01 val PER: 0.2987
2026-01-08 19:59:54,166: t15.2023.09.03 val PER: 0.3931
2026-01-08 19:59:54,166: t15.2023.09.24 val PER: 0.3216
2026-01-08 19:59:54,166: t15.2023.09.29 val PER: 0.3567
2026-01-08 19:59:54,166: t15.2023.10.01 val PER: 0.3996
2026-01-08 19:59:54,166: t15.2023.10.06 val PER: 0.3143
2026-01-08 19:59:54,167: t15.2023.10.08 val PER: 0.4506
2026-01-08 19:59:54,167: t15.2023.10.13 val PER: 0.4709
2026-01-08 19:59:54,167: t15.2023.10.15 val PER: 0.3705
2026-01-08 19:59:54,167: t15.2023.10.20 val PER: 0.3658
2026-01-08 19:59:54,167: t15.2023.10.22 val PER: 0.3552
2026-01-08 19:59:54,167: t15.2023.11.03 val PER: 0.3982
2026-01-08 19:59:54,167: t15.2023.11.04 val PER: 0.1706
2026-01-08 19:59:54,167: t15.2023.11.17 val PER: 0.2644
2026-01-08 19:59:54,167: t15.2023.11.19 val PER: 0.1976
2026-01-08 19:59:54,168: t15.2023.11.26 val PER: 0.4391
2026-01-08 19:59:54,168: t15.2023.12.03 val PER: 0.3981
2026-01-08 19:59:54,168: t15.2023.12.08 val PER: 0.4075
2026-01-08 19:59:54,168: t15.2023.12.10 val PER: 0.3403
2026-01-08 19:59:54,168: t15.2023.12.17 val PER: 0.4054
2026-01-08 19:59:54,168: t15.2023.12.29 val PER: 0.3981
2026-01-08 19:59:54,168: t15.2024.02.25 val PER: 0.3525
2026-01-08 19:59:54,168: t15.2024.03.08 val PER: 0.4950
2026-01-08 19:59:54,169: t15.2024.03.15 val PER: 0.4447
2026-01-08 19:59:54,169: t15.2024.03.17 val PER: 0.3989
2026-01-08 19:59:54,169: t15.2024.05.10 val PER: 0.4235
2026-01-08 19:59:54,169: t15.2024.06.14 val PER: 0.4022
2026-01-08 19:59:54,169: t15.2024.07.19 val PER: 0.5227
2026-01-08 19:59:54,169: t15.2024.07.21 val PER: 0.3752
2026-01-08 19:59:54,169: t15.2024.07.28 val PER: 0.4132
2026-01-08 19:59:54,169: t15.2025.01.10 val PER: 0.6047
2026-01-08 19:59:54,169: t15.2025.01.12 val PER: 0.4542
2026-01-08 19:59:54,169: t15.2025.03.14 val PER: 0.6361
2026-01-08 19:59:54,170: t15.2025.03.16 val PER: 0.4843
2026-01-08 19:59:54,170: t15.2025.03.30 val PER: 0.6368
2026-01-08 19:59:54,170: t15.2025.04.13 val PER: 0.4979
2026-01-08 19:59:54,170: New best val WER(5gram) 74.58% --> 52.61%
2026-01-08 19:59:54,360: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_1000
2026-01-08 20:00:10,995: Train batch 1200: loss: 33.07 grad norm: 75.45 time: 0.068
2026-01-08 20:00:28,332: Train batch 1400: loss: 36.85 grad norm: 84.76 time: 0.063
2026-01-08 20:00:37,110: Running test after training batch: 1500
2026-01-08 20:00:37,221: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:00:42,312: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-08 20:00:42,399: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us in
2026-01-08 20:01:00,762: Val batch 1500: PER (avg): 0.3814 CTC Loss (avg): 37.3225 WER(5gram): 36.57% (n=256) time: 23.652
2026-01-08 20:01:00,763: WER lens: avg_true_words=5.99 avg_pred_words=5.24 max_pred_words=12
2026-01-08 20:01:00,763: t15.2023.08.13 val PER: 0.3493
2026-01-08 20:01:00,763: t15.2023.08.18 val PER: 0.3194
2026-01-08 20:01:00,763: t15.2023.08.20 val PER: 0.2994
2026-01-08 20:01:00,763: t15.2023.08.25 val PER: 0.2620
2026-01-08 20:01:00,763: t15.2023.08.27 val PER: 0.3955
2026-01-08 20:01:00,763: t15.2023.09.01 val PER: 0.2744
2026-01-08 20:01:00,763: t15.2023.09.03 val PER: 0.3777
2026-01-08 20:01:00,763: t15.2023.09.24 val PER: 0.3107
2026-01-08 20:01:00,763: t15.2023.09.29 val PER: 0.3363
2026-01-08 20:01:00,764: t15.2023.10.01 val PER: 0.4016
2026-01-08 20:01:00,764: t15.2023.10.06 val PER: 0.2766
2026-01-08 20:01:00,764: t15.2023.10.08 val PER: 0.4330
2026-01-08 20:01:00,764: t15.2023.10.13 val PER: 0.4469
2026-01-08 20:01:00,764: t15.2023.10.15 val PER: 0.3659
2026-01-08 20:01:00,764: t15.2023.10.20 val PER: 0.3322
2026-01-08 20:01:00,764: t15.2023.10.22 val PER: 0.3151
2026-01-08 20:01:00,764: t15.2023.11.03 val PER: 0.3643
2026-01-08 20:01:00,764: t15.2023.11.04 val PER: 0.1195
2026-01-08 20:01:00,764: t15.2023.11.17 val PER: 0.2193
2026-01-08 20:01:00,764: t15.2023.11.19 val PER: 0.1557
2026-01-08 20:01:00,764: t15.2023.11.26 val PER: 0.4167
2026-01-08 20:01:00,764: t15.2023.12.03 val PER: 0.3834
2026-01-08 20:01:00,764: t15.2023.12.08 val PER: 0.3628
2026-01-08 20:01:00,765: t15.2023.12.10 val PER: 0.3127
2026-01-08 20:01:00,765: t15.2023.12.17 val PER: 0.3659
2026-01-08 20:01:00,765: t15.2023.12.29 val PER: 0.3754
2026-01-08 20:01:00,765: t15.2024.02.25 val PER: 0.3132
2026-01-08 20:01:00,765: t15.2024.03.08 val PER: 0.4651
2026-01-08 20:01:00,765: t15.2024.03.15 val PER: 0.4165
2026-01-08 20:01:00,765: t15.2024.03.17 val PER: 0.3821
2026-01-08 20:01:00,765: t15.2024.05.10 val PER: 0.3848
2026-01-08 20:01:00,765: t15.2024.06.14 val PER: 0.4022
2026-01-08 20:01:00,765: t15.2024.07.19 val PER: 0.5280
2026-01-08 20:01:00,765: t15.2024.07.21 val PER: 0.3469
2026-01-08 20:01:00,765: t15.2024.07.28 val PER: 0.3794
2026-01-08 20:01:00,765: t15.2025.01.10 val PER: 0.6006
2026-01-08 20:01:00,766: t15.2025.01.12 val PER: 0.4296
2026-01-08 20:01:00,766: t15.2025.03.14 val PER: 0.6124
2026-01-08 20:01:00,766: t15.2025.03.16 val PER: 0.4503
2026-01-08 20:01:00,766: t15.2025.03.30 val PER: 0.6310
2026-01-08 20:01:00,766: t15.2025.04.13 val PER: 0.4608
2026-01-08 20:01:00,767: New best val WER(5gram) 52.61% --> 36.57%
2026-01-08 20:01:00,958: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_1500
2026-01-08 20:01:09,993: Train batch 1600: loss: 37.24 grad norm: 80.36 time: 0.066
2026-01-08 20:01:28,256: Train batch 1800: loss: 35.48 grad norm: 72.11 time: 0.092
2026-01-08 20:01:46,442: Train batch 2000: loss: 33.35 grad norm: 69.43 time: 0.066
2026-01-08 20:01:46,442: Running test after training batch: 2000
2026-01-08 20:01:46,569: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:01:51,725: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-08 20:01:51,814: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us and
2026-01-08 20:02:09,468: Val batch 2000: PER (avg): 0.3287 CTC Loss (avg): 32.7591 WER(5gram): 29.47% (n=256) time: 23.025
2026-01-08 20:02:09,469: WER lens: avg_true_words=5.99 avg_pred_words=5.79 max_pred_words=12
2026-01-08 20:02:09,469: t15.2023.08.13 val PER: 0.3046
2026-01-08 20:02:09,469: t15.2023.08.18 val PER: 0.2531
2026-01-08 20:02:09,469: t15.2023.08.20 val PER: 0.2494
2026-01-08 20:02:09,470: t15.2023.08.25 val PER: 0.2334
2026-01-08 20:02:09,470: t15.2023.08.27 val PER: 0.3344
2026-01-08 20:02:09,470: t15.2023.09.01 val PER: 0.2256
2026-01-08 20:02:09,470: t15.2023.09.03 val PER: 0.3337
2026-01-08 20:02:09,470: t15.2023.09.24 val PER: 0.2549
2026-01-08 20:02:09,470: t15.2023.09.29 val PER: 0.2719
2026-01-08 20:02:09,470: t15.2023.10.01 val PER: 0.3269
2026-01-08 20:02:09,470: t15.2023.10.06 val PER: 0.2433
2026-01-08 20:02:09,470: t15.2023.10.08 val PER: 0.4032
2026-01-08 20:02:09,470: t15.2023.10.13 val PER: 0.3794
2026-01-08 20:02:09,470: t15.2023.10.15 val PER: 0.3019
2026-01-08 20:02:09,470: t15.2023.10.20 val PER: 0.2919
2026-01-08 20:02:09,471: t15.2023.10.22 val PER: 0.2684
2026-01-08 20:02:09,471: t15.2023.11.03 val PER: 0.3189
2026-01-08 20:02:09,471: t15.2023.11.04 val PER: 0.0785
2026-01-08 20:02:09,471: t15.2023.11.17 val PER: 0.1742
2026-01-08 20:02:09,471: t15.2023.11.19 val PER: 0.1317
2026-01-08 20:02:09,471: t15.2023.11.26 val PER: 0.3667
2026-01-08 20:02:09,471: t15.2023.12.03 val PER: 0.3193
2026-01-08 20:02:09,471: t15.2023.12.08 val PER: 0.3043
2026-01-08 20:02:09,471: t15.2023.12.10 val PER: 0.2615
2026-01-08 20:02:09,471: t15.2023.12.17 val PER: 0.3285
2026-01-08 20:02:09,471: t15.2023.12.29 val PER: 0.3288
2026-01-08 20:02:09,471: t15.2024.02.25 val PER: 0.2781
2026-01-08 20:02:09,471: t15.2024.03.08 val PER: 0.3983
2026-01-08 20:02:09,472: t15.2024.03.15 val PER: 0.3602
2026-01-08 20:02:09,472: t15.2024.03.17 val PER: 0.3501
2026-01-08 20:02:09,472: t15.2024.05.10 val PER: 0.3432
2026-01-08 20:02:09,472: t15.2024.06.14 val PER: 0.3344
2026-01-08 20:02:09,472: t15.2024.07.19 val PER: 0.4766
2026-01-08 20:02:09,472: t15.2024.07.21 val PER: 0.2883
2026-01-08 20:02:09,472: t15.2024.07.28 val PER: 0.3243
2026-01-08 20:02:09,472: t15.2025.01.10 val PER: 0.5386
2026-01-08 20:02:09,473: t15.2025.01.12 val PER: 0.3811
2026-01-08 20:02:09,473: t15.2025.03.14 val PER: 0.5296
2026-01-08 20:02:09,473: t15.2025.03.16 val PER: 0.3979
2026-01-08 20:02:09,473: t15.2025.03.30 val PER: 0.5540
2026-01-08 20:02:09,473: t15.2025.04.13 val PER: 0.4308
2026-01-08 20:02:09,474: New best val WER(5gram) 36.57% --> 29.47%
2026-01-08 20:02:09,668: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_2000
2026-01-08 20:02:26,960: Train batch 2200: loss: 29.09 grad norm: 72.42 time: 0.062
2026-01-08 20:02:44,764: Train batch 2400: loss: 29.15 grad norm: 61.85 time: 0.053
2026-01-08 20:02:53,337: Running test after training batch: 2500
2026-01-08 20:02:53,482: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:02:58,491: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-08 20:02:58,562: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:03:14,353: Val batch 2500: PER (avg): 0.3037 CTC Loss (avg): 30.0822 WER(5gram): 26.86% (n=256) time: 21.015
2026-01-08 20:03:14,353: WER lens: avg_true_words=5.99 avg_pred_words=5.89 max_pred_words=12
2026-01-08 20:03:14,353: t15.2023.08.13 val PER: 0.2869
2026-01-08 20:03:14,354: t15.2023.08.18 val PER: 0.2573
2026-01-08 20:03:14,354: t15.2023.08.20 val PER: 0.2351
2026-01-08 20:03:14,354: t15.2023.08.25 val PER: 0.2139
2026-01-08 20:03:14,354: t15.2023.08.27 val PER: 0.3071
2026-01-08 20:03:14,354: t15.2023.09.01 val PER: 0.2086
2026-01-08 20:03:14,354: t15.2023.09.03 val PER: 0.3029
2026-01-08 20:03:14,354: t15.2023.09.24 val PER: 0.2391
2026-01-08 20:03:14,354: t15.2023.09.29 val PER: 0.2546
2026-01-08 20:03:14,354: t15.2023.10.01 val PER: 0.3005
2026-01-08 20:03:14,354: t15.2023.10.06 val PER: 0.2121
2026-01-08 20:03:14,354: t15.2023.10.08 val PER: 0.3762
2026-01-08 20:03:14,354: t15.2023.10.13 val PER: 0.3499
2026-01-08 20:03:14,354: t15.2023.10.15 val PER: 0.2920
2026-01-08 20:03:14,355: t15.2023.10.20 val PER: 0.2886
2026-01-08 20:03:14,355: t15.2023.10.22 val PER: 0.2327
2026-01-08 20:03:14,355: t15.2023.11.03 val PER: 0.2958
2026-01-08 20:03:14,355: t15.2023.11.04 val PER: 0.0887
2026-01-08 20:03:14,355: t15.2023.11.17 val PER: 0.1477
2026-01-08 20:03:14,355: t15.2023.11.19 val PER: 0.1158
2026-01-08 20:03:14,355: t15.2023.11.26 val PER: 0.3522
2026-01-08 20:03:14,355: t15.2023.12.03 val PER: 0.2910
2026-01-08 20:03:14,355: t15.2023.12.08 val PER: 0.2796
2026-01-08 20:03:14,355: t15.2023.12.10 val PER: 0.2339
2026-01-08 20:03:14,356: t15.2023.12.17 val PER: 0.2775
2026-01-08 20:03:14,356: t15.2023.12.29 val PER: 0.3013
2026-01-08 20:03:14,356: t15.2024.02.25 val PER: 0.2388
2026-01-08 20:03:14,356: t15.2024.03.08 val PER: 0.3528
2026-01-08 20:03:14,356: t15.2024.03.15 val PER: 0.3465
2026-01-08 20:03:14,356: t15.2024.03.17 val PER: 0.3082
2026-01-08 20:03:14,356: t15.2024.05.10 val PER: 0.3135
2026-01-08 20:03:14,356: t15.2024.06.14 val PER: 0.3170
2026-01-08 20:03:14,356: t15.2024.07.19 val PER: 0.4410
2026-01-08 20:03:14,356: t15.2024.07.21 val PER: 0.2683
2026-01-08 20:03:14,356: t15.2024.07.28 val PER: 0.2971
2026-01-08 20:03:14,357: t15.2025.01.10 val PER: 0.5000
2026-01-08 20:03:14,357: t15.2025.01.12 val PER: 0.3495
2026-01-08 20:03:14,357: t15.2025.03.14 val PER: 0.5030
2026-01-08 20:03:14,357: t15.2025.03.16 val PER: 0.3626
2026-01-08 20:03:14,357: t15.2025.03.30 val PER: 0.5138
2026-01-08 20:03:14,357: t15.2025.04.13 val PER: 0.4009
2026-01-08 20:03:14,358: New best val WER(5gram) 29.47% --> 26.86%
2026-01-08 20:03:14,555: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_2500
2026-01-08 20:03:23,723: Train batch 2600: loss: 35.26 grad norm: 84.91 time: 0.061
2026-01-08 20:03:42,182: Train batch 2800: loss: 25.46 grad norm: 70.11 time: 0.092
2026-01-08 20:04:00,555: Train batch 3000: loss: 31.02 grad norm: 71.60 time: 0.085
2026-01-08 20:04:00,556: Running test after training batch: 3000
2026-01-08 20:04:00,667: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:04:05,732: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 20:04:05,796: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost at
2026-01-08 20:04:21,050: Val batch 3000: PER (avg): 0.2775 CTC Loss (avg): 27.6884 WER(5gram): 24.12% (n=256) time: 20.494
2026-01-08 20:04:21,050: WER lens: avg_true_words=5.99 avg_pred_words=6.02 max_pred_words=12
2026-01-08 20:04:21,050: t15.2023.08.13 val PER: 0.2568
2026-01-08 20:04:21,050: t15.2023.08.18 val PER: 0.2163
2026-01-08 20:04:21,050: t15.2023.08.20 val PER: 0.2097
2026-01-08 20:04:21,051: t15.2023.08.25 val PER: 0.1807
2026-01-08 20:04:21,051: t15.2023.08.27 val PER: 0.2830
2026-01-08 20:04:21,051: t15.2023.09.01 val PER: 0.1883
2026-01-08 20:04:21,051: t15.2023.09.03 val PER: 0.2827
2026-01-08 20:04:21,051: t15.2023.09.24 val PER: 0.2124
2026-01-08 20:04:21,051: t15.2023.09.29 val PER: 0.2329
2026-01-08 20:04:21,051: t15.2023.10.01 val PER: 0.2827
2026-01-08 20:04:21,051: t15.2023.10.06 val PER: 0.1916
2026-01-08 20:04:21,051: t15.2023.10.08 val PER: 0.3613
2026-01-08 20:04:21,051: t15.2023.10.13 val PER: 0.3413
2026-01-08 20:04:21,051: t15.2023.10.15 val PER: 0.2604
2026-01-08 20:04:21,051: t15.2023.10.20 val PER: 0.2450
2026-01-08 20:04:21,051: t15.2023.10.22 val PER: 0.2082
2026-01-08 20:04:21,052: t15.2023.11.03 val PER: 0.2761
2026-01-08 20:04:21,052: t15.2023.11.04 val PER: 0.0717
2026-01-08 20:04:21,052: t15.2023.11.17 val PER: 0.1291
2026-01-08 20:04:21,052: t15.2023.11.19 val PER: 0.1158
2026-01-08 20:04:21,052: t15.2023.11.26 val PER: 0.2993
2026-01-08 20:04:21,052: t15.2023.12.03 val PER: 0.2658
2026-01-08 20:04:21,052: t15.2023.12.08 val PER: 0.2530
2026-01-08 20:04:21,052: t15.2023.12.10 val PER: 0.2168
2026-01-08 20:04:21,052: t15.2023.12.17 val PER: 0.2703
2026-01-08 20:04:21,052: t15.2023.12.29 val PER: 0.2787
2026-01-08 20:04:21,052: t15.2024.02.25 val PER: 0.2261
2026-01-08 20:04:21,052: t15.2024.03.08 val PER: 0.3485
2026-01-08 20:04:21,053: t15.2024.03.15 val PER: 0.3427
2026-01-08 20:04:21,053: t15.2024.03.17 val PER: 0.2852
2026-01-08 20:04:21,053: t15.2024.05.10 val PER: 0.3001
2026-01-08 20:04:21,053: t15.2024.06.14 val PER: 0.2934
2026-01-08 20:04:21,053: t15.2024.07.19 val PER: 0.4008
2026-01-08 20:04:21,053: t15.2024.07.21 val PER: 0.2241
2026-01-08 20:04:21,053: t15.2024.07.28 val PER: 0.2669
2026-01-08 20:04:21,053: t15.2025.01.10 val PER: 0.4656
2026-01-08 20:04:21,056: t15.2025.01.12 val PER: 0.3226
2026-01-08 20:04:21,056: t15.2025.03.14 val PER: 0.4349
2026-01-08 20:04:21,056: t15.2025.03.16 val PER: 0.3181
2026-01-08 20:04:21,056: t15.2025.03.30 val PER: 0.4724
2026-01-08 20:04:21,056: t15.2025.04.13 val PER: 0.3509
2026-01-08 20:04:21,057: New best val WER(5gram) 26.86% --> 24.12%
2026-01-08 20:04:21,261: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_3000
2026-01-08 20:04:39,426: Train batch 3200: loss: 26.40 grad norm: 67.98 time: 0.077
2026-01-08 20:04:56,967: Train batch 3400: loss: 17.97 grad norm: 55.44 time: 0.049
2026-01-08 20:05:05,982: Running test after training batch: 3500
2026-01-08 20:05:06,111: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:05:11,306: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 20:05:11,356: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us at
2026-01-08 20:05:25,276: Val batch 3500: PER (avg): 0.2668 CTC Loss (avg): 26.3331 WER(5gram): 22.69% (n=256) time: 19.294
2026-01-08 20:05:25,277: WER lens: avg_true_words=5.99 avg_pred_words=6.05 max_pred_words=12
2026-01-08 20:05:25,277: t15.2023.08.13 val PER: 0.2484
2026-01-08 20:05:25,277: t15.2023.08.18 val PER: 0.2028
2026-01-08 20:05:25,278: t15.2023.08.20 val PER: 0.2160
2026-01-08 20:05:25,278: t15.2023.08.25 val PER: 0.1852
2026-01-08 20:05:25,278: t15.2023.08.27 val PER: 0.2765
2026-01-08 20:05:25,278: t15.2023.09.01 val PER: 0.1713
2026-01-08 20:05:25,278: t15.2023.09.03 val PER: 0.2648
2026-01-08 20:05:25,278: t15.2023.09.24 val PER: 0.2124
2026-01-08 20:05:25,278: t15.2023.09.29 val PER: 0.2151
2026-01-08 20:05:25,278: t15.2023.10.01 val PER: 0.2834
2026-01-08 20:05:25,278: t15.2023.10.06 val PER: 0.1851
2026-01-08 20:05:25,278: t15.2023.10.08 val PER: 0.3369
2026-01-08 20:05:25,278: t15.2023.10.13 val PER: 0.3165
2026-01-08 20:05:25,278: t15.2023.10.15 val PER: 0.2406
2026-01-08 20:05:25,278: t15.2023.10.20 val PER: 0.2584
2026-01-08 20:05:25,279: t15.2023.10.22 val PER: 0.2060
2026-01-08 20:05:25,279: t15.2023.11.03 val PER: 0.2653
2026-01-08 20:05:25,279: t15.2023.11.04 val PER: 0.0683
2026-01-08 20:05:25,279: t15.2023.11.17 val PER: 0.1213
2026-01-08 20:05:25,279: t15.2023.11.19 val PER: 0.0978
2026-01-08 20:05:25,279: t15.2023.11.26 val PER: 0.2942
2026-01-08 20:05:25,279: t15.2023.12.03 val PER: 0.2363
2026-01-08 20:05:25,279: t15.2023.12.08 val PER: 0.2503
2026-01-08 20:05:25,279: t15.2023.12.10 val PER: 0.1945
2026-01-08 20:05:25,280: t15.2023.12.17 val PER: 0.2464
2026-01-08 20:05:25,280: t15.2023.12.29 val PER: 0.2574
2026-01-08 20:05:25,280: t15.2024.02.25 val PER: 0.2065
2026-01-08 20:05:25,280: t15.2024.03.08 val PER: 0.3528
2026-01-08 20:05:25,280: t15.2024.03.15 val PER: 0.3202
2026-01-08 20:05:25,280: t15.2024.03.17 val PER: 0.2699
2026-01-08 20:05:25,280: t15.2024.05.10 val PER: 0.2734
2026-01-08 20:05:25,280: t15.2024.06.14 val PER: 0.2760
2026-01-08 20:05:25,280: t15.2024.07.19 val PER: 0.4008
2026-01-08 20:05:25,280: t15.2024.07.21 val PER: 0.2186
2026-01-08 20:05:25,280: t15.2024.07.28 val PER: 0.2787
2026-01-08 20:05:25,280: t15.2025.01.10 val PER: 0.4408
2026-01-08 20:05:25,280: t15.2025.01.12 val PER: 0.3110
2026-01-08 20:05:25,280: t15.2025.03.14 val PER: 0.4290
2026-01-08 20:05:25,280: t15.2025.03.16 val PER: 0.3377
2026-01-08 20:05:25,280: t15.2025.03.30 val PER: 0.4529
2026-01-08 20:05:25,281: t15.2025.04.13 val PER: 0.3395
2026-01-08 20:05:25,282: New best val WER(5gram) 24.12% --> 22.69%
2026-01-08 20:05:25,475: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_3500
2026-01-08 20:05:34,294: Train batch 3600: loss: 22.50 grad norm: 63.55 time: 0.067
2026-01-08 20:05:51,405: Train batch 3800: loss: 25.75 grad norm: 67.90 time: 0.067
2026-01-08 20:06:08,879: Train batch 4000: loss: 19.37 grad norm: 53.93 time: 0.056
2026-01-08 20:06:08,879: Running test after training batch: 4000
2026-01-08 20:06:09,043: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:06:13,998: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-08 20:06:14,048: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost at
2026-01-08 20:06:27,181: Val batch 4000: PER (avg): 0.2486 CTC Loss (avg): 24.5541 WER(5gram): 25.49% (n=256) time: 18.302
2026-01-08 20:06:27,182: WER lens: avg_true_words=5.99 avg_pred_words=6.08 max_pred_words=12
2026-01-08 20:06:27,182: t15.2023.08.13 val PER: 0.2297
2026-01-08 20:06:27,182: t15.2023.08.18 val PER: 0.1995
2026-01-08 20:06:27,183: t15.2023.08.20 val PER: 0.2002
2026-01-08 20:06:27,183: t15.2023.08.25 val PER: 0.1551
2026-01-08 20:06:27,183: t15.2023.08.27 val PER: 0.2797
2026-01-08 20:06:27,183: t15.2023.09.01 val PER: 0.1615
2026-01-08 20:06:27,183: t15.2023.09.03 val PER: 0.2423
2026-01-08 20:06:27,183: t15.2023.09.24 val PER: 0.2015
2026-01-08 20:06:27,183: t15.2023.09.29 val PER: 0.1991
2026-01-08 20:06:27,183: t15.2023.10.01 val PER: 0.2596
2026-01-08 20:06:27,183: t15.2023.10.06 val PER: 0.1755
2026-01-08 20:06:27,183: t15.2023.10.08 val PER: 0.3424
2026-01-08 20:06:27,183: t15.2023.10.13 val PER: 0.3010
2026-01-08 20:06:27,183: t15.2023.10.15 val PER: 0.2386
2026-01-08 20:06:27,184: t15.2023.10.20 val PER: 0.2517
2026-01-08 20:06:27,184: t15.2023.10.22 val PER: 0.1982
2026-01-08 20:06:27,184: t15.2023.11.03 val PER: 0.2476
2026-01-08 20:06:27,184: t15.2023.11.04 val PER: 0.0648
2026-01-08 20:06:27,184: t15.2023.11.17 val PER: 0.1011
2026-01-08 20:06:27,184: t15.2023.11.19 val PER: 0.1018
2026-01-08 20:06:27,184: t15.2023.11.26 val PER: 0.2645
2026-01-08 20:06:27,184: t15.2023.12.03 val PER: 0.2164
2026-01-08 20:06:27,184: t15.2023.12.08 val PER: 0.2184
2026-01-08 20:06:27,184: t15.2023.12.10 val PER: 0.1840
2026-01-08 20:06:27,184: t15.2023.12.17 val PER: 0.2412
2026-01-08 20:06:27,184: t15.2023.12.29 val PER: 0.2636
2026-01-08 20:06:27,184: t15.2024.02.25 val PER: 0.2177
2026-01-08 20:06:27,184: t15.2024.03.08 val PER: 0.3314
2026-01-08 20:06:27,185: t15.2024.03.15 val PER: 0.2902
2026-01-08 20:06:27,185: t15.2024.03.17 val PER: 0.2531
2026-01-08 20:06:27,185: t15.2024.05.10 val PER: 0.2675
2026-01-08 20:06:27,185: t15.2024.06.14 val PER: 0.2555
2026-01-08 20:06:27,185: t15.2024.07.19 val PER: 0.3573
2026-01-08 20:06:27,185: t15.2024.07.21 val PER: 0.1883
2026-01-08 20:06:27,185: t15.2024.07.28 val PER: 0.2353
2026-01-08 20:06:27,185: t15.2025.01.10 val PER: 0.4132
2026-01-08 20:06:27,185: t15.2025.01.12 val PER: 0.2802
2026-01-08 20:06:27,185: t15.2025.03.14 val PER: 0.4142
2026-01-08 20:06:27,186: t15.2025.03.16 val PER: 0.3063
2026-01-08 20:06:27,186: t15.2025.03.30 val PER: 0.4138
2026-01-08 20:06:27,186: t15.2025.04.13 val PER: 0.3224
2026-01-08 20:06:27,328: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_4000
2026-01-08 20:06:44,726: Train batch 4200: loss: 22.21 grad norm: 62.96 time: 0.080
2026-01-08 20:07:02,529: Train batch 4400: loss: 16.62 grad norm: 53.39 time: 0.066
2026-01-08 20:07:11,445: Running test after training batch: 4500
2026-01-08 20:07:11,572: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:07:16,546: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:07:16,600: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost get
2026-01-08 20:07:29,005: Val batch 4500: PER (avg): 0.2365 CTC Loss (avg): 23.2309 WER(5gram): 22.29% (n=256) time: 17.559
2026-01-08 20:07:29,005: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-08 20:07:29,005: t15.2023.08.13 val PER: 0.2058
2026-01-08 20:07:29,005: t15.2023.08.18 val PER: 0.1911
2026-01-08 20:07:29,005: t15.2023.08.20 val PER: 0.1859
2026-01-08 20:07:29,006: t15.2023.08.25 val PER: 0.1340
2026-01-08 20:07:29,006: t15.2023.08.27 val PER: 0.2588
2026-01-08 20:07:29,006: t15.2023.09.01 val PER: 0.1518
2026-01-08 20:07:29,006: t15.2023.09.03 val PER: 0.2435
2026-01-08 20:07:29,006: t15.2023.09.24 val PER: 0.1772
2026-01-08 20:07:29,006: t15.2023.09.29 val PER: 0.2049
2026-01-08 20:07:29,006: t15.2023.10.01 val PER: 0.2536
2026-01-08 20:07:29,006: t15.2023.10.06 val PER: 0.1593
2026-01-08 20:07:29,006: t15.2023.10.08 val PER: 0.3275
2026-01-08 20:07:29,006: t15.2023.10.13 val PER: 0.2956
2026-01-08 20:07:29,006: t15.2023.10.15 val PER: 0.2268
2026-01-08 20:07:29,006: t15.2023.10.20 val PER: 0.2383
2026-01-08 20:07:29,006: t15.2023.10.22 val PER: 0.1837
2026-01-08 20:07:29,006: t15.2023.11.03 val PER: 0.2408
2026-01-08 20:07:29,007: t15.2023.11.04 val PER: 0.0614
2026-01-08 20:07:29,007: t15.2023.11.17 val PER: 0.0995
2026-01-08 20:07:29,007: t15.2023.11.19 val PER: 0.0998
2026-01-08 20:07:29,007: t15.2023.11.26 val PER: 0.2623
2026-01-08 20:07:29,007: t15.2023.12.03 val PER: 0.2101
2026-01-08 20:07:29,007: t15.2023.12.08 val PER: 0.1984
2026-01-08 20:07:29,007: t15.2023.12.10 val PER: 0.1827
2026-01-08 20:07:29,007: t15.2023.12.17 val PER: 0.2277
2026-01-08 20:07:29,007: t15.2023.12.29 val PER: 0.2430
2026-01-08 20:07:29,007: t15.2024.02.25 val PER: 0.1854
2026-01-08 20:07:29,007: t15.2024.03.08 val PER: 0.3144
2026-01-08 20:07:29,007: t15.2024.03.15 val PER: 0.2858
2026-01-08 20:07:29,007: t15.2024.03.17 val PER: 0.2357
2026-01-08 20:07:29,007: t15.2024.05.10 val PER: 0.2407
2026-01-08 20:07:29,008: t15.2024.06.14 val PER: 0.2413
2026-01-08 20:07:29,008: t15.2024.07.19 val PER: 0.3382
2026-01-08 20:07:29,008: t15.2024.07.21 val PER: 0.1766
2026-01-08 20:07:29,008: t15.2024.07.28 val PER: 0.2243
2026-01-08 20:07:29,008: t15.2025.01.10 val PER: 0.4132
2026-01-08 20:07:29,009: t15.2025.01.12 val PER: 0.2648
2026-01-08 20:07:29,009: t15.2025.03.14 val PER: 0.3979
2026-01-08 20:07:29,009: t15.2025.03.16 val PER: 0.2866
2026-01-08 20:07:29,009: t15.2025.03.30 val PER: 0.4126
2026-01-08 20:07:29,009: t15.2025.04.13 val PER: 0.2810
2026-01-08 20:07:29,010: New best val WER(5gram) 22.69% --> 22.29%
2026-01-08 20:07:29,218: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_4500
2026-01-08 20:07:37,789: Train batch 4600: loss: 19.98 grad norm: 70.43 time: 0.063
2026-01-08 20:07:55,153: Train batch 4800: loss: 13.57 grad norm: 51.38 time: 0.064
2026-01-08 20:08:12,810: Train batch 5000: loss: 32.22 grad norm: 91.20 time: 0.065
2026-01-08 20:08:12,810: Running test after training batch: 5000
2026-01-08 20:08:12,939: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:08:17,932: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:08:17,992: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost net
2026-01-08 20:08:30,436: Val batch 5000: PER (avg): 0.2249 CTC Loss (avg): 21.9988 WER(5gram): 21.19% (n=256) time: 17.626
2026-01-08 20:08:30,437: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 20:08:30,437: t15.2023.08.13 val PER: 0.1913
2026-01-08 20:08:30,437: t15.2023.08.18 val PER: 0.1735
2026-01-08 20:08:30,437: t15.2023.08.20 val PER: 0.1676
2026-01-08 20:08:30,438: t15.2023.08.25 val PER: 0.1205
2026-01-08 20:08:30,438: t15.2023.08.27 val PER: 0.2395
2026-01-08 20:08:30,438: t15.2023.09.01 val PER: 0.1404
2026-01-08 20:08:30,438: t15.2023.09.03 val PER: 0.2340
2026-01-08 20:08:30,438: t15.2023.09.24 val PER: 0.1760
2026-01-08 20:08:30,438: t15.2023.09.29 val PER: 0.1806
2026-01-08 20:08:30,438: t15.2023.10.01 val PER: 0.2424
2026-01-08 20:08:30,438: t15.2023.10.06 val PER: 0.1475
2026-01-08 20:08:30,439: t15.2023.10.08 val PER: 0.3126
2026-01-08 20:08:30,439: t15.2023.10.13 val PER: 0.2785
2026-01-08 20:08:30,439: t15.2023.10.15 val PER: 0.2274
2026-01-08 20:08:30,439: t15.2023.10.20 val PER: 0.2215
2026-01-08 20:08:30,439: t15.2023.10.22 val PER: 0.1693
2026-01-08 20:08:30,439: t15.2023.11.03 val PER: 0.2280
2026-01-08 20:08:30,439: t15.2023.11.04 val PER: 0.0478
2026-01-08 20:08:30,440: t15.2023.11.17 val PER: 0.0855
2026-01-08 20:08:30,440: t15.2023.11.19 val PER: 0.0838
2026-01-08 20:08:30,440: t15.2023.11.26 val PER: 0.2290
2026-01-08 20:08:30,440: t15.2023.12.03 val PER: 0.2027
2026-01-08 20:08:30,440: t15.2023.12.08 val PER: 0.1964
2026-01-08 20:08:30,440: t15.2023.12.10 val PER: 0.1537
2026-01-08 20:08:30,440: t15.2023.12.17 val PER: 0.2245
2026-01-08 20:08:30,440: t15.2023.12.29 val PER: 0.2217
2026-01-08 20:08:30,440: t15.2024.02.25 val PER: 0.1742
2026-01-08 20:08:30,441: t15.2024.03.08 val PER: 0.3115
2026-01-08 20:08:30,441: t15.2024.03.15 val PER: 0.2789
2026-01-08 20:08:30,441: t15.2024.03.17 val PER: 0.2336
2026-01-08 20:08:30,441: t15.2024.05.10 val PER: 0.2437
2026-01-08 20:08:30,441: t15.2024.06.14 val PER: 0.2492
2026-01-08 20:08:30,441: t15.2024.07.19 val PER: 0.3309
2026-01-08 20:08:30,441: t15.2024.07.21 val PER: 0.1772
2026-01-08 20:08:30,441: t15.2024.07.28 val PER: 0.2103
2026-01-08 20:08:30,441: t15.2025.01.10 val PER: 0.3829
2026-01-08 20:08:30,442: t15.2025.01.12 val PER: 0.2394
2026-01-08 20:08:30,442: t15.2025.03.14 val PER: 0.3920
2026-01-08 20:08:30,442: t15.2025.03.16 val PER: 0.2670
2026-01-08 20:08:30,442: t15.2025.03.30 val PER: 0.4057
2026-01-08 20:08:30,442: t15.2025.04.13 val PER: 0.3024
2026-01-08 20:08:30,442: New best val WER(5gram) 22.29% --> 21.19%
2026-01-08 20:08:30,636: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_5000
2026-01-08 20:08:47,744: Train batch 5200: loss: 16.66 grad norm: 58.90 time: 0.053
2026-01-08 20:09:05,237: Train batch 5400: loss: 17.85 grad norm: 61.93 time: 0.068
2026-01-08 20:09:14,078: Running test after training batch: 5500
2026-01-08 20:09:14,176: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:09:19,214: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 20:09:19,259: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost net
2026-01-08 20:09:31,706: Val batch 5500: PER (avg): 0.2155 CTC Loss (avg): 21.1061 WER(5gram): 19.75% (n=256) time: 17.628
2026-01-08 20:09:31,706: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 20:09:31,707: t15.2023.08.13 val PER: 0.1809
2026-01-08 20:09:31,707: t15.2023.08.18 val PER: 0.1618
2026-01-08 20:09:31,707: t15.2023.08.20 val PER: 0.1652
2026-01-08 20:09:31,707: t15.2023.08.25 val PER: 0.1265
2026-01-08 20:09:31,707: t15.2023.08.27 val PER: 0.2444
2026-01-08 20:09:31,707: t15.2023.09.01 val PER: 0.1274
2026-01-08 20:09:31,707: t15.2023.09.03 val PER: 0.2150
2026-01-08 20:09:31,707: t15.2023.09.24 val PER: 0.1723
2026-01-08 20:09:31,707: t15.2023.09.29 val PER: 0.1774
2026-01-08 20:09:31,707: t15.2023.10.01 val PER: 0.2305
2026-01-08 20:09:31,707: t15.2023.10.06 val PER: 0.1335
2026-01-08 20:09:31,707: t15.2023.10.08 val PER: 0.2896
2026-01-08 20:09:31,707: t15.2023.10.13 val PER: 0.2816
2026-01-08 20:09:31,708: t15.2023.10.15 val PER: 0.2129
2026-01-08 20:09:31,708: t15.2023.10.20 val PER: 0.2383
2026-01-08 20:09:31,708: t15.2023.10.22 val PER: 0.1682
2026-01-08 20:09:31,708: t15.2023.11.03 val PER: 0.2273
2026-01-08 20:09:31,708: t15.2023.11.04 val PER: 0.0614
2026-01-08 20:09:31,708: t15.2023.11.17 val PER: 0.0747
2026-01-08 20:09:31,708: t15.2023.11.19 val PER: 0.0758
2026-01-08 20:09:31,708: t15.2023.11.26 val PER: 0.2203
2026-01-08 20:09:31,708: t15.2023.12.03 val PER: 0.1828
2026-01-08 20:09:31,708: t15.2023.12.08 val PER: 0.1911
2026-01-08 20:09:31,709: t15.2023.12.10 val PER: 0.1669
2026-01-08 20:09:31,709: t15.2023.12.17 val PER: 0.2089
2026-01-08 20:09:31,709: t15.2023.12.29 val PER: 0.2114
2026-01-08 20:09:31,709: t15.2024.02.25 val PER: 0.1784
2026-01-08 20:09:31,709: t15.2024.03.08 val PER: 0.2831
2026-01-08 20:09:31,709: t15.2024.03.15 val PER: 0.2564
2026-01-08 20:09:31,709: t15.2024.03.17 val PER: 0.2218
2026-01-08 20:09:31,709: t15.2024.05.10 val PER: 0.2125
2026-01-08 20:09:31,709: t15.2024.06.14 val PER: 0.2334
2026-01-08 20:09:31,709: t15.2024.07.19 val PER: 0.3289
2026-01-08 20:09:31,709: t15.2024.07.21 val PER: 0.1572
2026-01-08 20:09:31,709: t15.2024.07.28 val PER: 0.2103
2026-01-08 20:09:31,710: t15.2025.01.10 val PER: 0.3884
2026-01-08 20:09:31,710: t15.2025.01.12 val PER: 0.2371
2026-01-08 20:09:31,710: t15.2025.03.14 val PER: 0.3698
2026-01-08 20:09:31,710: t15.2025.03.16 val PER: 0.2762
2026-01-08 20:09:31,710: t15.2025.03.30 val PER: 0.3598
2026-01-08 20:09:31,710: t15.2025.04.13 val PER: 0.2767
2026-01-08 20:09:31,711: New best val WER(5gram) 21.19% --> 19.75%
2026-01-08 20:09:31,902: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_5500
2026-01-08 20:09:40,353: Train batch 5600: loss: 19.45 grad norm: 69.93 time: 0.073
2026-01-08 20:09:57,486: Train batch 5800: loss: 13.48 grad norm: 55.66 time: 0.083
2026-01-08 20:10:14,468: Train batch 6000: loss: 14.54 grad norm: 55.24 time: 0.049
2026-01-08 20:10:14,468: Running test after training batch: 6000
2026-01-08 20:10:14,573: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:10:19,586: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:10:19,646: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:10:31,108: Val batch 6000: PER (avg): 0.2103 CTC Loss (avg): 20.6309 WER(5gram): 22.03% (n=256) time: 16.640
2026-01-08 20:10:31,109: WER lens: avg_true_words=5.99 avg_pred_words=6.24 max_pred_words=12
2026-01-08 20:10:31,109: t15.2023.08.13 val PER: 0.1778
2026-01-08 20:10:31,109: t15.2023.08.18 val PER: 0.1551
2026-01-08 20:10:31,109: t15.2023.08.20 val PER: 0.1684
2026-01-08 20:10:31,109: t15.2023.08.25 val PER: 0.1205
2026-01-08 20:10:31,109: t15.2023.08.27 val PER: 0.2460
2026-01-08 20:10:31,109: t15.2023.09.01 val PER: 0.1347
2026-01-08 20:10:31,109: t15.2023.09.03 val PER: 0.2067
2026-01-08 20:10:31,109: t15.2023.09.24 val PER: 0.1675
2026-01-08 20:10:31,109: t15.2023.09.29 val PER: 0.1761
2026-01-08 20:10:31,109: t15.2023.10.01 val PER: 0.2285
2026-01-08 20:10:31,109: t15.2023.10.06 val PER: 0.1292
2026-01-08 20:10:31,110: t15.2023.10.08 val PER: 0.2882
2026-01-08 20:10:31,110: t15.2023.10.13 val PER: 0.2645
2026-01-08 20:10:31,110: t15.2023.10.15 val PER: 0.2156
2026-01-08 20:10:31,110: t15.2023.10.20 val PER: 0.2181
2026-01-08 20:10:31,110: t15.2023.10.22 val PER: 0.1682
2026-01-08 20:10:31,110: t15.2023.11.03 val PER: 0.2185
2026-01-08 20:10:31,110: t15.2023.11.04 val PER: 0.0546
2026-01-08 20:10:31,110: t15.2023.11.17 val PER: 0.0793
2026-01-08 20:10:31,110: t15.2023.11.19 val PER: 0.0838
2026-01-08 20:10:31,110: t15.2023.11.26 val PER: 0.2232
2026-01-08 20:10:31,110: t15.2023.12.03 val PER: 0.1712
2026-01-08 20:10:31,110: t15.2023.12.08 val PER: 0.1738
2026-01-08 20:10:31,111: t15.2023.12.10 val PER: 0.1445
2026-01-08 20:10:31,111: t15.2023.12.17 val PER: 0.1975
2026-01-08 20:10:31,111: t15.2023.12.29 val PER: 0.2073
2026-01-08 20:10:31,111: t15.2024.02.25 val PER: 0.1587
2026-01-08 20:10:31,111: t15.2024.03.08 val PER: 0.2959
2026-01-08 20:10:31,111: t15.2024.03.15 val PER: 0.2614
2026-01-08 20:10:31,111: t15.2024.03.17 val PER: 0.2113
2026-01-08 20:10:31,111: t15.2024.05.10 val PER: 0.2169
2026-01-08 20:10:31,111: t15.2024.06.14 val PER: 0.2192
2026-01-08 20:10:31,111: t15.2024.07.19 val PER: 0.3111
2026-01-08 20:10:31,111: t15.2024.07.21 val PER: 0.1579
2026-01-08 20:10:31,111: t15.2024.07.28 val PER: 0.2000
2026-01-08 20:10:31,112: t15.2025.01.10 val PER: 0.3829
2026-01-08 20:10:31,112: t15.2025.01.12 val PER: 0.2217
2026-01-08 20:10:31,112: t15.2025.03.14 val PER: 0.3817
2026-01-08 20:10:31,112: t15.2025.03.16 val PER: 0.2539
2026-01-08 20:10:31,112: t15.2025.03.30 val PER: 0.3678
2026-01-08 20:10:31,112: t15.2025.04.13 val PER: 0.2611
2026-01-08 20:10:31,258: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_6000
2026-01-08 20:10:49,561: Train batch 6200: loss: 16.30 grad norm: 58.60 time: 0.071
2026-01-08 20:11:07,136: Train batch 6400: loss: 18.50 grad norm: 66.91 time: 0.063
2026-01-08 20:11:15,666: Running test after training batch: 6500
2026-01-08 20:11:15,801: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:11:20,810: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:11:20,865: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 20:11:32,658: Val batch 6500: PER (avg): 0.2042 CTC Loss (avg): 19.9327 WER(5gram): 19.23% (n=256) time: 16.992
2026-01-08 20:11:32,659: WER lens: avg_true_words=5.99 avg_pred_words=6.22 max_pred_words=12
2026-01-08 20:11:32,659: t15.2023.08.13 val PER: 0.1684
2026-01-08 20:11:32,659: t15.2023.08.18 val PER: 0.1433
2026-01-08 20:11:32,659: t15.2023.08.20 val PER: 0.1636
2026-01-08 20:11:32,659: t15.2023.08.25 val PER: 0.1114
2026-01-08 20:11:32,659: t15.2023.08.27 val PER: 0.2267
2026-01-08 20:11:32,660: t15.2023.09.01 val PER: 0.1234
2026-01-08 20:11:32,660: t15.2023.09.03 val PER: 0.2019
2026-01-08 20:11:32,660: t15.2023.09.24 val PER: 0.1711
2026-01-08 20:11:32,660: t15.2023.09.29 val PER: 0.1698
2026-01-08 20:11:32,660: t15.2023.10.01 val PER: 0.2252
2026-01-08 20:11:32,660: t15.2023.10.06 val PER: 0.1302
2026-01-08 20:11:32,660: t15.2023.10.08 val PER: 0.3139
2026-01-08 20:11:32,660: t15.2023.10.13 val PER: 0.2723
2026-01-08 20:11:32,660: t15.2023.10.15 val PER: 0.2103
2026-01-08 20:11:32,660: t15.2023.10.20 val PER: 0.2114
2026-01-08 20:11:32,660: t15.2023.10.22 val PER: 0.1637
2026-01-08 20:11:32,660: t15.2023.11.03 val PER: 0.2090
2026-01-08 20:11:32,660: t15.2023.11.04 val PER: 0.0478
2026-01-08 20:11:32,660: t15.2023.11.17 val PER: 0.0622
2026-01-08 20:11:32,661: t15.2023.11.19 val PER: 0.0719
2026-01-08 20:11:32,661: t15.2023.11.26 val PER: 0.2080
2026-01-08 20:11:32,661: t15.2023.12.03 val PER: 0.1723
2026-01-08 20:11:32,661: t15.2023.12.08 val PER: 0.1711
2026-01-08 20:11:32,661: t15.2023.12.10 val PER: 0.1406
2026-01-08 20:11:32,661: t15.2023.12.17 val PER: 0.1861
2026-01-08 20:11:32,661: t15.2023.12.29 val PER: 0.1956
2026-01-08 20:11:32,661: t15.2024.02.25 val PER: 0.1629
2026-01-08 20:11:32,661: t15.2024.03.08 val PER: 0.2859
2026-01-08 20:11:32,661: t15.2024.03.15 val PER: 0.2595
2026-01-08 20:11:32,661: t15.2024.03.17 val PER: 0.2029
2026-01-08 20:11:32,661: t15.2024.05.10 val PER: 0.2184
2026-01-08 20:11:32,661: t15.2024.06.14 val PER: 0.2192
2026-01-08 20:11:32,661: t15.2024.07.19 val PER: 0.3111
2026-01-08 20:11:32,662: t15.2024.07.21 val PER: 0.1517
2026-01-08 20:11:32,662: t15.2024.07.28 val PER: 0.1816
2026-01-08 20:11:32,662: t15.2025.01.10 val PER: 0.3581
2026-01-08 20:11:32,662: t15.2025.01.12 val PER: 0.2109
2026-01-08 20:11:32,662: t15.2025.03.14 val PER: 0.3935
2026-01-08 20:11:32,662: t15.2025.03.16 val PER: 0.2421
2026-01-08 20:11:32,662: t15.2025.03.30 val PER: 0.3437
2026-01-08 20:11:32,662: t15.2025.04.13 val PER: 0.2639
2026-01-08 20:11:32,663: New best val WER(5gram) 19.75% --> 19.23%
2026-01-08 20:11:32,856: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_6500
2026-01-08 20:11:41,197: Train batch 6600: loss: 12.62 grad norm: 56.39 time: 0.046
2026-01-08 20:11:58,943: Train batch 6800: loss: 15.00 grad norm: 54.80 time: 0.049
2026-01-08 20:12:16,571: Train batch 7000: loss: 17.51 grad norm: 64.26 time: 0.062
2026-01-08 20:12:16,571: Running test after training batch: 7000
2026-01-08 20:12:16,722: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:12:21,805: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:12:21,849: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost net
2026-01-08 20:12:33,199: Val batch 7000: PER (avg): 0.1961 CTC Loss (avg): 19.1967 WER(5gram): 18.71% (n=256) time: 16.627
2026-01-08 20:12:33,199: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 20:12:33,199: t15.2023.08.13 val PER: 0.1632
2026-01-08 20:12:33,199: t15.2023.08.18 val PER: 0.1433
2026-01-08 20:12:33,200: t15.2023.08.20 val PER: 0.1485
2026-01-08 20:12:33,200: t15.2023.08.25 val PER: 0.1069
2026-01-08 20:12:33,200: t15.2023.08.27 val PER: 0.2138
2026-01-08 20:12:33,200: t15.2023.09.01 val PER: 0.1161
2026-01-08 20:12:33,200: t15.2023.09.03 val PER: 0.1888
2026-01-08 20:12:33,200: t15.2023.09.24 val PER: 0.1614
2026-01-08 20:12:33,200: t15.2023.09.29 val PER: 0.1723
2026-01-08 20:12:33,200: t15.2023.10.01 val PER: 0.2140
2026-01-08 20:12:33,200: t15.2023.10.06 val PER: 0.1195
2026-01-08 20:12:33,200: t15.2023.10.08 val PER: 0.2747
2026-01-08 20:12:33,200: t15.2023.10.13 val PER: 0.2599
2026-01-08 20:12:33,200: t15.2023.10.15 val PER: 0.1978
2026-01-08 20:12:33,200: t15.2023.10.20 val PER: 0.2114
2026-01-08 20:12:33,200: t15.2023.10.22 val PER: 0.1403
2026-01-08 20:12:33,200: t15.2023.11.03 val PER: 0.2008
2026-01-08 20:12:33,201: t15.2023.11.04 val PER: 0.0444
2026-01-08 20:12:33,201: t15.2023.11.17 val PER: 0.0747
2026-01-08 20:12:33,201: t15.2023.11.19 val PER: 0.0599
2026-01-08 20:12:33,201: t15.2023.11.26 val PER: 0.1993
2026-01-08 20:12:33,201: t15.2023.12.03 val PER: 0.1754
2026-01-08 20:12:33,201: t15.2023.12.08 val PER: 0.1571
2026-01-08 20:12:33,201: t15.2023.12.10 val PER: 0.1459
2026-01-08 20:12:33,201: t15.2023.12.17 val PER: 0.1757
2026-01-08 20:12:33,201: t15.2023.12.29 val PER: 0.1997
2026-01-08 20:12:33,201: t15.2024.02.25 val PER: 0.1629
2026-01-08 20:12:33,201: t15.2024.03.08 val PER: 0.2888
2026-01-08 20:12:33,201: t15.2024.03.15 val PER: 0.2427
2026-01-08 20:12:33,201: t15.2024.03.17 val PER: 0.1946
2026-01-08 20:12:33,201: t15.2024.05.10 val PER: 0.1902
2026-01-08 20:12:33,202: t15.2024.06.14 val PER: 0.2256
2026-01-08 20:12:33,202: t15.2024.07.19 val PER: 0.2973
2026-01-08 20:12:33,202: t15.2024.07.21 val PER: 0.1297
2026-01-08 20:12:33,202: t15.2024.07.28 val PER: 0.1831
2026-01-08 20:12:33,202: t15.2025.01.10 val PER: 0.3499
2026-01-08 20:12:33,202: t15.2025.01.12 val PER: 0.2125
2026-01-08 20:12:33,202: t15.2025.03.14 val PER: 0.3609
2026-01-08 20:12:33,202: t15.2025.03.16 val PER: 0.2395
2026-01-08 20:12:33,202: t15.2025.03.30 val PER: 0.3529
2026-01-08 20:12:33,202: t15.2025.04.13 val PER: 0.2682
2026-01-08 20:12:33,204: New best val WER(5gram) 19.23% --> 18.71%
2026-01-08 20:12:33,402: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_7000
2026-01-08 20:12:51,115: Train batch 7200: loss: 14.17 grad norm: 61.78 time: 0.080
2026-01-08 20:13:08,154: Train batch 7400: loss: 13.58 grad norm: 53.45 time: 0.076
2026-01-08 20:13:17,079: Running test after training batch: 7500
2026-01-08 20:13:17,200: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:13:22,450: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:13:22,497: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:13:34,043: Val batch 7500: PER (avg): 0.1897 CTC Loss (avg): 18.6745 WER(5gram): 18.12% (n=256) time: 16.963
2026-01-08 20:13:34,044: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-08 20:13:34,044: t15.2023.08.13 val PER: 0.1424
2026-01-08 20:13:34,044: t15.2023.08.18 val PER: 0.1366
2026-01-08 20:13:34,044: t15.2023.08.20 val PER: 0.1477
2026-01-08 20:13:34,044: t15.2023.08.25 val PER: 0.1024
2026-01-08 20:13:34,044: t15.2023.08.27 val PER: 0.2042
2026-01-08 20:13:34,044: t15.2023.09.01 val PER: 0.1144
2026-01-08 20:13:34,044: t15.2023.09.03 val PER: 0.1841
2026-01-08 20:13:34,044: t15.2023.09.24 val PER: 0.1505
2026-01-08 20:13:34,045: t15.2023.09.29 val PER: 0.1634
2026-01-08 20:13:34,045: t15.2023.10.01 val PER: 0.2034
2026-01-08 20:13:34,045: t15.2023.10.06 val PER: 0.1184
2026-01-08 20:13:34,045: t15.2023.10.08 val PER: 0.2882
2026-01-08 20:13:34,045: t15.2023.10.13 val PER: 0.2568
2026-01-08 20:13:34,045: t15.2023.10.15 val PER: 0.1839
2026-01-08 20:13:34,045: t15.2023.10.20 val PER: 0.1946
2026-01-08 20:13:34,045: t15.2023.10.22 val PER: 0.1470
2026-01-08 20:13:34,045: t15.2023.11.03 val PER: 0.2062
2026-01-08 20:13:34,045: t15.2023.11.04 val PER: 0.0512
2026-01-08 20:13:34,045: t15.2023.11.17 val PER: 0.0653
2026-01-08 20:13:34,045: t15.2023.11.19 val PER: 0.0479
2026-01-08 20:13:34,045: t15.2023.11.26 val PER: 0.1826
2026-01-08 20:13:34,045: t15.2023.12.03 val PER: 0.1523
2026-01-08 20:13:34,045: t15.2023.12.08 val PER: 0.1618
2026-01-08 20:13:34,046: t15.2023.12.10 val PER: 0.1261
2026-01-08 20:13:34,046: t15.2023.12.17 val PER: 0.1736
2026-01-08 20:13:34,046: t15.2023.12.29 val PER: 0.1805
2026-01-08 20:13:34,046: t15.2024.02.25 val PER: 0.1545
2026-01-08 20:13:34,046: t15.2024.03.08 val PER: 0.2560
2026-01-08 20:13:34,046: t15.2024.03.15 val PER: 0.2508
2026-01-08 20:13:34,046: t15.2024.03.17 val PER: 0.1848
2026-01-08 20:13:34,046: t15.2024.05.10 val PER: 0.2036
2026-01-08 20:13:34,046: t15.2024.06.14 val PER: 0.2082
2026-01-08 20:13:34,046: t15.2024.07.19 val PER: 0.2947
2026-01-08 20:13:34,046: t15.2024.07.21 val PER: 0.1276
2026-01-08 20:13:34,046: t15.2024.07.28 val PER: 0.1669
2026-01-08 20:13:34,046: t15.2025.01.10 val PER: 0.3402
2026-01-08 20:13:34,046: t15.2025.01.12 val PER: 0.1971
2026-01-08 20:13:34,046: t15.2025.03.14 val PER: 0.3609
2026-01-08 20:13:34,047: t15.2025.03.16 val PER: 0.2487
2026-01-08 20:13:34,047: t15.2025.03.30 val PER: 0.3598
2026-01-08 20:13:34,047: t15.2025.04.13 val PER: 0.2553
2026-01-08 20:13:34,048: New best val WER(5gram) 18.71% --> 18.12%
2026-01-08 20:13:34,241: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_7500
2026-01-08 20:13:42,987: Train batch 7600: loss: 15.75 grad norm: 58.89 time: 0.069
2026-01-08 20:14:00,408: Train batch 7800: loss: 14.38 grad norm: 57.12 time: 0.055
2026-01-08 20:14:17,717: Train batch 8000: loss: 11.25 grad norm: 52.32 time: 0.072
2026-01-08 20:14:17,718: Running test after training batch: 8000
2026-01-08 20:14:17,818: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:14:23,076: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:14:23,131: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:14:34,221: Val batch 8000: PER (avg): 0.1845 CTC Loss (avg): 18.0647 WER(5gram): 18.06% (n=256) time: 16.503
2026-01-08 20:14:34,221: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=12
2026-01-08 20:14:34,222: t15.2023.08.13 val PER: 0.1476
2026-01-08 20:14:34,222: t15.2023.08.18 val PER: 0.1282
2026-01-08 20:14:34,222: t15.2023.08.20 val PER: 0.1477
2026-01-08 20:14:34,222: t15.2023.08.25 val PER: 0.1114
2026-01-08 20:14:34,222: t15.2023.08.27 val PER: 0.2170
2026-01-08 20:14:34,222: t15.2023.09.01 val PER: 0.1063
2026-01-08 20:14:34,222: t15.2023.09.03 val PER: 0.2007
2026-01-08 20:14:34,222: t15.2023.09.24 val PER: 0.1493
2026-01-08 20:14:34,222: t15.2023.09.29 val PER: 0.1532
2026-01-08 20:14:34,223: t15.2023.10.01 val PER: 0.2041
2026-01-08 20:14:34,223: t15.2023.10.06 val PER: 0.1076
2026-01-08 20:14:34,223: t15.2023.10.08 val PER: 0.2679
2026-01-08 20:14:34,223: t15.2023.10.13 val PER: 0.2444
2026-01-08 20:14:34,223: t15.2023.10.15 val PER: 0.1951
2026-01-08 20:14:34,223: t15.2023.10.20 val PER: 0.2047
2026-01-08 20:14:34,223: t15.2023.10.22 val PER: 0.1370
2026-01-08 20:14:34,223: t15.2023.11.03 val PER: 0.1981
2026-01-08 20:14:34,223: t15.2023.11.04 val PER: 0.0410
2026-01-08 20:14:34,223: t15.2023.11.17 val PER: 0.0669
2026-01-08 20:14:34,223: t15.2023.11.19 val PER: 0.0619
2026-01-08 20:14:34,223: t15.2023.11.26 val PER: 0.1804
2026-01-08 20:14:34,223: t15.2023.12.03 val PER: 0.1502
2026-01-08 20:14:34,223: t15.2023.12.08 val PER: 0.1478
2026-01-08 20:14:34,223: t15.2023.12.10 val PER: 0.1209
2026-01-08 20:14:34,223: t15.2023.12.17 val PER: 0.1736
2026-01-08 20:14:34,224: t15.2023.12.29 val PER: 0.1654
2026-01-08 20:14:34,224: t15.2024.02.25 val PER: 0.1390
2026-01-08 20:14:34,224: t15.2024.03.08 val PER: 0.2603
2026-01-08 20:14:34,224: t15.2024.03.15 val PER: 0.2383
2026-01-08 20:14:34,224: t15.2024.03.17 val PER: 0.1736
2026-01-08 20:14:34,224: t15.2024.05.10 val PER: 0.2021
2026-01-08 20:14:34,224: t15.2024.06.14 val PER: 0.2114
2026-01-08 20:14:34,224: t15.2024.07.19 val PER: 0.2933
2026-01-08 20:14:34,224: t15.2024.07.21 val PER: 0.1124
2026-01-08 20:14:34,224: t15.2024.07.28 val PER: 0.1588
2026-01-08 20:14:34,224: t15.2025.01.10 val PER: 0.3347
2026-01-08 20:14:34,224: t15.2025.01.12 val PER: 0.1917
2026-01-08 20:14:34,224: t15.2025.03.14 val PER: 0.3521
2026-01-08 20:14:34,224: t15.2025.03.16 val PER: 0.2225
2026-01-08 20:14:34,225: t15.2025.03.30 val PER: 0.3448
2026-01-08 20:14:34,225: t15.2025.04.13 val PER: 0.2596
2026-01-08 20:14:34,226: New best val WER(5gram) 18.12% --> 18.06%
2026-01-08 20:14:34,419: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_8000
2026-01-08 20:14:51,766: Train batch 8200: loss: 9.61 grad norm: 46.38 time: 0.055
2026-01-08 20:15:09,076: Train batch 8400: loss: 10.47 grad norm: 49.40 time: 0.064
2026-01-08 20:15:18,249: Running test after training batch: 8500
2026-01-08 20:15:18,353: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:15:23,658: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:15:23,707: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 20:15:35,162: Val batch 8500: PER (avg): 0.1808 CTC Loss (avg): 17.9079 WER(5gram): 16.49% (n=256) time: 16.912
2026-01-08 20:15:35,162: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 20:15:35,163: t15.2023.08.13 val PER: 0.1403
2026-01-08 20:15:35,163: t15.2023.08.18 val PER: 0.1542
2026-01-08 20:15:35,163: t15.2023.08.20 val PER: 0.1366
2026-01-08 20:15:35,163: t15.2023.08.25 val PER: 0.1205
2026-01-08 20:15:35,163: t15.2023.08.27 val PER: 0.2154
2026-01-08 20:15:35,163: t15.2023.09.01 val PER: 0.1039
2026-01-08 20:15:35,163: t15.2023.09.03 val PER: 0.1983
2026-01-08 20:15:35,163: t15.2023.09.24 val PER: 0.1541
2026-01-08 20:15:35,163: t15.2023.09.29 val PER: 0.1538
2026-01-08 20:15:35,163: t15.2023.10.01 val PER: 0.1935
2026-01-08 20:15:35,163: t15.2023.10.06 val PER: 0.1087
2026-01-08 20:15:35,163: t15.2023.10.08 val PER: 0.2760
2026-01-08 20:15:35,163: t15.2023.10.13 val PER: 0.2397
2026-01-08 20:15:35,163: t15.2023.10.15 val PER: 0.1885
2026-01-08 20:15:35,163: t15.2023.10.20 val PER: 0.1946
2026-01-08 20:15:35,164: t15.2023.10.22 val PER: 0.1381
2026-01-08 20:15:35,164: t15.2023.11.03 val PER: 0.1913
2026-01-08 20:15:35,164: t15.2023.11.04 val PER: 0.0444
2026-01-08 20:15:35,164: t15.2023.11.17 val PER: 0.0653
2026-01-08 20:15:35,164: t15.2023.11.19 val PER: 0.0459
2026-01-08 20:15:35,164: t15.2023.11.26 val PER: 0.1790
2026-01-08 20:15:35,164: t15.2023.12.03 val PER: 0.1397
2026-01-08 20:15:35,164: t15.2023.12.08 val PER: 0.1405
2026-01-08 20:15:35,164: t15.2023.12.10 val PER: 0.1261
2026-01-08 20:15:35,164: t15.2023.12.17 val PER: 0.1538
2026-01-08 20:15:35,165: t15.2023.12.29 val PER: 0.1682
2026-01-08 20:15:35,165: t15.2024.02.25 val PER: 0.1348
2026-01-08 20:15:35,165: t15.2024.03.08 val PER: 0.2589
2026-01-08 20:15:35,165: t15.2024.03.15 val PER: 0.2276
2026-01-08 20:15:35,165: t15.2024.03.17 val PER: 0.1709
2026-01-08 20:15:35,165: t15.2024.05.10 val PER: 0.1917
2026-01-08 20:15:35,165: t15.2024.06.14 val PER: 0.1798
2026-01-08 20:15:35,165: t15.2024.07.19 val PER: 0.2841
2026-01-08 20:15:35,165: t15.2024.07.21 val PER: 0.1276
2026-01-08 20:15:35,165: t15.2024.07.28 val PER: 0.1669
2026-01-08 20:15:35,165: t15.2025.01.10 val PER: 0.3278
2026-01-08 20:15:35,165: t15.2025.01.12 val PER: 0.1794
2026-01-08 20:15:35,165: t15.2025.03.14 val PER: 0.3536
2026-01-08 20:15:35,165: t15.2025.03.16 val PER: 0.2147
2026-01-08 20:15:35,165: t15.2025.03.30 val PER: 0.3322
2026-01-08 20:15:35,166: t15.2025.04.13 val PER: 0.2340
2026-01-08 20:15:35,167: New best val WER(5gram) 18.06% --> 16.49%
2026-01-08 20:15:35,361: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_8500
2026-01-08 20:15:44,159: Train batch 8600: loss: 15.80 grad norm: 59.48 time: 0.055
2026-01-08 20:16:01,441: Train batch 8800: loss: 14.95 grad norm: 58.58 time: 0.061
2026-01-08 20:16:19,042: Train batch 9000: loss: 15.98 grad norm: 67.65 time: 0.075
2026-01-08 20:16:19,043: Running test after training batch: 9000
2026-01-08 20:16:19,152: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:16:24,486: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:16:24,534: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost net
2026-01-08 20:16:36,294: Val batch 9000: PER (avg): 0.1749 CTC Loss (avg): 17.3146 WER(5gram): 16.88% (n=256) time: 17.251
2026-01-08 20:16:36,294: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-08 20:16:36,295: t15.2023.08.13 val PER: 0.1393
2026-01-08 20:16:36,295: t15.2023.08.18 val PER: 0.1282
2026-01-08 20:16:36,295: t15.2023.08.20 val PER: 0.1334
2026-01-08 20:16:36,295: t15.2023.08.25 val PER: 0.0904
2026-01-08 20:16:36,295: t15.2023.08.27 val PER: 0.2106
2026-01-08 20:16:36,295: t15.2023.09.01 val PER: 0.0974
2026-01-08 20:16:36,295: t15.2023.09.03 val PER: 0.1793
2026-01-08 20:16:36,295: t15.2023.09.24 val PER: 0.1396
2026-01-08 20:16:36,295: t15.2023.09.29 val PER: 0.1474
2026-01-08 20:16:36,295: t15.2023.10.01 val PER: 0.1896
2026-01-08 20:16:36,296: t15.2023.10.06 val PER: 0.0990
2026-01-08 20:16:36,296: t15.2023.10.08 val PER: 0.2706
2026-01-08 20:16:36,296: t15.2023.10.13 val PER: 0.2351
2026-01-08 20:16:36,296: t15.2023.10.15 val PER: 0.1806
2026-01-08 20:16:36,296: t15.2023.10.20 val PER: 0.1913
2026-01-08 20:16:36,296: t15.2023.10.22 val PER: 0.1359
2026-01-08 20:16:36,296: t15.2023.11.03 val PER: 0.1954
2026-01-08 20:16:36,296: t15.2023.11.04 val PER: 0.0410
2026-01-08 20:16:36,296: t15.2023.11.17 val PER: 0.0591
2026-01-08 20:16:36,296: t15.2023.11.19 val PER: 0.0539
2026-01-08 20:16:36,297: t15.2023.11.26 val PER: 0.1710
2026-01-08 20:16:36,297: t15.2023.12.03 val PER: 0.1313
2026-01-08 20:16:36,297: t15.2023.12.08 val PER: 0.1305
2026-01-08 20:16:36,297: t15.2023.12.10 val PER: 0.1117
2026-01-08 20:16:36,297: t15.2023.12.17 val PER: 0.1622
2026-01-08 20:16:36,297: t15.2023.12.29 val PER: 0.1592
2026-01-08 20:16:36,297: t15.2024.02.25 val PER: 0.1489
2026-01-08 20:16:36,297: t15.2024.03.08 val PER: 0.2589
2026-01-08 20:16:36,297: t15.2024.03.15 val PER: 0.2308
2026-01-08 20:16:36,297: t15.2024.03.17 val PER: 0.1736
2026-01-08 20:16:36,298: t15.2024.05.10 val PER: 0.1828
2026-01-08 20:16:36,298: t15.2024.06.14 val PER: 0.1893
2026-01-08 20:16:36,298: t15.2024.07.19 val PER: 0.2690
2026-01-08 20:16:36,298: t15.2024.07.21 val PER: 0.1145
2026-01-08 20:16:36,298: t15.2024.07.28 val PER: 0.1537
2026-01-08 20:16:36,298: t15.2025.01.10 val PER: 0.3168
2026-01-08 20:16:36,298: t15.2025.01.12 val PER: 0.1747
2026-01-08 20:16:36,298: t15.2025.03.14 val PER: 0.3447
2026-01-08 20:16:36,298: t15.2025.03.16 val PER: 0.2186
2026-01-08 20:16:36,298: t15.2025.03.30 val PER: 0.3264
2026-01-08 20:16:36,298: t15.2025.04.13 val PER: 0.2454
2026-01-08 20:16:36,436: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_9000
2026-01-08 20:16:53,454: Train batch 9200: loss: 11.08 grad norm: 51.36 time: 0.056
2026-01-08 20:17:10,698: Train batch 9400: loss: 7.97 grad norm: 48.51 time: 0.068
2026-01-08 20:17:19,528: Running test after training batch: 9500
2026-01-08 20:17:19,671: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:17:25,049: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:17:25,095: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:17:36,722: Val batch 9500: PER (avg): 0.1735 CTC Loss (avg): 17.2685 WER(5gram): 15.78% (n=256) time: 17.194
2026-01-08 20:17:36,723: WER lens: avg_true_words=5.99 avg_pred_words=6.15 max_pred_words=12
2026-01-08 20:17:36,723: t15.2023.08.13 val PER: 0.1247
2026-01-08 20:17:36,723: t15.2023.08.18 val PER: 0.1249
2026-01-08 20:17:36,723: t15.2023.08.20 val PER: 0.1295
2026-01-08 20:17:36,723: t15.2023.08.25 val PER: 0.1054
2026-01-08 20:17:36,723: t15.2023.08.27 val PER: 0.1977
2026-01-08 20:17:36,723: t15.2023.09.01 val PER: 0.0974
2026-01-08 20:17:36,723: t15.2023.09.03 val PER: 0.1686
2026-01-08 20:17:36,723: t15.2023.09.24 val PER: 0.1420
2026-01-08 20:17:36,723: t15.2023.09.29 val PER: 0.1519
2026-01-08 20:17:36,723: t15.2023.10.01 val PER: 0.1902
2026-01-08 20:17:36,723: t15.2023.10.06 val PER: 0.1044
2026-01-08 20:17:36,724: t15.2023.10.08 val PER: 0.2598
2026-01-08 20:17:36,724: t15.2023.10.13 val PER: 0.2273
2026-01-08 20:17:36,724: t15.2023.10.15 val PER: 0.1767
2026-01-08 20:17:36,724: t15.2023.10.20 val PER: 0.2047
2026-01-08 20:17:36,724: t15.2023.10.22 val PER: 0.1281
2026-01-08 20:17:36,724: t15.2023.11.03 val PER: 0.1872
2026-01-08 20:17:36,724: t15.2023.11.04 val PER: 0.0375
2026-01-08 20:17:36,724: t15.2023.11.17 val PER: 0.0560
2026-01-08 20:17:36,724: t15.2023.11.19 val PER: 0.0459
2026-01-08 20:17:36,724: t15.2023.11.26 val PER: 0.1601
2026-01-08 20:17:36,724: t15.2023.12.03 val PER: 0.1292
2026-01-08 20:17:36,724: t15.2023.12.08 val PER: 0.1372
2026-01-08 20:17:36,724: t15.2023.12.10 val PER: 0.1183
2026-01-08 20:17:36,724: t15.2023.12.17 val PER: 0.1601
2026-01-08 20:17:36,724: t15.2023.12.29 val PER: 0.1537
2026-01-08 20:17:36,725: t15.2024.02.25 val PER: 0.1419
2026-01-08 20:17:36,725: t15.2024.03.08 val PER: 0.2632
2026-01-08 20:17:36,725: t15.2024.03.15 val PER: 0.2258
2026-01-08 20:17:36,725: t15.2024.03.17 val PER: 0.1646
2026-01-08 20:17:36,725: t15.2024.05.10 val PER: 0.1872
2026-01-08 20:17:36,725: t15.2024.06.14 val PER: 0.1735
2026-01-08 20:17:36,725: t15.2024.07.19 val PER: 0.2788
2026-01-08 20:17:36,725: t15.2024.07.21 val PER: 0.1234
2026-01-08 20:17:36,725: t15.2024.07.28 val PER: 0.1625
2026-01-08 20:17:36,725: t15.2025.01.10 val PER: 0.3264
2026-01-08 20:17:36,726: t15.2025.01.12 val PER: 0.1809
2026-01-08 20:17:36,726: t15.2025.03.14 val PER: 0.3609
2026-01-08 20:17:36,726: t15.2025.03.16 val PER: 0.2107
2026-01-08 20:17:36,726: t15.2025.03.30 val PER: 0.3195
2026-01-08 20:17:36,726: t15.2025.04.13 val PER: 0.2354
2026-01-08 20:17:36,727: New best val WER(5gram) 16.49% --> 15.78%
2026-01-08 20:17:36,917: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_9500
2026-01-08 20:17:45,697: Train batch 9600: loss: 8.05 grad norm: 44.35 time: 0.076
2026-01-08 20:18:03,319: Train batch 9800: loss: 11.86 grad norm: 56.10 time: 0.064
2026-01-08 20:18:20,769: Train batch 10000: loss: 5.31 grad norm: 32.66 time: 0.063
2026-01-08 20:18:20,769: Running test after training batch: 10000
2026-01-08 20:18:20,903: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:18:26,000: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:18:26,042: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:18:37,614: Val batch 10000: PER (avg): 0.1711 CTC Loss (avg): 17.0335 WER(5gram): 16.62% (n=256) time: 16.844
2026-01-08 20:18:37,614: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=13
2026-01-08 20:18:37,614: t15.2023.08.13 val PER: 0.1237
2026-01-08 20:18:37,615: t15.2023.08.18 val PER: 0.1299
2026-01-08 20:18:37,615: t15.2023.08.20 val PER: 0.1231
2026-01-08 20:18:37,615: t15.2023.08.25 val PER: 0.1145
2026-01-08 20:18:37,615: t15.2023.08.27 val PER: 0.2010
2026-01-08 20:18:37,615: t15.2023.09.01 val PER: 0.0844
2026-01-08 20:18:37,615: t15.2023.09.03 val PER: 0.1900
2026-01-08 20:18:37,615: t15.2023.09.24 val PER: 0.1408
2026-01-08 20:18:37,615: t15.2023.09.29 val PER: 0.1493
2026-01-08 20:18:37,616: t15.2023.10.01 val PER: 0.1843
2026-01-08 20:18:37,616: t15.2023.10.06 val PER: 0.1087
2026-01-08 20:18:37,616: t15.2023.10.08 val PER: 0.2679
2026-01-08 20:18:37,616: t15.2023.10.13 val PER: 0.2273
2026-01-08 20:18:37,616: t15.2023.10.15 val PER: 0.1674
2026-01-08 20:18:37,616: t15.2023.10.20 val PER: 0.1946
2026-01-08 20:18:37,616: t15.2023.10.22 val PER: 0.1303
2026-01-08 20:18:37,616: t15.2023.11.03 val PER: 0.1927
2026-01-08 20:18:37,616: t15.2023.11.04 val PER: 0.0478
2026-01-08 20:18:37,616: t15.2023.11.17 val PER: 0.0482
2026-01-08 20:18:37,616: t15.2023.11.19 val PER: 0.0439
2026-01-08 20:18:37,616: t15.2023.11.26 val PER: 0.1514
2026-01-08 20:18:37,616: t15.2023.12.03 val PER: 0.1313
2026-01-08 20:18:37,617: t15.2023.12.08 val PER: 0.1358
2026-01-08 20:18:37,617: t15.2023.12.10 val PER: 0.1117
2026-01-08 20:18:37,617: t15.2023.12.17 val PER: 0.1570
2026-01-08 20:18:37,617: t15.2023.12.29 val PER: 0.1524
2026-01-08 20:18:37,617: t15.2024.02.25 val PER: 0.1404
2026-01-08 20:18:37,617: t15.2024.03.08 val PER: 0.2489
2026-01-08 20:18:37,617: t15.2024.03.15 val PER: 0.2164
2026-01-08 20:18:37,617: t15.2024.03.17 val PER: 0.1597
2026-01-08 20:18:37,617: t15.2024.05.10 val PER: 0.1694
2026-01-08 20:18:37,617: t15.2024.06.14 val PER: 0.1767
2026-01-08 20:18:37,617: t15.2024.07.19 val PER: 0.2729
2026-01-08 20:18:37,617: t15.2024.07.21 val PER: 0.1193
2026-01-08 20:18:37,617: t15.2024.07.28 val PER: 0.1507
2026-01-08 20:18:37,617: t15.2025.01.10 val PER: 0.3168
2026-01-08 20:18:37,617: t15.2025.01.12 val PER: 0.1686
2026-01-08 20:18:37,617: t15.2025.03.14 val PER: 0.3595
2026-01-08 20:18:37,618: t15.2025.03.16 val PER: 0.2212
2026-01-08 20:18:37,618: t15.2025.03.30 val PER: 0.3368
2026-01-08 20:18:37,618: t15.2025.04.13 val PER: 0.2425
2026-01-08 20:18:37,754: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_10000
2026-01-08 20:18:54,441: Train batch 10200: loss: 6.26 grad norm: 38.54 time: 0.051
2026-01-08 20:19:11,766: Train batch 10400: loss: 9.33 grad norm: 50.78 time: 0.073
2026-01-08 20:19:20,995: Running test after training batch: 10500
2026-01-08 20:19:21,143: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:19:26,440: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:19:26,483: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 20:19:38,198: Val batch 10500: PER (avg): 0.1667 CTC Loss (avg): 16.7551 WER(5gram): 16.95% (n=256) time: 17.202
2026-01-08 20:19:38,198: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 20:19:38,198: t15.2023.08.13 val PER: 0.1258
2026-01-08 20:19:38,198: t15.2023.08.18 val PER: 0.1123
2026-01-08 20:19:38,198: t15.2023.08.20 val PER: 0.1303
2026-01-08 20:19:38,199: t15.2023.08.25 val PER: 0.1190
2026-01-08 20:19:38,199: t15.2023.08.27 val PER: 0.1994
2026-01-08 20:19:38,199: t15.2023.09.01 val PER: 0.0925
2026-01-08 20:19:38,199: t15.2023.09.03 val PER: 0.1722
2026-01-08 20:19:38,199: t15.2023.09.24 val PER: 0.1468
2026-01-08 20:19:38,199: t15.2023.09.29 val PER: 0.1455
2026-01-08 20:19:38,199: t15.2023.10.01 val PER: 0.1935
2026-01-08 20:19:38,199: t15.2023.10.06 val PER: 0.0926
2026-01-08 20:19:38,199: t15.2023.10.08 val PER: 0.2612
2026-01-08 20:19:38,199: t15.2023.10.13 val PER: 0.2079
2026-01-08 20:19:38,199: t15.2023.10.15 val PER: 0.1721
2026-01-08 20:19:38,199: t15.2023.10.20 val PER: 0.2081
2026-01-08 20:19:38,199: t15.2023.10.22 val PER: 0.1203
2026-01-08 20:19:38,199: t15.2023.11.03 val PER: 0.1866
2026-01-08 20:19:38,199: t15.2023.11.04 val PER: 0.0375
2026-01-08 20:19:38,200: t15.2023.11.17 val PER: 0.0591
2026-01-08 20:19:38,200: t15.2023.11.19 val PER: 0.0439
2026-01-08 20:19:38,200: t15.2023.11.26 val PER: 0.1449
2026-01-08 20:19:38,200: t15.2023.12.03 val PER: 0.1355
2026-01-08 20:19:38,200: t15.2023.12.08 val PER: 0.1232
2026-01-08 20:19:38,200: t15.2023.12.10 val PER: 0.1078
2026-01-08 20:19:38,200: t15.2023.12.17 val PER: 0.1497
2026-01-08 20:19:38,200: t15.2023.12.29 val PER: 0.1551
2026-01-08 20:19:38,200: t15.2024.02.25 val PER: 0.1292
2026-01-08 20:19:38,200: t15.2024.03.08 val PER: 0.2546
2026-01-08 20:19:38,200: t15.2024.03.15 val PER: 0.2158
2026-01-08 20:19:38,200: t15.2024.03.17 val PER: 0.1576
2026-01-08 20:19:38,200: t15.2024.05.10 val PER: 0.1605
2026-01-08 20:19:38,201: t15.2024.06.14 val PER: 0.1688
2026-01-08 20:19:38,201: t15.2024.07.19 val PER: 0.2722
2026-01-08 20:19:38,201: t15.2024.07.21 val PER: 0.1062
2026-01-08 20:19:38,201: t15.2024.07.28 val PER: 0.1419
2026-01-08 20:19:38,201: t15.2025.01.10 val PER: 0.3278
2026-01-08 20:19:38,201: t15.2025.01.12 val PER: 0.1617
2026-01-08 20:19:38,201: t15.2025.03.14 val PER: 0.3550
2026-01-08 20:19:38,201: t15.2025.03.16 val PER: 0.2016
2026-01-08 20:19:38,201: t15.2025.03.30 val PER: 0.3161
2026-01-08 20:19:38,201: t15.2025.04.13 val PER: 0.2211
2026-01-08 20:19:38,341: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_10500
2026-01-08 20:19:47,732: Train batch 10600: loss: 9.44 grad norm: 54.85 time: 0.074
2026-01-08 20:20:06,186: Train batch 10800: loss: 14.82 grad norm: 66.61 time: 0.066
2026-01-08 20:20:24,585: Train batch 11000: loss: 14.45 grad norm: 67.18 time: 0.060
2026-01-08 20:20:24,586: Running test after training batch: 11000
2026-01-08 20:20:24,699: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:20:29,671: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:20:29,722: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-08 20:20:41,003: Val batch 11000: PER (avg): 0.1630 CTC Loss (avg): 16.4267 WER(5gram): 16.69% (n=256) time: 16.417
2026-01-08 20:20:41,004: WER lens: avg_true_words=5.99 avg_pred_words=6.22 max_pred_words=12
2026-01-08 20:20:41,004: t15.2023.08.13 val PER: 0.1195
2026-01-08 20:20:41,004: t15.2023.08.18 val PER: 0.1199
2026-01-08 20:20:41,004: t15.2023.08.20 val PER: 0.1215
2026-01-08 20:20:41,004: t15.2023.08.25 val PER: 0.1024
2026-01-08 20:20:41,004: t15.2023.08.27 val PER: 0.1945
2026-01-08 20:20:41,004: t15.2023.09.01 val PER: 0.0836
2026-01-08 20:20:41,005: t15.2023.09.03 val PER: 0.1758
2026-01-08 20:20:41,005: t15.2023.09.24 val PER: 0.1444
2026-01-08 20:20:41,005: t15.2023.09.29 val PER: 0.1429
2026-01-08 20:20:41,005: t15.2023.10.01 val PER: 0.1902
2026-01-08 20:20:41,005: t15.2023.10.06 val PER: 0.0947
2026-01-08 20:20:41,005: t15.2023.10.08 val PER: 0.2652
2026-01-08 20:20:41,005: t15.2023.10.13 val PER: 0.2149
2026-01-08 20:20:41,005: t15.2023.10.15 val PER: 0.1661
2026-01-08 20:20:41,005: t15.2023.10.20 val PER: 0.1913
2026-01-08 20:20:41,005: t15.2023.10.22 val PER: 0.1214
2026-01-08 20:20:41,005: t15.2023.11.03 val PER: 0.1940
2026-01-08 20:20:41,005: t15.2023.11.04 val PER: 0.0341
2026-01-08 20:20:41,005: t15.2023.11.17 val PER: 0.0467
2026-01-08 20:20:41,005: t15.2023.11.19 val PER: 0.0399
2026-01-08 20:20:41,005: t15.2023.11.26 val PER: 0.1377
2026-01-08 20:20:41,006: t15.2023.12.03 val PER: 0.1271
2026-01-08 20:20:41,006: t15.2023.12.08 val PER: 0.1158
2026-01-08 20:20:41,006: t15.2023.12.10 val PER: 0.1012
2026-01-08 20:20:41,006: t15.2023.12.17 val PER: 0.1476
2026-01-08 20:20:41,006: t15.2023.12.29 val PER: 0.1421
2026-01-08 20:20:41,006: t15.2024.02.25 val PER: 0.1278
2026-01-08 20:20:41,006: t15.2024.03.08 val PER: 0.2390
2026-01-08 20:20:41,006: t15.2024.03.15 val PER: 0.2151
2026-01-08 20:20:41,007: t15.2024.03.17 val PER: 0.1520
2026-01-08 20:20:41,007: t15.2024.05.10 val PER: 0.1694
2026-01-08 20:20:41,007: t15.2024.06.14 val PER: 0.1656
2026-01-08 20:20:41,007: t15.2024.07.19 val PER: 0.2551
2026-01-08 20:20:41,007: t15.2024.07.21 val PER: 0.1062
2026-01-08 20:20:41,007: t15.2024.07.28 val PER: 0.1441
2026-01-08 20:20:41,007: t15.2025.01.10 val PER: 0.3072
2026-01-08 20:20:41,007: t15.2025.01.12 val PER: 0.1624
2026-01-08 20:20:41,007: t15.2025.03.14 val PER: 0.3491
2026-01-08 20:20:41,007: t15.2025.03.16 val PER: 0.1963
2026-01-08 20:20:41,007: t15.2025.03.30 val PER: 0.3046
2026-01-08 20:20:41,007: t15.2025.04.13 val PER: 0.2397
2026-01-08 20:20:41,147: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_11000
2026-01-08 20:20:59,269: Train batch 11200: loss: 10.49 grad norm: 50.44 time: 0.072
2026-01-08 20:21:17,441: Train batch 11400: loss: 9.67 grad norm: 53.24 time: 0.058
2026-01-08 20:21:26,082: Running test after training batch: 11500
2026-01-08 20:21:26,187: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:21:31,216: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:21:31,260: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 20:21:42,754: Val batch 11500: PER (avg): 0.1612 CTC Loss (avg): 16.3789 WER(5gram): 16.17% (n=256) time: 16.671
2026-01-08 20:21:42,754: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 20:21:42,754: t15.2023.08.13 val PER: 0.1227
2026-01-08 20:21:42,754: t15.2023.08.18 val PER: 0.1165
2026-01-08 20:21:42,754: t15.2023.08.20 val PER: 0.1231
2026-01-08 20:21:42,754: t15.2023.08.25 val PER: 0.1099
2026-01-08 20:21:42,755: t15.2023.08.27 val PER: 0.1977
2026-01-08 20:21:42,755: t15.2023.09.01 val PER: 0.0860
2026-01-08 20:21:42,755: t15.2023.09.03 val PER: 0.1675
2026-01-08 20:21:42,755: t15.2023.09.24 val PER: 0.1262
2026-01-08 20:21:42,755: t15.2023.09.29 val PER: 0.1391
2026-01-08 20:21:42,755: t15.2023.10.01 val PER: 0.1843
2026-01-08 20:21:42,755: t15.2023.10.06 val PER: 0.0926
2026-01-08 20:21:42,755: t15.2023.10.08 val PER: 0.2449
2026-01-08 20:21:42,755: t15.2023.10.13 val PER: 0.2180
2026-01-08 20:21:42,755: t15.2023.10.15 val PER: 0.1569
2026-01-08 20:21:42,755: t15.2023.10.20 val PER: 0.2081
2026-01-08 20:21:42,755: t15.2023.10.22 val PER: 0.1203
2026-01-08 20:21:42,755: t15.2023.11.03 val PER: 0.1798
2026-01-08 20:21:42,755: t15.2023.11.04 val PER: 0.0307
2026-01-08 20:21:42,755: t15.2023.11.17 val PER: 0.0498
2026-01-08 20:21:42,755: t15.2023.11.19 val PER: 0.0479
2026-01-08 20:21:42,755: t15.2023.11.26 val PER: 0.1370
2026-01-08 20:21:42,756: t15.2023.12.03 val PER: 0.1176
2026-01-08 20:21:42,756: t15.2023.12.08 val PER: 0.1152
2026-01-08 20:21:42,756: t15.2023.12.10 val PER: 0.0999
2026-01-08 20:21:42,756: t15.2023.12.17 val PER: 0.1559
2026-01-08 20:21:42,756: t15.2023.12.29 val PER: 0.1393
2026-01-08 20:21:42,756: t15.2024.02.25 val PER: 0.1306
2026-01-08 20:21:42,756: t15.2024.03.08 val PER: 0.2219
2026-01-08 20:21:42,756: t15.2024.03.15 val PER: 0.2164
2026-01-08 20:21:42,756: t15.2024.03.17 val PER: 0.1485
2026-01-08 20:21:42,756: t15.2024.05.10 val PER: 0.1724
2026-01-08 20:21:42,756: t15.2024.06.14 val PER: 0.1814
2026-01-08 20:21:42,756: t15.2024.07.19 val PER: 0.2505
2026-01-08 20:21:42,756: t15.2024.07.21 val PER: 0.1048
2026-01-08 20:21:42,756: t15.2024.07.28 val PER: 0.1493
2026-01-08 20:21:42,757: t15.2025.01.10 val PER: 0.3099
2026-01-08 20:21:42,757: t15.2025.01.12 val PER: 0.1609
2026-01-08 20:21:42,757: t15.2025.03.14 val PER: 0.3506
2026-01-08 20:21:42,757: t15.2025.03.16 val PER: 0.2016
2026-01-08 20:21:42,757: t15.2025.03.30 val PER: 0.3092
2026-01-08 20:21:42,757: t15.2025.04.13 val PER: 0.2268
2026-01-08 20:21:42,895: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_11500
2026-01-08 20:21:51,390: Train batch 11600: loss: 11.02 grad norm: 48.29 time: 0.062
2026-01-08 20:22:08,491: Train batch 11800: loss: 6.67 grad norm: 43.42 time: 0.045
2026-01-08 20:22:25,534: Train batch 12000: loss: 13.90 grad norm: 57.07 time: 0.072
2026-01-08 20:22:25,535: Running test after training batch: 12000
2026-01-08 20:22:25,628: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:22:30,564: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:22:30,610: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:22:42,081: Val batch 12000: PER (avg): 0.1601 CTC Loss (avg): 16.2590 WER(5gram): 16.82% (n=256) time: 16.547
2026-01-08 20:22:42,082: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-08 20:22:42,082: t15.2023.08.13 val PER: 0.1237
2026-01-08 20:22:42,082: t15.2023.08.18 val PER: 0.1090
2026-01-08 20:22:42,082: t15.2023.08.20 val PER: 0.1207
2026-01-08 20:22:42,082: t15.2023.08.25 val PER: 0.0979
2026-01-08 20:22:42,083: t15.2023.08.27 val PER: 0.2010
2026-01-08 20:22:42,083: t15.2023.09.01 val PER: 0.0933
2026-01-08 20:22:42,083: t15.2023.09.03 val PER: 0.1675
2026-01-08 20:22:42,083: t15.2023.09.24 val PER: 0.1274
2026-01-08 20:22:42,083: t15.2023.09.29 val PER: 0.1347
2026-01-08 20:22:42,083: t15.2023.10.01 val PER: 0.1843
2026-01-08 20:22:42,083: t15.2023.10.06 val PER: 0.0936
2026-01-08 20:22:42,083: t15.2023.10.08 val PER: 0.2503
2026-01-08 20:22:42,083: t15.2023.10.13 val PER: 0.2164
2026-01-08 20:22:42,083: t15.2023.10.15 val PER: 0.1641
2026-01-08 20:22:42,083: t15.2023.10.20 val PER: 0.2047
2026-01-08 20:22:42,083: t15.2023.10.22 val PER: 0.1158
2026-01-08 20:22:42,083: t15.2023.11.03 val PER: 0.1811
2026-01-08 20:22:42,084: t15.2023.11.04 val PER: 0.0410
2026-01-08 20:22:42,084: t15.2023.11.17 val PER: 0.0467
2026-01-08 20:22:42,084: t15.2023.11.19 val PER: 0.0379
2026-01-08 20:22:42,084: t15.2023.11.26 val PER: 0.1304
2026-01-08 20:22:42,084: t15.2023.12.03 val PER: 0.1166
2026-01-08 20:22:42,084: t15.2023.12.08 val PER: 0.1105
2026-01-08 20:22:42,084: t15.2023.12.10 val PER: 0.0933
2026-01-08 20:22:42,084: t15.2023.12.17 val PER: 0.1486
2026-01-08 20:22:42,084: t15.2023.12.29 val PER: 0.1380
2026-01-08 20:22:42,084: t15.2024.02.25 val PER: 0.1250
2026-01-08 20:22:42,084: t15.2024.03.08 val PER: 0.2347
2026-01-08 20:22:42,084: t15.2024.03.15 val PER: 0.2026
2026-01-08 20:22:42,084: t15.2024.03.17 val PER: 0.1513
2026-01-08 20:22:42,084: t15.2024.05.10 val PER: 0.1828
2026-01-08 20:22:42,084: t15.2024.06.14 val PER: 0.1940
2026-01-08 20:22:42,084: t15.2024.07.19 val PER: 0.2597
2026-01-08 20:22:42,084: t15.2024.07.21 val PER: 0.1076
2026-01-08 20:22:42,085: t15.2024.07.28 val PER: 0.1412
2026-01-08 20:22:42,085: t15.2025.01.10 val PER: 0.3003
2026-01-08 20:22:42,085: t15.2025.01.12 val PER: 0.1493
2026-01-08 20:22:42,085: t15.2025.03.14 val PER: 0.3580
2026-01-08 20:22:42,085: t15.2025.03.16 val PER: 0.2094
2026-01-08 20:22:42,085: t15.2025.03.30 val PER: 0.3103
2026-01-08 20:22:42,085: t15.2025.04.13 val PER: 0.2154
2026-01-08 20:22:42,223: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_12000
2026-01-08 20:22:59,203: Train batch 12200: loss: 5.18 grad norm: 37.28 time: 0.067
2026-01-08 20:23:16,155: Train batch 12400: loss: 4.76 grad norm: 35.31 time: 0.041
2026-01-08 20:23:25,129: Running test after training batch: 12500
2026-01-08 20:23:25,272: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:23:30,203: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:23:30,244: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:23:41,847: Val batch 12500: PER (avg): 0.1578 CTC Loss (avg): 15.9902 WER(5gram): 15.25% (n=256) time: 16.718
2026-01-08 20:23:41,847: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 20:23:41,848: t15.2023.08.13 val PER: 0.1237
2026-01-08 20:23:41,848: t15.2023.08.18 val PER: 0.1081
2026-01-08 20:23:41,848: t15.2023.08.20 val PER: 0.1104
2026-01-08 20:23:41,848: t15.2023.08.25 val PER: 0.0949
2026-01-08 20:23:41,848: t15.2023.08.27 val PER: 0.2074
2026-01-08 20:23:41,848: t15.2023.09.01 val PER: 0.0844
2026-01-08 20:23:41,848: t15.2023.09.03 val PER: 0.1580
2026-01-08 20:23:41,849: t15.2023.09.24 val PER: 0.1274
2026-01-08 20:23:41,849: t15.2023.09.29 val PER: 0.1340
2026-01-08 20:23:41,849: t15.2023.10.01 val PER: 0.1724
2026-01-08 20:23:41,849: t15.2023.10.06 val PER: 0.0893
2026-01-08 20:23:41,849: t15.2023.10.08 val PER: 0.2666
2026-01-08 20:23:41,849: t15.2023.10.13 val PER: 0.2172
2026-01-08 20:23:41,849: t15.2023.10.15 val PER: 0.1622
2026-01-08 20:23:41,849: t15.2023.10.20 val PER: 0.1980
2026-01-08 20:23:41,850: t15.2023.10.22 val PER: 0.1091
2026-01-08 20:23:41,850: t15.2023.11.03 val PER: 0.1852
2026-01-08 20:23:41,850: t15.2023.11.04 val PER: 0.0375
2026-01-08 20:23:41,850: t15.2023.11.17 val PER: 0.0544
2026-01-08 20:23:41,850: t15.2023.11.19 val PER: 0.0379
2026-01-08 20:23:41,850: t15.2023.11.26 val PER: 0.1355
2026-01-08 20:23:41,850: t15.2023.12.03 val PER: 0.1239
2026-01-08 20:23:41,850: t15.2023.12.08 val PER: 0.1065
2026-01-08 20:23:41,851: t15.2023.12.10 val PER: 0.0972
2026-01-08 20:23:41,851: t15.2023.12.17 val PER: 0.1414
2026-01-08 20:23:41,851: t15.2023.12.29 val PER: 0.1386
2026-01-08 20:23:41,851: t15.2024.02.25 val PER: 0.1152
2026-01-08 20:23:41,851: t15.2024.03.08 val PER: 0.2276
2026-01-08 20:23:41,851: t15.2024.03.15 val PER: 0.2114
2026-01-08 20:23:41,851: t15.2024.03.17 val PER: 0.1471
2026-01-08 20:23:41,851: t15.2024.05.10 val PER: 0.1679
2026-01-08 20:23:41,851: t15.2024.06.14 val PER: 0.1767
2026-01-08 20:23:41,852: t15.2024.07.19 val PER: 0.2498
2026-01-08 20:23:41,852: t15.2024.07.21 val PER: 0.0972
2026-01-08 20:23:41,852: t15.2024.07.28 val PER: 0.1360
2026-01-08 20:23:41,852: t15.2025.01.10 val PER: 0.3072
2026-01-08 20:23:41,852: t15.2025.01.12 val PER: 0.1532
2026-01-08 20:23:41,852: t15.2025.03.14 val PER: 0.3609
2026-01-08 20:23:41,852: t15.2025.03.16 val PER: 0.2055
2026-01-08 20:23:41,852: t15.2025.03.30 val PER: 0.3103
2026-01-08 20:23:41,852: t15.2025.04.13 val PER: 0.2126
2026-01-08 20:23:41,853: New best val WER(5gram) 15.78% --> 15.25%
2026-01-08 20:23:42,039: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_12500
2026-01-08 20:23:50,481: Train batch 12600: loss: 8.09 grad norm: 45.73 time: 0.058
2026-01-08 20:24:07,697: Train batch 12800: loss: 5.73 grad norm: 36.80 time: 0.053
2026-01-08 20:24:25,263: Train batch 13000: loss: 6.47 grad norm: 40.41 time: 0.066
2026-01-08 20:24:25,263: Running test after training batch: 13000
2026-01-08 20:24:25,363: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:24:30,279: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:24:30,322: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:24:41,787: Val batch 13000: PER (avg): 0.1548 CTC Loss (avg): 15.8334 WER(5gram): 17.21% (n=256) time: 16.523
2026-01-08 20:24:41,787: WER lens: avg_true_words=5.99 avg_pred_words=6.23 max_pred_words=13
2026-01-08 20:24:41,788: t15.2023.08.13 val PER: 0.1206
2026-01-08 20:24:41,788: t15.2023.08.18 val PER: 0.1157
2026-01-08 20:24:41,788: t15.2023.08.20 val PER: 0.1136
2026-01-08 20:24:41,788: t15.2023.08.25 val PER: 0.0904
2026-01-08 20:24:41,788: t15.2023.08.27 val PER: 0.1929
2026-01-08 20:24:41,788: t15.2023.09.01 val PER: 0.0779
2026-01-08 20:24:41,788: t15.2023.09.03 val PER: 0.1686
2026-01-08 20:24:41,788: t15.2023.09.24 val PER: 0.1274
2026-01-08 20:24:41,788: t15.2023.09.29 val PER: 0.1372
2026-01-08 20:24:41,788: t15.2023.10.01 val PER: 0.1704
2026-01-08 20:24:41,788: t15.2023.10.06 val PER: 0.0915
2026-01-08 20:24:41,788: t15.2023.10.08 val PER: 0.2530
2026-01-08 20:24:41,788: t15.2023.10.13 val PER: 0.2102
2026-01-08 20:24:41,788: t15.2023.10.15 val PER: 0.1589
2026-01-08 20:24:41,789: t15.2023.10.20 val PER: 0.1745
2026-01-08 20:24:41,789: t15.2023.10.22 val PER: 0.1147
2026-01-08 20:24:41,789: t15.2023.11.03 val PER: 0.1791
2026-01-08 20:24:41,789: t15.2023.11.04 val PER: 0.0341
2026-01-08 20:24:41,789: t15.2023.11.17 val PER: 0.0420
2026-01-08 20:24:41,789: t15.2023.11.19 val PER: 0.0459
2026-01-08 20:24:41,789: t15.2023.11.26 val PER: 0.1239
2026-01-08 20:24:41,789: t15.2023.12.03 val PER: 0.1134
2026-01-08 20:24:41,789: t15.2023.12.08 val PER: 0.1085
2026-01-08 20:24:41,789: t15.2023.12.10 val PER: 0.0920
2026-01-08 20:24:41,789: t15.2023.12.17 val PER: 0.1414
2026-01-08 20:24:41,789: t15.2023.12.29 val PER: 0.1421
2026-01-08 20:24:41,789: t15.2024.02.25 val PER: 0.1180
2026-01-08 20:24:41,790: t15.2024.03.08 val PER: 0.2404
2026-01-08 20:24:41,790: t15.2024.03.15 val PER: 0.2020
2026-01-08 20:24:41,790: t15.2024.03.17 val PER: 0.1437
2026-01-08 20:24:41,790: t15.2024.05.10 val PER: 0.1560
2026-01-08 20:24:41,790: t15.2024.06.14 val PER: 0.1656
2026-01-08 20:24:41,790: t15.2024.07.19 val PER: 0.2525
2026-01-08 20:24:41,790: t15.2024.07.21 val PER: 0.0945
2026-01-08 20:24:41,790: t15.2024.07.28 val PER: 0.1353
2026-01-08 20:24:41,790: t15.2025.01.10 val PER: 0.2920
2026-01-08 20:24:41,790: t15.2025.01.12 val PER: 0.1478
2026-01-08 20:24:41,790: t15.2025.03.14 val PER: 0.3402
2026-01-08 20:24:41,790: t15.2025.03.16 val PER: 0.1911
2026-01-08 20:24:41,790: t15.2025.03.30 val PER: 0.3023
2026-01-08 20:24:41,790: t15.2025.04.13 val PER: 0.2225
2026-01-08 20:24:41,929: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_13000
2026-01-08 20:24:59,628: Train batch 13200: loss: 12.74 grad norm: 61.68 time: 0.055
2026-01-08 20:25:17,271: Train batch 13400: loss: 8.78 grad norm: 53.35 time: 0.063
2026-01-08 20:25:26,149: Running test after training batch: 13500
2026-01-08 20:25:26,255: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:25:31,175: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:25:31,220: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 20:25:42,773: Val batch 13500: PER (avg): 0.1535 CTC Loss (avg): 15.5849 WER(5gram): 16.62% (n=256) time: 16.623
2026-01-08 20:25:42,773: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=12
2026-01-08 20:25:42,774: t15.2023.08.13 val PER: 0.1206
2026-01-08 20:25:42,774: t15.2023.08.18 val PER: 0.1123
2026-01-08 20:25:42,774: t15.2023.08.20 val PER: 0.1096
2026-01-08 20:25:42,774: t15.2023.08.25 val PER: 0.0843
2026-01-08 20:25:42,774: t15.2023.08.27 val PER: 0.1961
2026-01-08 20:25:42,774: t15.2023.09.01 val PER: 0.0860
2026-01-08 20:25:42,774: t15.2023.09.03 val PER: 0.1698
2026-01-08 20:25:42,774: t15.2023.09.24 val PER: 0.1274
2026-01-08 20:25:42,774: t15.2023.09.29 val PER: 0.1327
2026-01-08 20:25:42,774: t15.2023.10.01 val PER: 0.1704
2026-01-08 20:25:42,774: t15.2023.10.06 val PER: 0.0829
2026-01-08 20:25:42,774: t15.2023.10.08 val PER: 0.2625
2026-01-08 20:25:42,775: t15.2023.10.13 val PER: 0.2102
2026-01-08 20:25:42,775: t15.2023.10.15 val PER: 0.1569
2026-01-08 20:25:42,775: t15.2023.10.20 val PER: 0.1711
2026-01-08 20:25:42,775: t15.2023.10.22 val PER: 0.1136
2026-01-08 20:25:42,775: t15.2023.11.03 val PER: 0.1825
2026-01-08 20:25:42,775: t15.2023.11.04 val PER: 0.0375
2026-01-08 20:25:42,775: t15.2023.11.17 val PER: 0.0482
2026-01-08 20:25:42,775: t15.2023.11.19 val PER: 0.0299
2026-01-08 20:25:42,776: t15.2023.11.26 val PER: 0.1268
2026-01-08 20:25:42,776: t15.2023.12.03 val PER: 0.1166
2026-01-08 20:25:42,776: t15.2023.12.08 val PER: 0.1052
2026-01-08 20:25:42,776: t15.2023.12.10 val PER: 0.0920
2026-01-08 20:25:42,776: t15.2023.12.17 val PER: 0.1216
2026-01-08 20:25:42,776: t15.2023.12.29 val PER: 0.1366
2026-01-08 20:25:42,776: t15.2024.02.25 val PER: 0.1334
2026-01-08 20:25:42,776: t15.2024.03.08 val PER: 0.2290
2026-01-08 20:25:42,776: t15.2024.03.15 val PER: 0.1951
2026-01-08 20:25:42,776: t15.2024.03.17 val PER: 0.1485
2026-01-08 20:25:42,776: t15.2024.05.10 val PER: 0.1426
2026-01-08 20:25:42,776: t15.2024.06.14 val PER: 0.1767
2026-01-08 20:25:42,776: t15.2024.07.19 val PER: 0.2432
2026-01-08 20:25:42,777: t15.2024.07.21 val PER: 0.1000
2026-01-08 20:25:42,777: t15.2024.07.28 val PER: 0.1353
2026-01-08 20:25:42,777: t15.2025.01.10 val PER: 0.2934
2026-01-08 20:25:42,777: t15.2025.01.12 val PER: 0.1478
2026-01-08 20:25:42,777: t15.2025.03.14 val PER: 0.3343
2026-01-08 20:25:42,777: t15.2025.03.16 val PER: 0.1911
2026-01-08 20:25:42,777: t15.2025.03.30 val PER: 0.3011
2026-01-08 20:25:42,777: t15.2025.04.13 val PER: 0.2197
2026-01-08 20:25:42,922: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_13500
2026-01-08 20:25:51,432: Train batch 13600: loss: 12.65 grad norm: 63.71 time: 0.064
2026-01-08 20:26:08,846: Train batch 13800: loss: 8.83 grad norm: 55.26 time: 0.059
2026-01-08 20:26:26,308: Train batch 14000: loss: 11.82 grad norm: 58.39 time: 0.053
2026-01-08 20:26:26,308: Running test after training batch: 14000
2026-01-08 20:26:26,419: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:26:31,433: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:26:31,475: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 20:26:43,377: Val batch 14000: PER (avg): 0.1524 CTC Loss (avg): 15.5383 WER(5gram): 14.80% (n=256) time: 17.069
2026-01-08 20:26:43,378: WER lens: avg_true_words=5.99 avg_pred_words=6.15 max_pred_words=12
2026-01-08 20:26:43,378: t15.2023.08.13 val PER: 0.1133
2026-01-08 20:26:43,378: t15.2023.08.18 val PER: 0.1039
2026-01-08 20:26:43,378: t15.2023.08.20 val PER: 0.1056
2026-01-08 20:26:43,378: t15.2023.08.25 val PER: 0.1009
2026-01-08 20:26:43,378: t15.2023.08.27 val PER: 0.1961
2026-01-08 20:26:43,379: t15.2023.09.01 val PER: 0.0812
2026-01-08 20:26:43,379: t15.2023.09.03 val PER: 0.1710
2026-01-08 20:26:43,379: t15.2023.09.24 val PER: 0.1226
2026-01-08 20:26:43,379: t15.2023.09.29 val PER: 0.1372
2026-01-08 20:26:43,379: t15.2023.10.01 val PER: 0.1777
2026-01-08 20:26:43,379: t15.2023.10.06 val PER: 0.0893
2026-01-08 20:26:43,379: t15.2023.10.08 val PER: 0.2544
2026-01-08 20:26:43,379: t15.2023.10.13 val PER: 0.2110
2026-01-08 20:26:43,379: t15.2023.10.15 val PER: 0.1543
2026-01-08 20:26:43,379: t15.2023.10.20 val PER: 0.1846
2026-01-08 20:26:43,379: t15.2023.10.22 val PER: 0.1091
2026-01-08 20:26:43,379: t15.2023.11.03 val PER: 0.1737
2026-01-08 20:26:43,379: t15.2023.11.04 val PER: 0.0307
2026-01-08 20:26:43,379: t15.2023.11.17 val PER: 0.0529
2026-01-08 20:26:43,379: t15.2023.11.19 val PER: 0.0359
2026-01-08 20:26:43,379: t15.2023.11.26 val PER: 0.1283
2026-01-08 20:26:43,380: t15.2023.12.03 val PER: 0.1176
2026-01-08 20:26:43,380: t15.2023.12.08 val PER: 0.1045
2026-01-08 20:26:43,380: t15.2023.12.10 val PER: 0.1025
2026-01-08 20:26:43,380: t15.2023.12.17 val PER: 0.1320
2026-01-08 20:26:43,380: t15.2023.12.29 val PER: 0.1345
2026-01-08 20:26:43,380: t15.2024.02.25 val PER: 0.1124
2026-01-08 20:26:43,380: t15.2024.03.08 val PER: 0.2376
2026-01-08 20:26:43,380: t15.2024.03.15 val PER: 0.2014
2026-01-08 20:26:43,380: t15.2024.03.17 val PER: 0.1437
2026-01-08 20:26:43,380: t15.2024.05.10 val PER: 0.1486
2026-01-08 20:26:43,381: t15.2024.06.14 val PER: 0.1593
2026-01-08 20:26:43,382: t15.2024.07.19 val PER: 0.2419
2026-01-08 20:26:43,382: t15.2024.07.21 val PER: 0.0869
2026-01-08 20:26:43,383: t15.2024.07.28 val PER: 0.1324
2026-01-08 20:26:43,383: t15.2025.01.10 val PER: 0.2989
2026-01-08 20:26:43,383: t15.2025.01.12 val PER: 0.1470
2026-01-08 20:26:43,383: t15.2025.03.14 val PER: 0.3388
2026-01-08 20:26:43,383: t15.2025.03.16 val PER: 0.1819
2026-01-08 20:26:43,383: t15.2025.03.30 val PER: 0.2908
2026-01-08 20:26:43,383: t15.2025.04.13 val PER: 0.2183
2026-01-08 20:26:43,384: New best val WER(5gram) 15.25% --> 14.80%
2026-01-08 20:26:43,581: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_14000
2026-01-08 20:27:00,490: Train batch 14200: loss: 8.30 grad norm: 51.85 time: 0.057
2026-01-08 20:27:18,976: Train batch 14400: loss: 5.71 grad norm: 38.75 time: 0.064
2026-01-08 20:27:28,092: Running test after training batch: 14500
2026-01-08 20:27:28,200: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:27:33,075: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:27:33,120: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:27:44,439: Val batch 14500: PER (avg): 0.1520 CTC Loss (avg): 15.6215 WER(5gram): 16.95% (n=256) time: 16.347
2026-01-08 20:27:44,439: WER lens: avg_true_words=5.99 avg_pred_words=6.23 max_pred_words=12
2026-01-08 20:27:44,440: t15.2023.08.13 val PER: 0.1133
2026-01-08 20:27:44,440: t15.2023.08.18 val PER: 0.1098
2026-01-08 20:27:44,440: t15.2023.08.20 val PER: 0.1056
2026-01-08 20:27:44,440: t15.2023.08.25 val PER: 0.0949
2026-01-08 20:27:44,440: t15.2023.08.27 val PER: 0.1897
2026-01-08 20:27:44,440: t15.2023.09.01 val PER: 0.0804
2026-01-08 20:27:44,440: t15.2023.09.03 val PER: 0.1615
2026-01-08 20:27:44,440: t15.2023.09.24 val PER: 0.1262
2026-01-08 20:27:44,440: t15.2023.09.29 val PER: 0.1295
2026-01-08 20:27:44,440: t15.2023.10.01 val PER: 0.1757
2026-01-08 20:27:44,440: t15.2023.10.06 val PER: 0.0904
2026-01-08 20:27:44,440: t15.2023.10.08 val PER: 0.2639
2026-01-08 20:27:44,440: t15.2023.10.13 val PER: 0.2157
2026-01-08 20:27:44,440: t15.2023.10.15 val PER: 0.1589
2026-01-08 20:27:44,440: t15.2023.10.20 val PER: 0.1711
2026-01-08 20:27:44,440: t15.2023.10.22 val PER: 0.1102
2026-01-08 20:27:44,441: t15.2023.11.03 val PER: 0.1866
2026-01-08 20:27:44,441: t15.2023.11.04 val PER: 0.0375
2026-01-08 20:27:44,441: t15.2023.11.17 val PER: 0.0435
2026-01-08 20:27:44,441: t15.2023.11.19 val PER: 0.0339
2026-01-08 20:27:44,441: t15.2023.11.26 val PER: 0.1239
2026-01-08 20:27:44,441: t15.2023.12.03 val PER: 0.1082
2026-01-08 20:27:44,441: t15.2023.12.08 val PER: 0.1045
2026-01-08 20:27:44,441: t15.2023.12.10 val PER: 0.0907
2026-01-08 20:27:44,441: t15.2023.12.17 val PER: 0.1393
2026-01-08 20:27:44,441: t15.2023.12.29 val PER: 0.1345
2026-01-08 20:27:44,441: t15.2024.02.25 val PER: 0.1110
2026-01-08 20:27:44,442: t15.2024.03.08 val PER: 0.2262
2026-01-08 20:27:44,442: t15.2024.03.15 val PER: 0.1989
2026-01-08 20:27:44,442: t15.2024.03.17 val PER: 0.1367
2026-01-08 20:27:44,442: t15.2024.05.10 val PER: 0.1575
2026-01-08 20:27:44,442: t15.2024.06.14 val PER: 0.1672
2026-01-08 20:27:44,442: t15.2024.07.19 val PER: 0.2479
2026-01-08 20:27:44,442: t15.2024.07.21 val PER: 0.0876
2026-01-08 20:27:44,442: t15.2024.07.28 val PER: 0.1353
2026-01-08 20:27:44,442: t15.2025.01.10 val PER: 0.2865
2026-01-08 20:27:44,442: t15.2025.01.12 val PER: 0.1409
2026-01-08 20:27:44,442: t15.2025.03.14 val PER: 0.3417
2026-01-08 20:27:44,442: t15.2025.03.16 val PER: 0.1937
2026-01-08 20:27:44,442: t15.2025.03.30 val PER: 0.2943
2026-01-08 20:27:44,442: t15.2025.04.13 val PER: 0.2054
2026-01-08 20:27:44,584: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_14500
2026-01-08 20:27:53,368: Train batch 14600: loss: 12.77 grad norm: 64.11 time: 0.061
2026-01-08 20:28:10,357: Train batch 14800: loss: 5.80 grad norm: 42.66 time: 0.051
2026-01-08 20:28:27,999: Train batch 15000: loss: 8.67 grad norm: 48.81 time: 0.055
2026-01-08 20:28:27,999: Running test after training batch: 15000
2026-01-08 20:28:28,137: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:28:32,996: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:28:33,043: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:28:44,570: Val batch 15000: PER (avg): 0.1497 CTC Loss (avg): 15.3153 WER(5gram): 14.67% (n=256) time: 16.570
2026-01-08 20:28:44,570: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 20:28:44,570: t15.2023.08.13 val PER: 0.1040
2026-01-08 20:28:44,570: t15.2023.08.18 val PER: 0.1039
2026-01-08 20:28:44,571: t15.2023.08.20 val PER: 0.1128
2026-01-08 20:28:44,571: t15.2023.08.25 val PER: 0.0889
2026-01-08 20:28:44,571: t15.2023.08.27 val PER: 0.1849
2026-01-08 20:28:44,571: t15.2023.09.01 val PER: 0.0771
2026-01-08 20:28:44,571: t15.2023.09.03 val PER: 0.1544
2026-01-08 20:28:44,571: t15.2023.09.24 val PER: 0.1299
2026-01-08 20:28:44,571: t15.2023.09.29 val PER: 0.1315
2026-01-08 20:28:44,571: t15.2023.10.01 val PER: 0.1731
2026-01-08 20:28:44,571: t15.2023.10.06 val PER: 0.0786
2026-01-08 20:28:44,572: t15.2023.10.08 val PER: 0.2558
2026-01-08 20:28:44,572: t15.2023.10.13 val PER: 0.2056
2026-01-08 20:28:44,572: t15.2023.10.15 val PER: 0.1549
2026-01-08 20:28:44,572: t15.2023.10.20 val PER: 0.1779
2026-01-08 20:28:44,572: t15.2023.10.22 val PER: 0.1125
2026-01-08 20:28:44,572: t15.2023.11.03 val PER: 0.1811
2026-01-08 20:28:44,572: t15.2023.11.04 val PER: 0.0410
2026-01-08 20:28:44,572: t15.2023.11.17 val PER: 0.0435
2026-01-08 20:28:44,572: t15.2023.11.19 val PER: 0.0359
2026-01-08 20:28:44,572: t15.2023.11.26 val PER: 0.1167
2026-01-08 20:28:44,572: t15.2023.12.03 val PER: 0.1176
2026-01-08 20:28:44,572: t15.2023.12.08 val PER: 0.0965
2026-01-08 20:28:44,572: t15.2023.12.10 val PER: 0.0933
2026-01-08 20:28:44,572: t15.2023.12.17 val PER: 0.1351
2026-01-08 20:28:44,573: t15.2023.12.29 val PER: 0.1304
2026-01-08 20:28:44,573: t15.2024.02.25 val PER: 0.1039
2026-01-08 20:28:44,573: t15.2024.03.08 val PER: 0.2248
2026-01-08 20:28:44,573: t15.2024.03.15 val PER: 0.1939
2026-01-08 20:28:44,573: t15.2024.03.17 val PER: 0.1367
2026-01-08 20:28:44,573: t15.2024.05.10 val PER: 0.1634
2026-01-08 20:28:44,573: t15.2024.06.14 val PER: 0.1593
2026-01-08 20:28:44,573: t15.2024.07.19 val PER: 0.2399
2026-01-08 20:28:44,573: t15.2024.07.21 val PER: 0.0890
2026-01-08 20:28:44,574: t15.2024.07.28 val PER: 0.1287
2026-01-08 20:28:44,574: t15.2025.01.10 val PER: 0.3168
2026-01-08 20:28:44,574: t15.2025.01.12 val PER: 0.1409
2026-01-08 20:28:44,574: t15.2025.03.14 val PER: 0.3491
2026-01-08 20:28:44,574: t15.2025.03.16 val PER: 0.1898
2026-01-08 20:28:44,574: t15.2025.03.30 val PER: 0.2874
2026-01-08 20:28:44,574: t15.2025.04.13 val PER: 0.2097
2026-01-08 20:28:44,575: New best val WER(5gram) 14.80% --> 14.67%
2026-01-08 20:28:44,759: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_15000
2026-01-08 20:29:03,086: Train batch 15200: loss: 4.65 grad norm: 41.37 time: 0.059
2026-01-08 20:29:20,296: Train batch 15400: loss: 11.06 grad norm: 56.48 time: 0.050
2026-01-08 20:29:29,296: Running test after training batch: 15500
2026-01-08 20:29:29,449: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:29:34,433: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:29:34,479: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:29:46,417: Val batch 15500: PER (avg): 0.1487 CTC Loss (avg): 15.2235 WER(5gram): 14.99% (n=256) time: 17.121
2026-01-08 20:29:46,418: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 20:29:46,418: t15.2023.08.13 val PER: 0.1060
2026-01-08 20:29:46,418: t15.2023.08.18 val PER: 0.1048
2026-01-08 20:29:46,418: t15.2023.08.20 val PER: 0.1088
2026-01-08 20:29:46,418: t15.2023.08.25 val PER: 0.0873
2026-01-08 20:29:46,418: t15.2023.08.27 val PER: 0.1897
2026-01-08 20:29:46,419: t15.2023.09.01 val PER: 0.0763
2026-01-08 20:29:46,419: t15.2023.09.03 val PER: 0.1603
2026-01-08 20:29:46,419: t15.2023.09.24 val PER: 0.1299
2026-01-08 20:29:46,419: t15.2023.09.29 val PER: 0.1315
2026-01-08 20:29:46,419: t15.2023.10.01 val PER: 0.1717
2026-01-08 20:29:46,419: t15.2023.10.06 val PER: 0.0797
2026-01-08 20:29:46,419: t15.2023.10.08 val PER: 0.2558
2026-01-08 20:29:46,419: t15.2023.10.13 val PER: 0.2040
2026-01-08 20:29:46,419: t15.2023.10.15 val PER: 0.1523
2026-01-08 20:29:46,419: t15.2023.10.20 val PER: 0.1812
2026-01-08 20:29:46,419: t15.2023.10.22 val PER: 0.1114
2026-01-08 20:29:46,419: t15.2023.11.03 val PER: 0.1784
2026-01-08 20:29:46,420: t15.2023.11.04 val PER: 0.0341
2026-01-08 20:29:46,420: t15.2023.11.17 val PER: 0.0420
2026-01-08 20:29:46,420: t15.2023.11.19 val PER: 0.0319
2026-01-08 20:29:46,420: t15.2023.11.26 val PER: 0.1174
2026-01-08 20:29:46,420: t15.2023.12.03 val PER: 0.1113
2026-01-08 20:29:46,420: t15.2023.12.08 val PER: 0.1019
2026-01-08 20:29:46,420: t15.2023.12.10 val PER: 0.0907
2026-01-08 20:29:46,422: t15.2023.12.17 val PER: 0.1268
2026-01-08 20:29:46,422: t15.2023.12.29 val PER: 0.1256
2026-01-08 20:29:46,422: t15.2024.02.25 val PER: 0.1025
2026-01-08 20:29:46,423: t15.2024.03.08 val PER: 0.2219
2026-01-08 20:29:46,423: t15.2024.03.15 val PER: 0.1957
2026-01-08 20:29:46,423: t15.2024.03.17 val PER: 0.1381
2026-01-08 20:29:46,423: t15.2024.05.10 val PER: 0.1575
2026-01-08 20:29:46,423: t15.2024.06.14 val PER: 0.1577
2026-01-08 20:29:46,423: t15.2024.07.19 val PER: 0.2353
2026-01-08 20:29:46,423: t15.2024.07.21 val PER: 0.0876
2026-01-08 20:29:46,423: t15.2024.07.28 val PER: 0.1316
2026-01-08 20:29:46,423: t15.2025.01.10 val PER: 0.3030
2026-01-08 20:29:46,423: t15.2025.01.12 val PER: 0.1393
2026-01-08 20:29:46,423: t15.2025.03.14 val PER: 0.3432
2026-01-08 20:29:46,423: t15.2025.03.16 val PER: 0.1859
2026-01-08 20:29:46,423: t15.2025.03.30 val PER: 0.2954
2026-01-08 20:29:46,423: t15.2025.04.13 val PER: 0.2140
2026-01-08 20:29:46,560: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_15500
2026-01-08 20:29:55,293: Train batch 15600: loss: 11.40 grad norm: 55.46 time: 0.065
2026-01-08 20:30:12,860: Train batch 15800: loss: 13.85 grad norm: 63.72 time: 0.067
2026-01-08 20:30:30,219: Train batch 16000: loss: 8.65 grad norm: 46.59 time: 0.056
2026-01-08 20:30:30,219: Running test after training batch: 16000
2026-01-08 20:30:30,371: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:30:35,351: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:30:35,396: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:30:47,244: Val batch 16000: PER (avg): 0.1482 CTC Loss (avg): 15.2683 WER(5gram): 15.12% (n=256) time: 17.024
2026-01-08 20:30:47,244: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 20:30:47,244: t15.2023.08.13 val PER: 0.1071
2026-01-08 20:30:47,244: t15.2023.08.18 val PER: 0.1065
2026-01-08 20:30:47,245: t15.2023.08.20 val PER: 0.1096
2026-01-08 20:30:47,245: t15.2023.08.25 val PER: 0.0889
2026-01-08 20:30:47,245: t15.2023.08.27 val PER: 0.1913
2026-01-08 20:30:47,245: t15.2023.09.01 val PER: 0.0731
2026-01-08 20:30:47,245: t15.2023.09.03 val PER: 0.1627
2026-01-08 20:30:47,245: t15.2023.09.24 val PER: 0.1274
2026-01-08 20:30:47,245: t15.2023.09.29 val PER: 0.1334
2026-01-08 20:30:47,245: t15.2023.10.01 val PER: 0.1704
2026-01-08 20:30:47,245: t15.2023.10.06 val PER: 0.0840
2026-01-08 20:30:47,245: t15.2023.10.08 val PER: 0.2585
2026-01-08 20:30:47,246: t15.2023.10.13 val PER: 0.2025
2026-01-08 20:30:47,246: t15.2023.10.15 val PER: 0.1483
2026-01-08 20:30:47,246: t15.2023.10.20 val PER: 0.1812
2026-01-08 20:30:47,246: t15.2023.10.22 val PER: 0.1047
2026-01-08 20:30:47,246: t15.2023.11.03 val PER: 0.1805
2026-01-08 20:30:47,246: t15.2023.11.04 val PER: 0.0341
2026-01-08 20:30:47,246: t15.2023.11.17 val PER: 0.0435
2026-01-08 20:30:47,246: t15.2023.11.19 val PER: 0.0319
2026-01-08 20:30:47,246: t15.2023.11.26 val PER: 0.1196
2026-01-08 20:30:47,246: t15.2023.12.03 val PER: 0.1155
2026-01-08 20:30:47,246: t15.2023.12.08 val PER: 0.0999
2026-01-08 20:30:47,246: t15.2023.12.10 val PER: 0.0933
2026-01-08 20:30:47,246: t15.2023.12.17 val PER: 0.1237
2026-01-08 20:30:47,246: t15.2023.12.29 val PER: 0.1256
2026-01-08 20:30:47,246: t15.2024.02.25 val PER: 0.1039
2026-01-08 20:30:47,246: t15.2024.03.08 val PER: 0.2290
2026-01-08 20:30:47,247: t15.2024.03.15 val PER: 0.1939
2026-01-08 20:30:47,247: t15.2024.03.17 val PER: 0.1311
2026-01-08 20:30:47,247: t15.2024.05.10 val PER: 0.1605
2026-01-08 20:30:47,247: t15.2024.06.14 val PER: 0.1625
2026-01-08 20:30:47,247: t15.2024.07.19 val PER: 0.2327
2026-01-08 20:30:47,247: t15.2024.07.21 val PER: 0.0897
2026-01-08 20:30:47,247: t15.2024.07.28 val PER: 0.1301
2026-01-08 20:30:47,247: t15.2025.01.10 val PER: 0.2906
2026-01-08 20:30:47,247: t15.2025.01.12 val PER: 0.1409
2026-01-08 20:30:47,247: t15.2025.03.14 val PER: 0.3328
2026-01-08 20:30:47,247: t15.2025.03.16 val PER: 0.1924
2026-01-08 20:30:47,247: t15.2025.03.30 val PER: 0.2874
2026-01-08 20:30:47,247: t15.2025.04.13 val PER: 0.2126
2026-01-08 20:30:47,389: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_16000
2026-01-08 20:31:05,033: Train batch 16200: loss: 6.18 grad norm: 42.42 time: 0.058
2026-01-08 20:31:22,208: Train batch 16400: loss: 10.69 grad norm: 65.20 time: 0.060
2026-01-08 20:31:30,758: Running test after training batch: 16500
2026-01-08 20:31:30,878: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:31:35,997: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:31:36,042: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:31:47,963: Val batch 16500: PER (avg): 0.1481 CTC Loss (avg): 15.2570 WER(5gram): 14.99% (n=256) time: 17.204
2026-01-08 20:31:47,963: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 20:31:47,963: t15.2023.08.13 val PER: 0.1050
2026-01-08 20:31:47,963: t15.2023.08.18 val PER: 0.1073
2026-01-08 20:31:47,964: t15.2023.08.20 val PER: 0.1088
2026-01-08 20:31:47,964: t15.2023.08.25 val PER: 0.0858
2026-01-08 20:31:47,964: t15.2023.08.27 val PER: 0.1945
2026-01-08 20:31:47,964: t15.2023.09.01 val PER: 0.0755
2026-01-08 20:31:47,964: t15.2023.09.03 val PER: 0.1568
2026-01-08 20:31:47,964: t15.2023.09.24 val PER: 0.1262
2026-01-08 20:31:47,964: t15.2023.09.29 val PER: 0.1308
2026-01-08 20:31:47,964: t15.2023.10.01 val PER: 0.1711
2026-01-08 20:31:47,964: t15.2023.10.06 val PER: 0.0850
2026-01-08 20:31:47,964: t15.2023.10.08 val PER: 0.2571
2026-01-08 20:31:47,964: t15.2023.10.13 val PER: 0.2033
2026-01-08 20:31:47,964: t15.2023.10.15 val PER: 0.1490
2026-01-08 20:31:47,964: t15.2023.10.20 val PER: 0.1879
2026-01-08 20:31:47,964: t15.2023.10.22 val PER: 0.1058
2026-01-08 20:31:47,965: t15.2023.11.03 val PER: 0.1791
2026-01-08 20:31:47,965: t15.2023.11.04 val PER: 0.0307
2026-01-08 20:31:47,965: t15.2023.11.17 val PER: 0.0420
2026-01-08 20:31:47,965: t15.2023.11.19 val PER: 0.0299
2026-01-08 20:31:47,965: t15.2023.11.26 val PER: 0.1188
2026-01-08 20:31:47,965: t15.2023.12.03 val PER: 0.1145
2026-01-08 20:31:47,965: t15.2023.12.08 val PER: 0.0972
2026-01-08 20:31:47,965: t15.2023.12.10 val PER: 0.0920
2026-01-08 20:31:47,965: t15.2023.12.17 val PER: 0.1237
2026-01-08 20:31:47,965: t15.2023.12.29 val PER: 0.1242
2026-01-08 20:31:47,965: t15.2024.02.25 val PER: 0.1053
2026-01-08 20:31:47,965: t15.2024.03.08 val PER: 0.2304
2026-01-08 20:31:47,965: t15.2024.03.15 val PER: 0.1926
2026-01-08 20:31:47,966: t15.2024.03.17 val PER: 0.1346
2026-01-08 20:31:47,966: t15.2024.05.10 val PER: 0.1590
2026-01-08 20:31:47,966: t15.2024.06.14 val PER: 0.1625
2026-01-08 20:31:47,966: t15.2024.07.19 val PER: 0.2360
2026-01-08 20:31:47,966: t15.2024.07.21 val PER: 0.0897
2026-01-08 20:31:47,966: t15.2024.07.28 val PER: 0.1287
2026-01-08 20:31:47,966: t15.2025.01.10 val PER: 0.2865
2026-01-08 20:31:47,966: t15.2025.01.12 val PER: 0.1432
2026-01-08 20:31:47,966: t15.2025.03.14 val PER: 0.3432
2026-01-08 20:31:47,966: t15.2025.03.16 val PER: 0.1898
2026-01-08 20:31:47,966: t15.2025.03.30 val PER: 0.2897
2026-01-08 20:31:47,966: t15.2025.04.13 val PER: 0.2126
2026-01-08 20:31:48,105: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_16500
2026-01-08 20:31:57,122: Train batch 16600: loss: 8.40 grad norm: 54.18 time: 0.054
2026-01-08 20:32:15,602: Train batch 16800: loss: 16.12 grad norm: 71.88 time: 0.063
2026-01-08 20:32:33,393: Train batch 17000: loss: 8.54 grad norm: 49.66 time: 0.082
2026-01-08 20:32:33,393: Running test after training batch: 17000
2026-01-08 20:32:33,499: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:32:38,718: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:32:38,767: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:32:50,692: Val batch 17000: PER (avg): 0.1486 CTC Loss (avg): 15.2047 WER(5gram): 14.80% (n=256) time: 17.299
2026-01-08 20:32:50,693: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 20:32:50,693: t15.2023.08.13 val PER: 0.1050
2026-01-08 20:32:50,693: t15.2023.08.18 val PER: 0.1073
2026-01-08 20:32:50,693: t15.2023.08.20 val PER: 0.1112
2026-01-08 20:32:50,693: t15.2023.08.25 val PER: 0.0873
2026-01-08 20:32:50,694: t15.2023.08.27 val PER: 0.1897
2026-01-08 20:32:50,694: t15.2023.09.01 val PER: 0.0747
2026-01-08 20:32:50,694: t15.2023.09.03 val PER: 0.1580
2026-01-08 20:32:50,694: t15.2023.09.24 val PER: 0.1262
2026-01-08 20:32:50,694: t15.2023.09.29 val PER: 0.1295
2026-01-08 20:32:50,694: t15.2023.10.01 val PER: 0.1717
2026-01-08 20:32:50,694: t15.2023.10.06 val PER: 0.0883
2026-01-08 20:32:50,694: t15.2023.10.08 val PER: 0.2625
2026-01-08 20:32:50,694: t15.2023.10.13 val PER: 0.2048
2026-01-08 20:32:50,694: t15.2023.10.15 val PER: 0.1496
2026-01-08 20:32:50,694: t15.2023.10.20 val PER: 0.1846
2026-01-08 20:32:50,694: t15.2023.10.22 val PER: 0.1080
2026-01-08 20:32:50,694: t15.2023.11.03 val PER: 0.1798
2026-01-08 20:32:50,694: t15.2023.11.04 val PER: 0.0341
2026-01-08 20:32:50,694: t15.2023.11.17 val PER: 0.0389
2026-01-08 20:32:50,695: t15.2023.11.19 val PER: 0.0339
2026-01-08 20:32:50,695: t15.2023.11.26 val PER: 0.1174
2026-01-08 20:32:50,695: t15.2023.12.03 val PER: 0.1134
2026-01-08 20:32:50,695: t15.2023.12.08 val PER: 0.0965
2026-01-08 20:32:50,695: t15.2023.12.10 val PER: 0.0920
2026-01-08 20:32:50,695: t15.2023.12.17 val PER: 0.1247
2026-01-08 20:32:50,695: t15.2023.12.29 val PER: 0.1242
2026-01-08 20:32:50,695: t15.2024.02.25 val PER: 0.1081
2026-01-08 20:32:50,695: t15.2024.03.08 val PER: 0.2304
2026-01-08 20:32:50,695: t15.2024.03.15 val PER: 0.1939
2026-01-08 20:32:50,695: t15.2024.03.17 val PER: 0.1332
2026-01-08 20:32:50,695: t15.2024.05.10 val PER: 0.1605
2026-01-08 20:32:50,695: t15.2024.06.14 val PER: 0.1609
2026-01-08 20:32:50,695: t15.2024.07.19 val PER: 0.2360
2026-01-08 20:32:50,696: t15.2024.07.21 val PER: 0.0924
2026-01-08 20:32:50,696: t15.2024.07.28 val PER: 0.1294
2026-01-08 20:32:50,696: t15.2025.01.10 val PER: 0.2920
2026-01-08 20:32:50,696: t15.2025.01.12 val PER: 0.1432
2026-01-08 20:32:50,696: t15.2025.03.14 val PER: 0.3373
2026-01-08 20:32:50,696: t15.2025.03.16 val PER: 0.1885
2026-01-08 20:32:50,696: t15.2025.03.30 val PER: 0.2897
2026-01-08 20:32:50,696: t15.2025.04.13 val PER: 0.2140
2026-01-08 20:32:50,837: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_17000
2026-01-08 20:33:07,724: Train batch 17200: loss: 9.59 grad norm: 51.18 time: 0.087
2026-01-08 20:33:26,378: Train batch 17400: loss: 12.65 grad norm: 61.83 time: 0.073
2026-01-08 20:33:35,488: Running test after training batch: 17500
2026-01-08 20:33:35,647: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:33:40,938: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:33:40,985: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:33:53,350: Val batch 17500: PER (avg): 0.1478 CTC Loss (avg): 15.2161 WER(5gram): 14.67% (n=256) time: 17.861
2026-01-08 20:33:53,350: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 20:33:53,351: t15.2023.08.13 val PER: 0.1050
2026-01-08 20:33:53,351: t15.2023.08.18 val PER: 0.1073
2026-01-08 20:33:53,351: t15.2023.08.20 val PER: 0.1112
2026-01-08 20:33:53,351: t15.2023.08.25 val PER: 0.0858
2026-01-08 20:33:53,351: t15.2023.08.27 val PER: 0.1881
2026-01-08 20:33:53,351: t15.2023.09.01 val PER: 0.0739
2026-01-08 20:33:53,352: t15.2023.09.03 val PER: 0.1556
2026-01-08 20:33:53,352: t15.2023.09.24 val PER: 0.1250
2026-01-08 20:33:53,352: t15.2023.09.29 val PER: 0.1295
2026-01-08 20:33:53,352: t15.2023.10.01 val PER: 0.1704
2026-01-08 20:33:53,352: t15.2023.10.06 val PER: 0.0840
2026-01-08 20:33:53,352: t15.2023.10.08 val PER: 0.2585
2026-01-08 20:33:53,352: t15.2023.10.13 val PER: 0.2033
2026-01-08 20:33:53,352: t15.2023.10.15 val PER: 0.1503
2026-01-08 20:33:53,352: t15.2023.10.20 val PER: 0.1879
2026-01-08 20:33:53,352: t15.2023.10.22 val PER: 0.1091
2026-01-08 20:33:53,353: t15.2023.11.03 val PER: 0.1777
2026-01-08 20:33:53,353: t15.2023.11.04 val PER: 0.0341
2026-01-08 20:33:53,353: t15.2023.11.17 val PER: 0.0404
2026-01-08 20:33:53,353: t15.2023.11.19 val PER: 0.0319
2026-01-08 20:33:53,353: t15.2023.11.26 val PER: 0.1188
2026-01-08 20:33:53,353: t15.2023.12.03 val PER: 0.1124
2026-01-08 20:33:53,353: t15.2023.12.08 val PER: 0.0979
2026-01-08 20:33:53,353: t15.2023.12.10 val PER: 0.0920
2026-01-08 20:33:53,353: t15.2023.12.17 val PER: 0.1247
2026-01-08 20:33:53,353: t15.2023.12.29 val PER: 0.1263
2026-01-08 20:33:53,353: t15.2024.02.25 val PER: 0.1053
2026-01-08 20:33:53,354: t15.2024.03.08 val PER: 0.2276
2026-01-08 20:33:53,354: t15.2024.03.15 val PER: 0.1926
2026-01-08 20:33:53,354: t15.2024.03.17 val PER: 0.1360
2026-01-08 20:33:53,354: t15.2024.05.10 val PER: 0.1560
2026-01-08 20:33:53,354: t15.2024.06.14 val PER: 0.1609
2026-01-08 20:33:53,354: t15.2024.07.19 val PER: 0.2347
2026-01-08 20:33:53,354: t15.2024.07.21 val PER: 0.0903
2026-01-08 20:33:53,354: t15.2024.07.28 val PER: 0.1287
2026-01-08 20:33:53,354: t15.2025.01.10 val PER: 0.2893
2026-01-08 20:33:53,355: t15.2025.01.12 val PER: 0.1416
2026-01-08 20:33:53,355: t15.2025.03.14 val PER: 0.3388
2026-01-08 20:33:53,355: t15.2025.03.16 val PER: 0.1885
2026-01-08 20:33:53,355: t15.2025.03.30 val PER: 0.2851
2026-01-08 20:33:53,355: t15.2025.04.13 val PER: 0.2126
2026-01-08 20:33:53,494: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_17500
2026-01-08 20:34:01,872: Train batch 17600: loss: 10.09 grad norm: 56.20 time: 0.052
2026-01-08 20:34:19,136: Train batch 17800: loss: 6.44 grad norm: 47.90 time: 0.044
2026-01-08 20:34:37,448: Train batch 18000: loss: 11.28 grad norm: 62.44 time: 0.062
2026-01-08 20:34:37,449: Running test after training batch: 18000
2026-01-08 20:34:37,594: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:34:42,568: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:34:42,613: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:34:54,645: Val batch 18000: PER (avg): 0.1478 CTC Loss (avg): 15.2191 WER(5gram): 14.99% (n=256) time: 17.195
2026-01-08 20:34:54,645: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 20:34:54,645: t15.2023.08.13 val PER: 0.1060
2026-01-08 20:34:54,645: t15.2023.08.18 val PER: 0.1098
2026-01-08 20:34:54,645: t15.2023.08.20 val PER: 0.1112
2026-01-08 20:34:54,645: t15.2023.08.25 val PER: 0.0858
2026-01-08 20:34:54,645: t15.2023.08.27 val PER: 0.1929
2026-01-08 20:34:54,645: t15.2023.09.01 val PER: 0.0731
2026-01-08 20:34:54,645: t15.2023.09.03 val PER: 0.1568
2026-01-08 20:34:54,646: t15.2023.09.24 val PER: 0.1262
2026-01-08 20:34:54,646: t15.2023.09.29 val PER: 0.1295
2026-01-08 20:34:54,646: t15.2023.10.01 val PER: 0.1697
2026-01-08 20:34:54,646: t15.2023.10.06 val PER: 0.0840
2026-01-08 20:34:54,646: t15.2023.10.08 val PER: 0.2585
2026-01-08 20:34:54,646: t15.2023.10.13 val PER: 0.1994
2026-01-08 20:34:54,646: t15.2023.10.15 val PER: 0.1503
2026-01-08 20:34:54,646: t15.2023.10.20 val PER: 0.1879
2026-01-08 20:34:54,646: t15.2023.10.22 val PER: 0.1069
2026-01-08 20:34:54,646: t15.2023.11.03 val PER: 0.1777
2026-01-08 20:34:54,647: t15.2023.11.04 val PER: 0.0341
2026-01-08 20:34:54,647: t15.2023.11.17 val PER: 0.0404
2026-01-08 20:34:54,647: t15.2023.11.19 val PER: 0.0319
2026-01-08 20:34:54,647: t15.2023.11.26 val PER: 0.1181
2026-01-08 20:34:54,647: t15.2023.12.03 val PER: 0.1103
2026-01-08 20:34:54,647: t15.2023.12.08 val PER: 0.0999
2026-01-08 20:34:54,647: t15.2023.12.10 val PER: 0.0920
2026-01-08 20:34:54,647: t15.2023.12.17 val PER: 0.1299
2026-01-08 20:34:54,647: t15.2023.12.29 val PER: 0.1283
2026-01-08 20:34:54,647: t15.2024.02.25 val PER: 0.1025
2026-01-08 20:34:54,647: t15.2024.03.08 val PER: 0.2276
2026-01-08 20:34:54,647: t15.2024.03.15 val PER: 0.1932
2026-01-08 20:34:54,647: t15.2024.03.17 val PER: 0.1325
2026-01-08 20:34:54,647: t15.2024.05.10 val PER: 0.1590
2026-01-08 20:34:54,647: t15.2024.06.14 val PER: 0.1593
2026-01-08 20:34:54,647: t15.2024.07.19 val PER: 0.2334
2026-01-08 20:34:54,647: t15.2024.07.21 val PER: 0.0890
2026-01-08 20:34:54,648: t15.2024.07.28 val PER: 0.1294
2026-01-08 20:34:54,648: t15.2025.01.10 val PER: 0.2879
2026-01-08 20:34:54,648: t15.2025.01.12 val PER: 0.1416
2026-01-08 20:34:54,648: t15.2025.03.14 val PER: 0.3402
2026-01-08 20:34:54,648: t15.2025.03.16 val PER: 0.1911
2026-01-08 20:34:54,648: t15.2025.03.30 val PER: 0.2839
2026-01-08 20:34:54,648: t15.2025.04.13 val PER: 0.2097
2026-01-08 20:34:54,791: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_18000
2026-01-08 20:35:12,224: Train batch 18200: loss: 7.65 grad norm: 46.49 time: 0.073
2026-01-08 20:35:29,730: Train batch 18400: loss: 5.32 grad norm: 42.64 time: 0.059
2026-01-08 20:35:38,733: Running test after training batch: 18500
2026-01-08 20:35:38,836: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:35:43,836: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:35:43,880: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:35:56,251: Val batch 18500: PER (avg): 0.1483 CTC Loss (avg): 15.2057 WER(5gram): 14.99% (n=256) time: 17.518
2026-01-08 20:35:56,252: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 20:35:56,252: t15.2023.08.13 val PER: 0.1060
2026-01-08 20:35:56,252: t15.2023.08.18 val PER: 0.1081
2026-01-08 20:35:56,252: t15.2023.08.20 val PER: 0.1096
2026-01-08 20:35:56,252: t15.2023.08.25 val PER: 0.0843
2026-01-08 20:35:56,252: t15.2023.08.27 val PER: 0.1913
2026-01-08 20:35:56,252: t15.2023.09.01 val PER: 0.0755
2026-01-08 20:35:56,252: t15.2023.09.03 val PER: 0.1580
2026-01-08 20:35:56,252: t15.2023.09.24 val PER: 0.1262
2026-01-08 20:35:56,253: t15.2023.09.29 val PER: 0.1308
2026-01-08 20:35:56,253: t15.2023.10.01 val PER: 0.1717
2026-01-08 20:35:56,253: t15.2023.10.06 val PER: 0.0872
2026-01-08 20:35:56,253: t15.2023.10.08 val PER: 0.2558
2026-01-08 20:35:56,253: t15.2023.10.13 val PER: 0.2025
2026-01-08 20:35:56,253: t15.2023.10.15 val PER: 0.1510
2026-01-08 20:35:56,253: t15.2023.10.20 val PER: 0.1879
2026-01-08 20:35:56,253: t15.2023.10.22 val PER: 0.1047
2026-01-08 20:35:56,253: t15.2023.11.03 val PER: 0.1791
2026-01-08 20:35:56,253: t15.2023.11.04 val PER: 0.0307
2026-01-08 20:35:56,253: t15.2023.11.17 val PER: 0.0389
2026-01-08 20:35:56,253: t15.2023.11.19 val PER: 0.0339
2026-01-08 20:35:56,253: t15.2023.11.26 val PER: 0.1203
2026-01-08 20:35:56,253: t15.2023.12.03 val PER: 0.1124
2026-01-08 20:35:56,253: t15.2023.12.08 val PER: 0.0992
2026-01-08 20:35:56,254: t15.2023.12.10 val PER: 0.0920
2026-01-08 20:35:56,254: t15.2023.12.17 val PER: 0.1237
2026-01-08 20:35:56,254: t15.2023.12.29 val PER: 0.1283
2026-01-08 20:35:56,254: t15.2024.02.25 val PER: 0.1067
2026-01-08 20:35:56,254: t15.2024.03.08 val PER: 0.2248
2026-01-08 20:35:56,254: t15.2024.03.15 val PER: 0.1957
2026-01-08 20:35:56,254: t15.2024.03.17 val PER: 0.1332
2026-01-08 20:35:56,255: t15.2024.05.10 val PER: 0.1634
2026-01-08 20:35:56,255: t15.2024.06.14 val PER: 0.1625
2026-01-08 20:35:56,255: t15.2024.07.19 val PER: 0.2347
2026-01-08 20:35:56,255: t15.2024.07.21 val PER: 0.0903
2026-01-08 20:35:56,255: t15.2024.07.28 val PER: 0.1287
2026-01-08 20:35:56,255: t15.2025.01.10 val PER: 0.2893
2026-01-08 20:35:56,255: t15.2025.01.12 val PER: 0.1440
2026-01-08 20:35:56,255: t15.2025.03.14 val PER: 0.3402
2026-01-08 20:35:56,255: t15.2025.03.16 val PER: 0.1885
2026-01-08 20:35:56,255: t15.2025.03.30 val PER: 0.2839
2026-01-08 20:35:56,255: t15.2025.04.13 val PER: 0.2097
2026-01-08 20:35:56,396: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_18500
2026-01-08 20:36:04,806: Train batch 18600: loss: 12.79 grad norm: 62.61 time: 0.068
2026-01-08 20:36:21,803: Train batch 18800: loss: 8.75 grad norm: 51.04 time: 0.065
2026-01-08 20:36:39,209: Train batch 19000: loss: 8.58 grad norm: 46.52 time: 0.065
2026-01-08 20:36:39,209: Running test after training batch: 19000
2026-01-08 20:36:39,334: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:36:44,328: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:36:44,385: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:36:57,006: Val batch 19000: PER (avg): 0.1480 CTC Loss (avg): 15.1974 WER(5gram): 14.93% (n=256) time: 17.796
2026-01-08 20:36:57,006: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 20:36:57,007: t15.2023.08.13 val PER: 0.1071
2026-01-08 20:36:57,007: t15.2023.08.18 val PER: 0.1081
2026-01-08 20:36:57,007: t15.2023.08.20 val PER: 0.1104
2026-01-08 20:36:57,007: t15.2023.08.25 val PER: 0.0843
2026-01-08 20:36:57,007: t15.2023.08.27 val PER: 0.1897
2026-01-08 20:36:57,007: t15.2023.09.01 val PER: 0.0747
2026-01-08 20:36:57,007: t15.2023.09.03 val PER: 0.1568
2026-01-08 20:36:57,007: t15.2023.09.24 val PER: 0.1250
2026-01-08 20:36:57,007: t15.2023.09.29 val PER: 0.1315
2026-01-08 20:36:57,007: t15.2023.10.01 val PER: 0.1711
2026-01-08 20:36:57,007: t15.2023.10.06 val PER: 0.0872
2026-01-08 20:36:57,007: t15.2023.10.08 val PER: 0.2571
2026-01-08 20:36:57,007: t15.2023.10.13 val PER: 0.2002
2026-01-08 20:36:57,008: t15.2023.10.15 val PER: 0.1483
2026-01-08 20:36:57,008: t15.2023.10.20 val PER: 0.1879
2026-01-08 20:36:57,008: t15.2023.10.22 val PER: 0.1069
2026-01-08 20:36:57,008: t15.2023.11.03 val PER: 0.1777
2026-01-08 20:36:57,008: t15.2023.11.04 val PER: 0.0307
2026-01-08 20:36:57,008: t15.2023.11.17 val PER: 0.0420
2026-01-08 20:36:57,008: t15.2023.11.19 val PER: 0.0319
2026-01-08 20:36:57,008: t15.2023.11.26 val PER: 0.1188
2026-01-08 20:36:57,008: t15.2023.12.03 val PER: 0.1103
2026-01-08 20:36:57,008: t15.2023.12.08 val PER: 0.0985
2026-01-08 20:36:57,008: t15.2023.12.10 val PER: 0.0920
2026-01-08 20:36:57,008: t15.2023.12.17 val PER: 0.1258
2026-01-08 20:36:57,008: t15.2023.12.29 val PER: 0.1290
2026-01-08 20:36:57,008: t15.2024.02.25 val PER: 0.1039
2026-01-08 20:36:57,009: t15.2024.03.08 val PER: 0.2219
2026-01-08 20:36:57,009: t15.2024.03.15 val PER: 0.1957
2026-01-08 20:36:57,009: t15.2024.03.17 val PER: 0.1332
2026-01-08 20:36:57,009: t15.2024.05.10 val PER: 0.1590
2026-01-08 20:36:57,009: t15.2024.06.14 val PER: 0.1609
2026-01-08 20:36:57,009: t15.2024.07.19 val PER: 0.2380
2026-01-08 20:36:57,009: t15.2024.07.21 val PER: 0.0910
2026-01-08 20:36:57,009: t15.2024.07.28 val PER: 0.1309
2026-01-08 20:36:57,009: t15.2025.01.10 val PER: 0.2906
2026-01-08 20:36:57,009: t15.2025.01.12 val PER: 0.1409
2026-01-08 20:36:57,009: t15.2025.03.14 val PER: 0.3373
2026-01-08 20:36:57,009: t15.2025.03.16 val PER: 0.1859
2026-01-08 20:36:57,009: t15.2025.03.30 val PER: 0.2851
2026-01-08 20:36:57,009: t15.2025.04.13 val PER: 0.2111
2026-01-08 20:36:57,154: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_19000
2026-01-08 20:37:14,665: Train batch 19200: loss: 6.32 grad norm: 46.95 time: 0.064
2026-01-08 20:37:32,472: Train batch 19400: loss: 5.06 grad norm: 37.75 time: 0.056
2026-01-08 20:37:41,427: Running test after training batch: 19500
2026-01-08 20:37:41,565: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:37:46,510: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:37:46,557: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:37:59,137: Val batch 19500: PER (avg): 0.1483 CTC Loss (avg): 15.2008 WER(5gram): 14.86% (n=256) time: 17.709
2026-01-08 20:37:59,137: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 20:37:59,137: t15.2023.08.13 val PER: 0.1081
2026-01-08 20:37:59,137: t15.2023.08.18 val PER: 0.1081
2026-01-08 20:37:59,137: t15.2023.08.20 val PER: 0.1104
2026-01-08 20:37:59,138: t15.2023.08.25 val PER: 0.0843
2026-01-08 20:37:59,138: t15.2023.08.27 val PER: 0.1945
2026-01-08 20:37:59,138: t15.2023.09.01 val PER: 0.0747
2026-01-08 20:37:59,138: t15.2023.09.03 val PER: 0.1556
2026-01-08 20:37:59,138: t15.2023.09.24 val PER: 0.1250
2026-01-08 20:37:59,138: t15.2023.09.29 val PER: 0.1308
2026-01-08 20:37:59,138: t15.2023.10.01 val PER: 0.1717
2026-01-08 20:37:59,138: t15.2023.10.06 val PER: 0.0850
2026-01-08 20:37:59,138: t15.2023.10.08 val PER: 0.2598
2026-01-08 20:37:59,138: t15.2023.10.13 val PER: 0.2017
2026-01-08 20:37:59,138: t15.2023.10.15 val PER: 0.1496
2026-01-08 20:37:59,138: t15.2023.10.20 val PER: 0.1879
2026-01-08 20:37:59,138: t15.2023.10.22 val PER: 0.1047
2026-01-08 20:37:59,138: t15.2023.11.03 val PER: 0.1784
2026-01-08 20:37:59,138: t15.2023.11.04 val PER: 0.0307
2026-01-08 20:37:59,138: t15.2023.11.17 val PER: 0.0404
2026-01-08 20:37:59,138: t15.2023.11.19 val PER: 0.0319
2026-01-08 20:37:59,139: t15.2023.11.26 val PER: 0.1181
2026-01-08 20:37:59,139: t15.2023.12.03 val PER: 0.1082
2026-01-08 20:37:59,139: t15.2023.12.08 val PER: 0.1005
2026-01-08 20:37:59,139: t15.2023.12.10 val PER: 0.0920
2026-01-08 20:37:59,148: t15.2023.12.17 val PER: 0.1279
2026-01-08 20:37:59,149: t15.2023.12.29 val PER: 0.1277
2026-01-08 20:37:59,149: t15.2024.02.25 val PER: 0.1067
2026-01-08 20:37:59,149: t15.2024.03.08 val PER: 0.2248
2026-01-08 20:37:59,149: t15.2024.03.15 val PER: 0.1951
2026-01-08 20:37:59,149: t15.2024.03.17 val PER: 0.1325
2026-01-08 20:37:59,149: t15.2024.05.10 val PER: 0.1620
2026-01-08 20:37:59,149: t15.2024.06.14 val PER: 0.1609
2026-01-08 20:37:59,149: t15.2024.07.19 val PER: 0.2367
2026-01-08 20:37:59,149: t15.2024.07.21 val PER: 0.0917
2026-01-08 20:37:59,149: t15.2024.07.28 val PER: 0.1279
2026-01-08 20:37:59,150: t15.2025.01.10 val PER: 0.2920
2026-01-08 20:37:59,150: t15.2025.01.12 val PER: 0.1416
2026-01-08 20:37:59,150: t15.2025.03.14 val PER: 0.3402
2026-01-08 20:37:59,150: t15.2025.03.16 val PER: 0.1898
2026-01-08 20:37:59,150: t15.2025.03.30 val PER: 0.2839
2026-01-08 20:37:59,150: t15.2025.04.13 val PER: 0.2126
2026-01-08 20:37:59,288: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_19500
2026-01-08 20:38:07,778: Train batch 19600: loss: 7.88 grad norm: 47.13 time: 0.058
2026-01-08 20:38:25,054: Train batch 19800: loss: 7.60 grad norm: 48.42 time: 0.058
2026-01-08 20:38:43,368: Running test after training batch: 19999
2026-01-08 20:38:43,464: WER debug GT example: You can see the code at this point as well.
2026-01-08 20:38:48,406: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 20:38:48,452: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 20:39:00,957: Val batch 19999: PER (avg): 0.1478 CTC Loss (avg): 15.1944 WER(5gram): 14.93% (n=256) time: 17.589
2026-01-08 20:39:00,957: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 20:39:00,962: t15.2023.08.13 val PER: 0.1060
2026-01-08 20:39:00,962: t15.2023.08.18 val PER: 0.1056
2026-01-08 20:39:00,962: t15.2023.08.20 val PER: 0.1112
2026-01-08 20:39:00,962: t15.2023.08.25 val PER: 0.0843
2026-01-08 20:39:00,962: t15.2023.08.27 val PER: 0.1881
2026-01-08 20:39:00,962: t15.2023.09.01 val PER: 0.0755
2026-01-08 20:39:00,962: t15.2023.09.03 val PER: 0.1556
2026-01-08 20:39:00,962: t15.2023.09.24 val PER: 0.1250
2026-01-08 20:39:00,962: t15.2023.09.29 val PER: 0.1308
2026-01-08 20:39:00,962: t15.2023.10.01 val PER: 0.1711
2026-01-08 20:39:00,963: t15.2023.10.06 val PER: 0.0850
2026-01-08 20:39:00,963: t15.2023.10.08 val PER: 0.2585
2026-01-08 20:39:00,963: t15.2023.10.13 val PER: 0.2009
2026-01-08 20:39:00,963: t15.2023.10.15 val PER: 0.1483
2026-01-08 20:39:00,963: t15.2023.10.20 val PER: 0.1879
2026-01-08 20:39:00,963: t15.2023.10.22 val PER: 0.1069
2026-01-08 20:39:00,963: t15.2023.11.03 val PER: 0.1791
2026-01-08 20:39:00,963: t15.2023.11.04 val PER: 0.0307
2026-01-08 20:39:00,963: t15.2023.11.17 val PER: 0.0404
2026-01-08 20:39:00,963: t15.2023.11.19 val PER: 0.0319
2026-01-08 20:39:00,963: t15.2023.11.26 val PER: 0.1174
2026-01-08 20:39:00,964: t15.2023.12.03 val PER: 0.1103
2026-01-08 20:39:00,964: t15.2023.12.08 val PER: 0.1019
2026-01-08 20:39:00,964: t15.2023.12.10 val PER: 0.0907
2026-01-08 20:39:00,964: t15.2023.12.17 val PER: 0.1258
2026-01-08 20:39:00,964: t15.2023.12.29 val PER: 0.1277
2026-01-08 20:39:00,964: t15.2024.02.25 val PER: 0.1053
2026-01-08 20:39:00,965: t15.2024.03.08 val PER: 0.2248
2026-01-08 20:39:00,965: t15.2024.03.15 val PER: 0.1951
2026-01-08 20:39:00,965: t15.2024.03.17 val PER: 0.1311
2026-01-08 20:39:00,965: t15.2024.05.10 val PER: 0.1590
2026-01-08 20:39:00,965: t15.2024.06.14 val PER: 0.1609
2026-01-08 20:39:00,965: t15.2024.07.19 val PER: 0.2360
2026-01-08 20:39:00,965: t15.2024.07.21 val PER: 0.0897
2026-01-08 20:39:00,965: t15.2024.07.28 val PER: 0.1272
2026-01-08 20:39:00,965: t15.2025.01.10 val PER: 0.2920
2026-01-08 20:39:00,965: t15.2025.01.12 val PER: 0.1409
2026-01-08 20:39:00,965: t15.2025.03.14 val PER: 0.3402
2026-01-08 20:39:00,965: t15.2025.03.16 val PER: 0.1872
2026-01-08 20:39:00,965: t15.2025.03.30 val PER: 0.2874
2026-01-08 20:39:00,965: t15.2025.04.13 val PER: 0.2126
2026-01-08 20:39:01,106: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/ablations/stepdrop_15k/checkpoint/checkpoint_batch_19999
2026-01-08 20:39:01,737: Best avg val PER achieved: 0.14974
2026-01-08 20:39:01,737: Total training time: 46.08 minutes
All runs finished. Outputs in: /home/e12511253/Brain2Text/brain2text/trained_models
