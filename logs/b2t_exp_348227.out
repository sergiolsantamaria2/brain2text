torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  3 20:15 /tmp/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
PYTHONFAULTHANDLER=1
MAX_JOBS=8
TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_348227
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_348227/torch_extensions
TORCH_CUDA_ARCH_LIST=8.0
WANDB_DIR=/tmp/e12511253_b2t_348227/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib64:/tmp/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  3 20:15 /tmp/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
PYTHONFAULTHANDLER=1
MAX_JOBS=8
TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_348227
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_348227/torch_extensions
TORCH_CUDA_ARCH_LIST=8.0
WANDB_DIR=/tmp/e12511253_b2t_348227/wandb
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/pkgs/cuda-toolkit/lib64/libcudart.so.11.7.60
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/pkgs/cuda-toolkit/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
trained_models -> /tmp/e12511253_b2t_348227/trained_models
==============================================
Job: b2t_exp  ID: 348227
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/gru/rnn_dropout/lr40
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/gru/rnn_dropout/lr40 ==========
Num configs: 5

=== RUN base.yaml ===
2026-01-03 20:15:31,536: Using device: cuda:0
2026-01-03 20:15:32,935: Local LM WER requested (eval.compute_wer=true) but could not initialize lm_decoder. Disabling WER for this run and continuing. Reason: name 'Path' is not defined
2026-01-03 20:15:32,959: Using 45 sessions after filtering (from 45).
2026-01-03 20:15:33,371: Using torch.compile (if available)
2026-01-03 20:15:33,371: torch.compile not available (torch<2.0). Skipping.
2026-01-03 20:15:33,372: Initialized RNN decoding model
2026-01-03 20:15:33,372: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 20:15:33,372: Model has 44,907,305 parameters
2026-01-03 20:15:33,372: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 20:15:34,635: Successfully initialized datasets
2026-01-03 20:15:34,636: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 20:15:36,974: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.181
2026-01-03 20:15:36,974: Running test after training batch: 0
2026-01-03 20:15:41,735: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): nan% (n=0) time: 4.761
2026-01-03 20:15:41,735: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:15:41,736: t15.2023.08.13 val PER: 1.3056
2026-01-03 20:15:41,736: t15.2023.08.18 val PER: 1.4208
2026-01-03 20:15:41,736: t15.2023.08.20 val PER: 1.3002
2026-01-03 20:15:41,736: t15.2023.08.25 val PER: 1.3389
2026-01-03 20:15:41,736: t15.2023.08.27 val PER: 1.2460
2026-01-03 20:15:41,736: t15.2023.09.01 val PER: 1.4537
2026-01-03 20:15:41,736: t15.2023.09.03 val PER: 1.3171
2026-01-03 20:15:41,736: t15.2023.09.24 val PER: 1.5461
2026-01-03 20:15:41,736: t15.2023.09.29 val PER: 1.4671
2026-01-03 20:15:41,736: t15.2023.10.01 val PER: 1.2147
2026-01-03 20:15:41,737: t15.2023.10.06 val PER: 1.4876
2026-01-03 20:15:41,737: t15.2023.10.08 val PER: 1.1827
2026-01-03 20:15:41,737: t15.2023.10.13 val PER: 1.3964
2026-01-03 20:15:41,737: t15.2023.10.15 val PER: 1.3889
2026-01-03 20:15:41,737: t15.2023.10.20 val PER: 1.4866
2026-01-03 20:15:41,737: t15.2023.10.22 val PER: 1.3942
2026-01-03 20:15:41,737: t15.2023.11.03 val PER: 1.5923
2026-01-03 20:15:41,737: t15.2023.11.04 val PER: 2.0171
2026-01-03 20:15:41,737: t15.2023.11.17 val PER: 1.9518
2026-01-03 20:15:41,737: t15.2023.11.19 val PER: 1.6707
2026-01-03 20:15:41,737: t15.2023.11.26 val PER: 1.5413
2026-01-03 20:15:41,738: t15.2023.12.03 val PER: 1.4254
2026-01-03 20:15:41,738: t15.2023.12.08 val PER: 1.4487
2026-01-03 20:15:41,738: t15.2023.12.10 val PER: 1.6899
2026-01-03 20:15:41,738: t15.2023.12.17 val PER: 1.3077
2026-01-03 20:15:41,738: t15.2023.12.29 val PER: 1.4063
2026-01-03 20:15:41,738: t15.2024.02.25 val PER: 1.4228
2026-01-03 20:15:41,738: t15.2024.03.08 val PER: 1.3257
2026-01-03 20:15:41,738: t15.2024.03.15 val PER: 1.3196
2026-01-03 20:15:41,738: t15.2024.03.17 val PER: 1.4052
2026-01-03 20:15:41,738: t15.2024.05.10 val PER: 1.3224
2026-01-03 20:15:41,738: t15.2024.06.14 val PER: 1.5315
2026-01-03 20:15:41,738: t15.2024.07.19 val PER: 1.0817
2026-01-03 20:15:41,739: t15.2024.07.21 val PER: 1.6290
2026-01-03 20:15:41,739: t15.2024.07.28 val PER: 1.6588
2026-01-03 20:15:41,739: t15.2025.01.10 val PER: 1.0923
2026-01-03 20:15:41,739: t15.2025.01.12 val PER: 1.7629
2026-01-03 20:15:41,739: t15.2025.03.14 val PER: 1.0414
2026-01-03 20:15:41,739: t15.2025.03.16 val PER: 1.6257
2026-01-03 20:15:41,739: t15.2025.03.30 val PER: 1.2874
2026-01-03 20:15:41,739: t15.2025.04.13 val PER: 1.5949
2026-01-03 20:15:41,739: New best val PER inf --> 1.4293
2026-01-03 20:15:41,740: Checkpointing model
2026-01-03 20:15:41,979: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:15:42,222: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_0
2026-01-03 20:15:59,950: Train batch 200: loss: 77.59 grad norm: 106.07 time: 0.055
2026-01-03 20:16:17,038: Train batch 400: loss: 53.23 grad norm: 85.17 time: 0.063
2026-01-03 20:16:25,666: Running test after training batch: 500
2026-01-03 20:16:30,444: Val batch 500: PER (avg): 0.5219 CTC Loss (avg): 55.9422 WER(1gram): nan% (n=0) time: 4.777
2026-01-03 20:16:30,445: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:16:30,445: t15.2023.08.13 val PER: 0.4615
2026-01-03 20:16:30,445: t15.2023.08.18 val PER: 0.4543
2026-01-03 20:16:30,445: t15.2023.08.20 val PER: 0.4384
2026-01-03 20:16:30,445: t15.2023.08.25 val PER: 0.4383
2026-01-03 20:16:30,446: t15.2023.08.27 val PER: 0.5225
2026-01-03 20:16:30,446: t15.2023.09.01 val PER: 0.4188
2026-01-03 20:16:30,446: t15.2023.09.03 val PER: 0.5071
2026-01-03 20:16:30,446: t15.2023.09.24 val PER: 0.4466
2026-01-03 20:16:30,446: t15.2023.09.29 val PER: 0.4761
2026-01-03 20:16:30,446: t15.2023.10.01 val PER: 0.5211
2026-01-03 20:16:30,446: t15.2023.10.06 val PER: 0.4370
2026-01-03 20:16:30,446: t15.2023.10.08 val PER: 0.5345
2026-01-03 20:16:30,446: t15.2023.10.13 val PER: 0.5803
2026-01-03 20:16:30,446: t15.2023.10.15 val PER: 0.5023
2026-01-03 20:16:30,447: t15.2023.10.20 val PER: 0.4664
2026-01-03 20:16:30,447: t15.2023.10.22 val PER: 0.4454
2026-01-03 20:16:30,447: t15.2023.11.03 val PER: 0.5014
2026-01-03 20:16:30,447: t15.2023.11.04 val PER: 0.2730
2026-01-03 20:16:30,447: t15.2023.11.17 val PER: 0.3717
2026-01-03 20:16:30,447: t15.2023.11.19 val PER: 0.3333
2026-01-03 20:16:30,447: t15.2023.11.26 val PER: 0.5616
2026-01-03 20:16:30,447: t15.2023.12.03 val PER: 0.4989
2026-01-03 20:16:30,447: t15.2023.12.08 val PER: 0.5340
2026-01-03 20:16:30,447: t15.2023.12.10 val PER: 0.4573
2026-01-03 20:16:30,447: t15.2023.12.17 val PER: 0.5634
2026-01-03 20:16:30,448: t15.2023.12.29 val PER: 0.5607
2026-01-03 20:16:30,448: t15.2024.02.25 val PER: 0.4930
2026-01-03 20:16:30,448: t15.2024.03.08 val PER: 0.6287
2026-01-03 20:16:30,448: t15.2024.03.15 val PER: 0.5604
2026-01-03 20:16:30,448: t15.2024.03.17 val PER: 0.5098
2026-01-03 20:16:30,448: t15.2024.05.10 val PER: 0.5602
2026-01-03 20:16:30,448: t15.2024.06.14 val PER: 0.5079
2026-01-03 20:16:30,448: t15.2024.07.19 val PER: 0.6684
2026-01-03 20:16:30,448: t15.2024.07.21 val PER: 0.4703
2026-01-03 20:16:30,448: t15.2024.07.28 val PER: 0.5066
2026-01-03 20:16:30,448: t15.2025.01.10 val PER: 0.7410
2026-01-03 20:16:30,449: t15.2025.01.12 val PER: 0.5658
2026-01-03 20:16:30,449: t15.2025.03.14 val PER: 0.7219
2026-01-03 20:16:30,449: t15.2025.03.16 val PER: 0.6060
2026-01-03 20:16:30,449: t15.2025.03.30 val PER: 0.7299
2026-01-03 20:16:30,449: t15.2025.04.13 val PER: 0.5777
2026-01-03 20:16:30,449: New best val PER 1.4293 --> 0.5219
2026-01-03 20:16:30,449: Checkpointing model
2026-01-03 20:16:31,041: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:16:31,287: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_500
2026-01-03 20:16:40,008: Train batch 600: loss: 49.14 grad norm: 80.54 time: 0.077
2026-01-03 20:16:57,525: Train batch 800: loss: 40.74 grad norm: 84.94 time: 0.057
2026-01-03 20:17:14,816: Train batch 1000: loss: 42.67 grad norm: 81.18 time: 0.065
2026-01-03 20:17:14,816: Running test after training batch: 1000
2026-01-03 20:17:19,555: Val batch 1000: PER (avg): 0.4082 CTC Loss (avg): 42.3038 WER(1gram): nan% (n=0) time: 4.739
2026-01-03 20:17:19,556: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:17:19,556: t15.2023.08.13 val PER: 0.3763
2026-01-03 20:17:19,556: t15.2023.08.18 val PER: 0.3403
2026-01-03 20:17:19,556: t15.2023.08.20 val PER: 0.3542
2026-01-03 20:17:19,556: t15.2023.08.25 val PER: 0.2952
2026-01-03 20:17:19,557: t15.2023.08.27 val PER: 0.4228
2026-01-03 20:17:19,557: t15.2023.09.01 val PER: 0.3036
2026-01-03 20:17:19,557: t15.2023.09.03 val PER: 0.3955
2026-01-03 20:17:19,557: t15.2023.09.24 val PER: 0.3265
2026-01-03 20:17:19,557: t15.2023.09.29 val PER: 0.3574
2026-01-03 20:17:19,557: t15.2023.10.01 val PER: 0.4049
2026-01-03 20:17:19,557: t15.2023.10.06 val PER: 0.3057
2026-01-03 20:17:19,557: t15.2023.10.08 val PER: 0.4574
2026-01-03 20:17:19,557: t15.2023.10.13 val PER: 0.4631
2026-01-03 20:17:19,557: t15.2023.10.15 val PER: 0.3797
2026-01-03 20:17:19,558: t15.2023.10.20 val PER: 0.3691
2026-01-03 20:17:19,558: t15.2023.10.22 val PER: 0.3508
2026-01-03 20:17:19,558: t15.2023.11.03 val PER: 0.3969
2026-01-03 20:17:19,558: t15.2023.11.04 val PER: 0.1672
2026-01-03 20:17:19,558: t15.2023.11.17 val PER: 0.2737
2026-01-03 20:17:19,558: t15.2023.11.19 val PER: 0.2136
2026-01-03 20:17:19,558: t15.2023.11.26 val PER: 0.4420
2026-01-03 20:17:19,558: t15.2023.12.03 val PER: 0.4149
2026-01-03 20:17:19,558: t15.2023.12.08 val PER: 0.4095
2026-01-03 20:17:19,558: t15.2023.12.10 val PER: 0.3430
2026-01-03 20:17:19,558: t15.2023.12.17 val PER: 0.4075
2026-01-03 20:17:19,558: t15.2023.12.29 val PER: 0.4001
2026-01-03 20:17:19,558: t15.2024.02.25 val PER: 0.3455
2026-01-03 20:17:19,558: t15.2024.03.08 val PER: 0.4879
2026-01-03 20:17:19,559: t15.2024.03.15 val PER: 0.4353
2026-01-03 20:17:19,559: t15.2024.03.17 val PER: 0.4128
2026-01-03 20:17:19,559: t15.2024.05.10 val PER: 0.4324
2026-01-03 20:17:19,559: t15.2024.06.14 val PER: 0.3943
2026-01-03 20:17:19,559: t15.2024.07.19 val PER: 0.5274
2026-01-03 20:17:19,559: t15.2024.07.21 val PER: 0.3759
2026-01-03 20:17:19,559: t15.2024.07.28 val PER: 0.4176
2026-01-03 20:17:19,559: t15.2025.01.10 val PER: 0.6253
2026-01-03 20:17:19,559: t15.2025.01.12 val PER: 0.4488
2026-01-03 20:17:19,559: t15.2025.03.14 val PER: 0.6435
2026-01-03 20:17:19,559: t15.2025.03.16 val PER: 0.4895
2026-01-03 20:17:19,559: t15.2025.03.30 val PER: 0.6437
2026-01-03 20:17:19,559: t15.2025.04.13 val PER: 0.4893
2026-01-03 20:17:19,560: New best val PER 0.5219 --> 0.4082
2026-01-03 20:17:19,560: Checkpointing model
2026-01-03 20:17:20,145: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:17:20,393: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_1000
2026-01-03 20:17:37,761: Train batch 1200: loss: 32.80 grad norm: 71.85 time: 0.067
2026-01-03 20:17:55,460: Train batch 1400: loss: 35.76 grad norm: 76.65 time: 0.060
2026-01-03 20:18:04,170: Running test after training batch: 1500
2026-01-03 20:18:09,129: Val batch 1500: PER (avg): 0.3800 CTC Loss (avg): 37.3305 WER(1gram): nan% (n=0) time: 4.959
2026-01-03 20:18:09,130: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:18:09,130: t15.2023.08.13 val PER: 0.3389
2026-01-03 20:18:09,130: t15.2023.08.18 val PER: 0.3135
2026-01-03 20:18:09,130: t15.2023.08.20 val PER: 0.2963
2026-01-03 20:18:09,130: t15.2023.08.25 val PER: 0.2636
2026-01-03 20:18:09,130: t15.2023.08.27 val PER: 0.3955
2026-01-03 20:18:09,130: t15.2023.09.01 val PER: 0.2752
2026-01-03 20:18:09,131: t15.2023.09.03 val PER: 0.3610
2026-01-03 20:18:09,131: t15.2023.09.24 val PER: 0.3119
2026-01-03 20:18:09,131: t15.2023.09.29 val PER: 0.3408
2026-01-03 20:18:09,131: t15.2023.10.01 val PER: 0.3910
2026-01-03 20:18:09,131: t15.2023.10.06 val PER: 0.2939
2026-01-03 20:18:09,131: t15.2023.10.08 val PER: 0.4344
2026-01-03 20:18:09,131: t15.2023.10.13 val PER: 0.4360
2026-01-03 20:18:09,131: t15.2023.10.15 val PER: 0.3659
2026-01-03 20:18:09,131: t15.2023.10.20 val PER: 0.3389
2026-01-03 20:18:09,131: t15.2023.10.22 val PER: 0.3174
2026-01-03 20:18:09,132: t15.2023.11.03 val PER: 0.3657
2026-01-03 20:18:09,132: t15.2023.11.04 val PER: 0.1263
2026-01-03 20:18:09,132: t15.2023.11.17 val PER: 0.2131
2026-01-03 20:18:09,132: t15.2023.11.19 val PER: 0.1836
2026-01-03 20:18:09,132: t15.2023.11.26 val PER: 0.4217
2026-01-03 20:18:09,132: t15.2023.12.03 val PER: 0.3676
2026-01-03 20:18:09,132: t15.2023.12.08 val PER: 0.3542
2026-01-03 20:18:09,132: t15.2023.12.10 val PER: 0.2904
2026-01-03 20:18:09,132: t15.2023.12.17 val PER: 0.3825
2026-01-03 20:18:09,132: t15.2023.12.29 val PER: 0.3699
2026-01-03 20:18:09,132: t15.2024.02.25 val PER: 0.3048
2026-01-03 20:18:09,132: t15.2024.03.08 val PER: 0.4737
2026-01-03 20:18:09,133: t15.2024.03.15 val PER: 0.4103
2026-01-03 20:18:09,133: t15.2024.03.17 val PER: 0.3682
2026-01-03 20:18:09,133: t15.2024.05.10 val PER: 0.3863
2026-01-03 20:18:09,133: t15.2024.06.14 val PER: 0.3912
2026-01-03 20:18:09,133: t15.2024.07.19 val PER: 0.5287
2026-01-03 20:18:09,133: t15.2024.07.21 val PER: 0.3462
2026-01-03 20:18:09,133: t15.2024.07.28 val PER: 0.3691
2026-01-03 20:18:09,133: t15.2025.01.10 val PER: 0.6309
2026-01-03 20:18:09,133: t15.2025.01.12 val PER: 0.4219
2026-01-03 20:18:09,133: t15.2025.03.14 val PER: 0.6154
2026-01-03 20:18:09,133: t15.2025.03.16 val PER: 0.4437
2026-01-03 20:18:09,133: t15.2025.03.30 val PER: 0.6483
2026-01-03 20:18:09,133: t15.2025.04.13 val PER: 0.4907
2026-01-03 20:18:09,134: New best val PER 0.4082 --> 0.3800
2026-01-03 20:18:09,135: Checkpointing model
2026-01-03 20:18:09,724: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:18:09,970: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_1500
2026-01-03 20:18:18,540: Train batch 1600: loss: 36.32 grad norm: 77.42 time: 0.064
2026-01-03 20:18:35,849: Train batch 1800: loss: 35.35 grad norm: 70.51 time: 0.088
2026-01-03 20:18:53,112: Train batch 2000: loss: 34.42 grad norm: 69.80 time: 0.066
2026-01-03 20:18:53,112: Running test after training batch: 2000
2026-01-03 20:18:57,891: Val batch 2000: PER (avg): 0.3268 CTC Loss (avg): 32.9880 WER(1gram): nan% (n=0) time: 4.778
2026-01-03 20:18:57,891: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:18:57,892: t15.2023.08.13 val PER: 0.3202
2026-01-03 20:18:57,892: t15.2023.08.18 val PER: 0.2573
2026-01-03 20:18:57,892: t15.2023.08.20 val PER: 0.2542
2026-01-03 20:18:57,892: t15.2023.08.25 val PER: 0.2274
2026-01-03 20:18:57,892: t15.2023.08.27 val PER: 0.3392
2026-01-03 20:18:57,892: t15.2023.09.01 val PER: 0.2321
2026-01-03 20:18:57,892: t15.2023.09.03 val PER: 0.3266
2026-01-03 20:18:57,893: t15.2023.09.24 val PER: 0.2585
2026-01-03 20:18:57,893: t15.2023.09.29 val PER: 0.2731
2026-01-03 20:18:57,893: t15.2023.10.01 val PER: 0.3250
2026-01-03 20:18:57,893: t15.2023.10.06 val PER: 0.2379
2026-01-03 20:18:57,893: t15.2023.10.08 val PER: 0.3870
2026-01-03 20:18:57,893: t15.2023.10.13 val PER: 0.3701
2026-01-03 20:18:57,893: t15.2023.10.15 val PER: 0.3032
2026-01-03 20:18:57,893: t15.2023.10.20 val PER: 0.2886
2026-01-03 20:18:57,893: t15.2023.10.22 val PER: 0.2684
2026-01-03 20:18:57,894: t15.2023.11.03 val PER: 0.3202
2026-01-03 20:18:57,894: t15.2023.11.04 val PER: 0.1092
2026-01-03 20:18:57,894: t15.2023.11.17 val PER: 0.1835
2026-01-03 20:18:57,894: t15.2023.11.19 val PER: 0.1317
2026-01-03 20:18:57,894: t15.2023.11.26 val PER: 0.3652
2026-01-03 20:18:57,894: t15.2023.12.03 val PER: 0.3172
2026-01-03 20:18:57,894: t15.2023.12.08 val PER: 0.3083
2026-01-03 20:18:57,894: t15.2023.12.10 val PER: 0.2562
2026-01-03 20:18:57,894: t15.2023.12.17 val PER: 0.3098
2026-01-03 20:18:57,895: t15.2023.12.29 val PER: 0.3281
2026-01-03 20:18:57,895: t15.2024.02.25 val PER: 0.2683
2026-01-03 20:18:57,895: t15.2024.03.08 val PER: 0.3983
2026-01-03 20:18:57,895: t15.2024.03.15 val PER: 0.3577
2026-01-03 20:18:57,895: t15.2024.03.17 val PER: 0.3417
2026-01-03 20:18:57,895: t15.2024.05.10 val PER: 0.3418
2026-01-03 20:18:57,895: t15.2024.06.14 val PER: 0.3360
2026-01-03 20:18:57,895: t15.2024.07.19 val PER: 0.4535
2026-01-03 20:18:57,895: t15.2024.07.21 val PER: 0.2903
2026-01-03 20:18:57,895: t15.2024.07.28 val PER: 0.3184
2026-01-03 20:18:57,896: t15.2025.01.10 val PER: 0.5455
2026-01-03 20:18:57,896: t15.2025.01.12 val PER: 0.3903
2026-01-03 20:18:57,896: t15.2025.03.14 val PER: 0.5192
2026-01-03 20:18:57,896: t15.2025.03.16 val PER: 0.3992
2026-01-03 20:18:57,896: t15.2025.03.30 val PER: 0.5494
2026-01-03 20:18:57,896: t15.2025.04.13 val PER: 0.4037
2026-01-03 20:18:57,896: New best val PER 0.3800 --> 0.3268
2026-01-03 20:18:57,896: Checkpointing model
2026-01-03 20:18:58,477: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:18:58,721: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_2000
2026-01-03 20:19:16,532: Train batch 2200: loss: 28.86 grad norm: 67.98 time: 0.060
2026-01-03 20:19:34,291: Train batch 2400: loss: 29.20 grad norm: 62.76 time: 0.052
2026-01-03 20:19:43,059: Running test after training batch: 2500
2026-01-03 20:19:47,822: Val batch 2500: PER (avg): 0.3018 CTC Loss (avg): 30.0951 WER(1gram): nan% (n=0) time: 4.762
2026-01-03 20:19:47,822: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:19:47,822: t15.2023.08.13 val PER: 0.2911
2026-01-03 20:19:47,822: t15.2023.08.18 val PER: 0.2339
2026-01-03 20:19:47,823: t15.2023.08.20 val PER: 0.2303
2026-01-03 20:19:47,823: t15.2023.08.25 val PER: 0.2063
2026-01-03 20:19:47,823: t15.2023.08.27 val PER: 0.3183
2026-01-03 20:19:47,823: t15.2023.09.01 val PER: 0.2127
2026-01-03 20:19:47,823: t15.2023.09.03 val PER: 0.2815
2026-01-03 20:19:47,823: t15.2023.09.24 val PER: 0.2172
2026-01-03 20:19:47,824: t15.2023.09.29 val PER: 0.2540
2026-01-03 20:19:47,824: t15.2023.10.01 val PER: 0.2979
2026-01-03 20:19:47,824: t15.2023.10.06 val PER: 0.2185
2026-01-03 20:19:47,824: t15.2023.10.08 val PER: 0.3843
2026-01-03 20:19:47,824: t15.2023.10.13 val PER: 0.3592
2026-01-03 20:19:47,824: t15.2023.10.15 val PER: 0.2848
2026-01-03 20:19:47,824: t15.2023.10.20 val PER: 0.2819
2026-01-03 20:19:47,824: t15.2023.10.22 val PER: 0.2327
2026-01-03 20:19:47,824: t15.2023.11.03 val PER: 0.2944
2026-01-03 20:19:47,824: t15.2023.11.04 val PER: 0.0887
2026-01-03 20:19:47,825: t15.2023.11.17 val PER: 0.1400
2026-01-03 20:19:47,825: t15.2023.11.19 val PER: 0.1158
2026-01-03 20:19:47,825: t15.2023.11.26 val PER: 0.3420
2026-01-03 20:19:47,825: t15.2023.12.03 val PER: 0.2899
2026-01-03 20:19:47,825: t15.2023.12.08 val PER: 0.2756
2026-01-03 20:19:47,825: t15.2023.12.10 val PER: 0.2497
2026-01-03 20:19:47,825: t15.2023.12.17 val PER: 0.2890
2026-01-03 20:19:47,825: t15.2023.12.29 val PER: 0.3027
2026-01-03 20:19:47,825: t15.2024.02.25 val PER: 0.2430
2026-01-03 20:19:47,825: t15.2024.03.08 val PER: 0.3713
2026-01-03 20:19:47,826: t15.2024.03.15 val PER: 0.3527
2026-01-03 20:19:47,826: t15.2024.03.17 val PER: 0.3145
2026-01-03 20:19:47,826: t15.2024.05.10 val PER: 0.3120
2026-01-03 20:19:47,826: t15.2024.06.14 val PER: 0.3123
2026-01-03 20:19:47,826: t15.2024.07.19 val PER: 0.4331
2026-01-03 20:19:47,826: t15.2024.07.21 val PER: 0.2683
2026-01-03 20:19:47,826: t15.2024.07.28 val PER: 0.3059
2026-01-03 20:19:47,826: t15.2025.01.10 val PER: 0.4862
2026-01-03 20:19:47,827: t15.2025.01.12 val PER: 0.3480
2026-01-03 20:19:47,827: t15.2025.03.14 val PER: 0.4837
2026-01-03 20:19:47,827: t15.2025.03.16 val PER: 0.3599
2026-01-03 20:19:47,827: t15.2025.03.30 val PER: 0.4977
2026-01-03 20:19:47,827: t15.2025.04.13 val PER: 0.3823
2026-01-03 20:19:47,827: New best val PER 0.3268 --> 0.3018
2026-01-03 20:19:47,827: Checkpointing model
2026-01-03 20:19:48,422: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:19:48,668: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_2500
2026-01-03 20:19:57,156: Train batch 2600: loss: 34.85 grad norm: 81.14 time: 0.054
2026-01-03 20:20:14,429: Train batch 2800: loss: 26.20 grad norm: 70.73 time: 0.082
2026-01-03 20:20:31,797: Train batch 3000: loss: 31.33 grad norm: 76.21 time: 0.082
2026-01-03 20:20:31,797: Running test after training batch: 3000
2026-01-03 20:20:36,717: Val batch 3000: PER (avg): 0.2821 CTC Loss (avg): 27.8743 WER(1gram): nan% (n=0) time: 4.920
2026-01-03 20:20:36,718: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:20:36,718: t15.2023.08.13 val PER: 0.2640
2026-01-03 20:20:36,718: t15.2023.08.18 val PER: 0.2230
2026-01-03 20:20:36,718: t15.2023.08.20 val PER: 0.2248
2026-01-03 20:20:36,718: t15.2023.08.25 val PER: 0.1943
2026-01-03 20:20:36,719: t15.2023.08.27 val PER: 0.2926
2026-01-03 20:20:36,719: t15.2023.09.01 val PER: 0.1907
2026-01-03 20:20:36,719: t15.2023.09.03 val PER: 0.2779
2026-01-03 20:20:36,719: t15.2023.09.24 val PER: 0.2209
2026-01-03 20:20:36,719: t15.2023.09.29 val PER: 0.2329
2026-01-03 20:20:36,719: t15.2023.10.01 val PER: 0.2979
2026-01-03 20:20:36,719: t15.2023.10.06 val PER: 0.1948
2026-01-03 20:20:36,719: t15.2023.10.08 val PER: 0.3464
2026-01-03 20:20:36,719: t15.2023.10.13 val PER: 0.3413
2026-01-03 20:20:36,720: t15.2023.10.15 val PER: 0.2604
2026-01-03 20:20:36,720: t15.2023.10.20 val PER: 0.2752
2026-01-03 20:20:36,720: t15.2023.10.22 val PER: 0.2105
2026-01-03 20:20:36,720: t15.2023.11.03 val PER: 0.2761
2026-01-03 20:20:36,720: t15.2023.11.04 val PER: 0.0853
2026-01-03 20:20:36,720: t15.2023.11.17 val PER: 0.1337
2026-01-03 20:20:36,720: t15.2023.11.19 val PER: 0.1038
2026-01-03 20:20:36,720: t15.2023.11.26 val PER: 0.3043
2026-01-03 20:20:36,720: t15.2023.12.03 val PER: 0.2689
2026-01-03 20:20:36,720: t15.2023.12.08 val PER: 0.2617
2026-01-03 20:20:36,720: t15.2023.12.10 val PER: 0.2129
2026-01-03 20:20:36,721: t15.2023.12.17 val PER: 0.2869
2026-01-03 20:20:36,721: t15.2023.12.29 val PER: 0.2862
2026-01-03 20:20:36,721: t15.2024.02.25 val PER: 0.2346
2026-01-03 20:20:36,721: t15.2024.03.08 val PER: 0.3556
2026-01-03 20:20:36,721: t15.2024.03.15 val PER: 0.3315
2026-01-03 20:20:36,721: t15.2024.03.17 val PER: 0.2922
2026-01-03 20:20:36,721: t15.2024.05.10 val PER: 0.2942
2026-01-03 20:20:36,721: t15.2024.06.14 val PER: 0.2981
2026-01-03 20:20:36,721: t15.2024.07.19 val PER: 0.4001
2026-01-03 20:20:36,721: t15.2024.07.21 val PER: 0.2297
2026-01-03 20:20:36,721: t15.2024.07.28 val PER: 0.2757
2026-01-03 20:20:36,721: t15.2025.01.10 val PER: 0.4848
2026-01-03 20:20:36,722: t15.2025.01.12 val PER: 0.3256
2026-01-03 20:20:36,722: t15.2025.03.14 val PER: 0.4438
2026-01-03 20:20:36,722: t15.2025.03.16 val PER: 0.3272
2026-01-03 20:20:36,722: t15.2025.03.30 val PER: 0.4793
2026-01-03 20:20:36,722: t15.2025.04.13 val PER: 0.3552
2026-01-03 20:20:36,722: New best val PER 0.3018 --> 0.2821
2026-01-03 20:20:36,722: Checkpointing model
2026-01-03 20:20:37,296: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:20:37,536: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_3000
2026-01-03 20:20:55,096: Train batch 3200: loss: 26.24 grad norm: 66.01 time: 0.076
2026-01-03 20:21:12,399: Train batch 3400: loss: 18.09 grad norm: 53.74 time: 0.048
2026-01-03 20:21:21,252: Running test after training batch: 3500
2026-01-03 20:21:26,147: Val batch 3500: PER (avg): 0.2668 CTC Loss (avg): 26.5347 WER(1gram): nan% (n=0) time: 4.895
2026-01-03 20:21:26,147: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:21:26,148: t15.2023.08.13 val PER: 0.2464
2026-01-03 20:21:26,148: t15.2023.08.18 val PER: 0.2037
2026-01-03 20:21:26,148: t15.2023.08.20 val PER: 0.2168
2026-01-03 20:21:26,148: t15.2023.08.25 val PER: 0.1627
2026-01-03 20:21:26,148: t15.2023.08.27 val PER: 0.2669
2026-01-03 20:21:26,148: t15.2023.09.01 val PER: 0.1753
2026-01-03 20:21:26,148: t15.2023.09.03 val PER: 0.2482
2026-01-03 20:21:26,148: t15.2023.09.24 val PER: 0.2112
2026-01-03 20:21:26,149: t15.2023.09.29 val PER: 0.2176
2026-01-03 20:21:26,149: t15.2023.10.01 val PER: 0.2814
2026-01-03 20:21:26,149: t15.2023.10.06 val PER: 0.1981
2026-01-03 20:21:26,149: t15.2023.10.08 val PER: 0.3464
2026-01-03 20:21:26,149: t15.2023.10.13 val PER: 0.3119
2026-01-03 20:21:26,149: t15.2023.10.15 val PER: 0.2498
2026-01-03 20:21:26,149: t15.2023.10.20 val PER: 0.2282
2026-01-03 20:21:26,149: t15.2023.10.22 val PER: 0.2094
2026-01-03 20:21:26,149: t15.2023.11.03 val PER: 0.2510
2026-01-03 20:21:26,149: t15.2023.11.04 val PER: 0.0853
2026-01-03 20:21:26,150: t15.2023.11.17 val PER: 0.1182
2026-01-03 20:21:26,150: t15.2023.11.19 val PER: 0.1118
2026-01-03 20:21:26,150: t15.2023.11.26 val PER: 0.2812
2026-01-03 20:21:26,150: t15.2023.12.03 val PER: 0.2447
2026-01-03 20:21:26,150: t15.2023.12.08 val PER: 0.2450
2026-01-03 20:21:26,150: t15.2023.12.10 val PER: 0.1932
2026-01-03 20:21:26,150: t15.2023.12.17 val PER: 0.2620
2026-01-03 20:21:26,150: t15.2023.12.29 val PER: 0.2574
2026-01-03 20:21:26,150: t15.2024.02.25 val PER: 0.2079
2026-01-03 20:21:26,150: t15.2024.03.08 val PER: 0.3471
2026-01-03 20:21:26,151: t15.2024.03.15 val PER: 0.3152
2026-01-03 20:21:26,151: t15.2024.03.17 val PER: 0.2789
2026-01-03 20:21:26,151: t15.2024.05.10 val PER: 0.2749
2026-01-03 20:21:26,151: t15.2024.06.14 val PER: 0.2871
2026-01-03 20:21:26,151: t15.2024.07.19 val PER: 0.3982
2026-01-03 20:21:26,151: t15.2024.07.21 val PER: 0.2241
2026-01-03 20:21:26,151: t15.2024.07.28 val PER: 0.2728
2026-01-03 20:21:26,151: t15.2025.01.10 val PER: 0.4614
2026-01-03 20:21:26,151: t15.2025.01.12 val PER: 0.3064
2026-01-03 20:21:26,151: t15.2025.03.14 val PER: 0.4423
2026-01-03 20:21:26,151: t15.2025.03.16 val PER: 0.3233
2026-01-03 20:21:26,152: t15.2025.03.30 val PER: 0.4586
2026-01-03 20:21:26,152: t15.2025.04.13 val PER: 0.3481
2026-01-03 20:21:26,152: New best val PER 0.2821 --> 0.2668
2026-01-03 20:21:26,152: Checkpointing model
2026-01-03 20:21:26,756: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:21:26,997: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_3500
2026-01-03 20:21:35,682: Train batch 3600: loss: 22.67 grad norm: 64.29 time: 0.067
2026-01-03 20:21:52,960: Train batch 3800: loss: 25.08 grad norm: 66.11 time: 0.066
2026-01-03 20:22:10,640: Train batch 4000: loss: 19.40 grad norm: 54.57 time: 0.056
2026-01-03 20:22:10,643: Running test after training batch: 4000
2026-01-03 20:22:15,397: Val batch 4000: PER (avg): 0.2490 CTC Loss (avg): 24.4678 WER(1gram): nan% (n=0) time: 4.754
2026-01-03 20:22:15,398: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:22:15,398: t15.2023.08.13 val PER: 0.2339
2026-01-03 20:22:15,398: t15.2023.08.18 val PER: 0.2045
2026-01-03 20:22:15,398: t15.2023.08.20 val PER: 0.2010
2026-01-03 20:22:15,398: t15.2023.08.25 val PER: 0.1551
2026-01-03 20:22:15,398: t15.2023.08.27 val PER: 0.2781
2026-01-03 20:22:15,398: t15.2023.09.01 val PER: 0.1607
2026-01-03 20:22:15,398: t15.2023.09.03 val PER: 0.2435
2026-01-03 20:22:15,399: t15.2023.09.24 val PER: 0.2039
2026-01-03 20:22:15,399: t15.2023.09.29 val PER: 0.2061
2026-01-03 20:22:15,399: t15.2023.10.01 val PER: 0.2596
2026-01-03 20:22:15,399: t15.2023.10.06 val PER: 0.1625
2026-01-03 20:22:15,399: t15.2023.10.08 val PER: 0.3153
2026-01-03 20:22:15,399: t15.2023.10.13 val PER: 0.3057
2026-01-03 20:22:15,399: t15.2023.10.15 val PER: 0.2287
2026-01-03 20:22:15,399: t15.2023.10.20 val PER: 0.2450
2026-01-03 20:22:15,399: t15.2023.10.22 val PER: 0.1938
2026-01-03 20:22:15,399: t15.2023.11.03 val PER: 0.2476
2026-01-03 20:22:15,399: t15.2023.11.04 val PER: 0.0546
2026-01-03 20:22:15,400: t15.2023.11.17 val PER: 0.1011
2026-01-03 20:22:15,400: t15.2023.11.19 val PER: 0.0998
2026-01-03 20:22:15,400: t15.2023.11.26 val PER: 0.2580
2026-01-03 20:22:15,400: t15.2023.12.03 val PER: 0.2237
2026-01-03 20:22:15,400: t15.2023.12.08 val PER: 0.2224
2026-01-03 20:22:15,400: t15.2023.12.10 val PER: 0.1748
2026-01-03 20:22:15,400: t15.2023.12.17 val PER: 0.2412
2026-01-03 20:22:15,400: t15.2023.12.29 val PER: 0.2526
2026-01-03 20:22:15,400: t15.2024.02.25 val PER: 0.2107
2026-01-03 20:22:15,401: t15.2024.03.08 val PER: 0.3357
2026-01-03 20:22:15,401: t15.2024.03.15 val PER: 0.3077
2026-01-03 20:22:15,401: t15.2024.03.17 val PER: 0.2559
2026-01-03 20:22:15,401: t15.2024.05.10 val PER: 0.2660
2026-01-03 20:22:15,401: t15.2024.06.14 val PER: 0.2792
2026-01-03 20:22:15,401: t15.2024.07.19 val PER: 0.3645
2026-01-03 20:22:15,401: t15.2024.07.21 val PER: 0.1855
2026-01-03 20:22:15,401: t15.2024.07.28 val PER: 0.2404
2026-01-03 20:22:15,401: t15.2025.01.10 val PER: 0.4160
2026-01-03 20:22:15,401: t15.2025.01.12 val PER: 0.2741
2026-01-03 20:22:15,401: t15.2025.03.14 val PER: 0.4231
2026-01-03 20:22:15,402: t15.2025.03.16 val PER: 0.3102
2026-01-03 20:22:15,402: t15.2025.03.30 val PER: 0.4207
2026-01-03 20:22:15,402: t15.2025.04.13 val PER: 0.3153
2026-01-03 20:22:15,402: New best val PER 0.2668 --> 0.2490
2026-01-03 20:22:15,402: Checkpointing model
2026-01-03 20:22:15,969: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:22:16,205: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_4000
2026-01-03 20:22:33,725: Train batch 4200: loss: 22.58 grad norm: 62.85 time: 0.080
2026-01-03 20:22:51,247: Train batch 4400: loss: 16.89 grad norm: 52.58 time: 0.065
2026-01-03 20:23:00,302: Running test after training batch: 4500
2026-01-03 20:23:05,080: Val batch 4500: PER (avg): 0.2373 CTC Loss (avg): 23.2020 WER(1gram): nan% (n=0) time: 4.778
2026-01-03 20:23:05,080: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:23:05,081: t15.2023.08.13 val PER: 0.2079
2026-01-03 20:23:05,081: t15.2023.08.18 val PER: 0.1819
2026-01-03 20:23:05,081: t15.2023.08.20 val PER: 0.1867
2026-01-03 20:23:05,081: t15.2023.08.25 val PER: 0.1340
2026-01-03 20:23:05,081: t15.2023.08.27 val PER: 0.2556
2026-01-03 20:23:05,081: t15.2023.09.01 val PER: 0.1550
2026-01-03 20:23:05,081: t15.2023.09.03 val PER: 0.2363
2026-01-03 20:23:05,081: t15.2023.09.24 val PER: 0.1820
2026-01-03 20:23:05,081: t15.2023.09.29 val PER: 0.2029
2026-01-03 20:23:05,082: t15.2023.10.01 val PER: 0.2596
2026-01-03 20:23:05,082: t15.2023.10.06 val PER: 0.1507
2026-01-03 20:23:05,082: t15.2023.10.08 val PER: 0.3126
2026-01-03 20:23:05,082: t15.2023.10.13 val PER: 0.3002
2026-01-03 20:23:05,082: t15.2023.10.15 val PER: 0.2221
2026-01-03 20:23:05,082: t15.2023.10.20 val PER: 0.2081
2026-01-03 20:23:05,082: t15.2023.10.22 val PER: 0.1849
2026-01-03 20:23:05,082: t15.2023.11.03 val PER: 0.2354
2026-01-03 20:23:05,082: t15.2023.11.04 val PER: 0.0648
2026-01-03 20:23:05,082: t15.2023.11.17 val PER: 0.0980
2026-01-03 20:23:05,082: t15.2023.11.19 val PER: 0.0918
2026-01-03 20:23:05,082: t15.2023.11.26 val PER: 0.2674
2026-01-03 20:23:05,082: t15.2023.12.03 val PER: 0.2048
2026-01-03 20:23:05,083: t15.2023.12.08 val PER: 0.2124
2026-01-03 20:23:05,083: t15.2023.12.10 val PER: 0.1853
2026-01-03 20:23:05,083: t15.2023.12.17 val PER: 0.2308
2026-01-03 20:23:05,083: t15.2023.12.29 val PER: 0.2443
2026-01-03 20:23:05,083: t15.2024.02.25 val PER: 0.1980
2026-01-03 20:23:05,083: t15.2024.03.08 val PER: 0.3158
2026-01-03 20:23:05,083: t15.2024.03.15 val PER: 0.2827
2026-01-03 20:23:05,083: t15.2024.03.17 val PER: 0.2378
2026-01-03 20:23:05,083: t15.2024.05.10 val PER: 0.2541
2026-01-03 20:23:05,083: t15.2024.06.14 val PER: 0.2429
2026-01-03 20:23:05,083: t15.2024.07.19 val PER: 0.3434
2026-01-03 20:23:05,084: t15.2024.07.21 val PER: 0.1786
2026-01-03 20:23:05,084: t15.2024.07.28 val PER: 0.2206
2026-01-03 20:23:05,084: t15.2025.01.10 val PER: 0.4215
2026-01-03 20:23:05,084: t15.2025.01.12 val PER: 0.2656
2026-01-03 20:23:05,084: t15.2025.03.14 val PER: 0.4053
2026-01-03 20:23:05,084: t15.2025.03.16 val PER: 0.2853
2026-01-03 20:23:05,084: t15.2025.03.30 val PER: 0.4126
2026-01-03 20:23:05,084: t15.2025.04.13 val PER: 0.2967
2026-01-03 20:23:05,084: New best val PER 0.2490 --> 0.2373
2026-01-03 20:23:05,085: Checkpointing model
2026-01-03 20:23:05,695: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:23:05,936: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_4500
2026-01-03 20:23:14,648: Train batch 4600: loss: 20.31 grad norm: 64.37 time: 0.062
2026-01-03 20:23:32,474: Train batch 4800: loss: 13.84 grad norm: 52.09 time: 0.063
2026-01-03 20:23:49,715: Train batch 5000: loss: 32.18 grad norm: 85.31 time: 0.064
2026-01-03 20:23:49,716: Running test after training batch: 5000
2026-01-03 20:23:55,298: Val batch 5000: PER (avg): 0.2254 CTC Loss (avg): 21.9918 WER(1gram): nan% (n=0) time: 5.582
2026-01-03 20:23:55,298: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:23:55,298: t15.2023.08.13 val PER: 0.2006
2026-01-03 20:23:55,298: t15.2023.08.18 val PER: 0.1710
2026-01-03 20:23:55,299: t15.2023.08.20 val PER: 0.1755
2026-01-03 20:23:55,299: t15.2023.08.25 val PER: 0.1235
2026-01-03 20:23:55,299: t15.2023.08.27 val PER: 0.2347
2026-01-03 20:23:55,299: t15.2023.09.01 val PER: 0.1396
2026-01-03 20:23:55,299: t15.2023.09.03 val PER: 0.2268
2026-01-03 20:23:55,299: t15.2023.09.24 val PER: 0.1833
2026-01-03 20:23:55,299: t15.2023.09.29 val PER: 0.1819
2026-01-03 20:23:55,299: t15.2023.10.01 val PER: 0.2365
2026-01-03 20:23:55,299: t15.2023.10.06 val PER: 0.1378
2026-01-03 20:23:55,299: t15.2023.10.08 val PER: 0.3099
2026-01-03 20:23:55,299: t15.2023.10.13 val PER: 0.2847
2026-01-03 20:23:55,300: t15.2023.10.15 val PER: 0.2235
2026-01-03 20:23:55,300: t15.2023.10.20 val PER: 0.2315
2026-01-03 20:23:55,300: t15.2023.10.22 val PER: 0.1637
2026-01-03 20:23:55,300: t15.2023.11.03 val PER: 0.2232
2026-01-03 20:23:55,300: t15.2023.11.04 val PER: 0.0512
2026-01-03 20:23:55,300: t15.2023.11.17 val PER: 0.0840
2026-01-03 20:23:55,300: t15.2023.11.19 val PER: 0.0778
2026-01-03 20:23:55,300: t15.2023.11.26 val PER: 0.2355
2026-01-03 20:23:55,300: t15.2023.12.03 val PER: 0.1922
2026-01-03 20:23:55,300: t15.2023.12.08 val PER: 0.2011
2026-01-03 20:23:55,300: t15.2023.12.10 val PER: 0.1656
2026-01-03 20:23:55,300: t15.2023.12.17 val PER: 0.2235
2026-01-03 20:23:55,301: t15.2023.12.29 val PER: 0.2286
2026-01-03 20:23:55,301: t15.2024.02.25 val PER: 0.1784
2026-01-03 20:23:55,301: t15.2024.03.08 val PER: 0.3201
2026-01-03 20:23:55,301: t15.2024.03.15 val PER: 0.2764
2026-01-03 20:23:55,301: t15.2024.03.17 val PER: 0.2399
2026-01-03 20:23:55,301: t15.2024.05.10 val PER: 0.2392
2026-01-03 20:23:55,301: t15.2024.06.14 val PER: 0.2461
2026-01-03 20:23:55,301: t15.2024.07.19 val PER: 0.3289
2026-01-03 20:23:55,301: t15.2024.07.21 val PER: 0.1821
2026-01-03 20:23:55,301: t15.2024.07.28 val PER: 0.2125
2026-01-03 20:23:55,301: t15.2025.01.10 val PER: 0.3857
2026-01-03 20:23:55,302: t15.2025.01.12 val PER: 0.2433
2026-01-03 20:23:55,302: t15.2025.03.14 val PER: 0.3743
2026-01-03 20:23:55,302: t15.2025.03.16 val PER: 0.2657
2026-01-03 20:23:55,302: t15.2025.03.30 val PER: 0.4069
2026-01-03 20:23:55,302: t15.2025.04.13 val PER: 0.3024
2026-01-03 20:23:55,302: New best val PER 0.2373 --> 0.2254
2026-01-03 20:23:55,302: Checkpointing model
2026-01-03 20:23:55,881: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:23:56,118: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_5000
2026-01-03 20:24:13,498: Train batch 5200: loss: 16.59 grad norm: 58.97 time: 0.051
2026-01-03 20:24:30,772: Train batch 5400: loss: 17.56 grad norm: 59.68 time: 0.067
2026-01-03 20:24:39,343: Running test after training batch: 5500
2026-01-03 20:24:44,128: Val batch 5500: PER (avg): 0.2163 CTC Loss (avg): 21.2352 WER(1gram): nan% (n=0) time: 4.785
2026-01-03 20:24:44,128: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:24:44,128: t15.2023.08.13 val PER: 0.1850
2026-01-03 20:24:44,128: t15.2023.08.18 val PER: 0.1635
2026-01-03 20:24:44,129: t15.2023.08.20 val PER: 0.1700
2026-01-03 20:24:44,129: t15.2023.08.25 val PER: 0.1205
2026-01-03 20:24:44,129: t15.2023.08.27 val PER: 0.2363
2026-01-03 20:24:44,129: t15.2023.09.01 val PER: 0.1299
2026-01-03 20:24:44,129: t15.2023.09.03 val PER: 0.2304
2026-01-03 20:24:44,129: t15.2023.09.24 val PER: 0.1699
2026-01-03 20:24:44,129: t15.2023.09.29 val PER: 0.1787
2026-01-03 20:24:44,129: t15.2023.10.01 val PER: 0.2299
2026-01-03 20:24:44,129: t15.2023.10.06 val PER: 0.1356
2026-01-03 20:24:44,130: t15.2023.10.08 val PER: 0.3099
2026-01-03 20:24:44,130: t15.2023.10.13 val PER: 0.2824
2026-01-03 20:24:44,130: t15.2023.10.15 val PER: 0.2063
2026-01-03 20:24:44,130: t15.2023.10.20 val PER: 0.2181
2026-01-03 20:24:44,130: t15.2023.10.22 val PER: 0.1537
2026-01-03 20:24:44,130: t15.2023.11.03 val PER: 0.2327
2026-01-03 20:24:44,130: t15.2023.11.04 val PER: 0.0683
2026-01-03 20:24:44,130: t15.2023.11.17 val PER: 0.0855
2026-01-03 20:24:44,130: t15.2023.11.19 val PER: 0.0679
2026-01-03 20:24:44,130: t15.2023.11.26 val PER: 0.2225
2026-01-03 20:24:44,130: t15.2023.12.03 val PER: 0.1870
2026-01-03 20:24:44,130: t15.2023.12.08 val PER: 0.1917
2026-01-03 20:24:44,131: t15.2023.12.10 val PER: 0.1524
2026-01-03 20:24:44,131: t15.2023.12.17 val PER: 0.2204
2026-01-03 20:24:44,131: t15.2023.12.29 val PER: 0.2107
2026-01-03 20:24:44,131: t15.2024.02.25 val PER: 0.1629
2026-01-03 20:24:44,131: t15.2024.03.08 val PER: 0.2916
2026-01-03 20:24:44,131: t15.2024.03.15 val PER: 0.2614
2026-01-03 20:24:44,131: t15.2024.03.17 val PER: 0.2162
2026-01-03 20:24:44,131: t15.2024.05.10 val PER: 0.2244
2026-01-03 20:24:44,131: t15.2024.06.14 val PER: 0.2382
2026-01-03 20:24:44,131: t15.2024.07.19 val PER: 0.3217
2026-01-03 20:24:44,131: t15.2024.07.21 val PER: 0.1572
2026-01-03 20:24:44,131: t15.2024.07.28 val PER: 0.2169
2026-01-03 20:24:44,132: t15.2025.01.10 val PER: 0.3967
2026-01-03 20:24:44,132: t15.2025.01.12 val PER: 0.2302
2026-01-03 20:24:44,132: t15.2025.03.14 val PER: 0.3624
2026-01-03 20:24:44,132: t15.2025.03.16 val PER: 0.2644
2026-01-03 20:24:44,132: t15.2025.03.30 val PER: 0.3655
2026-01-03 20:24:44,132: t15.2025.04.13 val PER: 0.2967
2026-01-03 20:24:44,132: New best val PER 0.2254 --> 0.2163
2026-01-03 20:24:44,132: Checkpointing model
2026-01-03 20:24:44,736: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:24:44,974: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_5500
2026-01-03 20:24:53,603: Train batch 5600: loss: 19.86 grad norm: 68.06 time: 0.061
2026-01-03 20:25:11,591: Train batch 5800: loss: 14.06 grad norm: 62.24 time: 0.082
2026-01-03 20:25:28,930: Train batch 6000: loss: 14.05 grad norm: 53.41 time: 0.049
2026-01-03 20:25:28,930: Running test after training batch: 6000
2026-01-03 20:25:33,732: Val batch 6000: PER (avg): 0.2128 CTC Loss (avg): 20.9871 WER(1gram): nan% (n=0) time: 4.802
2026-01-03 20:25:33,733: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:25:33,733: t15.2023.08.13 val PER: 0.1746
2026-01-03 20:25:33,733: t15.2023.08.18 val PER: 0.1676
2026-01-03 20:25:33,733: t15.2023.08.20 val PER: 0.1763
2026-01-03 20:25:33,733: t15.2023.08.25 val PER: 0.1205
2026-01-03 20:25:33,733: t15.2023.08.27 val PER: 0.2540
2026-01-03 20:25:33,733: t15.2023.09.01 val PER: 0.1307
2026-01-03 20:25:33,733: t15.2023.09.03 val PER: 0.2067
2026-01-03 20:25:33,734: t15.2023.09.24 val PER: 0.1650
2026-01-03 20:25:33,734: t15.2023.09.29 val PER: 0.1780
2026-01-03 20:25:33,734: t15.2023.10.01 val PER: 0.2259
2026-01-03 20:25:33,734: t15.2023.10.06 val PER: 0.1335
2026-01-03 20:25:33,734: t15.2023.10.08 val PER: 0.2909
2026-01-03 20:25:33,734: t15.2023.10.13 val PER: 0.2661
2026-01-03 20:25:33,734: t15.2023.10.15 val PER: 0.2215
2026-01-03 20:25:33,734: t15.2023.10.20 val PER: 0.2081
2026-01-03 20:25:33,734: t15.2023.10.22 val PER: 0.1726
2026-01-03 20:25:33,734: t15.2023.11.03 val PER: 0.2225
2026-01-03 20:25:33,734: t15.2023.11.04 val PER: 0.0717
2026-01-03 20:25:33,734: t15.2023.11.17 val PER: 0.0778
2026-01-03 20:25:33,734: t15.2023.11.19 val PER: 0.0858
2026-01-03 20:25:33,735: t15.2023.11.26 val PER: 0.2196
2026-01-03 20:25:33,735: t15.2023.12.03 val PER: 0.1660
2026-01-03 20:25:33,735: t15.2023.12.08 val PER: 0.1764
2026-01-03 20:25:33,738: t15.2023.12.10 val PER: 0.1537
2026-01-03 20:25:33,738: t15.2023.12.17 val PER: 0.2079
2026-01-03 20:25:33,738: t15.2023.12.29 val PER: 0.2258
2026-01-03 20:25:33,738: t15.2024.02.25 val PER: 0.1601
2026-01-03 20:25:33,738: t15.2024.03.08 val PER: 0.2802
2026-01-03 20:25:33,738: t15.2024.03.15 val PER: 0.2733
2026-01-03 20:25:33,738: t15.2024.03.17 val PER: 0.2106
2026-01-03 20:25:33,738: t15.2024.05.10 val PER: 0.2110
2026-01-03 20:25:33,738: t15.2024.06.14 val PER: 0.2287
2026-01-03 20:25:33,738: t15.2024.07.19 val PER: 0.3098
2026-01-03 20:25:33,739: t15.2024.07.21 val PER: 0.1600
2026-01-03 20:25:33,739: t15.2024.07.28 val PER: 0.2029
2026-01-03 20:25:33,739: t15.2025.01.10 val PER: 0.3650
2026-01-03 20:25:33,739: t15.2025.01.12 val PER: 0.2286
2026-01-03 20:25:33,739: t15.2025.03.14 val PER: 0.3713
2026-01-03 20:25:33,739: t15.2025.03.16 val PER: 0.2605
2026-01-03 20:25:33,739: t15.2025.03.30 val PER: 0.3701
2026-01-03 20:25:33,739: t15.2025.04.13 val PER: 0.2739
2026-01-03 20:25:33,739: New best val PER 0.2163 --> 0.2128
2026-01-03 20:25:33,740: Checkpointing model
2026-01-03 20:25:34,330: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:25:34,571: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_6000
2026-01-03 20:25:51,805: Train batch 6200: loss: 16.70 grad norm: 63.20 time: 0.070
2026-01-03 20:26:09,131: Train batch 6400: loss: 18.74 grad norm: 63.94 time: 0.062
2026-01-03 20:26:17,658: Running test after training batch: 6500
2026-01-03 20:26:22,457: Val batch 6500: PER (avg): 0.2056 CTC Loss (avg): 20.1660 WER(1gram): nan% (n=0) time: 4.799
2026-01-03 20:26:22,458: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:26:22,458: t15.2023.08.13 val PER: 0.1788
2026-01-03 20:26:22,458: t15.2023.08.18 val PER: 0.1526
2026-01-03 20:26:22,458: t15.2023.08.20 val PER: 0.1581
2026-01-03 20:26:22,458: t15.2023.08.25 val PER: 0.1084
2026-01-03 20:26:22,458: t15.2023.08.27 val PER: 0.2251
2026-01-03 20:26:22,458: t15.2023.09.01 val PER: 0.1161
2026-01-03 20:26:22,458: t15.2023.09.03 val PER: 0.2138
2026-01-03 20:26:22,459: t15.2023.09.24 val PER: 0.1723
2026-01-03 20:26:22,459: t15.2023.09.29 val PER: 0.1634
2026-01-03 20:26:22,459: t15.2023.10.01 val PER: 0.2114
2026-01-03 20:26:22,459: t15.2023.10.06 val PER: 0.1292
2026-01-03 20:26:22,459: t15.2023.10.08 val PER: 0.3031
2026-01-03 20:26:22,459: t15.2023.10.13 val PER: 0.2700
2026-01-03 20:26:22,459: t15.2023.10.15 val PER: 0.2136
2026-01-03 20:26:22,459: t15.2023.10.20 val PER: 0.2047
2026-01-03 20:26:22,460: t15.2023.10.22 val PER: 0.1659
2026-01-03 20:26:22,460: t15.2023.11.03 val PER: 0.2178
2026-01-03 20:26:22,460: t15.2023.11.04 val PER: 0.0580
2026-01-03 20:26:22,460: t15.2023.11.17 val PER: 0.0669
2026-01-03 20:26:22,460: t15.2023.11.19 val PER: 0.0758
2026-01-03 20:26:22,460: t15.2023.11.26 val PER: 0.2181
2026-01-03 20:26:22,460: t15.2023.12.03 val PER: 0.1828
2026-01-03 20:26:22,460: t15.2023.12.08 val PER: 0.1664
2026-01-03 20:26:22,460: t15.2023.12.10 val PER: 0.1406
2026-01-03 20:26:22,460: t15.2023.12.17 val PER: 0.1944
2026-01-03 20:26:22,460: t15.2023.12.29 val PER: 0.2038
2026-01-03 20:26:22,461: t15.2024.02.25 val PER: 0.1615
2026-01-03 20:26:22,461: t15.2024.03.08 val PER: 0.2873
2026-01-03 20:26:22,461: t15.2024.03.15 val PER: 0.2589
2026-01-03 20:26:22,461: t15.2024.03.17 val PER: 0.2057
2026-01-03 20:26:22,461: t15.2024.05.10 val PER: 0.2259
2026-01-03 20:26:22,461: t15.2024.06.14 val PER: 0.2177
2026-01-03 20:26:22,461: t15.2024.07.19 val PER: 0.3039
2026-01-03 20:26:22,461: t15.2024.07.21 val PER: 0.1572
2026-01-03 20:26:22,461: t15.2024.07.28 val PER: 0.1963
2026-01-03 20:26:22,461: t15.2025.01.10 val PER: 0.3774
2026-01-03 20:26:22,462: t15.2025.01.12 val PER: 0.2094
2026-01-03 20:26:22,462: t15.2025.03.14 val PER: 0.3654
2026-01-03 20:26:22,462: t15.2025.03.16 val PER: 0.2435
2026-01-03 20:26:22,462: t15.2025.03.30 val PER: 0.3529
2026-01-03 20:26:22,462: t15.2025.04.13 val PER: 0.2668
2026-01-03 20:26:22,462: New best val PER 0.2128 --> 0.2056
2026-01-03 20:26:22,462: Checkpointing model
2026-01-03 20:26:23,064: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:26:23,300: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_6500
2026-01-03 20:26:31,730: Train batch 6600: loss: 12.74 grad norm: 53.89 time: 0.045
2026-01-03 20:26:49,329: Train batch 6800: loss: 15.37 grad norm: 55.21 time: 0.048
2026-01-03 20:27:07,301: Train batch 7000: loss: 17.47 grad norm: 65.23 time: 0.061
2026-01-03 20:27:07,302: Running test after training batch: 7000
2026-01-03 20:27:12,108: Val batch 7000: PER (avg): 0.1960 CTC Loss (avg): 19.4278 WER(1gram): nan% (n=0) time: 4.806
2026-01-03 20:27:12,109: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:27:12,109: t15.2023.08.13 val PER: 0.1632
2026-01-03 20:27:12,109: t15.2023.08.18 val PER: 0.1425
2026-01-03 20:27:12,109: t15.2023.08.20 val PER: 0.1573
2026-01-03 20:27:12,109: t15.2023.08.25 val PER: 0.1084
2026-01-03 20:27:12,109: t15.2023.08.27 val PER: 0.2186
2026-01-03 20:27:12,109: t15.2023.09.01 val PER: 0.1088
2026-01-03 20:27:12,109: t15.2023.09.03 val PER: 0.1960
2026-01-03 20:27:12,110: t15.2023.09.24 val PER: 0.1602
2026-01-03 20:27:12,110: t15.2023.09.29 val PER: 0.1691
2026-01-03 20:27:12,110: t15.2023.10.01 val PER: 0.2094
2026-01-03 20:27:12,110: t15.2023.10.06 val PER: 0.1141
2026-01-03 20:27:12,110: t15.2023.10.08 val PER: 0.2801
2026-01-03 20:27:12,110: t15.2023.10.13 val PER: 0.2552
2026-01-03 20:27:12,110: t15.2023.10.15 val PER: 0.1938
2026-01-03 20:27:12,110: t15.2023.10.20 val PER: 0.2349
2026-01-03 20:27:12,110: t15.2023.10.22 val PER: 0.1470
2026-01-03 20:27:12,110: t15.2023.11.03 val PER: 0.2022
2026-01-03 20:27:12,110: t15.2023.11.04 val PER: 0.0512
2026-01-03 20:27:12,110: t15.2023.11.17 val PER: 0.0715
2026-01-03 20:27:12,110: t15.2023.11.19 val PER: 0.0579
2026-01-03 20:27:12,111: t15.2023.11.26 val PER: 0.1964
2026-01-03 20:27:12,111: t15.2023.12.03 val PER: 0.1555
2026-01-03 20:27:12,111: t15.2023.12.08 val PER: 0.1578
2026-01-03 20:27:12,111: t15.2023.12.10 val PER: 0.1459
2026-01-03 20:27:12,111: t15.2023.12.17 val PER: 0.1850
2026-01-03 20:27:12,111: t15.2023.12.29 val PER: 0.2073
2026-01-03 20:27:12,111: t15.2024.02.25 val PER: 0.1517
2026-01-03 20:27:12,111: t15.2024.03.08 val PER: 0.2717
2026-01-03 20:27:12,111: t15.2024.03.15 val PER: 0.2464
2026-01-03 20:27:12,111: t15.2024.03.17 val PER: 0.2008
2026-01-03 20:27:12,111: t15.2024.05.10 val PER: 0.2051
2026-01-03 20:27:12,111: t15.2024.06.14 val PER: 0.2050
2026-01-03 20:27:12,111: t15.2024.07.19 val PER: 0.2980
2026-01-03 20:27:12,112: t15.2024.07.21 val PER: 0.1366
2026-01-03 20:27:12,112: t15.2024.07.28 val PER: 0.1765
2026-01-03 20:27:12,112: t15.2025.01.10 val PER: 0.3623
2026-01-03 20:27:12,112: t15.2025.01.12 val PER: 0.2048
2026-01-03 20:27:12,112: t15.2025.03.14 val PER: 0.3506
2026-01-03 20:27:12,112: t15.2025.03.16 val PER: 0.2435
2026-01-03 20:27:12,112: t15.2025.03.30 val PER: 0.3598
2026-01-03 20:27:12,112: t15.2025.04.13 val PER: 0.2725
2026-01-03 20:27:12,112: New best val PER 0.2056 --> 0.1960
2026-01-03 20:27:12,112: Checkpointing model
2026-01-03 20:27:12,692: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:27:12,929: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_7000
2026-01-03 20:27:30,884: Train batch 7200: loss: 13.83 grad norm: 54.37 time: 0.079
2026-01-03 20:27:47,997: Train batch 7400: loss: 13.53 grad norm: 52.30 time: 0.075
2026-01-03 20:27:56,482: Running test after training batch: 7500
2026-01-03 20:28:01,183: Val batch 7500: PER (avg): 0.1905 CTC Loss (avg): 18.8383 WER(1gram): nan% (n=0) time: 4.700
2026-01-03 20:28:01,183: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:28:01,183: t15.2023.08.13 val PER: 0.1455
2026-01-03 20:28:01,183: t15.2023.08.18 val PER: 0.1450
2026-01-03 20:28:01,183: t15.2023.08.20 val PER: 0.1485
2026-01-03 20:28:01,184: t15.2023.08.25 val PER: 0.1009
2026-01-03 20:28:01,184: t15.2023.08.27 val PER: 0.2122
2026-01-03 20:28:01,184: t15.2023.09.01 val PER: 0.1153
2026-01-03 20:28:01,184: t15.2023.09.03 val PER: 0.1829
2026-01-03 20:28:01,184: t15.2023.09.24 val PER: 0.1553
2026-01-03 20:28:01,184: t15.2023.09.29 val PER: 0.1595
2026-01-03 20:28:01,184: t15.2023.10.01 val PER: 0.2008
2026-01-03 20:28:01,184: t15.2023.10.06 val PER: 0.1173
2026-01-03 20:28:01,184: t15.2023.10.08 val PER: 0.2639
2026-01-03 20:28:01,185: t15.2023.10.13 val PER: 0.2552
2026-01-03 20:28:01,185: t15.2023.10.15 val PER: 0.2004
2026-01-03 20:28:01,185: t15.2023.10.20 val PER: 0.2047
2026-01-03 20:28:01,185: t15.2023.10.22 val PER: 0.1359
2026-01-03 20:28:01,185: t15.2023.11.03 val PER: 0.2049
2026-01-03 20:28:01,185: t15.2023.11.04 val PER: 0.0444
2026-01-03 20:28:01,185: t15.2023.11.17 val PER: 0.0684
2026-01-03 20:28:01,185: t15.2023.11.19 val PER: 0.0579
2026-01-03 20:28:01,185: t15.2023.11.26 val PER: 0.1841
2026-01-03 20:28:01,186: t15.2023.12.03 val PER: 0.1649
2026-01-03 20:28:01,186: t15.2023.12.08 val PER: 0.1538
2026-01-03 20:28:01,186: t15.2023.12.10 val PER: 0.1353
2026-01-03 20:28:01,186: t15.2023.12.17 val PER: 0.1778
2026-01-03 20:28:01,186: t15.2023.12.29 val PER: 0.1887
2026-01-03 20:28:01,186: t15.2024.02.25 val PER: 0.1461
2026-01-03 20:28:01,186: t15.2024.03.08 val PER: 0.2703
2026-01-03 20:28:01,186: t15.2024.03.15 val PER: 0.2445
2026-01-03 20:28:01,186: t15.2024.03.17 val PER: 0.1799
2026-01-03 20:28:01,187: t15.2024.05.10 val PER: 0.2051
2026-01-03 20:28:01,187: t15.2024.06.14 val PER: 0.1940
2026-01-03 20:28:01,187: t15.2024.07.19 val PER: 0.2999
2026-01-03 20:28:01,187: t15.2024.07.21 val PER: 0.1352
2026-01-03 20:28:01,187: t15.2024.07.28 val PER: 0.1765
2026-01-03 20:28:01,187: t15.2025.01.10 val PER: 0.3567
2026-01-03 20:28:01,187: t15.2025.01.12 val PER: 0.1917
2026-01-03 20:28:01,187: t15.2025.03.14 val PER: 0.3565
2026-01-03 20:28:01,187: t15.2025.03.16 val PER: 0.2395
2026-01-03 20:28:01,187: t15.2025.03.30 val PER: 0.3529
2026-01-03 20:28:01,188: t15.2025.04.13 val PER: 0.2482
2026-01-03 20:28:01,188: New best val PER 0.1960 --> 0.1905
2026-01-03 20:28:01,188: Checkpointing model
2026-01-03 20:28:01,791: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 20:28:02,027: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_7500
2026-01-03 20:28:10,654: Train batch 7600: loss: 16.42 grad norm: 59.40 time: 0.069
2026-01-03 20:28:27,764: Train batch 7800: loss: 14.37 grad norm: 58.07 time: 0.055
2026-01-03 20:28:45,452: Train batch 8000: loss: 11.63 grad norm: 53.05 time: 0.071
2026-01-03 20:28:45,452: Running test after training batch: 8000
2026-01-03 20:28:50,258: Val batch 8000: PER (avg): 0.1855 CTC Loss (avg): 18.1428 WER(1gram): nan% (n=0) time: 4.805
2026-01-03 20:28:50,258: WER lens: avg_true_words=nan avg_pred_words=nan max_pred_words=0
2026-01-03 20:28:50,258: t15.2023.08.13 val PER: 0.1435
2026-01-03 20:28:50,258: t15.2023.08.18 val PER: 0.1316
2026-01-03 20:28:50,258: t15.2023.08.20 val PER: 0.1422
2026-01-03 20:28:50,259: t15.2023.08.25 val PER: 0.1130
2026-01-03 20:28:50,259: t15.2023.08.27 val PER: 0.2154
2026-01-03 20:28:50,259: t15.2023.09.01 val PER: 0.1006
2026-01-03 20:28:50,259: t15.2023.09.03 val PER: 0.1876
2026-01-03 20:28:50,259: t15.2023.09.24 val PER: 0.1553
2026-01-03 20:28:50,259: t15.2023.09.29 val PER: 0.1532
2026-01-03 20:28:50,260: t15.2023.10.01 val PER: 0.2021
2026-01-03 20:28:50,260: t15.2023.10.06 val PER: 0.1141
2026-01-03 20:28:50,260: t15.2023.10.08 val PER: 0.2788
2026-01-03 20:28:50,260: t15.2023.10.13 val PER: 0.2436
2026-01-03 20:28:50,260: t15.2023.10.15 val PER: 0.1852
2026-01-03 20:28:50,260: t15.2023.10.20 val PER: 0.2081
2026-01-03 20:28:50,260: t15.2023.10.22 val PER: 0.1347
2026-01-03 20:28:50,260: t15.2023.11.03 val PER: 0.2042
2026-01-03 20:28:50,260: t15.2023.11.04 val PER: 0.0410
2026-01-03 20:28:50,260: t15.2023.11.17 val PER: 0.0653
2026-01-03 20:28:50,260: t15.2023.11.19 val PER: 0.0559
2026-01-03 20:28:50,261: t15.2023.11.26 val PER: 0.1819
2026-01-03 20:28:50,261: t15.2023.12.03 val PER: 0.1639
2026-01-03 20:28:50,261: t15.2023.12.08 val PER: 0.1511
2026-01-03 20:28:50,261: t15.2023.12.10 val PER: 0.1327
2026-01-03 20:28:50,261: t15.2023.12.17 val PER: 0.1819
2026-01-03 20:28:50,261: t15.2023.12.29 val PER: 0.1771
2026-01-03 20:28:50,261: t15.2024.02.25 val PER: 0.1433
2026-01-03 20:28:50,261: t15.2024.03.08 val PER: 0.2546
2026-01-03 20:28:50,261: t15.2024.03.15 val PER: 0.2339
2026-01-03 20:28:50,261: t15.2024.03.17 val PER: 0.1702
2026-01-03 20:28:50,261: t15.2024.05.10 val PER: 0.1932
2026-01-03 20:28:50,262: t15.2024.06.14 val PER: 0.2035
2026-01-03 20:28:50,262: t15.2024.07.19 val PER: 0.2828
2026-01-03 20:28:50,262: t15.2024.07.21 val PER: 0.1207
2026-01-03 20:28:50,262: t15.2024.07.28 val PER: 0.1654
2026-01-03 20:28:50,262: t15.2025.01.10 val PER: 0.3375
2026-01-03 20:28:50,262: t15.2025.01.12 val PER: 0.1932
2026-01-03 20:28:50,262: t15.2025.03.14 val PER: 0.3713
2026-01-03 20:28:50,262: t15.2025.03.16 val PER: 0.2317
2026-01-03 20:28:50,262: t15.2025.03.30 val PER: 0.3529
2026-01-03 20:28:50,262: t15.2025.04.13 val PER: 0.2553
2026-01-03 20:28:50,263: New best val PER 0.1905 --> 0.1855
2026-01-03 20:28:50,263: Checkpointing model
[1;34mwandb[0m: 
[1;34mwandb[0m:  View run [33mbase[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../tmp/e12511253_b2t_348227/wandb/wandb/run-20260103_201532-ew9ytayz/logs[0m
