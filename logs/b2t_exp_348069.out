CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  3 11:30 /tmp/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
B2T_DATA_DIR=/home/e12511253/Brain2Text/brain2text/data/hdf5_data_final
==============================================
Job: b2t_exp  ID: 348069
Base: configs/rnn_args.yaml
Global override: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/gru/lr_stepdrop/lr40
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
TMPDIR=/tmp
WANDB_DIR=/tmp/wandb
==============================================

========== FOLDER: configs/experiments/gru/lr_stepdrop/lr40 ==========
Num configs: 6

=== RUN base.yaml ===
2026-01-03 11:30:46,176: Using device: cuda:0
2026-01-03 11:30:47,797: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-03 11:30:59,206: Using 45 sessions after filtering (from 45).
2026-01-03 11:30:59,639: Using torch.compile (if available)
2026-01-03 11:30:59,641: torch.compile not available (torch<2.0). Skipping.
2026-01-03 11:30:59,643: Initialized RNN decoding model
2026-01-03 11:30:59,645: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 11:30:59,647: Model has 44,907,305 parameters
2026-01-03 11:30:59,648: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 11:31:05,774: Successfully initialized datasets
2026-01-03 11:31:05,776: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 11:31:07,891: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.364
2026-01-03 11:31:07,893: Running test after training batch: 0
2026-01-03 11:31:08,006: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:31:13,186: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-03 11:31:13,896: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-03 11:31:47,398: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 39.500
2026-01-03 11:31:47,400: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-03 11:31:47,403: t15.2023.08.13 val PER: 1.3056
2026-01-03 11:31:47,405: t15.2023.08.18 val PER: 1.4208
2026-01-03 11:31:47,406: t15.2023.08.20 val PER: 1.3002
2026-01-03 11:31:47,408: t15.2023.08.25 val PER: 1.3389
2026-01-03 11:31:47,410: t15.2023.08.27 val PER: 1.2460
2026-01-03 11:31:47,411: t15.2023.09.01 val PER: 1.4537
2026-01-03 11:31:47,413: t15.2023.09.03 val PER: 1.3171
2026-01-03 11:31:47,415: t15.2023.09.24 val PER: 1.5461
2026-01-03 11:31:47,416: t15.2023.09.29 val PER: 1.4671
2026-01-03 11:31:47,418: t15.2023.10.01 val PER: 1.2147
2026-01-03 11:31:47,419: t15.2023.10.06 val PER: 1.4876
2026-01-03 11:31:47,421: t15.2023.10.08 val PER: 1.1827
2026-01-03 11:31:47,423: t15.2023.10.13 val PER: 1.3964
2026-01-03 11:31:47,424: t15.2023.10.15 val PER: 1.3889
2026-01-03 11:31:47,426: t15.2023.10.20 val PER: 1.4866
2026-01-03 11:31:47,428: t15.2023.10.22 val PER: 1.3942
2026-01-03 11:31:47,429: t15.2023.11.03 val PER: 1.5923
2026-01-03 11:31:47,431: t15.2023.11.04 val PER: 2.0171
2026-01-03 11:31:47,433: t15.2023.11.17 val PER: 1.9518
2026-01-03 11:31:47,434: t15.2023.11.19 val PER: 1.6707
2026-01-03 11:31:47,436: t15.2023.11.26 val PER: 1.5413
2026-01-03 11:31:47,439: t15.2023.12.03 val PER: 1.4254
2026-01-03 11:31:47,441: t15.2023.12.08 val PER: 1.4487
2026-01-03 11:31:47,443: t15.2023.12.10 val PER: 1.6899
2026-01-03 11:31:47,444: t15.2023.12.17 val PER: 1.3077
2026-01-03 11:31:47,447: t15.2023.12.29 val PER: 1.4063
2026-01-03 11:31:47,448: t15.2024.02.25 val PER: 1.4228
2026-01-03 11:31:47,451: t15.2024.03.08 val PER: 1.3257
2026-01-03 11:31:47,453: t15.2024.03.15 val PER: 1.3196
2026-01-03 11:31:47,455: t15.2024.03.17 val PER: 1.4052
2026-01-03 11:31:47,457: t15.2024.05.10 val PER: 1.3224
2026-01-03 11:31:47,459: t15.2024.06.14 val PER: 1.5315
2026-01-03 11:31:47,461: t15.2024.07.19 val PER: 1.0817
2026-01-03 11:31:47,462: t15.2024.07.21 val PER: 1.6290
2026-01-03 11:31:47,464: t15.2024.07.28 val PER: 1.6588
2026-01-03 11:31:47,466: t15.2025.01.10 val PER: 1.0923
2026-01-03 11:31:47,467: t15.2025.01.12 val PER: 1.7629
2026-01-03 11:31:47,469: t15.2025.03.14 val PER: 1.0414
2026-01-03 11:31:47,471: t15.2025.03.16 val PER: 1.6257
2026-01-03 11:31:47,472: t15.2025.03.30 val PER: 1.2874
2026-01-03 11:31:47,474: t15.2025.04.13 val PER: 1.5949
2026-01-03 11:31:47,476: New best val WER(1gram) inf% --> 100.00%
2026-01-03 11:31:47,478: Checkpointing model
2026-01-03 11:31:47,721: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/best_checkpoint
2026-01-03 11:31:47,974: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_0
2026-01-03 11:32:03,889: Train batch 200: loss: 77.59 grad norm: 105.97 time: 0.053
2026-01-03 11:32:19,523: Train batch 400: loss: 53.33 grad norm: 85.10 time: 0.063
2026-01-03 11:32:27,592: Running test after training batch: 500
2026-01-03 11:32:27,704: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:32:32,473: WER debug example
  GT : you can see the code at this point as well
  PR : used and ease thus uhde at this ide is aisle
2026-01-03 11:32:32,508: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as adz
2026-01-03 11:32:34,812: Val batch 500: PER (avg): 0.5150 CTC Loss (avg): 55.8788 WER(1gram): 88.32% (n=64) time: 7.218
2026-01-03 11:32:34,814: WER lens: avg_true_words=6.16 avg_pred_words=5.56 max_pred_words=11
2026-01-03 11:32:34,816: t15.2023.08.13 val PER: 0.4647
2026-01-03 11:32:34,818: t15.2023.08.18 val PER: 0.4501
2026-01-03 11:32:34,820: t15.2023.08.20 val PER: 0.4488
2026-01-03 11:32:34,821: t15.2023.08.25 val PER: 0.4292
2026-01-03 11:32:34,823: t15.2023.08.27 val PER: 0.5209
2026-01-03 11:32:34,824: t15.2023.09.01 val PER: 0.4123
2026-01-03 11:32:34,826: t15.2023.09.03 val PER: 0.4822
2026-01-03 11:32:34,827: t15.2023.09.24 val PER: 0.4333
2026-01-03 11:32:34,829: t15.2023.09.29 val PER: 0.4646
2026-01-03 11:32:34,831: t15.2023.10.01 val PER: 0.5192
2026-01-03 11:32:34,832: t15.2023.10.06 val PER: 0.4198
2026-01-03 11:32:34,834: t15.2023.10.08 val PER: 0.5359
2026-01-03 11:32:34,835: t15.2023.10.13 val PER: 0.5718
2026-01-03 11:32:34,837: t15.2023.10.15 val PER: 0.4924
2026-01-03 11:32:34,838: t15.2023.10.20 val PER: 0.4698
2026-01-03 11:32:34,840: t15.2023.10.22 val PER: 0.4610
2026-01-03 11:32:34,841: t15.2023.11.03 val PER: 0.5020
2026-01-03 11:32:34,843: t15.2023.11.04 val PER: 0.2526
2026-01-03 11:32:34,844: t15.2023.11.17 val PER: 0.3639
2026-01-03 11:32:34,846: t15.2023.11.19 val PER: 0.3313
2026-01-03 11:32:34,847: t15.2023.11.26 val PER: 0.5500
2026-01-03 11:32:34,849: t15.2023.12.03 val PER: 0.5000
2026-01-03 11:32:34,851: t15.2023.12.08 val PER: 0.5180
2026-01-03 11:32:34,852: t15.2023.12.10 val PER: 0.4428
2026-01-03 11:32:34,854: t15.2023.12.17 val PER: 0.5520
2026-01-03 11:32:34,855: t15.2023.12.29 val PER: 0.5388
2026-01-03 11:32:34,857: t15.2024.02.25 val PER: 0.4705
2026-01-03 11:32:34,858: t15.2024.03.08 val PER: 0.6230
2026-01-03 11:32:34,860: t15.2024.03.15 val PER: 0.5485
2026-01-03 11:32:34,861: t15.2024.03.17 val PER: 0.4951
2026-01-03 11:32:34,863: t15.2024.05.10 val PER: 0.5483
2026-01-03 11:32:34,864: t15.2024.06.14 val PER: 0.4984
2026-01-03 11:32:34,866: t15.2024.07.19 val PER: 0.6684
2026-01-03 11:32:34,867: t15.2024.07.21 val PER: 0.4710
2026-01-03 11:32:34,869: t15.2024.07.28 val PER: 0.5015
2026-01-03 11:32:34,870: t15.2025.01.10 val PER: 0.7410
2026-01-03 11:32:34,871: t15.2025.01.12 val PER: 0.5543
2026-01-03 11:32:34,873: t15.2025.03.14 val PER: 0.7382
2026-01-03 11:32:34,874: t15.2025.03.16 val PER: 0.5929
2026-01-03 11:32:34,876: t15.2025.03.30 val PER: 0.7276
2026-01-03 11:32:34,877: t15.2025.04.13 val PER: 0.5735
2026-01-03 11:32:34,880: New best val WER(1gram) 100.00% --> 88.32%
2026-01-03 11:32:34,882: Checkpointing model
2026-01-03 11:32:35,147: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/best_checkpoint
2026-01-03 11:32:35,401: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_500
2026-01-03 11:32:43,548: Train batch 600: loss: 49.12 grad norm: 77.47 time: 0.077
2026-01-03 11:32:59,876: Train batch 800: loss: 41.30 grad norm: 86.38 time: 0.057
2026-01-03 11:33:16,230: Train batch 1000: loss: 42.89 grad norm: 77.56 time: 0.065
2026-01-03 11:33:16,233: Running test after training batch: 1000
2026-01-03 11:33:16,357: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:33:21,153: WER debug example
  GT : you can see the code at this point as well
  PR : used ent ease thus code it this royd is while
2026-01-03 11:33:21,188: WER debug example
  GT : how does it keep the cost down
  PR : houde is it eke thus wass it
2026-01-03 11:33:23,025: Val batch 1000: PER (avg): 0.4061 CTC Loss (avg): 42.4928 WER(1gram): 81.22% (n=64) time: 6.790
2026-01-03 11:33:23,027: WER lens: avg_true_words=6.16 avg_pred_words=5.62 max_pred_words=12
2026-01-03 11:33:23,030: t15.2023.08.13 val PER: 0.3794
2026-01-03 11:33:23,032: t15.2023.08.18 val PER: 0.3269
2026-01-03 11:33:23,034: t15.2023.08.20 val PER: 0.3455
2026-01-03 11:33:23,035: t15.2023.08.25 val PER: 0.2967
2026-01-03 11:33:23,037: t15.2023.08.27 val PER: 0.4293
2026-01-03 11:33:23,039: t15.2023.09.01 val PER: 0.2987
2026-01-03 11:33:23,040: t15.2023.09.03 val PER: 0.3895
2026-01-03 11:33:23,043: t15.2023.09.24 val PER: 0.3325
2026-01-03 11:33:23,045: t15.2023.09.29 val PER: 0.3548
2026-01-03 11:33:23,047: t15.2023.10.01 val PER: 0.3983
2026-01-03 11:33:23,048: t15.2023.10.06 val PER: 0.3100
2026-01-03 11:33:23,050: t15.2023.10.08 val PER: 0.4560
2026-01-03 11:33:23,052: t15.2023.10.13 val PER: 0.4740
2026-01-03 11:33:23,054: t15.2023.10.15 val PER: 0.3777
2026-01-03 11:33:23,056: t15.2023.10.20 val PER: 0.3691
2026-01-03 11:33:23,058: t15.2023.10.22 val PER: 0.3563
2026-01-03 11:33:23,060: t15.2023.11.03 val PER: 0.3982
2026-01-03 11:33:23,062: t15.2023.11.04 val PER: 0.1741
2026-01-03 11:33:23,066: t15.2023.11.17 val PER: 0.2582
2026-01-03 11:33:23,068: t15.2023.11.19 val PER: 0.2096
2026-01-03 11:33:23,069: t15.2023.11.26 val PER: 0.4442
2026-01-03 11:33:23,071: t15.2023.12.03 val PER: 0.3960
2026-01-03 11:33:23,073: t15.2023.12.08 val PER: 0.4001
2026-01-03 11:33:23,075: t15.2023.12.10 val PER: 0.3430
2026-01-03 11:33:23,076: t15.2023.12.17 val PER: 0.4137
2026-01-03 11:33:23,078: t15.2023.12.29 val PER: 0.4056
2026-01-03 11:33:23,080: t15.2024.02.25 val PER: 0.3666
2026-01-03 11:33:23,083: t15.2024.03.08 val PER: 0.5007
2026-01-03 11:33:23,085: t15.2024.03.15 val PER: 0.4390
2026-01-03 11:33:23,087: t15.2024.03.17 val PER: 0.4066
2026-01-03 11:33:23,088: t15.2024.05.10 val PER: 0.4131
2026-01-03 11:33:23,090: t15.2024.06.14 val PER: 0.3912
2026-01-03 11:33:23,092: t15.2024.07.19 val PER: 0.5267
2026-01-03 11:33:23,093: t15.2024.07.21 val PER: 0.3731
2026-01-03 11:33:23,095: t15.2024.07.28 val PER: 0.4103
2026-01-03 11:33:23,097: t15.2025.01.10 val PER: 0.6185
2026-01-03 11:33:23,099: t15.2025.01.12 val PER: 0.4403
2026-01-03 11:33:23,100: t15.2025.03.14 val PER: 0.6302
2026-01-03 11:33:23,103: t15.2025.03.16 val PER: 0.4895
2026-01-03 11:33:23,105: t15.2025.03.30 val PER: 0.6345
2026-01-03 11:33:23,107: t15.2025.04.13 val PER: 0.4864
2026-01-03 11:33:23,109: New best val WER(1gram) 88.32% --> 81.22%
2026-01-03 11:33:23,110: Checkpointing model
2026-01-03 11:33:23,414: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/best_checkpoint
2026-01-03 11:33:23,670: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_1000
2026-01-03 11:33:39,628: Train batch 1200: loss: 32.92 grad norm: 73.68 time: 0.067
2026-01-03 11:33:55,749: Train batch 1400: loss: 36.29 grad norm: 80.22 time: 0.059
2026-01-03 11:34:03,773: Running test after training batch: 1500
2026-01-03 11:34:03,910: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:34:08,722: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt e the good at this boyde is will
2026-01-03 11:34:08,753: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that us
2026-01-03 11:34:10,264: Val batch 1500: PER (avg): 0.3793 CTC Loss (avg): 37.0572 WER(1gram): 75.89% (n=64) time: 6.488
2026-01-03 11:34:10,267: WER lens: avg_true_words=6.16 avg_pred_words=4.92 max_pred_words=11
2026-01-03 11:34:10,269: t15.2023.08.13 val PER: 0.3451
2026-01-03 11:34:10,272: t15.2023.08.18 val PER: 0.3219
2026-01-03 11:34:10,274: t15.2023.08.20 val PER: 0.3026
2026-01-03 11:34:10,275: t15.2023.08.25 val PER: 0.2560
2026-01-03 11:34:10,277: t15.2023.08.27 val PER: 0.4148
2026-01-03 11:34:10,278: t15.2023.09.01 val PER: 0.2727
2026-01-03 11:34:10,280: t15.2023.09.03 val PER: 0.3765
2026-01-03 11:34:10,282: t15.2023.09.24 val PER: 0.3083
2026-01-03 11:34:10,284: t15.2023.09.29 val PER: 0.3325
2026-01-03 11:34:10,286: t15.2023.10.01 val PER: 0.3910
2026-01-03 11:34:10,287: t15.2023.10.06 val PER: 0.2896
2026-01-03 11:34:10,289: t15.2023.10.08 val PER: 0.4330
2026-01-03 11:34:10,291: t15.2023.10.13 val PER: 0.4375
2026-01-03 11:34:10,292: t15.2023.10.15 val PER: 0.3639
2026-01-03 11:34:10,294: t15.2023.10.20 val PER: 0.3389
2026-01-03 11:34:10,296: t15.2023.10.22 val PER: 0.3096
2026-01-03 11:34:10,298: t15.2023.11.03 val PER: 0.3636
2026-01-03 11:34:10,300: t15.2023.11.04 val PER: 0.1092
2026-01-03 11:34:10,302: t15.2023.11.17 val PER: 0.2271
2026-01-03 11:34:10,304: t15.2023.11.19 val PER: 0.1796
2026-01-03 11:34:10,305: t15.2023.11.26 val PER: 0.4145
2026-01-03 11:34:10,307: t15.2023.12.03 val PER: 0.3718
2026-01-03 11:34:10,309: t15.2023.12.08 val PER: 0.3569
2026-01-03 11:34:10,311: t15.2023.12.10 val PER: 0.2983
2026-01-03 11:34:10,313: t15.2023.12.17 val PER: 0.3815
2026-01-03 11:34:10,315: t15.2023.12.29 val PER: 0.3686
2026-01-03 11:34:10,316: t15.2024.02.25 val PER: 0.3174
2026-01-03 11:34:10,318: t15.2024.03.08 val PER: 0.4609
2026-01-03 11:34:10,320: t15.2024.03.15 val PER: 0.4134
2026-01-03 11:34:10,321: t15.2024.03.17 val PER: 0.3842
2026-01-03 11:34:10,323: t15.2024.05.10 val PER: 0.3774
2026-01-03 11:34:10,325: t15.2024.06.14 val PER: 0.3927
2026-01-03 11:34:10,326: t15.2024.07.19 val PER: 0.5234
2026-01-03 11:34:10,328: t15.2024.07.21 val PER: 0.3407
2026-01-03 11:34:10,329: t15.2024.07.28 val PER: 0.3640
2026-01-03 11:34:10,331: t15.2025.01.10 val PER: 0.6006
2026-01-03 11:34:10,333: t15.2025.01.12 val PER: 0.4288
2026-01-03 11:34:10,335: t15.2025.03.14 val PER: 0.6006
2026-01-03 11:34:10,338: t15.2025.03.16 val PER: 0.4581
2026-01-03 11:34:10,340: t15.2025.03.30 val PER: 0.6276
2026-01-03 11:34:10,342: t15.2025.04.13 val PER: 0.4693
2026-01-03 11:34:10,344: New best val WER(1gram) 81.22% --> 75.89%
2026-01-03 11:34:10,345: Checkpointing model
2026-01-03 11:34:10,611: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/best_checkpoint
2026-01-03 11:34:10,865: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_1500
2026-01-03 11:34:18,747: Train batch 1600: loss: 37.31 grad norm: 84.49 time: 0.063
2026-01-03 11:34:34,643: Train batch 1800: loss: 35.09 grad norm: 68.77 time: 0.087
2026-01-03 11:34:50,727: Train batch 2000: loss: 33.89 grad norm: 71.39 time: 0.066
2026-01-03 11:34:50,730: Running test after training batch: 2000
2026-01-03 11:34:50,867: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:34:55,995: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned ease the code at this boyte is wheel
2026-01-03 11:34:56,029: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the cost it
2026-01-03 11:34:57,639: Val batch 2000: PER (avg): 0.3276 CTC Loss (avg): 32.7765 WER(1gram): 69.29% (n=64) time: 6.906
2026-01-03 11:34:57,642: WER lens: avg_true_words=6.16 avg_pred_words=5.64 max_pred_words=11
2026-01-03 11:34:57,644: t15.2023.08.13 val PER: 0.2994
2026-01-03 11:34:57,646: t15.2023.08.18 val PER: 0.2666
2026-01-03 11:34:57,648: t15.2023.08.20 val PER: 0.2534
2026-01-03 11:34:57,649: t15.2023.08.25 val PER: 0.2304
2026-01-03 11:34:57,651: t15.2023.08.27 val PER: 0.3473
2026-01-03 11:34:57,653: t15.2023.09.01 val PER: 0.2362
2026-01-03 11:34:57,655: t15.2023.09.03 val PER: 0.3207
2026-01-03 11:34:57,657: t15.2023.09.24 val PER: 0.2536
2026-01-03 11:34:57,658: t15.2023.09.29 val PER: 0.2770
2026-01-03 11:34:57,660: t15.2023.10.01 val PER: 0.3283
2026-01-03 11:34:57,661: t15.2023.10.06 val PER: 0.2293
2026-01-03 11:34:57,663: t15.2023.10.08 val PER: 0.3897
2026-01-03 11:34:57,665: t15.2023.10.13 val PER: 0.3708
2026-01-03 11:34:57,666: t15.2023.10.15 val PER: 0.3092
2026-01-03 11:34:57,668: t15.2023.10.20 val PER: 0.2953
2026-01-03 11:34:57,669: t15.2023.10.22 val PER: 0.2728
2026-01-03 11:34:57,671: t15.2023.11.03 val PER: 0.3189
2026-01-03 11:34:57,672: t15.2023.11.04 val PER: 0.1024
2026-01-03 11:34:57,675: t15.2023.11.17 val PER: 0.1633
2026-01-03 11:34:57,677: t15.2023.11.19 val PER: 0.1437
2026-01-03 11:34:57,678: t15.2023.11.26 val PER: 0.3688
2026-01-03 11:34:57,681: t15.2023.12.03 val PER: 0.3193
2026-01-03 11:34:57,683: t15.2023.12.08 val PER: 0.3142
2026-01-03 11:34:57,685: t15.2023.12.10 val PER: 0.2668
2026-01-03 11:34:57,687: t15.2023.12.17 val PER: 0.3222
2026-01-03 11:34:57,689: t15.2023.12.29 val PER: 0.3281
2026-01-03 11:34:57,691: t15.2024.02.25 val PER: 0.2809
2026-01-03 11:34:57,692: t15.2024.03.08 val PER: 0.4054
2026-01-03 11:34:57,694: t15.2024.03.15 val PER: 0.3565
2026-01-03 11:34:57,698: t15.2024.03.17 val PER: 0.3410
2026-01-03 11:34:57,700: t15.2024.05.10 val PER: 0.3328
2026-01-03 11:34:57,702: t15.2024.06.14 val PER: 0.3249
2026-01-03 11:34:57,704: t15.2024.07.19 val PER: 0.4687
2026-01-03 11:34:57,705: t15.2024.07.21 val PER: 0.2890
2026-01-03 11:34:57,707: t15.2024.07.28 val PER: 0.3125
2026-01-03 11:34:57,709: t15.2025.01.10 val PER: 0.5455
2026-01-03 11:34:57,710: t15.2025.01.12 val PER: 0.3772
2026-01-03 11:34:57,712: t15.2025.03.14 val PER: 0.5237
2026-01-03 11:34:57,713: t15.2025.03.16 val PER: 0.3796
2026-01-03 11:34:57,714: t15.2025.03.30 val PER: 0.5506
2026-01-03 11:34:57,716: t15.2025.04.13 val PER: 0.4180
2026-01-03 11:34:57,717: New best val WER(1gram) 75.89% --> 69.29%
2026-01-03 11:34:57,719: Checkpointing model
2026-01-03 11:34:57,989: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/best_checkpoint
2026-01-03 11:34:58,243: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_2000
2026-01-03 11:35:14,071: Train batch 2200: loss: 28.81 grad norm: 76.05 time: 0.060
2026-01-03 11:35:30,378: Train batch 2400: loss: 29.25 grad norm: 63.13 time: 0.052
2026-01-03 11:35:38,639: Running test after training batch: 2500
2026-01-03 11:35:38,764: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:35:43,492: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-03 11:35:43,521: WER debug example
  GT : how does it keep the cost down
  PR : houde des it keep the wass it
2026-01-03 11:35:45,152: Val batch 2500: PER (avg): 0.3055 CTC Loss (avg): 30.2650 WER(1gram): 69.54% (n=64) time: 6.511
2026-01-03 11:35:45,153: WER lens: avg_true_words=6.16 avg_pred_words=5.58 max_pred_words=11
2026-01-03 11:35:45,153: t15.2023.08.13 val PER: 0.2890
2026-01-03 11:35:45,153: t15.2023.08.18 val PER: 0.2473
2026-01-03 11:35:45,153: t15.2023.08.20 val PER: 0.2359
2026-01-03 11:35:45,153: t15.2023.08.25 val PER: 0.2063
2026-01-03 11:35:45,153: t15.2023.08.27 val PER: 0.3215
2026-01-03 11:35:45,153: t15.2023.09.01 val PER: 0.2135
2026-01-03 11:35:45,153: t15.2023.09.03 val PER: 0.2981
2026-01-03 11:35:45,153: t15.2023.09.24 val PER: 0.2306
2026-01-03 11:35:45,153: t15.2023.09.29 val PER: 0.2597
2026-01-03 11:35:45,153: t15.2023.10.01 val PER: 0.3091
2026-01-03 11:35:45,153: t15.2023.10.06 val PER: 0.2260
2026-01-03 11:35:45,153: t15.2023.10.08 val PER: 0.3884
2026-01-03 11:35:45,154: t15.2023.10.13 val PER: 0.3607
2026-01-03 11:35:45,154: t15.2023.10.15 val PER: 0.2933
2026-01-03 11:35:45,154: t15.2023.10.20 val PER: 0.2819
2026-01-03 11:35:45,154: t15.2023.10.22 val PER: 0.2383
2026-01-03 11:35:45,154: t15.2023.11.03 val PER: 0.3033
2026-01-03 11:35:45,154: t15.2023.11.04 val PER: 0.0887
2026-01-03 11:35:45,154: t15.2023.11.17 val PER: 0.1353
2026-01-03 11:35:45,154: t15.2023.11.19 val PER: 0.1257
2026-01-03 11:35:45,154: t15.2023.11.26 val PER: 0.3399
2026-01-03 11:35:45,154: t15.2023.12.03 val PER: 0.2826
2026-01-03 11:35:45,154: t15.2023.12.08 val PER: 0.2836
2026-01-03 11:35:45,154: t15.2023.12.10 val PER: 0.2326
2026-01-03 11:35:45,154: t15.2023.12.17 val PER: 0.2911
2026-01-03 11:35:45,154: t15.2023.12.29 val PER: 0.3013
2026-01-03 11:35:45,154: t15.2024.02.25 val PER: 0.2514
2026-01-03 11:35:45,155: t15.2024.03.08 val PER: 0.3656
2026-01-03 11:35:45,155: t15.2024.03.15 val PER: 0.3508
2026-01-03 11:35:45,155: t15.2024.03.17 val PER: 0.3166
2026-01-03 11:35:45,155: t15.2024.05.10 val PER: 0.3031
2026-01-03 11:35:45,155: t15.2024.06.14 val PER: 0.2934
2026-01-03 11:35:45,155: t15.2024.07.19 val PER: 0.4397
2026-01-03 11:35:45,155: t15.2024.07.21 val PER: 0.2600
2026-01-03 11:35:45,155: t15.2024.07.28 val PER: 0.3000
2026-01-03 11:35:45,155: t15.2025.01.10 val PER: 0.5110
2026-01-03 11:35:45,155: t15.2025.01.12 val PER: 0.3557
2026-01-03 11:35:45,155: t15.2025.03.14 val PER: 0.5059
2026-01-03 11:35:45,155: t15.2025.03.16 val PER: 0.3652
2026-01-03 11:35:45,155: t15.2025.03.30 val PER: 0.5172
2026-01-03 11:35:45,155: t15.2025.04.13 val PER: 0.3923
2026-01-03 11:35:45,397: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_2500
2026-01-03 11:35:53,387: Train batch 2600: loss: 34.70 grad norm: 85.25 time: 0.055
2026-01-03 11:36:09,719: Train batch 2800: loss: 25.79 grad norm: 72.53 time: 0.081
2026-01-03 11:36:25,952: Train batch 3000: loss: 30.61 grad norm: 71.98 time: 0.083
2026-01-03 11:36:25,952: Running test after training batch: 3000
2026-01-03 11:36:26,049: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:36:30,788: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point is will
2026-01-03 11:36:30,818: WER debug example
  GT : how does it keep the cost down
  PR : houde does it hipp the cost get
2026-01-03 11:36:32,508: Val batch 3000: PER (avg): 0.2821 CTC Loss (avg): 27.9365 WER(1gram): 64.72% (n=64) time: 6.555
2026-01-03 11:36:32,508: WER lens: avg_true_words=6.16 avg_pred_words=5.91 max_pred_words=11
2026-01-03 11:36:32,508: t15.2023.08.13 val PER: 0.2578
2026-01-03 11:36:32,508: t15.2023.08.18 val PER: 0.2255
2026-01-03 11:36:32,508: t15.2023.08.20 val PER: 0.2192
2026-01-03 11:36:32,509: t15.2023.08.25 val PER: 0.2033
2026-01-03 11:36:32,509: t15.2023.08.27 val PER: 0.2942
2026-01-03 11:36:32,509: t15.2023.09.01 val PER: 0.1875
2026-01-03 11:36:32,509: t15.2023.09.03 val PER: 0.2803
2026-01-03 11:36:32,509: t15.2023.09.24 val PER: 0.2257
2026-01-03 11:36:32,509: t15.2023.09.29 val PER: 0.2348
2026-01-03 11:36:32,509: t15.2023.10.01 val PER: 0.2860
2026-01-03 11:36:32,509: t15.2023.10.06 val PER: 0.1959
2026-01-03 11:36:32,509: t15.2023.10.08 val PER: 0.3518
2026-01-03 11:36:32,509: t15.2023.10.13 val PER: 0.3336
2026-01-03 11:36:32,509: t15.2023.10.15 val PER: 0.2643
2026-01-03 11:36:32,509: t15.2023.10.20 val PER: 0.2584
2026-01-03 11:36:32,509: t15.2023.10.22 val PER: 0.2138
2026-01-03 11:36:32,510: t15.2023.11.03 val PER: 0.2849
2026-01-03 11:36:32,510: t15.2023.11.04 val PER: 0.0853
2026-01-03 11:36:32,510: t15.2023.11.17 val PER: 0.1166
2026-01-03 11:36:32,510: t15.2023.11.19 val PER: 0.1218
2026-01-03 11:36:32,510: t15.2023.11.26 val PER: 0.3051
2026-01-03 11:36:32,510: t15.2023.12.03 val PER: 0.2731
2026-01-03 11:36:32,510: t15.2023.12.08 val PER: 0.2603
2026-01-03 11:36:32,510: t15.2023.12.10 val PER: 0.2102
2026-01-03 11:36:32,510: t15.2023.12.17 val PER: 0.2775
2026-01-03 11:36:32,510: t15.2023.12.29 val PER: 0.2793
2026-01-03 11:36:32,510: t15.2024.02.25 val PER: 0.2458
2026-01-03 11:36:32,510: t15.2024.03.08 val PER: 0.3542
2026-01-03 11:36:32,510: t15.2024.03.15 val PER: 0.3258
2026-01-03 11:36:32,510: t15.2024.03.17 val PER: 0.2957
2026-01-03 11:36:32,510: t15.2024.05.10 val PER: 0.3016
2026-01-03 11:36:32,511: t15.2024.06.14 val PER: 0.2950
2026-01-03 11:36:32,511: t15.2024.07.19 val PER: 0.4001
2026-01-03 11:36:32,511: t15.2024.07.21 val PER: 0.2414
2026-01-03 11:36:32,511: t15.2024.07.28 val PER: 0.2772
2026-01-03 11:36:32,511: t15.2025.01.10 val PER: 0.4793
2026-01-03 11:36:32,511: t15.2025.01.12 val PER: 0.3272
2026-01-03 11:36:32,511: t15.2025.03.14 val PER: 0.4541
2026-01-03 11:36:32,511: t15.2025.03.16 val PER: 0.3194
2026-01-03 11:36:32,511: t15.2025.03.30 val PER: 0.4747
2026-01-03 11:36:32,511: t15.2025.04.13 val PER: 0.3623
2026-01-03 11:36:32,512: New best val WER(1gram) 69.29% --> 64.72%
2026-01-03 11:36:32,512: Checkpointing model
2026-01-03 11:36:32,778: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/best_checkpoint
2026-01-03 11:36:33,032: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_3000
2026-01-03 11:36:49,475: Train batch 3200: loss: 26.67 grad norm: 68.11 time: 0.075
2026-01-03 11:37:05,802: Train batch 3400: loss: 18.70 grad norm: 56.58 time: 0.049
2026-01-03 11:37:14,204: Running test after training batch: 3500
2026-01-03 11:37:14,328: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:37:19,043: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point will
2026-01-03 11:37:19,071: WER debug example
  GT : how does it keep the cost down
  PR : houde des it epp thus cussed get
2026-01-03 11:37:20,654: Val batch 3500: PER (avg): 0.2670 CTC Loss (avg): 26.6037 WER(1gram): 66.75% (n=64) time: 6.450
2026-01-03 11:37:20,655: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-03 11:37:20,655: t15.2023.08.13 val PER: 0.2443
2026-01-03 11:37:20,655: t15.2023.08.18 val PER: 0.2020
2026-01-03 11:37:20,655: t15.2023.08.20 val PER: 0.2232
2026-01-03 11:37:20,655: t15.2023.08.25 val PER: 0.1792
2026-01-03 11:37:20,655: t15.2023.08.27 val PER: 0.2556
2026-01-03 11:37:20,655: t15.2023.09.01 val PER: 0.1769
2026-01-03 11:37:20,655: t15.2023.09.03 val PER: 0.2518
2026-01-03 11:37:20,656: t15.2023.09.24 val PER: 0.2160
2026-01-03 11:37:20,656: t15.2023.09.29 val PER: 0.2208
2026-01-03 11:37:20,656: t15.2023.10.01 val PER: 0.2807
2026-01-03 11:37:20,656: t15.2023.10.06 val PER: 0.1916
2026-01-03 11:37:20,656: t15.2023.10.08 val PER: 0.3342
2026-01-03 11:37:20,656: t15.2023.10.13 val PER: 0.3181
2026-01-03 11:37:20,656: t15.2023.10.15 val PER: 0.2432
2026-01-03 11:37:20,656: t15.2023.10.20 val PER: 0.2383
2026-01-03 11:37:20,656: t15.2023.10.22 val PER: 0.2082
2026-01-03 11:37:20,656: t15.2023.11.03 val PER: 0.2578
2026-01-03 11:37:20,656: t15.2023.11.04 val PER: 0.0785
2026-01-03 11:37:20,656: t15.2023.11.17 val PER: 0.1244
2026-01-03 11:37:20,656: t15.2023.11.19 val PER: 0.1038
2026-01-03 11:37:20,656: t15.2023.11.26 val PER: 0.2913
2026-01-03 11:37:20,656: t15.2023.12.03 val PER: 0.2405
2026-01-03 11:37:20,657: t15.2023.12.08 val PER: 0.2423
2026-01-03 11:37:20,657: t15.2023.12.10 val PER: 0.2063
2026-01-03 11:37:20,657: t15.2023.12.17 val PER: 0.2547
2026-01-03 11:37:20,657: t15.2023.12.29 val PER: 0.2601
2026-01-03 11:37:20,657: t15.2024.02.25 val PER: 0.2065
2026-01-03 11:37:20,657: t15.2024.03.08 val PER: 0.3329
2026-01-03 11:37:20,657: t15.2024.03.15 val PER: 0.3208
2026-01-03 11:37:20,657: t15.2024.03.17 val PER: 0.2810
2026-01-03 11:37:20,657: t15.2024.05.10 val PER: 0.2793
2026-01-03 11:37:20,657: t15.2024.06.14 val PER: 0.2729
2026-01-03 11:37:20,657: t15.2024.07.19 val PER: 0.3949
2026-01-03 11:37:20,657: t15.2024.07.21 val PER: 0.2262
2026-01-03 11:37:20,657: t15.2024.07.28 val PER: 0.2824
2026-01-03 11:37:20,657: t15.2025.01.10 val PER: 0.4669
2026-01-03 11:37:20,657: t15.2025.01.12 val PER: 0.2933
2026-01-03 11:37:20,657: t15.2025.03.14 val PER: 0.4379
2026-01-03 11:37:20,657: t15.2025.03.16 val PER: 0.3168
2026-01-03 11:37:20,658: t15.2025.03.30 val PER: 0.4575
2026-01-03 11:37:20,658: t15.2025.04.13 val PER: 0.3409
2026-01-03 11:37:20,901: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_3500
2026-01-03 11:37:29,000: Train batch 3600: loss: 22.77 grad norm: 64.11 time: 0.066
2026-01-03 11:37:45,057: Train batch 3800: loss: 25.73 grad norm: 68.47 time: 0.066
2026-01-03 11:38:01,171: Train batch 4000: loss: 19.34 grad norm: 52.52 time: 0.055
2026-01-03 11:38:01,171: Running test after training batch: 4000
2026-01-03 11:38:01,307: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:38:06,195: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-03 11:38:06,222: WER debug example
  GT : how does it keep the cost down
  PR : aue dust it keep the cost et
2026-01-03 11:38:07,781: Val batch 4000: PER (avg): 0.2481 CTC Loss (avg): 24.4684 WER(1gram): 63.20% (n=64) time: 6.610
2026-01-03 11:38:07,781: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-03 11:38:07,782: t15.2023.08.13 val PER: 0.2297
2026-01-03 11:38:07,782: t15.2023.08.18 val PER: 0.2012
2026-01-03 11:38:07,782: t15.2023.08.20 val PER: 0.1978
2026-01-03 11:38:07,782: t15.2023.08.25 val PER: 0.1551
2026-01-03 11:38:07,782: t15.2023.08.27 val PER: 0.2926
2026-01-03 11:38:07,782: t15.2023.09.01 val PER: 0.1591
2026-01-03 11:38:07,782: t15.2023.09.03 val PER: 0.2435
2026-01-03 11:38:07,782: t15.2023.09.24 val PER: 0.2063
2026-01-03 11:38:07,782: t15.2023.09.29 val PER: 0.2036
2026-01-03 11:38:07,782: t15.2023.10.01 val PER: 0.2543
2026-01-03 11:38:07,782: t15.2023.10.06 val PER: 0.1647
2026-01-03 11:38:07,782: t15.2023.10.08 val PER: 0.3221
2026-01-03 11:38:07,783: t15.2023.10.13 val PER: 0.2956
2026-01-03 11:38:07,783: t15.2023.10.15 val PER: 0.2386
2026-01-03 11:38:07,783: t15.2023.10.20 val PER: 0.2081
2026-01-03 11:38:07,783: t15.2023.10.22 val PER: 0.2004
2026-01-03 11:38:07,783: t15.2023.11.03 val PER: 0.2415
2026-01-03 11:38:07,783: t15.2023.11.04 val PER: 0.0683
2026-01-03 11:38:07,783: t15.2023.11.17 val PER: 0.0995
2026-01-03 11:38:07,783: t15.2023.11.19 val PER: 0.1018
2026-01-03 11:38:07,783: t15.2023.11.26 val PER: 0.2717
2026-01-03 11:38:07,783: t15.2023.12.03 val PER: 0.2122
2026-01-03 11:38:07,783: t15.2023.12.08 val PER: 0.2224
2026-01-03 11:38:07,783: t15.2023.12.10 val PER: 0.1827
2026-01-03 11:38:07,783: t15.2023.12.17 val PER: 0.2557
2026-01-03 11:38:07,783: t15.2023.12.29 val PER: 0.2498
2026-01-03 11:38:07,784: t15.2024.02.25 val PER: 0.2205
2026-01-03 11:38:07,784: t15.2024.03.08 val PER: 0.3172
2026-01-03 11:38:07,784: t15.2024.03.15 val PER: 0.2958
2026-01-03 11:38:07,784: t15.2024.03.17 val PER: 0.2594
2026-01-03 11:38:07,784: t15.2024.05.10 val PER: 0.2511
2026-01-03 11:38:07,784: t15.2024.06.14 val PER: 0.2681
2026-01-03 11:38:07,784: t15.2024.07.19 val PER: 0.3612
2026-01-03 11:38:07,784: t15.2024.07.21 val PER: 0.1841
2026-01-03 11:38:07,784: t15.2024.07.28 val PER: 0.2434
2026-01-03 11:38:07,784: t15.2025.01.10 val PER: 0.4229
2026-01-03 11:38:07,785: t15.2025.01.12 val PER: 0.2787
2026-01-03 11:38:07,785: t15.2025.03.14 val PER: 0.4157
2026-01-03 11:38:07,785: t15.2025.03.16 val PER: 0.3063
2026-01-03 11:38:07,785: t15.2025.03.30 val PER: 0.4057
2026-01-03 11:38:07,785: t15.2025.04.13 val PER: 0.3238
2026-01-03 11:38:07,786: New best val WER(1gram) 64.72% --> 63.20%
2026-01-03 11:38:07,786: Checkpointing model
2026-01-03 11:38:08,051: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/best_checkpoint
2026-01-03 11:38:08,305: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_4000
2026-01-03 11:38:24,569: Train batch 4200: loss: 22.51 grad norm: 62.26 time: 0.079
2026-01-03 11:38:40,865: Train batch 4400: loss: 16.90 grad norm: 53.72 time: 0.066
2026-01-03 11:38:48,970: Running test after training batch: 4500
2026-01-03 11:38:49,078: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:38:54,051: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-03 11:38:54,081: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it yip the cost get
2026-01-03 11:38:55,677: Val batch 4500: PER (avg): 0.2391 CTC Loss (avg): 23.3335 WER(1gram): 58.12% (n=64) time: 6.706
2026-01-03 11:38:55,677: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-03 11:38:55,678: t15.2023.08.13 val PER: 0.2141
2026-01-03 11:38:55,678: t15.2023.08.18 val PER: 0.1827
2026-01-03 11:38:55,678: t15.2023.08.20 val PER: 0.1890
2026-01-03 11:38:55,678: t15.2023.08.25 val PER: 0.1461
2026-01-03 11:38:55,678: t15.2023.08.27 val PER: 0.2508
2026-01-03 11:38:55,678: t15.2023.09.01 val PER: 0.1542
2026-01-03 11:38:55,678: t15.2023.09.03 val PER: 0.2423
2026-01-03 11:38:55,678: t15.2023.09.24 val PER: 0.1760
2026-01-03 11:38:55,678: t15.2023.09.29 val PER: 0.1966
2026-01-03 11:38:55,678: t15.2023.10.01 val PER: 0.2589
2026-01-03 11:38:55,678: t15.2023.10.06 val PER: 0.1496
2026-01-03 11:38:55,678: t15.2023.10.08 val PER: 0.3221
2026-01-03 11:38:55,678: t15.2023.10.13 val PER: 0.2995
2026-01-03 11:38:55,678: t15.2023.10.15 val PER: 0.2301
2026-01-03 11:38:55,679: t15.2023.10.20 val PER: 0.2114
2026-01-03 11:38:55,679: t15.2023.10.22 val PER: 0.1782
2026-01-03 11:38:55,679: t15.2023.11.03 val PER: 0.2476
2026-01-03 11:38:55,679: t15.2023.11.04 val PER: 0.0580
2026-01-03 11:38:55,679: t15.2023.11.17 val PER: 0.0980
2026-01-03 11:38:55,679: t15.2023.11.19 val PER: 0.0918
2026-01-03 11:38:55,679: t15.2023.11.26 val PER: 0.2696
2026-01-03 11:38:55,679: t15.2023.12.03 val PER: 0.2122
2026-01-03 11:38:55,679: t15.2023.12.08 val PER: 0.2157
2026-01-03 11:38:55,679: t15.2023.12.10 val PER: 0.1866
2026-01-03 11:38:55,679: t15.2023.12.17 val PER: 0.2277
2026-01-03 11:38:55,679: t15.2023.12.29 val PER: 0.2443
2026-01-03 11:38:55,679: t15.2024.02.25 val PER: 0.2051
2026-01-03 11:38:55,679: t15.2024.03.08 val PER: 0.3215
2026-01-03 11:38:55,679: t15.2024.03.15 val PER: 0.2902
2026-01-03 11:38:55,680: t15.2024.03.17 val PER: 0.2497
2026-01-03 11:38:55,680: t15.2024.05.10 val PER: 0.2467
2026-01-03 11:38:55,680: t15.2024.06.14 val PER: 0.2461
2026-01-03 11:38:55,680: t15.2024.07.19 val PER: 0.3467
2026-01-03 11:38:55,680: t15.2024.07.21 val PER: 0.1752
2026-01-03 11:38:55,680: t15.2024.07.28 val PER: 0.2265
2026-01-03 11:38:55,680: t15.2025.01.10 val PER: 0.4242
2026-01-03 11:38:55,681: t15.2025.01.12 val PER: 0.2648
2026-01-03 11:38:55,681: t15.2025.03.14 val PER: 0.3994
2026-01-03 11:38:55,681: t15.2025.03.16 val PER: 0.2853
2026-01-03 11:38:55,681: t15.2025.03.30 val PER: 0.4138
2026-01-03 11:38:55,681: t15.2025.04.13 val PER: 0.2839
2026-01-03 11:38:55,682: New best val WER(1gram) 63.20% --> 58.12%
2026-01-03 11:38:55,682: Checkpointing model
2026-01-03 11:38:55,946: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/best_checkpoint
2026-01-03 11:38:56,199: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_4500
2026-01-03 11:39:04,363: Train batch 4600: loss: 20.27 grad norm: 68.08 time: 0.062
2026-01-03 11:39:20,520: Train batch 4800: loss: 13.37 grad norm: 52.43 time: 0.063
2026-01-03 11:39:36,599: Train batch 5000: loss: 31.40 grad norm: 84.18 time: 0.063
2026-01-03 11:39:36,599: Running test after training batch: 5000
2026-01-03 11:39:36,711: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:39:41,420: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-03 11:39:41,449: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the cost nett
2026-01-03 11:39:43,024: Val batch 5000: PER (avg): 0.2257 CTC Loss (avg): 21.9860 WER(1gram): 61.68% (n=64) time: 6.425
2026-01-03 11:39:43,025: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-03 11:39:43,025: t15.2023.08.13 val PER: 0.2037
2026-01-03 11:39:43,025: t15.2023.08.18 val PER: 0.1727
2026-01-03 11:39:43,025: t15.2023.08.20 val PER: 0.1739
2026-01-03 11:39:43,025: t15.2023.08.25 val PER: 0.1370
2026-01-03 11:39:43,025: t15.2023.08.27 val PER: 0.2508
2026-01-03 11:39:43,025: t15.2023.09.01 val PER: 0.1404
2026-01-03 11:39:43,025: t15.2023.09.03 val PER: 0.2257
2026-01-03 11:39:43,025: t15.2023.09.24 val PER: 0.1833
2026-01-03 11:39:43,025: t15.2023.09.29 val PER: 0.1838
2026-01-03 11:39:43,025: t15.2023.10.01 val PER: 0.2398
2026-01-03 11:39:43,025: t15.2023.10.06 val PER: 0.1346
2026-01-03 11:39:43,025: t15.2023.10.08 val PER: 0.3153
2026-01-03 11:39:43,026: t15.2023.10.13 val PER: 0.2839
2026-01-03 11:39:43,026: t15.2023.10.15 val PER: 0.2254
2026-01-03 11:39:43,026: t15.2023.10.20 val PER: 0.2349
2026-01-03 11:39:43,026: t15.2023.10.22 val PER: 0.1626
2026-01-03 11:39:43,026: t15.2023.11.03 val PER: 0.2198
2026-01-03 11:39:43,026: t15.2023.11.04 val PER: 0.0478
2026-01-03 11:39:43,026: t15.2023.11.17 val PER: 0.0809
2026-01-03 11:39:43,026: t15.2023.11.19 val PER: 0.0739
2026-01-03 11:39:43,026: t15.2023.11.26 val PER: 0.2442
2026-01-03 11:39:43,026: t15.2023.12.03 val PER: 0.1985
2026-01-03 11:39:43,026: t15.2023.12.08 val PER: 0.1904
2026-01-03 11:39:43,026: t15.2023.12.10 val PER: 0.1629
2026-01-03 11:39:43,027: t15.2023.12.17 val PER: 0.2235
2026-01-03 11:39:43,027: t15.2023.12.29 val PER: 0.2196
2026-01-03 11:39:43,027: t15.2024.02.25 val PER: 0.1868
2026-01-03 11:39:43,027: t15.2024.03.08 val PER: 0.3073
2026-01-03 11:39:43,027: t15.2024.03.15 val PER: 0.2808
2026-01-03 11:39:43,027: t15.2024.03.17 val PER: 0.2448
2026-01-03 11:39:43,027: t15.2024.05.10 val PER: 0.2422
2026-01-03 11:39:43,027: t15.2024.06.14 val PER: 0.2492
2026-01-03 11:39:43,027: t15.2024.07.19 val PER: 0.3362
2026-01-03 11:39:43,027: t15.2024.07.21 val PER: 0.1779
2026-01-03 11:39:43,027: t15.2024.07.28 val PER: 0.2103
2026-01-03 11:39:43,027: t15.2025.01.10 val PER: 0.3760
2026-01-03 11:39:43,027: t15.2025.01.12 val PER: 0.2371
2026-01-03 11:39:43,027: t15.2025.03.14 val PER: 0.3905
2026-01-03 11:39:43,027: t15.2025.03.16 val PER: 0.2696
2026-01-03 11:39:43,027: t15.2025.03.30 val PER: 0.3966
2026-01-03 11:39:43,027: t15.2025.04.13 val PER: 0.2981
2026-01-03 11:39:43,270: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_5000
2026-01-03 11:39:59,226: Train batch 5200: loss: 16.71 grad norm: 62.23 time: 0.051
2026-01-03 11:40:14,963: Train batch 5400: loss: 17.52 grad norm: 58.39 time: 0.067
2026-01-03 11:40:23,292: Running test after training batch: 5500
2026-01-03 11:40:23,427: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:40:28,134: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 11:40:28,163: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-03 11:40:29,737: Val batch 5500: PER (avg): 0.2171 CTC Loss (avg): 21.1405 WER(1gram): 57.36% (n=64) time: 6.444
2026-01-03 11:40:29,737: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 11:40:29,737: t15.2023.08.13 val PER: 0.1715
2026-01-03 11:40:29,737: t15.2023.08.18 val PER: 0.1559
2026-01-03 11:40:29,737: t15.2023.08.20 val PER: 0.1708
2026-01-03 11:40:29,737: t15.2023.08.25 val PER: 0.1265
2026-01-03 11:40:29,737: t15.2023.08.27 val PER: 0.2460
2026-01-03 11:40:29,737: t15.2023.09.01 val PER: 0.1299
2026-01-03 11:40:29,738: t15.2023.09.03 val PER: 0.2257
2026-01-03 11:40:29,738: t15.2023.09.24 val PER: 0.1711
2026-01-03 11:40:29,738: t15.2023.09.29 val PER: 0.1800
2026-01-03 11:40:29,738: t15.2023.10.01 val PER: 0.2299
2026-01-03 11:40:29,738: t15.2023.10.06 val PER: 0.1302
2026-01-03 11:40:29,738: t15.2023.10.08 val PER: 0.2963
2026-01-03 11:40:29,738: t15.2023.10.13 val PER: 0.2777
2026-01-03 11:40:29,738: t15.2023.10.15 val PER: 0.2129
2026-01-03 11:40:29,738: t15.2023.10.20 val PER: 0.2416
2026-01-03 11:40:29,738: t15.2023.10.22 val PER: 0.1615
2026-01-03 11:40:29,738: t15.2023.11.03 val PER: 0.2178
2026-01-03 11:40:29,738: t15.2023.11.04 val PER: 0.0478
2026-01-03 11:40:29,738: t15.2023.11.17 val PER: 0.0824
2026-01-03 11:40:29,738: t15.2023.11.19 val PER: 0.0739
2026-01-03 11:40:29,739: t15.2023.11.26 val PER: 0.2254
2026-01-03 11:40:29,739: t15.2023.12.03 val PER: 0.1880
2026-01-03 11:40:29,739: t15.2023.12.08 val PER: 0.1844
2026-01-03 11:40:29,739: t15.2023.12.10 val PER: 0.1708
2026-01-03 11:40:29,739: t15.2023.12.17 val PER: 0.2162
2026-01-03 11:40:29,739: t15.2023.12.29 val PER: 0.2121
2026-01-03 11:40:29,739: t15.2024.02.25 val PER: 0.1756
2026-01-03 11:40:29,739: t15.2024.03.08 val PER: 0.3001
2026-01-03 11:40:29,739: t15.2024.03.15 val PER: 0.2652
2026-01-03 11:40:29,739: t15.2024.03.17 val PER: 0.2169
2026-01-03 11:40:29,739: t15.2024.05.10 val PER: 0.2259
2026-01-03 11:40:29,739: t15.2024.06.14 val PER: 0.2413
2026-01-03 11:40:29,739: t15.2024.07.19 val PER: 0.3289
2026-01-03 11:40:29,739: t15.2024.07.21 val PER: 0.1683
2026-01-03 11:40:29,739: t15.2024.07.28 val PER: 0.2110
2026-01-03 11:40:29,739: t15.2025.01.10 val PER: 0.3953
2026-01-03 11:40:29,739: t15.2025.01.12 val PER: 0.2379
2026-01-03 11:40:29,740: t15.2025.03.14 val PER: 0.3683
2026-01-03 11:40:29,740: t15.2025.03.16 val PER: 0.2618
2026-01-03 11:40:29,740: t15.2025.03.30 val PER: 0.3793
2026-01-03 11:40:29,740: t15.2025.04.13 val PER: 0.2896
2026-01-03 11:40:29,741: New best val WER(1gram) 58.12% --> 57.36%
2026-01-03 11:40:29,741: Checkpointing model
2026-01-03 11:40:30,006: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/best_checkpoint
2026-01-03 11:40:30,258: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_5500
2026-01-03 11:40:38,342: Train batch 5600: loss: 19.70 grad norm: 64.64 time: 0.062
2026-01-03 11:40:54,455: Train batch 5800: loss: 13.81 grad norm: 57.16 time: 0.082
2026-01-03 11:41:10,225: Train batch 6000: loss: 14.68 grad norm: 56.52 time: 0.048
2026-01-03 11:41:10,226: Running test after training batch: 6000
2026-01-03 11:41:10,385: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:41:15,101: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the good at this point is will
2026-01-03 11:41:15,132: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-03 11:41:16,752: Val batch 6000: PER (avg): 0.2118 CTC Loss (avg): 20.7644 WER(1gram): 57.36% (n=64) time: 6.526
2026-01-03 11:41:16,752: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 11:41:16,752: t15.2023.08.13 val PER: 0.1840
2026-01-03 11:41:16,752: t15.2023.08.18 val PER: 0.1660
2026-01-03 11:41:16,753: t15.2023.08.20 val PER: 0.1644
2026-01-03 11:41:16,753: t15.2023.08.25 val PER: 0.1175
2026-01-03 11:41:16,753: t15.2023.08.27 val PER: 0.2363
2026-01-03 11:41:16,753: t15.2023.09.01 val PER: 0.1234
2026-01-03 11:41:16,753: t15.2023.09.03 val PER: 0.2102
2026-01-03 11:41:16,753: t15.2023.09.24 val PER: 0.1723
2026-01-03 11:41:16,753: t15.2023.09.29 val PER: 0.1634
2026-01-03 11:41:16,753: t15.2023.10.01 val PER: 0.2272
2026-01-03 11:41:16,753: t15.2023.10.06 val PER: 0.1313
2026-01-03 11:41:16,753: t15.2023.10.08 val PER: 0.2855
2026-01-03 11:41:16,753: t15.2023.10.13 val PER: 0.2754
2026-01-03 11:41:16,753: t15.2023.10.15 val PER: 0.2156
2026-01-03 11:41:16,753: t15.2023.10.20 val PER: 0.1980
2026-01-03 11:41:16,753: t15.2023.10.22 val PER: 0.1648
2026-01-03 11:41:16,753: t15.2023.11.03 val PER: 0.2252
2026-01-03 11:41:16,753: t15.2023.11.04 val PER: 0.0478
2026-01-03 11:41:16,754: t15.2023.11.17 val PER: 0.0793
2026-01-03 11:41:16,754: t15.2023.11.19 val PER: 0.0818
2026-01-03 11:41:16,754: t15.2023.11.26 val PER: 0.2210
2026-01-03 11:41:16,754: t15.2023.12.03 val PER: 0.1702
2026-01-03 11:41:16,754: t15.2023.12.08 val PER: 0.1751
2026-01-03 11:41:16,754: t15.2023.12.10 val PER: 0.1537
2026-01-03 11:41:16,754: t15.2023.12.17 val PER: 0.1913
2026-01-03 11:41:16,754: t15.2023.12.29 val PER: 0.2169
2026-01-03 11:41:16,754: t15.2024.02.25 val PER: 0.1728
2026-01-03 11:41:16,754: t15.2024.03.08 val PER: 0.3058
2026-01-03 11:41:16,754: t15.2024.03.15 val PER: 0.2633
2026-01-03 11:41:16,754: t15.2024.03.17 val PER: 0.2057
2026-01-03 11:41:16,754: t15.2024.05.10 val PER: 0.2214
2026-01-03 11:41:16,754: t15.2024.06.14 val PER: 0.2334
2026-01-03 11:41:16,754: t15.2024.07.19 val PER: 0.3125
2026-01-03 11:41:16,754: t15.2024.07.21 val PER: 0.1697
2026-01-03 11:41:16,755: t15.2024.07.28 val PER: 0.2037
2026-01-03 11:41:16,755: t15.2025.01.10 val PER: 0.4022
2026-01-03 11:41:16,755: t15.2025.01.12 val PER: 0.2179
2026-01-03 11:41:16,755: t15.2025.03.14 val PER: 0.3683
2026-01-03 11:41:16,755: t15.2025.03.16 val PER: 0.2500
2026-01-03 11:41:16,755: t15.2025.03.30 val PER: 0.3770
2026-01-03 11:41:16,755: t15.2025.04.13 val PER: 0.2725
2026-01-03 11:41:16,996: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_6000
2026-01-03 11:41:33,378: Train batch 6200: loss: 16.84 grad norm: 59.72 time: 0.069
2026-01-03 11:41:49,562: Train batch 6400: loss: 18.42 grad norm: 64.85 time: 0.063
2026-01-03 11:41:57,582: Running test after training batch: 6500
2026-01-03 11:41:57,680: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:42:02,382: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the could at this point as will
2026-01-03 11:42:02,410: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-03 11:42:04,018: Val batch 6500: PER (avg): 0.2066 CTC Loss (avg): 20.4169 WER(1gram): 53.30% (n=64) time: 6.435
2026-01-03 11:42:04,018: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 11:42:04,018: t15.2023.08.13 val PER: 0.1663
2026-01-03 11:42:04,019: t15.2023.08.18 val PER: 0.1467
2026-01-03 11:42:04,019: t15.2023.08.20 val PER: 0.1684
2026-01-03 11:42:04,019: t15.2023.08.25 val PER: 0.1039
2026-01-03 11:42:04,019: t15.2023.08.27 val PER: 0.2331
2026-01-03 11:42:04,019: t15.2023.09.01 val PER: 0.1250
2026-01-03 11:42:04,019: t15.2023.09.03 val PER: 0.2031
2026-01-03 11:42:04,019: t15.2023.09.24 val PER: 0.1723
2026-01-03 11:42:04,019: t15.2023.09.29 val PER: 0.1761
2026-01-03 11:42:04,019: t15.2023.10.01 val PER: 0.2206
2026-01-03 11:42:04,019: t15.2023.10.06 val PER: 0.1302
2026-01-03 11:42:04,019: t15.2023.10.08 val PER: 0.2991
2026-01-03 11:42:04,019: t15.2023.10.13 val PER: 0.2731
2026-01-03 11:42:04,020: t15.2023.10.15 val PER: 0.2076
2026-01-03 11:42:04,020: t15.2023.10.20 val PER: 0.2248
2026-01-03 11:42:04,020: t15.2023.10.22 val PER: 0.1581
2026-01-03 11:42:04,020: t15.2023.11.03 val PER: 0.2198
2026-01-03 11:42:04,020: t15.2023.11.04 val PER: 0.0648
2026-01-03 11:42:04,020: t15.2023.11.17 val PER: 0.0591
2026-01-03 11:42:04,020: t15.2023.11.19 val PER: 0.0719
2026-01-03 11:42:04,020: t15.2023.11.26 val PER: 0.2152
2026-01-03 11:42:04,020: t15.2023.12.03 val PER: 0.1849
2026-01-03 11:42:04,020: t15.2023.12.08 val PER: 0.1858
2026-01-03 11:42:04,020: t15.2023.12.10 val PER: 0.1472
2026-01-03 11:42:04,020: t15.2023.12.17 val PER: 0.1944
2026-01-03 11:42:04,020: t15.2023.12.29 val PER: 0.2086
2026-01-03 11:42:04,020: t15.2024.02.25 val PER: 0.1699
2026-01-03 11:42:04,020: t15.2024.03.08 val PER: 0.3058
2026-01-03 11:42:04,020: t15.2024.03.15 val PER: 0.2520
2026-01-03 11:42:04,021: t15.2024.03.17 val PER: 0.2001
2026-01-03 11:42:04,021: t15.2024.05.10 val PER: 0.2259
2026-01-03 11:42:04,021: t15.2024.06.14 val PER: 0.2161
2026-01-03 11:42:04,021: t15.2024.07.19 val PER: 0.3006
2026-01-03 11:42:04,021: t15.2024.07.21 val PER: 0.1531
2026-01-03 11:42:04,021: t15.2024.07.28 val PER: 0.1919
2026-01-03 11:42:04,021: t15.2025.01.10 val PER: 0.3609
2026-01-03 11:42:04,021: t15.2025.01.12 val PER: 0.2171
2026-01-03 11:42:04,021: t15.2025.03.14 val PER: 0.3817
2026-01-03 11:42:04,021: t15.2025.03.16 val PER: 0.2343
2026-01-03 11:42:04,021: t15.2025.03.30 val PER: 0.3402
2026-01-03 11:42:04,021: t15.2025.04.13 val PER: 0.2739
2026-01-03 11:42:04,022: New best val WER(1gram) 57.36% --> 53.30%
2026-01-03 11:42:04,022: Checkpointing model
2026-01-03 11:42:04,283: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/best_checkpoint
2026-01-03 11:42:04,537: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_6500
2026-01-03 11:42:12,484: Train batch 6600: loss: 12.22 grad norm: 52.57 time: 0.045
2026-01-03 11:42:28,727: Train batch 6800: loss: 15.18 grad norm: 56.96 time: 0.049
2026-01-03 11:42:44,863: Train batch 7000: loss: 17.59 grad norm: 66.47 time: 0.060
2026-01-03 11:42:44,863: Running test after training batch: 7000
2026-01-03 11:42:44,960: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:42:49,702: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point as will
2026-01-03 11:42:49,731: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 11:42:51,375: Val batch 7000: PER (avg): 0.1956 CTC Loss (avg): 19.2430 WER(1gram): 53.30% (n=64) time: 6.512
2026-01-03 11:42:51,376: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-03 11:42:51,376: t15.2023.08.13 val PER: 0.1559
2026-01-03 11:42:51,376: t15.2023.08.18 val PER: 0.1375
2026-01-03 11:42:51,376: t15.2023.08.20 val PER: 0.1509
2026-01-03 11:42:51,376: t15.2023.08.25 val PER: 0.0949
2026-01-03 11:42:51,376: t15.2023.08.27 val PER: 0.2154
2026-01-03 11:42:51,376: t15.2023.09.01 val PER: 0.1128
2026-01-03 11:42:51,376: t15.2023.09.03 val PER: 0.1995
2026-01-03 11:42:51,377: t15.2023.09.24 val PER: 0.1541
2026-01-03 11:42:51,377: t15.2023.09.29 val PER: 0.1685
2026-01-03 11:42:51,377: t15.2023.10.01 val PER: 0.2120
2026-01-03 11:42:51,377: t15.2023.10.06 val PER: 0.1087
2026-01-03 11:42:51,377: t15.2023.10.08 val PER: 0.2774
2026-01-03 11:42:51,377: t15.2023.10.13 val PER: 0.2614
2026-01-03 11:42:51,377: t15.2023.10.15 val PER: 0.1938
2026-01-03 11:42:51,377: t15.2023.10.20 val PER: 0.2282
2026-01-03 11:42:51,377: t15.2023.10.22 val PER: 0.1425
2026-01-03 11:42:51,377: t15.2023.11.03 val PER: 0.1988
2026-01-03 11:42:51,377: t15.2023.11.04 val PER: 0.0341
2026-01-03 11:42:51,377: t15.2023.11.17 val PER: 0.0700
2026-01-03 11:42:51,378: t15.2023.11.19 val PER: 0.0599
2026-01-03 11:42:51,378: t15.2023.11.26 val PER: 0.2014
2026-01-03 11:42:51,378: t15.2023.12.03 val PER: 0.1702
2026-01-03 11:42:51,378: t15.2023.12.08 val PER: 0.1571
2026-01-03 11:42:51,378: t15.2023.12.10 val PER: 0.1511
2026-01-03 11:42:51,378: t15.2023.12.17 val PER: 0.1757
2026-01-03 11:42:51,378: t15.2023.12.29 val PER: 0.1956
2026-01-03 11:42:51,378: t15.2024.02.25 val PER: 0.1573
2026-01-03 11:42:51,378: t15.2024.03.08 val PER: 0.2760
2026-01-03 11:42:51,378: t15.2024.03.15 val PER: 0.2458
2026-01-03 11:42:51,378: t15.2024.03.17 val PER: 0.2057
2026-01-03 11:42:51,378: t15.2024.05.10 val PER: 0.2021
2026-01-03 11:42:51,378: t15.2024.06.14 val PER: 0.1956
2026-01-03 11:42:51,378: t15.2024.07.19 val PER: 0.3052
2026-01-03 11:42:51,379: t15.2024.07.21 val PER: 0.1407
2026-01-03 11:42:51,379: t15.2024.07.28 val PER: 0.1794
2026-01-03 11:42:51,379: t15.2025.01.10 val PER: 0.3650
2026-01-03 11:42:51,379: t15.2025.01.12 val PER: 0.2117
2026-01-03 11:42:51,379: t15.2025.03.14 val PER: 0.3595
2026-01-03 11:42:51,379: t15.2025.03.16 val PER: 0.2382
2026-01-03 11:42:51,379: t15.2025.03.30 val PER: 0.3494
2026-01-03 11:42:51,379: t15.2025.04.13 val PER: 0.2668
2026-01-03 11:42:51,620: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_7000
2026-01-03 11:43:07,615: Train batch 7200: loss: 14.13 grad norm: 61.59 time: 0.078
2026-01-03 11:43:23,983: Train batch 7400: loss: 13.79 grad norm: 53.85 time: 0.075
2026-01-03 11:43:31,957: Running test after training batch: 7500
2026-01-03 11:43:32,126: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:43:36,852: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 11:43:36,882: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-03 11:43:38,548: Val batch 7500: PER (avg): 0.1910 CTC Loss (avg): 18.7459 WER(1gram): 55.08% (n=64) time: 6.591
2026-01-03 11:43:38,549: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 11:43:38,549: t15.2023.08.13 val PER: 0.1528
2026-01-03 11:43:38,549: t15.2023.08.18 val PER: 0.1391
2026-01-03 11:43:38,549: t15.2023.08.20 val PER: 0.1461
2026-01-03 11:43:38,549: t15.2023.08.25 val PER: 0.1099
2026-01-03 11:43:38,549: t15.2023.08.27 val PER: 0.2267
2026-01-03 11:43:38,549: t15.2023.09.01 val PER: 0.1136
2026-01-03 11:43:38,549: t15.2023.09.03 val PER: 0.1983
2026-01-03 11:43:38,549: t15.2023.09.24 val PER: 0.1541
2026-01-03 11:43:38,549: t15.2023.09.29 val PER: 0.1544
2026-01-03 11:43:38,549: t15.2023.10.01 val PER: 0.2061
2026-01-03 11:43:38,550: t15.2023.10.06 val PER: 0.1109
2026-01-03 11:43:38,550: t15.2023.10.08 val PER: 0.2652
2026-01-03 11:43:38,550: t15.2023.10.13 val PER: 0.2521
2026-01-03 11:43:38,550: t15.2023.10.15 val PER: 0.2011
2026-01-03 11:43:38,550: t15.2023.10.20 val PER: 0.2013
2026-01-03 11:43:38,550: t15.2023.10.22 val PER: 0.1403
2026-01-03 11:43:38,550: t15.2023.11.03 val PER: 0.2015
2026-01-03 11:43:38,550: t15.2023.11.04 val PER: 0.0341
2026-01-03 11:43:38,550: t15.2023.11.17 val PER: 0.0684
2026-01-03 11:43:38,550: t15.2023.11.19 val PER: 0.0479
2026-01-03 11:43:38,550: t15.2023.11.26 val PER: 0.1855
2026-01-03 11:43:38,550: t15.2023.12.03 val PER: 0.1597
2026-01-03 11:43:38,550: t15.2023.12.08 val PER: 0.1485
2026-01-03 11:43:38,550: t15.2023.12.10 val PER: 0.1314
2026-01-03 11:43:38,550: t15.2023.12.17 val PER: 0.1923
2026-01-03 11:43:38,551: t15.2023.12.29 val PER: 0.1908
2026-01-03 11:43:38,551: t15.2024.02.25 val PER: 0.1404
2026-01-03 11:43:38,551: t15.2024.03.08 val PER: 0.2831
2026-01-03 11:43:38,551: t15.2024.03.15 val PER: 0.2420
2026-01-03 11:43:38,551: t15.2024.03.17 val PER: 0.1806
2026-01-03 11:43:38,551: t15.2024.05.10 val PER: 0.2140
2026-01-03 11:43:38,551: t15.2024.06.14 val PER: 0.2145
2026-01-03 11:43:38,551: t15.2024.07.19 val PER: 0.2980
2026-01-03 11:43:38,551: t15.2024.07.21 val PER: 0.1379
2026-01-03 11:43:38,551: t15.2024.07.28 val PER: 0.1772
2026-01-03 11:43:38,551: t15.2025.01.10 val PER: 0.3540
2026-01-03 11:43:38,551: t15.2025.01.12 val PER: 0.1955
2026-01-03 11:43:38,551: t15.2025.03.14 val PER: 0.3491
2026-01-03 11:43:38,551: t15.2025.03.16 val PER: 0.2395
2026-01-03 11:43:38,551: t15.2025.03.30 val PER: 0.3471
2026-01-03 11:43:38,551: t15.2025.04.13 val PER: 0.2468
2026-01-03 11:43:38,793: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_7500
2026-01-03 11:43:46,923: Train batch 7600: loss: 16.23 grad norm: 59.13 time: 0.068
2026-01-03 11:44:03,071: Train batch 7800: loss: 14.56 grad norm: 57.92 time: 0.056
2026-01-03 11:44:19,869: Train batch 8000: loss: 11.56 grad norm: 51.49 time: 0.072
2026-01-03 11:44:19,870: Running test after training batch: 8000
2026-01-03 11:44:19,963: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:44:24,904: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-03 11:44:24,934: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-03 11:44:26,580: Val batch 8000: PER (avg): 0.1852 CTC Loss (avg): 18.1185 WER(1gram): 53.81% (n=64) time: 6.710
2026-01-03 11:44:26,580: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 11:44:26,580: t15.2023.08.13 val PER: 0.1466
2026-01-03 11:44:26,580: t15.2023.08.18 val PER: 0.1333
2026-01-03 11:44:26,580: t15.2023.08.20 val PER: 0.1422
2026-01-03 11:44:26,580: t15.2023.08.25 val PER: 0.1175
2026-01-03 11:44:26,581: t15.2023.08.27 val PER: 0.2203
2026-01-03 11:44:26,581: t15.2023.09.01 val PER: 0.1015
2026-01-03 11:44:26,581: t15.2023.09.03 val PER: 0.1900
2026-01-03 11:44:26,581: t15.2023.09.24 val PER: 0.1566
2026-01-03 11:44:26,581: t15.2023.09.29 val PER: 0.1519
2026-01-03 11:44:26,581: t15.2023.10.01 val PER: 0.2028
2026-01-03 11:44:26,584: t15.2023.10.06 val PER: 0.1130
2026-01-03 11:44:26,584: t15.2023.10.08 val PER: 0.2693
2026-01-03 11:44:26,584: t15.2023.10.13 val PER: 0.2382
2026-01-03 11:44:26,584: t15.2023.10.15 val PER: 0.1872
2026-01-03 11:44:26,584: t15.2023.10.20 val PER: 0.1980
2026-01-03 11:44:26,584: t15.2023.10.22 val PER: 0.1370
2026-01-03 11:44:26,584: t15.2023.11.03 val PER: 0.2049
2026-01-03 11:44:26,584: t15.2023.11.04 val PER: 0.0341
2026-01-03 11:44:26,584: t15.2023.11.17 val PER: 0.0622
2026-01-03 11:44:26,584: t15.2023.11.19 val PER: 0.0639
2026-01-03 11:44:26,585: t15.2023.11.26 val PER: 0.1804
2026-01-03 11:44:26,585: t15.2023.12.03 val PER: 0.1639
2026-01-03 11:44:26,585: t15.2023.12.08 val PER: 0.1465
2026-01-03 11:44:26,585: t15.2023.12.10 val PER: 0.1327
2026-01-03 11:44:26,585: t15.2023.12.17 val PER: 0.1715
2026-01-03 11:44:26,585: t15.2023.12.29 val PER: 0.1778
2026-01-03 11:44:26,585: t15.2024.02.25 val PER: 0.1404
2026-01-03 11:44:26,585: t15.2024.03.08 val PER: 0.2660
2026-01-03 11:44:26,585: t15.2024.03.15 val PER: 0.2433
2026-01-03 11:44:26,585: t15.2024.03.17 val PER: 0.1743
2026-01-03 11:44:26,585: t15.2024.05.10 val PER: 0.1917
2026-01-03 11:44:26,585: t15.2024.06.14 val PER: 0.2019
2026-01-03 11:44:26,585: t15.2024.07.19 val PER: 0.2835
2026-01-03 11:44:26,585: t15.2024.07.21 val PER: 0.1228
2026-01-03 11:44:26,585: t15.2024.07.28 val PER: 0.1566
2026-01-03 11:44:26,585: t15.2025.01.10 val PER: 0.3347
2026-01-03 11:44:26,585: t15.2025.01.12 val PER: 0.1925
2026-01-03 11:44:26,586: t15.2025.03.14 val PER: 0.3506
2026-01-03 11:44:26,586: t15.2025.03.16 val PER: 0.2382
2026-01-03 11:44:26,586: t15.2025.03.30 val PER: 0.3448
2026-01-03 11:44:26,586: t15.2025.04.13 val PER: 0.2639
2026-01-03 11:44:26,829: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_8000
2026-01-03 11:44:50,152: Train batch 8200: loss: 9.60 grad norm: 46.66 time: 0.054
2026-01-03 11:45:06,658: Train batch 8400: loss: 9.92 grad norm: 45.79 time: 0.064
2026-01-03 11:45:14,934: Running test after training batch: 8500
2026-01-03 11:45:15,036: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:45:19,828: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point will
2026-01-03 11:45:19,859: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 11:45:21,512: Val batch 8500: PER (avg): 0.1800 CTC Loss (avg): 17.8650 WER(1gram): 51.52% (n=64) time: 6.578
2026-01-03 11:45:21,513: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 11:45:21,513: t15.2023.08.13 val PER: 0.1372
2026-01-03 11:45:21,513: t15.2023.08.18 val PER: 0.1375
2026-01-03 11:45:21,513: t15.2023.08.20 val PER: 0.1390
2026-01-03 11:45:21,513: t15.2023.08.25 val PER: 0.1145
2026-01-03 11:45:21,513: t15.2023.08.27 val PER: 0.2170
2026-01-03 11:45:21,513: t15.2023.09.01 val PER: 0.1015
2026-01-03 11:45:21,514: t15.2023.09.03 val PER: 0.2007
2026-01-03 11:45:21,514: t15.2023.09.24 val PER: 0.1493
2026-01-03 11:45:21,514: t15.2023.09.29 val PER: 0.1525
2026-01-03 11:45:21,514: t15.2023.10.01 val PER: 0.1962
2026-01-03 11:45:21,514: t15.2023.10.06 val PER: 0.1076
2026-01-03 11:45:21,514: t15.2023.10.08 val PER: 0.2801
2026-01-03 11:45:21,514: t15.2023.10.13 val PER: 0.2343
2026-01-03 11:45:21,514: t15.2023.10.15 val PER: 0.1905
2026-01-03 11:45:21,514: t15.2023.10.20 val PER: 0.2047
2026-01-03 11:45:21,514: t15.2023.10.22 val PER: 0.1526
2026-01-03 11:45:21,514: t15.2023.11.03 val PER: 0.1974
2026-01-03 11:45:21,515: t15.2023.11.04 val PER: 0.0512
2026-01-03 11:45:21,515: t15.2023.11.17 val PER: 0.0544
2026-01-03 11:45:21,515: t15.2023.11.19 val PER: 0.0519
2026-01-03 11:45:21,515: t15.2023.11.26 val PER: 0.1659
2026-01-03 11:45:21,515: t15.2023.12.03 val PER: 0.1481
2026-01-03 11:45:21,515: t15.2023.12.08 val PER: 0.1418
2026-01-03 11:45:21,515: t15.2023.12.10 val PER: 0.1222
2026-01-03 11:45:21,515: t15.2023.12.17 val PER: 0.1632
2026-01-03 11:45:21,515: t15.2023.12.29 val PER: 0.1682
2026-01-03 11:45:21,515: t15.2024.02.25 val PER: 0.1433
2026-01-03 11:45:21,515: t15.2024.03.08 val PER: 0.2532
2026-01-03 11:45:21,515: t15.2024.03.15 val PER: 0.2264
2026-01-03 11:45:21,515: t15.2024.03.17 val PER: 0.1750
2026-01-03 11:45:21,515: t15.2024.05.10 val PER: 0.1947
2026-01-03 11:45:21,515: t15.2024.06.14 val PER: 0.1924
2026-01-03 11:45:21,515: t15.2024.07.19 val PER: 0.2762
2026-01-03 11:45:21,515: t15.2024.07.21 val PER: 0.1186
2026-01-03 11:45:21,515: t15.2024.07.28 val PER: 0.1610
2026-01-03 11:45:21,516: t15.2025.01.10 val PER: 0.3292
2026-01-03 11:45:21,516: t15.2025.01.12 val PER: 0.1886
2026-01-03 11:45:21,516: t15.2025.03.14 val PER: 0.3462
2026-01-03 11:45:21,516: t15.2025.03.16 val PER: 0.2120
2026-01-03 11:45:21,516: t15.2025.03.30 val PER: 0.3195
2026-01-03 11:45:21,516: t15.2025.04.13 val PER: 0.2268
2026-01-03 11:45:21,517: New best val WER(1gram) 53.30% --> 51.52%
2026-01-03 11:45:21,517: Checkpointing model
2026-01-03 11:45:21,779: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/best_checkpoint
2026-01-03 11:45:22,032: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_8500
2026-01-03 11:45:30,443: Train batch 8600: loss: 15.71 grad norm: 58.36 time: 0.054
2026-01-03 11:45:46,753: Train batch 8800: loss: 14.88 grad norm: 56.56 time: 0.060
2026-01-03 11:46:03,052: Train batch 9000: loss: 16.34 grad norm: 68.27 time: 0.072
2026-01-03 11:46:03,053: Running test after training batch: 9000
2026-01-03 11:46:03,162: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:46:08,140: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 11:46:08,169: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 11:46:09,864: Val batch 9000: PER (avg): 0.1744 CTC Loss (avg): 17.1400 WER(1gram): 52.28% (n=64) time: 6.811
2026-01-03 11:46:09,865: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 11:46:09,865: t15.2023.08.13 val PER: 0.1403
2026-01-03 11:46:09,865: t15.2023.08.18 val PER: 0.1282
2026-01-03 11:46:09,865: t15.2023.08.20 val PER: 0.1303
2026-01-03 11:46:09,865: t15.2023.08.25 val PER: 0.1099
2026-01-03 11:46:09,865: t15.2023.08.27 val PER: 0.2138
2026-01-03 11:46:09,865: t15.2023.09.01 val PER: 0.0877
2026-01-03 11:46:09,865: t15.2023.09.03 val PER: 0.1817
2026-01-03 11:46:09,865: t15.2023.09.24 val PER: 0.1553
2026-01-03 11:46:09,865: t15.2023.09.29 val PER: 0.1417
2026-01-03 11:46:09,865: t15.2023.10.01 val PER: 0.1942
2026-01-03 11:46:09,865: t15.2023.10.06 val PER: 0.0969
2026-01-03 11:46:09,865: t15.2023.10.08 val PER: 0.2639
2026-01-03 11:46:09,865: t15.2023.10.13 val PER: 0.2273
2026-01-03 11:46:09,866: t15.2023.10.15 val PER: 0.1905
2026-01-03 11:46:09,866: t15.2023.10.20 val PER: 0.1879
2026-01-03 11:46:09,866: t15.2023.10.22 val PER: 0.1325
2026-01-03 11:46:09,866: t15.2023.11.03 val PER: 0.1988
2026-01-03 11:46:09,866: t15.2023.11.04 val PER: 0.0341
2026-01-03 11:46:09,866: t15.2023.11.17 val PER: 0.0513
2026-01-03 11:46:09,866: t15.2023.11.19 val PER: 0.0459
2026-01-03 11:46:09,866: t15.2023.11.26 val PER: 0.1688
2026-01-03 11:46:09,866: t15.2023.12.03 val PER: 0.1376
2026-01-03 11:46:09,866: t15.2023.12.08 val PER: 0.1285
2026-01-03 11:46:09,866: t15.2023.12.10 val PER: 0.1104
2026-01-03 11:46:09,866: t15.2023.12.17 val PER: 0.1705
2026-01-03 11:46:09,866: t15.2023.12.29 val PER: 0.1620
2026-01-03 11:46:09,867: t15.2024.02.25 val PER: 0.1390
2026-01-03 11:46:09,867: t15.2024.03.08 val PER: 0.2660
2026-01-03 11:46:09,867: t15.2024.03.15 val PER: 0.2308
2026-01-03 11:46:09,867: t15.2024.03.17 val PER: 0.1639
2026-01-03 11:46:09,867: t15.2024.05.10 val PER: 0.1932
2026-01-03 11:46:09,867: t15.2024.06.14 val PER: 0.1845
2026-01-03 11:46:09,867: t15.2024.07.19 val PER: 0.2828
2026-01-03 11:46:09,867: t15.2024.07.21 val PER: 0.1069
2026-01-03 11:46:09,867: t15.2024.07.28 val PER: 0.1485
2026-01-03 11:46:09,867: t15.2025.01.10 val PER: 0.3113
2026-01-03 11:46:09,867: t15.2025.01.12 val PER: 0.1755
2026-01-03 11:46:09,867: t15.2025.03.14 val PER: 0.3417
2026-01-03 11:46:09,867: t15.2025.03.16 val PER: 0.2068
2026-01-03 11:46:09,867: t15.2025.03.30 val PER: 0.3149
2026-01-03 11:46:09,867: t15.2025.04.13 val PER: 0.2454
2026-01-03 11:46:10,111: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_9000
2026-01-03 11:46:26,403: Train batch 9200: loss: 10.66 grad norm: 48.86 time: 0.055
2026-01-03 11:46:42,531: Train batch 9400: loss: 7.78 grad norm: 46.48 time: 0.067
2026-01-03 11:46:50,670: Running test after training batch: 9500
2026-01-03 11:46:50,796: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:46:55,811: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point us will
2026-01-03 11:46:55,842: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-03 11:46:57,540: Val batch 9500: PER (avg): 0.1739 CTC Loss (avg): 17.2065 WER(1gram): 52.28% (n=64) time: 6.870
2026-01-03 11:46:57,540: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 11:46:57,541: t15.2023.08.13 val PER: 0.1372
2026-01-03 11:46:57,541: t15.2023.08.18 val PER: 0.1207
2026-01-03 11:46:57,541: t15.2023.08.20 val PER: 0.1287
2026-01-03 11:46:57,541: t15.2023.08.25 val PER: 0.1099
2026-01-03 11:46:57,541: t15.2023.08.27 val PER: 0.2010
2026-01-03 11:46:57,541: t15.2023.09.01 val PER: 0.0925
2026-01-03 11:46:57,541: t15.2023.09.03 val PER: 0.1770
2026-01-03 11:46:57,541: t15.2023.09.24 val PER: 0.1432
2026-01-03 11:46:57,541: t15.2023.09.29 val PER: 0.1487
2026-01-03 11:46:57,541: t15.2023.10.01 val PER: 0.1909
2026-01-03 11:46:57,541: t15.2023.10.06 val PER: 0.1098
2026-01-03 11:46:57,541: t15.2023.10.08 val PER: 0.2652
2026-01-03 11:46:57,542: t15.2023.10.13 val PER: 0.2258
2026-01-03 11:46:57,542: t15.2023.10.15 val PER: 0.1918
2026-01-03 11:46:57,542: t15.2023.10.20 val PER: 0.1846
2026-01-03 11:46:57,542: t15.2023.10.22 val PER: 0.1269
2026-01-03 11:46:57,542: t15.2023.11.03 val PER: 0.1981
2026-01-03 11:46:57,542: t15.2023.11.04 val PER: 0.0307
2026-01-03 11:46:57,542: t15.2023.11.17 val PER: 0.0544
2026-01-03 11:46:57,542: t15.2023.11.19 val PER: 0.0479
2026-01-03 11:46:57,542: t15.2023.11.26 val PER: 0.1587
2026-01-03 11:46:57,542: t15.2023.12.03 val PER: 0.1418
2026-01-03 11:46:57,542: t15.2023.12.08 val PER: 0.1398
2026-01-03 11:46:57,542: t15.2023.12.10 val PER: 0.1183
2026-01-03 11:46:57,543: t15.2023.12.17 val PER: 0.1778
2026-01-03 11:46:57,543: t15.2023.12.29 val PER: 0.1489
2026-01-03 11:46:57,543: t15.2024.02.25 val PER: 0.1334
2026-01-03 11:46:57,543: t15.2024.03.08 val PER: 0.2575
2026-01-03 11:46:57,543: t15.2024.03.15 val PER: 0.2258
2026-01-03 11:46:57,543: t15.2024.03.17 val PER: 0.1632
2026-01-03 11:46:57,543: t15.2024.05.10 val PER: 0.1872
2026-01-03 11:46:57,543: t15.2024.06.14 val PER: 0.1767
2026-01-03 11:46:57,543: t15.2024.07.19 val PER: 0.2676
2026-01-03 11:46:57,543: t15.2024.07.21 val PER: 0.1172
2026-01-03 11:46:57,543: t15.2024.07.28 val PER: 0.1507
2026-01-03 11:46:57,543: t15.2025.01.10 val PER: 0.3223
2026-01-03 11:46:57,543: t15.2025.01.12 val PER: 0.1840
2026-01-03 11:46:57,543: t15.2025.03.14 val PER: 0.3683
2026-01-03 11:46:57,543: t15.2025.03.16 val PER: 0.2120
2026-01-03 11:46:57,544: t15.2025.03.30 val PER: 0.3184
2026-01-03 11:46:57,544: t15.2025.04.13 val PER: 0.2268
2026-01-03 11:46:57,785: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_9500
2026-01-03 11:47:05,825: Train batch 9600: loss: 8.85 grad norm: 56.52 time: 0.074
2026-01-03 11:47:21,967: Train batch 9800: loss: 12.64 grad norm: 60.34 time: 0.063
2026-01-03 11:47:38,142: Train batch 10000: loss: 5.61 grad norm: 34.34 time: 0.061
2026-01-03 11:47:38,143: Running test after training batch: 10000
2026-01-03 11:47:38,252: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:47:42,914: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 11:47:42,944: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 11:47:44,679: Val batch 10000: PER (avg): 0.1709 CTC Loss (avg): 16.8749 WER(1gram): 51.52% (n=64) time: 6.537
2026-01-03 11:47:44,680: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 11:47:44,680: t15.2023.08.13 val PER: 0.1279
2026-01-03 11:47:44,680: t15.2023.08.18 val PER: 0.1266
2026-01-03 11:47:44,680: t15.2023.08.20 val PER: 0.1279
2026-01-03 11:47:44,680: t15.2023.08.25 val PER: 0.1069
2026-01-03 11:47:44,680: t15.2023.08.27 val PER: 0.2010
2026-01-03 11:47:44,680: t15.2023.09.01 val PER: 0.0893
2026-01-03 11:47:44,680: t15.2023.09.03 val PER: 0.1746
2026-01-03 11:47:44,680: t15.2023.09.24 val PER: 0.1517
2026-01-03 11:47:44,680: t15.2023.09.29 val PER: 0.1493
2026-01-03 11:47:44,680: t15.2023.10.01 val PER: 0.1915
2026-01-03 11:47:44,680: t15.2023.10.06 val PER: 0.1001
2026-01-03 11:47:44,680: t15.2023.10.08 val PER: 0.2652
2026-01-03 11:47:44,680: t15.2023.10.13 val PER: 0.2273
2026-01-03 11:47:44,680: t15.2023.10.15 val PER: 0.1714
2026-01-03 11:47:44,681: t15.2023.10.20 val PER: 0.2047
2026-01-03 11:47:44,681: t15.2023.10.22 val PER: 0.1247
2026-01-03 11:47:44,681: t15.2023.11.03 val PER: 0.1920
2026-01-03 11:47:44,681: t15.2023.11.04 val PER: 0.0375
2026-01-03 11:47:44,681: t15.2023.11.17 val PER: 0.0544
2026-01-03 11:47:44,681: t15.2023.11.19 val PER: 0.0439
2026-01-03 11:47:44,681: t15.2023.11.26 val PER: 0.1522
2026-01-03 11:47:44,681: t15.2023.12.03 val PER: 0.1218
2026-01-03 11:47:44,681: t15.2023.12.08 val PER: 0.1285
2026-01-03 11:47:44,681: t15.2023.12.10 val PER: 0.1130
2026-01-03 11:47:44,681: t15.2023.12.17 val PER: 0.1642
2026-01-03 11:47:44,681: t15.2023.12.29 val PER: 0.1558
2026-01-03 11:47:44,681: t15.2024.02.25 val PER: 0.1433
2026-01-03 11:47:44,682: t15.2024.03.08 val PER: 0.2632
2026-01-03 11:47:44,682: t15.2024.03.15 val PER: 0.2201
2026-01-03 11:47:44,682: t15.2024.03.17 val PER: 0.1618
2026-01-03 11:47:44,682: t15.2024.05.10 val PER: 0.1679
2026-01-03 11:47:44,682: t15.2024.06.14 val PER: 0.1798
2026-01-03 11:47:44,682: t15.2024.07.19 val PER: 0.2663
2026-01-03 11:47:44,682: t15.2024.07.21 val PER: 0.1172
2026-01-03 11:47:44,682: t15.2024.07.28 val PER: 0.1522
2026-01-03 11:47:44,682: t15.2025.01.10 val PER: 0.3113
2026-01-03 11:47:44,682: t15.2025.01.12 val PER: 0.1755
2026-01-03 11:47:44,682: t15.2025.03.14 val PER: 0.3536
2026-01-03 11:47:44,682: t15.2025.03.16 val PER: 0.2251
2026-01-03 11:47:44,682: t15.2025.03.30 val PER: 0.3149
2026-01-03 11:47:44,682: t15.2025.04.13 val PER: 0.2297
2026-01-03 11:47:44,929: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_10000
2026-01-03 11:48:00,969: Train batch 10200: loss: 6.45 grad norm: 39.46 time: 0.050
2026-01-03 11:48:17,268: Train batch 10400: loss: 9.15 grad norm: 48.98 time: 0.072
2026-01-03 11:48:25,429: Running test after training batch: 10500
2026-01-03 11:48:25,533: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:48:30,203: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 11:48:30,234: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-03 11:48:31,957: Val batch 10500: PER (avg): 0.1658 CTC Loss (avg): 16.6334 WER(1gram): 50.25% (n=64) time: 6.528
2026-01-03 11:48:31,958: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 11:48:31,958: t15.2023.08.13 val PER: 0.1227
2026-01-03 11:48:31,958: t15.2023.08.18 val PER: 0.1182
2026-01-03 11:48:31,958: t15.2023.08.20 val PER: 0.1231
2026-01-03 11:48:31,958: t15.2023.08.25 val PER: 0.1054
2026-01-03 11:48:31,958: t15.2023.08.27 val PER: 0.2010
2026-01-03 11:48:31,958: t15.2023.09.01 val PER: 0.0893
2026-01-03 11:48:31,958: t15.2023.09.03 val PER: 0.1686
2026-01-03 11:48:31,958: t15.2023.09.24 val PER: 0.1396
2026-01-03 11:48:31,958: t15.2023.09.29 val PER: 0.1512
2026-01-03 11:48:31,958: t15.2023.10.01 val PER: 0.1849
2026-01-03 11:48:31,958: t15.2023.10.06 val PER: 0.0936
2026-01-03 11:48:31,958: t15.2023.10.08 val PER: 0.2503
2026-01-03 11:48:31,959: t15.2023.10.13 val PER: 0.2087
2026-01-03 11:48:31,959: t15.2023.10.15 val PER: 0.1819
2026-01-03 11:48:31,959: t15.2023.10.20 val PER: 0.2013
2026-01-03 11:48:31,959: t15.2023.10.22 val PER: 0.1169
2026-01-03 11:48:31,959: t15.2023.11.03 val PER: 0.1954
2026-01-03 11:48:31,959: t15.2023.11.04 val PER: 0.0410
2026-01-03 11:48:31,959: t15.2023.11.17 val PER: 0.0467
2026-01-03 11:48:31,959: t15.2023.11.19 val PER: 0.0419
2026-01-03 11:48:31,959: t15.2023.11.26 val PER: 0.1355
2026-01-03 11:48:31,959: t15.2023.12.03 val PER: 0.1239
2026-01-03 11:48:31,959: t15.2023.12.08 val PER: 0.1205
2026-01-03 11:48:31,959: t15.2023.12.10 val PER: 0.0972
2026-01-03 11:48:31,959: t15.2023.12.17 val PER: 0.1580
2026-01-03 11:48:31,959: t15.2023.12.29 val PER: 0.1544
2026-01-03 11:48:31,959: t15.2024.02.25 val PER: 0.1292
2026-01-03 11:48:31,960: t15.2024.03.08 val PER: 0.2418
2026-01-03 11:48:31,960: t15.2024.03.15 val PER: 0.2108
2026-01-03 11:48:31,960: t15.2024.03.17 val PER: 0.1583
2026-01-03 11:48:31,960: t15.2024.05.10 val PER: 0.1768
2026-01-03 11:48:31,960: t15.2024.06.14 val PER: 0.1767
2026-01-03 11:48:31,960: t15.2024.07.19 val PER: 0.2683
2026-01-03 11:48:31,960: t15.2024.07.21 val PER: 0.1062
2026-01-03 11:48:31,960: t15.2024.07.28 val PER: 0.1441
2026-01-03 11:48:31,960: t15.2025.01.10 val PER: 0.3099
2026-01-03 11:48:31,960: t15.2025.01.12 val PER: 0.1755
2026-01-03 11:48:31,960: t15.2025.03.14 val PER: 0.3654
2026-01-03 11:48:31,960: t15.2025.03.16 val PER: 0.1937
2026-01-03 11:48:31,960: t15.2025.03.30 val PER: 0.3195
2026-01-03 11:48:31,960: t15.2025.04.13 val PER: 0.2254
2026-01-03 11:48:31,962: New best val WER(1gram) 51.52% --> 50.25%
2026-01-03 11:48:31,962: Checkpointing model
2026-01-03 11:48:32,235: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/best_checkpoint
2026-01-03 11:48:32,488: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_10500
2026-01-03 11:48:40,695: Train batch 10600: loss: 10.16 grad norm: 57.13 time: 0.072
2026-01-03 11:48:56,899: Train batch 10800: loss: 14.80 grad norm: 63.82 time: 0.064
2026-01-03 11:49:13,064: Train batch 11000: loss: 14.52 grad norm: 61.71 time: 0.056
2026-01-03 11:49:13,064: Running test after training batch: 11000
2026-01-03 11:49:13,187: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:49:17,850: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 11:49:17,880: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-03 11:49:19,596: Val batch 11000: PER (avg): 0.1638 CTC Loss (avg): 16.3734 WER(1gram): 46.95% (n=64) time: 6.532
2026-01-03 11:49:19,597: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 11:49:19,597: t15.2023.08.13 val PER: 0.1247
2026-01-03 11:49:19,597: t15.2023.08.18 val PER: 0.1199
2026-01-03 11:49:19,597: t15.2023.08.20 val PER: 0.1215
2026-01-03 11:49:19,597: t15.2023.08.25 val PER: 0.1024
2026-01-03 11:49:19,597: t15.2023.08.27 val PER: 0.1961
2026-01-03 11:49:19,598: t15.2023.09.01 val PER: 0.0860
2026-01-03 11:49:19,598: t15.2023.09.03 val PER: 0.1758
2026-01-03 11:49:19,598: t15.2023.09.24 val PER: 0.1408
2026-01-03 11:49:19,598: t15.2023.09.29 val PER: 0.1423
2026-01-03 11:49:19,598: t15.2023.10.01 val PER: 0.1902
2026-01-03 11:49:19,598: t15.2023.10.06 val PER: 0.0850
2026-01-03 11:49:19,598: t15.2023.10.08 val PER: 0.2544
2026-01-03 11:49:19,598: t15.2023.10.13 val PER: 0.2211
2026-01-03 11:49:19,598: t15.2023.10.15 val PER: 0.1661
2026-01-03 11:49:19,598: t15.2023.10.20 val PER: 0.1946
2026-01-03 11:49:19,599: t15.2023.10.22 val PER: 0.1214
2026-01-03 11:49:19,599: t15.2023.11.03 val PER: 0.1947
2026-01-03 11:49:19,599: t15.2023.11.04 val PER: 0.0341
2026-01-03 11:49:19,599: t15.2023.11.17 val PER: 0.0498
2026-01-03 11:49:19,599: t15.2023.11.19 val PER: 0.0439
2026-01-03 11:49:19,599: t15.2023.11.26 val PER: 0.1406
2026-01-03 11:49:19,599: t15.2023.12.03 val PER: 0.1334
2026-01-03 11:49:19,599: t15.2023.12.08 val PER: 0.1198
2026-01-03 11:49:19,599: t15.2023.12.10 val PER: 0.1051
2026-01-03 11:49:19,599: t15.2023.12.17 val PER: 0.1538
2026-01-03 11:49:19,599: t15.2023.12.29 val PER: 0.1441
2026-01-03 11:49:19,600: t15.2024.02.25 val PER: 0.1348
2026-01-03 11:49:19,600: t15.2024.03.08 val PER: 0.2447
2026-01-03 11:49:19,600: t15.2024.03.15 val PER: 0.2195
2026-01-03 11:49:19,600: t15.2024.03.17 val PER: 0.1492
2026-01-03 11:49:19,600: t15.2024.05.10 val PER: 0.1694
2026-01-03 11:49:19,600: t15.2024.06.14 val PER: 0.1625
2026-01-03 11:49:19,600: t15.2024.07.19 val PER: 0.2538
2026-01-03 11:49:19,600: t15.2024.07.21 val PER: 0.1069
2026-01-03 11:49:19,600: t15.2024.07.28 val PER: 0.1471
2026-01-03 11:49:19,600: t15.2025.01.10 val PER: 0.3127
2026-01-03 11:49:19,601: t15.2025.01.12 val PER: 0.1601
2026-01-03 11:49:19,601: t15.2025.03.14 val PER: 0.3565
2026-01-03 11:49:19,601: t15.2025.03.16 val PER: 0.1990
2026-01-03 11:49:19,601: t15.2025.03.30 val PER: 0.2989
2026-01-03 11:49:19,601: t15.2025.04.13 val PER: 0.2211
2026-01-03 11:49:19,601: New best val WER(1gram) 50.25% --> 46.95%
2026-01-03 11:49:19,601: Checkpointing model
2026-01-03 11:49:19,866: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/best_checkpoint
2026-01-03 11:49:20,119: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_11000
2026-01-03 11:49:36,132: Train batch 11200: loss: 10.68 grad norm: 52.32 time: 0.071
2026-01-03 11:49:52,254: Train batch 11400: loss: 9.75 grad norm: 52.46 time: 0.057
2026-01-03 11:50:00,376: Running test after training batch: 11500
2026-01-03 11:50:00,514: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:50:05,168: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 11:50:05,199: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-03 11:50:06,925: Val batch 11500: PER (avg): 0.1608 CTC Loss (avg): 16.3194 WER(1gram): 49.49% (n=64) time: 6.549
2026-01-03 11:50:06,925: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 11:50:06,925: t15.2023.08.13 val PER: 0.1154
2026-01-03 11:50:06,926: t15.2023.08.18 val PER: 0.1182
2026-01-03 11:50:06,926: t15.2023.08.20 val PER: 0.1183
2026-01-03 11:50:06,926: t15.2023.08.25 val PER: 0.0994
2026-01-03 11:50:06,926: t15.2023.08.27 val PER: 0.1961
2026-01-03 11:50:06,926: t15.2023.09.01 val PER: 0.0860
2026-01-03 11:50:06,926: t15.2023.09.03 val PER: 0.1686
2026-01-03 11:50:06,926: t15.2023.09.24 val PER: 0.1311
2026-01-03 11:50:06,926: t15.2023.09.29 val PER: 0.1347
2026-01-03 11:50:06,926: t15.2023.10.01 val PER: 0.1797
2026-01-03 11:50:06,926: t15.2023.10.06 val PER: 0.0926
2026-01-03 11:50:06,926: t15.2023.10.08 val PER: 0.2544
2026-01-03 11:50:06,927: t15.2023.10.13 val PER: 0.2056
2026-01-03 11:50:06,927: t15.2023.10.15 val PER: 0.1681
2026-01-03 11:50:06,927: t15.2023.10.20 val PER: 0.1779
2026-01-03 11:50:06,927: t15.2023.10.22 val PER: 0.1269
2026-01-03 11:50:06,927: t15.2023.11.03 val PER: 0.1832
2026-01-03 11:50:06,927: t15.2023.11.04 val PER: 0.0239
2026-01-03 11:50:06,927: t15.2023.11.17 val PER: 0.0435
2026-01-03 11:50:06,927: t15.2023.11.19 val PER: 0.0379
2026-01-03 11:50:06,927: t15.2023.11.26 val PER: 0.1377
2026-01-03 11:50:06,927: t15.2023.12.03 val PER: 0.1271
2026-01-03 11:50:06,927: t15.2023.12.08 val PER: 0.1105
2026-01-03 11:50:06,927: t15.2023.12.10 val PER: 0.1064
2026-01-03 11:50:06,927: t15.2023.12.17 val PER: 0.1486
2026-01-03 11:50:06,927: t15.2023.12.29 val PER: 0.1462
2026-01-03 11:50:06,927: t15.2024.02.25 val PER: 0.1208
2026-01-03 11:50:06,927: t15.2024.03.08 val PER: 0.2376
2026-01-03 11:50:06,927: t15.2024.03.15 val PER: 0.2170
2026-01-03 11:50:06,928: t15.2024.03.17 val PER: 0.1513
2026-01-03 11:50:06,928: t15.2024.05.10 val PER: 0.1813
2026-01-03 11:50:06,928: t15.2024.06.14 val PER: 0.1767
2026-01-03 11:50:06,928: t15.2024.07.19 val PER: 0.2498
2026-01-03 11:50:06,928: t15.2024.07.21 val PER: 0.1034
2026-01-03 11:50:06,928: t15.2024.07.28 val PER: 0.1419
2026-01-03 11:50:06,928: t15.2025.01.10 val PER: 0.3085
2026-01-03 11:50:06,928: t15.2025.01.12 val PER: 0.1647
2026-01-03 11:50:06,928: t15.2025.03.14 val PER: 0.3506
2026-01-03 11:50:06,928: t15.2025.03.16 val PER: 0.2094
2026-01-03 11:50:06,928: t15.2025.03.30 val PER: 0.3046
2026-01-03 11:50:06,929: t15.2025.04.13 val PER: 0.2240
2026-01-03 11:50:07,172: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_11500
2026-01-03 11:50:14,974: Train batch 11600: loss: 11.26 grad norm: 50.76 time: 0.060
2026-01-03 11:50:30,766: Train batch 11800: loss: 6.74 grad norm: 42.09 time: 0.044
2026-01-03 11:50:46,773: Train batch 12000: loss: 14.06 grad norm: 59.42 time: 0.070
2026-01-03 11:50:46,773: Running test after training batch: 12000
2026-01-03 11:50:46,880: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:50:51,681: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 11:50:51,712: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 11:50:53,466: Val batch 12000: PER (avg): 0.1612 CTC Loss (avg): 16.2307 WER(1gram): 49.24% (n=64) time: 6.693
2026-01-03 11:50:53,467: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 11:50:53,467: t15.2023.08.13 val PER: 0.1185
2026-01-03 11:50:53,467: t15.2023.08.18 val PER: 0.1098
2026-01-03 11:50:53,467: t15.2023.08.20 val PER: 0.1199
2026-01-03 11:50:53,467: t15.2023.08.25 val PER: 0.0994
2026-01-03 11:50:53,467: t15.2023.08.27 val PER: 0.1849
2026-01-03 11:50:53,467: t15.2023.09.01 val PER: 0.0836
2026-01-03 11:50:53,467: t15.2023.09.03 val PER: 0.1591
2026-01-03 11:50:53,467: t15.2023.09.24 val PER: 0.1347
2026-01-03 11:50:53,468: t15.2023.09.29 val PER: 0.1378
2026-01-03 11:50:53,468: t15.2023.10.01 val PER: 0.1777
2026-01-03 11:50:53,468: t15.2023.10.06 val PER: 0.0969
2026-01-03 11:50:53,468: t15.2023.10.08 val PER: 0.2598
2026-01-03 11:50:53,468: t15.2023.10.13 val PER: 0.2149
2026-01-03 11:50:53,468: t15.2023.10.15 val PER: 0.1701
2026-01-03 11:50:53,468: t15.2023.10.20 val PER: 0.1846
2026-01-03 11:50:53,468: t15.2023.10.22 val PER: 0.1225
2026-01-03 11:50:53,468: t15.2023.11.03 val PER: 0.1859
2026-01-03 11:50:53,468: t15.2023.11.04 val PER: 0.0375
2026-01-03 11:50:53,468: t15.2023.11.17 val PER: 0.0451
2026-01-03 11:50:53,469: t15.2023.11.19 val PER: 0.0339
2026-01-03 11:50:53,469: t15.2023.11.26 val PER: 0.1254
2026-01-03 11:50:53,469: t15.2023.12.03 val PER: 0.1229
2026-01-03 11:50:53,469: t15.2023.12.08 val PER: 0.1132
2026-01-03 11:50:53,469: t15.2023.12.10 val PER: 0.1025
2026-01-03 11:50:53,469: t15.2023.12.17 val PER: 0.1497
2026-01-03 11:50:53,469: t15.2023.12.29 val PER: 0.1386
2026-01-03 11:50:53,469: t15.2024.02.25 val PER: 0.1278
2026-01-03 11:50:53,469: t15.2024.03.08 val PER: 0.2447
2026-01-03 11:50:53,469: t15.2024.03.15 val PER: 0.2226
2026-01-03 11:50:53,469: t15.2024.03.17 val PER: 0.1437
2026-01-03 11:50:53,469: t15.2024.05.10 val PER: 0.1813
2026-01-03 11:50:53,469: t15.2024.06.14 val PER: 0.1830
2026-01-03 11:50:53,470: t15.2024.07.19 val PER: 0.2498
2026-01-03 11:50:53,470: t15.2024.07.21 val PER: 0.1103
2026-01-03 11:50:53,470: t15.2024.07.28 val PER: 0.1471
2026-01-03 11:50:53,470: t15.2025.01.10 val PER: 0.3196
2026-01-03 11:50:53,470: t15.2025.01.12 val PER: 0.1601
2026-01-03 11:50:53,470: t15.2025.03.14 val PER: 0.3713
2026-01-03 11:50:53,470: t15.2025.03.16 val PER: 0.1990
2026-01-03 11:50:53,470: t15.2025.03.30 val PER: 0.3069
2026-01-03 11:50:53,470: t15.2025.04.13 val PER: 0.2197
2026-01-03 11:50:53,710: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_12000
2026-01-03 11:51:09,880: Train batch 12200: loss: 5.88 grad norm: 39.27 time: 0.066
2026-01-03 11:51:26,145: Train batch 12400: loss: 4.93 grad norm: 35.06 time: 0.040
2026-01-03 11:51:34,227: Running test after training batch: 12500
2026-01-03 11:51:34,352: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:51:39,478: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 11:51:39,509: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost jent
2026-01-03 11:51:41,276: Val batch 12500: PER (avg): 0.1580 CTC Loss (avg): 15.9477 WER(1gram): 48.73% (n=64) time: 7.048
2026-01-03 11:51:41,276: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 11:51:41,276: t15.2023.08.13 val PER: 0.1258
2026-01-03 11:51:41,277: t15.2023.08.18 val PER: 0.1140
2026-01-03 11:51:41,277: t15.2023.08.20 val PER: 0.1160
2026-01-03 11:51:41,277: t15.2023.08.25 val PER: 0.0904
2026-01-03 11:51:41,277: t15.2023.08.27 val PER: 0.1897
2026-01-03 11:51:41,277: t15.2023.09.01 val PER: 0.0828
2026-01-03 11:51:41,277: t15.2023.09.03 val PER: 0.1651
2026-01-03 11:51:41,277: t15.2023.09.24 val PER: 0.1408
2026-01-03 11:51:41,277: t15.2023.09.29 val PER: 0.1340
2026-01-03 11:51:41,277: t15.2023.10.01 val PER: 0.1770
2026-01-03 11:51:41,277: t15.2023.10.06 val PER: 0.0861
2026-01-03 11:51:41,277: t15.2023.10.08 val PER: 0.2571
2026-01-03 11:51:41,277: t15.2023.10.13 val PER: 0.2102
2026-01-03 11:51:41,277: t15.2023.10.15 val PER: 0.1615
2026-01-03 11:51:41,277: t15.2023.10.20 val PER: 0.1846
2026-01-03 11:51:41,277: t15.2023.10.22 val PER: 0.1158
2026-01-03 11:51:41,278: t15.2023.11.03 val PER: 0.1866
2026-01-03 11:51:41,278: t15.2023.11.04 val PER: 0.0410
2026-01-03 11:51:41,278: t15.2023.11.17 val PER: 0.0404
2026-01-03 11:51:41,278: t15.2023.11.19 val PER: 0.0359
2026-01-03 11:51:41,278: t15.2023.11.26 val PER: 0.1290
2026-01-03 11:51:41,278: t15.2023.12.03 val PER: 0.1134
2026-01-03 11:51:41,278: t15.2023.12.08 val PER: 0.1099
2026-01-03 11:51:41,278: t15.2023.12.10 val PER: 0.0986
2026-01-03 11:51:41,278: t15.2023.12.17 val PER: 0.1580
2026-01-03 11:51:41,278: t15.2023.12.29 val PER: 0.1448
2026-01-03 11:51:41,278: t15.2024.02.25 val PER: 0.1138
2026-01-03 11:51:41,278: t15.2024.03.08 val PER: 0.2333
2026-01-03 11:51:41,278: t15.2024.03.15 val PER: 0.2126
2026-01-03 11:51:41,278: t15.2024.03.17 val PER: 0.1478
2026-01-03 11:51:41,278: t15.2024.05.10 val PER: 0.1783
2026-01-03 11:51:41,278: t15.2024.06.14 val PER: 0.1798
2026-01-03 11:51:41,279: t15.2024.07.19 val PER: 0.2452
2026-01-03 11:51:41,279: t15.2024.07.21 val PER: 0.0993
2026-01-03 11:51:41,279: t15.2024.07.28 val PER: 0.1346
2026-01-03 11:51:41,279: t15.2025.01.10 val PER: 0.3113
2026-01-03 11:51:41,279: t15.2025.01.12 val PER: 0.1540
2026-01-03 11:51:41,279: t15.2025.03.14 val PER: 0.3683
2026-01-03 11:51:41,279: t15.2025.03.16 val PER: 0.1937
2026-01-03 11:51:41,279: t15.2025.03.30 val PER: 0.2954
2026-01-03 11:51:41,279: t15.2025.04.13 val PER: 0.2097
2026-01-03 11:51:41,521: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_12500
2026-01-03 11:51:49,634: Train batch 12600: loss: 7.63 grad norm: 43.15 time: 0.058
2026-01-03 11:52:05,719: Train batch 12800: loss: 5.75 grad norm: 37.24 time: 0.052
2026-01-03 11:52:21,789: Train batch 13000: loss: 6.48 grad norm: 42.13 time: 0.066
2026-01-03 11:52:21,790: Running test after training batch: 13000
2026-01-03 11:52:21,887: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:52:26,571: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 11:52:26,603: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-03 11:52:28,384: Val batch 13000: PER (avg): 0.1577 CTC Loss (avg): 15.7880 WER(1gram): 46.95% (n=64) time: 6.594
2026-01-03 11:52:28,385: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 11:52:28,385: t15.2023.08.13 val PER: 0.1237
2026-01-03 11:52:28,385: t15.2023.08.18 val PER: 0.1140
2026-01-03 11:52:28,385: t15.2023.08.20 val PER: 0.1136
2026-01-03 11:52:28,385: t15.2023.08.25 val PER: 0.0919
2026-01-03 11:52:28,385: t15.2023.08.27 val PER: 0.1961
2026-01-03 11:52:28,385: t15.2023.09.01 val PER: 0.0795
2026-01-03 11:52:28,385: t15.2023.09.03 val PER: 0.1663
2026-01-03 11:52:28,385: t15.2023.09.24 val PER: 0.1408
2026-01-03 11:52:28,385: t15.2023.09.29 val PER: 0.1366
2026-01-03 11:52:28,385: t15.2023.10.01 val PER: 0.1717
2026-01-03 11:52:28,385: t15.2023.10.06 val PER: 0.0893
2026-01-03 11:52:28,385: t15.2023.10.08 val PER: 0.2544
2026-01-03 11:52:28,385: t15.2023.10.13 val PER: 0.2056
2026-01-03 11:52:28,386: t15.2023.10.15 val PER: 0.1608
2026-01-03 11:52:28,386: t15.2023.10.20 val PER: 0.1913
2026-01-03 11:52:28,386: t15.2023.10.22 val PER: 0.1169
2026-01-03 11:52:28,386: t15.2023.11.03 val PER: 0.1845
2026-01-03 11:52:28,386: t15.2023.11.04 val PER: 0.0410
2026-01-03 11:52:28,386: t15.2023.11.17 val PER: 0.0420
2026-01-03 11:52:28,386: t15.2023.11.19 val PER: 0.0339
2026-01-03 11:52:28,386: t15.2023.11.26 val PER: 0.1275
2026-01-03 11:52:28,386: t15.2023.12.03 val PER: 0.1166
2026-01-03 11:52:28,386: t15.2023.12.08 val PER: 0.1105
2026-01-03 11:52:28,386: t15.2023.12.10 val PER: 0.0999
2026-01-03 11:52:28,386: t15.2023.12.17 val PER: 0.1435
2026-01-03 11:52:28,386: t15.2023.12.29 val PER: 0.1414
2026-01-03 11:52:28,386: t15.2024.02.25 val PER: 0.1124
2026-01-03 11:52:28,386: t15.2024.03.08 val PER: 0.2447
2026-01-03 11:52:28,386: t15.2024.03.15 val PER: 0.2126
2026-01-03 11:52:28,387: t15.2024.03.17 val PER: 0.1485
2026-01-03 11:52:28,387: t15.2024.05.10 val PER: 0.1679
2026-01-03 11:52:28,387: t15.2024.06.14 val PER: 0.1703
2026-01-03 11:52:28,387: t15.2024.07.19 val PER: 0.2459
2026-01-03 11:52:28,387: t15.2024.07.21 val PER: 0.0979
2026-01-03 11:52:28,387: t15.2024.07.28 val PER: 0.1463
2026-01-03 11:52:28,387: t15.2025.01.10 val PER: 0.3072
2026-01-03 11:52:28,387: t15.2025.01.12 val PER: 0.1640
2026-01-03 11:52:28,387: t15.2025.03.14 val PER: 0.3476
2026-01-03 11:52:28,387: t15.2025.03.16 val PER: 0.1950
2026-01-03 11:52:28,387: t15.2025.03.30 val PER: 0.3023
2026-01-03 11:52:28,387: t15.2025.04.13 val PER: 0.2168
2026-01-03 11:52:28,649: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_13000
2026-01-03 11:52:44,670: Train batch 13200: loss: 12.64 grad norm: 62.08 time: 0.054
2026-01-03 11:53:00,496: Train batch 13400: loss: 8.63 grad norm: 51.85 time: 0.062
2026-01-03 11:53:08,470: Running test after training batch: 13500
2026-01-03 11:53:08,579: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:53:13,238: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 11:53:13,271: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-03 11:53:15,081: Val batch 13500: PER (avg): 0.1545 CTC Loss (avg): 15.6196 WER(1gram): 47.72% (n=64) time: 6.610
2026-01-03 11:53:15,081: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-03 11:53:15,081: t15.2023.08.13 val PER: 0.1185
2026-01-03 11:53:15,081: t15.2023.08.18 val PER: 0.1115
2026-01-03 11:53:15,082: t15.2023.08.20 val PER: 0.1088
2026-01-03 11:53:15,082: t15.2023.08.25 val PER: 0.0964
2026-01-03 11:53:15,082: t15.2023.08.27 val PER: 0.1833
2026-01-03 11:53:15,082: t15.2023.09.01 val PER: 0.0820
2026-01-03 11:53:15,082: t15.2023.09.03 val PER: 0.1651
2026-01-03 11:53:15,082: t15.2023.09.24 val PER: 0.1311
2026-01-03 11:53:15,082: t15.2023.09.29 val PER: 0.1366
2026-01-03 11:53:15,082: t15.2023.10.01 val PER: 0.1770
2026-01-03 11:53:15,082: t15.2023.10.06 val PER: 0.0850
2026-01-03 11:53:15,082: t15.2023.10.08 val PER: 0.2585
2026-01-03 11:53:15,083: t15.2023.10.13 val PER: 0.2071
2026-01-03 11:53:15,083: t15.2023.10.15 val PER: 0.1569
2026-01-03 11:53:15,083: t15.2023.10.20 val PER: 0.1946
2026-01-03 11:53:15,083: t15.2023.10.22 val PER: 0.1125
2026-01-03 11:53:15,083: t15.2023.11.03 val PER: 0.1839
2026-01-03 11:53:15,083: t15.2023.11.04 val PER: 0.0375
2026-01-03 11:53:15,083: t15.2023.11.17 val PER: 0.0451
2026-01-03 11:53:15,083: t15.2023.11.19 val PER: 0.0319
2026-01-03 11:53:15,083: t15.2023.11.26 val PER: 0.1333
2026-01-03 11:53:15,083: t15.2023.12.03 val PER: 0.1092
2026-01-03 11:53:15,083: t15.2023.12.08 val PER: 0.1032
2026-01-03 11:53:15,083: t15.2023.12.10 val PER: 0.0920
2026-01-03 11:53:15,083: t15.2023.12.17 val PER: 0.1393
2026-01-03 11:53:15,083: t15.2023.12.29 val PER: 0.1325
2026-01-03 11:53:15,083: t15.2024.02.25 val PER: 0.1194
2026-01-03 11:53:15,083: t15.2024.03.08 val PER: 0.2361
2026-01-03 11:53:15,084: t15.2024.03.15 val PER: 0.2089
2026-01-03 11:53:15,084: t15.2024.03.17 val PER: 0.1457
2026-01-03 11:53:15,084: t15.2024.05.10 val PER: 0.1664
2026-01-03 11:53:15,084: t15.2024.06.14 val PER: 0.1703
2026-01-03 11:53:15,084: t15.2024.07.19 val PER: 0.2459
2026-01-03 11:53:15,084: t15.2024.07.21 val PER: 0.0931
2026-01-03 11:53:15,084: t15.2024.07.28 val PER: 0.1338
2026-01-03 11:53:15,084: t15.2025.01.10 val PER: 0.2989
2026-01-03 11:53:15,084: t15.2025.01.12 val PER: 0.1463
2026-01-03 11:53:15,085: t15.2025.03.14 val PER: 0.3417
2026-01-03 11:53:15,085: t15.2025.03.16 val PER: 0.1937
2026-01-03 11:53:15,085: t15.2025.03.30 val PER: 0.3057
2026-01-03 11:53:15,085: t15.2025.04.13 val PER: 0.2068
2026-01-03 11:53:15,347: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_13500
2026-01-03 11:53:23,384: Train batch 13600: loss: 12.53 grad norm: 63.22 time: 0.062
2026-01-03 11:53:39,542: Train batch 13800: loss: 9.14 grad norm: 55.04 time: 0.056
2026-01-03 11:53:55,346: Train batch 14000: loss: 11.73 grad norm: 60.24 time: 0.050
2026-01-03 11:53:55,347: Running test after training batch: 14000
2026-01-03 11:53:55,504: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:54:00,167: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 11:54:00,200: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost get
2026-01-03 11:54:02,017: Val batch 14000: PER (avg): 0.1528 CTC Loss (avg): 15.5593 WER(1gram): 47.46% (n=64) time: 6.670
2026-01-03 11:54:02,018: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=11
2026-01-03 11:54:02,018: t15.2023.08.13 val PER: 0.1123
2026-01-03 11:54:02,018: t15.2023.08.18 val PER: 0.1056
2026-01-03 11:54:02,018: t15.2023.08.20 val PER: 0.1056
2026-01-03 11:54:02,018: t15.2023.08.25 val PER: 0.0979
2026-01-03 11:54:02,018: t15.2023.08.27 val PER: 0.1897
2026-01-03 11:54:02,018: t15.2023.09.01 val PER: 0.0804
2026-01-03 11:54:02,018: t15.2023.09.03 val PER: 0.1710
2026-01-03 11:54:02,018: t15.2023.09.24 val PER: 0.1226
2026-01-03 11:54:02,018: t15.2023.09.29 val PER: 0.1315
2026-01-03 11:54:02,019: t15.2023.10.01 val PER: 0.1797
2026-01-03 11:54:02,019: t15.2023.10.06 val PER: 0.0818
2026-01-03 11:54:02,019: t15.2023.10.08 val PER: 0.2558
2026-01-03 11:54:02,019: t15.2023.10.13 val PER: 0.2164
2026-01-03 11:54:02,019: t15.2023.10.15 val PER: 0.1595
2026-01-03 11:54:02,019: t15.2023.10.20 val PER: 0.1879
2026-01-03 11:54:02,019: t15.2023.10.22 val PER: 0.1047
2026-01-03 11:54:02,019: t15.2023.11.03 val PER: 0.1750
2026-01-03 11:54:02,019: t15.2023.11.04 val PER: 0.0307
2026-01-03 11:54:02,019: t15.2023.11.17 val PER: 0.0482
2026-01-03 11:54:02,019: t15.2023.11.19 val PER: 0.0299
2026-01-03 11:54:02,019: t15.2023.11.26 val PER: 0.1341
2026-01-03 11:54:02,019: t15.2023.12.03 val PER: 0.1166
2026-01-03 11:54:02,019: t15.2023.12.08 val PER: 0.1052
2026-01-03 11:54:02,020: t15.2023.12.10 val PER: 0.0972
2026-01-03 11:54:02,020: t15.2023.12.17 val PER: 0.1372
2026-01-03 11:54:02,020: t15.2023.12.29 val PER: 0.1325
2026-01-03 11:54:02,020: t15.2024.02.25 val PER: 0.1110
2026-01-03 11:54:02,020: t15.2024.03.08 val PER: 0.2333
2026-01-03 11:54:02,020: t15.2024.03.15 val PER: 0.2083
2026-01-03 11:54:02,020: t15.2024.03.17 val PER: 0.1492
2026-01-03 11:54:02,020: t15.2024.05.10 val PER: 0.1575
2026-01-03 11:54:02,020: t15.2024.06.14 val PER: 0.1609
2026-01-03 11:54:02,020: t15.2024.07.19 val PER: 0.2386
2026-01-03 11:54:02,020: t15.2024.07.21 val PER: 0.0910
2026-01-03 11:54:02,020: t15.2024.07.28 val PER: 0.1375
2026-01-03 11:54:02,020: t15.2025.01.10 val PER: 0.2934
2026-01-03 11:54:02,020: t15.2025.01.12 val PER: 0.1455
2026-01-03 11:54:02,021: t15.2025.03.14 val PER: 0.3417
2026-01-03 11:54:02,021: t15.2025.03.16 val PER: 0.1819
2026-01-03 11:54:02,021: t15.2025.03.30 val PER: 0.2931
2026-01-03 11:54:02,021: t15.2025.04.13 val PER: 0.2040
2026-01-03 11:54:02,287: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_14000
2026-01-03 11:54:18,353: Train batch 14200: loss: 8.55 grad norm: 51.30 time: 0.056
2026-01-03 11:54:34,580: Train batch 14400: loss: 5.71 grad norm: 39.45 time: 0.064
2026-01-03 11:54:42,968: Running test after training batch: 14500
2026-01-03 11:54:43,060: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:54:47,756: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 11:54:47,788: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 11:54:49,592: Val batch 14500: PER (avg): 0.1540 CTC Loss (avg): 15.5897 WER(1gram): 46.19% (n=64) time: 6.623
2026-01-03 11:54:49,592: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 11:54:49,592: t15.2023.08.13 val PER: 0.1143
2026-01-03 11:54:49,592: t15.2023.08.18 val PER: 0.1073
2026-01-03 11:54:49,592: t15.2023.08.20 val PER: 0.1064
2026-01-03 11:54:49,592: t15.2023.08.25 val PER: 0.0949
2026-01-03 11:54:49,592: t15.2023.08.27 val PER: 0.1945
2026-01-03 11:54:49,592: t15.2023.09.01 val PER: 0.0804
2026-01-03 11:54:49,593: t15.2023.09.03 val PER: 0.1603
2026-01-03 11:54:49,593: t15.2023.09.24 val PER: 0.1299
2026-01-03 11:54:49,593: t15.2023.09.29 val PER: 0.1321
2026-01-03 11:54:49,593: t15.2023.10.01 val PER: 0.1849
2026-01-03 11:54:49,593: t15.2023.10.06 val PER: 0.0840
2026-01-03 11:54:49,593: t15.2023.10.08 val PER: 0.2503
2026-01-03 11:54:49,593: t15.2023.10.13 val PER: 0.2064
2026-01-03 11:54:49,593: t15.2023.10.15 val PER: 0.1589
2026-01-03 11:54:49,593: t15.2023.10.20 val PER: 0.1913
2026-01-03 11:54:49,593: t15.2023.10.22 val PER: 0.1136
2026-01-03 11:54:49,593: t15.2023.11.03 val PER: 0.1852
2026-01-03 11:54:49,593: t15.2023.11.04 val PER: 0.0341
2026-01-03 11:54:49,593: t15.2023.11.17 val PER: 0.0435
2026-01-03 11:54:49,593: t15.2023.11.19 val PER: 0.0399
2026-01-03 11:54:49,594: t15.2023.11.26 val PER: 0.1261
2026-01-03 11:54:49,594: t15.2023.12.03 val PER: 0.1113
2026-01-03 11:54:49,594: t15.2023.12.08 val PER: 0.1052
2026-01-03 11:54:49,594: t15.2023.12.10 val PER: 0.0959
2026-01-03 11:54:49,594: t15.2023.12.17 val PER: 0.1497
2026-01-03 11:54:49,594: t15.2023.12.29 val PER: 0.1318
2026-01-03 11:54:49,594: t15.2024.02.25 val PER: 0.1124
2026-01-03 11:54:49,594: t15.2024.03.08 val PER: 0.2418
2026-01-03 11:54:49,594: t15.2024.03.15 val PER: 0.2089
2026-01-03 11:54:49,594: t15.2024.03.17 val PER: 0.1457
2026-01-03 11:54:49,594: t15.2024.05.10 val PER: 0.1530
2026-01-03 11:54:49,594: t15.2024.06.14 val PER: 0.1656
2026-01-03 11:54:49,594: t15.2024.07.19 val PER: 0.2472
2026-01-03 11:54:49,594: t15.2024.07.21 val PER: 0.0931
2026-01-03 11:54:49,594: t15.2024.07.28 val PER: 0.1368
2026-01-03 11:54:49,595: t15.2025.01.10 val PER: 0.2920
2026-01-03 11:54:49,595: t15.2025.01.12 val PER: 0.1486
2026-01-03 11:54:49,595: t15.2025.03.14 val PER: 0.3536
2026-01-03 11:54:49,595: t15.2025.03.16 val PER: 0.1793
2026-01-03 11:54:49,595: t15.2025.03.30 val PER: 0.2989
2026-01-03 11:54:49,595: t15.2025.04.13 val PER: 0.2097
2026-01-03 11:54:49,596: New best val WER(1gram) 46.95% --> 46.19%
2026-01-03 11:54:49,596: Checkpointing model
2026-01-03 11:54:49,859: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/best_checkpoint
2026-01-03 11:54:50,150: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_14500
2026-01-03 11:54:58,233: Train batch 14600: loss: 12.30 grad norm: 59.06 time: 0.058
2026-01-03 11:55:14,725: Train batch 14800: loss: 6.14 grad norm: 44.96 time: 0.050
2026-01-03 11:55:31,118: Train batch 15000: loss: 8.90 grad norm: 49.32 time: 0.052
2026-01-03 11:55:31,119: Running test after training batch: 15000
2026-01-03 11:55:31,238: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:55:36,233: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 11:55:36,265: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-03 11:55:38,054: Val batch 15000: PER (avg): 0.1510 CTC Loss (avg): 15.2875 WER(1gram): 46.19% (n=64) time: 6.935
2026-01-03 11:55:38,054: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-03 11:55:38,054: t15.2023.08.13 val PER: 0.1102
2026-01-03 11:55:38,054: t15.2023.08.18 val PER: 0.1031
2026-01-03 11:55:38,054: t15.2023.08.20 val PER: 0.1112
2026-01-03 11:55:38,054: t15.2023.08.25 val PER: 0.0934
2026-01-03 11:55:38,055: t15.2023.08.27 val PER: 0.1720
2026-01-03 11:55:38,055: t15.2023.09.01 val PER: 0.0763
2026-01-03 11:55:38,055: t15.2023.09.03 val PER: 0.1568
2026-01-03 11:55:38,055: t15.2023.09.24 val PER: 0.1359
2026-01-03 11:55:38,055: t15.2023.09.29 val PER: 0.1264
2026-01-03 11:55:38,055: t15.2023.10.01 val PER: 0.1770
2026-01-03 11:55:38,055: t15.2023.10.06 val PER: 0.0753
2026-01-03 11:55:38,055: t15.2023.10.08 val PER: 0.2598
2026-01-03 11:55:38,055: t15.2023.10.13 val PER: 0.2033
2026-01-03 11:55:38,055: t15.2023.10.15 val PER: 0.1549
2026-01-03 11:55:38,055: t15.2023.10.20 val PER: 0.1980
2026-01-03 11:55:38,055: t15.2023.10.22 val PER: 0.1169
2026-01-03 11:55:38,055: t15.2023.11.03 val PER: 0.1757
2026-01-03 11:55:38,055: t15.2023.11.04 val PER: 0.0375
2026-01-03 11:55:38,056: t15.2023.11.17 val PER: 0.0389
2026-01-03 11:55:38,056: t15.2023.11.19 val PER: 0.0279
2026-01-03 11:55:38,056: t15.2023.11.26 val PER: 0.1246
2026-01-03 11:55:38,056: t15.2023.12.03 val PER: 0.1124
2026-01-03 11:55:38,056: t15.2023.12.08 val PER: 0.1025
2026-01-03 11:55:38,056: t15.2023.12.10 val PER: 0.0880
2026-01-03 11:55:38,056: t15.2023.12.17 val PER: 0.1476
2026-01-03 11:55:38,056: t15.2023.12.29 val PER: 0.1325
2026-01-03 11:55:38,056: t15.2024.02.25 val PER: 0.1081
2026-01-03 11:55:38,056: t15.2024.03.08 val PER: 0.2290
2026-01-03 11:55:38,056: t15.2024.03.15 val PER: 0.2051
2026-01-03 11:55:38,056: t15.2024.03.17 val PER: 0.1381
2026-01-03 11:55:38,056: t15.2024.05.10 val PER: 0.1620
2026-01-03 11:55:38,057: t15.2024.06.14 val PER: 0.1672
2026-01-03 11:55:38,057: t15.2024.07.19 val PER: 0.2334
2026-01-03 11:55:38,057: t15.2024.07.21 val PER: 0.0897
2026-01-03 11:55:38,057: t15.2024.07.28 val PER: 0.1375
2026-01-03 11:55:38,057: t15.2025.01.10 val PER: 0.3099
2026-01-03 11:55:38,057: t15.2025.01.12 val PER: 0.1409
2026-01-03 11:55:38,057: t15.2025.03.14 val PER: 0.3595
2026-01-03 11:55:38,057: t15.2025.03.16 val PER: 0.1832
2026-01-03 11:55:38,057: t15.2025.03.30 val PER: 0.2954
2026-01-03 11:55:38,057: t15.2025.04.13 val PER: 0.2054
2026-01-03 11:55:38,320: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_15000
2026-01-03 11:55:54,906: Train batch 15200: loss: 5.28 grad norm: 42.49 time: 0.057
2026-01-03 11:56:10,724: Train batch 15400: loss: 11.54 grad norm: 54.22 time: 0.050
2026-01-03 11:56:18,909: Running test after training batch: 15500
2026-01-03 11:56:19,055: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:56:24,009: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-03 11:56:24,042: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 11:56:25,873: Val batch 15500: PER (avg): 0.1488 CTC Loss (avg): 15.2889 WER(1gram): 45.18% (n=64) time: 6.964
2026-01-03 11:56:25,874: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 11:56:25,874: t15.2023.08.13 val PER: 0.1154
2026-01-03 11:56:25,874: t15.2023.08.18 val PER: 0.1073
2026-01-03 11:56:25,874: t15.2023.08.20 val PER: 0.1096
2026-01-03 11:56:25,874: t15.2023.08.25 val PER: 0.0919
2026-01-03 11:56:25,874: t15.2023.08.27 val PER: 0.1785
2026-01-03 11:56:25,874: t15.2023.09.01 val PER: 0.0779
2026-01-03 11:56:25,874: t15.2023.09.03 val PER: 0.1544
2026-01-03 11:56:25,874: t15.2023.09.24 val PER: 0.1262
2026-01-03 11:56:25,875: t15.2023.09.29 val PER: 0.1289
2026-01-03 11:56:25,875: t15.2023.10.01 val PER: 0.1777
2026-01-03 11:56:25,875: t15.2023.10.06 val PER: 0.0807
2026-01-03 11:56:25,875: t15.2023.10.08 val PER: 0.2503
2026-01-03 11:56:25,875: t15.2023.10.13 val PER: 0.2048
2026-01-03 11:56:25,875: t15.2023.10.15 val PER: 0.1470
2026-01-03 11:56:25,875: t15.2023.10.20 val PER: 0.1779
2026-01-03 11:56:25,875: t15.2023.10.22 val PER: 0.1147
2026-01-03 11:56:25,875: t15.2023.11.03 val PER: 0.1696
2026-01-03 11:56:25,875: t15.2023.11.04 val PER: 0.0341
2026-01-03 11:56:25,875: t15.2023.11.17 val PER: 0.0358
2026-01-03 11:56:25,875: t15.2023.11.19 val PER: 0.0379
2026-01-03 11:56:25,875: t15.2023.11.26 val PER: 0.1167
2026-01-03 11:56:25,875: t15.2023.12.03 val PER: 0.1113
2026-01-03 11:56:25,875: t15.2023.12.08 val PER: 0.0965
2026-01-03 11:56:25,875: t15.2023.12.10 val PER: 0.0880
2026-01-03 11:56:25,876: t15.2023.12.17 val PER: 0.1383
2026-01-03 11:56:25,876: t15.2023.12.29 val PER: 0.1201
2026-01-03 11:56:25,876: t15.2024.02.25 val PER: 0.1039
2026-01-03 11:56:25,876: t15.2024.03.08 val PER: 0.2304
2026-01-03 11:56:25,876: t15.2024.03.15 val PER: 0.1995
2026-01-03 11:56:25,876: t15.2024.03.17 val PER: 0.1388
2026-01-03 11:56:25,876: t15.2024.05.10 val PER: 0.1530
2026-01-03 11:56:25,876: t15.2024.06.14 val PER: 0.1562
2026-01-03 11:56:25,876: t15.2024.07.19 val PER: 0.2426
2026-01-03 11:56:25,876: t15.2024.07.21 val PER: 0.0938
2026-01-03 11:56:25,876: t15.2024.07.28 val PER: 0.1324
2026-01-03 11:56:25,876: t15.2025.01.10 val PER: 0.2906
2026-01-03 11:56:25,876: t15.2025.01.12 val PER: 0.1447
2026-01-03 11:56:25,876: t15.2025.03.14 val PER: 0.3476
2026-01-03 11:56:25,876: t15.2025.03.16 val PER: 0.1859
2026-01-03 11:56:25,876: t15.2025.03.30 val PER: 0.2977
2026-01-03 11:56:25,877: t15.2025.04.13 val PER: 0.2040
2026-01-03 11:56:25,878: New best val WER(1gram) 46.19% --> 45.18%
2026-01-03 11:56:25,878: Checkpointing model
2026-01-03 11:56:26,138: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/best_checkpoint
2026-01-03 11:56:26,414: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_15500
2026-01-03 11:56:34,410: Train batch 15600: loss: 11.50 grad norm: 55.48 time: 0.062
2026-01-03 11:56:50,580: Train batch 15800: loss: 13.57 grad norm: 62.98 time: 0.066
2026-01-03 11:57:06,883: Train batch 16000: loss: 8.57 grad norm: 45.04 time: 0.055
2026-01-03 11:57:06,883: Running test after training batch: 16000
2026-01-03 11:57:07,012: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:57:11,700: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 11:57:11,734: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 11:57:13,587: Val batch 16000: PER (avg): 0.1490 CTC Loss (avg): 15.3830 WER(1gram): 46.95% (n=64) time: 6.704
2026-01-03 11:57:13,588: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-03 11:57:13,588: t15.2023.08.13 val PER: 0.1091
2026-01-03 11:57:13,588: t15.2023.08.18 val PER: 0.1048
2026-01-03 11:57:13,588: t15.2023.08.20 val PER: 0.1096
2026-01-03 11:57:13,588: t15.2023.08.25 val PER: 0.0934
2026-01-03 11:57:13,588: t15.2023.08.27 val PER: 0.1801
2026-01-03 11:57:13,588: t15.2023.09.01 val PER: 0.0763
2026-01-03 11:57:13,588: t15.2023.09.03 val PER: 0.1556
2026-01-03 11:57:13,588: t15.2023.09.24 val PER: 0.1299
2026-01-03 11:57:13,589: t15.2023.09.29 val PER: 0.1327
2026-01-03 11:57:13,589: t15.2023.10.01 val PER: 0.1744
2026-01-03 11:57:13,589: t15.2023.10.06 val PER: 0.0893
2026-01-03 11:57:13,589: t15.2023.10.08 val PER: 0.2544
2026-01-03 11:57:13,589: t15.2023.10.13 val PER: 0.1978
2026-01-03 11:57:13,589: t15.2023.10.15 val PER: 0.1523
2026-01-03 11:57:13,589: t15.2023.10.20 val PER: 0.1879
2026-01-03 11:57:13,589: t15.2023.10.22 val PER: 0.1102
2026-01-03 11:57:13,589: t15.2023.11.03 val PER: 0.1771
2026-01-03 11:57:13,589: t15.2023.11.04 val PER: 0.0375
2026-01-03 11:57:13,589: t15.2023.11.17 val PER: 0.0404
2026-01-03 11:57:13,589: t15.2023.11.19 val PER: 0.0399
2026-01-03 11:57:13,589: t15.2023.11.26 val PER: 0.1181
2026-01-03 11:57:13,589: t15.2023.12.03 val PER: 0.1155
2026-01-03 11:57:13,589: t15.2023.12.08 val PER: 0.0939
2026-01-03 11:57:13,590: t15.2023.12.10 val PER: 0.0907
2026-01-03 11:57:13,590: t15.2023.12.17 val PER: 0.1351
2026-01-03 11:57:13,590: t15.2023.12.29 val PER: 0.1290
2026-01-03 11:57:13,590: t15.2024.02.25 val PER: 0.1011
2026-01-03 11:57:13,590: t15.2024.03.08 val PER: 0.2262
2026-01-03 11:57:13,590: t15.2024.03.15 val PER: 0.1945
2026-01-03 11:57:13,590: t15.2024.03.17 val PER: 0.1374
2026-01-03 11:57:13,590: t15.2024.05.10 val PER: 0.1516
2026-01-03 11:57:13,590: t15.2024.06.14 val PER: 0.1609
2026-01-03 11:57:13,590: t15.2024.07.19 val PER: 0.2353
2026-01-03 11:57:13,590: t15.2024.07.21 val PER: 0.0979
2026-01-03 11:57:13,590: t15.2024.07.28 val PER: 0.1368
2026-01-03 11:57:13,590: t15.2025.01.10 val PER: 0.2906
2026-01-03 11:57:13,590: t15.2025.01.12 val PER: 0.1393
2026-01-03 11:57:13,591: t15.2025.03.14 val PER: 0.3388
2026-01-03 11:57:13,591: t15.2025.03.16 val PER: 0.1754
2026-01-03 11:57:13,591: t15.2025.03.30 val PER: 0.2954
2026-01-03 11:57:13,591: t15.2025.04.13 val PER: 0.2126
2026-01-03 11:57:13,856: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_16000
2026-01-03 11:57:29,875: Train batch 16200: loss: 6.07 grad norm: 45.17 time: 0.055
2026-01-03 11:57:45,894: Train batch 16400: loss: 10.81 grad norm: 65.46 time: 0.058
2026-01-03 11:57:53,947: Running test after training batch: 16500
2026-01-03 11:57:54,044: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:57:58,716: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 11:57:58,749: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 11:58:00,582: Val batch 16500: PER (avg): 0.1485 CTC Loss (avg): 15.2435 WER(1gram): 45.43% (n=64) time: 6.635
2026-01-03 11:58:00,583: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 11:58:00,583: t15.2023.08.13 val PER: 0.1091
2026-01-03 11:58:00,583: t15.2023.08.18 val PER: 0.0981
2026-01-03 11:58:00,583: t15.2023.08.20 val PER: 0.1080
2026-01-03 11:58:00,583: t15.2023.08.25 val PER: 0.0889
2026-01-03 11:58:00,583: t15.2023.08.27 val PER: 0.1752
2026-01-03 11:58:00,583: t15.2023.09.01 val PER: 0.0820
2026-01-03 11:58:00,583: t15.2023.09.03 val PER: 0.1544
2026-01-03 11:58:00,583: t15.2023.09.24 val PER: 0.1274
2026-01-03 11:58:00,583: t15.2023.09.29 val PER: 0.1321
2026-01-03 11:58:00,583: t15.2023.10.01 val PER: 0.1770
2026-01-03 11:58:00,583: t15.2023.10.06 val PER: 0.0829
2026-01-03 11:58:00,583: t15.2023.10.08 val PER: 0.2476
2026-01-03 11:58:00,584: t15.2023.10.13 val PER: 0.2002
2026-01-03 11:58:00,584: t15.2023.10.15 val PER: 0.1457
2026-01-03 11:58:00,584: t15.2023.10.20 val PER: 0.1879
2026-01-03 11:58:00,584: t15.2023.10.22 val PER: 0.1080
2026-01-03 11:58:00,585: t15.2023.11.03 val PER: 0.1757
2026-01-03 11:58:00,585: t15.2023.11.04 val PER: 0.0375
2026-01-03 11:58:00,585: t15.2023.11.17 val PER: 0.0404
2026-01-03 11:58:00,585: t15.2023.11.19 val PER: 0.0439
2026-01-03 11:58:00,585: t15.2023.11.26 val PER: 0.1167
2026-01-03 11:58:00,585: t15.2023.12.03 val PER: 0.1134
2026-01-03 11:58:00,586: t15.2023.12.08 val PER: 0.0972
2026-01-03 11:58:00,586: t15.2023.12.10 val PER: 0.0946
2026-01-03 11:58:00,586: t15.2023.12.17 val PER: 0.1310
2026-01-03 11:58:00,586: t15.2023.12.29 val PER: 0.1256
2026-01-03 11:58:00,586: t15.2024.02.25 val PER: 0.1011
2026-01-03 11:58:00,586: t15.2024.03.08 val PER: 0.2176
2026-01-03 11:58:00,586: t15.2024.03.15 val PER: 0.1995
2026-01-03 11:58:00,586: t15.2024.03.17 val PER: 0.1402
2026-01-03 11:58:00,586: t15.2024.05.10 val PER: 0.1575
2026-01-03 11:58:00,587: t15.2024.06.14 val PER: 0.1656
2026-01-03 11:58:00,587: t15.2024.07.19 val PER: 0.2439
2026-01-03 11:58:00,587: t15.2024.07.21 val PER: 0.0917
2026-01-03 11:58:00,587: t15.2024.07.28 val PER: 0.1324
2026-01-03 11:58:00,587: t15.2025.01.10 val PER: 0.2865
2026-01-03 11:58:00,587: t15.2025.01.12 val PER: 0.1363
2026-01-03 11:58:00,587: t15.2025.03.14 val PER: 0.3536
2026-01-03 11:58:00,587: t15.2025.03.16 val PER: 0.1859
2026-01-03 11:58:00,587: t15.2025.03.30 val PER: 0.2954
2026-01-03 11:58:00,588: t15.2025.04.13 val PER: 0.2026
2026-01-03 11:58:00,852: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_16500
2026-01-03 11:58:08,891: Train batch 16600: loss: 8.61 grad norm: 53.94 time: 0.052
2026-01-03 11:58:24,923: Train batch 16800: loss: 16.42 grad norm: 71.42 time: 0.060
2026-01-03 11:58:40,890: Train batch 17000: loss: 7.98 grad norm: 47.66 time: 0.081
2026-01-03 11:58:40,890: Running test after training batch: 17000
2026-01-03 11:58:41,003: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:58:45,662: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 11:58:45,695: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-03 11:58:47,581: Val batch 17000: PER (avg): 0.1471 CTC Loss (avg): 15.0223 WER(1gram): 46.45% (n=64) time: 6.690
2026-01-03 11:58:47,581: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=11
2026-01-03 11:58:47,581: t15.2023.08.13 val PER: 0.1060
2026-01-03 11:58:47,581: t15.2023.08.18 val PER: 0.1065
2026-01-03 11:58:47,581: t15.2023.08.20 val PER: 0.1017
2026-01-03 11:58:47,581: t15.2023.08.25 val PER: 0.0828
2026-01-03 11:58:47,581: t15.2023.08.27 val PER: 0.1801
2026-01-03 11:58:47,582: t15.2023.09.01 val PER: 0.0763
2026-01-03 11:58:47,582: t15.2023.09.03 val PER: 0.1580
2026-01-03 11:58:47,582: t15.2023.09.24 val PER: 0.1274
2026-01-03 11:58:47,582: t15.2023.09.29 val PER: 0.1289
2026-01-03 11:58:47,582: t15.2023.10.01 val PER: 0.1691
2026-01-03 11:58:47,582: t15.2023.10.06 val PER: 0.0807
2026-01-03 11:58:47,582: t15.2023.10.08 val PER: 0.2382
2026-01-03 11:58:47,582: t15.2023.10.13 val PER: 0.1947
2026-01-03 11:58:47,582: t15.2023.10.15 val PER: 0.1516
2026-01-03 11:58:47,582: t15.2023.10.20 val PER: 0.1812
2026-01-03 11:58:47,582: t15.2023.10.22 val PER: 0.1091
2026-01-03 11:58:47,582: t15.2023.11.03 val PER: 0.1777
2026-01-03 11:58:47,582: t15.2023.11.04 val PER: 0.0307
2026-01-03 11:58:47,583: t15.2023.11.17 val PER: 0.0420
2026-01-03 11:58:47,583: t15.2023.11.19 val PER: 0.0399
2026-01-03 11:58:47,583: t15.2023.11.26 val PER: 0.1159
2026-01-03 11:58:47,583: t15.2023.12.03 val PER: 0.1092
2026-01-03 11:58:47,583: t15.2023.12.08 val PER: 0.0965
2026-01-03 11:58:47,583: t15.2023.12.10 val PER: 0.0907
2026-01-03 11:58:47,583: t15.2023.12.17 val PER: 0.1351
2026-01-03 11:58:47,583: t15.2023.12.29 val PER: 0.1277
2026-01-03 11:58:47,583: t15.2024.02.25 val PER: 0.1124
2026-01-03 11:58:47,583: t15.2024.03.08 val PER: 0.2162
2026-01-03 11:58:47,583: t15.2024.03.15 val PER: 0.2014
2026-01-03 11:58:47,583: t15.2024.03.17 val PER: 0.1437
2026-01-03 11:58:47,583: t15.2024.05.10 val PER: 0.1545
2026-01-03 11:58:47,584: t15.2024.06.14 val PER: 0.1577
2026-01-03 11:58:47,584: t15.2024.07.19 val PER: 0.2340
2026-01-03 11:58:47,584: t15.2024.07.21 val PER: 0.0890
2026-01-03 11:58:47,584: t15.2024.07.28 val PER: 0.1324
2026-01-03 11:58:47,584: t15.2025.01.10 val PER: 0.2920
2026-01-03 11:58:47,584: t15.2025.01.12 val PER: 0.1332
2026-01-03 11:58:47,584: t15.2025.03.14 val PER: 0.3550
2026-01-03 11:58:47,584: t15.2025.03.16 val PER: 0.1741
2026-01-03 11:58:47,584: t15.2025.03.30 val PER: 0.2885
2026-01-03 11:58:47,584: t15.2025.04.13 val PER: 0.2026
2026-01-03 11:58:47,855: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_17000
2026-01-03 11:59:03,699: Train batch 17200: loss: 9.60 grad norm: 50.40 time: 0.083
2026-01-03 11:59:19,760: Train batch 17400: loss: 12.04 grad norm: 60.93 time: 0.070
2026-01-03 11:59:27,634: Running test after training batch: 17500
2026-01-03 11:59:27,756: WER debug GT example: You can see the code at this point as well.
2026-01-03 11:59:32,439: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 11:59:32,474: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 11:59:34,370: Val batch 17500: PER (avg): 0.1477 CTC Loss (avg): 15.0790 WER(1gram): 46.45% (n=64) time: 6.736
2026-01-03 11:59:34,370: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-03 11:59:34,370: t15.2023.08.13 val PER: 0.1123
2026-01-03 11:59:34,370: t15.2023.08.18 val PER: 0.1006
2026-01-03 11:59:34,371: t15.2023.08.20 val PER: 0.1080
2026-01-03 11:59:34,371: t15.2023.08.25 val PER: 0.0889
2026-01-03 11:59:34,371: t15.2023.08.27 val PER: 0.1833
2026-01-03 11:59:34,371: t15.2023.09.01 val PER: 0.0771
2026-01-03 11:59:34,371: t15.2023.09.03 val PER: 0.1532
2026-01-03 11:59:34,371: t15.2023.09.24 val PER: 0.1299
2026-01-03 11:59:34,371: t15.2023.09.29 val PER: 0.1321
2026-01-03 11:59:34,371: t15.2023.10.01 val PER: 0.1777
2026-01-03 11:59:34,371: t15.2023.10.06 val PER: 0.0807
2026-01-03 11:59:34,371: t15.2023.10.08 val PER: 0.2558
2026-01-03 11:59:34,371: t15.2023.10.13 val PER: 0.1924
2026-01-03 11:59:34,371: t15.2023.10.15 val PER: 0.1463
2026-01-03 11:59:34,371: t15.2023.10.20 val PER: 0.1879
2026-01-03 11:59:34,371: t15.2023.10.22 val PER: 0.1047
2026-01-03 11:59:34,371: t15.2023.11.03 val PER: 0.1757
2026-01-03 11:59:34,372: t15.2023.11.04 val PER: 0.0307
2026-01-03 11:59:34,372: t15.2023.11.17 val PER: 0.0373
2026-01-03 11:59:34,372: t15.2023.11.19 val PER: 0.0419
2026-01-03 11:59:34,372: t15.2023.11.26 val PER: 0.1130
2026-01-03 11:59:34,372: t15.2023.12.03 val PER: 0.1092
2026-01-03 11:59:34,372: t15.2023.12.08 val PER: 0.1025
2026-01-03 11:59:34,372: t15.2023.12.10 val PER: 0.0907
2026-01-03 11:59:34,372: t15.2023.12.17 val PER: 0.1351
2026-01-03 11:59:34,372: t15.2023.12.29 val PER: 0.1256
2026-01-03 11:59:34,372: t15.2024.02.25 val PER: 0.1025
2026-01-03 11:59:34,372: t15.2024.03.08 val PER: 0.2148
2026-01-03 11:59:34,372: t15.2024.03.15 val PER: 0.1982
2026-01-03 11:59:34,372: t15.2024.03.17 val PER: 0.1388
2026-01-03 11:59:34,373: t15.2024.05.10 val PER: 0.1545
2026-01-03 11:59:34,373: t15.2024.06.14 val PER: 0.1577
2026-01-03 11:59:34,373: t15.2024.07.19 val PER: 0.2327
2026-01-03 11:59:34,373: t15.2024.07.21 val PER: 0.0897
2026-01-03 11:59:34,373: t15.2024.07.28 val PER: 0.1301
2026-01-03 11:59:34,373: t15.2025.01.10 val PER: 0.2975
2026-01-03 11:59:34,373: t15.2025.01.12 val PER: 0.1355
2026-01-03 11:59:34,373: t15.2025.03.14 val PER: 0.3550
2026-01-03 11:59:34,373: t15.2025.03.16 val PER: 0.1819
2026-01-03 11:59:34,373: t15.2025.03.30 val PER: 0.2966
2026-01-03 11:59:34,373: t15.2025.04.13 val PER: 0.2026
2026-01-03 11:59:34,642: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_17500
2026-01-03 11:59:48,660: Train batch 17600: loss: 9.54 grad norm: 53.78 time: 0.051
2026-01-03 12:00:05,623: Train batch 17800: loss: 6.49 grad norm: 52.12 time: 0.042
2026-01-03 12:00:21,751: Train batch 18000: loss: 11.26 grad norm: 64.08 time: 0.061
2026-01-03 12:00:21,751: Running test after training batch: 18000
2026-01-03 12:00:21,872: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:00:27,025: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:00:27,059: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 12:00:28,955: Val batch 18000: PER (avg): 0.1469 CTC Loss (avg): 15.0249 WER(1gram): 46.45% (n=64) time: 7.204
2026-01-03 12:00:28,955: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 12:00:28,955: t15.2023.08.13 val PER: 0.1050
2026-01-03 12:00:28,955: t15.2023.08.18 val PER: 0.1006
2026-01-03 12:00:28,955: t15.2023.08.20 val PER: 0.1001
2026-01-03 12:00:28,956: t15.2023.08.25 val PER: 0.0889
2026-01-03 12:00:28,956: t15.2023.08.27 val PER: 0.1768
2026-01-03 12:00:28,956: t15.2023.09.01 val PER: 0.0763
2026-01-03 12:00:28,956: t15.2023.09.03 val PER: 0.1532
2026-01-03 12:00:28,956: t15.2023.09.24 val PER: 0.1299
2026-01-03 12:00:28,956: t15.2023.09.29 val PER: 0.1327
2026-01-03 12:00:28,956: t15.2023.10.01 val PER: 0.1757
2026-01-03 12:00:28,956: t15.2023.10.06 val PER: 0.0786
2026-01-03 12:00:28,956: t15.2023.10.08 val PER: 0.2436
2026-01-03 12:00:28,956: t15.2023.10.13 val PER: 0.1901
2026-01-03 12:00:28,956: t15.2023.10.15 val PER: 0.1529
2026-01-03 12:00:28,956: t15.2023.10.20 val PER: 0.1980
2026-01-03 12:00:28,956: t15.2023.10.22 val PER: 0.1058
2026-01-03 12:00:28,956: t15.2023.11.03 val PER: 0.1764
2026-01-03 12:00:28,956: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:00:28,957: t15.2023.11.17 val PER: 0.0358
2026-01-03 12:00:28,957: t15.2023.11.19 val PER: 0.0379
2026-01-03 12:00:28,957: t15.2023.11.26 val PER: 0.1159
2026-01-03 12:00:28,957: t15.2023.12.03 val PER: 0.1082
2026-01-03 12:00:28,957: t15.2023.12.08 val PER: 0.1019
2026-01-03 12:00:28,957: t15.2023.12.10 val PER: 0.0867
2026-01-03 12:00:28,957: t15.2023.12.17 val PER: 0.1403
2026-01-03 12:00:28,957: t15.2023.12.29 val PER: 0.1215
2026-01-03 12:00:28,957: t15.2024.02.25 val PER: 0.1025
2026-01-03 12:00:28,957: t15.2024.03.08 val PER: 0.2262
2026-01-03 12:00:28,957: t15.2024.03.15 val PER: 0.1957
2026-01-03 12:00:28,957: t15.2024.03.17 val PER: 0.1388
2026-01-03 12:00:28,957: t15.2024.05.10 val PER: 0.1471
2026-01-03 12:00:28,957: t15.2024.06.14 val PER: 0.1530
2026-01-03 12:00:28,958: t15.2024.07.19 val PER: 0.2334
2026-01-03 12:00:28,958: t15.2024.07.21 val PER: 0.0890
2026-01-03 12:00:28,958: t15.2024.07.28 val PER: 0.1316
2026-01-03 12:00:28,958: t15.2025.01.10 val PER: 0.2934
2026-01-03 12:00:28,958: t15.2025.01.12 val PER: 0.1409
2026-01-03 12:00:28,958: t15.2025.03.14 val PER: 0.3536
2026-01-03 12:00:28,958: t15.2025.03.16 val PER: 0.1806
2026-01-03 12:00:28,958: t15.2025.03.30 val PER: 0.2897
2026-01-03 12:00:28,959: t15.2025.04.13 val PER: 0.2040
2026-01-03 12:00:29,216: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_18000
2026-01-03 12:00:45,436: Train batch 18200: loss: 7.65 grad norm: 47.64 time: 0.074
2026-01-03 12:01:01,488: Train batch 18400: loss: 5.02 grad norm: 39.46 time: 0.057
2026-01-03 12:01:09,818: Running test after training batch: 18500
2026-01-03 12:01:09,961: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:01:14,673: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:01:14,708: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 12:01:16,618: Val batch 18500: PER (avg): 0.1476 CTC Loss (avg): 15.0511 WER(1gram): 46.45% (n=64) time: 6.800
2026-01-03 12:01:16,619: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-03 12:01:16,619: t15.2023.08.13 val PER: 0.1071
2026-01-03 12:01:16,619: t15.2023.08.18 val PER: 0.1006
2026-01-03 12:01:16,619: t15.2023.08.20 val PER: 0.1088
2026-01-03 12:01:16,619: t15.2023.08.25 val PER: 0.0919
2026-01-03 12:01:16,619: t15.2023.08.27 val PER: 0.1736
2026-01-03 12:01:16,619: t15.2023.09.01 val PER: 0.0763
2026-01-03 12:01:16,619: t15.2023.09.03 val PER: 0.1556
2026-01-03 12:01:16,619: t15.2023.09.24 val PER: 0.1311
2026-01-03 12:01:16,619: t15.2023.09.29 val PER: 0.1295
2026-01-03 12:01:16,620: t15.2023.10.01 val PER: 0.1744
2026-01-03 12:01:16,620: t15.2023.10.06 val PER: 0.0786
2026-01-03 12:01:16,620: t15.2023.10.08 val PER: 0.2463
2026-01-03 12:01:16,620: t15.2023.10.13 val PER: 0.1939
2026-01-03 12:01:16,620: t15.2023.10.15 val PER: 0.1516
2026-01-03 12:01:16,620: t15.2023.10.20 val PER: 0.1913
2026-01-03 12:01:16,620: t15.2023.10.22 val PER: 0.1047
2026-01-03 12:01:16,620: t15.2023.11.03 val PER: 0.1764
2026-01-03 12:01:16,620: t15.2023.11.04 val PER: 0.0341
2026-01-03 12:01:16,620: t15.2023.11.17 val PER: 0.0404
2026-01-03 12:01:16,620: t15.2023.11.19 val PER: 0.0399
2026-01-03 12:01:16,620: t15.2023.11.26 val PER: 0.1159
2026-01-03 12:01:16,620: t15.2023.12.03 val PER: 0.1103
2026-01-03 12:01:16,620: t15.2023.12.08 val PER: 0.0965
2026-01-03 12:01:16,620: t15.2023.12.10 val PER: 0.0986
2026-01-03 12:01:16,620: t15.2023.12.17 val PER: 0.1362
2026-01-03 12:01:16,621: t15.2023.12.29 val PER: 0.1270
2026-01-03 12:01:16,621: t15.2024.02.25 val PER: 0.0997
2026-01-03 12:01:16,621: t15.2024.03.08 val PER: 0.2162
2026-01-03 12:01:16,621: t15.2024.03.15 val PER: 0.2008
2026-01-03 12:01:16,621: t15.2024.03.17 val PER: 0.1395
2026-01-03 12:01:16,621: t15.2024.05.10 val PER: 0.1545
2026-01-03 12:01:16,621: t15.2024.06.14 val PER: 0.1593
2026-01-03 12:01:16,621: t15.2024.07.19 val PER: 0.2360
2026-01-03 12:01:16,621: t15.2024.07.21 val PER: 0.0848
2026-01-03 12:01:16,621: t15.2024.07.28 val PER: 0.1301
2026-01-03 12:01:16,621: t15.2025.01.10 val PER: 0.2989
2026-01-03 12:01:16,621: t15.2025.01.12 val PER: 0.1409
2026-01-03 12:01:16,621: t15.2025.03.14 val PER: 0.3536
2026-01-03 12:01:16,621: t15.2025.03.16 val PER: 0.1819
2026-01-03 12:01:16,621: t15.2025.03.30 val PER: 0.2851
2026-01-03 12:01:16,621: t15.2025.04.13 val PER: 0.2068
2026-01-03 12:01:16,909: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_18500
2026-01-03 12:01:25,397: Train batch 18600: loss: 12.60 grad norm: 62.20 time: 0.066
2026-01-03 12:01:42,154: Train batch 18800: loss: 8.48 grad norm: 50.54 time: 0.064
2026-01-03 12:01:58,405: Train batch 19000: loss: 8.45 grad norm: 44.61 time: 0.063
2026-01-03 12:01:58,406: Running test after training batch: 19000
2026-01-03 12:01:58,517: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:02:03,781: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:02:03,816: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 12:02:05,738: Val batch 19000: PER (avg): 0.1467 CTC Loss (avg): 15.0802 WER(1gram): 46.45% (n=64) time: 7.332
2026-01-03 12:02:05,738: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 12:02:05,738: t15.2023.08.13 val PER: 0.1060
2026-01-03 12:02:05,739: t15.2023.08.18 val PER: 0.1006
2026-01-03 12:02:05,739: t15.2023.08.20 val PER: 0.1072
2026-01-03 12:02:05,739: t15.2023.08.25 val PER: 0.0949
2026-01-03 12:02:05,739: t15.2023.08.27 val PER: 0.1720
2026-01-03 12:02:05,739: t15.2023.09.01 val PER: 0.0731
2026-01-03 12:02:05,739: t15.2023.09.03 val PER: 0.1544
2026-01-03 12:02:05,739: t15.2023.09.24 val PER: 0.1323
2026-01-03 12:02:05,739: t15.2023.09.29 val PER: 0.1270
2026-01-03 12:02:05,739: t15.2023.10.01 val PER: 0.1757
2026-01-03 12:02:05,739: t15.2023.10.06 val PER: 0.0850
2026-01-03 12:02:05,739: t15.2023.10.08 val PER: 0.2449
2026-01-03 12:02:05,740: t15.2023.10.13 val PER: 0.1916
2026-01-03 12:02:05,740: t15.2023.10.15 val PER: 0.1477
2026-01-03 12:02:05,740: t15.2023.10.20 val PER: 0.1946
2026-01-03 12:02:05,740: t15.2023.10.22 val PER: 0.1013
2026-01-03 12:02:05,740: t15.2023.11.03 val PER: 0.1805
2026-01-03 12:02:05,740: t15.2023.11.04 val PER: 0.0341
2026-01-03 12:02:05,740: t15.2023.11.17 val PER: 0.0373
2026-01-03 12:02:05,740: t15.2023.11.19 val PER: 0.0379
2026-01-03 12:02:05,740: t15.2023.11.26 val PER: 0.1123
2026-01-03 12:02:05,741: t15.2023.12.03 val PER: 0.1113
2026-01-03 12:02:05,741: t15.2023.12.08 val PER: 0.0952
2026-01-03 12:02:05,741: t15.2023.12.10 val PER: 0.0894
2026-01-03 12:02:05,741: t15.2023.12.17 val PER: 0.1362
2026-01-03 12:02:05,741: t15.2023.12.29 val PER: 0.1256
2026-01-03 12:02:05,741: t15.2024.02.25 val PER: 0.1053
2026-01-03 12:02:05,741: t15.2024.03.08 val PER: 0.2191
2026-01-03 12:02:05,741: t15.2024.03.15 val PER: 0.1964
2026-01-03 12:02:05,741: t15.2024.03.17 val PER: 0.1402
2026-01-03 12:02:05,741: t15.2024.05.10 val PER: 0.1501
2026-01-03 12:02:05,741: t15.2024.06.14 val PER: 0.1640
2026-01-03 12:02:05,741: t15.2024.07.19 val PER: 0.2393
2026-01-03 12:02:05,741: t15.2024.07.21 val PER: 0.0903
2026-01-03 12:02:05,741: t15.2024.07.28 val PER: 0.1279
2026-01-03 12:02:05,741: t15.2025.01.10 val PER: 0.2893
2026-01-03 12:02:05,741: t15.2025.01.12 val PER: 0.1339
2026-01-03 12:02:05,742: t15.2025.03.14 val PER: 0.3521
2026-01-03 12:02:05,742: t15.2025.03.16 val PER: 0.1767
2026-01-03 12:02:05,742: t15.2025.03.30 val PER: 0.2816
2026-01-03 12:02:05,742: t15.2025.04.13 val PER: 0.2126
2026-01-03 12:02:06,103: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_19000
2026-01-03 12:02:22,420: Train batch 19200: loss: 5.87 grad norm: 46.86 time: 0.063
2026-01-03 12:02:38,664: Train batch 19400: loss: 4.88 grad norm: 36.47 time: 0.052
2026-01-03 12:02:46,725: Running test after training batch: 19500
2026-01-03 12:02:46,851: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:02:52,006: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:02:52,042: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 12:02:53,976: Val batch 19500: PER (avg): 0.1475 CTC Loss (avg): 15.0070 WER(1gram): 46.45% (n=64) time: 7.250
2026-01-03 12:02:53,976: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 12:02:53,976: t15.2023.08.13 val PER: 0.1091
2026-01-03 12:02:53,976: t15.2023.08.18 val PER: 0.1014
2026-01-03 12:02:53,977: t15.2023.08.20 val PER: 0.1104
2026-01-03 12:02:53,977: t15.2023.08.25 val PER: 0.0904
2026-01-03 12:02:53,977: t15.2023.08.27 val PER: 0.1785
2026-01-03 12:02:53,977: t15.2023.09.01 val PER: 0.0763
2026-01-03 12:02:53,977: t15.2023.09.03 val PER: 0.1532
2026-01-03 12:02:53,977: t15.2023.09.24 val PER: 0.1335
2026-01-03 12:02:53,977: t15.2023.09.29 val PER: 0.1295
2026-01-03 12:02:53,977: t15.2023.10.01 val PER: 0.1797
2026-01-03 12:02:53,977: t15.2023.10.06 val PER: 0.0829
2026-01-03 12:02:53,977: t15.2023.10.08 val PER: 0.2517
2026-01-03 12:02:53,977: t15.2023.10.13 val PER: 0.1971
2026-01-03 12:02:53,977: t15.2023.10.15 val PER: 0.1503
2026-01-03 12:02:53,978: t15.2023.10.20 val PER: 0.1946
2026-01-03 12:02:53,978: t15.2023.10.22 val PER: 0.1047
2026-01-03 12:02:53,978: t15.2023.11.03 val PER: 0.1771
2026-01-03 12:02:53,978: t15.2023.11.04 val PER: 0.0341
2026-01-03 12:02:53,978: t15.2023.11.17 val PER: 0.0389
2026-01-03 12:02:53,978: t15.2023.11.19 val PER: 0.0399
2026-01-03 12:02:53,978: t15.2023.11.26 val PER: 0.1116
2026-01-03 12:02:53,978: t15.2023.12.03 val PER: 0.1050
2026-01-03 12:02:53,978: t15.2023.12.08 val PER: 0.0972
2026-01-03 12:02:53,978: t15.2023.12.10 val PER: 0.0854
2026-01-03 12:02:53,978: t15.2023.12.17 val PER: 0.1372
2026-01-03 12:02:53,978: t15.2023.12.29 val PER: 0.1311
2026-01-03 12:02:53,978: t15.2024.02.25 val PER: 0.1011
2026-01-03 12:02:53,978: t15.2024.03.08 val PER: 0.2148
2026-01-03 12:02:53,978: t15.2024.03.15 val PER: 0.1945
2026-01-03 12:02:53,978: t15.2024.03.17 val PER: 0.1353
2026-01-03 12:02:53,978: t15.2024.05.10 val PER: 0.1516
2026-01-03 12:02:53,978: t15.2024.06.14 val PER: 0.1546
2026-01-03 12:02:53,979: t15.2024.07.19 val PER: 0.2360
2026-01-03 12:02:53,979: t15.2024.07.21 val PER: 0.0876
2026-01-03 12:02:53,979: t15.2024.07.28 val PER: 0.1309
2026-01-03 12:02:53,979: t15.2025.01.10 val PER: 0.2989
2026-01-03 12:02:53,979: t15.2025.01.12 val PER: 0.1424
2026-01-03 12:02:53,979: t15.2025.03.14 val PER: 0.3550
2026-01-03 12:02:53,979: t15.2025.03.16 val PER: 0.1819
2026-01-03 12:02:53,979: t15.2025.03.30 val PER: 0.2759
2026-01-03 12:02:53,979: t15.2025.04.13 val PER: 0.2111
2026-01-03 12:02:54,287: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_19500
2026-01-03 12:03:02,451: Train batch 19600: loss: 7.82 grad norm: 45.98 time: 0.057
2026-01-03 12:03:19,386: Train batch 19800: loss: 7.19 grad norm: 51.44 time: 0.055
2026-01-03 12:03:35,788: Running test after training batch: 19999
2026-01-03 12:03:35,881: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:03:41,169: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:03:41,204: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 12:03:43,155: Val batch 19999: PER (avg): 0.1466 CTC Loss (avg): 15.0250 WER(1gram): 46.70% (n=64) time: 7.367
2026-01-03 12:03:43,155: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-03 12:03:43,155: t15.2023.08.13 val PER: 0.1050
2026-01-03 12:03:43,155: t15.2023.08.18 val PER: 0.1006
2026-01-03 12:03:43,156: t15.2023.08.20 val PER: 0.1104
2026-01-03 12:03:43,156: t15.2023.08.25 val PER: 0.0889
2026-01-03 12:03:43,156: t15.2023.08.27 val PER: 0.1736
2026-01-03 12:03:43,156: t15.2023.09.01 val PER: 0.0755
2026-01-03 12:03:43,156: t15.2023.09.03 val PER: 0.1461
2026-01-03 12:03:43,156: t15.2023.09.24 val PER: 0.1311
2026-01-03 12:03:43,156: t15.2023.09.29 val PER: 0.1295
2026-01-03 12:03:43,156: t15.2023.10.01 val PER: 0.1777
2026-01-03 12:03:43,156: t15.2023.10.06 val PER: 0.0807
2026-01-03 12:03:43,157: t15.2023.10.08 val PER: 0.2517
2026-01-03 12:03:43,157: t15.2023.10.13 val PER: 0.1971
2026-01-03 12:03:43,157: t15.2023.10.15 val PER: 0.1510
2026-01-03 12:03:43,157: t15.2023.10.20 val PER: 0.1846
2026-01-03 12:03:43,157: t15.2023.10.22 val PER: 0.1047
2026-01-03 12:03:43,157: t15.2023.11.03 val PER: 0.1805
2026-01-03 12:03:43,157: t15.2023.11.04 val PER: 0.0341
2026-01-03 12:03:43,157: t15.2023.11.17 val PER: 0.0373
2026-01-03 12:03:43,157: t15.2023.11.19 val PER: 0.0379
2026-01-03 12:03:43,157: t15.2023.11.26 val PER: 0.1065
2026-01-03 12:03:43,157: t15.2023.12.03 val PER: 0.1082
2026-01-03 12:03:43,158: t15.2023.12.08 val PER: 0.0959
2026-01-03 12:03:43,158: t15.2023.12.10 val PER: 0.0907
2026-01-03 12:03:43,158: t15.2023.12.17 val PER: 0.1393
2026-01-03 12:03:43,158: t15.2023.12.29 val PER: 0.1325
2026-01-03 12:03:43,158: t15.2024.02.25 val PER: 0.1039
2026-01-03 12:03:43,158: t15.2024.03.08 val PER: 0.2219
2026-01-03 12:03:43,158: t15.2024.03.15 val PER: 0.1889
2026-01-03 12:03:43,158: t15.2024.03.17 val PER: 0.1346
2026-01-03 12:03:43,158: t15.2024.05.10 val PER: 0.1456
2026-01-03 12:03:43,158: t15.2024.06.14 val PER: 0.1546
2026-01-03 12:03:43,158: t15.2024.07.19 val PER: 0.2380
2026-01-03 12:03:43,159: t15.2024.07.21 val PER: 0.0876
2026-01-03 12:03:43,159: t15.2024.07.28 val PER: 0.1309
2026-01-03 12:03:43,159: t15.2025.01.10 val PER: 0.2920
2026-01-03 12:03:43,159: t15.2025.01.12 val PER: 0.1386
2026-01-03 12:03:43,159: t15.2025.03.14 val PER: 0.3536
2026-01-03 12:03:43,159: t15.2025.03.16 val PER: 0.1793
2026-01-03 12:03:43,159: t15.2025.03.30 val PER: 0.2816
2026-01-03 12:03:43,159: t15.2025.04.13 val PER: 0.2040
2026-01-03 12:03:43,447: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/base/checkpoint/checkpoint_batch_19999
2026-01-03 12:03:43,511: Best avg val PER achieved: 0.14882
2026-01-03 12:03:43,512: Total training time: 32.61 minutes

=== RUN step10k_f01.yaml ===
2026-01-03 12:03:48,587: Using device: cuda:0
2026-01-03 12:03:51,188: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-03 12:03:51,214: Using 45 sessions after filtering (from 45).
2026-01-03 12:03:51,611: Using torch.compile (if available)
2026-01-03 12:03:51,612: torch.compile not available (torch<2.0). Skipping.
2026-01-03 12:03:51,612: Initialized RNN decoding model
2026-01-03 12:03:51,612: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 12:03:51,612: Model has 44,907,305 parameters
2026-01-03 12:03:51,612: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 12:03:52,911: Successfully initialized datasets
2026-01-03 12:03:52,912: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 12:03:53,959: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.199
2026-01-03 12:03:53,959: Running test after training batch: 0
2026-01-03 12:03:54,068: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:03:59,279: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-03 12:03:59,987: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-03 12:04:33,419: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 39.460
2026-01-03 12:04:33,420: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-03 12:04:33,420: t15.2023.08.13 val PER: 1.3056
2026-01-03 12:04:33,420: t15.2023.08.18 val PER: 1.4208
2026-01-03 12:04:33,420: t15.2023.08.20 val PER: 1.3002
2026-01-03 12:04:33,421: t15.2023.08.25 val PER: 1.3389
2026-01-03 12:04:33,421: t15.2023.08.27 val PER: 1.2460
2026-01-03 12:04:33,421: t15.2023.09.01 val PER: 1.4537
2026-01-03 12:04:33,421: t15.2023.09.03 val PER: 1.3171
2026-01-03 12:04:33,421: t15.2023.09.24 val PER: 1.5461
2026-01-03 12:04:33,421: t15.2023.09.29 val PER: 1.4671
2026-01-03 12:04:33,421: t15.2023.10.01 val PER: 1.2147
2026-01-03 12:04:33,421: t15.2023.10.06 val PER: 1.4876
2026-01-03 12:04:33,421: t15.2023.10.08 val PER: 1.1827
2026-01-03 12:04:33,421: t15.2023.10.13 val PER: 1.3964
2026-01-03 12:04:33,422: t15.2023.10.15 val PER: 1.3889
2026-01-03 12:04:33,422: t15.2023.10.20 val PER: 1.4866
2026-01-03 12:04:33,422: t15.2023.10.22 val PER: 1.3942
2026-01-03 12:04:33,422: t15.2023.11.03 val PER: 1.5923
2026-01-03 12:04:33,422: t15.2023.11.04 val PER: 2.0171
2026-01-03 12:04:33,422: t15.2023.11.17 val PER: 1.9518
2026-01-03 12:04:33,422: t15.2023.11.19 val PER: 1.6707
2026-01-03 12:04:33,422: t15.2023.11.26 val PER: 1.5413
2026-01-03 12:04:33,422: t15.2023.12.03 val PER: 1.4254
2026-01-03 12:04:33,422: t15.2023.12.08 val PER: 1.4487
2026-01-03 12:04:33,422: t15.2023.12.10 val PER: 1.6899
2026-01-03 12:04:33,423: t15.2023.12.17 val PER: 1.3077
2026-01-03 12:04:33,423: t15.2023.12.29 val PER: 1.4063
2026-01-03 12:04:33,423: t15.2024.02.25 val PER: 1.4228
2026-01-03 12:04:33,423: t15.2024.03.08 val PER: 1.3257
2026-01-03 12:04:33,423: t15.2024.03.15 val PER: 1.3196
2026-01-03 12:04:33,423: t15.2024.03.17 val PER: 1.4052
2026-01-03 12:04:33,423: t15.2024.05.10 val PER: 1.3224
2026-01-03 12:04:33,423: t15.2024.06.14 val PER: 1.5315
2026-01-03 12:04:33,423: t15.2024.07.19 val PER: 1.0817
2026-01-03 12:04:33,423: t15.2024.07.21 val PER: 1.6290
2026-01-03 12:04:33,423: t15.2024.07.28 val PER: 1.6588
2026-01-03 12:04:33,424: t15.2025.01.10 val PER: 1.0923
2026-01-03 12:04:33,424: t15.2025.01.12 val PER: 1.7629
2026-01-03 12:04:33,424: t15.2025.03.14 val PER: 1.0414
2026-01-03 12:04:33,424: t15.2025.03.16 val PER: 1.6257
2026-01-03 12:04:33,424: t15.2025.03.30 val PER: 1.2874
2026-01-03 12:04:33,424: t15.2025.04.13 val PER: 1.5949
2026-01-03 12:04:33,425: New best val WER(1gram) inf% --> 100.00%
2026-01-03 12:04:33,425: Checkpointing model
2026-01-03 12:04:33,669: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/best_checkpoint
2026-01-03 12:04:33,920: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_0
2026-01-03 12:04:51,866: Train batch 200: loss: 77.59 grad norm: 106.08 time: 0.054
2026-01-03 12:05:08,972: Train batch 400: loss: 53.40 grad norm: 86.32 time: 0.063
2026-01-03 12:05:17,452: Running test after training batch: 500
2026-01-03 12:05:17,587: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:05:22,943: WER debug example
  GT : you can see the code at this point as well
  PR : yule and ease thus uhde at this uhde is aisle
2026-01-03 12:05:22,976: WER debug example
  GT : how does it keep the cost down
  PR : houde does it yule thus as adz
2026-01-03 12:05:25,296: Val batch 500: PER (avg): 0.5160 CTC Loss (avg): 55.3671 WER(1gram): 88.58% (n=64) time: 7.844
2026-01-03 12:05:25,297: WER lens: avg_true_words=6.16 avg_pred_words=5.67 max_pred_words=12
2026-01-03 12:05:25,297: t15.2023.08.13 val PER: 0.4605
2026-01-03 12:05:25,297: t15.2023.08.18 val PER: 0.4552
2026-01-03 12:05:25,297: t15.2023.08.20 val PER: 0.4432
2026-01-03 12:05:25,297: t15.2023.08.25 val PER: 0.4307
2026-01-03 12:05:25,297: t15.2023.08.27 val PER: 0.5257
2026-01-03 12:05:25,297: t15.2023.09.01 val PER: 0.4148
2026-01-03 12:05:25,297: t15.2023.09.03 val PER: 0.4964
2026-01-03 12:05:25,297: t15.2023.09.24 val PER: 0.4296
2026-01-03 12:05:25,298: t15.2023.09.29 val PER: 0.4620
2026-01-03 12:05:25,298: t15.2023.10.01 val PER: 0.5159
2026-01-03 12:05:25,298: t15.2023.10.06 val PER: 0.4284
2026-01-03 12:05:25,298: t15.2023.10.08 val PER: 0.5386
2026-01-03 12:05:25,298: t15.2023.10.13 val PER: 0.5803
2026-01-03 12:05:25,298: t15.2023.10.15 val PER: 0.4931
2026-01-03 12:05:25,298: t15.2023.10.20 val PER: 0.4463
2026-01-03 12:05:25,298: t15.2023.10.22 val PER: 0.4465
2026-01-03 12:05:25,298: t15.2023.11.03 val PER: 0.5068
2026-01-03 12:05:25,298: t15.2023.11.04 val PER: 0.2696
2026-01-03 12:05:25,298: t15.2023.11.17 val PER: 0.3748
2026-01-03 12:05:25,298: t15.2023.11.19 val PER: 0.3313
2026-01-03 12:05:25,298: t15.2023.11.26 val PER: 0.5522
2026-01-03 12:05:25,298: t15.2023.12.03 val PER: 0.4884
2026-01-03 12:05:25,298: t15.2023.12.08 val PER: 0.5206
2026-01-03 12:05:25,299: t15.2023.12.10 val PER: 0.4442
2026-01-03 12:05:25,299: t15.2023.12.17 val PER: 0.5541
2026-01-03 12:05:25,299: t15.2023.12.29 val PER: 0.5326
2026-01-03 12:05:25,299: t15.2024.02.25 val PER: 0.4705
2026-01-03 12:05:25,299: t15.2024.03.08 val PER: 0.6287
2026-01-03 12:05:25,299: t15.2024.03.15 val PER: 0.5522
2026-01-03 12:05:25,299: t15.2024.03.17 val PER: 0.5000
2026-01-03 12:05:25,299: t15.2024.05.10 val PER: 0.5498
2026-01-03 12:05:25,299: t15.2024.06.14 val PER: 0.5047
2026-01-03 12:05:25,299: t15.2024.07.19 val PER: 0.6605
2026-01-03 12:05:25,299: t15.2024.07.21 val PER: 0.4766
2026-01-03 12:05:25,299: t15.2024.07.28 val PER: 0.5037
2026-01-03 12:05:25,299: t15.2025.01.10 val PER: 0.7424
2026-01-03 12:05:25,299: t15.2025.01.12 val PER: 0.5666
2026-01-03 12:05:25,299: t15.2025.03.14 val PER: 0.7219
2026-01-03 12:05:25,299: t15.2025.03.16 val PER: 0.5890
2026-01-03 12:05:25,300: t15.2025.03.30 val PER: 0.7299
2026-01-03 12:05:25,300: t15.2025.04.13 val PER: 0.5806
2026-01-03 12:05:25,301: New best val WER(1gram) 100.00% --> 88.58%
2026-01-03 12:05:25,301: Checkpointing model
2026-01-03 12:05:25,560: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/best_checkpoint
2026-01-03 12:05:25,813: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_500
2026-01-03 12:05:34,463: Train batch 600: loss: 49.47 grad norm: 79.60 time: 0.078
2026-01-03 12:05:51,527: Train batch 800: loss: 41.07 grad norm: 85.32 time: 0.057
2026-01-03 12:06:08,831: Train batch 1000: loss: 43.06 grad norm: 82.70 time: 0.067
2026-01-03 12:06:08,832: Running test after training batch: 1000
2026-01-03 12:06:08,967: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:06:13,726: WER debug example
  GT : you can see the code at this point as well
  PR : used ent ease thus owed it this uhde is while
2026-01-03 12:06:13,758: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus wass it
2026-01-03 12:06:15,548: Val batch 1000: PER (avg): 0.4101 CTC Loss (avg): 42.4962 WER(1gram): 82.74% (n=64) time: 6.716
2026-01-03 12:06:15,548: WER lens: avg_true_words=6.16 avg_pred_words=5.61 max_pred_words=12
2026-01-03 12:06:15,548: t15.2023.08.13 val PER: 0.3742
2026-01-03 12:06:15,548: t15.2023.08.18 val PER: 0.3370
2026-01-03 12:06:15,548: t15.2023.08.20 val PER: 0.3511
2026-01-03 12:06:15,548: t15.2023.08.25 val PER: 0.3117
2026-01-03 12:06:15,548: t15.2023.08.27 val PER: 0.4293
2026-01-03 12:06:15,548: t15.2023.09.01 val PER: 0.3101
2026-01-03 12:06:15,549: t15.2023.09.03 val PER: 0.4038
2026-01-03 12:06:15,549: t15.2023.09.24 val PER: 0.3289
2026-01-03 12:06:15,549: t15.2023.09.29 val PER: 0.3650
2026-01-03 12:06:15,549: t15.2023.10.01 val PER: 0.4069
2026-01-03 12:06:15,549: t15.2023.10.06 val PER: 0.3143
2026-01-03 12:06:15,549: t15.2023.10.08 val PER: 0.4601
2026-01-03 12:06:15,549: t15.2023.10.13 val PER: 0.4647
2026-01-03 12:06:15,549: t15.2023.10.15 val PER: 0.3837
2026-01-03 12:06:15,549: t15.2023.10.20 val PER: 0.3725
2026-01-03 12:06:15,550: t15.2023.10.22 val PER: 0.3563
2026-01-03 12:06:15,550: t15.2023.11.03 val PER: 0.4009
2026-01-03 12:06:15,550: t15.2023.11.04 val PER: 0.1741
2026-01-03 12:06:15,550: t15.2023.11.17 val PER: 0.2566
2026-01-03 12:06:15,550: t15.2023.11.19 val PER: 0.2076
2026-01-03 12:06:15,550: t15.2023.11.26 val PER: 0.4478
2026-01-03 12:06:15,550: t15.2023.12.03 val PER: 0.4034
2026-01-03 12:06:15,550: t15.2023.12.08 val PER: 0.4115
2026-01-03 12:06:15,550: t15.2023.12.10 val PER: 0.3482
2026-01-03 12:06:15,550: t15.2023.12.17 val PER: 0.4054
2026-01-03 12:06:15,550: t15.2023.12.29 val PER: 0.4152
2026-01-03 12:06:15,550: t15.2024.02.25 val PER: 0.3610
2026-01-03 12:06:15,550: t15.2024.03.08 val PER: 0.4922
2026-01-03 12:06:15,550: t15.2024.03.15 val PER: 0.4440
2026-01-03 12:06:15,550: t15.2024.03.17 val PER: 0.4052
2026-01-03 12:06:15,550: t15.2024.05.10 val PER: 0.4146
2026-01-03 12:06:15,550: t15.2024.06.14 val PER: 0.4022
2026-01-03 12:06:15,550: t15.2024.07.19 val PER: 0.5247
2026-01-03 12:06:15,551: t15.2024.07.21 val PER: 0.3717
2026-01-03 12:06:15,551: t15.2024.07.28 val PER: 0.4199
2026-01-03 12:06:15,551: t15.2025.01.10 val PER: 0.6198
2026-01-03 12:06:15,551: t15.2025.01.12 val PER: 0.4465
2026-01-03 12:06:15,551: t15.2025.03.14 val PER: 0.6450
2026-01-03 12:06:15,551: t15.2025.03.16 val PER: 0.4882
2026-01-03 12:06:15,551: t15.2025.03.30 val PER: 0.6471
2026-01-03 12:06:15,551: t15.2025.04.13 val PER: 0.4922
2026-01-03 12:06:15,552: New best val WER(1gram) 88.58% --> 82.74%
2026-01-03 12:06:15,552: Checkpointing model
2026-01-03 12:06:15,812: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/best_checkpoint
2026-01-03 12:06:16,065: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_1000
2026-01-03 12:06:33,019: Train batch 1200: loss: 32.50 grad norm: 72.27 time: 0.068
2026-01-03 12:06:50,321: Train batch 1400: loss: 36.76 grad norm: 80.23 time: 0.060
2026-01-03 12:06:58,981: Running test after training batch: 1500
2026-01-03 12:06:59,079: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:07:03,903: WER debug example
  GT : you can see the code at this point as well
  PR : yule kint e the owed it this boyde is will
2026-01-03 12:07:03,934: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that us
2026-01-03 12:07:05,520: Val batch 1500: PER (avg): 0.3815 CTC Loss (avg): 37.2658 WER(1gram): 75.38% (n=64) time: 6.539
2026-01-03 12:07:05,521: WER lens: avg_true_words=6.16 avg_pred_words=5.09 max_pred_words=11
2026-01-03 12:07:05,521: t15.2023.08.13 val PER: 0.3451
2026-01-03 12:07:05,521: t15.2023.08.18 val PER: 0.3261
2026-01-03 12:07:05,521: t15.2023.08.20 val PER: 0.3114
2026-01-03 12:07:05,521: t15.2023.08.25 val PER: 0.2560
2026-01-03 12:07:05,521: t15.2023.08.27 val PER: 0.4068
2026-01-03 12:07:05,521: t15.2023.09.01 val PER: 0.2719
2026-01-03 12:07:05,521: t15.2023.09.03 val PER: 0.3812
2026-01-03 12:07:05,521: t15.2023.09.24 val PER: 0.3070
2026-01-03 12:07:05,521: t15.2023.09.29 val PER: 0.3414
2026-01-03 12:07:05,521: t15.2023.10.01 val PER: 0.3950
2026-01-03 12:07:05,522: t15.2023.10.06 val PER: 0.2906
2026-01-03 12:07:05,522: t15.2023.10.08 val PER: 0.4344
2026-01-03 12:07:05,522: t15.2023.10.13 val PER: 0.4430
2026-01-03 12:07:05,522: t15.2023.10.15 val PER: 0.3599
2026-01-03 12:07:05,522: t15.2023.10.20 val PER: 0.3423
2026-01-03 12:07:05,522: t15.2023.10.22 val PER: 0.3163
2026-01-03 12:07:05,522: t15.2023.11.03 val PER: 0.3623
2026-01-03 12:07:05,522: t15.2023.11.04 val PER: 0.1126
2026-01-03 12:07:05,522: t15.2023.11.17 val PER: 0.2224
2026-01-03 12:07:05,522: t15.2023.11.19 val PER: 0.1697
2026-01-03 12:07:05,522: t15.2023.11.26 val PER: 0.4174
2026-01-03 12:07:05,522: t15.2023.12.03 val PER: 0.3718
2026-01-03 12:07:05,522: t15.2023.12.08 val PER: 0.3542
2026-01-03 12:07:05,522: t15.2023.12.10 val PER: 0.3127
2026-01-03 12:07:05,522: t15.2023.12.17 val PER: 0.3732
2026-01-03 12:07:05,522: t15.2023.12.29 val PER: 0.3699
2026-01-03 12:07:05,522: t15.2024.02.25 val PER: 0.3258
2026-01-03 12:07:05,523: t15.2024.03.08 val PER: 0.4538
2026-01-03 12:07:05,523: t15.2024.03.15 val PER: 0.4171
2026-01-03 12:07:05,523: t15.2024.03.17 val PER: 0.3682
2026-01-03 12:07:05,523: t15.2024.05.10 val PER: 0.3819
2026-01-03 12:07:05,523: t15.2024.06.14 val PER: 0.3959
2026-01-03 12:07:05,523: t15.2024.07.19 val PER: 0.5194
2026-01-03 12:07:05,523: t15.2024.07.21 val PER: 0.3572
2026-01-03 12:07:05,523: t15.2024.07.28 val PER: 0.3699
2026-01-03 12:07:05,523: t15.2025.01.10 val PER: 0.6088
2026-01-03 12:07:05,523: t15.2025.01.12 val PER: 0.4288
2026-01-03 12:07:05,523: t15.2025.03.14 val PER: 0.6080
2026-01-03 12:07:05,523: t15.2025.03.16 val PER: 0.4634
2026-01-03 12:07:05,523: t15.2025.03.30 val PER: 0.6540
2026-01-03 12:07:05,523: t15.2025.04.13 val PER: 0.4722
2026-01-03 12:07:05,525: New best val WER(1gram) 82.74% --> 75.38%
2026-01-03 12:07:05,525: Checkpointing model
2026-01-03 12:07:05,785: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/best_checkpoint
2026-01-03 12:07:06,038: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_1500
2026-01-03 12:07:14,498: Train batch 1600: loss: 36.93 grad norm: 83.17 time: 0.063
2026-01-03 12:07:31,784: Train batch 1800: loss: 35.23 grad norm: 68.65 time: 0.088
2026-01-03 12:07:49,110: Train batch 2000: loss: 34.62 grad norm: 73.51 time: 0.066
2026-01-03 12:07:49,110: Running test after training batch: 2000
2026-01-03 12:07:49,252: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:07:54,303: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this and is wheel
2026-01-03 12:07:54,331: WER debug example
  GT : how does it keep the cost down
  PR : houde buice it eke the us id
2026-01-03 12:07:55,925: Val batch 2000: PER (avg): 0.3279 CTC Loss (avg): 33.1852 WER(1gram): 70.56% (n=64) time: 6.814
2026-01-03 12:07:55,925: WER lens: avg_true_words=6.16 avg_pred_words=5.53 max_pred_words=11
2026-01-03 12:07:55,926: t15.2023.08.13 val PER: 0.3025
2026-01-03 12:07:55,926: t15.2023.08.18 val PER: 0.2598
2026-01-03 12:07:55,926: t15.2023.08.20 val PER: 0.2589
2026-01-03 12:07:55,926: t15.2023.08.25 val PER: 0.2334
2026-01-03 12:07:55,926: t15.2023.08.27 val PER: 0.3392
2026-01-03 12:07:55,926: t15.2023.09.01 val PER: 0.2370
2026-01-03 12:07:55,926: t15.2023.09.03 val PER: 0.3278
2026-01-03 12:07:55,926: t15.2023.09.24 val PER: 0.2536
2026-01-03 12:07:55,926: t15.2023.09.29 val PER: 0.2725
2026-01-03 12:07:55,927: t15.2023.10.01 val PER: 0.3256
2026-01-03 12:07:55,927: t15.2023.10.06 val PER: 0.2379
2026-01-03 12:07:55,927: t15.2023.10.08 val PER: 0.3978
2026-01-03 12:07:55,927: t15.2023.10.13 val PER: 0.3825
2026-01-03 12:07:55,927: t15.2023.10.15 val PER: 0.3092
2026-01-03 12:07:55,927: t15.2023.10.20 val PER: 0.2987
2026-01-03 12:07:55,927: t15.2023.10.22 val PER: 0.2795
2026-01-03 12:07:55,927: t15.2023.11.03 val PER: 0.3155
2026-01-03 12:07:55,927: t15.2023.11.04 val PER: 0.1092
2026-01-03 12:07:55,927: t15.2023.11.17 val PER: 0.1680
2026-01-03 12:07:55,927: t15.2023.11.19 val PER: 0.1417
2026-01-03 12:07:55,927: t15.2023.11.26 val PER: 0.3630
2026-01-03 12:07:55,927: t15.2023.12.03 val PER: 0.3183
2026-01-03 12:07:55,928: t15.2023.12.08 val PER: 0.3036
2026-01-03 12:07:55,928: t15.2023.12.10 val PER: 0.2562
2026-01-03 12:07:55,928: t15.2023.12.17 val PER: 0.3306
2026-01-03 12:07:55,928: t15.2023.12.29 val PER: 0.3233
2026-01-03 12:07:55,928: t15.2024.02.25 val PER: 0.2809
2026-01-03 12:07:55,928: t15.2024.03.08 val PER: 0.4011
2026-01-03 12:07:55,928: t15.2024.03.15 val PER: 0.3571
2026-01-03 12:07:55,928: t15.2024.03.17 val PER: 0.3368
2026-01-03 12:07:55,928: t15.2024.05.10 val PER: 0.3373
2026-01-03 12:07:55,928: t15.2024.06.14 val PER: 0.3407
2026-01-03 12:07:55,928: t15.2024.07.19 val PER: 0.4575
2026-01-03 12:07:55,928: t15.2024.07.21 val PER: 0.2993
2026-01-03 12:07:55,928: t15.2024.07.28 val PER: 0.3191
2026-01-03 12:07:55,928: t15.2025.01.10 val PER: 0.5427
2026-01-03 12:07:55,928: t15.2025.01.12 val PER: 0.3818
2026-01-03 12:07:55,929: t15.2025.03.14 val PER: 0.5325
2026-01-03 12:07:55,929: t15.2025.03.16 val PER: 0.3887
2026-01-03 12:07:55,929: t15.2025.03.30 val PER: 0.5402
2026-01-03 12:07:55,929: t15.2025.04.13 val PER: 0.4123
2026-01-03 12:07:55,930: New best val WER(1gram) 75.38% --> 70.56%
2026-01-03 12:07:55,930: Checkpointing model
2026-01-03 12:07:56,195: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/best_checkpoint
2026-01-03 12:07:56,447: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_2000
2026-01-03 12:08:13,491: Train batch 2200: loss: 28.94 grad norm: 72.35 time: 0.060
2026-01-03 12:08:30,688: Train batch 2400: loss: 29.78 grad norm: 64.35 time: 0.052
2026-01-03 12:08:39,307: Running test after training batch: 2500
2026-01-03 12:08:39,405: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:08:44,116: WER debug example
  GT : you can see the code at this point as well
  PR : yu end sze the code at this point is will
2026-01-03 12:08:44,144: WER debug example
  GT : how does it keep the cost down
  PR : houde just it yip the us it
2026-01-03 12:08:45,731: Val batch 2500: PER (avg): 0.3030 CTC Loss (avg): 29.9818 WER(1gram): 67.77% (n=64) time: 6.424
2026-01-03 12:08:45,731: WER lens: avg_true_words=6.16 avg_pred_words=5.58 max_pred_words=11
2026-01-03 12:08:45,731: t15.2023.08.13 val PER: 0.2827
2026-01-03 12:08:45,731: t15.2023.08.18 val PER: 0.2439
2026-01-03 12:08:45,732: t15.2023.08.20 val PER: 0.2351
2026-01-03 12:08:45,732: t15.2023.08.25 val PER: 0.2048
2026-01-03 12:08:45,732: t15.2023.08.27 val PER: 0.3215
2026-01-03 12:08:45,732: t15.2023.09.01 val PER: 0.2054
2026-01-03 12:08:45,732: t15.2023.09.03 val PER: 0.2957
2026-01-03 12:08:45,732: t15.2023.09.24 val PER: 0.2318
2026-01-03 12:08:45,732: t15.2023.09.29 val PER: 0.2514
2026-01-03 12:08:45,733: t15.2023.10.01 val PER: 0.3012
2026-01-03 12:08:45,733: t15.2023.10.06 val PER: 0.2099
2026-01-03 12:08:45,733: t15.2023.10.08 val PER: 0.3694
2026-01-03 12:08:45,733: t15.2023.10.13 val PER: 0.3576
2026-01-03 12:08:45,733: t15.2023.10.15 val PER: 0.2835
2026-01-03 12:08:45,733: t15.2023.10.20 val PER: 0.2852
2026-01-03 12:08:45,733: t15.2023.10.22 val PER: 0.2339
2026-01-03 12:08:45,733: t15.2023.11.03 val PER: 0.2924
2026-01-03 12:08:45,733: t15.2023.11.04 val PER: 0.0853
2026-01-03 12:08:45,733: t15.2023.11.17 val PER: 0.1571
2026-01-03 12:08:45,734: t15.2023.11.19 val PER: 0.1238
2026-01-03 12:08:45,734: t15.2023.11.26 val PER: 0.3442
2026-01-03 12:08:45,734: t15.2023.12.03 val PER: 0.2920
2026-01-03 12:08:45,734: t15.2023.12.08 val PER: 0.2690
2026-01-03 12:08:45,734: t15.2023.12.10 val PER: 0.2365
2026-01-03 12:08:45,734: t15.2023.12.17 val PER: 0.2859
2026-01-03 12:08:45,734: t15.2023.12.29 val PER: 0.2986
2026-01-03 12:08:45,734: t15.2024.02.25 val PER: 0.2472
2026-01-03 12:08:45,734: t15.2024.03.08 val PER: 0.3585
2026-01-03 12:08:45,734: t15.2024.03.15 val PER: 0.3558
2026-01-03 12:08:45,735: t15.2024.03.17 val PER: 0.3124
2026-01-03 12:08:45,735: t15.2024.05.10 val PER: 0.3031
2026-01-03 12:08:45,735: t15.2024.06.14 val PER: 0.3202
2026-01-03 12:08:45,735: t15.2024.07.19 val PER: 0.4469
2026-01-03 12:08:45,735: t15.2024.07.21 val PER: 0.2538
2026-01-03 12:08:45,735: t15.2024.07.28 val PER: 0.2978
2026-01-03 12:08:45,735: t15.2025.01.10 val PER: 0.5096
2026-01-03 12:08:45,735: t15.2025.01.12 val PER: 0.3603
2026-01-03 12:08:45,735: t15.2025.03.14 val PER: 0.5044
2026-01-03 12:08:45,735: t15.2025.03.16 val PER: 0.3613
2026-01-03 12:08:45,736: t15.2025.03.30 val PER: 0.5184
2026-01-03 12:08:45,736: t15.2025.04.13 val PER: 0.3937
2026-01-03 12:08:45,736: New best val WER(1gram) 70.56% --> 67.77%
2026-01-03 12:08:45,736: Checkpointing model
2026-01-03 12:08:45,998: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/best_checkpoint
2026-01-03 12:08:46,250: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_2500
2026-01-03 12:08:54,712: Train batch 2600: loss: 35.32 grad norm: 83.07 time: 0.054
2026-01-03 12:09:11,902: Train batch 2800: loss: 25.34 grad norm: 70.29 time: 0.081
2026-01-03 12:09:28,917: Train batch 3000: loss: 31.60 grad norm: 79.19 time: 0.081
2026-01-03 12:09:28,918: Running test after training batch: 3000
2026-01-03 12:09:29,018: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:09:33,777: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the could at this point is will
2026-01-03 12:09:33,806: WER debug example
  GT : how does it keep the cost down
  PR : houde des it hipp the rost get
2026-01-03 12:09:35,417: Val batch 3000: PER (avg): 0.2784 CTC Loss (avg): 27.9030 WER(1gram): 66.50% (n=64) time: 6.500
2026-01-03 12:09:35,418: WER lens: avg_true_words=6.16 avg_pred_words=5.84 max_pred_words=11
2026-01-03 12:09:35,418: t15.2023.08.13 val PER: 0.2516
2026-01-03 12:09:35,418: t15.2023.08.18 val PER: 0.2213
2026-01-03 12:09:35,418: t15.2023.08.20 val PER: 0.2121
2026-01-03 12:09:35,419: t15.2023.08.25 val PER: 0.1883
2026-01-03 12:09:35,419: t15.2023.08.27 val PER: 0.2830
2026-01-03 12:09:35,419: t15.2023.09.01 val PER: 0.1859
2026-01-03 12:09:35,419: t15.2023.09.03 val PER: 0.2850
2026-01-03 12:09:35,419: t15.2023.09.24 val PER: 0.2051
2026-01-03 12:09:35,419: t15.2023.09.29 val PER: 0.2348
2026-01-03 12:09:35,419: t15.2023.10.01 val PER: 0.2880
2026-01-03 12:09:35,419: t15.2023.10.06 val PER: 0.2067
2026-01-03 12:09:35,420: t15.2023.10.08 val PER: 0.3491
2026-01-03 12:09:35,420: t15.2023.10.13 val PER: 0.3351
2026-01-03 12:09:35,420: t15.2023.10.15 val PER: 0.2630
2026-01-03 12:09:35,420: t15.2023.10.20 val PER: 0.2450
2026-01-03 12:09:35,420: t15.2023.10.22 val PER: 0.2060
2026-01-03 12:09:35,420: t15.2023.11.03 val PER: 0.2775
2026-01-03 12:09:35,420: t15.2023.11.04 val PER: 0.0853
2026-01-03 12:09:35,420: t15.2023.11.17 val PER: 0.1260
2026-01-03 12:09:35,420: t15.2023.11.19 val PER: 0.1257
2026-01-03 12:09:35,420: t15.2023.11.26 val PER: 0.3051
2026-01-03 12:09:35,421: t15.2023.12.03 val PER: 0.2605
2026-01-03 12:09:35,421: t15.2023.12.08 val PER: 0.2550
2026-01-03 12:09:35,421: t15.2023.12.10 val PER: 0.2076
2026-01-03 12:09:35,421: t15.2023.12.17 val PER: 0.2672
2026-01-03 12:09:35,421: t15.2023.12.29 val PER: 0.2848
2026-01-03 12:09:35,421: t15.2024.02.25 val PER: 0.2233
2026-01-03 12:09:35,421: t15.2024.03.08 val PER: 0.3499
2026-01-03 12:09:35,421: t15.2024.03.15 val PER: 0.3377
2026-01-03 12:09:35,421: t15.2024.03.17 val PER: 0.2796
2026-01-03 12:09:35,422: t15.2024.05.10 val PER: 0.2987
2026-01-03 12:09:35,422: t15.2024.06.14 val PER: 0.2934
2026-01-03 12:09:35,422: t15.2024.07.19 val PER: 0.3968
2026-01-03 12:09:35,422: t15.2024.07.21 val PER: 0.2331
2026-01-03 12:09:35,422: t15.2024.07.28 val PER: 0.2750
2026-01-03 12:09:35,422: t15.2025.01.10 val PER: 0.4835
2026-01-03 12:09:35,422: t15.2025.01.12 val PER: 0.3149
2026-01-03 12:09:35,422: t15.2025.03.14 val PER: 0.4512
2026-01-03 12:09:35,422: t15.2025.03.16 val PER: 0.3285
2026-01-03 12:09:35,422: t15.2025.03.30 val PER: 0.4690
2026-01-03 12:09:35,423: t15.2025.04.13 val PER: 0.3395
2026-01-03 12:09:35,423: New best val WER(1gram) 67.77% --> 66.50%
2026-01-03 12:09:35,423: Checkpointing model
2026-01-03 12:09:35,685: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/best_checkpoint
2026-01-03 12:09:35,974: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_3000
2026-01-03 12:09:53,147: Train batch 3200: loss: 26.10 grad norm: 69.00 time: 0.075
2026-01-03 12:10:10,143: Train batch 3400: loss: 18.49 grad norm: 54.98 time: 0.048
2026-01-03 12:10:18,919: Running test after training batch: 3500
2026-01-03 12:10:19,059: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:10:24,007: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sikh the code at this point wheel
2026-01-03 12:10:24,035: WER debug example
  GT : how does it keep the cost down
  PR : aue dust it kipp thus cussed ent
2026-01-03 12:10:25,575: Val batch 3500: PER (avg): 0.2697 CTC Loss (avg): 26.6000 WER(1gram): 65.48% (n=64) time: 6.656
2026-01-03 12:10:25,576: WER lens: avg_true_words=6.16 avg_pred_words=5.91 max_pred_words=11
2026-01-03 12:10:25,576: t15.2023.08.13 val PER: 0.2401
2026-01-03 12:10:25,576: t15.2023.08.18 val PER: 0.1978
2026-01-03 12:10:25,576: t15.2023.08.20 val PER: 0.2216
2026-01-03 12:10:25,576: t15.2023.08.25 val PER: 0.1747
2026-01-03 12:10:25,576: t15.2023.08.27 val PER: 0.2685
2026-01-03 12:10:25,576: t15.2023.09.01 val PER: 0.1834
2026-01-03 12:10:25,576: t15.2023.09.03 val PER: 0.2708
2026-01-03 12:10:25,576: t15.2023.09.24 val PER: 0.2197
2026-01-03 12:10:25,577: t15.2023.09.29 val PER: 0.2208
2026-01-03 12:10:25,577: t15.2023.10.01 val PER: 0.2919
2026-01-03 12:10:25,577: t15.2023.10.06 val PER: 0.1798
2026-01-03 12:10:25,577: t15.2023.10.08 val PER: 0.3356
2026-01-03 12:10:25,577: t15.2023.10.13 val PER: 0.3150
2026-01-03 12:10:25,577: t15.2023.10.15 val PER: 0.2485
2026-01-03 12:10:25,577: t15.2023.10.20 val PER: 0.2450
2026-01-03 12:10:25,577: t15.2023.10.22 val PER: 0.2094
2026-01-03 12:10:25,577: t15.2023.11.03 val PER: 0.2659
2026-01-03 12:10:25,577: t15.2023.11.04 val PER: 0.0785
2026-01-03 12:10:25,577: t15.2023.11.17 val PER: 0.1198
2026-01-03 12:10:25,577: t15.2023.11.19 val PER: 0.1238
2026-01-03 12:10:25,578: t15.2023.11.26 val PER: 0.2906
2026-01-03 12:10:25,578: t15.2023.12.03 val PER: 0.2489
2026-01-03 12:10:25,578: t15.2023.12.08 val PER: 0.2410
2026-01-03 12:10:25,578: t15.2023.12.10 val PER: 0.2050
2026-01-03 12:10:25,578: t15.2023.12.17 val PER: 0.2578
2026-01-03 12:10:25,578: t15.2023.12.29 val PER: 0.2656
2026-01-03 12:10:25,578: t15.2024.02.25 val PER: 0.2135
2026-01-03 12:10:25,578: t15.2024.03.08 val PER: 0.3499
2026-01-03 12:10:25,578: t15.2024.03.15 val PER: 0.3189
2026-01-03 12:10:25,578: t15.2024.03.17 val PER: 0.2887
2026-01-03 12:10:25,579: t15.2024.05.10 val PER: 0.2838
2026-01-03 12:10:25,579: t15.2024.06.14 val PER: 0.2792
2026-01-03 12:10:25,579: t15.2024.07.19 val PER: 0.3949
2026-01-03 12:10:25,579: t15.2024.07.21 val PER: 0.2255
2026-01-03 12:10:25,579: t15.2024.07.28 val PER: 0.2801
2026-01-03 12:10:25,579: t15.2025.01.10 val PER: 0.4752
2026-01-03 12:10:25,579: t15.2025.01.12 val PER: 0.3025
2026-01-03 12:10:25,579: t15.2025.03.14 val PER: 0.4453
2026-01-03 12:10:25,579: t15.2025.03.16 val PER: 0.3076
2026-01-03 12:10:25,580: t15.2025.03.30 val PER: 0.4621
2026-01-03 12:10:25,580: t15.2025.04.13 val PER: 0.3338
2026-01-03 12:10:25,580: New best val WER(1gram) 66.50% --> 65.48%
2026-01-03 12:10:25,580: Checkpointing model
2026-01-03 12:10:25,845: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/best_checkpoint
2026-01-03 12:10:26,100: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_3500
2026-01-03 12:10:34,766: Train batch 3600: loss: 22.68 grad norm: 62.94 time: 0.067
2026-01-03 12:10:51,788: Train batch 3800: loss: 26.03 grad norm: 68.02 time: 0.066
2026-01-03 12:11:08,886: Train batch 4000: loss: 19.51 grad norm: 54.60 time: 0.055
2026-01-03 12:11:08,887: Running test after training batch: 4000
2026-01-03 12:11:09,042: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:11:13,758: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is while
2026-01-03 12:11:13,786: WER debug example
  GT : how does it keep the cost down
  PR : aue dust it kipp the cussed nett
2026-01-03 12:11:15,418: Val batch 4000: PER (avg): 0.2519 CTC Loss (avg): 24.6420 WER(1gram): 64.97% (n=64) time: 6.531
2026-01-03 12:11:15,419: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-03 12:11:15,419: t15.2023.08.13 val PER: 0.2339
2026-01-03 12:11:15,419: t15.2023.08.18 val PER: 0.2070
2026-01-03 12:11:15,419: t15.2023.08.20 val PER: 0.1938
2026-01-03 12:11:15,419: t15.2023.08.25 val PER: 0.1581
2026-01-03 12:11:15,419: t15.2023.08.27 val PER: 0.2878
2026-01-03 12:11:15,419: t15.2023.09.01 val PER: 0.1615
2026-01-03 12:11:15,419: t15.2023.09.03 val PER: 0.2447
2026-01-03 12:11:15,419: t15.2023.09.24 val PER: 0.1930
2026-01-03 12:11:15,419: t15.2023.09.29 val PER: 0.2004
2026-01-03 12:11:15,419: t15.2023.10.01 val PER: 0.2602
2026-01-03 12:11:15,419: t15.2023.10.06 val PER: 0.1668
2026-01-03 12:11:15,419: t15.2023.10.08 val PER: 0.3234
2026-01-03 12:11:15,420: t15.2023.10.13 val PER: 0.3033
2026-01-03 12:11:15,420: t15.2023.10.15 val PER: 0.2452
2026-01-03 12:11:15,420: t15.2023.10.20 val PER: 0.2483
2026-01-03 12:11:15,420: t15.2023.10.22 val PER: 0.2060
2026-01-03 12:11:15,420: t15.2023.11.03 val PER: 0.2476
2026-01-03 12:11:15,420: t15.2023.11.04 val PER: 0.0683
2026-01-03 12:11:15,420: t15.2023.11.17 val PER: 0.1120
2026-01-03 12:11:15,420: t15.2023.11.19 val PER: 0.0898
2026-01-03 12:11:15,420: t15.2023.11.26 val PER: 0.2630
2026-01-03 12:11:15,420: t15.2023.12.03 val PER: 0.2300
2026-01-03 12:11:15,420: t15.2023.12.08 val PER: 0.2224
2026-01-03 12:11:15,420: t15.2023.12.10 val PER: 0.1919
2026-01-03 12:11:15,420: t15.2023.12.17 val PER: 0.2505
2026-01-03 12:11:15,420: t15.2023.12.29 val PER: 0.2615
2026-01-03 12:11:15,421: t15.2024.02.25 val PER: 0.2247
2026-01-03 12:11:15,421: t15.2024.03.08 val PER: 0.3229
2026-01-03 12:11:15,421: t15.2024.03.15 val PER: 0.3039
2026-01-03 12:11:15,421: t15.2024.03.17 val PER: 0.2608
2026-01-03 12:11:15,421: t15.2024.05.10 val PER: 0.2808
2026-01-03 12:11:15,421: t15.2024.06.14 val PER: 0.2666
2026-01-03 12:11:15,421: t15.2024.07.19 val PER: 0.3764
2026-01-03 12:11:15,421: t15.2024.07.21 val PER: 0.1876
2026-01-03 12:11:15,421: t15.2024.07.28 val PER: 0.2426
2026-01-03 12:11:15,421: t15.2025.01.10 val PER: 0.4215
2026-01-03 12:11:15,421: t15.2025.01.12 val PER: 0.2679
2026-01-03 12:11:15,421: t15.2025.03.14 val PER: 0.4320
2026-01-03 12:11:15,421: t15.2025.03.16 val PER: 0.3141
2026-01-03 12:11:15,421: t15.2025.03.30 val PER: 0.4161
2026-01-03 12:11:15,421: t15.2025.04.13 val PER: 0.3238
2026-01-03 12:11:15,423: New best val WER(1gram) 65.48% --> 64.97%
2026-01-03 12:11:15,423: Checkpointing model
2026-01-03 12:11:15,720: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/best_checkpoint
2026-01-03 12:11:15,974: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_4000
2026-01-03 12:11:33,275: Train batch 4200: loss: 22.66 grad norm: 67.51 time: 0.079
2026-01-03 12:11:50,559: Train batch 4400: loss: 16.91 grad norm: 56.68 time: 0.066
2026-01-03 12:11:59,283: Running test after training batch: 4500
2026-01-03 12:11:59,395: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:12:04,136: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-03 12:12:04,165: WER debug example
  GT : how does it keep the cost down
  PR : houde just it heap thus cussed get
2026-01-03 12:12:05,740: Val batch 4500: PER (avg): 0.2392 CTC Loss (avg): 23.3113 WER(1gram): 61.42% (n=64) time: 6.457
2026-01-03 12:12:05,740: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 12:12:05,740: t15.2023.08.13 val PER: 0.2069
2026-01-03 12:12:05,741: t15.2023.08.18 val PER: 0.1911
2026-01-03 12:12:05,741: t15.2023.08.20 val PER: 0.1962
2026-01-03 12:12:05,741: t15.2023.08.25 val PER: 0.1370
2026-01-03 12:12:05,741: t15.2023.08.27 val PER: 0.2508
2026-01-03 12:12:05,741: t15.2023.09.01 val PER: 0.1607
2026-01-03 12:12:05,741: t15.2023.09.03 val PER: 0.2375
2026-01-03 12:12:05,741: t15.2023.09.24 val PER: 0.1905
2026-01-03 12:12:05,741: t15.2023.09.29 val PER: 0.1978
2026-01-03 12:12:05,741: t15.2023.10.01 val PER: 0.2596
2026-01-03 12:12:05,741: t15.2023.10.06 val PER: 0.1518
2026-01-03 12:12:05,741: t15.2023.10.08 val PER: 0.3139
2026-01-03 12:12:05,742: t15.2023.10.13 val PER: 0.2940
2026-01-03 12:12:05,742: t15.2023.10.15 val PER: 0.2334
2026-01-03 12:12:05,742: t15.2023.10.20 val PER: 0.2349
2026-01-03 12:12:05,742: t15.2023.10.22 val PER: 0.1938
2026-01-03 12:12:05,742: t15.2023.11.03 val PER: 0.2408
2026-01-03 12:12:05,742: t15.2023.11.04 val PER: 0.0478
2026-01-03 12:12:05,742: t15.2023.11.17 val PER: 0.1042
2026-01-03 12:12:05,742: t15.2023.11.19 val PER: 0.0898
2026-01-03 12:12:05,742: t15.2023.11.26 val PER: 0.2652
2026-01-03 12:12:05,742: t15.2023.12.03 val PER: 0.2153
2026-01-03 12:12:05,742: t15.2023.12.08 val PER: 0.2150
2026-01-03 12:12:05,742: t15.2023.12.10 val PER: 0.1774
2026-01-03 12:12:05,742: t15.2023.12.17 val PER: 0.2318
2026-01-03 12:12:05,742: t15.2023.12.29 val PER: 0.2388
2026-01-03 12:12:05,742: t15.2024.02.25 val PER: 0.1952
2026-01-03 12:12:05,743: t15.2024.03.08 val PER: 0.3186
2026-01-03 12:12:05,743: t15.2024.03.15 val PER: 0.2839
2026-01-03 12:12:05,743: t15.2024.03.17 val PER: 0.2497
2026-01-03 12:12:05,743: t15.2024.05.10 val PER: 0.2600
2026-01-03 12:12:05,743: t15.2024.06.14 val PER: 0.2555
2026-01-03 12:12:05,743: t15.2024.07.19 val PER: 0.3322
2026-01-03 12:12:05,743: t15.2024.07.21 val PER: 0.1834
2026-01-03 12:12:05,743: t15.2024.07.28 val PER: 0.2272
2026-01-03 12:12:05,743: t15.2025.01.10 val PER: 0.4105
2026-01-03 12:12:05,743: t15.2025.01.12 val PER: 0.2717
2026-01-03 12:12:05,743: t15.2025.03.14 val PER: 0.4083
2026-01-03 12:12:05,743: t15.2025.03.16 val PER: 0.2893
2026-01-03 12:12:05,743: t15.2025.03.30 val PER: 0.3874
2026-01-03 12:12:05,743: t15.2025.04.13 val PER: 0.3067
2026-01-03 12:12:05,745: New best val WER(1gram) 64.97% --> 61.42%
2026-01-03 12:12:05,745: Checkpointing model
2026-01-03 12:12:06,049: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/best_checkpoint
2026-01-03 12:12:06,330: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_4500
2026-01-03 12:12:14,960: Train batch 4600: loss: 20.56 grad norm: 64.40 time: 0.062
2026-01-03 12:12:32,185: Train batch 4800: loss: 14.05 grad norm: 53.83 time: 0.063
2026-01-03 12:12:49,374: Train batch 5000: loss: 31.92 grad norm: 84.20 time: 0.064
2026-01-03 12:12:49,375: Running test after training batch: 5000
2026-01-03 12:12:49,498: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:12:54,378: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point as will
2026-01-03 12:12:54,409: WER debug example
  GT : how does it keep the cost down
  PR : houde just it heap the cost get
2026-01-03 12:12:56,016: Val batch 5000: PER (avg): 0.2277 CTC Loss (avg): 22.0764 WER(1gram): 60.66% (n=64) time: 6.641
2026-01-03 12:12:56,016: WER lens: avg_true_words=6.16 avg_pred_words=6.02 max_pred_words=11
2026-01-03 12:12:56,017: t15.2023.08.13 val PER: 0.1965
2026-01-03 12:12:56,017: t15.2023.08.18 val PER: 0.1777
2026-01-03 12:12:56,017: t15.2023.08.20 val PER: 0.1755
2026-01-03 12:12:56,017: t15.2023.08.25 val PER: 0.1310
2026-01-03 12:12:56,017: t15.2023.08.27 val PER: 0.2428
2026-01-03 12:12:56,017: t15.2023.09.01 val PER: 0.1364
2026-01-03 12:12:56,017: t15.2023.09.03 val PER: 0.2328
2026-01-03 12:12:56,017: t15.2023.09.24 val PER: 0.1699
2026-01-03 12:12:56,017: t15.2023.09.29 val PER: 0.1806
2026-01-03 12:12:56,017: t15.2023.10.01 val PER: 0.2411
2026-01-03 12:12:56,017: t15.2023.10.06 val PER: 0.1464
2026-01-03 12:12:56,017: t15.2023.10.08 val PER: 0.3085
2026-01-03 12:12:56,018: t15.2023.10.13 val PER: 0.2878
2026-01-03 12:12:56,018: t15.2023.10.15 val PER: 0.2195
2026-01-03 12:12:56,018: t15.2023.10.20 val PER: 0.2315
2026-01-03 12:12:56,018: t15.2023.10.22 val PER: 0.1626
2026-01-03 12:12:56,018: t15.2023.11.03 val PER: 0.2246
2026-01-03 12:12:56,018: t15.2023.11.04 val PER: 0.0546
2026-01-03 12:12:56,018: t15.2023.11.17 val PER: 0.0840
2026-01-03 12:12:56,018: t15.2023.11.19 val PER: 0.0838
2026-01-03 12:12:56,018: t15.2023.11.26 val PER: 0.2377
2026-01-03 12:12:56,018: t15.2023.12.03 val PER: 0.2164
2026-01-03 12:12:56,018: t15.2023.12.08 val PER: 0.2011
2026-01-03 12:12:56,018: t15.2023.12.10 val PER: 0.1616
2026-01-03 12:12:56,018: t15.2023.12.17 val PER: 0.2193
2026-01-03 12:12:56,019: t15.2023.12.29 val PER: 0.2251
2026-01-03 12:12:56,019: t15.2024.02.25 val PER: 0.1938
2026-01-03 12:12:56,019: t15.2024.03.08 val PER: 0.3087
2026-01-03 12:12:56,019: t15.2024.03.15 val PER: 0.2770
2026-01-03 12:12:56,019: t15.2024.03.17 val PER: 0.2392
2026-01-03 12:12:56,019: t15.2024.05.10 val PER: 0.2392
2026-01-03 12:12:56,019: t15.2024.06.14 val PER: 0.2634
2026-01-03 12:12:56,019: t15.2024.07.19 val PER: 0.3375
2026-01-03 12:12:56,019: t15.2024.07.21 val PER: 0.1766
2026-01-03 12:12:56,019: t15.2024.07.28 val PER: 0.2140
2026-01-03 12:12:56,019: t15.2025.01.10 val PER: 0.3871
2026-01-03 12:12:56,019: t15.2025.01.12 val PER: 0.2640
2026-01-03 12:12:56,020: t15.2025.03.14 val PER: 0.3979
2026-01-03 12:12:56,020: t15.2025.03.16 val PER: 0.2762
2026-01-03 12:12:56,020: t15.2025.03.30 val PER: 0.3954
2026-01-03 12:12:56,020: t15.2025.04.13 val PER: 0.3067
2026-01-03 12:12:56,021: New best val WER(1gram) 61.42% --> 60.66%
2026-01-03 12:12:56,021: Checkpointing model
2026-01-03 12:12:56,288: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/best_checkpoint
2026-01-03 12:12:56,538: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_5000
2026-01-03 12:13:13,888: Train batch 5200: loss: 16.89 grad norm: 62.99 time: 0.051
2026-01-03 12:13:30,987: Train batch 5400: loss: 17.78 grad norm: 58.84 time: 0.067
2026-01-03 12:13:39,684: Running test after training batch: 5500
2026-01-03 12:13:39,847: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:13:44,577: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point will
2026-01-03 12:13:44,605: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-03 12:13:46,194: Val batch 5500: PER (avg): 0.2151 CTC Loss (avg): 21.0596 WER(1gram): 56.60% (n=64) time: 6.509
2026-01-03 12:13:46,195: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 12:13:46,195: t15.2023.08.13 val PER: 0.1757
2026-01-03 12:13:46,195: t15.2023.08.18 val PER: 0.1517
2026-01-03 12:13:46,195: t15.2023.08.20 val PER: 0.1763
2026-01-03 12:13:46,195: t15.2023.08.25 val PER: 0.1114
2026-01-03 12:13:46,195: t15.2023.08.27 val PER: 0.2363
2026-01-03 12:13:46,195: t15.2023.09.01 val PER: 0.1364
2026-01-03 12:13:46,195: t15.2023.09.03 val PER: 0.2197
2026-01-03 12:13:46,195: t15.2023.09.24 val PER: 0.1820
2026-01-03 12:13:46,195: t15.2023.09.29 val PER: 0.1710
2026-01-03 12:13:46,196: t15.2023.10.01 val PER: 0.2351
2026-01-03 12:13:46,196: t15.2023.10.06 val PER: 0.1367
2026-01-03 12:13:46,196: t15.2023.10.08 val PER: 0.2991
2026-01-03 12:13:46,196: t15.2023.10.13 val PER: 0.2700
2026-01-03 12:13:46,196: t15.2023.10.15 val PER: 0.2129
2026-01-03 12:13:46,196: t15.2023.10.20 val PER: 0.2349
2026-01-03 12:13:46,196: t15.2023.10.22 val PER: 0.1615
2026-01-03 12:13:46,196: t15.2023.11.03 val PER: 0.2252
2026-01-03 12:13:46,196: t15.2023.11.04 val PER: 0.0546
2026-01-03 12:13:46,196: t15.2023.11.17 val PER: 0.0809
2026-01-03 12:13:46,196: t15.2023.11.19 val PER: 0.0739
2026-01-03 12:13:46,196: t15.2023.11.26 val PER: 0.2109
2026-01-03 12:13:46,196: t15.2023.12.03 val PER: 0.1880
2026-01-03 12:13:46,196: t15.2023.12.08 val PER: 0.1851
2026-01-03 12:13:46,196: t15.2023.12.10 val PER: 0.1616
2026-01-03 12:13:46,196: t15.2023.12.17 val PER: 0.2131
2026-01-03 12:13:46,196: t15.2023.12.29 val PER: 0.2155
2026-01-03 12:13:46,197: t15.2024.02.25 val PER: 0.1798
2026-01-03 12:13:46,197: t15.2024.03.08 val PER: 0.3001
2026-01-03 12:13:46,197: t15.2024.03.15 val PER: 0.2570
2026-01-03 12:13:46,197: t15.2024.03.17 val PER: 0.2162
2026-01-03 12:13:46,197: t15.2024.05.10 val PER: 0.2377
2026-01-03 12:13:46,197: t15.2024.06.14 val PER: 0.2256
2026-01-03 12:13:46,197: t15.2024.07.19 val PER: 0.3138
2026-01-03 12:13:46,197: t15.2024.07.21 val PER: 0.1566
2026-01-03 12:13:46,197: t15.2024.07.28 val PER: 0.2125
2026-01-03 12:13:46,197: t15.2025.01.10 val PER: 0.3981
2026-01-03 12:13:46,198: t15.2025.01.12 val PER: 0.2371
2026-01-03 12:13:46,198: t15.2025.03.14 val PER: 0.3550
2026-01-03 12:13:46,198: t15.2025.03.16 val PER: 0.2670
2026-01-03 12:13:46,198: t15.2025.03.30 val PER: 0.3552
2026-01-03 12:13:46,198: t15.2025.04.13 val PER: 0.3096
2026-01-03 12:13:46,199: New best val WER(1gram) 60.66% --> 56.60%
2026-01-03 12:13:46,199: Checkpointing model
2026-01-03 12:13:46,481: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/best_checkpoint
2026-01-03 12:13:46,738: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_5500
2026-01-03 12:13:55,380: Train batch 5600: loss: 19.36 grad norm: 64.33 time: 0.061
2026-01-03 12:14:12,940: Train batch 5800: loss: 13.99 grad norm: 56.49 time: 0.082
2026-01-03 12:14:30,326: Train batch 6000: loss: 14.60 grad norm: 58.75 time: 0.048
2026-01-03 12:14:30,326: Running test after training batch: 6000
2026-01-03 12:14:30,548: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:14:35,410: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-03 12:14:35,440: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 12:14:37,077: Val batch 6000: PER (avg): 0.2097 CTC Loss (avg): 20.7647 WER(1gram): 57.11% (n=64) time: 6.750
2026-01-03 12:14:37,077: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 12:14:37,077: t15.2023.08.13 val PER: 0.1746
2026-01-03 12:14:37,077: t15.2023.08.18 val PER: 0.1626
2026-01-03 12:14:37,077: t15.2023.08.20 val PER: 0.1644
2026-01-03 12:14:37,077: t15.2023.08.25 val PER: 0.1145
2026-01-03 12:14:37,077: t15.2023.08.27 val PER: 0.2363
2026-01-03 12:14:37,077: t15.2023.09.01 val PER: 0.1356
2026-01-03 12:14:37,078: t15.2023.09.03 val PER: 0.2102
2026-01-03 12:14:37,078: t15.2023.09.24 val PER: 0.1638
2026-01-03 12:14:37,078: t15.2023.09.29 val PER: 0.1621
2026-01-03 12:14:37,078: t15.2023.10.01 val PER: 0.2206
2026-01-03 12:14:37,078: t15.2023.10.06 val PER: 0.1302
2026-01-03 12:14:37,078: t15.2023.10.08 val PER: 0.2936
2026-01-03 12:14:37,078: t15.2023.10.13 val PER: 0.2622
2026-01-03 12:14:37,078: t15.2023.10.15 val PER: 0.2169
2026-01-03 12:14:37,078: t15.2023.10.20 val PER: 0.2081
2026-01-03 12:14:37,078: t15.2023.10.22 val PER: 0.1670
2026-01-03 12:14:37,078: t15.2023.11.03 val PER: 0.2164
2026-01-03 12:14:37,078: t15.2023.11.04 val PER: 0.0546
2026-01-03 12:14:37,079: t15.2023.11.17 val PER: 0.0809
2026-01-03 12:14:37,079: t15.2023.11.19 val PER: 0.0838
2026-01-03 12:14:37,079: t15.2023.11.26 val PER: 0.2152
2026-01-03 12:14:37,079: t15.2023.12.03 val PER: 0.1681
2026-01-03 12:14:37,079: t15.2023.12.08 val PER: 0.1738
2026-01-03 12:14:37,079: t15.2023.12.10 val PER: 0.1432
2026-01-03 12:14:37,079: t15.2023.12.17 val PER: 0.2006
2026-01-03 12:14:37,079: t15.2023.12.29 val PER: 0.2121
2026-01-03 12:14:37,079: t15.2024.02.25 val PER: 0.1629
2026-01-03 12:14:37,079: t15.2024.03.08 val PER: 0.2930
2026-01-03 12:14:37,079: t15.2024.03.15 val PER: 0.2658
2026-01-03 12:14:37,079: t15.2024.03.17 val PER: 0.2099
2026-01-03 12:14:37,079: t15.2024.05.10 val PER: 0.2095
2026-01-03 12:14:37,079: t15.2024.06.14 val PER: 0.2303
2026-01-03 12:14:37,079: t15.2024.07.19 val PER: 0.3158
2026-01-03 12:14:37,079: t15.2024.07.21 val PER: 0.1572
2026-01-03 12:14:37,079: t15.2024.07.28 val PER: 0.1978
2026-01-03 12:14:37,080: t15.2025.01.10 val PER: 0.3760
2026-01-03 12:14:37,080: t15.2025.01.12 val PER: 0.2248
2026-01-03 12:14:37,080: t15.2025.03.14 val PER: 0.3743
2026-01-03 12:14:37,080: t15.2025.03.16 val PER: 0.2670
2026-01-03 12:14:37,080: t15.2025.03.30 val PER: 0.3690
2026-01-03 12:14:37,080: t15.2025.04.13 val PER: 0.2710
2026-01-03 12:14:37,334: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_6000
2026-01-03 12:14:54,244: Train batch 6200: loss: 16.91 grad norm: 60.38 time: 0.070
2026-01-03 12:15:11,022: Train batch 6400: loss: 19.10 grad norm: 66.93 time: 0.062
2026-01-03 12:15:19,301: Running test after training batch: 6500
2026-01-03 12:15:19,435: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:15:24,299: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 12:15:24,327: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-03 12:15:25,919: Val batch 6500: PER (avg): 0.2037 CTC Loss (avg): 20.1527 WER(1gram): 53.30% (n=64) time: 6.618
2026-01-03 12:15:25,919: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-03 12:15:25,919: t15.2023.08.13 val PER: 0.1622
2026-01-03 12:15:25,920: t15.2023.08.18 val PER: 0.1484
2026-01-03 12:15:25,920: t15.2023.08.20 val PER: 0.1557
2026-01-03 12:15:25,920: t15.2023.08.25 val PER: 0.1160
2026-01-03 12:15:25,920: t15.2023.08.27 val PER: 0.2283
2026-01-03 12:15:25,920: t15.2023.09.01 val PER: 0.1185
2026-01-03 12:15:25,920: t15.2023.09.03 val PER: 0.2031
2026-01-03 12:15:25,920: t15.2023.09.24 val PER: 0.1723
2026-01-03 12:15:25,920: t15.2023.09.29 val PER: 0.1691
2026-01-03 12:15:25,920: t15.2023.10.01 val PER: 0.2199
2026-01-03 12:15:25,920: t15.2023.10.06 val PER: 0.1281
2026-01-03 12:15:25,920: t15.2023.10.08 val PER: 0.2991
2026-01-03 12:15:25,920: t15.2023.10.13 val PER: 0.2692
2026-01-03 12:15:25,920: t15.2023.10.15 val PER: 0.2116
2026-01-03 12:15:25,920: t15.2023.10.20 val PER: 0.2148
2026-01-03 12:15:25,921: t15.2023.10.22 val PER: 0.1626
2026-01-03 12:15:25,921: t15.2023.11.03 val PER: 0.2144
2026-01-03 12:15:25,921: t15.2023.11.04 val PER: 0.0478
2026-01-03 12:15:25,921: t15.2023.11.17 val PER: 0.0700
2026-01-03 12:15:25,921: t15.2023.11.19 val PER: 0.0758
2026-01-03 12:15:25,921: t15.2023.11.26 val PER: 0.2022
2026-01-03 12:15:25,921: t15.2023.12.03 val PER: 0.1691
2026-01-03 12:15:25,921: t15.2023.12.08 val PER: 0.1671
2026-01-03 12:15:25,921: t15.2023.12.10 val PER: 0.1353
2026-01-03 12:15:25,921: t15.2023.12.17 val PER: 0.1819
2026-01-03 12:15:25,921: t15.2023.12.29 val PER: 0.2018
2026-01-03 12:15:25,921: t15.2024.02.25 val PER: 0.1728
2026-01-03 12:15:25,922: t15.2024.03.08 val PER: 0.3001
2026-01-03 12:15:25,922: t15.2024.03.15 val PER: 0.2552
2026-01-03 12:15:25,922: t15.2024.03.17 val PER: 0.2043
2026-01-03 12:15:25,922: t15.2024.05.10 val PER: 0.2259
2026-01-03 12:15:25,922: t15.2024.06.14 val PER: 0.2177
2026-01-03 12:15:25,922: t15.2024.07.19 val PER: 0.3065
2026-01-03 12:15:25,922: t15.2024.07.21 val PER: 0.1531
2026-01-03 12:15:25,922: t15.2024.07.28 val PER: 0.1941
2026-01-03 12:15:25,922: t15.2025.01.10 val PER: 0.3691
2026-01-03 12:15:25,922: t15.2025.01.12 val PER: 0.2079
2026-01-03 12:15:25,922: t15.2025.03.14 val PER: 0.3743
2026-01-03 12:15:25,922: t15.2025.03.16 val PER: 0.2330
2026-01-03 12:15:25,922: t15.2025.03.30 val PER: 0.3437
2026-01-03 12:15:25,922: t15.2025.04.13 val PER: 0.2696
2026-01-03 12:15:25,924: New best val WER(1gram) 56.60% --> 53.30%
2026-01-03 12:15:25,924: Checkpointing model
2026-01-03 12:15:26,223: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/best_checkpoint
2026-01-03 12:15:26,478: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_6500
2026-01-03 12:15:34,957: Train batch 6600: loss: 12.31 grad norm: 52.89 time: 0.044
2026-01-03 12:15:52,344: Train batch 6800: loss: 15.44 grad norm: 56.71 time: 0.048
2026-01-03 12:16:10,886: Train batch 7000: loss: 17.65 grad norm: 64.85 time: 0.060
2026-01-03 12:16:10,887: Running test after training batch: 7000
2026-01-03 12:16:11,048: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:16:15,783: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:16:15,812: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-03 12:16:17,474: Val batch 7000: PER (avg): 0.1937 CTC Loss (avg): 19.1550 WER(1gram): 54.31% (n=64) time: 6.587
2026-01-03 12:16:17,475: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 12:16:17,475: t15.2023.08.13 val PER: 0.1642
2026-01-03 12:16:17,475: t15.2023.08.18 val PER: 0.1391
2026-01-03 12:16:17,475: t15.2023.08.20 val PER: 0.1525
2026-01-03 12:16:17,475: t15.2023.08.25 val PER: 0.1039
2026-01-03 12:16:17,475: t15.2023.08.27 val PER: 0.2106
2026-01-03 12:16:17,475: t15.2023.09.01 val PER: 0.1055
2026-01-03 12:16:17,475: t15.2023.09.03 val PER: 0.1865
2026-01-03 12:16:17,476: t15.2023.09.24 val PER: 0.1602
2026-01-03 12:16:17,476: t15.2023.09.29 val PER: 0.1685
2026-01-03 12:16:17,476: t15.2023.10.01 val PER: 0.2133
2026-01-03 12:16:17,476: t15.2023.10.06 val PER: 0.1044
2026-01-03 12:16:17,476: t15.2023.10.08 val PER: 0.2774
2026-01-03 12:16:17,476: t15.2023.10.13 val PER: 0.2591
2026-01-03 12:16:17,476: t15.2023.10.15 val PER: 0.1931
2026-01-03 12:16:17,476: t15.2023.10.20 val PER: 0.2114
2026-01-03 12:16:17,476: t15.2023.10.22 val PER: 0.1414
2026-01-03 12:16:17,476: t15.2023.11.03 val PER: 0.2042
2026-01-03 12:16:17,476: t15.2023.11.04 val PER: 0.0341
2026-01-03 12:16:17,476: t15.2023.11.17 val PER: 0.0684
2026-01-03 12:16:17,476: t15.2023.11.19 val PER: 0.0499
2026-01-03 12:16:17,477: t15.2023.11.26 val PER: 0.1935
2026-01-03 12:16:17,477: t15.2023.12.03 val PER: 0.1660
2026-01-03 12:16:17,477: t15.2023.12.08 val PER: 0.1471
2026-01-03 12:16:17,477: t15.2023.12.10 val PER: 0.1380
2026-01-03 12:16:17,477: t15.2023.12.17 val PER: 0.1778
2026-01-03 12:16:17,477: t15.2023.12.29 val PER: 0.1977
2026-01-03 12:16:17,477: t15.2024.02.25 val PER: 0.1615
2026-01-03 12:16:17,477: t15.2024.03.08 val PER: 0.2788
2026-01-03 12:16:17,477: t15.2024.03.15 val PER: 0.2389
2026-01-03 12:16:17,477: t15.2024.03.17 val PER: 0.2071
2026-01-03 12:16:17,477: t15.2024.05.10 val PER: 0.1991
2026-01-03 12:16:17,477: t15.2024.06.14 val PER: 0.2019
2026-01-03 12:16:17,477: t15.2024.07.19 val PER: 0.3032
2026-01-03 12:16:17,477: t15.2024.07.21 val PER: 0.1338
2026-01-03 12:16:17,477: t15.2024.07.28 val PER: 0.1691
2026-01-03 12:16:17,477: t15.2025.01.10 val PER: 0.3788
2026-01-03 12:16:17,478: t15.2025.01.12 val PER: 0.1994
2026-01-03 12:16:17,478: t15.2025.03.14 val PER: 0.3506
2026-01-03 12:16:17,478: t15.2025.03.16 val PER: 0.2330
2026-01-03 12:16:17,478: t15.2025.03.30 val PER: 0.3621
2026-01-03 12:16:17,478: t15.2025.04.13 val PER: 0.2653
2026-01-03 12:16:17,723: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_7000
2026-01-03 12:16:34,727: Train batch 7200: loss: 14.64 grad norm: 56.79 time: 0.078
2026-01-03 12:16:51,484: Train batch 7400: loss: 13.49 grad norm: 54.96 time: 0.075
2026-01-03 12:16:59,889: Running test after training batch: 7500
2026-01-03 12:17:00,073: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:17:04,783: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 12:17:04,812: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cussed sette
2026-01-03 12:17:06,442: Val batch 7500: PER (avg): 0.1899 CTC Loss (avg): 18.7670 WER(1gram): 56.09% (n=64) time: 6.553
2026-01-03 12:17:06,443: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 12:17:06,443: t15.2023.08.13 val PER: 0.1455
2026-01-03 12:17:06,443: t15.2023.08.18 val PER: 0.1383
2026-01-03 12:17:06,443: t15.2023.08.20 val PER: 0.1414
2026-01-03 12:17:06,443: t15.2023.08.25 val PER: 0.1084
2026-01-03 12:17:06,443: t15.2023.08.27 val PER: 0.2138
2026-01-03 12:17:06,443: t15.2023.09.01 val PER: 0.1144
2026-01-03 12:17:06,443: t15.2023.09.03 val PER: 0.1888
2026-01-03 12:17:06,443: t15.2023.09.24 val PER: 0.1505
2026-01-03 12:17:06,443: t15.2023.09.29 val PER: 0.1595
2026-01-03 12:17:06,444: t15.2023.10.01 val PER: 0.2034
2026-01-03 12:17:06,444: t15.2023.10.06 val PER: 0.1206
2026-01-03 12:17:06,444: t15.2023.10.08 val PER: 0.2720
2026-01-03 12:17:06,444: t15.2023.10.13 val PER: 0.2498
2026-01-03 12:17:06,444: t15.2023.10.15 val PER: 0.1898
2026-01-03 12:17:06,444: t15.2023.10.20 val PER: 0.2114
2026-01-03 12:17:06,444: t15.2023.10.22 val PER: 0.1370
2026-01-03 12:17:06,444: t15.2023.11.03 val PER: 0.2049
2026-01-03 12:17:06,444: t15.2023.11.04 val PER: 0.0478
2026-01-03 12:17:06,444: t15.2023.11.17 val PER: 0.0762
2026-01-03 12:17:06,444: t15.2023.11.19 val PER: 0.0559
2026-01-03 12:17:06,445: t15.2023.11.26 val PER: 0.1899
2026-01-03 12:17:06,445: t15.2023.12.03 val PER: 0.1555
2026-01-03 12:17:06,445: t15.2023.12.08 val PER: 0.1425
2026-01-03 12:17:06,445: t15.2023.12.10 val PER: 0.1301
2026-01-03 12:17:06,445: t15.2023.12.17 val PER: 0.1736
2026-01-03 12:17:06,445: t15.2023.12.29 val PER: 0.1874
2026-01-03 12:17:06,445: t15.2024.02.25 val PER: 0.1587
2026-01-03 12:17:06,445: t15.2024.03.08 val PER: 0.2688
2026-01-03 12:17:06,445: t15.2024.03.15 val PER: 0.2439
2026-01-03 12:17:06,445: t15.2024.03.17 val PER: 0.1862
2026-01-03 12:17:06,445: t15.2024.05.10 val PER: 0.2021
2026-01-03 12:17:06,445: t15.2024.06.14 val PER: 0.2066
2026-01-03 12:17:06,445: t15.2024.07.19 val PER: 0.2927
2026-01-03 12:17:06,445: t15.2024.07.21 val PER: 0.1352
2026-01-03 12:17:06,446: t15.2024.07.28 val PER: 0.1831
2026-01-03 12:17:06,446: t15.2025.01.10 val PER: 0.3457
2026-01-03 12:17:06,446: t15.2025.01.12 val PER: 0.1948
2026-01-03 12:17:06,446: t15.2025.03.14 val PER: 0.3654
2026-01-03 12:17:06,446: t15.2025.03.16 val PER: 0.2487
2026-01-03 12:17:06,446: t15.2025.03.30 val PER: 0.3391
2026-01-03 12:17:06,446: t15.2025.04.13 val PER: 0.2482
2026-01-03 12:17:06,689: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_7500
2026-01-03 12:17:15,365: Train batch 7600: loss: 16.38 grad norm: 60.53 time: 0.069
2026-01-03 12:17:32,570: Train batch 7800: loss: 14.20 grad norm: 59.12 time: 0.056
2026-01-03 12:17:50,284: Train batch 8000: loss: 11.57 grad norm: 49.46 time: 0.072
2026-01-03 12:17:50,288: Running test after training batch: 8000
2026-01-03 12:17:50,386: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:17:55,320: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-03 12:17:55,352: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nit
2026-01-03 12:17:57,046: Val batch 8000: PER (avg): 0.1860 CTC Loss (avg): 18.2516 WER(1gram): 57.11% (n=64) time: 6.757
2026-01-03 12:17:57,046: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 12:17:57,046: t15.2023.08.13 val PER: 0.1466
2026-01-03 12:17:57,046: t15.2023.08.18 val PER: 0.1299
2026-01-03 12:17:57,046: t15.2023.08.20 val PER: 0.1454
2026-01-03 12:17:57,046: t15.2023.08.25 val PER: 0.1084
2026-01-03 12:17:57,046: t15.2023.08.27 val PER: 0.2122
2026-01-03 12:17:57,046: t15.2023.09.01 val PER: 0.1055
2026-01-03 12:17:57,047: t15.2023.09.03 val PER: 0.1912
2026-01-03 12:17:57,047: t15.2023.09.24 val PER: 0.1566
2026-01-03 12:17:57,047: t15.2023.09.29 val PER: 0.1532
2026-01-03 12:17:57,047: t15.2023.10.01 val PER: 0.2021
2026-01-03 12:17:57,047: t15.2023.10.06 val PER: 0.1152
2026-01-03 12:17:57,047: t15.2023.10.08 val PER: 0.2774
2026-01-03 12:17:57,047: t15.2023.10.13 val PER: 0.2389
2026-01-03 12:17:57,047: t15.2023.10.15 val PER: 0.1892
2026-01-03 12:17:57,047: t15.2023.10.20 val PER: 0.2148
2026-01-03 12:17:57,047: t15.2023.10.22 val PER: 0.1448
2026-01-03 12:17:57,047: t15.2023.11.03 val PER: 0.2090
2026-01-03 12:17:57,047: t15.2023.11.04 val PER: 0.0341
2026-01-03 12:17:57,047: t15.2023.11.17 val PER: 0.0560
2026-01-03 12:17:57,047: t15.2023.11.19 val PER: 0.0659
2026-01-03 12:17:57,048: t15.2023.11.26 val PER: 0.1877
2026-01-03 12:17:57,048: t15.2023.12.03 val PER: 0.1502
2026-01-03 12:17:57,048: t15.2023.12.08 val PER: 0.1478
2026-01-03 12:17:57,048: t15.2023.12.10 val PER: 0.1314
2026-01-03 12:17:57,048: t15.2023.12.17 val PER: 0.1705
2026-01-03 12:17:57,048: t15.2023.12.29 val PER: 0.1791
2026-01-03 12:17:57,048: t15.2024.02.25 val PER: 0.1461
2026-01-03 12:17:57,048: t15.2024.03.08 val PER: 0.2632
2026-01-03 12:17:57,048: t15.2024.03.15 val PER: 0.2370
2026-01-03 12:17:57,048: t15.2024.03.17 val PER: 0.1778
2026-01-03 12:17:57,048: t15.2024.05.10 val PER: 0.1902
2026-01-03 12:17:57,049: t15.2024.06.14 val PER: 0.2019
2026-01-03 12:17:57,049: t15.2024.07.19 val PER: 0.2920
2026-01-03 12:17:57,049: t15.2024.07.21 val PER: 0.1269
2026-01-03 12:17:57,049: t15.2024.07.28 val PER: 0.1647
2026-01-03 12:17:57,049: t15.2025.01.10 val PER: 0.3444
2026-01-03 12:17:57,049: t15.2025.01.12 val PER: 0.1832
2026-01-03 12:17:57,049: t15.2025.03.14 val PER: 0.3521
2026-01-03 12:17:57,049: t15.2025.03.16 val PER: 0.2238
2026-01-03 12:17:57,049: t15.2025.03.30 val PER: 0.3391
2026-01-03 12:17:57,049: t15.2025.04.13 val PER: 0.2725
2026-01-03 12:17:57,297: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_8000
2026-01-03 12:18:14,681: Train batch 8200: loss: 9.85 grad norm: 45.32 time: 0.054
2026-01-03 12:18:32,169: Train batch 8400: loss: 10.30 grad norm: 49.05 time: 0.064
2026-01-03 12:18:41,026: Running test after training batch: 8500
2026-01-03 12:18:41,123: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:18:45,840: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 12:18:45,871: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost ent
2026-01-03 12:18:47,530: Val batch 8500: PER (avg): 0.1812 CTC Loss (avg): 17.8395 WER(1gram): 51.27% (n=64) time: 6.504
2026-01-03 12:18:47,531: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 12:18:47,531: t15.2023.08.13 val PER: 0.1341
2026-01-03 12:18:47,531: t15.2023.08.18 val PER: 0.1358
2026-01-03 12:18:47,531: t15.2023.08.20 val PER: 0.1398
2026-01-03 12:18:47,531: t15.2023.08.25 val PER: 0.1145
2026-01-03 12:18:47,531: t15.2023.08.27 val PER: 0.2170
2026-01-03 12:18:47,531: t15.2023.09.01 val PER: 0.1047
2026-01-03 12:18:47,531: t15.2023.09.03 val PER: 0.1924
2026-01-03 12:18:47,532: t15.2023.09.24 val PER: 0.1541
2026-01-03 12:18:47,532: t15.2023.09.29 val PER: 0.1468
2026-01-03 12:18:47,532: t15.2023.10.01 val PER: 0.1988
2026-01-03 12:18:47,532: t15.2023.10.06 val PER: 0.1163
2026-01-03 12:18:47,532: t15.2023.10.08 val PER: 0.2639
2026-01-03 12:18:47,532: t15.2023.10.13 val PER: 0.2428
2026-01-03 12:18:47,532: t15.2023.10.15 val PER: 0.1905
2026-01-03 12:18:47,532: t15.2023.10.20 val PER: 0.1879
2026-01-03 12:18:47,532: t15.2023.10.22 val PER: 0.1526
2026-01-03 12:18:47,532: t15.2023.11.03 val PER: 0.2008
2026-01-03 12:18:47,532: t15.2023.11.04 val PER: 0.0444
2026-01-03 12:18:47,532: t15.2023.11.17 val PER: 0.0529
2026-01-03 12:18:47,533: t15.2023.11.19 val PER: 0.0479
2026-01-03 12:18:47,533: t15.2023.11.26 val PER: 0.1688
2026-01-03 12:18:47,533: t15.2023.12.03 val PER: 0.1471
2026-01-03 12:18:47,533: t15.2023.12.08 val PER: 0.1451
2026-01-03 12:18:47,533: t15.2023.12.10 val PER: 0.1170
2026-01-03 12:18:47,533: t15.2023.12.17 val PER: 0.1653
2026-01-03 12:18:47,533: t15.2023.12.29 val PER: 0.1730
2026-01-03 12:18:47,533: t15.2024.02.25 val PER: 0.1362
2026-01-03 12:18:47,533: t15.2024.03.08 val PER: 0.2660
2026-01-03 12:18:47,533: t15.2024.03.15 val PER: 0.2364
2026-01-03 12:18:47,533: t15.2024.03.17 val PER: 0.1785
2026-01-03 12:18:47,533: t15.2024.05.10 val PER: 0.1902
2026-01-03 12:18:47,533: t15.2024.06.14 val PER: 0.1877
2026-01-03 12:18:47,533: t15.2024.07.19 val PER: 0.2762
2026-01-03 12:18:47,533: t15.2024.07.21 val PER: 0.1228
2026-01-03 12:18:47,533: t15.2024.07.28 val PER: 0.1640
2026-01-03 12:18:47,533: t15.2025.01.10 val PER: 0.3182
2026-01-03 12:18:47,534: t15.2025.01.12 val PER: 0.1794
2026-01-03 12:18:47,534: t15.2025.03.14 val PER: 0.3565
2026-01-03 12:18:47,534: t15.2025.03.16 val PER: 0.2199
2026-01-03 12:18:47,534: t15.2025.03.30 val PER: 0.3322
2026-01-03 12:18:47,534: t15.2025.04.13 val PER: 0.2425
2026-01-03 12:18:47,535: New best val WER(1gram) 53.30% --> 51.27%
2026-01-03 12:18:47,535: Checkpointing model
2026-01-03 12:18:47,819: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/best_checkpoint
2026-01-03 12:18:48,076: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_8500
2026-01-03 12:18:56,702: Train batch 8600: loss: 16.33 grad norm: 62.15 time: 0.054
2026-01-03 12:19:13,549: Train batch 8800: loss: 15.34 grad norm: 58.00 time: 0.060
2026-01-03 12:19:30,872: Train batch 9000: loss: 16.20 grad norm: 63.95 time: 0.071
2026-01-03 12:19:30,873: Running test after training batch: 9000
2026-01-03 12:19:30,989: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:19:35,751: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:19:35,781: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost nett
2026-01-03 12:19:37,467: Val batch 9000: PER (avg): 0.1741 CTC Loss (avg): 17.2643 WER(1gram): 52.79% (n=64) time: 6.594
2026-01-03 12:19:37,467: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 12:19:37,468: t15.2023.08.13 val PER: 0.1331
2026-01-03 12:19:37,468: t15.2023.08.18 val PER: 0.1266
2026-01-03 12:19:37,468: t15.2023.08.20 val PER: 0.1350
2026-01-03 12:19:37,468: t15.2023.08.25 val PER: 0.0994
2026-01-03 12:19:37,468: t15.2023.08.27 val PER: 0.2138
2026-01-03 12:19:37,468: t15.2023.09.01 val PER: 0.0998
2026-01-03 12:19:37,468: t15.2023.09.03 val PER: 0.1817
2026-01-03 12:19:37,468: t15.2023.09.24 val PER: 0.1444
2026-01-03 12:19:37,468: t15.2023.09.29 val PER: 0.1436
2026-01-03 12:19:37,468: t15.2023.10.01 val PER: 0.1816
2026-01-03 12:19:37,469: t15.2023.10.06 val PER: 0.0980
2026-01-03 12:19:37,469: t15.2023.10.08 val PER: 0.2544
2026-01-03 12:19:37,469: t15.2023.10.13 val PER: 0.2358
2026-01-03 12:19:37,469: t15.2023.10.15 val PER: 0.1767
2026-01-03 12:19:37,469: t15.2023.10.20 val PER: 0.1946
2026-01-03 12:19:37,469: t15.2023.10.22 val PER: 0.1336
2026-01-03 12:19:37,469: t15.2023.11.03 val PER: 0.2015
2026-01-03 12:19:37,469: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:19:37,469: t15.2023.11.17 val PER: 0.0544
2026-01-03 12:19:37,469: t15.2023.11.19 val PER: 0.0459
2026-01-03 12:19:37,469: t15.2023.11.26 val PER: 0.1703
2026-01-03 12:19:37,469: t15.2023.12.03 val PER: 0.1355
2026-01-03 12:19:37,469: t15.2023.12.08 val PER: 0.1325
2026-01-03 12:19:37,469: t15.2023.12.10 val PER: 0.1196
2026-01-03 12:19:37,470: t15.2023.12.17 val PER: 0.1674
2026-01-03 12:19:37,470: t15.2023.12.29 val PER: 0.1620
2026-01-03 12:19:37,470: t15.2024.02.25 val PER: 0.1433
2026-01-03 12:19:37,470: t15.2024.03.08 val PER: 0.2560
2026-01-03 12:19:37,470: t15.2024.03.15 val PER: 0.2258
2026-01-03 12:19:37,470: t15.2024.03.17 val PER: 0.1729
2026-01-03 12:19:37,470: t15.2024.05.10 val PER: 0.1917
2026-01-03 12:19:37,470: t15.2024.06.14 val PER: 0.1735
2026-01-03 12:19:37,470: t15.2024.07.19 val PER: 0.2670
2026-01-03 12:19:37,470: t15.2024.07.21 val PER: 0.1152
2026-01-03 12:19:37,470: t15.2024.07.28 val PER: 0.1588
2026-01-03 12:19:37,470: t15.2025.01.10 val PER: 0.3072
2026-01-03 12:19:37,470: t15.2025.01.12 val PER: 0.1747
2026-01-03 12:19:37,470: t15.2025.03.14 val PER: 0.3565
2026-01-03 12:19:37,470: t15.2025.03.16 val PER: 0.2186
2026-01-03 12:19:37,471: t15.2025.03.30 val PER: 0.3207
2026-01-03 12:19:37,471: t15.2025.04.13 val PER: 0.2340
2026-01-03 12:19:37,740: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_9000
2026-01-03 12:19:54,606: Train batch 9200: loss: 10.65 grad norm: 50.33 time: 0.055
2026-01-03 12:20:11,725: Train batch 9400: loss: 8.10 grad norm: 46.98 time: 0.067
2026-01-03 12:20:20,423: Running test after training batch: 9500
2026-01-03 12:20:20,566: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:20:25,584: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:20:25,615: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-03 12:20:27,322: Val batch 9500: PER (avg): 0.1739 CTC Loss (avg): 17.2372 WER(1gram): 50.51% (n=64) time: 6.898
2026-01-03 12:20:27,322: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 12:20:27,322: t15.2023.08.13 val PER: 0.1331
2026-01-03 12:20:27,323: t15.2023.08.18 val PER: 0.1282
2026-01-03 12:20:27,323: t15.2023.08.20 val PER: 0.1303
2026-01-03 12:20:27,323: t15.2023.08.25 val PER: 0.0904
2026-01-03 12:20:27,323: t15.2023.08.27 val PER: 0.1961
2026-01-03 12:20:27,323: t15.2023.09.01 val PER: 0.0950
2026-01-03 12:20:27,323: t15.2023.09.03 val PER: 0.1793
2026-01-03 12:20:27,323: t15.2023.09.24 val PER: 0.1481
2026-01-03 12:20:27,323: t15.2023.09.29 val PER: 0.1461
2026-01-03 12:20:27,323: t15.2023.10.01 val PER: 0.1896
2026-01-03 12:20:27,323: t15.2023.10.06 val PER: 0.1033
2026-01-03 12:20:27,323: t15.2023.10.08 val PER: 0.2666
2026-01-03 12:20:27,324: t15.2023.10.13 val PER: 0.2320
2026-01-03 12:20:27,324: t15.2023.10.15 val PER: 0.1826
2026-01-03 12:20:27,324: t15.2023.10.20 val PER: 0.1946
2026-01-03 12:20:27,324: t15.2023.10.22 val PER: 0.1303
2026-01-03 12:20:27,324: t15.2023.11.03 val PER: 0.1859
2026-01-03 12:20:27,324: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:20:27,324: t15.2023.11.17 val PER: 0.0575
2026-01-03 12:20:27,324: t15.2023.11.19 val PER: 0.0459
2026-01-03 12:20:27,324: t15.2023.11.26 val PER: 0.1609
2026-01-03 12:20:27,324: t15.2023.12.03 val PER: 0.1355
2026-01-03 12:20:27,324: t15.2023.12.08 val PER: 0.1358
2026-01-03 12:20:27,324: t15.2023.12.10 val PER: 0.1183
2026-01-03 12:20:27,324: t15.2023.12.17 val PER: 0.1559
2026-01-03 12:20:27,324: t15.2023.12.29 val PER: 0.1585
2026-01-03 12:20:27,324: t15.2024.02.25 val PER: 0.1208
2026-01-03 12:20:27,324: t15.2024.03.08 val PER: 0.2575
2026-01-03 12:20:27,324: t15.2024.03.15 val PER: 0.2245
2026-01-03 12:20:27,325: t15.2024.03.17 val PER: 0.1660
2026-01-03 12:20:27,325: t15.2024.05.10 val PER: 0.1917
2026-01-03 12:20:27,325: t15.2024.06.14 val PER: 0.1814
2026-01-03 12:20:27,325: t15.2024.07.19 val PER: 0.2788
2026-01-03 12:20:27,326: t15.2024.07.21 val PER: 0.1193
2026-01-03 12:20:27,326: t15.2024.07.28 val PER: 0.1574
2026-01-03 12:20:27,326: t15.2025.01.10 val PER: 0.3196
2026-01-03 12:20:27,326: t15.2025.01.12 val PER: 0.1817
2026-01-03 12:20:27,326: t15.2025.03.14 val PER: 0.3565
2026-01-03 12:20:27,326: t15.2025.03.16 val PER: 0.2277
2026-01-03 12:20:27,326: t15.2025.03.30 val PER: 0.3253
2026-01-03 12:20:27,326: t15.2025.04.13 val PER: 0.2354
2026-01-03 12:20:27,327: New best val WER(1gram) 51.27% --> 50.51%
2026-01-03 12:20:27,327: Checkpointing model
2026-01-03 12:20:27,600: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/best_checkpoint
2026-01-03 12:20:27,874: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_9500
2026-01-03 12:20:37,044: Train batch 9600: loss: 8.31 grad norm: 47.12 time: 0.072
2026-01-03 12:20:54,603: Train batch 9800: loss: 12.67 grad norm: 62.18 time: 0.063
2026-01-03 12:21:11,661: Train batch 10000: loss: 5.26 grad norm: 35.01 time: 0.061
2026-01-03 12:21:11,662: Running test after training batch: 10000
2026-01-03 12:21:11,783: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:21:16,461: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:21:16,492: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sindt
2026-01-03 12:21:18,201: Val batch 10000: PER (avg): 0.1704 CTC Loss (avg): 17.0120 WER(1gram): 53.81% (n=64) time: 6.539
2026-01-03 12:21:18,201: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 12:21:18,201: t15.2023.08.13 val PER: 0.1331
2026-01-03 12:21:18,202: t15.2023.08.18 val PER: 0.1266
2026-01-03 12:21:18,202: t15.2023.08.20 val PER: 0.1287
2026-01-03 12:21:18,202: t15.2023.08.25 val PER: 0.1145
2026-01-03 12:21:18,202: t15.2023.08.27 val PER: 0.1945
2026-01-03 12:21:18,202: t15.2023.09.01 val PER: 0.0893
2026-01-03 12:21:18,202: t15.2023.09.03 val PER: 0.1770
2026-01-03 12:21:18,202: t15.2023.09.24 val PER: 0.1468
2026-01-03 12:21:18,202: t15.2023.09.29 val PER: 0.1455
2026-01-03 12:21:18,202: t15.2023.10.01 val PER: 0.1777
2026-01-03 12:21:18,202: t15.2023.10.06 val PER: 0.1044
2026-01-03 12:21:18,202: t15.2023.10.08 val PER: 0.2693
2026-01-03 12:21:18,202: t15.2023.10.13 val PER: 0.2289
2026-01-03 12:21:18,202: t15.2023.10.15 val PER: 0.1655
2026-01-03 12:21:18,202: t15.2023.10.20 val PER: 0.2013
2026-01-03 12:21:18,203: t15.2023.10.22 val PER: 0.1281
2026-01-03 12:21:18,203: t15.2023.11.03 val PER: 0.1940
2026-01-03 12:21:18,203: t15.2023.11.04 val PER: 0.0410
2026-01-03 12:21:18,207: t15.2023.11.17 val PER: 0.0513
2026-01-03 12:21:18,207: t15.2023.11.19 val PER: 0.0459
2026-01-03 12:21:18,207: t15.2023.11.26 val PER: 0.1543
2026-01-03 12:21:18,207: t15.2023.12.03 val PER: 0.1397
2026-01-03 12:21:18,207: t15.2023.12.08 val PER: 0.1365
2026-01-03 12:21:18,207: t15.2023.12.10 val PER: 0.1170
2026-01-03 12:21:18,207: t15.2023.12.17 val PER: 0.1663
2026-01-03 12:21:18,207: t15.2023.12.29 val PER: 0.1558
2026-01-03 12:21:18,207: t15.2024.02.25 val PER: 0.1390
2026-01-03 12:21:18,207: t15.2024.03.08 val PER: 0.2504
2026-01-03 12:21:18,207: t15.2024.03.15 val PER: 0.2151
2026-01-03 12:21:18,207: t15.2024.03.17 val PER: 0.1590
2026-01-03 12:21:18,207: t15.2024.05.10 val PER: 0.1679
2026-01-03 12:21:18,207: t15.2024.06.14 val PER: 0.1735
2026-01-03 12:21:18,207: t15.2024.07.19 val PER: 0.2683
2026-01-03 12:21:18,208: t15.2024.07.21 val PER: 0.1090
2026-01-03 12:21:18,208: t15.2024.07.28 val PER: 0.1478
2026-01-03 12:21:18,208: t15.2025.01.10 val PER: 0.3113
2026-01-03 12:21:18,208: t15.2025.01.12 val PER: 0.1724
2026-01-03 12:21:18,208: t15.2025.03.14 val PER: 0.3565
2026-01-03 12:21:18,208: t15.2025.03.16 val PER: 0.2212
2026-01-03 12:21:18,208: t15.2025.03.30 val PER: 0.3218
2026-01-03 12:21:18,208: t15.2025.04.13 val PER: 0.2354
2026-01-03 12:21:18,475: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_10000
2026-01-03 12:21:35,309: Train batch 10200: loss: 6.61 grad norm: 38.57 time: 0.050
2026-01-03 12:21:52,428: Train batch 10400: loss: 9.26 grad norm: 47.81 time: 0.071
2026-01-03 12:22:01,003: Running test after training batch: 10500
2026-01-03 12:22:01,128: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:22:05,833: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the cold at this point as will
2026-01-03 12:22:05,865: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 12:22:07,619: Val batch 10500: PER (avg): 0.1661 CTC Loss (avg): 16.5085 WER(1gram): 52.03% (n=64) time: 6.615
2026-01-03 12:22:07,619: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 12:22:07,619: t15.2023.08.13 val PER: 0.1299
2026-01-03 12:22:07,619: t15.2023.08.18 val PER: 0.1115
2026-01-03 12:22:07,620: t15.2023.08.20 val PER: 0.1295
2026-01-03 12:22:07,620: t15.2023.08.25 val PER: 0.1054
2026-01-03 12:22:07,620: t15.2023.08.27 val PER: 0.2010
2026-01-03 12:22:07,620: t15.2023.09.01 val PER: 0.0933
2026-01-03 12:22:07,620: t15.2023.09.03 val PER: 0.1793
2026-01-03 12:22:07,620: t15.2023.09.24 val PER: 0.1371
2026-01-03 12:22:07,620: t15.2023.09.29 val PER: 0.1391
2026-01-03 12:22:07,620: t15.2023.10.01 val PER: 0.1823
2026-01-03 12:22:07,620: t15.2023.10.06 val PER: 0.1012
2026-01-03 12:22:07,620: t15.2023.10.08 val PER: 0.2612
2026-01-03 12:22:07,620: t15.2023.10.13 val PER: 0.2234
2026-01-03 12:22:07,620: t15.2023.10.15 val PER: 0.1688
2026-01-03 12:22:07,621: t15.2023.10.20 val PER: 0.1980
2026-01-03 12:22:07,621: t15.2023.10.22 val PER: 0.1169
2026-01-03 12:22:07,621: t15.2023.11.03 val PER: 0.1811
2026-01-03 12:22:07,621: t15.2023.11.04 val PER: 0.0444
2026-01-03 12:22:07,621: t15.2023.11.17 val PER: 0.0575
2026-01-03 12:22:07,621: t15.2023.11.19 val PER: 0.0499
2026-01-03 12:22:07,621: t15.2023.11.26 val PER: 0.1413
2026-01-03 12:22:07,621: t15.2023.12.03 val PER: 0.1408
2026-01-03 12:22:07,621: t15.2023.12.08 val PER: 0.1265
2026-01-03 12:22:07,621: t15.2023.12.10 val PER: 0.1064
2026-01-03 12:22:07,621: t15.2023.12.17 val PER: 0.1580
2026-01-03 12:22:07,622: t15.2023.12.29 val PER: 0.1462
2026-01-03 12:22:07,622: t15.2024.02.25 val PER: 0.1306
2026-01-03 12:22:07,622: t15.2024.03.08 val PER: 0.2390
2026-01-03 12:22:07,622: t15.2024.03.15 val PER: 0.2170
2026-01-03 12:22:07,622: t15.2024.03.17 val PER: 0.1534
2026-01-03 12:22:07,622: t15.2024.05.10 val PER: 0.1694
2026-01-03 12:22:07,622: t15.2024.06.14 val PER: 0.1688
2026-01-03 12:22:07,622: t15.2024.07.19 val PER: 0.2584
2026-01-03 12:22:07,622: t15.2024.07.21 val PER: 0.1062
2026-01-03 12:22:07,622: t15.2024.07.28 val PER: 0.1441
2026-01-03 12:22:07,622: t15.2025.01.10 val PER: 0.3182
2026-01-03 12:22:07,622: t15.2025.01.12 val PER: 0.1709
2026-01-03 12:22:07,622: t15.2025.03.14 val PER: 0.3432
2026-01-03 12:22:07,623: t15.2025.03.16 val PER: 0.2199
2026-01-03 12:22:07,623: t15.2025.03.30 val PER: 0.3184
2026-01-03 12:22:07,623: t15.2025.04.13 val PER: 0.2325
2026-01-03 12:22:07,892: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_10500
2026-01-03 12:22:16,516: Train batch 10600: loss: 9.27 grad norm: 53.99 time: 0.072
2026-01-03 12:22:33,435: Train batch 10800: loss: 14.85 grad norm: 62.21 time: 0.064
2026-01-03 12:22:50,553: Train batch 11000: loss: 14.83 grad norm: 61.42 time: 0.056
2026-01-03 12:22:50,554: Running test after training batch: 11000
2026-01-03 12:22:50,660: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:22:55,577: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the cold at this point as will
2026-01-03 12:22:55,609: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 12:22:57,381: Val batch 11000: PER (avg): 0.1651 CTC Loss (avg): 16.3775 WER(1gram): 51.27% (n=64) time: 6.827
2026-01-03 12:22:57,381: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 12:22:57,381: t15.2023.08.13 val PER: 0.1237
2026-01-03 12:22:57,381: t15.2023.08.18 val PER: 0.1148
2026-01-03 12:22:57,381: t15.2023.08.20 val PER: 0.1247
2026-01-03 12:22:57,382: t15.2023.08.25 val PER: 0.0994
2026-01-03 12:22:57,382: t15.2023.08.27 val PER: 0.1977
2026-01-03 12:22:57,382: t15.2023.09.01 val PER: 0.0909
2026-01-03 12:22:57,382: t15.2023.09.03 val PER: 0.1853
2026-01-03 12:22:57,383: t15.2023.09.24 val PER: 0.1383
2026-01-03 12:22:57,383: t15.2023.09.29 val PER: 0.1410
2026-01-03 12:22:57,383: t15.2023.10.01 val PER: 0.1790
2026-01-03 12:22:57,383: t15.2023.10.06 val PER: 0.0947
2026-01-03 12:22:57,383: t15.2023.10.08 val PER: 0.2530
2026-01-03 12:22:57,383: t15.2023.10.13 val PER: 0.2219
2026-01-03 12:22:57,383: t15.2023.10.15 val PER: 0.1688
2026-01-03 12:22:57,384: t15.2023.10.20 val PER: 0.1879
2026-01-03 12:22:57,384: t15.2023.10.22 val PER: 0.1158
2026-01-03 12:22:57,384: t15.2023.11.03 val PER: 0.1872
2026-01-03 12:22:57,384: t15.2023.11.04 val PER: 0.0444
2026-01-03 12:22:57,384: t15.2023.11.17 val PER: 0.0513
2026-01-03 12:22:57,384: t15.2023.11.19 val PER: 0.0499
2026-01-03 12:22:57,384: t15.2023.11.26 val PER: 0.1449
2026-01-03 12:22:57,385: t15.2023.12.03 val PER: 0.1450
2026-01-03 12:22:57,385: t15.2023.12.08 val PER: 0.1232
2026-01-03 12:22:57,385: t15.2023.12.10 val PER: 0.1078
2026-01-03 12:22:57,385: t15.2023.12.17 val PER: 0.1549
2026-01-03 12:22:57,385: t15.2023.12.29 val PER: 0.1421
2026-01-03 12:22:57,385: t15.2024.02.25 val PER: 0.1278
2026-01-03 12:22:57,385: t15.2024.03.08 val PER: 0.2432
2026-01-03 12:22:57,385: t15.2024.03.15 val PER: 0.2133
2026-01-03 12:22:57,385: t15.2024.03.17 val PER: 0.1485
2026-01-03 12:22:57,386: t15.2024.05.10 val PER: 0.1694
2026-01-03 12:22:57,386: t15.2024.06.14 val PER: 0.1751
2026-01-03 12:22:57,386: t15.2024.07.19 val PER: 0.2624
2026-01-03 12:22:57,386: t15.2024.07.21 val PER: 0.1048
2026-01-03 12:22:57,386: t15.2024.07.28 val PER: 0.1463
2026-01-03 12:22:57,386: t15.2025.01.10 val PER: 0.3182
2026-01-03 12:22:57,386: t15.2025.01.12 val PER: 0.1663
2026-01-03 12:22:57,386: t15.2025.03.14 val PER: 0.3462
2026-01-03 12:22:57,386: t15.2025.03.16 val PER: 0.2107
2026-01-03 12:22:57,386: t15.2025.03.30 val PER: 0.3184
2026-01-03 12:22:57,387: t15.2025.04.13 val PER: 0.2311
2026-01-03 12:22:57,647: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_11000
2026-01-03 12:23:14,397: Train batch 11200: loss: 11.05 grad norm: 51.13 time: 0.071
2026-01-03 12:23:31,179: Train batch 11400: loss: 9.93 grad norm: 52.44 time: 0.057
2026-01-03 12:23:39,717: Running test after training batch: 11500
2026-01-03 12:23:39,815: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:23:44,656: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:23:44,688: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 12:23:46,475: Val batch 11500: PER (avg): 0.1647 CTC Loss (avg): 16.3538 WER(1gram): 51.02% (n=64) time: 6.758
2026-01-03 12:23:46,476: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 12:23:46,476: t15.2023.08.13 val PER: 0.1227
2026-01-03 12:23:46,476: t15.2023.08.18 val PER: 0.1148
2026-01-03 12:23:46,476: t15.2023.08.20 val PER: 0.1239
2026-01-03 12:23:46,476: t15.2023.08.25 val PER: 0.1024
2026-01-03 12:23:46,476: t15.2023.08.27 val PER: 0.2010
2026-01-03 12:23:46,476: t15.2023.09.01 val PER: 0.0885
2026-01-03 12:23:46,476: t15.2023.09.03 val PER: 0.1829
2026-01-03 12:23:46,476: t15.2023.09.24 val PER: 0.1359
2026-01-03 12:23:46,477: t15.2023.09.29 val PER: 0.1410
2026-01-03 12:23:46,477: t15.2023.10.01 val PER: 0.1810
2026-01-03 12:23:46,477: t15.2023.10.06 val PER: 0.1001
2026-01-03 12:23:46,477: t15.2023.10.08 val PER: 0.2558
2026-01-03 12:23:46,477: t15.2023.10.13 val PER: 0.2227
2026-01-03 12:23:46,477: t15.2023.10.15 val PER: 0.1688
2026-01-03 12:23:46,477: t15.2023.10.20 val PER: 0.1879
2026-01-03 12:23:46,477: t15.2023.10.22 val PER: 0.1158
2026-01-03 12:23:46,477: t15.2023.11.03 val PER: 0.1839
2026-01-03 12:23:46,477: t15.2023.11.04 val PER: 0.0410
2026-01-03 12:23:46,477: t15.2023.11.17 val PER: 0.0482
2026-01-03 12:23:46,477: t15.2023.11.19 val PER: 0.0559
2026-01-03 12:23:46,477: t15.2023.11.26 val PER: 0.1442
2026-01-03 12:23:46,477: t15.2023.12.03 val PER: 0.1355
2026-01-03 12:23:46,478: t15.2023.12.08 val PER: 0.1238
2026-01-03 12:23:46,478: t15.2023.12.10 val PER: 0.1038
2026-01-03 12:23:46,478: t15.2023.12.17 val PER: 0.1497
2026-01-03 12:23:46,478: t15.2023.12.29 val PER: 0.1448
2026-01-03 12:23:46,478: t15.2024.02.25 val PER: 0.1250
2026-01-03 12:23:46,478: t15.2024.03.08 val PER: 0.2418
2026-01-03 12:23:46,478: t15.2024.03.15 val PER: 0.2151
2026-01-03 12:23:46,478: t15.2024.03.17 val PER: 0.1541
2026-01-03 12:23:46,478: t15.2024.05.10 val PER: 0.1679
2026-01-03 12:23:46,478: t15.2024.06.14 val PER: 0.1703
2026-01-03 12:23:46,478: t15.2024.07.19 val PER: 0.2624
2026-01-03 12:23:46,478: t15.2024.07.21 val PER: 0.1048
2026-01-03 12:23:46,478: t15.2024.07.28 val PER: 0.1493
2026-01-03 12:23:46,479: t15.2025.01.10 val PER: 0.3140
2026-01-03 12:23:46,479: t15.2025.01.12 val PER: 0.1655
2026-01-03 12:23:46,479: t15.2025.03.14 val PER: 0.3373
2026-01-03 12:23:46,479: t15.2025.03.16 val PER: 0.2094
2026-01-03 12:23:46,479: t15.2025.03.30 val PER: 0.3172
2026-01-03 12:23:46,479: t15.2025.04.13 val PER: 0.2311
2026-01-03 12:23:46,751: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_11500
2026-01-03 12:23:55,243: Train batch 11600: loss: 11.81 grad norm: 49.36 time: 0.060
2026-01-03 12:24:12,482: Train batch 11800: loss: 7.19 grad norm: 40.33 time: 0.044
2026-01-03 12:24:29,891: Train batch 12000: loss: 14.62 grad norm: 52.72 time: 0.070
2026-01-03 12:24:29,891: Running test after training batch: 12000
2026-01-03 12:24:29,980: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:24:34,662: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:24:34,694: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-03 12:24:36,492: Val batch 12000: PER (avg): 0.1645 CTC Loss (avg): 16.2699 WER(1gram): 50.76% (n=64) time: 6.600
2026-01-03 12:24:36,492: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 12:24:36,492: t15.2023.08.13 val PER: 0.1268
2026-01-03 12:24:36,492: t15.2023.08.18 val PER: 0.1140
2026-01-03 12:24:36,492: t15.2023.08.20 val PER: 0.1271
2026-01-03 12:24:36,493: t15.2023.08.25 val PER: 0.1009
2026-01-03 12:24:36,493: t15.2023.08.27 val PER: 0.1994
2026-01-03 12:24:36,493: t15.2023.09.01 val PER: 0.0877
2026-01-03 12:24:36,493: t15.2023.09.03 val PER: 0.1793
2026-01-03 12:24:36,493: t15.2023.09.24 val PER: 0.1347
2026-01-03 12:24:36,493: t15.2023.09.29 val PER: 0.1423
2026-01-03 12:24:36,493: t15.2023.10.01 val PER: 0.1803
2026-01-03 12:24:36,493: t15.2023.10.06 val PER: 0.0915
2026-01-03 12:24:36,493: t15.2023.10.08 val PER: 0.2558
2026-01-03 12:24:36,493: t15.2023.10.13 val PER: 0.2188
2026-01-03 12:24:36,493: t15.2023.10.15 val PER: 0.1688
2026-01-03 12:24:36,493: t15.2023.10.20 val PER: 0.1846
2026-01-03 12:24:36,493: t15.2023.10.22 val PER: 0.1147
2026-01-03 12:24:36,494: t15.2023.11.03 val PER: 0.1852
2026-01-03 12:24:36,494: t15.2023.11.04 val PER: 0.0444
2026-01-03 12:24:36,494: t15.2023.11.17 val PER: 0.0482
2026-01-03 12:24:36,494: t15.2023.11.19 val PER: 0.0499
2026-01-03 12:24:36,494: t15.2023.11.26 val PER: 0.1420
2026-01-03 12:24:36,494: t15.2023.12.03 val PER: 0.1355
2026-01-03 12:24:36,494: t15.2023.12.08 val PER: 0.1212
2026-01-03 12:24:36,494: t15.2023.12.10 val PER: 0.1051
2026-01-03 12:24:36,494: t15.2023.12.17 val PER: 0.1549
2026-01-03 12:24:36,494: t15.2023.12.29 val PER: 0.1441
2026-01-03 12:24:36,494: t15.2024.02.25 val PER: 0.1320
2026-01-03 12:24:36,494: t15.2024.03.08 val PER: 0.2361
2026-01-03 12:24:36,494: t15.2024.03.15 val PER: 0.2133
2026-01-03 12:24:36,494: t15.2024.03.17 val PER: 0.1520
2026-01-03 12:24:36,495: t15.2024.05.10 val PER: 0.1679
2026-01-03 12:24:36,495: t15.2024.06.14 val PER: 0.1703
2026-01-03 12:24:36,495: t15.2024.07.19 val PER: 0.2637
2026-01-03 12:24:36,495: t15.2024.07.21 val PER: 0.1069
2026-01-03 12:24:36,495: t15.2024.07.28 val PER: 0.1456
2026-01-03 12:24:36,495: t15.2025.01.10 val PER: 0.3127
2026-01-03 12:24:36,495: t15.2025.01.12 val PER: 0.1663
2026-01-03 12:24:36,495: t15.2025.03.14 val PER: 0.3417
2026-01-03 12:24:36,495: t15.2025.03.16 val PER: 0.2160
2026-01-03 12:24:36,495: t15.2025.03.30 val PER: 0.3230
2026-01-03 12:24:36,495: t15.2025.04.13 val PER: 0.2311
2026-01-03 12:24:36,761: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_12000
2026-01-03 12:24:54,077: Train batch 12200: loss: 6.48 grad norm: 39.88 time: 0.065
2026-01-03 12:25:11,499: Train batch 12400: loss: 5.57 grad norm: 35.66 time: 0.041
2026-01-03 12:25:20,475: Running test after training batch: 12500
2026-01-03 12:25:20,726: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:25:25,561: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:25:25,592: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 12:25:27,365: Val batch 12500: PER (avg): 0.1632 CTC Loss (avg): 16.2283 WER(1gram): 49.24% (n=64) time: 6.890
2026-01-03 12:25:27,366: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 12:25:27,366: t15.2023.08.13 val PER: 0.1247
2026-01-03 12:25:27,366: t15.2023.08.18 val PER: 0.1115
2026-01-03 12:25:27,366: t15.2023.08.20 val PER: 0.1247
2026-01-03 12:25:27,366: t15.2023.08.25 val PER: 0.0919
2026-01-03 12:25:27,366: t15.2023.08.27 val PER: 0.1929
2026-01-03 12:25:27,366: t15.2023.09.01 val PER: 0.0901
2026-01-03 12:25:27,366: t15.2023.09.03 val PER: 0.1781
2026-01-03 12:25:27,366: t15.2023.09.24 val PER: 0.1299
2026-01-03 12:25:27,366: t15.2023.09.29 val PER: 0.1378
2026-01-03 12:25:27,366: t15.2023.10.01 val PER: 0.1777
2026-01-03 12:25:27,366: t15.2023.10.06 val PER: 0.0926
2026-01-03 12:25:27,367: t15.2023.10.08 val PER: 0.2517
2026-01-03 12:25:27,367: t15.2023.10.13 val PER: 0.2149
2026-01-03 12:25:27,367: t15.2023.10.15 val PER: 0.1701
2026-01-03 12:25:27,367: t15.2023.10.20 val PER: 0.1812
2026-01-03 12:25:27,367: t15.2023.10.22 val PER: 0.1114
2026-01-03 12:25:27,367: t15.2023.11.03 val PER: 0.1818
2026-01-03 12:25:27,367: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:25:27,367: t15.2023.11.17 val PER: 0.0513
2026-01-03 12:25:27,367: t15.2023.11.19 val PER: 0.0499
2026-01-03 12:25:27,367: t15.2023.11.26 val PER: 0.1464
2026-01-03 12:25:27,367: t15.2023.12.03 val PER: 0.1345
2026-01-03 12:25:27,368: t15.2023.12.08 val PER: 0.1225
2026-01-03 12:25:27,368: t15.2023.12.10 val PER: 0.1038
2026-01-03 12:25:27,368: t15.2023.12.17 val PER: 0.1538
2026-01-03 12:25:27,368: t15.2023.12.29 val PER: 0.1407
2026-01-03 12:25:27,368: t15.2024.02.25 val PER: 0.1306
2026-01-03 12:25:27,368: t15.2024.03.08 val PER: 0.2361
2026-01-03 12:25:27,368: t15.2024.03.15 val PER: 0.2151
2026-01-03 12:25:27,368: t15.2024.03.17 val PER: 0.1513
2026-01-03 12:25:27,368: t15.2024.05.10 val PER: 0.1694
2026-01-03 12:25:27,368: t15.2024.06.14 val PER: 0.1767
2026-01-03 12:25:27,368: t15.2024.07.19 val PER: 0.2643
2026-01-03 12:25:27,368: t15.2024.07.21 val PER: 0.1076
2026-01-03 12:25:27,368: t15.2024.07.28 val PER: 0.1419
2026-01-03 12:25:27,368: t15.2025.01.10 val PER: 0.3099
2026-01-03 12:25:27,368: t15.2025.01.12 val PER: 0.1686
2026-01-03 12:25:27,368: t15.2025.03.14 val PER: 0.3462
2026-01-03 12:25:27,369: t15.2025.03.16 val PER: 0.2003
2026-01-03 12:25:27,369: t15.2025.03.30 val PER: 0.3253
2026-01-03 12:25:27,369: t15.2025.04.13 val PER: 0.2225
2026-01-03 12:25:27,370: New best val WER(1gram) 50.51% --> 49.24%
2026-01-03 12:25:27,370: Checkpointing model
2026-01-03 12:25:27,646: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/best_checkpoint
2026-01-03 12:25:27,919: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_12500
2026-01-03 12:25:36,587: Train batch 12600: loss: 8.74 grad norm: 48.34 time: 0.058
2026-01-03 12:25:54,122: Train batch 12800: loss: 6.53 grad norm: 37.87 time: 0.052
2026-01-03 12:26:11,812: Train batch 13000: loss: 7.50 grad norm: 44.43 time: 0.066
2026-01-03 12:26:11,812: Running test after training batch: 13000
2026-01-03 12:26:11,924: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:26:16,590: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:26:16,623: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 12:26:18,438: Val batch 13000: PER (avg): 0.1635 CTC Loss (avg): 16.2248 WER(1gram): 48.22% (n=64) time: 6.625
2026-01-03 12:26:18,438: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 12:26:18,438: t15.2023.08.13 val PER: 0.1299
2026-01-03 12:26:18,438: t15.2023.08.18 val PER: 0.1081
2026-01-03 12:26:18,438: t15.2023.08.20 val PER: 0.1223
2026-01-03 12:26:18,438: t15.2023.08.25 val PER: 0.0994
2026-01-03 12:26:18,439: t15.2023.08.27 val PER: 0.2074
2026-01-03 12:26:18,439: t15.2023.09.01 val PER: 0.0909
2026-01-03 12:26:18,439: t15.2023.09.03 val PER: 0.1805
2026-01-03 12:26:18,439: t15.2023.09.24 val PER: 0.1311
2026-01-03 12:26:18,439: t15.2023.09.29 val PER: 0.1417
2026-01-03 12:26:18,439: t15.2023.10.01 val PER: 0.1783
2026-01-03 12:26:18,439: t15.2023.10.06 val PER: 0.0958
2026-01-03 12:26:18,439: t15.2023.10.08 val PER: 0.2544
2026-01-03 12:26:18,439: t15.2023.10.13 val PER: 0.2250
2026-01-03 12:26:18,439: t15.2023.10.15 val PER: 0.1668
2026-01-03 12:26:18,439: t15.2023.10.20 val PER: 0.1846
2026-01-03 12:26:18,439: t15.2023.10.22 val PER: 0.1136
2026-01-03 12:26:18,439: t15.2023.11.03 val PER: 0.1825
2026-01-03 12:26:18,439: t15.2023.11.04 val PER: 0.0410
2026-01-03 12:26:18,439: t15.2023.11.17 val PER: 0.0451
2026-01-03 12:26:18,440: t15.2023.11.19 val PER: 0.0499
2026-01-03 12:26:18,440: t15.2023.11.26 val PER: 0.1420
2026-01-03 12:26:18,440: t15.2023.12.03 val PER: 0.1366
2026-01-03 12:26:18,440: t15.2023.12.08 val PER: 0.1245
2026-01-03 12:26:18,440: t15.2023.12.10 val PER: 0.1012
2026-01-03 12:26:18,440: t15.2023.12.17 val PER: 0.1580
2026-01-03 12:26:18,440: t15.2023.12.29 val PER: 0.1414
2026-01-03 12:26:18,440: t15.2024.02.25 val PER: 0.1334
2026-01-03 12:26:18,440: t15.2024.03.08 val PER: 0.2333
2026-01-03 12:26:18,440: t15.2024.03.15 val PER: 0.2145
2026-01-03 12:26:18,440: t15.2024.03.17 val PER: 0.1492
2026-01-03 12:26:18,440: t15.2024.05.10 val PER: 0.1664
2026-01-03 12:26:18,440: t15.2024.06.14 val PER: 0.1735
2026-01-03 12:26:18,440: t15.2024.07.19 val PER: 0.2597
2026-01-03 12:26:18,440: t15.2024.07.21 val PER: 0.1028
2026-01-03 12:26:18,440: t15.2024.07.28 val PER: 0.1419
2026-01-03 12:26:18,440: t15.2025.01.10 val PER: 0.3072
2026-01-03 12:26:18,441: t15.2025.01.12 val PER: 0.1578
2026-01-03 12:26:18,441: t15.2025.03.14 val PER: 0.3476
2026-01-03 12:26:18,441: t15.2025.03.16 val PER: 0.2120
2026-01-03 12:26:18,441: t15.2025.03.30 val PER: 0.3207
2026-01-03 12:26:18,441: t15.2025.04.13 val PER: 0.2311
2026-01-03 12:26:18,442: New best val WER(1gram) 49.24% --> 48.22%
2026-01-03 12:26:18,442: Checkpointing model
2026-01-03 12:26:18,721: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/best_checkpoint
2026-01-03 12:26:18,994: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_13000
2026-01-03 12:26:36,633: Train batch 13200: loss: 15.32 grad norm: 64.85 time: 0.054
2026-01-03 12:26:53,483: Train batch 13400: loss: 10.84 grad norm: 56.17 time: 0.061
2026-01-03 12:27:01,934: Running test after training batch: 13500
2026-01-03 12:27:02,047: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:27:06,744: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:27:06,777: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 12:27:08,575: Val batch 13500: PER (avg): 0.1625 CTC Loss (avg): 16.2022 WER(1gram): 48.73% (n=64) time: 6.641
2026-01-03 12:27:08,575: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 12:27:08,575: t15.2023.08.13 val PER: 0.1227
2026-01-03 12:27:08,576: t15.2023.08.18 val PER: 0.1090
2026-01-03 12:27:08,576: t15.2023.08.20 val PER: 0.1231
2026-01-03 12:27:08,576: t15.2023.08.25 val PER: 0.0873
2026-01-03 12:27:08,576: t15.2023.08.27 val PER: 0.1945
2026-01-03 12:27:08,576: t15.2023.09.01 val PER: 0.0860
2026-01-03 12:27:08,576: t15.2023.09.03 val PER: 0.1746
2026-01-03 12:27:08,576: t15.2023.09.24 val PER: 0.1359
2026-01-03 12:27:08,576: t15.2023.09.29 val PER: 0.1398
2026-01-03 12:27:08,576: t15.2023.10.01 val PER: 0.1790
2026-01-03 12:27:08,576: t15.2023.10.06 val PER: 0.0980
2026-01-03 12:27:08,576: t15.2023.10.08 val PER: 0.2612
2026-01-03 12:27:08,576: t15.2023.10.13 val PER: 0.2211
2026-01-03 12:27:08,576: t15.2023.10.15 val PER: 0.1694
2026-01-03 12:27:08,577: t15.2023.10.20 val PER: 0.1846
2026-01-03 12:27:08,577: t15.2023.10.22 val PER: 0.1136
2026-01-03 12:27:08,577: t15.2023.11.03 val PER: 0.1832
2026-01-03 12:27:08,577: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:27:08,577: t15.2023.11.17 val PER: 0.0451
2026-01-03 12:27:08,577: t15.2023.11.19 val PER: 0.0519
2026-01-03 12:27:08,577: t15.2023.11.26 val PER: 0.1399
2026-01-03 12:27:08,577: t15.2023.12.03 val PER: 0.1324
2026-01-03 12:27:08,577: t15.2023.12.08 val PER: 0.1172
2026-01-03 12:27:08,577: t15.2023.12.10 val PER: 0.0972
2026-01-03 12:27:08,577: t15.2023.12.17 val PER: 0.1580
2026-01-03 12:27:08,577: t15.2023.12.29 val PER: 0.1455
2026-01-03 12:27:08,577: t15.2024.02.25 val PER: 0.1292
2026-01-03 12:27:08,577: t15.2024.03.08 val PER: 0.2319
2026-01-03 12:27:08,577: t15.2024.03.15 val PER: 0.2120
2026-01-03 12:27:08,577: t15.2024.03.17 val PER: 0.1492
2026-01-03 12:27:08,577: t15.2024.05.10 val PER: 0.1679
2026-01-03 12:27:08,578: t15.2024.06.14 val PER: 0.1751
2026-01-03 12:27:08,578: t15.2024.07.19 val PER: 0.2591
2026-01-03 12:27:08,578: t15.2024.07.21 val PER: 0.1062
2026-01-03 12:27:08,578: t15.2024.07.28 val PER: 0.1434
2026-01-03 12:27:08,578: t15.2025.01.10 val PER: 0.3127
2026-01-03 12:27:08,578: t15.2025.01.12 val PER: 0.1617
2026-01-03 12:27:08,578: t15.2025.03.14 val PER: 0.3462
2026-01-03 12:27:08,578: t15.2025.03.16 val PER: 0.2055
2026-01-03 12:27:08,578: t15.2025.03.30 val PER: 0.3172
2026-01-03 12:27:08,578: t15.2025.04.13 val PER: 0.2297
2026-01-03 12:27:08,844: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_13500
2026-01-03 12:27:17,671: Train batch 13600: loss: 14.75 grad norm: 64.13 time: 0.062
2026-01-03 12:27:35,412: Train batch 13800: loss: 10.84 grad norm: 55.22 time: 0.056
2026-01-03 12:27:53,246: Train batch 14000: loss: 13.74 grad norm: 60.04 time: 0.050
2026-01-03 12:27:53,246: Running test after training batch: 14000
2026-01-03 12:27:53,407: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:27:58,111: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:27:58,144: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 12:27:59,983: Val batch 14000: PER (avg): 0.1627 CTC Loss (avg): 16.1615 WER(1gram): 49.49% (n=64) time: 6.737
2026-01-03 12:27:59,983: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 12:27:59,984: t15.2023.08.13 val PER: 0.1268
2026-01-03 12:27:59,984: t15.2023.08.18 val PER: 0.1115
2026-01-03 12:27:59,984: t15.2023.08.20 val PER: 0.1255
2026-01-03 12:27:59,984: t15.2023.08.25 val PER: 0.0949
2026-01-03 12:27:59,984: t15.2023.08.27 val PER: 0.1977
2026-01-03 12:27:59,984: t15.2023.09.01 val PER: 0.0877
2026-01-03 12:27:59,984: t15.2023.09.03 val PER: 0.1758
2026-01-03 12:27:59,984: t15.2023.09.24 val PER: 0.1262
2026-01-03 12:27:59,984: t15.2023.09.29 val PER: 0.1359
2026-01-03 12:27:59,984: t15.2023.10.01 val PER: 0.1724
2026-01-03 12:27:59,984: t15.2023.10.06 val PER: 0.0990
2026-01-03 12:27:59,984: t15.2023.10.08 val PER: 0.2571
2026-01-03 12:27:59,984: t15.2023.10.13 val PER: 0.2196
2026-01-03 12:27:59,984: t15.2023.10.15 val PER: 0.1674
2026-01-03 12:27:59,985: t15.2023.10.20 val PER: 0.1812
2026-01-03 12:27:59,985: t15.2023.10.22 val PER: 0.1169
2026-01-03 12:27:59,985: t15.2023.11.03 val PER: 0.1845
2026-01-03 12:27:59,985: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:27:59,985: t15.2023.11.17 val PER: 0.0513
2026-01-03 12:27:59,985: t15.2023.11.19 val PER: 0.0479
2026-01-03 12:27:59,985: t15.2023.11.26 val PER: 0.1442
2026-01-03 12:27:59,985: t15.2023.12.03 val PER: 0.1345
2026-01-03 12:27:59,985: t15.2023.12.08 val PER: 0.1158
2026-01-03 12:27:59,985: t15.2023.12.10 val PER: 0.1038
2026-01-03 12:27:59,985: t15.2023.12.17 val PER: 0.1549
2026-01-03 12:27:59,985: t15.2023.12.29 val PER: 0.1421
2026-01-03 12:27:59,985: t15.2024.02.25 val PER: 0.1264
2026-01-03 12:27:59,985: t15.2024.03.08 val PER: 0.2290
2026-01-03 12:27:59,985: t15.2024.03.15 val PER: 0.2201
2026-01-03 12:27:59,986: t15.2024.03.17 val PER: 0.1506
2026-01-03 12:27:59,986: t15.2024.05.10 val PER: 0.1679
2026-01-03 12:27:59,986: t15.2024.06.14 val PER: 0.1719
2026-01-03 12:27:59,986: t15.2024.07.19 val PER: 0.2564
2026-01-03 12:27:59,986: t15.2024.07.21 val PER: 0.1048
2026-01-03 12:27:59,986: t15.2024.07.28 val PER: 0.1471
2026-01-03 12:27:59,986: t15.2025.01.10 val PER: 0.3154
2026-01-03 12:27:59,986: t15.2025.01.12 val PER: 0.1640
2026-01-03 12:27:59,986: t15.2025.03.14 val PER: 0.3402
2026-01-03 12:27:59,986: t15.2025.03.16 val PER: 0.2068
2026-01-03 12:27:59,986: t15.2025.03.30 val PER: 0.3218
2026-01-03 12:27:59,986: t15.2025.04.13 val PER: 0.2240
2026-01-03 12:28:00,250: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_14000
2026-01-03 12:28:17,878: Train batch 14200: loss: 9.92 grad norm: 49.75 time: 0.056
2026-01-03 12:28:35,476: Train batch 14400: loss: 6.80 grad norm: 40.27 time: 0.064
2026-01-03 12:28:44,360: Running test after training batch: 14500
2026-01-03 12:28:44,464: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:28:49,172: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:28:49,205: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 12:28:51,019: Val batch 14500: PER (avg): 0.1621 CTC Loss (avg): 16.1406 WER(1gram): 48.98% (n=64) time: 6.658
2026-01-03 12:28:51,020: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 12:28:51,020: t15.2023.08.13 val PER: 0.1237
2026-01-03 12:28:51,020: t15.2023.08.18 val PER: 0.1115
2026-01-03 12:28:51,020: t15.2023.08.20 val PER: 0.1191
2026-01-03 12:28:51,020: t15.2023.08.25 val PER: 0.0858
2026-01-03 12:28:51,020: t15.2023.08.27 val PER: 0.1945
2026-01-03 12:28:51,020: t15.2023.09.01 val PER: 0.0869
2026-01-03 12:28:51,020: t15.2023.09.03 val PER: 0.1793
2026-01-03 12:28:51,020: t15.2023.09.24 val PER: 0.1323
2026-01-03 12:28:51,020: t15.2023.09.29 val PER: 0.1359
2026-01-03 12:28:51,021: t15.2023.10.01 val PER: 0.1757
2026-01-03 12:28:51,021: t15.2023.10.06 val PER: 0.0936
2026-01-03 12:28:51,021: t15.2023.10.08 val PER: 0.2530
2026-01-03 12:28:51,021: t15.2023.10.13 val PER: 0.2188
2026-01-03 12:28:51,021: t15.2023.10.15 val PER: 0.1674
2026-01-03 12:28:51,021: t15.2023.10.20 val PER: 0.1779
2026-01-03 12:28:51,021: t15.2023.10.22 val PER: 0.1158
2026-01-03 12:28:51,021: t15.2023.11.03 val PER: 0.1920
2026-01-03 12:28:51,021: t15.2023.11.04 val PER: 0.0341
2026-01-03 12:28:51,021: t15.2023.11.17 val PER: 0.0482
2026-01-03 12:28:51,021: t15.2023.11.19 val PER: 0.0439
2026-01-03 12:28:51,021: t15.2023.11.26 val PER: 0.1435
2026-01-03 12:28:51,021: t15.2023.12.03 val PER: 0.1334
2026-01-03 12:28:51,022: t15.2023.12.08 val PER: 0.1172
2026-01-03 12:28:51,022: t15.2023.12.10 val PER: 0.0972
2026-01-03 12:28:51,022: t15.2023.12.17 val PER: 0.1518
2026-01-03 12:28:51,022: t15.2023.12.29 val PER: 0.1400
2026-01-03 12:28:51,022: t15.2024.02.25 val PER: 0.1264
2026-01-03 12:28:51,022: t15.2024.03.08 val PER: 0.2276
2026-01-03 12:28:51,022: t15.2024.03.15 val PER: 0.2120
2026-01-03 12:28:51,022: t15.2024.03.17 val PER: 0.1527
2026-01-03 12:28:51,022: t15.2024.05.10 val PER: 0.1679
2026-01-03 12:28:51,022: t15.2024.06.14 val PER: 0.1735
2026-01-03 12:28:51,023: t15.2024.07.19 val PER: 0.2597
2026-01-03 12:28:51,023: t15.2024.07.21 val PER: 0.1055
2026-01-03 12:28:51,023: t15.2024.07.28 val PER: 0.1434
2026-01-03 12:28:51,023: t15.2025.01.10 val PER: 0.3140
2026-01-03 12:28:51,023: t15.2025.01.12 val PER: 0.1601
2026-01-03 12:28:51,023: t15.2025.03.14 val PER: 0.3462
2026-01-03 12:28:51,023: t15.2025.03.16 val PER: 0.2134
2026-01-03 12:28:51,023: t15.2025.03.30 val PER: 0.3241
2026-01-03 12:28:51,023: t15.2025.04.13 val PER: 0.2225
2026-01-03 12:28:51,293: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_14500
2026-01-03 12:29:00,024: Train batch 14600: loss: 14.54 grad norm: 60.46 time: 0.058
2026-01-03 12:29:17,572: Train batch 14800: loss: 7.31 grad norm: 45.88 time: 0.050
2026-01-03 12:29:35,270: Train batch 15000: loss: 10.81 grad norm: 54.53 time: 0.052
2026-01-03 12:29:35,271: Running test after training batch: 15000
2026-01-03 12:29:35,368: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:29:40,240: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:29:40,273: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 12:29:42,131: Val batch 15000: PER (avg): 0.1621 CTC Loss (avg): 16.0919 WER(1gram): 49.49% (n=64) time: 6.860
2026-01-03 12:29:42,132: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 12:29:42,132: t15.2023.08.13 val PER: 0.1206
2026-01-03 12:29:42,132: t15.2023.08.18 val PER: 0.1081
2026-01-03 12:29:42,132: t15.2023.08.20 val PER: 0.1199
2026-01-03 12:29:42,132: t15.2023.08.25 val PER: 0.0904
2026-01-03 12:29:42,132: t15.2023.08.27 val PER: 0.1929
2026-01-03 12:29:42,133: t15.2023.09.01 val PER: 0.0877
2026-01-03 12:29:42,133: t15.2023.09.03 val PER: 0.1829
2026-01-03 12:29:42,133: t15.2023.09.24 val PER: 0.1323
2026-01-03 12:29:42,133: t15.2023.09.29 val PER: 0.1378
2026-01-03 12:29:42,133: t15.2023.10.01 val PER: 0.1731
2026-01-03 12:29:42,133: t15.2023.10.06 val PER: 0.0936
2026-01-03 12:29:42,133: t15.2023.10.08 val PER: 0.2544
2026-01-03 12:29:42,133: t15.2023.10.13 val PER: 0.2219
2026-01-03 12:29:42,133: t15.2023.10.15 val PER: 0.1661
2026-01-03 12:29:42,133: t15.2023.10.20 val PER: 0.1779
2026-01-03 12:29:42,133: t15.2023.10.22 val PER: 0.1147
2026-01-03 12:29:42,133: t15.2023.11.03 val PER: 0.1886
2026-01-03 12:29:42,134: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:29:42,134: t15.2023.11.17 val PER: 0.0560
2026-01-03 12:29:42,134: t15.2023.11.19 val PER: 0.0439
2026-01-03 12:29:42,134: t15.2023.11.26 val PER: 0.1442
2026-01-03 12:29:42,134: t15.2023.12.03 val PER: 0.1313
2026-01-03 12:29:42,134: t15.2023.12.08 val PER: 0.1112
2026-01-03 12:29:42,134: t15.2023.12.10 val PER: 0.1012
2026-01-03 12:29:42,134: t15.2023.12.17 val PER: 0.1549
2026-01-03 12:29:42,134: t15.2023.12.29 val PER: 0.1414
2026-01-03 12:29:42,134: t15.2024.02.25 val PER: 0.1278
2026-01-03 12:29:42,134: t15.2024.03.08 val PER: 0.2319
2026-01-03 12:29:42,134: t15.2024.03.15 val PER: 0.2139
2026-01-03 12:29:42,134: t15.2024.03.17 val PER: 0.1520
2026-01-03 12:29:42,134: t15.2024.05.10 val PER: 0.1724
2026-01-03 12:29:42,134: t15.2024.06.14 val PER: 0.1719
2026-01-03 12:29:42,134: t15.2024.07.19 val PER: 0.2597
2026-01-03 12:29:42,135: t15.2024.07.21 val PER: 0.1062
2026-01-03 12:29:42,135: t15.2024.07.28 val PER: 0.1426
2026-01-03 12:29:42,135: t15.2025.01.10 val PER: 0.3154
2026-01-03 12:29:42,135: t15.2025.01.12 val PER: 0.1632
2026-01-03 12:29:42,135: t15.2025.03.14 val PER: 0.3476
2026-01-03 12:29:42,135: t15.2025.03.16 val PER: 0.2094
2026-01-03 12:29:42,135: t15.2025.03.30 val PER: 0.3172
2026-01-03 12:29:42,135: t15.2025.04.13 val PER: 0.2154
2026-01-03 12:29:42,398: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_15000
2026-01-03 12:29:59,897: Train batch 15200: loss: 6.14 grad norm: 39.57 time: 0.057
2026-01-03 12:30:16,877: Train batch 15400: loss: 13.51 grad norm: 57.12 time: 0.049
2026-01-03 12:30:25,475: Running test after training batch: 15500
2026-01-03 12:30:25,575: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:30:30,204: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:30:30,238: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 12:30:32,116: Val batch 15500: PER (avg): 0.1616 CTC Loss (avg): 16.1034 WER(1gram): 48.22% (n=64) time: 6.641
2026-01-03 12:30:32,117: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 12:30:32,117: t15.2023.08.13 val PER: 0.1206
2026-01-03 12:30:32,117: t15.2023.08.18 val PER: 0.1106
2026-01-03 12:30:32,117: t15.2023.08.20 val PER: 0.1223
2026-01-03 12:30:32,117: t15.2023.08.25 val PER: 0.0934
2026-01-03 12:30:32,117: t15.2023.08.27 val PER: 0.1865
2026-01-03 12:30:32,117: t15.2023.09.01 val PER: 0.0901
2026-01-03 12:30:32,117: t15.2023.09.03 val PER: 0.1817
2026-01-03 12:30:32,117: t15.2023.09.24 val PER: 0.1274
2026-01-03 12:30:32,118: t15.2023.09.29 val PER: 0.1378
2026-01-03 12:30:32,118: t15.2023.10.01 val PER: 0.1744
2026-01-03 12:30:32,118: t15.2023.10.06 val PER: 0.0969
2026-01-03 12:30:32,118: t15.2023.10.08 val PER: 0.2571
2026-01-03 12:30:32,118: t15.2023.10.13 val PER: 0.2203
2026-01-03 12:30:32,118: t15.2023.10.15 val PER: 0.1681
2026-01-03 12:30:32,118: t15.2023.10.20 val PER: 0.1846
2026-01-03 12:30:32,118: t15.2023.10.22 val PER: 0.1136
2026-01-03 12:30:32,118: t15.2023.11.03 val PER: 0.1825
2026-01-03 12:30:32,118: t15.2023.11.04 val PER: 0.0410
2026-01-03 12:30:32,118: t15.2023.11.17 val PER: 0.0544
2026-01-03 12:30:32,119: t15.2023.11.19 val PER: 0.0479
2026-01-03 12:30:32,119: t15.2023.11.26 val PER: 0.1449
2026-01-03 12:30:32,119: t15.2023.12.03 val PER: 0.1324
2026-01-03 12:30:32,119: t15.2023.12.08 val PER: 0.1138
2026-01-03 12:30:32,119: t15.2023.12.10 val PER: 0.0999
2026-01-03 12:30:32,119: t15.2023.12.17 val PER: 0.1466
2026-01-03 12:30:32,119: t15.2023.12.29 val PER: 0.1386
2026-01-03 12:30:32,119: t15.2024.02.25 val PER: 0.1306
2026-01-03 12:30:32,119: t15.2024.03.08 val PER: 0.2276
2026-01-03 12:30:32,119: t15.2024.03.15 val PER: 0.2164
2026-01-03 12:30:32,119: t15.2024.03.17 val PER: 0.1478
2026-01-03 12:30:32,119: t15.2024.05.10 val PER: 0.1664
2026-01-03 12:30:32,119: t15.2024.06.14 val PER: 0.1719
2026-01-03 12:30:32,119: t15.2024.07.19 val PER: 0.2544
2026-01-03 12:30:32,119: t15.2024.07.21 val PER: 0.1034
2026-01-03 12:30:32,119: t15.2024.07.28 val PER: 0.1404
2026-01-03 12:30:32,119: t15.2025.01.10 val PER: 0.3058
2026-01-03 12:30:32,120: t15.2025.01.12 val PER: 0.1617
2026-01-03 12:30:32,120: t15.2025.03.14 val PER: 0.3432
2026-01-03 12:30:32,120: t15.2025.03.16 val PER: 0.2147
2026-01-03 12:30:32,120: t15.2025.03.30 val PER: 0.3230
2026-01-03 12:30:32,120: t15.2025.04.13 val PER: 0.2282
2026-01-03 12:30:32,386: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_15500
2026-01-03 12:30:41,113: Train batch 15600: loss: 14.72 grad norm: 56.79 time: 0.061
2026-01-03 12:30:58,519: Train batch 15800: loss: 17.07 grad norm: 66.39 time: 0.066
2026-01-03 12:31:16,055: Train batch 16000: loss: 10.66 grad norm: 45.50 time: 0.055
2026-01-03 12:31:16,055: Running test after training batch: 16000
2026-01-03 12:31:16,171: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:31:21,012: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:31:21,046: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 12:31:22,936: Val batch 16000: PER (avg): 0.1616 CTC Loss (avg): 16.1127 WER(1gram): 48.98% (n=64) time: 6.880
2026-01-03 12:31:22,936: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 12:31:22,936: t15.2023.08.13 val PER: 0.1247
2026-01-03 12:31:22,936: t15.2023.08.18 val PER: 0.1098
2026-01-03 12:31:22,936: t15.2023.08.20 val PER: 0.1215
2026-01-03 12:31:22,936: t15.2023.08.25 val PER: 0.0934
2026-01-03 12:31:22,936: t15.2023.08.27 val PER: 0.1913
2026-01-03 12:31:22,936: t15.2023.09.01 val PER: 0.0869
2026-01-03 12:31:22,936: t15.2023.09.03 val PER: 0.1817
2026-01-03 12:31:22,937: t15.2023.09.24 val PER: 0.1299
2026-01-03 12:31:22,937: t15.2023.09.29 val PER: 0.1353
2026-01-03 12:31:22,937: t15.2023.10.01 val PER: 0.1757
2026-01-03 12:31:22,937: t15.2023.10.06 val PER: 0.0915
2026-01-03 12:31:22,937: t15.2023.10.08 val PER: 0.2530
2026-01-03 12:31:22,937: t15.2023.10.13 val PER: 0.2211
2026-01-03 12:31:22,937: t15.2023.10.15 val PER: 0.1694
2026-01-03 12:31:22,937: t15.2023.10.20 val PER: 0.1812
2026-01-03 12:31:22,937: t15.2023.10.22 val PER: 0.1147
2026-01-03 12:31:22,937: t15.2023.11.03 val PER: 0.1859
2026-01-03 12:31:22,937: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:31:22,937: t15.2023.11.17 val PER: 0.0513
2026-01-03 12:31:22,937: t15.2023.11.19 val PER: 0.0479
2026-01-03 12:31:22,937: t15.2023.11.26 val PER: 0.1406
2026-01-03 12:31:22,937: t15.2023.12.03 val PER: 0.1313
2026-01-03 12:31:22,937: t15.2023.12.08 val PER: 0.1152
2026-01-03 12:31:22,938: t15.2023.12.10 val PER: 0.1038
2026-01-03 12:31:22,938: t15.2023.12.17 val PER: 0.1455
2026-01-03 12:31:22,938: t15.2023.12.29 val PER: 0.1386
2026-01-03 12:31:22,938: t15.2024.02.25 val PER: 0.1236
2026-01-03 12:31:22,938: t15.2024.03.08 val PER: 0.2347
2026-01-03 12:31:22,938: t15.2024.03.15 val PER: 0.2158
2026-01-03 12:31:22,938: t15.2024.03.17 val PER: 0.1450
2026-01-03 12:31:22,938: t15.2024.05.10 val PER: 0.1694
2026-01-03 12:31:22,938: t15.2024.06.14 val PER: 0.1767
2026-01-03 12:31:22,938: t15.2024.07.19 val PER: 0.2551
2026-01-03 12:31:22,938: t15.2024.07.21 val PER: 0.1034
2026-01-03 12:31:22,938: t15.2024.07.28 val PER: 0.1419
2026-01-03 12:31:22,938: t15.2025.01.10 val PER: 0.3030
2026-01-03 12:31:22,938: t15.2025.01.12 val PER: 0.1632
2026-01-03 12:31:22,938: t15.2025.03.14 val PER: 0.3506
2026-01-03 12:31:22,938: t15.2025.03.16 val PER: 0.2094
2026-01-03 12:31:22,938: t15.2025.03.30 val PER: 0.3253
2026-01-03 12:31:22,939: t15.2025.04.13 val PER: 0.2282
2026-01-03 12:31:23,203: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_16000
2026-01-03 12:31:40,906: Train batch 16200: loss: 8.08 grad norm: 43.12 time: 0.055
2026-01-03 12:31:58,593: Train batch 16400: loss: 12.37 grad norm: 59.78 time: 0.057
2026-01-03 12:32:07,480: Running test after training batch: 16500
2026-01-03 12:32:07,573: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:32:12,291: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:32:12,325: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 12:32:14,197: Val batch 16500: PER (avg): 0.1620 CTC Loss (avg): 16.1256 WER(1gram): 49.49% (n=64) time: 6.716
2026-01-03 12:32:14,197: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 12:32:14,198: t15.2023.08.13 val PER: 0.1258
2026-01-03 12:32:14,198: t15.2023.08.18 val PER: 0.1098
2026-01-03 12:32:14,198: t15.2023.08.20 val PER: 0.1247
2026-01-03 12:32:14,198: t15.2023.08.25 val PER: 0.0979
2026-01-03 12:32:14,198: t15.2023.08.27 val PER: 0.1913
2026-01-03 12:32:14,198: t15.2023.09.01 val PER: 0.0885
2026-01-03 12:32:14,198: t15.2023.09.03 val PER: 0.1805
2026-01-03 12:32:14,198: t15.2023.09.24 val PER: 0.1311
2026-01-03 12:32:14,198: t15.2023.09.29 val PER: 0.1378
2026-01-03 12:32:14,198: t15.2023.10.01 val PER: 0.1810
2026-01-03 12:32:14,198: t15.2023.10.06 val PER: 0.0926
2026-01-03 12:32:14,198: t15.2023.10.08 val PER: 0.2517
2026-01-03 12:32:14,198: t15.2023.10.13 val PER: 0.2234
2026-01-03 12:32:14,198: t15.2023.10.15 val PER: 0.1681
2026-01-03 12:32:14,198: t15.2023.10.20 val PER: 0.1879
2026-01-03 12:32:14,199: t15.2023.10.22 val PER: 0.1169
2026-01-03 12:32:14,199: t15.2023.11.03 val PER: 0.1839
2026-01-03 12:32:14,199: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:32:14,199: t15.2023.11.17 val PER: 0.0513
2026-01-03 12:32:14,199: t15.2023.11.19 val PER: 0.0479
2026-01-03 12:32:14,199: t15.2023.11.26 val PER: 0.1420
2026-01-03 12:32:14,199: t15.2023.12.03 val PER: 0.1282
2026-01-03 12:32:14,199: t15.2023.12.08 val PER: 0.1152
2026-01-03 12:32:14,199: t15.2023.12.10 val PER: 0.0972
2026-01-03 12:32:14,199: t15.2023.12.17 val PER: 0.1455
2026-01-03 12:32:14,199: t15.2023.12.29 val PER: 0.1400
2026-01-03 12:32:14,199: t15.2024.02.25 val PER: 0.1306
2026-01-03 12:32:14,199: t15.2024.03.08 val PER: 0.2304
2026-01-03 12:32:14,199: t15.2024.03.15 val PER: 0.2145
2026-01-03 12:32:14,199: t15.2024.03.17 val PER: 0.1450
2026-01-03 12:32:14,199: t15.2024.05.10 val PER: 0.1694
2026-01-03 12:32:14,200: t15.2024.06.14 val PER: 0.1735
2026-01-03 12:32:14,200: t15.2024.07.19 val PER: 0.2558
2026-01-03 12:32:14,200: t15.2024.07.21 val PER: 0.1041
2026-01-03 12:32:14,200: t15.2024.07.28 val PER: 0.1434
2026-01-03 12:32:14,200: t15.2025.01.10 val PER: 0.3072
2026-01-03 12:32:14,200: t15.2025.01.12 val PER: 0.1570
2026-01-03 12:32:14,200: t15.2025.03.14 val PER: 0.3491
2026-01-03 12:32:14,200: t15.2025.03.16 val PER: 0.2094
2026-01-03 12:32:14,200: t15.2025.03.30 val PER: 0.3207
2026-01-03 12:32:14,200: t15.2025.04.13 val PER: 0.2325
2026-01-03 12:32:14,469: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_16500
2026-01-03 12:32:23,424: Train batch 16600: loss: 10.48 grad norm: 54.42 time: 0.052
2026-01-03 12:32:41,168: Train batch 16800: loss: 20.14 grad norm: 75.79 time: 0.061
2026-01-03 12:32:59,210: Train batch 17000: loss: 9.64 grad norm: 47.59 time: 0.081
2026-01-03 12:32:59,211: Running test after training batch: 17000
2026-01-03 12:32:59,308: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:33:04,025: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:33:04,060: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 12:33:05,973: Val batch 17000: PER (avg): 0.1623 CTC Loss (avg): 16.0921 WER(1gram): 49.24% (n=64) time: 6.762
2026-01-03 12:33:05,973: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 12:33:05,973: t15.2023.08.13 val PER: 0.1237
2026-01-03 12:33:05,973: t15.2023.08.18 val PER: 0.1098
2026-01-03 12:33:05,973: t15.2023.08.20 val PER: 0.1231
2026-01-03 12:33:05,973: t15.2023.08.25 val PER: 0.0964
2026-01-03 12:33:05,973: t15.2023.08.27 val PER: 0.1929
2026-01-03 12:33:05,974: t15.2023.09.01 val PER: 0.0877
2026-01-03 12:33:05,974: t15.2023.09.03 val PER: 0.1817
2026-01-03 12:33:05,974: t15.2023.09.24 val PER: 0.1323
2026-01-03 12:33:05,974: t15.2023.09.29 val PER: 0.1372
2026-01-03 12:33:05,974: t15.2023.10.01 val PER: 0.1777
2026-01-03 12:33:05,974: t15.2023.10.06 val PER: 0.0926
2026-01-03 12:33:05,974: t15.2023.10.08 val PER: 0.2571
2026-01-03 12:33:05,974: t15.2023.10.13 val PER: 0.2219
2026-01-03 12:33:05,974: t15.2023.10.15 val PER: 0.1688
2026-01-03 12:33:05,974: t15.2023.10.20 val PER: 0.1879
2026-01-03 12:33:05,974: t15.2023.10.22 val PER: 0.1158
2026-01-03 12:33:05,974: t15.2023.11.03 val PER: 0.1852
2026-01-03 12:33:05,974: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:33:05,974: t15.2023.11.17 val PER: 0.0513
2026-01-03 12:33:05,975: t15.2023.11.19 val PER: 0.0479
2026-01-03 12:33:05,975: t15.2023.11.26 val PER: 0.1413
2026-01-03 12:33:05,975: t15.2023.12.03 val PER: 0.1324
2026-01-03 12:33:05,975: t15.2023.12.08 val PER: 0.1165
2026-01-03 12:33:05,975: t15.2023.12.10 val PER: 0.0999
2026-01-03 12:33:05,975: t15.2023.12.17 val PER: 0.1486
2026-01-03 12:33:05,975: t15.2023.12.29 val PER: 0.1393
2026-01-03 12:33:05,975: t15.2024.02.25 val PER: 0.1306
2026-01-03 12:33:05,975: t15.2024.03.08 val PER: 0.2290
2026-01-03 12:33:05,976: t15.2024.03.15 val PER: 0.2145
2026-01-03 12:33:05,976: t15.2024.03.17 val PER: 0.1457
2026-01-03 12:33:05,976: t15.2024.05.10 val PER: 0.1694
2026-01-03 12:33:05,976: t15.2024.06.14 val PER: 0.1767
2026-01-03 12:33:05,977: t15.2024.07.19 val PER: 0.2577
2026-01-03 12:33:05,977: t15.2024.07.21 val PER: 0.1021
2026-01-03 12:33:05,977: t15.2024.07.28 val PER: 0.1441
2026-01-03 12:33:05,977: t15.2025.01.10 val PER: 0.3017
2026-01-03 12:33:05,977: t15.2025.01.12 val PER: 0.1609
2026-01-03 12:33:05,977: t15.2025.03.14 val PER: 0.3462
2026-01-03 12:33:05,977: t15.2025.03.16 val PER: 0.2134
2026-01-03 12:33:05,977: t15.2025.03.30 val PER: 0.3253
2026-01-03 12:33:05,977: t15.2025.04.13 val PER: 0.2325
2026-01-03 12:33:06,249: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_17000
2026-01-03 12:33:23,935: Train batch 17200: loss: 12.28 grad norm: 51.16 time: 0.083
2026-01-03 12:33:41,419: Train batch 17400: loss: 15.14 grad norm: 61.45 time: 0.071
2026-01-03 12:33:49,946: Running test after training batch: 17500
2026-01-03 12:33:50,038: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:33:54,986: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:33:55,022: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 12:33:56,950: Val batch 17500: PER (avg): 0.1621 CTC Loss (avg): 16.0871 WER(1gram): 48.98% (n=64) time: 7.004
2026-01-03 12:33:56,950: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 12:33:56,951: t15.2023.08.13 val PER: 0.1247
2026-01-03 12:33:56,951: t15.2023.08.18 val PER: 0.1106
2026-01-03 12:33:56,951: t15.2023.08.20 val PER: 0.1239
2026-01-03 12:33:56,951: t15.2023.08.25 val PER: 0.0979
2026-01-03 12:33:56,951: t15.2023.08.27 val PER: 0.1945
2026-01-03 12:33:56,951: t15.2023.09.01 val PER: 0.0909
2026-01-03 12:33:56,951: t15.2023.09.03 val PER: 0.1781
2026-01-03 12:33:56,951: t15.2023.09.24 val PER: 0.1286
2026-01-03 12:33:56,951: t15.2023.09.29 val PER: 0.1391
2026-01-03 12:33:56,951: t15.2023.10.01 val PER: 0.1783
2026-01-03 12:33:56,951: t15.2023.10.06 val PER: 0.0915
2026-01-03 12:33:56,951: t15.2023.10.08 val PER: 0.2530
2026-01-03 12:33:56,952: t15.2023.10.13 val PER: 0.2188
2026-01-03 12:33:56,952: t15.2023.10.15 val PER: 0.1694
2026-01-03 12:33:56,952: t15.2023.10.20 val PER: 0.1846
2026-01-03 12:33:56,952: t15.2023.10.22 val PER: 0.1125
2026-01-03 12:33:56,952: t15.2023.11.03 val PER: 0.1839
2026-01-03 12:33:56,952: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:33:56,952: t15.2023.11.17 val PER: 0.0498
2026-01-03 12:33:56,952: t15.2023.11.19 val PER: 0.0459
2026-01-03 12:33:56,952: t15.2023.11.26 val PER: 0.1420
2026-01-03 12:33:56,952: t15.2023.12.03 val PER: 0.1292
2026-01-03 12:33:56,952: t15.2023.12.08 val PER: 0.1185
2026-01-03 12:33:56,952: t15.2023.12.10 val PER: 0.0986
2026-01-03 12:33:56,952: t15.2023.12.17 val PER: 0.1476
2026-01-03 12:33:56,952: t15.2023.12.29 val PER: 0.1380
2026-01-03 12:33:56,952: t15.2024.02.25 val PER: 0.1264
2026-01-03 12:33:56,952: t15.2024.03.08 val PER: 0.2304
2026-01-03 12:33:56,952: t15.2024.03.15 val PER: 0.2133
2026-01-03 12:33:56,953: t15.2024.03.17 val PER: 0.1464
2026-01-03 12:33:56,953: t15.2024.05.10 val PER: 0.1753
2026-01-03 12:33:56,953: t15.2024.06.14 val PER: 0.1767
2026-01-03 12:33:56,953: t15.2024.07.19 val PER: 0.2577
2026-01-03 12:33:56,953: t15.2024.07.21 val PER: 0.1041
2026-01-03 12:33:56,953: t15.2024.07.28 val PER: 0.1441
2026-01-03 12:33:56,953: t15.2025.01.10 val PER: 0.3030
2026-01-03 12:33:56,953: t15.2025.01.12 val PER: 0.1586
2026-01-03 12:33:56,953: t15.2025.03.14 val PER: 0.3462
2026-01-03 12:33:56,953: t15.2025.03.16 val PER: 0.2147
2026-01-03 12:33:56,953: t15.2025.03.30 val PER: 0.3276
2026-01-03 12:33:56,953: t15.2025.04.13 val PER: 0.2311
2026-01-03 12:33:57,224: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_17500
2026-01-03 12:34:06,185: Train batch 17600: loss: 13.84 grad norm: 62.36 time: 0.050
2026-01-03 12:34:24,056: Train batch 17800: loss: 8.43 grad norm: 57.24 time: 0.042
2026-01-03 12:34:41,818: Train batch 18000: loss: 14.67 grad norm: 63.79 time: 0.061
2026-01-03 12:34:41,819: Running test after training batch: 18000
2026-01-03 12:34:41,954: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:34:46,653: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:34:46,688: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 12:34:48,636: Val batch 18000: PER (avg): 0.1618 CTC Loss (avg): 16.0775 WER(1gram): 49.49% (n=64) time: 6.817
2026-01-03 12:34:48,636: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 12:34:48,637: t15.2023.08.13 val PER: 0.1247
2026-01-03 12:34:48,637: t15.2023.08.18 val PER: 0.1115
2026-01-03 12:34:48,637: t15.2023.08.20 val PER: 0.1223
2026-01-03 12:34:48,637: t15.2023.08.25 val PER: 0.0949
2026-01-03 12:34:48,637: t15.2023.08.27 val PER: 0.1977
2026-01-03 12:34:48,637: t15.2023.09.01 val PER: 0.0852
2026-01-03 12:34:48,637: t15.2023.09.03 val PER: 0.1793
2026-01-03 12:34:48,637: t15.2023.09.24 val PER: 0.1299
2026-01-03 12:34:48,637: t15.2023.09.29 val PER: 0.1366
2026-01-03 12:34:48,637: t15.2023.10.01 val PER: 0.1816
2026-01-03 12:34:48,637: t15.2023.10.06 val PER: 0.0926
2026-01-03 12:34:48,637: t15.2023.10.08 val PER: 0.2530
2026-01-03 12:34:48,637: t15.2023.10.13 val PER: 0.2203
2026-01-03 12:34:48,637: t15.2023.10.15 val PER: 0.1674
2026-01-03 12:34:48,637: t15.2023.10.20 val PER: 0.1879
2026-01-03 12:34:48,638: t15.2023.10.22 val PER: 0.1169
2026-01-03 12:34:48,638: t15.2023.11.03 val PER: 0.1872
2026-01-03 12:34:48,638: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:34:48,638: t15.2023.11.17 val PER: 0.0498
2026-01-03 12:34:48,638: t15.2023.11.19 val PER: 0.0479
2026-01-03 12:34:48,638: t15.2023.11.26 val PER: 0.1391
2026-01-03 12:34:48,638: t15.2023.12.03 val PER: 0.1292
2026-01-03 12:34:48,638: t15.2023.12.08 val PER: 0.1152
2026-01-03 12:34:48,638: t15.2023.12.10 val PER: 0.1012
2026-01-03 12:34:48,638: t15.2023.12.17 val PER: 0.1466
2026-01-03 12:34:48,638: t15.2023.12.29 val PER: 0.1393
2026-01-03 12:34:48,638: t15.2024.02.25 val PER: 0.1334
2026-01-03 12:34:48,638: t15.2024.03.08 val PER: 0.2290
2026-01-03 12:34:48,638: t15.2024.03.15 val PER: 0.2151
2026-01-03 12:34:48,638: t15.2024.03.17 val PER: 0.1457
2026-01-03 12:34:48,639: t15.2024.05.10 val PER: 0.1709
2026-01-03 12:34:48,639: t15.2024.06.14 val PER: 0.1656
2026-01-03 12:34:48,639: t15.2024.07.19 val PER: 0.2558
2026-01-03 12:34:48,639: t15.2024.07.21 val PER: 0.1028
2026-01-03 12:34:48,639: t15.2024.07.28 val PER: 0.1441
2026-01-03 12:34:48,639: t15.2025.01.10 val PER: 0.3099
2026-01-03 12:34:48,639: t15.2025.01.12 val PER: 0.1563
2026-01-03 12:34:48,639: t15.2025.03.14 val PER: 0.3476
2026-01-03 12:34:48,639: t15.2025.03.16 val PER: 0.2134
2026-01-03 12:34:48,639: t15.2025.03.30 val PER: 0.3207
2026-01-03 12:34:48,639: t15.2025.04.13 val PER: 0.2325
2026-01-03 12:34:48,913: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_18000
2026-01-03 12:35:07,688: Train batch 18200: loss: 10.58 grad norm: 52.18 time: 0.075
2026-01-03 12:35:25,407: Train batch 18400: loss: 7.37 grad norm: 47.86 time: 0.058
2026-01-03 12:35:34,372: Running test after training batch: 18500
2026-01-03 12:35:34,518: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:35:39,303: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:35:39,340: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 12:35:41,303: Val batch 18500: PER (avg): 0.1621 CTC Loss (avg): 16.0860 WER(1gram): 49.49% (n=64) time: 6.930
2026-01-03 12:35:41,303: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 12:35:41,303: t15.2023.08.13 val PER: 0.1268
2026-01-03 12:35:41,303: t15.2023.08.18 val PER: 0.1098
2026-01-03 12:35:41,304: t15.2023.08.20 val PER: 0.1231
2026-01-03 12:35:41,304: t15.2023.08.25 val PER: 0.0964
2026-01-03 12:35:41,304: t15.2023.08.27 val PER: 0.1961
2026-01-03 12:35:41,304: t15.2023.09.01 val PER: 0.0860
2026-01-03 12:35:41,304: t15.2023.09.03 val PER: 0.1781
2026-01-03 12:35:41,304: t15.2023.09.24 val PER: 0.1311
2026-01-03 12:35:41,304: t15.2023.09.29 val PER: 0.1378
2026-01-03 12:35:41,304: t15.2023.10.01 val PER: 0.1790
2026-01-03 12:35:41,304: t15.2023.10.06 val PER: 0.0936
2026-01-03 12:35:41,304: t15.2023.10.08 val PER: 0.2544
2026-01-03 12:35:41,304: t15.2023.10.13 val PER: 0.2196
2026-01-03 12:35:41,304: t15.2023.10.15 val PER: 0.1694
2026-01-03 12:35:41,304: t15.2023.10.20 val PER: 0.1946
2026-01-03 12:35:41,304: t15.2023.10.22 val PER: 0.1158
2026-01-03 12:35:41,305: t15.2023.11.03 val PER: 0.1852
2026-01-03 12:35:41,305: t15.2023.11.04 val PER: 0.0410
2026-01-03 12:35:41,305: t15.2023.11.17 val PER: 0.0529
2026-01-03 12:35:41,305: t15.2023.11.19 val PER: 0.0459
2026-01-03 12:35:41,305: t15.2023.11.26 val PER: 0.1413
2026-01-03 12:35:41,305: t15.2023.12.03 val PER: 0.1313
2026-01-03 12:35:41,305: t15.2023.12.08 val PER: 0.1192
2026-01-03 12:35:41,305: t15.2023.12.10 val PER: 0.0972
2026-01-03 12:35:41,305: t15.2023.12.17 val PER: 0.1466
2026-01-03 12:35:41,305: t15.2023.12.29 val PER: 0.1393
2026-01-03 12:35:41,305: t15.2024.02.25 val PER: 0.1264
2026-01-03 12:35:41,305: t15.2024.03.08 val PER: 0.2304
2026-01-03 12:35:41,305: t15.2024.03.15 val PER: 0.2120
2026-01-03 12:35:41,305: t15.2024.03.17 val PER: 0.1464
2026-01-03 12:35:41,305: t15.2024.05.10 val PER: 0.1724
2026-01-03 12:35:41,305: t15.2024.06.14 val PER: 0.1735
2026-01-03 12:35:41,305: t15.2024.07.19 val PER: 0.2571
2026-01-03 12:35:41,306: t15.2024.07.21 val PER: 0.1034
2026-01-03 12:35:41,306: t15.2024.07.28 val PER: 0.1441
2026-01-03 12:35:41,306: t15.2025.01.10 val PER: 0.3044
2026-01-03 12:35:41,306: t15.2025.01.12 val PER: 0.1563
2026-01-03 12:35:41,306: t15.2025.03.14 val PER: 0.3462
2026-01-03 12:35:41,306: t15.2025.03.16 val PER: 0.2147
2026-01-03 12:35:41,306: t15.2025.03.30 val PER: 0.3264
2026-01-03 12:35:41,306: t15.2025.04.13 val PER: 0.2297
2026-01-03 12:35:42,453: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_18500
2026-01-03 12:35:57,956: Train batch 18600: loss: 15.42 grad norm: 62.51 time: 0.067
2026-01-03 12:36:16,596: Train batch 18800: loss: 12.14 grad norm: 54.84 time: 0.065
2026-01-03 12:36:34,648: Train batch 19000: loss: 10.25 grad norm: 44.60 time: 0.064
2026-01-03 12:36:34,648: Running test after training batch: 19000
2026-01-03 12:36:34,774: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:36:40,545: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:36:40,581: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 12:36:42,510: Val batch 19000: PER (avg): 0.1620 CTC Loss (avg): 16.0867 WER(1gram): 49.24% (n=64) time: 7.862
2026-01-03 12:36:42,511: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 12:36:42,511: t15.2023.08.13 val PER: 0.1247
2026-01-03 12:36:42,511: t15.2023.08.18 val PER: 0.1098
2026-01-03 12:36:42,511: t15.2023.08.20 val PER: 0.1223
2026-01-03 12:36:42,511: t15.2023.08.25 val PER: 0.0994
2026-01-03 12:36:42,511: t15.2023.08.27 val PER: 0.1945
2026-01-03 12:36:42,511: t15.2023.09.01 val PER: 0.0885
2026-01-03 12:36:42,511: t15.2023.09.03 val PER: 0.1770
2026-01-03 12:36:42,512: t15.2023.09.24 val PER: 0.1286
2026-01-03 12:36:42,512: t15.2023.09.29 val PER: 0.1378
2026-01-03 12:36:42,512: t15.2023.10.01 val PER: 0.1810
2026-01-03 12:36:42,512: t15.2023.10.06 val PER: 0.0904
2026-01-03 12:36:42,512: t15.2023.10.08 val PER: 0.2558
2026-01-03 12:36:42,512: t15.2023.10.13 val PER: 0.2188
2026-01-03 12:36:42,512: t15.2023.10.15 val PER: 0.1668
2026-01-03 12:36:42,512: t15.2023.10.20 val PER: 0.1946
2026-01-03 12:36:42,512: t15.2023.10.22 val PER: 0.1136
2026-01-03 12:36:42,512: t15.2023.11.03 val PER: 0.1845
2026-01-03 12:36:42,513: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:36:42,513: t15.2023.11.17 val PER: 0.0498
2026-01-03 12:36:42,513: t15.2023.11.19 val PER: 0.0459
2026-01-03 12:36:42,513: t15.2023.11.26 val PER: 0.1391
2026-01-03 12:36:42,513: t15.2023.12.03 val PER: 0.1324
2026-01-03 12:36:42,513: t15.2023.12.08 val PER: 0.1172
2026-01-03 12:36:42,513: t15.2023.12.10 val PER: 0.0972
2026-01-03 12:36:42,513: t15.2023.12.17 val PER: 0.1486
2026-01-03 12:36:42,513: t15.2023.12.29 val PER: 0.1393
2026-01-03 12:36:42,513: t15.2024.02.25 val PER: 0.1250
2026-01-03 12:36:42,514: t15.2024.03.08 val PER: 0.2319
2026-01-03 12:36:42,514: t15.2024.03.15 val PER: 0.2126
2026-01-03 12:36:42,514: t15.2024.03.17 val PER: 0.1471
2026-01-03 12:36:42,514: t15.2024.05.10 val PER: 0.1738
2026-01-03 12:36:42,514: t15.2024.06.14 val PER: 0.1751
2026-01-03 12:36:42,514: t15.2024.07.19 val PER: 0.2564
2026-01-03 12:36:42,514: t15.2024.07.21 val PER: 0.1048
2026-01-03 12:36:42,514: t15.2024.07.28 val PER: 0.1449
2026-01-03 12:36:42,514: t15.2025.01.10 val PER: 0.3017
2026-01-03 12:36:42,514: t15.2025.01.12 val PER: 0.1586
2026-01-03 12:36:42,514: t15.2025.03.14 val PER: 0.3462
2026-01-03 12:36:42,515: t15.2025.03.16 val PER: 0.2147
2026-01-03 12:36:42,515: t15.2025.03.30 val PER: 0.3253
2026-01-03 12:36:42,515: t15.2025.04.13 val PER: 0.2354
2026-01-03 12:36:42,808: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_19000
2026-01-03 12:37:00,897: Train batch 19200: loss: 7.98 grad norm: 49.89 time: 0.063
2026-01-03 12:37:19,145: Train batch 19400: loss: 7.09 grad norm: 42.01 time: 0.053
2026-01-03 12:37:28,246: Running test after training batch: 19500
2026-01-03 12:37:28,413: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:37:33,120: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:37:33,157: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 12:37:35,132: Val batch 19500: PER (avg): 0.1622 CTC Loss (avg): 16.0957 WER(1gram): 49.75% (n=64) time: 6.885
2026-01-03 12:37:35,132: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 12:37:35,132: t15.2023.08.13 val PER: 0.1258
2026-01-03 12:37:35,133: t15.2023.08.18 val PER: 0.1115
2026-01-03 12:37:35,133: t15.2023.08.20 val PER: 0.1207
2026-01-03 12:37:35,133: t15.2023.08.25 val PER: 0.0994
2026-01-03 12:37:35,133: t15.2023.08.27 val PER: 0.1913
2026-01-03 12:37:35,133: t15.2023.09.01 val PER: 0.0885
2026-01-03 12:37:35,133: t15.2023.09.03 val PER: 0.1805
2026-01-03 12:37:35,133: t15.2023.09.24 val PER: 0.1286
2026-01-03 12:37:35,133: t15.2023.09.29 val PER: 0.1385
2026-01-03 12:37:35,133: t15.2023.10.01 val PER: 0.1790
2026-01-03 12:37:35,133: t15.2023.10.06 val PER: 0.0904
2026-01-03 12:37:35,134: t15.2023.10.08 val PER: 0.2598
2026-01-03 12:37:35,134: t15.2023.10.13 val PER: 0.2211
2026-01-03 12:37:35,134: t15.2023.10.15 val PER: 0.1681
2026-01-03 12:37:35,134: t15.2023.10.20 val PER: 0.1946
2026-01-03 12:37:35,134: t15.2023.10.22 val PER: 0.1136
2026-01-03 12:37:35,134: t15.2023.11.03 val PER: 0.1845
2026-01-03 12:37:35,134: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:37:35,134: t15.2023.11.17 val PER: 0.0529
2026-01-03 12:37:35,134: t15.2023.11.19 val PER: 0.0459
2026-01-03 12:37:35,134: t15.2023.11.26 val PER: 0.1413
2026-01-03 12:37:35,135: t15.2023.12.03 val PER: 0.1313
2026-01-03 12:37:35,135: t15.2023.12.08 val PER: 0.1158
2026-01-03 12:37:35,135: t15.2023.12.10 val PER: 0.0946
2026-01-03 12:37:35,135: t15.2023.12.17 val PER: 0.1497
2026-01-03 12:37:35,135: t15.2023.12.29 val PER: 0.1386
2026-01-03 12:37:35,135: t15.2024.02.25 val PER: 0.1292
2026-01-03 12:37:35,135: t15.2024.03.08 val PER: 0.2290
2026-01-03 12:37:35,135: t15.2024.03.15 val PER: 0.2108
2026-01-03 12:37:35,135: t15.2024.03.17 val PER: 0.1485
2026-01-03 12:37:35,135: t15.2024.05.10 val PER: 0.1738
2026-01-03 12:37:35,135: t15.2024.06.14 val PER: 0.1751
2026-01-03 12:37:35,135: t15.2024.07.19 val PER: 0.2558
2026-01-03 12:37:35,135: t15.2024.07.21 val PER: 0.1062
2026-01-03 12:37:35,135: t15.2024.07.28 val PER: 0.1426
2026-01-03 12:37:35,135: t15.2025.01.10 val PER: 0.3044
2026-01-03 12:37:35,135: t15.2025.01.12 val PER: 0.1586
2026-01-03 12:37:35,135: t15.2025.03.14 val PER: 0.3462
2026-01-03 12:37:35,136: t15.2025.03.16 val PER: 0.2173
2026-01-03 12:37:35,136: t15.2025.03.30 val PER: 0.3218
2026-01-03 12:37:35,136: t15.2025.04.13 val PER: 0.2354
2026-01-03 12:37:35,411: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_19500
2026-01-03 12:37:44,462: Train batch 19600: loss: 10.11 grad norm: 49.74 time: 0.057
2026-01-03 12:38:02,115: Train batch 19800: loss: 10.48 grad norm: 54.67 time: 0.055
2026-01-03 12:38:19,794: Running test after training batch: 19999
2026-01-03 12:38:19,892: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:38:25,097: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:38:25,134: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 12:38:27,128: Val batch 19999: PER (avg): 0.1618 CTC Loss (avg): 16.0829 WER(1gram): 49.75% (n=64) time: 7.333
2026-01-03 12:38:27,128: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 12:38:27,128: t15.2023.08.13 val PER: 0.1258
2026-01-03 12:38:27,128: t15.2023.08.18 val PER: 0.1098
2026-01-03 12:38:27,128: t15.2023.08.20 val PER: 0.1199
2026-01-03 12:38:27,128: t15.2023.08.25 val PER: 0.0964
2026-01-03 12:38:27,129: t15.2023.08.27 val PER: 0.1913
2026-01-03 12:38:27,129: t15.2023.09.01 val PER: 0.0901
2026-01-03 12:38:27,129: t15.2023.09.03 val PER: 0.1781
2026-01-03 12:38:27,129: t15.2023.09.24 val PER: 0.1274
2026-01-03 12:38:27,129: t15.2023.09.29 val PER: 0.1372
2026-01-03 12:38:27,129: t15.2023.10.01 val PER: 0.1764
2026-01-03 12:38:27,129: t15.2023.10.06 val PER: 0.0904
2026-01-03 12:38:27,129: t15.2023.10.08 val PER: 0.2571
2026-01-03 12:38:27,129: t15.2023.10.13 val PER: 0.2227
2026-01-03 12:38:27,129: t15.2023.10.15 val PER: 0.1674
2026-01-03 12:38:27,129: t15.2023.10.20 val PER: 0.1946
2026-01-03 12:38:27,129: t15.2023.10.22 val PER: 0.1136
2026-01-03 12:38:27,129: t15.2023.11.03 val PER: 0.1852
2026-01-03 12:38:27,129: t15.2023.11.04 val PER: 0.0410
2026-01-03 12:38:27,129: t15.2023.11.17 val PER: 0.0513
2026-01-03 12:38:27,130: t15.2023.11.19 val PER: 0.0459
2026-01-03 12:38:27,130: t15.2023.11.26 val PER: 0.1391
2026-01-03 12:38:27,130: t15.2023.12.03 val PER: 0.1313
2026-01-03 12:38:27,130: t15.2023.12.08 val PER: 0.1172
2026-01-03 12:38:27,130: t15.2023.12.10 val PER: 0.0946
2026-01-03 12:38:27,130: t15.2023.12.17 val PER: 0.1455
2026-01-03 12:38:27,130: t15.2023.12.29 val PER: 0.1373
2026-01-03 12:38:27,130: t15.2024.02.25 val PER: 0.1264
2026-01-03 12:38:27,130: t15.2024.03.08 val PER: 0.2333
2026-01-03 12:38:27,130: t15.2024.03.15 val PER: 0.2126
2026-01-03 12:38:27,130: t15.2024.03.17 val PER: 0.1478
2026-01-03 12:38:27,130: t15.2024.05.10 val PER: 0.1709
2026-01-03 12:38:27,130: t15.2024.06.14 val PER: 0.1751
2026-01-03 12:38:27,130: t15.2024.07.19 val PER: 0.2591
2026-01-03 12:38:27,130: t15.2024.07.21 val PER: 0.1076
2026-01-03 12:38:27,130: t15.2024.07.28 val PER: 0.1419
2026-01-03 12:38:27,131: t15.2025.01.10 val PER: 0.3044
2026-01-03 12:38:27,131: t15.2025.01.12 val PER: 0.1578
2026-01-03 12:38:27,131: t15.2025.03.14 val PER: 0.3491
2026-01-03 12:38:27,131: t15.2025.03.16 val PER: 0.2147
2026-01-03 12:38:27,131: t15.2025.03.30 val PER: 0.3218
2026-01-03 12:38:27,131: t15.2025.04.13 val PER: 0.2325
2026-01-03 12:38:27,432: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f01/checkpoint/checkpoint_batch_19999
2026-01-03 12:38:27,464: Best avg val PER achieved: 0.16349
2026-01-03 12:38:27,465: Total training time: 34.57 minutes

=== RUN step10k_f02.yaml ===
2026-01-03 12:38:32,065: Using device: cuda:0
2026-01-03 12:38:33,773: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-03 12:38:33,798: Using 45 sessions after filtering (from 45).
2026-01-03 12:38:34,210: Using torch.compile (if available)
2026-01-03 12:38:34,211: torch.compile not available (torch<2.0). Skipping.
2026-01-03 12:38:34,211: Initialized RNN decoding model
2026-01-03 12:38:34,211: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 12:38:34,211: Model has 44,907,305 parameters
2026-01-03 12:38:34,211: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 12:38:35,514: Successfully initialized datasets
2026-01-03 12:38:35,515: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 12:38:37,705: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.222
2026-01-03 12:38:37,705: Running test after training batch: 0
2026-01-03 12:38:37,849: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:38:43,101: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-03 12:38:43,815: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-03 12:39:17,714: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 40.008
2026-01-03 12:39:17,714: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-03 12:39:17,714: t15.2023.08.13 val PER: 1.3056
2026-01-03 12:39:17,714: t15.2023.08.18 val PER: 1.4208
2026-01-03 12:39:17,714: t15.2023.08.20 val PER: 1.3002
2026-01-03 12:39:17,715: t15.2023.08.25 val PER: 1.3389
2026-01-03 12:39:17,715: t15.2023.08.27 val PER: 1.2460
2026-01-03 12:39:17,715: t15.2023.09.01 val PER: 1.4537
2026-01-03 12:39:17,715: t15.2023.09.03 val PER: 1.3171
2026-01-03 12:39:17,715: t15.2023.09.24 val PER: 1.5461
2026-01-03 12:39:17,715: t15.2023.09.29 val PER: 1.4671
2026-01-03 12:39:17,715: t15.2023.10.01 val PER: 1.2147
2026-01-03 12:39:17,715: t15.2023.10.06 val PER: 1.4876
2026-01-03 12:39:17,715: t15.2023.10.08 val PER: 1.1827
2026-01-03 12:39:17,716: t15.2023.10.13 val PER: 1.3964
2026-01-03 12:39:17,716: t15.2023.10.15 val PER: 1.3889
2026-01-03 12:39:17,716: t15.2023.10.20 val PER: 1.4866
2026-01-03 12:39:17,716: t15.2023.10.22 val PER: 1.3942
2026-01-03 12:39:17,716: t15.2023.11.03 val PER: 1.5923
2026-01-03 12:39:17,716: t15.2023.11.04 val PER: 2.0171
2026-01-03 12:39:17,716: t15.2023.11.17 val PER: 1.9518
2026-01-03 12:39:17,716: t15.2023.11.19 val PER: 1.6707
2026-01-03 12:39:17,716: t15.2023.11.26 val PER: 1.5413
2026-01-03 12:39:17,716: t15.2023.12.03 val PER: 1.4254
2026-01-03 12:39:17,716: t15.2023.12.08 val PER: 1.4487
2026-01-03 12:39:17,716: t15.2023.12.10 val PER: 1.6899
2026-01-03 12:39:17,716: t15.2023.12.17 val PER: 1.3077
2026-01-03 12:39:17,716: t15.2023.12.29 val PER: 1.4063
2026-01-03 12:39:17,717: t15.2024.02.25 val PER: 1.4228
2026-01-03 12:39:17,717: t15.2024.03.08 val PER: 1.3257
2026-01-03 12:39:17,717: t15.2024.03.15 val PER: 1.3196
2026-01-03 12:39:17,717: t15.2024.03.17 val PER: 1.4052
2026-01-03 12:39:17,717: t15.2024.05.10 val PER: 1.3224
2026-01-03 12:39:17,717: t15.2024.06.14 val PER: 1.5315
2026-01-03 12:39:17,717: t15.2024.07.19 val PER: 1.0817
2026-01-03 12:39:17,717: t15.2024.07.21 val PER: 1.6290
2026-01-03 12:39:17,717: t15.2024.07.28 val PER: 1.6588
2026-01-03 12:39:17,717: t15.2025.01.10 val PER: 1.0923
2026-01-03 12:39:17,717: t15.2025.01.12 val PER: 1.7629
2026-01-03 12:39:17,717: t15.2025.03.14 val PER: 1.0414
2026-01-03 12:39:17,718: t15.2025.03.16 val PER: 1.6257
2026-01-03 12:39:17,718: t15.2025.03.30 val PER: 1.2874
2026-01-03 12:39:17,718: t15.2025.04.13 val PER: 1.5949
2026-01-03 12:39:17,719: New best val WER(1gram) inf% --> 100.00%
2026-01-03 12:39:17,719: Checkpointing model
2026-01-03 12:39:17,962: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 12:39:18,215: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_0
2026-01-03 12:39:36,346: Train batch 200: loss: 77.60 grad norm: 106.10 time: 0.054
2026-01-03 12:39:53,067: Train batch 400: loss: 53.68 grad norm: 84.95 time: 0.063
2026-01-03 12:40:01,420: Running test after training batch: 500
2026-01-03 12:40:01,551: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:40:06,378: WER debug example
  GT : you can see the code at this point as well
  PR : used end ease thus uhde at this ide is aisle
2026-01-03 12:40:06,412: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as adz
2026-01-03 12:40:08,791: Val batch 500: PER (avg): 0.5202 CTC Loss (avg): 55.4090 WER(1gram): 89.34% (n=64) time: 7.371
2026-01-03 12:40:08,792: WER lens: avg_true_words=6.16 avg_pred_words=5.61 max_pred_words=11
2026-01-03 12:40:08,792: t15.2023.08.13 val PER: 0.4636
2026-01-03 12:40:08,792: t15.2023.08.18 val PER: 0.4526
2026-01-03 12:40:08,792: t15.2023.08.20 val PER: 0.4424
2026-01-03 12:40:08,792: t15.2023.08.25 val PER: 0.4307
2026-01-03 12:40:08,792: t15.2023.08.27 val PER: 0.5209
2026-01-03 12:40:08,792: t15.2023.09.01 val PER: 0.4172
2026-01-03 12:40:08,792: t15.2023.09.03 val PER: 0.5024
2026-01-03 12:40:08,792: t15.2023.09.24 val PER: 0.4430
2026-01-03 12:40:08,792: t15.2023.09.29 val PER: 0.4716
2026-01-03 12:40:08,792: t15.2023.10.01 val PER: 0.5277
2026-01-03 12:40:08,792: t15.2023.10.06 val PER: 0.4252
2026-01-03 12:40:08,793: t15.2023.10.08 val PER: 0.5413
2026-01-03 12:40:08,793: t15.2023.10.13 val PER: 0.5826
2026-01-03 12:40:08,793: t15.2023.10.15 val PER: 0.4977
2026-01-03 12:40:08,793: t15.2023.10.20 val PER: 0.4597
2026-01-03 12:40:08,793: t15.2023.10.22 val PER: 0.4477
2026-01-03 12:40:08,793: t15.2023.11.03 val PER: 0.5081
2026-01-03 12:40:08,793: t15.2023.11.04 val PER: 0.2799
2026-01-03 12:40:08,793: t15.2023.11.17 val PER: 0.3577
2026-01-03 12:40:08,793: t15.2023.11.19 val PER: 0.3333
2026-01-03 12:40:08,793: t15.2023.11.26 val PER: 0.5500
2026-01-03 12:40:08,793: t15.2023.12.03 val PER: 0.4926
2026-01-03 12:40:08,793: t15.2023.12.08 val PER: 0.5213
2026-01-03 12:40:08,793: t15.2023.12.10 val PER: 0.4625
2026-01-03 12:40:08,793: t15.2023.12.17 val PER: 0.5582
2026-01-03 12:40:08,794: t15.2023.12.29 val PER: 0.5367
2026-01-03 12:40:08,794: t15.2024.02.25 val PER: 0.4747
2026-01-03 12:40:08,794: t15.2024.03.08 val PER: 0.6074
2026-01-03 12:40:08,794: t15.2024.03.15 val PER: 0.5585
2026-01-03 12:40:08,794: t15.2024.03.17 val PER: 0.5063
2026-01-03 12:40:08,794: t15.2024.05.10 val PER: 0.5334
2026-01-03 12:40:08,794: t15.2024.06.14 val PER: 0.5205
2026-01-03 12:40:08,794: t15.2024.07.19 val PER: 0.6724
2026-01-03 12:40:08,794: t15.2024.07.21 val PER: 0.4814
2026-01-03 12:40:08,794: t15.2024.07.28 val PER: 0.5191
2026-01-03 12:40:08,794: t15.2025.01.10 val PER: 0.7438
2026-01-03 12:40:08,794: t15.2025.01.12 val PER: 0.5674
2026-01-03 12:40:08,794: t15.2025.03.14 val PER: 0.7618
2026-01-03 12:40:08,794: t15.2025.03.16 val PER: 0.6086
2026-01-03 12:40:08,794: t15.2025.03.30 val PER: 0.7230
2026-01-03 12:40:08,794: t15.2025.04.13 val PER: 0.5777
2026-01-03 12:40:08,796: New best val WER(1gram) 100.00% --> 89.34%
2026-01-03 12:40:08,796: Checkpointing model
2026-01-03 12:40:09,058: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 12:40:09,312: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_500
2026-01-03 12:40:17,991: Train batch 600: loss: 49.37 grad norm: 76.26 time: 0.078
2026-01-03 12:40:35,435: Train batch 800: loss: 40.74 grad norm: 80.25 time: 0.057
2026-01-03 12:40:52,687: Train batch 1000: loss: 43.18 grad norm: 78.55 time: 0.066
2026-01-03 12:40:52,687: Running test after training batch: 1000
2026-01-03 12:40:52,782: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:40:57,590: WER debug example
  GT : you can see the code at this point as well
  PR : used ent ease thus code at this royd is while
2026-01-03 12:40:57,622: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke that wass it
2026-01-03 12:40:59,421: Val batch 1000: PER (avg): 0.4130 CTC Loss (avg): 42.6335 WER(1gram): 81.47% (n=64) time: 6.734
2026-01-03 12:40:59,421: WER lens: avg_true_words=6.16 avg_pred_words=5.61 max_pred_words=12
2026-01-03 12:40:59,421: t15.2023.08.13 val PER: 0.3815
2026-01-03 12:40:59,421: t15.2023.08.18 val PER: 0.3403
2026-01-03 12:40:59,421: t15.2023.08.20 val PER: 0.3455
2026-01-03 12:40:59,422: t15.2023.08.25 val PER: 0.3042
2026-01-03 12:40:59,422: t15.2023.08.27 val PER: 0.4293
2026-01-03 12:40:59,422: t15.2023.09.01 val PER: 0.3076
2026-01-03 12:40:59,422: t15.2023.09.03 val PER: 0.4014
2026-01-03 12:40:59,422: t15.2023.09.24 val PER: 0.3277
2026-01-03 12:40:59,422: t15.2023.09.29 val PER: 0.3752
2026-01-03 12:40:59,422: t15.2023.10.01 val PER: 0.4089
2026-01-03 12:40:59,422: t15.2023.10.06 val PER: 0.3208
2026-01-03 12:40:59,422: t15.2023.10.08 val PER: 0.4560
2026-01-03 12:40:59,422: t15.2023.10.13 val PER: 0.4686
2026-01-03 12:40:59,422: t15.2023.10.15 val PER: 0.3902
2026-01-03 12:40:59,422: t15.2023.10.20 val PER: 0.3624
2026-01-03 12:40:59,422: t15.2023.10.22 val PER: 0.3541
2026-01-03 12:40:59,422: t15.2023.11.03 val PER: 0.3976
2026-01-03 12:40:59,423: t15.2023.11.04 val PER: 0.1604
2026-01-03 12:40:59,423: t15.2023.11.17 val PER: 0.2582
2026-01-03 12:40:59,423: t15.2023.11.19 val PER: 0.2196
2026-01-03 12:40:59,423: t15.2023.11.26 val PER: 0.4514
2026-01-03 12:40:59,423: t15.2023.12.03 val PER: 0.4055
2026-01-03 12:40:59,423: t15.2023.12.08 val PER: 0.4015
2026-01-03 12:40:59,423: t15.2023.12.10 val PER: 0.3482
2026-01-03 12:40:59,423: t15.2023.12.17 val PER: 0.4137
2026-01-03 12:40:59,423: t15.2023.12.29 val PER: 0.4063
2026-01-03 12:40:59,423: t15.2024.02.25 val PER: 0.3497
2026-01-03 12:40:59,424: t15.2024.03.08 val PER: 0.5078
2026-01-03 12:40:59,424: t15.2024.03.15 val PER: 0.4503
2026-01-03 12:40:59,424: t15.2024.03.17 val PER: 0.4184
2026-01-03 12:40:59,424: t15.2024.05.10 val PER: 0.4324
2026-01-03 12:40:59,424: t15.2024.06.14 val PER: 0.4054
2026-01-03 12:40:59,424: t15.2024.07.19 val PER: 0.5386
2026-01-03 12:40:59,424: t15.2024.07.21 val PER: 0.3772
2026-01-03 12:40:59,424: t15.2024.07.28 val PER: 0.4221
2026-01-03 12:40:59,424: t15.2025.01.10 val PER: 0.6088
2026-01-03 12:40:59,424: t15.2025.01.12 val PER: 0.4650
2026-01-03 12:40:59,424: t15.2025.03.14 val PER: 0.6435
2026-01-03 12:40:59,424: t15.2025.03.16 val PER: 0.4974
2026-01-03 12:40:59,424: t15.2025.03.30 val PER: 0.6529
2026-01-03 12:40:59,424: t15.2025.04.13 val PER: 0.5007
2026-01-03 12:40:59,425: New best val WER(1gram) 89.34% --> 81.47%
2026-01-03 12:40:59,425: Checkpointing model
2026-01-03 12:40:59,693: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 12:40:59,953: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_1000
2026-01-03 12:41:16,851: Train batch 1200: loss: 33.14 grad norm: 72.72 time: 0.067
2026-01-03 12:41:34,141: Train batch 1400: loss: 36.65 grad norm: 78.56 time: 0.060
2026-01-03 12:41:42,831: Running test after training batch: 1500
2026-01-03 12:41:42,927: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:41:47,694: WER debug example
  GT : you can see the code at this point as well
  PR : yule ent sze the code it this boyde is will
2026-01-03 12:41:47,726: WER debug example
  GT : how does it keep the cost down
  PR : houde does it houp that os
2026-01-03 12:41:49,281: Val batch 1500: PER (avg): 0.3810 CTC Loss (avg): 37.2771 WER(1gram): 76.40% (n=64) time: 6.450
2026-01-03 12:41:49,281: WER lens: avg_true_words=6.16 avg_pred_words=5.03 max_pred_words=11
2026-01-03 12:41:49,281: t15.2023.08.13 val PER: 0.3534
2026-01-03 12:41:49,281: t15.2023.08.18 val PER: 0.3219
2026-01-03 12:41:49,282: t15.2023.08.20 val PER: 0.3074
2026-01-03 12:41:49,282: t15.2023.08.25 val PER: 0.2681
2026-01-03 12:41:49,282: t15.2023.08.27 val PER: 0.4019
2026-01-03 12:41:49,282: t15.2023.09.01 val PER: 0.2744
2026-01-03 12:41:49,282: t15.2023.09.03 val PER: 0.3789
2026-01-03 12:41:49,282: t15.2023.09.24 val PER: 0.3022
2026-01-03 12:41:49,282: t15.2023.09.29 val PER: 0.3357
2026-01-03 12:41:49,282: t15.2023.10.01 val PER: 0.3970
2026-01-03 12:41:49,282: t15.2023.10.06 val PER: 0.2820
2026-01-03 12:41:49,282: t15.2023.10.08 val PER: 0.4371
2026-01-03 12:41:49,282: t15.2023.10.13 val PER: 0.4453
2026-01-03 12:41:49,282: t15.2023.10.15 val PER: 0.3698
2026-01-03 12:41:49,282: t15.2023.10.20 val PER: 0.3289
2026-01-03 12:41:49,283: t15.2023.10.22 val PER: 0.3196
2026-01-03 12:41:49,283: t15.2023.11.03 val PER: 0.3704
2026-01-03 12:41:49,283: t15.2023.11.04 val PER: 0.1229
2026-01-03 12:41:49,283: t15.2023.11.17 val PER: 0.2255
2026-01-03 12:41:49,283: t15.2023.11.19 val PER: 0.1756
2026-01-03 12:41:49,283: t15.2023.11.26 val PER: 0.4116
2026-01-03 12:41:49,283: t15.2023.12.03 val PER: 0.3761
2026-01-03 12:41:49,283: t15.2023.12.08 val PER: 0.3595
2026-01-03 12:41:49,283: t15.2023.12.10 val PER: 0.2970
2026-01-03 12:41:49,283: t15.2023.12.17 val PER: 0.3763
2026-01-03 12:41:49,283: t15.2023.12.29 val PER: 0.3768
2026-01-03 12:41:49,283: t15.2024.02.25 val PER: 0.3062
2026-01-03 12:41:49,283: t15.2024.03.08 val PER: 0.4495
2026-01-03 12:41:49,283: t15.2024.03.15 val PER: 0.4165
2026-01-03 12:41:49,283: t15.2024.03.17 val PER: 0.3835
2026-01-03 12:41:49,283: t15.2024.05.10 val PER: 0.3819
2026-01-03 12:41:49,283: t15.2024.06.14 val PER: 0.3896
2026-01-03 12:41:49,283: t15.2024.07.19 val PER: 0.5260
2026-01-03 12:41:49,284: t15.2024.07.21 val PER: 0.3345
2026-01-03 12:41:49,284: t15.2024.07.28 val PER: 0.3566
2026-01-03 12:41:49,284: t15.2025.01.10 val PER: 0.6253
2026-01-03 12:41:49,284: t15.2025.01.12 val PER: 0.4319
2026-01-03 12:41:49,284: t15.2025.03.14 val PER: 0.6050
2026-01-03 12:41:49,284: t15.2025.03.16 val PER: 0.4516
2026-01-03 12:41:49,284: t15.2025.03.30 val PER: 0.6322
2026-01-03 12:41:49,284: t15.2025.04.13 val PER: 0.4750
2026-01-03 12:41:49,285: New best val WER(1gram) 81.47% --> 76.40%
2026-01-03 12:41:49,285: Checkpointing model
2026-01-03 12:41:49,549: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 12:41:49,800: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_1500
2026-01-03 12:41:58,231: Train batch 1600: loss: 36.74 grad norm: 82.02 time: 0.064
2026-01-03 12:42:15,857: Train batch 1800: loss: 35.57 grad norm: 73.41 time: 0.088
2026-01-03 12:42:33,278: Train batch 2000: loss: 34.41 grad norm: 76.73 time: 0.066
2026-01-03 12:42:33,279: Running test after training batch: 2000
2026-01-03 12:42:33,376: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:42:38,296: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned ease the code at this bonde is will
2026-01-03 12:42:38,324: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the wass
2026-01-03 12:42:39,865: Val batch 2000: PER (avg): 0.3262 CTC Loss (avg): 32.7787 WER(1gram): 69.80% (n=64) time: 6.587
2026-01-03 12:42:39,866: WER lens: avg_true_words=6.16 avg_pred_words=5.59 max_pred_words=11
2026-01-03 12:42:39,866: t15.2023.08.13 val PER: 0.3025
2026-01-03 12:42:39,866: t15.2023.08.18 val PER: 0.2523
2026-01-03 12:42:39,866: t15.2023.08.20 val PER: 0.2558
2026-01-03 12:42:39,866: t15.2023.08.25 val PER: 0.2184
2026-01-03 12:42:39,866: t15.2023.08.27 val PER: 0.3344
2026-01-03 12:42:39,866: t15.2023.09.01 val PER: 0.2370
2026-01-03 12:42:39,866: t15.2023.09.03 val PER: 0.3219
2026-01-03 12:42:39,867: t15.2023.09.24 val PER: 0.2476
2026-01-03 12:42:39,867: t15.2023.09.29 val PER: 0.2725
2026-01-03 12:42:39,867: t15.2023.10.01 val PER: 0.3322
2026-01-03 12:42:39,867: t15.2023.10.06 val PER: 0.2400
2026-01-03 12:42:39,867: t15.2023.10.08 val PER: 0.3911
2026-01-03 12:42:39,867: t15.2023.10.13 val PER: 0.3770
2026-01-03 12:42:39,867: t15.2023.10.15 val PER: 0.3039
2026-01-03 12:42:39,867: t15.2023.10.20 val PER: 0.3121
2026-01-03 12:42:39,867: t15.2023.10.22 val PER: 0.2595
2026-01-03 12:42:39,867: t15.2023.11.03 val PER: 0.3216
2026-01-03 12:42:39,867: t15.2023.11.04 val PER: 0.1092
2026-01-03 12:42:39,867: t15.2023.11.17 val PER: 0.1773
2026-01-03 12:42:39,867: t15.2023.11.19 val PER: 0.1337
2026-01-03 12:42:39,868: t15.2023.11.26 val PER: 0.3667
2026-01-03 12:42:39,868: t15.2023.12.03 val PER: 0.3130
2026-01-03 12:42:39,868: t15.2023.12.08 val PER: 0.3049
2026-01-03 12:42:39,868: t15.2023.12.10 val PER: 0.2668
2026-01-03 12:42:39,868: t15.2023.12.17 val PER: 0.3119
2026-01-03 12:42:39,868: t15.2023.12.29 val PER: 0.3322
2026-01-03 12:42:39,868: t15.2024.02.25 val PER: 0.2753
2026-01-03 12:42:39,868: t15.2024.03.08 val PER: 0.3954
2026-01-03 12:42:39,868: t15.2024.03.15 val PER: 0.3590
2026-01-03 12:42:39,868: t15.2024.03.17 val PER: 0.3389
2026-01-03 12:42:39,868: t15.2024.05.10 val PER: 0.3343
2026-01-03 12:42:39,868: t15.2024.06.14 val PER: 0.3344
2026-01-03 12:42:39,868: t15.2024.07.19 val PER: 0.4562
2026-01-03 12:42:39,868: t15.2024.07.21 val PER: 0.2917
2026-01-03 12:42:39,868: t15.2024.07.28 val PER: 0.3228
2026-01-03 12:42:39,868: t15.2025.01.10 val PER: 0.5344
2026-01-03 12:42:39,868: t15.2025.01.12 val PER: 0.3834
2026-01-03 12:42:39,868: t15.2025.03.14 val PER: 0.5207
2026-01-03 12:42:39,869: t15.2025.03.16 val PER: 0.4045
2026-01-03 12:42:39,869: t15.2025.03.30 val PER: 0.5333
2026-01-03 12:42:39,869: t15.2025.04.13 val PER: 0.4009
2026-01-03 12:42:39,870: New best val WER(1gram) 76.40% --> 69.80%
2026-01-03 12:42:39,870: Checkpointing model
2026-01-03 12:42:40,134: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 12:42:40,387: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_2000
2026-01-03 12:42:57,268: Train batch 2200: loss: 29.20 grad norm: 72.29 time: 0.059
2026-01-03 12:43:14,470: Train batch 2400: loss: 29.03 grad norm: 64.66 time: 0.052
2026-01-03 12:43:23,290: Running test after training batch: 2500
2026-01-03 12:43:23,434: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:43:28,182: WER debug example
  GT : you can see the code at this point as well
  PR : yule end e the code at this point is will
2026-01-03 12:43:28,210: WER debug example
  GT : how does it keep the cost down
  PR : houde just it eke thus us it
2026-01-03 12:43:29,846: Val batch 2500: PER (avg): 0.3020 CTC Loss (avg): 30.2104 WER(1gram): 67.77% (n=64) time: 6.556
2026-01-03 12:43:29,847: WER lens: avg_true_words=6.16 avg_pred_words=5.75 max_pred_words=11
2026-01-03 12:43:29,847: t15.2023.08.13 val PER: 0.2692
2026-01-03 12:43:29,847: t15.2023.08.18 val PER: 0.2364
2026-01-03 12:43:29,847: t15.2023.08.20 val PER: 0.2327
2026-01-03 12:43:29,847: t15.2023.08.25 val PER: 0.2078
2026-01-03 12:43:29,847: t15.2023.08.27 val PER: 0.3215
2026-01-03 12:43:29,847: t15.2023.09.01 val PER: 0.1972
2026-01-03 12:43:29,847: t15.2023.09.03 val PER: 0.2957
2026-01-03 12:43:29,847: t15.2023.09.24 val PER: 0.2233
2026-01-03 12:43:29,848: t15.2023.09.29 val PER: 0.2534
2026-01-03 12:43:29,848: t15.2023.10.01 val PER: 0.3091
2026-01-03 12:43:29,848: t15.2023.10.06 val PER: 0.2099
2026-01-03 12:43:29,848: t15.2023.10.08 val PER: 0.3802
2026-01-03 12:43:29,848: t15.2023.10.13 val PER: 0.3561
2026-01-03 12:43:29,848: t15.2023.10.15 val PER: 0.2920
2026-01-03 12:43:29,848: t15.2023.10.20 val PER: 0.2886
2026-01-03 12:43:29,848: t15.2023.10.22 val PER: 0.2405
2026-01-03 12:43:29,848: t15.2023.11.03 val PER: 0.3053
2026-01-03 12:43:29,848: t15.2023.11.04 val PER: 0.0785
2026-01-03 12:43:29,848: t15.2023.11.17 val PER: 0.1446
2026-01-03 12:43:29,848: t15.2023.11.19 val PER: 0.1198
2026-01-03 12:43:29,848: t15.2023.11.26 val PER: 0.3435
2026-01-03 12:43:29,848: t15.2023.12.03 val PER: 0.2805
2026-01-03 12:43:29,849: t15.2023.12.08 val PER: 0.2770
2026-01-03 12:43:29,849: t15.2023.12.10 val PER: 0.2300
2026-01-03 12:43:29,849: t15.2023.12.17 val PER: 0.2848
2026-01-03 12:43:29,849: t15.2023.12.29 val PER: 0.3040
2026-01-03 12:43:29,849: t15.2024.02.25 val PER: 0.2388
2026-01-03 12:43:29,849: t15.2024.03.08 val PER: 0.3698
2026-01-03 12:43:29,849: t15.2024.03.15 val PER: 0.3508
2026-01-03 12:43:29,849: t15.2024.03.17 val PER: 0.3047
2026-01-03 12:43:29,849: t15.2024.05.10 val PER: 0.3150
2026-01-03 12:43:29,849: t15.2024.06.14 val PER: 0.3155
2026-01-03 12:43:29,849: t15.2024.07.19 val PER: 0.4298
2026-01-03 12:43:29,849: t15.2024.07.21 val PER: 0.2690
2026-01-03 12:43:29,849: t15.2024.07.28 val PER: 0.2934
2026-01-03 12:43:29,849: t15.2025.01.10 val PER: 0.4986
2026-01-03 12:43:29,849: t15.2025.01.12 val PER: 0.3557
2026-01-03 12:43:29,849: t15.2025.03.14 val PER: 0.5059
2026-01-03 12:43:29,850: t15.2025.03.16 val PER: 0.3560
2026-01-03 12:43:29,850: t15.2025.03.30 val PER: 0.5149
2026-01-03 12:43:29,850: t15.2025.04.13 val PER: 0.3809
2026-01-03 12:43:29,851: New best val WER(1gram) 69.80% --> 67.77%
2026-01-03 12:43:29,852: Checkpointing model
2026-01-03 12:43:30,114: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 12:43:30,368: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_2500
2026-01-03 12:43:38,785: Train batch 2600: loss: 35.47 grad norm: 81.79 time: 0.054
2026-01-03 12:43:55,638: Train batch 2800: loss: 25.44 grad norm: 70.68 time: 0.081
2026-01-03 12:44:12,430: Train batch 3000: loss: 31.24 grad norm: 71.63 time: 0.083
2026-01-03 12:44:12,430: Running test after training batch: 3000
2026-01-03 12:44:12,533: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:44:17,257: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-03 12:44:17,285: WER debug example
  GT : how does it keep the cost down
  PR : houde des it yip the rost ent
2026-01-03 12:44:18,951: Val batch 3000: PER (avg): 0.2800 CTC Loss (avg): 27.7809 WER(1gram): 67.26% (n=64) time: 6.521
2026-01-03 12:44:18,951: WER lens: avg_true_words=6.16 avg_pred_words=5.80 max_pred_words=11
2026-01-03 12:44:18,951: t15.2023.08.13 val PER: 0.2692
2026-01-03 12:44:18,951: t15.2023.08.18 val PER: 0.2196
2026-01-03 12:44:18,952: t15.2023.08.20 val PER: 0.2073
2026-01-03 12:44:18,952: t15.2023.08.25 val PER: 0.1867
2026-01-03 12:44:18,952: t15.2023.08.27 val PER: 0.3023
2026-01-03 12:44:18,952: t15.2023.09.01 val PER: 0.1859
2026-01-03 12:44:18,952: t15.2023.09.03 val PER: 0.2767
2026-01-03 12:44:18,952: t15.2023.09.24 val PER: 0.2075
2026-01-03 12:44:18,952: t15.2023.09.29 val PER: 0.2278
2026-01-03 12:44:18,952: t15.2023.10.01 val PER: 0.2972
2026-01-03 12:44:18,952: t15.2023.10.06 val PER: 0.2121
2026-01-03 12:44:18,952: t15.2023.10.08 val PER: 0.3478
2026-01-03 12:44:18,952: t15.2023.10.13 val PER: 0.3499
2026-01-03 12:44:18,952: t15.2023.10.15 val PER: 0.2591
2026-01-03 12:44:18,952: t15.2023.10.20 val PER: 0.2517
2026-01-03 12:44:18,952: t15.2023.10.22 val PER: 0.2127
2026-01-03 12:44:18,953: t15.2023.11.03 val PER: 0.2727
2026-01-03 12:44:18,953: t15.2023.11.04 val PER: 0.0717
2026-01-03 12:44:18,953: t15.2023.11.17 val PER: 0.1244
2026-01-03 12:44:18,953: t15.2023.11.19 val PER: 0.1317
2026-01-03 12:44:18,953: t15.2023.11.26 val PER: 0.3058
2026-01-03 12:44:18,953: t15.2023.12.03 val PER: 0.2584
2026-01-03 12:44:18,953: t15.2023.12.08 val PER: 0.2590
2026-01-03 12:44:18,953: t15.2023.12.10 val PER: 0.2168
2026-01-03 12:44:18,953: t15.2023.12.17 val PER: 0.2723
2026-01-03 12:44:18,953: t15.2023.12.29 val PER: 0.2773
2026-01-03 12:44:18,953: t15.2024.02.25 val PER: 0.2346
2026-01-03 12:44:18,953: t15.2024.03.08 val PER: 0.3570
2026-01-03 12:44:18,953: t15.2024.03.15 val PER: 0.3258
2026-01-03 12:44:18,954: t15.2024.03.17 val PER: 0.2901
2026-01-03 12:44:18,954: t15.2024.05.10 val PER: 0.2957
2026-01-03 12:44:18,954: t15.2024.06.14 val PER: 0.3028
2026-01-03 12:44:18,954: t15.2024.07.19 val PER: 0.3988
2026-01-03 12:44:18,954: t15.2024.07.21 val PER: 0.2331
2026-01-03 12:44:18,954: t15.2024.07.28 val PER: 0.2721
2026-01-03 12:44:18,954: t15.2025.01.10 val PER: 0.4807
2026-01-03 12:44:18,954: t15.2025.01.12 val PER: 0.3218
2026-01-03 12:44:18,954: t15.2025.03.14 val PER: 0.4571
2026-01-03 12:44:18,954: t15.2025.03.16 val PER: 0.3233
2026-01-03 12:44:18,954: t15.2025.03.30 val PER: 0.4644
2026-01-03 12:44:18,955: t15.2025.04.13 val PER: 0.3524
2026-01-03 12:44:18,955: New best val WER(1gram) 67.77% --> 67.26%
2026-01-03 12:44:18,955: Checkpointing model
2026-01-03 12:44:19,216: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 12:44:19,467: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_3000
2026-01-03 12:44:36,758: Train batch 3200: loss: 26.44 grad norm: 68.19 time: 0.075
2026-01-03 12:44:54,031: Train batch 3400: loss: 18.32 grad norm: 55.67 time: 0.049
2026-01-03 12:45:02,916: Running test after training batch: 3500
2026-01-03 12:45:03,016: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:45:07,964: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point will
2026-01-03 12:45:07,991: WER debug example
  GT : how does it keep the cost down
  PR : houde des it eke thus us get
2026-01-03 12:45:09,534: Val batch 3500: PER (avg): 0.2662 CTC Loss (avg): 26.5680 WER(1gram): 66.75% (n=64) time: 6.618
2026-01-03 12:45:09,535: WER lens: avg_true_words=6.16 avg_pred_words=6.00 max_pred_words=11
2026-01-03 12:45:09,535: t15.2023.08.13 val PER: 0.2360
2026-01-03 12:45:09,535: t15.2023.08.18 val PER: 0.2121
2026-01-03 12:45:09,535: t15.2023.08.20 val PER: 0.2081
2026-01-03 12:45:09,535: t15.2023.08.25 val PER: 0.1762
2026-01-03 12:45:09,535: t15.2023.08.27 val PER: 0.2717
2026-01-03 12:45:09,535: t15.2023.09.01 val PER: 0.1843
2026-01-03 12:45:09,535: t15.2023.09.03 val PER: 0.2589
2026-01-03 12:45:09,535: t15.2023.09.24 val PER: 0.2075
2026-01-03 12:45:09,535: t15.2023.09.29 val PER: 0.2189
2026-01-03 12:45:09,536: t15.2023.10.01 val PER: 0.2774
2026-01-03 12:45:09,536: t15.2023.10.06 val PER: 0.1938
2026-01-03 12:45:09,536: t15.2023.10.08 val PER: 0.3383
2026-01-03 12:45:09,536: t15.2023.10.13 val PER: 0.3157
2026-01-03 12:45:09,536: t15.2023.10.15 val PER: 0.2452
2026-01-03 12:45:09,536: t15.2023.10.20 val PER: 0.2383
2026-01-03 12:45:09,536: t15.2023.10.22 val PER: 0.2060
2026-01-03 12:45:09,536: t15.2023.11.03 val PER: 0.2626
2026-01-03 12:45:09,536: t15.2023.11.04 val PER: 0.0819
2026-01-03 12:45:09,536: t15.2023.11.17 val PER: 0.1213
2026-01-03 12:45:09,536: t15.2023.11.19 val PER: 0.1078
2026-01-03 12:45:09,536: t15.2023.11.26 val PER: 0.2833
2026-01-03 12:45:09,536: t15.2023.12.03 val PER: 0.2458
2026-01-03 12:45:09,536: t15.2023.12.08 val PER: 0.2503
2026-01-03 12:45:09,536: t15.2023.12.10 val PER: 0.2089
2026-01-03 12:45:09,537: t15.2023.12.17 val PER: 0.2516
2026-01-03 12:45:09,537: t15.2023.12.29 val PER: 0.2546
2026-01-03 12:45:09,537: t15.2024.02.25 val PER: 0.2191
2026-01-03 12:45:09,537: t15.2024.03.08 val PER: 0.3343
2026-01-03 12:45:09,537: t15.2024.03.15 val PER: 0.3171
2026-01-03 12:45:09,537: t15.2024.03.17 val PER: 0.2803
2026-01-03 12:45:09,537: t15.2024.05.10 val PER: 0.2571
2026-01-03 12:45:09,537: t15.2024.06.14 val PER: 0.2855
2026-01-03 12:45:09,537: t15.2024.07.19 val PER: 0.3876
2026-01-03 12:45:09,537: t15.2024.07.21 val PER: 0.2352
2026-01-03 12:45:09,537: t15.2024.07.28 val PER: 0.2860
2026-01-03 12:45:09,537: t15.2025.01.10 val PER: 0.4504
2026-01-03 12:45:09,537: t15.2025.01.12 val PER: 0.3033
2026-01-03 12:45:09,538: t15.2025.03.14 val PER: 0.4379
2026-01-03 12:45:09,538: t15.2025.03.16 val PER: 0.3102
2026-01-03 12:45:09,538: t15.2025.03.30 val PER: 0.4356
2026-01-03 12:45:09,538: t15.2025.04.13 val PER: 0.3210
2026-01-03 12:45:09,539: New best val WER(1gram) 67.26% --> 66.75%
2026-01-03 12:45:09,539: Checkpointing model
2026-01-03 12:45:09,805: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 12:45:10,056: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_3500
2026-01-03 12:45:18,637: Train batch 3600: loss: 22.65 grad norm: 61.70 time: 0.066
2026-01-03 12:45:35,844: Train batch 3800: loss: 25.39 grad norm: 66.99 time: 0.067
2026-01-03 12:45:52,954: Train batch 4000: loss: 19.23 grad norm: 51.91 time: 0.056
2026-01-03 12:45:52,954: Running test after training batch: 4000
2026-01-03 12:45:53,102: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:45:57,840: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-03 12:45:57,868: WER debug example
  GT : how does it keep the cost down
  PR : out dust it keep the cost nett
2026-01-03 12:45:59,452: Val batch 4000: PER (avg): 0.2492 CTC Loss (avg): 24.4686 WER(1gram): 64.47% (n=64) time: 6.498
2026-01-03 12:45:59,452: WER lens: avg_true_words=6.16 avg_pred_words=6.02 max_pred_words=11
2026-01-03 12:45:59,452: t15.2023.08.13 val PER: 0.2277
2026-01-03 12:45:59,452: t15.2023.08.18 val PER: 0.2003
2026-01-03 12:45:59,453: t15.2023.08.20 val PER: 0.1922
2026-01-03 12:45:59,453: t15.2023.08.25 val PER: 0.1551
2026-01-03 12:45:59,453: t15.2023.08.27 val PER: 0.2910
2026-01-03 12:45:59,453: t15.2023.09.01 val PER: 0.1583
2026-01-03 12:45:59,453: t15.2023.09.03 val PER: 0.2494
2026-01-03 12:45:59,453: t15.2023.09.24 val PER: 0.1930
2026-01-03 12:45:59,453: t15.2023.09.29 val PER: 0.2036
2026-01-03 12:45:59,453: t15.2023.10.01 val PER: 0.2583
2026-01-03 12:45:59,454: t15.2023.10.06 val PER: 0.1712
2026-01-03 12:45:59,454: t15.2023.10.08 val PER: 0.3234
2026-01-03 12:45:59,454: t15.2023.10.13 val PER: 0.3095
2026-01-03 12:45:59,454: t15.2023.10.15 val PER: 0.2373
2026-01-03 12:45:59,454: t15.2023.10.20 val PER: 0.2315
2026-01-03 12:45:59,454: t15.2023.10.22 val PER: 0.1860
2026-01-03 12:45:59,454: t15.2023.11.03 val PER: 0.2429
2026-01-03 12:45:59,454: t15.2023.11.04 val PER: 0.0683
2026-01-03 12:45:59,454: t15.2023.11.17 val PER: 0.1104
2026-01-03 12:45:59,455: t15.2023.11.19 val PER: 0.1018
2026-01-03 12:45:59,455: t15.2023.11.26 val PER: 0.2580
2026-01-03 12:45:59,455: t15.2023.12.03 val PER: 0.2101
2026-01-03 12:45:59,455: t15.2023.12.08 val PER: 0.2217
2026-01-03 12:45:59,455: t15.2023.12.10 val PER: 0.1774
2026-01-03 12:45:59,455: t15.2023.12.17 val PER: 0.2484
2026-01-03 12:45:59,455: t15.2023.12.29 val PER: 0.2622
2026-01-03 12:45:59,455: t15.2024.02.25 val PER: 0.1924
2026-01-03 12:45:59,456: t15.2024.03.08 val PER: 0.3400
2026-01-03 12:45:59,456: t15.2024.03.15 val PER: 0.3071
2026-01-03 12:45:59,456: t15.2024.03.17 val PER: 0.2594
2026-01-03 12:45:59,456: t15.2024.05.10 val PER: 0.2600
2026-01-03 12:45:59,456: t15.2024.06.14 val PER: 0.2697
2026-01-03 12:45:59,456: t15.2024.07.19 val PER: 0.3691
2026-01-03 12:45:59,456: t15.2024.07.21 val PER: 0.1890
2026-01-03 12:45:59,456: t15.2024.07.28 val PER: 0.2382
2026-01-03 12:45:59,457: t15.2025.01.10 val PER: 0.4201
2026-01-03 12:45:59,457: t15.2025.01.12 val PER: 0.2864
2026-01-03 12:45:59,457: t15.2025.03.14 val PER: 0.4112
2026-01-03 12:45:59,457: t15.2025.03.16 val PER: 0.3076
2026-01-03 12:45:59,457: t15.2025.03.30 val PER: 0.4195
2026-01-03 12:45:59,457: t15.2025.04.13 val PER: 0.3167
2026-01-03 12:45:59,457: New best val WER(1gram) 66.75% --> 64.47%
2026-01-03 12:45:59,457: Checkpointing model
2026-01-03 12:45:59,721: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 12:45:59,973: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_4000
2026-01-03 12:46:17,407: Train batch 4200: loss: 22.92 grad norm: 66.46 time: 0.079
2026-01-03 12:46:34,915: Train batch 4400: loss: 17.31 grad norm: 54.29 time: 0.066
2026-01-03 12:46:43,790: Running test after training batch: 4500
2026-01-03 12:46:43,888: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:46:48,626: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-03 12:46:48,654: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it yip the cost get
2026-01-03 12:46:50,236: Val batch 4500: PER (avg): 0.2384 CTC Loss (avg): 23.3106 WER(1gram): 62.69% (n=64) time: 6.445
2026-01-03 12:46:50,237: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 12:46:50,237: t15.2023.08.13 val PER: 0.2131
2026-01-03 12:46:50,237: t15.2023.08.18 val PER: 0.1819
2026-01-03 12:46:50,237: t15.2023.08.20 val PER: 0.1906
2026-01-03 12:46:50,237: t15.2023.08.25 val PER: 0.1386
2026-01-03 12:46:50,237: t15.2023.08.27 val PER: 0.2492
2026-01-03 12:46:50,238: t15.2023.09.01 val PER: 0.1477
2026-01-03 12:46:50,238: t15.2023.09.03 val PER: 0.2268
2026-01-03 12:46:50,238: t15.2023.09.24 val PER: 0.1711
2026-01-03 12:46:50,238: t15.2023.09.29 val PER: 0.1997
2026-01-03 12:46:50,238: t15.2023.10.01 val PER: 0.2530
2026-01-03 12:46:50,238: t15.2023.10.06 val PER: 0.1507
2026-01-03 12:46:50,238: t15.2023.10.08 val PER: 0.3166
2026-01-03 12:46:50,238: t15.2023.10.13 val PER: 0.3064
2026-01-03 12:46:50,238: t15.2023.10.15 val PER: 0.2301
2026-01-03 12:46:50,238: t15.2023.10.20 val PER: 0.2282
2026-01-03 12:46:50,238: t15.2023.10.22 val PER: 0.1871
2026-01-03 12:46:50,238: t15.2023.11.03 val PER: 0.2374
2026-01-03 12:46:50,238: t15.2023.11.04 val PER: 0.0546
2026-01-03 12:46:50,239: t15.2023.11.17 val PER: 0.1042
2026-01-03 12:46:50,239: t15.2023.11.19 val PER: 0.0998
2026-01-03 12:46:50,239: t15.2023.11.26 val PER: 0.2732
2026-01-03 12:46:50,239: t15.2023.12.03 val PER: 0.2185
2026-01-03 12:46:50,239: t15.2023.12.08 val PER: 0.2077
2026-01-03 12:46:50,239: t15.2023.12.10 val PER: 0.1853
2026-01-03 12:46:50,239: t15.2023.12.17 val PER: 0.2308
2026-01-03 12:46:50,239: t15.2023.12.29 val PER: 0.2382
2026-01-03 12:46:50,239: t15.2024.02.25 val PER: 0.2079
2026-01-03 12:46:50,239: t15.2024.03.08 val PER: 0.3314
2026-01-03 12:46:50,239: t15.2024.03.15 val PER: 0.2946
2026-01-03 12:46:50,239: t15.2024.03.17 val PER: 0.2427
2026-01-03 12:46:50,239: t15.2024.05.10 val PER: 0.2585
2026-01-03 12:46:50,239: t15.2024.06.14 val PER: 0.2413
2026-01-03 12:46:50,239: t15.2024.07.19 val PER: 0.3388
2026-01-03 12:46:50,240: t15.2024.07.21 val PER: 0.1752
2026-01-03 12:46:50,240: t15.2024.07.28 val PER: 0.2235
2026-01-03 12:46:50,240: t15.2025.01.10 val PER: 0.4118
2026-01-03 12:46:50,240: t15.2025.01.12 val PER: 0.2594
2026-01-03 12:46:50,240: t15.2025.03.14 val PER: 0.3979
2026-01-03 12:46:50,240: t15.2025.03.16 val PER: 0.3050
2026-01-03 12:46:50,240: t15.2025.03.30 val PER: 0.4057
2026-01-03 12:46:50,240: t15.2025.04.13 val PER: 0.3053
2026-01-03 12:46:50,241: New best val WER(1gram) 64.47% --> 62.69%
2026-01-03 12:46:50,241: Checkpointing model
2026-01-03 12:46:50,507: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 12:46:50,759: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_4500
2026-01-03 12:46:59,414: Train batch 4600: loss: 20.19 grad norm: 67.79 time: 0.062
2026-01-03 12:47:16,994: Train batch 4800: loss: 13.39 grad norm: 51.33 time: 0.063
2026-01-03 12:47:34,789: Train batch 5000: loss: 31.63 grad norm: 86.43 time: 0.064
2026-01-03 12:47:34,790: Running test after training batch: 5000
2026-01-03 12:47:34,917: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:47:39,897: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point as will
2026-01-03 12:47:39,925: WER debug example
  GT : how does it keep the cost down
  PR : howled just it heap the cost nett
2026-01-03 12:47:41,521: Val batch 5000: PER (avg): 0.2278 CTC Loss (avg): 22.1834 WER(1gram): 59.39% (n=64) time: 6.731
2026-01-03 12:47:41,522: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=11
2026-01-03 12:47:41,522: t15.2023.08.13 val PER: 0.1985
2026-01-03 12:47:41,522: t15.2023.08.18 val PER: 0.1794
2026-01-03 12:47:41,522: t15.2023.08.20 val PER: 0.1755
2026-01-03 12:47:41,522: t15.2023.08.25 val PER: 0.1340
2026-01-03 12:47:41,522: t15.2023.08.27 val PER: 0.2428
2026-01-03 12:47:41,523: t15.2023.09.01 val PER: 0.1420
2026-01-03 12:47:41,523: t15.2023.09.03 val PER: 0.2316
2026-01-03 12:47:41,523: t15.2023.09.24 val PER: 0.1893
2026-01-03 12:47:41,523: t15.2023.09.29 val PER: 0.1883
2026-01-03 12:47:41,523: t15.2023.10.01 val PER: 0.2431
2026-01-03 12:47:41,523: t15.2023.10.06 val PER: 0.1496
2026-01-03 12:47:41,523: t15.2023.10.08 val PER: 0.3072
2026-01-03 12:47:41,523: t15.2023.10.13 val PER: 0.2785
2026-01-03 12:47:41,523: t15.2023.10.15 val PER: 0.2208
2026-01-03 12:47:41,523: t15.2023.10.20 val PER: 0.2349
2026-01-03 12:47:41,523: t15.2023.10.22 val PER: 0.1682
2026-01-03 12:47:41,524: t15.2023.11.03 val PER: 0.2293
2026-01-03 12:47:41,524: t15.2023.11.04 val PER: 0.0614
2026-01-03 12:47:41,524: t15.2023.11.17 val PER: 0.0871
2026-01-03 12:47:41,524: t15.2023.11.19 val PER: 0.0798
2026-01-03 12:47:41,524: t15.2023.11.26 val PER: 0.2442
2026-01-03 12:47:41,524: t15.2023.12.03 val PER: 0.1975
2026-01-03 12:47:41,524: t15.2023.12.08 val PER: 0.1957
2026-01-03 12:47:41,524: t15.2023.12.10 val PER: 0.1669
2026-01-03 12:47:41,524: t15.2023.12.17 val PER: 0.2193
2026-01-03 12:47:41,525: t15.2023.12.29 val PER: 0.2217
2026-01-03 12:47:41,525: t15.2024.02.25 val PER: 0.1994
2026-01-03 12:47:41,525: t15.2024.03.08 val PER: 0.3144
2026-01-03 12:47:41,525: t15.2024.03.15 val PER: 0.2770
2026-01-03 12:47:41,525: t15.2024.03.17 val PER: 0.2357
2026-01-03 12:47:41,525: t15.2024.05.10 val PER: 0.2452
2026-01-03 12:47:41,525: t15.2024.06.14 val PER: 0.2476
2026-01-03 12:47:41,525: t15.2024.07.19 val PER: 0.3441
2026-01-03 12:47:41,525: t15.2024.07.21 val PER: 0.1828
2026-01-03 12:47:41,525: t15.2024.07.28 val PER: 0.2110
2026-01-03 12:47:41,526: t15.2025.01.10 val PER: 0.3802
2026-01-03 12:47:41,526: t15.2025.01.12 val PER: 0.2579
2026-01-03 12:47:41,526: t15.2025.03.14 val PER: 0.3817
2026-01-03 12:47:41,526: t15.2025.03.16 val PER: 0.2644
2026-01-03 12:47:41,526: t15.2025.03.30 val PER: 0.3920
2026-01-03 12:47:41,526: t15.2025.04.13 val PER: 0.2981
2026-01-03 12:47:41,526: New best val WER(1gram) 62.69% --> 59.39%
2026-01-03 12:47:41,526: Checkpointing model
2026-01-03 12:47:41,793: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 12:47:42,046: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_5000
2026-01-03 12:47:59,329: Train batch 5200: loss: 16.54 grad norm: 58.58 time: 0.051
2026-01-03 12:48:16,305: Train batch 5400: loss: 17.72 grad norm: 59.81 time: 0.067
2026-01-03 12:48:24,883: Running test after training batch: 5500
2026-01-03 12:48:25,046: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:48:29,787: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point will
2026-01-03 12:48:29,816: WER debug example
  GT : how does it keep the cost down
  PR : aue just it keep the cost nett
2026-01-03 12:48:31,365: Val batch 5500: PER (avg): 0.2150 CTC Loss (avg): 21.0774 WER(1gram): 56.85% (n=64) time: 6.481
2026-01-03 12:48:31,366: WER lens: avg_true_words=6.16 avg_pred_words=6.00 max_pred_words=11
2026-01-03 12:48:31,366: t15.2023.08.13 val PER: 0.1830
2026-01-03 12:48:31,366: t15.2023.08.18 val PER: 0.1635
2026-01-03 12:48:31,366: t15.2023.08.20 val PER: 0.1676
2026-01-03 12:48:31,366: t15.2023.08.25 val PER: 0.1250
2026-01-03 12:48:31,366: t15.2023.08.27 val PER: 0.2379
2026-01-03 12:48:31,366: t15.2023.09.01 val PER: 0.1250
2026-01-03 12:48:31,366: t15.2023.09.03 val PER: 0.2197
2026-01-03 12:48:31,366: t15.2023.09.24 val PER: 0.1845
2026-01-03 12:48:31,366: t15.2023.09.29 val PER: 0.1755
2026-01-03 12:48:31,367: t15.2023.10.01 val PER: 0.2299
2026-01-03 12:48:31,367: t15.2023.10.06 val PER: 0.1378
2026-01-03 12:48:31,367: t15.2023.10.08 val PER: 0.2991
2026-01-03 12:48:31,367: t15.2023.10.13 val PER: 0.2684
2026-01-03 12:48:31,367: t15.2023.10.15 val PER: 0.2083
2026-01-03 12:48:31,367: t15.2023.10.20 val PER: 0.2215
2026-01-03 12:48:31,367: t15.2023.10.22 val PER: 0.1626
2026-01-03 12:48:31,367: t15.2023.11.03 val PER: 0.2205
2026-01-03 12:48:31,367: t15.2023.11.04 val PER: 0.0648
2026-01-03 12:48:31,367: t15.2023.11.17 val PER: 0.0902
2026-01-03 12:48:31,367: t15.2023.11.19 val PER: 0.0639
2026-01-03 12:48:31,368: t15.2023.11.26 val PER: 0.2239
2026-01-03 12:48:31,368: t15.2023.12.03 val PER: 0.1870
2026-01-03 12:48:31,368: t15.2023.12.08 val PER: 0.1891
2026-01-03 12:48:31,368: t15.2023.12.10 val PER: 0.1432
2026-01-03 12:48:31,368: t15.2023.12.17 val PER: 0.2297
2026-01-03 12:48:31,368: t15.2023.12.29 val PER: 0.2135
2026-01-03 12:48:31,368: t15.2024.02.25 val PER: 0.1854
2026-01-03 12:48:31,368: t15.2024.03.08 val PER: 0.3016
2026-01-03 12:48:31,368: t15.2024.03.15 val PER: 0.2570
2026-01-03 12:48:31,368: t15.2024.03.17 val PER: 0.2322
2026-01-03 12:48:31,368: t15.2024.05.10 val PER: 0.2377
2026-01-03 12:48:31,368: t15.2024.06.14 val PER: 0.2271
2026-01-03 12:48:31,368: t15.2024.07.19 val PER: 0.3125
2026-01-03 12:48:31,368: t15.2024.07.21 val PER: 0.1531
2026-01-03 12:48:31,368: t15.2024.07.28 val PER: 0.2022
2026-01-03 12:48:31,368: t15.2025.01.10 val PER: 0.3774
2026-01-03 12:48:31,368: t15.2025.01.12 val PER: 0.2263
2026-01-03 12:48:31,369: t15.2025.03.14 val PER: 0.3609
2026-01-03 12:48:31,369: t15.2025.03.16 val PER: 0.2736
2026-01-03 12:48:31,369: t15.2025.03.30 val PER: 0.3621
2026-01-03 12:48:31,369: t15.2025.04.13 val PER: 0.2939
2026-01-03 12:48:31,370: New best val WER(1gram) 59.39% --> 56.85%
2026-01-03 12:48:31,370: Checkpointing model
2026-01-03 12:48:31,633: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 12:48:31,884: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_5500
2026-01-03 12:48:40,572: Train batch 5600: loss: 19.75 grad norm: 62.92 time: 0.062
2026-01-03 12:48:58,092: Train batch 5800: loss: 13.69 grad norm: 58.09 time: 0.081
2026-01-03 12:49:15,428: Train batch 6000: loss: 13.97 grad norm: 54.38 time: 0.049
2026-01-03 12:49:15,428: Running test after training batch: 6000
2026-01-03 12:49:15,608: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:49:20,467: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-03 12:49:20,498: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 12:49:22,162: Val batch 6000: PER (avg): 0.2127 CTC Loss (avg): 20.7792 WER(1gram): 58.63% (n=64) time: 6.733
2026-01-03 12:49:22,162: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 12:49:22,163: t15.2023.08.13 val PER: 0.1850
2026-01-03 12:49:22,163: t15.2023.08.18 val PER: 0.1668
2026-01-03 12:49:22,163: t15.2023.08.20 val PER: 0.1684
2026-01-03 12:49:22,163: t15.2023.08.25 val PER: 0.1220
2026-01-03 12:49:22,163: t15.2023.08.27 val PER: 0.2460
2026-01-03 12:49:22,163: t15.2023.09.01 val PER: 0.1339
2026-01-03 12:49:22,163: t15.2023.09.03 val PER: 0.2209
2026-01-03 12:49:22,163: t15.2023.09.24 val PER: 0.1602
2026-01-03 12:49:22,163: t15.2023.09.29 val PER: 0.1698
2026-01-03 12:49:22,163: t15.2023.10.01 val PER: 0.2332
2026-01-03 12:49:22,163: t15.2023.10.06 val PER: 0.1292
2026-01-03 12:49:22,163: t15.2023.10.08 val PER: 0.2842
2026-01-03 12:49:22,164: t15.2023.10.13 val PER: 0.2630
2026-01-03 12:49:22,164: t15.2023.10.15 val PER: 0.2162
2026-01-03 12:49:22,164: t15.2023.10.20 val PER: 0.2047
2026-01-03 12:49:22,164: t15.2023.10.22 val PER: 0.1748
2026-01-03 12:49:22,164: t15.2023.11.03 val PER: 0.2307
2026-01-03 12:49:22,164: t15.2023.11.04 val PER: 0.0580
2026-01-03 12:49:22,164: t15.2023.11.17 val PER: 0.0855
2026-01-03 12:49:22,164: t15.2023.11.19 val PER: 0.0858
2026-01-03 12:49:22,164: t15.2023.11.26 val PER: 0.2181
2026-01-03 12:49:22,165: t15.2023.12.03 val PER: 0.1754
2026-01-03 12:49:22,165: t15.2023.12.08 val PER: 0.1771
2026-01-03 12:49:22,165: t15.2023.12.10 val PER: 0.1511
2026-01-03 12:49:22,165: t15.2023.12.17 val PER: 0.2100
2026-01-03 12:49:22,165: t15.2023.12.29 val PER: 0.2093
2026-01-03 12:49:22,165: t15.2024.02.25 val PER: 0.1643
2026-01-03 12:49:22,165: t15.2024.03.08 val PER: 0.2845
2026-01-03 12:49:22,165: t15.2024.03.15 val PER: 0.2639
2026-01-03 12:49:22,165: t15.2024.03.17 val PER: 0.2141
2026-01-03 12:49:22,165: t15.2024.05.10 val PER: 0.2288
2026-01-03 12:49:22,165: t15.2024.06.14 val PER: 0.2208
2026-01-03 12:49:22,165: t15.2024.07.19 val PER: 0.3078
2026-01-03 12:49:22,165: t15.2024.07.21 val PER: 0.1710
2026-01-03 12:49:22,165: t15.2024.07.28 val PER: 0.1963
2026-01-03 12:49:22,165: t15.2025.01.10 val PER: 0.3664
2026-01-03 12:49:22,165: t15.2025.01.12 val PER: 0.2271
2026-01-03 12:49:22,166: t15.2025.03.14 val PER: 0.3743
2026-01-03 12:49:22,166: t15.2025.03.16 val PER: 0.2631
2026-01-03 12:49:22,166: t15.2025.03.30 val PER: 0.3770
2026-01-03 12:49:22,166: t15.2025.04.13 val PER: 0.2668
2026-01-03 12:49:22,406: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_6000
2026-01-03 12:49:39,681: Train batch 6200: loss: 16.57 grad norm: 57.58 time: 0.069
2026-01-03 12:49:56,905: Train batch 6400: loss: 18.42 grad norm: 66.83 time: 0.062
2026-01-03 12:50:05,378: Running test after training batch: 6500
2026-01-03 12:50:05,502: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:50:10,426: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 12:50:10,455: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost tent
2026-01-03 12:50:12,058: Val batch 6500: PER (avg): 0.2037 CTC Loss (avg): 20.1626 WER(1gram): 53.55% (n=64) time: 6.680
2026-01-03 12:50:12,059: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 12:50:12,059: t15.2023.08.13 val PER: 0.1694
2026-01-03 12:50:12,059: t15.2023.08.18 val PER: 0.1425
2026-01-03 12:50:12,059: t15.2023.08.20 val PER: 0.1581
2026-01-03 12:50:12,059: t15.2023.08.25 val PER: 0.1054
2026-01-03 12:50:12,059: t15.2023.08.27 val PER: 0.2235
2026-01-03 12:50:12,059: t15.2023.09.01 val PER: 0.1201
2026-01-03 12:50:12,059: t15.2023.09.03 val PER: 0.2067
2026-01-03 12:50:12,059: t15.2023.09.24 val PER: 0.1650
2026-01-03 12:50:12,059: t15.2023.09.29 val PER: 0.1704
2026-01-03 12:50:12,060: t15.2023.10.01 val PER: 0.2166
2026-01-03 12:50:12,060: t15.2023.10.06 val PER: 0.1378
2026-01-03 12:50:12,060: t15.2023.10.08 val PER: 0.2977
2026-01-03 12:50:12,060: t15.2023.10.13 val PER: 0.2676
2026-01-03 12:50:12,060: t15.2023.10.15 val PER: 0.2076
2026-01-03 12:50:12,060: t15.2023.10.20 val PER: 0.2148
2026-01-03 12:50:12,060: t15.2023.10.22 val PER: 0.1626
2026-01-03 12:50:12,060: t15.2023.11.03 val PER: 0.2171
2026-01-03 12:50:12,060: t15.2023.11.04 val PER: 0.0546
2026-01-03 12:50:12,060: t15.2023.11.17 val PER: 0.0747
2026-01-03 12:50:12,060: t15.2023.11.19 val PER: 0.0639
2026-01-03 12:50:12,061: t15.2023.11.26 val PER: 0.2109
2026-01-03 12:50:12,061: t15.2023.12.03 val PER: 0.1691
2026-01-03 12:50:12,061: t15.2023.12.08 val PER: 0.1644
2026-01-03 12:50:12,061: t15.2023.12.10 val PER: 0.1419
2026-01-03 12:50:12,061: t15.2023.12.17 val PER: 0.1985
2026-01-03 12:50:12,061: t15.2023.12.29 val PER: 0.1887
2026-01-03 12:50:12,061: t15.2024.02.25 val PER: 0.1657
2026-01-03 12:50:12,061: t15.2024.03.08 val PER: 0.2788
2026-01-03 12:50:12,061: t15.2024.03.15 val PER: 0.2502
2026-01-03 12:50:12,062: t15.2024.03.17 val PER: 0.2120
2026-01-03 12:50:12,062: t15.2024.05.10 val PER: 0.2288
2026-01-03 12:50:12,062: t15.2024.06.14 val PER: 0.2208
2026-01-03 12:50:12,062: t15.2024.07.19 val PER: 0.3032
2026-01-03 12:50:12,062: t15.2024.07.21 val PER: 0.1579
2026-01-03 12:50:12,062: t15.2024.07.28 val PER: 0.1882
2026-01-03 12:50:12,062: t15.2025.01.10 val PER: 0.3595
2026-01-03 12:50:12,062: t15.2025.01.12 val PER: 0.2117
2026-01-03 12:50:12,062: t15.2025.03.14 val PER: 0.3787
2026-01-03 12:50:12,062: t15.2025.03.16 val PER: 0.2408
2026-01-03 12:50:12,062: t15.2025.03.30 val PER: 0.3575
2026-01-03 12:50:12,062: t15.2025.04.13 val PER: 0.2653
2026-01-03 12:50:12,063: New best val WER(1gram) 56.85% --> 53.55%
2026-01-03 12:50:12,063: Checkpointing model
2026-01-03 12:50:12,326: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 12:50:12,577: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_6500
2026-01-03 12:50:20,973: Train batch 6600: loss: 12.57 grad norm: 54.38 time: 0.045
2026-01-03 12:50:38,239: Train batch 6800: loss: 15.32 grad norm: 56.56 time: 0.048
2026-01-03 12:50:55,271: Train batch 7000: loss: 17.34 grad norm: 64.55 time: 0.060
2026-01-03 12:50:55,271: Running test after training batch: 7000
2026-01-03 12:50:55,414: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:51:00,130: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point as will
2026-01-03 12:51:00,160: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nett
2026-01-03 12:51:01,798: Val batch 7000: PER (avg): 0.1958 CTC Loss (avg): 19.1397 WER(1gram): 55.08% (n=64) time: 6.527
2026-01-03 12:51:01,799: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-03 12:51:01,799: t15.2023.08.13 val PER: 0.1549
2026-01-03 12:51:01,799: t15.2023.08.18 val PER: 0.1375
2026-01-03 12:51:01,799: t15.2023.08.20 val PER: 0.1573
2026-01-03 12:51:01,799: t15.2023.08.25 val PER: 0.0994
2026-01-03 12:51:01,799: t15.2023.08.27 val PER: 0.2251
2026-01-03 12:51:01,799: t15.2023.09.01 val PER: 0.1144
2026-01-03 12:51:01,799: t15.2023.09.03 val PER: 0.1865
2026-01-03 12:51:01,799: t15.2023.09.24 val PER: 0.1638
2026-01-03 12:51:01,799: t15.2023.09.29 val PER: 0.1634
2026-01-03 12:51:01,800: t15.2023.10.01 val PER: 0.2153
2026-01-03 12:51:01,800: t15.2023.10.06 val PER: 0.1055
2026-01-03 12:51:01,800: t15.2023.10.08 val PER: 0.2788
2026-01-03 12:51:01,800: t15.2023.10.13 val PER: 0.2583
2026-01-03 12:51:01,800: t15.2023.10.15 val PER: 0.1931
2026-01-03 12:51:01,800: t15.2023.10.20 val PER: 0.1980
2026-01-03 12:51:01,800: t15.2023.10.22 val PER: 0.1470
2026-01-03 12:51:01,800: t15.2023.11.03 val PER: 0.2049
2026-01-03 12:51:01,800: t15.2023.11.04 val PER: 0.0444
2026-01-03 12:51:01,800: t15.2023.11.17 val PER: 0.0669
2026-01-03 12:51:01,801: t15.2023.11.19 val PER: 0.0599
2026-01-03 12:51:01,801: t15.2023.11.26 val PER: 0.2065
2026-01-03 12:51:01,801: t15.2023.12.03 val PER: 0.1702
2026-01-03 12:51:01,801: t15.2023.12.08 val PER: 0.1558
2026-01-03 12:51:01,801: t15.2023.12.10 val PER: 0.1511
2026-01-03 12:51:01,801: t15.2023.12.17 val PER: 0.1798
2026-01-03 12:51:01,801: t15.2023.12.29 val PER: 0.1963
2026-01-03 12:51:01,801: t15.2024.02.25 val PER: 0.1531
2026-01-03 12:51:01,801: t15.2024.03.08 val PER: 0.2774
2026-01-03 12:51:01,801: t15.2024.03.15 val PER: 0.2483
2026-01-03 12:51:01,801: t15.2024.03.17 val PER: 0.1987
2026-01-03 12:51:01,801: t15.2024.05.10 val PER: 0.1961
2026-01-03 12:51:01,801: t15.2024.06.14 val PER: 0.2114
2026-01-03 12:51:01,801: t15.2024.07.19 val PER: 0.3065
2026-01-03 12:51:01,802: t15.2024.07.21 val PER: 0.1297
2026-01-03 12:51:01,802: t15.2024.07.28 val PER: 0.1772
2026-01-03 12:51:01,802: t15.2025.01.10 val PER: 0.3691
2026-01-03 12:51:01,802: t15.2025.01.12 val PER: 0.2156
2026-01-03 12:51:01,802: t15.2025.03.14 val PER: 0.3595
2026-01-03 12:51:01,802: t15.2025.03.16 val PER: 0.2356
2026-01-03 12:51:01,802: t15.2025.03.30 val PER: 0.3609
2026-01-03 12:51:01,802: t15.2025.04.13 val PER: 0.2553
2026-01-03 12:51:02,044: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_7000
2026-01-03 12:51:19,327: Train batch 7200: loss: 14.17 grad norm: 57.38 time: 0.078
2026-01-03 12:51:36,946: Train batch 7400: loss: 13.92 grad norm: 54.02 time: 0.075
2026-01-03 12:51:45,513: Running test after training batch: 7500
2026-01-03 12:51:45,693: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:51:50,426: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point will
2026-01-03 12:51:50,457: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost tet
2026-01-03 12:51:52,109: Val batch 7500: PER (avg): 0.1918 CTC Loss (avg): 18.7138 WER(1gram): 54.82% (n=64) time: 6.596
2026-01-03 12:51:52,109: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 12:51:52,110: t15.2023.08.13 val PER: 0.1466
2026-01-03 12:51:52,110: t15.2023.08.18 val PER: 0.1442
2026-01-03 12:51:52,110: t15.2023.08.20 val PER: 0.1461
2026-01-03 12:51:52,110: t15.2023.08.25 val PER: 0.1114
2026-01-03 12:51:52,110: t15.2023.08.27 val PER: 0.2042
2026-01-03 12:51:52,110: t15.2023.09.01 val PER: 0.1193
2026-01-03 12:51:52,110: t15.2023.09.03 val PER: 0.1876
2026-01-03 12:51:52,110: t15.2023.09.24 val PER: 0.1541
2026-01-03 12:51:52,110: t15.2023.09.29 val PER: 0.1576
2026-01-03 12:51:52,110: t15.2023.10.01 val PER: 0.2081
2026-01-03 12:51:52,111: t15.2023.10.06 val PER: 0.1141
2026-01-03 12:51:52,111: t15.2023.10.08 val PER: 0.2828
2026-01-03 12:51:52,111: t15.2023.10.13 val PER: 0.2498
2026-01-03 12:51:52,111: t15.2023.10.15 val PER: 0.1872
2026-01-03 12:51:52,111: t15.2023.10.20 val PER: 0.1879
2026-01-03 12:51:52,111: t15.2023.10.22 val PER: 0.1537
2026-01-03 12:51:52,111: t15.2023.11.03 val PER: 0.2069
2026-01-03 12:51:52,111: t15.2023.11.04 val PER: 0.0410
2026-01-03 12:51:52,111: t15.2023.11.17 val PER: 0.0669
2026-01-03 12:51:52,111: t15.2023.11.19 val PER: 0.0519
2026-01-03 12:51:52,111: t15.2023.11.26 val PER: 0.1899
2026-01-03 12:51:52,111: t15.2023.12.03 val PER: 0.1544
2026-01-03 12:51:52,111: t15.2023.12.08 val PER: 0.1618
2026-01-03 12:51:52,111: t15.2023.12.10 val PER: 0.1248
2026-01-03 12:51:52,111: t15.2023.12.17 val PER: 0.1778
2026-01-03 12:51:52,111: t15.2023.12.29 val PER: 0.1805
2026-01-03 12:51:52,111: t15.2024.02.25 val PER: 0.1573
2026-01-03 12:51:52,112: t15.2024.03.08 val PER: 0.2859
2026-01-03 12:51:52,112: t15.2024.03.15 val PER: 0.2452
2026-01-03 12:51:52,112: t15.2024.03.17 val PER: 0.1918
2026-01-03 12:51:52,112: t15.2024.05.10 val PER: 0.2095
2026-01-03 12:51:52,112: t15.2024.06.14 val PER: 0.2019
2026-01-03 12:51:52,112: t15.2024.07.19 val PER: 0.2920
2026-01-03 12:51:52,112: t15.2024.07.21 val PER: 0.1421
2026-01-03 12:51:52,112: t15.2024.07.28 val PER: 0.1713
2026-01-03 12:51:52,112: t15.2025.01.10 val PER: 0.3581
2026-01-03 12:51:52,112: t15.2025.01.12 val PER: 0.2009
2026-01-03 12:51:52,112: t15.2025.03.14 val PER: 0.3654
2026-01-03 12:51:52,112: t15.2025.03.16 val PER: 0.2382
2026-01-03 12:51:52,112: t15.2025.03.30 val PER: 0.3552
2026-01-03 12:51:52,112: t15.2025.04.13 val PER: 0.2511
2026-01-03 12:51:52,355: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_7500
2026-01-03 12:52:00,727: Train batch 7600: loss: 15.99 grad norm: 58.82 time: 0.068
2026-01-03 12:52:17,646: Train batch 7800: loss: 14.37 grad norm: 59.16 time: 0.055
2026-01-03 12:52:35,073: Train batch 8000: loss: 11.57 grad norm: 49.80 time: 0.071
2026-01-03 12:52:35,073: Running test after training batch: 8000
2026-01-03 12:52:35,167: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:52:40,029: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:52:40,060: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nit
2026-01-03 12:52:41,735: Val batch 8000: PER (avg): 0.1868 CTC Loss (avg): 18.2893 WER(1gram): 52.54% (n=64) time: 6.661
2026-01-03 12:52:41,735: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 12:52:41,735: t15.2023.08.13 val PER: 0.1518
2026-01-03 12:52:41,735: t15.2023.08.18 val PER: 0.1358
2026-01-03 12:52:41,735: t15.2023.08.20 val PER: 0.1469
2026-01-03 12:52:41,736: t15.2023.08.25 val PER: 0.1084
2026-01-03 12:52:41,736: t15.2023.08.27 val PER: 0.2170
2026-01-03 12:52:41,736: t15.2023.09.01 val PER: 0.1136
2026-01-03 12:52:41,736: t15.2023.09.03 val PER: 0.1888
2026-01-03 12:52:41,736: t15.2023.09.24 val PER: 0.1553
2026-01-03 12:52:41,736: t15.2023.09.29 val PER: 0.1506
2026-01-03 12:52:41,736: t15.2023.10.01 val PER: 0.2061
2026-01-03 12:52:41,736: t15.2023.10.06 val PER: 0.1141
2026-01-03 12:52:41,736: t15.2023.10.08 val PER: 0.2774
2026-01-03 12:52:41,736: t15.2023.10.13 val PER: 0.2389
2026-01-03 12:52:41,736: t15.2023.10.15 val PER: 0.1872
2026-01-03 12:52:41,736: t15.2023.10.20 val PER: 0.1913
2026-01-03 12:52:41,737: t15.2023.10.22 val PER: 0.1370
2026-01-03 12:52:41,737: t15.2023.11.03 val PER: 0.1967
2026-01-03 12:52:41,737: t15.2023.11.04 val PER: 0.0410
2026-01-03 12:52:41,737: t15.2023.11.17 val PER: 0.0684
2026-01-03 12:52:41,737: t15.2023.11.19 val PER: 0.0619
2026-01-03 12:52:41,737: t15.2023.11.26 val PER: 0.1819
2026-01-03 12:52:41,737: t15.2023.12.03 val PER: 0.1628
2026-01-03 12:52:41,737: t15.2023.12.08 val PER: 0.1511
2026-01-03 12:52:41,737: t15.2023.12.10 val PER: 0.1380
2026-01-03 12:52:41,738: t15.2023.12.17 val PER: 0.1767
2026-01-03 12:52:41,738: t15.2023.12.29 val PER: 0.1778
2026-01-03 12:52:41,738: t15.2024.02.25 val PER: 0.1419
2026-01-03 12:52:41,738: t15.2024.03.08 val PER: 0.2589
2026-01-03 12:52:41,738: t15.2024.03.15 val PER: 0.2508
2026-01-03 12:52:41,738: t15.2024.03.17 val PER: 0.1750
2026-01-03 12:52:41,738: t15.2024.05.10 val PER: 0.1991
2026-01-03 12:52:41,738: t15.2024.06.14 val PER: 0.1972
2026-01-03 12:52:41,738: t15.2024.07.19 val PER: 0.2835
2026-01-03 12:52:41,738: t15.2024.07.21 val PER: 0.1262
2026-01-03 12:52:41,739: t15.2024.07.28 val PER: 0.1662
2026-01-03 12:52:41,739: t15.2025.01.10 val PER: 0.3444
2026-01-03 12:52:41,739: t15.2025.01.12 val PER: 0.1971
2026-01-03 12:52:41,739: t15.2025.03.14 val PER: 0.3476
2026-01-03 12:52:41,739: t15.2025.03.16 val PER: 0.2317
2026-01-03 12:52:41,739: t15.2025.03.30 val PER: 0.3437
2026-01-03 12:52:41,739: t15.2025.04.13 val PER: 0.2553
2026-01-03 12:52:41,740: New best val WER(1gram) 53.55% --> 52.54%
2026-01-03 12:52:41,740: Checkpointing model
2026-01-03 12:52:42,002: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 12:52:42,345: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_8000
2026-01-03 12:52:59,828: Train batch 8200: loss: 9.87 grad norm: 47.21 time: 0.054
2026-01-03 12:53:17,349: Train batch 8400: loss: 10.12 grad norm: 46.44 time: 0.063
2026-01-03 12:53:26,359: Running test after training batch: 8500
2026-01-03 12:53:26,465: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:53:31,164: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 12:53:31,195: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ned
2026-01-03 12:53:32,863: Val batch 8500: PER (avg): 0.1811 CTC Loss (avg): 17.8293 WER(1gram): 48.48% (n=64) time: 6.503
2026-01-03 12:53:32,863: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 12:53:32,863: t15.2023.08.13 val PER: 0.1362
2026-01-03 12:53:32,863: t15.2023.08.18 val PER: 0.1341
2026-01-03 12:53:32,863: t15.2023.08.20 val PER: 0.1422
2026-01-03 12:53:32,863: t15.2023.08.25 val PER: 0.1114
2026-01-03 12:53:32,863: t15.2023.08.27 val PER: 0.2026
2026-01-03 12:53:32,864: t15.2023.09.01 val PER: 0.1080
2026-01-03 12:53:32,864: t15.2023.09.03 val PER: 0.1912
2026-01-03 12:53:32,864: t15.2023.09.24 val PER: 0.1481
2026-01-03 12:53:32,864: t15.2023.09.29 val PER: 0.1570
2026-01-03 12:53:32,864: t15.2023.10.01 val PER: 0.2028
2026-01-03 12:53:32,864: t15.2023.10.06 val PER: 0.1076
2026-01-03 12:53:32,864: t15.2023.10.08 val PER: 0.2639
2026-01-03 12:53:32,864: t15.2023.10.13 val PER: 0.2397
2026-01-03 12:53:32,864: t15.2023.10.15 val PER: 0.1819
2026-01-03 12:53:32,864: t15.2023.10.20 val PER: 0.1946
2026-01-03 12:53:32,864: t15.2023.10.22 val PER: 0.1448
2026-01-03 12:53:32,864: t15.2023.11.03 val PER: 0.2008
2026-01-03 12:53:32,864: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:53:32,865: t15.2023.11.17 val PER: 0.0622
2026-01-03 12:53:32,865: t15.2023.11.19 val PER: 0.0439
2026-01-03 12:53:32,865: t15.2023.11.26 val PER: 0.1797
2026-01-03 12:53:32,865: t15.2023.12.03 val PER: 0.1492
2026-01-03 12:53:32,865: t15.2023.12.08 val PER: 0.1465
2026-01-03 12:53:32,865: t15.2023.12.10 val PER: 0.1196
2026-01-03 12:53:32,865: t15.2023.12.17 val PER: 0.1632
2026-01-03 12:53:32,865: t15.2023.12.29 val PER: 0.1668
2026-01-03 12:53:32,865: t15.2024.02.25 val PER: 0.1348
2026-01-03 12:53:32,865: t15.2024.03.08 val PER: 0.2546
2026-01-03 12:53:32,865: t15.2024.03.15 val PER: 0.2314
2026-01-03 12:53:32,865: t15.2024.03.17 val PER: 0.1688
2026-01-03 12:53:32,865: t15.2024.05.10 val PER: 0.1932
2026-01-03 12:53:32,865: t15.2024.06.14 val PER: 0.1956
2026-01-03 12:53:32,865: t15.2024.07.19 val PER: 0.2755
2026-01-03 12:53:32,865: t15.2024.07.21 val PER: 0.1262
2026-01-03 12:53:32,865: t15.2024.07.28 val PER: 0.1669
2026-01-03 12:53:32,866: t15.2025.01.10 val PER: 0.3264
2026-01-03 12:53:32,866: t15.2025.01.12 val PER: 0.1894
2026-01-03 12:53:32,866: t15.2025.03.14 val PER: 0.3447
2026-01-03 12:53:32,866: t15.2025.03.16 val PER: 0.2186
2026-01-03 12:53:32,866: t15.2025.03.30 val PER: 0.3345
2026-01-03 12:53:32,866: t15.2025.04.13 val PER: 0.2539
2026-01-03 12:53:32,867: New best val WER(1gram) 52.54% --> 48.48%
2026-01-03 12:53:32,867: Checkpointing model
2026-01-03 12:53:33,133: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 12:53:33,390: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_8500
2026-01-03 12:53:42,200: Train batch 8600: loss: 16.18 grad norm: 61.62 time: 0.054
2026-01-03 12:53:59,222: Train batch 8800: loss: 15.06 grad norm: 57.28 time: 0.059
2026-01-03 12:54:16,599: Train batch 9000: loss: 16.37 grad norm: 64.44 time: 0.071
2026-01-03 12:54:16,599: Running test after training batch: 9000
2026-01-03 12:54:16,714: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:54:21,571: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 12:54:21,603: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nit
2026-01-03 12:54:23,340: Val batch 9000: PER (avg): 0.1750 CTC Loss (avg): 17.3266 WER(1gram): 50.00% (n=64) time: 6.741
2026-01-03 12:54:23,340: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 12:54:23,341: t15.2023.08.13 val PER: 0.1320
2026-01-03 12:54:23,341: t15.2023.08.18 val PER: 0.1299
2026-01-03 12:54:23,341: t15.2023.08.20 val PER: 0.1334
2026-01-03 12:54:23,341: t15.2023.08.25 val PER: 0.0994
2026-01-03 12:54:23,341: t15.2023.08.27 val PER: 0.2154
2026-01-03 12:54:23,341: t15.2023.09.01 val PER: 0.1015
2026-01-03 12:54:23,341: t15.2023.09.03 val PER: 0.1781
2026-01-03 12:54:23,342: t15.2023.09.24 val PER: 0.1566
2026-01-03 12:54:23,342: t15.2023.09.29 val PER: 0.1404
2026-01-03 12:54:23,342: t15.2023.10.01 val PER: 0.1876
2026-01-03 12:54:23,342: t15.2023.10.06 val PER: 0.0980
2026-01-03 12:54:23,342: t15.2023.10.08 val PER: 0.2639
2026-01-03 12:54:23,342: t15.2023.10.13 val PER: 0.2296
2026-01-03 12:54:23,342: t15.2023.10.15 val PER: 0.1721
2026-01-03 12:54:23,342: t15.2023.10.20 val PER: 0.1745
2026-01-03 12:54:23,342: t15.2023.10.22 val PER: 0.1370
2026-01-03 12:54:23,343: t15.2023.11.03 val PER: 0.1988
2026-01-03 12:54:23,343: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:54:23,343: t15.2023.11.17 val PER: 0.0638
2026-01-03 12:54:23,343: t15.2023.11.19 val PER: 0.0499
2026-01-03 12:54:23,343: t15.2023.11.26 val PER: 0.1725
2026-01-03 12:54:23,343: t15.2023.12.03 val PER: 0.1387
2026-01-03 12:54:23,343: t15.2023.12.08 val PER: 0.1372
2026-01-03 12:54:23,343: t15.2023.12.10 val PER: 0.1130
2026-01-03 12:54:23,343: t15.2023.12.17 val PER: 0.1528
2026-01-03 12:54:23,344: t15.2023.12.29 val PER: 0.1613
2026-01-03 12:54:23,344: t15.2024.02.25 val PER: 0.1419
2026-01-03 12:54:23,344: t15.2024.03.08 val PER: 0.2646
2026-01-03 12:54:23,344: t15.2024.03.15 val PER: 0.2258
2026-01-03 12:54:23,344: t15.2024.03.17 val PER: 0.1729
2026-01-03 12:54:23,344: t15.2024.05.10 val PER: 0.1917
2026-01-03 12:54:23,344: t15.2024.06.14 val PER: 0.1909
2026-01-03 12:54:23,344: t15.2024.07.19 val PER: 0.2749
2026-01-03 12:54:23,344: t15.2024.07.21 val PER: 0.1186
2026-01-03 12:54:23,345: t15.2024.07.28 val PER: 0.1603
2026-01-03 12:54:23,345: t15.2025.01.10 val PER: 0.3168
2026-01-03 12:54:23,345: t15.2025.01.12 val PER: 0.1694
2026-01-03 12:54:23,345: t15.2025.03.14 val PER: 0.3462
2026-01-03 12:54:23,345: t15.2025.03.16 val PER: 0.2147
2026-01-03 12:54:23,345: t15.2025.03.30 val PER: 0.3310
2026-01-03 12:54:23,345: t15.2025.04.13 val PER: 0.2368
2026-01-03 12:54:23,610: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_9000
2026-01-03 12:54:40,843: Train batch 9200: loss: 11.02 grad norm: 51.05 time: 0.056
2026-01-03 12:54:57,934: Train batch 9400: loss: 7.77 grad norm: 46.25 time: 0.067
2026-01-03 12:55:06,444: Running test after training batch: 9500
2026-01-03 12:55:06,586: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:55:11,381: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point us will
2026-01-03 12:55:11,412: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nit
2026-01-03 12:55:13,142: Val batch 9500: PER (avg): 0.1746 CTC Loss (avg): 17.2246 WER(1gram): 50.51% (n=64) time: 6.697
2026-01-03 12:55:13,143: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 12:55:13,143: t15.2023.08.13 val PER: 0.1393
2026-01-03 12:55:13,143: t15.2023.08.18 val PER: 0.1190
2026-01-03 12:55:13,143: t15.2023.08.20 val PER: 0.1319
2026-01-03 12:55:13,143: t15.2023.08.25 val PER: 0.1084
2026-01-03 12:55:13,143: t15.2023.08.27 val PER: 0.1929
2026-01-03 12:55:13,143: t15.2023.09.01 val PER: 0.0950
2026-01-03 12:55:13,143: t15.2023.09.03 val PER: 0.1770
2026-01-03 12:55:13,143: t15.2023.09.24 val PER: 0.1505
2026-01-03 12:55:13,143: t15.2023.09.29 val PER: 0.1500
2026-01-03 12:55:13,143: t15.2023.10.01 val PER: 0.1955
2026-01-03 12:55:13,144: t15.2023.10.06 val PER: 0.1023
2026-01-03 12:55:13,144: t15.2023.10.08 val PER: 0.2679
2026-01-03 12:55:13,144: t15.2023.10.13 val PER: 0.2281
2026-01-03 12:55:13,144: t15.2023.10.15 val PER: 0.1846
2026-01-03 12:55:13,144: t15.2023.10.20 val PER: 0.1779
2026-01-03 12:55:13,144: t15.2023.10.22 val PER: 0.1236
2026-01-03 12:55:13,144: t15.2023.11.03 val PER: 0.1967
2026-01-03 12:55:13,144: t15.2023.11.04 val PER: 0.0273
2026-01-03 12:55:13,144: t15.2023.11.17 val PER: 0.0575
2026-01-03 12:55:13,144: t15.2023.11.19 val PER: 0.0519
2026-01-03 12:55:13,144: t15.2023.11.26 val PER: 0.1659
2026-01-03 12:55:13,145: t15.2023.12.03 val PER: 0.1418
2026-01-03 12:55:13,145: t15.2023.12.08 val PER: 0.1458
2026-01-03 12:55:13,145: t15.2023.12.10 val PER: 0.1183
2026-01-03 12:55:13,145: t15.2023.12.17 val PER: 0.1622
2026-01-03 12:55:13,145: t15.2023.12.29 val PER: 0.1633
2026-01-03 12:55:13,145: t15.2024.02.25 val PER: 0.1320
2026-01-03 12:55:13,145: t15.2024.03.08 val PER: 0.2617
2026-01-03 12:55:13,145: t15.2024.03.15 val PER: 0.2245
2026-01-03 12:55:13,145: t15.2024.03.17 val PER: 0.1632
2026-01-03 12:55:13,145: t15.2024.05.10 val PER: 0.1961
2026-01-03 12:55:13,145: t15.2024.06.14 val PER: 0.1830
2026-01-03 12:55:13,145: t15.2024.07.19 val PER: 0.2663
2026-01-03 12:55:13,145: t15.2024.07.21 val PER: 0.1193
2026-01-03 12:55:13,145: t15.2024.07.28 val PER: 0.1610
2026-01-03 12:55:13,145: t15.2025.01.10 val PER: 0.3044
2026-01-03 12:55:13,145: t15.2025.01.12 val PER: 0.1832
2026-01-03 12:55:13,146: t15.2025.03.14 val PER: 0.3580
2026-01-03 12:55:13,146: t15.2025.03.16 val PER: 0.2003
2026-01-03 12:55:13,146: t15.2025.03.30 val PER: 0.3161
2026-01-03 12:55:13,146: t15.2025.04.13 val PER: 0.2411
2026-01-03 12:55:13,406: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_9500
2026-01-03 12:55:22,116: Train batch 9600: loss: 8.44 grad norm: 45.20 time: 0.073
2026-01-03 12:55:39,354: Train batch 9800: loss: 12.43 grad norm: 57.28 time: 0.064
2026-01-03 12:55:56,696: Train batch 10000: loss: 5.49 grad norm: 34.76 time: 0.060
2026-01-03 12:55:56,696: Running test after training batch: 10000
2026-01-03 12:55:56,824: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:56:01,540: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sikh the could at this point as will
2026-01-03 12:56:01,570: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sindt
2026-01-03 12:56:03,276: Val batch 10000: PER (avg): 0.1716 CTC Loss (avg): 16.8540 WER(1gram): 50.25% (n=64) time: 6.580
2026-01-03 12:56:03,276: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 12:56:03,277: t15.2023.08.13 val PER: 0.1320
2026-01-03 12:56:03,277: t15.2023.08.18 val PER: 0.1282
2026-01-03 12:56:03,277: t15.2023.08.20 val PER: 0.1287
2026-01-03 12:56:03,277: t15.2023.08.25 val PER: 0.1039
2026-01-03 12:56:03,277: t15.2023.08.27 val PER: 0.1929
2026-01-03 12:56:03,277: t15.2023.09.01 val PER: 0.0942
2026-01-03 12:56:03,277: t15.2023.09.03 val PER: 0.1805
2026-01-03 12:56:03,277: t15.2023.09.24 val PER: 0.1432
2026-01-03 12:56:03,277: t15.2023.09.29 val PER: 0.1474
2026-01-03 12:56:03,277: t15.2023.10.01 val PER: 0.1915
2026-01-03 12:56:03,277: t15.2023.10.06 val PER: 0.1044
2026-01-03 12:56:03,277: t15.2023.10.08 val PER: 0.2733
2026-01-03 12:56:03,278: t15.2023.10.13 val PER: 0.2289
2026-01-03 12:56:03,278: t15.2023.10.15 val PER: 0.1786
2026-01-03 12:56:03,278: t15.2023.10.20 val PER: 0.1913
2026-01-03 12:56:03,281: t15.2023.10.22 val PER: 0.1347
2026-01-03 12:56:03,281: t15.2023.11.03 val PER: 0.1920
2026-01-03 12:56:03,282: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:56:03,282: t15.2023.11.17 val PER: 0.0544
2026-01-03 12:56:03,282: t15.2023.11.19 val PER: 0.0439
2026-01-03 12:56:03,282: t15.2023.11.26 val PER: 0.1558
2026-01-03 12:56:03,282: t15.2023.12.03 val PER: 0.1303
2026-01-03 12:56:03,282: t15.2023.12.08 val PER: 0.1325
2026-01-03 12:56:03,282: t15.2023.12.10 val PER: 0.1117
2026-01-03 12:56:03,282: t15.2023.12.17 val PER: 0.1622
2026-01-03 12:56:03,282: t15.2023.12.29 val PER: 0.1510
2026-01-03 12:56:03,282: t15.2024.02.25 val PER: 0.1362
2026-01-03 12:56:03,282: t15.2024.03.08 val PER: 0.2560
2026-01-03 12:56:03,282: t15.2024.03.15 val PER: 0.2139
2026-01-03 12:56:03,282: t15.2024.03.17 val PER: 0.1639
2026-01-03 12:56:03,282: t15.2024.05.10 val PER: 0.1798
2026-01-03 12:56:03,283: t15.2024.06.14 val PER: 0.1893
2026-01-03 12:56:03,283: t15.2024.07.19 val PER: 0.2729
2026-01-03 12:56:03,283: t15.2024.07.21 val PER: 0.1152
2026-01-03 12:56:03,283: t15.2024.07.28 val PER: 0.1493
2026-01-03 12:56:03,283: t15.2025.01.10 val PER: 0.3044
2026-01-03 12:56:03,283: t15.2025.01.12 val PER: 0.1747
2026-01-03 12:56:03,283: t15.2025.03.14 val PER: 0.3521
2026-01-03 12:56:03,283: t15.2025.03.16 val PER: 0.2238
2026-01-03 12:56:03,283: t15.2025.03.30 val PER: 0.3149
2026-01-03 12:56:03,283: t15.2025.04.13 val PER: 0.2325
2026-01-03 12:56:03,541: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_10000
2026-01-03 12:56:20,666: Train batch 10200: loss: 6.54 grad norm: 36.36 time: 0.049
2026-01-03 12:56:38,079: Train batch 10400: loss: 9.30 grad norm: 50.84 time: 0.071
2026-01-03 12:56:46,757: Running test after training batch: 10500
2026-01-03 12:56:46,885: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:56:51,640: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 12:56:51,671: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 12:56:53,388: Val batch 10500: PER (avg): 0.1649 CTC Loss (avg): 16.5366 WER(1gram): 48.98% (n=64) time: 6.631
2026-01-03 12:56:53,389: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 12:56:53,389: t15.2023.08.13 val PER: 0.1289
2026-01-03 12:56:53,389: t15.2023.08.18 val PER: 0.1215
2026-01-03 12:56:53,389: t15.2023.08.20 val PER: 0.1271
2026-01-03 12:56:53,389: t15.2023.08.25 val PER: 0.0904
2026-01-03 12:56:53,389: t15.2023.08.27 val PER: 0.2074
2026-01-03 12:56:53,389: t15.2023.09.01 val PER: 0.0917
2026-01-03 12:56:53,389: t15.2023.09.03 val PER: 0.1793
2026-01-03 12:56:53,390: t15.2023.09.24 val PER: 0.1432
2026-01-03 12:56:53,390: t15.2023.09.29 val PER: 0.1366
2026-01-03 12:56:53,390: t15.2023.10.01 val PER: 0.1816
2026-01-03 12:56:53,390: t15.2023.10.06 val PER: 0.0872
2026-01-03 12:56:53,390: t15.2023.10.08 val PER: 0.2571
2026-01-03 12:56:53,390: t15.2023.10.13 val PER: 0.2250
2026-01-03 12:56:53,390: t15.2023.10.15 val PER: 0.1661
2026-01-03 12:56:53,390: t15.2023.10.20 val PER: 0.1745
2026-01-03 12:56:53,390: t15.2023.10.22 val PER: 0.1258
2026-01-03 12:56:53,390: t15.2023.11.03 val PER: 0.1859
2026-01-03 12:56:53,390: t15.2023.11.04 val PER: 0.0375
2026-01-03 12:56:53,390: t15.2023.11.17 val PER: 0.0513
2026-01-03 12:56:53,390: t15.2023.11.19 val PER: 0.0419
2026-01-03 12:56:53,390: t15.2023.11.26 val PER: 0.1457
2026-01-03 12:56:53,391: t15.2023.12.03 val PER: 0.1282
2026-01-03 12:56:53,391: t15.2023.12.08 val PER: 0.1252
2026-01-03 12:56:53,391: t15.2023.12.10 val PER: 0.1038
2026-01-03 12:56:53,391: t15.2023.12.17 val PER: 0.1466
2026-01-03 12:56:53,391: t15.2023.12.29 val PER: 0.1517
2026-01-03 12:56:53,391: t15.2024.02.25 val PER: 0.1236
2026-01-03 12:56:53,391: t15.2024.03.08 val PER: 0.2447
2026-01-03 12:56:53,391: t15.2024.03.15 val PER: 0.2183
2026-01-03 12:56:53,391: t15.2024.03.17 val PER: 0.1562
2026-01-03 12:56:53,391: t15.2024.05.10 val PER: 0.1813
2026-01-03 12:56:53,391: t15.2024.06.14 val PER: 0.1861
2026-01-03 12:56:53,391: t15.2024.07.19 val PER: 0.2538
2026-01-03 12:56:53,391: t15.2024.07.21 val PER: 0.1097
2026-01-03 12:56:53,391: t15.2024.07.28 val PER: 0.1493
2026-01-03 12:56:53,391: t15.2025.01.10 val PER: 0.3003
2026-01-03 12:56:53,391: t15.2025.01.12 val PER: 0.1655
2026-01-03 12:56:53,391: t15.2025.03.14 val PER: 0.3491
2026-01-03 12:56:53,392: t15.2025.03.16 val PER: 0.1872
2026-01-03 12:56:53,392: t15.2025.03.30 val PER: 0.3126
2026-01-03 12:56:53,392: t15.2025.04.13 val PER: 0.2325
2026-01-03 12:56:53,652: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_10500
2026-01-03 12:57:02,282: Train batch 10600: loss: 8.88 grad norm: 53.43 time: 0.071
2026-01-03 12:57:19,197: Train batch 10800: loss: 15.24 grad norm: 63.68 time: 0.064
2026-01-03 12:57:36,573: Train batch 11000: loss: 15.30 grad norm: 65.30 time: 0.057
2026-01-03 12:57:36,574: Running test after training batch: 11000
2026-01-03 12:57:36,671: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:57:41,405: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 12:57:41,436: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 12:57:43,165: Val batch 11000: PER (avg): 0.1643 CTC Loss (avg): 16.3921 WER(1gram): 47.97% (n=64) time: 6.591
2026-01-03 12:57:43,165: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 12:57:43,165: t15.2023.08.13 val PER: 0.1237
2026-01-03 12:57:43,165: t15.2023.08.18 val PER: 0.1190
2026-01-03 12:57:43,166: t15.2023.08.20 val PER: 0.1271
2026-01-03 12:57:43,166: t15.2023.08.25 val PER: 0.0904
2026-01-03 12:57:43,166: t15.2023.08.27 val PER: 0.1994
2026-01-03 12:57:43,166: t15.2023.09.01 val PER: 0.0942
2026-01-03 12:57:43,167: t15.2023.09.03 val PER: 0.1829
2026-01-03 12:57:43,167: t15.2023.09.24 val PER: 0.1396
2026-01-03 12:57:43,167: t15.2023.09.29 val PER: 0.1378
2026-01-03 12:57:43,167: t15.2023.10.01 val PER: 0.1836
2026-01-03 12:57:43,167: t15.2023.10.06 val PER: 0.0861
2026-01-03 12:57:43,167: t15.2023.10.08 val PER: 0.2585
2026-01-03 12:57:43,167: t15.2023.10.13 val PER: 0.2296
2026-01-03 12:57:43,168: t15.2023.10.15 val PER: 0.1661
2026-01-03 12:57:43,168: t15.2023.10.20 val PER: 0.1711
2026-01-03 12:57:43,168: t15.2023.10.22 val PER: 0.1192
2026-01-03 12:57:43,168: t15.2023.11.03 val PER: 0.1927
2026-01-03 12:57:43,168: t15.2023.11.04 val PER: 0.0307
2026-01-03 12:57:43,168: t15.2023.11.17 val PER: 0.0529
2026-01-03 12:57:43,168: t15.2023.11.19 val PER: 0.0479
2026-01-03 12:57:43,168: t15.2023.11.26 val PER: 0.1514
2026-01-03 12:57:43,168: t15.2023.12.03 val PER: 0.1334
2026-01-03 12:57:43,169: t15.2023.12.08 val PER: 0.1225
2026-01-03 12:57:43,169: t15.2023.12.10 val PER: 0.1078
2026-01-03 12:57:43,169: t15.2023.12.17 val PER: 0.1528
2026-01-03 12:57:43,169: t15.2023.12.29 val PER: 0.1469
2026-01-03 12:57:43,169: t15.2024.02.25 val PER: 0.1334
2026-01-03 12:57:43,169: t15.2024.03.08 val PER: 0.2432
2026-01-03 12:57:43,169: t15.2024.03.15 val PER: 0.2170
2026-01-03 12:57:43,169: t15.2024.03.17 val PER: 0.1499
2026-01-03 12:57:43,169: t15.2024.05.10 val PER: 0.1857
2026-01-03 12:57:43,169: t15.2024.06.14 val PER: 0.1830
2026-01-03 12:57:43,169: t15.2024.07.19 val PER: 0.2472
2026-01-03 12:57:43,170: t15.2024.07.21 val PER: 0.1055
2026-01-03 12:57:43,170: t15.2024.07.28 val PER: 0.1471
2026-01-03 12:57:43,170: t15.2025.01.10 val PER: 0.2920
2026-01-03 12:57:43,170: t15.2025.01.12 val PER: 0.1609
2026-01-03 12:57:43,170: t15.2025.03.14 val PER: 0.3491
2026-01-03 12:57:43,170: t15.2025.03.16 val PER: 0.1898
2026-01-03 12:57:43,170: t15.2025.03.30 val PER: 0.3126
2026-01-03 12:57:43,170: t15.2025.04.13 val PER: 0.2225
2026-01-03 12:57:43,170: New best val WER(1gram) 48.48% --> 47.97%
2026-01-03 12:57:43,171: Checkpointing model
2026-01-03 12:57:43,433: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 12:57:43,703: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_11000
2026-01-03 12:58:01,047: Train batch 11200: loss: 11.05 grad norm: 50.67 time: 0.070
2026-01-03 12:58:18,267: Train batch 11400: loss: 10.08 grad norm: 52.20 time: 0.056
2026-01-03 12:58:27,072: Running test after training batch: 11500
2026-01-03 12:58:27,178: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:58:31,966: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 12:58:31,999: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 12:58:33,748: Val batch 11500: PER (avg): 0.1642 CTC Loss (avg): 16.3734 WER(1gram): 47.97% (n=64) time: 6.676
2026-01-03 12:58:33,749: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 12:58:33,749: t15.2023.08.13 val PER: 0.1258
2026-01-03 12:58:33,749: t15.2023.08.18 val PER: 0.1190
2026-01-03 12:58:33,749: t15.2023.08.20 val PER: 0.1263
2026-01-03 12:58:33,749: t15.2023.08.25 val PER: 0.0904
2026-01-03 12:58:33,749: t15.2023.08.27 val PER: 0.1929
2026-01-03 12:58:33,749: t15.2023.09.01 val PER: 0.0933
2026-01-03 12:58:33,749: t15.2023.09.03 val PER: 0.1734
2026-01-03 12:58:33,750: t15.2023.09.24 val PER: 0.1335
2026-01-03 12:58:33,750: t15.2023.09.29 val PER: 0.1378
2026-01-03 12:58:33,750: t15.2023.10.01 val PER: 0.1863
2026-01-03 12:58:33,750: t15.2023.10.06 val PER: 0.0904
2026-01-03 12:58:33,750: t15.2023.10.08 val PER: 0.2585
2026-01-03 12:58:33,750: t15.2023.10.13 val PER: 0.2219
2026-01-03 12:58:33,750: t15.2023.10.15 val PER: 0.1681
2026-01-03 12:58:33,750: t15.2023.10.20 val PER: 0.1745
2026-01-03 12:58:33,750: t15.2023.10.22 val PER: 0.1180
2026-01-03 12:58:33,750: t15.2023.11.03 val PER: 0.1845
2026-01-03 12:58:33,750: t15.2023.11.04 val PER: 0.0273
2026-01-03 12:58:33,750: t15.2023.11.17 val PER: 0.0544
2026-01-03 12:58:33,751: t15.2023.11.19 val PER: 0.0459
2026-01-03 12:58:33,751: t15.2023.11.26 val PER: 0.1464
2026-01-03 12:58:33,751: t15.2023.12.03 val PER: 0.1271
2026-01-03 12:58:33,751: t15.2023.12.08 val PER: 0.1225
2026-01-03 12:58:33,751: t15.2023.12.10 val PER: 0.1038
2026-01-03 12:58:33,751: t15.2023.12.17 val PER: 0.1435
2026-01-03 12:58:33,751: t15.2023.12.29 val PER: 0.1489
2026-01-03 12:58:33,751: t15.2024.02.25 val PER: 0.1264
2026-01-03 12:58:33,751: t15.2024.03.08 val PER: 0.2432
2026-01-03 12:58:33,751: t15.2024.03.15 val PER: 0.2201
2026-01-03 12:58:33,751: t15.2024.03.17 val PER: 0.1569
2026-01-03 12:58:33,751: t15.2024.05.10 val PER: 0.1872
2026-01-03 12:58:33,751: t15.2024.06.14 val PER: 0.1830
2026-01-03 12:58:33,751: t15.2024.07.19 val PER: 0.2525
2026-01-03 12:58:33,751: t15.2024.07.21 val PER: 0.1083
2026-01-03 12:58:33,751: t15.2024.07.28 val PER: 0.1463
2026-01-03 12:58:33,751: t15.2025.01.10 val PER: 0.2948
2026-01-03 12:58:33,752: t15.2025.01.12 val PER: 0.1663
2026-01-03 12:58:33,752: t15.2025.03.14 val PER: 0.3521
2026-01-03 12:58:33,752: t15.2025.03.16 val PER: 0.1963
2026-01-03 12:58:33,752: t15.2025.03.30 val PER: 0.3172
2026-01-03 12:58:33,752: t15.2025.04.13 val PER: 0.2254
2026-01-03 12:58:34,017: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_11500
2026-01-03 12:58:42,397: Train batch 11600: loss: 11.99 grad norm: 48.58 time: 0.060
2026-01-03 12:58:59,476: Train batch 11800: loss: 7.08 grad norm: 40.53 time: 0.044
2026-01-03 12:59:16,547: Train batch 12000: loss: 14.59 grad norm: 50.44 time: 0.071
2026-01-03 12:59:16,547: Running test after training batch: 12000
2026-01-03 12:59:16,673: WER debug GT example: You can see the code at this point as well.
2026-01-03 12:59:21,413: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 12:59:21,446: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 12:59:23,214: Val batch 12000: PER (avg): 0.1630 CTC Loss (avg): 16.2578 WER(1gram): 48.98% (n=64) time: 6.666
2026-01-03 12:59:23,214: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 12:59:23,215: t15.2023.08.13 val PER: 0.1227
2026-01-03 12:59:23,215: t15.2023.08.18 val PER: 0.1115
2026-01-03 12:59:23,215: t15.2023.08.20 val PER: 0.1231
2026-01-03 12:59:23,215: t15.2023.08.25 val PER: 0.0904
2026-01-03 12:59:23,215: t15.2023.08.27 val PER: 0.1865
2026-01-03 12:59:23,215: t15.2023.09.01 val PER: 0.0885
2026-01-03 12:59:23,215: t15.2023.09.03 val PER: 0.1734
2026-01-03 12:59:23,215: t15.2023.09.24 val PER: 0.1371
2026-01-03 12:59:23,215: t15.2023.09.29 val PER: 0.1404
2026-01-03 12:59:23,215: t15.2023.10.01 val PER: 0.1797
2026-01-03 12:59:23,215: t15.2023.10.06 val PER: 0.0840
2026-01-03 12:59:23,216: t15.2023.10.08 val PER: 0.2612
2026-01-03 12:59:23,216: t15.2023.10.13 val PER: 0.2250
2026-01-03 12:59:23,216: t15.2023.10.15 val PER: 0.1707
2026-01-03 12:59:23,216: t15.2023.10.20 val PER: 0.1779
2026-01-03 12:59:23,216: t15.2023.10.22 val PER: 0.1214
2026-01-03 12:59:23,216: t15.2023.11.03 val PER: 0.1906
2026-01-03 12:59:23,216: t15.2023.11.04 val PER: 0.0341
2026-01-03 12:59:23,216: t15.2023.11.17 val PER: 0.0482
2026-01-03 12:59:23,216: t15.2023.11.19 val PER: 0.0439
2026-01-03 12:59:23,216: t15.2023.11.26 val PER: 0.1420
2026-01-03 12:59:23,216: t15.2023.12.03 val PER: 0.1303
2026-01-03 12:59:23,216: t15.2023.12.08 val PER: 0.1185
2026-01-03 12:59:23,216: t15.2023.12.10 val PER: 0.1051
2026-01-03 12:59:23,216: t15.2023.12.17 val PER: 0.1466
2026-01-03 12:59:23,217: t15.2023.12.29 val PER: 0.1469
2026-01-03 12:59:23,217: t15.2024.02.25 val PER: 0.1250
2026-01-03 12:59:23,217: t15.2024.03.08 val PER: 0.2489
2026-01-03 12:59:23,217: t15.2024.03.15 val PER: 0.2120
2026-01-03 12:59:23,217: t15.2024.03.17 val PER: 0.1604
2026-01-03 12:59:23,217: t15.2024.05.10 val PER: 0.1842
2026-01-03 12:59:23,217: t15.2024.06.14 val PER: 0.1751
2026-01-03 12:59:23,217: t15.2024.07.19 val PER: 0.2505
2026-01-03 12:59:23,217: t15.2024.07.21 val PER: 0.1110
2026-01-03 12:59:23,217: t15.2024.07.28 val PER: 0.1456
2026-01-03 12:59:23,217: t15.2025.01.10 val PER: 0.2961
2026-01-03 12:59:23,217: t15.2025.01.12 val PER: 0.1609
2026-01-03 12:59:23,217: t15.2025.03.14 val PER: 0.3536
2026-01-03 12:59:23,217: t15.2025.03.16 val PER: 0.1911
2026-01-03 12:59:23,217: t15.2025.03.30 val PER: 0.3080
2026-01-03 12:59:23,217: t15.2025.04.13 val PER: 0.2311
2026-01-03 12:59:23,476: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_12000
2026-01-03 12:59:40,652: Train batch 12200: loss: 6.13 grad norm: 39.96 time: 0.064
2026-01-03 12:59:57,698: Train batch 12400: loss: 5.10 grad norm: 34.35 time: 0.041
2026-01-03 13:00:06,390: Running test after training batch: 12500
2026-01-03 13:00:06,534: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:00:11,384: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:00:11,416: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:00:13,162: Val batch 12500: PER (avg): 0.1622 CTC Loss (avg): 16.1769 WER(1gram): 48.22% (n=64) time: 6.771
2026-01-03 13:00:13,162: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 13:00:13,162: t15.2023.08.13 val PER: 0.1227
2026-01-03 13:00:13,162: t15.2023.08.18 val PER: 0.1123
2026-01-03 13:00:13,163: t15.2023.08.20 val PER: 0.1255
2026-01-03 13:00:13,163: t15.2023.08.25 val PER: 0.0813
2026-01-03 13:00:13,163: t15.2023.08.27 val PER: 0.1945
2026-01-03 13:00:13,163: t15.2023.09.01 val PER: 0.0893
2026-01-03 13:00:13,163: t15.2023.09.03 val PER: 0.1698
2026-01-03 13:00:13,163: t15.2023.09.24 val PER: 0.1323
2026-01-03 13:00:13,163: t15.2023.09.29 val PER: 0.1347
2026-01-03 13:00:13,163: t15.2023.10.01 val PER: 0.1823
2026-01-03 13:00:13,163: t15.2023.10.06 val PER: 0.0861
2026-01-03 13:00:13,163: t15.2023.10.08 val PER: 0.2625
2026-01-03 13:00:13,163: t15.2023.10.13 val PER: 0.2227
2026-01-03 13:00:13,163: t15.2023.10.15 val PER: 0.1694
2026-01-03 13:00:13,163: t15.2023.10.20 val PER: 0.1812
2026-01-03 13:00:13,163: t15.2023.10.22 val PER: 0.1158
2026-01-03 13:00:13,164: t15.2023.11.03 val PER: 0.1927
2026-01-03 13:00:13,164: t15.2023.11.04 val PER: 0.0307
2026-01-03 13:00:13,164: t15.2023.11.17 val PER: 0.0498
2026-01-03 13:00:13,164: t15.2023.11.19 val PER: 0.0399
2026-01-03 13:00:13,164: t15.2023.11.26 val PER: 0.1442
2026-01-03 13:00:13,164: t15.2023.12.03 val PER: 0.1282
2026-01-03 13:00:13,164: t15.2023.12.08 val PER: 0.1172
2026-01-03 13:00:13,164: t15.2023.12.10 val PER: 0.1051
2026-01-03 13:00:13,164: t15.2023.12.17 val PER: 0.1435
2026-01-03 13:00:13,164: t15.2023.12.29 val PER: 0.1448
2026-01-03 13:00:13,164: t15.2024.02.25 val PER: 0.1320
2026-01-03 13:00:13,164: t15.2024.03.08 val PER: 0.2418
2026-01-03 13:00:13,164: t15.2024.03.15 val PER: 0.2126
2026-01-03 13:00:13,164: t15.2024.03.17 val PER: 0.1520
2026-01-03 13:00:13,164: t15.2024.05.10 val PER: 0.1842
2026-01-03 13:00:13,164: t15.2024.06.14 val PER: 0.1830
2026-01-03 13:00:13,164: t15.2024.07.19 val PER: 0.2525
2026-01-03 13:00:13,165: t15.2024.07.21 val PER: 0.1055
2026-01-03 13:00:13,165: t15.2024.07.28 val PER: 0.1434
2026-01-03 13:00:13,165: t15.2025.01.10 val PER: 0.2961
2026-01-03 13:00:13,165: t15.2025.01.12 val PER: 0.1601
2026-01-03 13:00:13,165: t15.2025.03.14 val PER: 0.3536
2026-01-03 13:00:13,165: t15.2025.03.16 val PER: 0.1950
2026-01-03 13:00:13,165: t15.2025.03.30 val PER: 0.3138
2026-01-03 13:00:13,165: t15.2025.04.13 val PER: 0.2240
2026-01-03 13:00:13,425: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_12500
2026-01-03 13:00:21,847: Train batch 12600: loss: 8.59 grad norm: 43.69 time: 0.057
2026-01-03 13:00:39,112: Train batch 12800: loss: 6.36 grad norm: 37.43 time: 0.053
2026-01-03 13:00:56,876: Train batch 13000: loss: 7.38 grad norm: 44.00 time: 0.067
2026-01-03 13:00:56,877: Running test after training batch: 13000
2026-01-03 13:00:56,995: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:01:01,702: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:01:01,734: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:01:03,512: Val batch 13000: PER (avg): 0.1622 CTC Loss (avg): 16.1384 WER(1gram): 47.21% (n=64) time: 6.635
2026-01-03 13:01:03,512: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-03 13:01:03,512: t15.2023.08.13 val PER: 0.1237
2026-01-03 13:01:03,512: t15.2023.08.18 val PER: 0.1106
2026-01-03 13:01:03,512: t15.2023.08.20 val PER: 0.1207
2026-01-03 13:01:03,513: t15.2023.08.25 val PER: 0.0828
2026-01-03 13:01:03,513: t15.2023.08.27 val PER: 0.1881
2026-01-03 13:01:03,513: t15.2023.09.01 val PER: 0.0901
2026-01-03 13:01:03,513: t15.2023.09.03 val PER: 0.1675
2026-01-03 13:01:03,513: t15.2023.09.24 val PER: 0.1335
2026-01-03 13:01:03,513: t15.2023.09.29 val PER: 0.1378
2026-01-03 13:01:03,513: t15.2023.10.01 val PER: 0.1843
2026-01-03 13:01:03,513: t15.2023.10.06 val PER: 0.0926
2026-01-03 13:01:03,513: t15.2023.10.08 val PER: 0.2598
2026-01-03 13:01:03,513: t15.2023.10.13 val PER: 0.2250
2026-01-03 13:01:03,513: t15.2023.10.15 val PER: 0.1681
2026-01-03 13:01:03,513: t15.2023.10.20 val PER: 0.1812
2026-01-03 13:01:03,513: t15.2023.10.22 val PER: 0.1214
2026-01-03 13:01:03,513: t15.2023.11.03 val PER: 0.1920
2026-01-03 13:01:03,514: t15.2023.11.04 val PER: 0.0273
2026-01-03 13:01:03,514: t15.2023.11.17 val PER: 0.0513
2026-01-03 13:01:03,514: t15.2023.11.19 val PER: 0.0459
2026-01-03 13:01:03,514: t15.2023.11.26 val PER: 0.1457
2026-01-03 13:01:03,514: t15.2023.12.03 val PER: 0.1271
2026-01-03 13:01:03,514: t15.2023.12.08 val PER: 0.1198
2026-01-03 13:01:03,514: t15.2023.12.10 val PER: 0.1051
2026-01-03 13:01:03,514: t15.2023.12.17 val PER: 0.1435
2026-01-03 13:01:03,514: t15.2023.12.29 val PER: 0.1476
2026-01-03 13:01:03,514: t15.2024.02.25 val PER: 0.1180
2026-01-03 13:01:03,514: t15.2024.03.08 val PER: 0.2461
2026-01-03 13:01:03,514: t15.2024.03.15 val PER: 0.2126
2026-01-03 13:01:03,514: t15.2024.03.17 val PER: 0.1534
2026-01-03 13:01:03,514: t15.2024.05.10 val PER: 0.1828
2026-01-03 13:01:03,514: t15.2024.06.14 val PER: 0.1782
2026-01-03 13:01:03,514: t15.2024.07.19 val PER: 0.2479
2026-01-03 13:01:03,515: t15.2024.07.21 val PER: 0.1076
2026-01-03 13:01:03,515: t15.2024.07.28 val PER: 0.1485
2026-01-03 13:01:03,515: t15.2025.01.10 val PER: 0.2948
2026-01-03 13:01:03,515: t15.2025.01.12 val PER: 0.1624
2026-01-03 13:01:03,515: t15.2025.03.14 val PER: 0.3402
2026-01-03 13:01:03,515: t15.2025.03.16 val PER: 0.1872
2026-01-03 13:01:03,515: t15.2025.03.30 val PER: 0.3115
2026-01-03 13:01:03,515: t15.2025.04.13 val PER: 0.2254
2026-01-03 13:01:03,517: New best val WER(1gram) 47.97% --> 47.21%
2026-01-03 13:01:03,517: Checkpointing model
2026-01-03 13:01:03,781: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 13:01:04,058: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_13000
2026-01-03 13:01:21,078: Train batch 13200: loss: 14.57 grad norm: 60.70 time: 0.053
2026-01-03 13:01:38,061: Train batch 13400: loss: 10.46 grad norm: 53.86 time: 0.062
2026-01-03 13:01:46,738: Running test after training batch: 13500
2026-01-03 13:01:46,876: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:01:51,601: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:01:51,633: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:01:53,415: Val batch 13500: PER (avg): 0.1610 CTC Loss (avg): 16.1317 WER(1gram): 48.22% (n=64) time: 6.677
2026-01-03 13:01:53,415: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 13:01:53,415: t15.2023.08.13 val PER: 0.1195
2026-01-03 13:01:53,415: t15.2023.08.18 val PER: 0.1115
2026-01-03 13:01:53,416: t15.2023.08.20 val PER: 0.1207
2026-01-03 13:01:53,416: t15.2023.08.25 val PER: 0.0873
2026-01-03 13:01:53,416: t15.2023.08.27 val PER: 0.1977
2026-01-03 13:01:53,416: t15.2023.09.01 val PER: 0.0885
2026-01-03 13:01:53,416: t15.2023.09.03 val PER: 0.1722
2026-01-03 13:01:53,416: t15.2023.09.24 val PER: 0.1335
2026-01-03 13:01:53,416: t15.2023.09.29 val PER: 0.1378
2026-01-03 13:01:53,416: t15.2023.10.01 val PER: 0.1803
2026-01-03 13:01:53,416: t15.2023.10.06 val PER: 0.0893
2026-01-03 13:01:53,416: t15.2023.10.08 val PER: 0.2503
2026-01-03 13:01:53,416: t15.2023.10.13 val PER: 0.2234
2026-01-03 13:01:53,416: t15.2023.10.15 val PER: 0.1655
2026-01-03 13:01:53,416: t15.2023.10.20 val PER: 0.1779
2026-01-03 13:01:53,417: t15.2023.10.22 val PER: 0.1236
2026-01-03 13:01:53,417: t15.2023.11.03 val PER: 0.1900
2026-01-03 13:01:53,417: t15.2023.11.04 val PER: 0.0307
2026-01-03 13:01:53,417: t15.2023.11.17 val PER: 0.0498
2026-01-03 13:01:53,417: t15.2023.11.19 val PER: 0.0439
2026-01-03 13:01:53,417: t15.2023.11.26 val PER: 0.1471
2026-01-03 13:01:53,417: t15.2023.12.03 val PER: 0.1229
2026-01-03 13:01:53,417: t15.2023.12.08 val PER: 0.1172
2026-01-03 13:01:53,417: t15.2023.12.10 val PER: 0.0999
2026-01-03 13:01:53,417: t15.2023.12.17 val PER: 0.1466
2026-01-03 13:01:53,417: t15.2023.12.29 val PER: 0.1455
2026-01-03 13:01:53,417: t15.2024.02.25 val PER: 0.1236
2026-01-03 13:01:53,417: t15.2024.03.08 val PER: 0.2347
2026-01-03 13:01:53,417: t15.2024.03.15 val PER: 0.2120
2026-01-03 13:01:53,417: t15.2024.03.17 val PER: 0.1555
2026-01-03 13:01:53,417: t15.2024.05.10 val PER: 0.1813
2026-01-03 13:01:53,417: t15.2024.06.14 val PER: 0.1751
2026-01-03 13:01:53,418: t15.2024.07.19 val PER: 0.2465
2026-01-03 13:01:53,418: t15.2024.07.21 val PER: 0.1048
2026-01-03 13:01:53,418: t15.2024.07.28 val PER: 0.1478
2026-01-03 13:01:53,418: t15.2025.01.10 val PER: 0.2906
2026-01-03 13:01:53,418: t15.2025.01.12 val PER: 0.1586
2026-01-03 13:01:53,418: t15.2025.03.14 val PER: 0.3388
2026-01-03 13:01:53,418: t15.2025.03.16 val PER: 0.1924
2026-01-03 13:01:53,418: t15.2025.03.30 val PER: 0.3034
2026-01-03 13:01:53,418: t15.2025.04.13 val PER: 0.2254
2026-01-03 13:01:53,685: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_13500
2026-01-03 13:02:02,185: Train batch 13600: loss: 14.52 grad norm: 65.90 time: 0.062
2026-01-03 13:02:19,342: Train batch 13800: loss: 10.70 grad norm: 55.20 time: 0.056
2026-01-03 13:02:36,813: Train batch 14000: loss: 13.36 grad norm: 57.10 time: 0.051
2026-01-03 13:02:36,814: Running test after training batch: 14000
2026-01-03 13:02:36,919: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:02:41,604: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:02:41,637: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:02:43,419: Val batch 14000: PER (avg): 0.1614 CTC Loss (avg): 16.0863 WER(1gram): 47.97% (n=64) time: 6.605
2026-01-03 13:02:43,420: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 13:02:43,420: t15.2023.08.13 val PER: 0.1206
2026-01-03 13:02:43,420: t15.2023.08.18 val PER: 0.1081
2026-01-03 13:02:43,420: t15.2023.08.20 val PER: 0.1223
2026-01-03 13:02:43,420: t15.2023.08.25 val PER: 0.0934
2026-01-03 13:02:43,420: t15.2023.08.27 val PER: 0.1945
2026-01-03 13:02:43,421: t15.2023.09.01 val PER: 0.0925
2026-01-03 13:02:43,421: t15.2023.09.03 val PER: 0.1675
2026-01-03 13:02:43,421: t15.2023.09.24 val PER: 0.1347
2026-01-03 13:02:43,421: t15.2023.09.29 val PER: 0.1398
2026-01-03 13:02:43,421: t15.2023.10.01 val PER: 0.1810
2026-01-03 13:02:43,421: t15.2023.10.06 val PER: 0.0904
2026-01-03 13:02:43,421: t15.2023.10.08 val PER: 0.2639
2026-01-03 13:02:43,421: t15.2023.10.13 val PER: 0.2234
2026-01-03 13:02:43,421: t15.2023.10.15 val PER: 0.1668
2026-01-03 13:02:43,421: t15.2023.10.20 val PER: 0.1879
2026-01-03 13:02:43,422: t15.2023.10.22 val PER: 0.1203
2026-01-03 13:02:43,422: t15.2023.11.03 val PER: 0.1879
2026-01-03 13:02:43,422: t15.2023.11.04 val PER: 0.0307
2026-01-03 13:02:43,422: t15.2023.11.17 val PER: 0.0467
2026-01-03 13:02:43,422: t15.2023.11.19 val PER: 0.0499
2026-01-03 13:02:43,422: t15.2023.11.26 val PER: 0.1442
2026-01-03 13:02:43,422: t15.2023.12.03 val PER: 0.1292
2026-01-03 13:02:43,422: t15.2023.12.08 val PER: 0.1185
2026-01-03 13:02:43,422: t15.2023.12.10 val PER: 0.1012
2026-01-03 13:02:43,422: t15.2023.12.17 val PER: 0.1435
2026-01-03 13:02:43,422: t15.2023.12.29 val PER: 0.1434
2026-01-03 13:02:43,423: t15.2024.02.25 val PER: 0.1194
2026-01-03 13:02:43,423: t15.2024.03.08 val PER: 0.2376
2026-01-03 13:02:43,423: t15.2024.03.15 val PER: 0.2145
2026-01-03 13:02:43,423: t15.2024.03.17 val PER: 0.1492
2026-01-03 13:02:43,423: t15.2024.05.10 val PER: 0.1842
2026-01-03 13:02:43,423: t15.2024.06.14 val PER: 0.1751
2026-01-03 13:02:43,423: t15.2024.07.19 val PER: 0.2492
2026-01-03 13:02:43,423: t15.2024.07.21 val PER: 0.1062
2026-01-03 13:02:43,423: t15.2024.07.28 val PER: 0.1471
2026-01-03 13:02:43,423: t15.2025.01.10 val PER: 0.3017
2026-01-03 13:02:43,423: t15.2025.01.12 val PER: 0.1578
2026-01-03 13:02:43,424: t15.2025.03.14 val PER: 0.3402
2026-01-03 13:02:43,424: t15.2025.03.16 val PER: 0.1872
2026-01-03 13:02:43,424: t15.2025.03.30 val PER: 0.3011
2026-01-03 13:02:43,424: t15.2025.04.13 val PER: 0.2240
2026-01-03 13:02:43,685: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_14000
2026-01-03 13:03:00,998: Train batch 14200: loss: 9.42 grad norm: 48.67 time: 0.055
2026-01-03 13:03:18,544: Train batch 14400: loss: 6.79 grad norm: 39.82 time: 0.064
2026-01-03 13:03:27,409: Running test after training batch: 14500
2026-01-03 13:03:27,506: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:03:32,226: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:03:32,258: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:03:34,042: Val batch 14500: PER (avg): 0.1616 CTC Loss (avg): 16.1081 WER(1gram): 47.46% (n=64) time: 6.633
2026-01-03 13:03:34,043: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 13:03:34,043: t15.2023.08.13 val PER: 0.1206
2026-01-03 13:03:34,043: t15.2023.08.18 val PER: 0.1090
2026-01-03 13:03:34,043: t15.2023.08.20 val PER: 0.1223
2026-01-03 13:03:34,043: t15.2023.08.25 val PER: 0.0889
2026-01-03 13:03:34,043: t15.2023.08.27 val PER: 0.1945
2026-01-03 13:03:34,043: t15.2023.09.01 val PER: 0.0917
2026-01-03 13:03:34,043: t15.2023.09.03 val PER: 0.1710
2026-01-03 13:03:34,043: t15.2023.09.24 val PER: 0.1335
2026-01-03 13:03:34,043: t15.2023.09.29 val PER: 0.1385
2026-01-03 13:03:34,043: t15.2023.10.01 val PER: 0.1823
2026-01-03 13:03:34,043: t15.2023.10.06 val PER: 0.0861
2026-01-03 13:03:34,043: t15.2023.10.08 val PER: 0.2585
2026-01-03 13:03:34,043: t15.2023.10.13 val PER: 0.2289
2026-01-03 13:03:34,044: t15.2023.10.15 val PER: 0.1694
2026-01-03 13:03:34,044: t15.2023.10.20 val PER: 0.1879
2026-01-03 13:03:34,044: t15.2023.10.22 val PER: 0.1214
2026-01-03 13:03:34,044: t15.2023.11.03 val PER: 0.1906
2026-01-03 13:03:34,044: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:03:34,044: t15.2023.11.17 val PER: 0.0482
2026-01-03 13:03:34,044: t15.2023.11.19 val PER: 0.0479
2026-01-03 13:03:34,044: t15.2023.11.26 val PER: 0.1464
2026-01-03 13:03:34,044: t15.2023.12.03 val PER: 0.1261
2026-01-03 13:03:34,044: t15.2023.12.08 val PER: 0.1165
2026-01-03 13:03:34,044: t15.2023.12.10 val PER: 0.0972
2026-01-03 13:03:34,045: t15.2023.12.17 val PER: 0.1393
2026-01-03 13:03:34,045: t15.2023.12.29 val PER: 0.1414
2026-01-03 13:03:34,045: t15.2024.02.25 val PER: 0.1222
2026-01-03 13:03:34,045: t15.2024.03.08 val PER: 0.2475
2026-01-03 13:03:34,045: t15.2024.03.15 val PER: 0.2126
2026-01-03 13:03:34,045: t15.2024.03.17 val PER: 0.1513
2026-01-03 13:03:34,045: t15.2024.05.10 val PER: 0.1872
2026-01-03 13:03:34,045: t15.2024.06.14 val PER: 0.1782
2026-01-03 13:03:34,045: t15.2024.07.19 val PER: 0.2525
2026-01-03 13:03:34,045: t15.2024.07.21 val PER: 0.1048
2026-01-03 13:03:34,045: t15.2024.07.28 val PER: 0.1434
2026-01-03 13:03:34,045: t15.2025.01.10 val PER: 0.2961
2026-01-03 13:03:34,045: t15.2025.01.12 val PER: 0.1601
2026-01-03 13:03:34,045: t15.2025.03.14 val PER: 0.3447
2026-01-03 13:03:34,046: t15.2025.03.16 val PER: 0.1846
2026-01-03 13:03:34,046: t15.2025.03.30 val PER: 0.3023
2026-01-03 13:03:34,046: t15.2025.04.13 val PER: 0.2211
2026-01-03 13:03:34,309: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_14500
2026-01-03 13:03:42,869: Train batch 14600: loss: 15.07 grad norm: 62.94 time: 0.058
2026-01-03 13:04:00,228: Train batch 14800: loss: 6.73 grad norm: 44.00 time: 0.050
2026-01-03 13:04:17,427: Train batch 15000: loss: 10.74 grad norm: 51.44 time: 0.051
2026-01-03 13:04:17,427: Running test after training batch: 15000
2026-01-03 13:04:17,518: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:04:22,467: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:04:22,500: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:04:24,297: Val batch 15000: PER (avg): 0.1608 CTC Loss (avg): 16.0360 WER(1gram): 47.72% (n=64) time: 6.870
2026-01-03 13:04:24,297: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 13:04:24,297: t15.2023.08.13 val PER: 0.1237
2026-01-03 13:04:24,297: t15.2023.08.18 val PER: 0.1081
2026-01-03 13:04:24,297: t15.2023.08.20 val PER: 0.1239
2026-01-03 13:04:24,297: t15.2023.08.25 val PER: 0.0828
2026-01-03 13:04:24,297: t15.2023.08.27 val PER: 0.1881
2026-01-03 13:04:24,297: t15.2023.09.01 val PER: 0.0901
2026-01-03 13:04:24,297: t15.2023.09.03 val PER: 0.1675
2026-01-03 13:04:24,298: t15.2023.09.24 val PER: 0.1347
2026-01-03 13:04:24,298: t15.2023.09.29 val PER: 0.1391
2026-01-03 13:04:24,298: t15.2023.10.01 val PER: 0.1764
2026-01-03 13:04:24,298: t15.2023.10.06 val PER: 0.0829
2026-01-03 13:04:24,298: t15.2023.10.08 val PER: 0.2598
2026-01-03 13:04:24,298: t15.2023.10.13 val PER: 0.2289
2026-01-03 13:04:24,298: t15.2023.10.15 val PER: 0.1681
2026-01-03 13:04:24,298: t15.2023.10.20 val PER: 0.1812
2026-01-03 13:04:24,298: t15.2023.10.22 val PER: 0.1214
2026-01-03 13:04:24,298: t15.2023.11.03 val PER: 0.1886
2026-01-03 13:04:24,298: t15.2023.11.04 val PER: 0.0307
2026-01-03 13:04:24,298: t15.2023.11.17 val PER: 0.0498
2026-01-03 13:04:24,299: t15.2023.11.19 val PER: 0.0479
2026-01-03 13:04:24,299: t15.2023.11.26 val PER: 0.1413
2026-01-03 13:04:24,299: t15.2023.12.03 val PER: 0.1271
2026-01-03 13:04:24,299: t15.2023.12.08 val PER: 0.1172
2026-01-03 13:04:24,299: t15.2023.12.10 val PER: 0.0986
2026-01-03 13:04:24,299: t15.2023.12.17 val PER: 0.1424
2026-01-03 13:04:24,299: t15.2023.12.29 val PER: 0.1455
2026-01-03 13:04:24,299: t15.2024.02.25 val PER: 0.1194
2026-01-03 13:04:24,299: t15.2024.03.08 val PER: 0.2447
2026-01-03 13:04:24,299: t15.2024.03.15 val PER: 0.2133
2026-01-03 13:04:24,299: t15.2024.03.17 val PER: 0.1513
2026-01-03 13:04:24,299: t15.2024.05.10 val PER: 0.1887
2026-01-03 13:04:24,299: t15.2024.06.14 val PER: 0.1719
2026-01-03 13:04:24,299: t15.2024.07.19 val PER: 0.2465
2026-01-03 13:04:24,300: t15.2024.07.21 val PER: 0.1069
2026-01-03 13:04:24,300: t15.2024.07.28 val PER: 0.1434
2026-01-03 13:04:24,300: t15.2025.01.10 val PER: 0.2975
2026-01-03 13:04:24,300: t15.2025.01.12 val PER: 0.1570
2026-01-03 13:04:24,300: t15.2025.03.14 val PER: 0.3388
2026-01-03 13:04:24,300: t15.2025.03.16 val PER: 0.1872
2026-01-03 13:04:24,300: t15.2025.03.30 val PER: 0.3069
2026-01-03 13:04:24,300: t15.2025.04.13 val PER: 0.2211
2026-01-03 13:04:24,562: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_15000
2026-01-03 13:04:42,141: Train batch 15200: loss: 5.68 grad norm: 39.86 time: 0.057
2026-01-03 13:04:59,551: Train batch 15400: loss: 13.51 grad norm: 58.96 time: 0.049
2026-01-03 13:05:08,435: Running test after training batch: 15500
2026-01-03 13:05:08,603: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:05:13,326: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:05:13,360: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:05:15,193: Val batch 15500: PER (avg): 0.1600 CTC Loss (avg): 15.9917 WER(1gram): 46.95% (n=64) time: 6.758
2026-01-03 13:05:15,193: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 13:05:15,193: t15.2023.08.13 val PER: 0.1216
2026-01-03 13:05:15,193: t15.2023.08.18 val PER: 0.1106
2026-01-03 13:05:15,193: t15.2023.08.20 val PER: 0.1199
2026-01-03 13:05:15,193: t15.2023.08.25 val PER: 0.0873
2026-01-03 13:05:15,194: t15.2023.08.27 val PER: 0.1945
2026-01-03 13:05:15,194: t15.2023.09.01 val PER: 0.0885
2026-01-03 13:05:15,194: t15.2023.09.03 val PER: 0.1758
2026-01-03 13:05:15,194: t15.2023.09.24 val PER: 0.1311
2026-01-03 13:05:15,194: t15.2023.09.29 val PER: 0.1385
2026-01-03 13:05:15,194: t15.2023.10.01 val PER: 0.1810
2026-01-03 13:05:15,194: t15.2023.10.06 val PER: 0.0861
2026-01-03 13:05:15,194: t15.2023.10.08 val PER: 0.2558
2026-01-03 13:05:15,194: t15.2023.10.13 val PER: 0.2234
2026-01-03 13:05:15,194: t15.2023.10.15 val PER: 0.1688
2026-01-03 13:05:15,194: t15.2023.10.20 val PER: 0.1745
2026-01-03 13:05:15,194: t15.2023.10.22 val PER: 0.1203
2026-01-03 13:05:15,195: t15.2023.11.03 val PER: 0.1866
2026-01-03 13:05:15,195: t15.2023.11.04 val PER: 0.0273
2026-01-03 13:05:15,195: t15.2023.11.17 val PER: 0.0529
2026-01-03 13:05:15,195: t15.2023.11.19 val PER: 0.0439
2026-01-03 13:05:15,195: t15.2023.11.26 val PER: 0.1399
2026-01-03 13:05:15,195: t15.2023.12.03 val PER: 0.1261
2026-01-03 13:05:15,195: t15.2023.12.08 val PER: 0.1172
2026-01-03 13:05:15,195: t15.2023.12.10 val PER: 0.0972
2026-01-03 13:05:15,195: t15.2023.12.17 val PER: 0.1414
2026-01-03 13:05:15,195: t15.2023.12.29 val PER: 0.1455
2026-01-03 13:05:15,195: t15.2024.02.25 val PER: 0.1152
2026-01-03 13:05:15,195: t15.2024.03.08 val PER: 0.2432
2026-01-03 13:05:15,195: t15.2024.03.15 val PER: 0.2126
2026-01-03 13:05:15,195: t15.2024.03.17 val PER: 0.1492
2026-01-03 13:05:15,195: t15.2024.05.10 val PER: 0.1828
2026-01-03 13:05:15,195: t15.2024.06.14 val PER: 0.1735
2026-01-03 13:05:15,196: t15.2024.07.19 val PER: 0.2419
2026-01-03 13:05:15,196: t15.2024.07.21 val PER: 0.1048
2026-01-03 13:05:15,196: t15.2024.07.28 val PER: 0.1441
2026-01-03 13:05:15,196: t15.2025.01.10 val PER: 0.2975
2026-01-03 13:05:15,196: t15.2025.01.12 val PER: 0.1532
2026-01-03 13:05:15,196: t15.2025.03.14 val PER: 0.3343
2026-01-03 13:05:15,196: t15.2025.03.16 val PER: 0.1924
2026-01-03 13:05:15,196: t15.2025.03.30 val PER: 0.3057
2026-01-03 13:05:15,196: t15.2025.04.13 val PER: 0.2211
2026-01-03 13:05:15,198: New best val WER(1gram) 47.21% --> 46.95%
2026-01-03 13:05:15,198: Checkpointing model
2026-01-03 13:05:15,467: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/best_checkpoint
2026-01-03 13:05:15,761: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_15500
2026-01-03 13:05:24,613: Train batch 15600: loss: 14.68 grad norm: 58.31 time: 0.061
2026-01-03 13:05:42,087: Train batch 15800: loss: 16.72 grad norm: 64.74 time: 0.066
2026-01-03 13:05:59,643: Train batch 16000: loss: 10.51 grad norm: 48.41 time: 0.054
2026-01-03 13:05:59,643: Running test after training batch: 16000
2026-01-03 13:05:59,744: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:06:04,482: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:06:04,515: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:06:06,375: Val batch 16000: PER (avg): 0.1604 CTC Loss (avg): 16.0535 WER(1gram): 48.22% (n=64) time: 6.732
2026-01-03 13:06:06,376: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 13:06:06,376: t15.2023.08.13 val PER: 0.1227
2026-01-03 13:06:06,376: t15.2023.08.18 val PER: 0.1098
2026-01-03 13:06:06,376: t15.2023.08.20 val PER: 0.1223
2026-01-03 13:06:06,376: t15.2023.08.25 val PER: 0.0889
2026-01-03 13:06:06,376: t15.2023.08.27 val PER: 0.1881
2026-01-03 13:06:06,376: t15.2023.09.01 val PER: 0.0844
2026-01-03 13:06:06,376: t15.2023.09.03 val PER: 0.1710
2026-01-03 13:06:06,376: t15.2023.09.24 val PER: 0.1335
2026-01-03 13:06:06,376: t15.2023.09.29 val PER: 0.1372
2026-01-03 13:06:06,377: t15.2023.10.01 val PER: 0.1783
2026-01-03 13:06:06,377: t15.2023.10.06 val PER: 0.0883
2026-01-03 13:06:06,377: t15.2023.10.08 val PER: 0.2585
2026-01-03 13:06:06,377: t15.2023.10.13 val PER: 0.2211
2026-01-03 13:06:06,377: t15.2023.10.15 val PER: 0.1674
2026-01-03 13:06:06,377: t15.2023.10.20 val PER: 0.1879
2026-01-03 13:06:06,377: t15.2023.10.22 val PER: 0.1180
2026-01-03 13:06:06,377: t15.2023.11.03 val PER: 0.1900
2026-01-03 13:06:06,377: t15.2023.11.04 val PER: 0.0307
2026-01-03 13:06:06,377: t15.2023.11.17 val PER: 0.0467
2026-01-03 13:06:06,377: t15.2023.11.19 val PER: 0.0439
2026-01-03 13:06:06,377: t15.2023.11.26 val PER: 0.1442
2026-01-03 13:06:06,377: t15.2023.12.03 val PER: 0.1208
2026-01-03 13:06:06,377: t15.2023.12.08 val PER: 0.1172
2026-01-03 13:06:06,378: t15.2023.12.10 val PER: 0.0986
2026-01-03 13:06:06,378: t15.2023.12.17 val PER: 0.1393
2026-01-03 13:06:06,378: t15.2023.12.29 val PER: 0.1441
2026-01-03 13:06:06,378: t15.2024.02.25 val PER: 0.1194
2026-01-03 13:06:06,378: t15.2024.03.08 val PER: 0.2504
2026-01-03 13:06:06,378: t15.2024.03.15 val PER: 0.2151
2026-01-03 13:06:06,378: t15.2024.03.17 val PER: 0.1499
2026-01-03 13:06:06,378: t15.2024.05.10 val PER: 0.1872
2026-01-03 13:06:06,379: t15.2024.06.14 val PER: 0.1751
2026-01-03 13:06:06,379: t15.2024.07.19 val PER: 0.2452
2026-01-03 13:06:06,379: t15.2024.07.21 val PER: 0.1048
2026-01-03 13:06:06,379: t15.2024.07.28 val PER: 0.1463
2026-01-03 13:06:06,379: t15.2025.01.10 val PER: 0.2961
2026-01-03 13:06:06,379: t15.2025.01.12 val PER: 0.1540
2026-01-03 13:06:06,379: t15.2025.03.14 val PER: 0.3432
2026-01-03 13:06:06,379: t15.2025.03.16 val PER: 0.1898
2026-01-03 13:06:06,379: t15.2025.03.30 val PER: 0.2989
2026-01-03 13:06:06,379: t15.2025.04.13 val PER: 0.2282
2026-01-03 13:06:06,641: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_16000
2026-01-03 13:06:24,497: Train batch 16200: loss: 8.04 grad norm: 43.23 time: 0.055
2026-01-03 13:06:42,143: Train batch 16400: loss: 12.05 grad norm: 60.04 time: 0.057
2026-01-03 13:06:50,861: Running test after training batch: 16500
2026-01-03 13:06:50,988: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:06:55,902: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:06:55,936: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:06:57,764: Val batch 16500: PER (avg): 0.1607 CTC Loss (avg): 16.0323 WER(1gram): 47.21% (n=64) time: 6.902
2026-01-03 13:06:57,764: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 13:06:57,764: t15.2023.08.13 val PER: 0.1247
2026-01-03 13:06:57,764: t15.2023.08.18 val PER: 0.1090
2026-01-03 13:06:57,764: t15.2023.08.20 val PER: 0.1223
2026-01-03 13:06:57,764: t15.2023.08.25 val PER: 0.0783
2026-01-03 13:06:57,764: t15.2023.08.27 val PER: 0.1913
2026-01-03 13:06:57,765: t15.2023.09.01 val PER: 0.0852
2026-01-03 13:06:57,765: t15.2023.09.03 val PER: 0.1698
2026-01-03 13:06:57,765: t15.2023.09.24 val PER: 0.1311
2026-01-03 13:06:57,765: t15.2023.09.29 val PER: 0.1385
2026-01-03 13:06:57,765: t15.2023.10.01 val PER: 0.1810
2026-01-03 13:06:57,765: t15.2023.10.06 val PER: 0.0904
2026-01-03 13:06:57,765: t15.2023.10.08 val PER: 0.2598
2026-01-03 13:06:57,765: t15.2023.10.13 val PER: 0.2234
2026-01-03 13:06:57,765: t15.2023.10.15 val PER: 0.1701
2026-01-03 13:06:57,765: t15.2023.10.20 val PER: 0.1879
2026-01-03 13:06:57,765: t15.2023.10.22 val PER: 0.1180
2026-01-03 13:06:57,765: t15.2023.11.03 val PER: 0.1866
2026-01-03 13:06:57,765: t15.2023.11.04 val PER: 0.0307
2026-01-03 13:06:57,765: t15.2023.11.17 val PER: 0.0451
2026-01-03 13:06:57,765: t15.2023.11.19 val PER: 0.0439
2026-01-03 13:06:57,766: t15.2023.11.26 val PER: 0.1428
2026-01-03 13:06:57,766: t15.2023.12.03 val PER: 0.1239
2026-01-03 13:06:57,766: t15.2023.12.08 val PER: 0.1165
2026-01-03 13:06:57,766: t15.2023.12.10 val PER: 0.1012
2026-01-03 13:06:57,766: t15.2023.12.17 val PER: 0.1424
2026-01-03 13:06:57,766: t15.2023.12.29 val PER: 0.1434
2026-01-03 13:06:57,766: t15.2024.02.25 val PER: 0.1208
2026-01-03 13:06:57,766: t15.2024.03.08 val PER: 0.2475
2026-01-03 13:06:57,766: t15.2024.03.15 val PER: 0.2120
2026-01-03 13:06:57,766: t15.2024.03.17 val PER: 0.1527
2026-01-03 13:06:57,766: t15.2024.05.10 val PER: 0.1887
2026-01-03 13:06:57,766: t15.2024.06.14 val PER: 0.1735
2026-01-03 13:06:57,766: t15.2024.07.19 val PER: 0.2472
2026-01-03 13:06:57,766: t15.2024.07.21 val PER: 0.1041
2026-01-03 13:06:57,766: t15.2024.07.28 val PER: 0.1419
2026-01-03 13:06:57,766: t15.2025.01.10 val PER: 0.2948
2026-01-03 13:06:57,766: t15.2025.01.12 val PER: 0.1555
2026-01-03 13:06:57,766: t15.2025.03.14 val PER: 0.3506
2026-01-03 13:06:57,766: t15.2025.03.16 val PER: 0.1885
2026-01-03 13:06:57,767: t15.2025.03.30 val PER: 0.3046
2026-01-03 13:06:57,767: t15.2025.04.13 val PER: 0.2311
2026-01-03 13:06:58,034: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_16500
2026-01-03 13:07:06,955: Train batch 16600: loss: 9.92 grad norm: 51.40 time: 0.051
2026-01-03 13:07:24,602: Train batch 16800: loss: 19.40 grad norm: 75.65 time: 0.061
2026-01-03 13:07:42,023: Train batch 17000: loss: 9.84 grad norm: 51.02 time: 0.081
2026-01-03 13:07:42,023: Running test after training batch: 17000
2026-01-03 13:07:42,119: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:07:46,867: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:07:46,902: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:07:48,791: Val batch 17000: PER (avg): 0.1606 CTC Loss (avg): 16.0136 WER(1gram): 48.22% (n=64) time: 6.768
2026-01-03 13:07:48,791: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 13:07:48,791: t15.2023.08.13 val PER: 0.1247
2026-01-03 13:07:48,791: t15.2023.08.18 val PER: 0.1081
2026-01-03 13:07:48,791: t15.2023.08.20 val PER: 0.1231
2026-01-03 13:07:48,791: t15.2023.08.25 val PER: 0.0858
2026-01-03 13:07:48,792: t15.2023.08.27 val PER: 0.1929
2026-01-03 13:07:48,792: t15.2023.09.01 val PER: 0.0893
2026-01-03 13:07:48,792: t15.2023.09.03 val PER: 0.1663
2026-01-03 13:07:48,792: t15.2023.09.24 val PER: 0.1396
2026-01-03 13:07:48,792: t15.2023.09.29 val PER: 0.1378
2026-01-03 13:07:48,792: t15.2023.10.01 val PER: 0.1810
2026-01-03 13:07:48,792: t15.2023.10.06 val PER: 0.0883
2026-01-03 13:07:48,792: t15.2023.10.08 val PER: 0.2598
2026-01-03 13:07:48,792: t15.2023.10.13 val PER: 0.2203
2026-01-03 13:07:48,793: t15.2023.10.15 val PER: 0.1668
2026-01-03 13:07:48,793: t15.2023.10.20 val PER: 0.1879
2026-01-03 13:07:48,793: t15.2023.10.22 val PER: 0.1147
2026-01-03 13:07:48,793: t15.2023.11.03 val PER: 0.1886
2026-01-03 13:07:48,793: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:07:48,793: t15.2023.11.17 val PER: 0.0451
2026-01-03 13:07:48,793: t15.2023.11.19 val PER: 0.0419
2026-01-03 13:07:48,793: t15.2023.11.26 val PER: 0.1420
2026-01-03 13:07:48,793: t15.2023.12.03 val PER: 0.1292
2026-01-03 13:07:48,793: t15.2023.12.08 val PER: 0.1112
2026-01-03 13:07:48,793: t15.2023.12.10 val PER: 0.0986
2026-01-03 13:07:48,793: t15.2023.12.17 val PER: 0.1393
2026-01-03 13:07:48,793: t15.2023.12.29 val PER: 0.1421
2026-01-03 13:07:48,793: t15.2024.02.25 val PER: 0.1194
2026-01-03 13:07:48,793: t15.2024.03.08 val PER: 0.2532
2026-01-03 13:07:48,793: t15.2024.03.15 val PER: 0.2083
2026-01-03 13:07:48,794: t15.2024.03.17 val PER: 0.1534
2026-01-03 13:07:48,794: t15.2024.05.10 val PER: 0.1842
2026-01-03 13:07:48,794: t15.2024.06.14 val PER: 0.1751
2026-01-03 13:07:48,794: t15.2024.07.19 val PER: 0.2485
2026-01-03 13:07:48,794: t15.2024.07.21 val PER: 0.1048
2026-01-03 13:07:48,794: t15.2024.07.28 val PER: 0.1426
2026-01-03 13:07:48,794: t15.2025.01.10 val PER: 0.2975
2026-01-03 13:07:48,794: t15.2025.01.12 val PER: 0.1586
2026-01-03 13:07:48,794: t15.2025.03.14 val PER: 0.3506
2026-01-03 13:07:48,794: t15.2025.03.16 val PER: 0.1846
2026-01-03 13:07:48,794: t15.2025.03.30 val PER: 0.3034
2026-01-03 13:07:48,794: t15.2025.04.13 val PER: 0.2340
2026-01-03 13:07:49,063: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_17000
2026-01-03 13:08:06,586: Train batch 17200: loss: 11.96 grad norm: 49.95 time: 0.083
2026-01-03 13:08:24,230: Train batch 17400: loss: 14.65 grad norm: 61.18 time: 0.070
2026-01-03 13:08:32,854: Running test after training batch: 17500
2026-01-03 13:08:32,947: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:08:38,030: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:08:38,065: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:08:39,978: Val batch 17500: PER (avg): 0.1603 CTC Loss (avg): 16.0118 WER(1gram): 47.72% (n=64) time: 7.123
2026-01-03 13:08:39,978: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 13:08:39,978: t15.2023.08.13 val PER: 0.1247
2026-01-03 13:08:39,978: t15.2023.08.18 val PER: 0.1098
2026-01-03 13:08:39,978: t15.2023.08.20 val PER: 0.1239
2026-01-03 13:08:39,979: t15.2023.08.25 val PER: 0.0873
2026-01-03 13:08:39,979: t15.2023.08.27 val PER: 0.1881
2026-01-03 13:08:39,979: t15.2023.09.01 val PER: 0.0877
2026-01-03 13:08:39,979: t15.2023.09.03 val PER: 0.1663
2026-01-03 13:08:39,979: t15.2023.09.24 val PER: 0.1383
2026-01-03 13:08:39,979: t15.2023.09.29 val PER: 0.1391
2026-01-03 13:08:39,979: t15.2023.10.01 val PER: 0.1797
2026-01-03 13:08:39,979: t15.2023.10.06 val PER: 0.0883
2026-01-03 13:08:39,979: t15.2023.10.08 val PER: 0.2585
2026-01-03 13:08:39,980: t15.2023.10.13 val PER: 0.2180
2026-01-03 13:08:39,980: t15.2023.10.15 val PER: 0.1674
2026-01-03 13:08:39,980: t15.2023.10.20 val PER: 0.1980
2026-01-03 13:08:39,980: t15.2023.10.22 val PER: 0.1136
2026-01-03 13:08:39,980: t15.2023.11.03 val PER: 0.1893
2026-01-03 13:08:39,980: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:08:39,980: t15.2023.11.17 val PER: 0.0467
2026-01-03 13:08:39,980: t15.2023.11.19 val PER: 0.0459
2026-01-03 13:08:39,980: t15.2023.11.26 val PER: 0.1420
2026-01-03 13:08:39,980: t15.2023.12.03 val PER: 0.1208
2026-01-03 13:08:39,981: t15.2023.12.08 val PER: 0.1119
2026-01-03 13:08:39,981: t15.2023.12.10 val PER: 0.0999
2026-01-03 13:08:39,981: t15.2023.12.17 val PER: 0.1414
2026-01-03 13:08:39,981: t15.2023.12.29 val PER: 0.1428
2026-01-03 13:08:39,981: t15.2024.02.25 val PER: 0.1180
2026-01-03 13:08:39,981: t15.2024.03.08 val PER: 0.2518
2026-01-03 13:08:39,981: t15.2024.03.15 val PER: 0.2139
2026-01-03 13:08:39,981: t15.2024.03.17 val PER: 0.1520
2026-01-03 13:08:39,981: t15.2024.05.10 val PER: 0.1902
2026-01-03 13:08:39,982: t15.2024.06.14 val PER: 0.1751
2026-01-03 13:08:39,982: t15.2024.07.19 val PER: 0.2465
2026-01-03 13:08:39,982: t15.2024.07.21 val PER: 0.1041
2026-01-03 13:08:39,982: t15.2024.07.28 val PER: 0.1397
2026-01-03 13:08:39,982: t15.2025.01.10 val PER: 0.2893
2026-01-03 13:08:39,982: t15.2025.01.12 val PER: 0.1555
2026-01-03 13:08:39,982: t15.2025.03.14 val PER: 0.3476
2026-01-03 13:08:39,982: t15.2025.03.16 val PER: 0.1806
2026-01-03 13:08:39,982: t15.2025.03.30 val PER: 0.3069
2026-01-03 13:08:39,983: t15.2025.04.13 val PER: 0.2325
2026-01-03 13:08:40,253: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_17500
2026-01-03 13:08:56,997: Train batch 17600: loss: 13.09 grad norm: 59.56 time: 0.050
2026-01-03 13:09:14,864: Train batch 17800: loss: 8.09 grad norm: 50.46 time: 0.041
2026-01-03 13:09:31,839: Train batch 18000: loss: 13.78 grad norm: 63.06 time: 0.060
2026-01-03 13:09:31,839: Running test after training batch: 18000
2026-01-03 13:09:31,973: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:09:37,656: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:09:37,690: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:09:39,570: Val batch 18000: PER (avg): 0.1603 CTC Loss (avg): 15.9919 WER(1gram): 47.46% (n=64) time: 7.731
2026-01-03 13:09:39,570: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 13:09:39,570: t15.2023.08.13 val PER: 0.1247
2026-01-03 13:09:39,570: t15.2023.08.18 val PER: 0.1106
2026-01-03 13:09:39,570: t15.2023.08.20 val PER: 0.1223
2026-01-03 13:09:39,571: t15.2023.08.25 val PER: 0.0843
2026-01-03 13:09:39,571: t15.2023.08.27 val PER: 0.1881
2026-01-03 13:09:39,571: t15.2023.09.01 val PER: 0.0885
2026-01-03 13:09:39,571: t15.2023.09.03 val PER: 0.1639
2026-01-03 13:09:39,571: t15.2023.09.24 val PER: 0.1383
2026-01-03 13:09:39,571: t15.2023.09.29 val PER: 0.1372
2026-01-03 13:09:39,571: t15.2023.10.01 val PER: 0.1797
2026-01-03 13:09:39,571: t15.2023.10.06 val PER: 0.0861
2026-01-03 13:09:39,571: t15.2023.10.08 val PER: 0.2571
2026-01-03 13:09:39,571: t15.2023.10.13 val PER: 0.2133
2026-01-03 13:09:39,571: t15.2023.10.15 val PER: 0.1688
2026-01-03 13:09:39,571: t15.2023.10.20 val PER: 0.1980
2026-01-03 13:09:39,571: t15.2023.10.22 val PER: 0.1147
2026-01-03 13:09:39,571: t15.2023.11.03 val PER: 0.1866
2026-01-03 13:09:39,571: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:09:39,571: t15.2023.11.17 val PER: 0.0482
2026-01-03 13:09:39,571: t15.2023.11.19 val PER: 0.0459
2026-01-03 13:09:39,572: t15.2023.11.26 val PER: 0.1413
2026-01-03 13:09:39,572: t15.2023.12.03 val PER: 0.1250
2026-01-03 13:09:39,572: t15.2023.12.08 val PER: 0.1152
2026-01-03 13:09:39,572: t15.2023.12.10 val PER: 0.0986
2026-01-03 13:09:39,572: t15.2023.12.17 val PER: 0.1445
2026-01-03 13:09:39,572: t15.2023.12.29 val PER: 0.1428
2026-01-03 13:09:39,572: t15.2024.02.25 val PER: 0.1208
2026-01-03 13:09:39,572: t15.2024.03.08 val PER: 0.2504
2026-01-03 13:09:39,573: t15.2024.03.15 val PER: 0.2114
2026-01-03 13:09:39,573: t15.2024.03.17 val PER: 0.1499
2026-01-03 13:09:39,573: t15.2024.05.10 val PER: 0.1917
2026-01-03 13:09:39,573: t15.2024.06.14 val PER: 0.1719
2026-01-03 13:09:39,573: t15.2024.07.19 val PER: 0.2512
2026-01-03 13:09:39,573: t15.2024.07.21 val PER: 0.1028
2026-01-03 13:09:39,573: t15.2024.07.28 val PER: 0.1434
2026-01-03 13:09:39,573: t15.2025.01.10 val PER: 0.2893
2026-01-03 13:09:39,573: t15.2025.01.12 val PER: 0.1563
2026-01-03 13:09:39,573: t15.2025.03.14 val PER: 0.3506
2026-01-03 13:09:39,573: t15.2025.03.16 val PER: 0.1872
2026-01-03 13:09:39,573: t15.2025.03.30 val PER: 0.3034
2026-01-03 13:09:39,573: t15.2025.04.13 val PER: 0.2340
2026-01-03 13:09:39,877: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_18000
2026-01-03 13:09:58,199: Train batch 18200: loss: 10.29 grad norm: 53.85 time: 0.074
2026-01-03 13:10:15,489: Train batch 18400: loss: 7.31 grad norm: 50.11 time: 0.057
2026-01-03 13:10:24,181: Running test after training batch: 18500
2026-01-03 13:10:24,328: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:10:29,051: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:10:29,086: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:10:31,013: Val batch 18500: PER (avg): 0.1605 CTC Loss (avg): 15.9881 WER(1gram): 47.97% (n=64) time: 6.832
2026-01-03 13:10:31,014: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 13:10:31,014: t15.2023.08.13 val PER: 0.1237
2026-01-03 13:10:31,014: t15.2023.08.18 val PER: 0.1115
2026-01-03 13:10:31,014: t15.2023.08.20 val PER: 0.1223
2026-01-03 13:10:31,014: t15.2023.08.25 val PER: 0.0858
2026-01-03 13:10:31,014: t15.2023.08.27 val PER: 0.1897
2026-01-03 13:10:31,014: t15.2023.09.01 val PER: 0.0877
2026-01-03 13:10:31,015: t15.2023.09.03 val PER: 0.1675
2026-01-03 13:10:31,015: t15.2023.09.24 val PER: 0.1359
2026-01-03 13:10:31,015: t15.2023.09.29 val PER: 0.1385
2026-01-03 13:10:31,015: t15.2023.10.01 val PER: 0.1790
2026-01-03 13:10:31,015: t15.2023.10.06 val PER: 0.0893
2026-01-03 13:10:31,015: t15.2023.10.08 val PER: 0.2585
2026-01-03 13:10:31,015: t15.2023.10.13 val PER: 0.2180
2026-01-03 13:10:31,015: t15.2023.10.15 val PER: 0.1674
2026-01-03 13:10:31,015: t15.2023.10.20 val PER: 0.1946
2026-01-03 13:10:31,016: t15.2023.10.22 val PER: 0.1147
2026-01-03 13:10:31,016: t15.2023.11.03 val PER: 0.1886
2026-01-03 13:10:31,016: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:10:31,016: t15.2023.11.17 val PER: 0.0467
2026-01-03 13:10:31,016: t15.2023.11.19 val PER: 0.0479
2026-01-03 13:10:31,016: t15.2023.11.26 val PER: 0.1391
2026-01-03 13:10:31,016: t15.2023.12.03 val PER: 0.1239
2026-01-03 13:10:31,016: t15.2023.12.08 val PER: 0.1145
2026-01-03 13:10:31,016: t15.2023.12.10 val PER: 0.0999
2026-01-03 13:10:31,017: t15.2023.12.17 val PER: 0.1393
2026-01-03 13:10:31,017: t15.2023.12.29 val PER: 0.1414
2026-01-03 13:10:31,017: t15.2024.02.25 val PER: 0.1222
2026-01-03 13:10:31,017: t15.2024.03.08 val PER: 0.2546
2026-01-03 13:10:31,017: t15.2024.03.15 val PER: 0.2114
2026-01-03 13:10:31,017: t15.2024.03.17 val PER: 0.1499
2026-01-03 13:10:31,017: t15.2024.05.10 val PER: 0.1887
2026-01-03 13:10:31,017: t15.2024.06.14 val PER: 0.1735
2026-01-03 13:10:31,017: t15.2024.07.19 val PER: 0.2505
2026-01-03 13:10:31,017: t15.2024.07.21 val PER: 0.1055
2026-01-03 13:10:31,018: t15.2024.07.28 val PER: 0.1449
2026-01-03 13:10:31,018: t15.2025.01.10 val PER: 0.2879
2026-01-03 13:10:31,018: t15.2025.01.12 val PER: 0.1578
2026-01-03 13:10:31,018: t15.2025.03.14 val PER: 0.3521
2026-01-03 13:10:31,018: t15.2025.03.16 val PER: 0.1872
2026-01-03 13:10:31,018: t15.2025.03.30 val PER: 0.3000
2026-01-03 13:10:31,018: t15.2025.04.13 val PER: 0.2311
2026-01-03 13:10:31,307: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_18500
2026-01-03 13:10:40,377: Train batch 18600: loss: 15.44 grad norm: 62.66 time: 0.066
2026-01-03 13:10:58,050: Train batch 18800: loss: 11.40 grad norm: 56.21 time: 0.063
2026-01-03 13:11:15,869: Train batch 19000: loss: 10.03 grad norm: 44.55 time: 0.065
2026-01-03 13:11:15,869: Running test after training batch: 19000
2026-01-03 13:11:16,000: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:11:21,710: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:11:21,747: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:11:23,675: Val batch 19000: PER (avg): 0.1606 CTC Loss (avg): 15.9948 WER(1gram): 47.46% (n=64) time: 7.806
2026-01-03 13:11:23,676: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 13:11:23,676: t15.2023.08.13 val PER: 0.1206
2026-01-03 13:11:23,676: t15.2023.08.18 val PER: 0.1132
2026-01-03 13:11:23,676: t15.2023.08.20 val PER: 0.1215
2026-01-03 13:11:23,676: t15.2023.08.25 val PER: 0.0858
2026-01-03 13:11:23,676: t15.2023.08.27 val PER: 0.1897
2026-01-03 13:11:23,676: t15.2023.09.01 val PER: 0.0869
2026-01-03 13:11:23,676: t15.2023.09.03 val PER: 0.1686
2026-01-03 13:11:23,677: t15.2023.09.24 val PER: 0.1383
2026-01-03 13:11:23,677: t15.2023.09.29 val PER: 0.1404
2026-01-03 13:11:23,677: t15.2023.10.01 val PER: 0.1803
2026-01-03 13:11:23,677: t15.2023.10.06 val PER: 0.0893
2026-01-03 13:11:23,677: t15.2023.10.08 val PER: 0.2598
2026-01-03 13:11:23,677: t15.2023.10.13 val PER: 0.2149
2026-01-03 13:11:23,677: t15.2023.10.15 val PER: 0.1681
2026-01-03 13:11:23,677: t15.2023.10.20 val PER: 0.1980
2026-01-03 13:11:23,677: t15.2023.10.22 val PER: 0.1147
2026-01-03 13:11:23,677: t15.2023.11.03 val PER: 0.1893
2026-01-03 13:11:23,677: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:11:23,677: t15.2023.11.17 val PER: 0.0482
2026-01-03 13:11:23,677: t15.2023.11.19 val PER: 0.0439
2026-01-03 13:11:23,677: t15.2023.11.26 val PER: 0.1406
2026-01-03 13:11:23,677: t15.2023.12.03 val PER: 0.1239
2026-01-03 13:11:23,677: t15.2023.12.08 val PER: 0.1132
2026-01-03 13:11:23,678: t15.2023.12.10 val PER: 0.1038
2026-01-03 13:11:23,678: t15.2023.12.17 val PER: 0.1445
2026-01-03 13:11:23,678: t15.2023.12.29 val PER: 0.1407
2026-01-03 13:11:23,678: t15.2024.02.25 val PER: 0.1250
2026-01-03 13:11:23,678: t15.2024.03.08 val PER: 0.2518
2026-01-03 13:11:23,678: t15.2024.03.15 val PER: 0.2120
2026-01-03 13:11:23,678: t15.2024.03.17 val PER: 0.1499
2026-01-03 13:11:23,678: t15.2024.05.10 val PER: 0.1857
2026-01-03 13:11:23,678: t15.2024.06.14 val PER: 0.1719
2026-01-03 13:11:23,678: t15.2024.07.19 val PER: 0.2492
2026-01-03 13:11:23,678: t15.2024.07.21 val PER: 0.1048
2026-01-03 13:11:23,678: t15.2024.07.28 val PER: 0.1426
2026-01-03 13:11:23,678: t15.2025.01.10 val PER: 0.2906
2026-01-03 13:11:23,678: t15.2025.01.12 val PER: 0.1532
2026-01-03 13:11:23,678: t15.2025.03.14 val PER: 0.3536
2026-01-03 13:11:23,678: t15.2025.03.16 val PER: 0.1846
2026-01-03 13:11:23,678: t15.2025.03.30 val PER: 0.3046
2026-01-03 13:11:23,679: t15.2025.04.13 val PER: 0.2311
2026-01-03 13:11:24,096: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_19000
2026-01-03 13:11:41,694: Train batch 19200: loss: 7.87 grad norm: 50.33 time: 0.062
2026-01-03 13:11:59,132: Train batch 19400: loss: 6.88 grad norm: 42.71 time: 0.052
2026-01-03 13:12:07,684: Running test after training batch: 19500
2026-01-03 13:12:07,830: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:12:12,945: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:12:12,980: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:12:14,944: Val batch 19500: PER (avg): 0.1604 CTC Loss (avg): 15.9854 WER(1gram): 47.72% (n=64) time: 7.259
2026-01-03 13:12:14,944: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 13:12:14,944: t15.2023.08.13 val PER: 0.1227
2026-01-03 13:12:14,944: t15.2023.08.18 val PER: 0.1106
2026-01-03 13:12:14,945: t15.2023.08.20 val PER: 0.1239
2026-01-03 13:12:14,945: t15.2023.08.25 val PER: 0.0889
2026-01-03 13:12:14,945: t15.2023.08.27 val PER: 0.1881
2026-01-03 13:12:14,945: t15.2023.09.01 val PER: 0.0877
2026-01-03 13:12:14,945: t15.2023.09.03 val PER: 0.1615
2026-01-03 13:12:14,945: t15.2023.09.24 val PER: 0.1396
2026-01-03 13:12:14,945: t15.2023.09.29 val PER: 0.1378
2026-01-03 13:12:14,945: t15.2023.10.01 val PER: 0.1797
2026-01-03 13:12:14,945: t15.2023.10.06 val PER: 0.0872
2026-01-03 13:12:14,945: t15.2023.10.08 val PER: 0.2585
2026-01-03 13:12:14,945: t15.2023.10.13 val PER: 0.2157
2026-01-03 13:12:14,945: t15.2023.10.15 val PER: 0.1674
2026-01-03 13:12:14,945: t15.2023.10.20 val PER: 0.1980
2026-01-03 13:12:14,946: t15.2023.10.22 val PER: 0.1158
2026-01-03 13:12:14,946: t15.2023.11.03 val PER: 0.1893
2026-01-03 13:12:14,946: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:12:14,946: t15.2023.11.17 val PER: 0.0451
2026-01-03 13:12:14,946: t15.2023.11.19 val PER: 0.0439
2026-01-03 13:12:14,946: t15.2023.11.26 val PER: 0.1399
2026-01-03 13:12:14,946: t15.2023.12.03 val PER: 0.1239
2026-01-03 13:12:14,946: t15.2023.12.08 val PER: 0.1152
2026-01-03 13:12:14,946: t15.2023.12.10 val PER: 0.0972
2026-01-03 13:12:14,946: t15.2023.12.17 val PER: 0.1424
2026-01-03 13:12:14,946: t15.2023.12.29 val PER: 0.1428
2026-01-03 13:12:14,946: t15.2024.02.25 val PER: 0.1208
2026-01-03 13:12:14,947: t15.2024.03.08 val PER: 0.2546
2026-01-03 13:12:14,947: t15.2024.03.15 val PER: 0.2139
2026-01-03 13:12:14,947: t15.2024.03.17 val PER: 0.1520
2026-01-03 13:12:14,947: t15.2024.05.10 val PER: 0.1842
2026-01-03 13:12:14,947: t15.2024.06.14 val PER: 0.1735
2026-01-03 13:12:14,947: t15.2024.07.19 val PER: 0.2505
2026-01-03 13:12:14,947: t15.2024.07.21 val PER: 0.1069
2026-01-03 13:12:14,947: t15.2024.07.28 val PER: 0.1434
2026-01-03 13:12:14,947: t15.2025.01.10 val PER: 0.2879
2026-01-03 13:12:14,947: t15.2025.01.12 val PER: 0.1509
2026-01-03 13:12:14,947: t15.2025.03.14 val PER: 0.3506
2026-01-03 13:12:14,947: t15.2025.03.16 val PER: 0.1859
2026-01-03 13:12:14,948: t15.2025.03.30 val PER: 0.3034
2026-01-03 13:12:14,948: t15.2025.04.13 val PER: 0.2340
2026-01-03 13:12:15,235: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_19500
2026-01-03 13:12:24,406: Train batch 19600: loss: 9.71 grad norm: 51.90 time: 0.057
2026-01-03 13:12:42,450: Train batch 19800: loss: 9.87 grad norm: 53.57 time: 0.054
2026-01-03 13:12:59,749: Running test after training batch: 19999
2026-01-03 13:12:59,844: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:13:05,034: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:13:05,068: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:13:06,885: Val batch 19999: PER (avg): 0.1605 CTC Loss (avg): 15.9698 WER(1gram): 47.72% (n=64) time: 7.135
2026-01-03 13:13:06,885: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 13:13:06,885: t15.2023.08.13 val PER: 0.1227
2026-01-03 13:13:06,885: t15.2023.08.18 val PER: 0.1132
2026-01-03 13:13:06,886: t15.2023.08.20 val PER: 0.1223
2026-01-03 13:13:06,886: t15.2023.08.25 val PER: 0.0873
2026-01-03 13:13:06,886: t15.2023.08.27 val PER: 0.1897
2026-01-03 13:13:06,886: t15.2023.09.01 val PER: 0.0885
2026-01-03 13:13:06,886: t15.2023.09.03 val PER: 0.1591
2026-01-03 13:13:06,886: t15.2023.09.24 val PER: 0.1371
2026-01-03 13:13:06,886: t15.2023.09.29 val PER: 0.1398
2026-01-03 13:13:06,886: t15.2023.10.01 val PER: 0.1783
2026-01-03 13:13:06,887: t15.2023.10.06 val PER: 0.0893
2026-01-03 13:13:06,887: t15.2023.10.08 val PER: 0.2598
2026-01-03 13:13:06,887: t15.2023.10.13 val PER: 0.2180
2026-01-03 13:13:06,887: t15.2023.10.15 val PER: 0.1668
2026-01-03 13:13:06,887: t15.2023.10.20 val PER: 0.1946
2026-01-03 13:13:06,887: t15.2023.10.22 val PER: 0.1158
2026-01-03 13:13:06,887: t15.2023.11.03 val PER: 0.1886
2026-01-03 13:13:06,887: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:13:06,887: t15.2023.11.17 val PER: 0.0467
2026-01-03 13:13:06,887: t15.2023.11.19 val PER: 0.0459
2026-01-03 13:13:06,887: t15.2023.11.26 val PER: 0.1413
2026-01-03 13:13:06,887: t15.2023.12.03 val PER: 0.1218
2026-01-03 13:13:06,887: t15.2023.12.08 val PER: 0.1145
2026-01-03 13:13:06,887: t15.2023.12.10 val PER: 0.0999
2026-01-03 13:13:06,887: t15.2023.12.17 val PER: 0.1435
2026-01-03 13:13:06,887: t15.2023.12.29 val PER: 0.1421
2026-01-03 13:13:06,888: t15.2024.02.25 val PER: 0.1208
2026-01-03 13:13:06,888: t15.2024.03.08 val PER: 0.2546
2026-01-03 13:13:06,888: t15.2024.03.15 val PER: 0.2120
2026-01-03 13:13:06,888: t15.2024.03.17 val PER: 0.1506
2026-01-03 13:13:06,888: t15.2024.05.10 val PER: 0.1842
2026-01-03 13:13:06,888: t15.2024.06.14 val PER: 0.1735
2026-01-03 13:13:06,888: t15.2024.07.19 val PER: 0.2518
2026-01-03 13:13:06,888: t15.2024.07.21 val PER: 0.1062
2026-01-03 13:13:06,888: t15.2024.07.28 val PER: 0.1419
2026-01-03 13:13:06,888: t15.2025.01.10 val PER: 0.2920
2026-01-03 13:13:06,888: t15.2025.01.12 val PER: 0.1540
2026-01-03 13:13:06,888: t15.2025.03.14 val PER: 0.3521
2026-01-03 13:13:06,888: t15.2025.03.16 val PER: 0.1819
2026-01-03 13:13:06,888: t15.2025.03.30 val PER: 0.3034
2026-01-03 13:13:06,888: t15.2025.04.13 val PER: 0.2340
2026-01-03 13:13:07,177: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step10k_f02/checkpoint/checkpoint_batch_19999
2026-01-03 13:13:07,211: Best avg val PER achieved: 0.15998
2026-01-03 13:13:07,211: Total training time: 34.50 minutes

=== RUN step12k_f01.yaml ===
2026-01-03 13:13:11,976: Using device: cuda:0
2026-01-03 13:13:13,588: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-03 13:13:13,610: Using 45 sessions after filtering (from 45).
2026-01-03 13:13:14,016: Using torch.compile (if available)
2026-01-03 13:13:14,017: torch.compile not available (torch<2.0). Skipping.
2026-01-03 13:13:14,017: Initialized RNN decoding model
2026-01-03 13:13:14,017: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 13:13:14,017: Model has 44,907,305 parameters
2026-01-03 13:13:14,017: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 13:13:15,318: Successfully initialized datasets
2026-01-03 13:13:15,319: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 13:13:16,436: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.212
2026-01-03 13:13:16,437: Running test after training batch: 0
2026-01-03 13:13:16,548: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:13:21,992: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-03 13:13:22,698: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-03 13:13:56,063: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 39.626
2026-01-03 13:13:56,063: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-03 13:13:56,063: t15.2023.08.13 val PER: 1.3056
2026-01-03 13:13:56,063: t15.2023.08.18 val PER: 1.4208
2026-01-03 13:13:56,064: t15.2023.08.20 val PER: 1.3002
2026-01-03 13:13:56,064: t15.2023.08.25 val PER: 1.3389
2026-01-03 13:13:56,064: t15.2023.08.27 val PER: 1.2460
2026-01-03 13:13:56,064: t15.2023.09.01 val PER: 1.4537
2026-01-03 13:13:56,064: t15.2023.09.03 val PER: 1.3171
2026-01-03 13:13:56,064: t15.2023.09.24 val PER: 1.5461
2026-01-03 13:13:56,064: t15.2023.09.29 val PER: 1.4671
2026-01-03 13:13:56,064: t15.2023.10.01 val PER: 1.2147
2026-01-03 13:13:56,064: t15.2023.10.06 val PER: 1.4876
2026-01-03 13:13:56,065: t15.2023.10.08 val PER: 1.1827
2026-01-03 13:13:56,065: t15.2023.10.13 val PER: 1.3964
2026-01-03 13:13:56,065: t15.2023.10.15 val PER: 1.3889
2026-01-03 13:13:56,065: t15.2023.10.20 val PER: 1.4866
2026-01-03 13:13:56,065: t15.2023.10.22 val PER: 1.3942
2026-01-03 13:13:56,065: t15.2023.11.03 val PER: 1.5923
2026-01-03 13:13:56,065: t15.2023.11.04 val PER: 2.0171
2026-01-03 13:13:56,065: t15.2023.11.17 val PER: 1.9518
2026-01-03 13:13:56,066: t15.2023.11.19 val PER: 1.6707
2026-01-03 13:13:56,066: t15.2023.11.26 val PER: 1.5413
2026-01-03 13:13:56,066: t15.2023.12.03 val PER: 1.4254
2026-01-03 13:13:56,066: t15.2023.12.08 val PER: 1.4487
2026-01-03 13:13:56,066: t15.2023.12.10 val PER: 1.6899
2026-01-03 13:13:56,066: t15.2023.12.17 val PER: 1.3077
2026-01-03 13:13:56,066: t15.2023.12.29 val PER: 1.4063
2026-01-03 13:13:56,066: t15.2024.02.25 val PER: 1.4228
2026-01-03 13:13:56,066: t15.2024.03.08 val PER: 1.3257
2026-01-03 13:13:56,067: t15.2024.03.15 val PER: 1.3196
2026-01-03 13:13:56,067: t15.2024.03.17 val PER: 1.4052
2026-01-03 13:13:56,067: t15.2024.05.10 val PER: 1.3224
2026-01-03 13:13:56,067: t15.2024.06.14 val PER: 1.5315
2026-01-03 13:13:56,067: t15.2024.07.19 val PER: 1.0817
2026-01-03 13:13:56,067: t15.2024.07.21 val PER: 1.6290
2026-01-03 13:13:56,067: t15.2024.07.28 val PER: 1.6588
2026-01-03 13:13:56,067: t15.2025.01.10 val PER: 1.0923
2026-01-03 13:13:56,067: t15.2025.01.12 val PER: 1.7629
2026-01-03 13:13:56,067: t15.2025.03.14 val PER: 1.0414
2026-01-03 13:13:56,067: t15.2025.03.16 val PER: 1.6257
2026-01-03 13:13:56,068: t15.2025.03.30 val PER: 1.2874
2026-01-03 13:13:56,068: t15.2025.04.13 val PER: 1.5949
2026-01-03 13:13:56,068: New best val WER(1gram) inf% --> 100.00%
2026-01-03 13:13:56,068: Checkpointing model
2026-01-03 13:13:56,311: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/best_checkpoint
2026-01-03 13:13:56,563: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_0
2026-01-03 13:14:14,201: Train batch 200: loss: 77.59 grad norm: 106.03 time: 0.054
2026-01-03 13:14:30,969: Train batch 400: loss: 53.46 grad norm: 88.04 time: 0.063
2026-01-03 13:14:39,515: Running test after training batch: 500
2026-01-03 13:14:39,647: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:14:44,480: WER debug example
  GT : you can see the code at this point as well
  PR : yule and ease thus uhde at this ide is aisle
2026-01-03 13:14:44,513: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as adz
2026-01-03 13:14:46,735: Val batch 500: PER (avg): 0.5170 CTC Loss (avg): 55.5552 WER(1gram): 89.59% (n=64) time: 7.220
2026-01-03 13:14:46,736: WER lens: avg_true_words=6.16 avg_pred_words=5.64 max_pred_words=12
2026-01-03 13:14:46,736: t15.2023.08.13 val PER: 0.4667
2026-01-03 13:14:46,736: t15.2023.08.18 val PER: 0.4484
2026-01-03 13:14:46,736: t15.2023.08.20 val PER: 0.4456
2026-01-03 13:14:46,736: t15.2023.08.25 val PER: 0.4307
2026-01-03 13:14:46,736: t15.2023.08.27 val PER: 0.5177
2026-01-03 13:14:46,736: t15.2023.09.01 val PER: 0.4294
2026-01-03 13:14:46,737: t15.2023.09.03 val PER: 0.4952
2026-01-03 13:14:46,737: t15.2023.09.24 val PER: 0.4333
2026-01-03 13:14:46,737: t15.2023.09.29 val PER: 0.4729
2026-01-03 13:14:46,737: t15.2023.10.01 val PER: 0.5185
2026-01-03 13:14:46,737: t15.2023.10.06 val PER: 0.4370
2026-01-03 13:14:46,737: t15.2023.10.08 val PER: 0.5318
2026-01-03 13:14:46,737: t15.2023.10.13 val PER: 0.5702
2026-01-03 13:14:46,737: t15.2023.10.15 val PER: 0.4951
2026-01-03 13:14:46,737: t15.2023.10.20 val PER: 0.4698
2026-01-03 13:14:46,737: t15.2023.10.22 val PER: 0.4677
2026-01-03 13:14:46,737: t15.2023.11.03 val PER: 0.5041
2026-01-03 13:14:46,737: t15.2023.11.04 val PER: 0.2560
2026-01-03 13:14:46,737: t15.2023.11.17 val PER: 0.3608
2026-01-03 13:14:46,737: t15.2023.11.19 val PER: 0.3413
2026-01-03 13:14:46,737: t15.2023.11.26 val PER: 0.5529
2026-01-03 13:14:46,738: t15.2023.12.03 val PER: 0.4916
2026-01-03 13:14:46,738: t15.2023.12.08 val PER: 0.5153
2026-01-03 13:14:46,738: t15.2023.12.10 val PER: 0.4507
2026-01-03 13:14:46,738: t15.2023.12.17 val PER: 0.5551
2026-01-03 13:14:46,738: t15.2023.12.29 val PER: 0.5353
2026-01-03 13:14:46,738: t15.2024.02.25 val PER: 0.4846
2026-01-03 13:14:46,738: t15.2024.03.08 val PER: 0.6131
2026-01-03 13:14:46,738: t15.2024.03.15 val PER: 0.5560
2026-01-03 13:14:46,738: t15.2024.03.17 val PER: 0.5028
2026-01-03 13:14:46,738: t15.2024.05.10 val PER: 0.5334
2026-01-03 13:14:46,738: t15.2024.06.14 val PER: 0.5205
2026-01-03 13:14:46,738: t15.2024.07.19 val PER: 0.6678
2026-01-03 13:14:46,738: t15.2024.07.21 val PER: 0.4683
2026-01-03 13:14:46,738: t15.2024.07.28 val PER: 0.5103
2026-01-03 13:14:46,739: t15.2025.01.10 val PER: 0.7314
2026-01-03 13:14:46,739: t15.2025.01.12 val PER: 0.5473
2026-01-03 13:14:46,740: t15.2025.03.14 val PER: 0.7308
2026-01-03 13:14:46,740: t15.2025.03.16 val PER: 0.5825
2026-01-03 13:14:46,740: t15.2025.03.30 val PER: 0.7333
2026-01-03 13:14:46,740: t15.2025.04.13 val PER: 0.5835
2026-01-03 13:14:46,741: New best val WER(1gram) 100.00% --> 89.59%
2026-01-03 13:14:46,741: Checkpointing model
2026-01-03 13:14:47,003: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/best_checkpoint
2026-01-03 13:14:47,257: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_500
2026-01-03 13:14:55,944: Train batch 600: loss: 49.94 grad norm: 80.31 time: 0.078
2026-01-03 13:15:13,290: Train batch 800: loss: 40.99 grad norm: 86.54 time: 0.057
2026-01-03 13:15:30,680: Train batch 1000: loss: 43.17 grad norm: 85.35 time: 0.065
2026-01-03 13:15:30,680: Running test after training batch: 1000
2026-01-03 13:15:30,804: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:15:35,591: WER debug example
  GT : you can see the code at this point as well
  PR : used ent ease thus code it this uhde is while
2026-01-03 13:15:35,622: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus wass it
2026-01-03 13:15:37,458: Val batch 1000: PER (avg): 0.4082 CTC Loss (avg): 42.6717 WER(1gram): 82.23% (n=64) time: 6.777
2026-01-03 13:15:37,458: WER lens: avg_true_words=6.16 avg_pred_words=5.56 max_pred_words=11
2026-01-03 13:15:37,459: t15.2023.08.13 val PER: 0.3753
2026-01-03 13:15:37,459: t15.2023.08.18 val PER: 0.3286
2026-01-03 13:15:37,459: t15.2023.08.20 val PER: 0.3503
2026-01-03 13:15:37,459: t15.2023.08.25 val PER: 0.2952
2026-01-03 13:15:37,459: t15.2023.08.27 val PER: 0.4277
2026-01-03 13:15:37,459: t15.2023.09.01 val PER: 0.2979
2026-01-03 13:15:37,459: t15.2023.09.03 val PER: 0.3955
2026-01-03 13:15:37,459: t15.2023.09.24 val PER: 0.3325
2026-01-03 13:15:37,459: t15.2023.09.29 val PER: 0.3586
2026-01-03 13:15:37,459: t15.2023.10.01 val PER: 0.4102
2026-01-03 13:15:37,459: t15.2023.10.06 val PER: 0.3100
2026-01-03 13:15:37,459: t15.2023.10.08 val PER: 0.4601
2026-01-03 13:15:37,460: t15.2023.10.13 val PER: 0.4585
2026-01-03 13:15:37,460: t15.2023.10.15 val PER: 0.3777
2026-01-03 13:15:37,460: t15.2023.10.20 val PER: 0.3893
2026-01-03 13:15:37,460: t15.2023.10.22 val PER: 0.3541
2026-01-03 13:15:37,460: t15.2023.11.03 val PER: 0.4003
2026-01-03 13:15:37,460: t15.2023.11.04 val PER: 0.1570
2026-01-03 13:15:37,460: t15.2023.11.17 val PER: 0.2597
2026-01-03 13:15:37,460: t15.2023.11.19 val PER: 0.2116
2026-01-03 13:15:37,460: t15.2023.11.26 val PER: 0.4471
2026-01-03 13:15:37,460: t15.2023.12.03 val PER: 0.4034
2026-01-03 13:15:37,460: t15.2023.12.08 val PER: 0.4075
2026-01-03 13:15:37,460: t15.2023.12.10 val PER: 0.3417
2026-01-03 13:15:37,460: t15.2023.12.17 val PER: 0.4002
2026-01-03 13:15:37,461: t15.2023.12.29 val PER: 0.4036
2026-01-03 13:15:37,461: t15.2024.02.25 val PER: 0.3497
2026-01-03 13:15:37,461: t15.2024.03.08 val PER: 0.5021
2026-01-03 13:15:37,461: t15.2024.03.15 val PER: 0.4422
2026-01-03 13:15:37,461: t15.2024.03.17 val PER: 0.4038
2026-01-03 13:15:37,461: t15.2024.05.10 val PER: 0.4205
2026-01-03 13:15:37,461: t15.2024.06.14 val PER: 0.3943
2026-01-03 13:15:37,461: t15.2024.07.19 val PER: 0.5419
2026-01-03 13:15:37,461: t15.2024.07.21 val PER: 0.3697
2026-01-03 13:15:37,461: t15.2024.07.28 val PER: 0.4162
2026-01-03 13:15:37,461: t15.2025.01.10 val PER: 0.6198
2026-01-03 13:15:37,461: t15.2025.01.12 val PER: 0.4480
2026-01-03 13:15:37,461: t15.2025.03.14 val PER: 0.6479
2026-01-03 13:15:37,461: t15.2025.03.16 val PER: 0.4921
2026-01-03 13:15:37,461: t15.2025.03.30 val PER: 0.6494
2026-01-03 13:15:37,461: t15.2025.04.13 val PER: 0.4964
2026-01-03 13:15:37,463: New best val WER(1gram) 89.59% --> 82.23%
2026-01-03 13:15:37,463: Checkpointing model
2026-01-03 13:15:37,728: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/best_checkpoint
2026-01-03 13:15:37,981: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_1000
2026-01-03 13:15:55,244: Train batch 1200: loss: 33.13 grad norm: 74.66 time: 0.067
2026-01-03 13:16:12,725: Train batch 1400: loss: 36.52 grad norm: 78.26 time: 0.060
2026-01-03 13:16:21,484: Running test after training batch: 1500
2026-01-03 13:16:21,582: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:16:26,418: WER debug example
  GT : you can see the code at this point as well
  PR : yule kint e the code at this boyde is wheel
2026-01-03 13:16:26,449: WER debug example
  GT : how does it keep the cost down
  PR : howled does it heap that cost
2026-01-03 13:16:27,943: Val batch 1500: PER (avg): 0.3790 CTC Loss (avg): 37.1481 WER(1gram): 75.38% (n=64) time: 6.459
2026-01-03 13:16:27,943: WER lens: avg_true_words=6.16 avg_pred_words=5.09 max_pred_words=11
2026-01-03 13:16:27,944: t15.2023.08.13 val PER: 0.3378
2026-01-03 13:16:27,944: t15.2023.08.18 val PER: 0.3135
2026-01-03 13:16:27,944: t15.2023.08.20 val PER: 0.3082
2026-01-03 13:16:27,944: t15.2023.08.25 val PER: 0.2696
2026-01-03 13:16:27,944: t15.2023.08.27 val PER: 0.4051
2026-01-03 13:16:27,944: t15.2023.09.01 val PER: 0.2687
2026-01-03 13:16:27,944: t15.2023.09.03 val PER: 0.3836
2026-01-03 13:16:27,944: t15.2023.09.24 val PER: 0.3034
2026-01-03 13:16:27,944: t15.2023.09.29 val PER: 0.3331
2026-01-03 13:16:27,944: t15.2023.10.01 val PER: 0.3877
2026-01-03 13:16:27,944: t15.2023.10.06 val PER: 0.2939
2026-01-03 13:16:27,944: t15.2023.10.08 val PER: 0.4344
2026-01-03 13:16:27,945: t15.2023.10.13 val PER: 0.4375
2026-01-03 13:16:27,945: t15.2023.10.15 val PER: 0.3612
2026-01-03 13:16:27,945: t15.2023.10.20 val PER: 0.3322
2026-01-03 13:16:27,945: t15.2023.10.22 val PER: 0.3096
2026-01-03 13:16:27,945: t15.2023.11.03 val PER: 0.3745
2026-01-03 13:16:27,945: t15.2023.11.04 val PER: 0.1126
2026-01-03 13:16:27,945: t15.2023.11.17 val PER: 0.2411
2026-01-03 13:16:27,945: t15.2023.11.19 val PER: 0.1816
2026-01-03 13:16:27,945: t15.2023.11.26 val PER: 0.4051
2026-01-03 13:16:27,945: t15.2023.12.03 val PER: 0.3655
2026-01-03 13:16:27,945: t15.2023.12.08 val PER: 0.3575
2026-01-03 13:16:27,945: t15.2023.12.10 val PER: 0.3088
2026-01-03 13:16:27,945: t15.2023.12.17 val PER: 0.3680
2026-01-03 13:16:27,945: t15.2023.12.29 val PER: 0.3782
2026-01-03 13:16:27,945: t15.2024.02.25 val PER: 0.3160
2026-01-03 13:16:27,945: t15.2024.03.08 val PER: 0.4595
2026-01-03 13:16:27,945: t15.2024.03.15 val PER: 0.4153
2026-01-03 13:16:27,945: t15.2024.03.17 val PER: 0.3766
2026-01-03 13:16:27,946: t15.2024.05.10 val PER: 0.3804
2026-01-03 13:16:27,946: t15.2024.06.14 val PER: 0.3833
2026-01-03 13:16:27,946: t15.2024.07.19 val PER: 0.5221
2026-01-03 13:16:27,946: t15.2024.07.21 val PER: 0.3531
2026-01-03 13:16:27,946: t15.2024.07.28 val PER: 0.3632
2026-01-03 13:16:27,946: t15.2025.01.10 val PER: 0.5909
2026-01-03 13:16:27,946: t15.2025.01.12 val PER: 0.4188
2026-01-03 13:16:27,946: t15.2025.03.14 val PER: 0.6036
2026-01-03 13:16:27,946: t15.2025.03.16 val PER: 0.4634
2026-01-03 13:16:27,946: t15.2025.03.30 val PER: 0.6287
2026-01-03 13:16:27,946: t15.2025.04.13 val PER: 0.4665
2026-01-03 13:16:27,947: New best val WER(1gram) 82.23% --> 75.38%
2026-01-03 13:16:27,947: Checkpointing model
2026-01-03 13:16:28,210: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/best_checkpoint
2026-01-03 13:16:28,462: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_1500
2026-01-03 13:16:37,037: Train batch 1600: loss: 36.65 grad norm: 78.36 time: 0.063
2026-01-03 13:16:54,423: Train batch 1800: loss: 35.19 grad norm: 72.18 time: 0.089
2026-01-03 13:17:11,932: Train batch 2000: loss: 33.66 grad norm: 70.36 time: 0.066
2026-01-03 13:17:11,933: Running test after training batch: 2000
2026-01-03 13:17:12,046: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:17:16,829: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this boyde is will
2026-01-03 13:17:16,859: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the us
2026-01-03 13:17:18,406: Val batch 2000: PER (avg): 0.3241 CTC Loss (avg): 32.8441 WER(1gram): 70.56% (n=64) time: 6.473
2026-01-03 13:17:18,407: WER lens: avg_true_words=6.16 avg_pred_words=5.66 max_pred_words=11
2026-01-03 13:17:18,407: t15.2023.08.13 val PER: 0.2973
2026-01-03 13:17:18,407: t15.2023.08.18 val PER: 0.2506
2026-01-03 13:17:18,407: t15.2023.08.20 val PER: 0.2526
2026-01-03 13:17:18,407: t15.2023.08.25 val PER: 0.2259
2026-01-03 13:17:18,407: t15.2023.08.27 val PER: 0.3441
2026-01-03 13:17:18,407: t15.2023.09.01 val PER: 0.2289
2026-01-03 13:17:18,407: t15.2023.09.03 val PER: 0.3171
2026-01-03 13:17:18,407: t15.2023.09.24 val PER: 0.2536
2026-01-03 13:17:18,408: t15.2023.09.29 val PER: 0.2725
2026-01-03 13:17:18,408: t15.2023.10.01 val PER: 0.3250
2026-01-03 13:17:18,408: t15.2023.10.06 val PER: 0.2347
2026-01-03 13:17:18,408: t15.2023.10.08 val PER: 0.4005
2026-01-03 13:17:18,408: t15.2023.10.13 val PER: 0.3716
2026-01-03 13:17:18,408: t15.2023.10.15 val PER: 0.2973
2026-01-03 13:17:18,408: t15.2023.10.20 val PER: 0.2987
2026-01-03 13:17:18,408: t15.2023.10.22 val PER: 0.2650
2026-01-03 13:17:18,408: t15.2023.11.03 val PER: 0.3148
2026-01-03 13:17:18,408: t15.2023.11.04 val PER: 0.1024
2026-01-03 13:17:18,408: t15.2023.11.17 val PER: 0.1695
2026-01-03 13:17:18,408: t15.2023.11.19 val PER: 0.1357
2026-01-03 13:17:18,408: t15.2023.11.26 val PER: 0.3688
2026-01-03 13:17:18,408: t15.2023.12.03 val PER: 0.3130
2026-01-03 13:17:18,408: t15.2023.12.08 val PER: 0.3056
2026-01-03 13:17:18,409: t15.2023.12.10 val PER: 0.2536
2026-01-03 13:17:18,409: t15.2023.12.17 val PER: 0.3139
2026-01-03 13:17:18,409: t15.2023.12.29 val PER: 0.3150
2026-01-03 13:17:18,409: t15.2024.02.25 val PER: 0.2739
2026-01-03 13:17:18,409: t15.2024.03.08 val PER: 0.3912
2026-01-03 13:17:18,409: t15.2024.03.15 val PER: 0.3596
2026-01-03 13:17:18,409: t15.2024.03.17 val PER: 0.3347
2026-01-03 13:17:18,409: t15.2024.05.10 val PER: 0.3388
2026-01-03 13:17:18,409: t15.2024.06.14 val PER: 0.3438
2026-01-03 13:17:18,409: t15.2024.07.19 val PER: 0.4693
2026-01-03 13:17:18,409: t15.2024.07.21 val PER: 0.2945
2026-01-03 13:17:18,409: t15.2024.07.28 val PER: 0.3154
2026-01-03 13:17:18,409: t15.2025.01.10 val PER: 0.5248
2026-01-03 13:17:18,409: t15.2025.01.12 val PER: 0.3718
2026-01-03 13:17:18,409: t15.2025.03.14 val PER: 0.5340
2026-01-03 13:17:18,409: t15.2025.03.16 val PER: 0.3940
2026-01-03 13:17:18,409: t15.2025.03.30 val PER: 0.5402
2026-01-03 13:17:18,410: t15.2025.04.13 val PER: 0.3937
2026-01-03 13:17:18,411: New best val WER(1gram) 75.38% --> 70.56%
2026-01-03 13:17:18,411: Checkpointing model
2026-01-03 13:17:18,678: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/best_checkpoint
2026-01-03 13:17:18,933: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_2000
2026-01-03 13:17:36,000: Train batch 2200: loss: 28.42 grad norm: 72.90 time: 0.059
2026-01-03 13:17:53,351: Train batch 2400: loss: 29.07 grad norm: 62.55 time: 0.052
2026-01-03 13:18:02,065: Running test after training batch: 2500
2026-01-03 13:18:02,212: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:18:07,028: WER debug example
  GT : you can see the code at this point as well
  PR : yule end e the code at this point is will
2026-01-03 13:18:07,056: WER debug example
  GT : how does it keep the cost down
  PR : houde des it eke the us it
2026-01-03 13:18:08,681: Val batch 2500: PER (avg): 0.3052 CTC Loss (avg): 30.1855 WER(1gram): 67.26% (n=64) time: 6.616
2026-01-03 13:18:08,681: WER lens: avg_true_words=6.16 avg_pred_words=5.59 max_pred_words=11
2026-01-03 13:18:08,681: t15.2023.08.13 val PER: 0.2869
2026-01-03 13:18:08,681: t15.2023.08.18 val PER: 0.2448
2026-01-03 13:18:08,682: t15.2023.08.20 val PER: 0.2494
2026-01-03 13:18:08,682: t15.2023.08.25 val PER: 0.2139
2026-01-03 13:18:08,682: t15.2023.08.27 val PER: 0.3232
2026-01-03 13:18:08,682: t15.2023.09.01 val PER: 0.2054
2026-01-03 13:18:08,682: t15.2023.09.03 val PER: 0.2898
2026-01-03 13:18:08,682: t15.2023.09.24 val PER: 0.2318
2026-01-03 13:18:08,682: t15.2023.09.29 val PER: 0.2616
2026-01-03 13:18:08,683: t15.2023.10.01 val PER: 0.3190
2026-01-03 13:18:08,683: t15.2023.10.06 val PER: 0.2078
2026-01-03 13:18:08,683: t15.2023.10.08 val PER: 0.3748
2026-01-03 13:18:08,683: t15.2023.10.13 val PER: 0.3499
2026-01-03 13:18:08,683: t15.2023.10.15 val PER: 0.2914
2026-01-03 13:18:08,683: t15.2023.10.20 val PER: 0.2919
2026-01-03 13:18:08,683: t15.2023.10.22 val PER: 0.2372
2026-01-03 13:18:08,683: t15.2023.11.03 val PER: 0.3019
2026-01-03 13:18:08,683: t15.2023.11.04 val PER: 0.0887
2026-01-03 13:18:08,684: t15.2023.11.17 val PER: 0.1431
2026-01-03 13:18:08,684: t15.2023.11.19 val PER: 0.1257
2026-01-03 13:18:08,684: t15.2023.11.26 val PER: 0.3384
2026-01-03 13:18:08,684: t15.2023.12.03 val PER: 0.2931
2026-01-03 13:18:08,684: t15.2023.12.08 val PER: 0.2790
2026-01-03 13:18:08,684: t15.2023.12.10 val PER: 0.2405
2026-01-03 13:18:08,684: t15.2023.12.17 val PER: 0.2890
2026-01-03 13:18:08,684: t15.2023.12.29 val PER: 0.3020
2026-01-03 13:18:08,684: t15.2024.02.25 val PER: 0.2388
2026-01-03 13:18:08,685: t15.2024.03.08 val PER: 0.3713
2026-01-03 13:18:08,685: t15.2024.03.15 val PER: 0.3527
2026-01-03 13:18:08,685: t15.2024.03.17 val PER: 0.3124
2026-01-03 13:18:08,685: t15.2024.05.10 val PER: 0.3135
2026-01-03 13:18:08,685: t15.2024.06.14 val PER: 0.3076
2026-01-03 13:18:08,685: t15.2024.07.19 val PER: 0.4469
2026-01-03 13:18:08,685: t15.2024.07.21 val PER: 0.2676
2026-01-03 13:18:08,685: t15.2024.07.28 val PER: 0.3007
2026-01-03 13:18:08,686: t15.2025.01.10 val PER: 0.4945
2026-01-03 13:18:08,686: t15.2025.01.12 val PER: 0.3564
2026-01-03 13:18:08,686: t15.2025.03.14 val PER: 0.4911
2026-01-03 13:18:08,686: t15.2025.03.16 val PER: 0.3770
2026-01-03 13:18:08,686: t15.2025.03.30 val PER: 0.5092
2026-01-03 13:18:08,686: t15.2025.04.13 val PER: 0.3766
2026-01-03 13:18:08,686: New best val WER(1gram) 70.56% --> 67.26%
2026-01-03 13:18:08,686: Checkpointing model
2026-01-03 13:18:08,955: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/best_checkpoint
2026-01-03 13:18:09,210: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_2500
2026-01-03 13:18:17,697: Train batch 2600: loss: 35.36 grad norm: 83.72 time: 0.054
2026-01-03 13:18:34,843: Train batch 2800: loss: 25.94 grad norm: 74.10 time: 0.080
2026-01-03 13:18:51,941: Train batch 3000: loss: 31.06 grad norm: 75.89 time: 0.083
2026-01-03 13:18:51,942: Running test after training batch: 3000
2026-01-03 13:18:52,047: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:18:56,811: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the owed at this point is will
2026-01-03 13:18:56,840: WER debug example
  GT : how does it keep the cost down
  PR : houde des it yip the west get
2026-01-03 13:18:58,465: Val batch 3000: PER (avg): 0.2827 CTC Loss (avg): 28.0924 WER(1gram): 66.24% (n=64) time: 6.524
2026-01-03 13:18:58,466: WER lens: avg_true_words=6.16 avg_pred_words=5.88 max_pred_words=11
2026-01-03 13:18:58,466: t15.2023.08.13 val PER: 0.2568
2026-01-03 13:18:58,466: t15.2023.08.18 val PER: 0.2205
2026-01-03 13:18:58,466: t15.2023.08.20 val PER: 0.2160
2026-01-03 13:18:58,466: t15.2023.08.25 val PER: 0.1958
2026-01-03 13:18:58,466: t15.2023.08.27 val PER: 0.2974
2026-01-03 13:18:58,466: t15.2023.09.01 val PER: 0.1907
2026-01-03 13:18:58,466: t15.2023.09.03 val PER: 0.2850
2026-01-03 13:18:58,466: t15.2023.09.24 val PER: 0.2282
2026-01-03 13:18:58,467: t15.2023.09.29 val PER: 0.2374
2026-01-03 13:18:58,467: t15.2023.10.01 val PER: 0.2939
2026-01-03 13:18:58,467: t15.2023.10.06 val PER: 0.2034
2026-01-03 13:18:58,467: t15.2023.10.08 val PER: 0.3613
2026-01-03 13:18:58,467: t15.2023.10.13 val PER: 0.3375
2026-01-03 13:18:58,467: t15.2023.10.15 val PER: 0.2709
2026-01-03 13:18:58,467: t15.2023.10.20 val PER: 0.2517
2026-01-03 13:18:58,467: t15.2023.10.22 val PER: 0.2194
2026-01-03 13:18:58,467: t15.2023.11.03 val PER: 0.2693
2026-01-03 13:18:58,467: t15.2023.11.04 val PER: 0.0887
2026-01-03 13:18:58,467: t15.2023.11.17 val PER: 0.1291
2026-01-03 13:18:58,467: t15.2023.11.19 val PER: 0.1098
2026-01-03 13:18:58,467: t15.2023.11.26 val PER: 0.3036
2026-01-03 13:18:58,467: t15.2023.12.03 val PER: 0.2647
2026-01-03 13:18:58,468: t15.2023.12.08 val PER: 0.2610
2026-01-03 13:18:58,468: t15.2023.12.10 val PER: 0.2037
2026-01-03 13:18:58,468: t15.2023.12.17 val PER: 0.2796
2026-01-03 13:18:58,468: t15.2023.12.29 val PER: 0.2780
2026-01-03 13:18:58,468: t15.2024.02.25 val PER: 0.2458
2026-01-03 13:18:58,468: t15.2024.03.08 val PER: 0.3556
2026-01-03 13:18:58,468: t15.2024.03.15 val PER: 0.3321
2026-01-03 13:18:58,468: t15.2024.03.17 val PER: 0.2957
2026-01-03 13:18:58,468: t15.2024.05.10 val PER: 0.2972
2026-01-03 13:18:58,468: t15.2024.06.14 val PER: 0.2902
2026-01-03 13:18:58,468: t15.2024.07.19 val PER: 0.4021
2026-01-03 13:18:58,468: t15.2024.07.21 val PER: 0.2297
2026-01-03 13:18:58,468: t15.2024.07.28 val PER: 0.2846
2026-01-03 13:18:58,468: t15.2025.01.10 val PER: 0.4931
2026-01-03 13:18:58,468: t15.2025.01.12 val PER: 0.3233
2026-01-03 13:18:58,468: t15.2025.03.14 val PER: 0.4512
2026-01-03 13:18:58,469: t15.2025.03.16 val PER: 0.3207
2026-01-03 13:18:58,469: t15.2025.03.30 val PER: 0.4862
2026-01-03 13:18:58,469: t15.2025.04.13 val PER: 0.3609
2026-01-03 13:18:58,470: New best val WER(1gram) 67.26% --> 66.24%
2026-01-03 13:18:58,470: Checkpointing model
2026-01-03 13:18:58,735: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/best_checkpoint
2026-01-03 13:18:58,989: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_3000
2026-01-03 13:19:15,936: Train batch 3200: loss: 26.72 grad norm: 67.42 time: 0.076
2026-01-03 13:19:32,887: Train batch 3400: loss: 18.30 grad norm: 53.29 time: 0.048
2026-01-03 13:19:41,464: Running test after training batch: 3500
2026-01-03 13:19:41,590: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:19:46,334: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point will
2026-01-03 13:19:46,361: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it epp thus cussed et
2026-01-03 13:19:47,918: Val batch 3500: PER (avg): 0.2667 CTC Loss (avg): 26.5346 WER(1gram): 66.50% (n=64) time: 6.454
2026-01-03 13:19:47,919: WER lens: avg_true_words=6.16 avg_pred_words=5.88 max_pred_words=11
2026-01-03 13:19:47,919: t15.2023.08.13 val PER: 0.2391
2026-01-03 13:19:47,919: t15.2023.08.18 val PER: 0.2070
2026-01-03 13:19:47,919: t15.2023.08.20 val PER: 0.2033
2026-01-03 13:19:47,919: t15.2023.08.25 val PER: 0.1732
2026-01-03 13:19:47,919: t15.2023.08.27 val PER: 0.2669
2026-01-03 13:19:47,919: t15.2023.09.01 val PER: 0.1867
2026-01-03 13:19:47,919: t15.2023.09.03 val PER: 0.2589
2026-01-03 13:19:47,919: t15.2023.09.24 val PER: 0.2002
2026-01-03 13:19:47,919: t15.2023.09.29 val PER: 0.2221
2026-01-03 13:19:47,920: t15.2023.10.01 val PER: 0.2774
2026-01-03 13:19:47,920: t15.2023.10.06 val PER: 0.1895
2026-01-03 13:19:47,920: t15.2023.10.08 val PER: 0.3545
2026-01-03 13:19:47,920: t15.2023.10.13 val PER: 0.3212
2026-01-03 13:19:47,920: t15.2023.10.15 val PER: 0.2479
2026-01-03 13:19:47,920: t15.2023.10.20 val PER: 0.2282
2026-01-03 13:19:47,920: t15.2023.10.22 val PER: 0.2082
2026-01-03 13:19:47,920: t15.2023.11.03 val PER: 0.2612
2026-01-03 13:19:47,920: t15.2023.11.04 val PER: 0.0717
2026-01-03 13:19:47,920: t15.2023.11.17 val PER: 0.1104
2026-01-03 13:19:47,920: t15.2023.11.19 val PER: 0.1058
2026-01-03 13:19:47,920: t15.2023.11.26 val PER: 0.2841
2026-01-03 13:19:47,920: t15.2023.12.03 val PER: 0.2374
2026-01-03 13:19:47,920: t15.2023.12.08 val PER: 0.2517
2026-01-03 13:19:47,920: t15.2023.12.10 val PER: 0.2011
2026-01-03 13:19:47,921: t15.2023.12.17 val PER: 0.2557
2026-01-03 13:19:47,921: t15.2023.12.29 val PER: 0.2608
2026-01-03 13:19:47,921: t15.2024.02.25 val PER: 0.2107
2026-01-03 13:19:47,921: t15.2024.03.08 val PER: 0.3400
2026-01-03 13:19:47,921: t15.2024.03.15 val PER: 0.3215
2026-01-03 13:19:47,921: t15.2024.03.17 val PER: 0.2782
2026-01-03 13:19:47,921: t15.2024.05.10 val PER: 0.2689
2026-01-03 13:19:47,921: t15.2024.06.14 val PER: 0.2855
2026-01-03 13:19:47,922: t15.2024.07.19 val PER: 0.3955
2026-01-03 13:19:47,922: t15.2024.07.21 val PER: 0.2228
2026-01-03 13:19:47,922: t15.2024.07.28 val PER: 0.2772
2026-01-03 13:19:47,922: t15.2025.01.10 val PER: 0.4545
2026-01-03 13:19:47,922: t15.2025.01.12 val PER: 0.2979
2026-01-03 13:19:47,922: t15.2025.03.14 val PER: 0.4231
2026-01-03 13:19:47,922: t15.2025.03.16 val PER: 0.3390
2026-01-03 13:19:47,922: t15.2025.03.30 val PER: 0.4575
2026-01-03 13:19:47,922: t15.2025.04.13 val PER: 0.3324
2026-01-03 13:19:48,165: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_3500
2026-01-03 13:19:57,041: Train batch 3600: loss: 23.13 grad norm: 67.85 time: 0.067
2026-01-03 13:20:14,121: Train batch 3800: loss: 25.62 grad norm: 67.20 time: 0.066
2026-01-03 13:20:31,211: Train batch 4000: loss: 19.58 grad norm: 53.06 time: 0.056
2026-01-03 13:20:31,212: Running test after training batch: 4000
2026-01-03 13:20:31,365: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:20:36,226: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point while
2026-01-03 13:20:36,254: WER debug example
  GT : how does it keep the cost down
  PR : aue dust it kipp the cussed nett
2026-01-03 13:20:37,849: Val batch 4000: PER (avg): 0.2489 CTC Loss (avg): 24.4340 WER(1gram): 65.23% (n=64) time: 6.637
2026-01-03 13:20:37,849: WER lens: avg_true_words=6.16 avg_pred_words=6.00 max_pred_words=11
2026-01-03 13:20:37,849: t15.2023.08.13 val PER: 0.2360
2026-01-03 13:20:37,849: t15.2023.08.18 val PER: 0.1995
2026-01-03 13:20:37,849: t15.2023.08.20 val PER: 0.1994
2026-01-03 13:20:37,849: t15.2023.08.25 val PER: 0.1747
2026-01-03 13:20:37,849: t15.2023.08.27 val PER: 0.2765
2026-01-03 13:20:37,850: t15.2023.09.01 val PER: 0.1769
2026-01-03 13:20:37,850: t15.2023.09.03 val PER: 0.2506
2026-01-03 13:20:37,850: t15.2023.09.24 val PER: 0.1990
2026-01-03 13:20:37,850: t15.2023.09.29 val PER: 0.2010
2026-01-03 13:20:37,850: t15.2023.10.01 val PER: 0.2550
2026-01-03 13:20:37,850: t15.2023.10.06 val PER: 0.1593
2026-01-03 13:20:37,850: t15.2023.10.08 val PER: 0.3166
2026-01-03 13:20:37,850: t15.2023.10.13 val PER: 0.3080
2026-01-03 13:20:37,850: t15.2023.10.15 val PER: 0.2294
2026-01-03 13:20:37,850: t15.2023.10.20 val PER: 0.2416
2026-01-03 13:20:37,850: t15.2023.10.22 val PER: 0.2004
2026-01-03 13:20:37,850: t15.2023.11.03 val PER: 0.2422
2026-01-03 13:20:37,850: t15.2023.11.04 val PER: 0.0751
2026-01-03 13:20:37,850: t15.2023.11.17 val PER: 0.0995
2026-01-03 13:20:37,851: t15.2023.11.19 val PER: 0.1058
2026-01-03 13:20:37,851: t15.2023.11.26 val PER: 0.2601
2026-01-03 13:20:37,851: t15.2023.12.03 val PER: 0.2185
2026-01-03 13:20:37,851: t15.2023.12.08 val PER: 0.2204
2026-01-03 13:20:37,851: t15.2023.12.10 val PER: 0.1892
2026-01-03 13:20:37,851: t15.2023.12.17 val PER: 0.2360
2026-01-03 13:20:37,851: t15.2023.12.29 val PER: 0.2512
2026-01-03 13:20:37,851: t15.2024.02.25 val PER: 0.2079
2026-01-03 13:20:37,851: t15.2024.03.08 val PER: 0.3314
2026-01-03 13:20:37,851: t15.2024.03.15 val PER: 0.2971
2026-01-03 13:20:37,851: t15.2024.03.17 val PER: 0.2622
2026-01-03 13:20:37,852: t15.2024.05.10 val PER: 0.2615
2026-01-03 13:20:37,852: t15.2024.06.14 val PER: 0.2571
2026-01-03 13:20:37,852: t15.2024.07.19 val PER: 0.3652
2026-01-03 13:20:37,852: t15.2024.07.21 val PER: 0.1869
2026-01-03 13:20:37,852: t15.2024.07.28 val PER: 0.2434
2026-01-03 13:20:37,852: t15.2025.01.10 val PER: 0.4174
2026-01-03 13:20:37,852: t15.2025.01.12 val PER: 0.2802
2026-01-03 13:20:37,852: t15.2025.03.14 val PER: 0.4127
2026-01-03 13:20:37,852: t15.2025.03.16 val PER: 0.2997
2026-01-03 13:20:37,852: t15.2025.03.30 val PER: 0.4172
2026-01-03 13:20:37,852: t15.2025.04.13 val PER: 0.3252
2026-01-03 13:20:37,853: New best val WER(1gram) 66.24% --> 65.23%
2026-01-03 13:20:37,854: Checkpointing model
2026-01-03 13:20:38,118: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/best_checkpoint
2026-01-03 13:20:38,372: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_4000
2026-01-03 13:20:55,540: Train batch 4200: loss: 22.48 grad norm: 60.29 time: 0.080
2026-01-03 13:21:12,904: Train batch 4400: loss: 16.46 grad norm: 50.56 time: 0.066
2026-01-03 13:21:21,504: Running test after training batch: 4500
2026-01-03 13:21:21,630: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:21:26,494: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is wheel
2026-01-03 13:21:26,524: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap thus cost get
2026-01-03 13:21:28,095: Val batch 4500: PER (avg): 0.2388 CTC Loss (avg): 23.3029 WER(1gram): 61.93% (n=64) time: 6.591
2026-01-03 13:21:28,096: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 13:21:28,096: t15.2023.08.13 val PER: 0.2058
2026-01-03 13:21:28,096: t15.2023.08.18 val PER: 0.1903
2026-01-03 13:21:28,096: t15.2023.08.20 val PER: 0.1851
2026-01-03 13:21:28,096: t15.2023.08.25 val PER: 0.1461
2026-01-03 13:21:28,096: t15.2023.08.27 val PER: 0.2524
2026-01-03 13:21:28,096: t15.2023.09.01 val PER: 0.1583
2026-01-03 13:21:28,097: t15.2023.09.03 val PER: 0.2506
2026-01-03 13:21:28,097: t15.2023.09.24 val PER: 0.1784
2026-01-03 13:21:28,097: t15.2023.09.29 val PER: 0.2080
2026-01-03 13:21:28,097: t15.2023.10.01 val PER: 0.2609
2026-01-03 13:21:28,097: t15.2023.10.06 val PER: 0.1561
2026-01-03 13:21:28,097: t15.2023.10.08 val PER: 0.3329
2026-01-03 13:21:28,097: t15.2023.10.13 val PER: 0.3026
2026-01-03 13:21:28,097: t15.2023.10.15 val PER: 0.2320
2026-01-03 13:21:28,097: t15.2023.10.20 val PER: 0.2282
2026-01-03 13:21:28,097: t15.2023.10.22 val PER: 0.1826
2026-01-03 13:21:28,097: t15.2023.11.03 val PER: 0.2415
2026-01-03 13:21:28,097: t15.2023.11.04 val PER: 0.0478
2026-01-03 13:21:28,098: t15.2023.11.17 val PER: 0.0886
2026-01-03 13:21:28,098: t15.2023.11.19 val PER: 0.0958
2026-01-03 13:21:28,098: t15.2023.11.26 val PER: 0.2732
2026-01-03 13:21:28,098: t15.2023.12.03 val PER: 0.2080
2026-01-03 13:21:28,098: t15.2023.12.08 val PER: 0.2031
2026-01-03 13:21:28,098: t15.2023.12.10 val PER: 0.1721
2026-01-03 13:21:28,098: t15.2023.12.17 val PER: 0.2297
2026-01-03 13:21:28,098: t15.2023.12.29 val PER: 0.2485
2026-01-03 13:21:28,098: t15.2024.02.25 val PER: 0.2065
2026-01-03 13:21:28,098: t15.2024.03.08 val PER: 0.3001
2026-01-03 13:21:28,098: t15.2024.03.15 val PER: 0.2964
2026-01-03 13:21:28,098: t15.2024.03.17 val PER: 0.2427
2026-01-03 13:21:28,098: t15.2024.05.10 val PER: 0.2600
2026-01-03 13:21:28,098: t15.2024.06.14 val PER: 0.2492
2026-01-03 13:21:28,098: t15.2024.07.19 val PER: 0.3336
2026-01-03 13:21:28,099: t15.2024.07.21 val PER: 0.1710
2026-01-03 13:21:28,099: t15.2024.07.28 val PER: 0.2272
2026-01-03 13:21:28,099: t15.2025.01.10 val PER: 0.4077
2026-01-03 13:21:28,099: t15.2025.01.12 val PER: 0.2640
2026-01-03 13:21:28,099: t15.2025.03.14 val PER: 0.4009
2026-01-03 13:21:28,099: t15.2025.03.16 val PER: 0.2971
2026-01-03 13:21:28,099: t15.2025.03.30 val PER: 0.4011
2026-01-03 13:21:28,099: t15.2025.04.13 val PER: 0.2910
2026-01-03 13:21:28,100: New best val WER(1gram) 65.23% --> 61.93%
2026-01-03 13:21:28,100: Checkpointing model
2026-01-03 13:21:28,364: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/best_checkpoint
2026-01-03 13:21:28,617: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_4500
2026-01-03 13:21:37,210: Train batch 4600: loss: 20.85 grad norm: 62.02 time: 0.062
2026-01-03 13:21:54,483: Train batch 4800: loss: 14.21 grad norm: 52.52 time: 0.064
2026-01-03 13:22:11,721: Train batch 5000: loss: 32.09 grad norm: 80.00 time: 0.064
2026-01-03 13:22:11,721: Running test after training batch: 5000
2026-01-03 13:22:11,840: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:22:16,604: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point as will
2026-01-03 13:22:16,632: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the cost get
2026-01-03 13:22:18,195: Val batch 5000: PER (avg): 0.2271 CTC Loss (avg): 22.3479 WER(1gram): 60.91% (n=64) time: 6.474
2026-01-03 13:22:18,196: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-03 13:22:18,196: t15.2023.08.13 val PER: 0.1965
2026-01-03 13:22:18,196: t15.2023.08.18 val PER: 0.1668
2026-01-03 13:22:18,196: t15.2023.08.20 val PER: 0.1763
2026-01-03 13:22:18,196: t15.2023.08.25 val PER: 0.1235
2026-01-03 13:22:18,196: t15.2023.08.27 val PER: 0.2331
2026-01-03 13:22:18,196: t15.2023.09.01 val PER: 0.1388
2026-01-03 13:22:18,196: t15.2023.09.03 val PER: 0.2340
2026-01-03 13:22:18,196: t15.2023.09.24 val PER: 0.1978
2026-01-03 13:22:18,197: t15.2023.09.29 val PER: 0.1793
2026-01-03 13:22:18,197: t15.2023.10.01 val PER: 0.2470
2026-01-03 13:22:18,197: t15.2023.10.06 val PER: 0.1529
2026-01-03 13:22:18,197: t15.2023.10.08 val PER: 0.3072
2026-01-03 13:22:18,197: t15.2023.10.13 val PER: 0.2808
2026-01-03 13:22:18,197: t15.2023.10.15 val PER: 0.2261
2026-01-03 13:22:18,197: t15.2023.10.20 val PER: 0.2450
2026-01-03 13:22:18,197: t15.2023.10.22 val PER: 0.1648
2026-01-03 13:22:18,197: t15.2023.11.03 val PER: 0.2259
2026-01-03 13:22:18,197: t15.2023.11.04 val PER: 0.0580
2026-01-03 13:22:18,197: t15.2023.11.17 val PER: 0.0840
2026-01-03 13:22:18,197: t15.2023.11.19 val PER: 0.0778
2026-01-03 13:22:18,197: t15.2023.11.26 val PER: 0.2486
2026-01-03 13:22:18,197: t15.2023.12.03 val PER: 0.1985
2026-01-03 13:22:18,197: t15.2023.12.08 val PER: 0.1957
2026-01-03 13:22:18,197: t15.2023.12.10 val PER: 0.1524
2026-01-03 13:22:18,198: t15.2023.12.17 val PER: 0.2266
2026-01-03 13:22:18,198: t15.2023.12.29 val PER: 0.2244
2026-01-03 13:22:18,198: t15.2024.02.25 val PER: 0.1826
2026-01-03 13:22:18,198: t15.2024.03.08 val PER: 0.3186
2026-01-03 13:22:18,198: t15.2024.03.15 val PER: 0.2789
2026-01-03 13:22:18,198: t15.2024.03.17 val PER: 0.2301
2026-01-03 13:22:18,198: t15.2024.05.10 val PER: 0.2467
2026-01-03 13:22:18,198: t15.2024.06.14 val PER: 0.2445
2026-01-03 13:22:18,198: t15.2024.07.19 val PER: 0.3342
2026-01-03 13:22:18,198: t15.2024.07.21 val PER: 0.1841
2026-01-03 13:22:18,198: t15.2024.07.28 val PER: 0.2088
2026-01-03 13:22:18,198: t15.2025.01.10 val PER: 0.4008
2026-01-03 13:22:18,198: t15.2025.01.12 val PER: 0.2479
2026-01-03 13:22:18,198: t15.2025.03.14 val PER: 0.4038
2026-01-03 13:22:18,199: t15.2025.03.16 val PER: 0.2670
2026-01-03 13:22:18,199: t15.2025.03.30 val PER: 0.3920
2026-01-03 13:22:18,199: t15.2025.04.13 val PER: 0.3024
2026-01-03 13:22:18,200: New best val WER(1gram) 61.93% --> 60.91%
2026-01-03 13:22:18,200: Checkpointing model
2026-01-03 13:22:18,464: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/best_checkpoint
2026-01-03 13:22:18,717: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_5000
2026-01-03 13:22:36,167: Train batch 5200: loss: 16.32 grad norm: 57.79 time: 0.052
2026-01-03 13:22:53,857: Train batch 5400: loss: 17.77 grad norm: 62.20 time: 0.067
2026-01-03 13:23:02,422: Running test after training batch: 5500
2026-01-03 13:23:02,581: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:23:07,555: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point will
2026-01-03 13:23:07,583: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost nett
2026-01-03 13:23:09,159: Val batch 5500: PER (avg): 0.2170 CTC Loss (avg): 21.1959 WER(1gram): 58.12% (n=64) time: 6.737
2026-01-03 13:23:09,160: WER lens: avg_true_words=6.16 avg_pred_words=6.02 max_pred_words=10
2026-01-03 13:23:09,160: t15.2023.08.13 val PER: 0.1933
2026-01-03 13:23:09,160: t15.2023.08.18 val PER: 0.1643
2026-01-03 13:23:09,160: t15.2023.08.20 val PER: 0.1755
2026-01-03 13:23:09,160: t15.2023.08.25 val PER: 0.1235
2026-01-03 13:23:09,160: t15.2023.08.27 val PER: 0.2347
2026-01-03 13:23:09,160: t15.2023.09.01 val PER: 0.1331
2026-01-03 13:23:09,160: t15.2023.09.03 val PER: 0.2197
2026-01-03 13:23:09,160: t15.2023.09.24 val PER: 0.1917
2026-01-03 13:23:09,160: t15.2023.09.29 val PER: 0.1729
2026-01-03 13:23:09,160: t15.2023.10.01 val PER: 0.2351
2026-01-03 13:23:09,161: t15.2023.10.06 val PER: 0.1346
2026-01-03 13:23:09,161: t15.2023.10.08 val PER: 0.2936
2026-01-03 13:23:09,161: t15.2023.10.13 val PER: 0.2762
2026-01-03 13:23:09,161: t15.2023.10.15 val PER: 0.2096
2026-01-03 13:23:09,161: t15.2023.10.20 val PER: 0.2215
2026-01-03 13:23:09,161: t15.2023.10.22 val PER: 0.1648
2026-01-03 13:23:09,161: t15.2023.11.03 val PER: 0.2232
2026-01-03 13:23:09,161: t15.2023.11.04 val PER: 0.0580
2026-01-03 13:23:09,161: t15.2023.11.17 val PER: 0.0778
2026-01-03 13:23:09,161: t15.2023.11.19 val PER: 0.0778
2026-01-03 13:23:09,161: t15.2023.11.26 val PER: 0.2239
2026-01-03 13:23:09,161: t15.2023.12.03 val PER: 0.1859
2026-01-03 13:23:09,161: t15.2023.12.08 val PER: 0.1811
2026-01-03 13:23:09,161: t15.2023.12.10 val PER: 0.1577
2026-01-03 13:23:09,161: t15.2023.12.17 val PER: 0.2183
2026-01-03 13:23:09,161: t15.2023.12.29 val PER: 0.2114
2026-01-03 13:23:09,162: t15.2024.02.25 val PER: 0.1812
2026-01-03 13:23:09,162: t15.2024.03.08 val PER: 0.2888
2026-01-03 13:23:09,162: t15.2024.03.15 val PER: 0.2645
2026-01-03 13:23:09,162: t15.2024.03.17 val PER: 0.2211
2026-01-03 13:23:09,162: t15.2024.05.10 val PER: 0.2318
2026-01-03 13:23:09,162: t15.2024.06.14 val PER: 0.2366
2026-01-03 13:23:09,162: t15.2024.07.19 val PER: 0.3223
2026-01-03 13:23:09,162: t15.2024.07.21 val PER: 0.1683
2026-01-03 13:23:09,162: t15.2024.07.28 val PER: 0.2125
2026-01-03 13:23:09,162: t15.2025.01.10 val PER: 0.3926
2026-01-03 13:23:09,162: t15.2025.01.12 val PER: 0.2394
2026-01-03 13:23:09,162: t15.2025.03.14 val PER: 0.3669
2026-01-03 13:23:09,163: t15.2025.03.16 val PER: 0.2539
2026-01-03 13:23:09,163: t15.2025.03.30 val PER: 0.3586
2026-01-03 13:23:09,163: t15.2025.04.13 val PER: 0.2967
2026-01-03 13:23:09,163: New best val WER(1gram) 60.91% --> 58.12%
2026-01-03 13:23:09,163: Checkpointing model
2026-01-03 13:23:09,429: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/best_checkpoint
2026-01-03 13:23:09,683: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_5500
2026-01-03 13:23:18,484: Train batch 5600: loss: 19.32 grad norm: 67.16 time: 0.062
2026-01-03 13:23:35,773: Train batch 5800: loss: 13.93 grad norm: 55.75 time: 0.082
2026-01-03 13:23:53,147: Train batch 6000: loss: 14.27 grad norm: 54.47 time: 0.049
2026-01-03 13:23:53,148: Running test after training batch: 6000
2026-01-03 13:23:53,255: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:23:58,056: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the good at this point is will
2026-01-03 13:23:58,086: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:23:59,700: Val batch 6000: PER (avg): 0.2129 CTC Loss (avg): 21.0154 WER(1gram): 59.14% (n=64) time: 6.552
2026-01-03 13:23:59,701: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 13:23:59,701: t15.2023.08.13 val PER: 0.1819
2026-01-03 13:23:59,701: t15.2023.08.18 val PER: 0.1676
2026-01-03 13:23:59,701: t15.2023.08.20 val PER: 0.1708
2026-01-03 13:23:59,701: t15.2023.08.25 val PER: 0.1295
2026-01-03 13:23:59,701: t15.2023.08.27 val PER: 0.2492
2026-01-03 13:23:59,701: t15.2023.09.01 val PER: 0.1266
2026-01-03 13:23:59,701: t15.2023.09.03 val PER: 0.2126
2026-01-03 13:23:59,701: t15.2023.09.24 val PER: 0.1699
2026-01-03 13:23:59,701: t15.2023.09.29 val PER: 0.1793
2026-01-03 13:23:59,701: t15.2023.10.01 val PER: 0.2305
2026-01-03 13:23:59,702: t15.2023.10.06 val PER: 0.1335
2026-01-03 13:23:59,702: t15.2023.10.08 val PER: 0.2842
2026-01-03 13:23:59,702: t15.2023.10.13 val PER: 0.2669
2026-01-03 13:23:59,702: t15.2023.10.15 val PER: 0.2182
2026-01-03 13:23:59,702: t15.2023.10.20 val PER: 0.2047
2026-01-03 13:23:59,702: t15.2023.10.22 val PER: 0.1670
2026-01-03 13:23:59,702: t15.2023.11.03 val PER: 0.2280
2026-01-03 13:23:59,702: t15.2023.11.04 val PER: 0.0512
2026-01-03 13:23:59,702: t15.2023.11.17 val PER: 0.0886
2026-01-03 13:23:59,702: t15.2023.11.19 val PER: 0.0778
2026-01-03 13:23:59,702: t15.2023.11.26 val PER: 0.2210
2026-01-03 13:23:59,702: t15.2023.12.03 val PER: 0.1796
2026-01-03 13:23:59,702: t15.2023.12.08 val PER: 0.1791
2026-01-03 13:23:59,702: t15.2023.12.10 val PER: 0.1524
2026-01-03 13:23:59,702: t15.2023.12.17 val PER: 0.1923
2026-01-03 13:23:59,703: t15.2023.12.29 val PER: 0.2189
2026-01-03 13:23:59,703: t15.2024.02.25 val PER: 0.1629
2026-01-03 13:23:59,703: t15.2024.03.08 val PER: 0.2745
2026-01-03 13:23:59,703: t15.2024.03.15 val PER: 0.2745
2026-01-03 13:23:59,703: t15.2024.03.17 val PER: 0.2113
2026-01-03 13:23:59,703: t15.2024.05.10 val PER: 0.2110
2026-01-03 13:23:59,703: t15.2024.06.14 val PER: 0.2161
2026-01-03 13:23:59,703: t15.2024.07.19 val PER: 0.3059
2026-01-03 13:23:59,703: t15.2024.07.21 val PER: 0.1662
2026-01-03 13:23:59,703: t15.2024.07.28 val PER: 0.2029
2026-01-03 13:23:59,703: t15.2025.01.10 val PER: 0.3760
2026-01-03 13:23:59,703: t15.2025.01.12 val PER: 0.2248
2026-01-03 13:23:59,703: t15.2025.03.14 val PER: 0.3861
2026-01-03 13:23:59,703: t15.2025.03.16 val PER: 0.2565
2026-01-03 13:23:59,703: t15.2025.03.30 val PER: 0.3667
2026-01-03 13:23:59,703: t15.2025.04.13 val PER: 0.2753
2026-01-03 13:23:59,947: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_6000
2026-01-03 13:24:17,021: Train batch 6200: loss: 16.56 grad norm: 58.49 time: 0.070
2026-01-03 13:24:33,775: Train batch 6400: loss: 19.81 grad norm: 69.69 time: 0.062
2026-01-03 13:24:42,072: Running test after training batch: 6500
2026-01-03 13:24:42,258: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:24:47,016: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:24:47,044: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 13:24:48,602: Val batch 6500: PER (avg): 0.2049 CTC Loss (avg): 20.1692 WER(1gram): 54.31% (n=64) time: 6.529
2026-01-03 13:24:48,602: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 13:24:48,602: t15.2023.08.13 val PER: 0.1861
2026-01-03 13:24:48,602: t15.2023.08.18 val PER: 0.1475
2026-01-03 13:24:48,603: t15.2023.08.20 val PER: 0.1597
2026-01-03 13:24:48,603: t15.2023.08.25 val PER: 0.1130
2026-01-03 13:24:48,603: t15.2023.08.27 val PER: 0.2219
2026-01-03 13:24:48,603: t15.2023.09.01 val PER: 0.1226
2026-01-03 13:24:48,603: t15.2023.09.03 val PER: 0.2114
2026-01-03 13:24:48,603: t15.2023.09.24 val PER: 0.1626
2026-01-03 13:24:48,603: t15.2023.09.29 val PER: 0.1685
2026-01-03 13:24:48,603: t15.2023.10.01 val PER: 0.2259
2026-01-03 13:24:48,603: t15.2023.10.06 val PER: 0.1270
2026-01-03 13:24:48,603: t15.2023.10.08 val PER: 0.3045
2026-01-03 13:24:48,603: t15.2023.10.13 val PER: 0.2715
2026-01-03 13:24:48,603: t15.2023.10.15 val PER: 0.2116
2026-01-03 13:24:48,603: t15.2023.10.20 val PER: 0.2248
2026-01-03 13:24:48,603: t15.2023.10.22 val PER: 0.1548
2026-01-03 13:24:48,603: t15.2023.11.03 val PER: 0.2205
2026-01-03 13:24:48,604: t15.2023.11.04 val PER: 0.0546
2026-01-03 13:24:48,604: t15.2023.11.17 val PER: 0.0684
2026-01-03 13:24:48,604: t15.2023.11.19 val PER: 0.0758
2026-01-03 13:24:48,604: t15.2023.11.26 val PER: 0.2087
2026-01-03 13:24:48,604: t15.2023.12.03 val PER: 0.1796
2026-01-03 13:24:48,604: t15.2023.12.08 val PER: 0.1678
2026-01-03 13:24:48,604: t15.2023.12.10 val PER: 0.1406
2026-01-03 13:24:48,604: t15.2023.12.17 val PER: 0.1923
2026-01-03 13:24:48,604: t15.2023.12.29 val PER: 0.2052
2026-01-03 13:24:48,604: t15.2024.02.25 val PER: 0.1713
2026-01-03 13:24:48,604: t15.2024.03.08 val PER: 0.2945
2026-01-03 13:24:48,604: t15.2024.03.15 val PER: 0.2527
2026-01-03 13:24:48,604: t15.2024.03.17 val PER: 0.1974
2026-01-03 13:24:48,604: t15.2024.05.10 val PER: 0.2229
2026-01-03 13:24:48,604: t15.2024.06.14 val PER: 0.2145
2026-01-03 13:24:48,605: t15.2024.07.19 val PER: 0.3105
2026-01-03 13:24:48,605: t15.2024.07.21 val PER: 0.1538
2026-01-03 13:24:48,605: t15.2024.07.28 val PER: 0.1904
2026-01-03 13:24:48,605: t15.2025.01.10 val PER: 0.3567
2026-01-03 13:24:48,605: t15.2025.01.12 val PER: 0.2094
2026-01-03 13:24:48,605: t15.2025.03.14 val PER: 0.3757
2026-01-03 13:24:48,605: t15.2025.03.16 val PER: 0.2264
2026-01-03 13:24:48,605: t15.2025.03.30 val PER: 0.3471
2026-01-03 13:24:48,605: t15.2025.04.13 val PER: 0.2625
2026-01-03 13:24:48,606: New best val WER(1gram) 58.12% --> 54.31%
2026-01-03 13:24:48,606: Checkpointing model
2026-01-03 13:24:48,870: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/best_checkpoint
2026-01-03 13:24:49,123: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_6500
2026-01-03 13:24:57,496: Train batch 6600: loss: 13.01 grad norm: 53.21 time: 0.044
2026-01-03 13:25:14,913: Train batch 6800: loss: 15.72 grad norm: 56.97 time: 0.048
2026-01-03 13:25:32,330: Train batch 7000: loss: 17.25 grad norm: 62.31 time: 0.059
2026-01-03 13:25:32,331: Running test after training batch: 7000
2026-01-03 13:25:32,490: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:25:37,431: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 13:25:37,460: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost nett
2026-01-03 13:25:39,048: Val batch 7000: PER (avg): 0.1977 CTC Loss (avg): 19.3574 WER(1gram): 53.05% (n=64) time: 6.717
2026-01-03 13:25:39,048: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 13:25:39,048: t15.2023.08.13 val PER: 0.1601
2026-01-03 13:25:39,048: t15.2023.08.18 val PER: 0.1475
2026-01-03 13:25:39,049: t15.2023.08.20 val PER: 0.1557
2026-01-03 13:25:39,049: t15.2023.08.25 val PER: 0.1114
2026-01-03 13:25:39,049: t15.2023.08.27 val PER: 0.2170
2026-01-03 13:25:39,049: t15.2023.09.01 val PER: 0.1153
2026-01-03 13:25:39,049: t15.2023.09.03 val PER: 0.1995
2026-01-03 13:25:39,049: t15.2023.09.24 val PER: 0.1553
2026-01-03 13:25:39,049: t15.2023.09.29 val PER: 0.1685
2026-01-03 13:25:39,049: t15.2023.10.01 val PER: 0.2147
2026-01-03 13:25:39,049: t15.2023.10.06 val PER: 0.1087
2026-01-03 13:25:39,049: t15.2023.10.08 val PER: 0.2950
2026-01-03 13:25:39,049: t15.2023.10.13 val PER: 0.2607
2026-01-03 13:25:39,049: t15.2023.10.15 val PER: 0.1918
2026-01-03 13:25:39,049: t15.2023.10.20 val PER: 0.1946
2026-01-03 13:25:39,049: t15.2023.10.22 val PER: 0.1448
2026-01-03 13:25:39,050: t15.2023.11.03 val PER: 0.2056
2026-01-03 13:25:39,050: t15.2023.11.04 val PER: 0.0273
2026-01-03 13:25:39,050: t15.2023.11.17 val PER: 0.0669
2026-01-03 13:25:39,050: t15.2023.11.19 val PER: 0.0619
2026-01-03 13:25:39,050: t15.2023.11.26 val PER: 0.2036
2026-01-03 13:25:39,050: t15.2023.12.03 val PER: 0.1754
2026-01-03 13:25:39,050: t15.2023.12.08 val PER: 0.1618
2026-01-03 13:25:39,050: t15.2023.12.10 val PER: 0.1432
2026-01-03 13:25:39,050: t15.2023.12.17 val PER: 0.1757
2026-01-03 13:25:39,050: t15.2023.12.29 val PER: 0.2018
2026-01-03 13:25:39,050: t15.2024.02.25 val PER: 0.1601
2026-01-03 13:25:39,050: t15.2024.03.08 val PER: 0.2731
2026-01-03 13:25:39,050: t15.2024.03.15 val PER: 0.2445
2026-01-03 13:25:39,050: t15.2024.03.17 val PER: 0.2036
2026-01-03 13:25:39,050: t15.2024.05.10 val PER: 0.1961
2026-01-03 13:25:39,051: t15.2024.06.14 val PER: 0.2098
2026-01-03 13:25:39,051: t15.2024.07.19 val PER: 0.3059
2026-01-03 13:25:39,051: t15.2024.07.21 val PER: 0.1386
2026-01-03 13:25:39,051: t15.2024.07.28 val PER: 0.1882
2026-01-03 13:25:39,051: t15.2025.01.10 val PER: 0.3567
2026-01-03 13:25:39,051: t15.2025.01.12 val PER: 0.2063
2026-01-03 13:25:39,051: t15.2025.03.14 val PER: 0.3683
2026-01-03 13:25:39,051: t15.2025.03.16 val PER: 0.2565
2026-01-03 13:25:39,051: t15.2025.03.30 val PER: 0.3598
2026-01-03 13:25:39,051: t15.2025.04.13 val PER: 0.2553
2026-01-03 13:25:39,053: New best val WER(1gram) 54.31% --> 53.05%
2026-01-03 13:25:39,053: Checkpointing model
2026-01-03 13:25:39,321: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/best_checkpoint
2026-01-03 13:25:39,573: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_7000
2026-01-03 13:25:56,646: Train batch 7200: loss: 14.42 grad norm: 56.92 time: 0.078
2026-01-03 13:26:13,498: Train batch 7400: loss: 13.46 grad norm: 55.21 time: 0.075
2026-01-03 13:26:21,906: Running test after training batch: 7500
2026-01-03 13:26:21,998: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:26:26,755: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 13:26:26,783: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-03 13:26:28,417: Val batch 7500: PER (avg): 0.1919 CTC Loss (avg): 18.9071 WER(1gram): 56.09% (n=64) time: 6.510
2026-01-03 13:26:28,417: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 13:26:28,418: t15.2023.08.13 val PER: 0.1611
2026-01-03 13:26:28,418: t15.2023.08.18 val PER: 0.1459
2026-01-03 13:26:28,418: t15.2023.08.20 val PER: 0.1493
2026-01-03 13:26:28,418: t15.2023.08.25 val PER: 0.1084
2026-01-03 13:26:28,418: t15.2023.08.27 val PER: 0.2203
2026-01-03 13:26:28,418: t15.2023.09.01 val PER: 0.1112
2026-01-03 13:26:28,418: t15.2023.09.03 val PER: 0.1876
2026-01-03 13:26:28,418: t15.2023.09.24 val PER: 0.1541
2026-01-03 13:26:28,418: t15.2023.09.29 val PER: 0.1512
2026-01-03 13:26:28,418: t15.2023.10.01 val PER: 0.2054
2026-01-03 13:26:28,418: t15.2023.10.06 val PER: 0.1227
2026-01-03 13:26:28,418: t15.2023.10.08 val PER: 0.2706
2026-01-03 13:26:28,419: t15.2023.10.13 val PER: 0.2467
2026-01-03 13:26:28,419: t15.2023.10.15 val PER: 0.1951
2026-01-03 13:26:28,419: t15.2023.10.20 val PER: 0.2047
2026-01-03 13:26:28,419: t15.2023.10.22 val PER: 0.1414
2026-01-03 13:26:28,419: t15.2023.11.03 val PER: 0.2049
2026-01-03 13:26:28,419: t15.2023.11.04 val PER: 0.0478
2026-01-03 13:26:28,419: t15.2023.11.17 val PER: 0.0638
2026-01-03 13:26:28,419: t15.2023.11.19 val PER: 0.0559
2026-01-03 13:26:28,419: t15.2023.11.26 val PER: 0.1848
2026-01-03 13:26:28,419: t15.2023.12.03 val PER: 0.1670
2026-01-03 13:26:28,419: t15.2023.12.08 val PER: 0.1678
2026-01-03 13:26:28,420: t15.2023.12.10 val PER: 0.1367
2026-01-03 13:26:28,420: t15.2023.12.17 val PER: 0.1757
2026-01-03 13:26:28,420: t15.2023.12.29 val PER: 0.2045
2026-01-03 13:26:28,420: t15.2024.02.25 val PER: 0.1475
2026-01-03 13:26:28,420: t15.2024.03.08 val PER: 0.2688
2026-01-03 13:26:28,420: t15.2024.03.15 val PER: 0.2489
2026-01-03 13:26:28,420: t15.2024.03.17 val PER: 0.1841
2026-01-03 13:26:28,420: t15.2024.05.10 val PER: 0.2051
2026-01-03 13:26:28,420: t15.2024.06.14 val PER: 0.2050
2026-01-03 13:26:28,420: t15.2024.07.19 val PER: 0.2907
2026-01-03 13:26:28,420: t15.2024.07.21 val PER: 0.1359
2026-01-03 13:26:28,421: t15.2024.07.28 val PER: 0.1750
2026-01-03 13:26:28,421: t15.2025.01.10 val PER: 0.3512
2026-01-03 13:26:28,421: t15.2025.01.12 val PER: 0.1986
2026-01-03 13:26:28,421: t15.2025.03.14 val PER: 0.3713
2026-01-03 13:26:28,421: t15.2025.03.16 val PER: 0.2343
2026-01-03 13:26:28,421: t15.2025.03.30 val PER: 0.3402
2026-01-03 13:26:28,421: t15.2025.04.13 val PER: 0.2439
2026-01-03 13:26:28,665: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_7500
2026-01-03 13:26:37,350: Train batch 7600: loss: 16.53 grad norm: 61.77 time: 0.069
2026-01-03 13:26:54,558: Train batch 7800: loss: 14.29 grad norm: 54.56 time: 0.055
2026-01-03 13:27:11,865: Train batch 8000: loss: 11.28 grad norm: 51.63 time: 0.075
2026-01-03 13:27:11,865: Running test after training batch: 8000
2026-01-03 13:27:11,958: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:27:16,693: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-03 13:27:16,723: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nit
2026-01-03 13:27:18,373: Val batch 8000: PER (avg): 0.1851 CTC Loss (avg): 18.1930 WER(1gram): 55.33% (n=64) time: 6.508
2026-01-03 13:27:18,373: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 13:27:18,373: t15.2023.08.13 val PER: 0.1507
2026-01-03 13:27:18,374: t15.2023.08.18 val PER: 0.1375
2026-01-03 13:27:18,374: t15.2023.08.20 val PER: 0.1461
2026-01-03 13:27:18,374: t15.2023.08.25 val PER: 0.1145
2026-01-03 13:27:18,374: t15.2023.08.27 val PER: 0.2074
2026-01-03 13:27:18,374: t15.2023.09.01 val PER: 0.1039
2026-01-03 13:27:18,374: t15.2023.09.03 val PER: 0.1888
2026-01-03 13:27:18,374: t15.2023.09.24 val PER: 0.1468
2026-01-03 13:27:18,374: t15.2023.09.29 val PER: 0.1519
2026-01-03 13:27:18,374: t15.2023.10.01 val PER: 0.2061
2026-01-03 13:27:18,374: t15.2023.10.06 val PER: 0.1119
2026-01-03 13:27:18,374: t15.2023.10.08 val PER: 0.2652
2026-01-03 13:27:18,374: t15.2023.10.13 val PER: 0.2459
2026-01-03 13:27:18,374: t15.2023.10.15 val PER: 0.1918
2026-01-03 13:27:18,374: t15.2023.10.20 val PER: 0.2013
2026-01-03 13:27:18,374: t15.2023.10.22 val PER: 0.1347
2026-01-03 13:27:18,374: t15.2023.11.03 val PER: 0.2042
2026-01-03 13:27:18,375: t15.2023.11.04 val PER: 0.0375
2026-01-03 13:27:18,375: t15.2023.11.17 val PER: 0.0607
2026-01-03 13:27:18,375: t15.2023.11.19 val PER: 0.0619
2026-01-03 13:27:18,375: t15.2023.11.26 val PER: 0.1768
2026-01-03 13:27:18,375: t15.2023.12.03 val PER: 0.1586
2026-01-03 13:27:18,375: t15.2023.12.08 val PER: 0.1445
2026-01-03 13:27:18,375: t15.2023.12.10 val PER: 0.1248
2026-01-03 13:27:18,375: t15.2023.12.17 val PER: 0.1715
2026-01-03 13:27:18,375: t15.2023.12.29 val PER: 0.1764
2026-01-03 13:27:18,375: t15.2024.02.25 val PER: 0.1376
2026-01-03 13:27:18,376: t15.2024.03.08 val PER: 0.2589
2026-01-03 13:27:18,376: t15.2024.03.15 val PER: 0.2470
2026-01-03 13:27:18,376: t15.2024.03.17 val PER: 0.1785
2026-01-03 13:27:18,376: t15.2024.05.10 val PER: 0.1961
2026-01-03 13:27:18,376: t15.2024.06.14 val PER: 0.2003
2026-01-03 13:27:18,376: t15.2024.07.19 val PER: 0.2848
2026-01-03 13:27:18,376: t15.2024.07.21 val PER: 0.1200
2026-01-03 13:27:18,376: t15.2024.07.28 val PER: 0.1610
2026-01-03 13:27:18,376: t15.2025.01.10 val PER: 0.3306
2026-01-03 13:27:18,376: t15.2025.01.12 val PER: 0.1886
2026-01-03 13:27:18,376: t15.2025.03.14 val PER: 0.3521
2026-01-03 13:27:18,376: t15.2025.03.16 val PER: 0.2448
2026-01-03 13:27:18,376: t15.2025.03.30 val PER: 0.3414
2026-01-03 13:27:18,376: t15.2025.04.13 val PER: 0.2596
2026-01-03 13:27:18,620: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_8000
2026-01-03 13:27:36,080: Train batch 8200: loss: 9.74 grad norm: 46.96 time: 0.054
2026-01-03 13:27:53,785: Train batch 8400: loss: 10.18 grad norm: 46.58 time: 0.064
2026-01-03 13:28:02,662: Running test after training batch: 8500
2026-01-03 13:28:02,767: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:28:07,736: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:28:07,766: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nett
2026-01-03 13:28:09,456: Val batch 8500: PER (avg): 0.1802 CTC Loss (avg): 17.9347 WER(1gram): 51.27% (n=64) time: 6.794
2026-01-03 13:28:09,456: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 13:28:09,456: t15.2023.08.13 val PER: 0.1331
2026-01-03 13:28:09,456: t15.2023.08.18 val PER: 0.1408
2026-01-03 13:28:09,456: t15.2023.08.20 val PER: 0.1334
2026-01-03 13:28:09,457: t15.2023.08.25 val PER: 0.1160
2026-01-03 13:28:09,457: t15.2023.08.27 val PER: 0.2170
2026-01-03 13:28:09,457: t15.2023.09.01 val PER: 0.0966
2026-01-03 13:28:09,457: t15.2023.09.03 val PER: 0.1948
2026-01-03 13:28:09,457: t15.2023.09.24 val PER: 0.1493
2026-01-03 13:28:09,457: t15.2023.09.29 val PER: 0.1525
2026-01-03 13:28:09,457: t15.2023.10.01 val PER: 0.1922
2026-01-03 13:28:09,457: t15.2023.10.06 val PER: 0.1076
2026-01-03 13:28:09,457: t15.2023.10.08 val PER: 0.2639
2026-01-03 13:28:09,458: t15.2023.10.13 val PER: 0.2273
2026-01-03 13:28:09,458: t15.2023.10.15 val PER: 0.1839
2026-01-03 13:28:09,458: t15.2023.10.20 val PER: 0.1980
2026-01-03 13:28:09,458: t15.2023.10.22 val PER: 0.1336
2026-01-03 13:28:09,458: t15.2023.11.03 val PER: 0.2015
2026-01-03 13:28:09,458: t15.2023.11.04 val PER: 0.0512
2026-01-03 13:28:09,458: t15.2023.11.17 val PER: 0.0622
2026-01-03 13:28:09,458: t15.2023.11.19 val PER: 0.0419
2026-01-03 13:28:09,458: t15.2023.11.26 val PER: 0.1790
2026-01-03 13:28:09,458: t15.2023.12.03 val PER: 0.1576
2026-01-03 13:28:09,459: t15.2023.12.08 val PER: 0.1345
2026-01-03 13:28:09,459: t15.2023.12.10 val PER: 0.1183
2026-01-03 13:28:09,459: t15.2023.12.17 val PER: 0.1570
2026-01-03 13:28:09,459: t15.2023.12.29 val PER: 0.1682
2026-01-03 13:28:09,459: t15.2024.02.25 val PER: 0.1433
2026-01-03 13:28:09,459: t15.2024.03.08 val PER: 0.2603
2026-01-03 13:28:09,459: t15.2024.03.15 val PER: 0.2389
2026-01-03 13:28:09,459: t15.2024.03.17 val PER: 0.1743
2026-01-03 13:28:09,459: t15.2024.05.10 val PER: 0.2021
2026-01-03 13:28:09,459: t15.2024.06.14 val PER: 0.1845
2026-01-03 13:28:09,460: t15.2024.07.19 val PER: 0.2848
2026-01-03 13:28:09,460: t15.2024.07.21 val PER: 0.1290
2026-01-03 13:28:09,460: t15.2024.07.28 val PER: 0.1750
2026-01-03 13:28:09,460: t15.2025.01.10 val PER: 0.3278
2026-01-03 13:28:09,460: t15.2025.01.12 val PER: 0.1771
2026-01-03 13:28:09,460: t15.2025.03.14 val PER: 0.3447
2026-01-03 13:28:09,460: t15.2025.03.16 val PER: 0.2147
2026-01-03 13:28:09,460: t15.2025.03.30 val PER: 0.3310
2026-01-03 13:28:09,460: t15.2025.04.13 val PER: 0.2325
2026-01-03 13:28:09,461: New best val WER(1gram) 53.05% --> 51.27%
2026-01-03 13:28:09,461: Checkpointing model
2026-01-03 13:28:09,739: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/best_checkpoint
2026-01-03 13:28:10,014: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_8500
2026-01-03 13:28:18,871: Train batch 8600: loss: 15.90 grad norm: 62.46 time: 0.054
2026-01-03 13:28:36,279: Train batch 8800: loss: 16.20 grad norm: 59.84 time: 0.060
2026-01-03 13:28:54,104: Train batch 9000: loss: 16.35 grad norm: 66.31 time: 0.072
2026-01-03 13:28:54,104: Running test after training batch: 9000
2026-01-03 13:28:54,231: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:28:58,960: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:28:58,991: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-03 13:29:00,694: Val batch 9000: PER (avg): 0.1770 CTC Loss (avg): 17.4683 WER(1gram): 52.28% (n=64) time: 6.589
2026-01-03 13:29:00,694: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 13:29:00,694: t15.2023.08.13 val PER: 0.1383
2026-01-03 13:29:00,694: t15.2023.08.18 val PER: 0.1324
2026-01-03 13:29:00,695: t15.2023.08.20 val PER: 0.1374
2026-01-03 13:29:00,695: t15.2023.08.25 val PER: 0.1024
2026-01-03 13:29:00,695: t15.2023.08.27 val PER: 0.2074
2026-01-03 13:29:00,695: t15.2023.09.01 val PER: 0.0974
2026-01-03 13:29:00,695: t15.2023.09.03 val PER: 0.1876
2026-01-03 13:29:00,695: t15.2023.09.24 val PER: 0.1529
2026-01-03 13:29:00,695: t15.2023.09.29 val PER: 0.1525
2026-01-03 13:29:00,695: t15.2023.10.01 val PER: 0.1942
2026-01-03 13:29:00,695: t15.2023.10.06 val PER: 0.1033
2026-01-03 13:29:00,695: t15.2023.10.08 val PER: 0.2598
2026-01-03 13:29:00,695: t15.2023.10.13 val PER: 0.2366
2026-01-03 13:29:00,695: t15.2023.10.15 val PER: 0.1879
2026-01-03 13:29:00,695: t15.2023.10.20 val PER: 0.2081
2026-01-03 13:29:00,695: t15.2023.10.22 val PER: 0.1303
2026-01-03 13:29:00,695: t15.2023.11.03 val PER: 0.2123
2026-01-03 13:29:00,696: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:29:00,696: t15.2023.11.17 val PER: 0.0544
2026-01-03 13:29:00,696: t15.2023.11.19 val PER: 0.0419
2026-01-03 13:29:00,696: t15.2023.11.26 val PER: 0.1594
2026-01-03 13:29:00,696: t15.2023.12.03 val PER: 0.1355
2026-01-03 13:29:00,696: t15.2023.12.08 val PER: 0.1312
2026-01-03 13:29:00,696: t15.2023.12.10 val PER: 0.1117
2026-01-03 13:29:00,696: t15.2023.12.17 val PER: 0.1726
2026-01-03 13:29:00,696: t15.2023.12.29 val PER: 0.1633
2026-01-03 13:29:00,696: t15.2024.02.25 val PER: 0.1503
2026-01-03 13:29:00,696: t15.2024.03.08 val PER: 0.2674
2026-01-03 13:29:00,696: t15.2024.03.15 val PER: 0.2283
2026-01-03 13:29:00,696: t15.2024.03.17 val PER: 0.1743
2026-01-03 13:29:00,696: t15.2024.05.10 val PER: 0.1753
2026-01-03 13:29:00,696: t15.2024.06.14 val PER: 0.1877
2026-01-03 13:29:00,696: t15.2024.07.19 val PER: 0.2742
2026-01-03 13:29:00,697: t15.2024.07.21 val PER: 0.1145
2026-01-03 13:29:00,697: t15.2024.07.28 val PER: 0.1493
2026-01-03 13:29:00,697: t15.2025.01.10 val PER: 0.3072
2026-01-03 13:29:00,697: t15.2025.01.12 val PER: 0.1732
2026-01-03 13:29:00,697: t15.2025.03.14 val PER: 0.3595
2026-01-03 13:29:00,697: t15.2025.03.16 val PER: 0.2199
2026-01-03 13:29:00,697: t15.2025.03.30 val PER: 0.3310
2026-01-03 13:29:00,697: t15.2025.04.13 val PER: 0.2525
2026-01-03 13:29:00,964: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_9000
2026-01-03 13:29:18,228: Train batch 9200: loss: 10.98 grad norm: 50.89 time: 0.056
2026-01-03 13:29:35,487: Train batch 9400: loss: 8.16 grad norm: 46.80 time: 0.068
2026-01-03 13:29:44,063: Running test after training batch: 9500
2026-01-03 13:29:44,162: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:29:48,965: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:29:48,994: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost it
2026-01-03 13:29:50,710: Val batch 9500: PER (avg): 0.1752 CTC Loss (avg): 17.3650 WER(1gram): 52.03% (n=64) time: 6.647
2026-01-03 13:29:50,710: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 13:29:50,711: t15.2023.08.13 val PER: 0.1299
2026-01-03 13:29:50,711: t15.2023.08.18 val PER: 0.1241
2026-01-03 13:29:50,711: t15.2023.08.20 val PER: 0.1342
2026-01-03 13:29:50,711: t15.2023.08.25 val PER: 0.1009
2026-01-03 13:29:50,711: t15.2023.08.27 val PER: 0.2138
2026-01-03 13:29:50,711: t15.2023.09.01 val PER: 0.0966
2026-01-03 13:29:50,711: t15.2023.09.03 val PER: 0.1805
2026-01-03 13:29:50,711: t15.2023.09.24 val PER: 0.1420
2026-01-03 13:29:50,711: t15.2023.09.29 val PER: 0.1410
2026-01-03 13:29:50,711: t15.2023.10.01 val PER: 0.1935
2026-01-03 13:29:50,711: t15.2023.10.06 val PER: 0.1033
2026-01-03 13:29:50,711: t15.2023.10.08 val PER: 0.2517
2026-01-03 13:29:50,711: t15.2023.10.13 val PER: 0.2258
2026-01-03 13:29:50,711: t15.2023.10.15 val PER: 0.1885
2026-01-03 13:29:50,712: t15.2023.10.20 val PER: 0.2081
2026-01-03 13:29:50,712: t15.2023.10.22 val PER: 0.1247
2026-01-03 13:29:50,712: t15.2023.11.03 val PER: 0.1954
2026-01-03 13:29:50,712: t15.2023.11.04 val PER: 0.0444
2026-01-03 13:29:50,712: t15.2023.11.17 val PER: 0.0544
2026-01-03 13:29:50,712: t15.2023.11.19 val PER: 0.0459
2026-01-03 13:29:50,712: t15.2023.11.26 val PER: 0.1587
2026-01-03 13:29:50,712: t15.2023.12.03 val PER: 0.1355
2026-01-03 13:29:50,712: t15.2023.12.08 val PER: 0.1411
2026-01-03 13:29:50,712: t15.2023.12.10 val PER: 0.1196
2026-01-03 13:29:50,712: t15.2023.12.17 val PER: 0.1580
2026-01-03 13:29:50,712: t15.2023.12.29 val PER: 0.1633
2026-01-03 13:29:50,712: t15.2024.02.25 val PER: 0.1390
2026-01-03 13:29:50,712: t15.2024.03.08 val PER: 0.2447
2026-01-03 13:29:50,712: t15.2024.03.15 val PER: 0.2301
2026-01-03 13:29:50,712: t15.2024.03.17 val PER: 0.1674
2026-01-03 13:29:50,712: t15.2024.05.10 val PER: 0.1902
2026-01-03 13:29:50,713: t15.2024.06.14 val PER: 0.1767
2026-01-03 13:29:50,713: t15.2024.07.19 val PER: 0.2657
2026-01-03 13:29:50,713: t15.2024.07.21 val PER: 0.1159
2026-01-03 13:29:50,713: t15.2024.07.28 val PER: 0.1662
2026-01-03 13:29:50,713: t15.2025.01.10 val PER: 0.3168
2026-01-03 13:29:50,713: t15.2025.01.12 val PER: 0.1878
2026-01-03 13:29:50,713: t15.2025.03.14 val PER: 0.3802
2026-01-03 13:29:50,713: t15.2025.03.16 val PER: 0.2173
2026-01-03 13:29:50,713: t15.2025.03.30 val PER: 0.3264
2026-01-03 13:29:50,713: t15.2025.04.13 val PER: 0.2539
2026-01-03 13:29:50,977: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_9500
2026-01-03 13:29:59,448: Train batch 9600: loss: 8.22 grad norm: 46.14 time: 0.073
2026-01-03 13:30:16,696: Train batch 9800: loss: 12.75 grad norm: 57.76 time: 0.062
2026-01-03 13:30:34,005: Train batch 10000: loss: 5.52 grad norm: 34.77 time: 0.061
2026-01-03 13:30:34,006: Running test after training batch: 10000
2026-01-03 13:30:34,134: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:30:38,941: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:30:38,973: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sette
2026-01-03 13:30:40,686: Val batch 10000: PER (avg): 0.1716 CTC Loss (avg): 17.1121 WER(1gram): 52.28% (n=64) time: 6.679
2026-01-03 13:30:40,686: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 13:30:40,686: t15.2023.08.13 val PER: 0.1331
2026-01-03 13:30:40,686: t15.2023.08.18 val PER: 0.1241
2026-01-03 13:30:40,686: t15.2023.08.20 val PER: 0.1366
2026-01-03 13:30:40,686: t15.2023.08.25 val PER: 0.1099
2026-01-03 13:30:40,686: t15.2023.08.27 val PER: 0.2154
2026-01-03 13:30:40,686: t15.2023.09.01 val PER: 0.0925
2026-01-03 13:30:40,687: t15.2023.09.03 val PER: 0.1912
2026-01-03 13:30:40,687: t15.2023.09.24 val PER: 0.1493
2026-01-03 13:30:40,687: t15.2023.09.29 val PER: 0.1423
2026-01-03 13:30:40,687: t15.2023.10.01 val PER: 0.1935
2026-01-03 13:30:40,687: t15.2023.10.06 val PER: 0.1033
2026-01-03 13:30:40,687: t15.2023.10.08 val PER: 0.2476
2026-01-03 13:30:40,687: t15.2023.10.13 val PER: 0.2320
2026-01-03 13:30:40,687: t15.2023.10.15 val PER: 0.1747
2026-01-03 13:30:40,687: t15.2023.10.20 val PER: 0.1711
2026-01-03 13:30:40,687: t15.2023.10.22 val PER: 0.1314
2026-01-03 13:30:40,687: t15.2023.11.03 val PER: 0.1927
2026-01-03 13:30:40,687: t15.2023.11.04 val PER: 0.0375
2026-01-03 13:30:40,687: t15.2023.11.17 val PER: 0.0467
2026-01-03 13:30:40,687: t15.2023.11.19 val PER: 0.0479
2026-01-03 13:30:40,691: t15.2023.11.26 val PER: 0.1572
2026-01-03 13:30:40,691: t15.2023.12.03 val PER: 0.1376
2026-01-03 13:30:40,691: t15.2023.12.08 val PER: 0.1318
2026-01-03 13:30:40,691: t15.2023.12.10 val PER: 0.1248
2026-01-03 13:30:40,691: t15.2023.12.17 val PER: 0.1570
2026-01-03 13:30:40,691: t15.2023.12.29 val PER: 0.1558
2026-01-03 13:30:40,691: t15.2024.02.25 val PER: 0.1390
2026-01-03 13:30:40,691: t15.2024.03.08 val PER: 0.2475
2026-01-03 13:30:40,691: t15.2024.03.15 val PER: 0.2195
2026-01-03 13:30:40,691: t15.2024.03.17 val PER: 0.1681
2026-01-03 13:30:40,692: t15.2024.05.10 val PER: 0.1679
2026-01-03 13:30:40,692: t15.2024.06.14 val PER: 0.1845
2026-01-03 13:30:40,692: t15.2024.07.19 val PER: 0.2564
2026-01-03 13:30:40,692: t15.2024.07.21 val PER: 0.1110
2026-01-03 13:30:40,692: t15.2024.07.28 val PER: 0.1551
2026-01-03 13:30:40,692: t15.2025.01.10 val PER: 0.3113
2026-01-03 13:30:40,692: t15.2025.01.12 val PER: 0.1724
2026-01-03 13:30:40,692: t15.2025.03.14 val PER: 0.3432
2026-01-03 13:30:40,692: t15.2025.03.16 val PER: 0.2264
2026-01-03 13:30:40,692: t15.2025.03.30 val PER: 0.3184
2026-01-03 13:30:40,692: t15.2025.04.13 val PER: 0.2354
2026-01-03 13:30:41,058: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_10000
2026-01-03 13:30:58,062: Train batch 10200: loss: 6.34 grad norm: 41.68 time: 0.049
2026-01-03 13:31:15,315: Train batch 10400: loss: 9.60 grad norm: 50.65 time: 0.072
2026-01-03 13:31:23,947: Running test after training batch: 10500
2026-01-03 13:31:24,087: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:31:28,806: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 13:31:28,836: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost nit
2026-01-03 13:31:30,573: Val batch 10500: PER (avg): 0.1685 CTC Loss (avg): 16.7760 WER(1gram): 51.27% (n=64) time: 6.625
2026-01-03 13:31:30,573: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-03 13:31:30,573: t15.2023.08.13 val PER: 0.1299
2026-01-03 13:31:30,573: t15.2023.08.18 val PER: 0.1257
2026-01-03 13:31:30,574: t15.2023.08.20 val PER: 0.1255
2026-01-03 13:31:30,574: t15.2023.08.25 val PER: 0.0964
2026-01-03 13:31:30,574: t15.2023.08.27 val PER: 0.2090
2026-01-03 13:31:30,574: t15.2023.09.01 val PER: 0.0917
2026-01-03 13:31:30,574: t15.2023.09.03 val PER: 0.1734
2026-01-03 13:31:30,574: t15.2023.09.24 val PER: 0.1432
2026-01-03 13:31:30,574: t15.2023.09.29 val PER: 0.1493
2026-01-03 13:31:30,574: t15.2023.10.01 val PER: 0.1823
2026-01-03 13:31:30,574: t15.2023.10.06 val PER: 0.1076
2026-01-03 13:31:30,574: t15.2023.10.08 val PER: 0.2395
2026-01-03 13:31:30,574: t15.2023.10.13 val PER: 0.2196
2026-01-03 13:31:30,575: t15.2023.10.15 val PER: 0.1753
2026-01-03 13:31:30,575: t15.2023.10.20 val PER: 0.2047
2026-01-03 13:31:30,575: t15.2023.10.22 val PER: 0.1359
2026-01-03 13:31:30,575: t15.2023.11.03 val PER: 0.1852
2026-01-03 13:31:30,575: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:31:30,575: t15.2023.11.17 val PER: 0.0575
2026-01-03 13:31:30,575: t15.2023.11.19 val PER: 0.0599
2026-01-03 13:31:30,575: t15.2023.11.26 val PER: 0.1457
2026-01-03 13:31:30,575: t15.2023.12.03 val PER: 0.1345
2026-01-03 13:31:30,575: t15.2023.12.08 val PER: 0.1298
2026-01-03 13:31:30,575: t15.2023.12.10 val PER: 0.1130
2026-01-03 13:31:30,575: t15.2023.12.17 val PER: 0.1476
2026-01-03 13:31:30,575: t15.2023.12.29 val PER: 0.1565
2026-01-03 13:31:30,575: t15.2024.02.25 val PER: 0.1320
2026-01-03 13:31:30,575: t15.2024.03.08 val PER: 0.2617
2026-01-03 13:31:30,575: t15.2024.03.15 val PER: 0.2195
2026-01-03 13:31:30,576: t15.2024.03.17 val PER: 0.1618
2026-01-03 13:31:30,576: t15.2024.05.10 val PER: 0.1842
2026-01-03 13:31:30,576: t15.2024.06.14 val PER: 0.1830
2026-01-03 13:31:30,576: t15.2024.07.19 val PER: 0.2564
2026-01-03 13:31:30,576: t15.2024.07.21 val PER: 0.1124
2026-01-03 13:31:30,576: t15.2024.07.28 val PER: 0.1434
2026-01-03 13:31:30,576: t15.2025.01.10 val PER: 0.3072
2026-01-03 13:31:30,576: t15.2025.01.12 val PER: 0.1694
2026-01-03 13:31:30,576: t15.2025.03.14 val PER: 0.3580
2026-01-03 13:31:30,576: t15.2025.03.16 val PER: 0.1950
2026-01-03 13:31:30,576: t15.2025.03.30 val PER: 0.3138
2026-01-03 13:31:30,576: t15.2025.04.13 val PER: 0.2368
2026-01-03 13:31:30,840: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_10500
2026-01-03 13:31:39,542: Train batch 10600: loss: 9.43 grad norm: 60.82 time: 0.071
2026-01-03 13:31:56,844: Train batch 10800: loss: 14.84 grad norm: 64.61 time: 0.064
2026-01-03 13:32:14,057: Train batch 11000: loss: 14.13 grad norm: 59.49 time: 0.056
2026-01-03 13:32:14,057: Running test after training batch: 11000
2026-01-03 13:32:14,163: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:32:18,884: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 13:32:18,914: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 13:32:20,636: Val batch 11000: PER (avg): 0.1645 CTC Loss (avg): 16.6160 WER(1gram): 47.46% (n=64) time: 6.579
2026-01-03 13:32:20,637: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 13:32:20,637: t15.2023.08.13 val PER: 0.1299
2026-01-03 13:32:20,637: t15.2023.08.18 val PER: 0.1199
2026-01-03 13:32:20,637: t15.2023.08.20 val PER: 0.1247
2026-01-03 13:32:20,637: t15.2023.08.25 val PER: 0.0904
2026-01-03 13:32:20,637: t15.2023.08.27 val PER: 0.1977
2026-01-03 13:32:20,637: t15.2023.09.01 val PER: 0.0869
2026-01-03 13:32:20,637: t15.2023.09.03 val PER: 0.1793
2026-01-03 13:32:20,637: t15.2023.09.24 val PER: 0.1335
2026-01-03 13:32:20,638: t15.2023.09.29 val PER: 0.1391
2026-01-03 13:32:20,638: t15.2023.10.01 val PER: 0.1902
2026-01-03 13:32:20,638: t15.2023.10.06 val PER: 0.0861
2026-01-03 13:32:20,638: t15.2023.10.08 val PER: 0.2436
2026-01-03 13:32:20,638: t15.2023.10.13 val PER: 0.2211
2026-01-03 13:32:20,638: t15.2023.10.15 val PER: 0.1694
2026-01-03 13:32:20,638: t15.2023.10.20 val PER: 0.1879
2026-01-03 13:32:20,638: t15.2023.10.22 val PER: 0.1203
2026-01-03 13:32:20,638: t15.2023.11.03 val PER: 0.1981
2026-01-03 13:32:20,638: t15.2023.11.04 val PER: 0.0307
2026-01-03 13:32:20,638: t15.2023.11.17 val PER: 0.0467
2026-01-03 13:32:20,638: t15.2023.11.19 val PER: 0.0379
2026-01-03 13:32:20,638: t15.2023.11.26 val PER: 0.1428
2026-01-03 13:32:20,638: t15.2023.12.03 val PER: 0.1282
2026-01-03 13:32:20,639: t15.2023.12.08 val PER: 0.1285
2026-01-03 13:32:20,639: t15.2023.12.10 val PER: 0.1038
2026-01-03 13:32:20,639: t15.2023.12.17 val PER: 0.1393
2026-01-03 13:32:20,639: t15.2023.12.29 val PER: 0.1496
2026-01-03 13:32:20,639: t15.2024.02.25 val PER: 0.1362
2026-01-03 13:32:20,639: t15.2024.03.08 val PER: 0.2617
2026-01-03 13:32:20,639: t15.2024.03.15 val PER: 0.2201
2026-01-03 13:32:20,639: t15.2024.03.17 val PER: 0.1604
2026-01-03 13:32:20,639: t15.2024.05.10 val PER: 0.1724
2026-01-03 13:32:20,639: t15.2024.06.14 val PER: 0.1751
2026-01-03 13:32:20,639: t15.2024.07.19 val PER: 0.2406
2026-01-03 13:32:20,639: t15.2024.07.21 val PER: 0.1083
2026-01-03 13:32:20,639: t15.2024.07.28 val PER: 0.1471
2026-01-03 13:32:20,639: t15.2025.01.10 val PER: 0.3085
2026-01-03 13:32:20,639: t15.2025.01.12 val PER: 0.1663
2026-01-03 13:32:20,640: t15.2025.03.14 val PER: 0.3565
2026-01-03 13:32:20,640: t15.2025.03.16 val PER: 0.2003
2026-01-03 13:32:20,640: t15.2025.03.30 val PER: 0.3046
2026-01-03 13:32:20,640: t15.2025.04.13 val PER: 0.2254
2026-01-03 13:32:20,641: New best val WER(1gram) 51.27% --> 47.46%
2026-01-03 13:32:20,641: Checkpointing model
2026-01-03 13:32:20,907: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/best_checkpoint
2026-01-03 13:32:21,179: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_11000
2026-01-03 13:32:38,345: Train batch 11200: loss: 10.54 grad norm: 51.30 time: 0.070
2026-01-03 13:32:55,469: Train batch 11400: loss: 9.69 grad norm: 52.87 time: 0.057
2026-01-03 13:33:04,099: Running test after training batch: 11500
2026-01-03 13:33:04,202: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:33:09,440: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:33:09,472: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost nett
2026-01-03 13:33:11,207: Val batch 11500: PER (avg): 0.1636 CTC Loss (avg): 16.4729 WER(1gram): 49.75% (n=64) time: 7.108
2026-01-03 13:33:11,207: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 13:33:11,207: t15.2023.08.13 val PER: 0.1175
2026-01-03 13:33:11,207: t15.2023.08.18 val PER: 0.1174
2026-01-03 13:33:11,208: t15.2023.08.20 val PER: 0.1271
2026-01-03 13:33:11,208: t15.2023.08.25 val PER: 0.0873
2026-01-03 13:33:11,208: t15.2023.08.27 val PER: 0.2010
2026-01-03 13:33:11,208: t15.2023.09.01 val PER: 0.0877
2026-01-03 13:33:11,208: t15.2023.09.03 val PER: 0.1865
2026-01-03 13:33:11,208: t15.2023.09.24 val PER: 0.1226
2026-01-03 13:33:11,208: t15.2023.09.29 val PER: 0.1398
2026-01-03 13:33:11,209: t15.2023.10.01 val PER: 0.1783
2026-01-03 13:33:11,209: t15.2023.10.06 val PER: 0.0840
2026-01-03 13:33:11,209: t15.2023.10.08 val PER: 0.2503
2026-01-03 13:33:11,209: t15.2023.10.13 val PER: 0.2258
2026-01-03 13:33:11,209: t15.2023.10.15 val PER: 0.1727
2026-01-03 13:33:11,209: t15.2023.10.20 val PER: 0.1913
2026-01-03 13:33:11,209: t15.2023.10.22 val PER: 0.1292
2026-01-03 13:33:11,209: t15.2023.11.03 val PER: 0.1872
2026-01-03 13:33:11,209: t15.2023.11.04 val PER: 0.0273
2026-01-03 13:33:11,209: t15.2023.11.17 val PER: 0.0467
2026-01-03 13:33:11,209: t15.2023.11.19 val PER: 0.0479
2026-01-03 13:33:11,209: t15.2023.11.26 val PER: 0.1420
2026-01-03 13:33:11,209: t15.2023.12.03 val PER: 0.1261
2026-01-03 13:33:11,209: t15.2023.12.08 val PER: 0.1178
2026-01-03 13:33:11,209: t15.2023.12.10 val PER: 0.1051
2026-01-03 13:33:11,210: t15.2023.12.17 val PER: 0.1414
2026-01-03 13:33:11,210: t15.2023.12.29 val PER: 0.1455
2026-01-03 13:33:11,210: t15.2024.02.25 val PER: 0.1236
2026-01-03 13:33:11,210: t15.2024.03.08 val PER: 0.2276
2026-01-03 13:33:11,210: t15.2024.03.15 val PER: 0.2220
2026-01-03 13:33:11,210: t15.2024.03.17 val PER: 0.1527
2026-01-03 13:33:11,210: t15.2024.05.10 val PER: 0.1813
2026-01-03 13:33:11,210: t15.2024.06.14 val PER: 0.1798
2026-01-03 13:33:11,210: t15.2024.07.19 val PER: 0.2551
2026-01-03 13:33:11,210: t15.2024.07.21 val PER: 0.1083
2026-01-03 13:33:11,210: t15.2024.07.28 val PER: 0.1426
2026-01-03 13:33:11,210: t15.2025.01.10 val PER: 0.3264
2026-01-03 13:33:11,210: t15.2025.01.12 val PER: 0.1678
2026-01-03 13:33:11,210: t15.2025.03.14 val PER: 0.3476
2026-01-03 13:33:11,210: t15.2025.03.16 val PER: 0.2107
2026-01-03 13:33:11,210: t15.2025.03.30 val PER: 0.3011
2026-01-03 13:33:11,211: t15.2025.04.13 val PER: 0.2311
2026-01-03 13:33:11,473: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_11500
2026-01-03 13:33:19,693: Train batch 11600: loss: 10.72 grad norm: 47.40 time: 0.061
2026-01-03 13:33:36,501: Train batch 11800: loss: 6.58 grad norm: 42.68 time: 0.044
2026-01-03 13:33:53,415: Train batch 12000: loss: 14.17 grad norm: 54.81 time: 0.071
2026-01-03 13:33:53,415: Running test after training batch: 12000
2026-01-03 13:33:53,510: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:33:58,240: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:33:58,272: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sette
2026-01-03 13:34:00,021: Val batch 12000: PER (avg): 0.1615 CTC Loss (avg): 16.1261 WER(1gram): 49.75% (n=64) time: 6.606
2026-01-03 13:34:00,021: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 13:34:00,021: t15.2023.08.13 val PER: 0.1247
2026-01-03 13:34:00,021: t15.2023.08.18 val PER: 0.1106
2026-01-03 13:34:00,021: t15.2023.08.20 val PER: 0.1223
2026-01-03 13:34:00,021: t15.2023.08.25 val PER: 0.0964
2026-01-03 13:34:00,021: t15.2023.08.27 val PER: 0.1945
2026-01-03 13:34:00,021: t15.2023.09.01 val PER: 0.0909
2026-01-03 13:34:00,022: t15.2023.09.03 val PER: 0.1663
2026-01-03 13:34:00,022: t15.2023.09.24 val PER: 0.1250
2026-01-03 13:34:00,022: t15.2023.09.29 val PER: 0.1340
2026-01-03 13:34:00,022: t15.2023.10.01 val PER: 0.1724
2026-01-03 13:34:00,022: t15.2023.10.06 val PER: 0.1012
2026-01-03 13:34:00,022: t15.2023.10.08 val PER: 0.2503
2026-01-03 13:34:00,022: t15.2023.10.13 val PER: 0.2196
2026-01-03 13:34:00,022: t15.2023.10.15 val PER: 0.1681
2026-01-03 13:34:00,022: t15.2023.10.20 val PER: 0.1946
2026-01-03 13:34:00,022: t15.2023.10.22 val PER: 0.1258
2026-01-03 13:34:00,022: t15.2023.11.03 val PER: 0.1818
2026-01-03 13:34:00,022: t15.2023.11.04 val PER: 0.0375
2026-01-03 13:34:00,022: t15.2023.11.17 val PER: 0.0420
2026-01-03 13:34:00,023: t15.2023.11.19 val PER: 0.0439
2026-01-03 13:34:00,023: t15.2023.11.26 val PER: 0.1304
2026-01-03 13:34:00,023: t15.2023.12.03 val PER: 0.1218
2026-01-03 13:34:00,023: t15.2023.12.08 val PER: 0.1172
2026-01-03 13:34:00,023: t15.2023.12.10 val PER: 0.0999
2026-01-03 13:34:00,023: t15.2023.12.17 val PER: 0.1403
2026-01-03 13:34:00,023: t15.2023.12.29 val PER: 0.1496
2026-01-03 13:34:00,023: t15.2024.02.25 val PER: 0.1306
2026-01-03 13:34:00,023: t15.2024.03.08 val PER: 0.2447
2026-01-03 13:34:00,023: t15.2024.03.15 val PER: 0.2170
2026-01-03 13:34:00,023: t15.2024.03.17 val PER: 0.1430
2026-01-03 13:34:00,023: t15.2024.05.10 val PER: 0.1813
2026-01-03 13:34:00,023: t15.2024.06.14 val PER: 0.1956
2026-01-03 13:34:00,023: t15.2024.07.19 val PER: 0.2584
2026-01-03 13:34:00,023: t15.2024.07.21 val PER: 0.1069
2026-01-03 13:34:00,024: t15.2024.07.28 val PER: 0.1434
2026-01-03 13:34:00,024: t15.2025.01.10 val PER: 0.3072
2026-01-03 13:34:00,024: t15.2025.01.12 val PER: 0.1594
2026-01-03 13:34:00,024: t15.2025.03.14 val PER: 0.3491
2026-01-03 13:34:00,024: t15.2025.03.16 val PER: 0.2016
2026-01-03 13:34:00,024: t15.2025.03.30 val PER: 0.3092
2026-01-03 13:34:00,024: t15.2025.04.13 val PER: 0.2254
2026-01-03 13:34:00,286: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_12000
2026-01-03 13:34:17,568: Train batch 12200: loss: 5.48 grad norm: 36.35 time: 0.066
2026-01-03 13:34:34,565: Train batch 12400: loss: 4.81 grad norm: 36.57 time: 0.040
2026-01-03 13:34:43,447: Running test after training batch: 12500
2026-01-03 13:34:43,589: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:34:48,275: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:34:48,307: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost sette
2026-01-03 13:34:50,047: Val batch 12500: PER (avg): 0.1587 CTC Loss (avg): 15.8005 WER(1gram): 48.73% (n=64) time: 6.599
2026-01-03 13:34:50,047: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 13:34:50,047: t15.2023.08.13 val PER: 0.1258
2026-01-03 13:34:50,047: t15.2023.08.18 val PER: 0.1098
2026-01-03 13:34:50,047: t15.2023.08.20 val PER: 0.1128
2026-01-03 13:34:50,047: t15.2023.08.25 val PER: 0.0904
2026-01-03 13:34:50,047: t15.2023.08.27 val PER: 0.1929
2026-01-03 13:34:50,047: t15.2023.09.01 val PER: 0.0852
2026-01-03 13:34:50,047: t15.2023.09.03 val PER: 0.1734
2026-01-03 13:34:50,048: t15.2023.09.24 val PER: 0.1262
2026-01-03 13:34:50,048: t15.2023.09.29 val PER: 0.1334
2026-01-03 13:34:50,048: t15.2023.10.01 val PER: 0.1770
2026-01-03 13:34:50,048: t15.2023.10.06 val PER: 0.0893
2026-01-03 13:34:50,048: t15.2023.10.08 val PER: 0.2490
2026-01-03 13:34:50,048: t15.2023.10.13 val PER: 0.2227
2026-01-03 13:34:50,048: t15.2023.10.15 val PER: 0.1648
2026-01-03 13:34:50,048: t15.2023.10.20 val PER: 0.1946
2026-01-03 13:34:50,048: t15.2023.10.22 val PER: 0.1203
2026-01-03 13:34:50,048: t15.2023.11.03 val PER: 0.1825
2026-01-03 13:34:50,048: t15.2023.11.04 val PER: 0.0307
2026-01-03 13:34:50,048: t15.2023.11.17 val PER: 0.0451
2026-01-03 13:34:50,048: t15.2023.11.19 val PER: 0.0359
2026-01-03 13:34:50,048: t15.2023.11.26 val PER: 0.1319
2026-01-03 13:34:50,048: t15.2023.12.03 val PER: 0.1208
2026-01-03 13:34:50,048: t15.2023.12.08 val PER: 0.1132
2026-01-03 13:34:50,049: t15.2023.12.10 val PER: 0.0972
2026-01-03 13:34:50,049: t15.2023.12.17 val PER: 0.1383
2026-01-03 13:34:50,049: t15.2023.12.29 val PER: 0.1462
2026-01-03 13:34:50,049: t15.2024.02.25 val PER: 0.1250
2026-01-03 13:34:50,049: t15.2024.03.08 val PER: 0.2447
2026-01-03 13:34:50,049: t15.2024.03.15 val PER: 0.2108
2026-01-03 13:34:50,049: t15.2024.03.17 val PER: 0.1499
2026-01-03 13:34:50,049: t15.2024.05.10 val PER: 0.1813
2026-01-03 13:34:50,049: t15.2024.06.14 val PER: 0.1830
2026-01-03 13:34:50,049: t15.2024.07.19 val PER: 0.2459
2026-01-03 13:34:50,049: t15.2024.07.21 val PER: 0.1021
2026-01-03 13:34:50,049: t15.2024.07.28 val PER: 0.1382
2026-01-03 13:34:50,049: t15.2025.01.10 val PER: 0.3140
2026-01-03 13:34:50,049: t15.2025.01.12 val PER: 0.1524
2026-01-03 13:34:50,049: t15.2025.03.14 val PER: 0.3462
2026-01-03 13:34:50,049: t15.2025.03.16 val PER: 0.1911
2026-01-03 13:34:50,050: t15.2025.03.30 val PER: 0.3011
2026-01-03 13:34:50,050: t15.2025.04.13 val PER: 0.2183
2026-01-03 13:34:50,314: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_12500
2026-01-03 13:34:58,662: Train batch 12600: loss: 7.94 grad norm: 42.57 time: 0.057
2026-01-03 13:35:15,924: Train batch 12800: loss: 6.17 grad norm: 37.92 time: 0.052
2026-01-03 13:35:33,246: Train batch 13000: loss: 6.71 grad norm: 42.77 time: 0.066
2026-01-03 13:35:33,246: Running test after training batch: 13000
2026-01-03 13:35:33,352: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:35:38,533: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:35:38,565: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost sette
2026-01-03 13:35:40,318: Val batch 13000: PER (avg): 0.1567 CTC Loss (avg): 15.7564 WER(1gram): 47.97% (n=64) time: 7.072
2026-01-03 13:35:40,318: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 13:35:40,319: t15.2023.08.13 val PER: 0.1216
2026-01-03 13:35:40,319: t15.2023.08.18 val PER: 0.1123
2026-01-03 13:35:40,319: t15.2023.08.20 val PER: 0.1056
2026-01-03 13:35:40,319: t15.2023.08.25 val PER: 0.0919
2026-01-03 13:35:40,319: t15.2023.08.27 val PER: 0.1913
2026-01-03 13:35:40,319: t15.2023.09.01 val PER: 0.0844
2026-01-03 13:35:40,319: t15.2023.09.03 val PER: 0.1698
2026-01-03 13:35:40,319: t15.2023.09.24 val PER: 0.1262
2026-01-03 13:35:40,319: t15.2023.09.29 val PER: 0.1378
2026-01-03 13:35:40,319: t15.2023.10.01 val PER: 0.1757
2026-01-03 13:35:40,319: t15.2023.10.06 val PER: 0.0893
2026-01-03 13:35:40,319: t15.2023.10.08 val PER: 0.2490
2026-01-03 13:35:40,320: t15.2023.10.13 val PER: 0.2203
2026-01-03 13:35:40,320: t15.2023.10.15 val PER: 0.1589
2026-01-03 13:35:40,320: t15.2023.10.20 val PER: 0.1946
2026-01-03 13:35:40,320: t15.2023.10.22 val PER: 0.1192
2026-01-03 13:35:40,320: t15.2023.11.03 val PER: 0.1791
2026-01-03 13:35:40,320: t15.2023.11.04 val PER: 0.0307
2026-01-03 13:35:40,320: t15.2023.11.17 val PER: 0.0451
2026-01-03 13:35:40,320: t15.2023.11.19 val PER: 0.0399
2026-01-03 13:35:40,320: t15.2023.11.26 val PER: 0.1290
2026-01-03 13:35:40,320: t15.2023.12.03 val PER: 0.1250
2026-01-03 13:35:40,320: t15.2023.12.08 val PER: 0.1132
2026-01-03 13:35:40,320: t15.2023.12.10 val PER: 0.0959
2026-01-03 13:35:40,320: t15.2023.12.17 val PER: 0.1414
2026-01-03 13:35:40,320: t15.2023.12.29 val PER: 0.1421
2026-01-03 13:35:40,320: t15.2024.02.25 val PER: 0.1236
2026-01-03 13:35:40,320: t15.2024.03.08 val PER: 0.2418
2026-01-03 13:35:40,320: t15.2024.03.15 val PER: 0.2083
2026-01-03 13:35:40,321: t15.2024.03.17 val PER: 0.1437
2026-01-03 13:35:40,321: t15.2024.05.10 val PER: 0.1694
2026-01-03 13:35:40,321: t15.2024.06.14 val PER: 0.1767
2026-01-03 13:35:40,321: t15.2024.07.19 val PER: 0.2393
2026-01-03 13:35:40,321: t15.2024.07.21 val PER: 0.1021
2026-01-03 13:35:40,321: t15.2024.07.28 val PER: 0.1368
2026-01-03 13:35:40,321: t15.2025.01.10 val PER: 0.3072
2026-01-03 13:35:40,321: t15.2025.01.12 val PER: 0.1478
2026-01-03 13:35:40,321: t15.2025.03.14 val PER: 0.3343
2026-01-03 13:35:40,321: t15.2025.03.16 val PER: 0.1872
2026-01-03 13:35:40,321: t15.2025.03.30 val PER: 0.3034
2026-01-03 13:35:40,321: t15.2025.04.13 val PER: 0.2254
2026-01-03 13:35:40,586: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_13000
2026-01-03 13:35:57,899: Train batch 13200: loss: 13.16 grad norm: 59.68 time: 0.054
2026-01-03 13:36:15,107: Train batch 13400: loss: 9.79 grad norm: 53.94 time: 0.063
2026-01-03 13:36:23,779: Running test after training batch: 13500
2026-01-03 13:36:23,892: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:36:28,602: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:36:28,636: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost sette
2026-01-03 13:36:30,441: Val batch 13500: PER (avg): 0.1562 CTC Loss (avg): 15.7418 WER(1gram): 48.73% (n=64) time: 6.662
2026-01-03 13:36:30,442: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 13:36:30,442: t15.2023.08.13 val PER: 0.1216
2026-01-03 13:36:30,442: t15.2023.08.18 val PER: 0.1081
2026-01-03 13:36:30,442: t15.2023.08.20 val PER: 0.1112
2026-01-03 13:36:30,442: t15.2023.08.25 val PER: 0.0904
2026-01-03 13:36:30,442: t15.2023.08.27 val PER: 0.1913
2026-01-03 13:36:30,442: t15.2023.09.01 val PER: 0.0836
2026-01-03 13:36:30,442: t15.2023.09.03 val PER: 0.1734
2026-01-03 13:36:30,442: t15.2023.09.24 val PER: 0.1286
2026-01-03 13:36:30,443: t15.2023.09.29 val PER: 0.1366
2026-01-03 13:36:30,443: t15.2023.10.01 val PER: 0.1744
2026-01-03 13:36:30,443: t15.2023.10.06 val PER: 0.0861
2026-01-03 13:36:30,443: t15.2023.10.08 val PER: 0.2503
2026-01-03 13:36:30,443: t15.2023.10.13 val PER: 0.2141
2026-01-03 13:36:30,443: t15.2023.10.15 val PER: 0.1628
2026-01-03 13:36:30,443: t15.2023.10.20 val PER: 0.2013
2026-01-03 13:36:30,443: t15.2023.10.22 val PER: 0.1158
2026-01-03 13:36:30,444: t15.2023.11.03 val PER: 0.1811
2026-01-03 13:36:30,444: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:36:30,444: t15.2023.11.17 val PER: 0.0435
2026-01-03 13:36:30,444: t15.2023.11.19 val PER: 0.0399
2026-01-03 13:36:30,444: t15.2023.11.26 val PER: 0.1326
2026-01-03 13:36:30,444: t15.2023.12.03 val PER: 0.1187
2026-01-03 13:36:30,444: t15.2023.12.08 val PER: 0.1092
2026-01-03 13:36:30,444: t15.2023.12.10 val PER: 0.0959
2026-01-03 13:36:30,444: t15.2023.12.17 val PER: 0.1310
2026-01-03 13:36:30,444: t15.2023.12.29 val PER: 0.1448
2026-01-03 13:36:30,444: t15.2024.02.25 val PER: 0.1208
2026-01-03 13:36:30,444: t15.2024.03.08 val PER: 0.2461
2026-01-03 13:36:30,444: t15.2024.03.15 val PER: 0.2076
2026-01-03 13:36:30,445: t15.2024.03.17 val PER: 0.1444
2026-01-03 13:36:30,445: t15.2024.05.10 val PER: 0.1738
2026-01-03 13:36:30,445: t15.2024.06.14 val PER: 0.1751
2026-01-03 13:36:30,445: t15.2024.07.19 val PER: 0.2406
2026-01-03 13:36:30,445: t15.2024.07.21 val PER: 0.1021
2026-01-03 13:36:30,445: t15.2024.07.28 val PER: 0.1324
2026-01-03 13:36:30,445: t15.2025.01.10 val PER: 0.3058
2026-01-03 13:36:30,445: t15.2025.01.12 val PER: 0.1470
2026-01-03 13:36:30,445: t15.2025.03.14 val PER: 0.3343
2026-01-03 13:36:30,445: t15.2025.03.16 val PER: 0.1885
2026-01-03 13:36:30,445: t15.2025.03.30 val PER: 0.2989
2026-01-03 13:36:30,445: t15.2025.04.13 val PER: 0.2254
2026-01-03 13:36:30,713: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_13500
2026-01-03 13:36:39,393: Train batch 13600: loss: 13.42 grad norm: 63.29 time: 0.062
2026-01-03 13:36:57,008: Train batch 13800: loss: 9.40 grad norm: 54.29 time: 0.056
2026-01-03 13:37:14,569: Train batch 14000: loss: 12.87 grad norm: 56.60 time: 0.050
2026-01-03 13:37:14,570: Running test after training batch: 14000
2026-01-03 13:37:14,732: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:37:19,442: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:37:19,477: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 13:37:21,300: Val batch 14000: PER (avg): 0.1550 CTC Loss (avg): 15.6648 WER(1gram): 48.48% (n=64) time: 6.730
2026-01-03 13:37:21,300: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 13:37:21,300: t15.2023.08.13 val PER: 0.1175
2026-01-03 13:37:21,301: t15.2023.08.18 val PER: 0.1115
2026-01-03 13:37:21,301: t15.2023.08.20 val PER: 0.1112
2026-01-03 13:37:21,301: t15.2023.08.25 val PER: 0.0904
2026-01-03 13:37:21,301: t15.2023.08.27 val PER: 0.1929
2026-01-03 13:37:21,301: t15.2023.09.01 val PER: 0.0812
2026-01-03 13:37:21,301: t15.2023.09.03 val PER: 0.1627
2026-01-03 13:37:21,301: t15.2023.09.24 val PER: 0.1274
2026-01-03 13:37:21,301: t15.2023.09.29 val PER: 0.1327
2026-01-03 13:37:21,301: t15.2023.10.01 val PER: 0.1737
2026-01-03 13:37:21,301: t15.2023.10.06 val PER: 0.0861
2026-01-03 13:37:21,301: t15.2023.10.08 val PER: 0.2490
2026-01-03 13:37:21,302: t15.2023.10.13 val PER: 0.2164
2026-01-03 13:37:21,302: t15.2023.10.15 val PER: 0.1595
2026-01-03 13:37:21,302: t15.2023.10.20 val PER: 0.1980
2026-01-03 13:37:21,302: t15.2023.10.22 val PER: 0.1125
2026-01-03 13:37:21,302: t15.2023.11.03 val PER: 0.1750
2026-01-03 13:37:21,302: t15.2023.11.04 val PER: 0.0307
2026-01-03 13:37:21,302: t15.2023.11.17 val PER: 0.0435
2026-01-03 13:37:21,302: t15.2023.11.19 val PER: 0.0359
2026-01-03 13:37:21,303: t15.2023.11.26 val PER: 0.1312
2026-01-03 13:37:21,303: t15.2023.12.03 val PER: 0.1229
2026-01-03 13:37:21,303: t15.2023.12.08 val PER: 0.1085
2026-01-03 13:37:21,303: t15.2023.12.10 val PER: 0.0986
2026-01-03 13:37:21,303: t15.2023.12.17 val PER: 0.1299
2026-01-03 13:37:21,303: t15.2023.12.29 val PER: 0.1414
2026-01-03 13:37:21,303: t15.2024.02.25 val PER: 0.1236
2026-01-03 13:37:21,303: t15.2024.03.08 val PER: 0.2432
2026-01-03 13:37:21,303: t15.2024.03.15 val PER: 0.2083
2026-01-03 13:37:21,303: t15.2024.03.17 val PER: 0.1485
2026-01-03 13:37:21,303: t15.2024.05.10 val PER: 0.1768
2026-01-03 13:37:21,303: t15.2024.06.14 val PER: 0.1703
2026-01-03 13:37:21,303: t15.2024.07.19 val PER: 0.2406
2026-01-03 13:37:21,303: t15.2024.07.21 val PER: 0.1014
2026-01-03 13:37:21,304: t15.2024.07.28 val PER: 0.1368
2026-01-03 13:37:21,304: t15.2025.01.10 val PER: 0.2975
2026-01-03 13:37:21,304: t15.2025.01.12 val PER: 0.1455
2026-01-03 13:37:21,304: t15.2025.03.14 val PER: 0.3358
2026-01-03 13:37:21,304: t15.2025.03.16 val PER: 0.1806
2026-01-03 13:37:21,304: t15.2025.03.30 val PER: 0.2989
2026-01-03 13:37:21,304: t15.2025.04.13 val PER: 0.2183
2026-01-03 13:37:21,571: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_14000
2026-01-03 13:37:38,993: Train batch 14200: loss: 8.51 grad norm: 46.27 time: 0.056
2026-01-03 13:37:56,585: Train batch 14400: loss: 6.40 grad norm: 39.95 time: 0.064
2026-01-03 13:38:05,525: Running test after training batch: 14500
2026-01-03 13:38:05,631: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:38:10,575: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:38:10,610: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost sette
2026-01-03 13:38:12,437: Val batch 14500: PER (avg): 0.1558 CTC Loss (avg): 15.6796 WER(1gram): 47.46% (n=64) time: 6.912
2026-01-03 13:38:12,437: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 13:38:12,437: t15.2023.08.13 val PER: 0.1164
2026-01-03 13:38:12,437: t15.2023.08.18 val PER: 0.1106
2026-01-03 13:38:12,437: t15.2023.08.20 val PER: 0.1104
2026-01-03 13:38:12,438: t15.2023.08.25 val PER: 0.0873
2026-01-03 13:38:12,438: t15.2023.08.27 val PER: 0.1881
2026-01-03 13:38:12,438: t15.2023.09.01 val PER: 0.0828
2026-01-03 13:38:12,438: t15.2023.09.03 val PER: 0.1710
2026-01-03 13:38:12,438: t15.2023.09.24 val PER: 0.1262
2026-01-03 13:38:12,438: t15.2023.09.29 val PER: 0.1378
2026-01-03 13:38:12,438: t15.2023.10.01 val PER: 0.1737
2026-01-03 13:38:12,438: t15.2023.10.06 val PER: 0.0840
2026-01-03 13:38:12,438: t15.2023.10.08 val PER: 0.2476
2026-01-03 13:38:12,438: t15.2023.10.13 val PER: 0.2126
2026-01-03 13:38:12,438: t15.2023.10.15 val PER: 0.1622
2026-01-03 13:38:12,439: t15.2023.10.20 val PER: 0.1980
2026-01-03 13:38:12,439: t15.2023.10.22 val PER: 0.1158
2026-01-03 13:38:12,439: t15.2023.11.03 val PER: 0.1757
2026-01-03 13:38:12,439: t15.2023.11.04 val PER: 0.0307
2026-01-03 13:38:12,439: t15.2023.11.17 val PER: 0.0451
2026-01-03 13:38:12,439: t15.2023.11.19 val PER: 0.0359
2026-01-03 13:38:12,439: t15.2023.11.26 val PER: 0.1312
2026-01-03 13:38:12,439: t15.2023.12.03 val PER: 0.1176
2026-01-03 13:38:12,439: t15.2023.12.08 val PER: 0.1092
2026-01-03 13:38:12,439: t15.2023.12.10 val PER: 0.0920
2026-01-03 13:38:12,439: t15.2023.12.17 val PER: 0.1372
2026-01-03 13:38:12,439: t15.2023.12.29 val PER: 0.1434
2026-01-03 13:38:12,439: t15.2024.02.25 val PER: 0.1264
2026-01-03 13:38:12,439: t15.2024.03.08 val PER: 0.2418
2026-01-03 13:38:12,439: t15.2024.03.15 val PER: 0.2076
2026-01-03 13:38:12,439: t15.2024.03.17 val PER: 0.1450
2026-01-03 13:38:12,440: t15.2024.05.10 val PER: 0.1738
2026-01-03 13:38:12,440: t15.2024.06.14 val PER: 0.1751
2026-01-03 13:38:12,440: t15.2024.07.19 val PER: 0.2432
2026-01-03 13:38:12,440: t15.2024.07.21 val PER: 0.1028
2026-01-03 13:38:12,440: t15.2024.07.28 val PER: 0.1368
2026-01-03 13:38:12,440: t15.2025.01.10 val PER: 0.3072
2026-01-03 13:38:12,440: t15.2025.01.12 val PER: 0.1470
2026-01-03 13:38:12,440: t15.2025.03.14 val PER: 0.3388
2026-01-03 13:38:12,440: t15.2025.03.16 val PER: 0.1806
2026-01-03 13:38:12,440: t15.2025.03.30 val PER: 0.3046
2026-01-03 13:38:12,440: t15.2025.04.13 val PER: 0.2254
2026-01-03 13:38:12,706: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_14500
2026-01-03 13:38:21,458: Train batch 14600: loss: 13.69 grad norm: 60.41 time: 0.058
2026-01-03 13:38:38,813: Train batch 14800: loss: 6.30 grad norm: 42.53 time: 0.050
2026-01-03 13:38:55,822: Train batch 15000: loss: 10.22 grad norm: 51.67 time: 0.052
2026-01-03 13:38:55,822: Running test after training batch: 15000
2026-01-03 13:38:55,948: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:39:00,684: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:39:00,718: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 13:39:02,568: Val batch 15000: PER (avg): 0.1559 CTC Loss (avg): 15.6487 WER(1gram): 47.97% (n=64) time: 6.745
2026-01-03 13:39:02,568: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 13:39:02,569: t15.2023.08.13 val PER: 0.1175
2026-01-03 13:39:02,569: t15.2023.08.18 val PER: 0.1106
2026-01-03 13:39:02,569: t15.2023.08.20 val PER: 0.1096
2026-01-03 13:39:02,569: t15.2023.08.25 val PER: 0.0904
2026-01-03 13:39:02,569: t15.2023.08.27 val PER: 0.1929
2026-01-03 13:39:02,569: t15.2023.09.01 val PER: 0.0852
2026-01-03 13:39:02,569: t15.2023.09.03 val PER: 0.1627
2026-01-03 13:39:02,569: t15.2023.09.24 val PER: 0.1274
2026-01-03 13:39:02,569: t15.2023.09.29 val PER: 0.1353
2026-01-03 13:39:02,569: t15.2023.10.01 val PER: 0.1744
2026-01-03 13:39:02,569: t15.2023.10.06 val PER: 0.0861
2026-01-03 13:39:02,570: t15.2023.10.08 val PER: 0.2449
2026-01-03 13:39:02,570: t15.2023.10.13 val PER: 0.2164
2026-01-03 13:39:02,570: t15.2023.10.15 val PER: 0.1615
2026-01-03 13:39:02,570: t15.2023.10.20 val PER: 0.2013
2026-01-03 13:39:02,570: t15.2023.10.22 val PER: 0.1158
2026-01-03 13:39:02,570: t15.2023.11.03 val PER: 0.1805
2026-01-03 13:39:02,570: t15.2023.11.04 val PER: 0.0307
2026-01-03 13:39:02,570: t15.2023.11.17 val PER: 0.0420
2026-01-03 13:39:02,570: t15.2023.11.19 val PER: 0.0379
2026-01-03 13:39:02,570: t15.2023.11.26 val PER: 0.1312
2026-01-03 13:39:02,570: t15.2023.12.03 val PER: 0.1229
2026-01-03 13:39:02,570: t15.2023.12.08 val PER: 0.1052
2026-01-03 13:39:02,570: t15.2023.12.10 val PER: 0.0972
2026-01-03 13:39:02,570: t15.2023.12.17 val PER: 0.1403
2026-01-03 13:39:02,571: t15.2023.12.29 val PER: 0.1434
2026-01-03 13:39:02,571: t15.2024.02.25 val PER: 0.1236
2026-01-03 13:39:02,571: t15.2024.03.08 val PER: 0.2432
2026-01-03 13:39:02,571: t15.2024.03.15 val PER: 0.2101
2026-01-03 13:39:02,571: t15.2024.03.17 val PER: 0.1457
2026-01-03 13:39:02,571: t15.2024.05.10 val PER: 0.1753
2026-01-03 13:39:02,571: t15.2024.06.14 val PER: 0.1703
2026-01-03 13:39:02,571: t15.2024.07.19 val PER: 0.2353
2026-01-03 13:39:02,571: t15.2024.07.21 val PER: 0.1028
2026-01-03 13:39:02,571: t15.2024.07.28 val PER: 0.1368
2026-01-03 13:39:02,571: t15.2025.01.10 val PER: 0.3072
2026-01-03 13:39:02,571: t15.2025.01.12 val PER: 0.1432
2026-01-03 13:39:02,571: t15.2025.03.14 val PER: 0.3447
2026-01-03 13:39:02,571: t15.2025.03.16 val PER: 0.1885
2026-01-03 13:39:02,571: t15.2025.03.30 val PER: 0.3000
2026-01-03 13:39:02,571: t15.2025.04.13 val PER: 0.2197
2026-01-03 13:39:02,838: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_15000
2026-01-03 13:39:20,525: Train batch 15200: loss: 5.37 grad norm: 41.13 time: 0.057
2026-01-03 13:39:37,925: Train batch 15400: loss: 12.66 grad norm: 57.96 time: 0.049
2026-01-03 13:39:46,752: Running test after training batch: 15500
2026-01-03 13:39:46,861: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:39:51,715: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:39:51,748: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 13:39:53,569: Val batch 15500: PER (avg): 0.1552 CTC Loss (avg): 15.6468 WER(1gram): 48.22% (n=64) time: 6.816
2026-01-03 13:39:53,569: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 13:39:53,569: t15.2023.08.13 val PER: 0.1175
2026-01-03 13:39:53,569: t15.2023.08.18 val PER: 0.1081
2026-01-03 13:39:53,569: t15.2023.08.20 val PER: 0.1120
2026-01-03 13:39:53,570: t15.2023.08.25 val PER: 0.0904
2026-01-03 13:39:53,570: t15.2023.08.27 val PER: 0.1929
2026-01-03 13:39:53,570: t15.2023.09.01 val PER: 0.0877
2026-01-03 13:39:53,570: t15.2023.09.03 val PER: 0.1639
2026-01-03 13:39:53,570: t15.2023.09.24 val PER: 0.1250
2026-01-03 13:39:53,570: t15.2023.09.29 val PER: 0.1347
2026-01-03 13:39:53,570: t15.2023.10.01 val PER: 0.1737
2026-01-03 13:39:53,570: t15.2023.10.06 val PER: 0.0883
2026-01-03 13:39:53,570: t15.2023.10.08 val PER: 0.2422
2026-01-03 13:39:53,570: t15.2023.10.13 val PER: 0.2188
2026-01-03 13:39:53,570: t15.2023.10.15 val PER: 0.1628
2026-01-03 13:39:53,570: t15.2023.10.20 val PER: 0.2013
2026-01-03 13:39:53,570: t15.2023.10.22 val PER: 0.1114
2026-01-03 13:39:53,570: t15.2023.11.03 val PER: 0.1784
2026-01-03 13:39:53,571: t15.2023.11.04 val PER: 0.0307
2026-01-03 13:39:53,571: t15.2023.11.17 val PER: 0.0435
2026-01-03 13:39:53,571: t15.2023.11.19 val PER: 0.0359
2026-01-03 13:39:53,571: t15.2023.11.26 val PER: 0.1297
2026-01-03 13:39:53,571: t15.2023.12.03 val PER: 0.1208
2026-01-03 13:39:53,571: t15.2023.12.08 val PER: 0.1012
2026-01-03 13:39:53,571: t15.2023.12.10 val PER: 0.0920
2026-01-03 13:39:53,571: t15.2023.12.17 val PER: 0.1393
2026-01-03 13:39:53,571: t15.2023.12.29 val PER: 0.1400
2026-01-03 13:39:53,571: t15.2024.02.25 val PER: 0.1222
2026-01-03 13:39:53,571: t15.2024.03.08 val PER: 0.2418
2026-01-03 13:39:53,571: t15.2024.03.15 val PER: 0.2083
2026-01-03 13:39:53,571: t15.2024.03.17 val PER: 0.1457
2026-01-03 13:39:53,571: t15.2024.05.10 val PER: 0.1709
2026-01-03 13:39:53,571: t15.2024.06.14 val PER: 0.1688
2026-01-03 13:39:53,571: t15.2024.07.19 val PER: 0.2399
2026-01-03 13:39:53,571: t15.2024.07.21 val PER: 0.1000
2026-01-03 13:39:53,572: t15.2024.07.28 val PER: 0.1331
2026-01-03 13:39:53,572: t15.2025.01.10 val PER: 0.3072
2026-01-03 13:39:53,572: t15.2025.01.12 val PER: 0.1432
2026-01-03 13:39:53,572: t15.2025.03.14 val PER: 0.3462
2026-01-03 13:39:53,572: t15.2025.03.16 val PER: 0.1911
2026-01-03 13:39:53,572: t15.2025.03.30 val PER: 0.2977
2026-01-03 13:39:53,572: t15.2025.04.13 val PER: 0.2211
2026-01-03 13:39:53,837: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_15500
2026-01-03 13:40:02,657: Train batch 15600: loss: 13.41 grad norm: 58.52 time: 0.063
2026-01-03 13:40:20,115: Train batch 15800: loss: 15.71 grad norm: 64.73 time: 0.067
2026-01-03 13:40:37,852: Train batch 16000: loss: 9.63 grad norm: 46.55 time: 0.055
2026-01-03 13:40:37,852: Running test after training batch: 16000
2026-01-03 13:40:37,996: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:40:42,721: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:40:42,756: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost sette
2026-01-03 13:40:44,626: Val batch 16000: PER (avg): 0.1551 CTC Loss (avg): 15.6538 WER(1gram): 47.72% (n=64) time: 6.774
2026-01-03 13:40:44,627: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 13:40:44,627: t15.2023.08.13 val PER: 0.1164
2026-01-03 13:40:44,627: t15.2023.08.18 val PER: 0.1081
2026-01-03 13:40:44,627: t15.2023.08.20 val PER: 0.1120
2026-01-03 13:40:44,627: t15.2023.08.25 val PER: 0.0889
2026-01-03 13:40:44,627: t15.2023.08.27 val PER: 0.1865
2026-01-03 13:40:44,627: t15.2023.09.01 val PER: 0.0828
2026-01-03 13:40:44,627: t15.2023.09.03 val PER: 0.1651
2026-01-03 13:40:44,627: t15.2023.09.24 val PER: 0.1250
2026-01-03 13:40:44,627: t15.2023.09.29 val PER: 0.1353
2026-01-03 13:40:44,628: t15.2023.10.01 val PER: 0.1744
2026-01-03 13:40:44,628: t15.2023.10.06 val PER: 0.0850
2026-01-03 13:40:44,628: t15.2023.10.08 val PER: 0.2449
2026-01-03 13:40:44,628: t15.2023.10.13 val PER: 0.2180
2026-01-03 13:40:44,628: t15.2023.10.15 val PER: 0.1602
2026-01-03 13:40:44,628: t15.2023.10.20 val PER: 0.1946
2026-01-03 13:40:44,628: t15.2023.10.22 val PER: 0.1114
2026-01-03 13:40:44,628: t15.2023.11.03 val PER: 0.1811
2026-01-03 13:40:44,628: t15.2023.11.04 val PER: 0.0307
2026-01-03 13:40:44,628: t15.2023.11.17 val PER: 0.0451
2026-01-03 13:40:44,628: t15.2023.11.19 val PER: 0.0359
2026-01-03 13:40:44,628: t15.2023.11.26 val PER: 0.1283
2026-01-03 13:40:44,628: t15.2023.12.03 val PER: 0.1208
2026-01-03 13:40:44,628: t15.2023.12.08 val PER: 0.1039
2026-01-03 13:40:44,628: t15.2023.12.10 val PER: 0.0920
2026-01-03 13:40:44,629: t15.2023.12.17 val PER: 0.1372
2026-01-03 13:40:44,629: t15.2023.12.29 val PER: 0.1386
2026-01-03 13:40:44,629: t15.2024.02.25 val PER: 0.1222
2026-01-03 13:40:44,629: t15.2024.03.08 val PER: 0.2418
2026-01-03 13:40:44,629: t15.2024.03.15 val PER: 0.2064
2026-01-03 13:40:44,629: t15.2024.03.17 val PER: 0.1450
2026-01-03 13:40:44,629: t15.2024.05.10 val PER: 0.1709
2026-01-03 13:40:44,629: t15.2024.06.14 val PER: 0.1703
2026-01-03 13:40:44,629: t15.2024.07.19 val PER: 0.2419
2026-01-03 13:40:44,629: t15.2024.07.21 val PER: 0.1021
2026-01-03 13:40:44,629: t15.2024.07.28 val PER: 0.1353
2026-01-03 13:40:44,629: t15.2025.01.10 val PER: 0.3072
2026-01-03 13:40:44,629: t15.2025.01.12 val PER: 0.1440
2026-01-03 13:40:44,629: t15.2025.03.14 val PER: 0.3447
2026-01-03 13:40:44,629: t15.2025.03.16 val PER: 0.1950
2026-01-03 13:40:44,629: t15.2025.03.30 val PER: 0.3011
2026-01-03 13:40:44,629: t15.2025.04.13 val PER: 0.2225
2026-01-03 13:40:44,896: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_16000
2026-01-03 13:41:02,609: Train batch 16200: loss: 7.36 grad norm: 41.05 time: 0.055
2026-01-03 13:41:20,119: Train batch 16400: loss: 11.51 grad norm: 63.73 time: 0.057
2026-01-03 13:41:29,077: Running test after training batch: 16500
2026-01-03 13:41:29,200: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:41:33,910: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:41:33,944: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 13:41:35,830: Val batch 16500: PER (avg): 0.1559 CTC Loss (avg): 15.6513 WER(1gram): 47.97% (n=64) time: 6.752
2026-01-03 13:41:35,830: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 13:41:35,830: t15.2023.08.13 val PER: 0.1175
2026-01-03 13:41:35,831: t15.2023.08.18 val PER: 0.1106
2026-01-03 13:41:35,831: t15.2023.08.20 val PER: 0.1104
2026-01-03 13:41:35,831: t15.2023.08.25 val PER: 0.0889
2026-01-03 13:41:35,831: t15.2023.08.27 val PER: 0.1929
2026-01-03 13:41:35,831: t15.2023.09.01 val PER: 0.0836
2026-01-03 13:41:35,831: t15.2023.09.03 val PER: 0.1663
2026-01-03 13:41:35,831: t15.2023.09.24 val PER: 0.1262
2026-01-03 13:41:35,831: t15.2023.09.29 val PER: 0.1334
2026-01-03 13:41:35,831: t15.2023.10.01 val PER: 0.1770
2026-01-03 13:41:35,831: t15.2023.10.06 val PER: 0.0872
2026-01-03 13:41:35,831: t15.2023.10.08 val PER: 0.2449
2026-01-03 13:41:35,831: t15.2023.10.13 val PER: 0.2188
2026-01-03 13:41:35,831: t15.2023.10.15 val PER: 0.1635
2026-01-03 13:41:35,832: t15.2023.10.20 val PER: 0.2047
2026-01-03 13:41:35,832: t15.2023.10.22 val PER: 0.1125
2026-01-03 13:41:35,832: t15.2023.11.03 val PER: 0.1798
2026-01-03 13:41:35,832: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:41:35,832: t15.2023.11.17 val PER: 0.0435
2026-01-03 13:41:35,832: t15.2023.11.19 val PER: 0.0359
2026-01-03 13:41:35,832: t15.2023.11.26 val PER: 0.1290
2026-01-03 13:41:35,832: t15.2023.12.03 val PER: 0.1239
2026-01-03 13:41:35,832: t15.2023.12.08 val PER: 0.1045
2026-01-03 13:41:35,832: t15.2023.12.10 val PER: 0.0920
2026-01-03 13:41:35,832: t15.2023.12.17 val PER: 0.1383
2026-01-03 13:41:35,832: t15.2023.12.29 val PER: 0.1407
2026-01-03 13:41:35,833: t15.2024.02.25 val PER: 0.1236
2026-01-03 13:41:35,833: t15.2024.03.08 val PER: 0.2432
2026-01-03 13:41:35,833: t15.2024.03.15 val PER: 0.2070
2026-01-03 13:41:35,833: t15.2024.03.17 val PER: 0.1450
2026-01-03 13:41:35,833: t15.2024.05.10 val PER: 0.1753
2026-01-03 13:41:35,833: t15.2024.06.14 val PER: 0.1688
2026-01-03 13:41:35,833: t15.2024.07.19 val PER: 0.2432
2026-01-03 13:41:35,833: t15.2024.07.21 val PER: 0.1014
2026-01-03 13:41:35,833: t15.2024.07.28 val PER: 0.1353
2026-01-03 13:41:35,833: t15.2025.01.10 val PER: 0.3030
2026-01-03 13:41:35,833: t15.2025.01.12 val PER: 0.1455
2026-01-03 13:41:35,833: t15.2025.03.14 val PER: 0.3402
2026-01-03 13:41:35,833: t15.2025.03.16 val PER: 0.1950
2026-01-03 13:41:35,833: t15.2025.03.30 val PER: 0.3000
2026-01-03 13:41:35,833: t15.2025.04.13 val PER: 0.2254
2026-01-03 13:41:36,098: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_16500
2026-01-03 13:41:44,858: Train batch 16600: loss: 9.55 grad norm: 52.23 time: 0.052
2026-01-03 13:42:02,681: Train batch 16800: loss: 17.95 grad norm: 70.52 time: 0.061
2026-01-03 13:42:20,163: Train batch 17000: loss: 9.45 grad norm: 48.63 time: 0.081
2026-01-03 13:42:20,163: Running test after training batch: 17000
2026-01-03 13:42:20,255: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:42:25,360: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:42:25,395: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 13:42:27,295: Val batch 17000: PER (avg): 0.1551 CTC Loss (avg): 15.6316 WER(1gram): 48.48% (n=64) time: 7.131
2026-01-03 13:42:27,295: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 13:42:27,295: t15.2023.08.13 val PER: 0.1175
2026-01-03 13:42:27,295: t15.2023.08.18 val PER: 0.1098
2026-01-03 13:42:27,295: t15.2023.08.20 val PER: 0.1104
2026-01-03 13:42:27,295: t15.2023.08.25 val PER: 0.0904
2026-01-03 13:42:27,295: t15.2023.08.27 val PER: 0.1945
2026-01-03 13:42:27,295: t15.2023.09.01 val PER: 0.0820
2026-01-03 13:42:27,295: t15.2023.09.03 val PER: 0.1686
2026-01-03 13:42:27,296: t15.2023.09.24 val PER: 0.1250
2026-01-03 13:42:27,296: t15.2023.09.29 val PER: 0.1321
2026-01-03 13:42:27,296: t15.2023.10.01 val PER: 0.1757
2026-01-03 13:42:27,296: t15.2023.10.06 val PER: 0.0893
2026-01-03 13:42:27,296: t15.2023.10.08 val PER: 0.2449
2026-01-03 13:42:27,296: t15.2023.10.13 val PER: 0.2172
2026-01-03 13:42:27,296: t15.2023.10.15 val PER: 0.1608
2026-01-03 13:42:27,296: t15.2023.10.20 val PER: 0.1946
2026-01-03 13:42:27,296: t15.2023.10.22 val PER: 0.1125
2026-01-03 13:42:27,296: t15.2023.11.03 val PER: 0.1784
2026-01-03 13:42:27,296: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:42:27,296: t15.2023.11.17 val PER: 0.0420
2026-01-03 13:42:27,296: t15.2023.11.19 val PER: 0.0379
2026-01-03 13:42:27,296: t15.2023.11.26 val PER: 0.1283
2026-01-03 13:42:27,297: t15.2023.12.03 val PER: 0.1197
2026-01-03 13:42:27,297: t15.2023.12.08 val PER: 0.1019
2026-01-03 13:42:27,297: t15.2023.12.10 val PER: 0.0933
2026-01-03 13:42:27,297: t15.2023.12.17 val PER: 0.1393
2026-01-03 13:42:27,297: t15.2023.12.29 val PER: 0.1421
2026-01-03 13:42:27,297: t15.2024.02.25 val PER: 0.1236
2026-01-03 13:42:27,297: t15.2024.03.08 val PER: 0.2361
2026-01-03 13:42:27,297: t15.2024.03.15 val PER: 0.2089
2026-01-03 13:42:27,297: t15.2024.03.17 val PER: 0.1444
2026-01-03 13:42:27,297: t15.2024.05.10 val PER: 0.1753
2026-01-03 13:42:27,297: t15.2024.06.14 val PER: 0.1719
2026-01-03 13:42:27,297: t15.2024.07.19 val PER: 0.2386
2026-01-03 13:42:27,297: t15.2024.07.21 val PER: 0.1007
2026-01-03 13:42:27,298: t15.2024.07.28 val PER: 0.1353
2026-01-03 13:42:27,298: t15.2025.01.10 val PER: 0.2975
2026-01-03 13:42:27,298: t15.2025.01.12 val PER: 0.1440
2026-01-03 13:42:27,298: t15.2025.03.14 val PER: 0.3402
2026-01-03 13:42:27,298: t15.2025.03.16 val PER: 0.1937
2026-01-03 13:42:27,298: t15.2025.03.30 val PER: 0.3000
2026-01-03 13:42:27,298: t15.2025.04.13 val PER: 0.2240
2026-01-03 13:42:27,567: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_17000
2026-01-03 13:42:44,549: Train batch 17200: loss: 11.08 grad norm: 50.28 time: 0.083
2026-01-03 13:43:02,081: Train batch 17400: loss: 13.70 grad norm: 61.03 time: 0.070
2026-01-03 13:43:10,629: Running test after training batch: 17500
2026-01-03 13:43:10,722: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:43:15,483: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:43:15,519: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 13:43:17,427: Val batch 17500: PER (avg): 0.1553 CTC Loss (avg): 15.6246 WER(1gram): 47.72% (n=64) time: 6.798
2026-01-03 13:43:17,428: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 13:43:17,428: t15.2023.08.13 val PER: 0.1175
2026-01-03 13:43:17,428: t15.2023.08.18 val PER: 0.1098
2026-01-03 13:43:17,428: t15.2023.08.20 val PER: 0.1088
2026-01-03 13:43:17,428: t15.2023.08.25 val PER: 0.0889
2026-01-03 13:43:17,428: t15.2023.08.27 val PER: 0.1913
2026-01-03 13:43:17,428: t15.2023.09.01 val PER: 0.0820
2026-01-03 13:43:17,428: t15.2023.09.03 val PER: 0.1651
2026-01-03 13:43:17,428: t15.2023.09.24 val PER: 0.1274
2026-01-03 13:43:17,428: t15.2023.09.29 val PER: 0.1327
2026-01-03 13:43:17,428: t15.2023.10.01 val PER: 0.1783
2026-01-03 13:43:17,429: t15.2023.10.06 val PER: 0.0915
2026-01-03 13:43:17,429: t15.2023.10.08 val PER: 0.2436
2026-01-03 13:43:17,429: t15.2023.10.13 val PER: 0.2164
2026-01-03 13:43:17,429: t15.2023.10.15 val PER: 0.1608
2026-01-03 13:43:17,429: t15.2023.10.20 val PER: 0.1946
2026-01-03 13:43:17,429: t15.2023.10.22 val PER: 0.1125
2026-01-03 13:43:17,429: t15.2023.11.03 val PER: 0.1784
2026-01-03 13:43:17,429: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:43:17,429: t15.2023.11.17 val PER: 0.0420
2026-01-03 13:43:17,429: t15.2023.11.19 val PER: 0.0379
2026-01-03 13:43:17,429: t15.2023.11.26 val PER: 0.1283
2026-01-03 13:43:17,429: t15.2023.12.03 val PER: 0.1197
2026-01-03 13:43:17,429: t15.2023.12.08 val PER: 0.1012
2026-01-03 13:43:17,429: t15.2023.12.10 val PER: 0.0907
2026-01-03 13:43:17,429: t15.2023.12.17 val PER: 0.1331
2026-01-03 13:43:17,429: t15.2023.12.29 val PER: 0.1428
2026-01-03 13:43:17,429: t15.2024.02.25 val PER: 0.1250
2026-01-03 13:43:17,429: t15.2024.03.08 val PER: 0.2418
2026-01-03 13:43:17,430: t15.2024.03.15 val PER: 0.2076
2026-01-03 13:43:17,430: t15.2024.03.17 val PER: 0.1492
2026-01-03 13:43:17,430: t15.2024.05.10 val PER: 0.1753
2026-01-03 13:43:17,430: t15.2024.06.14 val PER: 0.1703
2026-01-03 13:43:17,430: t15.2024.07.19 val PER: 0.2406
2026-01-03 13:43:17,430: t15.2024.07.21 val PER: 0.1021
2026-01-03 13:43:17,430: t15.2024.07.28 val PER: 0.1353
2026-01-03 13:43:17,430: t15.2025.01.10 val PER: 0.2961
2026-01-03 13:43:17,430: t15.2025.01.12 val PER: 0.1463
2026-01-03 13:43:17,430: t15.2025.03.14 val PER: 0.3417
2026-01-03 13:43:17,430: t15.2025.03.16 val PER: 0.1937
2026-01-03 13:43:17,430: t15.2025.03.30 val PER: 0.3000
2026-01-03 13:43:17,430: t15.2025.04.13 val PER: 0.2282
2026-01-03 13:43:17,701: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_17500
2026-01-03 13:43:34,440: Train batch 17600: loss: 11.78 grad norm: 56.60 time: 0.050
2026-01-03 13:43:52,428: Train batch 17800: loss: 7.70 grad norm: 54.91 time: 0.041
2026-01-03 13:44:09,882: Train batch 18000: loss: 13.16 grad norm: 68.80 time: 0.060
2026-01-03 13:44:09,882: Running test after training batch: 18000
2026-01-03 13:44:09,985: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:44:15,262: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:44:15,298: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 13:44:17,224: Val batch 18000: PER (avg): 0.1554 CTC Loss (avg): 15.6174 WER(1gram): 47.97% (n=64) time: 7.341
2026-01-03 13:44:17,224: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 13:44:17,224: t15.2023.08.13 val PER: 0.1185
2026-01-03 13:44:17,224: t15.2023.08.18 val PER: 0.1115
2026-01-03 13:44:17,224: t15.2023.08.20 val PER: 0.1120
2026-01-03 13:44:17,224: t15.2023.08.25 val PER: 0.0904
2026-01-03 13:44:17,224: t15.2023.08.27 val PER: 0.1929
2026-01-03 13:44:17,225: t15.2023.09.01 val PER: 0.0828
2026-01-03 13:44:17,225: t15.2023.09.03 val PER: 0.1639
2026-01-03 13:44:17,225: t15.2023.09.24 val PER: 0.1262
2026-01-03 13:44:17,225: t15.2023.09.29 val PER: 0.1321
2026-01-03 13:44:17,225: t15.2023.10.01 val PER: 0.1777
2026-01-03 13:44:17,225: t15.2023.10.06 val PER: 0.0893
2026-01-03 13:44:17,225: t15.2023.10.08 val PER: 0.2449
2026-01-03 13:44:17,225: t15.2023.10.13 val PER: 0.2172
2026-01-03 13:44:17,225: t15.2023.10.15 val PER: 0.1628
2026-01-03 13:44:17,225: t15.2023.10.20 val PER: 0.1980
2026-01-03 13:44:17,225: t15.2023.10.22 val PER: 0.1091
2026-01-03 13:44:17,225: t15.2023.11.03 val PER: 0.1784
2026-01-03 13:44:17,225: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:44:17,225: t15.2023.11.17 val PER: 0.0435
2026-01-03 13:44:17,225: t15.2023.11.19 val PER: 0.0379
2026-01-03 13:44:17,225: t15.2023.11.26 val PER: 0.1283
2026-01-03 13:44:17,226: t15.2023.12.03 val PER: 0.1208
2026-01-03 13:44:17,226: t15.2023.12.08 val PER: 0.0999
2026-01-03 13:44:17,226: t15.2023.12.10 val PER: 0.0907
2026-01-03 13:44:17,226: t15.2023.12.17 val PER: 0.1362
2026-01-03 13:44:17,226: t15.2023.12.29 val PER: 0.1407
2026-01-03 13:44:17,226: t15.2024.02.25 val PER: 0.1236
2026-01-03 13:44:17,226: t15.2024.03.08 val PER: 0.2447
2026-01-03 13:44:17,226: t15.2024.03.15 val PER: 0.2064
2026-01-03 13:44:17,226: t15.2024.03.17 val PER: 0.1499
2026-01-03 13:44:17,226: t15.2024.05.10 val PER: 0.1768
2026-01-03 13:44:17,226: t15.2024.06.14 val PER: 0.1688
2026-01-03 13:44:17,226: t15.2024.07.19 val PER: 0.2373
2026-01-03 13:44:17,227: t15.2024.07.21 val PER: 0.1000
2026-01-03 13:44:17,227: t15.2024.07.28 val PER: 0.1324
2026-01-03 13:44:17,227: t15.2025.01.10 val PER: 0.3030
2026-01-03 13:44:17,227: t15.2025.01.12 val PER: 0.1470
2026-01-03 13:44:17,227: t15.2025.03.14 val PER: 0.3402
2026-01-03 13:44:17,227: t15.2025.03.16 val PER: 0.1976
2026-01-03 13:44:17,227: t15.2025.03.30 val PER: 0.2977
2026-01-03 13:44:17,227: t15.2025.04.13 val PER: 0.2297
2026-01-03 13:44:17,492: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_18000
2026-01-03 13:44:35,641: Train batch 18200: loss: 9.38 grad norm: 48.97 time: 0.073
2026-01-03 13:44:53,407: Train batch 18400: loss: 6.08 grad norm: 47.13 time: 0.057
2026-01-03 13:45:02,404: Running test after training batch: 18500
2026-01-03 13:45:02,554: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:45:07,464: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:45:07,500: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 13:45:09,440: Val batch 18500: PER (avg): 0.1551 CTC Loss (avg): 15.6071 WER(1gram): 47.97% (n=64) time: 7.036
2026-01-03 13:45:09,441: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 13:45:09,441: t15.2023.08.13 val PER: 0.1185
2026-01-03 13:45:09,441: t15.2023.08.18 val PER: 0.1115
2026-01-03 13:45:09,441: t15.2023.08.20 val PER: 0.1128
2026-01-03 13:45:09,441: t15.2023.08.25 val PER: 0.0904
2026-01-03 13:45:09,441: t15.2023.08.27 val PER: 0.1929
2026-01-03 13:45:09,441: t15.2023.09.01 val PER: 0.0804
2026-01-03 13:45:09,441: t15.2023.09.03 val PER: 0.1663
2026-01-03 13:45:09,441: t15.2023.09.24 val PER: 0.1274
2026-01-03 13:45:09,441: t15.2023.09.29 val PER: 0.1321
2026-01-03 13:45:09,441: t15.2023.10.01 val PER: 0.1757
2026-01-03 13:45:09,442: t15.2023.10.06 val PER: 0.0850
2026-01-03 13:45:09,442: t15.2023.10.08 val PER: 0.2436
2026-01-03 13:45:09,442: t15.2023.10.13 val PER: 0.2157
2026-01-03 13:45:09,442: t15.2023.10.15 val PER: 0.1608
2026-01-03 13:45:09,442: t15.2023.10.20 val PER: 0.2013
2026-01-03 13:45:09,442: t15.2023.10.22 val PER: 0.1091
2026-01-03 13:45:09,442: t15.2023.11.03 val PER: 0.1805
2026-01-03 13:45:09,442: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:45:09,442: t15.2023.11.17 val PER: 0.0435
2026-01-03 13:45:09,442: t15.2023.11.19 val PER: 0.0379
2026-01-03 13:45:09,443: t15.2023.11.26 val PER: 0.1297
2026-01-03 13:45:09,443: t15.2023.12.03 val PER: 0.1218
2026-01-03 13:45:09,443: t15.2023.12.08 val PER: 0.1012
2026-01-03 13:45:09,443: t15.2023.12.10 val PER: 0.0907
2026-01-03 13:45:09,443: t15.2023.12.17 val PER: 0.1341
2026-01-03 13:45:09,443: t15.2023.12.29 val PER: 0.1434
2026-01-03 13:45:09,443: t15.2024.02.25 val PER: 0.1250
2026-01-03 13:45:09,443: t15.2024.03.08 val PER: 0.2475
2026-01-03 13:45:09,443: t15.2024.03.15 val PER: 0.2064
2026-01-03 13:45:09,443: t15.2024.03.17 val PER: 0.1464
2026-01-03 13:45:09,443: t15.2024.05.10 val PER: 0.1724
2026-01-03 13:45:09,443: t15.2024.06.14 val PER: 0.1672
2026-01-03 13:45:09,443: t15.2024.07.19 val PER: 0.2380
2026-01-03 13:45:09,443: t15.2024.07.21 val PER: 0.1007
2026-01-03 13:45:09,443: t15.2024.07.28 val PER: 0.1324
2026-01-03 13:45:09,443: t15.2025.01.10 val PER: 0.2989
2026-01-03 13:45:09,444: t15.2025.01.12 val PER: 0.1455
2026-01-03 13:45:09,444: t15.2025.03.14 val PER: 0.3388
2026-01-03 13:45:09,444: t15.2025.03.16 val PER: 0.1990
2026-01-03 13:45:09,444: t15.2025.03.30 val PER: 0.2989
2026-01-03 13:45:09,444: t15.2025.04.13 val PER: 0.2268
2026-01-03 13:45:09,733: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_18500
2026-01-03 13:45:18,859: Train batch 18600: loss: 13.96 grad norm: 59.58 time: 0.067
2026-01-03 13:45:37,016: Train batch 18800: loss: 10.08 grad norm: 53.36 time: 0.064
2026-01-03 13:45:54,798: Train batch 19000: loss: 9.70 grad norm: 46.05 time: 0.064
2026-01-03 13:45:54,798: Running test after training batch: 19000
2026-01-03 13:45:54,929: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:46:00,263: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:46:00,299: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 13:46:02,248: Val batch 19000: PER (avg): 0.1547 CTC Loss (avg): 15.6137 WER(1gram): 47.97% (n=64) time: 7.449
2026-01-03 13:46:02,248: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 13:46:02,248: t15.2023.08.13 val PER: 0.1185
2026-01-03 13:46:02,248: t15.2023.08.18 val PER: 0.1090
2026-01-03 13:46:02,248: t15.2023.08.20 val PER: 0.1120
2026-01-03 13:46:02,248: t15.2023.08.25 val PER: 0.0889
2026-01-03 13:46:02,248: t15.2023.08.27 val PER: 0.1913
2026-01-03 13:46:02,249: t15.2023.09.01 val PER: 0.0820
2026-01-03 13:46:02,249: t15.2023.09.03 val PER: 0.1651
2026-01-03 13:46:02,249: t15.2023.09.24 val PER: 0.1262
2026-01-03 13:46:02,249: t15.2023.09.29 val PER: 0.1302
2026-01-03 13:46:02,249: t15.2023.10.01 val PER: 0.1757
2026-01-03 13:46:02,249: t15.2023.10.06 val PER: 0.0850
2026-01-03 13:46:02,249: t15.2023.10.08 val PER: 0.2449
2026-01-03 13:46:02,249: t15.2023.10.13 val PER: 0.2149
2026-01-03 13:46:02,249: t15.2023.10.15 val PER: 0.1635
2026-01-03 13:46:02,249: t15.2023.10.20 val PER: 0.1980
2026-01-03 13:46:02,249: t15.2023.10.22 val PER: 0.1114
2026-01-03 13:46:02,250: t15.2023.11.03 val PER: 0.1811
2026-01-03 13:46:02,250: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:46:02,250: t15.2023.11.17 val PER: 0.0435
2026-01-03 13:46:02,250: t15.2023.11.19 val PER: 0.0379
2026-01-03 13:46:02,250: t15.2023.11.26 val PER: 0.1268
2026-01-03 13:46:02,250: t15.2023.12.03 val PER: 0.1229
2026-01-03 13:46:02,250: t15.2023.12.08 val PER: 0.1019
2026-01-03 13:46:02,250: t15.2023.12.10 val PER: 0.0920
2026-01-03 13:46:02,250: t15.2023.12.17 val PER: 0.1320
2026-01-03 13:46:02,250: t15.2023.12.29 val PER: 0.1428
2026-01-03 13:46:02,250: t15.2024.02.25 val PER: 0.1236
2026-01-03 13:46:02,250: t15.2024.03.08 val PER: 0.2447
2026-01-03 13:46:02,250: t15.2024.03.15 val PER: 0.2045
2026-01-03 13:46:02,250: t15.2024.03.17 val PER: 0.1485
2026-01-03 13:46:02,250: t15.2024.05.10 val PER: 0.1753
2026-01-03 13:46:02,250: t15.2024.06.14 val PER: 0.1672
2026-01-03 13:46:02,251: t15.2024.07.19 val PER: 0.2360
2026-01-03 13:46:02,251: t15.2024.07.21 val PER: 0.1000
2026-01-03 13:46:02,251: t15.2024.07.28 val PER: 0.1353
2026-01-03 13:46:02,251: t15.2025.01.10 val PER: 0.2893
2026-01-03 13:46:02,251: t15.2025.01.12 val PER: 0.1455
2026-01-03 13:46:02,251: t15.2025.03.14 val PER: 0.3388
2026-01-03 13:46:02,251: t15.2025.03.16 val PER: 0.1990
2026-01-03 13:46:02,251: t15.2025.03.30 val PER: 0.2966
2026-01-03 13:46:02,251: t15.2025.04.13 val PER: 0.2282
2026-01-03 13:46:02,637: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_19000
2026-01-03 13:46:20,380: Train batch 19200: loss: 6.97 grad norm: 45.83 time: 0.063
2026-01-03 13:46:38,085: Train batch 19400: loss: 6.27 grad norm: 38.26 time: 0.053
2026-01-03 13:46:46,922: Running test after training batch: 19500
2026-01-03 13:46:47,057: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:46:52,239: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:46:52,276: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 13:46:54,242: Val batch 19500: PER (avg): 0.1549 CTC Loss (avg): 15.6126 WER(1gram): 47.97% (n=64) time: 7.320
2026-01-03 13:46:54,243: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 13:46:54,243: t15.2023.08.13 val PER: 0.1164
2026-01-03 13:46:54,243: t15.2023.08.18 val PER: 0.1106
2026-01-03 13:46:54,243: t15.2023.08.20 val PER: 0.1112
2026-01-03 13:46:54,243: t15.2023.08.25 val PER: 0.0889
2026-01-03 13:46:54,243: t15.2023.08.27 val PER: 0.1897
2026-01-03 13:46:54,243: t15.2023.09.01 val PER: 0.0812
2026-01-03 13:46:54,243: t15.2023.09.03 val PER: 0.1639
2026-01-03 13:46:54,243: t15.2023.09.24 val PER: 0.1286
2026-01-03 13:46:54,243: t15.2023.09.29 val PER: 0.1327
2026-01-03 13:46:54,244: t15.2023.10.01 val PER: 0.1777
2026-01-03 13:46:54,244: t15.2023.10.06 val PER: 0.0861
2026-01-03 13:46:54,244: t15.2023.10.08 val PER: 0.2436
2026-01-03 13:46:54,244: t15.2023.10.13 val PER: 0.2164
2026-01-03 13:46:54,244: t15.2023.10.15 val PER: 0.1628
2026-01-03 13:46:54,244: t15.2023.10.20 val PER: 0.2013
2026-01-03 13:46:54,244: t15.2023.10.22 val PER: 0.1102
2026-01-03 13:46:54,244: t15.2023.11.03 val PER: 0.1805
2026-01-03 13:46:54,244: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:46:54,244: t15.2023.11.17 val PER: 0.0435
2026-01-03 13:46:54,244: t15.2023.11.19 val PER: 0.0359
2026-01-03 13:46:54,244: t15.2023.11.26 val PER: 0.1283
2026-01-03 13:46:54,245: t15.2023.12.03 val PER: 0.1218
2026-01-03 13:46:54,245: t15.2023.12.08 val PER: 0.1025
2026-01-03 13:46:54,245: t15.2023.12.10 val PER: 0.0933
2026-01-03 13:46:54,245: t15.2023.12.17 val PER: 0.1351
2026-01-03 13:46:54,245: t15.2023.12.29 val PER: 0.1400
2026-01-03 13:46:54,245: t15.2024.02.25 val PER: 0.1236
2026-01-03 13:46:54,245: t15.2024.03.08 val PER: 0.2461
2026-01-03 13:46:54,245: t15.2024.03.15 val PER: 0.2045
2026-01-03 13:46:54,245: t15.2024.03.17 val PER: 0.1471
2026-01-03 13:46:54,245: t15.2024.05.10 val PER: 0.1724
2026-01-03 13:46:54,245: t15.2024.06.14 val PER: 0.1672
2026-01-03 13:46:54,245: t15.2024.07.19 val PER: 0.2386
2026-01-03 13:46:54,245: t15.2024.07.21 val PER: 0.0993
2026-01-03 13:46:54,245: t15.2024.07.28 val PER: 0.1331
2026-01-03 13:46:54,245: t15.2025.01.10 val PER: 0.2920
2026-01-03 13:46:54,245: t15.2025.01.12 val PER: 0.1463
2026-01-03 13:46:54,246: t15.2025.03.14 val PER: 0.3402
2026-01-03 13:46:54,246: t15.2025.03.16 val PER: 0.1990
2026-01-03 13:46:54,246: t15.2025.03.30 val PER: 0.2989
2026-01-03 13:46:54,246: t15.2025.04.13 val PER: 0.2282
2026-01-03 13:46:54,558: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_19500
2026-01-03 13:47:03,623: Train batch 19600: loss: 9.23 grad norm: 51.80 time: 0.056
2026-01-03 13:47:21,484: Train batch 19800: loss: 9.19 grad norm: 54.07 time: 0.054
2026-01-03 13:47:38,932: Running test after training batch: 19999
2026-01-03 13:47:39,028: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:47:44,200: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 13:47:44,230: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 13:47:45,815: Val batch 19999: PER (avg): 0.1548 CTC Loss (avg): 15.6083 WER(1gram): 47.97% (n=64) time: 6.882
2026-01-03 13:47:45,815: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 13:47:45,815: t15.2023.08.13 val PER: 0.1164
2026-01-03 13:47:45,816: t15.2023.08.18 val PER: 0.1106
2026-01-03 13:47:45,816: t15.2023.08.20 val PER: 0.1120
2026-01-03 13:47:45,816: t15.2023.08.25 val PER: 0.0904
2026-01-03 13:47:45,816: t15.2023.08.27 val PER: 0.1913
2026-01-03 13:47:45,816: t15.2023.09.01 val PER: 0.0820
2026-01-03 13:47:45,816: t15.2023.09.03 val PER: 0.1627
2026-01-03 13:47:45,816: t15.2023.09.24 val PER: 0.1262
2026-01-03 13:47:45,816: t15.2023.09.29 val PER: 0.1334
2026-01-03 13:47:45,816: t15.2023.10.01 val PER: 0.1757
2026-01-03 13:47:45,816: t15.2023.10.06 val PER: 0.0872
2026-01-03 13:47:45,816: t15.2023.10.08 val PER: 0.2436
2026-01-03 13:47:45,816: t15.2023.10.13 val PER: 0.2149
2026-01-03 13:47:45,816: t15.2023.10.15 val PER: 0.1641
2026-01-03 13:47:45,817: t15.2023.10.20 val PER: 0.1946
2026-01-03 13:47:45,817: t15.2023.10.22 val PER: 0.1114
2026-01-03 13:47:45,817: t15.2023.11.03 val PER: 0.1811
2026-01-03 13:47:45,817: t15.2023.11.04 val PER: 0.0341
2026-01-03 13:47:45,817: t15.2023.11.17 val PER: 0.0420
2026-01-03 13:47:45,817: t15.2023.11.19 val PER: 0.0359
2026-01-03 13:47:45,817: t15.2023.11.26 val PER: 0.1297
2026-01-03 13:47:45,817: t15.2023.12.03 val PER: 0.1229
2026-01-03 13:47:45,817: t15.2023.12.08 val PER: 0.1019
2026-01-03 13:47:45,817: t15.2023.12.10 val PER: 0.0933
2026-01-03 13:47:45,817: t15.2023.12.17 val PER: 0.1341
2026-01-03 13:47:45,817: t15.2023.12.29 val PER: 0.1407
2026-01-03 13:47:45,817: t15.2024.02.25 val PER: 0.1222
2026-01-03 13:47:45,817: t15.2024.03.08 val PER: 0.2447
2026-01-03 13:47:45,817: t15.2024.03.15 val PER: 0.2014
2026-01-03 13:47:45,817: t15.2024.03.17 val PER: 0.1457
2026-01-03 13:47:45,817: t15.2024.05.10 val PER: 0.1753
2026-01-03 13:47:45,818: t15.2024.06.14 val PER: 0.1703
2026-01-03 13:47:45,818: t15.2024.07.19 val PER: 0.2367
2026-01-03 13:47:45,818: t15.2024.07.21 val PER: 0.1014
2026-01-03 13:47:45,818: t15.2024.07.28 val PER: 0.1346
2026-01-03 13:47:45,818: t15.2025.01.10 val PER: 0.2893
2026-01-03 13:47:45,818: t15.2025.01.12 val PER: 0.1478
2026-01-03 13:47:45,818: t15.2025.03.14 val PER: 0.3388
2026-01-03 13:47:45,818: t15.2025.03.16 val PER: 0.1937
2026-01-03 13:47:45,818: t15.2025.03.30 val PER: 0.3011
2026-01-03 13:47:45,818: t15.2025.04.13 val PER: 0.2254
2026-01-03 13:47:46,108: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step12k_f01/checkpoint/checkpoint_batch_19999
2026-01-03 13:47:46,140: Best avg val PER achieved: 0.16452
2026-01-03 13:47:46,141: Total training time: 34.50 minutes

=== RUN step15k_f01.yaml ===
2026-01-03 13:47:51,766: Using device: cuda:0
2026-01-03 13:47:53,446: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-03 13:47:53,466: Using 45 sessions after filtering (from 45).
2026-01-03 13:47:53,867: Using torch.compile (if available)
2026-01-03 13:47:53,867: torch.compile not available (torch<2.0). Skipping.
2026-01-03 13:47:53,868: Initialized RNN decoding model
2026-01-03 13:47:53,868: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 13:47:53,868: Model has 44,907,305 parameters
2026-01-03 13:47:53,868: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 13:47:55,173: Successfully initialized datasets
2026-01-03 13:47:55,173: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 13:47:56,172: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.227
2026-01-03 13:47:56,172: Running test after training batch: 0
2026-01-03 13:47:56,283: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:48:01,529: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-03 13:48:02,240: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-03 13:48:35,786: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 39.614
2026-01-03 13:48:35,786: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-03 13:48:35,786: t15.2023.08.13 val PER: 1.3056
2026-01-03 13:48:35,786: t15.2023.08.18 val PER: 1.4208
2026-01-03 13:48:35,787: t15.2023.08.20 val PER: 1.3002
2026-01-03 13:48:35,787: t15.2023.08.25 val PER: 1.3389
2026-01-03 13:48:35,787: t15.2023.08.27 val PER: 1.2460
2026-01-03 13:48:35,787: t15.2023.09.01 val PER: 1.4537
2026-01-03 13:48:35,787: t15.2023.09.03 val PER: 1.3171
2026-01-03 13:48:35,787: t15.2023.09.24 val PER: 1.5461
2026-01-03 13:48:35,788: t15.2023.09.29 val PER: 1.4671
2026-01-03 13:48:35,788: t15.2023.10.01 val PER: 1.2147
2026-01-03 13:48:35,788: t15.2023.10.06 val PER: 1.4876
2026-01-03 13:48:35,788: t15.2023.10.08 val PER: 1.1827
2026-01-03 13:48:35,788: t15.2023.10.13 val PER: 1.3964
2026-01-03 13:48:35,788: t15.2023.10.15 val PER: 1.3889
2026-01-03 13:48:35,788: t15.2023.10.20 val PER: 1.4866
2026-01-03 13:48:35,788: t15.2023.10.22 val PER: 1.3942
2026-01-03 13:48:35,788: t15.2023.11.03 val PER: 1.5923
2026-01-03 13:48:35,788: t15.2023.11.04 val PER: 2.0171
2026-01-03 13:48:35,788: t15.2023.11.17 val PER: 1.9518
2026-01-03 13:48:35,788: t15.2023.11.19 val PER: 1.6707
2026-01-03 13:48:35,788: t15.2023.11.26 val PER: 1.5413
2026-01-03 13:48:35,788: t15.2023.12.03 val PER: 1.4254
2026-01-03 13:48:35,788: t15.2023.12.08 val PER: 1.4487
2026-01-03 13:48:35,788: t15.2023.12.10 val PER: 1.6899
2026-01-03 13:48:35,789: t15.2023.12.17 val PER: 1.3077
2026-01-03 13:48:35,789: t15.2023.12.29 val PER: 1.4063
2026-01-03 13:48:35,789: t15.2024.02.25 val PER: 1.4228
2026-01-03 13:48:35,789: t15.2024.03.08 val PER: 1.3257
2026-01-03 13:48:35,789: t15.2024.03.15 val PER: 1.3196
2026-01-03 13:48:35,789: t15.2024.03.17 val PER: 1.4052
2026-01-03 13:48:35,789: t15.2024.05.10 val PER: 1.3224
2026-01-03 13:48:35,789: t15.2024.06.14 val PER: 1.5315
2026-01-03 13:48:35,789: t15.2024.07.19 val PER: 1.0817
2026-01-03 13:48:35,789: t15.2024.07.21 val PER: 1.6290
2026-01-03 13:48:35,789: t15.2024.07.28 val PER: 1.6588
2026-01-03 13:48:35,789: t15.2025.01.10 val PER: 1.0923
2026-01-03 13:48:35,789: t15.2025.01.12 val PER: 1.7629
2026-01-03 13:48:35,789: t15.2025.03.14 val PER: 1.0414
2026-01-03 13:48:35,790: t15.2025.03.16 val PER: 1.6257
2026-01-03 13:48:35,790: t15.2025.03.30 val PER: 1.2874
2026-01-03 13:48:35,790: t15.2025.04.13 val PER: 1.5949
2026-01-03 13:48:35,791: New best val WER(1gram) inf% --> 100.00%
2026-01-03 13:48:35,791: Checkpointing model
2026-01-03 13:48:36,035: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 13:48:36,286: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_0
2026-01-03 13:48:53,923: Train batch 200: loss: 77.58 grad norm: 105.79 time: 0.054
2026-01-03 13:49:10,672: Train batch 400: loss: 54.08 grad norm: 106.76 time: 0.063
2026-01-03 13:49:19,323: Running test after training batch: 500
2026-01-03 13:49:19,457: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:49:24,380: WER debug example
  GT : you can see the code at this point as well
  PR : used and ease thus uhde at this ide is aisles
2026-01-03 13:49:24,412: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus is adz
2026-01-03 13:49:26,684: Val batch 500: PER (avg): 0.5192 CTC Loss (avg): 55.7443 WER(1gram): 89.59% (n=64) time: 7.361
2026-01-03 13:49:26,685: WER lens: avg_true_words=6.16 avg_pred_words=5.67 max_pred_words=12
2026-01-03 13:49:26,685: t15.2023.08.13 val PER: 0.4595
2026-01-03 13:49:26,685: t15.2023.08.18 val PER: 0.4526
2026-01-03 13:49:26,685: t15.2023.08.20 val PER: 0.4480
2026-01-03 13:49:26,685: t15.2023.08.25 val PER: 0.4232
2026-01-03 13:49:26,685: t15.2023.08.27 val PER: 0.5257
2026-01-03 13:49:26,685: t15.2023.09.01 val PER: 0.4164
2026-01-03 13:49:26,685: t15.2023.09.03 val PER: 0.5012
2026-01-03 13:49:26,685: t15.2023.09.24 val PER: 0.4248
2026-01-03 13:49:26,686: t15.2023.09.29 val PER: 0.4684
2026-01-03 13:49:26,686: t15.2023.10.01 val PER: 0.5159
2026-01-03 13:49:26,686: t15.2023.10.06 val PER: 0.4273
2026-01-03 13:49:26,686: t15.2023.10.08 val PER: 0.5386
2026-01-03 13:49:26,686: t15.2023.10.13 val PER: 0.5694
2026-01-03 13:49:26,686: t15.2023.10.15 val PER: 0.4990
2026-01-03 13:49:26,686: t15.2023.10.20 val PER: 0.4564
2026-01-03 13:49:26,686: t15.2023.10.22 val PER: 0.4577
2026-01-03 13:49:26,686: t15.2023.11.03 val PER: 0.5034
2026-01-03 13:49:26,686: t15.2023.11.04 val PER: 0.2594
2026-01-03 13:49:26,686: t15.2023.11.17 val PER: 0.3484
2026-01-03 13:49:26,686: t15.2023.11.19 val PER: 0.3333
2026-01-03 13:49:26,686: t15.2023.11.26 val PER: 0.5536
2026-01-03 13:49:26,686: t15.2023.12.03 val PER: 0.5042
2026-01-03 13:49:26,686: t15.2023.12.08 val PER: 0.5206
2026-01-03 13:49:26,687: t15.2023.12.10 val PER: 0.4468
2026-01-03 13:49:26,687: t15.2023.12.17 val PER: 0.5624
2026-01-03 13:49:26,687: t15.2023.12.29 val PER: 0.5408
2026-01-03 13:49:26,687: t15.2024.02.25 val PER: 0.4789
2026-01-03 13:49:26,687: t15.2024.03.08 val PER: 0.6216
2026-01-03 13:49:26,687: t15.2024.03.15 val PER: 0.5629
2026-01-03 13:49:26,687: t15.2024.03.17 val PER: 0.5056
2026-01-03 13:49:26,687: t15.2024.05.10 val PER: 0.5423
2026-01-03 13:49:26,687: t15.2024.06.14 val PER: 0.5126
2026-01-03 13:49:26,687: t15.2024.07.19 val PER: 0.6803
2026-01-03 13:49:26,687: t15.2024.07.21 val PER: 0.4731
2026-01-03 13:49:26,687: t15.2024.07.28 val PER: 0.5096
2026-01-03 13:49:26,687: t15.2025.01.10 val PER: 0.7397
2026-01-03 13:49:26,687: t15.2025.01.12 val PER: 0.5643
2026-01-03 13:49:26,687: t15.2025.03.14 val PER: 0.7737
2026-01-03 13:49:26,687: t15.2025.03.16 val PER: 0.5969
2026-01-03 13:49:26,688: t15.2025.03.30 val PER: 0.7391
2026-01-03 13:49:26,688: t15.2025.04.13 val PER: 0.5777
2026-01-03 13:49:26,689: New best val WER(1gram) 100.00% --> 89.59%
2026-01-03 13:49:26,689: Checkpointing model
2026-01-03 13:49:26,963: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 13:49:27,220: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_500
2026-01-03 13:49:36,095: Train batch 600: loss: 49.13 grad norm: 82.25 time: 0.078
2026-01-03 13:49:53,443: Train batch 800: loss: 40.52 grad norm: 80.90 time: 0.057
2026-01-03 13:50:10,684: Train batch 1000: loss: 42.67 grad norm: 74.46 time: 0.065
2026-01-03 13:50:10,685: Running test after training batch: 1000
2026-01-03 13:50:10,810: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:50:15,624: WER debug example
  GT : you can see the code at this point as well
  PR : yule wint ease thus owed it this ard is while
2026-01-03 13:50:15,655: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus wass it
2026-01-03 13:50:17,448: Val batch 1000: PER (avg): 0.4094 CTC Loss (avg): 42.5541 WER(1gram): 80.71% (n=64) time: 6.763
2026-01-03 13:50:17,449: WER lens: avg_true_words=6.16 avg_pred_words=5.34 max_pred_words=12
2026-01-03 13:50:17,449: t15.2023.08.13 val PER: 0.3794
2026-01-03 13:50:17,449: t15.2023.08.18 val PER: 0.3336
2026-01-03 13:50:17,449: t15.2023.08.20 val PER: 0.3479
2026-01-03 13:50:17,449: t15.2023.08.25 val PER: 0.3027
2026-01-03 13:50:17,449: t15.2023.08.27 val PER: 0.4228
2026-01-03 13:50:17,449: t15.2023.09.01 val PER: 0.3052
2026-01-03 13:50:17,450: t15.2023.09.03 val PER: 0.3967
2026-01-03 13:50:17,450: t15.2023.09.24 val PER: 0.3216
2026-01-03 13:50:17,450: t15.2023.09.29 val PER: 0.3708
2026-01-03 13:50:17,450: t15.2023.10.01 val PER: 0.4049
2026-01-03 13:50:17,450: t15.2023.10.06 val PER: 0.3175
2026-01-03 13:50:17,450: t15.2023.10.08 val PER: 0.4533
2026-01-03 13:50:17,450: t15.2023.10.13 val PER: 0.4709
2026-01-03 13:50:17,450: t15.2023.10.15 val PER: 0.3837
2026-01-03 13:50:17,450: t15.2023.10.20 val PER: 0.3792
2026-01-03 13:50:17,450: t15.2023.10.22 val PER: 0.3530
2026-01-03 13:50:17,450: t15.2023.11.03 val PER: 0.4003
2026-01-03 13:50:17,450: t15.2023.11.04 val PER: 0.1570
2026-01-03 13:50:17,450: t15.2023.11.17 val PER: 0.2644
2026-01-03 13:50:17,450: t15.2023.11.19 val PER: 0.2036
2026-01-03 13:50:17,451: t15.2023.11.26 val PER: 0.4377
2026-01-03 13:50:17,451: t15.2023.12.03 val PER: 0.4034
2026-01-03 13:50:17,451: t15.2023.12.08 val PER: 0.4041
2026-01-03 13:50:17,451: t15.2023.12.10 val PER: 0.3338
2026-01-03 13:50:17,451: t15.2023.12.17 val PER: 0.4148
2026-01-03 13:50:17,451: t15.2023.12.29 val PER: 0.4118
2026-01-03 13:50:17,451: t15.2024.02.25 val PER: 0.3596
2026-01-03 13:50:17,451: t15.2024.03.08 val PER: 0.5007
2026-01-03 13:50:17,451: t15.2024.03.15 val PER: 0.4390
2026-01-03 13:50:17,451: t15.2024.03.17 val PER: 0.4135
2026-01-03 13:50:17,451: t15.2024.05.10 val PER: 0.4205
2026-01-03 13:50:17,451: t15.2024.06.14 val PER: 0.4054
2026-01-03 13:50:17,451: t15.2024.07.19 val PER: 0.5359
2026-01-03 13:50:17,451: t15.2024.07.21 val PER: 0.3897
2026-01-03 13:50:17,451: t15.2024.07.28 val PER: 0.4176
2026-01-03 13:50:17,451: t15.2025.01.10 val PER: 0.6157
2026-01-03 13:50:17,452: t15.2025.01.12 val PER: 0.4496
2026-01-03 13:50:17,452: t15.2025.03.14 val PER: 0.6213
2026-01-03 13:50:17,452: t15.2025.03.16 val PER: 0.4791
2026-01-03 13:50:17,452: t15.2025.03.30 val PER: 0.6391
2026-01-03 13:50:17,452: t15.2025.04.13 val PER: 0.4907
2026-01-03 13:50:17,453: New best val WER(1gram) 89.59% --> 80.71%
2026-01-03 13:50:17,454: Checkpointing model
2026-01-03 13:50:17,716: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 13:50:17,969: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_1000
2026-01-03 13:50:35,147: Train batch 1200: loss: 33.03 grad norm: 74.71 time: 0.067
2026-01-03 13:50:52,533: Train batch 1400: loss: 36.66 grad norm: 83.37 time: 0.060
2026-01-03 13:51:01,294: Running test after training batch: 1500
2026-01-03 13:51:01,388: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:51:06,326: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt sze the good it this boyde is wheel
2026-01-03 13:51:06,355: WER debug example
  GT : how does it keep the cost down
  PR : houde is it heap that cost
2026-01-03 13:51:07,898: Val batch 1500: PER (avg): 0.3786 CTC Loss (avg): 37.0175 WER(1gram): 75.89% (n=64) time: 6.604
2026-01-03 13:51:07,898: WER lens: avg_true_words=6.16 avg_pred_words=5.06 max_pred_words=11
2026-01-03 13:51:07,898: t15.2023.08.13 val PER: 0.3462
2026-01-03 13:51:07,898: t15.2023.08.18 val PER: 0.3085
2026-01-03 13:51:07,898: t15.2023.08.20 val PER: 0.3034
2026-01-03 13:51:07,898: t15.2023.08.25 val PER: 0.2636
2026-01-03 13:51:07,898: t15.2023.08.27 val PER: 0.3939
2026-01-03 13:51:07,898: t15.2023.09.01 val PER: 0.2808
2026-01-03 13:51:07,898: t15.2023.09.03 val PER: 0.3729
2026-01-03 13:51:07,898: t15.2023.09.24 val PER: 0.3046
2026-01-03 13:51:07,899: t15.2023.09.29 val PER: 0.3357
2026-01-03 13:51:07,899: t15.2023.10.01 val PER: 0.3831
2026-01-03 13:51:07,899: t15.2023.10.06 val PER: 0.2734
2026-01-03 13:51:07,899: t15.2023.10.08 val PER: 0.4344
2026-01-03 13:51:07,899: t15.2023.10.13 val PER: 0.4538
2026-01-03 13:51:07,899: t15.2023.10.15 val PER: 0.3586
2026-01-03 13:51:07,899: t15.2023.10.20 val PER: 0.3289
2026-01-03 13:51:07,899: t15.2023.10.22 val PER: 0.3096
2026-01-03 13:51:07,899: t15.2023.11.03 val PER: 0.3650
2026-01-03 13:51:07,899: t15.2023.11.04 val PER: 0.1126
2026-01-03 13:51:07,899: t15.2023.11.17 val PER: 0.2162
2026-01-03 13:51:07,899: t15.2023.11.19 val PER: 0.1756
2026-01-03 13:51:07,899: t15.2023.11.26 val PER: 0.4246
2026-01-03 13:51:07,899: t15.2023.12.03 val PER: 0.3687
2026-01-03 13:51:07,900: t15.2023.12.08 val PER: 0.3609
2026-01-03 13:51:07,900: t15.2023.12.10 val PER: 0.2996
2026-01-03 13:51:07,900: t15.2023.12.17 val PER: 0.3711
2026-01-03 13:51:07,900: t15.2023.12.29 val PER: 0.3775
2026-01-03 13:51:07,900: t15.2024.02.25 val PER: 0.3090
2026-01-03 13:51:07,900: t15.2024.03.08 val PER: 0.4623
2026-01-03 13:51:07,900: t15.2024.03.15 val PER: 0.4115
2026-01-03 13:51:07,900: t15.2024.03.17 val PER: 0.3794
2026-01-03 13:51:07,900: t15.2024.05.10 val PER: 0.3819
2026-01-03 13:51:07,900: t15.2024.06.14 val PER: 0.3959
2026-01-03 13:51:07,901: t15.2024.07.19 val PER: 0.5109
2026-01-03 13:51:07,901: t15.2024.07.21 val PER: 0.3497
2026-01-03 13:51:07,901: t15.2024.07.28 val PER: 0.3669
2026-01-03 13:51:07,901: t15.2025.01.10 val PER: 0.6006
2026-01-03 13:51:07,901: t15.2025.01.12 val PER: 0.4234
2026-01-03 13:51:07,901: t15.2025.03.14 val PER: 0.6036
2026-01-03 13:51:07,901: t15.2025.03.16 val PER: 0.4568
2026-01-03 13:51:07,901: t15.2025.03.30 val PER: 0.6230
2026-01-03 13:51:07,901: t15.2025.04.13 val PER: 0.4793
2026-01-03 13:51:07,902: New best val WER(1gram) 80.71% --> 75.89%
2026-01-03 13:51:07,902: Checkpointing model
2026-01-03 13:51:08,167: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 13:51:08,420: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_1500
2026-01-03 13:51:16,901: Train batch 1600: loss: 37.19 grad norm: 77.27 time: 0.063
2026-01-03 13:51:34,400: Train batch 1800: loss: 35.31 grad norm: 71.86 time: 0.087
2026-01-03 13:51:51,848: Train batch 2000: loss: 33.64 grad norm: 72.17 time: 0.066
2026-01-03 13:51:51,848: Running test after training batch: 2000
2026-01-03 13:51:51,971: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:51:56,714: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the good at this bonde is will
2026-01-03 13:51:56,743: WER debug example
  GT : how does it keep the cost down
  PR : houde dazs it heap the us id
2026-01-03 13:51:58,258: Val batch 2000: PER (avg): 0.3267 CTC Loss (avg): 32.7200 WER(1gram): 69.80% (n=64) time: 6.409
2026-01-03 13:51:58,258: WER lens: avg_true_words=6.16 avg_pred_words=5.53 max_pred_words=11
2026-01-03 13:51:58,258: t15.2023.08.13 val PER: 0.2994
2026-01-03 13:51:58,259: t15.2023.08.18 val PER: 0.2523
2026-01-03 13:51:58,259: t15.2023.08.20 val PER: 0.2478
2026-01-03 13:51:58,259: t15.2023.08.25 val PER: 0.2319
2026-01-03 13:51:58,259: t15.2023.08.27 val PER: 0.3424
2026-01-03 13:51:58,259: t15.2023.09.01 val PER: 0.2289
2026-01-03 13:51:58,259: t15.2023.09.03 val PER: 0.3112
2026-01-03 13:51:58,259: t15.2023.09.24 val PER: 0.2633
2026-01-03 13:51:58,259: t15.2023.09.29 val PER: 0.2782
2026-01-03 13:51:58,259: t15.2023.10.01 val PER: 0.3283
2026-01-03 13:51:58,259: t15.2023.10.06 val PER: 0.2400
2026-01-03 13:51:58,259: t15.2023.10.08 val PER: 0.3978
2026-01-03 13:51:58,260: t15.2023.10.13 val PER: 0.3747
2026-01-03 13:51:58,260: t15.2023.10.15 val PER: 0.3098
2026-01-03 13:51:58,260: t15.2023.10.20 val PER: 0.2852
2026-01-03 13:51:58,260: t15.2023.10.22 val PER: 0.2595
2026-01-03 13:51:58,260: t15.2023.11.03 val PER: 0.3229
2026-01-03 13:51:58,260: t15.2023.11.04 val PER: 0.0853
2026-01-03 13:51:58,260: t15.2023.11.17 val PER: 0.1820
2026-01-03 13:51:58,260: t15.2023.11.19 val PER: 0.1397
2026-01-03 13:51:58,260: t15.2023.11.26 val PER: 0.3703
2026-01-03 13:51:58,260: t15.2023.12.03 val PER: 0.3099
2026-01-03 13:51:58,260: t15.2023.12.08 val PER: 0.3076
2026-01-03 13:51:58,260: t15.2023.12.10 val PER: 0.2562
2026-01-03 13:51:58,260: t15.2023.12.17 val PER: 0.3202
2026-01-03 13:51:58,261: t15.2023.12.29 val PER: 0.3164
2026-01-03 13:51:58,261: t15.2024.02.25 val PER: 0.2865
2026-01-03 13:51:58,261: t15.2024.03.08 val PER: 0.3983
2026-01-03 13:51:58,261: t15.2024.03.15 val PER: 0.3615
2026-01-03 13:51:58,261: t15.2024.03.17 val PER: 0.3340
2026-01-03 13:51:58,261: t15.2024.05.10 val PER: 0.3373
2026-01-03 13:51:58,261: t15.2024.06.14 val PER: 0.3438
2026-01-03 13:51:58,261: t15.2024.07.19 val PER: 0.4707
2026-01-03 13:51:58,261: t15.2024.07.21 val PER: 0.2903
2026-01-03 13:51:58,261: t15.2024.07.28 val PER: 0.3118
2026-01-03 13:51:58,261: t15.2025.01.10 val PER: 0.5386
2026-01-03 13:51:58,261: t15.2025.01.12 val PER: 0.3834
2026-01-03 13:51:58,261: t15.2025.03.14 val PER: 0.5311
2026-01-03 13:51:58,261: t15.2025.03.16 val PER: 0.3861
2026-01-03 13:51:58,262: t15.2025.03.30 val PER: 0.5483
2026-01-03 13:51:58,262: t15.2025.04.13 val PER: 0.4123
2026-01-03 13:51:58,263: New best val WER(1gram) 75.89% --> 69.80%
2026-01-03 13:51:58,263: Checkpointing model
2026-01-03 13:51:58,524: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 13:51:58,776: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_2000
2026-01-03 13:52:15,523: Train batch 2200: loss: 28.57 grad norm: 70.26 time: 0.060
2026-01-03 13:52:32,472: Train batch 2400: loss: 28.93 grad norm: 64.55 time: 0.051
2026-01-03 13:52:41,140: Running test after training batch: 2500
2026-01-03 13:52:41,288: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:52:46,031: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-03 13:52:46,059: WER debug example
  GT : how does it keep the cost down
  PR : houde des it eke the wass it
2026-01-03 13:52:47,675: Val batch 2500: PER (avg): 0.3025 CTC Loss (avg): 30.3738 WER(1gram): 68.53% (n=64) time: 6.535
2026-01-03 13:52:47,675: WER lens: avg_true_words=6.16 avg_pred_words=5.56 max_pred_words=11
2026-01-03 13:52:47,675: t15.2023.08.13 val PER: 0.2817
2026-01-03 13:52:47,675: t15.2023.08.18 val PER: 0.2422
2026-01-03 13:52:47,676: t15.2023.08.20 val PER: 0.2407
2026-01-03 13:52:47,676: t15.2023.08.25 val PER: 0.2003
2026-01-03 13:52:47,676: t15.2023.08.27 val PER: 0.3344
2026-01-03 13:52:47,676: t15.2023.09.01 val PER: 0.2143
2026-01-03 13:52:47,676: t15.2023.09.03 val PER: 0.2910
2026-01-03 13:52:47,676: t15.2023.09.24 val PER: 0.2342
2026-01-03 13:52:47,676: t15.2023.09.29 val PER: 0.2482
2026-01-03 13:52:47,676: t15.2023.10.01 val PER: 0.3065
2026-01-03 13:52:47,676: t15.2023.10.06 val PER: 0.2185
2026-01-03 13:52:47,676: t15.2023.10.08 val PER: 0.3748
2026-01-03 13:52:47,677: t15.2023.10.13 val PER: 0.3576
2026-01-03 13:52:47,677: t15.2023.10.15 val PER: 0.2788
2026-01-03 13:52:47,677: t15.2023.10.20 val PER: 0.2919
2026-01-03 13:52:47,677: t15.2023.10.22 val PER: 0.2361
2026-01-03 13:52:47,677: t15.2023.11.03 val PER: 0.3026
2026-01-03 13:52:47,677: t15.2023.11.04 val PER: 0.0717
2026-01-03 13:52:47,677: t15.2023.11.17 val PER: 0.1353
2026-01-03 13:52:47,677: t15.2023.11.19 val PER: 0.1158
2026-01-03 13:52:47,677: t15.2023.11.26 val PER: 0.3551
2026-01-03 13:52:47,677: t15.2023.12.03 val PER: 0.2857
2026-01-03 13:52:47,677: t15.2023.12.08 val PER: 0.2750
2026-01-03 13:52:47,677: t15.2023.12.10 val PER: 0.2392
2026-01-03 13:52:47,677: t15.2023.12.17 val PER: 0.2848
2026-01-03 13:52:47,677: t15.2023.12.29 val PER: 0.2965
2026-01-03 13:52:47,677: t15.2024.02.25 val PER: 0.2388
2026-01-03 13:52:47,678: t15.2024.03.08 val PER: 0.3613
2026-01-03 13:52:47,678: t15.2024.03.15 val PER: 0.3446
2026-01-03 13:52:47,678: t15.2024.03.17 val PER: 0.3117
2026-01-03 13:52:47,678: t15.2024.05.10 val PER: 0.3105
2026-01-03 13:52:47,678: t15.2024.06.14 val PER: 0.3076
2026-01-03 13:52:47,678: t15.2024.07.19 val PER: 0.4410
2026-01-03 13:52:47,678: t15.2024.07.21 val PER: 0.2669
2026-01-03 13:52:47,678: t15.2024.07.28 val PER: 0.2941
2026-01-03 13:52:47,678: t15.2025.01.10 val PER: 0.4959
2026-01-03 13:52:47,678: t15.2025.01.12 val PER: 0.3572
2026-01-03 13:52:47,678: t15.2025.03.14 val PER: 0.4941
2026-01-03 13:52:47,678: t15.2025.03.16 val PER: 0.3560
2026-01-03 13:52:47,678: t15.2025.03.30 val PER: 0.5011
2026-01-03 13:52:47,679: t15.2025.04.13 val PER: 0.3923
2026-01-03 13:52:47,679: New best val WER(1gram) 69.80% --> 68.53%
2026-01-03 13:52:47,679: Checkpointing model
2026-01-03 13:52:47,959: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 13:52:48,213: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_2500
2026-01-03 13:52:56,859: Train batch 2600: loss: 35.07 grad norm: 82.88 time: 0.055
2026-01-03 13:53:14,156: Train batch 2800: loss: 26.29 grad norm: 71.55 time: 0.082
2026-01-03 13:53:31,647: Train batch 3000: loss: 30.89 grad norm: 73.31 time: 0.082
2026-01-03 13:53:31,648: Running test after training batch: 3000
2026-01-03 13:53:31,750: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:53:36,665: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the good at this point is will
2026-01-03 13:53:36,693: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it yip the cost get
2026-01-03 13:53:38,313: Val batch 3000: PER (avg): 0.2809 CTC Loss (avg): 27.9187 WER(1gram): 65.74% (n=64) time: 6.666
2026-01-03 13:53:38,314: WER lens: avg_true_words=6.16 avg_pred_words=5.75 max_pred_words=10
2026-01-03 13:53:38,314: t15.2023.08.13 val PER: 0.2630
2026-01-03 13:53:38,314: t15.2023.08.18 val PER: 0.2129
2026-01-03 13:53:38,314: t15.2023.08.20 val PER: 0.2216
2026-01-03 13:53:38,314: t15.2023.08.25 val PER: 0.1822
2026-01-03 13:53:38,314: t15.2023.08.27 val PER: 0.2910
2026-01-03 13:53:38,314: t15.2023.09.01 val PER: 0.1826
2026-01-03 13:53:38,314: t15.2023.09.03 val PER: 0.2803
2026-01-03 13:53:38,314: t15.2023.09.24 val PER: 0.2197
2026-01-03 13:53:38,314: t15.2023.09.29 val PER: 0.2304
2026-01-03 13:53:38,314: t15.2023.10.01 val PER: 0.2827
2026-01-03 13:53:38,315: t15.2023.10.06 val PER: 0.1927
2026-01-03 13:53:38,315: t15.2023.10.08 val PER: 0.3532
2026-01-03 13:53:38,315: t15.2023.10.13 val PER: 0.3344
2026-01-03 13:53:38,315: t15.2023.10.15 val PER: 0.2657
2026-01-03 13:53:38,315: t15.2023.10.20 val PER: 0.2651
2026-01-03 13:53:38,315: t15.2023.10.22 val PER: 0.1971
2026-01-03 13:53:38,315: t15.2023.11.03 val PER: 0.2836
2026-01-03 13:53:38,315: t15.2023.11.04 val PER: 0.0785
2026-01-03 13:53:38,315: t15.2023.11.17 val PER: 0.1275
2026-01-03 13:53:38,315: t15.2023.11.19 val PER: 0.1178
2026-01-03 13:53:38,315: t15.2023.11.26 val PER: 0.3007
2026-01-03 13:53:38,315: t15.2023.12.03 val PER: 0.2595
2026-01-03 13:53:38,315: t15.2023.12.08 val PER: 0.2583
2026-01-03 13:53:38,315: t15.2023.12.10 val PER: 0.2102
2026-01-03 13:53:38,316: t15.2023.12.17 val PER: 0.2827
2026-01-03 13:53:38,316: t15.2023.12.29 val PER: 0.2862
2026-01-03 13:53:38,316: t15.2024.02.25 val PER: 0.2374
2026-01-03 13:53:38,316: t15.2024.03.08 val PER: 0.3599
2026-01-03 13:53:38,316: t15.2024.03.15 val PER: 0.3340
2026-01-03 13:53:38,316: t15.2024.03.17 val PER: 0.2789
2026-01-03 13:53:38,316: t15.2024.05.10 val PER: 0.3105
2026-01-03 13:53:38,316: t15.2024.06.14 val PER: 0.2981
2026-01-03 13:53:38,316: t15.2024.07.19 val PER: 0.4008
2026-01-03 13:53:38,316: t15.2024.07.21 val PER: 0.2359
2026-01-03 13:53:38,316: t15.2024.07.28 val PER: 0.2757
2026-01-03 13:53:38,316: t15.2025.01.10 val PER: 0.4890
2026-01-03 13:53:38,316: t15.2025.01.12 val PER: 0.3318
2026-01-03 13:53:38,316: t15.2025.03.14 val PER: 0.4512
2026-01-03 13:53:38,316: t15.2025.03.16 val PER: 0.3272
2026-01-03 13:53:38,316: t15.2025.03.30 val PER: 0.4966
2026-01-03 13:53:38,317: t15.2025.04.13 val PER: 0.3509
2026-01-03 13:53:38,318: New best val WER(1gram) 68.53% --> 65.74%
2026-01-03 13:53:38,318: Checkpointing model
2026-01-03 13:53:38,584: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 13:53:38,838: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_3000
2026-01-03 13:53:56,144: Train batch 3200: loss: 26.85 grad norm: 66.92 time: 0.074
2026-01-03 13:54:13,195: Train batch 3400: loss: 18.50 grad norm: 55.03 time: 0.048
2026-01-03 13:54:21,882: Running test after training batch: 3500
2026-01-03 13:54:22,006: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:54:26,759: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-03 13:54:26,787: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it kipp thus cussed get
2026-01-03 13:54:28,331: Val batch 3500: PER (avg): 0.2665 CTC Loss (avg): 26.5053 WER(1gram): 64.47% (n=64) time: 6.448
2026-01-03 13:54:28,331: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-03 13:54:28,331: t15.2023.08.13 val PER: 0.2453
2026-01-03 13:54:28,331: t15.2023.08.18 val PER: 0.2054
2026-01-03 13:54:28,331: t15.2023.08.20 val PER: 0.2168
2026-01-03 13:54:28,332: t15.2023.08.25 val PER: 0.1792
2026-01-03 13:54:28,332: t15.2023.08.27 val PER: 0.2669
2026-01-03 13:54:28,332: t15.2023.09.01 val PER: 0.1778
2026-01-03 13:54:28,332: t15.2023.09.03 val PER: 0.2637
2026-01-03 13:54:28,332: t15.2023.09.24 val PER: 0.1990
2026-01-03 13:54:28,332: t15.2023.09.29 val PER: 0.2163
2026-01-03 13:54:28,332: t15.2023.10.01 val PER: 0.2768
2026-01-03 13:54:28,332: t15.2023.10.06 val PER: 0.1787
2026-01-03 13:54:28,332: t15.2023.10.08 val PER: 0.3437
2026-01-03 13:54:28,332: t15.2023.10.13 val PER: 0.3196
2026-01-03 13:54:28,332: t15.2023.10.15 val PER: 0.2479
2026-01-03 13:54:28,332: t15.2023.10.20 val PER: 0.2383
2026-01-03 13:54:28,332: t15.2023.10.22 val PER: 0.2171
2026-01-03 13:54:28,332: t15.2023.11.03 val PER: 0.2639
2026-01-03 13:54:28,333: t15.2023.11.04 val PER: 0.0546
2026-01-03 13:54:28,333: t15.2023.11.17 val PER: 0.1166
2026-01-03 13:54:28,333: t15.2023.11.19 val PER: 0.1038
2026-01-03 13:54:28,333: t15.2023.11.26 val PER: 0.2855
2026-01-03 13:54:28,333: t15.2023.12.03 val PER: 0.2437
2026-01-03 13:54:28,333: t15.2023.12.08 val PER: 0.2383
2026-01-03 13:54:28,333: t15.2023.12.10 val PER: 0.2011
2026-01-03 13:54:28,333: t15.2023.12.17 val PER: 0.2526
2026-01-03 13:54:28,333: t15.2023.12.29 val PER: 0.2567
2026-01-03 13:54:28,333: t15.2024.02.25 val PER: 0.2093
2026-01-03 13:54:28,333: t15.2024.03.08 val PER: 0.3499
2026-01-03 13:54:28,333: t15.2024.03.15 val PER: 0.3265
2026-01-03 13:54:28,333: t15.2024.03.17 val PER: 0.2734
2026-01-03 13:54:28,334: t15.2024.05.10 val PER: 0.2764
2026-01-03 13:54:28,334: t15.2024.06.14 val PER: 0.2776
2026-01-03 13:54:28,334: t15.2024.07.19 val PER: 0.3949
2026-01-03 13:54:28,334: t15.2024.07.21 val PER: 0.2234
2026-01-03 13:54:28,334: t15.2024.07.28 val PER: 0.2846
2026-01-03 13:54:28,334: t15.2025.01.10 val PER: 0.4601
2026-01-03 13:54:28,334: t15.2025.01.12 val PER: 0.2941
2026-01-03 13:54:28,334: t15.2025.03.14 val PER: 0.4305
2026-01-03 13:54:28,334: t15.2025.03.16 val PER: 0.3351
2026-01-03 13:54:28,334: t15.2025.03.30 val PER: 0.4471
2026-01-03 13:54:28,334: t15.2025.04.13 val PER: 0.3381
2026-01-03 13:54:28,336: New best val WER(1gram) 65.74% --> 64.47%
2026-01-03 13:54:28,336: Checkpointing model
2026-01-03 13:54:28,616: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 13:54:28,868: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_3500
2026-01-03 13:54:37,447: Train batch 3600: loss: 22.36 grad norm: 62.42 time: 0.066
2026-01-03 13:54:54,698: Train batch 3800: loss: 25.65 grad norm: 67.89 time: 0.066
2026-01-03 13:55:12,061: Train batch 4000: loss: 19.40 grad norm: 54.30 time: 0.057
2026-01-03 13:55:12,061: Running test after training batch: 4000
2026-01-03 13:55:12,223: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:55:16,976: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-03 13:55:17,003: WER debug example
  GT : how does it keep the cost down
  PR : aue dust it kipp the us nett
2026-01-03 13:55:18,563: Val batch 4000: PER (avg): 0.2501 CTC Loss (avg): 24.4278 WER(1gram): 63.71% (n=64) time: 6.502
2026-01-03 13:55:18,564: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=11
2026-01-03 13:55:18,564: t15.2023.08.13 val PER: 0.2256
2026-01-03 13:55:18,564: t15.2023.08.18 val PER: 0.2054
2026-01-03 13:55:18,564: t15.2023.08.20 val PER: 0.2097
2026-01-03 13:55:18,564: t15.2023.08.25 val PER: 0.1596
2026-01-03 13:55:18,564: t15.2023.08.27 val PER: 0.2717
2026-01-03 13:55:18,564: t15.2023.09.01 val PER: 0.1680
2026-01-03 13:55:18,564: t15.2023.09.03 val PER: 0.2458
2026-01-03 13:55:18,564: t15.2023.09.24 val PER: 0.1990
2026-01-03 13:55:18,564: t15.2023.09.29 val PER: 0.2042
2026-01-03 13:55:18,564: t15.2023.10.01 val PER: 0.2550
2026-01-03 13:55:18,565: t15.2023.10.06 val PER: 0.1615
2026-01-03 13:55:18,565: t15.2023.10.08 val PER: 0.3194
2026-01-03 13:55:18,565: t15.2023.10.13 val PER: 0.3033
2026-01-03 13:55:18,565: t15.2023.10.15 val PER: 0.2399
2026-01-03 13:55:18,565: t15.2023.10.20 val PER: 0.2248
2026-01-03 13:55:18,565: t15.2023.10.22 val PER: 0.1837
2026-01-03 13:55:18,565: t15.2023.11.03 val PER: 0.2497
2026-01-03 13:55:18,565: t15.2023.11.04 val PER: 0.0648
2026-01-03 13:55:18,565: t15.2023.11.17 val PER: 0.1026
2026-01-03 13:55:18,565: t15.2023.11.19 val PER: 0.1038
2026-01-03 13:55:18,565: t15.2023.11.26 val PER: 0.2580
2026-01-03 13:55:18,565: t15.2023.12.03 val PER: 0.2237
2026-01-03 13:55:18,565: t15.2023.12.08 val PER: 0.2250
2026-01-03 13:55:18,565: t15.2023.12.10 val PER: 0.1853
2026-01-03 13:55:18,565: t15.2023.12.17 val PER: 0.2412
2026-01-03 13:55:18,566: t15.2023.12.29 val PER: 0.2642
2026-01-03 13:55:18,566: t15.2024.02.25 val PER: 0.2093
2026-01-03 13:55:18,566: t15.2024.03.08 val PER: 0.3300
2026-01-03 13:55:18,566: t15.2024.03.15 val PER: 0.2983
2026-01-03 13:55:18,566: t15.2024.03.17 val PER: 0.2650
2026-01-03 13:55:18,566: t15.2024.05.10 val PER: 0.2675
2026-01-03 13:55:18,566: t15.2024.06.14 val PER: 0.2729
2026-01-03 13:55:18,566: t15.2024.07.19 val PER: 0.3724
2026-01-03 13:55:18,566: t15.2024.07.21 val PER: 0.1910
2026-01-03 13:55:18,566: t15.2024.07.28 val PER: 0.2434
2026-01-03 13:55:18,566: t15.2025.01.10 val PER: 0.4174
2026-01-03 13:55:18,566: t15.2025.01.12 val PER: 0.2748
2026-01-03 13:55:18,566: t15.2025.03.14 val PER: 0.4157
2026-01-03 13:55:18,566: t15.2025.03.16 val PER: 0.3089
2026-01-03 13:55:18,566: t15.2025.03.30 val PER: 0.4161
2026-01-03 13:55:18,567: t15.2025.04.13 val PER: 0.3181
2026-01-03 13:55:18,568: New best val WER(1gram) 64.47% --> 63.71%
2026-01-03 13:55:18,568: Checkpointing model
2026-01-03 13:55:18,834: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 13:55:19,087: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_4000
2026-01-03 13:55:36,148: Train batch 4200: loss: 22.22 grad norm: 64.86 time: 0.079
2026-01-03 13:55:53,161: Train batch 4400: loss: 17.18 grad norm: 55.26 time: 0.066
2026-01-03 13:56:01,807: Running test after training batch: 4500
2026-01-03 13:56:01,937: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:56:06,771: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-03 13:56:06,800: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cost get
2026-01-03 13:56:08,356: Val batch 4500: PER (avg): 0.2387 CTC Loss (avg): 23.0855 WER(1gram): 61.17% (n=64) time: 6.549
2026-01-03 13:56:08,356: WER lens: avg_true_words=6.16 avg_pred_words=5.94 max_pred_words=11
2026-01-03 13:56:08,357: t15.2023.08.13 val PER: 0.2089
2026-01-03 13:56:08,357: t15.2023.08.18 val PER: 0.1827
2026-01-03 13:56:08,357: t15.2023.08.20 val PER: 0.1938
2026-01-03 13:56:08,357: t15.2023.08.25 val PER: 0.1491
2026-01-03 13:56:08,357: t15.2023.08.27 val PER: 0.2556
2026-01-03 13:56:08,357: t15.2023.09.01 val PER: 0.1575
2026-01-03 13:56:08,358: t15.2023.09.03 val PER: 0.2447
2026-01-03 13:56:08,358: t15.2023.09.24 val PER: 0.1881
2026-01-03 13:56:08,358: t15.2023.09.29 val PER: 0.1946
2026-01-03 13:56:08,358: t15.2023.10.01 val PER: 0.2536
2026-01-03 13:56:08,358: t15.2023.10.06 val PER: 0.1507
2026-01-03 13:56:08,358: t15.2023.10.08 val PER: 0.3180
2026-01-03 13:56:08,358: t15.2023.10.13 val PER: 0.2964
2026-01-03 13:56:08,358: t15.2023.10.15 val PER: 0.2347
2026-01-03 13:56:08,358: t15.2023.10.20 val PER: 0.2181
2026-01-03 13:56:08,359: t15.2023.10.22 val PER: 0.1904
2026-01-03 13:56:08,359: t15.2023.11.03 val PER: 0.2354
2026-01-03 13:56:08,359: t15.2023.11.04 val PER: 0.0683
2026-01-03 13:56:08,359: t15.2023.11.17 val PER: 0.0980
2026-01-03 13:56:08,359: t15.2023.11.19 val PER: 0.0938
2026-01-03 13:56:08,359: t15.2023.11.26 val PER: 0.2696
2026-01-03 13:56:08,359: t15.2023.12.03 val PER: 0.2132
2026-01-03 13:56:08,359: t15.2023.12.08 val PER: 0.2077
2026-01-03 13:56:08,359: t15.2023.12.10 val PER: 0.1827
2026-01-03 13:56:08,359: t15.2023.12.17 val PER: 0.2349
2026-01-03 13:56:08,359: t15.2023.12.29 val PER: 0.2416
2026-01-03 13:56:08,360: t15.2024.02.25 val PER: 0.1994
2026-01-03 13:56:08,360: t15.2024.03.08 val PER: 0.3115
2026-01-03 13:56:08,360: t15.2024.03.15 val PER: 0.2877
2026-01-03 13:56:08,360: t15.2024.03.17 val PER: 0.2406
2026-01-03 13:56:08,360: t15.2024.05.10 val PER: 0.2571
2026-01-03 13:56:08,360: t15.2024.06.14 val PER: 0.2461
2026-01-03 13:56:08,360: t15.2024.07.19 val PER: 0.3415
2026-01-03 13:56:08,361: t15.2024.07.21 val PER: 0.1772
2026-01-03 13:56:08,361: t15.2024.07.28 val PER: 0.2250
2026-01-03 13:56:08,361: t15.2025.01.10 val PER: 0.4132
2026-01-03 13:56:08,361: t15.2025.01.12 val PER: 0.2671
2026-01-03 13:56:08,361: t15.2025.03.14 val PER: 0.3935
2026-01-03 13:56:08,361: t15.2025.03.16 val PER: 0.2919
2026-01-03 13:56:08,361: t15.2025.03.30 val PER: 0.4115
2026-01-03 13:56:08,361: t15.2025.04.13 val PER: 0.3081
2026-01-03 13:56:08,362: New best val WER(1gram) 63.71% --> 61.17%
2026-01-03 13:56:08,362: Checkpointing model
2026-01-03 13:56:08,628: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 13:56:08,882: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_4500
2026-01-03 13:56:17,428: Train batch 4600: loss: 20.71 grad norm: 65.82 time: 0.062
2026-01-03 13:56:34,578: Train batch 4800: loss: 14.08 grad norm: 54.43 time: 0.063
2026-01-03 13:56:51,596: Train batch 5000: loss: 32.35 grad norm: 88.50 time: 0.064
2026-01-03 13:56:51,596: Running test after training batch: 5000
2026-01-03 13:56:51,720: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:56:56,456: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-03 13:56:56,486: WER debug example
  GT : how does it keep the cost down
  PR : how just it heap the cost get
2026-01-03 13:56:58,078: Val batch 5000: PER (avg): 0.2272 CTC Loss (avg): 22.2846 WER(1gram): 59.90% (n=64) time: 6.482
2026-01-03 13:56:58,079: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-03 13:56:58,079: t15.2023.08.13 val PER: 0.1975
2026-01-03 13:56:58,079: t15.2023.08.18 val PER: 0.1769
2026-01-03 13:56:58,079: t15.2023.08.20 val PER: 0.1779
2026-01-03 13:56:58,079: t15.2023.08.25 val PER: 0.1340
2026-01-03 13:56:58,079: t15.2023.08.27 val PER: 0.2460
2026-01-03 13:56:58,079: t15.2023.09.01 val PER: 0.1412
2026-01-03 13:56:58,079: t15.2023.09.03 val PER: 0.2363
2026-01-03 13:56:58,079: t15.2023.09.24 val PER: 0.1881
2026-01-03 13:56:58,079: t15.2023.09.29 val PER: 0.1844
2026-01-03 13:56:58,080: t15.2023.10.01 val PER: 0.2391
2026-01-03 13:56:58,080: t15.2023.10.06 val PER: 0.1604
2026-01-03 13:56:58,080: t15.2023.10.08 val PER: 0.3085
2026-01-03 13:56:58,080: t15.2023.10.13 val PER: 0.2816
2026-01-03 13:56:58,080: t15.2023.10.15 val PER: 0.2248
2026-01-03 13:56:58,080: t15.2023.10.20 val PER: 0.2148
2026-01-03 13:56:58,080: t15.2023.10.22 val PER: 0.1648
2026-01-03 13:56:58,080: t15.2023.11.03 val PER: 0.2341
2026-01-03 13:56:58,080: t15.2023.11.04 val PER: 0.0580
2026-01-03 13:56:58,080: t15.2023.11.17 val PER: 0.0747
2026-01-03 13:56:58,080: t15.2023.11.19 val PER: 0.0758
2026-01-03 13:56:58,080: t15.2023.11.26 val PER: 0.2406
2026-01-03 13:56:58,080: t15.2023.12.03 val PER: 0.2017
2026-01-03 13:56:58,080: t15.2023.12.08 val PER: 0.1991
2026-01-03 13:56:58,080: t15.2023.12.10 val PER: 0.1590
2026-01-03 13:56:58,081: t15.2023.12.17 val PER: 0.2204
2026-01-03 13:56:58,081: t15.2023.12.29 val PER: 0.2169
2026-01-03 13:56:58,081: t15.2024.02.25 val PER: 0.1938
2026-01-03 13:56:58,081: t15.2024.03.08 val PER: 0.3030
2026-01-03 13:56:58,081: t15.2024.03.15 val PER: 0.2752
2026-01-03 13:56:58,081: t15.2024.03.17 val PER: 0.2364
2026-01-03 13:56:58,081: t15.2024.05.10 val PER: 0.2363
2026-01-03 13:56:58,081: t15.2024.06.14 val PER: 0.2445
2026-01-03 13:56:58,081: t15.2024.07.19 val PER: 0.3322
2026-01-03 13:56:58,081: t15.2024.07.21 val PER: 0.1779
2026-01-03 13:56:58,081: t15.2024.07.28 val PER: 0.2132
2026-01-03 13:56:58,081: t15.2025.01.10 val PER: 0.3898
2026-01-03 13:56:58,081: t15.2025.01.12 val PER: 0.2402
2026-01-03 13:56:58,081: t15.2025.03.14 val PER: 0.3994
2026-01-03 13:56:58,081: t15.2025.03.16 val PER: 0.2736
2026-01-03 13:56:58,081: t15.2025.03.30 val PER: 0.4126
2026-01-03 13:56:58,081: t15.2025.04.13 val PER: 0.3096
2026-01-03 13:56:58,083: New best val WER(1gram) 61.17% --> 59.90%
2026-01-03 13:56:58,083: Checkpointing model
2026-01-03 13:56:58,364: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 13:56:58,618: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_5000
2026-01-03 13:57:16,107: Train batch 5200: loss: 16.36 grad norm: 59.83 time: 0.051
2026-01-03 13:57:33,664: Train batch 5400: loss: 17.27 grad norm: 58.35 time: 0.068
2026-01-03 13:57:42,274: Running test after training batch: 5500
2026-01-03 13:57:42,438: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:57:47,187: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-03 13:57:47,216: WER debug example
  GT : how does it keep the cost down
  PR : aue dust it keep the cost nit
2026-01-03 13:57:48,796: Val batch 5500: PER (avg): 0.2156 CTC Loss (avg): 21.2761 WER(1gram): 56.60% (n=64) time: 6.521
2026-01-03 13:57:48,796: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-03 13:57:48,796: t15.2023.08.13 val PER: 0.1861
2026-01-03 13:57:48,796: t15.2023.08.18 val PER: 0.1685
2026-01-03 13:57:48,796: t15.2023.08.20 val PER: 0.1716
2026-01-03 13:57:48,796: t15.2023.08.25 val PER: 0.1205
2026-01-03 13:57:48,797: t15.2023.08.27 val PER: 0.2476
2026-01-03 13:57:48,797: t15.2023.09.01 val PER: 0.1282
2026-01-03 13:57:48,797: t15.2023.09.03 val PER: 0.2197
2026-01-03 13:57:48,797: t15.2023.09.24 val PER: 0.1663
2026-01-03 13:57:48,797: t15.2023.09.29 val PER: 0.1717
2026-01-03 13:57:48,797: t15.2023.10.01 val PER: 0.2305
2026-01-03 13:57:48,797: t15.2023.10.06 val PER: 0.1378
2026-01-03 13:57:48,798: t15.2023.10.08 val PER: 0.3045
2026-01-03 13:57:48,798: t15.2023.10.13 val PER: 0.2785
2026-01-03 13:57:48,798: t15.2023.10.15 val PER: 0.2070
2026-01-03 13:57:48,798: t15.2023.10.20 val PER: 0.2047
2026-01-03 13:57:48,798: t15.2023.10.22 val PER: 0.1581
2026-01-03 13:57:48,799: t15.2023.11.03 val PER: 0.2212
2026-01-03 13:57:48,799: t15.2023.11.04 val PER: 0.0683
2026-01-03 13:57:48,799: t15.2023.11.17 val PER: 0.0762
2026-01-03 13:57:48,799: t15.2023.11.19 val PER: 0.0699
2026-01-03 13:57:48,799: t15.2023.11.26 val PER: 0.2355
2026-01-03 13:57:48,799: t15.2023.12.03 val PER: 0.1838
2026-01-03 13:57:48,799: t15.2023.12.08 val PER: 0.1844
2026-01-03 13:57:48,800: t15.2023.12.10 val PER: 0.1511
2026-01-03 13:57:48,800: t15.2023.12.17 val PER: 0.2069
2026-01-03 13:57:48,800: t15.2023.12.29 val PER: 0.2203
2026-01-03 13:57:48,800: t15.2024.02.25 val PER: 0.1713
2026-01-03 13:57:48,800: t15.2024.03.08 val PER: 0.2959
2026-01-03 13:57:48,800: t15.2024.03.15 val PER: 0.2683
2026-01-03 13:57:48,800: t15.2024.03.17 val PER: 0.2259
2026-01-03 13:57:48,800: t15.2024.05.10 val PER: 0.2377
2026-01-03 13:57:48,800: t15.2024.06.14 val PER: 0.2319
2026-01-03 13:57:48,800: t15.2024.07.19 val PER: 0.3191
2026-01-03 13:57:48,801: t15.2024.07.21 val PER: 0.1634
2026-01-03 13:57:48,801: t15.2024.07.28 val PER: 0.2044
2026-01-03 13:57:48,801: t15.2025.01.10 val PER: 0.3747
2026-01-03 13:57:48,801: t15.2025.01.12 val PER: 0.2271
2026-01-03 13:57:48,801: t15.2025.03.14 val PER: 0.3669
2026-01-03 13:57:48,801: t15.2025.03.16 val PER: 0.2670
2026-01-03 13:57:48,801: t15.2025.03.30 val PER: 0.3713
2026-01-03 13:57:48,801: t15.2025.04.13 val PER: 0.2796
2026-01-03 13:57:48,802: New best val WER(1gram) 59.90% --> 56.60%
2026-01-03 13:57:48,802: Checkpointing model
2026-01-03 13:57:49,067: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 13:57:49,320: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_5500
2026-01-03 13:57:58,147: Train batch 5600: loss: 19.47 grad norm: 71.02 time: 0.061
2026-01-03 13:58:15,805: Train batch 5800: loss: 13.59 grad norm: 59.52 time: 0.082
2026-01-03 13:58:33,180: Train batch 6000: loss: 14.37 grad norm: 60.52 time: 0.049
2026-01-03 13:58:33,180: Running test after training batch: 6000
2026-01-03 13:58:33,293: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:58:38,231: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-03 13:58:38,262: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-03 13:58:39,926: Val batch 6000: PER (avg): 0.2147 CTC Loss (avg): 21.0284 WER(1gram): 58.63% (n=64) time: 6.746
2026-01-03 13:58:39,926: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 13:58:39,926: t15.2023.08.13 val PER: 0.1798
2026-01-03 13:58:39,926: t15.2023.08.18 val PER: 0.1785
2026-01-03 13:58:39,927: t15.2023.08.20 val PER: 0.1803
2026-01-03 13:58:39,927: t15.2023.08.25 val PER: 0.1235
2026-01-03 13:58:39,927: t15.2023.08.27 val PER: 0.2379
2026-01-03 13:58:39,927: t15.2023.09.01 val PER: 0.1315
2026-01-03 13:58:39,927: t15.2023.09.03 val PER: 0.2126
2026-01-03 13:58:39,927: t15.2023.09.24 val PER: 0.1699
2026-01-03 13:58:39,927: t15.2023.09.29 val PER: 0.1723
2026-01-03 13:58:39,927: t15.2023.10.01 val PER: 0.2325
2026-01-03 13:58:39,927: t15.2023.10.06 val PER: 0.1421
2026-01-03 13:58:39,927: t15.2023.10.08 val PER: 0.2869
2026-01-03 13:58:39,927: t15.2023.10.13 val PER: 0.2669
2026-01-03 13:58:39,927: t15.2023.10.15 val PER: 0.2195
2026-01-03 13:58:39,927: t15.2023.10.20 val PER: 0.2248
2026-01-03 13:58:39,928: t15.2023.10.22 val PER: 0.1659
2026-01-03 13:58:39,928: t15.2023.11.03 val PER: 0.2300
2026-01-03 13:58:39,928: t15.2023.11.04 val PER: 0.0717
2026-01-03 13:58:39,928: t15.2023.11.17 val PER: 0.0824
2026-01-03 13:58:39,928: t15.2023.11.19 val PER: 0.0758
2026-01-03 13:58:39,928: t15.2023.11.26 val PER: 0.2232
2026-01-03 13:58:39,928: t15.2023.12.03 val PER: 0.1786
2026-01-03 13:58:39,928: t15.2023.12.08 val PER: 0.1824
2026-01-03 13:58:39,928: t15.2023.12.10 val PER: 0.1603
2026-01-03 13:58:39,929: t15.2023.12.17 val PER: 0.2017
2026-01-03 13:58:39,929: t15.2023.12.29 val PER: 0.2148
2026-01-03 13:58:39,929: t15.2024.02.25 val PER: 0.1643
2026-01-03 13:58:39,929: t15.2024.03.08 val PER: 0.2959
2026-01-03 13:58:39,929: t15.2024.03.15 val PER: 0.2683
2026-01-03 13:58:39,929: t15.2024.03.17 val PER: 0.2197
2026-01-03 13:58:39,929: t15.2024.05.10 val PER: 0.2199
2026-01-03 13:58:39,929: t15.2024.06.14 val PER: 0.2334
2026-01-03 13:58:39,930: t15.2024.07.19 val PER: 0.3111
2026-01-03 13:58:39,930: t15.2024.07.21 val PER: 0.1614
2026-01-03 13:58:39,930: t15.2024.07.28 val PER: 0.2074
2026-01-03 13:58:39,930: t15.2025.01.10 val PER: 0.3747
2026-01-03 13:58:39,930: t15.2025.01.12 val PER: 0.2217
2026-01-03 13:58:39,930: t15.2025.03.14 val PER: 0.3698
2026-01-03 13:58:39,930: t15.2025.03.16 val PER: 0.2605
2026-01-03 13:58:39,930: t15.2025.03.30 val PER: 0.3701
2026-01-03 13:58:39,930: t15.2025.04.13 val PER: 0.2653
2026-01-03 13:58:40,173: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_6000
2026-01-03 13:58:57,713: Train batch 6200: loss: 17.18 grad norm: 63.02 time: 0.069
2026-01-03 13:59:14,867: Train batch 6400: loss: 19.50 grad norm: 68.92 time: 0.062
2026-01-03 13:59:23,412: Running test after training batch: 6500
2026-01-03 13:59:23,505: WER debug GT example: You can see the code at this point as well.
2026-01-03 13:59:28,262: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point is will
2026-01-03 13:59:28,292: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-03 13:59:29,922: Val batch 6500: PER (avg): 0.2047 CTC Loss (avg): 20.0708 WER(1gram): 52.79% (n=64) time: 6.510
2026-01-03 13:59:29,923: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 13:59:29,923: t15.2023.08.13 val PER: 0.1705
2026-01-03 13:59:29,923: t15.2023.08.18 val PER: 0.1484
2026-01-03 13:59:29,923: t15.2023.08.20 val PER: 0.1533
2026-01-03 13:59:29,923: t15.2023.08.25 val PER: 0.1160
2026-01-03 13:59:29,923: t15.2023.08.27 val PER: 0.2203
2026-01-03 13:59:29,923: t15.2023.09.01 val PER: 0.1315
2026-01-03 13:59:29,923: t15.2023.09.03 val PER: 0.1995
2026-01-03 13:59:29,923: t15.2023.09.24 val PER: 0.1650
2026-01-03 13:59:29,923: t15.2023.09.29 val PER: 0.1678
2026-01-03 13:59:29,923: t15.2023.10.01 val PER: 0.2160
2026-01-03 13:59:29,923: t15.2023.10.06 val PER: 0.1302
2026-01-03 13:59:29,924: t15.2023.10.08 val PER: 0.3058
2026-01-03 13:59:29,924: t15.2023.10.13 val PER: 0.2708
2026-01-03 13:59:29,924: t15.2023.10.15 val PER: 0.2149
2026-01-03 13:59:29,924: t15.2023.10.20 val PER: 0.2181
2026-01-03 13:59:29,924: t15.2023.10.22 val PER: 0.1514
2026-01-03 13:59:29,924: t15.2023.11.03 val PER: 0.2212
2026-01-03 13:59:29,924: t15.2023.11.04 val PER: 0.0648
2026-01-03 13:59:29,924: t15.2023.11.17 val PER: 0.0607
2026-01-03 13:59:29,924: t15.2023.11.19 val PER: 0.0659
2026-01-03 13:59:29,924: t15.2023.11.26 val PER: 0.2159
2026-01-03 13:59:29,924: t15.2023.12.03 val PER: 0.1754
2026-01-03 13:59:29,924: t15.2023.12.08 val PER: 0.1678
2026-01-03 13:59:29,924: t15.2023.12.10 val PER: 0.1432
2026-01-03 13:59:29,925: t15.2023.12.17 val PER: 0.1850
2026-01-03 13:59:29,925: t15.2023.12.29 val PER: 0.1956
2026-01-03 13:59:29,925: t15.2024.02.25 val PER: 0.1728
2026-01-03 13:59:29,925: t15.2024.03.08 val PER: 0.2987
2026-01-03 13:59:29,925: t15.2024.03.15 val PER: 0.2552
2026-01-03 13:59:29,925: t15.2024.03.17 val PER: 0.2064
2026-01-03 13:59:29,925: t15.2024.05.10 val PER: 0.2199
2026-01-03 13:59:29,925: t15.2024.06.14 val PER: 0.2129
2026-01-03 13:59:29,925: t15.2024.07.19 val PER: 0.3019
2026-01-03 13:59:29,925: t15.2024.07.21 val PER: 0.1455
2026-01-03 13:59:29,926: t15.2024.07.28 val PER: 0.1926
2026-01-03 13:59:29,926: t15.2025.01.10 val PER: 0.3733
2026-01-03 13:59:29,926: t15.2025.01.12 val PER: 0.2117
2026-01-03 13:59:29,926: t15.2025.03.14 val PER: 0.3757
2026-01-03 13:59:29,926: t15.2025.03.16 val PER: 0.2474
2026-01-03 13:59:29,926: t15.2025.03.30 val PER: 0.3575
2026-01-03 13:59:29,926: t15.2025.04.13 val PER: 0.2696
2026-01-03 13:59:29,927: New best val WER(1gram) 56.60% --> 52.79%
2026-01-03 13:59:29,927: Checkpointing model
2026-01-03 13:59:30,191: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 13:59:30,443: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_6500
2026-01-03 13:59:38,861: Train batch 6600: loss: 11.83 grad norm: 51.22 time: 0.045
2026-01-03 13:59:56,299: Train batch 6800: loss: 15.21 grad norm: 56.53 time: 0.048
2026-01-03 14:00:13,532: Train batch 7000: loss: 17.69 grad norm: 67.13 time: 0.060
2026-01-03 14:00:13,532: Running test after training batch: 7000
2026-01-03 14:00:13,682: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:00:18,397: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the cold at this point as will
2026-01-03 14:00:18,428: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-03 14:00:20,092: Val batch 7000: PER (avg): 0.1959 CTC Loss (avg): 19.2652 WER(1gram): 54.57% (n=64) time: 6.560
2026-01-03 14:00:20,093: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 14:00:20,093: t15.2023.08.13 val PER: 0.1622
2026-01-03 14:00:20,093: t15.2023.08.18 val PER: 0.1417
2026-01-03 14:00:20,093: t15.2023.08.20 val PER: 0.1517
2026-01-03 14:00:20,093: t15.2023.08.25 val PER: 0.1069
2026-01-03 14:00:20,093: t15.2023.08.27 val PER: 0.2219
2026-01-03 14:00:20,093: t15.2023.09.01 val PER: 0.1218
2026-01-03 14:00:20,094: t15.2023.09.03 val PER: 0.1960
2026-01-03 14:00:20,094: t15.2023.09.24 val PER: 0.1541
2026-01-03 14:00:20,094: t15.2023.09.29 val PER: 0.1678
2026-01-03 14:00:20,094: t15.2023.10.01 val PER: 0.2094
2026-01-03 14:00:20,094: t15.2023.10.06 val PER: 0.1130
2026-01-03 14:00:20,094: t15.2023.10.08 val PER: 0.2842
2026-01-03 14:00:20,094: t15.2023.10.13 val PER: 0.2521
2026-01-03 14:00:20,094: t15.2023.10.15 val PER: 0.1978
2026-01-03 14:00:20,094: t15.2023.10.20 val PER: 0.2148
2026-01-03 14:00:20,094: t15.2023.10.22 val PER: 0.1481
2026-01-03 14:00:20,094: t15.2023.11.03 val PER: 0.2015
2026-01-03 14:00:20,094: t15.2023.11.04 val PER: 0.0512
2026-01-03 14:00:20,094: t15.2023.11.17 val PER: 0.0607
2026-01-03 14:00:20,094: t15.2023.11.19 val PER: 0.0639
2026-01-03 14:00:20,095: t15.2023.11.26 val PER: 0.2029
2026-01-03 14:00:20,095: t15.2023.12.03 val PER: 0.1618
2026-01-03 14:00:20,095: t15.2023.12.08 val PER: 0.1625
2026-01-03 14:00:20,095: t15.2023.12.10 val PER: 0.1564
2026-01-03 14:00:20,095: t15.2023.12.17 val PER: 0.1778
2026-01-03 14:00:20,095: t15.2023.12.29 val PER: 0.1942
2026-01-03 14:00:20,095: t15.2024.02.25 val PER: 0.1503
2026-01-03 14:00:20,095: t15.2024.03.08 val PER: 0.2817
2026-01-03 14:00:20,095: t15.2024.03.15 val PER: 0.2351
2026-01-03 14:00:20,095: t15.2024.03.17 val PER: 0.1967
2026-01-03 14:00:20,095: t15.2024.05.10 val PER: 0.2021
2026-01-03 14:00:20,095: t15.2024.06.14 val PER: 0.2114
2026-01-03 14:00:20,095: t15.2024.07.19 val PER: 0.2986
2026-01-03 14:00:20,095: t15.2024.07.21 val PER: 0.1407
2026-01-03 14:00:20,095: t15.2024.07.28 val PER: 0.1743
2026-01-03 14:00:20,095: t15.2025.01.10 val PER: 0.3705
2026-01-03 14:00:20,096: t15.2025.01.12 val PER: 0.2102
2026-01-03 14:00:20,096: t15.2025.03.14 val PER: 0.3609
2026-01-03 14:00:20,096: t15.2025.03.16 val PER: 0.2461
2026-01-03 14:00:20,096: t15.2025.03.30 val PER: 0.3506
2026-01-03 14:00:20,096: t15.2025.04.13 val PER: 0.2611
2026-01-03 14:00:20,336: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_7000
2026-01-03 14:00:37,647: Train batch 7200: loss: 14.25 grad norm: 54.39 time: 0.077
2026-01-03 14:00:54,809: Train batch 7400: loss: 13.68 grad norm: 54.89 time: 0.075
2026-01-03 14:01:03,331: Running test after training batch: 7500
2026-01-03 14:01:03,605: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:01:08,776: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 14:01:08,807: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-03 14:01:10,447: Val batch 7500: PER (avg): 0.1905 CTC Loss (avg): 18.7878 WER(1gram): 54.06% (n=64) time: 7.116
2026-01-03 14:01:10,447: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 14:01:10,448: t15.2023.08.13 val PER: 0.1414
2026-01-03 14:01:10,448: t15.2023.08.18 val PER: 0.1375
2026-01-03 14:01:10,448: t15.2023.08.20 val PER: 0.1541
2026-01-03 14:01:10,448: t15.2023.08.25 val PER: 0.1069
2026-01-03 14:01:10,448: t15.2023.08.27 val PER: 0.2154
2026-01-03 14:01:10,448: t15.2023.09.01 val PER: 0.1185
2026-01-03 14:01:10,448: t15.2023.09.03 val PER: 0.1995
2026-01-03 14:01:10,448: t15.2023.09.24 val PER: 0.1553
2026-01-03 14:01:10,448: t15.2023.09.29 val PER: 0.1576
2026-01-03 14:01:10,448: t15.2023.10.01 val PER: 0.2094
2026-01-03 14:01:10,449: t15.2023.10.06 val PER: 0.1195
2026-01-03 14:01:10,449: t15.2023.10.08 val PER: 0.2720
2026-01-03 14:01:10,449: t15.2023.10.13 val PER: 0.2428
2026-01-03 14:01:10,449: t15.2023.10.15 val PER: 0.1984
2026-01-03 14:01:10,449: t15.2023.10.20 val PER: 0.2013
2026-01-03 14:01:10,449: t15.2023.10.22 val PER: 0.1359
2026-01-03 14:01:10,449: t15.2023.11.03 val PER: 0.2062
2026-01-03 14:01:10,449: t15.2023.11.04 val PER: 0.0410
2026-01-03 14:01:10,449: t15.2023.11.17 val PER: 0.0607
2026-01-03 14:01:10,449: t15.2023.11.19 val PER: 0.0519
2026-01-03 14:01:10,449: t15.2023.11.26 val PER: 0.1877
2026-01-03 14:01:10,449: t15.2023.12.03 val PER: 0.1618
2026-01-03 14:01:10,449: t15.2023.12.08 val PER: 0.1591
2026-01-03 14:01:10,449: t15.2023.12.10 val PER: 0.1261
2026-01-03 14:01:10,449: t15.2023.12.17 val PER: 0.1757
2026-01-03 14:01:10,449: t15.2023.12.29 val PER: 0.1881
2026-01-03 14:01:10,449: t15.2024.02.25 val PER: 0.1461
2026-01-03 14:01:10,450: t15.2024.03.08 val PER: 0.2817
2026-01-03 14:01:10,450: t15.2024.03.15 val PER: 0.2433
2026-01-03 14:01:10,450: t15.2024.03.17 val PER: 0.1750
2026-01-03 14:01:10,450: t15.2024.05.10 val PER: 0.2006
2026-01-03 14:01:10,450: t15.2024.06.14 val PER: 0.2256
2026-01-03 14:01:10,450: t15.2024.07.19 val PER: 0.2900
2026-01-03 14:01:10,450: t15.2024.07.21 val PER: 0.1262
2026-01-03 14:01:10,450: t15.2024.07.28 val PER: 0.1728
2026-01-03 14:01:10,450: t15.2025.01.10 val PER: 0.3471
2026-01-03 14:01:10,450: t15.2025.01.12 val PER: 0.1994
2026-01-03 14:01:10,450: t15.2025.03.14 val PER: 0.3713
2026-01-03 14:01:10,450: t15.2025.03.16 val PER: 0.2552
2026-01-03 14:01:10,450: t15.2025.03.30 val PER: 0.3356
2026-01-03 14:01:10,450: t15.2025.04.13 val PER: 0.2482
2026-01-03 14:01:10,739: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_7500
2026-01-03 14:01:19,338: Train batch 7600: loss: 16.39 grad norm: 61.86 time: 0.069
2026-01-03 14:01:36,606: Train batch 7800: loss: 14.88 grad norm: 60.85 time: 0.055
2026-01-03 14:01:54,085: Train batch 8000: loss: 11.22 grad norm: 51.84 time: 0.071
2026-01-03 14:01:54,085: Running test after training batch: 8000
2026-01-03 14:01:54,185: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:01:58,923: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 14:01:58,953: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the quast it
2026-01-03 14:02:00,662: Val batch 8000: PER (avg): 0.1844 CTC Loss (avg): 18.1416 WER(1gram): 56.09% (n=64) time: 6.577
2026-01-03 14:02:00,663: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 14:02:00,663: t15.2023.08.13 val PER: 0.1445
2026-01-03 14:02:00,663: t15.2023.08.18 val PER: 0.1282
2026-01-03 14:02:00,663: t15.2023.08.20 val PER: 0.1414
2026-01-03 14:02:00,663: t15.2023.08.25 val PER: 0.1069
2026-01-03 14:02:00,663: t15.2023.08.27 val PER: 0.2154
2026-01-03 14:02:00,663: t15.2023.09.01 val PER: 0.1055
2026-01-03 14:02:00,664: t15.2023.09.03 val PER: 0.1936
2026-01-03 14:02:00,664: t15.2023.09.24 val PER: 0.1529
2026-01-03 14:02:00,664: t15.2023.09.29 val PER: 0.1519
2026-01-03 14:02:00,664: t15.2023.10.01 val PER: 0.1982
2026-01-03 14:02:00,664: t15.2023.10.06 val PER: 0.1076
2026-01-03 14:02:00,664: t15.2023.10.08 val PER: 0.2666
2026-01-03 14:02:00,664: t15.2023.10.13 val PER: 0.2436
2026-01-03 14:02:00,664: t15.2023.10.15 val PER: 0.1852
2026-01-03 14:02:00,664: t15.2023.10.20 val PER: 0.2013
2026-01-03 14:02:00,664: t15.2023.10.22 val PER: 0.1448
2026-01-03 14:02:00,665: t15.2023.11.03 val PER: 0.2028
2026-01-03 14:02:00,665: t15.2023.11.04 val PER: 0.0444
2026-01-03 14:02:00,665: t15.2023.11.17 val PER: 0.0607
2026-01-03 14:02:00,665: t15.2023.11.19 val PER: 0.0639
2026-01-03 14:02:00,665: t15.2023.11.26 val PER: 0.1870
2026-01-03 14:02:00,665: t15.2023.12.03 val PER: 0.1544
2026-01-03 14:02:00,665: t15.2023.12.08 val PER: 0.1465
2026-01-03 14:02:00,665: t15.2023.12.10 val PER: 0.1327
2026-01-03 14:02:00,665: t15.2023.12.17 val PER: 0.1726
2026-01-03 14:02:00,665: t15.2023.12.29 val PER: 0.1750
2026-01-03 14:02:00,666: t15.2024.02.25 val PER: 0.1362
2026-01-03 14:02:00,666: t15.2024.03.08 val PER: 0.2646
2026-01-03 14:02:00,666: t15.2024.03.15 val PER: 0.2458
2026-01-03 14:02:00,666: t15.2024.03.17 val PER: 0.1806
2026-01-03 14:02:00,666: t15.2024.05.10 val PER: 0.2036
2026-01-03 14:02:00,666: t15.2024.06.14 val PER: 0.1987
2026-01-03 14:02:00,666: t15.2024.07.19 val PER: 0.2722
2026-01-03 14:02:00,666: t15.2024.07.21 val PER: 0.1166
2026-01-03 14:02:00,666: t15.2024.07.28 val PER: 0.1669
2026-01-03 14:02:00,666: t15.2025.01.10 val PER: 0.3444
2026-01-03 14:02:00,666: t15.2025.01.12 val PER: 0.1809
2026-01-03 14:02:00,667: t15.2025.03.14 val PER: 0.3595
2026-01-03 14:02:00,667: t15.2025.03.16 val PER: 0.2382
2026-01-03 14:02:00,667: t15.2025.03.30 val PER: 0.3402
2026-01-03 14:02:00,667: t15.2025.04.13 val PER: 0.2511
2026-01-03 14:02:00,911: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_8000
2026-01-03 14:02:18,223: Train batch 8200: loss: 9.94 grad norm: 47.32 time: 0.053
2026-01-03 14:02:35,428: Train batch 8400: loss: 10.17 grad norm: 47.20 time: 0.063
2026-01-03 14:02:44,145: Running test after training batch: 8500
2026-01-03 14:02:44,375: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:02:49,079: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 14:02:49,111: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost nett
2026-01-03 14:02:50,779: Val batch 8500: PER (avg): 0.1818 CTC Loss (avg): 17.8660 WER(1gram): 50.25% (n=64) time: 6.633
2026-01-03 14:02:50,779: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 14:02:50,779: t15.2023.08.13 val PER: 0.1393
2026-01-03 14:02:50,779: t15.2023.08.18 val PER: 0.1358
2026-01-03 14:02:50,779: t15.2023.08.20 val PER: 0.1334
2026-01-03 14:02:50,779: t15.2023.08.25 val PER: 0.1039
2026-01-03 14:02:50,779: t15.2023.08.27 val PER: 0.2170
2026-01-03 14:02:50,779: t15.2023.09.01 val PER: 0.1015
2026-01-03 14:02:50,780: t15.2023.09.03 val PER: 0.1960
2026-01-03 14:02:50,780: t15.2023.09.24 val PER: 0.1529
2026-01-03 14:02:50,780: t15.2023.09.29 val PER: 0.1512
2026-01-03 14:02:50,780: t15.2023.10.01 val PER: 0.1915
2026-01-03 14:02:50,780: t15.2023.10.06 val PER: 0.1098
2026-01-03 14:02:50,780: t15.2023.10.08 val PER: 0.2801
2026-01-03 14:02:50,780: t15.2023.10.13 val PER: 0.2436
2026-01-03 14:02:50,780: t15.2023.10.15 val PER: 0.1793
2026-01-03 14:02:50,781: t15.2023.10.20 val PER: 0.2013
2026-01-03 14:02:50,781: t15.2023.10.22 val PER: 0.1425
2026-01-03 14:02:50,781: t15.2023.11.03 val PER: 0.1995
2026-01-03 14:02:50,781: t15.2023.11.04 val PER: 0.0478
2026-01-03 14:02:50,781: t15.2023.11.17 val PER: 0.0529
2026-01-03 14:02:50,781: t15.2023.11.19 val PER: 0.0559
2026-01-03 14:02:50,781: t15.2023.11.26 val PER: 0.1783
2026-01-03 14:02:50,781: t15.2023.12.03 val PER: 0.1502
2026-01-03 14:02:50,781: t15.2023.12.08 val PER: 0.1431
2026-01-03 14:02:50,781: t15.2023.12.10 val PER: 0.1301
2026-01-03 14:02:50,781: t15.2023.12.17 val PER: 0.1674
2026-01-03 14:02:50,781: t15.2023.12.29 val PER: 0.1702
2026-01-03 14:02:50,782: t15.2024.02.25 val PER: 0.1475
2026-01-03 14:02:50,782: t15.2024.03.08 val PER: 0.2646
2026-01-03 14:02:50,782: t15.2024.03.15 val PER: 0.2358
2026-01-03 14:02:50,782: t15.2024.03.17 val PER: 0.1820
2026-01-03 14:02:50,782: t15.2024.05.10 val PER: 0.1932
2026-01-03 14:02:50,782: t15.2024.06.14 val PER: 0.1972
2026-01-03 14:02:50,782: t15.2024.07.19 val PER: 0.2762
2026-01-03 14:02:50,782: t15.2024.07.21 val PER: 0.1214
2026-01-03 14:02:50,782: t15.2024.07.28 val PER: 0.1662
2026-01-03 14:02:50,782: t15.2025.01.10 val PER: 0.3278
2026-01-03 14:02:50,782: t15.2025.01.12 val PER: 0.1909
2026-01-03 14:02:50,782: t15.2025.03.14 val PER: 0.3595
2026-01-03 14:02:50,782: t15.2025.03.16 val PER: 0.2120
2026-01-03 14:02:50,782: t15.2025.03.30 val PER: 0.3172
2026-01-03 14:02:50,782: t15.2025.04.13 val PER: 0.2496
2026-01-03 14:02:50,784: New best val WER(1gram) 52.79% --> 50.25%
2026-01-03 14:02:50,784: Checkpointing model
2026-01-03 14:02:51,047: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 14:02:51,400: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_8500
2026-01-03 14:03:00,054: Train batch 8600: loss: 16.43 grad norm: 59.91 time: 0.054
2026-01-03 14:03:18,009: Train batch 8800: loss: 15.45 grad norm: 59.35 time: 0.061
2026-01-03 14:03:35,672: Train batch 9000: loss: 15.85 grad norm: 64.87 time: 0.071
2026-01-03 14:03:35,672: Running test after training batch: 9000
2026-01-03 14:03:35,792: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:03:40,907: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 14:03:40,939: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 14:03:42,659: Val batch 9000: PER (avg): 0.1761 CTC Loss (avg): 17.5670 WER(1gram): 52.28% (n=64) time: 6.987
2026-01-03 14:03:42,659: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 14:03:42,660: t15.2023.08.13 val PER: 0.1372
2026-01-03 14:03:42,660: t15.2023.08.18 val PER: 0.1274
2026-01-03 14:03:42,660: t15.2023.08.20 val PER: 0.1334
2026-01-03 14:03:42,660: t15.2023.08.25 val PER: 0.1099
2026-01-03 14:03:42,660: t15.2023.08.27 val PER: 0.2074
2026-01-03 14:03:42,660: t15.2023.09.01 val PER: 0.0958
2026-01-03 14:03:42,660: t15.2023.09.03 val PER: 0.1829
2026-01-03 14:03:42,660: t15.2023.09.24 val PER: 0.1420
2026-01-03 14:03:42,660: t15.2023.09.29 val PER: 0.1481
2026-01-03 14:03:42,660: t15.2023.10.01 val PER: 0.1948
2026-01-03 14:03:42,660: t15.2023.10.06 val PER: 0.1023
2026-01-03 14:03:42,660: t15.2023.10.08 val PER: 0.2720
2026-01-03 14:03:42,660: t15.2023.10.13 val PER: 0.2327
2026-01-03 14:03:42,661: t15.2023.10.15 val PER: 0.1727
2026-01-03 14:03:42,661: t15.2023.10.20 val PER: 0.2013
2026-01-03 14:03:42,661: t15.2023.10.22 val PER: 0.1325
2026-01-03 14:03:42,661: t15.2023.11.03 val PER: 0.2049
2026-01-03 14:03:42,661: t15.2023.11.04 val PER: 0.0341
2026-01-03 14:03:42,661: t15.2023.11.17 val PER: 0.0638
2026-01-03 14:03:42,661: t15.2023.11.19 val PER: 0.0559
2026-01-03 14:03:42,661: t15.2023.11.26 val PER: 0.1659
2026-01-03 14:03:42,661: t15.2023.12.03 val PER: 0.1450
2026-01-03 14:03:42,661: t15.2023.12.08 val PER: 0.1265
2026-01-03 14:03:42,661: t15.2023.12.10 val PER: 0.1275
2026-01-03 14:03:42,661: t15.2023.12.17 val PER: 0.1632
2026-01-03 14:03:42,661: t15.2023.12.29 val PER: 0.1730
2026-01-03 14:03:42,661: t15.2024.02.25 val PER: 0.1433
2026-01-03 14:03:42,661: t15.2024.03.08 val PER: 0.2660
2026-01-03 14:03:42,661: t15.2024.03.15 val PER: 0.2276
2026-01-03 14:03:42,662: t15.2024.03.17 val PER: 0.1688
2026-01-03 14:03:42,662: t15.2024.05.10 val PER: 0.1813
2026-01-03 14:03:42,662: t15.2024.06.14 val PER: 0.1830
2026-01-03 14:03:42,662: t15.2024.07.19 val PER: 0.2729
2026-01-03 14:03:42,662: t15.2024.07.21 val PER: 0.1172
2026-01-03 14:03:42,662: t15.2024.07.28 val PER: 0.1537
2026-01-03 14:03:42,662: t15.2025.01.10 val PER: 0.3072
2026-01-03 14:03:42,662: t15.2025.01.12 val PER: 0.1771
2026-01-03 14:03:42,662: t15.2025.03.14 val PER: 0.3698
2026-01-03 14:03:42,662: t15.2025.03.16 val PER: 0.2094
2026-01-03 14:03:42,662: t15.2025.03.30 val PER: 0.3195
2026-01-03 14:03:42,662: t15.2025.04.13 val PER: 0.2439
2026-01-03 14:03:42,944: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_9000
2026-01-03 14:04:00,136: Train batch 9200: loss: 10.67 grad norm: 48.60 time: 0.056
2026-01-03 14:04:17,455: Train batch 9400: loss: 7.95 grad norm: 45.23 time: 0.067
2026-01-03 14:04:26,225: Running test after training batch: 9500
2026-01-03 14:04:26,363: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:04:31,100: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-03 14:04:31,130: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-03 14:04:32,789: Val batch 9500: PER (avg): 0.1737 CTC Loss (avg): 17.2641 WER(1gram): 50.00% (n=64) time: 6.563
2026-01-03 14:04:32,789: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 14:04:32,790: t15.2023.08.13 val PER: 0.1289
2026-01-03 14:04:32,790: t15.2023.08.18 val PER: 0.1215
2026-01-03 14:04:32,790: t15.2023.08.20 val PER: 0.1303
2026-01-03 14:04:32,790: t15.2023.08.25 val PER: 0.1009
2026-01-03 14:04:32,790: t15.2023.08.27 val PER: 0.1994
2026-01-03 14:04:32,790: t15.2023.09.01 val PER: 0.0917
2026-01-03 14:04:32,790: t15.2023.09.03 val PER: 0.1770
2026-01-03 14:04:32,790: t15.2023.09.24 val PER: 0.1493
2026-01-03 14:04:32,790: t15.2023.09.29 val PER: 0.1493
2026-01-03 14:04:32,790: t15.2023.10.01 val PER: 0.1909
2026-01-03 14:04:32,790: t15.2023.10.06 val PER: 0.1023
2026-01-03 14:04:32,791: t15.2023.10.08 val PER: 0.2639
2026-01-03 14:04:32,791: t15.2023.10.13 val PER: 0.2296
2026-01-03 14:04:32,791: t15.2023.10.15 val PER: 0.1833
2026-01-03 14:04:32,791: t15.2023.10.20 val PER: 0.1846
2026-01-03 14:04:32,791: t15.2023.10.22 val PER: 0.1281
2026-01-03 14:04:32,791: t15.2023.11.03 val PER: 0.1845
2026-01-03 14:04:32,791: t15.2023.11.04 val PER: 0.0375
2026-01-03 14:04:32,791: t15.2023.11.17 val PER: 0.0575
2026-01-03 14:04:32,791: t15.2023.11.19 val PER: 0.0619
2026-01-03 14:04:32,791: t15.2023.11.26 val PER: 0.1500
2026-01-03 14:04:32,791: t15.2023.12.03 val PER: 0.1387
2026-01-03 14:04:32,791: t15.2023.12.08 val PER: 0.1451
2026-01-03 14:04:32,791: t15.2023.12.10 val PER: 0.1156
2026-01-03 14:04:32,791: t15.2023.12.17 val PER: 0.1622
2026-01-03 14:04:32,791: t15.2023.12.29 val PER: 0.1682
2026-01-03 14:04:32,791: t15.2024.02.25 val PER: 0.1292
2026-01-03 14:04:32,791: t15.2024.03.08 val PER: 0.2546
2026-01-03 14:04:32,792: t15.2024.03.15 val PER: 0.2270
2026-01-03 14:04:32,792: t15.2024.03.17 val PER: 0.1611
2026-01-03 14:04:32,792: t15.2024.05.10 val PER: 0.1932
2026-01-03 14:04:32,792: t15.2024.06.14 val PER: 0.1940
2026-01-03 14:04:32,792: t15.2024.07.19 val PER: 0.2657
2026-01-03 14:04:32,792: t15.2024.07.21 val PER: 0.1145
2026-01-03 14:04:32,792: t15.2024.07.28 val PER: 0.1537
2026-01-03 14:04:32,792: t15.2025.01.10 val PER: 0.3196
2026-01-03 14:04:32,792: t15.2025.01.12 val PER: 0.1848
2026-01-03 14:04:32,792: t15.2025.03.14 val PER: 0.3609
2026-01-03 14:04:32,792: t15.2025.03.16 val PER: 0.2134
2026-01-03 14:04:32,792: t15.2025.03.30 val PER: 0.3310
2026-01-03 14:04:32,792: t15.2025.04.13 val PER: 0.2354
2026-01-03 14:04:32,793: New best val WER(1gram) 50.25% --> 50.00%
2026-01-03 14:04:32,794: Checkpointing model
2026-01-03 14:04:33,055: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 14:04:33,330: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_9500
2026-01-03 14:04:42,229: Train batch 9600: loss: 8.60 grad norm: 48.44 time: 0.073
2026-01-03 14:04:59,788: Train batch 9800: loss: 12.65 grad norm: 57.63 time: 0.063
2026-01-03 14:05:17,967: Train batch 10000: loss: 5.64 grad norm: 39.53 time: 0.061
2026-01-03 14:05:17,968: Running test after training batch: 10000
2026-01-03 14:05:18,097: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:05:23,207: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:05:23,237: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sette
2026-01-03 14:05:24,903: Val batch 10000: PER (avg): 0.1706 CTC Loss (avg): 16.9029 WER(1gram): 52.79% (n=64) time: 6.935
2026-01-03 14:05:24,903: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 14:05:24,903: t15.2023.08.13 val PER: 0.1237
2026-01-03 14:05:24,904: t15.2023.08.18 val PER: 0.1232
2026-01-03 14:05:24,904: t15.2023.08.20 val PER: 0.1239
2026-01-03 14:05:24,904: t15.2023.08.25 val PER: 0.1039
2026-01-03 14:05:24,904: t15.2023.08.27 val PER: 0.2106
2026-01-03 14:05:24,904: t15.2023.09.01 val PER: 0.0942
2026-01-03 14:05:24,904: t15.2023.09.03 val PER: 0.1746
2026-01-03 14:05:24,904: t15.2023.09.24 val PER: 0.1481
2026-01-03 14:05:24,904: t15.2023.09.29 val PER: 0.1468
2026-01-03 14:05:24,904: t15.2023.10.01 val PER: 0.1922
2026-01-03 14:05:24,908: t15.2023.10.06 val PER: 0.0990
2026-01-03 14:05:24,908: t15.2023.10.08 val PER: 0.2544
2026-01-03 14:05:24,909: t15.2023.10.13 val PER: 0.2250
2026-01-03 14:05:24,909: t15.2023.10.15 val PER: 0.1740
2026-01-03 14:05:24,909: t15.2023.10.20 val PER: 0.1812
2026-01-03 14:05:24,909: t15.2023.10.22 val PER: 0.1314
2026-01-03 14:05:24,909: t15.2023.11.03 val PER: 0.1872
2026-01-03 14:05:24,910: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:05:24,910: t15.2023.11.17 val PER: 0.0560
2026-01-03 14:05:24,910: t15.2023.11.19 val PER: 0.0459
2026-01-03 14:05:24,910: t15.2023.11.26 val PER: 0.1529
2026-01-03 14:05:24,910: t15.2023.12.03 val PER: 0.1313
2026-01-03 14:05:24,910: t15.2023.12.08 val PER: 0.1305
2026-01-03 14:05:24,910: t15.2023.12.10 val PER: 0.1130
2026-01-03 14:05:24,911: t15.2023.12.17 val PER: 0.1653
2026-01-03 14:05:24,911: t15.2023.12.29 val PER: 0.1544
2026-01-03 14:05:24,911: t15.2024.02.25 val PER: 0.1447
2026-01-03 14:05:24,911: t15.2024.03.08 val PER: 0.2504
2026-01-03 14:05:24,911: t15.2024.03.15 val PER: 0.2251
2026-01-03 14:05:24,911: t15.2024.03.17 val PER: 0.1548
2026-01-03 14:05:24,911: t15.2024.05.10 val PER: 0.1902
2026-01-03 14:05:24,911: t15.2024.06.14 val PER: 0.1909
2026-01-03 14:05:24,911: t15.2024.07.19 val PER: 0.2637
2026-01-03 14:05:24,911: t15.2024.07.21 val PER: 0.1193
2026-01-03 14:05:24,911: t15.2024.07.28 val PER: 0.1515
2026-01-03 14:05:24,912: t15.2025.01.10 val PER: 0.3044
2026-01-03 14:05:24,912: t15.2025.01.12 val PER: 0.1786
2026-01-03 14:05:24,912: t15.2025.03.14 val PER: 0.3417
2026-01-03 14:05:24,912: t15.2025.03.16 val PER: 0.2160
2026-01-03 14:05:24,912: t15.2025.03.30 val PER: 0.3218
2026-01-03 14:05:24,912: t15.2025.04.13 val PER: 0.2340
2026-01-03 14:05:25,169: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_10000
2026-01-03 14:05:42,610: Train batch 10200: loss: 6.53 grad norm: 40.20 time: 0.050
2026-01-03 14:06:00,768: Train batch 10400: loss: 9.45 grad norm: 51.32 time: 0.072
2026-01-03 14:06:09,624: Running test after training batch: 10500
2026-01-03 14:06:09,718: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:06:14,458: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 14:06:14,489: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-03 14:06:16,175: Val batch 10500: PER (avg): 0.1688 CTC Loss (avg): 16.7600 WER(1gram): 49.24% (n=64) time: 6.551
2026-01-03 14:06:16,175: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 14:06:16,175: t15.2023.08.13 val PER: 0.1268
2026-01-03 14:06:16,175: t15.2023.08.18 val PER: 0.1274
2026-01-03 14:06:16,176: t15.2023.08.20 val PER: 0.1191
2026-01-03 14:06:16,176: t15.2023.08.25 val PER: 0.1099
2026-01-03 14:06:16,176: t15.2023.08.27 val PER: 0.1994
2026-01-03 14:06:16,176: t15.2023.09.01 val PER: 0.0974
2026-01-03 14:06:16,176: t15.2023.09.03 val PER: 0.1781
2026-01-03 14:06:16,176: t15.2023.09.24 val PER: 0.1553
2026-01-03 14:06:16,176: t15.2023.09.29 val PER: 0.1461
2026-01-03 14:06:16,176: t15.2023.10.01 val PER: 0.1902
2026-01-03 14:06:16,176: t15.2023.10.06 val PER: 0.0904
2026-01-03 14:06:16,176: t15.2023.10.08 val PER: 0.2517
2026-01-03 14:06:16,176: t15.2023.10.13 val PER: 0.2180
2026-01-03 14:06:16,176: t15.2023.10.15 val PER: 0.1800
2026-01-03 14:06:16,176: t15.2023.10.20 val PER: 0.2013
2026-01-03 14:06:16,176: t15.2023.10.22 val PER: 0.1225
2026-01-03 14:06:16,176: t15.2023.11.03 val PER: 0.1906
2026-01-03 14:06:16,177: t15.2023.11.04 val PER: 0.0273
2026-01-03 14:06:16,177: t15.2023.11.17 val PER: 0.0544
2026-01-03 14:06:16,177: t15.2023.11.19 val PER: 0.0539
2026-01-03 14:06:16,177: t15.2023.11.26 val PER: 0.1420
2026-01-03 14:06:16,177: t15.2023.12.03 val PER: 0.1387
2026-01-03 14:06:16,177: t15.2023.12.08 val PER: 0.1232
2026-01-03 14:06:16,177: t15.2023.12.10 val PER: 0.1104
2026-01-03 14:06:16,177: t15.2023.12.17 val PER: 0.1476
2026-01-03 14:06:16,177: t15.2023.12.29 val PER: 0.1647
2026-01-03 14:06:16,177: t15.2024.02.25 val PER: 0.1292
2026-01-03 14:06:16,177: t15.2024.03.08 val PER: 0.2504
2026-01-03 14:06:16,177: t15.2024.03.15 val PER: 0.2258
2026-01-03 14:06:16,178: t15.2024.03.17 val PER: 0.1611
2026-01-03 14:06:16,178: t15.2024.05.10 val PER: 0.1768
2026-01-03 14:06:16,178: t15.2024.06.14 val PER: 0.1877
2026-01-03 14:06:16,178: t15.2024.07.19 val PER: 0.2564
2026-01-03 14:06:16,178: t15.2024.07.21 val PER: 0.1083
2026-01-03 14:06:16,178: t15.2024.07.28 val PER: 0.1449
2026-01-03 14:06:16,178: t15.2025.01.10 val PER: 0.3209
2026-01-03 14:06:16,178: t15.2025.01.12 val PER: 0.1732
2026-01-03 14:06:16,178: t15.2025.03.14 val PER: 0.3506
2026-01-03 14:06:16,178: t15.2025.03.16 val PER: 0.2055
2026-01-03 14:06:16,178: t15.2025.03.30 val PER: 0.3138
2026-01-03 14:06:16,178: t15.2025.04.13 val PER: 0.2268
2026-01-03 14:06:16,180: New best val WER(1gram) 50.00% --> 49.24%
2026-01-03 14:06:16,180: Checkpointing model
2026-01-03 14:06:16,439: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 14:06:16,712: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_10500
2026-01-03 14:06:25,680: Train batch 10600: loss: 9.01 grad norm: 52.82 time: 0.072
2026-01-03 14:06:42,880: Train batch 10800: loss: 14.96 grad norm: 66.10 time: 0.064
2026-01-03 14:07:00,296: Train batch 11000: loss: 14.39 grad norm: 62.41 time: 0.057
2026-01-03 14:07:00,296: Running test after training batch: 11000
2026-01-03 14:07:00,396: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:07:05,254: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 14:07:05,286: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 14:07:07,023: Val batch 11000: PER (avg): 0.1645 CTC Loss (avg): 16.4622 WER(1gram): 46.95% (n=64) time: 6.727
2026-01-03 14:07:07,024: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 14:07:07,024: t15.2023.08.13 val PER: 0.1206
2026-01-03 14:07:07,024: t15.2023.08.18 val PER: 0.1174
2026-01-03 14:07:07,024: t15.2023.08.20 val PER: 0.1207
2026-01-03 14:07:07,024: t15.2023.08.25 val PER: 0.1084
2026-01-03 14:07:07,024: t15.2023.08.27 val PER: 0.2074
2026-01-03 14:07:07,024: t15.2023.09.01 val PER: 0.0836
2026-01-03 14:07:07,024: t15.2023.09.03 val PER: 0.1746
2026-01-03 14:07:07,024: t15.2023.09.24 val PER: 0.1432
2026-01-03 14:07:07,024: t15.2023.09.29 val PER: 0.1391
2026-01-03 14:07:07,024: t15.2023.10.01 val PER: 0.1889
2026-01-03 14:07:07,024: t15.2023.10.06 val PER: 0.0883
2026-01-03 14:07:07,025: t15.2023.10.08 val PER: 0.2530
2026-01-03 14:07:07,025: t15.2023.10.13 val PER: 0.2172
2026-01-03 14:07:07,025: t15.2023.10.15 val PER: 0.1681
2026-01-03 14:07:07,025: t15.2023.10.20 val PER: 0.2013
2026-01-03 14:07:07,025: t15.2023.10.22 val PER: 0.1114
2026-01-03 14:07:07,025: t15.2023.11.03 val PER: 0.1893
2026-01-03 14:07:07,025: t15.2023.11.04 val PER: 0.0341
2026-01-03 14:07:07,025: t15.2023.11.17 val PER: 0.0467
2026-01-03 14:07:07,025: t15.2023.11.19 val PER: 0.0359
2026-01-03 14:07:07,025: t15.2023.11.26 val PER: 0.1507
2026-01-03 14:07:07,025: t15.2023.12.03 val PER: 0.1218
2026-01-03 14:07:07,025: t15.2023.12.08 val PER: 0.1145
2026-01-03 14:07:07,025: t15.2023.12.10 val PER: 0.0959
2026-01-03 14:07:07,025: t15.2023.12.17 val PER: 0.1518
2026-01-03 14:07:07,026: t15.2023.12.29 val PER: 0.1469
2026-01-03 14:07:07,026: t15.2024.02.25 val PER: 0.1334
2026-01-03 14:07:07,026: t15.2024.03.08 val PER: 0.2418
2026-01-03 14:07:07,026: t15.2024.03.15 val PER: 0.2195
2026-01-03 14:07:07,026: t15.2024.03.17 val PER: 0.1611
2026-01-03 14:07:07,026: t15.2024.05.10 val PER: 0.1768
2026-01-03 14:07:07,026: t15.2024.06.14 val PER: 0.1782
2026-01-03 14:07:07,026: t15.2024.07.19 val PER: 0.2591
2026-01-03 14:07:07,026: t15.2024.07.21 val PER: 0.1069
2026-01-03 14:07:07,026: t15.2024.07.28 val PER: 0.1529
2026-01-03 14:07:07,026: t15.2025.01.10 val PER: 0.2975
2026-01-03 14:07:07,026: t15.2025.01.12 val PER: 0.1671
2026-01-03 14:07:07,026: t15.2025.03.14 val PER: 0.3550
2026-01-03 14:07:07,026: t15.2025.03.16 val PER: 0.2016
2026-01-03 14:07:07,026: t15.2025.03.30 val PER: 0.3126
2026-01-03 14:07:07,026: t15.2025.04.13 val PER: 0.2354
2026-01-03 14:07:07,028: New best val WER(1gram) 49.24% --> 46.95%
2026-01-03 14:07:07,028: Checkpointing model
2026-01-03 14:07:07,289: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 14:07:07,562: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_11000
2026-01-03 14:07:25,093: Train batch 11200: loss: 10.70 grad norm: 53.27 time: 0.071
2026-01-03 14:07:42,857: Train batch 11400: loss: 9.95 grad norm: 54.97 time: 0.058
2026-01-03 14:07:52,231: Running test after training batch: 11500
2026-01-03 14:07:52,330: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:07:57,106: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 14:07:57,139: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost tit
2026-01-03 14:07:58,937: Val batch 11500: PER (avg): 0.1622 CTC Loss (avg): 16.3373 WER(1gram): 48.73% (n=64) time: 6.706
2026-01-03 14:07:58,937: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 14:07:58,937: t15.2023.08.13 val PER: 0.1185
2026-01-03 14:07:58,938: t15.2023.08.18 val PER: 0.1165
2026-01-03 14:07:58,938: t15.2023.08.20 val PER: 0.1160
2026-01-03 14:07:58,938: t15.2023.08.25 val PER: 0.0964
2026-01-03 14:07:58,938: t15.2023.08.27 val PER: 0.1929
2026-01-03 14:07:58,938: t15.2023.09.01 val PER: 0.0877
2026-01-03 14:07:58,938: t15.2023.09.03 val PER: 0.1639
2026-01-03 14:07:58,938: t15.2023.09.24 val PER: 0.1347
2026-01-03 14:07:58,938: t15.2023.09.29 val PER: 0.1385
2026-01-03 14:07:58,938: t15.2023.10.01 val PER: 0.1863
2026-01-03 14:07:58,938: t15.2023.10.06 val PER: 0.0872
2026-01-03 14:07:58,939: t15.2023.10.08 val PER: 0.2666
2026-01-03 14:07:58,939: t15.2023.10.13 val PER: 0.2126
2026-01-03 14:07:58,939: t15.2023.10.15 val PER: 0.1681
2026-01-03 14:07:58,939: t15.2023.10.20 val PER: 0.1980
2026-01-03 14:07:58,939: t15.2023.10.22 val PER: 0.1147
2026-01-03 14:07:58,939: t15.2023.11.03 val PER: 0.1845
2026-01-03 14:07:58,939: t15.2023.11.04 val PER: 0.0239
2026-01-03 14:07:58,939: t15.2023.11.17 val PER: 0.0404
2026-01-03 14:07:58,939: t15.2023.11.19 val PER: 0.0459
2026-01-03 14:07:58,939: t15.2023.11.26 val PER: 0.1399
2026-01-03 14:07:58,939: t15.2023.12.03 val PER: 0.1250
2026-01-03 14:07:58,939: t15.2023.12.08 val PER: 0.1172
2026-01-03 14:07:58,939: t15.2023.12.10 val PER: 0.0959
2026-01-03 14:07:58,939: t15.2023.12.17 val PER: 0.1476
2026-01-03 14:07:58,940: t15.2023.12.29 val PER: 0.1462
2026-01-03 14:07:58,940: t15.2024.02.25 val PER: 0.1208
2026-01-03 14:07:58,940: t15.2024.03.08 val PER: 0.2418
2026-01-03 14:07:58,940: t15.2024.03.15 val PER: 0.2176
2026-01-03 14:07:58,940: t15.2024.03.17 val PER: 0.1562
2026-01-03 14:07:58,940: t15.2024.05.10 val PER: 0.1709
2026-01-03 14:07:58,940: t15.2024.06.14 val PER: 0.1845
2026-01-03 14:07:58,940: t15.2024.07.19 val PER: 0.2538
2026-01-03 14:07:58,940: t15.2024.07.21 val PER: 0.1034
2026-01-03 14:07:58,940: t15.2024.07.28 val PER: 0.1456
2026-01-03 14:07:58,940: t15.2025.01.10 val PER: 0.3292
2026-01-03 14:07:58,940: t15.2025.01.12 val PER: 0.1640
2026-01-03 14:07:58,940: t15.2025.03.14 val PER: 0.3432
2026-01-03 14:07:58,940: t15.2025.03.16 val PER: 0.2016
2026-01-03 14:07:58,940: t15.2025.03.30 val PER: 0.3126
2026-01-03 14:07:58,940: t15.2025.04.13 val PER: 0.2240
2026-01-03 14:07:59,202: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_11500
2026-01-03 14:08:07,935: Train batch 11600: loss: 10.93 grad norm: 48.83 time: 0.060
2026-01-03 14:08:25,389: Train batch 11800: loss: 6.63 grad norm: 40.54 time: 0.044
2026-01-03 14:08:42,637: Train batch 12000: loss: 14.33 grad norm: 54.27 time: 0.070
2026-01-03 14:08:42,637: Running test after training batch: 12000
2026-01-03 14:08:42,736: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:08:47,434: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 14:08:47,466: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-03 14:08:49,218: Val batch 12000: PER (avg): 0.1601 CTC Loss (avg): 16.1469 WER(1gram): 49.49% (n=64) time: 6.581
2026-01-03 14:08:49,218: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 14:08:49,218: t15.2023.08.13 val PER: 0.1102
2026-01-03 14:08:49,218: t15.2023.08.18 val PER: 0.1106
2026-01-03 14:08:49,218: t15.2023.08.20 val PER: 0.1183
2026-01-03 14:08:49,219: t15.2023.08.25 val PER: 0.0964
2026-01-03 14:08:49,219: t15.2023.08.27 val PER: 0.1833
2026-01-03 14:08:49,219: t15.2023.09.01 val PER: 0.0877
2026-01-03 14:08:49,219: t15.2023.09.03 val PER: 0.1580
2026-01-03 14:08:49,219: t15.2023.09.24 val PER: 0.1383
2026-01-03 14:08:49,219: t15.2023.09.29 val PER: 0.1366
2026-01-03 14:08:49,219: t15.2023.10.01 val PER: 0.1757
2026-01-03 14:08:49,219: t15.2023.10.06 val PER: 0.0807
2026-01-03 14:08:49,219: t15.2023.10.08 val PER: 0.2530
2026-01-03 14:08:49,219: t15.2023.10.13 val PER: 0.2095
2026-01-03 14:08:49,219: t15.2023.10.15 val PER: 0.1721
2026-01-03 14:08:49,219: t15.2023.10.20 val PER: 0.2047
2026-01-03 14:08:49,219: t15.2023.10.22 val PER: 0.1247
2026-01-03 14:08:49,219: t15.2023.11.03 val PER: 0.1900
2026-01-03 14:08:49,219: t15.2023.11.04 val PER: 0.0444
2026-01-03 14:08:49,220: t15.2023.11.17 val PER: 0.0435
2026-01-03 14:08:49,220: t15.2023.11.19 val PER: 0.0399
2026-01-03 14:08:49,220: t15.2023.11.26 val PER: 0.1391
2026-01-03 14:08:49,220: t15.2023.12.03 val PER: 0.1208
2026-01-03 14:08:49,220: t15.2023.12.08 val PER: 0.1085
2026-01-03 14:08:49,220: t15.2023.12.10 val PER: 0.1012
2026-01-03 14:08:49,220: t15.2023.12.17 val PER: 0.1435
2026-01-03 14:08:49,220: t15.2023.12.29 val PER: 0.1421
2026-01-03 14:08:49,220: t15.2024.02.25 val PER: 0.1306
2026-01-03 14:08:49,220: t15.2024.03.08 val PER: 0.2461
2026-01-03 14:08:49,220: t15.2024.03.15 val PER: 0.2133
2026-01-03 14:08:49,221: t15.2024.03.17 val PER: 0.1464
2026-01-03 14:08:49,221: t15.2024.05.10 val PER: 0.1768
2026-01-03 14:08:49,221: t15.2024.06.14 val PER: 0.1845
2026-01-03 14:08:49,221: t15.2024.07.19 val PER: 0.2544
2026-01-03 14:08:49,221: t15.2024.07.21 val PER: 0.1007
2026-01-03 14:08:49,224: t15.2024.07.28 val PER: 0.1419
2026-01-03 14:08:49,224: t15.2025.01.10 val PER: 0.3113
2026-01-03 14:08:49,225: t15.2025.01.12 val PER: 0.1601
2026-01-03 14:08:49,225: t15.2025.03.14 val PER: 0.3521
2026-01-03 14:08:49,225: t15.2025.03.16 val PER: 0.2003
2026-01-03 14:08:49,225: t15.2025.03.30 val PER: 0.3092
2026-01-03 14:08:49,225: t15.2025.04.13 val PER: 0.2225
2026-01-03 14:08:49,482: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_12000
2026-01-03 14:09:08,091: Train batch 12200: loss: 5.71 grad norm: 40.01 time: 0.066
2026-01-03 14:09:25,431: Train batch 12400: loss: 4.69 grad norm: 33.78 time: 0.041
2026-01-03 14:09:34,289: Running test after training batch: 12500
2026-01-03 14:09:34,440: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:09:39,888: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 14:09:39,920: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-03 14:09:41,730: Val batch 12500: PER (avg): 0.1576 CTC Loss (avg): 15.9430 WER(1gram): 46.19% (n=64) time: 7.440
2026-01-03 14:09:41,730: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 14:09:41,730: t15.2023.08.13 val PER: 0.1133
2026-01-03 14:09:41,730: t15.2023.08.18 val PER: 0.1098
2026-01-03 14:09:41,730: t15.2023.08.20 val PER: 0.1080
2026-01-03 14:09:41,730: t15.2023.08.25 val PER: 0.0979
2026-01-03 14:09:41,730: t15.2023.08.27 val PER: 0.1929
2026-01-03 14:09:41,730: t15.2023.09.01 val PER: 0.0869
2026-01-03 14:09:41,731: t15.2023.09.03 val PER: 0.1603
2026-01-03 14:09:41,731: t15.2023.09.24 val PER: 0.1238
2026-01-03 14:09:41,731: t15.2023.09.29 val PER: 0.1334
2026-01-03 14:09:41,731: t15.2023.10.01 val PER: 0.1697
2026-01-03 14:09:41,731: t15.2023.10.06 val PER: 0.0807
2026-01-03 14:09:41,731: t15.2023.10.08 val PER: 0.2517
2026-01-03 14:09:41,731: t15.2023.10.13 val PER: 0.2157
2026-01-03 14:09:41,731: t15.2023.10.15 val PER: 0.1707
2026-01-03 14:09:41,731: t15.2023.10.20 val PER: 0.1946
2026-01-03 14:09:41,731: t15.2023.10.22 val PER: 0.1136
2026-01-03 14:09:41,731: t15.2023.11.03 val PER: 0.1859
2026-01-03 14:09:41,731: t15.2023.11.04 val PER: 0.0273
2026-01-03 14:09:41,731: t15.2023.11.17 val PER: 0.0451
2026-01-03 14:09:41,731: t15.2023.11.19 val PER: 0.0379
2026-01-03 14:09:41,731: t15.2023.11.26 val PER: 0.1239
2026-01-03 14:09:41,732: t15.2023.12.03 val PER: 0.1187
2026-01-03 14:09:41,732: t15.2023.12.08 val PER: 0.1052
2026-01-03 14:09:41,732: t15.2023.12.10 val PER: 0.0946
2026-01-03 14:09:41,732: t15.2023.12.17 val PER: 0.1362
2026-01-03 14:09:41,732: t15.2023.12.29 val PER: 0.1428
2026-01-03 14:09:41,732: t15.2024.02.25 val PER: 0.1138
2026-01-03 14:09:41,732: t15.2024.03.08 val PER: 0.2461
2026-01-03 14:09:41,732: t15.2024.03.15 val PER: 0.2201
2026-01-03 14:09:41,732: t15.2024.03.17 val PER: 0.1485
2026-01-03 14:09:41,732: t15.2024.05.10 val PER: 0.1605
2026-01-03 14:09:41,732: t15.2024.06.14 val PER: 0.1814
2026-01-03 14:09:41,732: t15.2024.07.19 val PER: 0.2479
2026-01-03 14:09:41,732: t15.2024.07.21 val PER: 0.1014
2026-01-03 14:09:41,733: t15.2024.07.28 val PER: 0.1404
2026-01-03 14:09:41,733: t15.2025.01.10 val PER: 0.3085
2026-01-03 14:09:41,733: t15.2025.01.12 val PER: 0.1601
2026-01-03 14:09:41,733: t15.2025.03.14 val PER: 0.3536
2026-01-03 14:09:41,733: t15.2025.03.16 val PER: 0.2042
2026-01-03 14:09:41,733: t15.2025.03.30 val PER: 0.3184
2026-01-03 14:09:41,733: t15.2025.04.13 val PER: 0.2183
2026-01-03 14:09:41,734: New best val WER(1gram) 46.95% --> 46.19%
2026-01-03 14:09:41,734: Checkpointing model
2026-01-03 14:09:42,004: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 14:09:42,275: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_12500
2026-01-03 14:09:50,947: Train batch 12600: loss: 8.13 grad norm: 45.34 time: 0.057
2026-01-03 14:10:08,372: Train batch 12800: loss: 5.92 grad norm: 38.11 time: 0.052
2026-01-03 14:10:25,677: Train batch 13000: loss: 6.53 grad norm: 39.36 time: 0.066
2026-01-03 14:10:25,677: Running test after training batch: 13000
2026-01-03 14:10:25,778: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:10:30,483: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-03 14:10:30,515: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 14:10:32,288: Val batch 13000: PER (avg): 0.1580 CTC Loss (avg): 15.8644 WER(1gram): 45.43% (n=64) time: 6.610
2026-01-03 14:10:32,288: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 14:10:32,288: t15.2023.08.13 val PER: 0.1175
2026-01-03 14:10:32,288: t15.2023.08.18 val PER: 0.1140
2026-01-03 14:10:32,288: t15.2023.08.20 val PER: 0.1088
2026-01-03 14:10:32,288: t15.2023.08.25 val PER: 0.1054
2026-01-03 14:10:32,289: t15.2023.08.27 val PER: 0.1897
2026-01-03 14:10:32,289: t15.2023.09.01 val PER: 0.0852
2026-01-03 14:10:32,289: t15.2023.09.03 val PER: 0.1710
2026-01-03 14:10:32,289: t15.2023.09.24 val PER: 0.1335
2026-01-03 14:10:32,289: t15.2023.09.29 val PER: 0.1366
2026-01-03 14:10:32,289: t15.2023.10.01 val PER: 0.1764
2026-01-03 14:10:32,289: t15.2023.10.06 val PER: 0.0947
2026-01-03 14:10:32,289: t15.2023.10.08 val PER: 0.2517
2026-01-03 14:10:32,289: t15.2023.10.13 val PER: 0.2133
2026-01-03 14:10:32,289: t15.2023.10.15 val PER: 0.1648
2026-01-03 14:10:32,289: t15.2023.10.20 val PER: 0.1846
2026-01-03 14:10:32,289: t15.2023.10.22 val PER: 0.1203
2026-01-03 14:10:32,289: t15.2023.11.03 val PER: 0.1811
2026-01-03 14:10:32,289: t15.2023.11.04 val PER: 0.0341
2026-01-03 14:10:32,290: t15.2023.11.17 val PER: 0.0373
2026-01-03 14:10:32,290: t15.2023.11.19 val PER: 0.0459
2026-01-03 14:10:32,290: t15.2023.11.26 val PER: 0.1319
2026-01-03 14:10:32,290: t15.2023.12.03 val PER: 0.1218
2026-01-03 14:10:32,290: t15.2023.12.08 val PER: 0.1085
2026-01-03 14:10:32,290: t15.2023.12.10 val PER: 0.0972
2026-01-03 14:10:32,290: t15.2023.12.17 val PER: 0.1497
2026-01-03 14:10:32,290: t15.2023.12.29 val PER: 0.1400
2026-01-03 14:10:32,290: t15.2024.02.25 val PER: 0.1138
2026-01-03 14:10:32,290: t15.2024.03.08 val PER: 0.2418
2026-01-03 14:10:32,291: t15.2024.03.15 val PER: 0.2158
2026-01-03 14:10:32,291: t15.2024.03.17 val PER: 0.1444
2026-01-03 14:10:32,291: t15.2024.05.10 val PER: 0.1679
2026-01-03 14:10:32,291: t15.2024.06.14 val PER: 0.1688
2026-01-03 14:10:32,291: t15.2024.07.19 val PER: 0.2584
2026-01-03 14:10:32,291: t15.2024.07.21 val PER: 0.0979
2026-01-03 14:10:32,291: t15.2024.07.28 val PER: 0.1456
2026-01-03 14:10:32,291: t15.2025.01.10 val PER: 0.3017
2026-01-03 14:10:32,291: t15.2025.01.12 val PER: 0.1570
2026-01-03 14:10:32,291: t15.2025.03.14 val PER: 0.3328
2026-01-03 14:10:32,291: t15.2025.03.16 val PER: 0.1754
2026-01-03 14:10:32,292: t15.2025.03.30 val PER: 0.3069
2026-01-03 14:10:32,292: t15.2025.04.13 val PER: 0.2211
2026-01-03 14:10:32,292: New best val WER(1gram) 46.19% --> 45.43%
2026-01-03 14:10:32,292: Checkpointing model
2026-01-03 14:10:32,575: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/best_checkpoint
2026-01-03 14:10:32,850: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_13000
2026-01-03 14:10:50,427: Train batch 13200: loss: 12.49 grad norm: 56.98 time: 0.054
2026-01-03 14:11:08,155: Train batch 13400: loss: 9.10 grad norm: 49.15 time: 0.062
2026-01-03 14:11:16,930: Running test after training batch: 13500
2026-01-03 14:11:17,054: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:11:22,253: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:11:22,286: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost it
2026-01-03 14:11:24,106: Val batch 13500: PER (avg): 0.1544 CTC Loss (avg): 15.6049 WER(1gram): 46.95% (n=64) time: 7.175
2026-01-03 14:11:24,106: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 14:11:24,106: t15.2023.08.13 val PER: 0.1143
2026-01-03 14:11:24,106: t15.2023.08.18 val PER: 0.1090
2026-01-03 14:11:24,106: t15.2023.08.20 val PER: 0.1072
2026-01-03 14:11:24,106: t15.2023.08.25 val PER: 0.1024
2026-01-03 14:11:24,106: t15.2023.08.27 val PER: 0.1881
2026-01-03 14:11:24,106: t15.2023.09.01 val PER: 0.0877
2026-01-03 14:11:24,106: t15.2023.09.03 val PER: 0.1686
2026-01-03 14:11:24,106: t15.2023.09.24 val PER: 0.1262
2026-01-03 14:11:24,107: t15.2023.09.29 val PER: 0.1302
2026-01-03 14:11:24,107: t15.2023.10.01 val PER: 0.1711
2026-01-03 14:11:24,107: t15.2023.10.06 val PER: 0.0861
2026-01-03 14:11:24,107: t15.2023.10.08 val PER: 0.2503
2026-01-03 14:11:24,107: t15.2023.10.13 val PER: 0.2118
2026-01-03 14:11:24,107: t15.2023.10.15 val PER: 0.1615
2026-01-03 14:11:24,107: t15.2023.10.20 val PER: 0.1846
2026-01-03 14:11:24,107: t15.2023.10.22 val PER: 0.1114
2026-01-03 14:11:24,107: t15.2023.11.03 val PER: 0.1811
2026-01-03 14:11:24,107: t15.2023.11.04 val PER: 0.0341
2026-01-03 14:11:24,107: t15.2023.11.17 val PER: 0.0389
2026-01-03 14:11:24,107: t15.2023.11.19 val PER: 0.0279
2026-01-03 14:11:24,108: t15.2023.11.26 val PER: 0.1246
2026-01-03 14:11:24,108: t15.2023.12.03 val PER: 0.1197
2026-01-03 14:11:24,108: t15.2023.12.08 val PER: 0.1059
2026-01-03 14:11:24,108: t15.2023.12.10 val PER: 0.0946
2026-01-03 14:11:24,108: t15.2023.12.17 val PER: 0.1320
2026-01-03 14:11:24,108: t15.2023.12.29 val PER: 0.1393
2026-01-03 14:11:24,108: t15.2024.02.25 val PER: 0.1180
2026-01-03 14:11:24,108: t15.2024.03.08 val PER: 0.2432
2026-01-03 14:11:24,108: t15.2024.03.15 val PER: 0.2089
2026-01-03 14:11:24,108: t15.2024.03.17 val PER: 0.1541
2026-01-03 14:11:24,108: t15.2024.05.10 val PER: 0.1575
2026-01-03 14:11:24,108: t15.2024.06.14 val PER: 0.1656
2026-01-03 14:11:24,109: t15.2024.07.19 val PER: 0.2406
2026-01-03 14:11:24,109: t15.2024.07.21 val PER: 0.0945
2026-01-03 14:11:24,109: t15.2024.07.28 val PER: 0.1346
2026-01-03 14:11:24,109: t15.2025.01.10 val PER: 0.2975
2026-01-03 14:11:24,109: t15.2025.01.12 val PER: 0.1486
2026-01-03 14:11:24,109: t15.2025.03.14 val PER: 0.3417
2026-01-03 14:11:24,109: t15.2025.03.16 val PER: 0.2003
2026-01-03 14:11:24,109: t15.2025.03.30 val PER: 0.3046
2026-01-03 14:11:24,109: t15.2025.04.13 val PER: 0.2097
2026-01-03 14:11:24,378: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_13500
2026-01-03 14:11:33,122: Train batch 13600: loss: 12.81 grad norm: 62.60 time: 0.063
2026-01-03 14:11:51,932: Train batch 13800: loss: 9.30 grad norm: 56.17 time: 0.056
2026-01-03 14:12:09,645: Train batch 14000: loss: 12.20 grad norm: 58.95 time: 0.050
2026-01-03 14:12:09,645: Running test after training batch: 14000
2026-01-03 14:12:09,811: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:12:14,514: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 14:12:14,546: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 14:12:16,332: Val batch 14000: PER (avg): 0.1513 CTC Loss (avg): 15.5267 WER(1gram): 46.70% (n=64) time: 6.687
2026-01-03 14:12:16,333: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 14:12:16,333: t15.2023.08.13 val PER: 0.1112
2026-01-03 14:12:16,333: t15.2023.08.18 val PER: 0.1065
2026-01-03 14:12:16,333: t15.2023.08.20 val PER: 0.1041
2026-01-03 14:12:16,333: t15.2023.08.25 val PER: 0.0904
2026-01-03 14:12:16,333: t15.2023.08.27 val PER: 0.1913
2026-01-03 14:12:16,333: t15.2023.09.01 val PER: 0.0787
2026-01-03 14:12:16,333: t15.2023.09.03 val PER: 0.1663
2026-01-03 14:12:16,333: t15.2023.09.24 val PER: 0.1286
2026-01-03 14:12:16,333: t15.2023.09.29 val PER: 0.1315
2026-01-03 14:12:16,333: t15.2023.10.01 val PER: 0.1724
2026-01-03 14:12:16,333: t15.2023.10.06 val PER: 0.0840
2026-01-03 14:12:16,333: t15.2023.10.08 val PER: 0.2558
2026-01-03 14:12:16,334: t15.2023.10.13 val PER: 0.2133
2026-01-03 14:12:16,334: t15.2023.10.15 val PER: 0.1615
2026-01-03 14:12:16,334: t15.2023.10.20 val PER: 0.1946
2026-01-03 14:12:16,334: t15.2023.10.22 val PER: 0.1136
2026-01-03 14:12:16,334: t15.2023.11.03 val PER: 0.1811
2026-01-03 14:12:16,334: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:12:16,334: t15.2023.11.17 val PER: 0.0451
2026-01-03 14:12:16,334: t15.2023.11.19 val PER: 0.0319
2026-01-03 14:12:16,334: t15.2023.11.26 val PER: 0.1283
2026-01-03 14:12:16,334: t15.2023.12.03 val PER: 0.1103
2026-01-03 14:12:16,334: t15.2023.12.08 val PER: 0.1059
2026-01-03 14:12:16,335: t15.2023.12.10 val PER: 0.1012
2026-01-03 14:12:16,335: t15.2023.12.17 val PER: 0.1289
2026-01-03 14:12:16,335: t15.2023.12.29 val PER: 0.1242
2026-01-03 14:12:16,335: t15.2024.02.25 val PER: 0.1194
2026-01-03 14:12:16,335: t15.2024.03.08 val PER: 0.2290
2026-01-03 14:12:16,335: t15.2024.03.15 val PER: 0.2020
2026-01-03 14:12:16,335: t15.2024.03.17 val PER: 0.1395
2026-01-03 14:12:16,335: t15.2024.05.10 val PER: 0.1575
2026-01-03 14:12:16,335: t15.2024.06.14 val PER: 0.1640
2026-01-03 14:12:16,335: t15.2024.07.19 val PER: 0.2254
2026-01-03 14:12:16,335: t15.2024.07.21 val PER: 0.0931
2026-01-03 14:12:16,335: t15.2024.07.28 val PER: 0.1353
2026-01-03 14:12:16,336: t15.2025.01.10 val PER: 0.2906
2026-01-03 14:12:16,336: t15.2025.01.12 val PER: 0.1501
2026-01-03 14:12:16,336: t15.2025.03.14 val PER: 0.3343
2026-01-03 14:12:16,336: t15.2025.03.16 val PER: 0.1859
2026-01-03 14:12:16,336: t15.2025.03.30 val PER: 0.2989
2026-01-03 14:12:16,336: t15.2025.04.13 val PER: 0.2040
2026-01-03 14:12:16,599: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_14000
2026-01-03 14:12:34,045: Train batch 14200: loss: 8.40 grad norm: 52.25 time: 0.056
2026-01-03 14:12:52,321: Train batch 14400: loss: 5.81 grad norm: 38.51 time: 0.064
2026-01-03 14:13:01,112: Running test after training batch: 14500
2026-01-03 14:13:01,219: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:13:06,446: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:13:06,479: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 14:13:08,271: Val batch 14500: PER (avg): 0.1525 CTC Loss (avg): 15.5704 WER(1gram): 47.72% (n=64) time: 7.159
2026-01-03 14:13:08,272: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 14:13:08,272: t15.2023.08.13 val PER: 0.1133
2026-01-03 14:13:08,272: t15.2023.08.18 val PER: 0.1081
2026-01-03 14:13:08,272: t15.2023.08.20 val PER: 0.1072
2026-01-03 14:13:08,272: t15.2023.08.25 val PER: 0.0904
2026-01-03 14:13:08,272: t15.2023.08.27 val PER: 0.1881
2026-01-03 14:13:08,272: t15.2023.09.01 val PER: 0.0779
2026-01-03 14:13:08,273: t15.2023.09.03 val PER: 0.1591
2026-01-03 14:13:08,273: t15.2023.09.24 val PER: 0.1347
2026-01-03 14:13:08,273: t15.2023.09.29 val PER: 0.1308
2026-01-03 14:13:08,273: t15.2023.10.01 val PER: 0.1770
2026-01-03 14:13:08,273: t15.2023.10.06 val PER: 0.0936
2026-01-03 14:13:08,273: t15.2023.10.08 val PER: 0.2382
2026-01-03 14:13:08,273: t15.2023.10.13 val PER: 0.2033
2026-01-03 14:13:08,273: t15.2023.10.15 val PER: 0.1648
2026-01-03 14:13:08,273: t15.2023.10.20 val PER: 0.1946
2026-01-03 14:13:08,274: t15.2023.10.22 val PER: 0.1125
2026-01-03 14:13:08,274: t15.2023.11.03 val PER: 0.1839
2026-01-03 14:13:08,274: t15.2023.11.04 val PER: 0.0375
2026-01-03 14:13:08,274: t15.2023.11.17 val PER: 0.0435
2026-01-03 14:13:08,274: t15.2023.11.19 val PER: 0.0299
2026-01-03 14:13:08,274: t15.2023.11.26 val PER: 0.1254
2026-01-03 14:13:08,275: t15.2023.12.03 val PER: 0.1134
2026-01-03 14:13:08,275: t15.2023.12.08 val PER: 0.1012
2026-01-03 14:13:08,275: t15.2023.12.10 val PER: 0.0907
2026-01-03 14:13:08,275: t15.2023.12.17 val PER: 0.1393
2026-01-03 14:13:08,275: t15.2023.12.29 val PER: 0.1256
2026-01-03 14:13:08,275: t15.2024.02.25 val PER: 0.1180
2026-01-03 14:13:08,275: t15.2024.03.08 val PER: 0.2447
2026-01-03 14:13:08,275: t15.2024.03.15 val PER: 0.2120
2026-01-03 14:13:08,276: t15.2024.03.17 val PER: 0.1444
2026-01-03 14:13:08,276: t15.2024.05.10 val PER: 0.1516
2026-01-03 14:13:08,276: t15.2024.06.14 val PER: 0.1640
2026-01-03 14:13:08,276: t15.2024.07.19 val PER: 0.2353
2026-01-03 14:13:08,276: t15.2024.07.21 val PER: 0.0952
2026-01-03 14:13:08,276: t15.2024.07.28 val PER: 0.1368
2026-01-03 14:13:08,276: t15.2025.01.10 val PER: 0.2879
2026-01-03 14:13:08,276: t15.2025.01.12 val PER: 0.1493
2026-01-03 14:13:08,276: t15.2025.03.14 val PER: 0.3432
2026-01-03 14:13:08,276: t15.2025.03.16 val PER: 0.1806
2026-01-03 14:13:08,276: t15.2025.03.30 val PER: 0.2931
2026-01-03 14:13:08,276: t15.2025.04.13 val PER: 0.2183
2026-01-03 14:13:08,543: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_14500
2026-01-03 14:13:17,113: Train batch 14600: loss: 12.61 grad norm: 60.33 time: 0.058
2026-01-03 14:13:34,786: Train batch 14800: loss: 6.04 grad norm: 42.81 time: 0.050
2026-01-03 14:13:53,801: Train batch 15000: loss: 9.13 grad norm: 50.55 time: 0.052
2026-01-03 14:13:53,801: Running test after training batch: 15000
2026-01-03 14:13:53,921: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:13:58,658: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:13:58,692: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-03 14:14:00,527: Val batch 15000: PER (avg): 0.1503 CTC Loss (avg): 15.3258 WER(1gram): 46.70% (n=64) time: 6.726
2026-01-03 14:14:00,528: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 14:14:00,528: t15.2023.08.13 val PER: 0.1060
2026-01-03 14:14:00,528: t15.2023.08.18 val PER: 0.1023
2026-01-03 14:14:00,528: t15.2023.08.20 val PER: 0.1112
2026-01-03 14:14:00,528: t15.2023.08.25 val PER: 0.0964
2026-01-03 14:14:00,528: t15.2023.08.27 val PER: 0.1833
2026-01-03 14:14:00,528: t15.2023.09.01 val PER: 0.0804
2026-01-03 14:14:00,528: t15.2023.09.03 val PER: 0.1544
2026-01-03 14:14:00,528: t15.2023.09.24 val PER: 0.1347
2026-01-03 14:14:00,528: t15.2023.09.29 val PER: 0.1302
2026-01-03 14:14:00,528: t15.2023.10.01 val PER: 0.1697
2026-01-03 14:14:00,528: t15.2023.10.06 val PER: 0.0850
2026-01-03 14:14:00,528: t15.2023.10.08 val PER: 0.2571
2026-01-03 14:14:00,528: t15.2023.10.13 val PER: 0.2009
2026-01-03 14:14:00,529: t15.2023.10.15 val PER: 0.1595
2026-01-03 14:14:00,529: t15.2023.10.20 val PER: 0.1913
2026-01-03 14:14:00,529: t15.2023.10.22 val PER: 0.1114
2026-01-03 14:14:00,529: t15.2023.11.03 val PER: 0.1744
2026-01-03 14:14:00,529: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:14:00,529: t15.2023.11.17 val PER: 0.0451
2026-01-03 14:14:00,529: t15.2023.11.19 val PER: 0.0319
2026-01-03 14:14:00,529: t15.2023.11.26 val PER: 0.1217
2026-01-03 14:14:00,529: t15.2023.12.03 val PER: 0.1155
2026-01-03 14:14:00,529: t15.2023.12.08 val PER: 0.0952
2026-01-03 14:14:00,529: t15.2023.12.10 val PER: 0.0880
2026-01-03 14:14:00,529: t15.2023.12.17 val PER: 0.1383
2026-01-03 14:14:00,530: t15.2023.12.29 val PER: 0.1325
2026-01-03 14:14:00,530: t15.2024.02.25 val PER: 0.1025
2026-01-03 14:14:00,530: t15.2024.03.08 val PER: 0.2304
2026-01-03 14:14:00,530: t15.2024.03.15 val PER: 0.2033
2026-01-03 14:14:00,530: t15.2024.03.17 val PER: 0.1430
2026-01-03 14:14:00,530: t15.2024.05.10 val PER: 0.1530
2026-01-03 14:14:00,530: t15.2024.06.14 val PER: 0.1577
2026-01-03 14:14:00,530: t15.2024.07.19 val PER: 0.2314
2026-01-03 14:14:00,530: t15.2024.07.21 val PER: 0.0938
2026-01-03 14:14:00,530: t15.2024.07.28 val PER: 0.1346
2026-01-03 14:14:00,530: t15.2025.01.10 val PER: 0.2893
2026-01-03 14:14:00,531: t15.2025.01.12 val PER: 0.1447
2026-01-03 14:14:00,531: t15.2025.03.14 val PER: 0.3462
2026-01-03 14:14:00,531: t15.2025.03.16 val PER: 0.1819
2026-01-03 14:14:00,531: t15.2025.03.30 val PER: 0.3011
2026-01-03 14:14:00,531: t15.2025.04.13 val PER: 0.2211
2026-01-03 14:14:00,797: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_15000
2026-01-03 14:14:18,621: Train batch 15200: loss: 5.04 grad norm: 44.39 time: 0.057
2026-01-03 14:14:36,132: Train batch 15400: loss: 11.41 grad norm: 55.54 time: 0.049
2026-01-03 14:14:44,752: Running test after training batch: 15500
2026-01-03 14:14:44,853: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:14:49,537: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 14:14:49,572: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 14:14:51,433: Val batch 15500: PER (avg): 0.1487 CTC Loss (avg): 15.2541 WER(1gram): 46.70% (n=64) time: 6.681
2026-01-03 14:14:51,434: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 14:14:51,434: t15.2023.08.13 val PER: 0.1081
2026-01-03 14:14:51,434: t15.2023.08.18 val PER: 0.0997
2026-01-03 14:14:51,434: t15.2023.08.20 val PER: 0.1072
2026-01-03 14:14:51,434: t15.2023.08.25 val PER: 0.0949
2026-01-03 14:14:51,434: t15.2023.08.27 val PER: 0.1833
2026-01-03 14:14:51,434: t15.2023.09.01 val PER: 0.0779
2026-01-03 14:14:51,434: t15.2023.09.03 val PER: 0.1532
2026-01-03 14:14:51,434: t15.2023.09.24 val PER: 0.1286
2026-01-03 14:14:51,435: t15.2023.09.29 val PER: 0.1276
2026-01-03 14:14:51,435: t15.2023.10.01 val PER: 0.1651
2026-01-03 14:14:51,435: t15.2023.10.06 val PER: 0.0829
2026-01-03 14:14:51,435: t15.2023.10.08 val PER: 0.2490
2026-01-03 14:14:51,435: t15.2023.10.13 val PER: 0.2040
2026-01-03 14:14:51,435: t15.2023.10.15 val PER: 0.1575
2026-01-03 14:14:51,435: t15.2023.10.20 val PER: 0.1879
2026-01-03 14:14:51,435: t15.2023.10.22 val PER: 0.1080
2026-01-03 14:14:51,435: t15.2023.11.03 val PER: 0.1764
2026-01-03 14:14:51,435: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:14:51,435: t15.2023.11.17 val PER: 0.0420
2026-01-03 14:14:51,435: t15.2023.11.19 val PER: 0.0319
2026-01-03 14:14:51,435: t15.2023.11.26 val PER: 0.1159
2026-01-03 14:14:51,435: t15.2023.12.03 val PER: 0.1113
2026-01-03 14:14:51,435: t15.2023.12.08 val PER: 0.0972
2026-01-03 14:14:51,435: t15.2023.12.10 val PER: 0.0880
2026-01-03 14:14:51,436: t15.2023.12.17 val PER: 0.1351
2026-01-03 14:14:51,436: t15.2023.12.29 val PER: 0.1304
2026-01-03 14:14:51,436: t15.2024.02.25 val PER: 0.1039
2026-01-03 14:14:51,436: t15.2024.03.08 val PER: 0.2347
2026-01-03 14:14:51,436: t15.2024.03.15 val PER: 0.1976
2026-01-03 14:14:51,436: t15.2024.03.17 val PER: 0.1409
2026-01-03 14:14:51,436: t15.2024.05.10 val PER: 0.1590
2026-01-03 14:14:51,436: t15.2024.06.14 val PER: 0.1530
2026-01-03 14:14:51,436: t15.2024.07.19 val PER: 0.2307
2026-01-03 14:14:51,436: t15.2024.07.21 val PER: 0.0924
2026-01-03 14:14:51,436: t15.2024.07.28 val PER: 0.1375
2026-01-03 14:14:51,436: t15.2025.01.10 val PER: 0.2920
2026-01-03 14:14:51,436: t15.2025.01.12 val PER: 0.1440
2026-01-03 14:14:51,436: t15.2025.03.14 val PER: 0.3328
2026-01-03 14:14:51,436: t15.2025.03.16 val PER: 0.1832
2026-01-03 14:14:51,437: t15.2025.03.30 val PER: 0.3034
2026-01-03 14:14:51,437: t15.2025.04.13 val PER: 0.2183
2026-01-03 14:14:51,692: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_15500
2026-01-03 14:15:00,486: Train batch 15600: loss: 12.19 grad norm: 55.31 time: 0.061
2026-01-03 14:15:18,446: Train batch 15800: loss: 13.78 grad norm: 61.37 time: 0.066
2026-01-03 14:15:36,010: Train batch 16000: loss: 8.82 grad norm: 45.38 time: 0.055
2026-01-03 14:15:36,010: Running test after training batch: 16000
2026-01-03 14:15:36,154: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:15:40,972: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 14:15:41,007: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 14:15:42,886: Val batch 16000: PER (avg): 0.1489 CTC Loss (avg): 15.2647 WER(1gram): 46.70% (n=64) time: 6.876
2026-01-03 14:15:42,887: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 14:15:42,887: t15.2023.08.13 val PER: 0.1060
2026-01-03 14:15:42,887: t15.2023.08.18 val PER: 0.1023
2026-01-03 14:15:42,887: t15.2023.08.20 val PER: 0.1064
2026-01-03 14:15:42,887: t15.2023.08.25 val PER: 0.0964
2026-01-03 14:15:42,887: t15.2023.08.27 val PER: 0.1881
2026-01-03 14:15:42,887: t15.2023.09.01 val PER: 0.0795
2026-01-03 14:15:42,887: t15.2023.09.03 val PER: 0.1544
2026-01-03 14:15:42,887: t15.2023.09.24 val PER: 0.1311
2026-01-03 14:15:42,887: t15.2023.09.29 val PER: 0.1340
2026-01-03 14:15:42,887: t15.2023.10.01 val PER: 0.1671
2026-01-03 14:15:42,887: t15.2023.10.06 val PER: 0.0807
2026-01-03 14:15:42,888: t15.2023.10.08 val PER: 0.2503
2026-01-03 14:15:42,888: t15.2023.10.13 val PER: 0.2064
2026-01-03 14:15:42,888: t15.2023.10.15 val PER: 0.1569
2026-01-03 14:15:42,888: t15.2023.10.20 val PER: 0.1846
2026-01-03 14:15:42,888: t15.2023.10.22 val PER: 0.1047
2026-01-03 14:15:42,888: t15.2023.11.03 val PER: 0.1771
2026-01-03 14:15:42,888: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:15:42,888: t15.2023.11.17 val PER: 0.0404
2026-01-03 14:15:42,888: t15.2023.11.19 val PER: 0.0319
2026-01-03 14:15:42,888: t15.2023.11.26 val PER: 0.1152
2026-01-03 14:15:42,888: t15.2023.12.03 val PER: 0.1103
2026-01-03 14:15:42,888: t15.2023.12.08 val PER: 0.0972
2026-01-03 14:15:42,888: t15.2023.12.10 val PER: 0.0841
2026-01-03 14:15:42,889: t15.2023.12.17 val PER: 0.1362
2026-01-03 14:15:42,889: t15.2023.12.29 val PER: 0.1263
2026-01-03 14:15:42,889: t15.2024.02.25 val PER: 0.1067
2026-01-03 14:15:42,889: t15.2024.03.08 val PER: 0.2319
2026-01-03 14:15:42,889: t15.2024.03.15 val PER: 0.1970
2026-01-03 14:15:42,889: t15.2024.03.17 val PER: 0.1416
2026-01-03 14:15:42,889: t15.2024.05.10 val PER: 0.1620
2026-01-03 14:15:42,889: t15.2024.06.14 val PER: 0.1514
2026-01-03 14:15:42,889: t15.2024.07.19 val PER: 0.2314
2026-01-03 14:15:42,889: t15.2024.07.21 val PER: 0.0931
2026-01-03 14:15:42,889: t15.2024.07.28 val PER: 0.1382
2026-01-03 14:15:42,889: t15.2025.01.10 val PER: 0.2851
2026-01-03 14:15:42,889: t15.2025.01.12 val PER: 0.1463
2026-01-03 14:15:42,889: t15.2025.03.14 val PER: 0.3343
2026-01-03 14:15:42,889: t15.2025.03.16 val PER: 0.1806
2026-01-03 14:15:42,889: t15.2025.03.30 val PER: 0.3000
2026-01-03 14:15:42,889: t15.2025.04.13 val PER: 0.2211
2026-01-03 14:15:43,155: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_16000
2026-01-03 14:16:00,450: Train batch 16200: loss: 6.48 grad norm: 40.58 time: 0.055
2026-01-03 14:16:18,430: Train batch 16400: loss: 10.51 grad norm: 60.43 time: 0.057
2026-01-03 14:16:27,182: Running test after training batch: 16500
2026-01-03 14:16:27,369: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:16:32,093: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 14:16:32,127: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 14:16:33,986: Val batch 16500: PER (avg): 0.1491 CTC Loss (avg): 15.2611 WER(1gram): 46.45% (n=64) time: 6.804
2026-01-03 14:16:33,986: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 14:16:33,987: t15.2023.08.13 val PER: 0.1081
2026-01-03 14:16:33,987: t15.2023.08.18 val PER: 0.1023
2026-01-03 14:16:33,987: t15.2023.08.20 val PER: 0.1072
2026-01-03 14:16:33,987: t15.2023.08.25 val PER: 0.0964
2026-01-03 14:16:33,987: t15.2023.08.27 val PER: 0.1897
2026-01-03 14:16:33,987: t15.2023.09.01 val PER: 0.0795
2026-01-03 14:16:33,987: t15.2023.09.03 val PER: 0.1508
2026-01-03 14:16:33,987: t15.2023.09.24 val PER: 0.1323
2026-01-03 14:16:33,987: t15.2023.09.29 val PER: 0.1340
2026-01-03 14:16:33,987: t15.2023.10.01 val PER: 0.1704
2026-01-03 14:16:33,987: t15.2023.10.06 val PER: 0.0818
2026-01-03 14:16:33,987: t15.2023.10.08 val PER: 0.2490
2026-01-03 14:16:33,988: t15.2023.10.13 val PER: 0.2048
2026-01-03 14:16:33,988: t15.2023.10.15 val PER: 0.1569
2026-01-03 14:16:33,988: t15.2023.10.20 val PER: 0.1812
2026-01-03 14:16:33,988: t15.2023.10.22 val PER: 0.1069
2026-01-03 14:16:33,988: t15.2023.11.03 val PER: 0.1764
2026-01-03 14:16:33,988: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:16:33,988: t15.2023.11.17 val PER: 0.0420
2026-01-03 14:16:33,988: t15.2023.11.19 val PER: 0.0319
2026-01-03 14:16:33,988: t15.2023.11.26 val PER: 0.1116
2026-01-03 14:16:33,988: t15.2023.12.03 val PER: 0.1145
2026-01-03 14:16:33,988: t15.2023.12.08 val PER: 0.0985
2026-01-03 14:16:33,988: t15.2023.12.10 val PER: 0.0828
2026-01-03 14:16:33,988: t15.2023.12.17 val PER: 0.1320
2026-01-03 14:16:33,988: t15.2023.12.29 val PER: 0.1256
2026-01-03 14:16:33,988: t15.2024.02.25 val PER: 0.1096
2026-01-03 14:16:33,989: t15.2024.03.08 val PER: 0.2304
2026-01-03 14:16:33,989: t15.2024.03.15 val PER: 0.1976
2026-01-03 14:16:33,989: t15.2024.03.17 val PER: 0.1402
2026-01-03 14:16:33,989: t15.2024.05.10 val PER: 0.1605
2026-01-03 14:16:33,989: t15.2024.06.14 val PER: 0.1562
2026-01-03 14:16:33,989: t15.2024.07.19 val PER: 0.2340
2026-01-03 14:16:33,989: t15.2024.07.21 val PER: 0.0938
2026-01-03 14:16:33,989: t15.2024.07.28 val PER: 0.1353
2026-01-03 14:16:33,989: t15.2025.01.10 val PER: 0.2851
2026-01-03 14:16:33,989: t15.2025.01.12 val PER: 0.1447
2026-01-03 14:16:33,989: t15.2025.03.14 val PER: 0.3358
2026-01-03 14:16:33,989: t15.2025.03.16 val PER: 0.1793
2026-01-03 14:16:33,989: t15.2025.03.30 val PER: 0.3023
2026-01-03 14:16:33,989: t15.2025.04.13 val PER: 0.2240
2026-01-03 14:16:34,258: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_16500
2026-01-03 14:16:43,079: Train batch 16600: loss: 8.69 grad norm: 53.36 time: 0.052
2026-01-03 14:17:00,799: Train batch 16800: loss: 16.94 grad norm: 74.14 time: 0.062
2026-01-03 14:17:18,228: Train batch 17000: loss: 8.25 grad norm: 47.10 time: 0.081
2026-01-03 14:17:18,228: Running test after training batch: 17000
2026-01-03 14:17:18,324: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:17:23,814: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 14:17:23,850: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 14:17:25,765: Val batch 17000: PER (avg): 0.1488 CTC Loss (avg): 15.2213 WER(1gram): 46.19% (n=64) time: 7.536
2026-01-03 14:17:25,765: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 14:17:25,765: t15.2023.08.13 val PER: 0.1081
2026-01-03 14:17:25,765: t15.2023.08.18 val PER: 0.1039
2026-01-03 14:17:25,765: t15.2023.08.20 val PER: 0.1064
2026-01-03 14:17:25,765: t15.2023.08.25 val PER: 0.0979
2026-01-03 14:17:25,765: t15.2023.08.27 val PER: 0.1897
2026-01-03 14:17:25,765: t15.2023.09.01 val PER: 0.0787
2026-01-03 14:17:25,765: t15.2023.09.03 val PER: 0.1508
2026-01-03 14:17:25,766: t15.2023.09.24 val PER: 0.1286
2026-01-03 14:17:25,766: t15.2023.09.29 val PER: 0.1334
2026-01-03 14:17:25,766: t15.2023.10.01 val PER: 0.1658
2026-01-03 14:17:25,766: t15.2023.10.06 val PER: 0.0797
2026-01-03 14:17:25,766: t15.2023.10.08 val PER: 0.2490
2026-01-03 14:17:25,766: t15.2023.10.13 val PER: 0.2071
2026-01-03 14:17:25,766: t15.2023.10.15 val PER: 0.1549
2026-01-03 14:17:25,766: t15.2023.10.20 val PER: 0.1779
2026-01-03 14:17:25,766: t15.2023.10.22 val PER: 0.1058
2026-01-03 14:17:25,766: t15.2023.11.03 val PER: 0.1764
2026-01-03 14:17:25,766: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:17:25,766: t15.2023.11.17 val PER: 0.0389
2026-01-03 14:17:25,766: t15.2023.11.19 val PER: 0.0339
2026-01-03 14:17:25,767: t15.2023.11.26 val PER: 0.1138
2026-01-03 14:17:25,767: t15.2023.12.03 val PER: 0.1124
2026-01-03 14:17:25,767: t15.2023.12.08 val PER: 0.0992
2026-01-03 14:17:25,767: t15.2023.12.10 val PER: 0.0841
2026-01-03 14:17:25,767: t15.2023.12.17 val PER: 0.1289
2026-01-03 14:17:25,767: t15.2023.12.29 val PER: 0.1256
2026-01-03 14:17:25,767: t15.2024.02.25 val PER: 0.1110
2026-01-03 14:17:25,767: t15.2024.03.08 val PER: 0.2333
2026-01-03 14:17:25,767: t15.2024.03.15 val PER: 0.2014
2026-01-03 14:17:25,767: t15.2024.03.17 val PER: 0.1416
2026-01-03 14:17:25,768: t15.2024.05.10 val PER: 0.1620
2026-01-03 14:17:25,768: t15.2024.06.14 val PER: 0.1546
2026-01-03 14:17:25,768: t15.2024.07.19 val PER: 0.2307
2026-01-03 14:17:25,768: t15.2024.07.21 val PER: 0.0924
2026-01-03 14:17:25,768: t15.2024.07.28 val PER: 0.1346
2026-01-03 14:17:25,768: t15.2025.01.10 val PER: 0.2865
2026-01-03 14:17:25,768: t15.2025.01.12 val PER: 0.1455
2026-01-03 14:17:25,768: t15.2025.03.14 val PER: 0.3432
2026-01-03 14:17:25,768: t15.2025.03.16 val PER: 0.1780
2026-01-03 14:17:25,768: t15.2025.03.30 val PER: 0.3011
2026-01-03 14:17:25,768: t15.2025.04.13 val PER: 0.2197
2026-01-03 14:17:26,037: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_17000
2026-01-03 14:17:43,516: Train batch 17200: loss: 9.92 grad norm: 49.64 time: 0.084
2026-01-03 14:18:01,542: Train batch 17400: loss: 12.42 grad norm: 59.49 time: 0.070
2026-01-03 14:18:10,266: Running test after training batch: 17500
2026-01-03 14:18:10,410: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:18:15,110: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 14:18:15,146: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 14:18:17,058: Val batch 17500: PER (avg): 0.1487 CTC Loss (avg): 15.2063 WER(1gram): 46.70% (n=64) time: 6.791
2026-01-03 14:18:17,058: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 14:18:17,058: t15.2023.08.13 val PER: 0.1091
2026-01-03 14:18:17,058: t15.2023.08.18 val PER: 0.1031
2026-01-03 14:18:17,058: t15.2023.08.20 val PER: 0.1080
2026-01-03 14:18:17,059: t15.2023.08.25 val PER: 0.0919
2026-01-03 14:18:17,059: t15.2023.08.27 val PER: 0.1881
2026-01-03 14:18:17,059: t15.2023.09.01 val PER: 0.0787
2026-01-03 14:18:17,059: t15.2023.09.03 val PER: 0.1520
2026-01-03 14:18:17,059: t15.2023.09.24 val PER: 0.1299
2026-01-03 14:18:17,059: t15.2023.09.29 val PER: 0.1340
2026-01-03 14:18:17,059: t15.2023.10.01 val PER: 0.1671
2026-01-03 14:18:17,059: t15.2023.10.06 val PER: 0.0786
2026-01-03 14:18:17,059: t15.2023.10.08 val PER: 0.2476
2026-01-03 14:18:17,060: t15.2023.10.13 val PER: 0.2056
2026-01-03 14:18:17,060: t15.2023.10.15 val PER: 0.1562
2026-01-03 14:18:17,060: t15.2023.10.20 val PER: 0.1879
2026-01-03 14:18:17,060: t15.2023.10.22 val PER: 0.1069
2026-01-03 14:18:17,060: t15.2023.11.03 val PER: 0.1771
2026-01-03 14:18:17,060: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:18:17,060: t15.2023.11.17 val PER: 0.0404
2026-01-03 14:18:17,060: t15.2023.11.19 val PER: 0.0319
2026-01-03 14:18:17,060: t15.2023.11.26 val PER: 0.1138
2026-01-03 14:18:17,060: t15.2023.12.03 val PER: 0.1124
2026-01-03 14:18:17,061: t15.2023.12.08 val PER: 0.1012
2026-01-03 14:18:17,061: t15.2023.12.10 val PER: 0.0841
2026-01-03 14:18:17,061: t15.2023.12.17 val PER: 0.1289
2026-01-03 14:18:17,061: t15.2023.12.29 val PER: 0.1242
2026-01-03 14:18:17,061: t15.2024.02.25 val PER: 0.1096
2026-01-03 14:18:17,061: t15.2024.03.08 val PER: 0.2333
2026-01-03 14:18:17,061: t15.2024.03.15 val PER: 0.1982
2026-01-03 14:18:17,061: t15.2024.03.17 val PER: 0.1430
2026-01-03 14:18:17,061: t15.2024.05.10 val PER: 0.1590
2026-01-03 14:18:17,061: t15.2024.06.14 val PER: 0.1514
2026-01-03 14:18:17,062: t15.2024.07.19 val PER: 0.2320
2026-01-03 14:18:17,062: t15.2024.07.21 val PER: 0.0931
2026-01-03 14:18:17,062: t15.2024.07.28 val PER: 0.1316
2026-01-03 14:18:17,062: t15.2025.01.10 val PER: 0.2810
2026-01-03 14:18:17,062: t15.2025.01.12 val PER: 0.1470
2026-01-03 14:18:17,062: t15.2025.03.14 val PER: 0.3432
2026-01-03 14:18:17,062: t15.2025.03.16 val PER: 0.1780
2026-01-03 14:18:17,062: t15.2025.03.30 val PER: 0.3011
2026-01-03 14:18:17,062: t15.2025.04.13 val PER: 0.2197
2026-01-03 14:18:17,327: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_17500
2026-01-03 14:18:26,205: Train batch 17600: loss: 10.60 grad norm: 54.95 time: 0.051
2026-01-03 14:18:44,150: Train batch 17800: loss: 6.61 grad norm: 49.14 time: 0.041
2026-01-03 14:19:01,943: Train batch 18000: loss: 11.60 grad norm: 64.51 time: 0.060
2026-01-03 14:19:01,943: Running test after training batch: 18000
2026-01-03 14:19:02,083: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:19:06,925: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 14:19:06,961: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 14:19:08,852: Val batch 18000: PER (avg): 0.1490 CTC Loss (avg): 15.1965 WER(1gram): 46.45% (n=64) time: 6.909
2026-01-03 14:19:08,853: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 14:19:08,853: t15.2023.08.13 val PER: 0.1102
2026-01-03 14:19:08,853: t15.2023.08.18 val PER: 0.1039
2026-01-03 14:19:08,853: t15.2023.08.20 val PER: 0.1088
2026-01-03 14:19:08,853: t15.2023.08.25 val PER: 0.0949
2026-01-03 14:19:08,853: t15.2023.08.27 val PER: 0.1897
2026-01-03 14:19:08,853: t15.2023.09.01 val PER: 0.0795
2026-01-03 14:19:08,853: t15.2023.09.03 val PER: 0.1532
2026-01-03 14:19:08,853: t15.2023.09.24 val PER: 0.1299
2026-01-03 14:19:08,853: t15.2023.09.29 val PER: 0.1334
2026-01-03 14:19:08,853: t15.2023.10.01 val PER: 0.1671
2026-01-03 14:19:08,853: t15.2023.10.06 val PER: 0.0786
2026-01-03 14:19:08,853: t15.2023.10.08 val PER: 0.2503
2026-01-03 14:19:08,853: t15.2023.10.13 val PER: 0.2071
2026-01-03 14:19:08,853: t15.2023.10.15 val PER: 0.1569
2026-01-03 14:19:08,853: t15.2023.10.20 val PER: 0.1913
2026-01-03 14:19:08,854: t15.2023.10.22 val PER: 0.1058
2026-01-03 14:19:08,854: t15.2023.11.03 val PER: 0.1737
2026-01-03 14:19:08,854: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:19:08,854: t15.2023.11.17 val PER: 0.0404
2026-01-03 14:19:08,854: t15.2023.11.19 val PER: 0.0299
2026-01-03 14:19:08,854: t15.2023.11.26 val PER: 0.1145
2026-01-03 14:19:08,854: t15.2023.12.03 val PER: 0.1145
2026-01-03 14:19:08,854: t15.2023.12.08 val PER: 0.1025
2026-01-03 14:19:08,855: t15.2023.12.10 val PER: 0.0841
2026-01-03 14:19:08,855: t15.2023.12.17 val PER: 0.1310
2026-01-03 14:19:08,855: t15.2023.12.29 val PER: 0.1242
2026-01-03 14:19:08,855: t15.2024.02.25 val PER: 0.1096
2026-01-03 14:19:08,855: t15.2024.03.08 val PER: 0.2418
2026-01-03 14:19:08,855: t15.2024.03.15 val PER: 0.1970
2026-01-03 14:19:08,855: t15.2024.03.17 val PER: 0.1402
2026-01-03 14:19:08,855: t15.2024.05.10 val PER: 0.1590
2026-01-03 14:19:08,855: t15.2024.06.14 val PER: 0.1562
2026-01-03 14:19:08,855: t15.2024.07.19 val PER: 0.2334
2026-01-03 14:19:08,855: t15.2024.07.21 val PER: 0.0917
2026-01-03 14:19:08,855: t15.2024.07.28 val PER: 0.1338
2026-01-03 14:19:08,855: t15.2025.01.10 val PER: 0.2824
2026-01-03 14:19:08,856: t15.2025.01.12 val PER: 0.1470
2026-01-03 14:19:08,856: t15.2025.03.14 val PER: 0.3402
2026-01-03 14:19:08,856: t15.2025.03.16 val PER: 0.1741
2026-01-03 14:19:08,856: t15.2025.03.30 val PER: 0.2977
2026-01-03 14:19:08,856: t15.2025.04.13 val PER: 0.2197
2026-01-03 14:19:09,129: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_18000
2026-01-03 14:19:26,449: Train batch 18200: loss: 8.17 grad norm: 47.66 time: 0.073
2026-01-03 14:19:43,581: Train batch 18400: loss: 5.28 grad norm: 45.96 time: 0.058
2026-01-03 14:19:52,116: Running test after training batch: 18500
2026-01-03 14:19:52,208: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:19:56,961: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 14:19:56,998: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 14:19:58,934: Val batch 18500: PER (avg): 0.1487 CTC Loss (avg): 15.1915 WER(1gram): 46.70% (n=64) time: 6.818
2026-01-03 14:19:58,935: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 14:19:58,935: t15.2023.08.13 val PER: 0.1081
2026-01-03 14:19:58,935: t15.2023.08.18 val PER: 0.1031
2026-01-03 14:19:58,935: t15.2023.08.20 val PER: 0.1104
2026-01-03 14:19:58,935: t15.2023.08.25 val PER: 0.0934
2026-01-03 14:19:58,935: t15.2023.08.27 val PER: 0.1897
2026-01-03 14:19:58,935: t15.2023.09.01 val PER: 0.0804
2026-01-03 14:19:58,935: t15.2023.09.03 val PER: 0.1520
2026-01-03 14:19:58,935: t15.2023.09.24 val PER: 0.1286
2026-01-03 14:19:58,935: t15.2023.09.29 val PER: 0.1334
2026-01-03 14:19:58,935: t15.2023.10.01 val PER: 0.1678
2026-01-03 14:19:58,936: t15.2023.10.06 val PER: 0.0786
2026-01-03 14:19:58,936: t15.2023.10.08 val PER: 0.2490
2026-01-03 14:19:58,936: t15.2023.10.13 val PER: 0.2040
2026-01-03 14:19:58,936: t15.2023.10.15 val PER: 0.1549
2026-01-03 14:19:58,936: t15.2023.10.20 val PER: 0.1879
2026-01-03 14:19:58,936: t15.2023.10.22 val PER: 0.1080
2026-01-03 14:19:58,936: t15.2023.11.03 val PER: 0.1757
2026-01-03 14:19:58,936: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:19:58,936: t15.2023.11.17 val PER: 0.0389
2026-01-03 14:19:58,936: t15.2023.11.19 val PER: 0.0319
2026-01-03 14:19:58,936: t15.2023.11.26 val PER: 0.1145
2026-01-03 14:19:58,936: t15.2023.12.03 val PER: 0.1124
2026-01-03 14:19:58,937: t15.2023.12.08 val PER: 0.1025
2026-01-03 14:19:58,937: t15.2023.12.10 val PER: 0.0841
2026-01-03 14:19:58,937: t15.2023.12.17 val PER: 0.1320
2026-01-03 14:19:58,937: t15.2023.12.29 val PER: 0.1263
2026-01-03 14:19:58,937: t15.2024.02.25 val PER: 0.1081
2026-01-03 14:19:58,937: t15.2024.03.08 val PER: 0.2333
2026-01-03 14:19:58,937: t15.2024.03.15 val PER: 0.1964
2026-01-03 14:19:58,937: t15.2024.03.17 val PER: 0.1402
2026-01-03 14:19:58,937: t15.2024.05.10 val PER: 0.1575
2026-01-03 14:19:58,937: t15.2024.06.14 val PER: 0.1562
2026-01-03 14:19:58,938: t15.2024.07.19 val PER: 0.2327
2026-01-03 14:19:58,938: t15.2024.07.21 val PER: 0.0910
2026-01-03 14:19:58,938: t15.2024.07.28 val PER: 0.1338
2026-01-03 14:19:58,938: t15.2025.01.10 val PER: 0.2851
2026-01-03 14:19:58,938: t15.2025.01.12 val PER: 0.1447
2026-01-03 14:19:58,938: t15.2025.03.14 val PER: 0.3417
2026-01-03 14:19:58,938: t15.2025.03.16 val PER: 0.1780
2026-01-03 14:19:58,938: t15.2025.03.30 val PER: 0.2966
2026-01-03 14:19:58,938: t15.2025.04.13 val PER: 0.2211
2026-01-03 14:19:59,212: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_18500
2026-01-03 14:20:14,674: Train batch 18600: loss: 13.20 grad norm: 61.74 time: 0.067
2026-01-03 14:20:33,192: Train batch 18800: loss: 8.85 grad norm: 50.36 time: 0.065
2026-01-03 14:20:51,299: Train batch 19000: loss: 8.80 grad norm: 45.37 time: 0.064
2026-01-03 14:20:51,300: Running test after training batch: 19000
2026-01-03 14:20:51,422: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:20:56,711: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 14:20:56,747: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 14:20:58,687: Val batch 19000: PER (avg): 0.1491 CTC Loss (avg): 15.1973 WER(1gram): 46.70% (n=64) time: 7.387
2026-01-03 14:20:58,687: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 14:20:58,687: t15.2023.08.13 val PER: 0.1091
2026-01-03 14:20:58,687: t15.2023.08.18 val PER: 0.1048
2026-01-03 14:20:58,687: t15.2023.08.20 val PER: 0.1128
2026-01-03 14:20:58,687: t15.2023.08.25 val PER: 0.0919
2026-01-03 14:20:58,687: t15.2023.08.27 val PER: 0.1881
2026-01-03 14:20:58,687: t15.2023.09.01 val PER: 0.0804
2026-01-03 14:20:58,688: t15.2023.09.03 val PER: 0.1508
2026-01-03 14:20:58,688: t15.2023.09.24 val PER: 0.1299
2026-01-03 14:20:58,688: t15.2023.09.29 val PER: 0.1327
2026-01-03 14:20:58,688: t15.2023.10.01 val PER: 0.1697
2026-01-03 14:20:58,688: t15.2023.10.06 val PER: 0.0775
2026-01-03 14:20:58,688: t15.2023.10.08 val PER: 0.2490
2026-01-03 14:20:58,688: t15.2023.10.13 val PER: 0.2040
2026-01-03 14:20:58,688: t15.2023.10.15 val PER: 0.1556
2026-01-03 14:20:58,688: t15.2023.10.20 val PER: 0.1913
2026-01-03 14:20:58,688: t15.2023.10.22 val PER: 0.1091
2026-01-03 14:20:58,688: t15.2023.11.03 val PER: 0.1777
2026-01-03 14:20:58,688: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:20:58,688: t15.2023.11.17 val PER: 0.0389
2026-01-03 14:20:58,689: t15.2023.11.19 val PER: 0.0319
2026-01-03 14:20:58,689: t15.2023.11.26 val PER: 0.1152
2026-01-03 14:20:58,689: t15.2023.12.03 val PER: 0.1113
2026-01-03 14:20:58,689: t15.2023.12.08 val PER: 0.1005
2026-01-03 14:20:58,689: t15.2023.12.10 val PER: 0.0854
2026-01-03 14:20:58,689: t15.2023.12.17 val PER: 0.1331
2026-01-03 14:20:58,689: t15.2023.12.29 val PER: 0.1270
2026-01-03 14:20:58,689: t15.2024.02.25 val PER: 0.1096
2026-01-03 14:20:58,689: t15.2024.03.08 val PER: 0.2390
2026-01-03 14:20:58,689: t15.2024.03.15 val PER: 0.1976
2026-01-03 14:20:58,689: t15.2024.03.17 val PER: 0.1409
2026-01-03 14:20:58,689: t15.2024.05.10 val PER: 0.1590
2026-01-03 14:20:58,689: t15.2024.06.14 val PER: 0.1514
2026-01-03 14:20:58,689: t15.2024.07.19 val PER: 0.2314
2026-01-03 14:20:58,690: t15.2024.07.21 val PER: 0.0924
2026-01-03 14:20:58,690: t15.2024.07.28 val PER: 0.1331
2026-01-03 14:20:58,690: t15.2025.01.10 val PER: 0.2851
2026-01-03 14:20:58,690: t15.2025.01.12 val PER: 0.1424
2026-01-03 14:20:58,690: t15.2025.03.14 val PER: 0.3447
2026-01-03 14:20:58,690: t15.2025.03.16 val PER: 0.1806
2026-01-03 14:20:58,690: t15.2025.03.30 val PER: 0.2977
2026-01-03 14:20:58,690: t15.2025.04.13 val PER: 0.2154
2026-01-03 14:20:58,943: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_19000
2026-01-03 14:21:17,108: Train batch 19200: loss: 6.19 grad norm: 44.49 time: 0.063
2026-01-03 14:21:35,779: Train batch 19400: loss: 5.48 grad norm: 37.12 time: 0.053
2026-01-03 14:21:44,824: Running test after training batch: 19500
2026-01-03 14:21:45,018: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:21:49,704: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 14:21:49,736: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 14:21:51,436: Val batch 19500: PER (avg): 0.1495 CTC Loss (avg): 15.1920 WER(1gram): 46.45% (n=64) time: 6.612
2026-01-03 14:21:51,436: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-03 14:21:51,436: t15.2023.08.13 val PER: 0.1091
2026-01-03 14:21:51,436: t15.2023.08.18 val PER: 0.1048
2026-01-03 14:21:51,436: t15.2023.08.20 val PER: 0.1136
2026-01-03 14:21:51,436: t15.2023.08.25 val PER: 0.0919
2026-01-03 14:21:51,436: t15.2023.08.27 val PER: 0.1897
2026-01-03 14:21:51,437: t15.2023.09.01 val PER: 0.0812
2026-01-03 14:21:51,437: t15.2023.09.03 val PER: 0.1520
2026-01-03 14:21:51,437: t15.2023.09.24 val PER: 0.1299
2026-01-03 14:21:51,437: t15.2023.09.29 val PER: 0.1340
2026-01-03 14:21:51,437: t15.2023.10.01 val PER: 0.1704
2026-01-03 14:21:51,437: t15.2023.10.06 val PER: 0.0775
2026-01-03 14:21:51,437: t15.2023.10.08 val PER: 0.2517
2026-01-03 14:21:51,437: t15.2023.10.13 val PER: 0.2048
2026-01-03 14:21:51,437: t15.2023.10.15 val PER: 0.1562
2026-01-03 14:21:51,437: t15.2023.10.20 val PER: 0.1846
2026-01-03 14:21:51,438: t15.2023.10.22 val PER: 0.1080
2026-01-03 14:21:51,438: t15.2023.11.03 val PER: 0.1771
2026-01-03 14:21:51,438: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:21:51,438: t15.2023.11.17 val PER: 0.0404
2026-01-03 14:21:51,439: t15.2023.11.19 val PER: 0.0319
2026-01-03 14:21:51,439: t15.2023.11.26 val PER: 0.1159
2026-01-03 14:21:51,439: t15.2023.12.03 val PER: 0.1155
2026-01-03 14:21:51,439: t15.2023.12.08 val PER: 0.1012
2026-01-03 14:21:51,439: t15.2023.12.10 val PER: 0.0815
2026-01-03 14:21:51,439: t15.2023.12.17 val PER: 0.1310
2026-01-03 14:21:51,439: t15.2023.12.29 val PER: 0.1283
2026-01-03 14:21:51,439: t15.2024.02.25 val PER: 0.1096
2026-01-03 14:21:51,439: t15.2024.03.08 val PER: 0.2390
2026-01-03 14:21:51,439: t15.2024.03.15 val PER: 0.1976
2026-01-03 14:21:51,439: t15.2024.03.17 val PER: 0.1409
2026-01-03 14:21:51,439: t15.2024.05.10 val PER: 0.1575
2026-01-03 14:21:51,439: t15.2024.06.14 val PER: 0.1530
2026-01-03 14:21:51,439: t15.2024.07.19 val PER: 0.2334
2026-01-03 14:21:51,439: t15.2024.07.21 val PER: 0.0938
2026-01-03 14:21:51,440: t15.2024.07.28 val PER: 0.1346
2026-01-03 14:21:51,440: t15.2025.01.10 val PER: 0.2893
2026-01-03 14:21:51,440: t15.2025.01.12 val PER: 0.1424
2026-01-03 14:21:51,440: t15.2025.03.14 val PER: 0.3417
2026-01-03 14:21:51,440: t15.2025.03.16 val PER: 0.1806
2026-01-03 14:21:51,440: t15.2025.03.30 val PER: 0.2966
2026-01-03 14:21:51,440: t15.2025.04.13 val PER: 0.2168
2026-01-03 14:21:51,730: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_19500
2026-01-03 14:22:00,559: Train batch 19600: loss: 8.32 grad norm: 48.67 time: 0.057
2026-01-03 14:22:17,873: Train batch 19800: loss: 8.23 grad norm: 51.73 time: 0.055
2026-01-03 14:22:34,971: Running test after training batch: 19999
2026-01-03 14:22:35,063: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:22:40,264: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 14:22:40,292: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 14:22:41,772: Val batch 19999: PER (avg): 0.1491 CTC Loss (avg): 15.1888 WER(1gram): 46.70% (n=64) time: 6.801
2026-01-03 14:22:41,773: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 14:22:41,773: t15.2023.08.13 val PER: 0.1091
2026-01-03 14:22:41,773: t15.2023.08.18 val PER: 0.1031
2026-01-03 14:22:41,773: t15.2023.08.20 val PER: 0.1128
2026-01-03 14:22:41,773: t15.2023.08.25 val PER: 0.0919
2026-01-03 14:22:41,773: t15.2023.08.27 val PER: 0.1865
2026-01-03 14:22:41,773: t15.2023.09.01 val PER: 0.0795
2026-01-03 14:22:41,773: t15.2023.09.03 val PER: 0.1520
2026-01-03 14:22:41,773: t15.2023.09.24 val PER: 0.1274
2026-01-03 14:22:41,774: t15.2023.09.29 val PER: 0.1334
2026-01-03 14:22:41,774: t15.2023.10.01 val PER: 0.1684
2026-01-03 14:22:41,774: t15.2023.10.06 val PER: 0.0775
2026-01-03 14:22:41,774: t15.2023.10.08 val PER: 0.2490
2026-01-03 14:22:41,774: t15.2023.10.13 val PER: 0.2071
2026-01-03 14:22:41,774: t15.2023.10.15 val PER: 0.1575
2026-01-03 14:22:41,774: t15.2023.10.20 val PER: 0.1879
2026-01-03 14:22:41,774: t15.2023.10.22 val PER: 0.1091
2026-01-03 14:22:41,774: t15.2023.11.03 val PER: 0.1757
2026-01-03 14:22:41,774: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:22:41,774: t15.2023.11.17 val PER: 0.0404
2026-01-03 14:22:41,774: t15.2023.11.19 val PER: 0.0319
2026-01-03 14:22:41,774: t15.2023.11.26 val PER: 0.1159
2026-01-03 14:22:41,774: t15.2023.12.03 val PER: 0.1134
2026-01-03 14:22:41,774: t15.2023.12.08 val PER: 0.1019
2026-01-03 14:22:41,774: t15.2023.12.10 val PER: 0.0815
2026-01-03 14:22:41,775: t15.2023.12.17 val PER: 0.1320
2026-01-03 14:22:41,775: t15.2023.12.29 val PER: 0.1249
2026-01-03 14:22:41,775: t15.2024.02.25 val PER: 0.1096
2026-01-03 14:22:41,775: t15.2024.03.08 val PER: 0.2376
2026-01-03 14:22:41,775: t15.2024.03.15 val PER: 0.1976
2026-01-03 14:22:41,775: t15.2024.03.17 val PER: 0.1402
2026-01-03 14:22:41,775: t15.2024.05.10 val PER: 0.1590
2026-01-03 14:22:41,775: t15.2024.06.14 val PER: 0.1577
2026-01-03 14:22:41,775: t15.2024.07.19 val PER: 0.2307
2026-01-03 14:22:41,776: t15.2024.07.21 val PER: 0.0924
2026-01-03 14:22:41,776: t15.2024.07.28 val PER: 0.1353
2026-01-03 14:22:41,776: t15.2025.01.10 val PER: 0.2906
2026-01-03 14:22:41,776: t15.2025.01.12 val PER: 0.1409
2026-01-03 14:22:41,776: t15.2025.03.14 val PER: 0.3417
2026-01-03 14:22:41,776: t15.2025.03.16 val PER: 0.1806
2026-01-03 14:22:41,776: t15.2025.03.30 val PER: 0.2977
2026-01-03 14:22:41,776: t15.2025.04.13 val PER: 0.2197
2026-01-03 14:22:42,073: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step15k_f01/checkpoint/checkpoint_batch_19999
2026-01-03 14:22:42,108: Best avg val PER achieved: 0.15805
2026-01-03 14:22:42,109: Total training time: 34.77 minutes

=== RUN step8k_f01.yaml ===
2026-01-03 14:22:46,706: Using device: cuda:0
2026-01-03 14:22:48,499: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-03 14:22:48,519: Using 45 sessions after filtering (from 45).
2026-01-03 14:22:48,910: Using torch.compile (if available)
2026-01-03 14:22:48,910: torch.compile not available (torch<2.0). Skipping.
2026-01-03 14:22:48,911: Initialized RNN decoding model
2026-01-03 14:22:48,911: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 14:22:48,911: Model has 44,907,305 parameters
2026-01-03 14:22:48,911: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 14:22:50,203: Successfully initialized datasets
2026-01-03 14:22:50,204: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 14:22:52,427: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.199
2026-01-03 14:22:52,427: Running test after training batch: 0
2026-01-03 14:22:52,536: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:22:57,740: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-03 14:22:58,448: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-03 14:23:34,916: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 42.488
2026-01-03 14:23:34,916: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-03 14:23:34,916: t15.2023.08.13 val PER: 1.3056
2026-01-03 14:23:34,916: t15.2023.08.18 val PER: 1.4208
2026-01-03 14:23:34,916: t15.2023.08.20 val PER: 1.3002
2026-01-03 14:23:34,916: t15.2023.08.25 val PER: 1.3389
2026-01-03 14:23:34,916: t15.2023.08.27 val PER: 1.2460
2026-01-03 14:23:34,916: t15.2023.09.01 val PER: 1.4537
2026-01-03 14:23:34,917: t15.2023.09.03 val PER: 1.3171
2026-01-03 14:23:34,917: t15.2023.09.24 val PER: 1.5461
2026-01-03 14:23:34,917: t15.2023.09.29 val PER: 1.4671
2026-01-03 14:23:34,917: t15.2023.10.01 val PER: 1.2147
2026-01-03 14:23:34,917: t15.2023.10.06 val PER: 1.4876
2026-01-03 14:23:34,917: t15.2023.10.08 val PER: 1.1827
2026-01-03 14:23:34,917: t15.2023.10.13 val PER: 1.3964
2026-01-03 14:23:34,917: t15.2023.10.15 val PER: 1.3889
2026-01-03 14:23:34,917: t15.2023.10.20 val PER: 1.4866
2026-01-03 14:23:34,917: t15.2023.10.22 val PER: 1.3942
2026-01-03 14:23:34,917: t15.2023.11.03 val PER: 1.5923
2026-01-03 14:23:34,917: t15.2023.11.04 val PER: 2.0171
2026-01-03 14:23:34,917: t15.2023.11.17 val PER: 1.9518
2026-01-03 14:23:34,917: t15.2023.11.19 val PER: 1.6707
2026-01-03 14:23:34,918: t15.2023.11.26 val PER: 1.5413
2026-01-03 14:23:34,918: t15.2023.12.03 val PER: 1.4254
2026-01-03 14:23:34,918: t15.2023.12.08 val PER: 1.4487
2026-01-03 14:23:34,918: t15.2023.12.10 val PER: 1.6899
2026-01-03 14:23:34,918: t15.2023.12.17 val PER: 1.3077
2026-01-03 14:23:34,918: t15.2023.12.29 val PER: 1.4063
2026-01-03 14:23:34,918: t15.2024.02.25 val PER: 1.4228
2026-01-03 14:23:34,918: t15.2024.03.08 val PER: 1.3257
2026-01-03 14:23:34,918: t15.2024.03.15 val PER: 1.3196
2026-01-03 14:23:34,918: t15.2024.03.17 val PER: 1.4052
2026-01-03 14:23:34,918: t15.2024.05.10 val PER: 1.3224
2026-01-03 14:23:34,918: t15.2024.06.14 val PER: 1.5315
2026-01-03 14:23:34,918: t15.2024.07.19 val PER: 1.0817
2026-01-03 14:23:34,918: t15.2024.07.21 val PER: 1.6290
2026-01-03 14:23:34,919: t15.2024.07.28 val PER: 1.6588
2026-01-03 14:23:34,919: t15.2025.01.10 val PER: 1.0923
2026-01-03 14:23:34,919: t15.2025.01.12 val PER: 1.7629
2026-01-03 14:23:34,919: t15.2025.03.14 val PER: 1.0414
2026-01-03 14:23:34,919: t15.2025.03.16 val PER: 1.6257
2026-01-03 14:23:34,919: t15.2025.03.30 val PER: 1.2874
2026-01-03 14:23:34,919: t15.2025.04.13 val PER: 1.5949
2026-01-03 14:23:34,920: New best val WER(1gram) inf% --> 100.00%
2026-01-03 14:23:34,920: Checkpointing model
2026-01-03 14:23:35,302: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/best_checkpoint
2026-01-03 14:23:35,732: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_0
2026-01-03 14:23:53,587: Train batch 200: loss: 77.59 grad norm: 105.91 time: 0.054
2026-01-03 14:24:11,257: Train batch 400: loss: 53.76 grad norm: 87.14 time: 0.063
2026-01-03 14:24:19,874: Running test after training batch: 500
2026-01-03 14:24:20,007: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:24:25,111: WER debug example
  GT : you can see the code at this point as well
  PR : yule and ease thus uhde at this ide is aisle
2026-01-03 14:24:25,145: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as adz
2026-01-03 14:24:27,527: Val batch 500: PER (avg): 0.5175 CTC Loss (avg): 55.3218 WER(1gram): 89.85% (n=64) time: 7.653
2026-01-03 14:24:27,528: WER lens: avg_true_words=6.16 avg_pred_words=5.67 max_pred_words=11
2026-01-03 14:24:27,528: t15.2023.08.13 val PER: 0.4636
2026-01-03 14:24:27,528: t15.2023.08.18 val PER: 0.4501
2026-01-03 14:24:27,528: t15.2023.08.20 val PER: 0.4488
2026-01-03 14:24:27,528: t15.2023.08.25 val PER: 0.4352
2026-01-03 14:24:27,528: t15.2023.08.27 val PER: 0.5129
2026-01-03 14:24:27,528: t15.2023.09.01 val PER: 0.4221
2026-01-03 14:24:27,528: t15.2023.09.03 val PER: 0.5048
2026-01-03 14:24:27,528: t15.2023.09.24 val PER: 0.4308
2026-01-03 14:24:27,528: t15.2023.09.29 val PER: 0.4722
2026-01-03 14:24:27,528: t15.2023.10.01 val PER: 0.5172
2026-01-03 14:24:27,529: t15.2023.10.06 val PER: 0.4230
2026-01-03 14:24:27,529: t15.2023.10.08 val PER: 0.5372
2026-01-03 14:24:27,529: t15.2023.10.13 val PER: 0.5787
2026-01-03 14:24:27,529: t15.2023.10.15 val PER: 0.4957
2026-01-03 14:24:27,529: t15.2023.10.20 val PER: 0.4497
2026-01-03 14:24:27,529: t15.2023.10.22 val PER: 0.4465
2026-01-03 14:24:27,529: t15.2023.11.03 val PER: 0.5068
2026-01-03 14:24:27,529: t15.2023.11.04 val PER: 0.2628
2026-01-03 14:24:27,529: t15.2023.11.17 val PER: 0.3577
2026-01-03 14:24:27,529: t15.2023.11.19 val PER: 0.3194
2026-01-03 14:24:27,529: t15.2023.11.26 val PER: 0.5507
2026-01-03 14:24:27,529: t15.2023.12.03 val PER: 0.4958
2026-01-03 14:24:27,529: t15.2023.12.08 val PER: 0.5160
2026-01-03 14:24:27,529: t15.2023.12.10 val PER: 0.4625
2026-01-03 14:24:27,529: t15.2023.12.17 val PER: 0.5582
2026-01-03 14:24:27,530: t15.2023.12.29 val PER: 0.5374
2026-01-03 14:24:27,530: t15.2024.02.25 val PER: 0.4733
2026-01-03 14:24:27,530: t15.2024.03.08 val PER: 0.6174
2026-01-03 14:24:27,530: t15.2024.03.15 val PER: 0.5547
2026-01-03 14:24:27,530: t15.2024.03.17 val PER: 0.5000
2026-01-03 14:24:27,530: t15.2024.05.10 val PER: 0.5438
2026-01-03 14:24:27,530: t15.2024.06.14 val PER: 0.5205
2026-01-03 14:24:27,530: t15.2024.07.19 val PER: 0.6684
2026-01-03 14:24:27,530: t15.2024.07.21 val PER: 0.4690
2026-01-03 14:24:27,530: t15.2024.07.28 val PER: 0.5081
2026-01-03 14:24:27,530: t15.2025.01.10 val PER: 0.7397
2026-01-03 14:24:27,530: t15.2025.01.12 val PER: 0.5574
2026-01-03 14:24:27,530: t15.2025.03.14 val PER: 0.7544
2026-01-03 14:24:27,530: t15.2025.03.16 val PER: 0.5955
2026-01-03 14:24:27,530: t15.2025.03.30 val PER: 0.7322
2026-01-03 14:24:27,530: t15.2025.04.13 val PER: 0.5792
2026-01-03 14:24:27,532: New best val WER(1gram) 100.00% --> 89.85%
2026-01-03 14:24:27,532: Checkpointing model
2026-01-03 14:24:27,793: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/best_checkpoint
2026-01-03 14:24:28,046: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_500
2026-01-03 14:24:37,592: Train batch 600: loss: 48.62 grad norm: 74.66 time: 0.077
2026-01-03 14:24:54,662: Train batch 800: loss: 41.28 grad norm: 85.73 time: 0.057
2026-01-03 14:25:12,352: Train batch 1000: loss: 42.84 grad norm: 86.20 time: 0.066
2026-01-03 14:25:12,353: Running test after training batch: 1000
2026-01-03 14:25:12,479: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:25:17,253: WER debug example
  GT : you can see the code at this point as well
  PR : wooed it ease thus good at this boyde is while
2026-01-03 14:25:17,284: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke that us it
2026-01-03 14:25:19,168: Val batch 1000: PER (avg): 0.4095 CTC Loss (avg): 42.5847 WER(1gram): 79.95% (n=64) time: 6.815
2026-01-03 14:25:19,168: WER lens: avg_true_words=6.16 avg_pred_words=5.50 max_pred_words=12
2026-01-03 14:25:19,168: t15.2023.08.13 val PER: 0.3877
2026-01-03 14:25:19,168: t15.2023.08.18 val PER: 0.3319
2026-01-03 14:25:19,169: t15.2023.08.20 val PER: 0.3376
2026-01-03 14:25:19,169: t15.2023.08.25 val PER: 0.2877
2026-01-03 14:25:19,169: t15.2023.08.27 val PER: 0.4180
2026-01-03 14:25:19,169: t15.2023.09.01 val PER: 0.3028
2026-01-03 14:25:19,169: t15.2023.09.03 val PER: 0.4026
2026-01-03 14:25:19,169: t15.2023.09.24 val PER: 0.3350
2026-01-03 14:25:19,169: t15.2023.09.29 val PER: 0.3612
2026-01-03 14:25:19,169: t15.2023.10.01 val PER: 0.4075
2026-01-03 14:25:19,169: t15.2023.10.06 val PER: 0.2982
2026-01-03 14:25:19,169: t15.2023.10.08 val PER: 0.4547
2026-01-03 14:25:19,169: t15.2023.10.13 val PER: 0.4639
2026-01-03 14:25:19,169: t15.2023.10.15 val PER: 0.3817
2026-01-03 14:25:19,170: t15.2023.10.20 val PER: 0.3826
2026-01-03 14:25:19,170: t15.2023.10.22 val PER: 0.3552
2026-01-03 14:25:19,170: t15.2023.11.03 val PER: 0.4084
2026-01-03 14:25:19,170: t15.2023.11.04 val PER: 0.1502
2026-01-03 14:25:19,170: t15.2023.11.17 val PER: 0.2628
2026-01-03 14:25:19,170: t15.2023.11.19 val PER: 0.2116
2026-01-03 14:25:19,170: t15.2023.11.26 val PER: 0.4500
2026-01-03 14:25:19,170: t15.2023.12.03 val PER: 0.4055
2026-01-03 14:25:19,170: t15.2023.12.08 val PER: 0.4081
2026-01-03 14:25:19,170: t15.2023.12.10 val PER: 0.3456
2026-01-03 14:25:19,170: t15.2023.12.17 val PER: 0.4127
2026-01-03 14:25:19,170: t15.2023.12.29 val PER: 0.4029
2026-01-03 14:25:19,171: t15.2024.02.25 val PER: 0.3581
2026-01-03 14:25:19,171: t15.2024.03.08 val PER: 0.4979
2026-01-03 14:25:19,171: t15.2024.03.15 val PER: 0.4422
2026-01-03 14:25:19,171: t15.2024.03.17 val PER: 0.4086
2026-01-03 14:25:19,171: t15.2024.05.10 val PER: 0.4264
2026-01-03 14:25:19,171: t15.2024.06.14 val PER: 0.4085
2026-01-03 14:25:19,171: t15.2024.07.19 val PER: 0.5300
2026-01-03 14:25:19,171: t15.2024.07.21 val PER: 0.3814
2026-01-03 14:25:19,171: t15.2024.07.28 val PER: 0.4140
2026-01-03 14:25:19,171: t15.2025.01.10 val PER: 0.6088
2026-01-03 14:25:19,171: t15.2025.01.12 val PER: 0.4588
2026-01-03 14:25:19,171: t15.2025.03.14 val PER: 0.6405
2026-01-03 14:25:19,171: t15.2025.03.16 val PER: 0.4935
2026-01-03 14:25:19,171: t15.2025.03.30 val PER: 0.6506
2026-01-03 14:25:19,171: t15.2025.04.13 val PER: 0.4893
2026-01-03 14:25:19,172: New best val WER(1gram) 89.85% --> 79.95%
2026-01-03 14:25:19,172: Checkpointing model
2026-01-03 14:25:19,434: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/best_checkpoint
2026-01-03 14:25:19,687: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_1000
2026-01-03 14:25:37,313: Train batch 1200: loss: 32.65 grad norm: 73.09 time: 0.068
2026-01-03 14:25:54,439: Train batch 1400: loss: 36.30 grad norm: 85.67 time: 0.060
2026-01-03 14:26:03,171: Running test after training batch: 1500
2026-01-03 14:26:03,273: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:26:08,367: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt ease the owed at this boyde is will
2026-01-03 14:26:08,397: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that os
2026-01-03 14:26:09,973: Val batch 1500: PER (avg): 0.3822 CTC Loss (avg): 37.3099 WER(1gram): 76.14% (n=64) time: 6.801
2026-01-03 14:26:09,973: WER lens: avg_true_words=6.16 avg_pred_words=5.05 max_pred_words=11
2026-01-03 14:26:09,973: t15.2023.08.13 val PER: 0.3462
2026-01-03 14:26:09,973: t15.2023.08.18 val PER: 0.3269
2026-01-03 14:26:09,973: t15.2023.08.20 val PER: 0.2994
2026-01-03 14:26:09,973: t15.2023.08.25 val PER: 0.2515
2026-01-03 14:26:09,973: t15.2023.08.27 val PER: 0.4035
2026-01-03 14:26:09,974: t15.2023.09.01 val PER: 0.2808
2026-01-03 14:26:09,974: t15.2023.09.03 val PER: 0.3705
2026-01-03 14:26:09,974: t15.2023.09.24 val PER: 0.3034
2026-01-03 14:26:09,974: t15.2023.09.29 val PER: 0.3401
2026-01-03 14:26:09,974: t15.2023.10.01 val PER: 0.3917
2026-01-03 14:26:09,974: t15.2023.10.06 val PER: 0.2745
2026-01-03 14:26:09,974: t15.2023.10.08 val PER: 0.4344
2026-01-03 14:26:09,974: t15.2023.10.13 val PER: 0.4422
2026-01-03 14:26:09,974: t15.2023.10.15 val PER: 0.3639
2026-01-03 14:26:09,974: t15.2023.10.20 val PER: 0.3356
2026-01-03 14:26:09,974: t15.2023.10.22 val PER: 0.3107
2026-01-03 14:26:09,974: t15.2023.11.03 val PER: 0.3704
2026-01-03 14:26:09,974: t15.2023.11.04 val PER: 0.1160
2026-01-03 14:26:09,974: t15.2023.11.17 val PER: 0.2240
2026-01-03 14:26:09,974: t15.2023.11.19 val PER: 0.1637
2026-01-03 14:26:09,975: t15.2023.11.26 val PER: 0.4225
2026-01-03 14:26:09,975: t15.2023.12.03 val PER: 0.3813
2026-01-03 14:26:09,975: t15.2023.12.08 val PER: 0.3562
2026-01-03 14:26:09,975: t15.2023.12.10 val PER: 0.3022
2026-01-03 14:26:09,975: t15.2023.12.17 val PER: 0.3805
2026-01-03 14:26:09,975: t15.2023.12.29 val PER: 0.3734
2026-01-03 14:26:09,975: t15.2024.02.25 val PER: 0.3258
2026-01-03 14:26:09,975: t15.2024.03.08 val PER: 0.4737
2026-01-03 14:26:09,975: t15.2024.03.15 val PER: 0.4165
2026-01-03 14:26:09,975: t15.2024.03.17 val PER: 0.3794
2026-01-03 14:26:09,975: t15.2024.05.10 val PER: 0.3952
2026-01-03 14:26:09,975: t15.2024.06.14 val PER: 0.4069
2026-01-03 14:26:09,975: t15.2024.07.19 val PER: 0.5353
2026-01-03 14:26:09,975: t15.2024.07.21 val PER: 0.3400
2026-01-03 14:26:09,975: t15.2024.07.28 val PER: 0.3603
2026-01-03 14:26:09,975: t15.2025.01.10 val PER: 0.6336
2026-01-03 14:26:09,975: t15.2025.01.12 val PER: 0.4265
2026-01-03 14:26:09,976: t15.2025.03.14 val PER: 0.6139
2026-01-03 14:26:09,976: t15.2025.03.16 val PER: 0.4647
2026-01-03 14:26:09,976: t15.2025.03.30 val PER: 0.6391
2026-01-03 14:26:09,976: t15.2025.04.13 val PER: 0.4679
2026-01-03 14:26:09,977: New best val WER(1gram) 79.95% --> 76.14%
2026-01-03 14:26:09,977: Checkpointing model
2026-01-03 14:26:10,250: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/best_checkpoint
2026-01-03 14:26:10,505: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_1500
2026-01-03 14:26:18,873: Train batch 1600: loss: 36.78 grad norm: 78.65 time: 0.064
2026-01-03 14:26:36,320: Train batch 1800: loss: 34.76 grad norm: 68.08 time: 0.087
2026-01-03 14:26:53,698: Train batch 2000: loss: 34.10 grad norm: 73.58 time: 0.065
2026-01-03 14:26:53,699: Running test after training batch: 2000
2026-01-03 14:26:53,847: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:26:58,609: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the good at this bonde is will
2026-01-03 14:26:58,638: WER debug example
  GT : how does it keep the cost down
  PR : houde boss it heap the wass id
2026-01-03 14:27:00,196: Val batch 2000: PER (avg): 0.3270 CTC Loss (avg): 32.8118 WER(1gram): 68.53% (n=64) time: 6.497
2026-01-03 14:27:00,196: WER lens: avg_true_words=6.16 avg_pred_words=5.59 max_pred_words=11
2026-01-03 14:27:00,196: t15.2023.08.13 val PER: 0.2931
2026-01-03 14:27:00,196: t15.2023.08.18 val PER: 0.2448
2026-01-03 14:27:00,196: t15.2023.08.20 val PER: 0.2526
2026-01-03 14:27:00,197: t15.2023.08.25 val PER: 0.2395
2026-01-03 14:27:00,197: t15.2023.08.27 val PER: 0.3408
2026-01-03 14:27:00,197: t15.2023.09.01 val PER: 0.2281
2026-01-03 14:27:00,197: t15.2023.09.03 val PER: 0.3290
2026-01-03 14:27:00,197: t15.2023.09.24 val PER: 0.2536
2026-01-03 14:27:00,197: t15.2023.09.29 val PER: 0.2744
2026-01-03 14:27:00,197: t15.2023.10.01 val PER: 0.3263
2026-01-03 14:27:00,197: t15.2023.10.06 val PER: 0.2347
2026-01-03 14:27:00,197: t15.2023.10.08 val PER: 0.3924
2026-01-03 14:27:00,197: t15.2023.10.13 val PER: 0.3794
2026-01-03 14:27:00,197: t15.2023.10.15 val PER: 0.3013
2026-01-03 14:27:00,197: t15.2023.10.20 val PER: 0.3054
2026-01-03 14:27:00,197: t15.2023.10.22 val PER: 0.2606
2026-01-03 14:27:00,197: t15.2023.11.03 val PER: 0.3209
2026-01-03 14:27:00,197: t15.2023.11.04 val PER: 0.0990
2026-01-03 14:27:00,198: t15.2023.11.17 val PER: 0.1695
2026-01-03 14:27:00,198: t15.2023.11.19 val PER: 0.1257
2026-01-03 14:27:00,198: t15.2023.11.26 val PER: 0.3630
2026-01-03 14:27:00,198: t15.2023.12.03 val PER: 0.3141
2026-01-03 14:27:00,198: t15.2023.12.08 val PER: 0.3123
2026-01-03 14:27:00,198: t15.2023.12.10 val PER: 0.2562
2026-01-03 14:27:00,198: t15.2023.12.17 val PER: 0.3139
2026-01-03 14:27:00,198: t15.2023.12.29 val PER: 0.3377
2026-01-03 14:27:00,198: t15.2024.02.25 val PER: 0.2767
2026-01-03 14:27:00,198: t15.2024.03.08 val PER: 0.4011
2026-01-03 14:27:00,199: t15.2024.03.15 val PER: 0.3590
2026-01-03 14:27:00,199: t15.2024.03.17 val PER: 0.3452
2026-01-03 14:27:00,199: t15.2024.05.10 val PER: 0.3358
2026-01-03 14:27:00,199: t15.2024.06.14 val PER: 0.3407
2026-01-03 14:27:00,199: t15.2024.07.19 val PER: 0.4542
2026-01-03 14:27:00,199: t15.2024.07.21 val PER: 0.2972
2026-01-03 14:27:00,199: t15.2024.07.28 val PER: 0.3199
2026-01-03 14:27:00,199: t15.2025.01.10 val PER: 0.5455
2026-01-03 14:27:00,199: t15.2025.01.12 val PER: 0.3872
2026-01-03 14:27:00,199: t15.2025.03.14 val PER: 0.5311
2026-01-03 14:27:00,199: t15.2025.03.16 val PER: 0.3966
2026-01-03 14:27:00,200: t15.2025.03.30 val PER: 0.5460
2026-01-03 14:27:00,200: t15.2025.04.13 val PER: 0.4137
2026-01-03 14:27:00,201: New best val WER(1gram) 76.14% --> 68.53%
2026-01-03 14:27:00,201: Checkpointing model
2026-01-03 14:27:00,464: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/best_checkpoint
2026-01-03 14:27:00,719: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_2000
2026-01-03 14:27:17,640: Train batch 2200: loss: 28.97 grad norm: 75.25 time: 0.060
2026-01-03 14:27:34,757: Train batch 2400: loss: 29.13 grad norm: 65.55 time: 0.052
2026-01-03 14:27:43,786: Running test after training batch: 2500
2026-01-03 14:27:43,940: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:27:48,700: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-03 14:27:48,728: WER debug example
  GT : how does it keep the cost down
  PR : houde just it eke thus us it
2026-01-03 14:27:50,347: Val batch 2500: PER (avg): 0.3046 CTC Loss (avg): 30.2276 WER(1gram): 68.53% (n=64) time: 6.560
2026-01-03 14:27:50,347: WER lens: avg_true_words=6.16 avg_pred_words=5.59 max_pred_words=11
2026-01-03 14:27:50,347: t15.2023.08.13 val PER: 0.2775
2026-01-03 14:27:50,347: t15.2023.08.18 val PER: 0.2364
2026-01-03 14:27:50,348: t15.2023.08.20 val PER: 0.2470
2026-01-03 14:27:50,348: t15.2023.08.25 val PER: 0.2184
2026-01-03 14:27:50,348: t15.2023.08.27 val PER: 0.3424
2026-01-03 14:27:50,348: t15.2023.09.01 val PER: 0.2151
2026-01-03 14:27:50,348: t15.2023.09.03 val PER: 0.2993
2026-01-03 14:27:50,348: t15.2023.09.24 val PER: 0.2245
2026-01-03 14:27:50,348: t15.2023.09.29 val PER: 0.2521
2026-01-03 14:27:50,348: t15.2023.10.01 val PER: 0.3078
2026-01-03 14:27:50,348: t15.2023.10.06 val PER: 0.2131
2026-01-03 14:27:50,348: t15.2023.10.08 val PER: 0.3748
2026-01-03 14:27:50,348: t15.2023.10.13 val PER: 0.3499
2026-01-03 14:27:50,348: t15.2023.10.15 val PER: 0.2769
2026-01-03 14:27:50,348: t15.2023.10.20 val PER: 0.2886
2026-01-03 14:27:50,348: t15.2023.10.22 val PER: 0.2361
2026-01-03 14:27:50,348: t15.2023.11.03 val PER: 0.2951
2026-01-03 14:27:50,348: t15.2023.11.04 val PER: 0.0853
2026-01-03 14:27:50,349: t15.2023.11.17 val PER: 0.1337
2026-01-03 14:27:50,349: t15.2023.11.19 val PER: 0.1198
2026-01-03 14:27:50,349: t15.2023.11.26 val PER: 0.3471
2026-01-03 14:27:50,349: t15.2023.12.03 val PER: 0.2899
2026-01-03 14:27:50,349: t15.2023.12.08 val PER: 0.2783
2026-01-03 14:27:50,349: t15.2023.12.10 val PER: 0.2457
2026-01-03 14:27:50,349: t15.2023.12.17 val PER: 0.2921
2026-01-03 14:27:50,349: t15.2023.12.29 val PER: 0.3075
2026-01-03 14:27:50,349: t15.2024.02.25 val PER: 0.2402
2026-01-03 14:27:50,349: t15.2024.03.08 val PER: 0.3670
2026-01-03 14:27:50,350: t15.2024.03.15 val PER: 0.3540
2026-01-03 14:27:50,350: t15.2024.03.17 val PER: 0.3110
2026-01-03 14:27:50,350: t15.2024.05.10 val PER: 0.3105
2026-01-03 14:27:50,350: t15.2024.06.14 val PER: 0.3060
2026-01-03 14:27:50,350: t15.2024.07.19 val PER: 0.4509
2026-01-03 14:27:50,350: t15.2024.07.21 val PER: 0.2641
2026-01-03 14:27:50,350: t15.2024.07.28 val PER: 0.3059
2026-01-03 14:27:50,350: t15.2025.01.10 val PER: 0.4945
2026-01-03 14:27:50,350: t15.2025.01.12 val PER: 0.3557
2026-01-03 14:27:50,350: t15.2025.03.14 val PER: 0.5059
2026-01-03 14:27:50,350: t15.2025.03.16 val PER: 0.3652
2026-01-03 14:27:50,350: t15.2025.03.30 val PER: 0.5092
2026-01-03 14:27:50,350: t15.2025.04.13 val PER: 0.3909
2026-01-03 14:27:50,840: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_2500
2026-01-03 14:27:59,660: Train batch 2600: loss: 35.58 grad norm: 88.12 time: 0.055
2026-01-03 14:28:16,766: Train batch 2800: loss: 25.88 grad norm: 73.08 time: 0.081
2026-01-03 14:28:33,653: Train batch 3000: loss: 31.42 grad norm: 71.97 time: 0.083
2026-01-03 14:28:33,653: Running test after training batch: 3000
2026-01-03 14:28:33,745: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:28:38,674: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-03 14:28:38,703: WER debug example
  GT : how does it keep the cost down
  PR : houde des it yip the cost get
2026-01-03 14:28:40,358: Val batch 3000: PER (avg): 0.2819 CTC Loss (avg): 27.9315 WER(1gram): 66.24% (n=64) time: 6.705
2026-01-03 14:28:40,359: WER lens: avg_true_words=6.16 avg_pred_words=5.94 max_pred_words=11
2026-01-03 14:28:40,359: t15.2023.08.13 val PER: 0.2599
2026-01-03 14:28:40,359: t15.2023.08.18 val PER: 0.2146
2026-01-03 14:28:40,359: t15.2023.08.20 val PER: 0.2073
2026-01-03 14:28:40,359: t15.2023.08.25 val PER: 0.1913
2026-01-03 14:28:40,359: t15.2023.08.27 val PER: 0.2942
2026-01-03 14:28:40,359: t15.2023.09.01 val PER: 0.1851
2026-01-03 14:28:40,359: t15.2023.09.03 val PER: 0.2743
2026-01-03 14:28:40,359: t15.2023.09.24 val PER: 0.2209
2026-01-03 14:28:40,359: t15.2023.09.29 val PER: 0.2348
2026-01-03 14:28:40,359: t15.2023.10.01 val PER: 0.2906
2026-01-03 14:28:40,359: t15.2023.10.06 val PER: 0.1927
2026-01-03 14:28:40,360: t15.2023.10.08 val PER: 0.3572
2026-01-03 14:28:40,360: t15.2023.10.13 val PER: 0.3437
2026-01-03 14:28:40,360: t15.2023.10.15 val PER: 0.2676
2026-01-03 14:28:40,360: t15.2023.10.20 val PER: 0.2718
2026-01-03 14:28:40,360: t15.2023.10.22 val PER: 0.2183
2026-01-03 14:28:40,360: t15.2023.11.03 val PER: 0.2720
2026-01-03 14:28:40,360: t15.2023.11.04 val PER: 0.0785
2026-01-03 14:28:40,360: t15.2023.11.17 val PER: 0.1369
2026-01-03 14:28:40,360: t15.2023.11.19 val PER: 0.1257
2026-01-03 14:28:40,360: t15.2023.11.26 val PER: 0.3043
2026-01-03 14:28:40,360: t15.2023.12.03 val PER: 0.2616
2026-01-03 14:28:40,360: t15.2023.12.08 val PER: 0.2663
2026-01-03 14:28:40,361: t15.2023.12.10 val PER: 0.2116
2026-01-03 14:28:40,361: t15.2023.12.17 val PER: 0.2817
2026-01-03 14:28:40,361: t15.2023.12.29 val PER: 0.2835
2026-01-03 14:28:40,361: t15.2024.02.25 val PER: 0.2486
2026-01-03 14:28:40,361: t15.2024.03.08 val PER: 0.3499
2026-01-03 14:28:40,361: t15.2024.03.15 val PER: 0.3358
2026-01-03 14:28:40,361: t15.2024.03.17 val PER: 0.2915
2026-01-03 14:28:40,361: t15.2024.05.10 val PER: 0.3016
2026-01-03 14:28:40,361: t15.2024.06.14 val PER: 0.3028
2026-01-03 14:28:40,361: t15.2024.07.19 val PER: 0.3995
2026-01-03 14:28:40,361: t15.2024.07.21 val PER: 0.2324
2026-01-03 14:28:40,362: t15.2024.07.28 val PER: 0.2728
2026-01-03 14:28:40,362: t15.2025.01.10 val PER: 0.4945
2026-01-03 14:28:40,362: t15.2025.01.12 val PER: 0.3179
2026-01-03 14:28:40,362: t15.2025.03.14 val PER: 0.4527
2026-01-03 14:28:40,362: t15.2025.03.16 val PER: 0.3298
2026-01-03 14:28:40,362: t15.2025.03.30 val PER: 0.4816
2026-01-03 14:28:40,362: t15.2025.04.13 val PER: 0.3538
2026-01-03 14:28:40,363: New best val WER(1gram) 68.53% --> 66.24%
2026-01-03 14:28:40,363: Checkpointing model
2026-01-03 14:28:40,625: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/best_checkpoint
2026-01-03 14:28:40,878: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_3000
2026-01-03 14:28:58,479: Train batch 3200: loss: 26.48 grad norm: 66.98 time: 0.077
2026-01-03 14:29:15,957: Train batch 3400: loss: 18.33 grad norm: 54.50 time: 0.048
2026-01-03 14:29:24,797: Running test after training batch: 3500
2026-01-03 14:29:24,927: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:29:29,683: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned eke the code at this point will
2026-01-03 14:29:29,710: WER debug example
  GT : how does it keep the cost down
  PR : houde des it eke the cussed get
2026-01-03 14:29:31,282: Val batch 3500: PER (avg): 0.2646 CTC Loss (avg): 26.5585 WER(1gram): 65.99% (n=64) time: 6.484
2026-01-03 14:29:31,282: WER lens: avg_true_words=6.16 avg_pred_words=5.95 max_pred_words=11
2026-01-03 14:29:31,282: t15.2023.08.13 val PER: 0.2370
2026-01-03 14:29:31,282: t15.2023.08.18 val PER: 0.2154
2026-01-03 14:29:31,282: t15.2023.08.20 val PER: 0.2097
2026-01-03 14:29:31,282: t15.2023.08.25 val PER: 0.1822
2026-01-03 14:29:31,283: t15.2023.08.27 val PER: 0.2669
2026-01-03 14:29:31,283: t15.2023.09.01 val PER: 0.1802
2026-01-03 14:29:31,283: t15.2023.09.03 val PER: 0.2470
2026-01-03 14:29:31,283: t15.2023.09.24 val PER: 0.2075
2026-01-03 14:29:31,283: t15.2023.09.29 val PER: 0.2151
2026-01-03 14:29:31,283: t15.2023.10.01 val PER: 0.2649
2026-01-03 14:29:31,283: t15.2023.10.06 val PER: 0.1851
2026-01-03 14:29:31,283: t15.2023.10.08 val PER: 0.3424
2026-01-03 14:29:31,283: t15.2023.10.13 val PER: 0.3119
2026-01-03 14:29:31,283: t15.2023.10.15 val PER: 0.2452
2026-01-03 14:29:31,283: t15.2023.10.20 val PER: 0.2517
2026-01-03 14:29:31,283: t15.2023.10.22 val PER: 0.2038
2026-01-03 14:29:31,283: t15.2023.11.03 val PER: 0.2605
2026-01-03 14:29:31,284: t15.2023.11.04 val PER: 0.0683
2026-01-03 14:29:31,284: t15.2023.11.17 val PER: 0.1260
2026-01-03 14:29:31,284: t15.2023.11.19 val PER: 0.1098
2026-01-03 14:29:31,284: t15.2023.11.26 val PER: 0.2783
2026-01-03 14:29:31,284: t15.2023.12.03 val PER: 0.2353
2026-01-03 14:29:31,284: t15.2023.12.08 val PER: 0.2517
2026-01-03 14:29:31,284: t15.2023.12.10 val PER: 0.1997
2026-01-03 14:29:31,284: t15.2023.12.17 val PER: 0.2578
2026-01-03 14:29:31,284: t15.2023.12.29 val PER: 0.2560
2026-01-03 14:29:31,284: t15.2024.02.25 val PER: 0.2051
2026-01-03 14:29:31,284: t15.2024.03.08 val PER: 0.3371
2026-01-03 14:29:31,284: t15.2024.03.15 val PER: 0.3221
2026-01-03 14:29:31,284: t15.2024.03.17 val PER: 0.2741
2026-01-03 14:29:31,284: t15.2024.05.10 val PER: 0.2719
2026-01-03 14:29:31,284: t15.2024.06.14 val PER: 0.2871
2026-01-03 14:29:31,284: t15.2024.07.19 val PER: 0.4021
2026-01-03 14:29:31,285: t15.2024.07.21 val PER: 0.2200
2026-01-03 14:29:31,285: t15.2024.07.28 val PER: 0.2816
2026-01-03 14:29:31,285: t15.2025.01.10 val PER: 0.4421
2026-01-03 14:29:31,285: t15.2025.01.12 val PER: 0.2956
2026-01-03 14:29:31,285: t15.2025.03.14 val PER: 0.4393
2026-01-03 14:29:31,285: t15.2025.03.16 val PER: 0.3181
2026-01-03 14:29:31,285: t15.2025.03.30 val PER: 0.4345
2026-01-03 14:29:31,285: t15.2025.04.13 val PER: 0.3267
2026-01-03 14:29:31,287: New best val WER(1gram) 66.24% --> 65.99%
2026-01-03 14:29:31,287: Checkpointing model
2026-01-03 14:29:31,550: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/best_checkpoint
2026-01-03 14:29:31,805: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_3500
2026-01-03 14:29:40,589: Train batch 3600: loss: 22.68 grad norm: 62.55 time: 0.066
2026-01-03 14:29:58,508: Train batch 3800: loss: 26.00 grad norm: 69.73 time: 0.065
2026-01-03 14:30:15,968: Train batch 4000: loss: 19.82 grad norm: 55.31 time: 0.056
2026-01-03 14:30:15,969: Running test after training batch: 4000
2026-01-03 14:30:16,129: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:30:21,478: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-03 14:30:21,505: WER debug example
  GT : how does it keep the cost down
  PR : aue dust it kipp the cost nett
2026-01-03 14:30:23,100: Val batch 4000: PER (avg): 0.2505 CTC Loss (avg): 24.4183 WER(1gram): 64.97% (n=64) time: 7.131
2026-01-03 14:30:23,100: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=11
2026-01-03 14:30:23,100: t15.2023.08.13 val PER: 0.2235
2026-01-03 14:30:23,100: t15.2023.08.18 val PER: 0.2012
2026-01-03 14:30:23,101: t15.2023.08.20 val PER: 0.2033
2026-01-03 14:30:23,101: t15.2023.08.25 val PER: 0.1732
2026-01-03 14:30:23,101: t15.2023.08.27 val PER: 0.2926
2026-01-03 14:30:23,101: t15.2023.09.01 val PER: 0.1591
2026-01-03 14:30:23,101: t15.2023.09.03 val PER: 0.2625
2026-01-03 14:30:23,101: t15.2023.09.24 val PER: 0.1954
2026-01-03 14:30:23,101: t15.2023.09.29 val PER: 0.2061
2026-01-03 14:30:23,101: t15.2023.10.01 val PER: 0.2642
2026-01-03 14:30:23,101: t15.2023.10.06 val PER: 0.1722
2026-01-03 14:30:23,101: t15.2023.10.08 val PER: 0.3221
2026-01-03 14:30:23,101: t15.2023.10.13 val PER: 0.3018
2026-01-03 14:30:23,101: t15.2023.10.15 val PER: 0.2399
2026-01-03 14:30:23,101: t15.2023.10.20 val PER: 0.2349
2026-01-03 14:30:23,101: t15.2023.10.22 val PER: 0.1971
2026-01-03 14:30:23,102: t15.2023.11.03 val PER: 0.2402
2026-01-03 14:30:23,102: t15.2023.11.04 val PER: 0.0580
2026-01-03 14:30:23,102: t15.2023.11.17 val PER: 0.1042
2026-01-03 14:30:23,102: t15.2023.11.19 val PER: 0.1058
2026-01-03 14:30:23,102: t15.2023.11.26 val PER: 0.2703
2026-01-03 14:30:23,102: t15.2023.12.03 val PER: 0.2227
2026-01-03 14:30:23,102: t15.2023.12.08 val PER: 0.2250
2026-01-03 14:30:23,102: t15.2023.12.10 val PER: 0.1853
2026-01-03 14:30:23,102: t15.2023.12.17 val PER: 0.2484
2026-01-03 14:30:23,102: t15.2023.12.29 val PER: 0.2567
2026-01-03 14:30:23,102: t15.2024.02.25 val PER: 0.2135
2026-01-03 14:30:23,102: t15.2024.03.08 val PER: 0.3357
2026-01-03 14:30:23,102: t15.2024.03.15 val PER: 0.3008
2026-01-03 14:30:23,102: t15.2024.03.17 val PER: 0.2517
2026-01-03 14:30:23,103: t15.2024.05.10 val PER: 0.2704
2026-01-03 14:30:23,103: t15.2024.06.14 val PER: 0.2650
2026-01-03 14:30:23,103: t15.2024.07.19 val PER: 0.3606
2026-01-03 14:30:23,103: t15.2024.07.21 val PER: 0.1821
2026-01-03 14:30:23,103: t15.2024.07.28 val PER: 0.2478
2026-01-03 14:30:23,103: t15.2025.01.10 val PER: 0.4229
2026-01-03 14:30:23,103: t15.2025.01.12 val PER: 0.2702
2026-01-03 14:30:23,103: t15.2025.03.14 val PER: 0.4038
2026-01-03 14:30:23,103: t15.2025.03.16 val PER: 0.3089
2026-01-03 14:30:23,103: t15.2025.03.30 val PER: 0.4230
2026-01-03 14:30:23,103: t15.2025.04.13 val PER: 0.3267
2026-01-03 14:30:23,104: New best val WER(1gram) 65.99% --> 64.97%
2026-01-03 14:30:23,105: Checkpointing model
2026-01-03 14:30:23,369: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/best_checkpoint
2026-01-03 14:30:23,623: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_4000
2026-01-03 14:30:40,667: Train batch 4200: loss: 22.88 grad norm: 65.56 time: 0.078
2026-01-03 14:30:57,820: Train batch 4400: loss: 17.30 grad norm: 53.97 time: 0.066
2026-01-03 14:31:06,368: Running test after training batch: 4500
2026-01-03 14:31:06,523: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:31:11,455: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-03 14:31:11,485: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap thus cost get
2026-01-03 14:31:13,070: Val batch 4500: PER (avg): 0.2397 CTC Loss (avg): 23.3778 WER(1gram): 61.17% (n=64) time: 6.702
2026-01-03 14:31:13,071: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 14:31:13,071: t15.2023.08.13 val PER: 0.2173
2026-01-03 14:31:13,071: t15.2023.08.18 val PER: 0.1878
2026-01-03 14:31:13,071: t15.2023.08.20 val PER: 0.1898
2026-01-03 14:31:13,071: t15.2023.08.25 val PER: 0.1325
2026-01-03 14:31:13,071: t15.2023.08.27 val PER: 0.2492
2026-01-03 14:31:13,071: t15.2023.09.01 val PER: 0.1567
2026-01-03 14:31:13,071: t15.2023.09.03 val PER: 0.2411
2026-01-03 14:31:13,071: t15.2023.09.24 val PER: 0.1833
2026-01-03 14:31:13,071: t15.2023.09.29 val PER: 0.2004
2026-01-03 14:31:13,071: t15.2023.10.01 val PER: 0.2649
2026-01-03 14:31:13,071: t15.2023.10.06 val PER: 0.1453
2026-01-03 14:31:13,071: t15.2023.10.08 val PER: 0.3248
2026-01-03 14:31:13,072: t15.2023.10.13 val PER: 0.3026
2026-01-03 14:31:13,072: t15.2023.10.15 val PER: 0.2294
2026-01-03 14:31:13,072: t15.2023.10.20 val PER: 0.2282
2026-01-03 14:31:13,072: t15.2023.10.22 val PER: 0.1882
2026-01-03 14:31:13,072: t15.2023.11.03 val PER: 0.2408
2026-01-03 14:31:13,072: t15.2023.11.04 val PER: 0.0546
2026-01-03 14:31:13,072: t15.2023.11.17 val PER: 0.1042
2026-01-03 14:31:13,072: t15.2023.11.19 val PER: 0.0958
2026-01-03 14:31:13,072: t15.2023.11.26 val PER: 0.2688
2026-01-03 14:31:13,072: t15.2023.12.03 val PER: 0.2153
2026-01-03 14:31:13,072: t15.2023.12.08 val PER: 0.2117
2026-01-03 14:31:13,072: t15.2023.12.10 val PER: 0.1774
2026-01-03 14:31:13,072: t15.2023.12.17 val PER: 0.2287
2026-01-03 14:31:13,072: t15.2023.12.29 val PER: 0.2402
2026-01-03 14:31:13,072: t15.2024.02.25 val PER: 0.2065
2026-01-03 14:31:13,073: t15.2024.03.08 val PER: 0.3257
2026-01-03 14:31:13,073: t15.2024.03.15 val PER: 0.2996
2026-01-03 14:31:13,073: t15.2024.03.17 val PER: 0.2497
2026-01-03 14:31:13,073: t15.2024.05.10 val PER: 0.2467
2026-01-03 14:31:13,073: t15.2024.06.14 val PER: 0.2492
2026-01-03 14:31:13,073: t15.2024.07.19 val PER: 0.3415
2026-01-03 14:31:13,073: t15.2024.07.21 val PER: 0.1738
2026-01-03 14:31:13,073: t15.2024.07.28 val PER: 0.2257
2026-01-03 14:31:13,073: t15.2025.01.10 val PER: 0.4187
2026-01-03 14:31:13,073: t15.2025.01.12 val PER: 0.2687
2026-01-03 14:31:13,073: t15.2025.03.14 val PER: 0.3994
2026-01-03 14:31:13,073: t15.2025.03.16 val PER: 0.2971
2026-01-03 14:31:13,073: t15.2025.03.30 val PER: 0.4011
2026-01-03 14:31:13,073: t15.2025.04.13 val PER: 0.2810
2026-01-03 14:31:13,075: New best val WER(1gram) 64.97% --> 61.17%
2026-01-03 14:31:13,075: Checkpointing model
2026-01-03 14:31:13,352: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/best_checkpoint
2026-01-03 14:31:13,608: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_4500
2026-01-03 14:31:22,388: Train batch 4600: loss: 20.51 grad norm: 67.58 time: 0.062
2026-01-03 14:31:40,114: Train batch 4800: loss: 13.71 grad norm: 55.59 time: 0.063
2026-01-03 14:31:57,413: Train batch 5000: loss: 32.73 grad norm: 82.16 time: 0.063
2026-01-03 14:31:57,414: Running test after training batch: 5000
2026-01-03 14:31:57,540: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:32:02,248: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-03 14:32:02,278: WER debug example
  GT : how does it keep the cost down
  PR : houde just it heap the cost et
2026-01-03 14:32:03,899: Val batch 5000: PER (avg): 0.2268 CTC Loss (avg): 22.3722 WER(1gram): 59.14% (n=64) time: 6.485
2026-01-03 14:32:03,900: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-03 14:32:03,900: t15.2023.08.13 val PER: 0.2027
2026-01-03 14:32:03,900: t15.2023.08.18 val PER: 0.1802
2026-01-03 14:32:03,900: t15.2023.08.20 val PER: 0.1716
2026-01-03 14:32:03,900: t15.2023.08.25 val PER: 0.1401
2026-01-03 14:32:03,900: t15.2023.08.27 val PER: 0.2428
2026-01-03 14:32:03,900: t15.2023.09.01 val PER: 0.1372
2026-01-03 14:32:03,900: t15.2023.09.03 val PER: 0.2280
2026-01-03 14:32:03,900: t15.2023.09.24 val PER: 0.1820
2026-01-03 14:32:03,900: t15.2023.09.29 val PER: 0.1774
2026-01-03 14:32:03,900: t15.2023.10.01 val PER: 0.2292
2026-01-03 14:32:03,901: t15.2023.10.06 val PER: 0.1421
2026-01-03 14:32:03,901: t15.2023.10.08 val PER: 0.3166
2026-01-03 14:32:03,901: t15.2023.10.13 val PER: 0.2824
2026-01-03 14:32:03,901: t15.2023.10.15 val PER: 0.2241
2026-01-03 14:32:03,901: t15.2023.10.20 val PER: 0.2450
2026-01-03 14:32:03,901: t15.2023.10.22 val PER: 0.1659
2026-01-03 14:32:03,901: t15.2023.11.03 val PER: 0.2259
2026-01-03 14:32:03,901: t15.2023.11.04 val PER: 0.0512
2026-01-03 14:32:03,901: t15.2023.11.17 val PER: 0.0762
2026-01-03 14:32:03,901: t15.2023.11.19 val PER: 0.0758
2026-01-03 14:32:03,901: t15.2023.11.26 val PER: 0.2384
2026-01-03 14:32:03,901: t15.2023.12.03 val PER: 0.2038
2026-01-03 14:32:03,901: t15.2023.12.08 val PER: 0.2011
2026-01-03 14:32:03,902: t15.2023.12.10 val PER: 0.1643
2026-01-03 14:32:03,902: t15.2023.12.17 val PER: 0.2339
2026-01-03 14:32:03,902: t15.2023.12.29 val PER: 0.2347
2026-01-03 14:32:03,902: t15.2024.02.25 val PER: 0.1980
2026-01-03 14:32:03,902: t15.2024.03.08 val PER: 0.3158
2026-01-03 14:32:03,902: t15.2024.03.15 val PER: 0.2777
2026-01-03 14:32:03,902: t15.2024.03.17 val PER: 0.2308
2026-01-03 14:32:03,902: t15.2024.05.10 val PER: 0.2407
2026-01-03 14:32:03,902: t15.2024.06.14 val PER: 0.2555
2026-01-03 14:32:03,902: t15.2024.07.19 val PER: 0.3270
2026-01-03 14:32:03,902: t15.2024.07.21 val PER: 0.1745
2026-01-03 14:32:03,902: t15.2024.07.28 val PER: 0.2103
2026-01-03 14:32:03,902: t15.2025.01.10 val PER: 0.3912
2026-01-03 14:32:03,902: t15.2025.01.12 val PER: 0.2487
2026-01-03 14:32:03,903: t15.2025.03.14 val PER: 0.3979
2026-01-03 14:32:03,903: t15.2025.03.16 val PER: 0.2618
2026-01-03 14:32:03,903: t15.2025.03.30 val PER: 0.4057
2026-01-03 14:32:03,903: t15.2025.04.13 val PER: 0.3010
2026-01-03 14:32:03,903: New best val WER(1gram) 61.17% --> 59.14%
2026-01-03 14:32:03,903: Checkpointing model
2026-01-03 14:32:04,182: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/best_checkpoint
2026-01-03 14:32:04,438: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_5000
2026-01-03 14:32:21,957: Train batch 5200: loss: 16.79 grad norm: 63.77 time: 0.051
2026-01-03 14:32:38,963: Train batch 5400: loss: 18.04 grad norm: 59.66 time: 0.068
2026-01-03 14:32:47,430: Running test after training batch: 5500
2026-01-03 14:32:47,527: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:32:52,365: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point will
2026-01-03 14:32:52,393: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-03 14:32:53,977: Val batch 5500: PER (avg): 0.2161 CTC Loss (avg): 21.1297 WER(1gram): 56.60% (n=64) time: 6.547
2026-01-03 14:32:53,977: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 14:32:53,977: t15.2023.08.13 val PER: 0.1830
2026-01-03 14:32:53,978: t15.2023.08.18 val PER: 0.1551
2026-01-03 14:32:53,978: t15.2023.08.20 val PER: 0.1716
2026-01-03 14:32:53,978: t15.2023.08.25 val PER: 0.1355
2026-01-03 14:32:53,978: t15.2023.08.27 val PER: 0.2540
2026-01-03 14:32:53,978: t15.2023.09.01 val PER: 0.1266
2026-01-03 14:32:53,978: t15.2023.09.03 val PER: 0.2173
2026-01-03 14:32:53,978: t15.2023.09.24 val PER: 0.1675
2026-01-03 14:32:53,978: t15.2023.09.29 val PER: 0.1800
2026-01-03 14:32:53,978: t15.2023.10.01 val PER: 0.2332
2026-01-03 14:32:53,978: t15.2023.10.06 val PER: 0.1389
2026-01-03 14:32:53,978: t15.2023.10.08 val PER: 0.3018
2026-01-03 14:32:53,978: t15.2023.10.13 val PER: 0.2824
2026-01-03 14:32:53,979: t15.2023.10.15 val PER: 0.2182
2026-01-03 14:32:53,979: t15.2023.10.20 val PER: 0.2315
2026-01-03 14:32:53,979: t15.2023.10.22 val PER: 0.1637
2026-01-03 14:32:53,979: t15.2023.11.03 val PER: 0.2293
2026-01-03 14:32:53,979: t15.2023.11.04 val PER: 0.0478
2026-01-03 14:32:53,979: t15.2023.11.17 val PER: 0.0715
2026-01-03 14:32:53,979: t15.2023.11.19 val PER: 0.0818
2026-01-03 14:32:53,979: t15.2023.11.26 val PER: 0.2203
2026-01-03 14:32:53,979: t15.2023.12.03 val PER: 0.1828
2026-01-03 14:32:53,979: t15.2023.12.08 val PER: 0.1871
2026-01-03 14:32:53,979: t15.2023.12.10 val PER: 0.1511
2026-01-03 14:32:53,979: t15.2023.12.17 val PER: 0.2037
2026-01-03 14:32:53,979: t15.2023.12.29 val PER: 0.2114
2026-01-03 14:32:53,980: t15.2024.02.25 val PER: 0.1812
2026-01-03 14:32:53,980: t15.2024.03.08 val PER: 0.2859
2026-01-03 14:32:53,980: t15.2024.03.15 val PER: 0.2520
2026-01-03 14:32:53,980: t15.2024.03.17 val PER: 0.2218
2026-01-03 14:32:53,980: t15.2024.05.10 val PER: 0.2229
2026-01-03 14:32:53,980: t15.2024.06.14 val PER: 0.2303
2026-01-03 14:32:53,980: t15.2024.07.19 val PER: 0.3243
2026-01-03 14:32:53,980: t15.2024.07.21 val PER: 0.1538
2026-01-03 14:32:53,980: t15.2024.07.28 val PER: 0.2103
2026-01-03 14:32:53,980: t15.2025.01.10 val PER: 0.3884
2026-01-03 14:32:53,980: t15.2025.01.12 val PER: 0.2402
2026-01-03 14:32:53,980: t15.2025.03.14 val PER: 0.3757
2026-01-03 14:32:53,981: t15.2025.03.16 val PER: 0.2709
2026-01-03 14:32:53,981: t15.2025.03.30 val PER: 0.3690
2026-01-03 14:32:53,981: t15.2025.04.13 val PER: 0.2896
2026-01-03 14:32:53,981: New best val WER(1gram) 59.14% --> 56.60%
2026-01-03 14:32:53,982: Checkpointing model
2026-01-03 14:32:54,244: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/best_checkpoint
2026-01-03 14:32:54,495: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_5500
2026-01-03 14:33:03,297: Train batch 5600: loss: 19.50 grad norm: 65.25 time: 0.062
2026-01-03 14:33:21,480: Train batch 5800: loss: 13.72 grad norm: 58.09 time: 0.082
2026-01-03 14:33:39,350: Train batch 6000: loss: 14.26 grad norm: 55.77 time: 0.049
2026-01-03 14:33:39,351: Running test after training batch: 6000
2026-01-03 14:33:39,589: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:33:44,313: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-03 14:33:44,343: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-03 14:33:45,971: Val batch 6000: PER (avg): 0.2121 CTC Loss (avg): 20.9209 WER(1gram): 58.88% (n=64) time: 6.620
2026-01-03 14:33:45,971: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 14:33:45,971: t15.2023.08.13 val PER: 0.1819
2026-01-03 14:33:45,971: t15.2023.08.18 val PER: 0.1710
2026-01-03 14:33:45,972: t15.2023.08.20 val PER: 0.1668
2026-01-03 14:33:45,972: t15.2023.08.25 val PER: 0.1190
2026-01-03 14:33:45,972: t15.2023.08.27 val PER: 0.2492
2026-01-03 14:33:45,972: t15.2023.09.01 val PER: 0.1380
2026-01-03 14:33:45,972: t15.2023.09.03 val PER: 0.2102
2026-01-03 14:33:45,972: t15.2023.09.24 val PER: 0.1590
2026-01-03 14:33:45,972: t15.2023.09.29 val PER: 0.1672
2026-01-03 14:33:45,972: t15.2023.10.01 val PER: 0.2226
2026-01-03 14:33:45,972: t15.2023.10.06 val PER: 0.1292
2026-01-03 14:33:45,973: t15.2023.10.08 val PER: 0.2977
2026-01-03 14:33:45,973: t15.2023.10.13 val PER: 0.2770
2026-01-03 14:33:45,973: t15.2023.10.15 val PER: 0.2123
2026-01-03 14:33:45,973: t15.2023.10.20 val PER: 0.2215
2026-01-03 14:33:45,973: t15.2023.10.22 val PER: 0.1581
2026-01-03 14:33:45,973: t15.2023.11.03 val PER: 0.2212
2026-01-03 14:33:45,973: t15.2023.11.04 val PER: 0.0614
2026-01-03 14:33:45,973: t15.2023.11.17 val PER: 0.0824
2026-01-03 14:33:45,973: t15.2023.11.19 val PER: 0.0758
2026-01-03 14:33:45,973: t15.2023.11.26 val PER: 0.2261
2026-01-03 14:33:45,973: t15.2023.12.03 val PER: 0.1691
2026-01-03 14:33:45,973: t15.2023.12.08 val PER: 0.1711
2026-01-03 14:33:45,973: t15.2023.12.10 val PER: 0.1511
2026-01-03 14:33:45,974: t15.2023.12.17 val PER: 0.2058
2026-01-03 14:33:45,974: t15.2023.12.29 val PER: 0.2169
2026-01-03 14:33:45,974: t15.2024.02.25 val PER: 0.1545
2026-01-03 14:33:45,974: t15.2024.03.08 val PER: 0.3073
2026-01-03 14:33:45,974: t15.2024.03.15 val PER: 0.2658
2026-01-03 14:33:45,974: t15.2024.03.17 val PER: 0.2134
2026-01-03 14:33:45,974: t15.2024.05.10 val PER: 0.2184
2026-01-03 14:33:45,974: t15.2024.06.14 val PER: 0.2208
2026-01-03 14:33:45,974: t15.2024.07.19 val PER: 0.3059
2026-01-03 14:33:45,974: t15.2024.07.21 val PER: 0.1662
2026-01-03 14:33:45,974: t15.2024.07.28 val PER: 0.2000
2026-01-03 14:33:45,974: t15.2025.01.10 val PER: 0.3843
2026-01-03 14:33:45,974: t15.2025.01.12 val PER: 0.2248
2026-01-03 14:33:45,974: t15.2025.03.14 val PER: 0.3669
2026-01-03 14:33:45,974: t15.2025.03.16 val PER: 0.2657
2026-01-03 14:33:45,975: t15.2025.03.30 val PER: 0.3862
2026-01-03 14:33:45,975: t15.2025.04.13 val PER: 0.2568
2026-01-03 14:33:46,217: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_6000
2026-01-03 14:34:03,378: Train batch 6200: loss: 16.76 grad norm: 58.50 time: 0.070
2026-01-03 14:34:20,748: Train batch 6400: loss: 18.54 grad norm: 64.15 time: 0.062
2026-01-03 14:34:29,187: Running test after training batch: 6500
2026-01-03 14:34:29,320: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:34:34,048: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 14:34:34,077: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-03 14:34:35,675: Val batch 6500: PER (avg): 0.2048 CTC Loss (avg): 20.1860 WER(1gram): 53.05% (n=64) time: 6.487
2026-01-03 14:34:35,675: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 14:34:35,675: t15.2023.08.13 val PER: 0.1684
2026-01-03 14:34:35,675: t15.2023.08.18 val PER: 0.1492
2026-01-03 14:34:35,676: t15.2023.08.20 val PER: 0.1573
2026-01-03 14:34:35,676: t15.2023.08.25 val PER: 0.1205
2026-01-03 14:34:35,676: t15.2023.08.27 val PER: 0.2428
2026-01-03 14:34:35,676: t15.2023.09.01 val PER: 0.1218
2026-01-03 14:34:35,676: t15.2023.09.03 val PER: 0.2114
2026-01-03 14:34:35,676: t15.2023.09.24 val PER: 0.1687
2026-01-03 14:34:35,676: t15.2023.09.29 val PER: 0.1653
2026-01-03 14:34:35,676: t15.2023.10.01 val PER: 0.2232
2026-01-03 14:34:35,676: t15.2023.10.06 val PER: 0.1206
2026-01-03 14:34:35,676: t15.2023.10.08 val PER: 0.2950
2026-01-03 14:34:35,676: t15.2023.10.13 val PER: 0.2638
2026-01-03 14:34:35,676: t15.2023.10.15 val PER: 0.2011
2026-01-03 14:34:35,676: t15.2023.10.20 val PER: 0.2114
2026-01-03 14:34:35,676: t15.2023.10.22 val PER: 0.1626
2026-01-03 14:34:35,676: t15.2023.11.03 val PER: 0.2144
2026-01-03 14:34:35,676: t15.2023.11.04 val PER: 0.0580
2026-01-03 14:34:35,676: t15.2023.11.17 val PER: 0.0560
2026-01-03 14:34:35,677: t15.2023.11.19 val PER: 0.0778
2026-01-03 14:34:35,677: t15.2023.11.26 val PER: 0.2130
2026-01-03 14:34:35,677: t15.2023.12.03 val PER: 0.1744
2026-01-03 14:34:35,677: t15.2023.12.08 val PER: 0.1684
2026-01-03 14:34:35,677: t15.2023.12.10 val PER: 0.1445
2026-01-03 14:34:35,677: t15.2023.12.17 val PER: 0.1954
2026-01-03 14:34:35,677: t15.2023.12.29 val PER: 0.1956
2026-01-03 14:34:35,677: t15.2024.02.25 val PER: 0.1713
2026-01-03 14:34:35,677: t15.2024.03.08 val PER: 0.2831
2026-01-03 14:34:35,678: t15.2024.03.15 val PER: 0.2545
2026-01-03 14:34:35,678: t15.2024.03.17 val PER: 0.1953
2026-01-03 14:34:35,678: t15.2024.05.10 val PER: 0.2244
2026-01-03 14:34:35,678: t15.2024.06.14 val PER: 0.2098
2026-01-03 14:34:35,678: t15.2024.07.19 val PER: 0.3078
2026-01-03 14:34:35,678: t15.2024.07.21 val PER: 0.1559
2026-01-03 14:34:35,678: t15.2024.07.28 val PER: 0.1890
2026-01-03 14:34:35,678: t15.2025.01.10 val PER: 0.3884
2026-01-03 14:34:35,678: t15.2025.01.12 val PER: 0.2148
2026-01-03 14:34:35,678: t15.2025.03.14 val PER: 0.3757
2026-01-03 14:34:35,678: t15.2025.03.16 val PER: 0.2461
2026-01-03 14:34:35,678: t15.2025.03.30 val PER: 0.3667
2026-01-03 14:34:35,678: t15.2025.04.13 val PER: 0.2725
2026-01-03 14:34:35,679: New best val WER(1gram) 56.60% --> 53.05%
2026-01-03 14:34:35,679: Checkpointing model
2026-01-03 14:34:35,958: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/best_checkpoint
2026-01-03 14:34:36,212: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_6500
2026-01-03 14:34:44,660: Train batch 6600: loss: 12.61 grad norm: 52.67 time: 0.045
2026-01-03 14:35:01,979: Train batch 6800: loss: 15.20 grad norm: 54.84 time: 0.048
2026-01-03 14:35:19,542: Train batch 7000: loss: 17.73 grad norm: 63.78 time: 0.060
2026-01-03 14:35:19,543: Running test after training batch: 7000
2026-01-03 14:35:19,692: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:35:24,675: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 14:35:24,705: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-03 14:35:26,365: Val batch 7000: PER (avg): 0.1963 CTC Loss (avg): 19.2527 WER(1gram): 55.08% (n=64) time: 6.822
2026-01-03 14:35:26,366: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 14:35:26,366: t15.2023.08.13 val PER: 0.1518
2026-01-03 14:35:26,366: t15.2023.08.18 val PER: 0.1433
2026-01-03 14:35:26,366: t15.2023.08.20 val PER: 0.1509
2026-01-03 14:35:26,366: t15.2023.08.25 val PER: 0.1039
2026-01-03 14:35:26,366: t15.2023.08.27 val PER: 0.2186
2026-01-03 14:35:26,366: t15.2023.09.01 val PER: 0.1185
2026-01-03 14:35:26,366: t15.2023.09.03 val PER: 0.1971
2026-01-03 14:35:26,366: t15.2023.09.24 val PER: 0.1602
2026-01-03 14:35:26,366: t15.2023.09.29 val PER: 0.1627
2026-01-03 14:35:26,366: t15.2023.10.01 val PER: 0.2120
2026-01-03 14:35:26,366: t15.2023.10.06 val PER: 0.1119
2026-01-03 14:35:26,367: t15.2023.10.08 val PER: 0.2842
2026-01-03 14:35:26,367: t15.2023.10.13 val PER: 0.2552
2026-01-03 14:35:26,367: t15.2023.10.15 val PER: 0.1958
2026-01-03 14:35:26,367: t15.2023.10.20 val PER: 0.1946
2026-01-03 14:35:26,367: t15.2023.10.22 val PER: 0.1392
2026-01-03 14:35:26,367: t15.2023.11.03 val PER: 0.1995
2026-01-03 14:35:26,367: t15.2023.11.04 val PER: 0.0444
2026-01-03 14:35:26,367: t15.2023.11.17 val PER: 0.0715
2026-01-03 14:35:26,367: t15.2023.11.19 val PER: 0.0699
2026-01-03 14:35:26,367: t15.2023.11.26 val PER: 0.1891
2026-01-03 14:35:26,367: t15.2023.12.03 val PER: 0.1670
2026-01-03 14:35:26,367: t15.2023.12.08 val PER: 0.1644
2026-01-03 14:35:26,367: t15.2023.12.10 val PER: 0.1393
2026-01-03 14:35:26,367: t15.2023.12.17 val PER: 0.1881
2026-01-03 14:35:26,368: t15.2023.12.29 val PER: 0.1997
2026-01-03 14:35:26,368: t15.2024.02.25 val PER: 0.1573
2026-01-03 14:35:26,368: t15.2024.03.08 val PER: 0.2774
2026-01-03 14:35:26,368: t15.2024.03.15 val PER: 0.2470
2026-01-03 14:35:26,368: t15.2024.03.17 val PER: 0.1967
2026-01-03 14:35:26,368: t15.2024.05.10 val PER: 0.2110
2026-01-03 14:35:26,368: t15.2024.06.14 val PER: 0.2066
2026-01-03 14:35:26,368: t15.2024.07.19 val PER: 0.3052
2026-01-03 14:35:26,368: t15.2024.07.21 val PER: 0.1366
2026-01-03 14:35:26,369: t15.2024.07.28 val PER: 0.1743
2026-01-03 14:35:26,369: t15.2025.01.10 val PER: 0.3691
2026-01-03 14:35:26,369: t15.2025.01.12 val PER: 0.2109
2026-01-03 14:35:26,369: t15.2025.03.14 val PER: 0.3683
2026-01-03 14:35:26,369: t15.2025.03.16 val PER: 0.2395
2026-01-03 14:35:26,369: t15.2025.03.30 val PER: 0.3621
2026-01-03 14:35:26,369: t15.2025.04.13 val PER: 0.2739
2026-01-03 14:35:26,610: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_7000
2026-01-03 14:35:44,981: Train batch 7200: loss: 14.51 grad norm: 56.83 time: 0.078
2026-01-03 14:36:02,285: Train batch 7400: loss: 13.85 grad norm: 54.48 time: 0.075
2026-01-03 14:36:10,865: Running test after training batch: 7500
2026-01-03 14:36:11,051: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:36:15,741: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 14:36:15,772: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost nett
2026-01-03 14:36:17,411: Val batch 7500: PER (avg): 0.1903 CTC Loss (avg): 18.9240 WER(1gram): 55.84% (n=64) time: 6.546
2026-01-03 14:36:17,411: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 14:36:17,411: t15.2023.08.13 val PER: 0.1507
2026-01-03 14:36:17,411: t15.2023.08.18 val PER: 0.1366
2026-01-03 14:36:17,411: t15.2023.08.20 val PER: 0.1485
2026-01-03 14:36:17,411: t15.2023.08.25 val PER: 0.1160
2026-01-03 14:36:17,411: t15.2023.08.27 val PER: 0.2090
2026-01-03 14:36:17,411: t15.2023.09.01 val PER: 0.1136
2026-01-03 14:36:17,412: t15.2023.09.03 val PER: 0.1912
2026-01-03 14:36:17,412: t15.2023.09.24 val PER: 0.1481
2026-01-03 14:36:17,412: t15.2023.09.29 val PER: 0.1583
2026-01-03 14:36:17,412: t15.2023.10.01 val PER: 0.1942
2026-01-03 14:36:17,412: t15.2023.10.06 val PER: 0.1216
2026-01-03 14:36:17,412: t15.2023.10.08 val PER: 0.2679
2026-01-03 14:36:17,412: t15.2023.10.13 val PER: 0.2490
2026-01-03 14:36:17,412: t15.2023.10.15 val PER: 0.1918
2026-01-03 14:36:17,412: t15.2023.10.20 val PER: 0.2013
2026-01-03 14:36:17,412: t15.2023.10.22 val PER: 0.1459
2026-01-03 14:36:17,412: t15.2023.11.03 val PER: 0.2015
2026-01-03 14:36:17,412: t15.2023.11.04 val PER: 0.0375
2026-01-03 14:36:17,412: t15.2023.11.17 val PER: 0.0684
2026-01-03 14:36:17,412: t15.2023.11.19 val PER: 0.0499
2026-01-03 14:36:17,412: t15.2023.11.26 val PER: 0.1848
2026-01-03 14:36:17,413: t15.2023.12.03 val PER: 0.1523
2026-01-03 14:36:17,413: t15.2023.12.08 val PER: 0.1631
2026-01-03 14:36:17,413: t15.2023.12.10 val PER: 0.1353
2026-01-03 14:36:17,413: t15.2023.12.17 val PER: 0.1798
2026-01-03 14:36:17,413: t15.2023.12.29 val PER: 0.1874
2026-01-03 14:36:17,413: t15.2024.02.25 val PER: 0.1447
2026-01-03 14:36:17,413: t15.2024.03.08 val PER: 0.2731
2026-01-03 14:36:17,413: t15.2024.03.15 val PER: 0.2427
2026-01-03 14:36:17,413: t15.2024.03.17 val PER: 0.1862
2026-01-03 14:36:17,413: t15.2024.05.10 val PER: 0.2169
2026-01-03 14:36:17,413: t15.2024.06.14 val PER: 0.2050
2026-01-03 14:36:17,413: t15.2024.07.19 val PER: 0.2861
2026-01-03 14:36:17,413: t15.2024.07.21 val PER: 0.1393
2026-01-03 14:36:17,413: t15.2024.07.28 val PER: 0.1838
2026-01-03 14:36:17,413: t15.2025.01.10 val PER: 0.3595
2026-01-03 14:36:17,413: t15.2025.01.12 val PER: 0.1878
2026-01-03 14:36:17,413: t15.2025.03.14 val PER: 0.3683
2026-01-03 14:36:17,414: t15.2025.03.16 val PER: 0.2421
2026-01-03 14:36:17,414: t15.2025.03.30 val PER: 0.3517
2026-01-03 14:36:17,414: t15.2025.04.13 val PER: 0.2411
2026-01-03 14:36:17,658: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_7500
2026-01-03 14:36:26,447: Train batch 7600: loss: 15.99 grad norm: 57.19 time: 0.068
2026-01-03 14:36:43,576: Train batch 7800: loss: 14.00 grad norm: 57.00 time: 0.055
2026-01-03 14:37:00,823: Train batch 8000: loss: 11.25 grad norm: 48.63 time: 0.071
2026-01-03 14:37:00,824: Running test after training batch: 8000
2026-01-03 14:37:00,919: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:37:05,627: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 14:37:05,658: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-03 14:37:07,331: Val batch 8000: PER (avg): 0.1868 CTC Loss (avg): 18.2384 WER(1gram): 54.82% (n=64) time: 6.507
2026-01-03 14:37:07,331: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-03 14:37:07,331: t15.2023.08.13 val PER: 0.1528
2026-01-03 14:37:07,331: t15.2023.08.18 val PER: 0.1224
2026-01-03 14:37:07,331: t15.2023.08.20 val PER: 0.1390
2026-01-03 14:37:07,332: t15.2023.08.25 val PER: 0.1160
2026-01-03 14:37:07,332: t15.2023.08.27 val PER: 0.2219
2026-01-03 14:37:07,332: t15.2023.09.01 val PER: 0.1104
2026-01-03 14:37:07,332: t15.2023.09.03 val PER: 0.1960
2026-01-03 14:37:07,332: t15.2023.09.24 val PER: 0.1578
2026-01-03 14:37:07,332: t15.2023.09.29 val PER: 0.1551
2026-01-03 14:37:07,332: t15.2023.10.01 val PER: 0.1975
2026-01-03 14:37:07,332: t15.2023.10.06 val PER: 0.1141
2026-01-03 14:37:07,332: t15.2023.10.08 val PER: 0.2774
2026-01-03 14:37:07,332: t15.2023.10.13 val PER: 0.2382
2026-01-03 14:37:07,332: t15.2023.10.15 val PER: 0.1892
2026-01-03 14:37:07,332: t15.2023.10.20 val PER: 0.2013
2026-01-03 14:37:07,332: t15.2023.10.22 val PER: 0.1492
2026-01-03 14:37:07,332: t15.2023.11.03 val PER: 0.1967
2026-01-03 14:37:07,333: t15.2023.11.04 val PER: 0.0375
2026-01-03 14:37:07,333: t15.2023.11.17 val PER: 0.0513
2026-01-03 14:37:07,333: t15.2023.11.19 val PER: 0.0559
2026-01-03 14:37:07,333: t15.2023.11.26 val PER: 0.1862
2026-01-03 14:37:07,333: t15.2023.12.03 val PER: 0.1481
2026-01-03 14:37:07,333: t15.2023.12.08 val PER: 0.1531
2026-01-03 14:37:07,333: t15.2023.12.10 val PER: 0.1380
2026-01-03 14:37:07,333: t15.2023.12.17 val PER: 0.1778
2026-01-03 14:37:07,333: t15.2023.12.29 val PER: 0.1764
2026-01-03 14:37:07,333: t15.2024.02.25 val PER: 0.1419
2026-01-03 14:37:07,334: t15.2024.03.08 val PER: 0.2745
2026-01-03 14:37:07,334: t15.2024.03.15 val PER: 0.2370
2026-01-03 14:37:07,334: t15.2024.03.17 val PER: 0.1813
2026-01-03 14:37:07,334: t15.2024.05.10 val PER: 0.2021
2026-01-03 14:37:07,334: t15.2024.06.14 val PER: 0.2019
2026-01-03 14:37:07,334: t15.2024.07.19 val PER: 0.2920
2026-01-03 14:37:07,334: t15.2024.07.21 val PER: 0.1241
2026-01-03 14:37:07,334: t15.2024.07.28 val PER: 0.1706
2026-01-03 14:37:07,334: t15.2025.01.10 val PER: 0.3499
2026-01-03 14:37:07,334: t15.2025.01.12 val PER: 0.1894
2026-01-03 14:37:07,334: t15.2025.03.14 val PER: 0.3476
2026-01-03 14:37:07,334: t15.2025.03.16 val PER: 0.2317
2026-01-03 14:37:07,334: t15.2025.03.30 val PER: 0.3460
2026-01-03 14:37:07,334: t15.2025.04.13 val PER: 0.2668
2026-01-03 14:37:07,576: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_8000
2026-01-03 14:37:25,024: Train batch 8200: loss: 9.71 grad norm: 48.79 time: 0.054
2026-01-03 14:37:42,401: Train batch 8400: loss: 10.15 grad norm: 47.80 time: 0.063
2026-01-03 14:37:51,156: Running test after training batch: 8500
2026-01-03 14:37:51,270: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:37:56,194: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 14:37:56,225: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-03 14:37:57,954: Val batch 8500: PER (avg): 0.1804 CTC Loss (avg): 17.7145 WER(1gram): 51.52% (n=64) time: 6.798
2026-01-03 14:37:57,954: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 14:37:57,954: t15.2023.08.13 val PER: 0.1455
2026-01-03 14:37:57,954: t15.2023.08.18 val PER: 0.1174
2026-01-03 14:37:57,954: t15.2023.08.20 val PER: 0.1366
2026-01-03 14:37:57,955: t15.2023.08.25 val PER: 0.1114
2026-01-03 14:37:57,955: t15.2023.08.27 val PER: 0.2203
2026-01-03 14:37:57,955: t15.2023.09.01 val PER: 0.0990
2026-01-03 14:37:57,955: t15.2023.09.03 val PER: 0.1865
2026-01-03 14:37:57,955: t15.2023.09.24 val PER: 0.1529
2026-01-03 14:37:57,955: t15.2023.09.29 val PER: 0.1449
2026-01-03 14:37:57,955: t15.2023.10.01 val PER: 0.2015
2026-01-03 14:37:57,955: t15.2023.10.06 val PER: 0.1033
2026-01-03 14:37:57,955: t15.2023.10.08 val PER: 0.2747
2026-01-03 14:37:57,955: t15.2023.10.13 val PER: 0.2312
2026-01-03 14:37:57,955: t15.2023.10.15 val PER: 0.1866
2026-01-03 14:37:57,955: t15.2023.10.20 val PER: 0.1946
2026-01-03 14:37:57,956: t15.2023.10.22 val PER: 0.1392
2026-01-03 14:37:57,956: t15.2023.11.03 val PER: 0.1961
2026-01-03 14:37:57,956: t15.2023.11.04 val PER: 0.0410
2026-01-03 14:37:57,956: t15.2023.11.17 val PER: 0.0529
2026-01-03 14:37:57,956: t15.2023.11.19 val PER: 0.0499
2026-01-03 14:37:57,956: t15.2023.11.26 val PER: 0.1768
2026-01-03 14:37:57,956: t15.2023.12.03 val PER: 0.1387
2026-01-03 14:37:57,956: t15.2023.12.08 val PER: 0.1405
2026-01-03 14:37:57,956: t15.2023.12.10 val PER: 0.1288
2026-01-03 14:37:57,956: t15.2023.12.17 val PER: 0.1611
2026-01-03 14:37:57,956: t15.2023.12.29 val PER: 0.1702
2026-01-03 14:37:57,956: t15.2024.02.25 val PER: 0.1461
2026-01-03 14:37:57,957: t15.2024.03.08 val PER: 0.2703
2026-01-03 14:37:57,957: t15.2024.03.15 val PER: 0.2389
2026-01-03 14:37:57,957: t15.2024.03.17 val PER: 0.1757
2026-01-03 14:37:57,957: t15.2024.05.10 val PER: 0.1947
2026-01-03 14:37:57,957: t15.2024.06.14 val PER: 0.1861
2026-01-03 14:37:57,957: t15.2024.07.19 val PER: 0.2769
2026-01-03 14:37:57,957: t15.2024.07.21 val PER: 0.1269
2026-01-03 14:37:57,957: t15.2024.07.28 val PER: 0.1699
2026-01-03 14:37:57,957: t15.2025.01.10 val PER: 0.3457
2026-01-03 14:37:57,957: t15.2025.01.12 val PER: 0.1801
2026-01-03 14:37:57,957: t15.2025.03.14 val PER: 0.3476
2026-01-03 14:37:57,958: t15.2025.03.16 val PER: 0.2212
2026-01-03 14:37:57,958: t15.2025.03.30 val PER: 0.3253
2026-01-03 14:37:57,958: t15.2025.04.13 val PER: 0.2482
2026-01-03 14:37:57,959: New best val WER(1gram) 53.05% --> 51.52%
2026-01-03 14:37:57,959: Checkpointing model
2026-01-03 14:37:58,230: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/best_checkpoint
2026-01-03 14:37:58,507: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_8500
2026-01-03 14:38:07,782: Train batch 8600: loss: 15.98 grad norm: 60.62 time: 0.054
2026-01-03 14:38:25,156: Train batch 8800: loss: 15.60 grad norm: 57.22 time: 0.060
2026-01-03 14:38:42,780: Train batch 9000: loss: 16.37 grad norm: 62.63 time: 0.072
2026-01-03 14:38:42,780: Running test after training batch: 9000
2026-01-03 14:38:42,945: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:38:47,654: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:38:47,685: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-03 14:38:49,409: Val batch 9000: PER (avg): 0.1780 CTC Loss (avg): 17.5421 WER(1gram): 50.76% (n=64) time: 6.629
2026-01-03 14:38:49,409: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 14:38:49,409: t15.2023.08.13 val PER: 0.1372
2026-01-03 14:38:49,409: t15.2023.08.18 val PER: 0.1090
2026-01-03 14:38:49,409: t15.2023.08.20 val PER: 0.1358
2026-01-03 14:38:49,410: t15.2023.08.25 val PER: 0.1054
2026-01-03 14:38:49,410: t15.2023.08.27 val PER: 0.2106
2026-01-03 14:38:49,410: t15.2023.09.01 val PER: 0.0998
2026-01-03 14:38:49,410: t15.2023.09.03 val PER: 0.1829
2026-01-03 14:38:49,410: t15.2023.09.24 val PER: 0.1493
2026-01-03 14:38:49,410: t15.2023.09.29 val PER: 0.1423
2026-01-03 14:38:49,410: t15.2023.10.01 val PER: 0.1902
2026-01-03 14:38:49,410: t15.2023.10.06 val PER: 0.1066
2026-01-03 14:38:49,410: t15.2023.10.08 val PER: 0.2666
2026-01-03 14:38:49,411: t15.2023.10.13 val PER: 0.2296
2026-01-03 14:38:49,411: t15.2023.10.15 val PER: 0.1872
2026-01-03 14:38:49,411: t15.2023.10.20 val PER: 0.1946
2026-01-03 14:38:49,411: t15.2023.10.22 val PER: 0.1292
2026-01-03 14:38:49,411: t15.2023.11.03 val PER: 0.1934
2026-01-03 14:38:49,411: t15.2023.11.04 val PER: 0.0375
2026-01-03 14:38:49,411: t15.2023.11.17 val PER: 0.0575
2026-01-03 14:38:49,411: t15.2023.11.19 val PER: 0.0519
2026-01-03 14:38:49,411: t15.2023.11.26 val PER: 0.1739
2026-01-03 14:38:49,411: t15.2023.12.03 val PER: 0.1439
2026-01-03 14:38:49,411: t15.2023.12.08 val PER: 0.1338
2026-01-03 14:38:49,411: t15.2023.12.10 val PER: 0.1261
2026-01-03 14:38:49,411: t15.2023.12.17 val PER: 0.1507
2026-01-03 14:38:49,411: t15.2023.12.29 val PER: 0.1695
2026-01-03 14:38:49,412: t15.2024.02.25 val PER: 0.1390
2026-01-03 14:38:49,412: t15.2024.03.08 val PER: 0.2745
2026-01-03 14:38:49,412: t15.2024.03.15 val PER: 0.2376
2026-01-03 14:38:49,412: t15.2024.03.17 val PER: 0.1792
2026-01-03 14:38:49,412: t15.2024.05.10 val PER: 0.1872
2026-01-03 14:38:49,412: t15.2024.06.14 val PER: 0.1909
2026-01-03 14:38:49,412: t15.2024.07.19 val PER: 0.2782
2026-01-03 14:38:49,412: t15.2024.07.21 val PER: 0.1255
2026-01-03 14:38:49,412: t15.2024.07.28 val PER: 0.1654
2026-01-03 14:38:49,412: t15.2025.01.10 val PER: 0.3375
2026-01-03 14:38:49,412: t15.2025.01.12 val PER: 0.1794
2026-01-03 14:38:49,412: t15.2025.03.14 val PER: 0.3550
2026-01-03 14:38:49,412: t15.2025.03.16 val PER: 0.2147
2026-01-03 14:38:49,412: t15.2025.03.30 val PER: 0.3333
2026-01-03 14:38:49,412: t15.2025.04.13 val PER: 0.2454
2026-01-03 14:38:49,414: New best val WER(1gram) 51.52% --> 50.76%
2026-01-03 14:38:49,414: Checkpointing model
2026-01-03 14:38:49,686: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/best_checkpoint
2026-01-03 14:38:49,961: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_9000
2026-01-03 14:39:07,574: Train batch 9200: loss: 11.20 grad norm: 48.47 time: 0.055
2026-01-03 14:39:24,554: Train batch 9400: loss: 8.40 grad norm: 43.61 time: 0.067
2026-01-03 14:39:33,098: Running test after training batch: 9500
2026-01-03 14:39:33,241: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:39:38,079: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:39:38,111: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 14:39:39,838: Val batch 9500: PER (avg): 0.1782 CTC Loss (avg): 17.4746 WER(1gram): 51.27% (n=64) time: 6.739
2026-01-03 14:39:39,838: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 14:39:39,839: t15.2023.08.13 val PER: 0.1362
2026-01-03 14:39:39,839: t15.2023.08.18 val PER: 0.1140
2026-01-03 14:39:39,839: t15.2023.08.20 val PER: 0.1350
2026-01-03 14:39:39,839: t15.2023.08.25 val PER: 0.1084
2026-01-03 14:39:39,839: t15.2023.08.27 val PER: 0.2219
2026-01-03 14:39:39,839: t15.2023.09.01 val PER: 0.0966
2026-01-03 14:39:39,839: t15.2023.09.03 val PER: 0.1841
2026-01-03 14:39:39,839: t15.2023.09.24 val PER: 0.1541
2026-01-03 14:39:39,839: t15.2023.09.29 val PER: 0.1423
2026-01-03 14:39:39,839: t15.2023.10.01 val PER: 0.1876
2026-01-03 14:39:39,839: t15.2023.10.06 val PER: 0.1076
2026-01-03 14:39:39,839: t15.2023.10.08 val PER: 0.2639
2026-01-03 14:39:39,840: t15.2023.10.13 val PER: 0.2327
2026-01-03 14:39:39,840: t15.2023.10.15 val PER: 0.1839
2026-01-03 14:39:39,840: t15.2023.10.20 val PER: 0.1946
2026-01-03 14:39:39,840: t15.2023.10.22 val PER: 0.1269
2026-01-03 14:39:39,840: t15.2023.11.03 val PER: 0.1906
2026-01-03 14:39:39,840: t15.2023.11.04 val PER: 0.0341
2026-01-03 14:39:39,840: t15.2023.11.17 val PER: 0.0591
2026-01-03 14:39:39,840: t15.2023.11.19 val PER: 0.0519
2026-01-03 14:39:39,840: t15.2023.11.26 val PER: 0.1768
2026-01-03 14:39:39,840: t15.2023.12.03 val PER: 0.1429
2026-01-03 14:39:39,840: t15.2023.12.08 val PER: 0.1445
2026-01-03 14:39:39,840: t15.2023.12.10 val PER: 0.1235
2026-01-03 14:39:39,840: t15.2023.12.17 val PER: 0.1590
2026-01-03 14:39:39,840: t15.2023.12.29 val PER: 0.1682
2026-01-03 14:39:39,840: t15.2024.02.25 val PER: 0.1419
2026-01-03 14:39:39,840: t15.2024.03.08 val PER: 0.2717
2026-01-03 14:39:39,840: t15.2024.03.15 val PER: 0.2383
2026-01-03 14:39:39,840: t15.2024.03.17 val PER: 0.1702
2026-01-03 14:39:39,841: t15.2024.05.10 val PER: 0.1857
2026-01-03 14:39:39,841: t15.2024.06.14 val PER: 0.1909
2026-01-03 14:39:39,841: t15.2024.07.19 val PER: 0.2795
2026-01-03 14:39:39,841: t15.2024.07.21 val PER: 0.1241
2026-01-03 14:39:39,841: t15.2024.07.28 val PER: 0.1610
2026-01-03 14:39:39,841: t15.2025.01.10 val PER: 0.3430
2026-01-03 14:39:39,841: t15.2025.01.12 val PER: 0.1794
2026-01-03 14:39:39,841: t15.2025.03.14 val PER: 0.3506
2026-01-03 14:39:39,841: t15.2025.03.16 val PER: 0.2055
2026-01-03 14:39:39,841: t15.2025.03.30 val PER: 0.3437
2026-01-03 14:39:39,841: t15.2025.04.13 val PER: 0.2397
2026-01-03 14:39:40,105: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_9500
2026-01-03 14:39:48,610: Train batch 9600: loss: 9.03 grad norm: 45.01 time: 0.074
2026-01-03 14:40:06,309: Train batch 9800: loss: 14.22 grad norm: 60.23 time: 0.063
2026-01-03 14:40:23,883: Train batch 10000: loss: 6.40 grad norm: 36.70 time: 0.061
2026-01-03 14:40:23,883: Running test after training batch: 10000
2026-01-03 14:40:23,977: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:40:28,681: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:40:28,713: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 14:40:30,469: Val batch 10000: PER (avg): 0.1778 CTC Loss (avg): 17.4429 WER(1gram): 52.03% (n=64) time: 6.586
2026-01-03 14:40:30,470: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 14:40:30,470: t15.2023.08.13 val PER: 0.1414
2026-01-03 14:40:30,470: t15.2023.08.18 val PER: 0.1174
2026-01-03 14:40:30,470: t15.2023.08.20 val PER: 0.1342
2026-01-03 14:40:30,470: t15.2023.08.25 val PER: 0.1024
2026-01-03 14:40:30,470: t15.2023.08.27 val PER: 0.2122
2026-01-03 14:40:30,470: t15.2023.09.01 val PER: 0.0974
2026-01-03 14:40:30,470: t15.2023.09.03 val PER: 0.1960
2026-01-03 14:40:30,470: t15.2023.09.24 val PER: 0.1517
2026-01-03 14:40:30,470: t15.2023.09.29 val PER: 0.1410
2026-01-03 14:40:30,470: t15.2023.10.01 val PER: 0.1968
2026-01-03 14:40:30,470: t15.2023.10.06 val PER: 0.1044
2026-01-03 14:40:30,470: t15.2023.10.08 val PER: 0.2652
2026-01-03 14:40:30,470: t15.2023.10.13 val PER: 0.2335
2026-01-03 14:40:30,474: t15.2023.10.15 val PER: 0.1872
2026-01-03 14:40:30,475: t15.2023.10.20 val PER: 0.1779
2026-01-03 14:40:30,475: t15.2023.10.22 val PER: 0.1325
2026-01-03 14:40:30,475: t15.2023.11.03 val PER: 0.2001
2026-01-03 14:40:30,475: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:40:30,475: t15.2023.11.17 val PER: 0.0575
2026-01-03 14:40:30,475: t15.2023.11.19 val PER: 0.0539
2026-01-03 14:40:30,475: t15.2023.11.26 val PER: 0.1732
2026-01-03 14:40:30,475: t15.2023.12.03 val PER: 0.1429
2026-01-03 14:40:30,475: t15.2023.12.08 val PER: 0.1411
2026-01-03 14:40:30,475: t15.2023.12.10 val PER: 0.1275
2026-01-03 14:40:30,475: t15.2023.12.17 val PER: 0.1538
2026-01-03 14:40:30,476: t15.2023.12.29 val PER: 0.1647
2026-01-03 14:40:30,476: t15.2024.02.25 val PER: 0.1362
2026-01-03 14:40:30,476: t15.2024.03.08 val PER: 0.2731
2026-01-03 14:40:30,476: t15.2024.03.15 val PER: 0.2358
2026-01-03 14:40:30,476: t15.2024.03.17 val PER: 0.1709
2026-01-03 14:40:30,476: t15.2024.05.10 val PER: 0.1828
2026-01-03 14:40:30,476: t15.2024.06.14 val PER: 0.1877
2026-01-03 14:40:30,476: t15.2024.07.19 val PER: 0.2683
2026-01-03 14:40:30,476: t15.2024.07.21 val PER: 0.1248
2026-01-03 14:40:30,476: t15.2024.07.28 val PER: 0.1654
2026-01-03 14:40:30,476: t15.2025.01.10 val PER: 0.3333
2026-01-03 14:40:30,476: t15.2025.01.12 val PER: 0.1740
2026-01-03 14:40:30,476: t15.2025.03.14 val PER: 0.3506
2026-01-03 14:40:30,476: t15.2025.03.16 val PER: 0.2147
2026-01-03 14:40:30,476: t15.2025.03.30 val PER: 0.3322
2026-01-03 14:40:30,477: t15.2025.04.13 val PER: 0.2425
2026-01-03 14:40:30,737: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_10000
2026-01-03 14:40:48,014: Train batch 10200: loss: 7.09 grad norm: 38.21 time: 0.049
2026-01-03 14:41:06,157: Train batch 10400: loss: 10.82 grad norm: 50.10 time: 0.071
2026-01-03 14:41:14,974: Running test after training batch: 10500
2026-01-03 14:41:15,109: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:41:19,855: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:41:19,887: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 14:41:21,646: Val batch 10500: PER (avg): 0.1761 CTC Loss (avg): 17.4010 WER(1gram): 51.02% (n=64) time: 6.672
2026-01-03 14:41:21,647: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 14:41:21,647: t15.2023.08.13 val PER: 0.1372
2026-01-03 14:41:21,647: t15.2023.08.18 val PER: 0.1106
2026-01-03 14:41:21,647: t15.2023.08.20 val PER: 0.1342
2026-01-03 14:41:21,647: t15.2023.08.25 val PER: 0.1069
2026-01-03 14:41:21,647: t15.2023.08.27 val PER: 0.2154
2026-01-03 14:41:21,647: t15.2023.09.01 val PER: 0.0974
2026-01-03 14:41:21,647: t15.2023.09.03 val PER: 0.1900
2026-01-03 14:41:21,647: t15.2023.09.24 val PER: 0.1493
2026-01-03 14:41:21,647: t15.2023.09.29 val PER: 0.1385
2026-01-03 14:41:21,647: t15.2023.10.01 val PER: 0.1915
2026-01-03 14:41:21,648: t15.2023.10.06 val PER: 0.1044
2026-01-03 14:41:21,648: t15.2023.10.08 val PER: 0.2598
2026-01-03 14:41:21,648: t15.2023.10.13 val PER: 0.2351
2026-01-03 14:41:21,648: t15.2023.10.15 val PER: 0.1872
2026-01-03 14:41:21,648: t15.2023.10.20 val PER: 0.1946
2026-01-03 14:41:21,648: t15.2023.10.22 val PER: 0.1303
2026-01-03 14:41:21,648: t15.2023.11.03 val PER: 0.1934
2026-01-03 14:41:21,648: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:41:21,648: t15.2023.11.17 val PER: 0.0529
2026-01-03 14:41:21,648: t15.2023.11.19 val PER: 0.0519
2026-01-03 14:41:21,648: t15.2023.11.26 val PER: 0.1732
2026-01-03 14:41:21,649: t15.2023.12.03 val PER: 0.1408
2026-01-03 14:41:21,649: t15.2023.12.08 val PER: 0.1418
2026-01-03 14:41:21,649: t15.2023.12.10 val PER: 0.1222
2026-01-03 14:41:21,649: t15.2023.12.17 val PER: 0.1580
2026-01-03 14:41:21,649: t15.2023.12.29 val PER: 0.1682
2026-01-03 14:41:21,649: t15.2024.02.25 val PER: 0.1334
2026-01-03 14:41:21,649: t15.2024.03.08 val PER: 0.2703
2026-01-03 14:41:21,649: t15.2024.03.15 val PER: 0.2333
2026-01-03 14:41:21,649: t15.2024.03.17 val PER: 0.1667
2026-01-03 14:41:21,649: t15.2024.05.10 val PER: 0.1828
2026-01-03 14:41:21,649: t15.2024.06.14 val PER: 0.1814
2026-01-03 14:41:21,649: t15.2024.07.19 val PER: 0.2683
2026-01-03 14:41:21,649: t15.2024.07.21 val PER: 0.1241
2026-01-03 14:41:21,649: t15.2024.07.28 val PER: 0.1625
2026-01-03 14:41:21,649: t15.2025.01.10 val PER: 0.3430
2026-01-03 14:41:21,649: t15.2025.01.12 val PER: 0.1763
2026-01-03 14:41:21,650: t15.2025.03.14 val PER: 0.3462
2026-01-03 14:41:21,650: t15.2025.03.16 val PER: 0.1990
2026-01-03 14:41:21,650: t15.2025.03.30 val PER: 0.3299
2026-01-03 14:41:21,650: t15.2025.04.13 val PER: 0.2397
2026-01-03 14:41:21,912: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_10500
2026-01-03 14:41:30,711: Train batch 10600: loss: 10.56 grad norm: 51.47 time: 0.071
2026-01-03 14:41:48,459: Train batch 10800: loss: 17.03 grad norm: 64.47 time: 0.065
2026-01-03 14:42:06,287: Train batch 11000: loss: 17.31 grad norm: 63.65 time: 0.056
2026-01-03 14:42:06,287: Running test after training batch: 11000
2026-01-03 14:42:06,386: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:42:11,118: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:42:11,150: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 14:42:12,929: Val batch 11000: PER (avg): 0.1764 CTC Loss (avg): 17.3634 WER(1gram): 51.52% (n=64) time: 6.641
2026-01-03 14:42:12,929: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 14:42:12,929: t15.2023.08.13 val PER: 0.1341
2026-01-03 14:42:12,929: t15.2023.08.18 val PER: 0.1165
2026-01-03 14:42:12,929: t15.2023.08.20 val PER: 0.1350
2026-01-03 14:42:12,929: t15.2023.08.25 val PER: 0.1009
2026-01-03 14:42:12,929: t15.2023.08.27 val PER: 0.2186
2026-01-03 14:42:12,930: t15.2023.09.01 val PER: 0.0966
2026-01-03 14:42:12,930: t15.2023.09.03 val PER: 0.1888
2026-01-03 14:42:12,930: t15.2023.09.24 val PER: 0.1529
2026-01-03 14:42:12,930: t15.2023.09.29 val PER: 0.1481
2026-01-03 14:42:12,930: t15.2023.10.01 val PER: 0.1955
2026-01-03 14:42:12,930: t15.2023.10.06 val PER: 0.1033
2026-01-03 14:42:12,930: t15.2023.10.08 val PER: 0.2571
2026-01-03 14:42:12,930: t15.2023.10.13 val PER: 0.2296
2026-01-03 14:42:12,930: t15.2023.10.15 val PER: 0.1885
2026-01-03 14:42:12,930: t15.2023.10.20 val PER: 0.1745
2026-01-03 14:42:12,930: t15.2023.10.22 val PER: 0.1269
2026-01-03 14:42:12,930: t15.2023.11.03 val PER: 0.1981
2026-01-03 14:42:12,930: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:42:12,930: t15.2023.11.17 val PER: 0.0544
2026-01-03 14:42:12,931: t15.2023.11.19 val PER: 0.0559
2026-01-03 14:42:12,931: t15.2023.11.26 val PER: 0.1674
2026-01-03 14:42:12,931: t15.2023.12.03 val PER: 0.1439
2026-01-03 14:42:12,931: t15.2023.12.08 val PER: 0.1418
2026-01-03 14:42:12,931: t15.2023.12.10 val PER: 0.1209
2026-01-03 14:42:12,931: t15.2023.12.17 val PER: 0.1590
2026-01-03 14:42:12,931: t15.2023.12.29 val PER: 0.1620
2026-01-03 14:42:12,931: t15.2024.02.25 val PER: 0.1404
2026-01-03 14:42:12,931: t15.2024.03.08 val PER: 0.2632
2026-01-03 14:42:12,931: t15.2024.03.15 val PER: 0.2314
2026-01-03 14:42:12,931: t15.2024.03.17 val PER: 0.1653
2026-01-03 14:42:12,931: t15.2024.05.10 val PER: 0.1798
2026-01-03 14:42:12,931: t15.2024.06.14 val PER: 0.1940
2026-01-03 14:42:12,931: t15.2024.07.19 val PER: 0.2749
2026-01-03 14:42:12,931: t15.2024.07.21 val PER: 0.1186
2026-01-03 14:42:12,931: t15.2024.07.28 val PER: 0.1640
2026-01-03 14:42:12,931: t15.2025.01.10 val PER: 0.3375
2026-01-03 14:42:12,932: t15.2025.01.12 val PER: 0.1771
2026-01-03 14:42:12,932: t15.2025.03.14 val PER: 0.3476
2026-01-03 14:42:12,932: t15.2025.03.16 val PER: 0.2055
2026-01-03 14:42:12,932: t15.2025.03.30 val PER: 0.3299
2026-01-03 14:42:12,932: t15.2025.04.13 val PER: 0.2354
2026-01-03 14:42:13,194: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_11000
2026-01-03 14:42:30,840: Train batch 11200: loss: 12.51 grad norm: 51.39 time: 0.071
2026-01-03 14:42:48,453: Train batch 11400: loss: 11.81 grad norm: 55.78 time: 0.057
2026-01-03 14:42:57,251: Running test after training batch: 11500
2026-01-03 14:42:57,404: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:43:02,118: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:43:02,152: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-03 14:43:03,958: Val batch 11500: PER (avg): 0.1763 CTC Loss (avg): 17.3312 WER(1gram): 50.76% (n=64) time: 6.706
2026-01-03 14:43:03,958: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 14:43:03,958: t15.2023.08.13 val PER: 0.1362
2026-01-03 14:43:03,958: t15.2023.08.18 val PER: 0.1140
2026-01-03 14:43:03,958: t15.2023.08.20 val PER: 0.1390
2026-01-03 14:43:03,958: t15.2023.08.25 val PER: 0.1024
2026-01-03 14:43:03,959: t15.2023.08.27 val PER: 0.2186
2026-01-03 14:43:03,959: t15.2023.09.01 val PER: 0.0942
2026-01-03 14:43:03,959: t15.2023.09.03 val PER: 0.1900
2026-01-03 14:43:03,959: t15.2023.09.24 val PER: 0.1444
2026-01-03 14:43:03,959: t15.2023.09.29 val PER: 0.1378
2026-01-03 14:43:03,959: t15.2023.10.01 val PER: 0.1889
2026-01-03 14:43:03,959: t15.2023.10.06 val PER: 0.1033
2026-01-03 14:43:03,959: t15.2023.10.08 val PER: 0.2639
2026-01-03 14:43:03,960: t15.2023.10.13 val PER: 0.2304
2026-01-03 14:43:03,960: t15.2023.10.15 val PER: 0.1872
2026-01-03 14:43:03,960: t15.2023.10.20 val PER: 0.1779
2026-01-03 14:43:03,960: t15.2023.10.22 val PER: 0.1292
2026-01-03 14:43:03,960: t15.2023.11.03 val PER: 0.1947
2026-01-03 14:43:03,960: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:43:03,960: t15.2023.11.17 val PER: 0.0560
2026-01-03 14:43:03,960: t15.2023.11.19 val PER: 0.0539
2026-01-03 14:43:03,960: t15.2023.11.26 val PER: 0.1681
2026-01-03 14:43:03,960: t15.2023.12.03 val PER: 0.1387
2026-01-03 14:43:03,960: t15.2023.12.08 val PER: 0.1445
2026-01-03 14:43:03,961: t15.2023.12.10 val PER: 0.1235
2026-01-03 14:43:03,961: t15.2023.12.17 val PER: 0.1559
2026-01-03 14:43:03,961: t15.2023.12.29 val PER: 0.1661
2026-01-03 14:43:03,961: t15.2024.02.25 val PER: 0.1419
2026-01-03 14:43:03,961: t15.2024.03.08 val PER: 0.2674
2026-01-03 14:43:03,961: t15.2024.03.15 val PER: 0.2370
2026-01-03 14:43:03,961: t15.2024.03.17 val PER: 0.1688
2026-01-03 14:43:03,961: t15.2024.05.10 val PER: 0.1872
2026-01-03 14:43:03,961: t15.2024.06.14 val PER: 0.1877
2026-01-03 14:43:03,962: t15.2024.07.19 val PER: 0.2690
2026-01-03 14:43:03,962: t15.2024.07.21 val PER: 0.1228
2026-01-03 14:43:03,962: t15.2024.07.28 val PER: 0.1654
2026-01-03 14:43:03,962: t15.2025.01.10 val PER: 0.3333
2026-01-03 14:43:03,962: t15.2025.01.12 val PER: 0.1778
2026-01-03 14:43:03,962: t15.2025.03.14 val PER: 0.3595
2026-01-03 14:43:03,962: t15.2025.03.16 val PER: 0.2029
2026-01-03 14:43:03,962: t15.2025.03.30 val PER: 0.3264
2026-01-03 14:43:03,962: t15.2025.04.13 val PER: 0.2397
2026-01-03 14:43:04,226: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_11500
2026-01-03 14:43:13,277: Train batch 11600: loss: 13.37 grad norm: 51.57 time: 0.061
2026-01-03 14:43:30,880: Train batch 11800: loss: 8.53 grad norm: 45.40 time: 0.044
2026-01-03 14:43:49,146: Train batch 12000: loss: 16.23 grad norm: 53.54 time: 0.072
2026-01-03 14:43:49,146: Running test after training batch: 12000
2026-01-03 14:43:49,248: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:43:54,123: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:43:54,155: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 14:43:55,954: Val batch 12000: PER (avg): 0.1753 CTC Loss (avg): 17.2187 WER(1gram): 51.52% (n=64) time: 6.807
2026-01-03 14:43:55,954: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 14:43:55,954: t15.2023.08.13 val PER: 0.1331
2026-01-03 14:43:55,954: t15.2023.08.18 val PER: 0.1174
2026-01-03 14:43:55,954: t15.2023.08.20 val PER: 0.1358
2026-01-03 14:43:55,954: t15.2023.08.25 val PER: 0.1024
2026-01-03 14:43:55,954: t15.2023.08.27 val PER: 0.2138
2026-01-03 14:43:55,954: t15.2023.09.01 val PER: 0.0925
2026-01-03 14:43:55,954: t15.2023.09.03 val PER: 0.1817
2026-01-03 14:43:55,955: t15.2023.09.24 val PER: 0.1468
2026-01-03 14:43:55,955: t15.2023.09.29 val PER: 0.1410
2026-01-03 14:43:55,955: t15.2023.10.01 val PER: 0.1863
2026-01-03 14:43:55,955: t15.2023.10.06 val PER: 0.1023
2026-01-03 14:43:55,955: t15.2023.10.08 val PER: 0.2571
2026-01-03 14:43:55,955: t15.2023.10.13 val PER: 0.2289
2026-01-03 14:43:55,955: t15.2023.10.15 val PER: 0.1826
2026-01-03 14:43:55,955: t15.2023.10.20 val PER: 0.1812
2026-01-03 14:43:55,955: t15.2023.10.22 val PER: 0.1336
2026-01-03 14:43:55,955: t15.2023.11.03 val PER: 0.1981
2026-01-03 14:43:55,955: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:43:55,955: t15.2023.11.17 val PER: 0.0513
2026-01-03 14:43:55,955: t15.2023.11.19 val PER: 0.0559
2026-01-03 14:43:55,955: t15.2023.11.26 val PER: 0.1645
2026-01-03 14:43:55,955: t15.2023.12.03 val PER: 0.1397
2026-01-03 14:43:55,956: t15.2023.12.08 val PER: 0.1365
2026-01-03 14:43:55,956: t15.2023.12.10 val PER: 0.1248
2026-01-03 14:43:55,956: t15.2023.12.17 val PER: 0.1497
2026-01-03 14:43:55,956: t15.2023.12.29 val PER: 0.1627
2026-01-03 14:43:55,956: t15.2024.02.25 val PER: 0.1419
2026-01-03 14:43:55,956: t15.2024.03.08 val PER: 0.2646
2026-01-03 14:43:55,956: t15.2024.03.15 val PER: 0.2345
2026-01-03 14:43:55,956: t15.2024.03.17 val PER: 0.1702
2026-01-03 14:43:55,956: t15.2024.05.10 val PER: 0.1857
2026-01-03 14:43:55,956: t15.2024.06.14 val PER: 0.1877
2026-01-03 14:43:55,956: t15.2024.07.19 val PER: 0.2709
2026-01-03 14:43:55,956: t15.2024.07.21 val PER: 0.1234
2026-01-03 14:43:55,956: t15.2024.07.28 val PER: 0.1699
2026-01-03 14:43:55,957: t15.2025.01.10 val PER: 0.3361
2026-01-03 14:43:55,957: t15.2025.01.12 val PER: 0.1771
2026-01-03 14:43:55,957: t15.2025.03.14 val PER: 0.3550
2026-01-03 14:43:55,957: t15.2025.03.16 val PER: 0.2055
2026-01-03 14:43:55,957: t15.2025.03.30 val PER: 0.3264
2026-01-03 14:43:55,957: t15.2025.04.13 val PER: 0.2340
2026-01-03 14:43:56,215: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_12000
2026-01-03 14:44:13,636: Train batch 12200: loss: 7.90 grad norm: 44.04 time: 0.065
2026-01-03 14:44:30,939: Train batch 12400: loss: 6.09 grad norm: 36.11 time: 0.040
2026-01-03 14:44:40,493: Running test after training batch: 12500
2026-01-03 14:44:40,590: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:44:45,349: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:44:45,383: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 14:44:47,183: Val batch 12500: PER (avg): 0.1751 CTC Loss (avg): 17.2116 WER(1gram): 51.02% (n=64) time: 6.689
2026-01-03 14:44:47,183: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 14:44:47,183: t15.2023.08.13 val PER: 0.1372
2026-01-03 14:44:47,183: t15.2023.08.18 val PER: 0.1132
2026-01-03 14:44:47,183: t15.2023.08.20 val PER: 0.1326
2026-01-03 14:44:47,183: t15.2023.08.25 val PER: 0.0964
2026-01-03 14:44:47,183: t15.2023.08.27 val PER: 0.2154
2026-01-03 14:44:47,183: t15.2023.09.01 val PER: 0.0942
2026-01-03 14:44:47,184: t15.2023.09.03 val PER: 0.1853
2026-01-03 14:44:47,184: t15.2023.09.24 val PER: 0.1444
2026-01-03 14:44:47,184: t15.2023.09.29 val PER: 0.1410
2026-01-03 14:44:47,184: t15.2023.10.01 val PER: 0.1896
2026-01-03 14:44:47,184: t15.2023.10.06 val PER: 0.1098
2026-01-03 14:44:47,184: t15.2023.10.08 val PER: 0.2639
2026-01-03 14:44:47,184: t15.2023.10.13 val PER: 0.2304
2026-01-03 14:44:47,184: t15.2023.10.15 val PER: 0.1833
2026-01-03 14:44:47,184: t15.2023.10.20 val PER: 0.1779
2026-01-03 14:44:47,184: t15.2023.10.22 val PER: 0.1225
2026-01-03 14:44:47,184: t15.2023.11.03 val PER: 0.1988
2026-01-03 14:44:47,184: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:44:47,184: t15.2023.11.17 val PER: 0.0513
2026-01-03 14:44:47,184: t15.2023.11.19 val PER: 0.0539
2026-01-03 14:44:47,184: t15.2023.11.26 val PER: 0.1688
2026-01-03 14:44:47,184: t15.2023.12.03 val PER: 0.1376
2026-01-03 14:44:47,185: t15.2023.12.08 val PER: 0.1451
2026-01-03 14:44:47,185: t15.2023.12.10 val PER: 0.1248
2026-01-03 14:44:47,185: t15.2023.12.17 val PER: 0.1559
2026-01-03 14:44:47,185: t15.2023.12.29 val PER: 0.1606
2026-01-03 14:44:47,185: t15.2024.02.25 val PER: 0.1334
2026-01-03 14:44:47,185: t15.2024.03.08 val PER: 0.2632
2026-01-03 14:44:47,185: t15.2024.03.15 val PER: 0.2351
2026-01-03 14:44:47,185: t15.2024.03.17 val PER: 0.1646
2026-01-03 14:44:47,185: t15.2024.05.10 val PER: 0.1842
2026-01-03 14:44:47,185: t15.2024.06.14 val PER: 0.1909
2026-01-03 14:44:47,185: t15.2024.07.19 val PER: 0.2749
2026-01-03 14:44:47,185: t15.2024.07.21 val PER: 0.1179
2026-01-03 14:44:47,185: t15.2024.07.28 val PER: 0.1625
2026-01-03 14:44:47,185: t15.2025.01.10 val PER: 0.3375
2026-01-03 14:44:47,185: t15.2025.01.12 val PER: 0.1747
2026-01-03 14:44:47,185: t15.2025.03.14 val PER: 0.3521
2026-01-03 14:44:47,186: t15.2025.03.16 val PER: 0.1976
2026-01-03 14:44:47,186: t15.2025.03.30 val PER: 0.3299
2026-01-03 14:44:47,186: t15.2025.04.13 val PER: 0.2325
2026-01-03 14:44:47,453: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_12500
2026-01-03 14:44:56,764: Train batch 12600: loss: 10.65 grad norm: 45.87 time: 0.057
2026-01-03 14:45:14,798: Train batch 12800: loss: 7.98 grad norm: 42.69 time: 0.053
2026-01-03 14:45:32,649: Train batch 13000: loss: 8.48 grad norm: 45.37 time: 0.066
2026-01-03 14:45:32,650: Running test after training batch: 13000
2026-01-03 14:45:32,743: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:45:37,619: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:45:37,652: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-03 14:45:39,507: Val batch 13000: PER (avg): 0.1743 CTC Loss (avg): 17.1654 WER(1gram): 50.25% (n=64) time: 6.857
2026-01-03 14:45:39,507: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 14:45:39,508: t15.2023.08.13 val PER: 0.1341
2026-01-03 14:45:39,508: t15.2023.08.18 val PER: 0.1073
2026-01-03 14:45:39,508: t15.2023.08.20 val PER: 0.1334
2026-01-03 14:45:39,508: t15.2023.08.25 val PER: 0.0964
2026-01-03 14:45:39,508: t15.2023.08.27 val PER: 0.2154
2026-01-03 14:45:39,508: t15.2023.09.01 val PER: 0.0942
2026-01-03 14:45:39,508: t15.2023.09.03 val PER: 0.1841
2026-01-03 14:45:39,508: t15.2023.09.24 val PER: 0.1505
2026-01-03 14:45:39,508: t15.2023.09.29 val PER: 0.1423
2026-01-03 14:45:39,508: t15.2023.10.01 val PER: 0.1935
2026-01-03 14:45:39,508: t15.2023.10.06 val PER: 0.1023
2026-01-03 14:45:39,508: t15.2023.10.08 val PER: 0.2598
2026-01-03 14:45:39,508: t15.2023.10.13 val PER: 0.2296
2026-01-03 14:45:39,508: t15.2023.10.15 val PER: 0.1819
2026-01-03 14:45:39,509: t15.2023.10.20 val PER: 0.1913
2026-01-03 14:45:39,509: t15.2023.10.22 val PER: 0.1258
2026-01-03 14:45:39,509: t15.2023.11.03 val PER: 0.1974
2026-01-03 14:45:39,509: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:45:39,509: t15.2023.11.17 val PER: 0.0529
2026-01-03 14:45:39,509: t15.2023.11.19 val PER: 0.0499
2026-01-03 14:45:39,509: t15.2023.11.26 val PER: 0.1688
2026-01-03 14:45:39,509: t15.2023.12.03 val PER: 0.1376
2026-01-03 14:45:39,509: t15.2023.12.08 val PER: 0.1418
2026-01-03 14:45:39,509: t15.2023.12.10 val PER: 0.1288
2026-01-03 14:45:39,509: t15.2023.12.17 val PER: 0.1486
2026-01-03 14:45:39,509: t15.2023.12.29 val PER: 0.1647
2026-01-03 14:45:39,509: t15.2024.02.25 val PER: 0.1433
2026-01-03 14:45:39,509: t15.2024.03.08 val PER: 0.2504
2026-01-03 14:45:39,509: t15.2024.03.15 val PER: 0.2301
2026-01-03 14:45:39,509: t15.2024.03.17 val PER: 0.1625
2026-01-03 14:45:39,510: t15.2024.05.10 val PER: 0.1887
2026-01-03 14:45:39,510: t15.2024.06.14 val PER: 0.1845
2026-01-03 14:45:39,510: t15.2024.07.19 val PER: 0.2709
2026-01-03 14:45:39,510: t15.2024.07.21 val PER: 0.1179
2026-01-03 14:45:39,510: t15.2024.07.28 val PER: 0.1632
2026-01-03 14:45:39,510: t15.2025.01.10 val PER: 0.3333
2026-01-03 14:45:39,510: t15.2025.01.12 val PER: 0.1709
2026-01-03 14:45:39,510: t15.2025.03.14 val PER: 0.3447
2026-01-03 14:45:39,510: t15.2025.03.16 val PER: 0.2029
2026-01-03 14:45:39,510: t15.2025.03.30 val PER: 0.3322
2026-01-03 14:45:39,510: t15.2025.04.13 val PER: 0.2368
2026-01-03 14:45:39,511: New best val WER(1gram) 50.76% --> 50.25%
2026-01-03 14:45:39,511: Checkpointing model
2026-01-03 14:45:39,782: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/best_checkpoint
2026-01-03 14:45:40,055: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_13000
2026-01-03 14:45:57,364: Train batch 13200: loss: 17.84 grad norm: 68.46 time: 0.054
2026-01-03 14:46:15,295: Train batch 13400: loss: 13.05 grad norm: 56.49 time: 0.062
2026-01-03 14:46:24,056: Running test after training batch: 13500
2026-01-03 14:46:24,176: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:46:28,907: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 14:46:28,941: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 14:46:30,723: Val batch 13500: PER (avg): 0.1742 CTC Loss (avg): 17.1968 WER(1gram): 51.52% (n=64) time: 6.666
2026-01-03 14:46:30,723: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 14:46:30,724: t15.2023.08.13 val PER: 0.1310
2026-01-03 14:46:30,724: t15.2023.08.18 val PER: 0.1157
2026-01-03 14:46:30,724: t15.2023.08.20 val PER: 0.1350
2026-01-03 14:46:30,724: t15.2023.08.25 val PER: 0.1009
2026-01-03 14:46:30,724: t15.2023.08.27 val PER: 0.2186
2026-01-03 14:46:30,724: t15.2023.09.01 val PER: 0.0925
2026-01-03 14:46:30,724: t15.2023.09.03 val PER: 0.1829
2026-01-03 14:46:30,724: t15.2023.09.24 val PER: 0.1481
2026-01-03 14:46:30,724: t15.2023.09.29 val PER: 0.1429
2026-01-03 14:46:30,724: t15.2023.10.01 val PER: 0.1889
2026-01-03 14:46:30,724: t15.2023.10.06 val PER: 0.1023
2026-01-03 14:46:30,725: t15.2023.10.08 val PER: 0.2585
2026-01-03 14:46:30,725: t15.2023.10.13 val PER: 0.2358
2026-01-03 14:46:30,725: t15.2023.10.15 val PER: 0.1819
2026-01-03 14:46:30,725: t15.2023.10.20 val PER: 0.1913
2026-01-03 14:46:30,725: t15.2023.10.22 val PER: 0.1314
2026-01-03 14:46:30,725: t15.2023.11.03 val PER: 0.2015
2026-01-03 14:46:30,725: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:46:30,725: t15.2023.11.17 val PER: 0.0560
2026-01-03 14:46:30,725: t15.2023.11.19 val PER: 0.0479
2026-01-03 14:46:30,725: t15.2023.11.26 val PER: 0.1652
2026-01-03 14:46:30,725: t15.2023.12.03 val PER: 0.1366
2026-01-03 14:46:30,725: t15.2023.12.08 val PER: 0.1411
2026-01-03 14:46:30,725: t15.2023.12.10 val PER: 0.1209
2026-01-03 14:46:30,725: t15.2023.12.17 val PER: 0.1507
2026-01-03 14:46:30,725: t15.2023.12.29 val PER: 0.1606
2026-01-03 14:46:30,725: t15.2024.02.25 val PER: 0.1362
2026-01-03 14:46:30,725: t15.2024.03.08 val PER: 0.2589
2026-01-03 14:46:30,726: t15.2024.03.15 val PER: 0.2301
2026-01-03 14:46:30,726: t15.2024.03.17 val PER: 0.1660
2026-01-03 14:46:30,726: t15.2024.05.10 val PER: 0.1828
2026-01-03 14:46:30,726: t15.2024.06.14 val PER: 0.1893
2026-01-03 14:46:30,726: t15.2024.07.19 val PER: 0.2709
2026-01-03 14:46:30,726: t15.2024.07.21 val PER: 0.1179
2026-01-03 14:46:30,726: t15.2024.07.28 val PER: 0.1610
2026-01-03 14:46:30,726: t15.2025.01.10 val PER: 0.3347
2026-01-03 14:46:30,726: t15.2025.01.12 val PER: 0.1709
2026-01-03 14:46:30,726: t15.2025.03.14 val PER: 0.3417
2026-01-03 14:46:30,726: t15.2025.03.16 val PER: 0.2003
2026-01-03 14:46:30,726: t15.2025.03.30 val PER: 0.3276
2026-01-03 14:46:30,726: t15.2025.04.13 val PER: 0.2311
2026-01-03 14:46:30,992: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_13500
2026-01-03 14:46:40,167: Train batch 13600: loss: 16.89 grad norm: 63.21 time: 0.062
2026-01-03 14:46:58,473: Train batch 13800: loss: 12.94 grad norm: 59.43 time: 0.056
2026-01-03 14:47:16,303: Train batch 14000: loss: 15.60 grad norm: 59.50 time: 0.051
2026-01-03 14:47:16,303: Running test after training batch: 14000
2026-01-03 14:47:16,405: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:47:21,232: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 14:47:21,265: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 14:47:23,093: Val batch 14000: PER (avg): 0.1741 CTC Loss (avg): 17.1320 WER(1gram): 50.76% (n=64) time: 6.790
2026-01-03 14:47:23,093: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 14:47:23,093: t15.2023.08.13 val PER: 0.1320
2026-01-03 14:47:23,093: t15.2023.08.18 val PER: 0.1106
2026-01-03 14:47:23,094: t15.2023.08.20 val PER: 0.1358
2026-01-03 14:47:23,094: t15.2023.08.25 val PER: 0.1009
2026-01-03 14:47:23,094: t15.2023.08.27 val PER: 0.2138
2026-01-03 14:47:23,094: t15.2023.09.01 val PER: 0.0933
2026-01-03 14:47:23,094: t15.2023.09.03 val PER: 0.1876
2026-01-03 14:47:23,094: t15.2023.09.24 val PER: 0.1493
2026-01-03 14:47:23,094: t15.2023.09.29 val PER: 0.1391
2026-01-03 14:47:23,094: t15.2023.10.01 val PER: 0.1876
2026-01-03 14:47:23,094: t15.2023.10.06 val PER: 0.0990
2026-01-03 14:47:23,094: t15.2023.10.08 val PER: 0.2585
2026-01-03 14:47:23,094: t15.2023.10.13 val PER: 0.2312
2026-01-03 14:47:23,094: t15.2023.10.15 val PER: 0.1879
2026-01-03 14:47:23,094: t15.2023.10.20 val PER: 0.1879
2026-01-03 14:47:23,094: t15.2023.10.22 val PER: 0.1281
2026-01-03 14:47:23,095: t15.2023.11.03 val PER: 0.1967
2026-01-03 14:47:23,095: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:47:23,095: t15.2023.11.17 val PER: 0.0544
2026-01-03 14:47:23,095: t15.2023.11.19 val PER: 0.0519
2026-01-03 14:47:23,095: t15.2023.11.26 val PER: 0.1638
2026-01-03 14:47:23,095: t15.2023.12.03 val PER: 0.1418
2026-01-03 14:47:23,095: t15.2023.12.08 val PER: 0.1405
2026-01-03 14:47:23,095: t15.2023.12.10 val PER: 0.1261
2026-01-03 14:47:23,095: t15.2023.12.17 val PER: 0.1445
2026-01-03 14:47:23,095: t15.2023.12.29 val PER: 0.1627
2026-01-03 14:47:23,095: t15.2024.02.25 val PER: 0.1390
2026-01-03 14:47:23,095: t15.2024.03.08 val PER: 0.2617
2026-01-03 14:47:23,095: t15.2024.03.15 val PER: 0.2301
2026-01-03 14:47:23,095: t15.2024.03.17 val PER: 0.1625
2026-01-03 14:47:23,095: t15.2024.05.10 val PER: 0.1813
2026-01-03 14:47:23,095: t15.2024.06.14 val PER: 0.1814
2026-01-03 14:47:23,096: t15.2024.07.19 val PER: 0.2722
2026-01-03 14:47:23,096: t15.2024.07.21 val PER: 0.1179
2026-01-03 14:47:23,096: t15.2024.07.28 val PER: 0.1654
2026-01-03 14:47:23,096: t15.2025.01.10 val PER: 0.3375
2026-01-03 14:47:23,096: t15.2025.01.12 val PER: 0.1755
2026-01-03 14:47:23,096: t15.2025.03.14 val PER: 0.3447
2026-01-03 14:47:23,096: t15.2025.03.16 val PER: 0.2016
2026-01-03 14:47:23,096: t15.2025.03.30 val PER: 0.3264
2026-01-03 14:47:23,096: t15.2025.04.13 val PER: 0.2340
2026-01-03 14:47:23,365: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_14000
2026-01-03 14:47:40,902: Train batch 14200: loss: 11.57 grad norm: 52.15 time: 0.055
2026-01-03 14:47:58,646: Train batch 14400: loss: 8.26 grad norm: 45.61 time: 0.064
2026-01-03 14:48:07,637: Running test after training batch: 14500
2026-01-03 14:48:07,739: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:48:12,466: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:48:12,499: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 14:48:14,381: Val batch 14500: PER (avg): 0.1740 CTC Loss (avg): 17.1332 WER(1gram): 50.25% (n=64) time: 6.744
2026-01-03 14:48:14,382: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 14:48:14,382: t15.2023.08.13 val PER: 0.1320
2026-01-03 14:48:14,382: t15.2023.08.18 val PER: 0.1123
2026-01-03 14:48:14,382: t15.2023.08.20 val PER: 0.1319
2026-01-03 14:48:14,382: t15.2023.08.25 val PER: 0.1039
2026-01-03 14:48:14,382: t15.2023.08.27 val PER: 0.2138
2026-01-03 14:48:14,382: t15.2023.09.01 val PER: 0.0950
2026-01-03 14:48:14,383: t15.2023.09.03 val PER: 0.1853
2026-01-03 14:48:14,383: t15.2023.09.24 val PER: 0.1481
2026-01-03 14:48:14,383: t15.2023.09.29 val PER: 0.1410
2026-01-03 14:48:14,383: t15.2023.10.01 val PER: 0.1876
2026-01-03 14:48:14,383: t15.2023.10.06 val PER: 0.1001
2026-01-03 14:48:14,383: t15.2023.10.08 val PER: 0.2612
2026-01-03 14:48:14,383: t15.2023.10.13 val PER: 0.2358
2026-01-03 14:48:14,383: t15.2023.10.15 val PER: 0.1833
2026-01-03 14:48:14,383: t15.2023.10.20 val PER: 0.1879
2026-01-03 14:48:14,383: t15.2023.10.22 val PER: 0.1269
2026-01-03 14:48:14,384: t15.2023.11.03 val PER: 0.1961
2026-01-03 14:48:14,384: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:48:14,384: t15.2023.11.17 val PER: 0.0544
2026-01-03 14:48:14,384: t15.2023.11.19 val PER: 0.0539
2026-01-03 14:48:14,384: t15.2023.11.26 val PER: 0.1638
2026-01-03 14:48:14,384: t15.2023.12.03 val PER: 0.1397
2026-01-03 14:48:14,384: t15.2023.12.08 val PER: 0.1398
2026-01-03 14:48:14,384: t15.2023.12.10 val PER: 0.1235
2026-01-03 14:48:14,384: t15.2023.12.17 val PER: 0.1486
2026-01-03 14:48:14,384: t15.2023.12.29 val PER: 0.1592
2026-01-03 14:48:14,384: t15.2024.02.25 val PER: 0.1404
2026-01-03 14:48:14,384: t15.2024.03.08 val PER: 0.2603
2026-01-03 14:48:14,385: t15.2024.03.15 val PER: 0.2308
2026-01-03 14:48:14,385: t15.2024.03.17 val PER: 0.1646
2026-01-03 14:48:14,385: t15.2024.05.10 val PER: 0.1857
2026-01-03 14:48:14,385: t15.2024.06.14 val PER: 0.1782
2026-01-03 14:48:14,385: t15.2024.07.19 val PER: 0.2722
2026-01-03 14:48:14,385: t15.2024.07.21 val PER: 0.1234
2026-01-03 14:48:14,385: t15.2024.07.28 val PER: 0.1647
2026-01-03 14:48:14,385: t15.2025.01.10 val PER: 0.3361
2026-01-03 14:48:14,385: t15.2025.01.12 val PER: 0.1732
2026-01-03 14:48:14,385: t15.2025.03.14 val PER: 0.3417
2026-01-03 14:48:14,385: t15.2025.03.16 val PER: 0.2003
2026-01-03 14:48:14,385: t15.2025.03.30 val PER: 0.3253
2026-01-03 14:48:14,385: t15.2025.04.13 val PER: 0.2268
2026-01-03 14:48:14,651: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_14500
2026-01-03 14:48:23,403: Train batch 14600: loss: 17.48 grad norm: 63.28 time: 0.058
2026-01-03 14:48:41,823: Train batch 14800: loss: 8.31 grad norm: 46.38 time: 0.050
2026-01-03 14:48:59,465: Train batch 15000: loss: 12.06 grad norm: 52.50 time: 0.052
2026-01-03 14:48:59,466: Running test after training batch: 15000
2026-01-03 14:48:59,589: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:49:04,318: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:49:04,350: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 14:49:06,222: Val batch 15000: PER (avg): 0.1738 CTC Loss (avg): 17.0920 WER(1gram): 50.76% (n=64) time: 6.757
2026-01-03 14:49:06,223: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 14:49:06,223: t15.2023.08.13 val PER: 0.1310
2026-01-03 14:49:06,223: t15.2023.08.18 val PER: 0.1098
2026-01-03 14:49:06,223: t15.2023.08.20 val PER: 0.1326
2026-01-03 14:49:06,223: t15.2023.08.25 val PER: 0.1024
2026-01-03 14:49:06,223: t15.2023.08.27 val PER: 0.2203
2026-01-03 14:49:06,223: t15.2023.09.01 val PER: 0.0925
2026-01-03 14:49:06,223: t15.2023.09.03 val PER: 0.1817
2026-01-03 14:49:06,223: t15.2023.09.24 val PER: 0.1456
2026-01-03 14:49:06,223: t15.2023.09.29 val PER: 0.1378
2026-01-03 14:49:06,224: t15.2023.10.01 val PER: 0.1876
2026-01-03 14:49:06,224: t15.2023.10.06 val PER: 0.0990
2026-01-03 14:49:06,224: t15.2023.10.08 val PER: 0.2585
2026-01-03 14:49:06,224: t15.2023.10.13 val PER: 0.2250
2026-01-03 14:49:06,224: t15.2023.10.15 val PER: 0.1839
2026-01-03 14:49:06,224: t15.2023.10.20 val PER: 0.1846
2026-01-03 14:49:06,224: t15.2023.10.22 val PER: 0.1258
2026-01-03 14:49:06,224: t15.2023.11.03 val PER: 0.1954
2026-01-03 14:49:06,224: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:49:06,224: t15.2023.11.17 val PER: 0.0544
2026-01-03 14:49:06,224: t15.2023.11.19 val PER: 0.0519
2026-01-03 14:49:06,224: t15.2023.11.26 val PER: 0.1623
2026-01-03 14:49:06,224: t15.2023.12.03 val PER: 0.1387
2026-01-03 14:49:06,224: t15.2023.12.08 val PER: 0.1385
2026-01-03 14:49:06,224: t15.2023.12.10 val PER: 0.1261
2026-01-03 14:49:06,225: t15.2023.12.17 val PER: 0.1497
2026-01-03 14:49:06,225: t15.2023.12.29 val PER: 0.1579
2026-01-03 14:49:06,225: t15.2024.02.25 val PER: 0.1376
2026-01-03 14:49:06,225: t15.2024.03.08 val PER: 0.2632
2026-01-03 14:49:06,225: t15.2024.03.15 val PER: 0.2345
2026-01-03 14:49:06,225: t15.2024.03.17 val PER: 0.1674
2026-01-03 14:49:06,225: t15.2024.05.10 val PER: 0.1813
2026-01-03 14:49:06,225: t15.2024.06.14 val PER: 0.1814
2026-01-03 14:49:06,225: t15.2024.07.19 val PER: 0.2749
2026-01-03 14:49:06,225: t15.2024.07.21 val PER: 0.1214
2026-01-03 14:49:06,225: t15.2024.07.28 val PER: 0.1625
2026-01-03 14:49:06,225: t15.2025.01.10 val PER: 0.3347
2026-01-03 14:49:06,225: t15.2025.01.12 val PER: 0.1786
2026-01-03 14:49:06,225: t15.2025.03.14 val PER: 0.3521
2026-01-03 14:49:06,225: t15.2025.03.16 val PER: 0.2042
2026-01-03 14:49:06,225: t15.2025.03.30 val PER: 0.3264
2026-01-03 14:49:06,226: t15.2025.04.13 val PER: 0.2325
2026-01-03 14:49:06,490: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_15000
2026-01-03 14:49:24,352: Train batch 15200: loss: 7.00 grad norm: 39.77 time: 0.057
2026-01-03 14:49:41,908: Train batch 15400: loss: 16.34 grad norm: 61.20 time: 0.049
2026-01-03 14:49:50,967: Running test after training batch: 15500
2026-01-03 14:49:51,073: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:49:55,802: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:49:55,835: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-03 14:49:57,664: Val batch 15500: PER (avg): 0.1736 CTC Loss (avg): 17.0780 WER(1gram): 50.76% (n=64) time: 6.696
2026-01-03 14:49:57,664: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 14:49:57,665: t15.2023.08.13 val PER: 0.1331
2026-01-03 14:49:57,665: t15.2023.08.18 val PER: 0.1148
2026-01-03 14:49:57,665: t15.2023.08.20 val PER: 0.1319
2026-01-03 14:49:57,665: t15.2023.08.25 val PER: 0.1024
2026-01-03 14:49:57,665: t15.2023.08.27 val PER: 0.2154
2026-01-03 14:49:57,665: t15.2023.09.01 val PER: 0.0917
2026-01-03 14:49:57,665: t15.2023.09.03 val PER: 0.1841
2026-01-03 14:49:57,665: t15.2023.09.24 val PER: 0.1456
2026-01-03 14:49:57,665: t15.2023.09.29 val PER: 0.1398
2026-01-03 14:49:57,665: t15.2023.10.01 val PER: 0.1902
2026-01-03 14:49:57,665: t15.2023.10.06 val PER: 0.0958
2026-01-03 14:49:57,665: t15.2023.10.08 val PER: 0.2598
2026-01-03 14:49:57,665: t15.2023.10.13 val PER: 0.2289
2026-01-03 14:49:57,666: t15.2023.10.15 val PER: 0.1846
2026-01-03 14:49:57,666: t15.2023.10.20 val PER: 0.1846
2026-01-03 14:49:57,666: t15.2023.10.22 val PER: 0.1269
2026-01-03 14:49:57,666: t15.2023.11.03 val PER: 0.1947
2026-01-03 14:49:57,666: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:49:57,666: t15.2023.11.17 val PER: 0.0544
2026-01-03 14:49:57,666: t15.2023.11.19 val PER: 0.0519
2026-01-03 14:49:57,666: t15.2023.11.26 val PER: 0.1674
2026-01-03 14:49:57,666: t15.2023.12.03 val PER: 0.1345
2026-01-03 14:49:57,666: t15.2023.12.08 val PER: 0.1405
2026-01-03 14:49:57,666: t15.2023.12.10 val PER: 0.1235
2026-01-03 14:49:57,666: t15.2023.12.17 val PER: 0.1507
2026-01-03 14:49:57,666: t15.2023.12.29 val PER: 0.1585
2026-01-03 14:49:57,666: t15.2024.02.25 val PER: 0.1362
2026-01-03 14:49:57,666: t15.2024.03.08 val PER: 0.2617
2026-01-03 14:49:57,666: t15.2024.03.15 val PER: 0.2339
2026-01-03 14:49:57,666: t15.2024.03.17 val PER: 0.1646
2026-01-03 14:49:57,667: t15.2024.05.10 val PER: 0.1857
2026-01-03 14:49:57,667: t15.2024.06.14 val PER: 0.1814
2026-01-03 14:49:57,667: t15.2024.07.19 val PER: 0.2716
2026-01-03 14:49:57,667: t15.2024.07.21 val PER: 0.1221
2026-01-03 14:49:57,667: t15.2024.07.28 val PER: 0.1610
2026-01-03 14:49:57,667: t15.2025.01.10 val PER: 0.3320
2026-01-03 14:49:57,667: t15.2025.01.12 val PER: 0.1724
2026-01-03 14:49:57,667: t15.2025.03.14 val PER: 0.3447
2026-01-03 14:49:57,667: t15.2025.03.16 val PER: 0.2016
2026-01-03 14:49:57,667: t15.2025.03.30 val PER: 0.3276
2026-01-03 14:49:57,667: t15.2025.04.13 val PER: 0.2282
2026-01-03 14:49:57,930: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_15500
2026-01-03 14:50:06,767: Train batch 15600: loss: 17.63 grad norm: 59.52 time: 0.062
2026-01-03 14:50:24,368: Train batch 15800: loss: 19.47 grad norm: 67.31 time: 0.067
2026-01-03 14:50:41,866: Train batch 16000: loss: 12.16 grad norm: 47.66 time: 0.055
2026-01-03 14:50:41,866: Running test after training batch: 16000
2026-01-03 14:50:41,966: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:50:46,714: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 14:50:46,747: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 14:50:48,680: Val batch 16000: PER (avg): 0.1737 CTC Loss (avg): 17.1254 WER(1gram): 51.02% (n=64) time: 6.814
2026-01-03 14:50:48,681: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 14:50:48,681: t15.2023.08.13 val PER: 0.1310
2026-01-03 14:50:48,681: t15.2023.08.18 val PER: 0.1174
2026-01-03 14:50:48,681: t15.2023.08.20 val PER: 0.1311
2026-01-03 14:50:48,681: t15.2023.08.25 val PER: 0.1009
2026-01-03 14:50:48,681: t15.2023.08.27 val PER: 0.2186
2026-01-03 14:50:48,681: t15.2023.09.01 val PER: 0.0901
2026-01-03 14:50:48,681: t15.2023.09.03 val PER: 0.1829
2026-01-03 14:50:48,681: t15.2023.09.24 val PER: 0.1456
2026-01-03 14:50:48,682: t15.2023.09.29 val PER: 0.1385
2026-01-03 14:50:48,682: t15.2023.10.01 val PER: 0.1882
2026-01-03 14:50:48,682: t15.2023.10.06 val PER: 0.1012
2026-01-03 14:50:48,682: t15.2023.10.08 val PER: 0.2585
2026-01-03 14:50:48,682: t15.2023.10.13 val PER: 0.2281
2026-01-03 14:50:48,682: t15.2023.10.15 val PER: 0.1813
2026-01-03 14:50:48,682: t15.2023.10.20 val PER: 0.1812
2026-01-03 14:50:48,682: t15.2023.10.22 val PER: 0.1258
2026-01-03 14:50:48,682: t15.2023.11.03 val PER: 0.1988
2026-01-03 14:50:48,682: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:50:48,682: t15.2023.11.17 val PER: 0.0544
2026-01-03 14:50:48,683: t15.2023.11.19 val PER: 0.0519
2026-01-03 14:50:48,683: t15.2023.11.26 val PER: 0.1659
2026-01-03 14:50:48,683: t15.2023.12.03 val PER: 0.1334
2026-01-03 14:50:48,683: t15.2023.12.08 val PER: 0.1385
2026-01-03 14:50:48,683: t15.2023.12.10 val PER: 0.1235
2026-01-03 14:50:48,683: t15.2023.12.17 val PER: 0.1528
2026-01-03 14:50:48,683: t15.2023.12.29 val PER: 0.1633
2026-01-03 14:50:48,684: t15.2024.02.25 val PER: 0.1376
2026-01-03 14:50:48,684: t15.2024.03.08 val PER: 0.2589
2026-01-03 14:50:48,684: t15.2024.03.15 val PER: 0.2326
2026-01-03 14:50:48,684: t15.2024.03.17 val PER: 0.1646
2026-01-03 14:50:48,684: t15.2024.05.10 val PER: 0.1857
2026-01-03 14:50:48,684: t15.2024.06.14 val PER: 0.1830
2026-01-03 14:50:48,685: t15.2024.07.19 val PER: 0.2722
2026-01-03 14:50:48,685: t15.2024.07.21 val PER: 0.1214
2026-01-03 14:50:48,685: t15.2024.07.28 val PER: 0.1618
2026-01-03 14:50:48,685: t15.2025.01.10 val PER: 0.3361
2026-01-03 14:50:48,685: t15.2025.01.12 val PER: 0.1755
2026-01-03 14:50:48,685: t15.2025.03.14 val PER: 0.3476
2026-01-03 14:50:48,685: t15.2025.03.16 val PER: 0.2016
2026-01-03 14:50:48,685: t15.2025.03.30 val PER: 0.3253
2026-01-03 14:50:48,685: t15.2025.04.13 val PER: 0.2297
2026-01-03 14:50:48,954: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_16000
2026-01-03 14:51:06,671: Train batch 16200: loss: 10.28 grad norm: 48.17 time: 0.055
2026-01-03 14:51:24,397: Train batch 16400: loss: 14.56 grad norm: 64.60 time: 0.058
2026-01-03 14:51:33,321: Running test after training batch: 16500
2026-01-03 14:51:33,410: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:51:38,560: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:51:38,593: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 14:51:40,496: Val batch 16500: PER (avg): 0.1734 CTC Loss (avg): 17.1018 WER(1gram): 50.51% (n=64) time: 7.175
2026-01-03 14:51:40,496: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 14:51:40,497: t15.2023.08.13 val PER: 0.1279
2026-01-03 14:51:40,497: t15.2023.08.18 val PER: 0.1165
2026-01-03 14:51:40,497: t15.2023.08.20 val PER: 0.1319
2026-01-03 14:51:40,497: t15.2023.08.25 val PER: 0.1009
2026-01-03 14:51:40,497: t15.2023.08.27 val PER: 0.2154
2026-01-03 14:51:40,497: t15.2023.09.01 val PER: 0.0901
2026-01-03 14:51:40,497: t15.2023.09.03 val PER: 0.1841
2026-01-03 14:51:40,497: t15.2023.09.24 val PER: 0.1444
2026-01-03 14:51:40,498: t15.2023.09.29 val PER: 0.1378
2026-01-03 14:51:40,498: t15.2023.10.01 val PER: 0.1876
2026-01-03 14:51:40,498: t15.2023.10.06 val PER: 0.1023
2026-01-03 14:51:40,498: t15.2023.10.08 val PER: 0.2598
2026-01-03 14:51:40,498: t15.2023.10.13 val PER: 0.2250
2026-01-03 14:51:40,499: t15.2023.10.15 val PER: 0.1826
2026-01-03 14:51:40,499: t15.2023.10.20 val PER: 0.1846
2026-01-03 14:51:40,499: t15.2023.10.22 val PER: 0.1269
2026-01-03 14:51:40,499: t15.2023.11.03 val PER: 0.2015
2026-01-03 14:51:40,499: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:51:40,499: t15.2023.11.17 val PER: 0.0544
2026-01-03 14:51:40,499: t15.2023.11.19 val PER: 0.0519
2026-01-03 14:51:40,499: t15.2023.11.26 val PER: 0.1645
2026-01-03 14:51:40,500: t15.2023.12.03 val PER: 0.1355
2026-01-03 14:51:40,500: t15.2023.12.08 val PER: 0.1391
2026-01-03 14:51:40,500: t15.2023.12.10 val PER: 0.1235
2026-01-03 14:51:40,500: t15.2023.12.17 val PER: 0.1497
2026-01-03 14:51:40,500: t15.2023.12.29 val PER: 0.1592
2026-01-03 14:51:40,500: t15.2024.02.25 val PER: 0.1376
2026-01-03 14:51:40,500: t15.2024.03.08 val PER: 0.2617
2026-01-03 14:51:40,500: t15.2024.03.15 val PER: 0.2364
2026-01-03 14:51:40,500: t15.2024.03.17 val PER: 0.1625
2026-01-03 14:51:40,500: t15.2024.05.10 val PER: 0.1813
2026-01-03 14:51:40,501: t15.2024.06.14 val PER: 0.1814
2026-01-03 14:51:40,501: t15.2024.07.19 val PER: 0.2709
2026-01-03 14:51:40,501: t15.2024.07.21 val PER: 0.1221
2026-01-03 14:51:40,501: t15.2024.07.28 val PER: 0.1625
2026-01-03 14:51:40,501: t15.2025.01.10 val PER: 0.3333
2026-01-03 14:51:40,501: t15.2025.01.12 val PER: 0.1717
2026-01-03 14:51:40,501: t15.2025.03.14 val PER: 0.3462
2026-01-03 14:51:40,501: t15.2025.03.16 val PER: 0.1990
2026-01-03 14:51:40,501: t15.2025.03.30 val PER: 0.3299
2026-01-03 14:51:40,501: t15.2025.04.13 val PER: 0.2297
2026-01-03 14:51:40,766: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_16500
2026-01-03 14:51:49,682: Train batch 16600: loss: 11.68 grad norm: 51.89 time: 0.052
2026-01-03 14:52:08,240: Train batch 16800: loss: 22.91 grad norm: 76.09 time: 0.062
2026-01-03 14:52:26,333: Train batch 17000: loss: 11.11 grad norm: 49.00 time: 0.081
2026-01-03 14:52:26,334: Running test after training batch: 17000
2026-01-03 14:52:26,431: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:52:31,393: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:52:31,427: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 14:52:33,364: Val batch 17000: PER (avg): 0.1738 CTC Loss (avg): 17.0827 WER(1gram): 50.51% (n=64) time: 7.030
2026-01-03 14:52:33,364: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 14:52:33,364: t15.2023.08.13 val PER: 0.1331
2026-01-03 14:52:33,364: t15.2023.08.18 val PER: 0.1132
2026-01-03 14:52:33,364: t15.2023.08.20 val PER: 0.1319
2026-01-03 14:52:33,364: t15.2023.08.25 val PER: 0.0979
2026-01-03 14:52:33,364: t15.2023.08.27 val PER: 0.2138
2026-01-03 14:52:33,364: t15.2023.09.01 val PER: 0.0893
2026-01-03 14:52:33,364: t15.2023.09.03 val PER: 0.1865
2026-01-03 14:52:33,365: t15.2023.09.24 val PER: 0.1444
2026-01-03 14:52:33,365: t15.2023.09.29 val PER: 0.1385
2026-01-03 14:52:33,365: t15.2023.10.01 val PER: 0.1869
2026-01-03 14:52:33,365: t15.2023.10.06 val PER: 0.0980
2026-01-03 14:52:33,365: t15.2023.10.08 val PER: 0.2625
2026-01-03 14:52:33,365: t15.2023.10.13 val PER: 0.2258
2026-01-03 14:52:33,365: t15.2023.10.15 val PER: 0.1786
2026-01-03 14:52:33,365: t15.2023.10.20 val PER: 0.1812
2026-01-03 14:52:33,365: t15.2023.10.22 val PER: 0.1281
2026-01-03 14:52:33,365: t15.2023.11.03 val PER: 0.1995
2026-01-03 14:52:33,365: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:52:33,365: t15.2023.11.17 val PER: 0.0544
2026-01-03 14:52:33,365: t15.2023.11.19 val PER: 0.0539
2026-01-03 14:52:33,365: t15.2023.11.26 val PER: 0.1630
2026-01-03 14:52:33,366: t15.2023.12.03 val PER: 0.1408
2026-01-03 14:52:33,366: t15.2023.12.08 val PER: 0.1398
2026-01-03 14:52:33,366: t15.2023.12.10 val PER: 0.1235
2026-01-03 14:52:33,366: t15.2023.12.17 val PER: 0.1497
2026-01-03 14:52:33,366: t15.2023.12.29 val PER: 0.1620
2026-01-03 14:52:33,366: t15.2024.02.25 val PER: 0.1362
2026-01-03 14:52:33,366: t15.2024.03.08 val PER: 0.2646
2026-01-03 14:52:33,366: t15.2024.03.15 val PER: 0.2345
2026-01-03 14:52:33,366: t15.2024.03.17 val PER: 0.1674
2026-01-03 14:52:33,366: t15.2024.05.10 val PER: 0.1828
2026-01-03 14:52:33,367: t15.2024.06.14 val PER: 0.1830
2026-01-03 14:52:33,367: t15.2024.07.19 val PER: 0.2709
2026-01-03 14:52:33,367: t15.2024.07.21 val PER: 0.1241
2026-01-03 14:52:33,367: t15.2024.07.28 val PER: 0.1625
2026-01-03 14:52:33,367: t15.2025.01.10 val PER: 0.3347
2026-01-03 14:52:33,367: t15.2025.01.12 val PER: 0.1724
2026-01-03 14:52:33,367: t15.2025.03.14 val PER: 0.3491
2026-01-03 14:52:33,367: t15.2025.03.16 val PER: 0.2055
2026-01-03 14:52:33,367: t15.2025.03.30 val PER: 0.3264
2026-01-03 14:52:33,367: t15.2025.04.13 val PER: 0.2354
2026-01-03 14:52:33,638: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_17000
2026-01-03 14:52:51,381: Train batch 17200: loss: 13.70 grad norm: 51.77 time: 0.084
2026-01-03 14:53:08,811: Train batch 17400: loss: 16.91 grad norm: 61.28 time: 0.071
2026-01-03 14:53:17,561: Running test after training batch: 17500
2026-01-03 14:53:17,713: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:53:22,675: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:53:22,710: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-03 14:53:24,660: Val batch 17500: PER (avg): 0.1733 CTC Loss (avg): 17.0913 WER(1gram): 51.02% (n=64) time: 7.099
2026-01-03 14:53:24,661: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 14:53:24,661: t15.2023.08.13 val PER: 0.1341
2026-01-03 14:53:24,661: t15.2023.08.18 val PER: 0.1157
2026-01-03 14:53:24,661: t15.2023.08.20 val PER: 0.1326
2026-01-03 14:53:24,661: t15.2023.08.25 val PER: 0.0994
2026-01-03 14:53:24,661: t15.2023.08.27 val PER: 0.2122
2026-01-03 14:53:24,661: t15.2023.09.01 val PER: 0.0885
2026-01-03 14:53:24,661: t15.2023.09.03 val PER: 0.1841
2026-01-03 14:53:24,661: t15.2023.09.24 val PER: 0.1444
2026-01-03 14:53:24,662: t15.2023.09.29 val PER: 0.1429
2026-01-03 14:53:24,662: t15.2023.10.01 val PER: 0.1856
2026-01-03 14:53:24,662: t15.2023.10.06 val PER: 0.1033
2026-01-03 14:53:24,662: t15.2023.10.08 val PER: 0.2625
2026-01-03 14:53:24,662: t15.2023.10.13 val PER: 0.2250
2026-01-03 14:53:24,662: t15.2023.10.15 val PER: 0.1773
2026-01-03 14:53:24,662: t15.2023.10.20 val PER: 0.1779
2026-01-03 14:53:24,662: t15.2023.10.22 val PER: 0.1258
2026-01-03 14:53:24,662: t15.2023.11.03 val PER: 0.1974
2026-01-03 14:53:24,662: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:53:24,662: t15.2023.11.17 val PER: 0.0560
2026-01-03 14:53:24,662: t15.2023.11.19 val PER: 0.0559
2026-01-03 14:53:24,662: t15.2023.11.26 val PER: 0.1616
2026-01-03 14:53:24,662: t15.2023.12.03 val PER: 0.1408
2026-01-03 14:53:24,662: t15.2023.12.08 val PER: 0.1391
2026-01-03 14:53:24,662: t15.2023.12.10 val PER: 0.1130
2026-01-03 14:53:24,662: t15.2023.12.17 val PER: 0.1559
2026-01-03 14:53:24,663: t15.2023.12.29 val PER: 0.1627
2026-01-03 14:53:24,663: t15.2024.02.25 val PER: 0.1348
2026-01-03 14:53:24,663: t15.2024.03.08 val PER: 0.2575
2026-01-03 14:53:24,663: t15.2024.03.15 val PER: 0.2301
2026-01-03 14:53:24,663: t15.2024.03.17 val PER: 0.1667
2026-01-03 14:53:24,663: t15.2024.05.10 val PER: 0.1842
2026-01-03 14:53:24,663: t15.2024.06.14 val PER: 0.1814
2026-01-03 14:53:24,663: t15.2024.07.19 val PER: 0.2709
2026-01-03 14:53:24,663: t15.2024.07.21 val PER: 0.1207
2026-01-03 14:53:24,663: t15.2024.07.28 val PER: 0.1596
2026-01-03 14:53:24,663: t15.2025.01.10 val PER: 0.3347
2026-01-03 14:53:24,663: t15.2025.01.12 val PER: 0.1717
2026-01-03 14:53:24,663: t15.2025.03.14 val PER: 0.3506
2026-01-03 14:53:24,663: t15.2025.03.16 val PER: 0.2016
2026-01-03 14:53:24,663: t15.2025.03.30 val PER: 0.3276
2026-01-03 14:53:24,663: t15.2025.04.13 val PER: 0.2368
2026-01-03 14:53:24,936: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_17500
2026-01-03 14:53:33,730: Train batch 17600: loss: 15.92 grad norm: 64.25 time: 0.051
2026-01-03 14:53:51,540: Train batch 17800: loss: 9.28 grad norm: 50.67 time: 0.042
2026-01-03 14:54:09,295: Train batch 18000: loss: 16.74 grad norm: 64.81 time: 0.060
2026-01-03 14:54:09,296: Running test after training batch: 18000
2026-01-03 14:54:09,438: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:54:14,162: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:54:14,196: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 14:54:16,081: Val batch 18000: PER (avg): 0.1735 CTC Loss (avg): 17.0677 WER(1gram): 50.76% (n=64) time: 6.785
2026-01-03 14:54:16,081: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 14:54:16,081: t15.2023.08.13 val PER: 0.1320
2026-01-03 14:54:16,081: t15.2023.08.18 val PER: 0.1140
2026-01-03 14:54:16,081: t15.2023.08.20 val PER: 0.1342
2026-01-03 14:54:16,081: t15.2023.08.25 val PER: 0.0979
2026-01-03 14:54:16,081: t15.2023.08.27 val PER: 0.2106
2026-01-03 14:54:16,081: t15.2023.09.01 val PER: 0.0901
2026-01-03 14:54:16,082: t15.2023.09.03 val PER: 0.1817
2026-01-03 14:54:16,082: t15.2023.09.24 val PER: 0.1444
2026-01-03 14:54:16,082: t15.2023.09.29 val PER: 0.1398
2026-01-03 14:54:16,082: t15.2023.10.01 val PER: 0.1863
2026-01-03 14:54:16,082: t15.2023.10.06 val PER: 0.1012
2026-01-03 14:54:16,082: t15.2023.10.08 val PER: 0.2625
2026-01-03 14:54:16,082: t15.2023.10.13 val PER: 0.2258
2026-01-03 14:54:16,082: t15.2023.10.15 val PER: 0.1786
2026-01-03 14:54:16,082: t15.2023.10.20 val PER: 0.1846
2026-01-03 14:54:16,082: t15.2023.10.22 val PER: 0.1281
2026-01-03 14:54:16,082: t15.2023.11.03 val PER: 0.1961
2026-01-03 14:54:16,082: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:54:16,082: t15.2023.11.17 val PER: 0.0544
2026-01-03 14:54:16,082: t15.2023.11.19 val PER: 0.0559
2026-01-03 14:54:16,082: t15.2023.11.26 val PER: 0.1667
2026-01-03 14:54:16,083: t15.2023.12.03 val PER: 0.1418
2026-01-03 14:54:16,083: t15.2023.12.08 val PER: 0.1411
2026-01-03 14:54:16,083: t15.2023.12.10 val PER: 0.1196
2026-01-03 14:54:16,083: t15.2023.12.17 val PER: 0.1549
2026-01-03 14:54:16,083: t15.2023.12.29 val PER: 0.1620
2026-01-03 14:54:16,083: t15.2024.02.25 val PER: 0.1376
2026-01-03 14:54:16,083: t15.2024.03.08 val PER: 0.2632
2026-01-03 14:54:16,083: t15.2024.03.15 val PER: 0.2333
2026-01-03 14:54:16,083: t15.2024.03.17 val PER: 0.1660
2026-01-03 14:54:16,083: t15.2024.05.10 val PER: 0.1842
2026-01-03 14:54:16,083: t15.2024.06.14 val PER: 0.1814
2026-01-03 14:54:16,083: t15.2024.07.19 val PER: 0.2690
2026-01-03 14:54:16,083: t15.2024.07.21 val PER: 0.1221
2026-01-03 14:54:16,083: t15.2024.07.28 val PER: 0.1618
2026-01-03 14:54:16,083: t15.2025.01.10 val PER: 0.3320
2026-01-03 14:54:16,083: t15.2025.01.12 val PER: 0.1724
2026-01-03 14:54:16,083: t15.2025.03.14 val PER: 0.3417
2026-01-03 14:54:16,083: t15.2025.03.16 val PER: 0.1990
2026-01-03 14:54:16,084: t15.2025.03.30 val PER: 0.3253
2026-01-03 14:54:16,084: t15.2025.04.13 val PER: 0.2354
2026-01-03 14:54:17,222: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_18000
2026-01-03 14:54:41,767: Train batch 18200: loss: 12.52 grad norm: 54.95 time: 0.073
2026-01-03 14:55:00,022: Train batch 18400: loss: 9.44 grad norm: 51.23 time: 0.059
2026-01-03 14:55:08,722: Running test after training batch: 18500
2026-01-03 14:55:08,863: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:55:14,095: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:55:14,129: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 14:55:16,073: Val batch 18500: PER (avg): 0.1733 CTC Loss (avg): 17.0697 WER(1gram): 50.76% (n=64) time: 7.350
2026-01-03 14:55:16,073: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 14:55:16,073: t15.2023.08.13 val PER: 0.1310
2026-01-03 14:55:16,073: t15.2023.08.18 val PER: 0.1148
2026-01-03 14:55:16,074: t15.2023.08.20 val PER: 0.1342
2026-01-03 14:55:16,074: t15.2023.08.25 val PER: 0.0994
2026-01-03 14:55:16,074: t15.2023.08.27 val PER: 0.2106
2026-01-03 14:55:16,075: t15.2023.09.01 val PER: 0.0885
2026-01-03 14:55:16,075: t15.2023.09.03 val PER: 0.1876
2026-01-03 14:55:16,075: t15.2023.09.24 val PER: 0.1456
2026-01-03 14:55:16,075: t15.2023.09.29 val PER: 0.1404
2026-01-03 14:55:16,075: t15.2023.10.01 val PER: 0.1849
2026-01-03 14:55:16,075: t15.2023.10.06 val PER: 0.1001
2026-01-03 14:55:16,075: t15.2023.10.08 val PER: 0.2639
2026-01-03 14:55:16,075: t15.2023.10.13 val PER: 0.2250
2026-01-03 14:55:16,076: t15.2023.10.15 val PER: 0.1793
2026-01-03 14:55:16,076: t15.2023.10.20 val PER: 0.1779
2026-01-03 14:55:16,076: t15.2023.10.22 val PER: 0.1292
2026-01-03 14:55:16,076: t15.2023.11.03 val PER: 0.1947
2026-01-03 14:55:16,076: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:55:16,076: t15.2023.11.17 val PER: 0.0560
2026-01-03 14:55:16,076: t15.2023.11.19 val PER: 0.0559
2026-01-03 14:55:16,076: t15.2023.11.26 val PER: 0.1652
2026-01-03 14:55:16,076: t15.2023.12.03 val PER: 0.1366
2026-01-03 14:55:16,076: t15.2023.12.08 val PER: 0.1378
2026-01-03 14:55:16,076: t15.2023.12.10 val PER: 0.1183
2026-01-03 14:55:16,076: t15.2023.12.17 val PER: 0.1528
2026-01-03 14:55:16,077: t15.2023.12.29 val PER: 0.1627
2026-01-03 14:55:16,077: t15.2024.02.25 val PER: 0.1334
2026-01-03 14:55:16,077: t15.2024.03.08 val PER: 0.2674
2026-01-03 14:55:16,077: t15.2024.03.15 val PER: 0.2314
2026-01-03 14:55:16,077: t15.2024.03.17 val PER: 0.1674
2026-01-03 14:55:16,077: t15.2024.05.10 val PER: 0.1872
2026-01-03 14:55:16,077: t15.2024.06.14 val PER: 0.1830
2026-01-03 14:55:16,077: t15.2024.07.19 val PER: 0.2703
2026-01-03 14:55:16,077: t15.2024.07.21 val PER: 0.1228
2026-01-03 14:55:16,077: t15.2024.07.28 val PER: 0.1596
2026-01-03 14:55:16,077: t15.2025.01.10 val PER: 0.3361
2026-01-03 14:55:16,078: t15.2025.01.12 val PER: 0.1724
2026-01-03 14:55:16,078: t15.2025.03.14 val PER: 0.3447
2026-01-03 14:55:16,078: t15.2025.03.16 val PER: 0.2003
2026-01-03 14:55:16,078: t15.2025.03.30 val PER: 0.3230
2026-01-03 14:55:16,078: t15.2025.04.13 val PER: 0.2340
2026-01-03 14:55:16,370: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_18500
2026-01-03 14:55:25,997: Train batch 18600: loss: 17.70 grad norm: 67.03 time: 0.067
2026-01-03 14:55:44,588: Train batch 18800: loss: 13.87 grad norm: 58.52 time: 0.064
2026-01-03 14:56:02,620: Train batch 19000: loss: 11.62 grad norm: 44.06 time: 0.064
2026-01-03 14:56:02,620: Running test after training batch: 19000
2026-01-03 14:56:02,765: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:56:07,528: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:56:07,562: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 14:56:09,466: Val batch 19000: PER (avg): 0.1738 CTC Loss (avg): 17.0577 WER(1gram): 50.51% (n=64) time: 6.846
2026-01-03 14:56:09,467: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 14:56:09,467: t15.2023.08.13 val PER: 0.1320
2026-01-03 14:56:09,467: t15.2023.08.18 val PER: 0.1165
2026-01-03 14:56:09,467: t15.2023.08.20 val PER: 0.1334
2026-01-03 14:56:09,467: t15.2023.08.25 val PER: 0.0979
2026-01-03 14:56:09,467: t15.2023.08.27 val PER: 0.2122
2026-01-03 14:56:09,467: t15.2023.09.01 val PER: 0.0901
2026-01-03 14:56:09,468: t15.2023.09.03 val PER: 0.1853
2026-01-03 14:56:09,468: t15.2023.09.24 val PER: 0.1444
2026-01-03 14:56:09,468: t15.2023.09.29 val PER: 0.1404
2026-01-03 14:56:09,468: t15.2023.10.01 val PER: 0.1856
2026-01-03 14:56:09,468: t15.2023.10.06 val PER: 0.1001
2026-01-03 14:56:09,468: t15.2023.10.08 val PER: 0.2625
2026-01-03 14:56:09,468: t15.2023.10.13 val PER: 0.2234
2026-01-03 14:56:09,468: t15.2023.10.15 val PER: 0.1800
2026-01-03 14:56:09,468: t15.2023.10.20 val PER: 0.1779
2026-01-03 14:56:09,468: t15.2023.10.22 val PER: 0.1281
2026-01-03 14:56:09,468: t15.2023.11.03 val PER: 0.1981
2026-01-03 14:56:09,469: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:56:09,469: t15.2023.11.17 val PER: 0.0544
2026-01-03 14:56:09,469: t15.2023.11.19 val PER: 0.0559
2026-01-03 14:56:09,469: t15.2023.11.26 val PER: 0.1674
2026-01-03 14:56:09,469: t15.2023.12.03 val PER: 0.1387
2026-01-03 14:56:09,469: t15.2023.12.08 val PER: 0.1391
2026-01-03 14:56:09,469: t15.2023.12.10 val PER: 0.1222
2026-01-03 14:56:09,469: t15.2023.12.17 val PER: 0.1538
2026-01-03 14:56:09,469: t15.2023.12.29 val PER: 0.1627
2026-01-03 14:56:09,469: t15.2024.02.25 val PER: 0.1334
2026-01-03 14:56:09,469: t15.2024.03.08 val PER: 0.2674
2026-01-03 14:56:09,469: t15.2024.03.15 val PER: 0.2351
2026-01-03 14:56:09,469: t15.2024.03.17 val PER: 0.1667
2026-01-03 14:56:09,469: t15.2024.05.10 val PER: 0.1842
2026-01-03 14:56:09,470: t15.2024.06.14 val PER: 0.1861
2026-01-03 14:56:09,470: t15.2024.07.19 val PER: 0.2716
2026-01-03 14:56:09,470: t15.2024.07.21 val PER: 0.1234
2026-01-03 14:56:09,470: t15.2024.07.28 val PER: 0.1610
2026-01-03 14:56:09,470: t15.2025.01.10 val PER: 0.3375
2026-01-03 14:56:09,470: t15.2025.01.12 val PER: 0.1694
2026-01-03 14:56:09,470: t15.2025.03.14 val PER: 0.3476
2026-01-03 14:56:09,470: t15.2025.03.16 val PER: 0.1990
2026-01-03 14:56:09,470: t15.2025.03.30 val PER: 0.3230
2026-01-03 14:56:09,470: t15.2025.04.13 val PER: 0.2340
2026-01-03 14:56:09,751: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_19000
2026-01-03 14:56:27,966: Train batch 19200: loss: 9.22 grad norm: 50.48 time: 0.063
2026-01-03 14:56:45,646: Train batch 19400: loss: 8.05 grad norm: 40.95 time: 0.052
2026-01-03 14:56:54,981: Running test after training batch: 19500
2026-01-03 14:56:55,124: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:57:00,490: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:57:00,527: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 14:57:02,478: Val batch 19500: PER (avg): 0.1737 CTC Loss (avg): 17.0557 WER(1gram): 50.51% (n=64) time: 7.496
2026-01-03 14:57:02,478: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 14:57:02,479: t15.2023.08.13 val PER: 0.1320
2026-01-03 14:57:02,479: t15.2023.08.18 val PER: 0.1123
2026-01-03 14:57:02,479: t15.2023.08.20 val PER: 0.1326
2026-01-03 14:57:02,479: t15.2023.08.25 val PER: 0.0979
2026-01-03 14:57:02,479: t15.2023.08.27 val PER: 0.2122
2026-01-03 14:57:02,479: t15.2023.09.01 val PER: 0.0893
2026-01-03 14:57:02,479: t15.2023.09.03 val PER: 0.1853
2026-01-03 14:57:02,479: t15.2023.09.24 val PER: 0.1444
2026-01-03 14:57:02,479: t15.2023.09.29 val PER: 0.1391
2026-01-03 14:57:02,479: t15.2023.10.01 val PER: 0.1876
2026-01-03 14:57:02,479: t15.2023.10.06 val PER: 0.1001
2026-01-03 14:57:02,479: t15.2023.10.08 val PER: 0.2598
2026-01-03 14:57:02,480: t15.2023.10.13 val PER: 0.2265
2026-01-03 14:57:02,480: t15.2023.10.15 val PER: 0.1800
2026-01-03 14:57:02,480: t15.2023.10.20 val PER: 0.1846
2026-01-03 14:57:02,480: t15.2023.10.22 val PER: 0.1281
2026-01-03 14:57:02,480: t15.2023.11.03 val PER: 0.1961
2026-01-03 14:57:02,480: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:57:02,480: t15.2023.11.17 val PER: 0.0544
2026-01-03 14:57:02,480: t15.2023.11.19 val PER: 0.0539
2026-01-03 14:57:02,480: t15.2023.11.26 val PER: 0.1696
2026-01-03 14:57:02,480: t15.2023.12.03 val PER: 0.1355
2026-01-03 14:57:02,480: t15.2023.12.08 val PER: 0.1385
2026-01-03 14:57:02,480: t15.2023.12.10 val PER: 0.1209
2026-01-03 14:57:02,480: t15.2023.12.17 val PER: 0.1528
2026-01-03 14:57:02,480: t15.2023.12.29 val PER: 0.1640
2026-01-03 14:57:02,480: t15.2024.02.25 val PER: 0.1348
2026-01-03 14:57:02,480: t15.2024.03.08 val PER: 0.2646
2026-01-03 14:57:02,481: t15.2024.03.15 val PER: 0.2339
2026-01-03 14:57:02,481: t15.2024.03.17 val PER: 0.1632
2026-01-03 14:57:02,481: t15.2024.05.10 val PER: 0.1842
2026-01-03 14:57:02,481: t15.2024.06.14 val PER: 0.1830
2026-01-03 14:57:02,481: t15.2024.07.19 val PER: 0.2722
2026-01-03 14:57:02,481: t15.2024.07.21 val PER: 0.1228
2026-01-03 14:57:02,481: t15.2024.07.28 val PER: 0.1632
2026-01-03 14:57:02,481: t15.2025.01.10 val PER: 0.3361
2026-01-03 14:57:02,481: t15.2025.01.12 val PER: 0.1717
2026-01-03 14:57:02,481: t15.2025.03.14 val PER: 0.3462
2026-01-03 14:57:02,482: t15.2025.03.16 val PER: 0.2029
2026-01-03 14:57:02,482: t15.2025.03.30 val PER: 0.3253
2026-01-03 14:57:02,482: t15.2025.04.13 val PER: 0.2354
2026-01-03 14:57:02,896: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_19500
2026-01-03 14:57:11,457: Train batch 19600: loss: 11.73 grad norm: 52.88 time: 0.057
2026-01-03 14:57:28,657: Train batch 19800: loss: 11.40 grad norm: 52.15 time: 0.055
2026-01-03 14:57:45,910: Running test after training batch: 19999
2026-01-03 14:57:45,998: WER debug GT example: You can see the code at this point as well.
2026-01-03 14:57:51,644: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 14:57:51,679: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-03 14:57:53,639: Val batch 19999: PER (avg): 0.1734 CTC Loss (avg): 17.0555 WER(1gram): 51.02% (n=64) time: 7.729
2026-01-03 14:57:53,640: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 14:57:53,640: t15.2023.08.13 val PER: 0.1320
2026-01-03 14:57:53,640: t15.2023.08.18 val PER: 0.1115
2026-01-03 14:57:53,640: t15.2023.08.20 val PER: 0.1334
2026-01-03 14:57:53,640: t15.2023.08.25 val PER: 0.0994
2026-01-03 14:57:53,640: t15.2023.08.27 val PER: 0.2122
2026-01-03 14:57:53,640: t15.2023.09.01 val PER: 0.0901
2026-01-03 14:57:53,640: t15.2023.09.03 val PER: 0.1829
2026-01-03 14:57:53,641: t15.2023.09.24 val PER: 0.1456
2026-01-03 14:57:53,641: t15.2023.09.29 val PER: 0.1417
2026-01-03 14:57:53,641: t15.2023.10.01 val PER: 0.1849
2026-01-03 14:57:53,641: t15.2023.10.06 val PER: 0.1001
2026-01-03 14:57:53,641: t15.2023.10.08 val PER: 0.2639
2026-01-03 14:57:53,641: t15.2023.10.13 val PER: 0.2258
2026-01-03 14:57:53,641: t15.2023.10.15 val PER: 0.1800
2026-01-03 14:57:53,641: t15.2023.10.20 val PER: 0.1846
2026-01-03 14:57:53,641: t15.2023.10.22 val PER: 0.1269
2026-01-03 14:57:53,641: t15.2023.11.03 val PER: 0.1981
2026-01-03 14:57:53,641: t15.2023.11.04 val PER: 0.0307
2026-01-03 14:57:53,641: t15.2023.11.17 val PER: 0.0544
2026-01-03 14:57:53,641: t15.2023.11.19 val PER: 0.0539
2026-01-03 14:57:53,641: t15.2023.11.26 val PER: 0.1659
2026-01-03 14:57:53,642: t15.2023.12.03 val PER: 0.1355
2026-01-03 14:57:53,642: t15.2023.12.08 val PER: 0.1391
2026-01-03 14:57:53,642: t15.2023.12.10 val PER: 0.1196
2026-01-03 14:57:53,642: t15.2023.12.17 val PER: 0.1507
2026-01-03 14:57:53,642: t15.2023.12.29 val PER: 0.1640
2026-01-03 14:57:53,642: t15.2024.02.25 val PER: 0.1348
2026-01-03 14:57:53,642: t15.2024.03.08 val PER: 0.2703
2026-01-03 14:57:53,642: t15.2024.03.15 val PER: 0.2320
2026-01-03 14:57:53,642: t15.2024.03.17 val PER: 0.1653
2026-01-03 14:57:53,642: t15.2024.05.10 val PER: 0.1872
2026-01-03 14:57:53,642: t15.2024.06.14 val PER: 0.1845
2026-01-03 14:57:53,642: t15.2024.07.19 val PER: 0.2709
2026-01-03 14:57:53,642: t15.2024.07.21 val PER: 0.1228
2026-01-03 14:57:53,642: t15.2024.07.28 val PER: 0.1603
2026-01-03 14:57:53,642: t15.2025.01.10 val PER: 0.3375
2026-01-03 14:57:53,642: t15.2025.01.12 val PER: 0.1694
2026-01-03 14:57:53,643: t15.2025.03.14 val PER: 0.3476
2026-01-03 14:57:53,643: t15.2025.03.16 val PER: 0.2003
2026-01-03 14:57:53,643: t15.2025.03.30 val PER: 0.3230
2026-01-03 14:57:53,643: t15.2025.04.13 val PER: 0.2311
2026-01-03 14:57:53,927: Saved model to checkpoint: trained_models/lr_stepdrop/lr40/step8k_f01/checkpoint/checkpoint_batch_19999
2026-01-03 14:57:53,961: Best avg val PER achieved: 0.17428
2026-01-03 14:57:53,961: Total training time: 35.04 minutes
All runs finished.
