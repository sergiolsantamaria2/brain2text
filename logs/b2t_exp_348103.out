torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  3 15:07 /tmp/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
PYTHONFAULTHANDLER=1
MAX_JOBS=8
TORCH_EXTENSIONS_DIR=/tmp/torch_extensions
TORCH_CUDA_ARCH_LIST=8.0
B2T_DATA_DIR=/home/e12511253/Brain2Text/brain2text/data/hdf5_data_final
==============================================
Job: b2t_exp  ID: 348103
Base: configs/rnn_args.yaml
Global override: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/rnn_dropout/lr40
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
TMPDIR=/tmp
WANDB_DIR=/tmp/wandb
==============================================

========== FOLDER: configs/experiments/rnn_dropout/lr40 ==========
Num configs: 5

=== RUN base.yaml ===
2026-01-03 15:08:01,256: Using device: cuda:0
2026-01-03 15:08:03,014: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-03 15:08:03,039: Using 45 sessions after filtering (from 45).
2026-01-03 15:08:03,458: Using torch.compile (if available)
2026-01-03 15:08:03,458: torch.compile not available (torch<2.0). Skipping.
2026-01-03 15:08:03,458: Initialized RNN decoding model
2026-01-03 15:08:03,458: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 15:08:03,459: Model has 44,907,305 parameters
2026-01-03 15:08:03,459: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 15:08:04,777: Successfully initialized datasets
2026-01-03 15:08:04,777: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 15:08:07,697: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.323
2026-01-03 15:08:07,697: Running test after training batch: 0
2026-01-03 15:08:07,840: WER debug GT example: You can see the code at this point as well.
2026-01-03 15:08:15,073: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-03 15:08:15,780: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-03 15:08:49,540: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 41.842
2026-01-03 15:08:49,540: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-03 15:08:49,540: t15.2023.08.13 val PER: 1.3056
2026-01-03 15:08:49,540: t15.2023.08.18 val PER: 1.4208
2026-01-03 15:08:49,540: t15.2023.08.20 val PER: 1.3002
2026-01-03 15:08:49,540: t15.2023.08.25 val PER: 1.3389
2026-01-03 15:08:49,541: t15.2023.08.27 val PER: 1.2460
2026-01-03 15:08:49,541: t15.2023.09.01 val PER: 1.4537
2026-01-03 15:08:49,541: t15.2023.09.03 val PER: 1.3171
2026-01-03 15:08:49,541: t15.2023.09.24 val PER: 1.5461
2026-01-03 15:08:49,541: t15.2023.09.29 val PER: 1.4671
2026-01-03 15:08:49,541: t15.2023.10.01 val PER: 1.2147
2026-01-03 15:08:49,541: t15.2023.10.06 val PER: 1.4876
2026-01-03 15:08:49,541: t15.2023.10.08 val PER: 1.1827
2026-01-03 15:08:49,542: t15.2023.10.13 val PER: 1.3964
2026-01-03 15:08:49,542: t15.2023.10.15 val PER: 1.3889
2026-01-03 15:08:49,542: t15.2023.10.20 val PER: 1.4866
2026-01-03 15:08:49,542: t15.2023.10.22 val PER: 1.3942
2026-01-03 15:08:49,542: t15.2023.11.03 val PER: 1.5923
2026-01-03 15:08:49,542: t15.2023.11.04 val PER: 2.0171
2026-01-03 15:08:49,542: t15.2023.11.17 val PER: 1.9518
2026-01-03 15:08:49,542: t15.2023.11.19 val PER: 1.6707
2026-01-03 15:08:49,542: t15.2023.11.26 val PER: 1.5413
2026-01-03 15:08:49,542: t15.2023.12.03 val PER: 1.4254
2026-01-03 15:08:49,542: t15.2023.12.08 val PER: 1.4487
2026-01-03 15:08:49,542: t15.2023.12.10 val PER: 1.6899
2026-01-03 15:08:49,542: t15.2023.12.17 val PER: 1.3077
2026-01-03 15:08:49,543: t15.2023.12.29 val PER: 1.4063
2026-01-03 15:08:49,543: t15.2024.02.25 val PER: 1.4228
2026-01-03 15:08:49,543: t15.2024.03.08 val PER: 1.3257
2026-01-03 15:08:49,543: t15.2024.03.15 val PER: 1.3196
2026-01-03 15:08:49,543: t15.2024.03.17 val PER: 1.4052
2026-01-03 15:08:49,543: t15.2024.05.10 val PER: 1.3224
2026-01-03 15:08:49,543: t15.2024.06.14 val PER: 1.5315
2026-01-03 15:08:49,543: t15.2024.07.19 val PER: 1.0817
2026-01-03 15:08:49,543: t15.2024.07.21 val PER: 1.6290
2026-01-03 15:08:49,543: t15.2024.07.28 val PER: 1.6588
2026-01-03 15:08:49,543: t15.2025.01.10 val PER: 1.0923
2026-01-03 15:08:49,543: t15.2025.01.12 val PER: 1.7629
2026-01-03 15:08:49,544: t15.2025.03.14 val PER: 1.0414
2026-01-03 15:08:49,544: t15.2025.03.16 val PER: 1.6257
2026-01-03 15:08:49,544: t15.2025.03.30 val PER: 1.2874
2026-01-03 15:08:49,544: t15.2025.04.13 val PER: 1.5949
2026-01-03 15:08:49,544: New best val WER(1gram) inf% --> 100.00%
2026-01-03 15:08:49,544: Checkpointing model
2026-01-03 15:08:49,830: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 15:08:50,083: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_0
2026-01-03 15:09:19,892: Train batch 200: loss: 77.59 grad norm: 106.22 time: 0.126
2026-01-03 15:09:49,327: Train batch 400: loss: 53.35 grad norm: 87.03 time: 0.146
2026-01-03 15:10:03,622: Running test after training batch: 500
2026-01-03 15:10:04,012: WER debug GT example: You can see the code at this point as well.
2026-01-03 15:10:11,582: WER debug example
  GT : you can see the code at this point as well
  PR : used and ease thus uhde at this ide is aisle
2026-01-03 15:10:11,613: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus as tides
2026-01-03 15:10:13,943: Val batch 500: PER (avg): 0.5192 CTC Loss (avg): 55.8080 WER(1gram): 87.82% (n=64) time: 10.321
2026-01-03 15:10:13,944: WER lens: avg_true_words=6.16 avg_pred_words=5.69 max_pred_words=12
2026-01-03 15:10:13,944: t15.2023.08.13 val PER: 0.4615
2026-01-03 15:10:13,944: t15.2023.08.18 val PER: 0.4593
2026-01-03 15:10:13,944: t15.2023.08.20 val PER: 0.4440
2026-01-03 15:10:13,944: t15.2023.08.25 val PER: 0.4337
2026-01-03 15:10:13,944: t15.2023.08.27 val PER: 0.5289
2026-01-03 15:10:13,944: t15.2023.09.01 val PER: 0.4148
2026-01-03 15:10:13,944: t15.2023.09.03 val PER: 0.5048
2026-01-03 15:10:13,945: t15.2023.09.24 val PER: 0.4320
2026-01-03 15:10:13,945: t15.2023.09.29 val PER: 0.4690
2026-01-03 15:10:13,945: t15.2023.10.01 val PER: 0.5238
2026-01-03 15:10:13,945: t15.2023.10.06 val PER: 0.4327
2026-01-03 15:10:13,945: t15.2023.10.08 val PER: 0.5332
2026-01-03 15:10:13,945: t15.2023.10.13 val PER: 0.5756
2026-01-03 15:10:13,945: t15.2023.10.15 val PER: 0.4970
2026-01-03 15:10:13,945: t15.2023.10.20 val PER: 0.4530
2026-01-03 15:10:13,945: t15.2023.10.22 val PER: 0.4477
2026-01-03 15:10:13,945: t15.2023.11.03 val PER: 0.5081
2026-01-03 15:10:13,945: t15.2023.11.04 val PER: 0.2730
2026-01-03 15:10:13,945: t15.2023.11.17 val PER: 0.3639
2026-01-03 15:10:13,945: t15.2023.11.19 val PER: 0.3373
2026-01-03 15:10:13,945: t15.2023.11.26 val PER: 0.5449
2026-01-03 15:10:13,946: t15.2023.12.03 val PER: 0.5021
2026-01-03 15:10:13,946: t15.2023.12.08 val PER: 0.5186
2026-01-03 15:10:13,946: t15.2023.12.10 val PER: 0.4573
2026-01-03 15:10:13,946: t15.2023.12.17 val PER: 0.5541
2026-01-03 15:10:13,946: t15.2023.12.29 val PER: 0.5573
2026-01-03 15:10:13,946: t15.2024.02.25 val PER: 0.4817
2026-01-03 15:10:13,946: t15.2024.03.08 val PER: 0.6302
2026-01-03 15:10:13,946: t15.2024.03.15 val PER: 0.5578
2026-01-03 15:10:13,946: t15.2024.03.17 val PER: 0.4986
2026-01-03 15:10:13,946: t15.2024.05.10 val PER: 0.5557
2026-01-03 15:10:13,946: t15.2024.06.14 val PER: 0.5095
2026-01-03 15:10:13,946: t15.2024.07.19 val PER: 0.6671
2026-01-03 15:10:13,946: t15.2024.07.21 val PER: 0.4655
2026-01-03 15:10:13,946: t15.2024.07.28 val PER: 0.5044
2026-01-03 15:10:13,946: t15.2025.01.10 val PER: 0.7410
2026-01-03 15:10:13,946: t15.2025.01.12 val PER: 0.5620
2026-01-03 15:10:13,946: t15.2025.03.14 val PER: 0.7441
2026-01-03 15:10:13,947: t15.2025.03.16 val PER: 0.5995
2026-01-03 15:10:13,947: t15.2025.03.30 val PER: 0.7379
2026-01-03 15:10:13,947: t15.2025.04.13 val PER: 0.5777
2026-01-03 15:10:13,948: New best val WER(1gram) 100.00% --> 87.82%
2026-01-03 15:10:13,948: Checkpointing model
2026-01-03 15:10:14,211: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 15:10:14,465: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_500
2026-01-03 15:10:28,941: Train batch 600: loss: 48.93 grad norm: 75.17 time: 0.179
2026-01-03 15:10:57,899: Train batch 800: loss: 41.09 grad norm: 88.08 time: 0.132
2026-01-03 15:11:27,031: Train batch 1000: loss: 43.11 grad norm: 83.08 time: 0.151
2026-01-03 15:11:27,032: Running test after training batch: 1000
2026-01-03 15:11:27,147: WER debug GT example: You can see the code at this point as well.
2026-01-03 15:11:33,830: WER debug example
  GT : you can see the code at this point as well
  PR : yule ent ease thus code it this uhde is while
2026-01-03 15:11:33,861: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus wass
2026-01-03 15:11:35,752: Val batch 1000: PER (avg): 0.4092 CTC Loss (avg): 42.4130 WER(1gram): 81.73% (n=64) time: 8.720
2026-01-03 15:11:35,752: WER lens: avg_true_words=6.16 avg_pred_words=5.48 max_pred_words=12
2026-01-03 15:11:35,752: t15.2023.08.13 val PER: 0.3773
2026-01-03 15:11:35,752: t15.2023.08.18 val PER: 0.3370
2026-01-03 15:11:35,753: t15.2023.08.20 val PER: 0.3415
2026-01-03 15:11:35,753: t15.2023.08.25 val PER: 0.2877
2026-01-03 15:11:35,753: t15.2023.08.27 val PER: 0.4212
2026-01-03 15:11:35,753: t15.2023.09.01 val PER: 0.3036
2026-01-03 15:11:35,753: t15.2023.09.03 val PER: 0.3884
2026-01-03 15:11:35,753: t15.2023.09.24 val PER: 0.3337
2026-01-03 15:11:35,753: t15.2023.09.29 val PER: 0.3631
2026-01-03 15:11:35,753: t15.2023.10.01 val PER: 0.4062
2026-01-03 15:11:35,753: t15.2023.10.06 val PER: 0.3186
2026-01-03 15:11:35,753: t15.2023.10.08 val PER: 0.4560
2026-01-03 15:11:35,753: t15.2023.10.13 val PER: 0.4670
2026-01-03 15:11:35,753: t15.2023.10.15 val PER: 0.3817
2026-01-03 15:11:35,753: t15.2023.10.20 val PER: 0.3792
2026-01-03 15:11:35,753: t15.2023.10.22 val PER: 0.3519
2026-01-03 15:11:35,753: t15.2023.11.03 val PER: 0.4050
2026-01-03 15:11:35,754: t15.2023.11.04 val PER: 0.1536
2026-01-03 15:11:35,754: t15.2023.11.17 val PER: 0.2644
2026-01-03 15:11:35,754: t15.2023.11.19 val PER: 0.2196
2026-01-03 15:11:35,754: t15.2023.11.26 val PER: 0.4486
2026-01-03 15:11:35,754: t15.2023.12.03 val PER: 0.4076
2026-01-03 15:11:35,754: t15.2023.12.08 val PER: 0.4075
2026-01-03 15:11:35,754: t15.2023.12.10 val PER: 0.3403
2026-01-03 15:11:35,754: t15.2023.12.17 val PER: 0.4148
2026-01-03 15:11:35,754: t15.2023.12.29 val PER: 0.4077
2026-01-03 15:11:35,754: t15.2024.02.25 val PER: 0.3455
2026-01-03 15:11:35,754: t15.2024.03.08 val PER: 0.5092
2026-01-03 15:11:35,754: t15.2024.03.15 val PER: 0.4459
2026-01-03 15:11:35,754: t15.2024.03.17 val PER: 0.4059
2026-01-03 15:11:35,754: t15.2024.05.10 val PER: 0.4160
2026-01-03 15:11:35,754: t15.2024.06.14 val PER: 0.4117
2026-01-03 15:11:35,754: t15.2024.07.19 val PER: 0.5260
2026-01-03 15:11:35,754: t15.2024.07.21 val PER: 0.3710
2026-01-03 15:11:35,754: t15.2024.07.28 val PER: 0.4176
2026-01-03 15:11:35,755: t15.2025.01.10 val PER: 0.6143
2026-01-03 15:11:35,755: t15.2025.01.12 val PER: 0.4473
2026-01-03 15:11:35,755: t15.2025.03.14 val PER: 0.6405
2026-01-03 15:11:35,755: t15.2025.03.16 val PER: 0.4830
2026-01-03 15:11:35,755: t15.2025.03.30 val PER: 0.6483
2026-01-03 15:11:35,755: t15.2025.04.13 val PER: 0.5078
2026-01-03 15:11:35,756: New best val WER(1gram) 87.82% --> 81.73%
2026-01-03 15:11:35,756: Checkpointing model
2026-01-03 15:11:36,021: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 15:11:36,278: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_1000
2026-01-03 15:12:05,962: Train batch 1200: loss: 32.42 grad norm: 73.53 time: 0.155
2026-01-03 15:12:36,505: Train batch 1400: loss: 36.03 grad norm: 79.43 time: 0.141
2026-01-03 15:12:51,841: Running test after training batch: 1500
2026-01-03 15:12:51,959: WER debug GT example: You can see the code at this point as well.
2026-01-03 15:12:58,669: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the good it this boyde is will
2026-01-03 15:12:58,699: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap thus wass
2026-01-03 15:13:00,268: Val batch 1500: PER (avg): 0.3803 CTC Loss (avg): 37.2828 WER(1gram): 76.14% (n=64) time: 8.427
2026-01-03 15:13:00,268: WER lens: avg_true_words=6.16 avg_pred_words=4.98 max_pred_words=10
2026-01-03 15:13:00,269: t15.2023.08.13 val PER: 0.3389
2026-01-03 15:13:00,269: t15.2023.08.18 val PER: 0.3118
2026-01-03 15:13:00,269: t15.2023.08.20 val PER: 0.3058
2026-01-03 15:13:00,269: t15.2023.08.25 val PER: 0.2681
2026-01-03 15:13:00,269: t15.2023.08.27 val PER: 0.4019
2026-01-03 15:13:00,269: t15.2023.09.01 val PER: 0.2695
2026-01-03 15:13:00,269: t15.2023.09.03 val PER: 0.3729
2026-01-03 15:13:00,269: t15.2023.09.24 val PER: 0.3083
2026-01-03 15:13:00,270: t15.2023.09.29 val PER: 0.3376
2026-01-03 15:13:00,270: t15.2023.10.01 val PER: 0.3956
2026-01-03 15:13:00,270: t15.2023.10.06 val PER: 0.2874
2026-01-03 15:13:00,270: t15.2023.10.08 val PER: 0.4357
2026-01-03 15:13:00,270: t15.2023.10.13 val PER: 0.4476
2026-01-03 15:13:00,270: t15.2023.10.15 val PER: 0.3645
2026-01-03 15:13:00,270: t15.2023.10.20 val PER: 0.3188
2026-01-03 15:13:00,270: t15.2023.10.22 val PER: 0.3151
2026-01-03 15:13:00,270: t15.2023.11.03 val PER: 0.3670
2026-01-03 15:13:00,270: t15.2023.11.04 val PER: 0.1092
2026-01-03 15:13:00,270: t15.2023.11.17 val PER: 0.2208
2026-01-03 15:13:00,270: t15.2023.11.19 val PER: 0.1916
2026-01-03 15:13:00,270: t15.2023.11.26 val PER: 0.4130
2026-01-03 15:13:00,270: t15.2023.12.03 val PER: 0.3771
2026-01-03 15:13:00,270: t15.2023.12.08 val PER: 0.3595
2026-01-03 15:13:00,270: t15.2023.12.10 val PER: 0.2983
2026-01-03 15:13:00,271: t15.2023.12.17 val PER: 0.3597
2026-01-03 15:13:00,271: t15.2023.12.29 val PER: 0.3789
2026-01-03 15:13:00,271: t15.2024.02.25 val PER: 0.3090
2026-01-03 15:13:00,271: t15.2024.03.08 val PER: 0.4523
2026-01-03 15:13:00,271: t15.2024.03.15 val PER: 0.4159
2026-01-03 15:13:00,271: t15.2024.03.17 val PER: 0.3828
2026-01-03 15:13:00,271: t15.2024.05.10 val PER: 0.3893
2026-01-03 15:13:00,271: t15.2024.06.14 val PER: 0.3864
2026-01-03 15:13:00,271: t15.2024.07.19 val PER: 0.5300
2026-01-03 15:13:00,271: t15.2024.07.21 val PER: 0.3483
2026-01-03 15:13:00,271: t15.2024.07.28 val PER: 0.3662
2026-01-03 15:13:00,271: t15.2025.01.10 val PER: 0.6116
2026-01-03 15:13:00,271: t15.2025.01.12 val PER: 0.4211
2026-01-03 15:13:00,272: t15.2025.03.14 val PER: 0.6080
2026-01-03 15:13:00,272: t15.2025.03.16 val PER: 0.4686
2026-01-03 15:13:00,272: t15.2025.03.30 val PER: 0.6264
2026-01-03 15:13:00,272: t15.2025.04.13 val PER: 0.4708
2026-01-03 15:13:00,273: New best val WER(1gram) 81.73% --> 76.14%
2026-01-03 15:13:00,273: Checkpointing model
2026-01-03 15:13:00,551: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 15:13:00,806: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_1500
2026-01-03 15:13:15,455: Train batch 1600: loss: 36.90 grad norm: 83.95 time: 0.146
2026-01-03 15:13:44,510: Train batch 1800: loss: 35.18 grad norm: 73.49 time: 0.207
2026-01-03 15:14:14,706: Train batch 2000: loss: 33.63 grad norm: 70.61 time: 0.160
2026-01-03 15:14:14,706: Running test after training batch: 2000
2026-01-03 15:14:15,087: WER debug GT example: You can see the code at this point as well.
2026-01-03 15:14:21,797: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this bonde is will
2026-01-03 15:14:21,828: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap the kos it
2026-01-03 15:14:23,411: Val batch 2000: PER (avg): 0.3265 CTC Loss (avg): 32.7433 WER(1gram): 71.32% (n=64) time: 8.705
2026-01-03 15:14:23,412: WER lens: avg_true_words=6.16 avg_pred_words=5.55 max_pred_words=11
2026-01-03 15:14:23,412: t15.2023.08.13 val PER: 0.3025
2026-01-03 15:14:23,412: t15.2023.08.18 val PER: 0.2573
2026-01-03 15:14:23,412: t15.2023.08.20 val PER: 0.2510
2026-01-03 15:14:23,412: t15.2023.08.25 val PER: 0.2304
2026-01-03 15:14:23,412: t15.2023.08.27 val PER: 0.3408
2026-01-03 15:14:23,412: t15.2023.09.01 val PER: 0.2297
2026-01-03 15:14:23,412: t15.2023.09.03 val PER: 0.3195
2026-01-03 15:14:23,412: t15.2023.09.24 val PER: 0.2609
2026-01-03 15:14:23,412: t15.2023.09.29 val PER: 0.2827
2026-01-03 15:14:23,412: t15.2023.10.01 val PER: 0.3197
2026-01-03 15:14:23,412: t15.2023.10.06 val PER: 0.2282
2026-01-03 15:14:23,413: t15.2023.10.08 val PER: 0.3978
2026-01-03 15:14:23,413: t15.2023.10.13 val PER: 0.3763
2026-01-03 15:14:23,413: t15.2023.10.15 val PER: 0.2993
2026-01-03 15:14:23,413: t15.2023.10.20 val PER: 0.2852
2026-01-03 15:14:23,413: t15.2023.10.22 val PER: 0.2572
2026-01-03 15:14:23,413: t15.2023.11.03 val PER: 0.3209
2026-01-03 15:14:23,413: t15.2023.11.04 val PER: 0.0683
2026-01-03 15:14:23,413: t15.2023.11.17 val PER: 0.1711
2026-01-03 15:14:23,413: t15.2023.11.19 val PER: 0.1457
2026-01-03 15:14:23,413: t15.2023.11.26 val PER: 0.3652
2026-01-03 15:14:23,413: t15.2023.12.03 val PER: 0.3172
2026-01-03 15:14:23,413: t15.2023.12.08 val PER: 0.3156
2026-01-03 15:14:23,413: t15.2023.12.10 val PER: 0.2589
2026-01-03 15:14:23,413: t15.2023.12.17 val PER: 0.3202
2026-01-03 15:14:23,414: t15.2023.12.29 val PER: 0.3191
2026-01-03 15:14:23,414: t15.2024.02.25 val PER: 0.2767
2026-01-03 15:14:23,414: t15.2024.03.08 val PER: 0.3869
2026-01-03 15:14:23,414: t15.2024.03.15 val PER: 0.3602
2026-01-03 15:14:23,414: t15.2024.03.17 val PER: 0.3375
2026-01-03 15:14:23,414: t15.2024.05.10 val PER: 0.3507
2026-01-03 15:14:23,414: t15.2024.06.14 val PER: 0.3533
2026-01-03 15:14:23,414: t15.2024.07.19 val PER: 0.4667
2026-01-03 15:14:23,414: t15.2024.07.21 val PER: 0.2910
2026-01-03 15:14:23,414: t15.2024.07.28 val PER: 0.3243
2026-01-03 15:14:23,414: t15.2025.01.10 val PER: 0.5386
2026-01-03 15:14:23,414: t15.2025.01.12 val PER: 0.3780
2026-01-03 15:14:23,414: t15.2025.03.14 val PER: 0.5178
2026-01-03 15:14:23,414: t15.2025.03.16 val PER: 0.3887
2026-01-03 15:14:23,414: t15.2025.03.30 val PER: 0.5460
2026-01-03 15:14:23,414: t15.2025.04.13 val PER: 0.4151
2026-01-03 15:14:23,416: New best val WER(1gram) 76.14% --> 71.32%
2026-01-03 15:14:23,416: Checkpointing model
2026-01-03 15:14:23,681: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 15:14:23,935: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_2000
2026-01-03 15:14:53,587: Train batch 2200: loss: 29.93 grad norm: 74.78 time: 0.138
2026-01-03 15:15:22,892: Train batch 2400: loss: 28.92 grad norm: 67.48 time: 0.125
2026-01-03 15:15:38,523: Running test after training batch: 2500
2026-01-03 15:15:38,690: WER debug GT example: You can see the code at this point as well.
2026-01-03 15:15:45,375: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is wheel
2026-01-03 15:15:45,404: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the wass nit
2026-01-03 15:15:47,061: Val batch 2500: PER (avg): 0.3035 CTC Loss (avg): 30.1631 WER(1gram): 68.78% (n=64) time: 8.537
2026-01-03 15:15:47,061: WER lens: avg_true_words=6.16 avg_pred_words=5.67 max_pred_words=11
2026-01-03 15:15:47,061: t15.2023.08.13 val PER: 0.2942
2026-01-03 15:15:47,062: t15.2023.08.18 val PER: 0.2414
2026-01-03 15:15:47,062: t15.2023.08.20 val PER: 0.2423
2026-01-03 15:15:47,062: t15.2023.08.25 val PER: 0.1943
2026-01-03 15:15:47,062: t15.2023.08.27 val PER: 0.3215
2026-01-03 15:15:47,062: t15.2023.09.01 val PER: 0.2135
2026-01-03 15:15:47,062: t15.2023.09.03 val PER: 0.2933
2026-01-03 15:15:47,062: t15.2023.09.24 val PER: 0.2330
2026-01-03 15:15:47,062: t15.2023.09.29 val PER: 0.2476
2026-01-03 15:15:47,062: t15.2023.10.01 val PER: 0.3137
2026-01-03 15:15:47,062: t15.2023.10.06 val PER: 0.2142
2026-01-03 15:15:47,062: t15.2023.10.08 val PER: 0.3721
2026-01-03 15:15:47,062: t15.2023.10.13 val PER: 0.3530
2026-01-03 15:15:47,062: t15.2023.10.15 val PER: 0.2854
2026-01-03 15:15:47,063: t15.2023.10.20 val PER: 0.2718
2026-01-03 15:15:47,063: t15.2023.10.22 val PER: 0.2283
2026-01-03 15:15:47,063: t15.2023.11.03 val PER: 0.3005
2026-01-03 15:15:47,063: t15.2023.11.04 val PER: 0.0887
2026-01-03 15:15:47,063: t15.2023.11.17 val PER: 0.1369
2026-01-03 15:15:47,063: t15.2023.11.19 val PER: 0.1178
2026-01-03 15:15:47,063: t15.2023.11.26 val PER: 0.3377
2026-01-03 15:15:47,063: t15.2023.12.03 val PER: 0.2910
2026-01-03 15:15:47,063: t15.2023.12.08 val PER: 0.2736
2026-01-03 15:15:47,063: t15.2023.12.10 val PER: 0.2418
2026-01-03 15:15:47,063: t15.2023.12.17 val PER: 0.3004
2026-01-03 15:15:47,063: t15.2023.12.29 val PER: 0.3082
2026-01-03 15:15:47,063: t15.2024.02.25 val PER: 0.2500
2026-01-03 15:15:47,063: t15.2024.03.08 val PER: 0.3542
2026-01-03 15:15:47,063: t15.2024.03.15 val PER: 0.3552
2026-01-03 15:15:47,063: t15.2024.03.17 val PER: 0.3068
2026-01-03 15:15:47,064: t15.2024.05.10 val PER: 0.3016
2026-01-03 15:15:47,064: t15.2024.06.14 val PER: 0.3060
2026-01-03 15:15:47,064: t15.2024.07.19 val PER: 0.4423
2026-01-03 15:15:47,064: t15.2024.07.21 val PER: 0.2648
2026-01-03 15:15:47,064: t15.2024.07.28 val PER: 0.2963
2026-01-03 15:15:47,064: t15.2025.01.10 val PER: 0.5014
2026-01-03 15:15:47,064: t15.2025.01.12 val PER: 0.3610
2026-01-03 15:15:47,064: t15.2025.03.14 val PER: 0.4941
2026-01-03 15:15:47,064: t15.2025.03.16 val PER: 0.3626
2026-01-03 15:15:47,064: t15.2025.03.30 val PER: 0.5057
2026-01-03 15:15:47,064: t15.2025.04.13 val PER: 0.3966
2026-01-03 15:15:47,065: New best val WER(1gram) 71.32% --> 68.78%
2026-01-03 15:15:47,065: Checkpointing model
2026-01-03 15:15:47,342: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 15:15:47,598: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_2500
2026-01-03 15:16:02,748: Train batch 2600: loss: 35.53 grad norm: 91.30 time: 0.127
2026-01-03 15:16:32,266: Train batch 2800: loss: 25.70 grad norm: 73.96 time: 0.190
2026-01-03 15:17:01,573: Train batch 3000: loss: 31.18 grad norm: 72.55 time: 0.190
2026-01-03 15:17:01,574: Running test after training batch: 3000
2026-01-03 15:17:01,690: WER debug GT example: You can see the code at this point as well.
2026-01-03 15:17:08,618: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the could at this point is will
2026-01-03 15:17:08,648: WER debug example
  GT : how does it keep the cost down
  PR : houde des it hipp the rost get
2026-01-03 15:17:10,308: Val batch 3000: PER (avg): 0.2815 CTC Loss (avg): 27.7820 WER(1gram): 65.48% (n=64) time: 8.734
2026-01-03 15:17:10,308: WER lens: avg_true_words=6.16 avg_pred_words=5.78 max_pred_words=10
2026-01-03 15:17:10,309: t15.2023.08.13 val PER: 0.2588
2026-01-03 15:17:10,309: t15.2023.08.18 val PER: 0.2246
2026-01-03 15:17:10,309: t15.2023.08.20 val PER: 0.2224
2026-01-03 15:17:10,309: t15.2023.08.25 val PER: 0.1867
2026-01-03 15:17:10,309: t15.2023.08.27 val PER: 0.2862
2026-01-03 15:17:10,309: t15.2023.09.01 val PER: 0.1907
2026-01-03 15:17:10,309: t15.2023.09.03 val PER: 0.2791
2026-01-03 15:17:10,309: t15.2023.09.24 val PER: 0.2112
2026-01-03 15:17:10,309: t15.2023.09.29 val PER: 0.2336
2026-01-03 15:17:10,309: t15.2023.10.01 val PER: 0.2966
2026-01-03 15:17:10,309: t15.2023.10.06 val PER: 0.2013
2026-01-03 15:17:10,309: t15.2023.10.08 val PER: 0.3396
2026-01-03 15:17:10,309: t15.2023.10.13 val PER: 0.3437
2026-01-03 15:17:10,310: t15.2023.10.15 val PER: 0.2663
2026-01-03 15:17:10,310: t15.2023.10.20 val PER: 0.2584
2026-01-03 15:17:10,310: t15.2023.10.22 val PER: 0.2160
2026-01-03 15:17:10,310: t15.2023.11.03 val PER: 0.2700
2026-01-03 15:17:10,310: t15.2023.11.04 val PER: 0.0751
2026-01-03 15:17:10,310: t15.2023.11.17 val PER: 0.1306
2026-01-03 15:17:10,310: t15.2023.11.19 val PER: 0.1218
2026-01-03 15:17:10,310: t15.2023.11.26 val PER: 0.3051
2026-01-03 15:17:10,310: t15.2023.12.03 val PER: 0.2658
2026-01-03 15:17:10,310: t15.2023.12.08 val PER: 0.2503
2026-01-03 15:17:10,310: t15.2023.12.10 val PER: 0.2076
2026-01-03 15:17:10,310: t15.2023.12.17 val PER: 0.2796
2026-01-03 15:17:10,310: t15.2023.12.29 val PER: 0.2793
2026-01-03 15:17:10,310: t15.2024.02.25 val PER: 0.2528
2026-01-03 15:17:10,310: t15.2024.03.08 val PER: 0.3585
2026-01-03 15:17:10,310: t15.2024.03.15 val PER: 0.3346
2026-01-03 15:17:10,311: t15.2024.03.17 val PER: 0.2859
2026-01-03 15:17:10,311: t15.2024.05.10 val PER: 0.2972
2026-01-03 15:17:10,311: t15.2024.06.14 val PER: 0.2997
2026-01-03 15:17:10,311: t15.2024.07.19 val PER: 0.4041
2026-01-03 15:17:10,311: t15.2024.07.21 val PER: 0.2221
2026-01-03 15:17:10,311: t15.2024.07.28 val PER: 0.2794
2026-01-03 15:17:10,311: t15.2025.01.10 val PER: 0.4904
2026-01-03 15:17:10,311: t15.2025.01.12 val PER: 0.3249
2026-01-03 15:17:10,311: t15.2025.03.14 val PER: 0.4571
2026-01-03 15:17:10,311: t15.2025.03.16 val PER: 0.3298
2026-01-03 15:17:10,311: t15.2025.03.30 val PER: 0.4793
2026-01-03 15:17:10,312: t15.2025.04.13 val PER: 0.3581
2026-01-03 15:17:10,313: New best val WER(1gram) 68.78% --> 65.48%
2026-01-03 15:17:10,313: Checkpointing model
2026-01-03 15:17:10,578: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 15:17:10,833: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_3000
2026-01-03 15:17:40,752: Train batch 3200: loss: 25.90 grad norm: 66.56 time: 0.173
2026-01-03 15:18:10,169: Train batch 3400: loss: 18.33 grad norm: 56.70 time: 0.116
2026-01-03 15:18:25,067: Running test after training batch: 3500
2026-01-03 15:18:25,187: WER debug GT example: You can see the code at this point as well.
2026-01-03 15:18:31,868: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-03 15:18:31,896: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it kipp the us et
2026-01-03 15:18:33,519: Val batch 3500: PER (avg): 0.2666 CTC Loss (avg): 26.6947 WER(1gram): 66.24% (n=64) time: 8.451
2026-01-03 15:18:33,520: WER lens: avg_true_words=6.16 avg_pred_words=5.95 max_pred_words=11
2026-01-03 15:18:33,520: t15.2023.08.13 val PER: 0.2474
2026-01-03 15:18:33,520: t15.2023.08.18 val PER: 0.2112
2026-01-03 15:18:33,520: t15.2023.08.20 val PER: 0.2129
2026-01-03 15:18:33,520: t15.2023.08.25 val PER: 0.1627
2026-01-03 15:18:33,520: t15.2023.08.27 val PER: 0.2701
2026-01-03 15:18:33,521: t15.2023.09.01 val PER: 0.1713
2026-01-03 15:18:33,521: t15.2023.09.03 val PER: 0.2458
2026-01-03 15:18:33,521: t15.2023.09.24 val PER: 0.2112
2026-01-03 15:18:33,521: t15.2023.09.29 val PER: 0.2125
2026-01-03 15:18:33,521: t15.2023.10.01 val PER: 0.2688
2026-01-03 15:18:33,521: t15.2023.10.06 val PER: 0.1873
2026-01-03 15:18:33,521: t15.2023.10.08 val PER: 0.3437
2026-01-03 15:18:33,521: t15.2023.10.13 val PER: 0.3235
2026-01-03 15:18:33,521: t15.2023.10.15 val PER: 0.2432
2026-01-03 15:18:33,521: t15.2023.10.20 val PER: 0.2416
2026-01-03 15:18:33,521: t15.2023.10.22 val PER: 0.2116
2026-01-03 15:18:33,522: t15.2023.11.03 val PER: 0.2612
2026-01-03 15:18:33,522: t15.2023.11.04 val PER: 0.0683
2026-01-03 15:18:33,522: t15.2023.11.17 val PER: 0.1213
2026-01-03 15:18:33,522: t15.2023.11.19 val PER: 0.0918
2026-01-03 15:18:33,522: t15.2023.11.26 val PER: 0.2812
2026-01-03 15:18:33,522: t15.2023.12.03 val PER: 0.2395
2026-01-03 15:18:33,522: t15.2023.12.08 val PER: 0.2537
2026-01-03 15:18:33,522: t15.2023.12.10 val PER: 0.2050
2026-01-03 15:18:33,522: t15.2023.12.17 val PER: 0.2422
2026-01-03 15:18:33,522: t15.2023.12.29 val PER: 0.2546
2026-01-03 15:18:33,522: t15.2024.02.25 val PER: 0.2331
2026-01-03 15:18:33,522: t15.2024.03.08 val PER: 0.3414
2026-01-03 15:18:33,522: t15.2024.03.15 val PER: 0.3258
2026-01-03 15:18:33,522: t15.2024.03.17 val PER: 0.2762
2026-01-03 15:18:33,522: t15.2024.05.10 val PER: 0.2808
2026-01-03 15:18:33,522: t15.2024.06.14 val PER: 0.2902
2026-01-03 15:18:33,523: t15.2024.07.19 val PER: 0.3962
2026-01-03 15:18:33,523: t15.2024.07.21 val PER: 0.2338
2026-01-03 15:18:33,523: t15.2024.07.28 val PER: 0.2787
2026-01-03 15:18:33,523: t15.2025.01.10 val PER: 0.4656
2026-01-03 15:18:33,523: t15.2025.01.12 val PER: 0.2902
2026-01-03 15:18:33,523: t15.2025.03.14 val PER: 0.4453
2026-01-03 15:18:33,523: t15.2025.03.16 val PER: 0.3154
2026-01-03 15:18:33,523: t15.2025.03.30 val PER: 0.4552
2026-01-03 15:18:33,523: t15.2025.04.13 val PER: 0.3424
2026-01-03 15:18:33,769: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_3500
2026-01-03 15:18:48,552: Train batch 3600: loss: 22.16 grad norm: 61.62 time: 0.152
2026-01-03 15:19:17,837: Train batch 3800: loss: 26.06 grad norm: 68.58 time: 0.154
2026-01-03 15:19:48,008: Train batch 4000: loss: 19.97 grad norm: 55.79 time: 0.129
2026-01-03 15:19:48,009: Running test after training batch: 4000
2026-01-03 15:19:48,127: WER debug GT example: You can see the code at this point as well.
2026-01-03 15:19:54,873: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-03 15:19:54,901: WER debug example
  GT : how does it keep the cost down
  PR : houde des it keep the cost nit
2026-01-03 15:19:56,537: Val batch 4000: PER (avg): 0.2481 CTC Loss (avg): 24.3716 WER(1gram): 65.48% (n=64) time: 8.529
2026-01-03 15:19:56,538: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-03 15:19:56,538: t15.2023.08.13 val PER: 0.2225
2026-01-03 15:19:56,538: t15.2023.08.18 val PER: 0.1995
2026-01-03 15:19:56,538: t15.2023.08.20 val PER: 0.1938
2026-01-03 15:19:56,538: t15.2023.08.25 val PER: 0.1581
2026-01-03 15:19:56,538: t15.2023.08.27 val PER: 0.2749
2026-01-03 15:19:56,538: t15.2023.09.01 val PER: 0.1696
2026-01-03 15:19:56,538: t15.2023.09.03 val PER: 0.2494
2026-01-03 15:19:56,539: t15.2023.09.24 val PER: 0.1845
2026-01-03 15:19:56,539: t15.2023.09.29 val PER: 0.2055
2026-01-03 15:19:56,539: t15.2023.10.01 val PER: 0.2609
2026-01-03 15:19:56,539: t15.2023.10.06 val PER: 0.1679
2026-01-03 15:19:56,539: t15.2023.10.08 val PER: 0.3288
2026-01-03 15:19:56,539: t15.2023.10.13 val PER: 0.3049
2026-01-03 15:19:56,539: t15.2023.10.15 val PER: 0.2360
2026-01-03 15:19:56,539: t15.2023.10.20 val PER: 0.2517
2026-01-03 15:19:56,539: t15.2023.10.22 val PER: 0.1993
2026-01-03 15:19:56,539: t15.2023.11.03 val PER: 0.2361
2026-01-03 15:19:56,539: t15.2023.11.04 val PER: 0.0683
2026-01-03 15:19:56,539: t15.2023.11.17 val PER: 0.1104
2026-01-03 15:19:56,539: t15.2023.11.19 val PER: 0.1018
2026-01-03 15:19:56,539: t15.2023.11.26 val PER: 0.2667
2026-01-03 15:19:56,540: t15.2023.12.03 val PER: 0.2143
2026-01-03 15:19:56,540: t15.2023.12.08 val PER: 0.2224
2026-01-03 15:19:56,540: t15.2023.12.10 val PER: 0.1787
2026-01-03 15:19:56,540: t15.2023.12.17 val PER: 0.2401
2026-01-03 15:19:56,540: t15.2023.12.29 val PER: 0.2553
2026-01-03 15:19:56,540: t15.2024.02.25 val PER: 0.2079
2026-01-03 15:19:56,540: t15.2024.03.08 val PER: 0.3371
2026-01-03 15:19:56,540: t15.2024.03.15 val PER: 0.2983
2026-01-03 15:19:56,540: t15.2024.03.17 val PER: 0.2503
2026-01-03 15:19:56,541: t15.2024.05.10 val PER: 0.2585
2026-01-03 15:19:56,541: t15.2024.06.14 val PER: 0.2603
2026-01-03 15:19:56,541: t15.2024.07.19 val PER: 0.3586
2026-01-03 15:19:56,541: t15.2024.07.21 val PER: 0.1903
2026-01-03 15:19:56,541: t15.2024.07.28 val PER: 0.2426
2026-01-03 15:19:56,541: t15.2025.01.10 val PER: 0.4118
2026-01-03 15:19:56,541: t15.2025.01.12 val PER: 0.2733
2026-01-03 15:19:56,541: t15.2025.03.14 val PER: 0.4053
2026-01-03 15:19:56,541: t15.2025.03.16 val PER: 0.3154
2026-01-03 15:19:56,541: t15.2025.03.30 val PER: 0.4172
2026-01-03 15:19:56,541: t15.2025.04.13 val PER: 0.3281
2026-01-03 15:19:56,787: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_4000
2026-01-03 15:20:26,665: Train batch 4200: loss: 21.97 grad norm: 65.17 time: 0.181
2026-01-03 15:20:55,765: Train batch 4400: loss: 17.52 grad norm: 56.28 time: 0.152
2026-01-03 15:21:10,317: Running test after training batch: 4500
2026-01-03 15:21:10,434: WER debug GT example: You can see the code at this point as well.
2026-01-03 15:21:17,093: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-03 15:21:17,122: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it yip the cost get
2026-01-03 15:21:18,711: Val batch 4500: PER (avg): 0.2391 CTC Loss (avg): 23.2624 WER(1gram): 62.18% (n=64) time: 8.393
2026-01-03 15:21:18,711: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-03 15:21:18,711: t15.2023.08.13 val PER: 0.2100
2026-01-03 15:21:18,711: t15.2023.08.18 val PER: 0.1903
2026-01-03 15:21:18,711: t15.2023.08.20 val PER: 0.1954
2026-01-03 15:21:18,711: t15.2023.08.25 val PER: 0.1431
2026-01-03 15:21:18,712: t15.2023.08.27 val PER: 0.2540
2026-01-03 15:21:18,712: t15.2023.09.01 val PER: 0.1510
2026-01-03 15:21:18,712: t15.2023.09.03 val PER: 0.2375
2026-01-03 15:21:18,712: t15.2023.09.24 val PER: 0.1905
2026-01-03 15:21:18,712: t15.2023.09.29 val PER: 0.2061
2026-01-03 15:21:18,712: t15.2023.10.01 val PER: 0.2655
2026-01-03 15:21:18,712: t15.2023.10.06 val PER: 0.1496
2026-01-03 15:21:18,712: t15.2023.10.08 val PER: 0.3275
2026-01-03 15:21:18,712: t15.2023.10.13 val PER: 0.2940
2026-01-03 15:21:18,712: t15.2023.10.15 val PER: 0.2202
2026-01-03 15:21:18,712: t15.2023.10.20 val PER: 0.2181
2026-01-03 15:21:18,712: t15.2023.10.22 val PER: 0.1771
2026-01-03 15:21:18,712: t15.2023.11.03 val PER: 0.2429
2026-01-03 15:21:18,712: t15.2023.11.04 val PER: 0.0580
2026-01-03 15:21:18,713: t15.2023.11.17 val PER: 0.1026
2026-01-03 15:21:18,713: t15.2023.11.19 val PER: 0.0998
2026-01-03 15:21:18,713: t15.2023.11.26 val PER: 0.2696
2026-01-03 15:21:18,713: t15.2023.12.03 val PER: 0.2122
2026-01-03 15:21:18,713: t15.2023.12.08 val PER: 0.2157
2026-01-03 15:21:18,713: t15.2023.12.10 val PER: 0.1748
2026-01-03 15:21:18,713: t15.2023.12.17 val PER: 0.2339
2026-01-03 15:21:18,713: t15.2023.12.29 val PER: 0.2450
2026-01-03 15:21:18,713: t15.2024.02.25 val PER: 0.2079
2026-01-03 15:21:18,713: t15.2024.03.08 val PER: 0.3044
2026-01-03 15:21:18,713: t15.2024.03.15 val PER: 0.2896
2026-01-03 15:21:18,713: t15.2024.03.17 val PER: 0.2455
2026-01-03 15:21:18,713: t15.2024.05.10 val PER: 0.2541
2026-01-03 15:21:18,713: t15.2024.06.14 val PER: 0.2587
2026-01-03 15:21:18,713: t15.2024.07.19 val PER: 0.3342
2026-01-03 15:21:18,713: t15.2024.07.21 val PER: 0.1800
2026-01-03 15:21:18,713: t15.2024.07.28 val PER: 0.2228
2026-01-03 15:21:18,714: t15.2025.01.10 val PER: 0.4132
2026-01-03 15:21:18,714: t15.2025.01.12 val PER: 0.2633
2026-01-03 15:21:18,714: t15.2025.03.14 val PER: 0.3876
2026-01-03 15:21:18,714: t15.2025.03.16 val PER: 0.2893
2026-01-03 15:21:18,714: t15.2025.03.30 val PER: 0.4115
2026-01-03 15:21:18,714: t15.2025.04.13 val PER: 0.3010
2026-01-03 15:21:18,715: New best val WER(1gram) 65.48% --> 62.18%
2026-01-03 15:21:18,715: Checkpointing model
2026-01-03 15:21:18,981: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 15:21:19,241: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_4500
2026-01-03 15:21:34,147: Train batch 4600: loss: 20.19 grad norm: 62.77 time: 0.144
2026-01-03 15:22:04,274: Train batch 4800: loss: 13.59 grad norm: 54.82 time: 0.146
2026-01-03 15:22:33,586: Train batch 5000: loss: 31.77 grad norm: 81.15 time: 0.147
2026-01-03 15:22:33,586: Running test after training batch: 5000
2026-01-03 15:22:33,704: WER debug GT example: You can see the code at this point as well.
2026-01-03 15:22:40,449: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-03 15:22:40,478: WER debug example
  GT : how does it keep the cost down
  PR : houde dest it heap the cost get
2026-01-03 15:22:42,130: Val batch 5000: PER (avg): 0.2259 CTC Loss (avg): 21.9680 WER(1gram): 61.93% (n=64) time: 8.544
2026-01-03 15:22:42,131: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 15:22:42,131: t15.2023.08.13 val PER: 0.1965
2026-01-03 15:22:42,131: t15.2023.08.18 val PER: 0.1760
2026-01-03 15:22:42,131: t15.2023.08.20 val PER: 0.1684
2026-01-03 15:22:42,131: t15.2023.08.25 val PER: 0.1401
2026-01-03 15:22:42,131: t15.2023.08.27 val PER: 0.2412
2026-01-03 15:22:42,131: t15.2023.09.01 val PER: 0.1404
2026-01-03 15:22:42,131: t15.2023.09.03 val PER: 0.2268
2026-01-03 15:22:42,132: t15.2023.09.24 val PER: 0.1857
2026-01-03 15:22:42,132: t15.2023.09.29 val PER: 0.1793
2026-01-03 15:22:42,132: t15.2023.10.01 val PER: 0.2358
2026-01-03 15:22:42,132: t15.2023.10.06 val PER: 0.1453
2026-01-03 15:22:42,132: t15.2023.10.08 val PER: 0.3112
2026-01-03 15:22:42,132: t15.2023.10.13 val PER: 0.2863
2026-01-03 15:22:42,132: t15.2023.10.15 val PER: 0.2215
2026-01-03 15:22:42,132: t15.2023.10.20 val PER: 0.2517
2026-01-03 15:22:42,132: t15.2023.10.22 val PER: 0.1637
2026-01-03 15:22:42,132: t15.2023.11.03 val PER: 0.2225
2026-01-03 15:22:42,132: t15.2023.11.04 val PER: 0.0580
2026-01-03 15:22:42,132: t15.2023.11.17 val PER: 0.0824
2026-01-03 15:22:42,132: t15.2023.11.19 val PER: 0.0699
2026-01-03 15:22:42,132: t15.2023.11.26 val PER: 0.2428
2026-01-03 15:22:42,133: t15.2023.12.03 val PER: 0.1870
2026-01-03 15:22:42,133: t15.2023.12.08 val PER: 0.2004
2026-01-03 15:22:42,133: t15.2023.12.10 val PER: 0.1708
2026-01-03 15:22:42,133: t15.2023.12.17 val PER: 0.2225
2026-01-03 15:22:42,133: t15.2023.12.29 val PER: 0.2251
2026-01-03 15:22:42,133: t15.2024.02.25 val PER: 0.1868
2026-01-03 15:22:42,133: t15.2024.03.08 val PER: 0.3172
2026-01-03 15:22:42,133: t15.2024.03.15 val PER: 0.2827
2026-01-03 15:22:42,133: t15.2024.03.17 val PER: 0.2315
2026-01-03 15:22:42,133: t15.2024.05.10 val PER: 0.2333
2026-01-03 15:22:42,133: t15.2024.06.14 val PER: 0.2382
2026-01-03 15:22:42,133: t15.2024.07.19 val PER: 0.3355
2026-01-03 15:22:42,133: t15.2024.07.21 val PER: 0.1814
2026-01-03 15:22:42,133: t15.2024.07.28 val PER: 0.2176
2026-01-03 15:22:42,133: t15.2025.01.10 val PER: 0.3843
2026-01-03 15:22:42,133: t15.2025.01.12 val PER: 0.2494
2026-01-03 15:22:42,133: t15.2025.03.14 val PER: 0.3861
2026-01-03 15:22:42,134: t15.2025.03.16 val PER: 0.2631
2026-01-03 15:22:42,134: t15.2025.03.30 val PER: 0.3862
2026-01-03 15:22:42,134: t15.2025.04.13 val PER: 0.3039
2026-01-03 15:22:42,135: New best val WER(1gram) 62.18% --> 61.93%
2026-01-03 15:22:42,135: Checkpointing model
2026-01-03 15:22:42,402: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 15:22:42,655: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_5000
2026-01-03 15:23:12,231: Train batch 5200: loss: 16.73 grad norm: 57.32 time: 0.118
2026-01-03 15:23:41,751: Train batch 5400: loss: 17.60 grad norm: 59.40 time: 0.155
2026-01-03 15:23:56,386: Running test after training batch: 5500
2026-01-03 15:23:56,503: WER debug GT example: You can see the code at this point as well.
2026-01-03 15:24:03,147: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 15:24:03,175: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost tet
2026-01-03 15:24:04,790: Val batch 5500: PER (avg): 0.2168 CTC Loss (avg): 21.0880 WER(1gram): 56.35% (n=64) time: 8.403
2026-01-03 15:24:04,790: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 15:24:04,790: t15.2023.08.13 val PER: 0.1850
2026-01-03 15:24:04,790: t15.2023.08.18 val PER: 0.1635
2026-01-03 15:24:04,790: t15.2023.08.20 val PER: 0.1755
2026-01-03 15:24:04,790: t15.2023.08.25 val PER: 0.1280
2026-01-03 15:24:04,791: t15.2023.08.27 val PER: 0.2379
2026-01-03 15:24:04,791: t15.2023.09.01 val PER: 0.1274
2026-01-03 15:24:04,791: t15.2023.09.03 val PER: 0.2245
2026-01-03 15:24:04,791: t15.2023.09.24 val PER: 0.1760
2026-01-03 15:24:04,791: t15.2023.09.29 val PER: 0.1761
2026-01-03 15:24:04,791: t15.2023.10.01 val PER: 0.2338
2026-01-03 15:24:04,791: t15.2023.10.06 val PER: 0.1410
2026-01-03 15:24:04,791: t15.2023.10.08 val PER: 0.2936
2026-01-03 15:24:04,791: t15.2023.10.13 val PER: 0.2607
2026-01-03 15:24:04,792: t15.2023.10.15 val PER: 0.2057
2026-01-03 15:24:04,792: t15.2023.10.20 val PER: 0.2416
2026-01-03 15:24:04,792: t15.2023.10.22 val PER: 0.1659
2026-01-03 15:24:04,792: t15.2023.11.03 val PER: 0.2212
2026-01-03 15:24:04,792: t15.2023.11.04 val PER: 0.0717
2026-01-03 15:24:04,792: t15.2023.11.17 val PER: 0.0840
2026-01-03 15:24:04,792: t15.2023.11.19 val PER: 0.0758
2026-01-03 15:24:04,792: t15.2023.11.26 val PER: 0.2254
2026-01-03 15:24:04,792: t15.2023.12.03 val PER: 0.1954
2026-01-03 15:24:04,792: t15.2023.12.08 val PER: 0.1931
2026-01-03 15:24:04,792: t15.2023.12.10 val PER: 0.1564
2026-01-03 15:24:04,792: t15.2023.12.17 val PER: 0.2058
2026-01-03 15:24:04,792: t15.2023.12.29 val PER: 0.2128
2026-01-03 15:24:04,792: t15.2024.02.25 val PER: 0.1784
2026-01-03 15:24:04,793: t15.2024.03.08 val PER: 0.2902
2026-01-03 15:24:04,793: t15.2024.03.15 val PER: 0.2714
2026-01-03 15:24:04,793: t15.2024.03.17 val PER: 0.2238
2026-01-03 15:24:04,793: t15.2024.05.10 val PER: 0.2169
2026-01-03 15:24:04,793: t15.2024.06.14 val PER: 0.2224
2026-01-03 15:24:04,793: t15.2024.07.19 val PER: 0.3184
2026-01-03 15:24:04,793: t15.2024.07.21 val PER: 0.1655
2026-01-03 15:24:04,793: t15.2024.07.28 val PER: 0.2169
2026-01-03 15:24:04,793: t15.2025.01.10 val PER: 0.3871
2026-01-03 15:24:04,793: t15.2025.01.12 val PER: 0.2333
2026-01-03 15:24:04,793: t15.2025.03.14 val PER: 0.3639
2026-01-03 15:24:04,794: t15.2025.03.16 val PER: 0.2657
2026-01-03 15:24:04,794: t15.2025.03.30 val PER: 0.3724
2026-01-03 15:24:04,794: t15.2025.04.13 val PER: 0.2953
2026-01-03 15:24:04,794: New best val WER(1gram) 61.93% --> 56.35%
2026-01-03 15:24:04,794: Checkpointing model
2026-01-03 15:24:05,064: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 15:24:05,317: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_5500
2026-01-03 15:24:20,237: Train batch 5600: loss: 20.21 grad norm: 71.32 time: 0.138
2026-01-03 15:24:50,043: Train batch 5800: loss: 13.98 grad norm: 59.69 time: 0.187
2026-01-03 15:25:19,423: Train batch 6000: loss: 13.91 grad norm: 54.43 time: 0.119
2026-01-03 15:25:19,424: Running test after training batch: 6000
2026-01-03 15:25:19,542: WER debug GT example: You can see the code at this point as well.
2026-01-03 15:25:26,330: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-03 15:25:26,362: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-03 15:25:28,077: Val batch 6000: PER (avg): 0.2121 CTC Loss (avg): 20.9164 WER(1gram): 58.63% (n=64) time: 8.652
2026-01-03 15:25:28,077: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 15:25:28,077: t15.2023.08.13 val PER: 0.1798
2026-01-03 15:25:28,077: t15.2023.08.18 val PER: 0.1668
2026-01-03 15:25:28,077: t15.2023.08.20 val PER: 0.1604
2026-01-03 15:25:28,077: t15.2023.08.25 val PER: 0.1220
2026-01-03 15:25:28,077: t15.2023.08.27 val PER: 0.2524
2026-01-03 15:25:28,078: t15.2023.09.01 val PER: 0.1323
2026-01-03 15:25:28,078: t15.2023.09.03 val PER: 0.2185
2026-01-03 15:25:28,078: t15.2023.09.24 val PER: 0.1650
2026-01-03 15:25:28,078: t15.2023.09.29 val PER: 0.1678
2026-01-03 15:25:28,078: t15.2023.10.01 val PER: 0.2252
2026-01-03 15:25:28,078: t15.2023.10.06 val PER: 0.1410
2026-01-03 15:25:28,078: t15.2023.10.08 val PER: 0.2923
2026-01-03 15:25:28,078: t15.2023.10.13 val PER: 0.2630
2026-01-03 15:25:28,078: t15.2023.10.15 val PER: 0.2156
2026-01-03 15:25:28,078: t15.2023.10.20 val PER: 0.2181
2026-01-03 15:25:28,078: t15.2023.10.22 val PER: 0.1670
2026-01-03 15:25:28,079: t15.2023.11.03 val PER: 0.2266
2026-01-03 15:25:28,079: t15.2023.11.04 val PER: 0.0648
2026-01-03 15:25:28,079: t15.2023.11.17 val PER: 0.0731
2026-01-03 15:25:28,079: t15.2023.11.19 val PER: 0.0798
2026-01-03 15:25:28,079: t15.2023.11.26 val PER: 0.2261
2026-01-03 15:25:28,079: t15.2023.12.03 val PER: 0.1660
2026-01-03 15:25:28,079: t15.2023.12.08 val PER: 0.1731
2026-01-03 15:25:28,079: t15.2023.12.10 val PER: 0.1603
2026-01-03 15:25:28,080: t15.2023.12.17 val PER: 0.2058
2026-01-03 15:25:28,080: t15.2023.12.29 val PER: 0.2251
2026-01-03 15:25:28,080: t15.2024.02.25 val PER: 0.1573
2026-01-03 15:25:28,080: t15.2024.03.08 val PER: 0.2959
2026-01-03 15:25:28,080: t15.2024.03.15 val PER: 0.2664
2026-01-03 15:25:28,080: t15.2024.03.17 val PER: 0.2176
2026-01-03 15:25:28,080: t15.2024.05.10 val PER: 0.2065
2026-01-03 15:25:28,080: t15.2024.06.14 val PER: 0.2256
2026-01-03 15:25:28,080: t15.2024.07.19 val PER: 0.3144
2026-01-03 15:25:28,081: t15.2024.07.21 val PER: 0.1531
2026-01-03 15:25:28,081: t15.2024.07.28 val PER: 0.2007
2026-01-03 15:25:28,081: t15.2025.01.10 val PER: 0.3926
2026-01-03 15:25:28,081: t15.2025.01.12 val PER: 0.2256
2026-01-03 15:25:28,081: t15.2025.03.14 val PER: 0.3713
2026-01-03 15:25:28,081: t15.2025.03.16 val PER: 0.2579
2026-01-03 15:25:28,081: t15.2025.03.30 val PER: 0.3586
2026-01-03 15:25:28,081: t15.2025.04.13 val PER: 0.2653
2026-01-03 15:25:28,325: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_6000
2026-01-03 15:25:58,393: Train batch 6200: loss: 16.86 grad norm: 57.58 time: 0.163
2026-01-03 15:26:28,000: Train batch 6400: loss: 18.80 grad norm: 67.46 time: 0.145
2026-01-03 15:26:42,712: Running test after training batch: 6500
2026-01-03 15:26:42,828: WER debug GT example: You can see the code at this point as well.
2026-01-03 15:26:49,457: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 15:26:49,487: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 15:26:51,085: Val batch 6500: PER (avg): 0.2029 CTC Loss (avg): 20.2647 WER(1gram): 54.57% (n=64) time: 8.373
2026-01-03 15:26:51,085: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 15:26:51,086: t15.2023.08.13 val PER: 0.1705
2026-01-03 15:26:51,086: t15.2023.08.18 val PER: 0.1366
2026-01-03 15:26:51,086: t15.2023.08.20 val PER: 0.1597
2026-01-03 15:26:51,086: t15.2023.08.25 val PER: 0.1084
2026-01-03 15:26:51,086: t15.2023.08.27 val PER: 0.2315
2026-01-03 15:26:51,086: t15.2023.09.01 val PER: 0.1193
2026-01-03 15:26:51,086: t15.2023.09.03 val PER: 0.2090
2026-01-03 15:26:51,086: t15.2023.09.24 val PER: 0.1626
2026-01-03 15:26:51,086: t15.2023.09.29 val PER: 0.1659
2026-01-03 15:26:51,086: t15.2023.10.01 val PER: 0.2213
2026-01-03 15:26:51,087: t15.2023.10.06 val PER: 0.1281
2026-01-03 15:26:51,087: t15.2023.10.08 val PER: 0.2882
2026-01-03 15:26:51,087: t15.2023.10.13 val PER: 0.2723
2026-01-03 15:26:51,087: t15.2023.10.15 val PER: 0.2096
2026-01-03 15:26:51,087: t15.2023.10.20 val PER: 0.2148
2026-01-03 15:26:51,087: t15.2023.10.22 val PER: 0.1615
2026-01-03 15:26:51,088: t15.2023.11.03 val PER: 0.2144
2026-01-03 15:26:51,088: t15.2023.11.04 val PER: 0.0444
2026-01-03 15:26:51,088: t15.2023.11.17 val PER: 0.0747
2026-01-03 15:26:51,088: t15.2023.11.19 val PER: 0.0739
2026-01-03 15:26:51,088: t15.2023.11.26 val PER: 0.2101
2026-01-03 15:26:51,088: t15.2023.12.03 val PER: 0.1712
2026-01-03 15:26:51,088: t15.2023.12.08 val PER: 0.1638
2026-01-03 15:26:51,088: t15.2023.12.10 val PER: 0.1380
2026-01-03 15:26:51,088: t15.2023.12.17 val PER: 0.1871
2026-01-03 15:26:51,088: t15.2023.12.29 val PER: 0.2052
2026-01-03 15:26:51,088: t15.2024.02.25 val PER: 0.1685
2026-01-03 15:26:51,088: t15.2024.03.08 val PER: 0.2717
2026-01-03 15:26:51,088: t15.2024.03.15 val PER: 0.2570
2026-01-03 15:26:51,089: t15.2024.03.17 val PER: 0.2001
2026-01-03 15:26:51,089: t15.2024.05.10 val PER: 0.2199
2026-01-03 15:26:51,089: t15.2024.06.14 val PER: 0.2145
2026-01-03 15:26:51,089: t15.2024.07.19 val PER: 0.3006
2026-01-03 15:26:51,089: t15.2024.07.21 val PER: 0.1476
2026-01-03 15:26:51,089: t15.2024.07.28 val PER: 0.1926
2026-01-03 15:26:51,089: t15.2025.01.10 val PER: 0.3664
2026-01-03 15:26:51,089: t15.2025.01.12 val PER: 0.2040
2026-01-03 15:26:51,089: t15.2025.03.14 val PER: 0.3950
2026-01-03 15:26:51,089: t15.2025.03.16 val PER: 0.2369
2026-01-03 15:26:51,089: t15.2025.03.30 val PER: 0.3460
2026-01-03 15:26:51,089: t15.2025.04.13 val PER: 0.2696
2026-01-03 15:26:51,090: New best val WER(1gram) 56.35% --> 54.57%
2026-01-03 15:26:51,090: Checkpointing model
2026-01-03 15:26:51,359: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 15:26:51,615: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_6500
2026-01-03 15:27:05,978: Train batch 6600: loss: 13.04 grad norm: 56.59 time: 0.103
2026-01-03 15:27:35,553: Train batch 6800: loss: 15.67 grad norm: 56.75 time: 0.111
2026-01-03 15:28:05,117: Train batch 7000: loss: 17.72 grad norm: 64.49 time: 0.135
2026-01-03 15:28:05,118: Running test after training batch: 7000
2026-01-03 15:28:05,235: WER debug GT example: You can see the code at this point as well.
2026-01-03 15:28:12,212: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-03 15:28:12,240: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost get
2026-01-03 15:28:13,874: Val batch 7000: PER (avg): 0.1958 CTC Loss (avg): 19.3744 WER(1gram): 53.81% (n=64) time: 8.756
2026-01-03 15:28:13,874: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 15:28:13,874: t15.2023.08.13 val PER: 0.1580
2026-01-03 15:28:13,874: t15.2023.08.18 val PER: 0.1383
2026-01-03 15:28:13,874: t15.2023.08.20 val PER: 0.1565
2026-01-03 15:28:13,874: t15.2023.08.25 val PER: 0.1009
2026-01-03 15:28:13,874: t15.2023.08.27 val PER: 0.2170
2026-01-03 15:28:13,874: t15.2023.09.01 val PER: 0.1153
2026-01-03 15:28:13,874: t15.2023.09.03 val PER: 0.1876
2026-01-03 15:28:13,875: t15.2023.09.24 val PER: 0.1566
2026-01-03 15:28:13,875: t15.2023.09.29 val PER: 0.1653
2026-01-03 15:28:13,875: t15.2023.10.01 val PER: 0.2107
2026-01-03 15:28:13,875: t15.2023.10.06 val PER: 0.1173
2026-01-03 15:28:13,875: t15.2023.10.08 val PER: 0.2828
2026-01-03 15:28:13,875: t15.2023.10.13 val PER: 0.2622
2026-01-03 15:28:13,875: t15.2023.10.15 val PER: 0.1879
2026-01-03 15:28:13,875: t15.2023.10.20 val PER: 0.2248
2026-01-03 15:28:13,875: t15.2023.10.22 val PER: 0.1481
2026-01-03 15:28:13,875: t15.2023.11.03 val PER: 0.2049
2026-01-03 15:28:13,875: t15.2023.11.04 val PER: 0.0512
2026-01-03 15:28:13,875: t15.2023.11.17 val PER: 0.0607
2026-01-03 15:28:13,875: t15.2023.11.19 val PER: 0.0659
2026-01-03 15:28:13,876: t15.2023.11.26 val PER: 0.2029
2026-01-03 15:28:13,876: t15.2023.12.03 val PER: 0.1618
2026-01-03 15:28:13,876: t15.2023.12.08 val PER: 0.1558
2026-01-03 15:28:13,876: t15.2023.12.10 val PER: 0.1445
2026-01-03 15:28:13,876: t15.2023.12.17 val PER: 0.1694
2026-01-03 15:28:13,876: t15.2023.12.29 val PER: 0.1970
2026-01-03 15:28:13,876: t15.2024.02.25 val PER: 0.1573
2026-01-03 15:28:13,876: t15.2024.03.08 val PER: 0.2774
2026-01-03 15:28:13,876: t15.2024.03.15 val PER: 0.2502
2026-01-03 15:28:13,877: t15.2024.03.17 val PER: 0.2036
2026-01-03 15:28:13,877: t15.2024.05.10 val PER: 0.2199
2026-01-03 15:28:13,877: t15.2024.06.14 val PER: 0.2114
2026-01-03 15:28:13,877: t15.2024.07.19 val PER: 0.3039
2026-01-03 15:28:13,877: t15.2024.07.21 val PER: 0.1379
2026-01-03 15:28:13,877: t15.2024.07.28 val PER: 0.1765
2026-01-03 15:28:13,877: t15.2025.01.10 val PER: 0.3595
2026-01-03 15:28:13,877: t15.2025.01.12 val PER: 0.2002
2026-01-03 15:28:13,877: t15.2025.03.14 val PER: 0.3550
2026-01-03 15:28:13,877: t15.2025.03.16 val PER: 0.2435
2026-01-03 15:28:13,877: t15.2025.03.30 val PER: 0.3598
2026-01-03 15:28:13,877: t15.2025.04.13 val PER: 0.2568
2026-01-03 15:28:13,878: New best val WER(1gram) 54.57% --> 53.81%
2026-01-03 15:28:13,878: Checkpointing model
2026-01-03 15:28:14,148: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 15:28:14,399: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_7000
2026-01-03 15:28:44,084: Train batch 7200: loss: 15.03 grad norm: 61.62 time: 0.180
2026-01-03 15:29:12,786: Train batch 7400: loss: 14.09 grad norm: 57.53 time: 0.173
2026-01-03 15:29:27,148: Running test after training batch: 7500
2026-01-03 15:29:27,270: WER debug GT example: You can see the code at this point as well.
2026-01-03 15:29:33,891: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 15:29:33,921: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost sette
2026-01-03 15:29:35,597: Val batch 7500: PER (avg): 0.1895 CTC Loss (avg): 18.7131 WER(1gram): 53.55% (n=64) time: 8.448
2026-01-03 15:29:35,597: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 15:29:35,597: t15.2023.08.13 val PER: 0.1528
2026-01-03 15:29:35,597: t15.2023.08.18 val PER: 0.1391
2026-01-03 15:29:35,598: t15.2023.08.20 val PER: 0.1454
2026-01-03 15:29:35,598: t15.2023.08.25 val PER: 0.1145
2026-01-03 15:29:35,598: t15.2023.08.27 val PER: 0.2074
2026-01-03 15:29:35,598: t15.2023.09.01 val PER: 0.1120
2026-01-03 15:29:35,598: t15.2023.09.03 val PER: 0.1900
2026-01-03 15:29:35,598: t15.2023.09.24 val PER: 0.1566
2026-01-03 15:29:35,598: t15.2023.09.29 val PER: 0.1608
2026-01-03 15:29:35,598: t15.2023.10.01 val PER: 0.2067
2026-01-03 15:29:35,598: t15.2023.10.06 val PER: 0.1087
2026-01-03 15:29:35,598: t15.2023.10.08 val PER: 0.2679
2026-01-03 15:29:35,598: t15.2023.10.13 val PER: 0.2490
2026-01-03 15:29:35,598: t15.2023.10.15 val PER: 0.1997
2026-01-03 15:29:35,598: t15.2023.10.20 val PER: 0.1779
2026-01-03 15:29:35,599: t15.2023.10.22 val PER: 0.1370
2026-01-03 15:29:35,599: t15.2023.11.03 val PER: 0.1974
2026-01-03 15:29:35,599: t15.2023.11.04 val PER: 0.0478
2026-01-03 15:29:35,599: t15.2023.11.17 val PER: 0.0653
2026-01-03 15:29:35,599: t15.2023.11.19 val PER: 0.0559
2026-01-03 15:29:35,599: t15.2023.11.26 val PER: 0.1906
2026-01-03 15:29:35,599: t15.2023.12.03 val PER: 0.1649
2026-01-03 15:29:35,599: t15.2023.12.08 val PER: 0.1471
2026-01-03 15:29:35,599: t15.2023.12.10 val PER: 0.1340
2026-01-03 15:29:35,599: t15.2023.12.17 val PER: 0.1830
2026-01-03 15:29:35,599: t15.2023.12.29 val PER: 0.1784
2026-01-03 15:29:35,600: t15.2024.02.25 val PER: 0.1461
2026-01-03 15:29:35,600: t15.2024.03.08 val PER: 0.2646
2026-01-03 15:29:35,600: t15.2024.03.15 val PER: 0.2439
2026-01-03 15:29:35,600: t15.2024.03.17 val PER: 0.1841
2026-01-03 15:29:35,600: t15.2024.05.10 val PER: 0.2051
2026-01-03 15:29:35,600: t15.2024.06.14 val PER: 0.1924
2026-01-03 15:29:35,600: t15.2024.07.19 val PER: 0.2854
2026-01-03 15:29:35,600: t15.2024.07.21 val PER: 0.1359
2026-01-03 15:29:35,600: t15.2024.07.28 val PER: 0.1750
2026-01-03 15:29:35,600: t15.2025.01.10 val PER: 0.3540
2026-01-03 15:29:35,600: t15.2025.01.12 val PER: 0.1925
2026-01-03 15:29:35,600: t15.2025.03.14 val PER: 0.3713
2026-01-03 15:29:35,600: t15.2025.03.16 val PER: 0.2421
2026-01-03 15:29:35,600: t15.2025.03.30 val PER: 0.3586
2026-01-03 15:29:35,600: t15.2025.04.13 val PER: 0.2454
2026-01-03 15:29:35,601: New best val WER(1gram) 53.81% --> 53.55%
2026-01-03 15:29:35,602: Checkpointing model
2026-01-03 15:29:35,868: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 15:29:36,120: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_7500
2026-01-03 15:29:51,037: Train batch 7600: loss: 16.58 grad norm: 62.11 time: 0.158
2026-01-03 15:30:20,532: Train batch 7800: loss: 14.35 grad norm: 58.40 time: 0.127
2026-01-03 15:30:50,413: Train batch 8000: loss: 11.67 grad norm: 50.68 time: 0.165
2026-01-03 15:30:50,413: Running test after training batch: 8000
2026-01-03 15:30:50,529: WER debug GT example: You can see the code at this point as well.
2026-01-03 15:30:57,351: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 15:30:57,381: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost it
2026-01-03 15:30:59,035: Val batch 8000: PER (avg): 0.1854 CTC Loss (avg): 18.1244 WER(1gram): 53.55% (n=64) time: 8.622
2026-01-03 15:30:59,036: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 15:30:59,036: t15.2023.08.13 val PER: 0.1497
2026-01-03 15:30:59,036: t15.2023.08.18 val PER: 0.1257
2026-01-03 15:30:59,036: t15.2023.08.20 val PER: 0.1477
2026-01-03 15:30:59,036: t15.2023.08.25 val PER: 0.1175
2026-01-03 15:30:59,036: t15.2023.08.27 val PER: 0.2138
2026-01-03 15:30:59,036: t15.2023.09.01 val PER: 0.1039
2026-01-03 15:30:59,036: t15.2023.09.03 val PER: 0.1841
2026-01-03 15:30:59,036: t15.2023.09.24 val PER: 0.1602
2026-01-03 15:30:59,037: t15.2023.09.29 val PER: 0.1474
2026-01-03 15:30:59,037: t15.2023.10.01 val PER: 0.2001
2026-01-03 15:30:59,037: t15.2023.10.06 val PER: 0.1044
2026-01-03 15:30:59,037: t15.2023.10.08 val PER: 0.2652
2026-01-03 15:30:59,037: t15.2023.10.13 val PER: 0.2459
2026-01-03 15:30:59,037: t15.2023.10.15 val PER: 0.1938
2026-01-03 15:30:59,037: t15.2023.10.20 val PER: 0.2013
2026-01-03 15:30:59,037: t15.2023.10.22 val PER: 0.1370
2026-01-03 15:30:59,037: t15.2023.11.03 val PER: 0.2062
2026-01-03 15:30:59,037: t15.2023.11.04 val PER: 0.0307
2026-01-03 15:30:59,037: t15.2023.11.17 val PER: 0.0591
2026-01-03 15:30:59,037: t15.2023.11.19 val PER: 0.0599
2026-01-03 15:30:59,037: t15.2023.11.26 val PER: 0.1790
2026-01-03 15:30:59,037: t15.2023.12.03 val PER: 0.1513
2026-01-03 15:30:59,037: t15.2023.12.08 val PER: 0.1471
2026-01-03 15:30:59,038: t15.2023.12.10 val PER: 0.1248
2026-01-03 15:30:59,038: t15.2023.12.17 val PER: 0.1798
2026-01-03 15:30:59,038: t15.2023.12.29 val PER: 0.1839
2026-01-03 15:30:59,038: t15.2024.02.25 val PER: 0.1475
2026-01-03 15:30:59,038: t15.2024.03.08 val PER: 0.2802
2026-01-03 15:30:59,038: t15.2024.03.15 val PER: 0.2477
2026-01-03 15:30:59,038: t15.2024.03.17 val PER: 0.1785
2026-01-03 15:30:59,038: t15.2024.05.10 val PER: 0.1902
2026-01-03 15:30:59,038: t15.2024.06.14 val PER: 0.2066
2026-01-03 15:30:59,038: t15.2024.07.19 val PER: 0.2828
2026-01-03 15:30:59,038: t15.2024.07.21 val PER: 0.1207
2026-01-03 15:30:59,038: t15.2024.07.28 val PER: 0.1610
2026-01-03 15:30:59,038: t15.2025.01.10 val PER: 0.3182
2026-01-03 15:30:59,038: t15.2025.01.12 val PER: 0.1971
2026-01-03 15:30:59,038: t15.2025.03.14 val PER: 0.3536
2026-01-03 15:30:59,038: t15.2025.03.16 val PER: 0.2160
2026-01-03 15:30:59,039: t15.2025.03.30 val PER: 0.3586
2026-01-03 15:30:59,039: t15.2025.04.13 val PER: 0.2582
2026-01-03 15:30:59,283: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_8000
