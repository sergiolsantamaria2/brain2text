TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_351046
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_351046/torch_extensions
WANDB_DIR=/tmp/e12511253_b2t_351046/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/e12511253_b2t_351046/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  8 12:34 /tmp/e12511253_b2t_351046/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
OUT_ROOT=/home/e12511253/Brain2Text/brain2text/trained_models
==============================================
Job: b2t  ID: 351046
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/gru/high_priority
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/gru/high_priority ==========
Num configs: 4

=== RUN head_2blocks_ln.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_2blocks_ln
2026-01-08 12:34:13,249: Using device: cuda:0
2026-01-08 12:34:15,062: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-08 12:34:15,085: Using 45 sessions after filtering (from 45).
2026-01-08 12:34:15,534: Using torch.compile (if available)
2026-01-08 12:34:15,535: torch.compile not available (torch<2.0). Skipping.
2026-01-08 12:34:15,535: Initialized RNN decoding model
2026-01-08 12:34:15,535: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
    (1): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-08 12:34:15,535: Model has 45,499,433 parameters
2026-01-08 12:34:15,535: Model has 11,819,520 day-specific parameters | 25.98% of total parameters
2026-01-08 12:34:16,855: Successfully initialized datasets
2026-01-08 12:34:16,856: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-08 12:34:17,861: Train batch 0: loss: 600.34 grad norm: 2334.58 time: 0.185
2026-01-08 12:34:17,861: Running test after training batch: 0
2026-01-08 12:34:17,971: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:34:22,684: WER debug example
  GT : you can see the code at this point as well
  PR : gilles hwang towle shout hetu
2026-01-08 12:34:22,742: WER debug example
  GT : how does it keep the cost down
  PR : willhelm ballyhoo
2026-01-08 12:34:27,739: Val batch 0: PER (avg): 1.3381 CTC Loss (avg): 626.6909 WER(1gram): 100.00% (n=64) time: 9.878
2026-01-08 12:34:27,739: WER lens: avg_true_words=6.16 avg_pred_words=1.55 max_pred_words=5
2026-01-08 12:34:27,740: t15.2023.08.13 val PER: 1.2037
2026-01-08 12:34:27,740: t15.2023.08.18 val PER: 1.2833
2026-01-08 12:34:27,740: t15.2023.08.20 val PER: 1.2740
2026-01-08 12:34:27,740: t15.2023.08.25 val PER: 1.3404
2026-01-08 12:34:27,740: t15.2023.08.27 val PER: 1.2058
2026-01-08 12:34:27,740: t15.2023.09.01 val PER: 1.3369
2026-01-08 12:34:27,740: t15.2023.09.03 val PER: 1.1746
2026-01-08 12:34:27,740: t15.2023.09.24 val PER: 1.4430
2026-01-08 12:34:27,741: t15.2023.09.29 val PER: 1.4773
2026-01-08 12:34:27,741: t15.2023.10.01 val PER: 1.1684
2026-01-08 12:34:27,741: t15.2023.10.06 val PER: 1.4273
2026-01-08 12:34:27,741: t15.2023.10.08 val PER: 1.1191
2026-01-08 12:34:27,741: t15.2023.10.13 val PER: 1.3569
2026-01-08 12:34:27,741: t15.2023.10.15 val PER: 1.3731
2026-01-08 12:34:27,741: t15.2023.10.20 val PER: 1.4866
2026-01-08 12:34:27,741: t15.2023.10.22 val PER: 1.3886
2026-01-08 12:34:27,741: t15.2023.11.03 val PER: 1.5631
2026-01-08 12:34:27,741: t15.2023.11.04 val PER: 1.5495
2026-01-08 12:34:27,741: t15.2023.11.17 val PER: 1.8398
2026-01-08 12:34:27,741: t15.2023.11.19 val PER: 1.5689
2026-01-08 12:34:27,742: t15.2023.11.26 val PER: 1.4188
2026-01-08 12:34:27,742: t15.2023.12.03 val PER: 1.3351
2026-01-08 12:34:27,742: t15.2023.12.08 val PER: 1.4068
2026-01-08 12:34:27,742: t15.2023.12.10 val PER: 1.4534
2026-01-08 12:34:27,742: t15.2023.12.17 val PER: 1.2391
2026-01-08 12:34:27,742: t15.2023.12.29 val PER: 1.2677
2026-01-08 12:34:27,742: t15.2024.02.25 val PER: 1.3174
2026-01-08 12:34:27,742: t15.2024.03.08 val PER: 1.2176
2026-01-08 12:34:27,742: t15.2024.03.15 val PER: 1.2745
2026-01-08 12:34:27,742: t15.2024.03.17 val PER: 1.3368
2026-01-08 12:34:27,742: t15.2024.05.10 val PER: 1.3744
2026-01-08 12:34:27,742: t15.2024.06.14 val PER: 1.4259
2026-01-08 12:34:27,743: t15.2024.07.19 val PER: 1.0000
2026-01-08 12:34:27,743: t15.2024.07.21 val PER: 1.4524
2026-01-08 12:34:27,743: t15.2024.07.28 val PER: 1.4728
2026-01-08 12:34:27,743: t15.2025.01.10 val PER: 0.9848
2026-01-08 12:34:27,743: t15.2025.01.12 val PER: 1.5304
2026-01-08 12:34:27,743: t15.2025.03.14 val PER: 1.0222
2026-01-08 12:34:27,743: t15.2025.03.16 val PER: 1.4476
2026-01-08 12:34:27,743: t15.2025.03.30 val PER: 1.2241
2026-01-08 12:34:27,743: t15.2025.04.13 val PER: 1.2625
2026-01-08 12:34:27,744: New best val WER(1gram) inf% --> 100.00%
2026-01-08 12:34:27,744: Checkpointing model
2026-01-08 12:34:27,885: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_2blocks_ln/checkpoint/best_checkpoint
2026-01-08 12:34:45,030: Train batch 200: loss: 93.79 grad norm: 195.29 time: 0.056
2026-01-08 12:35:01,524: Train batch 400: loss: 63.75 grad norm: 92.16 time: 0.065
2026-01-08 12:35:09,965: Running test after training batch: 500
2026-01-08 12:35:10,099: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:35:14,844: WER debug example
  GT : you can see the code at this point as well
  PR : hollyhead stees thus hodapp resisted tis
2026-01-08 12:35:14,934: WER debug example
  GT : how does it keep the cost down
  PR : knighthood sidhu lithe
2026-01-08 12:35:19,597: Val batch 500: PER (avg): 0.6178 CTC Loss (avg): 67.3291 WER(1gram): 99.24% (n=64) time: 9.631
2026-01-08 12:35:19,597: WER lens: avg_true_words=6.16 avg_pred_words=3.33 max_pred_words=7
2026-01-08 12:35:19,598: t15.2023.08.13 val PER: 0.5800
2026-01-08 12:35:19,598: t15.2023.08.18 val PER: 0.5574
2026-01-08 12:35:19,598: t15.2023.08.20 val PER: 0.5473
2026-01-08 12:35:19,598: t15.2023.08.25 val PER: 0.5557
2026-01-08 12:35:19,598: t15.2023.08.27 val PER: 0.6029
2026-01-08 12:35:19,598: t15.2023.09.01 val PER: 0.5487
2026-01-08 12:35:19,598: t15.2023.09.03 val PER: 0.6021
2026-01-08 12:35:19,599: t15.2023.09.24 val PER: 0.5473
2026-01-08 12:35:19,599: t15.2023.09.29 val PER: 0.5641
2026-01-08 12:35:19,599: t15.2023.10.01 val PER: 0.6136
2026-01-08 12:35:19,599: t15.2023.10.06 val PER: 0.5759
2026-01-08 12:35:19,599: t15.2023.10.08 val PER: 0.5981
2026-01-08 12:35:19,599: t15.2023.10.13 val PER: 0.6680
2026-01-08 12:35:19,599: t15.2023.10.15 val PER: 0.6058
2026-01-08 12:35:19,599: t15.2023.10.20 val PER: 0.5470
2026-01-08 12:35:19,599: t15.2023.10.22 val PER: 0.5969
2026-01-08 12:35:19,599: t15.2023.11.03 val PER: 0.6547
2026-01-08 12:35:19,599: t15.2023.11.04 val PER: 0.4881
2026-01-08 12:35:19,600: t15.2023.11.17 val PER: 0.5241
2026-01-08 12:35:19,600: t15.2023.11.19 val PER: 0.4611
2026-01-08 12:35:19,600: t15.2023.11.26 val PER: 0.6196
2026-01-08 12:35:19,600: t15.2023.12.03 val PER: 0.6040
2026-01-08 12:35:19,600: t15.2023.12.08 val PER: 0.5972
2026-01-08 12:35:19,600: t15.2023.12.10 val PER: 0.5703
2026-01-08 12:35:19,600: t15.2023.12.17 val PER: 0.6351
2026-01-08 12:35:19,600: t15.2023.12.29 val PER: 0.5930
2026-01-08 12:35:19,600: t15.2024.02.25 val PER: 0.6025
2026-01-08 12:35:19,600: t15.2024.03.08 val PER: 0.6671
2026-01-08 12:35:19,600: t15.2024.03.15 val PER: 0.6554
2026-01-08 12:35:19,600: t15.2024.03.17 val PER: 0.6060
2026-01-08 12:35:19,600: t15.2024.05.10 val PER: 0.6152
2026-01-08 12:35:19,600: t15.2024.06.14 val PER: 0.6751
2026-01-08 12:35:19,600: t15.2024.07.19 val PER: 0.7165
2026-01-08 12:35:19,600: t15.2024.07.21 val PER: 0.6083
2026-01-08 12:35:19,600: t15.2024.07.28 val PER: 0.6566
2026-01-08 12:35:19,601: t15.2025.01.10 val PER: 0.7769
2026-01-08 12:35:19,601: t15.2025.01.12 val PER: 0.6651
2026-01-08 12:35:19,601: t15.2025.03.14 val PER: 0.7396
2026-01-08 12:35:19,601: t15.2025.03.16 val PER: 0.6885
2026-01-08 12:35:19,601: t15.2025.03.30 val PER: 0.7701
2026-01-08 12:35:19,601: t15.2025.04.13 val PER: 0.6534
2026-01-08 12:35:19,602: New best val WER(1gram) 100.00% --> 99.24%
2026-01-08 12:35:19,602: Checkpointing model
2026-01-08 12:35:19,743: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_2blocks_ln/checkpoint/best_checkpoint
2026-01-08 12:35:28,260: Train batch 600: loss: 59.45 grad norm: 96.56 time: 0.081
2026-01-08 12:35:44,704: Train batch 800: loss: 50.95 grad norm: 96.29 time: 0.060
2026-01-08 12:36:02,031: Train batch 1000: loss: 51.01 grad norm: 99.35 time: 0.068
2026-01-08 12:36:02,031: Running test after training batch: 1000
2026-01-08 12:36:02,158: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:36:06,864: WER debug example
  GT : you can see the code at this point as well
  PR : hudak indices sutnick advocates spends
2026-01-08 12:36:06,910: WER debug example
  GT : how does it keep the cost down
  PR : diehards tusks thus
2026-01-08 12:36:10,138: Val batch 1000: PER (avg): 0.4769 CTC Loss (avg): 51.2925 WER(1gram): 99.24% (n=64) time: 8.107
2026-01-08 12:36:10,138: WER lens: avg_true_words=6.16 avg_pred_words=4.05 max_pred_words=8
2026-01-08 12:36:10,138: t15.2023.08.13 val PER: 0.4428
2026-01-08 12:36:10,138: t15.2023.08.18 val PER: 0.4191
2026-01-08 12:36:10,138: t15.2023.08.20 val PER: 0.4178
2026-01-08 12:36:10,139: t15.2023.08.25 val PER: 0.3825
2026-01-08 12:36:10,139: t15.2023.08.27 val PER: 0.5209
2026-01-08 12:36:10,139: t15.2023.09.01 val PER: 0.4002
2026-01-08 12:36:10,139: t15.2023.09.03 val PER: 0.5036
2026-01-08 12:36:10,139: t15.2023.09.24 val PER: 0.4005
2026-01-08 12:36:10,139: t15.2023.09.29 val PER: 0.4410
2026-01-08 12:36:10,139: t15.2023.10.01 val PER: 0.4716
2026-01-08 12:36:10,139: t15.2023.10.06 val PER: 0.4037
2026-01-08 12:36:10,140: t15.2023.10.08 val PER: 0.5223
2026-01-08 12:36:10,140: t15.2023.10.13 val PER: 0.5446
2026-01-08 12:36:10,140: t15.2023.10.15 val PER: 0.4450
2026-01-08 12:36:10,140: t15.2023.10.20 val PER: 0.4228
2026-01-08 12:36:10,140: t15.2023.10.22 val PER: 0.4298
2026-01-08 12:36:10,140: t15.2023.11.03 val PER: 0.4512
2026-01-08 12:36:10,140: t15.2023.11.04 val PER: 0.2287
2026-01-08 12:36:10,140: t15.2023.11.17 val PER: 0.3406
2026-01-08 12:36:10,141: t15.2023.11.19 val PER: 0.2934
2026-01-08 12:36:10,141: t15.2023.11.26 val PER: 0.5203
2026-01-08 12:36:10,141: t15.2023.12.03 val PER: 0.4779
2026-01-08 12:36:10,141: t15.2023.12.08 val PER: 0.4700
2026-01-08 12:36:10,141: t15.2023.12.10 val PER: 0.4100
2026-01-08 12:36:10,141: t15.2023.12.17 val PER: 0.4730
2026-01-08 12:36:10,141: t15.2023.12.29 val PER: 0.4791
2026-01-08 12:36:10,141: t15.2024.02.25 val PER: 0.4129
2026-01-08 12:36:10,141: t15.2024.03.08 val PER: 0.5647
2026-01-08 12:36:10,141: t15.2024.03.15 val PER: 0.4916
2026-01-08 12:36:10,141: t15.2024.03.17 val PER: 0.4700
2026-01-08 12:36:10,142: t15.2024.05.10 val PER: 0.4785
2026-01-08 12:36:10,142: t15.2024.06.14 val PER: 0.4716
2026-01-08 12:36:10,142: t15.2024.07.19 val PER: 0.6203
2026-01-08 12:36:10,142: t15.2024.07.21 val PER: 0.4317
2026-01-08 12:36:10,142: t15.2024.07.28 val PER: 0.4574
2026-01-08 12:36:10,142: t15.2025.01.10 val PER: 0.6983
2026-01-08 12:36:10,142: t15.2025.01.12 val PER: 0.5142
2026-01-08 12:36:10,142: t15.2025.03.14 val PER: 0.6879
2026-01-08 12:36:10,142: t15.2025.03.16 val PER: 0.5092
2026-01-08 12:36:10,142: t15.2025.03.30 val PER: 0.6931
2026-01-08 12:36:10,142: t15.2025.04.13 val PER: 0.5292
2026-01-08 12:36:27,700: Train batch 1200: loss: 44.13 grad norm: 100.35 time: 0.069
2026-01-08 12:36:44,783: Train batch 1400: loss: 43.55 grad norm: 94.31 time: 0.062
2026-01-08 12:36:53,539: Running test after training batch: 1500
2026-01-08 12:36:53,638: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:36:58,319: WER debug example
  GT : you can see the code at this point as well
  PR : hughley cadmium thusly handicapped eastbound tis
2026-01-08 12:36:58,364: WER debug example
  GT : how does it keep the cost down
  PR : holds pizzi hitty skeets thus
2026-01-08 12:37:00,608: Val batch 1500: PER (avg): 0.4234 CTC Loss (avg): 44.6479 WER(1gram): 96.95% (n=64) time: 7.068
2026-01-08 12:37:00,608: WER lens: avg_true_words=6.16 avg_pred_words=4.61 max_pred_words=11
2026-01-08 12:37:00,608: t15.2023.08.13 val PER: 0.3981
2026-01-08 12:37:00,609: t15.2023.08.18 val PER: 0.3831
2026-01-08 12:37:00,609: t15.2023.08.20 val PER: 0.3709
2026-01-08 12:37:00,609: t15.2023.08.25 val PER: 0.3283
2026-01-08 12:37:00,609: t15.2023.08.27 val PER: 0.4518
2026-01-08 12:37:00,609: t15.2023.09.01 val PER: 0.3612
2026-01-08 12:37:00,609: t15.2023.09.03 val PER: 0.4287
2026-01-08 12:37:00,609: t15.2023.09.24 val PER: 0.3434
2026-01-08 12:37:00,609: t15.2023.09.29 val PER: 0.3765
2026-01-08 12:37:00,609: t15.2023.10.01 val PER: 0.4207
2026-01-08 12:37:00,609: t15.2023.10.06 val PER: 0.3520
2026-01-08 12:37:00,610: t15.2023.10.08 val PER: 0.4763
2026-01-08 12:37:00,610: t15.2023.10.13 val PER: 0.4872
2026-01-08 12:37:00,610: t15.2023.10.15 val PER: 0.4087
2026-01-08 12:37:00,610: t15.2023.10.20 val PER: 0.4161
2026-01-08 12:37:00,610: t15.2023.10.22 val PER: 0.3641
2026-01-08 12:37:00,610: t15.2023.11.03 val PER: 0.4084
2026-01-08 12:37:00,610: t15.2023.11.04 val PER: 0.1809
2026-01-08 12:37:00,610: t15.2023.11.17 val PER: 0.2830
2026-01-08 12:37:00,610: t15.2023.11.19 val PER: 0.2415
2026-01-08 12:37:00,610: t15.2023.11.26 val PER: 0.4601
2026-01-08 12:37:00,610: t15.2023.12.03 val PER: 0.4223
2026-01-08 12:37:00,610: t15.2023.12.08 val PER: 0.4088
2026-01-08 12:37:00,610: t15.2023.12.10 val PER: 0.3627
2026-01-08 12:37:00,610: t15.2023.12.17 val PER: 0.4127
2026-01-08 12:37:00,610: t15.2023.12.29 val PER: 0.4097
2026-01-08 12:37:00,610: t15.2024.02.25 val PER: 0.3778
2026-01-08 12:37:00,611: t15.2024.03.08 val PER: 0.4723
2026-01-08 12:37:00,611: t15.2024.03.15 val PER: 0.4346
2026-01-08 12:37:00,611: t15.2024.03.17 val PER: 0.4226
2026-01-08 12:37:00,611: t15.2024.05.10 val PER: 0.4160
2026-01-08 12:37:00,611: t15.2024.06.14 val PER: 0.4227
2026-01-08 12:37:00,611: t15.2024.07.19 val PER: 0.5630
2026-01-08 12:37:00,611: t15.2024.07.21 val PER: 0.3559
2026-01-08 12:37:00,611: t15.2024.07.28 val PER: 0.4081
2026-01-08 12:37:00,611: t15.2025.01.10 val PER: 0.6377
2026-01-08 12:37:00,611: t15.2025.01.12 val PER: 0.4611
2026-01-08 12:37:00,611: t15.2025.03.14 val PER: 0.6346
2026-01-08 12:37:00,611: t15.2025.03.16 val PER: 0.4751
2026-01-08 12:37:00,611: t15.2025.03.30 val PER: 0.6552
2026-01-08 12:37:00,611: t15.2025.04.13 val PER: 0.4964
2026-01-08 12:37:00,613: New best val WER(1gram) 99.24% --> 96.95%
2026-01-08 12:37:00,613: Checkpointing model
2026-01-08 12:37:00,747: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_2blocks_ln/checkpoint/best_checkpoint
2026-01-08 12:37:08,993: Train batch 1600: loss: 40.98 grad norm: 78.13 time: 0.066
2026-01-08 12:37:25,616: Train batch 1800: loss: 41.33 grad norm: 82.24 time: 0.092
2026-01-08 12:37:42,449: Train batch 2000: loss: 38.72 grad norm: 77.31 time: 0.069
2026-01-08 12:37:42,449: Running test after training batch: 2000
2026-01-08 12:37:42,574: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:37:47,275: WER debug example
  GT : you can see the code at this point as well
  PR : hudy caddy seixas thus ahold hotspot his squeals
2026-01-08 12:37:47,310: WER debug example
  GT : how does it keep the cost down
  PR : jihadists hitty sikhs that
2026-01-08 12:37:49,251: Val batch 2000: PER (avg): 0.3860 CTC Loss (avg): 40.6910 WER(1gram): 98.98% (n=64) time: 6.801
2026-01-08 12:37:49,251: WER lens: avg_true_words=6.16 avg_pred_words=4.88 max_pred_words=12
2026-01-08 12:37:49,251: t15.2023.08.13 val PER: 0.3545
2026-01-08 12:37:49,251: t15.2023.08.18 val PER: 0.3403
2026-01-08 12:37:49,251: t15.2023.08.20 val PER: 0.3296
2026-01-08 12:37:49,251: t15.2023.08.25 val PER: 0.2922
2026-01-08 12:37:49,252: t15.2023.08.27 val PER: 0.4019
2026-01-08 12:37:49,252: t15.2023.09.01 val PER: 0.3109
2026-01-08 12:37:49,252: t15.2023.09.03 val PER: 0.3872
2026-01-08 12:37:49,252: t15.2023.09.24 val PER: 0.3228
2026-01-08 12:37:49,252: t15.2023.09.29 val PER: 0.3274
2026-01-08 12:37:49,252: t15.2023.10.01 val PER: 0.3838
2026-01-08 12:37:49,252: t15.2023.10.06 val PER: 0.3057
2026-01-08 12:37:49,252: t15.2023.10.08 val PER: 0.4411
2026-01-08 12:37:49,252: t15.2023.10.13 val PER: 0.4663
2026-01-08 12:37:49,252: t15.2023.10.15 val PER: 0.3566
2026-01-08 12:37:49,252: t15.2023.10.20 val PER: 0.3859
2026-01-08 12:37:49,252: t15.2023.10.22 val PER: 0.3330
2026-01-08 12:37:49,252: t15.2023.11.03 val PER: 0.3752
2026-01-08 12:37:49,252: t15.2023.11.04 val PER: 0.1741
2026-01-08 12:37:49,252: t15.2023.11.17 val PER: 0.2208
2026-01-08 12:37:49,253: t15.2023.11.19 val PER: 0.2236
2026-01-08 12:37:49,253: t15.2023.11.26 val PER: 0.4348
2026-01-08 12:37:49,253: t15.2023.12.03 val PER: 0.3761
2026-01-08 12:37:49,253: t15.2023.12.08 val PER: 0.3828
2026-01-08 12:37:49,253: t15.2023.12.10 val PER: 0.3233
2026-01-08 12:37:49,253: t15.2023.12.17 val PER: 0.3399
2026-01-08 12:37:49,253: t15.2023.12.29 val PER: 0.3830
2026-01-08 12:37:49,253: t15.2024.02.25 val PER: 0.3427
2026-01-08 12:37:49,253: t15.2024.03.08 val PER: 0.4509
2026-01-08 12:37:49,253: t15.2024.03.15 val PER: 0.4003
2026-01-08 12:37:49,253: t15.2024.03.17 val PER: 0.4024
2026-01-08 12:37:49,253: t15.2024.05.10 val PER: 0.3774
2026-01-08 12:37:49,253: t15.2024.06.14 val PER: 0.4085
2026-01-08 12:37:49,253: t15.2024.07.19 val PER: 0.5036
2026-01-08 12:37:49,253: t15.2024.07.21 val PER: 0.3614
2026-01-08 12:37:49,253: t15.2024.07.28 val PER: 0.3772
2026-01-08 12:37:49,254: t15.2025.01.10 val PER: 0.5730
2026-01-08 12:37:49,254: t15.2025.01.12 val PER: 0.4342
2026-01-08 12:37:49,254: t15.2025.03.14 val PER: 0.5799
2026-01-08 12:37:49,254: t15.2025.03.16 val PER: 0.4555
2026-01-08 12:37:49,254: t15.2025.03.30 val PER: 0.5609
2026-01-08 12:37:49,254: t15.2025.04.13 val PER: 0.4494
2026-01-08 12:38:07,041: Train batch 2200: loss: 36.61 grad norm: 79.33 time: 0.063
2026-01-08 12:38:25,164: Train batch 2400: loss: 35.97 grad norm: 79.74 time: 0.054
2026-01-08 12:38:33,827: Running test after training batch: 2500
2026-01-08 12:38:33,967: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:38:39,060: WER debug example
  GT : you can see the code at this point as well
  PR : hughley candidacies smutty headhunters cysts ponds swells
2026-01-08 12:38:39,089: WER debug example
  GT : how does it keep the cost down
  PR : longhand stys hitty skinks that snitzer
2026-01-08 12:38:40,783: Val batch 2500: PER (avg): 0.3584 CTC Loss (avg): 37.7250 WER(1gram): 98.73% (n=64) time: 6.955
2026-01-08 12:38:40,783: WER lens: avg_true_words=6.16 avg_pred_words=4.97 max_pred_words=10
2026-01-08 12:38:40,783: t15.2023.08.13 val PER: 0.3482
2026-01-08 12:38:40,784: t15.2023.08.18 val PER: 0.3202
2026-01-08 12:38:40,784: t15.2023.08.20 val PER: 0.3026
2026-01-08 12:38:40,784: t15.2023.08.25 val PER: 0.2786
2026-01-08 12:38:40,784: t15.2023.08.27 val PER: 0.3955
2026-01-08 12:38:40,784: t15.2023.09.01 val PER: 0.2922
2026-01-08 12:38:40,784: t15.2023.09.03 val PER: 0.3812
2026-01-08 12:38:40,784: t15.2023.09.24 val PER: 0.2961
2026-01-08 12:38:40,784: t15.2023.09.29 val PER: 0.3038
2026-01-08 12:38:40,784: t15.2023.10.01 val PER: 0.3527
2026-01-08 12:38:40,784: t15.2023.10.06 val PER: 0.2874
2026-01-08 12:38:40,784: t15.2023.10.08 val PER: 0.4303
2026-01-08 12:38:40,784: t15.2023.10.13 val PER: 0.4244
2026-01-08 12:38:40,784: t15.2023.10.15 val PER: 0.3428
2026-01-08 12:38:40,785: t15.2023.10.20 val PER: 0.3389
2026-01-08 12:38:40,785: t15.2023.10.22 val PER: 0.3051
2026-01-08 12:38:40,785: t15.2023.11.03 val PER: 0.3521
2026-01-08 12:38:40,785: t15.2023.11.04 val PER: 0.1604
2026-01-08 12:38:40,785: t15.2023.11.17 val PER: 0.2068
2026-01-08 12:38:40,785: t15.2023.11.19 val PER: 0.1976
2026-01-08 12:38:40,785: t15.2023.11.26 val PER: 0.4014
2026-01-08 12:38:40,785: t15.2023.12.03 val PER: 0.3466
2026-01-08 12:38:40,785: t15.2023.12.08 val PER: 0.3455
2026-01-08 12:38:40,785: t15.2023.12.10 val PER: 0.2970
2026-01-08 12:38:40,785: t15.2023.12.17 val PER: 0.3285
2026-01-08 12:38:40,785: t15.2023.12.29 val PER: 0.3603
2026-01-08 12:38:40,785: t15.2024.02.25 val PER: 0.3076
2026-01-08 12:38:40,785: t15.2024.03.08 val PER: 0.4054
2026-01-08 12:38:40,785: t15.2024.03.15 val PER: 0.3684
2026-01-08 12:38:40,785: t15.2024.03.17 val PER: 0.3668
2026-01-08 12:38:40,786: t15.2024.05.10 val PER: 0.3596
2026-01-08 12:38:40,786: t15.2024.06.14 val PER: 0.3502
2026-01-08 12:38:40,786: t15.2024.07.19 val PER: 0.4568
2026-01-08 12:38:40,786: t15.2024.07.21 val PER: 0.3041
2026-01-08 12:38:40,786: t15.2024.07.28 val PER: 0.3397
2026-01-08 12:38:40,786: t15.2025.01.10 val PER: 0.5482
2026-01-08 12:38:40,786: t15.2025.01.12 val PER: 0.4142
2026-01-08 12:38:40,786: t15.2025.03.14 val PER: 0.5547
2026-01-08 12:38:40,787: t15.2025.03.16 val PER: 0.3927
2026-01-08 12:38:40,787: t15.2025.03.30 val PER: 0.5425
2026-01-08 12:38:40,787: t15.2025.04.13 val PER: 0.4465
2026-01-08 12:38:49,005: Train batch 2600: loss: 43.52 grad norm: 95.96 time: 0.056
2026-01-08 12:39:05,584: Train batch 2800: loss: 32.77 grad norm: 79.84 time: 0.084
2026-01-08 12:39:22,139: Train batch 3000: loss: 36.43 grad norm: 87.69 time: 0.086
2026-01-08 12:39:22,140: Running test after training batch: 3000
2026-01-08 12:39:22,242: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:39:26,978: WER debug example
  GT : you can see the code at this point as well
  PR : hudy candy seixas the steelhead hottest spartans swells
2026-01-08 12:39:27,004: WER debug example
  GT : how does it keep the cost down
  PR : toehold stys hitty skinks the teast setzer
2026-01-08 12:39:28,678: Val batch 3000: PER (avg): 0.3420 CTC Loss (avg): 34.9744 WER(1gram): 97.46% (n=64) time: 6.538
2026-01-08 12:39:28,679: WER lens: avg_true_words=6.16 avg_pred_words=5.25 max_pred_words=11
2026-01-08 12:39:28,679: t15.2023.08.13 val PER: 0.3160
2026-01-08 12:39:28,679: t15.2023.08.18 val PER: 0.3026
2026-01-08 12:39:28,679: t15.2023.08.20 val PER: 0.2963
2026-01-08 12:39:28,679: t15.2023.08.25 val PER: 0.2651
2026-01-08 12:39:28,679: t15.2023.08.27 val PER: 0.3666
2026-01-08 12:39:28,679: t15.2023.09.01 val PER: 0.2679
2026-01-08 12:39:28,679: t15.2023.09.03 val PER: 0.3468
2026-01-08 12:39:28,679: t15.2023.09.24 val PER: 0.2767
2026-01-08 12:39:28,679: t15.2023.09.29 val PER: 0.2993
2026-01-08 12:39:28,680: t15.2023.10.01 val PER: 0.3349
2026-01-08 12:39:28,680: t15.2023.10.06 val PER: 0.2766
2026-01-08 12:39:28,680: t15.2023.10.08 val PER: 0.4168
2026-01-08 12:39:28,680: t15.2023.10.13 val PER: 0.4143
2026-01-08 12:39:28,680: t15.2023.10.15 val PER: 0.3230
2026-01-08 12:39:28,680: t15.2023.10.20 val PER: 0.3456
2026-01-08 12:39:28,680: t15.2023.10.22 val PER: 0.2817
2026-01-08 12:39:28,680: t15.2023.11.03 val PER: 0.3351
2026-01-08 12:39:28,680: t15.2023.11.04 val PER: 0.1536
2026-01-08 12:39:28,680: t15.2023.11.17 val PER: 0.2068
2026-01-08 12:39:28,680: t15.2023.11.19 val PER: 0.1996
2026-01-08 12:39:28,680: t15.2023.11.26 val PER: 0.3710
2026-01-08 12:39:28,680: t15.2023.12.03 val PER: 0.3078
2026-01-08 12:39:28,681: t15.2023.12.08 val PER: 0.3342
2026-01-08 12:39:28,681: t15.2023.12.10 val PER: 0.2786
2026-01-08 12:39:28,681: t15.2023.12.17 val PER: 0.3202
2026-01-08 12:39:28,681: t15.2023.12.29 val PER: 0.3425
2026-01-08 12:39:28,681: t15.2024.02.25 val PER: 0.3104
2026-01-08 12:39:28,681: t15.2024.03.08 val PER: 0.4125
2026-01-08 12:39:28,681: t15.2024.03.15 val PER: 0.3540
2026-01-08 12:39:28,681: t15.2024.03.17 val PER: 0.3508
2026-01-08 12:39:28,681: t15.2024.05.10 val PER: 0.3284
2026-01-08 12:39:28,681: t15.2024.06.14 val PER: 0.3502
2026-01-08 12:39:28,681: t15.2024.07.19 val PER: 0.4397
2026-01-08 12:39:28,681: t15.2024.07.21 val PER: 0.2710
2026-01-08 12:39:28,681: t15.2024.07.28 val PER: 0.3316
2026-01-08 12:39:28,681: t15.2025.01.10 val PER: 0.5344
2026-01-08 12:39:28,681: t15.2025.01.12 val PER: 0.3926
2026-01-08 12:39:28,681: t15.2025.03.14 val PER: 0.5281
2026-01-08 12:39:28,681: t15.2025.03.16 val PER: 0.3861
2026-01-08 12:39:28,682: t15.2025.03.30 val PER: 0.5218
2026-01-08 12:39:28,682: t15.2025.04.13 val PER: 0.4351
2026-01-08 12:39:45,788: Train batch 3200: loss: 30.07 grad norm: 68.13 time: 0.077
2026-01-08 12:40:03,099: Train batch 3400: loss: 25.19 grad norm: 60.71 time: 0.050
2026-01-08 12:40:11,702: Running test after training batch: 3500
2026-01-08 12:40:11,811: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:40:16,483: WER debug example
  GT : you can see the code at this point as well
  PR : hudy hands sikhs the steelhead hotspot his swells
2026-01-08 12:40:16,512: WER debug example
  GT : how does it keep the cost down
  PR : leasehold suss hitty skinks the tsetse
2026-01-08 12:40:17,970: Val batch 3500: PER (avg): 0.3272 CTC Loss (avg): 33.4304 WER(1gram): 100.00% (n=64) time: 6.268
2026-01-08 12:40:17,970: WER lens: avg_true_words=6.16 avg_pred_words=5.50 max_pred_words=12
2026-01-08 12:40:17,971: t15.2023.08.13 val PER: 0.3212
2026-01-08 12:40:17,971: t15.2023.08.18 val PER: 0.2825
2026-01-08 12:40:17,971: t15.2023.08.20 val PER: 0.2907
2026-01-08 12:40:17,971: t15.2023.08.25 val PER: 0.2410
2026-01-08 12:40:17,971: t15.2023.08.27 val PER: 0.3971
2026-01-08 12:40:17,971: t15.2023.09.01 val PER: 0.2606
2026-01-08 12:40:17,971: t15.2023.09.03 val PER: 0.3563
2026-01-08 12:40:17,971: t15.2023.09.24 val PER: 0.2755
2026-01-08 12:40:17,971: t15.2023.09.29 val PER: 0.2833
2026-01-08 12:40:17,971: t15.2023.10.01 val PER: 0.3276
2026-01-08 12:40:17,972: t15.2023.10.06 val PER: 0.2713
2026-01-08 12:40:17,972: t15.2023.10.08 val PER: 0.4141
2026-01-08 12:40:17,972: t15.2023.10.13 val PER: 0.3910
2026-01-08 12:40:17,972: t15.2023.10.15 val PER: 0.3013
2026-01-08 12:40:17,972: t15.2023.10.20 val PER: 0.3087
2026-01-08 12:40:17,972: t15.2023.10.22 val PER: 0.2728
2026-01-08 12:40:17,972: t15.2023.11.03 val PER: 0.3182
2026-01-08 12:40:17,972: t15.2023.11.04 val PER: 0.1297
2026-01-08 12:40:17,972: t15.2023.11.17 val PER: 0.1788
2026-01-08 12:40:17,972: t15.2023.11.19 val PER: 0.1916
2026-01-08 12:40:17,972: t15.2023.11.26 val PER: 0.3464
2026-01-08 12:40:17,972: t15.2023.12.03 val PER: 0.3057
2026-01-08 12:40:17,972: t15.2023.12.08 val PER: 0.2996
2026-01-08 12:40:17,972: t15.2023.12.10 val PER: 0.2641
2026-01-08 12:40:17,972: t15.2023.12.17 val PER: 0.2963
2026-01-08 12:40:17,973: t15.2023.12.29 val PER: 0.3205
2026-01-08 12:40:17,973: t15.2024.02.25 val PER: 0.2893
2026-01-08 12:40:17,973: t15.2024.03.08 val PER: 0.3926
2026-01-08 12:40:17,973: t15.2024.03.15 val PER: 0.3521
2026-01-08 12:40:17,973: t15.2024.03.17 val PER: 0.3285
2026-01-08 12:40:17,973: t15.2024.05.10 val PER: 0.3447
2026-01-08 12:40:17,973: t15.2024.06.14 val PER: 0.3391
2026-01-08 12:40:17,973: t15.2024.07.19 val PER: 0.4120
2026-01-08 12:40:17,973: t15.2024.07.21 val PER: 0.2662
2026-01-08 12:40:17,973: t15.2024.07.28 val PER: 0.3331
2026-01-08 12:40:17,973: t15.2025.01.10 val PER: 0.4945
2026-01-08 12:40:17,973: t15.2025.01.12 val PER: 0.3726
2026-01-08 12:40:17,973: t15.2025.03.14 val PER: 0.4956
2026-01-08 12:40:17,973: t15.2025.03.16 val PER: 0.3704
2026-01-08 12:40:17,973: t15.2025.03.30 val PER: 0.4862
2026-01-08 12:40:17,973: t15.2025.04.13 val PER: 0.3966
2026-01-08 12:40:26,447: Train batch 3600: loss: 28.42 grad norm: 64.88 time: 0.068
2026-01-08 12:40:43,750: Train batch 3800: loss: 32.46 grad norm: 73.48 time: 0.069
2026-01-08 12:41:01,189: Train batch 4000: loss: 23.91 grad norm: 58.95 time: 0.057
2026-01-08 12:41:01,189: Running test after training batch: 4000
2026-01-08 12:41:01,340: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:41:06,120: WER debug example
  GT : you can see the code at this point as well
  PR : hudy canned sikhs the stuccoed hatz nist appoints his swells
2026-01-08 12:41:06,149: WER debug example
  GT : how does it keep the cost down
  PR : niehaus osiecki coos thus costs
2026-01-08 12:41:07,548: Val batch 4000: PER (avg): 0.3061 CTC Loss (avg): 31.3665 WER(1gram): 99.24% (n=64) time: 6.359
2026-01-08 12:41:07,549: WER lens: avg_true_words=6.16 avg_pred_words=5.45 max_pred_words=11
2026-01-08 12:41:07,549: t15.2023.08.13 val PER: 0.2807
2026-01-08 12:41:07,549: t15.2023.08.18 val PER: 0.2733
2026-01-08 12:41:07,549: t15.2023.08.20 val PER: 0.2677
2026-01-08 12:41:07,549: t15.2023.08.25 val PER: 0.2349
2026-01-08 12:41:07,549: t15.2023.08.27 val PER: 0.3489
2026-01-08 12:41:07,549: t15.2023.09.01 val PER: 0.2451
2026-01-08 12:41:07,549: t15.2023.09.03 val PER: 0.3302
2026-01-08 12:41:07,549: t15.2023.09.24 val PER: 0.2536
2026-01-08 12:41:07,549: t15.2023.09.29 val PER: 0.2610
2026-01-08 12:41:07,549: t15.2023.10.01 val PER: 0.3111
2026-01-08 12:41:07,549: t15.2023.10.06 val PER: 0.2454
2026-01-08 12:41:07,549: t15.2023.10.08 val PER: 0.3843
2026-01-08 12:41:07,549: t15.2023.10.13 val PER: 0.3708
2026-01-08 12:41:07,549: t15.2023.10.15 val PER: 0.2933
2026-01-08 12:41:07,550: t15.2023.10.20 val PER: 0.2752
2026-01-08 12:41:07,550: t15.2023.10.22 val PER: 0.2617
2026-01-08 12:41:07,550: t15.2023.11.03 val PER: 0.2992
2026-01-08 12:41:07,550: t15.2023.11.04 val PER: 0.1365
2026-01-08 12:41:07,550: t15.2023.11.17 val PER: 0.1695
2026-01-08 12:41:07,550: t15.2023.11.19 val PER: 0.1697
2026-01-08 12:41:07,550: t15.2023.11.26 val PER: 0.3275
2026-01-08 12:41:07,550: t15.2023.12.03 val PER: 0.2710
2026-01-08 12:41:07,550: t15.2023.12.08 val PER: 0.2770
2026-01-08 12:41:07,551: t15.2023.12.10 val PER: 0.2484
2026-01-08 12:41:07,551: t15.2023.12.17 val PER: 0.2744
2026-01-08 12:41:07,551: t15.2023.12.29 val PER: 0.3075
2026-01-08 12:41:07,551: t15.2024.02.25 val PER: 0.2640
2026-01-08 12:41:07,551: t15.2024.03.08 val PER: 0.3727
2026-01-08 12:41:07,551: t15.2024.03.15 val PER: 0.3183
2026-01-08 12:41:07,551: t15.2024.03.17 val PER: 0.3075
2026-01-08 12:41:07,551: t15.2024.05.10 val PER: 0.3076
2026-01-08 12:41:07,551: t15.2024.06.14 val PER: 0.3076
2026-01-08 12:41:07,551: t15.2024.07.19 val PER: 0.4047
2026-01-08 12:41:07,551: t15.2024.07.21 val PER: 0.2434
2026-01-08 12:41:07,551: t15.2024.07.28 val PER: 0.2949
2026-01-08 12:41:07,551: t15.2025.01.10 val PER: 0.5000
2026-01-08 12:41:07,552: t15.2025.01.12 val PER: 0.3549
2026-01-08 12:41:07,552: t15.2025.03.14 val PER: 0.4497
2026-01-08 12:41:07,552: t15.2025.03.16 val PER: 0.3573
2026-01-08 12:41:07,552: t15.2025.03.30 val PER: 0.4655
2026-01-08 12:41:07,552: t15.2025.04.13 val PER: 0.3795
2026-01-08 12:41:24,566: Train batch 4200: loss: 28.72 grad norm: 68.46 time: 0.082
2026-01-08 12:41:42,633: Train batch 4400: loss: 23.81 grad norm: 61.18 time: 0.068
2026-01-08 12:41:51,748: Running test after training batch: 4500
2026-01-08 12:41:51,871: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:41:56,555: WER debug example
  GT : you can see the code at this point as well
  PR : udy kids seixas the toehold hatt cysts point his swells
2026-01-08 12:41:56,578: WER debug example
  GT : how does it keep the cost down
  PR : toehold suski hitty squeaked the teast
2026-01-08 12:41:57,884: Val batch 4500: PER (avg): 0.2961 CTC Loss (avg): 30.5415 WER(1gram): 99.75% (n=64) time: 6.135
2026-01-08 12:41:57,884: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=12
2026-01-08 12:41:57,884: t15.2023.08.13 val PER: 0.2817
2026-01-08 12:41:57,884: t15.2023.08.18 val PER: 0.2598
2026-01-08 12:41:57,884: t15.2023.08.20 val PER: 0.2518
2026-01-08 12:41:57,885: t15.2023.08.25 val PER: 0.2304
2026-01-08 12:41:57,885: t15.2023.08.27 val PER: 0.3457
2026-01-08 12:41:57,885: t15.2023.09.01 val PER: 0.2338
2026-01-08 12:41:57,885: t15.2023.09.03 val PER: 0.3278
2026-01-08 12:41:57,885: t15.2023.09.24 val PER: 0.2464
2026-01-08 12:41:57,885: t15.2023.09.29 val PER: 0.2623
2026-01-08 12:41:57,885: t15.2023.10.01 val PER: 0.2979
2026-01-08 12:41:57,885: t15.2023.10.06 val PER: 0.2174
2026-01-08 12:41:57,885: t15.2023.10.08 val PER: 0.3829
2026-01-08 12:41:57,885: t15.2023.10.13 val PER: 0.3701
2026-01-08 12:41:57,885: t15.2023.10.15 val PER: 0.2973
2026-01-08 12:41:57,885: t15.2023.10.20 val PER: 0.2886
2026-01-08 12:41:57,885: t15.2023.10.22 val PER: 0.2405
2026-01-08 12:41:57,885: t15.2023.11.03 val PER: 0.2992
2026-01-08 12:41:57,886: t15.2023.11.04 val PER: 0.1331
2026-01-08 12:41:57,886: t15.2023.11.17 val PER: 0.1477
2026-01-08 12:41:57,886: t15.2023.11.19 val PER: 0.1717
2026-01-08 12:41:57,886: t15.2023.11.26 val PER: 0.3138
2026-01-08 12:41:57,886: t15.2023.12.03 val PER: 0.2773
2026-01-08 12:41:57,886: t15.2023.12.08 val PER: 0.2623
2026-01-08 12:41:57,886: t15.2023.12.10 val PER: 0.2378
2026-01-08 12:41:57,886: t15.2023.12.17 val PER: 0.2755
2026-01-08 12:41:57,886: t15.2023.12.29 val PER: 0.3075
2026-01-08 12:41:57,886: t15.2024.02.25 val PER: 0.2528
2026-01-08 12:41:57,886: t15.2024.03.08 val PER: 0.3698
2026-01-08 12:41:57,886: t15.2024.03.15 val PER: 0.3296
2026-01-08 12:41:57,886: t15.2024.03.17 val PER: 0.2985
2026-01-08 12:41:57,886: t15.2024.05.10 val PER: 0.2987
2026-01-08 12:41:57,886: t15.2024.06.14 val PER: 0.2729
2026-01-08 12:41:57,886: t15.2024.07.19 val PER: 0.3817
2026-01-08 12:41:57,887: t15.2024.07.21 val PER: 0.2221
2026-01-08 12:41:57,887: t15.2024.07.28 val PER: 0.2779
2026-01-08 12:41:57,887: t15.2025.01.10 val PER: 0.4766
2026-01-08 12:41:57,887: t15.2025.01.12 val PER: 0.3318
2026-01-08 12:41:57,887: t15.2025.03.14 val PER: 0.4334
2026-01-08 12:41:57,887: t15.2025.03.16 val PER: 0.3338
2026-01-08 12:41:57,887: t15.2025.03.30 val PER: 0.4391
2026-01-08 12:41:57,887: t15.2025.04.13 val PER: 0.3509
2026-01-08 12:42:06,483: Train batch 4600: loss: 25.58 grad norm: 64.49 time: 0.065
2026-01-08 12:42:24,260: Train batch 4800: loss: 20.25 grad norm: 60.91 time: 0.066
2026-01-08 12:42:42,555: Train batch 5000: loss: 35.79 grad norm: 81.19 time: 0.066
2026-01-08 12:42:42,555: Running test after training batch: 5000
2026-01-08 12:42:42,684: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:42:47,395: WER debug example
  GT : you can see the code at this point as well
  PR : hudy kids sikhs the toehold hatz nitz point his swells
2026-01-08 12:42:47,417: WER debug example
  GT : how does it keep the cost down
  PR : toehold suss hitz hix the teast
2026-01-08 12:42:48,736: Val batch 5000: PER (avg): 0.2872 CTC Loss (avg): 29.5438 WER(1gram): 99.49% (n=64) time: 6.181
2026-01-08 12:42:48,736: WER lens: avg_true_words=6.16 avg_pred_words=5.83 max_pred_words=11
2026-01-08 12:42:48,737: t15.2023.08.13 val PER: 0.2890
2026-01-08 12:42:48,737: t15.2023.08.18 val PER: 0.2557
2026-01-08 12:42:48,737: t15.2023.08.20 val PER: 0.2605
2026-01-08 12:42:48,737: t15.2023.08.25 val PER: 0.2199
2026-01-08 12:42:48,737: t15.2023.08.27 val PER: 0.3360
2026-01-08 12:42:48,737: t15.2023.09.01 val PER: 0.2216
2026-01-08 12:42:48,737: t15.2023.09.03 val PER: 0.3195
2026-01-08 12:42:48,738: t15.2023.09.24 val PER: 0.2464
2026-01-08 12:42:48,738: t15.2023.09.29 val PER: 0.2431
2026-01-08 12:42:48,738: t15.2023.10.01 val PER: 0.2926
2026-01-08 12:42:48,738: t15.2023.10.06 val PER: 0.2217
2026-01-08 12:42:48,738: t15.2023.10.08 val PER: 0.3829
2026-01-08 12:42:48,738: t15.2023.10.13 val PER: 0.3576
2026-01-08 12:42:48,738: t15.2023.10.15 val PER: 0.2775
2026-01-08 12:42:48,738: t15.2023.10.20 val PER: 0.2617
2026-01-08 12:42:48,738: t15.2023.10.22 val PER: 0.2261
2026-01-08 12:42:48,738: t15.2023.11.03 val PER: 0.2877
2026-01-08 12:42:48,738: t15.2023.11.04 val PER: 0.1399
2026-01-08 12:42:48,739: t15.2023.11.17 val PER: 0.1540
2026-01-08 12:42:48,739: t15.2023.11.19 val PER: 0.1557
2026-01-08 12:42:48,739: t15.2023.11.26 val PER: 0.2957
2026-01-08 12:42:48,739: t15.2023.12.03 val PER: 0.2563
2026-01-08 12:42:48,739: t15.2023.12.08 val PER: 0.2590
2026-01-08 12:42:48,739: t15.2023.12.10 val PER: 0.2313
2026-01-08 12:42:48,739: t15.2023.12.17 val PER: 0.2588
2026-01-08 12:42:48,739: t15.2023.12.29 val PER: 0.3013
2026-01-08 12:42:48,739: t15.2024.02.25 val PER: 0.2458
2026-01-08 12:42:48,739: t15.2024.03.08 val PER: 0.3514
2026-01-08 12:42:48,739: t15.2024.03.15 val PER: 0.3064
2026-01-08 12:42:48,739: t15.2024.03.17 val PER: 0.2950
2026-01-08 12:42:48,739: t15.2024.05.10 val PER: 0.2987
2026-01-08 12:42:48,739: t15.2024.06.14 val PER: 0.2902
2026-01-08 12:42:48,739: t15.2024.07.19 val PER: 0.3579
2026-01-08 12:42:48,740: t15.2024.07.21 val PER: 0.2200
2026-01-08 12:42:48,740: t15.2024.07.28 val PER: 0.2691
2026-01-08 12:42:48,740: t15.2025.01.10 val PER: 0.4394
2026-01-08 12:42:48,740: t15.2025.01.12 val PER: 0.3226
2026-01-08 12:42:48,740: t15.2025.03.14 val PER: 0.4334
2026-01-08 12:42:48,740: t15.2025.03.16 val PER: 0.3285
2026-01-08 12:42:48,740: t15.2025.03.30 val PER: 0.4276
2026-01-08 12:42:48,740: t15.2025.04.13 val PER: 0.3609
2026-01-08 12:43:06,801: Train batch 5200: loss: 22.72 grad norm: 66.56 time: 0.054
2026-01-08 12:43:24,318: Train batch 5400: loss: 23.97 grad norm: 64.95 time: 0.070
2026-01-08 12:43:32,993: Running test after training batch: 5500
2026-01-08 12:43:33,093: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:43:37,982: WER debug example
  GT : you can see the code at this point as well
  PR : udy candy sikhs the tusk daihatsu cysts point his swells
2026-01-08 12:43:38,005: WER debug example
  GT : how does it keep the cost down
  PR : toehold stutz hitty sikhs thus cost
2026-01-08 12:43:39,364: Val batch 5500: PER (avg): 0.2791 CTC Loss (avg): 28.6535 WER(1gram): 99.24% (n=64) time: 6.371
2026-01-08 12:43:39,365: WER lens: avg_true_words=6.16 avg_pred_words=5.83 max_pred_words=11
2026-01-08 12:43:39,365: t15.2023.08.13 val PER: 0.2630
2026-01-08 12:43:39,365: t15.2023.08.18 val PER: 0.2515
2026-01-08 12:43:39,365: t15.2023.08.20 val PER: 0.2399
2026-01-08 12:43:39,365: t15.2023.08.25 val PER: 0.2184
2026-01-08 12:43:39,365: t15.2023.08.27 val PER: 0.3296
2026-01-08 12:43:39,365: t15.2023.09.01 val PER: 0.2216
2026-01-08 12:43:39,365: t15.2023.09.03 val PER: 0.3135
2026-01-08 12:43:39,365: t15.2023.09.24 val PER: 0.2427
2026-01-08 12:43:39,365: t15.2023.09.29 val PER: 0.2476
2026-01-08 12:43:39,365: t15.2023.10.01 val PER: 0.2933
2026-01-08 12:43:39,366: t15.2023.10.06 val PER: 0.2185
2026-01-08 12:43:39,366: t15.2023.10.08 val PER: 0.3667
2026-01-08 12:43:39,366: t15.2023.10.13 val PER: 0.3553
2026-01-08 12:43:39,366: t15.2023.10.15 val PER: 0.2657
2026-01-08 12:43:39,366: t15.2023.10.20 val PER: 0.3087
2026-01-08 12:43:39,366: t15.2023.10.22 val PER: 0.2249
2026-01-08 12:43:39,366: t15.2023.11.03 val PER: 0.2904
2026-01-08 12:43:39,366: t15.2023.11.04 val PER: 0.1263
2026-01-08 12:43:39,366: t15.2023.11.17 val PER: 0.1493
2026-01-08 12:43:39,366: t15.2023.11.19 val PER: 0.1597
2026-01-08 12:43:39,366: t15.2023.11.26 val PER: 0.2906
2026-01-08 12:43:39,367: t15.2023.12.03 val PER: 0.2458
2026-01-08 12:43:39,367: t15.2023.12.08 val PER: 0.2410
2026-01-08 12:43:39,367: t15.2023.12.10 val PER: 0.2076
2026-01-08 12:43:39,367: t15.2023.12.17 val PER: 0.2495
2026-01-08 12:43:39,367: t15.2023.12.29 val PER: 0.2848
2026-01-08 12:43:39,367: t15.2024.02.25 val PER: 0.2416
2026-01-08 12:43:39,367: t15.2024.03.08 val PER: 0.3286
2026-01-08 12:43:39,367: t15.2024.03.15 val PER: 0.3002
2026-01-08 12:43:39,367: t15.2024.03.17 val PER: 0.2992
2026-01-08 12:43:39,367: t15.2024.05.10 val PER: 0.2942
2026-01-08 12:43:39,367: t15.2024.06.14 val PER: 0.2823
2026-01-08 12:43:39,368: t15.2024.07.19 val PER: 0.3349
2026-01-08 12:43:39,368: t15.2024.07.21 val PER: 0.2034
2026-01-08 12:43:39,368: t15.2024.07.28 val PER: 0.2625
2026-01-08 12:43:39,368: t15.2025.01.10 val PER: 0.4311
2026-01-08 12:43:39,368: t15.2025.01.12 val PER: 0.3018
2026-01-08 12:43:39,368: t15.2025.03.14 val PER: 0.4172
2026-01-08 12:43:39,368: t15.2025.03.16 val PER: 0.3168
2026-01-08 12:43:39,368: t15.2025.03.30 val PER: 0.4184
2026-01-08 12:43:39,368: t15.2025.04.13 val PER: 0.3638
2026-01-08 12:43:47,896: Train batch 5600: loss: 24.30 grad norm: 72.13 time: 0.064
2026-01-08 12:44:05,504: Train batch 5800: loss: 19.05 grad norm: 62.66 time: 0.084
2026-01-08 12:44:22,857: Train batch 6000: loss: 19.02 grad norm: 63.14 time: 0.052
2026-01-08 12:44:22,858: Running test after training batch: 6000
2026-01-08 12:44:23,059: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:44:27,756: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the stuccoed hatt cysts points his swells
2026-01-08 12:44:27,781: WER debug example
  GT : how does it keep the cost down
  PR : leasehold stutz hitz keep keitz thus cost
2026-01-08 12:44:29,127: Val batch 6000: PER (avg): 0.2711 CTC Loss (avg): 27.8018 WER(1gram): 98.98% (n=64) time: 6.269
2026-01-08 12:44:29,127: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-08 12:44:29,127: t15.2023.08.13 val PER: 0.2568
2026-01-08 12:44:29,127: t15.2023.08.18 val PER: 0.2531
2026-01-08 12:44:29,128: t15.2023.08.20 val PER: 0.2319
2026-01-08 12:44:29,128: t15.2023.08.25 val PER: 0.2214
2026-01-08 12:44:29,128: t15.2023.08.27 val PER: 0.3328
2026-01-08 12:44:29,128: t15.2023.09.01 val PER: 0.2127
2026-01-08 12:44:29,128: t15.2023.09.03 val PER: 0.3076
2026-01-08 12:44:29,128: t15.2023.09.24 val PER: 0.2257
2026-01-08 12:44:29,128: t15.2023.09.29 val PER: 0.2444
2026-01-08 12:44:29,128: t15.2023.10.01 val PER: 0.2774
2026-01-08 12:44:29,128: t15.2023.10.06 val PER: 0.2067
2026-01-08 12:44:29,128: t15.2023.10.08 val PER: 0.3599
2026-01-08 12:44:29,128: t15.2023.10.13 val PER: 0.3437
2026-01-08 12:44:29,129: t15.2023.10.15 val PER: 0.2610
2026-01-08 12:44:29,129: t15.2023.10.20 val PER: 0.2718
2026-01-08 12:44:29,129: t15.2023.10.22 val PER: 0.2194
2026-01-08 12:44:29,129: t15.2023.11.03 val PER: 0.2856
2026-01-08 12:44:29,129: t15.2023.11.04 val PER: 0.1399
2026-01-08 12:44:29,129: t15.2023.11.17 val PER: 0.1493
2026-01-08 12:44:29,129: t15.2023.11.19 val PER: 0.1557
2026-01-08 12:44:29,129: t15.2023.11.26 val PER: 0.2870
2026-01-08 12:44:29,129: t15.2023.12.03 val PER: 0.2447
2026-01-08 12:44:29,129: t15.2023.12.08 val PER: 0.2390
2026-01-08 12:44:29,130: t15.2023.12.10 val PER: 0.2129
2026-01-08 12:44:29,130: t15.2023.12.17 val PER: 0.2516
2026-01-08 12:44:29,130: t15.2023.12.29 val PER: 0.2684
2026-01-08 12:44:29,130: t15.2024.02.25 val PER: 0.2444
2026-01-08 12:44:29,130: t15.2024.03.08 val PER: 0.3286
2026-01-08 12:44:29,130: t15.2024.03.15 val PER: 0.2846
2026-01-08 12:44:29,130: t15.2024.03.17 val PER: 0.2727
2026-01-08 12:44:29,134: t15.2024.05.10 val PER: 0.2808
2026-01-08 12:44:29,134: t15.2024.06.14 val PER: 0.2634
2026-01-08 12:44:29,134: t15.2024.07.19 val PER: 0.3316
2026-01-08 12:44:29,134: t15.2024.07.21 val PER: 0.1924
2026-01-08 12:44:29,134: t15.2024.07.28 val PER: 0.2382
2026-01-08 12:44:29,134: t15.2025.01.10 val PER: 0.4105
2026-01-08 12:44:29,135: t15.2025.01.12 val PER: 0.3041
2026-01-08 12:44:29,135: t15.2025.03.14 val PER: 0.4186
2026-01-08 12:44:29,135: t15.2025.03.16 val PER: 0.3207
2026-01-08 12:44:29,135: t15.2025.03.30 val PER: 0.4092
2026-01-08 12:44:29,135: t15.2025.04.13 val PER: 0.3324
2026-01-08 12:44:46,988: Train batch 6200: loss: 21.44 grad norm: 64.80 time: 0.071
2026-01-08 12:45:04,570: Train batch 6400: loss: 25.71 grad norm: 70.91 time: 0.065
2026-01-08 12:45:13,064: Running test after training batch: 6500
2026-01-08 12:45:13,161: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:45:17,876: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the toehold hatt cysts point his swells
2026-01-08 12:45:17,901: WER debug example
  GT : how does it keep the cost down
  PR : seaholm decency hitty skinks the teast
2026-01-08 12:45:19,233: Val batch 6500: PER (avg): 0.2655 CTC Loss (avg): 27.2063 WER(1gram): 100.76% (n=64) time: 6.169
2026-01-08 12:45:19,234: WER lens: avg_true_words=6.16 avg_pred_words=5.95 max_pred_words=11
2026-01-08 12:45:19,234: t15.2023.08.13 val PER: 0.2328
2026-01-08 12:45:19,234: t15.2023.08.18 val PER: 0.2422
2026-01-08 12:45:19,234: t15.2023.08.20 val PER: 0.2375
2026-01-08 12:45:19,234: t15.2023.08.25 val PER: 0.2229
2026-01-08 12:45:19,234: t15.2023.08.27 val PER: 0.2990
2026-01-08 12:45:19,234: t15.2023.09.01 val PER: 0.2127
2026-01-08 12:45:19,234: t15.2023.09.03 val PER: 0.2874
2026-01-08 12:45:19,235: t15.2023.09.24 val PER: 0.2257
2026-01-08 12:45:19,235: t15.2023.09.29 val PER: 0.2310
2026-01-08 12:45:19,235: t15.2023.10.01 val PER: 0.2748
2026-01-08 12:45:19,235: t15.2023.10.06 val PER: 0.2121
2026-01-08 12:45:19,235: t15.2023.10.08 val PER: 0.3640
2026-01-08 12:45:19,235: t15.2023.10.13 val PER: 0.3406
2026-01-08 12:45:19,235: t15.2023.10.15 val PER: 0.2624
2026-01-08 12:45:19,235: t15.2023.10.20 val PER: 0.2785
2026-01-08 12:45:19,235: t15.2023.10.22 val PER: 0.2138
2026-01-08 12:45:19,235: t15.2023.11.03 val PER: 0.2775
2026-01-08 12:45:19,236: t15.2023.11.04 val PER: 0.1263
2026-01-08 12:45:19,236: t15.2023.11.17 val PER: 0.1353
2026-01-08 12:45:19,236: t15.2023.11.19 val PER: 0.1437
2026-01-08 12:45:19,236: t15.2023.11.26 val PER: 0.2703
2026-01-08 12:45:19,236: t15.2023.12.03 val PER: 0.2353
2026-01-08 12:45:19,236: t15.2023.12.08 val PER: 0.2390
2026-01-08 12:45:19,236: t15.2023.12.10 val PER: 0.2011
2026-01-08 12:45:19,236: t15.2023.12.17 val PER: 0.2453
2026-01-08 12:45:19,237: t15.2023.12.29 val PER: 0.2732
2026-01-08 12:45:19,237: t15.2024.02.25 val PER: 0.2149
2026-01-08 12:45:19,237: t15.2024.03.08 val PER: 0.3073
2026-01-08 12:45:19,237: t15.2024.03.15 val PER: 0.2883
2026-01-08 12:45:19,237: t15.2024.03.17 val PER: 0.2622
2026-01-08 12:45:19,237: t15.2024.05.10 val PER: 0.2927
2026-01-08 12:45:19,237: t15.2024.06.14 val PER: 0.2634
2026-01-08 12:45:19,237: t15.2024.07.19 val PER: 0.3283
2026-01-08 12:45:19,237: t15.2024.07.21 val PER: 0.1972
2026-01-08 12:45:19,237: t15.2024.07.28 val PER: 0.2346
2026-01-08 12:45:19,238: t15.2025.01.10 val PER: 0.4284
2026-01-08 12:45:19,238: t15.2025.01.12 val PER: 0.2979
2026-01-08 12:45:19,238: t15.2025.03.14 val PER: 0.4053
2026-01-08 12:45:19,238: t15.2025.03.16 val PER: 0.3050
2026-01-08 12:45:19,238: t15.2025.03.30 val PER: 0.3966
2026-01-08 12:45:19,238: t15.2025.04.13 val PER: 0.3238
2026-01-08 12:45:27,520: Train batch 6600: loss: 16.41 grad norm: 50.56 time: 0.047
2026-01-08 12:45:45,120: Train batch 6800: loss: 21.75 grad norm: 56.44 time: 0.050
2026-01-08 12:46:03,614: Train batch 7000: loss: 22.18 grad norm: 68.93 time: 0.063
2026-01-08 12:46:03,614: Running test after training batch: 7000
2026-01-08 12:46:03,764: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:46:08,549: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the tusk daihatsu cyst appoint his swells
2026-01-08 12:46:08,572: WER debug example
  GT : how does it keep the cost down
  PR : turnham suss hitz kicks the teast
2026-01-08 12:46:09,902: Val batch 7000: PER (avg): 0.2559 CTC Loss (avg): 26.3108 WER(1gram): 98.22% (n=64) time: 6.288
2026-01-08 12:46:09,903: WER lens: avg_true_words=6.16 avg_pred_words=5.81 max_pred_words=10
2026-01-08 12:46:09,903: t15.2023.08.13 val PER: 0.2464
2026-01-08 12:46:09,903: t15.2023.08.18 val PER: 0.2213
2026-01-08 12:46:09,903: t15.2023.08.20 val PER: 0.2248
2026-01-08 12:46:09,903: t15.2023.08.25 val PER: 0.2123
2026-01-08 12:46:09,903: t15.2023.08.27 val PER: 0.3119
2026-01-08 12:46:09,903: t15.2023.09.01 val PER: 0.1972
2026-01-08 12:46:09,903: t15.2023.09.03 val PER: 0.2862
2026-01-08 12:46:09,903: t15.2023.09.24 val PER: 0.2282
2026-01-08 12:46:09,903: t15.2023.09.29 val PER: 0.2336
2026-01-08 12:46:09,903: t15.2023.10.01 val PER: 0.2536
2026-01-08 12:46:09,904: t15.2023.10.06 val PER: 0.1862
2026-01-08 12:46:09,904: t15.2023.10.08 val PER: 0.3451
2026-01-08 12:46:09,904: t15.2023.10.13 val PER: 0.3313
2026-01-08 12:46:09,904: t15.2023.10.15 val PER: 0.2505
2026-01-08 12:46:09,904: t15.2023.10.20 val PER: 0.2550
2026-01-08 12:46:09,904: t15.2023.10.22 val PER: 0.2004
2026-01-08 12:46:09,904: t15.2023.11.03 val PER: 0.2707
2026-01-08 12:46:09,904: t15.2023.11.04 val PER: 0.1058
2026-01-08 12:46:09,904: t15.2023.11.17 val PER: 0.1353
2026-01-08 12:46:09,904: t15.2023.11.19 val PER: 0.1397
2026-01-08 12:46:09,904: t15.2023.11.26 val PER: 0.2645
2026-01-08 12:46:09,904: t15.2023.12.03 val PER: 0.2468
2026-01-08 12:46:09,904: t15.2023.12.08 val PER: 0.2137
2026-01-08 12:46:09,904: t15.2023.12.10 val PER: 0.1945
2026-01-08 12:46:09,905: t15.2023.12.17 val PER: 0.2245
2026-01-08 12:46:09,905: t15.2023.12.29 val PER: 0.2457
2026-01-08 12:46:09,905: t15.2024.02.25 val PER: 0.2331
2026-01-08 12:46:09,905: t15.2024.03.08 val PER: 0.3300
2026-01-08 12:46:09,905: t15.2024.03.15 val PER: 0.2720
2026-01-08 12:46:09,905: t15.2024.03.17 val PER: 0.2531
2026-01-08 12:46:09,905: t15.2024.05.10 val PER: 0.2645
2026-01-08 12:46:09,905: t15.2024.06.14 val PER: 0.2445
2026-01-08 12:46:09,905: t15.2024.07.19 val PER: 0.3171
2026-01-08 12:46:09,905: t15.2024.07.21 val PER: 0.1800
2026-01-08 12:46:09,905: t15.2024.07.28 val PER: 0.2265
2026-01-08 12:46:09,905: t15.2025.01.10 val PER: 0.4229
2026-01-08 12:46:09,905: t15.2025.01.12 val PER: 0.2787
2026-01-08 12:46:09,905: t15.2025.03.14 val PER: 0.3994
2026-01-08 12:46:09,905: t15.2025.03.16 val PER: 0.3037
2026-01-08 12:46:09,905: t15.2025.03.30 val PER: 0.4092
2026-01-08 12:46:09,905: t15.2025.04.13 val PER: 0.3195
2026-01-08 12:46:27,330: Train batch 7200: loss: 19.79 grad norm: 67.27 time: 0.080
2026-01-08 12:46:45,096: Train batch 7400: loss: 19.41 grad norm: 63.49 time: 0.077
2026-01-08 12:46:54,059: Running test after training batch: 7500
2026-01-08 12:46:54,170: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:46:58,889: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the stuccoed hatz this appointees his swells
2026-01-08 12:46:58,914: WER debug example
  GT : how does it keep the cost down
  PR : shouse dusts hitz kicks thus cost
2026-01-08 12:47:00,228: Val batch 7500: PER (avg): 0.2557 CTC Loss (avg): 26.5413 WER(1gram): 98.73% (n=64) time: 6.168
2026-01-08 12:47:00,228: WER lens: avg_true_words=6.16 avg_pred_words=5.95 max_pred_words=11
2026-01-08 12:47:00,228: t15.2023.08.13 val PER: 0.2256
2026-01-08 12:47:00,228: t15.2023.08.18 val PER: 0.2347
2026-01-08 12:47:00,228: t15.2023.08.20 val PER: 0.2113
2026-01-08 12:47:00,228: t15.2023.08.25 val PER: 0.2154
2026-01-08 12:47:00,229: t15.2023.08.27 val PER: 0.3055
2026-01-08 12:47:00,229: t15.2023.09.01 val PER: 0.2005
2026-01-08 12:47:00,229: t15.2023.09.03 val PER: 0.2910
2026-01-08 12:47:00,229: t15.2023.09.24 val PER: 0.2257
2026-01-08 12:47:00,229: t15.2023.09.29 val PER: 0.2259
2026-01-08 12:47:00,229: t15.2023.10.01 val PER: 0.2701
2026-01-08 12:47:00,229: t15.2023.10.06 val PER: 0.2002
2026-01-08 12:47:00,229: t15.2023.10.08 val PER: 0.3451
2026-01-08 12:47:00,229: t15.2023.10.13 val PER: 0.3126
2026-01-08 12:47:00,229: t15.2023.10.15 val PER: 0.2558
2026-01-08 12:47:00,230: t15.2023.10.20 val PER: 0.2517
2026-01-08 12:47:00,230: t15.2023.10.22 val PER: 0.2082
2026-01-08 12:47:00,230: t15.2023.11.03 val PER: 0.2829
2026-01-08 12:47:00,230: t15.2023.11.04 val PER: 0.1229
2026-01-08 12:47:00,230: t15.2023.11.17 val PER: 0.1291
2026-01-08 12:47:00,230: t15.2023.11.19 val PER: 0.1457
2026-01-08 12:47:00,230: t15.2023.11.26 val PER: 0.2580
2026-01-08 12:47:00,230: t15.2023.12.03 val PER: 0.2300
2026-01-08 12:47:00,230: t15.2023.12.08 val PER: 0.2230
2026-01-08 12:47:00,230: t15.2023.12.10 val PER: 0.1932
2026-01-08 12:47:00,231: t15.2023.12.17 val PER: 0.2110
2026-01-08 12:47:00,231: t15.2023.12.29 val PER: 0.2656
2026-01-08 12:47:00,231: t15.2024.02.25 val PER: 0.2261
2026-01-08 12:47:00,231: t15.2024.03.08 val PER: 0.3144
2026-01-08 12:47:00,231: t15.2024.03.15 val PER: 0.2758
2026-01-08 12:47:00,231: t15.2024.03.17 val PER: 0.2427
2026-01-08 12:47:00,231: t15.2024.05.10 val PER: 0.2645
2026-01-08 12:47:00,231: t15.2024.06.14 val PER: 0.2555
2026-01-08 12:47:00,231: t15.2024.07.19 val PER: 0.3125
2026-01-08 12:47:00,231: t15.2024.07.21 val PER: 0.1793
2026-01-08 12:47:00,231: t15.2024.07.28 val PER: 0.2360
2026-01-08 12:47:00,231: t15.2025.01.10 val PER: 0.3884
2026-01-08 12:47:00,232: t15.2025.01.12 val PER: 0.2918
2026-01-08 12:47:00,232: t15.2025.03.14 val PER: 0.3920
2026-01-08 12:47:00,232: t15.2025.03.16 val PER: 0.2945
2026-01-08 12:47:00,232: t15.2025.03.30 val PER: 0.3966
2026-01-08 12:47:00,232: t15.2025.04.13 val PER: 0.3381
2026-01-08 12:47:09,174: Train batch 7600: loss: 21.78 grad norm: 67.95 time: 0.071
2026-01-08 12:47:26,859: Train batch 7800: loss: 21.46 grad norm: 74.74 time: 0.056
2026-01-08 12:47:44,836: Train batch 8000: loss: 18.50 grad norm: 60.66 time: 0.076
2026-01-08 12:47:44,836: Running test after training batch: 8000
2026-01-08 12:47:44,930: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:47:49,569: WER debug example
  GT : you can see the code at this point as well
  PR : hudy candies sikhs the toehold hatz this appoint his swells
2026-01-08 12:47:49,593: WER debug example
  GT : how does it keep the cost down
  PR : turnham decency hitz kicks thus costs
2026-01-08 12:47:50,912: Val batch 8000: PER (avg): 0.2459 CTC Loss (avg): 25.5043 WER(1gram): 98.22% (n=64) time: 6.076
2026-01-08 12:47:50,913: WER lens: avg_true_words=6.16 avg_pred_words=5.91 max_pred_words=11
2026-01-08 12:47:50,913: t15.2023.08.13 val PER: 0.2297
2026-01-08 12:47:50,913: t15.2023.08.18 val PER: 0.2305
2026-01-08 12:47:50,913: t15.2023.08.20 val PER: 0.2057
2026-01-08 12:47:50,913: t15.2023.08.25 val PER: 0.2154
2026-01-08 12:47:50,913: t15.2023.08.27 val PER: 0.2910
2026-01-08 12:47:50,913: t15.2023.09.01 val PER: 0.1826
2026-01-08 12:47:50,913: t15.2023.09.03 val PER: 0.2732
2026-01-08 12:47:50,913: t15.2023.09.24 val PER: 0.2148
2026-01-08 12:47:50,913: t15.2023.09.29 val PER: 0.2240
2026-01-08 12:47:50,914: t15.2023.10.01 val PER: 0.2596
2026-01-08 12:47:50,914: t15.2023.10.06 val PER: 0.1905
2026-01-08 12:47:50,914: t15.2023.10.08 val PER: 0.3288
2026-01-08 12:47:50,914: t15.2023.10.13 val PER: 0.3212
2026-01-08 12:47:50,914: t15.2023.10.15 val PER: 0.2380
2026-01-08 12:47:50,914: t15.2023.10.20 val PER: 0.2617
2026-01-08 12:47:50,914: t15.2023.10.22 val PER: 0.1915
2026-01-08 12:47:50,914: t15.2023.11.03 val PER: 0.2612
2026-01-08 12:47:50,915: t15.2023.11.04 val PER: 0.1126
2026-01-08 12:47:50,915: t15.2023.11.17 val PER: 0.1337
2026-01-08 12:47:50,915: t15.2023.11.19 val PER: 0.1417
2026-01-08 12:47:50,915: t15.2023.11.26 val PER: 0.2464
2026-01-08 12:47:50,915: t15.2023.12.03 val PER: 0.2300
2026-01-08 12:47:50,915: t15.2023.12.08 val PER: 0.2111
2026-01-08 12:47:50,916: t15.2023.12.10 val PER: 0.1879
2026-01-08 12:47:50,916: t15.2023.12.17 val PER: 0.2162
2026-01-08 12:47:50,916: t15.2023.12.29 val PER: 0.2402
2026-01-08 12:47:50,916: t15.2024.02.25 val PER: 0.2008
2026-01-08 12:47:50,916: t15.2024.03.08 val PER: 0.3058
2026-01-08 12:47:50,916: t15.2024.03.15 val PER: 0.2627
2026-01-08 12:47:50,917: t15.2024.03.17 val PER: 0.2385
2026-01-08 12:47:50,917: t15.2024.05.10 val PER: 0.2615
2026-01-08 12:47:50,917: t15.2024.06.14 val PER: 0.2382
2026-01-08 12:47:50,917: t15.2024.07.19 val PER: 0.3085
2026-01-08 12:47:50,917: t15.2024.07.21 val PER: 0.1676
2026-01-08 12:47:50,917: t15.2024.07.28 val PER: 0.2199
2026-01-08 12:47:50,917: t15.2025.01.10 val PER: 0.3774
2026-01-08 12:47:50,917: t15.2025.01.12 val PER: 0.2771
2026-01-08 12:47:50,917: t15.2025.03.14 val PER: 0.3964
2026-01-08 12:47:50,917: t15.2025.03.16 val PER: 0.2958
2026-01-08 12:47:50,917: t15.2025.03.30 val PER: 0.3667
2026-01-08 12:47:50,918: t15.2025.04.13 val PER: 0.3195
2026-01-08 12:48:08,610: Train batch 8200: loss: 16.86 grad norm: 54.17 time: 0.056
2026-01-08 12:48:27,314: Train batch 8400: loss: 15.61 grad norm: 62.05 time: 0.068
2026-01-08 12:48:36,723: Running test after training batch: 8500
2026-01-08 12:48:36,838: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:48:41,533: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the toehold hatz this appoint his swells
2026-01-08 12:48:41,557: WER debug example
  GT : how does it keep the cost down
  PR : seaholm decency hitz kicked thus costs
2026-01-08 12:48:42,903: Val batch 8500: PER (avg): 0.2430 CTC Loss (avg): 25.1195 WER(1gram): 98.22% (n=64) time: 6.180
2026-01-08 12:48:42,903: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=12
2026-01-08 12:48:42,904: t15.2023.08.13 val PER: 0.2173
2026-01-08 12:48:42,904: t15.2023.08.18 val PER: 0.2238
2026-01-08 12:48:42,904: t15.2023.08.20 val PER: 0.2113
2026-01-08 12:48:42,904: t15.2023.08.25 val PER: 0.2003
2026-01-08 12:48:42,904: t15.2023.08.27 val PER: 0.2926
2026-01-08 12:48:42,904: t15.2023.09.01 val PER: 0.1802
2026-01-08 12:48:42,904: t15.2023.09.03 val PER: 0.2720
2026-01-08 12:48:42,904: t15.2023.09.24 val PER: 0.2087
2026-01-08 12:48:42,904: t15.2023.09.29 val PER: 0.2176
2026-01-08 12:48:42,904: t15.2023.10.01 val PER: 0.2550
2026-01-08 12:48:42,904: t15.2023.10.06 val PER: 0.1862
2026-01-08 12:48:42,904: t15.2023.10.08 val PER: 0.3329
2026-01-08 12:48:42,905: t15.2023.10.13 val PER: 0.3119
2026-01-08 12:48:42,905: t15.2023.10.15 val PER: 0.2432
2026-01-08 12:48:42,905: t15.2023.10.20 val PER: 0.2785
2026-01-08 12:48:42,905: t15.2023.10.22 val PER: 0.1893
2026-01-08 12:48:42,905: t15.2023.11.03 val PER: 0.2619
2026-01-08 12:48:42,905: t15.2023.11.04 val PER: 0.1126
2026-01-08 12:48:42,905: t15.2023.11.17 val PER: 0.1306
2026-01-08 12:48:42,905: t15.2023.11.19 val PER: 0.1317
2026-01-08 12:48:42,905: t15.2023.11.26 val PER: 0.2413
2026-01-08 12:48:42,905: t15.2023.12.03 val PER: 0.2195
2026-01-08 12:48:42,905: t15.2023.12.08 val PER: 0.2111
2026-01-08 12:48:42,905: t15.2023.12.10 val PER: 0.1695
2026-01-08 12:48:42,905: t15.2023.12.17 val PER: 0.1985
2026-01-08 12:48:42,905: t15.2023.12.29 val PER: 0.2485
2026-01-08 12:48:42,906: t15.2024.02.25 val PER: 0.2107
2026-01-08 12:48:42,906: t15.2024.03.08 val PER: 0.2902
2026-01-08 12:48:42,906: t15.2024.03.15 val PER: 0.2564
2026-01-08 12:48:42,906: t15.2024.03.17 val PER: 0.2294
2026-01-08 12:48:42,906: t15.2024.05.10 val PER: 0.2511
2026-01-08 12:48:42,906: t15.2024.06.14 val PER: 0.2461
2026-01-08 12:48:42,906: t15.2024.07.19 val PER: 0.2980
2026-01-08 12:48:42,906: t15.2024.07.21 val PER: 0.1607
2026-01-08 12:48:42,906: t15.2024.07.28 val PER: 0.2191
2026-01-08 12:48:42,906: t15.2025.01.10 val PER: 0.3719
2026-01-08 12:48:42,906: t15.2025.01.12 val PER: 0.2825
2026-01-08 12:48:42,906: t15.2025.03.14 val PER: 0.4053
2026-01-08 12:48:42,906: t15.2025.03.16 val PER: 0.2971
2026-01-08 12:48:42,906: t15.2025.03.30 val PER: 0.3874
2026-01-08 12:48:42,906: t15.2025.04.13 val PER: 0.3153
2026-01-08 12:48:52,048: Train batch 8600: loss: 22.99 grad norm: 63.45 time: 0.056
2026-01-08 12:49:10,731: Train batch 8800: loss: 21.09 grad norm: 73.57 time: 0.062
2026-01-08 12:49:28,261: Train batch 9000: loss: 22.99 grad norm: 71.57 time: 0.074
2026-01-08 12:49:28,261: Running test after training batch: 9000
2026-01-08 12:49:28,376: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:49:33,060: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the toehold hatz nist appointees his
2026-01-08 12:49:33,085: WER debug example
  GT : how does it keep the cost down
  PR : seaholm dease dusty hitz kicked thus cost
2026-01-08 12:49:34,416: Val batch 9000: PER (avg): 0.2392 CTC Loss (avg): 24.8424 WER(1gram): 102.28% (n=64) time: 6.154
2026-01-08 12:49:34,416: WER lens: avg_true_words=6.16 avg_pred_words=6.27 max_pred_words=12
2026-01-08 12:49:34,416: t15.2023.08.13 val PER: 0.2318
2026-01-08 12:49:34,416: t15.2023.08.18 val PER: 0.2137
2026-01-08 12:49:34,416: t15.2023.08.20 val PER: 0.1986
2026-01-08 12:49:34,416: t15.2023.08.25 val PER: 0.1973
2026-01-08 12:49:34,416: t15.2023.08.27 val PER: 0.2990
2026-01-08 12:49:34,416: t15.2023.09.01 val PER: 0.1826
2026-01-08 12:49:34,417: t15.2023.09.03 val PER: 0.2660
2026-01-08 12:49:34,417: t15.2023.09.24 val PER: 0.2136
2026-01-08 12:49:34,417: t15.2023.09.29 val PER: 0.2189
2026-01-08 12:49:34,417: t15.2023.10.01 val PER: 0.2517
2026-01-08 12:49:34,417: t15.2023.10.06 val PER: 0.1873
2026-01-08 12:49:34,417: t15.2023.10.08 val PER: 0.3329
2026-01-08 12:49:34,417: t15.2023.10.13 val PER: 0.3072
2026-01-08 12:49:34,417: t15.2023.10.15 val PER: 0.2360
2026-01-08 12:49:34,417: t15.2023.10.20 val PER: 0.2852
2026-01-08 12:49:34,417: t15.2023.10.22 val PER: 0.1938
2026-01-08 12:49:34,417: t15.2023.11.03 val PER: 0.2734
2026-01-08 12:49:34,417: t15.2023.11.04 val PER: 0.0922
2026-01-08 12:49:34,418: t15.2023.11.17 val PER: 0.1166
2026-01-08 12:49:34,418: t15.2023.11.19 val PER: 0.1357
2026-01-08 12:49:34,418: t15.2023.11.26 val PER: 0.2413
2026-01-08 12:49:34,418: t15.2023.12.03 val PER: 0.2206
2026-01-08 12:49:34,418: t15.2023.12.08 val PER: 0.1984
2026-01-08 12:49:34,418: t15.2023.12.10 val PER: 0.1761
2026-01-08 12:49:34,418: t15.2023.12.17 val PER: 0.2100
2026-01-08 12:49:34,418: t15.2023.12.29 val PER: 0.2443
2026-01-08 12:49:34,418: t15.2024.02.25 val PER: 0.2022
2026-01-08 12:49:34,418: t15.2024.03.08 val PER: 0.2930
2026-01-08 12:49:34,418: t15.2024.03.15 val PER: 0.2633
2026-01-08 12:49:34,418: t15.2024.03.17 val PER: 0.2252
2026-01-08 12:49:34,418: t15.2024.05.10 val PER: 0.2467
2026-01-08 12:49:34,418: t15.2024.06.14 val PER: 0.2256
2026-01-08 12:49:34,418: t15.2024.07.19 val PER: 0.2848
2026-01-08 12:49:34,418: t15.2024.07.21 val PER: 0.1572
2026-01-08 12:49:34,419: t15.2024.07.28 val PER: 0.2110
2026-01-08 12:49:34,419: t15.2025.01.10 val PER: 0.3636
2026-01-08 12:49:34,419: t15.2025.01.12 val PER: 0.2664
2026-01-08 12:49:34,419: t15.2025.03.14 val PER: 0.3861
2026-01-08 12:49:34,419: t15.2025.03.16 val PER: 0.2880
2026-01-08 12:49:34,419: t15.2025.03.30 val PER: 0.3575
2026-01-08 12:49:34,419: t15.2025.04.13 val PER: 0.3081
2026-01-08 12:49:51,979: Train batch 9200: loss: 17.34 grad norm: 61.03 time: 0.058
2026-01-08 12:50:08,871: Train batch 9400: loss: 15.06 grad norm: 59.83 time: 0.070
2026-01-08 12:50:17,330: Running test after training batch: 9500
2026-01-08 12:50:17,476: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:50:22,311: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the stuccoed hatz this appointees his swells
2026-01-08 12:50:22,334: WER debug example
  GT : how does it keep the cost down
  PR : cederholm decency hitz kicks thus costs
2026-01-08 12:50:23,670: Val batch 9500: PER (avg): 0.2350 CTC Loss (avg): 24.3417 WER(1gram): 97.97% (n=64) time: 6.340
2026-01-08 12:50:23,670: WER lens: avg_true_words=6.16 avg_pred_words=5.97 max_pred_words=11
2026-01-08 12:50:23,670: t15.2023.08.13 val PER: 0.2141
2026-01-08 12:50:23,670: t15.2023.08.18 val PER: 0.2129
2026-01-08 12:50:23,670: t15.2023.08.20 val PER: 0.2017
2026-01-08 12:50:23,670: t15.2023.08.25 val PER: 0.2063
2026-01-08 12:50:23,671: t15.2023.08.27 val PER: 0.2910
2026-01-08 12:50:23,671: t15.2023.09.01 val PER: 0.1672
2026-01-08 12:50:23,671: t15.2023.09.03 val PER: 0.2660
2026-01-08 12:50:23,671: t15.2023.09.24 val PER: 0.2087
2026-01-08 12:50:23,671: t15.2023.09.29 val PER: 0.2131
2026-01-08 12:50:23,671: t15.2023.10.01 val PER: 0.2543
2026-01-08 12:50:23,671: t15.2023.10.06 val PER: 0.1830
2026-01-08 12:50:23,671: t15.2023.10.08 val PER: 0.3248
2026-01-08 12:50:23,671: t15.2023.10.13 val PER: 0.3095
2026-01-08 12:50:23,671: t15.2023.10.15 val PER: 0.2367
2026-01-08 12:50:23,671: t15.2023.10.20 val PER: 0.2685
2026-01-08 12:50:23,671: t15.2023.10.22 val PER: 0.1715
2026-01-08 12:50:23,671: t15.2023.11.03 val PER: 0.2564
2026-01-08 12:50:23,672: t15.2023.11.04 val PER: 0.0990
2026-01-08 12:50:23,672: t15.2023.11.17 val PER: 0.1151
2026-01-08 12:50:23,672: t15.2023.11.19 val PER: 0.1257
2026-01-08 12:50:23,672: t15.2023.11.26 val PER: 0.2326
2026-01-08 12:50:23,672: t15.2023.12.03 val PER: 0.2153
2026-01-08 12:50:23,672: t15.2023.12.08 val PER: 0.1971
2026-01-08 12:50:23,672: t15.2023.12.10 val PER: 0.1787
2026-01-08 12:50:23,672: t15.2023.12.17 val PER: 0.1954
2026-01-08 12:50:23,672: t15.2023.12.29 val PER: 0.2361
2026-01-08 12:50:23,672: t15.2024.02.25 val PER: 0.1966
2026-01-08 12:50:23,672: t15.2024.03.08 val PER: 0.2888
2026-01-08 12:50:23,673: t15.2024.03.15 val PER: 0.2564
2026-01-08 12:50:23,673: t15.2024.03.17 val PER: 0.2218
2026-01-08 12:50:23,673: t15.2024.05.10 val PER: 0.2422
2026-01-08 12:50:23,673: t15.2024.06.14 val PER: 0.2256
2026-01-08 12:50:23,673: t15.2024.07.19 val PER: 0.2874
2026-01-08 12:50:23,673: t15.2024.07.21 val PER: 0.1607
2026-01-08 12:50:23,673: t15.2024.07.28 val PER: 0.2110
2026-01-08 12:50:23,673: t15.2025.01.10 val PER: 0.3609
2026-01-08 12:50:23,673: t15.2025.01.12 val PER: 0.2625
2026-01-08 12:50:23,673: t15.2025.03.14 val PER: 0.3772
2026-01-08 12:50:23,673: t15.2025.03.16 val PER: 0.2827
2026-01-08 12:50:23,673: t15.2025.03.30 val PER: 0.3701
2026-01-08 12:50:23,673: t15.2025.04.13 val PER: 0.2981
2026-01-08 12:50:32,277: Train batch 9600: loss: 15.42 grad norm: 52.73 time: 0.076
2026-01-08 12:50:49,259: Train batch 9800: loss: 15.51 grad norm: 62.92 time: 0.065
2026-01-08 12:51:06,750: Train batch 10000: loss: 10.21 grad norm: 46.85 time: 0.064
2026-01-08 12:51:06,751: Running test after training batch: 10000
2026-01-08 12:51:06,894: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:51:11,573: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the stuccoed hatz this appoint his swells
2026-01-08 12:51:11,596: WER debug example
  GT : how does it keep the cost down
  PR : teet hounds stacy hitz kicked thus cost
2026-01-08 12:51:12,937: Val batch 10000: PER (avg): 0.2319 CTC Loss (avg): 24.4233 WER(1gram): 97.72% (n=64) time: 6.186
2026-01-08 12:51:12,937: WER lens: avg_true_words=6.16 avg_pred_words=5.95 max_pred_words=11
2026-01-08 12:51:12,937: t15.2023.08.13 val PER: 0.2162
2026-01-08 12:51:12,937: t15.2023.08.18 val PER: 0.2163
2026-01-08 12:51:12,937: t15.2023.08.20 val PER: 0.2041
2026-01-08 12:51:12,937: t15.2023.08.25 val PER: 0.1988
2026-01-08 12:51:12,938: t15.2023.08.27 val PER: 0.2878
2026-01-08 12:51:12,938: t15.2023.09.01 val PER: 0.1810
2026-01-08 12:51:12,938: t15.2023.09.03 val PER: 0.2648
2026-01-08 12:51:12,938: t15.2023.09.24 val PER: 0.1857
2026-01-08 12:51:12,938: t15.2023.09.29 val PER: 0.2100
2026-01-08 12:51:12,938: t15.2023.10.01 val PER: 0.2411
2026-01-08 12:51:12,938: t15.2023.10.06 val PER: 0.1873
2026-01-08 12:51:12,938: t15.2023.10.08 val PER: 0.3194
2026-01-08 12:51:12,938: t15.2023.10.13 val PER: 0.3026
2026-01-08 12:51:12,938: t15.2023.10.15 val PER: 0.2261
2026-01-08 12:51:12,938: t15.2023.10.20 val PER: 0.2718
2026-01-08 12:51:12,938: t15.2023.10.22 val PER: 0.1804
2026-01-08 12:51:12,938: t15.2023.11.03 val PER: 0.2612
2026-01-08 12:51:12,938: t15.2023.11.04 val PER: 0.1024
2026-01-08 12:51:12,939: t15.2023.11.17 val PER: 0.1120
2026-01-08 12:51:12,939: t15.2023.11.19 val PER: 0.1198
2026-01-08 12:51:12,939: t15.2023.11.26 val PER: 0.2246
2026-01-08 12:51:12,939: t15.2023.12.03 val PER: 0.2048
2026-01-08 12:51:12,939: t15.2023.12.08 val PER: 0.1904
2026-01-08 12:51:12,939: t15.2023.12.10 val PER: 0.1748
2026-01-08 12:51:12,939: t15.2023.12.17 val PER: 0.1985
2026-01-08 12:51:12,939: t15.2023.12.29 val PER: 0.2237
2026-01-08 12:51:12,939: t15.2024.02.25 val PER: 0.2051
2026-01-08 12:51:12,939: t15.2024.03.08 val PER: 0.2845
2026-01-08 12:51:12,939: t15.2024.03.15 val PER: 0.2483
2026-01-08 12:51:12,940: t15.2024.03.17 val PER: 0.2141
2026-01-08 12:51:12,940: t15.2024.05.10 val PER: 0.2556
2026-01-08 12:51:12,940: t15.2024.06.14 val PER: 0.2287
2026-01-08 12:51:12,940: t15.2024.07.19 val PER: 0.2854
2026-01-08 12:51:12,940: t15.2024.07.21 val PER: 0.1559
2026-01-08 12:51:12,940: t15.2024.07.28 val PER: 0.2125
2026-01-08 12:51:12,940: t15.2025.01.10 val PER: 0.3623
2026-01-08 12:51:12,940: t15.2025.01.12 val PER: 0.2571
2026-01-08 12:51:12,940: t15.2025.03.14 val PER: 0.3728
2026-01-08 12:51:12,940: t15.2025.03.16 val PER: 0.2788
2026-01-08 12:51:12,940: t15.2025.03.30 val PER: 0.3644
2026-01-08 12:51:12,940: t15.2025.04.13 val PER: 0.2882
2026-01-08 12:51:30,933: Train batch 10200: loss: 14.78 grad norm: 54.02 time: 0.052
2026-01-08 12:51:48,641: Train batch 10400: loss: 16.61 grad norm: 58.51 time: 0.074
2026-01-08 12:51:58,079: Running test after training batch: 10500
2026-01-08 12:51:58,181: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:52:02,843: WER debug example
  GT : you can see the code at this point as well
  PR : euclea candies sikhs the tutko deet gatz this appoint his swells
2026-01-08 12:52:02,867: WER debug example
  GT : how does it keep the cost down
  PR : shouse dust hitz keep coots thus cost
2026-01-08 12:52:04,209: Val batch 10500: PER (avg): 0.2271 CTC Loss (avg): 24.1885 WER(1gram): 98.48% (n=64) time: 6.130
2026-01-08 12:52:04,210: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=12
2026-01-08 12:52:04,210: t15.2023.08.13 val PER: 0.2141
2026-01-08 12:52:04,210: t15.2023.08.18 val PER: 0.2087
2026-01-08 12:52:04,210: t15.2023.08.20 val PER: 0.1962
2026-01-08 12:52:04,210: t15.2023.08.25 val PER: 0.2048
2026-01-08 12:52:04,210: t15.2023.08.27 val PER: 0.2701
2026-01-08 12:52:04,210: t15.2023.09.01 val PER: 0.1688
2026-01-08 12:52:04,210: t15.2023.09.03 val PER: 0.2482
2026-01-08 12:52:04,210: t15.2023.09.24 val PER: 0.2002
2026-01-08 12:52:04,210: t15.2023.09.29 val PER: 0.2093
2026-01-08 12:52:04,210: t15.2023.10.01 val PER: 0.2371
2026-01-08 12:52:04,210: t15.2023.10.06 val PER: 0.1744
2026-01-08 12:52:04,211: t15.2023.10.08 val PER: 0.3180
2026-01-08 12:52:04,211: t15.2023.10.13 val PER: 0.2901
2026-01-08 12:52:04,211: t15.2023.10.15 val PER: 0.2307
2026-01-08 12:52:04,211: t15.2023.10.20 val PER: 0.2651
2026-01-08 12:52:04,211: t15.2023.10.22 val PER: 0.1670
2026-01-08 12:52:04,211: t15.2023.11.03 val PER: 0.2517
2026-01-08 12:52:04,211: t15.2023.11.04 val PER: 0.1024
2026-01-08 12:52:04,211: t15.2023.11.17 val PER: 0.1182
2026-01-08 12:52:04,211: t15.2023.11.19 val PER: 0.1337
2026-01-08 12:52:04,211: t15.2023.11.26 val PER: 0.2123
2026-01-08 12:52:04,211: t15.2023.12.03 val PER: 0.2006
2026-01-08 12:52:04,211: t15.2023.12.08 val PER: 0.1917
2026-01-08 12:52:04,211: t15.2023.12.10 val PER: 0.1643
2026-01-08 12:52:04,211: t15.2023.12.17 val PER: 0.1809
2026-01-08 12:52:04,211: t15.2023.12.29 val PER: 0.2340
2026-01-08 12:52:04,212: t15.2024.02.25 val PER: 0.1966
2026-01-08 12:52:04,212: t15.2024.03.08 val PER: 0.2703
2026-01-08 12:52:04,212: t15.2024.03.15 val PER: 0.2583
2026-01-08 12:52:04,212: t15.2024.03.17 val PER: 0.2218
2026-01-08 12:52:04,212: t15.2024.05.10 val PER: 0.2377
2026-01-08 12:52:04,212: t15.2024.06.14 val PER: 0.2161
2026-01-08 12:52:04,212: t15.2024.07.19 val PER: 0.2821
2026-01-08 12:52:04,212: t15.2024.07.21 val PER: 0.1462
2026-01-08 12:52:04,212: t15.2024.07.28 val PER: 0.1956
2026-01-08 12:52:04,212: t15.2025.01.10 val PER: 0.3554
2026-01-08 12:52:04,212: t15.2025.01.12 val PER: 0.2540
2026-01-08 12:52:04,213: t15.2025.03.14 val PER: 0.3757
2026-01-08 12:52:04,213: t15.2025.03.16 val PER: 0.2644
2026-01-08 12:52:04,213: t15.2025.03.30 val PER: 0.3586
2026-01-08 12:52:04,213: t15.2025.04.13 val PER: 0.2839
2026-01-08 12:52:13,757: Train batch 10600: loss: 13.29 grad norm: 53.43 time: 0.075
2026-01-08 12:52:31,705: Train batch 10800: loss: 19.99 grad norm: 69.72 time: 0.068
2026-01-08 12:52:49,241: Train batch 11000: loss: 20.96 grad norm: 70.20 time: 0.058
2026-01-08 12:52:49,241: Running test after training batch: 11000
2026-01-08 12:52:49,392: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:52:54,206: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the stuccoed hatz this appoint has swells
2026-01-08 12:52:54,231: WER debug example
  GT : how does it keep the cost down
  PR : teet hounds stacy hitt kicked thus costs
2026-01-08 12:52:55,614: Val batch 11000: PER (avg): 0.2267 CTC Loss (avg): 24.0121 WER(1gram): 99.75% (n=64) time: 6.372
2026-01-08 12:52:55,614: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-08 12:52:55,614: t15.2023.08.13 val PER: 0.2100
2026-01-08 12:52:55,614: t15.2023.08.18 val PER: 0.2196
2026-01-08 12:52:55,615: t15.2023.08.20 val PER: 0.1938
2026-01-08 12:52:55,615: t15.2023.08.25 val PER: 0.2018
2026-01-08 12:52:55,615: t15.2023.08.27 val PER: 0.2685
2026-01-08 12:52:55,615: t15.2023.09.01 val PER: 0.1664
2026-01-08 12:52:55,615: t15.2023.09.03 val PER: 0.2565
2026-01-08 12:52:55,615: t15.2023.09.24 val PER: 0.1954
2026-01-08 12:52:55,615: t15.2023.09.29 val PER: 0.2036
2026-01-08 12:52:55,615: t15.2023.10.01 val PER: 0.2378
2026-01-08 12:52:55,615: t15.2023.10.06 val PER: 0.1701
2026-01-08 12:52:55,615: t15.2023.10.08 val PER: 0.3058
2026-01-08 12:52:55,616: t15.2023.10.13 val PER: 0.3041
2026-01-08 12:52:55,616: t15.2023.10.15 val PER: 0.2215
2026-01-08 12:52:55,616: t15.2023.10.20 val PER: 0.2584
2026-01-08 12:52:55,616: t15.2023.10.22 val PER: 0.1693
2026-01-08 12:52:55,616: t15.2023.11.03 val PER: 0.2524
2026-01-08 12:52:55,616: t15.2023.11.04 val PER: 0.1126
2026-01-08 12:52:55,616: t15.2023.11.17 val PER: 0.1198
2026-01-08 12:52:55,616: t15.2023.11.19 val PER: 0.1317
2026-01-08 12:52:55,616: t15.2023.11.26 val PER: 0.2239
2026-01-08 12:52:55,616: t15.2023.12.03 val PER: 0.1996
2026-01-08 12:52:55,617: t15.2023.12.08 val PER: 0.1864
2026-01-08 12:52:55,617: t15.2023.12.10 val PER: 0.1616
2026-01-08 12:52:55,617: t15.2023.12.17 val PER: 0.1819
2026-01-08 12:52:55,617: t15.2023.12.29 val PER: 0.2231
2026-01-08 12:52:55,617: t15.2024.02.25 val PER: 0.1868
2026-01-08 12:52:55,617: t15.2024.03.08 val PER: 0.2731
2026-01-08 12:52:55,617: t15.2024.03.15 val PER: 0.2520
2026-01-08 12:52:55,617: t15.2024.03.17 val PER: 0.2141
2026-01-08 12:52:55,617: t15.2024.05.10 val PER: 0.2377
2026-01-08 12:52:55,617: t15.2024.06.14 val PER: 0.2224
2026-01-08 12:52:55,618: t15.2024.07.19 val PER: 0.2762
2026-01-08 12:52:55,618: t15.2024.07.21 val PER: 0.1503
2026-01-08 12:52:55,618: t15.2024.07.28 val PER: 0.2000
2026-01-08 12:52:55,618: t15.2025.01.10 val PER: 0.3636
2026-01-08 12:52:55,618: t15.2025.01.12 val PER: 0.2587
2026-01-08 12:52:55,618: t15.2025.03.14 val PER: 0.3624
2026-01-08 12:52:55,618: t15.2025.03.16 val PER: 0.2788
2026-01-08 12:52:55,618: t15.2025.03.30 val PER: 0.3655
2026-01-08 12:52:55,618: t15.2025.04.13 val PER: 0.2867
2026-01-08 12:53:12,876: Train batch 11200: loss: 15.31 grad norm: 59.61 time: 0.073
2026-01-08 12:53:30,150: Train batch 11400: loss: 15.06 grad norm: 58.34 time: 0.060
2026-01-08 12:53:39,113: Running test after training batch: 11500
2026-01-08 12:53:39,216: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:53:43,961: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the stuccoed hatz this appointees his swells
2026-01-08 12:53:43,987: WER debug example
  GT : how does it keep the cost down
  PR : teet hounds stutz hitz kicks thus cost
2026-01-08 12:53:45,343: Val batch 11500: PER (avg): 0.2232 CTC Loss (avg): 23.8091 WER(1gram): 100.51% (n=64) time: 6.229
2026-01-08 12:53:45,343: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=12
2026-01-08 12:53:45,343: t15.2023.08.13 val PER: 0.2141
2026-01-08 12:53:45,343: t15.2023.08.18 val PER: 0.1987
2026-01-08 12:53:45,344: t15.2023.08.20 val PER: 0.1962
2026-01-08 12:53:45,344: t15.2023.08.25 val PER: 0.2003
2026-01-08 12:53:45,344: t15.2023.08.27 val PER: 0.2797
2026-01-08 12:53:45,344: t15.2023.09.01 val PER: 0.1591
2026-01-08 12:53:45,344: t15.2023.09.03 val PER: 0.2530
2026-01-08 12:53:45,344: t15.2023.09.24 val PER: 0.1954
2026-01-08 12:53:45,344: t15.2023.09.29 val PER: 0.2055
2026-01-08 12:53:45,344: t15.2023.10.01 val PER: 0.2450
2026-01-08 12:53:45,345: t15.2023.10.06 val PER: 0.1722
2026-01-08 12:53:45,345: t15.2023.10.08 val PER: 0.2963
2026-01-08 12:53:45,345: t15.2023.10.13 val PER: 0.2901
2026-01-08 12:53:45,345: t15.2023.10.15 val PER: 0.2314
2026-01-08 12:53:45,345: t15.2023.10.20 val PER: 0.2416
2026-01-08 12:53:45,345: t15.2023.10.22 val PER: 0.1715
2026-01-08 12:53:45,345: t15.2023.11.03 val PER: 0.2497
2026-01-08 12:53:45,345: t15.2023.11.04 val PER: 0.0922
2026-01-08 12:53:45,345: t15.2023.11.17 val PER: 0.1213
2026-01-08 12:53:45,346: t15.2023.11.19 val PER: 0.1257
2026-01-08 12:53:45,346: t15.2023.11.26 val PER: 0.2159
2026-01-08 12:53:45,346: t15.2023.12.03 val PER: 0.2006
2026-01-08 12:53:45,346: t15.2023.12.08 val PER: 0.1884
2026-01-08 12:53:45,346: t15.2023.12.10 val PER: 0.1616
2026-01-08 12:53:45,346: t15.2023.12.17 val PER: 0.1632
2026-01-08 12:53:45,346: t15.2023.12.29 val PER: 0.2121
2026-01-08 12:53:45,346: t15.2024.02.25 val PER: 0.1910
2026-01-08 12:53:45,346: t15.2024.03.08 val PER: 0.2717
2026-01-08 12:53:45,346: t15.2024.03.15 val PER: 0.2489
2026-01-08 12:53:45,347: t15.2024.03.17 val PER: 0.2120
2026-01-08 12:53:45,347: t15.2024.05.10 val PER: 0.2363
2026-01-08 12:53:45,347: t15.2024.06.14 val PER: 0.2303
2026-01-08 12:53:45,347: t15.2024.07.19 val PER: 0.2775
2026-01-08 12:53:45,347: t15.2024.07.21 val PER: 0.1386
2026-01-08 12:53:45,347: t15.2024.07.28 val PER: 0.1904
2026-01-08 12:53:45,347: t15.2025.01.10 val PER: 0.3540
2026-01-08 12:53:45,347: t15.2025.01.12 val PER: 0.2440
2026-01-08 12:53:45,347: t15.2025.03.14 val PER: 0.3609
2026-01-08 12:53:45,348: t15.2025.03.16 val PER: 0.2709
2026-01-08 12:53:45,348: t15.2025.03.30 val PER: 0.3471
2026-01-08 12:53:45,348: t15.2025.04.13 val PER: 0.2967
2026-01-08 12:53:53,811: Train batch 11600: loss: 17.26 grad norm: 60.64 time: 0.063
2026-01-08 12:54:10,918: Train batch 11800: loss: 13.81 grad norm: 51.73 time: 0.046
2026-01-08 12:54:28,296: Train batch 12000: loss: 18.80 grad norm: 57.33 time: 0.073
2026-01-08 12:54:28,296: Running test after training batch: 12000
2026-01-08 12:54:28,386: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:54:33,083: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the toehold hatz this appointees has swells
2026-01-08 12:54:33,109: WER debug example
  GT : how does it keep the cost down
  PR : shouse dusts hitz keep kerce thus cost
2026-01-08 12:54:34,479: Val batch 12000: PER (avg): 0.2202 CTC Loss (avg): 23.5535 WER(1gram): 98.73% (n=64) time: 6.183
2026-01-08 12:54:34,480: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-08 12:54:34,480: t15.2023.08.13 val PER: 0.2089
2026-01-08 12:54:34,480: t15.2023.08.18 val PER: 0.2062
2026-01-08 12:54:34,480: t15.2023.08.20 val PER: 0.1890
2026-01-08 12:54:34,481: t15.2023.08.25 val PER: 0.1867
2026-01-08 12:54:34,481: t15.2023.08.27 val PER: 0.2749
2026-01-08 12:54:34,481: t15.2023.09.01 val PER: 0.1623
2026-01-08 12:54:34,481: t15.2023.09.03 val PER: 0.2530
2026-01-08 12:54:34,481: t15.2023.09.24 val PER: 0.1857
2026-01-08 12:54:34,481: t15.2023.09.29 val PER: 0.2036
2026-01-08 12:54:34,481: t15.2023.10.01 val PER: 0.2325
2026-01-08 12:54:34,481: t15.2023.10.06 val PER: 0.1625
2026-01-08 12:54:34,481: t15.2023.10.08 val PER: 0.2977
2026-01-08 12:54:34,481: t15.2023.10.13 val PER: 0.2793
2026-01-08 12:54:34,482: t15.2023.10.15 val PER: 0.2268
2026-01-08 12:54:34,482: t15.2023.10.20 val PER: 0.2483
2026-01-08 12:54:34,482: t15.2023.10.22 val PER: 0.1626
2026-01-08 12:54:34,482: t15.2023.11.03 val PER: 0.2402
2026-01-08 12:54:34,482: t15.2023.11.04 val PER: 0.1092
2026-01-08 12:54:34,482: t15.2023.11.17 val PER: 0.1151
2026-01-08 12:54:34,482: t15.2023.11.19 val PER: 0.1178
2026-01-08 12:54:34,482: t15.2023.11.26 val PER: 0.2167
2026-01-08 12:54:34,482: t15.2023.12.03 val PER: 0.2080
2026-01-08 12:54:34,482: t15.2023.12.08 val PER: 0.1738
2026-01-08 12:54:34,483: t15.2023.12.10 val PER: 0.1669
2026-01-08 12:54:34,483: t15.2023.12.17 val PER: 0.1715
2026-01-08 12:54:34,483: t15.2023.12.29 val PER: 0.2121
2026-01-08 12:54:34,483: t15.2024.02.25 val PER: 0.1770
2026-01-08 12:54:34,483: t15.2024.03.08 val PER: 0.2688
2026-01-08 12:54:34,483: t15.2024.03.15 val PER: 0.2383
2026-01-08 12:54:34,483: t15.2024.03.17 val PER: 0.2106
2026-01-08 12:54:34,484: t15.2024.05.10 val PER: 0.2288
2026-01-08 12:54:34,484: t15.2024.06.14 val PER: 0.2240
2026-01-08 12:54:34,484: t15.2024.07.19 val PER: 0.2690
2026-01-08 12:54:34,484: t15.2024.07.21 val PER: 0.1441
2026-01-08 12:54:34,484: t15.2024.07.28 val PER: 0.1926
2026-01-08 12:54:34,484: t15.2025.01.10 val PER: 0.3471
2026-01-08 12:54:34,484: t15.2025.01.12 val PER: 0.2471
2026-01-08 12:54:34,484: t15.2025.03.14 val PER: 0.3787
2026-01-08 12:54:34,484: t15.2025.03.16 val PER: 0.2696
2026-01-08 12:54:34,485: t15.2025.03.30 val PER: 0.3529
2026-01-08 12:54:34,485: t15.2025.04.13 val PER: 0.2967
2026-01-08 12:54:51,733: Train batch 12200: loss: 13.53 grad norm: 55.01 time: 0.069
2026-01-08 12:55:09,207: Train batch 12400: loss: 10.26 grad norm: 43.22 time: 0.041
2026-01-08 12:55:18,116: Running test after training batch: 12500
2026-01-08 12:55:18,258: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:55:23,075: WER debug example
  GT : you can see the code at this point as well
  PR : euclea candies sikhs the stuccoed hatz this appointees tis swells
2026-01-08 12:55:23,101: WER debug example
  GT : how does it keep the cost down
  PR : steinhaus dusts hitz keep kerce thus cost
2026-01-08 12:55:24,477: Val batch 12500: PER (avg): 0.2191 CTC Loss (avg): 23.4137 WER(1gram): 96.19% (n=64) time: 6.361
2026-01-08 12:55:24,478: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=11
2026-01-08 12:55:24,478: t15.2023.08.13 val PER: 0.2069
2026-01-08 12:55:24,478: t15.2023.08.18 val PER: 0.2045
2026-01-08 12:55:24,478: t15.2023.08.20 val PER: 0.1875
2026-01-08 12:55:24,478: t15.2023.08.25 val PER: 0.1973
2026-01-08 12:55:24,478: t15.2023.08.27 val PER: 0.2669
2026-01-08 12:55:24,478: t15.2023.09.01 val PER: 0.1599
2026-01-08 12:55:24,478: t15.2023.09.03 val PER: 0.2411
2026-01-08 12:55:24,478: t15.2023.09.24 val PER: 0.1905
2026-01-08 12:55:24,478: t15.2023.09.29 val PER: 0.1972
2026-01-08 12:55:24,478: t15.2023.10.01 val PER: 0.2279
2026-01-08 12:55:24,478: t15.2023.10.06 val PER: 0.1690
2026-01-08 12:55:24,479: t15.2023.10.08 val PER: 0.3045
2026-01-08 12:55:24,479: t15.2023.10.13 val PER: 0.2847
2026-01-08 12:55:24,479: t15.2023.10.15 val PER: 0.2254
2026-01-08 12:55:24,479: t15.2023.10.20 val PER: 0.2517
2026-01-08 12:55:24,479: t15.2023.10.22 val PER: 0.1514
2026-01-08 12:55:24,479: t15.2023.11.03 val PER: 0.2415
2026-01-08 12:55:24,479: t15.2023.11.04 val PER: 0.0990
2026-01-08 12:55:24,479: t15.2023.11.17 val PER: 0.1135
2026-01-08 12:55:24,480: t15.2023.11.19 val PER: 0.1198
2026-01-08 12:55:24,480: t15.2023.11.26 val PER: 0.2138
2026-01-08 12:55:24,480: t15.2023.12.03 val PER: 0.1933
2026-01-08 12:55:24,480: t15.2023.12.08 val PER: 0.1844
2026-01-08 12:55:24,480: t15.2023.12.10 val PER: 0.1498
2026-01-08 12:55:24,480: t15.2023.12.17 val PER: 0.1642
2026-01-08 12:55:24,480: t15.2023.12.29 val PER: 0.2162
2026-01-08 12:55:24,480: t15.2024.02.25 val PER: 0.1713
2026-01-08 12:55:24,480: t15.2024.03.08 val PER: 0.2660
2026-01-08 12:55:24,480: t15.2024.03.15 val PER: 0.2445
2026-01-08 12:55:24,481: t15.2024.03.17 val PER: 0.2099
2026-01-08 12:55:24,481: t15.2024.05.10 val PER: 0.2348
2026-01-08 12:55:24,481: t15.2024.06.14 val PER: 0.2303
2026-01-08 12:55:24,481: t15.2024.07.19 val PER: 0.2637
2026-01-08 12:55:24,481: t15.2024.07.21 val PER: 0.1469
2026-01-08 12:55:24,481: t15.2024.07.28 val PER: 0.1971
2026-01-08 12:55:24,481: t15.2025.01.10 val PER: 0.3471
2026-01-08 12:55:24,482: t15.2025.01.12 val PER: 0.2487
2026-01-08 12:55:24,482: t15.2025.03.14 val PER: 0.3698
2026-01-08 12:55:24,482: t15.2025.03.16 val PER: 0.2696
2026-01-08 12:55:24,482: t15.2025.03.30 val PER: 0.3517
2026-01-08 12:55:24,482: t15.2025.04.13 val PER: 0.2796
2026-01-08 12:55:24,482: New best val WER(1gram) 96.95% --> 96.19%
2026-01-08 12:55:24,482: Checkpointing model
2026-01-08 12:55:24,627: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_2blocks_ln/checkpoint/best_checkpoint
2026-01-08 12:55:33,148: Train batch 12600: loss: 12.74 grad norm: 50.77 time: 0.060
2026-01-08 12:55:51,081: Train batch 12800: loss: 11.98 grad norm: 49.21 time: 0.054
2026-01-08 12:56:09,846: Train batch 13000: loss: 12.22 grad norm: 53.70 time: 0.069
2026-01-08 12:56:09,846: Running test after training batch: 13000
2026-01-08 12:56:09,976: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:56:14,701: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the stuccoed hatz this appointees his swells
2026-01-08 12:56:14,729: WER debug example
  GT : how does it keep the cost down
  PR : shouse dusts hitz kicks thus cost
2026-01-08 12:56:16,287: Val batch 13000: PER (avg): 0.2170 CTC Loss (avg): 23.4002 WER(1gram): 98.48% (n=64) time: 6.440
2026-01-08 12:56:16,287: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=12
2026-01-08 12:56:16,287: t15.2023.08.13 val PER: 0.2027
2026-01-08 12:56:16,287: t15.2023.08.18 val PER: 0.1945
2026-01-08 12:56:16,287: t15.2023.08.20 val PER: 0.1930
2026-01-08 12:56:16,287: t15.2023.08.25 val PER: 0.2018
2026-01-08 12:56:16,287: t15.2023.08.27 val PER: 0.2685
2026-01-08 12:56:16,288: t15.2023.09.01 val PER: 0.1567
2026-01-08 12:56:16,288: t15.2023.09.03 val PER: 0.2553
2026-01-08 12:56:16,288: t15.2023.09.24 val PER: 0.1905
2026-01-08 12:56:16,288: t15.2023.09.29 val PER: 0.1978
2026-01-08 12:56:16,288: t15.2023.10.01 val PER: 0.2299
2026-01-08 12:56:16,288: t15.2023.10.06 val PER: 0.1765
2026-01-08 12:56:16,288: t15.2023.10.08 val PER: 0.2991
2026-01-08 12:56:16,288: t15.2023.10.13 val PER: 0.2863
2026-01-08 12:56:16,288: t15.2023.10.15 val PER: 0.2261
2026-01-08 12:56:16,288: t15.2023.10.20 val PER: 0.2483
2026-01-08 12:56:16,288: t15.2023.10.22 val PER: 0.1548
2026-01-08 12:56:16,288: t15.2023.11.03 val PER: 0.2463
2026-01-08 12:56:16,288: t15.2023.11.04 val PER: 0.0887
2026-01-08 12:56:16,289: t15.2023.11.17 val PER: 0.1182
2026-01-08 12:56:16,289: t15.2023.11.19 val PER: 0.1118
2026-01-08 12:56:16,289: t15.2023.11.26 val PER: 0.2101
2026-01-08 12:56:16,289: t15.2023.12.03 val PER: 0.1996
2026-01-08 12:56:16,289: t15.2023.12.08 val PER: 0.1738
2026-01-08 12:56:16,289: t15.2023.12.10 val PER: 0.1524
2026-01-08 12:56:16,289: t15.2023.12.17 val PER: 0.1705
2026-01-08 12:56:16,289: t15.2023.12.29 val PER: 0.2141
2026-01-08 12:56:16,289: t15.2024.02.25 val PER: 0.1742
2026-01-08 12:56:16,289: t15.2024.03.08 val PER: 0.2674
2026-01-08 12:56:16,289: t15.2024.03.15 val PER: 0.2339
2026-01-08 12:56:16,289: t15.2024.03.17 val PER: 0.2029
2026-01-08 12:56:16,289: t15.2024.05.10 val PER: 0.2214
2026-01-08 12:56:16,289: t15.2024.06.14 val PER: 0.2192
2026-01-08 12:56:16,289: t15.2024.07.19 val PER: 0.2722
2026-01-08 12:56:16,289: t15.2024.07.21 val PER: 0.1400
2026-01-08 12:56:16,289: t15.2024.07.28 val PER: 0.1809
2026-01-08 12:56:16,290: t15.2025.01.10 val PER: 0.3375
2026-01-08 12:56:16,290: t15.2025.01.12 val PER: 0.2440
2026-01-08 12:56:16,290: t15.2025.03.14 val PER: 0.3595
2026-01-08 12:56:16,290: t15.2025.03.16 val PER: 0.2539
2026-01-08 12:56:16,290: t15.2025.03.30 val PER: 0.3425
2026-01-08 12:56:16,290: t15.2025.04.13 val PER: 0.2839
2026-01-08 12:56:34,245: Train batch 13200: loss: 17.76 grad norm: 73.36 time: 0.055
2026-01-08 12:56:51,613: Train batch 13400: loss: 14.10 grad norm: 62.27 time: 0.065
2026-01-08 12:57:00,248: Running test after training batch: 13500
2026-01-08 12:57:00,368: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:57:05,084: WER debug example
  GT : you can see the code at this point as well
  PR : yuli candies sikhs the tickled hatz this appointees has swells
2026-01-08 12:57:05,110: WER debug example
  GT : how does it keep the cost down
  PR : cederholm dease dusts hitz kicks thus costs detzel
2026-01-08 12:57:06,506: Val batch 13500: PER (avg): 0.2144 CTC Loss (avg): 23.0455 WER(1gram): 98.98% (n=64) time: 6.257
2026-01-08 12:57:06,506: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-08 12:57:06,506: t15.2023.08.13 val PER: 0.2037
2026-01-08 12:57:06,507: t15.2023.08.18 val PER: 0.1936
2026-01-08 12:57:06,507: t15.2023.08.20 val PER: 0.1811
2026-01-08 12:57:06,507: t15.2023.08.25 val PER: 0.1807
2026-01-08 12:57:06,507: t15.2023.08.27 val PER: 0.2508
2026-01-08 12:57:06,507: t15.2023.09.01 val PER: 0.1461
2026-01-08 12:57:06,507: t15.2023.09.03 val PER: 0.2447
2026-01-08 12:57:06,508: t15.2023.09.24 val PER: 0.1917
2026-01-08 12:57:06,508: t15.2023.09.29 val PER: 0.1978
2026-01-08 12:57:06,508: t15.2023.10.01 val PER: 0.2285
2026-01-08 12:57:06,508: t15.2023.10.06 val PER: 0.1798
2026-01-08 12:57:06,508: t15.2023.10.08 val PER: 0.2923
2026-01-08 12:57:06,508: t15.2023.10.13 val PER: 0.2855
2026-01-08 12:57:06,508: t15.2023.10.15 val PER: 0.2268
2026-01-08 12:57:06,508: t15.2023.10.20 val PER: 0.2282
2026-01-08 12:57:06,508: t15.2023.10.22 val PER: 0.1659
2026-01-08 12:57:06,508: t15.2023.11.03 val PER: 0.2415
2026-01-08 12:57:06,509: t15.2023.11.04 val PER: 0.0922
2026-01-08 12:57:06,509: t15.2023.11.17 val PER: 0.1011
2026-01-08 12:57:06,509: t15.2023.11.19 val PER: 0.1098
2026-01-08 12:57:06,509: t15.2023.11.26 val PER: 0.2014
2026-01-08 12:57:06,509: t15.2023.12.03 val PER: 0.1838
2026-01-08 12:57:06,509: t15.2023.12.08 val PER: 0.1678
2026-01-08 12:57:06,509: t15.2023.12.10 val PER: 0.1551
2026-01-08 12:57:06,509: t15.2023.12.17 val PER: 0.1663
2026-01-08 12:57:06,509: t15.2023.12.29 val PER: 0.2203
2026-01-08 12:57:06,510: t15.2024.02.25 val PER: 0.1643
2026-01-08 12:57:06,510: t15.2024.03.08 val PER: 0.2560
2026-01-08 12:57:06,510: t15.2024.03.15 val PER: 0.2289
2026-01-08 12:57:06,510: t15.2024.03.17 val PER: 0.2169
2026-01-08 12:57:06,510: t15.2024.05.10 val PER: 0.2318
2026-01-08 12:57:06,510: t15.2024.06.14 val PER: 0.2192
2026-01-08 12:57:06,510: t15.2024.07.19 val PER: 0.2676
2026-01-08 12:57:06,511: t15.2024.07.21 val PER: 0.1372
2026-01-08 12:57:06,511: t15.2024.07.28 val PER: 0.1868
2026-01-08 12:57:06,511: t15.2025.01.10 val PER: 0.3388
2026-01-08 12:57:06,511: t15.2025.01.12 val PER: 0.2371
2026-01-08 12:57:06,511: t15.2025.03.14 val PER: 0.3713
2026-01-08 12:57:06,511: t15.2025.03.16 val PER: 0.2513
2026-01-08 12:57:06,511: t15.2025.03.30 val PER: 0.3448
2026-01-08 12:57:06,511: t15.2025.04.13 val PER: 0.2825
2026-01-08 12:57:15,280: Train batch 13600: loss: 17.44 grad norm: 66.11 time: 0.063
2026-01-08 12:57:32,948: Train batch 13800: loss: 12.20 grad norm: 55.86 time: 0.057
2026-01-08 12:57:50,378: Train batch 14000: loss: 18.57 grad norm: 71.32 time: 0.052
2026-01-08 12:57:50,378: Running test after training batch: 14000
2026-01-08 12:57:50,488: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:57:55,457: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the stuccoed hatz this appoint his swells
2026-01-08 12:57:55,484: WER debug example
  GT : how does it keep the cost down
  PR : erte hounds stacy hitt keep coot thus costs
2026-01-08 12:57:57,156: Val batch 14000: PER (avg): 0.2152 CTC Loss (avg): 23.0689 WER(1gram): 97.97% (n=64) time: 6.778
2026-01-08 12:57:57,157: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-08 12:57:57,157: t15.2023.08.13 val PER: 0.2121
2026-01-08 12:57:57,157: t15.2023.08.18 val PER: 0.1953
2026-01-08 12:57:57,157: t15.2023.08.20 val PER: 0.1851
2026-01-08 12:57:57,158: t15.2023.08.25 val PER: 0.1822
2026-01-08 12:57:57,158: t15.2023.08.27 val PER: 0.2588
2026-01-08 12:57:57,158: t15.2023.09.01 val PER: 0.1526
2026-01-08 12:57:57,158: t15.2023.09.03 val PER: 0.2458
2026-01-08 12:57:57,158: t15.2023.09.24 val PER: 0.1881
2026-01-08 12:57:57,158: t15.2023.09.29 val PER: 0.2004
2026-01-08 12:57:57,158: t15.2023.10.01 val PER: 0.2246
2026-01-08 12:57:57,158: t15.2023.10.06 val PER: 0.1722
2026-01-08 12:57:57,158: t15.2023.10.08 val PER: 0.2991
2026-01-08 12:57:57,158: t15.2023.10.13 val PER: 0.2801
2026-01-08 12:57:57,158: t15.2023.10.15 val PER: 0.2235
2026-01-08 12:57:57,158: t15.2023.10.20 val PER: 0.2349
2026-01-08 12:57:57,158: t15.2023.10.22 val PER: 0.1637
2026-01-08 12:57:57,158: t15.2023.11.03 val PER: 0.2456
2026-01-08 12:57:57,159: t15.2023.11.04 val PER: 0.0990
2026-01-08 12:57:57,159: t15.2023.11.17 val PER: 0.1058
2026-01-08 12:57:57,159: t15.2023.11.19 val PER: 0.1178
2026-01-08 12:57:57,159: t15.2023.11.26 val PER: 0.1993
2026-01-08 12:57:57,159: t15.2023.12.03 val PER: 0.1996
2026-01-08 12:57:57,159: t15.2023.12.08 val PER: 0.1758
2026-01-08 12:57:57,159: t15.2023.12.10 val PER: 0.1524
2026-01-08 12:57:57,159: t15.2023.12.17 val PER: 0.1653
2026-01-08 12:57:57,159: t15.2023.12.29 val PER: 0.2128
2026-01-08 12:57:57,159: t15.2024.02.25 val PER: 0.1742
2026-01-08 12:57:57,159: t15.2024.03.08 val PER: 0.2646
2026-01-08 12:57:57,159: t15.2024.03.15 val PER: 0.2314
2026-01-08 12:57:57,159: t15.2024.03.17 val PER: 0.2064
2026-01-08 12:57:57,159: t15.2024.05.10 val PER: 0.2333
2026-01-08 12:57:57,159: t15.2024.06.14 val PER: 0.2208
2026-01-08 12:57:57,159: t15.2024.07.19 val PER: 0.2676
2026-01-08 12:57:57,160: t15.2024.07.21 val PER: 0.1359
2026-01-08 12:57:57,160: t15.2024.07.28 val PER: 0.1890
2026-01-08 12:57:57,160: t15.2025.01.10 val PER: 0.3416
2026-01-08 12:57:57,160: t15.2025.01.12 val PER: 0.2417
2026-01-08 12:57:57,160: t15.2025.03.14 val PER: 0.3654
2026-01-08 12:57:57,160: t15.2025.03.16 val PER: 0.2448
2026-01-08 12:57:57,160: t15.2025.03.30 val PER: 0.3448
2026-01-08 12:57:57,160: t15.2025.04.13 val PER: 0.2767
2026-01-08 12:58:14,929: Train batch 14200: loss: 14.43 grad norm: 58.53 time: 0.061
2026-01-08 12:58:32,450: Train batch 14400: loss: 11.25 grad norm: 60.81 time: 0.065
2026-01-08 12:58:41,427: Running test after training batch: 14500
2026-01-08 12:58:41,528: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:58:46,261: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the tickled hatz this appointees his swells
2026-01-08 12:58:46,287: WER debug example
  GT : how does it keep the cost down
  PR : turnham dease dusts hitz keep kurtz thus costs
2026-01-08 12:58:47,670: Val batch 14500: PER (avg): 0.2114 CTC Loss (avg): 22.8990 WER(1gram): 99.75% (n=64) time: 6.242
2026-01-08 12:58:47,670: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-08 12:58:47,670: t15.2023.08.13 val PER: 0.2089
2026-01-08 12:58:47,670: t15.2023.08.18 val PER: 0.1970
2026-01-08 12:58:47,670: t15.2023.08.20 val PER: 0.1811
2026-01-08 12:58:47,671: t15.2023.08.25 val PER: 0.1837
2026-01-08 12:58:47,671: t15.2023.08.27 val PER: 0.2653
2026-01-08 12:58:47,671: t15.2023.09.01 val PER: 0.1518
2026-01-08 12:58:47,671: t15.2023.09.03 val PER: 0.2447
2026-01-08 12:58:47,671: t15.2023.09.24 val PER: 0.1820
2026-01-08 12:58:47,671: t15.2023.09.29 val PER: 0.1966
2026-01-08 12:58:47,671: t15.2023.10.01 val PER: 0.2259
2026-01-08 12:58:47,671: t15.2023.10.06 val PER: 0.1668
2026-01-08 12:58:47,671: t15.2023.10.08 val PER: 0.3045
2026-01-08 12:58:47,671: t15.2023.10.13 val PER: 0.2770
2026-01-08 12:58:47,672: t15.2023.10.15 val PER: 0.2195
2026-01-08 12:58:47,672: t15.2023.10.20 val PER: 0.2215
2026-01-08 12:58:47,672: t15.2023.10.22 val PER: 0.1548
2026-01-08 12:58:47,672: t15.2023.11.03 val PER: 0.2408
2026-01-08 12:58:47,672: t15.2023.11.04 val PER: 0.0819
2026-01-08 12:58:47,672: t15.2023.11.17 val PER: 0.0964
2026-01-08 12:58:47,672: t15.2023.11.19 val PER: 0.1098
2026-01-08 12:58:47,672: t15.2023.11.26 val PER: 0.1899
2026-01-08 12:58:47,672: t15.2023.12.03 val PER: 0.1954
2026-01-08 12:58:47,672: t15.2023.12.08 val PER: 0.1664
2026-01-08 12:58:47,672: t15.2023.12.10 val PER: 0.1472
2026-01-08 12:58:47,672: t15.2023.12.17 val PER: 0.1580
2026-01-08 12:58:47,673: t15.2023.12.29 val PER: 0.2080
2026-01-08 12:58:47,673: t15.2024.02.25 val PER: 0.1643
2026-01-08 12:58:47,673: t15.2024.03.08 val PER: 0.2632
2026-01-08 12:58:47,673: t15.2024.03.15 val PER: 0.2264
2026-01-08 12:58:47,673: t15.2024.03.17 val PER: 0.2043
2026-01-08 12:58:47,673: t15.2024.05.10 val PER: 0.2288
2026-01-08 12:58:47,673: t15.2024.06.14 val PER: 0.2145
2026-01-08 12:58:47,673: t15.2024.07.19 val PER: 0.2610
2026-01-08 12:58:47,673: t15.2024.07.21 val PER: 0.1352
2026-01-08 12:58:47,673: t15.2024.07.28 val PER: 0.1897
2026-01-08 12:58:47,673: t15.2025.01.10 val PER: 0.3361
2026-01-08 12:58:47,673: t15.2025.01.12 val PER: 0.2394
2026-01-08 12:58:47,674: t15.2025.03.14 val PER: 0.3669
2026-01-08 12:58:47,674: t15.2025.03.16 val PER: 0.2408
2026-01-08 12:58:47,674: t15.2025.03.30 val PER: 0.3322
2026-01-08 12:58:47,674: t15.2025.04.13 val PER: 0.2796
2026-01-08 12:58:56,591: Train batch 14600: loss: 18.63 grad norm: 74.17 time: 0.060
2026-01-08 12:59:14,321: Train batch 14800: loss: 12.18 grad norm: 52.54 time: 0.052
2026-01-08 12:59:31,460: Train batch 15000: loss: 13.56 grad norm: 54.56 time: 0.054
2026-01-08 12:59:31,461: Running test after training batch: 15000
2026-01-08 12:59:31,570: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:59:36,366: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the tickled hatz this appoint has swells
2026-01-08 12:59:36,392: WER debug example
  GT : how does it keep the cost down
  PR : outhouse dusts hitz keep coots thus cost
2026-01-08 12:59:37,832: Val batch 15000: PER (avg): 0.2093 CTC Loss (avg): 22.6765 WER(1gram): 95.43% (n=64) time: 6.371
2026-01-08 12:59:37,833: WER lens: avg_true_words=6.16 avg_pred_words=5.88 max_pred_words=11
2026-01-08 12:59:37,833: t15.2023.08.13 val PER: 0.1965
2026-01-08 12:59:37,833: t15.2023.08.18 val PER: 0.1936
2026-01-08 12:59:37,833: t15.2023.08.20 val PER: 0.1755
2026-01-08 12:59:37,833: t15.2023.08.25 val PER: 0.1852
2026-01-08 12:59:37,833: t15.2023.08.27 val PER: 0.2669
2026-01-08 12:59:37,833: t15.2023.09.01 val PER: 0.1461
2026-01-08 12:59:37,833: t15.2023.09.03 val PER: 0.2482
2026-01-08 12:59:37,833: t15.2023.09.24 val PER: 0.1820
2026-01-08 12:59:37,833: t15.2023.09.29 val PER: 0.1972
2026-01-08 12:59:37,833: t15.2023.10.01 val PER: 0.2246
2026-01-08 12:59:37,834: t15.2023.10.06 val PER: 0.1593
2026-01-08 12:59:37,834: t15.2023.10.08 val PER: 0.2855
2026-01-08 12:59:37,834: t15.2023.10.13 val PER: 0.2731
2026-01-08 12:59:37,834: t15.2023.10.15 val PER: 0.2149
2026-01-08 12:59:37,834: t15.2023.10.20 val PER: 0.2349
2026-01-08 12:59:37,834: t15.2023.10.22 val PER: 0.1526
2026-01-08 12:59:37,834: t15.2023.11.03 val PER: 0.2408
2026-01-08 12:59:37,835: t15.2023.11.04 val PER: 0.0853
2026-01-08 12:59:37,835: t15.2023.11.17 val PER: 0.1058
2026-01-08 12:59:37,835: t15.2023.11.19 val PER: 0.1178
2026-01-08 12:59:37,835: t15.2023.11.26 val PER: 0.1920
2026-01-08 12:59:37,835: t15.2023.12.03 val PER: 0.1817
2026-01-08 12:59:37,835: t15.2023.12.08 val PER: 0.1671
2026-01-08 12:59:37,835: t15.2023.12.10 val PER: 0.1498
2026-01-08 12:59:37,835: t15.2023.12.17 val PER: 0.1611
2026-01-08 12:59:37,835: t15.2023.12.29 val PER: 0.2045
2026-01-08 12:59:37,835: t15.2024.02.25 val PER: 0.1587
2026-01-08 12:59:37,835: t15.2024.03.08 val PER: 0.2575
2026-01-08 12:59:37,835: t15.2024.03.15 val PER: 0.2245
2026-01-08 12:59:37,835: t15.2024.03.17 val PER: 0.1925
2026-01-08 12:59:37,835: t15.2024.05.10 val PER: 0.2273
2026-01-08 12:59:37,835: t15.2024.06.14 val PER: 0.2177
2026-01-08 12:59:37,835: t15.2024.07.19 val PER: 0.2643
2026-01-08 12:59:37,836: t15.2024.07.21 val PER: 0.1345
2026-01-08 12:59:37,836: t15.2024.07.28 val PER: 0.1890
2026-01-08 12:59:37,836: t15.2025.01.10 val PER: 0.3251
2026-01-08 12:59:37,836: t15.2025.01.12 val PER: 0.2279
2026-01-08 12:59:37,836: t15.2025.03.14 val PER: 0.3639
2026-01-08 12:59:37,836: t15.2025.03.16 val PER: 0.2500
2026-01-08 12:59:37,836: t15.2025.03.30 val PER: 0.3391
2026-01-08 12:59:37,836: t15.2025.04.13 val PER: 0.2867
2026-01-08 12:59:37,837: New best val WER(1gram) 96.19% --> 95.43%
2026-01-08 12:59:37,837: Checkpointing model
2026-01-08 12:59:37,984: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_2blocks_ln/checkpoint/best_checkpoint
2026-01-08 12:59:55,820: Train batch 15200: loss: 10.99 grad norm: 48.07 time: 0.059
2026-01-08 13:00:13,451: Train batch 15400: loss: 15.91 grad norm: 65.15 time: 0.050
2026-01-08 13:00:22,238: Running test after training batch: 15500
2026-01-08 13:00:22,337: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:00:27,084: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the stuccoed hatz this appointees tis swells
2026-01-08 13:00:27,111: WER debug example
  GT : how does it keep the cost down
  PR : outhouse dusts hitz keep kurtz thus cost
2026-01-08 13:00:28,516: Val batch 15500: PER (avg): 0.2093 CTC Loss (avg): 22.4506 WER(1gram): 99.75% (n=64) time: 6.278
2026-01-08 13:00:28,517: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-08 13:00:28,517: t15.2023.08.13 val PER: 0.1985
2026-01-08 13:00:28,517: t15.2023.08.18 val PER: 0.1936
2026-01-08 13:00:28,517: t15.2023.08.20 val PER: 0.1795
2026-01-08 13:00:28,517: t15.2023.08.25 val PER: 0.1852
2026-01-08 13:00:28,517: t15.2023.08.27 val PER: 0.2669
2026-01-08 13:00:28,517: t15.2023.09.01 val PER: 0.1477
2026-01-08 13:00:28,517: t15.2023.09.03 val PER: 0.2447
2026-01-08 13:00:28,518: t15.2023.09.24 val PER: 0.1857
2026-01-08 13:00:28,518: t15.2023.09.29 val PER: 0.1946
2026-01-08 13:00:28,518: t15.2023.10.01 val PER: 0.2246
2026-01-08 13:00:28,518: t15.2023.10.06 val PER: 0.1636
2026-01-08 13:00:28,518: t15.2023.10.08 val PER: 0.3004
2026-01-08 13:00:28,519: t15.2023.10.13 val PER: 0.2684
2026-01-08 13:00:28,519: t15.2023.10.15 val PER: 0.2169
2026-01-08 13:00:28,519: t15.2023.10.20 val PER: 0.2416
2026-01-08 13:00:28,519: t15.2023.10.22 val PER: 0.1459
2026-01-08 13:00:28,519: t15.2023.11.03 val PER: 0.2449
2026-01-08 13:00:28,519: t15.2023.11.04 val PER: 0.0887
2026-01-08 13:00:28,519: t15.2023.11.17 val PER: 0.1058
2026-01-08 13:00:28,519: t15.2023.11.19 val PER: 0.1118
2026-01-08 13:00:28,519: t15.2023.11.26 val PER: 0.1913
2026-01-08 13:00:28,520: t15.2023.12.03 val PER: 0.1786
2026-01-08 13:00:28,520: t15.2023.12.08 val PER: 0.1644
2026-01-08 13:00:28,520: t15.2023.12.10 val PER: 0.1472
2026-01-08 13:00:28,520: t15.2023.12.17 val PER: 0.1486
2026-01-08 13:00:28,520: t15.2023.12.29 val PER: 0.2011
2026-01-08 13:00:28,520: t15.2024.02.25 val PER: 0.1629
2026-01-08 13:00:28,520: t15.2024.03.08 val PER: 0.2560
2026-01-08 13:00:28,520: t15.2024.03.15 val PER: 0.2283
2026-01-08 13:00:28,520: t15.2024.03.17 val PER: 0.1939
2026-01-08 13:00:28,521: t15.2024.05.10 val PER: 0.2169
2026-01-08 13:00:28,521: t15.2024.06.14 val PER: 0.2114
2026-01-08 13:00:28,521: t15.2024.07.19 val PER: 0.2610
2026-01-08 13:00:28,521: t15.2024.07.21 val PER: 0.1421
2026-01-08 13:00:28,521: t15.2024.07.28 val PER: 0.1868
2026-01-08 13:00:28,521: t15.2025.01.10 val PER: 0.3388
2026-01-08 13:00:28,521: t15.2025.01.12 val PER: 0.2309
2026-01-08 13:00:28,521: t15.2025.03.14 val PER: 0.3683
2026-01-08 13:00:28,522: t15.2025.03.16 val PER: 0.2513
2026-01-08 13:00:28,522: t15.2025.03.30 val PER: 0.3368
2026-01-08 13:00:28,522: t15.2025.04.13 val PER: 0.2767
2026-01-08 13:00:37,303: Train batch 15600: loss: 19.60 grad norm: 72.63 time: 0.064
2026-01-08 13:00:55,444: Train batch 15800: loss: 19.89 grad norm: 67.79 time: 0.069
2026-01-08 13:01:14,087: Train batch 16000: loss: 14.56 grad norm: 65.41 time: 0.057
2026-01-08 13:01:14,088: Running test after training batch: 16000
2026-01-08 13:01:14,225: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:01:18,922: WER debug example
  GT : you can see the code at this point as well
  PR : euclea candies sikhs the stuccoed hatz this appointees his swells
2026-01-08 13:01:18,948: WER debug example
  GT : how does it keep the cost down
  PR : outhouse dusts hitz keep kerce thus cost
2026-01-08 13:01:20,360: Val batch 16000: PER (avg): 0.2083 CTC Loss (avg): 22.4888 WER(1gram): 96.45% (n=64) time: 6.272
2026-01-08 13:01:20,360: WER lens: avg_true_words=6.16 avg_pred_words=5.81 max_pred_words=11
2026-01-08 13:01:20,360: t15.2023.08.13 val PER: 0.1996
2026-01-08 13:01:20,360: t15.2023.08.18 val PER: 0.1903
2026-01-08 13:01:20,361: t15.2023.08.20 val PER: 0.1763
2026-01-08 13:01:20,361: t15.2023.08.25 val PER: 0.1822
2026-01-08 13:01:20,361: t15.2023.08.27 val PER: 0.2637
2026-01-08 13:01:20,361: t15.2023.09.01 val PER: 0.1494
2026-01-08 13:01:20,361: t15.2023.09.03 val PER: 0.2280
2026-01-08 13:01:20,361: t15.2023.09.24 val PER: 0.1808
2026-01-08 13:01:20,361: t15.2023.09.29 val PER: 0.1953
2026-01-08 13:01:20,361: t15.2023.10.01 val PER: 0.2259
2026-01-08 13:01:20,361: t15.2023.10.06 val PER: 0.1776
2026-01-08 13:01:20,361: t15.2023.10.08 val PER: 0.2896
2026-01-08 13:01:20,361: t15.2023.10.13 val PER: 0.2739
2026-01-08 13:01:20,361: t15.2023.10.15 val PER: 0.2123
2026-01-08 13:01:20,362: t15.2023.10.20 val PER: 0.2181
2026-01-08 13:01:20,362: t15.2023.10.22 val PER: 0.1503
2026-01-08 13:01:20,362: t15.2023.11.03 val PER: 0.2436
2026-01-08 13:01:20,362: t15.2023.11.04 val PER: 0.0785
2026-01-08 13:01:20,362: t15.2023.11.17 val PER: 0.0980
2026-01-08 13:01:20,362: t15.2023.11.19 val PER: 0.1198
2026-01-08 13:01:20,362: t15.2023.11.26 val PER: 0.1870
2026-01-08 13:01:20,362: t15.2023.12.03 val PER: 0.1943
2026-01-08 13:01:20,362: t15.2023.12.08 val PER: 0.1651
2026-01-08 13:01:20,362: t15.2023.12.10 val PER: 0.1459
2026-01-08 13:01:20,362: t15.2023.12.17 val PER: 0.1559
2026-01-08 13:01:20,362: t15.2023.12.29 val PER: 0.2045
2026-01-08 13:01:20,363: t15.2024.02.25 val PER: 0.1699
2026-01-08 13:01:20,363: t15.2024.03.08 val PER: 0.2589
2026-01-08 13:01:20,363: t15.2024.03.15 val PER: 0.2208
2026-01-08 13:01:20,363: t15.2024.03.17 val PER: 0.1883
2026-01-08 13:01:20,363: t15.2024.05.10 val PER: 0.2273
2026-01-08 13:01:20,363: t15.2024.06.14 val PER: 0.2145
2026-01-08 13:01:20,363: t15.2024.07.19 val PER: 0.2624
2026-01-08 13:01:20,363: t15.2024.07.21 val PER: 0.1338
2026-01-08 13:01:20,363: t15.2024.07.28 val PER: 0.1868
2026-01-08 13:01:20,363: t15.2025.01.10 val PER: 0.3223
2026-01-08 13:01:20,363: t15.2025.01.12 val PER: 0.2379
2026-01-08 13:01:20,363: t15.2025.03.14 val PER: 0.3595
2026-01-08 13:01:20,363: t15.2025.03.16 val PER: 0.2408
2026-01-08 13:01:20,363: t15.2025.03.30 val PER: 0.3310
2026-01-08 13:01:20,363: t15.2025.04.13 val PER: 0.2753
2026-01-08 13:01:37,685: Train batch 16200: loss: 11.32 grad norm: 54.35 time: 0.057
2026-01-08 13:01:55,220: Train batch 16400: loss: 15.04 grad norm: 64.49 time: 0.060
2026-01-08 13:02:04,098: Running test after training batch: 16500
2026-01-08 13:02:04,215: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:02:09,184: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the stuccoed hatz this appointees has swells
2026-01-08 13:02:09,211: WER debug example
  GT : how does it keep the cost down
  PR : ter holmes stacy hitz keep kerce thus cost
2026-01-08 13:02:10,659: Val batch 16500: PER (avg): 0.2059 CTC Loss (avg): 22.3452 WER(1gram): 98.48% (n=64) time: 6.561
2026-01-08 13:02:10,660: WER lens: avg_true_words=6.16 avg_pred_words=5.95 max_pred_words=11
2026-01-08 13:02:10,660: t15.2023.08.13 val PER: 0.2006
2026-01-08 13:02:10,660: t15.2023.08.18 val PER: 0.1928
2026-01-08 13:02:10,660: t15.2023.08.20 val PER: 0.1811
2026-01-08 13:02:10,660: t15.2023.08.25 val PER: 0.1837
2026-01-08 13:02:10,660: t15.2023.08.27 val PER: 0.2556
2026-01-08 13:02:10,660: t15.2023.09.01 val PER: 0.1412
2026-01-08 13:02:10,660: t15.2023.09.03 val PER: 0.2316
2026-01-08 13:02:10,660: t15.2023.09.24 val PER: 0.1772
2026-01-08 13:02:10,660: t15.2023.09.29 val PER: 0.1940
2026-01-08 13:02:10,661: t15.2023.10.01 val PER: 0.2206
2026-01-08 13:02:10,661: t15.2023.10.06 val PER: 0.1604
2026-01-08 13:02:10,661: t15.2023.10.08 val PER: 0.2882
2026-01-08 13:02:10,661: t15.2023.10.13 val PER: 0.2715
2026-01-08 13:02:10,661: t15.2023.10.15 val PER: 0.2162
2026-01-08 13:02:10,661: t15.2023.10.20 val PER: 0.2282
2026-01-08 13:02:10,661: t15.2023.10.22 val PER: 0.1559
2026-01-08 13:02:10,661: t15.2023.11.03 val PER: 0.2415
2026-01-08 13:02:10,661: t15.2023.11.04 val PER: 0.0887
2026-01-08 13:02:10,661: t15.2023.11.17 val PER: 0.1011
2026-01-08 13:02:10,661: t15.2023.11.19 val PER: 0.1098
2026-01-08 13:02:10,661: t15.2023.11.26 val PER: 0.1855
2026-01-08 13:02:10,662: t15.2023.12.03 val PER: 0.1838
2026-01-08 13:02:10,662: t15.2023.12.08 val PER: 0.1591
2026-01-08 13:02:10,662: t15.2023.12.10 val PER: 0.1485
2026-01-08 13:02:10,662: t15.2023.12.17 val PER: 0.1497
2026-01-08 13:02:10,662: t15.2023.12.29 val PER: 0.1963
2026-01-08 13:02:10,662: t15.2024.02.25 val PER: 0.1643
2026-01-08 13:02:10,662: t15.2024.03.08 val PER: 0.2575
2026-01-08 13:02:10,662: t15.2024.03.15 val PER: 0.2220
2026-01-08 13:02:10,662: t15.2024.03.17 val PER: 0.1897
2026-01-08 13:02:10,662: t15.2024.05.10 val PER: 0.2199
2026-01-08 13:02:10,662: t15.2024.06.14 val PER: 0.2129
2026-01-08 13:02:10,662: t15.2024.07.19 val PER: 0.2538
2026-01-08 13:02:10,662: t15.2024.07.21 val PER: 0.1290
2026-01-08 13:02:10,662: t15.2024.07.28 val PER: 0.1787
2026-01-08 13:02:10,662: t15.2025.01.10 val PER: 0.3333
2026-01-08 13:02:10,662: t15.2025.01.12 val PER: 0.2286
2026-01-08 13:02:10,663: t15.2025.03.14 val PER: 0.3550
2026-01-08 13:02:10,663: t15.2025.03.16 val PER: 0.2461
2026-01-08 13:02:10,663: t15.2025.03.30 val PER: 0.3322
2026-01-08 13:02:10,663: t15.2025.04.13 val PER: 0.2796
2026-01-08 13:02:19,495: Train batch 16600: loss: 15.96 grad norm: 55.55 time: 0.054
2026-01-08 13:02:36,874: Train batch 16800: loss: 21.30 grad norm: 81.14 time: 0.063
2026-01-08 13:02:54,402: Train batch 17000: loss: 13.98 grad norm: 62.93 time: 0.085
2026-01-08 13:02:54,402: Running test after training batch: 17000
2026-01-08 13:02:54,500: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:02:59,169: WER debug example
  GT : you can see the code at this point as well
  PR : euclea candies sikhs the stuccoed hatz this appoint has swells
2026-01-08 13:02:59,196: WER debug example
  GT : how does it keep the cost down
  PR : outhouse dusts hitz keep coots thus cost
2026-01-08 13:03:00,624: Val batch 17000: PER (avg): 0.2064 CTC Loss (avg): 22.3304 WER(1gram): 97.21% (n=64) time: 6.222
2026-01-08 13:03:00,624: WER lens: avg_true_words=6.16 avg_pred_words=5.84 max_pred_words=11
2026-01-08 13:03:00,624: t15.2023.08.13 val PER: 0.2006
2026-01-08 13:03:00,624: t15.2023.08.18 val PER: 0.1886
2026-01-08 13:03:00,624: t15.2023.08.20 val PER: 0.1795
2026-01-08 13:03:00,625: t15.2023.08.25 val PER: 0.1807
2026-01-08 13:03:00,625: t15.2023.08.27 val PER: 0.2476
2026-01-08 13:03:00,625: t15.2023.09.01 val PER: 0.1494
2026-01-08 13:03:00,625: t15.2023.09.03 val PER: 0.2328
2026-01-08 13:03:00,625: t15.2023.09.24 val PER: 0.1784
2026-01-08 13:03:00,625: t15.2023.09.29 val PER: 0.1927
2026-01-08 13:03:00,625: t15.2023.10.01 val PER: 0.2259
2026-01-08 13:03:00,625: t15.2023.10.06 val PER: 0.1593
2026-01-08 13:03:00,625: t15.2023.10.08 val PER: 0.2950
2026-01-08 13:03:00,625: t15.2023.10.13 val PER: 0.2762
2026-01-08 13:03:00,625: t15.2023.10.15 val PER: 0.2156
2026-01-08 13:03:00,626: t15.2023.10.20 val PER: 0.2315
2026-01-08 13:03:00,626: t15.2023.10.22 val PER: 0.1437
2026-01-08 13:03:00,626: t15.2023.11.03 val PER: 0.2415
2026-01-08 13:03:00,626: t15.2023.11.04 val PER: 0.0785
2026-01-08 13:03:00,626: t15.2023.11.17 val PER: 0.1011
2026-01-08 13:03:00,626: t15.2023.11.19 val PER: 0.1178
2026-01-08 13:03:00,626: t15.2023.11.26 val PER: 0.1862
2026-01-08 13:03:00,626: t15.2023.12.03 val PER: 0.1891
2026-01-08 13:03:00,626: t15.2023.12.08 val PER: 0.1551
2026-01-08 13:03:00,626: t15.2023.12.10 val PER: 0.1459
2026-01-08 13:03:00,626: t15.2023.12.17 val PER: 0.1476
2026-01-08 13:03:00,626: t15.2023.12.29 val PER: 0.1997
2026-01-08 13:03:00,626: t15.2024.02.25 val PER: 0.1545
2026-01-08 13:03:00,626: t15.2024.03.08 val PER: 0.2575
2026-01-08 13:03:00,626: t15.2024.03.15 val PER: 0.2208
2026-01-08 13:03:00,627: t15.2024.03.17 val PER: 0.1904
2026-01-08 13:03:00,627: t15.2024.05.10 val PER: 0.2244
2026-01-08 13:03:00,627: t15.2024.06.14 val PER: 0.2129
2026-01-08 13:03:00,627: t15.2024.07.19 val PER: 0.2577
2026-01-08 13:03:00,627: t15.2024.07.21 val PER: 0.1352
2026-01-08 13:03:00,627: t15.2024.07.28 val PER: 0.1809
2026-01-08 13:03:00,627: t15.2025.01.10 val PER: 0.3333
2026-01-08 13:03:00,627: t15.2025.01.12 val PER: 0.2325
2026-01-08 13:03:00,627: t15.2025.03.14 val PER: 0.3639
2026-01-08 13:03:00,627: t15.2025.03.16 val PER: 0.2382
2026-01-08 13:03:00,627: t15.2025.03.30 val PER: 0.3299
2026-01-08 13:03:00,627: t15.2025.04.13 val PER: 0.2796
2026-01-08 13:03:18,095: Train batch 17200: loss: 16.25 grad norm: 61.77 time: 0.086
2026-01-08 13:03:35,753: Train batch 17400: loss: 17.41 grad norm: 66.21 time: 0.074
2026-01-08 13:03:44,459: Running test after training batch: 17500
2026-01-08 13:03:44,560: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:03:49,322: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the stuccoed hatz this appointees has swells
2026-01-08 13:03:49,360: WER debug example
  GT : how does it keep the cost down
  PR : outhouse dusty hitz keep kerce thus costs detzel
2026-01-08 13:03:50,991: Val batch 17500: PER (avg): 0.2053 CTC Loss (avg): 22.1757 WER(1gram): 96.95% (n=64) time: 6.532
2026-01-08 13:03:50,992: WER lens: avg_true_words=6.16 avg_pred_words=5.94 max_pred_words=11
2026-01-08 13:03:50,992: t15.2023.08.13 val PER: 0.1913
2026-01-08 13:03:50,992: t15.2023.08.18 val PER: 0.1936
2026-01-08 13:03:50,992: t15.2023.08.20 val PER: 0.1755
2026-01-08 13:03:50,992: t15.2023.08.25 val PER: 0.1747
2026-01-08 13:03:50,992: t15.2023.08.27 val PER: 0.2556
2026-01-08 13:03:50,992: t15.2023.09.01 val PER: 0.1469
2026-01-08 13:03:50,992: t15.2023.09.03 val PER: 0.2316
2026-01-08 13:03:50,992: t15.2023.09.24 val PER: 0.1796
2026-01-08 13:03:50,993: t15.2023.09.29 val PER: 0.1902
2026-01-08 13:03:50,993: t15.2023.10.01 val PER: 0.2246
2026-01-08 13:03:50,993: t15.2023.10.06 val PER: 0.1615
2026-01-08 13:03:50,993: t15.2023.10.08 val PER: 0.2882
2026-01-08 13:03:50,993: t15.2023.10.13 val PER: 0.2715
2026-01-08 13:03:50,993: t15.2023.10.15 val PER: 0.2156
2026-01-08 13:03:50,993: t15.2023.10.20 val PER: 0.2349
2026-01-08 13:03:50,993: t15.2023.10.22 val PER: 0.1459
2026-01-08 13:03:50,993: t15.2023.11.03 val PER: 0.2395
2026-01-08 13:03:50,993: t15.2023.11.04 val PER: 0.0785
2026-01-08 13:03:50,993: t15.2023.11.17 val PER: 0.1011
2026-01-08 13:03:50,993: t15.2023.11.19 val PER: 0.1118
2026-01-08 13:03:50,994: t15.2023.11.26 val PER: 0.1841
2026-01-08 13:03:50,994: t15.2023.12.03 val PER: 0.1796
2026-01-08 13:03:50,994: t15.2023.12.08 val PER: 0.1531
2026-01-08 13:03:50,994: t15.2023.12.10 val PER: 0.1419
2026-01-08 13:03:50,994: t15.2023.12.17 val PER: 0.1528
2026-01-08 13:03:50,994: t15.2023.12.29 val PER: 0.2066
2026-01-08 13:03:50,994: t15.2024.02.25 val PER: 0.1559
2026-01-08 13:03:50,994: t15.2024.03.08 val PER: 0.2560
2026-01-08 13:03:50,994: t15.2024.03.15 val PER: 0.2220
2026-01-08 13:03:50,994: t15.2024.03.17 val PER: 0.1911
2026-01-08 13:03:50,994: t15.2024.05.10 val PER: 0.2199
2026-01-08 13:03:50,994: t15.2024.06.14 val PER: 0.2098
2026-01-08 13:03:50,994: t15.2024.07.19 val PER: 0.2531
2026-01-08 13:03:50,994: t15.2024.07.21 val PER: 0.1324
2026-01-08 13:03:50,994: t15.2024.07.28 val PER: 0.1772
2026-01-08 13:03:50,995: t15.2025.01.10 val PER: 0.3361
2026-01-08 13:03:50,995: t15.2025.01.12 val PER: 0.2309
2026-01-08 13:03:50,995: t15.2025.03.14 val PER: 0.3624
2026-01-08 13:03:50,995: t15.2025.03.16 val PER: 0.2448
2026-01-08 13:03:50,995: t15.2025.03.30 val PER: 0.3345
2026-01-08 13:03:50,995: t15.2025.04.13 val PER: 0.2739
2026-01-08 13:03:59,710: Train batch 17600: loss: 17.00 grad norm: 58.48 time: 0.052
2026-01-08 13:04:17,660: Train batch 17800: loss: 12.21 grad norm: 51.50 time: 0.042
2026-01-08 13:04:34,951: Train batch 18000: loss: 13.02 grad norm: 62.20 time: 0.063
2026-01-08 13:04:34,951: Running test after training batch: 18000
2026-01-08 13:04:35,090: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:04:40,190: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the stuccoed hatz this appointees has swells
2026-01-08 13:04:40,230: WER debug example
  GT : how does it keep the cost down
  PR : turnham dease dusts hitz keep kerce thus cost
2026-01-08 13:04:42,032: Val batch 18000: PER (avg): 0.2045 CTC Loss (avg): 22.1180 WER(1gram): 97.72% (n=64) time: 7.080
2026-01-08 13:04:42,032: WER lens: avg_true_words=6.16 avg_pred_words=5.91 max_pred_words=11
2026-01-08 13:04:42,032: t15.2023.08.13 val PER: 0.1985
2026-01-08 13:04:42,032: t15.2023.08.18 val PER: 0.1869
2026-01-08 13:04:42,033: t15.2023.08.20 val PER: 0.1732
2026-01-08 13:04:42,033: t15.2023.08.25 val PER: 0.1792
2026-01-08 13:04:42,033: t15.2023.08.27 val PER: 0.2572
2026-01-08 13:04:42,033: t15.2023.09.01 val PER: 0.1485
2026-01-08 13:04:42,033: t15.2023.09.03 val PER: 0.2245
2026-01-08 13:04:42,033: t15.2023.09.24 val PER: 0.1699
2026-01-08 13:04:42,033: t15.2023.09.29 val PER: 0.1914
2026-01-08 13:04:42,033: t15.2023.10.01 val PER: 0.2232
2026-01-08 13:04:42,034: t15.2023.10.06 val PER: 0.1593
2026-01-08 13:04:42,034: t15.2023.10.08 val PER: 0.2909
2026-01-08 13:04:42,034: t15.2023.10.13 val PER: 0.2708
2026-01-08 13:04:42,034: t15.2023.10.15 val PER: 0.2096
2026-01-08 13:04:42,034: t15.2023.10.20 val PER: 0.2282
2026-01-08 13:04:42,034: t15.2023.10.22 val PER: 0.1437
2026-01-08 13:04:42,034: t15.2023.11.03 val PER: 0.2408
2026-01-08 13:04:42,034: t15.2023.11.04 val PER: 0.0853
2026-01-08 13:04:42,034: t15.2023.11.17 val PER: 0.1011
2026-01-08 13:04:42,035: t15.2023.11.19 val PER: 0.1198
2026-01-08 13:04:42,035: t15.2023.11.26 val PER: 0.1833
2026-01-08 13:04:42,035: t15.2023.12.03 val PER: 0.1828
2026-01-08 13:04:42,035: t15.2023.12.08 val PER: 0.1545
2026-01-08 13:04:42,035: t15.2023.12.10 val PER: 0.1459
2026-01-08 13:04:42,035: t15.2023.12.17 val PER: 0.1393
2026-01-08 13:04:42,035: t15.2023.12.29 val PER: 0.1977
2026-01-08 13:04:42,035: t15.2024.02.25 val PER: 0.1545
2026-01-08 13:04:42,035: t15.2024.03.08 val PER: 0.2560
2026-01-08 13:04:42,036: t15.2024.03.15 val PER: 0.2251
2026-01-08 13:04:42,036: t15.2024.03.17 val PER: 0.1876
2026-01-08 13:04:42,036: t15.2024.05.10 val PER: 0.2214
2026-01-08 13:04:42,036: t15.2024.06.14 val PER: 0.2098
2026-01-08 13:04:42,036: t15.2024.07.19 val PER: 0.2630
2026-01-08 13:04:42,036: t15.2024.07.21 val PER: 0.1310
2026-01-08 13:04:42,036: t15.2024.07.28 val PER: 0.1787
2026-01-08 13:04:42,036: t15.2025.01.10 val PER: 0.3292
2026-01-08 13:04:42,036: t15.2025.01.12 val PER: 0.2271
2026-01-08 13:04:42,036: t15.2025.03.14 val PER: 0.3609
2026-01-08 13:04:42,037: t15.2025.03.16 val PER: 0.2421
2026-01-08 13:04:42,037: t15.2025.03.30 val PER: 0.3333
2026-01-08 13:04:42,037: t15.2025.04.13 val PER: 0.2753
2026-01-08 13:04:59,611: Train batch 18200: loss: 13.74 grad norm: 57.76 time: 0.077
2026-01-08 13:05:16,850: Train batch 18400: loss: 9.32 grad norm: 44.56 time: 0.060
2026-01-08 13:05:25,498: Running test after training batch: 18500
2026-01-08 13:05:25,648: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:05:30,346: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the stuccoed hatz this appointees tis swells
2026-01-08 13:05:30,372: WER debug example
  GT : how does it keep the cost down
  PR : herms stacy hitz keep kerce thus cost
2026-01-08 13:05:31,845: Val batch 18500: PER (avg): 0.2048 CTC Loss (avg): 22.4100 WER(1gram): 96.95% (n=64) time: 6.347
2026-01-08 13:05:31,845: WER lens: avg_true_words=6.16 avg_pred_words=5.94 max_pred_words=11
2026-01-08 13:05:31,846: t15.2023.08.13 val PER: 0.1985
2026-01-08 13:05:31,846: t15.2023.08.18 val PER: 0.1953
2026-01-08 13:05:31,846: t15.2023.08.20 val PER: 0.1716
2026-01-08 13:05:31,846: t15.2023.08.25 val PER: 0.1762
2026-01-08 13:05:31,846: t15.2023.08.27 val PER: 0.2524
2026-01-08 13:05:31,846: t15.2023.09.01 val PER: 0.1485
2026-01-08 13:05:31,846: t15.2023.09.03 val PER: 0.2292
2026-01-08 13:05:31,846: t15.2023.09.24 val PER: 0.1784
2026-01-08 13:05:31,846: t15.2023.09.29 val PER: 0.1914
2026-01-08 13:05:31,847: t15.2023.10.01 val PER: 0.2259
2026-01-08 13:05:31,847: t15.2023.10.06 val PER: 0.1658
2026-01-08 13:05:31,847: t15.2023.10.08 val PER: 0.2869
2026-01-08 13:05:31,847: t15.2023.10.13 val PER: 0.2708
2026-01-08 13:05:31,847: t15.2023.10.15 val PER: 0.2136
2026-01-08 13:05:31,847: t15.2023.10.20 val PER: 0.2315
2026-01-08 13:05:31,847: t15.2023.10.22 val PER: 0.1481
2026-01-08 13:05:31,847: t15.2023.11.03 val PER: 0.2415
2026-01-08 13:05:31,847: t15.2023.11.04 val PER: 0.0785
2026-01-08 13:05:31,847: t15.2023.11.17 val PER: 0.1011
2026-01-08 13:05:31,847: t15.2023.11.19 val PER: 0.1098
2026-01-08 13:05:31,847: t15.2023.11.26 val PER: 0.1819
2026-01-08 13:05:31,847: t15.2023.12.03 val PER: 0.1796
2026-01-08 13:05:31,847: t15.2023.12.08 val PER: 0.1518
2026-01-08 13:05:31,847: t15.2023.12.10 val PER: 0.1445
2026-01-08 13:05:31,848: t15.2023.12.17 val PER: 0.1445
2026-01-08 13:05:31,848: t15.2023.12.29 val PER: 0.1984
2026-01-08 13:05:31,848: t15.2024.02.25 val PER: 0.1545
2026-01-08 13:05:31,848: t15.2024.03.08 val PER: 0.2532
2026-01-08 13:05:31,848: t15.2024.03.15 val PER: 0.2176
2026-01-08 13:05:31,848: t15.2024.03.17 val PER: 0.1953
2026-01-08 13:05:31,848: t15.2024.05.10 val PER: 0.2229
2026-01-08 13:05:31,848: t15.2024.06.14 val PER: 0.2098
2026-01-08 13:05:31,848: t15.2024.07.19 val PER: 0.2544
2026-01-08 13:05:31,848: t15.2024.07.21 val PER: 0.1317
2026-01-08 13:05:31,848: t15.2024.07.28 val PER: 0.1794
2026-01-08 13:05:31,848: t15.2025.01.10 val PER: 0.3333
2026-01-08 13:05:31,848: t15.2025.01.12 val PER: 0.2271
2026-01-08 13:05:31,848: t15.2025.03.14 val PER: 0.3580
2026-01-08 13:05:31,848: t15.2025.03.16 val PER: 0.2474
2026-01-08 13:05:31,849: t15.2025.03.30 val PER: 0.3368
2026-01-08 13:05:31,849: t15.2025.04.13 val PER: 0.2682
2026-01-08 13:05:40,633: Train batch 18600: loss: 16.09 grad norm: 64.55 time: 0.070
2026-01-08 13:05:57,764: Train batch 18800: loss: 15.14 grad norm: 61.19 time: 0.067
2026-01-08 13:06:15,049: Train batch 19000: loss: 13.85 grad norm: 62.20 time: 0.066
2026-01-08 13:06:15,049: Running test after training batch: 19000
2026-01-08 13:06:15,175: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:06:19,874: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the stuccoed hatz this appointees tis swells
2026-01-08 13:06:19,902: WER debug example
  GT : how does it keep the cost down
  PR : herms stacy hitz keep kurtz thus cost
2026-01-08 13:06:21,410: Val batch 19000: PER (avg): 0.2040 CTC Loss (avg): 22.0371 WER(1gram): 96.45% (n=64) time: 6.361
2026-01-08 13:06:21,410: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=11
2026-01-08 13:06:21,410: t15.2023.08.13 val PER: 0.1954
2026-01-08 13:06:21,410: t15.2023.08.18 val PER: 0.1953
2026-01-08 13:06:21,411: t15.2023.08.20 val PER: 0.1732
2026-01-08 13:06:21,411: t15.2023.08.25 val PER: 0.1732
2026-01-08 13:06:21,411: t15.2023.08.27 val PER: 0.2524
2026-01-08 13:06:21,412: t15.2023.09.01 val PER: 0.1453
2026-01-08 13:06:21,412: t15.2023.09.03 val PER: 0.2328
2026-01-08 13:06:21,412: t15.2023.09.24 val PER: 0.1808
2026-01-08 13:06:21,412: t15.2023.09.29 val PER: 0.1883
2026-01-08 13:06:21,412: t15.2023.10.01 val PER: 0.2226
2026-01-08 13:06:21,412: t15.2023.10.06 val PER: 0.1572
2026-01-08 13:06:21,412: t15.2023.10.08 val PER: 0.2855
2026-01-08 13:06:21,413: t15.2023.10.13 val PER: 0.2692
2026-01-08 13:06:21,413: t15.2023.10.15 val PER: 0.2123
2026-01-08 13:06:21,413: t15.2023.10.20 val PER: 0.2215
2026-01-08 13:06:21,413: t15.2023.10.22 val PER: 0.1459
2026-01-08 13:06:21,413: t15.2023.11.03 val PER: 0.2422
2026-01-08 13:06:21,413: t15.2023.11.04 val PER: 0.0751
2026-01-08 13:06:21,413: t15.2023.11.17 val PER: 0.1042
2026-01-08 13:06:21,413: t15.2023.11.19 val PER: 0.1178
2026-01-08 13:06:21,414: t15.2023.11.26 val PER: 0.1848
2026-01-08 13:06:21,414: t15.2023.12.03 val PER: 0.1817
2026-01-08 13:06:21,414: t15.2023.12.08 val PER: 0.1585
2026-01-08 13:06:21,414: t15.2023.12.10 val PER: 0.1472
2026-01-08 13:06:21,414: t15.2023.12.17 val PER: 0.1435
2026-01-08 13:06:21,414: t15.2023.12.29 val PER: 0.1977
2026-01-08 13:06:21,414: t15.2024.02.25 val PER: 0.1503
2026-01-08 13:06:21,414: t15.2024.03.08 val PER: 0.2532
2026-01-08 13:06:21,415: t15.2024.03.15 val PER: 0.2201
2026-01-08 13:06:21,415: t15.2024.03.17 val PER: 0.1897
2026-01-08 13:06:21,415: t15.2024.05.10 val PER: 0.2199
2026-01-08 13:06:21,415: t15.2024.06.14 val PER: 0.2161
2026-01-08 13:06:21,415: t15.2024.07.19 val PER: 0.2558
2026-01-08 13:06:21,415: t15.2024.07.21 val PER: 0.1317
2026-01-08 13:06:21,415: t15.2024.07.28 val PER: 0.1794
2026-01-08 13:06:21,415: t15.2025.01.10 val PER: 0.3209
2026-01-08 13:06:21,416: t15.2025.01.12 val PER: 0.2248
2026-01-08 13:06:21,416: t15.2025.03.14 val PER: 0.3550
2026-01-08 13:06:21,416: t15.2025.03.16 val PER: 0.2408
2026-01-08 13:06:21,416: t15.2025.03.30 val PER: 0.3368
2026-01-08 13:06:21,416: t15.2025.04.13 val PER: 0.2668
2026-01-08 13:06:39,152: Train batch 19200: loss: 11.66 grad norm: 59.93 time: 0.065
2026-01-08 13:06:56,714: Train batch 19400: loss: 12.07 grad norm: 47.54 time: 0.055
2026-01-08 13:07:05,237: Running test after training batch: 19500
2026-01-08 13:07:05,361: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:07:10,352: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the stuccoed hatz this appointees tis swells
2026-01-08 13:07:10,380: WER debug example
  GT : how does it keep the cost down
  PR : herms stacy hitz keep coots thus cost
2026-01-08 13:07:11,900: Val batch 19500: PER (avg): 0.2040 CTC Loss (avg): 22.4124 WER(1gram): 95.69% (n=64) time: 6.662
2026-01-08 13:07:11,900: WER lens: avg_true_words=6.16 avg_pred_words=5.89 max_pred_words=11
2026-01-08 13:07:11,900: t15.2023.08.13 val PER: 0.1996
2026-01-08 13:07:11,900: t15.2023.08.18 val PER: 0.1928
2026-01-08 13:07:11,900: t15.2023.08.20 val PER: 0.1739
2026-01-08 13:07:11,901: t15.2023.08.25 val PER: 0.1747
2026-01-08 13:07:11,901: t15.2023.08.27 val PER: 0.2524
2026-01-08 13:07:11,901: t15.2023.09.01 val PER: 0.1469
2026-01-08 13:07:11,901: t15.2023.09.03 val PER: 0.2316
2026-01-08 13:07:11,901: t15.2023.09.24 val PER: 0.1748
2026-01-08 13:07:11,901: t15.2023.09.29 val PER: 0.1876
2026-01-08 13:07:11,901: t15.2023.10.01 val PER: 0.2206
2026-01-08 13:07:11,901: t15.2023.10.06 val PER: 0.1582
2026-01-08 13:07:11,901: t15.2023.10.08 val PER: 0.2936
2026-01-08 13:07:11,901: t15.2023.10.13 val PER: 0.2715
2026-01-08 13:07:11,901: t15.2023.10.15 val PER: 0.2070
2026-01-08 13:07:11,901: t15.2023.10.20 val PER: 0.2349
2026-01-08 13:07:11,901: t15.2023.10.22 val PER: 0.1503
2026-01-08 13:07:11,901: t15.2023.11.03 val PER: 0.2429
2026-01-08 13:07:11,901: t15.2023.11.04 val PER: 0.0819
2026-01-08 13:07:11,901: t15.2023.11.17 val PER: 0.1058
2026-01-08 13:07:11,902: t15.2023.11.19 val PER: 0.1098
2026-01-08 13:07:11,902: t15.2023.11.26 val PER: 0.1804
2026-01-08 13:07:11,902: t15.2023.12.03 val PER: 0.1807
2026-01-08 13:07:11,902: t15.2023.12.08 val PER: 0.1585
2026-01-08 13:07:11,902: t15.2023.12.10 val PER: 0.1353
2026-01-08 13:07:11,902: t15.2023.12.17 val PER: 0.1445
2026-01-08 13:07:11,902: t15.2023.12.29 val PER: 0.1949
2026-01-08 13:07:11,903: t15.2024.02.25 val PER: 0.1503
2026-01-08 13:07:11,903: t15.2024.03.08 val PER: 0.2617
2026-01-08 13:07:11,903: t15.2024.03.15 val PER: 0.2176
2026-01-08 13:07:11,903: t15.2024.03.17 val PER: 0.1897
2026-01-08 13:07:11,903: t15.2024.05.10 val PER: 0.2273
2026-01-08 13:07:11,903: t15.2024.06.14 val PER: 0.2114
2026-01-08 13:07:11,903: t15.2024.07.19 val PER: 0.2597
2026-01-08 13:07:11,904: t15.2024.07.21 val PER: 0.1310
2026-01-08 13:07:11,904: t15.2024.07.28 val PER: 0.1779
2026-01-08 13:07:11,904: t15.2025.01.10 val PER: 0.3223
2026-01-08 13:07:11,904: t15.2025.01.12 val PER: 0.2325
2026-01-08 13:07:11,904: t15.2025.03.14 val PER: 0.3476
2026-01-08 13:07:11,904: t15.2025.03.16 val PER: 0.2421
2026-01-08 13:07:11,904: t15.2025.03.30 val PER: 0.3310
2026-01-08 13:07:11,904: t15.2025.04.13 val PER: 0.2710
2026-01-08 13:07:20,450: Train batch 19600: loss: 14.94 grad norm: 55.58 time: 0.060
2026-01-08 13:07:38,231: Train batch 19800: loss: 11.76 grad norm: 56.05 time: 0.057
2026-01-08 13:07:57,018: Running test after training batch: 19999
2026-01-08 13:07:57,115: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:08:01,764: WER debug example
  GT : you can see the code at this point as well
  PR : udy candies sikhs the stuccoed hatz this appointees has swells
2026-01-08 13:08:01,793: WER debug example
  GT : how does it keep the cost down
  PR : outhouse dusty hitz keep kerce thus costs detzel
2026-01-08 13:08:03,303: Val batch 19999: PER (avg): 0.2037 CTC Loss (avg): 21.9975 WER(1gram): 96.70% (n=64) time: 6.284
2026-01-08 13:08:03,303: WER lens: avg_true_words=6.16 avg_pred_words=5.91 max_pred_words=11
2026-01-08 13:08:03,304: t15.2023.08.13 val PER: 0.1975
2026-01-08 13:08:03,304: t15.2023.08.18 val PER: 0.1920
2026-01-08 13:08:03,304: t15.2023.08.20 val PER: 0.1763
2026-01-08 13:08:03,304: t15.2023.08.25 val PER: 0.1777
2026-01-08 13:08:03,304: t15.2023.08.27 val PER: 0.2508
2026-01-08 13:08:03,304: t15.2023.09.01 val PER: 0.1485
2026-01-08 13:08:03,304: t15.2023.09.03 val PER: 0.2292
2026-01-08 13:08:03,304: t15.2023.09.24 val PER: 0.1796
2026-01-08 13:08:03,304: t15.2023.09.29 val PER: 0.1914
2026-01-08 13:08:03,304: t15.2023.10.01 val PER: 0.2166
2026-01-08 13:08:03,304: t15.2023.10.06 val PER: 0.1636
2026-01-08 13:08:03,304: t15.2023.10.08 val PER: 0.2882
2026-01-08 13:08:03,305: t15.2023.10.13 val PER: 0.2723
2026-01-08 13:08:03,305: t15.2023.10.15 val PER: 0.2103
2026-01-08 13:08:03,305: t15.2023.10.20 val PER: 0.2383
2026-01-08 13:08:03,305: t15.2023.10.22 val PER: 0.1481
2026-01-08 13:08:03,305: t15.2023.11.03 val PER: 0.2408
2026-01-08 13:08:03,305: t15.2023.11.04 val PER: 0.0751
2026-01-08 13:08:03,305: t15.2023.11.17 val PER: 0.1042
2026-01-08 13:08:03,305: t15.2023.11.19 val PER: 0.1158
2026-01-08 13:08:03,305: t15.2023.11.26 val PER: 0.1783
2026-01-08 13:08:03,305: t15.2023.12.03 val PER: 0.1807
2026-01-08 13:08:03,305: t15.2023.12.08 val PER: 0.1558
2026-01-08 13:08:03,305: t15.2023.12.10 val PER: 0.1445
2026-01-08 13:08:03,305: t15.2023.12.17 val PER: 0.1424
2026-01-08 13:08:03,305: t15.2023.12.29 val PER: 0.1990
2026-01-08 13:08:03,305: t15.2024.02.25 val PER: 0.1489
2026-01-08 13:08:03,305: t15.2024.03.08 val PER: 0.2560
2026-01-08 13:08:03,305: t15.2024.03.15 val PER: 0.2183
2026-01-08 13:08:03,306: t15.2024.03.17 val PER: 0.1918
2026-01-08 13:08:03,306: t15.2024.05.10 val PER: 0.2229
2026-01-08 13:08:03,306: t15.2024.06.14 val PER: 0.2098
2026-01-08 13:08:03,306: t15.2024.07.19 val PER: 0.2512
2026-01-08 13:08:03,306: t15.2024.07.21 val PER: 0.1276
2026-01-08 13:08:03,306: t15.2024.07.28 val PER: 0.1765
2026-01-08 13:08:03,306: t15.2025.01.10 val PER: 0.3196
2026-01-08 13:08:03,306: t15.2025.01.12 val PER: 0.2294
2026-01-08 13:08:03,306: t15.2025.03.14 val PER: 0.3536
2026-01-08 13:08:03,306: t15.2025.03.16 val PER: 0.2408
2026-01-08 13:08:03,306: t15.2025.03.30 val PER: 0.3356
2026-01-08 13:08:03,306: t15.2025.04.13 val PER: 0.2725
2026-01-08 13:08:03,332: Best avg val PER achieved: 0.20929
2026-01-08 13:08:03,332: Total training time: 33.77 minutes

=== RUN head_3blocks_ln.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln
2026-01-08 13:08:08,774: Using device: cuda:0
2026-01-08 13:08:10,323: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-08 13:08:10,346: Using 45 sessions after filtering (from 45).
2026-01-08 13:08:10,796: Using torch.compile (if available)
2026-01-08 13:08:10,797: torch.compile not available (torch<2.0). Skipping.
2026-01-08 13:08:10,797: Initialized RNN decoding model
2026-01-08 13:08:10,797: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
    (1): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
    (2): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-08 13:08:10,797: Model has 46,091,561 parameters
2026-01-08 13:08:10,798: Model has 11,819,520 day-specific parameters | 25.64% of total parameters
2026-01-08 13:08:12,068: Successfully initialized datasets
2026-01-08 13:08:12,069: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-08 13:08:13,099: Train batch 0: loss: 497.34 grad norm: 2935.67 time: 0.193
2026-01-08 13:08:13,100: Running test after training batch: 0
2026-01-08 13:08:13,213: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:08:17,999: WER debug example
  GT : you can see the code at this point as well
  PR : lather
2026-01-08 13:08:18,036: WER debug example
  GT : how does it keep the cost down
  PR : churchill than chill
2026-01-08 13:08:21,503: Val batch 0: PER (avg): 1.1042 CTC Loss (avg): 496.3378 WER(1gram): 100.00% (n=64) time: 8.403
2026-01-08 13:08:21,504: WER lens: avg_true_words=6.16 avg_pred_words=1.83 max_pred_words=4
2026-01-08 13:08:21,504: t15.2023.08.13 val PER: 1.0146
2026-01-08 13:08:21,504: t15.2023.08.18 val PER: 1.0947
2026-01-08 13:08:21,504: t15.2023.08.20 val PER: 1.0651
2026-01-08 13:08:21,504: t15.2023.08.25 val PER: 1.1386
2026-01-08 13:08:21,504: t15.2023.08.27 val PER: 0.9984
2026-01-08 13:08:21,504: t15.2023.09.01 val PER: 1.1120
2026-01-08 13:08:21,504: t15.2023.09.03 val PER: 1.0760
2026-01-08 13:08:21,504: t15.2023.09.24 val PER: 1.1566
2026-01-08 13:08:21,504: t15.2023.09.29 val PER: 1.1315
2026-01-08 13:08:21,504: t15.2023.10.01 val PER: 0.9901
2026-01-08 13:08:21,504: t15.2023.10.06 val PER: 1.1464
2026-01-08 13:08:21,504: t15.2023.10.08 val PER: 0.9526
2026-01-08 13:08:21,505: t15.2023.10.13 val PER: 1.0341
2026-01-08 13:08:21,505: t15.2023.10.15 val PER: 1.1081
2026-01-08 13:08:21,505: t15.2023.10.20 val PER: 1.1208
2026-01-08 13:08:21,505: t15.2023.10.22 val PER: 1.0824
2026-01-08 13:08:21,505: t15.2023.11.03 val PER: 1.1825
2026-01-08 13:08:21,505: t15.2023.11.04 val PER: 1.3174
2026-01-08 13:08:21,505: t15.2023.11.17 val PER: 1.4930
2026-01-08 13:08:21,505: t15.2023.11.19 val PER: 1.3014
2026-01-08 13:08:21,505: t15.2023.11.26 val PER: 1.1333
2026-01-08 13:08:21,506: t15.2023.12.03 val PER: 1.1113
2026-01-08 13:08:21,506: t15.2023.12.08 val PER: 1.1045
2026-01-08 13:08:21,506: t15.2023.12.10 val PER: 1.2470
2026-01-08 13:08:21,506: t15.2023.12.17 val PER: 1.0353
2026-01-08 13:08:21,506: t15.2023.12.29 val PER: 1.1064
2026-01-08 13:08:21,506: t15.2024.02.25 val PER: 1.1601
2026-01-08 13:08:21,506: t15.2024.03.08 val PER: 1.0484
2026-01-08 13:08:21,506: t15.2024.03.15 val PER: 1.0719
2026-01-08 13:08:21,506: t15.2024.03.17 val PER: 1.0739
2026-01-08 13:08:21,506: t15.2024.05.10 val PER: 1.0386
2026-01-08 13:08:21,506: t15.2024.06.14 val PER: 1.1798
2026-01-08 13:08:21,506: t15.2024.07.19 val PER: 0.9479
2026-01-08 13:08:21,506: t15.2024.07.21 val PER: 1.1855
2026-01-08 13:08:21,506: t15.2024.07.28 val PER: 1.2250
2026-01-08 13:08:21,507: t15.2025.01.10 val PER: 0.9160
2026-01-08 13:08:21,507: t15.2025.01.12 val PER: 1.1986
2026-01-08 13:08:21,507: t15.2025.03.14 val PER: 0.9305
2026-01-08 13:08:21,507: t15.2025.03.16 val PER: 1.1767
2026-01-08 13:08:21,507: t15.2025.03.30 val PER: 1.0161
2026-01-08 13:08:21,507: t15.2025.04.13 val PER: 1.1641
2026-01-08 13:08:21,508: New best val WER(1gram) inf% --> 100.00%
2026-01-08 13:08:21,508: Checkpointing model
2026-01-08 13:08:21,646: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:08:39,623: Train batch 200: loss: 79.95 grad norm: 122.96 time: 0.058
2026-01-08 13:08:57,058: Train batch 400: loss: 56.49 grad norm: 107.52 time: 0.068
2026-01-08 13:09:05,890: Running test after training batch: 500
2026-01-08 13:09:06,028: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:09:10,948: WER debug example
  GT : you can see the code at this point as well
  PR : ear uhde e uhde ter de ooohs
2026-01-08 13:09:10,975: WER debug example
  GT : how does it keep the cost down
  PR : ide z de us
2026-01-08 13:09:12,383: Val batch 500: PER (avg): 0.5632 CTC Loss (avg): 55.1259 WER(1gram): 99.24% (n=64) time: 6.492
2026-01-08 13:09:12,383: WER lens: avg_true_words=6.16 avg_pred_words=3.47 max_pred_words=7
2026-01-08 13:09:12,384: t15.2023.08.13 val PER: 0.5114
2026-01-08 13:09:12,384: t15.2023.08.18 val PER: 0.5122
2026-01-08 13:09:12,384: t15.2023.08.20 val PER: 0.5060
2026-01-08 13:09:12,384: t15.2023.08.25 val PER: 0.4639
2026-01-08 13:09:12,384: t15.2023.08.27 val PER: 0.5868
2026-01-08 13:09:12,384: t15.2023.09.01 val PER: 0.4594
2026-01-08 13:09:12,384: t15.2023.09.03 val PER: 0.5570
2026-01-08 13:09:12,384: t15.2023.09.24 val PER: 0.4951
2026-01-08 13:09:12,384: t15.2023.09.29 val PER: 0.5048
2026-01-08 13:09:12,384: t15.2023.10.01 val PER: 0.5522
2026-01-08 13:09:12,384: t15.2023.10.06 val PER: 0.4952
2026-01-08 13:09:12,385: t15.2023.10.08 val PER: 0.5873
2026-01-08 13:09:12,385: t15.2023.10.13 val PER: 0.5881
2026-01-08 13:09:12,385: t15.2023.10.15 val PER: 0.5524
2026-01-08 13:09:12,385: t15.2023.10.20 val PER: 0.5067
2026-01-08 13:09:12,385: t15.2023.10.22 val PER: 0.4944
2026-01-08 13:09:12,385: t15.2023.11.03 val PER: 0.5224
2026-01-08 13:09:12,385: t15.2023.11.04 val PER: 0.3345
2026-01-08 13:09:12,385: t15.2023.11.17 val PER: 0.3935
2026-01-08 13:09:12,385: t15.2023.11.19 val PER: 0.4052
2026-01-08 13:09:12,385: t15.2023.11.26 val PER: 0.5725
2026-01-08 13:09:12,385: t15.2023.12.03 val PER: 0.5389
2026-01-08 13:09:12,385: t15.2023.12.08 val PER: 0.5659
2026-01-08 13:09:12,386: t15.2023.12.10 val PER: 0.5177
2026-01-08 13:09:12,386: t15.2023.12.17 val PER: 0.6247
2026-01-08 13:09:12,386: t15.2023.12.29 val PER: 0.6108
2026-01-08 13:09:12,386: t15.2024.02.25 val PER: 0.5070
2026-01-08 13:09:12,386: t15.2024.03.08 val PER: 0.6558
2026-01-08 13:09:12,386: t15.2024.03.15 val PER: 0.5979
2026-01-08 13:09:12,386: t15.2024.03.17 val PER: 0.5642
2026-01-08 13:09:12,386: t15.2024.05.10 val PER: 0.5765
2026-01-08 13:09:12,387: t15.2024.06.14 val PER: 0.5962
2026-01-08 13:09:12,387: t15.2024.07.19 val PER: 0.7179
2026-01-08 13:09:12,387: t15.2024.07.21 val PER: 0.5324
2026-01-08 13:09:12,387: t15.2024.07.28 val PER: 0.5559
2026-01-08 13:09:12,387: t15.2025.01.10 val PER: 0.7796
2026-01-08 13:09:12,387: t15.2025.01.12 val PER: 0.5774
2026-01-08 13:09:12,387: t15.2025.03.14 val PER: 0.7574
2026-01-08 13:09:12,387: t15.2025.03.16 val PER: 0.6322
2026-01-08 13:09:12,387: t15.2025.03.30 val PER: 0.7690
2026-01-08 13:09:12,388: t15.2025.04.13 val PER: 0.6191
2026-01-08 13:09:12,388: New best val WER(1gram) 100.00% --> 99.24%
2026-01-08 13:09:12,388: Checkpointing model
2026-01-08 13:09:12,527: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:09:21,576: Train batch 600: loss: 49.99 grad norm: 138.72 time: 0.082
2026-01-08 13:09:38,897: Train batch 800: loss: 42.19 grad norm: 105.13 time: 0.061
2026-01-08 13:09:56,254: Train batch 1000: loss: 44.07 grad norm: 97.76 time: 0.071
2026-01-08 13:09:56,255: Running test after training batch: 1000
2026-01-08 13:09:56,370: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:10:01,100: WER debug example
  GT : you can see the code at this point as well
  PR : ou uhde e a uhde at and is
2026-01-08 13:10:01,125: WER debug example
  GT : how does it keep the cost down
  PR : oz ter eke a us utt
2026-01-08 13:10:02,428: Val batch 1000: PER (avg): 0.4252 CTC Loss (avg): 41.9115 WER(1gram): 93.91% (n=64) time: 6.173
2026-01-08 13:10:02,429: WER lens: avg_true_words=6.16 avg_pred_words=5.12 max_pred_words=10
2026-01-08 13:10:02,429: t15.2023.08.13 val PER: 0.3607
2026-01-08 13:10:02,429: t15.2023.08.18 val PER: 0.3579
2026-01-08 13:10:02,429: t15.2023.08.20 val PER: 0.3685
2026-01-08 13:10:02,429: t15.2023.08.25 val PER: 0.3193
2026-01-08 13:10:02,429: t15.2023.08.27 val PER: 0.4389
2026-01-08 13:10:02,429: t15.2023.09.01 val PER: 0.3279
2026-01-08 13:10:02,429: t15.2023.09.03 val PER: 0.4192
2026-01-08 13:10:02,429: t15.2023.09.24 val PER: 0.3629
2026-01-08 13:10:02,429: t15.2023.09.29 val PER: 0.3810
2026-01-08 13:10:02,429: t15.2023.10.01 val PER: 0.4359
2026-01-08 13:10:02,429: t15.2023.10.06 val PER: 0.3412
2026-01-08 13:10:02,430: t15.2023.10.08 val PER: 0.4723
2026-01-08 13:10:02,430: t15.2023.10.13 val PER: 0.4880
2026-01-08 13:10:02,430: t15.2023.10.15 val PER: 0.4034
2026-01-08 13:10:02,430: t15.2023.10.20 val PER: 0.3859
2026-01-08 13:10:02,430: t15.2023.10.22 val PER: 0.3575
2026-01-08 13:10:02,430: t15.2023.11.03 val PER: 0.4050
2026-01-08 13:10:02,430: t15.2023.11.04 val PER: 0.2048
2026-01-08 13:10:02,430: t15.2023.11.17 val PER: 0.2815
2026-01-08 13:10:02,430: t15.2023.11.19 val PER: 0.2275
2026-01-08 13:10:02,430: t15.2023.11.26 val PER: 0.4486
2026-01-08 13:10:02,430: t15.2023.12.03 val PER: 0.3929
2026-01-08 13:10:02,430: t15.2023.12.08 val PER: 0.4214
2026-01-08 13:10:02,431: t15.2023.12.10 val PER: 0.3443
2026-01-08 13:10:02,431: t15.2023.12.17 val PER: 0.4335
2026-01-08 13:10:02,431: t15.2023.12.29 val PER: 0.4296
2026-01-08 13:10:02,431: t15.2024.02.25 val PER: 0.3427
2026-01-08 13:10:02,431: t15.2024.03.08 val PER: 0.5078
2026-01-08 13:10:02,431: t15.2024.03.15 val PER: 0.4634
2026-01-08 13:10:02,431: t15.2024.03.17 val PER: 0.4240
2026-01-08 13:10:02,431: t15.2024.05.10 val PER: 0.4383
2026-01-08 13:10:02,431: t15.2024.06.14 val PER: 0.4464
2026-01-08 13:10:02,431: t15.2024.07.19 val PER: 0.5742
2026-01-08 13:10:02,432: t15.2024.07.21 val PER: 0.3931
2026-01-08 13:10:02,432: t15.2024.07.28 val PER: 0.4221
2026-01-08 13:10:02,432: t15.2025.01.10 val PER: 0.6529
2026-01-08 13:10:02,432: t15.2025.01.12 val PER: 0.4557
2026-01-08 13:10:02,432: t15.2025.03.14 val PER: 0.6331
2026-01-08 13:10:02,432: t15.2025.03.16 val PER: 0.4856
2026-01-08 13:10:02,432: t15.2025.03.30 val PER: 0.6609
2026-01-08 13:10:02,432: t15.2025.04.13 val PER: 0.5064
2026-01-08 13:10:02,433: New best val WER(1gram) 99.24% --> 93.91%
2026-01-08 13:10:02,433: Checkpointing model
2026-01-08 13:10:02,571: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:10:19,766: Train batch 1200: loss: 35.44 grad norm: 91.87 time: 0.072
2026-01-08 13:10:37,126: Train batch 1400: loss: 37.86 grad norm: 95.20 time: 0.064
2026-01-08 13:10:45,892: Running test after training batch: 1500
2026-01-08 13:10:46,053: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:10:50,785: WER debug example
  GT : you can see the code at this point as well
  PR : ou end e a uhde at uhde
2026-01-08 13:10:50,808: WER debug example
  GT : how does it keep the cost down
  PR : oz eat pu a otte
2026-01-08 13:10:52,090: Val batch 1500: PER (avg): 0.3726 CTC Loss (avg): 35.8624 WER(1gram): 95.18% (n=64) time: 6.197
2026-01-08 13:10:52,090: WER lens: avg_true_words=6.16 avg_pred_words=4.33 max_pred_words=9
2026-01-08 13:10:52,090: t15.2023.08.13 val PER: 0.3399
2026-01-08 13:10:52,090: t15.2023.08.18 val PER: 0.3043
2026-01-08 13:10:52,090: t15.2023.08.20 val PER: 0.2979
2026-01-08 13:10:52,090: t15.2023.08.25 val PER: 0.2711
2026-01-08 13:10:52,090: t15.2023.08.27 val PER: 0.3842
2026-01-08 13:10:52,090: t15.2023.09.01 val PER: 0.2768
2026-01-08 13:10:52,091: t15.2023.09.03 val PER: 0.3670
2026-01-08 13:10:52,091: t15.2023.09.24 val PER: 0.3010
2026-01-08 13:10:52,091: t15.2023.09.29 val PER: 0.3306
2026-01-08 13:10:52,091: t15.2023.10.01 val PER: 0.3884
2026-01-08 13:10:52,091: t15.2023.10.06 val PER: 0.3014
2026-01-08 13:10:52,091: t15.2023.10.08 val PER: 0.4195
2026-01-08 13:10:52,091: t15.2023.10.13 val PER: 0.4282
2026-01-08 13:10:52,091: t15.2023.10.15 val PER: 0.3474
2026-01-08 13:10:52,091: t15.2023.10.20 val PER: 0.3423
2026-01-08 13:10:52,091: t15.2023.10.22 val PER: 0.3252
2026-01-08 13:10:52,091: t15.2023.11.03 val PER: 0.3738
2026-01-08 13:10:52,092: t15.2023.11.04 val PER: 0.1399
2026-01-08 13:10:52,092: t15.2023.11.17 val PER: 0.2333
2026-01-08 13:10:52,092: t15.2023.11.19 val PER: 0.1756
2026-01-08 13:10:52,092: t15.2023.11.26 val PER: 0.3978
2026-01-08 13:10:52,092: t15.2023.12.03 val PER: 0.3435
2026-01-08 13:10:52,092: t15.2023.12.08 val PER: 0.3575
2026-01-08 13:10:52,092: t15.2023.12.10 val PER: 0.2930
2026-01-08 13:10:52,092: t15.2023.12.17 val PER: 0.3534
2026-01-08 13:10:52,092: t15.2023.12.29 val PER: 0.3747
2026-01-08 13:10:52,092: t15.2024.02.25 val PER: 0.3090
2026-01-08 13:10:52,092: t15.2024.03.08 val PER: 0.4424
2026-01-08 13:10:52,092: t15.2024.03.15 val PER: 0.4134
2026-01-08 13:10:52,092: t15.2024.03.17 val PER: 0.3689
2026-01-08 13:10:52,093: t15.2024.05.10 val PER: 0.3848
2026-01-08 13:10:52,093: t15.2024.06.14 val PER: 0.3912
2026-01-08 13:10:52,093: t15.2024.07.19 val PER: 0.5214
2026-01-08 13:10:52,093: t15.2024.07.21 val PER: 0.3310
2026-01-08 13:10:52,093: t15.2024.07.28 val PER: 0.3566
2026-01-08 13:10:52,093: t15.2025.01.10 val PER: 0.5964
2026-01-08 13:10:52,093: t15.2025.01.12 val PER: 0.4119
2026-01-08 13:10:52,093: t15.2025.03.14 val PER: 0.5799
2026-01-08 13:10:52,093: t15.2025.03.16 val PER: 0.4136
2026-01-08 13:10:52,093: t15.2025.03.30 val PER: 0.6264
2026-01-08 13:10:52,093: t15.2025.04.13 val PER: 0.4465
2026-01-08 13:11:00,928: Train batch 1600: loss: 36.21 grad norm: 92.10 time: 0.067
2026-01-08 13:11:19,099: Train batch 1800: loss: 36.05 grad norm: 95.70 time: 0.093
2026-01-08 13:11:37,351: Train batch 2000: loss: 32.70 grad norm: 84.03 time: 0.069
2026-01-08 13:11:37,352: Running test after training batch: 2000
2026-01-08 13:11:37,498: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:11:42,214: WER debug example
  GT : you can see the code at this point as well
  PR : ou ed e a owed at this otte eel
2026-01-08 13:11:42,239: WER debug example
  GT : how does it keep the cost down
  PR : us it purr a os end
2026-01-08 13:11:43,594: Val batch 2000: PER (avg): 0.3265 CTC Loss (avg): 31.9470 WER(1gram): 89.59% (n=64) time: 6.242
2026-01-08 13:11:43,595: WER lens: avg_true_words=6.16 avg_pred_words=5.12 max_pred_words=11
2026-01-08 13:11:43,595: t15.2023.08.13 val PER: 0.2963
2026-01-08 13:11:43,595: t15.2023.08.18 val PER: 0.2657
2026-01-08 13:11:43,595: t15.2023.08.20 val PER: 0.2558
2026-01-08 13:11:43,595: t15.2023.08.25 val PER: 0.2289
2026-01-08 13:11:43,595: t15.2023.08.27 val PER: 0.3537
2026-01-08 13:11:43,595: t15.2023.09.01 val PER: 0.2248
2026-01-08 13:11:43,595: t15.2023.09.03 val PER: 0.3183
2026-01-08 13:11:43,595: t15.2023.09.24 val PER: 0.2621
2026-01-08 13:11:43,595: t15.2023.09.29 val PER: 0.2770
2026-01-08 13:11:43,595: t15.2023.10.01 val PER: 0.3217
2026-01-08 13:11:43,595: t15.2023.10.06 val PER: 0.2400
2026-01-08 13:11:43,595: t15.2023.10.08 val PER: 0.3667
2026-01-08 13:11:43,596: t15.2023.10.13 val PER: 0.3770
2026-01-08 13:11:43,596: t15.2023.10.15 val PER: 0.3098
2026-01-08 13:11:43,596: t15.2023.10.20 val PER: 0.3054
2026-01-08 13:11:43,596: t15.2023.10.22 val PER: 0.2684
2026-01-08 13:11:43,596: t15.2023.11.03 val PER: 0.3121
2026-01-08 13:11:43,596: t15.2023.11.04 val PER: 0.1365
2026-01-08 13:11:43,597: t15.2023.11.17 val PER: 0.1851
2026-01-08 13:11:43,597: t15.2023.11.19 val PER: 0.1337
2026-01-08 13:11:43,597: t15.2023.11.26 val PER: 0.3594
2026-01-08 13:11:43,597: t15.2023.12.03 val PER: 0.2868
2026-01-08 13:11:43,597: t15.2023.12.08 val PER: 0.3063
2026-01-08 13:11:43,597: t15.2023.12.10 val PER: 0.2549
2026-01-08 13:11:43,597: t15.2023.12.17 val PER: 0.3108
2026-01-08 13:11:43,597: t15.2023.12.29 val PER: 0.3123
2026-01-08 13:11:43,598: t15.2024.02.25 val PER: 0.2809
2026-01-08 13:11:43,598: t15.2024.03.08 val PER: 0.4054
2026-01-08 13:11:43,598: t15.2024.03.15 val PER: 0.3402
2026-01-08 13:11:43,598: t15.2024.03.17 val PER: 0.3326
2026-01-08 13:11:43,598: t15.2024.05.10 val PER: 0.3388
2026-01-08 13:11:43,598: t15.2024.06.14 val PER: 0.3565
2026-01-08 13:11:43,598: t15.2024.07.19 val PER: 0.4885
2026-01-08 13:11:43,598: t15.2024.07.21 val PER: 0.2903
2026-01-08 13:11:43,598: t15.2024.07.28 val PER: 0.3140
2026-01-08 13:11:43,599: t15.2025.01.10 val PER: 0.5647
2026-01-08 13:11:43,599: t15.2025.01.12 val PER: 0.3726
2026-01-08 13:11:43,599: t15.2025.03.14 val PER: 0.5251
2026-01-08 13:11:43,599: t15.2025.03.16 val PER: 0.3770
2026-01-08 13:11:43,599: t15.2025.03.30 val PER: 0.6034
2026-01-08 13:11:43,599: t15.2025.04.13 val PER: 0.4009
2026-01-08 13:11:43,600: New best val WER(1gram) 93.91% --> 89.59%
2026-01-08 13:11:43,600: Checkpointing model
2026-01-08 13:11:43,744: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:12:02,924: Train batch 2200: loss: 30.71 grad norm: 85.71 time: 0.064
2026-01-08 13:12:21,801: Train batch 2400: loss: 30.00 grad norm: 80.83 time: 0.055
2026-01-08 13:12:31,494: Running test after training batch: 2500
2026-01-08 13:12:31,642: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:12:36,469: WER debug example
  GT : you can see the code at this point as well
  PR : ou end e a owed at this utt eel
2026-01-08 13:12:36,495: WER debug example
  GT : how does it keep the cost down
  PR : est it keep utt otte ent
2026-01-08 13:12:37,874: Val batch 2500: PER (avg): 0.3035 CTC Loss (avg): 29.6984 WER(1gram): 86.80% (n=64) time: 6.379
2026-01-08 13:12:37,874: WER lens: avg_true_words=6.16 avg_pred_words=5.39 max_pred_words=11
2026-01-08 13:12:37,874: t15.2023.08.13 val PER: 0.2942
2026-01-08 13:12:37,874: t15.2023.08.18 val PER: 0.2632
2026-01-08 13:12:37,874: t15.2023.08.20 val PER: 0.2367
2026-01-08 13:12:37,874: t15.2023.08.25 val PER: 0.2003
2026-01-08 13:12:37,874: t15.2023.08.27 val PER: 0.3071
2026-01-08 13:12:37,875: t15.2023.09.01 val PER: 0.2054
2026-01-08 13:12:37,875: t15.2023.09.03 val PER: 0.2969
2026-01-08 13:12:37,875: t15.2023.09.24 val PER: 0.2330
2026-01-08 13:12:37,875: t15.2023.09.29 val PER: 0.2559
2026-01-08 13:12:37,875: t15.2023.10.01 val PER: 0.2952
2026-01-08 13:12:37,875: t15.2023.10.06 val PER: 0.2196
2026-01-08 13:12:37,875: t15.2023.10.08 val PER: 0.3572
2026-01-08 13:12:37,875: t15.2023.10.13 val PER: 0.3553
2026-01-08 13:12:37,875: t15.2023.10.15 val PER: 0.2788
2026-01-08 13:12:37,875: t15.2023.10.20 val PER: 0.2819
2026-01-08 13:12:37,875: t15.2023.10.22 val PER: 0.2428
2026-01-08 13:12:37,875: t15.2023.11.03 val PER: 0.2795
2026-01-08 13:12:37,875: t15.2023.11.04 val PER: 0.0751
2026-01-08 13:12:37,875: t15.2023.11.17 val PER: 0.1477
2026-01-08 13:12:37,876: t15.2023.11.19 val PER: 0.1198
2026-01-08 13:12:37,876: t15.2023.11.26 val PER: 0.3261
2026-01-08 13:12:37,876: t15.2023.12.03 val PER: 0.2763
2026-01-08 13:12:37,876: t15.2023.12.08 val PER: 0.2856
2026-01-08 13:12:37,876: t15.2023.12.10 val PER: 0.2300
2026-01-08 13:12:37,876: t15.2023.12.17 val PER: 0.2890
2026-01-08 13:12:37,876: t15.2023.12.29 val PER: 0.3047
2026-01-08 13:12:37,876: t15.2024.02.25 val PER: 0.2402
2026-01-08 13:12:37,876: t15.2024.03.08 val PER: 0.3528
2026-01-08 13:12:37,876: t15.2024.03.15 val PER: 0.3402
2026-01-08 13:12:37,876: t15.2024.03.17 val PER: 0.3075
2026-01-08 13:12:37,876: t15.2024.05.10 val PER: 0.2927
2026-01-08 13:12:37,876: t15.2024.06.14 val PER: 0.3312
2026-01-08 13:12:37,876: t15.2024.07.19 val PER: 0.4575
2026-01-08 13:12:37,876: t15.2024.07.21 val PER: 0.2641
2026-01-08 13:12:37,877: t15.2024.07.28 val PER: 0.2985
2026-01-08 13:12:37,877: t15.2025.01.10 val PER: 0.5289
2026-01-08 13:12:37,877: t15.2025.01.12 val PER: 0.3557
2026-01-08 13:12:37,877: t15.2025.03.14 val PER: 0.5015
2026-01-08 13:12:37,877: t15.2025.03.16 val PER: 0.3573
2026-01-08 13:12:37,877: t15.2025.03.30 val PER: 0.5655
2026-01-08 13:12:37,877: t15.2025.04.13 val PER: 0.3966
2026-01-08 13:12:37,878: New best val WER(1gram) 89.59% --> 86.80%
2026-01-08 13:12:37,878: Checkpointing model
2026-01-08 13:12:38,028: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:12:47,180: Train batch 2600: loss: 34.56 grad norm: 97.24 time: 0.057
2026-01-08 13:13:05,359: Train batch 2800: loss: 24.85 grad norm: 77.72 time: 0.086
2026-01-08 13:13:23,567: Train batch 3000: loss: 32.07 grad norm: 92.57 time: 0.087
2026-01-08 13:13:23,568: Running test after training batch: 3000
2026-01-08 13:13:23,667: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:13:28,414: WER debug example
  GT : you can see the code at this point as well
  PR : ou end e a owed at this utt is aisle
2026-01-08 13:13:28,440: WER debug example
  GT : how does it keep the cost down
  PR : aue us it oop a otte ent
2026-01-08 13:13:29,825: Val batch 3000: PER (avg): 0.2818 CTC Loss (avg): 27.2877 WER(1gram): 86.04% (n=64) time: 6.258
2026-01-08 13:13:29,826: WER lens: avg_true_words=6.16 avg_pred_words=5.27 max_pred_words=11
2026-01-08 13:13:29,826: t15.2023.08.13 val PER: 0.2412
2026-01-08 13:13:29,826: t15.2023.08.18 val PER: 0.2347
2026-01-08 13:13:29,826: t15.2023.08.20 val PER: 0.2145
2026-01-08 13:13:29,826: t15.2023.08.25 val PER: 0.1807
2026-01-08 13:13:29,826: t15.2023.08.27 val PER: 0.3151
2026-01-08 13:13:29,826: t15.2023.09.01 val PER: 0.1940
2026-01-08 13:13:29,826: t15.2023.09.03 val PER: 0.2862
2026-01-08 13:13:29,826: t15.2023.09.24 val PER: 0.2015
2026-01-08 13:13:29,826: t15.2023.09.29 val PER: 0.2374
2026-01-08 13:13:29,826: t15.2023.10.01 val PER: 0.2933
2026-01-08 13:13:29,827: t15.2023.10.06 val PER: 0.1959
2026-01-08 13:13:29,827: t15.2023.10.08 val PER: 0.3478
2026-01-08 13:13:29,827: t15.2023.10.13 val PER: 0.3289
2026-01-08 13:13:29,827: t15.2023.10.15 val PER: 0.2624
2026-01-08 13:13:29,827: t15.2023.10.20 val PER: 0.2685
2026-01-08 13:13:29,827: t15.2023.10.22 val PER: 0.2294
2026-01-08 13:13:29,827: t15.2023.11.03 val PER: 0.2666
2026-01-08 13:13:29,827: t15.2023.11.04 val PER: 0.0751
2026-01-08 13:13:29,827: t15.2023.11.17 val PER: 0.1244
2026-01-08 13:13:29,827: t15.2023.11.19 val PER: 0.1058
2026-01-08 13:13:29,828: t15.2023.11.26 val PER: 0.3072
2026-01-08 13:13:29,828: t15.2023.12.03 val PER: 0.2458
2026-01-08 13:13:29,828: t15.2023.12.08 val PER: 0.2603
2026-01-08 13:13:29,828: t15.2023.12.10 val PER: 0.2037
2026-01-08 13:13:29,828: t15.2023.12.17 val PER: 0.2796
2026-01-08 13:13:29,828: t15.2023.12.29 val PER: 0.2745
2026-01-08 13:13:29,828: t15.2024.02.25 val PER: 0.2374
2026-01-08 13:13:29,828: t15.2024.03.08 val PER: 0.3400
2026-01-08 13:13:29,828: t15.2024.03.15 val PER: 0.3177
2026-01-08 13:13:29,828: t15.2024.03.17 val PER: 0.2706
2026-01-08 13:13:29,828: t15.2024.05.10 val PER: 0.2912
2026-01-08 13:13:29,828: t15.2024.06.14 val PER: 0.3060
2026-01-08 13:13:29,829: t15.2024.07.19 val PER: 0.4094
2026-01-08 13:13:29,829: t15.2024.07.21 val PER: 0.2421
2026-01-08 13:13:29,829: t15.2024.07.28 val PER: 0.2809
2026-01-08 13:13:29,829: t15.2025.01.10 val PER: 0.5041
2026-01-08 13:13:29,829: t15.2025.01.12 val PER: 0.3272
2026-01-08 13:13:29,829: t15.2025.03.14 val PER: 0.4689
2026-01-08 13:13:29,829: t15.2025.03.16 val PER: 0.3599
2026-01-08 13:13:29,829: t15.2025.03.30 val PER: 0.5103
2026-01-08 13:13:29,829: t15.2025.04.13 val PER: 0.3723
2026-01-08 13:13:29,830: New best val WER(1gram) 86.80% --> 86.04%
2026-01-08 13:13:29,830: Checkpointing model
2026-01-08 13:13:29,972: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:13:47,602: Train batch 3200: loss: 26.56 grad norm: 81.28 time: 0.080
2026-01-08 13:14:05,379: Train batch 3400: loss: 18.14 grad norm: 71.63 time: 0.052
2026-01-08 13:14:15,124: Running test after training batch: 3500
2026-01-08 13:14:15,257: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:14:19,932: WER debug example
  GT : you can see the code at this point as well
  PR : ou end e a owed at this otte ill
2026-01-08 13:14:19,960: WER debug example
  GT : how does it keep the cost down
  PR : aue us it hipp a otte et
2026-01-08 13:14:21,678: Val batch 3500: PER (avg): 0.2677 CTC Loss (avg): 25.6492 WER(1gram): 81.47% (n=64) time: 6.554
2026-01-08 13:14:21,679: WER lens: avg_true_words=6.16 avg_pred_words=5.28 max_pred_words=10
2026-01-08 13:14:21,679: t15.2023.08.13 val PER: 0.2277
2026-01-08 13:14:21,679: t15.2023.08.18 val PER: 0.2188
2026-01-08 13:14:21,679: t15.2023.08.20 val PER: 0.2065
2026-01-08 13:14:21,679: t15.2023.08.25 val PER: 0.1702
2026-01-08 13:14:21,679: t15.2023.08.27 val PER: 0.2990
2026-01-08 13:14:21,679: t15.2023.09.01 val PER: 0.1705
2026-01-08 13:14:21,679: t15.2023.09.03 val PER: 0.2732
2026-01-08 13:14:21,680: t15.2023.09.24 val PER: 0.2002
2026-01-08 13:14:21,680: t15.2023.09.29 val PER: 0.2259
2026-01-08 13:14:21,680: t15.2023.10.01 val PER: 0.2721
2026-01-08 13:14:21,680: t15.2023.10.06 val PER: 0.1873
2026-01-08 13:14:21,680: t15.2023.10.08 val PER: 0.3491
2026-01-08 13:14:21,680: t15.2023.10.13 val PER: 0.3196
2026-01-08 13:14:21,680: t15.2023.10.15 val PER: 0.2551
2026-01-08 13:14:21,680: t15.2023.10.20 val PER: 0.2651
2026-01-08 13:14:21,680: t15.2023.10.22 val PER: 0.2094
2026-01-08 13:14:21,680: t15.2023.11.03 val PER: 0.2558
2026-01-08 13:14:21,681: t15.2023.11.04 val PER: 0.0580
2026-01-08 13:14:21,681: t15.2023.11.17 val PER: 0.1166
2026-01-08 13:14:21,681: t15.2023.11.19 val PER: 0.1118
2026-01-08 13:14:21,681: t15.2023.11.26 val PER: 0.2819
2026-01-08 13:14:21,681: t15.2023.12.03 val PER: 0.2353
2026-01-08 13:14:21,681: t15.2023.12.08 val PER: 0.2530
2026-01-08 13:14:21,681: t15.2023.12.10 val PER: 0.1879
2026-01-08 13:14:21,681: t15.2023.12.17 val PER: 0.2536
2026-01-08 13:14:21,681: t15.2023.12.29 val PER: 0.2498
2026-01-08 13:14:21,681: t15.2024.02.25 val PER: 0.2107
2026-01-08 13:14:21,681: t15.2024.03.08 val PER: 0.3272
2026-01-08 13:14:21,681: t15.2024.03.15 val PER: 0.3046
2026-01-08 13:14:21,682: t15.2024.03.17 val PER: 0.2775
2026-01-08 13:14:21,682: t15.2024.05.10 val PER: 0.2823
2026-01-08 13:14:21,682: t15.2024.06.14 val PER: 0.2808
2026-01-08 13:14:21,682: t15.2024.07.19 val PER: 0.3968
2026-01-08 13:14:21,682: t15.2024.07.21 val PER: 0.2317
2026-01-08 13:14:21,682: t15.2024.07.28 val PER: 0.2706
2026-01-08 13:14:21,682: t15.2025.01.10 val PER: 0.4490
2026-01-08 13:14:21,682: t15.2025.01.12 val PER: 0.2995
2026-01-08 13:14:21,682: t15.2025.03.14 val PER: 0.4645
2026-01-08 13:14:21,682: t15.2025.03.16 val PER: 0.3390
2026-01-08 13:14:21,682: t15.2025.03.30 val PER: 0.4897
2026-01-08 13:14:21,682: t15.2025.04.13 val PER: 0.3566
2026-01-08 13:14:21,683: New best val WER(1gram) 86.04% --> 81.47%
2026-01-08 13:14:21,683: Checkpointing model
2026-01-08 13:14:21,822: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:14:31,460: Train batch 3600: loss: 23.34 grad norm: 75.48 time: 0.070
2026-01-08 13:14:49,327: Train batch 3800: loss: 24.33 grad norm: 82.97 time: 0.071
2026-01-08 13:15:07,247: Train batch 4000: loss: 19.53 grad norm: 69.87 time: 0.058
2026-01-08 13:15:07,247: Running test after training batch: 4000
2026-01-08 13:15:07,349: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:15:12,096: WER debug example
  GT : you can see the code at this point as well
  PR : ou aunt e a owed at this otte ill
2026-01-08 13:15:12,122: WER debug example
  GT : how does it keep the cost down
  PR : aue us it keep utt otte et
2026-01-08 13:15:13,547: Val batch 4000: PER (avg): 0.2479 CTC Loss (avg): 24.3695 WER(1gram): 78.17% (n=64) time: 6.299
2026-01-08 13:15:13,547: WER lens: avg_true_words=6.16 avg_pred_words=5.39 max_pred_words=10
2026-01-08 13:15:13,547: t15.2023.08.13 val PER: 0.2089
2026-01-08 13:15:13,547: t15.2023.08.18 val PER: 0.2121
2026-01-08 13:15:13,547: t15.2023.08.20 val PER: 0.1898
2026-01-08 13:15:13,547: t15.2023.08.25 val PER: 0.1446
2026-01-08 13:15:13,547: t15.2023.08.27 val PER: 0.2862
2026-01-08 13:15:13,547: t15.2023.09.01 val PER: 0.1591
2026-01-08 13:15:13,548: t15.2023.09.03 val PER: 0.2542
2026-01-08 13:15:13,548: t15.2023.09.24 val PER: 0.1784
2026-01-08 13:15:13,548: t15.2023.09.29 val PER: 0.1978
2026-01-08 13:15:13,548: t15.2023.10.01 val PER: 0.2490
2026-01-08 13:15:13,548: t15.2023.10.06 val PER: 0.1668
2026-01-08 13:15:13,548: t15.2023.10.08 val PER: 0.3342
2026-01-08 13:15:13,548: t15.2023.10.13 val PER: 0.3002
2026-01-08 13:15:13,548: t15.2023.10.15 val PER: 0.2281
2026-01-08 13:15:13,548: t15.2023.10.20 val PER: 0.2349
2026-01-08 13:15:13,548: t15.2023.10.22 val PER: 0.2049
2026-01-08 13:15:13,548: t15.2023.11.03 val PER: 0.2408
2026-01-08 13:15:13,548: t15.2023.11.04 val PER: 0.0546
2026-01-08 13:15:13,548: t15.2023.11.17 val PER: 0.1260
2026-01-08 13:15:13,548: t15.2023.11.19 val PER: 0.0918
2026-01-08 13:15:13,549: t15.2023.11.26 val PER: 0.2623
2026-01-08 13:15:13,549: t15.2023.12.03 val PER: 0.2059
2026-01-08 13:15:13,549: t15.2023.12.08 val PER: 0.2230
2026-01-08 13:15:13,549: t15.2023.12.10 val PER: 0.1813
2026-01-08 13:15:13,549: t15.2023.12.17 val PER: 0.2443
2026-01-08 13:15:13,549: t15.2023.12.29 val PER: 0.2546
2026-01-08 13:15:13,549: t15.2024.02.25 val PER: 0.2051
2026-01-08 13:15:13,549: t15.2024.03.08 val PER: 0.3115
2026-01-08 13:15:13,549: t15.2024.03.15 val PER: 0.2896
2026-01-08 13:15:13,549: t15.2024.03.17 val PER: 0.2538
2026-01-08 13:15:13,549: t15.2024.05.10 val PER: 0.2377
2026-01-08 13:15:13,549: t15.2024.06.14 val PER: 0.2618
2026-01-08 13:15:13,549: t15.2024.07.19 val PER: 0.3672
2026-01-08 13:15:13,549: t15.2024.07.21 val PER: 0.2097
2026-01-08 13:15:13,549: t15.2024.07.28 val PER: 0.2449
2026-01-08 13:15:13,549: t15.2025.01.10 val PER: 0.4339
2026-01-08 13:15:13,550: t15.2025.01.12 val PER: 0.2771
2026-01-08 13:15:13,550: t15.2025.03.14 val PER: 0.4320
2026-01-08 13:15:13,550: t15.2025.03.16 val PER: 0.3050
2026-01-08 13:15:13,550: t15.2025.03.30 val PER: 0.4310
2026-01-08 13:15:13,550: t15.2025.04.13 val PER: 0.3395
2026-01-08 13:15:13,552: New best val WER(1gram) 81.47% --> 78.17%
2026-01-08 13:15:13,552: Checkpointing model
2026-01-08 13:15:13,694: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:15:31,011: Train batch 4200: loss: 22.40 grad norm: 79.56 time: 0.084
2026-01-08 13:15:49,130: Train batch 4400: loss: 17.83 grad norm: 74.46 time: 0.070
2026-01-08 13:15:58,091: Running test after training batch: 4500
2026-01-08 13:15:58,220: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:16:02,926: WER debug example
  GT : you can see the code at this point as well
  PR : ou an e a old at this art is ill
2026-01-08 13:16:02,952: WER debug example
  GT : how does it keep the cost down
  PR : aue us it keep the us ent
2026-01-08 13:16:04,421: Val batch 4500: PER (avg): 0.2348 CTC Loss (avg): 22.9560 WER(1gram): 76.14% (n=64) time: 6.329
2026-01-08 13:16:04,421: WER lens: avg_true_words=6.16 avg_pred_words=5.53 max_pred_words=10
2026-01-08 13:16:04,421: t15.2023.08.13 val PER: 0.2017
2026-01-08 13:16:04,421: t15.2023.08.18 val PER: 0.1886
2026-01-08 13:16:04,421: t15.2023.08.20 val PER: 0.1755
2026-01-08 13:16:04,421: t15.2023.08.25 val PER: 0.1386
2026-01-08 13:16:04,421: t15.2023.08.27 val PER: 0.2685
2026-01-08 13:16:04,421: t15.2023.09.01 val PER: 0.1453
2026-01-08 13:16:04,422: t15.2023.09.03 val PER: 0.2482
2026-01-08 13:16:04,422: t15.2023.09.24 val PER: 0.1784
2026-01-08 13:16:04,422: t15.2023.09.29 val PER: 0.1908
2026-01-08 13:16:04,422: t15.2023.10.01 val PER: 0.2404
2026-01-08 13:16:04,422: t15.2023.10.06 val PER: 0.1410
2026-01-08 13:16:04,422: t15.2023.10.08 val PER: 0.3153
2026-01-08 13:16:04,422: t15.2023.10.13 val PER: 0.2909
2026-01-08 13:16:04,423: t15.2023.10.15 val PER: 0.2367
2026-01-08 13:16:04,423: t15.2023.10.20 val PER: 0.2349
2026-01-08 13:16:04,423: t15.2023.10.22 val PER: 0.1871
2026-01-08 13:16:04,423: t15.2023.11.03 val PER: 0.2449
2026-01-08 13:16:04,423: t15.2023.11.04 val PER: 0.0307
2026-01-08 13:16:04,423: t15.2023.11.17 val PER: 0.0933
2026-01-08 13:16:04,423: t15.2023.11.19 val PER: 0.0878
2026-01-08 13:16:04,423: t15.2023.11.26 val PER: 0.2543
2026-01-08 13:16:04,423: t15.2023.12.03 val PER: 0.2038
2026-01-08 13:16:04,423: t15.2023.12.08 val PER: 0.2064
2026-01-08 13:16:04,423: t15.2023.12.10 val PER: 0.1748
2026-01-08 13:16:04,423: t15.2023.12.17 val PER: 0.2256
2026-01-08 13:16:04,423: t15.2023.12.29 val PER: 0.2279
2026-01-08 13:16:04,423: t15.2024.02.25 val PER: 0.2037
2026-01-08 13:16:04,423: t15.2024.03.08 val PER: 0.3058
2026-01-08 13:16:04,424: t15.2024.03.15 val PER: 0.2764
2026-01-08 13:16:04,424: t15.2024.03.17 val PER: 0.2280
2026-01-08 13:16:04,424: t15.2024.05.10 val PER: 0.2348
2026-01-08 13:16:04,424: t15.2024.06.14 val PER: 0.2445
2026-01-08 13:16:04,424: t15.2024.07.19 val PER: 0.3520
2026-01-08 13:16:04,424: t15.2024.07.21 val PER: 0.1828
2026-01-08 13:16:04,424: t15.2024.07.28 val PER: 0.2338
2026-01-08 13:16:04,424: t15.2025.01.10 val PER: 0.4091
2026-01-08 13:16:04,424: t15.2025.01.12 val PER: 0.2571
2026-01-08 13:16:04,424: t15.2025.03.14 val PER: 0.4068
2026-01-08 13:16:04,424: t15.2025.03.16 val PER: 0.2997
2026-01-08 13:16:04,424: t15.2025.03.30 val PER: 0.4092
2026-01-08 13:16:04,424: t15.2025.04.13 val PER: 0.3224
2026-01-08 13:16:04,425: New best val WER(1gram) 78.17% --> 76.14%
2026-01-08 13:16:04,425: Checkpointing model
2026-01-08 13:16:04,565: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:16:13,146: Train batch 4600: loss: 19.45 grad norm: 73.13 time: 0.066
2026-01-08 13:16:31,136: Train batch 4800: loss: 14.43 grad norm: 66.02 time: 0.066
2026-01-08 13:16:49,174: Train batch 5000: loss: 31.17 grad norm: 89.28 time: 0.067
2026-01-08 13:16:49,174: Running test after training batch: 5000
2026-01-08 13:16:49,268: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:16:54,085: WER debug example
  GT : you can see the code at this point as well
  PR : yu end e a old at this otte is ill
2026-01-08 13:16:54,111: WER debug example
  GT : how does it keep the cost down
  PR : aue est it heap the est ent
2026-01-08 13:16:55,535: Val batch 5000: PER (avg): 0.2203 CTC Loss (avg): 21.6389 WER(1gram): 72.08% (n=64) time: 6.360
2026-01-08 13:16:55,535: WER lens: avg_true_words=6.16 avg_pred_words=5.80 max_pred_words=12
2026-01-08 13:16:55,535: t15.2023.08.13 val PER: 0.2017
2026-01-08 13:16:55,535: t15.2023.08.18 val PER: 0.1710
2026-01-08 13:16:55,535: t15.2023.08.20 val PER: 0.1620
2026-01-08 13:16:55,535: t15.2023.08.25 val PER: 0.1370
2026-01-08 13:16:55,536: t15.2023.08.27 val PER: 0.2492
2026-01-08 13:16:55,536: t15.2023.09.01 val PER: 0.1356
2026-01-08 13:16:55,536: t15.2023.09.03 val PER: 0.2209
2026-01-08 13:16:55,536: t15.2023.09.24 val PER: 0.1566
2026-01-08 13:16:55,536: t15.2023.09.29 val PER: 0.1857
2026-01-08 13:16:55,536: t15.2023.10.01 val PER: 0.2219
2026-01-08 13:16:55,536: t15.2023.10.06 val PER: 0.1410
2026-01-08 13:16:55,536: t15.2023.10.08 val PER: 0.3180
2026-01-08 13:16:55,536: t15.2023.10.13 val PER: 0.2684
2026-01-08 13:16:55,536: t15.2023.10.15 val PER: 0.2129
2026-01-08 13:16:55,536: t15.2023.10.20 val PER: 0.2416
2026-01-08 13:16:55,536: t15.2023.10.22 val PER: 0.1626
2026-01-08 13:16:55,537: t15.2023.11.03 val PER: 0.2266
2026-01-08 13:16:55,537: t15.2023.11.04 val PER: 0.0273
2026-01-08 13:16:55,537: t15.2023.11.17 val PER: 0.0731
2026-01-08 13:16:55,537: t15.2023.11.19 val PER: 0.0659
2026-01-08 13:16:55,537: t15.2023.11.26 val PER: 0.2130
2026-01-08 13:16:55,537: t15.2023.12.03 val PER: 0.1891
2026-01-08 13:16:55,537: t15.2023.12.08 val PER: 0.1937
2026-01-08 13:16:55,537: t15.2023.12.10 val PER: 0.1577
2026-01-08 13:16:55,537: t15.2023.12.17 val PER: 0.2058
2026-01-08 13:16:55,537: t15.2023.12.29 val PER: 0.2169
2026-01-08 13:16:55,537: t15.2024.02.25 val PER: 0.1770
2026-01-08 13:16:55,537: t15.2024.03.08 val PER: 0.3030
2026-01-08 13:16:55,537: t15.2024.03.15 val PER: 0.2645
2026-01-08 13:16:55,537: t15.2024.03.17 val PER: 0.2322
2026-01-08 13:16:55,537: t15.2024.05.10 val PER: 0.2318
2026-01-08 13:16:55,537: t15.2024.06.14 val PER: 0.2382
2026-01-08 13:16:55,538: t15.2024.07.19 val PER: 0.3349
2026-01-08 13:16:55,538: t15.2024.07.21 val PER: 0.1703
2026-01-08 13:16:55,538: t15.2024.07.28 val PER: 0.2206
2026-01-08 13:16:55,538: t15.2025.01.10 val PER: 0.3939
2026-01-08 13:16:55,538: t15.2025.01.12 val PER: 0.2394
2026-01-08 13:16:55,538: t15.2025.03.14 val PER: 0.3935
2026-01-08 13:16:55,538: t15.2025.03.16 val PER: 0.2853
2026-01-08 13:16:55,538: t15.2025.03.30 val PER: 0.3908
2026-01-08 13:16:55,538: t15.2025.04.13 val PER: 0.2996
2026-01-08 13:16:55,539: New best val WER(1gram) 76.14% --> 72.08%
2026-01-08 13:16:55,540: Checkpointing model
2026-01-08 13:16:55,679: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:17:13,858: Train batch 5200: loss: 18.06 grad norm: 78.79 time: 0.054
2026-01-08 13:17:32,332: Train batch 5400: loss: 17.67 grad norm: 69.30 time: 0.072
2026-01-08 13:17:41,498: Running test after training batch: 5500
2026-01-08 13:17:41,599: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:17:46,350: WER debug example
  GT : you can see the code at this point as well
  PR : ou an e a owed at this art its ill
2026-01-08 13:17:46,377: WER debug example
  GT : how does it keep the cost down
  PR : aue us it keep the ost ent
2026-01-08 13:17:47,788: Val batch 5500: PER (avg): 0.2125 CTC Loss (avg): 20.8423 WER(1gram): 74.37% (n=64) time: 6.290
2026-01-08 13:17:47,788: WER lens: avg_true_words=6.16 avg_pred_words=5.83 max_pred_words=12
2026-01-08 13:17:47,788: t15.2023.08.13 val PER: 0.1736
2026-01-08 13:17:47,789: t15.2023.08.18 val PER: 0.1827
2026-01-08 13:17:47,789: t15.2023.08.20 val PER: 0.1477
2026-01-08 13:17:47,789: t15.2023.08.25 val PER: 0.1386
2026-01-08 13:17:47,789: t15.2023.08.27 val PER: 0.2540
2026-01-08 13:17:47,789: t15.2023.09.01 val PER: 0.1226
2026-01-08 13:17:47,789: t15.2023.09.03 val PER: 0.2245
2026-01-08 13:17:47,789: t15.2023.09.24 val PER: 0.1675
2026-01-08 13:17:47,789: t15.2023.09.29 val PER: 0.1761
2026-01-08 13:17:47,789: t15.2023.10.01 val PER: 0.2140
2026-01-08 13:17:47,789: t15.2023.10.06 val PER: 0.1281
2026-01-08 13:17:47,789: t15.2023.10.08 val PER: 0.3099
2026-01-08 13:17:47,789: t15.2023.10.13 val PER: 0.2676
2026-01-08 13:17:47,789: t15.2023.10.15 val PER: 0.2169
2026-01-08 13:17:47,790: t15.2023.10.20 val PER: 0.2114
2026-01-08 13:17:47,790: t15.2023.10.22 val PER: 0.1670
2026-01-08 13:17:47,790: t15.2023.11.03 val PER: 0.2198
2026-01-08 13:17:47,790: t15.2023.11.04 val PER: 0.0410
2026-01-08 13:17:47,790: t15.2023.11.17 val PER: 0.0778
2026-01-08 13:17:47,790: t15.2023.11.19 val PER: 0.0619
2026-01-08 13:17:47,790: t15.2023.11.26 val PER: 0.2123
2026-01-08 13:17:47,790: t15.2023.12.03 val PER: 0.1933
2026-01-08 13:17:47,790: t15.2023.12.08 val PER: 0.1811
2026-01-08 13:17:47,790: t15.2023.12.10 val PER: 0.1524
2026-01-08 13:17:47,790: t15.2023.12.17 val PER: 0.2069
2026-01-08 13:17:47,790: t15.2023.12.29 val PER: 0.2114
2026-01-08 13:17:47,791: t15.2024.02.25 val PER: 0.1742
2026-01-08 13:17:47,791: t15.2024.03.08 val PER: 0.3030
2026-01-08 13:17:47,791: t15.2024.03.15 val PER: 0.2545
2026-01-08 13:17:47,791: t15.2024.03.17 val PER: 0.2099
2026-01-08 13:17:47,791: t15.2024.05.10 val PER: 0.2199
2026-01-08 13:17:47,791: t15.2024.06.14 val PER: 0.2303
2026-01-08 13:17:47,791: t15.2024.07.19 val PER: 0.3368
2026-01-08 13:17:47,791: t15.2024.07.21 val PER: 0.1538
2026-01-08 13:17:47,791: t15.2024.07.28 val PER: 0.2051
2026-01-08 13:17:47,791: t15.2025.01.10 val PER: 0.3664
2026-01-08 13:17:47,791: t15.2025.01.12 val PER: 0.2171
2026-01-08 13:17:47,791: t15.2025.03.14 val PER: 0.3639
2026-01-08 13:17:47,791: t15.2025.03.16 val PER: 0.2657
2026-01-08 13:17:47,791: t15.2025.03.30 val PER: 0.3747
2026-01-08 13:17:47,792: t15.2025.04.13 val PER: 0.2839
2026-01-08 13:17:56,863: Train batch 5600: loss: 19.88 grad norm: 72.69 time: 0.065
2026-01-08 13:18:14,326: Train batch 5800: loss: 13.77 grad norm: 74.26 time: 0.087
2026-01-08 13:18:31,610: Train batch 6000: loss: 13.00 grad norm: 63.63 time: 0.051
2026-01-08 13:18:31,610: Running test after training batch: 6000
2026-01-08 13:18:31,713: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:18:36,632: WER debug example
  GT : you can see the code at this point as well
  PR : yu an e a cold at this art is ill
2026-01-08 13:18:36,659: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the otte ent
2026-01-08 13:18:38,191: Val batch 6000: PER (avg): 0.2055 CTC Loss (avg): 20.3216 WER(1gram): 70.81% (n=64) time: 6.581
2026-01-08 13:18:38,191: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=12
2026-01-08 13:18:38,191: t15.2023.08.13 val PER: 0.1746
2026-01-08 13:18:38,191: t15.2023.08.18 val PER: 0.1534
2026-01-08 13:18:38,192: t15.2023.08.20 val PER: 0.1549
2026-01-08 13:18:38,192: t15.2023.08.25 val PER: 0.1295
2026-01-08 13:18:38,192: t15.2023.08.27 val PER: 0.2379
2026-01-08 13:18:38,192: t15.2023.09.01 val PER: 0.1291
2026-01-08 13:18:38,192: t15.2023.09.03 val PER: 0.2138
2026-01-08 13:18:38,192: t15.2023.09.24 val PER: 0.1578
2026-01-08 13:18:38,192: t15.2023.09.29 val PER: 0.1710
2026-01-08 13:18:38,192: t15.2023.10.01 val PER: 0.2127
2026-01-08 13:18:38,192: t15.2023.10.06 val PER: 0.1313
2026-01-08 13:18:38,192: t15.2023.10.08 val PER: 0.2923
2026-01-08 13:18:38,192: t15.2023.10.13 val PER: 0.2537
2026-01-08 13:18:38,192: t15.2023.10.15 val PER: 0.2070
2026-01-08 13:18:38,193: t15.2023.10.20 val PER: 0.2148
2026-01-08 13:18:38,193: t15.2023.10.22 val PER: 0.1626
2026-01-08 13:18:38,193: t15.2023.11.03 val PER: 0.2225
2026-01-08 13:18:38,193: t15.2023.11.04 val PER: 0.0478
2026-01-08 13:18:38,193: t15.2023.11.17 val PER: 0.0731
2026-01-08 13:18:38,193: t15.2023.11.19 val PER: 0.0619
2026-01-08 13:18:38,193: t15.2023.11.26 val PER: 0.1848
2026-01-08 13:18:38,193: t15.2023.12.03 val PER: 0.1733
2026-01-08 13:18:38,193: t15.2023.12.08 val PER: 0.1658
2026-01-08 13:18:38,193: t15.2023.12.10 val PER: 0.1577
2026-01-08 13:18:38,193: t15.2023.12.17 val PER: 0.1892
2026-01-08 13:18:38,193: t15.2023.12.29 val PER: 0.1908
2026-01-08 13:18:38,194: t15.2024.02.25 val PER: 0.1699
2026-01-08 13:18:38,194: t15.2024.03.08 val PER: 0.2945
2026-01-08 13:18:38,194: t15.2024.03.15 val PER: 0.2489
2026-01-08 13:18:38,194: t15.2024.03.17 val PER: 0.2057
2026-01-08 13:18:38,194: t15.2024.05.10 val PER: 0.2437
2026-01-08 13:18:38,194: t15.2024.06.14 val PER: 0.2145
2026-01-08 13:18:38,194: t15.2024.07.19 val PER: 0.3131
2026-01-08 13:18:38,194: t15.2024.07.21 val PER: 0.1717
2026-01-08 13:18:38,194: t15.2024.07.28 val PER: 0.1985
2026-01-08 13:18:38,194: t15.2025.01.10 val PER: 0.3581
2026-01-08 13:18:38,194: t15.2025.01.12 val PER: 0.2186
2026-01-08 13:18:38,194: t15.2025.03.14 val PER: 0.3669
2026-01-08 13:18:38,194: t15.2025.03.16 val PER: 0.2513
2026-01-08 13:18:38,195: t15.2025.03.30 val PER: 0.3563
2026-01-08 13:18:38,195: t15.2025.04.13 val PER: 0.2839
2026-01-08 13:18:38,195: New best val WER(1gram) 72.08% --> 70.81%
2026-01-08 13:18:38,196: Checkpointing model
2026-01-08 13:18:38,334: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:18:56,213: Train batch 6200: loss: 15.44 grad norm: 72.87 time: 0.074
2026-01-08 13:19:14,458: Train batch 6400: loss: 18.35 grad norm: 75.93 time: 0.066
2026-01-08 13:19:23,458: Running test after training batch: 6500
2026-01-08 13:19:23,602: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:19:28,311: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e utt cold at this otte as ill
2026-01-08 13:19:28,338: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the quast et
2026-01-08 13:19:29,842: Val batch 6500: PER (avg): 0.2013 CTC Loss (avg): 19.9490 WER(1gram): 68.27% (n=64) time: 6.384
2026-01-08 13:19:29,842: WER lens: avg_true_words=6.16 avg_pred_words=6.02 max_pred_words=11
2026-01-08 13:19:29,842: t15.2023.08.13 val PER: 0.1549
2026-01-08 13:19:29,842: t15.2023.08.18 val PER: 0.1618
2026-01-08 13:19:29,842: t15.2023.08.20 val PER: 0.1326
2026-01-08 13:19:29,843: t15.2023.08.25 val PER: 0.1175
2026-01-08 13:19:29,843: t15.2023.08.27 val PER: 0.2283
2026-01-08 13:19:29,843: t15.2023.09.01 val PER: 0.1096
2026-01-08 13:19:29,843: t15.2023.09.03 val PER: 0.2138
2026-01-08 13:19:29,843: t15.2023.09.24 val PER: 0.1529
2026-01-08 13:19:29,843: t15.2023.09.29 val PER: 0.1640
2026-01-08 13:19:29,843: t15.2023.10.01 val PER: 0.2034
2026-01-08 13:19:29,843: t15.2023.10.06 val PER: 0.1346
2026-01-08 13:19:29,843: t15.2023.10.08 val PER: 0.2963
2026-01-08 13:19:29,843: t15.2023.10.13 val PER: 0.2498
2026-01-08 13:19:29,843: t15.2023.10.15 val PER: 0.1984
2026-01-08 13:19:29,844: t15.2023.10.20 val PER: 0.1913
2026-01-08 13:19:29,844: t15.2023.10.22 val PER: 0.1637
2026-01-08 13:19:29,844: t15.2023.11.03 val PER: 0.2225
2026-01-08 13:19:29,844: t15.2023.11.04 val PER: 0.0375
2026-01-08 13:19:29,844: t15.2023.11.17 val PER: 0.0700
2026-01-08 13:19:29,844: t15.2023.11.19 val PER: 0.0559
2026-01-08 13:19:29,844: t15.2023.11.26 val PER: 0.1862
2026-01-08 13:19:29,844: t15.2023.12.03 val PER: 0.1702
2026-01-08 13:19:29,844: t15.2023.12.08 val PER: 0.1718
2026-01-08 13:19:29,844: t15.2023.12.10 val PER: 0.1301
2026-01-08 13:19:29,844: t15.2023.12.17 val PER: 0.1913
2026-01-08 13:19:29,844: t15.2023.12.29 val PER: 0.1977
2026-01-08 13:19:29,844: t15.2024.02.25 val PER: 0.1643
2026-01-08 13:19:29,844: t15.2024.03.08 val PER: 0.2802
2026-01-08 13:19:29,844: t15.2024.03.15 val PER: 0.2470
2026-01-08 13:19:29,845: t15.2024.03.17 val PER: 0.2050
2026-01-08 13:19:29,845: t15.2024.05.10 val PER: 0.2452
2026-01-08 13:19:29,845: t15.2024.06.14 val PER: 0.2208
2026-01-08 13:19:29,845: t15.2024.07.19 val PER: 0.3072
2026-01-08 13:19:29,845: t15.2024.07.21 val PER: 0.1517
2026-01-08 13:19:29,845: t15.2024.07.28 val PER: 0.1868
2026-01-08 13:19:29,845: t15.2025.01.10 val PER: 0.3678
2026-01-08 13:19:29,845: t15.2025.01.12 val PER: 0.2156
2026-01-08 13:19:29,845: t15.2025.03.14 val PER: 0.3595
2026-01-08 13:19:29,845: t15.2025.03.16 val PER: 0.2644
2026-01-08 13:19:29,845: t15.2025.03.30 val PER: 0.3724
2026-01-08 13:19:29,845: t15.2025.04.13 val PER: 0.2882
2026-01-08 13:19:29,846: New best val WER(1gram) 70.81% --> 68.27%
2026-01-08 13:19:29,846: Checkpointing model
2026-01-08 13:19:29,993: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:19:39,190: Train batch 6600: loss: 11.70 grad norm: 57.89 time: 0.046
2026-01-08 13:19:57,411: Train batch 6800: loss: 15.25 grad norm: 74.44 time: 0.050
2026-01-08 13:20:16,029: Train batch 7000: loss: 15.99 grad norm: 71.29 time: 0.064
2026-01-08 13:20:16,029: Running test after training batch: 7000
2026-01-08 13:20:16,123: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:20:20,823: WER debug example
  GT : you can see the code at this point as well
  PR : yu an e a owed at this otte is ill
2026-01-08 13:20:20,850: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the otte et
2026-01-08 13:20:22,376: Val batch 7000: PER (avg): 0.1903 CTC Loss (avg): 19.1348 WER(1gram): 69.29% (n=64) time: 6.347
2026-01-08 13:20:22,377: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=11
2026-01-08 13:20:22,377: t15.2023.08.13 val PER: 0.1601
2026-01-08 13:20:22,377: t15.2023.08.18 val PER: 0.1366
2026-01-08 13:20:22,377: t15.2023.08.20 val PER: 0.1279
2026-01-08 13:20:22,377: t15.2023.08.25 val PER: 0.1265
2026-01-08 13:20:22,377: t15.2023.08.27 val PER: 0.2203
2026-01-08 13:20:22,377: t15.2023.09.01 val PER: 0.1071
2026-01-08 13:20:22,377: t15.2023.09.03 val PER: 0.1948
2026-01-08 13:20:22,377: t15.2023.09.24 val PER: 0.1468
2026-01-08 13:20:22,378: t15.2023.09.29 val PER: 0.1589
2026-01-08 13:20:22,378: t15.2023.10.01 val PER: 0.2021
2026-01-08 13:20:22,378: t15.2023.10.06 val PER: 0.1152
2026-01-08 13:20:22,378: t15.2023.10.08 val PER: 0.2828
2026-01-08 13:20:22,378: t15.2023.10.13 val PER: 0.2444
2026-01-08 13:20:22,378: t15.2023.10.15 val PER: 0.1819
2026-01-08 13:20:22,378: t15.2023.10.20 val PER: 0.1980
2026-01-08 13:20:22,378: t15.2023.10.22 val PER: 0.1347
2026-01-08 13:20:22,379: t15.2023.11.03 val PER: 0.2049
2026-01-08 13:20:22,379: t15.2023.11.04 val PER: 0.0307
2026-01-08 13:20:22,379: t15.2023.11.17 val PER: 0.0653
2026-01-08 13:20:22,379: t15.2023.11.19 val PER: 0.0599
2026-01-08 13:20:22,379: t15.2023.11.26 val PER: 0.1783
2026-01-08 13:20:22,379: t15.2023.12.03 val PER: 0.1618
2026-01-08 13:20:22,379: t15.2023.12.08 val PER: 0.1558
2026-01-08 13:20:22,379: t15.2023.12.10 val PER: 0.1301
2026-01-08 13:20:22,379: t15.2023.12.17 val PER: 0.1767
2026-01-08 13:20:22,379: t15.2023.12.29 val PER: 0.1901
2026-01-08 13:20:22,379: t15.2024.02.25 val PER: 0.1671
2026-01-08 13:20:22,379: t15.2024.03.08 val PER: 0.2546
2026-01-08 13:20:22,379: t15.2024.03.15 val PER: 0.2301
2026-01-08 13:20:22,380: t15.2024.03.17 val PER: 0.1820
2026-01-08 13:20:22,380: t15.2024.05.10 val PER: 0.2036
2026-01-08 13:20:22,380: t15.2024.06.14 val PER: 0.2145
2026-01-08 13:20:22,380: t15.2024.07.19 val PER: 0.3013
2026-01-08 13:20:22,380: t15.2024.07.21 val PER: 0.1393
2026-01-08 13:20:22,380: t15.2024.07.28 val PER: 0.1801
2026-01-08 13:20:22,380: t15.2025.01.10 val PER: 0.3499
2026-01-08 13:20:22,380: t15.2025.01.12 val PER: 0.2109
2026-01-08 13:20:22,380: t15.2025.03.14 val PER: 0.3698
2026-01-08 13:20:22,381: t15.2025.03.16 val PER: 0.2264
2026-01-08 13:20:22,381: t15.2025.03.30 val PER: 0.3540
2026-01-08 13:20:22,381: t15.2025.04.13 val PER: 0.2767
2026-01-08 13:20:40,689: Train batch 7200: loss: 14.69 grad norm: 69.93 time: 0.083
2026-01-08 13:20:58,484: Train batch 7400: loss: 12.80 grad norm: 67.65 time: 0.081
2026-01-08 13:21:07,182: Running test after training batch: 7500
2026-01-08 13:21:07,275: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:21:11,992: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e a ude at this art is will
2026-01-08 13:21:12,021: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the est ent
2026-01-08 13:21:13,545: Val batch 7500: PER (avg): 0.1846 CTC Loss (avg): 18.4730 WER(1gram): 65.99% (n=64) time: 6.362
2026-01-08 13:21:13,545: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=12
2026-01-08 13:21:13,545: t15.2023.08.13 val PER: 0.1435
2026-01-08 13:21:13,545: t15.2023.08.18 val PER: 0.1442
2026-01-08 13:21:13,545: t15.2023.08.20 val PER: 0.1223
2026-01-08 13:21:13,546: t15.2023.08.25 val PER: 0.1130
2026-01-08 13:21:13,546: t15.2023.08.27 val PER: 0.2154
2026-01-08 13:21:13,546: t15.2023.09.01 val PER: 0.1055
2026-01-08 13:21:13,546: t15.2023.09.03 val PER: 0.1971
2026-01-08 13:21:13,546: t15.2023.09.24 val PER: 0.1505
2026-01-08 13:21:13,546: t15.2023.09.29 val PER: 0.1500
2026-01-08 13:21:13,546: t15.2023.10.01 val PER: 0.1975
2026-01-08 13:21:13,546: t15.2023.10.06 val PER: 0.1152
2026-01-08 13:21:13,546: t15.2023.10.08 val PER: 0.2706
2026-01-08 13:21:13,546: t15.2023.10.13 val PER: 0.2242
2026-01-08 13:21:13,546: t15.2023.10.15 val PER: 0.1800
2026-01-08 13:21:13,546: t15.2023.10.20 val PER: 0.1946
2026-01-08 13:21:13,546: t15.2023.10.22 val PER: 0.1392
2026-01-08 13:21:13,547: t15.2023.11.03 val PER: 0.1988
2026-01-08 13:21:13,547: t15.2023.11.04 val PER: 0.0410
2026-01-08 13:21:13,547: t15.2023.11.17 val PER: 0.0622
2026-01-08 13:21:13,547: t15.2023.11.19 val PER: 0.0519
2026-01-08 13:21:13,547: t15.2023.11.26 val PER: 0.1746
2026-01-08 13:21:13,547: t15.2023.12.03 val PER: 0.1555
2026-01-08 13:21:13,547: t15.2023.12.08 val PER: 0.1405
2026-01-08 13:21:13,547: t15.2023.12.10 val PER: 0.1209
2026-01-08 13:21:13,547: t15.2023.12.17 val PER: 0.1840
2026-01-08 13:21:13,547: t15.2023.12.29 val PER: 0.1723
2026-01-08 13:21:13,548: t15.2024.02.25 val PER: 0.1559
2026-01-08 13:21:13,548: t15.2024.03.08 val PER: 0.2589
2026-01-08 13:21:13,548: t15.2024.03.15 val PER: 0.2226
2026-01-08 13:21:13,548: t15.2024.03.17 val PER: 0.1890
2026-01-08 13:21:13,548: t15.2024.05.10 val PER: 0.2125
2026-01-08 13:21:13,548: t15.2024.06.14 val PER: 0.1972
2026-01-08 13:21:13,548: t15.2024.07.19 val PER: 0.2841
2026-01-08 13:21:13,548: t15.2024.07.21 val PER: 0.1400
2026-01-08 13:21:13,548: t15.2024.07.28 val PER: 0.1846
2026-01-08 13:21:13,548: t15.2025.01.10 val PER: 0.3540
2026-01-08 13:21:13,548: t15.2025.01.12 val PER: 0.1863
2026-01-08 13:21:13,548: t15.2025.03.14 val PER: 0.3580
2026-01-08 13:21:13,548: t15.2025.03.16 val PER: 0.2264
2026-01-08 13:21:13,549: t15.2025.03.30 val PER: 0.3460
2026-01-08 13:21:13,549: t15.2025.04.13 val PER: 0.2625
2026-01-08 13:21:13,550: New best val WER(1gram) 68.27% --> 65.99%
2026-01-08 13:21:13,550: Checkpointing model
2026-01-08 13:21:13,690: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:21:22,868: Train batch 7600: loss: 15.78 grad norm: 74.50 time: 0.072
2026-01-08 13:21:40,819: Train batch 7800: loss: 15.05 grad norm: 75.19 time: 0.059
2026-01-08 13:21:59,451: Train batch 8000: loss: 11.60 grad norm: 61.13 time: 0.075
2026-01-08 13:21:59,451: Running test after training batch: 8000
2026-01-08 13:21:59,558: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:22:04,229: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e a owed at this otte is ill
2026-01-08 13:22:04,258: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep a utt ent
2026-01-08 13:22:05,891: Val batch 8000: PER (avg): 0.1808 CTC Loss (avg): 18.0640 WER(1gram): 68.78% (n=64) time: 6.440
2026-01-08 13:22:05,891: WER lens: avg_true_words=6.16 avg_pred_words=5.97 max_pred_words=11
2026-01-08 13:22:05,891: t15.2023.08.13 val PER: 0.1403
2026-01-08 13:22:05,891: t15.2023.08.18 val PER: 0.1341
2026-01-08 13:22:05,891: t15.2023.08.20 val PER: 0.1223
2026-01-08 13:22:05,892: t15.2023.08.25 val PER: 0.1190
2026-01-08 13:22:05,895: t15.2023.08.27 val PER: 0.1994
2026-01-08 13:22:05,895: t15.2023.09.01 val PER: 0.0998
2026-01-08 13:22:05,895: t15.2023.09.03 val PER: 0.1900
2026-01-08 13:22:05,895: t15.2023.09.24 val PER: 0.1456
2026-01-08 13:22:05,895: t15.2023.09.29 val PER: 0.1429
2026-01-08 13:22:05,895: t15.2023.10.01 val PER: 0.1962
2026-01-08 13:22:05,895: t15.2023.10.06 val PER: 0.1216
2026-01-08 13:22:05,896: t15.2023.10.08 val PER: 0.2869
2026-01-08 13:22:05,896: t15.2023.10.13 val PER: 0.2413
2026-01-08 13:22:05,896: t15.2023.10.15 val PER: 0.1879
2026-01-08 13:22:05,896: t15.2023.10.20 val PER: 0.2181
2026-01-08 13:22:05,896: t15.2023.10.22 val PER: 0.1392
2026-01-08 13:22:05,896: t15.2023.11.03 val PER: 0.1886
2026-01-08 13:22:05,896: t15.2023.11.04 val PER: 0.0273
2026-01-08 13:22:05,897: t15.2023.11.17 val PER: 0.0560
2026-01-08 13:22:05,897: t15.2023.11.19 val PER: 0.0579
2026-01-08 13:22:05,897: t15.2023.11.26 val PER: 0.1587
2026-01-08 13:22:05,897: t15.2023.12.03 val PER: 0.1534
2026-01-08 13:22:05,897: t15.2023.12.08 val PER: 0.1352
2026-01-08 13:22:05,897: t15.2023.12.10 val PER: 0.1248
2026-01-08 13:22:05,897: t15.2023.12.17 val PER: 0.1726
2026-01-08 13:22:05,897: t15.2023.12.29 val PER: 0.1716
2026-01-08 13:22:05,897: t15.2024.02.25 val PER: 0.1489
2026-01-08 13:22:05,897: t15.2024.03.08 val PER: 0.2589
2026-01-08 13:22:05,898: t15.2024.03.15 val PER: 0.2195
2026-01-08 13:22:05,898: t15.2024.03.17 val PER: 0.1813
2026-01-08 13:22:05,898: t15.2024.05.10 val PER: 0.2065
2026-01-08 13:22:05,898: t15.2024.06.14 val PER: 0.2003
2026-01-08 13:22:05,898: t15.2024.07.19 val PER: 0.2894
2026-01-08 13:22:05,898: t15.2024.07.21 val PER: 0.1262
2026-01-08 13:22:05,898: t15.2024.07.28 val PER: 0.1779
2026-01-08 13:22:05,898: t15.2025.01.10 val PER: 0.3209
2026-01-08 13:22:05,898: t15.2025.01.12 val PER: 0.1840
2026-01-08 13:22:05,898: t15.2025.03.14 val PER: 0.3358
2026-01-08 13:22:05,898: t15.2025.03.16 val PER: 0.2225
2026-01-08 13:22:05,898: t15.2025.03.30 val PER: 0.3333
2026-01-08 13:22:05,898: t15.2025.04.13 val PER: 0.2553
2026-01-08 13:22:23,221: Train batch 8200: loss: 8.78 grad norm: 51.61 time: 0.058
2026-01-08 13:22:41,232: Train batch 8400: loss: 11.50 grad norm: 69.67 time: 0.068
2026-01-08 13:22:50,655: Running test after training batch: 8500
2026-01-08 13:22:50,755: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:22:55,528: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e a owed at this art is will
2026-01-08 13:22:55,557: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the ost it
2026-01-08 13:22:57,129: Val batch 8500: PER (avg): 0.1760 CTC Loss (avg): 17.6473 WER(1gram): 66.75% (n=64) time: 6.474
2026-01-08 13:22:57,129: WER lens: avg_true_words=6.16 avg_pred_words=6.00 max_pred_words=12
2026-01-08 13:22:57,130: t15.2023.08.13 val PER: 0.1476
2026-01-08 13:22:57,130: t15.2023.08.18 val PER: 0.1282
2026-01-08 13:22:57,130: t15.2023.08.20 val PER: 0.1223
2026-01-08 13:22:57,130: t15.2023.08.25 val PER: 0.1160
2026-01-08 13:22:57,130: t15.2023.08.27 val PER: 0.2058
2026-01-08 13:22:57,130: t15.2023.09.01 val PER: 0.0998
2026-01-08 13:22:57,130: t15.2023.09.03 val PER: 0.1888
2026-01-08 13:22:57,130: t15.2023.09.24 val PER: 0.1335
2026-01-08 13:22:57,130: t15.2023.09.29 val PER: 0.1468
2026-01-08 13:22:57,130: t15.2023.10.01 val PER: 0.1995
2026-01-08 13:22:57,130: t15.2023.10.06 val PER: 0.1033
2026-01-08 13:22:57,130: t15.2023.10.08 val PER: 0.2788
2026-01-08 13:22:57,130: t15.2023.10.13 val PER: 0.2351
2026-01-08 13:22:57,131: t15.2023.10.15 val PER: 0.1885
2026-01-08 13:22:57,131: t15.2023.10.20 val PER: 0.1946
2026-01-08 13:22:57,131: t15.2023.10.22 val PER: 0.1247
2026-01-08 13:22:57,131: t15.2023.11.03 val PER: 0.1967
2026-01-08 13:22:57,131: t15.2023.11.04 val PER: 0.0307
2026-01-08 13:22:57,131: t15.2023.11.17 val PER: 0.0700
2026-01-08 13:22:57,131: t15.2023.11.19 val PER: 0.0419
2026-01-08 13:22:57,131: t15.2023.11.26 val PER: 0.1507
2026-01-08 13:22:57,131: t15.2023.12.03 val PER: 0.1439
2026-01-08 13:22:57,131: t15.2023.12.08 val PER: 0.1338
2026-01-08 13:22:57,131: t15.2023.12.10 val PER: 0.1209
2026-01-08 13:22:57,131: t15.2023.12.17 val PER: 0.1632
2026-01-08 13:22:57,131: t15.2023.12.29 val PER: 0.1661
2026-01-08 13:22:57,131: t15.2024.02.25 val PER: 0.1348
2026-01-08 13:22:57,132: t15.2024.03.08 val PER: 0.2376
2026-01-08 13:22:57,132: t15.2024.03.15 val PER: 0.2108
2026-01-08 13:22:57,132: t15.2024.03.17 val PER: 0.1674
2026-01-08 13:22:57,132: t15.2024.05.10 val PER: 0.2051
2026-01-08 13:22:57,132: t15.2024.06.14 val PER: 0.1845
2026-01-08 13:22:57,132: t15.2024.07.19 val PER: 0.2716
2026-01-08 13:22:57,132: t15.2024.07.21 val PER: 0.1241
2026-01-08 13:22:57,132: t15.2024.07.28 val PER: 0.1654
2026-01-08 13:22:57,132: t15.2025.01.10 val PER: 0.3278
2026-01-08 13:22:57,132: t15.2025.01.12 val PER: 0.1809
2026-01-08 13:22:57,132: t15.2025.03.14 val PER: 0.3314
2026-01-08 13:22:57,132: t15.2025.03.16 val PER: 0.2120
2026-01-08 13:22:57,132: t15.2025.03.30 val PER: 0.3345
2026-01-08 13:22:57,132: t15.2025.04.13 val PER: 0.2568
2026-01-08 13:23:06,343: Train batch 8600: loss: 14.06 grad norm: 69.57 time: 0.058
2026-01-08 13:23:24,336: Train batch 8800: loss: 17.36 grad norm: 79.70 time: 0.064
2026-01-08 13:23:41,762: Train batch 9000: loss: 14.71 grad norm: 72.41 time: 0.076
2026-01-08 13:23:41,762: Running test after training batch: 9000
2026-01-08 13:23:41,876: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:23:46,552: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e a owed at this otte as will
2026-01-08 13:23:46,582: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the ost ent
2026-01-08 13:23:48,183: Val batch 9000: PER (avg): 0.1713 CTC Loss (avg): 17.6825 WER(1gram): 63.71% (n=64) time: 6.421
2026-01-08 13:23:48,183: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=12
2026-01-08 13:23:48,184: t15.2023.08.13 val PER: 0.1331
2026-01-08 13:23:48,184: t15.2023.08.18 val PER: 0.1266
2026-01-08 13:23:48,184: t15.2023.08.20 val PER: 0.1207
2026-01-08 13:23:48,184: t15.2023.08.25 val PER: 0.0949
2026-01-08 13:23:48,184: t15.2023.08.27 val PER: 0.1913
2026-01-08 13:23:48,184: t15.2023.09.01 val PER: 0.0885
2026-01-08 13:23:48,184: t15.2023.09.03 val PER: 0.1758
2026-01-08 13:23:48,184: t15.2023.09.24 val PER: 0.1456
2026-01-08 13:23:48,184: t15.2023.09.29 val PER: 0.1385
2026-01-08 13:23:48,184: t15.2023.10.01 val PER: 0.1942
2026-01-08 13:23:48,184: t15.2023.10.06 val PER: 0.0958
2026-01-08 13:23:48,185: t15.2023.10.08 val PER: 0.2571
2026-01-08 13:23:48,185: t15.2023.10.13 val PER: 0.2211
2026-01-08 13:23:48,185: t15.2023.10.15 val PER: 0.1773
2026-01-08 13:23:48,185: t15.2023.10.20 val PER: 0.1678
2026-01-08 13:23:48,185: t15.2023.10.22 val PER: 0.1269
2026-01-08 13:23:48,185: t15.2023.11.03 val PER: 0.1893
2026-01-08 13:23:48,185: t15.2023.11.04 val PER: 0.0341
2026-01-08 13:23:48,185: t15.2023.11.17 val PER: 0.0482
2026-01-08 13:23:48,185: t15.2023.11.19 val PER: 0.0499
2026-01-08 13:23:48,185: t15.2023.11.26 val PER: 0.1464
2026-01-08 13:23:48,185: t15.2023.12.03 val PER: 0.1471
2026-01-08 13:23:48,185: t15.2023.12.08 val PER: 0.1338
2026-01-08 13:23:48,185: t15.2023.12.10 val PER: 0.1183
2026-01-08 13:23:48,185: t15.2023.12.17 val PER: 0.1590
2026-01-08 13:23:48,186: t15.2023.12.29 val PER: 0.1613
2026-01-08 13:23:48,186: t15.2024.02.25 val PER: 0.1348
2026-01-08 13:23:48,186: t15.2024.03.08 val PER: 0.2390
2026-01-08 13:23:48,186: t15.2024.03.15 val PER: 0.2176
2026-01-08 13:23:48,186: t15.2024.03.17 val PER: 0.1729
2026-01-08 13:23:48,186: t15.2024.05.10 val PER: 0.2051
2026-01-08 13:23:48,186: t15.2024.06.14 val PER: 0.1909
2026-01-08 13:23:48,186: t15.2024.07.19 val PER: 0.2795
2026-01-08 13:23:48,187: t15.2024.07.21 val PER: 0.1200
2026-01-08 13:23:48,187: t15.2024.07.28 val PER: 0.1500
2026-01-08 13:23:48,187: t15.2025.01.10 val PER: 0.3072
2026-01-08 13:23:48,187: t15.2025.01.12 val PER: 0.1701
2026-01-08 13:23:48,187: t15.2025.03.14 val PER: 0.3624
2026-01-08 13:23:48,187: t15.2025.03.16 val PER: 0.2225
2026-01-08 13:23:48,187: t15.2025.03.30 val PER: 0.3207
2026-01-08 13:23:48,187: t15.2025.04.13 val PER: 0.2397
2026-01-08 13:23:48,188: New best val WER(1gram) 65.99% --> 63.71%
2026-01-08 13:23:48,188: Checkpointing model
2026-01-08 13:23:48,330: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:24:07,307: Train batch 9200: loss: 9.83 grad norm: 63.01 time: 0.059
2026-01-08 13:24:26,186: Train batch 9400: loss: 8.20 grad norm: 59.73 time: 0.073
2026-01-08 13:24:35,637: Running test after training batch: 9500
2026-01-08 13:24:35,781: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:24:40,629: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e utt old at this otte as will
2026-01-08 13:24:40,660: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the ost ent
2026-01-08 13:24:42,282: Val batch 9500: PER (avg): 0.1675 CTC Loss (avg): 17.0618 WER(1gram): 64.72% (n=64) time: 6.644
2026-01-08 13:24:42,282: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=12
2026-01-08 13:24:42,282: t15.2023.08.13 val PER: 0.1383
2026-01-08 13:24:42,282: t15.2023.08.18 val PER: 0.1182
2026-01-08 13:24:42,282: t15.2023.08.20 val PER: 0.1136
2026-01-08 13:24:42,282: t15.2023.08.25 val PER: 0.0873
2026-01-08 13:24:42,283: t15.2023.08.27 val PER: 0.1961
2026-01-08 13:24:42,283: t15.2023.09.01 val PER: 0.0820
2026-01-08 13:24:42,283: t15.2023.09.03 val PER: 0.1793
2026-01-08 13:24:42,283: t15.2023.09.24 val PER: 0.1359
2026-01-08 13:24:42,283: t15.2023.09.29 val PER: 0.1391
2026-01-08 13:24:42,283: t15.2023.10.01 val PER: 0.1876
2026-01-08 13:24:42,283: t15.2023.10.06 val PER: 0.1109
2026-01-08 13:24:42,283: t15.2023.10.08 val PER: 0.2449
2026-01-08 13:24:42,283: t15.2023.10.13 val PER: 0.2149
2026-01-08 13:24:42,283: t15.2023.10.15 val PER: 0.1773
2026-01-08 13:24:42,283: t15.2023.10.20 val PER: 0.1711
2026-01-08 13:24:42,283: t15.2023.10.22 val PER: 0.1247
2026-01-08 13:24:42,283: t15.2023.11.03 val PER: 0.1744
2026-01-08 13:24:42,283: t15.2023.11.04 val PER: 0.0307
2026-01-08 13:24:42,283: t15.2023.11.17 val PER: 0.0451
2026-01-08 13:24:42,284: t15.2023.11.19 val PER: 0.0479
2026-01-08 13:24:42,284: t15.2023.11.26 val PER: 0.1362
2026-01-08 13:24:42,284: t15.2023.12.03 val PER: 0.1345
2026-01-08 13:24:42,284: t15.2023.12.08 val PER: 0.1218
2026-01-08 13:24:42,284: t15.2023.12.10 val PER: 0.1130
2026-01-08 13:24:42,284: t15.2023.12.17 val PER: 0.1518
2026-01-08 13:24:42,284: t15.2023.12.29 val PER: 0.1551
2026-01-08 13:24:42,284: t15.2024.02.25 val PER: 0.1264
2026-01-08 13:24:42,284: t15.2024.03.08 val PER: 0.2319
2026-01-08 13:24:42,284: t15.2024.03.15 val PER: 0.2145
2026-01-08 13:24:42,284: t15.2024.03.17 val PER: 0.1674
2026-01-08 13:24:42,284: t15.2024.05.10 val PER: 0.2021
2026-01-08 13:24:42,284: t15.2024.06.14 val PER: 0.1861
2026-01-08 13:24:42,284: t15.2024.07.19 val PER: 0.2755
2026-01-08 13:24:42,284: t15.2024.07.21 val PER: 0.1262
2026-01-08 13:24:42,284: t15.2024.07.28 val PER: 0.1581
2026-01-08 13:24:42,284: t15.2025.01.10 val PER: 0.3196
2026-01-08 13:24:42,284: t15.2025.01.12 val PER: 0.1663
2026-01-08 13:24:42,285: t15.2025.03.14 val PER: 0.3521
2026-01-08 13:24:42,285: t15.2025.03.16 val PER: 0.2173
2026-01-08 13:24:42,285: t15.2025.03.30 val PER: 0.3115
2026-01-08 13:24:42,285: t15.2025.04.13 val PER: 0.2439
2026-01-08 13:24:51,185: Train batch 9600: loss: 7.87 grad norm: 56.11 time: 0.078
2026-01-08 13:25:09,592: Train batch 9800: loss: 11.36 grad norm: 66.49 time: 0.068
2026-01-08 13:25:27,834: Train batch 10000: loss: 5.11 grad norm: 41.96 time: 0.064
2026-01-08 13:25:27,834: Running test after training batch: 10000
2026-01-08 13:25:27,942: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:25:32,652: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e a cold at this otte as will
2026-01-08 13:25:32,682: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the quast ent
2026-01-08 13:25:34,338: Val batch 10000: PER (avg): 0.1650 CTC Loss (avg): 16.8141 WER(1gram): 61.17% (n=64) time: 6.504
2026-01-08 13:25:34,339: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=12
2026-01-08 13:25:34,339: t15.2023.08.13 val PER: 0.1268
2026-01-08 13:25:34,339: t15.2023.08.18 val PER: 0.1199
2026-01-08 13:25:34,339: t15.2023.08.20 val PER: 0.1128
2026-01-08 13:25:34,339: t15.2023.08.25 val PER: 0.0949
2026-01-08 13:25:34,339: t15.2023.08.27 val PER: 0.1945
2026-01-08 13:25:34,339: t15.2023.09.01 val PER: 0.0812
2026-01-08 13:25:34,339: t15.2023.09.03 val PER: 0.1770
2026-01-08 13:25:34,339: t15.2023.09.24 val PER: 0.1323
2026-01-08 13:25:34,339: t15.2023.09.29 val PER: 0.1391
2026-01-08 13:25:34,340: t15.2023.10.01 val PER: 0.1876
2026-01-08 13:25:34,340: t15.2023.10.06 val PER: 0.1076
2026-01-08 13:25:34,340: t15.2023.10.08 val PER: 0.2449
2026-01-08 13:25:34,340: t15.2023.10.13 val PER: 0.2102
2026-01-08 13:25:34,340: t15.2023.10.15 val PER: 0.1813
2026-01-08 13:25:34,340: t15.2023.10.20 val PER: 0.1477
2026-01-08 13:25:34,340: t15.2023.10.22 val PER: 0.1180
2026-01-08 13:25:34,340: t15.2023.11.03 val PER: 0.1825
2026-01-08 13:25:34,341: t15.2023.11.04 val PER: 0.0205
2026-01-08 13:25:34,341: t15.2023.11.17 val PER: 0.0513
2026-01-08 13:25:34,341: t15.2023.11.19 val PER: 0.0459
2026-01-08 13:25:34,341: t15.2023.11.26 val PER: 0.1413
2026-01-08 13:25:34,341: t15.2023.12.03 val PER: 0.1345
2026-01-08 13:25:34,341: t15.2023.12.08 val PER: 0.1145
2026-01-08 13:25:34,341: t15.2023.12.10 val PER: 0.1012
2026-01-08 13:25:34,341: t15.2023.12.17 val PER: 0.1611
2026-01-08 13:25:34,341: t15.2023.12.29 val PER: 0.1469
2026-01-08 13:25:34,341: t15.2024.02.25 val PER: 0.1320
2026-01-08 13:25:34,341: t15.2024.03.08 val PER: 0.2347
2026-01-08 13:25:34,342: t15.2024.03.15 val PER: 0.2039
2026-01-08 13:25:34,342: t15.2024.03.17 val PER: 0.1695
2026-01-08 13:25:34,342: t15.2024.05.10 val PER: 0.1857
2026-01-08 13:25:34,345: t15.2024.06.14 val PER: 0.1845
2026-01-08 13:25:34,346: t15.2024.07.19 val PER: 0.2670
2026-01-08 13:25:34,346: t15.2024.07.21 val PER: 0.1186
2026-01-08 13:25:34,346: t15.2024.07.28 val PER: 0.1596
2026-01-08 13:25:34,346: t15.2025.01.10 val PER: 0.3182
2026-01-08 13:25:34,346: t15.2025.01.12 val PER: 0.1563
2026-01-08 13:25:34,346: t15.2025.03.14 val PER: 0.3432
2026-01-08 13:25:34,346: t15.2025.03.16 val PER: 0.2016
2026-01-08 13:25:34,346: t15.2025.03.30 val PER: 0.3172
2026-01-08 13:25:34,346: t15.2025.04.13 val PER: 0.2411
2026-01-08 13:25:34,347: New best val WER(1gram) 63.71% --> 61.17%
2026-01-08 13:25:34,347: Checkpointing model
2026-01-08 13:25:34,488: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:25:52,301: Train batch 10200: loss: 6.76 grad norm: 54.68 time: 0.053
2026-01-08 13:26:10,453: Train batch 10400: loss: 8.59 grad norm: 56.11 time: 0.076
2026-01-08 13:26:19,509: Running test after training batch: 10500
2026-01-08 13:26:19,632: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:26:24,613: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e a old at this arndt as will
2026-01-08 13:26:24,645: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the ost ent
2026-01-08 13:26:26,347: Val batch 10500: PER (avg): 0.1629 CTC Loss (avg): 16.9031 WER(1gram): 65.23% (n=64) time: 6.838
2026-01-08 13:26:26,348: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=12
2026-01-08 13:26:26,348: t15.2023.08.13 val PER: 0.1279
2026-01-08 13:26:26,348: t15.2023.08.18 val PER: 0.1190
2026-01-08 13:26:26,348: t15.2023.08.20 val PER: 0.1255
2026-01-08 13:26:26,348: t15.2023.08.25 val PER: 0.1039
2026-01-08 13:26:26,348: t15.2023.08.27 val PER: 0.1736
2026-01-08 13:26:26,348: t15.2023.09.01 val PER: 0.0747
2026-01-08 13:26:26,348: t15.2023.09.03 val PER: 0.1710
2026-01-08 13:26:26,350: t15.2023.09.24 val PER: 0.1335
2026-01-08 13:26:26,350: t15.2023.09.29 val PER: 0.1449
2026-01-08 13:26:26,350: t15.2023.10.01 val PER: 0.1902
2026-01-08 13:26:26,350: t15.2023.10.06 val PER: 0.1087
2026-01-08 13:26:26,350: t15.2023.10.08 val PER: 0.2409
2026-01-08 13:26:26,350: t15.2023.10.13 val PER: 0.2102
2026-01-08 13:26:26,351: t15.2023.10.15 val PER: 0.1721
2026-01-08 13:26:26,351: t15.2023.10.20 val PER: 0.1779
2026-01-08 13:26:26,351: t15.2023.10.22 val PER: 0.1203
2026-01-08 13:26:26,351: t15.2023.11.03 val PER: 0.1893
2026-01-08 13:26:26,351: t15.2023.11.04 val PER: 0.0410
2026-01-08 13:26:26,351: t15.2023.11.17 val PER: 0.0607
2026-01-08 13:26:26,351: t15.2023.11.19 val PER: 0.0399
2026-01-08 13:26:26,351: t15.2023.11.26 val PER: 0.1435
2026-01-08 13:26:26,351: t15.2023.12.03 val PER: 0.1239
2026-01-08 13:26:26,352: t15.2023.12.08 val PER: 0.1165
2026-01-08 13:26:26,352: t15.2023.12.10 val PER: 0.1025
2026-01-08 13:26:26,352: t15.2023.12.17 val PER: 0.1549
2026-01-08 13:26:26,352: t15.2023.12.29 val PER: 0.1462
2026-01-08 13:26:26,352: t15.2024.02.25 val PER: 0.1250
2026-01-08 13:26:26,352: t15.2024.03.08 val PER: 0.2205
2026-01-08 13:26:26,352: t15.2024.03.15 val PER: 0.2008
2026-01-08 13:26:26,352: t15.2024.03.17 val PER: 0.1618
2026-01-08 13:26:26,353: t15.2024.05.10 val PER: 0.1932
2026-01-08 13:26:26,353: t15.2024.06.14 val PER: 0.1877
2026-01-08 13:26:26,353: t15.2024.07.19 val PER: 0.2432
2026-01-08 13:26:26,353: t15.2024.07.21 val PER: 0.1207
2026-01-08 13:26:26,353: t15.2024.07.28 val PER: 0.1360
2026-01-08 13:26:26,353: t15.2025.01.10 val PER: 0.3058
2026-01-08 13:26:26,354: t15.2025.01.12 val PER: 0.1578
2026-01-08 13:26:26,354: t15.2025.03.14 val PER: 0.3358
2026-01-08 13:26:26,354: t15.2025.03.16 val PER: 0.2094
2026-01-08 13:26:26,354: t15.2025.03.30 val PER: 0.3034
2026-01-08 13:26:26,354: t15.2025.04.13 val PER: 0.2496
2026-01-08 13:26:35,701: Train batch 10600: loss: 8.94 grad norm: 60.21 time: 0.077
2026-01-08 13:26:53,085: Train batch 10800: loss: 13.26 grad norm: 74.16 time: 0.068
2026-01-08 13:27:10,325: Train batch 11000: loss: 13.17 grad norm: 71.19 time: 0.061
2026-01-08 13:27:10,326: Running test after training batch: 11000
2026-01-08 13:27:10,463: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:27:15,153: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e a cold at this otte as will
2026-01-08 13:27:15,185: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the ost et
2026-01-08 13:27:16,875: Val batch 11000: PER (avg): 0.1585 CTC Loss (avg): 16.3885 WER(1gram): 61.93% (n=64) time: 6.549
2026-01-08 13:27:16,876: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=12
2026-01-08 13:27:16,876: t15.2023.08.13 val PER: 0.1185
2026-01-08 13:27:16,876: t15.2023.08.18 val PER: 0.1115
2026-01-08 13:27:16,876: t15.2023.08.20 val PER: 0.1025
2026-01-08 13:27:16,876: t15.2023.08.25 val PER: 0.0873
2026-01-08 13:27:16,876: t15.2023.08.27 val PER: 0.1881
2026-01-08 13:27:16,876: t15.2023.09.01 val PER: 0.0787
2026-01-08 13:27:16,876: t15.2023.09.03 val PER: 0.1639
2026-01-08 13:27:16,876: t15.2023.09.24 val PER: 0.1323
2026-01-08 13:27:16,876: t15.2023.09.29 val PER: 0.1372
2026-01-08 13:27:16,876: t15.2023.10.01 val PER: 0.1816
2026-01-08 13:27:16,876: t15.2023.10.06 val PER: 0.0990
2026-01-08 13:27:16,877: t15.2023.10.08 val PER: 0.2503
2026-01-08 13:27:16,877: t15.2023.10.13 val PER: 0.2079
2026-01-08 13:27:16,877: t15.2023.10.15 val PER: 0.1734
2026-01-08 13:27:16,877: t15.2023.10.20 val PER: 0.1544
2026-01-08 13:27:16,877: t15.2023.10.22 val PER: 0.1214
2026-01-08 13:27:16,877: t15.2023.11.03 val PER: 0.1757
2026-01-08 13:27:16,877: t15.2023.11.04 val PER: 0.0341
2026-01-08 13:27:16,877: t15.2023.11.17 val PER: 0.0451
2026-01-08 13:27:16,877: t15.2023.11.19 val PER: 0.0419
2026-01-08 13:27:16,878: t15.2023.11.26 val PER: 0.1319
2026-01-08 13:27:16,878: t15.2023.12.03 val PER: 0.1176
2026-01-08 13:27:16,878: t15.2023.12.08 val PER: 0.1005
2026-01-08 13:27:16,878: t15.2023.12.10 val PER: 0.0986
2026-01-08 13:27:16,878: t15.2023.12.17 val PER: 0.1580
2026-01-08 13:27:16,878: t15.2023.12.29 val PER: 0.1359
2026-01-08 13:27:16,878: t15.2024.02.25 val PER: 0.1138
2026-01-08 13:27:16,878: t15.2024.03.08 val PER: 0.2376
2026-01-08 13:27:16,878: t15.2024.03.15 val PER: 0.2014
2026-01-08 13:27:16,878: t15.2024.03.17 val PER: 0.1695
2026-01-08 13:27:16,878: t15.2024.05.10 val PER: 0.1813
2026-01-08 13:27:16,878: t15.2024.06.14 val PER: 0.1735
2026-01-08 13:27:16,879: t15.2024.07.19 val PER: 0.2544
2026-01-08 13:27:16,879: t15.2024.07.21 val PER: 0.1138
2026-01-08 13:27:16,879: t15.2024.07.28 val PER: 0.1360
2026-01-08 13:27:16,879: t15.2025.01.10 val PER: 0.3113
2026-01-08 13:27:16,879: t15.2025.01.12 val PER: 0.1517
2026-01-08 13:27:16,879: t15.2025.03.14 val PER: 0.3432
2026-01-08 13:27:16,879: t15.2025.03.16 val PER: 0.2081
2026-01-08 13:27:16,879: t15.2025.03.30 val PER: 0.3023
2026-01-08 13:27:16,879: t15.2025.04.13 val PER: 0.2368
2026-01-08 13:27:34,231: Train batch 11200: loss: 11.89 grad norm: 72.20 time: 0.074
2026-01-08 13:27:52,377: Train batch 11400: loss: 8.39 grad norm: 54.86 time: 0.060
2026-01-08 13:28:01,243: Running test after training batch: 11500
2026-01-08 13:28:01,350: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:28:06,163: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e a cold at this otte as will
2026-01-08 13:28:06,196: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the quast ent
2026-01-08 13:28:07,899: Val batch 11500: PER (avg): 0.1553 CTC Loss (avg): 16.3807 WER(1gram): 61.93% (n=64) time: 6.655
2026-01-08 13:28:07,899: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=12
2026-01-08 13:28:07,899: t15.2023.08.13 val PER: 0.1289
2026-01-08 13:28:07,899: t15.2023.08.18 val PER: 0.1182
2026-01-08 13:28:07,899: t15.2023.08.20 val PER: 0.1041
2026-01-08 13:28:07,899: t15.2023.08.25 val PER: 0.0949
2026-01-08 13:28:07,899: t15.2023.08.27 val PER: 0.1817
2026-01-08 13:28:07,899: t15.2023.09.01 val PER: 0.0804
2026-01-08 13:28:07,900: t15.2023.09.03 val PER: 0.1686
2026-01-08 13:28:07,900: t15.2023.09.24 val PER: 0.1311
2026-01-08 13:28:07,900: t15.2023.09.29 val PER: 0.1347
2026-01-08 13:28:07,900: t15.2023.10.01 val PER: 0.1810
2026-01-08 13:28:07,900: t15.2023.10.06 val PER: 0.0936
2026-01-08 13:28:07,900: t15.2023.10.08 val PER: 0.2436
2026-01-08 13:28:07,900: t15.2023.10.13 val PER: 0.1947
2026-01-08 13:28:07,900: t15.2023.10.15 val PER: 0.1655
2026-01-08 13:28:07,900: t15.2023.10.20 val PER: 0.1544
2026-01-08 13:28:07,901: t15.2023.10.22 val PER: 0.1114
2026-01-08 13:28:07,901: t15.2023.11.03 val PER: 0.1805
2026-01-08 13:28:07,901: t15.2023.11.04 val PER: 0.0273
2026-01-08 13:28:07,901: t15.2023.11.17 val PER: 0.0498
2026-01-08 13:28:07,901: t15.2023.11.19 val PER: 0.0339
2026-01-08 13:28:07,901: t15.2023.11.26 val PER: 0.1188
2026-01-08 13:28:07,901: t15.2023.12.03 val PER: 0.1103
2026-01-08 13:28:07,901: t15.2023.12.08 val PER: 0.0979
2026-01-08 13:28:07,901: t15.2023.12.10 val PER: 0.0920
2026-01-08 13:28:07,901: t15.2023.12.17 val PER: 0.1403
2026-01-08 13:28:07,901: t15.2023.12.29 val PER: 0.1380
2026-01-08 13:28:07,901: t15.2024.02.25 val PER: 0.1194
2026-01-08 13:28:07,902: t15.2024.03.08 val PER: 0.2162
2026-01-08 13:28:07,902: t15.2024.03.15 val PER: 0.2070
2026-01-08 13:28:07,902: t15.2024.03.17 val PER: 0.1499
2026-01-08 13:28:07,902: t15.2024.05.10 val PER: 0.1961
2026-01-08 13:28:07,902: t15.2024.06.14 val PER: 0.1798
2026-01-08 13:28:07,902: t15.2024.07.19 val PER: 0.2393
2026-01-08 13:28:07,903: t15.2024.07.21 val PER: 0.1110
2026-01-08 13:28:07,903: t15.2024.07.28 val PER: 0.1426
2026-01-08 13:28:07,903: t15.2025.01.10 val PER: 0.3030
2026-01-08 13:28:07,903: t15.2025.01.12 val PER: 0.1594
2026-01-08 13:28:07,903: t15.2025.03.14 val PER: 0.3269
2026-01-08 13:28:07,903: t15.2025.03.16 val PER: 0.1950
2026-01-08 13:28:07,903: t15.2025.03.30 val PER: 0.3034
2026-01-08 13:28:07,903: t15.2025.04.13 val PER: 0.2197
2026-01-08 13:28:16,385: Train batch 11600: loss: 11.98 grad norm: 69.66 time: 0.064
2026-01-08 13:28:33,924: Train batch 11800: loss: 7.18 grad norm: 55.50 time: 0.047
2026-01-08 13:28:51,265: Train batch 12000: loss: 12.93 grad norm: 58.99 time: 0.075
2026-01-08 13:28:51,265: Running test after training batch: 12000
2026-01-08 13:28:51,362: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:28:56,261: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e a old at this otte as will
2026-01-08 13:28:56,293: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost ent
2026-01-08 13:28:58,032: Val batch 12000: PER (avg): 0.1549 CTC Loss (avg): 16.0434 WER(1gram): 59.90% (n=64) time: 6.767
2026-01-08 13:28:58,032: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=12
2026-01-08 13:28:58,033: t15.2023.08.13 val PER: 0.1279
2026-01-08 13:28:58,033: t15.2023.08.18 val PER: 0.1207
2026-01-08 13:28:58,033: t15.2023.08.20 val PER: 0.1112
2026-01-08 13:28:58,033: t15.2023.08.25 val PER: 0.0904
2026-01-08 13:28:58,033: t15.2023.08.27 val PER: 0.1801
2026-01-08 13:28:58,033: t15.2023.09.01 val PER: 0.0706
2026-01-08 13:28:58,033: t15.2023.09.03 val PER: 0.1532
2026-01-08 13:28:58,033: t15.2023.09.24 val PER: 0.1214
2026-01-08 13:28:58,033: t15.2023.09.29 val PER: 0.1398
2026-01-08 13:28:58,033: t15.2023.10.01 val PER: 0.1750
2026-01-08 13:28:58,033: t15.2023.10.06 val PER: 0.1076
2026-01-08 13:28:58,033: t15.2023.10.08 val PER: 0.2449
2026-01-08 13:28:58,033: t15.2023.10.13 val PER: 0.2009
2026-01-08 13:28:58,034: t15.2023.10.15 val PER: 0.1628
2026-01-08 13:28:58,034: t15.2023.10.20 val PER: 0.1879
2026-01-08 13:28:58,034: t15.2023.10.22 val PER: 0.1203
2026-01-08 13:28:58,034: t15.2023.11.03 val PER: 0.1730
2026-01-08 13:28:58,034: t15.2023.11.04 val PER: 0.0307
2026-01-08 13:28:58,034: t15.2023.11.17 val PER: 0.0435
2026-01-08 13:28:58,034: t15.2023.11.19 val PER: 0.0259
2026-01-08 13:28:58,034: t15.2023.11.26 val PER: 0.1138
2026-01-08 13:28:58,035: t15.2023.12.03 val PER: 0.1124
2026-01-08 13:28:58,035: t15.2023.12.08 val PER: 0.0999
2026-01-08 13:28:58,035: t15.2023.12.10 val PER: 0.0933
2026-01-08 13:28:58,035: t15.2023.12.17 val PER: 0.1455
2026-01-08 13:28:58,035: t15.2023.12.29 val PER: 0.1338
2026-01-08 13:28:58,035: t15.2024.02.25 val PER: 0.1138
2026-01-08 13:28:58,035: t15.2024.03.08 val PER: 0.2205
2026-01-08 13:28:58,035: t15.2024.03.15 val PER: 0.2008
2026-01-08 13:28:58,035: t15.2024.03.17 val PER: 0.1555
2026-01-08 13:28:58,035: t15.2024.05.10 val PER: 0.1828
2026-01-08 13:28:58,035: t15.2024.06.14 val PER: 0.1782
2026-01-08 13:28:58,035: t15.2024.07.19 val PER: 0.2531
2026-01-08 13:28:58,036: t15.2024.07.21 val PER: 0.1069
2026-01-08 13:28:58,036: t15.2024.07.28 val PER: 0.1441
2026-01-08 13:28:58,036: t15.2025.01.10 val PER: 0.3085
2026-01-08 13:28:58,036: t15.2025.01.12 val PER: 0.1532
2026-01-08 13:28:58,036: t15.2025.03.14 val PER: 0.3269
2026-01-08 13:28:58,036: t15.2025.03.16 val PER: 0.1872
2026-01-08 13:28:58,036: t15.2025.03.30 val PER: 0.3011
2026-01-08 13:28:58,036: t15.2025.04.13 val PER: 0.2211
2026-01-08 13:28:58,037: New best val WER(1gram) 61.17% --> 59.90%
2026-01-08 13:28:58,037: Checkpointing model
2026-01-08 13:28:58,172: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:29:15,357: Train batch 12200: loss: 5.70 grad norm: 53.30 time: 0.069
2026-01-08 13:29:32,368: Train batch 12400: loss: 4.47 grad norm: 42.29 time: 0.043
2026-01-08 13:29:41,181: Running test after training batch: 12500
2026-01-08 13:29:41,323: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:29:46,033: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e the cold at this otte as will
2026-01-08 13:29:46,065: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the quast et
2026-01-08 13:29:47,833: Val batch 12500: PER (avg): 0.1523 CTC Loss (avg): 16.0554 WER(1gram): 57.87% (n=64) time: 6.652
2026-01-08 13:29:47,833: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=12
2026-01-08 13:29:47,834: t15.2023.08.13 val PER: 0.1185
2026-01-08 13:29:47,834: t15.2023.08.18 val PER: 0.1148
2026-01-08 13:29:47,834: t15.2023.08.20 val PER: 0.1041
2026-01-08 13:29:47,834: t15.2023.08.25 val PER: 0.0843
2026-01-08 13:29:47,834: t15.2023.08.27 val PER: 0.1817
2026-01-08 13:29:47,834: t15.2023.09.01 val PER: 0.0722
2026-01-08 13:29:47,834: t15.2023.09.03 val PER: 0.1580
2026-01-08 13:29:47,834: t15.2023.09.24 val PER: 0.1286
2026-01-08 13:29:47,834: t15.2023.09.29 val PER: 0.1385
2026-01-08 13:29:47,834: t15.2023.10.01 val PER: 0.1671
2026-01-08 13:29:47,834: t15.2023.10.06 val PER: 0.0980
2026-01-08 13:29:47,834: t15.2023.10.08 val PER: 0.2530
2026-01-08 13:29:47,835: t15.2023.10.13 val PER: 0.2056
2026-01-08 13:29:47,835: t15.2023.10.15 val PER: 0.1582
2026-01-08 13:29:47,835: t15.2023.10.20 val PER: 0.1644
2026-01-08 13:29:47,835: t15.2023.10.22 val PER: 0.1036
2026-01-08 13:29:47,835: t15.2023.11.03 val PER: 0.1750
2026-01-08 13:29:47,835: t15.2023.11.04 val PER: 0.0239
2026-01-08 13:29:47,835: t15.2023.11.17 val PER: 0.0451
2026-01-08 13:29:47,835: t15.2023.11.19 val PER: 0.0279
2026-01-08 13:29:47,835: t15.2023.11.26 val PER: 0.1239
2026-01-08 13:29:47,835: t15.2023.12.03 val PER: 0.1166
2026-01-08 13:29:47,835: t15.2023.12.08 val PER: 0.0912
2026-01-08 13:29:47,835: t15.2023.12.10 val PER: 0.0946
2026-01-08 13:29:47,836: t15.2023.12.17 val PER: 0.1320
2026-01-08 13:29:47,836: t15.2023.12.29 val PER: 0.1386
2026-01-08 13:29:47,836: t15.2024.02.25 val PER: 0.1194
2026-01-08 13:29:47,836: t15.2024.03.08 val PER: 0.2191
2026-01-08 13:29:47,836: t15.2024.03.15 val PER: 0.2008
2026-01-08 13:29:47,836: t15.2024.03.17 val PER: 0.1450
2026-01-08 13:29:47,836: t15.2024.05.10 val PER: 0.1857
2026-01-08 13:29:47,836: t15.2024.06.14 val PER: 0.1703
2026-01-08 13:29:47,836: t15.2024.07.19 val PER: 0.2459
2026-01-08 13:29:47,836: t15.2024.07.21 val PER: 0.1021
2026-01-08 13:29:47,836: t15.2024.07.28 val PER: 0.1368
2026-01-08 13:29:47,836: t15.2025.01.10 val PER: 0.2961
2026-01-08 13:29:47,836: t15.2025.01.12 val PER: 0.1517
2026-01-08 13:29:47,836: t15.2025.03.14 val PER: 0.3299
2026-01-08 13:29:47,836: t15.2025.03.16 val PER: 0.1950
2026-01-08 13:29:47,836: t15.2025.03.30 val PER: 0.2977
2026-01-08 13:29:47,837: t15.2025.04.13 val PER: 0.2183
2026-01-08 13:29:47,838: New best val WER(1gram) 59.90% --> 57.87%
2026-01-08 13:29:47,838: Checkpointing model
2026-01-08 13:29:47,973: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:29:56,476: Train batch 12600: loss: 8.52 grad norm: 58.94 time: 0.062
2026-01-08 13:30:13,799: Train batch 12800: loss: 4.68 grad norm: 47.09 time: 0.056
2026-01-08 13:30:31,508: Train batch 13000: loss: 5.20 grad norm: 50.31 time: 0.070
2026-01-08 13:30:31,509: Running test after training batch: 13000
2026-01-08 13:30:31,602: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:30:36,324: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e a cold at this arndt as will
2026-01-08 13:30:36,356: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the quast ent
2026-01-08 13:30:38,147: Val batch 13000: PER (avg): 0.1493 CTC Loss (avg): 15.7083 WER(1gram): 60.91% (n=64) time: 6.638
2026-01-08 13:30:38,147: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-08 13:30:38,147: t15.2023.08.13 val PER: 0.1175
2026-01-08 13:30:38,148: t15.2023.08.18 val PER: 0.1081
2026-01-08 13:30:38,148: t15.2023.08.20 val PER: 0.1112
2026-01-08 13:30:38,148: t15.2023.08.25 val PER: 0.0949
2026-01-08 13:30:38,148: t15.2023.08.27 val PER: 0.1849
2026-01-08 13:30:38,148: t15.2023.09.01 val PER: 0.0698
2026-01-08 13:30:38,148: t15.2023.09.03 val PER: 0.1520
2026-01-08 13:30:38,148: t15.2023.09.24 val PER: 0.1274
2026-01-08 13:30:38,148: t15.2023.09.29 val PER: 0.1321
2026-01-08 13:30:38,148: t15.2023.10.01 val PER: 0.1684
2026-01-08 13:30:38,149: t15.2023.10.06 val PER: 0.0915
2026-01-08 13:30:38,149: t15.2023.10.08 val PER: 0.2368
2026-01-08 13:30:38,149: t15.2023.10.13 val PER: 0.2102
2026-01-08 13:30:38,149: t15.2023.10.15 val PER: 0.1562
2026-01-08 13:30:38,149: t15.2023.10.20 val PER: 0.1678
2026-01-08 13:30:38,149: t15.2023.10.22 val PER: 0.1069
2026-01-08 13:30:38,149: t15.2023.11.03 val PER: 0.1723
2026-01-08 13:30:38,149: t15.2023.11.04 val PER: 0.0307
2026-01-08 13:30:38,149: t15.2023.11.17 val PER: 0.0435
2026-01-08 13:30:38,149: t15.2023.11.19 val PER: 0.0299
2026-01-08 13:30:38,149: t15.2023.11.26 val PER: 0.1203
2026-01-08 13:30:38,149: t15.2023.12.03 val PER: 0.1124
2026-01-08 13:30:38,149: t15.2023.12.08 val PER: 0.0892
2026-01-08 13:30:38,149: t15.2023.12.10 val PER: 0.0894
2026-01-08 13:30:38,150: t15.2023.12.17 val PER: 0.1528
2026-01-08 13:30:38,150: t15.2023.12.29 val PER: 0.1311
2026-01-08 13:30:38,150: t15.2024.02.25 val PER: 0.1138
2026-01-08 13:30:38,150: t15.2024.03.08 val PER: 0.2105
2026-01-08 13:30:38,150: t15.2024.03.15 val PER: 0.1995
2026-01-08 13:30:38,150: t15.2024.03.17 val PER: 0.1464
2026-01-08 13:30:38,150: t15.2024.05.10 val PER: 0.1783
2026-01-08 13:30:38,150: t15.2024.06.14 val PER: 0.1672
2026-01-08 13:30:38,150: t15.2024.07.19 val PER: 0.2406
2026-01-08 13:30:38,150: t15.2024.07.21 val PER: 0.0931
2026-01-08 13:30:38,150: t15.2024.07.28 val PER: 0.1324
2026-01-08 13:30:38,150: t15.2025.01.10 val PER: 0.2906
2026-01-08 13:30:38,150: t15.2025.01.12 val PER: 0.1432
2026-01-08 13:30:38,150: t15.2025.03.14 val PER: 0.3180
2026-01-08 13:30:38,150: t15.2025.03.16 val PER: 0.1741
2026-01-08 13:30:38,151: t15.2025.03.30 val PER: 0.2828
2026-01-08 13:30:38,151: t15.2025.04.13 val PER: 0.2168
2026-01-08 13:30:55,622: Train batch 13200: loss: 13.50 grad norm: 77.12 time: 0.056
2026-01-08 13:31:12,759: Train batch 13400: loss: 10.69 grad norm: 77.31 time: 0.066
2026-01-08 13:31:22,167: Running test after training batch: 13500
2026-01-08 13:31:22,300: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:31:26,974: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e a cold at this arndt as will
2026-01-08 13:31:27,005: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the ost ent
2026-01-08 13:31:28,802: Val batch 13500: PER (avg): 0.1490 CTC Loss (avg): 15.6698 WER(1gram): 59.90% (n=64) time: 6.635
2026-01-08 13:31:28,803: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-08 13:31:28,803: t15.2023.08.13 val PER: 0.1175
2026-01-08 13:31:28,803: t15.2023.08.18 val PER: 0.1065
2026-01-08 13:31:28,803: t15.2023.08.20 val PER: 0.1025
2026-01-08 13:31:28,803: t15.2023.08.25 val PER: 0.0873
2026-01-08 13:31:28,803: t15.2023.08.27 val PER: 0.1768
2026-01-08 13:31:28,803: t15.2023.09.01 val PER: 0.0714
2026-01-08 13:31:28,803: t15.2023.09.03 val PER: 0.1591
2026-01-08 13:31:28,803: t15.2023.09.24 val PER: 0.1286
2026-01-08 13:31:28,804: t15.2023.09.29 val PER: 0.1340
2026-01-08 13:31:28,804: t15.2023.10.01 val PER: 0.1711
2026-01-08 13:31:28,804: t15.2023.10.06 val PER: 0.1044
2026-01-08 13:31:28,804: t15.2023.10.08 val PER: 0.2490
2026-01-08 13:31:28,804: t15.2023.10.13 val PER: 0.2033
2026-01-08 13:31:28,804: t15.2023.10.15 val PER: 0.1569
2026-01-08 13:31:28,804: t15.2023.10.20 val PER: 0.1544
2026-01-08 13:31:28,804: t15.2023.10.22 val PER: 0.1047
2026-01-08 13:31:28,804: t15.2023.11.03 val PER: 0.1716
2026-01-08 13:31:28,804: t15.2023.11.04 val PER: 0.0273
2026-01-08 13:31:28,804: t15.2023.11.17 val PER: 0.0482
2026-01-08 13:31:28,804: t15.2023.11.19 val PER: 0.0220
2026-01-08 13:31:28,804: t15.2023.11.26 val PER: 0.1203
2026-01-08 13:31:28,804: t15.2023.12.03 val PER: 0.1008
2026-01-08 13:31:28,804: t15.2023.12.08 val PER: 0.0952
2026-01-08 13:31:28,805: t15.2023.12.10 val PER: 0.0841
2026-01-08 13:31:28,805: t15.2023.12.17 val PER: 0.1362
2026-01-08 13:31:28,805: t15.2023.12.29 val PER: 0.1283
2026-01-08 13:31:28,805: t15.2024.02.25 val PER: 0.1166
2026-01-08 13:31:28,805: t15.2024.03.08 val PER: 0.2162
2026-01-08 13:31:28,805: t15.2024.03.15 val PER: 0.1901
2026-01-08 13:31:28,805: t15.2024.03.17 val PER: 0.1423
2026-01-08 13:31:28,805: t15.2024.05.10 val PER: 0.1842
2026-01-08 13:31:28,805: t15.2024.06.14 val PER: 0.1767
2026-01-08 13:31:28,805: t15.2024.07.19 val PER: 0.2340
2026-01-08 13:31:28,805: t15.2024.07.21 val PER: 0.1007
2026-01-08 13:31:28,806: t15.2024.07.28 val PER: 0.1331
2026-01-08 13:31:28,806: t15.2025.01.10 val PER: 0.2769
2026-01-08 13:31:28,806: t15.2025.01.12 val PER: 0.1463
2026-01-08 13:31:28,806: t15.2025.03.14 val PER: 0.3373
2026-01-08 13:31:28,806: t15.2025.03.16 val PER: 0.1819
2026-01-08 13:31:28,806: t15.2025.03.30 val PER: 0.2908
2026-01-08 13:31:28,806: t15.2025.04.13 val PER: 0.2126
2026-01-08 13:31:37,736: Train batch 13600: loss: 9.97 grad norm: 65.27 time: 0.066
2026-01-08 13:31:55,630: Train batch 13800: loss: 7.75 grad norm: 58.28 time: 0.059
2026-01-08 13:32:13,307: Train batch 14000: loss: 11.56 grad norm: 67.96 time: 0.054
2026-01-08 13:32:13,307: Running test after training batch: 14000
2026-01-08 13:32:13,414: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:32:18,142: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e a cold at this arndt as will
2026-01-08 13:32:18,174: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the ost ent
2026-01-08 13:32:19,954: Val batch 14000: PER (avg): 0.1477 CTC Loss (avg): 15.6175 WER(1gram): 59.64% (n=64) time: 6.647
2026-01-08 13:32:19,955: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=12
2026-01-08 13:32:19,955: t15.2023.08.13 val PER: 0.1123
2026-01-08 13:32:19,955: t15.2023.08.18 val PER: 0.1073
2026-01-08 13:32:19,955: t15.2023.08.20 val PER: 0.1041
2026-01-08 13:32:19,955: t15.2023.08.25 val PER: 0.0889
2026-01-08 13:32:19,955: t15.2023.08.27 val PER: 0.1785
2026-01-08 13:32:19,955: t15.2023.09.01 val PER: 0.0657
2026-01-08 13:32:19,955: t15.2023.09.03 val PER: 0.1508
2026-01-08 13:32:19,955: t15.2023.09.24 val PER: 0.1189
2026-01-08 13:32:19,955: t15.2023.09.29 val PER: 0.1276
2026-01-08 13:32:19,955: t15.2023.10.01 val PER: 0.1678
2026-01-08 13:32:19,955: t15.2023.10.06 val PER: 0.0926
2026-01-08 13:32:19,956: t15.2023.10.08 val PER: 0.2490
2026-01-08 13:32:19,956: t15.2023.10.13 val PER: 0.2017
2026-01-08 13:32:19,956: t15.2023.10.15 val PER: 0.1543
2026-01-08 13:32:19,956: t15.2023.10.20 val PER: 0.1711
2026-01-08 13:32:19,956: t15.2023.10.22 val PER: 0.0958
2026-01-08 13:32:19,956: t15.2023.11.03 val PER: 0.1750
2026-01-08 13:32:19,956: t15.2023.11.04 val PER: 0.0239
2026-01-08 13:32:19,956: t15.2023.11.17 val PER: 0.0467
2026-01-08 13:32:19,956: t15.2023.11.19 val PER: 0.0259
2026-01-08 13:32:19,956: t15.2023.11.26 val PER: 0.1058
2026-01-08 13:32:19,956: t15.2023.12.03 val PER: 0.1176
2026-01-08 13:32:19,956: t15.2023.12.08 val PER: 0.0892
2026-01-08 13:32:19,956: t15.2023.12.10 val PER: 0.0880
2026-01-08 13:32:19,956: t15.2023.12.17 val PER: 0.1414
2026-01-08 13:32:19,956: t15.2023.12.29 val PER: 0.1277
2026-01-08 13:32:19,956: t15.2024.02.25 val PER: 0.1096
2026-01-08 13:32:19,957: t15.2024.03.08 val PER: 0.2091
2026-01-08 13:32:19,957: t15.2024.03.15 val PER: 0.1951
2026-01-08 13:32:19,957: t15.2024.03.17 val PER: 0.1423
2026-01-08 13:32:19,957: t15.2024.05.10 val PER: 0.1842
2026-01-08 13:32:19,957: t15.2024.06.14 val PER: 0.1625
2026-01-08 13:32:19,957: t15.2024.07.19 val PER: 0.2432
2026-01-08 13:32:19,957: t15.2024.07.21 val PER: 0.0959
2026-01-08 13:32:19,957: t15.2024.07.28 val PER: 0.1301
2026-01-08 13:32:19,957: t15.2025.01.10 val PER: 0.2975
2026-01-08 13:32:19,957: t15.2025.01.12 val PER: 0.1440
2026-01-08 13:32:19,957: t15.2025.03.14 val PER: 0.3254
2026-01-08 13:32:19,957: t15.2025.03.16 val PER: 0.1806
2026-01-08 13:32:19,957: t15.2025.03.30 val PER: 0.2943
2026-01-08 13:32:19,957: t15.2025.04.13 val PER: 0.2254
2026-01-08 13:32:37,034: Train batch 14200: loss: 8.00 grad norm: 59.40 time: 0.060
2026-01-08 13:32:55,400: Train batch 14400: loss: 5.84 grad norm: 48.79 time: 0.068
2026-01-08 13:33:04,325: Running test after training batch: 14500
2026-01-08 13:33:04,417: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:33:09,290: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e a cold at this arndt as will
2026-01-08 13:33:09,324: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost ent
2026-01-08 13:33:11,144: Val batch 14500: PER (avg): 0.1461 CTC Loss (avg): 15.5392 WER(1gram): 57.36% (n=64) time: 6.818
2026-01-08 13:33:11,145: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-08 13:33:11,145: t15.2023.08.13 val PER: 0.1154
2026-01-08 13:33:11,145: t15.2023.08.18 val PER: 0.1065
2026-01-08 13:33:11,145: t15.2023.08.20 val PER: 0.1096
2026-01-08 13:33:11,145: t15.2023.08.25 val PER: 0.0889
2026-01-08 13:33:11,145: t15.2023.08.27 val PER: 0.1704
2026-01-08 13:33:11,145: t15.2023.09.01 val PER: 0.0682
2026-01-08 13:33:11,145: t15.2023.09.03 val PER: 0.1532
2026-01-08 13:33:11,145: t15.2023.09.24 val PER: 0.1153
2026-01-08 13:33:11,145: t15.2023.09.29 val PER: 0.1327
2026-01-08 13:33:11,145: t15.2023.10.01 val PER: 0.1731
2026-01-08 13:33:11,145: t15.2023.10.06 val PER: 0.0969
2026-01-08 13:33:11,145: t15.2023.10.08 val PER: 0.2368
2026-01-08 13:33:11,146: t15.2023.10.13 val PER: 0.1877
2026-01-08 13:33:11,146: t15.2023.10.15 val PER: 0.1536
2026-01-08 13:33:11,146: t15.2023.10.20 val PER: 0.1711
2026-01-08 13:33:11,146: t15.2023.10.22 val PER: 0.1102
2026-01-08 13:33:11,146: t15.2023.11.03 val PER: 0.1710
2026-01-08 13:33:11,146: t15.2023.11.04 val PER: 0.0375
2026-01-08 13:33:11,146: t15.2023.11.17 val PER: 0.0420
2026-01-08 13:33:11,146: t15.2023.11.19 val PER: 0.0319
2026-01-08 13:33:11,146: t15.2023.11.26 val PER: 0.1080
2026-01-08 13:33:11,146: t15.2023.12.03 val PER: 0.1113
2026-01-08 13:33:11,146: t15.2023.12.08 val PER: 0.0852
2026-01-08 13:33:11,146: t15.2023.12.10 val PER: 0.0880
2026-01-08 13:33:11,146: t15.2023.12.17 val PER: 0.1268
2026-01-08 13:33:11,147: t15.2023.12.29 val PER: 0.1277
2026-01-08 13:33:11,147: t15.2024.02.25 val PER: 0.1067
2026-01-08 13:33:11,147: t15.2024.03.08 val PER: 0.2119
2026-01-08 13:33:11,147: t15.2024.03.15 val PER: 0.1920
2026-01-08 13:33:11,147: t15.2024.03.17 val PER: 0.1471
2026-01-08 13:33:11,147: t15.2024.05.10 val PER: 0.1842
2026-01-08 13:33:11,147: t15.2024.06.14 val PER: 0.1688
2026-01-08 13:33:11,147: t15.2024.07.19 val PER: 0.2334
2026-01-08 13:33:11,147: t15.2024.07.21 val PER: 0.0966
2026-01-08 13:33:11,147: t15.2024.07.28 val PER: 0.1235
2026-01-08 13:33:11,147: t15.2025.01.10 val PER: 0.2851
2026-01-08 13:33:11,147: t15.2025.01.12 val PER: 0.1339
2026-01-08 13:33:11,147: t15.2025.03.14 val PER: 0.3299
2026-01-08 13:33:11,147: t15.2025.03.16 val PER: 0.1767
2026-01-08 13:33:11,148: t15.2025.03.30 val PER: 0.2897
2026-01-08 13:33:11,148: t15.2025.04.13 val PER: 0.2126
2026-01-08 13:33:11,149: New best val WER(1gram) 57.87% --> 57.36%
2026-01-08 13:33:11,149: Checkpointing model
2026-01-08 13:33:11,287: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:33:19,929: Train batch 14600: loss: 11.60 grad norm: 67.58 time: 0.062
2026-01-08 13:33:37,267: Train batch 14800: loss: 5.80 grad norm: 51.71 time: 0.053
2026-01-08 13:33:54,901: Train batch 15000: loss: 8.44 grad norm: 59.61 time: 0.055
2026-01-08 13:33:54,901: Running test after training batch: 15000
2026-01-08 13:33:55,000: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:33:59,717: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e a old at this arndt as will
2026-01-08 13:33:59,750: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the ost ent
2026-01-08 13:34:01,596: Val batch 15000: PER (avg): 0.1458 CTC Loss (avg): 15.3144 WER(1gram): 61.42% (n=64) time: 6.694
2026-01-08 13:34:01,596: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-08 13:34:01,596: t15.2023.08.13 val PER: 0.1206
2026-01-08 13:34:01,596: t15.2023.08.18 val PER: 0.1031
2026-01-08 13:34:01,597: t15.2023.08.20 val PER: 0.1056
2026-01-08 13:34:01,597: t15.2023.08.25 val PER: 0.0889
2026-01-08 13:34:01,597: t15.2023.08.27 val PER: 0.1720
2026-01-08 13:34:01,597: t15.2023.09.01 val PER: 0.0706
2026-01-08 13:34:01,597: t15.2023.09.03 val PER: 0.1485
2026-01-08 13:34:01,597: t15.2023.09.24 val PER: 0.1177
2026-01-08 13:34:01,597: t15.2023.09.29 val PER: 0.1308
2026-01-08 13:34:01,597: t15.2023.10.01 val PER: 0.1750
2026-01-08 13:34:01,598: t15.2023.10.06 val PER: 0.0883
2026-01-08 13:34:01,598: t15.2023.10.08 val PER: 0.2382
2026-01-08 13:34:01,598: t15.2023.10.13 val PER: 0.1924
2026-01-08 13:34:01,598: t15.2023.10.15 val PER: 0.1496
2026-01-08 13:34:01,598: t15.2023.10.20 val PER: 0.1577
2026-01-08 13:34:01,598: t15.2023.10.22 val PER: 0.1047
2026-01-08 13:34:01,598: t15.2023.11.03 val PER: 0.1669
2026-01-08 13:34:01,598: t15.2023.11.04 val PER: 0.0273
2026-01-08 13:34:01,598: t15.2023.11.17 val PER: 0.0389
2026-01-08 13:34:01,598: t15.2023.11.19 val PER: 0.0240
2026-01-08 13:34:01,598: t15.2023.11.26 val PER: 0.1109
2026-01-08 13:34:01,598: t15.2023.12.03 val PER: 0.0935
2026-01-08 13:34:01,598: t15.2023.12.08 val PER: 0.0872
2026-01-08 13:34:01,599: t15.2023.12.10 val PER: 0.0867
2026-01-08 13:34:01,599: t15.2023.12.17 val PER: 0.1331
2026-01-08 13:34:01,599: t15.2023.12.29 val PER: 0.1290
2026-01-08 13:34:01,599: t15.2024.02.25 val PER: 0.1025
2026-01-08 13:34:01,599: t15.2024.03.08 val PER: 0.2219
2026-01-08 13:34:01,599: t15.2024.03.15 val PER: 0.1914
2026-01-08 13:34:01,599: t15.2024.03.17 val PER: 0.1430
2026-01-08 13:34:01,599: t15.2024.05.10 val PER: 0.1872
2026-01-08 13:34:01,599: t15.2024.06.14 val PER: 0.1814
2026-01-08 13:34:01,599: t15.2024.07.19 val PER: 0.2320
2026-01-08 13:34:01,599: t15.2024.07.21 val PER: 0.0959
2026-01-08 13:34:01,599: t15.2024.07.28 val PER: 0.1279
2026-01-08 13:34:01,599: t15.2025.01.10 val PER: 0.2975
2026-01-08 13:34:01,599: t15.2025.01.12 val PER: 0.1378
2026-01-08 13:34:01,599: t15.2025.03.14 val PER: 0.3269
2026-01-08 13:34:01,599: t15.2025.03.16 val PER: 0.1688
2026-01-08 13:34:01,599: t15.2025.03.30 val PER: 0.2908
2026-01-08 13:34:01,600: t15.2025.04.13 val PER: 0.2211
2026-01-08 13:34:19,598: Train batch 15200: loss: 5.50 grad norm: 52.22 time: 0.061
2026-01-08 13:34:37,452: Train batch 15400: loss: 12.16 grad norm: 69.27 time: 0.052
2026-01-08 13:34:46,417: Running test after training batch: 15500
2026-01-08 13:34:46,521: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:34:51,552: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e a cold at this arndt as will
2026-01-08 13:34:51,585: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the ost ent
2026-01-08 13:34:53,383: Val batch 15500: PER (avg): 0.1438 CTC Loss (avg): 15.2236 WER(1gram): 59.39% (n=64) time: 6.966
2026-01-08 13:34:53,383: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-08 13:34:53,384: t15.2023.08.13 val PER: 0.1112
2026-01-08 13:34:53,384: t15.2023.08.18 val PER: 0.1065
2026-01-08 13:34:53,384: t15.2023.08.20 val PER: 0.1048
2026-01-08 13:34:53,384: t15.2023.08.25 val PER: 0.0873
2026-01-08 13:34:53,384: t15.2023.08.27 val PER: 0.1752
2026-01-08 13:34:53,384: t15.2023.09.01 val PER: 0.0657
2026-01-08 13:34:53,384: t15.2023.09.03 val PER: 0.1580
2026-01-08 13:34:53,385: t15.2023.09.24 val PER: 0.1177
2026-01-08 13:34:53,385: t15.2023.09.29 val PER: 0.1302
2026-01-08 13:34:53,385: t15.2023.10.01 val PER: 0.1803
2026-01-08 13:34:53,385: t15.2023.10.06 val PER: 0.0904
2026-01-08 13:34:53,385: t15.2023.10.08 val PER: 0.2314
2026-01-08 13:34:53,385: t15.2023.10.13 val PER: 0.1901
2026-01-08 13:34:53,385: t15.2023.10.15 val PER: 0.1556
2026-01-08 13:34:53,385: t15.2023.10.20 val PER: 0.1409
2026-01-08 13:34:53,385: t15.2023.10.22 val PER: 0.1013
2026-01-08 13:34:53,385: t15.2023.11.03 val PER: 0.1615
2026-01-08 13:34:53,385: t15.2023.11.04 val PER: 0.0273
2026-01-08 13:34:53,385: t15.2023.11.17 val PER: 0.0404
2026-01-08 13:34:53,385: t15.2023.11.19 val PER: 0.0240
2026-01-08 13:34:53,386: t15.2023.11.26 val PER: 0.1036
2026-01-08 13:34:53,386: t15.2023.12.03 val PER: 0.0903
2026-01-08 13:34:53,386: t15.2023.12.08 val PER: 0.0839
2026-01-08 13:34:53,386: t15.2023.12.10 val PER: 0.0828
2026-01-08 13:34:53,386: t15.2023.12.17 val PER: 0.1351
2026-01-08 13:34:53,386: t15.2023.12.29 val PER: 0.1201
2026-01-08 13:34:53,386: t15.2024.02.25 val PER: 0.1067
2026-01-08 13:34:53,386: t15.2024.03.08 val PER: 0.2048
2026-01-08 13:34:53,386: t15.2024.03.15 val PER: 0.1920
2026-01-08 13:34:53,386: t15.2024.03.17 val PER: 0.1388
2026-01-08 13:34:53,386: t15.2024.05.10 val PER: 0.1798
2026-01-08 13:34:53,386: t15.2024.06.14 val PER: 0.1719
2026-01-08 13:34:53,386: t15.2024.07.19 val PER: 0.2307
2026-01-08 13:34:53,386: t15.2024.07.21 val PER: 0.0945
2026-01-08 13:34:53,386: t15.2024.07.28 val PER: 0.1353
2026-01-08 13:34:53,386: t15.2025.01.10 val PER: 0.2810
2026-01-08 13:34:53,387: t15.2025.01.12 val PER: 0.1347
2026-01-08 13:34:53,387: t15.2025.03.14 val PER: 0.3210
2026-01-08 13:34:53,387: t15.2025.03.16 val PER: 0.1649
2026-01-08 13:34:53,387: t15.2025.03.30 val PER: 0.2897
2026-01-08 13:34:53,387: t15.2025.04.13 val PER: 0.2225
2026-01-08 13:35:02,159: Train batch 15600: loss: 10.13 grad norm: 67.30 time: 0.065
2026-01-08 13:35:19,729: Train batch 15800: loss: 13.14 grad norm: 78.19 time: 0.070
2026-01-08 13:35:37,822: Train batch 16000: loss: 7.39 grad norm: 52.04 time: 0.058
2026-01-08 13:35:37,822: Running test after training batch: 16000
2026-01-08 13:35:37,960: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:35:42,686: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e the cold at this arndt as will
2026-01-08 13:35:42,718: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost ent
2026-01-08 13:35:44,575: Val batch 16000: PER (avg): 0.1435 CTC Loss (avg): 15.2992 WER(1gram): 58.88% (n=64) time: 6.753
2026-01-08 13:35:44,576: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-08 13:35:44,576: t15.2023.08.13 val PER: 0.1050
2026-01-08 13:35:44,576: t15.2023.08.18 val PER: 0.1048
2026-01-08 13:35:44,576: t15.2023.08.20 val PER: 0.0993
2026-01-08 13:35:44,576: t15.2023.08.25 val PER: 0.0843
2026-01-08 13:35:44,577: t15.2023.08.27 val PER: 0.1688
2026-01-08 13:35:44,577: t15.2023.09.01 val PER: 0.0649
2026-01-08 13:35:44,577: t15.2023.09.03 val PER: 0.1556
2026-01-08 13:35:44,577: t15.2023.09.24 val PER: 0.1177
2026-01-08 13:35:44,577: t15.2023.09.29 val PER: 0.1264
2026-01-08 13:35:44,577: t15.2023.10.01 val PER: 0.1750
2026-01-08 13:35:44,577: t15.2023.10.06 val PER: 0.0958
2026-01-08 13:35:44,577: t15.2023.10.08 val PER: 0.2436
2026-01-08 13:35:44,577: t15.2023.10.13 val PER: 0.1932
2026-01-08 13:35:44,577: t15.2023.10.15 val PER: 0.1523
2026-01-08 13:35:44,577: t15.2023.10.20 val PER: 0.1510
2026-01-08 13:35:44,577: t15.2023.10.22 val PER: 0.0947
2026-01-08 13:35:44,577: t15.2023.11.03 val PER: 0.1676
2026-01-08 13:35:44,578: t15.2023.11.04 val PER: 0.0273
2026-01-08 13:35:44,578: t15.2023.11.17 val PER: 0.0373
2026-01-08 13:35:44,578: t15.2023.11.19 val PER: 0.0220
2026-01-08 13:35:44,578: t15.2023.11.26 val PER: 0.1058
2026-01-08 13:35:44,578: t15.2023.12.03 val PER: 0.0914
2026-01-08 13:35:44,578: t15.2023.12.08 val PER: 0.0839
2026-01-08 13:35:44,578: t15.2023.12.10 val PER: 0.0802
2026-01-08 13:35:44,578: t15.2023.12.17 val PER: 0.1341
2026-01-08 13:35:44,578: t15.2023.12.29 val PER: 0.1194
2026-01-08 13:35:44,578: t15.2024.02.25 val PER: 0.1025
2026-01-08 13:35:44,578: t15.2024.03.08 val PER: 0.2091
2026-01-08 13:35:44,578: t15.2024.03.15 val PER: 0.1870
2026-01-08 13:35:44,578: t15.2024.03.17 val PER: 0.1437
2026-01-08 13:35:44,579: t15.2024.05.10 val PER: 0.1798
2026-01-08 13:35:44,579: t15.2024.06.14 val PER: 0.1703
2026-01-08 13:35:44,579: t15.2024.07.19 val PER: 0.2307
2026-01-08 13:35:44,579: t15.2024.07.21 val PER: 0.0979
2026-01-08 13:35:44,579: t15.2024.07.28 val PER: 0.1265
2026-01-08 13:35:44,579: t15.2025.01.10 val PER: 0.3017
2026-01-08 13:35:44,579: t15.2025.01.12 val PER: 0.1339
2026-01-08 13:35:44,579: t15.2025.03.14 val PER: 0.3225
2026-01-08 13:35:44,579: t15.2025.03.16 val PER: 0.1715
2026-01-08 13:35:44,579: t15.2025.03.30 val PER: 0.2885
2026-01-08 13:35:44,579: t15.2025.04.13 val PER: 0.2211
2026-01-08 13:36:02,777: Train batch 16200: loss: 5.70 grad norm: 47.92 time: 0.058
2026-01-08 13:36:20,740: Train batch 16400: loss: 10.24 grad norm: 68.17 time: 0.059
2026-01-08 13:36:29,521: Running test after training batch: 16500
2026-01-08 13:36:29,613: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:36:34,347: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e the cold at this otte as will
2026-01-08 13:36:34,381: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the ost ent
2026-01-08 13:36:36,309: Val batch 16500: PER (avg): 0.1428 CTC Loss (avg): 15.1765 WER(1gram): 58.88% (n=64) time: 6.788
2026-01-08 13:36:36,310: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-08 13:36:36,310: t15.2023.08.13 val PER: 0.1081
2026-01-08 13:36:36,310: t15.2023.08.18 val PER: 0.0989
2026-01-08 13:36:36,310: t15.2023.08.20 val PER: 0.1001
2026-01-08 13:36:36,310: t15.2023.08.25 val PER: 0.0813
2026-01-08 13:36:36,310: t15.2023.08.27 val PER: 0.1785
2026-01-08 13:36:36,310: t15.2023.09.01 val PER: 0.0714
2026-01-08 13:36:36,310: t15.2023.09.03 val PER: 0.1568
2026-01-08 13:36:36,311: t15.2023.09.24 val PER: 0.1153
2026-01-08 13:36:36,311: t15.2023.09.29 val PER: 0.1244
2026-01-08 13:36:36,311: t15.2023.10.01 val PER: 0.1678
2026-01-08 13:36:36,311: t15.2023.10.06 val PER: 0.0926
2026-01-08 13:36:36,311: t15.2023.10.08 val PER: 0.2436
2026-01-08 13:36:36,311: t15.2023.10.13 val PER: 0.1870
2026-01-08 13:36:36,311: t15.2023.10.15 val PER: 0.1510
2026-01-08 13:36:36,311: t15.2023.10.20 val PER: 0.1577
2026-01-08 13:36:36,312: t15.2023.10.22 val PER: 0.0958
2026-01-08 13:36:36,312: t15.2023.11.03 val PER: 0.1716
2026-01-08 13:36:36,312: t15.2023.11.04 val PER: 0.0239
2026-01-08 13:36:36,312: t15.2023.11.17 val PER: 0.0404
2026-01-08 13:36:36,312: t15.2023.11.19 val PER: 0.0220
2026-01-08 13:36:36,312: t15.2023.11.26 val PER: 0.1058
2026-01-08 13:36:36,312: t15.2023.12.03 val PER: 0.0945
2026-01-08 13:36:36,313: t15.2023.12.08 val PER: 0.0859
2026-01-08 13:36:36,313: t15.2023.12.10 val PER: 0.0854
2026-01-08 13:36:36,313: t15.2023.12.17 val PER: 0.1216
2026-01-08 13:36:36,313: t15.2023.12.29 val PER: 0.1208
2026-01-08 13:36:36,313: t15.2024.02.25 val PER: 0.0997
2026-01-08 13:36:36,313: t15.2024.03.08 val PER: 0.2134
2026-01-08 13:36:36,313: t15.2024.03.15 val PER: 0.1907
2026-01-08 13:36:36,313: t15.2024.03.17 val PER: 0.1374
2026-01-08 13:36:36,313: t15.2024.05.10 val PER: 0.1753
2026-01-08 13:36:36,313: t15.2024.06.14 val PER: 0.1672
2026-01-08 13:36:36,313: t15.2024.07.19 val PER: 0.2360
2026-01-08 13:36:36,313: t15.2024.07.21 val PER: 0.0952
2026-01-08 13:36:36,314: t15.2024.07.28 val PER: 0.1206
2026-01-08 13:36:36,314: t15.2025.01.10 val PER: 0.2948
2026-01-08 13:36:36,314: t15.2025.01.12 val PER: 0.1324
2026-01-08 13:36:36,314: t15.2025.03.14 val PER: 0.3284
2026-01-08 13:36:36,314: t15.2025.03.16 val PER: 0.1688
2026-01-08 13:36:36,314: t15.2025.03.30 val PER: 0.2851
2026-01-08 13:36:36,314: t15.2025.04.13 val PER: 0.2183
2026-01-08 13:36:45,220: Train batch 16600: loss: 8.01 grad norm: 66.54 time: 0.056
2026-01-08 13:37:02,463: Train batch 16800: loss: 13.83 grad norm: 77.41 time: 0.065
2026-01-08 13:37:20,189: Train batch 17000: loss: 7.60 grad norm: 60.51 time: 0.087
2026-01-08 13:37:20,190: Running test after training batch: 17000
2026-01-08 13:37:20,286: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:37:25,308: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e a cold at this arndt as will
2026-01-08 13:37:25,342: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost ent
2026-01-08 13:37:27,217: Val batch 17000: PER (avg): 0.1420 CTC Loss (avg): 15.0547 WER(1gram): 57.11% (n=64) time: 7.027
2026-01-08 13:37:27,217: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-08 13:37:27,217: t15.2023.08.13 val PER: 0.1071
2026-01-08 13:37:27,217: t15.2023.08.18 val PER: 0.1065
2026-01-08 13:37:27,217: t15.2023.08.20 val PER: 0.0993
2026-01-08 13:37:27,217: t15.2023.08.25 val PER: 0.0843
2026-01-08 13:37:27,217: t15.2023.08.27 val PER: 0.1736
2026-01-08 13:37:27,217: t15.2023.09.01 val PER: 0.0682
2026-01-08 13:37:27,217: t15.2023.09.03 val PER: 0.1568
2026-01-08 13:37:27,218: t15.2023.09.24 val PER: 0.1165
2026-01-08 13:37:27,218: t15.2023.09.29 val PER: 0.1283
2026-01-08 13:37:27,218: t15.2023.10.01 val PER: 0.1711
2026-01-08 13:37:27,218: t15.2023.10.06 val PER: 0.0915
2026-01-08 13:37:27,218: t15.2023.10.08 val PER: 0.2368
2026-01-08 13:37:27,218: t15.2023.10.13 val PER: 0.1893
2026-01-08 13:37:27,218: t15.2023.10.15 val PER: 0.1490
2026-01-08 13:37:27,218: t15.2023.10.20 val PER: 0.1577
2026-01-08 13:37:27,218: t15.2023.10.22 val PER: 0.1002
2026-01-08 13:37:27,218: t15.2023.11.03 val PER: 0.1669
2026-01-08 13:37:27,219: t15.2023.11.04 val PER: 0.0239
2026-01-08 13:37:27,219: t15.2023.11.17 val PER: 0.0435
2026-01-08 13:37:27,219: t15.2023.11.19 val PER: 0.0240
2026-01-08 13:37:27,219: t15.2023.11.26 val PER: 0.0971
2026-01-08 13:37:27,219: t15.2023.12.03 val PER: 0.0966
2026-01-08 13:37:27,219: t15.2023.12.08 val PER: 0.0832
2026-01-08 13:37:27,219: t15.2023.12.10 val PER: 0.0815
2026-01-08 13:37:27,219: t15.2023.12.17 val PER: 0.1216
2026-01-08 13:37:27,219: t15.2023.12.29 val PER: 0.1187
2026-01-08 13:37:27,219: t15.2024.02.25 val PER: 0.1011
2026-01-08 13:37:27,219: t15.2024.03.08 val PER: 0.2091
2026-01-08 13:37:27,219: t15.2024.03.15 val PER: 0.1889
2026-01-08 13:37:27,219: t15.2024.03.17 val PER: 0.1402
2026-01-08 13:37:27,219: t15.2024.05.10 val PER: 0.1694
2026-01-08 13:37:27,219: t15.2024.06.14 val PER: 0.1735
2026-01-08 13:37:27,219: t15.2024.07.19 val PER: 0.2254
2026-01-08 13:37:27,220: t15.2024.07.21 val PER: 0.0924
2026-01-08 13:37:27,220: t15.2024.07.28 val PER: 0.1257
2026-01-08 13:37:27,220: t15.2025.01.10 val PER: 0.2782
2026-01-08 13:37:27,220: t15.2025.01.12 val PER: 0.1424
2026-01-08 13:37:27,220: t15.2025.03.14 val PER: 0.3210
2026-01-08 13:37:27,220: t15.2025.03.16 val PER: 0.1636
2026-01-08 13:37:27,220: t15.2025.03.30 val PER: 0.2897
2026-01-08 13:37:27,220: t15.2025.04.13 val PER: 0.2140
2026-01-08 13:37:27,221: New best val WER(1gram) 57.36% --> 57.11%
2026-01-08 13:37:27,221: Checkpointing model
2026-01-08 13:37:27,363: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/head_3blocks_ln/checkpoint/best_checkpoint
2026-01-08 13:37:46,075: Train batch 17200: loss: 8.42 grad norm: 56.67 time: 0.088
2026-01-08 13:38:04,910: Train batch 17400: loss: 10.75 grad norm: 74.14 time: 0.076
2026-01-08 13:38:14,215: Running test after training batch: 17500
2026-01-08 13:38:14,354: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:38:19,063: WER debug example
  GT : you can see the code at this point as well
  PR : yu can e a cold at this otte as will
2026-01-08 13:38:19,096: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost ent
2026-01-08 13:38:21,014: Val batch 17500: PER (avg): 0.1420 CTC Loss (avg): 15.0874 WER(1gram): 58.12% (n=64) time: 6.799
2026-01-08 13:38:21,014: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-08 13:38:21,015: t15.2023.08.13 val PER: 0.1071
2026-01-08 13:38:21,015: t15.2023.08.18 val PER: 0.1014
2026-01-08 13:38:21,015: t15.2023.08.20 val PER: 0.0993
2026-01-08 13:38:21,015: t15.2023.08.25 val PER: 0.0798
2026-01-08 13:38:21,015: t15.2023.08.27 val PER: 0.1801
2026-01-08 13:38:21,015: t15.2023.09.01 val PER: 0.0731
2026-01-08 13:38:21,015: t15.2023.09.03 val PER: 0.1568
2026-01-08 13:38:21,015: t15.2023.09.24 val PER: 0.1177
2026-01-08 13:38:21,015: t15.2023.09.29 val PER: 0.1200
2026-01-08 13:38:21,015: t15.2023.10.01 val PER: 0.1717
2026-01-08 13:38:21,015: t15.2023.10.06 val PER: 0.0872
2026-01-08 13:38:21,015: t15.2023.10.08 val PER: 0.2436
2026-01-08 13:38:21,015: t15.2023.10.13 val PER: 0.1854
2026-01-08 13:38:21,016: t15.2023.10.15 val PER: 0.1496
2026-01-08 13:38:21,016: t15.2023.10.20 val PER: 0.1678
2026-01-08 13:38:21,016: t15.2023.10.22 val PER: 0.0958
2026-01-08 13:38:21,016: t15.2023.11.03 val PER: 0.1642
2026-01-08 13:38:21,016: t15.2023.11.04 val PER: 0.0273
2026-01-08 13:38:21,016: t15.2023.11.17 val PER: 0.0373
2026-01-08 13:38:21,016: t15.2023.11.19 val PER: 0.0240
2026-01-08 13:38:21,016: t15.2023.11.26 val PER: 0.0993
2026-01-08 13:38:21,016: t15.2023.12.03 val PER: 0.0914
2026-01-08 13:38:21,016: t15.2023.12.08 val PER: 0.0826
2026-01-08 13:38:21,016: t15.2023.12.10 val PER: 0.0854
2026-01-08 13:38:21,016: t15.2023.12.17 val PER: 0.1268
2026-01-08 13:38:21,017: t15.2023.12.29 val PER: 0.1187
2026-01-08 13:38:21,017: t15.2024.02.25 val PER: 0.1053
2026-01-08 13:38:21,017: t15.2024.03.08 val PER: 0.2134
2026-01-08 13:38:21,017: t15.2024.03.15 val PER: 0.1864
2026-01-08 13:38:21,017: t15.2024.03.17 val PER: 0.1416
2026-01-08 13:38:21,017: t15.2024.05.10 val PER: 0.1694
2026-01-08 13:38:21,017: t15.2024.06.14 val PER: 0.1735
2026-01-08 13:38:21,017: t15.2024.07.19 val PER: 0.2287
2026-01-08 13:38:21,017: t15.2024.07.21 val PER: 0.0945
2026-01-08 13:38:21,017: t15.2024.07.28 val PER: 0.1272
2026-01-08 13:38:21,017: t15.2025.01.10 val PER: 0.2824
2026-01-08 13:38:21,018: t15.2025.01.12 val PER: 0.1432
2026-01-08 13:38:21,018: t15.2025.03.14 val PER: 0.3180
2026-01-08 13:38:21,018: t15.2025.03.16 val PER: 0.1571
2026-01-08 13:38:21,018: t15.2025.03.30 val PER: 0.2908
2026-01-08 13:38:21,018: t15.2025.04.13 val PER: 0.2168
2026-01-08 13:38:29,597: Train batch 17600: loss: 8.76 grad norm: 67.04 time: 0.054
2026-01-08 13:38:47,549: Train batch 17800: loss: 4.95 grad norm: 45.06 time: 0.045
2026-01-08 13:39:05,574: Train batch 18000: loss: 7.82 grad norm: 57.38 time: 0.063
2026-01-08 13:39:05,574: Running test after training batch: 18000
2026-01-08 13:39:05,716: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:39:10,691: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e the cold at this arndt as will
2026-01-08 13:39:10,725: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the ost ent
2026-01-08 13:39:12,827: Val batch 18000: PER (avg): 0.1418 CTC Loss (avg): 15.0007 WER(1gram): 59.14% (n=64) time: 7.253
2026-01-08 13:39:12,828: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-08 13:39:12,828: t15.2023.08.13 val PER: 0.1081
2026-01-08 13:39:12,828: t15.2023.08.18 val PER: 0.0997
2026-01-08 13:39:12,828: t15.2023.08.20 val PER: 0.1001
2026-01-08 13:39:12,828: t15.2023.08.25 val PER: 0.0858
2026-01-08 13:39:12,828: t15.2023.08.27 val PER: 0.1704
2026-01-08 13:39:12,828: t15.2023.09.01 val PER: 0.0690
2026-01-08 13:39:12,829: t15.2023.09.03 val PER: 0.1580
2026-01-08 13:39:12,829: t15.2023.09.24 val PER: 0.1141
2026-01-08 13:39:12,829: t15.2023.09.29 val PER: 0.1213
2026-01-08 13:39:12,829: t15.2023.10.01 val PER: 0.1671
2026-01-08 13:39:12,829: t15.2023.10.06 val PER: 0.0915
2026-01-08 13:39:12,829: t15.2023.10.08 val PER: 0.2314
2026-01-08 13:39:12,829: t15.2023.10.13 val PER: 0.1846
2026-01-08 13:39:12,829: t15.2023.10.15 val PER: 0.1496
2026-01-08 13:39:12,829: t15.2023.10.20 val PER: 0.1510
2026-01-08 13:39:12,829: t15.2023.10.22 val PER: 0.0924
2026-01-08 13:39:12,829: t15.2023.11.03 val PER: 0.1635
2026-01-08 13:39:12,829: t15.2023.11.04 val PER: 0.0239
2026-01-08 13:39:12,829: t15.2023.11.17 val PER: 0.0389
2026-01-08 13:39:12,830: t15.2023.11.19 val PER: 0.0220
2026-01-08 13:39:12,830: t15.2023.11.26 val PER: 0.0964
2026-01-08 13:39:12,830: t15.2023.12.03 val PER: 0.0966
2026-01-08 13:39:12,830: t15.2023.12.08 val PER: 0.0832
2026-01-08 13:39:12,830: t15.2023.12.10 val PER: 0.0815
2026-01-08 13:39:12,830: t15.2023.12.17 val PER: 0.1247
2026-01-08 13:39:12,830: t15.2023.12.29 val PER: 0.1160
2026-01-08 13:39:12,830: t15.2024.02.25 val PER: 0.1081
2026-01-08 13:39:12,830: t15.2024.03.08 val PER: 0.2176
2026-01-08 13:39:12,830: t15.2024.03.15 val PER: 0.1870
2026-01-08 13:39:12,830: t15.2024.03.17 val PER: 0.1416
2026-01-08 13:39:12,830: t15.2024.05.10 val PER: 0.1724
2026-01-08 13:39:12,830: t15.2024.06.14 val PER: 0.1735
2026-01-08 13:39:12,830: t15.2024.07.19 val PER: 0.2340
2026-01-08 13:39:12,830: t15.2024.07.21 val PER: 0.0959
2026-01-08 13:39:12,830: t15.2024.07.28 val PER: 0.1287
2026-01-08 13:39:12,830: t15.2025.01.10 val PER: 0.2893
2026-01-08 13:39:12,831: t15.2025.01.12 val PER: 0.1401
2026-01-08 13:39:12,831: t15.2025.03.14 val PER: 0.3225
2026-01-08 13:39:12,831: t15.2025.03.16 val PER: 0.1623
2026-01-08 13:39:12,831: t15.2025.03.30 val PER: 0.2908
2026-01-08 13:39:12,831: t15.2025.04.13 val PER: 0.2183
2026-01-08 13:39:30,717: Train batch 18200: loss: 6.58 grad norm: 54.98 time: 0.078
2026-01-08 13:39:48,630: Train batch 18400: loss: 5.64 grad norm: 48.64 time: 0.061
2026-01-08 13:39:57,365: Running test after training batch: 18500
2026-01-08 13:39:57,504: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:40:02,221: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e a cold at this arndt as will
2026-01-08 13:40:02,256: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the ost ent
2026-01-08 13:40:04,198: Val batch 18500: PER (avg): 0.1412 CTC Loss (avg): 15.0148 WER(1gram): 59.90% (n=64) time: 6.833
2026-01-08 13:40:04,198: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-08 13:40:04,198: t15.2023.08.13 val PER: 0.1060
2026-01-08 13:40:04,198: t15.2023.08.18 val PER: 0.1048
2026-01-08 13:40:04,199: t15.2023.08.20 val PER: 0.0985
2026-01-08 13:40:04,199: t15.2023.08.25 val PER: 0.0813
2026-01-08 13:40:04,199: t15.2023.08.27 val PER: 0.1720
2026-01-08 13:40:04,199: t15.2023.09.01 val PER: 0.0666
2026-01-08 13:40:04,199: t15.2023.09.03 val PER: 0.1556
2026-01-08 13:40:04,199: t15.2023.09.24 val PER: 0.1165
2026-01-08 13:40:04,199: t15.2023.09.29 val PER: 0.1219
2026-01-08 13:40:04,199: t15.2023.10.01 val PER: 0.1671
2026-01-08 13:40:04,199: t15.2023.10.06 val PER: 0.0861
2026-01-08 13:40:04,199: t15.2023.10.08 val PER: 0.2368
2026-01-08 13:40:04,199: t15.2023.10.13 val PER: 0.1908
2026-01-08 13:40:04,200: t15.2023.10.15 val PER: 0.1483
2026-01-08 13:40:04,200: t15.2023.10.20 val PER: 0.1544
2026-01-08 13:40:04,200: t15.2023.10.22 val PER: 0.1002
2026-01-08 13:40:04,200: t15.2023.11.03 val PER: 0.1655
2026-01-08 13:40:04,200: t15.2023.11.04 val PER: 0.0239
2026-01-08 13:40:04,200: t15.2023.11.17 val PER: 0.0404
2026-01-08 13:40:04,200: t15.2023.11.19 val PER: 0.0240
2026-01-08 13:40:04,200: t15.2023.11.26 val PER: 0.0986
2026-01-08 13:40:04,200: t15.2023.12.03 val PER: 0.0935
2026-01-08 13:40:04,200: t15.2023.12.08 val PER: 0.0846
2026-01-08 13:40:04,200: t15.2023.12.10 val PER: 0.0815
2026-01-08 13:40:04,200: t15.2023.12.17 val PER: 0.1310
2026-01-08 13:40:04,200: t15.2023.12.29 val PER: 0.1208
2026-01-08 13:40:04,200: t15.2024.02.25 val PER: 0.1039
2026-01-08 13:40:04,200: t15.2024.03.08 val PER: 0.2091
2026-01-08 13:40:04,200: t15.2024.03.15 val PER: 0.1870
2026-01-08 13:40:04,200: t15.2024.03.17 val PER: 0.1395
2026-01-08 13:40:04,201: t15.2024.05.10 val PER: 0.1738
2026-01-08 13:40:04,201: t15.2024.06.14 val PER: 0.1672
2026-01-08 13:40:04,201: t15.2024.07.19 val PER: 0.2261
2026-01-08 13:40:04,201: t15.2024.07.21 val PER: 0.0924
2026-01-08 13:40:04,201: t15.2024.07.28 val PER: 0.1287
2026-01-08 13:40:04,201: t15.2025.01.10 val PER: 0.2810
2026-01-08 13:40:04,201: t15.2025.01.12 val PER: 0.1324
2026-01-08 13:40:04,201: t15.2025.03.14 val PER: 0.3166
2026-01-08 13:40:04,201: t15.2025.03.16 val PER: 0.1662
2026-01-08 13:40:04,201: t15.2025.03.30 val PER: 0.2839
2026-01-08 13:40:04,201: t15.2025.04.13 val PER: 0.2168
2026-01-08 13:40:13,307: Train batch 18600: loss: 11.15 grad norm: 75.34 time: 0.071
2026-01-08 13:40:31,363: Train batch 18800: loss: 7.11 grad norm: 58.12 time: 0.069
2026-01-08 13:40:49,583: Train batch 19000: loss: 8.01 grad norm: 63.25 time: 0.068
2026-01-08 13:40:49,584: Running test after training batch: 19000
2026-01-08 13:40:49,716: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:40:54,640: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e a cold at this arndt as will
2026-01-08 13:40:54,683: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the ost ent
2026-01-08 13:40:56,853: Val batch 19000: PER (avg): 0.1409 CTC Loss (avg): 14.9940 WER(1gram): 58.63% (n=64) time: 7.269
2026-01-08 13:40:56,853: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-08 13:40:56,853: t15.2023.08.13 val PER: 0.1071
2026-01-08 13:40:56,853: t15.2023.08.18 val PER: 0.0997
2026-01-08 13:40:56,853: t15.2023.08.20 val PER: 0.0953
2026-01-08 13:40:56,853: t15.2023.08.25 val PER: 0.0798
2026-01-08 13:40:56,854: t15.2023.08.27 val PER: 0.1752
2026-01-08 13:40:56,854: t15.2023.09.01 val PER: 0.0682
2026-01-08 13:40:56,854: t15.2023.09.03 val PER: 0.1556
2026-01-08 13:40:56,854: t15.2023.09.24 val PER: 0.1189
2026-01-08 13:40:56,854: t15.2023.09.29 val PER: 0.1238
2026-01-08 13:40:56,854: t15.2023.10.01 val PER: 0.1697
2026-01-08 13:40:56,854: t15.2023.10.06 val PER: 0.0893
2026-01-08 13:40:56,854: t15.2023.10.08 val PER: 0.2368
2026-01-08 13:40:56,854: t15.2023.10.13 val PER: 0.1862
2026-01-08 13:40:56,855: t15.2023.10.15 val PER: 0.1490
2026-01-08 13:40:56,855: t15.2023.10.20 val PER: 0.1577
2026-01-08 13:40:56,855: t15.2023.10.22 val PER: 0.0935
2026-01-08 13:40:56,855: t15.2023.11.03 val PER: 0.1676
2026-01-08 13:40:56,855: t15.2023.11.04 val PER: 0.0239
2026-01-08 13:40:56,855: t15.2023.11.17 val PER: 0.0389
2026-01-08 13:40:56,855: t15.2023.11.19 val PER: 0.0259
2026-01-08 13:40:56,855: t15.2023.11.26 val PER: 0.0978
2026-01-08 13:40:56,855: t15.2023.12.03 val PER: 0.1019
2026-01-08 13:40:56,855: t15.2023.12.08 val PER: 0.0826
2026-01-08 13:40:56,855: t15.2023.12.10 val PER: 0.0802
2026-01-08 13:40:56,855: t15.2023.12.17 val PER: 0.1320
2026-01-08 13:40:56,855: t15.2023.12.29 val PER: 0.1187
2026-01-08 13:40:56,855: t15.2024.02.25 val PER: 0.1053
2026-01-08 13:40:56,855: t15.2024.03.08 val PER: 0.2105
2026-01-08 13:40:56,855: t15.2024.03.15 val PER: 0.1870
2026-01-08 13:40:56,856: t15.2024.03.17 val PER: 0.1367
2026-01-08 13:40:56,856: t15.2024.05.10 val PER: 0.1694
2026-01-08 13:40:56,856: t15.2024.06.14 val PER: 0.1656
2026-01-08 13:40:56,856: t15.2024.07.19 val PER: 0.2268
2026-01-08 13:40:56,856: t15.2024.07.21 val PER: 0.0910
2026-01-08 13:40:56,856: t15.2024.07.28 val PER: 0.1243
2026-01-08 13:40:56,856: t15.2025.01.10 val PER: 0.2837
2026-01-08 13:40:56,856: t15.2025.01.12 val PER: 0.1370
2026-01-08 13:40:56,856: t15.2025.03.14 val PER: 0.3166
2026-01-08 13:40:56,856: t15.2025.03.16 val PER: 0.1584
2026-01-08 13:40:56,856: t15.2025.03.30 val PER: 0.2851
2026-01-08 13:40:56,856: t15.2025.04.13 val PER: 0.2154
2026-01-08 13:41:15,092: Train batch 19200: loss: 5.57 grad norm: 58.55 time: 0.067
2026-01-08 13:41:33,383: Train batch 19400: loss: 4.66 grad norm: 48.22 time: 0.056
2026-01-08 13:41:42,227: Running test after training batch: 19500
2026-01-08 13:41:42,374: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:41:47,170: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e a cold at this arndt as will
2026-01-08 13:41:47,207: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the ost ent
2026-01-08 13:41:49,648: Val batch 19500: PER (avg): 0.1401 CTC Loss (avg): 14.9814 WER(1gram): 58.38% (n=64) time: 7.420
2026-01-08 13:41:49,648: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-08 13:41:49,648: t15.2023.08.13 val PER: 0.1071
2026-01-08 13:41:49,648: t15.2023.08.18 val PER: 0.0997
2026-01-08 13:41:49,648: t15.2023.08.20 val PER: 0.1025
2026-01-08 13:41:49,649: t15.2023.08.25 val PER: 0.0858
2026-01-08 13:41:49,649: t15.2023.08.27 val PER: 0.1720
2026-01-08 13:41:49,649: t15.2023.09.01 val PER: 0.0633
2026-01-08 13:41:49,649: t15.2023.09.03 val PER: 0.1556
2026-01-08 13:41:49,649: t15.2023.09.24 val PER: 0.1153
2026-01-08 13:41:49,649: t15.2023.09.29 val PER: 0.1232
2026-01-08 13:41:49,649: t15.2023.10.01 val PER: 0.1678
2026-01-08 13:41:49,649: t15.2023.10.06 val PER: 0.0883
2026-01-08 13:41:49,649: t15.2023.10.08 val PER: 0.2409
2026-01-08 13:41:49,649: t15.2023.10.13 val PER: 0.1870
2026-01-08 13:41:49,649: t15.2023.10.15 val PER: 0.1450
2026-01-08 13:41:49,649: t15.2023.10.20 val PER: 0.1510
2026-01-08 13:41:49,649: t15.2023.10.22 val PER: 0.0947
2026-01-08 13:41:49,649: t15.2023.11.03 val PER: 0.1628
2026-01-08 13:41:49,650: t15.2023.11.04 val PER: 0.0239
2026-01-08 13:41:49,650: t15.2023.11.17 val PER: 0.0420
2026-01-08 13:41:49,650: t15.2023.11.19 val PER: 0.0259
2026-01-08 13:41:49,650: t15.2023.11.26 val PER: 0.0964
2026-01-08 13:41:49,650: t15.2023.12.03 val PER: 0.0945
2026-01-08 13:41:49,650: t15.2023.12.08 val PER: 0.0839
2026-01-08 13:41:49,650: t15.2023.12.10 val PER: 0.0880
2026-01-08 13:41:49,650: t15.2023.12.17 val PER: 0.1247
2026-01-08 13:41:49,650: t15.2023.12.29 val PER: 0.1167
2026-01-08 13:41:49,650: t15.2024.02.25 val PER: 0.0983
2026-01-08 13:41:49,650: t15.2024.03.08 val PER: 0.2091
2026-01-08 13:41:49,650: t15.2024.03.15 val PER: 0.1857
2026-01-08 13:41:49,651: t15.2024.03.17 val PER: 0.1332
2026-01-08 13:41:49,651: t15.2024.05.10 val PER: 0.1738
2026-01-08 13:41:49,651: t15.2024.06.14 val PER: 0.1688
2026-01-08 13:41:49,651: t15.2024.07.19 val PER: 0.2294
2026-01-08 13:41:49,651: t15.2024.07.21 val PER: 0.0917
2026-01-08 13:41:49,651: t15.2024.07.28 val PER: 0.1250
2026-01-08 13:41:49,651: t15.2025.01.10 val PER: 0.2755
2026-01-08 13:41:49,651: t15.2025.01.12 val PER: 0.1355
2026-01-08 13:41:49,651: t15.2025.03.14 val PER: 0.3166
2026-01-08 13:41:49,651: t15.2025.03.16 val PER: 0.1597
2026-01-08 13:41:49,651: t15.2025.03.30 val PER: 0.2816
2026-01-08 13:41:49,651: t15.2025.04.13 val PER: 0.2168
2026-01-08 13:41:58,492: Train batch 19600: loss: 7.27 grad norm: 59.50 time: 0.060
2026-01-08 13:42:16,711: Train batch 19800: loss: 6.72 grad norm: 56.92 time: 0.057
2026-01-08 13:42:34,734: Running test after training batch: 19999
2026-01-08 13:42:34,823: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:42:39,450: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e a cold at this arndt as will
2026-01-08 13:42:39,487: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the ost ent
2026-01-08 13:42:41,459: Val batch 19999: PER (avg): 0.1399 CTC Loss (avg): 15.0061 WER(1gram): 57.36% (n=64) time: 6.725
2026-01-08 13:42:41,459: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-08 13:42:41,459: t15.2023.08.13 val PER: 0.1091
2026-01-08 13:42:41,459: t15.2023.08.18 val PER: 0.0947
2026-01-08 13:42:41,459: t15.2023.08.20 val PER: 0.0977
2026-01-08 13:42:41,459: t15.2023.08.25 val PER: 0.0783
2026-01-08 13:42:41,460: t15.2023.08.27 val PER: 0.1720
2026-01-08 13:42:41,460: t15.2023.09.01 val PER: 0.0641
2026-01-08 13:42:41,460: t15.2023.09.03 val PER: 0.1544
2026-01-08 13:42:41,460: t15.2023.09.24 val PER: 0.1153
2026-01-08 13:42:41,460: t15.2023.09.29 val PER: 0.1238
2026-01-08 13:42:41,460: t15.2023.10.01 val PER: 0.1678
2026-01-08 13:42:41,460: t15.2023.10.06 val PER: 0.0850
2026-01-08 13:42:41,460: t15.2023.10.08 val PER: 0.2368
2026-01-08 13:42:41,460: t15.2023.10.13 val PER: 0.1870
2026-01-08 13:42:41,460: t15.2023.10.15 val PER: 0.1430
2026-01-08 13:42:41,460: t15.2023.10.20 val PER: 0.1443
2026-01-08 13:42:41,460: t15.2023.10.22 val PER: 0.0969
2026-01-08 13:42:41,460: t15.2023.11.03 val PER: 0.1649
2026-01-08 13:42:41,461: t15.2023.11.04 val PER: 0.0239
2026-01-08 13:42:41,461: t15.2023.11.17 val PER: 0.0404
2026-01-08 13:42:41,461: t15.2023.11.19 val PER: 0.0240
2026-01-08 13:42:41,461: t15.2023.11.26 val PER: 0.0949
2026-01-08 13:42:41,461: t15.2023.12.03 val PER: 0.0977
2026-01-08 13:42:41,461: t15.2023.12.08 val PER: 0.0799
2026-01-08 13:42:41,461: t15.2023.12.10 val PER: 0.0841
2026-01-08 13:42:41,461: t15.2023.12.17 val PER: 0.1247
2026-01-08 13:42:41,461: t15.2023.12.29 val PER: 0.1187
2026-01-08 13:42:41,461: t15.2024.02.25 val PER: 0.1011
2026-01-08 13:42:41,461: t15.2024.03.08 val PER: 0.2063
2026-01-08 13:42:41,462: t15.2024.03.15 val PER: 0.1845
2026-01-08 13:42:41,462: t15.2024.03.17 val PER: 0.1381
2026-01-08 13:42:41,462: t15.2024.05.10 val PER: 0.1753
2026-01-08 13:42:41,462: t15.2024.06.14 val PER: 0.1640
2026-01-08 13:42:41,462: t15.2024.07.19 val PER: 0.2294
2026-01-08 13:42:41,462: t15.2024.07.21 val PER: 0.0924
2026-01-08 13:42:41,462: t15.2024.07.28 val PER: 0.1272
2026-01-08 13:42:41,462: t15.2025.01.10 val PER: 0.2741
2026-01-08 13:42:41,462: t15.2025.01.12 val PER: 0.1332
2026-01-08 13:42:41,462: t15.2025.03.14 val PER: 0.3284
2026-01-08 13:42:41,462: t15.2025.03.16 val PER: 0.1571
2026-01-08 13:42:41,462: t15.2025.03.30 val PER: 0.2885
2026-01-08 13:42:41,462: t15.2025.04.13 val PER: 0.2240
2026-01-08 13:42:41,489: Best avg val PER achieved: 0.14201
2026-01-08 13:42:41,489: Total training time: 34.48 minutes

=== RUN speckle_p20.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20
2026-01-08 13:42:46,103: Using device: cuda:0
2026-01-08 13:42:47,816: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-08 13:42:47,839: Using 45 sessions after filtering (from 45).
2026-01-08 13:42:48,292: Using torch.compile (if available)
2026-01-08 13:42:48,292: torch.compile not available (torch<2.0). Skipping.
2026-01-08 13:42:48,292: Initialized RNN decoding model
2026-01-08 13:42:48,292: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-08 13:42:48,293: Model has 44,907,305 parameters
2026-01-08 13:42:48,293: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-08 13:42:49,564: Successfully initialized datasets
2026-01-08 13:42:49,565: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-08 13:42:51,869: Train batch 0: loss: 575.62 grad norm: 1427.42 time: 0.196
2026-01-08 13:42:51,869: Running test after training batch: 0
2026-01-08 13:42:51,980: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:42:57,411: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-08 13:42:58,237: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-08 13:43:37,957: Val batch 0: PER (avg): 1.4296 CTC Loss (avg): 633.1960 WER(1gram): 100.00% (n=64) time: 46.088
2026-01-08 13:43:37,958: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-08 13:43:37,958: t15.2023.08.13 val PER: 1.3035
2026-01-08 13:43:37,958: t15.2023.08.18 val PER: 1.4267
2026-01-08 13:43:37,958: t15.2023.08.20 val PER: 1.3034
2026-01-08 13:43:37,958: t15.2023.08.25 val PER: 1.3373
2026-01-08 13:43:37,958: t15.2023.08.27 val PER: 1.2556
2026-01-08 13:43:37,958: t15.2023.09.01 val PER: 1.4505
2026-01-08 13:43:37,958: t15.2023.09.03 val PER: 1.3242
2026-01-08 13:43:37,959: t15.2023.09.24 val PER: 1.5328
2026-01-08 13:43:37,959: t15.2023.09.29 val PER: 1.4690
2026-01-08 13:43:37,959: t15.2023.10.01 val PER: 1.2100
2026-01-08 13:43:37,959: t15.2023.10.06 val PER: 1.4919
2026-01-08 13:43:37,959: t15.2023.10.08 val PER: 1.1867
2026-01-08 13:43:37,959: t15.2023.10.13 val PER: 1.3964
2026-01-08 13:43:37,959: t15.2023.10.15 val PER: 1.3889
2026-01-08 13:43:37,959: t15.2023.10.20 val PER: 1.4899
2026-01-08 13:43:37,959: t15.2023.10.22 val PER: 1.3953
2026-01-08 13:43:37,959: t15.2023.11.03 val PER: 1.5950
2026-01-08 13:43:37,959: t15.2023.11.04 val PER: 2.0307
2026-01-08 13:43:37,960: t15.2023.11.17 val PER: 1.9502
2026-01-08 13:43:37,960: t15.2023.11.19 val PER: 1.6786
2026-01-08 13:43:37,960: t15.2023.11.26 val PER: 1.5391
2026-01-08 13:43:37,960: t15.2023.12.03 val PER: 1.4275
2026-01-08 13:43:37,960: t15.2023.12.08 val PER: 1.4487
2026-01-08 13:43:37,960: t15.2023.12.10 val PER: 1.7030
2026-01-08 13:43:37,960: t15.2023.12.17 val PER: 1.3035
2026-01-08 13:43:37,960: t15.2023.12.29 val PER: 1.4104
2026-01-08 13:43:37,960: t15.2024.02.25 val PER: 1.4242
2026-01-08 13:43:37,960: t15.2024.03.08 val PER: 1.3229
2026-01-08 13:43:37,960: t15.2024.03.15 val PER: 1.3158
2026-01-08 13:43:37,960: t15.2024.03.17 val PER: 1.4010
2026-01-08 13:43:37,960: t15.2024.05.10 val PER: 1.3343
2026-01-08 13:43:37,960: t15.2024.06.14 val PER: 1.5268
2026-01-08 13:43:37,960: t15.2024.07.19 val PER: 1.0817
2026-01-08 13:43:37,960: t15.2024.07.21 val PER: 1.6317
2026-01-08 13:43:37,961: t15.2024.07.28 val PER: 1.6603
2026-01-08 13:43:37,961: t15.2025.01.10 val PER: 1.0868
2026-01-08 13:43:37,961: t15.2025.01.12 val PER: 1.7621
2026-01-08 13:43:37,961: t15.2025.03.14 val PER: 1.0370
2026-01-08 13:43:37,961: t15.2025.03.16 val PER: 1.6230
2026-01-08 13:43:37,961: t15.2025.03.30 val PER: 1.2885
2026-01-08 13:43:37,961: t15.2025.04.13 val PER: 1.5849
2026-01-08 13:43:37,963: New best val WER(1gram) inf% --> 100.00%
2026-01-08 13:43:37,963: Checkpointing model
2026-01-08 13:43:38,107: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 13:43:55,192: Train batch 200: loss: 81.13 grad norm: 100.97 time: 0.054
2026-01-08 13:44:12,013: Train batch 400: loss: 56.55 grad norm: 77.75 time: 0.063
2026-01-08 13:44:20,498: Running test after training batch: 500
2026-01-08 13:44:20,631: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:44:25,777: WER debug example
  GT : you can see the code at this point as well
  PR : yule ede ease thus uhde at this ide is aisle
2026-01-08 13:44:25,815: WER debug example
  GT : how does it keep the cost down
  PR : ide does it ink thus as adz
2026-01-08 13:44:28,572: Val batch 500: PER (avg): 0.5389 CTC Loss (avg): 58.3143 WER(1gram): 90.10% (n=64) time: 8.074
2026-01-08 13:44:28,573: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=12
2026-01-08 13:44:28,573: t15.2023.08.13 val PER: 0.4699
2026-01-08 13:44:28,573: t15.2023.08.18 val PER: 0.4820
2026-01-08 13:44:28,573: t15.2023.08.20 val PER: 0.4726
2026-01-08 13:44:28,573: t15.2023.08.25 val PER: 0.4488
2026-01-08 13:44:28,573: t15.2023.08.27 val PER: 0.5450
2026-01-08 13:44:28,574: t15.2023.09.01 val PER: 0.4424
2026-01-08 13:44:28,574: t15.2023.09.03 val PER: 0.5107
2026-01-08 13:44:28,574: t15.2023.09.24 val PER: 0.4393
2026-01-08 13:44:28,574: t15.2023.09.29 val PER: 0.4882
2026-01-08 13:44:28,574: t15.2023.10.01 val PER: 0.5410
2026-01-08 13:44:28,574: t15.2023.10.06 val PER: 0.4424
2026-01-08 13:44:28,574: t15.2023.10.08 val PER: 0.5440
2026-01-08 13:44:28,574: t15.2023.10.13 val PER: 0.5756
2026-01-08 13:44:28,574: t15.2023.10.15 val PER: 0.5188
2026-01-08 13:44:28,574: t15.2023.10.20 val PER: 0.4799
2026-01-08 13:44:28,574: t15.2023.10.22 val PER: 0.4710
2026-01-08 13:44:28,574: t15.2023.11.03 val PER: 0.5258
2026-01-08 13:44:28,574: t15.2023.11.04 val PER: 0.2935
2026-01-08 13:44:28,574: t15.2023.11.17 val PER: 0.3748
2026-01-08 13:44:28,575: t15.2023.11.19 val PER: 0.3653
2026-01-08 13:44:28,575: t15.2023.11.26 val PER: 0.5797
2026-01-08 13:44:28,575: t15.2023.12.03 val PER: 0.5147
2026-01-08 13:44:28,575: t15.2023.12.08 val PER: 0.5379
2026-01-08 13:44:28,575: t15.2023.12.10 val PER: 0.4796
2026-01-08 13:44:28,575: t15.2023.12.17 val PER: 0.5946
2026-01-08 13:44:28,575: t15.2023.12.29 val PER: 0.5710
2026-01-08 13:44:28,575: t15.2024.02.25 val PER: 0.4916
2026-01-08 13:44:28,575: t15.2024.03.08 val PER: 0.6415
2026-01-08 13:44:28,575: t15.2024.03.15 val PER: 0.5866
2026-01-08 13:44:28,575: t15.2024.03.17 val PER: 0.5195
2026-01-08 13:44:28,575: t15.2024.05.10 val PER: 0.5661
2026-01-08 13:44:28,575: t15.2024.06.14 val PER: 0.5394
2026-01-08 13:44:28,575: t15.2024.07.19 val PER: 0.7053
2026-01-08 13:44:28,575: t15.2024.07.21 val PER: 0.4910
2026-01-08 13:44:28,575: t15.2024.07.28 val PER: 0.5390
2026-01-08 13:44:28,575: t15.2025.01.10 val PER: 0.7658
2026-01-08 13:44:28,576: t15.2025.01.12 val PER: 0.5858
2026-01-08 13:44:28,576: t15.2025.03.14 val PER: 0.7530
2026-01-08 13:44:28,576: t15.2025.03.16 val PER: 0.6178
2026-01-08 13:44:28,576: t15.2025.03.30 val PER: 0.7253
2026-01-08 13:44:28,576: t15.2025.04.13 val PER: 0.5949
2026-01-08 13:44:28,577: New best val WER(1gram) 100.00% --> 90.10%
2026-01-08 13:44:28,577: Checkpointing model
2026-01-08 13:44:28,716: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 13:44:37,163: Train batch 600: loss: 51.16 grad norm: 81.95 time: 0.079
2026-01-08 13:44:55,073: Train batch 800: loss: 43.03 grad norm: 86.46 time: 0.059
2026-01-08 13:45:13,241: Train batch 1000: loss: 44.61 grad norm: 79.09 time: 0.067
2026-01-08 13:45:13,242: Running test after training batch: 1000
2026-01-08 13:45:13,347: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:45:18,154: WER debug example
  GT : you can see the code at this point as well
  PR : used ent ease that owed it this royd is while
2026-01-08 13:45:18,186: WER debug example
  GT : how does it keep the cost down
  PR : houde buzze it ink that wass it
2026-01-08 13:45:20,198: Val batch 1000: PER (avg): 0.4180 CTC Loss (avg): 44.1389 WER(1gram): 84.52% (n=64) time: 6.956
2026-01-08 13:45:20,199: WER lens: avg_true_words=6.16 avg_pred_words=5.59 max_pred_words=12
2026-01-08 13:45:20,199: t15.2023.08.13 val PER: 0.3950
2026-01-08 13:45:20,199: t15.2023.08.18 val PER: 0.3613
2026-01-08 13:45:20,199: t15.2023.08.20 val PER: 0.3495
2026-01-08 13:45:20,199: t15.2023.08.25 val PER: 0.3012
2026-01-08 13:45:20,199: t15.2023.08.27 val PER: 0.4293
2026-01-08 13:45:20,199: t15.2023.09.01 val PER: 0.3125
2026-01-08 13:45:20,199: t15.2023.09.03 val PER: 0.4133
2026-01-08 13:45:20,199: t15.2023.09.24 val PER: 0.3398
2026-01-08 13:45:20,199: t15.2023.09.29 val PER: 0.3740
2026-01-08 13:45:20,199: t15.2023.10.01 val PER: 0.4122
2026-01-08 13:45:20,199: t15.2023.10.06 val PER: 0.3229
2026-01-08 13:45:20,199: t15.2023.10.08 val PER: 0.4628
2026-01-08 13:45:20,200: t15.2023.10.13 val PER: 0.4732
2026-01-08 13:45:20,200: t15.2023.10.15 val PER: 0.3922
2026-01-08 13:45:20,200: t15.2023.10.20 val PER: 0.3893
2026-01-08 13:45:20,200: t15.2023.10.22 val PER: 0.3641
2026-01-08 13:45:20,200: t15.2023.11.03 val PER: 0.3976
2026-01-08 13:45:20,200: t15.2023.11.04 val PER: 0.1570
2026-01-08 13:45:20,200: t15.2023.11.17 val PER: 0.2768
2026-01-08 13:45:20,200: t15.2023.11.19 val PER: 0.2395
2026-01-08 13:45:20,200: t15.2023.11.26 val PER: 0.4522
2026-01-08 13:45:20,200: t15.2023.12.03 val PER: 0.4128
2026-01-08 13:45:20,200: t15.2023.12.08 val PER: 0.4088
2026-01-08 13:45:20,200: t15.2023.12.10 val PER: 0.3679
2026-01-08 13:45:20,201: t15.2023.12.17 val PER: 0.4376
2026-01-08 13:45:20,201: t15.2023.12.29 val PER: 0.4091
2026-01-08 13:45:20,201: t15.2024.02.25 val PER: 0.3624
2026-01-08 13:45:20,201: t15.2024.03.08 val PER: 0.5206
2026-01-08 13:45:20,201: t15.2024.03.15 val PER: 0.4478
2026-01-08 13:45:20,201: t15.2024.03.17 val PER: 0.4107
2026-01-08 13:45:20,201: t15.2024.05.10 val PER: 0.4294
2026-01-08 13:45:20,201: t15.2024.06.14 val PER: 0.4069
2026-01-08 13:45:20,201: t15.2024.07.19 val PER: 0.5531
2026-01-08 13:45:20,202: t15.2024.07.21 val PER: 0.3772
2026-01-08 13:45:20,202: t15.2024.07.28 val PER: 0.4272
2026-01-08 13:45:20,202: t15.2025.01.10 val PER: 0.6336
2026-01-08 13:45:20,202: t15.2025.01.12 val PER: 0.4496
2026-01-08 13:45:20,202: t15.2025.03.14 val PER: 0.6538
2026-01-08 13:45:20,202: t15.2025.03.16 val PER: 0.4751
2026-01-08 13:45:20,202: t15.2025.03.30 val PER: 0.6632
2026-01-08 13:45:20,202: t15.2025.04.13 val PER: 0.4907
2026-01-08 13:45:20,204: New best val WER(1gram) 90.10% --> 84.52%
2026-01-08 13:45:20,204: Checkpointing model
2026-01-08 13:45:20,344: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 13:45:37,762: Train batch 1200: loss: 34.91 grad norm: 76.44 time: 0.071
2026-01-08 13:45:55,919: Train batch 1400: loss: 37.07 grad norm: 73.48 time: 0.061
2026-01-08 13:46:04,341: Running test after training batch: 1500
2026-01-08 13:46:04,440: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:46:09,582: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt ease thus owed at this boyde is wheel
2026-01-08 13:46:09,618: WER debug example
  GT : how does it keep the cost down
  PR : houde is it heap that cost it
2026-01-08 13:46:11,254: Val batch 1500: PER (avg): 0.3854 CTC Loss (avg): 38.2118 WER(1gram): 77.92% (n=64) time: 6.912
2026-01-08 13:46:11,254: WER lens: avg_true_words=6.16 avg_pred_words=5.12 max_pred_words=11
2026-01-08 13:46:11,254: t15.2023.08.13 val PER: 0.3482
2026-01-08 13:46:11,254: t15.2023.08.18 val PER: 0.3101
2026-01-08 13:46:11,255: t15.2023.08.20 val PER: 0.3002
2026-01-08 13:46:11,255: t15.2023.08.25 val PER: 0.2605
2026-01-08 13:46:11,255: t15.2023.08.27 val PER: 0.3971
2026-01-08 13:46:11,255: t15.2023.09.01 val PER: 0.2752
2026-01-08 13:46:11,255: t15.2023.09.03 val PER: 0.3694
2026-01-08 13:46:11,255: t15.2023.09.24 val PER: 0.3083
2026-01-08 13:46:11,255: t15.2023.09.29 val PER: 0.3414
2026-01-08 13:46:11,255: t15.2023.10.01 val PER: 0.3970
2026-01-08 13:46:11,255: t15.2023.10.06 val PER: 0.2949
2026-01-08 13:46:11,256: t15.2023.10.08 val PER: 0.4371
2026-01-08 13:46:11,256: t15.2023.10.13 val PER: 0.4453
2026-01-08 13:46:11,256: t15.2023.10.15 val PER: 0.3612
2026-01-08 13:46:11,256: t15.2023.10.20 val PER: 0.3188
2026-01-08 13:46:11,256: t15.2023.10.22 val PER: 0.3285
2026-01-08 13:46:11,256: t15.2023.11.03 val PER: 0.3657
2026-01-08 13:46:11,256: t15.2023.11.04 val PER: 0.1263
2026-01-08 13:46:11,256: t15.2023.11.17 val PER: 0.2193
2026-01-08 13:46:11,256: t15.2023.11.19 val PER: 0.1856
2026-01-08 13:46:11,256: t15.2023.11.26 val PER: 0.4268
2026-01-08 13:46:11,257: t15.2023.12.03 val PER: 0.3803
2026-01-08 13:46:11,257: t15.2023.12.08 val PER: 0.3695
2026-01-08 13:46:11,257: t15.2023.12.10 val PER: 0.2957
2026-01-08 13:46:11,257: t15.2023.12.17 val PER: 0.3773
2026-01-08 13:46:11,257: t15.2023.12.29 val PER: 0.3816
2026-01-08 13:46:11,257: t15.2024.02.25 val PER: 0.3146
2026-01-08 13:46:11,257: t15.2024.03.08 val PER: 0.4566
2026-01-08 13:46:11,257: t15.2024.03.15 val PER: 0.4171
2026-01-08 13:46:11,257: t15.2024.03.17 val PER: 0.3752
2026-01-08 13:46:11,257: t15.2024.05.10 val PER: 0.3967
2026-01-08 13:46:11,258: t15.2024.06.14 val PER: 0.4054
2026-01-08 13:46:11,258: t15.2024.07.19 val PER: 0.5432
2026-01-08 13:46:11,258: t15.2024.07.21 val PER: 0.3531
2026-01-08 13:46:11,258: t15.2024.07.28 val PER: 0.3735
2026-01-08 13:46:11,258: t15.2025.01.10 val PER: 0.6240
2026-01-08 13:46:11,258: t15.2025.01.12 val PER: 0.4373
2026-01-08 13:46:11,258: t15.2025.03.14 val PER: 0.6257
2026-01-08 13:46:11,258: t15.2025.03.16 val PER: 0.4738
2026-01-08 13:46:11,258: t15.2025.03.30 val PER: 0.6632
2026-01-08 13:46:11,258: t15.2025.04.13 val PER: 0.4836
2026-01-08 13:46:11,259: New best val WER(1gram) 84.52% --> 77.92%
2026-01-08 13:46:11,259: Checkpointing model
2026-01-08 13:46:11,399: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 13:46:20,041: Train batch 1600: loss: 37.83 grad norm: 78.35 time: 0.064
2026-01-08 13:46:37,946: Train batch 1800: loss: 35.38 grad norm: 67.31 time: 0.089
2026-01-08 13:46:55,494: Train batch 2000: loss: 34.93 grad norm: 68.32 time: 0.066
2026-01-08 13:46:55,495: Running test after training batch: 2000
2026-01-08 13:46:55,624: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:47:00,375: WER debug example
  GT : you can see the code at this point as well
  PR : yoor end ease the good at this boyde is will
2026-01-08 13:47:00,404: WER debug example
  GT : how does it keep the cost down
  PR : houde des it eke thus wass id
2026-01-08 13:47:01,947: Val batch 2000: PER (avg): 0.3363 CTC Loss (avg): 33.6497 WER(1gram): 71.32% (n=64) time: 6.453
2026-01-08 13:47:01,948: WER lens: avg_true_words=6.16 avg_pred_words=5.53 max_pred_words=11
2026-01-08 13:47:01,948: t15.2023.08.13 val PER: 0.3181
2026-01-08 13:47:01,948: t15.2023.08.18 val PER: 0.2741
2026-01-08 13:47:01,948: t15.2023.08.20 val PER: 0.2573
2026-01-08 13:47:01,948: t15.2023.08.25 val PER: 0.2395
2026-01-08 13:47:01,948: t15.2023.08.27 val PER: 0.3408
2026-01-08 13:47:01,948: t15.2023.09.01 val PER: 0.2313
2026-01-08 13:47:01,949: t15.2023.09.03 val PER: 0.3456
2026-01-08 13:47:01,949: t15.2023.09.24 val PER: 0.2597
2026-01-08 13:47:01,949: t15.2023.09.29 val PER: 0.2814
2026-01-08 13:47:01,949: t15.2023.10.01 val PER: 0.3421
2026-01-08 13:47:01,949: t15.2023.10.06 val PER: 0.2304
2026-01-08 13:47:01,949: t15.2023.10.08 val PER: 0.3951
2026-01-08 13:47:01,949: t15.2023.10.13 val PER: 0.3871
2026-01-08 13:47:01,949: t15.2023.10.15 val PER: 0.3065
2026-01-08 13:47:01,949: t15.2023.10.20 val PER: 0.2819
2026-01-08 13:47:01,949: t15.2023.10.22 val PER: 0.2650
2026-01-08 13:47:01,949: t15.2023.11.03 val PER: 0.3338
2026-01-08 13:47:01,950: t15.2023.11.04 val PER: 0.0922
2026-01-08 13:47:01,950: t15.2023.11.17 val PER: 0.1851
2026-01-08 13:47:01,950: t15.2023.11.19 val PER: 0.1477
2026-01-08 13:47:01,950: t15.2023.11.26 val PER: 0.3659
2026-01-08 13:47:01,950: t15.2023.12.03 val PER: 0.3193
2026-01-08 13:47:01,950: t15.2023.12.08 val PER: 0.3043
2026-01-08 13:47:01,950: t15.2023.12.10 val PER: 0.2707
2026-01-08 13:47:01,950: t15.2023.12.17 val PER: 0.3285
2026-01-08 13:47:01,950: t15.2023.12.29 val PER: 0.3404
2026-01-08 13:47:01,950: t15.2024.02.25 val PER: 0.2935
2026-01-08 13:47:01,951: t15.2024.03.08 val PER: 0.4097
2026-01-08 13:47:01,951: t15.2024.03.15 val PER: 0.3752
2026-01-08 13:47:01,951: t15.2024.03.17 val PER: 0.3501
2026-01-08 13:47:01,951: t15.2024.05.10 val PER: 0.3507
2026-01-08 13:47:01,951: t15.2024.06.14 val PER: 0.3454
2026-01-08 13:47:01,951: t15.2024.07.19 val PER: 0.4740
2026-01-08 13:47:01,951: t15.2024.07.21 val PER: 0.3062
2026-01-08 13:47:01,951: t15.2024.07.28 val PER: 0.3324
2026-01-08 13:47:01,951: t15.2025.01.10 val PER: 0.5455
2026-01-08 13:47:01,951: t15.2025.01.12 val PER: 0.3980
2026-01-08 13:47:01,951: t15.2025.03.14 val PER: 0.5503
2026-01-08 13:47:01,951: t15.2025.03.16 val PER: 0.3992
2026-01-08 13:47:01,951: t15.2025.03.30 val PER: 0.5782
2026-01-08 13:47:01,952: t15.2025.04.13 val PER: 0.4180
2026-01-08 13:47:01,952: New best val WER(1gram) 77.92% --> 71.32%
2026-01-08 13:47:01,952: Checkpointing model
2026-01-08 13:47:02,091: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 13:47:19,325: Train batch 2200: loss: 29.87 grad norm: 70.74 time: 0.061
2026-01-08 13:47:37,118: Train batch 2400: loss: 29.83 grad norm: 61.06 time: 0.053
2026-01-08 13:47:45,886: Running test after training batch: 2500
2026-01-08 13:47:46,034: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:47:51,510: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the good at this point is will
2026-01-08 13:47:51,539: WER debug example
  GT : how does it keep the cost down
  PR : houde des it keep the us it
2026-01-08 13:47:53,233: Val batch 2500: PER (avg): 0.3063 CTC Loss (avg): 30.6922 WER(1gram): 67.77% (n=64) time: 7.347
2026-01-08 13:47:53,233: WER lens: avg_true_words=6.16 avg_pred_words=5.62 max_pred_words=11
2026-01-08 13:47:53,233: t15.2023.08.13 val PER: 0.2890
2026-01-08 13:47:53,233: t15.2023.08.18 val PER: 0.2431
2026-01-08 13:47:53,233: t15.2023.08.20 val PER: 0.2375
2026-01-08 13:47:53,233: t15.2023.08.25 val PER: 0.2169
2026-01-08 13:47:53,233: t15.2023.08.27 val PER: 0.3199
2026-01-08 13:47:53,234: t15.2023.09.01 val PER: 0.2045
2026-01-08 13:47:53,234: t15.2023.09.03 val PER: 0.2862
2026-01-08 13:47:53,234: t15.2023.09.24 val PER: 0.2367
2026-01-08 13:47:53,234: t15.2023.09.29 val PER: 0.2534
2026-01-08 13:47:53,234: t15.2023.10.01 val PER: 0.3190
2026-01-08 13:47:53,234: t15.2023.10.06 val PER: 0.2121
2026-01-08 13:47:53,234: t15.2023.10.08 val PER: 0.3762
2026-01-08 13:47:53,234: t15.2023.10.13 val PER: 0.3670
2026-01-08 13:47:53,234: t15.2023.10.15 val PER: 0.2775
2026-01-08 13:47:53,234: t15.2023.10.20 val PER: 0.2919
2026-01-08 13:47:53,234: t15.2023.10.22 val PER: 0.2494
2026-01-08 13:47:53,234: t15.2023.11.03 val PER: 0.3114
2026-01-08 13:47:53,234: t15.2023.11.04 val PER: 0.0887
2026-01-08 13:47:53,234: t15.2023.11.17 val PER: 0.1540
2026-01-08 13:47:53,235: t15.2023.11.19 val PER: 0.1218
2026-01-08 13:47:53,235: t15.2023.11.26 val PER: 0.3449
2026-01-08 13:47:53,235: t15.2023.12.03 val PER: 0.3067
2026-01-08 13:47:53,235: t15.2023.12.08 val PER: 0.2723
2026-01-08 13:47:53,235: t15.2023.12.10 val PER: 0.2326
2026-01-08 13:47:53,235: t15.2023.12.17 val PER: 0.2879
2026-01-08 13:47:53,235: t15.2023.12.29 val PER: 0.3054
2026-01-08 13:47:53,235: t15.2024.02.25 val PER: 0.2500
2026-01-08 13:47:53,235: t15.2024.03.08 val PER: 0.3599
2026-01-08 13:47:53,235: t15.2024.03.15 val PER: 0.3490
2026-01-08 13:47:53,235: t15.2024.03.17 val PER: 0.3124
2026-01-08 13:47:53,235: t15.2024.05.10 val PER: 0.3135
2026-01-08 13:47:53,235: t15.2024.06.14 val PER: 0.3076
2026-01-08 13:47:53,236: t15.2024.07.19 val PER: 0.4403
2026-01-08 13:47:53,236: t15.2024.07.21 val PER: 0.2655
2026-01-08 13:47:53,236: t15.2024.07.28 val PER: 0.3037
2026-01-08 13:47:53,236: t15.2025.01.10 val PER: 0.4890
2026-01-08 13:47:53,236: t15.2025.01.12 val PER: 0.3757
2026-01-08 13:47:53,236: t15.2025.03.14 val PER: 0.5104
2026-01-08 13:47:53,236: t15.2025.03.16 val PER: 0.3613
2026-01-08 13:47:53,236: t15.2025.03.30 val PER: 0.5184
2026-01-08 13:47:53,236: t15.2025.04.13 val PER: 0.3909
2026-01-08 13:47:53,237: New best val WER(1gram) 71.32% --> 67.77%
2026-01-08 13:47:53,237: Checkpointing model
2026-01-08 13:47:53,378: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 13:48:02,655: Train batch 2600: loss: 36.22 grad norm: 85.20 time: 0.056
2026-01-08 13:48:21,455: Train batch 2800: loss: 27.12 grad norm: 71.84 time: 0.083
2026-01-08 13:48:39,611: Train batch 3000: loss: 31.92 grad norm: 69.89 time: 0.082
2026-01-08 13:48:39,611: Running test after training batch: 3000
2026-01-08 13:48:39,709: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:48:44,518: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the good at this point is will
2026-01-08 13:48:44,547: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the rost nett
2026-01-08 13:48:46,221: Val batch 3000: PER (avg): 0.2832 CTC Loss (avg): 28.2764 WER(1gram): 65.23% (n=64) time: 6.610
2026-01-08 13:48:46,222: WER lens: avg_true_words=6.16 avg_pred_words=5.75 max_pred_words=11
2026-01-08 13:48:46,222: t15.2023.08.13 val PER: 0.2682
2026-01-08 13:48:46,222: t15.2023.08.18 val PER: 0.2221
2026-01-08 13:48:46,222: t15.2023.08.20 val PER: 0.2145
2026-01-08 13:48:46,222: t15.2023.08.25 val PER: 0.2003
2026-01-08 13:48:46,222: t15.2023.08.27 val PER: 0.2894
2026-01-08 13:48:46,222: t15.2023.09.01 val PER: 0.1843
2026-01-08 13:48:46,222: t15.2023.09.03 val PER: 0.2874
2026-01-08 13:48:46,222: t15.2023.09.24 val PER: 0.2087
2026-01-08 13:48:46,223: t15.2023.09.29 val PER: 0.2348
2026-01-08 13:48:46,223: t15.2023.10.01 val PER: 0.2886
2026-01-08 13:48:46,223: t15.2023.10.06 val PER: 0.1948
2026-01-08 13:48:46,223: t15.2023.10.08 val PER: 0.3478
2026-01-08 13:48:46,223: t15.2023.10.13 val PER: 0.3344
2026-01-08 13:48:46,223: t15.2023.10.15 val PER: 0.2637
2026-01-08 13:48:46,223: t15.2023.10.20 val PER: 0.2651
2026-01-08 13:48:46,223: t15.2023.10.22 val PER: 0.2071
2026-01-08 13:48:46,223: t15.2023.11.03 val PER: 0.2788
2026-01-08 13:48:46,223: t15.2023.11.04 val PER: 0.0819
2026-01-08 13:48:46,223: t15.2023.11.17 val PER: 0.1369
2026-01-08 13:48:46,223: t15.2023.11.19 val PER: 0.1218
2026-01-08 13:48:46,223: t15.2023.11.26 val PER: 0.3101
2026-01-08 13:48:46,223: t15.2023.12.03 val PER: 0.2773
2026-01-08 13:48:46,223: t15.2023.12.08 val PER: 0.2670
2026-01-08 13:48:46,224: t15.2023.12.10 val PER: 0.2142
2026-01-08 13:48:46,224: t15.2023.12.17 val PER: 0.2827
2026-01-08 13:48:46,224: t15.2023.12.29 val PER: 0.2780
2026-01-08 13:48:46,224: t15.2024.02.25 val PER: 0.2374
2026-01-08 13:48:46,224: t15.2024.03.08 val PER: 0.3755
2026-01-08 13:48:46,224: t15.2024.03.15 val PER: 0.3290
2026-01-08 13:48:46,224: t15.2024.03.17 val PER: 0.2950
2026-01-08 13:48:46,224: t15.2024.05.10 val PER: 0.2838
2026-01-08 13:48:46,224: t15.2024.06.14 val PER: 0.2934
2026-01-08 13:48:46,224: t15.2024.07.19 val PER: 0.4127
2026-01-08 13:48:46,224: t15.2024.07.21 val PER: 0.2352
2026-01-08 13:48:46,224: t15.2024.07.28 val PER: 0.2728
2026-01-08 13:48:46,224: t15.2025.01.10 val PER: 0.4848
2026-01-08 13:48:46,224: t15.2025.01.12 val PER: 0.3272
2026-01-08 13:48:46,224: t15.2025.03.14 val PER: 0.4630
2026-01-08 13:48:46,225: t15.2025.03.16 val PER: 0.3416
2026-01-08 13:48:46,225: t15.2025.03.30 val PER: 0.4793
2026-01-08 13:48:46,225: t15.2025.04.13 val PER: 0.3623
2026-01-08 13:48:46,226: New best val WER(1gram) 67.77% --> 65.23%
2026-01-08 13:48:46,226: Checkpointing model
2026-01-08 13:48:46,368: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 13:49:03,796: Train batch 3200: loss: 28.03 grad norm: 67.63 time: 0.075
2026-01-08 13:49:21,387: Train batch 3400: loss: 19.04 grad norm: 54.38 time: 0.049
2026-01-08 13:49:30,625: Running test after training batch: 3500
2026-01-08 13:49:30,726: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:49:35,557: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e the code at this point will
2026-01-08 13:49:35,587: WER debug example
  GT : how does it keep the cost down
  PR : houde des it eke thus cussed get
2026-01-08 13:49:37,208: Val batch 3500: PER (avg): 0.2712 CTC Loss (avg): 26.9551 WER(1gram): 64.47% (n=64) time: 6.583
2026-01-08 13:49:37,209: WER lens: avg_true_words=6.16 avg_pred_words=5.84 max_pred_words=11
2026-01-08 13:49:37,209: t15.2023.08.13 val PER: 0.2484
2026-01-08 13:49:37,209: t15.2023.08.18 val PER: 0.2163
2026-01-08 13:49:37,209: t15.2023.08.20 val PER: 0.2121
2026-01-08 13:49:37,209: t15.2023.08.25 val PER: 0.2003
2026-01-08 13:49:37,209: t15.2023.08.27 val PER: 0.2846
2026-01-08 13:49:37,210: t15.2023.09.01 val PER: 0.1859
2026-01-08 13:49:37,210: t15.2023.09.03 val PER: 0.2589
2026-01-08 13:49:37,210: t15.2023.09.24 val PER: 0.2075
2026-01-08 13:49:37,210: t15.2023.09.29 val PER: 0.2227
2026-01-08 13:49:37,210: t15.2023.10.01 val PER: 0.2840
2026-01-08 13:49:37,210: t15.2023.10.06 val PER: 0.1895
2026-01-08 13:49:37,210: t15.2023.10.08 val PER: 0.3396
2026-01-08 13:49:37,210: t15.2023.10.13 val PER: 0.3251
2026-01-08 13:49:37,211: t15.2023.10.15 val PER: 0.2479
2026-01-08 13:49:37,211: t15.2023.10.20 val PER: 0.2550
2026-01-08 13:49:37,211: t15.2023.10.22 val PER: 0.2149
2026-01-08 13:49:37,211: t15.2023.11.03 val PER: 0.2680
2026-01-08 13:49:37,211: t15.2023.11.04 val PER: 0.0887
2026-01-08 13:49:37,211: t15.2023.11.17 val PER: 0.1229
2026-01-08 13:49:37,211: t15.2023.11.19 val PER: 0.1138
2026-01-08 13:49:37,211: t15.2023.11.26 val PER: 0.2797
2026-01-08 13:49:37,211: t15.2023.12.03 val PER: 0.2468
2026-01-08 13:49:37,211: t15.2023.12.08 val PER: 0.2457
2026-01-08 13:49:37,211: t15.2023.12.10 val PER: 0.1919
2026-01-08 13:49:37,211: t15.2023.12.17 val PER: 0.2599
2026-01-08 13:49:37,212: t15.2023.12.29 val PER: 0.2656
2026-01-08 13:49:37,212: t15.2024.02.25 val PER: 0.2191
2026-01-08 13:49:37,212: t15.2024.03.08 val PER: 0.3357
2026-01-08 13:49:37,212: t15.2024.03.15 val PER: 0.3221
2026-01-08 13:49:37,212: t15.2024.03.17 val PER: 0.2755
2026-01-08 13:49:37,212: t15.2024.05.10 val PER: 0.2853
2026-01-08 13:49:37,212: t15.2024.06.14 val PER: 0.2839
2026-01-08 13:49:37,212: t15.2024.07.19 val PER: 0.3975
2026-01-08 13:49:37,212: t15.2024.07.21 val PER: 0.2214
2026-01-08 13:49:37,212: t15.2024.07.28 val PER: 0.2816
2026-01-08 13:49:37,212: t15.2025.01.10 val PER: 0.4766
2026-01-08 13:49:37,212: t15.2025.01.12 val PER: 0.3041
2026-01-08 13:49:37,212: t15.2025.03.14 val PER: 0.4512
2026-01-08 13:49:37,212: t15.2025.03.16 val PER: 0.3194
2026-01-08 13:49:37,212: t15.2025.03.30 val PER: 0.4897
2026-01-08 13:49:37,212: t15.2025.04.13 val PER: 0.3352
2026-01-08 13:49:37,213: New best val WER(1gram) 65.23% --> 64.47%
2026-01-08 13:49:37,213: Checkpointing model
2026-01-08 13:49:37,356: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 13:49:45,837: Train batch 3600: loss: 22.67 grad norm: 61.50 time: 0.067
2026-01-08 13:50:03,064: Train batch 3800: loss: 26.15 grad norm: 69.35 time: 0.067
2026-01-08 13:50:20,778: Train batch 4000: loss: 19.84 grad norm: 52.61 time: 0.056
2026-01-08 13:50:20,778: Running test after training batch: 4000
2026-01-08 13:50:20,925: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:50:25,843: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-08 13:50:25,871: WER debug example
  GT : how does it keep the cost down
  PR : how des it keep the cussed nett
2026-01-08 13:50:27,545: Val batch 4000: PER (avg): 0.2536 CTC Loss (avg): 24.9544 WER(1gram): 63.96% (n=64) time: 6.766
2026-01-08 13:50:27,545: WER lens: avg_true_words=6.16 avg_pred_words=5.94 max_pred_words=11
2026-01-08 13:50:27,545: t15.2023.08.13 val PER: 0.2214
2026-01-08 13:50:27,545: t15.2023.08.18 val PER: 0.2079
2026-01-08 13:50:27,546: t15.2023.08.20 val PER: 0.2025
2026-01-08 13:50:27,546: t15.2023.08.25 val PER: 0.1627
2026-01-08 13:50:27,546: t15.2023.08.27 val PER: 0.2814
2026-01-08 13:50:27,546: t15.2023.09.01 val PER: 0.1575
2026-01-08 13:50:27,546: t15.2023.09.03 val PER: 0.2542
2026-01-08 13:50:27,546: t15.2023.09.24 val PER: 0.1869
2026-01-08 13:50:27,546: t15.2023.09.29 val PER: 0.2068
2026-01-08 13:50:27,546: t15.2023.10.01 val PER: 0.2695
2026-01-08 13:50:27,546: t15.2023.10.06 val PER: 0.1798
2026-01-08 13:50:27,546: t15.2023.10.08 val PER: 0.3153
2026-01-08 13:50:27,546: t15.2023.10.13 val PER: 0.3072
2026-01-08 13:50:27,546: t15.2023.10.15 val PER: 0.2426
2026-01-08 13:50:27,546: t15.2023.10.20 val PER: 0.2450
2026-01-08 13:50:27,546: t15.2023.10.22 val PER: 0.2060
2026-01-08 13:50:27,546: t15.2023.11.03 val PER: 0.2422
2026-01-08 13:50:27,547: t15.2023.11.04 val PER: 0.0683
2026-01-08 13:50:27,547: t15.2023.11.17 val PER: 0.1026
2026-01-08 13:50:27,547: t15.2023.11.19 val PER: 0.0998
2026-01-08 13:50:27,547: t15.2023.11.26 val PER: 0.2790
2026-01-08 13:50:27,547: t15.2023.12.03 val PER: 0.2384
2026-01-08 13:50:27,547: t15.2023.12.08 val PER: 0.2217
2026-01-08 13:50:27,547: t15.2023.12.10 val PER: 0.1853
2026-01-08 13:50:27,547: t15.2023.12.17 val PER: 0.2443
2026-01-08 13:50:27,548: t15.2023.12.29 val PER: 0.2539
2026-01-08 13:50:27,548: t15.2024.02.25 val PER: 0.2107
2026-01-08 13:50:27,548: t15.2024.03.08 val PER: 0.3300
2026-01-08 13:50:27,548: t15.2024.03.15 val PER: 0.3064
2026-01-08 13:50:27,548: t15.2024.03.17 val PER: 0.2573
2026-01-08 13:50:27,548: t15.2024.05.10 val PER: 0.2615
2026-01-08 13:50:27,548: t15.2024.06.14 val PER: 0.2618
2026-01-08 13:50:27,548: t15.2024.07.19 val PER: 0.3724
2026-01-08 13:50:27,548: t15.2024.07.21 val PER: 0.1917
2026-01-08 13:50:27,548: t15.2024.07.28 val PER: 0.2449
2026-01-08 13:50:27,548: t15.2025.01.10 val PER: 0.4229
2026-01-08 13:50:27,548: t15.2025.01.12 val PER: 0.2833
2026-01-08 13:50:27,548: t15.2025.03.14 val PER: 0.4423
2026-01-08 13:50:27,549: t15.2025.03.16 val PER: 0.3272
2026-01-08 13:50:27,549: t15.2025.03.30 val PER: 0.4345
2026-01-08 13:50:27,549: t15.2025.04.13 val PER: 0.3409
2026-01-08 13:50:27,550: New best val WER(1gram) 64.47% --> 63.96%
2026-01-08 13:50:27,550: Checkpointing model
2026-01-08 13:50:27,694: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 13:50:45,495: Train batch 4200: loss: 22.55 grad norm: 67.09 time: 0.080
2026-01-08 13:51:02,830: Train batch 4400: loss: 17.40 grad norm: 55.24 time: 0.067
2026-01-08 13:51:11,718: Running test after training batch: 4500
2026-01-08 13:51:11,848: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:51:16,726: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-08 13:51:16,762: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cussed get
2026-01-08 13:51:18,755: Val batch 4500: PER (avg): 0.2434 CTC Loss (avg): 23.6589 WER(1gram): 61.42% (n=64) time: 7.037
2026-01-08 13:51:18,756: WER lens: avg_true_words=6.16 avg_pred_words=6.00 max_pred_words=11
2026-01-08 13:51:18,756: t15.2023.08.13 val PER: 0.2193
2026-01-08 13:51:18,756: t15.2023.08.18 val PER: 0.1861
2026-01-08 13:51:18,756: t15.2023.08.20 val PER: 0.2025
2026-01-08 13:51:18,756: t15.2023.08.25 val PER: 0.1627
2026-01-08 13:51:18,756: t15.2023.08.27 val PER: 0.2637
2026-01-08 13:51:18,756: t15.2023.09.01 val PER: 0.1648
2026-01-08 13:51:18,756: t15.2023.09.03 val PER: 0.2423
2026-01-08 13:51:18,756: t15.2023.09.24 val PER: 0.1905
2026-01-08 13:51:18,757: t15.2023.09.29 val PER: 0.2004
2026-01-08 13:51:18,757: t15.2023.10.01 val PER: 0.2708
2026-01-08 13:51:18,757: t15.2023.10.06 val PER: 0.1518
2026-01-08 13:51:18,757: t15.2023.10.08 val PER: 0.3248
2026-01-08 13:51:18,757: t15.2023.10.13 val PER: 0.3018
2026-01-08 13:51:18,757: t15.2023.10.15 val PER: 0.2353
2026-01-08 13:51:18,757: t15.2023.10.20 val PER: 0.2282
2026-01-08 13:51:18,757: t15.2023.10.22 val PER: 0.1960
2026-01-08 13:51:18,757: t15.2023.11.03 val PER: 0.2537
2026-01-08 13:51:18,757: t15.2023.11.04 val PER: 0.0580
2026-01-08 13:51:18,757: t15.2023.11.17 val PER: 0.0933
2026-01-08 13:51:18,757: t15.2023.11.19 val PER: 0.0978
2026-01-08 13:51:18,757: t15.2023.11.26 val PER: 0.2616
2026-01-08 13:51:18,758: t15.2023.12.03 val PER: 0.2143
2026-01-08 13:51:18,758: t15.2023.12.08 val PER: 0.2164
2026-01-08 13:51:18,758: t15.2023.12.10 val PER: 0.1721
2026-01-08 13:51:18,758: t15.2023.12.17 val PER: 0.2443
2026-01-08 13:51:18,758: t15.2023.12.29 val PER: 0.2388
2026-01-08 13:51:18,758: t15.2024.02.25 val PER: 0.1896
2026-01-08 13:51:18,758: t15.2024.03.08 val PER: 0.3243
2026-01-08 13:51:18,758: t15.2024.03.15 val PER: 0.2939
2026-01-08 13:51:18,758: t15.2024.03.17 val PER: 0.2406
2026-01-08 13:51:18,758: t15.2024.05.10 val PER: 0.2541
2026-01-08 13:51:18,758: t15.2024.06.14 val PER: 0.2429
2026-01-08 13:51:18,758: t15.2024.07.19 val PER: 0.3434
2026-01-08 13:51:18,758: t15.2024.07.21 val PER: 0.1738
2026-01-08 13:51:18,758: t15.2024.07.28 val PER: 0.2324
2026-01-08 13:51:18,758: t15.2025.01.10 val PER: 0.4174
2026-01-08 13:51:18,758: t15.2025.01.12 val PER: 0.2764
2026-01-08 13:51:18,758: t15.2025.03.14 val PER: 0.4112
2026-01-08 13:51:18,759: t15.2025.03.16 val PER: 0.3128
2026-01-08 13:51:18,759: t15.2025.03.30 val PER: 0.4276
2026-01-08 13:51:18,759: t15.2025.04.13 val PER: 0.2996
2026-01-08 13:51:18,760: New best val WER(1gram) 63.96% --> 61.42%
2026-01-08 13:51:18,760: Checkpointing model
2026-01-08 13:51:18,896: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 13:51:28,240: Train batch 4600: loss: 21.75 grad norm: 64.67 time: 0.063
2026-01-08 13:51:46,616: Train batch 4800: loss: 14.95 grad norm: 55.46 time: 0.063
2026-01-08 13:52:04,111: Train batch 5000: loss: 32.39 grad norm: 82.35 time: 0.065
2026-01-08 13:52:04,111: Running test after training batch: 5000
2026-01-08 13:52:04,212: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:52:09,278: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-08 13:52:09,308: WER debug example
  GT : how does it keep the cost down
  PR : houde des it keep the cost nett
2026-01-08 13:52:10,945: Val batch 5000: PER (avg): 0.2290 CTC Loss (avg): 22.5466 WER(1gram): 62.94% (n=64) time: 6.834
2026-01-08 13:52:10,946: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-08 13:52:10,946: t15.2023.08.13 val PER: 0.2141
2026-01-08 13:52:10,946: t15.2023.08.18 val PER: 0.1718
2026-01-08 13:52:10,946: t15.2023.08.20 val PER: 0.1692
2026-01-08 13:52:10,946: t15.2023.08.25 val PER: 0.1370
2026-01-08 13:52:10,946: t15.2023.08.27 val PER: 0.2540
2026-01-08 13:52:10,946: t15.2023.09.01 val PER: 0.1412
2026-01-08 13:52:10,946: t15.2023.09.03 val PER: 0.2328
2026-01-08 13:52:10,947: t15.2023.09.24 val PER: 0.1833
2026-01-08 13:52:10,947: t15.2023.09.29 val PER: 0.1838
2026-01-08 13:52:10,947: t15.2023.10.01 val PER: 0.2365
2026-01-08 13:52:10,947: t15.2023.10.06 val PER: 0.1421
2026-01-08 13:52:10,947: t15.2023.10.08 val PER: 0.3018
2026-01-08 13:52:10,947: t15.2023.10.13 val PER: 0.2948
2026-01-08 13:52:10,947: t15.2023.10.15 val PER: 0.2274
2026-01-08 13:52:10,947: t15.2023.10.20 val PER: 0.2450
2026-01-08 13:52:10,947: t15.2023.10.22 val PER: 0.1804
2026-01-08 13:52:10,947: t15.2023.11.03 val PER: 0.2300
2026-01-08 13:52:10,948: t15.2023.11.04 val PER: 0.0512
2026-01-08 13:52:10,948: t15.2023.11.17 val PER: 0.0809
2026-01-08 13:52:10,948: t15.2023.11.19 val PER: 0.0818
2026-01-08 13:52:10,948: t15.2023.11.26 val PER: 0.2478
2026-01-08 13:52:10,948: t15.2023.12.03 val PER: 0.2059
2026-01-08 13:52:10,948: t15.2023.12.08 val PER: 0.2017
2026-01-08 13:52:10,948: t15.2023.12.10 val PER: 0.1603
2026-01-08 13:52:10,948: t15.2023.12.17 val PER: 0.2256
2026-01-08 13:52:10,948: t15.2023.12.29 val PER: 0.2409
2026-01-08 13:52:10,948: t15.2024.02.25 val PER: 0.1742
2026-01-08 13:52:10,948: t15.2024.03.08 val PER: 0.3158
2026-01-08 13:52:10,948: t15.2024.03.15 val PER: 0.2877
2026-01-08 13:52:10,948: t15.2024.03.17 val PER: 0.2357
2026-01-08 13:52:10,949: t15.2024.05.10 val PER: 0.2392
2026-01-08 13:52:10,949: t15.2024.06.14 val PER: 0.2492
2026-01-08 13:52:10,949: t15.2024.07.19 val PER: 0.3309
2026-01-08 13:52:10,949: t15.2024.07.21 val PER: 0.1759
2026-01-08 13:52:10,949: t15.2024.07.28 val PER: 0.2176
2026-01-08 13:52:10,949: t15.2025.01.10 val PER: 0.3953
2026-01-08 13:52:10,949: t15.2025.01.12 val PER: 0.2494
2026-01-08 13:52:10,949: t15.2025.03.14 val PER: 0.3861
2026-01-08 13:52:10,949: t15.2025.03.16 val PER: 0.2657
2026-01-08 13:52:10,949: t15.2025.03.30 val PER: 0.3874
2026-01-08 13:52:10,949: t15.2025.04.13 val PER: 0.3096
2026-01-08 13:52:27,751: Train batch 5200: loss: 17.84 grad norm: 62.37 time: 0.052
2026-01-08 13:52:44,590: Train batch 5400: loss: 18.60 grad norm: 58.14 time: 0.069
2026-01-08 13:52:53,276: Running test after training batch: 5500
2026-01-08 13:52:53,378: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:52:58,099: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point will
2026-01-08 13:52:58,128: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cussed nett
2026-01-08 13:52:59,774: Val batch 5500: PER (avg): 0.2197 CTC Loss (avg): 21.4483 WER(1gram): 58.63% (n=64) time: 6.497
2026-01-08 13:52:59,774: WER lens: avg_true_words=6.16 avg_pred_words=5.97 max_pred_words=11
2026-01-08 13:52:59,774: t15.2023.08.13 val PER: 0.1840
2026-01-08 13:52:59,775: t15.2023.08.18 val PER: 0.1660
2026-01-08 13:52:59,775: t15.2023.08.20 val PER: 0.1700
2026-01-08 13:52:59,775: t15.2023.08.25 val PER: 0.1310
2026-01-08 13:52:59,775: t15.2023.08.27 val PER: 0.2476
2026-01-08 13:52:59,775: t15.2023.09.01 val PER: 0.1364
2026-01-08 13:52:59,775: t15.2023.09.03 val PER: 0.2197
2026-01-08 13:52:59,775: t15.2023.09.24 val PER: 0.1711
2026-01-08 13:52:59,775: t15.2023.09.29 val PER: 0.1844
2026-01-08 13:52:59,775: t15.2023.10.01 val PER: 0.2345
2026-01-08 13:52:59,775: t15.2023.10.06 val PER: 0.1389
2026-01-08 13:52:59,775: t15.2023.10.08 val PER: 0.2950
2026-01-08 13:52:59,775: t15.2023.10.13 val PER: 0.2839
2026-01-08 13:52:59,775: t15.2023.10.15 val PER: 0.2142
2026-01-08 13:52:59,776: t15.2023.10.20 val PER: 0.2550
2026-01-08 13:52:59,776: t15.2023.10.22 val PER: 0.1748
2026-01-08 13:52:59,776: t15.2023.11.03 val PER: 0.2273
2026-01-08 13:52:59,776: t15.2023.11.04 val PER: 0.0648
2026-01-08 13:52:59,776: t15.2023.11.17 val PER: 0.0871
2026-01-08 13:52:59,776: t15.2023.11.19 val PER: 0.0778
2026-01-08 13:52:59,776: t15.2023.11.26 val PER: 0.2326
2026-01-08 13:52:59,776: t15.2023.12.03 val PER: 0.1775
2026-01-08 13:52:59,776: t15.2023.12.08 val PER: 0.1951
2026-01-08 13:52:59,776: t15.2023.12.10 val PER: 0.1682
2026-01-08 13:52:59,776: t15.2023.12.17 val PER: 0.2193
2026-01-08 13:52:59,776: t15.2023.12.29 val PER: 0.2100
2026-01-08 13:52:59,776: t15.2024.02.25 val PER: 0.1812
2026-01-08 13:52:59,776: t15.2024.03.08 val PER: 0.2930
2026-01-08 13:52:59,777: t15.2024.03.15 val PER: 0.2664
2026-01-08 13:52:59,777: t15.2024.03.17 val PER: 0.2099
2026-01-08 13:52:59,777: t15.2024.05.10 val PER: 0.2422
2026-01-08 13:52:59,777: t15.2024.06.14 val PER: 0.2429
2026-01-08 13:52:59,777: t15.2024.07.19 val PER: 0.3250
2026-01-08 13:52:59,777: t15.2024.07.21 val PER: 0.1628
2026-01-08 13:52:59,777: t15.2024.07.28 val PER: 0.2154
2026-01-08 13:52:59,777: t15.2025.01.10 val PER: 0.3953
2026-01-08 13:52:59,777: t15.2025.01.12 val PER: 0.2394
2026-01-08 13:52:59,777: t15.2025.03.14 val PER: 0.3624
2026-01-08 13:52:59,777: t15.2025.03.16 val PER: 0.2788
2026-01-08 13:52:59,777: t15.2025.03.30 val PER: 0.3540
2026-01-08 13:52:59,777: t15.2025.04.13 val PER: 0.3024
2026-01-08 13:52:59,779: New best val WER(1gram) 61.42% --> 58.63%
2026-01-08 13:52:59,779: Checkpointing model
2026-01-08 13:52:59,920: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 13:53:09,001: Train batch 5600: loss: 19.17 grad norm: 61.65 time: 0.062
2026-01-08 13:53:26,671: Train batch 5800: loss: 14.16 grad norm: 54.12 time: 0.083
2026-01-08 13:53:44,466: Train batch 6000: loss: 15.66 grad norm: 60.90 time: 0.049
2026-01-08 13:53:44,466: Running test after training batch: 6000
2026-01-08 13:53:44,585: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:53:49,352: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the good at this point is will
2026-01-08 13:53:49,382: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-08 13:53:51,038: Val batch 6000: PER (avg): 0.2157 CTC Loss (avg): 21.2759 WER(1gram): 58.38% (n=64) time: 6.571
2026-01-08 13:53:51,038: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-08 13:53:51,038: t15.2023.08.13 val PER: 0.1892
2026-01-08 13:53:51,038: t15.2023.08.18 val PER: 0.1702
2026-01-08 13:53:51,038: t15.2023.08.20 val PER: 0.1612
2026-01-08 13:53:51,038: t15.2023.08.25 val PER: 0.1295
2026-01-08 13:53:51,038: t15.2023.08.27 val PER: 0.2540
2026-01-08 13:53:51,039: t15.2023.09.01 val PER: 0.1282
2026-01-08 13:53:51,039: t15.2023.09.03 val PER: 0.2197
2026-01-08 13:53:51,039: t15.2023.09.24 val PER: 0.1614
2026-01-08 13:53:51,039: t15.2023.09.29 val PER: 0.1749
2026-01-08 13:53:51,039: t15.2023.10.01 val PER: 0.2279
2026-01-08 13:53:51,039: t15.2023.10.06 val PER: 0.1475
2026-01-08 13:53:51,039: t15.2023.10.08 val PER: 0.3045
2026-01-08 13:53:51,039: t15.2023.10.13 val PER: 0.2731
2026-01-08 13:53:51,039: t15.2023.10.15 val PER: 0.2175
2026-01-08 13:53:51,039: t15.2023.10.20 val PER: 0.2181
2026-01-08 13:53:51,039: t15.2023.10.22 val PER: 0.1670
2026-01-08 13:53:51,039: t15.2023.11.03 val PER: 0.2341
2026-01-08 13:53:51,039: t15.2023.11.04 val PER: 0.0512
2026-01-08 13:53:51,039: t15.2023.11.17 val PER: 0.0871
2026-01-08 13:53:51,039: t15.2023.11.19 val PER: 0.0838
2026-01-08 13:53:51,040: t15.2023.11.26 val PER: 0.2210
2026-01-08 13:53:51,040: t15.2023.12.03 val PER: 0.1796
2026-01-08 13:53:51,040: t15.2023.12.08 val PER: 0.1771
2026-01-08 13:53:51,040: t15.2023.12.10 val PER: 0.1603
2026-01-08 13:53:51,040: t15.2023.12.17 val PER: 0.2100
2026-01-08 13:53:51,040: t15.2023.12.29 val PER: 0.2162
2026-01-08 13:53:51,040: t15.2024.02.25 val PER: 0.1657
2026-01-08 13:53:51,040: t15.2024.03.08 val PER: 0.2973
2026-01-08 13:53:51,040: t15.2024.03.15 val PER: 0.2620
2026-01-08 13:53:51,040: t15.2024.03.17 val PER: 0.2113
2026-01-08 13:53:51,040: t15.2024.05.10 val PER: 0.2155
2026-01-08 13:53:51,040: t15.2024.06.14 val PER: 0.2177
2026-01-08 13:53:51,040: t15.2024.07.19 val PER: 0.3276
2026-01-08 13:53:51,040: t15.2024.07.21 val PER: 0.1697
2026-01-08 13:53:51,040: t15.2024.07.28 val PER: 0.2096
2026-01-08 13:53:51,040: t15.2025.01.10 val PER: 0.3857
2026-01-08 13:53:51,041: t15.2025.01.12 val PER: 0.2309
2026-01-08 13:53:51,041: t15.2025.03.14 val PER: 0.3846
2026-01-08 13:53:51,041: t15.2025.03.16 val PER: 0.2683
2026-01-08 13:53:51,041: t15.2025.03.30 val PER: 0.3586
2026-01-08 13:53:51,041: t15.2025.04.13 val PER: 0.2639
2026-01-08 13:53:51,042: New best val WER(1gram) 58.63% --> 58.38%
2026-01-08 13:53:51,042: Checkpointing model
2026-01-08 13:53:51,268: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 13:54:09,782: Train batch 6200: loss: 17.46 grad norm: 56.48 time: 0.073
2026-01-08 13:54:28,285: Train batch 6400: loss: 20.05 grad norm: 63.19 time: 0.064
2026-01-08 13:54:37,290: Running test after training batch: 6500
2026-01-08 13:54:37,446: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:54:42,154: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point is will
2026-01-08 13:54:42,183: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ned
2026-01-08 13:54:43,784: Val batch 6500: PER (avg): 0.2063 CTC Loss (avg): 20.5770 WER(1gram): 54.57% (n=64) time: 6.494
2026-01-08 13:54:43,784: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-08 13:54:43,784: t15.2023.08.13 val PER: 0.1778
2026-01-08 13:54:43,785: t15.2023.08.18 val PER: 0.1542
2026-01-08 13:54:43,785: t15.2023.08.20 val PER: 0.1509
2026-01-08 13:54:43,785: t15.2023.08.25 val PER: 0.1175
2026-01-08 13:54:43,785: t15.2023.08.27 val PER: 0.2412
2026-01-08 13:54:43,785: t15.2023.09.01 val PER: 0.1128
2026-01-08 13:54:43,785: t15.2023.09.03 val PER: 0.2043
2026-01-08 13:54:43,785: t15.2023.09.24 val PER: 0.1748
2026-01-08 13:54:43,785: t15.2023.09.29 val PER: 0.1621
2026-01-08 13:54:43,785: t15.2023.10.01 val PER: 0.2153
2026-01-08 13:54:43,785: t15.2023.10.06 val PER: 0.1410
2026-01-08 13:54:43,785: t15.2023.10.08 val PER: 0.2950
2026-01-08 13:54:43,785: t15.2023.10.13 val PER: 0.2723
2026-01-08 13:54:43,785: t15.2023.10.15 val PER: 0.2103
2026-01-08 13:54:43,785: t15.2023.10.20 val PER: 0.2349
2026-01-08 13:54:43,786: t15.2023.10.22 val PER: 0.1581
2026-01-08 13:54:43,786: t15.2023.11.03 val PER: 0.2218
2026-01-08 13:54:43,786: t15.2023.11.04 val PER: 0.0580
2026-01-08 13:54:43,786: t15.2023.11.17 val PER: 0.0684
2026-01-08 13:54:43,786: t15.2023.11.19 val PER: 0.0818
2026-01-08 13:54:43,786: t15.2023.11.26 val PER: 0.2065
2026-01-08 13:54:43,786: t15.2023.12.03 val PER: 0.1733
2026-01-08 13:54:43,786: t15.2023.12.08 val PER: 0.1651
2026-01-08 13:54:43,786: t15.2023.12.10 val PER: 0.1419
2026-01-08 13:54:43,786: t15.2023.12.17 val PER: 0.1913
2026-01-08 13:54:43,786: t15.2023.12.29 val PER: 0.2148
2026-01-08 13:54:43,786: t15.2024.02.25 val PER: 0.1685
2026-01-08 13:54:43,786: t15.2024.03.08 val PER: 0.2959
2026-01-08 13:54:43,786: t15.2024.03.15 val PER: 0.2627
2026-01-08 13:54:43,786: t15.2024.03.17 val PER: 0.2043
2026-01-08 13:54:43,786: t15.2024.05.10 val PER: 0.2259
2026-01-08 13:54:43,786: t15.2024.06.14 val PER: 0.2129
2026-01-08 13:54:43,787: t15.2024.07.19 val PER: 0.3118
2026-01-08 13:54:43,787: t15.2024.07.21 val PER: 0.1400
2026-01-08 13:54:43,787: t15.2024.07.28 val PER: 0.1985
2026-01-08 13:54:43,787: t15.2025.01.10 val PER: 0.3705
2026-01-08 13:54:43,787: t15.2025.01.12 val PER: 0.2148
2026-01-08 13:54:43,787: t15.2025.03.14 val PER: 0.3802
2026-01-08 13:54:43,787: t15.2025.03.16 val PER: 0.2461
2026-01-08 13:54:43,787: t15.2025.03.30 val PER: 0.3609
2026-01-08 13:54:43,787: t15.2025.04.13 val PER: 0.2668
2026-01-08 13:54:43,789: New best val WER(1gram) 58.38% --> 54.57%
2026-01-08 13:54:43,789: Checkpointing model
2026-01-08 13:54:43,931: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 13:54:52,939: Train batch 6600: loss: 12.35 grad norm: 49.73 time: 0.046
2026-01-08 13:55:10,081: Train batch 6800: loss: 15.95 grad norm: 55.66 time: 0.049
2026-01-08 13:55:28,254: Train batch 7000: loss: 17.27 grad norm: 61.49 time: 0.064
2026-01-08 13:55:28,254: Running test after training batch: 7000
2026-01-08 13:55:28,411: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:55:33,139: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-08 13:55:33,168: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-08 13:55:34,795: Val batch 7000: PER (avg): 0.1972 CTC Loss (avg): 19.4904 WER(1gram): 55.84% (n=64) time: 6.541
2026-01-08 13:55:34,795: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-08 13:55:34,796: t15.2023.08.13 val PER: 0.1674
2026-01-08 13:55:34,796: t15.2023.08.18 val PER: 0.1551
2026-01-08 13:55:34,796: t15.2023.08.20 val PER: 0.1581
2026-01-08 13:55:34,796: t15.2023.08.25 val PER: 0.1084
2026-01-08 13:55:34,796: t15.2023.08.27 val PER: 0.2251
2026-01-08 13:55:34,796: t15.2023.09.01 val PER: 0.1144
2026-01-08 13:55:34,796: t15.2023.09.03 val PER: 0.1960
2026-01-08 13:55:34,796: t15.2023.09.24 val PER: 0.1541
2026-01-08 13:55:34,796: t15.2023.09.29 val PER: 0.1691
2026-01-08 13:55:34,796: t15.2023.10.01 val PER: 0.2074
2026-01-08 13:55:34,796: t15.2023.10.06 val PER: 0.1249
2026-01-08 13:55:34,797: t15.2023.10.08 val PER: 0.2855
2026-01-08 13:55:34,797: t15.2023.10.13 val PER: 0.2498
2026-01-08 13:55:34,797: t15.2023.10.15 val PER: 0.1964
2026-01-08 13:55:34,797: t15.2023.10.20 val PER: 0.2047
2026-01-08 13:55:34,797: t15.2023.10.22 val PER: 0.1448
2026-01-08 13:55:34,797: t15.2023.11.03 val PER: 0.2144
2026-01-08 13:55:34,797: t15.2023.11.04 val PER: 0.0478
2026-01-08 13:55:34,797: t15.2023.11.17 val PER: 0.0747
2026-01-08 13:55:34,797: t15.2023.11.19 val PER: 0.0639
2026-01-08 13:55:34,797: t15.2023.11.26 val PER: 0.1957
2026-01-08 13:55:34,797: t15.2023.12.03 val PER: 0.1670
2026-01-08 13:55:34,798: t15.2023.12.08 val PER: 0.1578
2026-01-08 13:55:34,798: t15.2023.12.10 val PER: 0.1406
2026-01-08 13:55:34,798: t15.2023.12.17 val PER: 0.1902
2026-01-08 13:55:34,798: t15.2023.12.29 val PER: 0.1874
2026-01-08 13:55:34,798: t15.2024.02.25 val PER: 0.1601
2026-01-08 13:55:34,798: t15.2024.03.08 val PER: 0.2845
2026-01-08 13:55:34,798: t15.2024.03.15 val PER: 0.2527
2026-01-08 13:55:34,798: t15.2024.03.17 val PER: 0.1980
2026-01-08 13:55:34,798: t15.2024.05.10 val PER: 0.1991
2026-01-08 13:55:34,798: t15.2024.06.14 val PER: 0.2114
2026-01-08 13:55:34,798: t15.2024.07.19 val PER: 0.2920
2026-01-08 13:55:34,798: t15.2024.07.21 val PER: 0.1352
2026-01-08 13:55:34,798: t15.2024.07.28 val PER: 0.1838
2026-01-08 13:55:34,798: t15.2025.01.10 val PER: 0.3609
2026-01-08 13:55:34,798: t15.2025.01.12 val PER: 0.2102
2026-01-08 13:55:34,798: t15.2025.03.14 val PER: 0.3683
2026-01-08 13:55:34,798: t15.2025.03.16 val PER: 0.2304
2026-01-08 13:55:34,799: t15.2025.03.30 val PER: 0.3598
2026-01-08 13:55:34,799: t15.2025.04.13 val PER: 0.2653
2026-01-08 13:55:52,409: Train batch 7200: loss: 14.55 grad norm: 58.92 time: 0.079
2026-01-08 13:56:11,351: Train batch 7400: loss: 14.49 grad norm: 57.82 time: 0.075
2026-01-08 13:56:19,957: Running test after training batch: 7500
2026-01-08 13:56:20,272: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:56:25,043: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point is will
2026-01-08 13:56:25,072: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cussed ned
2026-01-08 13:56:26,718: Val batch 7500: PER (avg): 0.1901 CTC Loss (avg): 18.8470 WER(1gram): 55.58% (n=64) time: 6.761
2026-01-08 13:56:26,718: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-08 13:56:26,718: t15.2023.08.13 val PER: 0.1559
2026-01-08 13:56:26,718: t15.2023.08.18 val PER: 0.1442
2026-01-08 13:56:26,718: t15.2023.08.20 val PER: 0.1469
2026-01-08 13:56:26,718: t15.2023.08.25 val PER: 0.1175
2026-01-08 13:56:26,719: t15.2023.08.27 val PER: 0.2219
2026-01-08 13:56:26,719: t15.2023.09.01 val PER: 0.1120
2026-01-08 13:56:26,719: t15.2023.09.03 val PER: 0.1936
2026-01-08 13:56:26,719: t15.2023.09.24 val PER: 0.1638
2026-01-08 13:56:26,719: t15.2023.09.29 val PER: 0.1615
2026-01-08 13:56:26,719: t15.2023.10.01 val PER: 0.2001
2026-01-08 13:56:26,719: t15.2023.10.06 val PER: 0.1227
2026-01-08 13:56:26,719: t15.2023.10.08 val PER: 0.2612
2026-01-08 13:56:26,719: t15.2023.10.13 val PER: 0.2490
2026-01-08 13:56:26,720: t15.2023.10.15 val PER: 0.1872
2026-01-08 13:56:26,720: t15.2023.10.20 val PER: 0.2081
2026-01-08 13:56:26,720: t15.2023.10.22 val PER: 0.1403
2026-01-08 13:56:26,720: t15.2023.11.03 val PER: 0.2076
2026-01-08 13:56:26,720: t15.2023.11.04 val PER: 0.0444
2026-01-08 13:56:26,720: t15.2023.11.17 val PER: 0.0622
2026-01-08 13:56:26,720: t15.2023.11.19 val PER: 0.0639
2026-01-08 13:56:26,720: t15.2023.11.26 val PER: 0.1906
2026-01-08 13:56:26,720: t15.2023.12.03 val PER: 0.1628
2026-01-08 13:56:26,720: t15.2023.12.08 val PER: 0.1431
2026-01-08 13:56:26,720: t15.2023.12.10 val PER: 0.1314
2026-01-08 13:56:26,720: t15.2023.12.17 val PER: 0.1778
2026-01-08 13:56:26,720: t15.2023.12.29 val PER: 0.1860
2026-01-08 13:56:26,720: t15.2024.02.25 val PER: 0.1545
2026-01-08 13:56:26,720: t15.2024.03.08 val PER: 0.2788
2026-01-08 13:56:26,721: t15.2024.03.15 val PER: 0.2402
2026-01-08 13:56:26,721: t15.2024.03.17 val PER: 0.1827
2026-01-08 13:56:26,721: t15.2024.05.10 val PER: 0.2021
2026-01-08 13:56:26,721: t15.2024.06.14 val PER: 0.2066
2026-01-08 13:56:26,721: t15.2024.07.19 val PER: 0.2848
2026-01-08 13:56:26,721: t15.2024.07.21 val PER: 0.1345
2026-01-08 13:56:26,721: t15.2024.07.28 val PER: 0.1662
2026-01-08 13:56:26,721: t15.2025.01.10 val PER: 0.3609
2026-01-08 13:56:26,721: t15.2025.01.12 val PER: 0.1824
2026-01-08 13:56:26,721: t15.2025.03.14 val PER: 0.3817
2026-01-08 13:56:26,721: t15.2025.03.16 val PER: 0.2395
2026-01-08 13:56:26,722: t15.2025.03.30 val PER: 0.3437
2026-01-08 13:56:26,722: t15.2025.04.13 val PER: 0.2496
2026-01-08 13:56:35,381: Train batch 7600: loss: 15.91 grad norm: 61.64 time: 0.069
2026-01-08 13:56:52,871: Train batch 7800: loss: 14.88 grad norm: 57.70 time: 0.057
2026-01-08 13:57:10,885: Train batch 8000: loss: 11.99 grad norm: 54.10 time: 0.072
2026-01-08 13:57:10,885: Running test after training batch: 8000
2026-01-08 13:57:10,987: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:57:15,710: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e the code at this point is will
2026-01-08 13:57:15,740: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cussed ned
2026-01-08 13:57:17,409: Val batch 8000: PER (avg): 0.1863 CTC Loss (avg): 18.3314 WER(1gram): 54.31% (n=64) time: 6.524
2026-01-08 13:57:17,409: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=12
2026-01-08 13:57:17,409: t15.2023.08.13 val PER: 0.1497
2026-01-08 13:57:17,409: t15.2023.08.18 val PER: 0.1366
2026-01-08 13:57:17,410: t15.2023.08.20 val PER: 0.1406
2026-01-08 13:57:17,410: t15.2023.08.25 val PER: 0.1145
2026-01-08 13:57:17,410: t15.2023.08.27 val PER: 0.2299
2026-01-08 13:57:17,410: t15.2023.09.01 val PER: 0.1039
2026-01-08 13:57:17,410: t15.2023.09.03 val PER: 0.1888
2026-01-08 13:57:17,410: t15.2023.09.24 val PER: 0.1541
2026-01-08 13:57:17,410: t15.2023.09.29 val PER: 0.1538
2026-01-08 13:57:17,410: t15.2023.10.01 val PER: 0.1995
2026-01-08 13:57:17,410: t15.2023.10.06 val PER: 0.1163
2026-01-08 13:57:17,410: t15.2023.10.08 val PER: 0.2639
2026-01-08 13:57:17,410: t15.2023.10.13 val PER: 0.2475
2026-01-08 13:57:17,410: t15.2023.10.15 val PER: 0.1879
2026-01-08 13:57:17,410: t15.2023.10.20 val PER: 0.2248
2026-01-08 13:57:17,411: t15.2023.10.22 val PER: 0.1381
2026-01-08 13:57:17,411: t15.2023.11.03 val PER: 0.2049
2026-01-08 13:57:17,411: t15.2023.11.04 val PER: 0.0307
2026-01-08 13:57:17,411: t15.2023.11.17 val PER: 0.0575
2026-01-08 13:57:17,411: t15.2023.11.19 val PER: 0.0619
2026-01-08 13:57:17,411: t15.2023.11.26 val PER: 0.1841
2026-01-08 13:57:17,411: t15.2023.12.03 val PER: 0.1576
2026-01-08 13:57:17,411: t15.2023.12.08 val PER: 0.1505
2026-01-08 13:57:17,411: t15.2023.12.10 val PER: 0.1314
2026-01-08 13:57:17,411: t15.2023.12.17 val PER: 0.1788
2026-01-08 13:57:17,412: t15.2023.12.29 val PER: 0.1750
2026-01-08 13:57:17,412: t15.2024.02.25 val PER: 0.1517
2026-01-08 13:57:17,412: t15.2024.03.08 val PER: 0.2703
2026-01-08 13:57:17,412: t15.2024.03.15 val PER: 0.2364
2026-01-08 13:57:17,412: t15.2024.03.17 val PER: 0.1729
2026-01-08 13:57:17,412: t15.2024.05.10 val PER: 0.2051
2026-01-08 13:57:17,412: t15.2024.06.14 val PER: 0.2019
2026-01-08 13:57:17,413: t15.2024.07.19 val PER: 0.2927
2026-01-08 13:57:17,413: t15.2024.07.21 val PER: 0.1248
2026-01-08 13:57:17,413: t15.2024.07.28 val PER: 0.1618
2026-01-08 13:57:17,413: t15.2025.01.10 val PER: 0.3375
2026-01-08 13:57:17,413: t15.2025.01.12 val PER: 0.1786
2026-01-08 13:57:17,413: t15.2025.03.14 val PER: 0.3536
2026-01-08 13:57:17,413: t15.2025.03.16 val PER: 0.2382
2026-01-08 13:57:17,413: t15.2025.03.30 val PER: 0.3552
2026-01-08 13:57:17,413: t15.2025.04.13 val PER: 0.2525
2026-01-08 13:57:17,414: New best val WER(1gram) 54.57% --> 54.31%
2026-01-08 13:57:17,414: Checkpointing model
2026-01-08 13:57:17,555: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 13:57:34,611: Train batch 8200: loss: 10.28 grad norm: 46.99 time: 0.054
2026-01-08 13:57:52,143: Train batch 8400: loss: 10.76 grad norm: 47.00 time: 0.064
2026-01-08 13:58:01,296: Running test after training batch: 8500
2026-01-08 13:58:01,399: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:58:06,240: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point will
2026-01-08 13:58:06,270: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ned
2026-01-08 13:58:07,896: Val batch 8500: PER (avg): 0.1804 CTC Loss (avg): 17.8896 WER(1gram): 51.27% (n=64) time: 6.600
2026-01-08 13:58:07,896: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-08 13:58:07,896: t15.2023.08.13 val PER: 0.1424
2026-01-08 13:58:07,896: t15.2023.08.18 val PER: 0.1400
2026-01-08 13:58:07,897: t15.2023.08.20 val PER: 0.1398
2026-01-08 13:58:07,897: t15.2023.08.25 val PER: 0.1175
2026-01-08 13:58:07,897: t15.2023.08.27 val PER: 0.2026
2026-01-08 13:58:07,897: t15.2023.09.01 val PER: 0.1023
2026-01-08 13:58:07,897: t15.2023.09.03 val PER: 0.1781
2026-01-08 13:58:07,897: t15.2023.09.24 val PER: 0.1602
2026-01-08 13:58:07,897: t15.2023.09.29 val PER: 0.1538
2026-01-08 13:58:07,897: t15.2023.10.01 val PER: 0.1882
2026-01-08 13:58:07,897: t15.2023.10.06 val PER: 0.1109
2026-01-08 13:58:07,897: t15.2023.10.08 val PER: 0.2598
2026-01-08 13:58:07,897: t15.2023.10.13 val PER: 0.2258
2026-01-08 13:58:07,898: t15.2023.10.15 val PER: 0.1734
2026-01-08 13:58:07,898: t15.2023.10.20 val PER: 0.1913
2026-01-08 13:58:07,898: t15.2023.10.22 val PER: 0.1392
2026-01-08 13:58:07,898: t15.2023.11.03 val PER: 0.1900
2026-01-08 13:58:07,898: t15.2023.11.04 val PER: 0.0478
2026-01-08 13:58:07,898: t15.2023.11.17 val PER: 0.0638
2026-01-08 13:58:07,898: t15.2023.11.19 val PER: 0.0479
2026-01-08 13:58:07,898: t15.2023.11.26 val PER: 0.1848
2026-01-08 13:58:07,898: t15.2023.12.03 val PER: 0.1555
2026-01-08 13:58:07,898: t15.2023.12.08 val PER: 0.1325
2026-01-08 13:58:07,898: t15.2023.12.10 val PER: 0.1248
2026-01-08 13:58:07,898: t15.2023.12.17 val PER: 0.1694
2026-01-08 13:58:07,898: t15.2023.12.29 val PER: 0.1702
2026-01-08 13:58:07,898: t15.2024.02.25 val PER: 0.1447
2026-01-08 13:58:07,899: t15.2024.03.08 val PER: 0.2660
2026-01-08 13:58:07,899: t15.2024.03.15 val PER: 0.2308
2026-01-08 13:58:07,899: t15.2024.03.17 val PER: 0.1792
2026-01-08 13:58:07,899: t15.2024.05.10 val PER: 0.1976
2026-01-08 13:58:07,899: t15.2024.06.14 val PER: 0.1877
2026-01-08 13:58:07,899: t15.2024.07.19 val PER: 0.2815
2026-01-08 13:58:07,899: t15.2024.07.21 val PER: 0.1186
2026-01-08 13:58:07,899: t15.2024.07.28 val PER: 0.1662
2026-01-08 13:58:07,899: t15.2025.01.10 val PER: 0.3347
2026-01-08 13:58:07,899: t15.2025.01.12 val PER: 0.1832
2026-01-08 13:58:07,899: t15.2025.03.14 val PER: 0.3609
2026-01-08 13:58:07,899: t15.2025.03.16 val PER: 0.2160
2026-01-08 13:58:07,900: t15.2025.03.30 val PER: 0.3483
2026-01-08 13:58:07,900: t15.2025.04.13 val PER: 0.2368
2026-01-08 13:58:07,901: New best val WER(1gram) 54.31% --> 51.27%
2026-01-08 13:58:07,901: Checkpointing model
2026-01-08 13:58:08,044: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 13:58:16,647: Train batch 8600: loss: 16.07 grad norm: 57.92 time: 0.055
2026-01-08 13:58:33,453: Train batch 8800: loss: 16.03 grad norm: 57.51 time: 0.061
2026-01-08 13:58:51,831: Train batch 9000: loss: 16.21 grad norm: 60.89 time: 0.073
2026-01-08 13:58:51,831: Running test after training batch: 9000
2026-01-08 13:58:51,971: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:58:56,698: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 13:58:56,727: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ged
2026-01-08 13:58:58,399: Val batch 9000: PER (avg): 0.1785 CTC Loss (avg): 17.7175 WER(1gram): 52.79% (n=64) time: 6.567
2026-01-08 13:58:58,399: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-08 13:58:58,399: t15.2023.08.13 val PER: 0.1403
2026-01-08 13:58:58,400: t15.2023.08.18 val PER: 0.1400
2026-01-08 13:58:58,400: t15.2023.08.20 val PER: 0.1168
2026-01-08 13:58:58,400: t15.2023.08.25 val PER: 0.1024
2026-01-08 13:58:58,400: t15.2023.08.27 val PER: 0.2122
2026-01-08 13:58:58,400: t15.2023.09.01 val PER: 0.1006
2026-01-08 13:58:58,400: t15.2023.09.03 val PER: 0.1805
2026-01-08 13:58:58,400: t15.2023.09.24 val PER: 0.1578
2026-01-08 13:58:58,400: t15.2023.09.29 val PER: 0.1506
2026-01-08 13:58:58,400: t15.2023.10.01 val PER: 0.1988
2026-01-08 13:58:58,400: t15.2023.10.06 val PER: 0.1001
2026-01-08 13:58:58,400: t15.2023.10.08 val PER: 0.2544
2026-01-08 13:58:58,400: t15.2023.10.13 val PER: 0.2273
2026-01-08 13:58:58,400: t15.2023.10.15 val PER: 0.1872
2026-01-08 13:58:58,400: t15.2023.10.20 val PER: 0.2081
2026-01-08 13:58:58,401: t15.2023.10.22 val PER: 0.1425
2026-01-08 13:58:58,401: t15.2023.11.03 val PER: 0.2049
2026-01-08 13:58:58,401: t15.2023.11.04 val PER: 0.0444
2026-01-08 13:58:58,401: t15.2023.11.17 val PER: 0.0591
2026-01-08 13:58:58,401: t15.2023.11.19 val PER: 0.0539
2026-01-08 13:58:58,401: t15.2023.11.26 val PER: 0.1688
2026-01-08 13:58:58,401: t15.2023.12.03 val PER: 0.1481
2026-01-08 13:58:58,401: t15.2023.12.08 val PER: 0.1312
2026-01-08 13:58:58,401: t15.2023.12.10 val PER: 0.1143
2026-01-08 13:58:58,401: t15.2023.12.17 val PER: 0.1684
2026-01-08 13:58:58,401: t15.2023.12.29 val PER: 0.1743
2026-01-08 13:58:58,402: t15.2024.02.25 val PER: 0.1306
2026-01-08 13:58:58,402: t15.2024.03.08 val PER: 0.2674
2026-01-08 13:58:58,402: t15.2024.03.15 val PER: 0.2270
2026-01-08 13:58:58,402: t15.2024.03.17 val PER: 0.1778
2026-01-08 13:58:58,402: t15.2024.05.10 val PER: 0.1887
2026-01-08 13:58:58,402: t15.2024.06.14 val PER: 0.1877
2026-01-08 13:58:58,402: t15.2024.07.19 val PER: 0.2775
2026-01-08 13:58:58,402: t15.2024.07.21 val PER: 0.1179
2026-01-08 13:58:58,402: t15.2024.07.28 val PER: 0.1632
2026-01-08 13:58:58,402: t15.2025.01.10 val PER: 0.3168
2026-01-08 13:58:58,402: t15.2025.01.12 val PER: 0.1701
2026-01-08 13:58:58,402: t15.2025.03.14 val PER: 0.3565
2026-01-08 13:58:58,402: t15.2025.03.16 val PER: 0.2199
2026-01-08 13:58:58,402: t15.2025.03.30 val PER: 0.3333
2026-01-08 13:58:58,402: t15.2025.04.13 val PER: 0.2625
2026-01-08 13:59:15,758: Train batch 9200: loss: 11.06 grad norm: 50.01 time: 0.057
2026-01-08 13:59:32,869: Train batch 9400: loss: 7.86 grad norm: 41.74 time: 0.068
2026-01-08 13:59:41,259: Running test after training batch: 9500
2026-01-08 13:59:41,358: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:59:46,063: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-08 13:59:46,092: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ed
2026-01-08 13:59:47,778: Val batch 9500: PER (avg): 0.1752 CTC Loss (avg): 17.4871 WER(1gram): 49.49% (n=64) time: 6.518
2026-01-08 13:59:47,778: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-08 13:59:47,778: t15.2023.08.13 val PER: 0.1310
2026-01-08 13:59:47,778: t15.2023.08.18 val PER: 0.1215
2026-01-08 13:59:47,778: t15.2023.08.20 val PER: 0.1287
2026-01-08 13:59:47,779: t15.2023.08.25 val PER: 0.1114
2026-01-08 13:59:47,779: t15.2023.08.27 val PER: 0.2010
2026-01-08 13:59:47,779: t15.2023.09.01 val PER: 0.0990
2026-01-08 13:59:47,779: t15.2023.09.03 val PER: 0.1758
2026-01-08 13:59:47,779: t15.2023.09.24 val PER: 0.1468
2026-01-08 13:59:47,779: t15.2023.09.29 val PER: 0.1544
2026-01-08 13:59:47,779: t15.2023.10.01 val PER: 0.1988
2026-01-08 13:59:47,779: t15.2023.10.06 val PER: 0.1130
2026-01-08 13:59:47,779: t15.2023.10.08 val PER: 0.2571
2026-01-08 13:59:47,779: t15.2023.10.13 val PER: 0.2242
2026-01-08 13:59:47,779: t15.2023.10.15 val PER: 0.1826
2026-01-08 13:59:47,779: t15.2023.10.20 val PER: 0.1913
2026-01-08 13:59:47,779: t15.2023.10.22 val PER: 0.1381
2026-01-08 13:59:47,779: t15.2023.11.03 val PER: 0.1906
2026-01-08 13:59:47,780: t15.2023.11.04 val PER: 0.0341
2026-01-08 13:59:47,780: t15.2023.11.17 val PER: 0.0560
2026-01-08 13:59:47,780: t15.2023.11.19 val PER: 0.0559
2026-01-08 13:59:47,780: t15.2023.11.26 val PER: 0.1609
2026-01-08 13:59:47,780: t15.2023.12.03 val PER: 0.1460
2026-01-08 13:59:47,780: t15.2023.12.08 val PER: 0.1405
2026-01-08 13:59:47,780: t15.2023.12.10 val PER: 0.1156
2026-01-08 13:59:47,780: t15.2023.12.17 val PER: 0.1549
2026-01-08 13:59:47,780: t15.2023.12.29 val PER: 0.1531
2026-01-08 13:59:47,780: t15.2024.02.25 val PER: 0.1292
2026-01-08 13:59:47,780: t15.2024.03.08 val PER: 0.2646
2026-01-08 13:59:47,780: t15.2024.03.15 val PER: 0.2251
2026-01-08 13:59:47,780: t15.2024.03.17 val PER: 0.1632
2026-01-08 13:59:47,780: t15.2024.05.10 val PER: 0.2021
2026-01-08 13:59:47,780: t15.2024.06.14 val PER: 0.1909
2026-01-08 13:59:47,780: t15.2024.07.19 val PER: 0.2755
2026-01-08 13:59:47,781: t15.2024.07.21 val PER: 0.1083
2026-01-08 13:59:47,781: t15.2024.07.28 val PER: 0.1596
2026-01-08 13:59:47,781: t15.2025.01.10 val PER: 0.3072
2026-01-08 13:59:47,781: t15.2025.01.12 val PER: 0.1824
2026-01-08 13:59:47,781: t15.2025.03.14 val PER: 0.3757
2026-01-08 13:59:47,781: t15.2025.03.16 val PER: 0.2120
2026-01-08 13:59:47,781: t15.2025.03.30 val PER: 0.3264
2026-01-08 13:59:47,781: t15.2025.04.13 val PER: 0.2511
2026-01-08 13:59:47,782: New best val WER(1gram) 51.27% --> 49.49%
2026-01-08 13:59:47,782: Checkpointing model
2026-01-08 13:59:47,920: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 13:59:56,439: Train batch 9600: loss: 8.53 grad norm: 45.83 time: 0.073
2026-01-08 14:00:14,025: Train batch 9800: loss: 13.18 grad norm: 60.09 time: 0.062
2026-01-08 14:00:31,505: Train batch 10000: loss: 6.24 grad norm: 36.78 time: 0.060
2026-01-08 14:00:31,505: Running test after training batch: 10000
2026-01-08 14:00:31,635: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:00:36,923: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-08 14:00:36,953: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sent
2026-01-08 14:00:38,678: Val batch 10000: PER (avg): 0.1693 CTC Loss (avg): 16.9989 WER(1gram): 50.51% (n=64) time: 7.172
2026-01-08 14:00:38,678: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=12
2026-01-08 14:00:38,678: t15.2023.08.13 val PER: 0.1279
2026-01-08 14:00:38,678: t15.2023.08.18 val PER: 0.1282
2026-01-08 14:00:38,678: t15.2023.08.20 val PER: 0.1144
2026-01-08 14:00:38,679: t15.2023.08.25 val PER: 0.1145
2026-01-08 14:00:38,679: t15.2023.08.27 val PER: 0.2026
2026-01-08 14:00:38,679: t15.2023.09.01 val PER: 0.0950
2026-01-08 14:00:38,679: t15.2023.09.03 val PER: 0.1722
2026-01-08 14:00:38,679: t15.2023.09.24 val PER: 0.1371
2026-01-08 14:00:38,679: t15.2023.09.29 val PER: 0.1487
2026-01-08 14:00:38,679: t15.2023.10.01 val PER: 0.1863
2026-01-08 14:00:38,679: t15.2023.10.06 val PER: 0.1012
2026-01-08 14:00:38,679: t15.2023.10.08 val PER: 0.2503
2026-01-08 14:00:38,679: t15.2023.10.13 val PER: 0.2164
2026-01-08 14:00:38,679: t15.2023.10.15 val PER: 0.1734
2026-01-08 14:00:38,680: t15.2023.10.20 val PER: 0.2013
2026-01-08 14:00:38,680: t15.2023.10.22 val PER: 0.1347
2026-01-08 14:00:38,680: t15.2023.11.03 val PER: 0.1852
2026-01-08 14:00:38,680: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:00:38,680: t15.2023.11.17 val PER: 0.0482
2026-01-08 14:00:38,685: t15.2023.11.19 val PER: 0.0439
2026-01-08 14:00:38,685: t15.2023.11.26 val PER: 0.1486
2026-01-08 14:00:38,685: t15.2023.12.03 val PER: 0.1334
2026-01-08 14:00:38,685: t15.2023.12.08 val PER: 0.1292
2026-01-08 14:00:38,685: t15.2023.12.10 val PER: 0.1078
2026-01-08 14:00:38,685: t15.2023.12.17 val PER: 0.1538
2026-01-08 14:00:38,685: t15.2023.12.29 val PER: 0.1620
2026-01-08 14:00:38,685: t15.2024.02.25 val PER: 0.1334
2026-01-08 14:00:38,686: t15.2024.03.08 val PER: 0.2532
2026-01-08 14:00:38,686: t15.2024.03.15 val PER: 0.2195
2026-01-08 14:00:38,686: t15.2024.03.17 val PER: 0.1520
2026-01-08 14:00:38,686: t15.2024.05.10 val PER: 0.1738
2026-01-08 14:00:38,686: t15.2024.06.14 val PER: 0.1767
2026-01-08 14:00:38,686: t15.2024.07.19 val PER: 0.2729
2026-01-08 14:00:38,686: t15.2024.07.21 val PER: 0.1145
2026-01-08 14:00:38,686: t15.2024.07.28 val PER: 0.1529
2026-01-08 14:00:38,686: t15.2025.01.10 val PER: 0.3058
2026-01-08 14:00:38,686: t15.2025.01.12 val PER: 0.1771
2026-01-08 14:00:38,686: t15.2025.03.14 val PER: 0.3580
2026-01-08 14:00:38,686: t15.2025.03.16 val PER: 0.2173
2026-01-08 14:00:38,686: t15.2025.03.30 val PER: 0.3172
2026-01-08 14:00:38,686: t15.2025.04.13 val PER: 0.2425
2026-01-08 14:00:56,837: Train batch 10200: loss: 6.70 grad norm: 39.49 time: 0.050
2026-01-08 14:01:13,989: Train batch 10400: loss: 10.10 grad norm: 60.38 time: 0.073
2026-01-08 14:01:22,523: Running test after training batch: 10500
2026-01-08 14:01:22,615: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:01:27,360: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the cold at this point as will
2026-01-08 14:01:27,403: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-08 14:01:29,348: Val batch 10500: PER (avg): 0.1685 CTC Loss (avg): 16.8820 WER(1gram): 50.00% (n=64) time: 6.825
2026-01-08 14:01:29,349: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=12
2026-01-08 14:01:29,349: t15.2023.08.13 val PER: 0.1247
2026-01-08 14:01:29,349: t15.2023.08.18 val PER: 0.1299
2026-01-08 14:01:29,349: t15.2023.08.20 val PER: 0.1231
2026-01-08 14:01:29,349: t15.2023.08.25 val PER: 0.1190
2026-01-08 14:01:29,349: t15.2023.08.27 val PER: 0.2154
2026-01-08 14:01:29,349: t15.2023.09.01 val PER: 0.0966
2026-01-08 14:01:29,349: t15.2023.09.03 val PER: 0.1758
2026-01-08 14:01:29,349: t15.2023.09.24 val PER: 0.1408
2026-01-08 14:01:29,349: t15.2023.09.29 val PER: 0.1449
2026-01-08 14:01:29,349: t15.2023.10.01 val PER: 0.1823
2026-01-08 14:01:29,349: t15.2023.10.06 val PER: 0.1044
2026-01-08 14:01:29,350: t15.2023.10.08 val PER: 0.2517
2026-01-08 14:01:29,350: t15.2023.10.13 val PER: 0.2110
2026-01-08 14:01:29,350: t15.2023.10.15 val PER: 0.1740
2026-01-08 14:01:29,350: t15.2023.10.20 val PER: 0.1980
2026-01-08 14:01:29,350: t15.2023.10.22 val PER: 0.1292
2026-01-08 14:01:29,350: t15.2023.11.03 val PER: 0.1900
2026-01-08 14:01:29,350: t15.2023.11.04 val PER: 0.0410
2026-01-08 14:01:29,350: t15.2023.11.17 val PER: 0.0451
2026-01-08 14:01:29,350: t15.2023.11.19 val PER: 0.0499
2026-01-08 14:01:29,350: t15.2023.11.26 val PER: 0.1406
2026-01-08 14:01:29,350: t15.2023.12.03 val PER: 0.1303
2026-01-08 14:01:29,350: t15.2023.12.08 val PER: 0.1318
2026-01-08 14:01:29,351: t15.2023.12.10 val PER: 0.1078
2026-01-08 14:01:29,351: t15.2023.12.17 val PER: 0.1424
2026-01-08 14:01:29,351: t15.2023.12.29 val PER: 0.1592
2026-01-08 14:01:29,351: t15.2024.02.25 val PER: 0.1362
2026-01-08 14:01:29,351: t15.2024.03.08 val PER: 0.2432
2026-01-08 14:01:29,351: t15.2024.03.15 val PER: 0.2183
2026-01-08 14:01:29,351: t15.2024.03.17 val PER: 0.1667
2026-01-08 14:01:29,351: t15.2024.05.10 val PER: 0.1887
2026-01-08 14:01:29,351: t15.2024.06.14 val PER: 0.1782
2026-01-08 14:01:29,351: t15.2024.07.19 val PER: 0.2637
2026-01-08 14:01:29,351: t15.2024.07.21 val PER: 0.1152
2026-01-08 14:01:29,351: t15.2024.07.28 val PER: 0.1434
2026-01-08 14:01:29,351: t15.2025.01.10 val PER: 0.3127
2026-01-08 14:01:29,351: t15.2025.01.12 val PER: 0.1663
2026-01-08 14:01:29,351: t15.2025.03.14 val PER: 0.3595
2026-01-08 14:01:29,351: t15.2025.03.16 val PER: 0.2029
2026-01-08 14:01:29,351: t15.2025.03.30 val PER: 0.3161
2026-01-08 14:01:29,352: t15.2025.04.13 val PER: 0.2340
2026-01-08 14:01:38,887: Train batch 10600: loss: 9.34 grad norm: 50.90 time: 0.075
2026-01-08 14:01:58,377: Train batch 10800: loss: 15.47 grad norm: 66.25 time: 0.066
2026-01-08 14:02:17,036: Train batch 11000: loss: 15.48 grad norm: 61.07 time: 0.057
2026-01-08 14:02:17,037: Running test after training batch: 11000
2026-01-08 14:02:17,138: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:02:21,932: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code atz this point as will
2026-01-08 14:02:21,972: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ent
2026-01-08 14:02:23,897: Val batch 11000: PER (avg): 0.1645 CTC Loss (avg): 16.5920 WER(1gram): 50.00% (n=64) time: 6.861
2026-01-08 14:02:23,898: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=12
2026-01-08 14:02:23,898: t15.2023.08.13 val PER: 0.1289
2026-01-08 14:02:23,898: t15.2023.08.18 val PER: 0.1324
2026-01-08 14:02:23,898: t15.2023.08.20 val PER: 0.1144
2026-01-08 14:02:23,898: t15.2023.08.25 val PER: 0.1054
2026-01-08 14:02:23,898: t15.2023.08.27 val PER: 0.2058
2026-01-08 14:02:23,898: t15.2023.09.01 val PER: 0.0877
2026-01-08 14:02:23,898: t15.2023.09.03 val PER: 0.1793
2026-01-08 14:02:23,898: t15.2023.09.24 val PER: 0.1371
2026-01-08 14:02:23,898: t15.2023.09.29 val PER: 0.1455
2026-01-08 14:02:23,899: t15.2023.10.01 val PER: 0.1863
2026-01-08 14:02:23,899: t15.2023.10.06 val PER: 0.0861
2026-01-08 14:02:23,899: t15.2023.10.08 val PER: 0.2463
2026-01-08 14:02:23,899: t15.2023.10.13 val PER: 0.2141
2026-01-08 14:02:23,899: t15.2023.10.15 val PER: 0.1747
2026-01-08 14:02:23,899: t15.2023.10.20 val PER: 0.1913
2026-01-08 14:02:23,899: t15.2023.10.22 val PER: 0.1136
2026-01-08 14:02:23,899: t15.2023.11.03 val PER: 0.1947
2026-01-08 14:02:23,899: t15.2023.11.04 val PER: 0.0444
2026-01-08 14:02:23,899: t15.2023.11.17 val PER: 0.0498
2026-01-08 14:02:23,899: t15.2023.11.19 val PER: 0.0479
2026-01-08 14:02:23,899: t15.2023.11.26 val PER: 0.1399
2026-01-08 14:02:23,899: t15.2023.12.03 val PER: 0.1303
2026-01-08 14:02:23,899: t15.2023.12.08 val PER: 0.1185
2026-01-08 14:02:23,900: t15.2023.12.10 val PER: 0.1038
2026-01-08 14:02:23,900: t15.2023.12.17 val PER: 0.1476
2026-01-08 14:02:23,900: t15.2023.12.29 val PER: 0.1380
2026-01-08 14:02:23,900: t15.2024.02.25 val PER: 0.1306
2026-01-08 14:02:23,900: t15.2024.03.08 val PER: 0.2376
2026-01-08 14:02:23,900: t15.2024.03.15 val PER: 0.2126
2026-01-08 14:02:23,900: t15.2024.03.17 val PER: 0.1527
2026-01-08 14:02:23,900: t15.2024.05.10 val PER: 0.1813
2026-01-08 14:02:23,900: t15.2024.06.14 val PER: 0.1735
2026-01-08 14:02:23,900: t15.2024.07.19 val PER: 0.2657
2026-01-08 14:02:23,900: t15.2024.07.21 val PER: 0.1048
2026-01-08 14:02:23,900: t15.2024.07.28 val PER: 0.1471
2026-01-08 14:02:23,901: t15.2025.01.10 val PER: 0.2989
2026-01-08 14:02:23,901: t15.2025.01.12 val PER: 0.1601
2026-01-08 14:02:23,901: t15.2025.03.14 val PER: 0.3550
2026-01-08 14:02:23,901: t15.2025.03.16 val PER: 0.2120
2026-01-08 14:02:23,901: t15.2025.03.30 val PER: 0.3138
2026-01-08 14:02:23,901: t15.2025.04.13 val PER: 0.2325
2026-01-08 14:02:42,913: Train batch 11200: loss: 11.31 grad norm: 51.95 time: 0.072
2026-01-08 14:03:02,172: Train batch 11400: loss: 10.25 grad norm: 54.03 time: 0.058
2026-01-08 14:03:10,774: Running test after training batch: 11500
2026-01-08 14:03:10,915: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:03:15,661: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the cold atz this point is will
2026-01-08 14:03:15,692: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost ent
2026-01-08 14:03:17,534: Val batch 11500: PER (avg): 0.1636 CTC Loss (avg): 16.5158 WER(1gram): 48.73% (n=64) time: 6.760
2026-01-08 14:03:17,534: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=12
2026-01-08 14:03:17,535: t15.2023.08.13 val PER: 0.1299
2026-01-08 14:03:17,535: t15.2023.08.18 val PER: 0.1174
2026-01-08 14:03:17,535: t15.2023.08.20 val PER: 0.1136
2026-01-08 14:03:17,535: t15.2023.08.25 val PER: 0.1114
2026-01-08 14:03:17,535: t15.2023.08.27 val PER: 0.2058
2026-01-08 14:03:17,535: t15.2023.09.01 val PER: 0.0885
2026-01-08 14:03:17,535: t15.2023.09.03 val PER: 0.1698
2026-01-08 14:03:17,535: t15.2023.09.24 val PER: 0.1311
2026-01-08 14:03:17,536: t15.2023.09.29 val PER: 0.1404
2026-01-08 14:03:17,536: t15.2023.10.01 val PER: 0.1823
2026-01-08 14:03:17,536: t15.2023.10.06 val PER: 0.0915
2026-01-08 14:03:17,536: t15.2023.10.08 val PER: 0.2422
2026-01-08 14:03:17,536: t15.2023.10.13 val PER: 0.2087
2026-01-08 14:03:17,536: t15.2023.10.15 val PER: 0.1681
2026-01-08 14:03:17,536: t15.2023.10.20 val PER: 0.1879
2026-01-08 14:03:17,536: t15.2023.10.22 val PER: 0.1281
2026-01-08 14:03:17,536: t15.2023.11.03 val PER: 0.1906
2026-01-08 14:03:17,536: t15.2023.11.04 val PER: 0.0307
2026-01-08 14:03:17,536: t15.2023.11.17 val PER: 0.0544
2026-01-08 14:03:17,536: t15.2023.11.19 val PER: 0.0439
2026-01-08 14:03:17,536: t15.2023.11.26 val PER: 0.1370
2026-01-08 14:03:17,536: t15.2023.12.03 val PER: 0.1334
2026-01-08 14:03:17,536: t15.2023.12.08 val PER: 0.1119
2026-01-08 14:03:17,537: t15.2023.12.10 val PER: 0.1104
2026-01-08 14:03:17,537: t15.2023.12.17 val PER: 0.1476
2026-01-08 14:03:17,537: t15.2023.12.29 val PER: 0.1469
2026-01-08 14:03:17,537: t15.2024.02.25 val PER: 0.1208
2026-01-08 14:03:17,537: t15.2024.03.08 val PER: 0.2461
2026-01-08 14:03:17,537: t15.2024.03.15 val PER: 0.2208
2026-01-08 14:03:17,537: t15.2024.03.17 val PER: 0.1485
2026-01-08 14:03:17,537: t15.2024.05.10 val PER: 0.1724
2026-01-08 14:03:17,537: t15.2024.06.14 val PER: 0.1782
2026-01-08 14:03:17,537: t15.2024.07.19 val PER: 0.2538
2026-01-08 14:03:17,537: t15.2024.07.21 val PER: 0.1097
2026-01-08 14:03:17,537: t15.2024.07.28 val PER: 0.1485
2026-01-08 14:03:17,537: t15.2025.01.10 val PER: 0.3154
2026-01-08 14:03:17,537: t15.2025.01.12 val PER: 0.1694
2026-01-08 14:03:17,537: t15.2025.03.14 val PER: 0.3447
2026-01-08 14:03:17,537: t15.2025.03.16 val PER: 0.1990
2026-01-08 14:03:17,538: t15.2025.03.30 val PER: 0.3149
2026-01-08 14:03:17,538: t15.2025.04.13 val PER: 0.2411
2026-01-08 14:03:17,539: New best val WER(1gram) 49.49% --> 48.73%
2026-01-08 14:03:17,539: Checkpointing model
2026-01-08 14:03:17,679: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 14:03:27,487: Train batch 11600: loss: 10.94 grad norm: 48.83 time: 0.061
2026-01-08 14:03:46,691: Train batch 11800: loss: 6.68 grad norm: 39.43 time: 0.046
2026-01-08 14:04:05,667: Train batch 12000: loss: 14.22 grad norm: 60.43 time: 0.073
2026-01-08 14:04:05,668: Running test after training batch: 12000
2026-01-08 14:04:05,908: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:04:10,709: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-08 14:04:10,740: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost ent
2026-01-08 14:04:12,478: Val batch 12000: PER (avg): 0.1622 CTC Loss (avg): 16.3663 WER(1gram): 50.25% (n=64) time: 6.810
2026-01-08 14:04:12,478: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-08 14:04:12,479: t15.2023.08.13 val PER: 0.1237
2026-01-08 14:04:12,479: t15.2023.08.18 val PER: 0.1157
2026-01-08 14:04:12,479: t15.2023.08.20 val PER: 0.1120
2026-01-08 14:04:12,479: t15.2023.08.25 val PER: 0.1009
2026-01-08 14:04:12,479: t15.2023.08.27 val PER: 0.1913
2026-01-08 14:04:12,479: t15.2023.09.01 val PER: 0.0917
2026-01-08 14:04:12,479: t15.2023.09.03 val PER: 0.1568
2026-01-08 14:04:12,479: t15.2023.09.24 val PER: 0.1286
2026-01-08 14:04:12,479: t15.2023.09.29 val PER: 0.1442
2026-01-08 14:04:12,479: t15.2023.10.01 val PER: 0.1849
2026-01-08 14:04:12,479: t15.2023.10.06 val PER: 0.0936
2026-01-08 14:04:12,479: t15.2023.10.08 val PER: 0.2571
2026-01-08 14:04:12,479: t15.2023.10.13 val PER: 0.2110
2026-01-08 14:04:12,479: t15.2023.10.15 val PER: 0.1648
2026-01-08 14:04:12,480: t15.2023.10.20 val PER: 0.1980
2026-01-08 14:04:12,480: t15.2023.10.22 val PER: 0.1269
2026-01-08 14:04:12,480: t15.2023.11.03 val PER: 0.1900
2026-01-08 14:04:12,480: t15.2023.11.04 val PER: 0.0273
2026-01-08 14:04:12,480: t15.2023.11.17 val PER: 0.0420
2026-01-08 14:04:12,480: t15.2023.11.19 val PER: 0.0439
2026-01-08 14:04:12,480: t15.2023.11.26 val PER: 0.1399
2026-01-08 14:04:12,480: t15.2023.12.03 val PER: 0.1250
2026-01-08 14:04:12,480: t15.2023.12.08 val PER: 0.1119
2026-01-08 14:04:12,480: t15.2023.12.10 val PER: 0.1104
2026-01-08 14:04:12,480: t15.2023.12.17 val PER: 0.1570
2026-01-08 14:04:12,480: t15.2023.12.29 val PER: 0.1455
2026-01-08 14:04:12,480: t15.2024.02.25 val PER: 0.1124
2026-01-08 14:04:12,480: t15.2024.03.08 val PER: 0.2461
2026-01-08 14:04:12,480: t15.2024.03.15 val PER: 0.2158
2026-01-08 14:04:12,481: t15.2024.03.17 val PER: 0.1513
2026-01-08 14:04:12,481: t15.2024.05.10 val PER: 0.1828
2026-01-08 14:04:12,481: t15.2024.06.14 val PER: 0.1893
2026-01-08 14:04:12,481: t15.2024.07.19 val PER: 0.2512
2026-01-08 14:04:12,481: t15.2024.07.21 val PER: 0.1083
2026-01-08 14:04:12,481: t15.2024.07.28 val PER: 0.1426
2026-01-08 14:04:12,481: t15.2025.01.10 val PER: 0.2920
2026-01-08 14:04:12,481: t15.2025.01.12 val PER: 0.1594
2026-01-08 14:04:12,482: t15.2025.03.14 val PER: 0.3609
2026-01-08 14:04:12,482: t15.2025.03.16 val PER: 0.2042
2026-01-08 14:04:12,482: t15.2025.03.30 val PER: 0.3069
2026-01-08 14:04:12,482: t15.2025.04.13 val PER: 0.2282
2026-01-08 14:04:30,684: Train batch 12200: loss: 6.11 grad norm: 43.07 time: 0.066
2026-01-08 14:04:49,539: Train batch 12400: loss: 5.01 grad norm: 33.76 time: 0.042
2026-01-08 14:04:59,601: Running test after training batch: 12500
2026-01-08 14:04:59,740: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:05:04,460: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:05:04,491: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost edds
2026-01-08 14:05:06,255: Val batch 12500: PER (avg): 0.1602 CTC Loss (avg): 16.2797 WER(1gram): 48.22% (n=64) time: 6.654
2026-01-08 14:05:06,256: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-08 14:05:06,256: t15.2023.08.13 val PER: 0.1279
2026-01-08 14:05:06,256: t15.2023.08.18 val PER: 0.1215
2026-01-08 14:05:06,256: t15.2023.08.20 val PER: 0.1064
2026-01-08 14:05:06,256: t15.2023.08.25 val PER: 0.1024
2026-01-08 14:05:06,256: t15.2023.08.27 val PER: 0.1961
2026-01-08 14:05:06,256: t15.2023.09.01 val PER: 0.0812
2026-01-08 14:05:06,256: t15.2023.09.03 val PER: 0.1651
2026-01-08 14:05:06,256: t15.2023.09.24 val PER: 0.1274
2026-01-08 14:05:06,256: t15.2023.09.29 val PER: 0.1391
2026-01-08 14:05:06,256: t15.2023.10.01 val PER: 0.1816
2026-01-08 14:05:06,257: t15.2023.10.06 val PER: 0.1044
2026-01-08 14:05:06,257: t15.2023.10.08 val PER: 0.2612
2026-01-08 14:05:06,257: t15.2023.10.13 val PER: 0.2095
2026-01-08 14:05:06,257: t15.2023.10.15 val PER: 0.1635
2026-01-08 14:05:06,257: t15.2023.10.20 val PER: 0.2013
2026-01-08 14:05:06,257: t15.2023.10.22 val PER: 0.1269
2026-01-08 14:05:06,258: t15.2023.11.03 val PER: 0.1845
2026-01-08 14:05:06,258: t15.2023.11.04 val PER: 0.0375
2026-01-08 14:05:06,258: t15.2023.11.17 val PER: 0.0482
2026-01-08 14:05:06,258: t15.2023.11.19 val PER: 0.0419
2026-01-08 14:05:06,258: t15.2023.11.26 val PER: 0.1319
2026-01-08 14:05:06,258: t15.2023.12.03 val PER: 0.1282
2026-01-08 14:05:06,258: t15.2023.12.08 val PER: 0.1092
2026-01-08 14:05:06,258: t15.2023.12.10 val PER: 0.1025
2026-01-08 14:05:06,258: t15.2023.12.17 val PER: 0.1476
2026-01-08 14:05:06,258: t15.2023.12.29 val PER: 0.1428
2026-01-08 14:05:06,258: t15.2024.02.25 val PER: 0.1081
2026-01-08 14:05:06,258: t15.2024.03.08 val PER: 0.2404
2026-01-08 14:05:06,259: t15.2024.03.15 val PER: 0.2189
2026-01-08 14:05:06,259: t15.2024.03.17 val PER: 0.1478
2026-01-08 14:05:06,259: t15.2024.05.10 val PER: 0.1709
2026-01-08 14:05:06,259: t15.2024.06.14 val PER: 0.1814
2026-01-08 14:05:06,259: t15.2024.07.19 val PER: 0.2465
2026-01-08 14:05:06,259: t15.2024.07.21 val PER: 0.1014
2026-01-08 14:05:06,259: t15.2024.07.28 val PER: 0.1390
2026-01-08 14:05:06,259: t15.2025.01.10 val PER: 0.3017
2026-01-08 14:05:06,259: t15.2025.01.12 val PER: 0.1501
2026-01-08 14:05:06,259: t15.2025.03.14 val PER: 0.3639
2026-01-08 14:05:06,259: t15.2025.03.16 val PER: 0.1937
2026-01-08 14:05:06,259: t15.2025.03.30 val PER: 0.3069
2026-01-08 14:05:06,259: t15.2025.04.13 val PER: 0.2382
2026-01-08 14:05:06,261: New best val WER(1gram) 48.73% --> 48.22%
2026-01-08 14:05:06,261: Checkpointing model
2026-01-08 14:05:06,400: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 14:05:14,809: Train batch 12600: loss: 7.94 grad norm: 43.55 time: 0.058
2026-01-08 14:05:33,144: Train batch 12800: loss: 6.10 grad norm: 38.02 time: 0.053
2026-01-08 14:05:52,999: Train batch 13000: loss: 6.82 grad norm: 42.67 time: 0.067
2026-01-08 14:05:52,999: Running test after training batch: 13000
2026-01-08 14:05:53,125: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:05:57,893: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:05:57,924: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost edds
2026-01-08 14:05:59,891: Val batch 13000: PER (avg): 0.1578 CTC Loss (avg): 15.8824 WER(1gram): 46.45% (n=64) time: 6.891
2026-01-08 14:05:59,891: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-08 14:05:59,891: t15.2023.08.13 val PER: 0.1164
2026-01-08 14:05:59,891: t15.2023.08.18 val PER: 0.1182
2026-01-08 14:05:59,892: t15.2023.08.20 val PER: 0.1017
2026-01-08 14:05:59,892: t15.2023.08.25 val PER: 0.0934
2026-01-08 14:05:59,892: t15.2023.08.27 val PER: 0.1881
2026-01-08 14:05:59,892: t15.2023.09.01 val PER: 0.0812
2026-01-08 14:05:59,892: t15.2023.09.03 val PER: 0.1615
2026-01-08 14:05:59,892: t15.2023.09.24 val PER: 0.1335
2026-01-08 14:05:59,892: t15.2023.09.29 val PER: 0.1340
2026-01-08 14:05:59,892: t15.2023.10.01 val PER: 0.1783
2026-01-08 14:05:59,892: t15.2023.10.06 val PER: 0.0915
2026-01-08 14:05:59,892: t15.2023.10.08 val PER: 0.2517
2026-01-08 14:05:59,892: t15.2023.10.13 val PER: 0.2040
2026-01-08 14:05:59,892: t15.2023.10.15 val PER: 0.1622
2026-01-08 14:05:59,892: t15.2023.10.20 val PER: 0.2013
2026-01-08 14:05:59,893: t15.2023.10.22 val PER: 0.1192
2026-01-08 14:05:59,893: t15.2023.11.03 val PER: 0.1798
2026-01-08 14:05:59,893: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:05:59,893: t15.2023.11.17 val PER: 0.0498
2026-01-08 14:05:59,893: t15.2023.11.19 val PER: 0.0379
2026-01-08 14:05:59,893: t15.2023.11.26 val PER: 0.1326
2026-01-08 14:05:59,893: t15.2023.12.03 val PER: 0.1239
2026-01-08 14:05:59,893: t15.2023.12.08 val PER: 0.1158
2026-01-08 14:05:59,893: t15.2023.12.10 val PER: 0.0972
2026-01-08 14:05:59,893: t15.2023.12.17 val PER: 0.1331
2026-01-08 14:05:59,893: t15.2023.12.29 val PER: 0.1531
2026-01-08 14:05:59,893: t15.2024.02.25 val PER: 0.1194
2026-01-08 14:05:59,893: t15.2024.03.08 val PER: 0.2319
2026-01-08 14:05:59,893: t15.2024.03.15 val PER: 0.2108
2026-01-08 14:05:59,893: t15.2024.03.17 val PER: 0.1499
2026-01-08 14:05:59,893: t15.2024.05.10 val PER: 0.1753
2026-01-08 14:05:59,894: t15.2024.06.14 val PER: 0.1751
2026-01-08 14:05:59,894: t15.2024.07.19 val PER: 0.2564
2026-01-08 14:05:59,894: t15.2024.07.21 val PER: 0.0938
2026-01-08 14:05:59,894: t15.2024.07.28 val PER: 0.1485
2026-01-08 14:05:59,894: t15.2025.01.10 val PER: 0.2769
2026-01-08 14:05:59,894: t15.2025.01.12 val PER: 0.1524
2026-01-08 14:05:59,894: t15.2025.03.14 val PER: 0.3521
2026-01-08 14:05:59,894: t15.2025.03.16 val PER: 0.2003
2026-01-08 14:05:59,894: t15.2025.03.30 val PER: 0.3080
2026-01-08 14:05:59,894: t15.2025.04.13 val PER: 0.2254
2026-01-08 14:05:59,896: New best val WER(1gram) 48.22% --> 46.45%
2026-01-08 14:05:59,896: Checkpointing model
2026-01-08 14:06:00,041: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 14:06:18,637: Train batch 13200: loss: 13.80 grad norm: 61.61 time: 0.054
2026-01-08 14:06:36,359: Train batch 13400: loss: 9.56 grad norm: 52.10 time: 0.063
2026-01-08 14:06:45,523: Running test after training batch: 13500
2026-01-08 14:06:45,644: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:06:50,275: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could atz this point as will
2026-01-08 14:06:50,304: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ent
2026-01-08 14:06:52,079: Val batch 13500: PER (avg): 0.1550 CTC Loss (avg): 15.7568 WER(1gram): 49.24% (n=64) time: 6.556
2026-01-08 14:06:52,079: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-08 14:06:52,079: t15.2023.08.13 val PER: 0.1185
2026-01-08 14:06:52,079: t15.2023.08.18 val PER: 0.1157
2026-01-08 14:06:52,080: t15.2023.08.20 val PER: 0.1056
2026-01-08 14:06:52,080: t15.2023.08.25 val PER: 0.0979
2026-01-08 14:06:52,080: t15.2023.08.27 val PER: 0.1977
2026-01-08 14:06:52,080: t15.2023.09.01 val PER: 0.0869
2026-01-08 14:06:52,080: t15.2023.09.03 val PER: 0.1615
2026-01-08 14:06:52,080: t15.2023.09.24 val PER: 0.1274
2026-01-08 14:06:52,080: t15.2023.09.29 val PER: 0.1372
2026-01-08 14:06:52,080: t15.2023.10.01 val PER: 0.1678
2026-01-08 14:06:52,080: t15.2023.10.06 val PER: 0.0936
2026-01-08 14:06:52,080: t15.2023.10.08 val PER: 0.2571
2026-01-08 14:06:52,080: t15.2023.10.13 val PER: 0.2033
2026-01-08 14:06:52,080: t15.2023.10.15 val PER: 0.1562
2026-01-08 14:06:52,080: t15.2023.10.20 val PER: 0.2047
2026-01-08 14:06:52,080: t15.2023.10.22 val PER: 0.1158
2026-01-08 14:06:52,081: t15.2023.11.03 val PER: 0.1777
2026-01-08 14:06:52,081: t15.2023.11.04 val PER: 0.0444
2026-01-08 14:06:52,081: t15.2023.11.17 val PER: 0.0435
2026-01-08 14:06:52,081: t15.2023.11.19 val PER: 0.0299
2026-01-08 14:06:52,081: t15.2023.11.26 val PER: 0.1312
2026-01-08 14:06:52,081: t15.2023.12.03 val PER: 0.1176
2026-01-08 14:06:52,081: t15.2023.12.08 val PER: 0.1112
2026-01-08 14:06:52,081: t15.2023.12.10 val PER: 0.0972
2026-01-08 14:06:52,081: t15.2023.12.17 val PER: 0.1258
2026-01-08 14:06:52,081: t15.2023.12.29 val PER: 0.1332
2026-01-08 14:06:52,081: t15.2024.02.25 val PER: 0.1081
2026-01-08 14:06:52,081: t15.2024.03.08 val PER: 0.2361
2026-01-08 14:06:52,081: t15.2024.03.15 val PER: 0.2014
2026-01-08 14:06:52,081: t15.2024.03.17 val PER: 0.1437
2026-01-08 14:06:52,081: t15.2024.05.10 val PER: 0.1605
2026-01-08 14:06:52,081: t15.2024.06.14 val PER: 0.1672
2026-01-08 14:06:52,081: t15.2024.07.19 val PER: 0.2452
2026-01-08 14:06:52,082: t15.2024.07.21 val PER: 0.0966
2026-01-08 14:06:52,082: t15.2024.07.28 val PER: 0.1493
2026-01-08 14:06:52,082: t15.2025.01.10 val PER: 0.3030
2026-01-08 14:06:52,082: t15.2025.01.12 val PER: 0.1463
2026-01-08 14:06:52,082: t15.2025.03.14 val PER: 0.3595
2026-01-08 14:06:52,082: t15.2025.03.16 val PER: 0.1911
2026-01-08 14:06:52,082: t15.2025.03.30 val PER: 0.3046
2026-01-08 14:06:52,082: t15.2025.04.13 val PER: 0.2211
2026-01-08 14:07:01,144: Train batch 13600: loss: 12.38 grad norm: 61.20 time: 0.065
2026-01-08 14:07:19,540: Train batch 13800: loss: 9.99 grad norm: 59.04 time: 0.056
2026-01-08 14:07:37,549: Train batch 14000: loss: 12.32 grad norm: 58.03 time: 0.050
2026-01-08 14:07:37,549: Running test after training batch: 14000
2026-01-08 14:07:37,653: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:07:42,379: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code atz this point as will
2026-01-08 14:07:42,410: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost edds
2026-01-08 14:07:44,187: Val batch 14000: PER (avg): 0.1524 CTC Loss (avg): 15.5529 WER(1gram): 46.45% (n=64) time: 6.638
2026-01-08 14:07:44,188: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-08 14:07:44,188: t15.2023.08.13 val PER: 0.1102
2026-01-08 14:07:44,188: t15.2023.08.18 val PER: 0.1123
2026-01-08 14:07:44,188: t15.2023.08.20 val PER: 0.1025
2026-01-08 14:07:44,188: t15.2023.08.25 val PER: 0.0994
2026-01-08 14:07:44,188: t15.2023.08.27 val PER: 0.1929
2026-01-08 14:07:44,188: t15.2023.09.01 val PER: 0.0804
2026-01-08 14:07:44,188: t15.2023.09.03 val PER: 0.1746
2026-01-08 14:07:44,188: t15.2023.09.24 val PER: 0.1286
2026-01-08 14:07:44,188: t15.2023.09.29 val PER: 0.1321
2026-01-08 14:07:44,188: t15.2023.10.01 val PER: 0.1684
2026-01-08 14:07:44,189: t15.2023.10.06 val PER: 0.0883
2026-01-08 14:07:44,189: t15.2023.10.08 val PER: 0.2490
2026-01-08 14:07:44,189: t15.2023.10.13 val PER: 0.2025
2026-01-08 14:07:44,189: t15.2023.10.15 val PER: 0.1615
2026-01-08 14:07:44,189: t15.2023.10.20 val PER: 0.2081
2026-01-08 14:07:44,189: t15.2023.10.22 val PER: 0.1180
2026-01-08 14:07:44,189: t15.2023.11.03 val PER: 0.1750
2026-01-08 14:07:44,189: t15.2023.11.04 val PER: 0.0307
2026-01-08 14:07:44,189: t15.2023.11.17 val PER: 0.0420
2026-01-08 14:07:44,190: t15.2023.11.19 val PER: 0.0339
2026-01-08 14:07:44,190: t15.2023.11.26 val PER: 0.1275
2026-01-08 14:07:44,190: t15.2023.12.03 val PER: 0.1155
2026-01-08 14:07:44,190: t15.2023.12.08 val PER: 0.1079
2026-01-08 14:07:44,190: t15.2023.12.10 val PER: 0.0986
2026-01-08 14:07:44,190: t15.2023.12.17 val PER: 0.1216
2026-01-08 14:07:44,191: t15.2023.12.29 val PER: 0.1366
2026-01-08 14:07:44,191: t15.2024.02.25 val PER: 0.1053
2026-01-08 14:07:44,191: t15.2024.03.08 val PER: 0.2248
2026-01-08 14:07:44,191: t15.2024.03.15 val PER: 0.2014
2026-01-08 14:07:44,191: t15.2024.03.17 val PER: 0.1416
2026-01-08 14:07:44,191: t15.2024.05.10 val PER: 0.1560
2026-01-08 14:07:44,191: t15.2024.06.14 val PER: 0.1593
2026-01-08 14:07:44,191: t15.2024.07.19 val PER: 0.2373
2026-01-08 14:07:44,191: t15.2024.07.21 val PER: 0.0917
2026-01-08 14:07:44,191: t15.2024.07.28 val PER: 0.1382
2026-01-08 14:07:44,192: t15.2025.01.10 val PER: 0.2948
2026-01-08 14:07:44,192: t15.2025.01.12 val PER: 0.1517
2026-01-08 14:07:44,192: t15.2025.03.14 val PER: 0.3536
2026-01-08 14:07:44,192: t15.2025.03.16 val PER: 0.1872
2026-01-08 14:07:44,192: t15.2025.03.30 val PER: 0.2943
2026-01-08 14:07:44,192: t15.2025.04.13 val PER: 0.2197
2026-01-08 14:08:01,591: Train batch 14200: loss: 8.33 grad norm: 48.47 time: 0.056
2026-01-08 14:08:18,957: Train batch 14400: loss: 5.91 grad norm: 36.68 time: 0.065
2026-01-08 14:08:27,722: Running test after training batch: 14500
2026-01-08 14:08:27,817: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:08:32,501: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code atz this point as will
2026-01-08 14:08:32,531: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ent
2026-01-08 14:08:34,294: Val batch 14500: PER (avg): 0.1532 CTC Loss (avg): 15.5427 WER(1gram): 47.72% (n=64) time: 6.572
2026-01-08 14:08:34,294: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-08 14:08:34,295: t15.2023.08.13 val PER: 0.1123
2026-01-08 14:08:34,295: t15.2023.08.18 val PER: 0.1065
2026-01-08 14:08:34,295: t15.2023.08.20 val PER: 0.1033
2026-01-08 14:08:34,295: t15.2023.08.25 val PER: 0.0949
2026-01-08 14:08:34,295: t15.2023.08.27 val PER: 0.1833
2026-01-08 14:08:34,295: t15.2023.09.01 val PER: 0.0804
2026-01-08 14:08:34,295: t15.2023.09.03 val PER: 0.1663
2026-01-08 14:08:34,295: t15.2023.09.24 val PER: 0.1226
2026-01-08 14:08:34,295: t15.2023.09.29 val PER: 0.1315
2026-01-08 14:08:34,295: t15.2023.10.01 val PER: 0.1724
2026-01-08 14:08:34,295: t15.2023.10.06 val PER: 0.0850
2026-01-08 14:08:34,295: t15.2023.10.08 val PER: 0.2503
2026-01-08 14:08:34,295: t15.2023.10.13 val PER: 0.2009
2026-01-08 14:08:34,296: t15.2023.10.15 val PER: 0.1734
2026-01-08 14:08:34,296: t15.2023.10.20 val PER: 0.1812
2026-01-08 14:08:34,296: t15.2023.10.22 val PER: 0.1192
2026-01-08 14:08:34,296: t15.2023.11.03 val PER: 0.1825
2026-01-08 14:08:34,296: t15.2023.11.04 val PER: 0.0273
2026-01-08 14:08:34,296: t15.2023.11.17 val PER: 0.0451
2026-01-08 14:08:34,296: t15.2023.11.19 val PER: 0.0319
2026-01-08 14:08:34,296: t15.2023.11.26 val PER: 0.1283
2026-01-08 14:08:34,296: t15.2023.12.03 val PER: 0.1082
2026-01-08 14:08:34,296: t15.2023.12.08 val PER: 0.1059
2026-01-08 14:08:34,296: t15.2023.12.10 val PER: 0.0959
2026-01-08 14:08:34,296: t15.2023.12.17 val PER: 0.1331
2026-01-08 14:08:34,296: t15.2023.12.29 val PER: 0.1311
2026-01-08 14:08:34,297: t15.2024.02.25 val PER: 0.1180
2026-01-08 14:08:34,297: t15.2024.03.08 val PER: 0.2290
2026-01-08 14:08:34,297: t15.2024.03.15 val PER: 0.1995
2026-01-08 14:08:34,297: t15.2024.03.17 val PER: 0.1444
2026-01-08 14:08:34,297: t15.2024.05.10 val PER: 0.1753
2026-01-08 14:08:34,297: t15.2024.06.14 val PER: 0.1672
2026-01-08 14:08:34,297: t15.2024.07.19 val PER: 0.2485
2026-01-08 14:08:34,297: t15.2024.07.21 val PER: 0.0972
2026-01-08 14:08:34,297: t15.2024.07.28 val PER: 0.1412
2026-01-08 14:08:34,297: t15.2025.01.10 val PER: 0.2824
2026-01-08 14:08:34,297: t15.2025.01.12 val PER: 0.1532
2026-01-08 14:08:34,297: t15.2025.03.14 val PER: 0.3565
2026-01-08 14:08:34,297: t15.2025.03.16 val PER: 0.1885
2026-01-08 14:08:34,298: t15.2025.03.30 val PER: 0.2782
2026-01-08 14:08:34,298: t15.2025.04.13 val PER: 0.2225
2026-01-08 14:08:43,384: Train batch 14600: loss: 12.76 grad norm: 59.45 time: 0.059
2026-01-08 14:09:01,031: Train batch 14800: loss: 6.04 grad norm: 44.34 time: 0.052
2026-01-08 14:09:18,936: Train batch 15000: loss: 8.72 grad norm: 52.74 time: 0.053
2026-01-08 14:09:18,937: Running test after training batch: 15000
2026-01-08 14:09:19,047: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:09:23,952: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:09:23,984: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ent
2026-01-08 14:09:25,749: Val batch 15000: PER (avg): 0.1499 CTC Loss (avg): 15.3146 WER(1gram): 45.94% (n=64) time: 6.812
2026-01-08 14:09:25,749: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=12
2026-01-08 14:09:25,749: t15.2023.08.13 val PER: 0.1091
2026-01-08 14:09:25,749: t15.2023.08.18 val PER: 0.1065
2026-01-08 14:09:25,750: t15.2023.08.20 val PER: 0.1017
2026-01-08 14:09:25,750: t15.2023.08.25 val PER: 0.0919
2026-01-08 14:09:25,750: t15.2023.08.27 val PER: 0.1865
2026-01-08 14:09:25,750: t15.2023.09.01 val PER: 0.0795
2026-01-08 14:09:25,750: t15.2023.09.03 val PER: 0.1663
2026-01-08 14:09:25,750: t15.2023.09.24 val PER: 0.1262
2026-01-08 14:09:25,750: t15.2023.09.29 val PER: 0.1353
2026-01-08 14:09:25,750: t15.2023.10.01 val PER: 0.1645
2026-01-08 14:09:25,750: t15.2023.10.06 val PER: 0.0883
2026-01-08 14:09:25,750: t15.2023.10.08 val PER: 0.2436
2026-01-08 14:09:25,750: t15.2023.10.13 val PER: 0.2017
2026-01-08 14:09:25,750: t15.2023.10.15 val PER: 0.1569
2026-01-08 14:09:25,750: t15.2023.10.20 val PER: 0.1913
2026-01-08 14:09:25,751: t15.2023.10.22 val PER: 0.1136
2026-01-08 14:09:25,751: t15.2023.11.03 val PER: 0.1744
2026-01-08 14:09:25,751: t15.2023.11.04 val PER: 0.0375
2026-01-08 14:09:25,751: t15.2023.11.17 val PER: 0.0404
2026-01-08 14:09:25,751: t15.2023.11.19 val PER: 0.0379
2026-01-08 14:09:25,751: t15.2023.11.26 val PER: 0.1261
2026-01-08 14:09:25,751: t15.2023.12.03 val PER: 0.1103
2026-01-08 14:09:25,751: t15.2023.12.08 val PER: 0.1065
2026-01-08 14:09:25,751: t15.2023.12.10 val PER: 0.0946
2026-01-08 14:09:25,751: t15.2023.12.17 val PER: 0.1299
2026-01-08 14:09:25,751: t15.2023.12.29 val PER: 0.1297
2026-01-08 14:09:25,751: t15.2024.02.25 val PER: 0.1067
2026-01-08 14:09:25,751: t15.2024.03.08 val PER: 0.2162
2026-01-08 14:09:25,751: t15.2024.03.15 val PER: 0.2020
2026-01-08 14:09:25,751: t15.2024.03.17 val PER: 0.1360
2026-01-08 14:09:25,752: t15.2024.05.10 val PER: 0.1649
2026-01-08 14:09:25,752: t15.2024.06.14 val PER: 0.1672
2026-01-08 14:09:25,752: t15.2024.07.19 val PER: 0.2353
2026-01-08 14:09:25,752: t15.2024.07.21 val PER: 0.0883
2026-01-08 14:09:25,752: t15.2024.07.28 val PER: 0.1272
2026-01-08 14:09:25,752: t15.2025.01.10 val PER: 0.2920
2026-01-08 14:09:25,752: t15.2025.01.12 val PER: 0.1463
2026-01-08 14:09:25,752: t15.2025.03.14 val PER: 0.3506
2026-01-08 14:09:25,752: t15.2025.03.16 val PER: 0.1885
2026-01-08 14:09:25,752: t15.2025.03.30 val PER: 0.2908
2026-01-08 14:09:25,752: t15.2025.04.13 val PER: 0.2168
2026-01-08 14:09:25,753: New best val WER(1gram) 46.45% --> 45.94%
2026-01-08 14:09:25,753: Checkpointing model
2026-01-08 14:09:25,892: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 14:09:44,288: Train batch 15200: loss: 5.19 grad norm: 41.12 time: 0.057
2026-01-08 14:10:01,449: Train batch 15400: loss: 11.22 grad norm: 57.70 time: 0.049
2026-01-08 14:10:10,378: Running test after training batch: 15500
2026-01-08 14:10:10,484: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:10:15,214: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the cold at this point as will
2026-01-08 14:10:15,245: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ged
2026-01-08 14:10:17,044: Val batch 15500: PER (avg): 0.1507 CTC Loss (avg): 15.3287 WER(1gram): 47.21% (n=64) time: 6.665
2026-01-08 14:10:17,044: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-08 14:10:17,044: t15.2023.08.13 val PER: 0.1071
2026-01-08 14:10:17,044: t15.2023.08.18 val PER: 0.1115
2026-01-08 14:10:17,044: t15.2023.08.20 val PER: 0.1009
2026-01-08 14:10:17,044: t15.2023.08.25 val PER: 0.0949
2026-01-08 14:10:17,044: t15.2023.08.27 val PER: 0.1961
2026-01-08 14:10:17,045: t15.2023.09.01 val PER: 0.0771
2026-01-08 14:10:17,045: t15.2023.09.03 val PER: 0.1627
2026-01-08 14:10:17,045: t15.2023.09.24 val PER: 0.1335
2026-01-08 14:10:17,045: t15.2023.09.29 val PER: 0.1334
2026-01-08 14:10:17,045: t15.2023.10.01 val PER: 0.1631
2026-01-08 14:10:17,045: t15.2023.10.06 val PER: 0.0926
2026-01-08 14:10:17,045: t15.2023.10.08 val PER: 0.2368
2026-01-08 14:10:17,045: t15.2023.10.13 val PER: 0.2002
2026-01-08 14:10:17,045: t15.2023.10.15 val PER: 0.1589
2026-01-08 14:10:17,045: t15.2023.10.20 val PER: 0.1879
2026-01-08 14:10:17,046: t15.2023.10.22 val PER: 0.1192
2026-01-08 14:10:17,046: t15.2023.11.03 val PER: 0.1818
2026-01-08 14:10:17,046: t15.2023.11.04 val PER: 0.0410
2026-01-08 14:10:17,046: t15.2023.11.17 val PER: 0.0373
2026-01-08 14:10:17,046: t15.2023.11.19 val PER: 0.0379
2026-01-08 14:10:17,046: t15.2023.11.26 val PER: 0.1196
2026-01-08 14:10:17,046: t15.2023.12.03 val PER: 0.1071
2026-01-08 14:10:17,046: t15.2023.12.08 val PER: 0.1052
2026-01-08 14:10:17,046: t15.2023.12.10 val PER: 0.0933
2026-01-08 14:10:17,046: t15.2023.12.17 val PER: 0.1310
2026-01-08 14:10:17,046: t15.2023.12.29 val PER: 0.1366
2026-01-08 14:10:17,047: t15.2024.02.25 val PER: 0.0955
2026-01-08 14:10:17,047: t15.2024.03.08 val PER: 0.2361
2026-01-08 14:10:17,047: t15.2024.03.15 val PER: 0.2014
2026-01-08 14:10:17,047: t15.2024.03.17 val PER: 0.1346
2026-01-08 14:10:17,047: t15.2024.05.10 val PER: 0.1620
2026-01-08 14:10:17,047: t15.2024.06.14 val PER: 0.1546
2026-01-08 14:10:17,047: t15.2024.07.19 val PER: 0.2439
2026-01-08 14:10:17,047: t15.2024.07.21 val PER: 0.0917
2026-01-08 14:10:17,047: t15.2024.07.28 val PER: 0.1294
2026-01-08 14:10:17,047: t15.2025.01.10 val PER: 0.2975
2026-01-08 14:10:17,047: t15.2025.01.12 val PER: 0.1486
2026-01-08 14:10:17,048: t15.2025.03.14 val PER: 0.3476
2026-01-08 14:10:17,048: t15.2025.03.16 val PER: 0.1885
2026-01-08 14:10:17,048: t15.2025.03.30 val PER: 0.2931
2026-01-08 14:10:17,048: t15.2025.04.13 val PER: 0.2068
2026-01-08 14:10:25,576: Train batch 15600: loss: 11.77 grad norm: 53.86 time: 0.063
2026-01-08 14:10:43,752: Train batch 15800: loss: 14.06 grad norm: 61.42 time: 0.067
2026-01-08 14:11:02,406: Train batch 16000: loss: 9.19 grad norm: 47.47 time: 0.057
2026-01-08 14:11:02,406: Running test after training batch: 16000
2026-01-08 14:11:02,515: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:11:07,362: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-08 14:11:07,395: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ged
2026-01-08 14:11:09,242: Val batch 16000: PER (avg): 0.1507 CTC Loss (avg): 15.3954 WER(1gram): 45.18% (n=64) time: 6.836
2026-01-08 14:11:09,243: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-08 14:11:09,243: t15.2023.08.13 val PER: 0.1081
2026-01-08 14:11:09,243: t15.2023.08.18 val PER: 0.1056
2026-01-08 14:11:09,243: t15.2023.08.20 val PER: 0.1025
2026-01-08 14:11:09,243: t15.2023.08.25 val PER: 0.0994
2026-01-08 14:11:09,243: t15.2023.08.27 val PER: 0.1865
2026-01-08 14:11:09,243: t15.2023.09.01 val PER: 0.0722
2026-01-08 14:11:09,243: t15.2023.09.03 val PER: 0.1591
2026-01-08 14:11:09,243: t15.2023.09.24 val PER: 0.1274
2026-01-08 14:11:09,244: t15.2023.09.29 val PER: 0.1391
2026-01-08 14:11:09,244: t15.2023.10.01 val PER: 0.1625
2026-01-08 14:11:09,244: t15.2023.10.06 val PER: 0.0904
2026-01-08 14:11:09,244: t15.2023.10.08 val PER: 0.2544
2026-01-08 14:11:09,244: t15.2023.10.13 val PER: 0.1978
2026-01-08 14:11:09,244: t15.2023.10.15 val PER: 0.1529
2026-01-08 14:11:09,244: t15.2023.10.20 val PER: 0.1946
2026-01-08 14:11:09,244: t15.2023.10.22 val PER: 0.1203
2026-01-08 14:11:09,244: t15.2023.11.03 val PER: 0.1764
2026-01-08 14:11:09,244: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:11:09,244: t15.2023.11.17 val PER: 0.0373
2026-01-08 14:11:09,244: t15.2023.11.19 val PER: 0.0359
2026-01-08 14:11:09,244: t15.2023.11.26 val PER: 0.1246
2026-01-08 14:11:09,244: t15.2023.12.03 val PER: 0.1082
2026-01-08 14:11:09,244: t15.2023.12.08 val PER: 0.1045
2026-01-08 14:11:09,245: t15.2023.12.10 val PER: 0.0920
2026-01-08 14:11:09,245: t15.2023.12.17 val PER: 0.1351
2026-01-08 14:11:09,245: t15.2023.12.29 val PER: 0.1290
2026-01-08 14:11:09,245: t15.2024.02.25 val PER: 0.1067
2026-01-08 14:11:09,245: t15.2024.03.08 val PER: 0.2347
2026-01-08 14:11:09,245: t15.2024.03.15 val PER: 0.2008
2026-01-08 14:11:09,245: t15.2024.03.17 val PER: 0.1395
2026-01-08 14:11:09,245: t15.2024.05.10 val PER: 0.1664
2026-01-08 14:11:09,245: t15.2024.06.14 val PER: 0.1625
2026-01-08 14:11:09,245: t15.2024.07.19 val PER: 0.2347
2026-01-08 14:11:09,245: t15.2024.07.21 val PER: 0.0841
2026-01-08 14:11:09,245: t15.2024.07.28 val PER: 0.1382
2026-01-08 14:11:09,245: t15.2025.01.10 val PER: 0.3127
2026-01-08 14:11:09,245: t15.2025.01.12 val PER: 0.1486
2026-01-08 14:11:09,245: t15.2025.03.14 val PER: 0.3506
2026-01-08 14:11:09,245: t15.2025.03.16 val PER: 0.1846
2026-01-08 14:11:09,245: t15.2025.03.30 val PER: 0.2897
2026-01-08 14:11:09,246: t15.2025.04.13 val PER: 0.2211
2026-01-08 14:11:09,247: New best val WER(1gram) 45.94% --> 45.18%
2026-01-08 14:11:09,248: Checkpointing model
2026-01-08 14:11:09,390: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 14:11:27,973: Train batch 16200: loss: 6.47 grad norm: 42.82 time: 0.056
2026-01-08 14:11:46,248: Train batch 16400: loss: 11.60 grad norm: 65.72 time: 0.057
2026-01-08 14:11:54,776: Running test after training batch: 16500
2026-01-08 14:11:54,898: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:11:59,548: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-08 14:11:59,581: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-08 14:12:01,396: Val batch 16500: PER (avg): 0.1482 CTC Loss (avg): 15.2593 WER(1gram): 45.94% (n=64) time: 6.620
2026-01-08 14:12:01,397: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-08 14:12:01,397: t15.2023.08.13 val PER: 0.1091
2026-01-08 14:12:01,397: t15.2023.08.18 val PER: 0.1073
2026-01-08 14:12:01,397: t15.2023.08.20 val PER: 0.1064
2026-01-08 14:12:01,397: t15.2023.08.25 val PER: 0.0979
2026-01-08 14:12:01,397: t15.2023.08.27 val PER: 0.1833
2026-01-08 14:12:01,397: t15.2023.09.01 val PER: 0.0706
2026-01-08 14:12:01,397: t15.2023.09.03 val PER: 0.1603
2026-01-08 14:12:01,397: t15.2023.09.24 val PER: 0.1250
2026-01-08 14:12:01,397: t15.2023.09.29 val PER: 0.1359
2026-01-08 14:12:01,398: t15.2023.10.01 val PER: 0.1651
2026-01-08 14:12:01,398: t15.2023.10.06 val PER: 0.0893
2026-01-08 14:12:01,398: t15.2023.10.08 val PER: 0.2436
2026-01-08 14:12:01,398: t15.2023.10.13 val PER: 0.1955
2026-01-08 14:12:01,398: t15.2023.10.15 val PER: 0.1503
2026-01-08 14:12:01,398: t15.2023.10.20 val PER: 0.1946
2026-01-08 14:12:01,398: t15.2023.10.22 val PER: 0.1214
2026-01-08 14:12:01,398: t15.2023.11.03 val PER: 0.1771
2026-01-08 14:12:01,398: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:12:01,398: t15.2023.11.17 val PER: 0.0373
2026-01-08 14:12:01,399: t15.2023.11.19 val PER: 0.0339
2026-01-08 14:12:01,399: t15.2023.11.26 val PER: 0.1254
2026-01-08 14:12:01,399: t15.2023.12.03 val PER: 0.1050
2026-01-08 14:12:01,399: t15.2023.12.08 val PER: 0.0972
2026-01-08 14:12:01,399: t15.2023.12.10 val PER: 0.0907
2026-01-08 14:12:01,399: t15.2023.12.17 val PER: 0.1351
2026-01-08 14:12:01,399: t15.2023.12.29 val PER: 0.1263
2026-01-08 14:12:01,399: t15.2024.02.25 val PER: 0.1025
2026-01-08 14:12:01,399: t15.2024.03.08 val PER: 0.2361
2026-01-08 14:12:01,399: t15.2024.03.15 val PER: 0.1951
2026-01-08 14:12:01,399: t15.2024.03.17 val PER: 0.1304
2026-01-08 14:12:01,399: t15.2024.05.10 val PER: 0.1575
2026-01-08 14:12:01,399: t15.2024.06.14 val PER: 0.1656
2026-01-08 14:12:01,399: t15.2024.07.19 val PER: 0.2353
2026-01-08 14:12:01,399: t15.2024.07.21 val PER: 0.0834
2026-01-08 14:12:01,399: t15.2024.07.28 val PER: 0.1316
2026-01-08 14:12:01,400: t15.2025.01.10 val PER: 0.3030
2026-01-08 14:12:01,400: t15.2025.01.12 val PER: 0.1463
2026-01-08 14:12:01,400: t15.2025.03.14 val PER: 0.3462
2026-01-08 14:12:01,400: t15.2025.03.16 val PER: 0.1780
2026-01-08 14:12:01,400: t15.2025.03.30 val PER: 0.2793
2026-01-08 14:12:01,400: t15.2025.04.13 val PER: 0.2083
2026-01-08 14:12:10,317: Train batch 16600: loss: 8.61 grad norm: 50.32 time: 0.053
2026-01-08 14:12:27,285: Train batch 16800: loss: 17.24 grad norm: 72.03 time: 0.062
2026-01-08 14:12:44,226: Train batch 17000: loss: 8.17 grad norm: 47.02 time: 0.082
2026-01-08 14:12:44,226: Running test after training batch: 17000
2026-01-08 14:12:45,251: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:12:50,522: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-08 14:12:50,555: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-08 14:12:52,372: Val batch 17000: PER (avg): 0.1469 CTC Loss (avg): 15.1802 WER(1gram): 46.70% (n=64) time: 8.146
2026-01-08 14:12:52,372: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-08 14:12:52,372: t15.2023.08.13 val PER: 0.1019
2026-01-08 14:12:52,372: t15.2023.08.18 val PER: 0.1115
2026-01-08 14:12:52,372: t15.2023.08.20 val PER: 0.0969
2026-01-08 14:12:52,372: t15.2023.08.25 val PER: 0.0994
2026-01-08 14:12:52,372: t15.2023.08.27 val PER: 0.1865
2026-01-08 14:12:52,373: t15.2023.09.01 val PER: 0.0698
2026-01-08 14:12:52,373: t15.2023.09.03 val PER: 0.1580
2026-01-08 14:12:52,373: t15.2023.09.24 val PER: 0.1214
2026-01-08 14:12:52,373: t15.2023.09.29 val PER: 0.1334
2026-01-08 14:12:52,373: t15.2023.10.01 val PER: 0.1612
2026-01-08 14:12:52,373: t15.2023.10.06 val PER: 0.0915
2026-01-08 14:12:52,373: t15.2023.10.08 val PER: 0.2490
2026-01-08 14:12:52,373: t15.2023.10.13 val PER: 0.2017
2026-01-08 14:12:52,373: t15.2023.10.15 val PER: 0.1582
2026-01-08 14:12:52,373: t15.2023.10.20 val PER: 0.1846
2026-01-08 14:12:52,373: t15.2023.10.22 val PER: 0.1125
2026-01-08 14:12:52,373: t15.2023.11.03 val PER: 0.1764
2026-01-08 14:12:52,373: t15.2023.11.04 val PER: 0.0375
2026-01-08 14:12:52,373: t15.2023.11.17 val PER: 0.0358
2026-01-08 14:12:52,373: t15.2023.11.19 val PER: 0.0359
2026-01-08 14:12:52,373: t15.2023.11.26 val PER: 0.1188
2026-01-08 14:12:52,374: t15.2023.12.03 val PER: 0.1019
2026-01-08 14:12:52,374: t15.2023.12.08 val PER: 0.0979
2026-01-08 14:12:52,374: t15.2023.12.10 val PER: 0.0959
2026-01-08 14:12:52,374: t15.2023.12.17 val PER: 0.1372
2026-01-08 14:12:52,374: t15.2023.12.29 val PER: 0.1256
2026-01-08 14:12:52,374: t15.2024.02.25 val PER: 0.0955
2026-01-08 14:12:52,374: t15.2024.03.08 val PER: 0.2262
2026-01-08 14:12:52,374: t15.2024.03.15 val PER: 0.1951
2026-01-08 14:12:52,374: t15.2024.03.17 val PER: 0.1297
2026-01-08 14:12:52,374: t15.2024.05.10 val PER: 0.1605
2026-01-08 14:12:52,374: t15.2024.06.14 val PER: 0.1546
2026-01-08 14:12:52,374: t15.2024.07.19 val PER: 0.2274
2026-01-08 14:12:52,374: t15.2024.07.21 val PER: 0.0876
2026-01-08 14:12:52,374: t15.2024.07.28 val PER: 0.1309
2026-01-08 14:12:52,374: t15.2025.01.10 val PER: 0.3044
2026-01-08 14:12:52,374: t15.2025.01.12 val PER: 0.1416
2026-01-08 14:12:52,374: t15.2025.03.14 val PER: 0.3462
2026-01-08 14:12:52,375: t15.2025.03.16 val PER: 0.1780
2026-01-08 14:12:52,375: t15.2025.03.30 val PER: 0.2805
2026-01-08 14:12:52,375: t15.2025.04.13 val PER: 0.2026
2026-01-08 14:13:10,791: Train batch 17200: loss: 9.54 grad norm: 46.66 time: 0.087
2026-01-08 14:13:28,573: Train batch 17400: loss: 12.26 grad norm: 60.17 time: 0.071
2026-01-08 14:13:37,134: Running test after training batch: 17500
2026-01-08 14:13:37,230: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:13:41,961: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code atz this point as will
2026-01-08 14:13:41,994: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sends
2026-01-08 14:13:43,852: Val batch 17500: PER (avg): 0.1475 CTC Loss (avg): 15.1586 WER(1gram): 47.72% (n=64) time: 6.717
2026-01-08 14:13:43,852: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-08 14:13:43,853: t15.2023.08.13 val PER: 0.1112
2026-01-08 14:13:43,853: t15.2023.08.18 val PER: 0.1090
2026-01-08 14:13:43,853: t15.2023.08.20 val PER: 0.1017
2026-01-08 14:13:43,853: t15.2023.08.25 val PER: 0.0994
2026-01-08 14:13:43,853: t15.2023.08.27 val PER: 0.1833
2026-01-08 14:13:43,853: t15.2023.09.01 val PER: 0.0739
2026-01-08 14:13:43,853: t15.2023.09.03 val PER: 0.1556
2026-01-08 14:13:43,853: t15.2023.09.24 val PER: 0.1250
2026-01-08 14:13:43,853: t15.2023.09.29 val PER: 0.1302
2026-01-08 14:13:43,853: t15.2023.10.01 val PER: 0.1664
2026-01-08 14:13:43,853: t15.2023.10.06 val PER: 0.0893
2026-01-08 14:13:43,854: t15.2023.10.08 val PER: 0.2368
2026-01-08 14:13:43,854: t15.2023.10.13 val PER: 0.1924
2026-01-08 14:13:43,854: t15.2023.10.15 val PER: 0.1536
2026-01-08 14:13:43,854: t15.2023.10.20 val PER: 0.2047
2026-01-08 14:13:43,854: t15.2023.10.22 val PER: 0.1114
2026-01-08 14:13:43,854: t15.2023.11.03 val PER: 0.1757
2026-01-08 14:13:43,854: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:13:43,854: t15.2023.11.17 val PER: 0.0342
2026-01-08 14:13:43,854: t15.2023.11.19 val PER: 0.0339
2026-01-08 14:13:43,854: t15.2023.11.26 val PER: 0.1188
2026-01-08 14:13:43,854: t15.2023.12.03 val PER: 0.1019
2026-01-08 14:13:43,854: t15.2023.12.08 val PER: 0.1045
2026-01-08 14:13:43,854: t15.2023.12.10 val PER: 0.0880
2026-01-08 14:13:43,854: t15.2023.12.17 val PER: 0.1341
2026-01-08 14:13:43,854: t15.2023.12.29 val PER: 0.1256
2026-01-08 14:13:43,855: t15.2024.02.25 val PER: 0.0997
2026-01-08 14:13:43,855: t15.2024.03.08 val PER: 0.2290
2026-01-08 14:13:43,855: t15.2024.03.15 val PER: 0.1964
2026-01-08 14:13:43,855: t15.2024.03.17 val PER: 0.1311
2026-01-08 14:13:43,855: t15.2024.05.10 val PER: 0.1560
2026-01-08 14:13:43,855: t15.2024.06.14 val PER: 0.1577
2026-01-08 14:13:43,855: t15.2024.07.19 val PER: 0.2334
2026-01-08 14:13:43,855: t15.2024.07.21 val PER: 0.0834
2026-01-08 14:13:43,855: t15.2024.07.28 val PER: 0.1338
2026-01-08 14:13:43,855: t15.2025.01.10 val PER: 0.3030
2026-01-08 14:13:43,855: t15.2025.01.12 val PER: 0.1424
2026-01-08 14:13:43,855: t15.2025.03.14 val PER: 0.3447
2026-01-08 14:13:43,855: t15.2025.03.16 val PER: 0.1819
2026-01-08 14:13:43,855: t15.2025.03.30 val PER: 0.2862
2026-01-08 14:13:43,855: t15.2025.04.13 val PER: 0.2154
2026-01-08 14:13:52,240: Train batch 17600: loss: 9.91 grad norm: 55.36 time: 0.051
2026-01-08 14:14:09,720: Train batch 17800: loss: 6.32 grad norm: 49.61 time: 0.044
2026-01-08 14:14:28,311: Train batch 18000: loss: 11.05 grad norm: 58.56 time: 0.062
2026-01-08 14:14:28,311: Running test after training batch: 18000
2026-01-08 14:14:28,449: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:14:33,138: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-08 14:14:33,171: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-08 14:14:35,024: Val batch 18000: PER (avg): 0.1467 CTC Loss (avg): 15.1139 WER(1gram): 45.43% (n=64) time: 6.713
2026-01-08 14:14:35,024: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-08 14:14:35,025: t15.2023.08.13 val PER: 0.1050
2026-01-08 14:14:35,025: t15.2023.08.18 val PER: 0.1098
2026-01-08 14:14:35,025: t15.2023.08.20 val PER: 0.0985
2026-01-08 14:14:35,025: t15.2023.08.25 val PER: 0.0964
2026-01-08 14:14:35,025: t15.2023.08.27 val PER: 0.1801
2026-01-08 14:14:35,025: t15.2023.09.01 val PER: 0.0739
2026-01-08 14:14:35,025: t15.2023.09.03 val PER: 0.1544
2026-01-08 14:14:35,025: t15.2023.09.24 val PER: 0.1226
2026-01-08 14:14:35,025: t15.2023.09.29 val PER: 0.1321
2026-01-08 14:14:35,025: t15.2023.10.01 val PER: 0.1605
2026-01-08 14:14:35,025: t15.2023.10.06 val PER: 0.0893
2026-01-08 14:14:35,025: t15.2023.10.08 val PER: 0.2463
2026-01-08 14:14:35,026: t15.2023.10.13 val PER: 0.1939
2026-01-08 14:14:35,026: t15.2023.10.15 val PER: 0.1523
2026-01-08 14:14:35,026: t15.2023.10.20 val PER: 0.1946
2026-01-08 14:14:35,026: t15.2023.10.22 val PER: 0.1069
2026-01-08 14:14:35,026: t15.2023.11.03 val PER: 0.1784
2026-01-08 14:14:35,026: t15.2023.11.04 val PER: 0.0410
2026-01-08 14:14:35,026: t15.2023.11.17 val PER: 0.0404
2026-01-08 14:14:35,026: t15.2023.11.19 val PER: 0.0299
2026-01-08 14:14:35,026: t15.2023.11.26 val PER: 0.1181
2026-01-08 14:14:35,026: t15.2023.12.03 val PER: 0.1071
2026-01-08 14:14:35,026: t15.2023.12.08 val PER: 0.1039
2026-01-08 14:14:35,027: t15.2023.12.10 val PER: 0.0933
2026-01-08 14:14:35,027: t15.2023.12.17 val PER: 0.1331
2026-01-08 14:14:35,027: t15.2023.12.29 val PER: 0.1304
2026-01-08 14:14:35,027: t15.2024.02.25 val PER: 0.1067
2026-01-08 14:14:35,027: t15.2024.03.08 val PER: 0.2347
2026-01-08 14:14:35,027: t15.2024.03.15 val PER: 0.1951
2026-01-08 14:14:35,027: t15.2024.03.17 val PER: 0.1262
2026-01-08 14:14:35,027: t15.2024.05.10 val PER: 0.1605
2026-01-08 14:14:35,027: t15.2024.06.14 val PER: 0.1546
2026-01-08 14:14:35,027: t15.2024.07.19 val PER: 0.2353
2026-01-08 14:14:35,027: t15.2024.07.21 val PER: 0.0834
2026-01-08 14:14:35,027: t15.2024.07.28 val PER: 0.1257
2026-01-08 14:14:35,027: t15.2025.01.10 val PER: 0.2989
2026-01-08 14:14:35,028: t15.2025.01.12 val PER: 0.1432
2026-01-08 14:14:35,028: t15.2025.03.14 val PER: 0.3373
2026-01-08 14:14:35,028: t15.2025.03.16 val PER: 0.1728
2026-01-08 14:14:35,028: t15.2025.03.30 val PER: 0.2782
2026-01-08 14:14:35,028: t15.2025.04.13 val PER: 0.2140
2026-01-08 14:14:53,696: Train batch 18200: loss: 8.23 grad norm: 48.00 time: 0.077
2026-01-08 14:15:12,028: Train batch 18400: loss: 5.00 grad norm: 42.36 time: 0.058
2026-01-08 14:15:21,182: Running test after training batch: 18500
2026-01-08 14:15:21,324: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:15:26,148: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code atz this point as will
2026-01-08 14:15:26,187: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost edds
2026-01-08 14:15:28,236: Val batch 18500: PER (avg): 0.1463 CTC Loss (avg): 15.1213 WER(1gram): 46.19% (n=64) time: 7.054
2026-01-08 14:15:28,237: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=12
2026-01-08 14:15:28,237: t15.2023.08.13 val PER: 0.1050
2026-01-08 14:15:28,237: t15.2023.08.18 val PER: 0.1132
2026-01-08 14:15:28,237: t15.2023.08.20 val PER: 0.1001
2026-01-08 14:15:28,237: t15.2023.08.25 val PER: 0.0949
2026-01-08 14:15:28,237: t15.2023.08.27 val PER: 0.1865
2026-01-08 14:15:28,237: t15.2023.09.01 val PER: 0.0747
2026-01-08 14:15:28,237: t15.2023.09.03 val PER: 0.1615
2026-01-08 14:15:28,237: t15.2023.09.24 val PER: 0.1214
2026-01-08 14:15:28,238: t15.2023.09.29 val PER: 0.1334
2026-01-08 14:15:28,238: t15.2023.10.01 val PER: 0.1605
2026-01-08 14:15:28,238: t15.2023.10.06 val PER: 0.0850
2026-01-08 14:15:28,238: t15.2023.10.08 val PER: 0.2476
2026-01-08 14:15:28,238: t15.2023.10.13 val PER: 0.1932
2026-01-08 14:15:28,238: t15.2023.10.15 val PER: 0.1536
2026-01-08 14:15:28,238: t15.2023.10.20 val PER: 0.2013
2026-01-08 14:15:28,238: t15.2023.10.22 val PER: 0.1058
2026-01-08 14:15:28,238: t15.2023.11.03 val PER: 0.1737
2026-01-08 14:15:28,238: t15.2023.11.04 val PER: 0.0375
2026-01-08 14:15:28,238: t15.2023.11.17 val PER: 0.0373
2026-01-08 14:15:28,238: t15.2023.11.19 val PER: 0.0279
2026-01-08 14:15:28,238: t15.2023.11.26 val PER: 0.1210
2026-01-08 14:15:28,238: t15.2023.12.03 val PER: 0.1008
2026-01-08 14:15:28,238: t15.2023.12.08 val PER: 0.1072
2026-01-08 14:15:28,239: t15.2023.12.10 val PER: 0.0907
2026-01-08 14:15:28,239: t15.2023.12.17 val PER: 0.1320
2026-01-08 14:15:28,239: t15.2023.12.29 val PER: 0.1235
2026-01-08 14:15:28,239: t15.2024.02.25 val PER: 0.1025
2026-01-08 14:15:28,239: t15.2024.03.08 val PER: 0.2319
2026-01-08 14:15:28,239: t15.2024.03.15 val PER: 0.1926
2026-01-08 14:15:28,239: t15.2024.03.17 val PER: 0.1262
2026-01-08 14:15:28,239: t15.2024.05.10 val PER: 0.1516
2026-01-08 14:15:28,239: t15.2024.06.14 val PER: 0.1577
2026-01-08 14:15:28,239: t15.2024.07.19 val PER: 0.2307
2026-01-08 14:15:28,239: t15.2024.07.21 val PER: 0.0821
2026-01-08 14:15:28,239: t15.2024.07.28 val PER: 0.1287
2026-01-08 14:15:28,239: t15.2025.01.10 val PER: 0.2989
2026-01-08 14:15:28,239: t15.2025.01.12 val PER: 0.1424
2026-01-08 14:15:28,239: t15.2025.03.14 val PER: 0.3417
2026-01-08 14:15:28,239: t15.2025.03.16 val PER: 0.1754
2026-01-08 14:15:28,239: t15.2025.03.30 val PER: 0.2816
2026-01-08 14:15:28,240: t15.2025.04.13 val PER: 0.2111
2026-01-08 14:15:37,095: Train batch 18600: loss: 12.36 grad norm: 58.65 time: 0.070
2026-01-08 14:15:54,434: Train batch 18800: loss: 8.71 grad norm: 50.85 time: 0.066
2026-01-08 14:16:12,075: Train batch 19000: loss: 8.63 grad norm: 45.73 time: 0.067
2026-01-08 14:16:12,076: Running test after training batch: 19000
2026-01-08 14:16:12,170: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:16:16,987: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-08 14:16:17,025: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost edds
2026-01-08 14:16:19,287: Val batch 19000: PER (avg): 0.1455 CTC Loss (avg): 15.0967 WER(1gram): 44.92% (n=64) time: 7.212
2026-01-08 14:16:19,288: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=12
2026-01-08 14:16:19,288: t15.2023.08.13 val PER: 0.1050
2026-01-08 14:16:19,288: t15.2023.08.18 val PER: 0.1073
2026-01-08 14:16:19,288: t15.2023.08.20 val PER: 0.1009
2026-01-08 14:16:19,288: t15.2023.08.25 val PER: 0.0919
2026-01-08 14:16:19,288: t15.2023.08.27 val PER: 0.1801
2026-01-08 14:16:19,288: t15.2023.09.01 val PER: 0.0739
2026-01-08 14:16:19,288: t15.2023.09.03 val PER: 0.1580
2026-01-08 14:16:19,289: t15.2023.09.24 val PER: 0.1165
2026-01-08 14:16:19,289: t15.2023.09.29 val PER: 0.1334
2026-01-08 14:16:19,289: t15.2023.10.01 val PER: 0.1631
2026-01-08 14:16:19,289: t15.2023.10.06 val PER: 0.0872
2026-01-08 14:16:19,289: t15.2023.10.08 val PER: 0.2449
2026-01-08 14:16:19,289: t15.2023.10.13 val PER: 0.1978
2026-01-08 14:16:19,289: t15.2023.10.15 val PER: 0.1536
2026-01-08 14:16:19,289: t15.2023.10.20 val PER: 0.1913
2026-01-08 14:16:19,289: t15.2023.10.22 val PER: 0.1125
2026-01-08 14:16:19,289: t15.2023.11.03 val PER: 0.1771
2026-01-08 14:16:19,290: t15.2023.11.04 val PER: 0.0307
2026-01-08 14:16:19,290: t15.2023.11.17 val PER: 0.0358
2026-01-08 14:16:19,290: t15.2023.11.19 val PER: 0.0279
2026-01-08 14:16:19,290: t15.2023.11.26 val PER: 0.1181
2026-01-08 14:16:19,290: t15.2023.12.03 val PER: 0.1040
2026-01-08 14:16:19,290: t15.2023.12.08 val PER: 0.1019
2026-01-08 14:16:19,290: t15.2023.12.10 val PER: 0.0920
2026-01-08 14:16:19,290: t15.2023.12.17 val PER: 0.1279
2026-01-08 14:16:19,290: t15.2023.12.29 val PER: 0.1242
2026-01-08 14:16:19,290: t15.2024.02.25 val PER: 0.0969
2026-01-08 14:16:19,290: t15.2024.03.08 val PER: 0.2276
2026-01-08 14:16:19,290: t15.2024.03.15 val PER: 0.1907
2026-01-08 14:16:19,290: t15.2024.03.17 val PER: 0.1311
2026-01-08 14:16:19,291: t15.2024.05.10 val PER: 0.1471
2026-01-08 14:16:19,291: t15.2024.06.14 val PER: 0.1562
2026-01-08 14:16:19,291: t15.2024.07.19 val PER: 0.2307
2026-01-08 14:16:19,291: t15.2024.07.21 val PER: 0.0841
2026-01-08 14:16:19,291: t15.2024.07.28 val PER: 0.1294
2026-01-08 14:16:19,291: t15.2025.01.10 val PER: 0.2906
2026-01-08 14:16:19,291: t15.2025.01.12 val PER: 0.1401
2026-01-08 14:16:19,291: t15.2025.03.14 val PER: 0.3447
2026-01-08 14:16:19,291: t15.2025.03.16 val PER: 0.1754
2026-01-08 14:16:19,291: t15.2025.03.30 val PER: 0.2770
2026-01-08 14:16:19,291: t15.2025.04.13 val PER: 0.2083
2026-01-08 14:16:19,292: New best val WER(1gram) 45.18% --> 44.92%
2026-01-08 14:16:19,292: Checkpointing model
2026-01-08 14:16:19,432: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p20/checkpoint/best_checkpoint
2026-01-08 14:16:38,048: Train batch 19200: loss: 6.44 grad norm: 47.50 time: 0.064
2026-01-08 14:16:55,582: Train batch 19400: loss: 4.87 grad norm: 36.23 time: 0.054
2026-01-08 14:17:04,336: Running test after training batch: 19500
2026-01-08 14:17:04,476: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:17:09,406: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the cold atz this point as will
2026-01-08 14:17:09,439: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-08 14:17:11,282: Val batch 19500: PER (avg): 0.1458 CTC Loss (avg): 15.0678 WER(1gram): 45.94% (n=64) time: 6.946
2026-01-08 14:17:11,282: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=12
2026-01-08 14:17:11,282: t15.2023.08.13 val PER: 0.1071
2026-01-08 14:17:11,282: t15.2023.08.18 val PER: 0.1106
2026-01-08 14:17:11,282: t15.2023.08.20 val PER: 0.1033
2026-01-08 14:17:11,283: t15.2023.08.25 val PER: 0.0949
2026-01-08 14:17:11,283: t15.2023.08.27 val PER: 0.1768
2026-01-08 14:17:11,283: t15.2023.09.01 val PER: 0.0722
2026-01-08 14:17:11,283: t15.2023.09.03 val PER: 0.1532
2026-01-08 14:17:11,283: t15.2023.09.24 val PER: 0.1177
2026-01-08 14:17:11,283: t15.2023.09.29 val PER: 0.1359
2026-01-08 14:17:11,283: t15.2023.10.01 val PER: 0.1631
2026-01-08 14:17:11,283: t15.2023.10.06 val PER: 0.0893
2026-01-08 14:17:11,283: t15.2023.10.08 val PER: 0.2490
2026-01-08 14:17:11,283: t15.2023.10.13 val PER: 0.1916
2026-01-08 14:17:11,284: t15.2023.10.15 val PER: 0.1536
2026-01-08 14:17:11,284: t15.2023.10.20 val PER: 0.1879
2026-01-08 14:17:11,284: t15.2023.10.22 val PER: 0.1102
2026-01-08 14:17:11,284: t15.2023.11.03 val PER: 0.1777
2026-01-08 14:17:11,284: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:17:11,284: t15.2023.11.17 val PER: 0.0295
2026-01-08 14:17:11,284: t15.2023.11.19 val PER: 0.0299
2026-01-08 14:17:11,284: t15.2023.11.26 val PER: 0.1174
2026-01-08 14:17:11,284: t15.2023.12.03 val PER: 0.1019
2026-01-08 14:17:11,284: t15.2023.12.08 val PER: 0.1019
2026-01-08 14:17:11,284: t15.2023.12.10 val PER: 0.0933
2026-01-08 14:17:11,284: t15.2023.12.17 val PER: 0.1247
2026-01-08 14:17:11,284: t15.2023.12.29 val PER: 0.1235
2026-01-08 14:17:11,284: t15.2024.02.25 val PER: 0.0997
2026-01-08 14:17:11,284: t15.2024.03.08 val PER: 0.2276
2026-01-08 14:17:11,284: t15.2024.03.15 val PER: 0.1901
2026-01-08 14:17:11,284: t15.2024.03.17 val PER: 0.1269
2026-01-08 14:17:11,285: t15.2024.05.10 val PER: 0.1501
2026-01-08 14:17:11,285: t15.2024.06.14 val PER: 0.1593
2026-01-08 14:17:11,285: t15.2024.07.19 val PER: 0.2320
2026-01-08 14:17:11,285: t15.2024.07.21 val PER: 0.0821
2026-01-08 14:17:11,285: t15.2024.07.28 val PER: 0.1287
2026-01-08 14:17:11,285: t15.2025.01.10 val PER: 0.3017
2026-01-08 14:17:11,285: t15.2025.01.12 val PER: 0.1440
2026-01-08 14:17:11,285: t15.2025.03.14 val PER: 0.3417
2026-01-08 14:17:11,285: t15.2025.03.16 val PER: 0.1780
2026-01-08 14:17:11,285: t15.2025.03.30 val PER: 0.2759
2026-01-08 14:17:11,285: t15.2025.04.13 val PER: 0.2168
2026-01-08 14:17:19,831: Train batch 19600: loss: 7.74 grad norm: 47.26 time: 0.057
2026-01-08 14:17:36,986: Train batch 19800: loss: 7.78 grad norm: 52.67 time: 0.055
2026-01-08 14:17:54,334: Running test after training batch: 19999
2026-01-08 14:17:54,423: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:17:59,061: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code atz this point as will
2026-01-08 14:17:59,094: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-08 14:18:00,990: Val batch 19999: PER (avg): 0.1460 CTC Loss (avg): 15.0607 WER(1gram): 45.18% (n=64) time: 6.656
2026-01-08 14:18:00,990: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=12
2026-01-08 14:18:00,990: t15.2023.08.13 val PER: 0.1040
2026-01-08 14:18:00,990: t15.2023.08.18 val PER: 0.1106
2026-01-08 14:18:00,990: t15.2023.08.20 val PER: 0.1025
2026-01-08 14:18:00,990: t15.2023.08.25 val PER: 0.0934
2026-01-08 14:18:00,990: t15.2023.08.27 val PER: 0.1817
2026-01-08 14:18:00,990: t15.2023.09.01 val PER: 0.0771
2026-01-08 14:18:00,990: t15.2023.09.03 val PER: 0.1603
2026-01-08 14:18:00,990: t15.2023.09.24 val PER: 0.1189
2026-01-08 14:18:00,991: t15.2023.09.29 val PER: 0.1327
2026-01-08 14:18:00,991: t15.2023.10.01 val PER: 0.1625
2026-01-08 14:18:00,991: t15.2023.10.06 val PER: 0.0850
2026-01-08 14:18:00,991: t15.2023.10.08 val PER: 0.2463
2026-01-08 14:18:00,991: t15.2023.10.13 val PER: 0.1963
2026-01-08 14:18:00,991: t15.2023.10.15 val PER: 0.1510
2026-01-08 14:18:00,991: t15.2023.10.20 val PER: 0.1946
2026-01-08 14:18:00,991: t15.2023.10.22 val PER: 0.1047
2026-01-08 14:18:00,991: t15.2023.11.03 val PER: 0.1784
2026-01-08 14:18:00,991: t15.2023.11.04 val PER: 0.0375
2026-01-08 14:18:00,991: t15.2023.11.17 val PER: 0.0327
2026-01-08 14:18:00,991: t15.2023.11.19 val PER: 0.0319
2026-01-08 14:18:00,991: t15.2023.11.26 val PER: 0.1159
2026-01-08 14:18:00,991: t15.2023.12.03 val PER: 0.1050
2026-01-08 14:18:00,991: t15.2023.12.08 val PER: 0.1065
2026-01-08 14:18:00,991: t15.2023.12.10 val PER: 0.0920
2026-01-08 14:18:00,992: t15.2023.12.17 val PER: 0.1227
2026-01-08 14:18:00,992: t15.2023.12.29 val PER: 0.1235
2026-01-08 14:18:00,992: t15.2024.02.25 val PER: 0.1039
2026-01-08 14:18:00,992: t15.2024.03.08 val PER: 0.2333
2026-01-08 14:18:00,992: t15.2024.03.15 val PER: 0.1932
2026-01-08 14:18:00,992: t15.2024.03.17 val PER: 0.1283
2026-01-08 14:18:00,992: t15.2024.05.10 val PER: 0.1516
2026-01-08 14:18:00,992: t15.2024.06.14 val PER: 0.1577
2026-01-08 14:18:00,992: t15.2024.07.19 val PER: 0.2314
2026-01-08 14:18:00,992: t15.2024.07.21 val PER: 0.0828
2026-01-08 14:18:00,992: t15.2024.07.28 val PER: 0.1257
2026-01-08 14:18:00,993: t15.2025.01.10 val PER: 0.2851
2026-01-08 14:18:00,993: t15.2025.01.12 val PER: 0.1440
2026-01-08 14:18:00,993: t15.2025.03.14 val PER: 0.3462
2026-01-08 14:18:00,993: t15.2025.03.16 val PER: 0.1793
2026-01-08 14:18:00,993: t15.2025.03.30 val PER: 0.2782
2026-01-08 14:18:00,993: t15.2025.04.13 val PER: 0.2140
2026-01-08 14:18:01,019: Best avg val PER achieved: 0.14554
2026-01-08 14:18:01,019: Total training time: 35.16 minutes

=== RUN speckle_p30.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30
2026-01-08 14:18:07,945: Using device: cuda:0
2026-01-08 14:18:09,739: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-08 14:18:09,766: Using 45 sessions after filtering (from 45).
2026-01-08 14:18:10,201: Using torch.compile (if available)
2026-01-08 14:18:10,201: torch.compile not available (torch<2.0). Skipping.
2026-01-08 14:18:10,201: Initialized RNN decoding model
2026-01-08 14:18:10,201: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-08 14:18:10,202: Model has 44,907,305 parameters
2026-01-08 14:18:10,202: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-08 14:18:11,511: Successfully initialized datasets
2026-01-08 14:18:11,511: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-08 14:18:12,565: Train batch 0: loss: 572.52 grad norm: 1495.05 time: 0.186
2026-01-08 14:18:12,565: Running test after training batch: 0
2026-01-08 14:18:12,681: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:18:17,994: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-08 14:18:18,729: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-08 14:18:53,478: Val batch 0: PER (avg): 1.4296 CTC Loss (avg): 633.1590 WER(1gram): 100.00% (n=64) time: 40.912
2026-01-08 14:18:53,478: WER lens: avg_true_words=6.16 avg_pred_words=1.78 max_pred_words=4
2026-01-08 14:18:53,479: t15.2023.08.13 val PER: 1.3025
2026-01-08 14:18:53,479: t15.2023.08.18 val PER: 1.4258
2026-01-08 14:18:53,479: t15.2023.08.20 val PER: 1.3066
2026-01-08 14:18:53,479: t15.2023.08.25 val PER: 1.3328
2026-01-08 14:18:53,479: t15.2023.08.27 val PER: 1.2428
2026-01-08 14:18:53,479: t15.2023.09.01 val PER: 1.4529
2026-01-08 14:18:53,479: t15.2023.09.03 val PER: 1.3171
2026-01-08 14:18:53,479: t15.2023.09.24 val PER: 1.5449
2026-01-08 14:18:53,479: t15.2023.09.29 val PER: 1.4678
2026-01-08 14:18:53,479: t15.2023.10.01 val PER: 1.2140
2026-01-08 14:18:53,480: t15.2023.10.06 val PER: 1.4909
2026-01-08 14:18:53,480: t15.2023.10.08 val PER: 1.1867
2026-01-08 14:18:53,480: t15.2023.10.13 val PER: 1.3964
2026-01-08 14:18:53,480: t15.2023.10.15 val PER: 1.3902
2026-01-08 14:18:53,480: t15.2023.10.20 val PER: 1.5067
2026-01-08 14:18:53,480: t15.2023.10.22 val PER: 1.3953
2026-01-08 14:18:53,480: t15.2023.11.03 val PER: 1.5943
2026-01-08 14:18:53,480: t15.2023.11.04 val PER: 2.0410
2026-01-08 14:18:53,480: t15.2023.11.17 val PER: 1.9456
2026-01-08 14:18:53,480: t15.2023.11.19 val PER: 1.6766
2026-01-08 14:18:53,480: t15.2023.11.26 val PER: 1.5384
2026-01-08 14:18:53,480: t15.2023.12.03 val PER: 1.4254
2026-01-08 14:18:53,480: t15.2023.12.08 val PER: 1.4521
2026-01-08 14:18:53,480: t15.2023.12.10 val PER: 1.6978
2026-01-08 14:18:53,480: t15.2023.12.17 val PER: 1.3035
2026-01-08 14:18:53,481: t15.2023.12.29 val PER: 1.4070
2026-01-08 14:18:53,481: t15.2024.02.25 val PER: 1.4242
2026-01-08 14:18:53,481: t15.2024.03.08 val PER: 1.3243
2026-01-08 14:18:53,481: t15.2024.03.15 val PER: 1.3183
2026-01-08 14:18:53,481: t15.2024.03.17 val PER: 1.4031
2026-01-08 14:18:53,481: t15.2024.05.10 val PER: 1.3165
2026-01-08 14:18:53,481: t15.2024.06.14 val PER: 1.5284
2026-01-08 14:18:53,481: t15.2024.07.19 val PER: 1.0824
2026-01-08 14:18:53,481: t15.2024.07.21 val PER: 1.6290
2026-01-08 14:18:53,481: t15.2024.07.28 val PER: 1.6581
2026-01-08 14:18:53,481: t15.2025.01.10 val PER: 1.0895
2026-01-08 14:18:53,481: t15.2025.01.12 val PER: 1.7606
2026-01-08 14:18:53,481: t15.2025.03.14 val PER: 1.0370
2026-01-08 14:18:53,481: t15.2025.03.16 val PER: 1.6204
2026-01-08 14:18:53,482: t15.2025.03.30 val PER: 1.2908
2026-01-08 14:18:53,482: t15.2025.04.13 val PER: 1.5934
2026-01-08 14:18:53,483: New best val WER(1gram) inf% --> 100.00%
2026-01-08 14:18:53,483: Checkpointing model
2026-01-08 14:18:53,621: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:19:10,746: Train batch 200: loss: 82.28 grad norm: 85.74 time: 0.055
2026-01-08 14:19:27,379: Train batch 400: loss: 59.71 grad norm: 107.88 time: 0.063
2026-01-08 14:19:35,719: Running test after training batch: 500
2026-01-08 14:19:35,818: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:19:41,009: WER debug example
  GT : you can see the code at this point as well
  PR : yule ennes ease ooohs uhde at this ide is aisle
2026-01-08 14:19:41,052: WER debug example
  GT : how does it keep the cost down
  PR : ide does it yule thus as ooohs
2026-01-08 14:19:44,063: Val batch 500: PER (avg): 0.5517 CTC Loss (avg): 60.5917 WER(1gram): 91.37% (n=64) time: 8.344
2026-01-08 14:19:44,063: WER lens: avg_true_words=6.16 avg_pred_words=6.02 max_pred_words=12
2026-01-08 14:19:44,063: t15.2023.08.13 val PER: 0.4802
2026-01-08 14:19:44,063: t15.2023.08.18 val PER: 0.4920
2026-01-08 14:19:44,064: t15.2023.08.20 val PER: 0.4861
2026-01-08 14:19:44,064: t15.2023.08.25 val PER: 0.4533
2026-01-08 14:19:44,064: t15.2023.08.27 val PER: 0.5514
2026-01-08 14:19:44,064: t15.2023.09.01 val PER: 0.4562
2026-01-08 14:19:44,064: t15.2023.09.03 val PER: 0.5178
2026-01-08 14:19:44,064: t15.2023.09.24 val PER: 0.4648
2026-01-08 14:19:44,064: t15.2023.09.29 val PER: 0.4927
2026-01-08 14:19:44,064: t15.2023.10.01 val PER: 0.5376
2026-01-08 14:19:44,064: t15.2023.10.06 val PER: 0.4682
2026-01-08 14:19:44,064: t15.2023.10.08 val PER: 0.5548
2026-01-08 14:19:44,064: t15.2023.10.13 val PER: 0.5818
2026-01-08 14:19:44,064: t15.2023.10.15 val PER: 0.5274
2026-01-08 14:19:44,065: t15.2023.10.20 val PER: 0.4866
2026-01-08 14:19:44,065: t15.2023.10.22 val PER: 0.4788
2026-01-08 14:19:44,065: t15.2023.11.03 val PER: 0.5434
2026-01-08 14:19:44,065: t15.2023.11.04 val PER: 0.3106
2026-01-08 14:19:44,065: t15.2023.11.17 val PER: 0.4044
2026-01-08 14:19:44,065: t15.2023.11.19 val PER: 0.3892
2026-01-08 14:19:44,065: t15.2023.11.26 val PER: 0.5819
2026-01-08 14:19:44,065: t15.2023.12.03 val PER: 0.5452
2026-01-08 14:19:44,065: t15.2023.12.08 val PER: 0.5513
2026-01-08 14:19:44,065: t15.2023.12.10 val PER: 0.4967
2026-01-08 14:19:44,065: t15.2023.12.17 val PER: 0.6216
2026-01-08 14:19:44,066: t15.2023.12.29 val PER: 0.5930
2026-01-08 14:19:44,066: t15.2024.02.25 val PER: 0.5084
2026-01-08 14:19:44,066: t15.2024.03.08 val PER: 0.6671
2026-01-08 14:19:44,066: t15.2024.03.15 val PER: 0.6035
2026-01-08 14:19:44,066: t15.2024.03.17 val PER: 0.5453
2026-01-08 14:19:44,066: t15.2024.05.10 val PER: 0.5795
2026-01-08 14:19:44,066: t15.2024.06.14 val PER: 0.5457
2026-01-08 14:19:44,066: t15.2024.07.19 val PER: 0.7165
2026-01-08 14:19:44,066: t15.2024.07.21 val PER: 0.5110
2026-01-08 14:19:44,066: t15.2024.07.28 val PER: 0.5610
2026-01-08 14:19:44,066: t15.2025.01.10 val PER: 0.7438
2026-01-08 14:19:44,066: t15.2025.01.12 val PER: 0.5905
2026-01-08 14:19:44,066: t15.2025.03.14 val PER: 0.7322
2026-01-08 14:19:44,067: t15.2025.03.16 val PER: 0.6283
2026-01-08 14:19:44,067: t15.2025.03.30 val PER: 0.7402
2026-01-08 14:19:44,067: t15.2025.04.13 val PER: 0.6106
2026-01-08 14:19:44,068: New best val WER(1gram) 100.00% --> 91.37%
2026-01-08 14:19:44,068: Checkpointing model
2026-01-08 14:19:44,206: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:19:52,938: Train batch 600: loss: 52.40 grad norm: 91.00 time: 0.079
2026-01-08 14:20:10,048: Train batch 800: loss: 44.73 grad norm: 87.96 time: 0.058
2026-01-08 14:20:27,710: Train batch 1000: loss: 45.56 grad norm: 77.73 time: 0.067
2026-01-08 14:20:27,711: Running test after training batch: 1000
2026-01-08 14:20:27,831: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:20:32,625: WER debug example
  GT : you can see the code at this point as well
  PR : wooed ent ease thus hood at this ide is while
2026-01-08 14:20:32,660: WER debug example
  GT : how does it keep the cost down
  PR : houde buzze it eke that wass towed
2026-01-08 14:20:34,769: Val batch 1000: PER (avg): 0.4264 CTC Loss (avg): 45.4145 WER(1gram): 83.25% (n=64) time: 7.058
2026-01-08 14:20:34,770: WER lens: avg_true_words=6.16 avg_pred_words=5.72 max_pred_words=12
2026-01-08 14:20:34,770: t15.2023.08.13 val PER: 0.3857
2026-01-08 14:20:34,770: t15.2023.08.18 val PER: 0.3722
2026-01-08 14:20:34,770: t15.2023.08.20 val PER: 0.3654
2026-01-08 14:20:34,770: t15.2023.08.25 val PER: 0.3102
2026-01-08 14:20:34,770: t15.2023.08.27 val PER: 0.4244
2026-01-08 14:20:34,770: t15.2023.09.01 val PER: 0.3222
2026-01-08 14:20:34,771: t15.2023.09.03 val PER: 0.4133
2026-01-08 14:20:34,771: t15.2023.09.24 val PER: 0.3350
2026-01-08 14:20:34,771: t15.2023.09.29 val PER: 0.3867
2026-01-08 14:20:34,771: t15.2023.10.01 val PER: 0.4227
2026-01-08 14:20:34,771: t15.2023.10.06 val PER: 0.3391
2026-01-08 14:20:34,771: t15.2023.10.08 val PER: 0.4614
2026-01-08 14:20:34,771: t15.2023.10.13 val PER: 0.4748
2026-01-08 14:20:34,771: t15.2023.10.15 val PER: 0.4028
2026-01-08 14:20:34,771: t15.2023.10.20 val PER: 0.3893
2026-01-08 14:20:34,772: t15.2023.10.22 val PER: 0.3842
2026-01-08 14:20:34,772: t15.2023.11.03 val PER: 0.4111
2026-01-08 14:20:34,772: t15.2023.11.04 val PER: 0.1911
2026-01-08 14:20:34,772: t15.2023.11.17 val PER: 0.2830
2026-01-08 14:20:34,772: t15.2023.11.19 val PER: 0.2415
2026-01-08 14:20:34,772: t15.2023.11.26 val PER: 0.4587
2026-01-08 14:20:34,772: t15.2023.12.03 val PER: 0.4370
2026-01-08 14:20:34,772: t15.2023.12.08 val PER: 0.4194
2026-01-08 14:20:34,772: t15.2023.12.10 val PER: 0.3719
2026-01-08 14:20:34,772: t15.2023.12.17 val PER: 0.4283
2026-01-08 14:20:34,772: t15.2023.12.29 val PER: 0.4200
2026-01-08 14:20:34,772: t15.2024.02.25 val PER: 0.3820
2026-01-08 14:20:34,772: t15.2024.03.08 val PER: 0.5164
2026-01-08 14:20:34,772: t15.2024.03.15 val PER: 0.4709
2026-01-08 14:20:34,772: t15.2024.03.17 val PER: 0.4275
2026-01-08 14:20:34,773: t15.2024.05.10 val PER: 0.4413
2026-01-08 14:20:34,773: t15.2024.06.14 val PER: 0.4164
2026-01-08 14:20:34,773: t15.2024.07.19 val PER: 0.5498
2026-01-08 14:20:34,773: t15.2024.07.21 val PER: 0.4014
2026-01-08 14:20:34,773: t15.2024.07.28 val PER: 0.4390
2026-01-08 14:20:34,773: t15.2025.01.10 val PER: 0.6322
2026-01-08 14:20:34,773: t15.2025.01.12 val PER: 0.4588
2026-01-08 14:20:34,773: t15.2025.03.14 val PER: 0.6272
2026-01-08 14:20:34,773: t15.2025.03.16 val PER: 0.4921
2026-01-08 14:20:34,773: t15.2025.03.30 val PER: 0.6586
2026-01-08 14:20:34,774: t15.2025.04.13 val PER: 0.4879
2026-01-08 14:20:34,775: New best val WER(1gram) 91.37% --> 83.25%
2026-01-08 14:20:34,775: Checkpointing model
2026-01-08 14:20:35,051: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:20:51,743: Train batch 1200: loss: 36.01 grad norm: 76.40 time: 0.068
2026-01-08 14:21:08,560: Train batch 1400: loss: 38.42 grad norm: 78.64 time: 0.061
2026-01-08 14:21:16,956: Running test after training batch: 1500
2026-01-08 14:21:17,100: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:21:22,025: WER debug example
  GT : you can see the code at this point as well
  PR : yule kint e thus good at this boyde is will
2026-01-08 14:21:22,058: WER debug example
  GT : how does it keep the cost down
  PR : howled does it eke that wass
2026-01-08 14:21:23,701: Val batch 1500: PER (avg): 0.3985 CTC Loss (avg): 39.3685 WER(1gram): 78.43% (n=64) time: 6.745
2026-01-08 14:21:23,701: WER lens: avg_true_words=6.16 avg_pred_words=5.03 max_pred_words=11
2026-01-08 14:21:23,702: t15.2023.08.13 val PER: 0.3534
2026-01-08 14:21:23,702: t15.2023.08.18 val PER: 0.3412
2026-01-08 14:21:23,702: t15.2023.08.20 val PER: 0.3264
2026-01-08 14:21:23,702: t15.2023.08.25 val PER: 0.2861
2026-01-08 14:21:23,702: t15.2023.08.27 val PER: 0.4051
2026-01-08 14:21:23,702: t15.2023.09.01 val PER: 0.3028
2026-01-08 14:21:23,702: t15.2023.09.03 val PER: 0.3812
2026-01-08 14:21:23,702: t15.2023.09.24 val PER: 0.3313
2026-01-08 14:21:23,702: t15.2023.09.29 val PER: 0.3618
2026-01-08 14:21:23,702: t15.2023.10.01 val PER: 0.4102
2026-01-08 14:21:23,702: t15.2023.10.06 val PER: 0.3079
2026-01-08 14:21:23,703: t15.2023.10.08 val PER: 0.4438
2026-01-08 14:21:23,703: t15.2023.10.13 val PER: 0.4500
2026-01-08 14:21:23,703: t15.2023.10.15 val PER: 0.3771
2026-01-08 14:21:23,703: t15.2023.10.20 val PER: 0.3456
2026-01-08 14:21:23,703: t15.2023.10.22 val PER: 0.3408
2026-01-08 14:21:23,703: t15.2023.11.03 val PER: 0.3853
2026-01-08 14:21:23,703: t15.2023.11.04 val PER: 0.1365
2026-01-08 14:21:23,703: t15.2023.11.17 val PER: 0.2364
2026-01-08 14:21:23,703: t15.2023.11.19 val PER: 0.1916
2026-01-08 14:21:23,703: t15.2023.11.26 val PER: 0.4500
2026-01-08 14:21:23,703: t15.2023.12.03 val PER: 0.4086
2026-01-08 14:21:23,703: t15.2023.12.08 val PER: 0.3862
2026-01-08 14:21:23,703: t15.2023.12.10 val PER: 0.3141
2026-01-08 14:21:23,703: t15.2023.12.17 val PER: 0.3867
2026-01-08 14:21:23,703: t15.2023.12.29 val PER: 0.3844
2026-01-08 14:21:23,703: t15.2024.02.25 val PER: 0.3146
2026-01-08 14:21:23,704: t15.2024.03.08 val PER: 0.4737
2026-01-08 14:21:23,704: t15.2024.03.15 val PER: 0.4334
2026-01-08 14:21:23,704: t15.2024.03.17 val PER: 0.3996
2026-01-08 14:21:23,704: t15.2024.05.10 val PER: 0.4042
2026-01-08 14:21:23,704: t15.2024.06.14 val PER: 0.4006
2026-01-08 14:21:23,704: t15.2024.07.19 val PER: 0.5386
2026-01-08 14:21:23,704: t15.2024.07.21 val PER: 0.3669
2026-01-08 14:21:23,704: t15.2024.07.28 val PER: 0.3956
2026-01-08 14:21:23,704: t15.2025.01.10 val PER: 0.6226
2026-01-08 14:21:23,704: t15.2025.01.12 val PER: 0.4419
2026-01-08 14:21:23,704: t15.2025.03.14 val PER: 0.6080
2026-01-08 14:21:23,704: t15.2025.03.16 val PER: 0.4660
2026-01-08 14:21:23,704: t15.2025.03.30 val PER: 0.6529
2026-01-08 14:21:23,704: t15.2025.04.13 val PER: 0.4979
2026-01-08 14:21:23,705: New best val WER(1gram) 83.25% --> 78.43%
2026-01-08 14:21:23,705: Checkpointing model
2026-01-08 14:21:23,865: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:21:32,618: Train batch 1600: loss: 39.51 grad norm: 81.06 time: 0.064
2026-01-08 14:21:49,933: Train batch 1800: loss: 36.25 grad norm: 71.44 time: 0.088
2026-01-08 14:22:06,900: Train batch 2000: loss: 35.81 grad norm: 67.14 time: 0.066
2026-01-08 14:22:06,900: Running test after training batch: 2000
2026-01-08 14:22:07,023: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:22:11,839: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this boyde is will
2026-01-08 14:22:11,870: WER debug example
  GT : how does it keep the cost down
  PR : houde des it eke the cost id
2026-01-08 14:22:13,504: Val batch 2000: PER (avg): 0.3405 CTC Loss (avg): 34.2084 WER(1gram): 70.56% (n=64) time: 6.604
2026-01-08 14:22:13,505: WER lens: avg_true_words=6.16 avg_pred_words=5.45 max_pred_words=11
2026-01-08 14:22:13,505: t15.2023.08.13 val PER: 0.3181
2026-01-08 14:22:13,505: t15.2023.08.18 val PER: 0.2749
2026-01-08 14:22:13,505: t15.2023.08.20 val PER: 0.2693
2026-01-08 14:22:13,505: t15.2023.08.25 val PER: 0.2425
2026-01-08 14:22:13,505: t15.2023.08.27 val PER: 0.3424
2026-01-08 14:22:13,505: t15.2023.09.01 val PER: 0.2451
2026-01-08 14:22:13,505: t15.2023.09.03 val PER: 0.3325
2026-01-08 14:22:13,505: t15.2023.09.24 val PER: 0.2536
2026-01-08 14:22:13,505: t15.2023.09.29 val PER: 0.2770
2026-01-08 14:22:13,505: t15.2023.10.01 val PER: 0.3487
2026-01-08 14:22:13,506: t15.2023.10.06 val PER: 0.2519
2026-01-08 14:22:13,506: t15.2023.10.08 val PER: 0.4073
2026-01-08 14:22:13,506: t15.2023.10.13 val PER: 0.4011
2026-01-08 14:22:13,506: t15.2023.10.15 val PER: 0.3098
2026-01-08 14:22:13,506: t15.2023.10.20 val PER: 0.3087
2026-01-08 14:22:13,506: t15.2023.10.22 val PER: 0.2784
2026-01-08 14:22:13,506: t15.2023.11.03 val PER: 0.3318
2026-01-08 14:22:13,506: t15.2023.11.04 val PER: 0.1024
2026-01-08 14:22:13,506: t15.2023.11.17 val PER: 0.1757
2026-01-08 14:22:13,506: t15.2023.11.19 val PER: 0.1317
2026-01-08 14:22:13,506: t15.2023.11.26 val PER: 0.3942
2026-01-08 14:22:13,506: t15.2023.12.03 val PER: 0.3298
2026-01-08 14:22:13,506: t15.2023.12.08 val PER: 0.3196
2026-01-08 14:22:13,506: t15.2023.12.10 val PER: 0.2681
2026-01-08 14:22:13,507: t15.2023.12.17 val PER: 0.3326
2026-01-08 14:22:13,507: t15.2023.12.29 val PER: 0.3308
2026-01-08 14:22:13,507: t15.2024.02.25 val PER: 0.2921
2026-01-08 14:22:13,507: t15.2024.03.08 val PER: 0.4125
2026-01-08 14:22:13,507: t15.2024.03.15 val PER: 0.3727
2026-01-08 14:22:13,507: t15.2024.03.17 val PER: 0.3536
2026-01-08 14:22:13,507: t15.2024.05.10 val PER: 0.3551
2026-01-08 14:22:13,507: t15.2024.06.14 val PER: 0.3580
2026-01-08 14:22:13,507: t15.2024.07.19 val PER: 0.4871
2026-01-08 14:22:13,507: t15.2024.07.21 val PER: 0.3117
2026-01-08 14:22:13,507: t15.2024.07.28 val PER: 0.3375
2026-01-08 14:22:13,507: t15.2025.01.10 val PER: 0.5510
2026-01-08 14:22:13,507: t15.2025.01.12 val PER: 0.3903
2026-01-08 14:22:13,507: t15.2025.03.14 val PER: 0.5459
2026-01-08 14:22:13,507: t15.2025.03.16 val PER: 0.3992
2026-01-08 14:22:13,507: t15.2025.03.30 val PER: 0.5747
2026-01-08 14:22:13,508: t15.2025.04.13 val PER: 0.4108
2026-01-08 14:22:13,509: New best val WER(1gram) 78.43% --> 70.56%
2026-01-08 14:22:13,509: Checkpointing model
2026-01-08 14:22:13,647: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:22:30,487: Train batch 2200: loss: 30.16 grad norm: 72.28 time: 0.061
2026-01-08 14:22:48,613: Train batch 2400: loss: 30.31 grad norm: 60.55 time: 0.053
2026-01-08 14:22:57,763: Running test after training batch: 2500
2026-01-08 14:22:57,905: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:23:02,696: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is wheel
2026-01-08 14:23:02,726: WER debug example
  GT : how does it keep the cost down
  PR : houde des it eke the cost it
2026-01-08 14:23:04,466: Val batch 2500: PER (avg): 0.3169 CTC Loss (avg): 31.8462 WER(1gram): 69.29% (n=64) time: 6.703
2026-01-08 14:23:04,467: WER lens: avg_true_words=6.16 avg_pred_words=5.69 max_pred_words=11
2026-01-08 14:23:04,467: t15.2023.08.13 val PER: 0.2848
2026-01-08 14:23:04,467: t15.2023.08.18 val PER: 0.2573
2026-01-08 14:23:04,467: t15.2023.08.20 val PER: 0.2566
2026-01-08 14:23:04,467: t15.2023.08.25 val PER: 0.2304
2026-01-08 14:23:04,467: t15.2023.08.27 val PER: 0.3360
2026-01-08 14:23:04,467: t15.2023.09.01 val PER: 0.2273
2026-01-08 14:23:04,467: t15.2023.09.03 val PER: 0.3005
2026-01-08 14:23:04,467: t15.2023.09.24 val PER: 0.2427
2026-01-08 14:23:04,467: t15.2023.09.29 val PER: 0.2674
2026-01-08 14:23:04,467: t15.2023.10.01 val PER: 0.3263
2026-01-08 14:23:04,467: t15.2023.10.06 val PER: 0.2282
2026-01-08 14:23:04,468: t15.2023.10.08 val PER: 0.3829
2026-01-08 14:23:04,468: t15.2023.10.13 val PER: 0.3716
2026-01-08 14:23:04,468: t15.2023.10.15 val PER: 0.2986
2026-01-08 14:23:04,468: t15.2023.10.20 val PER: 0.3020
2026-01-08 14:23:04,468: t15.2023.10.22 val PER: 0.2416
2026-01-08 14:23:04,468: t15.2023.11.03 val PER: 0.3100
2026-01-08 14:23:04,468: t15.2023.11.04 val PER: 0.1024
2026-01-08 14:23:04,468: t15.2023.11.17 val PER: 0.1633
2026-01-08 14:23:04,468: t15.2023.11.19 val PER: 0.1198
2026-01-08 14:23:04,468: t15.2023.11.26 val PER: 0.3645
2026-01-08 14:23:04,469: t15.2023.12.03 val PER: 0.3172
2026-01-08 14:23:04,469: t15.2023.12.08 val PER: 0.3029
2026-01-08 14:23:04,469: t15.2023.12.10 val PER: 0.2497
2026-01-08 14:23:04,469: t15.2023.12.17 val PER: 0.3150
2026-01-08 14:23:04,469: t15.2023.12.29 val PER: 0.3260
2026-01-08 14:23:04,469: t15.2024.02.25 val PER: 0.2514
2026-01-08 14:23:04,469: t15.2024.03.08 val PER: 0.3784
2026-01-08 14:23:04,469: t15.2024.03.15 val PER: 0.3471
2026-01-08 14:23:04,469: t15.2024.03.17 val PER: 0.3152
2026-01-08 14:23:04,469: t15.2024.05.10 val PER: 0.3254
2026-01-08 14:23:04,469: t15.2024.06.14 val PER: 0.3076
2026-01-08 14:23:04,469: t15.2024.07.19 val PER: 0.4522
2026-01-08 14:23:04,469: t15.2024.07.21 val PER: 0.2676
2026-01-08 14:23:04,469: t15.2024.07.28 val PER: 0.3103
2026-01-08 14:23:04,469: t15.2025.01.10 val PER: 0.5096
2026-01-08 14:23:04,469: t15.2025.01.12 val PER: 0.3680
2026-01-08 14:23:04,470: t15.2025.03.14 val PER: 0.5325
2026-01-08 14:23:04,470: t15.2025.03.16 val PER: 0.3717
2026-01-08 14:23:04,470: t15.2025.03.30 val PER: 0.5391
2026-01-08 14:23:04,470: t15.2025.04.13 val PER: 0.3837
2026-01-08 14:23:04,471: New best val WER(1gram) 70.56% --> 69.29%
2026-01-08 14:23:04,471: Checkpointing model
2026-01-08 14:23:04,608: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:23:13,511: Train batch 2600: loss: 37.59 grad norm: 81.76 time: 0.055
2026-01-08 14:23:31,671: Train batch 2800: loss: 27.00 grad norm: 66.30 time: 0.083
2026-01-08 14:23:49,895: Train batch 3000: loss: 32.46 grad norm: 77.00 time: 0.084
2026-01-08 14:23:49,895: Running test after training batch: 3000
2026-01-08 14:23:49,994: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:23:54,797: WER debug example
  GT : you can see the code at this point as well
  PR : yu end sci the owed at this point is will
2026-01-08 14:23:54,827: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap the rost it
2026-01-08 14:23:56,555: Val batch 3000: PER (avg): 0.2934 CTC Loss (avg): 29.3462 WER(1gram): 66.75% (n=64) time: 6.659
2026-01-08 14:23:56,555: WER lens: avg_true_words=6.16 avg_pred_words=5.88 max_pred_words=11
2026-01-08 14:23:56,555: t15.2023.08.13 val PER: 0.2651
2026-01-08 14:23:56,556: t15.2023.08.18 val PER: 0.2406
2026-01-08 14:23:56,556: t15.2023.08.20 val PER: 0.2367
2026-01-08 14:23:56,556: t15.2023.08.25 val PER: 0.2063
2026-01-08 14:23:56,556: t15.2023.08.27 val PER: 0.3103
2026-01-08 14:23:56,556: t15.2023.09.01 val PER: 0.1997
2026-01-08 14:23:56,556: t15.2023.09.03 val PER: 0.2862
2026-01-08 14:23:56,556: t15.2023.09.24 val PER: 0.2257
2026-01-08 14:23:56,556: t15.2023.09.29 val PER: 0.2387
2026-01-08 14:23:56,556: t15.2023.10.01 val PER: 0.2959
2026-01-08 14:23:56,556: t15.2023.10.06 val PER: 0.2067
2026-01-08 14:23:56,556: t15.2023.10.08 val PER: 0.3654
2026-01-08 14:23:56,556: t15.2023.10.13 val PER: 0.3491
2026-01-08 14:23:56,556: t15.2023.10.15 val PER: 0.2703
2026-01-08 14:23:56,556: t15.2023.10.20 val PER: 0.2785
2026-01-08 14:23:56,557: t15.2023.10.22 val PER: 0.2272
2026-01-08 14:23:56,557: t15.2023.11.03 val PER: 0.2924
2026-01-08 14:23:56,557: t15.2023.11.04 val PER: 0.0990
2026-01-08 14:23:56,557: t15.2023.11.17 val PER: 0.1400
2026-01-08 14:23:56,557: t15.2023.11.19 val PER: 0.1198
2026-01-08 14:23:56,557: t15.2023.11.26 val PER: 0.3145
2026-01-08 14:23:56,557: t15.2023.12.03 val PER: 0.2700
2026-01-08 14:23:56,557: t15.2023.12.08 val PER: 0.2716
2026-01-08 14:23:56,557: t15.2023.12.10 val PER: 0.2142
2026-01-08 14:23:56,557: t15.2023.12.17 val PER: 0.2827
2026-01-08 14:23:56,557: t15.2023.12.29 val PER: 0.3054
2026-01-08 14:23:56,557: t15.2024.02.25 val PER: 0.2612
2026-01-08 14:23:56,557: t15.2024.03.08 val PER: 0.3755
2026-01-08 14:23:56,558: t15.2024.03.15 val PER: 0.3333
2026-01-08 14:23:56,558: t15.2024.03.17 val PER: 0.3033
2026-01-08 14:23:56,558: t15.2024.05.10 val PER: 0.3120
2026-01-08 14:23:56,558: t15.2024.06.14 val PER: 0.2918
2026-01-08 14:23:56,558: t15.2024.07.19 val PER: 0.4173
2026-01-08 14:23:56,558: t15.2024.07.21 val PER: 0.2469
2026-01-08 14:23:56,558: t15.2024.07.28 val PER: 0.2838
2026-01-08 14:23:56,558: t15.2025.01.10 val PER: 0.4904
2026-01-08 14:23:56,558: t15.2025.01.12 val PER: 0.3480
2026-01-08 14:23:56,559: t15.2025.03.14 val PER: 0.4719
2026-01-08 14:23:56,559: t15.2025.03.16 val PER: 0.3194
2026-01-08 14:23:56,559: t15.2025.03.30 val PER: 0.5276
2026-01-08 14:23:56,559: t15.2025.04.13 val PER: 0.3709
2026-01-08 14:23:56,560: New best val WER(1gram) 69.29% --> 66.75%
2026-01-08 14:23:56,560: Checkpointing model
2026-01-08 14:23:56,701: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:24:14,996: Train batch 3200: loss: 28.22 grad norm: 66.51 time: 0.078
2026-01-08 14:24:33,022: Train batch 3400: loss: 20.03 grad norm: 58.57 time: 0.049
2026-01-08 14:24:42,217: Running test after training batch: 3500
2026-01-08 14:24:42,324: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:24:47,086: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point wheel
2026-01-08 14:24:47,114: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it eke the cussed sette
2026-01-08 14:24:48,712: Val batch 3500: PER (avg): 0.2780 CTC Loss (avg): 27.8337 WER(1gram): 68.78% (n=64) time: 6.495
2026-01-08 14:24:48,712: WER lens: avg_true_words=6.16 avg_pred_words=5.81 max_pred_words=11
2026-01-08 14:24:48,712: t15.2023.08.13 val PER: 0.2588
2026-01-08 14:24:48,712: t15.2023.08.18 val PER: 0.2246
2026-01-08 14:24:48,713: t15.2023.08.20 val PER: 0.2200
2026-01-08 14:24:48,713: t15.2023.08.25 val PER: 0.1852
2026-01-08 14:24:48,713: t15.2023.08.27 val PER: 0.2958
2026-01-08 14:24:48,713: t15.2023.09.01 val PER: 0.1891
2026-01-08 14:24:48,713: t15.2023.09.03 val PER: 0.2660
2026-01-08 14:24:48,713: t15.2023.09.24 val PER: 0.2172
2026-01-08 14:24:48,713: t15.2023.09.29 val PER: 0.2265
2026-01-08 14:24:48,713: t15.2023.10.01 val PER: 0.2847
2026-01-08 14:24:48,714: t15.2023.10.06 val PER: 0.2067
2026-01-08 14:24:48,714: t15.2023.10.08 val PER: 0.3451
2026-01-08 14:24:48,714: t15.2023.10.13 val PER: 0.3351
2026-01-08 14:24:48,714: t15.2023.10.15 val PER: 0.2525
2026-01-08 14:24:48,714: t15.2023.10.20 val PER: 0.2517
2026-01-08 14:24:48,714: t15.2023.10.22 val PER: 0.2261
2026-01-08 14:24:48,714: t15.2023.11.03 val PER: 0.2795
2026-01-08 14:24:48,714: t15.2023.11.04 val PER: 0.0819
2026-01-08 14:24:48,714: t15.2023.11.17 val PER: 0.1260
2026-01-08 14:24:48,714: t15.2023.11.19 val PER: 0.1118
2026-01-08 14:24:48,714: t15.2023.11.26 val PER: 0.2957
2026-01-08 14:24:48,714: t15.2023.12.03 val PER: 0.2595
2026-01-08 14:24:48,714: t15.2023.12.08 val PER: 0.2630
2026-01-08 14:24:48,714: t15.2023.12.10 val PER: 0.2194
2026-01-08 14:24:48,714: t15.2023.12.17 val PER: 0.2599
2026-01-08 14:24:48,714: t15.2023.12.29 val PER: 0.2711
2026-01-08 14:24:48,714: t15.2024.02.25 val PER: 0.2317
2026-01-08 14:24:48,715: t15.2024.03.08 val PER: 0.3414
2026-01-08 14:24:48,715: t15.2024.03.15 val PER: 0.3177
2026-01-08 14:24:48,715: t15.2024.03.17 val PER: 0.2880
2026-01-08 14:24:48,715: t15.2024.05.10 val PER: 0.2719
2026-01-08 14:24:48,715: t15.2024.06.14 val PER: 0.2855
2026-01-08 14:24:48,715: t15.2024.07.19 val PER: 0.4113
2026-01-08 14:24:48,715: t15.2024.07.21 val PER: 0.2248
2026-01-08 14:24:48,715: t15.2024.07.28 val PER: 0.2868
2026-01-08 14:24:48,715: t15.2025.01.10 val PER: 0.4697
2026-01-08 14:24:48,715: t15.2025.01.12 val PER: 0.3156
2026-01-08 14:24:48,715: t15.2025.03.14 val PER: 0.4482
2026-01-08 14:24:48,715: t15.2025.03.16 val PER: 0.3377
2026-01-08 14:24:48,715: t15.2025.03.30 val PER: 0.4851
2026-01-08 14:24:48,715: t15.2025.04.13 val PER: 0.3466
2026-01-08 14:24:57,636: Train batch 3600: loss: 23.78 grad norm: 62.38 time: 0.066
2026-01-08 14:25:14,972: Train batch 3800: loss: 26.70 grad norm: 68.46 time: 0.067
2026-01-08 14:25:31,822: Train batch 4000: loss: 21.27 grad norm: 52.93 time: 0.057
2026-01-08 14:25:31,822: Running test after training batch: 4000
2026-01-08 14:25:31,977: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:25:36,765: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point wheel
2026-01-08 14:25:36,794: WER debug example
  GT : how does it keep the cost down
  PR : howl des it keep the cost nett
2026-01-08 14:25:38,457: Val batch 4000: PER (avg): 0.2572 CTC Loss (avg): 25.6165 WER(1gram): 64.97% (n=64) time: 6.635
2026-01-08 14:25:38,458: WER lens: avg_true_words=6.16 avg_pred_words=5.91 max_pred_words=11
2026-01-08 14:25:38,458: t15.2023.08.13 val PER: 0.2256
2026-01-08 14:25:38,458: t15.2023.08.18 val PER: 0.1978
2026-01-08 14:25:38,458: t15.2023.08.20 val PER: 0.2081
2026-01-08 14:25:38,458: t15.2023.08.25 val PER: 0.1611
2026-01-08 14:25:38,458: t15.2023.08.27 val PER: 0.2846
2026-01-08 14:25:38,458: t15.2023.09.01 val PER: 0.1843
2026-01-08 14:25:38,458: t15.2023.09.03 val PER: 0.2565
2026-01-08 14:25:38,458: t15.2023.09.24 val PER: 0.2015
2026-01-08 14:25:38,459: t15.2023.09.29 val PER: 0.1966
2026-01-08 14:25:38,459: t15.2023.10.01 val PER: 0.2576
2026-01-08 14:25:38,459: t15.2023.10.06 val PER: 0.1873
2026-01-08 14:25:38,459: t15.2023.10.08 val PER: 0.3315
2026-01-08 14:25:38,459: t15.2023.10.13 val PER: 0.3150
2026-01-08 14:25:38,459: t15.2023.10.15 val PER: 0.2452
2026-01-08 14:25:38,459: t15.2023.10.20 val PER: 0.2248
2026-01-08 14:25:38,459: t15.2023.10.22 val PER: 0.2160
2026-01-08 14:25:38,459: t15.2023.11.03 val PER: 0.2571
2026-01-08 14:25:38,459: t15.2023.11.04 val PER: 0.0648
2026-01-08 14:25:38,459: t15.2023.11.17 val PER: 0.1120
2026-01-08 14:25:38,459: t15.2023.11.19 val PER: 0.0978
2026-01-08 14:25:38,459: t15.2023.11.26 val PER: 0.2899
2026-01-08 14:25:38,459: t15.2023.12.03 val PER: 0.2269
2026-01-08 14:25:38,460: t15.2023.12.08 val PER: 0.2330
2026-01-08 14:25:38,460: t15.2023.12.10 val PER: 0.1932
2026-01-08 14:25:38,460: t15.2023.12.17 val PER: 0.2443
2026-01-08 14:25:38,460: t15.2023.12.29 val PER: 0.2663
2026-01-08 14:25:38,460: t15.2024.02.25 val PER: 0.2233
2026-01-08 14:25:38,460: t15.2024.03.08 val PER: 0.3371
2026-01-08 14:25:38,460: t15.2024.03.15 val PER: 0.3058
2026-01-08 14:25:38,460: t15.2024.03.17 val PER: 0.2720
2026-01-08 14:25:38,460: t15.2024.05.10 val PER: 0.2734
2026-01-08 14:25:38,460: t15.2024.06.14 val PER: 0.2729
2026-01-08 14:25:38,460: t15.2024.07.19 val PER: 0.3744
2026-01-08 14:25:38,460: t15.2024.07.21 val PER: 0.1986
2026-01-08 14:25:38,460: t15.2024.07.28 val PER: 0.2390
2026-01-08 14:25:38,460: t15.2025.01.10 val PER: 0.4408
2026-01-08 14:25:38,461: t15.2025.01.12 val PER: 0.2879
2026-01-08 14:25:38,461: t15.2025.03.14 val PER: 0.4305
2026-01-08 14:25:38,461: t15.2025.03.16 val PER: 0.3076
2026-01-08 14:25:38,461: t15.2025.03.30 val PER: 0.4195
2026-01-08 14:25:38,461: t15.2025.04.13 val PER: 0.3252
2026-01-08 14:25:38,462: New best val WER(1gram) 66.75% --> 64.97%
2026-01-08 14:25:38,462: Checkpointing model
2026-01-08 14:25:38,604: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:25:55,819: Train batch 4200: loss: 23.48 grad norm: 64.67 time: 0.080
2026-01-08 14:26:13,389: Train batch 4400: loss: 17.81 grad norm: 56.40 time: 0.067
2026-01-08 14:26:21,817: Running test after training batch: 4500
2026-01-08 14:26:21,941: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:26:26,750: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the could at this point is will
2026-01-08 14:26:26,781: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cost get
2026-01-08 14:26:28,417: Val batch 4500: PER (avg): 0.2462 CTC Loss (avg): 24.3356 WER(1gram): 63.71% (n=64) time: 6.600
2026-01-08 14:26:28,418: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-08 14:26:28,418: t15.2023.08.13 val PER: 0.2141
2026-01-08 14:26:28,418: t15.2023.08.18 val PER: 0.1844
2026-01-08 14:26:28,418: t15.2023.08.20 val PER: 0.1938
2026-01-08 14:26:28,418: t15.2023.08.25 val PER: 0.1687
2026-01-08 14:26:28,418: t15.2023.08.27 val PER: 0.2701
2026-01-08 14:26:28,418: t15.2023.09.01 val PER: 0.1656
2026-01-08 14:26:28,418: t15.2023.09.03 val PER: 0.2447
2026-01-08 14:26:28,418: t15.2023.09.24 val PER: 0.1917
2026-01-08 14:26:28,418: t15.2023.09.29 val PER: 0.1997
2026-01-08 14:26:28,418: t15.2023.10.01 val PER: 0.2596
2026-01-08 14:26:28,419: t15.2023.10.06 val PER: 0.1755
2026-01-08 14:26:28,419: t15.2023.10.08 val PER: 0.3234
2026-01-08 14:26:28,419: t15.2023.10.13 val PER: 0.3088
2026-01-08 14:26:28,419: t15.2023.10.15 val PER: 0.2320
2026-01-08 14:26:28,419: t15.2023.10.20 val PER: 0.2315
2026-01-08 14:26:28,419: t15.2023.10.22 val PER: 0.2082
2026-01-08 14:26:28,419: t15.2023.11.03 val PER: 0.2517
2026-01-08 14:26:28,419: t15.2023.11.04 val PER: 0.0648
2026-01-08 14:26:28,419: t15.2023.11.17 val PER: 0.1073
2026-01-08 14:26:28,419: t15.2023.11.19 val PER: 0.0998
2026-01-08 14:26:28,419: t15.2023.11.26 val PER: 0.2754
2026-01-08 14:26:28,420: t15.2023.12.03 val PER: 0.2311
2026-01-08 14:26:28,420: t15.2023.12.08 val PER: 0.2184
2026-01-08 14:26:28,420: t15.2023.12.10 val PER: 0.1919
2026-01-08 14:26:28,420: t15.2023.12.17 val PER: 0.2308
2026-01-08 14:26:28,420: t15.2023.12.29 val PER: 0.2388
2026-01-08 14:26:28,420: t15.2024.02.25 val PER: 0.2051
2026-01-08 14:26:28,420: t15.2024.03.08 val PER: 0.3201
2026-01-08 14:26:28,420: t15.2024.03.15 val PER: 0.2971
2026-01-08 14:26:28,420: t15.2024.03.17 val PER: 0.2497
2026-01-08 14:26:28,420: t15.2024.05.10 val PER: 0.2689
2026-01-08 14:26:28,420: t15.2024.06.14 val PER: 0.2587
2026-01-08 14:26:28,420: t15.2024.07.19 val PER: 0.3546
2026-01-08 14:26:28,421: t15.2024.07.21 val PER: 0.1834
2026-01-08 14:26:28,421: t15.2024.07.28 val PER: 0.2272
2026-01-08 14:26:28,421: t15.2025.01.10 val PER: 0.4174
2026-01-08 14:26:28,421: t15.2025.01.12 val PER: 0.2625
2026-01-08 14:26:28,421: t15.2025.03.14 val PER: 0.4053
2026-01-08 14:26:28,421: t15.2025.03.16 val PER: 0.3037
2026-01-08 14:26:28,421: t15.2025.03.30 val PER: 0.4379
2026-01-08 14:26:28,421: t15.2025.04.13 val PER: 0.3010
2026-01-08 14:26:28,422: New best val WER(1gram) 64.97% --> 63.71%
2026-01-08 14:26:28,422: Checkpointing model
2026-01-08 14:26:28,560: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:26:37,029: Train batch 4600: loss: 22.07 grad norm: 65.13 time: 0.063
2026-01-08 14:26:54,653: Train batch 4800: loss: 15.47 grad norm: 54.06 time: 0.064
2026-01-08 14:27:12,474: Train batch 5000: loss: 34.63 grad norm: 90.10 time: 0.064
2026-01-08 14:27:12,474: Running test after training batch: 5000
2026-01-08 14:27:12,608: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:27:17,425: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-08 14:27:17,455: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cussed get
2026-01-08 14:27:19,118: Val batch 5000: PER (avg): 0.2337 CTC Loss (avg): 22.9776 WER(1gram): 62.18% (n=64) time: 6.643
2026-01-08 14:27:19,118: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-08 14:27:19,118: t15.2023.08.13 val PER: 0.2121
2026-01-08 14:27:19,118: t15.2023.08.18 val PER: 0.1802
2026-01-08 14:27:19,118: t15.2023.08.20 val PER: 0.1755
2026-01-08 14:27:19,119: t15.2023.08.25 val PER: 0.1416
2026-01-08 14:27:19,119: t15.2023.08.27 val PER: 0.2540
2026-01-08 14:27:19,119: t15.2023.09.01 val PER: 0.1356
2026-01-08 14:27:19,119: t15.2023.09.03 val PER: 0.2340
2026-01-08 14:27:19,119: t15.2023.09.24 val PER: 0.1784
2026-01-08 14:27:19,119: t15.2023.09.29 val PER: 0.1902
2026-01-08 14:27:19,119: t15.2023.10.01 val PER: 0.2384
2026-01-08 14:27:19,119: t15.2023.10.06 val PER: 0.1561
2026-01-08 14:27:19,119: t15.2023.10.08 val PER: 0.3194
2026-01-08 14:27:19,119: t15.2023.10.13 val PER: 0.2956
2026-01-08 14:27:19,119: t15.2023.10.15 val PER: 0.2215
2026-01-08 14:27:19,119: t15.2023.10.20 val PER: 0.2416
2026-01-08 14:27:19,119: t15.2023.10.22 val PER: 0.1704
2026-01-08 14:27:19,119: t15.2023.11.03 val PER: 0.2402
2026-01-08 14:27:19,120: t15.2023.11.04 val PER: 0.0614
2026-01-08 14:27:19,120: t15.2023.11.17 val PER: 0.0902
2026-01-08 14:27:19,120: t15.2023.11.19 val PER: 0.0918
2026-01-08 14:27:19,120: t15.2023.11.26 val PER: 0.2529
2026-01-08 14:27:19,120: t15.2023.12.03 val PER: 0.2080
2026-01-08 14:27:19,120: t15.2023.12.08 val PER: 0.2210
2026-01-08 14:27:19,120: t15.2023.12.10 val PER: 0.1708
2026-01-08 14:27:19,120: t15.2023.12.17 val PER: 0.2328
2026-01-08 14:27:19,120: t15.2023.12.29 val PER: 0.2313
2026-01-08 14:27:19,120: t15.2024.02.25 val PER: 0.1770
2026-01-08 14:27:19,120: t15.2024.03.08 val PER: 0.3144
2026-01-08 14:27:19,120: t15.2024.03.15 val PER: 0.2877
2026-01-08 14:27:19,120: t15.2024.03.17 val PER: 0.2280
2026-01-08 14:27:19,120: t15.2024.05.10 val PER: 0.2467
2026-01-08 14:27:19,120: t15.2024.06.14 val PER: 0.2571
2026-01-08 14:27:19,120: t15.2024.07.19 val PER: 0.3494
2026-01-08 14:27:19,120: t15.2024.07.21 val PER: 0.1876
2026-01-08 14:27:19,121: t15.2024.07.28 val PER: 0.2228
2026-01-08 14:27:19,121: t15.2025.01.10 val PER: 0.4022
2026-01-08 14:27:19,121: t15.2025.01.12 val PER: 0.2517
2026-01-08 14:27:19,121: t15.2025.03.14 val PER: 0.4009
2026-01-08 14:27:19,121: t15.2025.03.16 val PER: 0.2801
2026-01-08 14:27:19,121: t15.2025.03.30 val PER: 0.4161
2026-01-08 14:27:19,121: t15.2025.04.13 val PER: 0.2981
2026-01-08 14:27:19,122: New best val WER(1gram) 63.71% --> 62.18%
2026-01-08 14:27:19,122: Checkpointing model
2026-01-08 14:27:19,260: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:27:36,357: Train batch 5200: loss: 18.31 grad norm: 60.43 time: 0.053
2026-01-08 14:27:53,725: Train batch 5400: loss: 18.40 grad norm: 60.42 time: 0.068
2026-01-08 14:28:02,373: Running test after training batch: 5500
2026-01-08 14:28:02,483: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:28:07,429: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point will
2026-01-08 14:28:07,460: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost nett
2026-01-08 14:28:09,111: Val batch 5500: PER (avg): 0.2237 CTC Loss (avg): 21.9380 WER(1gram): 60.66% (n=64) time: 6.738
2026-01-08 14:28:09,112: WER lens: avg_true_words=6.16 avg_pred_words=5.97 max_pred_words=11
2026-01-08 14:28:09,112: t15.2023.08.13 val PER: 0.1913
2026-01-08 14:28:09,112: t15.2023.08.18 val PER: 0.1685
2026-01-08 14:28:09,112: t15.2023.08.20 val PER: 0.1763
2026-01-08 14:28:09,112: t15.2023.08.25 val PER: 0.1295
2026-01-08 14:28:09,112: t15.2023.08.27 val PER: 0.2588
2026-01-08 14:28:09,112: t15.2023.09.01 val PER: 0.1331
2026-01-08 14:28:09,112: t15.2023.09.03 val PER: 0.2114
2026-01-08 14:28:09,112: t15.2023.09.24 val PER: 0.1808
2026-01-08 14:28:09,113: t15.2023.09.29 val PER: 0.1812
2026-01-08 14:28:09,113: t15.2023.10.01 val PER: 0.2345
2026-01-08 14:28:09,113: t15.2023.10.06 val PER: 0.1378
2026-01-08 14:28:09,113: t15.2023.10.08 val PER: 0.3045
2026-01-08 14:28:09,113: t15.2023.10.13 val PER: 0.2878
2026-01-08 14:28:09,113: t15.2023.10.15 val PER: 0.2268
2026-01-08 14:28:09,113: t15.2023.10.20 val PER: 0.2383
2026-01-08 14:28:09,113: t15.2023.10.22 val PER: 0.1771
2026-01-08 14:28:09,113: t15.2023.11.03 val PER: 0.2408
2026-01-08 14:28:09,113: t15.2023.11.04 val PER: 0.0614
2026-01-08 14:28:09,113: t15.2023.11.17 val PER: 0.0902
2026-01-08 14:28:09,113: t15.2023.11.19 val PER: 0.0818
2026-01-08 14:28:09,113: t15.2023.11.26 val PER: 0.2399
2026-01-08 14:28:09,113: t15.2023.12.03 val PER: 0.1901
2026-01-08 14:28:09,113: t15.2023.12.08 val PER: 0.1971
2026-01-08 14:28:09,113: t15.2023.12.10 val PER: 0.1708
2026-01-08 14:28:09,114: t15.2023.12.17 val PER: 0.2256
2026-01-08 14:28:09,114: t15.2023.12.29 val PER: 0.2237
2026-01-08 14:28:09,114: t15.2024.02.25 val PER: 0.1826
2026-01-08 14:28:09,114: t15.2024.03.08 val PER: 0.3001
2026-01-08 14:28:09,114: t15.2024.03.15 val PER: 0.2670
2026-01-08 14:28:09,114: t15.2024.03.17 val PER: 0.2204
2026-01-08 14:28:09,114: t15.2024.05.10 val PER: 0.2348
2026-01-08 14:28:09,114: t15.2024.06.14 val PER: 0.2334
2026-01-08 14:28:09,114: t15.2024.07.19 val PER: 0.3316
2026-01-08 14:28:09,114: t15.2024.07.21 val PER: 0.1683
2026-01-08 14:28:09,114: t15.2024.07.28 val PER: 0.2154
2026-01-08 14:28:09,114: t15.2025.01.10 val PER: 0.4022
2026-01-08 14:28:09,115: t15.2025.01.12 val PER: 0.2340
2026-01-08 14:28:09,115: t15.2025.03.14 val PER: 0.3772
2026-01-08 14:28:09,115: t15.2025.03.16 val PER: 0.2736
2026-01-08 14:28:09,115: t15.2025.03.30 val PER: 0.3724
2026-01-08 14:28:09,115: t15.2025.04.13 val PER: 0.3010
2026-01-08 14:28:09,116: New best val WER(1gram) 62.18% --> 60.66%
2026-01-08 14:28:09,117: Checkpointing model
2026-01-08 14:28:09,261: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:28:17,944: Train batch 5600: loss: 21.30 grad norm: 66.72 time: 0.062
2026-01-08 14:28:35,849: Train batch 5800: loss: 14.68 grad norm: 56.29 time: 0.082
2026-01-08 14:28:53,349: Train batch 6000: loss: 15.56 grad norm: 54.03 time: 0.049
2026-01-08 14:28:53,349: Running test after training batch: 6000
2026-01-08 14:28:53,452: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:28:58,239: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the could at this point is will
2026-01-08 14:28:58,269: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-08 14:28:59,931: Val batch 6000: PER (avg): 0.2182 CTC Loss (avg): 21.4682 WER(1gram): 59.14% (n=64) time: 6.582
2026-01-08 14:28:59,932: WER lens: avg_true_words=6.16 avg_pred_words=6.02 max_pred_words=11
2026-01-08 14:28:59,932: t15.2023.08.13 val PER: 0.1830
2026-01-08 14:28:59,932: t15.2023.08.18 val PER: 0.1735
2026-01-08 14:28:59,932: t15.2023.08.20 val PER: 0.1795
2026-01-08 14:28:59,932: t15.2023.08.25 val PER: 0.1325
2026-01-08 14:28:59,932: t15.2023.08.27 val PER: 0.2621
2026-01-08 14:28:59,932: t15.2023.09.01 val PER: 0.1299
2026-01-08 14:28:59,932: t15.2023.09.03 val PER: 0.2280
2026-01-08 14:28:59,932: t15.2023.09.24 val PER: 0.1784
2026-01-08 14:28:59,932: t15.2023.09.29 val PER: 0.1787
2026-01-08 14:28:59,932: t15.2023.10.01 val PER: 0.2292
2026-01-08 14:28:59,932: t15.2023.10.06 val PER: 0.1485
2026-01-08 14:28:59,932: t15.2023.10.08 val PER: 0.2923
2026-01-08 14:28:59,932: t15.2023.10.13 val PER: 0.2723
2026-01-08 14:28:59,933: t15.2023.10.15 val PER: 0.2195
2026-01-08 14:28:59,933: t15.2023.10.20 val PER: 0.2215
2026-01-08 14:28:59,933: t15.2023.10.22 val PER: 0.1737
2026-01-08 14:28:59,933: t15.2023.11.03 val PER: 0.2334
2026-01-08 14:28:59,933: t15.2023.11.04 val PER: 0.0444
2026-01-08 14:28:59,933: t15.2023.11.17 val PER: 0.0762
2026-01-08 14:28:59,933: t15.2023.11.19 val PER: 0.0758
2026-01-08 14:28:59,933: t15.2023.11.26 val PER: 0.2319
2026-01-08 14:28:59,933: t15.2023.12.03 val PER: 0.1775
2026-01-08 14:28:59,933: t15.2023.12.08 val PER: 0.1818
2026-01-08 14:28:59,933: t15.2023.12.10 val PER: 0.1629
2026-01-08 14:28:59,933: t15.2023.12.17 val PER: 0.1975
2026-01-08 14:28:59,933: t15.2023.12.29 val PER: 0.2155
2026-01-08 14:28:59,933: t15.2024.02.25 val PER: 0.1615
2026-01-08 14:28:59,933: t15.2024.03.08 val PER: 0.2873
2026-01-08 14:28:59,934: t15.2024.03.15 val PER: 0.2720
2026-01-08 14:28:59,934: t15.2024.03.17 val PER: 0.2162
2026-01-08 14:28:59,934: t15.2024.05.10 val PER: 0.2229
2026-01-08 14:28:59,934: t15.2024.06.14 val PER: 0.2397
2026-01-08 14:28:59,934: t15.2024.07.19 val PER: 0.3204
2026-01-08 14:28:59,934: t15.2024.07.21 val PER: 0.1669
2026-01-08 14:28:59,934: t15.2024.07.28 val PER: 0.2169
2026-01-08 14:28:59,934: t15.2025.01.10 val PER: 0.3981
2026-01-08 14:28:59,934: t15.2025.01.12 val PER: 0.2163
2026-01-08 14:28:59,934: t15.2025.03.14 val PER: 0.3861
2026-01-08 14:28:59,934: t15.2025.03.16 val PER: 0.2709
2026-01-08 14:28:59,934: t15.2025.03.30 val PER: 0.3816
2026-01-08 14:28:59,934: t15.2025.04.13 val PER: 0.2767
2026-01-08 14:28:59,936: New best val WER(1gram) 60.66% --> 59.14%
2026-01-08 14:28:59,936: Checkpointing model
2026-01-08 14:29:00,080: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:29:16,561: Train batch 6200: loss: 17.62 grad norm: 57.30 time: 0.071
2026-01-08 14:29:33,240: Train batch 6400: loss: 20.82 grad norm: 64.37 time: 0.063
2026-01-08 14:29:41,478: Running test after training batch: 6500
2026-01-08 14:29:41,575: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:29:46,333: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e the could at this point as will
2026-01-08 14:29:46,363: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nett
2026-01-08 14:29:48,008: Val batch 6500: PER (avg): 0.2093 CTC Loss (avg): 20.8232 WER(1gram): 54.31% (n=64) time: 6.529
2026-01-08 14:29:48,008: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-08 14:29:48,008: t15.2023.08.13 val PER: 0.1715
2026-01-08 14:29:48,008: t15.2023.08.18 val PER: 0.1459
2026-01-08 14:29:48,009: t15.2023.08.20 val PER: 0.1557
2026-01-08 14:29:48,009: t15.2023.08.25 val PER: 0.1235
2026-01-08 14:29:48,009: t15.2023.08.27 val PER: 0.2556
2026-01-08 14:29:48,009: t15.2023.09.01 val PER: 0.1258
2026-01-08 14:29:48,009: t15.2023.09.03 val PER: 0.2126
2026-01-08 14:29:48,009: t15.2023.09.24 val PER: 0.1699
2026-01-08 14:29:48,009: t15.2023.09.29 val PER: 0.1717
2026-01-08 14:29:48,009: t15.2023.10.01 val PER: 0.2213
2026-01-08 14:29:48,009: t15.2023.10.06 val PER: 0.1378
2026-01-08 14:29:48,009: t15.2023.10.08 val PER: 0.2963
2026-01-08 14:29:48,009: t15.2023.10.13 val PER: 0.2645
2026-01-08 14:29:48,010: t15.2023.10.15 val PER: 0.2175
2026-01-08 14:29:48,010: t15.2023.10.20 val PER: 0.2315
2026-01-08 14:29:48,010: t15.2023.10.22 val PER: 0.1748
2026-01-08 14:29:48,010: t15.2023.11.03 val PER: 0.2252
2026-01-08 14:29:48,010: t15.2023.11.04 val PER: 0.0512
2026-01-08 14:29:48,010: t15.2023.11.17 val PER: 0.0731
2026-01-08 14:29:48,010: t15.2023.11.19 val PER: 0.0798
2026-01-08 14:29:48,010: t15.2023.11.26 val PER: 0.2130
2026-01-08 14:29:48,010: t15.2023.12.03 val PER: 0.1733
2026-01-08 14:29:48,010: t15.2023.12.08 val PER: 0.1758
2026-01-08 14:29:48,010: t15.2023.12.10 val PER: 0.1629
2026-01-08 14:29:48,010: t15.2023.12.17 val PER: 0.1975
2026-01-08 14:29:48,010: t15.2023.12.29 val PER: 0.2080
2026-01-08 14:29:48,010: t15.2024.02.25 val PER: 0.1713
2026-01-08 14:29:48,010: t15.2024.03.08 val PER: 0.3030
2026-01-08 14:29:48,010: t15.2024.03.15 val PER: 0.2527
2026-01-08 14:29:48,011: t15.2024.03.17 val PER: 0.2057
2026-01-08 14:29:48,011: t15.2024.05.10 val PER: 0.2214
2026-01-08 14:29:48,011: t15.2024.06.14 val PER: 0.2145
2026-01-08 14:29:48,011: t15.2024.07.19 val PER: 0.3065
2026-01-08 14:29:48,011: t15.2024.07.21 val PER: 0.1600
2026-01-08 14:29:48,011: t15.2024.07.28 val PER: 0.1993
2026-01-08 14:29:48,011: t15.2025.01.10 val PER: 0.3815
2026-01-08 14:29:48,011: t15.2025.01.12 val PER: 0.2109
2026-01-08 14:29:48,011: t15.2025.03.14 val PER: 0.3802
2026-01-08 14:29:48,011: t15.2025.03.16 val PER: 0.2474
2026-01-08 14:29:48,011: t15.2025.03.30 val PER: 0.3586
2026-01-08 14:29:48,011: t15.2025.04.13 val PER: 0.2796
2026-01-08 14:29:48,012: New best val WER(1gram) 59.14% --> 54.31%
2026-01-08 14:29:48,012: Checkpointing model
2026-01-08 14:29:48,159: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:29:56,469: Train batch 6600: loss: 12.95 grad norm: 50.02 time: 0.045
2026-01-08 14:30:14,822: Train batch 6800: loss: 15.80 grad norm: 55.43 time: 0.049
2026-01-08 14:30:32,903: Train batch 7000: loss: 19.15 grad norm: 67.89 time: 0.060
2026-01-08 14:30:32,904: Running test after training batch: 7000
2026-01-08 14:30:33,053: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:30:38,147: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point as will
2026-01-08 14:30:38,176: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost ent
2026-01-08 14:30:39,861: Val batch 7000: PER (avg): 0.2004 CTC Loss (avg): 19.9078 WER(1gram): 55.33% (n=64) time: 6.957
2026-01-08 14:30:39,862: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=12
2026-01-08 14:30:39,862: t15.2023.08.13 val PER: 0.1736
2026-01-08 14:30:39,862: t15.2023.08.18 val PER: 0.1509
2026-01-08 14:30:39,862: t15.2023.08.20 val PER: 0.1589
2026-01-08 14:30:39,862: t15.2023.08.25 val PER: 0.1280
2026-01-08 14:30:39,862: t15.2023.08.27 val PER: 0.2138
2026-01-08 14:30:39,862: t15.2023.09.01 val PER: 0.1128
2026-01-08 14:30:39,862: t15.2023.09.03 val PER: 0.2043
2026-01-08 14:30:39,862: t15.2023.09.24 val PER: 0.1626
2026-01-08 14:30:39,862: t15.2023.09.29 val PER: 0.1691
2026-01-08 14:30:39,862: t15.2023.10.01 val PER: 0.2193
2026-01-08 14:30:39,863: t15.2023.10.06 val PER: 0.1259
2026-01-08 14:30:39,863: t15.2023.10.08 val PER: 0.2774
2026-01-08 14:30:39,863: t15.2023.10.13 val PER: 0.2576
2026-01-08 14:30:39,863: t15.2023.10.15 val PER: 0.1991
2026-01-08 14:30:39,863: t15.2023.10.20 val PER: 0.2416
2026-01-08 14:30:39,863: t15.2023.10.22 val PER: 0.1459
2026-01-08 14:30:39,863: t15.2023.11.03 val PER: 0.2123
2026-01-08 14:30:39,863: t15.2023.11.04 val PER: 0.0375
2026-01-08 14:30:39,863: t15.2023.11.17 val PER: 0.0669
2026-01-08 14:30:39,863: t15.2023.11.19 val PER: 0.0659
2026-01-08 14:30:39,863: t15.2023.11.26 val PER: 0.2167
2026-01-08 14:30:39,863: t15.2023.12.03 val PER: 0.1670
2026-01-08 14:30:39,864: t15.2023.12.08 val PER: 0.1571
2026-01-08 14:30:39,864: t15.2023.12.10 val PER: 0.1340
2026-01-08 14:30:39,864: t15.2023.12.17 val PER: 0.1871
2026-01-08 14:30:39,864: t15.2023.12.29 val PER: 0.1915
2026-01-08 14:30:39,864: t15.2024.02.25 val PER: 0.1629
2026-01-08 14:30:39,864: t15.2024.03.08 val PER: 0.2817
2026-01-08 14:30:39,864: t15.2024.03.15 val PER: 0.2527
2026-01-08 14:30:39,864: t15.2024.03.17 val PER: 0.1980
2026-01-08 14:30:39,864: t15.2024.05.10 val PER: 0.1947
2026-01-08 14:30:39,864: t15.2024.06.14 val PER: 0.2114
2026-01-08 14:30:39,864: t15.2024.07.19 val PER: 0.3045
2026-01-08 14:30:39,864: t15.2024.07.21 val PER: 0.1434
2026-01-08 14:30:39,865: t15.2024.07.28 val PER: 0.1853
2026-01-08 14:30:39,865: t15.2025.01.10 val PER: 0.3719
2026-01-08 14:30:39,865: t15.2025.01.12 val PER: 0.1963
2026-01-08 14:30:39,865: t15.2025.03.14 val PER: 0.3772
2026-01-08 14:30:39,865: t15.2025.03.16 val PER: 0.2500
2026-01-08 14:30:39,865: t15.2025.03.30 val PER: 0.3770
2026-01-08 14:30:39,865: t15.2025.04.13 val PER: 0.2668
2026-01-08 14:30:57,552: Train batch 7200: loss: 15.32 grad norm: 58.92 time: 0.080
2026-01-08 14:31:15,033: Train batch 7400: loss: 15.12 grad norm: 56.57 time: 0.076
2026-01-08 14:31:23,569: Running test after training batch: 7500
2026-01-08 14:31:23,679: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:31:28,422: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point is will
2026-01-08 14:31:28,452: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost sent
2026-01-08 14:31:30,094: Val batch 7500: PER (avg): 0.1964 CTC Loss (avg): 19.3971 WER(1gram): 54.82% (n=64) time: 6.525
2026-01-08 14:31:30,094: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-08 14:31:30,094: t15.2023.08.13 val PER: 0.1632
2026-01-08 14:31:30,094: t15.2023.08.18 val PER: 0.1492
2026-01-08 14:31:30,095: t15.2023.08.20 val PER: 0.1533
2026-01-08 14:31:30,095: t15.2023.08.25 val PER: 0.1160
2026-01-08 14:31:30,095: t15.2023.08.27 val PER: 0.2315
2026-01-08 14:31:30,095: t15.2023.09.01 val PER: 0.1144
2026-01-08 14:31:30,095: t15.2023.09.03 val PER: 0.1960
2026-01-08 14:31:30,095: t15.2023.09.24 val PER: 0.1614
2026-01-08 14:31:30,095: t15.2023.09.29 val PER: 0.1640
2026-01-08 14:31:30,095: t15.2023.10.01 val PER: 0.2061
2026-01-08 14:31:30,095: t15.2023.10.06 val PER: 0.1270
2026-01-08 14:31:30,095: t15.2023.10.08 val PER: 0.2801
2026-01-08 14:31:30,095: t15.2023.10.13 val PER: 0.2459
2026-01-08 14:31:30,096: t15.2023.10.15 val PER: 0.2037
2026-01-08 14:31:30,096: t15.2023.10.20 val PER: 0.2081
2026-01-08 14:31:30,096: t15.2023.10.22 val PER: 0.1526
2026-01-08 14:31:30,096: t15.2023.11.03 val PER: 0.2164
2026-01-08 14:31:30,096: t15.2023.11.04 val PER: 0.0512
2026-01-08 14:31:30,096: t15.2023.11.17 val PER: 0.0544
2026-01-08 14:31:30,096: t15.2023.11.19 val PER: 0.0559
2026-01-08 14:31:30,096: t15.2023.11.26 val PER: 0.1964
2026-01-08 14:31:30,096: t15.2023.12.03 val PER: 0.1660
2026-01-08 14:31:30,096: t15.2023.12.08 val PER: 0.1538
2026-01-08 14:31:30,096: t15.2023.12.10 val PER: 0.1353
2026-01-08 14:31:30,096: t15.2023.12.17 val PER: 0.1809
2026-01-08 14:31:30,096: t15.2023.12.29 val PER: 0.1935
2026-01-08 14:31:30,096: t15.2024.02.25 val PER: 0.1629
2026-01-08 14:31:30,096: t15.2024.03.08 val PER: 0.2873
2026-01-08 14:31:30,096: t15.2024.03.15 val PER: 0.2508
2026-01-08 14:31:30,096: t15.2024.03.17 val PER: 0.1862
2026-01-08 14:31:30,097: t15.2024.05.10 val PER: 0.2080
2026-01-08 14:31:30,097: t15.2024.06.14 val PER: 0.2003
2026-01-08 14:31:30,097: t15.2024.07.19 val PER: 0.2993
2026-01-08 14:31:30,097: t15.2024.07.21 val PER: 0.1428
2026-01-08 14:31:30,097: t15.2024.07.28 val PER: 0.1743
2026-01-08 14:31:30,097: t15.2025.01.10 val PER: 0.3719
2026-01-08 14:31:30,097: t15.2025.01.12 val PER: 0.1925
2026-01-08 14:31:30,097: t15.2025.03.14 val PER: 0.3728
2026-01-08 14:31:30,097: t15.2025.03.16 val PER: 0.2369
2026-01-08 14:31:30,097: t15.2025.03.30 val PER: 0.3540
2026-01-08 14:31:30,097: t15.2025.04.13 val PER: 0.2767
2026-01-08 14:31:38,695: Train batch 7600: loss: 16.95 grad norm: 59.50 time: 0.069
2026-01-08 14:31:56,014: Train batch 7800: loss: 15.32 grad norm: 60.46 time: 0.055
2026-01-08 14:32:13,757: Train batch 8000: loss: 12.52 grad norm: 51.21 time: 0.072
2026-01-08 14:32:13,758: Running test after training batch: 8000
2026-01-08 14:32:13,855: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:32:18,631: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point us will
2026-01-08 14:32:18,661: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost send
2026-01-08 14:32:20,385: Val batch 8000: PER (avg): 0.1896 CTC Loss (avg): 18.7867 WER(1gram): 54.31% (n=64) time: 6.627
2026-01-08 14:32:20,389: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-08 14:32:20,389: t15.2023.08.13 val PER: 0.1674
2026-01-08 14:32:20,389: t15.2023.08.18 val PER: 0.1467
2026-01-08 14:32:20,389: t15.2023.08.20 val PER: 0.1477
2026-01-08 14:32:20,389: t15.2023.08.25 val PER: 0.1295
2026-01-08 14:32:20,390: t15.2023.08.27 val PER: 0.2219
2026-01-08 14:32:20,390: t15.2023.09.01 val PER: 0.1063
2026-01-08 14:32:20,390: t15.2023.09.03 val PER: 0.1900
2026-01-08 14:32:20,390: t15.2023.09.24 val PER: 0.1626
2026-01-08 14:32:20,390: t15.2023.09.29 val PER: 0.1519
2026-01-08 14:32:20,390: t15.2023.10.01 val PER: 0.1896
2026-01-08 14:32:20,390: t15.2023.10.06 val PER: 0.1216
2026-01-08 14:32:20,390: t15.2023.10.08 val PER: 0.2706
2026-01-08 14:32:20,390: t15.2023.10.13 val PER: 0.2420
2026-01-08 14:32:20,390: t15.2023.10.15 val PER: 0.1898
2026-01-08 14:32:20,390: t15.2023.10.20 val PER: 0.2114
2026-01-08 14:32:20,390: t15.2023.10.22 val PER: 0.1448
2026-01-08 14:32:20,390: t15.2023.11.03 val PER: 0.2110
2026-01-08 14:32:20,391: t15.2023.11.04 val PER: 0.0444
2026-01-08 14:32:20,391: t15.2023.11.17 val PER: 0.0622
2026-01-08 14:32:20,391: t15.2023.11.19 val PER: 0.0599
2026-01-08 14:32:20,391: t15.2023.11.26 val PER: 0.1957
2026-01-08 14:32:20,391: t15.2023.12.03 val PER: 0.1565
2026-01-08 14:32:20,391: t15.2023.12.08 val PER: 0.1511
2026-01-08 14:32:20,391: t15.2023.12.10 val PER: 0.1353
2026-01-08 14:32:20,391: t15.2023.12.17 val PER: 0.1757
2026-01-08 14:32:20,391: t15.2023.12.29 val PER: 0.1723
2026-01-08 14:32:20,391: t15.2024.02.25 val PER: 0.1643
2026-01-08 14:32:20,391: t15.2024.03.08 val PER: 0.2902
2026-01-08 14:32:20,391: t15.2024.03.15 val PER: 0.2395
2026-01-08 14:32:20,392: t15.2024.03.17 val PER: 0.1897
2026-01-08 14:32:20,392: t15.2024.05.10 val PER: 0.1857
2026-01-08 14:32:20,392: t15.2024.06.14 val PER: 0.2035
2026-01-08 14:32:20,392: t15.2024.07.19 val PER: 0.2815
2026-01-08 14:32:20,392: t15.2024.07.21 val PER: 0.1283
2026-01-08 14:32:20,392: t15.2024.07.28 val PER: 0.1647
2026-01-08 14:32:20,392: t15.2025.01.10 val PER: 0.3526
2026-01-08 14:32:20,392: t15.2025.01.12 val PER: 0.1809
2026-01-08 14:32:20,392: t15.2025.03.14 val PER: 0.3624
2026-01-08 14:32:20,392: t15.2025.03.16 val PER: 0.2421
2026-01-08 14:32:20,392: t15.2025.03.30 val PER: 0.3506
2026-01-08 14:32:20,392: t15.2025.04.13 val PER: 0.2682
2026-01-08 14:32:38,128: Train batch 8200: loss: 10.75 grad norm: 48.80 time: 0.054
2026-01-08 14:32:55,254: Train batch 8400: loss: 10.94 grad norm: 47.75 time: 0.064
2026-01-08 14:33:04,038: Running test after training batch: 8500
2026-01-08 14:33:04,144: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:33:09,090: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:33:09,120: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost end
2026-01-08 14:33:10,817: Val batch 8500: PER (avg): 0.1860 CTC Loss (avg): 18.3553 WER(1gram): 51.52% (n=64) time: 6.779
2026-01-08 14:33:10,818: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-08 14:33:10,818: t15.2023.08.13 val PER: 0.1497
2026-01-08 14:33:10,818: t15.2023.08.18 val PER: 0.1400
2026-01-08 14:33:10,818: t15.2023.08.20 val PER: 0.1406
2026-01-08 14:33:10,818: t15.2023.08.25 val PER: 0.1220
2026-01-08 14:33:10,818: t15.2023.08.27 val PER: 0.2251
2026-01-08 14:33:10,818: t15.2023.09.01 val PER: 0.1071
2026-01-08 14:33:10,818: t15.2023.09.03 val PER: 0.1960
2026-01-08 14:33:10,818: t15.2023.09.24 val PER: 0.1553
2026-01-08 14:33:10,818: t15.2023.09.29 val PER: 0.1544
2026-01-08 14:33:10,818: t15.2023.10.01 val PER: 0.1876
2026-01-08 14:33:10,818: t15.2023.10.06 val PER: 0.1281
2026-01-08 14:33:10,818: t15.2023.10.08 val PER: 0.2760
2026-01-08 14:33:10,819: t15.2023.10.13 val PER: 0.2467
2026-01-08 14:33:10,819: t15.2023.10.15 val PER: 0.1931
2026-01-08 14:33:10,819: t15.2023.10.20 val PER: 0.1779
2026-01-08 14:33:10,819: t15.2023.10.22 val PER: 0.1481
2026-01-08 14:33:10,819: t15.2023.11.03 val PER: 0.2028
2026-01-08 14:33:10,819: t15.2023.11.04 val PER: 0.0444
2026-01-08 14:33:10,819: t15.2023.11.17 val PER: 0.0575
2026-01-08 14:33:10,819: t15.2023.11.19 val PER: 0.0519
2026-01-08 14:33:10,819: t15.2023.11.26 val PER: 0.1833
2026-01-08 14:33:10,819: t15.2023.12.03 val PER: 0.1555
2026-01-08 14:33:10,819: t15.2023.12.08 val PER: 0.1478
2026-01-08 14:33:10,820: t15.2023.12.10 val PER: 0.1261
2026-01-08 14:33:10,820: t15.2023.12.17 val PER: 0.1767
2026-01-08 14:33:10,820: t15.2023.12.29 val PER: 0.1736
2026-01-08 14:33:10,820: t15.2024.02.25 val PER: 0.1461
2026-01-08 14:33:10,820: t15.2024.03.08 val PER: 0.2603
2026-01-08 14:33:10,820: t15.2024.03.15 val PER: 0.2364
2026-01-08 14:33:10,820: t15.2024.03.17 val PER: 0.1764
2026-01-08 14:33:10,820: t15.2024.05.10 val PER: 0.2006
2026-01-08 14:33:10,820: t15.2024.06.14 val PER: 0.1987
2026-01-08 14:33:10,820: t15.2024.07.19 val PER: 0.2848
2026-01-08 14:33:10,820: t15.2024.07.21 val PER: 0.1262
2026-01-08 14:33:10,820: t15.2024.07.28 val PER: 0.1713
2026-01-08 14:33:10,820: t15.2025.01.10 val PER: 0.3444
2026-01-08 14:33:10,821: t15.2025.01.12 val PER: 0.1886
2026-01-08 14:33:10,821: t15.2025.03.14 val PER: 0.3506
2026-01-08 14:33:10,821: t15.2025.03.16 val PER: 0.2343
2026-01-08 14:33:10,821: t15.2025.03.30 val PER: 0.3287
2026-01-08 14:33:10,821: t15.2025.04.13 val PER: 0.2553
2026-01-08 14:33:10,821: New best val WER(1gram) 54.31% --> 51.52%
2026-01-08 14:33:10,821: Checkpointing model
2026-01-08 14:33:10,964: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:33:20,051: Train batch 8600: loss: 16.85 grad norm: 59.11 time: 0.054
2026-01-08 14:33:37,976: Train batch 8800: loss: 16.34 grad norm: 59.48 time: 0.060
2026-01-08 14:33:56,018: Train batch 9000: loss: 16.54 grad norm: 69.12 time: 0.074
2026-01-08 14:33:56,018: Running test after training batch: 9000
2026-01-08 14:33:56,143: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:34:00,981: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point as will
2026-01-08 14:34:01,013: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost ed
2026-01-08 14:34:02,948: Val batch 9000: PER (avg): 0.1825 CTC Loss (avg): 18.1519 WER(1gram): 54.06% (n=64) time: 6.930
2026-01-08 14:34:02,948: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-08 14:34:02,949: t15.2023.08.13 val PER: 0.1528
2026-01-08 14:34:02,949: t15.2023.08.18 val PER: 0.1400
2026-01-08 14:34:02,949: t15.2023.08.20 val PER: 0.1342
2026-01-08 14:34:02,949: t15.2023.08.25 val PER: 0.1175
2026-01-08 14:34:02,949: t15.2023.08.27 val PER: 0.2170
2026-01-08 14:34:02,949: t15.2023.09.01 val PER: 0.1088
2026-01-08 14:34:02,949: t15.2023.09.03 val PER: 0.1888
2026-01-08 14:34:02,949: t15.2023.09.24 val PER: 0.1481
2026-01-08 14:34:02,949: t15.2023.09.29 val PER: 0.1519
2026-01-08 14:34:02,949: t15.2023.10.01 val PER: 0.1962
2026-01-08 14:34:02,949: t15.2023.10.06 val PER: 0.1001
2026-01-08 14:34:02,949: t15.2023.10.08 val PER: 0.2733
2026-01-08 14:34:02,950: t15.2023.10.13 val PER: 0.2389
2026-01-08 14:34:02,950: t15.2023.10.15 val PER: 0.1892
2026-01-08 14:34:02,950: t15.2023.10.20 val PER: 0.2081
2026-01-08 14:34:02,950: t15.2023.10.22 val PER: 0.1448
2026-01-08 14:34:02,950: t15.2023.11.03 val PER: 0.2117
2026-01-08 14:34:02,950: t15.2023.11.04 val PER: 0.0444
2026-01-08 14:34:02,950: t15.2023.11.17 val PER: 0.0591
2026-01-08 14:34:02,950: t15.2023.11.19 val PER: 0.0619
2026-01-08 14:34:02,950: t15.2023.11.26 val PER: 0.1732
2026-01-08 14:34:02,950: t15.2023.12.03 val PER: 0.1481
2026-01-08 14:34:02,950: t15.2023.12.08 val PER: 0.1358
2026-01-08 14:34:02,950: t15.2023.12.10 val PER: 0.1143
2026-01-08 14:34:02,951: t15.2023.12.17 val PER: 0.1705
2026-01-08 14:34:02,951: t15.2023.12.29 val PER: 0.1757
2026-01-08 14:34:02,951: t15.2024.02.25 val PER: 0.1404
2026-01-08 14:34:02,951: t15.2024.03.08 val PER: 0.2632
2026-01-08 14:34:02,951: t15.2024.03.15 val PER: 0.2314
2026-01-08 14:34:02,951: t15.2024.03.17 val PER: 0.1743
2026-01-08 14:34:02,951: t15.2024.05.10 val PER: 0.1679
2026-01-08 14:34:02,951: t15.2024.06.14 val PER: 0.1972
2026-01-08 14:34:02,951: t15.2024.07.19 val PER: 0.2861
2026-01-08 14:34:02,951: t15.2024.07.21 val PER: 0.1159
2026-01-08 14:34:02,952: t15.2024.07.28 val PER: 0.1662
2026-01-08 14:34:02,952: t15.2025.01.10 val PER: 0.3278
2026-01-08 14:34:02,952: t15.2025.01.12 val PER: 0.1809
2026-01-08 14:34:02,952: t15.2025.03.14 val PER: 0.3491
2026-01-08 14:34:02,952: t15.2025.03.16 val PER: 0.2395
2026-01-08 14:34:02,952: t15.2025.03.30 val PER: 0.3425
2026-01-08 14:34:02,952: t15.2025.04.13 val PER: 0.2568
2026-01-08 14:34:20,247: Train batch 9200: loss: 10.75 grad norm: 44.62 time: 0.057
2026-01-08 14:34:37,309: Train batch 9400: loss: 8.33 grad norm: 44.23 time: 0.067
2026-01-08 14:34:46,189: Running test after training batch: 9500
2026-01-08 14:34:46,351: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:34:51,673: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point us will
2026-01-08 14:34:51,703: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sent
2026-01-08 14:34:53,339: Val batch 9500: PER (avg): 0.1788 CTC Loss (avg): 17.6877 WER(1gram): 52.28% (n=64) time: 7.150
2026-01-08 14:34:53,339: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-08 14:34:53,339: t15.2023.08.13 val PER: 0.1424
2026-01-08 14:34:53,340: t15.2023.08.18 val PER: 0.1299
2026-01-08 14:34:53,340: t15.2023.08.20 val PER: 0.1311
2026-01-08 14:34:53,340: t15.2023.08.25 val PER: 0.1145
2026-01-08 14:34:53,340: t15.2023.08.27 val PER: 0.2042
2026-01-08 14:34:53,340: t15.2023.09.01 val PER: 0.1006
2026-01-08 14:34:53,340: t15.2023.09.03 val PER: 0.1710
2026-01-08 14:34:53,340: t15.2023.09.24 val PER: 0.1444
2026-01-08 14:34:53,340: t15.2023.09.29 val PER: 0.1512
2026-01-08 14:34:53,340: t15.2023.10.01 val PER: 0.1876
2026-01-08 14:34:53,340: t15.2023.10.06 val PER: 0.1184
2026-01-08 14:34:53,341: t15.2023.10.08 val PER: 0.2720
2026-01-08 14:34:53,341: t15.2023.10.13 val PER: 0.2320
2026-01-08 14:34:53,341: t15.2023.10.15 val PER: 0.1819
2026-01-08 14:34:53,341: t15.2023.10.20 val PER: 0.2047
2026-01-08 14:34:53,341: t15.2023.10.22 val PER: 0.1414
2026-01-08 14:34:53,341: t15.2023.11.03 val PER: 0.1995
2026-01-08 14:34:53,341: t15.2023.11.04 val PER: 0.0307
2026-01-08 14:34:53,341: t15.2023.11.17 val PER: 0.0591
2026-01-08 14:34:53,341: t15.2023.11.19 val PER: 0.0559
2026-01-08 14:34:53,341: t15.2023.11.26 val PER: 0.1674
2026-01-08 14:34:53,341: t15.2023.12.03 val PER: 0.1492
2026-01-08 14:34:53,341: t15.2023.12.08 val PER: 0.1372
2026-01-08 14:34:53,341: t15.2023.12.10 val PER: 0.1209
2026-01-08 14:34:53,341: t15.2023.12.17 val PER: 0.1757
2026-01-08 14:34:53,342: t15.2023.12.29 val PER: 0.1572
2026-01-08 14:34:53,342: t15.2024.02.25 val PER: 0.1292
2026-01-08 14:34:53,342: t15.2024.03.08 val PER: 0.2674
2026-01-08 14:34:53,342: t15.2024.03.15 val PER: 0.2301
2026-01-08 14:34:53,342: t15.2024.03.17 val PER: 0.1778
2026-01-08 14:34:53,342: t15.2024.05.10 val PER: 0.1961
2026-01-08 14:34:53,342: t15.2024.06.14 val PER: 0.1909
2026-01-08 14:34:53,342: t15.2024.07.19 val PER: 0.2782
2026-01-08 14:34:53,342: t15.2024.07.21 val PER: 0.1179
2026-01-08 14:34:53,342: t15.2024.07.28 val PER: 0.1625
2026-01-08 14:34:53,342: t15.2025.01.10 val PER: 0.3209
2026-01-08 14:34:53,342: t15.2025.01.12 val PER: 0.1817
2026-01-08 14:34:53,342: t15.2025.03.14 val PER: 0.3506
2026-01-08 14:34:53,342: t15.2025.03.16 val PER: 0.2356
2026-01-08 14:34:53,343: t15.2025.03.30 val PER: 0.3287
2026-01-08 14:34:53,343: t15.2025.04.13 val PER: 0.2568
2026-01-08 14:35:01,622: Train batch 9600: loss: 9.48 grad norm: 47.99 time: 0.074
2026-01-08 14:35:18,519: Train batch 9800: loss: 13.51 grad norm: 62.18 time: 0.062
2026-01-08 14:35:37,012: Train batch 10000: loss: 6.34 grad norm: 39.57 time: 0.060
2026-01-08 14:35:37,012: Running test after training batch: 10000
2026-01-08 14:35:37,132: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:35:41,841: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:35:41,871: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ent
2026-01-08 14:35:43,610: Val batch 10000: PER (avg): 0.1745 CTC Loss (avg): 17.5069 WER(1gram): 51.52% (n=64) time: 6.598
2026-01-08 14:35:43,611: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-08 14:35:43,611: t15.2023.08.13 val PER: 0.1310
2026-01-08 14:35:43,611: t15.2023.08.18 val PER: 0.1375
2026-01-08 14:35:43,611: t15.2023.08.20 val PER: 0.1247
2026-01-08 14:35:43,611: t15.2023.08.25 val PER: 0.1205
2026-01-08 14:35:43,611: t15.2023.08.27 val PER: 0.2090
2026-01-08 14:35:43,611: t15.2023.09.01 val PER: 0.0958
2026-01-08 14:35:43,611: t15.2023.09.03 val PER: 0.1746
2026-01-08 14:35:43,611: t15.2023.09.24 val PER: 0.1408
2026-01-08 14:35:43,611: t15.2023.09.29 val PER: 0.1544
2026-01-08 14:35:43,612: t15.2023.10.01 val PER: 0.1882
2026-01-08 14:35:43,612: t15.2023.10.06 val PER: 0.1119
2026-01-08 14:35:43,612: t15.2023.10.08 val PER: 0.2639
2026-01-08 14:35:43,612: t15.2023.10.13 val PER: 0.2327
2026-01-08 14:35:43,612: t15.2023.10.15 val PER: 0.1773
2026-01-08 14:35:43,612: t15.2023.10.20 val PER: 0.2148
2026-01-08 14:35:43,612: t15.2023.10.22 val PER: 0.1303
2026-01-08 14:35:43,612: t15.2023.11.03 val PER: 0.2001
2026-01-08 14:35:43,612: t15.2023.11.04 val PER: 0.0410
2026-01-08 14:35:43,612: t15.2023.11.17 val PER: 0.0591
2026-01-08 14:35:43,612: t15.2023.11.19 val PER: 0.0499
2026-01-08 14:35:43,612: t15.2023.11.26 val PER: 0.1529
2026-01-08 14:35:43,612: t15.2023.12.03 val PER: 0.1376
2026-01-08 14:35:43,612: t15.2023.12.08 val PER: 0.1332
2026-01-08 14:35:43,613: t15.2023.12.10 val PER: 0.1196
2026-01-08 14:35:43,613: t15.2023.12.17 val PER: 0.1653
2026-01-08 14:35:43,613: t15.2023.12.29 val PER: 0.1551
2026-01-08 14:35:43,613: t15.2024.02.25 val PER: 0.1306
2026-01-08 14:35:43,613: t15.2024.03.08 val PER: 0.2518
2026-01-08 14:35:43,613: t15.2024.03.15 val PER: 0.2176
2026-01-08 14:35:43,613: t15.2024.03.17 val PER: 0.1750
2026-01-08 14:35:43,613: t15.2024.05.10 val PER: 0.1738
2026-01-08 14:35:43,613: t15.2024.06.14 val PER: 0.1861
2026-01-08 14:35:43,613: t15.2024.07.19 val PER: 0.2755
2026-01-08 14:35:43,614: t15.2024.07.21 val PER: 0.1110
2026-01-08 14:35:43,614: t15.2024.07.28 val PER: 0.1551
2026-01-08 14:35:43,614: t15.2025.01.10 val PER: 0.3306
2026-01-08 14:35:43,616: t15.2025.01.12 val PER: 0.1724
2026-01-08 14:35:43,617: t15.2025.03.14 val PER: 0.3565
2026-01-08 14:35:43,617: t15.2025.03.16 val PER: 0.2225
2026-01-08 14:35:43,617: t15.2025.03.30 val PER: 0.3207
2026-01-08 14:35:43,618: t15.2025.04.13 val PER: 0.2482
2026-01-08 14:36:01,347: Train batch 10200: loss: 6.40 grad norm: 34.58 time: 0.050
2026-01-08 14:36:18,899: Train batch 10400: loss: 10.38 grad norm: 51.25 time: 0.074
2026-01-08 14:36:27,913: Running test after training batch: 10500
2026-01-08 14:36:28,035: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:36:32,781: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:36:32,813: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost ent
2026-01-08 14:36:34,532: Val batch 10500: PER (avg): 0.1721 CTC Loss (avg): 17.3624 WER(1gram): 51.52% (n=64) time: 6.618
2026-01-08 14:36:34,532: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-08 14:36:34,532: t15.2023.08.13 val PER: 0.1362
2026-01-08 14:36:34,532: t15.2023.08.18 val PER: 0.1400
2026-01-08 14:36:34,532: t15.2023.08.20 val PER: 0.1303
2026-01-08 14:36:34,533: t15.2023.08.25 val PER: 0.1190
2026-01-08 14:36:34,533: t15.2023.08.27 val PER: 0.1913
2026-01-08 14:36:34,533: t15.2023.09.01 val PER: 0.0990
2026-01-08 14:36:34,533: t15.2023.09.03 val PER: 0.1734
2026-01-08 14:36:34,533: t15.2023.09.24 val PER: 0.1420
2026-01-08 14:36:34,533: t15.2023.09.29 val PER: 0.1468
2026-01-08 14:36:34,533: t15.2023.10.01 val PER: 0.1922
2026-01-08 14:36:34,533: t15.2023.10.06 val PER: 0.1119
2026-01-08 14:36:34,533: t15.2023.10.08 val PER: 0.2571
2026-01-08 14:36:34,533: t15.2023.10.13 val PER: 0.2133
2026-01-08 14:36:34,533: t15.2023.10.15 val PER: 0.1800
2026-01-08 14:36:34,533: t15.2023.10.20 val PER: 0.2181
2026-01-08 14:36:34,533: t15.2023.10.22 val PER: 0.1214
2026-01-08 14:36:34,533: t15.2023.11.03 val PER: 0.1900
2026-01-08 14:36:34,534: t15.2023.11.04 val PER: 0.0444
2026-01-08 14:36:34,534: t15.2023.11.17 val PER: 0.0498
2026-01-08 14:36:34,534: t15.2023.11.19 val PER: 0.0459
2026-01-08 14:36:34,534: t15.2023.11.26 val PER: 0.1565
2026-01-08 14:36:34,534: t15.2023.12.03 val PER: 0.1313
2026-01-08 14:36:34,534: t15.2023.12.08 val PER: 0.1305
2026-01-08 14:36:34,534: t15.2023.12.10 val PER: 0.1104
2026-01-08 14:36:34,534: t15.2023.12.17 val PER: 0.1590
2026-01-08 14:36:34,534: t15.2023.12.29 val PER: 0.1544
2026-01-08 14:36:34,534: t15.2024.02.25 val PER: 0.1404
2026-01-08 14:36:34,534: t15.2024.03.08 val PER: 0.2603
2026-01-08 14:36:34,534: t15.2024.03.15 val PER: 0.2189
2026-01-08 14:36:34,534: t15.2024.03.17 val PER: 0.1604
2026-01-08 14:36:34,534: t15.2024.05.10 val PER: 0.1649
2026-01-08 14:36:34,534: t15.2024.06.14 val PER: 0.1972
2026-01-08 14:36:34,534: t15.2024.07.19 val PER: 0.2815
2026-01-08 14:36:34,535: t15.2024.07.21 val PER: 0.1110
2026-01-08 14:36:34,535: t15.2024.07.28 val PER: 0.1529
2026-01-08 14:36:34,535: t15.2025.01.10 val PER: 0.3347
2026-01-08 14:36:34,535: t15.2025.01.12 val PER: 0.1663
2026-01-08 14:36:34,535: t15.2025.03.14 val PER: 0.3476
2026-01-08 14:36:34,535: t15.2025.03.16 val PER: 0.2134
2026-01-08 14:36:34,535: t15.2025.03.30 val PER: 0.3161
2026-01-08 14:36:34,535: t15.2025.04.13 val PER: 0.2397
2026-01-08 14:36:43,707: Train batch 10600: loss: 10.08 grad norm: 56.70 time: 0.073
2026-01-08 14:37:01,729: Train batch 10800: loss: 15.65 grad norm: 63.43 time: 0.065
2026-01-08 14:37:19,575: Train batch 11000: loss: 16.25 grad norm: 63.62 time: 0.057
2026-01-08 14:37:19,575: Running test after training batch: 11000
2026-01-08 14:37:19,714: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:37:24,826: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point us will
2026-01-08 14:37:24,857: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost ent
2026-01-08 14:37:26,616: Val batch 11000: PER (avg): 0.1694 CTC Loss (avg): 16.9907 WER(1gram): 49.75% (n=64) time: 7.040
2026-01-08 14:37:26,616: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=12
2026-01-08 14:37:26,616: t15.2023.08.13 val PER: 0.1320
2026-01-08 14:37:26,616: t15.2023.08.18 val PER: 0.1366
2026-01-08 14:37:26,616: t15.2023.08.20 val PER: 0.1311
2026-01-08 14:37:26,616: t15.2023.08.25 val PER: 0.1084
2026-01-08 14:37:26,616: t15.2023.08.27 val PER: 0.2058
2026-01-08 14:37:26,616: t15.2023.09.01 val PER: 0.0990
2026-01-08 14:37:26,616: t15.2023.09.03 val PER: 0.1722
2026-01-08 14:37:26,617: t15.2023.09.24 val PER: 0.1432
2026-01-08 14:37:26,617: t15.2023.09.29 val PER: 0.1461
2026-01-08 14:37:26,617: t15.2023.10.01 val PER: 0.1836
2026-01-08 14:37:26,617: t15.2023.10.06 val PER: 0.1044
2026-01-08 14:37:26,617: t15.2023.10.08 val PER: 0.2612
2026-01-08 14:37:26,617: t15.2023.10.13 val PER: 0.2149
2026-01-08 14:37:26,617: t15.2023.10.15 val PER: 0.1707
2026-01-08 14:37:26,617: t15.2023.10.20 val PER: 0.1846
2026-01-08 14:37:26,617: t15.2023.10.22 val PER: 0.1303
2026-01-08 14:37:26,617: t15.2023.11.03 val PER: 0.1940
2026-01-08 14:37:26,617: t15.2023.11.04 val PER: 0.0444
2026-01-08 14:37:26,617: t15.2023.11.17 val PER: 0.0591
2026-01-08 14:37:26,617: t15.2023.11.19 val PER: 0.0419
2026-01-08 14:37:26,617: t15.2023.11.26 val PER: 0.1536
2026-01-08 14:37:26,618: t15.2023.12.03 val PER: 0.1303
2026-01-08 14:37:26,618: t15.2023.12.08 val PER: 0.1205
2026-01-08 14:37:26,618: t15.2023.12.10 val PER: 0.1064
2026-01-08 14:37:26,618: t15.2023.12.17 val PER: 0.1497
2026-01-08 14:37:26,618: t15.2023.12.29 val PER: 0.1599
2026-01-08 14:37:26,618: t15.2024.02.25 val PER: 0.1250
2026-01-08 14:37:26,618: t15.2024.03.08 val PER: 0.2603
2026-01-08 14:37:26,618: t15.2024.03.15 val PER: 0.2095
2026-01-08 14:37:26,618: t15.2024.03.17 val PER: 0.1604
2026-01-08 14:37:26,618: t15.2024.05.10 val PER: 0.1768
2026-01-08 14:37:26,618: t15.2024.06.14 val PER: 0.1830
2026-01-08 14:37:26,618: t15.2024.07.19 val PER: 0.2657
2026-01-08 14:37:26,618: t15.2024.07.21 val PER: 0.1083
2026-01-08 14:37:26,618: t15.2024.07.28 val PER: 0.1559
2026-01-08 14:37:26,618: t15.2025.01.10 val PER: 0.3292
2026-01-08 14:37:26,618: t15.2025.01.12 val PER: 0.1578
2026-01-08 14:37:26,618: t15.2025.03.14 val PER: 0.3550
2026-01-08 14:37:26,619: t15.2025.03.16 val PER: 0.2134
2026-01-08 14:37:26,619: t15.2025.03.30 val PER: 0.3195
2026-01-08 14:37:26,619: t15.2025.04.13 val PER: 0.2325
2026-01-08 14:37:26,620: New best val WER(1gram) 51.52% --> 49.75%
2026-01-08 14:37:26,620: Checkpointing model
2026-01-08 14:37:26,764: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:37:44,045: Train batch 11200: loss: 11.91 grad norm: 52.87 time: 0.071
2026-01-08 14:38:01,456: Train batch 11400: loss: 10.48 grad norm: 51.55 time: 0.057
2026-01-08 14:38:10,196: Running test after training batch: 11500
2026-01-08 14:38:10,301: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:38:15,113: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:38:15,145: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost sent
2026-01-08 14:38:16,908: Val batch 11500: PER (avg): 0.1677 CTC Loss (avg): 16.8326 WER(1gram): 47.72% (n=64) time: 6.712
2026-01-08 14:38:16,909: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=12
2026-01-08 14:38:16,909: t15.2023.08.13 val PER: 0.1195
2026-01-08 14:38:16,909: t15.2023.08.18 val PER: 0.1249
2026-01-08 14:38:16,909: t15.2023.08.20 val PER: 0.1247
2026-01-08 14:38:16,909: t15.2023.08.25 val PER: 0.1084
2026-01-08 14:38:16,909: t15.2023.08.27 val PER: 0.2042
2026-01-08 14:38:16,910: t15.2023.09.01 val PER: 0.0917
2026-01-08 14:38:16,910: t15.2023.09.03 val PER: 0.1781
2026-01-08 14:38:16,910: t15.2023.09.24 val PER: 0.1359
2026-01-08 14:38:16,910: t15.2023.09.29 val PER: 0.1449
2026-01-08 14:38:16,910: t15.2023.10.01 val PER: 0.1816
2026-01-08 14:38:16,910: t15.2023.10.06 val PER: 0.0926
2026-01-08 14:38:16,910: t15.2023.10.08 val PER: 0.2544
2026-01-08 14:38:16,910: t15.2023.10.13 val PER: 0.2110
2026-01-08 14:38:16,910: t15.2023.10.15 val PER: 0.1648
2026-01-08 14:38:16,911: t15.2023.10.20 val PER: 0.1913
2026-01-08 14:38:16,911: t15.2023.10.22 val PER: 0.1381
2026-01-08 14:38:16,911: t15.2023.11.03 val PER: 0.1852
2026-01-08 14:38:16,911: t15.2023.11.04 val PER: 0.0375
2026-01-08 14:38:16,911: t15.2023.11.17 val PER: 0.0529
2026-01-08 14:38:16,911: t15.2023.11.19 val PER: 0.0419
2026-01-08 14:38:16,911: t15.2023.11.26 val PER: 0.1529
2026-01-08 14:38:16,911: t15.2023.12.03 val PER: 0.1334
2026-01-08 14:38:16,911: t15.2023.12.08 val PER: 0.1158
2026-01-08 14:38:16,911: t15.2023.12.10 val PER: 0.1025
2026-01-08 14:38:16,911: t15.2023.12.17 val PER: 0.1528
2026-01-08 14:38:16,912: t15.2023.12.29 val PER: 0.1462
2026-01-08 14:38:16,912: t15.2024.02.25 val PER: 0.1362
2026-01-08 14:38:16,912: t15.2024.03.08 val PER: 0.2532
2026-01-08 14:38:16,912: t15.2024.03.15 val PER: 0.2101
2026-01-08 14:38:16,912: t15.2024.03.17 val PER: 0.1618
2026-01-08 14:38:16,912: t15.2024.05.10 val PER: 0.1917
2026-01-08 14:38:16,912: t15.2024.06.14 val PER: 0.1830
2026-01-08 14:38:16,912: t15.2024.07.19 val PER: 0.2709
2026-01-08 14:38:16,912: t15.2024.07.21 val PER: 0.1097
2026-01-08 14:38:16,912: t15.2024.07.28 val PER: 0.1522
2026-01-08 14:38:16,913: t15.2025.01.10 val PER: 0.3292
2026-01-08 14:38:16,913: t15.2025.01.12 val PER: 0.1701
2026-01-08 14:38:16,913: t15.2025.03.14 val PER: 0.3565
2026-01-08 14:38:16,913: t15.2025.03.16 val PER: 0.2225
2026-01-08 14:38:16,913: t15.2025.03.30 val PER: 0.3230
2026-01-08 14:38:16,913: t15.2025.04.13 val PER: 0.2325
2026-01-08 14:38:16,913: New best val WER(1gram) 49.75% --> 47.72%
2026-01-08 14:38:16,914: Checkpointing model
2026-01-08 14:38:17,058: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:38:25,699: Train batch 11600: loss: 11.90 grad norm: 54.10 time: 0.062
2026-01-08 14:38:42,729: Train batch 11800: loss: 7.17 grad norm: 39.65 time: 0.045
2026-01-08 14:38:59,866: Train batch 12000: loss: 14.24 grad norm: 58.16 time: 0.072
2026-01-08 14:38:59,866: Running test after training batch: 12000
2026-01-08 14:38:59,957: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:39:04,691: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:39:04,722: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sent
2026-01-08 14:39:06,498: Val batch 12000: PER (avg): 0.1635 CTC Loss (avg): 16.8012 WER(1gram): 48.48% (n=64) time: 6.632
2026-01-08 14:39:06,499: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-08 14:39:06,499: t15.2023.08.13 val PER: 0.1206
2026-01-08 14:39:06,499: t15.2023.08.18 val PER: 0.1241
2026-01-08 14:39:06,499: t15.2023.08.20 val PER: 0.1215
2026-01-08 14:39:06,499: t15.2023.08.25 val PER: 0.1099
2026-01-08 14:39:06,499: t15.2023.08.27 val PER: 0.1945
2026-01-08 14:39:06,499: t15.2023.09.01 val PER: 0.0885
2026-01-08 14:39:06,499: t15.2023.09.03 val PER: 0.1639
2026-01-08 14:39:06,499: t15.2023.09.24 val PER: 0.1371
2026-01-08 14:39:06,500: t15.2023.09.29 val PER: 0.1423
2026-01-08 14:39:06,500: t15.2023.10.01 val PER: 0.1810
2026-01-08 14:39:06,500: t15.2023.10.06 val PER: 0.0926
2026-01-08 14:39:06,500: t15.2023.10.08 val PER: 0.2530
2026-01-08 14:39:06,500: t15.2023.10.13 val PER: 0.2164
2026-01-08 14:39:06,500: t15.2023.10.15 val PER: 0.1661
2026-01-08 14:39:06,500: t15.2023.10.20 val PER: 0.1846
2026-01-08 14:39:06,500: t15.2023.10.22 val PER: 0.1269
2026-01-08 14:39:06,500: t15.2023.11.03 val PER: 0.1818
2026-01-08 14:39:06,500: t15.2023.11.04 val PER: 0.0375
2026-01-08 14:39:06,500: t15.2023.11.17 val PER: 0.0482
2026-01-08 14:39:06,500: t15.2023.11.19 val PER: 0.0379
2026-01-08 14:39:06,500: t15.2023.11.26 val PER: 0.1507
2026-01-08 14:39:06,500: t15.2023.12.03 val PER: 0.1271
2026-01-08 14:39:06,501: t15.2023.12.08 val PER: 0.1165
2026-01-08 14:39:06,501: t15.2023.12.10 val PER: 0.1130
2026-01-08 14:39:06,501: t15.2023.12.17 val PER: 0.1538
2026-01-08 14:39:06,501: t15.2023.12.29 val PER: 0.1462
2026-01-08 14:39:06,501: t15.2024.02.25 val PER: 0.1180
2026-01-08 14:39:06,501: t15.2024.03.08 val PER: 0.2390
2026-01-08 14:39:06,501: t15.2024.03.15 val PER: 0.2076
2026-01-08 14:39:06,501: t15.2024.03.17 val PER: 0.1541
2026-01-08 14:39:06,501: t15.2024.05.10 val PER: 0.1679
2026-01-08 14:39:06,501: t15.2024.06.14 val PER: 0.1767
2026-01-08 14:39:06,501: t15.2024.07.19 val PER: 0.2518
2026-01-08 14:39:06,501: t15.2024.07.21 val PER: 0.1097
2026-01-08 14:39:06,501: t15.2024.07.28 val PER: 0.1544
2026-01-08 14:39:06,501: t15.2025.01.10 val PER: 0.3168
2026-01-08 14:39:06,501: t15.2025.01.12 val PER: 0.1686
2026-01-08 14:39:06,501: t15.2025.03.14 val PER: 0.3491
2026-01-08 14:39:06,501: t15.2025.03.16 val PER: 0.2134
2026-01-08 14:39:06,502: t15.2025.03.30 val PER: 0.3069
2026-01-08 14:39:06,502: t15.2025.04.13 val PER: 0.2140
2026-01-08 14:39:23,339: Train batch 12200: loss: 6.91 grad norm: 42.11 time: 0.065
2026-01-08 14:39:41,278: Train batch 12400: loss: 4.97 grad norm: 32.28 time: 0.042
2026-01-08 14:39:50,673: Running test after training batch: 12500
2026-01-08 14:39:50,817: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:39:55,948: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point as will
2026-01-08 14:39:55,980: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost sent
2026-01-08 14:39:57,757: Val batch 12500: PER (avg): 0.1603 CTC Loss (avg): 16.2847 WER(1gram): 47.97% (n=64) time: 7.084
2026-01-08 14:39:57,758: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-08 14:39:57,758: t15.2023.08.13 val PER: 0.1289
2026-01-08 14:39:57,758: t15.2023.08.18 val PER: 0.1215
2026-01-08 14:39:57,758: t15.2023.08.20 val PER: 0.1215
2026-01-08 14:39:57,758: t15.2023.08.25 val PER: 0.1024
2026-01-08 14:39:57,758: t15.2023.08.27 val PER: 0.2026
2026-01-08 14:39:57,758: t15.2023.09.01 val PER: 0.0917
2026-01-08 14:39:57,758: t15.2023.09.03 val PER: 0.1675
2026-01-08 14:39:57,758: t15.2023.09.24 val PER: 0.1274
2026-01-08 14:39:57,758: t15.2023.09.29 val PER: 0.1327
2026-01-08 14:39:57,758: t15.2023.10.01 val PER: 0.1704
2026-01-08 14:39:57,759: t15.2023.10.06 val PER: 0.0893
2026-01-08 14:39:57,759: t15.2023.10.08 val PER: 0.2463
2026-01-08 14:39:57,759: t15.2023.10.13 val PER: 0.2087
2026-01-08 14:39:57,759: t15.2023.10.15 val PER: 0.1694
2026-01-08 14:39:57,759: t15.2023.10.20 val PER: 0.1879
2026-01-08 14:39:57,759: t15.2023.10.22 val PER: 0.1281
2026-01-08 14:39:57,759: t15.2023.11.03 val PER: 0.1791
2026-01-08 14:39:57,759: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:39:57,759: t15.2023.11.17 val PER: 0.0451
2026-01-08 14:39:57,759: t15.2023.11.19 val PER: 0.0359
2026-01-08 14:39:57,759: t15.2023.11.26 val PER: 0.1428
2026-01-08 14:39:57,759: t15.2023.12.03 val PER: 0.1145
2026-01-08 14:39:57,759: t15.2023.12.08 val PER: 0.1059
2026-01-08 14:39:57,759: t15.2023.12.10 val PER: 0.0959
2026-01-08 14:39:57,760: t15.2023.12.17 val PER: 0.1528
2026-01-08 14:39:57,760: t15.2023.12.29 val PER: 0.1400
2026-01-08 14:39:57,760: t15.2024.02.25 val PER: 0.1152
2026-01-08 14:39:57,760: t15.2024.03.08 val PER: 0.2304
2026-01-08 14:39:57,760: t15.2024.03.15 val PER: 0.2095
2026-01-08 14:39:57,760: t15.2024.03.17 val PER: 0.1534
2026-01-08 14:39:57,760: t15.2024.05.10 val PER: 0.1649
2026-01-08 14:39:57,760: t15.2024.06.14 val PER: 0.1814
2026-01-08 14:39:57,760: t15.2024.07.19 val PER: 0.2604
2026-01-08 14:39:57,760: t15.2024.07.21 val PER: 0.1041
2026-01-08 14:39:57,760: t15.2024.07.28 val PER: 0.1382
2026-01-08 14:39:57,760: t15.2025.01.10 val PER: 0.3292
2026-01-08 14:39:57,760: t15.2025.01.12 val PER: 0.1501
2026-01-08 14:39:57,760: t15.2025.03.14 val PER: 0.3565
2026-01-08 14:39:57,760: t15.2025.03.16 val PER: 0.2134
2026-01-08 14:39:57,761: t15.2025.03.30 val PER: 0.3080
2026-01-08 14:39:57,761: t15.2025.04.13 val PER: 0.2225
2026-01-08 14:40:06,828: Train batch 12600: loss: 8.17 grad norm: 44.54 time: 0.059
2026-01-08 14:40:25,238: Train batch 12800: loss: 7.17 grad norm: 43.38 time: 0.052
2026-01-08 14:40:42,817: Train batch 13000: loss: 7.16 grad norm: 41.44 time: 0.065
2026-01-08 14:40:42,818: Running test after training batch: 13000
2026-01-08 14:40:42,936: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:40:47,654: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:40:47,686: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sindt
2026-01-08 14:40:49,481: Val batch 13000: PER (avg): 0.1598 CTC Loss (avg): 16.1571 WER(1gram): 46.70% (n=64) time: 6.664
2026-01-08 14:40:49,482: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-08 14:40:49,482: t15.2023.08.13 val PER: 0.1185
2026-01-08 14:40:49,482: t15.2023.08.18 val PER: 0.1232
2026-01-08 14:40:49,482: t15.2023.08.20 val PER: 0.1144
2026-01-08 14:40:49,482: t15.2023.08.25 val PER: 0.1039
2026-01-08 14:40:49,482: t15.2023.08.27 val PER: 0.1945
2026-01-08 14:40:49,482: t15.2023.09.01 val PER: 0.0893
2026-01-08 14:40:49,483: t15.2023.09.03 val PER: 0.1734
2026-01-08 14:40:49,483: t15.2023.09.24 val PER: 0.1262
2026-01-08 14:40:49,483: t15.2023.09.29 val PER: 0.1391
2026-01-08 14:40:49,483: t15.2023.10.01 val PER: 0.1797
2026-01-08 14:40:49,483: t15.2023.10.06 val PER: 0.0947
2026-01-08 14:40:49,483: t15.2023.10.08 val PER: 0.2530
2026-01-08 14:40:49,483: t15.2023.10.13 val PER: 0.2102
2026-01-08 14:40:49,483: t15.2023.10.15 val PER: 0.1582
2026-01-08 14:40:49,483: t15.2023.10.20 val PER: 0.1846
2026-01-08 14:40:49,483: t15.2023.10.22 val PER: 0.1281
2026-01-08 14:40:49,483: t15.2023.11.03 val PER: 0.1839
2026-01-08 14:40:49,483: t15.2023.11.04 val PER: 0.0307
2026-01-08 14:40:49,483: t15.2023.11.17 val PER: 0.0451
2026-01-08 14:40:49,483: t15.2023.11.19 val PER: 0.0319
2026-01-08 14:40:49,484: t15.2023.11.26 val PER: 0.1304
2026-01-08 14:40:49,484: t15.2023.12.03 val PER: 0.1271
2026-01-08 14:40:49,484: t15.2023.12.08 val PER: 0.1119
2026-01-08 14:40:49,484: t15.2023.12.10 val PER: 0.0986
2026-01-08 14:40:49,484: t15.2023.12.17 val PER: 0.1393
2026-01-08 14:40:49,484: t15.2023.12.29 val PER: 0.1455
2026-01-08 14:40:49,484: t15.2024.02.25 val PER: 0.1222
2026-01-08 14:40:49,484: t15.2024.03.08 val PER: 0.2504
2026-01-08 14:40:49,484: t15.2024.03.15 val PER: 0.2051
2026-01-08 14:40:49,484: t15.2024.03.17 val PER: 0.1520
2026-01-08 14:40:49,484: t15.2024.05.10 val PER: 0.1634
2026-01-08 14:40:49,484: t15.2024.06.14 val PER: 0.1767
2026-01-08 14:40:49,484: t15.2024.07.19 val PER: 0.2558
2026-01-08 14:40:49,484: t15.2024.07.21 val PER: 0.0959
2026-01-08 14:40:49,484: t15.2024.07.28 val PER: 0.1559
2026-01-08 14:40:49,484: t15.2025.01.10 val PER: 0.3072
2026-01-08 14:40:49,484: t15.2025.01.12 val PER: 0.1486
2026-01-08 14:40:49,485: t15.2025.03.14 val PER: 0.3580
2026-01-08 14:40:49,485: t15.2025.03.16 val PER: 0.2055
2026-01-08 14:40:49,485: t15.2025.03.30 val PER: 0.2897
2026-01-08 14:40:49,485: t15.2025.04.13 val PER: 0.2297
2026-01-08 14:40:49,486: New best val WER(1gram) 47.72% --> 46.70%
2026-01-08 14:40:49,486: Checkpointing model
2026-01-08 14:40:49,625: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:41:06,655: Train batch 13200: loss: 13.89 grad norm: 58.91 time: 0.055
2026-01-08 14:41:24,034: Train batch 13400: loss: 9.26 grad norm: 49.72 time: 0.063
2026-01-08 14:41:33,332: Running test after training batch: 13500
2026-01-08 14:41:33,436: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:41:38,616: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:41:38,647: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sent
2026-01-08 14:41:40,433: Val batch 13500: PER (avg): 0.1585 CTC Loss (avg): 16.0590 WER(1gram): 47.72% (n=64) time: 7.101
2026-01-08 14:41:40,433: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-08 14:41:40,433: t15.2023.08.13 val PER: 0.1195
2026-01-08 14:41:40,433: t15.2023.08.18 val PER: 0.1215
2026-01-08 14:41:40,433: t15.2023.08.20 val PER: 0.1144
2026-01-08 14:41:40,433: t15.2023.08.25 val PER: 0.1069
2026-01-08 14:41:40,433: t15.2023.08.27 val PER: 0.1994
2026-01-08 14:41:40,433: t15.2023.09.01 val PER: 0.0893
2026-01-08 14:41:40,433: t15.2023.09.03 val PER: 0.1615
2026-01-08 14:41:40,434: t15.2023.09.24 val PER: 0.1286
2026-01-08 14:41:40,434: t15.2023.09.29 val PER: 0.1353
2026-01-08 14:41:40,434: t15.2023.10.01 val PER: 0.1757
2026-01-08 14:41:40,434: t15.2023.10.06 val PER: 0.1001
2026-01-08 14:41:40,434: t15.2023.10.08 val PER: 0.2571
2026-01-08 14:41:40,434: t15.2023.10.13 val PER: 0.2087
2026-01-08 14:41:40,434: t15.2023.10.15 val PER: 0.1582
2026-01-08 14:41:40,434: t15.2023.10.20 val PER: 0.1678
2026-01-08 14:41:40,434: t15.2023.10.22 val PER: 0.1314
2026-01-08 14:41:40,434: t15.2023.11.03 val PER: 0.1920
2026-01-08 14:41:40,435: t15.2023.11.04 val PER: 0.0307
2026-01-08 14:41:40,435: t15.2023.11.17 val PER: 0.0420
2026-01-08 14:41:40,435: t15.2023.11.19 val PER: 0.0299
2026-01-08 14:41:40,435: t15.2023.11.26 val PER: 0.1290
2026-01-08 14:41:40,435: t15.2023.12.03 val PER: 0.1239
2026-01-08 14:41:40,435: t15.2023.12.08 val PER: 0.1119
2026-01-08 14:41:40,435: t15.2023.12.10 val PER: 0.0999
2026-01-08 14:41:40,435: t15.2023.12.17 val PER: 0.1362
2026-01-08 14:41:40,435: t15.2023.12.29 val PER: 0.1407
2026-01-08 14:41:40,435: t15.2024.02.25 val PER: 0.1208
2026-01-08 14:41:40,435: t15.2024.03.08 val PER: 0.2418
2026-01-08 14:41:40,435: t15.2024.03.15 val PER: 0.2033
2026-01-08 14:41:40,435: t15.2024.03.17 val PER: 0.1478
2026-01-08 14:41:40,435: t15.2024.05.10 val PER: 0.1456
2026-01-08 14:41:40,435: t15.2024.06.14 val PER: 0.1719
2026-01-08 14:41:40,435: t15.2024.07.19 val PER: 0.2452
2026-01-08 14:41:40,436: t15.2024.07.21 val PER: 0.1034
2026-01-08 14:41:40,436: t15.2024.07.28 val PER: 0.1441
2026-01-08 14:41:40,436: t15.2025.01.10 val PER: 0.3264
2026-01-08 14:41:40,436: t15.2025.01.12 val PER: 0.1455
2026-01-08 14:41:40,436: t15.2025.03.14 val PER: 0.3580
2026-01-08 14:41:40,436: t15.2025.03.16 val PER: 0.1963
2026-01-08 14:41:40,436: t15.2025.03.30 val PER: 0.3069
2026-01-08 14:41:40,436: t15.2025.04.13 val PER: 0.2254
2026-01-08 14:41:49,689: Train batch 13600: loss: 13.52 grad norm: 62.57 time: 0.064
2026-01-08 14:42:07,307: Train batch 13800: loss: 9.83 grad norm: 57.58 time: 0.056
2026-01-08 14:42:24,337: Train batch 14000: loss: 13.34 grad norm: 61.20 time: 0.051
2026-01-08 14:42:24,338: Running test after training batch: 14000
2026-01-08 14:42:24,454: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:42:29,236: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:42:29,268: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost sent
2026-01-08 14:42:31,073: Val batch 14000: PER (avg): 0.1575 CTC Loss (avg): 16.0130 WER(1gram): 46.70% (n=64) time: 6.735
2026-01-08 14:42:31,073: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-08 14:42:31,074: t15.2023.08.13 val PER: 0.1154
2026-01-08 14:42:31,074: t15.2023.08.18 val PER: 0.1182
2026-01-08 14:42:31,074: t15.2023.08.20 val PER: 0.1231
2026-01-08 14:42:31,074: t15.2023.08.25 val PER: 0.1084
2026-01-08 14:42:31,074: t15.2023.08.27 val PER: 0.1849
2026-01-08 14:42:31,074: t15.2023.09.01 val PER: 0.0869
2026-01-08 14:42:31,074: t15.2023.09.03 val PER: 0.1686
2026-01-08 14:42:31,074: t15.2023.09.24 val PER: 0.1323
2026-01-08 14:42:31,074: t15.2023.09.29 val PER: 0.1417
2026-01-08 14:42:31,074: t15.2023.10.01 val PER: 0.1764
2026-01-08 14:42:31,074: t15.2023.10.06 val PER: 0.0958
2026-01-08 14:42:31,074: t15.2023.10.08 val PER: 0.2409
2026-01-08 14:42:31,074: t15.2023.10.13 val PER: 0.2118
2026-01-08 14:42:31,074: t15.2023.10.15 val PER: 0.1595
2026-01-08 14:42:31,075: t15.2023.10.20 val PER: 0.1812
2026-01-08 14:42:31,075: t15.2023.10.22 val PER: 0.1236
2026-01-08 14:42:31,075: t15.2023.11.03 val PER: 0.1791
2026-01-08 14:42:31,075: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:42:31,075: t15.2023.11.17 val PER: 0.0467
2026-01-08 14:42:31,075: t15.2023.11.19 val PER: 0.0359
2026-01-08 14:42:31,075: t15.2023.11.26 val PER: 0.1348
2026-01-08 14:42:31,075: t15.2023.12.03 val PER: 0.1176
2026-01-08 14:42:31,075: t15.2023.12.08 val PER: 0.1032
2026-01-08 14:42:31,075: t15.2023.12.10 val PER: 0.1012
2026-01-08 14:42:31,075: t15.2023.12.17 val PER: 0.1403
2026-01-08 14:42:31,075: t15.2023.12.29 val PER: 0.1359
2026-01-08 14:42:31,075: t15.2024.02.25 val PER: 0.1236
2026-01-08 14:42:31,075: t15.2024.03.08 val PER: 0.2304
2026-01-08 14:42:31,076: t15.2024.03.15 val PER: 0.1989
2026-01-08 14:42:31,076: t15.2024.03.17 val PER: 0.1534
2026-01-08 14:42:31,076: t15.2024.05.10 val PER: 0.1560
2026-01-08 14:42:31,076: t15.2024.06.14 val PER: 0.1767
2026-01-08 14:42:31,076: t15.2024.07.19 val PER: 0.2459
2026-01-08 14:42:31,076: t15.2024.07.21 val PER: 0.1014
2026-01-08 14:42:31,076: t15.2024.07.28 val PER: 0.1449
2026-01-08 14:42:31,076: t15.2025.01.10 val PER: 0.3099
2026-01-08 14:42:31,076: t15.2025.01.12 val PER: 0.1501
2026-01-08 14:42:31,076: t15.2025.03.14 val PER: 0.3506
2026-01-08 14:42:31,076: t15.2025.03.16 val PER: 0.1963
2026-01-08 14:42:31,076: t15.2025.03.30 val PER: 0.3000
2026-01-08 14:42:31,076: t15.2025.04.13 val PER: 0.2225
2026-01-08 14:42:48,117: Train batch 14200: loss: 8.94 grad norm: 54.67 time: 0.056
2026-01-08 14:43:06,826: Train batch 14400: loss: 6.05 grad norm: 39.48 time: 0.067
2026-01-08 14:43:16,160: Running test after training batch: 14500
2026-01-08 14:43:16,266: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:43:21,044: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:43:21,076: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sent
2026-01-08 14:43:22,893: Val batch 14500: PER (avg): 0.1548 CTC Loss (avg): 15.8713 WER(1gram): 47.97% (n=64) time: 6.733
2026-01-08 14:43:22,894: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-08 14:43:22,894: t15.2023.08.13 val PER: 0.1175
2026-01-08 14:43:22,894: t15.2023.08.18 val PER: 0.1115
2026-01-08 14:43:22,894: t15.2023.08.20 val PER: 0.1152
2026-01-08 14:43:22,894: t15.2023.08.25 val PER: 0.1039
2026-01-08 14:43:22,894: t15.2023.08.27 val PER: 0.1977
2026-01-08 14:43:22,894: t15.2023.09.01 val PER: 0.0852
2026-01-08 14:43:22,894: t15.2023.09.03 val PER: 0.1627
2026-01-08 14:43:22,894: t15.2023.09.24 val PER: 0.1286
2026-01-08 14:43:22,894: t15.2023.09.29 val PER: 0.1366
2026-01-08 14:43:22,894: t15.2023.10.01 val PER: 0.1737
2026-01-08 14:43:22,894: t15.2023.10.06 val PER: 0.0861
2026-01-08 14:43:22,894: t15.2023.10.08 val PER: 0.2517
2026-01-08 14:43:22,895: t15.2023.10.13 val PER: 0.2040
2026-01-08 14:43:22,895: t15.2023.10.15 val PER: 0.1608
2026-01-08 14:43:22,895: t15.2023.10.20 val PER: 0.1644
2026-01-08 14:43:22,895: t15.2023.10.22 val PER: 0.1114
2026-01-08 14:43:22,895: t15.2023.11.03 val PER: 0.1757
2026-01-08 14:43:22,895: t15.2023.11.04 val PER: 0.0410
2026-01-08 14:43:22,895: t15.2023.11.17 val PER: 0.0404
2026-01-08 14:43:22,895: t15.2023.11.19 val PER: 0.0419
2026-01-08 14:43:22,896: t15.2023.11.26 val PER: 0.1239
2026-01-08 14:43:22,896: t15.2023.12.03 val PER: 0.1134
2026-01-08 14:43:22,896: t15.2023.12.08 val PER: 0.1012
2026-01-08 14:43:22,896: t15.2023.12.10 val PER: 0.0972
2026-01-08 14:43:22,896: t15.2023.12.17 val PER: 0.1362
2026-01-08 14:43:22,896: t15.2023.12.29 val PER: 0.1332
2026-01-08 14:43:22,896: t15.2024.02.25 val PER: 0.1067
2026-01-08 14:43:22,896: t15.2024.03.08 val PER: 0.2489
2026-01-08 14:43:22,896: t15.2024.03.15 val PER: 0.2020
2026-01-08 14:43:22,896: t15.2024.03.17 val PER: 0.1499
2026-01-08 14:43:22,896: t15.2024.05.10 val PER: 0.1530
2026-01-08 14:43:22,896: t15.2024.06.14 val PER: 0.1719
2026-01-08 14:43:22,896: t15.2024.07.19 val PER: 0.2531
2026-01-08 14:43:22,896: t15.2024.07.21 val PER: 0.1021
2026-01-08 14:43:22,896: t15.2024.07.28 val PER: 0.1449
2026-01-08 14:43:22,896: t15.2025.01.10 val PER: 0.3030
2026-01-08 14:43:22,896: t15.2025.01.12 val PER: 0.1416
2026-01-08 14:43:22,897: t15.2025.03.14 val PER: 0.3506
2026-01-08 14:43:22,897: t15.2025.03.16 val PER: 0.2003
2026-01-08 14:43:22,897: t15.2025.03.30 val PER: 0.2897
2026-01-08 14:43:22,897: t15.2025.04.13 val PER: 0.2211
2026-01-08 14:43:31,308: Train batch 14600: loss: 13.80 grad norm: 61.01 time: 0.059
2026-01-08 14:43:49,180: Train batch 14800: loss: 6.12 grad norm: 42.19 time: 0.051
2026-01-08 14:44:06,236: Train batch 15000: loss: 9.96 grad norm: 49.71 time: 0.052
2026-01-08 14:44:06,236: Running test after training batch: 15000
2026-01-08 14:44:06,353: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:44:11,235: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:44:11,267: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sent
2026-01-08 14:44:13,129: Val batch 15000: PER (avg): 0.1526 CTC Loss (avg): 15.6395 WER(1gram): 46.70% (n=64) time: 6.892
2026-01-08 14:44:13,129: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-08 14:44:13,129: t15.2023.08.13 val PER: 0.1185
2026-01-08 14:44:13,129: t15.2023.08.18 val PER: 0.1140
2026-01-08 14:44:13,129: t15.2023.08.20 val PER: 0.1096
2026-01-08 14:44:13,129: t15.2023.08.25 val PER: 0.1009
2026-01-08 14:44:13,129: t15.2023.08.27 val PER: 0.1881
2026-01-08 14:44:13,129: t15.2023.09.01 val PER: 0.0869
2026-01-08 14:44:13,130: t15.2023.09.03 val PER: 0.1627
2026-01-08 14:44:13,130: t15.2023.09.24 val PER: 0.1226
2026-01-08 14:44:13,130: t15.2023.09.29 val PER: 0.1398
2026-01-08 14:44:13,130: t15.2023.10.01 val PER: 0.1744
2026-01-08 14:44:13,130: t15.2023.10.06 val PER: 0.0904
2026-01-08 14:44:13,130: t15.2023.10.08 val PER: 0.2422
2026-01-08 14:44:13,130: t15.2023.10.13 val PER: 0.2033
2026-01-08 14:44:13,130: t15.2023.10.15 val PER: 0.1582
2026-01-08 14:44:13,130: t15.2023.10.20 val PER: 0.1812
2026-01-08 14:44:13,130: t15.2023.10.22 val PER: 0.1236
2026-01-08 14:44:13,130: t15.2023.11.03 val PER: 0.1723
2026-01-08 14:44:13,130: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:44:13,131: t15.2023.11.17 val PER: 0.0389
2026-01-08 14:44:13,131: t15.2023.11.19 val PER: 0.0359
2026-01-08 14:44:13,131: t15.2023.11.26 val PER: 0.1217
2026-01-08 14:44:13,131: t15.2023.12.03 val PER: 0.1155
2026-01-08 14:44:13,131: t15.2023.12.08 val PER: 0.1005
2026-01-08 14:44:13,131: t15.2023.12.10 val PER: 0.0894
2026-01-08 14:44:13,131: t15.2023.12.17 val PER: 0.1310
2026-01-08 14:44:13,131: t15.2023.12.29 val PER: 0.1366
2026-01-08 14:44:13,131: t15.2024.02.25 val PER: 0.1152
2026-01-08 14:44:13,131: t15.2024.03.08 val PER: 0.2248
2026-01-08 14:44:13,131: t15.2024.03.15 val PER: 0.1982
2026-01-08 14:44:13,131: t15.2024.03.17 val PER: 0.1381
2026-01-08 14:44:13,131: t15.2024.05.10 val PER: 0.1530
2026-01-08 14:44:13,131: t15.2024.06.14 val PER: 0.1719
2026-01-08 14:44:13,132: t15.2024.07.19 val PER: 0.2439
2026-01-08 14:44:13,132: t15.2024.07.21 val PER: 0.0945
2026-01-08 14:44:13,132: t15.2024.07.28 val PER: 0.1463
2026-01-08 14:44:13,132: t15.2025.01.10 val PER: 0.3154
2026-01-08 14:44:13,132: t15.2025.01.12 val PER: 0.1347
2026-01-08 14:44:13,132: t15.2025.03.14 val PER: 0.3462
2026-01-08 14:44:13,132: t15.2025.03.16 val PER: 0.1859
2026-01-08 14:44:13,132: t15.2025.03.30 val PER: 0.2839
2026-01-08 14:44:13,132: t15.2025.04.13 val PER: 0.2225
2026-01-08 14:44:31,482: Train batch 15200: loss: 5.27 grad norm: 40.31 time: 0.057
2026-01-08 14:44:48,659: Train batch 15400: loss: 11.81 grad norm: 56.05 time: 0.049
2026-01-08 14:44:57,599: Running test after training batch: 15500
2026-01-08 14:44:57,706: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:45:02,487: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:45:02,519: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sent
2026-01-08 14:45:04,347: Val batch 15500: PER (avg): 0.1526 CTC Loss (avg): 15.6617 WER(1gram): 45.43% (n=64) time: 6.748
2026-01-08 14:45:04,347: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-08 14:45:04,347: t15.2023.08.13 val PER: 0.1091
2026-01-08 14:45:04,347: t15.2023.08.18 val PER: 0.1123
2026-01-08 14:45:04,347: t15.2023.08.20 val PER: 0.1112
2026-01-08 14:45:04,347: t15.2023.08.25 val PER: 0.0934
2026-01-08 14:45:04,348: t15.2023.08.27 val PER: 0.1833
2026-01-08 14:45:04,348: t15.2023.09.01 val PER: 0.0828
2026-01-08 14:45:04,348: t15.2023.09.03 val PER: 0.1675
2026-01-08 14:45:04,348: t15.2023.09.24 val PER: 0.1262
2026-01-08 14:45:04,348: t15.2023.09.29 val PER: 0.1334
2026-01-08 14:45:04,348: t15.2023.10.01 val PER: 0.1717
2026-01-08 14:45:04,348: t15.2023.10.06 val PER: 0.0947
2026-01-08 14:45:04,349: t15.2023.10.08 val PER: 0.2449
2026-01-08 14:45:04,349: t15.2023.10.13 val PER: 0.2040
2026-01-08 14:45:04,349: t15.2023.10.15 val PER: 0.1477
2026-01-08 14:45:04,349: t15.2023.10.20 val PER: 0.1946
2026-01-08 14:45:04,349: t15.2023.10.22 val PER: 0.1281
2026-01-08 14:45:04,349: t15.2023.11.03 val PER: 0.1771
2026-01-08 14:45:04,349: t15.2023.11.04 val PER: 0.0375
2026-01-08 14:45:04,349: t15.2023.11.17 val PER: 0.0342
2026-01-08 14:45:04,349: t15.2023.11.19 val PER: 0.0379
2026-01-08 14:45:04,349: t15.2023.11.26 val PER: 0.1210
2026-01-08 14:45:04,349: t15.2023.12.03 val PER: 0.1071
2026-01-08 14:45:04,349: t15.2023.12.08 val PER: 0.0985
2026-01-08 14:45:04,350: t15.2023.12.10 val PER: 0.0933
2026-01-08 14:45:04,350: t15.2023.12.17 val PER: 0.1341
2026-01-08 14:45:04,350: t15.2023.12.29 val PER: 0.1332
2026-01-08 14:45:04,350: t15.2024.02.25 val PER: 0.1152
2026-01-08 14:45:04,350: t15.2024.03.08 val PER: 0.2276
2026-01-08 14:45:04,350: t15.2024.03.15 val PER: 0.2008
2026-01-08 14:45:04,350: t15.2024.03.17 val PER: 0.1513
2026-01-08 14:45:04,350: t15.2024.05.10 val PER: 0.1516
2026-01-08 14:45:04,350: t15.2024.06.14 val PER: 0.1735
2026-01-08 14:45:04,350: t15.2024.07.19 val PER: 0.2446
2026-01-08 14:45:04,350: t15.2024.07.21 val PER: 0.1000
2026-01-08 14:45:04,351: t15.2024.07.28 val PER: 0.1368
2026-01-08 14:45:04,351: t15.2025.01.10 val PER: 0.3030
2026-01-08 14:45:04,351: t15.2025.01.12 val PER: 0.1409
2026-01-08 14:45:04,351: t15.2025.03.14 val PER: 0.3565
2026-01-08 14:45:04,351: t15.2025.03.16 val PER: 0.1911
2026-01-08 14:45:04,351: t15.2025.03.30 val PER: 0.2943
2026-01-08 14:45:04,351: t15.2025.04.13 val PER: 0.2154
2026-01-08 14:45:04,352: New best val WER(1gram) 46.70% --> 45.43%
2026-01-08 14:45:04,352: Checkpointing model
2026-01-08 14:45:04,504: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/high_priority/speckle_p30/checkpoint/best_checkpoint
2026-01-08 14:45:13,600: Train batch 15600: loss: 12.74 grad norm: 54.86 time: 0.062
2026-01-08 14:45:32,273: Train batch 15800: loss: 14.08 grad norm: 62.20 time: 0.069
2026-01-08 14:45:51,064: Train batch 16000: loss: 9.49 grad norm: 45.63 time: 0.057
2026-01-08 14:45:51,064: Running test after training batch: 16000
2026-01-08 14:45:51,174: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:45:56,040: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:45:56,073: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sent
2026-01-08 14:45:57,957: Val batch 16000: PER (avg): 0.1535 CTC Loss (avg): 15.6254 WER(1gram): 46.70% (n=64) time: 6.893
2026-01-08 14:45:57,957: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=12
2026-01-08 14:45:57,957: t15.2023.08.13 val PER: 0.1164
2026-01-08 14:45:57,958: t15.2023.08.18 val PER: 0.1132
2026-01-08 14:45:57,958: t15.2023.08.20 val PER: 0.1112
2026-01-08 14:45:57,958: t15.2023.08.25 val PER: 0.0979
2026-01-08 14:45:57,958: t15.2023.08.27 val PER: 0.1881
2026-01-08 14:45:57,958: t15.2023.09.01 val PER: 0.0836
2026-01-08 14:45:57,958: t15.2023.09.03 val PER: 0.1544
2026-01-08 14:45:57,958: t15.2023.09.24 val PER: 0.1274
2026-01-08 14:45:57,958: t15.2023.09.29 val PER: 0.1398
2026-01-08 14:45:57,958: t15.2023.10.01 val PER: 0.1717
2026-01-08 14:45:57,958: t15.2023.10.06 val PER: 0.0969
2026-01-08 14:45:57,958: t15.2023.10.08 val PER: 0.2490
2026-01-08 14:45:57,958: t15.2023.10.13 val PER: 0.2064
2026-01-08 14:45:57,959: t15.2023.10.15 val PER: 0.1510
2026-01-08 14:45:57,959: t15.2023.10.20 val PER: 0.1812
2026-01-08 14:45:57,959: t15.2023.10.22 val PER: 0.1169
2026-01-08 14:45:57,959: t15.2023.11.03 val PER: 0.1777
2026-01-08 14:45:57,959: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:45:57,959: t15.2023.11.17 val PER: 0.0373
2026-01-08 14:45:57,959: t15.2023.11.19 val PER: 0.0339
2026-01-08 14:45:57,959: t15.2023.11.26 val PER: 0.1196
2026-01-08 14:45:57,959: t15.2023.12.03 val PER: 0.1092
2026-01-08 14:45:57,959: t15.2023.12.08 val PER: 0.0965
2026-01-08 14:45:57,959: t15.2023.12.10 val PER: 0.0946
2026-01-08 14:45:57,959: t15.2023.12.17 val PER: 0.1414
2026-01-08 14:45:57,959: t15.2023.12.29 val PER: 0.1304
2026-01-08 14:45:57,959: t15.2024.02.25 val PER: 0.1166
2026-01-08 14:45:57,959: t15.2024.03.08 val PER: 0.2390
2026-01-08 14:45:57,960: t15.2024.03.15 val PER: 0.1989
2026-01-08 14:45:57,960: t15.2024.03.17 val PER: 0.1409
2026-01-08 14:45:57,960: t15.2024.05.10 val PER: 0.1575
2026-01-08 14:45:57,960: t15.2024.06.14 val PER: 0.1782
2026-01-08 14:45:57,960: t15.2024.07.19 val PER: 0.2544
2026-01-08 14:45:57,960: t15.2024.07.21 val PER: 0.0979
2026-01-08 14:45:57,960: t15.2024.07.28 val PER: 0.1412
2026-01-08 14:45:57,960: t15.2025.01.10 val PER: 0.3099
2026-01-08 14:45:57,960: t15.2025.01.12 val PER: 0.1478
2026-01-08 14:45:57,960: t15.2025.03.14 val PER: 0.3402
2026-01-08 14:45:57,960: t15.2025.03.16 val PER: 0.1924
2026-01-08 14:45:57,960: t15.2025.03.30 val PER: 0.2908
2026-01-08 14:45:57,960: t15.2025.04.13 val PER: 0.2225
2026-01-08 14:46:15,247: Train batch 16200: loss: 6.86 grad norm: 42.27 time: 0.056
2026-01-08 14:46:32,601: Train batch 16400: loss: 12.13 grad norm: 62.45 time: 0.058
2026-01-08 14:46:41,342: Running test after training batch: 16500
2026-01-08 14:46:41,472: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:46:46,301: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:46:46,340: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sent
2026-01-08 14:46:48,491: Val batch 16500: PER (avg): 0.1522 CTC Loss (avg): 15.6058 WER(1gram): 46.45% (n=64) time: 7.149
2026-01-08 14:46:48,492: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-08 14:46:48,492: t15.2023.08.13 val PER: 0.1081
2026-01-08 14:46:48,492: t15.2023.08.18 val PER: 0.1090
2026-01-08 14:46:48,492: t15.2023.08.20 val PER: 0.1128
2026-01-08 14:46:48,492: t15.2023.08.25 val PER: 0.0919
2026-01-08 14:46:48,493: t15.2023.08.27 val PER: 0.1977
2026-01-08 14:46:48,493: t15.2023.09.01 val PER: 0.0795
2026-01-08 14:46:48,493: t15.2023.09.03 val PER: 0.1591
2026-01-08 14:46:48,493: t15.2023.09.24 val PER: 0.1238
2026-01-08 14:46:48,493: t15.2023.09.29 val PER: 0.1410
2026-01-08 14:46:48,493: t15.2023.10.01 val PER: 0.1711
2026-01-08 14:46:48,493: t15.2023.10.06 val PER: 0.0980
2026-01-08 14:46:48,493: t15.2023.10.08 val PER: 0.2382
2026-01-08 14:46:48,493: t15.2023.10.13 val PER: 0.2056
2026-01-08 14:46:48,493: t15.2023.10.15 val PER: 0.1569
2026-01-08 14:46:48,493: t15.2023.10.20 val PER: 0.1711
2026-01-08 14:46:48,494: t15.2023.10.22 val PER: 0.1125
2026-01-08 14:46:48,494: t15.2023.11.03 val PER: 0.1777
2026-01-08 14:46:48,494: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:46:48,494: t15.2023.11.17 val PER: 0.0327
2026-01-08 14:46:48,494: t15.2023.11.19 val PER: 0.0319
2026-01-08 14:46:48,494: t15.2023.11.26 val PER: 0.1210
2026-01-08 14:46:48,494: t15.2023.12.03 val PER: 0.1092
2026-01-08 14:46:48,494: t15.2023.12.08 val PER: 0.1005
2026-01-08 14:46:48,494: t15.2023.12.10 val PER: 0.0972
2026-01-08 14:46:48,494: t15.2023.12.17 val PER: 0.1372
2026-01-08 14:46:48,495: t15.2023.12.29 val PER: 0.1338
2026-01-08 14:46:48,495: t15.2024.02.25 val PER: 0.1166
2026-01-08 14:46:48,495: t15.2024.03.08 val PER: 0.2404
2026-01-08 14:46:48,495: t15.2024.03.15 val PER: 0.2020
2026-01-08 14:46:48,495: t15.2024.03.17 val PER: 0.1450
2026-01-08 14:46:48,495: t15.2024.05.10 val PER: 0.1530
2026-01-08 14:46:48,495: t15.2024.06.14 val PER: 0.1625
2026-01-08 14:46:48,495: t15.2024.07.19 val PER: 0.2472
2026-01-08 14:46:48,495: t15.2024.07.21 val PER: 0.0986
2026-01-08 14:46:48,495: t15.2024.07.28 val PER: 0.1338
2026-01-08 14:46:48,495: t15.2025.01.10 val PER: 0.2975
2026-01-08 14:46:48,495: t15.2025.01.12 val PER: 0.1424
2026-01-08 14:46:48,495: t15.2025.03.14 val PER: 0.3476
2026-01-08 14:46:48,495: t15.2025.03.16 val PER: 0.1898
2026-01-08 14:46:48,496: t15.2025.03.30 val PER: 0.2885
2026-01-08 14:46:48,496: t15.2025.04.13 val PER: 0.2168
2026-01-08 14:46:57,387: Train batch 16600: loss: 9.31 grad norm: 53.32 time: 0.053
2026-01-08 14:47:14,287: Train batch 16800: loss: 17.21 grad norm: 67.87 time: 0.062
2026-01-08 14:47:31,277: Train batch 17000: loss: 9.01 grad norm: 46.88 time: 0.082
2026-01-08 14:47:31,277: Running test after training batch: 17000
2026-01-08 14:47:31,378: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:47:36,106: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:47:36,140: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sent
2026-01-08 14:47:37,971: Val batch 17000: PER (avg): 0.1510 CTC Loss (avg): 15.4755 WER(1gram): 47.21% (n=64) time: 6.693
2026-01-08 14:47:37,971: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-08 14:47:37,971: t15.2023.08.13 val PER: 0.1081
2026-01-08 14:47:37,971: t15.2023.08.18 val PER: 0.1199
2026-01-08 14:47:37,971: t15.2023.08.20 val PER: 0.1072
2026-01-08 14:47:37,972: t15.2023.08.25 val PER: 0.0979
2026-01-08 14:47:37,972: t15.2023.08.27 val PER: 0.1994
2026-01-08 14:47:37,972: t15.2023.09.01 val PER: 0.0828
2026-01-08 14:47:37,972: t15.2023.09.03 val PER: 0.1556
2026-01-08 14:47:37,972: t15.2023.09.24 val PER: 0.1226
2026-01-08 14:47:37,972: t15.2023.09.29 val PER: 0.1359
2026-01-08 14:47:37,972: t15.2023.10.01 val PER: 0.1658
2026-01-08 14:47:37,972: t15.2023.10.06 val PER: 0.0904
2026-01-08 14:47:37,972: t15.2023.10.08 val PER: 0.2395
2026-01-08 14:47:37,972: t15.2023.10.13 val PER: 0.2017
2026-01-08 14:47:37,973: t15.2023.10.15 val PER: 0.1549
2026-01-08 14:47:37,973: t15.2023.10.20 val PER: 0.1711
2026-01-08 14:47:37,973: t15.2023.10.22 val PER: 0.1169
2026-01-08 14:47:37,973: t15.2023.11.03 val PER: 0.1777
2026-01-08 14:47:37,973: t15.2023.11.04 val PER: 0.0307
2026-01-08 14:47:37,973: t15.2023.11.17 val PER: 0.0389
2026-01-08 14:47:37,973: t15.2023.11.19 val PER: 0.0319
2026-01-08 14:47:37,973: t15.2023.11.26 val PER: 0.1188
2026-01-08 14:47:37,973: t15.2023.12.03 val PER: 0.1061
2026-01-08 14:47:37,973: t15.2023.12.08 val PER: 0.0972
2026-01-08 14:47:37,973: t15.2023.12.10 val PER: 0.0841
2026-01-08 14:47:37,973: t15.2023.12.17 val PER: 0.1362
2026-01-08 14:47:37,974: t15.2023.12.29 val PER: 0.1325
2026-01-08 14:47:37,974: t15.2024.02.25 val PER: 0.1081
2026-01-08 14:47:37,974: t15.2024.03.08 val PER: 0.2376
2026-01-08 14:47:37,974: t15.2024.03.15 val PER: 0.1976
2026-01-08 14:47:37,974: t15.2024.03.17 val PER: 0.1444
2026-01-08 14:47:37,974: t15.2024.05.10 val PER: 0.1560
2026-01-08 14:47:37,974: t15.2024.06.14 val PER: 0.1593
2026-01-08 14:47:37,974: t15.2024.07.19 val PER: 0.2505
2026-01-08 14:47:37,974: t15.2024.07.21 val PER: 0.0945
2026-01-08 14:47:37,974: t15.2024.07.28 val PER: 0.1382
2026-01-08 14:47:37,974: t15.2025.01.10 val PER: 0.3017
2026-01-08 14:47:37,974: t15.2025.01.12 val PER: 0.1332
2026-01-08 14:47:37,974: t15.2025.03.14 val PER: 0.3476
2026-01-08 14:47:37,974: t15.2025.03.16 val PER: 0.1924
2026-01-08 14:47:37,974: t15.2025.03.30 val PER: 0.2851
2026-01-08 14:47:37,975: t15.2025.04.13 val PER: 0.2268
2026-01-08 14:47:55,106: Train batch 17200: loss: 10.53 grad norm: 52.85 time: 0.084
2026-01-08 14:48:12,403: Train batch 17400: loss: 13.62 grad norm: 62.33 time: 0.071
2026-01-08 14:48:20,874: Running test after training batch: 17500
2026-01-08 14:48:21,007: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:48:25,886: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:48:25,918: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sent
2026-01-08 14:48:27,832: Val batch 17500: PER (avg): 0.1498 CTC Loss (avg): 15.4387 WER(1gram): 47.21% (n=64) time: 6.957
2026-01-08 14:48:27,832: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-08 14:48:27,832: t15.2023.08.13 val PER: 0.1081
2026-01-08 14:48:27,833: t15.2023.08.18 val PER: 0.1115
2026-01-08 14:48:27,833: t15.2023.08.20 val PER: 0.1072
2026-01-08 14:48:27,833: t15.2023.08.25 val PER: 0.0979
2026-01-08 14:48:27,833: t15.2023.08.27 val PER: 0.1929
2026-01-08 14:48:27,833: t15.2023.09.01 val PER: 0.0779
2026-01-08 14:48:27,833: t15.2023.09.03 val PER: 0.1556
2026-01-08 14:48:27,833: t15.2023.09.24 val PER: 0.1201
2026-01-08 14:48:27,833: t15.2023.09.29 val PER: 0.1372
2026-01-08 14:48:27,833: t15.2023.10.01 val PER: 0.1731
2026-01-08 14:48:27,833: t15.2023.10.06 val PER: 0.0936
2026-01-08 14:48:27,833: t15.2023.10.08 val PER: 0.2382
2026-01-08 14:48:27,833: t15.2023.10.13 val PER: 0.2002
2026-01-08 14:48:27,833: t15.2023.10.15 val PER: 0.1543
2026-01-08 14:48:27,834: t15.2023.10.20 val PER: 0.1779
2026-01-08 14:48:27,834: t15.2023.10.22 val PER: 0.1180
2026-01-08 14:48:27,834: t15.2023.11.03 val PER: 0.1723
2026-01-08 14:48:27,834: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:48:27,834: t15.2023.11.17 val PER: 0.0327
2026-01-08 14:48:27,834: t15.2023.11.19 val PER: 0.0299
2026-01-08 14:48:27,834: t15.2023.11.26 val PER: 0.1123
2026-01-08 14:48:27,834: t15.2023.12.03 val PER: 0.1071
2026-01-08 14:48:27,834: t15.2023.12.08 val PER: 0.0972
2026-01-08 14:48:27,834: t15.2023.12.10 val PER: 0.0933
2026-01-08 14:48:27,834: t15.2023.12.17 val PER: 0.1289
2026-01-08 14:48:27,835: t15.2023.12.29 val PER: 0.1277
2026-01-08 14:48:27,835: t15.2024.02.25 val PER: 0.1096
2026-01-08 14:48:27,835: t15.2024.03.08 val PER: 0.2333
2026-01-08 14:48:27,835: t15.2024.03.15 val PER: 0.1970
2026-01-08 14:48:27,835: t15.2024.03.17 val PER: 0.1367
2026-01-08 14:48:27,835: t15.2024.05.10 val PER: 0.1516
2026-01-08 14:48:27,835: t15.2024.06.14 val PER: 0.1609
2026-01-08 14:48:27,835: t15.2024.07.19 val PER: 0.2479
2026-01-08 14:48:27,835: t15.2024.07.21 val PER: 0.0938
2026-01-08 14:48:27,835: t15.2024.07.28 val PER: 0.1353
2026-01-08 14:48:27,835: t15.2025.01.10 val PER: 0.3030
2026-01-08 14:48:27,835: t15.2025.01.12 val PER: 0.1378
2026-01-08 14:48:27,835: t15.2025.03.14 val PER: 0.3462
2026-01-08 14:48:27,835: t15.2025.03.16 val PER: 0.1859
2026-01-08 14:48:27,836: t15.2025.03.30 val PER: 0.2920
2026-01-08 14:48:27,836: t15.2025.04.13 val PER: 0.2282
2026-01-08 14:48:36,294: Train batch 17600: loss: 10.83 grad norm: 52.48 time: 0.052
2026-01-08 14:48:53,822: Train batch 17800: loss: 6.69 grad norm: 50.59 time: 0.043
2026-01-08 14:49:11,203: Train batch 18000: loss: 11.67 grad norm: 58.06 time: 0.061
2026-01-08 14:49:11,203: Running test after training batch: 18000
2026-01-08 14:49:11,316: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:49:16,075: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:49:16,108: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sent
2026-01-08 14:49:18,233: Val batch 18000: PER (avg): 0.1487 CTC Loss (avg): 15.4136 WER(1gram): 45.69% (n=64) time: 7.030
2026-01-08 14:49:18,233: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=12
2026-01-08 14:49:18,234: t15.2023.08.13 val PER: 0.1091
2026-01-08 14:49:18,234: t15.2023.08.18 val PER: 0.1106
2026-01-08 14:49:18,234: t15.2023.08.20 val PER: 0.1064
2026-01-08 14:49:18,234: t15.2023.08.25 val PER: 0.0934
2026-01-08 14:49:18,234: t15.2023.08.27 val PER: 0.1961
2026-01-08 14:49:18,234: t15.2023.09.01 val PER: 0.0820
2026-01-08 14:49:18,234: t15.2023.09.03 val PER: 0.1544
2026-01-08 14:49:18,234: t15.2023.09.24 val PER: 0.1214
2026-01-08 14:49:18,234: t15.2023.09.29 val PER: 0.1372
2026-01-08 14:49:18,234: t15.2023.10.01 val PER: 0.1671
2026-01-08 14:49:18,234: t15.2023.10.06 val PER: 0.0936
2026-01-08 14:49:18,234: t15.2023.10.08 val PER: 0.2382
2026-01-08 14:49:18,234: t15.2023.10.13 val PER: 0.2040
2026-01-08 14:49:18,234: t15.2023.10.15 val PER: 0.1543
2026-01-08 14:49:18,234: t15.2023.10.20 val PER: 0.1745
2026-01-08 14:49:18,235: t15.2023.10.22 val PER: 0.1169
2026-01-08 14:49:18,235: t15.2023.11.03 val PER: 0.1696
2026-01-08 14:49:18,235: t15.2023.11.04 val PER: 0.0307
2026-01-08 14:49:18,235: t15.2023.11.17 val PER: 0.0358
2026-01-08 14:49:18,235: t15.2023.11.19 val PER: 0.0299
2026-01-08 14:49:18,235: t15.2023.11.26 val PER: 0.1116
2026-01-08 14:49:18,235: t15.2023.12.03 val PER: 0.1082
2026-01-08 14:49:18,235: t15.2023.12.08 val PER: 0.0965
2026-01-08 14:49:18,235: t15.2023.12.10 val PER: 0.0880
2026-01-08 14:49:18,235: t15.2023.12.17 val PER: 0.1320
2026-01-08 14:49:18,235: t15.2023.12.29 val PER: 0.1263
2026-01-08 14:49:18,235: t15.2024.02.25 val PER: 0.1011
2026-01-08 14:49:18,235: t15.2024.03.08 val PER: 0.2233
2026-01-08 14:49:18,235: t15.2024.03.15 val PER: 0.1945
2026-01-08 14:49:18,236: t15.2024.03.17 val PER: 0.1388
2026-01-08 14:49:18,236: t15.2024.05.10 val PER: 0.1575
2026-01-08 14:49:18,236: t15.2024.06.14 val PER: 0.1625
2026-01-08 14:49:18,236: t15.2024.07.19 val PER: 0.2485
2026-01-08 14:49:18,236: t15.2024.07.21 val PER: 0.0897
2026-01-08 14:49:18,236: t15.2024.07.28 val PER: 0.1390
2026-01-08 14:49:18,236: t15.2025.01.10 val PER: 0.2989
2026-01-08 14:49:18,236: t15.2025.01.12 val PER: 0.1363
2026-01-08 14:49:18,236: t15.2025.03.14 val PER: 0.3432
2026-01-08 14:49:18,236: t15.2025.03.16 val PER: 0.1832
2026-01-08 14:49:18,236: t15.2025.03.30 val PER: 0.2805
2026-01-08 14:49:18,236: t15.2025.04.13 val PER: 0.2197
2026-01-08 14:49:35,728: Train batch 18200: loss: 8.45 grad norm: 48.76 time: 0.074
2026-01-08 14:49:52,757: Train batch 18400: loss: 5.73 grad norm: 45.55 time: 0.058
2026-01-08 14:50:01,151: Running test after training batch: 18500
2026-01-08 14:50:01,284: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:50:06,105: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:50:06,138: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sent
2026-01-08 14:50:08,050: Val batch 18500: PER (avg): 0.1496 CTC Loss (avg): 15.4694 WER(1gram): 46.95% (n=64) time: 6.899
2026-01-08 14:50:08,050: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=12
2026-01-08 14:50:08,051: t15.2023.08.13 val PER: 0.1102
2026-01-08 14:50:08,051: t15.2023.08.18 val PER: 0.1132
2026-01-08 14:50:08,051: t15.2023.08.20 val PER: 0.1104
2026-01-08 14:50:08,051: t15.2023.08.25 val PER: 0.0964
2026-01-08 14:50:08,051: t15.2023.08.27 val PER: 0.1881
2026-01-08 14:50:08,051: t15.2023.09.01 val PER: 0.0747
2026-01-08 14:50:08,051: t15.2023.09.03 val PER: 0.1615
2026-01-08 14:50:08,051: t15.2023.09.24 val PER: 0.1201
2026-01-08 14:50:08,051: t15.2023.09.29 val PER: 0.1391
2026-01-08 14:50:08,051: t15.2023.10.01 val PER: 0.1691
2026-01-08 14:50:08,051: t15.2023.10.06 val PER: 0.0926
2026-01-08 14:50:08,051: t15.2023.10.08 val PER: 0.2395
2026-01-08 14:50:08,052: t15.2023.10.13 val PER: 0.2040
2026-01-08 14:50:08,052: t15.2023.10.15 val PER: 0.1608
2026-01-08 14:50:08,052: t15.2023.10.20 val PER: 0.1678
2026-01-08 14:50:08,052: t15.2023.10.22 val PER: 0.1136
2026-01-08 14:50:08,052: t15.2023.11.03 val PER: 0.1737
2026-01-08 14:50:08,052: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:50:08,052: t15.2023.11.17 val PER: 0.0327
2026-01-08 14:50:08,052: t15.2023.11.19 val PER: 0.0339
2026-01-08 14:50:08,052: t15.2023.11.26 val PER: 0.1159
2026-01-08 14:50:08,052: t15.2023.12.03 val PER: 0.1050
2026-01-08 14:50:08,052: t15.2023.12.08 val PER: 0.0985
2026-01-08 14:50:08,052: t15.2023.12.10 val PER: 0.0854
2026-01-08 14:50:08,052: t15.2023.12.17 val PER: 0.1362
2026-01-08 14:50:08,052: t15.2023.12.29 val PER: 0.1283
2026-01-08 14:50:08,053: t15.2024.02.25 val PER: 0.1110
2026-01-08 14:50:08,053: t15.2024.03.08 val PER: 0.2304
2026-01-08 14:50:08,053: t15.2024.03.15 val PER: 0.1982
2026-01-08 14:50:08,053: t15.2024.03.17 val PER: 0.1409
2026-01-08 14:50:08,053: t15.2024.05.10 val PER: 0.1545
2026-01-08 14:50:08,053: t15.2024.06.14 val PER: 0.1593
2026-01-08 14:50:08,053: t15.2024.07.19 val PER: 0.2413
2026-01-08 14:50:08,053: t15.2024.07.21 val PER: 0.0903
2026-01-08 14:50:08,053: t15.2024.07.28 val PER: 0.1382
2026-01-08 14:50:08,053: t15.2025.01.10 val PER: 0.2975
2026-01-08 14:50:08,053: t15.2025.01.12 val PER: 0.1339
2026-01-08 14:50:08,053: t15.2025.03.14 val PER: 0.3402
2026-01-08 14:50:08,053: t15.2025.03.16 val PER: 0.1872
2026-01-08 14:50:08,053: t15.2025.03.30 val PER: 0.2839
2026-01-08 14:50:08,054: t15.2025.04.13 val PER: 0.2197
2026-01-08 14:50:16,576: Train batch 18600: loss: 14.29 grad norm: 61.00 time: 0.069
2026-01-08 14:50:34,102: Train batch 18800: loss: 9.18 grad norm: 51.83 time: 0.066
2026-01-08 14:50:51,821: Train batch 19000: loss: 9.33 grad norm: 46.41 time: 0.064
2026-01-08 14:50:51,822: Running test after training batch: 19000
2026-01-08 14:50:51,924: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:50:56,673: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:50:56,705: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sent
2026-01-08 14:50:58,621: Val batch 19000: PER (avg): 0.1491 CTC Loss (avg): 15.4458 WER(1gram): 45.43% (n=64) time: 6.799
2026-01-08 14:50:58,622: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=12
2026-01-08 14:50:58,622: t15.2023.08.13 val PER: 0.1060
2026-01-08 14:50:58,622: t15.2023.08.18 val PER: 0.1090
2026-01-08 14:50:58,622: t15.2023.08.20 val PER: 0.1080
2026-01-08 14:50:58,622: t15.2023.08.25 val PER: 0.0979
2026-01-08 14:50:58,622: t15.2023.08.27 val PER: 0.1833
2026-01-08 14:50:58,622: t15.2023.09.01 val PER: 0.0755
2026-01-08 14:50:58,622: t15.2023.09.03 val PER: 0.1568
2026-01-08 14:50:58,622: t15.2023.09.24 val PER: 0.1201
2026-01-08 14:50:58,622: t15.2023.09.29 val PER: 0.1398
2026-01-08 14:50:58,622: t15.2023.10.01 val PER: 0.1731
2026-01-08 14:50:58,623: t15.2023.10.06 val PER: 0.0926
2026-01-08 14:50:58,623: t15.2023.10.08 val PER: 0.2395
2026-01-08 14:50:58,623: t15.2023.10.13 val PER: 0.2025
2026-01-08 14:50:58,623: t15.2023.10.15 val PER: 0.1608
2026-01-08 14:50:58,623: t15.2023.10.20 val PER: 0.1711
2026-01-08 14:50:58,623: t15.2023.10.22 val PER: 0.1169
2026-01-08 14:50:58,623: t15.2023.11.03 val PER: 0.1716
2026-01-08 14:50:58,624: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:50:58,624: t15.2023.11.17 val PER: 0.0295
2026-01-08 14:50:58,624: t15.2023.11.19 val PER: 0.0359
2026-01-08 14:50:58,624: t15.2023.11.26 val PER: 0.1152
2026-01-08 14:50:58,624: t15.2023.12.03 val PER: 0.1050
2026-01-08 14:50:58,624: t15.2023.12.08 val PER: 0.0945
2026-01-08 14:50:58,624: t15.2023.12.10 val PER: 0.0867
2026-01-08 14:50:58,624: t15.2023.12.17 val PER: 0.1383
2026-01-08 14:50:58,624: t15.2023.12.29 val PER: 0.1256
2026-01-08 14:50:58,624: t15.2024.02.25 val PER: 0.1152
2026-01-08 14:50:58,624: t15.2024.03.08 val PER: 0.2233
2026-01-08 14:50:58,624: t15.2024.03.15 val PER: 0.1976
2026-01-08 14:50:58,624: t15.2024.03.17 val PER: 0.1430
2026-01-08 14:50:58,624: t15.2024.05.10 val PER: 0.1530
2026-01-08 14:50:58,624: t15.2024.06.14 val PER: 0.1593
2026-01-08 14:50:58,625: t15.2024.07.19 val PER: 0.2373
2026-01-08 14:50:58,625: t15.2024.07.21 val PER: 0.0924
2026-01-08 14:50:58,625: t15.2024.07.28 val PER: 0.1368
2026-01-08 14:50:58,625: t15.2025.01.10 val PER: 0.3030
2026-01-08 14:50:58,625: t15.2025.01.12 val PER: 0.1355
2026-01-08 14:50:58,625: t15.2025.03.14 val PER: 0.3417
2026-01-08 14:50:58,625: t15.2025.03.16 val PER: 0.1924
2026-01-08 14:50:58,625: t15.2025.03.30 val PER: 0.2759
2026-01-08 14:50:58,625: t15.2025.04.13 val PER: 0.2183
2026-01-08 14:51:16,217: Train batch 19200: loss: 6.80 grad norm: 47.49 time: 0.065
2026-01-08 14:51:33,660: Train batch 19400: loss: 5.57 grad norm: 40.65 time: 0.055
2026-01-08 14:51:42,236: Running test after training batch: 19500
2026-01-08 14:51:42,339: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:51:47,081: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:51:47,116: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sent
2026-01-08 14:51:49,282: Val batch 19500: PER (avg): 0.1484 CTC Loss (avg): 15.3694 WER(1gram): 45.69% (n=64) time: 7.045
2026-01-08 14:51:49,282: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=12
2026-01-08 14:51:49,282: t15.2023.08.13 val PER: 0.1102
2026-01-08 14:51:49,282: t15.2023.08.18 val PER: 0.1115
2026-01-08 14:51:49,282: t15.2023.08.20 val PER: 0.1080
2026-01-08 14:51:49,282: t15.2023.08.25 val PER: 0.0949
2026-01-08 14:51:49,283: t15.2023.08.27 val PER: 0.1849
2026-01-08 14:51:49,283: t15.2023.09.01 val PER: 0.0747
2026-01-08 14:51:49,283: t15.2023.09.03 val PER: 0.1544
2026-01-08 14:51:49,283: t15.2023.09.24 val PER: 0.1165
2026-01-08 14:51:49,283: t15.2023.09.29 val PER: 0.1359
2026-01-08 14:51:49,283: t15.2023.10.01 val PER: 0.1704
2026-01-08 14:51:49,283: t15.2023.10.06 val PER: 0.0904
2026-01-08 14:51:49,283: t15.2023.10.08 val PER: 0.2436
2026-01-08 14:51:49,283: t15.2023.10.13 val PER: 0.2017
2026-01-08 14:51:49,284: t15.2023.10.15 val PER: 0.1569
2026-01-08 14:51:49,284: t15.2023.10.20 val PER: 0.1711
2026-01-08 14:51:49,284: t15.2023.10.22 val PER: 0.1147
2026-01-08 14:51:49,284: t15.2023.11.03 val PER: 0.1784
2026-01-08 14:51:49,284: t15.2023.11.04 val PER: 0.0375
2026-01-08 14:51:49,284: t15.2023.11.17 val PER: 0.0295
2026-01-08 14:51:49,284: t15.2023.11.19 val PER: 0.0319
2026-01-08 14:51:49,284: t15.2023.11.26 val PER: 0.1123
2026-01-08 14:51:49,284: t15.2023.12.03 val PER: 0.0998
2026-01-08 14:51:49,284: t15.2023.12.08 val PER: 0.0939
2026-01-08 14:51:49,284: t15.2023.12.10 val PER: 0.0867
2026-01-08 14:51:49,284: t15.2023.12.17 val PER: 0.1341
2026-01-08 14:51:49,284: t15.2023.12.29 val PER: 0.1290
2026-01-08 14:51:49,284: t15.2024.02.25 val PER: 0.1152
2026-01-08 14:51:49,285: t15.2024.03.08 val PER: 0.2304
2026-01-08 14:51:49,285: t15.2024.03.15 val PER: 0.1901
2026-01-08 14:51:49,285: t15.2024.03.17 val PER: 0.1450
2026-01-08 14:51:49,285: t15.2024.05.10 val PER: 0.1545
2026-01-08 14:51:49,285: t15.2024.06.14 val PER: 0.1577
2026-01-08 14:51:49,285: t15.2024.07.19 val PER: 0.2393
2026-01-08 14:51:49,285: t15.2024.07.21 val PER: 0.0869
2026-01-08 14:51:49,285: t15.2024.07.28 val PER: 0.1382
2026-01-08 14:51:49,285: t15.2025.01.10 val PER: 0.2948
2026-01-08 14:51:49,285: t15.2025.01.12 val PER: 0.1339
2026-01-08 14:51:49,285: t15.2025.03.14 val PER: 0.3432
2026-01-08 14:51:49,285: t15.2025.03.16 val PER: 0.1898
2026-01-08 14:51:49,285: t15.2025.03.30 val PER: 0.2782
2026-01-08 14:51:49,285: t15.2025.04.13 val PER: 0.2240
2026-01-08 14:51:57,616: Train batch 19600: loss: 8.48 grad norm: 49.50 time: 0.058
2026-01-08 14:52:14,887: Train batch 19800: loss: 7.95 grad norm: 49.29 time: 0.056
2026-01-08 14:52:32,514: Running test after training batch: 19999
2026-01-08 14:52:32,608: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:52:37,588: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-08 14:52:37,630: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sent
2026-01-08 14:52:39,885: Val batch 19999: PER (avg): 0.1480 CTC Loss (avg): 15.2980 WER(1gram): 45.69% (n=64) time: 7.371
2026-01-08 14:52:39,886: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=12
2026-01-08 14:52:39,886: t15.2023.08.13 val PER: 0.1081
2026-01-08 14:52:39,886: t15.2023.08.18 val PER: 0.1123
2026-01-08 14:52:39,886: t15.2023.08.20 val PER: 0.1088
2026-01-08 14:52:39,886: t15.2023.08.25 val PER: 0.0934
2026-01-08 14:52:39,886: t15.2023.08.27 val PER: 0.1849
2026-01-08 14:52:39,886: t15.2023.09.01 val PER: 0.0795
2026-01-08 14:52:39,886: t15.2023.09.03 val PER: 0.1496
2026-01-08 14:52:39,887: t15.2023.09.24 val PER: 0.1201
2026-01-08 14:52:39,887: t15.2023.09.29 val PER: 0.1366
2026-01-08 14:52:39,887: t15.2023.10.01 val PER: 0.1671
2026-01-08 14:52:39,887: t15.2023.10.06 val PER: 0.0915
2026-01-08 14:52:39,887: t15.2023.10.08 val PER: 0.2422
2026-01-08 14:52:39,887: t15.2023.10.13 val PER: 0.2002
2026-01-08 14:52:39,887: t15.2023.10.15 val PER: 0.1562
2026-01-08 14:52:39,887: t15.2023.10.20 val PER: 0.1711
2026-01-08 14:52:39,887: t15.2023.10.22 val PER: 0.1158
2026-01-08 14:52:39,887: t15.2023.11.03 val PER: 0.1737
2026-01-08 14:52:39,887: t15.2023.11.04 val PER: 0.0307
2026-01-08 14:52:39,887: t15.2023.11.17 val PER: 0.0327
2026-01-08 14:52:39,887: t15.2023.11.19 val PER: 0.0279
2026-01-08 14:52:39,887: t15.2023.11.26 val PER: 0.1130
2026-01-08 14:52:39,887: t15.2023.12.03 val PER: 0.1050
2026-01-08 14:52:39,887: t15.2023.12.08 val PER: 0.0965
2026-01-08 14:52:39,888: t15.2023.12.10 val PER: 0.0894
2026-01-08 14:52:39,888: t15.2023.12.17 val PER: 0.1351
2026-01-08 14:52:39,888: t15.2023.12.29 val PER: 0.1242
2026-01-08 14:52:39,888: t15.2024.02.25 val PER: 0.1110
2026-01-08 14:52:39,888: t15.2024.03.08 val PER: 0.2290
2026-01-08 14:52:39,888: t15.2024.03.15 val PER: 0.1901
2026-01-08 14:52:39,888: t15.2024.03.17 val PER: 0.1395
2026-01-08 14:52:39,888: t15.2024.05.10 val PER: 0.1501
2026-01-08 14:52:39,888: t15.2024.06.14 val PER: 0.1640
2026-01-08 14:52:39,888: t15.2024.07.19 val PER: 0.2419
2026-01-08 14:52:39,888: t15.2024.07.21 val PER: 0.0931
2026-01-08 14:52:39,888: t15.2024.07.28 val PER: 0.1360
2026-01-08 14:52:39,888: t15.2025.01.10 val PER: 0.2961
2026-01-08 14:52:39,888: t15.2025.01.12 val PER: 0.1355
2026-01-08 14:52:39,888: t15.2025.03.14 val PER: 0.3343
2026-01-08 14:52:39,888: t15.2025.03.16 val PER: 0.1846
2026-01-08 14:52:39,889: t15.2025.03.30 val PER: 0.2828
2026-01-08 14:52:39,889: t15.2025.04.13 val PER: 0.2168
2026-01-08 14:52:39,919: Best avg val PER achieved: 0.15257
2026-01-08 14:52:39,919: Total training time: 34.47 minutes
All runs finished. Outputs in: /home/e12511253/Brain2Text/brain2text/trained_models
