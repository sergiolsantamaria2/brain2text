TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_348482
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_348482/torch_extensions
WANDB_DIR=/tmp/e12511253_b2t_348482/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/e12511253_b2t_348482/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  3 23:25 /tmp/e12511253_b2t_348482/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
trained_models -> /tmp/e12511253_b2t_348482/trained_models
OUT_ROOT=/tmp/e12511253_b2t_348482/trained_models
==============================================
Job: b2t_exp  ID: 348482
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/gru/rnn_dropout/lr40
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/gru/rnn_dropout/lr40 ==========
Num configs: 4

=== RUN d05.yaml ===
2026-01-03 23:25:14,003: Using device: cuda:0
2026-01-03 23:25:15,861: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-03 23:25:15,885: Using 45 sessions after filtering (from 45).
2026-01-03 23:25:16,317: Using torch.compile (if available)
2026-01-03 23:25:16,317: torch.compile not available (torch<2.0). Skipping.
2026-01-03 23:25:16,318: Initialized RNN decoding model
2026-01-03 23:25:16,318: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.05)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 23:25:16,318: Model has 44,907,305 parameters
2026-01-03 23:25:16,318: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 23:25:17,592: Successfully initialized datasets
2026-01-03 23:25:17,592: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 23:25:18,652: Train batch 0: loss: 573.63 grad norm: 1487.74 time: 0.157
2026-01-03 23:25:18,652: Running test after training batch: 0
2026-01-03 23:25:18,762: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:25:24,299: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-03 23:25:25,013: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-03 23:25:58,430: Val batch 0: PER (avg): 1.4296 CTC Loss (avg): 633.1692 WER(1gram): 100.00% (n=64) time: 39.778
2026-01-03 23:25:58,431: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-03 23:25:58,431: t15.2023.08.13 val PER: 1.3077
2026-01-03 23:25:58,431: t15.2023.08.18 val PER: 1.4216
2026-01-03 23:25:58,431: t15.2023.08.20 val PER: 1.2979
2026-01-03 23:25:58,431: t15.2023.08.25 val PER: 1.3358
2026-01-03 23:25:58,431: t15.2023.08.27 val PER: 1.2540
2026-01-03 23:25:58,431: t15.2023.09.01 val PER: 1.4529
2026-01-03 23:25:58,431: t15.2023.09.03 val PER: 1.3183
2026-01-03 23:25:58,431: t15.2023.09.24 val PER: 1.5437
2026-01-03 23:25:58,431: t15.2023.09.29 val PER: 1.4722
2026-01-03 23:25:58,431: t15.2023.10.01 val PER: 1.2114
2026-01-03 23:25:58,432: t15.2023.10.06 val PER: 1.4962
2026-01-03 23:25:58,432: t15.2023.10.08 val PER: 1.1813
2026-01-03 23:25:58,432: t15.2023.10.13 val PER: 1.3926
2026-01-03 23:25:58,432: t15.2023.10.15 val PER: 1.3883
2026-01-03 23:25:58,432: t15.2023.10.20 val PER: 1.4966
2026-01-03 23:25:58,432: t15.2023.10.22 val PER: 1.3942
2026-01-03 23:25:58,432: t15.2023.11.03 val PER: 1.5950
2026-01-03 23:25:58,432: t15.2023.11.04 val PER: 2.0239
2026-01-03 23:25:58,432: t15.2023.11.17 val PER: 1.9393
2026-01-03 23:25:58,432: t15.2023.11.19 val PER: 1.6786
2026-01-03 23:25:58,432: t15.2023.11.26 val PER: 1.5449
2026-01-03 23:25:58,432: t15.2023.12.03 val PER: 1.4286
2026-01-03 23:25:58,432: t15.2023.12.08 val PER: 1.4514
2026-01-03 23:25:58,432: t15.2023.12.10 val PER: 1.7043
2026-01-03 23:25:58,433: t15.2023.12.17 val PER: 1.2983
2026-01-03 23:25:58,433: t15.2023.12.29 val PER: 1.4077
2026-01-03 23:25:58,433: t15.2024.02.25 val PER: 1.4270
2026-01-03 23:25:58,433: t15.2024.03.08 val PER: 1.3215
2026-01-03 23:25:58,433: t15.2024.03.15 val PER: 1.3208
2026-01-03 23:25:58,433: t15.2024.03.17 val PER: 1.3982
2026-01-03 23:25:58,433: t15.2024.05.10 val PER: 1.3269
2026-01-03 23:25:58,433: t15.2024.06.14 val PER: 1.5205
2026-01-03 23:25:58,433: t15.2024.07.19 val PER: 1.0824
2026-01-03 23:25:58,433: t15.2024.07.21 val PER: 1.6359
2026-01-03 23:25:58,433: t15.2024.07.28 val PER: 1.6596
2026-01-03 23:25:58,433: t15.2025.01.10 val PER: 1.0895
2026-01-03 23:25:58,433: t15.2025.01.12 val PER: 1.7590
2026-01-03 23:25:58,433: t15.2025.03.14 val PER: 1.0355
2026-01-03 23:25:58,433: t15.2025.03.16 val PER: 1.6178
2026-01-03 23:25:58,433: t15.2025.03.30 val PER: 1.2897
2026-01-03 23:25:58,433: t15.2025.04.13 val PER: 1.5991
2026-01-03 23:25:58,435: New best val WER(1gram) inf% --> 100.00%
2026-01-03 23:25:58,435: Checkpointing model
2026-01-03 23:25:58,669: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/best_checkpoint
2026-01-03 23:25:58,910: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_0
2026-01-03 23:26:15,967: Train batch 200: loss: 76.28 grad norm: 114.75 time: 0.054
2026-01-03 23:26:32,838: Train batch 400: loss: 52.99 grad norm: 102.76 time: 0.063
2026-01-03 23:26:41,653: Running test after training batch: 500
2026-01-03 23:26:41,785: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:26:46,618: WER debug example
  GT : you can see the code at this point as well
  PR : used and ease thus uhde at this ide is aisle
2026-01-03 23:26:46,652: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus ass add
2026-01-03 23:26:49,000: Val batch 500: PER (avg): 0.5160 CTC Loss (avg): 54.7305 WER(1gram): 88.83% (n=64) time: 7.346
2026-01-03 23:26:49,000: WER lens: avg_true_words=6.16 avg_pred_words=5.45 max_pred_words=11
2026-01-03 23:26:49,000: t15.2023.08.13 val PER: 0.4605
2026-01-03 23:26:49,000: t15.2023.08.18 val PER: 0.4468
2026-01-03 23:26:49,001: t15.2023.08.20 val PER: 0.4329
2026-01-03 23:26:49,001: t15.2023.08.25 val PER: 0.4307
2026-01-03 23:26:49,001: t15.2023.08.27 val PER: 0.5402
2026-01-03 23:26:49,001: t15.2023.09.01 val PER: 0.4148
2026-01-03 23:26:49,001: t15.2023.09.03 val PER: 0.5000
2026-01-03 23:26:49,001: t15.2023.09.24 val PER: 0.4150
2026-01-03 23:26:49,001: t15.2023.09.29 val PER: 0.4665
2026-01-03 23:26:49,001: t15.2023.10.01 val PER: 0.5152
2026-01-03 23:26:49,001: t15.2023.10.06 val PER: 0.4144
2026-01-03 23:26:49,002: t15.2023.10.08 val PER: 0.5277
2026-01-03 23:26:49,002: t15.2023.10.13 val PER: 0.5842
2026-01-03 23:26:49,002: t15.2023.10.15 val PER: 0.4951
2026-01-03 23:26:49,002: t15.2023.10.20 val PER: 0.4430
2026-01-03 23:26:49,002: t15.2023.10.22 val PER: 0.4365
2026-01-03 23:26:49,002: t15.2023.11.03 val PER: 0.4959
2026-01-03 23:26:49,002: t15.2023.11.04 val PER: 0.2355
2026-01-03 23:26:49,002: t15.2023.11.17 val PER: 0.3561
2026-01-03 23:26:49,002: t15.2023.11.19 val PER: 0.3174
2026-01-03 23:26:49,003: t15.2023.11.26 val PER: 0.5428
2026-01-03 23:26:49,003: t15.2023.12.03 val PER: 0.5032
2026-01-03 23:26:49,003: t15.2023.12.08 val PER: 0.5206
2026-01-03 23:26:49,003: t15.2023.12.10 val PER: 0.4415
2026-01-03 23:26:49,003: t15.2023.12.17 val PER: 0.5520
2026-01-03 23:26:49,003: t15.2023.12.29 val PER: 0.5429
2026-01-03 23:26:49,003: t15.2024.02.25 val PER: 0.4677
2026-01-03 23:26:49,003: t15.2024.03.08 val PER: 0.6230
2026-01-03 23:26:49,003: t15.2024.03.15 val PER: 0.5572
2026-01-03 23:26:49,003: t15.2024.03.17 val PER: 0.5091
2026-01-03 23:26:49,004: t15.2024.05.10 val PER: 0.5468
2026-01-03 23:26:49,004: t15.2024.06.14 val PER: 0.5095
2026-01-03 23:26:49,004: t15.2024.07.19 val PER: 0.6638
2026-01-03 23:26:49,004: t15.2024.07.21 val PER: 0.4690
2026-01-03 23:26:49,004: t15.2024.07.28 val PER: 0.5051
2026-01-03 23:26:49,004: t15.2025.01.10 val PER: 0.7590
2026-01-03 23:26:49,004: t15.2025.01.12 val PER: 0.5758
2026-01-03 23:26:49,004: t15.2025.03.14 val PER: 0.7396
2026-01-03 23:26:49,004: t15.2025.03.16 val PER: 0.5969
2026-01-03 23:26:49,004: t15.2025.03.30 val PER: 0.7425
2026-01-03 23:26:49,004: t15.2025.04.13 val PER: 0.5877
2026-01-03 23:26:49,005: New best val WER(1gram) 100.00% --> 88.83%
2026-01-03 23:26:49,005: Checkpointing model
2026-01-03 23:26:49,596: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/best_checkpoint
2026-01-03 23:26:49,840: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_500
2026-01-03 23:26:58,482: Train batch 600: loss: 48.73 grad norm: 85.50 time: 0.077
2026-01-03 23:27:15,742: Train batch 800: loss: 39.91 grad norm: 89.08 time: 0.057
2026-01-03 23:27:33,302: Train batch 1000: loss: 41.06 grad norm: 76.18 time: 0.066
2026-01-03 23:27:33,302: Running test after training batch: 1000
2026-01-03 23:27:33,419: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:27:38,507: WER debug example
  GT : you can see the code at this point as well
  PR : yule end ease thus owed it this uhde is will
2026-01-03 23:27:38,541: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus wass
2026-01-03 23:27:40,418: Val batch 1000: PER (avg): 0.4034 CTC Loss (avg): 41.4613 WER(1gram): 81.22% (n=64) time: 7.115
2026-01-03 23:27:40,418: WER lens: avg_true_words=6.16 avg_pred_words=5.47 max_pred_words=12
2026-01-03 23:27:40,418: t15.2023.08.13 val PER: 0.3721
2026-01-03 23:27:40,418: t15.2023.08.18 val PER: 0.3403
2026-01-03 23:27:40,419: t15.2023.08.20 val PER: 0.3344
2026-01-03 23:27:40,419: t15.2023.08.25 val PER: 0.2892
2026-01-03 23:27:40,419: t15.2023.08.27 val PER: 0.4196
2026-01-03 23:27:40,419: t15.2023.09.01 val PER: 0.2995
2026-01-03 23:27:40,419: t15.2023.09.03 val PER: 0.3979
2026-01-03 23:27:40,419: t15.2023.09.24 val PER: 0.3204
2026-01-03 23:27:40,419: t15.2023.09.29 val PER: 0.3542
2026-01-03 23:27:40,420: t15.2023.10.01 val PER: 0.4003
2026-01-03 23:27:40,420: t15.2023.10.06 val PER: 0.3079
2026-01-03 23:27:40,420: t15.2023.10.08 val PER: 0.4601
2026-01-03 23:27:40,420: t15.2023.10.13 val PER: 0.4647
2026-01-03 23:27:40,420: t15.2023.10.15 val PER: 0.3691
2026-01-03 23:27:40,420: t15.2023.10.20 val PER: 0.3456
2026-01-03 23:27:40,420: t15.2023.10.22 val PER: 0.3419
2026-01-03 23:27:40,420: t15.2023.11.03 val PER: 0.3894
2026-01-03 23:27:40,420: t15.2023.11.04 val PER: 0.1433
2026-01-03 23:27:40,421: t15.2023.11.17 val PER: 0.2473
2026-01-03 23:27:40,421: t15.2023.11.19 val PER: 0.2176
2026-01-03 23:27:40,421: t15.2023.11.26 val PER: 0.4362
2026-01-03 23:27:40,421: t15.2023.12.03 val PER: 0.4002
2026-01-03 23:27:40,421: t15.2023.12.08 val PER: 0.3995
2026-01-03 23:27:40,421: t15.2023.12.10 val PER: 0.3495
2026-01-03 23:27:40,421: t15.2023.12.17 val PER: 0.4033
2026-01-03 23:27:40,421: t15.2023.12.29 val PER: 0.4187
2026-01-03 23:27:40,421: t15.2024.02.25 val PER: 0.3399
2026-01-03 23:27:40,421: t15.2024.03.08 val PER: 0.4979
2026-01-03 23:27:40,422: t15.2024.03.15 val PER: 0.4290
2026-01-03 23:27:40,422: t15.2024.03.17 val PER: 0.4079
2026-01-03 23:27:40,422: t15.2024.05.10 val PER: 0.4175
2026-01-03 23:27:40,422: t15.2024.06.14 val PER: 0.3975
2026-01-03 23:27:40,422: t15.2024.07.19 val PER: 0.5214
2026-01-03 23:27:40,422: t15.2024.07.21 val PER: 0.3724
2026-01-03 23:27:40,422: t15.2024.07.28 val PER: 0.4088
2026-01-03 23:27:40,422: t15.2025.01.10 val PER: 0.6116
2026-01-03 23:27:40,422: t15.2025.01.12 val PER: 0.4457
2026-01-03 23:27:40,423: t15.2025.03.14 val PER: 0.6317
2026-01-03 23:27:40,423: t15.2025.03.16 val PER: 0.4751
2026-01-03 23:27:40,423: t15.2025.03.30 val PER: 0.6540
2026-01-03 23:27:40,423: t15.2025.04.13 val PER: 0.4864
2026-01-03 23:27:40,423: New best val WER(1gram) 88.83% --> 81.22%
2026-01-03 23:27:40,423: Checkpointing model
2026-01-03 23:27:40,995: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/best_checkpoint
2026-01-03 23:27:41,238: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_1000
2026-01-03 23:27:58,479: Train batch 1200: loss: 31.69 grad norm: 73.44 time: 0.068
2026-01-03 23:28:15,903: Train batch 1400: loss: 34.76 grad norm: 81.74 time: 0.061
2026-01-03 23:28:24,425: Running test after training batch: 1500
2026-01-03 23:28:24,526: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:28:29,357: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt sze the owed it this boyde is wheel
2026-01-03 23:28:29,388: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap that os
2026-01-03 23:28:31,019: Val batch 1500: PER (avg): 0.3749 CTC Loss (avg): 36.7000 WER(1gram): 74.87% (n=64) time: 6.594
2026-01-03 23:28:31,020: WER lens: avg_true_words=6.16 avg_pred_words=5.05 max_pred_words=11
2026-01-03 23:28:31,020: t15.2023.08.13 val PER: 0.3337
2026-01-03 23:28:31,020: t15.2023.08.18 val PER: 0.3101
2026-01-03 23:28:31,020: t15.2023.08.20 val PER: 0.2994
2026-01-03 23:28:31,020: t15.2023.08.25 val PER: 0.2590
2026-01-03 23:28:31,020: t15.2023.08.27 val PER: 0.4051
2026-01-03 23:28:31,020: t15.2023.09.01 val PER: 0.2727
2026-01-03 23:28:31,020: t15.2023.09.03 val PER: 0.3753
2026-01-03 23:28:31,020: t15.2023.09.24 val PER: 0.3095
2026-01-03 23:28:31,020: t15.2023.09.29 val PER: 0.3267
2026-01-03 23:28:31,020: t15.2023.10.01 val PER: 0.3851
2026-01-03 23:28:31,020: t15.2023.10.06 val PER: 0.2896
2026-01-03 23:28:31,020: t15.2023.10.08 val PER: 0.4222
2026-01-03 23:28:31,021: t15.2023.10.13 val PER: 0.4422
2026-01-03 23:28:31,021: t15.2023.10.15 val PER: 0.3527
2026-01-03 23:28:31,021: t15.2023.10.20 val PER: 0.3255
2026-01-03 23:28:31,021: t15.2023.10.22 val PER: 0.3051
2026-01-03 23:28:31,021: t15.2023.11.03 val PER: 0.3602
2026-01-03 23:28:31,021: t15.2023.11.04 val PER: 0.1092
2026-01-03 23:28:31,021: t15.2023.11.17 val PER: 0.2131
2026-01-03 23:28:31,021: t15.2023.11.19 val PER: 0.1737
2026-01-03 23:28:31,021: t15.2023.11.26 val PER: 0.4152
2026-01-03 23:28:31,021: t15.2023.12.03 val PER: 0.3571
2026-01-03 23:28:31,021: t15.2023.12.08 val PER: 0.3489
2026-01-03 23:28:31,021: t15.2023.12.10 val PER: 0.3101
2026-01-03 23:28:31,021: t15.2023.12.17 val PER: 0.3701
2026-01-03 23:28:31,021: t15.2023.12.29 val PER: 0.3706
2026-01-03 23:28:31,021: t15.2024.02.25 val PER: 0.3076
2026-01-03 23:28:31,022: t15.2024.03.08 val PER: 0.4595
2026-01-03 23:28:31,022: t15.2024.03.15 val PER: 0.4053
2026-01-03 23:28:31,022: t15.2024.03.17 val PER: 0.3731
2026-01-03 23:28:31,022: t15.2024.05.10 val PER: 0.3744
2026-01-03 23:28:31,022: t15.2024.06.14 val PER: 0.3896
2026-01-03 23:28:31,022: t15.2024.07.19 val PER: 0.5162
2026-01-03 23:28:31,022: t15.2024.07.21 val PER: 0.3428
2026-01-03 23:28:31,022: t15.2024.07.28 val PER: 0.3662
2026-01-03 23:28:31,022: t15.2025.01.10 val PER: 0.5992
2026-01-03 23:28:31,022: t15.2025.01.12 val PER: 0.4257
2026-01-03 23:28:31,022: t15.2025.03.14 val PER: 0.6006
2026-01-03 23:28:31,022: t15.2025.03.16 val PER: 0.4516
2026-01-03 23:28:31,022: t15.2025.03.30 val PER: 0.6230
2026-01-03 23:28:31,022: t15.2025.04.13 val PER: 0.4665
2026-01-03 23:28:31,023: New best val WER(1gram) 81.22% --> 74.87%
2026-01-03 23:28:31,023: Checkpointing model
2026-01-03 23:28:31,621: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/best_checkpoint
2026-01-03 23:28:31,863: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_1500
2026-01-03 23:28:40,301: Train batch 1600: loss: 35.82 grad norm: 79.29 time: 0.064
2026-01-03 23:28:57,781: Train batch 1800: loss: 34.19 grad norm: 77.24 time: 0.088
2026-01-03 23:29:15,181: Train batch 2000: loss: 32.51 grad norm: 77.33 time: 0.067
2026-01-03 23:29:15,181: Running test after training batch: 2000
2026-01-03 23:29:15,309: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:29:20,050: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code hatt this boyde is will
2026-01-03 23:29:20,082: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap thus cost it
2026-01-03 23:29:21,720: Val batch 2000: PER (avg): 0.3213 CTC Loss (avg): 32.3474 WER(1gram): 70.30% (n=64) time: 6.538
2026-01-03 23:29:21,721: WER lens: avg_true_words=6.16 avg_pred_words=5.72 max_pred_words=12
2026-01-03 23:29:21,721: t15.2023.08.13 val PER: 0.2827
2026-01-03 23:29:21,721: t15.2023.08.18 val PER: 0.2607
2026-01-03 23:29:21,721: t15.2023.08.20 val PER: 0.2383
2026-01-03 23:29:21,721: t15.2023.08.25 val PER: 0.2304
2026-01-03 23:29:21,721: t15.2023.08.27 val PER: 0.3312
2026-01-03 23:29:21,721: t15.2023.09.01 val PER: 0.2183
2026-01-03 23:29:21,721: t15.2023.09.03 val PER: 0.3337
2026-01-03 23:29:21,721: t15.2023.09.24 val PER: 0.2573
2026-01-03 23:29:21,721: t15.2023.09.29 val PER: 0.2655
2026-01-03 23:29:21,721: t15.2023.10.01 val PER: 0.3223
2026-01-03 23:29:21,721: t15.2023.10.06 val PER: 0.2217
2026-01-03 23:29:21,722: t15.2023.10.08 val PER: 0.3762
2026-01-03 23:29:21,722: t15.2023.10.13 val PER: 0.3708
2026-01-03 23:29:21,722: t15.2023.10.15 val PER: 0.2914
2026-01-03 23:29:21,722: t15.2023.10.20 val PER: 0.2852
2026-01-03 23:29:21,722: t15.2023.10.22 val PER: 0.2483
2026-01-03 23:29:21,722: t15.2023.11.03 val PER: 0.3229
2026-01-03 23:29:21,722: t15.2023.11.04 val PER: 0.0819
2026-01-03 23:29:21,722: t15.2023.11.17 val PER: 0.1617
2026-01-03 23:29:21,722: t15.2023.11.19 val PER: 0.1257
2026-01-03 23:29:21,722: t15.2023.11.26 val PER: 0.3688
2026-01-03 23:29:21,722: t15.2023.12.03 val PER: 0.3004
2026-01-03 23:29:21,722: t15.2023.12.08 val PER: 0.2983
2026-01-03 23:29:21,723: t15.2023.12.10 val PER: 0.2641
2026-01-03 23:29:21,723: t15.2023.12.17 val PER: 0.3150
2026-01-03 23:29:21,723: t15.2023.12.29 val PER: 0.3267
2026-01-03 23:29:21,723: t15.2024.02.25 val PER: 0.2711
2026-01-03 23:29:21,723: t15.2024.03.08 val PER: 0.3869
2026-01-03 23:29:21,723: t15.2024.03.15 val PER: 0.3533
2026-01-03 23:29:21,723: t15.2024.03.17 val PER: 0.3229
2026-01-03 23:29:21,723: t15.2024.05.10 val PER: 0.3388
2026-01-03 23:29:21,723: t15.2024.06.14 val PER: 0.3454
2026-01-03 23:29:21,723: t15.2024.07.19 val PER: 0.4529
2026-01-03 23:29:21,723: t15.2024.07.21 val PER: 0.2917
2026-01-03 23:29:21,723: t15.2024.07.28 val PER: 0.3206
2026-01-03 23:29:21,723: t15.2025.01.10 val PER: 0.5207
2026-01-03 23:29:21,723: t15.2025.01.12 val PER: 0.3895
2026-01-03 23:29:21,723: t15.2025.03.14 val PER: 0.5340
2026-01-03 23:29:21,723: t15.2025.03.16 val PER: 0.3927
2026-01-03 23:29:21,723: t15.2025.03.30 val PER: 0.5276
2026-01-03 23:29:21,724: t15.2025.04.13 val PER: 0.4251
2026-01-03 23:29:21,724: New best val WER(1gram) 74.87% --> 70.30%
2026-01-03 23:29:21,724: Checkpointing model
2026-01-03 23:29:22,301: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/best_checkpoint
2026-01-03 23:29:22,544: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_2000
2026-01-03 23:29:39,558: Train batch 2200: loss: 28.74 grad norm: 73.38 time: 0.061
2026-01-03 23:29:56,817: Train batch 2400: loss: 28.22 grad norm: 65.90 time: 0.052
2026-01-03 23:30:05,683: Running test after training batch: 2500
2026-01-03 23:30:05,825: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:30:11,050: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-03 23:30:11,080: WER debug example
  GT : how does it keep the cost down
  PR : houde does it keep the cost it
2026-01-03 23:30:12,804: Val batch 2500: PER (avg): 0.2985 CTC Loss (avg): 29.9655 WER(1gram): 66.50% (n=64) time: 7.120
2026-01-03 23:30:12,804: WER lens: avg_true_words=6.16 avg_pred_words=5.58 max_pred_words=11
2026-01-03 23:30:12,804: t15.2023.08.13 val PER: 0.2879
2026-01-03 23:30:12,804: t15.2023.08.18 val PER: 0.2347
2026-01-03 23:30:12,805: t15.2023.08.20 val PER: 0.2327
2026-01-03 23:30:12,805: t15.2023.08.25 val PER: 0.2033
2026-01-03 23:30:12,805: t15.2023.08.27 val PER: 0.3087
2026-01-03 23:30:12,805: t15.2023.09.01 val PER: 0.1956
2026-01-03 23:30:12,805: t15.2023.09.03 val PER: 0.2755
2026-01-03 23:30:12,805: t15.2023.09.24 val PER: 0.2184
2026-01-03 23:30:12,805: t15.2023.09.29 val PER: 0.2451
2026-01-03 23:30:12,805: t15.2023.10.01 val PER: 0.3038
2026-01-03 23:30:12,805: t15.2023.10.06 val PER: 0.1981
2026-01-03 23:30:12,805: t15.2023.10.08 val PER: 0.3599
2026-01-03 23:30:12,805: t15.2023.10.13 val PER: 0.3530
2026-01-03 23:30:12,805: t15.2023.10.15 val PER: 0.2821
2026-01-03 23:30:12,805: t15.2023.10.20 val PER: 0.2617
2026-01-03 23:30:12,806: t15.2023.10.22 val PER: 0.2249
2026-01-03 23:30:12,806: t15.2023.11.03 val PER: 0.2910
2026-01-03 23:30:12,806: t15.2023.11.04 val PER: 0.0922
2026-01-03 23:30:12,806: t15.2023.11.17 val PER: 0.1540
2026-01-03 23:30:12,806: t15.2023.11.19 val PER: 0.1178
2026-01-03 23:30:12,806: t15.2023.11.26 val PER: 0.3377
2026-01-03 23:30:12,806: t15.2023.12.03 val PER: 0.2815
2026-01-03 23:30:12,806: t15.2023.12.08 val PER: 0.2750
2026-01-03 23:30:12,806: t15.2023.12.10 val PER: 0.2208
2026-01-03 23:30:12,806: t15.2023.12.17 val PER: 0.2807
2026-01-03 23:30:12,806: t15.2023.12.29 val PER: 0.2979
2026-01-03 23:30:12,806: t15.2024.02.25 val PER: 0.2303
2026-01-03 23:30:12,806: t15.2024.03.08 val PER: 0.3556
2026-01-03 23:30:12,806: t15.2024.03.15 val PER: 0.3565
2026-01-03 23:30:12,806: t15.2024.03.17 val PER: 0.3047
2026-01-03 23:30:12,806: t15.2024.05.10 val PER: 0.3076
2026-01-03 23:30:12,806: t15.2024.06.14 val PER: 0.3107
2026-01-03 23:30:12,807: t15.2024.07.19 val PER: 0.4417
2026-01-03 23:30:12,807: t15.2024.07.21 val PER: 0.2545
2026-01-03 23:30:12,807: t15.2024.07.28 val PER: 0.3022
2026-01-03 23:30:12,807: t15.2025.01.10 val PER: 0.4931
2026-01-03 23:30:12,807: t15.2025.01.12 val PER: 0.3618
2026-01-03 23:30:12,807: t15.2025.03.14 val PER: 0.5030
2026-01-03 23:30:12,807: t15.2025.03.16 val PER: 0.3652
2026-01-03 23:30:12,807: t15.2025.03.30 val PER: 0.5046
2026-01-03 23:30:12,807: t15.2025.04.13 val PER: 0.3923
2026-01-03 23:30:12,808: New best val WER(1gram) 70.30% --> 66.50%
2026-01-03 23:30:12,808: Checkpointing model
2026-01-03 23:30:13,404: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/best_checkpoint
2026-01-03 23:30:13,646: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_2500
2026-01-03 23:30:22,045: Train batch 2600: loss: 34.17 grad norm: 87.58 time: 0.054
2026-01-03 23:30:39,415: Train batch 2800: loss: 24.96 grad norm: 74.31 time: 0.083
2026-01-03 23:30:56,839: Train batch 3000: loss: 30.24 grad norm: 71.58 time: 0.083
2026-01-03 23:30:56,839: Running test after training batch: 3000
2026-01-03 23:30:56,941: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:31:01,678: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the could at this point is will
2026-01-03 23:31:01,707: WER debug example
  GT : how does it keep the cost down
  PR : houde does it kipp the cost get
2026-01-03 23:31:03,405: Val batch 3000: PER (avg): 0.2755 CTC Loss (avg): 27.3946 WER(1gram): 65.48% (n=64) time: 6.565
2026-01-03 23:31:03,405: WER lens: avg_true_words=6.16 avg_pred_words=5.84 max_pred_words=11
2026-01-03 23:31:03,405: t15.2023.08.13 val PER: 0.2505
2026-01-03 23:31:03,405: t15.2023.08.18 val PER: 0.2280
2026-01-03 23:31:03,406: t15.2023.08.20 val PER: 0.2105
2026-01-03 23:31:03,406: t15.2023.08.25 val PER: 0.1807
2026-01-03 23:31:03,406: t15.2023.08.27 val PER: 0.2878
2026-01-03 23:31:03,406: t15.2023.09.01 val PER: 0.1859
2026-01-03 23:31:03,406: t15.2023.09.03 val PER: 0.2743
2026-01-03 23:31:03,406: t15.2023.09.24 val PER: 0.2051
2026-01-03 23:31:03,406: t15.2023.09.29 val PER: 0.2291
2026-01-03 23:31:03,406: t15.2023.10.01 val PER: 0.2807
2026-01-03 23:31:03,406: t15.2023.10.06 val PER: 0.1851
2026-01-03 23:31:03,407: t15.2023.10.08 val PER: 0.3451
2026-01-03 23:31:03,407: t15.2023.10.13 val PER: 0.3320
2026-01-03 23:31:03,407: t15.2023.10.15 val PER: 0.2571
2026-01-03 23:31:03,407: t15.2023.10.20 val PER: 0.2617
2026-01-03 23:31:03,407: t15.2023.10.22 val PER: 0.2071
2026-01-03 23:31:03,407: t15.2023.11.03 val PER: 0.2598
2026-01-03 23:31:03,407: t15.2023.11.04 val PER: 0.0922
2026-01-03 23:31:03,407: t15.2023.11.17 val PER: 0.1400
2026-01-03 23:31:03,407: t15.2023.11.19 val PER: 0.1058
2026-01-03 23:31:03,407: t15.2023.11.26 val PER: 0.2841
2026-01-03 23:31:03,408: t15.2023.12.03 val PER: 0.2574
2026-01-03 23:31:03,408: t15.2023.12.08 val PER: 0.2503
2026-01-03 23:31:03,408: t15.2023.12.10 val PER: 0.2089
2026-01-03 23:31:03,408: t15.2023.12.17 val PER: 0.2713
2026-01-03 23:31:03,408: t15.2023.12.29 val PER: 0.2677
2026-01-03 23:31:03,408: t15.2024.02.25 val PER: 0.2346
2026-01-03 23:31:03,408: t15.2024.03.08 val PER: 0.3585
2026-01-03 23:31:03,408: t15.2024.03.15 val PER: 0.3258
2026-01-03 23:31:03,408: t15.2024.03.17 val PER: 0.2789
2026-01-03 23:31:03,408: t15.2024.05.10 val PER: 0.2897
2026-01-03 23:31:03,408: t15.2024.06.14 val PER: 0.3028
2026-01-03 23:31:03,408: t15.2024.07.19 val PER: 0.3982
2026-01-03 23:31:03,409: t15.2024.07.21 val PER: 0.2248
2026-01-03 23:31:03,409: t15.2024.07.28 val PER: 0.2794
2026-01-03 23:31:03,409: t15.2025.01.10 val PER: 0.4807
2026-01-03 23:31:03,409: t15.2025.01.12 val PER: 0.3272
2026-01-03 23:31:03,409: t15.2025.03.14 val PER: 0.4423
2026-01-03 23:31:03,409: t15.2025.03.16 val PER: 0.3377
2026-01-03 23:31:03,409: t15.2025.03.30 val PER: 0.4667
2026-01-03 23:31:03,409: t15.2025.04.13 val PER: 0.3609
2026-01-03 23:31:03,410: New best val WER(1gram) 66.50% --> 65.48%
2026-01-03 23:31:03,410: Checkpointing model
2026-01-03 23:31:03,972: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/best_checkpoint
2026-01-03 23:31:04,216: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_3000
2026-01-03 23:31:21,692: Train batch 3200: loss: 25.42 grad norm: 66.81 time: 0.076
2026-01-03 23:31:38,783: Train batch 3400: loss: 17.83 grad norm: 59.57 time: 0.048
2026-01-03 23:31:47,729: Running test after training batch: 3500
2026-01-03 23:31:47,851: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:31:52,709: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-03 23:31:52,739: WER debug example
  GT : how does it keep the cost down
  PR : houde des it hipp the cussed get
2026-01-03 23:31:54,393: Val batch 3500: PER (avg): 0.2655 CTC Loss (avg): 26.0942 WER(1gram): 64.21% (n=64) time: 6.663
2026-01-03 23:31:54,393: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-03 23:31:54,393: t15.2023.08.13 val PER: 0.2401
2026-01-03 23:31:54,393: t15.2023.08.18 val PER: 0.2146
2026-01-03 23:31:54,393: t15.2023.08.20 val PER: 0.2192
2026-01-03 23:31:54,393: t15.2023.08.25 val PER: 0.1747
2026-01-03 23:31:54,393: t15.2023.08.27 val PER: 0.2572
2026-01-03 23:31:54,393: t15.2023.09.01 val PER: 0.1761
2026-01-03 23:31:54,394: t15.2023.09.03 val PER: 0.2447
2026-01-03 23:31:54,394: t15.2023.09.24 val PER: 0.2100
2026-01-03 23:31:54,394: t15.2023.09.29 val PER: 0.2221
2026-01-03 23:31:54,394: t15.2023.10.01 val PER: 0.2768
2026-01-03 23:31:54,394: t15.2023.10.06 val PER: 0.1851
2026-01-03 23:31:54,394: t15.2023.10.08 val PER: 0.3329
2026-01-03 23:31:54,394: t15.2023.10.13 val PER: 0.3095
2026-01-03 23:31:54,394: t15.2023.10.15 val PER: 0.2426
2026-01-03 23:31:54,394: t15.2023.10.20 val PER: 0.2517
2026-01-03 23:31:54,394: t15.2023.10.22 val PER: 0.2071
2026-01-03 23:31:54,394: t15.2023.11.03 val PER: 0.2598
2026-01-03 23:31:54,394: t15.2023.11.04 val PER: 0.0683
2026-01-03 23:31:54,394: t15.2023.11.17 val PER: 0.1135
2026-01-03 23:31:54,394: t15.2023.11.19 val PER: 0.1018
2026-01-03 23:31:54,395: t15.2023.11.26 val PER: 0.2725
2026-01-03 23:31:54,395: t15.2023.12.03 val PER: 0.2405
2026-01-03 23:31:54,395: t15.2023.12.08 val PER: 0.2310
2026-01-03 23:31:54,395: t15.2023.12.10 val PER: 0.2011
2026-01-03 23:31:54,395: t15.2023.12.17 val PER: 0.2672
2026-01-03 23:31:54,395: t15.2023.12.29 val PER: 0.2615
2026-01-03 23:31:54,395: t15.2024.02.25 val PER: 0.2107
2026-01-03 23:31:54,395: t15.2024.03.08 val PER: 0.3499
2026-01-03 23:31:54,395: t15.2024.03.15 val PER: 0.3133
2026-01-03 23:31:54,395: t15.2024.03.17 val PER: 0.2824
2026-01-03 23:31:54,395: t15.2024.05.10 val PER: 0.2630
2026-01-03 23:31:54,395: t15.2024.06.14 val PER: 0.2965
2026-01-03 23:31:54,395: t15.2024.07.19 val PER: 0.3988
2026-01-03 23:31:54,395: t15.2024.07.21 val PER: 0.2186
2026-01-03 23:31:54,395: t15.2024.07.28 val PER: 0.2713
2026-01-03 23:31:54,395: t15.2025.01.10 val PER: 0.4711
2026-01-03 23:31:54,396: t15.2025.01.12 val PER: 0.2933
2026-01-03 23:31:54,396: t15.2025.03.14 val PER: 0.4393
2026-01-03 23:31:54,396: t15.2025.03.16 val PER: 0.3207
2026-01-03 23:31:54,396: t15.2025.03.30 val PER: 0.4655
2026-01-03 23:31:54,396: t15.2025.04.13 val PER: 0.3466
2026-01-03 23:31:54,397: New best val WER(1gram) 65.48% --> 64.21%
2026-01-03 23:31:54,397: Checkpointing model
2026-01-03 23:31:55,002: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/best_checkpoint
2026-01-03 23:31:55,247: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_3500
2026-01-03 23:32:03,812: Train batch 3600: loss: 21.39 grad norm: 63.52 time: 0.067
2026-01-03 23:32:20,885: Train batch 3800: loss: 24.22 grad norm: 72.27 time: 0.066
2026-01-03 23:32:38,211: Train batch 4000: loss: 19.27 grad norm: 60.21 time: 0.056
2026-01-03 23:32:38,212: Running test after training batch: 4000
2026-01-03 23:32:38,359: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:32:43,070: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-03 23:32:43,099: WER debug example
  GT : how does it keep the cost down
  PR : aue dust it keep the cussed it
2026-01-03 23:32:44,747: Val batch 4000: PER (avg): 0.2430 CTC Loss (avg): 24.0331 WER(1gram): 64.72% (n=64) time: 6.536
2026-01-03 23:32:44,748: WER lens: avg_true_words=6.16 avg_pred_words=6.00 max_pred_words=11
2026-01-03 23:32:44,748: t15.2023.08.13 val PER: 0.2152
2026-01-03 23:32:44,748: t15.2023.08.18 val PER: 0.2037
2026-01-03 23:32:44,748: t15.2023.08.20 val PER: 0.1970
2026-01-03 23:32:44,748: t15.2023.08.25 val PER: 0.1521
2026-01-03 23:32:44,748: t15.2023.08.27 val PER: 0.2846
2026-01-03 23:32:44,748: t15.2023.09.01 val PER: 0.1518
2026-01-03 23:32:44,748: t15.2023.09.03 val PER: 0.2292
2026-01-03 23:32:44,749: t15.2023.09.24 val PER: 0.1845
2026-01-03 23:32:44,749: t15.2023.09.29 val PER: 0.1927
2026-01-03 23:32:44,749: t15.2023.10.01 val PER: 0.2530
2026-01-03 23:32:44,749: t15.2023.10.06 val PER: 0.1582
2026-01-03 23:32:44,749: t15.2023.10.08 val PER: 0.3058
2026-01-03 23:32:44,749: t15.2023.10.13 val PER: 0.2917
2026-01-03 23:32:44,749: t15.2023.10.15 val PER: 0.2334
2026-01-03 23:32:44,749: t15.2023.10.20 val PER: 0.2248
2026-01-03 23:32:44,749: t15.2023.10.22 val PER: 0.1860
2026-01-03 23:32:44,749: t15.2023.11.03 val PER: 0.2320
2026-01-03 23:32:44,749: t15.2023.11.04 val PER: 0.0717
2026-01-03 23:32:44,750: t15.2023.11.17 val PER: 0.1042
2026-01-03 23:32:44,750: t15.2023.11.19 val PER: 0.0978
2026-01-03 23:32:44,750: t15.2023.11.26 val PER: 0.2587
2026-01-03 23:32:44,750: t15.2023.12.03 val PER: 0.2048
2026-01-03 23:32:44,750: t15.2023.12.08 val PER: 0.2170
2026-01-03 23:32:44,750: t15.2023.12.10 val PER: 0.1708
2026-01-03 23:32:44,750: t15.2023.12.17 val PER: 0.2464
2026-01-03 23:32:44,750: t15.2023.12.29 val PER: 0.2505
2026-01-03 23:32:44,750: t15.2024.02.25 val PER: 0.2149
2026-01-03 23:32:44,750: t15.2024.03.08 val PER: 0.3243
2026-01-03 23:32:44,750: t15.2024.03.15 val PER: 0.2989
2026-01-03 23:32:44,750: t15.2024.03.17 val PER: 0.2483
2026-01-03 23:32:44,750: t15.2024.05.10 val PER: 0.2689
2026-01-03 23:32:44,750: t15.2024.06.14 val PER: 0.2650
2026-01-03 23:32:44,750: t15.2024.07.19 val PER: 0.3533
2026-01-03 23:32:44,750: t15.2024.07.21 val PER: 0.1779
2026-01-03 23:32:44,750: t15.2024.07.28 val PER: 0.2316
2026-01-03 23:32:44,751: t15.2025.01.10 val PER: 0.4256
2026-01-03 23:32:44,751: t15.2025.01.12 val PER: 0.2771
2026-01-03 23:32:44,751: t15.2025.03.14 val PER: 0.4201
2026-01-03 23:32:44,751: t15.2025.03.16 val PER: 0.3128
2026-01-03 23:32:44,751: t15.2025.03.30 val PER: 0.4057
2026-01-03 23:32:44,751: t15.2025.04.13 val PER: 0.3181
2026-01-03 23:32:44,982: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_4000
2026-01-03 23:33:02,192: Train batch 4200: loss: 21.51 grad norm: 65.00 time: 0.078
2026-01-03 23:33:19,585: Train batch 4400: loss: 16.22 grad norm: 60.25 time: 0.066
2026-01-03 23:33:28,210: Running test after training batch: 4500
2026-01-03 23:33:28,336: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:33:33,109: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-03 23:33:33,139: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cost get
2026-01-03 23:33:34,780: Val batch 4500: PER (avg): 0.2373 CTC Loss (avg): 23.0596 WER(1gram): 61.93% (n=64) time: 6.570
2026-01-03 23:33:34,780: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 23:33:34,780: t15.2023.08.13 val PER: 0.2214
2026-01-03 23:33:34,781: t15.2023.08.18 val PER: 0.1794
2026-01-03 23:33:34,781: t15.2023.08.20 val PER: 0.1835
2026-01-03 23:33:34,781: t15.2023.08.25 val PER: 0.1521
2026-01-03 23:33:34,781: t15.2023.08.27 val PER: 0.2637
2026-01-03 23:33:34,781: t15.2023.09.01 val PER: 0.1583
2026-01-03 23:33:34,781: t15.2023.09.03 val PER: 0.2316
2026-01-03 23:33:34,781: t15.2023.09.24 val PER: 0.1881
2026-01-03 23:33:34,781: t15.2023.09.29 val PER: 0.1940
2026-01-03 23:33:34,781: t15.2023.10.01 val PER: 0.2497
2026-01-03 23:33:34,781: t15.2023.10.06 val PER: 0.1518
2026-01-03 23:33:34,781: t15.2023.10.08 val PER: 0.3072
2026-01-03 23:33:34,781: t15.2023.10.13 val PER: 0.2956
2026-01-03 23:33:34,781: t15.2023.10.15 val PER: 0.2287
2026-01-03 23:33:34,782: t15.2023.10.20 val PER: 0.2148
2026-01-03 23:33:34,782: t15.2023.10.22 val PER: 0.1782
2026-01-03 23:33:34,782: t15.2023.11.03 val PER: 0.2490
2026-01-03 23:33:34,782: t15.2023.11.04 val PER: 0.0546
2026-01-03 23:33:34,782: t15.2023.11.17 val PER: 0.0933
2026-01-03 23:33:34,782: t15.2023.11.19 val PER: 0.0838
2026-01-03 23:33:34,782: t15.2023.11.26 val PER: 0.2565
2026-01-03 23:33:34,782: t15.2023.12.03 val PER: 0.2038
2026-01-03 23:33:34,782: t15.2023.12.08 val PER: 0.2031
2026-01-03 23:33:34,782: t15.2023.12.10 val PER: 0.1800
2026-01-03 23:33:34,782: t15.2023.12.17 val PER: 0.2245
2026-01-03 23:33:34,782: t15.2023.12.29 val PER: 0.2505
2026-01-03 23:33:34,783: t15.2024.02.25 val PER: 0.1966
2026-01-03 23:33:34,783: t15.2024.03.08 val PER: 0.3144
2026-01-03 23:33:34,783: t15.2024.03.15 val PER: 0.2902
2026-01-03 23:33:34,783: t15.2024.03.17 val PER: 0.2448
2026-01-03 23:33:34,783: t15.2024.05.10 val PER: 0.2660
2026-01-03 23:33:34,783: t15.2024.06.14 val PER: 0.2382
2026-01-03 23:33:34,783: t15.2024.07.19 val PER: 0.3428
2026-01-03 23:33:34,783: t15.2024.07.21 val PER: 0.1745
2026-01-03 23:33:34,783: t15.2024.07.28 val PER: 0.2199
2026-01-03 23:33:34,783: t15.2025.01.10 val PER: 0.4146
2026-01-03 23:33:34,783: t15.2025.01.12 val PER: 0.2694
2026-01-03 23:33:34,783: t15.2025.03.14 val PER: 0.4112
2026-01-03 23:33:34,783: t15.2025.03.16 val PER: 0.2997
2026-01-03 23:33:34,783: t15.2025.03.30 val PER: 0.4138
2026-01-03 23:33:34,783: t15.2025.04.13 val PER: 0.2839
2026-01-03 23:33:34,784: New best val WER(1gram) 64.21% --> 61.93%
2026-01-03 23:33:34,784: Checkpointing model
2026-01-03 23:33:35,415: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/best_checkpoint
2026-01-03 23:33:35,661: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_4500
2026-01-03 23:33:44,262: Train batch 4600: loss: 19.32 grad norm: 70.54 time: 0.062
2026-01-03 23:34:01,461: Train batch 4800: loss: 13.34 grad norm: 58.08 time: 0.063
2026-01-03 23:34:18,563: Train batch 5000: loss: 29.52 grad norm: 85.87 time: 0.064
2026-01-03 23:34:18,563: Running test after training batch: 5000
2026-01-03 23:34:18,662: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:34:23,602: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point as will
2026-01-03 23:34:23,631: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-03 23:34:25,281: Val batch 5000: PER (avg): 0.2220 CTC Loss (avg): 21.8759 WER(1gram): 59.14% (n=64) time: 6.718
2026-01-03 23:34:25,282: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 23:34:25,282: t15.2023.08.13 val PER: 0.1757
2026-01-03 23:34:25,282: t15.2023.08.18 val PER: 0.1668
2026-01-03 23:34:25,282: t15.2023.08.20 val PER: 0.1771
2026-01-03 23:34:25,282: t15.2023.08.25 val PER: 0.1386
2026-01-03 23:34:25,282: t15.2023.08.27 val PER: 0.2379
2026-01-03 23:34:25,282: t15.2023.09.01 val PER: 0.1266
2026-01-03 23:34:25,282: t15.2023.09.03 val PER: 0.2162
2026-01-03 23:34:25,282: t15.2023.09.24 val PER: 0.1893
2026-01-03 23:34:25,282: t15.2023.09.29 val PER: 0.1883
2026-01-03 23:34:25,282: t15.2023.10.01 val PER: 0.2332
2026-01-03 23:34:25,283: t15.2023.10.06 val PER: 0.1389
2026-01-03 23:34:25,283: t15.2023.10.08 val PER: 0.2882
2026-01-03 23:34:25,283: t15.2023.10.13 val PER: 0.2684
2026-01-03 23:34:25,283: t15.2023.10.15 val PER: 0.2248
2026-01-03 23:34:25,283: t15.2023.10.20 val PER: 0.2282
2026-01-03 23:34:25,283: t15.2023.10.22 val PER: 0.1659
2026-01-03 23:34:25,283: t15.2023.11.03 val PER: 0.2212
2026-01-03 23:34:25,283: t15.2023.11.04 val PER: 0.0512
2026-01-03 23:34:25,283: t15.2023.11.17 val PER: 0.0762
2026-01-03 23:34:25,283: t15.2023.11.19 val PER: 0.0639
2026-01-03 23:34:25,283: t15.2023.11.26 val PER: 0.2225
2026-01-03 23:34:25,283: t15.2023.12.03 val PER: 0.1996
2026-01-03 23:34:25,283: t15.2023.12.08 val PER: 0.1838
2026-01-03 23:34:25,283: t15.2023.12.10 val PER: 0.1590
2026-01-03 23:34:25,284: t15.2023.12.17 val PER: 0.2235
2026-01-03 23:34:25,284: t15.2023.12.29 val PER: 0.2210
2026-01-03 23:34:25,284: t15.2024.02.25 val PER: 0.1910
2026-01-03 23:34:25,284: t15.2024.03.08 val PER: 0.3087
2026-01-03 23:34:25,284: t15.2024.03.15 val PER: 0.2789
2026-01-03 23:34:25,284: t15.2024.03.17 val PER: 0.2308
2026-01-03 23:34:25,284: t15.2024.05.10 val PER: 0.2407
2026-01-03 23:34:25,284: t15.2024.06.14 val PER: 0.2350
2026-01-03 23:34:25,284: t15.2024.07.19 val PER: 0.3415
2026-01-03 23:34:25,285: t15.2024.07.21 val PER: 0.1614
2026-01-03 23:34:25,285: t15.2024.07.28 val PER: 0.2088
2026-01-03 23:34:25,285: t15.2025.01.10 val PER: 0.3760
2026-01-03 23:34:25,285: t15.2025.01.12 val PER: 0.2517
2026-01-03 23:34:25,285: t15.2025.03.14 val PER: 0.3905
2026-01-03 23:34:25,285: t15.2025.03.16 val PER: 0.2775
2026-01-03 23:34:25,285: t15.2025.03.30 val PER: 0.3954
2026-01-03 23:34:25,285: t15.2025.04.13 val PER: 0.3110
2026-01-03 23:34:25,286: New best val WER(1gram) 61.93% --> 59.14%
2026-01-03 23:34:25,286: Checkpointing model
2026-01-03 23:34:25,894: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/best_checkpoint
2026-01-03 23:34:26,137: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_5000
2026-01-03 23:34:43,238: Train batch 5200: loss: 15.58 grad norm: 57.99 time: 0.051
2026-01-03 23:35:00,417: Train batch 5400: loss: 16.72 grad norm: 63.84 time: 0.068
2026-01-03 23:35:09,090: Running test after training batch: 5500
2026-01-03 23:35:09,236: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:35:13,958: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point will
2026-01-03 23:35:13,987: WER debug example
  GT : how does it keep the cost down
  PR : aue dust it keep the cost it
2026-01-03 23:35:15,564: Val batch 5500: PER (avg): 0.2139 CTC Loss (avg): 20.8527 WER(1gram): 56.35% (n=64) time: 6.473
2026-01-03 23:35:15,564: WER lens: avg_true_words=6.16 avg_pred_words=6.00 max_pred_words=11
2026-01-03 23:35:15,564: t15.2023.08.13 val PER: 0.1830
2026-01-03 23:35:15,564: t15.2023.08.18 val PER: 0.1668
2026-01-03 23:35:15,565: t15.2023.08.20 val PER: 0.1612
2026-01-03 23:35:15,565: t15.2023.08.25 val PER: 0.1325
2026-01-03 23:35:15,565: t15.2023.08.27 val PER: 0.2444
2026-01-03 23:35:15,565: t15.2023.09.01 val PER: 0.1234
2026-01-03 23:35:15,565: t15.2023.09.03 val PER: 0.2185
2026-01-03 23:35:15,565: t15.2023.09.24 val PER: 0.1663
2026-01-03 23:35:15,565: t15.2023.09.29 val PER: 0.1729
2026-01-03 23:35:15,565: t15.2023.10.01 val PER: 0.2266
2026-01-03 23:35:15,565: t15.2023.10.06 val PER: 0.1378
2026-01-03 23:35:15,566: t15.2023.10.08 val PER: 0.2909
2026-01-03 23:35:15,566: t15.2023.10.13 val PER: 0.2723
2026-01-03 23:35:15,566: t15.2023.10.15 val PER: 0.1978
2026-01-03 23:35:15,566: t15.2023.10.20 val PER: 0.2483
2026-01-03 23:35:15,566: t15.2023.10.22 val PER: 0.1670
2026-01-03 23:35:15,566: t15.2023.11.03 val PER: 0.2225
2026-01-03 23:35:15,566: t15.2023.11.04 val PER: 0.0648
2026-01-03 23:35:15,566: t15.2023.11.17 val PER: 0.0871
2026-01-03 23:35:15,566: t15.2023.11.19 val PER: 0.0719
2026-01-03 23:35:15,566: t15.2023.11.26 val PER: 0.2174
2026-01-03 23:35:15,566: t15.2023.12.03 val PER: 0.1775
2026-01-03 23:35:15,566: t15.2023.12.08 val PER: 0.1844
2026-01-03 23:35:15,566: t15.2023.12.10 val PER: 0.1498
2026-01-03 23:35:15,566: t15.2023.12.17 val PER: 0.2131
2026-01-03 23:35:15,566: t15.2023.12.29 val PER: 0.2100
2026-01-03 23:35:15,567: t15.2024.02.25 val PER: 0.1615
2026-01-03 23:35:15,567: t15.2024.03.08 val PER: 0.2888
2026-01-03 23:35:15,567: t15.2024.03.15 val PER: 0.2702
2026-01-03 23:35:15,567: t15.2024.03.17 val PER: 0.2134
2026-01-03 23:35:15,567: t15.2024.05.10 val PER: 0.2377
2026-01-03 23:35:15,567: t15.2024.06.14 val PER: 0.2271
2026-01-03 23:35:15,567: t15.2024.07.19 val PER: 0.3283
2026-01-03 23:35:15,567: t15.2024.07.21 val PER: 0.1628
2026-01-03 23:35:15,567: t15.2024.07.28 val PER: 0.1978
2026-01-03 23:35:15,567: t15.2025.01.10 val PER: 0.3953
2026-01-03 23:35:15,567: t15.2025.01.12 val PER: 0.2317
2026-01-03 23:35:15,567: t15.2025.03.14 val PER: 0.3639
2026-01-03 23:35:15,568: t15.2025.03.16 val PER: 0.2644
2026-01-03 23:35:15,568: t15.2025.03.30 val PER: 0.3621
2026-01-03 23:35:15,568: t15.2025.04.13 val PER: 0.2967
2026-01-03 23:35:15,569: New best val WER(1gram) 59.14% --> 56.35%
2026-01-03 23:35:15,569: Checkpointing model
2026-01-03 23:35:16,193: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/best_checkpoint
2026-01-03 23:35:16,436: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_5500
2026-01-03 23:35:24,939: Train batch 5600: loss: 18.62 grad norm: 67.27 time: 0.062
2026-01-03 23:35:42,393: Train batch 5800: loss: 12.79 grad norm: 59.17 time: 0.082
2026-01-03 23:35:59,451: Train batch 6000: loss: 13.32 grad norm: 58.57 time: 0.049
2026-01-03 23:35:59,452: Running test after training batch: 6000
2026-01-03 23:35:59,569: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:36:04,358: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 23:36:04,390: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 23:36:06,055: Val batch 6000: PER (avg): 0.2097 CTC Loss (avg): 20.5052 WER(1gram): 58.12% (n=64) time: 6.603
2026-01-03 23:36:06,055: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 23:36:06,055: t15.2023.08.13 val PER: 0.1830
2026-01-03 23:36:06,055: t15.2023.08.18 val PER: 0.1567
2026-01-03 23:36:06,055: t15.2023.08.20 val PER: 0.1700
2026-01-03 23:36:06,055: t15.2023.08.25 val PER: 0.1190
2026-01-03 23:36:06,055: t15.2023.08.27 val PER: 0.2267
2026-01-03 23:36:06,055: t15.2023.09.01 val PER: 0.1299
2026-01-03 23:36:06,055: t15.2023.09.03 val PER: 0.2114
2026-01-03 23:36:06,056: t15.2023.09.24 val PER: 0.1505
2026-01-03 23:36:06,056: t15.2023.09.29 val PER: 0.1698
2026-01-03 23:36:06,056: t15.2023.10.01 val PER: 0.2266
2026-01-03 23:36:06,056: t15.2023.10.06 val PER: 0.1292
2026-01-03 23:36:06,056: t15.2023.10.08 val PER: 0.2828
2026-01-03 23:36:06,056: t15.2023.10.13 val PER: 0.2576
2026-01-03 23:36:06,056: t15.2023.10.15 val PER: 0.2050
2026-01-03 23:36:06,056: t15.2023.10.20 val PER: 0.2114
2026-01-03 23:36:06,056: t15.2023.10.22 val PER: 0.1592
2026-01-03 23:36:06,056: t15.2023.11.03 val PER: 0.2239
2026-01-03 23:36:06,057: t15.2023.11.04 val PER: 0.0580
2026-01-03 23:36:06,057: t15.2023.11.17 val PER: 0.0809
2026-01-03 23:36:06,057: t15.2023.11.19 val PER: 0.0659
2026-01-03 23:36:06,057: t15.2023.11.26 val PER: 0.2072
2026-01-03 23:36:06,057: t15.2023.12.03 val PER: 0.1660
2026-01-03 23:36:06,057: t15.2023.12.08 val PER: 0.1764
2026-01-03 23:36:06,057: t15.2023.12.10 val PER: 0.1367
2026-01-03 23:36:06,057: t15.2023.12.17 val PER: 0.1975
2026-01-03 23:36:06,057: t15.2023.12.29 val PER: 0.2135
2026-01-03 23:36:06,057: t15.2024.02.25 val PER: 0.1657
2026-01-03 23:36:06,057: t15.2024.03.08 val PER: 0.3073
2026-01-03 23:36:06,057: t15.2024.03.15 val PER: 0.2670
2026-01-03 23:36:06,057: t15.2024.03.17 val PER: 0.2176
2026-01-03 23:36:06,057: t15.2024.05.10 val PER: 0.2363
2026-01-03 23:36:06,057: t15.2024.06.14 val PER: 0.2240
2026-01-03 23:36:06,057: t15.2024.07.19 val PER: 0.3138
2026-01-03 23:36:06,058: t15.2024.07.21 val PER: 0.1497
2026-01-03 23:36:06,058: t15.2024.07.28 val PER: 0.2044
2026-01-03 23:36:06,058: t15.2025.01.10 val PER: 0.3774
2026-01-03 23:36:06,058: t15.2025.01.12 val PER: 0.2263
2026-01-03 23:36:06,058: t15.2025.03.14 val PER: 0.3757
2026-01-03 23:36:06,058: t15.2025.03.16 val PER: 0.2775
2026-01-03 23:36:06,058: t15.2025.03.30 val PER: 0.3632
2026-01-03 23:36:06,058: t15.2025.04.13 val PER: 0.2725
2026-01-03 23:36:06,292: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_6000
2026-01-03 23:36:23,482: Train batch 6200: loss: 15.37 grad norm: 62.84 time: 0.070
2026-01-03 23:36:40,547: Train batch 6400: loss: 18.17 grad norm: 70.98 time: 0.062
2026-01-03 23:36:49,078: Running test after training batch: 6500
2026-01-03 23:36:49,211: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:36:54,243: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 23:36:54,274: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 23:36:55,868: Val batch 6500: PER (avg): 0.2012 CTC Loss (avg): 19.7585 WER(1gram): 54.06% (n=64) time: 6.790
2026-01-03 23:36:55,869: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 23:36:55,869: t15.2023.08.13 val PER: 0.1674
2026-01-03 23:36:55,869: t15.2023.08.18 val PER: 0.1475
2026-01-03 23:36:55,869: t15.2023.08.20 val PER: 0.1557
2026-01-03 23:36:55,869: t15.2023.08.25 val PER: 0.1145
2026-01-03 23:36:55,869: t15.2023.08.27 val PER: 0.2154
2026-01-03 23:36:55,869: t15.2023.09.01 val PER: 0.1177
2026-01-03 23:36:55,869: t15.2023.09.03 val PER: 0.2031
2026-01-03 23:36:55,869: t15.2023.09.24 val PER: 0.1638
2026-01-03 23:36:55,870: t15.2023.09.29 val PER: 0.1672
2026-01-03 23:36:55,870: t15.2023.10.01 val PER: 0.2107
2026-01-03 23:36:55,870: t15.2023.10.06 val PER: 0.1173
2026-01-03 23:36:55,870: t15.2023.10.08 val PER: 0.2896
2026-01-03 23:36:55,870: t15.2023.10.13 val PER: 0.2545
2026-01-03 23:36:55,870: t15.2023.10.15 val PER: 0.2103
2026-01-03 23:36:55,870: t15.2023.10.20 val PER: 0.2047
2026-01-03 23:36:55,870: t15.2023.10.22 val PER: 0.1548
2026-01-03 23:36:55,870: t15.2023.11.03 val PER: 0.2178
2026-01-03 23:36:55,870: t15.2023.11.04 val PER: 0.0546
2026-01-03 23:36:55,870: t15.2023.11.17 val PER: 0.0622
2026-01-03 23:36:55,870: t15.2023.11.19 val PER: 0.0639
2026-01-03 23:36:55,870: t15.2023.11.26 val PER: 0.1978
2026-01-03 23:36:55,870: t15.2023.12.03 val PER: 0.1744
2026-01-03 23:36:55,870: t15.2023.12.08 val PER: 0.1591
2026-01-03 23:36:55,871: t15.2023.12.10 val PER: 0.1353
2026-01-03 23:36:55,871: t15.2023.12.17 val PER: 0.1850
2026-01-03 23:36:55,871: t15.2023.12.29 val PER: 0.2073
2026-01-03 23:36:55,871: t15.2024.02.25 val PER: 0.1601
2026-01-03 23:36:55,871: t15.2024.03.08 val PER: 0.2959
2026-01-03 23:36:55,871: t15.2024.03.15 val PER: 0.2545
2026-01-03 23:36:55,871: t15.2024.03.17 val PER: 0.2057
2026-01-03 23:36:55,871: t15.2024.05.10 val PER: 0.2244
2026-01-03 23:36:55,871: t15.2024.06.14 val PER: 0.2161
2026-01-03 23:36:55,871: t15.2024.07.19 val PER: 0.2927
2026-01-03 23:36:55,871: t15.2024.07.21 val PER: 0.1497
2026-01-03 23:36:55,871: t15.2024.07.28 val PER: 0.1912
2026-01-03 23:36:55,871: t15.2025.01.10 val PER: 0.3760
2026-01-03 23:36:55,871: t15.2025.01.12 val PER: 0.2163
2026-01-03 23:36:55,871: t15.2025.03.14 val PER: 0.3728
2026-01-03 23:36:55,871: t15.2025.03.16 val PER: 0.2487
2026-01-03 23:36:55,872: t15.2025.03.30 val PER: 0.3414
2026-01-03 23:36:55,872: t15.2025.04.13 val PER: 0.2696
2026-01-03 23:36:55,873: New best val WER(1gram) 56.35% --> 54.06%
2026-01-03 23:36:55,873: Checkpointing model
2026-01-03 23:36:56,461: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/best_checkpoint
2026-01-03 23:36:56,703: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_6500
2026-01-03 23:37:05,011: Train batch 6600: loss: 11.27 grad norm: 53.33 time: 0.045
2026-01-03 23:37:22,436: Train batch 6800: loss: 14.62 grad norm: 61.51 time: 0.048
2026-01-03 23:37:39,799: Train batch 7000: loss: 15.95 grad norm: 69.17 time: 0.060
2026-01-03 23:37:39,799: Running test after training batch: 7000
2026-01-03 23:37:39,946: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:37:44,644: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as wheel
2026-01-03 23:37:44,675: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-03 23:37:46,373: Val batch 7000: PER (avg): 0.1951 CTC Loss (avg): 19.4022 WER(1gram): 54.57% (n=64) time: 6.574
2026-01-03 23:37:46,373: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 23:37:46,373: t15.2023.08.13 val PER: 0.1674
2026-01-03 23:37:46,373: t15.2023.08.18 val PER: 0.1500
2026-01-03 23:37:46,373: t15.2023.08.20 val PER: 0.1541
2026-01-03 23:37:46,373: t15.2023.08.25 val PER: 0.0979
2026-01-03 23:37:46,373: t15.2023.08.27 val PER: 0.2090
2026-01-03 23:37:46,373: t15.2023.09.01 val PER: 0.1080
2026-01-03 23:37:46,373: t15.2023.09.03 val PER: 0.1912
2026-01-03 23:37:46,374: t15.2023.09.24 val PER: 0.1614
2026-01-03 23:37:46,374: t15.2023.09.29 val PER: 0.1653
2026-01-03 23:37:46,374: t15.2023.10.01 val PER: 0.2087
2026-01-03 23:37:46,374: t15.2023.10.06 val PER: 0.1173
2026-01-03 23:37:46,374: t15.2023.10.08 val PER: 0.2652
2026-01-03 23:37:46,374: t15.2023.10.13 val PER: 0.2475
2026-01-03 23:37:46,374: t15.2023.10.15 val PER: 0.1839
2026-01-03 23:37:46,374: t15.2023.10.20 val PER: 0.2148
2026-01-03 23:37:46,374: t15.2023.10.22 val PER: 0.1403
2026-01-03 23:37:46,374: t15.2023.11.03 val PER: 0.1961
2026-01-03 23:37:46,374: t15.2023.11.04 val PER: 0.0444
2026-01-03 23:37:46,374: t15.2023.11.17 val PER: 0.0747
2026-01-03 23:37:46,374: t15.2023.11.19 val PER: 0.0539
2026-01-03 23:37:46,375: t15.2023.11.26 val PER: 0.1942
2026-01-03 23:37:46,375: t15.2023.12.03 val PER: 0.1628
2026-01-03 23:37:46,375: t15.2023.12.08 val PER: 0.1558
2026-01-03 23:37:46,375: t15.2023.12.10 val PER: 0.1275
2026-01-03 23:37:46,375: t15.2023.12.17 val PER: 0.1871
2026-01-03 23:37:46,375: t15.2023.12.29 val PER: 0.1977
2026-01-03 23:37:46,375: t15.2024.02.25 val PER: 0.1517
2026-01-03 23:37:46,375: t15.2024.03.08 val PER: 0.2760
2026-01-03 23:37:46,375: t15.2024.03.15 val PER: 0.2508
2026-01-03 23:37:46,375: t15.2024.03.17 val PER: 0.2120
2026-01-03 23:37:46,375: t15.2024.05.10 val PER: 0.2184
2026-01-03 23:37:46,375: t15.2024.06.14 val PER: 0.2082
2026-01-03 23:37:46,375: t15.2024.07.19 val PER: 0.3026
2026-01-03 23:37:46,375: t15.2024.07.21 val PER: 0.1317
2026-01-03 23:37:46,375: t15.2024.07.28 val PER: 0.1824
2026-01-03 23:37:46,375: t15.2025.01.10 val PER: 0.3581
2026-01-03 23:37:46,375: t15.2025.01.12 val PER: 0.2086
2026-01-03 23:37:46,376: t15.2025.03.14 val PER: 0.3683
2026-01-03 23:37:46,376: t15.2025.03.16 val PER: 0.2435
2026-01-03 23:37:46,376: t15.2025.03.30 val PER: 0.3632
2026-01-03 23:37:46,376: t15.2025.04.13 val PER: 0.2653
2026-01-03 23:37:46,608: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_7000
2026-01-03 23:38:03,594: Train batch 7200: loss: 13.35 grad norm: 60.85 time: 0.078
2026-01-03 23:38:20,655: Train batch 7400: loss: 13.31 grad norm: 60.61 time: 0.075
2026-01-03 23:38:29,127: Running test after training batch: 7500
2026-01-03 23:38:29,312: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:38:34,032: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 23:38:34,064: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost tet
2026-01-03 23:38:35,768: Val batch 7500: PER (avg): 0.1896 CTC Loss (avg): 18.5415 WER(1gram): 55.08% (n=64) time: 6.640
2026-01-03 23:38:35,768: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 23:38:35,769: t15.2023.08.13 val PER: 0.1674
2026-01-03 23:38:35,769: t15.2023.08.18 val PER: 0.1442
2026-01-03 23:38:35,769: t15.2023.08.20 val PER: 0.1477
2026-01-03 23:38:35,769: t15.2023.08.25 val PER: 0.1114
2026-01-03 23:38:35,769: t15.2023.08.27 val PER: 0.2026
2026-01-03 23:38:35,769: t15.2023.09.01 val PER: 0.1071
2026-01-03 23:38:35,769: t15.2023.09.03 val PER: 0.1865
2026-01-03 23:38:35,769: t15.2023.09.24 val PER: 0.1505
2026-01-03 23:38:35,769: t15.2023.09.29 val PER: 0.1576
2026-01-03 23:38:35,769: t15.2023.10.01 val PER: 0.2001
2026-01-03 23:38:35,770: t15.2023.10.06 val PER: 0.1163
2026-01-03 23:38:35,770: t15.2023.10.08 val PER: 0.2706
2026-01-03 23:38:35,770: t15.2023.10.13 val PER: 0.2452
2026-01-03 23:38:35,770: t15.2023.10.15 val PER: 0.1918
2026-01-03 23:38:35,770: t15.2023.10.20 val PER: 0.1812
2026-01-03 23:38:35,770: t15.2023.10.22 val PER: 0.1359
2026-01-03 23:38:35,770: t15.2023.11.03 val PER: 0.2008
2026-01-03 23:38:35,770: t15.2023.11.04 val PER: 0.0478
2026-01-03 23:38:35,770: t15.2023.11.17 val PER: 0.0700
2026-01-03 23:38:35,770: t15.2023.11.19 val PER: 0.0539
2026-01-03 23:38:35,770: t15.2023.11.26 val PER: 0.1790
2026-01-03 23:38:35,770: t15.2023.12.03 val PER: 0.1544
2026-01-03 23:38:35,771: t15.2023.12.08 val PER: 0.1518
2026-01-03 23:38:35,771: t15.2023.12.10 val PER: 0.1222
2026-01-03 23:38:35,771: t15.2023.12.17 val PER: 0.1767
2026-01-03 23:38:35,771: t15.2023.12.29 val PER: 0.1833
2026-01-03 23:38:35,771: t15.2024.02.25 val PER: 0.1433
2026-01-03 23:38:35,771: t15.2024.03.08 val PER: 0.2760
2026-01-03 23:38:35,771: t15.2024.03.15 val PER: 0.2364
2026-01-03 23:38:35,771: t15.2024.03.17 val PER: 0.1918
2026-01-03 23:38:35,771: t15.2024.05.10 val PER: 0.2199
2026-01-03 23:38:35,771: t15.2024.06.14 val PER: 0.2177
2026-01-03 23:38:35,771: t15.2024.07.19 val PER: 0.2861
2026-01-03 23:38:35,772: t15.2024.07.21 val PER: 0.1352
2026-01-03 23:38:35,772: t15.2024.07.28 val PER: 0.1735
2026-01-03 23:38:35,772: t15.2025.01.10 val PER: 0.3664
2026-01-03 23:38:35,772: t15.2025.01.12 val PER: 0.2032
2026-01-03 23:38:35,772: t15.2025.03.14 val PER: 0.3565
2026-01-03 23:38:35,772: t15.2025.03.16 val PER: 0.2487
2026-01-03 23:38:35,772: t15.2025.03.30 val PER: 0.3506
2026-01-03 23:38:35,772: t15.2025.04.13 val PER: 0.2496
2026-01-03 23:38:36,008: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_7500
2026-01-03 23:38:44,479: Train batch 7600: loss: 14.54 grad norm: 61.33 time: 0.068
2026-01-03 23:39:01,633: Train batch 7800: loss: 14.15 grad norm: 64.41 time: 0.055
2026-01-03 23:39:19,138: Train batch 8000: loss: 9.94 grad norm: 52.36 time: 0.075
2026-01-03 23:39:19,138: Running test after training batch: 8000
2026-01-03 23:39:19,231: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:39:24,087: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-03 23:39:24,118: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 23:39:25,782: Val batch 8000: PER (avg): 0.1814 CTC Loss (avg): 17.8737 WER(1gram): 53.05% (n=64) time: 6.643
2026-01-03 23:39:25,782: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 23:39:25,782: t15.2023.08.13 val PER: 0.1507
2026-01-03 23:39:25,782: t15.2023.08.18 val PER: 0.1308
2026-01-03 23:39:25,782: t15.2023.08.20 val PER: 0.1454
2026-01-03 23:39:25,782: t15.2023.08.25 val PER: 0.1024
2026-01-03 23:39:25,782: t15.2023.08.27 val PER: 0.2090
2026-01-03 23:39:25,782: t15.2023.09.01 val PER: 0.0990
2026-01-03 23:39:25,783: t15.2023.09.03 val PER: 0.1770
2026-01-03 23:39:25,783: t15.2023.09.24 val PER: 0.1456
2026-01-03 23:39:25,783: t15.2023.09.29 val PER: 0.1442
2026-01-03 23:39:25,783: t15.2023.10.01 val PER: 0.1968
2026-01-03 23:39:25,783: t15.2023.10.06 val PER: 0.1087
2026-01-03 23:39:25,783: t15.2023.10.08 val PER: 0.2828
2026-01-03 23:39:25,783: t15.2023.10.13 val PER: 0.2343
2026-01-03 23:39:25,783: t15.2023.10.15 val PER: 0.1866
2026-01-03 23:39:25,783: t15.2023.10.20 val PER: 0.1779
2026-01-03 23:39:25,783: t15.2023.10.22 val PER: 0.1347
2026-01-03 23:39:25,783: t15.2023.11.03 val PER: 0.2008
2026-01-03 23:39:25,783: t15.2023.11.04 val PER: 0.0341
2026-01-03 23:39:25,784: t15.2023.11.17 val PER: 0.0467
2026-01-03 23:39:25,784: t15.2023.11.19 val PER: 0.0479
2026-01-03 23:39:25,784: t15.2023.11.26 val PER: 0.1703
2026-01-03 23:39:25,784: t15.2023.12.03 val PER: 0.1544
2026-01-03 23:39:25,784: t15.2023.12.08 val PER: 0.1352
2026-01-03 23:39:25,784: t15.2023.12.10 val PER: 0.1196
2026-01-03 23:39:25,784: t15.2023.12.17 val PER: 0.1642
2026-01-03 23:39:25,784: t15.2023.12.29 val PER: 0.1757
2026-01-03 23:39:25,784: t15.2024.02.25 val PER: 0.1475
2026-01-03 23:39:25,784: t15.2024.03.08 val PER: 0.2731
2026-01-03 23:39:25,784: t15.2024.03.15 val PER: 0.2258
2026-01-03 23:39:25,784: t15.2024.03.17 val PER: 0.1757
2026-01-03 23:39:25,784: t15.2024.05.10 val PER: 0.2110
2026-01-03 23:39:25,785: t15.2024.06.14 val PER: 0.1987
2026-01-03 23:39:25,785: t15.2024.07.19 val PER: 0.2775
2026-01-03 23:39:25,785: t15.2024.07.21 val PER: 0.1241
2026-01-03 23:39:25,785: t15.2024.07.28 val PER: 0.1603
2026-01-03 23:39:25,785: t15.2025.01.10 val PER: 0.3416
2026-01-03 23:39:25,785: t15.2025.01.12 val PER: 0.1917
2026-01-03 23:39:25,785: t15.2025.03.14 val PER: 0.3476
2026-01-03 23:39:25,785: t15.2025.03.16 val PER: 0.2382
2026-01-03 23:39:25,785: t15.2025.03.30 val PER: 0.3437
2026-01-03 23:39:25,785: t15.2025.04.13 val PER: 0.2611
2026-01-03 23:39:25,786: New best val WER(1gram) 54.06% --> 53.05%
2026-01-03 23:39:25,786: Checkpointing model
2026-01-03 23:39:26,376: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/best_checkpoint
2026-01-03 23:39:26,621: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_8000
2026-01-03 23:39:43,983: Train batch 8200: loss: 8.57 grad norm: 47.92 time: 0.054
2026-01-03 23:40:01,246: Train batch 8400: loss: 9.05 grad norm: 50.72 time: 0.064
2026-01-03 23:40:10,107: Running test after training batch: 8500
2026-01-03 23:40:10,200: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:40:14,893: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 23:40:14,925: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ghent
2026-01-03 23:40:16,617: Val batch 8500: PER (avg): 0.1783 CTC Loss (avg): 17.6111 WER(1gram): 52.03% (n=64) time: 6.509
2026-01-03 23:40:16,617: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 23:40:16,618: t15.2023.08.13 val PER: 0.1466
2026-01-03 23:40:16,618: t15.2023.08.18 val PER: 0.1282
2026-01-03 23:40:16,618: t15.2023.08.20 val PER: 0.1398
2026-01-03 23:40:16,618: t15.2023.08.25 val PER: 0.1084
2026-01-03 23:40:16,618: t15.2023.08.27 val PER: 0.2058
2026-01-03 23:40:16,618: t15.2023.09.01 val PER: 0.1006
2026-01-03 23:40:16,618: t15.2023.09.03 val PER: 0.1829
2026-01-03 23:40:16,618: t15.2023.09.24 val PER: 0.1396
2026-01-03 23:40:16,618: t15.2023.09.29 val PER: 0.1474
2026-01-03 23:40:16,619: t15.2023.10.01 val PER: 0.1995
2026-01-03 23:40:16,619: t15.2023.10.06 val PER: 0.1076
2026-01-03 23:40:16,619: t15.2023.10.08 val PER: 0.2666
2026-01-03 23:40:16,619: t15.2023.10.13 val PER: 0.2327
2026-01-03 23:40:16,619: t15.2023.10.15 val PER: 0.1826
2026-01-03 23:40:16,619: t15.2023.10.20 val PER: 0.1846
2026-01-03 23:40:16,619: t15.2023.10.22 val PER: 0.1481
2026-01-03 23:40:16,619: t15.2023.11.03 val PER: 0.1940
2026-01-03 23:40:16,619: t15.2023.11.04 val PER: 0.0512
2026-01-03 23:40:16,619: t15.2023.11.17 val PER: 0.0607
2026-01-03 23:40:16,619: t15.2023.11.19 val PER: 0.0399
2026-01-03 23:40:16,619: t15.2023.11.26 val PER: 0.1638
2026-01-03 23:40:16,619: t15.2023.12.03 val PER: 0.1387
2026-01-03 23:40:16,619: t15.2023.12.08 val PER: 0.1365
2026-01-03 23:40:16,620: t15.2023.12.10 val PER: 0.1170
2026-01-03 23:40:16,620: t15.2023.12.17 val PER: 0.1653
2026-01-03 23:40:16,620: t15.2023.12.29 val PER: 0.1682
2026-01-03 23:40:16,620: t15.2024.02.25 val PER: 0.1461
2026-01-03 23:40:16,620: t15.2024.03.08 val PER: 0.2660
2026-01-03 23:40:16,620: t15.2024.03.15 val PER: 0.2276
2026-01-03 23:40:16,620: t15.2024.03.17 val PER: 0.1771
2026-01-03 23:40:16,620: t15.2024.05.10 val PER: 0.1991
2026-01-03 23:40:16,620: t15.2024.06.14 val PER: 0.1861
2026-01-03 23:40:16,620: t15.2024.07.19 val PER: 0.2690
2026-01-03 23:40:16,620: t15.2024.07.21 val PER: 0.1186
2026-01-03 23:40:16,620: t15.2024.07.28 val PER: 0.1684
2026-01-03 23:40:16,620: t15.2025.01.10 val PER: 0.3361
2026-01-03 23:40:16,620: t15.2025.01.12 val PER: 0.1878
2026-01-03 23:40:16,621: t15.2025.03.14 val PER: 0.3358
2026-01-03 23:40:16,621: t15.2025.03.16 val PER: 0.2147
2026-01-03 23:40:16,621: t15.2025.03.30 val PER: 0.3299
2026-01-03 23:40:16,621: t15.2025.04.13 val PER: 0.2454
2026-01-03 23:40:16,622: New best val WER(1gram) 53.05% --> 52.03%
2026-01-03 23:40:16,622: Checkpointing model
2026-01-03 23:40:17,234: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/best_checkpoint
2026-01-03 23:40:17,475: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_8500
2026-01-03 23:40:26,167: Train batch 8600: loss: 14.06 grad norm: 59.40 time: 0.054
2026-01-03 23:40:43,772: Train batch 8800: loss: 14.09 grad norm: 58.75 time: 0.061
2026-01-03 23:41:01,357: Train batch 9000: loss: 15.09 grad norm: 69.16 time: 0.072
2026-01-03 23:41:01,358: Running test after training batch: 9000
2026-01-03 23:41:01,453: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:41:06,292: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-03 23:41:06,324: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 23:41:08,051: Val batch 9000: PER (avg): 0.1731 CTC Loss (avg): 17.1798 WER(1gram): 52.03% (n=64) time: 6.692
2026-01-03 23:41:08,051: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 23:41:08,051: t15.2023.08.13 val PER: 0.1331
2026-01-03 23:41:08,051: t15.2023.08.18 val PER: 0.1333
2026-01-03 23:41:08,051: t15.2023.08.20 val PER: 0.1303
2026-01-03 23:41:08,051: t15.2023.08.25 val PER: 0.1009
2026-01-03 23:41:08,051: t15.2023.08.27 val PER: 0.1994
2026-01-03 23:41:08,051: t15.2023.09.01 val PER: 0.0966
2026-01-03 23:41:08,051: t15.2023.09.03 val PER: 0.1770
2026-01-03 23:41:08,052: t15.2023.09.24 val PER: 0.1468
2026-01-03 23:41:08,052: t15.2023.09.29 val PER: 0.1474
2026-01-03 23:41:08,052: t15.2023.10.01 val PER: 0.1995
2026-01-03 23:41:08,052: t15.2023.10.06 val PER: 0.1012
2026-01-03 23:41:08,052: t15.2023.10.08 val PER: 0.2585
2026-01-03 23:41:08,052: t15.2023.10.13 val PER: 0.2234
2026-01-03 23:41:08,052: t15.2023.10.15 val PER: 0.1727
2026-01-03 23:41:08,052: t15.2023.10.20 val PER: 0.1812
2026-01-03 23:41:08,052: t15.2023.10.22 val PER: 0.1247
2026-01-03 23:41:08,052: t15.2023.11.03 val PER: 0.1940
2026-01-03 23:41:08,053: t15.2023.11.04 val PER: 0.0444
2026-01-03 23:41:08,053: t15.2023.11.17 val PER: 0.0498
2026-01-03 23:41:08,053: t15.2023.11.19 val PER: 0.0459
2026-01-03 23:41:08,053: t15.2023.11.26 val PER: 0.1717
2026-01-03 23:41:08,053: t15.2023.12.03 val PER: 0.1387
2026-01-03 23:41:08,053: t15.2023.12.08 val PER: 0.1212
2026-01-03 23:41:08,053: t15.2023.12.10 val PER: 0.1117
2026-01-03 23:41:08,053: t15.2023.12.17 val PER: 0.1674
2026-01-03 23:41:08,053: t15.2023.12.29 val PER: 0.1606
2026-01-03 23:41:08,053: t15.2024.02.25 val PER: 0.1348
2026-01-03 23:41:08,053: t15.2024.03.08 val PER: 0.2546
2026-01-03 23:41:08,053: t15.2024.03.15 val PER: 0.2251
2026-01-03 23:41:08,053: t15.2024.03.17 val PER: 0.1702
2026-01-03 23:41:08,053: t15.2024.05.10 val PER: 0.1828
2026-01-03 23:41:08,053: t15.2024.06.14 val PER: 0.1814
2026-01-03 23:41:08,053: t15.2024.07.19 val PER: 0.2703
2026-01-03 23:41:08,054: t15.2024.07.21 val PER: 0.1124
2026-01-03 23:41:08,054: t15.2024.07.28 val PER: 0.1618
2026-01-03 23:41:08,054: t15.2025.01.10 val PER: 0.3196
2026-01-03 23:41:08,054: t15.2025.01.12 val PER: 0.1832
2026-01-03 23:41:08,054: t15.2025.03.14 val PER: 0.3388
2026-01-03 23:41:08,054: t15.2025.03.16 val PER: 0.2120
2026-01-03 23:41:08,054: t15.2025.03.30 val PER: 0.3115
2026-01-03 23:41:08,054: t15.2025.04.13 val PER: 0.2454
2026-01-03 23:41:08,288: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_9000
2026-01-03 23:41:25,584: Train batch 9200: loss: 10.51 grad norm: 55.68 time: 0.056
2026-01-03 23:41:43,047: Train batch 9400: loss: 6.39 grad norm: 44.03 time: 0.068
2026-01-03 23:41:51,792: Running test after training batch: 9500
2026-01-03 23:41:51,886: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:41:56,606: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 23:41:56,636: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 23:41:58,341: Val batch 9500: PER (avg): 0.1721 CTC Loss (avg): 17.2258 WER(1gram): 49.49% (n=64) time: 6.549
2026-01-03 23:41:58,342: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 23:41:58,342: t15.2023.08.13 val PER: 0.1279
2026-01-03 23:41:58,342: t15.2023.08.18 val PER: 0.1232
2026-01-03 23:41:58,342: t15.2023.08.20 val PER: 0.1295
2026-01-03 23:41:58,342: t15.2023.08.25 val PER: 0.1009
2026-01-03 23:41:58,342: t15.2023.08.27 val PER: 0.2074
2026-01-03 23:41:58,342: t15.2023.09.01 val PER: 0.0942
2026-01-03 23:41:58,342: t15.2023.09.03 val PER: 0.1663
2026-01-03 23:41:58,342: t15.2023.09.24 val PER: 0.1456
2026-01-03 23:41:58,342: t15.2023.09.29 val PER: 0.1378
2026-01-03 23:41:58,343: t15.2023.10.01 val PER: 0.1955
2026-01-03 23:41:58,343: t15.2023.10.06 val PER: 0.1055
2026-01-03 23:41:58,343: t15.2023.10.08 val PER: 0.2625
2026-01-03 23:41:58,343: t15.2023.10.13 val PER: 0.2273
2026-01-03 23:41:58,343: t15.2023.10.15 val PER: 0.1800
2026-01-03 23:41:58,343: t15.2023.10.20 val PER: 0.1846
2026-01-03 23:41:58,343: t15.2023.10.22 val PER: 0.1292
2026-01-03 23:41:58,343: t15.2023.11.03 val PER: 0.1954
2026-01-03 23:41:58,343: t15.2023.11.04 val PER: 0.0444
2026-01-03 23:41:58,343: t15.2023.11.17 val PER: 0.0513
2026-01-03 23:41:58,343: t15.2023.11.19 val PER: 0.0519
2026-01-03 23:41:58,343: t15.2023.11.26 val PER: 0.1449
2026-01-03 23:41:58,343: t15.2023.12.03 val PER: 0.1376
2026-01-03 23:41:58,344: t15.2023.12.08 val PER: 0.1218
2026-01-03 23:41:58,344: t15.2023.12.10 val PER: 0.1117
2026-01-03 23:41:58,344: t15.2023.12.17 val PER: 0.1580
2026-01-03 23:41:58,344: t15.2023.12.29 val PER: 0.1640
2026-01-03 23:41:58,344: t15.2024.02.25 val PER: 0.1320
2026-01-03 23:41:58,344: t15.2024.03.08 val PER: 0.2461
2026-01-03 23:41:58,344: t15.2024.03.15 val PER: 0.2176
2026-01-03 23:41:58,344: t15.2024.03.17 val PER: 0.1653
2026-01-03 23:41:58,344: t15.2024.05.10 val PER: 0.1947
2026-01-03 23:41:58,344: t15.2024.06.14 val PER: 0.1798
2026-01-03 23:41:58,344: t15.2024.07.19 val PER: 0.2558
2026-01-03 23:41:58,344: t15.2024.07.21 val PER: 0.1200
2026-01-03 23:41:58,344: t15.2024.07.28 val PER: 0.1676
2026-01-03 23:41:58,344: t15.2025.01.10 val PER: 0.3182
2026-01-03 23:41:58,345: t15.2025.01.12 val PER: 0.1932
2026-01-03 23:41:58,345: t15.2025.03.14 val PER: 0.3683
2026-01-03 23:41:58,345: t15.2025.03.16 val PER: 0.2107
2026-01-03 23:41:58,345: t15.2025.03.30 val PER: 0.3172
2026-01-03 23:41:58,345: t15.2025.04.13 val PER: 0.2439
2026-01-03 23:41:58,346: New best val WER(1gram) 52.03% --> 49.49%
2026-01-03 23:41:58,346: Checkpointing model
2026-01-03 23:41:58,971: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/best_checkpoint
2026-01-03 23:41:59,214: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_9500
2026-01-03 23:42:07,628: Train batch 9600: loss: 8.00 grad norm: 49.98 time: 0.073
2026-01-03 23:42:24,813: Train batch 9800: loss: 11.32 grad norm: 62.43 time: 0.063
2026-01-03 23:42:42,193: Train batch 10000: loss: 5.32 grad norm: 43.78 time: 0.060
2026-01-03 23:42:42,193: Running test after training batch: 10000
2026-01-03 23:42:42,288: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:42:46,988: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 23:42:47,019: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-03 23:42:48,731: Val batch 10000: PER (avg): 0.1674 CTC Loss (avg): 16.7412 WER(1gram): 53.55% (n=64) time: 6.538
2026-01-03 23:42:48,732: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-03 23:42:48,732: t15.2023.08.13 val PER: 0.1279
2026-01-03 23:42:48,732: t15.2023.08.18 val PER: 0.1215
2026-01-03 23:42:48,732: t15.2023.08.20 val PER: 0.1255
2026-01-03 23:42:48,732: t15.2023.08.25 val PER: 0.1009
2026-01-03 23:42:48,732: t15.2023.08.27 val PER: 0.2074
2026-01-03 23:42:48,732: t15.2023.09.01 val PER: 0.0860
2026-01-03 23:42:48,733: t15.2023.09.03 val PER: 0.1746
2026-01-03 23:42:48,733: t15.2023.09.24 val PER: 0.1347
2026-01-03 23:42:48,733: t15.2023.09.29 val PER: 0.1404
2026-01-03 23:42:48,733: t15.2023.10.01 val PER: 0.1803
2026-01-03 23:42:48,736: t15.2023.10.06 val PER: 0.0969
2026-01-03 23:42:48,736: t15.2023.10.08 val PER: 0.2463
2026-01-03 23:42:48,736: t15.2023.10.13 val PER: 0.2203
2026-01-03 23:42:48,736: t15.2023.10.15 val PER: 0.1681
2026-01-03 23:42:48,737: t15.2023.10.20 val PER: 0.1812
2026-01-03 23:42:48,737: t15.2023.10.22 val PER: 0.1214
2026-01-03 23:42:48,737: t15.2023.11.03 val PER: 0.1872
2026-01-03 23:42:48,737: t15.2023.11.04 val PER: 0.0410
2026-01-03 23:42:48,737: t15.2023.11.17 val PER: 0.0482
2026-01-03 23:42:48,737: t15.2023.11.19 val PER: 0.0519
2026-01-03 23:42:48,737: t15.2023.11.26 val PER: 0.1486
2026-01-03 23:42:48,737: t15.2023.12.03 val PER: 0.1376
2026-01-03 23:42:48,737: t15.2023.12.08 val PER: 0.1258
2026-01-03 23:42:48,737: t15.2023.12.10 val PER: 0.1064
2026-01-03 23:42:48,737: t15.2023.12.17 val PER: 0.1601
2026-01-03 23:42:48,737: t15.2023.12.29 val PER: 0.1455
2026-01-03 23:42:48,738: t15.2024.02.25 val PER: 0.1376
2026-01-03 23:42:48,738: t15.2024.03.08 val PER: 0.2546
2026-01-03 23:42:48,738: t15.2024.03.15 val PER: 0.2276
2026-01-03 23:42:48,738: t15.2024.03.17 val PER: 0.1604
2026-01-03 23:42:48,738: t15.2024.05.10 val PER: 0.1649
2026-01-03 23:42:48,738: t15.2024.06.14 val PER: 0.1877
2026-01-03 23:42:48,738: t15.2024.07.19 val PER: 0.2643
2026-01-03 23:42:48,738: t15.2024.07.21 val PER: 0.1083
2026-01-03 23:42:48,738: t15.2024.07.28 val PER: 0.1529
2026-01-03 23:42:48,738: t15.2025.01.10 val PER: 0.3058
2026-01-03 23:42:48,738: t15.2025.01.12 val PER: 0.1878
2026-01-03 23:42:48,738: t15.2025.03.14 val PER: 0.3402
2026-01-03 23:42:48,738: t15.2025.03.16 val PER: 0.2016
2026-01-03 23:42:48,738: t15.2025.03.30 val PER: 0.3161
2026-01-03 23:42:48,739: t15.2025.04.13 val PER: 0.2354
2026-01-03 23:42:48,973: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_10000
2026-01-03 23:43:06,047: Train batch 10200: loss: 5.85 grad norm: 40.82 time: 0.050
2026-01-03 23:43:23,456: Train batch 10400: loss: 8.20 grad norm: 48.00 time: 0.071
2026-01-03 23:43:32,262: Running test after training batch: 10500
2026-01-03 23:43:32,387: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:43:37,171: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 23:43:37,203: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-03 23:43:38,958: Val batch 10500: PER (avg): 0.1661 CTC Loss (avg): 16.7908 WER(1gram): 49.75% (n=64) time: 6.696
2026-01-03 23:43:38,958: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 23:43:38,959: t15.2023.08.13 val PER: 0.1289
2026-01-03 23:43:38,959: t15.2023.08.18 val PER: 0.1266
2026-01-03 23:43:38,959: t15.2023.08.20 val PER: 0.1271
2026-01-03 23:43:38,959: t15.2023.08.25 val PER: 0.1054
2026-01-03 23:43:38,959: t15.2023.08.27 val PER: 0.1929
2026-01-03 23:43:38,959: t15.2023.09.01 val PER: 0.0933
2026-01-03 23:43:38,959: t15.2023.09.03 val PER: 0.1651
2026-01-03 23:43:38,959: t15.2023.09.24 val PER: 0.1359
2026-01-03 23:43:38,959: t15.2023.09.29 val PER: 0.1410
2026-01-03 23:43:38,959: t15.2023.10.01 val PER: 0.1863
2026-01-03 23:43:38,959: t15.2023.10.06 val PER: 0.0936
2026-01-03 23:43:38,960: t15.2023.10.08 val PER: 0.2382
2026-01-03 23:43:38,960: t15.2023.10.13 val PER: 0.2126
2026-01-03 23:43:38,960: t15.2023.10.15 val PER: 0.1747
2026-01-03 23:43:38,960: t15.2023.10.20 val PER: 0.1779
2026-01-03 23:43:38,960: t15.2023.10.22 val PER: 0.1169
2026-01-03 23:43:38,960: t15.2023.11.03 val PER: 0.1934
2026-01-03 23:43:38,960: t15.2023.11.04 val PER: 0.0410
2026-01-03 23:43:38,960: t15.2023.11.17 val PER: 0.0482
2026-01-03 23:43:38,960: t15.2023.11.19 val PER: 0.0579
2026-01-03 23:43:38,960: t15.2023.11.26 val PER: 0.1406
2026-01-03 23:43:38,960: t15.2023.12.03 val PER: 0.1345
2026-01-03 23:43:38,960: t15.2023.12.08 val PER: 0.1125
2026-01-03 23:43:38,960: t15.2023.12.10 val PER: 0.1012
2026-01-03 23:43:38,960: t15.2023.12.17 val PER: 0.1476
2026-01-03 23:43:38,960: t15.2023.12.29 val PER: 0.1558
2026-01-03 23:43:38,960: t15.2024.02.25 val PER: 0.1264
2026-01-03 23:43:38,960: t15.2024.03.08 val PER: 0.2603
2026-01-03 23:43:38,961: t15.2024.03.15 val PER: 0.2208
2026-01-03 23:43:38,961: t15.2024.03.17 val PER: 0.1597
2026-01-03 23:43:38,961: t15.2024.05.10 val PER: 0.1768
2026-01-03 23:43:38,961: t15.2024.06.14 val PER: 0.1924
2026-01-03 23:43:38,961: t15.2024.07.19 val PER: 0.2696
2026-01-03 23:43:38,961: t15.2024.07.21 val PER: 0.1055
2026-01-03 23:43:38,961: t15.2024.07.28 val PER: 0.1412
2026-01-03 23:43:38,961: t15.2025.01.10 val PER: 0.3237
2026-01-03 23:43:38,961: t15.2025.01.12 val PER: 0.1863
2026-01-03 23:43:38,961: t15.2025.03.14 val PER: 0.3402
2026-01-03 23:43:38,961: t15.2025.03.16 val PER: 0.1950
2026-01-03 23:43:38,961: t15.2025.03.30 val PER: 0.2977
2026-01-03 23:43:38,961: t15.2025.04.13 val PER: 0.2297
2026-01-03 23:43:39,194: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_10500
2026-01-03 23:43:48,021: Train batch 10600: loss: 8.38 grad norm: 57.85 time: 0.072
2026-01-03 23:44:05,296: Train batch 10800: loss: 13.85 grad norm: 71.66 time: 0.064
2026-01-03 23:44:22,487: Train batch 11000: loss: 13.39 grad norm: 69.41 time: 0.057
2026-01-03 23:44:22,487: Running test after training batch: 11000
2026-01-03 23:44:22,632: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:44:27,313: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-03 23:44:27,345: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 23:44:29,061: Val batch 11000: PER (avg): 0.1622 CTC Loss (avg): 16.5620 WER(1gram): 47.72% (n=64) time: 6.573
2026-01-03 23:44:29,061: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-03 23:44:29,061: t15.2023.08.13 val PER: 0.1310
2026-01-03 23:44:29,061: t15.2023.08.18 val PER: 0.1123
2026-01-03 23:44:29,061: t15.2023.08.20 val PER: 0.1223
2026-01-03 23:44:29,061: t15.2023.08.25 val PER: 0.1039
2026-01-03 23:44:29,061: t15.2023.08.27 val PER: 0.2074
2026-01-03 23:44:29,062: t15.2023.09.01 val PER: 0.0812
2026-01-03 23:44:29,062: t15.2023.09.03 val PER: 0.1710
2026-01-03 23:44:29,062: t15.2023.09.24 val PER: 0.1299
2026-01-03 23:44:29,062: t15.2023.09.29 val PER: 0.1366
2026-01-03 23:44:29,062: t15.2023.10.01 val PER: 0.1830
2026-01-03 23:44:29,062: t15.2023.10.06 val PER: 0.0969
2026-01-03 23:44:29,062: t15.2023.10.08 val PER: 0.2395
2026-01-03 23:44:29,062: t15.2023.10.13 val PER: 0.2056
2026-01-03 23:44:29,062: t15.2023.10.15 val PER: 0.1694
2026-01-03 23:44:29,062: t15.2023.10.20 val PER: 0.1946
2026-01-03 23:44:29,063: t15.2023.10.22 val PER: 0.1125
2026-01-03 23:44:29,063: t15.2023.11.03 val PER: 0.1879
2026-01-03 23:44:29,063: t15.2023.11.04 val PER: 0.0375
2026-01-03 23:44:29,063: t15.2023.11.17 val PER: 0.0404
2026-01-03 23:44:29,063: t15.2023.11.19 val PER: 0.0459
2026-01-03 23:44:29,063: t15.2023.11.26 val PER: 0.1341
2026-01-03 23:44:29,063: t15.2023.12.03 val PER: 0.1292
2026-01-03 23:44:29,063: t15.2023.12.08 val PER: 0.1092
2026-01-03 23:44:29,063: t15.2023.12.10 val PER: 0.1012
2026-01-03 23:44:29,063: t15.2023.12.17 val PER: 0.1580
2026-01-03 23:44:29,063: t15.2023.12.29 val PER: 0.1482
2026-01-03 23:44:29,063: t15.2024.02.25 val PER: 0.1306
2026-01-03 23:44:29,063: t15.2024.03.08 val PER: 0.2447
2026-01-03 23:44:29,064: t15.2024.03.15 val PER: 0.2126
2026-01-03 23:44:29,064: t15.2024.03.17 val PER: 0.1527
2026-01-03 23:44:29,064: t15.2024.05.10 val PER: 0.1724
2026-01-03 23:44:29,064: t15.2024.06.14 val PER: 0.1830
2026-01-03 23:44:29,064: t15.2024.07.19 val PER: 0.2544
2026-01-03 23:44:29,064: t15.2024.07.21 val PER: 0.1110
2026-01-03 23:44:29,064: t15.2024.07.28 val PER: 0.1507
2026-01-03 23:44:29,064: t15.2025.01.10 val PER: 0.3072
2026-01-03 23:44:29,064: t15.2025.01.12 val PER: 0.1724
2026-01-03 23:44:29,064: t15.2025.03.14 val PER: 0.3328
2026-01-03 23:44:29,064: t15.2025.03.16 val PER: 0.2003
2026-01-03 23:44:29,064: t15.2025.03.30 val PER: 0.3069
2026-01-03 23:44:29,064: t15.2025.04.13 val PER: 0.2254
2026-01-03 23:44:29,065: New best val WER(1gram) 49.49% --> 47.72%
2026-01-03 23:44:29,065: Checkpointing model
2026-01-03 23:44:29,683: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/best_checkpoint
2026-01-03 23:44:29,926: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_11000
2026-01-03 23:44:47,247: Train batch 11200: loss: 10.20 grad norm: 56.27 time: 0.071
2026-01-03 23:45:04,283: Train batch 11400: loss: 8.49 grad norm: 54.62 time: 0.056
2026-01-03 23:45:12,986: Running test after training batch: 11500
2026-01-03 23:45:13,091: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:45:17,812: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 23:45:17,844: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-03 23:45:19,620: Val batch 11500: PER (avg): 0.1619 CTC Loss (avg): 16.4994 WER(1gram): 47.72% (n=64) time: 6.634
2026-01-03 23:45:19,621: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-03 23:45:19,621: t15.2023.08.13 val PER: 0.1164
2026-01-03 23:45:19,621: t15.2023.08.18 val PER: 0.1140
2026-01-03 23:45:19,621: t15.2023.08.20 val PER: 0.1207
2026-01-03 23:45:19,621: t15.2023.08.25 val PER: 0.1069
2026-01-03 23:45:19,621: t15.2023.08.27 val PER: 0.1913
2026-01-03 23:45:19,621: t15.2023.09.01 val PER: 0.0877
2026-01-03 23:45:19,621: t15.2023.09.03 val PER: 0.1663
2026-01-03 23:45:19,621: t15.2023.09.24 val PER: 0.1262
2026-01-03 23:45:19,621: t15.2023.09.29 val PER: 0.1340
2026-01-03 23:45:19,622: t15.2023.10.01 val PER: 0.1849
2026-01-03 23:45:19,622: t15.2023.10.06 val PER: 0.0904
2026-01-03 23:45:19,622: t15.2023.10.08 val PER: 0.2517
2026-01-03 23:45:19,622: t15.2023.10.13 val PER: 0.2102
2026-01-03 23:45:19,622: t15.2023.10.15 val PER: 0.1707
2026-01-03 23:45:19,622: t15.2023.10.20 val PER: 0.1846
2026-01-03 23:45:19,622: t15.2023.10.22 val PER: 0.1292
2026-01-03 23:45:19,622: t15.2023.11.03 val PER: 0.1845
2026-01-03 23:45:19,622: t15.2023.11.04 val PER: 0.0375
2026-01-03 23:45:19,622: t15.2023.11.17 val PER: 0.0513
2026-01-03 23:45:19,622: t15.2023.11.19 val PER: 0.0459
2026-01-03 23:45:19,622: t15.2023.11.26 val PER: 0.1297
2026-01-03 23:45:19,622: t15.2023.12.03 val PER: 0.1271
2026-01-03 23:45:19,622: t15.2023.12.08 val PER: 0.1072
2026-01-03 23:45:19,622: t15.2023.12.10 val PER: 0.1025
2026-01-03 23:45:19,622: t15.2023.12.17 val PER: 0.1570
2026-01-03 23:45:19,623: t15.2023.12.29 val PER: 0.1407
2026-01-03 23:45:19,623: t15.2024.02.25 val PER: 0.1250
2026-01-03 23:45:19,623: t15.2024.03.08 val PER: 0.2319
2026-01-03 23:45:19,623: t15.2024.03.15 val PER: 0.2289
2026-01-03 23:45:19,623: t15.2024.03.17 val PER: 0.1562
2026-01-03 23:45:19,623: t15.2024.05.10 val PER: 0.1768
2026-01-03 23:45:19,623: t15.2024.06.14 val PER: 0.1830
2026-01-03 23:45:19,623: t15.2024.07.19 val PER: 0.2512
2026-01-03 23:45:19,623: t15.2024.07.21 val PER: 0.0986
2026-01-03 23:45:19,623: t15.2024.07.28 val PER: 0.1419
2026-01-03 23:45:19,623: t15.2025.01.10 val PER: 0.3058
2026-01-03 23:45:19,623: t15.2025.01.12 val PER: 0.1701
2026-01-03 23:45:19,623: t15.2025.03.14 val PER: 0.3521
2026-01-03 23:45:19,623: t15.2025.03.16 val PER: 0.2029
2026-01-03 23:45:19,624: t15.2025.03.30 val PER: 0.3126
2026-01-03 23:45:19,624: t15.2025.04.13 val PER: 0.2297
2026-01-03 23:45:19,858: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_11500
2026-01-03 23:45:28,139: Train batch 11600: loss: 10.23 grad norm: 55.34 time: 0.060
2026-01-03 23:45:45,143: Train batch 11800: loss: 5.86 grad norm: 43.99 time: 0.044
2026-01-03 23:46:02,288: Train batch 12000: loss: 13.01 grad norm: 59.21 time: 0.071
2026-01-03 23:46:02,288: Running test after training batch: 12000
2026-01-03 23:46:02,385: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:46:07,091: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 23:46:07,123: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sette
2026-01-03 23:46:08,886: Val batch 12000: PER (avg): 0.1587 CTC Loss (avg): 16.2609 WER(1gram): 48.73% (n=64) time: 6.598
2026-01-03 23:46:08,887: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 23:46:08,887: t15.2023.08.13 val PER: 0.1195
2026-01-03 23:46:08,887: t15.2023.08.18 val PER: 0.1090
2026-01-03 23:46:08,887: t15.2023.08.20 val PER: 0.1191
2026-01-03 23:46:08,887: t15.2023.08.25 val PER: 0.1024
2026-01-03 23:46:08,887: t15.2023.08.27 val PER: 0.1913
2026-01-03 23:46:08,887: t15.2023.09.01 val PER: 0.0828
2026-01-03 23:46:08,887: t15.2023.09.03 val PER: 0.1615
2026-01-03 23:46:08,887: t15.2023.09.24 val PER: 0.1347
2026-01-03 23:46:08,888: t15.2023.09.29 val PER: 0.1308
2026-01-03 23:46:08,888: t15.2023.10.01 val PER: 0.1731
2026-01-03 23:46:08,888: t15.2023.10.06 val PER: 0.0883
2026-01-03 23:46:08,888: t15.2023.10.08 val PER: 0.2476
2026-01-03 23:46:08,888: t15.2023.10.13 val PER: 0.2040
2026-01-03 23:46:08,888: t15.2023.10.15 val PER: 0.1608
2026-01-03 23:46:08,888: t15.2023.10.20 val PER: 0.1812
2026-01-03 23:46:08,888: t15.2023.10.22 val PER: 0.1169
2026-01-03 23:46:08,888: t15.2023.11.03 val PER: 0.1832
2026-01-03 23:46:08,888: t15.2023.11.04 val PER: 0.0205
2026-01-03 23:46:08,888: t15.2023.11.17 val PER: 0.0513
2026-01-03 23:46:08,888: t15.2023.11.19 val PER: 0.0299
2026-01-03 23:46:08,888: t15.2023.11.26 val PER: 0.1290
2026-01-03 23:46:08,888: t15.2023.12.03 val PER: 0.1239
2026-01-03 23:46:08,888: t15.2023.12.08 val PER: 0.1112
2026-01-03 23:46:08,889: t15.2023.12.10 val PER: 0.1012
2026-01-03 23:46:08,889: t15.2023.12.17 val PER: 0.1435
2026-01-03 23:46:08,889: t15.2023.12.29 val PER: 0.1421
2026-01-03 23:46:08,889: t15.2024.02.25 val PER: 0.1194
2026-01-03 23:46:08,889: t15.2024.03.08 val PER: 0.2290
2026-01-03 23:46:08,889: t15.2024.03.15 val PER: 0.2208
2026-01-03 23:46:08,889: t15.2024.03.17 val PER: 0.1506
2026-01-03 23:46:08,889: t15.2024.05.10 val PER: 0.1664
2026-01-03 23:46:08,889: t15.2024.06.14 val PER: 0.1861
2026-01-03 23:46:08,889: t15.2024.07.19 val PER: 0.2465
2026-01-03 23:46:08,889: t15.2024.07.21 val PER: 0.1034
2026-01-03 23:46:08,889: t15.2024.07.28 val PER: 0.1456
2026-01-03 23:46:08,889: t15.2025.01.10 val PER: 0.3072
2026-01-03 23:46:08,889: t15.2025.01.12 val PER: 0.1701
2026-01-03 23:46:08,889: t15.2025.03.14 val PER: 0.3447
2026-01-03 23:46:08,889: t15.2025.03.16 val PER: 0.2094
2026-01-03 23:46:08,889: t15.2025.03.30 val PER: 0.3069
2026-01-03 23:46:08,890: t15.2025.04.13 val PER: 0.2282
2026-01-03 23:46:09,127: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_12000
2026-01-03 23:46:26,353: Train batch 12200: loss: 4.39 grad norm: 38.02 time: 0.065
2026-01-03 23:46:43,282: Train batch 12400: loss: 4.45 grad norm: 37.14 time: 0.041
2026-01-03 23:46:52,238: Running test after training batch: 12500
2026-01-03 23:46:52,380: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:46:57,061: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the could at this point as will
2026-01-03 23:46:57,094: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 23:46:58,874: Val batch 12500: PER (avg): 0.1563 CTC Loss (avg): 16.0055 WER(1gram): 44.67% (n=64) time: 6.635
2026-01-03 23:46:58,874: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 23:46:58,875: t15.2023.08.13 val PER: 0.1216
2026-01-03 23:46:58,875: t15.2023.08.18 val PER: 0.1106
2026-01-03 23:46:58,875: t15.2023.08.20 val PER: 0.1183
2026-01-03 23:46:58,875: t15.2023.08.25 val PER: 0.0919
2026-01-03 23:46:58,875: t15.2023.08.27 val PER: 0.1881
2026-01-03 23:46:58,875: t15.2023.09.01 val PER: 0.0755
2026-01-03 23:46:58,875: t15.2023.09.03 val PER: 0.1544
2026-01-03 23:46:58,875: t15.2023.09.24 val PER: 0.1323
2026-01-03 23:46:58,875: t15.2023.09.29 val PER: 0.1264
2026-01-03 23:46:58,875: t15.2023.10.01 val PER: 0.1697
2026-01-03 23:46:58,875: t15.2023.10.06 val PER: 0.0915
2026-01-03 23:46:58,875: t15.2023.10.08 val PER: 0.2490
2026-01-03 23:46:58,876: t15.2023.10.13 val PER: 0.2009
2026-01-03 23:46:58,876: t15.2023.10.15 val PER: 0.1569
2026-01-03 23:46:58,876: t15.2023.10.20 val PER: 0.1812
2026-01-03 23:46:58,876: t15.2023.10.22 val PER: 0.1102
2026-01-03 23:46:58,876: t15.2023.11.03 val PER: 0.1798
2026-01-03 23:46:58,876: t15.2023.11.04 val PER: 0.0307
2026-01-03 23:46:58,876: t15.2023.11.17 val PER: 0.0498
2026-01-03 23:46:58,876: t15.2023.11.19 val PER: 0.0259
2026-01-03 23:46:58,876: t15.2023.11.26 val PER: 0.1312
2026-01-03 23:46:58,876: t15.2023.12.03 val PER: 0.1124
2026-01-03 23:46:58,876: t15.2023.12.08 val PER: 0.1032
2026-01-03 23:46:58,876: t15.2023.12.10 val PER: 0.0933
2026-01-03 23:46:58,877: t15.2023.12.17 val PER: 0.1486
2026-01-03 23:46:58,877: t15.2023.12.29 val PER: 0.1476
2026-01-03 23:46:58,877: t15.2024.02.25 val PER: 0.1236
2026-01-03 23:46:58,877: t15.2024.03.08 val PER: 0.2304
2026-01-03 23:46:58,877: t15.2024.03.15 val PER: 0.2226
2026-01-03 23:46:58,877: t15.2024.03.17 val PER: 0.1590
2026-01-03 23:46:58,877: t15.2024.05.10 val PER: 0.1664
2026-01-03 23:46:58,877: t15.2024.06.14 val PER: 0.1814
2026-01-03 23:46:58,877: t15.2024.07.19 val PER: 0.2465
2026-01-03 23:46:58,877: t15.2024.07.21 val PER: 0.0952
2026-01-03 23:46:58,877: t15.2024.07.28 val PER: 0.1412
2026-01-03 23:46:58,877: t15.2025.01.10 val PER: 0.2934
2026-01-03 23:46:58,877: t15.2025.01.12 val PER: 0.1563
2026-01-03 23:46:58,877: t15.2025.03.14 val PER: 0.3462
2026-01-03 23:46:58,877: t15.2025.03.16 val PER: 0.1872
2026-01-03 23:46:58,877: t15.2025.03.30 val PER: 0.3115
2026-01-03 23:46:58,878: t15.2025.04.13 val PER: 0.2368
2026-01-03 23:46:58,878: New best val WER(1gram) 47.72% --> 44.67%
2026-01-03 23:46:58,878: Checkpointing model
2026-01-03 23:46:59,499: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/best_checkpoint
2026-01-03 23:46:59,743: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_12500
2026-01-03 23:47:08,373: Train batch 12600: loss: 7.00 grad norm: 45.32 time: 0.057
2026-01-03 23:47:25,566: Train batch 12800: loss: 5.06 grad norm: 42.58 time: 0.052
2026-01-03 23:47:43,054: Train batch 13000: loss: 5.51 grad norm: 43.68 time: 0.066
2026-01-03 23:47:43,054: Running test after training batch: 13000
2026-01-03 23:47:43,165: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:47:47,837: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-03 23:47:47,870: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 23:47:49,625: Val batch 13000: PER (avg): 0.1534 CTC Loss (avg): 15.7909 WER(1gram): 46.19% (n=64) time: 6.571
2026-01-03 23:47:49,626: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-03 23:47:49,626: t15.2023.08.13 val PER: 0.1237
2026-01-03 23:47:49,626: t15.2023.08.18 val PER: 0.1073
2026-01-03 23:47:49,626: t15.2023.08.20 val PER: 0.1041
2026-01-03 23:47:49,626: t15.2023.08.25 val PER: 0.0979
2026-01-03 23:47:49,626: t15.2023.08.27 val PER: 0.1785
2026-01-03 23:47:49,626: t15.2023.09.01 val PER: 0.0682
2026-01-03 23:47:49,626: t15.2023.09.03 val PER: 0.1675
2026-01-03 23:47:49,627: t15.2023.09.24 val PER: 0.1226
2026-01-03 23:47:49,627: t15.2023.09.29 val PER: 0.1244
2026-01-03 23:47:49,627: t15.2023.10.01 val PER: 0.1684
2026-01-03 23:47:49,627: t15.2023.10.06 val PER: 0.0969
2026-01-03 23:47:49,627: t15.2023.10.08 val PER: 0.2355
2026-01-03 23:47:49,627: t15.2023.10.13 val PER: 0.1986
2026-01-03 23:47:49,627: t15.2023.10.15 val PER: 0.1549
2026-01-03 23:47:49,627: t15.2023.10.20 val PER: 0.1678
2026-01-03 23:47:49,627: t15.2023.10.22 val PER: 0.1180
2026-01-03 23:47:49,627: t15.2023.11.03 val PER: 0.1818
2026-01-03 23:47:49,627: t15.2023.11.04 val PER: 0.0375
2026-01-03 23:47:49,628: t15.2023.11.17 val PER: 0.0420
2026-01-03 23:47:49,628: t15.2023.11.19 val PER: 0.0379
2026-01-03 23:47:49,628: t15.2023.11.26 val PER: 0.1268
2026-01-03 23:47:49,628: t15.2023.12.03 val PER: 0.1271
2026-01-03 23:47:49,628: t15.2023.12.08 val PER: 0.1005
2026-01-03 23:47:49,628: t15.2023.12.10 val PER: 0.1025
2026-01-03 23:47:49,628: t15.2023.12.17 val PER: 0.1372
2026-01-03 23:47:49,628: t15.2023.12.29 val PER: 0.1414
2026-01-03 23:47:49,628: t15.2024.02.25 val PER: 0.1208
2026-01-03 23:47:49,628: t15.2024.03.08 val PER: 0.2276
2026-01-03 23:47:49,628: t15.2024.03.15 val PER: 0.2164
2026-01-03 23:47:49,628: t15.2024.03.17 val PER: 0.1409
2026-01-03 23:47:49,628: t15.2024.05.10 val PER: 0.1620
2026-01-03 23:47:49,628: t15.2024.06.14 val PER: 0.1703
2026-01-03 23:47:49,628: t15.2024.07.19 val PER: 0.2393
2026-01-03 23:47:49,628: t15.2024.07.21 val PER: 0.0924
2026-01-03 23:47:49,628: t15.2024.07.28 val PER: 0.1529
2026-01-03 23:47:49,629: t15.2025.01.10 val PER: 0.2975
2026-01-03 23:47:49,629: t15.2025.01.12 val PER: 0.1601
2026-01-03 23:47:49,629: t15.2025.03.14 val PER: 0.3269
2026-01-03 23:47:49,629: t15.2025.03.16 val PER: 0.1767
2026-01-03 23:47:49,629: t15.2025.03.30 val PER: 0.3080
2026-01-03 23:47:49,629: t15.2025.04.13 val PER: 0.2211
2026-01-03 23:47:49,868: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_13000
2026-01-03 23:48:06,936: Train batch 13200: loss: 11.03 grad norm: 65.84 time: 0.054
2026-01-03 23:48:24,143: Train batch 13400: loss: 8.22 grad norm: 59.16 time: 0.061
2026-01-03 23:48:32,923: Running test after training batch: 13500
2026-01-03 23:48:33,050: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:48:37,935: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the could at this point as will
2026-01-03 23:48:37,968: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost get
2026-01-03 23:48:39,792: Val batch 13500: PER (avg): 0.1534 CTC Loss (avg): 15.6013 WER(1gram): 47.21% (n=64) time: 6.869
2026-01-03 23:48:39,792: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-03 23:48:39,793: t15.2023.08.13 val PER: 0.1102
2026-01-03 23:48:39,793: t15.2023.08.18 val PER: 0.1115
2026-01-03 23:48:39,793: t15.2023.08.20 val PER: 0.1088
2026-01-03 23:48:39,793: t15.2023.08.25 val PER: 0.1024
2026-01-03 23:48:39,793: t15.2023.08.27 val PER: 0.1833
2026-01-03 23:48:39,793: t15.2023.09.01 val PER: 0.0812
2026-01-03 23:48:39,793: t15.2023.09.03 val PER: 0.1698
2026-01-03 23:48:39,793: t15.2023.09.24 val PER: 0.1226
2026-01-03 23:48:39,793: t15.2023.09.29 val PER: 0.1232
2026-01-03 23:48:39,793: t15.2023.10.01 val PER: 0.1737
2026-01-03 23:48:39,794: t15.2023.10.06 val PER: 0.0936
2026-01-03 23:48:39,794: t15.2023.10.08 val PER: 0.2395
2026-01-03 23:48:39,794: t15.2023.10.13 val PER: 0.2017
2026-01-03 23:48:39,794: t15.2023.10.15 val PER: 0.1635
2026-01-03 23:48:39,794: t15.2023.10.20 val PER: 0.1711
2026-01-03 23:48:39,794: t15.2023.10.22 val PER: 0.1192
2026-01-03 23:48:39,794: t15.2023.11.03 val PER: 0.1757
2026-01-03 23:48:39,794: t15.2023.11.04 val PER: 0.0307
2026-01-03 23:48:39,794: t15.2023.11.17 val PER: 0.0513
2026-01-03 23:48:39,794: t15.2023.11.19 val PER: 0.0279
2026-01-03 23:48:39,794: t15.2023.11.26 val PER: 0.1210
2026-01-03 23:48:39,794: t15.2023.12.03 val PER: 0.1082
2026-01-03 23:48:39,794: t15.2023.12.08 val PER: 0.1005
2026-01-03 23:48:39,794: t15.2023.12.10 val PER: 0.1012
2026-01-03 23:48:39,794: t15.2023.12.17 val PER: 0.1310
2026-01-03 23:48:39,795: t15.2023.12.29 val PER: 0.1421
2026-01-03 23:48:39,795: t15.2024.02.25 val PER: 0.1208
2026-01-03 23:48:39,795: t15.2024.03.08 val PER: 0.2475
2026-01-03 23:48:39,795: t15.2024.03.15 val PER: 0.2101
2026-01-03 23:48:39,795: t15.2024.03.17 val PER: 0.1499
2026-01-03 23:48:39,795: t15.2024.05.10 val PER: 0.1560
2026-01-03 23:48:39,795: t15.2024.06.14 val PER: 0.1609
2026-01-03 23:48:39,795: t15.2024.07.19 val PER: 0.2373
2026-01-03 23:48:39,795: t15.2024.07.21 val PER: 0.0959
2026-01-03 23:48:39,795: t15.2024.07.28 val PER: 0.1375
2026-01-03 23:48:39,795: t15.2025.01.10 val PER: 0.3099
2026-01-03 23:48:39,795: t15.2025.01.12 val PER: 0.1586
2026-01-03 23:48:39,796: t15.2025.03.14 val PER: 0.3402
2026-01-03 23:48:39,796: t15.2025.03.16 val PER: 0.1846
2026-01-03 23:48:39,796: t15.2025.03.30 val PER: 0.2943
2026-01-03 23:48:39,796: t15.2025.04.13 val PER: 0.2211
2026-01-03 23:48:40,029: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_13500
2026-01-03 23:48:48,613: Train batch 13600: loss: 11.87 grad norm: 76.19 time: 0.062
2026-01-03 23:49:05,927: Train batch 13800: loss: 7.70 grad norm: 55.38 time: 0.056
2026-01-03 23:49:23,096: Train batch 14000: loss: 11.29 grad norm: 75.00 time: 0.050
2026-01-03 23:49:23,096: Running test after training batch: 14000
2026-01-03 23:49:23,214: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:49:27,940: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 23:49:27,974: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost get
2026-01-03 23:49:29,836: Val batch 14000: PER (avg): 0.1523 CTC Loss (avg): 15.7426 WER(1gram): 47.21% (n=64) time: 6.740
2026-01-03 23:49:29,837: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 23:49:29,837: t15.2023.08.13 val PER: 0.1206
2026-01-03 23:49:29,837: t15.2023.08.18 val PER: 0.0997
2026-01-03 23:49:29,837: t15.2023.08.20 val PER: 0.1104
2026-01-03 23:49:29,837: t15.2023.08.25 val PER: 0.1009
2026-01-03 23:49:29,837: t15.2023.08.27 val PER: 0.1736
2026-01-03 23:49:29,837: t15.2023.09.01 val PER: 0.0755
2026-01-03 23:49:29,837: t15.2023.09.03 val PER: 0.1710
2026-01-03 23:49:29,837: t15.2023.09.24 val PER: 0.1311
2026-01-03 23:49:29,838: t15.2023.09.29 val PER: 0.1257
2026-01-03 23:49:29,838: t15.2023.10.01 val PER: 0.1724
2026-01-03 23:49:29,838: t15.2023.10.06 val PER: 0.0915
2026-01-03 23:49:29,838: t15.2023.10.08 val PER: 0.2409
2026-01-03 23:49:29,838: t15.2023.10.13 val PER: 0.2033
2026-01-03 23:49:29,838: t15.2023.10.15 val PER: 0.1681
2026-01-03 23:49:29,838: t15.2023.10.20 val PER: 0.1779
2026-01-03 23:49:29,838: t15.2023.10.22 val PER: 0.1203
2026-01-03 23:49:29,838: t15.2023.11.03 val PER: 0.1744
2026-01-03 23:49:29,838: t15.2023.11.04 val PER: 0.0341
2026-01-03 23:49:29,838: t15.2023.11.17 val PER: 0.0513
2026-01-03 23:49:29,838: t15.2023.11.19 val PER: 0.0379
2026-01-03 23:49:29,838: t15.2023.11.26 val PER: 0.1225
2026-01-03 23:49:29,838: t15.2023.12.03 val PER: 0.1166
2026-01-03 23:49:29,838: t15.2023.12.08 val PER: 0.0979
2026-01-03 23:49:29,839: t15.2023.12.10 val PER: 0.0986
2026-01-03 23:49:29,839: t15.2023.12.17 val PER: 0.1310
2026-01-03 23:49:29,839: t15.2023.12.29 val PER: 0.1345
2026-01-03 23:49:29,839: t15.2024.02.25 val PER: 0.1264
2026-01-03 23:49:29,839: t15.2024.03.08 val PER: 0.2191
2026-01-03 23:49:29,839: t15.2024.03.15 val PER: 0.2095
2026-01-03 23:49:29,840: t15.2024.03.17 val PER: 0.1430
2026-01-03 23:49:29,840: t15.2024.05.10 val PER: 0.1560
2026-01-03 23:49:29,840: t15.2024.06.14 val PER: 0.1703
2026-01-03 23:49:29,840: t15.2024.07.19 val PER: 0.2380
2026-01-03 23:49:29,840: t15.2024.07.21 val PER: 0.0910
2026-01-03 23:49:29,840: t15.2024.07.28 val PER: 0.1360
2026-01-03 23:49:29,840: t15.2025.01.10 val PER: 0.3058
2026-01-03 23:49:29,840: t15.2025.01.12 val PER: 0.1586
2026-01-03 23:49:29,840: t15.2025.03.14 val PER: 0.3328
2026-01-03 23:49:29,840: t15.2025.03.16 val PER: 0.1741
2026-01-03 23:49:29,840: t15.2025.03.30 val PER: 0.2943
2026-01-03 23:49:29,840: t15.2025.04.13 val PER: 0.2140
2026-01-03 23:49:30,094: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_14000
2026-01-03 23:49:47,147: Train batch 14200: loss: 7.29 grad norm: 56.62 time: 0.056
2026-01-03 23:50:04,448: Train batch 14400: loss: 5.17 grad norm: 42.73 time: 0.064
2026-01-03 23:50:13,121: Running test after training batch: 14500
2026-01-03 23:50:13,220: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:50:17,920: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 23:50:17,954: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-03 23:50:19,812: Val batch 14500: PER (avg): 0.1533 CTC Loss (avg): 15.7387 WER(1gram): 46.70% (n=64) time: 6.690
2026-01-03 23:50:19,812: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 23:50:19,812: t15.2023.08.13 val PER: 0.1175
2026-01-03 23:50:19,812: t15.2023.08.18 val PER: 0.1031
2026-01-03 23:50:19,813: t15.2023.08.20 val PER: 0.1136
2026-01-03 23:50:19,813: t15.2023.08.25 val PER: 0.0934
2026-01-03 23:50:19,813: t15.2023.08.27 val PER: 0.1720
2026-01-03 23:50:19,813: t15.2023.09.01 val PER: 0.0795
2026-01-03 23:50:19,813: t15.2023.09.03 val PER: 0.1603
2026-01-03 23:50:19,813: t15.2023.09.24 val PER: 0.1299
2026-01-03 23:50:19,813: t15.2023.09.29 val PER: 0.1257
2026-01-03 23:50:19,813: t15.2023.10.01 val PER: 0.1810
2026-01-03 23:50:19,813: t15.2023.10.06 val PER: 0.0850
2026-01-03 23:50:19,813: t15.2023.10.08 val PER: 0.2449
2026-01-03 23:50:19,813: t15.2023.10.13 val PER: 0.1947
2026-01-03 23:50:19,813: t15.2023.10.15 val PER: 0.1641
2026-01-03 23:50:19,813: t15.2023.10.20 val PER: 0.1644
2026-01-03 23:50:19,814: t15.2023.10.22 val PER: 0.1180
2026-01-03 23:50:19,814: t15.2023.11.03 val PER: 0.1757
2026-01-03 23:50:19,814: t15.2023.11.04 val PER: 0.0307
2026-01-03 23:50:19,814: t15.2023.11.17 val PER: 0.0513
2026-01-03 23:50:19,814: t15.2023.11.19 val PER: 0.0319
2026-01-03 23:50:19,814: t15.2023.11.26 val PER: 0.1239
2026-01-03 23:50:19,814: t15.2023.12.03 val PER: 0.1071
2026-01-03 23:50:19,814: t15.2023.12.08 val PER: 0.0985
2026-01-03 23:50:19,814: t15.2023.12.10 val PER: 0.0986
2026-01-03 23:50:19,814: t15.2023.12.17 val PER: 0.1466
2026-01-03 23:50:19,814: t15.2023.12.29 val PER: 0.1469
2026-01-03 23:50:19,814: t15.2024.02.25 val PER: 0.1152
2026-01-03 23:50:19,814: t15.2024.03.08 val PER: 0.2347
2026-01-03 23:50:19,815: t15.2024.03.15 val PER: 0.2108
2026-01-03 23:50:19,815: t15.2024.03.17 val PER: 0.1457
2026-01-03 23:50:19,815: t15.2024.05.10 val PER: 0.1620
2026-01-03 23:50:19,815: t15.2024.06.14 val PER: 0.1782
2026-01-03 23:50:19,815: t15.2024.07.19 val PER: 0.2419
2026-01-03 23:50:19,815: t15.2024.07.21 val PER: 0.0952
2026-01-03 23:50:19,815: t15.2024.07.28 val PER: 0.1331
2026-01-03 23:50:19,815: t15.2025.01.10 val PER: 0.3030
2026-01-03 23:50:19,815: t15.2025.01.12 val PER: 0.1609
2026-01-03 23:50:19,816: t15.2025.03.14 val PER: 0.3476
2026-01-03 23:50:19,816: t15.2025.03.16 val PER: 0.1793
2026-01-03 23:50:19,816: t15.2025.03.30 val PER: 0.2805
2026-01-03 23:50:19,816: t15.2025.04.13 val PER: 0.2268
2026-01-03 23:50:20,070: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_14500
2026-01-03 23:50:28,633: Train batch 14600: loss: 10.88 grad norm: 65.83 time: 0.058
2026-01-03 23:50:46,340: Train batch 14800: loss: 5.22 grad norm: 49.47 time: 0.050
2026-01-03 23:51:03,465: Train batch 15000: loss: 7.88 grad norm: 56.62 time: 0.052
2026-01-03 23:51:03,465: Running test after training batch: 15000
2026-01-03 23:51:03,583: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:51:08,754: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 23:51:08,789: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost get
2026-01-03 23:51:10,641: Val batch 15000: PER (avg): 0.1492 CTC Loss (avg): 15.4426 WER(1gram): 45.94% (n=64) time: 7.175
2026-01-03 23:51:10,641: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-03 23:51:10,641: t15.2023.08.13 val PER: 0.1154
2026-01-03 23:51:10,641: t15.2023.08.18 val PER: 0.0956
2026-01-03 23:51:10,641: t15.2023.08.20 val PER: 0.1072
2026-01-03 23:51:10,641: t15.2023.08.25 val PER: 0.0858
2026-01-03 23:51:10,641: t15.2023.08.27 val PER: 0.1704
2026-01-03 23:51:10,642: t15.2023.09.01 val PER: 0.0804
2026-01-03 23:51:10,642: t15.2023.09.03 val PER: 0.1496
2026-01-03 23:51:10,642: t15.2023.09.24 val PER: 0.1201
2026-01-03 23:51:10,642: t15.2023.09.29 val PER: 0.1238
2026-01-03 23:51:10,642: t15.2023.10.01 val PER: 0.1658
2026-01-03 23:51:10,642: t15.2023.10.06 val PER: 0.0797
2026-01-03 23:51:10,642: t15.2023.10.08 val PER: 0.2449
2026-01-03 23:51:10,642: t15.2023.10.13 val PER: 0.1924
2026-01-03 23:51:10,642: t15.2023.10.15 val PER: 0.1589
2026-01-03 23:51:10,642: t15.2023.10.20 val PER: 0.1745
2026-01-03 23:51:10,642: t15.2023.10.22 val PER: 0.1203
2026-01-03 23:51:10,642: t15.2023.11.03 val PER: 0.1771
2026-01-03 23:51:10,642: t15.2023.11.04 val PER: 0.0410
2026-01-03 23:51:10,642: t15.2023.11.17 val PER: 0.0482
2026-01-03 23:51:10,643: t15.2023.11.19 val PER: 0.0299
2026-01-03 23:51:10,643: t15.2023.11.26 val PER: 0.1116
2026-01-03 23:51:10,643: t15.2023.12.03 val PER: 0.1124
2026-01-03 23:51:10,643: t15.2023.12.08 val PER: 0.0992
2026-01-03 23:51:10,643: t15.2023.12.10 val PER: 0.0867
2026-01-03 23:51:10,643: t15.2023.12.17 val PER: 0.1445
2026-01-03 23:51:10,643: t15.2023.12.29 val PER: 0.1373
2026-01-03 23:51:10,643: t15.2024.02.25 val PER: 0.1180
2026-01-03 23:51:10,643: t15.2024.03.08 val PER: 0.2248
2026-01-03 23:51:10,643: t15.2024.03.15 val PER: 0.2033
2026-01-03 23:51:10,643: t15.2024.03.17 val PER: 0.1402
2026-01-03 23:51:10,644: t15.2024.05.10 val PER: 0.1575
2026-01-03 23:51:10,644: t15.2024.06.14 val PER: 0.1640
2026-01-03 23:51:10,644: t15.2024.07.19 val PER: 0.2360
2026-01-03 23:51:10,644: t15.2024.07.21 val PER: 0.0931
2026-01-03 23:51:10,644: t15.2024.07.28 val PER: 0.1272
2026-01-03 23:51:10,644: t15.2025.01.10 val PER: 0.2961
2026-01-03 23:51:10,644: t15.2025.01.12 val PER: 0.1540
2026-01-03 23:51:10,644: t15.2025.03.14 val PER: 0.3402
2026-01-03 23:51:10,644: t15.2025.03.16 val PER: 0.1780
2026-01-03 23:51:10,644: t15.2025.03.30 val PER: 0.2966
2026-01-03 23:51:10,644: t15.2025.04.13 val PER: 0.2297
2026-01-03 23:51:10,898: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_15000
2026-01-03 23:51:28,132: Train batch 15200: loss: 3.96 grad norm: 40.82 time: 0.057
2026-01-03 23:51:45,176: Train batch 15400: loss: 10.40 grad norm: 64.82 time: 0.049
2026-01-03 23:51:54,099: Running test after training batch: 15500
2026-01-03 23:51:54,270: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:51:58,955: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 23:51:58,990: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost get
2026-01-03 23:52:00,858: Val batch 15500: PER (avg): 0.1480 CTC Loss (avg): 15.2959 WER(1gram): 45.69% (n=64) time: 6.758
2026-01-03 23:52:00,858: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 23:52:00,858: t15.2023.08.13 val PER: 0.1081
2026-01-03 23:52:00,858: t15.2023.08.18 val PER: 0.1006
2026-01-03 23:52:00,858: t15.2023.08.20 val PER: 0.1120
2026-01-03 23:52:00,859: t15.2023.08.25 val PER: 0.0949
2026-01-03 23:52:00,859: t15.2023.08.27 val PER: 0.1817
2026-01-03 23:52:00,859: t15.2023.09.01 val PER: 0.0722
2026-01-03 23:52:00,859: t15.2023.09.03 val PER: 0.1520
2026-01-03 23:52:00,859: t15.2023.09.24 val PER: 0.1129
2026-01-03 23:52:00,859: t15.2023.09.29 val PER: 0.1174
2026-01-03 23:52:00,859: t15.2023.10.01 val PER: 0.1750
2026-01-03 23:52:00,859: t15.2023.10.06 val PER: 0.0850
2026-01-03 23:52:00,859: t15.2023.10.08 val PER: 0.2382
2026-01-03 23:52:00,859: t15.2023.10.13 val PER: 0.1978
2026-01-03 23:52:00,859: t15.2023.10.15 val PER: 0.1582
2026-01-03 23:52:00,859: t15.2023.10.20 val PER: 0.1779
2026-01-03 23:52:00,860: t15.2023.10.22 val PER: 0.1147
2026-01-03 23:52:00,860: t15.2023.11.03 val PER: 0.1737
2026-01-03 23:52:00,860: t15.2023.11.04 val PER: 0.0341
2026-01-03 23:52:00,860: t15.2023.11.17 val PER: 0.0513
2026-01-03 23:52:00,860: t15.2023.11.19 val PER: 0.0279
2026-01-03 23:52:00,860: t15.2023.11.26 val PER: 0.1159
2026-01-03 23:52:00,860: t15.2023.12.03 val PER: 0.1103
2026-01-03 23:52:00,860: t15.2023.12.08 val PER: 0.0919
2026-01-03 23:52:00,860: t15.2023.12.10 val PER: 0.0841
2026-01-03 23:52:00,860: t15.2023.12.17 val PER: 0.1383
2026-01-03 23:52:00,860: t15.2023.12.29 val PER: 0.1311
2026-01-03 23:52:00,860: t15.2024.02.25 val PER: 0.1067
2026-01-03 23:52:00,860: t15.2024.03.08 val PER: 0.2333
2026-01-03 23:52:00,860: t15.2024.03.15 val PER: 0.2064
2026-01-03 23:52:00,860: t15.2024.03.17 val PER: 0.1381
2026-01-03 23:52:00,861: t15.2024.05.10 val PER: 0.1545
2026-01-03 23:52:00,861: t15.2024.06.14 val PER: 0.1625
2026-01-03 23:52:00,861: t15.2024.07.19 val PER: 0.2248
2026-01-03 23:52:00,861: t15.2024.07.21 val PER: 0.0952
2026-01-03 23:52:00,861: t15.2024.07.28 val PER: 0.1375
2026-01-03 23:52:00,861: t15.2025.01.10 val PER: 0.2893
2026-01-03 23:52:00,861: t15.2025.01.12 val PER: 0.1501
2026-01-03 23:52:00,861: t15.2025.03.14 val PER: 0.3314
2026-01-03 23:52:00,861: t15.2025.03.16 val PER: 0.1859
2026-01-03 23:52:00,861: t15.2025.03.30 val PER: 0.2908
2026-01-03 23:52:00,861: t15.2025.04.13 val PER: 0.2183
2026-01-03 23:52:01,114: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_15500
2026-01-03 23:52:09,860: Train batch 15600: loss: 9.88 grad norm: 61.51 time: 0.062
2026-01-03 23:52:27,076: Train batch 15800: loss: 11.97 grad norm: 66.20 time: 0.067
2026-01-03 23:52:44,632: Train batch 16000: loss: 8.25 grad norm: 50.26 time: 0.055
2026-01-03 23:52:44,632: Running test after training batch: 16000
2026-01-03 23:52:44,722: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:52:49,427: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 23:52:49,461: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 23:52:51,346: Val batch 16000: PER (avg): 0.1490 CTC Loss (avg): 15.3950 WER(1gram): 47.21% (n=64) time: 6.713
2026-01-03 23:52:51,346: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-03 23:52:51,346: t15.2023.08.13 val PER: 0.1123
2026-01-03 23:52:51,346: t15.2023.08.18 val PER: 0.1073
2026-01-03 23:52:51,347: t15.2023.08.20 val PER: 0.1096
2026-01-03 23:52:51,347: t15.2023.08.25 val PER: 0.0934
2026-01-03 23:52:51,347: t15.2023.08.27 val PER: 0.1817
2026-01-03 23:52:51,347: t15.2023.09.01 val PER: 0.0731
2026-01-03 23:52:51,347: t15.2023.09.03 val PER: 0.1556
2026-01-03 23:52:51,347: t15.2023.09.24 val PER: 0.1201
2026-01-03 23:52:51,347: t15.2023.09.29 val PER: 0.1213
2026-01-03 23:52:51,347: t15.2023.10.01 val PER: 0.1671
2026-01-03 23:52:51,347: t15.2023.10.06 val PER: 0.0840
2026-01-03 23:52:51,347: t15.2023.10.08 val PER: 0.2368
2026-01-03 23:52:51,347: t15.2023.10.13 val PER: 0.1947
2026-01-03 23:52:51,347: t15.2023.10.15 val PER: 0.1549
2026-01-03 23:52:51,347: t15.2023.10.20 val PER: 0.1644
2026-01-03 23:52:51,347: t15.2023.10.22 val PER: 0.1125
2026-01-03 23:52:51,348: t15.2023.11.03 val PER: 0.1730
2026-01-03 23:52:51,348: t15.2023.11.04 val PER: 0.0341
2026-01-03 23:52:51,348: t15.2023.11.17 val PER: 0.0498
2026-01-03 23:52:51,348: t15.2023.11.19 val PER: 0.0339
2026-01-03 23:52:51,348: t15.2023.11.26 val PER: 0.1094
2026-01-03 23:52:51,348: t15.2023.12.03 val PER: 0.1092
2026-01-03 23:52:51,348: t15.2023.12.08 val PER: 0.0965
2026-01-03 23:52:51,348: t15.2023.12.10 val PER: 0.0972
2026-01-03 23:52:51,348: t15.2023.12.17 val PER: 0.1435
2026-01-03 23:52:51,348: t15.2023.12.29 val PER: 0.1352
2026-01-03 23:52:51,348: t15.2024.02.25 val PER: 0.1180
2026-01-03 23:52:51,348: t15.2024.03.08 val PER: 0.2290
2026-01-03 23:52:51,348: t15.2024.03.15 val PER: 0.2039
2026-01-03 23:52:51,348: t15.2024.03.17 val PER: 0.1367
2026-01-03 23:52:51,349: t15.2024.05.10 val PER: 0.1590
2026-01-03 23:52:51,349: t15.2024.06.14 val PER: 0.1640
2026-01-03 23:52:51,349: t15.2024.07.19 val PER: 0.2360
2026-01-03 23:52:51,349: t15.2024.07.21 val PER: 0.0979
2026-01-03 23:52:51,349: t15.2024.07.28 val PER: 0.1338
2026-01-03 23:52:51,349: t15.2025.01.10 val PER: 0.2865
2026-01-03 23:52:51,349: t15.2025.01.12 val PER: 0.1570
2026-01-03 23:52:51,349: t15.2025.03.14 val PER: 0.3388
2026-01-03 23:52:51,349: t15.2025.03.16 val PER: 0.1780
2026-01-03 23:52:51,349: t15.2025.03.30 val PER: 0.2977
2026-01-03 23:52:51,349: t15.2025.04.13 val PER: 0.2126
2026-01-03 23:52:51,606: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_16000
2026-01-03 23:53:09,031: Train batch 16200: loss: 5.81 grad norm: 47.36 time: 0.055
2026-01-03 23:53:26,198: Train batch 16400: loss: 9.48 grad norm: 67.34 time: 0.057
2026-01-03 23:53:34,974: Running test after training batch: 16500
2026-01-03 23:53:35,093: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:53:40,018: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-03 23:53:40,052: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost get
2026-01-03 23:53:41,967: Val batch 16500: PER (avg): 0.1477 CTC Loss (avg): 15.2599 WER(1gram): 45.94% (n=64) time: 6.992
2026-01-03 23:53:41,968: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 23:53:41,968: t15.2023.08.13 val PER: 0.1102
2026-01-03 23:53:41,968: t15.2023.08.18 val PER: 0.1039
2026-01-03 23:53:41,968: t15.2023.08.20 val PER: 0.1080
2026-01-03 23:53:41,968: t15.2023.08.25 val PER: 0.0828
2026-01-03 23:53:41,968: t15.2023.08.27 val PER: 0.1752
2026-01-03 23:53:41,969: t15.2023.09.01 val PER: 0.0787
2026-01-03 23:53:41,969: t15.2023.09.03 val PER: 0.1544
2026-01-03 23:53:41,969: t15.2023.09.24 val PER: 0.1286
2026-01-03 23:53:41,969: t15.2023.09.29 val PER: 0.1168
2026-01-03 23:53:41,969: t15.2023.10.01 val PER: 0.1631
2026-01-03 23:53:41,969: t15.2023.10.06 val PER: 0.0893
2026-01-03 23:53:41,969: t15.2023.10.08 val PER: 0.2395
2026-01-03 23:53:41,969: t15.2023.10.13 val PER: 0.1831
2026-01-03 23:53:41,969: t15.2023.10.15 val PER: 0.1516
2026-01-03 23:53:41,969: t15.2023.10.20 val PER: 0.1678
2026-01-03 23:53:41,969: t15.2023.10.22 val PER: 0.1169
2026-01-03 23:53:41,970: t15.2023.11.03 val PER: 0.1764
2026-01-03 23:53:41,970: t15.2023.11.04 val PER: 0.0341
2026-01-03 23:53:41,970: t15.2023.11.17 val PER: 0.0482
2026-01-03 23:53:41,970: t15.2023.11.19 val PER: 0.0319
2026-01-03 23:53:41,970: t15.2023.11.26 val PER: 0.1138
2026-01-03 23:53:41,970: t15.2023.12.03 val PER: 0.1082
2026-01-03 23:53:41,970: t15.2023.12.08 val PER: 0.0872
2026-01-03 23:53:41,970: t15.2023.12.10 val PER: 0.0894
2026-01-03 23:53:41,970: t15.2023.12.17 val PER: 0.1351
2026-01-03 23:53:41,970: t15.2023.12.29 val PER: 0.1277
2026-01-03 23:53:41,970: t15.2024.02.25 val PER: 0.1110
2026-01-03 23:53:41,971: t15.2024.03.08 val PER: 0.2262
2026-01-03 23:53:41,971: t15.2024.03.15 val PER: 0.2114
2026-01-03 23:53:41,971: t15.2024.03.17 val PER: 0.1444
2026-01-03 23:53:41,971: t15.2024.05.10 val PER: 0.1456
2026-01-03 23:53:41,971: t15.2024.06.14 val PER: 0.1703
2026-01-03 23:53:41,971: t15.2024.07.19 val PER: 0.2347
2026-01-03 23:53:41,971: t15.2024.07.21 val PER: 0.0938
2026-01-03 23:53:41,971: t15.2024.07.28 val PER: 0.1375
2026-01-03 23:53:41,971: t15.2025.01.10 val PER: 0.2975
2026-01-03 23:53:41,972: t15.2025.01.12 val PER: 0.1532
2026-01-03 23:53:41,972: t15.2025.03.14 val PER: 0.3299
2026-01-03 23:53:41,972: t15.2025.03.16 val PER: 0.1767
2026-01-03 23:53:41,972: t15.2025.03.30 val PER: 0.2943
2026-01-03 23:53:41,972: t15.2025.04.13 val PER: 0.2168
2026-01-03 23:53:42,226: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_16500
2026-01-03 23:53:50,898: Train batch 16600: loss: 6.64 grad norm: 51.17 time: 0.052
2026-01-03 23:54:08,305: Train batch 16800: loss: 14.57 grad norm: 81.50 time: 0.062
2026-01-03 23:54:25,706: Train batch 17000: loss: 7.10 grad norm: 51.14 time: 0.082
2026-01-03 23:54:25,706: Running test after training batch: 17000
2026-01-03 23:54:25,800: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:54:30,496: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 23:54:30,531: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-03 23:54:32,464: Val batch 17000: PER (avg): 0.1459 CTC Loss (avg): 15.1212 WER(1gram): 46.45% (n=64) time: 6.758
2026-01-03 23:54:32,465: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 23:54:32,465: t15.2023.08.13 val PER: 0.1040
2026-01-03 23:54:32,465: t15.2023.08.18 val PER: 0.1081
2026-01-03 23:54:32,465: t15.2023.08.20 val PER: 0.1088
2026-01-03 23:54:32,465: t15.2023.08.25 val PER: 0.0889
2026-01-03 23:54:32,465: t15.2023.08.27 val PER: 0.1688
2026-01-03 23:54:32,465: t15.2023.09.01 val PER: 0.0836
2026-01-03 23:54:32,465: t15.2023.09.03 val PER: 0.1508
2026-01-03 23:54:32,466: t15.2023.09.24 val PER: 0.1153
2026-01-03 23:54:32,466: t15.2023.09.29 val PER: 0.1168
2026-01-03 23:54:32,466: t15.2023.10.01 val PER: 0.1691
2026-01-03 23:54:32,466: t15.2023.10.06 val PER: 0.0850
2026-01-03 23:54:32,466: t15.2023.10.08 val PER: 0.2422
2026-01-03 23:54:32,466: t15.2023.10.13 val PER: 0.1908
2026-01-03 23:54:32,466: t15.2023.10.15 val PER: 0.1575
2026-01-03 23:54:32,466: t15.2023.10.20 val PER: 0.1644
2026-01-03 23:54:32,466: t15.2023.10.22 val PER: 0.1192
2026-01-03 23:54:32,466: t15.2023.11.03 val PER: 0.1723
2026-01-03 23:54:32,466: t15.2023.11.04 val PER: 0.0410
2026-01-03 23:54:32,466: t15.2023.11.17 val PER: 0.0467
2026-01-03 23:54:32,466: t15.2023.11.19 val PER: 0.0240
2026-01-03 23:54:32,466: t15.2023.11.26 val PER: 0.1065
2026-01-03 23:54:32,467: t15.2023.12.03 val PER: 0.1103
2026-01-03 23:54:32,467: t15.2023.12.08 val PER: 0.0839
2026-01-03 23:54:32,467: t15.2023.12.10 val PER: 0.0815
2026-01-03 23:54:32,467: t15.2023.12.17 val PER: 0.1362
2026-01-03 23:54:32,467: t15.2023.12.29 val PER: 0.1235
2026-01-03 23:54:32,467: t15.2024.02.25 val PER: 0.1067
2026-01-03 23:54:32,467: t15.2024.03.08 val PER: 0.2248
2026-01-03 23:54:32,467: t15.2024.03.15 val PER: 0.2039
2026-01-03 23:54:32,467: t15.2024.03.17 val PER: 0.1409
2026-01-03 23:54:32,467: t15.2024.05.10 val PER: 0.1590
2026-01-03 23:54:32,467: t15.2024.06.14 val PER: 0.1593
2026-01-03 23:54:32,467: t15.2024.07.19 val PER: 0.2274
2026-01-03 23:54:32,467: t15.2024.07.21 val PER: 0.0903
2026-01-03 23:54:32,467: t15.2024.07.28 val PER: 0.1331
2026-01-03 23:54:32,467: t15.2025.01.10 val PER: 0.2893
2026-01-03 23:54:32,468: t15.2025.01.12 val PER: 0.1478
2026-01-03 23:54:32,468: t15.2025.03.14 val PER: 0.3240
2026-01-03 23:54:32,468: t15.2025.03.16 val PER: 0.1741
2026-01-03 23:54:32,468: t15.2025.03.30 val PER: 0.2954
2026-01-03 23:54:32,468: t15.2025.04.13 val PER: 0.2154
2026-01-03 23:54:32,718: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_17000
2026-01-03 23:54:50,006: Train batch 17200: loss: 8.76 grad norm: 56.17 time: 0.084
2026-01-03 23:55:07,433: Train batch 17400: loss: 9.85 grad norm: 62.45 time: 0.071
2026-01-03 23:55:16,005: Running test after training batch: 17500
2026-01-03 23:55:16,147: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:55:20,866: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 23:55:20,901: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost get
2026-01-03 23:55:22,819: Val batch 17500: PER (avg): 0.1448 CTC Loss (avg): 15.1474 WER(1gram): 45.18% (n=64) time: 6.814
2026-01-03 23:55:22,819: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=12
2026-01-03 23:55:22,820: t15.2023.08.13 val PER: 0.1081
2026-01-03 23:55:22,820: t15.2023.08.18 val PER: 0.0989
2026-01-03 23:55:22,820: t15.2023.08.20 val PER: 0.1056
2026-01-03 23:55:22,820: t15.2023.08.25 val PER: 0.0783
2026-01-03 23:55:22,820: t15.2023.08.27 val PER: 0.1688
2026-01-03 23:55:22,820: t15.2023.09.01 val PER: 0.0787
2026-01-03 23:55:22,820: t15.2023.09.03 val PER: 0.1544
2026-01-03 23:55:22,820: t15.2023.09.24 val PER: 0.1141
2026-01-03 23:55:22,820: t15.2023.09.29 val PER: 0.1200
2026-01-03 23:55:22,820: t15.2023.10.01 val PER: 0.1684
2026-01-03 23:55:22,820: t15.2023.10.06 val PER: 0.0775
2026-01-03 23:55:22,821: t15.2023.10.08 val PER: 0.2395
2026-01-03 23:55:22,821: t15.2023.10.13 val PER: 0.1862
2026-01-03 23:55:22,821: t15.2023.10.15 val PER: 0.1496
2026-01-03 23:55:22,821: t15.2023.10.20 val PER: 0.1711
2026-01-03 23:55:22,821: t15.2023.10.22 val PER: 0.1114
2026-01-03 23:55:22,821: t15.2023.11.03 val PER: 0.1703
2026-01-03 23:55:22,821: t15.2023.11.04 val PER: 0.0307
2026-01-03 23:55:22,821: t15.2023.11.17 val PER: 0.0451
2026-01-03 23:55:22,821: t15.2023.11.19 val PER: 0.0279
2026-01-03 23:55:22,821: t15.2023.11.26 val PER: 0.1065
2026-01-03 23:55:22,821: t15.2023.12.03 val PER: 0.1040
2026-01-03 23:55:22,821: t15.2023.12.08 val PER: 0.0885
2026-01-03 23:55:22,821: t15.2023.12.10 val PER: 0.0867
2026-01-03 23:55:22,821: t15.2023.12.17 val PER: 0.1289
2026-01-03 23:55:22,821: t15.2023.12.29 val PER: 0.1311
2026-01-03 23:55:22,822: t15.2024.02.25 val PER: 0.1053
2026-01-03 23:55:22,822: t15.2024.03.08 val PER: 0.2233
2026-01-03 23:55:22,822: t15.2024.03.15 val PER: 0.2026
2026-01-03 23:55:22,822: t15.2024.03.17 val PER: 0.1388
2026-01-03 23:55:22,822: t15.2024.05.10 val PER: 0.1560
2026-01-03 23:55:22,822: t15.2024.06.14 val PER: 0.1546
2026-01-03 23:55:22,822: t15.2024.07.19 val PER: 0.2314
2026-01-03 23:55:22,822: t15.2024.07.21 val PER: 0.0862
2026-01-03 23:55:22,822: t15.2024.07.28 val PER: 0.1272
2026-01-03 23:55:22,822: t15.2025.01.10 val PER: 0.2906
2026-01-03 23:55:22,822: t15.2025.01.12 val PER: 0.1532
2026-01-03 23:55:22,822: t15.2025.03.14 val PER: 0.3358
2026-01-03 23:55:22,822: t15.2025.03.16 val PER: 0.1741
2026-01-03 23:55:22,822: t15.2025.03.30 val PER: 0.2966
2026-01-03 23:55:22,822: t15.2025.04.13 val PER: 0.2183
2026-01-03 23:55:23,077: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_17500
2026-01-03 23:55:31,707: Train batch 17600: loss: 8.77 grad norm: 64.21 time: 0.051
2026-01-03 23:55:49,503: Train batch 17800: loss: 6.05 grad norm: 50.79 time: 0.042
2026-01-03 23:56:06,755: Train batch 18000: loss: 9.15 grad norm: 64.31 time: 0.061
2026-01-03 23:56:06,755: Running test after training batch: 18000
2026-01-03 23:56:06,891: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:56:11,578: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 23:56:11,613: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 23:56:13,544: Val batch 18000: PER (avg): 0.1462 CTC Loss (avg): 15.1644 WER(1gram): 46.95% (n=64) time: 6.788
2026-01-03 23:56:13,544: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 23:56:13,545: t15.2023.08.13 val PER: 0.1008
2026-01-03 23:56:13,545: t15.2023.08.18 val PER: 0.1056
2026-01-03 23:56:13,545: t15.2023.08.20 val PER: 0.1056
2026-01-03 23:56:13,545: t15.2023.08.25 val PER: 0.0889
2026-01-03 23:56:13,545: t15.2023.08.27 val PER: 0.1817
2026-01-03 23:56:13,545: t15.2023.09.01 val PER: 0.0804
2026-01-03 23:56:13,545: t15.2023.09.03 val PER: 0.1508
2026-01-03 23:56:13,545: t15.2023.09.24 val PER: 0.1189
2026-01-03 23:56:13,545: t15.2023.09.29 val PER: 0.1174
2026-01-03 23:56:13,545: t15.2023.10.01 val PER: 0.1664
2026-01-03 23:56:13,545: t15.2023.10.06 val PER: 0.0775
2026-01-03 23:56:13,545: t15.2023.10.08 val PER: 0.2449
2026-01-03 23:56:13,545: t15.2023.10.13 val PER: 0.1955
2026-01-03 23:56:13,545: t15.2023.10.15 val PER: 0.1575
2026-01-03 23:56:13,545: t15.2023.10.20 val PER: 0.1711
2026-01-03 23:56:13,546: t15.2023.10.22 val PER: 0.1136
2026-01-03 23:56:13,546: t15.2023.11.03 val PER: 0.1737
2026-01-03 23:56:13,546: t15.2023.11.04 val PER: 0.0307
2026-01-03 23:56:13,546: t15.2023.11.17 val PER: 0.0467
2026-01-03 23:56:13,546: t15.2023.11.19 val PER: 0.0259
2026-01-03 23:56:13,546: t15.2023.11.26 val PER: 0.1072
2026-01-03 23:56:13,546: t15.2023.12.03 val PER: 0.1071
2026-01-03 23:56:13,546: t15.2023.12.08 val PER: 0.0892
2026-01-03 23:56:13,546: t15.2023.12.10 val PER: 0.0907
2026-01-03 23:56:13,546: t15.2023.12.17 val PER: 0.1351
2026-01-03 23:56:13,546: t15.2023.12.29 val PER: 0.1277
2026-01-03 23:56:13,546: t15.2024.02.25 val PER: 0.1011
2026-01-03 23:56:13,546: t15.2024.03.08 val PER: 0.2262
2026-01-03 23:56:13,546: t15.2024.03.15 val PER: 0.2083
2026-01-03 23:56:13,547: t15.2024.03.17 val PER: 0.1388
2026-01-03 23:56:13,547: t15.2024.05.10 val PER: 0.1590
2026-01-03 23:56:13,547: t15.2024.06.14 val PER: 0.1562
2026-01-03 23:56:13,547: t15.2024.07.19 val PER: 0.2320
2026-01-03 23:56:13,547: t15.2024.07.21 val PER: 0.0862
2026-01-03 23:56:13,547: t15.2024.07.28 val PER: 0.1287
2026-01-03 23:56:13,547: t15.2025.01.10 val PER: 0.2837
2026-01-03 23:56:13,547: t15.2025.01.12 val PER: 0.1470
2026-01-03 23:56:13,547: t15.2025.03.14 val PER: 0.3328
2026-01-03 23:56:13,547: t15.2025.03.16 val PER: 0.1806
2026-01-03 23:56:13,547: t15.2025.03.30 val PER: 0.2885
2026-01-03 23:56:13,547: t15.2025.04.13 val PER: 0.2254
2026-01-03 23:56:13,801: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_18000
2026-01-03 23:56:31,644: Train batch 18200: loss: 6.79 grad norm: 57.27 time: 0.073
2026-01-03 23:56:48,916: Train batch 18400: loss: 3.68 grad norm: 40.92 time: 0.058
2026-01-03 23:56:57,808: Running test after training batch: 18500
2026-01-03 23:56:57,950: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:57:02,664: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 23:57:02,699: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 23:57:04,640: Val batch 18500: PER (avg): 0.1458 CTC Loss (avg): 15.0825 WER(1gram): 46.95% (n=64) time: 6.831
2026-01-03 23:57:04,640: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 23:57:04,640: t15.2023.08.13 val PER: 0.1040
2026-01-03 23:57:04,640: t15.2023.08.18 val PER: 0.1048
2026-01-03 23:57:04,640: t15.2023.08.20 val PER: 0.1096
2026-01-03 23:57:04,640: t15.2023.08.25 val PER: 0.0828
2026-01-03 23:57:04,641: t15.2023.08.27 val PER: 0.1801
2026-01-03 23:57:04,641: t15.2023.09.01 val PER: 0.0787
2026-01-03 23:57:04,641: t15.2023.09.03 val PER: 0.1508
2026-01-03 23:57:04,641: t15.2023.09.24 val PER: 0.1189
2026-01-03 23:57:04,641: t15.2023.09.29 val PER: 0.1213
2026-01-03 23:57:04,641: t15.2023.10.01 val PER: 0.1691
2026-01-03 23:57:04,641: t15.2023.10.06 val PER: 0.0775
2026-01-03 23:57:04,641: t15.2023.10.08 val PER: 0.2395
2026-01-03 23:57:04,641: t15.2023.10.13 val PER: 0.1947
2026-01-03 23:57:04,641: t15.2023.10.15 val PER: 0.1516
2026-01-03 23:57:04,641: t15.2023.10.20 val PER: 0.1544
2026-01-03 23:57:04,641: t15.2023.10.22 val PER: 0.1125
2026-01-03 23:57:04,641: t15.2023.11.03 val PER: 0.1744
2026-01-03 23:57:04,641: t15.2023.11.04 val PER: 0.0375
2026-01-03 23:57:04,642: t15.2023.11.17 val PER: 0.0513
2026-01-03 23:57:04,642: t15.2023.11.19 val PER: 0.0299
2026-01-03 23:57:04,642: t15.2023.11.26 val PER: 0.1087
2026-01-03 23:57:04,642: t15.2023.12.03 val PER: 0.1124
2026-01-03 23:57:04,642: t15.2023.12.08 val PER: 0.0872
2026-01-03 23:57:04,642: t15.2023.12.10 val PER: 0.0880
2026-01-03 23:57:04,642: t15.2023.12.17 val PER: 0.1362
2026-01-03 23:57:04,642: t15.2023.12.29 val PER: 0.1235
2026-01-03 23:57:04,642: t15.2024.02.25 val PER: 0.1124
2026-01-03 23:57:04,642: t15.2024.03.08 val PER: 0.2191
2026-01-03 23:57:04,642: t15.2024.03.15 val PER: 0.2026
2026-01-03 23:57:04,642: t15.2024.03.17 val PER: 0.1346
2026-01-03 23:57:04,642: t15.2024.05.10 val PER: 0.1545
2026-01-03 23:57:04,642: t15.2024.06.14 val PER: 0.1562
2026-01-03 23:57:04,642: t15.2024.07.19 val PER: 0.2268
2026-01-03 23:57:04,642: t15.2024.07.21 val PER: 0.0890
2026-01-03 23:57:04,642: t15.2024.07.28 val PER: 0.1272
2026-01-03 23:57:04,643: t15.2025.01.10 val PER: 0.2865
2026-01-03 23:57:04,643: t15.2025.01.12 val PER: 0.1601
2026-01-03 23:57:04,643: t15.2025.03.14 val PER: 0.3284
2026-01-03 23:57:04,643: t15.2025.03.16 val PER: 0.1728
2026-01-03 23:57:04,643: t15.2025.03.30 val PER: 0.2954
2026-01-03 23:57:04,643: t15.2025.04.13 val PER: 0.2154
2026-01-03 23:57:04,901: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_18500
2026-01-03 23:57:13,718: Train batch 18600: loss: 11.69 grad norm: 70.33 time: 0.067
2026-01-03 23:57:31,106: Train batch 18800: loss: 7.14 grad norm: 58.99 time: 0.064
2026-01-03 23:57:48,484: Train batch 19000: loss: 6.95 grad norm: 48.63 time: 0.064
2026-01-03 23:57:48,484: Running test after training batch: 19000
2026-01-03 23:57:48,608: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:57:53,468: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 23:57:53,504: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-03 23:57:55,487: Val batch 19000: PER (avg): 0.1447 CTC Loss (avg): 15.0918 WER(1gram): 45.18% (n=64) time: 7.002
2026-01-03 23:57:55,487: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-03 23:57:55,487: t15.2023.08.13 val PER: 0.1029
2026-01-03 23:57:55,488: t15.2023.08.18 val PER: 0.1023
2026-01-03 23:57:55,488: t15.2023.08.20 val PER: 0.1033
2026-01-03 23:57:55,488: t15.2023.08.25 val PER: 0.0828
2026-01-03 23:57:55,488: t15.2023.08.27 val PER: 0.1785
2026-01-03 23:57:55,488: t15.2023.09.01 val PER: 0.0795
2026-01-03 23:57:55,488: t15.2023.09.03 val PER: 0.1473
2026-01-03 23:57:55,488: t15.2023.09.24 val PER: 0.1153
2026-01-03 23:57:55,488: t15.2023.09.29 val PER: 0.1193
2026-01-03 23:57:55,488: t15.2023.10.01 val PER: 0.1638
2026-01-03 23:57:55,488: t15.2023.10.06 val PER: 0.0818
2026-01-03 23:57:55,488: t15.2023.10.08 val PER: 0.2409
2026-01-03 23:57:55,488: t15.2023.10.13 val PER: 0.1870
2026-01-03 23:57:55,488: t15.2023.10.15 val PER: 0.1543
2026-01-03 23:57:55,489: t15.2023.10.20 val PER: 0.1812
2026-01-03 23:57:55,489: t15.2023.10.22 val PER: 0.1080
2026-01-03 23:57:55,489: t15.2023.11.03 val PER: 0.1737
2026-01-03 23:57:55,489: t15.2023.11.04 val PER: 0.0307
2026-01-03 23:57:55,489: t15.2023.11.17 val PER: 0.0467
2026-01-03 23:57:55,489: t15.2023.11.19 val PER: 0.0279
2026-01-03 23:57:55,489: t15.2023.11.26 val PER: 0.1029
2026-01-03 23:57:55,489: t15.2023.12.03 val PER: 0.1124
2026-01-03 23:57:55,489: t15.2023.12.08 val PER: 0.0852
2026-01-03 23:57:55,489: t15.2023.12.10 val PER: 0.0854
2026-01-03 23:57:55,489: t15.2023.12.17 val PER: 0.1341
2026-01-03 23:57:55,489: t15.2023.12.29 val PER: 0.1304
2026-01-03 23:57:55,489: t15.2024.02.25 val PER: 0.1039
2026-01-03 23:57:55,489: t15.2024.03.08 val PER: 0.2176
2026-01-03 23:57:55,489: t15.2024.03.15 val PER: 0.2026
2026-01-03 23:57:55,489: t15.2024.03.17 val PER: 0.1395
2026-01-03 23:57:55,490: t15.2024.05.10 val PER: 0.1605
2026-01-03 23:57:55,490: t15.2024.06.14 val PER: 0.1546
2026-01-03 23:57:55,490: t15.2024.07.19 val PER: 0.2241
2026-01-03 23:57:55,490: t15.2024.07.21 val PER: 0.0903
2026-01-03 23:57:55,490: t15.2024.07.28 val PER: 0.1294
2026-01-03 23:57:55,490: t15.2025.01.10 val PER: 0.2879
2026-01-03 23:57:55,490: t15.2025.01.12 val PER: 0.1478
2026-01-03 23:57:55,490: t15.2025.03.14 val PER: 0.3462
2026-01-03 23:57:55,490: t15.2025.03.16 val PER: 0.1741
2026-01-03 23:57:55,490: t15.2025.03.30 val PER: 0.2839
2026-01-03 23:57:55,490: t15.2025.04.13 val PER: 0.2154
2026-01-03 23:57:55,747: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_19000
2026-01-03 23:58:13,076: Train batch 19200: loss: 5.44 grad norm: 54.12 time: 0.063
2026-01-03 23:58:30,498: Train batch 19400: loss: 4.05 grad norm: 40.99 time: 0.053
2026-01-03 23:58:39,228: Running test after training batch: 19500
2026-01-03 23:58:39,322: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:58:44,029: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 23:58:44,066: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost get
2026-01-03 23:58:46,036: Val batch 19500: PER (avg): 0.1448 CTC Loss (avg): 15.0475 WER(1gram): 45.18% (n=64) time: 6.808
2026-01-03 23:58:46,036: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-03 23:58:46,036: t15.2023.08.13 val PER: 0.1071
2026-01-03 23:58:46,036: t15.2023.08.18 val PER: 0.0956
2026-01-03 23:58:46,037: t15.2023.08.20 val PER: 0.1041
2026-01-03 23:58:46,037: t15.2023.08.25 val PER: 0.0783
2026-01-03 23:58:46,037: t15.2023.08.27 val PER: 0.1704
2026-01-03 23:58:46,037: t15.2023.09.01 val PER: 0.0787
2026-01-03 23:58:46,037: t15.2023.09.03 val PER: 0.1532
2026-01-03 23:58:46,037: t15.2023.09.24 val PER: 0.1177
2026-01-03 23:58:46,037: t15.2023.09.29 val PER: 0.1181
2026-01-03 23:58:46,037: t15.2023.10.01 val PER: 0.1678
2026-01-03 23:58:46,037: t15.2023.10.06 val PER: 0.0753
2026-01-03 23:58:46,037: t15.2023.10.08 val PER: 0.2368
2026-01-03 23:58:46,037: t15.2023.10.13 val PER: 0.1932
2026-01-03 23:58:46,037: t15.2023.10.15 val PER: 0.1529
2026-01-03 23:58:46,038: t15.2023.10.20 val PER: 0.1678
2026-01-03 23:58:46,038: t15.2023.10.22 val PER: 0.1125
2026-01-03 23:58:46,038: t15.2023.11.03 val PER: 0.1710
2026-01-03 23:58:46,038: t15.2023.11.04 val PER: 0.0307
2026-01-03 23:58:46,038: t15.2023.11.17 val PER: 0.0435
2026-01-03 23:58:46,038: t15.2023.11.19 val PER: 0.0319
2026-01-03 23:58:46,038: t15.2023.11.26 val PER: 0.1065
2026-01-03 23:58:46,038: t15.2023.12.03 val PER: 0.1071
2026-01-03 23:58:46,038: t15.2023.12.08 val PER: 0.0899
2026-01-03 23:58:46,038: t15.2023.12.10 val PER: 0.0920
2026-01-03 23:58:46,038: t15.2023.12.17 val PER: 0.1310
2026-01-03 23:58:46,038: t15.2023.12.29 val PER: 0.1311
2026-01-03 23:58:46,039: t15.2024.02.25 val PER: 0.1081
2026-01-03 23:58:46,039: t15.2024.03.08 val PER: 0.2248
2026-01-03 23:58:46,039: t15.2024.03.15 val PER: 0.2008
2026-01-03 23:58:46,039: t15.2024.03.17 val PER: 0.1353
2026-01-03 23:58:46,039: t15.2024.05.10 val PER: 0.1575
2026-01-03 23:58:46,039: t15.2024.06.14 val PER: 0.1546
2026-01-03 23:58:46,039: t15.2024.07.19 val PER: 0.2281
2026-01-03 23:58:46,039: t15.2024.07.21 val PER: 0.0869
2026-01-03 23:58:46,039: t15.2024.07.28 val PER: 0.1272
2026-01-03 23:58:46,039: t15.2025.01.10 val PER: 0.2851
2026-01-03 23:58:46,039: t15.2025.01.12 val PER: 0.1509
2026-01-03 23:58:46,039: t15.2025.03.14 val PER: 0.3432
2026-01-03 23:58:46,039: t15.2025.03.16 val PER: 0.1767
2026-01-03 23:58:46,039: t15.2025.03.30 val PER: 0.2816
2026-01-03 23:58:46,039: t15.2025.04.13 val PER: 0.2240
2026-01-03 23:58:46,297: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_19500
2026-01-03 23:58:54,808: Train batch 19600: loss: 6.60 grad norm: 53.52 time: 0.057
2026-01-03 23:59:11,924: Train batch 19800: loss: 6.56 grad norm: 55.47 time: 0.056
2026-01-03 23:59:29,119: Running test after training batch: 19999
2026-01-03 23:59:29,209: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:59:33,822: WER debug example
  GT : you can see the code at this point as well
  PR : yu can sci the code at this point as will
2026-01-03 23:59:33,858: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost get
2026-01-03 23:59:35,856: Val batch 19999: PER (avg): 0.1449 CTC Loss (avg): 15.0225 WER(1gram): 46.19% (n=64) time: 6.737
2026-01-03 23:59:35,856: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-03 23:59:35,856: t15.2023.08.13 val PER: 0.1102
2026-01-03 23:59:35,857: t15.2023.08.18 val PER: 0.0997
2026-01-03 23:59:35,857: t15.2023.08.20 val PER: 0.1056
2026-01-03 23:59:35,857: t15.2023.08.25 val PER: 0.0798
2026-01-03 23:59:35,857: t15.2023.08.27 val PER: 0.1704
2026-01-03 23:59:35,857: t15.2023.09.01 val PER: 0.0755
2026-01-03 23:59:35,857: t15.2023.09.03 val PER: 0.1508
2026-01-03 23:59:35,857: t15.2023.09.24 val PER: 0.1153
2026-01-03 23:59:35,857: t15.2023.09.29 val PER: 0.1168
2026-01-03 23:59:35,858: t15.2023.10.01 val PER: 0.1664
2026-01-03 23:59:35,858: t15.2023.10.06 val PER: 0.0764
2026-01-03 23:59:35,858: t15.2023.10.08 val PER: 0.2449
2026-01-03 23:59:35,858: t15.2023.10.13 val PER: 0.1908
2026-01-03 23:59:35,858: t15.2023.10.15 val PER: 0.1556
2026-01-03 23:59:35,858: t15.2023.10.20 val PER: 0.1711
2026-01-03 23:59:35,858: t15.2023.10.22 val PER: 0.1147
2026-01-03 23:59:35,858: t15.2023.11.03 val PER: 0.1737
2026-01-03 23:59:35,858: t15.2023.11.04 val PER: 0.0307
2026-01-03 23:59:35,859: t15.2023.11.17 val PER: 0.0467
2026-01-03 23:59:35,859: t15.2023.11.19 val PER: 0.0279
2026-01-03 23:59:35,859: t15.2023.11.26 val PER: 0.1058
2026-01-03 23:59:35,859: t15.2023.12.03 val PER: 0.1061
2026-01-03 23:59:35,859: t15.2023.12.08 val PER: 0.0872
2026-01-03 23:59:35,859: t15.2023.12.10 val PER: 0.0894
2026-01-03 23:59:35,859: t15.2023.12.17 val PER: 0.1341
2026-01-03 23:59:35,859: t15.2023.12.29 val PER: 0.1304
2026-01-03 23:59:35,859: t15.2024.02.25 val PER: 0.1067
2026-01-03 23:59:35,859: t15.2024.03.08 val PER: 0.2262
2026-01-03 23:59:35,859: t15.2024.03.15 val PER: 0.1995
2026-01-03 23:59:35,859: t15.2024.03.17 val PER: 0.1353
2026-01-03 23:59:35,859: t15.2024.05.10 val PER: 0.1486
2026-01-03 23:59:35,859: t15.2024.06.14 val PER: 0.1514
2026-01-03 23:59:35,859: t15.2024.07.19 val PER: 0.2294
2026-01-03 23:59:35,860: t15.2024.07.21 val PER: 0.0890
2026-01-03 23:59:35,860: t15.2024.07.28 val PER: 0.1287
2026-01-03 23:59:35,860: t15.2025.01.10 val PER: 0.2906
2026-01-03 23:59:35,860: t15.2025.01.12 val PER: 0.1455
2026-01-03 23:59:35,860: t15.2025.03.14 val PER: 0.3314
2026-01-03 23:59:35,860: t15.2025.03.16 val PER: 0.1754
2026-01-03 23:59:35,860: t15.2025.03.30 val PER: 0.3023
2026-01-03 23:59:35,860: t15.2025.04.13 val PER: 0.2168
2026-01-03 23:59:36,107: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d05/checkpoint/checkpoint_batch_19999
2026-01-03 23:59:36,131: Best avg val PER achieved: 0.15626
2026-01-03 23:59:36,131: Total training time: 34.30 minutes

=== RUN d10.yaml ===
2026-01-03 23:59:40,631: Using device: cuda:0
2026-01-03 23:59:42,315: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-03 23:59:42,336: Using 45 sessions after filtering (from 45).
2026-01-03 23:59:42,748: Using torch.compile (if available)
2026-01-03 23:59:42,749: torch.compile not available (torch<2.0). Skipping.
2026-01-03 23:59:42,749: Initialized RNN decoding model
2026-01-03 23:59:42,749: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.1)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 23:59:42,749: Model has 44,907,305 parameters
2026-01-03 23:59:42,749: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 23:59:44,019: Successfully initialized datasets
2026-01-03 23:59:44,020: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 23:59:45,018: Train batch 0: loss: 575.84 grad norm: 1474.69 time: 0.176
2026-01-03 23:59:45,019: Running test after training batch: 0
2026-01-03 23:59:45,133: WER debug GT example: You can see the code at this point as well.
2026-01-03 23:59:50,354: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-03 23:59:51,067: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-04 00:00:24,930: Val batch 0: PER (avg): 1.4294 CTC Loss (avg): 633.1788 WER(1gram): 100.00% (n=64) time: 39.911
2026-01-04 00:00:24,931: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-04 00:00:24,931: t15.2023.08.13 val PER: 1.3056
2026-01-04 00:00:24,931: t15.2023.08.18 val PER: 1.4283
2026-01-04 00:00:24,931: t15.2023.08.20 val PER: 1.2994
2026-01-04 00:00:24,931: t15.2023.08.25 val PER: 1.3434
2026-01-04 00:00:24,931: t15.2023.08.27 val PER: 1.2476
2026-01-04 00:00:24,931: t15.2023.09.01 val PER: 1.4456
2026-01-04 00:00:24,931: t15.2023.09.03 val PER: 1.3219
2026-01-04 00:00:24,931: t15.2023.09.24 val PER: 1.5425
2026-01-04 00:00:24,932: t15.2023.09.29 val PER: 1.4671
2026-01-04 00:00:24,932: t15.2023.10.01 val PER: 1.2127
2026-01-04 00:00:24,932: t15.2023.10.06 val PER: 1.4930
2026-01-04 00:00:24,932: t15.2023.10.08 val PER: 1.1881
2026-01-04 00:00:24,932: t15.2023.10.13 val PER: 1.3972
2026-01-04 00:00:24,932: t15.2023.10.15 val PER: 1.3856
2026-01-04 00:00:24,932: t15.2023.10.20 val PER: 1.5034
2026-01-04 00:00:24,932: t15.2023.10.22 val PER: 1.3942
2026-01-04 00:00:24,932: t15.2023.11.03 val PER: 1.5896
2026-01-04 00:00:24,932: t15.2023.11.04 val PER: 2.0307
2026-01-04 00:00:24,932: t15.2023.11.17 val PER: 1.9580
2026-01-04 00:00:24,932: t15.2023.11.19 val PER: 1.6786
2026-01-04 00:00:24,932: t15.2023.11.26 val PER: 1.5362
2026-01-04 00:00:24,932: t15.2023.12.03 val PER: 1.4233
2026-01-04 00:00:24,932: t15.2023.12.08 val PER: 1.4467
2026-01-04 00:00:24,933: t15.2023.12.10 val PER: 1.6938
2026-01-04 00:00:24,933: t15.2023.12.17 val PER: 1.3056
2026-01-04 00:00:24,933: t15.2023.12.29 val PER: 1.4084
2026-01-04 00:00:24,933: t15.2024.02.25 val PER: 1.4340
2026-01-04 00:00:24,933: t15.2024.03.08 val PER: 1.3272
2026-01-04 00:00:24,933: t15.2024.03.15 val PER: 1.3202
2026-01-04 00:00:24,933: t15.2024.03.17 val PER: 1.3954
2026-01-04 00:00:24,933: t15.2024.05.10 val PER: 1.3254
2026-01-04 00:00:24,933: t15.2024.06.14 val PER: 1.5347
2026-01-04 00:00:24,933: t15.2024.07.19 val PER: 1.0817
2026-01-04 00:00:24,933: t15.2024.07.21 val PER: 1.6359
2026-01-04 00:00:24,933: t15.2024.07.28 val PER: 1.6529
2026-01-04 00:00:24,933: t15.2025.01.10 val PER: 1.0964
2026-01-04 00:00:24,933: t15.2025.01.12 val PER: 1.7644
2026-01-04 00:00:24,933: t15.2025.03.14 val PER: 1.0325
2026-01-04 00:00:24,933: t15.2025.03.16 val PER: 1.6139
2026-01-04 00:00:24,933: t15.2025.03.30 val PER: 1.2920
2026-01-04 00:00:24,934: t15.2025.04.13 val PER: 1.5934
2026-01-04 00:00:24,935: New best val WER(1gram) inf% --> 100.00%
2026-01-04 00:00:24,935: Checkpointing model
2026-01-04 00:00:25,172: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:00:25,415: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_0
2026-01-04 00:00:42,734: Train batch 200: loss: 77.09 grad norm: 118.88 time: 0.054
2026-01-04 00:00:59,652: Train batch 400: loss: 53.19 grad norm: 109.06 time: 0.063
2026-01-04 00:01:08,193: Running test after training batch: 500
2026-01-04 00:01:08,289: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:01:13,155: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt ease thus uhde at this ide is aisle
2026-01-04 00:01:13,193: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus ass adz
2026-01-04 00:01:15,765: Val batch 500: PER (avg): 0.5099 CTC Loss (avg): 55.1238 WER(1gram): 88.32% (n=64) time: 7.572
2026-01-04 00:01:15,766: WER lens: avg_true_words=6.16 avg_pred_words=5.75 max_pred_words=11
2026-01-04 00:01:15,766: t15.2023.08.13 val PER: 0.4480
2026-01-04 00:01:15,766: t15.2023.08.18 val PER: 0.4526
2026-01-04 00:01:15,766: t15.2023.08.20 val PER: 0.4321
2026-01-04 00:01:15,766: t15.2023.08.25 val PER: 0.4352
2026-01-04 00:01:15,766: t15.2023.08.27 val PER: 0.5096
2026-01-04 00:01:15,766: t15.2023.09.01 val PER: 0.4140
2026-01-04 00:01:15,767: t15.2023.09.03 val PER: 0.4857
2026-01-04 00:01:15,767: t15.2023.09.24 val PER: 0.4199
2026-01-04 00:01:15,767: t15.2023.09.29 val PER: 0.4563
2026-01-04 00:01:15,767: t15.2023.10.01 val PER: 0.5033
2026-01-04 00:01:15,767: t15.2023.10.06 val PER: 0.4069
2026-01-04 00:01:15,767: t15.2023.10.08 val PER: 0.5318
2026-01-04 00:01:15,767: t15.2023.10.13 val PER: 0.5780
2026-01-04 00:01:15,767: t15.2023.10.15 val PER: 0.4931
2026-01-04 00:01:15,767: t15.2023.10.20 val PER: 0.4463
2026-01-04 00:01:15,767: t15.2023.10.22 val PER: 0.4499
2026-01-04 00:01:15,767: t15.2023.11.03 val PER: 0.5000
2026-01-04 00:01:15,767: t15.2023.11.04 val PER: 0.2628
2026-01-04 00:01:15,767: t15.2023.11.17 val PER: 0.3593
2026-01-04 00:01:15,767: t15.2023.11.19 val PER: 0.3114
2026-01-04 00:01:15,767: t15.2023.11.26 val PER: 0.5464
2026-01-04 00:01:15,768: t15.2023.12.03 val PER: 0.5021
2026-01-04 00:01:15,768: t15.2023.12.08 val PER: 0.5060
2026-01-04 00:01:15,768: t15.2023.12.10 val PER: 0.4376
2026-01-04 00:01:15,768: t15.2023.12.17 val PER: 0.5509
2026-01-04 00:01:15,768: t15.2023.12.29 val PER: 0.5312
2026-01-04 00:01:15,768: t15.2024.02.25 val PER: 0.4663
2026-01-04 00:01:15,768: t15.2024.03.08 val PER: 0.6003
2026-01-04 00:01:15,768: t15.2024.03.15 val PER: 0.5453
2026-01-04 00:01:15,768: t15.2024.03.17 val PER: 0.4979
2026-01-04 00:01:15,768: t15.2024.05.10 val PER: 0.5364
2026-01-04 00:01:15,768: t15.2024.06.14 val PER: 0.5189
2026-01-04 00:01:15,768: t15.2024.07.19 val PER: 0.6671
2026-01-04 00:01:15,768: t15.2024.07.21 val PER: 0.4779
2026-01-04 00:01:15,768: t15.2024.07.28 val PER: 0.5132
2026-01-04 00:01:15,768: t15.2025.01.10 val PER: 0.7410
2026-01-04 00:01:15,768: t15.2025.01.12 val PER: 0.5535
2026-01-04 00:01:15,768: t15.2025.03.14 val PER: 0.7071
2026-01-04 00:01:15,769: t15.2025.03.16 val PER: 0.5746
2026-01-04 00:01:15,769: t15.2025.03.30 val PER: 0.7034
2026-01-04 00:01:15,769: t15.2025.04.13 val PER: 0.5606
2026-01-04 00:01:15,770: New best val WER(1gram) 100.00% --> 88.32%
2026-01-04 00:01:15,770: Checkpointing model
2026-01-04 00:01:16,358: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:01:16,596: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_500
2026-01-04 00:01:25,195: Train batch 600: loss: 48.45 grad norm: 81.80 time: 0.078
2026-01-04 00:01:42,221: Train batch 800: loss: 40.10 grad norm: 85.17 time: 0.057
2026-01-04 00:01:59,508: Train batch 1000: loss: 42.18 grad norm: 79.78 time: 0.066
2026-01-04 00:01:59,509: Running test after training batch: 1000
2026-01-04 00:01:59,634: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:02:04,396: WER debug example
  GT : you can see the code at this point as well
  PR : yield hynd ease thus good it this uhde is wheel
2026-01-04 00:02:04,428: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke that wass it
2026-01-04 00:02:06,306: Val batch 1000: PER (avg): 0.4036 CTC Loss (avg): 41.8483 WER(1gram): 81.22% (n=64) time: 6.797
2026-01-04 00:02:06,306: WER lens: avg_true_words=6.16 avg_pred_words=5.59 max_pred_words=12
2026-01-04 00:02:06,306: t15.2023.08.13 val PER: 0.3711
2026-01-04 00:02:06,306: t15.2023.08.18 val PER: 0.3361
2026-01-04 00:02:06,306: t15.2023.08.20 val PER: 0.3376
2026-01-04 00:02:06,307: t15.2023.08.25 val PER: 0.2967
2026-01-04 00:02:06,307: t15.2023.08.27 val PER: 0.4164
2026-01-04 00:02:06,307: t15.2023.09.01 val PER: 0.2995
2026-01-04 00:02:06,307: t15.2023.09.03 val PER: 0.3907
2026-01-04 00:02:06,307: t15.2023.09.24 val PER: 0.3228
2026-01-04 00:02:06,307: t15.2023.09.29 val PER: 0.3580
2026-01-04 00:02:06,307: t15.2023.10.01 val PER: 0.3956
2026-01-04 00:02:06,307: t15.2023.10.06 val PER: 0.3143
2026-01-04 00:02:06,307: t15.2023.10.08 val PER: 0.4506
2026-01-04 00:02:06,307: t15.2023.10.13 val PER: 0.4562
2026-01-04 00:02:06,307: t15.2023.10.15 val PER: 0.3632
2026-01-04 00:02:06,307: t15.2023.10.20 val PER: 0.3523
2026-01-04 00:02:06,307: t15.2023.10.22 val PER: 0.3408
2026-01-04 00:02:06,307: t15.2023.11.03 val PER: 0.3948
2026-01-04 00:02:06,308: t15.2023.11.04 val PER: 0.1468
2026-01-04 00:02:06,308: t15.2023.11.17 val PER: 0.2644
2026-01-04 00:02:06,308: t15.2023.11.19 val PER: 0.2176
2026-01-04 00:02:06,308: t15.2023.11.26 val PER: 0.4384
2026-01-04 00:02:06,308: t15.2023.12.03 val PER: 0.4002
2026-01-04 00:02:06,308: t15.2023.12.08 val PER: 0.3961
2026-01-04 00:02:06,308: t15.2023.12.10 val PER: 0.3443
2026-01-04 00:02:06,308: t15.2023.12.17 val PER: 0.4106
2026-01-04 00:02:06,308: t15.2023.12.29 val PER: 0.4049
2026-01-04 00:02:06,308: t15.2024.02.25 val PER: 0.3511
2026-01-04 00:02:06,308: t15.2024.03.08 val PER: 0.4908
2026-01-04 00:02:06,308: t15.2024.03.15 val PER: 0.4409
2026-01-04 00:02:06,308: t15.2024.03.17 val PER: 0.4017
2026-01-04 00:02:06,308: t15.2024.05.10 val PER: 0.4012
2026-01-04 00:02:06,308: t15.2024.06.14 val PER: 0.4069
2026-01-04 00:02:06,308: t15.2024.07.19 val PER: 0.5274
2026-01-04 00:02:06,308: t15.2024.07.21 val PER: 0.3759
2026-01-04 00:02:06,309: t15.2024.07.28 val PER: 0.4147
2026-01-04 00:02:06,309: t15.2025.01.10 val PER: 0.6157
2026-01-04 00:02:06,309: t15.2025.01.12 val PER: 0.4519
2026-01-04 00:02:06,309: t15.2025.03.14 val PER: 0.6435
2026-01-04 00:02:06,309: t15.2025.03.16 val PER: 0.4660
2026-01-04 00:02:06,309: t15.2025.03.30 val PER: 0.6483
2026-01-04 00:02:06,309: t15.2025.04.13 val PER: 0.4736
2026-01-04 00:02:06,310: New best val WER(1gram) 88.32% --> 81.22%
2026-01-04 00:02:06,310: Checkpointing model
2026-01-04 00:02:06,923: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:02:07,161: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_1000
2026-01-04 00:02:24,137: Train batch 1200: loss: 32.31 grad norm: 76.86 time: 0.068
2026-01-04 00:02:41,264: Train batch 1400: loss: 35.24 grad norm: 79.18 time: 0.061
2026-01-04 00:02:49,790: Running test after training batch: 1500
2026-01-04 00:02:49,939: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:02:55,046: WER debug example
  GT : you can see the code at this point as well
  PR : yule kint e the good it this boyde is will
2026-01-04 00:02:55,078: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that wass
2026-01-04 00:02:56,684: Val batch 1500: PER (avg): 0.3760 CTC Loss (avg): 36.7497 WER(1gram): 74.62% (n=64) time: 6.893
2026-01-04 00:02:56,684: WER lens: avg_true_words=6.16 avg_pred_words=5.02 max_pred_words=11
2026-01-04 00:02:56,684: t15.2023.08.13 val PER: 0.3420
2026-01-04 00:02:56,684: t15.2023.08.18 val PER: 0.3127
2026-01-04 00:02:56,685: t15.2023.08.20 val PER: 0.3002
2026-01-04 00:02:56,685: t15.2023.08.25 val PER: 0.2605
2026-01-04 00:02:56,685: t15.2023.08.27 val PER: 0.4019
2026-01-04 00:02:56,685: t15.2023.09.01 val PER: 0.2735
2026-01-04 00:02:56,685: t15.2023.09.03 val PER: 0.3634
2026-01-04 00:02:56,685: t15.2023.09.24 val PER: 0.3095
2026-01-04 00:02:56,685: t15.2023.09.29 val PER: 0.3293
2026-01-04 00:02:56,685: t15.2023.10.01 val PER: 0.3897
2026-01-04 00:02:56,685: t15.2023.10.06 val PER: 0.2723
2026-01-04 00:02:56,686: t15.2023.10.08 val PER: 0.4222
2026-01-04 00:02:56,686: t15.2023.10.13 val PER: 0.4352
2026-01-04 00:02:56,686: t15.2023.10.15 val PER: 0.3500
2026-01-04 00:02:56,686: t15.2023.10.20 val PER: 0.3255
2026-01-04 00:02:56,686: t15.2023.10.22 val PER: 0.3118
2026-01-04 00:02:56,686: t15.2023.11.03 val PER: 0.3487
2026-01-04 00:02:56,686: t15.2023.11.04 val PER: 0.1160
2026-01-04 00:02:56,686: t15.2023.11.17 val PER: 0.2271
2026-01-04 00:02:56,686: t15.2023.11.19 val PER: 0.1717
2026-01-04 00:02:56,686: t15.2023.11.26 val PER: 0.4196
2026-01-04 00:02:56,686: t15.2023.12.03 val PER: 0.3634
2026-01-04 00:02:56,686: t15.2023.12.08 val PER: 0.3569
2026-01-04 00:02:56,686: t15.2023.12.10 val PER: 0.2983
2026-01-04 00:02:56,686: t15.2023.12.17 val PER: 0.3888
2026-01-04 00:02:56,686: t15.2023.12.29 val PER: 0.3638
2026-01-04 00:02:56,687: t15.2024.02.25 val PER: 0.3034
2026-01-04 00:02:56,687: t15.2024.03.08 val PER: 0.4452
2026-01-04 00:02:56,687: t15.2024.03.15 val PER: 0.4040
2026-01-04 00:02:56,687: t15.2024.03.17 val PER: 0.3766
2026-01-04 00:02:56,687: t15.2024.05.10 val PER: 0.3908
2026-01-04 00:02:56,687: t15.2024.06.14 val PER: 0.4006
2026-01-04 00:02:56,687: t15.2024.07.19 val PER: 0.5201
2026-01-04 00:02:56,687: t15.2024.07.21 val PER: 0.3524
2026-01-04 00:02:56,687: t15.2024.07.28 val PER: 0.3691
2026-01-04 00:02:56,687: t15.2025.01.10 val PER: 0.6143
2026-01-04 00:02:56,687: t15.2025.01.12 val PER: 0.4188
2026-01-04 00:02:56,687: t15.2025.03.14 val PER: 0.6080
2026-01-04 00:02:56,687: t15.2025.03.16 val PER: 0.4594
2026-01-04 00:02:56,687: t15.2025.03.30 val PER: 0.6184
2026-01-04 00:02:56,687: t15.2025.04.13 val PER: 0.4665
2026-01-04 00:02:56,689: New best val WER(1gram) 81.22% --> 74.62%
2026-01-04 00:02:56,689: Checkpointing model
2026-01-04 00:02:57,286: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:02:57,524: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_1500
2026-01-04 00:03:05,821: Train batch 1600: loss: 36.12 grad norm: 79.28 time: 0.064
2026-01-04 00:03:22,922: Train batch 1800: loss: 34.34 grad norm: 74.32 time: 0.087
2026-01-04 00:03:40,101: Train batch 2000: loss: 32.76 grad norm: 73.73 time: 0.065
2026-01-04 00:03:40,101: Running test after training batch: 2000
2026-01-04 00:03:40,228: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:03:45,008: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt ease the owed at this bonde is will
2026-01-04 00:03:45,037: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap thus us it
2026-01-04 00:03:46,620: Val batch 2000: PER (avg): 0.3203 CTC Loss (avg): 32.3958 WER(1gram): 70.81% (n=64) time: 6.519
2026-01-04 00:03:46,620: WER lens: avg_true_words=6.16 avg_pred_words=5.48 max_pred_words=11
2026-01-04 00:03:46,621: t15.2023.08.13 val PER: 0.2952
2026-01-04 00:03:46,621: t15.2023.08.18 val PER: 0.2531
2026-01-04 00:03:46,621: t15.2023.08.20 val PER: 0.2446
2026-01-04 00:03:46,621: t15.2023.08.25 val PER: 0.2289
2026-01-04 00:03:46,621: t15.2023.08.27 val PER: 0.3457
2026-01-04 00:03:46,621: t15.2023.09.01 val PER: 0.2135
2026-01-04 00:03:46,621: t15.2023.09.03 val PER: 0.3278
2026-01-04 00:03:46,621: t15.2023.09.24 val PER: 0.2536
2026-01-04 00:03:46,621: t15.2023.09.29 val PER: 0.2706
2026-01-04 00:03:46,621: t15.2023.10.01 val PER: 0.3210
2026-01-04 00:03:46,621: t15.2023.10.06 val PER: 0.2325
2026-01-04 00:03:46,621: t15.2023.10.08 val PER: 0.3884
2026-01-04 00:03:46,622: t15.2023.10.13 val PER: 0.3708
2026-01-04 00:03:46,622: t15.2023.10.15 val PER: 0.2980
2026-01-04 00:03:46,622: t15.2023.10.20 val PER: 0.2886
2026-01-04 00:03:46,622: t15.2023.10.22 val PER: 0.2472
2026-01-04 00:03:46,622: t15.2023.11.03 val PER: 0.3066
2026-01-04 00:03:46,622: t15.2023.11.04 val PER: 0.0887
2026-01-04 00:03:46,622: t15.2023.11.17 val PER: 0.1742
2026-01-04 00:03:46,622: t15.2023.11.19 val PER: 0.1357
2026-01-04 00:03:46,622: t15.2023.11.26 val PER: 0.3529
2026-01-04 00:03:46,623: t15.2023.12.03 val PER: 0.2973
2026-01-04 00:03:46,623: t15.2023.12.08 val PER: 0.2956
2026-01-04 00:03:46,623: t15.2023.12.10 val PER: 0.2523
2026-01-04 00:03:46,623: t15.2023.12.17 val PER: 0.3254
2026-01-04 00:03:46,623: t15.2023.12.29 val PER: 0.3205
2026-01-04 00:03:46,623: t15.2024.02.25 val PER: 0.2711
2026-01-04 00:03:46,623: t15.2024.03.08 val PER: 0.4011
2026-01-04 00:03:46,623: t15.2024.03.15 val PER: 0.3571
2026-01-04 00:03:46,623: t15.2024.03.17 val PER: 0.3347
2026-01-04 00:03:46,623: t15.2024.05.10 val PER: 0.3328
2026-01-04 00:03:46,623: t15.2024.06.14 val PER: 0.3344
2026-01-04 00:03:46,623: t15.2024.07.19 val PER: 0.4476
2026-01-04 00:03:46,623: t15.2024.07.21 val PER: 0.2862
2026-01-04 00:03:46,623: t15.2024.07.28 val PER: 0.3169
2026-01-04 00:03:46,623: t15.2025.01.10 val PER: 0.5179
2026-01-04 00:03:46,623: t15.2025.01.12 val PER: 0.3826
2026-01-04 00:03:46,623: t15.2025.03.14 val PER: 0.5000
2026-01-04 00:03:46,624: t15.2025.03.16 val PER: 0.3992
2026-01-04 00:03:46,624: t15.2025.03.30 val PER: 0.5276
2026-01-04 00:03:46,624: t15.2025.04.13 val PER: 0.4151
2026-01-04 00:03:46,625: New best val WER(1gram) 74.62% --> 70.81%
2026-01-04 00:03:46,625: Checkpointing model
2026-01-04 00:03:47,241: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:03:47,479: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_2000
2026-01-04 00:04:04,285: Train batch 2200: loss: 27.85 grad norm: 71.41 time: 0.060
2026-01-04 00:04:21,335: Train batch 2400: loss: 28.46 grad norm: 63.38 time: 0.052
2026-01-04 00:04:29,916: Running test after training batch: 2500
2026-01-04 00:04:30,063: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:04:34,812: WER debug example
  GT : you can see the code at this point as well
  PR : yule kint sze the code at this point is wheel
2026-01-04 00:04:34,845: WER debug example
  GT : how does it keep the cost down
  PR : houde just it heap the wass nit
2026-01-04 00:04:36,575: Val batch 2500: PER (avg): 0.3058 CTC Loss (avg): 30.7567 WER(1gram): 68.78% (n=64) time: 6.659
2026-01-04 00:04:36,576: WER lens: avg_true_words=6.16 avg_pred_words=5.72 max_pred_words=11
2026-01-04 00:04:36,576: t15.2023.08.13 val PER: 0.2879
2026-01-04 00:04:36,576: t15.2023.08.18 val PER: 0.2464
2026-01-04 00:04:36,576: t15.2023.08.20 val PER: 0.2391
2026-01-04 00:04:36,576: t15.2023.08.25 val PER: 0.2154
2026-01-04 00:04:36,576: t15.2023.08.27 val PER: 0.3232
2026-01-04 00:04:36,576: t15.2023.09.01 val PER: 0.2127
2026-01-04 00:04:36,576: t15.2023.09.03 val PER: 0.2945
2026-01-04 00:04:36,576: t15.2023.09.24 val PER: 0.2415
2026-01-04 00:04:36,576: t15.2023.09.29 val PER: 0.2597
2026-01-04 00:04:36,576: t15.2023.10.01 val PER: 0.3111
2026-01-04 00:04:36,576: t15.2023.10.06 val PER: 0.2196
2026-01-04 00:04:36,576: t15.2023.10.08 val PER: 0.3721
2026-01-04 00:04:36,577: t15.2023.10.13 val PER: 0.3491
2026-01-04 00:04:36,577: t15.2023.10.15 val PER: 0.2894
2026-01-04 00:04:36,577: t15.2023.10.20 val PER: 0.2852
2026-01-04 00:04:36,577: t15.2023.10.22 val PER: 0.2361
2026-01-04 00:04:36,577: t15.2023.11.03 val PER: 0.3012
2026-01-04 00:04:36,577: t15.2023.11.04 val PER: 0.0922
2026-01-04 00:04:36,577: t15.2023.11.17 val PER: 0.1555
2026-01-04 00:04:36,577: t15.2023.11.19 val PER: 0.1198
2026-01-04 00:04:36,577: t15.2023.11.26 val PER: 0.3326
2026-01-04 00:04:36,577: t15.2023.12.03 val PER: 0.3046
2026-01-04 00:04:36,577: t15.2023.12.08 val PER: 0.2776
2026-01-04 00:04:36,577: t15.2023.12.10 val PER: 0.2392
2026-01-04 00:04:36,578: t15.2023.12.17 val PER: 0.2994
2026-01-04 00:04:36,578: t15.2023.12.29 val PER: 0.3137
2026-01-04 00:04:36,578: t15.2024.02.25 val PER: 0.2331
2026-01-04 00:04:36,578: t15.2024.03.08 val PER: 0.3627
2026-01-04 00:04:36,578: t15.2024.03.15 val PER: 0.3502
2026-01-04 00:04:36,578: t15.2024.03.17 val PER: 0.3243
2026-01-04 00:04:36,578: t15.2024.05.10 val PER: 0.3328
2026-01-04 00:04:36,578: t15.2024.06.14 val PER: 0.3265
2026-01-04 00:04:36,578: t15.2024.07.19 val PER: 0.4364
2026-01-04 00:04:36,579: t15.2024.07.21 val PER: 0.2710
2026-01-04 00:04:36,579: t15.2024.07.28 val PER: 0.2890
2026-01-04 00:04:36,579: t15.2025.01.10 val PER: 0.4972
2026-01-04 00:04:36,579: t15.2025.01.12 val PER: 0.3564
2026-01-04 00:04:36,579: t15.2025.03.14 val PER: 0.4941
2026-01-04 00:04:36,579: t15.2025.03.16 val PER: 0.3639
2026-01-04 00:04:36,579: t15.2025.03.30 val PER: 0.5069
2026-01-04 00:04:36,579: t15.2025.04.13 val PER: 0.3766
2026-01-04 00:04:36,580: New best val WER(1gram) 70.81% --> 68.78%
2026-01-04 00:04:36,580: Checkpointing model
2026-01-04 00:04:37,193: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:04:37,431: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_2500
2026-01-04 00:04:46,165: Train batch 2600: loss: 34.29 grad norm: 83.21 time: 0.055
2026-01-04 00:05:03,620: Train batch 2800: loss: 24.83 grad norm: 72.24 time: 0.081
2026-01-04 00:05:20,826: Train batch 3000: loss: 30.21 grad norm: 76.05 time: 0.083
2026-01-04 00:05:20,827: Running test after training batch: 3000
2026-01-04 00:05:20,936: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:05:25,923: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-04 00:05:25,952: WER debug example
  GT : how does it keep the cost down
  PR : houde des it hipp the cost nit
2026-01-04 00:05:27,599: Val batch 3000: PER (avg): 0.2757 CTC Loss (avg): 27.5968 WER(1gram): 67.01% (n=64) time: 6.772
2026-01-04 00:05:27,599: WER lens: avg_true_words=6.16 avg_pred_words=5.95 max_pred_words=11
2026-01-04 00:05:27,599: t15.2023.08.13 val PER: 0.2380
2026-01-04 00:05:27,599: t15.2023.08.18 val PER: 0.2188
2026-01-04 00:05:27,599: t15.2023.08.20 val PER: 0.2121
2026-01-04 00:05:27,599: t15.2023.08.25 val PER: 0.1852
2026-01-04 00:05:27,600: t15.2023.08.27 val PER: 0.2974
2026-01-04 00:05:27,600: t15.2023.09.01 val PER: 0.1802
2026-01-04 00:05:27,600: t15.2023.09.03 val PER: 0.2743
2026-01-04 00:05:27,600: t15.2023.09.24 val PER: 0.2075
2026-01-04 00:05:27,600: t15.2023.09.29 val PER: 0.2342
2026-01-04 00:05:27,600: t15.2023.10.01 val PER: 0.2774
2026-01-04 00:05:27,600: t15.2023.10.06 val PER: 0.1851
2026-01-04 00:05:27,600: t15.2023.10.08 val PER: 0.3424
2026-01-04 00:05:27,600: t15.2023.10.13 val PER: 0.3344
2026-01-04 00:05:27,600: t15.2023.10.15 val PER: 0.2577
2026-01-04 00:05:27,601: t15.2023.10.20 val PER: 0.2517
2026-01-04 00:05:27,601: t15.2023.10.22 val PER: 0.2004
2026-01-04 00:05:27,601: t15.2023.11.03 val PER: 0.2727
2026-01-04 00:05:27,601: t15.2023.11.04 val PER: 0.0648
2026-01-04 00:05:27,601: t15.2023.11.17 val PER: 0.1198
2026-01-04 00:05:27,601: t15.2023.11.19 val PER: 0.1098
2026-01-04 00:05:27,601: t15.2023.11.26 val PER: 0.2899
2026-01-04 00:05:27,601: t15.2023.12.03 val PER: 0.2553
2026-01-04 00:05:27,601: t15.2023.12.08 val PER: 0.2523
2026-01-04 00:05:27,601: t15.2023.12.10 val PER: 0.2063
2026-01-04 00:05:27,601: t15.2023.12.17 val PER: 0.2744
2026-01-04 00:05:27,602: t15.2023.12.29 val PER: 0.2725
2026-01-04 00:05:27,602: t15.2024.02.25 val PER: 0.2247
2026-01-04 00:05:27,602: t15.2024.03.08 val PER: 0.3585
2026-01-04 00:05:27,602: t15.2024.03.15 val PER: 0.3258
2026-01-04 00:05:27,602: t15.2024.03.17 val PER: 0.2852
2026-01-04 00:05:27,602: t15.2024.05.10 val PER: 0.2972
2026-01-04 00:05:27,602: t15.2024.06.14 val PER: 0.3076
2026-01-04 00:05:27,602: t15.2024.07.19 val PER: 0.4015
2026-01-04 00:05:27,602: t15.2024.07.21 val PER: 0.2269
2026-01-04 00:05:27,602: t15.2024.07.28 val PER: 0.2699
2026-01-04 00:05:27,602: t15.2025.01.10 val PER: 0.4876
2026-01-04 00:05:27,603: t15.2025.01.12 val PER: 0.3264
2026-01-04 00:05:27,603: t15.2025.03.14 val PER: 0.4438
2026-01-04 00:05:27,603: t15.2025.03.16 val PER: 0.3220
2026-01-04 00:05:27,603: t15.2025.03.30 val PER: 0.4874
2026-01-04 00:05:27,603: t15.2025.04.13 val PER: 0.3595
2026-01-04 00:05:27,603: New best val WER(1gram) 68.78% --> 67.01%
2026-01-04 00:05:27,603: Checkpointing model
2026-01-04 00:05:28,227: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:05:28,462: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_3000
2026-01-04 00:05:45,895: Train batch 3200: loss: 25.71 grad norm: 66.49 time: 0.076
2026-01-04 00:06:03,296: Train batch 3400: loss: 18.19 grad norm: 56.39 time: 0.049
2026-01-04 00:06:12,057: Running test after training batch: 3500
2026-01-04 00:06:12,183: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:06:16,938: WER debug example
  GT : you can see the code at this point as well
  PR : yule end sci the code at this point will
2026-01-04 00:06:16,967: WER debug example
  GT : how does it keep the cost down
  PR : houde des it epp the cussed get
2026-01-04 00:06:18,538: Val batch 3500: PER (avg): 0.2636 CTC Loss (avg): 26.0632 WER(1gram): 63.45% (n=64) time: 6.480
2026-01-04 00:06:18,538: WER lens: avg_true_words=6.16 avg_pred_words=5.97 max_pred_words=11
2026-01-04 00:06:18,539: t15.2023.08.13 val PER: 0.2277
2026-01-04 00:06:18,539: t15.2023.08.18 val PER: 0.2045
2026-01-04 00:06:18,539: t15.2023.08.20 val PER: 0.2168
2026-01-04 00:06:18,539: t15.2023.08.25 val PER: 0.1717
2026-01-04 00:06:18,539: t15.2023.08.27 val PER: 0.2830
2026-01-04 00:06:18,539: t15.2023.09.01 val PER: 0.1688
2026-01-04 00:06:18,539: t15.2023.09.03 val PER: 0.2565
2026-01-04 00:06:18,539: t15.2023.09.24 val PER: 0.1990
2026-01-04 00:06:18,539: t15.2023.09.29 val PER: 0.2234
2026-01-04 00:06:18,539: t15.2023.10.01 val PER: 0.2741
2026-01-04 00:06:18,539: t15.2023.10.06 val PER: 0.1787
2026-01-04 00:06:18,539: t15.2023.10.08 val PER: 0.3383
2026-01-04 00:06:18,539: t15.2023.10.13 val PER: 0.3173
2026-01-04 00:06:18,540: t15.2023.10.15 val PER: 0.2419
2026-01-04 00:06:18,540: t15.2023.10.20 val PER: 0.2282
2026-01-04 00:06:18,540: t15.2023.10.22 val PER: 0.2127
2026-01-04 00:06:18,540: t15.2023.11.03 val PER: 0.2592
2026-01-04 00:06:18,540: t15.2023.11.04 val PER: 0.0853
2026-01-04 00:06:18,540: t15.2023.11.17 val PER: 0.1151
2026-01-04 00:06:18,540: t15.2023.11.19 val PER: 0.1018
2026-01-04 00:06:18,540: t15.2023.11.26 val PER: 0.2746
2026-01-04 00:06:18,540: t15.2023.12.03 val PER: 0.2426
2026-01-04 00:06:18,540: t15.2023.12.08 val PER: 0.2350
2026-01-04 00:06:18,540: t15.2023.12.10 val PER: 0.2063
2026-01-04 00:06:18,540: t15.2023.12.17 val PER: 0.2557
2026-01-04 00:06:18,540: t15.2023.12.29 val PER: 0.2601
2026-01-04 00:06:18,540: t15.2024.02.25 val PER: 0.1994
2026-01-04 00:06:18,540: t15.2024.03.08 val PER: 0.3257
2026-01-04 00:06:18,540: t15.2024.03.15 val PER: 0.3171
2026-01-04 00:06:18,540: t15.2024.03.17 val PER: 0.2734
2026-01-04 00:06:18,541: t15.2024.05.10 val PER: 0.2600
2026-01-04 00:06:18,541: t15.2024.06.14 val PER: 0.2603
2026-01-04 00:06:18,541: t15.2024.07.19 val PER: 0.3955
2026-01-04 00:06:18,541: t15.2024.07.21 val PER: 0.2159
2026-01-04 00:06:18,541: t15.2024.07.28 val PER: 0.2691
2026-01-04 00:06:18,541: t15.2025.01.10 val PER: 0.4697
2026-01-04 00:06:18,541: t15.2025.01.12 val PER: 0.2972
2026-01-04 00:06:18,541: t15.2025.03.14 val PER: 0.4527
2026-01-04 00:06:18,542: t15.2025.03.16 val PER: 0.3298
2026-01-04 00:06:18,542: t15.2025.03.30 val PER: 0.4552
2026-01-04 00:06:18,542: t15.2025.04.13 val PER: 0.3338
2026-01-04 00:06:18,542: New best val WER(1gram) 67.01% --> 63.45%
2026-01-04 00:06:18,543: Checkpointing model
2026-01-04 00:06:19,157: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:06:19,395: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_3500
2026-01-04 00:06:28,037: Train batch 3600: loss: 21.34 grad norm: 59.08 time: 0.067
2026-01-04 00:06:45,204: Train batch 3800: loss: 24.94 grad norm: 73.48 time: 0.066
2026-01-04 00:07:02,426: Train batch 4000: loss: 19.07 grad norm: 57.13 time: 0.056
2026-01-04 00:07:02,426: Running test after training batch: 4000
2026-01-04 00:07:02,561: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:07:07,457: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is wheel
2026-01-04 00:07:07,486: WER debug example
  GT : how does it keep the cost down
  PR : aue des it keep the cussed nett
2026-01-04 00:07:09,075: Val batch 4000: PER (avg): 0.2458 CTC Loss (avg): 24.1011 WER(1gram): 63.20% (n=64) time: 6.649
2026-01-04 00:07:09,076: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=11
2026-01-04 00:07:09,076: t15.2023.08.13 val PER: 0.2173
2026-01-04 00:07:09,076: t15.2023.08.18 val PER: 0.2028
2026-01-04 00:07:09,076: t15.2023.08.20 val PER: 0.1962
2026-01-04 00:07:09,076: t15.2023.08.25 val PER: 0.1461
2026-01-04 00:07:09,076: t15.2023.08.27 val PER: 0.2733
2026-01-04 00:07:09,076: t15.2023.09.01 val PER: 0.1502
2026-01-04 00:07:09,076: t15.2023.09.03 val PER: 0.2518
2026-01-04 00:07:09,076: t15.2023.09.24 val PER: 0.1966
2026-01-04 00:07:09,077: t15.2023.09.29 val PER: 0.1966
2026-01-04 00:07:09,077: t15.2023.10.01 val PER: 0.2629
2026-01-04 00:07:09,077: t15.2023.10.06 val PER: 0.1722
2026-01-04 00:07:09,077: t15.2023.10.08 val PER: 0.3153
2026-01-04 00:07:09,077: t15.2023.10.13 val PER: 0.3080
2026-01-04 00:07:09,077: t15.2023.10.15 val PER: 0.2320
2026-01-04 00:07:09,077: t15.2023.10.20 val PER: 0.2248
2026-01-04 00:07:09,077: t15.2023.10.22 val PER: 0.1927
2026-01-04 00:07:09,077: t15.2023.11.03 val PER: 0.2341
2026-01-04 00:07:09,077: t15.2023.11.04 val PER: 0.0785
2026-01-04 00:07:09,077: t15.2023.11.17 val PER: 0.1026
2026-01-04 00:07:09,078: t15.2023.11.19 val PER: 0.1058
2026-01-04 00:07:09,078: t15.2023.11.26 val PER: 0.2645
2026-01-04 00:07:09,078: t15.2023.12.03 val PER: 0.2090
2026-01-04 00:07:09,078: t15.2023.12.08 val PER: 0.2164
2026-01-04 00:07:09,078: t15.2023.12.10 val PER: 0.1669
2026-01-04 00:07:09,078: t15.2023.12.17 val PER: 0.2349
2026-01-04 00:07:09,078: t15.2023.12.29 val PER: 0.2533
2026-01-04 00:07:09,078: t15.2024.02.25 val PER: 0.2191
2026-01-04 00:07:09,078: t15.2024.03.08 val PER: 0.3257
2026-01-04 00:07:09,078: t15.2024.03.15 val PER: 0.3014
2026-01-04 00:07:09,078: t15.2024.03.17 val PER: 0.2406
2026-01-04 00:07:09,078: t15.2024.05.10 val PER: 0.2496
2026-01-04 00:07:09,078: t15.2024.06.14 val PER: 0.2681
2026-01-04 00:07:09,078: t15.2024.07.19 val PER: 0.3652
2026-01-04 00:07:09,078: t15.2024.07.21 val PER: 0.1897
2026-01-04 00:07:09,078: t15.2024.07.28 val PER: 0.2324
2026-01-04 00:07:09,078: t15.2025.01.10 val PER: 0.4325
2026-01-04 00:07:09,079: t15.2025.01.12 val PER: 0.2771
2026-01-04 00:07:09,079: t15.2025.03.14 val PER: 0.4201
2026-01-04 00:07:09,079: t15.2025.03.16 val PER: 0.3181
2026-01-04 00:07:09,079: t15.2025.03.30 val PER: 0.4034
2026-01-04 00:07:09,079: t15.2025.04.13 val PER: 0.3096
2026-01-04 00:07:09,080: New best val WER(1gram) 63.45% --> 63.20%
2026-01-04 00:07:09,080: Checkpointing model
2026-01-04 00:07:09,700: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:07:09,935: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_4000
2026-01-04 00:07:27,399: Train batch 4200: loss: 22.58 grad norm: 65.71 time: 0.078
2026-01-04 00:07:44,683: Train batch 4400: loss: 16.46 grad norm: 55.44 time: 0.066
2026-01-04 00:07:53,405: Running test after training batch: 4500
2026-01-04 00:07:53,533: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:07:58,270: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 00:07:58,298: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the rust nett
2026-01-04 00:07:59,841: Val batch 4500: PER (avg): 0.2363 CTC Loss (avg): 23.1062 WER(1gram): 61.93% (n=64) time: 6.435
2026-01-04 00:07:59,841: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 00:07:59,842: t15.2023.08.13 val PER: 0.2089
2026-01-04 00:07:59,842: t15.2023.08.18 val PER: 0.1802
2026-01-04 00:07:59,842: t15.2023.08.20 val PER: 0.1946
2026-01-04 00:07:59,842: t15.2023.08.25 val PER: 0.1370
2026-01-04 00:07:59,842: t15.2023.08.27 val PER: 0.2605
2026-01-04 00:07:59,842: t15.2023.09.01 val PER: 0.1485
2026-01-04 00:07:59,842: t15.2023.09.03 val PER: 0.2352
2026-01-04 00:07:59,842: t15.2023.09.24 val PER: 0.1760
2026-01-04 00:07:59,842: t15.2023.09.29 val PER: 0.1927
2026-01-04 00:07:59,842: t15.2023.10.01 val PER: 0.2550
2026-01-04 00:07:59,842: t15.2023.10.06 val PER: 0.1572
2026-01-04 00:07:59,842: t15.2023.10.08 val PER: 0.3126
2026-01-04 00:07:59,843: t15.2023.10.13 val PER: 0.2964
2026-01-04 00:07:59,843: t15.2023.10.15 val PER: 0.2301
2026-01-04 00:07:59,843: t15.2023.10.20 val PER: 0.2148
2026-01-04 00:07:59,843: t15.2023.10.22 val PER: 0.1915
2026-01-04 00:07:59,843: t15.2023.11.03 val PER: 0.2361
2026-01-04 00:07:59,843: t15.2023.11.04 val PER: 0.0683
2026-01-04 00:07:59,843: t15.2023.11.17 val PER: 0.0918
2026-01-04 00:07:59,843: t15.2023.11.19 val PER: 0.0858
2026-01-04 00:07:59,843: t15.2023.11.26 val PER: 0.2674
2026-01-04 00:07:59,843: t15.2023.12.03 val PER: 0.2143
2026-01-04 00:07:59,843: t15.2023.12.08 val PER: 0.2044
2026-01-04 00:07:59,843: t15.2023.12.10 val PER: 0.1708
2026-01-04 00:07:59,843: t15.2023.12.17 val PER: 0.2287
2026-01-04 00:07:59,843: t15.2023.12.29 val PER: 0.2478
2026-01-04 00:07:59,844: t15.2024.02.25 val PER: 0.1980
2026-01-04 00:07:59,844: t15.2024.03.08 val PER: 0.3229
2026-01-04 00:07:59,844: t15.2024.03.15 val PER: 0.2902
2026-01-04 00:07:59,844: t15.2024.03.17 val PER: 0.2350
2026-01-04 00:07:59,844: t15.2024.05.10 val PER: 0.2467
2026-01-04 00:07:59,844: t15.2024.06.14 val PER: 0.2350
2026-01-04 00:07:59,844: t15.2024.07.19 val PER: 0.3415
2026-01-04 00:07:59,844: t15.2024.07.21 val PER: 0.1710
2026-01-04 00:07:59,844: t15.2024.07.28 val PER: 0.2169
2026-01-04 00:07:59,844: t15.2025.01.10 val PER: 0.4036
2026-01-04 00:07:59,844: t15.2025.01.12 val PER: 0.2694
2026-01-04 00:07:59,844: t15.2025.03.14 val PER: 0.3935
2026-01-04 00:07:59,844: t15.2025.03.16 val PER: 0.3024
2026-01-04 00:07:59,844: t15.2025.03.30 val PER: 0.3977
2026-01-04 00:07:59,844: t15.2025.04.13 val PER: 0.3067
2026-01-04 00:07:59,846: New best val WER(1gram) 63.20% --> 61.93%
2026-01-04 00:07:59,846: Checkpointing model
2026-01-04 00:08:00,464: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:08:00,704: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_4500
2026-01-04 00:08:09,144: Train batch 4600: loss: 19.98 grad norm: 69.44 time: 0.062
2026-01-04 00:08:26,166: Train batch 4800: loss: 13.13 grad norm: 53.52 time: 0.063
2026-01-04 00:08:43,267: Train batch 5000: loss: 31.13 grad norm: 85.28 time: 0.063
2026-01-04 00:08:43,267: Running test after training batch: 5000
2026-01-04 00:08:43,394: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:08:48,120: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the cold at this point as will
2026-01-04 00:08:48,148: WER debug example
  GT : how does it keep the cost down
  PR : houde dest it heap the lust nett
2026-01-04 00:08:49,754: Val batch 5000: PER (avg): 0.2241 CTC Loss (avg): 21.9198 WER(1gram): 61.68% (n=64) time: 6.486
2026-01-04 00:08:49,754: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 00:08:49,754: t15.2023.08.13 val PER: 0.1892
2026-01-04 00:08:49,754: t15.2023.08.18 val PER: 0.1718
2026-01-04 00:08:49,755: t15.2023.08.20 val PER: 0.1739
2026-01-04 00:08:49,755: t15.2023.08.25 val PER: 0.1310
2026-01-04 00:08:49,755: t15.2023.08.27 val PER: 0.2412
2026-01-04 00:08:49,755: t15.2023.09.01 val PER: 0.1331
2026-01-04 00:08:49,755: t15.2023.09.03 val PER: 0.2245
2026-01-04 00:08:49,755: t15.2023.09.24 val PER: 0.1893
2026-01-04 00:08:49,755: t15.2023.09.29 val PER: 0.1787
2026-01-04 00:08:49,755: t15.2023.10.01 val PER: 0.2332
2026-01-04 00:08:49,755: t15.2023.10.06 val PER: 0.1389
2026-01-04 00:08:49,756: t15.2023.10.08 val PER: 0.3139
2026-01-04 00:08:49,756: t15.2023.10.13 val PER: 0.2684
2026-01-04 00:08:49,756: t15.2023.10.15 val PER: 0.2169
2026-01-04 00:08:49,756: t15.2023.10.20 val PER: 0.2383
2026-01-04 00:08:49,756: t15.2023.10.22 val PER: 0.1804
2026-01-04 00:08:49,756: t15.2023.11.03 val PER: 0.2191
2026-01-04 00:08:49,756: t15.2023.11.04 val PER: 0.0546
2026-01-04 00:08:49,756: t15.2023.11.17 val PER: 0.0778
2026-01-04 00:08:49,756: t15.2023.11.19 val PER: 0.0639
2026-01-04 00:08:49,756: t15.2023.11.26 val PER: 0.2428
2026-01-04 00:08:49,756: t15.2023.12.03 val PER: 0.1912
2026-01-04 00:08:49,756: t15.2023.12.08 val PER: 0.1858
2026-01-04 00:08:49,756: t15.2023.12.10 val PER: 0.1708
2026-01-04 00:08:49,756: t15.2023.12.17 val PER: 0.2183
2026-01-04 00:08:49,756: t15.2023.12.29 val PER: 0.2217
2026-01-04 00:08:49,757: t15.2024.02.25 val PER: 0.1896
2026-01-04 00:08:49,757: t15.2024.03.08 val PER: 0.3314
2026-01-04 00:08:49,757: t15.2024.03.15 val PER: 0.2821
2026-01-04 00:08:49,757: t15.2024.03.17 val PER: 0.2343
2026-01-04 00:08:49,757: t15.2024.05.10 val PER: 0.2363
2026-01-04 00:08:49,757: t15.2024.06.14 val PER: 0.2508
2026-01-04 00:08:49,757: t15.2024.07.19 val PER: 0.3355
2026-01-04 00:08:49,757: t15.2024.07.21 val PER: 0.1662
2026-01-04 00:08:49,757: t15.2024.07.28 val PER: 0.2007
2026-01-04 00:08:49,757: t15.2025.01.10 val PER: 0.3994
2026-01-04 00:08:49,757: t15.2025.01.12 val PER: 0.2571
2026-01-04 00:08:49,757: t15.2025.03.14 val PER: 0.3743
2026-01-04 00:08:49,757: t15.2025.03.16 val PER: 0.2827
2026-01-04 00:08:49,757: t15.2025.03.30 val PER: 0.3966
2026-01-04 00:08:49,758: t15.2025.04.13 val PER: 0.2996
2026-01-04 00:08:49,758: New best val WER(1gram) 61.93% --> 61.68%
2026-01-04 00:08:49,758: Checkpointing model
2026-01-04 00:08:50,379: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:08:50,618: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_5000
2026-01-04 00:09:07,720: Train batch 5200: loss: 16.02 grad norm: 62.40 time: 0.051
2026-01-04 00:09:24,766: Train batch 5400: loss: 16.63 grad norm: 57.59 time: 0.067
2026-01-04 00:09:33,333: Running test after training batch: 5500
2026-01-04 00:09:33,474: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:09:38,524: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point will
2026-01-04 00:09:38,553: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cussed it
2026-01-04 00:09:40,102: Val batch 5500: PER (avg): 0.2156 CTC Loss (avg): 20.8375 WER(1gram): 57.11% (n=64) time: 6.768
2026-01-04 00:09:40,103: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-04 00:09:40,103: t15.2023.08.13 val PER: 0.1819
2026-01-04 00:09:40,103: t15.2023.08.18 val PER: 0.1635
2026-01-04 00:09:40,103: t15.2023.08.20 val PER: 0.1708
2026-01-04 00:09:40,103: t15.2023.08.25 val PER: 0.1265
2026-01-04 00:09:40,103: t15.2023.08.27 val PER: 0.2331
2026-01-04 00:09:40,103: t15.2023.09.01 val PER: 0.1356
2026-01-04 00:09:40,103: t15.2023.09.03 val PER: 0.2233
2026-01-04 00:09:40,103: t15.2023.09.24 val PER: 0.1784
2026-01-04 00:09:40,103: t15.2023.09.29 val PER: 0.1717
2026-01-04 00:09:40,104: t15.2023.10.01 val PER: 0.2325
2026-01-04 00:09:40,104: t15.2023.10.06 val PER: 0.1281
2026-01-04 00:09:40,104: t15.2023.10.08 val PER: 0.2828
2026-01-04 00:09:40,104: t15.2023.10.13 val PER: 0.2801
2026-01-04 00:09:40,104: t15.2023.10.15 val PER: 0.2044
2026-01-04 00:09:40,104: t15.2023.10.20 val PER: 0.2248
2026-01-04 00:09:40,104: t15.2023.10.22 val PER: 0.1682
2026-01-04 00:09:40,104: t15.2023.11.03 val PER: 0.2239
2026-01-04 00:09:40,104: t15.2023.11.04 val PER: 0.0683
2026-01-04 00:09:40,104: t15.2023.11.17 val PER: 0.0778
2026-01-04 00:09:40,104: t15.2023.11.19 val PER: 0.0619
2026-01-04 00:09:40,104: t15.2023.11.26 val PER: 0.2196
2026-01-04 00:09:40,104: t15.2023.12.03 val PER: 0.1859
2026-01-04 00:09:40,104: t15.2023.12.08 val PER: 0.1877
2026-01-04 00:09:40,104: t15.2023.12.10 val PER: 0.1551
2026-01-04 00:09:40,104: t15.2023.12.17 val PER: 0.2183
2026-01-04 00:09:40,105: t15.2023.12.29 val PER: 0.2162
2026-01-04 00:09:40,105: t15.2024.02.25 val PER: 0.1671
2026-01-04 00:09:40,105: t15.2024.03.08 val PER: 0.3044
2026-01-04 00:09:40,105: t15.2024.03.15 val PER: 0.2633
2026-01-04 00:09:40,105: t15.2024.03.17 val PER: 0.2204
2026-01-04 00:09:40,105: t15.2024.05.10 val PER: 0.2333
2026-01-04 00:09:40,105: t15.2024.06.14 val PER: 0.2303
2026-01-04 00:09:40,105: t15.2024.07.19 val PER: 0.3217
2026-01-04 00:09:40,105: t15.2024.07.21 val PER: 0.1676
2026-01-04 00:09:40,105: t15.2024.07.28 val PER: 0.2037
2026-01-04 00:09:40,105: t15.2025.01.10 val PER: 0.3802
2026-01-04 00:09:40,105: t15.2025.01.12 val PER: 0.2379
2026-01-04 00:09:40,105: t15.2025.03.14 val PER: 0.3521
2026-01-04 00:09:40,105: t15.2025.03.16 val PER: 0.2709
2026-01-04 00:09:40,105: t15.2025.03.30 val PER: 0.3632
2026-01-04 00:09:40,105: t15.2025.04.13 val PER: 0.2953
2026-01-04 00:09:40,107: New best val WER(1gram) 61.68% --> 57.11%
2026-01-04 00:09:40,107: Checkpointing model
2026-01-04 00:09:40,730: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:09:40,969: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_5500
2026-01-04 00:09:49,463: Train batch 5600: loss: 18.45 grad norm: 74.14 time: 0.062
2026-01-04 00:10:06,522: Train batch 5800: loss: 12.79 grad norm: 56.18 time: 0.082
2026-01-04 00:10:23,646: Train batch 6000: loss: 13.69 grad norm: 55.11 time: 0.048
2026-01-04 00:10:23,646: Running test after training batch: 6000
2026-01-04 00:10:23,759: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:10:28,517: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 00:10:28,547: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 00:10:30,171: Val batch 6000: PER (avg): 0.2114 CTC Loss (avg): 20.7586 WER(1gram): 55.84% (n=64) time: 6.525
2026-01-04 00:10:30,172: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 00:10:30,172: t15.2023.08.13 val PER: 0.1798
2026-01-04 00:10:30,172: t15.2023.08.18 val PER: 0.1685
2026-01-04 00:10:30,172: t15.2023.08.20 val PER: 0.1716
2026-01-04 00:10:30,172: t15.2023.08.25 val PER: 0.1130
2026-01-04 00:10:30,172: t15.2023.08.27 val PER: 0.2476
2026-01-04 00:10:30,172: t15.2023.09.01 val PER: 0.1331
2026-01-04 00:10:30,172: t15.2023.09.03 val PER: 0.2078
2026-01-04 00:10:30,172: t15.2023.09.24 val PER: 0.1748
2026-01-04 00:10:30,172: t15.2023.09.29 val PER: 0.1768
2026-01-04 00:10:30,172: t15.2023.10.01 val PER: 0.2232
2026-01-04 00:10:30,172: t15.2023.10.06 val PER: 0.1335
2026-01-04 00:10:30,173: t15.2023.10.08 val PER: 0.2963
2026-01-04 00:10:30,173: t15.2023.10.13 val PER: 0.2622
2026-01-04 00:10:30,173: t15.2023.10.15 val PER: 0.2070
2026-01-04 00:10:30,173: t15.2023.10.20 val PER: 0.2282
2026-01-04 00:10:30,173: t15.2023.10.22 val PER: 0.1737
2026-01-04 00:10:30,173: t15.2023.11.03 val PER: 0.2266
2026-01-04 00:10:30,173: t15.2023.11.04 val PER: 0.0683
2026-01-04 00:10:30,173: t15.2023.11.17 val PER: 0.0809
2026-01-04 00:10:30,173: t15.2023.11.19 val PER: 0.0699
2026-01-04 00:10:30,173: t15.2023.11.26 val PER: 0.2116
2026-01-04 00:10:30,173: t15.2023.12.03 val PER: 0.1670
2026-01-04 00:10:30,173: t15.2023.12.08 val PER: 0.1764
2026-01-04 00:10:30,173: t15.2023.12.10 val PER: 0.1524
2026-01-04 00:10:30,173: t15.2023.12.17 val PER: 0.2225
2026-01-04 00:10:30,173: t15.2023.12.29 val PER: 0.2107
2026-01-04 00:10:30,174: t15.2024.02.25 val PER: 0.1601
2026-01-04 00:10:30,174: t15.2024.03.08 val PER: 0.2987
2026-01-04 00:10:30,174: t15.2024.03.15 val PER: 0.2627
2026-01-04 00:10:30,174: t15.2024.03.17 val PER: 0.2092
2026-01-04 00:10:30,174: t15.2024.05.10 val PER: 0.2303
2026-01-04 00:10:30,174: t15.2024.06.14 val PER: 0.2098
2026-01-04 00:10:30,174: t15.2024.07.19 val PER: 0.3164
2026-01-04 00:10:30,174: t15.2024.07.21 val PER: 0.1655
2026-01-04 00:10:30,174: t15.2024.07.28 val PER: 0.1941
2026-01-04 00:10:30,174: t15.2025.01.10 val PER: 0.3829
2026-01-04 00:10:30,174: t15.2025.01.12 val PER: 0.2209
2026-01-04 00:10:30,175: t15.2025.03.14 val PER: 0.3728
2026-01-04 00:10:30,175: t15.2025.03.16 val PER: 0.2435
2026-01-04 00:10:30,175: t15.2025.03.30 val PER: 0.3575
2026-01-04 00:10:30,175: t15.2025.04.13 val PER: 0.2710
2026-01-04 00:10:30,176: New best val WER(1gram) 57.11% --> 55.84%
2026-01-04 00:10:30,176: Checkpointing model
2026-01-04 00:10:30,796: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:10:31,033: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_6000
2026-01-04 00:10:48,268: Train batch 6200: loss: 15.13 grad norm: 59.98 time: 0.070
2026-01-04 00:11:05,692: Train batch 6400: loss: 18.30 grad norm: 67.06 time: 0.062
2026-01-04 00:11:14,259: Running test after training batch: 6500
2026-01-04 00:11:14,390: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:11:19,132: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point is will
2026-01-04 00:11:19,162: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 00:11:20,753: Val batch 6500: PER (avg): 0.2055 CTC Loss (avg): 20.1227 WER(1gram): 52.28% (n=64) time: 6.494
2026-01-04 00:11:20,754: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-04 00:11:20,754: t15.2023.08.13 val PER: 0.1653
2026-01-04 00:11:20,754: t15.2023.08.18 val PER: 0.1400
2026-01-04 00:11:20,754: t15.2023.08.20 val PER: 0.1557
2026-01-04 00:11:20,754: t15.2023.08.25 val PER: 0.1114
2026-01-04 00:11:20,754: t15.2023.08.27 val PER: 0.2138
2026-01-04 00:11:20,754: t15.2023.09.01 val PER: 0.1242
2026-01-04 00:11:20,754: t15.2023.09.03 val PER: 0.2007
2026-01-04 00:11:20,754: t15.2023.09.24 val PER: 0.1650
2026-01-04 00:11:20,754: t15.2023.09.29 val PER: 0.1768
2026-01-04 00:11:20,754: t15.2023.10.01 val PER: 0.2087
2026-01-04 00:11:20,754: t15.2023.10.06 val PER: 0.1238
2026-01-04 00:11:20,755: t15.2023.10.08 val PER: 0.2855
2026-01-04 00:11:20,755: t15.2023.10.13 val PER: 0.2661
2026-01-04 00:11:20,755: t15.2023.10.15 val PER: 0.2149
2026-01-04 00:11:20,755: t15.2023.10.20 val PER: 0.2148
2026-01-04 00:11:20,755: t15.2023.10.22 val PER: 0.1592
2026-01-04 00:11:20,755: t15.2023.11.03 val PER: 0.2218
2026-01-04 00:11:20,755: t15.2023.11.04 val PER: 0.0648
2026-01-04 00:11:20,755: t15.2023.11.17 val PER: 0.0700
2026-01-04 00:11:20,755: t15.2023.11.19 val PER: 0.0699
2026-01-04 00:11:20,755: t15.2023.11.26 val PER: 0.2152
2026-01-04 00:11:20,756: t15.2023.12.03 val PER: 0.1859
2026-01-04 00:11:20,756: t15.2023.12.08 val PER: 0.1678
2026-01-04 00:11:20,756: t15.2023.12.10 val PER: 0.1393
2026-01-04 00:11:20,756: t15.2023.12.17 val PER: 0.2089
2026-01-04 00:11:20,756: t15.2023.12.29 val PER: 0.2128
2026-01-04 00:11:20,756: t15.2024.02.25 val PER: 0.1671
2026-01-04 00:11:20,757: t15.2024.03.08 val PER: 0.2973
2026-01-04 00:11:20,757: t15.2024.03.15 val PER: 0.2658
2026-01-04 00:11:20,757: t15.2024.03.17 val PER: 0.2008
2026-01-04 00:11:20,757: t15.2024.05.10 val PER: 0.2303
2026-01-04 00:11:20,757: t15.2024.06.14 val PER: 0.2129
2026-01-04 00:11:20,757: t15.2024.07.19 val PER: 0.3092
2026-01-04 00:11:20,757: t15.2024.07.21 val PER: 0.1510
2026-01-04 00:11:20,757: t15.2024.07.28 val PER: 0.1853
2026-01-04 00:11:20,757: t15.2025.01.10 val PER: 0.3595
2026-01-04 00:11:20,757: t15.2025.01.12 val PER: 0.2225
2026-01-04 00:11:20,757: t15.2025.03.14 val PER: 0.3861
2026-01-04 00:11:20,758: t15.2025.03.16 val PER: 0.2461
2026-01-04 00:11:20,758: t15.2025.03.30 val PER: 0.3368
2026-01-04 00:11:20,758: t15.2025.04.13 val PER: 0.2753
2026-01-04 00:11:20,758: New best val WER(1gram) 55.84% --> 52.28%
2026-01-04 00:11:20,758: Checkpointing model
2026-01-04 00:11:21,379: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:11:21,617: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_6500
2026-01-04 00:11:30,091: Train batch 6600: loss: 11.97 grad norm: 57.37 time: 0.045
2026-01-04 00:11:47,368: Train batch 6800: loss: 14.90 grad norm: 57.48 time: 0.048
2026-01-04 00:12:04,680: Train batch 7000: loss: 16.39 grad norm: 65.06 time: 0.060
2026-01-04 00:12:04,680: Running test after training batch: 7000
2026-01-04 00:12:04,830: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:12:10,007: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-04 00:12:10,036: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost nett
2026-01-04 00:12:11,663: Val batch 7000: PER (avg): 0.1969 CTC Loss (avg): 19.2861 WER(1gram): 55.58% (n=64) time: 6.982
2026-01-04 00:12:11,663: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 00:12:11,663: t15.2023.08.13 val PER: 0.1684
2026-01-04 00:12:11,663: t15.2023.08.18 val PER: 0.1425
2026-01-04 00:12:11,664: t15.2023.08.20 val PER: 0.1477
2026-01-04 00:12:11,664: t15.2023.08.25 val PER: 0.1084
2026-01-04 00:12:11,664: t15.2023.08.27 val PER: 0.2090
2026-01-04 00:12:11,665: t15.2023.09.01 val PER: 0.1112
2026-01-04 00:12:11,665: t15.2023.09.03 val PER: 0.1948
2026-01-04 00:12:11,665: t15.2023.09.24 val PER: 0.1675
2026-01-04 00:12:11,665: t15.2023.09.29 val PER: 0.1717
2026-01-04 00:12:11,665: t15.2023.10.01 val PER: 0.2127
2026-01-04 00:12:11,665: t15.2023.10.06 val PER: 0.1119
2026-01-04 00:12:11,665: t15.2023.10.08 val PER: 0.2923
2026-01-04 00:12:11,665: t15.2023.10.13 val PER: 0.2498
2026-01-04 00:12:11,666: t15.2023.10.15 val PER: 0.1925
2026-01-04 00:12:11,666: t15.2023.10.20 val PER: 0.2215
2026-01-04 00:12:11,666: t15.2023.10.22 val PER: 0.1437
2026-01-04 00:12:11,666: t15.2023.11.03 val PER: 0.2028
2026-01-04 00:12:11,666: t15.2023.11.04 val PER: 0.0478
2026-01-04 00:12:11,666: t15.2023.11.17 val PER: 0.0669
2026-01-04 00:12:11,666: t15.2023.11.19 val PER: 0.0559
2026-01-04 00:12:11,666: t15.2023.11.26 val PER: 0.1978
2026-01-04 00:12:11,666: t15.2023.12.03 val PER: 0.1607
2026-01-04 00:12:11,666: t15.2023.12.08 val PER: 0.1551
2026-01-04 00:12:11,667: t15.2023.12.10 val PER: 0.1380
2026-01-04 00:12:11,667: t15.2023.12.17 val PER: 0.1861
2026-01-04 00:12:11,667: t15.2023.12.29 val PER: 0.1997
2026-01-04 00:12:11,667: t15.2024.02.25 val PER: 0.1629
2026-01-04 00:12:11,667: t15.2024.03.08 val PER: 0.2873
2026-01-04 00:12:11,667: t15.2024.03.15 val PER: 0.2402
2026-01-04 00:12:11,667: t15.2024.03.17 val PER: 0.1994
2026-01-04 00:12:11,667: t15.2024.05.10 val PER: 0.2214
2026-01-04 00:12:11,667: t15.2024.06.14 val PER: 0.2177
2026-01-04 00:12:11,667: t15.2024.07.19 val PER: 0.3065
2026-01-04 00:12:11,667: t15.2024.07.21 val PER: 0.1331
2026-01-04 00:12:11,668: t15.2024.07.28 val PER: 0.1801
2026-01-04 00:12:11,668: t15.2025.01.10 val PER: 0.3719
2026-01-04 00:12:11,668: t15.2025.01.12 val PER: 0.2102
2026-01-04 00:12:11,668: t15.2025.03.14 val PER: 0.3609
2026-01-04 00:12:11,668: t15.2025.03.16 val PER: 0.2605
2026-01-04 00:12:11,668: t15.2025.03.30 val PER: 0.3540
2026-01-04 00:12:11,668: t15.2025.04.13 val PER: 0.2582
2026-01-04 00:12:11,896: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_7000
2026-01-04 00:12:29,299: Train batch 7200: loss: 13.57 grad norm: 60.00 time: 0.078
2026-01-04 00:12:46,626: Train batch 7400: loss: 13.06 grad norm: 56.14 time: 0.075
2026-01-04 00:12:55,144: Running test after training batch: 7500
2026-01-04 00:12:55,332: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:13:00,052: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 00:13:00,081: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cussed et
2026-01-04 00:13:01,704: Val batch 7500: PER (avg): 0.1891 CTC Loss (avg): 18.5997 WER(1gram): 55.33% (n=64) time: 6.559
2026-01-04 00:13:01,704: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 00:13:01,704: t15.2023.08.13 val PER: 0.1549
2026-01-04 00:13:01,704: t15.2023.08.18 val PER: 0.1391
2026-01-04 00:13:01,705: t15.2023.08.20 val PER: 0.1390
2026-01-04 00:13:01,705: t15.2023.08.25 val PER: 0.1009
2026-01-04 00:13:01,705: t15.2023.08.27 val PER: 0.2058
2026-01-04 00:13:01,705: t15.2023.09.01 val PER: 0.1096
2026-01-04 00:13:01,705: t15.2023.09.03 val PER: 0.1829
2026-01-04 00:13:01,705: t15.2023.09.24 val PER: 0.1493
2026-01-04 00:13:01,705: t15.2023.09.29 val PER: 0.1621
2026-01-04 00:13:01,705: t15.2023.10.01 val PER: 0.1915
2026-01-04 00:13:01,705: t15.2023.10.06 val PER: 0.1195
2026-01-04 00:13:01,705: t15.2023.10.08 val PER: 0.2639
2026-01-04 00:13:01,705: t15.2023.10.13 val PER: 0.2475
2026-01-04 00:13:01,705: t15.2023.10.15 val PER: 0.1912
2026-01-04 00:13:01,705: t15.2023.10.20 val PER: 0.2013
2026-01-04 00:13:01,705: t15.2023.10.22 val PER: 0.1414
2026-01-04 00:13:01,705: t15.2023.11.03 val PER: 0.2022
2026-01-04 00:13:01,706: t15.2023.11.04 val PER: 0.0410
2026-01-04 00:13:01,706: t15.2023.11.17 val PER: 0.0653
2026-01-04 00:13:01,706: t15.2023.11.19 val PER: 0.0499
2026-01-04 00:13:01,706: t15.2023.11.26 val PER: 0.1732
2026-01-04 00:13:01,706: t15.2023.12.03 val PER: 0.1544
2026-01-04 00:13:01,706: t15.2023.12.08 val PER: 0.1565
2026-01-04 00:13:01,706: t15.2023.12.10 val PER: 0.1314
2026-01-04 00:13:01,706: t15.2023.12.17 val PER: 0.1726
2026-01-04 00:13:01,706: t15.2023.12.29 val PER: 0.1915
2026-01-04 00:13:01,706: t15.2024.02.25 val PER: 0.1517
2026-01-04 00:13:01,706: t15.2024.03.08 val PER: 0.2774
2026-01-04 00:13:01,706: t15.2024.03.15 val PER: 0.2489
2026-01-04 00:13:01,706: t15.2024.03.17 val PER: 0.1897
2026-01-04 00:13:01,706: t15.2024.05.10 val PER: 0.2259
2026-01-04 00:13:01,707: t15.2024.06.14 val PER: 0.2050
2026-01-04 00:13:01,707: t15.2024.07.19 val PER: 0.2920
2026-01-04 00:13:01,707: t15.2024.07.21 val PER: 0.1359
2026-01-04 00:13:01,707: t15.2024.07.28 val PER: 0.1684
2026-01-04 00:13:01,707: t15.2025.01.10 val PER: 0.3402
2026-01-04 00:13:01,707: t15.2025.01.12 val PER: 0.2032
2026-01-04 00:13:01,707: t15.2025.03.14 val PER: 0.3713
2026-01-04 00:13:01,707: t15.2025.03.16 val PER: 0.2317
2026-01-04 00:13:01,707: t15.2025.03.30 val PER: 0.3471
2026-01-04 00:13:01,707: t15.2025.04.13 val PER: 0.2511
2026-01-04 00:13:01,933: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_7500
2026-01-04 00:13:10,628: Train batch 7600: loss: 15.29 grad norm: 62.72 time: 0.068
2026-01-04 00:13:27,895: Train batch 7800: loss: 13.65 grad norm: 61.34 time: 0.055
2026-01-04 00:13:45,559: Train batch 8000: loss: 10.59 grad norm: 53.54 time: 0.071
2026-01-04 00:13:45,559: Running test after training batch: 8000
2026-01-04 00:13:45,654: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:13:50,378: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-04 00:13:50,408: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-04 00:13:52,086: Val batch 8000: PER (avg): 0.1851 CTC Loss (avg): 18.1370 WER(1gram): 54.06% (n=64) time: 6.527
2026-01-04 00:13:52,086: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-04 00:13:52,087: t15.2023.08.13 val PER: 0.1559
2026-01-04 00:13:52,087: t15.2023.08.18 val PER: 0.1308
2026-01-04 00:13:52,087: t15.2023.08.20 val PER: 0.1485
2026-01-04 00:13:52,087: t15.2023.08.25 val PER: 0.1114
2026-01-04 00:13:52,087: t15.2023.08.27 val PER: 0.2138
2026-01-04 00:13:52,087: t15.2023.09.01 val PER: 0.1063
2026-01-04 00:13:52,087: t15.2023.09.03 val PER: 0.1805
2026-01-04 00:13:52,087: t15.2023.09.24 val PER: 0.1541
2026-01-04 00:13:52,087: t15.2023.09.29 val PER: 0.1506
2026-01-04 00:13:52,087: t15.2023.10.01 val PER: 0.1975
2026-01-04 00:13:52,087: t15.2023.10.06 val PER: 0.1076
2026-01-04 00:13:52,087: t15.2023.10.08 val PER: 0.2666
2026-01-04 00:13:52,087: t15.2023.10.13 val PER: 0.2374
2026-01-04 00:13:52,088: t15.2023.10.15 val PER: 0.1925
2026-01-04 00:13:52,088: t15.2023.10.20 val PER: 0.2114
2026-01-04 00:13:52,088: t15.2023.10.22 val PER: 0.1381
2026-01-04 00:13:52,088: t15.2023.11.03 val PER: 0.1995
2026-01-04 00:13:52,088: t15.2023.11.04 val PER: 0.0512
2026-01-04 00:13:52,088: t15.2023.11.17 val PER: 0.0607
2026-01-04 00:13:52,088: t15.2023.11.19 val PER: 0.0519
2026-01-04 00:13:52,088: t15.2023.11.26 val PER: 0.1746
2026-01-04 00:13:52,088: t15.2023.12.03 val PER: 0.1607
2026-01-04 00:13:52,088: t15.2023.12.08 val PER: 0.1411
2026-01-04 00:13:52,088: t15.2023.12.10 val PER: 0.1353
2026-01-04 00:13:52,088: t15.2023.12.17 val PER: 0.1819
2026-01-04 00:13:52,088: t15.2023.12.29 val PER: 0.1743
2026-01-04 00:13:52,088: t15.2024.02.25 val PER: 0.1489
2026-01-04 00:13:52,089: t15.2024.03.08 val PER: 0.2788
2026-01-04 00:13:52,089: t15.2024.03.15 val PER: 0.2301
2026-01-04 00:13:52,089: t15.2024.03.17 val PER: 0.1841
2026-01-04 00:13:52,089: t15.2024.05.10 val PER: 0.2051
2026-01-04 00:13:52,089: t15.2024.06.14 val PER: 0.2098
2026-01-04 00:13:52,089: t15.2024.07.19 val PER: 0.2861
2026-01-04 00:13:52,089: t15.2024.07.21 val PER: 0.1248
2026-01-04 00:13:52,089: t15.2024.07.28 val PER: 0.1596
2026-01-04 00:13:52,089: t15.2025.01.10 val PER: 0.3444
2026-01-04 00:13:52,089: t15.2025.01.12 val PER: 0.1994
2026-01-04 00:13:52,089: t15.2025.03.14 val PER: 0.3476
2026-01-04 00:13:52,089: t15.2025.03.16 val PER: 0.2134
2026-01-04 00:13:52,089: t15.2025.03.30 val PER: 0.3414
2026-01-04 00:13:52,089: t15.2025.04.13 val PER: 0.2611
2026-01-04 00:13:52,324: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_8000
2026-01-04 00:14:09,846: Train batch 8200: loss: 9.51 grad norm: 50.36 time: 0.053
2026-01-04 00:14:27,130: Train batch 8400: loss: 9.44 grad norm: 48.82 time: 0.064
2026-01-04 00:14:35,934: Running test after training batch: 8500
2026-01-04 00:14:36,041: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:14:41,160: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the cold at this point as will
2026-01-04 00:14:41,190: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ent
2026-01-04 00:14:42,834: Val batch 8500: PER (avg): 0.1800 CTC Loss (avg): 17.7213 WER(1gram): 49.75% (n=64) time: 6.900
2026-01-04 00:14:42,834: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 00:14:42,834: t15.2023.08.13 val PER: 0.1466
2026-01-04 00:14:42,835: t15.2023.08.18 val PER: 0.1341
2026-01-04 00:14:42,835: t15.2023.08.20 val PER: 0.1430
2026-01-04 00:14:42,835: t15.2023.08.25 val PER: 0.1175
2026-01-04 00:14:42,835: t15.2023.08.27 val PER: 0.1994
2026-01-04 00:14:42,835: t15.2023.09.01 val PER: 0.0990
2026-01-04 00:14:42,835: t15.2023.09.03 val PER: 0.2031
2026-01-04 00:14:42,835: t15.2023.09.24 val PER: 0.1444
2026-01-04 00:14:42,835: t15.2023.09.29 val PER: 0.1544
2026-01-04 00:14:42,835: t15.2023.10.01 val PER: 0.2001
2026-01-04 00:14:42,836: t15.2023.10.06 val PER: 0.0936
2026-01-04 00:14:42,836: t15.2023.10.08 val PER: 0.2625
2026-01-04 00:14:42,836: t15.2023.10.13 val PER: 0.2358
2026-01-04 00:14:42,836: t15.2023.10.15 val PER: 0.1760
2026-01-04 00:14:42,836: t15.2023.10.20 val PER: 0.1846
2026-01-04 00:14:42,836: t15.2023.10.22 val PER: 0.1514
2026-01-04 00:14:42,836: t15.2023.11.03 val PER: 0.2008
2026-01-04 00:14:42,836: t15.2023.11.04 val PER: 0.0444
2026-01-04 00:14:42,836: t15.2023.11.17 val PER: 0.0498
2026-01-04 00:14:42,836: t15.2023.11.19 val PER: 0.0359
2026-01-04 00:14:42,836: t15.2023.11.26 val PER: 0.1638
2026-01-04 00:14:42,836: t15.2023.12.03 val PER: 0.1450
2026-01-04 00:14:42,836: t15.2023.12.08 val PER: 0.1398
2026-01-04 00:14:42,836: t15.2023.12.10 val PER: 0.1104
2026-01-04 00:14:42,837: t15.2023.12.17 val PER: 0.1746
2026-01-04 00:14:42,837: t15.2023.12.29 val PER: 0.1709
2026-01-04 00:14:42,837: t15.2024.02.25 val PER: 0.1320
2026-01-04 00:14:42,837: t15.2024.03.08 val PER: 0.2760
2026-01-04 00:14:42,837: t15.2024.03.15 val PER: 0.2208
2026-01-04 00:14:42,837: t15.2024.03.17 val PER: 0.1729
2026-01-04 00:14:42,837: t15.2024.05.10 val PER: 0.2065
2026-01-04 00:14:42,837: t15.2024.06.14 val PER: 0.1940
2026-01-04 00:14:42,837: t15.2024.07.19 val PER: 0.2729
2026-01-04 00:14:42,837: t15.2024.07.21 val PER: 0.1193
2026-01-04 00:14:42,837: t15.2024.07.28 val PER: 0.1691
2026-01-04 00:14:42,837: t15.2025.01.10 val PER: 0.3292
2026-01-04 00:14:42,837: t15.2025.01.12 val PER: 0.1832
2026-01-04 00:14:42,837: t15.2025.03.14 val PER: 0.3654
2026-01-04 00:14:42,837: t15.2025.03.16 val PER: 0.2304
2026-01-04 00:14:42,838: t15.2025.03.30 val PER: 0.3345
2026-01-04 00:14:42,838: t15.2025.04.13 val PER: 0.2468
2026-01-04 00:14:42,838: New best val WER(1gram) 52.28% --> 49.75%
2026-01-04 00:14:42,838: Checkpointing model
2026-01-04 00:14:43,437: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:14:43,673: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_8500
2026-01-04 00:14:52,273: Train batch 8600: loss: 15.10 grad norm: 63.72 time: 0.054
2026-01-04 00:15:09,566: Train batch 8800: loss: 14.25 grad norm: 59.64 time: 0.060
2026-01-04 00:15:26,978: Train batch 9000: loss: 14.90 grad norm: 69.15 time: 0.071
2026-01-04 00:15:26,978: Running test after training batch: 9000
2026-01-04 00:15:27,095: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:15:31,815: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the code at this point as will
2026-01-04 00:15:31,846: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-04 00:15:33,511: Val batch 9000: PER (avg): 0.1746 CTC Loss (avg): 17.3877 WER(1gram): 51.02% (n=64) time: 6.533
2026-01-04 00:15:33,512: WER lens: avg_true_words=6.16 avg_pred_words=6.25 max_pred_words=11
2026-01-04 00:15:33,512: t15.2023.08.13 val PER: 0.1362
2026-01-04 00:15:33,512: t15.2023.08.18 val PER: 0.1249
2026-01-04 00:15:33,512: t15.2023.08.20 val PER: 0.1334
2026-01-04 00:15:33,512: t15.2023.08.25 val PER: 0.1114
2026-01-04 00:15:33,512: t15.2023.08.27 val PER: 0.2010
2026-01-04 00:15:33,512: t15.2023.09.01 val PER: 0.0844
2026-01-04 00:15:33,512: t15.2023.09.03 val PER: 0.1817
2026-01-04 00:15:33,512: t15.2023.09.24 val PER: 0.1432
2026-01-04 00:15:33,512: t15.2023.09.29 val PER: 0.1442
2026-01-04 00:15:33,512: t15.2023.10.01 val PER: 0.1869
2026-01-04 00:15:33,513: t15.2023.10.06 val PER: 0.1066
2026-01-04 00:15:33,513: t15.2023.10.08 val PER: 0.2612
2026-01-04 00:15:33,513: t15.2023.10.13 val PER: 0.2289
2026-01-04 00:15:33,513: t15.2023.10.15 val PER: 0.1786
2026-01-04 00:15:33,513: t15.2023.10.20 val PER: 0.1946
2026-01-04 00:15:33,513: t15.2023.10.22 val PER: 0.1281
2026-01-04 00:15:33,513: t15.2023.11.03 val PER: 0.2035
2026-01-04 00:15:33,513: t15.2023.11.04 val PER: 0.0444
2026-01-04 00:15:33,513: t15.2023.11.17 val PER: 0.0591
2026-01-04 00:15:33,513: t15.2023.11.19 val PER: 0.0459
2026-01-04 00:15:33,514: t15.2023.11.26 val PER: 0.1638
2026-01-04 00:15:33,514: t15.2023.12.03 val PER: 0.1334
2026-01-04 00:15:33,514: t15.2023.12.08 val PER: 0.1318
2026-01-04 00:15:33,514: t15.2023.12.10 val PER: 0.1130
2026-01-04 00:15:33,514: t15.2023.12.17 val PER: 0.1684
2026-01-04 00:15:33,514: t15.2023.12.29 val PER: 0.1647
2026-01-04 00:15:33,514: t15.2024.02.25 val PER: 0.1419
2026-01-04 00:15:33,514: t15.2024.03.08 val PER: 0.2603
2026-01-04 00:15:33,514: t15.2024.03.15 val PER: 0.2283
2026-01-04 00:15:33,514: t15.2024.03.17 val PER: 0.1715
2026-01-04 00:15:33,514: t15.2024.05.10 val PER: 0.1813
2026-01-04 00:15:33,514: t15.2024.06.14 val PER: 0.1814
2026-01-04 00:15:33,514: t15.2024.07.19 val PER: 0.2716
2026-01-04 00:15:33,514: t15.2024.07.21 val PER: 0.1172
2026-01-04 00:15:33,514: t15.2024.07.28 val PER: 0.1529
2026-01-04 00:15:33,514: t15.2025.01.10 val PER: 0.3196
2026-01-04 00:15:33,515: t15.2025.01.12 val PER: 0.1848
2026-01-04 00:15:33,515: t15.2025.03.14 val PER: 0.3388
2026-01-04 00:15:33,515: t15.2025.03.16 val PER: 0.2238
2026-01-04 00:15:33,515: t15.2025.03.30 val PER: 0.3287
2026-01-04 00:15:33,515: t15.2025.04.13 val PER: 0.2482
2026-01-04 00:15:33,755: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_9000
2026-01-04 00:15:51,211: Train batch 9200: loss: 10.75 grad norm: 51.33 time: 0.056
2026-01-04 00:16:08,642: Train batch 9400: loss: 7.12 grad norm: 45.50 time: 0.068
2026-01-04 00:16:17,307: Running test after training batch: 9500
2026-01-04 00:16:17,454: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:16:22,136: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-04 00:16:22,167: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 00:16:23,830: Val batch 9500: PER (avg): 0.1716 CTC Loss (avg): 17.1313 WER(1gram): 50.00% (n=64) time: 6.523
2026-01-04 00:16:23,830: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 00:16:23,830: t15.2023.08.13 val PER: 0.1279
2026-01-04 00:16:23,830: t15.2023.08.18 val PER: 0.1232
2026-01-04 00:16:23,830: t15.2023.08.20 val PER: 0.1255
2026-01-04 00:16:23,830: t15.2023.08.25 val PER: 0.1114
2026-01-04 00:16:23,830: t15.2023.08.27 val PER: 0.1961
2026-01-04 00:16:23,830: t15.2023.09.01 val PER: 0.0974
2026-01-04 00:16:23,831: t15.2023.09.03 val PER: 0.1651
2026-01-04 00:16:23,831: t15.2023.09.24 val PER: 0.1456
2026-01-04 00:16:23,831: t15.2023.09.29 val PER: 0.1461
2026-01-04 00:16:23,831: t15.2023.10.01 val PER: 0.1915
2026-01-04 00:16:23,831: t15.2023.10.06 val PER: 0.1033
2026-01-04 00:16:23,831: t15.2023.10.08 val PER: 0.2530
2026-01-04 00:16:23,831: t15.2023.10.13 val PER: 0.2289
2026-01-04 00:16:23,831: t15.2023.10.15 val PER: 0.1734
2026-01-04 00:16:23,831: t15.2023.10.20 val PER: 0.1779
2026-01-04 00:16:23,831: t15.2023.10.22 val PER: 0.1325
2026-01-04 00:16:23,831: t15.2023.11.03 val PER: 0.1879
2026-01-04 00:16:23,831: t15.2023.11.04 val PER: 0.0341
2026-01-04 00:16:23,831: t15.2023.11.17 val PER: 0.0513
2026-01-04 00:16:23,831: t15.2023.11.19 val PER: 0.0519
2026-01-04 00:16:23,832: t15.2023.11.26 val PER: 0.1594
2026-01-04 00:16:23,832: t15.2023.12.03 val PER: 0.1439
2026-01-04 00:16:23,832: t15.2023.12.08 val PER: 0.1272
2026-01-04 00:16:23,832: t15.2023.12.10 val PER: 0.1209
2026-01-04 00:16:23,832: t15.2023.12.17 val PER: 0.1736
2026-01-04 00:16:23,832: t15.2023.12.29 val PER: 0.1585
2026-01-04 00:16:23,832: t15.2024.02.25 val PER: 0.1447
2026-01-04 00:16:23,832: t15.2024.03.08 val PER: 0.2475
2026-01-04 00:16:23,832: t15.2024.03.15 val PER: 0.2233
2026-01-04 00:16:23,832: t15.2024.03.17 val PER: 0.1576
2026-01-04 00:16:23,832: t15.2024.05.10 val PER: 0.1828
2026-01-04 00:16:23,833: t15.2024.06.14 val PER: 0.1861
2026-01-04 00:16:23,833: t15.2024.07.19 val PER: 0.2485
2026-01-04 00:16:23,833: t15.2024.07.21 val PER: 0.1228
2026-01-04 00:16:23,833: t15.2024.07.28 val PER: 0.1581
2026-01-04 00:16:23,833: t15.2025.01.10 val PER: 0.3168
2026-01-04 00:16:23,833: t15.2025.01.12 val PER: 0.1840
2026-01-04 00:16:23,833: t15.2025.03.14 val PER: 0.3639
2026-01-04 00:16:23,833: t15.2025.03.16 val PER: 0.2107
2026-01-04 00:16:23,833: t15.2025.03.30 val PER: 0.3184
2026-01-04 00:16:23,833: t15.2025.04.13 val PER: 0.2254
2026-01-04 00:16:24,084: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_9500
2026-01-04 00:16:32,849: Train batch 9600: loss: 8.07 grad norm: 51.00 time: 0.073
2026-01-04 00:16:50,254: Train batch 9800: loss: 12.46 grad norm: 61.10 time: 0.063
2026-01-04 00:17:07,948: Train batch 10000: loss: 5.37 grad norm: 41.05 time: 0.061
2026-01-04 00:17:07,948: Running test after training batch: 10000
2026-01-04 00:17:08,094: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:17:12,758: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the cold at this point as will
2026-01-04 00:17:12,789: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 00:17:14,487: Val batch 10000: PER (avg): 0.1683 CTC Loss (avg): 16.7661 WER(1gram): 49.75% (n=64) time: 6.539
2026-01-04 00:17:14,488: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 00:17:14,488: t15.2023.08.13 val PER: 0.1227
2026-01-04 00:17:14,488: t15.2023.08.18 val PER: 0.1207
2026-01-04 00:17:14,488: t15.2023.08.20 val PER: 0.1255
2026-01-04 00:17:14,488: t15.2023.08.25 val PER: 0.1024
2026-01-04 00:17:14,488: t15.2023.08.27 val PER: 0.1994
2026-01-04 00:17:14,488: t15.2023.09.01 val PER: 0.0804
2026-01-04 00:17:14,488: t15.2023.09.03 val PER: 0.1817
2026-01-04 00:17:14,488: t15.2023.09.24 val PER: 0.1383
2026-01-04 00:17:14,488: t15.2023.09.29 val PER: 0.1455
2026-01-04 00:17:14,488: t15.2023.10.01 val PER: 0.1816
2026-01-04 00:17:14,492: t15.2023.10.06 val PER: 0.0936
2026-01-04 00:17:14,492: t15.2023.10.08 val PER: 0.2490
2026-01-04 00:17:14,492: t15.2023.10.13 val PER: 0.2320
2026-01-04 00:17:14,492: t15.2023.10.15 val PER: 0.1734
2026-01-04 00:17:14,492: t15.2023.10.20 val PER: 0.1913
2026-01-04 00:17:14,492: t15.2023.10.22 val PER: 0.1292
2026-01-04 00:17:14,493: t15.2023.11.03 val PER: 0.1866
2026-01-04 00:17:14,493: t15.2023.11.04 val PER: 0.0307
2026-01-04 00:17:14,493: t15.2023.11.17 val PER: 0.0513
2026-01-04 00:17:14,493: t15.2023.11.19 val PER: 0.0379
2026-01-04 00:17:14,493: t15.2023.11.26 val PER: 0.1486
2026-01-04 00:17:14,493: t15.2023.12.03 val PER: 0.1387
2026-01-04 00:17:14,493: t15.2023.12.08 val PER: 0.1245
2026-01-04 00:17:14,493: t15.2023.12.10 val PER: 0.1130
2026-01-04 00:17:14,493: t15.2023.12.17 val PER: 0.1601
2026-01-04 00:17:14,493: t15.2023.12.29 val PER: 0.1510
2026-01-04 00:17:14,493: t15.2024.02.25 val PER: 0.1334
2026-01-04 00:17:14,494: t15.2024.03.08 val PER: 0.2617
2026-01-04 00:17:14,494: t15.2024.03.15 val PER: 0.2276
2026-01-04 00:17:14,494: t15.2024.03.17 val PER: 0.1569
2026-01-04 00:17:14,494: t15.2024.05.10 val PER: 0.1768
2026-01-04 00:17:14,494: t15.2024.06.14 val PER: 0.1703
2026-01-04 00:17:14,494: t15.2024.07.19 val PER: 0.2584
2026-01-04 00:17:14,494: t15.2024.07.21 val PER: 0.1097
2026-01-04 00:17:14,494: t15.2024.07.28 val PER: 0.1544
2026-01-04 00:17:14,494: t15.2025.01.10 val PER: 0.3209
2026-01-04 00:17:14,494: t15.2025.01.12 val PER: 0.1740
2026-01-04 00:17:14,494: t15.2025.03.14 val PER: 0.3536
2026-01-04 00:17:14,494: t15.2025.03.16 val PER: 0.2134
2026-01-04 00:17:14,494: t15.2025.03.30 val PER: 0.3184
2026-01-04 00:17:14,494: t15.2025.04.13 val PER: 0.2340
2026-01-04 00:17:14,743: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_10000
2026-01-04 00:17:32,260: Train batch 10200: loss: 6.03 grad norm: 41.04 time: 0.050
2026-01-04 00:17:50,234: Train batch 10400: loss: 8.84 grad norm: 50.97 time: 0.072
2026-01-04 00:17:59,131: Running test after training batch: 10500
2026-01-04 00:17:59,228: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:18:04,050: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:18:04,080: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-04 00:18:05,772: Val batch 10500: PER (avg): 0.1650 CTC Loss (avg): 16.5740 WER(1gram): 49.75% (n=64) time: 6.641
2026-01-04 00:18:05,773: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=11
2026-01-04 00:18:05,773: t15.2023.08.13 val PER: 0.1175
2026-01-04 00:18:05,773: t15.2023.08.18 val PER: 0.1215
2026-01-04 00:18:05,773: t15.2023.08.20 val PER: 0.1303
2026-01-04 00:18:05,773: t15.2023.08.25 val PER: 0.0994
2026-01-04 00:18:05,773: t15.2023.08.27 val PER: 0.1929
2026-01-04 00:18:05,773: t15.2023.09.01 val PER: 0.0869
2026-01-04 00:18:05,773: t15.2023.09.03 val PER: 0.1805
2026-01-04 00:18:05,773: t15.2023.09.24 val PER: 0.1383
2026-01-04 00:18:05,773: t15.2023.09.29 val PER: 0.1417
2026-01-04 00:18:05,774: t15.2023.10.01 val PER: 0.1856
2026-01-04 00:18:05,774: t15.2023.10.06 val PER: 0.0969
2026-01-04 00:18:05,774: t15.2023.10.08 val PER: 0.2422
2026-01-04 00:18:05,774: t15.2023.10.13 val PER: 0.2172
2026-01-04 00:18:05,774: t15.2023.10.15 val PER: 0.1734
2026-01-04 00:18:05,774: t15.2023.10.20 val PER: 0.1812
2026-01-04 00:18:05,774: t15.2023.10.22 val PER: 0.1203
2026-01-04 00:18:05,774: t15.2023.11.03 val PER: 0.1913
2026-01-04 00:18:05,774: t15.2023.11.04 val PER: 0.0410
2026-01-04 00:18:05,774: t15.2023.11.17 val PER: 0.0498
2026-01-04 00:18:05,774: t15.2023.11.19 val PER: 0.0579
2026-01-04 00:18:05,774: t15.2023.11.26 val PER: 0.1399
2026-01-04 00:18:05,774: t15.2023.12.03 val PER: 0.1324
2026-01-04 00:18:05,775: t15.2023.12.08 val PER: 0.1152
2026-01-04 00:18:05,775: t15.2023.12.10 val PER: 0.1078
2026-01-04 00:18:05,775: t15.2023.12.17 val PER: 0.1414
2026-01-04 00:18:05,775: t15.2023.12.29 val PER: 0.1579
2026-01-04 00:18:05,775: t15.2024.02.25 val PER: 0.1376
2026-01-04 00:18:05,775: t15.2024.03.08 val PER: 0.2447
2026-01-04 00:18:05,775: t15.2024.03.15 val PER: 0.2158
2026-01-04 00:18:05,775: t15.2024.03.17 val PER: 0.1541
2026-01-04 00:18:05,775: t15.2024.05.10 val PER: 0.1768
2026-01-04 00:18:05,775: t15.2024.06.14 val PER: 0.1924
2026-01-04 00:18:05,775: t15.2024.07.19 val PER: 0.2518
2026-01-04 00:18:05,775: t15.2024.07.21 val PER: 0.1007
2026-01-04 00:18:05,776: t15.2024.07.28 val PER: 0.1434
2026-01-04 00:18:05,776: t15.2025.01.10 val PER: 0.3154
2026-01-04 00:18:05,776: t15.2025.01.12 val PER: 0.1701
2026-01-04 00:18:05,776: t15.2025.03.14 val PER: 0.3491
2026-01-04 00:18:05,776: t15.2025.03.16 val PER: 0.1963
2026-01-04 00:18:05,776: t15.2025.03.30 val PER: 0.3207
2026-01-04 00:18:05,776: t15.2025.04.13 val PER: 0.2282
2026-01-04 00:18:06,030: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_10500
2026-01-04 00:18:15,206: Train batch 10600: loss: 8.66 grad norm: 56.37 time: 0.072
2026-01-04 00:18:32,637: Train batch 10800: loss: 14.08 grad norm: 69.43 time: 0.063
2026-01-04 00:18:50,117: Train batch 11000: loss: 13.48 grad norm: 62.58 time: 0.056
2026-01-04 00:18:50,118: Running test after training batch: 11000
2026-01-04 00:18:50,263: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:18:55,216: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:18:55,247: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 00:18:56,971: Val batch 11000: PER (avg): 0.1615 CTC Loss (avg): 16.4718 WER(1gram): 46.95% (n=64) time: 6.853
2026-01-04 00:18:56,971: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 00:18:56,972: t15.2023.08.13 val PER: 0.1206
2026-01-04 00:18:56,972: t15.2023.08.18 val PER: 0.1182
2026-01-04 00:18:56,972: t15.2023.08.20 val PER: 0.1160
2026-01-04 00:18:56,972: t15.2023.08.25 val PER: 0.0919
2026-01-04 00:18:56,972: t15.2023.08.27 val PER: 0.1945
2026-01-04 00:18:56,972: t15.2023.09.01 val PER: 0.0852
2026-01-04 00:18:56,972: t15.2023.09.03 val PER: 0.1770
2026-01-04 00:18:56,972: t15.2023.09.24 val PER: 0.1383
2026-01-04 00:18:56,972: t15.2023.09.29 val PER: 0.1391
2026-01-04 00:18:56,972: t15.2023.10.01 val PER: 0.1935
2026-01-04 00:18:56,972: t15.2023.10.06 val PER: 0.0872
2026-01-04 00:18:56,972: t15.2023.10.08 val PER: 0.2422
2026-01-04 00:18:56,973: t15.2023.10.13 val PER: 0.2149
2026-01-04 00:18:56,973: t15.2023.10.15 val PER: 0.1753
2026-01-04 00:18:56,973: t15.2023.10.20 val PER: 0.1879
2026-01-04 00:18:56,973: t15.2023.10.22 val PER: 0.1114
2026-01-04 00:18:56,973: t15.2023.11.03 val PER: 0.1784
2026-01-04 00:18:56,973: t15.2023.11.04 val PER: 0.0444
2026-01-04 00:18:56,973: t15.2023.11.17 val PER: 0.0591
2026-01-04 00:18:56,973: t15.2023.11.19 val PER: 0.0419
2026-01-04 00:18:56,973: t15.2023.11.26 val PER: 0.1420
2026-01-04 00:18:56,973: t15.2023.12.03 val PER: 0.1229
2026-01-04 00:18:56,973: t15.2023.12.08 val PER: 0.1099
2026-01-04 00:18:56,973: t15.2023.12.10 val PER: 0.1078
2026-01-04 00:18:56,973: t15.2023.12.17 val PER: 0.1497
2026-01-04 00:18:56,973: t15.2023.12.29 val PER: 0.1455
2026-01-04 00:18:56,974: t15.2024.02.25 val PER: 0.1320
2026-01-04 00:18:56,974: t15.2024.03.08 val PER: 0.2418
2026-01-04 00:18:56,974: t15.2024.03.15 val PER: 0.2126
2026-01-04 00:18:56,974: t15.2024.03.17 val PER: 0.1492
2026-01-04 00:18:56,974: t15.2024.05.10 val PER: 0.1828
2026-01-04 00:18:56,974: t15.2024.06.14 val PER: 0.1688
2026-01-04 00:18:56,974: t15.2024.07.19 val PER: 0.2452
2026-01-04 00:18:56,974: t15.2024.07.21 val PER: 0.1014
2026-01-04 00:18:56,975: t15.2024.07.28 val PER: 0.1434
2026-01-04 00:18:56,975: t15.2025.01.10 val PER: 0.2934
2026-01-04 00:18:56,975: t15.2025.01.12 val PER: 0.1701
2026-01-04 00:18:56,975: t15.2025.03.14 val PER: 0.3432
2026-01-04 00:18:56,975: t15.2025.03.16 val PER: 0.1950
2026-01-04 00:18:56,975: t15.2025.03.30 val PER: 0.3057
2026-01-04 00:18:56,975: t15.2025.04.13 val PER: 0.2368
2026-01-04 00:18:56,976: New best val WER(1gram) 49.75% --> 46.95%
2026-01-04 00:18:56,977: Checkpointing model
2026-01-04 00:18:57,563: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:18:57,823: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_11000
2026-01-04 00:19:15,169: Train batch 11200: loss: 10.49 grad norm: 57.68 time: 0.070
2026-01-04 00:19:32,457: Train batch 11400: loss: 8.64 grad norm: 52.67 time: 0.056
2026-01-04 00:19:41,294: Running test after training batch: 11500
2026-01-04 00:19:41,452: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:19:46,163: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:19:46,195: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost jett
2026-01-04 00:19:47,943: Val batch 11500: PER (avg): 0.1624 CTC Loss (avg): 16.4714 WER(1gram): 48.98% (n=64) time: 6.649
2026-01-04 00:19:47,943: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 00:19:47,944: t15.2023.08.13 val PER: 0.1237
2026-01-04 00:19:47,944: t15.2023.08.18 val PER: 0.1232
2026-01-04 00:19:47,944: t15.2023.08.20 val PER: 0.1152
2026-01-04 00:19:47,944: t15.2023.08.25 val PER: 0.0934
2026-01-04 00:19:47,944: t15.2023.08.27 val PER: 0.1897
2026-01-04 00:19:47,944: t15.2023.09.01 val PER: 0.0885
2026-01-04 00:19:47,944: t15.2023.09.03 val PER: 0.1746
2026-01-04 00:19:47,944: t15.2023.09.24 val PER: 0.1262
2026-01-04 00:19:47,944: t15.2023.09.29 val PER: 0.1442
2026-01-04 00:19:47,944: t15.2023.10.01 val PER: 0.1790
2026-01-04 00:19:47,944: t15.2023.10.06 val PER: 0.0926
2026-01-04 00:19:47,945: t15.2023.10.08 val PER: 0.2530
2026-01-04 00:19:47,945: t15.2023.10.13 val PER: 0.2040
2026-01-04 00:19:47,945: t15.2023.10.15 val PER: 0.1661
2026-01-04 00:19:47,945: t15.2023.10.20 val PER: 0.1980
2026-01-04 00:19:47,945: t15.2023.10.22 val PER: 0.1147
2026-01-04 00:19:47,945: t15.2023.11.03 val PER: 0.1811
2026-01-04 00:19:47,945: t15.2023.11.04 val PER: 0.0375
2026-01-04 00:19:47,945: t15.2023.11.17 val PER: 0.0498
2026-01-04 00:19:47,945: t15.2023.11.19 val PER: 0.0499
2026-01-04 00:19:47,945: t15.2023.11.26 val PER: 0.1312
2026-01-04 00:19:47,945: t15.2023.12.03 val PER: 0.1218
2026-01-04 00:19:47,945: t15.2023.12.08 val PER: 0.1092
2026-01-04 00:19:47,945: t15.2023.12.10 val PER: 0.1038
2026-01-04 00:19:47,945: t15.2023.12.17 val PER: 0.1466
2026-01-04 00:19:47,945: t15.2023.12.29 val PER: 0.1489
2026-01-04 00:19:47,945: t15.2024.02.25 val PER: 0.1278
2026-01-04 00:19:47,946: t15.2024.03.08 val PER: 0.2404
2026-01-04 00:19:47,946: t15.2024.03.15 val PER: 0.2189
2026-01-04 00:19:47,946: t15.2024.03.17 val PER: 0.1597
2026-01-04 00:19:47,946: t15.2024.05.10 val PER: 0.1783
2026-01-04 00:19:47,946: t15.2024.06.14 val PER: 0.1893
2026-01-04 00:19:47,946: t15.2024.07.19 val PER: 0.2591
2026-01-04 00:19:47,946: t15.2024.07.21 val PER: 0.1041
2026-01-04 00:19:47,946: t15.2024.07.28 val PER: 0.1382
2026-01-04 00:19:47,946: t15.2025.01.10 val PER: 0.3154
2026-01-04 00:19:47,946: t15.2025.01.12 val PER: 0.1717
2026-01-04 00:19:47,946: t15.2025.03.14 val PER: 0.3550
2026-01-04 00:19:47,946: t15.2025.03.16 val PER: 0.2147
2026-01-04 00:19:47,946: t15.2025.03.30 val PER: 0.3069
2026-01-04 00:19:47,947: t15.2025.04.13 val PER: 0.2240
2026-01-04 00:19:48,199: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_11500
2026-01-04 00:19:56,646: Train batch 11600: loss: 10.48 grad norm: 49.95 time: 0.061
2026-01-04 00:20:13,886: Train batch 11800: loss: 6.47 grad norm: 46.17 time: 0.045
2026-01-04 00:20:31,392: Train batch 12000: loss: 13.28 grad norm: 55.09 time: 0.071
2026-01-04 00:20:31,392: Running test after training batch: 12000
2026-01-04 00:20:31,488: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:20:36,335: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 00:20:36,368: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-04 00:20:38,113: Val batch 12000: PER (avg): 0.1590 CTC Loss (avg): 16.1513 WER(1gram): 49.49% (n=64) time: 6.721
2026-01-04 00:20:38,114: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 00:20:38,114: t15.2023.08.13 val PER: 0.1237
2026-01-04 00:20:38,114: t15.2023.08.18 val PER: 0.1157
2026-01-04 00:20:38,114: t15.2023.08.20 val PER: 0.1231
2026-01-04 00:20:38,114: t15.2023.08.25 val PER: 0.0889
2026-01-04 00:20:38,114: t15.2023.08.27 val PER: 0.1865
2026-01-04 00:20:38,114: t15.2023.09.01 val PER: 0.0893
2026-01-04 00:20:38,115: t15.2023.09.03 val PER: 0.1639
2026-01-04 00:20:38,115: t15.2023.09.24 val PER: 0.1323
2026-01-04 00:20:38,115: t15.2023.09.29 val PER: 0.1404
2026-01-04 00:20:38,115: t15.2023.10.01 val PER: 0.1843
2026-01-04 00:20:38,115: t15.2023.10.06 val PER: 0.0904
2026-01-04 00:20:38,115: t15.2023.10.08 val PER: 0.2409
2026-01-04 00:20:38,115: t15.2023.10.13 val PER: 0.2009
2026-01-04 00:20:38,115: t15.2023.10.15 val PER: 0.1701
2026-01-04 00:20:38,115: t15.2023.10.20 val PER: 0.1879
2026-01-04 00:20:38,115: t15.2023.10.22 val PER: 0.1125
2026-01-04 00:20:38,115: t15.2023.11.03 val PER: 0.1818
2026-01-04 00:20:38,116: t15.2023.11.04 val PER: 0.0410
2026-01-04 00:20:38,116: t15.2023.11.17 val PER: 0.0513
2026-01-04 00:20:38,116: t15.2023.11.19 val PER: 0.0319
2026-01-04 00:20:38,116: t15.2023.11.26 val PER: 0.1268
2026-01-04 00:20:38,116: t15.2023.12.03 val PER: 0.1218
2026-01-04 00:20:38,116: t15.2023.12.08 val PER: 0.1092
2026-01-04 00:20:38,116: t15.2023.12.10 val PER: 0.1038
2026-01-04 00:20:38,116: t15.2023.12.17 val PER: 0.1445
2026-01-04 00:20:38,116: t15.2023.12.29 val PER: 0.1421
2026-01-04 00:20:38,116: t15.2024.02.25 val PER: 0.1194
2026-01-04 00:20:38,116: t15.2024.03.08 val PER: 0.2376
2026-01-04 00:20:38,116: t15.2024.03.15 val PER: 0.2058
2026-01-04 00:20:38,117: t15.2024.03.17 val PER: 0.1527
2026-01-04 00:20:38,117: t15.2024.05.10 val PER: 0.1634
2026-01-04 00:20:38,117: t15.2024.06.14 val PER: 0.1893
2026-01-04 00:20:38,117: t15.2024.07.19 val PER: 0.2452
2026-01-04 00:20:38,117: t15.2024.07.21 val PER: 0.1055
2026-01-04 00:20:38,121: t15.2024.07.28 val PER: 0.1471
2026-01-04 00:20:38,121: t15.2025.01.10 val PER: 0.2961
2026-01-04 00:20:38,121: t15.2025.01.12 val PER: 0.1617
2026-01-04 00:20:38,121: t15.2025.03.14 val PER: 0.3328
2026-01-04 00:20:38,121: t15.2025.03.16 val PER: 0.2134
2026-01-04 00:20:38,121: t15.2025.03.30 val PER: 0.3080
2026-01-04 00:20:38,121: t15.2025.04.13 val PER: 0.2225
2026-01-04 00:20:38,371: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_12000
2026-01-04 00:20:55,614: Train batch 12200: loss: 4.91 grad norm: 37.57 time: 0.065
2026-01-04 00:21:12,846: Train batch 12400: loss: 4.82 grad norm: 39.33 time: 0.041
2026-01-04 00:21:21,734: Running test after training batch: 12500
2026-01-04 00:21:21,877: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:21:26,625: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 00:21:26,657: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 00:21:28,424: Val batch 12500: PER (avg): 0.1561 CTC Loss (avg): 15.9033 WER(1gram): 46.19% (n=64) time: 6.690
2026-01-04 00:21:28,424: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 00:21:28,425: t15.2023.08.13 val PER: 0.1237
2026-01-04 00:21:28,425: t15.2023.08.18 val PER: 0.1215
2026-01-04 00:21:28,425: t15.2023.08.20 val PER: 0.1096
2026-01-04 00:21:28,425: t15.2023.08.25 val PER: 0.0798
2026-01-04 00:21:28,425: t15.2023.08.27 val PER: 0.1961
2026-01-04 00:21:28,425: t15.2023.09.01 val PER: 0.0755
2026-01-04 00:21:28,425: t15.2023.09.03 val PER: 0.1544
2026-01-04 00:21:28,425: t15.2023.09.24 val PER: 0.1226
2026-01-04 00:21:28,425: t15.2023.09.29 val PER: 0.1359
2026-01-04 00:21:28,425: t15.2023.10.01 val PER: 0.1717
2026-01-04 00:21:28,425: t15.2023.10.06 val PER: 0.0840
2026-01-04 00:21:28,425: t15.2023.10.08 val PER: 0.2530
2026-01-04 00:21:28,425: t15.2023.10.13 val PER: 0.2040
2026-01-04 00:21:28,425: t15.2023.10.15 val PER: 0.1602
2026-01-04 00:21:28,426: t15.2023.10.20 val PER: 0.1711
2026-01-04 00:21:28,426: t15.2023.10.22 val PER: 0.1147
2026-01-04 00:21:28,426: t15.2023.11.03 val PER: 0.1798
2026-01-04 00:21:28,426: t15.2023.11.04 val PER: 0.0375
2026-01-04 00:21:28,426: t15.2023.11.17 val PER: 0.0451
2026-01-04 00:21:28,426: t15.2023.11.19 val PER: 0.0319
2026-01-04 00:21:28,426: t15.2023.11.26 val PER: 0.1333
2026-01-04 00:21:28,426: t15.2023.12.03 val PER: 0.1124
2026-01-04 00:21:28,426: t15.2023.12.08 val PER: 0.1012
2026-01-04 00:21:28,426: t15.2023.12.10 val PER: 0.0959
2026-01-04 00:21:28,426: t15.2023.12.17 val PER: 0.1538
2026-01-04 00:21:28,427: t15.2023.12.29 val PER: 0.1448
2026-01-04 00:21:28,427: t15.2024.02.25 val PER: 0.1180
2026-01-04 00:21:28,427: t15.2024.03.08 val PER: 0.2347
2026-01-04 00:21:28,427: t15.2024.03.15 val PER: 0.2083
2026-01-04 00:21:28,427: t15.2024.03.17 val PER: 0.1457
2026-01-04 00:21:28,427: t15.2024.05.10 val PER: 0.1694
2026-01-04 00:21:28,427: t15.2024.06.14 val PER: 0.1672
2026-01-04 00:21:28,427: t15.2024.07.19 val PER: 0.2459
2026-01-04 00:21:28,427: t15.2024.07.21 val PER: 0.1055
2026-01-04 00:21:28,427: t15.2024.07.28 val PER: 0.1441
2026-01-04 00:21:28,427: t15.2025.01.10 val PER: 0.2851
2026-01-04 00:21:28,427: t15.2025.01.12 val PER: 0.1624
2026-01-04 00:21:28,427: t15.2025.03.14 val PER: 0.3447
2026-01-04 00:21:28,427: t15.2025.03.16 val PER: 0.1937
2026-01-04 00:21:28,427: t15.2025.03.30 val PER: 0.3080
2026-01-04 00:21:28,427: t15.2025.04.13 val PER: 0.2382
2026-01-04 00:21:28,428: New best val WER(1gram) 46.95% --> 46.19%
2026-01-04 00:21:28,428: Checkpointing model
2026-01-04 00:21:29,011: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:21:29,271: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_12500
2026-01-04 00:21:37,758: Train batch 12600: loss: 7.59 grad norm: 46.17 time: 0.058
2026-01-04 00:21:55,070: Train batch 12800: loss: 5.67 grad norm: 44.15 time: 0.052
2026-01-04 00:22:12,637: Train batch 13000: loss: 5.99 grad norm: 43.67 time: 0.066
2026-01-04 00:22:12,638: Running test after training batch: 13000
2026-01-04 00:22:12,754: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:22:17,438: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-04 00:22:17,470: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 00:22:19,219: Val batch 13000: PER (avg): 0.1544 CTC Loss (avg): 15.7709 WER(1gram): 46.70% (n=64) time: 6.582
2026-01-04 00:22:19,220: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-04 00:22:19,220: t15.2023.08.13 val PER: 0.1206
2026-01-04 00:22:19,220: t15.2023.08.18 val PER: 0.1165
2026-01-04 00:22:19,220: t15.2023.08.20 val PER: 0.1072
2026-01-04 00:22:19,221: t15.2023.08.25 val PER: 0.0813
2026-01-04 00:22:19,221: t15.2023.08.27 val PER: 0.1881
2026-01-04 00:22:19,221: t15.2023.09.01 val PER: 0.0706
2026-01-04 00:22:19,221: t15.2023.09.03 val PER: 0.1651
2026-01-04 00:22:19,221: t15.2023.09.24 val PER: 0.1177
2026-01-04 00:22:19,221: t15.2023.09.29 val PER: 0.1321
2026-01-04 00:22:19,221: t15.2023.10.01 val PER: 0.1757
2026-01-04 00:22:19,221: t15.2023.10.06 val PER: 0.0926
2026-01-04 00:22:19,221: t15.2023.10.08 val PER: 0.2463
2026-01-04 00:22:19,221: t15.2023.10.13 val PER: 0.1994
2026-01-04 00:22:19,221: t15.2023.10.15 val PER: 0.1589
2026-01-04 00:22:19,221: t15.2023.10.20 val PER: 0.1644
2026-01-04 00:22:19,221: t15.2023.10.22 val PER: 0.1192
2026-01-04 00:22:19,221: t15.2023.11.03 val PER: 0.1777
2026-01-04 00:22:19,222: t15.2023.11.04 val PER: 0.0444
2026-01-04 00:22:19,222: t15.2023.11.17 val PER: 0.0404
2026-01-04 00:22:19,222: t15.2023.11.19 val PER: 0.0419
2026-01-04 00:22:19,222: t15.2023.11.26 val PER: 0.1283
2026-01-04 00:22:19,222: t15.2023.12.03 val PER: 0.1282
2026-01-04 00:22:19,222: t15.2023.12.08 val PER: 0.1005
2026-01-04 00:22:19,222: t15.2023.12.10 val PER: 0.0933
2026-01-04 00:22:19,222: t15.2023.12.17 val PER: 0.1466
2026-01-04 00:22:19,222: t15.2023.12.29 val PER: 0.1510
2026-01-04 00:22:19,222: t15.2024.02.25 val PER: 0.1124
2026-01-04 00:22:19,222: t15.2024.03.08 val PER: 0.2361
2026-01-04 00:22:19,222: t15.2024.03.15 val PER: 0.2070
2026-01-04 00:22:19,222: t15.2024.03.17 val PER: 0.1388
2026-01-04 00:22:19,222: t15.2024.05.10 val PER: 0.1664
2026-01-04 00:22:19,222: t15.2024.06.14 val PER: 0.1719
2026-01-04 00:22:19,222: t15.2024.07.19 val PER: 0.2479
2026-01-04 00:22:19,222: t15.2024.07.21 val PER: 0.1014
2026-01-04 00:22:19,223: t15.2024.07.28 val PER: 0.1426
2026-01-04 00:22:19,223: t15.2025.01.10 val PER: 0.2851
2026-01-04 00:22:19,223: t15.2025.01.12 val PER: 0.1586
2026-01-04 00:22:19,223: t15.2025.03.14 val PER: 0.3314
2026-01-04 00:22:19,223: t15.2025.03.16 val PER: 0.1754
2026-01-04 00:22:19,223: t15.2025.03.30 val PER: 0.3080
2026-01-04 00:22:19,223: t15.2025.04.13 val PER: 0.2197
2026-01-04 00:22:19,479: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_13000
2026-01-04 00:22:36,485: Train batch 13200: loss: 11.70 grad norm: 62.67 time: 0.054
2026-01-04 00:22:53,464: Train batch 13400: loss: 8.27 grad norm: 53.71 time: 0.061
2026-01-04 00:23:02,092: Running test after training batch: 13500
2026-01-04 00:23:02,215: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:23:07,076: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 00:23:07,108: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 00:23:08,885: Val batch 13500: PER (avg): 0.1528 CTC Loss (avg): 15.5246 WER(1gram): 48.22% (n=64) time: 6.793
2026-01-04 00:23:08,885: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-04 00:23:08,886: t15.2023.08.13 val PER: 0.1195
2026-01-04 00:23:08,886: t15.2023.08.18 val PER: 0.1048
2026-01-04 00:23:08,886: t15.2023.08.20 val PER: 0.1048
2026-01-04 00:23:08,886: t15.2023.08.25 val PER: 0.0858
2026-01-04 00:23:08,886: t15.2023.08.27 val PER: 0.1752
2026-01-04 00:23:08,886: t15.2023.09.01 val PER: 0.0828
2026-01-04 00:23:08,886: t15.2023.09.03 val PER: 0.1675
2026-01-04 00:23:08,886: t15.2023.09.24 val PER: 0.1214
2026-01-04 00:23:08,886: t15.2023.09.29 val PER: 0.1327
2026-01-04 00:23:08,887: t15.2023.10.01 val PER: 0.1737
2026-01-04 00:23:08,887: t15.2023.10.06 val PER: 0.0936
2026-01-04 00:23:08,887: t15.2023.10.08 val PER: 0.2395
2026-01-04 00:23:08,887: t15.2023.10.13 val PER: 0.2033
2026-01-04 00:23:08,887: t15.2023.10.15 val PER: 0.1595
2026-01-04 00:23:08,887: t15.2023.10.20 val PER: 0.1812
2026-01-04 00:23:08,887: t15.2023.10.22 val PER: 0.1147
2026-01-04 00:23:08,887: t15.2023.11.03 val PER: 0.1737
2026-01-04 00:23:08,887: t15.2023.11.04 val PER: 0.0410
2026-01-04 00:23:08,887: t15.2023.11.17 val PER: 0.0513
2026-01-04 00:23:08,887: t15.2023.11.19 val PER: 0.0299
2026-01-04 00:23:08,887: t15.2023.11.26 val PER: 0.1217
2026-01-04 00:23:08,887: t15.2023.12.03 val PER: 0.1103
2026-01-04 00:23:08,887: t15.2023.12.08 val PER: 0.1072
2026-01-04 00:23:08,887: t15.2023.12.10 val PER: 0.0959
2026-01-04 00:23:08,887: t15.2023.12.17 val PER: 0.1351
2026-01-04 00:23:08,887: t15.2023.12.29 val PER: 0.1380
2026-01-04 00:23:08,888: t15.2024.02.25 val PER: 0.1138
2026-01-04 00:23:08,888: t15.2024.03.08 val PER: 0.2361
2026-01-04 00:23:08,888: t15.2024.03.15 val PER: 0.2020
2026-01-04 00:23:08,888: t15.2024.03.17 val PER: 0.1450
2026-01-04 00:23:08,888: t15.2024.05.10 val PER: 0.1575
2026-01-04 00:23:08,888: t15.2024.06.14 val PER: 0.1577
2026-01-04 00:23:08,888: t15.2024.07.19 val PER: 0.2353
2026-01-04 00:23:08,888: t15.2024.07.21 val PER: 0.1007
2026-01-04 00:23:08,888: t15.2024.07.28 val PER: 0.1456
2026-01-04 00:23:08,888: t15.2025.01.10 val PER: 0.2920
2026-01-04 00:23:08,888: t15.2025.01.12 val PER: 0.1624
2026-01-04 00:23:08,888: t15.2025.03.14 val PER: 0.3314
2026-01-04 00:23:08,888: t15.2025.03.16 val PER: 0.1780
2026-01-04 00:23:08,888: t15.2025.03.30 val PER: 0.3138
2026-01-04 00:23:08,888: t15.2025.04.13 val PER: 0.2154
2026-01-04 00:23:09,139: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_13500
2026-01-04 00:23:17,791: Train batch 13600: loss: 12.42 grad norm: 73.09 time: 0.062
2026-01-04 00:23:35,150: Train batch 13800: loss: 8.11 grad norm: 55.76 time: 0.056
2026-01-04 00:23:52,466: Train batch 14000: loss: 11.14 grad norm: 64.04 time: 0.050
2026-01-04 00:23:52,466: Running test after training batch: 14000
2026-01-04 00:23:52,556: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:23:57,258: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:23:57,290: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost et
2026-01-04 00:23:59,105: Val batch 14000: PER (avg): 0.1517 CTC Loss (avg): 15.5238 WER(1gram): 48.22% (n=64) time: 6.639
2026-01-04 00:23:59,106: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-04 00:23:59,106: t15.2023.08.13 val PER: 0.1237
2026-01-04 00:23:59,106: t15.2023.08.18 val PER: 0.1090
2026-01-04 00:23:59,106: t15.2023.08.20 val PER: 0.1088
2026-01-04 00:23:59,106: t15.2023.08.25 val PER: 0.0919
2026-01-04 00:23:59,106: t15.2023.08.27 val PER: 0.1801
2026-01-04 00:23:59,107: t15.2023.09.01 val PER: 0.0755
2026-01-04 00:23:59,107: t15.2023.09.03 val PER: 0.1686
2026-01-04 00:23:59,107: t15.2023.09.24 val PER: 0.1274
2026-01-04 00:23:59,107: t15.2023.09.29 val PER: 0.1315
2026-01-04 00:23:59,107: t15.2023.10.01 val PER: 0.1724
2026-01-04 00:23:59,107: t15.2023.10.06 val PER: 0.0915
2026-01-04 00:23:59,107: t15.2023.10.08 val PER: 0.2355
2026-01-04 00:23:59,107: t15.2023.10.13 val PER: 0.2033
2026-01-04 00:23:59,107: t15.2023.10.15 val PER: 0.1543
2026-01-04 00:23:59,108: t15.2023.10.20 val PER: 0.1812
2026-01-04 00:23:59,108: t15.2023.10.22 val PER: 0.1214
2026-01-04 00:23:59,108: t15.2023.11.03 val PER: 0.1744
2026-01-04 00:23:59,108: t15.2023.11.04 val PER: 0.0307
2026-01-04 00:23:59,108: t15.2023.11.17 val PER: 0.0498
2026-01-04 00:23:59,108: t15.2023.11.19 val PER: 0.0279
2026-01-04 00:23:59,108: t15.2023.11.26 val PER: 0.1261
2026-01-04 00:23:59,108: t15.2023.12.03 val PER: 0.1166
2026-01-04 00:23:59,108: t15.2023.12.08 val PER: 0.0965
2026-01-04 00:23:59,109: t15.2023.12.10 val PER: 0.1064
2026-01-04 00:23:59,109: t15.2023.12.17 val PER: 0.1403
2026-01-04 00:23:59,109: t15.2023.12.29 val PER: 0.1263
2026-01-04 00:23:59,109: t15.2024.02.25 val PER: 0.1096
2026-01-04 00:23:59,109: t15.2024.03.08 val PER: 0.2347
2026-01-04 00:23:59,109: t15.2024.03.15 val PER: 0.1976
2026-01-04 00:23:59,109: t15.2024.03.17 val PER: 0.1304
2026-01-04 00:23:59,109: t15.2024.05.10 val PER: 0.1634
2026-01-04 00:23:59,109: t15.2024.06.14 val PER: 0.1688
2026-01-04 00:23:59,109: t15.2024.07.19 val PER: 0.2386
2026-01-04 00:23:59,109: t15.2024.07.21 val PER: 0.0952
2026-01-04 00:23:59,110: t15.2024.07.28 val PER: 0.1412
2026-01-04 00:23:59,110: t15.2025.01.10 val PER: 0.2934
2026-01-04 00:23:59,110: t15.2025.01.12 val PER: 0.1586
2026-01-04 00:23:59,110: t15.2025.03.14 val PER: 0.3328
2026-01-04 00:23:59,110: t15.2025.03.16 val PER: 0.1885
2026-01-04 00:23:59,110: t15.2025.03.30 val PER: 0.2966
2026-01-04 00:23:59,110: t15.2025.04.13 val PER: 0.2168
2026-01-04 00:23:59,364: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_14000
2026-01-04 00:24:16,817: Train batch 14200: loss: 7.44 grad norm: 54.74 time: 0.056
2026-01-04 00:24:34,411: Train batch 14400: loss: 5.46 grad norm: 41.30 time: 0.064
2026-01-04 00:24:43,180: Running test after training batch: 14500
2026-01-04 00:24:43,290: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:24:48,022: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:24:48,054: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-04 00:24:49,857: Val batch 14500: PER (avg): 0.1516 CTC Loss (avg): 15.5083 WER(1gram): 46.70% (n=64) time: 6.677
2026-01-04 00:24:49,857: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-04 00:24:49,858: t15.2023.08.13 val PER: 0.1175
2026-01-04 00:24:49,858: t15.2023.08.18 val PER: 0.1081
2026-01-04 00:24:49,858: t15.2023.08.20 val PER: 0.1112
2026-01-04 00:24:49,858: t15.2023.08.25 val PER: 0.0889
2026-01-04 00:24:49,858: t15.2023.08.27 val PER: 0.1736
2026-01-04 00:24:49,858: t15.2023.09.01 val PER: 0.0804
2026-01-04 00:24:49,858: t15.2023.09.03 val PER: 0.1651
2026-01-04 00:24:49,858: t15.2023.09.24 val PER: 0.1238
2026-01-04 00:24:49,858: t15.2023.09.29 val PER: 0.1276
2026-01-04 00:24:49,858: t15.2023.10.01 val PER: 0.1757
2026-01-04 00:24:49,858: t15.2023.10.06 val PER: 0.0926
2026-01-04 00:24:49,858: t15.2023.10.08 val PER: 0.2476
2026-01-04 00:24:49,858: t15.2023.10.13 val PER: 0.2017
2026-01-04 00:24:49,858: t15.2023.10.15 val PER: 0.1641
2026-01-04 00:24:49,859: t15.2023.10.20 val PER: 0.1611
2026-01-04 00:24:49,859: t15.2023.10.22 val PER: 0.1136
2026-01-04 00:24:49,859: t15.2023.11.03 val PER: 0.1798
2026-01-04 00:24:49,859: t15.2023.11.04 val PER: 0.0410
2026-01-04 00:24:49,859: t15.2023.11.17 val PER: 0.0404
2026-01-04 00:24:49,859: t15.2023.11.19 val PER: 0.0279
2026-01-04 00:24:49,860: t15.2023.11.26 val PER: 0.1196
2026-01-04 00:24:49,860: t15.2023.12.03 val PER: 0.1061
2026-01-04 00:24:49,860: t15.2023.12.08 val PER: 0.1039
2026-01-04 00:24:49,860: t15.2023.12.10 val PER: 0.1012
2026-01-04 00:24:49,860: t15.2023.12.17 val PER: 0.1538
2026-01-04 00:24:49,860: t15.2023.12.29 val PER: 0.1318
2026-01-04 00:24:49,860: t15.2024.02.25 val PER: 0.1138
2026-01-04 00:24:49,860: t15.2024.03.08 val PER: 0.2319
2026-01-04 00:24:49,860: t15.2024.03.15 val PER: 0.1951
2026-01-04 00:24:49,861: t15.2024.03.17 val PER: 0.1381
2026-01-04 00:24:49,861: t15.2024.05.10 val PER: 0.1560
2026-01-04 00:24:49,861: t15.2024.06.14 val PER: 0.1625
2026-01-04 00:24:49,861: t15.2024.07.19 val PER: 0.2380
2026-01-04 00:24:49,861: t15.2024.07.21 val PER: 0.0952
2026-01-04 00:24:49,861: t15.2024.07.28 val PER: 0.1368
2026-01-04 00:24:49,861: t15.2025.01.10 val PER: 0.2851
2026-01-04 00:24:49,861: t15.2025.01.12 val PER: 0.1609
2026-01-04 00:24:49,861: t15.2025.03.14 val PER: 0.3358
2026-01-04 00:24:49,861: t15.2025.03.16 val PER: 0.1846
2026-01-04 00:24:49,861: t15.2025.03.30 val PER: 0.2897
2026-01-04 00:24:49,861: t15.2025.04.13 val PER: 0.2211
2026-01-04 00:24:50,118: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_14500
2026-01-04 00:24:58,806: Train batch 14600: loss: 11.57 grad norm: 63.08 time: 0.058
2026-01-04 00:25:16,259: Train batch 14800: loss: 5.15 grad norm: 47.82 time: 0.050
2026-01-04 00:25:33,630: Train batch 15000: loss: 7.98 grad norm: 46.73 time: 0.052
2026-01-04 00:25:33,630: Running test after training batch: 15000
2026-01-04 00:25:33,724: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:25:38,705: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:25:38,738: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 00:25:40,555: Val batch 15000: PER (avg): 0.1481 CTC Loss (avg): 15.2992 WER(1gram): 45.18% (n=64) time: 6.924
2026-01-04 00:25:40,555: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-04 00:25:40,555: t15.2023.08.13 val PER: 0.1071
2026-01-04 00:25:40,555: t15.2023.08.18 val PER: 0.1081
2026-01-04 00:25:40,555: t15.2023.08.20 val PER: 0.1080
2026-01-04 00:25:40,555: t15.2023.08.25 val PER: 0.0858
2026-01-04 00:25:40,555: t15.2023.08.27 val PER: 0.1720
2026-01-04 00:25:40,555: t15.2023.09.01 val PER: 0.0771
2026-01-04 00:25:40,556: t15.2023.09.03 val PER: 0.1485
2026-01-04 00:25:40,556: t15.2023.09.24 val PER: 0.1238
2026-01-04 00:25:40,556: t15.2023.09.29 val PER: 0.1289
2026-01-04 00:25:40,556: t15.2023.10.01 val PER: 0.1724
2026-01-04 00:25:40,556: t15.2023.10.06 val PER: 0.0829
2026-01-04 00:25:40,556: t15.2023.10.08 val PER: 0.2355
2026-01-04 00:25:40,556: t15.2023.10.13 val PER: 0.1971
2026-01-04 00:25:40,556: t15.2023.10.15 val PER: 0.1556
2026-01-04 00:25:40,556: t15.2023.10.20 val PER: 0.1779
2026-01-04 00:25:40,556: t15.2023.10.22 val PER: 0.1136
2026-01-04 00:25:40,556: t15.2023.11.03 val PER: 0.1710
2026-01-04 00:25:40,556: t15.2023.11.04 val PER: 0.0444
2026-01-04 00:25:40,556: t15.2023.11.17 val PER: 0.0373
2026-01-04 00:25:40,556: t15.2023.11.19 val PER: 0.0240
2026-01-04 00:25:40,557: t15.2023.11.26 val PER: 0.1051
2026-01-04 00:25:40,557: t15.2023.12.03 val PER: 0.1166
2026-01-04 00:25:40,557: t15.2023.12.08 val PER: 0.0992
2026-01-04 00:25:40,557: t15.2023.12.10 val PER: 0.0986
2026-01-04 00:25:40,557: t15.2023.12.17 val PER: 0.1435
2026-01-04 00:25:40,557: t15.2023.12.29 val PER: 0.1386
2026-01-04 00:25:40,557: t15.2024.02.25 val PER: 0.1053
2026-01-04 00:25:40,557: t15.2024.03.08 val PER: 0.2233
2026-01-04 00:25:40,557: t15.2024.03.15 val PER: 0.1951
2026-01-04 00:25:40,557: t15.2024.03.17 val PER: 0.1325
2026-01-04 00:25:40,557: t15.2024.05.10 val PER: 0.1605
2026-01-04 00:25:40,557: t15.2024.06.14 val PER: 0.1546
2026-01-04 00:25:40,557: t15.2024.07.19 val PER: 0.2268
2026-01-04 00:25:40,557: t15.2024.07.21 val PER: 0.0903
2026-01-04 00:25:40,557: t15.2024.07.28 val PER: 0.1338
2026-01-04 00:25:40,557: t15.2025.01.10 val PER: 0.2865
2026-01-04 00:25:40,557: t15.2025.01.12 val PER: 0.1501
2026-01-04 00:25:40,558: t15.2025.03.14 val PER: 0.3491
2026-01-04 00:25:40,558: t15.2025.03.16 val PER: 0.1832
2026-01-04 00:25:40,558: t15.2025.03.30 val PER: 0.3046
2026-01-04 00:25:40,558: t15.2025.04.13 val PER: 0.2197
2026-01-04 00:25:40,559: New best val WER(1gram) 46.19% --> 45.18%
2026-01-04 00:25:40,559: Checkpointing model
2026-01-04 00:25:41,170: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:25:41,431: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_15000
2026-01-04 00:25:59,045: Train batch 15200: loss: 4.56 grad norm: 41.70 time: 0.056
2026-01-04 00:26:16,132: Train batch 15400: loss: 10.81 grad norm: 58.89 time: 0.048
2026-01-04 00:26:24,844: Running test after training batch: 15500
2026-01-04 00:26:25,002: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:26:29,697: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:26:29,730: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 00:26:31,552: Val batch 15500: PER (avg): 0.1488 CTC Loss (avg): 15.1724 WER(1gram): 46.19% (n=64) time: 6.708
2026-01-04 00:26:31,552: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 00:26:31,553: t15.2023.08.13 val PER: 0.1123
2026-01-04 00:26:31,553: t15.2023.08.18 val PER: 0.1065
2026-01-04 00:26:31,553: t15.2023.08.20 val PER: 0.1056
2026-01-04 00:26:31,553: t15.2023.08.25 val PER: 0.0828
2026-01-04 00:26:31,553: t15.2023.08.27 val PER: 0.1720
2026-01-04 00:26:31,553: t15.2023.09.01 val PER: 0.0755
2026-01-04 00:26:31,553: t15.2023.09.03 val PER: 0.1639
2026-01-04 00:26:31,553: t15.2023.09.24 val PER: 0.1189
2026-01-04 00:26:31,553: t15.2023.09.29 val PER: 0.1206
2026-01-04 00:26:31,554: t15.2023.10.01 val PER: 0.1717
2026-01-04 00:26:31,554: t15.2023.10.06 val PER: 0.0883
2026-01-04 00:26:31,554: t15.2023.10.08 val PER: 0.2436
2026-01-04 00:26:31,554: t15.2023.10.13 val PER: 0.1893
2026-01-04 00:26:31,554: t15.2023.10.15 val PER: 0.1622
2026-01-04 00:26:31,554: t15.2023.10.20 val PER: 0.1779
2026-01-04 00:26:31,554: t15.2023.10.22 val PER: 0.1125
2026-01-04 00:26:31,554: t15.2023.11.03 val PER: 0.1771
2026-01-04 00:26:31,554: t15.2023.11.04 val PER: 0.0410
2026-01-04 00:26:31,554: t15.2023.11.17 val PER: 0.0435
2026-01-04 00:26:31,554: t15.2023.11.19 val PER: 0.0399
2026-01-04 00:26:31,554: t15.2023.11.26 val PER: 0.1159
2026-01-04 00:26:31,554: t15.2023.12.03 val PER: 0.1166
2026-01-04 00:26:31,554: t15.2023.12.08 val PER: 0.0932
2026-01-04 00:26:31,554: t15.2023.12.10 val PER: 0.0894
2026-01-04 00:26:31,554: t15.2023.12.17 val PER: 0.1414
2026-01-04 00:26:31,555: t15.2023.12.29 val PER: 0.1345
2026-01-04 00:26:31,555: t15.2024.02.25 val PER: 0.1081
2026-01-04 00:26:31,555: t15.2024.03.08 val PER: 0.2333
2026-01-04 00:26:31,555: t15.2024.03.15 val PER: 0.1945
2026-01-04 00:26:31,555: t15.2024.03.17 val PER: 0.1409
2026-01-04 00:26:31,555: t15.2024.05.10 val PER: 0.1605
2026-01-04 00:26:31,555: t15.2024.06.14 val PER: 0.1546
2026-01-04 00:26:31,555: t15.2024.07.19 val PER: 0.2281
2026-01-04 00:26:31,555: t15.2024.07.21 val PER: 0.0910
2026-01-04 00:26:31,555: t15.2024.07.28 val PER: 0.1404
2026-01-04 00:26:31,555: t15.2025.01.10 val PER: 0.2810
2026-01-04 00:26:31,555: t15.2025.01.12 val PER: 0.1540
2026-01-04 00:26:31,555: t15.2025.03.14 val PER: 0.3254
2026-01-04 00:26:31,555: t15.2025.03.16 val PER: 0.1937
2026-01-04 00:26:31,555: t15.2025.03.30 val PER: 0.3046
2026-01-04 00:26:31,556: t15.2025.04.13 val PER: 0.2083
2026-01-04 00:26:31,810: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_15500
2026-01-04 00:26:40,284: Train batch 15600: loss: 10.69 grad norm: 59.87 time: 0.061
2026-01-04 00:26:57,364: Train batch 15800: loss: 12.67 grad norm: 65.65 time: 0.066
2026-01-04 00:27:15,484: Train batch 16000: loss: 8.13 grad norm: 50.84 time: 0.055
2026-01-04 00:27:15,485: Running test after training batch: 16000
2026-01-04 00:27:15,627: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:27:21,479: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:27:21,512: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 00:27:23,358: Val batch 16000: PER (avg): 0.1483 CTC Loss (avg): 15.2647 WER(1gram): 45.69% (n=64) time: 7.873
2026-01-04 00:27:23,359: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 00:27:23,359: t15.2023.08.13 val PER: 0.1143
2026-01-04 00:27:23,359: t15.2023.08.18 val PER: 0.1056
2026-01-04 00:27:23,359: t15.2023.08.20 val PER: 0.1025
2026-01-04 00:27:23,359: t15.2023.08.25 val PER: 0.0904
2026-01-04 00:27:23,359: t15.2023.08.27 val PER: 0.1736
2026-01-04 00:27:23,359: t15.2023.09.01 val PER: 0.0698
2026-01-04 00:27:23,359: t15.2023.09.03 val PER: 0.1580
2026-01-04 00:27:23,360: t15.2023.09.24 val PER: 0.1226
2026-01-04 00:27:23,360: t15.2023.09.29 val PER: 0.1321
2026-01-04 00:27:23,360: t15.2023.10.01 val PER: 0.1731
2026-01-04 00:27:23,360: t15.2023.10.06 val PER: 0.0829
2026-01-04 00:27:23,360: t15.2023.10.08 val PER: 0.2409
2026-01-04 00:27:23,360: t15.2023.10.13 val PER: 0.1885
2026-01-04 00:27:23,360: t15.2023.10.15 val PER: 0.1562
2026-01-04 00:27:23,360: t15.2023.10.20 val PER: 0.1779
2026-01-04 00:27:23,360: t15.2023.10.22 val PER: 0.1047
2026-01-04 00:27:23,360: t15.2023.11.03 val PER: 0.1710
2026-01-04 00:27:23,360: t15.2023.11.04 val PER: 0.0478
2026-01-04 00:27:23,360: t15.2023.11.17 val PER: 0.0404
2026-01-04 00:27:23,360: t15.2023.11.19 val PER: 0.0379
2026-01-04 00:27:23,360: t15.2023.11.26 val PER: 0.1072
2026-01-04 00:27:23,360: t15.2023.12.03 val PER: 0.1124
2026-01-04 00:27:23,361: t15.2023.12.08 val PER: 0.0959
2026-01-04 00:27:23,361: t15.2023.12.10 val PER: 0.0920
2026-01-04 00:27:23,361: t15.2023.12.17 val PER: 0.1424
2026-01-04 00:27:23,361: t15.2023.12.29 val PER: 0.1318
2026-01-04 00:27:23,361: t15.2024.02.25 val PER: 0.1067
2026-01-04 00:27:23,361: t15.2024.03.08 val PER: 0.2347
2026-01-04 00:27:23,361: t15.2024.03.15 val PER: 0.1920
2026-01-04 00:27:23,361: t15.2024.03.17 val PER: 0.1283
2026-01-04 00:27:23,361: t15.2024.05.10 val PER: 0.1694
2026-01-04 00:27:23,361: t15.2024.06.14 val PER: 0.1672
2026-01-04 00:27:23,361: t15.2024.07.19 val PER: 0.2334
2026-01-04 00:27:23,361: t15.2024.07.21 val PER: 0.0938
2026-01-04 00:27:23,361: t15.2024.07.28 val PER: 0.1397
2026-01-04 00:27:23,361: t15.2025.01.10 val PER: 0.2851
2026-01-04 00:27:23,362: t15.2025.01.12 val PER: 0.1524
2026-01-04 00:27:23,362: t15.2025.03.14 val PER: 0.3269
2026-01-04 00:27:23,362: t15.2025.03.16 val PER: 0.1885
2026-01-04 00:27:23,362: t15.2025.03.30 val PER: 0.3092
2026-01-04 00:27:23,362: t15.2025.04.13 val PER: 0.2183
2026-01-04 00:27:23,621: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_16000
2026-01-04 00:27:41,296: Train batch 16200: loss: 5.94 grad norm: 45.04 time: 0.055
2026-01-04 00:27:59,022: Train batch 16400: loss: 9.70 grad norm: 65.31 time: 0.057
2026-01-04 00:28:08,050: Running test after training batch: 16500
2026-01-04 00:28:08,176: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:28:12,861: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:28:12,895: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 00:28:14,739: Val batch 16500: PER (avg): 0.1461 CTC Loss (avg): 15.1084 WER(1gram): 45.69% (n=64) time: 6.689
2026-01-04 00:28:14,740: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 00:28:14,740: t15.2023.08.13 val PER: 0.1102
2026-01-04 00:28:14,740: t15.2023.08.18 val PER: 0.1073
2026-01-04 00:28:14,740: t15.2023.08.20 val PER: 0.1017
2026-01-04 00:28:14,740: t15.2023.08.25 val PER: 0.0753
2026-01-04 00:28:14,740: t15.2023.08.27 val PER: 0.1768
2026-01-04 00:28:14,740: t15.2023.09.01 val PER: 0.0812
2026-01-04 00:28:14,740: t15.2023.09.03 val PER: 0.1496
2026-01-04 00:28:14,740: t15.2023.09.24 val PER: 0.1226
2026-01-04 00:28:14,740: t15.2023.09.29 val PER: 0.1264
2026-01-04 00:28:14,741: t15.2023.10.01 val PER: 0.1664
2026-01-04 00:28:14,741: t15.2023.10.06 val PER: 0.0807
2026-01-04 00:28:14,741: t15.2023.10.08 val PER: 0.2314
2026-01-04 00:28:14,741: t15.2023.10.13 val PER: 0.1924
2026-01-04 00:28:14,741: t15.2023.10.15 val PER: 0.1549
2026-01-04 00:28:14,741: t15.2023.10.20 val PER: 0.1779
2026-01-04 00:28:14,741: t15.2023.10.22 val PER: 0.1114
2026-01-04 00:28:14,741: t15.2023.11.03 val PER: 0.1744
2026-01-04 00:28:14,741: t15.2023.11.04 val PER: 0.0375
2026-01-04 00:28:14,741: t15.2023.11.17 val PER: 0.0404
2026-01-04 00:28:14,741: t15.2023.11.19 val PER: 0.0259
2026-01-04 00:28:14,741: t15.2023.11.26 val PER: 0.1029
2026-01-04 00:28:14,742: t15.2023.12.03 val PER: 0.1040
2026-01-04 00:28:14,742: t15.2023.12.08 val PER: 0.0952
2026-01-04 00:28:14,742: t15.2023.12.10 val PER: 0.0894
2026-01-04 00:28:14,742: t15.2023.12.17 val PER: 0.1424
2026-01-04 00:28:14,742: t15.2023.12.29 val PER: 0.1297
2026-01-04 00:28:14,742: t15.2024.02.25 val PER: 0.1053
2026-01-04 00:28:14,742: t15.2024.03.08 val PER: 0.2361
2026-01-04 00:28:14,742: t15.2024.03.15 val PER: 0.1964
2026-01-04 00:28:14,742: t15.2024.03.17 val PER: 0.1367
2026-01-04 00:28:14,742: t15.2024.05.10 val PER: 0.1545
2026-01-04 00:28:14,742: t15.2024.06.14 val PER: 0.1546
2026-01-04 00:28:14,742: t15.2024.07.19 val PER: 0.2301
2026-01-04 00:28:14,742: t15.2024.07.21 val PER: 0.0848
2026-01-04 00:28:14,742: t15.2024.07.28 val PER: 0.1279
2026-01-04 00:28:14,743: t15.2025.01.10 val PER: 0.2782
2026-01-04 00:28:14,743: t15.2025.01.12 val PER: 0.1563
2026-01-04 00:28:14,743: t15.2025.03.14 val PER: 0.3284
2026-01-04 00:28:14,743: t15.2025.03.16 val PER: 0.1911
2026-01-04 00:28:14,743: t15.2025.03.30 val PER: 0.2977
2026-01-04 00:28:14,743: t15.2025.04.13 val PER: 0.2126
2026-01-04 00:28:14,997: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_16500
2026-01-04 00:28:23,723: Train batch 16600: loss: 7.34 grad norm: 52.69 time: 0.052
2026-01-04 00:28:41,422: Train batch 16800: loss: 15.00 grad norm: 76.67 time: 0.062
2026-01-04 00:28:59,213: Train batch 17000: loss: 7.19 grad norm: 49.29 time: 0.081
2026-01-04 00:28:59,213: Running test after training batch: 17000
2026-01-04 00:28:59,309: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:29:04,003: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:29:04,037: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 00:29:05,908: Val batch 17000: PER (avg): 0.1444 CTC Loss (avg): 14.9794 WER(1gram): 46.45% (n=64) time: 6.695
2026-01-04 00:29:05,908: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-04 00:29:05,909: t15.2023.08.13 val PER: 0.1050
2026-01-04 00:29:05,909: t15.2023.08.18 val PER: 0.1098
2026-01-04 00:29:05,909: t15.2023.08.20 val PER: 0.1041
2026-01-04 00:29:05,909: t15.2023.08.25 val PER: 0.0798
2026-01-04 00:29:05,909: t15.2023.08.27 val PER: 0.1720
2026-01-04 00:29:05,909: t15.2023.09.01 val PER: 0.0763
2026-01-04 00:29:05,909: t15.2023.09.03 val PER: 0.1520
2026-01-04 00:29:05,909: t15.2023.09.24 val PER: 0.1250
2026-01-04 00:29:05,909: t15.2023.09.29 val PER: 0.1264
2026-01-04 00:29:05,909: t15.2023.10.01 val PER: 0.1671
2026-01-04 00:29:05,909: t15.2023.10.06 val PER: 0.0807
2026-01-04 00:29:05,909: t15.2023.10.08 val PER: 0.2341
2026-01-04 00:29:05,909: t15.2023.10.13 val PER: 0.1924
2026-01-04 00:29:05,909: t15.2023.10.15 val PER: 0.1496
2026-01-04 00:29:05,983: t15.2023.10.20 val PER: 0.1577
2026-01-04 00:29:05,983: t15.2023.10.22 val PER: 0.1080
2026-01-04 00:29:05,984: t15.2023.11.03 val PER: 0.1723
2026-01-04 00:29:05,984: t15.2023.11.04 val PER: 0.0444
2026-01-04 00:29:05,984: t15.2023.11.17 val PER: 0.0327
2026-01-04 00:29:05,984: t15.2023.11.19 val PER: 0.0279
2026-01-04 00:29:05,984: t15.2023.11.26 val PER: 0.0993
2026-01-04 00:29:05,984: t15.2023.12.03 val PER: 0.1082
2026-01-04 00:29:05,984: t15.2023.12.08 val PER: 0.0866
2026-01-04 00:29:05,984: t15.2023.12.10 val PER: 0.0880
2026-01-04 00:29:05,984: t15.2023.12.17 val PER: 0.1362
2026-01-04 00:29:05,985: t15.2023.12.29 val PER: 0.1297
2026-01-04 00:29:05,985: t15.2024.02.25 val PER: 0.1053
2026-01-04 00:29:05,985: t15.2024.03.08 val PER: 0.2219
2026-01-04 00:29:05,985: t15.2024.03.15 val PER: 0.1951
2026-01-04 00:29:05,985: t15.2024.03.17 val PER: 0.1325
2026-01-04 00:29:05,985: t15.2024.05.10 val PER: 0.1605
2026-01-04 00:29:05,985: t15.2024.06.14 val PER: 0.1562
2026-01-04 00:29:05,985: t15.2024.07.19 val PER: 0.2254
2026-01-04 00:29:05,985: t15.2024.07.21 val PER: 0.0903
2026-01-04 00:29:05,985: t15.2024.07.28 val PER: 0.1338
2026-01-04 00:29:05,985: t15.2025.01.10 val PER: 0.2796
2026-01-04 00:29:05,985: t15.2025.01.12 val PER: 0.1509
2026-01-04 00:29:05,985: t15.2025.03.14 val PER: 0.3195
2026-01-04 00:29:05,985: t15.2025.03.16 val PER: 0.1859
2026-01-04 00:29:05,985: t15.2025.03.30 val PER: 0.2897
2026-01-04 00:29:05,985: t15.2025.04.13 val PER: 0.2140
2026-01-04 00:29:06,245: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_17000
2026-01-04 00:29:23,999: Train batch 17200: loss: 9.09 grad norm: 54.12 time: 0.084
2026-01-04 00:29:41,919: Train batch 17400: loss: 10.38 grad norm: 60.08 time: 0.072
2026-01-04 00:29:50,651: Running test after training batch: 17500
2026-01-04 00:29:50,755: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:29:55,729: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:29:55,764: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 00:29:57,638: Val batch 17500: PER (avg): 0.1444 CTC Loss (avg): 15.0188 WER(1gram): 46.19% (n=64) time: 6.986
2026-01-04 00:29:57,638: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 00:29:57,638: t15.2023.08.13 val PER: 0.1029
2026-01-04 00:29:57,638: t15.2023.08.18 val PER: 0.1073
2026-01-04 00:29:57,639: t15.2023.08.20 val PER: 0.1017
2026-01-04 00:29:57,639: t15.2023.08.25 val PER: 0.0798
2026-01-04 00:29:57,639: t15.2023.08.27 val PER: 0.1672
2026-01-04 00:29:57,639: t15.2023.09.01 val PER: 0.0804
2026-01-04 00:29:57,639: t15.2023.09.03 val PER: 0.1544
2026-01-04 00:29:57,639: t15.2023.09.24 val PER: 0.1201
2026-01-04 00:29:57,639: t15.2023.09.29 val PER: 0.1270
2026-01-04 00:29:57,639: t15.2023.10.01 val PER: 0.1671
2026-01-04 00:29:57,639: t15.2023.10.06 val PER: 0.0797
2026-01-04 00:29:57,639: t15.2023.10.08 val PER: 0.2382
2026-01-04 00:29:57,639: t15.2023.10.13 val PER: 0.1823
2026-01-04 00:29:57,639: t15.2023.10.15 val PER: 0.1543
2026-01-04 00:29:57,639: t15.2023.10.20 val PER: 0.1779
2026-01-04 00:29:57,640: t15.2023.10.22 val PER: 0.1047
2026-01-04 00:29:57,640: t15.2023.11.03 val PER: 0.1689
2026-01-04 00:29:57,640: t15.2023.11.04 val PER: 0.0410
2026-01-04 00:29:57,640: t15.2023.11.17 val PER: 0.0373
2026-01-04 00:29:57,640: t15.2023.11.19 val PER: 0.0259
2026-01-04 00:29:57,640: t15.2023.11.26 val PER: 0.1051
2026-01-04 00:29:57,640: t15.2023.12.03 val PER: 0.1061
2026-01-04 00:29:57,640: t15.2023.12.08 val PER: 0.0945
2026-01-04 00:29:57,640: t15.2023.12.10 val PER: 0.0907
2026-01-04 00:29:57,640: t15.2023.12.17 val PER: 0.1372
2026-01-04 00:29:57,641: t15.2023.12.29 val PER: 0.1318
2026-01-04 00:29:57,641: t15.2024.02.25 val PER: 0.1011
2026-01-04 00:29:57,641: t15.2024.03.08 val PER: 0.2148
2026-01-04 00:29:57,641: t15.2024.03.15 val PER: 0.1870
2026-01-04 00:29:57,641: t15.2024.03.17 val PER: 0.1395
2026-01-04 00:29:57,641: t15.2024.05.10 val PER: 0.1590
2026-01-04 00:29:57,641: t15.2024.06.14 val PER: 0.1546
2026-01-04 00:29:57,641: t15.2024.07.19 val PER: 0.2241
2026-01-04 00:29:57,641: t15.2024.07.21 val PER: 0.0897
2026-01-04 00:29:57,641: t15.2024.07.28 val PER: 0.1324
2026-01-04 00:29:57,641: t15.2025.01.10 val PER: 0.2865
2026-01-04 00:29:57,641: t15.2025.01.12 val PER: 0.1501
2026-01-04 00:29:57,641: t15.2025.03.14 val PER: 0.3210
2026-01-04 00:29:57,641: t15.2025.03.16 val PER: 0.1846
2026-01-04 00:29:57,641: t15.2025.03.30 val PER: 0.2885
2026-01-04 00:29:57,642: t15.2025.04.13 val PER: 0.2140
2026-01-04 00:29:57,897: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_17500
2026-01-04 00:30:06,655: Train batch 17600: loss: 9.30 grad norm: 58.66 time: 0.051
2026-01-04 00:30:24,245: Train batch 17800: loss: 6.13 grad norm: 53.82 time: 0.042
2026-01-04 00:30:41,872: Train batch 18000: loss: 10.35 grad norm: 69.09 time: 0.060
2026-01-04 00:30:41,873: Running test after training batch: 18000
2026-01-04 00:30:42,009: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:30:46,683: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:30:46,717: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 00:30:48,593: Val batch 18000: PER (avg): 0.1447 CTC Loss (avg): 15.0381 WER(1gram): 44.92% (n=64) time: 6.720
2026-01-04 00:30:48,594: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 00:30:48,594: t15.2023.08.13 val PER: 0.1071
2026-01-04 00:30:48,594: t15.2023.08.18 val PER: 0.1039
2026-01-04 00:30:48,594: t15.2023.08.20 val PER: 0.1017
2026-01-04 00:30:48,594: t15.2023.08.25 val PER: 0.0813
2026-01-04 00:30:48,594: t15.2023.08.27 val PER: 0.1736
2026-01-04 00:30:48,594: t15.2023.09.01 val PER: 0.0804
2026-01-04 00:30:48,594: t15.2023.09.03 val PER: 0.1580
2026-01-04 00:30:48,594: t15.2023.09.24 val PER: 0.1189
2026-01-04 00:30:48,594: t15.2023.09.29 val PER: 0.1238
2026-01-04 00:30:48,594: t15.2023.10.01 val PER: 0.1691
2026-01-04 00:30:48,594: t15.2023.10.06 val PER: 0.0797
2026-01-04 00:30:48,595: t15.2023.10.08 val PER: 0.2382
2026-01-04 00:30:48,595: t15.2023.10.13 val PER: 0.1885
2026-01-04 00:30:48,595: t15.2023.10.15 val PER: 0.1556
2026-01-04 00:30:48,595: t15.2023.10.20 val PER: 0.1779
2026-01-04 00:30:48,595: t15.2023.10.22 val PER: 0.1080
2026-01-04 00:30:48,595: t15.2023.11.03 val PER: 0.1710
2026-01-04 00:30:48,595: t15.2023.11.04 val PER: 0.0410
2026-01-04 00:30:48,595: t15.2023.11.17 val PER: 0.0358
2026-01-04 00:30:48,595: t15.2023.11.19 val PER: 0.0279
2026-01-04 00:30:48,595: t15.2023.11.26 val PER: 0.1036
2026-01-04 00:30:48,595: t15.2023.12.03 val PER: 0.1082
2026-01-04 00:30:48,595: t15.2023.12.08 val PER: 0.0932
2026-01-04 00:30:48,595: t15.2023.12.10 val PER: 0.0894
2026-01-04 00:30:48,595: t15.2023.12.17 val PER: 0.1362
2026-01-04 00:30:48,595: t15.2023.12.29 val PER: 0.1318
2026-01-04 00:30:48,596: t15.2024.02.25 val PER: 0.1025
2026-01-04 00:30:48,596: t15.2024.03.08 val PER: 0.2176
2026-01-04 00:30:48,596: t15.2024.03.15 val PER: 0.1914
2026-01-04 00:30:48,596: t15.2024.03.17 val PER: 0.1409
2026-01-04 00:30:48,596: t15.2024.05.10 val PER: 0.1516
2026-01-04 00:30:48,596: t15.2024.06.14 val PER: 0.1546
2026-01-04 00:30:48,596: t15.2024.07.19 val PER: 0.2254
2026-01-04 00:30:48,596: t15.2024.07.21 val PER: 0.0841
2026-01-04 00:30:48,596: t15.2024.07.28 val PER: 0.1309
2026-01-04 00:30:48,596: t15.2025.01.10 val PER: 0.2796
2026-01-04 00:30:48,596: t15.2025.01.12 val PER: 0.1493
2026-01-04 00:30:48,597: t15.2025.03.14 val PER: 0.3210
2026-01-04 00:30:48,597: t15.2025.03.16 val PER: 0.1898
2026-01-04 00:30:48,597: t15.2025.03.30 val PER: 0.2885
2026-01-04 00:30:48,597: t15.2025.04.13 val PER: 0.2126
2026-01-04 00:30:48,597: New best val WER(1gram) 45.18% --> 44.92%
2026-01-04 00:30:48,598: Checkpointing model
2026-01-04 00:30:49,236: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/best_checkpoint
2026-01-04 00:30:49,500: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_18000
2026-01-04 00:31:07,107: Train batch 18200: loss: 6.92 grad norm: 49.78 time: 0.073
2026-01-04 00:31:24,698: Train batch 18400: loss: 4.12 grad norm: 44.10 time: 0.057
2026-01-04 00:31:33,567: Running test after training batch: 18500
2026-01-04 00:31:33,706: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:31:38,659: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:31:38,694: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 00:31:40,591: Val batch 18500: PER (avg): 0.1456 CTC Loss (avg): 14.9924 WER(1gram): 45.43% (n=64) time: 7.023
2026-01-04 00:31:40,591: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 00:31:40,591: t15.2023.08.13 val PER: 0.1071
2026-01-04 00:31:40,592: t15.2023.08.18 val PER: 0.1056
2026-01-04 00:31:40,592: t15.2023.08.20 val PER: 0.1064
2026-01-04 00:31:40,592: t15.2023.08.25 val PER: 0.0768
2026-01-04 00:31:40,592: t15.2023.08.27 val PER: 0.1768
2026-01-04 00:31:40,592: t15.2023.09.01 val PER: 0.0812
2026-01-04 00:31:40,592: t15.2023.09.03 val PER: 0.1568
2026-01-04 00:31:40,592: t15.2023.09.24 val PER: 0.1214
2026-01-04 00:31:40,592: t15.2023.09.29 val PER: 0.1251
2026-01-04 00:31:40,592: t15.2023.10.01 val PER: 0.1658
2026-01-04 00:31:40,592: t15.2023.10.06 val PER: 0.0829
2026-01-04 00:31:40,593: t15.2023.10.08 val PER: 0.2368
2026-01-04 00:31:40,593: t15.2023.10.13 val PER: 0.1924
2026-01-04 00:31:40,593: t15.2023.10.15 val PER: 0.1582
2026-01-04 00:31:40,593: t15.2023.10.20 val PER: 0.1779
2026-01-04 00:31:40,593: t15.2023.10.22 val PER: 0.1024
2026-01-04 00:31:40,593: t15.2023.11.03 val PER: 0.1696
2026-01-04 00:31:40,593: t15.2023.11.04 val PER: 0.0375
2026-01-04 00:31:40,593: t15.2023.11.17 val PER: 0.0358
2026-01-04 00:31:40,593: t15.2023.11.19 val PER: 0.0339
2026-01-04 00:31:40,593: t15.2023.11.26 val PER: 0.1065
2026-01-04 00:31:40,593: t15.2023.12.03 val PER: 0.1050
2026-01-04 00:31:40,593: t15.2023.12.08 val PER: 0.0952
2026-01-04 00:31:40,594: t15.2023.12.10 val PER: 0.0828
2026-01-04 00:31:40,594: t15.2023.12.17 val PER: 0.1331
2026-01-04 00:31:40,594: t15.2023.12.29 val PER: 0.1318
2026-01-04 00:31:40,594: t15.2024.02.25 val PER: 0.1025
2026-01-04 00:31:40,594: t15.2024.03.08 val PER: 0.2219
2026-01-04 00:31:40,594: t15.2024.03.15 val PER: 0.1920
2026-01-04 00:31:40,594: t15.2024.03.17 val PER: 0.1367
2026-01-04 00:31:40,594: t15.2024.05.10 val PER: 0.1530
2026-01-04 00:31:40,594: t15.2024.06.14 val PER: 0.1514
2026-01-04 00:31:40,594: t15.2024.07.19 val PER: 0.2294
2026-01-04 00:31:40,595: t15.2024.07.21 val PER: 0.0924
2026-01-04 00:31:40,595: t15.2024.07.28 val PER: 0.1301
2026-01-04 00:31:40,595: t15.2025.01.10 val PER: 0.2810
2026-01-04 00:31:40,595: t15.2025.01.12 val PER: 0.1517
2026-01-04 00:31:40,595: t15.2025.03.14 val PER: 0.3284
2026-01-04 00:31:40,595: t15.2025.03.16 val PER: 0.1885
2026-01-04 00:31:40,595: t15.2025.03.30 val PER: 0.2966
2026-01-04 00:31:40,595: t15.2025.04.13 val PER: 0.2111
2026-01-04 00:31:40,860: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_18500
2026-01-04 00:31:49,784: Train batch 18600: loss: 12.00 grad norm: 65.58 time: 0.068
2026-01-04 00:32:07,380: Train batch 18800: loss: 7.88 grad norm: 53.28 time: 0.064
2026-01-04 00:32:25,299: Train batch 19000: loss: 7.53 grad norm: 46.81 time: 0.063
2026-01-04 00:32:25,299: Running test after training batch: 19000
2026-01-04 00:32:25,430: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:32:30,100: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:32:30,135: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 00:32:32,042: Val batch 19000: PER (avg): 0.1443 CTC Loss (avg): 14.9766 WER(1gram): 45.69% (n=64) time: 6.743
2026-01-04 00:32:32,042: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 00:32:32,043: t15.2023.08.13 val PER: 0.1050
2026-01-04 00:32:32,043: t15.2023.08.18 val PER: 0.1056
2026-01-04 00:32:32,043: t15.2023.08.20 val PER: 0.1025
2026-01-04 00:32:32,043: t15.2023.08.25 val PER: 0.0798
2026-01-04 00:32:32,043: t15.2023.08.27 val PER: 0.1704
2026-01-04 00:32:32,043: t15.2023.09.01 val PER: 0.0812
2026-01-04 00:32:32,043: t15.2023.09.03 val PER: 0.1556
2026-01-04 00:32:32,043: t15.2023.09.24 val PER: 0.1201
2026-01-04 00:32:32,043: t15.2023.09.29 val PER: 0.1251
2026-01-04 00:32:32,043: t15.2023.10.01 val PER: 0.1651
2026-01-04 00:32:32,044: t15.2023.10.06 val PER: 0.0797
2026-01-04 00:32:32,044: t15.2023.10.08 val PER: 0.2395
2026-01-04 00:32:32,044: t15.2023.10.13 val PER: 0.1885
2026-01-04 00:32:32,044: t15.2023.10.15 val PER: 0.1536
2026-01-04 00:32:32,044: t15.2023.10.20 val PER: 0.1846
2026-01-04 00:32:32,044: t15.2023.10.22 val PER: 0.1024
2026-01-04 00:32:32,044: t15.2023.11.03 val PER: 0.1662
2026-01-04 00:32:32,044: t15.2023.11.04 val PER: 0.0375
2026-01-04 00:32:32,044: t15.2023.11.17 val PER: 0.0358
2026-01-04 00:32:32,045: t15.2023.11.19 val PER: 0.0279
2026-01-04 00:32:32,045: t15.2023.11.26 val PER: 0.0986
2026-01-04 00:32:32,045: t15.2023.12.03 val PER: 0.1061
2026-01-04 00:32:32,045: t15.2023.12.08 val PER: 0.0965
2026-01-04 00:32:32,045: t15.2023.12.10 val PER: 0.0933
2026-01-04 00:32:32,045: t15.2023.12.17 val PER: 0.1331
2026-01-04 00:32:32,045: t15.2023.12.29 val PER: 0.1359
2026-01-04 00:32:32,045: t15.2024.02.25 val PER: 0.0997
2026-01-04 00:32:32,045: t15.2024.03.08 val PER: 0.2248
2026-01-04 00:32:32,045: t15.2024.03.15 val PER: 0.1857
2026-01-04 00:32:32,045: t15.2024.03.17 val PER: 0.1318
2026-01-04 00:32:32,045: t15.2024.05.10 val PER: 0.1516
2026-01-04 00:32:32,045: t15.2024.06.14 val PER: 0.1562
2026-01-04 00:32:32,045: t15.2024.07.19 val PER: 0.2268
2026-01-04 00:32:32,045: t15.2024.07.21 val PER: 0.0897
2026-01-04 00:32:32,045: t15.2024.07.28 val PER: 0.1338
2026-01-04 00:32:32,046: t15.2025.01.10 val PER: 0.2769
2026-01-04 00:32:32,046: t15.2025.01.12 val PER: 0.1540
2026-01-04 00:32:32,046: t15.2025.03.14 val PER: 0.3254
2026-01-04 00:32:32,046: t15.2025.03.16 val PER: 0.1846
2026-01-04 00:32:32,046: t15.2025.03.30 val PER: 0.2920
2026-01-04 00:32:32,046: t15.2025.04.13 val PER: 0.2126
2026-01-04 00:32:32,307: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_19000
2026-01-04 00:32:49,760: Train batch 19200: loss: 5.46 grad norm: 47.96 time: 0.062
2026-01-04 00:33:07,258: Train batch 19400: loss: 4.50 grad norm: 40.43 time: 0.052
2026-01-04 00:33:15,967: Running test after training batch: 19500
2026-01-04 00:33:16,104: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:33:21,056: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:33:21,092: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 00:33:23,039: Val batch 19500: PER (avg): 0.1446 CTC Loss (avg): 14.9185 WER(1gram): 45.94% (n=64) time: 7.072
2026-01-04 00:33:23,039: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 00:33:23,039: t15.2023.08.13 val PER: 0.1060
2026-01-04 00:33:23,039: t15.2023.08.18 val PER: 0.1065
2026-01-04 00:33:23,039: t15.2023.08.20 val PER: 0.1009
2026-01-04 00:33:23,040: t15.2023.08.25 val PER: 0.0798
2026-01-04 00:33:23,040: t15.2023.08.27 val PER: 0.1656
2026-01-04 00:33:23,040: t15.2023.09.01 val PER: 0.0779
2026-01-04 00:33:23,040: t15.2023.09.03 val PER: 0.1508
2026-01-04 00:33:23,040: t15.2023.09.24 val PER: 0.1226
2026-01-04 00:33:23,040: t15.2023.09.29 val PER: 0.1251
2026-01-04 00:33:23,040: t15.2023.10.01 val PER: 0.1651
2026-01-04 00:33:23,040: t15.2023.10.06 val PER: 0.0818
2026-01-04 00:33:23,040: t15.2023.10.08 val PER: 0.2368
2026-01-04 00:33:23,041: t15.2023.10.13 val PER: 0.1908
2026-01-04 00:33:23,041: t15.2023.10.15 val PER: 0.1569
2026-01-04 00:33:23,041: t15.2023.10.20 val PER: 0.1812
2026-01-04 00:33:23,041: t15.2023.10.22 val PER: 0.1047
2026-01-04 00:33:23,041: t15.2023.11.03 val PER: 0.1703
2026-01-04 00:33:23,041: t15.2023.11.04 val PER: 0.0410
2026-01-04 00:33:23,041: t15.2023.11.17 val PER: 0.0373
2026-01-04 00:33:23,041: t15.2023.11.19 val PER: 0.0319
2026-01-04 00:33:23,041: t15.2023.11.26 val PER: 0.1007
2026-01-04 00:33:23,042: t15.2023.12.03 val PER: 0.1071
2026-01-04 00:33:23,042: t15.2023.12.08 val PER: 0.0952
2026-01-04 00:33:23,042: t15.2023.12.10 val PER: 0.0880
2026-01-04 00:33:23,042: t15.2023.12.17 val PER: 0.1383
2026-01-04 00:33:23,042: t15.2023.12.29 val PER: 0.1297
2026-01-04 00:33:23,042: t15.2024.02.25 val PER: 0.1053
2026-01-04 00:33:23,042: t15.2024.03.08 val PER: 0.2290
2026-01-04 00:33:23,042: t15.2024.03.15 val PER: 0.1876
2026-01-04 00:33:23,042: t15.2024.03.17 val PER: 0.1360
2026-01-04 00:33:23,043: t15.2024.05.10 val PER: 0.1530
2026-01-04 00:33:23,043: t15.2024.06.14 val PER: 0.1546
2026-01-04 00:33:23,043: t15.2024.07.19 val PER: 0.2287
2026-01-04 00:33:23,043: t15.2024.07.21 val PER: 0.0841
2026-01-04 00:33:23,043: t15.2024.07.28 val PER: 0.1316
2026-01-04 00:33:23,043: t15.2025.01.10 val PER: 0.2769
2026-01-04 00:33:23,043: t15.2025.01.12 val PER: 0.1509
2026-01-04 00:33:23,043: t15.2025.03.14 val PER: 0.3210
2026-01-04 00:33:23,043: t15.2025.03.16 val PER: 0.1885
2026-01-04 00:33:23,044: t15.2025.03.30 val PER: 0.2931
2026-01-04 00:33:23,044: t15.2025.04.13 val PER: 0.2154
2026-01-04 00:33:23,309: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_19500
2026-01-04 00:33:31,943: Train batch 19600: loss: 6.88 grad norm: 50.53 time: 0.056
2026-01-04 00:33:48,960: Train batch 19800: loss: 6.77 grad norm: 49.95 time: 0.055
2026-01-04 00:34:05,964: Running test after training batch: 19999
2026-01-04 00:34:06,058: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:34:10,801: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:34:10,836: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 00:34:12,772: Val batch 19999: PER (avg): 0.1439 CTC Loss (avg): 14.9205 WER(1gram): 45.94% (n=64) time: 6.807
2026-01-04 00:34:12,772: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 00:34:12,772: t15.2023.08.13 val PER: 0.1091
2026-01-04 00:34:12,772: t15.2023.08.18 val PER: 0.1065
2026-01-04 00:34:12,772: t15.2023.08.20 val PER: 0.0993
2026-01-04 00:34:12,772: t15.2023.08.25 val PER: 0.0783
2026-01-04 00:34:12,773: t15.2023.08.27 val PER: 0.1672
2026-01-04 00:34:12,773: t15.2023.09.01 val PER: 0.0795
2026-01-04 00:34:12,773: t15.2023.09.03 val PER: 0.1520
2026-01-04 00:34:12,773: t15.2023.09.24 val PER: 0.1226
2026-01-04 00:34:12,773: t15.2023.09.29 val PER: 0.1232
2026-01-04 00:34:12,773: t15.2023.10.01 val PER: 0.1658
2026-01-04 00:34:12,773: t15.2023.10.06 val PER: 0.0797
2026-01-04 00:34:12,773: t15.2023.10.08 val PER: 0.2327
2026-01-04 00:34:12,773: t15.2023.10.13 val PER: 0.1901
2026-01-04 00:34:12,773: t15.2023.10.15 val PER: 0.1562
2026-01-04 00:34:12,773: t15.2023.10.20 val PER: 0.1779
2026-01-04 00:34:12,774: t15.2023.10.22 val PER: 0.1069
2026-01-04 00:34:12,774: t15.2023.11.03 val PER: 0.1716
2026-01-04 00:34:12,774: t15.2023.11.04 val PER: 0.0375
2026-01-04 00:34:12,774: t15.2023.11.17 val PER: 0.0389
2026-01-04 00:34:12,774: t15.2023.11.19 val PER: 0.0299
2026-01-04 00:34:12,774: t15.2023.11.26 val PER: 0.1022
2026-01-04 00:34:12,774: t15.2023.12.03 val PER: 0.1061
2026-01-04 00:34:12,774: t15.2023.12.08 val PER: 0.0952
2026-01-04 00:34:12,774: t15.2023.12.10 val PER: 0.0894
2026-01-04 00:34:12,774: t15.2023.12.17 val PER: 0.1372
2026-01-04 00:34:12,774: t15.2023.12.29 val PER: 0.1249
2026-01-04 00:34:12,774: t15.2024.02.25 val PER: 0.1039
2026-01-04 00:34:12,774: t15.2024.03.08 val PER: 0.2205
2026-01-04 00:34:12,774: t15.2024.03.15 val PER: 0.1801
2026-01-04 00:34:12,774: t15.2024.03.17 val PER: 0.1332
2026-01-04 00:34:12,774: t15.2024.05.10 val PER: 0.1545
2026-01-04 00:34:12,775: t15.2024.06.14 val PER: 0.1640
2026-01-04 00:34:12,775: t15.2024.07.19 val PER: 0.2281
2026-01-04 00:34:12,775: t15.2024.07.21 val PER: 0.0876
2026-01-04 00:34:12,775: t15.2024.07.28 val PER: 0.1294
2026-01-04 00:34:12,775: t15.2025.01.10 val PER: 0.2810
2026-01-04 00:34:12,775: t15.2025.01.12 val PER: 0.1532
2026-01-04 00:34:12,775: t15.2025.03.14 val PER: 0.3254
2026-01-04 00:34:12,775: t15.2025.03.16 val PER: 0.1832
2026-01-04 00:34:12,775: t15.2025.03.30 val PER: 0.2874
2026-01-04 00:34:12,776: t15.2025.04.13 val PER: 0.2068
2026-01-04 00:34:13,037: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d10/checkpoint/checkpoint_batch_19999
2026-01-04 00:34:13,064: Best avg val PER achieved: 0.14469
2026-01-04 00:34:13,064: Total training time: 34.48 minutes

=== RUN d15.yaml ===
2026-01-04 00:34:18,748: Using device: cuda:0
2026-01-04 00:34:20,664: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-04 00:34:20,686: Using 45 sessions after filtering (from 45).
2026-01-04 00:34:21,095: Using torch.compile (if available)
2026-01-04 00:34:21,096: torch.compile not available (torch<2.0). Skipping.
2026-01-04 00:34:21,096: Initialized RNN decoding model
2026-01-04 00:34:21,096: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.15)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-04 00:34:21,096: Model has 44,907,305 parameters
2026-01-04 00:34:21,096: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-04 00:34:22,357: Successfully initialized datasets
2026-01-04 00:34:22,357: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-04 00:34:23,338: Train batch 0: loss: 578.74 grad norm: 1475.94 time: 0.173
2026-01-04 00:34:23,338: Running test after training batch: 0
2026-01-04 00:34:23,449: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:34:28,607: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-04 00:34:29,315: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-04 00:35:03,339: Val batch 0: PER (avg): 1.4288 CTC Loss (avg): 633.1606 WER(1gram): 100.00% (n=64) time: 40.000
2026-01-04 00:35:03,340: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-04 00:35:03,340: t15.2023.08.13 val PER: 1.3025
2026-01-04 00:35:03,340: t15.2023.08.18 val PER: 1.4250
2026-01-04 00:35:03,340: t15.2023.08.20 val PER: 1.3050
2026-01-04 00:35:03,340: t15.2023.08.25 val PER: 1.3464
2026-01-04 00:35:03,340: t15.2023.08.27 val PER: 1.2460
2026-01-04 00:35:03,340: t15.2023.09.01 val PER: 1.4537
2026-01-04 00:35:03,340: t15.2023.09.03 val PER: 1.3183
2026-01-04 00:35:03,340: t15.2023.09.24 val PER: 1.5449
2026-01-04 00:35:03,340: t15.2023.09.29 val PER: 1.4697
2026-01-04 00:35:03,341: t15.2023.10.01 val PER: 1.2114
2026-01-04 00:35:03,341: t15.2023.10.06 val PER: 1.4844
2026-01-04 00:35:03,341: t15.2023.10.08 val PER: 1.1854
2026-01-04 00:35:03,341: t15.2023.10.13 val PER: 1.3949
2026-01-04 00:35:03,341: t15.2023.10.15 val PER: 1.3823
2026-01-04 00:35:03,341: t15.2023.10.20 val PER: 1.5000
2026-01-04 00:35:03,341: t15.2023.10.22 val PER: 1.3931
2026-01-04 00:35:03,341: t15.2023.11.03 val PER: 1.5923
2026-01-04 00:35:03,341: t15.2023.11.04 val PER: 2.0239
2026-01-04 00:35:03,341: t15.2023.11.17 val PER: 1.9549
2026-01-04 00:35:03,341: t15.2023.11.19 val PER: 1.6747
2026-01-04 00:35:03,341: t15.2023.11.26 val PER: 1.5348
2026-01-04 00:35:03,341: t15.2023.12.03 val PER: 1.4265
2026-01-04 00:35:03,341: t15.2023.12.08 val PER: 1.4514
2026-01-04 00:35:03,342: t15.2023.12.10 val PER: 1.6978
2026-01-04 00:35:03,342: t15.2023.12.17 val PER: 1.3035
2026-01-04 00:35:03,342: t15.2023.12.29 val PER: 1.4091
2026-01-04 00:35:03,342: t15.2024.02.25 val PER: 1.4256
2026-01-04 00:35:03,342: t15.2024.03.08 val PER: 1.3215
2026-01-04 00:35:03,342: t15.2024.03.15 val PER: 1.3177
2026-01-04 00:35:03,342: t15.2024.03.17 val PER: 1.3989
2026-01-04 00:35:03,342: t15.2024.05.10 val PER: 1.3135
2026-01-04 00:35:03,342: t15.2024.06.14 val PER: 1.5315
2026-01-04 00:35:03,342: t15.2024.07.19 val PER: 1.0831
2026-01-04 00:35:03,342: t15.2024.07.21 val PER: 1.6297
2026-01-04 00:35:03,342: t15.2024.07.28 val PER: 1.6566
2026-01-04 00:35:03,342: t15.2025.01.10 val PER: 1.0868
2026-01-04 00:35:03,343: t15.2025.01.12 val PER: 1.7729
2026-01-04 00:35:03,343: t15.2025.03.14 val PER: 1.0399
2026-01-04 00:35:03,343: t15.2025.03.16 val PER: 1.6073
2026-01-04 00:35:03,343: t15.2025.03.30 val PER: 1.2885
2026-01-04 00:35:03,343: t15.2025.04.13 val PER: 1.5863
2026-01-04 00:35:03,344: New best val WER(1gram) inf% --> 100.00%
2026-01-04 00:35:03,344: Checkpointing model
2026-01-04 00:35:03,581: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/best_checkpoint
2026-01-04 00:35:03,825: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_0
2026-01-04 00:35:21,328: Train batch 200: loss: 77.59 grad norm: 114.31 time: 0.054
2026-01-04 00:35:38,208: Train batch 400: loss: 53.76 grad norm: 103.64 time: 0.063
2026-01-04 00:35:46,811: Running test after training batch: 500
2026-01-04 00:35:46,951: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:35:51,839: WER debug example
  GT : you can see the code at this point as well
  PR : ewald aunt ease thus uhde at this uhde is aisle
2026-01-04 00:35:51,875: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus ass adz
2026-01-04 00:35:54,538: Val batch 500: PER (avg): 0.5156 CTC Loss (avg): 55.6892 WER(1gram): 88.58% (n=64) time: 7.726
2026-01-04 00:35:54,539: WER lens: avg_true_words=6.16 avg_pred_words=5.83 max_pred_words=12
2026-01-04 00:35:54,539: t15.2023.08.13 val PER: 0.4667
2026-01-04 00:35:54,539: t15.2023.08.18 val PER: 0.4468
2026-01-04 00:35:54,539: t15.2023.08.20 val PER: 0.4392
2026-01-04 00:35:54,539: t15.2023.08.25 val PER: 0.4473
2026-01-04 00:35:54,539: t15.2023.08.27 val PER: 0.5193
2026-01-04 00:35:54,539: t15.2023.09.01 val PER: 0.4245
2026-01-04 00:35:54,539: t15.2023.09.03 val PER: 0.4988
2026-01-04 00:35:54,539: t15.2023.09.24 val PER: 0.4223
2026-01-04 00:35:54,539: t15.2023.09.29 val PER: 0.4614
2026-01-04 00:35:54,540: t15.2023.10.01 val PER: 0.5178
2026-01-04 00:35:54,540: t15.2023.10.06 val PER: 0.4316
2026-01-04 00:35:54,540: t15.2023.10.08 val PER: 0.5304
2026-01-04 00:35:54,540: t15.2023.10.13 val PER: 0.5578
2026-01-04 00:35:54,540: t15.2023.10.15 val PER: 0.4924
2026-01-04 00:35:54,540: t15.2023.10.20 val PER: 0.4530
2026-01-04 00:35:54,540: t15.2023.10.22 val PER: 0.4443
2026-01-04 00:35:54,540: t15.2023.11.03 val PER: 0.5068
2026-01-04 00:35:54,540: t15.2023.11.04 val PER: 0.2594
2026-01-04 00:35:54,540: t15.2023.11.17 val PER: 0.3655
2026-01-04 00:35:54,541: t15.2023.11.19 val PER: 0.3273
2026-01-04 00:35:54,541: t15.2023.11.26 val PER: 0.5500
2026-01-04 00:35:54,541: t15.2023.12.03 val PER: 0.4937
2026-01-04 00:35:54,541: t15.2023.12.08 val PER: 0.5113
2026-01-04 00:35:54,541: t15.2023.12.10 val PER: 0.4455
2026-01-04 00:35:54,541: t15.2023.12.17 val PER: 0.5676
2026-01-04 00:35:54,541: t15.2023.12.29 val PER: 0.5429
2026-01-04 00:35:54,541: t15.2024.02.25 val PER: 0.4621
2026-01-04 00:35:54,541: t15.2024.03.08 val PER: 0.6316
2026-01-04 00:35:54,541: t15.2024.03.15 val PER: 0.5522
2026-01-04 00:35:54,541: t15.2024.03.17 val PER: 0.5098
2026-01-04 00:35:54,541: t15.2024.05.10 val PER: 0.5349
2026-01-04 00:35:54,542: t15.2024.06.14 val PER: 0.5126
2026-01-04 00:35:54,542: t15.2024.07.19 val PER: 0.6711
2026-01-04 00:35:54,542: t15.2024.07.21 val PER: 0.4697
2026-01-04 00:35:54,542: t15.2024.07.28 val PER: 0.5029
2026-01-04 00:35:54,542: t15.2025.01.10 val PER: 0.7410
2026-01-04 00:35:54,542: t15.2025.01.12 val PER: 0.5527
2026-01-04 00:35:54,542: t15.2025.03.14 val PER: 0.7618
2026-01-04 00:35:54,542: t15.2025.03.16 val PER: 0.5825
2026-01-04 00:35:54,542: t15.2025.03.30 val PER: 0.7322
2026-01-04 00:35:54,542: t15.2025.04.13 val PER: 0.5621
2026-01-04 00:35:54,543: New best val WER(1gram) 100.00% --> 88.58%
2026-01-04 00:35:54,543: Checkpointing model
2026-01-04 00:35:55,163: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/best_checkpoint
2026-01-04 00:35:55,408: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_500
2026-01-04 00:36:04,329: Train batch 600: loss: 48.84 grad norm: 80.20 time: 0.078
2026-01-04 00:36:21,732: Train batch 800: loss: 40.22 grad norm: 83.25 time: 0.057
2026-01-04 00:36:39,397: Train batch 1000: loss: 42.27 grad norm: 77.50 time: 0.067
2026-01-04 00:36:39,397: Running test after training batch: 1000
2026-01-04 00:36:39,529: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:36:44,240: WER debug example
  GT : you can see the code at this point as well
  PR : used end ease thus code it this royd is will
2026-01-04 00:36:44,271: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke that wass
2026-01-04 00:36:46,127: Val batch 1000: PER (avg): 0.4088 CTC Loss (avg): 42.2336 WER(1gram): 81.73% (n=64) time: 6.729
2026-01-04 00:36:46,127: WER lens: avg_true_words=6.16 avg_pred_words=5.47 max_pred_words=11
2026-01-04 00:36:46,127: t15.2023.08.13 val PER: 0.3711
2026-01-04 00:36:46,127: t15.2023.08.18 val PER: 0.3428
2026-01-04 00:36:46,127: t15.2023.08.20 val PER: 0.3503
2026-01-04 00:36:46,127: t15.2023.08.25 val PER: 0.2982
2026-01-04 00:36:46,127: t15.2023.08.27 val PER: 0.4309
2026-01-04 00:36:46,127: t15.2023.09.01 val PER: 0.3044
2026-01-04 00:36:46,127: t15.2023.09.03 val PER: 0.3943
2026-01-04 00:36:46,128: t15.2023.09.24 val PER: 0.3362
2026-01-04 00:36:46,128: t15.2023.09.29 val PER: 0.3618
2026-01-04 00:36:46,128: t15.2023.10.01 val PER: 0.4009
2026-01-04 00:36:46,128: t15.2023.10.06 val PER: 0.3175
2026-01-04 00:36:46,128: t15.2023.10.08 val PER: 0.4574
2026-01-04 00:36:46,128: t15.2023.10.13 val PER: 0.4655
2026-01-04 00:36:46,128: t15.2023.10.15 val PER: 0.3757
2026-01-04 00:36:46,128: t15.2023.10.20 val PER: 0.3591
2026-01-04 00:36:46,128: t15.2023.10.22 val PER: 0.3530
2026-01-04 00:36:46,128: t15.2023.11.03 val PER: 0.3969
2026-01-04 00:36:46,128: t15.2023.11.04 val PER: 0.1672
2026-01-04 00:36:46,128: t15.2023.11.17 val PER: 0.2628
2026-01-04 00:36:46,128: t15.2023.11.19 val PER: 0.2176
2026-01-04 00:36:46,128: t15.2023.11.26 val PER: 0.4500
2026-01-04 00:36:46,129: t15.2023.12.03 val PER: 0.3960
2026-01-04 00:36:46,129: t15.2023.12.08 val PER: 0.4021
2026-01-04 00:36:46,129: t15.2023.12.10 val PER: 0.3430
2026-01-04 00:36:46,129: t15.2023.12.17 val PER: 0.4241
2026-01-04 00:36:46,129: t15.2023.12.29 val PER: 0.4001
2026-01-04 00:36:46,129: t15.2024.02.25 val PER: 0.3525
2026-01-04 00:36:46,129: t15.2024.03.08 val PER: 0.4908
2026-01-04 00:36:46,129: t15.2024.03.15 val PER: 0.4396
2026-01-04 00:36:46,129: t15.2024.03.17 val PER: 0.3996
2026-01-04 00:36:46,129: t15.2024.05.10 val PER: 0.4220
2026-01-04 00:36:46,129: t15.2024.06.14 val PER: 0.3927
2026-01-04 00:36:46,129: t15.2024.07.19 val PER: 0.5366
2026-01-04 00:36:46,129: t15.2024.07.21 val PER: 0.3793
2026-01-04 00:36:46,129: t15.2024.07.28 val PER: 0.4191
2026-01-04 00:36:46,129: t15.2025.01.10 val PER: 0.6102
2026-01-04 00:36:46,129: t15.2025.01.12 val PER: 0.4511
2026-01-04 00:36:46,129: t15.2025.03.14 val PER: 0.6435
2026-01-04 00:36:46,130: t15.2025.03.16 val PER: 0.4882
2026-01-04 00:36:46,130: t15.2025.03.30 val PER: 0.6529
2026-01-04 00:36:46,130: t15.2025.04.13 val PER: 0.5007
2026-01-04 00:36:46,131: New best val WER(1gram) 88.58% --> 81.73%
2026-01-04 00:36:46,131: Checkpointing model
2026-01-04 00:36:46,728: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/best_checkpoint
2026-01-04 00:36:46,974: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_1000
2026-01-04 00:37:04,316: Train batch 1200: loss: 32.70 grad norm: 72.76 time: 0.068
2026-01-04 00:37:21,833: Train batch 1400: loss: 35.78 grad norm: 86.85 time: 0.060
2026-01-04 00:37:30,661: Running test after training batch: 1500
2026-01-04 00:37:30,765: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:37:35,530: WER debug example
  GT : you can see the code at this point as well
  PR : yule id e the good it this boyde is will
2026-01-04 00:37:35,562: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that us
2026-01-04 00:37:37,174: Val batch 1500: PER (avg): 0.3805 CTC Loss (avg): 37.1600 WER(1gram): 76.40% (n=64) time: 6.513
2026-01-04 00:37:37,174: WER lens: avg_true_words=6.16 avg_pred_words=5.00 max_pred_words=11
2026-01-04 00:37:37,175: t15.2023.08.13 val PER: 0.3410
2026-01-04 00:37:37,175: t15.2023.08.18 val PER: 0.3018
2026-01-04 00:37:37,175: t15.2023.08.20 val PER: 0.3034
2026-01-04 00:37:37,175: t15.2023.08.25 val PER: 0.2560
2026-01-04 00:37:37,175: t15.2023.08.27 val PER: 0.3875
2026-01-04 00:37:37,175: t15.2023.09.01 val PER: 0.2719
2026-01-04 00:37:37,175: t15.2023.09.03 val PER: 0.3634
2026-01-04 00:37:37,175: t15.2023.09.24 val PER: 0.3119
2026-01-04 00:37:37,175: t15.2023.09.29 val PER: 0.3414
2026-01-04 00:37:37,175: t15.2023.10.01 val PER: 0.3864
2026-01-04 00:37:37,175: t15.2023.10.06 val PER: 0.2853
2026-01-04 00:37:37,175: t15.2023.10.08 val PER: 0.4249
2026-01-04 00:37:37,176: t15.2023.10.13 val PER: 0.4476
2026-01-04 00:37:37,176: t15.2023.10.15 val PER: 0.3593
2026-01-04 00:37:37,176: t15.2023.10.20 val PER: 0.3188
2026-01-04 00:37:37,176: t15.2023.10.22 val PER: 0.3151
2026-01-04 00:37:37,176: t15.2023.11.03 val PER: 0.3616
2026-01-04 00:37:37,176: t15.2023.11.04 val PER: 0.1126
2026-01-04 00:37:37,176: t15.2023.11.17 val PER: 0.2193
2026-01-04 00:37:37,176: t15.2023.11.19 val PER: 0.1796
2026-01-04 00:37:37,176: t15.2023.11.26 val PER: 0.4167
2026-01-04 00:37:37,176: t15.2023.12.03 val PER: 0.3634
2026-01-04 00:37:37,176: t15.2023.12.08 val PER: 0.3582
2026-01-04 00:37:37,176: t15.2023.12.10 val PER: 0.3154
2026-01-04 00:37:37,176: t15.2023.12.17 val PER: 0.3711
2026-01-04 00:37:37,176: t15.2023.12.29 val PER: 0.3699
2026-01-04 00:37:37,176: t15.2024.02.25 val PER: 0.3020
2026-01-04 00:37:37,177: t15.2024.03.08 val PER: 0.4680
2026-01-04 00:37:37,177: t15.2024.03.15 val PER: 0.4115
2026-01-04 00:37:37,177: t15.2024.03.17 val PER: 0.3821
2026-01-04 00:37:37,177: t15.2024.05.10 val PER: 0.4056
2026-01-04 00:37:37,177: t15.2024.06.14 val PER: 0.4164
2026-01-04 00:37:37,177: t15.2024.07.19 val PER: 0.5241
2026-01-04 00:37:37,177: t15.2024.07.21 val PER: 0.3490
2026-01-04 00:37:37,177: t15.2024.07.28 val PER: 0.3721
2026-01-04 00:37:37,177: t15.2025.01.10 val PER: 0.6267
2026-01-04 00:37:37,177: t15.2025.01.12 val PER: 0.4426
2026-01-04 00:37:37,177: t15.2025.03.14 val PER: 0.6065
2026-01-04 00:37:37,177: t15.2025.03.16 val PER: 0.4542
2026-01-04 00:37:37,177: t15.2025.03.30 val PER: 0.6494
2026-01-04 00:37:37,177: t15.2025.04.13 val PER: 0.4736
2026-01-04 00:37:37,178: New best val WER(1gram) 81.73% --> 76.40%
2026-01-04 00:37:37,178: Checkpointing model
2026-01-04 00:37:37,815: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/best_checkpoint
2026-01-04 00:37:38,059: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_1500
2026-01-04 00:37:46,706: Train batch 1600: loss: 36.64 grad norm: 78.57 time: 0.064
2026-01-04 00:38:04,343: Train batch 1800: loss: 35.15 grad norm: 73.21 time: 0.088
2026-01-04 00:38:21,995: Train batch 2000: loss: 33.11 grad norm: 71.84 time: 0.066
2026-01-04 00:38:21,996: Running test after training batch: 2000
2026-01-04 00:38:22,123: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:38:26,799: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this bonde is wheel
2026-01-04 00:38:26,829: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the us it
2026-01-04 00:38:28,371: Val batch 2000: PER (avg): 0.3244 CTC Loss (avg): 32.4075 WER(1gram): 70.05% (n=64) time: 6.375
2026-01-04 00:38:28,371: WER lens: avg_true_words=6.16 avg_pred_words=5.48 max_pred_words=11
2026-01-04 00:38:28,371: t15.2023.08.13 val PER: 0.3056
2026-01-04 00:38:28,371: t15.2023.08.18 val PER: 0.2498
2026-01-04 00:38:28,371: t15.2023.08.20 val PER: 0.2558
2026-01-04 00:38:28,371: t15.2023.08.25 val PER: 0.2349
2026-01-04 00:38:28,371: t15.2023.08.27 val PER: 0.3553
2026-01-04 00:38:28,372: t15.2023.09.01 val PER: 0.2208
2026-01-04 00:38:28,372: t15.2023.09.03 val PER: 0.3171
2026-01-04 00:38:28,372: t15.2023.09.24 val PER: 0.2694
2026-01-04 00:38:28,372: t15.2023.09.29 val PER: 0.2782
2026-01-04 00:38:28,372: t15.2023.10.01 val PER: 0.3223
2026-01-04 00:38:28,372: t15.2023.10.06 val PER: 0.2260
2026-01-04 00:38:28,372: t15.2023.10.08 val PER: 0.3843
2026-01-04 00:38:28,372: t15.2023.10.13 val PER: 0.3801
2026-01-04 00:38:28,372: t15.2023.10.15 val PER: 0.2927
2026-01-04 00:38:28,372: t15.2023.10.20 val PER: 0.2852
2026-01-04 00:38:28,372: t15.2023.10.22 val PER: 0.2494
2026-01-04 00:38:28,372: t15.2023.11.03 val PER: 0.3148
2026-01-04 00:38:28,372: t15.2023.11.04 val PER: 0.0956
2026-01-04 00:38:28,372: t15.2023.11.17 val PER: 0.1773
2026-01-04 00:38:28,373: t15.2023.11.19 val PER: 0.1397
2026-01-04 00:38:28,373: t15.2023.11.26 val PER: 0.3580
2026-01-04 00:38:28,373: t15.2023.12.03 val PER: 0.3088
2026-01-04 00:38:28,373: t15.2023.12.08 val PER: 0.3043
2026-01-04 00:38:28,373: t15.2023.12.10 val PER: 0.2615
2026-01-04 00:38:28,373: t15.2023.12.17 val PER: 0.3285
2026-01-04 00:38:28,373: t15.2023.12.29 val PER: 0.3102
2026-01-04 00:38:28,373: t15.2024.02.25 val PER: 0.2837
2026-01-04 00:38:28,373: t15.2024.03.08 val PER: 0.3898
2026-01-04 00:38:28,373: t15.2024.03.15 val PER: 0.3558
2026-01-04 00:38:28,373: t15.2024.03.17 val PER: 0.3340
2026-01-04 00:38:28,373: t15.2024.05.10 val PER: 0.3343
2026-01-04 00:38:28,374: t15.2024.06.14 val PER: 0.3454
2026-01-04 00:38:28,374: t15.2024.07.19 val PER: 0.4621
2026-01-04 00:38:28,374: t15.2024.07.21 val PER: 0.2910
2026-01-04 00:38:28,374: t15.2024.07.28 val PER: 0.3162
2026-01-04 00:38:28,374: t15.2025.01.10 val PER: 0.5427
2026-01-04 00:38:28,374: t15.2025.01.12 val PER: 0.3857
2026-01-04 00:38:28,374: t15.2025.03.14 val PER: 0.5325
2026-01-04 00:38:28,374: t15.2025.03.16 val PER: 0.3822
2026-01-04 00:38:28,374: t15.2025.03.30 val PER: 0.5494
2026-01-04 00:38:28,374: t15.2025.04.13 val PER: 0.4009
2026-01-04 00:38:28,375: New best val WER(1gram) 76.40% --> 70.05%
2026-01-04 00:38:28,375: Checkpointing model
2026-01-04 00:38:28,975: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/best_checkpoint
2026-01-04 00:38:29,220: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_2000
2026-01-04 00:38:46,363: Train batch 2200: loss: 28.91 grad norm: 73.53 time: 0.060
2026-01-04 00:39:03,522: Train batch 2400: loss: 28.77 grad norm: 62.19 time: 0.052
2026-01-04 00:39:12,125: Running test after training batch: 2500
2026-01-04 00:39:12,229: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:39:16,954: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the good at this boyte is will
2026-01-04 00:39:16,985: WER debug example
  GT : how does it keep the cost down
  PR : houde des it eke thus cost get
2026-01-04 00:39:18,593: Val batch 2500: PER (avg): 0.3002 CTC Loss (avg): 29.9874 WER(1gram): 68.53% (n=64) time: 6.467
2026-01-04 00:39:18,593: WER lens: avg_true_words=6.16 avg_pred_words=5.59 max_pred_words=11
2026-01-04 00:39:18,594: t15.2023.08.13 val PER: 0.2848
2026-01-04 00:39:18,594: t15.2023.08.18 val PER: 0.2431
2026-01-04 00:39:18,594: t15.2023.08.20 val PER: 0.2327
2026-01-04 00:39:18,594: t15.2023.08.25 val PER: 0.2108
2026-01-04 00:39:18,594: t15.2023.08.27 val PER: 0.3151
2026-01-04 00:39:18,594: t15.2023.09.01 val PER: 0.2005
2026-01-04 00:39:18,594: t15.2023.09.03 val PER: 0.2933
2026-01-04 00:39:18,594: t15.2023.09.24 val PER: 0.2245
2026-01-04 00:39:18,594: t15.2023.09.29 val PER: 0.2502
2026-01-04 00:39:18,594: t15.2023.10.01 val PER: 0.3071
2026-01-04 00:39:18,594: t15.2023.10.06 val PER: 0.2110
2026-01-04 00:39:18,595: t15.2023.10.08 val PER: 0.3613
2026-01-04 00:39:18,595: t15.2023.10.13 val PER: 0.3483
2026-01-04 00:39:18,595: t15.2023.10.15 val PER: 0.2874
2026-01-04 00:39:18,595: t15.2023.10.20 val PER: 0.2752
2026-01-04 00:39:18,595: t15.2023.10.22 val PER: 0.2372
2026-01-04 00:39:18,595: t15.2023.11.03 val PER: 0.2904
2026-01-04 00:39:18,595: t15.2023.11.04 val PER: 0.0819
2026-01-04 00:39:18,595: t15.2023.11.17 val PER: 0.1477
2026-01-04 00:39:18,595: t15.2023.11.19 val PER: 0.1238
2026-01-04 00:39:18,595: t15.2023.11.26 val PER: 0.3391
2026-01-04 00:39:18,596: t15.2023.12.03 val PER: 0.2731
2026-01-04 00:39:18,596: t15.2023.12.08 val PER: 0.2730
2026-01-04 00:39:18,596: t15.2023.12.10 val PER: 0.2313
2026-01-04 00:39:18,596: t15.2023.12.17 val PER: 0.2755
2026-01-04 00:39:18,596: t15.2023.12.29 val PER: 0.2931
2026-01-04 00:39:18,596: t15.2024.02.25 val PER: 0.2346
2026-01-04 00:39:18,596: t15.2024.03.08 val PER: 0.3599
2026-01-04 00:39:18,596: t15.2024.03.15 val PER: 0.3446
2026-01-04 00:39:18,596: t15.2024.03.17 val PER: 0.3054
2026-01-04 00:39:18,596: t15.2024.05.10 val PER: 0.3076
2026-01-04 00:39:18,596: t15.2024.06.14 val PER: 0.3312
2026-01-04 00:39:18,596: t15.2024.07.19 val PER: 0.4509
2026-01-04 00:39:18,596: t15.2024.07.21 val PER: 0.2517
2026-01-04 00:39:18,596: t15.2024.07.28 val PER: 0.2963
2026-01-04 00:39:18,596: t15.2025.01.10 val PER: 0.5014
2026-01-04 00:39:18,596: t15.2025.01.12 val PER: 0.3641
2026-01-04 00:39:18,597: t15.2025.03.14 val PER: 0.5059
2026-01-04 00:39:18,597: t15.2025.03.16 val PER: 0.3730
2026-01-04 00:39:18,597: t15.2025.03.30 val PER: 0.4954
2026-01-04 00:39:18,597: t15.2025.04.13 val PER: 0.3795
2026-01-04 00:39:18,598: New best val WER(1gram) 70.05% --> 68.53%
2026-01-04 00:39:18,598: Checkpointing model
2026-01-04 00:39:19,225: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/best_checkpoint
2026-01-04 00:39:19,468: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_2500
2026-01-04 00:39:28,059: Train batch 2600: loss: 34.19 grad norm: 82.82 time: 0.055
2026-01-04 00:39:45,506: Train batch 2800: loss: 25.91 grad norm: 72.47 time: 0.081
2026-01-04 00:40:02,741: Train batch 3000: loss: 30.61 grad norm: 71.74 time: 0.082
2026-01-04 00:40:02,741: Running test after training batch: 3000
2026-01-04 00:40:02,852: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:40:07,750: WER debug example
  GT : you can see the code at this point as well
  PR : yule end sheik the could at this point is will
2026-01-04 00:40:07,779: WER debug example
  GT : how does it keep the cost down
  PR : houde des it kipp the rost nett
2026-01-04 00:40:09,398: Val batch 3000: PER (avg): 0.2811 CTC Loss (avg): 27.7167 WER(1gram): 68.02% (n=64) time: 6.657
2026-01-04 00:40:09,399: WER lens: avg_true_words=6.16 avg_pred_words=5.86 max_pred_words=11
2026-01-04 00:40:09,399: t15.2023.08.13 val PER: 0.2692
2026-01-04 00:40:09,399: t15.2023.08.18 val PER: 0.2280
2026-01-04 00:40:09,399: t15.2023.08.20 val PER: 0.2105
2026-01-04 00:40:09,399: t15.2023.08.25 val PER: 0.2048
2026-01-04 00:40:09,399: t15.2023.08.27 val PER: 0.3103
2026-01-04 00:40:09,399: t15.2023.09.01 val PER: 0.2013
2026-01-04 00:40:09,399: t15.2023.09.03 val PER: 0.2815
2026-01-04 00:40:09,399: t15.2023.09.24 val PER: 0.2087
2026-01-04 00:40:09,399: t15.2023.09.29 val PER: 0.2323
2026-01-04 00:40:09,399: t15.2023.10.01 val PER: 0.2926
2026-01-04 00:40:09,400: t15.2023.10.06 val PER: 0.1938
2026-01-04 00:40:09,400: t15.2023.10.08 val PER: 0.3491
2026-01-04 00:40:09,400: t15.2023.10.13 val PER: 0.3344
2026-01-04 00:40:09,400: t15.2023.10.15 val PER: 0.2604
2026-01-04 00:40:09,400: t15.2023.10.20 val PER: 0.2584
2026-01-04 00:40:09,400: t15.2023.10.22 val PER: 0.2105
2026-01-04 00:40:09,400: t15.2023.11.03 val PER: 0.2632
2026-01-04 00:40:09,400: t15.2023.11.04 val PER: 0.0751
2026-01-04 00:40:09,400: t15.2023.11.17 val PER: 0.1322
2026-01-04 00:40:09,400: t15.2023.11.19 val PER: 0.1158
2026-01-04 00:40:09,400: t15.2023.11.26 val PER: 0.3007
2026-01-04 00:40:09,400: t15.2023.12.03 val PER: 0.2668
2026-01-04 00:40:09,400: t15.2023.12.08 val PER: 0.2610
2026-01-04 00:40:09,400: t15.2023.12.10 val PER: 0.2063
2026-01-04 00:40:09,401: t15.2023.12.17 val PER: 0.2703
2026-01-04 00:40:09,401: t15.2023.12.29 val PER: 0.2745
2026-01-04 00:40:09,401: t15.2024.02.25 val PER: 0.2317
2026-01-04 00:40:09,401: t15.2024.03.08 val PER: 0.3627
2026-01-04 00:40:09,401: t15.2024.03.15 val PER: 0.3358
2026-01-04 00:40:09,401: t15.2024.03.17 val PER: 0.2838
2026-01-04 00:40:09,401: t15.2024.05.10 val PER: 0.2912
2026-01-04 00:40:09,401: t15.2024.06.14 val PER: 0.3060
2026-01-04 00:40:09,401: t15.2024.07.19 val PER: 0.3982
2026-01-04 00:40:09,401: t15.2024.07.21 val PER: 0.2241
2026-01-04 00:40:09,401: t15.2024.07.28 val PER: 0.2794
2026-01-04 00:40:09,401: t15.2025.01.10 val PER: 0.4931
2026-01-04 00:40:09,401: t15.2025.01.12 val PER: 0.3318
2026-01-04 00:40:09,401: t15.2025.03.14 val PER: 0.4467
2026-01-04 00:40:09,401: t15.2025.03.16 val PER: 0.3325
2026-01-04 00:40:09,401: t15.2025.03.30 val PER: 0.4977
2026-01-04 00:40:09,401: t15.2025.04.13 val PER: 0.3466
2026-01-04 00:40:09,403: New best val WER(1gram) 68.53% --> 68.02%
2026-01-04 00:40:09,403: Checkpointing model
2026-01-04 00:40:09,994: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/best_checkpoint
2026-01-04 00:40:10,238: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_3000
2026-01-04 00:40:27,944: Train batch 3200: loss: 26.16 grad norm: 67.62 time: 0.075
2026-01-04 00:40:45,158: Train batch 3400: loss: 18.30 grad norm: 56.80 time: 0.048
2026-01-04 00:40:53,975: Running test after training batch: 3500
2026-01-04 00:40:54,122: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:40:58,817: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 00:40:58,846: WER debug example
  GT : how does it keep the cost down
  PR : houde des it kipp the cussed get
2026-01-04 00:41:00,380: Val batch 3500: PER (avg): 0.2659 CTC Loss (avg): 26.3106 WER(1gram): 66.50% (n=64) time: 6.405
2026-01-04 00:41:00,381: WER lens: avg_true_words=6.16 avg_pred_words=6.00 max_pred_words=11
2026-01-04 00:41:00,381: t15.2023.08.13 val PER: 0.2432
2026-01-04 00:41:00,381: t15.2023.08.18 val PER: 0.2137
2026-01-04 00:41:00,381: t15.2023.08.20 val PER: 0.2248
2026-01-04 00:41:00,381: t15.2023.08.25 val PER: 0.1687
2026-01-04 00:41:00,381: t15.2023.08.27 val PER: 0.2653
2026-01-04 00:41:00,381: t15.2023.09.01 val PER: 0.1802
2026-01-04 00:41:00,381: t15.2023.09.03 val PER: 0.2435
2026-01-04 00:41:00,381: t15.2023.09.24 val PER: 0.1954
2026-01-04 00:41:00,381: t15.2023.09.29 val PER: 0.2131
2026-01-04 00:41:00,381: t15.2023.10.01 val PER: 0.2741
2026-01-04 00:41:00,381: t15.2023.10.06 val PER: 0.1798
2026-01-04 00:41:00,382: t15.2023.10.08 val PER: 0.3383
2026-01-04 00:41:00,382: t15.2023.10.13 val PER: 0.3126
2026-01-04 00:41:00,382: t15.2023.10.15 val PER: 0.2465
2026-01-04 00:41:00,382: t15.2023.10.20 val PER: 0.2315
2026-01-04 00:41:00,382: t15.2023.10.22 val PER: 0.2038
2026-01-04 00:41:00,382: t15.2023.11.03 val PER: 0.2646
2026-01-04 00:41:00,382: t15.2023.11.04 val PER: 0.0717
2026-01-04 00:41:00,382: t15.2023.11.17 val PER: 0.1198
2026-01-04 00:41:00,382: t15.2023.11.19 val PER: 0.0978
2026-01-04 00:41:00,382: t15.2023.11.26 val PER: 0.2754
2026-01-04 00:41:00,382: t15.2023.12.03 val PER: 0.2458
2026-01-04 00:41:00,382: t15.2023.12.08 val PER: 0.2410
2026-01-04 00:41:00,382: t15.2023.12.10 val PER: 0.2155
2026-01-04 00:41:00,382: t15.2023.12.17 val PER: 0.2588
2026-01-04 00:41:00,382: t15.2023.12.29 val PER: 0.2656
2026-01-04 00:41:00,382: t15.2024.02.25 val PER: 0.2149
2026-01-04 00:41:00,383: t15.2024.03.08 val PER: 0.3329
2026-01-04 00:41:00,383: t15.2024.03.15 val PER: 0.3208
2026-01-04 00:41:00,383: t15.2024.03.17 val PER: 0.2894
2026-01-04 00:41:00,383: t15.2024.05.10 val PER: 0.2749
2026-01-04 00:41:00,383: t15.2024.06.14 val PER: 0.2823
2026-01-04 00:41:00,383: t15.2024.07.19 val PER: 0.3929
2026-01-04 00:41:00,383: t15.2024.07.21 val PER: 0.2200
2026-01-04 00:41:00,383: t15.2024.07.28 val PER: 0.2787
2026-01-04 00:41:00,384: t15.2025.01.10 val PER: 0.4669
2026-01-04 00:41:00,384: t15.2025.01.12 val PER: 0.2925
2026-01-04 00:41:00,384: t15.2025.03.14 val PER: 0.4482
2026-01-04 00:41:00,384: t15.2025.03.16 val PER: 0.3181
2026-01-04 00:41:00,384: t15.2025.03.30 val PER: 0.4552
2026-01-04 00:41:00,384: t15.2025.04.13 val PER: 0.3252
2026-01-04 00:41:00,385: New best val WER(1gram) 68.02% --> 66.50%
2026-01-04 00:41:00,385: Checkpointing model
2026-01-04 00:41:01,007: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/best_checkpoint
2026-01-04 00:41:01,253: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_3500
2026-01-04 00:41:09,700: Train batch 3600: loss: 21.90 grad norm: 62.55 time: 0.066
2026-01-04 00:41:26,499: Train batch 3800: loss: 25.02 grad norm: 66.16 time: 0.066
2026-01-04 00:41:43,470: Train batch 4000: loss: 19.22 grad norm: 57.62 time: 0.056
2026-01-04 00:41:43,471: Running test after training batch: 4000
2026-01-04 00:41:43,618: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:41:48,302: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this boyte is will
2026-01-04 00:41:48,331: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nett
2026-01-04 00:41:49,900: Val batch 4000: PER (avg): 0.2483 CTC Loss (avg): 24.2139 WER(1gram): 64.21% (n=64) time: 6.429
2026-01-04 00:41:49,901: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-04 00:41:49,901: t15.2023.08.13 val PER: 0.2131
2026-01-04 00:41:49,901: t15.2023.08.18 val PER: 0.2003
2026-01-04 00:41:49,901: t15.2023.08.20 val PER: 0.2057
2026-01-04 00:41:49,901: t15.2023.08.25 val PER: 0.1551
2026-01-04 00:41:49,901: t15.2023.08.27 val PER: 0.2830
2026-01-04 00:41:49,901: t15.2023.09.01 val PER: 0.1534
2026-01-04 00:41:49,901: t15.2023.09.03 val PER: 0.2458
2026-01-04 00:41:49,901: t15.2023.09.24 val PER: 0.1857
2026-01-04 00:41:49,901: t15.2023.09.29 val PER: 0.2010
2026-01-04 00:41:49,901: t15.2023.10.01 val PER: 0.2649
2026-01-04 00:41:49,901: t15.2023.10.06 val PER: 0.1690
2026-01-04 00:41:49,902: t15.2023.10.08 val PER: 0.3072
2026-01-04 00:41:49,902: t15.2023.10.13 val PER: 0.3111
2026-01-04 00:41:49,902: t15.2023.10.15 val PER: 0.2413
2026-01-04 00:41:49,902: t15.2023.10.20 val PER: 0.2148
2026-01-04 00:41:49,902: t15.2023.10.22 val PER: 0.1871
2026-01-04 00:41:49,902: t15.2023.11.03 val PER: 0.2436
2026-01-04 00:41:49,902: t15.2023.11.04 val PER: 0.0717
2026-01-04 00:41:49,902: t15.2023.11.17 val PER: 0.1058
2026-01-04 00:41:49,902: t15.2023.11.19 val PER: 0.1018
2026-01-04 00:41:49,902: t15.2023.11.26 val PER: 0.2601
2026-01-04 00:41:49,902: t15.2023.12.03 val PER: 0.2090
2026-01-04 00:41:49,902: t15.2023.12.08 val PER: 0.2204
2026-01-04 00:41:49,902: t15.2023.12.10 val PER: 0.1748
2026-01-04 00:41:49,902: t15.2023.12.17 val PER: 0.2391
2026-01-04 00:41:49,902: t15.2023.12.29 val PER: 0.2588
2026-01-04 00:41:49,902: t15.2024.02.25 val PER: 0.2149
2026-01-04 00:41:49,903: t15.2024.03.08 val PER: 0.3286
2026-01-04 00:41:49,903: t15.2024.03.15 val PER: 0.3002
2026-01-04 00:41:49,903: t15.2024.03.17 val PER: 0.2566
2026-01-04 00:41:49,903: t15.2024.05.10 val PER: 0.2660
2026-01-04 00:41:49,903: t15.2024.06.14 val PER: 0.2713
2026-01-04 00:41:49,903: t15.2024.07.19 val PER: 0.3698
2026-01-04 00:41:49,903: t15.2024.07.21 val PER: 0.1903
2026-01-04 00:41:49,903: t15.2024.07.28 val PER: 0.2419
2026-01-04 00:41:49,903: t15.2025.01.10 val PER: 0.4311
2026-01-04 00:41:49,903: t15.2025.01.12 val PER: 0.2717
2026-01-04 00:41:49,903: t15.2025.03.14 val PER: 0.4290
2026-01-04 00:41:49,903: t15.2025.03.16 val PER: 0.3010
2026-01-04 00:41:49,903: t15.2025.03.30 val PER: 0.4115
2026-01-04 00:41:49,903: t15.2025.04.13 val PER: 0.3224
2026-01-04 00:41:49,904: New best val WER(1gram) 66.50% --> 64.21%
2026-01-04 00:41:49,905: Checkpointing model
2026-01-04 00:41:50,502: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/best_checkpoint
2026-01-04 00:41:50,746: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_4000
2026-01-04 00:42:07,849: Train batch 4200: loss: 22.77 grad norm: 66.89 time: 0.078
2026-01-04 00:42:25,114: Train batch 4400: loss: 16.65 grad norm: 56.31 time: 0.065
2026-01-04 00:42:33,749: Running test after training batch: 4500
2026-01-04 00:42:33,843: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:42:38,644: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point as will
2026-01-04 00:42:38,674: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap thus cost get
2026-01-04 00:42:40,251: Val batch 4500: PER (avg): 0.2349 CTC Loss (avg): 23.0327 WER(1gram): 61.68% (n=64) time: 6.502
2026-01-04 00:42:40,252: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 00:42:40,252: t15.2023.08.13 val PER: 0.2006
2026-01-04 00:42:40,252: t15.2023.08.18 val PER: 0.1735
2026-01-04 00:42:40,252: t15.2023.08.20 val PER: 0.1811
2026-01-04 00:42:40,252: t15.2023.08.25 val PER: 0.1416
2026-01-04 00:42:40,252: t15.2023.08.27 val PER: 0.2492
2026-01-04 00:42:40,252: t15.2023.09.01 val PER: 0.1412
2026-01-04 00:42:40,252: t15.2023.09.03 val PER: 0.2363
2026-01-04 00:42:40,252: t15.2023.09.24 val PER: 0.1820
2026-01-04 00:42:40,252: t15.2023.09.29 val PER: 0.1953
2026-01-04 00:42:40,253: t15.2023.10.01 val PER: 0.2536
2026-01-04 00:42:40,253: t15.2023.10.06 val PER: 0.1507
2026-01-04 00:42:40,253: t15.2023.10.08 val PER: 0.3180
2026-01-04 00:42:40,253: t15.2023.10.13 val PER: 0.2971
2026-01-04 00:42:40,253: t15.2023.10.15 val PER: 0.2221
2026-01-04 00:42:40,253: t15.2023.10.20 val PER: 0.2114
2026-01-04 00:42:40,253: t15.2023.10.22 val PER: 0.1849
2026-01-04 00:42:40,253: t15.2023.11.03 val PER: 0.2415
2026-01-04 00:42:40,253: t15.2023.11.04 val PER: 0.0648
2026-01-04 00:42:40,253: t15.2023.11.17 val PER: 0.0964
2026-01-04 00:42:40,253: t15.2023.11.19 val PER: 0.0958
2026-01-04 00:42:40,253: t15.2023.11.26 val PER: 0.2674
2026-01-04 00:42:40,253: t15.2023.12.03 val PER: 0.2059
2026-01-04 00:42:40,253: t15.2023.12.08 val PER: 0.2037
2026-01-04 00:42:40,254: t15.2023.12.10 val PER: 0.1800
2026-01-04 00:42:40,254: t15.2023.12.17 val PER: 0.2308
2026-01-04 00:42:40,254: t15.2023.12.29 val PER: 0.2485
2026-01-04 00:42:40,254: t15.2024.02.25 val PER: 0.1896
2026-01-04 00:42:40,254: t15.2024.03.08 val PER: 0.3087
2026-01-04 00:42:40,254: t15.2024.03.15 val PER: 0.2827
2026-01-04 00:42:40,254: t15.2024.03.17 val PER: 0.2392
2026-01-04 00:42:40,254: t15.2024.05.10 val PER: 0.2600
2026-01-04 00:42:40,254: t15.2024.06.14 val PER: 0.2524
2026-01-04 00:42:40,254: t15.2024.07.19 val PER: 0.3428
2026-01-04 00:42:40,254: t15.2024.07.21 val PER: 0.1703
2026-01-04 00:42:40,254: t15.2024.07.28 val PER: 0.2191
2026-01-04 00:42:40,254: t15.2025.01.10 val PER: 0.4077
2026-01-04 00:42:40,254: t15.2025.01.12 val PER: 0.2540
2026-01-04 00:42:40,254: t15.2025.03.14 val PER: 0.4038
2026-01-04 00:42:40,254: t15.2025.03.16 val PER: 0.2997
2026-01-04 00:42:40,254: t15.2025.03.30 val PER: 0.4057
2026-01-04 00:42:40,255: t15.2025.04.13 val PER: 0.2939
2026-01-04 00:42:40,256: New best val WER(1gram) 64.21% --> 61.68%
2026-01-04 00:42:40,256: Checkpointing model
2026-01-04 00:42:40,877: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/best_checkpoint
2026-01-04 00:42:41,121: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_4500
2026-01-04 00:42:49,580: Train batch 4600: loss: 20.03 grad norm: 62.65 time: 0.062
2026-01-04 00:43:06,746: Train batch 4800: loss: 13.72 grad norm: 54.17 time: 0.063
2026-01-04 00:43:23,943: Train batch 5000: loss: 31.30 grad norm: 85.50 time: 0.064
2026-01-04 00:43:23,943: Running test after training batch: 5000
2026-01-04 00:43:24,069: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:43:28,764: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 00:43:28,792: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the cost nett
2026-01-04 00:43:30,412: Val batch 5000: PER (avg): 0.2242 CTC Loss (avg): 21.8110 WER(1gram): 61.17% (n=64) time: 6.469
2026-01-04 00:43:30,413: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 00:43:30,413: t15.2023.08.13 val PER: 0.1965
2026-01-04 00:43:30,413: t15.2023.08.18 val PER: 0.1584
2026-01-04 00:43:30,413: t15.2023.08.20 val PER: 0.1739
2026-01-04 00:43:30,413: t15.2023.08.25 val PER: 0.1310
2026-01-04 00:43:30,413: t15.2023.08.27 val PER: 0.2331
2026-01-04 00:43:30,413: t15.2023.09.01 val PER: 0.1347
2026-01-04 00:43:30,413: t15.2023.09.03 val PER: 0.2221
2026-01-04 00:43:30,413: t15.2023.09.24 val PER: 0.1808
2026-01-04 00:43:30,413: t15.2023.09.29 val PER: 0.1787
2026-01-04 00:43:30,414: t15.2023.10.01 val PER: 0.2365
2026-01-04 00:43:30,414: t15.2023.10.06 val PER: 0.1475
2026-01-04 00:43:30,414: t15.2023.10.08 val PER: 0.3126
2026-01-04 00:43:30,414: t15.2023.10.13 val PER: 0.2808
2026-01-04 00:43:30,414: t15.2023.10.15 val PER: 0.2248
2026-01-04 00:43:30,414: t15.2023.10.20 val PER: 0.2349
2026-01-04 00:43:30,414: t15.2023.10.22 val PER: 0.1726
2026-01-04 00:43:30,414: t15.2023.11.03 val PER: 0.2218
2026-01-04 00:43:30,414: t15.2023.11.04 val PER: 0.0683
2026-01-04 00:43:30,414: t15.2023.11.17 val PER: 0.0886
2026-01-04 00:43:30,414: t15.2023.11.19 val PER: 0.0758
2026-01-04 00:43:30,414: t15.2023.11.26 val PER: 0.2420
2026-01-04 00:43:30,415: t15.2023.12.03 val PER: 0.1954
2026-01-04 00:43:30,415: t15.2023.12.08 val PER: 0.1924
2026-01-04 00:43:30,415: t15.2023.12.10 val PER: 0.1564
2026-01-04 00:43:30,415: t15.2023.12.17 val PER: 0.2318
2026-01-04 00:43:30,415: t15.2023.12.29 val PER: 0.2169
2026-01-04 00:43:30,415: t15.2024.02.25 val PER: 0.1812
2026-01-04 00:43:30,415: t15.2024.03.08 val PER: 0.3030
2026-01-04 00:43:30,415: t15.2024.03.15 val PER: 0.2770
2026-01-04 00:43:30,415: t15.2024.03.17 val PER: 0.2343
2026-01-04 00:43:30,415: t15.2024.05.10 val PER: 0.2526
2026-01-04 00:43:30,415: t15.2024.06.14 val PER: 0.2508
2026-01-04 00:43:30,415: t15.2024.07.19 val PER: 0.3250
2026-01-04 00:43:30,416: t15.2024.07.21 val PER: 0.1731
2026-01-04 00:43:30,416: t15.2024.07.28 val PER: 0.2074
2026-01-04 00:43:30,416: t15.2025.01.10 val PER: 0.3967
2026-01-04 00:43:30,416: t15.2025.01.12 val PER: 0.2402
2026-01-04 00:43:30,416: t15.2025.03.14 val PER: 0.3964
2026-01-04 00:43:30,416: t15.2025.03.16 val PER: 0.2853
2026-01-04 00:43:30,416: t15.2025.03.30 val PER: 0.3862
2026-01-04 00:43:30,416: t15.2025.04.13 val PER: 0.2981
2026-01-04 00:43:30,417: New best val WER(1gram) 61.68% --> 61.17%
2026-01-04 00:43:30,417: Checkpointing model
2026-01-04 00:43:31,011: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/best_checkpoint
2026-01-04 00:43:31,254: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_5000
2026-01-04 00:43:48,619: Train batch 5200: loss: 15.71 grad norm: 54.58 time: 0.051
2026-01-04 00:44:05,692: Train batch 5400: loss: 17.01 grad norm: 58.33 time: 0.068
2026-01-04 00:44:14,416: Running test after training batch: 5500
2026-01-04 00:44:14,566: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:44:19,229: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 00:44:19,258: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost tet
2026-01-04 00:44:20,859: Val batch 5500: PER (avg): 0.2139 CTC Loss (avg): 20.9094 WER(1gram): 55.58% (n=64) time: 6.442
2026-01-04 00:44:20,860: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-04 00:44:20,860: t15.2023.08.13 val PER: 0.1819
2026-01-04 00:44:20,860: t15.2023.08.18 val PER: 0.1559
2026-01-04 00:44:20,860: t15.2023.08.20 val PER: 0.1708
2026-01-04 00:44:20,860: t15.2023.08.25 val PER: 0.1265
2026-01-04 00:44:20,860: t15.2023.08.27 val PER: 0.2363
2026-01-04 00:44:20,860: t15.2023.09.01 val PER: 0.1193
2026-01-04 00:44:20,860: t15.2023.09.03 val PER: 0.2233
2026-01-04 00:44:20,860: t15.2023.09.24 val PER: 0.1723
2026-01-04 00:44:20,861: t15.2023.09.29 val PER: 0.1729
2026-01-04 00:44:20,861: t15.2023.10.01 val PER: 0.2305
2026-01-04 00:44:20,861: t15.2023.10.06 val PER: 0.1313
2026-01-04 00:44:20,861: t15.2023.10.08 val PER: 0.2815
2026-01-04 00:44:20,861: t15.2023.10.13 val PER: 0.2754
2026-01-04 00:44:20,861: t15.2023.10.15 val PER: 0.1997
2026-01-04 00:44:20,861: t15.2023.10.20 val PER: 0.2114
2026-01-04 00:44:20,861: t15.2023.10.22 val PER: 0.1670
2026-01-04 00:44:20,861: t15.2023.11.03 val PER: 0.2198
2026-01-04 00:44:20,861: t15.2023.11.04 val PER: 0.0648
2026-01-04 00:44:20,861: t15.2023.11.17 val PER: 0.0871
2026-01-04 00:44:20,861: t15.2023.11.19 val PER: 0.0679
2026-01-04 00:44:20,861: t15.2023.11.26 val PER: 0.2109
2026-01-04 00:44:20,861: t15.2023.12.03 val PER: 0.1733
2026-01-04 00:44:20,861: t15.2023.12.08 val PER: 0.1791
2026-01-04 00:44:20,862: t15.2023.12.10 val PER: 0.1445
2026-01-04 00:44:20,862: t15.2023.12.17 val PER: 0.2183
2026-01-04 00:44:20,862: t15.2023.12.29 val PER: 0.2183
2026-01-04 00:44:20,862: t15.2024.02.25 val PER: 0.1784
2026-01-04 00:44:20,862: t15.2024.03.08 val PER: 0.2930
2026-01-04 00:44:20,862: t15.2024.03.15 val PER: 0.2639
2026-01-04 00:44:20,862: t15.2024.03.17 val PER: 0.2211
2026-01-04 00:44:20,862: t15.2024.05.10 val PER: 0.2377
2026-01-04 00:44:20,862: t15.2024.06.14 val PER: 0.2429
2026-01-04 00:44:20,862: t15.2024.07.19 val PER: 0.3276
2026-01-04 00:44:20,863: t15.2024.07.21 val PER: 0.1607
2026-01-04 00:44:20,863: t15.2024.07.28 val PER: 0.2103
2026-01-04 00:44:20,863: t15.2025.01.10 val PER: 0.3843
2026-01-04 00:44:20,863: t15.2025.01.12 val PER: 0.2356
2026-01-04 00:44:20,863: t15.2025.03.14 val PER: 0.3521
2026-01-04 00:44:20,863: t15.2025.03.16 val PER: 0.2827
2026-01-04 00:44:20,863: t15.2025.03.30 val PER: 0.3609
2026-01-04 00:44:20,863: t15.2025.04.13 val PER: 0.2825
2026-01-04 00:44:20,864: New best val WER(1gram) 61.17% --> 55.58%
2026-01-04 00:44:20,864: Checkpointing model
2026-01-04 00:44:21,516: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/best_checkpoint
2026-01-04 00:44:21,765: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_5500
2026-01-04 00:44:30,462: Train batch 5600: loss: 19.07 grad norm: 65.79 time: 0.062
2026-01-04 00:44:47,889: Train batch 5800: loss: 13.61 grad norm: 56.09 time: 0.082
2026-01-04 00:45:05,062: Train batch 6000: loss: 14.22 grad norm: 55.64 time: 0.049
2026-01-04 00:45:05,062: Running test after training batch: 6000
2026-01-04 00:45:05,167: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:45:10,029: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-04 00:45:10,060: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 00:45:11,643: Val batch 6000: PER (avg): 0.2112 CTC Loss (avg): 20.6815 WER(1gram): 55.84% (n=64) time: 6.581
2026-01-04 00:45:11,643: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 00:45:11,643: t15.2023.08.13 val PER: 0.1798
2026-01-04 00:45:11,644: t15.2023.08.18 val PER: 0.1609
2026-01-04 00:45:11,644: t15.2023.08.20 val PER: 0.1755
2026-01-04 00:45:11,644: t15.2023.08.25 val PER: 0.1175
2026-01-04 00:45:11,644: t15.2023.08.27 val PER: 0.2492
2026-01-04 00:45:11,644: t15.2023.09.01 val PER: 0.1266
2026-01-04 00:45:11,644: t15.2023.09.03 val PER: 0.2067
2026-01-04 00:45:11,644: t15.2023.09.24 val PER: 0.1663
2026-01-04 00:45:11,644: t15.2023.09.29 val PER: 0.1761
2026-01-04 00:45:11,644: t15.2023.10.01 val PER: 0.2246
2026-01-04 00:45:11,644: t15.2023.10.06 val PER: 0.1313
2026-01-04 00:45:11,644: t15.2023.10.08 val PER: 0.2950
2026-01-04 00:45:11,644: t15.2023.10.13 val PER: 0.2676
2026-01-04 00:45:11,644: t15.2023.10.15 val PER: 0.2129
2026-01-04 00:45:11,644: t15.2023.10.20 val PER: 0.2114
2026-01-04 00:45:11,645: t15.2023.10.22 val PER: 0.1682
2026-01-04 00:45:11,645: t15.2023.11.03 val PER: 0.2273
2026-01-04 00:45:11,645: t15.2023.11.04 val PER: 0.0614
2026-01-04 00:45:11,645: t15.2023.11.17 val PER: 0.0793
2026-01-04 00:45:11,645: t15.2023.11.19 val PER: 0.0778
2026-01-04 00:45:11,645: t15.2023.11.26 val PER: 0.2203
2026-01-04 00:45:11,645: t15.2023.12.03 val PER: 0.1586
2026-01-04 00:45:11,645: t15.2023.12.08 val PER: 0.1791
2026-01-04 00:45:11,645: t15.2023.12.10 val PER: 0.1511
2026-01-04 00:45:11,645: t15.2023.12.17 val PER: 0.1902
2026-01-04 00:45:11,645: t15.2023.12.29 val PER: 0.2162
2026-01-04 00:45:11,645: t15.2024.02.25 val PER: 0.1615
2026-01-04 00:45:11,645: t15.2024.03.08 val PER: 0.3073
2026-01-04 00:45:11,645: t15.2024.03.15 val PER: 0.2633
2026-01-04 00:45:11,645: t15.2024.03.17 val PER: 0.2127
2026-01-04 00:45:11,646: t15.2024.05.10 val PER: 0.2125
2026-01-04 00:45:11,646: t15.2024.06.14 val PER: 0.2256
2026-01-04 00:45:11,646: t15.2024.07.19 val PER: 0.3078
2026-01-04 00:45:11,646: t15.2024.07.21 val PER: 0.1566
2026-01-04 00:45:11,646: t15.2024.07.28 val PER: 0.1985
2026-01-04 00:45:11,646: t15.2025.01.10 val PER: 0.3829
2026-01-04 00:45:11,646: t15.2025.01.12 val PER: 0.2202
2026-01-04 00:45:11,646: t15.2025.03.14 val PER: 0.3713
2026-01-04 00:45:11,646: t15.2025.03.16 val PER: 0.2723
2026-01-04 00:45:11,646: t15.2025.03.30 val PER: 0.3632
2026-01-04 00:45:11,646: t15.2025.04.13 val PER: 0.2725
2026-01-04 00:45:11,882: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_6000
2026-01-04 00:45:29,240: Train batch 6200: loss: 16.66 grad norm: 59.63 time: 0.070
2026-01-04 00:45:46,446: Train batch 6400: loss: 18.40 grad norm: 65.27 time: 0.063
2026-01-04 00:45:54,932: Running test after training batch: 6500
2026-01-04 00:45:55,064: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:45:59,728: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 00:45:59,758: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 00:46:01,385: Val batch 6500: PER (avg): 0.2045 CTC Loss (avg): 19.9780 WER(1gram): 53.55% (n=64) time: 6.453
2026-01-04 00:46:01,386: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 00:46:01,386: t15.2023.08.13 val PER: 0.1746
2026-01-04 00:46:01,386: t15.2023.08.18 val PER: 0.1467
2026-01-04 00:46:01,386: t15.2023.08.20 val PER: 0.1565
2026-01-04 00:46:01,386: t15.2023.08.25 val PER: 0.1145
2026-01-04 00:46:01,386: t15.2023.08.27 val PER: 0.2363
2026-01-04 00:46:01,387: t15.2023.09.01 val PER: 0.1153
2026-01-04 00:46:01,387: t15.2023.09.03 val PER: 0.2138
2026-01-04 00:46:01,387: t15.2023.09.24 val PER: 0.1699
2026-01-04 00:46:01,387: t15.2023.09.29 val PER: 0.1672
2026-01-04 00:46:01,387: t15.2023.10.01 val PER: 0.2213
2026-01-04 00:46:01,387: t15.2023.10.06 val PER: 0.1281
2026-01-04 00:46:01,387: t15.2023.10.08 val PER: 0.2936
2026-01-04 00:46:01,387: t15.2023.10.13 val PER: 0.2645
2026-01-04 00:46:01,387: t15.2023.10.15 val PER: 0.2083
2026-01-04 00:46:01,387: t15.2023.10.20 val PER: 0.2215
2026-01-04 00:46:01,388: t15.2023.10.22 val PER: 0.1670
2026-01-04 00:46:01,388: t15.2023.11.03 val PER: 0.2110
2026-01-04 00:46:01,388: t15.2023.11.04 val PER: 0.0580
2026-01-04 00:46:01,388: t15.2023.11.17 val PER: 0.0669
2026-01-04 00:46:01,388: t15.2023.11.19 val PER: 0.0758
2026-01-04 00:46:01,388: t15.2023.11.26 val PER: 0.2072
2026-01-04 00:46:01,388: t15.2023.12.03 val PER: 0.1754
2026-01-04 00:46:01,388: t15.2023.12.08 val PER: 0.1684
2026-01-04 00:46:01,388: t15.2023.12.10 val PER: 0.1459
2026-01-04 00:46:01,388: t15.2023.12.17 val PER: 0.2048
2026-01-04 00:46:01,388: t15.2023.12.29 val PER: 0.2038
2026-01-04 00:46:01,389: t15.2024.02.25 val PER: 0.1699
2026-01-04 00:46:01,389: t15.2024.03.08 val PER: 0.2888
2026-01-04 00:46:01,389: t15.2024.03.15 val PER: 0.2570
2026-01-04 00:46:01,389: t15.2024.03.17 val PER: 0.2050
2026-01-04 00:46:01,389: t15.2024.05.10 val PER: 0.2080
2026-01-04 00:46:01,389: t15.2024.06.14 val PER: 0.2208
2026-01-04 00:46:01,389: t15.2024.07.19 val PER: 0.3006
2026-01-04 00:46:01,389: t15.2024.07.21 val PER: 0.1441
2026-01-04 00:46:01,389: t15.2024.07.28 val PER: 0.1890
2026-01-04 00:46:01,389: t15.2025.01.10 val PER: 0.3705
2026-01-04 00:46:01,389: t15.2025.01.12 val PER: 0.2132
2026-01-04 00:46:01,389: t15.2025.03.14 val PER: 0.3817
2026-01-04 00:46:01,390: t15.2025.03.16 val PER: 0.2408
2026-01-04 00:46:01,390: t15.2025.03.30 val PER: 0.3517
2026-01-04 00:46:01,390: t15.2025.04.13 val PER: 0.2668
2026-01-04 00:46:01,390: New best val WER(1gram) 55.58% --> 53.55%
2026-01-04 00:46:01,390: Checkpointing model
2026-01-04 00:46:01,984: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/best_checkpoint
2026-01-04 00:46:02,226: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_6500
2026-01-04 00:46:10,782: Train batch 6600: loss: 12.25 grad norm: 52.50 time: 0.045
2026-01-04 00:46:28,098: Train batch 6800: loss: 15.49 grad norm: 57.82 time: 0.048
2026-01-04 00:46:45,344: Train batch 7000: loss: 16.73 grad norm: 62.99 time: 0.061
2026-01-04 00:46:45,344: Running test after training batch: 7000
2026-01-04 00:46:45,483: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:46:50,199: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the cold at this point as will
2026-01-04 00:46:50,228: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-04 00:46:51,904: Val batch 7000: PER (avg): 0.1947 CTC Loss (avg): 19.0915 WER(1gram): 54.82% (n=64) time: 6.560
2026-01-04 00:46:51,905: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-04 00:46:51,905: t15.2023.08.13 val PER: 0.1663
2026-01-04 00:46:51,905: t15.2023.08.18 val PER: 0.1350
2026-01-04 00:46:51,905: t15.2023.08.20 val PER: 0.1557
2026-01-04 00:46:51,905: t15.2023.08.25 val PER: 0.1039
2026-01-04 00:46:51,905: t15.2023.08.27 val PER: 0.2186
2026-01-04 00:46:51,905: t15.2023.09.01 val PER: 0.1088
2026-01-04 00:46:51,905: t15.2023.09.03 val PER: 0.1960
2026-01-04 00:46:51,905: t15.2023.09.24 val PER: 0.1517
2026-01-04 00:46:51,905: t15.2023.09.29 val PER: 0.1698
2026-01-04 00:46:51,906: t15.2023.10.01 val PER: 0.2127
2026-01-04 00:46:51,906: t15.2023.10.06 val PER: 0.1055
2026-01-04 00:46:51,906: t15.2023.10.08 val PER: 0.2828
2026-01-04 00:46:51,906: t15.2023.10.13 val PER: 0.2506
2026-01-04 00:46:51,906: t15.2023.10.15 val PER: 0.1898
2026-01-04 00:46:51,906: t15.2023.10.20 val PER: 0.2013
2026-01-04 00:46:51,906: t15.2023.10.22 val PER: 0.1459
2026-01-04 00:46:51,906: t15.2023.11.03 val PER: 0.2056
2026-01-04 00:46:51,906: t15.2023.11.04 val PER: 0.0410
2026-01-04 00:46:51,906: t15.2023.11.17 val PER: 0.0591
2026-01-04 00:46:51,906: t15.2023.11.19 val PER: 0.0619
2026-01-04 00:46:51,906: t15.2023.11.26 val PER: 0.1877
2026-01-04 00:46:51,906: t15.2023.12.03 val PER: 0.1618
2026-01-04 00:46:51,907: t15.2023.12.08 val PER: 0.1591
2026-01-04 00:46:51,907: t15.2023.12.10 val PER: 0.1419
2026-01-04 00:46:51,907: t15.2023.12.17 val PER: 0.1871
2026-01-04 00:46:51,907: t15.2023.12.29 val PER: 0.1922
2026-01-04 00:46:51,907: t15.2024.02.25 val PER: 0.1601
2026-01-04 00:46:51,907: t15.2024.03.08 val PER: 0.2745
2026-01-04 00:46:51,907: t15.2024.03.15 val PER: 0.2402
2026-01-04 00:46:51,907: t15.2024.03.17 val PER: 0.2078
2026-01-04 00:46:51,907: t15.2024.05.10 val PER: 0.1991
2026-01-04 00:46:51,907: t15.2024.06.14 val PER: 0.2161
2026-01-04 00:46:51,907: t15.2024.07.19 val PER: 0.2947
2026-01-04 00:46:51,907: t15.2024.07.21 val PER: 0.1386
2026-01-04 00:46:51,907: t15.2024.07.28 val PER: 0.1794
2026-01-04 00:46:51,907: t15.2025.01.10 val PER: 0.3691
2026-01-04 00:46:51,907: t15.2025.01.12 val PER: 0.2132
2026-01-04 00:46:51,907: t15.2025.03.14 val PER: 0.3476
2026-01-04 00:46:51,908: t15.2025.03.16 val PER: 0.2408
2026-01-04 00:46:51,908: t15.2025.03.30 val PER: 0.3586
2026-01-04 00:46:51,908: t15.2025.04.13 val PER: 0.2596
2026-01-04 00:46:52,147: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_7000
2026-01-04 00:47:09,157: Train batch 7200: loss: 14.05 grad norm: 55.03 time: 0.078
2026-01-04 00:47:25,878: Train batch 7400: loss: 13.54 grad norm: 55.01 time: 0.075
2026-01-04 00:47:34,345: Running test after training batch: 7500
2026-01-04 00:47:34,457: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:47:39,507: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-04 00:47:39,539: WER debug example
  GT : how does it keep the cost down
  PR : houde dusk it keep the cussed nit
2026-01-04 00:47:41,250: Val batch 7500: PER (avg): 0.1891 CTC Loss (avg): 18.5917 WER(1gram): 55.08% (n=64) time: 6.905
2026-01-04 00:47:41,251: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 00:47:41,251: t15.2023.08.13 val PER: 0.1507
2026-01-04 00:47:41,251: t15.2023.08.18 val PER: 0.1408
2026-01-04 00:47:41,251: t15.2023.08.20 val PER: 0.1493
2026-01-04 00:47:41,251: t15.2023.08.25 val PER: 0.0964
2026-01-04 00:47:41,251: t15.2023.08.27 val PER: 0.2122
2026-01-04 00:47:41,251: t15.2023.09.01 val PER: 0.1055
2026-01-04 00:47:41,251: t15.2023.09.03 val PER: 0.1971
2026-01-04 00:47:41,251: t15.2023.09.24 val PER: 0.1456
2026-01-04 00:47:41,252: t15.2023.09.29 val PER: 0.1525
2026-01-04 00:47:41,252: t15.2023.10.01 val PER: 0.2061
2026-01-04 00:47:41,252: t15.2023.10.06 val PER: 0.1206
2026-01-04 00:47:41,252: t15.2023.10.08 val PER: 0.2693
2026-01-04 00:47:41,252: t15.2023.10.13 val PER: 0.2420
2026-01-04 00:47:41,252: t15.2023.10.15 val PER: 0.1885
2026-01-04 00:47:41,252: t15.2023.10.20 val PER: 0.1846
2026-01-04 00:47:41,252: t15.2023.10.22 val PER: 0.1392
2026-01-04 00:47:41,252: t15.2023.11.03 val PER: 0.2056
2026-01-04 00:47:41,252: t15.2023.11.04 val PER: 0.0478
2026-01-04 00:47:41,252: t15.2023.11.17 val PER: 0.0731
2026-01-04 00:47:41,252: t15.2023.11.19 val PER: 0.0499
2026-01-04 00:47:41,252: t15.2023.11.26 val PER: 0.1783
2026-01-04 00:47:41,252: t15.2023.12.03 val PER: 0.1513
2026-01-04 00:47:41,253: t15.2023.12.08 val PER: 0.1631
2026-01-04 00:47:41,253: t15.2023.12.10 val PER: 0.1353
2026-01-04 00:47:41,253: t15.2023.12.17 val PER: 0.1757
2026-01-04 00:47:41,253: t15.2023.12.29 val PER: 0.1887
2026-01-04 00:47:41,253: t15.2024.02.25 val PER: 0.1531
2026-01-04 00:47:41,253: t15.2024.03.08 val PER: 0.2589
2026-01-04 00:47:41,253: t15.2024.03.15 val PER: 0.2389
2026-01-04 00:47:41,253: t15.2024.03.17 val PER: 0.1820
2026-01-04 00:47:41,253: t15.2024.05.10 val PER: 0.2051
2026-01-04 00:47:41,253: t15.2024.06.14 val PER: 0.1972
2026-01-04 00:47:41,253: t15.2024.07.19 val PER: 0.2907
2026-01-04 00:47:41,253: t15.2024.07.21 val PER: 0.1462
2026-01-04 00:47:41,253: t15.2024.07.28 val PER: 0.1721
2026-01-04 00:47:41,253: t15.2025.01.10 val PER: 0.3567
2026-01-04 00:47:41,253: t15.2025.01.12 val PER: 0.1940
2026-01-04 00:47:41,253: t15.2025.03.14 val PER: 0.3595
2026-01-04 00:47:41,254: t15.2025.03.16 val PER: 0.2474
2026-01-04 00:47:41,254: t15.2025.03.30 val PER: 0.3483
2026-01-04 00:47:41,254: t15.2025.04.13 val PER: 0.2496
2026-01-04 00:47:41,486: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_7500
2026-01-04 00:47:49,947: Train batch 7600: loss: 16.13 grad norm: 61.37 time: 0.068
2026-01-04 00:48:07,052: Train batch 7800: loss: 14.41 grad norm: 62.48 time: 0.055
2026-01-04 00:48:24,563: Train batch 8000: loss: 10.69 grad norm: 50.02 time: 0.071
2026-01-04 00:48:24,563: Running test after training batch: 8000
2026-01-04 00:48:24,667: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:48:29,361: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 00:48:29,393: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cussed ned
2026-01-04 00:48:31,085: Val batch 8000: PER (avg): 0.1855 CTC Loss (avg): 18.1280 WER(1gram): 55.08% (n=64) time: 6.522
2026-01-04 00:48:31,086: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 00:48:31,086: t15.2023.08.13 val PER: 0.1403
2026-01-04 00:48:31,086: t15.2023.08.18 val PER: 0.1282
2026-01-04 00:48:31,086: t15.2023.08.20 val PER: 0.1422
2026-01-04 00:48:31,087: t15.2023.08.25 val PER: 0.1084
2026-01-04 00:48:31,087: t15.2023.08.27 val PER: 0.2154
2026-01-04 00:48:31,087: t15.2023.09.01 val PER: 0.0990
2026-01-04 00:48:31,087: t15.2023.09.03 val PER: 0.1936
2026-01-04 00:48:31,087: t15.2023.09.24 val PER: 0.1529
2026-01-04 00:48:31,087: t15.2023.09.29 val PER: 0.1461
2026-01-04 00:48:31,088: t15.2023.10.01 val PER: 0.2028
2026-01-04 00:48:31,088: t15.2023.10.06 val PER: 0.1119
2026-01-04 00:48:31,088: t15.2023.10.08 val PER: 0.2760
2026-01-04 00:48:31,088: t15.2023.10.13 val PER: 0.2444
2026-01-04 00:48:31,088: t15.2023.10.15 val PER: 0.1964
2026-01-04 00:48:31,088: t15.2023.10.20 val PER: 0.1980
2026-01-04 00:48:31,088: t15.2023.10.22 val PER: 0.1448
2026-01-04 00:48:31,088: t15.2023.11.03 val PER: 0.2035
2026-01-04 00:48:31,089: t15.2023.11.04 val PER: 0.0444
2026-01-04 00:48:31,089: t15.2023.11.17 val PER: 0.0575
2026-01-04 00:48:31,089: t15.2023.11.19 val PER: 0.0659
2026-01-04 00:48:31,089: t15.2023.11.26 val PER: 0.1746
2026-01-04 00:48:31,089: t15.2023.12.03 val PER: 0.1439
2026-01-04 00:48:31,089: t15.2023.12.08 val PER: 0.1445
2026-01-04 00:48:31,089: t15.2023.12.10 val PER: 0.1314
2026-01-04 00:48:31,089: t15.2023.12.17 val PER: 0.1757
2026-01-04 00:48:31,089: t15.2023.12.29 val PER: 0.1798
2026-01-04 00:48:31,089: t15.2024.02.25 val PER: 0.1334
2026-01-04 00:48:31,090: t15.2024.03.08 val PER: 0.2888
2026-01-04 00:48:31,090: t15.2024.03.15 val PER: 0.2351
2026-01-04 00:48:31,090: t15.2024.03.17 val PER: 0.1827
2026-01-04 00:48:31,090: t15.2024.05.10 val PER: 0.1991
2026-01-04 00:48:31,090: t15.2024.06.14 val PER: 0.2098
2026-01-04 00:48:31,090: t15.2024.07.19 val PER: 0.2927
2026-01-04 00:48:31,090: t15.2024.07.21 val PER: 0.1172
2026-01-04 00:48:31,090: t15.2024.07.28 val PER: 0.1603
2026-01-04 00:48:31,090: t15.2025.01.10 val PER: 0.3375
2026-01-04 00:48:31,090: t15.2025.01.12 val PER: 0.2009
2026-01-04 00:48:31,090: t15.2025.03.14 val PER: 0.3609
2026-01-04 00:48:31,091: t15.2025.03.16 val PER: 0.2212
2026-01-04 00:48:31,091: t15.2025.03.30 val PER: 0.3425
2026-01-04 00:48:31,091: t15.2025.04.13 val PER: 0.2668
2026-01-04 00:48:31,325: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_8000
2026-01-04 00:48:48,389: Train batch 8200: loss: 9.42 grad norm: 46.96 time: 0.054
2026-01-04 00:49:05,630: Train batch 8400: loss: 10.17 grad norm: 48.42 time: 0.064
2026-01-04 00:49:14,390: Running test after training batch: 8500
2026-01-04 00:49:14,505: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:49:19,150: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 00:49:19,182: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost ned
2026-01-04 00:49:20,871: Val batch 8500: PER (avg): 0.1794 CTC Loss (avg): 17.6818 WER(1gram): 48.48% (n=64) time: 6.480
2026-01-04 00:49:20,871: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 00:49:20,871: t15.2023.08.13 val PER: 0.1393
2026-01-04 00:49:20,872: t15.2023.08.18 val PER: 0.1391
2026-01-04 00:49:20,872: t15.2023.08.20 val PER: 0.1342
2026-01-04 00:49:20,872: t15.2023.08.25 val PER: 0.1145
2026-01-04 00:49:20,872: t15.2023.08.27 val PER: 0.2106
2026-01-04 00:49:20,872: t15.2023.09.01 val PER: 0.0998
2026-01-04 00:49:20,872: t15.2023.09.03 val PER: 0.1971
2026-01-04 00:49:20,872: t15.2023.09.24 val PER: 0.1420
2026-01-04 00:49:20,872: t15.2023.09.29 val PER: 0.1557
2026-01-04 00:49:20,872: t15.2023.10.01 val PER: 0.1942
2026-01-04 00:49:20,872: t15.2023.10.06 val PER: 0.1087
2026-01-04 00:49:20,872: t15.2023.10.08 val PER: 0.2530
2026-01-04 00:49:20,872: t15.2023.10.13 val PER: 0.2389
2026-01-04 00:49:20,872: t15.2023.10.15 val PER: 0.1707
2026-01-04 00:49:20,872: t15.2023.10.20 val PER: 0.1812
2026-01-04 00:49:20,873: t15.2023.10.22 val PER: 0.1437
2026-01-04 00:49:20,873: t15.2023.11.03 val PER: 0.2015
2026-01-04 00:49:20,873: t15.2023.11.04 val PER: 0.0444
2026-01-04 00:49:20,873: t15.2023.11.17 val PER: 0.0560
2026-01-04 00:49:20,873: t15.2023.11.19 val PER: 0.0499
2026-01-04 00:49:20,873: t15.2023.11.26 val PER: 0.1725
2026-01-04 00:49:20,873: t15.2023.12.03 val PER: 0.1345
2026-01-04 00:49:20,873: t15.2023.12.08 val PER: 0.1431
2026-01-04 00:49:20,873: t15.2023.12.10 val PER: 0.1156
2026-01-04 00:49:20,873: t15.2023.12.17 val PER: 0.1746
2026-01-04 00:49:20,874: t15.2023.12.29 val PER: 0.1661
2026-01-04 00:49:20,874: t15.2024.02.25 val PER: 0.1362
2026-01-04 00:49:20,874: t15.2024.03.08 val PER: 0.2674
2026-01-04 00:49:20,874: t15.2024.03.15 val PER: 0.2345
2026-01-04 00:49:20,874: t15.2024.03.17 val PER: 0.1715
2026-01-04 00:49:20,874: t15.2024.05.10 val PER: 0.1991
2026-01-04 00:49:20,874: t15.2024.06.14 val PER: 0.1940
2026-01-04 00:49:20,874: t15.2024.07.19 val PER: 0.2663
2026-01-04 00:49:20,874: t15.2024.07.21 val PER: 0.1193
2026-01-04 00:49:20,874: t15.2024.07.28 val PER: 0.1625
2026-01-04 00:49:20,874: t15.2025.01.10 val PER: 0.3209
2026-01-04 00:49:20,875: t15.2025.01.12 val PER: 0.1940
2026-01-04 00:49:20,875: t15.2025.03.14 val PER: 0.3624
2026-01-04 00:49:20,875: t15.2025.03.16 val PER: 0.2107
2026-01-04 00:49:20,875: t15.2025.03.30 val PER: 0.3345
2026-01-04 00:49:20,875: t15.2025.04.13 val PER: 0.2468
2026-01-04 00:49:20,876: New best val WER(1gram) 53.55% --> 48.48%
2026-01-04 00:49:20,876: Checkpointing model
2026-01-04 00:49:21,501: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/best_checkpoint
2026-01-04 00:49:21,750: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_8500
2026-01-04 00:49:30,368: Train batch 8600: loss: 16.07 grad norm: 62.22 time: 0.054
2026-01-04 00:49:47,591: Train batch 8800: loss: 14.51 grad norm: 57.00 time: 0.060
2026-01-04 00:50:04,921: Train batch 9000: loss: 16.11 grad norm: 66.05 time: 0.072
2026-01-04 00:50:04,922: Running test after training batch: 9000
2026-01-04 00:50:05,018: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:50:09,847: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 00:50:09,878: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost ged
2026-01-04 00:50:11,626: Val batch 9000: PER (avg): 0.1764 CTC Loss (avg): 17.2979 WER(1gram): 54.31% (n=64) time: 6.704
2026-01-04 00:50:11,626: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 00:50:11,626: t15.2023.08.13 val PER: 0.1362
2026-01-04 00:50:11,627: t15.2023.08.18 val PER: 0.1241
2026-01-04 00:50:11,627: t15.2023.08.20 val PER: 0.1366
2026-01-04 00:50:11,627: t15.2023.08.25 val PER: 0.0994
2026-01-04 00:50:11,627: t15.2023.08.27 val PER: 0.2122
2026-01-04 00:50:11,627: t15.2023.09.01 val PER: 0.0958
2026-01-04 00:50:11,627: t15.2023.09.03 val PER: 0.1876
2026-01-04 00:50:11,627: t15.2023.09.24 val PER: 0.1493
2026-01-04 00:50:11,627: t15.2023.09.29 val PER: 0.1429
2026-01-04 00:50:11,627: t15.2023.10.01 val PER: 0.1929
2026-01-04 00:50:11,627: t15.2023.10.06 val PER: 0.0980
2026-01-04 00:50:11,627: t15.2023.10.08 val PER: 0.2639
2026-01-04 00:50:11,627: t15.2023.10.13 val PER: 0.2304
2026-01-04 00:50:11,627: t15.2023.10.15 val PER: 0.1747
2026-01-04 00:50:11,627: t15.2023.10.20 val PER: 0.2047
2026-01-04 00:50:11,628: t15.2023.10.22 val PER: 0.1347
2026-01-04 00:50:11,628: t15.2023.11.03 val PER: 0.1954
2026-01-04 00:50:11,628: t15.2023.11.04 val PER: 0.0410
2026-01-04 00:50:11,628: t15.2023.11.17 val PER: 0.0700
2026-01-04 00:50:11,628: t15.2023.11.19 val PER: 0.0519
2026-01-04 00:50:11,628: t15.2023.11.26 val PER: 0.1623
2026-01-04 00:50:11,628: t15.2023.12.03 val PER: 0.1418
2026-01-04 00:50:11,628: t15.2023.12.08 val PER: 0.1338
2026-01-04 00:50:11,628: t15.2023.12.10 val PER: 0.1156
2026-01-04 00:50:11,629: t15.2023.12.17 val PER: 0.1684
2026-01-04 00:50:11,629: t15.2023.12.29 val PER: 0.1647
2026-01-04 00:50:11,629: t15.2024.02.25 val PER: 0.1433
2026-01-04 00:50:11,629: t15.2024.03.08 val PER: 0.2774
2026-01-04 00:50:11,629: t15.2024.03.15 val PER: 0.2270
2026-01-04 00:50:11,629: t15.2024.03.17 val PER: 0.1736
2026-01-04 00:50:11,629: t15.2024.05.10 val PER: 0.1991
2026-01-04 00:50:11,629: t15.2024.06.14 val PER: 0.1798
2026-01-04 00:50:11,629: t15.2024.07.19 val PER: 0.2703
2026-01-04 00:50:11,629: t15.2024.07.21 val PER: 0.1166
2026-01-04 00:50:11,629: t15.2024.07.28 val PER: 0.1603
2026-01-04 00:50:11,629: t15.2025.01.10 val PER: 0.3264
2026-01-04 00:50:11,629: t15.2025.01.12 val PER: 0.1809
2026-01-04 00:50:11,630: t15.2025.03.14 val PER: 0.3565
2026-01-04 00:50:11,630: t15.2025.03.16 val PER: 0.2147
2026-01-04 00:50:11,630: t15.2025.03.30 val PER: 0.3218
2026-01-04 00:50:11,630: t15.2025.04.13 val PER: 0.2496
2026-01-04 00:50:11,871: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_9000
2026-01-04 00:50:29,245: Train batch 9200: loss: 10.88 grad norm: 50.49 time: 0.055
2026-01-04 00:50:46,637: Train batch 9400: loss: 7.26 grad norm: 46.32 time: 0.067
2026-01-04 00:50:55,393: Running test after training batch: 9500
2026-01-04 00:50:55,494: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:51:00,113: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:51:00,143: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 00:51:01,864: Val batch 9500: PER (avg): 0.1724 CTC Loss (avg): 17.0618 WER(1gram): 51.02% (n=64) time: 6.471
2026-01-04 00:51:01,865: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 00:51:01,865: t15.2023.08.13 val PER: 0.1299
2026-01-04 00:51:01,865: t15.2023.08.18 val PER: 0.1182
2026-01-04 00:51:01,865: t15.2023.08.20 val PER: 0.1326
2026-01-04 00:51:01,865: t15.2023.08.25 val PER: 0.0964
2026-01-04 00:51:01,865: t15.2023.08.27 val PER: 0.2154
2026-01-04 00:51:01,865: t15.2023.09.01 val PER: 0.0950
2026-01-04 00:51:01,865: t15.2023.09.03 val PER: 0.1722
2026-01-04 00:51:01,865: t15.2023.09.24 val PER: 0.1456
2026-01-04 00:51:01,866: t15.2023.09.29 val PER: 0.1436
2026-01-04 00:51:01,866: t15.2023.10.01 val PER: 0.1955
2026-01-04 00:51:01,866: t15.2023.10.06 val PER: 0.0990
2026-01-04 00:51:01,866: t15.2023.10.08 val PER: 0.2720
2026-01-04 00:51:01,866: t15.2023.10.13 val PER: 0.2242
2026-01-04 00:51:01,866: t15.2023.10.15 val PER: 0.1806
2026-01-04 00:51:01,866: t15.2023.10.20 val PER: 0.1980
2026-01-04 00:51:01,866: t15.2023.10.22 val PER: 0.1258
2026-01-04 00:51:01,866: t15.2023.11.03 val PER: 0.1886
2026-01-04 00:51:01,866: t15.2023.11.04 val PER: 0.0307
2026-01-04 00:51:01,867: t15.2023.11.17 val PER: 0.0544
2026-01-04 00:51:01,867: t15.2023.11.19 val PER: 0.0599
2026-01-04 00:51:01,867: t15.2023.11.26 val PER: 0.1558
2026-01-04 00:51:01,867: t15.2023.12.03 val PER: 0.1324
2026-01-04 00:51:01,867: t15.2023.12.08 val PER: 0.1298
2026-01-04 00:51:01,867: t15.2023.12.10 val PER: 0.1222
2026-01-04 00:51:01,867: t15.2023.12.17 val PER: 0.1632
2026-01-04 00:51:01,867: t15.2023.12.29 val PER: 0.1572
2026-01-04 00:51:01,867: t15.2024.02.25 val PER: 0.1362
2026-01-04 00:51:01,867: t15.2024.03.08 val PER: 0.2475
2026-01-04 00:51:01,867: t15.2024.03.15 val PER: 0.2176
2026-01-04 00:51:01,867: t15.2024.03.17 val PER: 0.1562
2026-01-04 00:51:01,867: t15.2024.05.10 val PER: 0.1947
2026-01-04 00:51:01,867: t15.2024.06.14 val PER: 0.1767
2026-01-04 00:51:01,867: t15.2024.07.19 val PER: 0.2630
2026-01-04 00:51:01,867: t15.2024.07.21 val PER: 0.1179
2026-01-04 00:51:01,868: t15.2024.07.28 val PER: 0.1529
2026-01-04 00:51:01,868: t15.2025.01.10 val PER: 0.3154
2026-01-04 00:51:01,868: t15.2025.01.12 val PER: 0.1940
2026-01-04 00:51:01,868: t15.2025.03.14 val PER: 0.3639
2026-01-04 00:51:01,868: t15.2025.03.16 val PER: 0.2147
2026-01-04 00:51:01,868: t15.2025.03.30 val PER: 0.3310
2026-01-04 00:51:01,868: t15.2025.04.13 val PER: 0.2311
2026-01-04 00:51:02,120: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_9500
2026-01-04 00:51:10,864: Train batch 9600: loss: 8.21 grad norm: 47.78 time: 0.073
2026-01-04 00:51:28,356: Train batch 9800: loss: 11.90 grad norm: 54.74 time: 0.063
2026-01-04 00:51:46,242: Train batch 10000: loss: 5.78 grad norm: 37.43 time: 0.061
2026-01-04 00:51:46,243: Running test after training batch: 10000
2026-01-04 00:51:46,371: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:51:51,138: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:51:51,169: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-04 00:51:52,928: Val batch 10000: PER (avg): 0.1697 CTC Loss (avg): 16.8463 WER(1gram): 51.02% (n=64) time: 6.685
2026-01-04 00:51:52,928: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 00:51:52,928: t15.2023.08.13 val PER: 0.1341
2026-01-04 00:51:52,928: t15.2023.08.18 val PER: 0.1224
2026-01-04 00:51:52,929: t15.2023.08.20 val PER: 0.1255
2026-01-04 00:51:52,929: t15.2023.08.25 val PER: 0.1054
2026-01-04 00:51:52,929: t15.2023.08.27 val PER: 0.2042
2026-01-04 00:51:52,929: t15.2023.09.01 val PER: 0.0893
2026-01-04 00:51:52,929: t15.2023.09.03 val PER: 0.1793
2026-01-04 00:51:52,929: t15.2023.09.24 val PER: 0.1420
2026-01-04 00:51:52,929: t15.2023.09.29 val PER: 0.1385
2026-01-04 00:51:52,929: t15.2023.10.01 val PER: 0.1836
2026-01-04 00:51:52,929: t15.2023.10.06 val PER: 0.1141
2026-01-04 00:51:52,929: t15.2023.10.08 val PER: 0.2625
2026-01-04 00:51:52,929: t15.2023.10.13 val PER: 0.2211
2026-01-04 00:51:52,929: t15.2023.10.15 val PER: 0.1707
2026-01-04 00:51:52,929: t15.2023.10.20 val PER: 0.1846
2026-01-04 00:51:52,929: t15.2023.10.22 val PER: 0.1292
2026-01-04 00:51:52,933: t15.2023.11.03 val PER: 0.1866
2026-01-04 00:51:52,933: t15.2023.11.04 val PER: 0.0444
2026-01-04 00:51:52,933: t15.2023.11.17 val PER: 0.0575
2026-01-04 00:51:52,933: t15.2023.11.19 val PER: 0.0479
2026-01-04 00:51:52,933: t15.2023.11.26 val PER: 0.1391
2026-01-04 00:51:52,933: t15.2023.12.03 val PER: 0.1376
2026-01-04 00:51:52,933: t15.2023.12.08 val PER: 0.1305
2026-01-04 00:51:52,933: t15.2023.12.10 val PER: 0.1130
2026-01-04 00:51:52,933: t15.2023.12.17 val PER: 0.1601
2026-01-04 00:51:52,934: t15.2023.12.29 val PER: 0.1531
2026-01-04 00:51:52,934: t15.2024.02.25 val PER: 0.1362
2026-01-04 00:51:52,934: t15.2024.03.08 val PER: 0.2489
2026-01-04 00:51:52,934: t15.2024.03.15 val PER: 0.2145
2026-01-04 00:51:52,934: t15.2024.03.17 val PER: 0.1709
2026-01-04 00:51:52,934: t15.2024.05.10 val PER: 0.1753
2026-01-04 00:51:52,934: t15.2024.06.14 val PER: 0.1893
2026-01-04 00:51:52,934: t15.2024.07.19 val PER: 0.2762
2026-01-04 00:51:52,934: t15.2024.07.21 val PER: 0.1097
2026-01-04 00:51:52,934: t15.2024.07.28 val PER: 0.1515
2026-01-04 00:51:52,934: t15.2025.01.10 val PER: 0.3058
2026-01-04 00:51:52,935: t15.2025.01.12 val PER: 0.1801
2026-01-04 00:51:52,935: t15.2025.03.14 val PER: 0.3491
2026-01-04 00:51:52,935: t15.2025.03.16 val PER: 0.2147
2026-01-04 00:51:52,935: t15.2025.03.30 val PER: 0.3184
2026-01-04 00:51:52,935: t15.2025.04.13 val PER: 0.2297
2026-01-04 00:51:53,186: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_10000
2026-01-04 00:52:10,970: Train batch 10200: loss: 6.45 grad norm: 40.24 time: 0.050
2026-01-04 00:52:28,579: Train batch 10400: loss: 9.16 grad norm: 51.31 time: 0.071
2026-01-04 00:52:37,593: Running test after training batch: 10500
2026-01-04 00:52:37,729: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:52:42,404: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:52:42,436: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-04 00:52:44,189: Val batch 10500: PER (avg): 0.1675 CTC Loss (avg): 16.5616 WER(1gram): 48.98% (n=64) time: 6.595
2026-01-04 00:52:44,189: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 00:52:44,190: t15.2023.08.13 val PER: 0.1237
2026-01-04 00:52:44,190: t15.2023.08.18 val PER: 0.1123
2026-01-04 00:52:44,190: t15.2023.08.20 val PER: 0.1319
2026-01-04 00:52:44,190: t15.2023.08.25 val PER: 0.1024
2026-01-04 00:52:44,190: t15.2023.08.27 val PER: 0.2090
2026-01-04 00:52:44,190: t15.2023.09.01 val PER: 0.0974
2026-01-04 00:52:44,190: t15.2023.09.03 val PER: 0.1722
2026-01-04 00:52:44,190: t15.2023.09.24 val PER: 0.1517
2026-01-04 00:52:44,190: t15.2023.09.29 val PER: 0.1493
2026-01-04 00:52:44,190: t15.2023.10.01 val PER: 0.1889
2026-01-04 00:52:44,190: t15.2023.10.06 val PER: 0.1033
2026-01-04 00:52:44,190: t15.2023.10.08 val PER: 0.2503
2026-01-04 00:52:44,191: t15.2023.10.13 val PER: 0.2110
2026-01-04 00:52:44,191: t15.2023.10.15 val PER: 0.1760
2026-01-04 00:52:44,191: t15.2023.10.20 val PER: 0.1980
2026-01-04 00:52:44,191: t15.2023.10.22 val PER: 0.1247
2026-01-04 00:52:44,191: t15.2023.11.03 val PER: 0.1920
2026-01-04 00:52:44,191: t15.2023.11.04 val PER: 0.0444
2026-01-04 00:52:44,191: t15.2023.11.17 val PER: 0.0467
2026-01-04 00:52:44,191: t15.2023.11.19 val PER: 0.0479
2026-01-04 00:52:44,191: t15.2023.11.26 val PER: 0.1435
2026-01-04 00:52:44,191: t15.2023.12.03 val PER: 0.1250
2026-01-04 00:52:44,191: t15.2023.12.08 val PER: 0.1158
2026-01-04 00:52:44,191: t15.2023.12.10 val PER: 0.1130
2026-01-04 00:52:44,191: t15.2023.12.17 val PER: 0.1486
2026-01-04 00:52:44,191: t15.2023.12.29 val PER: 0.1592
2026-01-04 00:52:44,192: t15.2024.02.25 val PER: 0.1264
2026-01-04 00:52:44,192: t15.2024.03.08 val PER: 0.2575
2026-01-04 00:52:44,192: t15.2024.03.15 val PER: 0.2151
2026-01-04 00:52:44,192: t15.2024.03.17 val PER: 0.1618
2026-01-04 00:52:44,192: t15.2024.05.10 val PER: 0.1724
2026-01-04 00:52:44,192: t15.2024.06.14 val PER: 0.1909
2026-01-04 00:52:44,192: t15.2024.07.19 val PER: 0.2577
2026-01-04 00:52:44,192: t15.2024.07.21 val PER: 0.1062
2026-01-04 00:52:44,192: t15.2024.07.28 val PER: 0.1368
2026-01-04 00:52:44,192: t15.2025.01.10 val PER: 0.3113
2026-01-04 00:52:44,192: t15.2025.01.12 val PER: 0.1848
2026-01-04 00:52:44,192: t15.2025.03.14 val PER: 0.3669
2026-01-04 00:52:44,192: t15.2025.03.16 val PER: 0.2003
2026-01-04 00:52:44,192: t15.2025.03.30 val PER: 0.3138
2026-01-04 00:52:44,192: t15.2025.04.13 val PER: 0.2282
2026-01-04 00:52:44,447: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_10500
2026-01-04 00:52:53,420: Train batch 10600: loss: 9.36 grad norm: 58.11 time: 0.072
2026-01-04 00:53:10,740: Train batch 10800: loss: 14.73 grad norm: 68.26 time: 0.064
2026-01-04 00:53:28,723: Train batch 11000: loss: 14.36 grad norm: 65.34 time: 0.056
2026-01-04 00:53:28,723: Running test after training batch: 11000
2026-01-04 00:53:28,894: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:53:33,524: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:53:33,555: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 00:53:35,325: Val batch 11000: PER (avg): 0.1638 CTC Loss (avg): 16.3778 WER(1gram): 48.98% (n=64) time: 6.602
2026-01-04 00:53:35,325: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=12
2026-01-04 00:53:35,326: t15.2023.08.13 val PER: 0.1310
2026-01-04 00:53:35,326: t15.2023.08.18 val PER: 0.1157
2026-01-04 00:53:35,326: t15.2023.08.20 val PER: 0.1183
2026-01-04 00:53:35,326: t15.2023.08.25 val PER: 0.0934
2026-01-04 00:53:35,326: t15.2023.08.27 val PER: 0.1897
2026-01-04 00:53:35,326: t15.2023.09.01 val PER: 0.0877
2026-01-04 00:53:35,326: t15.2023.09.03 val PER: 0.1722
2026-01-04 00:53:35,326: t15.2023.09.24 val PER: 0.1420
2026-01-04 00:53:35,326: t15.2023.09.29 val PER: 0.1359
2026-01-04 00:53:35,326: t15.2023.10.01 val PER: 0.1863
2026-01-04 00:53:35,326: t15.2023.10.06 val PER: 0.0926
2026-01-04 00:53:35,326: t15.2023.10.08 val PER: 0.2544
2026-01-04 00:53:35,326: t15.2023.10.13 val PER: 0.2157
2026-01-04 00:53:35,326: t15.2023.10.15 val PER: 0.1694
2026-01-04 00:53:35,327: t15.2023.10.20 val PER: 0.1913
2026-01-04 00:53:35,327: t15.2023.10.22 val PER: 0.1214
2026-01-04 00:53:35,327: t15.2023.11.03 val PER: 0.1967
2026-01-04 00:53:35,327: t15.2023.11.04 val PER: 0.0375
2026-01-04 00:53:35,327: t15.2023.11.17 val PER: 0.0529
2026-01-04 00:53:35,327: t15.2023.11.19 val PER: 0.0419
2026-01-04 00:53:35,327: t15.2023.11.26 val PER: 0.1384
2026-01-04 00:53:35,327: t15.2023.12.03 val PER: 0.1261
2026-01-04 00:53:35,327: t15.2023.12.08 val PER: 0.1232
2026-01-04 00:53:35,327: t15.2023.12.10 val PER: 0.1091
2026-01-04 00:53:35,327: t15.2023.12.17 val PER: 0.1497
2026-01-04 00:53:35,327: t15.2023.12.29 val PER: 0.1531
2026-01-04 00:53:35,327: t15.2024.02.25 val PER: 0.1264
2026-01-04 00:53:35,327: t15.2024.03.08 val PER: 0.2546
2026-01-04 00:53:35,327: t15.2024.03.15 val PER: 0.2051
2026-01-04 00:53:35,328: t15.2024.03.17 val PER: 0.1583
2026-01-04 00:53:35,328: t15.2024.05.10 val PER: 0.1902
2026-01-04 00:53:35,328: t15.2024.06.14 val PER: 0.1751
2026-01-04 00:53:35,328: t15.2024.07.19 val PER: 0.2544
2026-01-04 00:53:35,328: t15.2024.07.21 val PER: 0.1021
2026-01-04 00:53:35,328: t15.2024.07.28 val PER: 0.1404
2026-01-04 00:53:35,328: t15.2025.01.10 val PER: 0.3030
2026-01-04 00:53:35,328: t15.2025.01.12 val PER: 0.1732
2026-01-04 00:53:35,328: t15.2025.03.14 val PER: 0.3432
2026-01-04 00:53:35,328: t15.2025.03.16 val PER: 0.1937
2026-01-04 00:53:35,328: t15.2025.03.30 val PER: 0.3184
2026-01-04 00:53:35,328: t15.2025.04.13 val PER: 0.2211
2026-01-04 00:53:35,565: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_11000
2026-01-04 00:53:53,060: Train batch 11200: loss: 9.91 grad norm: 52.28 time: 0.071
2026-01-04 00:54:10,445: Train batch 11400: loss: 8.99 grad norm: 52.02 time: 0.056
2026-01-04 00:54:19,219: Running test after training batch: 11500
2026-01-04 00:54:19,378: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:54:24,235: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 00:54:24,266: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-04 00:54:26,027: Val batch 11500: PER (avg): 0.1614 CTC Loss (avg): 16.2500 WER(1gram): 49.24% (n=64) time: 6.807
2026-01-04 00:54:26,027: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=11
2026-01-04 00:54:26,027: t15.2023.08.13 val PER: 0.1175
2026-01-04 00:54:26,027: t15.2023.08.18 val PER: 0.1165
2026-01-04 00:54:26,027: t15.2023.08.20 val PER: 0.1239
2026-01-04 00:54:26,027: t15.2023.08.25 val PER: 0.0949
2026-01-04 00:54:26,027: t15.2023.08.27 val PER: 0.1977
2026-01-04 00:54:26,027: t15.2023.09.01 val PER: 0.0885
2026-01-04 00:54:26,028: t15.2023.09.03 val PER: 0.1675
2026-01-04 00:54:26,028: t15.2023.09.24 val PER: 0.1396
2026-01-04 00:54:26,028: t15.2023.09.29 val PER: 0.1391
2026-01-04 00:54:26,028: t15.2023.10.01 val PER: 0.1816
2026-01-04 00:54:26,028: t15.2023.10.06 val PER: 0.0915
2026-01-04 00:54:26,028: t15.2023.10.08 val PER: 0.2517
2026-01-04 00:54:26,028: t15.2023.10.13 val PER: 0.2188
2026-01-04 00:54:26,028: t15.2023.10.15 val PER: 0.1608
2026-01-04 00:54:26,028: t15.2023.10.20 val PER: 0.1913
2026-01-04 00:54:26,028: t15.2023.10.22 val PER: 0.1325
2026-01-04 00:54:26,028: t15.2023.11.03 val PER: 0.1805
2026-01-04 00:54:26,028: t15.2023.11.04 val PER: 0.0239
2026-01-04 00:54:26,028: t15.2023.11.17 val PER: 0.0435
2026-01-04 00:54:26,028: t15.2023.11.19 val PER: 0.0519
2026-01-04 00:54:26,028: t15.2023.11.26 val PER: 0.1283
2026-01-04 00:54:26,029: t15.2023.12.03 val PER: 0.1197
2026-01-04 00:54:26,029: t15.2023.12.08 val PER: 0.1119
2026-01-04 00:54:26,029: t15.2023.12.10 val PER: 0.1091
2026-01-04 00:54:26,029: t15.2023.12.17 val PER: 0.1476
2026-01-04 00:54:26,029: t15.2023.12.29 val PER: 0.1448
2026-01-04 00:54:26,029: t15.2024.02.25 val PER: 0.1264
2026-01-04 00:54:26,029: t15.2024.03.08 val PER: 0.2432
2026-01-04 00:54:26,029: t15.2024.03.15 val PER: 0.2076
2026-01-04 00:54:26,029: t15.2024.03.17 val PER: 0.1527
2026-01-04 00:54:26,029: t15.2024.05.10 val PER: 0.1813
2026-01-04 00:54:26,029: t15.2024.06.14 val PER: 0.1893
2026-01-04 00:54:26,029: t15.2024.07.19 val PER: 0.2479
2026-01-04 00:54:26,029: t15.2024.07.21 val PER: 0.1062
2026-01-04 00:54:26,029: t15.2024.07.28 val PER: 0.1463
2026-01-04 00:54:26,029: t15.2025.01.10 val PER: 0.3017
2026-01-04 00:54:26,029: t15.2025.01.12 val PER: 0.1732
2026-01-04 00:54:26,029: t15.2025.03.14 val PER: 0.3432
2026-01-04 00:54:26,030: t15.2025.03.16 val PER: 0.2055
2026-01-04 00:54:26,030: t15.2025.03.30 val PER: 0.3046
2026-01-04 00:54:26,030: t15.2025.04.13 val PER: 0.2197
2026-01-04 00:54:26,285: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_11500
2026-01-04 00:54:34,943: Train batch 11600: loss: 10.56 grad norm: 48.55 time: 0.060
2026-01-04 00:54:52,449: Train batch 11800: loss: 6.22 grad norm: 42.35 time: 0.045
2026-01-04 00:55:09,799: Train batch 12000: loss: 13.06 grad norm: 52.87 time: 0.071
2026-01-04 00:55:09,800: Running test after training batch: 12000
2026-01-04 00:55:09,889: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:55:14,543: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 00:55:14,576: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-04 00:55:16,346: Val batch 12000: PER (avg): 0.1601 CTC Loss (avg): 16.0793 WER(1gram): 49.49% (n=64) time: 6.546
2026-01-04 00:55:16,346: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-04 00:55:16,347: t15.2023.08.13 val PER: 0.1206
2026-01-04 00:55:16,347: t15.2023.08.18 val PER: 0.1098
2026-01-04 00:55:16,347: t15.2023.08.20 val PER: 0.1271
2026-01-04 00:55:16,347: t15.2023.08.25 val PER: 0.0979
2026-01-04 00:55:16,347: t15.2023.08.27 val PER: 0.1929
2026-01-04 00:55:16,347: t15.2023.09.01 val PER: 0.0942
2026-01-04 00:55:16,347: t15.2023.09.03 val PER: 0.1568
2026-01-04 00:55:16,347: t15.2023.09.24 val PER: 0.1250
2026-01-04 00:55:16,347: t15.2023.09.29 val PER: 0.1334
2026-01-04 00:55:16,347: t15.2023.10.01 val PER: 0.1830
2026-01-04 00:55:16,347: t15.2023.10.06 val PER: 0.0947
2026-01-04 00:55:16,347: t15.2023.10.08 val PER: 0.2517
2026-01-04 00:55:16,347: t15.2023.10.13 val PER: 0.2087
2026-01-04 00:55:16,348: t15.2023.10.15 val PER: 0.1648
2026-01-04 00:55:16,348: t15.2023.10.20 val PER: 0.1913
2026-01-04 00:55:16,348: t15.2023.10.22 val PER: 0.1225
2026-01-04 00:55:16,348: t15.2023.11.03 val PER: 0.1879
2026-01-04 00:55:16,348: t15.2023.11.04 val PER: 0.0375
2026-01-04 00:55:16,348: t15.2023.11.17 val PER: 0.0467
2026-01-04 00:55:16,348: t15.2023.11.19 val PER: 0.0379
2026-01-04 00:55:16,348: t15.2023.11.26 val PER: 0.1246
2026-01-04 00:55:16,348: t15.2023.12.03 val PER: 0.1134
2026-01-04 00:55:16,348: t15.2023.12.08 val PER: 0.1085
2026-01-04 00:55:16,348: t15.2023.12.10 val PER: 0.0986
2026-01-04 00:55:16,348: t15.2023.12.17 val PER: 0.1414
2026-01-04 00:55:16,349: t15.2023.12.29 val PER: 0.1482
2026-01-04 00:55:16,349: t15.2024.02.25 val PER: 0.1180
2026-01-04 00:55:16,349: t15.2024.03.08 val PER: 0.2432
2026-01-04 00:55:16,349: t15.2024.03.15 val PER: 0.2039
2026-01-04 00:55:16,349: t15.2024.03.17 val PER: 0.1485
2026-01-04 00:55:16,349: t15.2024.05.10 val PER: 0.1887
2026-01-04 00:55:16,349: t15.2024.06.14 val PER: 0.1940
2026-01-04 00:55:16,349: t15.2024.07.19 val PER: 0.2525
2026-01-04 00:55:16,349: t15.2024.07.21 val PER: 0.1034
2026-01-04 00:55:16,349: t15.2024.07.28 val PER: 0.1426
2026-01-04 00:55:16,349: t15.2025.01.10 val PER: 0.3017
2026-01-04 00:55:16,349: t15.2025.01.12 val PER: 0.1663
2026-01-04 00:55:16,350: t15.2025.03.14 val PER: 0.3506
2026-01-04 00:55:16,350: t15.2025.03.16 val PER: 0.2094
2026-01-04 00:55:16,350: t15.2025.03.30 val PER: 0.3080
2026-01-04 00:55:16,350: t15.2025.04.13 val PER: 0.2297
2026-01-04 00:55:16,611: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_12000
2026-01-04 00:55:34,127: Train batch 12200: loss: 5.72 grad norm: 40.95 time: 0.065
2026-01-04 00:55:51,651: Train batch 12400: loss: 4.84 grad norm: 36.05 time: 0.041
2026-01-04 00:56:00,450: Running test after training batch: 12500
2026-01-04 00:56:00,550: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:56:05,210: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 00:56:05,244: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost edds
2026-01-04 00:56:07,067: Val batch 12500: PER (avg): 0.1562 CTC Loss (avg): 15.8511 WER(1gram): 49.75% (n=64) time: 6.616
2026-01-04 00:56:07,067: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-04 00:56:07,067: t15.2023.08.13 val PER: 0.1164
2026-01-04 00:56:07,067: t15.2023.08.18 val PER: 0.1065
2026-01-04 00:56:07,067: t15.2023.08.20 val PER: 0.1128
2026-01-04 00:56:07,067: t15.2023.08.25 val PER: 0.0828
2026-01-04 00:56:07,067: t15.2023.08.27 val PER: 0.1945
2026-01-04 00:56:07,067: t15.2023.09.01 val PER: 0.0795
2026-01-04 00:56:07,067: t15.2023.09.03 val PER: 0.1591
2026-01-04 00:56:07,067: t15.2023.09.24 val PER: 0.1262
2026-01-04 00:56:07,068: t15.2023.09.29 val PER: 0.1315
2026-01-04 00:56:07,068: t15.2023.10.01 val PER: 0.1651
2026-01-04 00:56:07,068: t15.2023.10.06 val PER: 0.0947
2026-01-04 00:56:07,068: t15.2023.10.08 val PER: 0.2558
2026-01-04 00:56:07,068: t15.2023.10.13 val PER: 0.2009
2026-01-04 00:56:07,068: t15.2023.10.15 val PER: 0.1529
2026-01-04 00:56:07,068: t15.2023.10.20 val PER: 0.1879
2026-01-04 00:56:07,068: t15.2023.10.22 val PER: 0.1203
2026-01-04 00:56:07,068: t15.2023.11.03 val PER: 0.1886
2026-01-04 00:56:07,068: t15.2023.11.04 val PER: 0.0307
2026-01-04 00:56:07,068: t15.2023.11.17 val PER: 0.0482
2026-01-04 00:56:07,068: t15.2023.11.19 val PER: 0.0339
2026-01-04 00:56:07,068: t15.2023.11.26 val PER: 0.1239
2026-01-04 00:56:07,068: t15.2023.12.03 val PER: 0.1187
2026-01-04 00:56:07,069: t15.2023.12.08 val PER: 0.1025
2026-01-04 00:56:07,069: t15.2023.12.10 val PER: 0.1025
2026-01-04 00:56:07,069: t15.2023.12.17 val PER: 0.1414
2026-01-04 00:56:07,069: t15.2023.12.29 val PER: 0.1503
2026-01-04 00:56:07,069: t15.2024.02.25 val PER: 0.1110
2026-01-04 00:56:07,069: t15.2024.03.08 val PER: 0.2447
2026-01-04 00:56:07,069: t15.2024.03.15 val PER: 0.2064
2026-01-04 00:56:07,069: t15.2024.03.17 val PER: 0.1541
2026-01-04 00:56:07,069: t15.2024.05.10 val PER: 0.1649
2026-01-04 00:56:07,069: t15.2024.06.14 val PER: 0.1735
2026-01-04 00:56:07,069: t15.2024.07.19 val PER: 0.2498
2026-01-04 00:56:07,069: t15.2024.07.21 val PER: 0.0986
2026-01-04 00:56:07,069: t15.2024.07.28 val PER: 0.1346
2026-01-04 00:56:07,069: t15.2025.01.10 val PER: 0.3058
2026-01-04 00:56:07,069: t15.2025.01.12 val PER: 0.1601
2026-01-04 00:56:07,069: t15.2025.03.14 val PER: 0.3639
2026-01-04 00:56:07,069: t15.2025.03.16 val PER: 0.2094
2026-01-04 00:56:07,070: t15.2025.03.30 val PER: 0.3034
2026-01-04 00:56:07,070: t15.2025.04.13 val PER: 0.2168
2026-01-04 00:56:07,329: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_12500
2026-01-04 00:56:16,137: Train batch 12600: loss: 7.70 grad norm: 45.13 time: 0.058
2026-01-04 00:56:33,763: Train batch 12800: loss: 5.81 grad norm: 39.88 time: 0.052
2026-01-04 00:56:51,653: Train batch 13000: loss: 6.09 grad norm: 41.90 time: 0.066
2026-01-04 00:56:51,653: Running test after training batch: 13000
2026-01-04 00:56:51,761: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:56:56,399: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:56:56,431: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost ennes
2026-01-04 00:56:58,233: Val batch 13000: PER (avg): 0.1549 CTC Loss (avg): 15.7109 WER(1gram): 46.95% (n=64) time: 6.580
2026-01-04 00:56:58,234: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-04 00:56:58,234: t15.2023.08.13 val PER: 0.1123
2026-01-04 00:56:58,234: t15.2023.08.18 val PER: 0.1090
2026-01-04 00:56:58,234: t15.2023.08.20 val PER: 0.1072
2026-01-04 00:56:58,234: t15.2023.08.25 val PER: 0.0798
2026-01-04 00:56:58,234: t15.2023.08.27 val PER: 0.1897
2026-01-04 00:56:58,234: t15.2023.09.01 val PER: 0.0852
2026-01-04 00:56:58,234: t15.2023.09.03 val PER: 0.1710
2026-01-04 00:56:58,234: t15.2023.09.24 val PER: 0.1238
2026-01-04 00:56:58,234: t15.2023.09.29 val PER: 0.1302
2026-01-04 00:56:58,235: t15.2023.10.01 val PER: 0.1770
2026-01-04 00:56:58,235: t15.2023.10.06 val PER: 0.0904
2026-01-04 00:56:58,235: t15.2023.10.08 val PER: 0.2463
2026-01-04 00:56:58,235: t15.2023.10.13 val PER: 0.1994
2026-01-04 00:56:58,235: t15.2023.10.15 val PER: 0.1562
2026-01-04 00:56:58,235: t15.2023.10.20 val PER: 0.1644
2026-01-04 00:56:58,235: t15.2023.10.22 val PER: 0.1192
2026-01-04 00:56:58,235: t15.2023.11.03 val PER: 0.1872
2026-01-04 00:56:58,236: t15.2023.11.04 val PER: 0.0341
2026-01-04 00:56:58,236: t15.2023.11.17 val PER: 0.0435
2026-01-04 00:56:58,236: t15.2023.11.19 val PER: 0.0399
2026-01-04 00:56:58,236: t15.2023.11.26 val PER: 0.1145
2026-01-04 00:56:58,236: t15.2023.12.03 val PER: 0.1176
2026-01-04 00:56:58,236: t15.2023.12.08 val PER: 0.1065
2026-01-04 00:56:58,236: t15.2023.12.10 val PER: 0.1012
2026-01-04 00:56:58,236: t15.2023.12.17 val PER: 0.1403
2026-01-04 00:56:58,236: t15.2023.12.29 val PER: 0.1428
2026-01-04 00:56:58,236: t15.2024.02.25 val PER: 0.1067
2026-01-04 00:56:58,236: t15.2024.03.08 val PER: 0.2390
2026-01-04 00:56:58,236: t15.2024.03.15 val PER: 0.2008
2026-01-04 00:56:58,236: t15.2024.03.17 val PER: 0.1416
2026-01-04 00:56:58,236: t15.2024.05.10 val PER: 0.1724
2026-01-04 00:56:58,236: t15.2024.06.14 val PER: 0.1782
2026-01-04 00:56:58,236: t15.2024.07.19 val PER: 0.2525
2026-01-04 00:56:58,236: t15.2024.07.21 val PER: 0.1007
2026-01-04 00:56:58,237: t15.2024.07.28 val PER: 0.1500
2026-01-04 00:56:58,237: t15.2025.01.10 val PER: 0.2920
2026-01-04 00:56:58,237: t15.2025.01.12 val PER: 0.1609
2026-01-04 00:56:58,237: t15.2025.03.14 val PER: 0.3417
2026-01-04 00:56:58,237: t15.2025.03.16 val PER: 0.1832
2026-01-04 00:56:58,237: t15.2025.03.30 val PER: 0.3080
2026-01-04 00:56:58,237: t15.2025.04.13 val PER: 0.2211
2026-01-04 00:56:58,238: New best val WER(1gram) 48.48% --> 46.95%
2026-01-04 00:56:58,238: Checkpointing model
2026-01-04 00:56:58,820: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/best_checkpoint
2026-01-04 00:56:59,086: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_13000
2026-01-04 00:57:16,673: Train batch 13200: loss: 12.35 grad norm: 64.48 time: 0.054
2026-01-04 00:57:34,126: Train batch 13400: loss: 8.71 grad norm: 53.30 time: 0.062
2026-01-04 00:57:42,799: Running test after training batch: 13500
2026-01-04 00:57:42,897: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:57:47,539: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 00:57:47,572: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost get
2026-01-04 00:57:49,402: Val batch 13500: PER (avg): 0.1531 CTC Loss (avg): 15.4508 WER(1gram): 47.72% (n=64) time: 6.602
2026-01-04 00:57:49,402: WER lens: avg_true_words=6.16 avg_pred_words=6.27 max_pred_words=12
2026-01-04 00:57:49,402: t15.2023.08.13 val PER: 0.1071
2026-01-04 00:57:49,402: t15.2023.08.18 val PER: 0.1106
2026-01-04 00:57:49,402: t15.2023.08.20 val PER: 0.1080
2026-01-04 00:57:49,402: t15.2023.08.25 val PER: 0.0813
2026-01-04 00:57:49,402: t15.2023.08.27 val PER: 0.1881
2026-01-04 00:57:49,403: t15.2023.09.01 val PER: 0.0836
2026-01-04 00:57:49,403: t15.2023.09.03 val PER: 0.1663
2026-01-04 00:57:49,403: t15.2023.09.24 val PER: 0.1189
2026-01-04 00:57:49,403: t15.2023.09.29 val PER: 0.1302
2026-01-04 00:57:49,403: t15.2023.10.01 val PER: 0.1757
2026-01-04 00:57:49,403: t15.2023.10.06 val PER: 0.0936
2026-01-04 00:57:49,403: t15.2023.10.08 val PER: 0.2598
2026-01-04 00:57:49,403: t15.2023.10.13 val PER: 0.1971
2026-01-04 00:57:49,403: t15.2023.10.15 val PER: 0.1529
2026-01-04 00:57:49,403: t15.2023.10.20 val PER: 0.1779
2026-01-04 00:57:49,403: t15.2023.10.22 val PER: 0.1192
2026-01-04 00:57:49,403: t15.2023.11.03 val PER: 0.1784
2026-01-04 00:57:49,403: t15.2023.11.04 val PER: 0.0444
2026-01-04 00:57:49,403: t15.2023.11.17 val PER: 0.0467
2026-01-04 00:57:49,403: t15.2023.11.19 val PER: 0.0359
2026-01-04 00:57:49,404: t15.2023.11.26 val PER: 0.1203
2026-01-04 00:57:49,404: t15.2023.12.03 val PER: 0.1071
2026-01-04 00:57:49,404: t15.2023.12.08 val PER: 0.1079
2026-01-04 00:57:49,404: t15.2023.12.10 val PER: 0.0933
2026-01-04 00:57:49,404: t15.2023.12.17 val PER: 0.1310
2026-01-04 00:57:49,404: t15.2023.12.29 val PER: 0.1366
2026-01-04 00:57:49,404: t15.2024.02.25 val PER: 0.1124
2026-01-04 00:57:49,404: t15.2024.03.08 val PER: 0.2347
2026-01-04 00:57:49,404: t15.2024.03.15 val PER: 0.1982
2026-01-04 00:57:49,404: t15.2024.03.17 val PER: 0.1485
2026-01-04 00:57:49,404: t15.2024.05.10 val PER: 0.1664
2026-01-04 00:57:49,405: t15.2024.06.14 val PER: 0.1672
2026-01-04 00:57:49,405: t15.2024.07.19 val PER: 0.2459
2026-01-04 00:57:49,405: t15.2024.07.21 val PER: 0.0986
2026-01-04 00:57:49,405: t15.2024.07.28 val PER: 0.1375
2026-01-04 00:57:49,405: t15.2025.01.10 val PER: 0.2948
2026-01-04 00:57:49,405: t15.2025.01.12 val PER: 0.1663
2026-01-04 00:57:49,405: t15.2025.03.14 val PER: 0.3462
2026-01-04 00:57:49,405: t15.2025.03.16 val PER: 0.1898
2026-01-04 00:57:49,405: t15.2025.03.30 val PER: 0.2943
2026-01-04 00:57:49,405: t15.2025.04.13 val PER: 0.2140
2026-01-04 00:57:49,660: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_13500
2026-01-04 00:57:58,453: Train batch 13600: loss: 12.31 grad norm: 63.88 time: 0.062
2026-01-04 00:58:16,496: Train batch 13800: loss: 8.97 grad norm: 58.14 time: 0.056
2026-01-04 00:58:34,492: Train batch 14000: loss: 11.03 grad norm: 58.33 time: 0.051
2026-01-04 00:58:34,492: Running test after training batch: 14000
2026-01-04 00:58:34,643: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:58:39,382: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 00:58:39,415: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 00:58:41,250: Val batch 14000: PER (avg): 0.1521 CTC Loss (avg): 15.4777 WER(1gram): 47.46% (n=64) time: 6.758
2026-01-04 00:58:41,250: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 00:58:41,250: t15.2023.08.13 val PER: 0.1154
2026-01-04 00:58:41,250: t15.2023.08.18 val PER: 0.1014
2026-01-04 00:58:41,250: t15.2023.08.20 val PER: 0.1136
2026-01-04 00:58:41,251: t15.2023.08.25 val PER: 0.0889
2026-01-04 00:58:41,251: t15.2023.08.27 val PER: 0.1720
2026-01-04 00:58:41,251: t15.2023.09.01 val PER: 0.0836
2026-01-04 00:58:41,251: t15.2023.09.03 val PER: 0.1698
2026-01-04 00:58:41,251: t15.2023.09.24 val PER: 0.1250
2026-01-04 00:58:41,251: t15.2023.09.29 val PER: 0.1283
2026-01-04 00:58:41,251: t15.2023.10.01 val PER: 0.1783
2026-01-04 00:58:41,251: t15.2023.10.06 val PER: 0.0861
2026-01-04 00:58:41,251: t15.2023.10.08 val PER: 0.2639
2026-01-04 00:58:41,251: t15.2023.10.13 val PER: 0.2079
2026-01-04 00:58:41,251: t15.2023.10.15 val PER: 0.1589
2026-01-04 00:58:41,251: t15.2023.10.20 val PER: 0.2047
2026-01-04 00:58:41,251: t15.2023.10.22 val PER: 0.1147
2026-01-04 00:58:41,251: t15.2023.11.03 val PER: 0.1798
2026-01-04 00:58:41,251: t15.2023.11.04 val PER: 0.0307
2026-01-04 00:58:41,251: t15.2023.11.17 val PER: 0.0420
2026-01-04 00:58:41,251: t15.2023.11.19 val PER: 0.0359
2026-01-04 00:58:41,252: t15.2023.11.26 val PER: 0.1246
2026-01-04 00:58:41,252: t15.2023.12.03 val PER: 0.1155
2026-01-04 00:58:41,252: t15.2023.12.08 val PER: 0.1119
2026-01-04 00:58:41,252: t15.2023.12.10 val PER: 0.1012
2026-01-04 00:58:41,252: t15.2023.12.17 val PER: 0.1310
2026-01-04 00:58:41,252: t15.2023.12.29 val PER: 0.1256
2026-01-04 00:58:41,252: t15.2024.02.25 val PER: 0.1067
2026-01-04 00:58:41,252: t15.2024.03.08 val PER: 0.2233
2026-01-04 00:58:41,252: t15.2024.03.15 val PER: 0.2045
2026-01-04 00:58:41,252: t15.2024.03.17 val PER: 0.1423
2026-01-04 00:58:41,252: t15.2024.05.10 val PER: 0.1530
2026-01-04 00:58:41,252: t15.2024.06.14 val PER: 0.1798
2026-01-04 00:58:41,253: t15.2024.07.19 val PER: 0.2386
2026-01-04 00:58:41,253: t15.2024.07.21 val PER: 0.0890
2026-01-04 00:58:41,253: t15.2024.07.28 val PER: 0.1287
2026-01-04 00:58:41,253: t15.2025.01.10 val PER: 0.2851
2026-01-04 00:58:41,253: t15.2025.01.12 val PER: 0.1609
2026-01-04 00:58:41,253: t15.2025.03.14 val PER: 0.3358
2026-01-04 00:58:41,253: t15.2025.03.16 val PER: 0.1832
2026-01-04 00:58:41,253: t15.2025.03.30 val PER: 0.2908
2026-01-04 00:58:41,253: t15.2025.04.13 val PER: 0.2126
2026-01-04 00:58:41,512: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_14000
2026-01-04 00:58:59,053: Train batch 14200: loss: 7.97 grad norm: 51.90 time: 0.056
2026-01-04 00:59:17,140: Train batch 14400: loss: 5.52 grad norm: 38.30 time: 0.064
2026-01-04 00:59:25,973: Running test after training batch: 14500
2026-01-04 00:59:26,065: WER debug GT example: You can see the code at this point as well.
2026-01-04 00:59:30,714: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 00:59:30,746: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-04 00:59:32,596: Val batch 14500: PER (avg): 0.1516 CTC Loss (avg): 15.4750 WER(1gram): 50.51% (n=64) time: 6.623
2026-01-04 00:59:32,597: WER lens: avg_true_words=6.16 avg_pred_words=6.31 max_pred_words=12
2026-01-04 00:59:32,597: t15.2023.08.13 val PER: 0.1123
2026-01-04 00:59:32,597: t15.2023.08.18 val PER: 0.1023
2026-01-04 00:59:32,597: t15.2023.08.20 val PER: 0.1072
2026-01-04 00:59:32,597: t15.2023.08.25 val PER: 0.0828
2026-01-04 00:59:32,597: t15.2023.08.27 val PER: 0.1801
2026-01-04 00:59:32,597: t15.2023.09.01 val PER: 0.0812
2026-01-04 00:59:32,597: t15.2023.09.03 val PER: 0.1651
2026-01-04 00:59:32,597: t15.2023.09.24 val PER: 0.1214
2026-01-04 00:59:32,597: t15.2023.09.29 val PER: 0.1270
2026-01-04 00:59:32,598: t15.2023.10.01 val PER: 0.1803
2026-01-04 00:59:32,598: t15.2023.10.06 val PER: 0.0926
2026-01-04 00:59:32,598: t15.2023.10.08 val PER: 0.2571
2026-01-04 00:59:32,598: t15.2023.10.13 val PER: 0.2064
2026-01-04 00:59:32,598: t15.2023.10.15 val PER: 0.1575
2026-01-04 00:59:32,598: t15.2023.10.20 val PER: 0.1846
2026-01-04 00:59:32,598: t15.2023.10.22 val PER: 0.1169
2026-01-04 00:59:32,598: t15.2023.11.03 val PER: 0.1832
2026-01-04 00:59:32,598: t15.2023.11.04 val PER: 0.0410
2026-01-04 00:59:32,598: t15.2023.11.17 val PER: 0.0467
2026-01-04 00:59:32,598: t15.2023.11.19 val PER: 0.0359
2026-01-04 00:59:32,598: t15.2023.11.26 val PER: 0.1167
2026-01-04 00:59:32,598: t15.2023.12.03 val PER: 0.1050
2026-01-04 00:59:32,599: t15.2023.12.08 val PER: 0.0979
2026-01-04 00:59:32,599: t15.2023.12.10 val PER: 0.0894
2026-01-04 00:59:32,599: t15.2023.12.17 val PER: 0.1455
2026-01-04 00:59:32,599: t15.2023.12.29 val PER: 0.1338
2026-01-04 00:59:32,599: t15.2024.02.25 val PER: 0.1110
2026-01-04 00:59:32,599: t15.2024.03.08 val PER: 0.2333
2026-01-04 00:59:32,599: t15.2024.03.15 val PER: 0.2020
2026-01-04 00:59:32,599: t15.2024.03.17 val PER: 0.1457
2026-01-04 00:59:32,599: t15.2024.05.10 val PER: 0.1605
2026-01-04 00:59:32,599: t15.2024.06.14 val PER: 0.1703
2026-01-04 00:59:32,599: t15.2024.07.19 val PER: 0.2399
2026-01-04 00:59:32,599: t15.2024.07.21 val PER: 0.0931
2026-01-04 00:59:32,599: t15.2024.07.28 val PER: 0.1346
2026-01-04 00:59:32,599: t15.2025.01.10 val PER: 0.2865
2026-01-04 00:59:32,599: t15.2025.01.12 val PER: 0.1524
2026-01-04 00:59:32,599: t15.2025.03.14 val PER: 0.3565
2026-01-04 00:59:32,600: t15.2025.03.16 val PER: 0.1859
2026-01-04 00:59:32,600: t15.2025.03.30 val PER: 0.2782
2026-01-04 00:59:32,600: t15.2025.04.13 val PER: 0.2111
2026-01-04 00:59:32,858: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_14500
2026-01-04 00:59:41,572: Train batch 14600: loss: 12.17 grad norm: 62.59 time: 0.058
2026-01-04 00:59:59,134: Train batch 14800: loss: 5.52 grad norm: 44.71 time: 0.050
2026-01-04 01:00:16,641: Train batch 15000: loss: 8.80 grad norm: 51.83 time: 0.052
2026-01-04 01:00:16,641: Running test after training batch: 15000
2026-01-04 01:00:16,766: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:00:21,474: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 01:00:21,506: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost et
2026-01-04 01:00:23,350: Val batch 15000: PER (avg): 0.1494 CTC Loss (avg): 15.2464 WER(1gram): 47.97% (n=64) time: 6.709
2026-01-04 01:00:23,350: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 01:00:23,350: t15.2023.08.13 val PER: 0.1091
2026-01-04 01:00:23,351: t15.2023.08.18 val PER: 0.1031
2026-01-04 01:00:23,351: t15.2023.08.20 val PER: 0.1096
2026-01-04 01:00:23,351: t15.2023.08.25 val PER: 0.0858
2026-01-04 01:00:23,351: t15.2023.08.27 val PER: 0.1849
2026-01-04 01:00:23,351: t15.2023.09.01 val PER: 0.0828
2026-01-04 01:00:23,351: t15.2023.09.03 val PER: 0.1425
2026-01-04 01:00:23,351: t15.2023.09.24 val PER: 0.1201
2026-01-04 01:00:23,351: t15.2023.09.29 val PER: 0.1289
2026-01-04 01:00:23,351: t15.2023.10.01 val PER: 0.1717
2026-01-04 01:00:23,351: t15.2023.10.06 val PER: 0.0829
2026-01-04 01:00:23,351: t15.2023.10.08 val PER: 0.2517
2026-01-04 01:00:23,351: t15.2023.10.13 val PER: 0.1947
2026-01-04 01:00:23,352: t15.2023.10.15 val PER: 0.1516
2026-01-04 01:00:23,352: t15.2023.10.20 val PER: 0.1946
2026-01-04 01:00:23,352: t15.2023.10.22 val PER: 0.1192
2026-01-04 01:00:23,352: t15.2023.11.03 val PER: 0.1737
2026-01-04 01:00:23,352: t15.2023.11.04 val PER: 0.0375
2026-01-04 01:00:23,352: t15.2023.11.17 val PER: 0.0420
2026-01-04 01:00:23,352: t15.2023.11.19 val PER: 0.0359
2026-01-04 01:00:23,352: t15.2023.11.26 val PER: 0.1145
2026-01-04 01:00:23,352: t15.2023.12.03 val PER: 0.1124
2026-01-04 01:00:23,352: t15.2023.12.08 val PER: 0.0952
2026-01-04 01:00:23,352: t15.2023.12.10 val PER: 0.0933
2026-01-04 01:00:23,352: t15.2023.12.17 val PER: 0.1486
2026-01-04 01:00:23,352: t15.2023.12.29 val PER: 0.1304
2026-01-04 01:00:23,352: t15.2024.02.25 val PER: 0.1025
2026-01-04 01:00:23,352: t15.2024.03.08 val PER: 0.2319
2026-01-04 01:00:23,352: t15.2024.03.15 val PER: 0.1982
2026-01-04 01:00:23,353: t15.2024.03.17 val PER: 0.1367
2026-01-04 01:00:23,353: t15.2024.05.10 val PER: 0.1664
2026-01-04 01:00:23,353: t15.2024.06.14 val PER: 0.1782
2026-01-04 01:00:23,353: t15.2024.07.19 val PER: 0.2353
2026-01-04 01:00:23,353: t15.2024.07.21 val PER: 0.0972
2026-01-04 01:00:23,353: t15.2024.07.28 val PER: 0.1324
2026-01-04 01:00:23,353: t15.2025.01.10 val PER: 0.2865
2026-01-04 01:00:23,353: t15.2025.01.12 val PER: 0.1509
2026-01-04 01:00:23,353: t15.2025.03.14 val PER: 0.3373
2026-01-04 01:00:23,353: t15.2025.03.16 val PER: 0.1924
2026-01-04 01:00:23,353: t15.2025.03.30 val PER: 0.2885
2026-01-04 01:00:23,353: t15.2025.04.13 val PER: 0.2097
2026-01-04 01:00:23,608: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_15000
2026-01-04 01:00:41,379: Train batch 15200: loss: 4.88 grad norm: 39.58 time: 0.057
2026-01-04 01:00:58,621: Train batch 15400: loss: 10.76 grad norm: 59.43 time: 0.050
2026-01-04 01:01:07,424: Running test after training batch: 15500
2026-01-04 01:01:07,519: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:01:12,095: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:01:12,128: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost edds
2026-01-04 01:01:13,977: Val batch 15500: PER (avg): 0.1481 CTC Loss (avg): 15.1246 WER(1gram): 46.19% (n=64) time: 6.553
2026-01-04 01:01:13,978: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 01:01:13,978: t15.2023.08.13 val PER: 0.1133
2026-01-04 01:01:13,978: t15.2023.08.18 val PER: 0.1006
2026-01-04 01:01:13,978: t15.2023.08.20 val PER: 0.1096
2026-01-04 01:01:13,978: t15.2023.08.25 val PER: 0.0858
2026-01-04 01:01:13,978: t15.2023.08.27 val PER: 0.1768
2026-01-04 01:01:13,978: t15.2023.09.01 val PER: 0.0755
2026-01-04 01:01:13,978: t15.2023.09.03 val PER: 0.1544
2026-01-04 01:01:13,979: t15.2023.09.24 val PER: 0.1214
2026-01-04 01:01:13,979: t15.2023.09.29 val PER: 0.1206
2026-01-04 01:01:13,979: t15.2023.10.01 val PER: 0.1704
2026-01-04 01:01:13,979: t15.2023.10.06 val PER: 0.0883
2026-01-04 01:01:13,979: t15.2023.10.08 val PER: 0.2395
2026-01-04 01:01:13,979: t15.2023.10.13 val PER: 0.2009
2026-01-04 01:01:13,979: t15.2023.10.15 val PER: 0.1543
2026-01-04 01:01:13,979: t15.2023.10.20 val PER: 0.1846
2026-01-04 01:01:13,979: t15.2023.10.22 val PER: 0.1180
2026-01-04 01:01:13,980: t15.2023.11.03 val PER: 0.1750
2026-01-04 01:01:13,980: t15.2023.11.04 val PER: 0.0375
2026-01-04 01:01:13,980: t15.2023.11.17 val PER: 0.0373
2026-01-04 01:01:13,980: t15.2023.11.19 val PER: 0.0359
2026-01-04 01:01:13,980: t15.2023.11.26 val PER: 0.1116
2026-01-04 01:01:13,980: t15.2023.12.03 val PER: 0.1155
2026-01-04 01:01:13,980: t15.2023.12.08 val PER: 0.0992
2026-01-04 01:01:13,980: t15.2023.12.10 val PER: 0.0880
2026-01-04 01:01:13,980: t15.2023.12.17 val PER: 0.1372
2026-01-04 01:01:13,980: t15.2023.12.29 val PER: 0.1304
2026-01-04 01:01:13,980: t15.2024.02.25 val PER: 0.1053
2026-01-04 01:01:13,980: t15.2024.03.08 val PER: 0.2319
2026-01-04 01:01:13,980: t15.2024.03.15 val PER: 0.1970
2026-01-04 01:01:13,981: t15.2024.03.17 val PER: 0.1381
2026-01-04 01:01:13,981: t15.2024.05.10 val PER: 0.1516
2026-01-04 01:01:13,981: t15.2024.06.14 val PER: 0.1640
2026-01-04 01:01:13,981: t15.2024.07.19 val PER: 0.2340
2026-01-04 01:01:13,981: t15.2024.07.21 val PER: 0.0952
2026-01-04 01:01:13,981: t15.2024.07.28 val PER: 0.1324
2026-01-04 01:01:13,981: t15.2025.01.10 val PER: 0.2741
2026-01-04 01:01:13,981: t15.2025.01.12 val PER: 0.1555
2026-01-04 01:01:13,981: t15.2025.03.14 val PER: 0.3328
2026-01-04 01:01:13,981: t15.2025.03.16 val PER: 0.1872
2026-01-04 01:01:13,981: t15.2025.03.30 val PER: 0.2931
2026-01-04 01:01:13,981: t15.2025.04.13 val PER: 0.2068
2026-01-04 01:01:13,982: New best val WER(1gram) 46.95% --> 46.19%
2026-01-04 01:01:13,982: Checkpointing model
2026-01-04 01:01:14,602: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/best_checkpoint
2026-01-04 01:01:14,856: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_15500
2026-01-04 01:01:23,449: Train batch 15600: loss: 11.36 grad norm: 61.01 time: 0.062
2026-01-04 01:01:40,960: Train batch 15800: loss: 13.48 grad norm: 63.92 time: 0.067
2026-01-04 01:01:58,425: Train batch 16000: loss: 7.86 grad norm: 43.87 time: 0.055
2026-01-04 01:01:58,426: Running test after training batch: 16000
2026-01-04 01:01:58,527: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:02:03,207: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:02:03,242: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 01:02:05,116: Val batch 16000: PER (avg): 0.1494 CTC Loss (avg): 15.2190 WER(1gram): 47.97% (n=64) time: 6.690
2026-01-04 01:02:05,116: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-04 01:02:05,116: t15.2023.08.13 val PER: 0.1091
2026-01-04 01:02:05,116: t15.2023.08.18 val PER: 0.1031
2026-01-04 01:02:05,116: t15.2023.08.20 val PER: 0.1104
2026-01-04 01:02:05,116: t15.2023.08.25 val PER: 0.0798
2026-01-04 01:02:05,116: t15.2023.08.27 val PER: 0.1801
2026-01-04 01:02:05,116: t15.2023.09.01 val PER: 0.0787
2026-01-04 01:02:05,116: t15.2023.09.03 val PER: 0.1544
2026-01-04 01:02:05,117: t15.2023.09.24 val PER: 0.1117
2026-01-04 01:02:05,117: t15.2023.09.29 val PER: 0.1270
2026-01-04 01:02:05,117: t15.2023.10.01 val PER: 0.1797
2026-01-04 01:02:05,117: t15.2023.10.06 val PER: 0.0893
2026-01-04 01:02:05,117: t15.2023.10.08 val PER: 0.2558
2026-01-04 01:02:05,117: t15.2023.10.13 val PER: 0.1986
2026-01-04 01:02:05,117: t15.2023.10.15 val PER: 0.1470
2026-01-04 01:02:05,117: t15.2023.10.20 val PER: 0.1913
2026-01-04 01:02:05,117: t15.2023.10.22 val PER: 0.1047
2026-01-04 01:02:05,117: t15.2023.11.03 val PER: 0.1750
2026-01-04 01:02:05,117: t15.2023.11.04 val PER: 0.0444
2026-01-04 01:02:05,117: t15.2023.11.17 val PER: 0.0389
2026-01-04 01:02:05,117: t15.2023.11.19 val PER: 0.0439
2026-01-04 01:02:05,117: t15.2023.11.26 val PER: 0.1181
2026-01-04 01:02:05,117: t15.2023.12.03 val PER: 0.1124
2026-01-04 01:02:05,118: t15.2023.12.08 val PER: 0.0985
2026-01-04 01:02:05,118: t15.2023.12.10 val PER: 0.0867
2026-01-04 01:02:05,118: t15.2023.12.17 val PER: 0.1393
2026-01-04 01:02:05,118: t15.2023.12.29 val PER: 0.1270
2026-01-04 01:02:05,118: t15.2024.02.25 val PER: 0.1067
2026-01-04 01:02:05,118: t15.2024.03.08 val PER: 0.2219
2026-01-04 01:02:05,118: t15.2024.03.15 val PER: 0.2051
2026-01-04 01:02:05,118: t15.2024.03.17 val PER: 0.1325
2026-01-04 01:02:05,118: t15.2024.05.10 val PER: 0.1605
2026-01-04 01:02:05,118: t15.2024.06.14 val PER: 0.1672
2026-01-04 01:02:05,118: t15.2024.07.19 val PER: 0.2399
2026-01-04 01:02:05,118: t15.2024.07.21 val PER: 0.0952
2026-01-04 01:02:05,118: t15.2024.07.28 val PER: 0.1368
2026-01-04 01:02:05,118: t15.2025.01.10 val PER: 0.2920
2026-01-04 01:02:05,118: t15.2025.01.12 val PER: 0.1470
2026-01-04 01:02:05,118: t15.2025.03.14 val PER: 0.3462
2026-01-04 01:02:05,118: t15.2025.03.16 val PER: 0.1950
2026-01-04 01:02:05,119: t15.2025.03.30 val PER: 0.2851
2026-01-04 01:02:05,119: t15.2025.04.13 val PER: 0.2154
2026-01-04 01:02:05,374: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_16000
2026-01-04 01:02:22,577: Train batch 16200: loss: 5.99 grad norm: 42.44 time: 0.055
2026-01-04 01:02:39,833: Train batch 16400: loss: 10.14 grad norm: 63.53 time: 0.057
2026-01-04 01:02:48,524: Running test after training batch: 16500
2026-01-04 01:02:48,647: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:02:53,738: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:02:53,770: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost edds
2026-01-04 01:02:55,660: Val batch 16500: PER (avg): 0.1483 CTC Loss (avg): 15.1063 WER(1gram): 46.70% (n=64) time: 7.135
2026-01-04 01:02:55,660: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-04 01:02:55,660: t15.2023.08.13 val PER: 0.1112
2026-01-04 01:02:55,660: t15.2023.08.18 val PER: 0.1014
2026-01-04 01:02:55,660: t15.2023.08.20 val PER: 0.1152
2026-01-04 01:02:55,660: t15.2023.08.25 val PER: 0.0798
2026-01-04 01:02:55,661: t15.2023.08.27 val PER: 0.1752
2026-01-04 01:02:55,661: t15.2023.09.01 val PER: 0.0787
2026-01-04 01:02:55,661: t15.2023.09.03 val PER: 0.1544
2026-01-04 01:02:55,661: t15.2023.09.24 val PER: 0.1226
2026-01-04 01:02:55,661: t15.2023.09.29 val PER: 0.1219
2026-01-04 01:02:55,661: t15.2023.10.01 val PER: 0.1783
2026-01-04 01:02:55,661: t15.2023.10.06 val PER: 0.0861
2026-01-04 01:02:55,661: t15.2023.10.08 val PER: 0.2436
2026-01-04 01:02:55,661: t15.2023.10.13 val PER: 0.1916
2026-01-04 01:02:55,661: t15.2023.10.15 val PER: 0.1556
2026-01-04 01:02:55,661: t15.2023.10.20 val PER: 0.1913
2026-01-04 01:02:55,662: t15.2023.10.22 val PER: 0.1114
2026-01-04 01:02:55,662: t15.2023.11.03 val PER: 0.1757
2026-01-04 01:02:55,662: t15.2023.11.04 val PER: 0.0341
2026-01-04 01:02:55,662: t15.2023.11.17 val PER: 0.0389
2026-01-04 01:02:55,662: t15.2023.11.19 val PER: 0.0359
2026-01-04 01:02:55,662: t15.2023.11.26 val PER: 0.1101
2026-01-04 01:02:55,662: t15.2023.12.03 val PER: 0.1145
2026-01-04 01:02:55,662: t15.2023.12.08 val PER: 0.0945
2026-01-04 01:02:55,662: t15.2023.12.10 val PER: 0.0880
2026-01-04 01:02:55,662: t15.2023.12.17 val PER: 0.1362
2026-01-04 01:02:55,662: t15.2023.12.29 val PER: 0.1235
2026-01-04 01:02:55,662: t15.2024.02.25 val PER: 0.1053
2026-01-04 01:02:55,662: t15.2024.03.08 val PER: 0.2304
2026-01-04 01:02:55,662: t15.2024.03.15 val PER: 0.2033
2026-01-04 01:02:55,662: t15.2024.03.17 val PER: 0.1311
2026-01-04 01:02:55,663: t15.2024.05.10 val PER: 0.1516
2026-01-04 01:02:55,663: t15.2024.06.14 val PER: 0.1656
2026-01-04 01:02:55,663: t15.2024.07.19 val PER: 0.2413
2026-01-04 01:02:55,663: t15.2024.07.21 val PER: 0.0938
2026-01-04 01:02:55,663: t15.2024.07.28 val PER: 0.1324
2026-01-04 01:02:55,663: t15.2025.01.10 val PER: 0.2796
2026-01-04 01:02:55,663: t15.2025.01.12 val PER: 0.1524
2026-01-04 01:02:55,663: t15.2025.03.14 val PER: 0.3402
2026-01-04 01:02:55,663: t15.2025.03.16 val PER: 0.1963
2026-01-04 01:02:55,663: t15.2025.03.30 val PER: 0.2874
2026-01-04 01:02:55,663: t15.2025.04.13 val PER: 0.2140
2026-01-04 01:02:55,923: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_16500
2026-01-04 01:03:04,787: Train batch 16600: loss: 7.79 grad norm: 53.46 time: 0.052
2026-01-04 01:03:22,358: Train batch 16800: loss: 16.27 grad norm: 77.02 time: 0.061
2026-01-04 01:03:40,092: Train batch 17000: loss: 7.82 grad norm: 51.63 time: 0.081
2026-01-04 01:03:40,092: Running test after training batch: 17000
2026-01-04 01:03:40,187: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:03:44,868: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:03:44,901: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ennes
2026-01-04 01:03:46,817: Val batch 17000: PER (avg): 0.1469 CTC Loss (avg): 14.9586 WER(1gram): 47.21% (n=64) time: 6.724
2026-01-04 01:03:46,817: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-04 01:03:46,818: t15.2023.08.13 val PER: 0.1123
2026-01-04 01:03:46,818: t15.2023.08.18 val PER: 0.1031
2026-01-04 01:03:46,818: t15.2023.08.20 val PER: 0.1112
2026-01-04 01:03:46,818: t15.2023.08.25 val PER: 0.0798
2026-01-04 01:03:46,818: t15.2023.08.27 val PER: 0.1688
2026-01-04 01:03:46,818: t15.2023.09.01 val PER: 0.0763
2026-01-04 01:03:46,818: t15.2023.09.03 val PER: 0.1580
2026-01-04 01:03:46,819: t15.2023.09.24 val PER: 0.1214
2026-01-04 01:03:46,819: t15.2023.09.29 val PER: 0.1257
2026-01-04 01:03:46,819: t15.2023.10.01 val PER: 0.1724
2026-01-04 01:03:46,819: t15.2023.10.06 val PER: 0.0829
2026-01-04 01:03:46,819: t15.2023.10.08 val PER: 0.2490
2026-01-04 01:03:46,819: t15.2023.10.13 val PER: 0.1955
2026-01-04 01:03:46,819: t15.2023.10.15 val PER: 0.1503
2026-01-04 01:03:46,819: t15.2023.10.20 val PER: 0.1812
2026-01-04 01:03:46,819: t15.2023.10.22 val PER: 0.1102
2026-01-04 01:03:46,819: t15.2023.11.03 val PER: 0.1750
2026-01-04 01:03:46,819: t15.2023.11.04 val PER: 0.0341
2026-01-04 01:03:46,820: t15.2023.11.17 val PER: 0.0373
2026-01-04 01:03:46,820: t15.2023.11.19 val PER: 0.0319
2026-01-04 01:03:46,820: t15.2023.11.26 val PER: 0.1116
2026-01-04 01:03:46,820: t15.2023.12.03 val PER: 0.1145
2026-01-04 01:03:46,820: t15.2023.12.08 val PER: 0.0939
2026-01-04 01:03:46,820: t15.2023.12.10 val PER: 0.0854
2026-01-04 01:03:46,820: t15.2023.12.17 val PER: 0.1341
2026-01-04 01:03:46,820: t15.2023.12.29 val PER: 0.1249
2026-01-04 01:03:46,820: t15.2024.02.25 val PER: 0.1053
2026-01-04 01:03:46,820: t15.2024.03.08 val PER: 0.2347
2026-01-04 01:03:46,820: t15.2024.03.15 val PER: 0.2033
2026-01-04 01:03:46,820: t15.2024.03.17 val PER: 0.1332
2026-01-04 01:03:46,821: t15.2024.05.10 val PER: 0.1530
2026-01-04 01:03:46,821: t15.2024.06.14 val PER: 0.1672
2026-01-04 01:03:46,821: t15.2024.07.19 val PER: 0.2347
2026-01-04 01:03:46,821: t15.2024.07.21 val PER: 0.0917
2026-01-04 01:03:46,821: t15.2024.07.28 val PER: 0.1309
2026-01-04 01:03:46,821: t15.2025.01.10 val PER: 0.2810
2026-01-04 01:03:46,821: t15.2025.01.12 val PER: 0.1455
2026-01-04 01:03:46,821: t15.2025.03.14 val PER: 0.3343
2026-01-04 01:03:46,821: t15.2025.03.16 val PER: 0.1885
2026-01-04 01:03:46,822: t15.2025.03.30 val PER: 0.2874
2026-01-04 01:03:46,822: t15.2025.04.13 val PER: 0.1997
2026-01-04 01:03:47,077: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_17000
2026-01-04 01:04:04,359: Train batch 17200: loss: 9.70 grad norm: 52.41 time: 0.084
2026-01-04 01:04:21,864: Train batch 17400: loss: 10.65 grad norm: 56.69 time: 0.071
2026-01-04 01:04:30,454: Running test after training batch: 17500
2026-01-04 01:04:30,590: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:04:35,379: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:04:35,413: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ennes
2026-01-04 01:04:37,338: Val batch 17500: PER (avg): 0.1464 CTC Loss (avg): 14.9088 WER(1gram): 46.45% (n=64) time: 6.884
2026-01-04 01:04:37,338: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-04 01:04:37,339: t15.2023.08.13 val PER: 0.1091
2026-01-04 01:04:37,339: t15.2023.08.18 val PER: 0.1023
2026-01-04 01:04:37,339: t15.2023.08.20 val PER: 0.1152
2026-01-04 01:04:37,339: t15.2023.08.25 val PER: 0.0828
2026-01-04 01:04:37,339: t15.2023.08.27 val PER: 0.1752
2026-01-04 01:04:37,339: t15.2023.09.01 val PER: 0.0755
2026-01-04 01:04:37,339: t15.2023.09.03 val PER: 0.1591
2026-01-04 01:04:37,339: t15.2023.09.24 val PER: 0.1201
2026-01-04 01:04:37,339: t15.2023.09.29 val PER: 0.1276
2026-01-04 01:04:37,339: t15.2023.10.01 val PER: 0.1731
2026-01-04 01:04:37,339: t15.2023.10.06 val PER: 0.0786
2026-01-04 01:04:37,339: t15.2023.10.08 val PER: 0.2490
2026-01-04 01:04:37,340: t15.2023.10.13 val PER: 0.1901
2026-01-04 01:04:37,340: t15.2023.10.15 val PER: 0.1490
2026-01-04 01:04:37,340: t15.2023.10.20 val PER: 0.1879
2026-01-04 01:04:37,340: t15.2023.10.22 val PER: 0.1125
2026-01-04 01:04:37,340: t15.2023.11.03 val PER: 0.1723
2026-01-04 01:04:37,340: t15.2023.11.04 val PER: 0.0375
2026-01-04 01:04:37,340: t15.2023.11.17 val PER: 0.0389
2026-01-04 01:04:37,340: t15.2023.11.19 val PER: 0.0339
2026-01-04 01:04:37,340: t15.2023.11.26 val PER: 0.1138
2026-01-04 01:04:37,340: t15.2023.12.03 val PER: 0.1103
2026-01-04 01:04:37,341: t15.2023.12.08 val PER: 0.0945
2026-01-04 01:04:37,341: t15.2023.12.10 val PER: 0.0880
2026-01-04 01:04:37,341: t15.2023.12.17 val PER: 0.1362
2026-01-04 01:04:37,341: t15.2023.12.29 val PER: 0.1338
2026-01-04 01:04:37,341: t15.2024.02.25 val PER: 0.1025
2026-01-04 01:04:37,341: t15.2024.03.08 val PER: 0.2219
2026-01-04 01:04:37,341: t15.2024.03.15 val PER: 0.1945
2026-01-04 01:04:37,341: t15.2024.03.17 val PER: 0.1269
2026-01-04 01:04:37,341: t15.2024.05.10 val PER: 0.1412
2026-01-04 01:04:37,341: t15.2024.06.14 val PER: 0.1656
2026-01-04 01:04:37,341: t15.2024.07.19 val PER: 0.2320
2026-01-04 01:04:37,341: t15.2024.07.21 val PER: 0.0917
2026-01-04 01:04:37,341: t15.2024.07.28 val PER: 0.1235
2026-01-04 01:04:37,342: t15.2025.01.10 val PER: 0.2893
2026-01-04 01:04:37,342: t15.2025.01.12 val PER: 0.1463
2026-01-04 01:04:37,342: t15.2025.03.14 val PER: 0.3417
2026-01-04 01:04:37,342: t15.2025.03.16 val PER: 0.1963
2026-01-04 01:04:37,342: t15.2025.03.30 val PER: 0.2805
2026-01-04 01:04:37,342: t15.2025.04.13 val PER: 0.2083
2026-01-04 01:04:37,595: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_17500
2026-01-04 01:04:46,448: Train batch 17600: loss: 9.34 grad norm: 56.36 time: 0.051
2026-01-04 01:05:04,359: Train batch 17800: loss: 6.33 grad norm: 51.57 time: 0.041
2026-01-04 01:05:21,551: Train batch 18000: loss: 10.17 grad norm: 62.37 time: 0.060
2026-01-04 01:05:21,552: Running test after training batch: 18000
2026-01-04 01:05:21,643: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:05:26,481: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:05:26,514: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost edds
2026-01-04 01:05:28,432: Val batch 18000: PER (avg): 0.1459 CTC Loss (avg): 14.9380 WER(1gram): 46.45% (n=64) time: 6.881
2026-01-04 01:05:28,433: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-04 01:05:28,433: t15.2023.08.13 val PER: 0.1060
2026-01-04 01:05:28,433: t15.2023.08.18 val PER: 0.0947
2026-01-04 01:05:28,433: t15.2023.08.20 val PER: 0.1104
2026-01-04 01:05:28,433: t15.2023.08.25 val PER: 0.0798
2026-01-04 01:05:28,433: t15.2023.08.27 val PER: 0.1817
2026-01-04 01:05:28,433: t15.2023.09.01 val PER: 0.0771
2026-01-04 01:05:28,433: t15.2023.09.03 val PER: 0.1485
2026-01-04 01:05:28,434: t15.2023.09.24 val PER: 0.1214
2026-01-04 01:05:28,434: t15.2023.09.29 val PER: 0.1206
2026-01-04 01:05:28,434: t15.2023.10.01 val PER: 0.1697
2026-01-04 01:05:28,434: t15.2023.10.06 val PER: 0.0829
2026-01-04 01:05:28,434: t15.2023.10.08 val PER: 0.2490
2026-01-04 01:05:28,434: t15.2023.10.13 val PER: 0.1908
2026-01-04 01:05:28,434: t15.2023.10.15 val PER: 0.1549
2026-01-04 01:05:28,434: t15.2023.10.20 val PER: 0.1846
2026-01-04 01:05:28,434: t15.2023.10.22 val PER: 0.1080
2026-01-04 01:05:28,434: t15.2023.11.03 val PER: 0.1757
2026-01-04 01:05:28,434: t15.2023.11.04 val PER: 0.0410
2026-01-04 01:05:28,434: t15.2023.11.17 val PER: 0.0373
2026-01-04 01:05:28,434: t15.2023.11.19 val PER: 0.0299
2026-01-04 01:05:28,434: t15.2023.11.26 val PER: 0.1080
2026-01-04 01:05:28,434: t15.2023.12.03 val PER: 0.1145
2026-01-04 01:05:28,434: t15.2023.12.08 val PER: 0.0939
2026-01-04 01:05:28,435: t15.2023.12.10 val PER: 0.0854
2026-01-04 01:05:28,435: t15.2023.12.17 val PER: 0.1289
2026-01-04 01:05:28,435: t15.2023.12.29 val PER: 0.1311
2026-01-04 01:05:28,435: t15.2024.02.25 val PER: 0.1011
2026-01-04 01:05:28,435: t15.2024.03.08 val PER: 0.2191
2026-01-04 01:05:28,435: t15.2024.03.15 val PER: 0.1970
2026-01-04 01:05:28,435: t15.2024.03.17 val PER: 0.1325
2026-01-04 01:05:28,435: t15.2024.05.10 val PER: 0.1456
2026-01-04 01:05:28,435: t15.2024.06.14 val PER: 0.1672
2026-01-04 01:05:28,435: t15.2024.07.19 val PER: 0.2281
2026-01-04 01:05:28,435: t15.2024.07.21 val PER: 0.0959
2026-01-04 01:05:28,435: t15.2024.07.28 val PER: 0.1272
2026-01-04 01:05:28,435: t15.2025.01.10 val PER: 0.2837
2026-01-04 01:05:28,435: t15.2025.01.12 val PER: 0.1509
2026-01-04 01:05:28,435: t15.2025.03.14 val PER: 0.3402
2026-01-04 01:05:28,436: t15.2025.03.16 val PER: 0.1911
2026-01-04 01:05:28,436: t15.2025.03.30 val PER: 0.2897
2026-01-04 01:05:28,436: t15.2025.04.13 val PER: 0.2083
2026-01-04 01:05:28,689: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_18000
2026-01-04 01:05:46,571: Train batch 18200: loss: 7.20 grad norm: 50.63 time: 0.073
2026-01-04 01:06:04,066: Train batch 18400: loss: 4.41 grad norm: 43.83 time: 0.058
2026-01-04 01:06:12,958: Running test after training batch: 18500
2026-01-04 01:06:13,100: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:06:17,798: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:06:17,832: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 01:06:19,767: Val batch 18500: PER (avg): 0.1450 CTC Loss (avg): 14.8970 WER(1gram): 47.21% (n=64) time: 6.808
2026-01-04 01:06:19,767: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-04 01:06:19,768: t15.2023.08.13 val PER: 0.1091
2026-01-04 01:06:19,768: t15.2023.08.18 val PER: 0.0989
2026-01-04 01:06:19,768: t15.2023.08.20 val PER: 0.1080
2026-01-04 01:06:19,768: t15.2023.08.25 val PER: 0.0798
2026-01-04 01:06:19,768: t15.2023.08.27 val PER: 0.1801
2026-01-04 01:06:19,768: t15.2023.09.01 val PER: 0.0771
2026-01-04 01:06:19,768: t15.2023.09.03 val PER: 0.1485
2026-01-04 01:06:19,768: t15.2023.09.24 val PER: 0.1189
2026-01-04 01:06:19,769: t15.2023.09.29 val PER: 0.1225
2026-01-04 01:06:19,769: t15.2023.10.01 val PER: 0.1731
2026-01-04 01:06:19,769: t15.2023.10.06 val PER: 0.0850
2026-01-04 01:06:19,769: t15.2023.10.08 val PER: 0.2463
2026-01-04 01:06:19,769: t15.2023.10.13 val PER: 0.1924
2026-01-04 01:06:19,769: t15.2023.10.15 val PER: 0.1516
2026-01-04 01:06:19,770: t15.2023.10.20 val PER: 0.1913
2026-01-04 01:06:19,770: t15.2023.10.22 val PER: 0.1047
2026-01-04 01:06:19,770: t15.2023.11.03 val PER: 0.1757
2026-01-04 01:06:19,770: t15.2023.11.04 val PER: 0.0375
2026-01-04 01:06:19,770: t15.2023.11.17 val PER: 0.0358
2026-01-04 01:06:19,770: t15.2023.11.19 val PER: 0.0279
2026-01-04 01:06:19,770: t15.2023.11.26 val PER: 0.1080
2026-01-04 01:06:19,771: t15.2023.12.03 val PER: 0.1134
2026-01-04 01:06:19,771: t15.2023.12.08 val PER: 0.0905
2026-01-04 01:06:19,771: t15.2023.12.10 val PER: 0.0867
2026-01-04 01:06:19,771: t15.2023.12.17 val PER: 0.1268
2026-01-04 01:06:19,771: t15.2023.12.29 val PER: 0.1263
2026-01-04 01:06:19,771: t15.2024.02.25 val PER: 0.0997
2026-01-04 01:06:19,772: t15.2024.03.08 val PER: 0.2162
2026-01-04 01:06:19,772: t15.2024.03.15 val PER: 0.1951
2026-01-04 01:06:19,772: t15.2024.03.17 val PER: 0.1241
2026-01-04 01:06:19,772: t15.2024.05.10 val PER: 0.1501
2026-01-04 01:06:19,772: t15.2024.06.14 val PER: 0.1703
2026-01-04 01:06:19,772: t15.2024.07.19 val PER: 0.2334
2026-01-04 01:06:19,772: t15.2024.07.21 val PER: 0.0917
2026-01-04 01:06:19,772: t15.2024.07.28 val PER: 0.1228
2026-01-04 01:06:19,772: t15.2025.01.10 val PER: 0.2837
2026-01-04 01:06:19,772: t15.2025.01.12 val PER: 0.1486
2026-01-04 01:06:19,772: t15.2025.03.14 val PER: 0.3432
2026-01-04 01:06:19,772: t15.2025.03.16 val PER: 0.1898
2026-01-04 01:06:19,772: t15.2025.03.30 val PER: 0.2862
2026-01-04 01:06:19,772: t15.2025.04.13 val PER: 0.2040
2026-01-04 01:06:20,029: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_18500
2026-01-04 01:06:28,808: Train batch 18600: loss: 12.39 grad norm: 63.34 time: 0.067
2026-01-04 01:06:46,390: Train batch 18800: loss: 8.18 grad norm: 53.08 time: 0.064
2026-01-04 01:07:04,097: Train batch 19000: loss: 8.01 grad norm: 46.27 time: 0.065
2026-01-04 01:07:04,097: Running test after training batch: 19000
2026-01-04 01:07:04,226: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:07:09,312: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:07:09,347: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost edds
2026-01-04 01:07:11,295: Val batch 19000: PER (avg): 0.1453 CTC Loss (avg): 14.8884 WER(1gram): 46.19% (n=64) time: 7.198
2026-01-04 01:07:11,296: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 01:07:11,296: t15.2023.08.13 val PER: 0.1071
2026-01-04 01:07:11,296: t15.2023.08.18 val PER: 0.1031
2026-01-04 01:07:11,296: t15.2023.08.20 val PER: 0.1120
2026-01-04 01:07:11,296: t15.2023.08.25 val PER: 0.0783
2026-01-04 01:07:11,296: t15.2023.08.27 val PER: 0.1752
2026-01-04 01:07:11,296: t15.2023.09.01 val PER: 0.0795
2026-01-04 01:07:11,296: t15.2023.09.03 val PER: 0.1556
2026-01-04 01:07:11,296: t15.2023.09.24 val PER: 0.1177
2026-01-04 01:07:11,296: t15.2023.09.29 val PER: 0.1200
2026-01-04 01:07:11,296: t15.2023.10.01 val PER: 0.1697
2026-01-04 01:07:11,296: t15.2023.10.06 val PER: 0.0786
2026-01-04 01:07:11,297: t15.2023.10.08 val PER: 0.2449
2026-01-04 01:07:11,297: t15.2023.10.13 val PER: 0.1885
2026-01-04 01:07:11,297: t15.2023.10.15 val PER: 0.1516
2026-01-04 01:07:11,297: t15.2023.10.20 val PER: 0.1879
2026-01-04 01:07:11,297: t15.2023.10.22 val PER: 0.1091
2026-01-04 01:07:11,297: t15.2023.11.03 val PER: 0.1777
2026-01-04 01:07:11,297: t15.2023.11.04 val PER: 0.0375
2026-01-04 01:07:11,297: t15.2023.11.17 val PER: 0.0358
2026-01-04 01:07:11,297: t15.2023.11.19 val PER: 0.0339
2026-01-04 01:07:11,297: t15.2023.11.26 val PER: 0.1094
2026-01-04 01:07:11,297: t15.2023.12.03 val PER: 0.1155
2026-01-04 01:07:11,297: t15.2023.12.08 val PER: 0.0925
2026-01-04 01:07:11,297: t15.2023.12.10 val PER: 0.0880
2026-01-04 01:07:11,297: t15.2023.12.17 val PER: 0.1341
2026-01-04 01:07:11,297: t15.2023.12.29 val PER: 0.1297
2026-01-04 01:07:11,297: t15.2024.02.25 val PER: 0.1025
2026-01-04 01:07:11,298: t15.2024.03.08 val PER: 0.2248
2026-01-04 01:07:11,298: t15.2024.03.15 val PER: 0.1914
2026-01-04 01:07:11,298: t15.2024.03.17 val PER: 0.1276
2026-01-04 01:07:11,298: t15.2024.05.10 val PER: 0.1471
2026-01-04 01:07:11,298: t15.2024.06.14 val PER: 0.1625
2026-01-04 01:07:11,298: t15.2024.07.19 val PER: 0.2307
2026-01-04 01:07:11,298: t15.2024.07.21 val PER: 0.0924
2026-01-04 01:07:11,298: t15.2024.07.28 val PER: 0.1279
2026-01-04 01:07:11,298: t15.2025.01.10 val PER: 0.2824
2026-01-04 01:07:11,298: t15.2025.01.12 val PER: 0.1447
2026-01-04 01:07:11,298: t15.2025.03.14 val PER: 0.3432
2026-01-04 01:07:11,299: t15.2025.03.16 val PER: 0.1950
2026-01-04 01:07:11,299: t15.2025.03.30 val PER: 0.2782
2026-01-04 01:07:11,299: t15.2025.04.13 val PER: 0.1983
2026-01-04 01:07:11,559: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_19000
2026-01-04 01:07:29,166: Train batch 19200: loss: 5.57 grad norm: 45.32 time: 0.063
2026-01-04 01:07:46,958: Train batch 19400: loss: 4.60 grad norm: 37.73 time: 0.053
2026-01-04 01:07:55,635: Running test after training batch: 19500
2026-01-04 01:07:55,770: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:08:00,438: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:08:00,472: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost edds
2026-01-04 01:08:02,429: Val batch 19500: PER (avg): 0.1453 CTC Loss (avg): 14.8536 WER(1gram): 46.45% (n=64) time: 6.793
2026-01-04 01:08:02,429: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 01:08:02,429: t15.2023.08.13 val PER: 0.1060
2026-01-04 01:08:02,429: t15.2023.08.18 val PER: 0.1048
2026-01-04 01:08:02,430: t15.2023.08.20 val PER: 0.1104
2026-01-04 01:08:02,430: t15.2023.08.25 val PER: 0.0768
2026-01-04 01:08:02,430: t15.2023.08.27 val PER: 0.1720
2026-01-04 01:08:02,430: t15.2023.09.01 val PER: 0.0755
2026-01-04 01:08:02,430: t15.2023.09.03 val PER: 0.1496
2026-01-04 01:08:02,430: t15.2023.09.24 val PER: 0.1141
2026-01-04 01:08:02,430: t15.2023.09.29 val PER: 0.1238
2026-01-04 01:08:02,430: t15.2023.10.01 val PER: 0.1737
2026-01-04 01:08:02,430: t15.2023.10.06 val PER: 0.0797
2026-01-04 01:08:02,430: t15.2023.10.08 val PER: 0.2503
2026-01-04 01:08:02,430: t15.2023.10.13 val PER: 0.1955
2026-01-04 01:08:02,430: t15.2023.10.15 val PER: 0.1470
2026-01-04 01:08:02,430: t15.2023.10.20 val PER: 0.1980
2026-01-04 01:08:02,430: t15.2023.10.22 val PER: 0.1102
2026-01-04 01:08:02,431: t15.2023.11.03 val PER: 0.1744
2026-01-04 01:08:02,431: t15.2023.11.04 val PER: 0.0410
2026-01-04 01:08:02,431: t15.2023.11.17 val PER: 0.0342
2026-01-04 01:08:02,431: t15.2023.11.19 val PER: 0.0339
2026-01-04 01:08:02,431: t15.2023.11.26 val PER: 0.1109
2026-01-04 01:08:02,431: t15.2023.12.03 val PER: 0.1124
2026-01-04 01:08:02,431: t15.2023.12.08 val PER: 0.0939
2026-01-04 01:08:02,431: t15.2023.12.10 val PER: 0.0880
2026-01-04 01:08:02,431: t15.2023.12.17 val PER: 0.1310
2026-01-04 01:08:02,431: t15.2023.12.29 val PER: 0.1318
2026-01-04 01:08:02,431: t15.2024.02.25 val PER: 0.1011
2026-01-04 01:08:02,431: t15.2024.03.08 val PER: 0.2219
2026-01-04 01:08:02,432: t15.2024.03.15 val PER: 0.1857
2026-01-04 01:08:02,432: t15.2024.03.17 val PER: 0.1276
2026-01-04 01:08:02,432: t15.2024.05.10 val PER: 0.1516
2026-01-04 01:08:02,432: t15.2024.06.14 val PER: 0.1640
2026-01-04 01:08:02,432: t15.2024.07.19 val PER: 0.2320
2026-01-04 01:08:02,432: t15.2024.07.21 val PER: 0.0945
2026-01-04 01:08:02,432: t15.2024.07.28 val PER: 0.1272
2026-01-04 01:08:02,432: t15.2025.01.10 val PER: 0.2755
2026-01-04 01:08:02,432: t15.2025.01.12 val PER: 0.1470
2026-01-04 01:08:02,432: t15.2025.03.14 val PER: 0.3417
2026-01-04 01:08:02,432: t15.2025.03.16 val PER: 0.1898
2026-01-04 01:08:02,432: t15.2025.03.30 val PER: 0.2816
2026-01-04 01:08:02,432: t15.2025.04.13 val PER: 0.2068
2026-01-04 01:08:02,693: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_19500
2026-01-04 01:08:11,424: Train batch 19600: loss: 7.57 grad norm: 51.01 time: 0.057
2026-01-04 01:08:28,780: Train batch 19800: loss: 7.21 grad norm: 53.36 time: 0.055
2026-01-04 01:08:46,547: Running test after training batch: 19999
2026-01-04 01:08:46,634: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:08:51,421: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:08:51,455: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 01:08:53,423: Val batch 19999: PER (avg): 0.1449 CTC Loss (avg): 14.8607 WER(1gram): 46.45% (n=64) time: 6.875
2026-01-04 01:08:53,423: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-04 01:08:53,423: t15.2023.08.13 val PER: 0.1040
2026-01-04 01:08:53,424: t15.2023.08.18 val PER: 0.0989
2026-01-04 01:08:53,424: t15.2023.08.20 val PER: 0.1080
2026-01-04 01:08:53,424: t15.2023.08.25 val PER: 0.0843
2026-01-04 01:08:53,424: t15.2023.08.27 val PER: 0.1752
2026-01-04 01:08:53,424: t15.2023.09.01 val PER: 0.0787
2026-01-04 01:08:53,424: t15.2023.09.03 val PER: 0.1556
2026-01-04 01:08:53,424: t15.2023.09.24 val PER: 0.1129
2026-01-04 01:08:53,424: t15.2023.09.29 val PER: 0.1193
2026-01-04 01:08:53,424: t15.2023.10.01 val PER: 0.1750
2026-01-04 01:08:53,424: t15.2023.10.06 val PER: 0.0775
2026-01-04 01:08:53,424: t15.2023.10.08 val PER: 0.2490
2026-01-04 01:08:53,424: t15.2023.10.13 val PER: 0.1901
2026-01-04 01:08:53,424: t15.2023.10.15 val PER: 0.1470
2026-01-04 01:08:53,425: t15.2023.10.20 val PER: 0.2013
2026-01-04 01:08:53,425: t15.2023.10.22 val PER: 0.1091
2026-01-04 01:08:53,425: t15.2023.11.03 val PER: 0.1737
2026-01-04 01:08:53,425: t15.2023.11.04 val PER: 0.0341
2026-01-04 01:08:53,425: t15.2023.11.17 val PER: 0.0342
2026-01-04 01:08:53,425: t15.2023.11.19 val PER: 0.0319
2026-01-04 01:08:53,425: t15.2023.11.26 val PER: 0.1116
2026-01-04 01:08:53,425: t15.2023.12.03 val PER: 0.1124
2026-01-04 01:08:53,425: t15.2023.12.08 val PER: 0.0952
2026-01-04 01:08:53,425: t15.2023.12.10 val PER: 0.0867
2026-01-04 01:08:53,425: t15.2023.12.17 val PER: 0.1331
2026-01-04 01:08:53,425: t15.2023.12.29 val PER: 0.1270
2026-01-04 01:08:53,425: t15.2024.02.25 val PER: 0.1025
2026-01-04 01:08:53,425: t15.2024.03.08 val PER: 0.2176
2026-01-04 01:08:53,425: t15.2024.03.15 val PER: 0.1945
2026-01-04 01:08:53,425: t15.2024.03.17 val PER: 0.1227
2026-01-04 01:08:53,426: t15.2024.05.10 val PER: 0.1501
2026-01-04 01:08:53,426: t15.2024.06.14 val PER: 0.1640
2026-01-04 01:08:53,426: t15.2024.07.19 val PER: 0.2314
2026-01-04 01:08:53,426: t15.2024.07.21 val PER: 0.0903
2026-01-04 01:08:53,426: t15.2024.07.28 val PER: 0.1265
2026-01-04 01:08:53,426: t15.2025.01.10 val PER: 0.2727
2026-01-04 01:08:53,426: t15.2025.01.12 val PER: 0.1470
2026-01-04 01:08:53,426: t15.2025.03.14 val PER: 0.3358
2026-01-04 01:08:53,426: t15.2025.03.16 val PER: 0.1963
2026-01-04 01:08:53,426: t15.2025.03.30 val PER: 0.2920
2026-01-04 01:08:53,426: t15.2025.04.13 val PER: 0.2068
2026-01-04 01:08:53,675: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d15/checkpoint/checkpoint_batch_19999
2026-01-04 01:08:53,706: Best avg val PER achieved: 0.14807
2026-01-04 01:08:53,706: Total training time: 34.52 minutes

=== RUN d20.yaml ===
2026-01-04 01:08:58,982: Using device: cuda:0
2026-01-04 01:09:00,664: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-04 01:09:00,685: Using 45 sessions after filtering (from 45).
2026-01-04 01:09:01,111: Using torch.compile (if available)
2026-01-04 01:09:01,111: torch.compile not available (torch<2.0). Skipping.
2026-01-04 01:09:01,112: Initialized RNN decoding model
2026-01-04 01:09:01,112: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-04 01:09:01,112: Model has 44,907,305 parameters
2026-01-04 01:09:01,112: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-04 01:09:02,386: Successfully initialized datasets
2026-01-04 01:09:02,387: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-04 01:09:03,294: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.180
2026-01-04 01:09:03,295: Running test after training batch: 0
2026-01-04 01:09:03,415: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:09:08,900: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-04 01:09:09,607: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-04 01:09:43,058: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 39.764
2026-01-04 01:09:43,059: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-04 01:09:43,059: t15.2023.08.13 val PER: 1.3056
2026-01-04 01:09:43,059: t15.2023.08.18 val PER: 1.4208
2026-01-04 01:09:43,059: t15.2023.08.20 val PER: 1.3002
2026-01-04 01:09:43,059: t15.2023.08.25 val PER: 1.3389
2026-01-04 01:09:43,059: t15.2023.08.27 val PER: 1.2460
2026-01-04 01:09:43,059: t15.2023.09.01 val PER: 1.4537
2026-01-04 01:09:43,060: t15.2023.09.03 val PER: 1.3171
2026-01-04 01:09:43,060: t15.2023.09.24 val PER: 1.5461
2026-01-04 01:09:43,060: t15.2023.09.29 val PER: 1.4671
2026-01-04 01:09:43,060: t15.2023.10.01 val PER: 1.2147
2026-01-04 01:09:43,060: t15.2023.10.06 val PER: 1.4876
2026-01-04 01:09:43,060: t15.2023.10.08 val PER: 1.1827
2026-01-04 01:09:43,060: t15.2023.10.13 val PER: 1.3964
2026-01-04 01:09:43,060: t15.2023.10.15 val PER: 1.3889
2026-01-04 01:09:43,060: t15.2023.10.20 val PER: 1.4866
2026-01-04 01:09:43,060: t15.2023.10.22 val PER: 1.3942
2026-01-04 01:09:43,060: t15.2023.11.03 val PER: 1.5923
2026-01-04 01:09:43,060: t15.2023.11.04 val PER: 2.0171
2026-01-04 01:09:43,060: t15.2023.11.17 val PER: 1.9518
2026-01-04 01:09:43,061: t15.2023.11.19 val PER: 1.6707
2026-01-04 01:09:43,061: t15.2023.11.26 val PER: 1.5413
2026-01-04 01:09:43,061: t15.2023.12.03 val PER: 1.4254
2026-01-04 01:09:43,061: t15.2023.12.08 val PER: 1.4487
2026-01-04 01:09:43,061: t15.2023.12.10 val PER: 1.6899
2026-01-04 01:09:43,061: t15.2023.12.17 val PER: 1.3077
2026-01-04 01:09:43,061: t15.2023.12.29 val PER: 1.4063
2026-01-04 01:09:43,061: t15.2024.02.25 val PER: 1.4228
2026-01-04 01:09:43,061: t15.2024.03.08 val PER: 1.3257
2026-01-04 01:09:43,061: t15.2024.03.15 val PER: 1.3196
2026-01-04 01:09:43,061: t15.2024.03.17 val PER: 1.4052
2026-01-04 01:09:43,061: t15.2024.05.10 val PER: 1.3224
2026-01-04 01:09:43,061: t15.2024.06.14 val PER: 1.5315
2026-01-04 01:09:43,062: t15.2024.07.19 val PER: 1.0817
2026-01-04 01:09:43,062: t15.2024.07.21 val PER: 1.6290
2026-01-04 01:09:43,062: t15.2024.07.28 val PER: 1.6588
2026-01-04 01:09:43,062: t15.2025.01.10 val PER: 1.0923
2026-01-04 01:09:43,062: t15.2025.01.12 val PER: 1.7629
2026-01-04 01:09:43,062: t15.2025.03.14 val PER: 1.0414
2026-01-04 01:09:43,062: t15.2025.03.16 val PER: 1.6257
2026-01-04 01:09:43,062: t15.2025.03.30 val PER: 1.2874
2026-01-04 01:09:43,062: t15.2025.04.13 val PER: 1.5949
2026-01-04 01:09:43,063: New best val WER(1gram) inf% --> 100.00%
2026-01-04 01:09:43,063: Checkpointing model
2026-01-04 01:09:43,299: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/best_checkpoint
2026-01-04 01:09:43,543: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_0
2026-01-04 01:10:00,902: Train batch 200: loss: 77.60 grad norm: 106.01 time: 0.054
2026-01-04 01:10:17,889: Train batch 400: loss: 53.50 grad norm: 90.98 time: 0.062
2026-01-04 01:10:26,571: Running test after training batch: 500
2026-01-04 01:10:26,704: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:10:31,446: WER debug example
  GT : you can see the code at this point as well
  PR : used and ease thus uhde at this ide is aisle
2026-01-04 01:10:31,478: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as adz
2026-01-04 01:10:33,721: Val batch 500: PER (avg): 0.5225 CTC Loss (avg): 55.6797 WER(1gram): 88.83% (n=64) time: 7.150
2026-01-04 01:10:33,721: WER lens: avg_true_words=6.16 avg_pred_words=5.61 max_pred_words=12
2026-01-04 01:10:33,722: t15.2023.08.13 val PER: 0.4678
2026-01-04 01:10:33,722: t15.2023.08.18 val PER: 0.4568
2026-01-04 01:10:33,722: t15.2023.08.20 val PER: 0.4472
2026-01-04 01:10:33,722: t15.2023.08.25 val PER: 0.4292
2026-01-04 01:10:33,722: t15.2023.08.27 val PER: 0.5305
2026-01-04 01:10:33,722: t15.2023.09.01 val PER: 0.4229
2026-01-04 01:10:33,722: t15.2023.09.03 val PER: 0.5071
2026-01-04 01:10:33,722: t15.2023.09.24 val PER: 0.4393
2026-01-04 01:10:33,722: t15.2023.09.29 val PER: 0.4722
2026-01-04 01:10:33,723: t15.2023.10.01 val PER: 0.5231
2026-01-04 01:10:33,723: t15.2023.10.06 val PER: 0.4360
2026-01-04 01:10:33,723: t15.2023.10.08 val PER: 0.5304
2026-01-04 01:10:33,723: t15.2023.10.13 val PER: 0.5764
2026-01-04 01:10:33,723: t15.2023.10.15 val PER: 0.4937
2026-01-04 01:10:33,723: t15.2023.10.20 val PER: 0.4564
2026-01-04 01:10:33,723: t15.2023.10.22 val PER: 0.4644
2026-01-04 01:10:33,723: t15.2023.11.03 val PER: 0.5041
2026-01-04 01:10:33,723: t15.2023.11.04 val PER: 0.2560
2026-01-04 01:10:33,723: t15.2023.11.17 val PER: 0.3701
2026-01-04 01:10:33,723: t15.2023.11.19 val PER: 0.3293
2026-01-04 01:10:33,723: t15.2023.11.26 val PER: 0.5558
2026-01-04 01:10:33,724: t15.2023.12.03 val PER: 0.5011
2026-01-04 01:10:33,724: t15.2023.12.08 val PER: 0.5300
2026-01-04 01:10:33,724: t15.2023.12.10 val PER: 0.4507
2026-01-04 01:10:33,724: t15.2023.12.17 val PER: 0.5728
2026-01-04 01:10:33,724: t15.2023.12.29 val PER: 0.5587
2026-01-04 01:10:33,724: t15.2024.02.25 val PER: 0.4846
2026-01-04 01:10:33,724: t15.2024.03.08 val PER: 0.6415
2026-01-04 01:10:33,724: t15.2024.03.15 val PER: 0.5585
2026-01-04 01:10:33,724: t15.2024.03.17 val PER: 0.5070
2026-01-04 01:10:33,724: t15.2024.05.10 val PER: 0.5587
2026-01-04 01:10:33,724: t15.2024.06.14 val PER: 0.5032
2026-01-04 01:10:33,724: t15.2024.07.19 val PER: 0.6809
2026-01-04 01:10:33,724: t15.2024.07.21 val PER: 0.4648
2026-01-04 01:10:33,724: t15.2024.07.28 val PER: 0.5162
2026-01-04 01:10:33,724: t15.2025.01.10 val PER: 0.7493
2026-01-04 01:10:33,724: t15.2025.01.12 val PER: 0.5643
2026-01-04 01:10:33,724: t15.2025.03.14 val PER: 0.7411
2026-01-04 01:10:33,725: t15.2025.03.16 val PER: 0.5903
2026-01-04 01:10:33,725: t15.2025.03.30 val PER: 0.7391
2026-01-04 01:10:33,725: t15.2025.04.13 val PER: 0.5863
2026-01-04 01:10:33,726: New best val WER(1gram) 100.00% --> 88.83%
2026-01-04 01:10:33,726: Checkpointing model
2026-01-04 01:10:34,341: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/best_checkpoint
2026-01-04 01:10:34,586: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_500
2026-01-04 01:10:43,255: Train batch 600: loss: 49.06 grad norm: 80.51 time: 0.078
2026-01-04 01:11:00,351: Train batch 800: loss: 41.21 grad norm: 85.90 time: 0.057
2026-01-04 01:11:17,863: Train batch 1000: loss: 42.90 grad norm: 82.35 time: 0.065
2026-01-04 01:11:17,864: Running test after training batch: 1000
2026-01-04 01:11:17,998: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:11:22,864: WER debug example
  GT : you can see the code at this point as well
  PR : hued wend ease thus good it this uhde is will
2026-01-04 01:11:22,897: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus wass it
2026-01-04 01:11:24,719: Val batch 1000: PER (avg): 0.4075 CTC Loss (avg): 42.3956 WER(1gram): 82.49% (n=64) time: 6.856
2026-01-04 01:11:24,720: WER lens: avg_true_words=6.16 avg_pred_words=5.50 max_pred_words=12
2026-01-04 01:11:24,720: t15.2023.08.13 val PER: 0.3794
2026-01-04 01:11:24,720: t15.2023.08.18 val PER: 0.3428
2026-01-04 01:11:24,720: t15.2023.08.20 val PER: 0.3407
2026-01-04 01:11:24,720: t15.2023.08.25 val PER: 0.2907
2026-01-04 01:11:24,720: t15.2023.08.27 val PER: 0.4196
2026-01-04 01:11:24,720: t15.2023.09.01 val PER: 0.3011
2026-01-04 01:11:24,721: t15.2023.09.03 val PER: 0.3943
2026-01-04 01:11:24,721: t15.2023.09.24 val PER: 0.3313
2026-01-04 01:11:24,721: t15.2023.09.29 val PER: 0.3542
2026-01-04 01:11:24,721: t15.2023.10.01 val PER: 0.4022
2026-01-04 01:11:24,721: t15.2023.10.06 val PER: 0.3154
2026-01-04 01:11:24,721: t15.2023.10.08 val PER: 0.4465
2026-01-04 01:11:24,721: t15.2023.10.13 val PER: 0.4670
2026-01-04 01:11:24,721: t15.2023.10.15 val PER: 0.3790
2026-01-04 01:11:24,721: t15.2023.10.20 val PER: 0.3490
2026-01-04 01:11:24,721: t15.2023.10.22 val PER: 0.3508
2026-01-04 01:11:24,722: t15.2023.11.03 val PER: 0.4037
2026-01-04 01:11:24,722: t15.2023.11.04 val PER: 0.1536
2026-01-04 01:11:24,722: t15.2023.11.17 val PER: 0.2659
2026-01-04 01:11:24,722: t15.2023.11.19 val PER: 0.2076
2026-01-04 01:11:24,722: t15.2023.11.26 val PER: 0.4493
2026-01-04 01:11:24,722: t15.2023.12.03 val PER: 0.3992
2026-01-04 01:11:24,722: t15.2023.12.08 val PER: 0.4001
2026-01-04 01:11:24,722: t15.2023.12.10 val PER: 0.3601
2026-01-04 01:11:24,722: t15.2023.12.17 val PER: 0.4023
2026-01-04 01:11:24,722: t15.2023.12.29 val PER: 0.4070
2026-01-04 01:11:24,722: t15.2024.02.25 val PER: 0.3441
2026-01-04 01:11:24,722: t15.2024.03.08 val PER: 0.4893
2026-01-04 01:11:24,723: t15.2024.03.15 val PER: 0.4422
2026-01-04 01:11:24,723: t15.2024.03.17 val PER: 0.4066
2026-01-04 01:11:24,723: t15.2024.05.10 val PER: 0.4264
2026-01-04 01:11:24,723: t15.2024.06.14 val PER: 0.4038
2026-01-04 01:11:24,723: t15.2024.07.19 val PER: 0.5346
2026-01-04 01:11:24,723: t15.2024.07.21 val PER: 0.3717
2026-01-04 01:11:24,723: t15.2024.07.28 val PER: 0.4184
2026-01-04 01:11:24,723: t15.2025.01.10 val PER: 0.6212
2026-01-04 01:11:24,723: t15.2025.01.12 val PER: 0.4450
2026-01-04 01:11:24,723: t15.2025.03.14 val PER: 0.6302
2026-01-04 01:11:24,723: t15.2025.03.16 val PER: 0.4817
2026-01-04 01:11:24,723: t15.2025.03.30 val PER: 0.6517
2026-01-04 01:11:24,723: t15.2025.04.13 val PER: 0.5007
2026-01-04 01:11:24,725: New best val WER(1gram) 88.83% --> 82.49%
2026-01-04 01:11:24,725: Checkpointing model
2026-01-04 01:11:25,323: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/best_checkpoint
2026-01-04 01:11:25,568: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_1000
2026-01-04 01:11:42,727: Train batch 1200: loss: 32.93 grad norm: 76.00 time: 0.068
2026-01-04 01:12:00,190: Train batch 1400: loss: 35.94 grad norm: 79.78 time: 0.060
2026-01-04 01:12:08,895: Running test after training batch: 1500
2026-01-04 01:12:09,047: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:12:13,761: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt sze the owed it this boyde is will
2026-01-04 01:12:13,793: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap thus os
2026-01-04 01:12:15,387: Val batch 1500: PER (avg): 0.3807 CTC Loss (avg): 37.0968 WER(1gram): 75.63% (n=64) time: 6.491
2026-01-04 01:12:15,387: WER lens: avg_true_words=6.16 avg_pred_words=5.12 max_pred_words=11
2026-01-04 01:12:15,387: t15.2023.08.13 val PER: 0.3514
2026-01-04 01:12:15,387: t15.2023.08.18 val PER: 0.3101
2026-01-04 01:12:15,387: t15.2023.08.20 val PER: 0.3058
2026-01-04 01:12:15,388: t15.2023.08.25 val PER: 0.2575
2026-01-04 01:12:15,388: t15.2023.08.27 val PER: 0.3987
2026-01-04 01:12:15,388: t15.2023.09.01 val PER: 0.2727
2026-01-04 01:12:15,388: t15.2023.09.03 val PER: 0.3765
2026-01-04 01:12:15,388: t15.2023.09.24 val PER: 0.3046
2026-01-04 01:12:15,388: t15.2023.09.29 val PER: 0.3344
2026-01-04 01:12:15,388: t15.2023.10.01 val PER: 0.4009
2026-01-04 01:12:15,388: t15.2023.10.06 val PER: 0.2842
2026-01-04 01:12:15,388: t15.2023.10.08 val PER: 0.4493
2026-01-04 01:12:15,388: t15.2023.10.13 val PER: 0.4422
2026-01-04 01:12:15,388: t15.2023.10.15 val PER: 0.3612
2026-01-04 01:12:15,388: t15.2023.10.20 val PER: 0.3356
2026-01-04 01:12:15,388: t15.2023.10.22 val PER: 0.3196
2026-01-04 01:12:15,388: t15.2023.11.03 val PER: 0.3643
2026-01-04 01:12:15,389: t15.2023.11.04 val PER: 0.1092
2026-01-04 01:12:15,389: t15.2023.11.17 val PER: 0.2115
2026-01-04 01:12:15,389: t15.2023.11.19 val PER: 0.1816
2026-01-04 01:12:15,389: t15.2023.11.26 val PER: 0.4203
2026-01-04 01:12:15,389: t15.2023.12.03 val PER: 0.3634
2026-01-04 01:12:15,389: t15.2023.12.08 val PER: 0.3469
2026-01-04 01:12:15,389: t15.2023.12.10 val PER: 0.3022
2026-01-04 01:12:15,389: t15.2023.12.17 val PER: 0.3742
2026-01-04 01:12:15,389: t15.2023.12.29 val PER: 0.3672
2026-01-04 01:12:15,389: t15.2024.02.25 val PER: 0.3244
2026-01-04 01:12:15,389: t15.2024.03.08 val PER: 0.4595
2026-01-04 01:12:15,389: t15.2024.03.15 val PER: 0.4234
2026-01-04 01:12:15,389: t15.2024.03.17 val PER: 0.3766
2026-01-04 01:12:15,389: t15.2024.05.10 val PER: 0.3952
2026-01-04 01:12:15,389: t15.2024.06.14 val PER: 0.4022
2026-01-04 01:12:15,389: t15.2024.07.19 val PER: 0.5326
2026-01-04 01:12:15,390: t15.2024.07.21 val PER: 0.3434
2026-01-04 01:12:15,390: t15.2024.07.28 val PER: 0.3743
2026-01-04 01:12:15,390: t15.2025.01.10 val PER: 0.6157
2026-01-04 01:12:15,390: t15.2025.01.12 val PER: 0.4196
2026-01-04 01:12:15,390: t15.2025.03.14 val PER: 0.6095
2026-01-04 01:12:15,390: t15.2025.03.16 val PER: 0.4490
2026-01-04 01:12:15,390: t15.2025.03.30 val PER: 0.6264
2026-01-04 01:12:15,390: t15.2025.04.13 val PER: 0.4893
2026-01-04 01:12:15,391: New best val WER(1gram) 82.49% --> 75.63%
2026-01-04 01:12:15,392: Checkpointing model
2026-01-04 01:12:16,012: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/best_checkpoint
2026-01-04 01:12:16,258: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_1500
2026-01-04 01:12:24,636: Train batch 1600: loss: 37.07 grad norm: 80.82 time: 0.063
2026-01-04 01:12:41,636: Train batch 1800: loss: 35.47 grad norm: 73.34 time: 0.088
2026-01-04 01:12:58,753: Train batch 2000: loss: 33.44 grad norm: 77.74 time: 0.066
2026-01-04 01:12:58,753: Running test after training batch: 2000
2026-01-04 01:12:58,850: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:13:03,617: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt e the code at this want is will
2026-01-04 01:13:03,648: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heeke thus kos it
2026-01-04 01:13:05,206: Val batch 2000: PER (avg): 0.3252 CTC Loss (avg): 32.6448 WER(1gram): 69.54% (n=64) time: 6.452
2026-01-04 01:13:05,206: WER lens: avg_true_words=6.16 avg_pred_words=5.59 max_pred_words=11
2026-01-04 01:13:05,206: t15.2023.08.13 val PER: 0.2911
2026-01-04 01:13:05,206: t15.2023.08.18 val PER: 0.2439
2026-01-04 01:13:05,207: t15.2023.08.20 val PER: 0.2566
2026-01-04 01:13:05,207: t15.2023.08.25 val PER: 0.2199
2026-01-04 01:13:05,207: t15.2023.08.27 val PER: 0.3408
2026-01-04 01:13:05,207: t15.2023.09.01 val PER: 0.2330
2026-01-04 01:13:05,207: t15.2023.09.03 val PER: 0.3266
2026-01-04 01:13:05,207: t15.2023.09.24 val PER: 0.2573
2026-01-04 01:13:05,207: t15.2023.09.29 val PER: 0.2719
2026-01-04 01:13:05,207: t15.2023.10.01 val PER: 0.3256
2026-01-04 01:13:05,207: t15.2023.10.06 val PER: 0.2347
2026-01-04 01:13:05,207: t15.2023.10.08 val PER: 0.3938
2026-01-04 01:13:05,207: t15.2023.10.13 val PER: 0.3732
2026-01-04 01:13:05,207: t15.2023.10.15 val PER: 0.3092
2026-01-04 01:13:05,207: t15.2023.10.20 val PER: 0.2919
2026-01-04 01:13:05,207: t15.2023.10.22 val PER: 0.2584
2026-01-04 01:13:05,207: t15.2023.11.03 val PER: 0.3141
2026-01-04 01:13:05,207: t15.2023.11.04 val PER: 0.0785
2026-01-04 01:13:05,208: t15.2023.11.17 val PER: 0.1742
2026-01-04 01:13:05,208: t15.2023.11.19 val PER: 0.1357
2026-01-04 01:13:05,208: t15.2023.11.26 val PER: 0.3587
2026-01-04 01:13:05,208: t15.2023.12.03 val PER: 0.3078
2026-01-04 01:13:05,208: t15.2023.12.08 val PER: 0.3016
2026-01-04 01:13:05,208: t15.2023.12.10 val PER: 0.2536
2026-01-04 01:13:05,208: t15.2023.12.17 val PER: 0.3181
2026-01-04 01:13:05,208: t15.2023.12.29 val PER: 0.3315
2026-01-04 01:13:05,208: t15.2024.02.25 val PER: 0.2626
2026-01-04 01:13:05,208: t15.2024.03.08 val PER: 0.3983
2026-01-04 01:13:05,208: t15.2024.03.15 val PER: 0.3577
2026-01-04 01:13:05,208: t15.2024.03.17 val PER: 0.3382
2026-01-04 01:13:05,208: t15.2024.05.10 val PER: 0.3418
2026-01-04 01:13:05,208: t15.2024.06.14 val PER: 0.3486
2026-01-04 01:13:05,209: t15.2024.07.19 val PER: 0.4641
2026-01-04 01:13:05,209: t15.2024.07.21 val PER: 0.2938
2026-01-04 01:13:05,209: t15.2024.07.28 val PER: 0.3272
2026-01-04 01:13:05,209: t15.2025.01.10 val PER: 0.5510
2026-01-04 01:13:05,209: t15.2025.01.12 val PER: 0.3711
2026-01-04 01:13:05,209: t15.2025.03.14 val PER: 0.5237
2026-01-04 01:13:05,209: t15.2025.03.16 val PER: 0.3940
2026-01-04 01:13:05,209: t15.2025.03.30 val PER: 0.5517
2026-01-04 01:13:05,209: t15.2025.04.13 val PER: 0.4023
2026-01-04 01:13:05,210: New best val WER(1gram) 75.63% --> 69.54%
2026-01-04 01:13:05,210: Checkpointing model
2026-01-04 01:13:05,835: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/best_checkpoint
2026-01-04 01:13:06,083: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_2000
2026-01-04 01:13:23,062: Train batch 2200: loss: 28.98 grad norm: 73.28 time: 0.059
2026-01-04 01:13:40,411: Train batch 2400: loss: 29.11 grad norm: 62.61 time: 0.051
2026-01-04 01:13:49,049: Running test after training batch: 2500
2026-01-04 01:13:49,140: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:13:54,227: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-04 01:13:54,259: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-04 01:13:55,937: Val batch 2500: PER (avg): 0.3013 CTC Loss (avg): 30.2350 WER(1gram): 67.26% (n=64) time: 6.887
2026-01-04 01:13:55,937: WER lens: avg_true_words=6.16 avg_pred_words=5.75 max_pred_words=11
2026-01-04 01:13:55,937: t15.2023.08.13 val PER: 0.2703
2026-01-04 01:13:55,937: t15.2023.08.18 val PER: 0.2439
2026-01-04 01:13:55,937: t15.2023.08.20 val PER: 0.2415
2026-01-04 01:13:55,937: t15.2023.08.25 val PER: 0.1988
2026-01-04 01:13:55,937: t15.2023.08.27 val PER: 0.3151
2026-01-04 01:13:55,937: t15.2023.09.01 val PER: 0.2078
2026-01-04 01:13:55,937: t15.2023.09.03 val PER: 0.2838
2026-01-04 01:13:55,938: t15.2023.09.24 val PER: 0.2233
2026-01-04 01:13:55,938: t15.2023.09.29 val PER: 0.2559
2026-01-04 01:13:55,938: t15.2023.10.01 val PER: 0.3065
2026-01-04 01:13:55,938: t15.2023.10.06 val PER: 0.2088
2026-01-04 01:13:55,938: t15.2023.10.08 val PER: 0.3951
2026-01-04 01:13:55,938: t15.2023.10.13 val PER: 0.3514
2026-01-04 01:13:55,938: t15.2023.10.15 val PER: 0.2900
2026-01-04 01:13:55,938: t15.2023.10.20 val PER: 0.2718
2026-01-04 01:13:55,938: t15.2023.10.22 val PER: 0.2305
2026-01-04 01:13:55,939: t15.2023.11.03 val PER: 0.2890
2026-01-04 01:13:55,939: t15.2023.11.04 val PER: 0.0819
2026-01-04 01:13:55,939: t15.2023.11.17 val PER: 0.1446
2026-01-04 01:13:55,939: t15.2023.11.19 val PER: 0.1198
2026-01-04 01:13:55,939: t15.2023.11.26 val PER: 0.3514
2026-01-04 01:13:55,939: t15.2023.12.03 val PER: 0.2847
2026-01-04 01:13:55,939: t15.2023.12.08 val PER: 0.2736
2026-01-04 01:13:55,939: t15.2023.12.10 val PER: 0.2234
2026-01-04 01:13:55,939: t15.2023.12.17 val PER: 0.2931
2026-01-04 01:13:55,939: t15.2023.12.29 val PER: 0.3034
2026-01-04 01:13:55,939: t15.2024.02.25 val PER: 0.2275
2026-01-04 01:13:55,939: t15.2024.03.08 val PER: 0.3556
2026-01-04 01:13:55,939: t15.2024.03.15 val PER: 0.3427
2026-01-04 01:13:55,939: t15.2024.03.17 val PER: 0.3138
2026-01-04 01:13:55,939: t15.2024.05.10 val PER: 0.3046
2026-01-04 01:13:55,939: t15.2024.06.14 val PER: 0.3123
2026-01-04 01:13:55,940: t15.2024.07.19 val PER: 0.4390
2026-01-04 01:13:55,940: t15.2024.07.21 val PER: 0.2517
2026-01-04 01:13:55,940: t15.2024.07.28 val PER: 0.3022
2026-01-04 01:13:55,940: t15.2025.01.10 val PER: 0.5014
2026-01-04 01:13:55,940: t15.2025.01.12 val PER: 0.3487
2026-01-04 01:13:55,940: t15.2025.03.14 val PER: 0.5030
2026-01-04 01:13:55,940: t15.2025.03.16 val PER: 0.3560
2026-01-04 01:13:55,940: t15.2025.03.30 val PER: 0.5080
2026-01-04 01:13:55,940: t15.2025.04.13 val PER: 0.4080
2026-01-04 01:13:55,941: New best val WER(1gram) 69.54% --> 67.26%
2026-01-04 01:13:55,941: Checkpointing model
2026-01-04 01:13:56,561: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/best_checkpoint
2026-01-04 01:13:56,806: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_2500
2026-01-04 01:14:05,394: Train batch 2600: loss: 35.39 grad norm: 82.04 time: 0.055
2026-01-04 01:14:22,745: Train batch 2800: loss: 26.10 grad norm: 71.21 time: 0.081
2026-01-04 01:14:40,158: Train batch 3000: loss: 31.53 grad norm: 73.19 time: 0.082
2026-01-04 01:14:40,158: Running test after training batch: 3000
2026-01-04 01:14:40,259: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:14:44,963: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-04 01:14:44,993: WER debug example
  GT : how does it keep the cost down
  PR : houde des it hipp the cost get
2026-01-04 01:14:46,659: Val batch 3000: PER (avg): 0.2803 CTC Loss (avg): 27.8139 WER(1gram): 64.47% (n=64) time: 6.500
2026-01-04 01:14:46,659: WER lens: avg_true_words=6.16 avg_pred_words=5.75 max_pred_words=11
2026-01-04 01:14:46,659: t15.2023.08.13 val PER: 0.2609
2026-01-04 01:14:46,659: t15.2023.08.18 val PER: 0.2188
2026-01-04 01:14:46,660: t15.2023.08.20 val PER: 0.2153
2026-01-04 01:14:46,660: t15.2023.08.25 val PER: 0.1973
2026-01-04 01:14:46,660: t15.2023.08.27 val PER: 0.2910
2026-01-04 01:14:46,660: t15.2023.09.01 val PER: 0.1867
2026-01-04 01:14:46,660: t15.2023.09.03 val PER: 0.2803
2026-01-04 01:14:46,660: t15.2023.09.24 val PER: 0.2172
2026-01-04 01:14:46,660: t15.2023.09.29 val PER: 0.2374
2026-01-04 01:14:46,660: t15.2023.10.01 val PER: 0.2886
2026-01-04 01:14:46,660: t15.2023.10.06 val PER: 0.1927
2026-01-04 01:14:46,660: t15.2023.10.08 val PER: 0.3410
2026-01-04 01:14:46,660: t15.2023.10.13 val PER: 0.3406
2026-01-04 01:14:46,660: t15.2023.10.15 val PER: 0.2709
2026-01-04 01:14:46,660: t15.2023.10.20 val PER: 0.2617
2026-01-04 01:14:46,660: t15.2023.10.22 val PER: 0.2105
2026-01-04 01:14:46,661: t15.2023.11.03 val PER: 0.2659
2026-01-04 01:14:46,661: t15.2023.11.04 val PER: 0.0887
2026-01-04 01:14:46,661: t15.2023.11.17 val PER: 0.1291
2026-01-04 01:14:46,661: t15.2023.11.19 val PER: 0.1257
2026-01-04 01:14:46,661: t15.2023.11.26 val PER: 0.2964
2026-01-04 01:14:46,661: t15.2023.12.03 val PER: 0.2584
2026-01-04 01:14:46,661: t15.2023.12.08 val PER: 0.2530
2026-01-04 01:14:46,661: t15.2023.12.10 val PER: 0.2273
2026-01-04 01:14:46,661: t15.2023.12.17 val PER: 0.2900
2026-01-04 01:14:46,661: t15.2023.12.29 val PER: 0.2766
2026-01-04 01:14:46,661: t15.2024.02.25 val PER: 0.2472
2026-01-04 01:14:46,661: t15.2024.03.08 val PER: 0.3542
2026-01-04 01:14:46,661: t15.2024.03.15 val PER: 0.3265
2026-01-04 01:14:46,661: t15.2024.03.17 val PER: 0.2887
2026-01-04 01:14:46,661: t15.2024.05.10 val PER: 0.2779
2026-01-04 01:14:46,662: t15.2024.06.14 val PER: 0.3013
2026-01-04 01:14:46,662: t15.2024.07.19 val PER: 0.3995
2026-01-04 01:14:46,662: t15.2024.07.21 val PER: 0.2303
2026-01-04 01:14:46,662: t15.2024.07.28 val PER: 0.2765
2026-01-04 01:14:46,662: t15.2025.01.10 val PER: 0.4917
2026-01-04 01:14:46,662: t15.2025.01.12 val PER: 0.3210
2026-01-04 01:14:46,662: t15.2025.03.14 val PER: 0.4393
2026-01-04 01:14:46,662: t15.2025.03.16 val PER: 0.3312
2026-01-04 01:14:46,662: t15.2025.03.30 val PER: 0.4851
2026-01-04 01:14:46,662: t15.2025.04.13 val PER: 0.3581
2026-01-04 01:14:46,663: New best val WER(1gram) 67.26% --> 64.47%
2026-01-04 01:14:46,663: Checkpointing model
2026-01-04 01:14:47,248: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/best_checkpoint
2026-01-04 01:14:47,494: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_3000
2026-01-04 01:15:04,881: Train batch 3200: loss: 26.28 grad norm: 71.64 time: 0.075
2026-01-04 01:15:21,947: Train batch 3400: loss: 18.63 grad norm: 57.58 time: 0.048
2026-01-04 01:15:30,678: Running test after training batch: 3500
2026-01-04 01:15:30,805: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:15:35,671: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned eke the code at this point will
2026-01-04 01:15:35,699: WER debug example
  GT : how does it keep the cost down
  PR : houde des it keep thus cussed get
2026-01-04 01:15:37,240: Val batch 3500: PER (avg): 0.2679 CTC Loss (avg): 26.6456 WER(1gram): 64.97% (n=64) time: 6.562
2026-01-04 01:15:37,241: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-04 01:15:37,241: t15.2023.08.13 val PER: 0.2391
2026-01-04 01:15:37,241: t15.2023.08.18 val PER: 0.2096
2026-01-04 01:15:37,241: t15.2023.08.20 val PER: 0.2240
2026-01-04 01:15:37,241: t15.2023.08.25 val PER: 0.1777
2026-01-04 01:15:37,241: t15.2023.08.27 val PER: 0.2653
2026-01-04 01:15:37,241: t15.2023.09.01 val PER: 0.1818
2026-01-04 01:15:37,241: t15.2023.09.03 val PER: 0.2470
2026-01-04 01:15:37,241: t15.2023.09.24 val PER: 0.2136
2026-01-04 01:15:37,242: t15.2023.09.29 val PER: 0.2189
2026-01-04 01:15:37,242: t15.2023.10.01 val PER: 0.2682
2026-01-04 01:15:37,242: t15.2023.10.06 val PER: 0.1905
2026-01-04 01:15:37,242: t15.2023.10.08 val PER: 0.3396
2026-01-04 01:15:37,242: t15.2023.10.13 val PER: 0.3119
2026-01-04 01:15:37,242: t15.2023.10.15 val PER: 0.2459
2026-01-04 01:15:37,242: t15.2023.10.20 val PER: 0.2617
2026-01-04 01:15:37,242: t15.2023.10.22 val PER: 0.2038
2026-01-04 01:15:37,242: t15.2023.11.03 val PER: 0.2571
2026-01-04 01:15:37,242: t15.2023.11.04 val PER: 0.0717
2026-01-04 01:15:37,242: t15.2023.11.17 val PER: 0.1229
2026-01-04 01:15:37,242: t15.2023.11.19 val PER: 0.0978
2026-01-04 01:15:37,242: t15.2023.11.26 val PER: 0.2942
2026-01-04 01:15:37,242: t15.2023.12.03 val PER: 0.2290
2026-01-04 01:15:37,243: t15.2023.12.08 val PER: 0.2550
2026-01-04 01:15:37,243: t15.2023.12.10 val PER: 0.2024
2026-01-04 01:15:37,243: t15.2023.12.17 val PER: 0.2609
2026-01-04 01:15:37,243: t15.2023.12.29 val PER: 0.2581
2026-01-04 01:15:37,243: t15.2024.02.25 val PER: 0.2135
2026-01-04 01:15:37,243: t15.2024.03.08 val PER: 0.3471
2026-01-04 01:15:37,243: t15.2024.03.15 val PER: 0.3233
2026-01-04 01:15:37,243: t15.2024.03.17 val PER: 0.2727
2026-01-04 01:15:37,243: t15.2024.05.10 val PER: 0.2838
2026-01-04 01:15:37,244: t15.2024.06.14 val PER: 0.2792
2026-01-04 01:15:37,244: t15.2024.07.19 val PER: 0.3962
2026-01-04 01:15:37,244: t15.2024.07.21 val PER: 0.2303
2026-01-04 01:15:37,244: t15.2024.07.28 val PER: 0.2824
2026-01-04 01:15:37,244: t15.2025.01.10 val PER: 0.4738
2026-01-04 01:15:37,244: t15.2025.01.12 val PER: 0.3018
2026-01-04 01:15:37,244: t15.2025.03.14 val PER: 0.4438
2026-01-04 01:15:37,244: t15.2025.03.16 val PER: 0.3325
2026-01-04 01:15:37,244: t15.2025.03.30 val PER: 0.4552
2026-01-04 01:15:37,245: t15.2025.04.13 val PER: 0.3295
2026-01-04 01:15:37,481: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_3500
2026-01-04 01:15:46,050: Train batch 3600: loss: 22.33 grad norm: 61.46 time: 0.066
2026-01-04 01:16:02,969: Train batch 3800: loss: 25.64 grad norm: 67.36 time: 0.066
2026-01-04 01:16:20,483: Train batch 4000: loss: 19.55 grad norm: 53.11 time: 0.056
2026-01-04 01:16:20,484: Running test after training batch: 4000
2026-01-04 01:16:20,631: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:16:25,620: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point will
2026-01-04 01:16:25,648: WER debug example
  GT : how does it keep the cost down
  PR : aue dust it keep the cussed et
2026-01-04 01:16:27,271: Val batch 4000: PER (avg): 0.2488 CTC Loss (avg): 24.4205 WER(1gram): 64.21% (n=64) time: 6.787
2026-01-04 01:16:27,272: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-04 01:16:27,272: t15.2023.08.13 val PER: 0.2277
2026-01-04 01:16:27,272: t15.2023.08.18 val PER: 0.2079
2026-01-04 01:16:27,272: t15.2023.08.20 val PER: 0.1930
2026-01-04 01:16:27,272: t15.2023.08.25 val PER: 0.1596
2026-01-04 01:16:27,272: t15.2023.08.27 val PER: 0.2717
2026-01-04 01:16:27,272: t15.2023.09.01 val PER: 0.1607
2026-01-04 01:16:27,272: t15.2023.09.03 val PER: 0.2506
2026-01-04 01:16:27,272: t15.2023.09.24 val PER: 0.2039
2026-01-04 01:16:27,272: t15.2023.09.29 val PER: 0.2087
2026-01-04 01:16:27,273: t15.2023.10.01 val PER: 0.2668
2026-01-04 01:16:27,273: t15.2023.10.06 val PER: 0.1636
2026-01-04 01:16:27,273: t15.2023.10.08 val PER: 0.3153
2026-01-04 01:16:27,273: t15.2023.10.13 val PER: 0.2979
2026-01-04 01:16:27,273: t15.2023.10.15 val PER: 0.2393
2026-01-04 01:16:27,273: t15.2023.10.20 val PER: 0.2315
2026-01-04 01:16:27,273: t15.2023.10.22 val PER: 0.2016
2026-01-04 01:16:27,273: t15.2023.11.03 val PER: 0.2531
2026-01-04 01:16:27,273: t15.2023.11.04 val PER: 0.0648
2026-01-04 01:16:27,273: t15.2023.11.17 val PER: 0.1058
2026-01-04 01:16:27,273: t15.2023.11.19 val PER: 0.1038
2026-01-04 01:16:27,274: t15.2023.11.26 val PER: 0.2659
2026-01-04 01:16:27,274: t15.2023.12.03 val PER: 0.2153
2026-01-04 01:16:27,274: t15.2023.12.08 val PER: 0.2184
2026-01-04 01:16:27,274: t15.2023.12.10 val PER: 0.1761
2026-01-04 01:16:27,274: t15.2023.12.17 val PER: 0.2360
2026-01-04 01:16:27,274: t15.2023.12.29 val PER: 0.2553
2026-01-04 01:16:27,274: t15.2024.02.25 val PER: 0.2177
2026-01-04 01:16:27,274: t15.2024.03.08 val PER: 0.3243
2026-01-04 01:16:27,274: t15.2024.03.15 val PER: 0.2921
2026-01-04 01:16:27,274: t15.2024.03.17 val PER: 0.2636
2026-01-04 01:16:27,275: t15.2024.05.10 val PER: 0.2645
2026-01-04 01:16:27,275: t15.2024.06.14 val PER: 0.2634
2026-01-04 01:16:27,275: t15.2024.07.19 val PER: 0.3632
2026-01-04 01:16:27,275: t15.2024.07.21 val PER: 0.1945
2026-01-04 01:16:27,275: t15.2024.07.28 val PER: 0.2338
2026-01-04 01:16:27,275: t15.2025.01.10 val PER: 0.4091
2026-01-04 01:16:27,275: t15.2025.01.12 val PER: 0.2833
2026-01-04 01:16:27,275: t15.2025.03.14 val PER: 0.4053
2026-01-04 01:16:27,275: t15.2025.03.16 val PER: 0.3207
2026-01-04 01:16:27,275: t15.2025.03.30 val PER: 0.4011
2026-01-04 01:16:27,275: t15.2025.04.13 val PER: 0.3167
2026-01-04 01:16:27,277: New best val WER(1gram) 64.47% --> 64.21%
2026-01-04 01:16:27,277: Checkpointing model
2026-01-04 01:16:27,854: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/best_checkpoint
2026-01-04 01:16:28,100: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_4000
2026-01-04 01:16:45,402: Train batch 4200: loss: 22.94 grad norm: 66.37 time: 0.080
2026-01-04 01:17:02,823: Train batch 4400: loss: 17.11 grad norm: 60.58 time: 0.066
2026-01-04 01:17:11,587: Running test after training batch: 4500
2026-01-04 01:17:11,718: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:17:16,426: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-04 01:17:16,455: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the rust get
2026-01-04 01:17:18,062: Val batch 4500: PER (avg): 0.2378 CTC Loss (avg): 23.2619 WER(1gram): 59.90% (n=64) time: 6.475
2026-01-04 01:17:18,063: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=11
2026-01-04 01:17:18,063: t15.2023.08.13 val PER: 0.2110
2026-01-04 01:17:18,063: t15.2023.08.18 val PER: 0.1785
2026-01-04 01:17:18,063: t15.2023.08.20 val PER: 0.1930
2026-01-04 01:17:18,063: t15.2023.08.25 val PER: 0.1461
2026-01-04 01:17:18,063: t15.2023.08.27 val PER: 0.2508
2026-01-04 01:17:18,063: t15.2023.09.01 val PER: 0.1469
2026-01-04 01:17:18,063: t15.2023.09.03 val PER: 0.2375
2026-01-04 01:17:18,063: t15.2023.09.24 val PER: 0.1687
2026-01-04 01:17:18,063: t15.2023.09.29 val PER: 0.2023
2026-01-04 01:17:18,063: t15.2023.10.01 val PER: 0.2596
2026-01-04 01:17:18,063: t15.2023.10.06 val PER: 0.1550
2026-01-04 01:17:18,064: t15.2023.10.08 val PER: 0.3288
2026-01-04 01:17:18,064: t15.2023.10.13 val PER: 0.2979
2026-01-04 01:17:18,064: t15.2023.10.15 val PER: 0.2235
2026-01-04 01:17:18,064: t15.2023.10.20 val PER: 0.2383
2026-01-04 01:17:18,064: t15.2023.10.22 val PER: 0.1815
2026-01-04 01:17:18,064: t15.2023.11.03 val PER: 0.2381
2026-01-04 01:17:18,064: t15.2023.11.04 val PER: 0.0717
2026-01-04 01:17:18,064: t15.2023.11.17 val PER: 0.0902
2026-01-04 01:17:18,064: t15.2023.11.19 val PER: 0.0938
2026-01-04 01:17:18,064: t15.2023.11.26 val PER: 0.2645
2026-01-04 01:17:18,065: t15.2023.12.03 val PER: 0.2143
2026-01-04 01:17:18,065: t15.2023.12.08 val PER: 0.2017
2026-01-04 01:17:18,065: t15.2023.12.10 val PER: 0.1656
2026-01-04 01:17:18,065: t15.2023.12.17 val PER: 0.2162
2026-01-04 01:17:18,065: t15.2023.12.29 val PER: 0.2388
2026-01-04 01:17:18,065: t15.2024.02.25 val PER: 0.1896
2026-01-04 01:17:18,065: t15.2024.03.08 val PER: 0.3101
2026-01-04 01:17:18,065: t15.2024.03.15 val PER: 0.2883
2026-01-04 01:17:18,065: t15.2024.03.17 val PER: 0.2517
2026-01-04 01:17:18,065: t15.2024.05.10 val PER: 0.2571
2026-01-04 01:17:18,065: t15.2024.06.14 val PER: 0.2508
2026-01-04 01:17:18,065: t15.2024.07.19 val PER: 0.3454
2026-01-04 01:17:18,065: t15.2024.07.21 val PER: 0.1814
2026-01-04 01:17:18,065: t15.2024.07.28 val PER: 0.2265
2026-01-04 01:17:18,065: t15.2025.01.10 val PER: 0.4311
2026-01-04 01:17:18,066: t15.2025.01.12 val PER: 0.2702
2026-01-04 01:17:18,066: t15.2025.03.14 val PER: 0.4053
2026-01-04 01:17:18,066: t15.2025.03.16 val PER: 0.2827
2026-01-04 01:17:18,066: t15.2025.03.30 val PER: 0.4230
2026-01-04 01:17:18,066: t15.2025.04.13 val PER: 0.2867
2026-01-04 01:17:18,067: New best val WER(1gram) 64.21% --> 59.90%
2026-01-04 01:17:18,067: Checkpointing model
2026-01-04 01:17:18,663: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/best_checkpoint
2026-01-04 01:17:18,910: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_4500
2026-01-04 01:17:27,609: Train batch 4600: loss: 20.52 grad norm: 67.36 time: 0.063
2026-01-04 01:17:45,186: Train batch 4800: loss: 13.56 grad norm: 51.31 time: 0.063
2026-01-04 01:18:02,708: Train batch 5000: loss: 31.29 grad norm: 82.55 time: 0.064
2026-01-04 01:18:02,708: Running test after training batch: 5000
2026-01-04 01:18:02,801: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:18:07,871: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-04 01:18:07,899: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cost get
2026-01-04 01:18:09,518: Val batch 5000: PER (avg): 0.2257 CTC Loss (avg): 22.2067 WER(1gram): 61.17% (n=64) time: 6.810
2026-01-04 01:18:09,518: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 01:18:09,519: t15.2023.08.13 val PER: 0.2069
2026-01-04 01:18:09,519: t15.2023.08.18 val PER: 0.1727
2026-01-04 01:18:09,519: t15.2023.08.20 val PER: 0.1795
2026-01-04 01:18:09,519: t15.2023.08.25 val PER: 0.1325
2026-01-04 01:18:09,519: t15.2023.08.27 val PER: 0.2395
2026-01-04 01:18:09,519: t15.2023.09.01 val PER: 0.1380
2026-01-04 01:18:09,519: t15.2023.09.03 val PER: 0.2268
2026-01-04 01:18:09,519: t15.2023.09.24 val PER: 0.1893
2026-01-04 01:18:09,519: t15.2023.09.29 val PER: 0.1819
2026-01-04 01:18:09,520: t15.2023.10.01 val PER: 0.2464
2026-01-04 01:18:09,520: t15.2023.10.06 val PER: 0.1529
2026-01-04 01:18:09,520: t15.2023.10.08 val PER: 0.3004
2026-01-04 01:18:09,520: t15.2023.10.13 val PER: 0.2894
2026-01-04 01:18:09,520: t15.2023.10.15 val PER: 0.2235
2026-01-04 01:18:09,520: t15.2023.10.20 val PER: 0.2383
2026-01-04 01:18:09,520: t15.2023.10.22 val PER: 0.1693
2026-01-04 01:18:09,520: t15.2023.11.03 val PER: 0.2225
2026-01-04 01:18:09,520: t15.2023.11.04 val PER: 0.0614
2026-01-04 01:18:09,520: t15.2023.11.17 val PER: 0.0840
2026-01-04 01:18:09,520: t15.2023.11.19 val PER: 0.0758
2026-01-04 01:18:09,520: t15.2023.11.26 val PER: 0.2348
2026-01-04 01:18:09,520: t15.2023.12.03 val PER: 0.1996
2026-01-04 01:18:09,520: t15.2023.12.08 val PER: 0.1911
2026-01-04 01:18:09,521: t15.2023.12.10 val PER: 0.1537
2026-01-04 01:18:09,521: t15.2023.12.17 val PER: 0.2173
2026-01-04 01:18:09,521: t15.2023.12.29 val PER: 0.2210
2026-01-04 01:18:09,521: t15.2024.02.25 val PER: 0.1826
2026-01-04 01:18:09,521: t15.2024.03.08 val PER: 0.3073
2026-01-04 01:18:09,521: t15.2024.03.15 val PER: 0.2839
2026-01-04 01:18:09,521: t15.2024.03.17 val PER: 0.2322
2026-01-04 01:18:09,521: t15.2024.05.10 val PER: 0.2437
2026-01-04 01:18:09,521: t15.2024.06.14 val PER: 0.2492
2026-01-04 01:18:09,521: t15.2024.07.19 val PER: 0.3322
2026-01-04 01:18:09,521: t15.2024.07.21 val PER: 0.1800
2026-01-04 01:18:09,521: t15.2024.07.28 val PER: 0.2074
2026-01-04 01:18:09,521: t15.2025.01.10 val PER: 0.3829
2026-01-04 01:18:09,521: t15.2025.01.12 val PER: 0.2417
2026-01-04 01:18:09,521: t15.2025.03.14 val PER: 0.3920
2026-01-04 01:18:09,521: t15.2025.03.16 val PER: 0.2670
2026-01-04 01:18:09,522: t15.2025.03.30 val PER: 0.3966
2026-01-04 01:18:09,522: t15.2025.04.13 val PER: 0.2924
2026-01-04 01:18:09,756: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_5000
2026-01-04 01:18:27,304: Train batch 5200: loss: 16.25 grad norm: 56.97 time: 0.051
2026-01-04 01:18:44,856: Train batch 5400: loss: 18.20 grad norm: 60.24 time: 0.068
2026-01-04 01:18:53,718: Running test after training batch: 5500
2026-01-04 01:18:53,869: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:18:58,585: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 01:18:58,613: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 01:19:00,184: Val batch 5500: PER (avg): 0.2173 CTC Loss (avg): 21.1454 WER(1gram): 57.11% (n=64) time: 6.466
2026-01-04 01:19:00,185: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-04 01:19:00,185: t15.2023.08.13 val PER: 0.1923
2026-01-04 01:19:00,185: t15.2023.08.18 val PER: 0.1601
2026-01-04 01:19:00,185: t15.2023.08.20 val PER: 0.1787
2026-01-04 01:19:00,185: t15.2023.08.25 val PER: 0.1295
2026-01-04 01:19:00,185: t15.2023.08.27 val PER: 0.2331
2026-01-04 01:19:00,185: t15.2023.09.01 val PER: 0.1323
2026-01-04 01:19:00,185: t15.2023.09.03 val PER: 0.2221
2026-01-04 01:19:00,185: t15.2023.09.24 val PER: 0.1748
2026-01-04 01:19:00,186: t15.2023.09.29 val PER: 0.1755
2026-01-04 01:19:00,186: t15.2023.10.01 val PER: 0.2338
2026-01-04 01:19:00,186: t15.2023.10.06 val PER: 0.1356
2026-01-04 01:19:00,186: t15.2023.10.08 val PER: 0.3085
2026-01-04 01:19:00,186: t15.2023.10.13 val PER: 0.2793
2026-01-04 01:19:00,186: t15.2023.10.15 val PER: 0.2076
2026-01-04 01:19:00,186: t15.2023.10.20 val PER: 0.2416
2026-01-04 01:19:00,186: t15.2023.10.22 val PER: 0.1514
2026-01-04 01:19:00,186: t15.2023.11.03 val PER: 0.2225
2026-01-04 01:19:00,186: t15.2023.11.04 val PER: 0.0751
2026-01-04 01:19:00,186: t15.2023.11.17 val PER: 0.0871
2026-01-04 01:19:00,186: t15.2023.11.19 val PER: 0.0739
2026-01-04 01:19:00,186: t15.2023.11.26 val PER: 0.2181
2026-01-04 01:19:00,186: t15.2023.12.03 val PER: 0.1933
2026-01-04 01:19:00,187: t15.2023.12.08 val PER: 0.1844
2026-01-04 01:19:00,187: t15.2023.12.10 val PER: 0.1537
2026-01-04 01:19:00,187: t15.2023.12.17 val PER: 0.2141
2026-01-04 01:19:00,187: t15.2023.12.29 val PER: 0.2155
2026-01-04 01:19:00,187: t15.2024.02.25 val PER: 0.1699
2026-01-04 01:19:00,187: t15.2024.03.08 val PER: 0.2945
2026-01-04 01:19:00,187: t15.2024.03.15 val PER: 0.2645
2026-01-04 01:19:00,187: t15.2024.03.17 val PER: 0.2245
2026-01-04 01:19:00,187: t15.2024.05.10 val PER: 0.2333
2026-01-04 01:19:00,187: t15.2024.06.14 val PER: 0.2492
2026-01-04 01:19:00,187: t15.2024.07.19 val PER: 0.3250
2026-01-04 01:19:00,187: t15.2024.07.21 val PER: 0.1641
2026-01-04 01:19:00,187: t15.2024.07.28 val PER: 0.1919
2026-01-04 01:19:00,188: t15.2025.01.10 val PER: 0.4008
2026-01-04 01:19:00,188: t15.2025.01.12 val PER: 0.2402
2026-01-04 01:19:00,188: t15.2025.03.14 val PER: 0.3580
2026-01-04 01:19:00,188: t15.2025.03.16 val PER: 0.2657
2026-01-04 01:19:00,188: t15.2025.03.30 val PER: 0.3724
2026-01-04 01:19:00,188: t15.2025.04.13 val PER: 0.2953
2026-01-04 01:19:00,189: New best val WER(1gram) 59.90% --> 57.11%
2026-01-04 01:19:00,189: Checkpointing model
2026-01-04 01:19:00,805: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/best_checkpoint
2026-01-04 01:19:01,049: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_5500
2026-01-04 01:19:09,657: Train batch 5600: loss: 19.77 grad norm: 66.66 time: 0.061
2026-01-04 01:19:26,989: Train batch 5800: loss: 13.67 grad norm: 55.49 time: 0.081
2026-01-04 01:19:44,095: Train batch 6000: loss: 14.33 grad norm: 55.88 time: 0.049
2026-01-04 01:19:44,095: Running test after training batch: 6000
2026-01-04 01:19:44,203: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:19:48,940: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 01:19:48,971: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 01:19:50,610: Val batch 6000: PER (avg): 0.2149 CTC Loss (avg): 21.0888 WER(1gram): 57.87% (n=64) time: 6.514
2026-01-04 01:19:50,610: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-04 01:19:50,610: t15.2023.08.13 val PER: 0.1746
2026-01-04 01:19:50,610: t15.2023.08.18 val PER: 0.1668
2026-01-04 01:19:50,610: t15.2023.08.20 val PER: 0.1676
2026-01-04 01:19:50,610: t15.2023.08.25 val PER: 0.1190
2026-01-04 01:19:50,610: t15.2023.08.27 val PER: 0.2379
2026-01-04 01:19:50,611: t15.2023.09.01 val PER: 0.1388
2026-01-04 01:19:50,611: t15.2023.09.03 val PER: 0.2197
2026-01-04 01:19:50,611: t15.2023.09.24 val PER: 0.1638
2026-01-04 01:19:50,611: t15.2023.09.29 val PER: 0.1698
2026-01-04 01:19:50,611: t15.2023.10.01 val PER: 0.2292
2026-01-04 01:19:50,611: t15.2023.10.06 val PER: 0.1324
2026-01-04 01:19:50,611: t15.2023.10.08 val PER: 0.2828
2026-01-04 01:19:50,611: t15.2023.10.13 val PER: 0.2700
2026-01-04 01:19:50,611: t15.2023.10.15 val PER: 0.2254
2026-01-04 01:19:50,611: t15.2023.10.20 val PER: 0.2181
2026-01-04 01:19:50,611: t15.2023.10.22 val PER: 0.1682
2026-01-04 01:19:50,611: t15.2023.11.03 val PER: 0.2341
2026-01-04 01:19:50,612: t15.2023.11.04 val PER: 0.0683
2026-01-04 01:19:50,612: t15.2023.11.17 val PER: 0.0902
2026-01-04 01:19:50,612: t15.2023.11.19 val PER: 0.0798
2026-01-04 01:19:50,612: t15.2023.11.26 val PER: 0.2319
2026-01-04 01:19:50,612: t15.2023.12.03 val PER: 0.1712
2026-01-04 01:19:50,612: t15.2023.12.08 val PER: 0.1798
2026-01-04 01:19:50,612: t15.2023.12.10 val PER: 0.1564
2026-01-04 01:19:50,612: t15.2023.12.17 val PER: 0.2027
2026-01-04 01:19:50,612: t15.2023.12.29 val PER: 0.2313
2026-01-04 01:19:50,612: t15.2024.02.25 val PER: 0.1601
2026-01-04 01:19:50,612: t15.2024.03.08 val PER: 0.3058
2026-01-04 01:19:50,612: t15.2024.03.15 val PER: 0.2770
2026-01-04 01:19:50,612: t15.2024.03.17 val PER: 0.2099
2026-01-04 01:19:50,613: t15.2024.05.10 val PER: 0.2333
2026-01-04 01:19:50,613: t15.2024.06.14 val PER: 0.2256
2026-01-04 01:19:50,613: t15.2024.07.19 val PER: 0.3144
2026-01-04 01:19:50,613: t15.2024.07.21 val PER: 0.1683
2026-01-04 01:19:50,613: t15.2024.07.28 val PER: 0.1985
2026-01-04 01:19:50,613: t15.2025.01.10 val PER: 0.3760
2026-01-04 01:19:50,613: t15.2025.01.12 val PER: 0.2163
2026-01-04 01:19:50,613: t15.2025.03.14 val PER: 0.3772
2026-01-04 01:19:50,613: t15.2025.03.16 val PER: 0.2618
2026-01-04 01:19:50,613: t15.2025.03.30 val PER: 0.3701
2026-01-04 01:19:50,613: t15.2025.04.13 val PER: 0.2668
2026-01-04 01:19:50,851: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_6000
2026-01-04 01:20:07,922: Train batch 6200: loss: 16.63 grad norm: 57.83 time: 0.070
2026-01-04 01:20:25,055: Train batch 6400: loss: 18.87 grad norm: 65.28 time: 0.061
2026-01-04 01:20:33,497: Running test after training batch: 6500
2026-01-04 01:20:33,635: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:20:38,568: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point as will
2026-01-04 01:20:38,596: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-04 01:20:40,187: Val batch 6500: PER (avg): 0.2038 CTC Loss (avg): 20.0863 WER(1gram): 52.03% (n=64) time: 6.690
2026-01-04 01:20:40,188: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 01:20:40,188: t15.2023.08.13 val PER: 0.1663
2026-01-04 01:20:40,188: t15.2023.08.18 val PER: 0.1425
2026-01-04 01:20:40,188: t15.2023.08.20 val PER: 0.1581
2026-01-04 01:20:40,188: t15.2023.08.25 val PER: 0.1084
2026-01-04 01:20:40,188: t15.2023.08.27 val PER: 0.2379
2026-01-04 01:20:40,188: t15.2023.09.01 val PER: 0.1193
2026-01-04 01:20:40,188: t15.2023.09.03 val PER: 0.2185
2026-01-04 01:20:40,189: t15.2023.09.24 val PER: 0.1638
2026-01-04 01:20:40,189: t15.2023.09.29 val PER: 0.1717
2026-01-04 01:20:40,189: t15.2023.10.01 val PER: 0.2173
2026-01-04 01:20:40,189: t15.2023.10.06 val PER: 0.1184
2026-01-04 01:20:40,189: t15.2023.10.08 val PER: 0.2991
2026-01-04 01:20:40,189: t15.2023.10.13 val PER: 0.2653
2026-01-04 01:20:40,189: t15.2023.10.15 val PER: 0.2090
2026-01-04 01:20:40,189: t15.2023.10.20 val PER: 0.2215
2026-01-04 01:20:40,189: t15.2023.10.22 val PER: 0.1537
2026-01-04 01:20:40,189: t15.2023.11.03 val PER: 0.2151
2026-01-04 01:20:40,189: t15.2023.11.04 val PER: 0.0580
2026-01-04 01:20:40,189: t15.2023.11.17 val PER: 0.0591
2026-01-04 01:20:40,189: t15.2023.11.19 val PER: 0.0739
2026-01-04 01:20:40,189: t15.2023.11.26 val PER: 0.2203
2026-01-04 01:20:40,189: t15.2023.12.03 val PER: 0.1586
2026-01-04 01:20:40,190: t15.2023.12.08 val PER: 0.1764
2026-01-04 01:20:40,190: t15.2023.12.10 val PER: 0.1445
2026-01-04 01:20:40,190: t15.2023.12.17 val PER: 0.1892
2026-01-04 01:20:40,190: t15.2023.12.29 val PER: 0.2018
2026-01-04 01:20:40,190: t15.2024.02.25 val PER: 0.1713
2026-01-04 01:20:40,190: t15.2024.03.08 val PER: 0.2817
2026-01-04 01:20:40,190: t15.2024.03.15 val PER: 0.2558
2026-01-04 01:20:40,190: t15.2024.03.17 val PER: 0.1925
2026-01-04 01:20:40,190: t15.2024.05.10 val PER: 0.2214
2026-01-04 01:20:40,190: t15.2024.06.14 val PER: 0.2129
2026-01-04 01:20:40,190: t15.2024.07.19 val PER: 0.3045
2026-01-04 01:20:40,190: t15.2024.07.21 val PER: 0.1462
2026-01-04 01:20:40,190: t15.2024.07.28 val PER: 0.1890
2026-01-04 01:20:40,191: t15.2025.01.10 val PER: 0.3788
2026-01-04 01:20:40,191: t15.2025.01.12 val PER: 0.2109
2026-01-04 01:20:40,191: t15.2025.03.14 val PER: 0.3876
2026-01-04 01:20:40,191: t15.2025.03.16 val PER: 0.2382
2026-01-04 01:20:40,192: t15.2025.03.30 val PER: 0.3552
2026-01-04 01:20:40,192: t15.2025.04.13 val PER: 0.2725
2026-01-04 01:20:40,192: New best val WER(1gram) 57.11% --> 52.03%
2026-01-04 01:20:40,192: Checkpointing model
2026-01-04 01:20:40,759: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/best_checkpoint
2026-01-04 01:20:41,004: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_6500
2026-01-04 01:20:49,296: Train batch 6600: loss: 12.59 grad norm: 53.86 time: 0.044
2026-01-04 01:21:06,883: Train batch 6800: loss: 16.02 grad norm: 57.86 time: 0.047
2026-01-04 01:21:24,366: Train batch 7000: loss: 17.44 grad norm: 66.29 time: 0.060
2026-01-04 01:21:24,366: Running test after training batch: 7000
2026-01-04 01:21:24,510: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:21:29,206: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-04 01:21:29,236: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-04 01:21:30,901: Val batch 7000: PER (avg): 0.1964 CTC Loss (avg): 19.3183 WER(1gram): 54.06% (n=64) time: 6.534
2026-01-04 01:21:30,901: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-04 01:21:30,902: t15.2023.08.13 val PER: 0.1632
2026-01-04 01:21:30,902: t15.2023.08.18 val PER: 0.1391
2026-01-04 01:21:30,902: t15.2023.08.20 val PER: 0.1533
2026-01-04 01:21:30,902: t15.2023.08.25 val PER: 0.0964
2026-01-04 01:21:30,902: t15.2023.08.27 val PER: 0.2090
2026-01-04 01:21:30,902: t15.2023.09.01 val PER: 0.1136
2026-01-04 01:21:30,902: t15.2023.09.03 val PER: 0.1924
2026-01-04 01:21:30,902: t15.2023.09.24 val PER: 0.1650
2026-01-04 01:21:30,902: t15.2023.09.29 val PER: 0.1646
2026-01-04 01:21:30,902: t15.2023.10.01 val PER: 0.2120
2026-01-04 01:21:30,902: t15.2023.10.06 val PER: 0.1109
2026-01-04 01:21:30,903: t15.2023.10.08 val PER: 0.2801
2026-01-04 01:21:30,903: t15.2023.10.13 val PER: 0.2622
2026-01-04 01:21:30,903: t15.2023.10.15 val PER: 0.1964
2026-01-04 01:21:30,903: t15.2023.10.20 val PER: 0.2013
2026-01-04 01:21:30,903: t15.2023.10.22 val PER: 0.1381
2026-01-04 01:21:30,903: t15.2023.11.03 val PER: 0.1981
2026-01-04 01:21:30,903: t15.2023.11.04 val PER: 0.0478
2026-01-04 01:21:30,903: t15.2023.11.17 val PER: 0.0684
2026-01-04 01:21:30,904: t15.2023.11.19 val PER: 0.0599
2026-01-04 01:21:30,904: t15.2023.11.26 val PER: 0.2007
2026-01-04 01:21:30,904: t15.2023.12.03 val PER: 0.1628
2026-01-04 01:21:30,904: t15.2023.12.08 val PER: 0.1571
2026-01-04 01:21:30,904: t15.2023.12.10 val PER: 0.1419
2026-01-04 01:21:30,904: t15.2023.12.17 val PER: 0.1840
2026-01-04 01:21:30,904: t15.2023.12.29 val PER: 0.2045
2026-01-04 01:21:30,904: t15.2024.02.25 val PER: 0.1601
2026-01-04 01:21:30,904: t15.2024.03.08 val PER: 0.2888
2026-01-04 01:21:30,904: t15.2024.03.15 val PER: 0.2420
2026-01-04 01:21:30,904: t15.2024.03.17 val PER: 0.2057
2026-01-04 01:21:30,904: t15.2024.05.10 val PER: 0.1976
2026-01-04 01:21:30,904: t15.2024.06.14 val PER: 0.2129
2026-01-04 01:21:30,904: t15.2024.07.19 val PER: 0.3032
2026-01-04 01:21:30,904: t15.2024.07.21 val PER: 0.1303
2026-01-04 01:21:30,905: t15.2024.07.28 val PER: 0.1713
2026-01-04 01:21:30,905: t15.2025.01.10 val PER: 0.3691
2026-01-04 01:21:30,905: t15.2025.01.12 val PER: 0.2156
2026-01-04 01:21:30,905: t15.2025.03.14 val PER: 0.3669
2026-01-04 01:21:30,905: t15.2025.03.16 val PER: 0.2552
2026-01-04 01:21:30,905: t15.2025.03.30 val PER: 0.3540
2026-01-04 01:21:30,905: t15.2025.04.13 val PER: 0.2739
2026-01-04 01:21:31,140: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_7000
2026-01-04 01:21:48,358: Train batch 7200: loss: 14.62 grad norm: 58.64 time: 0.078
2026-01-04 01:22:05,497: Train batch 7400: loss: 13.80 grad norm: 53.94 time: 0.075
2026-01-04 01:22:14,020: Running test after training batch: 7500
2026-01-04 01:22:14,114: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:22:18,814: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-04 01:22:18,844: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 01:22:20,553: Val batch 7500: PER (avg): 0.1925 CTC Loss (avg): 18.8725 WER(1gram): 54.31% (n=64) time: 6.533
2026-01-04 01:22:20,553: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 01:22:20,554: t15.2023.08.13 val PER: 0.1518
2026-01-04 01:22:20,554: t15.2023.08.18 val PER: 0.1358
2026-01-04 01:22:20,554: t15.2023.08.20 val PER: 0.1565
2026-01-04 01:22:20,554: t15.2023.08.25 val PER: 0.1039
2026-01-04 01:22:20,554: t15.2023.08.27 val PER: 0.2154
2026-01-04 01:22:20,554: t15.2023.09.01 val PER: 0.1185
2026-01-04 01:22:20,554: t15.2023.09.03 val PER: 0.1876
2026-01-04 01:22:20,554: t15.2023.09.24 val PER: 0.1505
2026-01-04 01:22:20,555: t15.2023.09.29 val PER: 0.1570
2026-01-04 01:22:20,555: t15.2023.10.01 val PER: 0.2061
2026-01-04 01:22:20,555: t15.2023.10.06 val PER: 0.1259
2026-01-04 01:22:20,555: t15.2023.10.08 val PER: 0.2788
2026-01-04 01:22:20,555: t15.2023.10.13 val PER: 0.2537
2026-01-04 01:22:20,555: t15.2023.10.15 val PER: 0.1991
2026-01-04 01:22:20,555: t15.2023.10.20 val PER: 0.1913
2026-01-04 01:22:20,555: t15.2023.10.22 val PER: 0.1459
2026-01-04 01:22:20,555: t15.2023.11.03 val PER: 0.2110
2026-01-04 01:22:20,555: t15.2023.11.04 val PER: 0.0546
2026-01-04 01:22:20,555: t15.2023.11.17 val PER: 0.0653
2026-01-04 01:22:20,555: t15.2023.11.19 val PER: 0.0599
2026-01-04 01:22:20,555: t15.2023.11.26 val PER: 0.1891
2026-01-04 01:22:20,555: t15.2023.12.03 val PER: 0.1639
2026-01-04 01:22:20,556: t15.2023.12.08 val PER: 0.1591
2026-01-04 01:22:20,556: t15.2023.12.10 val PER: 0.1314
2026-01-04 01:22:20,556: t15.2023.12.17 val PER: 0.1788
2026-01-04 01:22:20,556: t15.2023.12.29 val PER: 0.1839
2026-01-04 01:22:20,556: t15.2024.02.25 val PER: 0.1517
2026-01-04 01:22:20,556: t15.2024.03.08 val PER: 0.2802
2026-01-04 01:22:20,556: t15.2024.03.15 val PER: 0.2464
2026-01-04 01:22:20,556: t15.2024.03.17 val PER: 0.1925
2026-01-04 01:22:20,556: t15.2024.05.10 val PER: 0.1961
2026-01-04 01:22:20,556: t15.2024.06.14 val PER: 0.2035
2026-01-04 01:22:20,556: t15.2024.07.19 val PER: 0.2874
2026-01-04 01:22:20,556: t15.2024.07.21 val PER: 0.1359
2026-01-04 01:22:20,557: t15.2024.07.28 val PER: 0.1728
2026-01-04 01:22:20,557: t15.2025.01.10 val PER: 0.3512
2026-01-04 01:22:20,557: t15.2025.01.12 val PER: 0.2025
2026-01-04 01:22:20,557: t15.2025.03.14 val PER: 0.3550
2026-01-04 01:22:20,557: t15.2025.03.16 val PER: 0.2526
2026-01-04 01:22:20,557: t15.2025.03.30 val PER: 0.3494
2026-01-04 01:22:20,557: t15.2025.04.13 val PER: 0.2582
2026-01-04 01:22:20,794: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_7500
2026-01-04 01:22:29,499: Train batch 7600: loss: 16.67 grad norm: 60.29 time: 0.069
2026-01-04 01:22:46,877: Train batch 7800: loss: 15.28 grad norm: 68.62 time: 0.056
2026-01-04 01:23:04,473: Train batch 8000: loss: 11.40 grad norm: 52.84 time: 0.072
2026-01-04 01:23:04,473: Running test after training batch: 8000
2026-01-04 01:23:04,573: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:23:09,819: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-04 01:23:09,849: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost it
2026-01-04 01:23:11,543: Val batch 8000: PER (avg): 0.1880 CTC Loss (avg): 18.3182 WER(1gram): 53.05% (n=64) time: 7.069
2026-01-04 01:23:11,543: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-04 01:23:11,543: t15.2023.08.13 val PER: 0.1528
2026-01-04 01:23:11,543: t15.2023.08.18 val PER: 0.1282
2026-01-04 01:23:11,543: t15.2023.08.20 val PER: 0.1485
2026-01-04 01:23:11,543: t15.2023.08.25 val PER: 0.1145
2026-01-04 01:23:11,544: t15.2023.08.27 val PER: 0.2170
2026-01-04 01:23:11,544: t15.2023.09.01 val PER: 0.1031
2026-01-04 01:23:11,544: t15.2023.09.03 val PER: 0.1948
2026-01-04 01:23:11,544: t15.2023.09.24 val PER: 0.1553
2026-01-04 01:23:11,544: t15.2023.09.29 val PER: 0.1583
2026-01-04 01:23:11,544: t15.2023.10.01 val PER: 0.2008
2026-01-04 01:23:11,544: t15.2023.10.06 val PER: 0.1098
2026-01-04 01:23:11,544: t15.2023.10.08 val PER: 0.2801
2026-01-04 01:23:11,544: t15.2023.10.13 val PER: 0.2498
2026-01-04 01:23:11,544: t15.2023.10.15 val PER: 0.1958
2026-01-04 01:23:11,544: t15.2023.10.20 val PER: 0.1913
2026-01-04 01:23:11,544: t15.2023.10.22 val PER: 0.1414
2026-01-04 01:23:11,544: t15.2023.11.03 val PER: 0.2110
2026-01-04 01:23:11,544: t15.2023.11.04 val PER: 0.0341
2026-01-04 01:23:11,545: t15.2023.11.17 val PER: 0.0638
2026-01-04 01:23:11,545: t15.2023.11.19 val PER: 0.0639
2026-01-04 01:23:11,545: t15.2023.11.26 val PER: 0.1833
2026-01-04 01:23:11,545: t15.2023.12.03 val PER: 0.1597
2026-01-04 01:23:11,545: t15.2023.12.08 val PER: 0.1485
2026-01-04 01:23:11,545: t15.2023.12.10 val PER: 0.1353
2026-01-04 01:23:11,545: t15.2023.12.17 val PER: 0.1809
2026-01-04 01:23:11,545: t15.2023.12.29 val PER: 0.1812
2026-01-04 01:23:11,545: t15.2024.02.25 val PER: 0.1433
2026-01-04 01:23:11,545: t15.2024.03.08 val PER: 0.2745
2026-01-04 01:23:11,545: t15.2024.03.15 val PER: 0.2402
2026-01-04 01:23:11,545: t15.2024.03.17 val PER: 0.1834
2026-01-04 01:23:11,546: t15.2024.05.10 val PER: 0.1976
2026-01-04 01:23:11,546: t15.2024.06.14 val PER: 0.2050
2026-01-04 01:23:11,546: t15.2024.07.19 val PER: 0.2940
2026-01-04 01:23:11,546: t15.2024.07.21 val PER: 0.1255
2026-01-04 01:23:11,546: t15.2024.07.28 val PER: 0.1603
2026-01-04 01:23:11,546: t15.2025.01.10 val PER: 0.3292
2026-01-04 01:23:11,546: t15.2025.01.12 val PER: 0.1886
2026-01-04 01:23:11,546: t15.2025.03.14 val PER: 0.3521
2026-01-04 01:23:11,546: t15.2025.03.16 val PER: 0.2382
2026-01-04 01:23:11,546: t15.2025.03.30 val PER: 0.3437
2026-01-04 01:23:11,546: t15.2025.04.13 val PER: 0.2596
2026-01-04 01:23:11,780: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_8000
2026-01-04 01:23:29,105: Train batch 8200: loss: 9.86 grad norm: 48.11 time: 0.054
2026-01-04 01:23:46,513: Train batch 8400: loss: 9.73 grad norm: 46.08 time: 0.064
2026-01-04 01:23:55,438: Running test after training batch: 8500
2026-01-04 01:23:55,545: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:24:00,216: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 01:24:00,246: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 01:24:01,952: Val batch 8500: PER (avg): 0.1803 CTC Loss (avg): 17.8490 WER(1gram): 50.25% (n=64) time: 6.514
2026-01-04 01:24:01,952: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-04 01:24:01,952: t15.2023.08.13 val PER: 0.1414
2026-01-04 01:24:01,953: t15.2023.08.18 val PER: 0.1299
2026-01-04 01:24:01,953: t15.2023.08.20 val PER: 0.1414
2026-01-04 01:24:01,953: t15.2023.08.25 val PER: 0.1160
2026-01-04 01:24:01,953: t15.2023.08.27 val PER: 0.2170
2026-01-04 01:24:01,953: t15.2023.09.01 val PER: 0.1023
2026-01-04 01:24:01,953: t15.2023.09.03 val PER: 0.1924
2026-01-04 01:24:01,953: t15.2023.09.24 val PER: 0.1468
2026-01-04 01:24:01,953: t15.2023.09.29 val PER: 0.1532
2026-01-04 01:24:01,953: t15.2023.10.01 val PER: 0.1909
2026-01-04 01:24:01,953: t15.2023.10.06 val PER: 0.1033
2026-01-04 01:24:01,953: t15.2023.10.08 val PER: 0.2666
2026-01-04 01:24:01,953: t15.2023.10.13 val PER: 0.2389
2026-01-04 01:24:01,953: t15.2023.10.15 val PER: 0.1859
2026-01-04 01:24:01,953: t15.2023.10.20 val PER: 0.1946
2026-01-04 01:24:01,954: t15.2023.10.22 val PER: 0.1425
2026-01-04 01:24:01,954: t15.2023.11.03 val PER: 0.1967
2026-01-04 01:24:01,954: t15.2023.11.04 val PER: 0.0478
2026-01-04 01:24:01,954: t15.2023.11.17 val PER: 0.0544
2026-01-04 01:24:01,954: t15.2023.11.19 val PER: 0.0499
2026-01-04 01:24:01,954: t15.2023.11.26 val PER: 0.1746
2026-01-04 01:24:01,954: t15.2023.12.03 val PER: 0.1418
2026-01-04 01:24:01,954: t15.2023.12.08 val PER: 0.1465
2026-01-04 01:24:01,954: t15.2023.12.10 val PER: 0.1327
2026-01-04 01:24:01,954: t15.2023.12.17 val PER: 0.1653
2026-01-04 01:24:01,954: t15.2023.12.29 val PER: 0.1633
2026-01-04 01:24:01,954: t15.2024.02.25 val PER: 0.1348
2026-01-04 01:24:01,955: t15.2024.03.08 val PER: 0.2802
2026-01-04 01:24:01,955: t15.2024.03.15 val PER: 0.2289
2026-01-04 01:24:01,955: t15.2024.03.17 val PER: 0.1729
2026-01-04 01:24:01,955: t15.2024.05.10 val PER: 0.1976
2026-01-04 01:24:01,955: t15.2024.06.14 val PER: 0.1924
2026-01-04 01:24:01,955: t15.2024.07.19 val PER: 0.2709
2026-01-04 01:24:01,955: t15.2024.07.21 val PER: 0.1179
2026-01-04 01:24:01,955: t15.2024.07.28 val PER: 0.1676
2026-01-04 01:24:01,955: t15.2025.01.10 val PER: 0.3237
2026-01-04 01:24:01,955: t15.2025.01.12 val PER: 0.1809
2026-01-04 01:24:01,955: t15.2025.03.14 val PER: 0.3550
2026-01-04 01:24:01,955: t15.2025.03.16 val PER: 0.2173
2026-01-04 01:24:01,955: t15.2025.03.30 val PER: 0.3460
2026-01-04 01:24:01,955: t15.2025.04.13 val PER: 0.2368
2026-01-04 01:24:01,956: New best val WER(1gram) 52.03% --> 50.25%
2026-01-04 01:24:01,956: Checkpointing model
2026-01-04 01:24:02,568: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/best_checkpoint
2026-01-04 01:24:02,827: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_8500
2026-01-04 01:24:11,913: Train batch 8600: loss: 15.83 grad norm: 60.63 time: 0.056
2026-01-04 01:24:29,706: Train batch 8800: loss: 15.52 grad norm: 60.69 time: 0.060
2026-01-04 01:24:47,558: Train batch 9000: loss: 16.01 grad norm: 63.16 time: 0.072
2026-01-04 01:24:47,558: Running test after training batch: 9000
2026-01-04 01:24:47,676: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:24:52,607: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 01:24:52,637: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-04 01:24:54,355: Val batch 9000: PER (avg): 0.1766 CTC Loss (avg): 17.6142 WER(1gram): 52.28% (n=64) time: 6.797
2026-01-04 01:24:54,356: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 01:24:54,356: t15.2023.08.13 val PER: 0.1424
2026-01-04 01:24:54,356: t15.2023.08.18 val PER: 0.1232
2026-01-04 01:24:54,356: t15.2023.08.20 val PER: 0.1382
2026-01-04 01:24:54,356: t15.2023.08.25 val PER: 0.0949
2026-01-04 01:24:54,356: t15.2023.08.27 val PER: 0.2138
2026-01-04 01:24:54,356: t15.2023.09.01 val PER: 0.1023
2026-01-04 01:24:54,356: t15.2023.09.03 val PER: 0.1841
2026-01-04 01:24:54,356: t15.2023.09.24 val PER: 0.1444
2026-01-04 01:24:54,356: t15.2023.09.29 val PER: 0.1468
2026-01-04 01:24:54,357: t15.2023.10.01 val PER: 0.1929
2026-01-04 01:24:54,357: t15.2023.10.06 val PER: 0.0990
2026-01-04 01:24:54,357: t15.2023.10.08 val PER: 0.2503
2026-01-04 01:24:54,357: t15.2023.10.13 val PER: 0.2327
2026-01-04 01:24:54,357: t15.2023.10.15 val PER: 0.1852
2026-01-04 01:24:54,357: t15.2023.10.20 val PER: 0.1946
2026-01-04 01:24:54,357: t15.2023.10.22 val PER: 0.1347
2026-01-04 01:24:54,357: t15.2023.11.03 val PER: 0.2069
2026-01-04 01:24:54,357: t15.2023.11.04 val PER: 0.0444
2026-01-04 01:24:54,357: t15.2023.11.17 val PER: 0.0591
2026-01-04 01:24:54,357: t15.2023.11.19 val PER: 0.0479
2026-01-04 01:24:54,357: t15.2023.11.26 val PER: 0.1710
2026-01-04 01:24:54,357: t15.2023.12.03 val PER: 0.1355
2026-01-04 01:24:54,358: t15.2023.12.08 val PER: 0.1352
2026-01-04 01:24:54,358: t15.2023.12.10 val PER: 0.1156
2026-01-04 01:24:54,358: t15.2023.12.17 val PER: 0.1653
2026-01-04 01:24:54,358: t15.2023.12.29 val PER: 0.1757
2026-01-04 01:24:54,358: t15.2024.02.25 val PER: 0.1433
2026-01-04 01:24:54,358: t15.2024.03.08 val PER: 0.2589
2026-01-04 01:24:54,358: t15.2024.03.15 val PER: 0.2270
2026-01-04 01:24:54,358: t15.2024.03.17 val PER: 0.1750
2026-01-04 01:24:54,358: t15.2024.05.10 val PER: 0.1857
2026-01-04 01:24:54,358: t15.2024.06.14 val PER: 0.1782
2026-01-04 01:24:54,358: t15.2024.07.19 val PER: 0.2690
2026-01-04 01:24:54,358: t15.2024.07.21 val PER: 0.1131
2026-01-04 01:24:54,358: t15.2024.07.28 val PER: 0.1632
2026-01-04 01:24:54,358: t15.2025.01.10 val PER: 0.3044
2026-01-04 01:24:54,358: t15.2025.01.12 val PER: 0.1763
2026-01-04 01:24:54,359: t15.2025.03.14 val PER: 0.3476
2026-01-04 01:24:54,359: t15.2025.03.16 val PER: 0.2212
2026-01-04 01:24:54,359: t15.2025.03.30 val PER: 0.3333
2026-01-04 01:24:54,359: t15.2025.04.13 val PER: 0.2439
2026-01-04 01:24:54,615: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_9000
2026-01-04 01:25:12,091: Train batch 9200: loss: 11.04 grad norm: 52.09 time: 0.055
2026-01-04 01:25:29,950: Train batch 9400: loss: 7.70 grad norm: 45.10 time: 0.068
2026-01-04 01:25:38,568: Running test after training batch: 9500
2026-01-04 01:25:38,665: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:25:43,396: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-04 01:25:43,426: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-04 01:25:45,139: Val batch 9500: PER (avg): 0.1759 CTC Loss (avg): 17.3904 WER(1gram): 49.24% (n=64) time: 6.570
2026-01-04 01:25:45,139: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 01:25:45,139: t15.2023.08.13 val PER: 0.1289
2026-01-04 01:25:45,139: t15.2023.08.18 val PER: 0.1266
2026-01-04 01:25:45,139: t15.2023.08.20 val PER: 0.1390
2026-01-04 01:25:45,139: t15.2023.08.25 val PER: 0.1160
2026-01-04 01:25:45,139: t15.2023.08.27 val PER: 0.2090
2026-01-04 01:25:45,140: t15.2023.09.01 val PER: 0.0990
2026-01-04 01:25:45,140: t15.2023.09.03 val PER: 0.1793
2026-01-04 01:25:45,140: t15.2023.09.24 val PER: 0.1359
2026-01-04 01:25:45,140: t15.2023.09.29 val PER: 0.1429
2026-01-04 01:25:45,140: t15.2023.10.01 val PER: 0.1882
2026-01-04 01:25:45,140: t15.2023.10.06 val PER: 0.1012
2026-01-04 01:25:45,140: t15.2023.10.08 val PER: 0.2720
2026-01-04 01:25:45,140: t15.2023.10.13 val PER: 0.2273
2026-01-04 01:25:45,140: t15.2023.10.15 val PER: 0.1852
2026-01-04 01:25:45,140: t15.2023.10.20 val PER: 0.1879
2026-01-04 01:25:45,140: t15.2023.10.22 val PER: 0.1269
2026-01-04 01:25:45,141: t15.2023.11.03 val PER: 0.2008
2026-01-04 01:25:45,141: t15.2023.11.04 val PER: 0.0307
2026-01-04 01:25:45,141: t15.2023.11.17 val PER: 0.0544
2026-01-04 01:25:45,141: t15.2023.11.19 val PER: 0.0659
2026-01-04 01:25:45,141: t15.2023.11.26 val PER: 0.1623
2026-01-04 01:25:45,141: t15.2023.12.03 val PER: 0.1376
2026-01-04 01:25:45,141: t15.2023.12.08 val PER: 0.1411
2026-01-04 01:25:45,141: t15.2023.12.10 val PER: 0.1209
2026-01-04 01:25:45,141: t15.2023.12.17 val PER: 0.1580
2026-01-04 01:25:45,141: t15.2023.12.29 val PER: 0.1620
2026-01-04 01:25:45,141: t15.2024.02.25 val PER: 0.1461
2026-01-04 01:25:45,141: t15.2024.03.08 val PER: 0.2532
2026-01-04 01:25:45,141: t15.2024.03.15 val PER: 0.2326
2026-01-04 01:25:45,141: t15.2024.03.17 val PER: 0.1681
2026-01-04 01:25:45,141: t15.2024.05.10 val PER: 0.1857
2026-01-04 01:25:45,141: t15.2024.06.14 val PER: 0.1798
2026-01-04 01:25:45,141: t15.2024.07.19 val PER: 0.2663
2026-01-04 01:25:45,142: t15.2024.07.21 val PER: 0.1200
2026-01-04 01:25:45,142: t15.2024.07.28 val PER: 0.1603
2026-01-04 01:25:45,142: t15.2025.01.10 val PER: 0.3140
2026-01-04 01:25:45,142: t15.2025.01.12 val PER: 0.1901
2026-01-04 01:25:45,142: t15.2025.03.14 val PER: 0.3669
2026-01-04 01:25:45,142: t15.2025.03.16 val PER: 0.2160
2026-01-04 01:25:45,142: t15.2025.03.30 val PER: 0.3333
2026-01-04 01:25:45,142: t15.2025.04.13 val PER: 0.2325
2026-01-04 01:25:45,143: New best val WER(1gram) 50.25% --> 49.24%
2026-01-04 01:25:45,143: Checkpointing model
2026-01-04 01:25:45,714: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/best_checkpoint
2026-01-04 01:25:45,981: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_9500
2026-01-04 01:25:54,440: Train batch 9600: loss: 8.32 grad norm: 45.11 time: 0.074
2026-01-04 01:26:11,612: Train batch 9800: loss: 12.55 grad norm: 56.46 time: 0.063
2026-01-04 01:26:28,898: Train batch 10000: loss: 5.67 grad norm: 37.11 time: 0.060
2026-01-04 01:26:28,899: Running test after training batch: 10000
2026-01-04 01:26:29,025: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:26:33,704: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 01:26:33,736: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sette
2026-01-04 01:26:35,475: Val batch 10000: PER (avg): 0.1719 CTC Loss (avg): 17.0321 WER(1gram): 49.24% (n=64) time: 6.576
2026-01-04 01:26:35,476: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 01:26:35,476: t15.2023.08.13 val PER: 0.1299
2026-01-04 01:26:35,476: t15.2023.08.18 val PER: 0.1274
2026-01-04 01:26:35,476: t15.2023.08.20 val PER: 0.1374
2026-01-04 01:26:35,476: t15.2023.08.25 val PER: 0.1084
2026-01-04 01:26:35,476: t15.2023.08.27 val PER: 0.2090
2026-01-04 01:26:35,477: t15.2023.09.01 val PER: 0.1015
2026-01-04 01:26:35,477: t15.2023.09.03 val PER: 0.1853
2026-01-04 01:26:35,477: t15.2023.09.24 val PER: 0.1420
2026-01-04 01:26:35,477: t15.2023.09.29 val PER: 0.1404
2026-01-04 01:26:35,477: t15.2023.10.01 val PER: 0.1843
2026-01-04 01:26:35,477: t15.2023.10.06 val PER: 0.1001
2026-01-04 01:26:35,477: t15.2023.10.08 val PER: 0.2625
2026-01-04 01:26:35,477: t15.2023.10.13 val PER: 0.2281
2026-01-04 01:26:35,477: t15.2023.10.15 val PER: 0.1753
2026-01-04 01:26:35,481: t15.2023.10.20 val PER: 0.1913
2026-01-04 01:26:35,481: t15.2023.10.22 val PER: 0.1269
2026-01-04 01:26:35,481: t15.2023.11.03 val PER: 0.1906
2026-01-04 01:26:35,481: t15.2023.11.04 val PER: 0.0341
2026-01-04 01:26:35,481: t15.2023.11.17 val PER: 0.0560
2026-01-04 01:26:35,482: t15.2023.11.19 val PER: 0.0459
2026-01-04 01:26:35,482: t15.2023.11.26 val PER: 0.1471
2026-01-04 01:26:35,482: t15.2023.12.03 val PER: 0.1387
2026-01-04 01:26:35,482: t15.2023.12.08 val PER: 0.1272
2026-01-04 01:26:35,482: t15.2023.12.10 val PER: 0.1143
2026-01-04 01:26:35,482: t15.2023.12.17 val PER: 0.1528
2026-01-04 01:26:35,482: t15.2023.12.29 val PER: 0.1510
2026-01-04 01:26:35,482: t15.2024.02.25 val PER: 0.1461
2026-01-04 01:26:35,482: t15.2024.03.08 val PER: 0.2560
2026-01-04 01:26:35,482: t15.2024.03.15 val PER: 0.2226
2026-01-04 01:26:35,482: t15.2024.03.17 val PER: 0.1702
2026-01-04 01:26:35,483: t15.2024.05.10 val PER: 0.1857
2026-01-04 01:26:35,483: t15.2024.06.14 val PER: 0.1861
2026-01-04 01:26:35,483: t15.2024.07.19 val PER: 0.2676
2026-01-04 01:26:35,483: t15.2024.07.21 val PER: 0.1110
2026-01-04 01:26:35,483: t15.2024.07.28 val PER: 0.1574
2026-01-04 01:26:35,483: t15.2025.01.10 val PER: 0.3044
2026-01-04 01:26:35,483: t15.2025.01.12 val PER: 0.1801
2026-01-04 01:26:35,483: t15.2025.03.14 val PER: 0.3565
2026-01-04 01:26:35,483: t15.2025.03.16 val PER: 0.2238
2026-01-04 01:26:35,483: t15.2025.03.30 val PER: 0.3207
2026-01-04 01:26:35,483: t15.2025.04.13 val PER: 0.2340
2026-01-04 01:26:35,735: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_10000
2026-01-04 01:26:53,171: Train batch 10200: loss: 6.77 grad norm: 40.86 time: 0.049
2026-01-04 01:27:11,030: Train batch 10400: loss: 9.48 grad norm: 52.31 time: 0.071
2026-01-04 01:27:19,877: Running test after training batch: 10500
2026-01-04 01:27:20,007: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:27:24,920: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:27:24,950: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-04 01:27:26,664: Val batch 10500: PER (avg): 0.1673 CTC Loss (avg): 16.7154 WER(1gram): 49.24% (n=64) time: 6.786
2026-01-04 01:27:26,664: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 01:27:26,664: t15.2023.08.13 val PER: 0.1237
2026-01-04 01:27:26,664: t15.2023.08.18 val PER: 0.1241
2026-01-04 01:27:26,664: t15.2023.08.20 val PER: 0.1358
2026-01-04 01:27:26,665: t15.2023.08.25 val PER: 0.1099
2026-01-04 01:27:26,665: t15.2023.08.27 val PER: 0.2090
2026-01-04 01:27:26,665: t15.2023.09.01 val PER: 0.0958
2026-01-04 01:27:26,665: t15.2023.09.03 val PER: 0.1781
2026-01-04 01:27:26,665: t15.2023.09.24 val PER: 0.1444
2026-01-04 01:27:26,665: t15.2023.09.29 val PER: 0.1487
2026-01-04 01:27:26,665: t15.2023.10.01 val PER: 0.1889
2026-01-04 01:27:26,665: t15.2023.10.06 val PER: 0.0947
2026-01-04 01:27:26,665: t15.2023.10.08 val PER: 0.2544
2026-01-04 01:27:26,665: t15.2023.10.13 val PER: 0.2110
2026-01-04 01:27:26,665: t15.2023.10.15 val PER: 0.1707
2026-01-04 01:27:26,665: t15.2023.10.20 val PER: 0.1879
2026-01-04 01:27:26,665: t15.2023.10.22 val PER: 0.1203
2026-01-04 01:27:26,666: t15.2023.11.03 val PER: 0.1900
2026-01-04 01:27:26,666: t15.2023.11.04 val PER: 0.0375
2026-01-04 01:27:26,666: t15.2023.11.17 val PER: 0.0513
2026-01-04 01:27:26,666: t15.2023.11.19 val PER: 0.0459
2026-01-04 01:27:26,666: t15.2023.11.26 val PER: 0.1406
2026-01-04 01:27:26,666: t15.2023.12.03 val PER: 0.1261
2026-01-04 01:27:26,666: t15.2023.12.08 val PER: 0.1245
2026-01-04 01:27:26,666: t15.2023.12.10 val PER: 0.1038
2026-01-04 01:27:26,666: t15.2023.12.17 val PER: 0.1528
2026-01-04 01:27:26,666: t15.2023.12.29 val PER: 0.1482
2026-01-04 01:27:26,666: t15.2024.02.25 val PER: 0.1306
2026-01-04 01:27:26,666: t15.2024.03.08 val PER: 0.2518
2026-01-04 01:27:26,666: t15.2024.03.15 val PER: 0.2139
2026-01-04 01:27:26,666: t15.2024.03.17 val PER: 0.1604
2026-01-04 01:27:26,666: t15.2024.05.10 val PER: 0.1932
2026-01-04 01:27:26,666: t15.2024.06.14 val PER: 0.1782
2026-01-04 01:27:26,666: t15.2024.07.19 val PER: 0.2716
2026-01-04 01:27:26,667: t15.2024.07.21 val PER: 0.1034
2026-01-04 01:27:26,667: t15.2024.07.28 val PER: 0.1353
2026-01-04 01:27:26,667: t15.2025.01.10 val PER: 0.3085
2026-01-04 01:27:26,667: t15.2025.01.12 val PER: 0.1671
2026-01-04 01:27:26,667: t15.2025.03.14 val PER: 0.3624
2026-01-04 01:27:26,667: t15.2025.03.16 val PER: 0.2094
2026-01-04 01:27:26,668: t15.2025.03.30 val PER: 0.3253
2026-01-04 01:27:26,668: t15.2025.04.13 val PER: 0.2211
2026-01-04 01:27:26,931: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_10500
2026-01-04 01:27:35,845: Train batch 10600: loss: 9.53 grad norm: 57.42 time: 0.071
2026-01-04 01:27:53,376: Train batch 10800: loss: 14.72 grad norm: 63.36 time: 0.065
2026-01-04 01:28:10,845: Train batch 11000: loss: 14.70 grad norm: 65.51 time: 0.056
2026-01-04 01:28:10,845: Running test after training batch: 11000
2026-01-04 01:28:10,993: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:28:16,291: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:28:16,323: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 01:28:18,051: Val batch 11000: PER (avg): 0.1656 CTC Loss (avg): 16.6369 WER(1gram): 47.46% (n=64) time: 7.205
2026-01-04 01:28:18,051: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 01:28:18,051: t15.2023.08.13 val PER: 0.1195
2026-01-04 01:28:18,051: t15.2023.08.18 val PER: 0.1148
2026-01-04 01:28:18,051: t15.2023.08.20 val PER: 0.1303
2026-01-04 01:28:18,051: t15.2023.08.25 val PER: 0.1039
2026-01-04 01:28:18,052: t15.2023.08.27 val PER: 0.2090
2026-01-04 01:28:18,052: t15.2023.09.01 val PER: 0.0852
2026-01-04 01:28:18,052: t15.2023.09.03 val PER: 0.1758
2026-01-04 01:28:18,052: t15.2023.09.24 val PER: 0.1420
2026-01-04 01:28:18,052: t15.2023.09.29 val PER: 0.1417
2026-01-04 01:28:18,052: t15.2023.10.01 val PER: 0.1896
2026-01-04 01:28:18,052: t15.2023.10.06 val PER: 0.0936
2026-01-04 01:28:18,052: t15.2023.10.08 val PER: 0.2571
2026-01-04 01:28:18,052: t15.2023.10.13 val PER: 0.2211
2026-01-04 01:28:18,052: t15.2023.10.15 val PER: 0.1714
2026-01-04 01:28:18,052: t15.2023.10.20 val PER: 0.1980
2026-01-04 01:28:18,052: t15.2023.10.22 val PER: 0.1214
2026-01-04 01:28:18,052: t15.2023.11.03 val PER: 0.1893
2026-01-04 01:28:18,053: t15.2023.11.04 val PER: 0.0375
2026-01-04 01:28:18,053: t15.2023.11.17 val PER: 0.0513
2026-01-04 01:28:18,053: t15.2023.11.19 val PER: 0.0499
2026-01-04 01:28:18,053: t15.2023.11.26 val PER: 0.1406
2026-01-04 01:28:18,053: t15.2023.12.03 val PER: 0.1313
2026-01-04 01:28:18,053: t15.2023.12.08 val PER: 0.1185
2026-01-04 01:28:18,053: t15.2023.12.10 val PER: 0.1038
2026-01-04 01:28:18,053: t15.2023.12.17 val PER: 0.1507
2026-01-04 01:28:18,053: t15.2023.12.29 val PER: 0.1407
2026-01-04 01:28:18,053: t15.2024.02.25 val PER: 0.1390
2026-01-04 01:28:18,053: t15.2024.03.08 val PER: 0.2475
2026-01-04 01:28:18,053: t15.2024.03.15 val PER: 0.2189
2026-01-04 01:28:18,053: t15.2024.03.17 val PER: 0.1660
2026-01-04 01:28:18,053: t15.2024.05.10 val PER: 0.1813
2026-01-04 01:28:18,053: t15.2024.06.14 val PER: 0.1719
2026-01-04 01:28:18,053: t15.2024.07.19 val PER: 0.2558
2026-01-04 01:28:18,054: t15.2024.07.21 val PER: 0.1062
2026-01-04 01:28:18,054: t15.2024.07.28 val PER: 0.1485
2026-01-04 01:28:18,054: t15.2025.01.10 val PER: 0.3127
2026-01-04 01:28:18,054: t15.2025.01.12 val PER: 0.1617
2026-01-04 01:28:18,054: t15.2025.03.14 val PER: 0.3550
2026-01-04 01:28:18,054: t15.2025.03.16 val PER: 0.1990
2026-01-04 01:28:18,054: t15.2025.03.30 val PER: 0.3080
2026-01-04 01:28:18,054: t15.2025.04.13 val PER: 0.2354
2026-01-04 01:28:18,055: New best val WER(1gram) 49.24% --> 47.46%
2026-01-04 01:28:18,056: Checkpointing model
2026-01-04 01:28:18,631: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/best_checkpoint
2026-01-04 01:28:18,901: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_11000
2026-01-04 01:28:36,180: Train batch 11200: loss: 10.59 grad norm: 51.60 time: 0.070
2026-01-04 01:28:53,362: Train batch 11400: loss: 9.86 grad norm: 51.23 time: 0.056
2026-01-04 01:29:02,122: Running test after training batch: 11500
2026-01-04 01:29:02,279: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:29:07,118: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:29:07,151: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 01:29:08,912: Val batch 11500: PER (avg): 0.1629 CTC Loss (avg): 16.5542 WER(1gram): 50.76% (n=64) time: 6.790
2026-01-04 01:29:08,912: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 01:29:08,913: t15.2023.08.13 val PER: 0.1227
2026-01-04 01:29:08,913: t15.2023.08.18 val PER: 0.1081
2026-01-04 01:29:08,913: t15.2023.08.20 val PER: 0.1231
2026-01-04 01:29:08,913: t15.2023.08.25 val PER: 0.1145
2026-01-04 01:29:08,913: t15.2023.08.27 val PER: 0.1929
2026-01-04 01:29:08,913: t15.2023.09.01 val PER: 0.0860
2026-01-04 01:29:08,913: t15.2023.09.03 val PER: 0.1746
2026-01-04 01:29:08,913: t15.2023.09.24 val PER: 0.1299
2026-01-04 01:29:08,913: t15.2023.09.29 val PER: 0.1315
2026-01-04 01:29:08,914: t15.2023.10.01 val PER: 0.1836
2026-01-04 01:29:08,914: t15.2023.10.06 val PER: 0.1001
2026-01-04 01:29:08,914: t15.2023.10.08 val PER: 0.2625
2026-01-04 01:29:08,914: t15.2023.10.13 val PER: 0.2064
2026-01-04 01:29:08,914: t15.2023.10.15 val PER: 0.1622
2026-01-04 01:29:08,914: t15.2023.10.20 val PER: 0.1711
2026-01-04 01:29:08,914: t15.2023.10.22 val PER: 0.1203
2026-01-04 01:29:08,914: t15.2023.11.03 val PER: 0.1879
2026-01-04 01:29:08,914: t15.2023.11.04 val PER: 0.0307
2026-01-04 01:29:08,914: t15.2023.11.17 val PER: 0.0482
2026-01-04 01:29:08,914: t15.2023.11.19 val PER: 0.0459
2026-01-04 01:29:08,914: t15.2023.11.26 val PER: 0.1319
2026-01-04 01:29:08,914: t15.2023.12.03 val PER: 0.1303
2026-01-04 01:29:08,914: t15.2023.12.08 val PER: 0.1138
2026-01-04 01:29:08,915: t15.2023.12.10 val PER: 0.0999
2026-01-04 01:29:08,915: t15.2023.12.17 val PER: 0.1559
2026-01-04 01:29:08,915: t15.2023.12.29 val PER: 0.1407
2026-01-04 01:29:08,915: t15.2024.02.25 val PER: 0.1208
2026-01-04 01:29:08,915: t15.2024.03.08 val PER: 0.2347
2026-01-04 01:29:08,915: t15.2024.03.15 val PER: 0.2195
2026-01-04 01:29:08,915: t15.2024.03.17 val PER: 0.1632
2026-01-04 01:29:08,915: t15.2024.05.10 val PER: 0.1783
2026-01-04 01:29:08,915: t15.2024.06.14 val PER: 0.1814
2026-01-04 01:29:08,915: t15.2024.07.19 val PER: 0.2512
2026-01-04 01:29:08,915: t15.2024.07.21 val PER: 0.1021
2026-01-04 01:29:08,915: t15.2024.07.28 val PER: 0.1485
2026-01-04 01:29:08,915: t15.2025.01.10 val PER: 0.3375
2026-01-04 01:29:08,915: t15.2025.01.12 val PER: 0.1540
2026-01-04 01:29:08,915: t15.2025.03.14 val PER: 0.3580
2026-01-04 01:29:08,915: t15.2025.03.16 val PER: 0.2160
2026-01-04 01:29:08,916: t15.2025.03.30 val PER: 0.3161
2026-01-04 01:29:08,916: t15.2025.04.13 val PER: 0.2368
2026-01-04 01:29:09,172: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_11500
2026-01-04 01:29:17,675: Train batch 11600: loss: 11.07 grad norm: 48.86 time: 0.061
2026-01-04 01:29:34,727: Train batch 11800: loss: 6.88 grad norm: 43.32 time: 0.044
2026-01-04 01:29:51,840: Train batch 12000: loss: 13.59 grad norm: 57.51 time: 0.071
2026-01-04 01:29:51,840: Running test after training batch: 12000
2026-01-04 01:29:51,931: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:29:56,655: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:29:56,687: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-04 01:29:58,458: Val batch 12000: PER (avg): 0.1622 CTC Loss (avg): 16.2489 WER(1gram): 51.02% (n=64) time: 6.618
2026-01-04 01:29:58,458: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 01:29:58,459: t15.2023.08.13 val PER: 0.1279
2026-01-04 01:29:58,459: t15.2023.08.18 val PER: 0.1073
2026-01-04 01:29:58,459: t15.2023.08.20 val PER: 0.1263
2026-01-04 01:29:58,459: t15.2023.08.25 val PER: 0.0949
2026-01-04 01:29:58,459: t15.2023.08.27 val PER: 0.2026
2026-01-04 01:29:58,459: t15.2023.09.01 val PER: 0.0893
2026-01-04 01:29:58,459: t15.2023.09.03 val PER: 0.1651
2026-01-04 01:29:58,459: t15.2023.09.24 val PER: 0.1335
2026-01-04 01:29:58,459: t15.2023.09.29 val PER: 0.1334
2026-01-04 01:29:58,459: t15.2023.10.01 val PER: 0.1810
2026-01-04 01:29:58,459: t15.2023.10.06 val PER: 0.0893
2026-01-04 01:29:58,459: t15.2023.10.08 val PER: 0.2517
2026-01-04 01:29:58,459: t15.2023.10.13 val PER: 0.2133
2026-01-04 01:29:58,459: t15.2023.10.15 val PER: 0.1655
2026-01-04 01:29:58,459: t15.2023.10.20 val PER: 0.1980
2026-01-04 01:29:58,459: t15.2023.10.22 val PER: 0.1192
2026-01-04 01:29:58,459: t15.2023.11.03 val PER: 0.1893
2026-01-04 01:29:58,460: t15.2023.11.04 val PER: 0.0341
2026-01-04 01:29:58,460: t15.2023.11.17 val PER: 0.0482
2026-01-04 01:29:58,460: t15.2023.11.19 val PER: 0.0459
2026-01-04 01:29:58,460: t15.2023.11.26 val PER: 0.1319
2026-01-04 01:29:58,460: t15.2023.12.03 val PER: 0.1282
2026-01-04 01:29:58,460: t15.2023.12.08 val PER: 0.1132
2026-01-04 01:29:58,460: t15.2023.12.10 val PER: 0.0959
2026-01-04 01:29:58,460: t15.2023.12.17 val PER: 0.1507
2026-01-04 01:29:58,460: t15.2023.12.29 val PER: 0.1482
2026-01-04 01:29:58,460: t15.2024.02.25 val PER: 0.1264
2026-01-04 01:29:58,460: t15.2024.03.08 val PER: 0.2432
2026-01-04 01:29:58,460: t15.2024.03.15 val PER: 0.2114
2026-01-04 01:29:58,460: t15.2024.03.17 val PER: 0.1590
2026-01-04 01:29:58,460: t15.2024.05.10 val PER: 0.1857
2026-01-04 01:29:58,460: t15.2024.06.14 val PER: 0.1956
2026-01-04 01:29:58,461: t15.2024.07.19 val PER: 0.2538
2026-01-04 01:29:58,461: t15.2024.07.21 val PER: 0.1028
2026-01-04 01:29:58,461: t15.2024.07.28 val PER: 0.1456
2026-01-04 01:29:58,461: t15.2025.01.10 val PER: 0.3072
2026-01-04 01:29:58,461: t15.2025.01.12 val PER: 0.1570
2026-01-04 01:29:58,461: t15.2025.03.14 val PER: 0.3476
2026-01-04 01:29:58,461: t15.2025.03.16 val PER: 0.2173
2026-01-04 01:29:58,461: t15.2025.03.30 val PER: 0.3126
2026-01-04 01:29:58,461: t15.2025.04.13 val PER: 0.2183
2026-01-04 01:29:58,714: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_12000
2026-01-04 01:30:16,033: Train batch 12200: loss: 6.04 grad norm: 43.45 time: 0.065
2026-01-04 01:30:33,148: Train batch 12400: loss: 4.91 grad norm: 35.41 time: 0.040
2026-01-04 01:30:41,942: Running test after training batch: 12500
2026-01-04 01:30:42,039: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:30:46,806: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 01:30:46,841: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-04 01:30:48,626: Val batch 12500: PER (avg): 0.1588 CTC Loss (avg): 16.0727 WER(1gram): 48.98% (n=64) time: 6.683
2026-01-04 01:30:48,626: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 01:30:48,626: t15.2023.08.13 val PER: 0.1247
2026-01-04 01:30:48,626: t15.2023.08.18 val PER: 0.1106
2026-01-04 01:30:48,626: t15.2023.08.20 val PER: 0.1160
2026-01-04 01:30:48,626: t15.2023.08.25 val PER: 0.0904
2026-01-04 01:30:48,626: t15.2023.08.27 val PER: 0.2074
2026-01-04 01:30:48,627: t15.2023.09.01 val PER: 0.0820
2026-01-04 01:30:48,627: t15.2023.09.03 val PER: 0.1698
2026-01-04 01:30:48,627: t15.2023.09.24 val PER: 0.1286
2026-01-04 01:30:48,627: t15.2023.09.29 val PER: 0.1398
2026-01-04 01:30:48,628: t15.2023.10.01 val PER: 0.1764
2026-01-04 01:30:48,628: t15.2023.10.06 val PER: 0.0926
2026-01-04 01:30:48,628: t15.2023.10.08 val PER: 0.2666
2026-01-04 01:30:48,628: t15.2023.10.13 val PER: 0.2180
2026-01-04 01:30:48,628: t15.2023.10.15 val PER: 0.1595
2026-01-04 01:30:48,628: t15.2023.10.20 val PER: 0.2047
2026-01-04 01:30:48,628: t15.2023.10.22 val PER: 0.1136
2026-01-04 01:30:48,629: t15.2023.11.03 val PER: 0.1859
2026-01-04 01:30:48,629: t15.2023.11.04 val PER: 0.0341
2026-01-04 01:30:48,629: t15.2023.11.17 val PER: 0.0451
2026-01-04 01:30:48,629: t15.2023.11.19 val PER: 0.0419
2026-01-04 01:30:48,629: t15.2023.11.26 val PER: 0.1283
2026-01-04 01:30:48,629: t15.2023.12.03 val PER: 0.1239
2026-01-04 01:30:48,629: t15.2023.12.08 val PER: 0.1092
2026-01-04 01:30:48,629: t15.2023.12.10 val PER: 0.0999
2026-01-04 01:30:48,629: t15.2023.12.17 val PER: 0.1414
2026-01-04 01:30:48,629: t15.2023.12.29 val PER: 0.1380
2026-01-04 01:30:48,629: t15.2024.02.25 val PER: 0.1138
2026-01-04 01:30:48,629: t15.2024.03.08 val PER: 0.2376
2026-01-04 01:30:48,629: t15.2024.03.15 val PER: 0.2176
2026-01-04 01:30:48,629: t15.2024.03.17 val PER: 0.1527
2026-01-04 01:30:48,629: t15.2024.05.10 val PER: 0.1753
2026-01-04 01:30:48,630: t15.2024.06.14 val PER: 0.1719
2026-01-04 01:30:48,630: t15.2024.07.19 val PER: 0.2399
2026-01-04 01:30:48,630: t15.2024.07.21 val PER: 0.1090
2026-01-04 01:30:48,630: t15.2024.07.28 val PER: 0.1338
2026-01-04 01:30:48,630: t15.2025.01.10 val PER: 0.3044
2026-01-04 01:30:48,630: t15.2025.01.12 val PER: 0.1609
2026-01-04 01:30:48,630: t15.2025.03.14 val PER: 0.3506
2026-01-04 01:30:48,630: t15.2025.03.16 val PER: 0.1911
2026-01-04 01:30:48,630: t15.2025.03.30 val PER: 0.3034
2026-01-04 01:30:48,630: t15.2025.04.13 val PER: 0.2197
2026-01-04 01:30:48,890: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_12500
2026-01-04 01:30:57,533: Train batch 12600: loss: 8.13 grad norm: 43.36 time: 0.058
2026-01-04 01:31:15,203: Train batch 12800: loss: 5.99 grad norm: 38.89 time: 0.052
2026-01-04 01:31:32,938: Train batch 13000: loss: 6.63 grad norm: 42.35 time: 0.066
2026-01-04 01:31:32,938: Running test after training batch: 13000
2026-01-04 01:31:33,045: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:31:37,720: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:31:37,751: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost it
2026-01-04 01:31:39,529: Val batch 13000: PER (avg): 0.1564 CTC Loss (avg): 15.8305 WER(1gram): 45.43% (n=64) time: 6.590
2026-01-04 01:31:39,529: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 01:31:39,529: t15.2023.08.13 val PER: 0.1123
2026-01-04 01:31:39,529: t15.2023.08.18 val PER: 0.1031
2026-01-04 01:31:39,529: t15.2023.08.20 val PER: 0.1152
2026-01-04 01:31:39,529: t15.2023.08.25 val PER: 0.0919
2026-01-04 01:31:39,529: t15.2023.08.27 val PER: 0.1897
2026-01-04 01:31:39,529: t15.2023.09.01 val PER: 0.0828
2026-01-04 01:31:39,529: t15.2023.09.03 val PER: 0.1675
2026-01-04 01:31:39,529: t15.2023.09.24 val PER: 0.1323
2026-01-04 01:31:39,530: t15.2023.09.29 val PER: 0.1251
2026-01-04 01:31:39,530: t15.2023.10.01 val PER: 0.1691
2026-01-04 01:31:39,530: t15.2023.10.06 val PER: 0.0936
2026-01-04 01:31:39,530: t15.2023.10.08 val PER: 0.2436
2026-01-04 01:31:39,530: t15.2023.10.13 val PER: 0.2102
2026-01-04 01:31:39,530: t15.2023.10.15 val PER: 0.1602
2026-01-04 01:31:39,530: t15.2023.10.20 val PER: 0.1879
2026-01-04 01:31:39,530: t15.2023.10.22 val PER: 0.1125
2026-01-04 01:31:39,530: t15.2023.11.03 val PER: 0.1852
2026-01-04 01:31:39,530: t15.2023.11.04 val PER: 0.0307
2026-01-04 01:31:39,530: t15.2023.11.17 val PER: 0.0482
2026-01-04 01:31:39,531: t15.2023.11.19 val PER: 0.0459
2026-01-04 01:31:39,531: t15.2023.11.26 val PER: 0.1290
2026-01-04 01:31:39,531: t15.2023.12.03 val PER: 0.1197
2026-01-04 01:31:39,531: t15.2023.12.08 val PER: 0.1072
2026-01-04 01:31:39,531: t15.2023.12.10 val PER: 0.0933
2026-01-04 01:31:39,531: t15.2023.12.17 val PER: 0.1424
2026-01-04 01:31:39,531: t15.2023.12.29 val PER: 0.1386
2026-01-04 01:31:39,531: t15.2024.02.25 val PER: 0.1180
2026-01-04 01:31:39,531: t15.2024.03.08 val PER: 0.2418
2026-01-04 01:31:39,531: t15.2024.03.15 val PER: 0.2120
2026-01-04 01:31:39,531: t15.2024.03.17 val PER: 0.1492
2026-01-04 01:31:39,531: t15.2024.05.10 val PER: 0.1634
2026-01-04 01:31:39,531: t15.2024.06.14 val PER: 0.1703
2026-01-04 01:31:39,532: t15.2024.07.19 val PER: 0.2426
2026-01-04 01:31:39,532: t15.2024.07.21 val PER: 0.0972
2026-01-04 01:31:39,532: t15.2024.07.28 val PER: 0.1404
2026-01-04 01:31:39,532: t15.2025.01.10 val PER: 0.3113
2026-01-04 01:31:39,532: t15.2025.01.12 val PER: 0.1655
2026-01-04 01:31:39,532: t15.2025.03.14 val PER: 0.3506
2026-01-04 01:31:39,532: t15.2025.03.16 val PER: 0.1872
2026-01-04 01:31:39,532: t15.2025.03.30 val PER: 0.3115
2026-01-04 01:31:39,532: t15.2025.04.13 val PER: 0.2297
2026-01-04 01:31:39,533: New best val WER(1gram) 47.46% --> 45.43%
2026-01-04 01:31:39,533: Checkpointing model
2026-01-04 01:31:40,137: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/best_checkpoint
2026-01-04 01:31:40,406: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_13000
2026-01-04 01:31:57,854: Train batch 13200: loss: 12.40 grad norm: 60.61 time: 0.054
2026-01-04 01:32:15,104: Train batch 13400: loss: 8.90 grad norm: 51.93 time: 0.062
2026-01-04 01:32:23,745: Running test after training batch: 13500
2026-01-04 01:32:23,876: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:32:28,580: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-04 01:32:28,612: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-04 01:32:30,402: Val batch 13500: PER (avg): 0.1554 CTC Loss (avg): 15.6744 WER(1gram): 47.97% (n=64) time: 6.656
2026-01-04 01:32:30,402: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 01:32:30,402: t15.2023.08.13 val PER: 0.1247
2026-01-04 01:32:30,403: t15.2023.08.18 val PER: 0.1065
2026-01-04 01:32:30,403: t15.2023.08.20 val PER: 0.1183
2026-01-04 01:32:30,403: t15.2023.08.25 val PER: 0.0813
2026-01-04 01:32:30,403: t15.2023.08.27 val PER: 0.2090
2026-01-04 01:32:30,403: t15.2023.09.01 val PER: 0.0869
2026-01-04 01:32:30,403: t15.2023.09.03 val PER: 0.1675
2026-01-04 01:32:30,403: t15.2023.09.24 val PER: 0.1226
2026-01-04 01:32:30,403: t15.2023.09.29 val PER: 0.1334
2026-01-04 01:32:30,403: t15.2023.10.01 val PER: 0.1717
2026-01-04 01:32:30,404: t15.2023.10.06 val PER: 0.0861
2026-01-04 01:32:30,404: t15.2023.10.08 val PER: 0.2422
2026-01-04 01:32:30,404: t15.2023.10.13 val PER: 0.2110
2026-01-04 01:32:30,404: t15.2023.10.15 val PER: 0.1556
2026-01-04 01:32:30,404: t15.2023.10.20 val PER: 0.1812
2026-01-04 01:32:30,404: t15.2023.10.22 val PER: 0.1269
2026-01-04 01:32:30,404: t15.2023.11.03 val PER: 0.1859
2026-01-04 01:32:30,404: t15.2023.11.04 val PER: 0.0410
2026-01-04 01:32:30,404: t15.2023.11.17 val PER: 0.0513
2026-01-04 01:32:30,405: t15.2023.11.19 val PER: 0.0379
2026-01-04 01:32:30,405: t15.2023.11.26 val PER: 0.1210
2026-01-04 01:32:30,405: t15.2023.12.03 val PER: 0.1145
2026-01-04 01:32:30,405: t15.2023.12.08 val PER: 0.1119
2026-01-04 01:32:30,405: t15.2023.12.10 val PER: 0.0972
2026-01-04 01:32:30,405: t15.2023.12.17 val PER: 0.1341
2026-01-04 01:32:30,405: t15.2023.12.29 val PER: 0.1318
2026-01-04 01:32:30,405: t15.2024.02.25 val PER: 0.1194
2026-01-04 01:32:30,405: t15.2024.03.08 val PER: 0.2304
2026-01-04 01:32:30,406: t15.2024.03.15 val PER: 0.2070
2026-01-04 01:32:30,406: t15.2024.03.17 val PER: 0.1520
2026-01-04 01:32:30,406: t15.2024.05.10 val PER: 0.1620
2026-01-04 01:32:30,406: t15.2024.06.14 val PER: 0.1688
2026-01-04 01:32:30,406: t15.2024.07.19 val PER: 0.2432
2026-01-04 01:32:30,406: t15.2024.07.21 val PER: 0.1000
2026-01-04 01:32:30,406: t15.2024.07.28 val PER: 0.1368
2026-01-04 01:32:30,406: t15.2025.01.10 val PER: 0.3017
2026-01-04 01:32:30,406: t15.2025.01.12 val PER: 0.1509
2026-01-04 01:32:30,406: t15.2025.03.14 val PER: 0.3476
2026-01-04 01:32:30,407: t15.2025.03.16 val PER: 0.1976
2026-01-04 01:32:30,407: t15.2025.03.30 val PER: 0.3011
2026-01-04 01:32:30,407: t15.2025.04.13 val PER: 0.2126
2026-01-04 01:32:30,664: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_13500
2026-01-04 01:32:39,484: Train batch 13600: loss: 13.13 grad norm: 65.51 time: 0.062
2026-01-04 01:32:57,099: Train batch 13800: loss: 9.46 grad norm: 57.85 time: 0.056
2026-01-04 01:33:14,401: Train batch 14000: loss: 11.94 grad norm: 57.62 time: 0.050
2026-01-04 01:33:14,401: Running test after training batch: 14000
2026-01-04 01:33:14,557: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:33:19,222: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:33:19,256: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 01:33:21,072: Val batch 14000: PER (avg): 0.1543 CTC Loss (avg): 15.6304 WER(1gram): 48.48% (n=64) time: 6.670
2026-01-04 01:33:21,072: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 01:33:21,072: t15.2023.08.13 val PER: 0.1175
2026-01-04 01:33:21,072: t15.2023.08.18 val PER: 0.1065
2026-01-04 01:33:21,072: t15.2023.08.20 val PER: 0.1160
2026-01-04 01:33:21,073: t15.2023.08.25 val PER: 0.1024
2026-01-04 01:33:21,073: t15.2023.08.27 val PER: 0.1913
2026-01-04 01:33:21,073: t15.2023.09.01 val PER: 0.0836
2026-01-04 01:33:21,073: t15.2023.09.03 val PER: 0.1781
2026-01-04 01:33:21,073: t15.2023.09.24 val PER: 0.1262
2026-01-04 01:33:21,073: t15.2023.09.29 val PER: 0.1347
2026-01-04 01:33:21,073: t15.2023.10.01 val PER: 0.1744
2026-01-04 01:33:21,073: t15.2023.10.06 val PER: 0.0893
2026-01-04 01:33:21,073: t15.2023.10.08 val PER: 0.2517
2026-01-04 01:33:21,073: t15.2023.10.13 val PER: 0.2157
2026-01-04 01:33:21,073: t15.2023.10.15 val PER: 0.1516
2026-01-04 01:33:21,073: t15.2023.10.20 val PER: 0.1812
2026-01-04 01:33:21,073: t15.2023.10.22 val PER: 0.1136
2026-01-04 01:33:21,074: t15.2023.11.03 val PER: 0.1777
2026-01-04 01:33:21,074: t15.2023.11.04 val PER: 0.0273
2026-01-04 01:33:21,074: t15.2023.11.17 val PER: 0.0420
2026-01-04 01:33:21,074: t15.2023.11.19 val PER: 0.0399
2026-01-04 01:33:21,074: t15.2023.11.26 val PER: 0.1261
2026-01-04 01:33:21,074: t15.2023.12.03 val PER: 0.1208
2026-01-04 01:33:21,074: t15.2023.12.08 val PER: 0.1099
2026-01-04 01:33:21,074: t15.2023.12.10 val PER: 0.0933
2026-01-04 01:33:21,074: t15.2023.12.17 val PER: 0.1393
2026-01-04 01:33:21,074: t15.2023.12.29 val PER: 0.1311
2026-01-04 01:33:21,074: t15.2024.02.25 val PER: 0.1180
2026-01-04 01:33:21,074: t15.2024.03.08 val PER: 0.2319
2026-01-04 01:33:21,074: t15.2024.03.15 val PER: 0.2026
2026-01-04 01:33:21,075: t15.2024.03.17 val PER: 0.1520
2026-01-04 01:33:21,075: t15.2024.05.10 val PER: 0.1664
2026-01-04 01:33:21,075: t15.2024.06.14 val PER: 0.1593
2026-01-04 01:33:21,075: t15.2024.07.19 val PER: 0.2373
2026-01-04 01:33:21,075: t15.2024.07.21 val PER: 0.0945
2026-01-04 01:33:21,075: t15.2024.07.28 val PER: 0.1309
2026-01-04 01:33:21,075: t15.2025.01.10 val PER: 0.2989
2026-01-04 01:33:21,075: t15.2025.01.12 val PER: 0.1563
2026-01-04 01:33:21,075: t15.2025.03.14 val PER: 0.3462
2026-01-04 01:33:21,075: t15.2025.03.16 val PER: 0.1859
2026-01-04 01:33:21,075: t15.2025.03.30 val PER: 0.2943
2026-01-04 01:33:21,075: t15.2025.04.13 val PER: 0.2183
2026-01-04 01:33:21,332: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_14000
2026-01-04 01:33:38,663: Train batch 14200: loss: 8.23 grad norm: 48.31 time: 0.056
2026-01-04 01:33:56,127: Train batch 14400: loss: 5.70 grad norm: 38.90 time: 0.063
2026-01-04 01:34:04,896: Running test after training batch: 14500
2026-01-04 01:34:04,998: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:34:09,999: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:34:10,033: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-04 01:34:11,847: Val batch 14500: PER (avg): 0.1530 CTC Loss (avg): 15.5999 WER(1gram): 47.97% (n=64) time: 6.951
2026-01-04 01:34:11,848: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 01:34:11,848: t15.2023.08.13 val PER: 0.1091
2026-01-04 01:34:11,848: t15.2023.08.18 val PER: 0.0997
2026-01-04 01:34:11,848: t15.2023.08.20 val PER: 0.1080
2026-01-04 01:34:11,848: t15.2023.08.25 val PER: 0.0949
2026-01-04 01:34:11,848: t15.2023.08.27 val PER: 0.1913
2026-01-04 01:34:11,848: t15.2023.09.01 val PER: 0.0820
2026-01-04 01:34:11,848: t15.2023.09.03 val PER: 0.1758
2026-01-04 01:34:11,848: t15.2023.09.24 val PER: 0.1311
2026-01-04 01:34:11,848: t15.2023.09.29 val PER: 0.1283
2026-01-04 01:34:11,849: t15.2023.10.01 val PER: 0.1823
2026-01-04 01:34:11,849: t15.2023.10.06 val PER: 0.0883
2026-01-04 01:34:11,849: t15.2023.10.08 val PER: 0.2449
2026-01-04 01:34:11,849: t15.2023.10.13 val PER: 0.2079
2026-01-04 01:34:11,849: t15.2023.10.15 val PER: 0.1582
2026-01-04 01:34:11,849: t15.2023.10.20 val PER: 0.1779
2026-01-04 01:34:11,849: t15.2023.10.22 val PER: 0.1091
2026-01-04 01:34:11,849: t15.2023.11.03 val PER: 0.1784
2026-01-04 01:34:11,849: t15.2023.11.04 val PER: 0.0307
2026-01-04 01:34:11,849: t15.2023.11.17 val PER: 0.0467
2026-01-04 01:34:11,849: t15.2023.11.19 val PER: 0.0339
2026-01-04 01:34:11,849: t15.2023.11.26 val PER: 0.1283
2026-01-04 01:34:11,849: t15.2023.12.03 val PER: 0.1134
2026-01-04 01:34:11,849: t15.2023.12.08 val PER: 0.1032
2026-01-04 01:34:11,850: t15.2023.12.10 val PER: 0.0894
2026-01-04 01:34:11,850: t15.2023.12.17 val PER: 0.1414
2026-01-04 01:34:11,850: t15.2023.12.29 val PER: 0.1242
2026-01-04 01:34:11,850: t15.2024.02.25 val PER: 0.1236
2026-01-04 01:34:11,850: t15.2024.03.08 val PER: 0.2248
2026-01-04 01:34:11,850: t15.2024.03.15 val PER: 0.2083
2026-01-04 01:34:11,850: t15.2024.03.17 val PER: 0.1513
2026-01-04 01:34:11,850: t15.2024.05.10 val PER: 0.1560
2026-01-04 01:34:11,850: t15.2024.06.14 val PER: 0.1672
2026-01-04 01:34:11,850: t15.2024.07.19 val PER: 0.2386
2026-01-04 01:34:11,850: t15.2024.07.21 val PER: 0.0897
2026-01-04 01:34:11,850: t15.2024.07.28 val PER: 0.1375
2026-01-04 01:34:11,850: t15.2025.01.10 val PER: 0.2920
2026-01-04 01:34:11,850: t15.2025.01.12 val PER: 0.1517
2026-01-04 01:34:11,850: t15.2025.03.14 val PER: 0.3580
2026-01-04 01:34:11,850: t15.2025.03.16 val PER: 0.1911
2026-01-04 01:34:11,850: t15.2025.03.30 val PER: 0.2966
2026-01-04 01:34:11,851: t15.2025.04.13 val PER: 0.2168
2026-01-04 01:34:12,106: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_14500
2026-01-04 01:34:20,764: Train batch 14600: loss: 11.80 grad norm: 56.55 time: 0.057
2026-01-04 01:34:38,089: Train batch 14800: loss: 5.91 grad norm: 43.35 time: 0.049
2026-01-04 01:34:55,404: Train batch 15000: loss: 9.53 grad norm: 53.22 time: 0.051
2026-01-04 01:34:55,404: Running test after training batch: 15000
2026-01-04 01:34:55,528: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:35:00,231: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:35:00,264: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 01:35:02,108: Val batch 15000: PER (avg): 0.1511 CTC Loss (avg): 15.3528 WER(1gram): 46.70% (n=64) time: 6.703
2026-01-04 01:35:02,108: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-04 01:35:02,108: t15.2023.08.13 val PER: 0.1123
2026-01-04 01:35:02,108: t15.2023.08.18 val PER: 0.0981
2026-01-04 01:35:02,108: t15.2023.08.20 val PER: 0.1160
2026-01-04 01:35:02,108: t15.2023.08.25 val PER: 0.0949
2026-01-04 01:35:02,108: t15.2023.08.27 val PER: 0.1865
2026-01-04 01:35:02,109: t15.2023.09.01 val PER: 0.0779
2026-01-04 01:35:02,109: t15.2023.09.03 val PER: 0.1508
2026-01-04 01:35:02,109: t15.2023.09.24 val PER: 0.1311
2026-01-04 01:35:02,109: t15.2023.09.29 val PER: 0.1289
2026-01-04 01:35:02,109: t15.2023.10.01 val PER: 0.1737
2026-01-04 01:35:02,109: t15.2023.10.06 val PER: 0.0829
2026-01-04 01:35:02,109: t15.2023.10.08 val PER: 0.2558
2026-01-04 01:35:02,109: t15.2023.10.13 val PER: 0.2064
2026-01-04 01:35:02,109: t15.2023.10.15 val PER: 0.1503
2026-01-04 01:35:02,109: t15.2023.10.20 val PER: 0.1779
2026-01-04 01:35:02,110: t15.2023.10.22 val PER: 0.1102
2026-01-04 01:35:02,110: t15.2023.11.03 val PER: 0.1811
2026-01-04 01:35:02,110: t15.2023.11.04 val PER: 0.0341
2026-01-04 01:35:02,110: t15.2023.11.17 val PER: 0.0420
2026-01-04 01:35:02,110: t15.2023.11.19 val PER: 0.0399
2026-01-04 01:35:02,110: t15.2023.11.26 val PER: 0.1232
2026-01-04 01:35:02,110: t15.2023.12.03 val PER: 0.1208
2026-01-04 01:35:02,110: t15.2023.12.08 val PER: 0.1072
2026-01-04 01:35:02,110: t15.2023.12.10 val PER: 0.0920
2026-01-04 01:35:02,110: t15.2023.12.17 val PER: 0.1455
2026-01-04 01:35:02,110: t15.2023.12.29 val PER: 0.1297
2026-01-04 01:35:02,110: t15.2024.02.25 val PER: 0.1124
2026-01-04 01:35:02,110: t15.2024.03.08 val PER: 0.2276
2026-01-04 01:35:02,110: t15.2024.03.15 val PER: 0.2045
2026-01-04 01:35:02,110: t15.2024.03.17 val PER: 0.1388
2026-01-04 01:35:02,110: t15.2024.05.10 val PER: 0.1634
2026-01-04 01:35:02,111: t15.2024.06.14 val PER: 0.1609
2026-01-04 01:35:02,111: t15.2024.07.19 val PER: 0.2419
2026-01-04 01:35:02,111: t15.2024.07.21 val PER: 0.0910
2026-01-04 01:35:02,111: t15.2024.07.28 val PER: 0.1235
2026-01-04 01:35:02,111: t15.2025.01.10 val PER: 0.2906
2026-01-04 01:35:02,111: t15.2025.01.12 val PER: 0.1455
2026-01-04 01:35:02,111: t15.2025.03.14 val PER: 0.3550
2026-01-04 01:35:02,111: t15.2025.03.16 val PER: 0.1793
2026-01-04 01:35:02,111: t15.2025.03.30 val PER: 0.2931
2026-01-04 01:35:02,111: t15.2025.04.13 val PER: 0.2168
2026-01-04 01:35:02,366: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_15000
2026-01-04 01:35:19,553: Train batch 15200: loss: 5.06 grad norm: 39.23 time: 0.057
2026-01-04 01:35:36,498: Train batch 15400: loss: 11.67 grad norm: 60.87 time: 0.049
2026-01-04 01:35:45,108: Running test after training batch: 15500
2026-01-04 01:35:45,273: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:35:49,986: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:35:50,021: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 01:35:51,877: Val batch 15500: PER (avg): 0.1502 CTC Loss (avg): 15.2856 WER(1gram): 46.95% (n=64) time: 6.769
2026-01-04 01:35:51,878: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-04 01:35:51,878: t15.2023.08.13 val PER: 0.1133
2026-01-04 01:35:51,878: t15.2023.08.18 val PER: 0.0964
2026-01-04 01:35:51,878: t15.2023.08.20 val PER: 0.1191
2026-01-04 01:35:51,878: t15.2023.08.25 val PER: 0.0964
2026-01-04 01:35:51,878: t15.2023.08.27 val PER: 0.1913
2026-01-04 01:35:51,878: t15.2023.09.01 val PER: 0.0795
2026-01-04 01:35:51,878: t15.2023.09.03 val PER: 0.1615
2026-01-04 01:35:51,878: t15.2023.09.24 val PER: 0.1262
2026-01-04 01:35:51,878: t15.2023.09.29 val PER: 0.1283
2026-01-04 01:35:51,878: t15.2023.10.01 val PER: 0.1691
2026-01-04 01:35:51,879: t15.2023.10.06 val PER: 0.0818
2026-01-04 01:35:51,879: t15.2023.10.08 val PER: 0.2368
2026-01-04 01:35:51,879: t15.2023.10.13 val PER: 0.2002
2026-01-04 01:35:51,879: t15.2023.10.15 val PER: 0.1523
2026-01-04 01:35:51,879: t15.2023.10.20 val PER: 0.1812
2026-01-04 01:35:51,879: t15.2023.10.22 val PER: 0.1114
2026-01-04 01:35:51,879: t15.2023.11.03 val PER: 0.1703
2026-01-04 01:35:51,879: t15.2023.11.04 val PER: 0.0307
2026-01-04 01:35:51,879: t15.2023.11.17 val PER: 0.0420
2026-01-04 01:35:51,879: t15.2023.11.19 val PER: 0.0359
2026-01-04 01:35:51,879: t15.2023.11.26 val PER: 0.1145
2026-01-04 01:35:51,879: t15.2023.12.03 val PER: 0.1103
2026-01-04 01:35:51,879: t15.2023.12.08 val PER: 0.1052
2026-01-04 01:35:51,880: t15.2023.12.10 val PER: 0.0880
2026-01-04 01:35:51,880: t15.2023.12.17 val PER: 0.1393
2026-01-04 01:35:51,880: t15.2023.12.29 val PER: 0.1249
2026-01-04 01:35:51,880: t15.2024.02.25 val PER: 0.1081
2026-01-04 01:35:51,880: t15.2024.03.08 val PER: 0.2333
2026-01-04 01:35:51,880: t15.2024.03.15 val PER: 0.2033
2026-01-04 01:35:51,880: t15.2024.03.17 val PER: 0.1457
2026-01-04 01:35:51,880: t15.2024.05.10 val PER: 0.1590
2026-01-04 01:35:51,880: t15.2024.06.14 val PER: 0.1656
2026-01-04 01:35:51,880: t15.2024.07.19 val PER: 0.2380
2026-01-04 01:35:51,880: t15.2024.07.21 val PER: 0.0931
2026-01-04 01:35:51,880: t15.2024.07.28 val PER: 0.1338
2026-01-04 01:35:51,880: t15.2025.01.10 val PER: 0.2934
2026-01-04 01:35:51,880: t15.2025.01.12 val PER: 0.1524
2026-01-04 01:35:51,880: t15.2025.03.14 val PER: 0.3506
2026-01-04 01:35:51,881: t15.2025.03.16 val PER: 0.1846
2026-01-04 01:35:51,881: t15.2025.03.30 val PER: 0.2966
2026-01-04 01:35:51,881: t15.2025.04.13 val PER: 0.2140
2026-01-04 01:35:52,139: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_15500
2026-01-04 01:36:00,612: Train batch 15600: loss: 11.86 grad norm: 60.71 time: 0.062
2026-01-04 01:36:17,633: Train batch 15800: loss: 13.79 grad norm: 62.52 time: 0.066
2026-01-04 01:36:34,817: Train batch 16000: loss: 8.29 grad norm: 43.14 time: 0.055
2026-01-04 01:36:34,817: Running test after training batch: 16000
2026-01-04 01:36:34,919: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:36:40,074: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:36:40,108: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 01:36:41,970: Val batch 16000: PER (avg): 0.1494 CTC Loss (avg): 15.3363 WER(1gram): 47.21% (n=64) time: 7.153
2026-01-04 01:36:41,971: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-04 01:36:41,971: t15.2023.08.13 val PER: 0.1081
2026-01-04 01:36:41,971: t15.2023.08.18 val PER: 0.0989
2026-01-04 01:36:41,971: t15.2023.08.20 val PER: 0.1160
2026-01-04 01:36:41,971: t15.2023.08.25 val PER: 0.0964
2026-01-04 01:36:41,971: t15.2023.08.27 val PER: 0.1961
2026-01-04 01:36:41,971: t15.2023.09.01 val PER: 0.0771
2026-01-04 01:36:41,971: t15.2023.09.03 val PER: 0.1580
2026-01-04 01:36:41,971: t15.2023.09.24 val PER: 0.1299
2026-01-04 01:36:41,971: t15.2023.09.29 val PER: 0.1289
2026-01-04 01:36:41,971: t15.2023.10.01 val PER: 0.1704
2026-01-04 01:36:41,971: t15.2023.10.06 val PER: 0.0861
2026-01-04 01:36:41,972: t15.2023.10.08 val PER: 0.2422
2026-01-04 01:36:41,972: t15.2023.10.13 val PER: 0.1978
2026-01-04 01:36:41,972: t15.2023.10.15 val PER: 0.1536
2026-01-04 01:36:41,972: t15.2023.10.20 val PER: 0.1812
2026-01-04 01:36:41,972: t15.2023.10.22 val PER: 0.1069
2026-01-04 01:36:41,972: t15.2023.11.03 val PER: 0.1805
2026-01-04 01:36:41,972: t15.2023.11.04 val PER: 0.0341
2026-01-04 01:36:41,972: t15.2023.11.17 val PER: 0.0420
2026-01-04 01:36:41,972: t15.2023.11.19 val PER: 0.0479
2026-01-04 01:36:41,972: t15.2023.11.26 val PER: 0.1181
2026-01-04 01:36:41,972: t15.2023.12.03 val PER: 0.1103
2026-01-04 01:36:41,972: t15.2023.12.08 val PER: 0.1005
2026-01-04 01:36:41,972: t15.2023.12.10 val PER: 0.0933
2026-01-04 01:36:41,972: t15.2023.12.17 val PER: 0.1299
2026-01-04 01:36:41,972: t15.2023.12.29 val PER: 0.1242
2026-01-04 01:36:41,973: t15.2024.02.25 val PER: 0.1053
2026-01-04 01:36:41,973: t15.2024.03.08 val PER: 0.2304
2026-01-04 01:36:41,973: t15.2024.03.15 val PER: 0.1932
2026-01-04 01:36:41,973: t15.2024.03.17 val PER: 0.1381
2026-01-04 01:36:41,973: t15.2024.05.10 val PER: 0.1620
2026-01-04 01:36:41,973: t15.2024.06.14 val PER: 0.1546
2026-01-04 01:36:41,973: t15.2024.07.19 val PER: 0.2327
2026-01-04 01:36:41,973: t15.2024.07.21 val PER: 0.0897
2026-01-04 01:36:41,973: t15.2024.07.28 val PER: 0.1324
2026-01-04 01:36:41,973: t15.2025.01.10 val PER: 0.2975
2026-01-04 01:36:41,973: t15.2025.01.12 val PER: 0.1517
2026-01-04 01:36:41,973: t15.2025.03.14 val PER: 0.3432
2026-01-04 01:36:41,973: t15.2025.03.16 val PER: 0.1990
2026-01-04 01:36:41,973: t15.2025.03.30 val PER: 0.2966
2026-01-04 01:36:41,973: t15.2025.04.13 val PER: 0.2097
2026-01-04 01:36:42,226: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_16000
2026-01-04 01:36:59,443: Train batch 16200: loss: 6.27 grad norm: 44.35 time: 0.055
2026-01-04 01:37:16,762: Train batch 16400: loss: 10.14 grad norm: 59.38 time: 0.057
2026-01-04 01:37:25,446: Running test after training batch: 16500
2026-01-04 01:37:25,538: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:37:30,298: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:37:30,331: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 01:37:32,207: Val batch 16500: PER (avg): 0.1488 CTC Loss (avg): 15.2538 WER(1gram): 45.18% (n=64) time: 6.761
2026-01-04 01:37:32,208: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 01:37:32,208: t15.2023.08.13 val PER: 0.1102
2026-01-04 01:37:32,208: t15.2023.08.18 val PER: 0.0947
2026-01-04 01:37:32,208: t15.2023.08.20 val PER: 0.1017
2026-01-04 01:37:32,208: t15.2023.08.25 val PER: 0.0934
2026-01-04 01:37:32,208: t15.2023.08.27 val PER: 0.1929
2026-01-04 01:37:32,208: t15.2023.09.01 val PER: 0.0787
2026-01-04 01:37:32,208: t15.2023.09.03 val PER: 0.1627
2026-01-04 01:37:32,208: t15.2023.09.24 val PER: 0.1299
2026-01-04 01:37:32,208: t15.2023.09.29 val PER: 0.1289
2026-01-04 01:37:32,208: t15.2023.10.01 val PER: 0.1678
2026-01-04 01:37:32,208: t15.2023.10.06 val PER: 0.0850
2026-01-04 01:37:32,209: t15.2023.10.08 val PER: 0.2463
2026-01-04 01:37:32,209: t15.2023.10.13 val PER: 0.1994
2026-01-04 01:37:32,209: t15.2023.10.15 val PER: 0.1470
2026-01-04 01:37:32,209: t15.2023.10.20 val PER: 0.1745
2026-01-04 01:37:32,209: t15.2023.10.22 val PER: 0.1125
2026-01-04 01:37:32,209: t15.2023.11.03 val PER: 0.1750
2026-01-04 01:37:32,209: t15.2023.11.04 val PER: 0.0307
2026-01-04 01:37:32,209: t15.2023.11.17 val PER: 0.0404
2026-01-04 01:37:32,209: t15.2023.11.19 val PER: 0.0399
2026-01-04 01:37:32,209: t15.2023.11.26 val PER: 0.1217
2026-01-04 01:37:32,209: t15.2023.12.03 val PER: 0.1187
2026-01-04 01:37:32,209: t15.2023.12.08 val PER: 0.0972
2026-01-04 01:37:32,209: t15.2023.12.10 val PER: 0.0907
2026-01-04 01:37:32,209: t15.2023.12.17 val PER: 0.1320
2026-01-04 01:37:32,209: t15.2023.12.29 val PER: 0.1263
2026-01-04 01:37:32,210: t15.2024.02.25 val PER: 0.1124
2026-01-04 01:37:32,210: t15.2024.03.08 val PER: 0.2418
2026-01-04 01:37:32,210: t15.2024.03.15 val PER: 0.1920
2026-01-04 01:37:32,210: t15.2024.03.17 val PER: 0.1395
2026-01-04 01:37:32,210: t15.2024.05.10 val PER: 0.1560
2026-01-04 01:37:32,210: t15.2024.06.14 val PER: 0.1625
2026-01-04 01:37:32,210: t15.2024.07.19 val PER: 0.2347
2026-01-04 01:37:32,210: t15.2024.07.21 val PER: 0.0945
2026-01-04 01:37:32,210: t15.2024.07.28 val PER: 0.1250
2026-01-04 01:37:32,210: t15.2025.01.10 val PER: 0.2961
2026-01-04 01:37:32,210: t15.2025.01.12 val PER: 0.1363
2026-01-04 01:37:32,210: t15.2025.03.14 val PER: 0.3521
2026-01-04 01:37:32,210: t15.2025.03.16 val PER: 0.2042
2026-01-04 01:37:32,210: t15.2025.03.30 val PER: 0.2943
2026-01-04 01:37:32,210: t15.2025.04.13 val PER: 0.2140
2026-01-04 01:37:32,212: New best val WER(1gram) 45.43% --> 45.18%
2026-01-04 01:37:32,212: Checkpointing model
2026-01-04 01:37:32,815: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/best_checkpoint
2026-01-04 01:37:33,080: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_16500
2026-01-04 01:37:41,757: Train batch 16600: loss: 8.70 grad norm: 53.69 time: 0.052
2026-01-04 01:37:59,220: Train batch 16800: loss: 16.54 grad norm: 73.61 time: 0.061
2026-01-04 01:38:16,655: Train batch 17000: loss: 8.14 grad norm: 50.73 time: 0.081
2026-01-04 01:38:16,655: Running test after training batch: 17000
2026-01-04 01:38:16,756: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:38:21,475: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:38:21,508: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 01:38:23,398: Val batch 17000: PER (avg): 0.1471 CTC Loss (avg): 15.1125 WER(1gram): 46.45% (n=64) time: 6.743
2026-01-04 01:38:23,399: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 01:38:23,399: t15.2023.08.13 val PER: 0.1040
2026-01-04 01:38:23,399: t15.2023.08.18 val PER: 0.1014
2026-01-04 01:38:23,399: t15.2023.08.20 val PER: 0.1088
2026-01-04 01:38:23,399: t15.2023.08.25 val PER: 0.0934
2026-01-04 01:38:23,399: t15.2023.08.27 val PER: 0.1945
2026-01-04 01:38:23,399: t15.2023.09.01 val PER: 0.0739
2026-01-04 01:38:23,399: t15.2023.09.03 val PER: 0.1615
2026-01-04 01:38:23,399: t15.2023.09.24 val PER: 0.1299
2026-01-04 01:38:23,399: t15.2023.09.29 val PER: 0.1264
2026-01-04 01:38:23,399: t15.2023.10.01 val PER: 0.1631
2026-01-04 01:38:23,400: t15.2023.10.06 val PER: 0.0775
2026-01-04 01:38:23,400: t15.2023.10.08 val PER: 0.2409
2026-01-04 01:38:23,400: t15.2023.10.13 val PER: 0.1955
2026-01-04 01:38:23,400: t15.2023.10.15 val PER: 0.1463
2026-01-04 01:38:23,400: t15.2023.10.20 val PER: 0.1745
2026-01-04 01:38:23,400: t15.2023.10.22 val PER: 0.1091
2026-01-04 01:38:23,400: t15.2023.11.03 val PER: 0.1771
2026-01-04 01:38:23,400: t15.2023.11.04 val PER: 0.0375
2026-01-04 01:38:23,400: t15.2023.11.17 val PER: 0.0389
2026-01-04 01:38:23,400: t15.2023.11.19 val PER: 0.0399
2026-01-04 01:38:23,400: t15.2023.11.26 val PER: 0.1109
2026-01-04 01:38:23,400: t15.2023.12.03 val PER: 0.1145
2026-01-04 01:38:23,400: t15.2023.12.08 val PER: 0.0992
2026-01-04 01:38:23,400: t15.2023.12.10 val PER: 0.0894
2026-01-04 01:38:23,401: t15.2023.12.17 val PER: 0.1289
2026-01-04 01:38:23,401: t15.2023.12.29 val PER: 0.1235
2026-01-04 01:38:23,401: t15.2024.02.25 val PER: 0.1124
2026-01-04 01:38:23,401: t15.2024.03.08 val PER: 0.2290
2026-01-04 01:38:23,401: t15.2024.03.15 val PER: 0.1982
2026-01-04 01:38:23,401: t15.2024.03.17 val PER: 0.1402
2026-01-04 01:38:23,401: t15.2024.05.10 val PER: 0.1560
2026-01-04 01:38:23,401: t15.2024.06.14 val PER: 0.1530
2026-01-04 01:38:23,401: t15.2024.07.19 val PER: 0.2281
2026-01-04 01:38:23,401: t15.2024.07.21 val PER: 0.0917
2026-01-04 01:38:23,401: t15.2024.07.28 val PER: 0.1243
2026-01-04 01:38:23,401: t15.2025.01.10 val PER: 0.2961
2026-01-04 01:38:23,401: t15.2025.01.12 val PER: 0.1470
2026-01-04 01:38:23,401: t15.2025.03.14 val PER: 0.3432
2026-01-04 01:38:23,401: t15.2025.03.16 val PER: 0.1898
2026-01-04 01:38:23,401: t15.2025.03.30 val PER: 0.2885
2026-01-04 01:38:23,402: t15.2025.04.13 val PER: 0.2154
2026-01-04 01:38:23,657: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_17000
2026-01-04 01:38:40,952: Train batch 17200: loss: 9.87 grad norm: 53.03 time: 0.083
2026-01-04 01:38:58,298: Train batch 17400: loss: 12.41 grad norm: 59.12 time: 0.071
2026-01-04 01:39:06,857: Running test after training batch: 17500
2026-01-04 01:39:06,948: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:39:11,666: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:39:11,700: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 01:39:13,588: Val batch 17500: PER (avg): 0.1485 CTC Loss (avg): 15.1402 WER(1gram): 45.43% (n=64) time: 6.730
2026-01-04 01:39:13,588: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 01:39:13,589: t15.2023.08.13 val PER: 0.1143
2026-01-04 01:39:13,589: t15.2023.08.18 val PER: 0.1014
2026-01-04 01:39:13,589: t15.2023.08.20 val PER: 0.1080
2026-01-04 01:39:13,589: t15.2023.08.25 val PER: 0.0934
2026-01-04 01:39:13,589: t15.2023.08.27 val PER: 0.1929
2026-01-04 01:39:13,589: t15.2023.09.01 val PER: 0.0755
2026-01-04 01:39:13,589: t15.2023.09.03 val PER: 0.1639
2026-01-04 01:39:13,589: t15.2023.09.24 val PER: 0.1286
2026-01-04 01:39:13,589: t15.2023.09.29 val PER: 0.1315
2026-01-04 01:39:13,590: t15.2023.10.01 val PER: 0.1697
2026-01-04 01:39:13,590: t15.2023.10.06 val PER: 0.0840
2026-01-04 01:39:13,590: t15.2023.10.08 val PER: 0.2449
2026-01-04 01:39:13,590: t15.2023.10.13 val PER: 0.1885
2026-01-04 01:39:13,590: t15.2023.10.15 val PER: 0.1503
2026-01-04 01:39:13,590: t15.2023.10.20 val PER: 0.1879
2026-01-04 01:39:13,590: t15.2023.10.22 val PER: 0.1114
2026-01-04 01:39:13,590: t15.2023.11.03 val PER: 0.1798
2026-01-04 01:39:13,590: t15.2023.11.04 val PER: 0.0375
2026-01-04 01:39:13,590: t15.2023.11.17 val PER: 0.0404
2026-01-04 01:39:13,590: t15.2023.11.19 val PER: 0.0419
2026-01-04 01:39:13,590: t15.2023.11.26 val PER: 0.1152
2026-01-04 01:39:13,591: t15.2023.12.03 val PER: 0.1092
2026-01-04 01:39:13,591: t15.2023.12.08 val PER: 0.0992
2026-01-04 01:39:13,591: t15.2023.12.10 val PER: 0.0880
2026-01-04 01:39:13,591: t15.2023.12.17 val PER: 0.1341
2026-01-04 01:39:13,591: t15.2023.12.29 val PER: 0.1277
2026-01-04 01:39:13,591: t15.2024.02.25 val PER: 0.1110
2026-01-04 01:39:13,591: t15.2024.03.08 val PER: 0.2319
2026-01-04 01:39:13,591: t15.2024.03.15 val PER: 0.2001
2026-01-04 01:39:13,591: t15.2024.03.17 val PER: 0.1395
2026-01-04 01:39:13,591: t15.2024.05.10 val PER: 0.1560
2026-01-04 01:39:13,591: t15.2024.06.14 val PER: 0.1483
2026-01-04 01:39:13,591: t15.2024.07.19 val PER: 0.2294
2026-01-04 01:39:13,592: t15.2024.07.21 val PER: 0.0910
2026-01-04 01:39:13,592: t15.2024.07.28 val PER: 0.1301
2026-01-04 01:39:13,592: t15.2025.01.10 val PER: 0.2906
2026-01-04 01:39:13,592: t15.2025.01.12 val PER: 0.1378
2026-01-04 01:39:13,592: t15.2025.03.14 val PER: 0.3595
2026-01-04 01:39:13,592: t15.2025.03.16 val PER: 0.1819
2026-01-04 01:39:13,592: t15.2025.03.30 val PER: 0.2908
2026-01-04 01:39:13,592: t15.2025.04.13 val PER: 0.2154
2026-01-04 01:39:13,847: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_17500
2026-01-04 01:39:22,622: Train batch 17600: loss: 9.99 grad norm: 54.08 time: 0.051
2026-01-04 01:39:40,371: Train batch 17800: loss: 6.40 grad norm: 47.32 time: 0.042
2026-01-04 01:39:58,043: Train batch 18000: loss: 11.09 grad norm: 65.08 time: 0.061
2026-01-04 01:39:58,043: Running test after training batch: 18000
2026-01-04 01:39:58,178: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:40:02,872: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:40:02,906: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 01:40:04,808: Val batch 18000: PER (avg): 0.1477 CTC Loss (avg): 15.0906 WER(1gram): 45.94% (n=64) time: 6.764
2026-01-04 01:40:04,808: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 01:40:04,808: t15.2023.08.13 val PER: 0.1133
2026-01-04 01:40:04,808: t15.2023.08.18 val PER: 0.1023
2026-01-04 01:40:04,808: t15.2023.08.20 val PER: 0.1104
2026-01-04 01:40:04,808: t15.2023.08.25 val PER: 0.0964
2026-01-04 01:40:04,808: t15.2023.08.27 val PER: 0.1929
2026-01-04 01:40:04,808: t15.2023.09.01 val PER: 0.0771
2026-01-04 01:40:04,808: t15.2023.09.03 val PER: 0.1603
2026-01-04 01:40:04,809: t15.2023.09.24 val PER: 0.1262
2026-01-04 01:40:04,809: t15.2023.09.29 val PER: 0.1283
2026-01-04 01:40:04,809: t15.2023.10.01 val PER: 0.1678
2026-01-04 01:40:04,809: t15.2023.10.06 val PER: 0.0840
2026-01-04 01:40:04,809: t15.2023.10.08 val PER: 0.2382
2026-01-04 01:40:04,809: t15.2023.10.13 val PER: 0.1908
2026-01-04 01:40:04,809: t15.2023.10.15 val PER: 0.1483
2026-01-04 01:40:04,809: t15.2023.10.20 val PER: 0.1913
2026-01-04 01:40:04,809: t15.2023.10.22 val PER: 0.1058
2026-01-04 01:40:04,809: t15.2023.11.03 val PER: 0.1777
2026-01-04 01:40:04,809: t15.2023.11.04 val PER: 0.0375
2026-01-04 01:40:04,809: t15.2023.11.17 val PER: 0.0389
2026-01-04 01:40:04,809: t15.2023.11.19 val PER: 0.0379
2026-01-04 01:40:04,809: t15.2023.11.26 val PER: 0.1109
2026-01-04 01:40:04,810: t15.2023.12.03 val PER: 0.1113
2026-01-04 01:40:04,810: t15.2023.12.08 val PER: 0.1012
2026-01-04 01:40:04,810: t15.2023.12.10 val PER: 0.0880
2026-01-04 01:40:04,810: t15.2023.12.17 val PER: 0.1331
2026-01-04 01:40:04,810: t15.2023.12.29 val PER: 0.1256
2026-01-04 01:40:04,810: t15.2024.02.25 val PER: 0.1124
2026-01-04 01:40:04,810: t15.2024.03.08 val PER: 0.2304
2026-01-04 01:40:04,810: t15.2024.03.15 val PER: 0.1970
2026-01-04 01:40:04,810: t15.2024.03.17 val PER: 0.1395
2026-01-04 01:40:04,810: t15.2024.05.10 val PER: 0.1530
2026-01-04 01:40:04,810: t15.2024.06.14 val PER: 0.1498
2026-01-04 01:40:04,811: t15.2024.07.19 val PER: 0.2314
2026-01-04 01:40:04,811: t15.2024.07.21 val PER: 0.0862
2026-01-04 01:40:04,811: t15.2024.07.28 val PER: 0.1294
2026-01-04 01:40:04,811: t15.2025.01.10 val PER: 0.2920
2026-01-04 01:40:04,811: t15.2025.01.12 val PER: 0.1440
2026-01-04 01:40:04,811: t15.2025.03.14 val PER: 0.3447
2026-01-04 01:40:04,811: t15.2025.03.16 val PER: 0.1846
2026-01-04 01:40:04,811: t15.2025.03.30 val PER: 0.2920
2026-01-04 01:40:04,812: t15.2025.04.13 val PER: 0.2183
2026-01-04 01:40:05,075: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_18000
2026-01-04 01:40:22,894: Train batch 18200: loss: 7.60 grad norm: 48.51 time: 0.075
2026-01-04 01:40:40,586: Train batch 18400: loss: 4.58 grad norm: 38.69 time: 0.057
2026-01-04 01:40:49,344: Running test after training batch: 18500
2026-01-04 01:40:49,494: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:40:54,358: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:40:54,392: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 01:40:56,308: Val batch 18500: PER (avg): 0.1473 CTC Loss (avg): 15.0446 WER(1gram): 45.94% (n=64) time: 6.963
2026-01-04 01:40:56,309: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 01:40:56,309: t15.2023.08.13 val PER: 0.1071
2026-01-04 01:40:56,309: t15.2023.08.18 val PER: 0.0997
2026-01-04 01:40:56,309: t15.2023.08.20 val PER: 0.1136
2026-01-04 01:40:56,309: t15.2023.08.25 val PER: 0.0979
2026-01-04 01:40:56,309: t15.2023.08.27 val PER: 0.1929
2026-01-04 01:40:56,309: t15.2023.09.01 val PER: 0.0747
2026-01-04 01:40:56,309: t15.2023.09.03 val PER: 0.1639
2026-01-04 01:40:56,309: t15.2023.09.24 val PER: 0.1201
2026-01-04 01:40:56,309: t15.2023.09.29 val PER: 0.1302
2026-01-04 01:40:56,309: t15.2023.10.01 val PER: 0.1671
2026-01-04 01:40:56,310: t15.2023.10.06 val PER: 0.0786
2026-01-04 01:40:56,310: t15.2023.10.08 val PER: 0.2395
2026-01-04 01:40:56,310: t15.2023.10.13 val PER: 0.1994
2026-01-04 01:40:56,310: t15.2023.10.15 val PER: 0.1477
2026-01-04 01:40:56,310: t15.2023.10.20 val PER: 0.1779
2026-01-04 01:40:56,310: t15.2023.10.22 val PER: 0.1069
2026-01-04 01:40:56,310: t15.2023.11.03 val PER: 0.1784
2026-01-04 01:40:56,310: t15.2023.11.04 val PER: 0.0307
2026-01-04 01:40:56,310: t15.2023.11.17 val PER: 0.0373
2026-01-04 01:40:56,310: t15.2023.11.19 val PER: 0.0399
2026-01-04 01:40:56,310: t15.2023.11.26 val PER: 0.1145
2026-01-04 01:40:56,310: t15.2023.12.03 val PER: 0.1071
2026-01-04 01:40:56,310: t15.2023.12.08 val PER: 0.1005
2026-01-04 01:40:56,310: t15.2023.12.10 val PER: 0.0854
2026-01-04 01:40:56,311: t15.2023.12.17 val PER: 0.1289
2026-01-04 01:40:56,311: t15.2023.12.29 val PER: 0.1242
2026-01-04 01:40:56,311: t15.2024.02.25 val PER: 0.1110
2026-01-04 01:40:56,311: t15.2024.03.08 val PER: 0.2276
2026-01-04 01:40:56,311: t15.2024.03.15 val PER: 0.1951
2026-01-04 01:40:56,311: t15.2024.03.17 val PER: 0.1416
2026-01-04 01:40:56,311: t15.2024.05.10 val PER: 0.1545
2026-01-04 01:40:56,311: t15.2024.06.14 val PER: 0.1546
2026-01-04 01:40:56,311: t15.2024.07.19 val PER: 0.2353
2026-01-04 01:40:56,311: t15.2024.07.21 val PER: 0.0862
2026-01-04 01:40:56,311: t15.2024.07.28 val PER: 0.1279
2026-01-04 01:40:56,311: t15.2025.01.10 val PER: 0.2906
2026-01-04 01:40:56,311: t15.2025.01.12 val PER: 0.1432
2026-01-04 01:40:56,311: t15.2025.03.14 val PER: 0.3462
2026-01-04 01:40:56,311: t15.2025.03.16 val PER: 0.1846
2026-01-04 01:40:56,311: t15.2025.03.30 val PER: 0.2920
2026-01-04 01:40:56,312: t15.2025.04.13 val PER: 0.2111
2026-01-04 01:40:56,578: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_18500
2026-01-04 01:41:05,318: Train batch 18600: loss: 12.71 grad norm: 61.73 time: 0.067
2026-01-04 01:41:22,661: Train batch 18800: loss: 8.38 grad norm: 53.79 time: 0.063
2026-01-04 01:41:40,298: Train batch 19000: loss: 8.13 grad norm: 45.57 time: 0.063
2026-01-04 01:41:40,299: Running test after training batch: 19000
2026-01-04 01:41:40,431: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:41:45,189: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:41:45,224: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-04 01:41:47,149: Val batch 19000: PER (avg): 0.1472 CTC Loss (avg): 15.0670 WER(1gram): 45.69% (n=64) time: 6.850
2026-01-04 01:41:47,149: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 01:41:47,149: t15.2023.08.13 val PER: 0.1123
2026-01-04 01:41:47,150: t15.2023.08.18 val PER: 0.0972
2026-01-04 01:41:47,150: t15.2023.08.20 val PER: 0.1104
2026-01-04 01:41:47,150: t15.2023.08.25 val PER: 0.0934
2026-01-04 01:41:47,150: t15.2023.08.27 val PER: 0.1897
2026-01-04 01:41:47,150: t15.2023.09.01 val PER: 0.0739
2026-01-04 01:41:47,150: t15.2023.09.03 val PER: 0.1591
2026-01-04 01:41:47,150: t15.2023.09.24 val PER: 0.1262
2026-01-04 01:41:47,150: t15.2023.09.29 val PER: 0.1315
2026-01-04 01:41:47,150: t15.2023.10.01 val PER: 0.1664
2026-01-04 01:41:47,151: t15.2023.10.06 val PER: 0.0807
2026-01-04 01:41:47,151: t15.2023.10.08 val PER: 0.2449
2026-01-04 01:41:47,151: t15.2023.10.13 val PER: 0.1893
2026-01-04 01:41:47,151: t15.2023.10.15 val PER: 0.1483
2026-01-04 01:41:47,151: t15.2023.10.20 val PER: 0.1779
2026-01-04 01:41:47,151: t15.2023.10.22 val PER: 0.1047
2026-01-04 01:41:47,151: t15.2023.11.03 val PER: 0.1832
2026-01-04 01:41:47,151: t15.2023.11.04 val PER: 0.0273
2026-01-04 01:41:47,151: t15.2023.11.17 val PER: 0.0342
2026-01-04 01:41:47,151: t15.2023.11.19 val PER: 0.0359
2026-01-04 01:41:47,151: t15.2023.11.26 val PER: 0.1116
2026-01-04 01:41:47,151: t15.2023.12.03 val PER: 0.1113
2026-01-04 01:41:47,152: t15.2023.12.08 val PER: 0.0959
2026-01-04 01:41:47,152: t15.2023.12.10 val PER: 0.0907
2026-01-04 01:41:47,152: t15.2023.12.17 val PER: 0.1372
2026-01-04 01:41:47,152: t15.2023.12.29 val PER: 0.1283
2026-01-04 01:41:47,152: t15.2024.02.25 val PER: 0.1067
2026-01-04 01:41:47,152: t15.2024.03.08 val PER: 0.2304
2026-01-04 01:41:47,152: t15.2024.03.15 val PER: 0.1926
2026-01-04 01:41:47,152: t15.2024.03.17 val PER: 0.1409
2026-01-04 01:41:47,152: t15.2024.05.10 val PER: 0.1516
2026-01-04 01:41:47,152: t15.2024.06.14 val PER: 0.1514
2026-01-04 01:41:47,152: t15.2024.07.19 val PER: 0.2241
2026-01-04 01:41:47,152: t15.2024.07.21 val PER: 0.0869
2026-01-04 01:41:47,152: t15.2024.07.28 val PER: 0.1294
2026-01-04 01:41:47,152: t15.2025.01.10 val PER: 0.2961
2026-01-04 01:41:47,152: t15.2025.01.12 val PER: 0.1432
2026-01-04 01:41:47,152: t15.2025.03.14 val PER: 0.3565
2026-01-04 01:41:47,152: t15.2025.03.16 val PER: 0.1872
2026-01-04 01:41:47,153: t15.2025.03.30 val PER: 0.2885
2026-01-04 01:41:47,153: t15.2025.04.13 val PER: 0.2225
2026-01-04 01:41:47,397: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_19000
2026-01-04 01:42:04,534: Train batch 19200: loss: 5.57 grad norm: 45.07 time: 0.063
2026-01-04 01:42:22,294: Train batch 19400: loss: 5.10 grad norm: 36.56 time: 0.053
2026-01-04 01:42:31,205: Running test after training batch: 19500
2026-01-04 01:42:31,338: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:42:36,038: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:42:36,073: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 01:42:38,000: Val batch 19500: PER (avg): 0.1470 CTC Loss (avg): 15.0083 WER(1gram): 45.69% (n=64) time: 6.794
2026-01-04 01:42:38,000: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-04 01:42:38,001: t15.2023.08.13 val PER: 0.1060
2026-01-04 01:42:38,001: t15.2023.08.18 val PER: 0.0997
2026-01-04 01:42:38,001: t15.2023.08.20 val PER: 0.1136
2026-01-04 01:42:38,001: t15.2023.08.25 val PER: 0.0934
2026-01-04 01:42:38,001: t15.2023.08.27 val PER: 0.1945
2026-01-04 01:42:38,001: t15.2023.09.01 val PER: 0.0739
2026-01-04 01:42:38,001: t15.2023.09.03 val PER: 0.1603
2026-01-04 01:42:38,001: t15.2023.09.24 val PER: 0.1201
2026-01-04 01:42:38,001: t15.2023.09.29 val PER: 0.1327
2026-01-04 01:42:38,001: t15.2023.10.01 val PER: 0.1697
2026-01-04 01:42:38,001: t15.2023.10.06 val PER: 0.0807
2026-01-04 01:42:38,002: t15.2023.10.08 val PER: 0.2409
2026-01-04 01:42:38,002: t15.2023.10.13 val PER: 0.1924
2026-01-04 01:42:38,002: t15.2023.10.15 val PER: 0.1477
2026-01-04 01:42:38,002: t15.2023.10.20 val PER: 0.1913
2026-01-04 01:42:38,002: t15.2023.10.22 val PER: 0.1024
2026-01-04 01:42:38,002: t15.2023.11.03 val PER: 0.1805
2026-01-04 01:42:38,002: t15.2023.11.04 val PER: 0.0375
2026-01-04 01:42:38,002: t15.2023.11.17 val PER: 0.0358
2026-01-04 01:42:38,002: t15.2023.11.19 val PER: 0.0379
2026-01-04 01:42:38,002: t15.2023.11.26 val PER: 0.1109
2026-01-04 01:42:38,002: t15.2023.12.03 val PER: 0.1082
2026-01-04 01:42:38,003: t15.2023.12.08 val PER: 0.0985
2026-01-04 01:42:38,003: t15.2023.12.10 val PER: 0.0867
2026-01-04 01:42:38,003: t15.2023.12.17 val PER: 0.1320
2026-01-04 01:42:38,003: t15.2023.12.29 val PER: 0.1229
2026-01-04 01:42:38,003: t15.2024.02.25 val PER: 0.1110
2026-01-04 01:42:38,003: t15.2024.03.08 val PER: 0.2304
2026-01-04 01:42:38,003: t15.2024.03.15 val PER: 0.1926
2026-01-04 01:42:38,003: t15.2024.03.17 val PER: 0.1318
2026-01-04 01:42:38,003: t15.2024.05.10 val PER: 0.1560
2026-01-04 01:42:38,003: t15.2024.06.14 val PER: 0.1562
2026-01-04 01:42:38,003: t15.2024.07.19 val PER: 0.2307
2026-01-04 01:42:38,003: t15.2024.07.21 val PER: 0.0869
2026-01-04 01:42:38,003: t15.2024.07.28 val PER: 0.1287
2026-01-04 01:42:38,003: t15.2025.01.10 val PER: 0.2893
2026-01-04 01:42:38,004: t15.2025.01.12 val PER: 0.1463
2026-01-04 01:42:38,004: t15.2025.03.14 val PER: 0.3521
2026-01-04 01:42:38,004: t15.2025.03.16 val PER: 0.1832
2026-01-04 01:42:38,004: t15.2025.03.30 val PER: 0.2943
2026-01-04 01:42:38,004: t15.2025.04.13 val PER: 0.2183
2026-01-04 01:42:38,270: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_19500
2026-01-04 01:42:47,096: Train batch 19600: loss: 7.85 grad norm: 46.67 time: 0.057
2026-01-04 01:43:04,585: Train batch 19800: loss: 7.38 grad norm: 54.04 time: 0.055
2026-01-04 01:43:22,171: Running test after training batch: 19999
2026-01-04 01:43:22,278: WER debug GT example: You can see the code at this point as well.
2026-01-04 01:43:26,926: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-04 01:43:26,962: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-04 01:43:28,900: Val batch 19999: PER (avg): 0.1458 CTC Loss (avg): 14.9989 WER(1gram): 45.94% (n=64) time: 6.728
2026-01-04 01:43:28,900: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-04 01:43:28,901: t15.2023.08.13 val PER: 0.1081
2026-01-04 01:43:28,901: t15.2023.08.18 val PER: 0.1006
2026-01-04 01:43:28,901: t15.2023.08.20 val PER: 0.1136
2026-01-04 01:43:28,901: t15.2023.08.25 val PER: 0.0919
2026-01-04 01:43:28,901: t15.2023.08.27 val PER: 0.1897
2026-01-04 01:43:28,901: t15.2023.09.01 val PER: 0.0731
2026-01-04 01:43:28,901: t15.2023.09.03 val PER: 0.1591
2026-01-04 01:43:28,901: t15.2023.09.24 val PER: 0.1250
2026-01-04 01:43:28,901: t15.2023.09.29 val PER: 0.1295
2026-01-04 01:43:28,901: t15.2023.10.01 val PER: 0.1658
2026-01-04 01:43:28,902: t15.2023.10.06 val PER: 0.0818
2026-01-04 01:43:28,902: t15.2023.10.08 val PER: 0.2327
2026-01-04 01:43:28,902: t15.2023.10.13 val PER: 0.1932
2026-01-04 01:43:28,902: t15.2023.10.15 val PER: 0.1477
2026-01-04 01:43:28,902: t15.2023.10.20 val PER: 0.1745
2026-01-04 01:43:28,902: t15.2023.10.22 val PER: 0.1024
2026-01-04 01:43:28,902: t15.2023.11.03 val PER: 0.1777
2026-01-04 01:43:28,902: t15.2023.11.04 val PER: 0.0375
2026-01-04 01:43:28,902: t15.2023.11.17 val PER: 0.0404
2026-01-04 01:43:28,902: t15.2023.11.19 val PER: 0.0399
2026-01-04 01:43:28,902: t15.2023.11.26 val PER: 0.1058
2026-01-04 01:43:28,902: t15.2023.12.03 val PER: 0.1082
2026-01-04 01:43:28,902: t15.2023.12.08 val PER: 0.0992
2026-01-04 01:43:28,902: t15.2023.12.10 val PER: 0.0907
2026-01-04 01:43:28,903: t15.2023.12.17 val PER: 0.1268
2026-01-04 01:43:28,903: t15.2023.12.29 val PER: 0.1229
2026-01-04 01:43:28,903: t15.2024.02.25 val PER: 0.1081
2026-01-04 01:43:28,903: t15.2024.03.08 val PER: 0.2290
2026-01-04 01:43:28,903: t15.2024.03.15 val PER: 0.1945
2026-01-04 01:43:28,903: t15.2024.03.17 val PER: 0.1311
2026-01-04 01:43:28,903: t15.2024.05.10 val PER: 0.1575
2026-01-04 01:43:28,903: t15.2024.06.14 val PER: 0.1483
2026-01-04 01:43:28,903: t15.2024.07.19 val PER: 0.2301
2026-01-04 01:43:28,903: t15.2024.07.21 val PER: 0.0883
2026-01-04 01:43:28,903: t15.2024.07.28 val PER: 0.1265
2026-01-04 01:43:28,903: t15.2025.01.10 val PER: 0.2920
2026-01-04 01:43:28,903: t15.2025.01.12 val PER: 0.1424
2026-01-04 01:43:28,903: t15.2025.03.14 val PER: 0.3506
2026-01-04 01:43:28,903: t15.2025.03.16 val PER: 0.1767
2026-01-04 01:43:28,903: t15.2025.03.30 val PER: 0.2943
2026-01-04 01:43:28,904: t15.2025.04.13 val PER: 0.2097
2026-01-04 01:43:29,154: Saved model to checkpoint: /tmp/e12511253_b2t_348482/trained_models/rnn_dropout/lr40/d20/checkpoint/checkpoint_batch_19999
2026-01-04 01:43:29,185: Best avg val PER achieved: 0.14882
2026-01-04 01:43:29,185: Total training time: 34.44 minutes
All runs finished. Outputs in: /tmp/e12511253_b2t_348482/trained_models
