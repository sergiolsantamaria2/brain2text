TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_349139
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_349139/torch_extensions
WANDB_DIR=/tmp/e12511253_b2t_349139/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/e12511253_b2t_349139/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  5 12:48 /tmp/e12511253_b2t_349139/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
trained_models -> /tmp/e12511253_b2t_349139/trained_models
OUT_ROOT=/tmp/e12511253_b2t_349139/trained_models
==============================================
Job: b2t_exp  ID: 349139
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/speckleFeat_p/lr40_wd1e-5
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/speckleFeat_p/lr40_wd1e-5 ==========
Num configs: 5

=== RUN base_speckleFeat_wd1e-5.yaml ===
2026-01-05 12:48:25,531: Using device: cuda:0
2026-01-05 12:48:27,358: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-05 12:48:27,383: Using 45 sessions after filtering (from 45).
2026-01-05 12:48:27,785: Using torch.compile (if available)
2026-01-05 12:48:27,786: torch.compile not available (torch<2.0). Skipping.
2026-01-05 12:48:27,786: Initialized RNN decoding model
2026-01-05 12:48:27,786: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-05 12:48:27,787: Model has 44,907,305 parameters
2026-01-05 12:48:27,787: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-05 12:48:29,104: Successfully initialized datasets
2026-01-05 12:48:29,105: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-05 12:48:30,112: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.193
2026-01-05 12:48:30,112: Running test after training batch: 0
2026-01-05 12:48:30,234: WER debug GT example: You can see the code at this point as well.
2026-01-05 12:48:35,853: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-05 12:48:36,617: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-05 12:49:13,348: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 43.236
2026-01-05 12:49:13,348: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-05 12:49:13,349: t15.2023.08.13 val PER: 1.3056
2026-01-05 12:49:13,349: t15.2023.08.18 val PER: 1.4208
2026-01-05 12:49:13,349: t15.2023.08.20 val PER: 1.3002
2026-01-05 12:49:13,349: t15.2023.08.25 val PER: 1.3389
2026-01-05 12:49:13,349: t15.2023.08.27 val PER: 1.2460
2026-01-05 12:49:13,349: t15.2023.09.01 val PER: 1.4537
2026-01-05 12:49:13,349: t15.2023.09.03 val PER: 1.3171
2026-01-05 12:49:13,349: t15.2023.09.24 val PER: 1.5461
2026-01-05 12:49:13,349: t15.2023.09.29 val PER: 1.4671
2026-01-05 12:49:13,349: t15.2023.10.01 val PER: 1.2147
2026-01-05 12:49:13,349: t15.2023.10.06 val PER: 1.4876
2026-01-05 12:49:13,349: t15.2023.10.08 val PER: 1.1827
2026-01-05 12:49:13,349: t15.2023.10.13 val PER: 1.3964
2026-01-05 12:49:13,349: t15.2023.10.15 val PER: 1.3889
2026-01-05 12:49:13,350: t15.2023.10.20 val PER: 1.4866
2026-01-05 12:49:13,350: t15.2023.10.22 val PER: 1.3942
2026-01-05 12:49:13,350: t15.2023.11.03 val PER: 1.5923
2026-01-05 12:49:13,350: t15.2023.11.04 val PER: 2.0171
2026-01-05 12:49:13,350: t15.2023.11.17 val PER: 1.9518
2026-01-05 12:49:13,350: t15.2023.11.19 val PER: 1.6707
2026-01-05 12:49:13,350: t15.2023.11.26 val PER: 1.5413
2026-01-05 12:49:13,350: t15.2023.12.03 val PER: 1.4254
2026-01-05 12:49:13,350: t15.2023.12.08 val PER: 1.4487
2026-01-05 12:49:13,350: t15.2023.12.10 val PER: 1.6899
2026-01-05 12:49:13,350: t15.2023.12.17 val PER: 1.3077
2026-01-05 12:49:13,351: t15.2023.12.29 val PER: 1.4063
2026-01-05 12:49:13,351: t15.2024.02.25 val PER: 1.4228
2026-01-05 12:49:13,351: t15.2024.03.08 val PER: 1.3257
2026-01-05 12:49:13,351: t15.2024.03.15 val PER: 1.3196
2026-01-05 12:49:13,351: t15.2024.03.17 val PER: 1.4052
2026-01-05 12:49:13,351: t15.2024.05.10 val PER: 1.3224
2026-01-05 12:49:13,351: t15.2024.06.14 val PER: 1.5315
2026-01-05 12:49:13,351: t15.2024.07.19 val PER: 1.0817
2026-01-05 12:49:13,351: t15.2024.07.21 val PER: 1.6290
2026-01-05 12:49:13,351: t15.2024.07.28 val PER: 1.6588
2026-01-05 12:49:13,351: t15.2025.01.10 val PER: 1.0923
2026-01-05 12:49:13,351: t15.2025.01.12 val PER: 1.7629
2026-01-05 12:49:13,351: t15.2025.03.14 val PER: 1.0414
2026-01-05 12:49:13,351: t15.2025.03.16 val PER: 1.6257
2026-01-05 12:49:13,351: t15.2025.03.30 val PER: 1.2874
2026-01-05 12:49:13,351: t15.2025.04.13 val PER: 1.5949
2026-01-05 12:49:13,352: New best val WER(1gram) inf% --> 100.00%
2026-01-05 12:49:13,352: Checkpointing model
2026-01-05 12:49:13,651: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 12:49:13,951: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_0
2026-01-05 12:49:32,824: Train batch 200: loss: 77.59 grad norm: 106.19 time: 0.054
2026-01-05 12:49:51,875: Train batch 400: loss: 53.57 grad norm: 96.21 time: 0.063
2026-01-05 12:50:01,075: Running test after training batch: 500
2026-01-05 12:50:01,224: WER debug GT example: You can see the code at this point as well.
2026-01-05 12:50:06,438: WER debug example
  GT : you can see the code at this point as well
  PR : yule and ease thus uhde at this uhde is aisle
2026-01-05 12:50:06,472: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as adz
2026-01-05 12:50:08,702: Val batch 500: PER (avg): 0.5220 CTC Loss (avg): 55.3104 WER(1gram): 89.09% (n=64) time: 7.626
2026-01-05 12:50:08,702: WER lens: avg_true_words=6.16 avg_pred_words=5.52 max_pred_words=11
2026-01-05 12:50:08,702: t15.2023.08.13 val PER: 0.4636
2026-01-05 12:50:08,702: t15.2023.08.18 val PER: 0.4577
2026-01-05 12:50:08,702: t15.2023.08.20 val PER: 0.4488
2026-01-05 12:50:08,703: t15.2023.08.25 val PER: 0.4292
2026-01-05 12:50:08,703: t15.2023.08.27 val PER: 0.5386
2026-01-05 12:50:08,703: t15.2023.09.01 val PER: 0.4221
2026-01-05 12:50:08,703: t15.2023.09.03 val PER: 0.5036
2026-01-05 12:50:08,703: t15.2023.09.24 val PER: 0.4284
2026-01-05 12:50:08,703: t15.2023.09.29 val PER: 0.4716
2026-01-05 12:50:08,703: t15.2023.10.01 val PER: 0.5284
2026-01-05 12:50:08,703: t15.2023.10.06 val PER: 0.4338
2026-01-05 12:50:08,703: t15.2023.10.08 val PER: 0.5413
2026-01-05 12:50:08,704: t15.2023.10.13 val PER: 0.5795
2026-01-05 12:50:08,704: t15.2023.10.15 val PER: 0.5010
2026-01-05 12:50:08,704: t15.2023.10.20 val PER: 0.4530
2026-01-05 12:50:08,704: t15.2023.10.22 val PER: 0.4555
2026-01-05 12:50:08,704: t15.2023.11.03 val PER: 0.5115
2026-01-05 12:50:08,704: t15.2023.11.04 val PER: 0.2594
2026-01-05 12:50:08,704: t15.2023.11.17 val PER: 0.3701
2026-01-05 12:50:08,704: t15.2023.11.19 val PER: 0.3253
2026-01-05 12:50:08,704: t15.2023.11.26 val PER: 0.5493
2026-01-05 12:50:08,704: t15.2023.12.03 val PER: 0.5074
2026-01-05 12:50:08,704: t15.2023.12.08 val PER: 0.5273
2026-01-05 12:50:08,704: t15.2023.12.10 val PER: 0.4573
2026-01-05 12:50:08,704: t15.2023.12.17 val PER: 0.5582
2026-01-05 12:50:08,704: t15.2023.12.29 val PER: 0.5463
2026-01-05 12:50:08,704: t15.2024.02.25 val PER: 0.4944
2026-01-05 12:50:08,705: t15.2024.03.08 val PER: 0.6259
2026-01-05 12:50:08,705: t15.2024.03.15 val PER: 0.5635
2026-01-05 12:50:08,705: t15.2024.03.17 val PER: 0.5035
2026-01-05 12:50:08,705: t15.2024.05.10 val PER: 0.5409
2026-01-05 12:50:08,705: t15.2024.06.14 val PER: 0.5079
2026-01-05 12:50:08,705: t15.2024.07.19 val PER: 0.6737
2026-01-05 12:50:08,705: t15.2024.07.21 val PER: 0.4841
2026-01-05 12:50:08,705: t15.2024.07.28 val PER: 0.5154
2026-01-05 12:50:08,705: t15.2025.01.10 val PER: 0.7493
2026-01-05 12:50:08,705: t15.2025.01.12 val PER: 0.5589
2026-01-05 12:50:08,705: t15.2025.03.14 val PER: 0.7515
2026-01-05 12:50:08,705: t15.2025.03.16 val PER: 0.5890
2026-01-05 12:50:08,705: t15.2025.03.30 val PER: 0.7379
2026-01-05 12:50:08,706: t15.2025.04.13 val PER: 0.5692
2026-01-05 12:50:08,706: New best val WER(1gram) 100.00% --> 89.09%
2026-01-05 12:50:08,706: Checkpointing model
2026-01-05 12:50:09,354: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 12:50:09,643: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_500
2026-01-05 12:50:18,793: Train batch 600: loss: 49.07 grad norm: 82.09 time: 0.078
2026-01-05 12:50:38,658: Train batch 800: loss: 40.92 grad norm: 84.39 time: 0.057
2026-01-05 12:50:57,250: Train batch 1000: loss: 43.00 grad norm: 81.66 time: 0.066
2026-01-05 12:50:57,251: Running test after training batch: 1000
2026-01-05 12:50:57,393: WER debug GT example: You can see the code at this point as well.
2026-01-05 12:51:02,524: WER debug example
  GT : you can see the code at this point as well
  PR : used ent ease thus owed at this boyde is will
2026-01-05 12:51:02,559: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus wass it
2026-01-05 12:51:04,488: Val batch 1000: PER (avg): 0.4082 CTC Loss (avg): 42.4790 WER(1gram): 81.22% (n=64) time: 7.237
2026-01-05 12:51:04,488: WER lens: avg_true_words=6.16 avg_pred_words=5.53 max_pred_words=12
2026-01-05 12:51:04,488: t15.2023.08.13 val PER: 0.3825
2026-01-05 12:51:04,489: t15.2023.08.18 val PER: 0.3395
2026-01-05 12:51:04,489: t15.2023.08.20 val PER: 0.3439
2026-01-05 12:51:04,489: t15.2023.08.25 val PER: 0.2997
2026-01-05 12:51:04,489: t15.2023.08.27 val PER: 0.4341
2026-01-05 12:51:04,489: t15.2023.09.01 val PER: 0.3028
2026-01-05 12:51:04,489: t15.2023.09.03 val PER: 0.3990
2026-01-05 12:51:04,489: t15.2023.09.24 val PER: 0.3374
2026-01-05 12:51:04,489: t15.2023.09.29 val PER: 0.3695
2026-01-05 12:51:04,489: t15.2023.10.01 val PER: 0.4075
2026-01-05 12:51:04,489: t15.2023.10.06 val PER: 0.3122
2026-01-05 12:51:04,489: t15.2023.10.08 val PER: 0.4574
2026-01-05 12:51:04,489: t15.2023.10.13 val PER: 0.4631
2026-01-05 12:51:04,490: t15.2023.10.15 val PER: 0.3757
2026-01-05 12:51:04,490: t15.2023.10.20 val PER: 0.3725
2026-01-05 12:51:04,490: t15.2023.10.22 val PER: 0.3385
2026-01-05 12:51:04,490: t15.2023.11.03 val PER: 0.4043
2026-01-05 12:51:04,490: t15.2023.11.04 val PER: 0.1672
2026-01-05 12:51:04,490: t15.2023.11.17 val PER: 0.2426
2026-01-05 12:51:04,490: t15.2023.11.19 val PER: 0.2116
2026-01-05 12:51:04,490: t15.2023.11.26 val PER: 0.4435
2026-01-05 12:51:04,490: t15.2023.12.03 val PER: 0.4076
2026-01-05 12:51:04,490: t15.2023.12.08 val PER: 0.4075
2026-01-05 12:51:04,491: t15.2023.12.10 val PER: 0.3469
2026-01-05 12:51:04,491: t15.2023.12.17 val PER: 0.4116
2026-01-05 12:51:04,491: t15.2023.12.29 val PER: 0.3946
2026-01-05 12:51:04,491: t15.2024.02.25 val PER: 0.3497
2026-01-05 12:51:04,491: t15.2024.03.08 val PER: 0.5050
2026-01-05 12:51:04,491: t15.2024.03.15 val PER: 0.4409
2026-01-05 12:51:04,491: t15.2024.03.17 val PER: 0.4038
2026-01-05 12:51:04,491: t15.2024.05.10 val PER: 0.4220
2026-01-05 12:51:04,491: t15.2024.06.14 val PER: 0.3991
2026-01-05 12:51:04,491: t15.2024.07.19 val PER: 0.5346
2026-01-05 12:51:04,491: t15.2024.07.21 val PER: 0.3738
2026-01-05 12:51:04,491: t15.2024.07.28 val PER: 0.4176
2026-01-05 12:51:04,491: t15.2025.01.10 val PER: 0.5950
2026-01-05 12:51:04,491: t15.2025.01.12 val PER: 0.4550
2026-01-05 12:51:04,491: t15.2025.03.14 val PER: 0.6302
2026-01-05 12:51:04,491: t15.2025.03.16 val PER: 0.4804
2026-01-05 12:51:04,492: t15.2025.03.30 val PER: 0.6448
2026-01-05 12:51:04,492: t15.2025.04.13 val PER: 0.5036
2026-01-05 12:51:04,493: New best val WER(1gram) 89.09% --> 81.22%
2026-01-05 12:51:04,493: Checkpointing model
2026-01-05 12:51:05,172: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 12:51:05,476: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_1000
2026-01-05 12:51:24,102: Train batch 1200: loss: 32.73 grad norm: 75.07 time: 0.068
2026-01-05 12:51:43,600: Train batch 1400: loss: 36.43 grad norm: 79.49 time: 0.061
2026-01-05 12:51:52,994: Running test after training batch: 1500
2026-01-05 12:51:53,169: WER debug GT example: You can see the code at this point as well.
2026-01-05 12:51:58,334: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt e the good it this boyde is wheel
2026-01-05 12:51:58,367: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that us it
2026-01-05 12:52:00,001: Val batch 1500: PER (avg): 0.3813 CTC Loss (avg): 37.2081 WER(1gram): 75.63% (n=64) time: 7.006
2026-01-05 12:52:00,002: WER lens: avg_true_words=6.16 avg_pred_words=4.97 max_pred_words=11
2026-01-05 12:52:00,002: t15.2023.08.13 val PER: 0.3441
2026-01-05 12:52:00,002: t15.2023.08.18 val PER: 0.3110
2026-01-05 12:52:00,002: t15.2023.08.20 val PER: 0.3042
2026-01-05 12:52:00,002: t15.2023.08.25 val PER: 0.2636
2026-01-05 12:52:00,002: t15.2023.08.27 val PER: 0.4116
2026-01-05 12:52:00,002: t15.2023.09.01 val PER: 0.2825
2026-01-05 12:52:00,002: t15.2023.09.03 val PER: 0.3717
2026-01-05 12:52:00,002: t15.2023.09.24 val PER: 0.3046
2026-01-05 12:52:00,003: t15.2023.09.29 val PER: 0.3306
2026-01-05 12:52:00,003: t15.2023.10.01 val PER: 0.3970
2026-01-05 12:52:00,003: t15.2023.10.06 val PER: 0.2874
2026-01-05 12:52:00,003: t15.2023.10.08 val PER: 0.4357
2026-01-05 12:52:00,003: t15.2023.10.13 val PER: 0.4414
2026-01-05 12:52:00,003: t15.2023.10.15 val PER: 0.3606
2026-01-05 12:52:00,003: t15.2023.10.20 val PER: 0.3423
2026-01-05 12:52:00,003: t15.2023.10.22 val PER: 0.3129
2026-01-05 12:52:00,003: t15.2023.11.03 val PER: 0.3562
2026-01-05 12:52:00,003: t15.2023.11.04 val PER: 0.1263
2026-01-05 12:52:00,003: t15.2023.11.17 val PER: 0.2240
2026-01-05 12:52:00,003: t15.2023.11.19 val PER: 0.1776
2026-01-05 12:52:00,003: t15.2023.11.26 val PER: 0.4174
2026-01-05 12:52:00,003: t15.2023.12.03 val PER: 0.3750
2026-01-05 12:52:00,004: t15.2023.12.08 val PER: 0.3582
2026-01-05 12:52:00,004: t15.2023.12.10 val PER: 0.3246
2026-01-05 12:52:00,004: t15.2023.12.17 val PER: 0.3773
2026-01-05 12:52:00,004: t15.2023.12.29 val PER: 0.3741
2026-01-05 12:52:00,004: t15.2024.02.25 val PER: 0.3048
2026-01-05 12:52:00,004: t15.2024.03.08 val PER: 0.4609
2026-01-05 12:52:00,004: t15.2024.03.15 val PER: 0.4153
2026-01-05 12:52:00,004: t15.2024.03.17 val PER: 0.3780
2026-01-05 12:52:00,004: t15.2024.05.10 val PER: 0.3997
2026-01-05 12:52:00,004: t15.2024.06.14 val PER: 0.4148
2026-01-05 12:52:00,004: t15.2024.07.19 val PER: 0.5313
2026-01-05 12:52:00,004: t15.2024.07.21 val PER: 0.3393
2026-01-05 12:52:00,005: t15.2024.07.28 val PER: 0.3684
2026-01-05 12:52:00,005: t15.2025.01.10 val PER: 0.6267
2026-01-05 12:52:00,005: t15.2025.01.12 val PER: 0.4265
2026-01-05 12:52:00,005: t15.2025.03.14 val PER: 0.6050
2026-01-05 12:52:00,005: t15.2025.03.16 val PER: 0.4647
2026-01-05 12:52:00,005: t15.2025.03.30 val PER: 0.6276
2026-01-05 12:52:00,005: t15.2025.04.13 val PER: 0.4708
2026-01-05 12:52:00,006: New best val WER(1gram) 81.22% --> 75.63%
2026-01-05 12:52:00,006: Checkpointing model
2026-01-05 12:52:00,646: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 12:52:00,930: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_1500
2026-01-05 12:52:10,121: Train batch 1600: loss: 36.90 grad norm: 78.90 time: 0.064
2026-01-05 12:52:28,792: Train batch 1800: loss: 35.07 grad norm: 70.32 time: 0.088
2026-01-05 12:52:47,435: Train batch 2000: loss: 33.47 grad norm: 67.54 time: 0.067
2026-01-05 12:52:47,435: Running test after training batch: 2000
2026-01-05 12:52:47,583: WER debug GT example: You can see the code at this point as well.
2026-01-05 12:52:52,841: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned ease the code at this and is will
2026-01-05 12:52:52,874: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap thus las it
2026-01-05 12:52:54,566: Val batch 2000: PER (avg): 0.3279 CTC Loss (avg): 32.8600 WER(1gram): 71.07% (n=64) time: 7.131
2026-01-05 12:52:54,567: WER lens: avg_true_words=6.16 avg_pred_words=5.66 max_pred_words=11
2026-01-05 12:52:54,567: t15.2023.08.13 val PER: 0.3108
2026-01-05 12:52:54,567: t15.2023.08.18 val PER: 0.2506
2026-01-05 12:52:54,567: t15.2023.08.20 val PER: 0.2534
2026-01-05 12:52:54,567: t15.2023.08.25 val PER: 0.2380
2026-01-05 12:52:54,567: t15.2023.08.27 val PER: 0.3424
2026-01-05 12:52:54,567: t15.2023.09.01 val PER: 0.2281
2026-01-05 12:52:54,567: t15.2023.09.03 val PER: 0.3171
2026-01-05 12:52:54,567: t15.2023.09.24 val PER: 0.2549
2026-01-05 12:52:54,567: t15.2023.09.29 val PER: 0.2738
2026-01-05 12:52:54,567: t15.2023.10.01 val PER: 0.3243
2026-01-05 12:52:54,568: t15.2023.10.06 val PER: 0.2357
2026-01-05 12:52:54,568: t15.2023.10.08 val PER: 0.3951
2026-01-05 12:52:54,568: t15.2023.10.13 val PER: 0.3654
2026-01-05 12:52:54,568: t15.2023.10.15 val PER: 0.3052
2026-01-05 12:52:54,568: t15.2023.10.20 val PER: 0.2919
2026-01-05 12:52:54,568: t15.2023.10.22 val PER: 0.2539
2026-01-05 12:52:54,568: t15.2023.11.03 val PER: 0.3263
2026-01-05 12:52:54,568: t15.2023.11.04 val PER: 0.0990
2026-01-05 12:52:54,568: t15.2023.11.17 val PER: 0.1742
2026-01-05 12:52:54,568: t15.2023.11.19 val PER: 0.1357
2026-01-05 12:52:54,568: t15.2023.11.26 val PER: 0.3681
2026-01-05 12:52:54,568: t15.2023.12.03 val PER: 0.3267
2026-01-05 12:52:54,568: t15.2023.12.08 val PER: 0.3142
2026-01-05 12:52:54,568: t15.2023.12.10 val PER: 0.2628
2026-01-05 12:52:54,569: t15.2023.12.17 val PER: 0.3150
2026-01-05 12:52:54,569: t15.2023.12.29 val PER: 0.3356
2026-01-05 12:52:54,569: t15.2024.02.25 val PER: 0.2725
2026-01-05 12:52:54,569: t15.2024.03.08 val PER: 0.4054
2026-01-05 12:52:54,569: t15.2024.03.15 val PER: 0.3552
2026-01-05 12:52:54,569: t15.2024.03.17 val PER: 0.3424
2026-01-05 12:52:54,569: t15.2024.05.10 val PER: 0.3373
2026-01-05 12:52:54,569: t15.2024.06.14 val PER: 0.3375
2026-01-05 12:52:54,569: t15.2024.07.19 val PER: 0.4654
2026-01-05 12:52:54,569: t15.2024.07.21 val PER: 0.2938
2026-01-05 12:52:54,569: t15.2024.07.28 val PER: 0.3250
2026-01-05 12:52:54,569: t15.2025.01.10 val PER: 0.5441
2026-01-05 12:52:54,569: t15.2025.01.12 val PER: 0.3849
2026-01-05 12:52:54,569: t15.2025.03.14 val PER: 0.5325
2026-01-05 12:52:54,569: t15.2025.03.16 val PER: 0.3874
2026-01-05 12:52:54,569: t15.2025.03.30 val PER: 0.5540
2026-01-05 12:52:54,570: t15.2025.04.13 val PER: 0.4108
2026-01-05 12:52:54,571: New best val WER(1gram) 75.63% --> 71.07%
2026-01-05 12:52:54,571: Checkpointing model
2026-01-05 12:52:55,225: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 12:52:55,513: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_2000
2026-01-05 12:53:14,016: Train batch 2200: loss: 29.04 grad norm: 73.72 time: 0.061
2026-01-05 12:53:32,695: Train batch 2400: loss: 28.66 grad norm: 62.28 time: 0.052
2026-01-05 12:53:42,631: Running test after training batch: 2500
2026-01-05 12:53:42,796: WER debug GT example: You can see the code at this point as well.
2026-01-05 12:53:47,903: WER debug example
  GT : you can see the code at this point as well
  PR : yule end e the could at this point is will
2026-01-05 12:53:47,932: WER debug example
  GT : how does it keep the cost down
  PR : houde does it keep the us it
2026-01-05 12:53:49,668: Val batch 2500: PER (avg): 0.3050 CTC Loss (avg): 30.1682 WER(1gram): 66.50% (n=64) time: 7.037
2026-01-05 12:53:49,668: WER lens: avg_true_words=6.16 avg_pred_words=5.47 max_pred_words=11
2026-01-05 12:53:49,669: t15.2023.08.13 val PER: 0.2942
2026-01-05 12:53:49,669: t15.2023.08.18 val PER: 0.2355
2026-01-05 12:53:49,669: t15.2023.08.20 val PER: 0.2407
2026-01-05 12:53:49,669: t15.2023.08.25 val PER: 0.2169
2026-01-05 12:53:49,669: t15.2023.08.27 val PER: 0.3296
2026-01-05 12:53:49,669: t15.2023.09.01 val PER: 0.2005
2026-01-05 12:53:49,669: t15.2023.09.03 val PER: 0.2850
2026-01-05 12:53:49,669: t15.2023.09.24 val PER: 0.2306
2026-01-05 12:53:49,669: t15.2023.09.29 val PER: 0.2591
2026-01-05 12:53:49,669: t15.2023.10.01 val PER: 0.3045
2026-01-05 12:53:49,670: t15.2023.10.06 val PER: 0.2153
2026-01-05 12:53:49,670: t15.2023.10.08 val PER: 0.3802
2026-01-05 12:53:49,670: t15.2023.10.13 val PER: 0.3623
2026-01-05 12:53:49,670: t15.2023.10.15 val PER: 0.2887
2026-01-05 12:53:49,670: t15.2023.10.20 val PER: 0.2685
2026-01-05 12:53:49,670: t15.2023.10.22 val PER: 0.2372
2026-01-05 12:53:49,670: t15.2023.11.03 val PER: 0.2978
2026-01-05 12:53:49,670: t15.2023.11.04 val PER: 0.0956
2026-01-05 12:53:49,670: t15.2023.11.17 val PER: 0.1462
2026-01-05 12:53:49,670: t15.2023.11.19 val PER: 0.1238
2026-01-05 12:53:49,670: t15.2023.11.26 val PER: 0.3471
2026-01-05 12:53:49,670: t15.2023.12.03 val PER: 0.2973
2026-01-05 12:53:49,670: t15.2023.12.08 val PER: 0.2750
2026-01-05 12:53:49,671: t15.2023.12.10 val PER: 0.2418
2026-01-05 12:53:49,671: t15.2023.12.17 val PER: 0.3025
2026-01-05 12:53:49,671: t15.2023.12.29 val PER: 0.3075
2026-01-05 12:53:49,671: t15.2024.02.25 val PER: 0.2444
2026-01-05 12:53:49,671: t15.2024.03.08 val PER: 0.3656
2026-01-05 12:53:49,671: t15.2024.03.15 val PER: 0.3502
2026-01-05 12:53:49,671: t15.2024.03.17 val PER: 0.3131
2026-01-05 12:53:49,671: t15.2024.05.10 val PER: 0.3031
2026-01-05 12:53:49,671: t15.2024.06.14 val PER: 0.3155
2026-01-05 12:53:49,671: t15.2024.07.19 val PER: 0.4410
2026-01-05 12:53:49,671: t15.2024.07.21 val PER: 0.2676
2026-01-05 12:53:49,671: t15.2024.07.28 val PER: 0.3059
2026-01-05 12:53:49,671: t15.2025.01.10 val PER: 0.5096
2026-01-05 12:53:49,671: t15.2025.01.12 val PER: 0.3610
2026-01-05 12:53:49,671: t15.2025.03.14 val PER: 0.4956
2026-01-05 12:53:49,671: t15.2025.03.16 val PER: 0.3547
2026-01-05 12:53:49,672: t15.2025.03.30 val PER: 0.4966
2026-01-05 12:53:49,672: t15.2025.04.13 val PER: 0.3880
2026-01-05 12:53:49,673: New best val WER(1gram) 71.07% --> 66.50%
2026-01-05 12:53:49,673: Checkpointing model
2026-01-05 12:53:50,320: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 12:53:50,610: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_2500
2026-01-05 12:53:59,716: Train batch 2600: loss: 34.94 grad norm: 85.91 time: 0.054
2026-01-05 12:54:18,069: Train batch 2800: loss: 25.60 grad norm: 69.48 time: 0.081
2026-01-05 12:54:36,970: Train batch 3000: loss: 31.51 grad norm: 70.83 time: 0.083
2026-01-05 12:54:36,970: Running test after training batch: 3000
2026-01-05 12:54:37,088: WER debug GT example: You can see the code at this point as well.
2026-01-05 12:54:42,250: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the good at this point is will
2026-01-05 12:54:42,279: WER debug example
  GT : how does it keep the cost down
  PR : houde des it yip the cost et
2026-01-05 12:54:43,975: Val batch 3000: PER (avg): 0.2796 CTC Loss (avg): 27.8529 WER(1gram): 65.99% (n=64) time: 7.004
2026-01-05 12:54:43,975: WER lens: avg_true_words=6.16 avg_pred_words=5.80 max_pred_words=11
2026-01-05 12:54:43,975: t15.2023.08.13 val PER: 0.2672
2026-01-05 12:54:43,975: t15.2023.08.18 val PER: 0.2221
2026-01-05 12:54:43,975: t15.2023.08.20 val PER: 0.2145
2026-01-05 12:54:43,975: t15.2023.08.25 val PER: 0.1837
2026-01-05 12:54:43,975: t15.2023.08.27 val PER: 0.3006
2026-01-05 12:54:43,976: t15.2023.09.01 val PER: 0.1810
2026-01-05 12:54:43,976: t15.2023.09.03 val PER: 0.2779
2026-01-05 12:54:43,976: t15.2023.09.24 val PER: 0.2063
2026-01-05 12:54:43,976: t15.2023.09.29 val PER: 0.2336
2026-01-05 12:54:43,976: t15.2023.10.01 val PER: 0.2893
2026-01-05 12:54:43,976: t15.2023.10.06 val PER: 0.1873
2026-01-05 12:54:43,976: t15.2023.10.08 val PER: 0.3491
2026-01-05 12:54:43,976: t15.2023.10.13 val PER: 0.3367
2026-01-05 12:54:43,976: t15.2023.10.15 val PER: 0.2650
2026-01-05 12:54:43,976: t15.2023.10.20 val PER: 0.2517
2026-01-05 12:54:43,976: t15.2023.10.22 val PER: 0.2149
2026-01-05 12:54:43,976: t15.2023.11.03 val PER: 0.2788
2026-01-05 12:54:43,976: t15.2023.11.04 val PER: 0.0614
2026-01-05 12:54:43,977: t15.2023.11.17 val PER: 0.1291
2026-01-05 12:54:43,977: t15.2023.11.19 val PER: 0.1257
2026-01-05 12:54:43,977: t15.2023.11.26 val PER: 0.3043
2026-01-05 12:54:43,977: t15.2023.12.03 val PER: 0.2532
2026-01-05 12:54:43,977: t15.2023.12.08 val PER: 0.2583
2026-01-05 12:54:43,977: t15.2023.12.10 val PER: 0.2116
2026-01-05 12:54:43,977: t15.2023.12.17 val PER: 0.2734
2026-01-05 12:54:43,977: t15.2023.12.29 val PER: 0.2938
2026-01-05 12:54:43,977: t15.2024.02.25 val PER: 0.2388
2026-01-05 12:54:43,977: t15.2024.03.08 val PER: 0.3642
2026-01-05 12:54:43,977: t15.2024.03.15 val PER: 0.3327
2026-01-05 12:54:43,977: t15.2024.03.17 val PER: 0.2810
2026-01-05 12:54:43,977: t15.2024.05.10 val PER: 0.2942
2026-01-05 12:54:43,977: t15.2024.06.14 val PER: 0.2981
2026-01-05 12:54:43,977: t15.2024.07.19 val PER: 0.4028
2026-01-05 12:54:43,977: t15.2024.07.21 val PER: 0.2276
2026-01-05 12:54:43,977: t15.2024.07.28 val PER: 0.2743
2026-01-05 12:54:43,978: t15.2025.01.10 val PER: 0.4848
2026-01-05 12:54:43,978: t15.2025.01.12 val PER: 0.3256
2026-01-05 12:54:43,978: t15.2025.03.14 val PER: 0.4349
2026-01-05 12:54:43,978: t15.2025.03.16 val PER: 0.3351
2026-01-05 12:54:43,978: t15.2025.03.30 val PER: 0.4563
2026-01-05 12:54:43,978: t15.2025.04.13 val PER: 0.3481
2026-01-05 12:54:43,979: New best val WER(1gram) 66.50% --> 65.99%
2026-01-05 12:54:43,979: Checkpointing model
2026-01-05 12:54:44,609: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 12:54:44,894: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_3000
2026-01-05 12:55:03,364: Train batch 3200: loss: 26.25 grad norm: 64.45 time: 0.076
2026-01-05 12:55:22,827: Train batch 3400: loss: 18.79 grad norm: 58.59 time: 0.049
2026-01-05 12:55:32,116: Running test after training batch: 3500
2026-01-05 12:55:32,264: WER debug GT example: You can see the code at this point as well.
2026-01-05 12:55:38,029: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point will
2026-01-05 12:55:38,058: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it kipp thus us get
2026-01-05 12:55:39,701: Val batch 3500: PER (avg): 0.2651 CTC Loss (avg): 26.4479 WER(1gram): 65.23% (n=64) time: 7.585
2026-01-05 12:55:39,702: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=11
2026-01-05 12:55:39,702: t15.2023.08.13 val PER: 0.2360
2026-01-05 12:55:39,702: t15.2023.08.18 val PER: 0.2112
2026-01-05 12:55:39,702: t15.2023.08.20 val PER: 0.2137
2026-01-05 12:55:39,702: t15.2023.08.25 val PER: 0.1777
2026-01-05 12:55:39,702: t15.2023.08.27 val PER: 0.2765
2026-01-05 12:55:39,702: t15.2023.09.01 val PER: 0.1729
2026-01-05 12:55:39,703: t15.2023.09.03 val PER: 0.2637
2026-01-05 12:55:39,703: t15.2023.09.24 val PER: 0.2015
2026-01-05 12:55:39,703: t15.2023.09.29 val PER: 0.2151
2026-01-05 12:55:39,703: t15.2023.10.01 val PER: 0.2741
2026-01-05 12:55:39,703: t15.2023.10.06 val PER: 0.1895
2026-01-05 12:55:39,703: t15.2023.10.08 val PER: 0.3356
2026-01-05 12:55:39,703: t15.2023.10.13 val PER: 0.3119
2026-01-05 12:55:39,703: t15.2023.10.15 val PER: 0.2360
2026-01-05 12:55:39,704: t15.2023.10.20 val PER: 0.2315
2026-01-05 12:55:39,704: t15.2023.10.22 val PER: 0.2171
2026-01-05 12:55:39,704: t15.2023.11.03 val PER: 0.2612
2026-01-05 12:55:39,704: t15.2023.11.04 val PER: 0.0853
2026-01-05 12:55:39,704: t15.2023.11.17 val PER: 0.1229
2026-01-05 12:55:39,704: t15.2023.11.19 val PER: 0.0978
2026-01-05 12:55:39,704: t15.2023.11.26 val PER: 0.2790
2026-01-05 12:55:39,704: t15.2023.12.03 val PER: 0.2353
2026-01-05 12:55:39,705: t15.2023.12.08 val PER: 0.2397
2026-01-05 12:55:39,705: t15.2023.12.10 val PER: 0.1958
2026-01-05 12:55:39,705: t15.2023.12.17 val PER: 0.2599
2026-01-05 12:55:39,705: t15.2023.12.29 val PER: 0.2629
2026-01-05 12:55:39,705: t15.2024.02.25 val PER: 0.2177
2026-01-05 12:55:39,705: t15.2024.03.08 val PER: 0.3357
2026-01-05 12:55:39,705: t15.2024.03.15 val PER: 0.3177
2026-01-05 12:55:39,705: t15.2024.03.17 val PER: 0.2657
2026-01-05 12:55:39,705: t15.2024.05.10 val PER: 0.2704
2026-01-05 12:55:39,705: t15.2024.06.14 val PER: 0.2823
2026-01-05 12:55:39,705: t15.2024.07.19 val PER: 0.4041
2026-01-05 12:55:39,706: t15.2024.07.21 val PER: 0.2152
2026-01-05 12:55:39,706: t15.2024.07.28 val PER: 0.2765
2026-01-05 12:55:39,706: t15.2025.01.10 val PER: 0.4601
2026-01-05 12:55:39,706: t15.2025.01.12 val PER: 0.2964
2026-01-05 12:55:39,706: t15.2025.03.14 val PER: 0.4453
2026-01-05 12:55:39,706: t15.2025.03.16 val PER: 0.3220
2026-01-05 12:55:39,706: t15.2025.03.30 val PER: 0.4586
2026-01-05 12:55:39,706: t15.2025.04.13 val PER: 0.3395
2026-01-05 12:55:39,707: New best val WER(1gram) 65.99% --> 65.23%
2026-01-05 12:55:39,707: Checkpointing model
2026-01-05 12:55:40,333: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 12:55:40,633: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_3500
2026-01-05 12:55:50,003: Train batch 3600: loss: 22.31 grad norm: 64.31 time: 0.066
2026-01-05 12:56:08,318: Train batch 3800: loss: 25.61 grad norm: 69.71 time: 0.066
2026-01-05 12:56:27,054: Train batch 4000: loss: 19.76 grad norm: 56.60 time: 0.057
2026-01-05 12:56:27,054: Running test after training batch: 4000
2026-01-05 12:56:27,220: WER debug GT example: You can see the code at this point as well.
2026-01-05 12:56:32,317: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-05 12:56:32,346: WER debug example
  GT : how does it keep the cost down
  PR : houde des it kipp thus cussed nit
2026-01-05 12:56:34,025: Val batch 4000: PER (avg): 0.2513 CTC Loss (avg): 24.4813 WER(1gram): 65.74% (n=64) time: 6.970
2026-01-05 12:56:34,025: WER lens: avg_true_words=6.16 avg_pred_words=6.02 max_pred_words=11
2026-01-05 12:56:34,025: t15.2023.08.13 val PER: 0.2245
2026-01-05 12:56:34,025: t15.2023.08.18 val PER: 0.2054
2026-01-05 12:56:34,025: t15.2023.08.20 val PER: 0.2057
2026-01-05 12:56:34,026: t15.2023.08.25 val PER: 0.1687
2026-01-05 12:56:34,026: t15.2023.08.27 val PER: 0.2701
2026-01-05 12:56:34,026: t15.2023.09.01 val PER: 0.1672
2026-01-05 12:56:34,026: t15.2023.09.03 val PER: 0.2482
2026-01-05 12:56:34,026: t15.2023.09.24 val PER: 0.1930
2026-01-05 12:56:34,026: t15.2023.09.29 val PER: 0.2036
2026-01-05 12:56:34,026: t15.2023.10.01 val PER: 0.2609
2026-01-05 12:56:34,026: t15.2023.10.06 val PER: 0.1701
2026-01-05 12:56:34,026: t15.2023.10.08 val PER: 0.3369
2026-01-05 12:56:34,026: t15.2023.10.13 val PER: 0.2948
2026-01-05 12:56:34,026: t15.2023.10.15 val PER: 0.2393
2026-01-05 12:56:34,026: t15.2023.10.20 val PER: 0.2483
2026-01-05 12:56:34,026: t15.2023.10.22 val PER: 0.2071
2026-01-05 12:56:34,027: t15.2023.11.03 val PER: 0.2429
2026-01-05 12:56:34,027: t15.2023.11.04 val PER: 0.0785
2026-01-05 12:56:34,027: t15.2023.11.17 val PER: 0.1026
2026-01-05 12:56:34,027: t15.2023.11.19 val PER: 0.0998
2026-01-05 12:56:34,027: t15.2023.11.26 val PER: 0.2645
2026-01-05 12:56:34,027: t15.2023.12.03 val PER: 0.2174
2026-01-05 12:56:34,027: t15.2023.12.08 val PER: 0.2184
2026-01-05 12:56:34,027: t15.2023.12.10 val PER: 0.1879
2026-01-05 12:56:34,027: t15.2023.12.17 val PER: 0.2474
2026-01-05 12:56:34,027: t15.2023.12.29 val PER: 0.2622
2026-01-05 12:56:34,027: t15.2024.02.25 val PER: 0.2093
2026-01-05 12:56:34,028: t15.2024.03.08 val PER: 0.3300
2026-01-05 12:56:34,028: t15.2024.03.15 val PER: 0.2983
2026-01-05 12:56:34,028: t15.2024.03.17 val PER: 0.2545
2026-01-05 12:56:34,028: t15.2024.05.10 val PER: 0.2793
2026-01-05 12:56:34,028: t15.2024.06.14 val PER: 0.2760
2026-01-05 12:56:34,028: t15.2024.07.19 val PER: 0.3606
2026-01-05 12:56:34,028: t15.2024.07.21 val PER: 0.1959
2026-01-05 12:56:34,028: t15.2024.07.28 val PER: 0.2493
2026-01-05 12:56:34,028: t15.2025.01.10 val PER: 0.4146
2026-01-05 12:56:34,028: t15.2025.01.12 val PER: 0.2871
2026-01-05 12:56:34,028: t15.2025.03.14 val PER: 0.4157
2026-01-05 12:56:34,028: t15.2025.03.16 val PER: 0.3168
2026-01-05 12:56:34,028: t15.2025.03.30 val PER: 0.4195
2026-01-05 12:56:34,028: t15.2025.04.13 val PER: 0.3210
2026-01-05 12:56:34,297: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_4000
2026-01-05 12:56:52,963: Train batch 4200: loss: 22.74 grad norm: 63.90 time: 0.079
2026-01-05 12:57:11,684: Train batch 4400: loss: 16.99 grad norm: 51.85 time: 0.066
2026-01-05 12:57:20,964: Running test after training batch: 4500
2026-01-05 12:57:21,111: WER debug GT example: You can see the code at this point as well.
2026-01-05 12:57:26,279: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point us will
2026-01-05 12:57:26,309: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cost get
2026-01-05 12:57:27,948: Val batch 4500: PER (avg): 0.2373 CTC Loss (avg): 23.0976 WER(1gram): 62.44% (n=64) time: 6.984
2026-01-05 12:57:27,948: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=11
2026-01-05 12:57:27,948: t15.2023.08.13 val PER: 0.2079
2026-01-05 12:57:27,949: t15.2023.08.18 val PER: 0.1911
2026-01-05 12:57:27,949: t15.2023.08.20 val PER: 0.1986
2026-01-05 12:57:27,949: t15.2023.08.25 val PER: 0.1461
2026-01-05 12:57:27,949: t15.2023.08.27 val PER: 0.2588
2026-01-05 12:57:27,949: t15.2023.09.01 val PER: 0.1615
2026-01-05 12:57:27,949: t15.2023.09.03 val PER: 0.2268
2026-01-05 12:57:27,949: t15.2023.09.24 val PER: 0.1820
2026-01-05 12:57:27,949: t15.2023.09.29 val PER: 0.1966
2026-01-05 12:57:27,949: t15.2023.10.01 val PER: 0.2543
2026-01-05 12:57:27,949: t15.2023.10.06 val PER: 0.1539
2026-01-05 12:57:27,949: t15.2023.10.08 val PER: 0.3031
2026-01-05 12:57:27,949: t15.2023.10.13 val PER: 0.2995
2026-01-05 12:57:27,949: t15.2023.10.15 val PER: 0.2334
2026-01-05 12:57:27,949: t15.2023.10.20 val PER: 0.2349
2026-01-05 12:57:27,950: t15.2023.10.22 val PER: 0.1915
2026-01-05 12:57:27,950: t15.2023.11.03 val PER: 0.2497
2026-01-05 12:57:27,950: t15.2023.11.04 val PER: 0.0785
2026-01-05 12:57:27,950: t15.2023.11.17 val PER: 0.0980
2026-01-05 12:57:27,950: t15.2023.11.19 val PER: 0.0838
2026-01-05 12:57:27,950: t15.2023.11.26 val PER: 0.2601
2026-01-05 12:57:27,950: t15.2023.12.03 val PER: 0.1996
2026-01-05 12:57:27,950: t15.2023.12.08 val PER: 0.2017
2026-01-05 12:57:27,950: t15.2023.12.10 val PER: 0.1695
2026-01-05 12:57:27,950: t15.2023.12.17 val PER: 0.2266
2026-01-05 12:57:27,951: t15.2023.12.29 val PER: 0.2361
2026-01-05 12:57:27,951: t15.2024.02.25 val PER: 0.1966
2026-01-05 12:57:27,951: t15.2024.03.08 val PER: 0.3186
2026-01-05 12:57:27,951: t15.2024.03.15 val PER: 0.2889
2026-01-05 12:57:27,951: t15.2024.03.17 val PER: 0.2420
2026-01-05 12:57:27,951: t15.2024.05.10 val PER: 0.2541
2026-01-05 12:57:27,951: t15.2024.06.14 val PER: 0.2445
2026-01-05 12:57:27,951: t15.2024.07.19 val PER: 0.3382
2026-01-05 12:57:27,951: t15.2024.07.21 val PER: 0.1738
2026-01-05 12:57:27,951: t15.2024.07.28 val PER: 0.2206
2026-01-05 12:57:27,951: t15.2025.01.10 val PER: 0.4132
2026-01-05 12:57:27,952: t15.2025.01.12 val PER: 0.2640
2026-01-05 12:57:27,952: t15.2025.03.14 val PER: 0.4038
2026-01-05 12:57:27,952: t15.2025.03.16 val PER: 0.2893
2026-01-05 12:57:27,952: t15.2025.03.30 val PER: 0.4080
2026-01-05 12:57:27,952: t15.2025.04.13 val PER: 0.2981
2026-01-05 12:57:27,952: New best val WER(1gram) 65.23% --> 62.44%
2026-01-05 12:57:27,952: Checkpointing model
2026-01-05 12:57:28,614: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 12:57:28,892: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_4500
2026-01-05 12:57:38,469: Train batch 4600: loss: 19.95 grad norm: 62.30 time: 0.063
2026-01-05 12:57:57,381: Train batch 4800: loss: 13.79 grad norm: 52.59 time: 0.063
2026-01-05 12:58:15,937: Train batch 5000: loss: 31.16 grad norm: 83.41 time: 0.064
2026-01-05 12:58:15,937: Running test after training batch: 5000
2026-01-05 12:58:16,088: WER debug GT example: You can see the code at this point as well.
2026-01-05 12:58:21,152: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-05 12:58:21,181: WER debug example
  GT : how does it keep the cost down
  PR : aue des it heap the cost nett
2026-01-05 12:58:22,826: Val batch 5000: PER (avg): 0.2244 CTC Loss (avg): 21.8732 WER(1gram): 61.17% (n=64) time: 6.889
2026-01-05 12:58:22,827: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-05 12:58:22,827: t15.2023.08.13 val PER: 0.1892
2026-01-05 12:58:22,827: t15.2023.08.18 val PER: 0.1727
2026-01-05 12:58:22,827: t15.2023.08.20 val PER: 0.1747
2026-01-05 12:58:22,827: t15.2023.08.25 val PER: 0.1325
2026-01-05 12:58:22,827: t15.2023.08.27 val PER: 0.2347
2026-01-05 12:58:22,827: t15.2023.09.01 val PER: 0.1404
2026-01-05 12:58:22,827: t15.2023.09.03 val PER: 0.2316
2026-01-05 12:58:22,827: t15.2023.09.24 val PER: 0.1845
2026-01-05 12:58:22,827: t15.2023.09.29 val PER: 0.1806
2026-01-05 12:58:22,827: t15.2023.10.01 val PER: 0.2437
2026-01-05 12:58:22,827: t15.2023.10.06 val PER: 0.1410
2026-01-05 12:58:22,828: t15.2023.10.08 val PER: 0.3112
2026-01-05 12:58:22,828: t15.2023.10.13 val PER: 0.2785
2026-01-05 12:58:22,828: t15.2023.10.15 val PER: 0.2228
2026-01-05 12:58:22,828: t15.2023.10.20 val PER: 0.2383
2026-01-05 12:58:22,828: t15.2023.10.22 val PER: 0.1748
2026-01-05 12:58:22,828: t15.2023.11.03 val PER: 0.2212
2026-01-05 12:58:22,828: t15.2023.11.04 val PER: 0.0410
2026-01-05 12:58:22,828: t15.2023.11.17 val PER: 0.0855
2026-01-05 12:58:22,828: t15.2023.11.19 val PER: 0.0639
2026-01-05 12:58:22,828: t15.2023.11.26 val PER: 0.2333
2026-01-05 12:58:22,828: t15.2023.12.03 val PER: 0.2048
2026-01-05 12:58:22,828: t15.2023.12.08 val PER: 0.1871
2026-01-05 12:58:22,828: t15.2023.12.10 val PER: 0.1656
2026-01-05 12:58:22,828: t15.2023.12.17 val PER: 0.2245
2026-01-05 12:58:22,829: t15.2023.12.29 val PER: 0.2203
2026-01-05 12:58:22,829: t15.2024.02.25 val PER: 0.1812
2026-01-05 12:58:22,829: t15.2024.03.08 val PER: 0.3229
2026-01-05 12:58:22,829: t15.2024.03.15 val PER: 0.2733
2026-01-05 12:58:22,829: t15.2024.03.17 val PER: 0.2385
2026-01-05 12:58:22,829: t15.2024.05.10 val PER: 0.2288
2026-01-05 12:58:22,829: t15.2024.06.14 val PER: 0.2413
2026-01-05 12:58:22,829: t15.2024.07.19 val PER: 0.3296
2026-01-05 12:58:22,829: t15.2024.07.21 val PER: 0.1800
2026-01-05 12:58:22,829: t15.2024.07.28 val PER: 0.2096
2026-01-05 12:58:22,829: t15.2025.01.10 val PER: 0.3953
2026-01-05 12:58:22,829: t15.2025.01.12 val PER: 0.2371
2026-01-05 12:58:22,829: t15.2025.03.14 val PER: 0.3861
2026-01-05 12:58:22,829: t15.2025.03.16 val PER: 0.2526
2026-01-05 12:58:22,829: t15.2025.03.30 val PER: 0.3989
2026-01-05 12:58:22,829: t15.2025.04.13 val PER: 0.3096
2026-01-05 12:58:22,831: New best val WER(1gram) 62.44% --> 61.17%
2026-01-05 12:58:22,831: Checkpointing model
2026-01-05 12:58:23,510: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 12:58:23,808: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_5000
2026-01-05 12:58:42,442: Train batch 5200: loss: 16.47 grad norm: 58.25 time: 0.052
2026-01-05 12:59:01,370: Train batch 5400: loss: 17.50 grad norm: 60.37 time: 0.067
2026-01-05 12:59:11,503: Running test after training batch: 5500
2026-01-05 12:59:11,616: WER debug GT example: You can see the code at this point as well.
2026-01-05 12:59:16,736: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-05 12:59:16,766: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost tet
2026-01-05 12:59:18,417: Val batch 5500: PER (avg): 0.2155 CTC Loss (avg): 21.1259 WER(1gram): 53.81% (n=64) time: 6.913
2026-01-05 12:59:18,417: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-05 12:59:18,417: t15.2023.08.13 val PER: 0.1798
2026-01-05 12:59:18,417: t15.2023.08.18 val PER: 0.1601
2026-01-05 12:59:18,418: t15.2023.08.20 val PER: 0.1724
2026-01-05 12:59:18,418: t15.2023.08.25 val PER: 0.1175
2026-01-05 12:59:18,418: t15.2023.08.27 val PER: 0.2508
2026-01-05 12:59:18,418: t15.2023.09.01 val PER: 0.1282
2026-01-05 12:59:18,418: t15.2023.09.03 val PER: 0.2173
2026-01-05 12:59:18,418: t15.2023.09.24 val PER: 0.1723
2026-01-05 12:59:18,418: t15.2023.09.29 val PER: 0.1704
2026-01-05 12:59:18,418: t15.2023.10.01 val PER: 0.2259
2026-01-05 12:59:18,418: t15.2023.10.06 val PER: 0.1389
2026-01-05 12:59:18,418: t15.2023.10.08 val PER: 0.2963
2026-01-05 12:59:18,419: t15.2023.10.13 val PER: 0.2684
2026-01-05 12:59:18,419: t15.2023.10.15 val PER: 0.2096
2026-01-05 12:59:18,419: t15.2023.10.20 val PER: 0.2349
2026-01-05 12:59:18,419: t15.2023.10.22 val PER: 0.1537
2026-01-05 12:59:18,419: t15.2023.11.03 val PER: 0.2266
2026-01-05 12:59:18,419: t15.2023.11.04 val PER: 0.0648
2026-01-05 12:59:18,419: t15.2023.11.17 val PER: 0.0824
2026-01-05 12:59:18,419: t15.2023.11.19 val PER: 0.0798
2026-01-05 12:59:18,419: t15.2023.11.26 val PER: 0.2167
2026-01-05 12:59:18,420: t15.2023.12.03 val PER: 0.1849
2026-01-05 12:59:18,420: t15.2023.12.08 val PER: 0.1838
2026-01-05 12:59:18,420: t15.2023.12.10 val PER: 0.1564
2026-01-05 12:59:18,420: t15.2023.12.17 val PER: 0.2058
2026-01-05 12:59:18,420: t15.2023.12.29 val PER: 0.2135
2026-01-05 12:59:18,420: t15.2024.02.25 val PER: 0.1713
2026-01-05 12:59:18,420: t15.2024.03.08 val PER: 0.3030
2026-01-05 12:59:18,420: t15.2024.03.15 val PER: 0.2677
2026-01-05 12:59:18,420: t15.2024.03.17 val PER: 0.2085
2026-01-05 12:59:18,421: t15.2024.05.10 val PER: 0.2244
2026-01-05 12:59:18,421: t15.2024.06.14 val PER: 0.2350
2026-01-05 12:59:18,421: t15.2024.07.19 val PER: 0.3250
2026-01-05 12:59:18,421: t15.2024.07.21 val PER: 0.1683
2026-01-05 12:59:18,421: t15.2024.07.28 val PER: 0.2191
2026-01-05 12:59:18,421: t15.2025.01.10 val PER: 0.3953
2026-01-05 12:59:18,421: t15.2025.01.12 val PER: 0.2386
2026-01-05 12:59:18,421: t15.2025.03.14 val PER: 0.3595
2026-01-05 12:59:18,421: t15.2025.03.16 val PER: 0.2827
2026-01-05 12:59:18,421: t15.2025.03.30 val PER: 0.3609
2026-01-05 12:59:18,421: t15.2025.04.13 val PER: 0.2867
2026-01-05 12:59:18,422: New best val WER(1gram) 61.17% --> 53.81%
2026-01-05 12:59:18,422: Checkpointing model
2026-01-05 12:59:19,090: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 12:59:19,373: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_5500
2026-01-05 12:59:28,595: Train batch 5600: loss: 19.35 grad norm: 63.98 time: 0.062
2026-01-05 12:59:47,365: Train batch 5800: loss: 13.78 grad norm: 56.05 time: 0.082
2026-01-05 13:00:06,162: Train batch 6000: loss: 14.57 grad norm: 56.82 time: 0.049
2026-01-05 13:00:06,162: Running test after training batch: 6000
2026-01-05 13:00:06,372: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:00:11,494: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point will
2026-01-05 13:00:11,526: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the rost get
2026-01-05 13:00:13,193: Val batch 6000: PER (avg): 0.2121 CTC Loss (avg): 20.7527 WER(1gram): 59.64% (n=64) time: 7.030
2026-01-05 13:00:13,193: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-05 13:00:13,193: t15.2023.08.13 val PER: 0.1757
2026-01-05 13:00:13,193: t15.2023.08.18 val PER: 0.1626
2026-01-05 13:00:13,194: t15.2023.08.20 val PER: 0.1652
2026-01-05 13:00:13,194: t15.2023.08.25 val PER: 0.1145
2026-01-05 13:00:13,194: t15.2023.08.27 val PER: 0.2299
2026-01-05 13:00:13,194: t15.2023.09.01 val PER: 0.1323
2026-01-05 13:00:13,194: t15.2023.09.03 val PER: 0.2090
2026-01-05 13:00:13,194: t15.2023.09.24 val PER: 0.1675
2026-01-05 13:00:13,194: t15.2023.09.29 val PER: 0.1704
2026-01-05 13:00:13,194: t15.2023.10.01 val PER: 0.2186
2026-01-05 13:00:13,195: t15.2023.10.06 val PER: 0.1324
2026-01-05 13:00:13,195: t15.2023.10.08 val PER: 0.2896
2026-01-05 13:00:13,195: t15.2023.10.13 val PER: 0.2731
2026-01-05 13:00:13,195: t15.2023.10.15 val PER: 0.2248
2026-01-05 13:00:13,195: t15.2023.10.20 val PER: 0.2047
2026-01-05 13:00:13,195: t15.2023.10.22 val PER: 0.1759
2026-01-05 13:00:13,195: t15.2023.11.03 val PER: 0.2313
2026-01-05 13:00:13,195: t15.2023.11.04 val PER: 0.0546
2026-01-05 13:00:13,195: t15.2023.11.17 val PER: 0.0871
2026-01-05 13:00:13,195: t15.2023.11.19 val PER: 0.0898
2026-01-05 13:00:13,195: t15.2023.11.26 val PER: 0.2217
2026-01-05 13:00:13,196: t15.2023.12.03 val PER: 0.1733
2026-01-05 13:00:13,196: t15.2023.12.08 val PER: 0.1784
2026-01-05 13:00:13,196: t15.2023.12.10 val PER: 0.1498
2026-01-05 13:00:13,196: t15.2023.12.17 val PER: 0.2027
2026-01-05 13:00:13,196: t15.2023.12.29 val PER: 0.2237
2026-01-05 13:00:13,196: t15.2024.02.25 val PER: 0.1657
2026-01-05 13:00:13,196: t15.2024.03.08 val PER: 0.2902
2026-01-05 13:00:13,196: t15.2024.03.15 val PER: 0.2595
2026-01-05 13:00:13,196: t15.2024.03.17 val PER: 0.2141
2026-01-05 13:00:13,196: t15.2024.05.10 val PER: 0.2140
2026-01-05 13:00:13,196: t15.2024.06.14 val PER: 0.2256
2026-01-05 13:00:13,196: t15.2024.07.19 val PER: 0.3052
2026-01-05 13:00:13,196: t15.2024.07.21 val PER: 0.1683
2026-01-05 13:00:13,197: t15.2024.07.28 val PER: 0.1949
2026-01-05 13:00:13,197: t15.2025.01.10 val PER: 0.3857
2026-01-05 13:00:13,197: t15.2025.01.12 val PER: 0.2225
2026-01-05 13:00:13,197: t15.2025.03.14 val PER: 0.3787
2026-01-05 13:00:13,197: t15.2025.03.16 val PER: 0.2644
2026-01-05 13:00:13,197: t15.2025.03.30 val PER: 0.3701
2026-01-05 13:00:13,197: t15.2025.04.13 val PER: 0.2596
2026-01-05 13:00:13,467: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_6000
2026-01-05 13:00:32,050: Train batch 6200: loss: 16.84 grad norm: 60.26 time: 0.070
2026-01-05 13:00:50,607: Train batch 6400: loss: 18.70 grad norm: 65.88 time: 0.062
2026-01-05 13:00:59,547: Running test after training batch: 6500
2026-01-05 13:00:59,699: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:01:04,748: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point as will
2026-01-05 13:01:04,778: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 13:01:06,394: Val batch 6500: PER (avg): 0.2037 CTC Loss (avg): 20.0630 WER(1gram): 53.55% (n=64) time: 6.847
2026-01-05 13:01:06,395: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 13:01:06,395: t15.2023.08.13 val PER: 0.1663
2026-01-05 13:01:06,395: t15.2023.08.18 val PER: 0.1417
2026-01-05 13:01:06,395: t15.2023.08.20 val PER: 0.1612
2026-01-05 13:01:06,395: t15.2023.08.25 val PER: 0.1175
2026-01-05 13:01:06,395: t15.2023.08.27 val PER: 0.2251
2026-01-05 13:01:06,395: t15.2023.09.01 val PER: 0.1128
2026-01-05 13:01:06,396: t15.2023.09.03 val PER: 0.2043
2026-01-05 13:01:06,396: t15.2023.09.24 val PER: 0.1626
2026-01-05 13:01:06,396: t15.2023.09.29 val PER: 0.1678
2026-01-05 13:01:06,396: t15.2023.10.01 val PER: 0.2213
2026-01-05 13:01:06,396: t15.2023.10.06 val PER: 0.1281
2026-01-05 13:01:06,396: t15.2023.10.08 val PER: 0.2950
2026-01-05 13:01:06,396: t15.2023.10.13 val PER: 0.2645
2026-01-05 13:01:06,396: t15.2023.10.15 val PER: 0.2063
2026-01-05 13:01:06,396: t15.2023.10.20 val PER: 0.2047
2026-01-05 13:01:06,396: t15.2023.10.22 val PER: 0.1537
2026-01-05 13:01:06,396: t15.2023.11.03 val PER: 0.2198
2026-01-05 13:01:06,396: t15.2023.11.04 val PER: 0.0614
2026-01-05 13:01:06,397: t15.2023.11.17 val PER: 0.0622
2026-01-05 13:01:06,397: t15.2023.11.19 val PER: 0.0659
2026-01-05 13:01:06,397: t15.2023.11.26 val PER: 0.2101
2026-01-05 13:01:06,397: t15.2023.12.03 val PER: 0.1744
2026-01-05 13:01:06,397: t15.2023.12.08 val PER: 0.1751
2026-01-05 13:01:06,397: t15.2023.12.10 val PER: 0.1406
2026-01-05 13:01:06,397: t15.2023.12.17 val PER: 0.1798
2026-01-05 13:01:06,397: t15.2023.12.29 val PER: 0.2059
2026-01-05 13:01:06,397: t15.2024.02.25 val PER: 0.1728
2026-01-05 13:01:06,397: t15.2024.03.08 val PER: 0.2859
2026-01-05 13:01:06,397: t15.2024.03.15 val PER: 0.2633
2026-01-05 13:01:06,397: t15.2024.03.17 val PER: 0.2078
2026-01-05 13:01:06,398: t15.2024.05.10 val PER: 0.2110
2026-01-05 13:01:06,398: t15.2024.06.14 val PER: 0.2177
2026-01-05 13:01:06,398: t15.2024.07.19 val PER: 0.3078
2026-01-05 13:01:06,398: t15.2024.07.21 val PER: 0.1490
2026-01-05 13:01:06,398: t15.2024.07.28 val PER: 0.1963
2026-01-05 13:01:06,398: t15.2025.01.10 val PER: 0.3609
2026-01-05 13:01:06,398: t15.2025.01.12 val PER: 0.2102
2026-01-05 13:01:06,398: t15.2025.03.14 val PER: 0.3698
2026-01-05 13:01:06,398: t15.2025.03.16 val PER: 0.2408
2026-01-05 13:01:06,398: t15.2025.03.30 val PER: 0.3460
2026-01-05 13:01:06,398: t15.2025.04.13 val PER: 0.2710
2026-01-05 13:01:06,399: New best val WER(1gram) 53.81% --> 53.55%
2026-01-05 13:01:06,399: Checkpointing model
2026-01-05 13:01:07,056: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:01:07,339: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_6500
2026-01-05 13:01:16,416: Train batch 6600: loss: 12.66 grad norm: 53.84 time: 0.045
2026-01-05 13:01:35,162: Train batch 6800: loss: 15.19 grad norm: 55.40 time: 0.050
2026-01-05 13:01:53,755: Train batch 7000: loss: 17.29 grad norm: 63.39 time: 0.061
2026-01-05 13:01:53,755: Running test after training batch: 7000
2026-01-05 13:01:53,867: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:01:59,022: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-05 13:01:59,053: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-05 13:02:00,746: Val batch 7000: PER (avg): 0.1950 CTC Loss (avg): 19.1366 WER(1gram): 53.55% (n=64) time: 6.990
2026-01-05 13:02:00,746: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-05 13:02:00,747: t15.2023.08.13 val PER: 0.1486
2026-01-05 13:02:00,747: t15.2023.08.18 val PER: 0.1425
2026-01-05 13:02:00,747: t15.2023.08.20 val PER: 0.1549
2026-01-05 13:02:00,747: t15.2023.08.25 val PER: 0.1054
2026-01-05 13:02:00,747: t15.2023.08.27 val PER: 0.2154
2026-01-05 13:02:00,747: t15.2023.09.01 val PER: 0.1104
2026-01-05 13:02:00,747: t15.2023.09.03 val PER: 0.1876
2026-01-05 13:02:00,747: t15.2023.09.24 val PER: 0.1626
2026-01-05 13:02:00,748: t15.2023.09.29 val PER: 0.1723
2026-01-05 13:02:00,748: t15.2023.10.01 val PER: 0.2153
2026-01-05 13:02:00,748: t15.2023.10.06 val PER: 0.1044
2026-01-05 13:02:00,748: t15.2023.10.08 val PER: 0.2747
2026-01-05 13:02:00,748: t15.2023.10.13 val PER: 0.2552
2026-01-05 13:02:00,748: t15.2023.10.15 val PER: 0.1964
2026-01-05 13:02:00,748: t15.2023.10.20 val PER: 0.2013
2026-01-05 13:02:00,748: t15.2023.10.22 val PER: 0.1503
2026-01-05 13:02:00,748: t15.2023.11.03 val PER: 0.2056
2026-01-05 13:02:00,748: t15.2023.11.04 val PER: 0.0444
2026-01-05 13:02:00,748: t15.2023.11.17 val PER: 0.0669
2026-01-05 13:02:00,748: t15.2023.11.19 val PER: 0.0579
2026-01-05 13:02:00,749: t15.2023.11.26 val PER: 0.1964
2026-01-05 13:02:00,749: t15.2023.12.03 val PER: 0.1670
2026-01-05 13:02:00,749: t15.2023.12.08 val PER: 0.1585
2026-01-05 13:02:00,749: t15.2023.12.10 val PER: 0.1406
2026-01-05 13:02:00,749: t15.2023.12.17 val PER: 0.1830
2026-01-05 13:02:00,749: t15.2023.12.29 val PER: 0.1956
2026-01-05 13:02:00,749: t15.2024.02.25 val PER: 0.1587
2026-01-05 13:02:00,749: t15.2024.03.08 val PER: 0.2760
2026-01-05 13:02:00,749: t15.2024.03.15 val PER: 0.2495
2026-01-05 13:02:00,749: t15.2024.03.17 val PER: 0.1911
2026-01-05 13:02:00,749: t15.2024.05.10 val PER: 0.1947
2026-01-05 13:02:00,749: t15.2024.06.14 val PER: 0.1972
2026-01-05 13:02:00,749: t15.2024.07.19 val PER: 0.3032
2026-01-05 13:02:00,749: t15.2024.07.21 val PER: 0.1414
2026-01-05 13:02:00,749: t15.2024.07.28 val PER: 0.1735
2026-01-05 13:02:00,750: t15.2025.01.10 val PER: 0.3595
2026-01-05 13:02:00,750: t15.2025.01.12 val PER: 0.1978
2026-01-05 13:02:00,750: t15.2025.03.14 val PER: 0.3447
2026-01-05 13:02:00,750: t15.2025.03.16 val PER: 0.2474
2026-01-05 13:02:00,750: t15.2025.03.30 val PER: 0.3655
2026-01-05 13:02:00,750: t15.2025.04.13 val PER: 0.2782
2026-01-05 13:02:01,019: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_7000
2026-01-05 13:02:20,294: Train batch 7200: loss: 14.75 grad norm: 62.00 time: 0.078
2026-01-05 13:02:38,631: Train batch 7400: loss: 13.44 grad norm: 57.25 time: 0.075
2026-01-05 13:02:47,884: Running test after training batch: 7500
2026-01-05 13:02:48,108: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:02:53,354: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-05 13:02:53,384: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-05 13:02:55,075: Val batch 7500: PER (avg): 0.1896 CTC Loss (avg): 18.6913 WER(1gram): 52.54% (n=64) time: 7.190
2026-01-05 13:02:55,075: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 13:02:55,075: t15.2023.08.13 val PER: 0.1486
2026-01-05 13:02:55,075: t15.2023.08.18 val PER: 0.1400
2026-01-05 13:02:55,075: t15.2023.08.20 val PER: 0.1533
2026-01-05 13:02:55,076: t15.2023.08.25 val PER: 0.0964
2026-01-05 13:02:55,076: t15.2023.08.27 val PER: 0.2122
2026-01-05 13:02:55,076: t15.2023.09.01 val PER: 0.1201
2026-01-05 13:02:55,076: t15.2023.09.03 val PER: 0.1853
2026-01-05 13:02:55,076: t15.2023.09.24 val PER: 0.1566
2026-01-05 13:02:55,076: t15.2023.09.29 val PER: 0.1589
2026-01-05 13:02:55,076: t15.2023.10.01 val PER: 0.1995
2026-01-05 13:02:55,076: t15.2023.10.06 val PER: 0.1109
2026-01-05 13:02:55,076: t15.2023.10.08 val PER: 0.2706
2026-01-05 13:02:55,077: t15.2023.10.13 val PER: 0.2452
2026-01-05 13:02:55,077: t15.2023.10.15 val PER: 0.1912
2026-01-05 13:02:55,077: t15.2023.10.20 val PER: 0.2047
2026-01-05 13:02:55,077: t15.2023.10.22 val PER: 0.1359
2026-01-05 13:02:55,077: t15.2023.11.03 val PER: 0.2049
2026-01-05 13:02:55,077: t15.2023.11.04 val PER: 0.0546
2026-01-05 13:02:55,077: t15.2023.11.17 val PER: 0.0747
2026-01-05 13:02:55,077: t15.2023.11.19 val PER: 0.0559
2026-01-05 13:02:55,077: t15.2023.11.26 val PER: 0.1870
2026-01-05 13:02:55,077: t15.2023.12.03 val PER: 0.1639
2026-01-05 13:02:55,077: t15.2023.12.08 val PER: 0.1385
2026-01-05 13:02:55,077: t15.2023.12.10 val PER: 0.1288
2026-01-05 13:02:55,077: t15.2023.12.17 val PER: 0.1798
2026-01-05 13:02:55,077: t15.2023.12.29 val PER: 0.1874
2026-01-05 13:02:55,077: t15.2024.02.25 val PER: 0.1559
2026-01-05 13:02:55,077: t15.2024.03.08 val PER: 0.2817
2026-01-05 13:02:55,077: t15.2024.03.15 val PER: 0.2464
2026-01-05 13:02:55,078: t15.2024.03.17 val PER: 0.1778
2026-01-05 13:02:55,078: t15.2024.05.10 val PER: 0.1932
2026-01-05 13:02:55,078: t15.2024.06.14 val PER: 0.2098
2026-01-05 13:02:55,078: t15.2024.07.19 val PER: 0.2914
2026-01-05 13:02:55,078: t15.2024.07.21 val PER: 0.1400
2026-01-05 13:02:55,078: t15.2024.07.28 val PER: 0.1750
2026-01-05 13:02:55,078: t15.2025.01.10 val PER: 0.3526
2026-01-05 13:02:55,078: t15.2025.01.12 val PER: 0.1894
2026-01-05 13:02:55,079: t15.2025.03.14 val PER: 0.3683
2026-01-05 13:02:55,079: t15.2025.03.16 val PER: 0.2330
2026-01-05 13:02:55,079: t15.2025.03.30 val PER: 0.3437
2026-01-05 13:02:55,079: t15.2025.04.13 val PER: 0.2582
2026-01-05 13:02:55,080: New best val WER(1gram) 53.55% --> 52.54%
2026-01-05 13:02:55,080: Checkpointing model
2026-01-05 13:02:55,761: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:02:56,047: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_7500
2026-01-05 13:03:05,357: Train batch 7600: loss: 16.03 grad norm: 59.58 time: 0.070
2026-01-05 13:03:23,953: Train batch 7800: loss: 14.47 grad norm: 58.14 time: 0.055
2026-01-05 13:03:42,975: Train batch 8000: loss: 11.53 grad norm: 50.99 time: 0.073
2026-01-05 13:03:42,976: Running test after training batch: 8000
2026-01-05 13:03:43,091: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:03:48,193: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-05 13:03:48,225: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost id
2026-01-05 13:03:49,962: Val batch 8000: PER (avg): 0.1854 CTC Loss (avg): 18.1472 WER(1gram): 52.79% (n=64) time: 6.986
2026-01-05 13:03:49,962: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-05 13:03:49,962: t15.2023.08.13 val PER: 0.1393
2026-01-05 13:03:49,962: t15.2023.08.18 val PER: 0.1316
2026-01-05 13:03:49,962: t15.2023.08.20 val PER: 0.1454
2026-01-05 13:03:49,962: t15.2023.08.25 val PER: 0.1069
2026-01-05 13:03:49,962: t15.2023.08.27 val PER: 0.2090
2026-01-05 13:03:49,963: t15.2023.09.01 val PER: 0.1023
2026-01-05 13:03:49,963: t15.2023.09.03 val PER: 0.1876
2026-01-05 13:03:49,963: t15.2023.09.24 val PER: 0.1517
2026-01-05 13:03:49,963: t15.2023.09.29 val PER: 0.1538
2026-01-05 13:03:49,963: t15.2023.10.01 val PER: 0.2008
2026-01-05 13:03:49,963: t15.2023.10.06 val PER: 0.1152
2026-01-05 13:03:49,963: t15.2023.10.08 val PER: 0.2706
2026-01-05 13:03:49,963: t15.2023.10.13 val PER: 0.2428
2026-01-05 13:03:49,963: t15.2023.10.15 val PER: 0.1931
2026-01-05 13:03:49,963: t15.2023.10.20 val PER: 0.2013
2026-01-05 13:03:49,963: t15.2023.10.22 val PER: 0.1459
2026-01-05 13:03:49,964: t15.2023.11.03 val PER: 0.2056
2026-01-05 13:03:49,964: t15.2023.11.04 val PER: 0.0341
2026-01-05 13:03:49,964: t15.2023.11.17 val PER: 0.0622
2026-01-05 13:03:49,964: t15.2023.11.19 val PER: 0.0599
2026-01-05 13:03:49,964: t15.2023.11.26 val PER: 0.1812
2026-01-05 13:03:49,964: t15.2023.12.03 val PER: 0.1639
2026-01-05 13:03:49,964: t15.2023.12.08 val PER: 0.1465
2026-01-05 13:03:49,964: t15.2023.12.10 val PER: 0.1432
2026-01-05 13:03:49,964: t15.2023.12.17 val PER: 0.1684
2026-01-05 13:03:49,964: t15.2023.12.29 val PER: 0.1736
2026-01-05 13:03:49,964: t15.2024.02.25 val PER: 0.1517
2026-01-05 13:03:49,964: t15.2024.03.08 val PER: 0.2646
2026-01-05 13:03:49,964: t15.2024.03.15 val PER: 0.2458
2026-01-05 13:03:49,964: t15.2024.03.17 val PER: 0.1785
2026-01-05 13:03:49,964: t15.2024.05.10 val PER: 0.1947
2026-01-05 13:03:49,965: t15.2024.06.14 val PER: 0.2035
2026-01-05 13:03:49,965: t15.2024.07.19 val PER: 0.2841
2026-01-05 13:03:49,965: t15.2024.07.21 val PER: 0.1283
2026-01-05 13:03:49,965: t15.2024.07.28 val PER: 0.1618
2026-01-05 13:03:49,965: t15.2025.01.10 val PER: 0.3154
2026-01-05 13:03:49,965: t15.2025.01.12 val PER: 0.1848
2026-01-05 13:03:49,965: t15.2025.03.14 val PER: 0.3550
2026-01-05 13:03:49,965: t15.2025.03.16 val PER: 0.2304
2026-01-05 13:03:49,965: t15.2025.03.30 val PER: 0.3437
2026-01-05 13:03:49,965: t15.2025.04.13 val PER: 0.2639
2026-01-05 13:03:50,267: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_8000
2026-01-05 13:04:08,824: Train batch 8200: loss: 9.62 grad norm: 46.96 time: 0.056
2026-01-05 13:04:28,285: Train batch 8400: loss: 10.05 grad norm: 46.48 time: 0.063
2026-01-05 13:04:38,573: Running test after training batch: 8500
2026-01-05 13:04:38,696: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:04:43,718: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 13:04:43,749: WER debug example
  GT : how does it keep the cost down
  PR : houde dusts it keep the cost nit
2026-01-05 13:04:45,458: Val batch 8500: PER (avg): 0.1804 CTC Loss (avg): 17.7899 WER(1gram): 51.27% (n=64) time: 6.885
2026-01-05 13:04:45,458: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 13:04:45,459: t15.2023.08.13 val PER: 0.1455
2026-01-05 13:04:45,459: t15.2023.08.18 val PER: 0.1375
2026-01-05 13:04:45,459: t15.2023.08.20 val PER: 0.1382
2026-01-05 13:04:45,459: t15.2023.08.25 val PER: 0.1114
2026-01-05 13:04:45,459: t15.2023.08.27 val PER: 0.2010
2026-01-05 13:04:45,459: t15.2023.09.01 val PER: 0.1006
2026-01-05 13:04:45,459: t15.2023.09.03 val PER: 0.1865
2026-01-05 13:04:45,459: t15.2023.09.24 val PER: 0.1590
2026-01-05 13:04:45,459: t15.2023.09.29 val PER: 0.1487
2026-01-05 13:04:45,459: t15.2023.10.01 val PER: 0.1955
2026-01-05 13:04:45,459: t15.2023.10.06 val PER: 0.0980
2026-01-05 13:04:45,459: t15.2023.10.08 val PER: 0.2585
2026-01-05 13:04:45,459: t15.2023.10.13 val PER: 0.2335
2026-01-05 13:04:45,460: t15.2023.10.15 val PER: 0.1793
2026-01-05 13:04:45,460: t15.2023.10.20 val PER: 0.2013
2026-01-05 13:04:45,460: t15.2023.10.22 val PER: 0.1481
2026-01-05 13:04:45,460: t15.2023.11.03 val PER: 0.2001
2026-01-05 13:04:45,460: t15.2023.11.04 val PER: 0.0512
2026-01-05 13:04:45,460: t15.2023.11.17 val PER: 0.0591
2026-01-05 13:04:45,460: t15.2023.11.19 val PER: 0.0559
2026-01-05 13:04:45,460: t15.2023.11.26 val PER: 0.1746
2026-01-05 13:04:45,460: t15.2023.12.03 val PER: 0.1481
2026-01-05 13:04:45,460: t15.2023.12.08 val PER: 0.1425
2026-01-05 13:04:45,460: t15.2023.12.10 val PER: 0.1156
2026-01-05 13:04:45,460: t15.2023.12.17 val PER: 0.1694
2026-01-05 13:04:45,460: t15.2023.12.29 val PER: 0.1798
2026-01-05 13:04:45,460: t15.2024.02.25 val PER: 0.1503
2026-01-05 13:04:45,460: t15.2024.03.08 val PER: 0.2646
2026-01-05 13:04:45,461: t15.2024.03.15 val PER: 0.2333
2026-01-05 13:04:45,461: t15.2024.03.17 val PER: 0.1722
2026-01-05 13:04:45,461: t15.2024.05.10 val PER: 0.1887
2026-01-05 13:04:45,461: t15.2024.06.14 val PER: 0.1924
2026-01-05 13:04:45,461: t15.2024.07.19 val PER: 0.2821
2026-01-05 13:04:45,461: t15.2024.07.21 val PER: 0.1228
2026-01-05 13:04:45,461: t15.2024.07.28 val PER: 0.1618
2026-01-05 13:04:45,461: t15.2025.01.10 val PER: 0.3237
2026-01-05 13:04:45,461: t15.2025.01.12 val PER: 0.1832
2026-01-05 13:04:45,461: t15.2025.03.14 val PER: 0.3506
2026-01-05 13:04:45,461: t15.2025.03.16 val PER: 0.2173
2026-01-05 13:04:45,461: t15.2025.03.30 val PER: 0.3241
2026-01-05 13:04:45,461: t15.2025.04.13 val PER: 0.2411
2026-01-05 13:04:45,462: New best val WER(1gram) 52.54% --> 51.27%
2026-01-05 13:04:45,462: Checkpointing model
2026-01-05 13:04:46,083: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:04:46,369: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_8500
2026-01-05 13:04:56,216: Train batch 8600: loss: 16.19 grad norm: 63.30 time: 0.054
2026-01-05 13:05:14,691: Train batch 8800: loss: 15.38 grad norm: 63.33 time: 0.060
2026-01-05 13:05:34,085: Train batch 9000: loss: 16.29 grad norm: 68.09 time: 0.072
2026-01-05 13:05:34,086: Running test after training batch: 9000
2026-01-05 13:05:34,230: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:05:39,455: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 13:05:39,488: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 13:05:41,290: Val batch 9000: PER (avg): 0.1739 CTC Loss (avg): 17.3039 WER(1gram): 51.02% (n=64) time: 7.205
2026-01-05 13:05:41,291: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-05 13:05:41,291: t15.2023.08.13 val PER: 0.1331
2026-01-05 13:05:41,291: t15.2023.08.18 val PER: 0.1215
2026-01-05 13:05:41,291: t15.2023.08.20 val PER: 0.1295
2026-01-05 13:05:41,291: t15.2023.08.25 val PER: 0.0964
2026-01-05 13:05:41,291: t15.2023.08.27 val PER: 0.2170
2026-01-05 13:05:41,292: t15.2023.09.01 val PER: 0.0990
2026-01-05 13:05:41,292: t15.2023.09.03 val PER: 0.1781
2026-01-05 13:05:41,292: t15.2023.09.24 val PER: 0.1481
2026-01-05 13:05:41,292: t15.2023.09.29 val PER: 0.1436
2026-01-05 13:05:41,292: t15.2023.10.01 val PER: 0.1882
2026-01-05 13:05:41,292: t15.2023.10.06 val PER: 0.0969
2026-01-05 13:05:41,292: t15.2023.10.08 val PER: 0.2571
2026-01-05 13:05:41,293: t15.2023.10.13 val PER: 0.2273
2026-01-05 13:05:41,293: t15.2023.10.15 val PER: 0.1773
2026-01-05 13:05:41,293: t15.2023.10.20 val PER: 0.1980
2026-01-05 13:05:41,293: t15.2023.10.22 val PER: 0.1381
2026-01-05 13:05:41,293: t15.2023.11.03 val PER: 0.1954
2026-01-05 13:05:41,293: t15.2023.11.04 val PER: 0.0410
2026-01-05 13:05:41,293: t15.2023.11.17 val PER: 0.0544
2026-01-05 13:05:41,293: t15.2023.11.19 val PER: 0.0459
2026-01-05 13:05:41,293: t15.2023.11.26 val PER: 0.1681
2026-01-05 13:05:41,293: t15.2023.12.03 val PER: 0.1355
2026-01-05 13:05:41,293: t15.2023.12.08 val PER: 0.1218
2026-01-05 13:05:41,294: t15.2023.12.10 val PER: 0.1064
2026-01-05 13:05:41,294: t15.2023.12.17 val PER: 0.1590
2026-01-05 13:05:41,294: t15.2023.12.29 val PER: 0.1627
2026-01-05 13:05:41,294: t15.2024.02.25 val PER: 0.1362
2026-01-05 13:05:41,294: t15.2024.03.08 val PER: 0.2717
2026-01-05 13:05:41,294: t15.2024.03.15 val PER: 0.2333
2026-01-05 13:05:41,294: t15.2024.03.17 val PER: 0.1757
2026-01-05 13:05:41,294: t15.2024.05.10 val PER: 0.1902
2026-01-05 13:05:41,294: t15.2024.06.14 val PER: 0.1845
2026-01-05 13:05:41,294: t15.2024.07.19 val PER: 0.2670
2026-01-05 13:05:41,294: t15.2024.07.21 val PER: 0.1138
2026-01-05 13:05:41,295: t15.2024.07.28 val PER: 0.1559
2026-01-05 13:05:41,295: t15.2025.01.10 val PER: 0.3113
2026-01-05 13:05:41,295: t15.2025.01.12 val PER: 0.1732
2026-01-05 13:05:41,295: t15.2025.03.14 val PER: 0.3580
2026-01-05 13:05:41,295: t15.2025.03.16 val PER: 0.2120
2026-01-05 13:05:41,295: t15.2025.03.30 val PER: 0.3356
2026-01-05 13:05:41,295: t15.2025.04.13 val PER: 0.2482
2026-01-05 13:05:41,296: New best val WER(1gram) 51.27% --> 51.02%
2026-01-05 13:05:41,296: Checkpointing model
2026-01-05 13:05:41,940: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:05:42,216: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_9000
2026-01-05 13:06:00,700: Train batch 9200: loss: 10.86 grad norm: 48.93 time: 0.056
2026-01-05 13:06:19,196: Train batch 9400: loss: 7.57 grad norm: 46.04 time: 0.068
2026-01-05 13:06:28,475: Running test after training batch: 9500
2026-01-05 13:06:28,642: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:06:33,719: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 13:06:33,752: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost get
2026-01-05 13:06:35,599: Val batch 9500: PER (avg): 0.1724 CTC Loss (avg): 17.2539 WER(1gram): 49.75% (n=64) time: 7.124
2026-01-05 13:06:35,600: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 13:06:35,600: t15.2023.08.13 val PER: 0.1341
2026-01-05 13:06:35,600: t15.2023.08.18 val PER: 0.1199
2026-01-05 13:06:35,600: t15.2023.08.20 val PER: 0.1311
2026-01-05 13:06:35,600: t15.2023.08.25 val PER: 0.1054
2026-01-05 13:06:35,600: t15.2023.08.27 val PER: 0.2026
2026-01-05 13:06:35,601: t15.2023.09.01 val PER: 0.0925
2026-01-05 13:06:35,601: t15.2023.09.03 val PER: 0.1900
2026-01-05 13:06:35,601: t15.2023.09.24 val PER: 0.1456
2026-01-05 13:06:35,601: t15.2023.09.29 val PER: 0.1417
2026-01-05 13:06:35,601: t15.2023.10.01 val PER: 0.1909
2026-01-05 13:06:35,601: t15.2023.10.06 val PER: 0.1001
2026-01-05 13:06:35,601: t15.2023.10.08 val PER: 0.2585
2026-01-05 13:06:35,601: t15.2023.10.13 val PER: 0.2335
2026-01-05 13:06:35,601: t15.2023.10.15 val PER: 0.1839
2026-01-05 13:06:35,601: t15.2023.10.20 val PER: 0.1879
2026-01-05 13:06:35,601: t15.2023.10.22 val PER: 0.1303
2026-01-05 13:06:35,602: t15.2023.11.03 val PER: 0.1961
2026-01-05 13:06:35,602: t15.2023.11.04 val PER: 0.0307
2026-01-05 13:06:35,602: t15.2023.11.17 val PER: 0.0529
2026-01-05 13:06:35,602: t15.2023.11.19 val PER: 0.0419
2026-01-05 13:06:35,602: t15.2023.11.26 val PER: 0.1594
2026-01-05 13:06:35,602: t15.2023.12.03 val PER: 0.1418
2026-01-05 13:06:35,602: t15.2023.12.08 val PER: 0.1305
2026-01-05 13:06:35,602: t15.2023.12.10 val PER: 0.1170
2026-01-05 13:06:35,602: t15.2023.12.17 val PER: 0.1476
2026-01-05 13:06:35,602: t15.2023.12.29 val PER: 0.1531
2026-01-05 13:06:35,602: t15.2024.02.25 val PER: 0.1306
2026-01-05 13:06:35,602: t15.2024.03.08 val PER: 0.2688
2026-01-05 13:06:35,602: t15.2024.03.15 val PER: 0.2258
2026-01-05 13:06:35,602: t15.2024.03.17 val PER: 0.1618
2026-01-05 13:06:35,602: t15.2024.05.10 val PER: 0.1813
2026-01-05 13:06:35,603: t15.2024.06.14 val PER: 0.1688
2026-01-05 13:06:35,603: t15.2024.07.19 val PER: 0.2683
2026-01-05 13:06:35,603: t15.2024.07.21 val PER: 0.1152
2026-01-05 13:06:35,603: t15.2024.07.28 val PER: 0.1522
2026-01-05 13:06:35,603: t15.2025.01.10 val PER: 0.3320
2026-01-05 13:06:35,603: t15.2025.01.12 val PER: 0.1832
2026-01-05 13:06:35,603: t15.2025.03.14 val PER: 0.3713
2026-01-05 13:06:35,603: t15.2025.03.16 val PER: 0.2094
2026-01-05 13:06:35,603: t15.2025.03.30 val PER: 0.3126
2026-01-05 13:06:35,603: t15.2025.04.13 val PER: 0.2254
2026-01-05 13:06:35,604: New best val WER(1gram) 51.02% --> 49.75%
2026-01-05 13:06:35,604: Checkpointing model
2026-01-05 13:06:36,245: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:06:36,534: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_9500
2026-01-05 13:06:45,700: Train batch 9600: loss: 8.56 grad norm: 48.72 time: 0.074
2026-01-05 13:07:04,221: Train batch 9800: loss: 12.37 grad norm: 56.48 time: 0.063
2026-01-05 13:07:23,105: Train batch 10000: loss: 5.25 grad norm: 35.03 time: 0.061
2026-01-05 13:07:23,106: Running test after training batch: 10000
2026-01-05 13:07:23,250: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:07:28,401: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 13:07:28,434: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sindt
2026-01-05 13:07:30,267: Val batch 10000: PER (avg): 0.1697 CTC Loss (avg): 16.8175 WER(1gram): 51.78% (n=64) time: 7.161
2026-01-05 13:07:30,267: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 13:07:30,267: t15.2023.08.13 val PER: 0.1258
2026-01-05 13:07:30,268: t15.2023.08.18 val PER: 0.1165
2026-01-05 13:07:30,268: t15.2023.08.20 val PER: 0.1287
2026-01-05 13:07:30,268: t15.2023.08.25 val PER: 0.1069
2026-01-05 13:07:30,268: t15.2023.08.27 val PER: 0.1977
2026-01-05 13:07:30,268: t15.2023.09.01 val PER: 0.0917
2026-01-05 13:07:30,268: t15.2023.09.03 val PER: 0.1793
2026-01-05 13:07:30,268: t15.2023.09.24 val PER: 0.1517
2026-01-05 13:07:30,268: t15.2023.09.29 val PER: 0.1423
2026-01-05 13:07:30,268: t15.2023.10.01 val PER: 0.1849
2026-01-05 13:07:30,268: t15.2023.10.06 val PER: 0.1109
2026-01-05 13:07:30,268: t15.2023.10.08 val PER: 0.2558
2026-01-05 13:07:30,268: t15.2023.10.13 val PER: 0.2188
2026-01-05 13:07:30,269: t15.2023.10.15 val PER: 0.1707
2026-01-05 13:07:30,269: t15.2023.10.20 val PER: 0.1913
2026-01-05 13:07:30,269: t15.2023.10.22 val PER: 0.1292
2026-01-05 13:07:30,269: t15.2023.11.03 val PER: 0.1967
2026-01-05 13:07:30,269: t15.2023.11.04 val PER: 0.0375
2026-01-05 13:07:30,269: t15.2023.11.17 val PER: 0.0513
2026-01-05 13:07:30,269: t15.2023.11.19 val PER: 0.0459
2026-01-05 13:07:30,269: t15.2023.11.26 val PER: 0.1464
2026-01-05 13:07:30,269: t15.2023.12.03 val PER: 0.1355
2026-01-05 13:07:30,269: t15.2023.12.08 val PER: 0.1305
2026-01-05 13:07:30,269: t15.2023.12.10 val PER: 0.1091
2026-01-05 13:07:30,269: t15.2023.12.17 val PER: 0.1559
2026-01-05 13:07:30,269: t15.2023.12.29 val PER: 0.1469
2026-01-05 13:07:30,270: t15.2024.02.25 val PER: 0.1489
2026-01-05 13:07:30,270: t15.2024.03.08 val PER: 0.2447
2026-01-05 13:07:30,270: t15.2024.03.15 val PER: 0.2289
2026-01-05 13:07:30,270: t15.2024.03.17 val PER: 0.1709
2026-01-05 13:07:30,270: t15.2024.05.10 val PER: 0.1783
2026-01-05 13:07:30,270: t15.2024.06.14 val PER: 0.1830
2026-01-05 13:07:30,270: t15.2024.07.19 val PER: 0.2630
2026-01-05 13:07:30,270: t15.2024.07.21 val PER: 0.1103
2026-01-05 13:07:30,270: t15.2024.07.28 val PER: 0.1603
2026-01-05 13:07:30,270: t15.2025.01.10 val PER: 0.3058
2026-01-05 13:07:30,270: t15.2025.01.12 val PER: 0.1732
2026-01-05 13:07:30,270: t15.2025.03.14 val PER: 0.3417
2026-01-05 13:07:30,270: t15.2025.03.16 val PER: 0.2160
2026-01-05 13:07:30,271: t15.2025.03.30 val PER: 0.3138
2026-01-05 13:07:30,271: t15.2025.04.13 val PER: 0.2282
2026-01-05 13:07:30,568: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_10000
2026-01-05 13:07:49,165: Train batch 10200: loss: 6.55 grad norm: 38.31 time: 0.050
2026-01-05 13:08:08,037: Train batch 10400: loss: 9.62 grad norm: 54.56 time: 0.072
2026-01-05 13:08:17,684: Running test after training batch: 10500
2026-01-05 13:08:17,844: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:08:23,098: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:08:23,130: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-05 13:08:24,902: Val batch 10500: PER (avg): 0.1663 CTC Loss (avg): 16.6570 WER(1gram): 48.98% (n=64) time: 7.218
2026-01-05 13:08:24,903: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 13:08:24,903: t15.2023.08.13 val PER: 0.1216
2026-01-05 13:08:24,903: t15.2023.08.18 val PER: 0.1182
2026-01-05 13:08:24,903: t15.2023.08.20 val PER: 0.1239
2026-01-05 13:08:24,903: t15.2023.08.25 val PER: 0.1084
2026-01-05 13:08:24,903: t15.2023.08.27 val PER: 0.1977
2026-01-05 13:08:24,903: t15.2023.09.01 val PER: 0.0917
2026-01-05 13:08:24,903: t15.2023.09.03 val PER: 0.1805
2026-01-05 13:08:24,903: t15.2023.09.24 val PER: 0.1444
2026-01-05 13:08:24,903: t15.2023.09.29 val PER: 0.1512
2026-01-05 13:08:24,903: t15.2023.10.01 val PER: 0.1922
2026-01-05 13:08:24,903: t15.2023.10.06 val PER: 0.0893
2026-01-05 13:08:24,904: t15.2023.10.08 val PER: 0.2517
2026-01-05 13:08:24,904: t15.2023.10.13 val PER: 0.2071
2026-01-05 13:08:24,904: t15.2023.10.15 val PER: 0.1753
2026-01-05 13:08:24,904: t15.2023.10.20 val PER: 0.1812
2026-01-05 13:08:24,904: t15.2023.10.22 val PER: 0.1192
2026-01-05 13:08:24,904: t15.2023.11.03 val PER: 0.1913
2026-01-05 13:08:24,904: t15.2023.11.04 val PER: 0.0375
2026-01-05 13:08:24,904: t15.2023.11.17 val PER: 0.0575
2026-01-05 13:08:24,904: t15.2023.11.19 val PER: 0.0479
2026-01-05 13:08:24,904: t15.2023.11.26 val PER: 0.1406
2026-01-05 13:08:24,904: t15.2023.12.03 val PER: 0.1387
2026-01-05 13:08:24,904: t15.2023.12.08 val PER: 0.1192
2026-01-05 13:08:24,904: t15.2023.12.10 val PER: 0.1064
2026-01-05 13:08:24,905: t15.2023.12.17 val PER: 0.1549
2026-01-05 13:08:24,905: t15.2023.12.29 val PER: 0.1551
2026-01-05 13:08:24,905: t15.2024.02.25 val PER: 0.1236
2026-01-05 13:08:24,905: t15.2024.03.08 val PER: 0.2518
2026-01-05 13:08:24,905: t15.2024.03.15 val PER: 0.2170
2026-01-05 13:08:24,905: t15.2024.03.17 val PER: 0.1541
2026-01-05 13:08:24,905: t15.2024.05.10 val PER: 0.1694
2026-01-05 13:08:24,905: t15.2024.06.14 val PER: 0.1767
2026-01-05 13:08:24,905: t15.2024.07.19 val PER: 0.2577
2026-01-05 13:08:24,905: t15.2024.07.21 val PER: 0.1041
2026-01-05 13:08:24,906: t15.2024.07.28 val PER: 0.1419
2026-01-05 13:08:24,906: t15.2025.01.10 val PER: 0.3292
2026-01-05 13:08:24,906: t15.2025.01.12 val PER: 0.1655
2026-01-05 13:08:24,906: t15.2025.03.14 val PER: 0.3580
2026-01-05 13:08:24,906: t15.2025.03.16 val PER: 0.2042
2026-01-05 13:08:24,906: t15.2025.03.30 val PER: 0.3276
2026-01-05 13:08:24,906: t15.2025.04.13 val PER: 0.2211
2026-01-05 13:08:24,907: New best val WER(1gram) 49.75% --> 48.98%
2026-01-05 13:08:24,907: Checkpointing model
2026-01-05 13:08:25,559: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:08:25,850: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_10500
2026-01-05 13:08:35,441: Train batch 10600: loss: 9.69 grad norm: 60.67 time: 0.072
2026-01-05 13:08:54,130: Train batch 10800: loss: 14.47 grad norm: 64.82 time: 0.065
2026-01-05 13:09:13,040: Train batch 11000: loss: 14.68 grad norm: 64.51 time: 0.059
2026-01-05 13:09:13,040: Running test after training batch: 11000
2026-01-05 13:09:13,207: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:09:18,303: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:09:18,336: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 13:09:20,174: Val batch 11000: PER (avg): 0.1647 CTC Loss (avg): 16.5407 WER(1gram): 48.73% (n=64) time: 7.133
2026-01-05 13:09:20,174: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 13:09:20,174: t15.2023.08.13 val PER: 0.1331
2026-01-05 13:09:20,175: t15.2023.08.18 val PER: 0.1182
2026-01-05 13:09:20,175: t15.2023.08.20 val PER: 0.1207
2026-01-05 13:09:20,175: t15.2023.08.25 val PER: 0.0994
2026-01-05 13:09:20,175: t15.2023.08.27 val PER: 0.1994
2026-01-05 13:09:20,175: t15.2023.09.01 val PER: 0.0933
2026-01-05 13:09:20,175: t15.2023.09.03 val PER: 0.1663
2026-01-05 13:09:20,175: t15.2023.09.24 val PER: 0.1432
2026-01-05 13:09:20,175: t15.2023.09.29 val PER: 0.1366
2026-01-05 13:09:20,175: t15.2023.10.01 val PER: 0.1935
2026-01-05 13:09:20,175: t15.2023.10.06 val PER: 0.0915
2026-01-05 13:09:20,175: t15.2023.10.08 val PER: 0.2571
2026-01-05 13:09:20,176: t15.2023.10.13 val PER: 0.2196
2026-01-05 13:09:20,176: t15.2023.10.15 val PER: 0.1727
2026-01-05 13:09:20,176: t15.2023.10.20 val PER: 0.1913
2026-01-05 13:09:20,176: t15.2023.10.22 val PER: 0.1269
2026-01-05 13:09:20,176: t15.2023.11.03 val PER: 0.1940
2026-01-05 13:09:20,176: t15.2023.11.04 val PER: 0.0341
2026-01-05 13:09:20,176: t15.2023.11.17 val PER: 0.0513
2026-01-05 13:09:20,176: t15.2023.11.19 val PER: 0.0399
2026-01-05 13:09:20,176: t15.2023.11.26 val PER: 0.1355
2026-01-05 13:09:20,176: t15.2023.12.03 val PER: 0.1334
2026-01-05 13:09:20,177: t15.2023.12.08 val PER: 0.1172
2026-01-05 13:09:20,177: t15.2023.12.10 val PER: 0.0946
2026-01-05 13:09:20,177: t15.2023.12.17 val PER: 0.1455
2026-01-05 13:09:20,177: t15.2023.12.29 val PER: 0.1496
2026-01-05 13:09:20,177: t15.2024.02.25 val PER: 0.1264
2026-01-05 13:09:20,177: t15.2024.03.08 val PER: 0.2432
2026-01-05 13:09:20,177: t15.2024.03.15 val PER: 0.2208
2026-01-05 13:09:20,178: t15.2024.03.17 val PER: 0.1527
2026-01-05 13:09:20,178: t15.2024.05.10 val PER: 0.1679
2026-01-05 13:09:20,178: t15.2024.06.14 val PER: 0.1625
2026-01-05 13:09:20,178: t15.2024.07.19 val PER: 0.2551
2026-01-05 13:09:20,178: t15.2024.07.21 val PER: 0.1041
2026-01-05 13:09:20,178: t15.2024.07.28 val PER: 0.1478
2026-01-05 13:09:20,178: t15.2025.01.10 val PER: 0.3113
2026-01-05 13:09:20,178: t15.2025.01.12 val PER: 0.1647
2026-01-05 13:09:20,179: t15.2025.03.14 val PER: 0.3447
2026-01-05 13:09:20,179: t15.2025.03.16 val PER: 0.2081
2026-01-05 13:09:20,179: t15.2025.03.30 val PER: 0.3264
2026-01-05 13:09:20,179: t15.2025.04.13 val PER: 0.2297
2026-01-05 13:09:20,179: New best val WER(1gram) 48.98% --> 48.73%
2026-01-05 13:09:20,179: Checkpointing model
2026-01-05 13:09:20,885: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:09:21,203: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_11000
2026-01-05 13:09:39,884: Train batch 11200: loss: 10.62 grad norm: 52.27 time: 0.071
2026-01-05 13:09:58,500: Train batch 11400: loss: 9.91 grad norm: 55.25 time: 0.057
2026-01-05 13:10:07,844: Running test after training batch: 11500
2026-01-05 13:10:07,963: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:10:13,055: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 13:10:13,089: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-05 13:10:14,921: Val batch 11500: PER (avg): 0.1618 CTC Loss (avg): 16.5861 WER(1gram): 49.24% (n=64) time: 7.077
2026-01-05 13:10:14,921: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 13:10:14,921: t15.2023.08.13 val PER: 0.1258
2026-01-05 13:10:14,921: t15.2023.08.18 val PER: 0.1132
2026-01-05 13:10:14,922: t15.2023.08.20 val PER: 0.1223
2026-01-05 13:10:14,922: t15.2023.08.25 val PER: 0.1009
2026-01-05 13:10:14,922: t15.2023.08.27 val PER: 0.1994
2026-01-05 13:10:14,922: t15.2023.09.01 val PER: 0.0812
2026-01-05 13:10:14,922: t15.2023.09.03 val PER: 0.1734
2026-01-05 13:10:14,922: t15.2023.09.24 val PER: 0.1299
2026-01-05 13:10:14,922: t15.2023.09.29 val PER: 0.1308
2026-01-05 13:10:14,922: t15.2023.10.01 val PER: 0.1783
2026-01-05 13:10:14,922: t15.2023.10.06 val PER: 0.0958
2026-01-05 13:10:14,922: t15.2023.10.08 val PER: 0.2612
2026-01-05 13:10:14,922: t15.2023.10.13 val PER: 0.2095
2026-01-05 13:10:14,922: t15.2023.10.15 val PER: 0.1622
2026-01-05 13:10:14,923: t15.2023.10.20 val PER: 0.1879
2026-01-05 13:10:14,923: t15.2023.10.22 val PER: 0.1303
2026-01-05 13:10:14,923: t15.2023.11.03 val PER: 0.1852
2026-01-05 13:10:14,923: t15.2023.11.04 val PER: 0.0375
2026-01-05 13:10:14,923: t15.2023.11.17 val PER: 0.0451
2026-01-05 13:10:14,923: t15.2023.11.19 val PER: 0.0519
2026-01-05 13:10:14,923: t15.2023.11.26 val PER: 0.1370
2026-01-05 13:10:14,923: t15.2023.12.03 val PER: 0.1313
2026-01-05 13:10:14,923: t15.2023.12.08 val PER: 0.1132
2026-01-05 13:10:14,923: t15.2023.12.10 val PER: 0.1051
2026-01-05 13:10:14,923: t15.2023.12.17 val PER: 0.1424
2026-01-05 13:10:14,924: t15.2023.12.29 val PER: 0.1482
2026-01-05 13:10:14,924: t15.2024.02.25 val PER: 0.1180
2026-01-05 13:10:14,924: t15.2024.03.08 val PER: 0.2333
2026-01-05 13:10:14,924: t15.2024.03.15 val PER: 0.2189
2026-01-05 13:10:14,924: t15.2024.03.17 val PER: 0.1513
2026-01-05 13:10:14,924: t15.2024.05.10 val PER: 0.1649
2026-01-05 13:10:14,924: t15.2024.06.14 val PER: 0.1893
2026-01-05 13:10:14,924: t15.2024.07.19 val PER: 0.2643
2026-01-05 13:10:14,924: t15.2024.07.21 val PER: 0.1034
2026-01-05 13:10:14,924: t15.2024.07.28 val PER: 0.1434
2026-01-05 13:10:14,924: t15.2025.01.10 val PER: 0.3182
2026-01-05 13:10:14,924: t15.2025.01.12 val PER: 0.1601
2026-01-05 13:10:14,924: t15.2025.03.14 val PER: 0.3447
2026-01-05 13:10:14,924: t15.2025.03.16 val PER: 0.2029
2026-01-05 13:10:14,925: t15.2025.03.30 val PER: 0.3092
2026-01-05 13:10:14,925: t15.2025.04.13 val PER: 0.2197
2026-01-05 13:10:15,193: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_11500
2026-01-05 13:10:24,173: Train batch 11600: loss: 10.90 grad norm: 48.77 time: 0.060
2026-01-05 13:10:42,787: Train batch 11800: loss: 6.77 grad norm: 41.54 time: 0.045
2026-01-05 13:11:01,641: Train batch 12000: loss: 13.20 grad norm: 56.77 time: 0.072
2026-01-05 13:11:01,642: Running test after training batch: 12000
2026-01-05 13:11:01,746: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:11:06,994: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 13:11:07,027: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-05 13:11:08,861: Val batch 12000: PER (avg): 0.1592 CTC Loss (avg): 16.1966 WER(1gram): 52.03% (n=64) time: 7.219
2026-01-05 13:11:08,862: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-05 13:11:08,862: t15.2023.08.13 val PER: 0.1279
2026-01-05 13:11:08,862: t15.2023.08.18 val PER: 0.1140
2026-01-05 13:11:08,862: t15.2023.08.20 val PER: 0.1152
2026-01-05 13:11:08,862: t15.2023.08.25 val PER: 0.1024
2026-01-05 13:11:08,862: t15.2023.08.27 val PER: 0.1994
2026-01-05 13:11:08,862: t15.2023.09.01 val PER: 0.0836
2026-01-05 13:11:08,862: t15.2023.09.03 val PER: 0.1651
2026-01-05 13:11:08,863: t15.2023.09.24 val PER: 0.1347
2026-01-05 13:11:08,863: t15.2023.09.29 val PER: 0.1398
2026-01-05 13:11:08,863: t15.2023.10.01 val PER: 0.1783
2026-01-05 13:11:08,863: t15.2023.10.06 val PER: 0.0872
2026-01-05 13:11:08,863: t15.2023.10.08 val PER: 0.2530
2026-01-05 13:11:08,863: t15.2023.10.13 val PER: 0.2040
2026-01-05 13:11:08,863: t15.2023.10.15 val PER: 0.1569
2026-01-05 13:11:08,863: t15.2023.10.20 val PER: 0.1879
2026-01-05 13:11:08,863: t15.2023.10.22 val PER: 0.1192
2026-01-05 13:11:08,863: t15.2023.11.03 val PER: 0.1825
2026-01-05 13:11:08,863: t15.2023.11.04 val PER: 0.0307
2026-01-05 13:11:08,863: t15.2023.11.17 val PER: 0.0358
2026-01-05 13:11:08,863: t15.2023.11.19 val PER: 0.0339
2026-01-05 13:11:08,864: t15.2023.11.26 val PER: 0.1297
2026-01-05 13:11:08,864: t15.2023.12.03 val PER: 0.1218
2026-01-05 13:11:08,864: t15.2023.12.08 val PER: 0.1079
2026-01-05 13:11:08,864: t15.2023.12.10 val PER: 0.0972
2026-01-05 13:11:08,864: t15.2023.12.17 val PER: 0.1414
2026-01-05 13:11:08,864: t15.2023.12.29 val PER: 0.1414
2026-01-05 13:11:08,864: t15.2024.02.25 val PER: 0.1180
2026-01-05 13:11:08,864: t15.2024.03.08 val PER: 0.2461
2026-01-05 13:11:08,864: t15.2024.03.15 val PER: 0.2120
2026-01-05 13:11:08,864: t15.2024.03.17 val PER: 0.1464
2026-01-05 13:11:08,864: t15.2024.05.10 val PER: 0.1842
2026-01-05 13:11:08,864: t15.2024.06.14 val PER: 0.1814
2026-01-05 13:11:08,865: t15.2024.07.19 val PER: 0.2551
2026-01-05 13:11:08,865: t15.2024.07.21 val PER: 0.0986
2026-01-05 13:11:08,865: t15.2024.07.28 val PER: 0.1412
2026-01-05 13:11:08,865: t15.2025.01.10 val PER: 0.3127
2026-01-05 13:11:08,865: t15.2025.01.12 val PER: 0.1517
2026-01-05 13:11:08,865: t15.2025.03.14 val PER: 0.3536
2026-01-05 13:11:08,865: t15.2025.03.16 val PER: 0.2120
2026-01-05 13:11:08,865: t15.2025.03.30 val PER: 0.3172
2026-01-05 13:11:08,865: t15.2025.04.13 val PER: 0.2282
2026-01-05 13:11:09,144: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_12000
2026-01-05 13:11:27,887: Train batch 12200: loss: 6.04 grad norm: 43.61 time: 0.065
2026-01-05 13:11:46,384: Train batch 12400: loss: 4.73 grad norm: 35.17 time: 0.041
2026-01-05 13:11:55,984: Running test after training batch: 12500
2026-01-05 13:11:56,090: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:12:01,233: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 13:12:01,266: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-05 13:12:03,136: Val batch 12500: PER (avg): 0.1573 CTC Loss (avg): 16.0350 WER(1gram): 47.46% (n=64) time: 7.151
2026-01-05 13:12:03,137: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 13:12:03,137: t15.2023.08.13 val PER: 0.1227
2026-01-05 13:12:03,137: t15.2023.08.18 val PER: 0.1123
2026-01-05 13:12:03,137: t15.2023.08.20 val PER: 0.1144
2026-01-05 13:12:03,137: t15.2023.08.25 val PER: 0.0843
2026-01-05 13:12:03,138: t15.2023.08.27 val PER: 0.1945
2026-01-05 13:12:03,138: t15.2023.09.01 val PER: 0.0771
2026-01-05 13:12:03,138: t15.2023.09.03 val PER: 0.1710
2026-01-05 13:12:03,138: t15.2023.09.24 val PER: 0.1250
2026-01-05 13:12:03,138: t15.2023.09.29 val PER: 0.1334
2026-01-05 13:12:03,138: t15.2023.10.01 val PER: 0.1770
2026-01-05 13:12:03,138: t15.2023.10.06 val PER: 0.0872
2026-01-05 13:12:03,138: t15.2023.10.08 val PER: 0.2612
2026-01-05 13:12:03,138: t15.2023.10.13 val PER: 0.2118
2026-01-05 13:12:03,138: t15.2023.10.15 val PER: 0.1595
2026-01-05 13:12:03,139: t15.2023.10.20 val PER: 0.1846
2026-01-05 13:12:03,139: t15.2023.10.22 val PER: 0.1214
2026-01-05 13:12:03,139: t15.2023.11.03 val PER: 0.1825
2026-01-05 13:12:03,139: t15.2023.11.04 val PER: 0.0307
2026-01-05 13:12:03,139: t15.2023.11.17 val PER: 0.0560
2026-01-05 13:12:03,139: t15.2023.11.19 val PER: 0.0359
2026-01-05 13:12:03,139: t15.2023.11.26 val PER: 0.1348
2026-01-05 13:12:03,139: t15.2023.12.03 val PER: 0.1271
2026-01-05 13:12:03,139: t15.2023.12.08 val PER: 0.0999
2026-01-05 13:12:03,139: t15.2023.12.10 val PER: 0.0907
2026-01-05 13:12:03,139: t15.2023.12.17 val PER: 0.1435
2026-01-05 13:12:03,139: t15.2023.12.29 val PER: 0.1400
2026-01-05 13:12:03,140: t15.2024.02.25 val PER: 0.1194
2026-01-05 13:12:03,140: t15.2024.03.08 val PER: 0.2333
2026-01-05 13:12:03,140: t15.2024.03.15 val PER: 0.2126
2026-01-05 13:12:03,140: t15.2024.03.17 val PER: 0.1430
2026-01-05 13:12:03,140: t15.2024.05.10 val PER: 0.1724
2026-01-05 13:12:03,140: t15.2024.06.14 val PER: 0.1751
2026-01-05 13:12:03,140: t15.2024.07.19 val PER: 0.2525
2026-01-05 13:12:03,140: t15.2024.07.21 val PER: 0.0931
2026-01-05 13:12:03,140: t15.2024.07.28 val PER: 0.1360
2026-01-05 13:12:03,141: t15.2025.01.10 val PER: 0.3182
2026-01-05 13:12:03,141: t15.2025.01.12 val PER: 0.1440
2026-01-05 13:12:03,141: t15.2025.03.14 val PER: 0.3565
2026-01-05 13:12:03,141: t15.2025.03.16 val PER: 0.2029
2026-01-05 13:12:03,141: t15.2025.03.30 val PER: 0.3092
2026-01-05 13:12:03,141: t15.2025.04.13 val PER: 0.2282
2026-01-05 13:12:03,142: New best val WER(1gram) 48.73% --> 47.46%
2026-01-05 13:12:03,142: Checkpointing model
2026-01-05 13:12:03,843: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:12:04,134: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_12500
2026-01-05 13:12:13,441: Train batch 12600: loss: 7.93 grad norm: 43.39 time: 0.058
2026-01-05 13:12:32,212: Train batch 12800: loss: 5.84 grad norm: 36.37 time: 0.053
2026-01-05 13:12:51,383: Train batch 13000: loss: 6.59 grad norm: 44.15 time: 0.068
2026-01-05 13:12:51,384: Running test after training batch: 13000
2026-01-05 13:12:51,520: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:12:56,627: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:12:56,661: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-05 13:12:58,510: Val batch 13000: PER (avg): 0.1563 CTC Loss (avg): 15.9451 WER(1gram): 47.21% (n=64) time: 7.126
2026-01-05 13:12:58,511: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-05 13:12:58,511: t15.2023.08.13 val PER: 0.1143
2026-01-05 13:12:58,511: t15.2023.08.18 val PER: 0.1132
2026-01-05 13:12:58,511: t15.2023.08.20 val PER: 0.1088
2026-01-05 13:12:58,511: t15.2023.08.25 val PER: 0.0964
2026-01-05 13:12:58,511: t15.2023.08.27 val PER: 0.1961
2026-01-05 13:12:58,511: t15.2023.09.01 val PER: 0.0877
2026-01-05 13:12:58,511: t15.2023.09.03 val PER: 0.1698
2026-01-05 13:12:58,511: t15.2023.09.24 val PER: 0.1347
2026-01-05 13:12:58,512: t15.2023.09.29 val PER: 0.1353
2026-01-05 13:12:58,512: t15.2023.10.01 val PER: 0.1777
2026-01-05 13:12:58,512: t15.2023.10.06 val PER: 0.0883
2026-01-05 13:12:58,512: t15.2023.10.08 val PER: 0.2625
2026-01-05 13:12:58,512: t15.2023.10.13 val PER: 0.2071
2026-01-05 13:12:58,512: t15.2023.10.15 val PER: 0.1510
2026-01-05 13:12:58,512: t15.2023.10.20 val PER: 0.1846
2026-01-05 13:12:58,512: t15.2023.10.22 val PER: 0.1169
2026-01-05 13:12:58,512: t15.2023.11.03 val PER: 0.1791
2026-01-05 13:12:58,512: t15.2023.11.04 val PER: 0.0375
2026-01-05 13:12:58,513: t15.2023.11.17 val PER: 0.0482
2026-01-05 13:12:58,513: t15.2023.11.19 val PER: 0.0359
2026-01-05 13:12:58,513: t15.2023.11.26 val PER: 0.1225
2026-01-05 13:12:58,513: t15.2023.12.03 val PER: 0.1313
2026-01-05 13:12:58,513: t15.2023.12.08 val PER: 0.1045
2026-01-05 13:12:58,513: t15.2023.12.10 val PER: 0.0999
2026-01-05 13:12:58,513: t15.2023.12.17 val PER: 0.1466
2026-01-05 13:12:58,513: t15.2023.12.29 val PER: 0.1455
2026-01-05 13:12:58,513: t15.2024.02.25 val PER: 0.1138
2026-01-05 13:12:58,513: t15.2024.03.08 val PER: 0.2390
2026-01-05 13:12:58,513: t15.2024.03.15 val PER: 0.2126
2026-01-05 13:12:58,513: t15.2024.03.17 val PER: 0.1409
2026-01-05 13:12:58,513: t15.2024.05.10 val PER: 0.1694
2026-01-05 13:12:58,513: t15.2024.06.14 val PER: 0.1767
2026-01-05 13:12:58,514: t15.2024.07.19 val PER: 0.2512
2026-01-05 13:12:58,514: t15.2024.07.21 val PER: 0.0952
2026-01-05 13:12:58,514: t15.2024.07.28 val PER: 0.1419
2026-01-05 13:12:58,514: t15.2025.01.10 val PER: 0.3044
2026-01-05 13:12:58,514: t15.2025.01.12 val PER: 0.1339
2026-01-05 13:12:58,514: t15.2025.03.14 val PER: 0.3550
2026-01-05 13:12:58,514: t15.2025.03.16 val PER: 0.1846
2026-01-05 13:12:58,514: t15.2025.03.30 val PER: 0.3080
2026-01-05 13:12:58,514: t15.2025.04.13 val PER: 0.2154
2026-01-05 13:12:58,515: New best val WER(1gram) 47.46% --> 47.21%
2026-01-05 13:12:58,515: Checkpointing model
2026-01-05 13:12:59,183: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:12:59,477: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_13000
2026-01-05 13:13:18,042: Train batch 13200: loss: 12.25 grad norm: 58.98 time: 0.054
2026-01-05 13:13:36,680: Train batch 13400: loss: 8.67 grad norm: 49.91 time: 0.062
2026-01-05 13:13:45,843: Running test after training batch: 13500
2026-01-05 13:13:45,982: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:13:51,093: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 13:13:51,127: WER debug example
  GT : how does it keep the cost down
  PR : how us it keep the cost it
2026-01-05 13:13:53,037: Val batch 13500: PER (avg): 0.1546 CTC Loss (avg): 15.6947 WER(1gram): 48.48% (n=64) time: 7.193
2026-01-05 13:13:53,037: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 13:13:53,037: t15.2023.08.13 val PER: 0.1185
2026-01-05 13:13:53,037: t15.2023.08.18 val PER: 0.1115
2026-01-05 13:13:53,038: t15.2023.08.20 val PER: 0.1096
2026-01-05 13:13:53,038: t15.2023.08.25 val PER: 0.0858
2026-01-05 13:13:53,038: t15.2023.08.27 val PER: 0.1977
2026-01-05 13:13:53,038: t15.2023.09.01 val PER: 0.0860
2026-01-05 13:13:53,038: t15.2023.09.03 val PER: 0.1710
2026-01-05 13:13:53,038: t15.2023.09.24 val PER: 0.1323
2026-01-05 13:13:53,038: t15.2023.09.29 val PER: 0.1315
2026-01-05 13:13:53,038: t15.2023.10.01 val PER: 0.1790
2026-01-05 13:13:53,039: t15.2023.10.06 val PER: 0.0904
2026-01-05 13:13:53,039: t15.2023.10.08 val PER: 0.2558
2026-01-05 13:13:53,039: t15.2023.10.13 val PER: 0.2095
2026-01-05 13:13:53,039: t15.2023.10.15 val PER: 0.1582
2026-01-05 13:13:53,039: t15.2023.10.20 val PER: 0.1879
2026-01-05 13:13:53,039: t15.2023.10.22 val PER: 0.1180
2026-01-05 13:13:53,039: t15.2023.11.03 val PER: 0.1818
2026-01-05 13:13:53,040: t15.2023.11.04 val PER: 0.0341
2026-01-05 13:13:53,040: t15.2023.11.17 val PER: 0.0435
2026-01-05 13:13:53,040: t15.2023.11.19 val PER: 0.0359
2026-01-05 13:13:53,040: t15.2023.11.26 val PER: 0.1254
2026-01-05 13:13:53,040: t15.2023.12.03 val PER: 0.1208
2026-01-05 13:13:53,040: t15.2023.12.08 val PER: 0.1052
2026-01-05 13:13:53,040: t15.2023.12.10 val PER: 0.0920
2026-01-05 13:13:53,040: t15.2023.12.17 val PER: 0.1351
2026-01-05 13:13:53,040: t15.2023.12.29 val PER: 0.1386
2026-01-05 13:13:53,040: t15.2024.02.25 val PER: 0.1053
2026-01-05 13:13:53,040: t15.2024.03.08 val PER: 0.2361
2026-01-05 13:13:53,041: t15.2024.03.15 val PER: 0.2076
2026-01-05 13:13:53,041: t15.2024.03.17 val PER: 0.1457
2026-01-05 13:13:53,041: t15.2024.05.10 val PER: 0.1471
2026-01-05 13:13:53,041: t15.2024.06.14 val PER: 0.1672
2026-01-05 13:13:53,041: t15.2024.07.19 val PER: 0.2446
2026-01-05 13:13:53,041: t15.2024.07.21 val PER: 0.0966
2026-01-05 13:13:53,041: t15.2024.07.28 val PER: 0.1404
2026-01-05 13:13:53,041: t15.2025.01.10 val PER: 0.2920
2026-01-05 13:13:53,041: t15.2025.01.12 val PER: 0.1424
2026-01-05 13:13:53,041: t15.2025.03.14 val PER: 0.3536
2026-01-05 13:13:53,042: t15.2025.03.16 val PER: 0.1872
2026-01-05 13:13:53,042: t15.2025.03.30 val PER: 0.3000
2026-01-05 13:13:53,042: t15.2025.04.13 val PER: 0.2197
2026-01-05 13:13:53,335: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_13500
2026-01-05 13:14:02,855: Train batch 13600: loss: 12.57 grad norm: 63.65 time: 0.062
2026-01-05 13:14:21,631: Train batch 13800: loss: 8.98 grad norm: 55.06 time: 0.056
2026-01-05 13:14:40,407: Train batch 14000: loss: 11.84 grad norm: 59.79 time: 0.051
2026-01-05 13:14:40,408: Running test after training batch: 14000
2026-01-05 13:14:40,523: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:14:45,674: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:14:45,711: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost get
2026-01-05 13:14:47,671: Val batch 14000: PER (avg): 0.1518 CTC Loss (avg): 15.5092 WER(1gram): 46.95% (n=64) time: 7.264
2026-01-05 13:14:47,672: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 13:14:47,672: t15.2023.08.13 val PER: 0.1102
2026-01-05 13:14:47,672: t15.2023.08.18 val PER: 0.1056
2026-01-05 13:14:47,672: t15.2023.08.20 val PER: 0.1104
2026-01-05 13:14:47,672: t15.2023.08.25 val PER: 0.0904
2026-01-05 13:14:47,672: t15.2023.08.27 val PER: 0.1865
2026-01-05 13:14:47,673: t15.2023.09.01 val PER: 0.0828
2026-01-05 13:14:47,673: t15.2023.09.03 val PER: 0.1710
2026-01-05 13:14:47,673: t15.2023.09.24 val PER: 0.1286
2026-01-05 13:14:47,673: t15.2023.09.29 val PER: 0.1283
2026-01-05 13:14:47,673: t15.2023.10.01 val PER: 0.1757
2026-01-05 13:14:47,673: t15.2023.10.06 val PER: 0.0775
2026-01-05 13:14:47,673: t15.2023.10.08 val PER: 0.2598
2026-01-05 13:14:47,673: t15.2023.10.13 val PER: 0.1986
2026-01-05 13:14:47,673: t15.2023.10.15 val PER: 0.1575
2026-01-05 13:14:47,673: t15.2023.10.20 val PER: 0.2013
2026-01-05 13:14:47,673: t15.2023.10.22 val PER: 0.1136
2026-01-05 13:14:47,674: t15.2023.11.03 val PER: 0.1805
2026-01-05 13:14:47,674: t15.2023.11.04 val PER: 0.0341
2026-01-05 13:14:47,679: t15.2023.11.17 val PER: 0.0404
2026-01-05 13:14:47,679: t15.2023.11.19 val PER: 0.0240
2026-01-05 13:14:47,679: t15.2023.11.26 val PER: 0.1283
2026-01-05 13:14:47,679: t15.2023.12.03 val PER: 0.1261
2026-01-05 13:14:47,679: t15.2023.12.08 val PER: 0.1059
2026-01-05 13:14:47,680: t15.2023.12.10 val PER: 0.0933
2026-01-05 13:14:47,680: t15.2023.12.17 val PER: 0.1341
2026-01-05 13:14:47,680: t15.2023.12.29 val PER: 0.1325
2026-01-05 13:14:47,680: t15.2024.02.25 val PER: 0.1096
2026-01-05 13:14:47,680: t15.2024.03.08 val PER: 0.2319
2026-01-05 13:14:47,680: t15.2024.03.15 val PER: 0.2045
2026-01-05 13:14:47,680: t15.2024.03.17 val PER: 0.1430
2026-01-05 13:14:47,680: t15.2024.05.10 val PER: 0.1545
2026-01-05 13:14:47,680: t15.2024.06.14 val PER: 0.1609
2026-01-05 13:14:47,680: t15.2024.07.19 val PER: 0.2301
2026-01-05 13:14:47,680: t15.2024.07.21 val PER: 0.0966
2026-01-05 13:14:47,681: t15.2024.07.28 val PER: 0.1324
2026-01-05 13:14:47,681: t15.2025.01.10 val PER: 0.3044
2026-01-05 13:14:47,681: t15.2025.01.12 val PER: 0.1409
2026-01-05 13:14:47,681: t15.2025.03.14 val PER: 0.3476
2026-01-05 13:14:47,681: t15.2025.03.16 val PER: 0.1859
2026-01-05 13:14:47,681: t15.2025.03.30 val PER: 0.2920
2026-01-05 13:14:47,681: t15.2025.04.13 val PER: 0.2154
2026-01-05 13:14:47,681: New best val WER(1gram) 47.21% --> 46.95%
2026-01-05 13:14:47,681: Checkpointing model
2026-01-05 13:14:48,347: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:14:48,656: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_14000
2026-01-05 13:15:07,271: Train batch 14200: loss: 7.91 grad norm: 50.32 time: 0.056
2026-01-05 13:15:26,248: Train batch 14400: loss: 5.67 grad norm: 39.35 time: 0.064
2026-01-05 13:15:35,663: Running test after training batch: 14500
2026-01-05 13:15:35,780: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:15:41,200: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:15:41,235: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 13:15:43,123: Val batch 14500: PER (avg): 0.1526 CTC Loss (avg): 15.5786 WER(1gram): 46.70% (n=64) time: 7.459
2026-01-05 13:15:43,123: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 13:15:43,124: t15.2023.08.13 val PER: 0.1154
2026-01-05 13:15:43,124: t15.2023.08.18 val PER: 0.1073
2026-01-05 13:15:43,124: t15.2023.08.20 val PER: 0.1072
2026-01-05 13:15:43,124: t15.2023.08.25 val PER: 0.0843
2026-01-05 13:15:43,124: t15.2023.08.27 val PER: 0.1929
2026-01-05 13:15:43,124: t15.2023.09.01 val PER: 0.0828
2026-01-05 13:15:43,124: t15.2023.09.03 val PER: 0.1663
2026-01-05 13:15:43,124: t15.2023.09.24 val PER: 0.1323
2026-01-05 13:15:43,124: t15.2023.09.29 val PER: 0.1276
2026-01-05 13:15:43,124: t15.2023.10.01 val PER: 0.1810
2026-01-05 13:15:43,124: t15.2023.10.06 val PER: 0.0818
2026-01-05 13:15:43,125: t15.2023.10.08 val PER: 0.2463
2026-01-05 13:15:43,125: t15.2023.10.13 val PER: 0.2079
2026-01-05 13:15:43,125: t15.2023.10.15 val PER: 0.1602
2026-01-05 13:15:43,125: t15.2023.10.20 val PER: 0.1812
2026-01-05 13:15:43,125: t15.2023.10.22 val PER: 0.1125
2026-01-05 13:15:43,125: t15.2023.11.03 val PER: 0.1818
2026-01-05 13:15:43,125: t15.2023.11.04 val PER: 0.0341
2026-01-05 13:15:43,125: t15.2023.11.17 val PER: 0.0404
2026-01-05 13:15:43,125: t15.2023.11.19 val PER: 0.0299
2026-01-05 13:15:43,125: t15.2023.11.26 val PER: 0.1217
2026-01-05 13:15:43,125: t15.2023.12.03 val PER: 0.1134
2026-01-05 13:15:43,125: t15.2023.12.08 val PER: 0.0979
2026-01-05 13:15:43,126: t15.2023.12.10 val PER: 0.0894
2026-01-05 13:15:43,126: t15.2023.12.17 val PER: 0.1445
2026-01-05 13:15:43,126: t15.2023.12.29 val PER: 0.1332
2026-01-05 13:15:43,126: t15.2024.02.25 val PER: 0.1152
2026-01-05 13:15:43,126: t15.2024.03.08 val PER: 0.2376
2026-01-05 13:15:43,126: t15.2024.03.15 val PER: 0.2083
2026-01-05 13:15:43,126: t15.2024.03.17 val PER: 0.1381
2026-01-05 13:15:43,126: t15.2024.05.10 val PER: 0.1530
2026-01-05 13:15:43,126: t15.2024.06.14 val PER: 0.1688
2026-01-05 13:15:43,126: t15.2024.07.19 val PER: 0.2413
2026-01-05 13:15:43,126: t15.2024.07.21 val PER: 0.0952
2026-01-05 13:15:43,126: t15.2024.07.28 val PER: 0.1397
2026-01-05 13:15:43,127: t15.2025.01.10 val PER: 0.3003
2026-01-05 13:15:43,127: t15.2025.01.12 val PER: 0.1440
2026-01-05 13:15:43,127: t15.2025.03.14 val PER: 0.3491
2026-01-05 13:15:43,127: t15.2025.03.16 val PER: 0.1819
2026-01-05 13:15:43,127: t15.2025.03.30 val PER: 0.2954
2026-01-05 13:15:43,127: t15.2025.04.13 val PER: 0.2211
2026-01-05 13:15:43,128: New best val WER(1gram) 46.95% --> 46.70%
2026-01-05 13:15:43,128: Checkpointing model
2026-01-05 13:15:43,769: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:15:44,100: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_14500
2026-01-05 13:15:53,412: Train batch 14600: loss: 12.15 grad norm: 60.58 time: 0.058
2026-01-05 13:16:12,357: Train batch 14800: loss: 5.81 grad norm: 42.85 time: 0.050
2026-01-05 13:16:30,949: Train batch 15000: loss: 8.58 grad norm: 47.26 time: 0.052
2026-01-05 13:16:30,949: Running test after training batch: 15000
2026-01-05 13:16:31,090: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:16:36,652: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 13:16:36,688: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-05 13:16:38,615: Val batch 15000: PER (avg): 0.1492 CTC Loss (avg): 15.3264 WER(1gram): 47.21% (n=64) time: 7.665
2026-01-05 13:16:38,615: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-05 13:16:38,615: t15.2023.08.13 val PER: 0.1123
2026-01-05 13:16:38,615: t15.2023.08.18 val PER: 0.0930
2026-01-05 13:16:38,615: t15.2023.08.20 val PER: 0.1072
2026-01-05 13:16:38,615: t15.2023.08.25 val PER: 0.0934
2026-01-05 13:16:38,615: t15.2023.08.27 val PER: 0.1817
2026-01-05 13:16:38,616: t15.2023.09.01 val PER: 0.0812
2026-01-05 13:16:38,616: t15.2023.09.03 val PER: 0.1568
2026-01-05 13:16:38,616: t15.2023.09.24 val PER: 0.1335
2026-01-05 13:16:38,616: t15.2023.09.29 val PER: 0.1264
2026-01-05 13:16:38,616: t15.2023.10.01 val PER: 0.1724
2026-01-05 13:16:38,616: t15.2023.10.06 val PER: 0.0753
2026-01-05 13:16:38,616: t15.2023.10.08 val PER: 0.2544
2026-01-05 13:16:38,616: t15.2023.10.13 val PER: 0.1978
2026-01-05 13:16:38,616: t15.2023.10.15 val PER: 0.1510
2026-01-05 13:16:38,616: t15.2023.10.20 val PER: 0.2013
2026-01-05 13:16:38,616: t15.2023.10.22 val PER: 0.1136
2026-01-05 13:16:38,616: t15.2023.11.03 val PER: 0.1791
2026-01-05 13:16:38,616: t15.2023.11.04 val PER: 0.0375
2026-01-05 13:16:38,617: t15.2023.11.17 val PER: 0.0404
2026-01-05 13:16:38,617: t15.2023.11.19 val PER: 0.0339
2026-01-05 13:16:38,617: t15.2023.11.26 val PER: 0.1225
2026-01-05 13:16:38,617: t15.2023.12.03 val PER: 0.1187
2026-01-05 13:16:38,617: t15.2023.12.08 val PER: 0.1019
2026-01-05 13:16:38,617: t15.2023.12.10 val PER: 0.0828
2026-01-05 13:16:38,617: t15.2023.12.17 val PER: 0.1383
2026-01-05 13:16:38,617: t15.2023.12.29 val PER: 0.1318
2026-01-05 13:16:38,617: t15.2024.02.25 val PER: 0.1025
2026-01-05 13:16:38,617: t15.2024.03.08 val PER: 0.2219
2026-01-05 13:16:38,617: t15.2024.03.15 val PER: 0.2014
2026-01-05 13:16:38,617: t15.2024.03.17 val PER: 0.1304
2026-01-05 13:16:38,618: t15.2024.05.10 val PER: 0.1649
2026-01-05 13:16:38,618: t15.2024.06.14 val PER: 0.1640
2026-01-05 13:16:38,618: t15.2024.07.19 val PER: 0.2334
2026-01-05 13:16:38,618: t15.2024.07.21 val PER: 0.0938
2026-01-05 13:16:38,618: t15.2024.07.28 val PER: 0.1324
2026-01-05 13:16:38,618: t15.2025.01.10 val PER: 0.3113
2026-01-05 13:16:38,618: t15.2025.01.12 val PER: 0.1386
2026-01-05 13:16:38,618: t15.2025.03.14 val PER: 0.3388
2026-01-05 13:16:38,618: t15.2025.03.16 val PER: 0.1950
2026-01-05 13:16:38,618: t15.2025.03.30 val PER: 0.2828
2026-01-05 13:16:38,618: t15.2025.04.13 val PER: 0.2126
2026-01-05 13:16:38,917: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_15000
2026-01-05 13:16:58,433: Train batch 15200: loss: 5.08 grad norm: 43.85 time: 0.057
2026-01-05 13:17:17,194: Train batch 15400: loss: 11.10 grad norm: 56.29 time: 0.049
2026-01-05 13:17:26,708: Running test after training batch: 15500
2026-01-05 13:17:26,817: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:17:31,933: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:17:31,969: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 13:17:33,860: Val batch 15500: PER (avg): 0.1492 CTC Loss (avg): 15.2539 WER(1gram): 44.67% (n=64) time: 7.151
2026-01-05 13:17:33,860: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 13:17:33,860: t15.2023.08.13 val PER: 0.1133
2026-01-05 13:17:33,861: t15.2023.08.18 val PER: 0.1023
2026-01-05 13:17:33,861: t15.2023.08.20 val PER: 0.1096
2026-01-05 13:17:33,861: t15.2023.08.25 val PER: 0.0934
2026-01-05 13:17:33,861: t15.2023.08.27 val PER: 0.1897
2026-01-05 13:17:33,861: t15.2023.09.01 val PER: 0.0804
2026-01-05 13:17:33,861: t15.2023.09.03 val PER: 0.1627
2026-01-05 13:17:33,861: t15.2023.09.24 val PER: 0.1238
2026-01-05 13:17:33,861: t15.2023.09.29 val PER: 0.1283
2026-01-05 13:17:33,861: t15.2023.10.01 val PER: 0.1711
2026-01-05 13:17:33,861: t15.2023.10.06 val PER: 0.0775
2026-01-05 13:17:33,862: t15.2023.10.08 val PER: 0.2544
2026-01-05 13:17:33,862: t15.2023.10.13 val PER: 0.1971
2026-01-05 13:17:33,862: t15.2023.10.15 val PER: 0.1490
2026-01-05 13:17:33,862: t15.2023.10.20 val PER: 0.1913
2026-01-05 13:17:33,862: t15.2023.10.22 val PER: 0.1169
2026-01-05 13:17:33,862: t15.2023.11.03 val PER: 0.1737
2026-01-05 13:17:33,862: t15.2023.11.04 val PER: 0.0341
2026-01-05 13:17:33,863: t15.2023.11.17 val PER: 0.0358
2026-01-05 13:17:33,863: t15.2023.11.19 val PER: 0.0339
2026-01-05 13:17:33,863: t15.2023.11.26 val PER: 0.1174
2026-01-05 13:17:33,863: t15.2023.12.03 val PER: 0.1187
2026-01-05 13:17:33,863: t15.2023.12.08 val PER: 0.0945
2026-01-05 13:17:33,863: t15.2023.12.10 val PER: 0.0894
2026-01-05 13:17:33,863: t15.2023.12.17 val PER: 0.1279
2026-01-05 13:17:33,863: t15.2023.12.29 val PER: 0.1352
2026-01-05 13:17:33,863: t15.2024.02.25 val PER: 0.1025
2026-01-05 13:17:33,863: t15.2024.03.08 val PER: 0.2390
2026-01-05 13:17:33,863: t15.2024.03.15 val PER: 0.2026
2026-01-05 13:17:33,863: t15.2024.03.17 val PER: 0.1332
2026-01-05 13:17:33,863: t15.2024.05.10 val PER: 0.1530
2026-01-05 13:17:33,863: t15.2024.06.14 val PER: 0.1577
2026-01-05 13:17:33,863: t15.2024.07.19 val PER: 0.2432
2026-01-05 13:17:33,863: t15.2024.07.21 val PER: 0.0924
2026-01-05 13:17:33,864: t15.2024.07.28 val PER: 0.1368
2026-01-05 13:17:33,864: t15.2025.01.10 val PER: 0.3044
2026-01-05 13:17:33,864: t15.2025.01.12 val PER: 0.1386
2026-01-05 13:17:33,864: t15.2025.03.14 val PER: 0.3358
2026-01-05 13:17:33,864: t15.2025.03.16 val PER: 0.1872
2026-01-05 13:17:33,864: t15.2025.03.30 val PER: 0.2908
2026-01-05 13:17:33,864: t15.2025.04.13 val PER: 0.2068
2026-01-05 13:17:33,865: New best val WER(1gram) 46.70% --> 44.67%
2026-01-05 13:17:33,865: Checkpointing model
2026-01-05 13:17:34,505: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:17:34,830: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_15500
2026-01-05 13:17:44,262: Train batch 15600: loss: 11.62 grad norm: 60.46 time: 0.062
2026-01-05 13:18:03,423: Train batch 15800: loss: 13.51 grad norm: 63.12 time: 0.067
2026-01-05 13:18:22,793: Train batch 16000: loss: 8.41 grad norm: 46.35 time: 0.056
2026-01-05 13:18:22,794: Running test after training batch: 16000
2026-01-05 13:18:22,963: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:18:28,077: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:18:28,112: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost get
2026-01-05 13:18:30,057: Val batch 16000: PER (avg): 0.1495 CTC Loss (avg): 15.2768 WER(1gram): 46.19% (n=64) time: 7.263
2026-01-05 13:18:30,057: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-05 13:18:30,057: t15.2023.08.13 val PER: 0.1112
2026-01-05 13:18:30,057: t15.2023.08.18 val PER: 0.1031
2026-01-05 13:18:30,057: t15.2023.08.20 val PER: 0.1080
2026-01-05 13:18:30,057: t15.2023.08.25 val PER: 0.0904
2026-01-05 13:18:30,058: t15.2023.08.27 val PER: 0.1849
2026-01-05 13:18:30,058: t15.2023.09.01 val PER: 0.0804
2026-01-05 13:18:30,058: t15.2023.09.03 val PER: 0.1580
2026-01-05 13:18:30,058: t15.2023.09.24 val PER: 0.1286
2026-01-05 13:18:30,058: t15.2023.09.29 val PER: 0.1334
2026-01-05 13:18:30,058: t15.2023.10.01 val PER: 0.1724
2026-01-05 13:18:30,058: t15.2023.10.06 val PER: 0.0797
2026-01-05 13:18:30,058: t15.2023.10.08 val PER: 0.2517
2026-01-05 13:18:30,058: t15.2023.10.13 val PER: 0.1994
2026-01-05 13:18:30,058: t15.2023.10.15 val PER: 0.1496
2026-01-05 13:18:30,058: t15.2023.10.20 val PER: 0.1946
2026-01-05 13:18:30,058: t15.2023.10.22 val PER: 0.1125
2026-01-05 13:18:30,058: t15.2023.11.03 val PER: 0.1744
2026-01-05 13:18:30,058: t15.2023.11.04 val PER: 0.0375
2026-01-05 13:18:30,059: t15.2023.11.17 val PER: 0.0404
2026-01-05 13:18:30,059: t15.2023.11.19 val PER: 0.0359
2026-01-05 13:18:30,059: t15.2023.11.26 val PER: 0.1130
2026-01-05 13:18:30,059: t15.2023.12.03 val PER: 0.1229
2026-01-05 13:18:30,059: t15.2023.12.08 val PER: 0.1032
2026-01-05 13:18:30,059: t15.2023.12.10 val PER: 0.0867
2026-01-05 13:18:30,059: t15.2023.12.17 val PER: 0.1299
2026-01-05 13:18:30,059: t15.2023.12.29 val PER: 0.1263
2026-01-05 13:18:30,059: t15.2024.02.25 val PER: 0.0955
2026-01-05 13:18:30,059: t15.2024.03.08 val PER: 0.2418
2026-01-05 13:18:30,059: t15.2024.03.15 val PER: 0.1995
2026-01-05 13:18:30,060: t15.2024.03.17 val PER: 0.1374
2026-01-05 13:18:30,060: t15.2024.05.10 val PER: 0.1545
2026-01-05 13:18:30,060: t15.2024.06.14 val PER: 0.1530
2026-01-05 13:18:30,060: t15.2024.07.19 val PER: 0.2360
2026-01-05 13:18:30,060: t15.2024.07.21 val PER: 0.0945
2026-01-05 13:18:30,060: t15.2024.07.28 val PER: 0.1397
2026-01-05 13:18:30,060: t15.2025.01.10 val PER: 0.2961
2026-01-05 13:18:30,065: t15.2025.01.12 val PER: 0.1386
2026-01-05 13:18:30,065: t15.2025.03.14 val PER: 0.3550
2026-01-05 13:18:30,065: t15.2025.03.16 val PER: 0.1937
2026-01-05 13:18:30,065: t15.2025.03.30 val PER: 0.2862
2026-01-05 13:18:30,065: t15.2025.04.13 val PER: 0.2140
2026-01-05 13:18:30,360: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_16000
2026-01-05 13:18:49,229: Train batch 16200: loss: 5.96 grad norm: 42.37 time: 0.056
2026-01-05 13:19:08,199: Train batch 16400: loss: 10.30 grad norm: 61.71 time: 0.057
2026-01-05 13:19:17,846: Running test after training batch: 16500
2026-01-05 13:19:17,990: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:19:23,184: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:19:23,221: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost get
2026-01-05 13:19:25,176: Val batch 16500: PER (avg): 0.1477 CTC Loss (avg): 15.1818 WER(1gram): 44.92% (n=64) time: 7.329
2026-01-05 13:19:25,176: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-05 13:19:25,176: t15.2023.08.13 val PER: 0.1123
2026-01-05 13:19:25,176: t15.2023.08.18 val PER: 0.0956
2026-01-05 13:19:25,176: t15.2023.08.20 val PER: 0.1072
2026-01-05 13:19:25,176: t15.2023.08.25 val PER: 0.0843
2026-01-05 13:19:25,177: t15.2023.08.27 val PER: 0.1785
2026-01-05 13:19:25,177: t15.2023.09.01 val PER: 0.0820
2026-01-05 13:19:25,177: t15.2023.09.03 val PER: 0.1663
2026-01-05 13:19:25,177: t15.2023.09.24 val PER: 0.1274
2026-01-05 13:19:25,177: t15.2023.09.29 val PER: 0.1270
2026-01-05 13:19:25,177: t15.2023.10.01 val PER: 0.1757
2026-01-05 13:19:25,177: t15.2023.10.06 val PER: 0.0786
2026-01-05 13:19:25,177: t15.2023.10.08 val PER: 0.2503
2026-01-05 13:19:25,177: t15.2023.10.13 val PER: 0.1947
2026-01-05 13:19:25,177: t15.2023.10.15 val PER: 0.1496
2026-01-05 13:19:25,177: t15.2023.10.20 val PER: 0.1846
2026-01-05 13:19:25,177: t15.2023.10.22 val PER: 0.1136
2026-01-05 13:19:25,177: t15.2023.11.03 val PER: 0.1723
2026-01-05 13:19:25,177: t15.2023.11.04 val PER: 0.0307
2026-01-05 13:19:25,178: t15.2023.11.17 val PER: 0.0342
2026-01-05 13:19:25,178: t15.2023.11.19 val PER: 0.0339
2026-01-05 13:19:25,178: t15.2023.11.26 val PER: 0.1167
2026-01-05 13:19:25,178: t15.2023.12.03 val PER: 0.1197
2026-01-05 13:19:25,178: t15.2023.12.08 val PER: 0.0959
2026-01-05 13:19:25,178: t15.2023.12.10 val PER: 0.0867
2026-01-05 13:19:25,178: t15.2023.12.17 val PER: 0.1279
2026-01-05 13:19:25,178: t15.2023.12.29 val PER: 0.1235
2026-01-05 13:19:25,178: t15.2024.02.25 val PER: 0.1025
2026-01-05 13:19:25,178: t15.2024.03.08 val PER: 0.2319
2026-01-05 13:19:25,178: t15.2024.03.15 val PER: 0.1995
2026-01-05 13:19:25,179: t15.2024.03.17 val PER: 0.1304
2026-01-05 13:19:25,179: t15.2024.05.10 val PER: 0.1486
2026-01-05 13:19:25,179: t15.2024.06.14 val PER: 0.1640
2026-01-05 13:19:25,179: t15.2024.07.19 val PER: 0.2386
2026-01-05 13:19:25,179: t15.2024.07.21 val PER: 0.0910
2026-01-05 13:19:25,179: t15.2024.07.28 val PER: 0.1346
2026-01-05 13:19:25,179: t15.2025.01.10 val PER: 0.2893
2026-01-05 13:19:25,179: t15.2025.01.12 val PER: 0.1409
2026-01-05 13:19:25,179: t15.2025.03.14 val PER: 0.3491
2026-01-05 13:19:25,179: t15.2025.03.16 val PER: 0.1859
2026-01-05 13:19:25,179: t15.2025.03.30 val PER: 0.2862
2026-01-05 13:19:25,179: t15.2025.04.13 val PER: 0.2154
2026-01-05 13:19:25,483: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_16500
2026-01-05 13:19:35,004: Train batch 16600: loss: 8.55 grad norm: 52.53 time: 0.052
2026-01-05 13:19:54,114: Train batch 16800: loss: 16.30 grad norm: 73.01 time: 0.061
2026-01-05 13:20:13,173: Train batch 17000: loss: 7.80 grad norm: 45.65 time: 0.082
2026-01-05 13:20:13,173: Running test after training batch: 17000
2026-01-05 13:20:13,290: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:20:18,440: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:20:18,477: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 13:20:20,458: Val batch 17000: PER (avg): 0.1479 CTC Loss (avg): 15.0946 WER(1gram): 46.45% (n=64) time: 7.285
2026-01-05 13:20:20,458: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-05 13:20:20,459: t15.2023.08.13 val PER: 0.1154
2026-01-05 13:20:20,459: t15.2023.08.18 val PER: 0.1023
2026-01-05 13:20:20,459: t15.2023.08.20 val PER: 0.1001
2026-01-05 13:20:20,459: t15.2023.08.25 val PER: 0.0858
2026-01-05 13:20:20,459: t15.2023.08.27 val PER: 0.1785
2026-01-05 13:20:20,459: t15.2023.09.01 val PER: 0.0739
2026-01-05 13:20:20,459: t15.2023.09.03 val PER: 0.1615
2026-01-05 13:20:20,459: t15.2023.09.24 val PER: 0.1250
2026-01-05 13:20:20,459: t15.2023.09.29 val PER: 0.1308
2026-01-05 13:20:20,459: t15.2023.10.01 val PER: 0.1691
2026-01-05 13:20:20,459: t15.2023.10.06 val PER: 0.0797
2026-01-05 13:20:20,459: t15.2023.10.08 val PER: 0.2463
2026-01-05 13:20:20,459: t15.2023.10.13 val PER: 0.1986
2026-01-05 13:20:20,460: t15.2023.10.15 val PER: 0.1536
2026-01-05 13:20:20,460: t15.2023.10.20 val PER: 0.1812
2026-01-05 13:20:20,460: t15.2023.10.22 val PER: 0.1069
2026-01-05 13:20:20,460: t15.2023.11.03 val PER: 0.1744
2026-01-05 13:20:20,460: t15.2023.11.04 val PER: 0.0375
2026-01-05 13:20:20,460: t15.2023.11.17 val PER: 0.0373
2026-01-05 13:20:20,460: t15.2023.11.19 val PER: 0.0339
2026-01-05 13:20:20,460: t15.2023.11.26 val PER: 0.1116
2026-01-05 13:20:20,460: t15.2023.12.03 val PER: 0.1187
2026-01-05 13:20:20,460: t15.2023.12.08 val PER: 0.0972
2026-01-05 13:20:20,460: t15.2023.12.10 val PER: 0.0907
2026-01-05 13:20:20,461: t15.2023.12.17 val PER: 0.1372
2026-01-05 13:20:20,461: t15.2023.12.29 val PER: 0.1311
2026-01-05 13:20:20,461: t15.2024.02.25 val PER: 0.1067
2026-01-05 13:20:20,461: t15.2024.03.08 val PER: 0.2404
2026-01-05 13:20:20,461: t15.2024.03.15 val PER: 0.2020
2026-01-05 13:20:20,461: t15.2024.03.17 val PER: 0.1339
2026-01-05 13:20:20,461: t15.2024.05.10 val PER: 0.1456
2026-01-05 13:20:20,461: t15.2024.06.14 val PER: 0.1609
2026-01-05 13:20:20,461: t15.2024.07.19 val PER: 0.2367
2026-01-05 13:20:20,461: t15.2024.07.21 val PER: 0.0917
2026-01-05 13:20:20,461: t15.2024.07.28 val PER: 0.1338
2026-01-05 13:20:20,461: t15.2025.01.10 val PER: 0.2989
2026-01-05 13:20:20,461: t15.2025.01.12 val PER: 0.1347
2026-01-05 13:20:20,461: t15.2025.03.14 val PER: 0.3432
2026-01-05 13:20:20,462: t15.2025.03.16 val PER: 0.1885
2026-01-05 13:20:20,462: t15.2025.03.30 val PER: 0.2862
2026-01-05 13:20:20,462: t15.2025.04.13 val PER: 0.2054
2026-01-05 13:20:20,767: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_17000
2026-01-05 13:20:39,640: Train batch 17200: loss: 9.25 grad norm: 51.11 time: 0.084
2026-01-05 13:20:58,905: Train batch 17400: loss: 11.67 grad norm: 59.18 time: 0.071
2026-01-05 13:21:08,348: Running test after training batch: 17500
2026-01-05 13:21:08,452: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:21:13,583: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:21:13,619: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost get
2026-01-05 13:21:15,569: Val batch 17500: PER (avg): 0.1458 CTC Loss (avg): 15.0201 WER(1gram): 45.43% (n=64) time: 7.221
2026-01-05 13:21:15,570: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-05 13:21:15,570: t15.2023.08.13 val PER: 0.1143
2026-01-05 13:21:15,570: t15.2023.08.18 val PER: 0.0981
2026-01-05 13:21:15,570: t15.2023.08.20 val PER: 0.1064
2026-01-05 13:21:15,570: t15.2023.08.25 val PER: 0.0873
2026-01-05 13:21:15,570: t15.2023.08.27 val PER: 0.1752
2026-01-05 13:21:15,570: t15.2023.09.01 val PER: 0.0731
2026-01-05 13:21:15,570: t15.2023.09.03 val PER: 0.1639
2026-01-05 13:21:15,570: t15.2023.09.24 val PER: 0.1299
2026-01-05 13:21:15,571: t15.2023.09.29 val PER: 0.1283
2026-01-05 13:21:15,571: t15.2023.10.01 val PER: 0.1704
2026-01-05 13:21:15,571: t15.2023.10.06 val PER: 0.0775
2026-01-05 13:21:15,571: t15.2023.10.08 val PER: 0.2490
2026-01-05 13:21:15,571: t15.2023.10.13 val PER: 0.1916
2026-01-05 13:21:15,571: t15.2023.10.15 val PER: 0.1463
2026-01-05 13:21:15,571: t15.2023.10.20 val PER: 0.1846
2026-01-05 13:21:15,571: t15.2023.10.22 val PER: 0.1091
2026-01-05 13:21:15,571: t15.2023.11.03 val PER: 0.1703
2026-01-05 13:21:15,571: t15.2023.11.04 val PER: 0.0375
2026-01-05 13:21:15,571: t15.2023.11.17 val PER: 0.0389
2026-01-05 13:21:15,572: t15.2023.11.19 val PER: 0.0299
2026-01-05 13:21:15,572: t15.2023.11.26 val PER: 0.1101
2026-01-05 13:21:15,572: t15.2023.12.03 val PER: 0.1092
2026-01-05 13:21:15,572: t15.2023.12.08 val PER: 0.0965
2026-01-05 13:21:15,572: t15.2023.12.10 val PER: 0.0867
2026-01-05 13:21:15,572: t15.2023.12.17 val PER: 0.1320
2026-01-05 13:21:15,572: t15.2023.12.29 val PER: 0.1256
2026-01-05 13:21:15,572: t15.2024.02.25 val PER: 0.0997
2026-01-05 13:21:15,572: t15.2024.03.08 val PER: 0.2333
2026-01-05 13:21:15,572: t15.2024.03.15 val PER: 0.1914
2026-01-05 13:21:15,572: t15.2024.03.17 val PER: 0.1276
2026-01-05 13:21:15,572: t15.2024.05.10 val PER: 0.1516
2026-01-05 13:21:15,572: t15.2024.06.14 val PER: 0.1609
2026-01-05 13:21:15,572: t15.2024.07.19 val PER: 0.2347
2026-01-05 13:21:15,572: t15.2024.07.21 val PER: 0.0959
2026-01-05 13:21:15,573: t15.2024.07.28 val PER: 0.1309
2026-01-05 13:21:15,573: t15.2025.01.10 val PER: 0.2879
2026-01-05 13:21:15,573: t15.2025.01.12 val PER: 0.1324
2026-01-05 13:21:15,573: t15.2025.03.14 val PER: 0.3462
2026-01-05 13:21:15,573: t15.2025.03.16 val PER: 0.1872
2026-01-05 13:21:15,573: t15.2025.03.30 val PER: 0.2828
2026-01-05 13:21:15,573: t15.2025.04.13 val PER: 0.2140
2026-01-05 13:21:15,872: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_17500
2026-01-05 13:21:25,187: Train batch 17600: loss: 9.77 grad norm: 55.75 time: 0.052
2026-01-05 13:21:44,358: Train batch 17800: loss: 6.15 grad norm: 48.91 time: 0.042
2026-01-05 13:22:03,345: Train batch 18000: loss: 10.57 grad norm: 60.22 time: 0.061
2026-01-05 13:22:03,346: Running test after training batch: 18000
2026-01-05 13:22:03,502: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:22:08,854: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:22:08,890: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 13:22:10,842: Val batch 18000: PER (avg): 0.1451 CTC Loss (avg): 15.0087 WER(1gram): 45.69% (n=64) time: 7.496
2026-01-05 13:22:10,842: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-05 13:22:10,843: t15.2023.08.13 val PER: 0.1133
2026-01-05 13:22:10,843: t15.2023.08.18 val PER: 0.1014
2026-01-05 13:22:10,843: t15.2023.08.20 val PER: 0.1009
2026-01-05 13:22:10,843: t15.2023.08.25 val PER: 0.0889
2026-01-05 13:22:10,843: t15.2023.08.27 val PER: 0.1833
2026-01-05 13:22:10,843: t15.2023.09.01 val PER: 0.0731
2026-01-05 13:22:10,844: t15.2023.09.03 val PER: 0.1520
2026-01-05 13:22:10,844: t15.2023.09.24 val PER: 0.1286
2026-01-05 13:22:10,844: t15.2023.09.29 val PER: 0.1283
2026-01-05 13:22:10,844: t15.2023.10.01 val PER: 0.1638
2026-01-05 13:22:10,844: t15.2023.10.06 val PER: 0.0743
2026-01-05 13:22:10,844: t15.2023.10.08 val PER: 0.2476
2026-01-05 13:22:10,844: t15.2023.10.13 val PER: 0.1885
2026-01-05 13:22:10,844: t15.2023.10.15 val PER: 0.1496
2026-01-05 13:22:10,844: t15.2023.10.20 val PER: 0.1913
2026-01-05 13:22:10,844: t15.2023.10.22 val PER: 0.1047
2026-01-05 13:22:10,844: t15.2023.11.03 val PER: 0.1784
2026-01-05 13:22:10,845: t15.2023.11.04 val PER: 0.0410
2026-01-05 13:22:10,845: t15.2023.11.17 val PER: 0.0373
2026-01-05 13:22:10,845: t15.2023.11.19 val PER: 0.0339
2026-01-05 13:22:10,845: t15.2023.11.26 val PER: 0.1058
2026-01-05 13:22:10,845: t15.2023.12.03 val PER: 0.1082
2026-01-05 13:22:10,845: t15.2023.12.08 val PER: 0.0919
2026-01-05 13:22:10,845: t15.2023.12.10 val PER: 0.0828
2026-01-05 13:22:10,845: t15.2023.12.17 val PER: 0.1362
2026-01-05 13:22:10,845: t15.2023.12.29 val PER: 0.1277
2026-01-05 13:22:10,846: t15.2024.02.25 val PER: 0.0997
2026-01-05 13:22:10,846: t15.2024.03.08 val PER: 0.2304
2026-01-05 13:22:10,846: t15.2024.03.15 val PER: 0.1945
2026-01-05 13:22:10,846: t15.2024.03.17 val PER: 0.1332
2026-01-05 13:22:10,846: t15.2024.05.10 val PER: 0.1456
2026-01-05 13:22:10,846: t15.2024.06.14 val PER: 0.1530
2026-01-05 13:22:10,846: t15.2024.07.19 val PER: 0.2320
2026-01-05 13:22:10,846: t15.2024.07.21 val PER: 0.0924
2026-01-05 13:22:10,846: t15.2024.07.28 val PER: 0.1316
2026-01-05 13:22:10,847: t15.2025.01.10 val PER: 0.2879
2026-01-05 13:22:10,847: t15.2025.01.12 val PER: 0.1316
2026-01-05 13:22:10,847: t15.2025.03.14 val PER: 0.3447
2026-01-05 13:22:10,847: t15.2025.03.16 val PER: 0.1976
2026-01-05 13:22:10,847: t15.2025.03.30 val PER: 0.2782
2026-01-05 13:22:10,847: t15.2025.04.13 val PER: 0.2097
2026-01-05 13:22:11,147: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_18000
2026-01-05 13:22:30,252: Train batch 18200: loss: 7.18 grad norm: 45.29 time: 0.073
2026-01-05 13:22:49,010: Train batch 18400: loss: 4.70 grad norm: 44.63 time: 0.059
2026-01-05 13:22:58,485: Running test after training batch: 18500
2026-01-05 13:22:58,588: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:23:03,654: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:23:03,690: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost get
2026-01-05 13:23:05,643: Val batch 18500: PER (avg): 0.1459 CTC Loss (avg): 15.0340 WER(1gram): 45.69% (n=64) time: 7.158
2026-01-05 13:23:05,644: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-05 13:23:05,644: t15.2023.08.13 val PER: 0.1123
2026-01-05 13:23:05,644: t15.2023.08.18 val PER: 0.1006
2026-01-05 13:23:05,644: t15.2023.08.20 val PER: 0.1025
2026-01-05 13:23:05,644: t15.2023.08.25 val PER: 0.0813
2026-01-05 13:23:05,644: t15.2023.08.27 val PER: 0.1865
2026-01-05 13:23:05,644: t15.2023.09.01 val PER: 0.0747
2026-01-05 13:23:05,644: t15.2023.09.03 val PER: 0.1532
2026-01-05 13:23:05,645: t15.2023.09.24 val PER: 0.1262
2026-01-05 13:23:05,645: t15.2023.09.29 val PER: 0.1257
2026-01-05 13:23:05,645: t15.2023.10.01 val PER: 0.1658
2026-01-05 13:23:05,645: t15.2023.10.06 val PER: 0.0797
2026-01-05 13:23:05,645: t15.2023.10.08 val PER: 0.2558
2026-01-05 13:23:05,645: t15.2023.10.13 val PER: 0.1955
2026-01-05 13:23:05,645: t15.2023.10.15 val PER: 0.1477
2026-01-05 13:23:05,645: t15.2023.10.20 val PER: 0.1913
2026-01-05 13:23:05,645: t15.2023.10.22 val PER: 0.1091
2026-01-05 13:23:05,645: t15.2023.11.03 val PER: 0.1744
2026-01-05 13:23:05,645: t15.2023.11.04 val PER: 0.0410
2026-01-05 13:23:05,645: t15.2023.11.17 val PER: 0.0342
2026-01-05 13:23:05,645: t15.2023.11.19 val PER: 0.0339
2026-01-05 13:23:05,645: t15.2023.11.26 val PER: 0.1130
2026-01-05 13:23:05,646: t15.2023.12.03 val PER: 0.1082
2026-01-05 13:23:05,646: t15.2023.12.08 val PER: 0.0932
2026-01-05 13:23:05,646: t15.2023.12.10 val PER: 0.0788
2026-01-05 13:23:05,646: t15.2023.12.17 val PER: 0.1299
2026-01-05 13:23:05,646: t15.2023.12.29 val PER: 0.1235
2026-01-05 13:23:05,646: t15.2024.02.25 val PER: 0.1025
2026-01-05 13:23:05,646: t15.2024.03.08 val PER: 0.2347
2026-01-05 13:23:05,646: t15.2024.03.15 val PER: 0.1970
2026-01-05 13:23:05,646: t15.2024.03.17 val PER: 0.1311
2026-01-05 13:23:05,646: t15.2024.05.10 val PER: 0.1456
2026-01-05 13:23:05,646: t15.2024.06.14 val PER: 0.1609
2026-01-05 13:23:05,646: t15.2024.07.19 val PER: 0.2380
2026-01-05 13:23:05,647: t15.2024.07.21 val PER: 0.0917
2026-01-05 13:23:05,647: t15.2024.07.28 val PER: 0.1375
2026-01-05 13:23:05,647: t15.2025.01.10 val PER: 0.2824
2026-01-05 13:23:05,647: t15.2025.01.12 val PER: 0.1386
2026-01-05 13:23:05,647: t15.2025.03.14 val PER: 0.3388
2026-01-05 13:23:05,647: t15.2025.03.16 val PER: 0.1976
2026-01-05 13:23:05,647: t15.2025.03.30 val PER: 0.2793
2026-01-05 13:23:05,648: t15.2025.04.13 val PER: 0.2054
2026-01-05 13:23:05,968: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_18500
2026-01-05 13:23:15,436: Train batch 18600: loss: 11.95 grad norm: 59.04 time: 0.067
2026-01-05 13:23:34,377: Train batch 18800: loss: 8.03 grad norm: 49.58 time: 0.064
2026-01-05 13:23:53,570: Train batch 19000: loss: 7.86 grad norm: 44.31 time: 0.064
2026-01-05 13:23:53,571: Running test after training batch: 19000
2026-01-05 13:23:53,724: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:23:58,863: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:23:58,900: WER debug example
  GT : how does it keep the cost down
  PR : houde dusts it keep the cost get
2026-01-05 13:24:00,882: Val batch 19000: PER (avg): 0.1461 CTC Loss (avg): 15.0447 WER(1gram): 45.43% (n=64) time: 7.311
2026-01-05 13:24:00,883: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-05 13:24:00,883: t15.2023.08.13 val PER: 0.1091
2026-01-05 13:24:00,883: t15.2023.08.18 val PER: 0.0989
2026-01-05 13:24:00,883: t15.2023.08.20 val PER: 0.1064
2026-01-05 13:24:00,883: t15.2023.08.25 val PER: 0.0843
2026-01-05 13:24:00,883: t15.2023.08.27 val PER: 0.1849
2026-01-05 13:24:00,883: t15.2023.09.01 val PER: 0.0763
2026-01-05 13:24:00,883: t15.2023.09.03 val PER: 0.1544
2026-01-05 13:24:00,883: t15.2023.09.24 val PER: 0.1299
2026-01-05 13:24:00,884: t15.2023.09.29 val PER: 0.1270
2026-01-05 13:24:00,884: t15.2023.10.01 val PER: 0.1684
2026-01-05 13:24:00,884: t15.2023.10.06 val PER: 0.0786
2026-01-05 13:24:00,884: t15.2023.10.08 val PER: 0.2530
2026-01-05 13:24:00,884: t15.2023.10.13 val PER: 0.1916
2026-01-05 13:24:00,884: t15.2023.10.15 val PER: 0.1463
2026-01-05 13:24:00,884: t15.2023.10.20 val PER: 0.1879
2026-01-05 13:24:00,884: t15.2023.10.22 val PER: 0.1102
2026-01-05 13:24:00,884: t15.2023.11.03 val PER: 0.1744
2026-01-05 13:24:00,884: t15.2023.11.04 val PER: 0.0410
2026-01-05 13:24:00,884: t15.2023.11.17 val PER: 0.0373
2026-01-05 13:24:00,884: t15.2023.11.19 val PER: 0.0379
2026-01-05 13:24:00,885: t15.2023.11.26 val PER: 0.1116
2026-01-05 13:24:00,885: t15.2023.12.03 val PER: 0.1124
2026-01-05 13:24:00,885: t15.2023.12.08 val PER: 0.0932
2026-01-05 13:24:00,885: t15.2023.12.10 val PER: 0.0828
2026-01-05 13:24:00,885: t15.2023.12.17 val PER: 0.1279
2026-01-05 13:24:00,885: t15.2023.12.29 val PER: 0.1235
2026-01-05 13:24:00,885: t15.2024.02.25 val PER: 0.0983
2026-01-05 13:24:00,885: t15.2024.03.08 val PER: 0.2361
2026-01-05 13:24:00,885: t15.2024.03.15 val PER: 0.1989
2026-01-05 13:24:00,885: t15.2024.03.17 val PER: 0.1297
2026-01-05 13:24:00,885: t15.2024.05.10 val PER: 0.1426
2026-01-05 13:24:00,885: t15.2024.06.14 val PER: 0.1625
2026-01-05 13:24:00,885: t15.2024.07.19 val PER: 0.2367
2026-01-05 13:24:00,885: t15.2024.07.21 val PER: 0.0903
2026-01-05 13:24:00,885: t15.2024.07.28 val PER: 0.1346
2026-01-05 13:24:00,885: t15.2025.01.10 val PER: 0.2810
2026-01-05 13:24:00,886: t15.2025.01.12 val PER: 0.1378
2026-01-05 13:24:00,886: t15.2025.03.14 val PER: 0.3373
2026-01-05 13:24:00,886: t15.2025.03.16 val PER: 0.2003
2026-01-05 13:24:00,886: t15.2025.03.30 val PER: 0.2828
2026-01-05 13:24:00,886: t15.2025.04.13 val PER: 0.2140
2026-01-05 13:24:01,183: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_19000
2026-01-05 13:24:20,306: Train batch 19200: loss: 5.79 grad norm: 45.73 time: 0.063
2026-01-05 13:24:39,752: Train batch 19400: loss: 4.86 grad norm: 35.99 time: 0.053
2026-01-05 13:24:49,226: Running test after training batch: 19500
2026-01-05 13:24:49,387: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:24:54,651: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:24:54,688: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost get
2026-01-05 13:24:56,680: Val batch 19500: PER (avg): 0.1450 CTC Loss (avg): 14.9710 WER(1gram): 45.43% (n=64) time: 7.454
2026-01-05 13:24:56,680: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-05 13:24:56,681: t15.2023.08.13 val PER: 0.1102
2026-01-05 13:24:56,681: t15.2023.08.18 val PER: 0.0972
2026-01-05 13:24:56,681: t15.2023.08.20 val PER: 0.1017
2026-01-05 13:24:56,681: t15.2023.08.25 val PER: 0.0843
2026-01-05 13:24:56,681: t15.2023.08.27 val PER: 0.1833
2026-01-05 13:24:56,681: t15.2023.09.01 val PER: 0.0731
2026-01-05 13:24:56,681: t15.2023.09.03 val PER: 0.1556
2026-01-05 13:24:56,681: t15.2023.09.24 val PER: 0.1299
2026-01-05 13:24:56,681: t15.2023.09.29 val PER: 0.1276
2026-01-05 13:24:56,682: t15.2023.10.01 val PER: 0.1704
2026-01-05 13:24:56,682: t15.2023.10.06 val PER: 0.0764
2026-01-05 13:24:56,682: t15.2023.10.08 val PER: 0.2490
2026-01-05 13:24:56,682: t15.2023.10.13 val PER: 0.1885
2026-01-05 13:24:56,682: t15.2023.10.15 val PER: 0.1490
2026-01-05 13:24:56,682: t15.2023.10.20 val PER: 0.1879
2026-01-05 13:24:56,682: t15.2023.10.22 val PER: 0.1047
2026-01-05 13:24:56,682: t15.2023.11.03 val PER: 0.1784
2026-01-05 13:24:56,682: t15.2023.11.04 val PER: 0.0410
2026-01-05 13:24:56,682: t15.2023.11.17 val PER: 0.0342
2026-01-05 13:24:56,682: t15.2023.11.19 val PER: 0.0339
2026-01-05 13:24:56,682: t15.2023.11.26 val PER: 0.1094
2026-01-05 13:24:56,682: t15.2023.12.03 val PER: 0.1082
2026-01-05 13:24:56,683: t15.2023.12.08 val PER: 0.0885
2026-01-05 13:24:56,683: t15.2023.12.10 val PER: 0.0828
2026-01-05 13:24:56,683: t15.2023.12.17 val PER: 0.1289
2026-01-05 13:24:56,683: t15.2023.12.29 val PER: 0.1242
2026-01-05 13:24:56,683: t15.2024.02.25 val PER: 0.1053
2026-01-05 13:24:56,683: t15.2024.03.08 val PER: 0.2319
2026-01-05 13:24:56,683: t15.2024.03.15 val PER: 0.1976
2026-01-05 13:24:56,683: t15.2024.03.17 val PER: 0.1297
2026-01-05 13:24:56,683: t15.2024.05.10 val PER: 0.1486
2026-01-05 13:24:56,683: t15.2024.06.14 val PER: 0.1562
2026-01-05 13:24:56,683: t15.2024.07.19 val PER: 0.2334
2026-01-05 13:24:56,683: t15.2024.07.21 val PER: 0.0924
2026-01-05 13:24:56,684: t15.2024.07.28 val PER: 0.1324
2026-01-05 13:24:56,684: t15.2025.01.10 val PER: 0.2796
2026-01-05 13:24:56,684: t15.2025.01.12 val PER: 0.1363
2026-01-05 13:24:56,684: t15.2025.03.14 val PER: 0.3476
2026-01-05 13:24:56,684: t15.2025.03.16 val PER: 0.1872
2026-01-05 13:24:56,684: t15.2025.03.30 val PER: 0.2793
2026-01-05 13:24:56,684: t15.2025.04.13 val PER: 0.2054
2026-01-05 13:24:56,989: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_19500
2026-01-05 13:25:06,429: Train batch 19600: loss: 7.79 grad norm: 47.35 time: 0.057
2026-01-05 13:25:25,584: Train batch 19800: loss: 7.23 grad norm: 51.91 time: 0.055
2026-01-05 13:25:44,340: Running test after training batch: 19999
2026-01-05 13:25:44,440: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:25:49,484: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:25:49,523: WER debug example
  GT : how does it keep the cost down
  PR : houde dusts it keep the cost get
2026-01-05 13:25:51,591: Val batch 19999: PER (avg): 0.1453 CTC Loss (avg): 14.9858 WER(1gram): 45.43% (n=64) time: 7.250
2026-01-05 13:25:51,591: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-05 13:25:51,591: t15.2023.08.13 val PER: 0.1091
2026-01-05 13:25:51,592: t15.2023.08.18 val PER: 0.0981
2026-01-05 13:25:51,592: t15.2023.08.20 val PER: 0.1009
2026-01-05 13:25:51,592: t15.2023.08.25 val PER: 0.0889
2026-01-05 13:25:51,592: t15.2023.08.27 val PER: 0.1785
2026-01-05 13:25:51,592: t15.2023.09.01 val PER: 0.0763
2026-01-05 13:25:51,592: t15.2023.09.03 val PER: 0.1580
2026-01-05 13:25:51,592: t15.2023.09.24 val PER: 0.1286
2026-01-05 13:25:51,592: t15.2023.09.29 val PER: 0.1276
2026-01-05 13:25:51,592: t15.2023.10.01 val PER: 0.1697
2026-01-05 13:25:51,592: t15.2023.10.06 val PER: 0.0775
2026-01-05 13:25:51,592: t15.2023.10.08 val PER: 0.2490
2026-01-05 13:25:51,592: t15.2023.10.13 val PER: 0.1916
2026-01-05 13:25:51,592: t15.2023.10.15 val PER: 0.1457
2026-01-05 13:25:51,593: t15.2023.10.20 val PER: 0.1879
2026-01-05 13:25:51,593: t15.2023.10.22 val PER: 0.1080
2026-01-05 13:25:51,593: t15.2023.11.03 val PER: 0.1757
2026-01-05 13:25:51,593: t15.2023.11.04 val PER: 0.0410
2026-01-05 13:25:51,593: t15.2023.11.17 val PER: 0.0342
2026-01-05 13:25:51,593: t15.2023.11.19 val PER: 0.0339
2026-01-05 13:25:51,593: t15.2023.11.26 val PER: 0.1109
2026-01-05 13:25:51,593: t15.2023.12.03 val PER: 0.1134
2026-01-05 13:25:51,593: t15.2023.12.08 val PER: 0.0892
2026-01-05 13:25:51,593: t15.2023.12.10 val PER: 0.0815
2026-01-05 13:25:51,593: t15.2023.12.17 val PER: 0.1341
2026-01-05 13:25:51,593: t15.2023.12.29 val PER: 0.1256
2026-01-05 13:25:51,593: t15.2024.02.25 val PER: 0.1025
2026-01-05 13:25:51,593: t15.2024.03.08 val PER: 0.2361
2026-01-05 13:25:51,593: t15.2024.03.15 val PER: 0.1957
2026-01-05 13:25:51,593: t15.2024.03.17 val PER: 0.1311
2026-01-05 13:25:51,594: t15.2024.05.10 val PER: 0.1486
2026-01-05 13:25:51,594: t15.2024.06.14 val PER: 0.1577
2026-01-05 13:25:51,594: t15.2024.07.19 val PER: 0.2301
2026-01-05 13:25:51,594: t15.2024.07.21 val PER: 0.0931
2026-01-05 13:25:51,594: t15.2024.07.28 val PER: 0.1324
2026-01-05 13:25:51,594: t15.2025.01.10 val PER: 0.2837
2026-01-05 13:25:51,594: t15.2025.01.12 val PER: 0.1355
2026-01-05 13:25:51,594: t15.2025.03.14 val PER: 0.3462
2026-01-05 13:25:51,594: t15.2025.03.16 val PER: 0.1872
2026-01-05 13:25:51,594: t15.2025.03.30 val PER: 0.2782
2026-01-05 13:25:51,595: t15.2025.04.13 val PER: 0.2083
2026-01-05 13:25:51,889: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/base_speckleFeat_wd1e-5/checkpoint/checkpoint_batch_19999
2026-01-05 13:25:51,922: Best avg val PER achieved: 0.14923
2026-01-05 13:25:51,922: Total training time: 37.37 minutes

=== RUN p002_wd1e-5.yaml ===
2026-01-05 13:25:57,191: Using device: cuda:0
2026-01-05 13:25:58,862: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-05 13:25:58,888: Using 45 sessions after filtering (from 45).
2026-01-05 13:25:59,300: Using torch.compile (if available)
2026-01-05 13:25:59,300: torch.compile not available (torch<2.0). Skipping.
2026-01-05 13:25:59,300: Initialized RNN decoding model
2026-01-05 13:25:59,300: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-05 13:25:59,301: Model has 44,907,305 parameters
2026-01-05 13:25:59,301: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-05 13:26:00,641: Successfully initialized datasets
2026-01-05 13:26:00,642: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-05 13:26:01,680: Train batch 0: loss: 579.44 grad norm: 1362.12 time: 0.198
2026-01-05 13:26:01,681: Running test after training batch: 0
2026-01-05 13:26:01,807: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:26:07,710: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-05 13:26:08,498: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-05 13:26:45,443: Val batch 0: PER (avg): 1.4298 CTC Loss (avg): 633.1467 WER(1gram): 100.00% (n=64) time: 43.762
2026-01-05 13:26:45,444: WER lens: avg_true_words=6.16 avg_pred_words=1.78 max_pred_words=4
2026-01-05 13:26:45,444: t15.2023.08.13 val PER: 1.3046
2026-01-05 13:26:45,444: t15.2023.08.18 val PER: 1.4225
2026-01-05 13:26:45,444: t15.2023.08.20 val PER: 1.3066
2026-01-05 13:26:45,444: t15.2023.08.25 val PER: 1.3373
2026-01-05 13:26:45,445: t15.2023.08.27 val PER: 1.2476
2026-01-05 13:26:45,445: t15.2023.09.01 val PER: 1.4537
2026-01-05 13:26:45,445: t15.2023.09.03 val PER: 1.3207
2026-01-05 13:26:45,445: t15.2023.09.24 val PER: 1.5328
2026-01-05 13:26:45,445: t15.2023.09.29 val PER: 1.4710
2026-01-05 13:26:45,445: t15.2023.10.01 val PER: 1.2120
2026-01-05 13:26:45,445: t15.2023.10.06 val PER: 1.4865
2026-01-05 13:26:45,445: t15.2023.10.08 val PER: 1.1800
2026-01-05 13:26:45,445: t15.2023.10.13 val PER: 1.4034
2026-01-05 13:26:45,445: t15.2023.10.15 val PER: 1.3889
2026-01-05 13:26:45,446: t15.2023.10.20 val PER: 1.4933
2026-01-05 13:26:45,446: t15.2023.10.22 val PER: 1.3942
2026-01-05 13:26:45,446: t15.2023.11.03 val PER: 1.5896
2026-01-05 13:26:45,446: t15.2023.11.04 val PER: 2.0137
2026-01-05 13:26:45,446: t15.2023.11.17 val PER: 1.9533
2026-01-05 13:26:45,446: t15.2023.11.19 val PER: 1.6826
2026-01-05 13:26:45,446: t15.2023.11.26 val PER: 1.5391
2026-01-05 13:26:45,446: t15.2023.12.03 val PER: 1.4233
2026-01-05 13:26:45,446: t15.2023.12.08 val PER: 1.4487
2026-01-05 13:26:45,446: t15.2023.12.10 val PER: 1.7017
2026-01-05 13:26:45,447: t15.2023.12.17 val PER: 1.3098
2026-01-05 13:26:45,447: t15.2023.12.29 val PER: 1.4118
2026-01-05 13:26:45,447: t15.2024.02.25 val PER: 1.4270
2026-01-05 13:26:45,447: t15.2024.03.08 val PER: 1.3186
2026-01-05 13:26:45,447: t15.2024.03.15 val PER: 1.3164
2026-01-05 13:26:45,447: t15.2024.03.17 val PER: 1.3996
2026-01-05 13:26:45,447: t15.2024.05.10 val PER: 1.3150
2026-01-05 13:26:45,447: t15.2024.06.14 val PER: 1.5331
2026-01-05 13:26:45,447: t15.2024.07.19 val PER: 1.0844
2026-01-05 13:26:45,447: t15.2024.07.21 val PER: 1.6366
2026-01-05 13:26:45,448: t15.2024.07.28 val PER: 1.6603
2026-01-05 13:26:45,448: t15.2025.01.10 val PER: 1.0909
2026-01-05 13:26:45,448: t15.2025.01.12 val PER: 1.7660
2026-01-05 13:26:45,448: t15.2025.03.14 val PER: 1.0385
2026-01-05 13:26:45,448: t15.2025.03.16 val PER: 1.6165
2026-01-05 13:26:45,448: t15.2025.03.30 val PER: 1.2943
2026-01-05 13:26:45,448: t15.2025.04.13 val PER: 1.5877
2026-01-05 13:26:45,449: New best val WER(1gram) inf% --> 100.00%
2026-01-05 13:26:45,449: Checkpointing model
2026-01-05 13:26:45,728: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:26:46,012: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_0
2026-01-05 13:27:05,124: Train batch 200: loss: 77.47 grad norm: 106.19 time: 0.054
2026-01-05 13:27:24,012: Train batch 400: loss: 53.25 grad norm: 109.36 time: 0.063
2026-01-05 13:27:33,271: Running test after training batch: 500
2026-01-05 13:27:33,374: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:27:38,919: WER debug example
  GT : you can see the code at this point as well
  PR : used and ease thus uhde at this ide is aisle
2026-01-05 13:27:38,953: WER debug example
  GT : how does it keep the cost down
  PR : houde is it ink thus ass adz
2026-01-05 13:27:41,415: Val batch 500: PER (avg): 0.5096 CTC Loss (avg): 55.1474 WER(1gram): 88.58% (n=64) time: 8.143
2026-01-05 13:27:41,415: WER lens: avg_true_words=6.16 avg_pred_words=5.56 max_pred_words=11
2026-01-05 13:27:41,415: t15.2023.08.13 val PER: 0.4605
2026-01-05 13:27:41,416: t15.2023.08.18 val PER: 0.4451
2026-01-05 13:27:41,416: t15.2023.08.20 val PER: 0.4551
2026-01-05 13:27:41,416: t15.2023.08.25 val PER: 0.4202
2026-01-05 13:27:41,416: t15.2023.08.27 val PER: 0.5273
2026-01-05 13:27:41,416: t15.2023.09.01 val PER: 0.4115
2026-01-05 13:27:41,416: t15.2023.09.03 val PER: 0.4976
2026-01-05 13:27:41,416: t15.2023.09.24 val PER: 0.4163
2026-01-05 13:27:41,416: t15.2023.09.29 val PER: 0.4633
2026-01-05 13:27:41,417: t15.2023.10.01 val PER: 0.5145
2026-01-05 13:27:41,417: t15.2023.10.06 val PER: 0.4101
2026-01-05 13:27:41,417: t15.2023.10.08 val PER: 0.5277
2026-01-05 13:27:41,417: t15.2023.10.13 val PER: 0.5601
2026-01-05 13:27:41,417: t15.2023.10.15 val PER: 0.4904
2026-01-05 13:27:41,417: t15.2023.10.20 val PER: 0.4564
2026-01-05 13:27:41,417: t15.2023.10.22 val PER: 0.4332
2026-01-05 13:27:41,417: t15.2023.11.03 val PER: 0.4993
2026-01-05 13:27:41,417: t15.2023.11.04 val PER: 0.2491
2026-01-05 13:27:41,417: t15.2023.11.17 val PER: 0.3546
2026-01-05 13:27:41,417: t15.2023.11.19 val PER: 0.3293
2026-01-05 13:27:41,417: t15.2023.11.26 val PER: 0.5348
2026-01-05 13:27:41,418: t15.2023.12.03 val PER: 0.4853
2026-01-05 13:27:41,418: t15.2023.12.08 val PER: 0.4920
2026-01-05 13:27:41,418: t15.2023.12.10 val PER: 0.4442
2026-01-05 13:27:41,418: t15.2023.12.17 val PER: 0.5426
2026-01-05 13:27:41,418: t15.2023.12.29 val PER: 0.5326
2026-01-05 13:27:41,418: t15.2024.02.25 val PER: 0.4817
2026-01-05 13:27:41,418: t15.2024.03.08 val PER: 0.6003
2026-01-05 13:27:41,418: t15.2024.03.15 val PER: 0.5516
2026-01-05 13:27:41,418: t15.2024.03.17 val PER: 0.4937
2026-01-05 13:27:41,419: t15.2024.05.10 val PER: 0.5334
2026-01-05 13:27:41,419: t15.2024.06.14 val PER: 0.5142
2026-01-05 13:27:41,419: t15.2024.07.19 val PER: 0.6625
2026-01-05 13:27:41,419: t15.2024.07.21 val PER: 0.4738
2026-01-05 13:27:41,419: t15.2024.07.28 val PER: 0.5059
2026-01-05 13:27:41,419: t15.2025.01.10 val PER: 0.7438
2026-01-05 13:27:41,419: t15.2025.01.12 val PER: 0.5574
2026-01-05 13:27:41,419: t15.2025.03.14 val PER: 0.7263
2026-01-05 13:27:41,419: t15.2025.03.16 val PER: 0.5864
2026-01-05 13:27:41,419: t15.2025.03.30 val PER: 0.7046
2026-01-05 13:27:41,420: t15.2025.04.13 val PER: 0.5578
2026-01-05 13:27:41,420: New best val WER(1gram) 100.00% --> 88.58%
2026-01-05 13:27:41,420: Checkpointing model
2026-01-05 13:27:42,043: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:27:42,331: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_500
2026-01-05 13:27:51,731: Train batch 600: loss: 47.79 grad norm: 77.55 time: 0.078
2026-01-05 13:28:10,358: Train batch 800: loss: 41.18 grad norm: 89.81 time: 0.058
2026-01-05 13:28:29,219: Train batch 1000: loss: 41.65 grad norm: 81.47 time: 0.066
2026-01-05 13:28:29,219: Running test after training batch: 1000
2026-01-05 13:28:29,366: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:28:34,542: WER debug example
  GT : you can see the code at this point as well
  PR : used wend ease thus owed it this royd is will
2026-01-05 13:28:34,576: WER debug example
  GT : how does it keep the cost down
  PR : houde is it ink thus wass it
2026-01-05 13:28:36,490: Val batch 1000: PER (avg): 0.4044 CTC Loss (avg): 41.9958 WER(1gram): 82.49% (n=64) time: 7.270
2026-01-05 13:28:36,490: WER lens: avg_true_words=6.16 avg_pred_words=5.44 max_pred_words=12
2026-01-05 13:28:36,490: t15.2023.08.13 val PER: 0.3794
2026-01-05 13:28:36,491: t15.2023.08.18 val PER: 0.3303
2026-01-05 13:28:36,491: t15.2023.08.20 val PER: 0.3415
2026-01-05 13:28:36,491: t15.2023.08.25 val PER: 0.2937
2026-01-05 13:28:36,491: t15.2023.08.27 val PER: 0.4277
2026-01-05 13:28:36,491: t15.2023.09.01 val PER: 0.2955
2026-01-05 13:28:36,491: t15.2023.09.03 val PER: 0.3860
2026-01-05 13:28:36,491: t15.2023.09.24 val PER: 0.3350
2026-01-05 13:28:36,491: t15.2023.09.29 val PER: 0.3618
2026-01-05 13:28:36,491: t15.2023.10.01 val PER: 0.3904
2026-01-05 13:28:36,492: t15.2023.10.06 val PER: 0.3003
2026-01-05 13:28:36,492: t15.2023.10.08 val PER: 0.4371
2026-01-05 13:28:36,492: t15.2023.10.13 val PER: 0.4647
2026-01-05 13:28:36,492: t15.2023.10.15 val PER: 0.3757
2026-01-05 13:28:36,492: t15.2023.10.20 val PER: 0.3557
2026-01-05 13:28:36,492: t15.2023.10.22 val PER: 0.3419
2026-01-05 13:28:36,492: t15.2023.11.03 val PER: 0.3976
2026-01-05 13:28:36,492: t15.2023.11.04 val PER: 0.1502
2026-01-05 13:28:36,492: t15.2023.11.17 val PER: 0.2566
2026-01-05 13:28:36,492: t15.2023.11.19 val PER: 0.2236
2026-01-05 13:28:36,492: t15.2023.11.26 val PER: 0.4500
2026-01-05 13:28:36,493: t15.2023.12.03 val PER: 0.3855
2026-01-05 13:28:36,493: t15.2023.12.08 val PER: 0.4001
2026-01-05 13:28:36,493: t15.2023.12.10 val PER: 0.3509
2026-01-05 13:28:36,493: t15.2023.12.17 val PER: 0.4127
2026-01-05 13:28:36,493: t15.2023.12.29 val PER: 0.4029
2026-01-05 13:28:36,493: t15.2024.02.25 val PER: 0.3652
2026-01-05 13:28:36,493: t15.2024.03.08 val PER: 0.4851
2026-01-05 13:28:36,493: t15.2024.03.15 val PER: 0.4415
2026-01-05 13:28:36,493: t15.2024.03.17 val PER: 0.4024
2026-01-05 13:28:36,493: t15.2024.05.10 val PER: 0.4220
2026-01-05 13:28:36,493: t15.2024.06.14 val PER: 0.4069
2026-01-05 13:28:36,494: t15.2024.07.19 val PER: 0.5274
2026-01-05 13:28:36,494: t15.2024.07.21 val PER: 0.3724
2026-01-05 13:28:36,494: t15.2024.07.28 val PER: 0.4066
2026-01-05 13:28:36,494: t15.2025.01.10 val PER: 0.6240
2026-01-05 13:28:36,494: t15.2025.01.12 val PER: 0.4396
2026-01-05 13:28:36,494: t15.2025.03.14 val PER: 0.6272
2026-01-05 13:28:36,494: t15.2025.03.16 val PER: 0.4751
2026-01-05 13:28:36,494: t15.2025.03.30 val PER: 0.6402
2026-01-05 13:28:36,494: t15.2025.04.13 val PER: 0.4964
2026-01-05 13:28:36,495: New best val WER(1gram) 88.58% --> 82.49%
2026-01-05 13:28:36,495: Checkpointing model
2026-01-05 13:28:37,155: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:28:37,441: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_1000
2026-01-05 13:28:56,161: Train batch 1200: loss: 32.58 grad norm: 72.81 time: 0.067
2026-01-05 13:29:15,085: Train batch 1400: loss: 36.22 grad norm: 84.95 time: 0.060
2026-01-05 13:29:24,468: Running test after training batch: 1500
2026-01-05 13:29:24,580: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:29:29,790: WER debug example
  GT : you can see the code at this point as well
  PR : yule kint ease the owed it this boyde is wheel
2026-01-05 13:29:29,824: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that cost
2026-01-05 13:29:31,471: Val batch 1500: PER (avg): 0.3806 CTC Loss (avg): 36.9681 WER(1gram): 75.38% (n=64) time: 7.003
2026-01-05 13:29:31,471: WER lens: avg_true_words=6.16 avg_pred_words=5.00 max_pred_words=11
2026-01-05 13:29:31,472: t15.2023.08.13 val PER: 0.3462
2026-01-05 13:29:31,472: t15.2023.08.18 val PER: 0.3085
2026-01-05 13:29:31,472: t15.2023.08.20 val PER: 0.3098
2026-01-05 13:29:31,472: t15.2023.08.25 val PER: 0.2636
2026-01-05 13:29:31,472: t15.2023.08.27 val PER: 0.4132
2026-01-05 13:29:31,472: t15.2023.09.01 val PER: 0.2792
2026-01-05 13:29:31,472: t15.2023.09.03 val PER: 0.3729
2026-01-05 13:29:31,472: t15.2023.09.24 val PER: 0.2961
2026-01-05 13:29:31,472: t15.2023.09.29 val PER: 0.3299
2026-01-05 13:29:31,472: t15.2023.10.01 val PER: 0.3884
2026-01-05 13:29:31,472: t15.2023.10.06 val PER: 0.2853
2026-01-05 13:29:31,472: t15.2023.10.08 val PER: 0.4276
2026-01-05 13:29:31,473: t15.2023.10.13 val PER: 0.4375
2026-01-05 13:29:31,473: t15.2023.10.15 val PER: 0.3553
2026-01-05 13:29:31,473: t15.2023.10.20 val PER: 0.3255
2026-01-05 13:29:31,473: t15.2023.10.22 val PER: 0.3218
2026-01-05 13:29:31,473: t15.2023.11.03 val PER: 0.3616
2026-01-05 13:29:31,473: t15.2023.11.04 val PER: 0.1024
2026-01-05 13:29:31,473: t15.2023.11.17 val PER: 0.2115
2026-01-05 13:29:31,473: t15.2023.11.19 val PER: 0.1737
2026-01-05 13:29:31,473: t15.2023.11.26 val PER: 0.4304
2026-01-05 13:29:31,473: t15.2023.12.03 val PER: 0.3771
2026-01-05 13:29:31,473: t15.2023.12.08 val PER: 0.3442
2026-01-05 13:29:31,473: t15.2023.12.10 val PER: 0.2996
2026-01-05 13:29:31,474: t15.2023.12.17 val PER: 0.3805
2026-01-05 13:29:31,474: t15.2023.12.29 val PER: 0.3795
2026-01-05 13:29:31,474: t15.2024.02.25 val PER: 0.3048
2026-01-05 13:29:31,474: t15.2024.03.08 val PER: 0.4723
2026-01-05 13:29:31,474: t15.2024.03.15 val PER: 0.4246
2026-01-05 13:29:31,474: t15.2024.03.17 val PER: 0.3717
2026-01-05 13:29:31,474: t15.2024.05.10 val PER: 0.3952
2026-01-05 13:29:31,474: t15.2024.06.14 val PER: 0.4117
2026-01-05 13:29:31,474: t15.2024.07.19 val PER: 0.5260
2026-01-05 13:29:31,474: t15.2024.07.21 val PER: 0.3483
2026-01-05 13:29:31,474: t15.2024.07.28 val PER: 0.3647
2026-01-05 13:29:31,475: t15.2025.01.10 val PER: 0.6226
2026-01-05 13:29:31,475: t15.2025.01.12 val PER: 0.4234
2026-01-05 13:29:31,475: t15.2025.03.14 val PER: 0.6065
2026-01-05 13:29:31,475: t15.2025.03.16 val PER: 0.4738
2026-01-05 13:29:31,475: t15.2025.03.30 val PER: 0.6437
2026-01-05 13:29:31,475: t15.2025.04.13 val PER: 0.4708
2026-01-05 13:29:31,476: New best val WER(1gram) 82.49% --> 75.38%
2026-01-05 13:29:31,476: Checkpointing model
2026-01-05 13:29:32,124: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:29:32,411: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_1500
2026-01-05 13:29:42,486: Train batch 1600: loss: 36.33 grad norm: 80.56 time: 0.064
2026-01-05 13:30:01,536: Train batch 1800: loss: 34.40 grad norm: 70.22 time: 0.089
2026-01-05 13:30:20,367: Train batch 2000: loss: 32.69 grad norm: 70.04 time: 0.067
2026-01-05 13:30:20,368: Running test after training batch: 2000
2026-01-05 13:30:20,509: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:30:25,930: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this boyde is wheel
2026-01-05 13:30:25,962: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap the cus it
2026-01-05 13:30:27,585: Val batch 2000: PER (avg): 0.3234 CTC Loss (avg): 32.4082 WER(1gram): 71.32% (n=64) time: 7.217
2026-01-05 13:30:27,586: WER lens: avg_true_words=6.16 avg_pred_words=5.62 max_pred_words=11
2026-01-05 13:30:27,586: t15.2023.08.13 val PER: 0.2921
2026-01-05 13:30:27,586: t15.2023.08.18 val PER: 0.2498
2026-01-05 13:30:27,586: t15.2023.08.20 val PER: 0.2526
2026-01-05 13:30:27,586: t15.2023.08.25 val PER: 0.2199
2026-01-05 13:30:27,586: t15.2023.08.27 val PER: 0.3408
2026-01-05 13:30:27,586: t15.2023.09.01 val PER: 0.2143
2026-01-05 13:30:27,586: t15.2023.09.03 val PER: 0.3183
2026-01-05 13:30:27,586: t15.2023.09.24 val PER: 0.2524
2026-01-05 13:30:27,586: t15.2023.09.29 val PER: 0.2763
2026-01-05 13:30:27,586: t15.2023.10.01 val PER: 0.3269
2026-01-05 13:30:27,587: t15.2023.10.06 val PER: 0.2304
2026-01-05 13:30:27,587: t15.2023.10.08 val PER: 0.3816
2026-01-05 13:30:27,587: t15.2023.10.13 val PER: 0.3763
2026-01-05 13:30:27,587: t15.2023.10.15 val PER: 0.2966
2026-01-05 13:30:27,587: t15.2023.10.20 val PER: 0.2718
2026-01-05 13:30:27,587: t15.2023.10.22 val PER: 0.2528
2026-01-05 13:30:27,587: t15.2023.11.03 val PER: 0.3141
2026-01-05 13:30:27,587: t15.2023.11.04 val PER: 0.0956
2026-01-05 13:30:27,587: t15.2023.11.17 val PER: 0.1571
2026-01-05 13:30:27,587: t15.2023.11.19 val PER: 0.1397
2026-01-05 13:30:27,587: t15.2023.11.26 val PER: 0.3659
2026-01-05 13:30:27,587: t15.2023.12.03 val PER: 0.3067
2026-01-05 13:30:27,588: t15.2023.12.08 val PER: 0.3129
2026-01-05 13:30:27,588: t15.2023.12.10 val PER: 0.2654
2026-01-05 13:30:27,588: t15.2023.12.17 val PER: 0.3067
2026-01-05 13:30:27,588: t15.2023.12.29 val PER: 0.3267
2026-01-05 13:30:27,588: t15.2024.02.25 val PER: 0.2823
2026-01-05 13:30:27,588: t15.2024.03.08 val PER: 0.3841
2026-01-05 13:30:27,588: t15.2024.03.15 val PER: 0.3602
2026-01-05 13:30:27,588: t15.2024.03.17 val PER: 0.3361
2026-01-05 13:30:27,588: t15.2024.05.10 val PER: 0.3358
2026-01-05 13:30:27,588: t15.2024.06.14 val PER: 0.3344
2026-01-05 13:30:27,588: t15.2024.07.19 val PER: 0.4647
2026-01-05 13:30:27,588: t15.2024.07.21 val PER: 0.2952
2026-01-05 13:30:27,588: t15.2024.07.28 val PER: 0.3228
2026-01-05 13:30:27,588: t15.2025.01.10 val PER: 0.5331
2026-01-05 13:30:27,588: t15.2025.01.12 val PER: 0.3803
2026-01-05 13:30:27,588: t15.2025.03.14 val PER: 0.5163
2026-01-05 13:30:27,589: t15.2025.03.16 val PER: 0.3861
2026-01-05 13:30:27,589: t15.2025.03.30 val PER: 0.5391
2026-01-05 13:30:27,589: t15.2025.04.13 val PER: 0.4137
2026-01-05 13:30:27,590: New best val WER(1gram) 75.38% --> 71.32%
2026-01-05 13:30:27,590: Checkpointing model
2026-01-05 13:30:28,259: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:30:28,547: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_2000
2026-01-05 13:30:47,237: Train batch 2200: loss: 28.70 grad norm: 72.27 time: 0.061
2026-01-05 13:31:06,757: Train batch 2400: loss: 28.54 grad norm: 63.05 time: 0.052
2026-01-05 13:31:16,401: Running test after training batch: 2500
2026-01-05 13:31:16,575: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:31:21,967: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-05 13:31:21,999: WER debug example
  GT : how does it keep the cost down
  PR : houde des it kipp the us it
2026-01-05 13:31:23,748: Val batch 2500: PER (avg): 0.2979 CTC Loss (avg): 29.7416 WER(1gram): 69.80% (n=64) time: 7.346
2026-01-05 13:31:23,748: WER lens: avg_true_words=6.16 avg_pred_words=5.47 max_pred_words=11
2026-01-05 13:31:23,749: t15.2023.08.13 val PER: 0.2859
2026-01-05 13:31:23,749: t15.2023.08.18 val PER: 0.2397
2026-01-05 13:31:23,749: t15.2023.08.20 val PER: 0.2343
2026-01-05 13:31:23,749: t15.2023.08.25 val PER: 0.2018
2026-01-05 13:31:23,749: t15.2023.08.27 val PER: 0.3183
2026-01-05 13:31:23,749: t15.2023.09.01 val PER: 0.1916
2026-01-05 13:31:23,749: t15.2023.09.03 val PER: 0.2993
2026-01-05 13:31:23,749: t15.2023.09.24 val PER: 0.2233
2026-01-05 13:31:23,749: t15.2023.09.29 val PER: 0.2502
2026-01-05 13:31:23,749: t15.2023.10.01 val PER: 0.2979
2026-01-05 13:31:23,749: t15.2023.10.06 val PER: 0.2110
2026-01-05 13:31:23,749: t15.2023.10.08 val PER: 0.3694
2026-01-05 13:31:23,749: t15.2023.10.13 val PER: 0.3460
2026-01-05 13:31:23,750: t15.2023.10.15 val PER: 0.2868
2026-01-05 13:31:23,750: t15.2023.10.20 val PER: 0.2450
2026-01-05 13:31:23,750: t15.2023.10.22 val PER: 0.2361
2026-01-05 13:31:23,750: t15.2023.11.03 val PER: 0.2978
2026-01-05 13:31:23,750: t15.2023.11.04 val PER: 0.0853
2026-01-05 13:31:23,750: t15.2023.11.17 val PER: 0.1384
2026-01-05 13:31:23,750: t15.2023.11.19 val PER: 0.1238
2026-01-05 13:31:23,751: t15.2023.11.26 val PER: 0.3333
2026-01-05 13:31:23,751: t15.2023.12.03 val PER: 0.2857
2026-01-05 13:31:23,751: t15.2023.12.08 val PER: 0.2783
2026-01-05 13:31:23,751: t15.2023.12.10 val PER: 0.2313
2026-01-05 13:31:23,751: t15.2023.12.17 val PER: 0.2796
2026-01-05 13:31:23,751: t15.2023.12.29 val PER: 0.2828
2026-01-05 13:31:23,751: t15.2024.02.25 val PER: 0.2360
2026-01-05 13:31:23,751: t15.2024.03.08 val PER: 0.3627
2026-01-05 13:31:23,751: t15.2024.03.15 val PER: 0.3383
2026-01-05 13:31:23,751: t15.2024.03.17 val PER: 0.3075
2026-01-05 13:31:23,751: t15.2024.05.10 val PER: 0.3016
2026-01-05 13:31:23,751: t15.2024.06.14 val PER: 0.3233
2026-01-05 13:31:23,751: t15.2024.07.19 val PER: 0.4417
2026-01-05 13:31:23,752: t15.2024.07.21 val PER: 0.2503
2026-01-05 13:31:23,752: t15.2024.07.28 val PER: 0.2993
2026-01-05 13:31:23,752: t15.2025.01.10 val PER: 0.4862
2026-01-05 13:31:23,752: t15.2025.01.12 val PER: 0.3510
2026-01-05 13:31:23,752: t15.2025.03.14 val PER: 0.4867
2026-01-05 13:31:23,752: t15.2025.03.16 val PER: 0.3665
2026-01-05 13:31:23,752: t15.2025.03.30 val PER: 0.5000
2026-01-05 13:31:23,752: t15.2025.04.13 val PER: 0.3780
2026-01-05 13:31:23,753: New best val WER(1gram) 71.32% --> 69.80%
2026-01-05 13:31:23,753: Checkpointing model
2026-01-05 13:31:24,437: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:31:24,731: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_2500
2026-01-05 13:31:34,003: Train batch 2600: loss: 34.40 grad norm: 81.52 time: 0.055
2026-01-05 13:31:52,967: Train batch 2800: loss: 25.08 grad norm: 71.23 time: 0.082
2026-01-05 13:32:12,022: Train batch 3000: loss: 30.05 grad norm: 79.49 time: 0.082
2026-01-05 13:32:12,023: Running test after training batch: 3000
2026-01-05 13:32:12,143: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:32:17,312: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sheik the good at this point is will
2026-01-05 13:32:17,342: WER debug example
  GT : how does it keep the cost down
  PR : houde des it keep the rust et
2026-01-05 13:32:19,092: Val batch 3000: PER (avg): 0.2799 CTC Loss (avg): 27.5122 WER(1gram): 67.51% (n=64) time: 7.069
2026-01-05 13:32:19,093: WER lens: avg_true_words=6.16 avg_pred_words=5.88 max_pred_words=10
2026-01-05 13:32:19,093: t15.2023.08.13 val PER: 0.2620
2026-01-05 13:32:19,093: t15.2023.08.18 val PER: 0.2163
2026-01-05 13:32:19,093: t15.2023.08.20 val PER: 0.2153
2026-01-05 13:32:19,093: t15.2023.08.25 val PER: 0.1837
2026-01-05 13:32:19,093: t15.2023.08.27 val PER: 0.2862
2026-01-05 13:32:19,094: t15.2023.09.01 val PER: 0.1899
2026-01-05 13:32:19,094: t15.2023.09.03 val PER: 0.2791
2026-01-05 13:32:19,094: t15.2023.09.24 val PER: 0.2184
2026-01-05 13:32:19,094: t15.2023.09.29 val PER: 0.2380
2026-01-05 13:32:19,094: t15.2023.10.01 val PER: 0.2933
2026-01-05 13:32:19,094: t15.2023.10.06 val PER: 0.1916
2026-01-05 13:32:19,094: t15.2023.10.08 val PER: 0.3464
2026-01-05 13:32:19,094: t15.2023.10.13 val PER: 0.3336
2026-01-05 13:32:19,094: t15.2023.10.15 val PER: 0.2551
2026-01-05 13:32:19,094: t15.2023.10.20 val PER: 0.2752
2026-01-05 13:32:19,094: t15.2023.10.22 val PER: 0.2038
2026-01-05 13:32:19,094: t15.2023.11.03 val PER: 0.2734
2026-01-05 13:32:19,095: t15.2023.11.04 val PER: 0.0785
2026-01-05 13:32:19,095: t15.2023.11.17 val PER: 0.1306
2026-01-05 13:32:19,095: t15.2023.11.19 val PER: 0.1118
2026-01-05 13:32:19,095: t15.2023.11.26 val PER: 0.2957
2026-01-05 13:32:19,095: t15.2023.12.03 val PER: 0.2679
2026-01-05 13:32:19,095: t15.2023.12.08 val PER: 0.2577
2026-01-05 13:32:19,095: t15.2023.12.10 val PER: 0.2181
2026-01-05 13:32:19,095: t15.2023.12.17 val PER: 0.2755
2026-01-05 13:32:19,095: t15.2023.12.29 val PER: 0.2841
2026-01-05 13:32:19,095: t15.2024.02.25 val PER: 0.2303
2026-01-05 13:32:19,095: t15.2024.03.08 val PER: 0.3713
2026-01-05 13:32:19,096: t15.2024.03.15 val PER: 0.3283
2026-01-05 13:32:19,096: t15.2024.03.17 val PER: 0.2852
2026-01-05 13:32:19,096: t15.2024.05.10 val PER: 0.3031
2026-01-05 13:32:19,096: t15.2024.06.14 val PER: 0.2965
2026-01-05 13:32:19,096: t15.2024.07.19 val PER: 0.3949
2026-01-05 13:32:19,096: t15.2024.07.21 val PER: 0.2317
2026-01-05 13:32:19,096: t15.2024.07.28 val PER: 0.2809
2026-01-05 13:32:19,096: t15.2025.01.10 val PER: 0.4890
2026-01-05 13:32:19,096: t15.2025.01.12 val PER: 0.3272
2026-01-05 13:32:19,096: t15.2025.03.14 val PER: 0.4512
2026-01-05 13:32:19,097: t15.2025.03.16 val PER: 0.3233
2026-01-05 13:32:19,097: t15.2025.03.30 val PER: 0.4759
2026-01-05 13:32:19,097: t15.2025.04.13 val PER: 0.3552
2026-01-05 13:32:19,097: New best val WER(1gram) 69.80% --> 67.51%
2026-01-05 13:32:19,097: Checkpointing model
2026-01-05 13:32:19,786: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:32:20,084: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_3000
2026-01-05 13:32:38,932: Train batch 3200: loss: 26.40 grad norm: 72.67 time: 0.076
2026-01-05 13:32:57,758: Train batch 3400: loss: 17.68 grad norm: 57.71 time: 0.049
2026-01-05 13:33:07,218: Running test after training batch: 3500
2026-01-05 13:33:07,366: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:33:12,530: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point will
2026-01-05 13:33:12,561: WER debug example
  GT : how does it keep the cost down
  PR : aue des it hipp thus cussed get
2026-01-05 13:33:14,210: Val batch 3500: PER (avg): 0.2662 CTC Loss (avg): 26.0896 WER(1gram): 67.01% (n=64) time: 6.991
2026-01-05 13:33:14,210: WER lens: avg_true_words=6.16 avg_pred_words=5.88 max_pred_words=11
2026-01-05 13:33:14,210: t15.2023.08.13 val PER: 0.2328
2026-01-05 13:33:14,210: t15.2023.08.18 val PER: 0.2096
2026-01-05 13:33:14,211: t15.2023.08.20 val PER: 0.2192
2026-01-05 13:33:14,211: t15.2023.08.25 val PER: 0.1747
2026-01-05 13:33:14,211: t15.2023.08.27 val PER: 0.2733
2026-01-05 13:33:14,211: t15.2023.09.01 val PER: 0.1680
2026-01-05 13:33:14,211: t15.2023.09.03 val PER: 0.2542
2026-01-05 13:33:14,211: t15.2023.09.24 val PER: 0.2051
2026-01-05 13:33:14,211: t15.2023.09.29 val PER: 0.2214
2026-01-05 13:33:14,211: t15.2023.10.01 val PER: 0.2814
2026-01-05 13:33:14,211: t15.2023.10.06 val PER: 0.1830
2026-01-05 13:33:14,211: t15.2023.10.08 val PER: 0.3451
2026-01-05 13:33:14,211: t15.2023.10.13 val PER: 0.3157
2026-01-05 13:33:14,212: t15.2023.10.15 val PER: 0.2498
2026-01-05 13:33:14,212: t15.2023.10.20 val PER: 0.2550
2026-01-05 13:33:14,212: t15.2023.10.22 val PER: 0.2094
2026-01-05 13:33:14,212: t15.2023.11.03 val PER: 0.2612
2026-01-05 13:33:14,212: t15.2023.11.04 val PER: 0.0717
2026-01-05 13:33:14,212: t15.2023.11.17 val PER: 0.1213
2026-01-05 13:33:14,212: t15.2023.11.19 val PER: 0.0898
2026-01-05 13:33:14,212: t15.2023.11.26 val PER: 0.2775
2026-01-05 13:33:14,212: t15.2023.12.03 val PER: 0.2363
2026-01-05 13:33:14,212: t15.2023.12.08 val PER: 0.2383
2026-01-05 13:33:14,212: t15.2023.12.10 val PER: 0.1958
2026-01-05 13:33:14,213: t15.2023.12.17 val PER: 0.2526
2026-01-05 13:33:14,213: t15.2023.12.29 val PER: 0.2594
2026-01-05 13:33:14,213: t15.2024.02.25 val PER: 0.2093
2026-01-05 13:33:14,213: t15.2024.03.08 val PER: 0.3471
2026-01-05 13:33:14,213: t15.2024.03.15 val PER: 0.3196
2026-01-05 13:33:14,213: t15.2024.03.17 val PER: 0.2824
2026-01-05 13:33:14,213: t15.2024.05.10 val PER: 0.2764
2026-01-05 13:33:14,213: t15.2024.06.14 val PER: 0.2855
2026-01-05 13:33:14,213: t15.2024.07.19 val PER: 0.3896
2026-01-05 13:33:14,213: t15.2024.07.21 val PER: 0.2193
2026-01-05 13:33:14,214: t15.2024.07.28 val PER: 0.2735
2026-01-05 13:33:14,214: t15.2025.01.10 val PER: 0.4573
2026-01-05 13:33:14,214: t15.2025.01.12 val PER: 0.2995
2026-01-05 13:33:14,214: t15.2025.03.14 val PER: 0.4512
2026-01-05 13:33:14,214: t15.2025.03.16 val PER: 0.3312
2026-01-05 13:33:14,214: t15.2025.03.30 val PER: 0.4678
2026-01-05 13:33:14,214: t15.2025.04.13 val PER: 0.3352
2026-01-05 13:33:14,214: New best val WER(1gram) 67.51% --> 67.01%
2026-01-05 13:33:14,214: Checkpointing model
2026-01-05 13:33:14,877: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:33:15,164: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_3500
2026-01-05 13:33:24,813: Train batch 3600: loss: 21.88 grad norm: 64.25 time: 0.069
2026-01-05 13:33:44,254: Train batch 3800: loss: 24.99 grad norm: 69.77 time: 0.066
2026-01-05 13:34:03,774: Train batch 4000: loss: 18.92 grad norm: 57.33 time: 0.056
2026-01-05 13:34:03,774: Running test after training batch: 4000
2026-01-05 13:34:03,944: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:34:09,208: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-05 13:34:09,239: WER debug example
  GT : how does it keep the cost down
  PR : aue des it keep the cost it
2026-01-05 13:34:10,942: Val batch 4000: PER (avg): 0.2443 CTC Loss (avg): 23.9546 WER(1gram): 65.23% (n=64) time: 7.168
2026-01-05 13:34:10,942: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-05 13:34:10,943: t15.2023.08.13 val PER: 0.2297
2026-01-05 13:34:10,943: t15.2023.08.18 val PER: 0.1978
2026-01-05 13:34:10,943: t15.2023.08.20 val PER: 0.2010
2026-01-05 13:34:10,943: t15.2023.08.25 val PER: 0.1657
2026-01-05 13:34:10,943: t15.2023.08.27 val PER: 0.2846
2026-01-05 13:34:10,943: t15.2023.09.01 val PER: 0.1518
2026-01-05 13:34:10,943: t15.2023.09.03 val PER: 0.2387
2026-01-05 13:34:10,943: t15.2023.09.24 val PER: 0.1881
2026-01-05 13:34:10,943: t15.2023.09.29 val PER: 0.2049
2026-01-05 13:34:10,943: t15.2023.10.01 val PER: 0.2503
2026-01-05 13:34:10,943: t15.2023.10.06 val PER: 0.1604
2026-01-05 13:34:10,943: t15.2023.10.08 val PER: 0.3207
2026-01-05 13:34:10,943: t15.2023.10.13 val PER: 0.2940
2026-01-05 13:34:10,944: t15.2023.10.15 val PER: 0.2274
2026-01-05 13:34:10,944: t15.2023.10.20 val PER: 0.2450
2026-01-05 13:34:10,944: t15.2023.10.22 val PER: 0.1849
2026-01-05 13:34:10,944: t15.2023.11.03 val PER: 0.2436
2026-01-05 13:34:10,944: t15.2023.11.04 val PER: 0.0546
2026-01-05 13:34:10,944: t15.2023.11.17 val PER: 0.0918
2026-01-05 13:34:10,944: t15.2023.11.19 val PER: 0.0858
2026-01-05 13:34:10,944: t15.2023.11.26 val PER: 0.2594
2026-01-05 13:34:10,945: t15.2023.12.03 val PER: 0.2111
2026-01-05 13:34:10,945: t15.2023.12.08 val PER: 0.2250
2026-01-05 13:34:10,945: t15.2023.12.10 val PER: 0.1813
2026-01-05 13:34:10,945: t15.2023.12.17 val PER: 0.2380
2026-01-05 13:34:10,945: t15.2023.12.29 val PER: 0.2443
2026-01-05 13:34:10,945: t15.2024.02.25 val PER: 0.2065
2026-01-05 13:34:10,945: t15.2024.03.08 val PER: 0.3229
2026-01-05 13:34:10,945: t15.2024.03.15 val PER: 0.2939
2026-01-05 13:34:10,945: t15.2024.03.17 val PER: 0.2615
2026-01-05 13:34:10,945: t15.2024.05.10 val PER: 0.2481
2026-01-05 13:34:10,945: t15.2024.06.14 val PER: 0.2792
2026-01-05 13:34:10,946: t15.2024.07.19 val PER: 0.3553
2026-01-05 13:34:10,946: t15.2024.07.21 val PER: 0.1779
2026-01-05 13:34:10,946: t15.2024.07.28 val PER: 0.2360
2026-01-05 13:34:10,946: t15.2025.01.10 val PER: 0.4077
2026-01-05 13:34:10,946: t15.2025.01.12 val PER: 0.2702
2026-01-05 13:34:10,946: t15.2025.03.14 val PER: 0.4201
2026-01-05 13:34:10,946: t15.2025.03.16 val PER: 0.2997
2026-01-05 13:34:10,946: t15.2025.03.30 val PER: 0.4138
2026-01-05 13:34:10,946: t15.2025.04.13 val PER: 0.3210
2026-01-05 13:34:10,947: New best val WER(1gram) 67.01% --> 65.23%
2026-01-05 13:34:10,948: Checkpointing model
2026-01-05 13:34:11,608: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:34:11,894: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_4000
2026-01-05 13:34:30,745: Train batch 4200: loss: 22.10 grad norm: 67.96 time: 0.079
2026-01-05 13:34:50,762: Train batch 4400: loss: 16.43 grad norm: 54.32 time: 0.066
2026-01-05 13:35:00,364: Running test after training batch: 4500
2026-01-05 13:35:00,473: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:35:05,687: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-05 13:35:05,717: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cost et
2026-01-05 13:35:07,407: Val batch 4500: PER (avg): 0.2325 CTC Loss (avg): 22.8880 WER(1gram): 59.64% (n=64) time: 7.042
2026-01-05 13:35:07,407: WER lens: avg_true_words=6.16 avg_pred_words=6.02 max_pred_words=11
2026-01-05 13:35:07,407: t15.2023.08.13 val PER: 0.2193
2026-01-05 13:35:07,407: t15.2023.08.18 val PER: 0.1844
2026-01-05 13:35:07,407: t15.2023.08.20 val PER: 0.1843
2026-01-05 13:35:07,407: t15.2023.08.25 val PER: 0.1280
2026-01-05 13:35:07,408: t15.2023.08.27 val PER: 0.2460
2026-01-05 13:35:07,408: t15.2023.09.01 val PER: 0.1583
2026-01-05 13:35:07,408: t15.2023.09.03 val PER: 0.2280
2026-01-05 13:35:07,408: t15.2023.09.24 val PER: 0.1784
2026-01-05 13:35:07,408: t15.2023.09.29 val PER: 0.1895
2026-01-05 13:35:07,408: t15.2023.10.01 val PER: 0.2417
2026-01-05 13:35:07,408: t15.2023.10.06 val PER: 0.1421
2026-01-05 13:35:07,408: t15.2023.10.08 val PER: 0.3004
2026-01-05 13:35:07,408: t15.2023.10.13 val PER: 0.2940
2026-01-05 13:35:07,409: t15.2023.10.15 val PER: 0.2301
2026-01-05 13:35:07,409: t15.2023.10.20 val PER: 0.2315
2026-01-05 13:35:07,409: t15.2023.10.22 val PER: 0.1893
2026-01-05 13:35:07,409: t15.2023.11.03 val PER: 0.2347
2026-01-05 13:35:07,409: t15.2023.11.04 val PER: 0.0546
2026-01-05 13:35:07,409: t15.2023.11.17 val PER: 0.1042
2026-01-05 13:35:07,409: t15.2023.11.19 val PER: 0.0878
2026-01-05 13:35:07,409: t15.2023.11.26 val PER: 0.2587
2026-01-05 13:35:07,409: t15.2023.12.03 val PER: 0.2027
2026-01-05 13:35:07,409: t15.2023.12.08 val PER: 0.2011
2026-01-05 13:35:07,409: t15.2023.12.10 val PER: 0.1616
2026-01-05 13:35:07,409: t15.2023.12.17 val PER: 0.2328
2026-01-05 13:35:07,409: t15.2023.12.29 val PER: 0.2340
2026-01-05 13:35:07,410: t15.2024.02.25 val PER: 0.1868
2026-01-05 13:35:07,410: t15.2024.03.08 val PER: 0.3186
2026-01-05 13:35:07,410: t15.2024.03.15 val PER: 0.2858
2026-01-05 13:35:07,410: t15.2024.03.17 val PER: 0.2336
2026-01-05 13:35:07,410: t15.2024.05.10 val PER: 0.2481
2026-01-05 13:35:07,410: t15.2024.06.14 val PER: 0.2492
2026-01-05 13:35:07,410: t15.2024.07.19 val PER: 0.3342
2026-01-05 13:35:07,410: t15.2024.07.21 val PER: 0.1621
2026-01-05 13:35:07,410: t15.2024.07.28 val PER: 0.2162
2026-01-05 13:35:07,410: t15.2025.01.10 val PER: 0.3898
2026-01-05 13:35:07,410: t15.2025.01.12 val PER: 0.2625
2026-01-05 13:35:07,411: t15.2025.03.14 val PER: 0.4053
2026-01-05 13:35:07,411: t15.2025.03.16 val PER: 0.2775
2026-01-05 13:35:07,411: t15.2025.03.30 val PER: 0.4149
2026-01-05 13:35:07,411: t15.2025.04.13 val PER: 0.3010
2026-01-05 13:35:07,412: New best val WER(1gram) 65.23% --> 59.64%
2026-01-05 13:35:07,412: Checkpointing model
2026-01-05 13:35:08,100: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:35:08,398: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_4500
2026-01-05 13:35:17,738: Train batch 4600: loss: 19.62 grad norm: 62.90 time: 0.062
2026-01-05 13:35:36,628: Train batch 4800: loss: 13.54 grad norm: 53.39 time: 0.064
2026-01-05 13:35:55,660: Train batch 5000: loss: 31.48 grad norm: 87.11 time: 0.064
2026-01-05 13:35:55,661: Running test after training batch: 5000
2026-01-05 13:35:55,799: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:36:01,124: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is wheel
2026-01-05 13:36:01,154: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the cost nett
2026-01-05 13:36:02,877: Val batch 5000: PER (avg): 0.2221 CTC Loss (avg): 21.8879 WER(1gram): 63.45% (n=64) time: 7.216
2026-01-05 13:36:02,878: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-05 13:36:02,878: t15.2023.08.13 val PER: 0.2048
2026-01-05 13:36:02,878: t15.2023.08.18 val PER: 0.1584
2026-01-05 13:36:02,878: t15.2023.08.20 val PER: 0.1597
2026-01-05 13:36:02,878: t15.2023.08.25 val PER: 0.1295
2026-01-05 13:36:02,878: t15.2023.08.27 val PER: 0.2315
2026-01-05 13:36:02,878: t15.2023.09.01 val PER: 0.1477
2026-01-05 13:36:02,879: t15.2023.09.03 val PER: 0.2268
2026-01-05 13:36:02,879: t15.2023.09.24 val PER: 0.1772
2026-01-05 13:36:02,879: t15.2023.09.29 val PER: 0.1844
2026-01-05 13:36:02,879: t15.2023.10.01 val PER: 0.2378
2026-01-05 13:36:02,879: t15.2023.10.06 val PER: 0.1356
2026-01-05 13:36:02,879: t15.2023.10.08 val PER: 0.3112
2026-01-05 13:36:02,879: t15.2023.10.13 val PER: 0.2723
2026-01-05 13:36:02,879: t15.2023.10.15 val PER: 0.2248
2026-01-05 13:36:02,879: t15.2023.10.20 val PER: 0.2416
2026-01-05 13:36:02,879: t15.2023.10.22 val PER: 0.1693
2026-01-05 13:36:02,879: t15.2023.11.03 val PER: 0.2239
2026-01-05 13:36:02,879: t15.2023.11.04 val PER: 0.0375
2026-01-05 13:36:02,880: t15.2023.11.17 val PER: 0.0855
2026-01-05 13:36:02,880: t15.2023.11.19 val PER: 0.0858
2026-01-05 13:36:02,880: t15.2023.11.26 val PER: 0.2283
2026-01-05 13:36:02,880: t15.2023.12.03 val PER: 0.1996
2026-01-05 13:36:02,880: t15.2023.12.08 val PER: 0.1937
2026-01-05 13:36:02,880: t15.2023.12.10 val PER: 0.1603
2026-01-05 13:36:02,880: t15.2023.12.17 val PER: 0.2183
2026-01-05 13:36:02,880: t15.2023.12.29 val PER: 0.2176
2026-01-05 13:36:02,880: t15.2024.02.25 val PER: 0.1798
2026-01-05 13:36:02,880: t15.2024.03.08 val PER: 0.3044
2026-01-05 13:36:02,880: t15.2024.03.15 val PER: 0.2764
2026-01-05 13:36:02,880: t15.2024.03.17 val PER: 0.2315
2026-01-05 13:36:02,881: t15.2024.05.10 val PER: 0.2318
2026-01-05 13:36:02,881: t15.2024.06.14 val PER: 0.2461
2026-01-05 13:36:02,881: t15.2024.07.19 val PER: 0.3336
2026-01-05 13:36:02,881: t15.2024.07.21 val PER: 0.1766
2026-01-05 13:36:02,881: t15.2024.07.28 val PER: 0.2066
2026-01-05 13:36:02,881: t15.2025.01.10 val PER: 0.3760
2026-01-05 13:36:02,881: t15.2025.01.12 val PER: 0.2379
2026-01-05 13:36:02,881: t15.2025.03.14 val PER: 0.3979
2026-01-05 13:36:02,881: t15.2025.03.16 val PER: 0.2539
2026-01-05 13:36:02,881: t15.2025.03.30 val PER: 0.3839
2026-01-05 13:36:02,881: t15.2025.04.13 val PER: 0.2910
2026-01-05 13:36:03,184: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_5000
2026-01-05 13:36:22,074: Train batch 5200: loss: 15.93 grad norm: 58.06 time: 0.052
2026-01-05 13:36:40,973: Train batch 5400: loss: 16.94 grad norm: 59.96 time: 0.068
2026-01-05 13:36:50,387: Running test after training batch: 5500
2026-01-05 13:36:50,552: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:36:56,138: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point will
2026-01-05 13:36:56,168: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost nit
2026-01-05 13:36:57,821: Val batch 5500: PER (avg): 0.2132 CTC Loss (avg): 20.8177 WER(1gram): 56.60% (n=64) time: 7.434
2026-01-05 13:36:57,821: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-05 13:36:57,822: t15.2023.08.13 val PER: 0.1778
2026-01-05 13:36:57,822: t15.2023.08.18 val PER: 0.1593
2026-01-05 13:36:57,822: t15.2023.08.20 val PER: 0.1700
2026-01-05 13:36:57,822: t15.2023.08.25 val PER: 0.1220
2026-01-05 13:36:57,822: t15.2023.08.27 val PER: 0.2347
2026-01-05 13:36:57,822: t15.2023.09.01 val PER: 0.1315
2026-01-05 13:36:57,822: t15.2023.09.03 val PER: 0.2150
2026-01-05 13:36:57,822: t15.2023.09.24 val PER: 0.1808
2026-01-05 13:36:57,823: t15.2023.09.29 val PER: 0.1755
2026-01-05 13:36:57,823: t15.2023.10.01 val PER: 0.2206
2026-01-05 13:36:57,823: t15.2023.10.06 val PER: 0.1356
2026-01-05 13:36:57,823: t15.2023.10.08 val PER: 0.2936
2026-01-05 13:36:57,823: t15.2023.10.13 val PER: 0.2777
2026-01-05 13:36:57,823: t15.2023.10.15 val PER: 0.2050
2026-01-05 13:36:57,823: t15.2023.10.20 val PER: 0.2114
2026-01-05 13:36:57,823: t15.2023.10.22 val PER: 0.1704
2026-01-05 13:36:57,823: t15.2023.11.03 val PER: 0.2239
2026-01-05 13:36:57,823: t15.2023.11.04 val PER: 0.0683
2026-01-05 13:36:57,823: t15.2023.11.17 val PER: 0.0793
2026-01-05 13:36:57,823: t15.2023.11.19 val PER: 0.0778
2026-01-05 13:36:57,823: t15.2023.11.26 val PER: 0.2152
2026-01-05 13:36:57,823: t15.2023.12.03 val PER: 0.1838
2026-01-05 13:36:57,823: t15.2023.12.08 val PER: 0.1804
2026-01-05 13:36:57,824: t15.2023.12.10 val PER: 0.1485
2026-01-05 13:36:57,824: t15.2023.12.17 val PER: 0.2110
2026-01-05 13:36:57,824: t15.2023.12.29 val PER: 0.2155
2026-01-05 13:36:57,824: t15.2024.02.25 val PER: 0.1784
2026-01-05 13:36:57,824: t15.2024.03.08 val PER: 0.2859
2026-01-05 13:36:57,824: t15.2024.03.15 val PER: 0.2689
2026-01-05 13:36:57,824: t15.2024.03.17 val PER: 0.2183
2026-01-05 13:36:57,824: t15.2024.05.10 val PER: 0.2214
2026-01-05 13:36:57,824: t15.2024.06.14 val PER: 0.2334
2026-01-05 13:36:57,825: t15.2024.07.19 val PER: 0.3171
2026-01-05 13:36:57,825: t15.2024.07.21 val PER: 0.1566
2026-01-05 13:36:57,825: t15.2024.07.28 val PER: 0.2051
2026-01-05 13:36:57,825: t15.2025.01.10 val PER: 0.3871
2026-01-05 13:36:57,825: t15.2025.01.12 val PER: 0.2256
2026-01-05 13:36:57,825: t15.2025.03.14 val PER: 0.3654
2026-01-05 13:36:57,825: t15.2025.03.16 val PER: 0.2487
2026-01-05 13:36:57,825: t15.2025.03.30 val PER: 0.3609
2026-01-05 13:36:57,825: t15.2025.04.13 val PER: 0.2882
2026-01-05 13:36:57,826: New best val WER(1gram) 59.64% --> 56.60%
2026-01-05 13:36:57,826: Checkpointing model
2026-01-05 13:36:58,482: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:36:58,795: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_5500
2026-01-05 13:37:08,236: Train batch 5600: loss: 19.27 grad norm: 69.32 time: 0.062
2026-01-05 13:37:27,055: Train batch 5800: loss: 13.23 grad norm: 61.13 time: 0.083
2026-01-05 13:37:46,044: Train batch 6000: loss: 14.04 grad norm: 59.19 time: 0.049
2026-01-05 13:37:46,044: Running test after training batch: 6000
2026-01-05 13:37:46,159: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:37:51,394: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-05 13:37:51,428: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 13:37:53,189: Val batch 6000: PER (avg): 0.2108 CTC Loss (avg): 20.5265 WER(1gram): 58.63% (n=64) time: 7.144
2026-01-05 13:37:53,189: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 13:37:53,190: t15.2023.08.13 val PER: 0.1767
2026-01-05 13:37:53,190: t15.2023.08.18 val PER: 0.1685
2026-01-05 13:37:53,190: t15.2023.08.20 val PER: 0.1652
2026-01-05 13:37:53,190: t15.2023.08.25 val PER: 0.1160
2026-01-05 13:37:53,190: t15.2023.08.27 val PER: 0.2347
2026-01-05 13:37:53,190: t15.2023.09.01 val PER: 0.1226
2026-01-05 13:37:53,190: t15.2023.09.03 val PER: 0.2114
2026-01-05 13:37:53,190: t15.2023.09.24 val PER: 0.1650
2026-01-05 13:37:53,190: t15.2023.09.29 val PER: 0.1672
2026-01-05 13:37:53,190: t15.2023.10.01 val PER: 0.2285
2026-01-05 13:37:53,190: t15.2023.10.06 val PER: 0.1313
2026-01-05 13:37:53,191: t15.2023.10.08 val PER: 0.2842
2026-01-05 13:37:53,191: t15.2023.10.13 val PER: 0.2731
2026-01-05 13:37:53,191: t15.2023.10.15 val PER: 0.2070
2026-01-05 13:37:53,191: t15.2023.10.20 val PER: 0.2047
2026-01-05 13:37:53,191: t15.2023.10.22 val PER: 0.1737
2026-01-05 13:37:53,191: t15.2023.11.03 val PER: 0.2381
2026-01-05 13:37:53,191: t15.2023.11.04 val PER: 0.0580
2026-01-05 13:37:53,191: t15.2023.11.17 val PER: 0.0871
2026-01-05 13:37:53,191: t15.2023.11.19 val PER: 0.0778
2026-01-05 13:37:53,191: t15.2023.11.26 val PER: 0.2167
2026-01-05 13:37:53,191: t15.2023.12.03 val PER: 0.1712
2026-01-05 13:37:53,191: t15.2023.12.08 val PER: 0.1724
2026-01-05 13:37:53,191: t15.2023.12.10 val PER: 0.1445
2026-01-05 13:37:53,192: t15.2023.12.17 val PER: 0.1923
2026-01-05 13:37:53,192: t15.2023.12.29 val PER: 0.2189
2026-01-05 13:37:53,192: t15.2024.02.25 val PER: 0.1699
2026-01-05 13:37:53,192: t15.2024.03.08 val PER: 0.2945
2026-01-05 13:37:53,192: t15.2024.03.15 val PER: 0.2677
2026-01-05 13:37:53,192: t15.2024.03.17 val PER: 0.2176
2026-01-05 13:37:53,192: t15.2024.05.10 val PER: 0.2125
2026-01-05 13:37:53,192: t15.2024.06.14 val PER: 0.2177
2026-01-05 13:37:53,192: t15.2024.07.19 val PER: 0.3092
2026-01-05 13:37:53,192: t15.2024.07.21 val PER: 0.1621
2026-01-05 13:37:53,192: t15.2024.07.28 val PER: 0.2015
2026-01-05 13:37:53,192: t15.2025.01.10 val PER: 0.3747
2026-01-05 13:37:53,193: t15.2025.01.12 val PER: 0.2240
2026-01-05 13:37:53,193: t15.2025.03.14 val PER: 0.3846
2026-01-05 13:37:53,193: t15.2025.03.16 val PER: 0.2552
2026-01-05 13:37:53,193: t15.2025.03.30 val PER: 0.3563
2026-01-05 13:37:53,193: t15.2025.04.13 val PER: 0.2611
2026-01-05 13:37:53,493: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_6000
2026-01-05 13:38:11,982: Train batch 6200: loss: 15.53 grad norm: 60.55 time: 0.070
2026-01-05 13:38:30,524: Train batch 6400: loss: 18.77 grad norm: 65.18 time: 0.062
2026-01-05 13:38:39,709: Running test after training batch: 6500
2026-01-05 13:38:39,861: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:38:44,978: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the good at this point as will
2026-01-05 13:38:45,009: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 13:38:46,694: Val batch 6500: PER (avg): 0.2039 CTC Loss (avg): 20.0879 WER(1gram): 54.06% (n=64) time: 6.984
2026-01-05 13:38:46,695: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 13:38:46,695: t15.2023.08.13 val PER: 0.1757
2026-01-05 13:38:46,695: t15.2023.08.18 val PER: 0.1425
2026-01-05 13:38:46,695: t15.2023.08.20 val PER: 0.1573
2026-01-05 13:38:46,695: t15.2023.08.25 val PER: 0.1039
2026-01-05 13:38:46,695: t15.2023.08.27 val PER: 0.2235
2026-01-05 13:38:46,695: t15.2023.09.01 val PER: 0.1177
2026-01-05 13:38:46,696: t15.2023.09.03 val PER: 0.2007
2026-01-05 13:38:46,696: t15.2023.09.24 val PER: 0.1687
2026-01-05 13:38:46,696: t15.2023.09.29 val PER: 0.1634
2026-01-05 13:38:46,696: t15.2023.10.01 val PER: 0.2153
2026-01-05 13:38:46,696: t15.2023.10.06 val PER: 0.1259
2026-01-05 13:38:46,696: t15.2023.10.08 val PER: 0.2950
2026-01-05 13:38:46,696: t15.2023.10.13 val PER: 0.2614
2026-01-05 13:38:46,696: t15.2023.10.15 val PER: 0.2149
2026-01-05 13:38:46,696: t15.2023.10.20 val PER: 0.2315
2026-01-05 13:38:46,697: t15.2023.10.22 val PER: 0.1604
2026-01-05 13:38:46,697: t15.2023.11.03 val PER: 0.2232
2026-01-05 13:38:46,697: t15.2023.11.04 val PER: 0.0546
2026-01-05 13:38:46,697: t15.2023.11.17 val PER: 0.0700
2026-01-05 13:38:46,697: t15.2023.11.19 val PER: 0.0758
2026-01-05 13:38:46,697: t15.2023.11.26 val PER: 0.1993
2026-01-05 13:38:46,697: t15.2023.12.03 val PER: 0.1681
2026-01-05 13:38:46,697: t15.2023.12.08 val PER: 0.1678
2026-01-05 13:38:46,697: t15.2023.12.10 val PER: 0.1380
2026-01-05 13:38:46,697: t15.2023.12.17 val PER: 0.1954
2026-01-05 13:38:46,697: t15.2023.12.29 val PER: 0.2025
2026-01-05 13:38:46,697: t15.2024.02.25 val PER: 0.1728
2026-01-05 13:38:46,697: t15.2024.03.08 val PER: 0.2802
2026-01-05 13:38:46,697: t15.2024.03.15 val PER: 0.2627
2026-01-05 13:38:46,697: t15.2024.03.17 val PER: 0.2029
2026-01-05 13:38:46,698: t15.2024.05.10 val PER: 0.2348
2026-01-05 13:38:46,698: t15.2024.06.14 val PER: 0.2224
2026-01-05 13:38:46,698: t15.2024.07.19 val PER: 0.3085
2026-01-05 13:38:46,698: t15.2024.07.21 val PER: 0.1510
2026-01-05 13:38:46,698: t15.2024.07.28 val PER: 0.1779
2026-01-05 13:38:46,698: t15.2025.01.10 val PER: 0.3843
2026-01-05 13:38:46,698: t15.2025.01.12 val PER: 0.2086
2026-01-05 13:38:46,698: t15.2025.03.14 val PER: 0.3905
2026-01-05 13:38:46,698: t15.2025.03.16 val PER: 0.2330
2026-01-05 13:38:46,698: t15.2025.03.30 val PER: 0.3552
2026-01-05 13:38:46,698: t15.2025.04.13 val PER: 0.2653
2026-01-05 13:38:46,699: New best val WER(1gram) 56.60% --> 54.06%
2026-01-05 13:38:46,699: Checkpointing model
2026-01-05 13:38:47,390: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:38:47,696: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_6500
2026-01-05 13:38:56,876: Train batch 6600: loss: 12.05 grad norm: 54.83 time: 0.045
2026-01-05 13:39:15,712: Train batch 6800: loss: 15.02 grad norm: 56.80 time: 0.048
2026-01-05 13:39:34,671: Train batch 7000: loss: 16.63 grad norm: 65.58 time: 0.061
2026-01-05 13:39:34,672: Running test after training batch: 7000
2026-01-05 13:39:34,848: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:39:40,311: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:39:40,342: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost nett
2026-01-05 13:39:42,079: Val batch 7000: PER (avg): 0.1943 CTC Loss (avg): 19.2047 WER(1gram): 56.60% (n=64) time: 7.407
2026-01-05 13:39:42,079: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-05 13:39:42,080: t15.2023.08.13 val PER: 0.1611
2026-01-05 13:39:42,080: t15.2023.08.18 val PER: 0.1341
2026-01-05 13:39:42,080: t15.2023.08.20 val PER: 0.1517
2026-01-05 13:39:42,080: t15.2023.08.25 val PER: 0.1160
2026-01-05 13:39:42,080: t15.2023.08.27 val PER: 0.2203
2026-01-05 13:39:42,080: t15.2023.09.01 val PER: 0.1161
2026-01-05 13:39:42,080: t15.2023.09.03 val PER: 0.1912
2026-01-05 13:39:42,080: t15.2023.09.24 val PER: 0.1614
2026-01-05 13:39:42,080: t15.2023.09.29 val PER: 0.1646
2026-01-05 13:39:42,080: t15.2023.10.01 val PER: 0.2021
2026-01-05 13:39:42,081: t15.2023.10.06 val PER: 0.1216
2026-01-05 13:39:42,081: t15.2023.10.08 val PER: 0.2720
2026-01-05 13:39:42,081: t15.2023.10.13 val PER: 0.2591
2026-01-05 13:39:42,081: t15.2023.10.15 val PER: 0.1931
2026-01-05 13:39:42,081: t15.2023.10.20 val PER: 0.2215
2026-01-05 13:39:42,081: t15.2023.10.22 val PER: 0.1459
2026-01-05 13:39:42,081: t15.2023.11.03 val PER: 0.2042
2026-01-05 13:39:42,081: t15.2023.11.04 val PER: 0.0444
2026-01-05 13:39:42,081: t15.2023.11.17 val PER: 0.0653
2026-01-05 13:39:42,081: t15.2023.11.19 val PER: 0.0679
2026-01-05 13:39:42,082: t15.2023.11.26 val PER: 0.1855
2026-01-05 13:39:42,082: t15.2023.12.03 val PER: 0.1660
2026-01-05 13:39:42,082: t15.2023.12.08 val PER: 0.1531
2026-01-05 13:39:42,082: t15.2023.12.10 val PER: 0.1459
2026-01-05 13:39:42,082: t15.2023.12.17 val PER: 0.1705
2026-01-05 13:39:42,082: t15.2023.12.29 val PER: 0.1942
2026-01-05 13:39:42,082: t15.2024.02.25 val PER: 0.1461
2026-01-05 13:39:42,082: t15.2024.03.08 val PER: 0.2845
2026-01-05 13:39:42,082: t15.2024.03.15 val PER: 0.2483
2026-01-05 13:39:42,082: t15.2024.03.17 val PER: 0.1904
2026-01-05 13:39:42,082: t15.2024.05.10 val PER: 0.2110
2026-01-05 13:39:42,082: t15.2024.06.14 val PER: 0.2192
2026-01-05 13:39:42,083: t15.2024.07.19 val PER: 0.3026
2026-01-05 13:39:42,083: t15.2024.07.21 val PER: 0.1331
2026-01-05 13:39:42,083: t15.2024.07.28 val PER: 0.1713
2026-01-05 13:39:42,083: t15.2025.01.10 val PER: 0.3719
2026-01-05 13:39:42,083: t15.2025.01.12 val PER: 0.2125
2026-01-05 13:39:42,083: t15.2025.03.14 val PER: 0.3624
2026-01-05 13:39:42,083: t15.2025.03.16 val PER: 0.2212
2026-01-05 13:39:42,083: t15.2025.03.30 val PER: 0.3517
2026-01-05 13:39:42,083: t15.2025.04.13 val PER: 0.2653
2026-01-05 13:39:42,365: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_7000
2026-01-05 13:40:01,326: Train batch 7200: loss: 13.38 grad norm: 56.65 time: 0.079
2026-01-05 13:40:20,048: Train batch 7400: loss: 13.32 grad norm: 56.73 time: 0.075
2026-01-05 13:40:29,440: Running test after training batch: 7500
2026-01-05 13:40:29,565: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:40:34,705: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-05 13:40:34,737: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-05 13:40:36,484: Val batch 7500: PER (avg): 0.1893 CTC Loss (avg): 18.5990 WER(1gram): 56.09% (n=64) time: 7.043
2026-01-05 13:40:36,484: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 13:40:36,484: t15.2023.08.13 val PER: 0.1549
2026-01-05 13:40:36,485: t15.2023.08.18 val PER: 0.1358
2026-01-05 13:40:36,485: t15.2023.08.20 val PER: 0.1438
2026-01-05 13:40:36,485: t15.2023.08.25 val PER: 0.0964
2026-01-05 13:40:36,485: t15.2023.08.27 val PER: 0.2042
2026-01-05 13:40:36,485: t15.2023.09.01 val PER: 0.1055
2026-01-05 13:40:36,485: t15.2023.09.03 val PER: 0.1948
2026-01-05 13:40:36,485: t15.2023.09.24 val PER: 0.1553
2026-01-05 13:40:36,485: t15.2023.09.29 val PER: 0.1627
2026-01-05 13:40:36,486: t15.2023.10.01 val PER: 0.2034
2026-01-05 13:40:36,486: t15.2023.10.06 val PER: 0.1076
2026-01-05 13:40:36,486: t15.2023.10.08 val PER: 0.2760
2026-01-05 13:40:36,486: t15.2023.10.13 val PER: 0.2545
2026-01-05 13:40:36,486: t15.2023.10.15 val PER: 0.1912
2026-01-05 13:40:36,486: t15.2023.10.20 val PER: 0.1980
2026-01-05 13:40:36,486: t15.2023.10.22 val PER: 0.1403
2026-01-05 13:40:36,486: t15.2023.11.03 val PER: 0.2008
2026-01-05 13:40:36,486: t15.2023.11.04 val PER: 0.0614
2026-01-05 13:40:36,486: t15.2023.11.17 val PER: 0.0591
2026-01-05 13:40:36,486: t15.2023.11.19 val PER: 0.0579
2026-01-05 13:40:36,486: t15.2023.11.26 val PER: 0.1833
2026-01-05 13:40:36,486: t15.2023.12.03 val PER: 0.1460
2026-01-05 13:40:36,487: t15.2023.12.08 val PER: 0.1578
2026-01-05 13:40:36,487: t15.2023.12.10 val PER: 0.1248
2026-01-05 13:40:36,487: t15.2023.12.17 val PER: 0.1746
2026-01-05 13:40:36,487: t15.2023.12.29 val PER: 0.1867
2026-01-05 13:40:36,487: t15.2024.02.25 val PER: 0.1601
2026-01-05 13:40:36,487: t15.2024.03.08 val PER: 0.2760
2026-01-05 13:40:36,487: t15.2024.03.15 val PER: 0.2383
2026-01-05 13:40:36,487: t15.2024.03.17 val PER: 0.1848
2026-01-05 13:40:36,487: t15.2024.05.10 val PER: 0.2110
2026-01-05 13:40:36,487: t15.2024.06.14 val PER: 0.2035
2026-01-05 13:40:36,487: t15.2024.07.19 val PER: 0.2940
2026-01-05 13:40:36,487: t15.2024.07.21 val PER: 0.1331
2026-01-05 13:40:36,487: t15.2024.07.28 val PER: 0.1757
2026-01-05 13:40:36,487: t15.2025.01.10 val PER: 0.3416
2026-01-05 13:40:36,488: t15.2025.01.12 val PER: 0.1986
2026-01-05 13:40:36,488: t15.2025.03.14 val PER: 0.3861
2026-01-05 13:40:36,488: t15.2025.03.16 val PER: 0.2277
2026-01-05 13:40:36,488: t15.2025.03.30 val PER: 0.3425
2026-01-05 13:40:36,488: t15.2025.04.13 val PER: 0.2468
2026-01-05 13:40:36,775: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_7500
2026-01-05 13:40:46,148: Train batch 7600: loss: 15.73 grad norm: 61.77 time: 0.069
2026-01-05 13:41:05,084: Train batch 7800: loss: 13.47 grad norm: 58.77 time: 0.056
2026-01-05 13:41:24,264: Train batch 8000: loss: 10.74 grad norm: 52.93 time: 0.072
2026-01-05 13:41:24,265: Running test after training batch: 8000
2026-01-05 13:41:24,382: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:41:29,507: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-05 13:41:29,539: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nit
2026-01-05 13:41:31,297: Val batch 8000: PER (avg): 0.1816 CTC Loss (avg): 17.7785 WER(1gram): 53.55% (n=64) time: 7.033
2026-01-05 13:41:31,298: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-05 13:41:31,298: t15.2023.08.13 val PER: 0.1383
2026-01-05 13:41:31,298: t15.2023.08.18 val PER: 0.1299
2026-01-05 13:41:31,298: t15.2023.08.20 val PER: 0.1454
2026-01-05 13:41:31,298: t15.2023.08.25 val PER: 0.1039
2026-01-05 13:41:31,298: t15.2023.08.27 val PER: 0.2138
2026-01-05 13:41:31,298: t15.2023.09.01 val PER: 0.0974
2026-01-05 13:41:31,299: t15.2023.09.03 val PER: 0.1746
2026-01-05 13:41:31,299: t15.2023.09.24 val PER: 0.1456
2026-01-05 13:41:31,299: t15.2023.09.29 val PER: 0.1493
2026-01-05 13:41:31,299: t15.2023.10.01 val PER: 0.2054
2026-01-05 13:41:31,299: t15.2023.10.06 val PER: 0.1109
2026-01-05 13:41:31,299: t15.2023.10.08 val PER: 0.2639
2026-01-05 13:41:31,299: t15.2023.10.13 val PER: 0.2420
2026-01-05 13:41:31,299: t15.2023.10.15 val PER: 0.1793
2026-01-05 13:41:31,299: t15.2023.10.20 val PER: 0.1913
2026-01-05 13:41:31,299: t15.2023.10.22 val PER: 0.1459
2026-01-05 13:41:31,299: t15.2023.11.03 val PER: 0.1995
2026-01-05 13:41:31,300: t15.2023.11.04 val PER: 0.0410
2026-01-05 13:41:31,300: t15.2023.11.17 val PER: 0.0575
2026-01-05 13:41:31,300: t15.2023.11.19 val PER: 0.0599
2026-01-05 13:41:31,300: t15.2023.11.26 val PER: 0.1790
2026-01-05 13:41:31,300: t15.2023.12.03 val PER: 0.1408
2026-01-05 13:41:31,300: t15.2023.12.08 val PER: 0.1418
2026-01-05 13:41:31,300: t15.2023.12.10 val PER: 0.1209
2026-01-05 13:41:31,300: t15.2023.12.17 val PER: 0.1715
2026-01-05 13:41:31,300: t15.2023.12.29 val PER: 0.1688
2026-01-05 13:41:31,300: t15.2024.02.25 val PER: 0.1390
2026-01-05 13:41:31,300: t15.2024.03.08 val PER: 0.2674
2026-01-05 13:41:31,300: t15.2024.03.15 val PER: 0.2383
2026-01-05 13:41:31,301: t15.2024.03.17 val PER: 0.1757
2026-01-05 13:41:31,301: t15.2024.05.10 val PER: 0.1902
2026-01-05 13:41:31,301: t15.2024.06.14 val PER: 0.2035
2026-01-05 13:41:31,301: t15.2024.07.19 val PER: 0.2881
2026-01-05 13:41:31,301: t15.2024.07.21 val PER: 0.1193
2026-01-05 13:41:31,301: t15.2024.07.28 val PER: 0.1529
2026-01-05 13:41:31,301: t15.2025.01.10 val PER: 0.3361
2026-01-05 13:41:31,301: t15.2025.01.12 val PER: 0.1871
2026-01-05 13:41:31,301: t15.2025.03.14 val PER: 0.3669
2026-01-05 13:41:31,301: t15.2025.03.16 val PER: 0.2317
2026-01-05 13:41:31,301: t15.2025.03.30 val PER: 0.3322
2026-01-05 13:41:31,301: t15.2025.04.13 val PER: 0.2525
2026-01-05 13:41:31,302: New best val WER(1gram) 54.06% --> 53.55%
2026-01-05 13:41:31,302: Checkpointing model
2026-01-05 13:41:31,994: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:41:32,300: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_8000
2026-01-05 13:41:51,110: Train batch 8200: loss: 9.55 grad norm: 50.00 time: 0.055
2026-01-05 13:42:09,857: Train batch 8400: loss: 9.73 grad norm: 47.57 time: 0.064
2026-01-05 13:42:19,393: Running test after training batch: 8500
2026-01-05 13:42:19,496: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:42:24,916: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point us will
2026-01-05 13:42:24,950: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nit
2026-01-05 13:42:26,815: Val batch 8500: PER (avg): 0.1779 CTC Loss (avg): 17.6359 WER(1gram): 51.52% (n=64) time: 7.422
2026-01-05 13:42:26,815: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 13:42:26,816: t15.2023.08.13 val PER: 0.1362
2026-01-05 13:42:26,816: t15.2023.08.18 val PER: 0.1299
2026-01-05 13:42:26,816: t15.2023.08.20 val PER: 0.1382
2026-01-05 13:42:26,816: t15.2023.08.25 val PER: 0.1114
2026-01-05 13:42:26,816: t15.2023.08.27 val PER: 0.2074
2026-01-05 13:42:26,816: t15.2023.09.01 val PER: 0.1015
2026-01-05 13:42:26,816: t15.2023.09.03 val PER: 0.1853
2026-01-05 13:42:26,816: t15.2023.09.24 val PER: 0.1420
2026-01-05 13:42:26,816: t15.2023.09.29 val PER: 0.1519
2026-01-05 13:42:26,816: t15.2023.10.01 val PER: 0.2008
2026-01-05 13:42:26,816: t15.2023.10.06 val PER: 0.0958
2026-01-05 13:42:26,816: t15.2023.10.08 val PER: 0.2585
2026-01-05 13:42:26,817: t15.2023.10.13 val PER: 0.2327
2026-01-05 13:42:26,817: t15.2023.10.15 val PER: 0.1793
2026-01-05 13:42:26,817: t15.2023.10.20 val PER: 0.2013
2026-01-05 13:42:26,817: t15.2023.10.22 val PER: 0.1414
2026-01-05 13:42:26,817: t15.2023.11.03 val PER: 0.1934
2026-01-05 13:42:26,817: t15.2023.11.04 val PER: 0.0546
2026-01-05 13:42:26,817: t15.2023.11.17 val PER: 0.0560
2026-01-05 13:42:26,817: t15.2023.11.19 val PER: 0.0399
2026-01-05 13:42:26,817: t15.2023.11.26 val PER: 0.1732
2026-01-05 13:42:26,817: t15.2023.12.03 val PER: 0.1471
2026-01-05 13:42:26,817: t15.2023.12.08 val PER: 0.1318
2026-01-05 13:42:26,817: t15.2023.12.10 val PER: 0.1156
2026-01-05 13:42:26,817: t15.2023.12.17 val PER: 0.1736
2026-01-05 13:42:26,818: t15.2023.12.29 val PER: 0.1688
2026-01-05 13:42:26,818: t15.2024.02.25 val PER: 0.1489
2026-01-05 13:42:26,818: t15.2024.03.08 val PER: 0.2632
2026-01-05 13:42:26,818: t15.2024.03.15 val PER: 0.2333
2026-01-05 13:42:26,818: t15.2024.03.17 val PER: 0.1618
2026-01-05 13:42:26,818: t15.2024.05.10 val PER: 0.1842
2026-01-05 13:42:26,818: t15.2024.06.14 val PER: 0.1956
2026-01-05 13:42:26,818: t15.2024.07.19 val PER: 0.2755
2026-01-05 13:42:26,818: t15.2024.07.21 val PER: 0.1172
2026-01-05 13:42:26,818: t15.2024.07.28 val PER: 0.1654
2026-01-05 13:42:26,819: t15.2025.01.10 val PER: 0.3306
2026-01-05 13:42:26,819: t15.2025.01.12 val PER: 0.1932
2026-01-05 13:42:26,819: t15.2025.03.14 val PER: 0.3447
2026-01-05 13:42:26,819: t15.2025.03.16 val PER: 0.2042
2026-01-05 13:42:26,819: t15.2025.03.30 val PER: 0.3253
2026-01-05 13:42:26,819: t15.2025.04.13 val PER: 0.2354
2026-01-05 13:42:26,820: New best val WER(1gram) 53.55% --> 51.52%
2026-01-05 13:42:26,821: Checkpointing model
2026-01-05 13:42:27,543: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:42:27,850: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_8500
2026-01-05 13:42:37,228: Train batch 8600: loss: 15.33 grad norm: 56.84 time: 0.055
2026-01-05 13:42:55,977: Train batch 8800: loss: 14.86 grad norm: 59.74 time: 0.061
2026-01-05 13:43:15,055: Train batch 9000: loss: 15.00 grad norm: 63.27 time: 0.072
2026-01-05 13:43:15,055: Running test after training batch: 9000
2026-01-05 13:43:15,195: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:43:20,437: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 13:43:20,469: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-05 13:43:22,266: Val batch 9000: PER (avg): 0.1727 CTC Loss (avg): 17.1638 WER(1gram): 51.78% (n=64) time: 7.211
2026-01-05 13:43:22,267: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-05 13:43:22,267: t15.2023.08.13 val PER: 0.1351
2026-01-05 13:43:22,267: t15.2023.08.18 val PER: 0.1249
2026-01-05 13:43:22,267: t15.2023.08.20 val PER: 0.1271
2026-01-05 13:43:22,267: t15.2023.08.25 val PER: 0.0964
2026-01-05 13:43:22,267: t15.2023.08.27 val PER: 0.2058
2026-01-05 13:43:22,267: t15.2023.09.01 val PER: 0.0942
2026-01-05 13:43:22,267: t15.2023.09.03 val PER: 0.1770
2026-01-05 13:43:22,267: t15.2023.09.24 val PER: 0.1396
2026-01-05 13:43:22,268: t15.2023.09.29 val PER: 0.1493
2026-01-05 13:43:22,268: t15.2023.10.01 val PER: 0.2008
2026-01-05 13:43:22,268: t15.2023.10.06 val PER: 0.0980
2026-01-05 13:43:22,268: t15.2023.10.08 val PER: 0.2585
2026-01-05 13:43:22,268: t15.2023.10.13 val PER: 0.2335
2026-01-05 13:43:22,268: t15.2023.10.15 val PER: 0.1753
2026-01-05 13:43:22,268: t15.2023.10.20 val PER: 0.1812
2026-01-05 13:43:22,269: t15.2023.10.22 val PER: 0.1303
2026-01-05 13:43:22,269: t15.2023.11.03 val PER: 0.2035
2026-01-05 13:43:22,269: t15.2023.11.04 val PER: 0.0410
2026-01-05 13:43:22,269: t15.2023.11.17 val PER: 0.0529
2026-01-05 13:43:22,269: t15.2023.11.19 val PER: 0.0519
2026-01-05 13:43:22,269: t15.2023.11.26 val PER: 0.1681
2026-01-05 13:43:22,269: t15.2023.12.03 val PER: 0.1355
2026-01-05 13:43:22,269: t15.2023.12.08 val PER: 0.1305
2026-01-05 13:43:22,270: t15.2023.12.10 val PER: 0.1130
2026-01-05 13:43:22,270: t15.2023.12.17 val PER: 0.1538
2026-01-05 13:43:22,270: t15.2023.12.29 val PER: 0.1585
2026-01-05 13:43:22,270: t15.2024.02.25 val PER: 0.1433
2026-01-05 13:43:22,270: t15.2024.03.08 val PER: 0.2603
2026-01-05 13:43:22,270: t15.2024.03.15 val PER: 0.2233
2026-01-05 13:43:22,270: t15.2024.03.17 val PER: 0.1681
2026-01-05 13:43:22,270: t15.2024.05.10 val PER: 0.1649
2026-01-05 13:43:22,270: t15.2024.06.14 val PER: 0.1877
2026-01-05 13:43:22,271: t15.2024.07.19 val PER: 0.2709
2026-01-05 13:43:22,271: t15.2024.07.21 val PER: 0.1103
2026-01-05 13:43:22,271: t15.2024.07.28 val PER: 0.1537
2026-01-05 13:43:22,271: t15.2025.01.10 val PER: 0.3113
2026-01-05 13:43:22,271: t15.2025.01.12 val PER: 0.1786
2026-01-05 13:43:22,271: t15.2025.03.14 val PER: 0.3447
2026-01-05 13:43:22,271: t15.2025.03.16 val PER: 0.2055
2026-01-05 13:43:22,271: t15.2025.03.30 val PER: 0.3149
2026-01-05 13:43:22,271: t15.2025.04.13 val PER: 0.2411
2026-01-05 13:43:22,587: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_9000
2026-01-05 13:43:41,356: Train batch 9200: loss: 10.35 grad norm: 49.36 time: 0.056
2026-01-05 13:43:59,984: Train batch 9400: loss: 6.83 grad norm: 41.32 time: 0.068
2026-01-05 13:44:09,583: Running test after training batch: 9500
2026-01-05 13:44:09,752: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:44:14,868: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:44:14,901: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 13:44:16,696: Val batch 9500: PER (avg): 0.1707 CTC Loss (avg): 17.0575 WER(1gram): 48.98% (n=64) time: 7.113
2026-01-05 13:44:16,697: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 13:44:16,697: t15.2023.08.13 val PER: 0.1372
2026-01-05 13:44:16,697: t15.2023.08.18 val PER: 0.1190
2026-01-05 13:44:16,697: t15.2023.08.20 val PER: 0.1279
2026-01-05 13:44:16,697: t15.2023.08.25 val PER: 0.0979
2026-01-05 13:44:16,697: t15.2023.08.27 val PER: 0.2042
2026-01-05 13:44:16,697: t15.2023.09.01 val PER: 0.0860
2026-01-05 13:44:16,698: t15.2023.09.03 val PER: 0.1770
2026-01-05 13:44:16,698: t15.2023.09.24 val PER: 0.1396
2026-01-05 13:44:16,698: t15.2023.09.29 val PER: 0.1423
2026-01-05 13:44:16,698: t15.2023.10.01 val PER: 0.1889
2026-01-05 13:44:16,698: t15.2023.10.06 val PER: 0.1012
2026-01-05 13:44:16,698: t15.2023.10.08 val PER: 0.2598
2026-01-05 13:44:16,698: t15.2023.10.13 val PER: 0.2320
2026-01-05 13:44:16,698: t15.2023.10.15 val PER: 0.1800
2026-01-05 13:44:16,698: t15.2023.10.20 val PER: 0.1846
2026-01-05 13:44:16,698: t15.2023.10.22 val PER: 0.1325
2026-01-05 13:44:16,698: t15.2023.11.03 val PER: 0.1974
2026-01-05 13:44:16,698: t15.2023.11.04 val PER: 0.0341
2026-01-05 13:44:16,699: t15.2023.11.17 val PER: 0.0529
2026-01-05 13:44:16,699: t15.2023.11.19 val PER: 0.0499
2026-01-05 13:44:16,699: t15.2023.11.26 val PER: 0.1659
2026-01-05 13:44:16,699: t15.2023.12.03 val PER: 0.1429
2026-01-05 13:44:16,699: t15.2023.12.08 val PER: 0.1318
2026-01-05 13:44:16,699: t15.2023.12.10 val PER: 0.1078
2026-01-05 13:44:16,699: t15.2023.12.17 val PER: 0.1580
2026-01-05 13:44:16,699: t15.2023.12.29 val PER: 0.1482
2026-01-05 13:44:16,699: t15.2024.02.25 val PER: 0.1348
2026-01-05 13:44:16,699: t15.2024.03.08 val PER: 0.2461
2026-01-05 13:44:16,699: t15.2024.03.15 val PER: 0.2258
2026-01-05 13:44:16,699: t15.2024.03.17 val PER: 0.1667
2026-01-05 13:44:16,699: t15.2024.05.10 val PER: 0.1828
2026-01-05 13:44:16,699: t15.2024.06.14 val PER: 0.1924
2026-01-05 13:44:16,699: t15.2024.07.19 val PER: 0.2531
2026-01-05 13:44:16,700: t15.2024.07.21 val PER: 0.1131
2026-01-05 13:44:16,700: t15.2024.07.28 val PER: 0.1463
2026-01-05 13:44:16,700: t15.2025.01.10 val PER: 0.3085
2026-01-05 13:44:16,700: t15.2025.01.12 val PER: 0.1794
2026-01-05 13:44:16,700: t15.2025.03.14 val PER: 0.3639
2026-01-05 13:44:16,700: t15.2025.03.16 val PER: 0.2081
2026-01-05 13:44:16,700: t15.2025.03.30 val PER: 0.3092
2026-01-05 13:44:16,700: t15.2025.04.13 val PER: 0.2340
2026-01-05 13:44:16,701: New best val WER(1gram) 51.52% --> 48.98%
2026-01-05 13:44:16,701: Checkpointing model
2026-01-05 13:44:17,342: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:44:17,677: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_9500
2026-01-05 13:44:27,167: Train batch 9600: loss: 8.15 grad norm: 50.07 time: 0.073
2026-01-05 13:44:46,042: Train batch 9800: loss: 11.54 grad norm: 54.72 time: 0.063
2026-01-05 13:45:05,199: Train batch 10000: loss: 5.34 grad norm: 39.21 time: 0.061
2026-01-05 13:45:05,200: Running test after training batch: 10000
2026-01-05 13:45:05,309: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:45:10,614: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 13:45:10,648: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sette
2026-01-05 13:45:12,480: Val batch 10000: PER (avg): 0.1676 CTC Loss (avg): 16.7381 WER(1gram): 51.02% (n=64) time: 7.281
2026-01-05 13:45:12,481: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 13:45:12,481: t15.2023.08.13 val PER: 0.1320
2026-01-05 13:45:12,481: t15.2023.08.18 val PER: 0.1224
2026-01-05 13:45:12,481: t15.2023.08.20 val PER: 0.1215
2026-01-05 13:45:12,481: t15.2023.08.25 val PER: 0.1009
2026-01-05 13:45:12,481: t15.2023.08.27 val PER: 0.1994
2026-01-05 13:45:12,482: t15.2023.09.01 val PER: 0.0787
2026-01-05 13:45:12,482: t15.2023.09.03 val PER: 0.1734
2026-01-05 13:45:12,482: t15.2023.09.24 val PER: 0.1432
2026-01-05 13:45:12,482: t15.2023.09.29 val PER: 0.1481
2026-01-05 13:45:12,482: t15.2023.10.01 val PER: 0.1896
2026-01-05 13:45:12,482: t15.2023.10.06 val PER: 0.1012
2026-01-05 13:45:12,482: t15.2023.10.08 val PER: 0.2544
2026-01-05 13:45:12,482: t15.2023.10.13 val PER: 0.2296
2026-01-05 13:45:12,482: t15.2023.10.15 val PER: 0.1648
2026-01-05 13:45:12,482: t15.2023.10.20 val PER: 0.1745
2026-01-05 13:45:12,483: t15.2023.10.22 val PER: 0.1247
2026-01-05 13:45:12,483: t15.2023.11.03 val PER: 0.1859
2026-01-05 13:45:12,483: t15.2023.11.04 val PER: 0.0273
2026-01-05 13:45:12,483: t15.2023.11.17 val PER: 0.0404
2026-01-05 13:45:12,483: t15.2023.11.19 val PER: 0.0399
2026-01-05 13:45:12,483: t15.2023.11.26 val PER: 0.1543
2026-01-05 13:45:12,483: t15.2023.12.03 val PER: 0.1387
2026-01-05 13:45:12,483: t15.2023.12.08 val PER: 0.1245
2026-01-05 13:45:12,483: t15.2023.12.10 val PER: 0.1104
2026-01-05 13:45:12,483: t15.2023.12.17 val PER: 0.1601
2026-01-05 13:45:12,483: t15.2023.12.29 val PER: 0.1482
2026-01-05 13:45:12,484: t15.2024.02.25 val PER: 0.1461
2026-01-05 13:45:12,484: t15.2024.03.08 val PER: 0.2390
2026-01-05 13:45:12,484: t15.2024.03.15 val PER: 0.2133
2026-01-05 13:45:12,484: t15.2024.03.17 val PER: 0.1506
2026-01-05 13:45:12,484: t15.2024.05.10 val PER: 0.1753
2026-01-05 13:45:12,484: t15.2024.06.14 val PER: 0.1877
2026-01-05 13:45:12,484: t15.2024.07.19 val PER: 0.2604
2026-01-05 13:45:12,484: t15.2024.07.21 val PER: 0.1124
2026-01-05 13:45:12,484: t15.2024.07.28 val PER: 0.1618
2026-01-05 13:45:12,484: t15.2025.01.10 val PER: 0.3003
2026-01-05 13:45:12,484: t15.2025.01.12 val PER: 0.1755
2026-01-05 13:45:12,485: t15.2025.03.14 val PER: 0.3447
2026-01-05 13:45:12,485: t15.2025.03.16 val PER: 0.2238
2026-01-05 13:45:12,485: t15.2025.03.30 val PER: 0.3195
2026-01-05 13:45:12,485: t15.2025.04.13 val PER: 0.2297
2026-01-05 13:45:12,814: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_10000
2026-01-05 13:45:31,500: Train batch 10200: loss: 6.10 grad norm: 40.03 time: 0.050
2026-01-05 13:45:50,618: Train batch 10400: loss: 8.61 grad norm: 48.15 time: 0.073
2026-01-05 13:46:00,293: Running test after training batch: 10500
2026-01-05 13:46:00,398: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:46:05,604: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:46:05,639: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-05 13:46:07,551: Val batch 10500: PER (avg): 0.1658 CTC Loss (avg): 16.6461 WER(1gram): 48.48% (n=64) time: 7.258
2026-01-05 13:46:07,552: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 13:46:07,552: t15.2023.08.13 val PER: 0.1268
2026-01-05 13:46:07,552: t15.2023.08.18 val PER: 0.1165
2026-01-05 13:46:07,552: t15.2023.08.20 val PER: 0.1191
2026-01-05 13:46:07,552: t15.2023.08.25 val PER: 0.1024
2026-01-05 13:46:07,553: t15.2023.08.27 val PER: 0.1994
2026-01-05 13:46:07,553: t15.2023.09.01 val PER: 0.0917
2026-01-05 13:46:07,553: t15.2023.09.03 val PER: 0.1746
2026-01-05 13:46:07,553: t15.2023.09.24 val PER: 0.1493
2026-01-05 13:46:07,553: t15.2023.09.29 val PER: 0.1461
2026-01-05 13:46:07,553: t15.2023.10.01 val PER: 0.1816
2026-01-05 13:46:07,553: t15.2023.10.06 val PER: 0.0915
2026-01-05 13:46:07,553: t15.2023.10.08 val PER: 0.2449
2026-01-05 13:46:07,553: t15.2023.10.13 val PER: 0.2258
2026-01-05 13:46:07,553: t15.2023.10.15 val PER: 0.1727
2026-01-05 13:46:07,554: t15.2023.10.20 val PER: 0.1879
2026-01-05 13:46:07,554: t15.2023.10.22 val PER: 0.1180
2026-01-05 13:46:07,554: t15.2023.11.03 val PER: 0.1886
2026-01-05 13:46:07,554: t15.2023.11.04 val PER: 0.0410
2026-01-05 13:46:07,554: t15.2023.11.17 val PER: 0.0529
2026-01-05 13:46:07,554: t15.2023.11.19 val PER: 0.0599
2026-01-05 13:46:07,554: t15.2023.11.26 val PER: 0.1384
2026-01-05 13:46:07,554: t15.2023.12.03 val PER: 0.1229
2026-01-05 13:46:07,555: t15.2023.12.08 val PER: 0.1192
2026-01-05 13:46:07,555: t15.2023.12.10 val PER: 0.1064
2026-01-05 13:46:07,555: t15.2023.12.17 val PER: 0.1497
2026-01-05 13:46:07,555: t15.2023.12.29 val PER: 0.1537
2026-01-05 13:46:07,555: t15.2024.02.25 val PER: 0.1419
2026-01-05 13:46:07,555: t15.2024.03.08 val PER: 0.2560
2026-01-05 13:46:07,555: t15.2024.03.15 val PER: 0.2195
2026-01-05 13:46:07,555: t15.2024.03.17 val PER: 0.1485
2026-01-05 13:46:07,555: t15.2024.05.10 val PER: 0.1753
2026-01-05 13:46:07,556: t15.2024.06.14 val PER: 0.1861
2026-01-05 13:46:07,556: t15.2024.07.19 val PER: 0.2558
2026-01-05 13:46:07,556: t15.2024.07.21 val PER: 0.1062
2026-01-05 13:46:07,556: t15.2024.07.28 val PER: 0.1382
2026-01-05 13:46:07,556: t15.2025.01.10 val PER: 0.3196
2026-01-05 13:46:07,556: t15.2025.01.12 val PER: 0.1740
2026-01-05 13:46:07,556: t15.2025.03.14 val PER: 0.3624
2026-01-05 13:46:07,556: t15.2025.03.16 val PER: 0.2055
2026-01-05 13:46:07,556: t15.2025.03.30 val PER: 0.3103
2026-01-05 13:46:07,556: t15.2025.04.13 val PER: 0.2282
2026-01-05 13:46:07,557: New best val WER(1gram) 48.98% --> 48.48%
2026-01-05 13:46:07,557: Checkpointing model
2026-01-05 13:46:08,264: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:46:08,611: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_10500
2026-01-05 13:46:18,514: Train batch 10600: loss: 9.26 grad norm: 60.71 time: 0.072
2026-01-05 13:46:37,713: Train batch 10800: loss: 14.18 grad norm: 65.61 time: 0.065
2026-01-05 13:46:57,037: Train batch 11000: loss: 13.45 grad norm: 59.44 time: 0.057
2026-01-05 13:46:57,038: Running test after training batch: 11000
2026-01-05 13:46:57,157: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:47:02,340: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:47:02,373: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-05 13:47:04,233: Val batch 11000: PER (avg): 0.1613 CTC Loss (avg): 16.3868 WER(1gram): 48.98% (n=64) time: 7.196
2026-01-05 13:47:04,234: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 13:47:04,234: t15.2023.08.13 val PER: 0.1164
2026-01-05 13:47:04,234: t15.2023.08.18 val PER: 0.1199
2026-01-05 13:47:04,235: t15.2023.08.20 val PER: 0.1136
2026-01-05 13:47:04,235: t15.2023.08.25 val PER: 0.1009
2026-01-05 13:47:04,235: t15.2023.08.27 val PER: 0.1945
2026-01-05 13:47:04,235: t15.2023.09.01 val PER: 0.0877
2026-01-05 13:47:04,235: t15.2023.09.03 val PER: 0.1746
2026-01-05 13:47:04,235: t15.2023.09.24 val PER: 0.1456
2026-01-05 13:47:04,235: t15.2023.09.29 val PER: 0.1423
2026-01-05 13:47:04,235: t15.2023.10.01 val PER: 0.1915
2026-01-05 13:47:04,235: t15.2023.10.06 val PER: 0.0915
2026-01-05 13:47:04,235: t15.2023.10.08 val PER: 0.2368
2026-01-05 13:47:04,235: t15.2023.10.13 val PER: 0.2180
2026-01-05 13:47:04,236: t15.2023.10.15 val PER: 0.1681
2026-01-05 13:47:04,236: t15.2023.10.20 val PER: 0.1678
2026-01-05 13:47:04,236: t15.2023.10.22 val PER: 0.1158
2026-01-05 13:47:04,236: t15.2023.11.03 val PER: 0.1920
2026-01-05 13:47:04,236: t15.2023.11.04 val PER: 0.0410
2026-01-05 13:47:04,236: t15.2023.11.17 val PER: 0.0513
2026-01-05 13:47:04,236: t15.2023.11.19 val PER: 0.0419
2026-01-05 13:47:04,236: t15.2023.11.26 val PER: 0.1391
2026-01-05 13:47:04,236: t15.2023.12.03 val PER: 0.1124
2026-01-05 13:47:04,236: t15.2023.12.08 val PER: 0.1099
2026-01-05 13:47:04,236: t15.2023.12.10 val PER: 0.1012
2026-01-05 13:47:04,237: t15.2023.12.17 val PER: 0.1455
2026-01-05 13:47:04,237: t15.2023.12.29 val PER: 0.1393
2026-01-05 13:47:04,237: t15.2024.02.25 val PER: 0.1334
2026-01-05 13:47:04,237: t15.2024.03.08 val PER: 0.2248
2026-01-05 13:47:04,237: t15.2024.03.15 val PER: 0.2176
2026-01-05 13:47:04,237: t15.2024.03.17 val PER: 0.1527
2026-01-05 13:47:04,237: t15.2024.05.10 val PER: 0.1813
2026-01-05 13:47:04,237: t15.2024.06.14 val PER: 0.1688
2026-01-05 13:47:04,237: t15.2024.07.19 val PER: 0.2465
2026-01-05 13:47:04,237: t15.2024.07.21 val PER: 0.1014
2026-01-05 13:47:04,237: t15.2024.07.28 val PER: 0.1412
2026-01-05 13:47:04,237: t15.2025.01.10 val PER: 0.3058
2026-01-05 13:47:04,237: t15.2025.01.12 val PER: 0.1617
2026-01-05 13:47:04,237: t15.2025.03.14 val PER: 0.3550
2026-01-05 13:47:04,237: t15.2025.03.16 val PER: 0.2016
2026-01-05 13:47:04,238: t15.2025.03.30 val PER: 0.3080
2026-01-05 13:47:04,238: t15.2025.04.13 val PER: 0.2311
2026-01-05 13:47:04,577: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_11000
2026-01-05 13:47:23,640: Train batch 11200: loss: 10.53 grad norm: 57.63 time: 0.071
2026-01-05 13:47:42,635: Train batch 11400: loss: 9.11 grad norm: 52.07 time: 0.059
2026-01-05 13:47:52,363: Running test after training batch: 11500
2026-01-05 13:47:52,545: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:47:57,656: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:47:57,689: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost nett
2026-01-05 13:47:59,520: Val batch 11500: PER (avg): 0.1614 CTC Loss (avg): 16.2643 WER(1gram): 48.73% (n=64) time: 7.156
2026-01-05 13:47:59,521: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 13:47:59,521: t15.2023.08.13 val PER: 0.1227
2026-01-05 13:47:59,521: t15.2023.08.18 val PER: 0.1174
2026-01-05 13:47:59,521: t15.2023.08.20 val PER: 0.1223
2026-01-05 13:47:59,521: t15.2023.08.25 val PER: 0.1069
2026-01-05 13:47:59,521: t15.2023.08.27 val PER: 0.2042
2026-01-05 13:47:59,521: t15.2023.09.01 val PER: 0.0893
2026-01-05 13:47:59,521: t15.2023.09.03 val PER: 0.1698
2026-01-05 13:47:59,521: t15.2023.09.24 val PER: 0.1311
2026-01-05 13:47:59,522: t15.2023.09.29 val PER: 0.1385
2026-01-05 13:47:59,522: t15.2023.10.01 val PER: 0.1843
2026-01-05 13:47:59,522: t15.2023.10.06 val PER: 0.0904
2026-01-05 13:47:59,522: t15.2023.10.08 val PER: 0.2355
2026-01-05 13:47:59,522: t15.2023.10.13 val PER: 0.2188
2026-01-05 13:47:59,522: t15.2023.10.15 val PER: 0.1549
2026-01-05 13:47:59,522: t15.2023.10.20 val PER: 0.1913
2026-01-05 13:47:59,522: t15.2023.10.22 val PER: 0.1236
2026-01-05 13:47:59,522: t15.2023.11.03 val PER: 0.1866
2026-01-05 13:47:59,523: t15.2023.11.04 val PER: 0.0375
2026-01-05 13:47:59,523: t15.2023.11.17 val PER: 0.0482
2026-01-05 13:47:59,523: t15.2023.11.19 val PER: 0.0579
2026-01-05 13:47:59,523: t15.2023.11.26 val PER: 0.1348
2026-01-05 13:47:59,523: t15.2023.12.03 val PER: 0.1208
2026-01-05 13:47:59,523: t15.2023.12.08 val PER: 0.1119
2026-01-05 13:47:59,523: t15.2023.12.10 val PER: 0.1038
2026-01-05 13:47:59,523: t15.2023.12.17 val PER: 0.1611
2026-01-05 13:47:59,523: t15.2023.12.29 val PER: 0.1462
2026-01-05 13:47:59,523: t15.2024.02.25 val PER: 0.1264
2026-01-05 13:47:59,523: t15.2024.03.08 val PER: 0.2290
2026-01-05 13:47:59,524: t15.2024.03.15 val PER: 0.2095
2026-01-05 13:47:59,524: t15.2024.03.17 val PER: 0.1478
2026-01-05 13:47:59,524: t15.2024.05.10 val PER: 0.1738
2026-01-05 13:47:59,524: t15.2024.06.14 val PER: 0.1751
2026-01-05 13:47:59,524: t15.2024.07.19 val PER: 0.2498
2026-01-05 13:47:59,524: t15.2024.07.21 val PER: 0.1083
2026-01-05 13:47:59,524: t15.2024.07.28 val PER: 0.1397
2026-01-05 13:47:59,524: t15.2025.01.10 val PER: 0.3182
2026-01-05 13:47:59,524: t15.2025.01.12 val PER: 0.1663
2026-01-05 13:47:59,524: t15.2025.03.14 val PER: 0.3447
2026-01-05 13:47:59,524: t15.2025.03.16 val PER: 0.2042
2026-01-05 13:47:59,525: t15.2025.03.30 val PER: 0.3034
2026-01-05 13:47:59,525: t15.2025.04.13 val PER: 0.2254
2026-01-05 13:47:59,859: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_11500
2026-01-05 13:48:09,122: Train batch 11600: loss: 10.61 grad norm: 50.15 time: 0.061
2026-01-05 13:48:28,149: Train batch 11800: loss: 6.59 grad norm: 42.74 time: 0.045
2026-01-05 13:48:47,310: Train batch 12000: loss: 13.71 grad norm: 54.89 time: 0.074
2026-01-05 13:48:47,310: Running test after training batch: 12000
2026-01-05 13:48:47,417: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:48:52,634: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:48:52,669: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost et
2026-01-05 13:48:54,538: Val batch 12000: PER (avg): 0.1606 CTC Loss (avg): 16.1856 WER(1gram): 51.27% (n=64) time: 7.228
2026-01-05 13:48:54,538: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 13:48:54,539: t15.2023.08.13 val PER: 0.1206
2026-01-05 13:48:54,543: t15.2023.08.18 val PER: 0.1140
2026-01-05 13:48:54,543: t15.2023.08.20 val PER: 0.1112
2026-01-05 13:48:54,543: t15.2023.08.25 val PER: 0.1039
2026-01-05 13:48:54,543: t15.2023.08.27 val PER: 0.1833
2026-01-05 13:48:54,544: t15.2023.09.01 val PER: 0.0860
2026-01-05 13:48:54,544: t15.2023.09.03 val PER: 0.1639
2026-01-05 13:48:54,544: t15.2023.09.24 val PER: 0.1323
2026-01-05 13:48:54,544: t15.2023.09.29 val PER: 0.1334
2026-01-05 13:48:54,544: t15.2023.10.01 val PER: 0.1836
2026-01-05 13:48:54,544: t15.2023.10.06 val PER: 0.0947
2026-01-05 13:48:54,544: t15.2023.10.08 val PER: 0.2558
2026-01-05 13:48:54,544: t15.2023.10.13 val PER: 0.2118
2026-01-05 13:48:54,544: t15.2023.10.15 val PER: 0.1701
2026-01-05 13:48:54,545: t15.2023.10.20 val PER: 0.1913
2026-01-05 13:48:54,545: t15.2023.10.22 val PER: 0.1281
2026-01-05 13:48:54,545: t15.2023.11.03 val PER: 0.1852
2026-01-05 13:48:54,545: t15.2023.11.04 val PER: 0.0410
2026-01-05 13:48:54,545: t15.2023.11.17 val PER: 0.0420
2026-01-05 13:48:54,545: t15.2023.11.19 val PER: 0.0459
2026-01-05 13:48:54,545: t15.2023.11.26 val PER: 0.1355
2026-01-05 13:48:54,545: t15.2023.12.03 val PER: 0.1313
2026-01-05 13:48:54,545: t15.2023.12.08 val PER: 0.1032
2026-01-05 13:48:54,545: t15.2023.12.10 val PER: 0.0999
2026-01-05 13:48:54,545: t15.2023.12.17 val PER: 0.1466
2026-01-05 13:48:54,545: t15.2023.12.29 val PER: 0.1421
2026-01-05 13:48:54,546: t15.2024.02.25 val PER: 0.1306
2026-01-05 13:48:54,546: t15.2024.03.08 val PER: 0.2418
2026-01-05 13:48:54,546: t15.2024.03.15 val PER: 0.2170
2026-01-05 13:48:54,546: t15.2024.03.17 val PER: 0.1492
2026-01-05 13:48:54,546: t15.2024.05.10 val PER: 0.1694
2026-01-05 13:48:54,546: t15.2024.06.14 val PER: 0.1940
2026-01-05 13:48:54,546: t15.2024.07.19 val PER: 0.2518
2026-01-05 13:48:54,546: t15.2024.07.21 val PER: 0.1097
2026-01-05 13:48:54,546: t15.2024.07.28 val PER: 0.1463
2026-01-05 13:48:54,546: t15.2025.01.10 val PER: 0.3058
2026-01-05 13:48:54,547: t15.2025.01.12 val PER: 0.1601
2026-01-05 13:48:54,547: t15.2025.03.14 val PER: 0.3550
2026-01-05 13:48:54,547: t15.2025.03.16 val PER: 0.1990
2026-01-05 13:48:54,547: t15.2025.03.30 val PER: 0.2943
2026-01-05 13:48:54,547: t15.2025.04.13 val PER: 0.2126
2026-01-05 13:48:54,865: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_12000
2026-01-05 13:49:13,886: Train batch 12200: loss: 5.68 grad norm: 42.29 time: 0.065
2026-01-05 13:49:32,734: Train batch 12400: loss: 4.53 grad norm: 35.30 time: 0.041
2026-01-05 13:49:42,446: Running test after training batch: 12500
2026-01-05 13:49:42,613: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:49:47,769: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 13:49:47,804: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-05 13:49:49,675: Val batch 12500: PER (avg): 0.1548 CTC Loss (avg): 15.9403 WER(1gram): 48.48% (n=64) time: 7.228
2026-01-05 13:49:49,676: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-05 13:49:49,676: t15.2023.08.13 val PER: 0.1164
2026-01-05 13:49:49,676: t15.2023.08.18 val PER: 0.1081
2026-01-05 13:49:49,676: t15.2023.08.20 val PER: 0.1104
2026-01-05 13:49:49,676: t15.2023.08.25 val PER: 0.0813
2026-01-05 13:49:49,676: t15.2023.08.27 val PER: 0.1977
2026-01-05 13:49:49,676: t15.2023.09.01 val PER: 0.0820
2026-01-05 13:49:49,676: t15.2023.09.03 val PER: 0.1556
2026-01-05 13:49:49,676: t15.2023.09.24 val PER: 0.1286
2026-01-05 13:49:49,676: t15.2023.09.29 val PER: 0.1334
2026-01-05 13:49:49,676: t15.2023.10.01 val PER: 0.1717
2026-01-05 13:49:49,676: t15.2023.10.06 val PER: 0.0829
2026-01-05 13:49:49,677: t15.2023.10.08 val PER: 0.2571
2026-01-05 13:49:49,677: t15.2023.10.13 val PER: 0.2095
2026-01-05 13:49:49,677: t15.2023.10.15 val PER: 0.1556
2026-01-05 13:49:49,677: t15.2023.10.20 val PER: 0.1846
2026-01-05 13:49:49,677: t15.2023.10.22 val PER: 0.1125
2026-01-05 13:49:49,677: t15.2023.11.03 val PER: 0.1920
2026-01-05 13:49:49,677: t15.2023.11.04 val PER: 0.0307
2026-01-05 13:49:49,677: t15.2023.11.17 val PER: 0.0482
2026-01-05 13:49:49,677: t15.2023.11.19 val PER: 0.0339
2026-01-05 13:49:49,678: t15.2023.11.26 val PER: 0.1283
2026-01-05 13:49:49,678: t15.2023.12.03 val PER: 0.1176
2026-01-05 13:49:49,678: t15.2023.12.08 val PER: 0.1025
2026-01-05 13:49:49,678: t15.2023.12.10 val PER: 0.0972
2026-01-05 13:49:49,678: t15.2023.12.17 val PER: 0.1486
2026-01-05 13:49:49,678: t15.2023.12.29 val PER: 0.1407
2026-01-05 13:49:49,678: t15.2024.02.25 val PER: 0.1138
2026-01-05 13:49:49,678: t15.2024.03.08 val PER: 0.2304
2026-01-05 13:49:49,678: t15.2024.03.15 val PER: 0.2108
2026-01-05 13:49:49,678: t15.2024.03.17 val PER: 0.1367
2026-01-05 13:49:49,678: t15.2024.05.10 val PER: 0.1768
2026-01-05 13:49:49,678: t15.2024.06.14 val PER: 0.1814
2026-01-05 13:49:49,678: t15.2024.07.19 val PER: 0.2373
2026-01-05 13:49:49,678: t15.2024.07.21 val PER: 0.1000
2026-01-05 13:49:49,679: t15.2024.07.28 val PER: 0.1397
2026-01-05 13:49:49,679: t15.2025.01.10 val PER: 0.2920
2026-01-05 13:49:49,679: t15.2025.01.12 val PER: 0.1463
2026-01-05 13:49:49,679: t15.2025.03.14 val PER: 0.3580
2026-01-05 13:49:49,679: t15.2025.03.16 val PER: 0.1885
2026-01-05 13:49:49,679: t15.2025.03.30 val PER: 0.3057
2026-01-05 13:49:49,679: t15.2025.04.13 val PER: 0.2154
2026-01-05 13:49:50,019: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_12500
2026-01-05 13:50:07,963: Train batch 12600: loss: 7.58 grad norm: 45.94 time: 0.058
2026-01-05 13:50:28,155: Train batch 12800: loss: 5.44 grad norm: 36.66 time: 0.052
2026-01-05 13:50:47,533: Train batch 13000: loss: 5.99 grad norm: 38.64 time: 0.066
2026-01-05 13:50:47,534: Running test after training batch: 13000
2026-01-05 13:50:47,671: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:50:53,820: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 13:50:53,854: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-05 13:50:55,697: Val batch 13000: PER (avg): 0.1540 CTC Loss (avg): 15.7524 WER(1gram): 47.97% (n=64) time: 8.163
2026-01-05 13:50:55,698: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=11
2026-01-05 13:50:55,698: t15.2023.08.13 val PER: 0.1195
2026-01-05 13:50:55,698: t15.2023.08.18 val PER: 0.1123
2026-01-05 13:50:55,698: t15.2023.08.20 val PER: 0.0969
2026-01-05 13:50:55,698: t15.2023.08.25 val PER: 0.0858
2026-01-05 13:50:55,698: t15.2023.08.27 val PER: 0.1752
2026-01-05 13:50:55,698: t15.2023.09.01 val PER: 0.0812
2026-01-05 13:50:55,699: t15.2023.09.03 val PER: 0.1686
2026-01-05 13:50:55,699: t15.2023.09.24 val PER: 0.1274
2026-01-05 13:50:55,699: t15.2023.09.29 val PER: 0.1289
2026-01-05 13:50:55,699: t15.2023.10.01 val PER: 0.1783
2026-01-05 13:50:55,699: t15.2023.10.06 val PER: 0.0786
2026-01-05 13:50:55,699: t15.2023.10.08 val PER: 0.2476
2026-01-05 13:50:55,699: t15.2023.10.13 val PER: 0.2079
2026-01-05 13:50:55,699: t15.2023.10.15 val PER: 0.1575
2026-01-05 13:50:55,699: t15.2023.10.20 val PER: 0.1879
2026-01-05 13:50:55,699: t15.2023.10.22 val PER: 0.1169
2026-01-05 13:50:55,700: t15.2023.11.03 val PER: 0.1845
2026-01-05 13:50:55,700: t15.2023.11.04 val PER: 0.0410
2026-01-05 13:50:55,700: t15.2023.11.17 val PER: 0.0389
2026-01-05 13:50:55,700: t15.2023.11.19 val PER: 0.0439
2026-01-05 13:50:55,700: t15.2023.11.26 val PER: 0.1304
2026-01-05 13:50:55,700: t15.2023.12.03 val PER: 0.1229
2026-01-05 13:50:55,700: t15.2023.12.08 val PER: 0.1045
2026-01-05 13:50:55,700: t15.2023.12.10 val PER: 0.0959
2026-01-05 13:50:55,700: t15.2023.12.17 val PER: 0.1414
2026-01-05 13:50:55,700: t15.2023.12.29 val PER: 0.1366
2026-01-05 13:50:55,700: t15.2024.02.25 val PER: 0.1236
2026-01-05 13:50:55,701: t15.2024.03.08 val PER: 0.2290
2026-01-05 13:50:55,701: t15.2024.03.15 val PER: 0.2070
2026-01-05 13:50:55,701: t15.2024.03.17 val PER: 0.1381
2026-01-05 13:50:55,701: t15.2024.05.10 val PER: 0.1649
2026-01-05 13:50:55,701: t15.2024.06.14 val PER: 0.1735
2026-01-05 13:50:55,701: t15.2024.07.19 val PER: 0.2446
2026-01-05 13:50:55,701: t15.2024.07.21 val PER: 0.0993
2026-01-05 13:50:55,701: t15.2024.07.28 val PER: 0.1449
2026-01-05 13:50:55,701: t15.2025.01.10 val PER: 0.2906
2026-01-05 13:50:55,701: t15.2025.01.12 val PER: 0.1463
2026-01-05 13:50:55,702: t15.2025.03.14 val PER: 0.3432
2026-01-05 13:50:55,702: t15.2025.03.16 val PER: 0.1819
2026-01-05 13:50:55,702: t15.2025.03.30 val PER: 0.3011
2026-01-05 13:50:55,702: t15.2025.04.13 val PER: 0.2254
2026-01-05 13:50:55,702: New best val WER(1gram) 48.48% --> 47.97%
2026-01-05 13:50:55,702: Checkpointing model
2026-01-05 13:50:56,427: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:50:56,765: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_13000
2026-01-05 13:51:15,609: Train batch 13200: loss: 12.01 grad norm: 59.10 time: 0.054
2026-01-05 13:51:34,355: Train batch 13400: loss: 8.14 grad norm: 51.19 time: 0.062
2026-01-05 13:51:43,819: Running test after training batch: 13500
2026-01-05 13:51:43,956: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:51:49,041: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 13:51:49,076: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-05 13:51:50,952: Val batch 13500: PER (avg): 0.1506 CTC Loss (avg): 15.4844 WER(1gram): 48.73% (n=64) time: 7.133
2026-01-05 13:51:50,952: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=11
2026-01-05 13:51:50,952: t15.2023.08.13 val PER: 0.1175
2026-01-05 13:51:50,952: t15.2023.08.18 val PER: 0.1065
2026-01-05 13:51:50,953: t15.2023.08.20 val PER: 0.1025
2026-01-05 13:51:50,953: t15.2023.08.25 val PER: 0.0919
2026-01-05 13:51:50,953: t15.2023.08.27 val PER: 0.1865
2026-01-05 13:51:50,953: t15.2023.09.01 val PER: 0.0860
2026-01-05 13:51:50,953: t15.2023.09.03 val PER: 0.1603
2026-01-05 13:51:50,953: t15.2023.09.24 val PER: 0.1335
2026-01-05 13:51:50,953: t15.2023.09.29 val PER: 0.1264
2026-01-05 13:51:50,953: t15.2023.10.01 val PER: 0.1724
2026-01-05 13:51:50,953: t15.2023.10.06 val PER: 0.0861
2026-01-05 13:51:50,953: t15.2023.10.08 val PER: 0.2422
2026-01-05 13:51:50,954: t15.2023.10.13 val PER: 0.2095
2026-01-05 13:51:50,954: t15.2023.10.15 val PER: 0.1523
2026-01-05 13:51:50,954: t15.2023.10.20 val PER: 0.1913
2026-01-05 13:51:50,954: t15.2023.10.22 val PER: 0.1080
2026-01-05 13:51:50,954: t15.2023.11.03 val PER: 0.1825
2026-01-05 13:51:50,954: t15.2023.11.04 val PER: 0.0375
2026-01-05 13:51:50,954: t15.2023.11.17 val PER: 0.0435
2026-01-05 13:51:50,954: t15.2023.11.19 val PER: 0.0339
2026-01-05 13:51:50,954: t15.2023.11.26 val PER: 0.1196
2026-01-05 13:51:50,954: t15.2023.12.03 val PER: 0.1103
2026-01-05 13:51:50,954: t15.2023.12.08 val PER: 0.1025
2026-01-05 13:51:50,954: t15.2023.12.10 val PER: 0.0933
2026-01-05 13:51:50,954: t15.2023.12.17 val PER: 0.1362
2026-01-05 13:51:50,955: t15.2023.12.29 val PER: 0.1215
2026-01-05 13:51:50,955: t15.2024.02.25 val PER: 0.1194
2026-01-05 13:51:50,955: t15.2024.03.08 val PER: 0.2248
2026-01-05 13:51:50,955: t15.2024.03.15 val PER: 0.2020
2026-01-05 13:51:50,955: t15.2024.03.17 val PER: 0.1374
2026-01-05 13:51:50,955: t15.2024.05.10 val PER: 0.1664
2026-01-05 13:51:50,955: t15.2024.06.14 val PER: 0.1562
2026-01-05 13:51:50,955: t15.2024.07.19 val PER: 0.2314
2026-01-05 13:51:50,955: t15.2024.07.21 val PER: 0.0924
2026-01-05 13:51:50,955: t15.2024.07.28 val PER: 0.1346
2026-01-05 13:51:50,955: t15.2025.01.10 val PER: 0.3044
2026-01-05 13:51:50,956: t15.2025.01.12 val PER: 0.1440
2026-01-05 13:51:50,956: t15.2025.03.14 val PER: 0.3402
2026-01-05 13:51:50,956: t15.2025.03.16 val PER: 0.1872
2026-01-05 13:51:50,956: t15.2025.03.30 val PER: 0.3023
2026-01-05 13:51:50,956: t15.2025.04.13 val PER: 0.2111
2026-01-05 13:51:51,309: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_13500
2026-01-05 13:52:00,857: Train batch 13600: loss: 12.18 grad norm: 65.50 time: 0.062
2026-01-05 13:52:19,968: Train batch 13800: loss: 8.68 grad norm: 57.03 time: 0.056
2026-01-05 13:52:39,036: Train batch 14000: loss: 11.35 grad norm: 61.51 time: 0.051
2026-01-05 13:52:39,037: Running test after training batch: 14000
2026-01-05 13:52:39,156: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:52:44,338: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:52:44,373: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-05 13:52:46,277: Val batch 14000: PER (avg): 0.1501 CTC Loss (avg): 15.4578 WER(1gram): 47.46% (n=64) time: 7.240
2026-01-05 13:52:46,277: WER lens: avg_true_words=6.16 avg_pred_words=6.25 max_pred_words=11
2026-01-05 13:52:46,277: t15.2023.08.13 val PER: 0.1175
2026-01-05 13:52:46,278: t15.2023.08.18 val PER: 0.0956
2026-01-05 13:52:46,278: t15.2023.08.20 val PER: 0.1056
2026-01-05 13:52:46,278: t15.2023.08.25 val PER: 0.0889
2026-01-05 13:52:46,278: t15.2023.08.27 val PER: 0.1849
2026-01-05 13:52:46,278: t15.2023.09.01 val PER: 0.0804
2026-01-05 13:52:46,278: t15.2023.09.03 val PER: 0.1639
2026-01-05 13:52:46,278: t15.2023.09.24 val PER: 0.1335
2026-01-05 13:52:46,278: t15.2023.09.29 val PER: 0.1251
2026-01-05 13:52:46,278: t15.2023.10.01 val PER: 0.1810
2026-01-05 13:52:46,279: t15.2023.10.06 val PER: 0.0840
2026-01-05 13:52:46,279: t15.2023.10.08 val PER: 0.2422
2026-01-05 13:52:46,279: t15.2023.10.13 val PER: 0.2071
2026-01-05 13:52:46,279: t15.2023.10.15 val PER: 0.1595
2026-01-05 13:52:46,279: t15.2023.10.20 val PER: 0.1879
2026-01-05 13:52:46,279: t15.2023.10.22 val PER: 0.1169
2026-01-05 13:52:46,279: t15.2023.11.03 val PER: 0.1777
2026-01-05 13:52:46,279: t15.2023.11.04 val PER: 0.0307
2026-01-05 13:52:46,279: t15.2023.11.17 val PER: 0.0420
2026-01-05 13:52:46,279: t15.2023.11.19 val PER: 0.0319
2026-01-05 13:52:46,279: t15.2023.11.26 val PER: 0.1225
2026-01-05 13:52:46,279: t15.2023.12.03 val PER: 0.1124
2026-01-05 13:52:46,279: t15.2023.12.08 val PER: 0.1012
2026-01-05 13:52:46,279: t15.2023.12.10 val PER: 0.1038
2026-01-05 13:52:46,280: t15.2023.12.17 val PER: 0.1320
2026-01-05 13:52:46,280: t15.2023.12.29 val PER: 0.1249
2026-01-05 13:52:46,284: t15.2024.02.25 val PER: 0.1180
2026-01-05 13:52:46,284: t15.2024.03.08 val PER: 0.2233
2026-01-05 13:52:46,284: t15.2024.03.15 val PER: 0.1989
2026-01-05 13:52:46,284: t15.2024.03.17 val PER: 0.1409
2026-01-05 13:52:46,284: t15.2024.05.10 val PER: 0.1605
2026-01-05 13:52:46,284: t15.2024.06.14 val PER: 0.1577
2026-01-05 13:52:46,284: t15.2024.07.19 val PER: 0.2228
2026-01-05 13:52:46,284: t15.2024.07.21 val PER: 0.0931
2026-01-05 13:52:46,284: t15.2024.07.28 val PER: 0.1338
2026-01-05 13:52:46,284: t15.2025.01.10 val PER: 0.2934
2026-01-05 13:52:46,284: t15.2025.01.12 val PER: 0.1432
2026-01-05 13:52:46,285: t15.2025.03.14 val PER: 0.3373
2026-01-05 13:52:46,285: t15.2025.03.16 val PER: 0.1924
2026-01-05 13:52:46,285: t15.2025.03.30 val PER: 0.2966
2026-01-05 13:52:46,285: t15.2025.04.13 val PER: 0.2140
2026-01-05 13:52:46,286: New best val WER(1gram) 47.97% --> 47.46%
2026-01-05 13:52:46,286: Checkpointing model
2026-01-05 13:52:46,946: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:52:47,268: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_14000
2026-01-05 13:53:06,439: Train batch 14200: loss: 7.39 grad norm: 48.29 time: 0.056
2026-01-05 13:53:25,622: Train batch 14400: loss: 5.48 grad norm: 39.15 time: 0.064
2026-01-05 13:53:35,177: Running test after training batch: 14500
2026-01-05 13:53:35,296: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:53:40,481: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 13:53:40,516: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-05 13:53:42,419: Val batch 14500: PER (avg): 0.1510 CTC Loss (avg): 15.5088 WER(1gram): 45.94% (n=64) time: 7.241
2026-01-05 13:53:42,420: WER lens: avg_true_words=6.16 avg_pred_words=6.27 max_pred_words=11
2026-01-05 13:53:42,420: t15.2023.08.13 val PER: 0.1133
2026-01-05 13:53:42,420: t15.2023.08.18 val PER: 0.1056
2026-01-05 13:53:42,420: t15.2023.08.20 val PER: 0.1088
2026-01-05 13:53:42,420: t15.2023.08.25 val PER: 0.0828
2026-01-05 13:53:42,420: t15.2023.08.27 val PER: 0.1785
2026-01-05 13:53:42,420: t15.2023.09.01 val PER: 0.0804
2026-01-05 13:53:42,420: t15.2023.09.03 val PER: 0.1568
2026-01-05 13:53:42,421: t15.2023.09.24 val PER: 0.1299
2026-01-05 13:53:42,421: t15.2023.09.29 val PER: 0.1276
2026-01-05 13:53:42,421: t15.2023.10.01 val PER: 0.1803
2026-01-05 13:53:42,421: t15.2023.10.06 val PER: 0.0850
2026-01-05 13:53:42,421: t15.2023.10.08 val PER: 0.2422
2026-01-05 13:53:42,421: t15.2023.10.13 val PER: 0.2157
2026-01-05 13:53:42,421: t15.2023.10.15 val PER: 0.1589
2026-01-05 13:53:42,421: t15.2023.10.20 val PER: 0.1779
2026-01-05 13:53:42,421: t15.2023.10.22 val PER: 0.1102
2026-01-05 13:53:42,421: t15.2023.11.03 val PER: 0.1818
2026-01-05 13:53:42,422: t15.2023.11.04 val PER: 0.0410
2026-01-05 13:53:42,422: t15.2023.11.17 val PER: 0.0435
2026-01-05 13:53:42,422: t15.2023.11.19 val PER: 0.0339
2026-01-05 13:53:42,422: t15.2023.11.26 val PER: 0.1275
2026-01-05 13:53:42,422: t15.2023.12.03 val PER: 0.1113
2026-01-05 13:53:42,422: t15.2023.12.08 val PER: 0.0972
2026-01-05 13:53:42,422: t15.2023.12.10 val PER: 0.0933
2026-01-05 13:53:42,422: t15.2023.12.17 val PER: 0.1435
2026-01-05 13:53:42,422: t15.2023.12.29 val PER: 0.1277
2026-01-05 13:53:42,422: t15.2024.02.25 val PER: 0.1180
2026-01-05 13:53:42,422: t15.2024.03.08 val PER: 0.2290
2026-01-05 13:53:42,423: t15.2024.03.15 val PER: 0.2076
2026-01-05 13:53:42,423: t15.2024.03.17 val PER: 0.1339
2026-01-05 13:53:42,423: t15.2024.05.10 val PER: 0.1605
2026-01-05 13:53:42,423: t15.2024.06.14 val PER: 0.1688
2026-01-05 13:53:42,423: t15.2024.07.19 val PER: 0.2320
2026-01-05 13:53:42,423: t15.2024.07.21 val PER: 0.0897
2026-01-05 13:53:42,423: t15.2024.07.28 val PER: 0.1309
2026-01-05 13:53:42,423: t15.2025.01.10 val PER: 0.2837
2026-01-05 13:53:42,423: t15.2025.01.12 val PER: 0.1416
2026-01-05 13:53:42,424: t15.2025.03.14 val PER: 0.3536
2026-01-05 13:53:42,424: t15.2025.03.16 val PER: 0.1832
2026-01-05 13:53:42,424: t15.2025.03.30 val PER: 0.2977
2026-01-05 13:53:42,424: t15.2025.04.13 val PER: 0.2183
2026-01-05 13:53:42,424: New best val WER(1gram) 47.46% --> 45.94%
2026-01-05 13:53:42,424: Checkpointing model
2026-01-05 13:53:43,101: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:53:43,428: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_14500
2026-01-05 13:53:52,877: Train batch 14600: loss: 12.09 grad norm: 59.60 time: 0.058
2026-01-05 13:54:11,961: Train batch 14800: loss: 5.35 grad norm: 44.20 time: 0.051
2026-01-05 13:54:30,966: Train batch 15000: loss: 8.50 grad norm: 42.77 time: 0.052
2026-01-05 13:54:30,968: Running test after training batch: 15000
2026-01-05 13:54:31,119: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:54:36,290: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 13:54:36,325: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-05 13:54:38,224: Val batch 15000: PER (avg): 0.1469 CTC Loss (avg): 15.2923 WER(1gram): 46.45% (n=64) time: 7.256
2026-01-05 13:54:38,224: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-05 13:54:38,224: t15.2023.08.13 val PER: 0.1112
2026-01-05 13:54:38,225: t15.2023.08.18 val PER: 0.0997
2026-01-05 13:54:38,225: t15.2023.08.20 val PER: 0.1009
2026-01-05 13:54:38,225: t15.2023.08.25 val PER: 0.0843
2026-01-05 13:54:38,225: t15.2023.08.27 val PER: 0.1768
2026-01-05 13:54:38,225: t15.2023.09.01 val PER: 0.0755
2026-01-05 13:54:38,225: t15.2023.09.03 val PER: 0.1496
2026-01-05 13:54:38,225: t15.2023.09.24 val PER: 0.1323
2026-01-05 13:54:38,225: t15.2023.09.29 val PER: 0.1276
2026-01-05 13:54:38,226: t15.2023.10.01 val PER: 0.1744
2026-01-05 13:54:38,226: t15.2023.10.06 val PER: 0.0775
2026-01-05 13:54:38,226: t15.2023.10.08 val PER: 0.2449
2026-01-05 13:54:38,226: t15.2023.10.13 val PER: 0.2017
2026-01-05 13:54:38,226: t15.2023.10.15 val PER: 0.1529
2026-01-05 13:54:38,226: t15.2023.10.20 val PER: 0.1812
2026-01-05 13:54:38,226: t15.2023.10.22 val PER: 0.1125
2026-01-05 13:54:38,226: t15.2023.11.03 val PER: 0.1764
2026-01-05 13:54:38,226: t15.2023.11.04 val PER: 0.0375
2026-01-05 13:54:38,226: t15.2023.11.17 val PER: 0.0482
2026-01-05 13:54:38,226: t15.2023.11.19 val PER: 0.0319
2026-01-05 13:54:38,226: t15.2023.11.26 val PER: 0.1130
2026-01-05 13:54:38,227: t15.2023.12.03 val PER: 0.1092
2026-01-05 13:54:38,227: t15.2023.12.08 val PER: 0.0939
2026-01-05 13:54:38,227: t15.2023.12.10 val PER: 0.0946
2026-01-05 13:54:38,227: t15.2023.12.17 val PER: 0.1414
2026-01-05 13:54:38,227: t15.2023.12.29 val PER: 0.1311
2026-01-05 13:54:38,227: t15.2024.02.25 val PER: 0.1124
2026-01-05 13:54:38,227: t15.2024.03.08 val PER: 0.2176
2026-01-05 13:54:38,227: t15.2024.03.15 val PER: 0.2064
2026-01-05 13:54:38,227: t15.2024.03.17 val PER: 0.1297
2026-01-05 13:54:38,228: t15.2024.05.10 val PER: 0.1545
2026-01-05 13:54:38,228: t15.2024.06.14 val PER: 0.1640
2026-01-05 13:54:38,228: t15.2024.07.19 val PER: 0.2208
2026-01-05 13:54:38,228: t15.2024.07.21 val PER: 0.0848
2026-01-05 13:54:38,228: t15.2024.07.28 val PER: 0.1250
2026-01-05 13:54:38,228: t15.2025.01.10 val PER: 0.2948
2026-01-05 13:54:38,228: t15.2025.01.12 val PER: 0.1455
2026-01-05 13:54:38,228: t15.2025.03.14 val PER: 0.3491
2026-01-05 13:54:38,228: t15.2025.03.16 val PER: 0.1702
2026-01-05 13:54:38,228: t15.2025.03.30 val PER: 0.2897
2026-01-05 13:54:38,228: t15.2025.04.13 val PER: 0.2068
2026-01-05 13:54:38,548: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_15000
2026-01-05 13:54:57,581: Train batch 15200: loss: 4.37 grad norm: 37.13 time: 0.058
2026-01-05 13:55:16,479: Train batch 15400: loss: 11.09 grad norm: 61.07 time: 0.049
2026-01-05 13:55:26,027: Running test after training batch: 15500
2026-01-05 13:55:26,214: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:55:31,317: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:55:31,354: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-05 13:55:33,342: Val batch 15500: PER (avg): 0.1472 CTC Loss (avg): 15.1036 WER(1gram): 45.69% (n=64) time: 7.314
2026-01-05 13:55:33,342: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 13:55:33,343: t15.2023.08.13 val PER: 0.1143
2026-01-05 13:55:33,343: t15.2023.08.18 val PER: 0.1023
2026-01-05 13:55:33,343: t15.2023.08.20 val PER: 0.1017
2026-01-05 13:55:33,343: t15.2023.08.25 val PER: 0.0843
2026-01-05 13:55:33,343: t15.2023.08.27 val PER: 0.1704
2026-01-05 13:55:33,343: t15.2023.09.01 val PER: 0.0771
2026-01-05 13:55:33,343: t15.2023.09.03 val PER: 0.1603
2026-01-05 13:55:33,343: t15.2023.09.24 val PER: 0.1226
2026-01-05 13:55:33,343: t15.2023.09.29 val PER: 0.1264
2026-01-05 13:55:33,344: t15.2023.10.01 val PER: 0.1717
2026-01-05 13:55:33,344: t15.2023.10.06 val PER: 0.0893
2026-01-05 13:55:33,344: t15.2023.10.08 val PER: 0.2341
2026-01-05 13:55:33,344: t15.2023.10.13 val PER: 0.2025
2026-01-05 13:55:33,344: t15.2023.10.15 val PER: 0.1543
2026-01-05 13:55:33,344: t15.2023.10.20 val PER: 0.1745
2026-01-05 13:55:33,344: t15.2023.10.22 val PER: 0.1125
2026-01-05 13:55:33,344: t15.2023.11.03 val PER: 0.1777
2026-01-05 13:55:33,344: t15.2023.11.04 val PER: 0.0375
2026-01-05 13:55:33,344: t15.2023.11.17 val PER: 0.0404
2026-01-05 13:55:33,345: t15.2023.11.19 val PER: 0.0419
2026-01-05 13:55:33,345: t15.2023.11.26 val PER: 0.1101
2026-01-05 13:55:33,345: t15.2023.12.03 val PER: 0.1092
2026-01-05 13:55:33,345: t15.2023.12.08 val PER: 0.0972
2026-01-05 13:55:33,345: t15.2023.12.10 val PER: 0.0841
2026-01-05 13:55:33,345: t15.2023.12.17 val PER: 0.1341
2026-01-05 13:55:33,345: t15.2023.12.29 val PER: 0.1235
2026-01-05 13:55:33,345: t15.2024.02.25 val PER: 0.1152
2026-01-05 13:55:33,345: t15.2024.03.08 val PER: 0.2290
2026-01-05 13:55:33,345: t15.2024.03.15 val PER: 0.1989
2026-01-05 13:55:33,345: t15.2024.03.17 val PER: 0.1374
2026-01-05 13:55:33,345: t15.2024.05.10 val PER: 0.1634
2026-01-05 13:55:33,345: t15.2024.06.14 val PER: 0.1640
2026-01-05 13:55:33,345: t15.2024.07.19 val PER: 0.2254
2026-01-05 13:55:33,346: t15.2024.07.21 val PER: 0.0917
2026-01-05 13:55:33,346: t15.2024.07.28 val PER: 0.1287
2026-01-05 13:55:33,346: t15.2025.01.10 val PER: 0.2796
2026-01-05 13:55:33,346: t15.2025.01.12 val PER: 0.1470
2026-01-05 13:55:33,346: t15.2025.03.14 val PER: 0.3314
2026-01-05 13:55:33,346: t15.2025.03.16 val PER: 0.1754
2026-01-05 13:55:33,346: t15.2025.03.30 val PER: 0.2920
2026-01-05 13:55:33,346: t15.2025.04.13 val PER: 0.2183
2026-01-05 13:55:33,347: New best val WER(1gram) 45.94% --> 45.69%
2026-01-05 13:55:33,347: Checkpointing model
2026-01-05 13:55:34,069: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/best_checkpoint
2026-01-05 13:55:34,406: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_15500
2026-01-05 13:55:43,812: Train batch 15600: loss: 10.62 grad norm: 56.55 time: 0.063
2026-01-05 13:56:02,754: Train batch 15800: loss: 12.97 grad norm: 60.91 time: 0.067
2026-01-05 13:56:22,037: Train batch 16000: loss: 8.09 grad norm: 45.56 time: 0.056
2026-01-05 13:56:22,037: Running test after training batch: 16000
2026-01-05 13:56:22,206: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:56:27,299: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 13:56:27,334: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-05 13:56:29,268: Val batch 16000: PER (avg): 0.1465 CTC Loss (avg): 15.2838 WER(1gram): 46.95% (n=64) time: 7.231
2026-01-05 13:56:29,268: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-05 13:56:29,269: t15.2023.08.13 val PER: 0.1206
2026-01-05 13:56:29,269: t15.2023.08.18 val PER: 0.1039
2026-01-05 13:56:29,269: t15.2023.08.20 val PER: 0.0977
2026-01-05 13:56:29,269: t15.2023.08.25 val PER: 0.0873
2026-01-05 13:56:29,269: t15.2023.08.27 val PER: 0.1785
2026-01-05 13:56:29,269: t15.2023.09.01 val PER: 0.0820
2026-01-05 13:56:29,269: t15.2023.09.03 val PER: 0.1532
2026-01-05 13:56:29,270: t15.2023.09.24 val PER: 0.1250
2026-01-05 13:56:29,270: t15.2023.09.29 val PER: 0.1257
2026-01-05 13:56:29,270: t15.2023.10.01 val PER: 0.1711
2026-01-05 13:56:29,270: t15.2023.10.06 val PER: 0.0807
2026-01-05 13:56:29,270: t15.2023.10.08 val PER: 0.2327
2026-01-05 13:56:29,270: t15.2023.10.13 val PER: 0.1893
2026-01-05 13:56:29,270: t15.2023.10.15 val PER: 0.1523
2026-01-05 13:56:29,270: t15.2023.10.20 val PER: 0.1879
2026-01-05 13:56:29,270: t15.2023.10.22 val PER: 0.1058
2026-01-05 13:56:29,270: t15.2023.11.03 val PER: 0.1764
2026-01-05 13:56:29,270: t15.2023.11.04 val PER: 0.0341
2026-01-05 13:56:29,271: t15.2023.11.17 val PER: 0.0404
2026-01-05 13:56:29,271: t15.2023.11.19 val PER: 0.0419
2026-01-05 13:56:29,271: t15.2023.11.26 val PER: 0.1080
2026-01-05 13:56:29,271: t15.2023.12.03 val PER: 0.1071
2026-01-05 13:56:29,271: t15.2023.12.08 val PER: 0.0965
2026-01-05 13:56:29,271: t15.2023.12.10 val PER: 0.0867
2026-01-05 13:56:29,271: t15.2023.12.17 val PER: 0.1351
2026-01-05 13:56:29,275: t15.2023.12.29 val PER: 0.1201
2026-01-05 13:56:29,275: t15.2024.02.25 val PER: 0.0997
2026-01-05 13:56:29,275: t15.2024.03.08 val PER: 0.2248
2026-01-05 13:56:29,275: t15.2024.03.15 val PER: 0.2033
2026-01-05 13:56:29,276: t15.2024.03.17 val PER: 0.1332
2026-01-05 13:56:29,276: t15.2024.05.10 val PER: 0.1649
2026-01-05 13:56:29,276: t15.2024.06.14 val PER: 0.1656
2026-01-05 13:56:29,276: t15.2024.07.19 val PER: 0.2294
2026-01-05 13:56:29,276: t15.2024.07.21 val PER: 0.0883
2026-01-05 13:56:29,276: t15.2024.07.28 val PER: 0.1301
2026-01-05 13:56:29,276: t15.2025.01.10 val PER: 0.2948
2026-01-05 13:56:29,276: t15.2025.01.12 val PER: 0.1416
2026-01-05 13:56:29,276: t15.2025.03.14 val PER: 0.3343
2026-01-05 13:56:29,276: t15.2025.03.16 val PER: 0.1767
2026-01-05 13:56:29,276: t15.2025.03.30 val PER: 0.2977
2026-01-05 13:56:29,276: t15.2025.04.13 val PER: 0.2183
2026-01-05 13:56:29,586: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_16000
2026-01-05 13:56:48,675: Train batch 16200: loss: 6.25 grad norm: 42.76 time: 0.056
2026-01-05 13:57:08,907: Train batch 16400: loss: 10.20 grad norm: 60.14 time: 0.058
2026-01-05 13:57:18,609: Running test after training batch: 16500
2026-01-05 13:57:18,761: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:57:24,020: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:57:24,055: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-05 13:57:25,988: Val batch 16500: PER (avg): 0.1460 CTC Loss (avg): 15.1342 WER(1gram): 46.19% (n=64) time: 7.379
2026-01-05 13:57:25,989: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-05 13:57:25,989: t15.2023.08.13 val PER: 0.1123
2026-01-05 13:57:25,989: t15.2023.08.18 val PER: 0.0981
2026-01-05 13:57:25,989: t15.2023.08.20 val PER: 0.1001
2026-01-05 13:57:25,989: t15.2023.08.25 val PER: 0.0843
2026-01-05 13:57:25,989: t15.2023.08.27 val PER: 0.1736
2026-01-05 13:57:25,989: t15.2023.09.01 val PER: 0.0771
2026-01-05 13:57:25,989: t15.2023.09.03 val PER: 0.1603
2026-01-05 13:57:25,989: t15.2023.09.24 val PER: 0.1274
2026-01-05 13:57:25,989: t15.2023.09.29 val PER: 0.1257
2026-01-05 13:57:25,989: t15.2023.10.01 val PER: 0.1704
2026-01-05 13:57:25,989: t15.2023.10.06 val PER: 0.0797
2026-01-05 13:57:25,990: t15.2023.10.08 val PER: 0.2449
2026-01-05 13:57:25,990: t15.2023.10.13 val PER: 0.1978
2026-01-05 13:57:25,990: t15.2023.10.15 val PER: 0.1477
2026-01-05 13:57:25,990: t15.2023.10.20 val PER: 0.1812
2026-01-05 13:57:25,990: t15.2023.10.22 val PER: 0.1091
2026-01-05 13:57:25,990: t15.2023.11.03 val PER: 0.1777
2026-01-05 13:57:25,990: t15.2023.11.04 val PER: 0.0410
2026-01-05 13:57:25,990: t15.2023.11.17 val PER: 0.0342
2026-01-05 13:57:25,990: t15.2023.11.19 val PER: 0.0359
2026-01-05 13:57:25,990: t15.2023.11.26 val PER: 0.1036
2026-01-05 13:57:25,990: t15.2023.12.03 val PER: 0.1071
2026-01-05 13:57:25,990: t15.2023.12.08 val PER: 0.0972
2026-01-05 13:57:25,990: t15.2023.12.10 val PER: 0.0907
2026-01-05 13:57:25,990: t15.2023.12.17 val PER: 0.1258
2026-01-05 13:57:25,991: t15.2023.12.29 val PER: 0.1208
2026-01-05 13:57:25,991: t15.2024.02.25 val PER: 0.1081
2026-01-05 13:57:25,991: t15.2024.03.08 val PER: 0.2276
2026-01-05 13:57:25,991: t15.2024.03.15 val PER: 0.2033
2026-01-05 13:57:25,991: t15.2024.03.17 val PER: 0.1325
2026-01-05 13:57:25,991: t15.2024.05.10 val PER: 0.1560
2026-01-05 13:57:25,991: t15.2024.06.14 val PER: 0.1546
2026-01-05 13:57:25,991: t15.2024.07.19 val PER: 0.2281
2026-01-05 13:57:25,991: t15.2024.07.21 val PER: 0.0917
2026-01-05 13:57:25,991: t15.2024.07.28 val PER: 0.1257
2026-01-05 13:57:25,991: t15.2025.01.10 val PER: 0.2865
2026-01-05 13:57:25,991: t15.2025.01.12 val PER: 0.1501
2026-01-05 13:57:25,991: t15.2025.03.14 val PER: 0.3417
2026-01-05 13:57:25,991: t15.2025.03.16 val PER: 0.1846
2026-01-05 13:57:25,991: t15.2025.03.30 val PER: 0.2920
2026-01-05 13:57:25,991: t15.2025.04.13 val PER: 0.2097
2026-01-05 13:57:26,316: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_16500
2026-01-05 13:57:35,954: Train batch 16600: loss: 7.51 grad norm: 51.25 time: 0.053
2026-01-05 13:57:55,399: Train batch 16800: loss: 15.78 grad norm: 73.31 time: 0.062
2026-01-05 13:58:14,758: Train batch 17000: loss: 7.44 grad norm: 48.23 time: 0.082
2026-01-05 13:58:14,758: Running test after training batch: 17000
2026-01-05 13:58:14,870: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:58:19,991: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:58:20,026: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-05 13:58:21,959: Val batch 17000: PER (avg): 0.1444 CTC Loss (avg): 14.9897 WER(1gram): 47.21% (n=64) time: 7.200
2026-01-05 13:58:21,959: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-05 13:58:21,959: t15.2023.08.13 val PER: 0.1164
2026-01-05 13:58:21,959: t15.2023.08.18 val PER: 0.1065
2026-01-05 13:58:21,959: t15.2023.08.20 val PER: 0.0953
2026-01-05 13:58:21,959: t15.2023.08.25 val PER: 0.0858
2026-01-05 13:58:21,959: t15.2023.08.27 val PER: 0.1752
2026-01-05 13:58:21,960: t15.2023.09.01 val PER: 0.0795
2026-01-05 13:58:21,960: t15.2023.09.03 val PER: 0.1496
2026-01-05 13:58:21,960: t15.2023.09.24 val PER: 0.1226
2026-01-05 13:58:21,960: t15.2023.09.29 val PER: 0.1276
2026-01-05 13:58:21,960: t15.2023.10.01 val PER: 0.1711
2026-01-05 13:58:21,960: t15.2023.10.06 val PER: 0.0807
2026-01-05 13:58:21,960: t15.2023.10.08 val PER: 0.2314
2026-01-05 13:58:21,960: t15.2023.10.13 val PER: 0.2033
2026-01-05 13:58:21,960: t15.2023.10.15 val PER: 0.1470
2026-01-05 13:58:21,960: t15.2023.10.20 val PER: 0.1678
2026-01-05 13:58:21,961: t15.2023.10.22 val PER: 0.1080
2026-01-05 13:58:21,961: t15.2023.11.03 val PER: 0.1798
2026-01-05 13:58:21,961: t15.2023.11.04 val PER: 0.0307
2026-01-05 13:58:21,961: t15.2023.11.17 val PER: 0.0404
2026-01-05 13:58:21,961: t15.2023.11.19 val PER: 0.0359
2026-01-05 13:58:21,961: t15.2023.11.26 val PER: 0.1022
2026-01-05 13:58:21,961: t15.2023.12.03 val PER: 0.1071
2026-01-05 13:58:21,961: t15.2023.12.08 val PER: 0.0932
2026-01-05 13:58:21,961: t15.2023.12.10 val PER: 0.0841
2026-01-05 13:58:21,961: t15.2023.12.17 val PER: 0.1258
2026-01-05 13:58:21,961: t15.2023.12.29 val PER: 0.1201
2026-01-05 13:58:21,961: t15.2024.02.25 val PER: 0.1025
2026-01-05 13:58:21,961: t15.2024.03.08 val PER: 0.2176
2026-01-05 13:58:21,961: t15.2024.03.15 val PER: 0.1995
2026-01-05 13:58:21,961: t15.2024.03.17 val PER: 0.1297
2026-01-05 13:58:21,961: t15.2024.05.10 val PER: 0.1530
2026-01-05 13:58:21,961: t15.2024.06.14 val PER: 0.1577
2026-01-05 13:58:21,962: t15.2024.07.19 val PER: 0.2215
2026-01-05 13:58:21,962: t15.2024.07.21 val PER: 0.0917
2026-01-05 13:58:21,962: t15.2024.07.28 val PER: 0.1265
2026-01-05 13:58:21,962: t15.2025.01.10 val PER: 0.2851
2026-01-05 13:58:21,962: t15.2025.01.12 val PER: 0.1409
2026-01-05 13:58:21,962: t15.2025.03.14 val PER: 0.3447
2026-01-05 13:58:21,962: t15.2025.03.16 val PER: 0.1754
2026-01-05 13:58:21,962: t15.2025.03.30 val PER: 0.2885
2026-01-05 13:58:21,962: t15.2025.04.13 val PER: 0.2111
2026-01-05 13:58:22,287: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_17000
2026-01-05 13:58:42,398: Train batch 17200: loss: 9.27 grad norm: 51.95 time: 0.084
2026-01-05 13:59:01,676: Train batch 17400: loss: 11.25 grad norm: 58.68 time: 0.071
2026-01-05 13:59:11,850: Running test after training batch: 17500
2026-01-05 13:59:11,957: WER debug GT example: You can see the code at this point as well.
2026-01-05 13:59:17,093: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 13:59:17,129: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-05 13:59:19,136: Val batch 17500: PER (avg): 0.1446 CTC Loss (avg): 14.9777 WER(1gram): 46.70% (n=64) time: 7.285
2026-01-05 13:59:19,136: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 13:59:19,136: t15.2023.08.13 val PER: 0.1123
2026-01-05 13:59:19,136: t15.2023.08.18 val PER: 0.1073
2026-01-05 13:59:19,136: t15.2023.08.20 val PER: 0.1001
2026-01-05 13:59:19,136: t15.2023.08.25 val PER: 0.0858
2026-01-05 13:59:19,137: t15.2023.08.27 val PER: 0.1785
2026-01-05 13:59:19,137: t15.2023.09.01 val PER: 0.0731
2026-01-05 13:59:19,137: t15.2023.09.03 val PER: 0.1556
2026-01-05 13:59:19,137: t15.2023.09.24 val PER: 0.1299
2026-01-05 13:59:19,137: t15.2023.09.29 val PER: 0.1213
2026-01-05 13:59:19,137: t15.2023.10.01 val PER: 0.1684
2026-01-05 13:59:19,137: t15.2023.10.06 val PER: 0.0818
2026-01-05 13:59:19,137: t15.2023.10.08 val PER: 0.2422
2026-01-05 13:59:19,137: t15.2023.10.13 val PER: 0.1963
2026-01-05 13:59:19,137: t15.2023.10.15 val PER: 0.1470
2026-01-05 13:59:19,137: t15.2023.10.20 val PER: 0.1779
2026-01-05 13:59:19,137: t15.2023.10.22 val PER: 0.1102
2026-01-05 13:59:19,137: t15.2023.11.03 val PER: 0.1750
2026-01-05 13:59:19,138: t15.2023.11.04 val PER: 0.0341
2026-01-05 13:59:19,138: t15.2023.11.17 val PER: 0.0373
2026-01-05 13:59:19,138: t15.2023.11.19 val PER: 0.0379
2026-01-05 13:59:19,138: t15.2023.11.26 val PER: 0.1043
2026-01-05 13:59:19,138: t15.2023.12.03 val PER: 0.1113
2026-01-05 13:59:19,138: t15.2023.12.08 val PER: 0.0945
2026-01-05 13:59:19,138: t15.2023.12.10 val PER: 0.0854
2026-01-05 13:59:19,138: t15.2023.12.17 val PER: 0.1289
2026-01-05 13:59:19,138: t15.2023.12.29 val PER: 0.1229
2026-01-05 13:59:19,138: t15.2024.02.25 val PER: 0.1011
2026-01-05 13:59:19,138: t15.2024.03.08 val PER: 0.2219
2026-01-05 13:59:19,138: t15.2024.03.15 val PER: 0.1939
2026-01-05 13:59:19,138: t15.2024.03.17 val PER: 0.1332
2026-01-05 13:59:19,138: t15.2024.05.10 val PER: 0.1560
2026-01-05 13:59:19,138: t15.2024.06.14 val PER: 0.1577
2026-01-05 13:59:19,139: t15.2024.07.19 val PER: 0.2162
2026-01-05 13:59:19,139: t15.2024.07.21 val PER: 0.0897
2026-01-05 13:59:19,139: t15.2024.07.28 val PER: 0.1243
2026-01-05 13:59:19,139: t15.2025.01.10 val PER: 0.2920
2026-01-05 13:59:19,139: t15.2025.01.12 val PER: 0.1416
2026-01-05 13:59:19,139: t15.2025.03.14 val PER: 0.3417
2026-01-05 13:59:19,139: t15.2025.03.16 val PER: 0.1741
2026-01-05 13:59:19,139: t15.2025.03.30 val PER: 0.2977
2026-01-05 13:59:19,139: t15.2025.04.13 val PER: 0.2111
2026-01-05 13:59:19,468: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_17500
2026-01-05 13:59:28,963: Train batch 17600: loss: 9.15 grad norm: 53.56 time: 0.051
2026-01-05 13:59:48,614: Train batch 17800: loss: 5.92 grad norm: 48.07 time: 0.042
2026-01-05 14:00:07,857: Train batch 18000: loss: 10.41 grad norm: 62.67 time: 0.061
2026-01-05 14:00:07,858: Running test after training batch: 18000
2026-01-05 14:00:08,036: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:00:13,289: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:00:13,324: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-05 14:00:15,278: Val batch 18000: PER (avg): 0.1442 CTC Loss (avg): 14.9731 WER(1gram): 45.94% (n=64) time: 7.420
2026-01-05 14:00:15,278: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 14:00:15,279: t15.2023.08.13 val PER: 0.1112
2026-01-05 14:00:15,279: t15.2023.08.18 val PER: 0.1014
2026-01-05 14:00:15,279: t15.2023.08.20 val PER: 0.0985
2026-01-05 14:00:15,279: t15.2023.08.25 val PER: 0.0858
2026-01-05 14:00:15,279: t15.2023.08.27 val PER: 0.1785
2026-01-05 14:00:15,279: t15.2023.09.01 val PER: 0.0787
2026-01-05 14:00:15,279: t15.2023.09.03 val PER: 0.1556
2026-01-05 14:00:15,279: t15.2023.09.24 val PER: 0.1214
2026-01-05 14:00:15,279: t15.2023.09.29 val PER: 0.1295
2026-01-05 14:00:15,279: t15.2023.10.01 val PER: 0.1717
2026-01-05 14:00:15,279: t15.2023.10.06 val PER: 0.0764
2026-01-05 14:00:15,280: t15.2023.10.08 val PER: 0.2314
2026-01-05 14:00:15,280: t15.2023.10.13 val PER: 0.1947
2026-01-05 14:00:15,280: t15.2023.10.15 val PER: 0.1516
2026-01-05 14:00:15,280: t15.2023.10.20 val PER: 0.1745
2026-01-05 14:00:15,280: t15.2023.10.22 val PER: 0.1036
2026-01-05 14:00:15,280: t15.2023.11.03 val PER: 0.1750
2026-01-05 14:00:15,280: t15.2023.11.04 val PER: 0.0341
2026-01-05 14:00:15,280: t15.2023.11.17 val PER: 0.0358
2026-01-05 14:00:15,280: t15.2023.11.19 val PER: 0.0359
2026-01-05 14:00:15,280: t15.2023.11.26 val PER: 0.1014
2026-01-05 14:00:15,281: t15.2023.12.03 val PER: 0.1050
2026-01-05 14:00:15,281: t15.2023.12.08 val PER: 0.0972
2026-01-05 14:00:15,281: t15.2023.12.10 val PER: 0.0854
2026-01-05 14:00:15,281: t15.2023.12.17 val PER: 0.1268
2026-01-05 14:00:15,281: t15.2023.12.29 val PER: 0.1201
2026-01-05 14:00:15,281: t15.2024.02.25 val PER: 0.1011
2026-01-05 14:00:15,281: t15.2024.03.08 val PER: 0.2219
2026-01-05 14:00:15,281: t15.2024.03.15 val PER: 0.1964
2026-01-05 14:00:15,281: t15.2024.03.17 val PER: 0.1304
2026-01-05 14:00:15,281: t15.2024.05.10 val PER: 0.1590
2026-01-05 14:00:15,281: t15.2024.06.14 val PER: 0.1577
2026-01-05 14:00:15,281: t15.2024.07.19 val PER: 0.2182
2026-01-05 14:00:15,281: t15.2024.07.21 val PER: 0.0890
2026-01-05 14:00:15,281: t15.2024.07.28 val PER: 0.1279
2026-01-05 14:00:15,281: t15.2025.01.10 val PER: 0.2906
2026-01-05 14:00:15,282: t15.2025.01.12 val PER: 0.1401
2026-01-05 14:00:15,282: t15.2025.03.14 val PER: 0.3373
2026-01-05 14:00:15,282: t15.2025.03.16 val PER: 0.1741
2026-01-05 14:00:15,282: t15.2025.03.30 val PER: 0.3000
2026-01-05 14:00:15,282: t15.2025.04.13 val PER: 0.2126
2026-01-05 14:00:15,604: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_18000
2026-01-05 14:00:35,190: Train batch 18200: loss: 7.21 grad norm: 47.06 time: 0.073
2026-01-05 14:00:55,077: Train batch 18400: loss: 4.25 grad norm: 39.74 time: 0.058
2026-01-05 14:01:04,689: Running test after training batch: 18500
2026-01-05 14:01:04,853: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:01:10,612: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:01:10,648: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-05 14:01:12,611: Val batch 18500: PER (avg): 0.1442 CTC Loss (avg): 14.9534 WER(1gram): 45.94% (n=64) time: 7.921
2026-01-05 14:01:12,611: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-05 14:01:12,611: t15.2023.08.13 val PER: 0.1112
2026-01-05 14:01:12,611: t15.2023.08.18 val PER: 0.1031
2026-01-05 14:01:12,611: t15.2023.08.20 val PER: 0.0977
2026-01-05 14:01:12,612: t15.2023.08.25 val PER: 0.0858
2026-01-05 14:01:12,612: t15.2023.08.27 val PER: 0.1817
2026-01-05 14:01:12,612: t15.2023.09.01 val PER: 0.0763
2026-01-05 14:01:12,612: t15.2023.09.03 val PER: 0.1485
2026-01-05 14:01:12,612: t15.2023.09.24 val PER: 0.1286
2026-01-05 14:01:12,612: t15.2023.09.29 val PER: 0.1251
2026-01-05 14:01:12,612: t15.2023.10.01 val PER: 0.1717
2026-01-05 14:01:12,612: t15.2023.10.06 val PER: 0.0775
2026-01-05 14:01:12,612: t15.2023.10.08 val PER: 0.2314
2026-01-05 14:01:12,612: t15.2023.10.13 val PER: 0.1932
2026-01-05 14:01:12,613: t15.2023.10.15 val PER: 0.1556
2026-01-05 14:01:12,613: t15.2023.10.20 val PER: 0.1745
2026-01-05 14:01:12,613: t15.2023.10.22 val PER: 0.1036
2026-01-05 14:01:12,613: t15.2023.11.03 val PER: 0.1777
2026-01-05 14:01:12,613: t15.2023.11.04 val PER: 0.0341
2026-01-05 14:01:12,613: t15.2023.11.17 val PER: 0.0389
2026-01-05 14:01:12,613: t15.2023.11.19 val PER: 0.0379
2026-01-05 14:01:12,613: t15.2023.11.26 val PER: 0.1029
2026-01-05 14:01:12,613: t15.2023.12.03 val PER: 0.1061
2026-01-05 14:01:12,613: t15.2023.12.08 val PER: 0.0919
2026-01-05 14:01:12,613: t15.2023.12.10 val PER: 0.0854
2026-01-05 14:01:12,613: t15.2023.12.17 val PER: 0.1227
2026-01-05 14:01:12,613: t15.2023.12.29 val PER: 0.1235
2026-01-05 14:01:12,613: t15.2024.02.25 val PER: 0.0997
2026-01-05 14:01:12,613: t15.2024.03.08 val PER: 0.2233
2026-01-05 14:01:12,614: t15.2024.03.15 val PER: 0.1932
2026-01-05 14:01:12,614: t15.2024.03.17 val PER: 0.1332
2026-01-05 14:01:12,614: t15.2024.05.10 val PER: 0.1545
2026-01-05 14:01:12,614: t15.2024.06.14 val PER: 0.1514
2026-01-05 14:01:12,614: t15.2024.07.19 val PER: 0.2195
2026-01-05 14:01:12,614: t15.2024.07.21 val PER: 0.0862
2026-01-05 14:01:12,614: t15.2024.07.28 val PER: 0.1221
2026-01-05 14:01:12,614: t15.2025.01.10 val PER: 0.2961
2026-01-05 14:01:12,614: t15.2025.01.12 val PER: 0.1463
2026-01-05 14:01:12,614: t15.2025.03.14 val PER: 0.3506
2026-01-05 14:01:12,614: t15.2025.03.16 val PER: 0.1793
2026-01-05 14:01:12,615: t15.2025.03.30 val PER: 0.2885
2026-01-05 14:01:12,615: t15.2025.04.13 val PER: 0.2111
2026-01-05 14:01:12,928: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_18500
2026-01-05 14:01:22,565: Train batch 18600: loss: 12.38 grad norm: 61.81 time: 0.067
2026-01-05 14:01:41,567: Train batch 18800: loss: 8.06 grad norm: 54.79 time: 0.064
2026-01-05 14:02:00,975: Train batch 19000: loss: 8.08 grad norm: 47.20 time: 0.064
2026-01-05 14:02:00,975: Running test after training batch: 19000
2026-01-05 14:02:01,124: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:02:06,276: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:02:06,313: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-05 14:02:08,286: Val batch 19000: PER (avg): 0.1440 CTC Loss (avg): 14.9574 WER(1gram): 45.69% (n=64) time: 7.311
2026-01-05 14:02:08,287: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 14:02:08,287: t15.2023.08.13 val PER: 0.1123
2026-01-05 14:02:08,287: t15.2023.08.18 val PER: 0.1023
2026-01-05 14:02:08,287: t15.2023.08.20 val PER: 0.1001
2026-01-05 14:02:08,287: t15.2023.08.25 val PER: 0.0813
2026-01-05 14:02:08,287: t15.2023.08.27 val PER: 0.1688
2026-01-05 14:02:08,287: t15.2023.09.01 val PER: 0.0763
2026-01-05 14:02:08,287: t15.2023.09.03 val PER: 0.1544
2026-01-05 14:02:08,288: t15.2023.09.24 val PER: 0.1262
2026-01-05 14:02:08,288: t15.2023.09.29 val PER: 0.1251
2026-01-05 14:02:08,288: t15.2023.10.01 val PER: 0.1711
2026-01-05 14:02:08,288: t15.2023.10.06 val PER: 0.0764
2026-01-05 14:02:08,288: t15.2023.10.08 val PER: 0.2287
2026-01-05 14:02:08,288: t15.2023.10.13 val PER: 0.1939
2026-01-05 14:02:08,288: t15.2023.10.15 val PER: 0.1496
2026-01-05 14:02:08,289: t15.2023.10.20 val PER: 0.1779
2026-01-05 14:02:08,289: t15.2023.10.22 val PER: 0.1080
2026-01-05 14:02:08,289: t15.2023.11.03 val PER: 0.1744
2026-01-05 14:02:08,289: t15.2023.11.04 val PER: 0.0375
2026-01-05 14:02:08,289: t15.2023.11.17 val PER: 0.0404
2026-01-05 14:02:08,289: t15.2023.11.19 val PER: 0.0319
2026-01-05 14:02:08,289: t15.2023.11.26 val PER: 0.1022
2026-01-05 14:02:08,289: t15.2023.12.03 val PER: 0.1050
2026-01-05 14:02:08,289: t15.2023.12.08 val PER: 0.0925
2026-01-05 14:02:08,289: t15.2023.12.10 val PER: 0.0841
2026-01-05 14:02:08,289: t15.2023.12.17 val PER: 0.1289
2026-01-05 14:02:08,289: t15.2023.12.29 val PER: 0.1201
2026-01-05 14:02:08,289: t15.2024.02.25 val PER: 0.1067
2026-01-05 14:02:08,289: t15.2024.03.08 val PER: 0.2262
2026-01-05 14:02:08,289: t15.2024.03.15 val PER: 0.1951
2026-01-05 14:02:08,289: t15.2024.03.17 val PER: 0.1332
2026-01-05 14:02:08,289: t15.2024.05.10 val PER: 0.1516
2026-01-05 14:02:08,290: t15.2024.06.14 val PER: 0.1577
2026-01-05 14:02:08,290: t15.2024.07.19 val PER: 0.2248
2026-01-05 14:02:08,290: t15.2024.07.21 val PER: 0.0848
2026-01-05 14:02:08,290: t15.2024.07.28 val PER: 0.1279
2026-01-05 14:02:08,290: t15.2025.01.10 val PER: 0.2879
2026-01-05 14:02:08,290: t15.2025.01.12 val PER: 0.1416
2026-01-05 14:02:08,290: t15.2025.03.14 val PER: 0.3462
2026-01-05 14:02:08,290: t15.2025.03.16 val PER: 0.1754
2026-01-05 14:02:08,290: t15.2025.03.30 val PER: 0.2931
2026-01-05 14:02:08,290: t15.2025.04.13 val PER: 0.2097
2026-01-05 14:02:08,613: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_19000
2026-01-05 14:02:27,651: Train batch 19200: loss: 5.64 grad norm: 45.52 time: 0.063
2026-01-05 14:02:46,923: Train batch 19400: loss: 4.54 grad norm: 37.82 time: 0.053
2026-01-05 14:02:56,651: Running test after training batch: 19500
2026-01-05 14:02:56,816: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:03:01,924: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:03:01,961: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-05 14:03:03,980: Val batch 19500: PER (avg): 0.1435 CTC Loss (avg): 14.9184 WER(1gram): 47.21% (n=64) time: 7.328
2026-01-05 14:03:03,980: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-05 14:03:03,980: t15.2023.08.13 val PER: 0.1123
2026-01-05 14:03:03,980: t15.2023.08.18 val PER: 0.1039
2026-01-05 14:03:03,980: t15.2023.08.20 val PER: 0.0977
2026-01-05 14:03:03,981: t15.2023.08.25 val PER: 0.0798
2026-01-05 14:03:03,981: t15.2023.08.27 val PER: 0.1720
2026-01-05 14:03:03,981: t15.2023.09.01 val PER: 0.0755
2026-01-05 14:03:03,981: t15.2023.09.03 val PER: 0.1449
2026-01-05 14:03:03,981: t15.2023.09.24 val PER: 0.1262
2026-01-05 14:03:03,981: t15.2023.09.29 val PER: 0.1225
2026-01-05 14:03:03,981: t15.2023.10.01 val PER: 0.1678
2026-01-05 14:03:03,981: t15.2023.10.06 val PER: 0.0753
2026-01-05 14:03:03,981: t15.2023.10.08 val PER: 0.2287
2026-01-05 14:03:03,982: t15.2023.10.13 val PER: 0.1939
2026-01-05 14:03:03,982: t15.2023.10.15 val PER: 0.1523
2026-01-05 14:03:03,982: t15.2023.10.20 val PER: 0.1812
2026-01-05 14:03:03,982: t15.2023.10.22 val PER: 0.1058
2026-01-05 14:03:03,982: t15.2023.11.03 val PER: 0.1777
2026-01-05 14:03:03,982: t15.2023.11.04 val PER: 0.0375
2026-01-05 14:03:03,982: t15.2023.11.17 val PER: 0.0342
2026-01-05 14:03:03,982: t15.2023.11.19 val PER: 0.0339
2026-01-05 14:03:03,982: t15.2023.11.26 val PER: 0.1043
2026-01-05 14:03:03,982: t15.2023.12.03 val PER: 0.1040
2026-01-05 14:03:03,982: t15.2023.12.08 val PER: 0.0912
2026-01-05 14:03:03,982: t15.2023.12.10 val PER: 0.0854
2026-01-05 14:03:03,982: t15.2023.12.17 val PER: 0.1268
2026-01-05 14:03:03,982: t15.2023.12.29 val PER: 0.1235
2026-01-05 14:03:03,983: t15.2024.02.25 val PER: 0.1025
2026-01-05 14:03:03,983: t15.2024.03.08 val PER: 0.2148
2026-01-05 14:03:03,983: t15.2024.03.15 val PER: 0.1914
2026-01-05 14:03:03,983: t15.2024.03.17 val PER: 0.1311
2026-01-05 14:03:03,983: t15.2024.05.10 val PER: 0.1545
2026-01-05 14:03:03,983: t15.2024.06.14 val PER: 0.1640
2026-01-05 14:03:03,983: t15.2024.07.19 val PER: 0.2208
2026-01-05 14:03:03,983: t15.2024.07.21 val PER: 0.0848
2026-01-05 14:03:03,983: t15.2024.07.28 val PER: 0.1243
2026-01-05 14:03:03,983: t15.2025.01.10 val PER: 0.2948
2026-01-05 14:03:03,983: t15.2025.01.12 val PER: 0.1470
2026-01-05 14:03:03,984: t15.2025.03.14 val PER: 0.3447
2026-01-05 14:03:03,984: t15.2025.03.16 val PER: 0.1754
2026-01-05 14:03:03,984: t15.2025.03.30 val PER: 0.2908
2026-01-05 14:03:03,984: t15.2025.04.13 val PER: 0.2168
2026-01-05 14:03:04,307: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_19500
2026-01-05 14:03:13,737: Train batch 19600: loss: 7.65 grad norm: 51.27 time: 0.057
2026-01-05 14:03:32,698: Train batch 19800: loss: 6.79 grad norm: 46.62 time: 0.056
2026-01-05 14:03:51,866: Running test after training batch: 19999
2026-01-05 14:03:51,963: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:03:56,931: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:03:56,967: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-05 14:03:59,005: Val batch 19999: PER (avg): 0.1440 CTC Loss (avg): 14.9415 WER(1gram): 45.69% (n=64) time: 7.139
2026-01-05 14:03:59,006: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 14:03:59,006: t15.2023.08.13 val PER: 0.1060
2026-01-05 14:03:59,006: t15.2023.08.18 val PER: 0.1048
2026-01-05 14:03:59,006: t15.2023.08.20 val PER: 0.1025
2026-01-05 14:03:59,006: t15.2023.08.25 val PER: 0.0873
2026-01-05 14:03:59,006: t15.2023.08.27 val PER: 0.1736
2026-01-05 14:03:59,006: t15.2023.09.01 val PER: 0.0747
2026-01-05 14:03:59,006: t15.2023.09.03 val PER: 0.1520
2026-01-05 14:03:59,006: t15.2023.09.24 val PER: 0.1274
2026-01-05 14:03:59,006: t15.2023.09.29 val PER: 0.1257
2026-01-05 14:03:59,006: t15.2023.10.01 val PER: 0.1678
2026-01-05 14:03:59,007: t15.2023.10.06 val PER: 0.0753
2026-01-05 14:03:59,007: t15.2023.10.08 val PER: 0.2287
2026-01-05 14:03:59,007: t15.2023.10.13 val PER: 0.1971
2026-01-05 14:03:59,007: t15.2023.10.15 val PER: 0.1463
2026-01-05 14:03:59,007: t15.2023.10.20 val PER: 0.1846
2026-01-05 14:03:59,007: t15.2023.10.22 val PER: 0.1047
2026-01-05 14:03:59,007: t15.2023.11.03 val PER: 0.1744
2026-01-05 14:03:59,007: t15.2023.11.04 val PER: 0.0375
2026-01-05 14:03:59,007: t15.2023.11.17 val PER: 0.0358
2026-01-05 14:03:59,007: t15.2023.11.19 val PER: 0.0339
2026-01-05 14:03:59,007: t15.2023.11.26 val PER: 0.1058
2026-01-05 14:03:59,007: t15.2023.12.03 val PER: 0.1008
2026-01-05 14:03:59,008: t15.2023.12.08 val PER: 0.0912
2026-01-05 14:03:59,008: t15.2023.12.10 val PER: 0.0894
2026-01-05 14:03:59,008: t15.2023.12.17 val PER: 0.1289
2026-01-05 14:03:59,008: t15.2023.12.29 val PER: 0.1235
2026-01-05 14:03:59,008: t15.2024.02.25 val PER: 0.1081
2026-01-05 14:03:59,008: t15.2024.03.08 val PER: 0.2248
2026-01-05 14:03:59,008: t15.2024.03.15 val PER: 0.1970
2026-01-05 14:03:59,008: t15.2024.03.17 val PER: 0.1325
2026-01-05 14:03:59,008: t15.2024.05.10 val PER: 0.1545
2026-01-05 14:03:59,009: t15.2024.06.14 val PER: 0.1546
2026-01-05 14:03:59,009: t15.2024.07.19 val PER: 0.2221
2026-01-05 14:03:59,009: t15.2024.07.21 val PER: 0.0862
2026-01-05 14:03:59,009: t15.2024.07.28 val PER: 0.1265
2026-01-05 14:03:59,009: t15.2025.01.10 val PER: 0.2893
2026-01-05 14:03:59,009: t15.2025.01.12 val PER: 0.1440
2026-01-05 14:03:59,010: t15.2025.03.14 val PER: 0.3476
2026-01-05 14:03:59,010: t15.2025.03.16 val PER: 0.1780
2026-01-05 14:03:59,010: t15.2025.03.30 val PER: 0.2885
2026-01-05 14:03:59,010: t15.2025.04.13 val PER: 0.2083
2026-01-05 14:03:59,319: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p002_wd1e-5/checkpoint/checkpoint_batch_19999
2026-01-05 14:03:59,354: Best avg val PER achieved: 0.14715
2026-01-05 14:03:59,354: Total training time: 37.97 minutes

=== RUN p005_wd1e-5.yaml ===
2026-01-05 14:04:05,869: Using device: cuda:0
2026-01-05 14:04:07,980: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-05 14:04:08,001: Using 45 sessions after filtering (from 45).
2026-01-05 14:04:08,416: Using torch.compile (if available)
2026-01-05 14:04:08,416: torch.compile not available (torch<2.0). Skipping.
2026-01-05 14:04:08,416: Initialized RNN decoding model
2026-01-05 14:04:08,417: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-05 14:04:08,417: Model has 44,907,305 parameters
2026-01-05 14:04:08,417: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-05 14:04:09,754: Successfully initialized datasets
2026-01-05 14:04:09,755: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-05 14:04:10,828: Train batch 0: loss: 579.33 grad norm: 1379.00 time: 0.205
2026-01-05 14:04:10,829: Running test after training batch: 0
2026-01-05 14:04:10,954: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:04:16,623: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-05 14:04:17,415: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-05 14:04:55,138: Val batch 0: PER (avg): 1.4296 CTC Loss (avg): 633.1396 WER(1gram): 100.00% (n=64) time: 44.309
2026-01-05 14:04:55,138: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-05 14:04:55,139: t15.2023.08.13 val PER: 1.3067
2026-01-05 14:04:55,139: t15.2023.08.18 val PER: 1.4241
2026-01-05 14:04:55,139: t15.2023.08.20 val PER: 1.3034
2026-01-05 14:04:55,139: t15.2023.08.25 val PER: 1.3434
2026-01-05 14:04:55,139: t15.2023.08.27 val PER: 1.2508
2026-01-05 14:04:55,139: t15.2023.09.01 val PER: 1.4562
2026-01-05 14:04:55,139: t15.2023.09.03 val PER: 1.3064
2026-01-05 14:04:55,139: t15.2023.09.24 val PER: 1.5400
2026-01-05 14:04:55,139: t15.2023.09.29 val PER: 1.4697
2026-01-05 14:04:55,139: t15.2023.10.01 val PER: 1.2120
2026-01-05 14:04:55,139: t15.2023.10.06 val PER: 1.4941
2026-01-05 14:04:55,139: t15.2023.10.08 val PER: 1.1813
2026-01-05 14:04:55,139: t15.2023.10.13 val PER: 1.4073
2026-01-05 14:04:55,140: t15.2023.10.15 val PER: 1.3863
2026-01-05 14:04:55,140: t15.2023.10.20 val PER: 1.4866
2026-01-05 14:04:55,140: t15.2023.10.22 val PER: 1.3953
2026-01-05 14:04:55,140: t15.2023.11.03 val PER: 1.5943
2026-01-05 14:04:55,140: t15.2023.11.04 val PER: 2.0273
2026-01-05 14:04:55,140: t15.2023.11.17 val PER: 1.9596
2026-01-05 14:04:55,140: t15.2023.11.19 val PER: 1.6806
2026-01-05 14:04:55,140: t15.2023.11.26 val PER: 1.5384
2026-01-05 14:04:55,140: t15.2023.12.03 val PER: 1.4233
2026-01-05 14:04:55,140: t15.2023.12.08 val PER: 1.4487
2026-01-05 14:04:55,141: t15.2023.12.10 val PER: 1.6951
2026-01-05 14:04:55,141: t15.2023.12.17 val PER: 1.3067
2026-01-05 14:04:55,141: t15.2023.12.29 val PER: 1.4084
2026-01-05 14:04:55,141: t15.2024.02.25 val PER: 1.4213
2026-01-05 14:04:55,141: t15.2024.03.08 val PER: 1.3215
2026-01-05 14:04:55,141: t15.2024.03.15 val PER: 1.3183
2026-01-05 14:04:55,141: t15.2024.03.17 val PER: 1.4003
2026-01-05 14:04:55,141: t15.2024.05.10 val PER: 1.3224
2026-01-05 14:04:55,141: t15.2024.06.14 val PER: 1.5315
2026-01-05 14:04:55,141: t15.2024.07.19 val PER: 1.0817
2026-01-05 14:04:55,141: t15.2024.07.21 val PER: 1.6345
2026-01-05 14:04:55,141: t15.2024.07.28 val PER: 1.6618
2026-01-05 14:04:55,141: t15.2025.01.10 val PER: 1.0854
2026-01-05 14:04:55,141: t15.2025.01.12 val PER: 1.7652
2026-01-05 14:04:55,141: t15.2025.03.14 val PER: 1.0355
2026-01-05 14:04:55,142: t15.2025.03.16 val PER: 1.6086
2026-01-05 14:04:55,142: t15.2025.03.30 val PER: 1.2897
2026-01-05 14:04:55,142: t15.2025.04.13 val PER: 1.5849
2026-01-05 14:04:55,143: New best val WER(1gram) inf% --> 100.00%
2026-01-05 14:04:55,143: Checkpointing model
2026-01-05 14:04:55,441: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:04:55,742: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_0
2026-01-05 14:05:14,971: Train batch 200: loss: 77.03 grad norm: 106.25 time: 0.054
2026-01-05 14:05:33,662: Train batch 400: loss: 53.53 grad norm: 95.72 time: 0.063
2026-01-05 14:05:43,156: Running test after training batch: 500
2026-01-05 14:05:43,311: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:05:48,489: WER debug example
  GT : you can see the code at this point as well
  PR : used end ease thus uhde at this ide is aisle
2026-01-05 14:05:48,526: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus ass ide
2026-01-05 14:05:51,065: Val batch 500: PER (avg): 0.5130 CTC Loss (avg): 55.4871 WER(1gram): 88.83% (n=64) time: 7.909
2026-01-05 14:05:51,065: WER lens: avg_true_words=6.16 avg_pred_words=5.64 max_pred_words=11
2026-01-05 14:05:51,066: t15.2023.08.13 val PER: 0.4647
2026-01-05 14:05:51,066: t15.2023.08.18 val PER: 0.4384
2026-01-05 14:05:51,066: t15.2023.08.20 val PER: 0.4345
2026-01-05 14:05:51,066: t15.2023.08.25 val PER: 0.4172
2026-01-05 14:05:51,066: t15.2023.08.27 val PER: 0.5322
2026-01-05 14:05:51,066: t15.2023.09.01 val PER: 0.4026
2026-01-05 14:05:51,067: t15.2023.09.03 val PER: 0.4988
2026-01-05 14:05:51,067: t15.2023.09.24 val PER: 0.4223
2026-01-05 14:05:51,067: t15.2023.09.29 val PER: 0.4652
2026-01-05 14:05:51,067: t15.2023.10.01 val PER: 0.5185
2026-01-05 14:05:51,067: t15.2023.10.06 val PER: 0.4187
2026-01-05 14:05:51,067: t15.2023.10.08 val PER: 0.5304
2026-01-05 14:05:51,067: t15.2023.10.13 val PER: 0.5663
2026-01-05 14:05:51,067: t15.2023.10.15 val PER: 0.4931
2026-01-05 14:05:51,067: t15.2023.10.20 val PER: 0.4530
2026-01-05 14:05:51,067: t15.2023.10.22 val PER: 0.4477
2026-01-05 14:05:51,068: t15.2023.11.03 val PER: 0.4953
2026-01-05 14:05:51,068: t15.2023.11.04 val PER: 0.2662
2026-01-05 14:05:51,068: t15.2023.11.17 val PER: 0.3499
2026-01-05 14:05:51,068: t15.2023.11.19 val PER: 0.3253
2026-01-05 14:05:51,068: t15.2023.11.26 val PER: 0.5449
2026-01-05 14:05:51,068: t15.2023.12.03 val PER: 0.4916
2026-01-05 14:05:51,068: t15.2023.12.08 val PER: 0.5093
2026-01-05 14:05:51,068: t15.2023.12.10 val PER: 0.4586
2026-01-05 14:05:51,068: t15.2023.12.17 val PER: 0.5665
2026-01-05 14:05:51,069: t15.2023.12.29 val PER: 0.5305
2026-01-05 14:05:51,069: t15.2024.02.25 val PER: 0.4691
2026-01-05 14:05:51,069: t15.2024.03.08 val PER: 0.6102
2026-01-05 14:05:51,069: t15.2024.03.15 val PER: 0.5466
2026-01-05 14:05:51,069: t15.2024.03.17 val PER: 0.5091
2026-01-05 14:05:51,069: t15.2024.05.10 val PER: 0.5305
2026-01-05 14:05:51,069: t15.2024.06.14 val PER: 0.5063
2026-01-05 14:05:51,069: t15.2024.07.19 val PER: 0.6730
2026-01-05 14:05:51,069: t15.2024.07.21 val PER: 0.4772
2026-01-05 14:05:51,070: t15.2024.07.28 val PER: 0.4949
2026-01-05 14:05:51,070: t15.2025.01.10 val PER: 0.7521
2026-01-05 14:05:51,070: t15.2025.01.12 val PER: 0.5574
2026-01-05 14:05:51,070: t15.2025.03.14 val PER: 0.7382
2026-01-05 14:05:51,070: t15.2025.03.16 val PER: 0.5916
2026-01-05 14:05:51,070: t15.2025.03.30 val PER: 0.7356
2026-01-05 14:05:51,070: t15.2025.04.13 val PER: 0.5621
2026-01-05 14:05:51,071: New best val WER(1gram) 100.00% --> 88.83%
2026-01-05 14:05:51,071: Checkpointing model
2026-01-05 14:05:51,730: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:05:52,030: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_500
2026-01-05 14:06:01,589: Train batch 600: loss: 48.12 grad norm: 78.50 time: 0.078
2026-01-05 14:06:20,892: Train batch 800: loss: 41.04 grad norm: 92.01 time: 0.057
2026-01-05 14:06:40,201: Train batch 1000: loss: 42.61 grad norm: 82.77 time: 0.067
2026-01-05 14:06:40,201: Running test after training batch: 1000
2026-01-05 14:06:40,345: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:06:45,465: WER debug example
  GT : you can see the code at this point as well
  PR : used went ease thus code it this and is while
2026-01-05 14:06:45,498: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus wass it
2026-01-05 14:06:47,352: Val batch 1000: PER (avg): 0.4086 CTC Loss (avg): 42.3383 WER(1gram): 81.47% (n=64) time: 7.151
2026-01-05 14:06:47,353: WER lens: avg_true_words=6.16 avg_pred_words=5.52 max_pred_words=12
2026-01-05 14:06:47,353: t15.2023.08.13 val PER: 0.3836
2026-01-05 14:06:47,353: t15.2023.08.18 val PER: 0.3386
2026-01-05 14:06:47,353: t15.2023.08.20 val PER: 0.3455
2026-01-05 14:06:47,353: t15.2023.08.25 val PER: 0.2861
2026-01-05 14:06:47,354: t15.2023.08.27 val PER: 0.4212
2026-01-05 14:06:47,354: t15.2023.09.01 val PER: 0.3125
2026-01-05 14:06:47,354: t15.2023.09.03 val PER: 0.3955
2026-01-05 14:06:47,354: t15.2023.09.24 val PER: 0.3325
2026-01-05 14:06:47,354: t15.2023.09.29 val PER: 0.3689
2026-01-05 14:06:47,354: t15.2023.10.01 val PER: 0.4102
2026-01-05 14:06:47,354: t15.2023.10.06 val PER: 0.3219
2026-01-05 14:06:47,355: t15.2023.10.08 val PER: 0.4574
2026-01-05 14:06:47,355: t15.2023.10.13 val PER: 0.4616
2026-01-05 14:06:47,355: t15.2023.10.15 val PER: 0.3771
2026-01-05 14:06:47,355: t15.2023.10.20 val PER: 0.3792
2026-01-05 14:06:47,355: t15.2023.10.22 val PER: 0.3474
2026-01-05 14:06:47,355: t15.2023.11.03 val PER: 0.3989
2026-01-05 14:06:47,355: t15.2023.11.04 val PER: 0.1570
2026-01-05 14:06:47,355: t15.2023.11.17 val PER: 0.2659
2026-01-05 14:06:47,355: t15.2023.11.19 val PER: 0.2116
2026-01-05 14:06:47,355: t15.2023.11.26 val PER: 0.4449
2026-01-05 14:06:47,355: t15.2023.12.03 val PER: 0.3960
2026-01-05 14:06:47,355: t15.2023.12.08 val PER: 0.4008
2026-01-05 14:06:47,355: t15.2023.12.10 val PER: 0.3482
2026-01-05 14:06:47,355: t15.2023.12.17 val PER: 0.4044
2026-01-05 14:06:47,355: t15.2023.12.29 val PER: 0.4118
2026-01-05 14:06:47,355: t15.2024.02.25 val PER: 0.3539
2026-01-05 14:06:47,356: t15.2024.03.08 val PER: 0.5050
2026-01-05 14:06:47,356: t15.2024.03.15 val PER: 0.4428
2026-01-05 14:06:47,356: t15.2024.03.17 val PER: 0.3996
2026-01-05 14:06:47,356: t15.2024.05.10 val PER: 0.4116
2026-01-05 14:06:47,356: t15.2024.06.14 val PER: 0.4117
2026-01-05 14:06:47,356: t15.2024.07.19 val PER: 0.5333
2026-01-05 14:06:47,356: t15.2024.07.21 val PER: 0.3738
2026-01-05 14:06:47,356: t15.2024.07.28 val PER: 0.4184
2026-01-05 14:06:47,356: t15.2025.01.10 val PER: 0.6074
2026-01-05 14:06:47,356: t15.2025.01.12 val PER: 0.4496
2026-01-05 14:06:47,356: t15.2025.03.14 val PER: 0.6228
2026-01-05 14:06:47,356: t15.2025.03.16 val PER: 0.4908
2026-01-05 14:06:47,356: t15.2025.03.30 val PER: 0.6437
2026-01-05 14:06:47,356: t15.2025.04.13 val PER: 0.4979
2026-01-05 14:06:47,357: New best val WER(1gram) 88.83% --> 81.47%
2026-01-05 14:06:47,357: Checkpointing model
2026-01-05 14:06:48,001: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:06:48,308: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_1000
2026-01-05 14:07:08,296: Train batch 1200: loss: 32.69 grad norm: 72.80 time: 0.068
2026-01-05 14:07:27,809: Train batch 1400: loss: 35.43 grad norm: 75.83 time: 0.061
2026-01-05 14:07:37,853: Running test after training batch: 1500
2026-01-05 14:07:38,033: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:07:43,133: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt ease the owed it this boyde is will
2026-01-05 14:07:43,164: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap that os
2026-01-05 14:07:44,794: Val batch 1500: PER (avg): 0.3778 CTC Loss (avg): 37.0924 WER(1gram): 76.65% (n=64) time: 6.940
2026-01-05 14:07:44,794: WER lens: avg_true_words=6.16 avg_pred_words=5.09 max_pred_words=11
2026-01-05 14:07:44,794: t15.2023.08.13 val PER: 0.3410
2026-01-05 14:07:44,795: t15.2023.08.18 val PER: 0.3219
2026-01-05 14:07:44,795: t15.2023.08.20 val PER: 0.3010
2026-01-05 14:07:44,795: t15.2023.08.25 val PER: 0.2575
2026-01-05 14:07:44,795: t15.2023.08.27 val PER: 0.3939
2026-01-05 14:07:44,795: t15.2023.09.01 val PER: 0.2768
2026-01-05 14:07:44,795: t15.2023.09.03 val PER: 0.3670
2026-01-05 14:07:44,795: t15.2023.09.24 val PER: 0.3046
2026-01-05 14:07:44,795: t15.2023.09.29 val PER: 0.3331
2026-01-05 14:07:44,795: t15.2023.10.01 val PER: 0.3983
2026-01-05 14:07:44,796: t15.2023.10.06 val PER: 0.2853
2026-01-05 14:07:44,796: t15.2023.10.08 val PER: 0.4181
2026-01-05 14:07:44,796: t15.2023.10.13 val PER: 0.4430
2026-01-05 14:07:44,796: t15.2023.10.15 val PER: 0.3553
2026-01-05 14:07:44,796: t15.2023.10.20 val PER: 0.3255
2026-01-05 14:07:44,796: t15.2023.10.22 val PER: 0.3018
2026-01-05 14:07:44,796: t15.2023.11.03 val PER: 0.3602
2026-01-05 14:07:44,796: t15.2023.11.04 val PER: 0.1126
2026-01-05 14:07:44,796: t15.2023.11.17 val PER: 0.2224
2026-01-05 14:07:44,796: t15.2023.11.19 val PER: 0.1637
2026-01-05 14:07:44,796: t15.2023.11.26 val PER: 0.4109
2026-01-05 14:07:44,797: t15.2023.12.03 val PER: 0.3687
2026-01-05 14:07:44,797: t15.2023.12.08 val PER: 0.3622
2026-01-05 14:07:44,797: t15.2023.12.10 val PER: 0.3180
2026-01-05 14:07:44,797: t15.2023.12.17 val PER: 0.3794
2026-01-05 14:07:44,797: t15.2023.12.29 val PER: 0.3761
2026-01-05 14:07:44,797: t15.2024.02.25 val PER: 0.3034
2026-01-05 14:07:44,797: t15.2024.03.08 val PER: 0.4723
2026-01-05 14:07:44,797: t15.2024.03.15 val PER: 0.4096
2026-01-05 14:07:44,797: t15.2024.03.17 val PER: 0.3759
2026-01-05 14:07:44,797: t15.2024.05.10 val PER: 0.3952
2026-01-05 14:07:44,797: t15.2024.06.14 val PER: 0.4006
2026-01-05 14:07:44,798: t15.2024.07.19 val PER: 0.5241
2026-01-05 14:07:44,798: t15.2024.07.21 val PER: 0.3483
2026-01-05 14:07:44,798: t15.2024.07.28 val PER: 0.3588
2026-01-05 14:07:44,798: t15.2025.01.10 val PER: 0.6006
2026-01-05 14:07:44,798: t15.2025.01.12 val PER: 0.4234
2026-01-05 14:07:44,798: t15.2025.03.14 val PER: 0.5873
2026-01-05 14:07:44,798: t15.2025.03.16 val PER: 0.4424
2026-01-05 14:07:44,798: t15.2025.03.30 val PER: 0.6230
2026-01-05 14:07:44,798: t15.2025.04.13 val PER: 0.4750
2026-01-05 14:07:44,799: New best val WER(1gram) 81.47% --> 76.65%
2026-01-05 14:07:44,799: Checkpointing model
2026-01-05 14:07:45,473: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:07:45,772: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_1500
2026-01-05 14:07:55,717: Train batch 1600: loss: 36.05 grad norm: 77.84 time: 0.064
2026-01-05 14:08:15,063: Train batch 1800: loss: 34.92 grad norm: 72.74 time: 0.089
2026-01-05 14:08:34,019: Train batch 2000: loss: 33.75 grad norm: 69.96 time: 0.067
2026-01-05 14:08:34,019: Running test after training batch: 2000
2026-01-05 14:08:34,127: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:08:39,830: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this bonde is will
2026-01-05 14:08:39,860: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the us id
2026-01-05 14:08:41,459: Val batch 2000: PER (avg): 0.3262 CTC Loss (avg): 32.5676 WER(1gram): 68.02% (n=64) time: 7.439
2026-01-05 14:08:41,459: WER lens: avg_true_words=6.16 avg_pred_words=5.45 max_pred_words=11
2026-01-05 14:08:41,459: t15.2023.08.13 val PER: 0.2942
2026-01-05 14:08:41,459: t15.2023.08.18 val PER: 0.2540
2026-01-05 14:08:41,460: t15.2023.08.20 val PER: 0.2566
2026-01-05 14:08:41,460: t15.2023.08.25 val PER: 0.2349
2026-01-05 14:08:41,460: t15.2023.08.27 val PER: 0.3392
2026-01-05 14:08:41,460: t15.2023.09.01 val PER: 0.2354
2026-01-05 14:08:41,460: t15.2023.09.03 val PER: 0.3266
2026-01-05 14:08:41,460: t15.2023.09.24 val PER: 0.2512
2026-01-05 14:08:41,460: t15.2023.09.29 val PER: 0.2789
2026-01-05 14:08:41,460: t15.2023.10.01 val PER: 0.3283
2026-01-05 14:08:41,460: t15.2023.10.06 val PER: 0.2379
2026-01-05 14:08:41,460: t15.2023.10.08 val PER: 0.3884
2026-01-05 14:08:41,460: t15.2023.10.13 val PER: 0.3871
2026-01-05 14:08:41,460: t15.2023.10.15 val PER: 0.3138
2026-01-05 14:08:41,460: t15.2023.10.20 val PER: 0.2785
2026-01-05 14:08:41,460: t15.2023.10.22 val PER: 0.2695
2026-01-05 14:08:41,460: t15.2023.11.03 val PER: 0.3121
2026-01-05 14:08:41,460: t15.2023.11.04 val PER: 0.0887
2026-01-05 14:08:41,461: t15.2023.11.17 val PER: 0.1726
2026-01-05 14:08:41,461: t15.2023.11.19 val PER: 0.1277
2026-01-05 14:08:41,461: t15.2023.11.26 val PER: 0.3565
2026-01-05 14:08:41,461: t15.2023.12.03 val PER: 0.3204
2026-01-05 14:08:41,461: t15.2023.12.08 val PER: 0.3096
2026-01-05 14:08:41,461: t15.2023.12.10 val PER: 0.2707
2026-01-05 14:08:41,461: t15.2023.12.17 val PER: 0.3202
2026-01-05 14:08:41,461: t15.2023.12.29 val PER: 0.3246
2026-01-05 14:08:41,461: t15.2024.02.25 val PER: 0.2654
2026-01-05 14:08:41,461: t15.2024.03.08 val PER: 0.3997
2026-01-05 14:08:41,461: t15.2024.03.15 val PER: 0.3533
2026-01-05 14:08:41,461: t15.2024.03.17 val PER: 0.3326
2026-01-05 14:08:41,462: t15.2024.05.10 val PER: 0.3373
2026-01-05 14:08:41,462: t15.2024.06.14 val PER: 0.3344
2026-01-05 14:08:41,462: t15.2024.07.19 val PER: 0.4707
2026-01-05 14:08:41,462: t15.2024.07.21 val PER: 0.2869
2026-01-05 14:08:41,462: t15.2024.07.28 val PER: 0.3110
2026-01-05 14:08:41,462: t15.2025.01.10 val PER: 0.5344
2026-01-05 14:08:41,462: t15.2025.01.12 val PER: 0.3834
2026-01-05 14:08:41,462: t15.2025.03.14 val PER: 0.5148
2026-01-05 14:08:41,462: t15.2025.03.16 val PER: 0.3874
2026-01-05 14:08:41,462: t15.2025.03.30 val PER: 0.5517
2026-01-05 14:08:41,462: t15.2025.04.13 val PER: 0.4123
2026-01-05 14:08:41,464: New best val WER(1gram) 76.65% --> 68.02%
2026-01-05 14:08:41,464: Checkpointing model
2026-01-05 14:08:42,118: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:08:42,417: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_2000
2026-01-05 14:09:01,629: Train batch 2200: loss: 28.13 grad norm: 68.92 time: 0.060
2026-01-05 14:09:20,772: Train batch 2400: loss: 28.82 grad norm: 61.70 time: 0.052
2026-01-05 14:09:30,248: Running test after training batch: 2500
2026-01-05 14:09:30,418: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:09:35,693: WER debug example
  GT : you can see the code at this point as well
  PR : yule id e the good at this point is will
2026-01-05 14:09:35,724: WER debug example
  GT : how does it keep the cost down
  PR : owl des it keep the us it
2026-01-05 14:09:37,460: Val batch 2500: PER (avg): 0.3050 CTC Loss (avg): 30.0219 WER(1gram): 67.77% (n=64) time: 7.211
2026-01-05 14:09:37,460: WER lens: avg_true_words=6.16 avg_pred_words=5.41 max_pred_words=11
2026-01-05 14:09:37,460: t15.2023.08.13 val PER: 0.2890
2026-01-05 14:09:37,460: t15.2023.08.18 val PER: 0.2381
2026-01-05 14:09:37,460: t15.2023.08.20 val PER: 0.2343
2026-01-05 14:09:37,461: t15.2023.08.25 val PER: 0.2033
2026-01-05 14:09:37,461: t15.2023.08.27 val PER: 0.3280
2026-01-05 14:09:37,461: t15.2023.09.01 val PER: 0.2143
2026-01-05 14:09:37,461: t15.2023.09.03 val PER: 0.3088
2026-01-05 14:09:37,461: t15.2023.09.24 val PER: 0.2221
2026-01-05 14:09:37,461: t15.2023.09.29 val PER: 0.2578
2026-01-05 14:09:37,461: t15.2023.10.01 val PER: 0.3071
2026-01-05 14:09:37,461: t15.2023.10.06 val PER: 0.2088
2026-01-05 14:09:37,461: t15.2023.10.08 val PER: 0.3681
2026-01-05 14:09:37,461: t15.2023.10.13 val PER: 0.3592
2026-01-05 14:09:37,461: t15.2023.10.15 val PER: 0.2966
2026-01-05 14:09:37,461: t15.2023.10.20 val PER: 0.2617
2026-01-05 14:09:37,461: t15.2023.10.22 val PER: 0.2339
2026-01-05 14:09:37,461: t15.2023.11.03 val PER: 0.2972
2026-01-05 14:09:37,462: t15.2023.11.04 val PER: 0.0717
2026-01-05 14:09:37,462: t15.2023.11.17 val PER: 0.1369
2026-01-05 14:09:37,462: t15.2023.11.19 val PER: 0.1158
2026-01-05 14:09:37,462: t15.2023.11.26 val PER: 0.3457
2026-01-05 14:09:37,462: t15.2023.12.03 val PER: 0.2815
2026-01-05 14:09:37,462: t15.2023.12.08 val PER: 0.2743
2026-01-05 14:09:37,462: t15.2023.12.10 val PER: 0.2392
2026-01-05 14:09:37,462: t15.2023.12.17 val PER: 0.2838
2026-01-05 14:09:37,462: t15.2023.12.29 val PER: 0.2999
2026-01-05 14:09:37,462: t15.2024.02.25 val PER: 0.2416
2026-01-05 14:09:37,463: t15.2024.03.08 val PER: 0.3613
2026-01-05 14:09:37,463: t15.2024.03.15 val PER: 0.3527
2026-01-05 14:09:37,463: t15.2024.03.17 val PER: 0.3166
2026-01-05 14:09:37,463: t15.2024.05.10 val PER: 0.3210
2026-01-05 14:09:37,463: t15.2024.06.14 val PER: 0.3233
2026-01-05 14:09:37,463: t15.2024.07.19 val PER: 0.4483
2026-01-05 14:09:37,463: t15.2024.07.21 val PER: 0.2676
2026-01-05 14:09:37,463: t15.2024.07.28 val PER: 0.3022
2026-01-05 14:09:37,463: t15.2025.01.10 val PER: 0.5014
2026-01-05 14:09:37,463: t15.2025.01.12 val PER: 0.3649
2026-01-05 14:09:37,463: t15.2025.03.14 val PER: 0.4911
2026-01-05 14:09:37,463: t15.2025.03.16 val PER: 0.3809
2026-01-05 14:09:37,463: t15.2025.03.30 val PER: 0.5161
2026-01-05 14:09:37,463: t15.2025.04.13 val PER: 0.3951
2026-01-05 14:09:37,464: New best val WER(1gram) 68.02% --> 67.77%
2026-01-05 14:09:37,465: Checkpointing model
2026-01-05 14:09:38,139: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:09:38,437: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_2500
2026-01-05 14:09:47,767: Train batch 2600: loss: 34.59 grad norm: 89.50 time: 0.055
2026-01-05 14:10:06,741: Train batch 2800: loss: 25.79 grad norm: 71.04 time: 0.082
2026-01-05 14:10:25,692: Train batch 3000: loss: 29.99 grad norm: 68.18 time: 0.082
2026-01-05 14:10:25,693: Running test after training batch: 3000
2026-01-05 14:10:25,809: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:10:30,947: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the good at this point is will
2026-01-05 14:10:30,979: WER debug example
  GT : how does it keep the cost down
  PR : houde des it epp the cost get
2026-01-05 14:10:32,714: Val batch 3000: PER (avg): 0.2792 CTC Loss (avg): 27.7468 WER(1gram): 64.72% (n=64) time: 7.021
2026-01-05 14:10:32,715: WER lens: avg_true_words=6.16 avg_pred_words=5.83 max_pred_words=11
2026-01-05 14:10:32,715: t15.2023.08.13 val PER: 0.2609
2026-01-05 14:10:32,715: t15.2023.08.18 val PER: 0.2213
2026-01-05 14:10:32,715: t15.2023.08.20 val PER: 0.2049
2026-01-05 14:10:32,715: t15.2023.08.25 val PER: 0.1958
2026-01-05 14:10:32,715: t15.2023.08.27 val PER: 0.3071
2026-01-05 14:10:32,716: t15.2023.09.01 val PER: 0.1875
2026-01-05 14:10:32,716: t15.2023.09.03 val PER: 0.2743
2026-01-05 14:10:32,716: t15.2023.09.24 val PER: 0.2172
2026-01-05 14:10:32,716: t15.2023.09.29 val PER: 0.2285
2026-01-05 14:10:32,716: t15.2023.10.01 val PER: 0.2919
2026-01-05 14:10:32,716: t15.2023.10.06 val PER: 0.1959
2026-01-05 14:10:32,716: t15.2023.10.08 val PER: 0.3532
2026-01-05 14:10:32,716: t15.2023.10.13 val PER: 0.3336
2026-01-05 14:10:32,717: t15.2023.10.15 val PER: 0.2604
2026-01-05 14:10:32,717: t15.2023.10.20 val PER: 0.2315
2026-01-05 14:10:32,717: t15.2023.10.22 val PER: 0.2138
2026-01-05 14:10:32,717: t15.2023.11.03 val PER: 0.2795
2026-01-05 14:10:32,717: t15.2023.11.04 val PER: 0.0853
2026-01-05 14:10:32,717: t15.2023.11.17 val PER: 0.1446
2026-01-05 14:10:32,717: t15.2023.11.19 val PER: 0.1198
2026-01-05 14:10:32,717: t15.2023.11.26 val PER: 0.2986
2026-01-05 14:10:32,717: t15.2023.12.03 val PER: 0.2647
2026-01-05 14:10:32,717: t15.2023.12.08 val PER: 0.2557
2026-01-05 14:10:32,717: t15.2023.12.10 val PER: 0.2089
2026-01-05 14:10:32,718: t15.2023.12.17 val PER: 0.2682
2026-01-05 14:10:32,718: t15.2023.12.29 val PER: 0.2773
2026-01-05 14:10:32,718: t15.2024.02.25 val PER: 0.2289
2026-01-05 14:10:32,718: t15.2024.03.08 val PER: 0.3514
2026-01-05 14:10:32,718: t15.2024.03.15 val PER: 0.3258
2026-01-05 14:10:32,718: t15.2024.03.17 val PER: 0.2887
2026-01-05 14:10:32,718: t15.2024.05.10 val PER: 0.3031
2026-01-05 14:10:32,718: t15.2024.06.14 val PER: 0.2871
2026-01-05 14:10:32,718: t15.2024.07.19 val PER: 0.4028
2026-01-05 14:10:32,718: t15.2024.07.21 val PER: 0.2241
2026-01-05 14:10:32,719: t15.2024.07.28 val PER: 0.2735
2026-01-05 14:10:32,719: t15.2025.01.10 val PER: 0.4876
2026-01-05 14:10:32,719: t15.2025.01.12 val PER: 0.3172
2026-01-05 14:10:32,719: t15.2025.03.14 val PER: 0.4541
2026-01-05 14:10:32,719: t15.2025.03.16 val PER: 0.3285
2026-01-05 14:10:32,719: t15.2025.03.30 val PER: 0.4931
2026-01-05 14:10:32,719: t15.2025.04.13 val PER: 0.3438
2026-01-05 14:10:32,719: New best val WER(1gram) 67.77% --> 64.72%
2026-01-05 14:10:32,719: Checkpointing model
2026-01-05 14:10:33,373: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:10:33,678: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_3000
2026-01-05 14:10:52,516: Train batch 3200: loss: 25.66 grad norm: 67.50 time: 0.075
2026-01-05 14:11:11,403: Train batch 3400: loss: 17.89 grad norm: 55.71 time: 0.049
2026-01-05 14:11:21,060: Running test after training batch: 3500
2026-01-05 14:11:21,166: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:11:26,483: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-05 14:11:26,513: WER debug example
  GT : how does it keep the cost down
  PR : houde des it kipp thus us get
2026-01-05 14:11:28,133: Val batch 3500: PER (avg): 0.2663 CTC Loss (avg): 26.1162 WER(1gram): 67.26% (n=64) time: 7.073
2026-01-05 14:11:28,133: WER lens: avg_true_words=6.16 avg_pred_words=5.95 max_pred_words=11
2026-01-05 14:11:28,134: t15.2023.08.13 val PER: 0.2349
2026-01-05 14:11:28,134: t15.2023.08.18 val PER: 0.2121
2026-01-05 14:11:28,134: t15.2023.08.20 val PER: 0.2129
2026-01-05 14:11:28,134: t15.2023.08.25 val PER: 0.1777
2026-01-05 14:11:28,134: t15.2023.08.27 val PER: 0.2749
2026-01-05 14:11:28,135: t15.2023.09.01 val PER: 0.1680
2026-01-05 14:11:28,135: t15.2023.09.03 val PER: 0.2494
2026-01-05 14:11:28,135: t15.2023.09.24 val PER: 0.2075
2026-01-05 14:11:28,135: t15.2023.09.29 val PER: 0.2176
2026-01-05 14:11:28,135: t15.2023.10.01 val PER: 0.2741
2026-01-05 14:11:28,135: t15.2023.10.06 val PER: 0.1808
2026-01-05 14:11:28,135: t15.2023.10.08 val PER: 0.3383
2026-01-05 14:11:28,135: t15.2023.10.13 val PER: 0.3220
2026-01-05 14:11:28,135: t15.2023.10.15 val PER: 0.2459
2026-01-05 14:11:28,136: t15.2023.10.20 val PER: 0.2349
2026-01-05 14:11:28,136: t15.2023.10.22 val PER: 0.1960
2026-01-05 14:11:28,136: t15.2023.11.03 val PER: 0.2598
2026-01-05 14:11:28,136: t15.2023.11.04 val PER: 0.0751
2026-01-05 14:11:28,136: t15.2023.11.17 val PER: 0.1198
2026-01-05 14:11:28,136: t15.2023.11.19 val PER: 0.1038
2026-01-05 14:11:28,136: t15.2023.11.26 val PER: 0.2841
2026-01-05 14:11:28,136: t15.2023.12.03 val PER: 0.2237
2026-01-05 14:11:28,136: t15.2023.12.08 val PER: 0.2463
2026-01-05 14:11:28,136: t15.2023.12.10 val PER: 0.1919
2026-01-05 14:11:28,137: t15.2023.12.17 val PER: 0.2609
2026-01-05 14:11:28,137: t15.2023.12.29 val PER: 0.2622
2026-01-05 14:11:28,137: t15.2024.02.25 val PER: 0.2079
2026-01-05 14:11:28,137: t15.2024.03.08 val PER: 0.3414
2026-01-05 14:11:28,137: t15.2024.03.15 val PER: 0.3152
2026-01-05 14:11:28,137: t15.2024.03.17 val PER: 0.2803
2026-01-05 14:11:28,137: t15.2024.05.10 val PER: 0.2689
2026-01-05 14:11:28,137: t15.2024.06.14 val PER: 0.2823
2026-01-05 14:11:28,138: t15.2024.07.19 val PER: 0.4054
2026-01-05 14:11:28,138: t15.2024.07.21 val PER: 0.2166
2026-01-05 14:11:28,138: t15.2024.07.28 val PER: 0.2735
2026-01-05 14:11:28,138: t15.2025.01.10 val PER: 0.4628
2026-01-05 14:11:28,138: t15.2025.01.12 val PER: 0.3087
2026-01-05 14:11:28,138: t15.2025.03.14 val PER: 0.4438
2026-01-05 14:11:28,138: t15.2025.03.16 val PER: 0.3364
2026-01-05 14:11:28,138: t15.2025.03.30 val PER: 0.4655
2026-01-05 14:11:28,138: t15.2025.04.13 val PER: 0.3524
2026-01-05 14:11:28,432: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_3500
2026-01-05 14:11:37,913: Train batch 3600: loss: 22.37 grad norm: 62.76 time: 0.067
2026-01-05 14:11:56,761: Train batch 3800: loss: 25.11 grad norm: 68.32 time: 0.067
2026-01-05 14:12:16,455: Train batch 4000: loss: 19.07 grad norm: 54.06 time: 0.056
2026-01-05 14:12:16,455: Running test after training batch: 4000
2026-01-05 14:12:16,624: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:12:21,911: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is while
2026-01-05 14:12:21,941: WER debug example
  GT : how does it keep the cost down
  PR : hout des it kipp thus cussed it
2026-01-05 14:12:23,601: Val batch 4000: PER (avg): 0.2475 CTC Loss (avg): 24.0884 WER(1gram): 66.24% (n=64) time: 7.145
2026-01-05 14:12:23,601: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=11
2026-01-05 14:12:23,601: t15.2023.08.13 val PER: 0.2266
2026-01-05 14:12:23,601: t15.2023.08.18 val PER: 0.2062
2026-01-05 14:12:23,601: t15.2023.08.20 val PER: 0.1970
2026-01-05 14:12:23,602: t15.2023.08.25 val PER: 0.1536
2026-01-05 14:12:23,602: t15.2023.08.27 val PER: 0.2878
2026-01-05 14:12:23,602: t15.2023.09.01 val PER: 0.1558
2026-01-05 14:12:23,602: t15.2023.09.03 val PER: 0.2399
2026-01-05 14:12:23,602: t15.2023.09.24 val PER: 0.1930
2026-01-05 14:12:23,602: t15.2023.09.29 val PER: 0.1978
2026-01-05 14:12:23,602: t15.2023.10.01 val PER: 0.2589
2026-01-05 14:12:23,602: t15.2023.10.06 val PER: 0.1647
2026-01-05 14:12:23,602: t15.2023.10.08 val PER: 0.3207
2026-01-05 14:12:23,602: t15.2023.10.13 val PER: 0.3010
2026-01-05 14:12:23,602: t15.2023.10.15 val PER: 0.2334
2026-01-05 14:12:23,602: t15.2023.10.20 val PER: 0.2383
2026-01-05 14:12:23,602: t15.2023.10.22 val PER: 0.1938
2026-01-05 14:12:23,602: t15.2023.11.03 val PER: 0.2374
2026-01-05 14:12:23,603: t15.2023.11.04 val PER: 0.0648
2026-01-05 14:12:23,603: t15.2023.11.17 val PER: 0.0980
2026-01-05 14:12:23,603: t15.2023.11.19 val PER: 0.0938
2026-01-05 14:12:23,603: t15.2023.11.26 val PER: 0.2630
2026-01-05 14:12:23,603: t15.2023.12.03 val PER: 0.2143
2026-01-05 14:12:23,603: t15.2023.12.08 val PER: 0.2210
2026-01-05 14:12:23,603: t15.2023.12.10 val PER: 0.1813
2026-01-05 14:12:23,603: t15.2023.12.17 val PER: 0.2453
2026-01-05 14:12:23,603: t15.2023.12.29 val PER: 0.2553
2026-01-05 14:12:23,603: t15.2024.02.25 val PER: 0.2135
2026-01-05 14:12:23,603: t15.2024.03.08 val PER: 0.3215
2026-01-05 14:12:23,603: t15.2024.03.15 val PER: 0.3008
2026-01-05 14:12:23,604: t15.2024.03.17 val PER: 0.2427
2026-01-05 14:12:23,604: t15.2024.05.10 val PER: 0.2689
2026-01-05 14:12:23,604: t15.2024.06.14 val PER: 0.2792
2026-01-05 14:12:23,604: t15.2024.07.19 val PER: 0.3612
2026-01-05 14:12:23,604: t15.2024.07.21 val PER: 0.1814
2026-01-05 14:12:23,604: t15.2024.07.28 val PER: 0.2419
2026-01-05 14:12:23,604: t15.2025.01.10 val PER: 0.4408
2026-01-05 14:12:23,604: t15.2025.01.12 val PER: 0.2871
2026-01-05 14:12:23,604: t15.2025.03.14 val PER: 0.4038
2026-01-05 14:12:23,604: t15.2025.03.16 val PER: 0.3154
2026-01-05 14:12:23,604: t15.2025.03.30 val PER: 0.4115
2026-01-05 14:12:23,604: t15.2025.04.13 val PER: 0.3238
2026-01-05 14:12:23,901: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_4000
2026-01-05 14:12:42,852: Train batch 4200: loss: 22.07 grad norm: 66.89 time: 0.079
2026-01-05 14:13:01,871: Train batch 4400: loss: 16.37 grad norm: 55.17 time: 0.066
2026-01-05 14:13:11,398: Running test after training batch: 4500
2026-01-05 14:13:11,550: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:13:16,659: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-05 14:13:16,691: WER debug example
  GT : how does it keep the cost down
  PR : houde just it heap thus cost et
2026-01-05 14:13:18,423: Val batch 4500: PER (avg): 0.2365 CTC Loss (avg): 23.1914 WER(1gram): 61.17% (n=64) time: 7.024
2026-01-05 14:13:18,423: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-05 14:13:18,423: t15.2023.08.13 val PER: 0.1985
2026-01-05 14:13:18,424: t15.2023.08.18 val PER: 0.1970
2026-01-05 14:13:18,424: t15.2023.08.20 val PER: 0.1882
2026-01-05 14:13:18,424: t15.2023.08.25 val PER: 0.1386
2026-01-05 14:13:18,424: t15.2023.08.27 val PER: 0.2637
2026-01-05 14:13:18,424: t15.2023.09.01 val PER: 0.1607
2026-01-05 14:13:18,424: t15.2023.09.03 val PER: 0.2375
2026-01-05 14:13:18,424: t15.2023.09.24 val PER: 0.1930
2026-01-05 14:13:18,424: t15.2023.09.29 val PER: 0.1997
2026-01-05 14:13:18,424: t15.2023.10.01 val PER: 0.2530
2026-01-05 14:13:18,424: t15.2023.10.06 val PER: 0.1539
2026-01-05 14:13:18,424: t15.2023.10.08 val PER: 0.3045
2026-01-05 14:13:18,424: t15.2023.10.13 val PER: 0.3010
2026-01-05 14:13:18,425: t15.2023.10.15 val PER: 0.2367
2026-01-05 14:13:18,425: t15.2023.10.20 val PER: 0.2450
2026-01-05 14:13:18,425: t15.2023.10.22 val PER: 0.1849
2026-01-05 14:13:18,425: t15.2023.11.03 val PER: 0.2436
2026-01-05 14:13:18,425: t15.2023.11.04 val PER: 0.0751
2026-01-05 14:13:18,425: t15.2023.11.17 val PER: 0.0949
2026-01-05 14:13:18,425: t15.2023.11.19 val PER: 0.0898
2026-01-05 14:13:18,425: t15.2023.11.26 val PER: 0.2551
2026-01-05 14:13:18,425: t15.2023.12.03 val PER: 0.1975
2026-01-05 14:13:18,426: t15.2023.12.08 val PER: 0.2084
2026-01-05 14:13:18,426: t15.2023.12.10 val PER: 0.1735
2026-01-05 14:13:18,426: t15.2023.12.17 val PER: 0.2370
2026-01-05 14:13:18,426: t15.2023.12.29 val PER: 0.2327
2026-01-05 14:13:18,426: t15.2024.02.25 val PER: 0.2051
2026-01-05 14:13:18,426: t15.2024.03.08 val PER: 0.3101
2026-01-05 14:13:18,426: t15.2024.03.15 val PER: 0.2896
2026-01-05 14:13:18,426: t15.2024.03.17 val PER: 0.2483
2026-01-05 14:13:18,426: t15.2024.05.10 val PER: 0.2422
2026-01-05 14:13:18,427: t15.2024.06.14 val PER: 0.2366
2026-01-05 14:13:18,427: t15.2024.07.19 val PER: 0.3270
2026-01-05 14:13:18,427: t15.2024.07.21 val PER: 0.1724
2026-01-05 14:13:18,427: t15.2024.07.28 val PER: 0.2221
2026-01-05 14:13:18,427: t15.2025.01.10 val PER: 0.4077
2026-01-05 14:13:18,427: t15.2025.01.12 val PER: 0.2571
2026-01-05 14:13:18,427: t15.2025.03.14 val PER: 0.3935
2026-01-05 14:13:18,427: t15.2025.03.16 val PER: 0.2814
2026-01-05 14:13:18,428: t15.2025.03.30 val PER: 0.4092
2026-01-05 14:13:18,428: t15.2025.04.13 val PER: 0.3053
2026-01-05 14:13:18,428: New best val WER(1gram) 64.72% --> 61.17%
2026-01-05 14:13:18,428: Checkpointing model
2026-01-05 14:13:19,092: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:13:19,399: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_4500
2026-01-05 14:13:28,892: Train batch 4600: loss: 19.55 grad norm: 63.91 time: 0.063
2026-01-05 14:13:48,007: Train batch 4800: loss: 13.64 grad norm: 53.04 time: 0.063
2026-01-05 14:14:07,054: Train batch 5000: loss: 31.62 grad norm: 85.68 time: 0.064
2026-01-05 14:14:07,055: Running test after training batch: 5000
2026-01-05 14:14:07,200: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:14:12,294: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-05 14:14:12,326: WER debug example
  GT : how does it keep the cost down
  PR : houde dest it heap the cost nit
2026-01-05 14:14:14,025: Val batch 5000: PER (avg): 0.2240 CTC Loss (avg): 21.9958 WER(1gram): 61.93% (n=64) time: 6.970
2026-01-05 14:14:14,026: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 14:14:14,026: t15.2023.08.13 val PER: 0.2058
2026-01-05 14:14:14,026: t15.2023.08.18 val PER: 0.1685
2026-01-05 14:14:14,026: t15.2023.08.20 val PER: 0.1668
2026-01-05 14:14:14,026: t15.2023.08.25 val PER: 0.1310
2026-01-05 14:14:14,026: t15.2023.08.27 val PER: 0.2395
2026-01-05 14:14:14,026: t15.2023.09.01 val PER: 0.1347
2026-01-05 14:14:14,026: t15.2023.09.03 val PER: 0.2221
2026-01-05 14:14:14,026: t15.2023.09.24 val PER: 0.1833
2026-01-05 14:14:14,026: t15.2023.09.29 val PER: 0.1838
2026-01-05 14:14:14,026: t15.2023.10.01 val PER: 0.2398
2026-01-05 14:14:14,026: t15.2023.10.06 val PER: 0.1539
2026-01-05 14:14:14,026: t15.2023.10.08 val PER: 0.3045
2026-01-05 14:14:14,026: t15.2023.10.13 val PER: 0.2793
2026-01-05 14:14:14,027: t15.2023.10.15 val PER: 0.2261
2026-01-05 14:14:14,027: t15.2023.10.20 val PER: 0.2248
2026-01-05 14:14:14,027: t15.2023.10.22 val PER: 0.1726
2026-01-05 14:14:14,027: t15.2023.11.03 val PER: 0.2205
2026-01-05 14:14:14,027: t15.2023.11.04 val PER: 0.0512
2026-01-05 14:14:14,027: t15.2023.11.17 val PER: 0.0855
2026-01-05 14:14:14,027: t15.2023.11.19 val PER: 0.0898
2026-01-05 14:14:14,027: t15.2023.11.26 val PER: 0.2413
2026-01-05 14:14:14,027: t15.2023.12.03 val PER: 0.1996
2026-01-05 14:14:14,027: t15.2023.12.08 val PER: 0.1951
2026-01-05 14:14:14,028: t15.2023.12.10 val PER: 0.1590
2026-01-05 14:14:14,028: t15.2023.12.17 val PER: 0.2173
2026-01-05 14:14:14,028: t15.2023.12.29 val PER: 0.2196
2026-01-05 14:14:14,028: t15.2024.02.25 val PER: 0.1840
2026-01-05 14:14:14,028: t15.2024.03.08 val PER: 0.3044
2026-01-05 14:14:14,028: t15.2024.03.15 val PER: 0.2777
2026-01-05 14:14:14,028: t15.2024.03.17 val PER: 0.2336
2026-01-05 14:14:14,028: t15.2024.05.10 val PER: 0.2348
2026-01-05 14:14:14,028: t15.2024.06.14 val PER: 0.2476
2026-01-05 14:14:14,028: t15.2024.07.19 val PER: 0.3362
2026-01-05 14:14:14,028: t15.2024.07.21 val PER: 0.1752
2026-01-05 14:14:14,028: t15.2024.07.28 val PER: 0.2059
2026-01-05 14:14:14,028: t15.2025.01.10 val PER: 0.3926
2026-01-05 14:14:14,028: t15.2025.01.12 val PER: 0.2356
2026-01-05 14:14:14,028: t15.2025.03.14 val PER: 0.3861
2026-01-05 14:14:14,029: t15.2025.03.16 val PER: 0.2592
2026-01-05 14:14:14,029: t15.2025.03.30 val PER: 0.3839
2026-01-05 14:14:14,029: t15.2025.04.13 val PER: 0.2953
2026-01-05 14:14:14,321: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_5000
2026-01-05 14:14:33,117: Train batch 5200: loss: 16.42 grad norm: 64.88 time: 0.052
2026-01-05 14:14:52,058: Train batch 5400: loss: 16.90 grad norm: 56.45 time: 0.068
2026-01-05 14:15:01,488: Running test after training batch: 5500
2026-01-05 14:15:01,676: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:15:06,817: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-05 14:15:06,847: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-05 14:15:08,470: Val batch 5500: PER (avg): 0.2160 CTC Loss (avg): 20.9326 WER(1gram): 56.85% (n=64) time: 6.981
2026-01-05 14:15:08,470: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-05 14:15:08,470: t15.2023.08.13 val PER: 0.1767
2026-01-05 14:15:08,470: t15.2023.08.18 val PER: 0.1609
2026-01-05 14:15:08,471: t15.2023.08.20 val PER: 0.1716
2026-01-05 14:15:08,471: t15.2023.08.25 val PER: 0.1250
2026-01-05 14:15:08,471: t15.2023.08.27 val PER: 0.2460
2026-01-05 14:15:08,471: t15.2023.09.01 val PER: 0.1315
2026-01-05 14:15:08,471: t15.2023.09.03 val PER: 0.2268
2026-01-05 14:15:08,471: t15.2023.09.24 val PER: 0.1820
2026-01-05 14:15:08,471: t15.2023.09.29 val PER: 0.1793
2026-01-05 14:15:08,471: t15.2023.10.01 val PER: 0.2299
2026-01-05 14:15:08,472: t15.2023.10.06 val PER: 0.1206
2026-01-05 14:15:08,472: t15.2023.10.08 val PER: 0.2950
2026-01-05 14:15:08,472: t15.2023.10.13 val PER: 0.2801
2026-01-05 14:15:08,472: t15.2023.10.15 val PER: 0.2116
2026-01-05 14:15:08,472: t15.2023.10.20 val PER: 0.2315
2026-01-05 14:15:08,472: t15.2023.10.22 val PER: 0.1693
2026-01-05 14:15:08,472: t15.2023.11.03 val PER: 0.2320
2026-01-05 14:15:08,472: t15.2023.11.04 val PER: 0.0614
2026-01-05 14:15:08,472: t15.2023.11.17 val PER: 0.0793
2026-01-05 14:15:08,473: t15.2023.11.19 val PER: 0.0878
2026-01-05 14:15:08,473: t15.2023.11.26 val PER: 0.2261
2026-01-05 14:15:08,473: t15.2023.12.03 val PER: 0.1807
2026-01-05 14:15:08,473: t15.2023.12.08 val PER: 0.1838
2026-01-05 14:15:08,473: t15.2023.12.10 val PER: 0.1459
2026-01-05 14:15:08,473: t15.2023.12.17 val PER: 0.2204
2026-01-05 14:15:08,473: t15.2023.12.29 val PER: 0.2128
2026-01-05 14:15:08,473: t15.2024.02.25 val PER: 0.1784
2026-01-05 14:15:08,473: t15.2024.03.08 val PER: 0.2845
2026-01-05 14:15:08,473: t15.2024.03.15 val PER: 0.2570
2026-01-05 14:15:08,473: t15.2024.03.17 val PER: 0.2225
2026-01-05 14:15:08,473: t15.2024.05.10 val PER: 0.2377
2026-01-05 14:15:08,474: t15.2024.06.14 val PER: 0.2413
2026-01-05 14:15:08,474: t15.2024.07.19 val PER: 0.3131
2026-01-05 14:15:08,474: t15.2024.07.21 val PER: 0.1572
2026-01-05 14:15:08,474: t15.2024.07.28 val PER: 0.2066
2026-01-05 14:15:08,474: t15.2025.01.10 val PER: 0.3843
2026-01-05 14:15:08,474: t15.2025.01.12 val PER: 0.2448
2026-01-05 14:15:08,474: t15.2025.03.14 val PER: 0.3609
2026-01-05 14:15:08,474: t15.2025.03.16 val PER: 0.2448
2026-01-05 14:15:08,474: t15.2025.03.30 val PER: 0.3690
2026-01-05 14:15:08,474: t15.2025.04.13 val PER: 0.2981
2026-01-05 14:15:08,475: New best val WER(1gram) 61.17% --> 56.85%
2026-01-05 14:15:08,475: Checkpointing model
2026-01-05 14:15:09,141: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:15:09,438: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_5500
2026-01-05 14:15:18,959: Train batch 5600: loss: 18.81 grad norm: 65.85 time: 0.063
2026-01-05 14:15:38,112: Train batch 5800: loss: 13.43 grad norm: 57.15 time: 0.083
2026-01-05 14:15:57,029: Train batch 6000: loss: 14.15 grad norm: 60.58 time: 0.049
2026-01-05 14:15:57,029: Running test after training batch: 6000
2026-01-05 14:15:57,148: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:16:02,308: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-05 14:16:02,341: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost nit
2026-01-05 14:16:04,053: Val batch 6000: PER (avg): 0.2107 CTC Loss (avg): 20.5866 WER(1gram): 60.15% (n=64) time: 7.024
2026-01-05 14:16:04,053: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 14:16:04,053: t15.2023.08.13 val PER: 0.1850
2026-01-05 14:16:04,054: t15.2023.08.18 val PER: 0.1635
2026-01-05 14:16:04,054: t15.2023.08.20 val PER: 0.1620
2026-01-05 14:16:04,054: t15.2023.08.25 val PER: 0.1130
2026-01-05 14:16:04,054: t15.2023.08.27 val PER: 0.2476
2026-01-05 14:16:04,054: t15.2023.09.01 val PER: 0.1242
2026-01-05 14:16:04,054: t15.2023.09.03 val PER: 0.2138
2026-01-05 14:16:04,054: t15.2023.09.24 val PER: 0.1638
2026-01-05 14:16:04,054: t15.2023.09.29 val PER: 0.1685
2026-01-05 14:16:04,054: t15.2023.10.01 val PER: 0.2266
2026-01-05 14:16:04,054: t15.2023.10.06 val PER: 0.1313
2026-01-05 14:16:04,054: t15.2023.10.08 val PER: 0.2936
2026-01-05 14:16:04,054: t15.2023.10.13 val PER: 0.2630
2026-01-05 14:16:04,054: t15.2023.10.15 val PER: 0.2109
2026-01-05 14:16:04,055: t15.2023.10.20 val PER: 0.2181
2026-01-05 14:16:04,055: t15.2023.10.22 val PER: 0.1670
2026-01-05 14:16:04,055: t15.2023.11.03 val PER: 0.2252
2026-01-05 14:16:04,055: t15.2023.11.04 val PER: 0.0512
2026-01-05 14:16:04,055: t15.2023.11.17 val PER: 0.0731
2026-01-05 14:16:04,055: t15.2023.11.19 val PER: 0.0858
2026-01-05 14:16:04,055: t15.2023.11.26 val PER: 0.2196
2026-01-05 14:16:04,055: t15.2023.12.03 val PER: 0.1649
2026-01-05 14:16:04,055: t15.2023.12.08 val PER: 0.1658
2026-01-05 14:16:04,056: t15.2023.12.10 val PER: 0.1419
2026-01-05 14:16:04,056: t15.2023.12.17 val PER: 0.1985
2026-01-05 14:16:04,056: t15.2023.12.29 val PER: 0.2217
2026-01-05 14:16:04,056: t15.2024.02.25 val PER: 0.1643
2026-01-05 14:16:04,056: t15.2024.03.08 val PER: 0.2945
2026-01-05 14:16:04,056: t15.2024.03.15 val PER: 0.2758
2026-01-05 14:16:04,056: t15.2024.03.17 val PER: 0.2106
2026-01-05 14:16:04,056: t15.2024.05.10 val PER: 0.2125
2026-01-05 14:16:04,056: t15.2024.06.14 val PER: 0.2303
2026-01-05 14:16:04,057: t15.2024.07.19 val PER: 0.2947
2026-01-05 14:16:04,057: t15.2024.07.21 val PER: 0.1566
2026-01-05 14:16:04,057: t15.2024.07.28 val PER: 0.2007
2026-01-05 14:16:04,057: t15.2025.01.10 val PER: 0.3788
2026-01-05 14:16:04,057: t15.2025.01.12 val PER: 0.2256
2026-01-05 14:16:04,057: t15.2025.03.14 val PER: 0.3846
2026-01-05 14:16:04,057: t15.2025.03.16 val PER: 0.2670
2026-01-05 14:16:04,057: t15.2025.03.30 val PER: 0.3828
2026-01-05 14:16:04,057: t15.2025.04.13 val PER: 0.2725
2026-01-05 14:16:04,361: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_6000
2026-01-05 14:16:23,161: Train batch 6200: loss: 15.72 grad norm: 57.26 time: 0.070
2026-01-05 14:16:41,857: Train batch 6400: loss: 18.39 grad norm: 65.00 time: 0.062
2026-01-05 14:16:51,042: Running test after training batch: 6500
2026-01-05 14:16:51,199: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:16:56,438: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned e the could at this point is will
2026-01-05 14:16:56,470: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the quast et
2026-01-05 14:16:58,196: Val batch 6500: PER (avg): 0.2032 CTC Loss (avg): 20.0516 WER(1gram): 54.31% (n=64) time: 7.153
2026-01-05 14:16:58,196: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 14:16:58,196: t15.2023.08.13 val PER: 0.1705
2026-01-05 14:16:58,196: t15.2023.08.18 val PER: 0.1459
2026-01-05 14:16:58,196: t15.2023.08.20 val PER: 0.1604
2026-01-05 14:16:58,196: t15.2023.08.25 val PER: 0.1099
2026-01-05 14:16:58,196: t15.2023.08.27 val PER: 0.2331
2026-01-05 14:16:58,197: t15.2023.09.01 val PER: 0.1185
2026-01-05 14:16:58,197: t15.2023.09.03 val PER: 0.1948
2026-01-05 14:16:58,197: t15.2023.09.24 val PER: 0.1602
2026-01-05 14:16:58,197: t15.2023.09.29 val PER: 0.1704
2026-01-05 14:16:58,197: t15.2023.10.01 val PER: 0.2140
2026-01-05 14:16:58,197: t15.2023.10.06 val PER: 0.1302
2026-01-05 14:16:58,197: t15.2023.10.08 val PER: 0.2896
2026-01-05 14:16:58,197: t15.2023.10.13 val PER: 0.2630
2026-01-05 14:16:58,198: t15.2023.10.15 val PER: 0.2149
2026-01-05 14:16:58,198: t15.2023.10.20 val PER: 0.2047
2026-01-05 14:16:58,198: t15.2023.10.22 val PER: 0.1548
2026-01-05 14:16:58,198: t15.2023.11.03 val PER: 0.2191
2026-01-05 14:16:58,198: t15.2023.11.04 val PER: 0.0444
2026-01-05 14:16:58,198: t15.2023.11.17 val PER: 0.0638
2026-01-05 14:16:58,198: t15.2023.11.19 val PER: 0.0699
2026-01-05 14:16:58,198: t15.2023.11.26 val PER: 0.2094
2026-01-05 14:16:58,198: t15.2023.12.03 val PER: 0.1754
2026-01-05 14:16:58,198: t15.2023.12.08 val PER: 0.1744
2026-01-05 14:16:58,198: t15.2023.12.10 val PER: 0.1419
2026-01-05 14:16:58,198: t15.2023.12.17 val PER: 0.1954
2026-01-05 14:16:58,198: t15.2023.12.29 val PER: 0.2052
2026-01-05 14:16:58,198: t15.2024.02.25 val PER: 0.1685
2026-01-05 14:16:58,199: t15.2024.03.08 val PER: 0.2731
2026-01-05 14:16:58,199: t15.2024.03.15 val PER: 0.2577
2026-01-05 14:16:58,199: t15.2024.03.17 val PER: 0.2085
2026-01-05 14:16:58,199: t15.2024.05.10 val PER: 0.2244
2026-01-05 14:16:58,199: t15.2024.06.14 val PER: 0.2114
2026-01-05 14:16:58,199: t15.2024.07.19 val PER: 0.3039
2026-01-05 14:16:58,199: t15.2024.07.21 val PER: 0.1517
2026-01-05 14:16:58,199: t15.2024.07.28 val PER: 0.1846
2026-01-05 14:16:58,199: t15.2025.01.10 val PER: 0.3526
2026-01-05 14:16:58,200: t15.2025.01.12 val PER: 0.2071
2026-01-05 14:16:58,200: t15.2025.03.14 val PER: 0.3743
2026-01-05 14:16:58,200: t15.2025.03.16 val PER: 0.2408
2026-01-05 14:16:58,200: t15.2025.03.30 val PER: 0.3494
2026-01-05 14:16:58,200: t15.2025.04.13 val PER: 0.2710
2026-01-05 14:16:58,201: New best val WER(1gram) 56.85% --> 54.31%
2026-01-05 14:16:58,201: Checkpointing model
2026-01-05 14:16:58,865: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:16:59,181: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_6500
2026-01-05 14:17:08,484: Train batch 6600: loss: 11.61 grad norm: 50.21 time: 0.045
2026-01-05 14:17:27,322: Train batch 6800: loss: 14.83 grad norm: 54.55 time: 0.049
2026-01-05 14:17:46,312: Train batch 7000: loss: 16.78 grad norm: 63.19 time: 0.061
2026-01-05 14:17:46,312: Running test after training batch: 7000
2026-01-05 14:17:46,503: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:17:51,667: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:17:51,699: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-05 14:17:53,483: Val batch 7000: PER (avg): 0.1960 CTC Loss (avg): 19.1759 WER(1gram): 52.79% (n=64) time: 7.170
2026-01-05 14:17:53,483: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-05 14:17:53,483: t15.2023.08.13 val PER: 0.1559
2026-01-05 14:17:53,483: t15.2023.08.18 val PER: 0.1450
2026-01-05 14:17:53,483: t15.2023.08.20 val PER: 0.1541
2026-01-05 14:17:53,484: t15.2023.08.25 val PER: 0.1069
2026-01-05 14:17:53,484: t15.2023.08.27 val PER: 0.2106
2026-01-05 14:17:53,484: t15.2023.09.01 val PER: 0.1169
2026-01-05 14:17:53,484: t15.2023.09.03 val PER: 0.1865
2026-01-05 14:17:53,484: t15.2023.09.24 val PER: 0.1553
2026-01-05 14:17:53,484: t15.2023.09.29 val PER: 0.1666
2026-01-05 14:17:53,484: t15.2023.10.01 val PER: 0.2127
2026-01-05 14:17:53,484: t15.2023.10.06 val PER: 0.1109
2026-01-05 14:17:53,484: t15.2023.10.08 val PER: 0.2828
2026-01-05 14:17:53,484: t15.2023.10.13 val PER: 0.2568
2026-01-05 14:17:53,484: t15.2023.10.15 val PER: 0.1951
2026-01-05 14:17:53,484: t15.2023.10.20 val PER: 0.2114
2026-01-05 14:17:53,485: t15.2023.10.22 val PER: 0.1492
2026-01-05 14:17:53,485: t15.2023.11.03 val PER: 0.2123
2026-01-05 14:17:53,485: t15.2023.11.04 val PER: 0.0444
2026-01-05 14:17:53,485: t15.2023.11.17 val PER: 0.0622
2026-01-05 14:17:53,485: t15.2023.11.19 val PER: 0.0519
2026-01-05 14:17:53,485: t15.2023.11.26 val PER: 0.2029
2026-01-05 14:17:53,485: t15.2023.12.03 val PER: 0.1681
2026-01-05 14:17:53,485: t15.2023.12.08 val PER: 0.1578
2026-01-05 14:17:53,485: t15.2023.12.10 val PER: 0.1380
2026-01-05 14:17:53,485: t15.2023.12.17 val PER: 0.1830
2026-01-05 14:17:53,485: t15.2023.12.29 val PER: 0.1949
2026-01-05 14:17:53,485: t15.2024.02.25 val PER: 0.1587
2026-01-05 14:17:53,485: t15.2024.03.08 val PER: 0.2703
2026-01-05 14:17:53,485: t15.2024.03.15 val PER: 0.2489
2026-01-05 14:17:53,485: t15.2024.03.17 val PER: 0.2008
2026-01-05 14:17:53,486: t15.2024.05.10 val PER: 0.2021
2026-01-05 14:17:53,486: t15.2024.06.14 val PER: 0.2082
2026-01-05 14:17:53,486: t15.2024.07.19 val PER: 0.3019
2026-01-05 14:17:53,486: t15.2024.07.21 val PER: 0.1359
2026-01-05 14:17:53,486: t15.2024.07.28 val PER: 0.1713
2026-01-05 14:17:53,486: t15.2025.01.10 val PER: 0.3664
2026-01-05 14:17:53,486: t15.2025.01.12 val PER: 0.2102
2026-01-05 14:17:53,486: t15.2025.03.14 val PER: 0.3654
2026-01-05 14:17:53,486: t15.2025.03.16 val PER: 0.2317
2026-01-05 14:17:53,486: t15.2025.03.30 val PER: 0.3598
2026-01-05 14:17:53,486: t15.2025.04.13 val PER: 0.2696
2026-01-05 14:17:53,488: New best val WER(1gram) 54.31% --> 52.79%
2026-01-05 14:17:53,488: Checkpointing model
2026-01-05 14:17:54,154: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:17:54,466: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_7000
2026-01-05 14:18:13,273: Train batch 7200: loss: 13.78 grad norm: 59.15 time: 0.078
2026-01-05 14:18:31,876: Train batch 7400: loss: 14.04 grad norm: 59.20 time: 0.076
2026-01-05 14:18:41,901: Running test after training batch: 7500
2026-01-05 14:18:42,003: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:18:47,146: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-05 14:18:47,179: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nit
2026-01-05 14:18:48,927: Val batch 7500: PER (avg): 0.1893 CTC Loss (avg): 18.6787 WER(1gram): 56.09% (n=64) time: 7.026
2026-01-05 14:18:48,927: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 14:18:48,928: t15.2023.08.13 val PER: 0.1486
2026-01-05 14:18:48,928: t15.2023.08.18 val PER: 0.1316
2026-01-05 14:18:48,928: t15.2023.08.20 val PER: 0.1422
2026-01-05 14:18:48,928: t15.2023.08.25 val PER: 0.1024
2026-01-05 14:18:48,928: t15.2023.08.27 val PER: 0.2074
2026-01-05 14:18:48,928: t15.2023.09.01 val PER: 0.1063
2026-01-05 14:18:48,928: t15.2023.09.03 val PER: 0.1876
2026-01-05 14:18:48,928: t15.2023.09.24 val PER: 0.1529
2026-01-05 14:18:48,928: t15.2023.09.29 val PER: 0.1634
2026-01-05 14:18:48,928: t15.2023.10.01 val PER: 0.2061
2026-01-05 14:18:48,929: t15.2023.10.06 val PER: 0.1206
2026-01-05 14:18:48,929: t15.2023.10.08 val PER: 0.2882
2026-01-05 14:18:48,929: t15.2023.10.13 val PER: 0.2428
2026-01-05 14:18:48,929: t15.2023.10.15 val PER: 0.1885
2026-01-05 14:18:48,929: t15.2023.10.20 val PER: 0.1946
2026-01-05 14:18:48,929: t15.2023.10.22 val PER: 0.1425
2026-01-05 14:18:48,929: t15.2023.11.03 val PER: 0.2083
2026-01-05 14:18:48,929: t15.2023.11.04 val PER: 0.0546
2026-01-05 14:18:48,929: t15.2023.11.17 val PER: 0.0669
2026-01-05 14:18:48,929: t15.2023.11.19 val PER: 0.0539
2026-01-05 14:18:48,929: t15.2023.11.26 val PER: 0.1812
2026-01-05 14:18:48,929: t15.2023.12.03 val PER: 0.1639
2026-01-05 14:18:48,929: t15.2023.12.08 val PER: 0.1591
2026-01-05 14:18:48,930: t15.2023.12.10 val PER: 0.1367
2026-01-05 14:18:48,930: t15.2023.12.17 val PER: 0.1778
2026-01-05 14:18:48,930: t15.2023.12.29 val PER: 0.1764
2026-01-05 14:18:48,930: t15.2024.02.25 val PER: 0.1447
2026-01-05 14:18:48,930: t15.2024.03.08 val PER: 0.2603
2026-01-05 14:18:48,930: t15.2024.03.15 val PER: 0.2389
2026-01-05 14:18:48,930: t15.2024.03.17 val PER: 0.1904
2026-01-05 14:18:48,930: t15.2024.05.10 val PER: 0.2155
2026-01-05 14:18:48,930: t15.2024.06.14 val PER: 0.2114
2026-01-05 14:18:48,930: t15.2024.07.19 val PER: 0.2927
2026-01-05 14:18:48,930: t15.2024.07.21 val PER: 0.1310
2026-01-05 14:18:48,930: t15.2024.07.28 val PER: 0.1721
2026-01-05 14:18:48,931: t15.2025.01.10 val PER: 0.3430
2026-01-05 14:18:48,931: t15.2025.01.12 val PER: 0.1986
2026-01-05 14:18:48,931: t15.2025.03.14 val PER: 0.3683
2026-01-05 14:18:48,931: t15.2025.03.16 val PER: 0.2356
2026-01-05 14:18:48,931: t15.2025.03.30 val PER: 0.3471
2026-01-05 14:18:48,931: t15.2025.04.13 val PER: 0.2454
2026-01-05 14:18:49,229: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_7500
2026-01-05 14:18:58,615: Train batch 7600: loss: 15.77 grad norm: 60.32 time: 0.069
2026-01-05 14:19:17,639: Train batch 7800: loss: 13.93 grad norm: 58.53 time: 0.056
2026-01-05 14:19:36,855: Train batch 8000: loss: 11.19 grad norm: 53.03 time: 0.072
2026-01-05 14:19:36,855: Running test after training batch: 8000
2026-01-05 14:19:36,964: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:19:42,024: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-05 14:19:42,056: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nit
2026-01-05 14:19:43,775: Val batch 8000: PER (avg): 0.1834 CTC Loss (avg): 18.0336 WER(1gram): 51.78% (n=64) time: 6.920
2026-01-05 14:19:43,775: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-05 14:19:43,776: t15.2023.08.13 val PER: 0.1455
2026-01-05 14:19:43,776: t15.2023.08.18 val PER: 0.1308
2026-01-05 14:19:43,776: t15.2023.08.20 val PER: 0.1454
2026-01-05 14:19:43,776: t15.2023.08.25 val PER: 0.1099
2026-01-05 14:19:43,776: t15.2023.08.27 val PER: 0.2074
2026-01-05 14:19:43,776: t15.2023.09.01 val PER: 0.1055
2026-01-05 14:19:43,776: t15.2023.09.03 val PER: 0.2007
2026-01-05 14:19:43,777: t15.2023.09.24 val PER: 0.1517
2026-01-05 14:19:43,777: t15.2023.09.29 val PER: 0.1563
2026-01-05 14:19:43,777: t15.2023.10.01 val PER: 0.1995
2026-01-05 14:19:43,777: t15.2023.10.06 val PER: 0.1044
2026-01-05 14:19:43,777: t15.2023.10.08 val PER: 0.2720
2026-01-05 14:19:43,777: t15.2023.10.13 val PER: 0.2420
2026-01-05 14:19:43,777: t15.2023.10.15 val PER: 0.1866
2026-01-05 14:19:43,777: t15.2023.10.20 val PER: 0.1946
2026-01-05 14:19:43,777: t15.2023.10.22 val PER: 0.1403
2026-01-05 14:19:43,777: t15.2023.11.03 val PER: 0.2001
2026-01-05 14:19:43,777: t15.2023.11.04 val PER: 0.0410
2026-01-05 14:19:43,778: t15.2023.11.17 val PER: 0.0498
2026-01-05 14:19:43,778: t15.2023.11.19 val PER: 0.0619
2026-01-05 14:19:43,778: t15.2023.11.26 val PER: 0.1855
2026-01-05 14:19:43,778: t15.2023.12.03 val PER: 0.1586
2026-01-05 14:19:43,778: t15.2023.12.08 val PER: 0.1405
2026-01-05 14:19:43,778: t15.2023.12.10 val PER: 0.1340
2026-01-05 14:19:43,778: t15.2023.12.17 val PER: 0.1694
2026-01-05 14:19:43,778: t15.2023.12.29 val PER: 0.1757
2026-01-05 14:19:43,778: t15.2024.02.25 val PER: 0.1433
2026-01-05 14:19:43,779: t15.2024.03.08 val PER: 0.2390
2026-01-05 14:19:43,779: t15.2024.03.15 val PER: 0.2402
2026-01-05 14:19:43,779: t15.2024.03.17 val PER: 0.1771
2026-01-05 14:19:43,779: t15.2024.05.10 val PER: 0.1961
2026-01-05 14:19:43,779: t15.2024.06.14 val PER: 0.2019
2026-01-05 14:19:43,779: t15.2024.07.19 val PER: 0.2828
2026-01-05 14:19:43,779: t15.2024.07.21 val PER: 0.1138
2026-01-05 14:19:43,779: t15.2024.07.28 val PER: 0.1529
2026-01-05 14:19:43,779: t15.2025.01.10 val PER: 0.3278
2026-01-05 14:19:43,780: t15.2025.01.12 val PER: 0.1917
2026-01-05 14:19:43,780: t15.2025.03.14 val PER: 0.3624
2026-01-05 14:19:43,780: t15.2025.03.16 val PER: 0.2264
2026-01-05 14:19:43,780: t15.2025.03.30 val PER: 0.3552
2026-01-05 14:19:43,780: t15.2025.04.13 val PER: 0.2439
2026-01-05 14:19:43,780: New best val WER(1gram) 52.79% --> 51.78%
2026-01-05 14:19:43,780: Checkpointing model
2026-01-05 14:19:44,479: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:19:44,793: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_8000
2026-01-05 14:20:03,594: Train batch 8200: loss: 9.41 grad norm: 45.55 time: 0.054
2026-01-05 14:20:22,422: Train batch 8400: loss: 9.65 grad norm: 46.02 time: 0.064
2026-01-05 14:20:31,944: Running test after training batch: 8500
2026-01-05 14:20:32,067: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:20:37,356: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 14:20:37,389: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nett
2026-01-05 14:20:39,137: Val batch 8500: PER (avg): 0.1778 CTC Loss (avg): 17.6598 WER(1gram): 49.24% (n=64) time: 7.193
2026-01-05 14:20:39,138: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 14:20:39,138: t15.2023.08.13 val PER: 0.1445
2026-01-05 14:20:39,138: t15.2023.08.18 val PER: 0.1442
2026-01-05 14:20:39,138: t15.2023.08.20 val PER: 0.1342
2026-01-05 14:20:39,138: t15.2023.08.25 val PER: 0.1024
2026-01-05 14:20:39,138: t15.2023.08.27 val PER: 0.1961
2026-01-05 14:20:39,138: t15.2023.09.01 val PER: 0.0942
2026-01-05 14:20:39,138: t15.2023.09.03 val PER: 0.1912
2026-01-05 14:20:39,139: t15.2023.09.24 val PER: 0.1408
2026-01-05 14:20:39,139: t15.2023.09.29 val PER: 0.1455
2026-01-05 14:20:39,139: t15.2023.10.01 val PER: 0.2008
2026-01-05 14:20:39,139: t15.2023.10.06 val PER: 0.1023
2026-01-05 14:20:39,139: t15.2023.10.08 val PER: 0.2612
2026-01-05 14:20:39,139: t15.2023.10.13 val PER: 0.2382
2026-01-05 14:20:39,139: t15.2023.10.15 val PER: 0.1747
2026-01-05 14:20:39,139: t15.2023.10.20 val PER: 0.1812
2026-01-05 14:20:39,139: t15.2023.10.22 val PER: 0.1425
2026-01-05 14:20:39,140: t15.2023.11.03 val PER: 0.1920
2026-01-05 14:20:39,140: t15.2023.11.04 val PER: 0.0478
2026-01-05 14:20:39,140: t15.2023.11.17 val PER: 0.0529
2026-01-05 14:20:39,140: t15.2023.11.19 val PER: 0.0439
2026-01-05 14:20:39,140: t15.2023.11.26 val PER: 0.1783
2026-01-05 14:20:39,140: t15.2023.12.03 val PER: 0.1502
2026-01-05 14:20:39,140: t15.2023.12.08 val PER: 0.1391
2026-01-05 14:20:39,140: t15.2023.12.10 val PER: 0.1183
2026-01-05 14:20:39,140: t15.2023.12.17 val PER: 0.1684
2026-01-05 14:20:39,140: t15.2023.12.29 val PER: 0.1640
2026-01-05 14:20:39,141: t15.2024.02.25 val PER: 0.1433
2026-01-05 14:20:39,141: t15.2024.03.08 val PER: 0.2489
2026-01-05 14:20:39,141: t15.2024.03.15 val PER: 0.2264
2026-01-05 14:20:39,141: t15.2024.03.17 val PER: 0.1729
2026-01-05 14:20:39,141: t15.2024.05.10 val PER: 0.1842
2026-01-05 14:20:39,141: t15.2024.06.14 val PER: 0.2050
2026-01-05 14:20:39,141: t15.2024.07.19 val PER: 0.2597
2026-01-05 14:20:39,141: t15.2024.07.21 val PER: 0.1138
2026-01-05 14:20:39,141: t15.2024.07.28 val PER: 0.1654
2026-01-05 14:20:39,141: t15.2025.01.10 val PER: 0.3306
2026-01-05 14:20:39,141: t15.2025.01.12 val PER: 0.1925
2026-01-05 14:20:39,142: t15.2025.03.14 val PER: 0.3595
2026-01-05 14:20:39,142: t15.2025.03.16 val PER: 0.2107
2026-01-05 14:20:39,142: t15.2025.03.30 val PER: 0.3379
2026-01-05 14:20:39,142: t15.2025.04.13 val PER: 0.2382
2026-01-05 14:20:39,142: New best val WER(1gram) 51.78% --> 49.24%
2026-01-05 14:20:39,142: Checkpointing model
2026-01-05 14:20:39,843: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:20:40,146: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_8500
2026-01-05 14:20:49,653: Train batch 8600: loss: 15.97 grad norm: 64.00 time: 0.054
2026-01-05 14:21:08,437: Train batch 8800: loss: 14.87 grad norm: 60.59 time: 0.061
2026-01-05 14:21:27,225: Train batch 9000: loss: 15.70 grad norm: 67.03 time: 0.072
2026-01-05 14:21:27,225: Running test after training batch: 9000
2026-01-05 14:21:27,362: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:21:32,420: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 14:21:32,452: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 14:21:34,201: Val batch 9000: PER (avg): 0.1727 CTC Loss (avg): 17.2142 WER(1gram): 49.75% (n=64) time: 6.975
2026-01-05 14:21:34,201: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 14:21:34,201: t15.2023.08.13 val PER: 0.1351
2026-01-05 14:21:34,201: t15.2023.08.18 val PER: 0.1224
2026-01-05 14:21:34,202: t15.2023.08.20 val PER: 0.1295
2026-01-05 14:21:34,202: t15.2023.08.25 val PER: 0.0994
2026-01-05 14:21:34,202: t15.2023.08.27 val PER: 0.1994
2026-01-05 14:21:34,202: t15.2023.09.01 val PER: 0.0974
2026-01-05 14:21:34,202: t15.2023.09.03 val PER: 0.1781
2026-01-05 14:21:34,202: t15.2023.09.24 val PER: 0.1359
2026-01-05 14:21:34,202: t15.2023.09.29 val PER: 0.1487
2026-01-05 14:21:34,202: t15.2023.10.01 val PER: 0.1922
2026-01-05 14:21:34,202: t15.2023.10.06 val PER: 0.1033
2026-01-05 14:21:34,202: t15.2023.10.08 val PER: 0.2625
2026-01-05 14:21:34,203: t15.2023.10.13 val PER: 0.2219
2026-01-05 14:21:34,203: t15.2023.10.15 val PER: 0.1740
2026-01-05 14:21:34,203: t15.2023.10.20 val PER: 0.1980
2026-01-05 14:21:34,203: t15.2023.10.22 val PER: 0.1325
2026-01-05 14:21:34,203: t15.2023.11.03 val PER: 0.2035
2026-01-05 14:21:34,203: t15.2023.11.04 val PER: 0.0307
2026-01-05 14:21:34,203: t15.2023.11.17 val PER: 0.0575
2026-01-05 14:21:34,203: t15.2023.11.19 val PER: 0.0499
2026-01-05 14:21:34,203: t15.2023.11.26 val PER: 0.1638
2026-01-05 14:21:34,203: t15.2023.12.03 val PER: 0.1355
2026-01-05 14:21:34,203: t15.2023.12.08 val PER: 0.1245
2026-01-05 14:21:34,204: t15.2023.12.10 val PER: 0.1091
2026-01-05 14:21:34,204: t15.2023.12.17 val PER: 0.1663
2026-01-05 14:21:34,204: t15.2023.12.29 val PER: 0.1592
2026-01-05 14:21:34,204: t15.2024.02.25 val PER: 0.1376
2026-01-05 14:21:34,204: t15.2024.03.08 val PER: 0.2532
2026-01-05 14:21:34,204: t15.2024.03.15 val PER: 0.2251
2026-01-05 14:21:34,204: t15.2024.03.17 val PER: 0.1667
2026-01-05 14:21:34,204: t15.2024.05.10 val PER: 0.1857
2026-01-05 14:21:34,204: t15.2024.06.14 val PER: 0.1861
2026-01-05 14:21:34,204: t15.2024.07.19 val PER: 0.2722
2026-01-05 14:21:34,204: t15.2024.07.21 val PER: 0.1138
2026-01-05 14:21:34,205: t15.2024.07.28 val PER: 0.1500
2026-01-05 14:21:34,205: t15.2025.01.10 val PER: 0.3113
2026-01-05 14:21:34,205: t15.2025.01.12 val PER: 0.1740
2026-01-05 14:21:34,205: t15.2025.03.14 val PER: 0.3462
2026-01-05 14:21:34,205: t15.2025.03.16 val PER: 0.2160
2026-01-05 14:21:34,205: t15.2025.03.30 val PER: 0.3333
2026-01-05 14:21:34,205: t15.2025.04.13 val PER: 0.2368
2026-01-05 14:21:34,518: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_9000
2026-01-05 14:21:53,440: Train batch 9200: loss: 10.88 grad norm: 52.08 time: 0.056
2026-01-05 14:22:12,147: Train batch 9400: loss: 7.11 grad norm: 45.41 time: 0.068
2026-01-05 14:22:21,594: Running test after training batch: 9500
2026-01-05 14:22:21,701: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:22:26,835: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point us will
2026-01-05 14:22:26,868: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-05 14:22:28,654: Val batch 9500: PER (avg): 0.1734 CTC Loss (avg): 17.2987 WER(1gram): 50.76% (n=64) time: 7.059
2026-01-05 14:22:28,654: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 14:22:28,654: t15.2023.08.13 val PER: 0.1351
2026-01-05 14:22:28,654: t15.2023.08.18 val PER: 0.1241
2026-01-05 14:22:28,654: t15.2023.08.20 val PER: 0.1303
2026-01-05 14:22:28,655: t15.2023.08.25 val PER: 0.1054
2026-01-05 14:22:28,655: t15.2023.08.27 val PER: 0.2010
2026-01-05 14:22:28,655: t15.2023.09.01 val PER: 0.0942
2026-01-05 14:22:28,655: t15.2023.09.03 val PER: 0.1722
2026-01-05 14:22:28,655: t15.2023.09.24 val PER: 0.1432
2026-01-05 14:22:28,655: t15.2023.09.29 val PER: 0.1506
2026-01-05 14:22:28,655: t15.2023.10.01 val PER: 0.1896
2026-01-05 14:22:28,655: t15.2023.10.06 val PER: 0.1076
2026-01-05 14:22:28,655: t15.2023.10.08 val PER: 0.2625
2026-01-05 14:22:28,655: t15.2023.10.13 val PER: 0.2250
2026-01-05 14:22:28,655: t15.2023.10.15 val PER: 0.1773
2026-01-05 14:22:28,655: t15.2023.10.20 val PER: 0.1980
2026-01-05 14:22:28,655: t15.2023.10.22 val PER: 0.1292
2026-01-05 14:22:28,656: t15.2023.11.03 val PER: 0.1940
2026-01-05 14:22:28,656: t15.2023.11.04 val PER: 0.0444
2026-01-05 14:22:28,656: t15.2023.11.17 val PER: 0.0622
2026-01-05 14:22:28,656: t15.2023.11.19 val PER: 0.0579
2026-01-05 14:22:28,656: t15.2023.11.26 val PER: 0.1645
2026-01-05 14:22:28,656: t15.2023.12.03 val PER: 0.1450
2026-01-05 14:22:28,656: t15.2023.12.08 val PER: 0.1345
2026-01-05 14:22:28,656: t15.2023.12.10 val PER: 0.1130
2026-01-05 14:22:28,656: t15.2023.12.17 val PER: 0.1726
2026-01-05 14:22:28,656: t15.2023.12.29 val PER: 0.1627
2026-01-05 14:22:28,656: t15.2024.02.25 val PER: 0.1461
2026-01-05 14:22:28,657: t15.2024.03.08 val PER: 0.2532
2026-01-05 14:22:28,657: t15.2024.03.15 val PER: 0.2214
2026-01-05 14:22:28,657: t15.2024.03.17 val PER: 0.1632
2026-01-05 14:22:28,657: t15.2024.05.10 val PER: 0.1961
2026-01-05 14:22:28,657: t15.2024.06.14 val PER: 0.1751
2026-01-05 14:22:28,657: t15.2024.07.19 val PER: 0.2571
2026-01-05 14:22:28,657: t15.2024.07.21 val PER: 0.1124
2026-01-05 14:22:28,657: t15.2024.07.28 val PER: 0.1625
2026-01-05 14:22:28,657: t15.2025.01.10 val PER: 0.3030
2026-01-05 14:22:28,657: t15.2025.01.12 val PER: 0.1794
2026-01-05 14:22:28,657: t15.2025.03.14 val PER: 0.3609
2026-01-05 14:22:28,657: t15.2025.03.16 val PER: 0.2134
2026-01-05 14:22:28,657: t15.2025.03.30 val PER: 0.3207
2026-01-05 14:22:28,657: t15.2025.04.13 val PER: 0.2297
2026-01-05 14:22:28,991: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_9500
2026-01-05 14:22:38,448: Train batch 9600: loss: 7.91 grad norm: 43.02 time: 0.073
2026-01-05 14:22:57,696: Train batch 9800: loss: 12.18 grad norm: 57.42 time: 0.063
2026-01-05 14:23:17,016: Train batch 10000: loss: 5.26 grad norm: 36.78 time: 0.067
2026-01-05 14:23:17,017: Running test after training batch: 10000
2026-01-05 14:23:17,154: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:23:22,382: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 14:23:22,416: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sit
2026-01-05 14:23:24,190: Val batch 10000: PER (avg): 0.1693 CTC Loss (avg): 16.9339 WER(1gram): 52.28% (n=64) time: 7.173
2026-01-05 14:23:24,190: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 14:23:24,191: t15.2023.08.13 val PER: 0.1320
2026-01-05 14:23:24,191: t15.2023.08.18 val PER: 0.1308
2026-01-05 14:23:24,191: t15.2023.08.20 val PER: 0.1247
2026-01-05 14:23:24,191: t15.2023.08.25 val PER: 0.1039
2026-01-05 14:23:24,191: t15.2023.08.27 val PER: 0.1913
2026-01-05 14:23:24,191: t15.2023.09.01 val PER: 0.0950
2026-01-05 14:23:24,191: t15.2023.09.03 val PER: 0.1888
2026-01-05 14:23:24,191: t15.2023.09.24 val PER: 0.1359
2026-01-05 14:23:24,191: t15.2023.09.29 val PER: 0.1398
2026-01-05 14:23:24,191: t15.2023.10.01 val PER: 0.1797
2026-01-05 14:23:24,191: t15.2023.10.06 val PER: 0.1033
2026-01-05 14:23:24,191: t15.2023.10.08 val PER: 0.2652
2026-01-05 14:23:24,191: t15.2023.10.13 val PER: 0.2196
2026-01-05 14:23:24,191: t15.2023.10.15 val PER: 0.1648
2026-01-05 14:23:24,192: t15.2023.10.20 val PER: 0.1812
2026-01-05 14:23:24,192: t15.2023.10.22 val PER: 0.1214
2026-01-05 14:23:24,192: t15.2023.11.03 val PER: 0.1934
2026-01-05 14:23:24,192: t15.2023.11.04 val PER: 0.0239
2026-01-05 14:23:24,192: t15.2023.11.17 val PER: 0.0560
2026-01-05 14:23:24,192: t15.2023.11.19 val PER: 0.0439
2026-01-05 14:23:24,192: t15.2023.11.26 val PER: 0.1572
2026-01-05 14:23:24,192: t15.2023.12.03 val PER: 0.1439
2026-01-05 14:23:24,192: t15.2023.12.08 val PER: 0.1325
2026-01-05 14:23:24,192: t15.2023.12.10 val PER: 0.1117
2026-01-05 14:23:24,193: t15.2023.12.17 val PER: 0.1674
2026-01-05 14:23:24,193: t15.2023.12.29 val PER: 0.1496
2026-01-05 14:23:24,193: t15.2024.02.25 val PER: 0.1419
2026-01-05 14:23:24,193: t15.2024.03.08 val PER: 0.2390
2026-01-05 14:23:24,193: t15.2024.03.15 val PER: 0.2176
2026-01-05 14:23:24,193: t15.2024.03.17 val PER: 0.1667
2026-01-05 14:23:24,193: t15.2024.05.10 val PER: 0.1738
2026-01-05 14:23:24,193: t15.2024.06.14 val PER: 0.1877
2026-01-05 14:23:24,193: t15.2024.07.19 val PER: 0.2683
2026-01-05 14:23:24,193: t15.2024.07.21 val PER: 0.1145
2026-01-05 14:23:24,193: t15.2024.07.28 val PER: 0.1507
2026-01-05 14:23:24,193: t15.2025.01.10 val PER: 0.2975
2026-01-05 14:23:24,193: t15.2025.01.12 val PER: 0.1747
2026-01-05 14:23:24,193: t15.2025.03.14 val PER: 0.3491
2026-01-05 14:23:24,193: t15.2025.03.16 val PER: 0.2042
2026-01-05 14:23:24,194: t15.2025.03.30 val PER: 0.3207
2026-01-05 14:23:24,194: t15.2025.04.13 val PER: 0.2297
2026-01-05 14:23:24,512: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_10000
2026-01-05 14:23:43,400: Train batch 10200: loss: 6.30 grad norm: 40.56 time: 0.050
2026-01-05 14:24:03,306: Train batch 10400: loss: 8.96 grad norm: 47.79 time: 0.072
2026-01-05 14:24:13,611: Running test after training batch: 10500
2026-01-05 14:24:13,778: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:24:18,934: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:24:18,967: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-05 14:24:20,780: Val batch 10500: PER (avg): 0.1663 CTC Loss (avg): 16.6049 WER(1gram): 50.00% (n=64) time: 7.169
2026-01-05 14:24:20,780: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 14:24:20,780: t15.2023.08.13 val PER: 0.1268
2026-01-05 14:24:20,780: t15.2023.08.18 val PER: 0.1232
2026-01-05 14:24:20,781: t15.2023.08.20 val PER: 0.1223
2026-01-05 14:24:20,781: t15.2023.08.25 val PER: 0.1069
2026-01-05 14:24:20,781: t15.2023.08.27 val PER: 0.1913
2026-01-05 14:24:20,781: t15.2023.09.01 val PER: 0.0950
2026-01-05 14:24:20,781: t15.2023.09.03 val PER: 0.1781
2026-01-05 14:24:20,781: t15.2023.09.24 val PER: 0.1468
2026-01-05 14:24:20,781: t15.2023.09.29 val PER: 0.1506
2026-01-05 14:24:20,781: t15.2023.10.01 val PER: 0.1810
2026-01-05 14:24:20,781: t15.2023.10.06 val PER: 0.0936
2026-01-05 14:24:20,781: t15.2023.10.08 val PER: 0.2530
2026-01-05 14:24:20,781: t15.2023.10.13 val PER: 0.2110
2026-01-05 14:24:20,781: t15.2023.10.15 val PER: 0.1760
2026-01-05 14:24:20,782: t15.2023.10.20 val PER: 0.1946
2026-01-05 14:24:20,782: t15.2023.10.22 val PER: 0.1247
2026-01-05 14:24:20,782: t15.2023.11.03 val PER: 0.1906
2026-01-05 14:24:20,782: t15.2023.11.04 val PER: 0.0341
2026-01-05 14:24:20,782: t15.2023.11.17 val PER: 0.0591
2026-01-05 14:24:20,782: t15.2023.11.19 val PER: 0.0579
2026-01-05 14:24:20,782: t15.2023.11.26 val PER: 0.1348
2026-01-05 14:24:20,782: t15.2023.12.03 val PER: 0.1376
2026-01-05 14:24:20,782: t15.2023.12.08 val PER: 0.1185
2026-01-05 14:24:20,782: t15.2023.12.10 val PER: 0.1064
2026-01-05 14:24:20,782: t15.2023.12.17 val PER: 0.1518
2026-01-05 14:24:20,782: t15.2023.12.29 val PER: 0.1503
2026-01-05 14:24:20,783: t15.2024.02.25 val PER: 0.1236
2026-01-05 14:24:20,783: t15.2024.03.08 val PER: 0.2418
2026-01-05 14:24:20,783: t15.2024.03.15 val PER: 0.2158
2026-01-05 14:24:20,783: t15.2024.03.17 val PER: 0.1618
2026-01-05 14:24:20,783: t15.2024.05.10 val PER: 0.1813
2026-01-05 14:24:20,783: t15.2024.06.14 val PER: 0.1814
2026-01-05 14:24:20,783: t15.2024.07.19 val PER: 0.2544
2026-01-05 14:24:20,783: t15.2024.07.21 val PER: 0.1110
2026-01-05 14:24:20,783: t15.2024.07.28 val PER: 0.1419
2026-01-05 14:24:20,783: t15.2025.01.10 val PER: 0.3085
2026-01-05 14:24:20,783: t15.2025.01.12 val PER: 0.1694
2026-01-05 14:24:20,783: t15.2025.03.14 val PER: 0.3595
2026-01-05 14:24:20,783: t15.2025.03.16 val PER: 0.2003
2026-01-05 14:24:20,783: t15.2025.03.30 val PER: 0.3172
2026-01-05 14:24:20,784: t15.2025.04.13 val PER: 0.2311
2026-01-05 14:24:21,127: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_10500
2026-01-05 14:24:30,974: Train batch 10600: loss: 9.17 grad norm: 58.24 time: 0.072
2026-01-05 14:24:50,199: Train batch 10800: loss: 14.45 grad norm: 62.72 time: 0.065
2026-01-05 14:25:09,562: Train batch 11000: loss: 13.93 grad norm: 60.83 time: 0.057
2026-01-05 14:25:09,562: Running test after training batch: 11000
2026-01-05 14:25:09,736: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:25:14,789: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:25:14,822: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 14:25:16,628: Val batch 11000: PER (avg): 0.1638 CTC Loss (avg): 16.4814 WER(1gram): 49.49% (n=64) time: 7.066
2026-01-05 14:25:16,628: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 14:25:16,628: t15.2023.08.13 val PER: 0.1216
2026-01-05 14:25:16,628: t15.2023.08.18 val PER: 0.1249
2026-01-05 14:25:16,629: t15.2023.08.20 val PER: 0.1279
2026-01-05 14:25:16,629: t15.2023.08.25 val PER: 0.1039
2026-01-05 14:25:16,629: t15.2023.08.27 val PER: 0.1913
2026-01-05 14:25:16,629: t15.2023.09.01 val PER: 0.0885
2026-01-05 14:25:16,629: t15.2023.09.03 val PER: 0.1841
2026-01-05 14:25:16,629: t15.2023.09.24 val PER: 0.1432
2026-01-05 14:25:16,629: t15.2023.09.29 val PER: 0.1391
2026-01-05 14:25:16,629: t15.2023.10.01 val PER: 0.1823
2026-01-05 14:25:16,629: t15.2023.10.06 val PER: 0.0915
2026-01-05 14:25:16,629: t15.2023.10.08 val PER: 0.2666
2026-01-05 14:25:16,630: t15.2023.10.13 val PER: 0.2095
2026-01-05 14:25:16,630: t15.2023.10.15 val PER: 0.1780
2026-01-05 14:25:16,630: t15.2023.10.20 val PER: 0.2047
2026-01-05 14:25:16,630: t15.2023.10.22 val PER: 0.1203
2026-01-05 14:25:16,630: t15.2023.11.03 val PER: 0.1893
2026-01-05 14:25:16,630: t15.2023.11.04 val PER: 0.0307
2026-01-05 14:25:16,630: t15.2023.11.17 val PER: 0.0575
2026-01-05 14:25:16,630: t15.2023.11.19 val PER: 0.0479
2026-01-05 14:25:16,630: t15.2023.11.26 val PER: 0.1406
2026-01-05 14:25:16,631: t15.2023.12.03 val PER: 0.1345
2026-01-05 14:25:16,631: t15.2023.12.08 val PER: 0.1192
2026-01-05 14:25:16,631: t15.2023.12.10 val PER: 0.1012
2026-01-05 14:25:16,631: t15.2023.12.17 val PER: 0.1403
2026-01-05 14:25:16,631: t15.2023.12.29 val PER: 0.1421
2026-01-05 14:25:16,631: t15.2024.02.25 val PER: 0.1292
2026-01-05 14:25:16,631: t15.2024.03.08 val PER: 0.2361
2026-01-05 14:25:16,631: t15.2024.03.15 val PER: 0.2083
2026-01-05 14:25:16,631: t15.2024.03.17 val PER: 0.1569
2026-01-05 14:25:16,631: t15.2024.05.10 val PER: 0.1783
2026-01-05 14:25:16,632: t15.2024.06.14 val PER: 0.1656
2026-01-05 14:25:16,632: t15.2024.07.19 val PER: 0.2617
2026-01-05 14:25:16,632: t15.2024.07.21 val PER: 0.1048
2026-01-05 14:25:16,632: t15.2024.07.28 val PER: 0.1397
2026-01-05 14:25:16,632: t15.2025.01.10 val PER: 0.3085
2026-01-05 14:25:16,632: t15.2025.01.12 val PER: 0.1655
2026-01-05 14:25:16,632: t15.2025.03.14 val PER: 0.3462
2026-01-05 14:25:16,632: t15.2025.03.16 val PER: 0.1911
2026-01-05 14:25:16,632: t15.2025.03.30 val PER: 0.3149
2026-01-05 14:25:16,633: t15.2025.04.13 val PER: 0.2211
2026-01-05 14:25:16,970: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_11000
2026-01-05 14:25:36,073: Train batch 11200: loss: 10.54 grad norm: 52.58 time: 0.073
2026-01-05 14:25:55,370: Train batch 11400: loss: 9.19 grad norm: 52.82 time: 0.057
2026-01-05 14:26:05,062: Running test after training batch: 11500
2026-01-05 14:26:05,184: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:26:10,732: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:26:10,767: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-05 14:26:12,664: Val batch 11500: PER (avg): 0.1606 CTC Loss (avg): 16.3532 WER(1gram): 50.25% (n=64) time: 7.600
2026-01-05 14:26:12,664: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-05 14:26:12,665: t15.2023.08.13 val PER: 0.1237
2026-01-05 14:26:12,665: t15.2023.08.18 val PER: 0.1182
2026-01-05 14:26:12,665: t15.2023.08.20 val PER: 0.1207
2026-01-05 14:26:12,665: t15.2023.08.25 val PER: 0.1009
2026-01-05 14:26:12,665: t15.2023.08.27 val PER: 0.1994
2026-01-05 14:26:12,665: t15.2023.09.01 val PER: 0.0844
2026-01-05 14:26:12,665: t15.2023.09.03 val PER: 0.1734
2026-01-05 14:26:12,665: t15.2023.09.24 val PER: 0.1311
2026-01-05 14:26:12,666: t15.2023.09.29 val PER: 0.1404
2026-01-05 14:26:12,666: t15.2023.10.01 val PER: 0.1770
2026-01-05 14:26:12,666: t15.2023.10.06 val PER: 0.0904
2026-01-05 14:26:12,666: t15.2023.10.08 val PER: 0.2585
2026-01-05 14:26:12,666: t15.2023.10.13 val PER: 0.2118
2026-01-05 14:26:12,666: t15.2023.10.15 val PER: 0.1661
2026-01-05 14:26:12,666: t15.2023.10.20 val PER: 0.1913
2026-01-05 14:26:12,666: t15.2023.10.22 val PER: 0.1125
2026-01-05 14:26:12,666: t15.2023.11.03 val PER: 0.1859
2026-01-05 14:26:12,666: t15.2023.11.04 val PER: 0.0341
2026-01-05 14:26:12,666: t15.2023.11.17 val PER: 0.0498
2026-01-05 14:26:12,667: t15.2023.11.19 val PER: 0.0479
2026-01-05 14:26:12,667: t15.2023.11.26 val PER: 0.1312
2026-01-05 14:26:12,667: t15.2023.12.03 val PER: 0.1218
2026-01-05 14:26:12,667: t15.2023.12.08 val PER: 0.1172
2026-01-05 14:26:12,667: t15.2023.12.10 val PER: 0.0999
2026-01-05 14:26:12,667: t15.2023.12.17 val PER: 0.1559
2026-01-05 14:26:12,667: t15.2023.12.29 val PER: 0.1386
2026-01-05 14:26:12,667: t15.2024.02.25 val PER: 0.1236
2026-01-05 14:26:12,667: t15.2024.03.08 val PER: 0.2333
2026-01-05 14:26:12,667: t15.2024.03.15 val PER: 0.2095
2026-01-05 14:26:12,667: t15.2024.03.17 val PER: 0.1527
2026-01-05 14:26:12,667: t15.2024.05.10 val PER: 0.1620
2026-01-05 14:26:12,667: t15.2024.06.14 val PER: 0.1703
2026-01-05 14:26:12,667: t15.2024.07.19 val PER: 0.2538
2026-01-05 14:26:12,667: t15.2024.07.21 val PER: 0.0966
2026-01-05 14:26:12,668: t15.2024.07.28 val PER: 0.1441
2026-01-05 14:26:12,668: t15.2025.01.10 val PER: 0.3127
2026-01-05 14:26:12,668: t15.2025.01.12 val PER: 0.1601
2026-01-05 14:26:12,668: t15.2025.03.14 val PER: 0.3476
2026-01-05 14:26:12,668: t15.2025.03.16 val PER: 0.2055
2026-01-05 14:26:12,668: t15.2025.03.30 val PER: 0.3126
2026-01-05 14:26:12,668: t15.2025.04.13 val PER: 0.2254
2026-01-05 14:26:13,005: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_11500
2026-01-05 14:26:22,267: Train batch 11600: loss: 10.86 grad norm: 50.86 time: 0.060
2026-01-05 14:26:41,050: Train batch 11800: loss: 6.65 grad norm: 42.16 time: 0.045
2026-01-05 14:27:00,022: Train batch 12000: loss: 14.28 grad norm: 52.82 time: 0.072
2026-01-05 14:27:00,023: Running test after training batch: 12000
2026-01-05 14:27:00,123: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:27:05,294: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:27:05,326: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-05 14:27:07,167: Val batch 12000: PER (avg): 0.1589 CTC Loss (avg): 16.0275 WER(1gram): 52.54% (n=64) time: 7.144
2026-01-05 14:27:07,167: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 14:27:07,168: t15.2023.08.13 val PER: 0.1227
2026-01-05 14:27:07,168: t15.2023.08.18 val PER: 0.1132
2026-01-05 14:27:07,168: t15.2023.08.20 val PER: 0.1152
2026-01-05 14:27:07,168: t15.2023.08.25 val PER: 0.0994
2026-01-05 14:27:07,168: t15.2023.08.27 val PER: 0.1881
2026-01-05 14:27:07,168: t15.2023.09.01 val PER: 0.0844
2026-01-05 14:27:07,168: t15.2023.09.03 val PER: 0.1651
2026-01-05 14:27:07,168: t15.2023.09.24 val PER: 0.1323
2026-01-05 14:27:07,168: t15.2023.09.29 val PER: 0.1359
2026-01-05 14:27:07,168: t15.2023.10.01 val PER: 0.1704
2026-01-05 14:27:07,169: t15.2023.10.06 val PER: 0.0915
2026-01-05 14:27:07,169: t15.2023.10.08 val PER: 0.2544
2026-01-05 14:27:07,173: t15.2023.10.13 val PER: 0.2079
2026-01-05 14:27:07,173: t15.2023.10.15 val PER: 0.1674
2026-01-05 14:27:07,173: t15.2023.10.20 val PER: 0.2013
2026-01-05 14:27:07,173: t15.2023.10.22 val PER: 0.1180
2026-01-05 14:27:07,173: t15.2023.11.03 val PER: 0.1947
2026-01-05 14:27:07,173: t15.2023.11.04 val PER: 0.0341
2026-01-05 14:27:07,173: t15.2023.11.17 val PER: 0.0451
2026-01-05 14:27:07,173: t15.2023.11.19 val PER: 0.0359
2026-01-05 14:27:07,173: t15.2023.11.26 val PER: 0.1290
2026-01-05 14:27:07,173: t15.2023.12.03 val PER: 0.1197
2026-01-05 14:27:07,173: t15.2023.12.08 val PER: 0.1052
2026-01-05 14:27:07,174: t15.2023.12.10 val PER: 0.0894
2026-01-05 14:27:07,174: t15.2023.12.17 val PER: 0.1486
2026-01-05 14:27:07,174: t15.2023.12.29 val PER: 0.1407
2026-01-05 14:27:07,174: t15.2024.02.25 val PER: 0.1348
2026-01-05 14:27:07,174: t15.2024.03.08 val PER: 0.2418
2026-01-05 14:27:07,174: t15.2024.03.15 val PER: 0.2145
2026-01-05 14:27:07,174: t15.2024.03.17 val PER: 0.1520
2026-01-05 14:27:07,174: t15.2024.05.10 val PER: 0.1932
2026-01-05 14:27:07,174: t15.2024.06.14 val PER: 0.1924
2026-01-05 14:27:07,175: t15.2024.07.19 val PER: 0.2393
2026-01-05 14:27:07,175: t15.2024.07.21 val PER: 0.1034
2026-01-05 14:27:07,175: t15.2024.07.28 val PER: 0.1412
2026-01-05 14:27:07,175: t15.2025.01.10 val PER: 0.3017
2026-01-05 14:27:07,175: t15.2025.01.12 val PER: 0.1486
2026-01-05 14:27:07,175: t15.2025.03.14 val PER: 0.3506
2026-01-05 14:27:07,175: t15.2025.03.16 val PER: 0.1976
2026-01-05 14:27:07,175: t15.2025.03.30 val PER: 0.3011
2026-01-05 14:27:07,175: t15.2025.04.13 val PER: 0.2240
2026-01-05 14:27:07,501: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_12000
2026-01-05 14:27:26,535: Train batch 12200: loss: 5.52 grad norm: 40.77 time: 0.065
2026-01-05 14:27:45,077: Train batch 12400: loss: 4.81 grad norm: 37.00 time: 0.041
2026-01-05 14:27:54,607: Running test after training batch: 12500
2026-01-05 14:27:54,716: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:27:59,818: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:27:59,852: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-05 14:28:01,679: Val batch 12500: PER (avg): 0.1562 CTC Loss (avg): 15.9730 WER(1gram): 48.98% (n=64) time: 7.072
2026-01-05 14:28:01,679: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 14:28:01,679: t15.2023.08.13 val PER: 0.1258
2026-01-05 14:28:01,679: t15.2023.08.18 val PER: 0.1165
2026-01-05 14:28:01,680: t15.2023.08.20 val PER: 0.1128
2026-01-05 14:28:01,680: t15.2023.08.25 val PER: 0.0873
2026-01-05 14:28:01,680: t15.2023.08.27 val PER: 0.2042
2026-01-05 14:28:01,680: t15.2023.09.01 val PER: 0.0893
2026-01-05 14:28:01,680: t15.2023.09.03 val PER: 0.1603
2026-01-05 14:28:01,680: t15.2023.09.24 val PER: 0.1383
2026-01-05 14:28:01,680: t15.2023.09.29 val PER: 0.1308
2026-01-05 14:28:01,680: t15.2023.10.01 val PER: 0.1724
2026-01-05 14:28:01,680: t15.2023.10.06 val PER: 0.0861
2026-01-05 14:28:01,680: t15.2023.10.08 val PER: 0.2598
2026-01-05 14:28:01,680: t15.2023.10.13 val PER: 0.2064
2026-01-05 14:28:01,681: t15.2023.10.15 val PER: 0.1595
2026-01-05 14:28:01,681: t15.2023.10.20 val PER: 0.1913
2026-01-05 14:28:01,681: t15.2023.10.22 val PER: 0.1102
2026-01-05 14:28:01,681: t15.2023.11.03 val PER: 0.1866
2026-01-05 14:28:01,681: t15.2023.11.04 val PER: 0.0341
2026-01-05 14:28:01,681: t15.2023.11.17 val PER: 0.0435
2026-01-05 14:28:01,681: t15.2023.11.19 val PER: 0.0319
2026-01-05 14:28:01,681: t15.2023.11.26 val PER: 0.1297
2026-01-05 14:28:01,681: t15.2023.12.03 val PER: 0.1176
2026-01-05 14:28:01,681: t15.2023.12.08 val PER: 0.1138
2026-01-05 14:28:01,681: t15.2023.12.10 val PER: 0.0959
2026-01-05 14:28:01,682: t15.2023.12.17 val PER: 0.1445
2026-01-05 14:28:01,682: t15.2023.12.29 val PER: 0.1352
2026-01-05 14:28:01,682: t15.2024.02.25 val PER: 0.1081
2026-01-05 14:28:01,682: t15.2024.03.08 val PER: 0.2290
2026-01-05 14:28:01,682: t15.2024.03.15 val PER: 0.2070
2026-01-05 14:28:01,682: t15.2024.03.17 val PER: 0.1499
2026-01-05 14:28:01,682: t15.2024.05.10 val PER: 0.1679
2026-01-05 14:28:01,682: t15.2024.06.14 val PER: 0.1751
2026-01-05 14:28:01,682: t15.2024.07.19 val PER: 0.2393
2026-01-05 14:28:01,682: t15.2024.07.21 val PER: 0.0931
2026-01-05 14:28:01,682: t15.2024.07.28 val PER: 0.1368
2026-01-05 14:28:01,682: t15.2025.01.10 val PER: 0.3099
2026-01-05 14:28:01,682: t15.2025.01.12 val PER: 0.1563
2026-01-05 14:28:01,682: t15.2025.03.14 val PER: 0.3506
2026-01-05 14:28:01,682: t15.2025.03.16 val PER: 0.1872
2026-01-05 14:28:01,683: t15.2025.03.30 val PER: 0.3069
2026-01-05 14:28:01,683: t15.2025.04.13 val PER: 0.2140
2026-01-05 14:28:01,683: New best val WER(1gram) 49.24% --> 48.98%
2026-01-05 14:28:01,684: Checkpointing model
2026-01-05 14:28:02,390: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:28:02,729: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_12500
2026-01-05 14:28:12,163: Train batch 12600: loss: 7.79 grad norm: 43.78 time: 0.058
2026-01-05 14:28:31,165: Train batch 12800: loss: 5.59 grad norm: 38.93 time: 0.052
2026-01-05 14:28:50,699: Train batch 13000: loss: 6.33 grad norm: 41.63 time: 0.066
2026-01-05 14:28:50,700: Running test after training batch: 13000
2026-01-05 14:28:50,800: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:28:56,189: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:28:56,223: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-05 14:28:58,120: Val batch 13000: PER (avg): 0.1548 CTC Loss (avg): 15.7778 WER(1gram): 47.72% (n=64) time: 7.420
2026-01-05 14:28:58,120: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 14:28:58,120: t15.2023.08.13 val PER: 0.1206
2026-01-05 14:28:58,120: t15.2023.08.18 val PER: 0.1132
2026-01-05 14:28:58,121: t15.2023.08.20 val PER: 0.1096
2026-01-05 14:28:58,121: t15.2023.08.25 val PER: 0.0994
2026-01-05 14:28:58,121: t15.2023.08.27 val PER: 0.1881
2026-01-05 14:28:58,121: t15.2023.09.01 val PER: 0.0812
2026-01-05 14:28:58,121: t15.2023.09.03 val PER: 0.1651
2026-01-05 14:28:58,121: t15.2023.09.24 val PER: 0.1250
2026-01-05 14:28:58,121: t15.2023.09.29 val PER: 0.1315
2026-01-05 14:28:58,121: t15.2023.10.01 val PER: 0.1764
2026-01-05 14:28:58,121: t15.2023.10.06 val PER: 0.0893
2026-01-05 14:28:58,122: t15.2023.10.08 val PER: 0.2449
2026-01-05 14:28:58,122: t15.2023.10.13 val PER: 0.2087
2026-01-05 14:28:58,122: t15.2023.10.15 val PER: 0.1589
2026-01-05 14:28:58,122: t15.2023.10.20 val PER: 0.1711
2026-01-05 14:28:58,122: t15.2023.10.22 val PER: 0.1203
2026-01-05 14:28:58,122: t15.2023.11.03 val PER: 0.1859
2026-01-05 14:28:58,122: t15.2023.11.04 val PER: 0.0341
2026-01-05 14:28:58,122: t15.2023.11.17 val PER: 0.0404
2026-01-05 14:28:58,122: t15.2023.11.19 val PER: 0.0479
2026-01-05 14:28:58,123: t15.2023.11.26 val PER: 0.1232
2026-01-05 14:28:58,123: t15.2023.12.03 val PER: 0.1145
2026-01-05 14:28:58,123: t15.2023.12.08 val PER: 0.1112
2026-01-05 14:28:58,123: t15.2023.12.10 val PER: 0.0920
2026-01-05 14:28:58,123: t15.2023.12.17 val PER: 0.1403
2026-01-05 14:28:58,123: t15.2023.12.29 val PER: 0.1345
2026-01-05 14:28:58,123: t15.2024.02.25 val PER: 0.1096
2026-01-05 14:28:58,123: t15.2024.03.08 val PER: 0.2233
2026-01-05 14:28:58,123: t15.2024.03.15 val PER: 0.2045
2026-01-05 14:28:58,123: t15.2024.03.17 val PER: 0.1444
2026-01-05 14:28:58,123: t15.2024.05.10 val PER: 0.1709
2026-01-05 14:28:58,123: t15.2024.06.14 val PER: 0.1640
2026-01-05 14:28:58,124: t15.2024.07.19 val PER: 0.2426
2026-01-05 14:28:58,124: t15.2024.07.21 val PER: 0.0938
2026-01-05 14:28:58,124: t15.2024.07.28 val PER: 0.1456
2026-01-05 14:28:58,124: t15.2025.01.10 val PER: 0.3072
2026-01-05 14:28:58,124: t15.2025.01.12 val PER: 0.1540
2026-01-05 14:28:58,124: t15.2025.03.14 val PER: 0.3476
2026-01-05 14:28:58,124: t15.2025.03.16 val PER: 0.1754
2026-01-05 14:28:58,125: t15.2025.03.30 val PER: 0.3069
2026-01-05 14:28:58,125: t15.2025.04.13 val PER: 0.2225
2026-01-05 14:28:58,125: New best val WER(1gram) 48.98% --> 47.72%
2026-01-05 14:28:58,125: Checkpointing model
2026-01-05 14:28:58,799: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:28:59,143: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_13000
2026-01-05 14:29:18,274: Train batch 13200: loss: 11.76 grad norm: 56.06 time: 0.054
2026-01-05 14:29:37,153: Train batch 13400: loss: 8.56 grad norm: 53.13 time: 0.062
2026-01-05 14:29:46,733: Running test after training batch: 13500
2026-01-05 14:29:46,839: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:29:52,210: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 14:29:52,243: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-05 14:29:54,094: Val batch 13500: PER (avg): 0.1515 CTC Loss (avg): 15.5966 WER(1gram): 48.22% (n=64) time: 7.359
2026-01-05 14:29:54,094: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 14:29:54,094: t15.2023.08.13 val PER: 0.1237
2026-01-05 14:29:54,094: t15.2023.08.18 val PER: 0.1056
2026-01-05 14:29:54,094: t15.2023.08.20 val PER: 0.1041
2026-01-05 14:29:54,094: t15.2023.08.25 val PER: 0.0919
2026-01-05 14:29:54,095: t15.2023.08.27 val PER: 0.1961
2026-01-05 14:29:54,095: t15.2023.09.01 val PER: 0.0852
2026-01-05 14:29:54,095: t15.2023.09.03 val PER: 0.1627
2026-01-05 14:29:54,095: t15.2023.09.24 val PER: 0.1189
2026-01-05 14:29:54,095: t15.2023.09.29 val PER: 0.1257
2026-01-05 14:29:54,095: t15.2023.10.01 val PER: 0.1764
2026-01-05 14:29:54,095: t15.2023.10.06 val PER: 0.0904
2026-01-05 14:29:54,095: t15.2023.10.08 val PER: 0.2544
2026-01-05 14:29:54,095: t15.2023.10.13 val PER: 0.2048
2026-01-05 14:29:54,095: t15.2023.10.15 val PER: 0.1622
2026-01-05 14:29:54,095: t15.2023.10.20 val PER: 0.1711
2026-01-05 14:29:54,096: t15.2023.10.22 val PER: 0.1169
2026-01-05 14:29:54,096: t15.2023.11.03 val PER: 0.1832
2026-01-05 14:29:54,096: t15.2023.11.04 val PER: 0.0307
2026-01-05 14:29:54,096: t15.2023.11.17 val PER: 0.0435
2026-01-05 14:29:54,096: t15.2023.11.19 val PER: 0.0279
2026-01-05 14:29:54,096: t15.2023.11.26 val PER: 0.1174
2026-01-05 14:29:54,096: t15.2023.12.03 val PER: 0.1103
2026-01-05 14:29:54,096: t15.2023.12.08 val PER: 0.1045
2026-01-05 14:29:54,096: t15.2023.12.10 val PER: 0.0933
2026-01-05 14:29:54,096: t15.2023.12.17 val PER: 0.1362
2026-01-05 14:29:54,097: t15.2023.12.29 val PER: 0.1338
2026-01-05 14:29:54,097: t15.2024.02.25 val PER: 0.1067
2026-01-05 14:29:54,097: t15.2024.03.08 val PER: 0.2262
2026-01-05 14:29:54,097: t15.2024.03.15 val PER: 0.2033
2026-01-05 14:29:54,097: t15.2024.03.17 val PER: 0.1402
2026-01-05 14:29:54,097: t15.2024.05.10 val PER: 0.1634
2026-01-05 14:29:54,097: t15.2024.06.14 val PER: 0.1656
2026-01-05 14:29:54,097: t15.2024.07.19 val PER: 0.2347
2026-01-05 14:29:54,097: t15.2024.07.21 val PER: 0.0924
2026-01-05 14:29:54,097: t15.2024.07.28 val PER: 0.1265
2026-01-05 14:29:54,097: t15.2025.01.10 val PER: 0.3058
2026-01-05 14:29:54,097: t15.2025.01.12 val PER: 0.1463
2026-01-05 14:29:54,098: t15.2025.03.14 val PER: 0.3358
2026-01-05 14:29:54,098: t15.2025.03.16 val PER: 0.1859
2026-01-05 14:29:54,098: t15.2025.03.30 val PER: 0.3034
2026-01-05 14:29:54,098: t15.2025.04.13 val PER: 0.2083
2026-01-05 14:29:54,429: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_13500
2026-01-05 14:30:03,918: Train batch 13600: loss: 12.26 grad norm: 64.83 time: 0.062
2026-01-05 14:30:23,014: Train batch 13800: loss: 8.59 grad norm: 54.09 time: 0.056
2026-01-05 14:30:42,218: Train batch 14000: loss: 11.40 grad norm: 59.04 time: 0.051
2026-01-05 14:30:42,219: Running test after training batch: 14000
2026-01-05 14:30:42,340: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:30:47,423: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:30:47,457: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-05 14:30:49,355: Val batch 14000: PER (avg): 0.1504 CTC Loss (avg): 15.4757 WER(1gram): 48.22% (n=64) time: 7.136
2026-01-05 14:30:49,355: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=11
2026-01-05 14:30:49,355: t15.2023.08.13 val PER: 0.1195
2026-01-05 14:30:49,356: t15.2023.08.18 val PER: 0.1039
2026-01-05 14:30:49,356: t15.2023.08.20 val PER: 0.1041
2026-01-05 14:30:49,356: t15.2023.08.25 val PER: 0.0934
2026-01-05 14:30:49,356: t15.2023.08.27 val PER: 0.1913
2026-01-05 14:30:49,356: t15.2023.09.01 val PER: 0.0804
2026-01-05 14:30:49,356: t15.2023.09.03 val PER: 0.1710
2026-01-05 14:30:49,356: t15.2023.09.24 val PER: 0.1274
2026-01-05 14:30:49,356: t15.2023.09.29 val PER: 0.1289
2026-01-05 14:30:49,356: t15.2023.10.01 val PER: 0.1691
2026-01-05 14:30:49,356: t15.2023.10.06 val PER: 0.0936
2026-01-05 14:30:49,356: t15.2023.10.08 val PER: 0.2503
2026-01-05 14:30:49,356: t15.2023.10.13 val PER: 0.2048
2026-01-05 14:30:49,356: t15.2023.10.15 val PER: 0.1543
2026-01-05 14:30:49,356: t15.2023.10.20 val PER: 0.1946
2026-01-05 14:30:49,357: t15.2023.10.22 val PER: 0.1147
2026-01-05 14:30:49,357: t15.2023.11.03 val PER: 0.1866
2026-01-05 14:30:49,357: t15.2023.11.04 val PER: 0.0239
2026-01-05 14:30:49,357: t15.2023.11.17 val PER: 0.0467
2026-01-05 14:30:49,357: t15.2023.11.19 val PER: 0.0220
2026-01-05 14:30:49,357: t15.2023.11.26 val PER: 0.1297
2026-01-05 14:30:49,357: t15.2023.12.03 val PER: 0.1134
2026-01-05 14:30:49,357: t15.2023.12.08 val PER: 0.1025
2026-01-05 14:30:49,357: t15.2023.12.10 val PER: 0.0933
2026-01-05 14:30:49,357: t15.2023.12.17 val PER: 0.1268
2026-01-05 14:30:49,357: t15.2023.12.29 val PER: 0.1242
2026-01-05 14:30:49,357: t15.2024.02.25 val PER: 0.1081
2026-01-05 14:30:49,358: t15.2024.03.08 val PER: 0.2162
2026-01-05 14:30:49,358: t15.2024.03.15 val PER: 0.1970
2026-01-05 14:30:49,358: t15.2024.03.17 val PER: 0.1367
2026-01-05 14:30:49,358: t15.2024.05.10 val PER: 0.1620
2026-01-05 14:30:49,358: t15.2024.06.14 val PER: 0.1625
2026-01-05 14:30:49,358: t15.2024.07.19 val PER: 0.2314
2026-01-05 14:30:49,358: t15.2024.07.21 val PER: 0.0834
2026-01-05 14:30:49,358: t15.2024.07.28 val PER: 0.1331
2026-01-05 14:30:49,358: t15.2025.01.10 val PER: 0.2989
2026-01-05 14:30:49,358: t15.2025.01.12 val PER: 0.1486
2026-01-05 14:30:49,362: t15.2025.03.14 val PER: 0.3373
2026-01-05 14:30:49,362: t15.2025.03.16 val PER: 0.1846
2026-01-05 14:30:49,362: t15.2025.03.30 val PER: 0.3011
2026-01-05 14:30:49,362: t15.2025.04.13 val PER: 0.2211
2026-01-05 14:30:49,673: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_14000
2026-01-05 14:31:08,390: Train batch 14200: loss: 7.77 grad norm: 50.15 time: 0.056
2026-01-05 14:31:27,460: Train batch 14400: loss: 5.80 grad norm: 40.65 time: 0.064
2026-01-05 14:31:37,064: Running test after training batch: 14500
2026-01-05 14:31:37,184: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:31:42,264: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:31:42,298: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-05 14:31:44,179: Val batch 14500: PER (avg): 0.1509 CTC Loss (avg): 15.6053 WER(1gram): 48.73% (n=64) time: 7.115
2026-01-05 14:31:44,180: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=11
2026-01-05 14:31:44,180: t15.2023.08.13 val PER: 0.1206
2026-01-05 14:31:44,180: t15.2023.08.18 val PER: 0.1098
2026-01-05 14:31:44,180: t15.2023.08.20 val PER: 0.1064
2026-01-05 14:31:44,180: t15.2023.08.25 val PER: 0.0904
2026-01-05 14:31:44,180: t15.2023.08.27 val PER: 0.1849
2026-01-05 14:31:44,180: t15.2023.09.01 val PER: 0.0812
2026-01-05 14:31:44,180: t15.2023.09.03 val PER: 0.1663
2026-01-05 14:31:44,180: t15.2023.09.24 val PER: 0.1238
2026-01-05 14:31:44,180: t15.2023.09.29 val PER: 0.1295
2026-01-05 14:31:44,180: t15.2023.10.01 val PER: 0.1711
2026-01-05 14:31:44,180: t15.2023.10.06 val PER: 0.0904
2026-01-05 14:31:44,181: t15.2023.10.08 val PER: 0.2490
2026-01-05 14:31:44,181: t15.2023.10.13 val PER: 0.2064
2026-01-05 14:31:44,181: t15.2023.10.15 val PER: 0.1543
2026-01-05 14:31:44,181: t15.2023.10.20 val PER: 0.1711
2026-01-05 14:31:44,181: t15.2023.10.22 val PER: 0.1102
2026-01-05 14:31:44,181: t15.2023.11.03 val PER: 0.1798
2026-01-05 14:31:44,181: t15.2023.11.04 val PER: 0.0273
2026-01-05 14:31:44,181: t15.2023.11.17 val PER: 0.0435
2026-01-05 14:31:44,181: t15.2023.11.19 val PER: 0.0319
2026-01-05 14:31:44,181: t15.2023.11.26 val PER: 0.1181
2026-01-05 14:31:44,181: t15.2023.12.03 val PER: 0.1050
2026-01-05 14:31:44,181: t15.2023.12.08 val PER: 0.1025
2026-01-05 14:31:44,182: t15.2023.12.10 val PER: 0.0946
2026-01-05 14:31:44,182: t15.2023.12.17 val PER: 0.1424
2026-01-05 14:31:44,182: t15.2023.12.29 val PER: 0.1304
2026-01-05 14:31:44,182: t15.2024.02.25 val PER: 0.1152
2026-01-05 14:31:44,182: t15.2024.03.08 val PER: 0.2290
2026-01-05 14:31:44,182: t15.2024.03.15 val PER: 0.2001
2026-01-05 14:31:44,182: t15.2024.03.17 val PER: 0.1395
2026-01-05 14:31:44,182: t15.2024.05.10 val PER: 0.1694
2026-01-05 14:31:44,182: t15.2024.06.14 val PER: 0.1546
2026-01-05 14:31:44,182: t15.2024.07.19 val PER: 0.2334
2026-01-05 14:31:44,182: t15.2024.07.21 val PER: 0.0938
2026-01-05 14:31:44,182: t15.2024.07.28 val PER: 0.1397
2026-01-05 14:31:44,182: t15.2025.01.10 val PER: 0.2851
2026-01-05 14:31:44,182: t15.2025.01.12 val PER: 0.1532
2026-01-05 14:31:44,183: t15.2025.03.14 val PER: 0.3373
2026-01-05 14:31:44,183: t15.2025.03.16 val PER: 0.1806
2026-01-05 14:31:44,183: t15.2025.03.30 val PER: 0.2966
2026-01-05 14:31:44,183: t15.2025.04.13 val PER: 0.2168
2026-01-05 14:31:44,500: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_14500
2026-01-05 14:31:54,159: Train batch 14600: loss: 11.95 grad norm: 60.28 time: 0.059
2026-01-05 14:32:13,667: Train batch 14800: loss: 5.38 grad norm: 43.89 time: 0.050
2026-01-05 14:32:32,516: Train batch 15000: loss: 8.62 grad norm: 49.84 time: 0.052
2026-01-05 14:32:32,518: Running test after training batch: 15000
2026-01-05 14:32:32,666: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:32:38,255: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:32:38,290: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-05 14:32:40,181: Val batch 15000: PER (avg): 0.1499 CTC Loss (avg): 15.3322 WER(1gram): 46.45% (n=64) time: 7.663
2026-01-05 14:32:40,182: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-05 14:32:40,182: t15.2023.08.13 val PER: 0.1227
2026-01-05 14:32:40,182: t15.2023.08.18 val PER: 0.1056
2026-01-05 14:32:40,182: t15.2023.08.20 val PER: 0.1088
2026-01-05 14:32:40,182: t15.2023.08.25 val PER: 0.0934
2026-01-05 14:32:40,182: t15.2023.08.27 val PER: 0.1785
2026-01-05 14:32:40,182: t15.2023.09.01 val PER: 0.0763
2026-01-05 14:32:40,182: t15.2023.09.03 val PER: 0.1556
2026-01-05 14:32:40,182: t15.2023.09.24 val PER: 0.1201
2026-01-05 14:32:40,182: t15.2023.09.29 val PER: 0.1270
2026-01-05 14:32:40,182: t15.2023.10.01 val PER: 0.1750
2026-01-05 14:32:40,183: t15.2023.10.06 val PER: 0.0786
2026-01-05 14:32:40,183: t15.2023.10.08 val PER: 0.2517
2026-01-05 14:32:40,183: t15.2023.10.13 val PER: 0.2033
2026-01-05 14:32:40,183: t15.2023.10.15 val PER: 0.1575
2026-01-05 14:32:40,183: t15.2023.10.20 val PER: 0.1913
2026-01-05 14:32:40,183: t15.2023.10.22 val PER: 0.1036
2026-01-05 14:32:40,183: t15.2023.11.03 val PER: 0.1832
2026-01-05 14:32:40,183: t15.2023.11.04 val PER: 0.0410
2026-01-05 14:32:40,183: t15.2023.11.17 val PER: 0.0420
2026-01-05 14:32:40,183: t15.2023.11.19 val PER: 0.0299
2026-01-05 14:32:40,183: t15.2023.11.26 val PER: 0.1203
2026-01-05 14:32:40,184: t15.2023.12.03 val PER: 0.1092
2026-01-05 14:32:40,184: t15.2023.12.08 val PER: 0.1045
2026-01-05 14:32:40,184: t15.2023.12.10 val PER: 0.0867
2026-01-05 14:32:40,184: t15.2023.12.17 val PER: 0.1507
2026-01-05 14:32:40,184: t15.2023.12.29 val PER: 0.1345
2026-01-05 14:32:40,184: t15.2024.02.25 val PER: 0.1166
2026-01-05 14:32:40,184: t15.2024.03.08 val PER: 0.2248
2026-01-05 14:32:40,184: t15.2024.03.15 val PER: 0.1951
2026-01-05 14:32:40,184: t15.2024.03.17 val PER: 0.1339
2026-01-05 14:32:40,184: t15.2024.05.10 val PER: 0.1530
2026-01-05 14:32:40,184: t15.2024.06.14 val PER: 0.1593
2026-01-05 14:32:40,184: t15.2024.07.19 val PER: 0.2268
2026-01-05 14:32:40,184: t15.2024.07.21 val PER: 0.0841
2026-01-05 14:32:40,184: t15.2024.07.28 val PER: 0.1279
2026-01-05 14:32:40,184: t15.2025.01.10 val PER: 0.3030
2026-01-05 14:32:40,185: t15.2025.01.12 val PER: 0.1524
2026-01-05 14:32:40,185: t15.2025.03.14 val PER: 0.3462
2026-01-05 14:32:40,185: t15.2025.03.16 val PER: 0.1806
2026-01-05 14:32:40,185: t15.2025.03.30 val PER: 0.2977
2026-01-05 14:32:40,185: t15.2025.04.13 val PER: 0.2254
2026-01-05 14:32:40,186: New best val WER(1gram) 47.72% --> 46.45%
2026-01-05 14:32:40,186: Checkpointing model
2026-01-05 14:32:40,861: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:32:41,183: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_15000
2026-01-05 14:33:00,664: Train batch 15200: loss: 4.65 grad norm: 40.18 time: 0.058
2026-01-05 14:33:19,948: Train batch 15400: loss: 11.38 grad norm: 58.56 time: 0.050
2026-01-05 14:33:29,459: Running test after training batch: 15500
2026-01-05 14:33:29,645: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:33:34,662: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:33:34,698: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 14:33:36,581: Val batch 15500: PER (avg): 0.1489 CTC Loss (avg): 15.2387 WER(1gram): 46.45% (n=64) time: 7.122
2026-01-05 14:33:36,581: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 14:33:36,582: t15.2023.08.13 val PER: 0.1154
2026-01-05 14:33:36,582: t15.2023.08.18 val PER: 0.1048
2026-01-05 14:33:36,582: t15.2023.08.20 val PER: 0.1104
2026-01-05 14:33:36,582: t15.2023.08.25 val PER: 0.0949
2026-01-05 14:33:36,582: t15.2023.08.27 val PER: 0.1865
2026-01-05 14:33:36,582: t15.2023.09.01 val PER: 0.0787
2026-01-05 14:33:36,582: t15.2023.09.03 val PER: 0.1651
2026-01-05 14:33:36,582: t15.2023.09.24 val PER: 0.1226
2026-01-05 14:33:36,582: t15.2023.09.29 val PER: 0.1295
2026-01-05 14:33:36,582: t15.2023.10.01 val PER: 0.1671
2026-01-05 14:33:36,583: t15.2023.10.06 val PER: 0.0850
2026-01-05 14:33:36,583: t15.2023.10.08 val PER: 0.2463
2026-01-05 14:33:36,583: t15.2023.10.13 val PER: 0.1994
2026-01-05 14:33:36,583: t15.2023.10.15 val PER: 0.1536
2026-01-05 14:33:36,583: t15.2023.10.20 val PER: 0.1879
2026-01-05 14:33:36,583: t15.2023.10.22 val PER: 0.1125
2026-01-05 14:33:36,583: t15.2023.11.03 val PER: 0.1798
2026-01-05 14:33:36,583: t15.2023.11.04 val PER: 0.0307
2026-01-05 14:33:36,583: t15.2023.11.17 val PER: 0.0404
2026-01-05 14:33:36,583: t15.2023.11.19 val PER: 0.0379
2026-01-05 14:33:36,583: t15.2023.11.26 val PER: 0.1196
2026-01-05 14:33:36,583: t15.2023.12.03 val PER: 0.1071
2026-01-05 14:33:36,583: t15.2023.12.08 val PER: 0.1059
2026-01-05 14:33:36,583: t15.2023.12.10 val PER: 0.0775
2026-01-05 14:33:36,583: t15.2023.12.17 val PER: 0.1341
2026-01-05 14:33:36,583: t15.2023.12.29 val PER: 0.1263
2026-01-05 14:33:36,583: t15.2024.02.25 val PER: 0.1110
2026-01-05 14:33:36,584: t15.2024.03.08 val PER: 0.2290
2026-01-05 14:33:36,584: t15.2024.03.15 val PER: 0.1926
2026-01-05 14:33:36,584: t15.2024.03.17 val PER: 0.1457
2026-01-05 14:33:36,584: t15.2024.05.10 val PER: 0.1634
2026-01-05 14:33:36,584: t15.2024.06.14 val PER: 0.1514
2026-01-05 14:33:36,584: t15.2024.07.19 val PER: 0.2307
2026-01-05 14:33:36,584: t15.2024.07.21 val PER: 0.0897
2026-01-05 14:33:36,584: t15.2024.07.28 val PER: 0.1353
2026-01-05 14:33:36,584: t15.2025.01.10 val PER: 0.3003
2026-01-05 14:33:36,584: t15.2025.01.12 val PER: 0.1463
2026-01-05 14:33:36,584: t15.2025.03.14 val PER: 0.3299
2026-01-05 14:33:36,584: t15.2025.03.16 val PER: 0.1806
2026-01-05 14:33:36,585: t15.2025.03.30 val PER: 0.2920
2026-01-05 14:33:36,585: t15.2025.04.13 val PER: 0.2040
2026-01-05 14:33:36,892: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_15500
2026-01-05 14:33:46,267: Train batch 15600: loss: 11.34 grad norm: 57.13 time: 0.062
2026-01-05 14:34:05,084: Train batch 15800: loss: 12.75 grad norm: 61.64 time: 0.069
2026-01-05 14:34:24,420: Train batch 16000: loss: 8.38 grad norm: 46.40 time: 0.056
2026-01-05 14:34:24,421: Running test after training batch: 16000
2026-01-05 14:34:24,526: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:34:29,624: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:34:29,660: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-05 14:34:31,617: Val batch 16000: PER (avg): 0.1488 CTC Loss (avg): 15.2484 WER(1gram): 47.46% (n=64) time: 7.196
2026-01-05 14:34:31,617: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-05 14:34:31,617: t15.2023.08.13 val PER: 0.1123
2026-01-05 14:34:31,617: t15.2023.08.18 val PER: 0.1039
2026-01-05 14:34:31,618: t15.2023.08.20 val PER: 0.1104
2026-01-05 14:34:31,618: t15.2023.08.25 val PER: 0.0934
2026-01-05 14:34:31,618: t15.2023.08.27 val PER: 0.1801
2026-01-05 14:34:31,618: t15.2023.09.01 val PER: 0.0739
2026-01-05 14:34:31,618: t15.2023.09.03 val PER: 0.1532
2026-01-05 14:34:31,618: t15.2023.09.24 val PER: 0.1262
2026-01-05 14:34:31,618: t15.2023.09.29 val PER: 0.1264
2026-01-05 14:34:31,618: t15.2023.10.01 val PER: 0.1704
2026-01-05 14:34:31,618: t15.2023.10.06 val PER: 0.0861
2026-01-05 14:34:31,618: t15.2023.10.08 val PER: 0.2449
2026-01-05 14:34:31,618: t15.2023.10.13 val PER: 0.1893
2026-01-05 14:34:31,618: t15.2023.10.15 val PER: 0.1543
2026-01-05 14:34:31,618: t15.2023.10.20 val PER: 0.1812
2026-01-05 14:34:31,618: t15.2023.10.22 val PER: 0.1058
2026-01-05 14:34:31,618: t15.2023.11.03 val PER: 0.1839
2026-01-05 14:34:31,618: t15.2023.11.04 val PER: 0.0375
2026-01-05 14:34:31,619: t15.2023.11.17 val PER: 0.0358
2026-01-05 14:34:31,619: t15.2023.11.19 val PER: 0.0399
2026-01-05 14:34:31,619: t15.2023.11.26 val PER: 0.1116
2026-01-05 14:34:31,619: t15.2023.12.03 val PER: 0.1124
2026-01-05 14:34:31,619: t15.2023.12.08 val PER: 0.1072
2026-01-05 14:34:31,619: t15.2023.12.10 val PER: 0.0880
2026-01-05 14:34:31,619: t15.2023.12.17 val PER: 0.1362
2026-01-05 14:34:31,619: t15.2023.12.29 val PER: 0.1263
2026-01-05 14:34:31,619: t15.2024.02.25 val PER: 0.1110
2026-01-05 14:34:31,620: t15.2024.03.08 val PER: 0.2319
2026-01-05 14:34:31,620: t15.2024.03.15 val PER: 0.1939
2026-01-05 14:34:31,620: t15.2024.03.17 val PER: 0.1402
2026-01-05 14:34:31,620: t15.2024.05.10 val PER: 0.1605
2026-01-05 14:34:31,620: t15.2024.06.14 val PER: 0.1514
2026-01-05 14:34:31,620: t15.2024.07.19 val PER: 0.2320
2026-01-05 14:34:31,620: t15.2024.07.21 val PER: 0.0890
2026-01-05 14:34:31,620: t15.2024.07.28 val PER: 0.1390
2026-01-05 14:34:31,620: t15.2025.01.10 val PER: 0.2989
2026-01-05 14:34:31,620: t15.2025.01.12 val PER: 0.1486
2026-01-05 14:34:31,620: t15.2025.03.14 val PER: 0.3358
2026-01-05 14:34:31,620: t15.2025.03.16 val PER: 0.1780
2026-01-05 14:34:31,620: t15.2025.03.30 val PER: 0.3023
2026-01-05 14:34:31,620: t15.2025.04.13 val PER: 0.2197
2026-01-05 14:34:31,939: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_16000
2026-01-05 14:34:51,244: Train batch 16200: loss: 6.05 grad norm: 40.68 time: 0.057
2026-01-05 14:35:10,137: Train batch 16400: loss: 10.20 grad norm: 65.45 time: 0.058
2026-01-05 14:35:19,713: Running test after training batch: 16500
2026-01-05 14:35:19,863: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:35:25,136: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:35:25,173: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 14:35:27,146: Val batch 16500: PER (avg): 0.1473 CTC Loss (avg): 15.1322 WER(1gram): 47.46% (n=64) time: 7.433
2026-01-05 14:35:27,147: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=11
2026-01-05 14:35:27,147: t15.2023.08.13 val PER: 0.1143
2026-01-05 14:35:27,147: t15.2023.08.18 val PER: 0.1056
2026-01-05 14:35:27,147: t15.2023.08.20 val PER: 0.1072
2026-01-05 14:35:27,147: t15.2023.08.25 val PER: 0.0873
2026-01-05 14:35:27,147: t15.2023.08.27 val PER: 0.1752
2026-01-05 14:35:27,147: t15.2023.09.01 val PER: 0.0739
2026-01-05 14:35:27,147: t15.2023.09.03 val PER: 0.1556
2026-01-05 14:35:27,147: t15.2023.09.24 val PER: 0.1238
2026-01-05 14:35:27,147: t15.2023.09.29 val PER: 0.1213
2026-01-05 14:35:27,147: t15.2023.10.01 val PER: 0.1724
2026-01-05 14:35:27,148: t15.2023.10.06 val PER: 0.0797
2026-01-05 14:35:27,148: t15.2023.10.08 val PER: 0.2476
2026-01-05 14:35:27,148: t15.2023.10.13 val PER: 0.2017
2026-01-05 14:35:27,148: t15.2023.10.15 val PER: 0.1510
2026-01-05 14:35:27,149: t15.2023.10.20 val PER: 0.1846
2026-01-05 14:35:27,149: t15.2023.10.22 val PER: 0.1102
2026-01-05 14:35:27,149: t15.2023.11.03 val PER: 0.1825
2026-01-05 14:35:27,149: t15.2023.11.04 val PER: 0.0273
2026-01-05 14:35:27,149: t15.2023.11.17 val PER: 0.0389
2026-01-05 14:35:27,149: t15.2023.11.19 val PER: 0.0359
2026-01-05 14:35:27,150: t15.2023.11.26 val PER: 0.1043
2026-01-05 14:35:27,150: t15.2023.12.03 val PER: 0.1134
2026-01-05 14:35:27,150: t15.2023.12.08 val PER: 0.1025
2026-01-05 14:35:27,150: t15.2023.12.10 val PER: 0.0867
2026-01-05 14:35:27,150: t15.2023.12.17 val PER: 0.1331
2026-01-05 14:35:27,150: t15.2023.12.29 val PER: 0.1201
2026-01-05 14:35:27,150: t15.2024.02.25 val PER: 0.0997
2026-01-05 14:35:27,150: t15.2024.03.08 val PER: 0.2219
2026-01-05 14:35:27,150: t15.2024.03.15 val PER: 0.1926
2026-01-05 14:35:27,150: t15.2024.03.17 val PER: 0.1416
2026-01-05 14:35:27,150: t15.2024.05.10 val PER: 0.1694
2026-01-05 14:35:27,150: t15.2024.06.14 val PER: 0.1546
2026-01-05 14:35:27,150: t15.2024.07.19 val PER: 0.2287
2026-01-05 14:35:27,150: t15.2024.07.21 val PER: 0.0855
2026-01-05 14:35:27,151: t15.2024.07.28 val PER: 0.1346
2026-01-05 14:35:27,151: t15.2025.01.10 val PER: 0.2961
2026-01-05 14:35:27,151: t15.2025.01.12 val PER: 0.1463
2026-01-05 14:35:27,151: t15.2025.03.14 val PER: 0.3388
2026-01-05 14:35:27,151: t15.2025.03.16 val PER: 0.1793
2026-01-05 14:35:27,151: t15.2025.03.30 val PER: 0.3034
2026-01-05 14:35:27,151: t15.2025.04.13 val PER: 0.2154
2026-01-05 14:35:27,469: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_16500
2026-01-05 14:35:37,234: Train batch 16600: loss: 7.87 grad norm: 52.85 time: 0.053
2026-01-05 14:35:56,788: Train batch 16800: loss: 15.92 grad norm: 71.26 time: 0.064
2026-01-05 14:36:16,366: Train batch 17000: loss: 7.42 grad norm: 48.37 time: 0.082
2026-01-05 14:36:16,367: Running test after training batch: 17000
2026-01-05 14:36:16,481: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:36:21,584: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:36:21,620: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost et
2026-01-05 14:36:23,562: Val batch 17000: PER (avg): 0.1451 CTC Loss (avg): 14.9598 WER(1gram): 46.95% (n=64) time: 7.195
2026-01-05 14:36:23,562: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-05 14:36:23,562: t15.2023.08.13 val PER: 0.1123
2026-01-05 14:36:23,562: t15.2023.08.18 val PER: 0.1056
2026-01-05 14:36:23,562: t15.2023.08.20 val PER: 0.1041
2026-01-05 14:36:23,563: t15.2023.08.25 val PER: 0.0858
2026-01-05 14:36:23,563: t15.2023.08.27 val PER: 0.1881
2026-01-05 14:36:23,563: t15.2023.09.01 val PER: 0.0731
2026-01-05 14:36:23,563: t15.2023.09.03 val PER: 0.1603
2026-01-05 14:36:23,563: t15.2023.09.24 val PER: 0.1250
2026-01-05 14:36:23,563: t15.2023.09.29 val PER: 0.1200
2026-01-05 14:36:23,563: t15.2023.10.01 val PER: 0.1664
2026-01-05 14:36:23,563: t15.2023.10.06 val PER: 0.0775
2026-01-05 14:36:23,563: t15.2023.10.08 val PER: 0.2449
2026-01-05 14:36:23,563: t15.2023.10.13 val PER: 0.1885
2026-01-05 14:36:23,563: t15.2023.10.15 val PER: 0.1543
2026-01-05 14:36:23,563: t15.2023.10.20 val PER: 0.1779
2026-01-05 14:36:23,563: t15.2023.10.22 val PER: 0.1114
2026-01-05 14:36:23,563: t15.2023.11.03 val PER: 0.1832
2026-01-05 14:36:23,564: t15.2023.11.04 val PER: 0.0273
2026-01-05 14:36:23,564: t15.2023.11.17 val PER: 0.0389
2026-01-05 14:36:23,564: t15.2023.11.19 val PER: 0.0359
2026-01-05 14:36:23,564: t15.2023.11.26 val PER: 0.1022
2026-01-05 14:36:23,564: t15.2023.12.03 val PER: 0.1082
2026-01-05 14:36:23,564: t15.2023.12.08 val PER: 0.0952
2026-01-05 14:36:23,564: t15.2023.12.10 val PER: 0.0880
2026-01-05 14:36:23,564: t15.2023.12.17 val PER: 0.1331
2026-01-05 14:36:23,564: t15.2023.12.29 val PER: 0.1194
2026-01-05 14:36:23,564: t15.2024.02.25 val PER: 0.1039
2026-01-05 14:36:23,564: t15.2024.03.08 val PER: 0.2191
2026-01-05 14:36:23,564: t15.2024.03.15 val PER: 0.1889
2026-01-05 14:36:23,564: t15.2024.03.17 val PER: 0.1409
2026-01-05 14:36:23,564: t15.2024.05.10 val PER: 0.1605
2026-01-05 14:36:23,564: t15.2024.06.14 val PER: 0.1530
2026-01-05 14:36:23,565: t15.2024.07.19 val PER: 0.2254
2026-01-05 14:36:23,565: t15.2024.07.21 val PER: 0.0821
2026-01-05 14:36:23,565: t15.2024.07.28 val PER: 0.1279
2026-01-05 14:36:23,565: t15.2025.01.10 val PER: 0.2920
2026-01-05 14:36:23,565: t15.2025.01.12 val PER: 0.1409
2026-01-05 14:36:23,565: t15.2025.03.14 val PER: 0.3373
2026-01-05 14:36:23,565: t15.2025.03.16 val PER: 0.1767
2026-01-05 14:36:23,565: t15.2025.03.30 val PER: 0.2989
2026-01-05 14:36:23,565: t15.2025.04.13 val PER: 0.2126
2026-01-05 14:36:23,888: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_17000
2026-01-05 14:36:43,011: Train batch 17200: loss: 9.37 grad norm: 50.91 time: 0.084
2026-01-05 14:37:02,274: Train batch 17400: loss: 11.40 grad norm: 59.14 time: 0.071
2026-01-05 14:37:11,955: Running test after training batch: 17500
2026-01-05 14:37:12,132: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:37:17,174: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:37:17,210: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 14:37:19,162: Val batch 17500: PER (avg): 0.1458 CTC Loss (avg): 14.9900 WER(1gram): 47.21% (n=64) time: 7.206
2026-01-05 14:37:19,162: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-05 14:37:19,162: t15.2023.08.13 val PER: 0.1112
2026-01-05 14:37:19,162: t15.2023.08.18 val PER: 0.1048
2026-01-05 14:37:19,162: t15.2023.08.20 val PER: 0.1096
2026-01-05 14:37:19,163: t15.2023.08.25 val PER: 0.0873
2026-01-05 14:37:19,163: t15.2023.08.27 val PER: 0.1817
2026-01-05 14:37:19,163: t15.2023.09.01 val PER: 0.0771
2026-01-05 14:37:19,163: t15.2023.09.03 val PER: 0.1508
2026-01-05 14:37:19,163: t15.2023.09.24 val PER: 0.1286
2026-01-05 14:37:19,163: t15.2023.09.29 val PER: 0.1270
2026-01-05 14:37:19,163: t15.2023.10.01 val PER: 0.1783
2026-01-05 14:37:19,163: t15.2023.10.06 val PER: 0.0797
2026-01-05 14:37:19,163: t15.2023.10.08 val PER: 0.2436
2026-01-05 14:37:19,164: t15.2023.10.13 val PER: 0.1901
2026-01-05 14:37:19,164: t15.2023.10.15 val PER: 0.1523
2026-01-05 14:37:19,164: t15.2023.10.20 val PER: 0.1846
2026-01-05 14:37:19,164: t15.2023.10.22 val PER: 0.1114
2026-01-05 14:37:19,164: t15.2023.11.03 val PER: 0.1798
2026-01-05 14:37:19,164: t15.2023.11.04 val PER: 0.0273
2026-01-05 14:37:19,164: t15.2023.11.17 val PER: 0.0373
2026-01-05 14:37:19,164: t15.2023.11.19 val PER: 0.0339
2026-01-05 14:37:19,164: t15.2023.11.26 val PER: 0.1072
2026-01-05 14:37:19,165: t15.2023.12.03 val PER: 0.1040
2026-01-05 14:37:19,165: t15.2023.12.08 val PER: 0.1012
2026-01-05 14:37:19,165: t15.2023.12.10 val PER: 0.0841
2026-01-05 14:37:19,165: t15.2023.12.17 val PER: 0.1310
2026-01-05 14:37:19,165: t15.2023.12.29 val PER: 0.1249
2026-01-05 14:37:19,165: t15.2024.02.25 val PER: 0.1011
2026-01-05 14:37:19,165: t15.2024.03.08 val PER: 0.2233
2026-01-05 14:37:19,165: t15.2024.03.15 val PER: 0.1864
2026-01-05 14:37:19,165: t15.2024.03.17 val PER: 0.1388
2026-01-05 14:37:19,165: t15.2024.05.10 val PER: 0.1486
2026-01-05 14:37:19,166: t15.2024.06.14 val PER: 0.1498
2026-01-05 14:37:19,166: t15.2024.07.19 val PER: 0.2314
2026-01-05 14:37:19,166: t15.2024.07.21 val PER: 0.0848
2026-01-05 14:37:19,166: t15.2024.07.28 val PER: 0.1353
2026-01-05 14:37:19,166: t15.2025.01.10 val PER: 0.2893
2026-01-05 14:37:19,166: t15.2025.01.12 val PER: 0.1378
2026-01-05 14:37:19,166: t15.2025.03.14 val PER: 0.3299
2026-01-05 14:37:19,166: t15.2025.03.16 val PER: 0.1728
2026-01-05 14:37:19,166: t15.2025.03.30 val PER: 0.2931
2026-01-05 14:37:19,167: t15.2025.04.13 val PER: 0.2154
2026-01-05 14:37:19,485: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_17500
2026-01-05 14:37:28,821: Train batch 17600: loss: 9.03 grad norm: 51.14 time: 0.051
2026-01-05 14:37:48,067: Train batch 17800: loss: 5.74 grad norm: 47.28 time: 0.042
2026-01-05 14:38:07,149: Train batch 18000: loss: 10.39 grad norm: 64.37 time: 0.061
2026-01-05 14:38:07,151: Running test after training batch: 18000
2026-01-05 14:38:07,313: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:38:12,392: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-05 14:38:12,431: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 14:38:14,474: Val batch 18000: PER (avg): 0.1450 CTC Loss (avg): 14.9680 WER(1gram): 46.70% (n=64) time: 7.323
2026-01-05 14:38:14,475: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-05 14:38:14,475: t15.2023.08.13 val PER: 0.1123
2026-01-05 14:38:14,475: t15.2023.08.18 val PER: 0.1006
2026-01-05 14:38:14,475: t15.2023.08.20 val PER: 0.1048
2026-01-05 14:38:14,475: t15.2023.08.25 val PER: 0.0873
2026-01-05 14:38:14,475: t15.2023.08.27 val PER: 0.1897
2026-01-05 14:38:14,476: t15.2023.09.01 val PER: 0.0771
2026-01-05 14:38:14,476: t15.2023.09.03 val PER: 0.1556
2026-01-05 14:38:14,476: t15.2023.09.24 val PER: 0.1262
2026-01-05 14:38:14,476: t15.2023.09.29 val PER: 0.1264
2026-01-05 14:38:14,476: t15.2023.10.01 val PER: 0.1697
2026-01-05 14:38:14,476: t15.2023.10.06 val PER: 0.0807
2026-01-05 14:38:14,476: t15.2023.10.08 val PER: 0.2382
2026-01-05 14:38:14,476: t15.2023.10.13 val PER: 0.1924
2026-01-05 14:38:14,476: t15.2023.10.15 val PER: 0.1543
2026-01-05 14:38:14,476: t15.2023.10.20 val PER: 0.1846
2026-01-05 14:38:14,476: t15.2023.10.22 val PER: 0.1069
2026-01-05 14:38:14,476: t15.2023.11.03 val PER: 0.1818
2026-01-05 14:38:14,476: t15.2023.11.04 val PER: 0.0341
2026-01-05 14:38:14,477: t15.2023.11.17 val PER: 0.0327
2026-01-05 14:38:14,477: t15.2023.11.19 val PER: 0.0359
2026-01-05 14:38:14,477: t15.2023.11.26 val PER: 0.1014
2026-01-05 14:38:14,477: t15.2023.12.03 val PER: 0.1082
2026-01-05 14:38:14,477: t15.2023.12.08 val PER: 0.1032
2026-01-05 14:38:14,477: t15.2023.12.10 val PER: 0.0815
2026-01-05 14:38:14,477: t15.2023.12.17 val PER: 0.1289
2026-01-05 14:38:14,477: t15.2023.12.29 val PER: 0.1201
2026-01-05 14:38:14,477: t15.2024.02.25 val PER: 0.1011
2026-01-05 14:38:14,477: t15.2024.03.08 val PER: 0.2148
2026-01-05 14:38:14,477: t15.2024.03.15 val PER: 0.1926
2026-01-05 14:38:14,477: t15.2024.03.17 val PER: 0.1339
2026-01-05 14:38:14,478: t15.2024.05.10 val PER: 0.1620
2026-01-05 14:38:14,478: t15.2024.06.14 val PER: 0.1483
2026-01-05 14:38:14,478: t15.2024.07.19 val PER: 0.2261
2026-01-05 14:38:14,478: t15.2024.07.21 val PER: 0.0855
2026-01-05 14:38:14,478: t15.2024.07.28 val PER: 0.1287
2026-01-05 14:38:14,478: t15.2025.01.10 val PER: 0.2865
2026-01-05 14:38:14,478: t15.2025.01.12 val PER: 0.1416
2026-01-05 14:38:14,478: t15.2025.03.14 val PER: 0.3299
2026-01-05 14:38:14,478: t15.2025.03.16 val PER: 0.1675
2026-01-05 14:38:14,478: t15.2025.03.30 val PER: 0.2920
2026-01-05 14:38:14,478: t15.2025.04.13 val PER: 0.2211
2026-01-05 14:38:14,811: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_18000
2026-01-05 14:38:34,132: Train batch 18200: loss: 7.49 grad norm: 47.89 time: 0.073
2026-01-05 14:38:53,087: Train batch 18400: loss: 4.40 grad norm: 43.37 time: 0.059
2026-01-05 14:39:02,678: Running test after training batch: 18500
2026-01-05 14:39:02,840: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:39:07,977: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:39:08,014: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 14:39:10,002: Val batch 18500: PER (avg): 0.1453 CTC Loss (avg): 14.9856 WER(1gram): 46.45% (n=64) time: 7.324
2026-01-05 14:39:10,002: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-05 14:39:10,003: t15.2023.08.13 val PER: 0.1123
2026-01-05 14:39:10,003: t15.2023.08.18 val PER: 0.1006
2026-01-05 14:39:10,003: t15.2023.08.20 val PER: 0.1096
2026-01-05 14:39:10,003: t15.2023.08.25 val PER: 0.0919
2026-01-05 14:39:10,003: t15.2023.08.27 val PER: 0.1833
2026-01-05 14:39:10,003: t15.2023.09.01 val PER: 0.0771
2026-01-05 14:39:10,003: t15.2023.09.03 val PER: 0.1520
2026-01-05 14:39:10,003: t15.2023.09.24 val PER: 0.1214
2026-01-05 14:39:10,003: t15.2023.09.29 val PER: 0.1213
2026-01-05 14:39:10,003: t15.2023.10.01 val PER: 0.1691
2026-01-05 14:39:10,003: t15.2023.10.06 val PER: 0.0807
2026-01-05 14:39:10,003: t15.2023.10.08 val PER: 0.2395
2026-01-05 14:39:10,004: t15.2023.10.13 val PER: 0.1924
2026-01-05 14:39:10,004: t15.2023.10.15 val PER: 0.1523
2026-01-05 14:39:10,004: t15.2023.10.20 val PER: 0.1913
2026-01-05 14:39:10,004: t15.2023.10.22 val PER: 0.1102
2026-01-05 14:39:10,004: t15.2023.11.03 val PER: 0.1805
2026-01-05 14:39:10,004: t15.2023.11.04 val PER: 0.0341
2026-01-05 14:39:10,004: t15.2023.11.17 val PER: 0.0389
2026-01-05 14:39:10,004: t15.2023.11.19 val PER: 0.0359
2026-01-05 14:39:10,004: t15.2023.11.26 val PER: 0.1087
2026-01-05 14:39:10,005: t15.2023.12.03 val PER: 0.1040
2026-01-05 14:39:10,005: t15.2023.12.08 val PER: 0.1019
2026-01-05 14:39:10,005: t15.2023.12.10 val PER: 0.0815
2026-01-05 14:39:10,005: t15.2023.12.17 val PER: 0.1299
2026-01-05 14:39:10,005: t15.2023.12.29 val PER: 0.1208
2026-01-05 14:39:10,005: t15.2024.02.25 val PER: 0.1067
2026-01-05 14:39:10,005: t15.2024.03.08 val PER: 0.2205
2026-01-05 14:39:10,005: t15.2024.03.15 val PER: 0.1870
2026-01-05 14:39:10,005: t15.2024.03.17 val PER: 0.1395
2026-01-05 14:39:10,005: t15.2024.05.10 val PER: 0.1545
2026-01-05 14:39:10,005: t15.2024.06.14 val PER: 0.1546
2026-01-05 14:39:10,005: t15.2024.07.19 val PER: 0.2301
2026-01-05 14:39:10,005: t15.2024.07.21 val PER: 0.0841
2026-01-05 14:39:10,005: t15.2024.07.28 val PER: 0.1324
2026-01-05 14:39:10,005: t15.2025.01.10 val PER: 0.2851
2026-01-05 14:39:10,006: t15.2025.01.12 val PER: 0.1393
2026-01-05 14:39:10,006: t15.2025.03.14 val PER: 0.3284
2026-01-05 14:39:10,006: t15.2025.03.16 val PER: 0.1754
2026-01-05 14:39:10,006: t15.2025.03.30 val PER: 0.2954
2026-01-05 14:39:10,006: t15.2025.04.13 val PER: 0.2154
2026-01-05 14:39:10,330: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_18500
2026-01-05 14:39:19,674: Train batch 18600: loss: 11.83 grad norm: 61.00 time: 0.068
2026-01-05 14:39:38,670: Train batch 18800: loss: 8.14 grad norm: 52.50 time: 0.064
2026-01-05 14:39:57,902: Train batch 19000: loss: 7.75 grad norm: 44.71 time: 0.064
2026-01-05 14:39:57,903: Running test after training batch: 19000
2026-01-05 14:39:58,055: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:40:03,104: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:40:03,142: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 14:40:05,175: Val batch 19000: PER (avg): 0.1449 CTC Loss (avg): 15.0090 WER(1gram): 46.19% (n=64) time: 7.272
2026-01-05 14:40:05,175: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-05 14:40:05,175: t15.2023.08.13 val PER: 0.1123
2026-01-05 14:40:05,175: t15.2023.08.18 val PER: 0.0981
2026-01-05 14:40:05,175: t15.2023.08.20 val PER: 0.1064
2026-01-05 14:40:05,175: t15.2023.08.25 val PER: 0.0873
2026-01-05 14:40:05,176: t15.2023.08.27 val PER: 0.1768
2026-01-05 14:40:05,176: t15.2023.09.01 val PER: 0.0787
2026-01-05 14:40:05,176: t15.2023.09.03 val PER: 0.1568
2026-01-05 14:40:05,176: t15.2023.09.24 val PER: 0.1274
2026-01-05 14:40:05,176: t15.2023.09.29 val PER: 0.1213
2026-01-05 14:40:05,176: t15.2023.10.01 val PER: 0.1684
2026-01-05 14:40:05,176: t15.2023.10.06 val PER: 0.0786
2026-01-05 14:40:05,176: t15.2023.10.08 val PER: 0.2395
2026-01-05 14:40:05,176: t15.2023.10.13 val PER: 0.1908
2026-01-05 14:40:05,176: t15.2023.10.15 val PER: 0.1510
2026-01-05 14:40:05,176: t15.2023.10.20 val PER: 0.1913
2026-01-05 14:40:05,176: t15.2023.10.22 val PER: 0.1114
2026-01-05 14:40:05,176: t15.2023.11.03 val PER: 0.1832
2026-01-05 14:40:05,176: t15.2023.11.04 val PER: 0.0341
2026-01-05 14:40:05,177: t15.2023.11.17 val PER: 0.0327
2026-01-05 14:40:05,177: t15.2023.11.19 val PER: 0.0299
2026-01-05 14:40:05,177: t15.2023.11.26 val PER: 0.1065
2026-01-05 14:40:05,177: t15.2023.12.03 val PER: 0.1071
2026-01-05 14:40:05,177: t15.2023.12.08 val PER: 0.0959
2026-01-05 14:40:05,177: t15.2023.12.10 val PER: 0.0841
2026-01-05 14:40:05,177: t15.2023.12.17 val PER: 0.1310
2026-01-05 14:40:05,177: t15.2023.12.29 val PER: 0.1201
2026-01-05 14:40:05,177: t15.2024.02.25 val PER: 0.1053
2026-01-05 14:40:05,177: t15.2024.03.08 val PER: 0.2205
2026-01-05 14:40:05,177: t15.2024.03.15 val PER: 0.1807
2026-01-05 14:40:05,178: t15.2024.03.17 val PER: 0.1416
2026-01-05 14:40:05,178: t15.2024.05.10 val PER: 0.1471
2026-01-05 14:40:05,178: t15.2024.06.14 val PER: 0.1530
2026-01-05 14:40:05,178: t15.2024.07.19 val PER: 0.2320
2026-01-05 14:40:05,178: t15.2024.07.21 val PER: 0.0883
2026-01-05 14:40:05,179: t15.2024.07.28 val PER: 0.1324
2026-01-05 14:40:05,179: t15.2025.01.10 val PER: 0.2920
2026-01-05 14:40:05,179: t15.2025.01.12 val PER: 0.1424
2026-01-05 14:40:05,179: t15.2025.03.14 val PER: 0.3388
2026-01-05 14:40:05,179: t15.2025.03.16 val PER: 0.1741
2026-01-05 14:40:05,179: t15.2025.03.30 val PER: 0.2885
2026-01-05 14:40:05,179: t15.2025.04.13 val PER: 0.2111
2026-01-05 14:40:05,180: New best val WER(1gram) 46.45% --> 46.19%
2026-01-05 14:40:05,180: Checkpointing model
2026-01-05 14:40:05,865: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:40:06,218: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_19000
2026-01-05 14:40:24,933: Train batch 19200: loss: 5.54 grad norm: 45.73 time: 0.063
2026-01-05 14:40:44,042: Train batch 19400: loss: 4.57 grad norm: 35.62 time: 0.053
2026-01-05 14:40:54,039: Running test after training batch: 19500
2026-01-05 14:40:54,196: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:40:59,242: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:40:59,280: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 14:41:01,293: Val batch 19500: PER (avg): 0.1447 CTC Loss (avg): 14.9379 WER(1gram): 46.70% (n=64) time: 7.253
2026-01-05 14:41:01,293: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-05 14:41:01,293: t15.2023.08.13 val PER: 0.1143
2026-01-05 14:41:01,294: t15.2023.08.18 val PER: 0.1023
2026-01-05 14:41:01,294: t15.2023.08.20 val PER: 0.1088
2026-01-05 14:41:01,294: t15.2023.08.25 val PER: 0.0873
2026-01-05 14:41:01,294: t15.2023.08.27 val PER: 0.1881
2026-01-05 14:41:01,294: t15.2023.09.01 val PER: 0.0795
2026-01-05 14:41:01,294: t15.2023.09.03 val PER: 0.1496
2026-01-05 14:41:01,294: t15.2023.09.24 val PER: 0.1201
2026-01-05 14:41:01,294: t15.2023.09.29 val PER: 0.1244
2026-01-05 14:41:01,294: t15.2023.10.01 val PER: 0.1724
2026-01-05 14:41:01,294: t15.2023.10.06 val PER: 0.0775
2026-01-05 14:41:01,294: t15.2023.10.08 val PER: 0.2368
2026-01-05 14:41:01,295: t15.2023.10.13 val PER: 0.1893
2026-01-05 14:41:01,295: t15.2023.10.15 val PER: 0.1510
2026-01-05 14:41:01,295: t15.2023.10.20 val PER: 0.1879
2026-01-05 14:41:01,295: t15.2023.10.22 val PER: 0.1036
2026-01-05 14:41:01,295: t15.2023.11.03 val PER: 0.1818
2026-01-05 14:41:01,295: t15.2023.11.04 val PER: 0.0375
2026-01-05 14:41:01,295: t15.2023.11.17 val PER: 0.0373
2026-01-05 14:41:01,295: t15.2023.11.19 val PER: 0.0319
2026-01-05 14:41:01,295: t15.2023.11.26 val PER: 0.1022
2026-01-05 14:41:01,295: t15.2023.12.03 val PER: 0.1050
2026-01-05 14:41:01,295: t15.2023.12.08 val PER: 0.0992
2026-01-05 14:41:01,295: t15.2023.12.10 val PER: 0.0815
2026-01-05 14:41:01,295: t15.2023.12.17 val PER: 0.1216
2026-01-05 14:41:01,295: t15.2023.12.29 val PER: 0.1208
2026-01-05 14:41:01,296: t15.2024.02.25 val PER: 0.1053
2026-01-05 14:41:01,296: t15.2024.03.08 val PER: 0.2248
2026-01-05 14:41:01,296: t15.2024.03.15 val PER: 0.1820
2026-01-05 14:41:01,296: t15.2024.03.17 val PER: 0.1395
2026-01-05 14:41:01,296: t15.2024.05.10 val PER: 0.1516
2026-01-05 14:41:01,296: t15.2024.06.14 val PER: 0.1467
2026-01-05 14:41:01,296: t15.2024.07.19 val PER: 0.2327
2026-01-05 14:41:01,296: t15.2024.07.21 val PER: 0.0848
2026-01-05 14:41:01,296: t15.2024.07.28 val PER: 0.1331
2026-01-05 14:41:01,297: t15.2025.01.10 val PER: 0.2893
2026-01-05 14:41:01,297: t15.2025.01.12 val PER: 0.1424
2026-01-05 14:41:01,297: t15.2025.03.14 val PER: 0.3314
2026-01-05 14:41:01,297: t15.2025.03.16 val PER: 0.1728
2026-01-05 14:41:01,297: t15.2025.03.30 val PER: 0.2920
2026-01-05 14:41:01,297: t15.2025.04.13 val PER: 0.2168
2026-01-05 14:41:01,617: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_19500
2026-01-05 14:41:11,041: Train batch 19600: loss: 7.54 grad norm: 48.55 time: 0.057
2026-01-05 14:41:30,540: Train batch 19800: loss: 7.08 grad norm: 53.04 time: 0.055
2026-01-05 14:41:49,463: Running test after training batch: 19999
2026-01-05 14:41:49,559: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:41:54,725: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:41:54,763: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 14:41:56,810: Val batch 19999: PER (avg): 0.1455 CTC Loss (avg): 14.9355 WER(1gram): 45.18% (n=64) time: 7.346
2026-01-05 14:41:56,810: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-05 14:41:56,810: t15.2023.08.13 val PER: 0.1091
2026-01-05 14:41:56,810: t15.2023.08.18 val PER: 0.0989
2026-01-05 14:41:56,811: t15.2023.08.20 val PER: 0.1080
2026-01-05 14:41:56,811: t15.2023.08.25 val PER: 0.0904
2026-01-05 14:41:56,811: t15.2023.08.27 val PER: 0.1897
2026-01-05 14:41:56,811: t15.2023.09.01 val PER: 0.0755
2026-01-05 14:41:56,811: t15.2023.09.03 val PER: 0.1520
2026-01-05 14:41:56,811: t15.2023.09.24 val PER: 0.1250
2026-01-05 14:41:56,811: t15.2023.09.29 val PER: 0.1232
2026-01-05 14:41:56,811: t15.2023.10.01 val PER: 0.1678
2026-01-05 14:41:56,812: t15.2023.10.06 val PER: 0.0797
2026-01-05 14:41:56,812: t15.2023.10.08 val PER: 0.2409
2026-01-05 14:41:56,812: t15.2023.10.13 val PER: 0.1901
2026-01-05 14:41:56,812: t15.2023.10.15 val PER: 0.1562
2026-01-05 14:41:56,812: t15.2023.10.20 val PER: 0.1913
2026-01-05 14:41:56,812: t15.2023.10.22 val PER: 0.1069
2026-01-05 14:41:56,812: t15.2023.11.03 val PER: 0.1845
2026-01-05 14:41:56,812: t15.2023.11.04 val PER: 0.0341
2026-01-05 14:41:56,812: t15.2023.11.17 val PER: 0.0358
2026-01-05 14:41:56,812: t15.2023.11.19 val PER: 0.0339
2026-01-05 14:41:56,812: t15.2023.11.26 val PER: 0.1043
2026-01-05 14:41:56,812: t15.2023.12.03 val PER: 0.1019
2026-01-05 14:41:56,812: t15.2023.12.08 val PER: 0.1025
2026-01-05 14:41:56,812: t15.2023.12.10 val PER: 0.0880
2026-01-05 14:41:56,812: t15.2023.12.17 val PER: 0.1258
2026-01-05 14:41:56,813: t15.2023.12.29 val PER: 0.1215
2026-01-05 14:41:56,813: t15.2024.02.25 val PER: 0.1039
2026-01-05 14:41:56,813: t15.2024.03.08 val PER: 0.2191
2026-01-05 14:41:56,813: t15.2024.03.15 val PER: 0.1832
2026-01-05 14:41:56,813: t15.2024.03.17 val PER: 0.1374
2026-01-05 14:41:56,813: t15.2024.05.10 val PER: 0.1560
2026-01-05 14:41:56,813: t15.2024.06.14 val PER: 0.1546
2026-01-05 14:41:56,813: t15.2024.07.19 val PER: 0.2320
2026-01-05 14:41:56,813: t15.2024.07.21 val PER: 0.0883
2026-01-05 14:41:56,813: t15.2024.07.28 val PER: 0.1316
2026-01-05 14:41:56,813: t15.2025.01.10 val PER: 0.2851
2026-01-05 14:41:56,813: t15.2025.01.12 val PER: 0.1432
2026-01-05 14:41:56,813: t15.2025.03.14 val PER: 0.3358
2026-01-05 14:41:56,813: t15.2025.03.16 val PER: 0.1780
2026-01-05 14:41:56,813: t15.2025.03.30 val PER: 0.2954
2026-01-05 14:41:56,813: t15.2025.04.13 val PER: 0.2183
2026-01-05 14:41:56,814: New best val WER(1gram) 46.19% --> 45.18%
2026-01-05 14:41:56,815: Checkpointing model
2026-01-05 14:41:57,511: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:41:57,834: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p005_wd1e-5/checkpoint/checkpoint_batch_19999
2026-01-05 14:41:57,962: Best avg val PER achieved: 0.14551
2026-01-05 14:41:57,962: Total training time: 37.80 minutes

=== RUN p008_wd1e-5.yaml ===
2026-01-05 14:42:03,422: Using device: cuda:0
2026-01-05 14:42:05,167: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-05 14:42:05,193: Using 45 sessions after filtering (from 45).
2026-01-05 14:42:05,604: Using torch.compile (if available)
2026-01-05 14:42:05,605: torch.compile not available (torch<2.0). Skipping.
2026-01-05 14:42:05,605: Initialized RNN decoding model
2026-01-05 14:42:05,605: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-05 14:42:05,605: Model has 44,907,305 parameters
2026-01-05 14:42:05,605: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-05 14:42:06,939: Successfully initialized datasets
2026-01-05 14:42:06,940: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-05 14:42:07,976: Train batch 0: loss: 579.26 grad norm: 1409.98 time: 0.192
2026-01-05 14:42:07,977: Running test after training batch: 0
2026-01-05 14:42:08,101: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:42:13,751: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-05 14:42:14,555: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-05 14:42:52,215: Val batch 0: PER (avg): 1.4292 CTC Loss (avg): 633.1652 WER(1gram): 100.00% (n=64) time: 44.238
2026-01-05 14:42:52,216: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-05 14:42:52,216: t15.2023.08.13 val PER: 1.3046
2026-01-05 14:42:52,216: t15.2023.08.18 val PER: 1.4275
2026-01-05 14:42:52,216: t15.2023.08.20 val PER: 1.3034
2026-01-05 14:42:52,216: t15.2023.08.25 val PER: 1.3389
2026-01-05 14:42:52,217: t15.2023.08.27 val PER: 1.2524
2026-01-05 14:42:52,217: t15.2023.09.01 val PER: 1.4529
2026-01-05 14:42:52,217: t15.2023.09.03 val PER: 1.3112
2026-01-05 14:42:52,217: t15.2023.09.24 val PER: 1.5388
2026-01-05 14:42:52,217: t15.2023.09.29 val PER: 1.4671
2026-01-05 14:42:52,217: t15.2023.10.01 val PER: 1.2133
2026-01-05 14:42:52,217: t15.2023.10.06 val PER: 1.4865
2026-01-05 14:42:52,217: t15.2023.10.08 val PER: 1.1867
2026-01-05 14:42:52,217: t15.2023.10.13 val PER: 1.3980
2026-01-05 14:42:52,217: t15.2023.10.15 val PER: 1.3883
2026-01-05 14:42:52,218: t15.2023.10.20 val PER: 1.4899
2026-01-05 14:42:52,218: t15.2023.10.22 val PER: 1.3909
2026-01-05 14:42:52,218: t15.2023.11.03 val PER: 1.5916
2026-01-05 14:42:52,218: t15.2023.11.04 val PER: 2.0307
2026-01-05 14:42:52,218: t15.2023.11.17 val PER: 1.9565
2026-01-05 14:42:52,218: t15.2023.11.19 val PER: 1.6766
2026-01-05 14:42:52,218: t15.2023.11.26 val PER: 1.5370
2026-01-05 14:42:52,218: t15.2023.12.03 val PER: 1.4244
2026-01-05 14:42:52,218: t15.2023.12.08 val PER: 1.4507
2026-01-05 14:42:52,218: t15.2023.12.10 val PER: 1.6925
2026-01-05 14:42:52,218: t15.2023.12.17 val PER: 1.3056
2026-01-05 14:42:52,219: t15.2023.12.29 val PER: 1.4097
2026-01-05 14:42:52,219: t15.2024.02.25 val PER: 1.4256
2026-01-05 14:42:52,219: t15.2024.03.08 val PER: 1.3229
2026-01-05 14:42:52,219: t15.2024.03.15 val PER: 1.3196
2026-01-05 14:42:52,219: t15.2024.03.17 val PER: 1.4017
2026-01-05 14:42:52,219: t15.2024.05.10 val PER: 1.3269
2026-01-05 14:42:52,219: t15.2024.06.14 val PER: 1.5284
2026-01-05 14:42:52,219: t15.2024.07.19 val PER: 1.0811
2026-01-05 14:42:52,219: t15.2024.07.21 val PER: 1.6352
2026-01-05 14:42:52,219: t15.2024.07.28 val PER: 1.6559
2026-01-05 14:42:52,220: t15.2025.01.10 val PER: 1.0909
2026-01-05 14:42:52,220: t15.2025.01.12 val PER: 1.7683
2026-01-05 14:42:52,220: t15.2025.03.14 val PER: 1.0385
2026-01-05 14:42:52,220: t15.2025.03.16 val PER: 1.6178
2026-01-05 14:42:52,220: t15.2025.03.30 val PER: 1.2816
2026-01-05 14:42:52,220: t15.2025.04.13 val PER: 1.5835
2026-01-05 14:42:52,220: New best val WER(1gram) inf% --> 100.00%
2026-01-05 14:42:52,220: Checkpointing model
2026-01-05 14:42:52,520: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:42:52,824: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_0
2026-01-05 14:43:12,150: Train batch 200: loss: 77.59 grad norm: 109.70 time: 0.054
2026-01-05 14:43:30,920: Train batch 400: loss: 53.79 grad norm: 94.98 time: 0.064
2026-01-05 14:43:40,366: Running test after training batch: 500
2026-01-05 14:43:40,522: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:43:45,680: WER debug example
  GT : you can see the code at this point as well
  PR : yoor and ease thus uhde at this uhde is aisle
2026-01-05 14:43:45,714: WER debug example
  GT : how does it keep the cost down
  PR : houde does it yule thus ass tides
2026-01-05 14:43:48,094: Val batch 500: PER (avg): 0.5184 CTC Loss (avg): 55.5736 WER(1gram): 89.09% (n=64) time: 7.727
2026-01-05 14:43:48,095: WER lens: avg_true_words=6.16 avg_pred_words=5.64 max_pred_words=11
2026-01-05 14:43:48,095: t15.2023.08.13 val PER: 0.4522
2026-01-05 14:43:48,095: t15.2023.08.18 val PER: 0.4443
2026-01-05 14:43:48,095: t15.2023.08.20 val PER: 0.4416
2026-01-05 14:43:48,095: t15.2023.08.25 val PER: 0.4322
2026-01-05 14:43:48,095: t15.2023.08.27 val PER: 0.5322
2026-01-05 14:43:48,095: t15.2023.09.01 val PER: 0.4172
2026-01-05 14:43:48,095: t15.2023.09.03 val PER: 0.5059
2026-01-05 14:43:48,095: t15.2023.09.24 val PER: 0.4272
2026-01-05 14:43:48,095: t15.2023.09.29 val PER: 0.4665
2026-01-05 14:43:48,096: t15.2023.10.01 val PER: 0.5205
2026-01-05 14:43:48,096: t15.2023.10.06 val PER: 0.4220
2026-01-05 14:43:48,096: t15.2023.10.08 val PER: 0.5372
2026-01-05 14:43:48,096: t15.2023.10.13 val PER: 0.5710
2026-01-05 14:43:48,096: t15.2023.10.15 val PER: 0.5036
2026-01-05 14:43:48,096: t15.2023.10.20 val PER: 0.4530
2026-01-05 14:43:48,096: t15.2023.10.22 val PER: 0.4399
2026-01-05 14:43:48,096: t15.2023.11.03 val PER: 0.5014
2026-01-05 14:43:48,096: t15.2023.11.04 val PER: 0.2594
2026-01-05 14:43:48,096: t15.2023.11.17 val PER: 0.3639
2026-01-05 14:43:48,096: t15.2023.11.19 val PER: 0.3074
2026-01-05 14:43:48,096: t15.2023.11.26 val PER: 0.5493
2026-01-05 14:43:48,097: t15.2023.12.03 val PER: 0.5021
2026-01-05 14:43:48,097: t15.2023.12.08 val PER: 0.5133
2026-01-05 14:43:48,097: t15.2023.12.10 val PER: 0.4586
2026-01-05 14:43:48,097: t15.2023.12.17 val PER: 0.5665
2026-01-05 14:43:48,097: t15.2023.12.29 val PER: 0.5525
2026-01-05 14:43:48,097: t15.2024.02.25 val PER: 0.4860
2026-01-05 14:43:48,097: t15.2024.03.08 val PER: 0.6188
2026-01-05 14:43:48,097: t15.2024.03.15 val PER: 0.5572
2026-01-05 14:43:48,097: t15.2024.03.17 val PER: 0.5042
2026-01-05 14:43:48,098: t15.2024.05.10 val PER: 0.5409
2026-01-05 14:43:48,098: t15.2024.06.14 val PER: 0.5142
2026-01-05 14:43:48,098: t15.2024.07.19 val PER: 0.6790
2026-01-05 14:43:48,098: t15.2024.07.21 val PER: 0.4752
2026-01-05 14:43:48,098: t15.2024.07.28 val PER: 0.5118
2026-01-05 14:43:48,098: t15.2025.01.10 val PER: 0.7521
2026-01-05 14:43:48,098: t15.2025.01.12 val PER: 0.5627
2026-01-05 14:43:48,098: t15.2025.03.14 val PER: 0.7500
2026-01-05 14:43:48,098: t15.2025.03.16 val PER: 0.5877
2026-01-05 14:43:48,098: t15.2025.03.30 val PER: 0.7368
2026-01-05 14:43:48,098: t15.2025.04.13 val PER: 0.5763
2026-01-05 14:43:48,099: New best val WER(1gram) 100.00% --> 89.09%
2026-01-05 14:43:48,099: Checkpointing model
2026-01-05 14:43:48,777: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:43:49,081: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_500
2026-01-05 14:43:58,499: Train batch 600: loss: 48.76 grad norm: 75.28 time: 0.078
2026-01-05 14:44:17,172: Train batch 800: loss: 41.17 grad norm: 89.04 time: 0.057
2026-01-05 14:44:36,047: Train batch 1000: loss: 42.62 grad norm: 76.57 time: 0.066
2026-01-05 14:44:36,048: Running test after training batch: 1000
2026-01-05 14:44:36,152: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:44:41,465: WER debug example
  GT : you can see the code at this point as well
  PR : wooed end ease thus code it this uhde is while
2026-01-05 14:44:41,499: WER debug example
  GT : how does it keep the cost down
  PR : houde is it eke thus us it
2026-01-05 14:44:43,463: Val batch 1000: PER (avg): 0.4068 CTC Loss (avg): 42.2098 WER(1gram): 80.46% (n=64) time: 7.415
2026-01-05 14:44:43,464: WER lens: avg_true_words=6.16 avg_pred_words=5.59 max_pred_words=11
2026-01-05 14:44:43,464: t15.2023.08.13 val PER: 0.3721
2026-01-05 14:44:43,464: t15.2023.08.18 val PER: 0.3328
2026-01-05 14:44:43,464: t15.2023.08.20 val PER: 0.3264
2026-01-05 14:44:43,464: t15.2023.08.25 val PER: 0.3012
2026-01-05 14:44:43,464: t15.2023.08.27 val PER: 0.4196
2026-01-05 14:44:43,464: t15.2023.09.01 val PER: 0.3060
2026-01-05 14:44:43,464: t15.2023.09.03 val PER: 0.3967
2026-01-05 14:44:43,464: t15.2023.09.24 val PER: 0.3350
2026-01-05 14:44:43,464: t15.2023.09.29 val PER: 0.3618
2026-01-05 14:44:43,464: t15.2023.10.01 val PER: 0.4003
2026-01-05 14:44:43,465: t15.2023.10.06 val PER: 0.3122
2026-01-05 14:44:43,465: t15.2023.10.08 val PER: 0.4547
2026-01-05 14:44:43,465: t15.2023.10.13 val PER: 0.4631
2026-01-05 14:44:43,465: t15.2023.10.15 val PER: 0.3784
2026-01-05 14:44:43,465: t15.2023.10.20 val PER: 0.3725
2026-01-05 14:44:43,465: t15.2023.10.22 val PER: 0.3508
2026-01-05 14:44:43,465: t15.2023.11.03 val PER: 0.3969
2026-01-05 14:44:43,465: t15.2023.11.04 val PER: 0.1399
2026-01-05 14:44:43,465: t15.2023.11.17 val PER: 0.2628
2026-01-05 14:44:43,465: t15.2023.11.19 val PER: 0.1976
2026-01-05 14:44:43,466: t15.2023.11.26 val PER: 0.4486
2026-01-05 14:44:43,466: t15.2023.12.03 val PER: 0.4013
2026-01-05 14:44:43,466: t15.2023.12.08 val PER: 0.4048
2026-01-05 14:44:43,466: t15.2023.12.10 val PER: 0.3509
2026-01-05 14:44:43,466: t15.2023.12.17 val PER: 0.4054
2026-01-05 14:44:43,466: t15.2023.12.29 val PER: 0.4015
2026-01-05 14:44:43,466: t15.2024.02.25 val PER: 0.3511
2026-01-05 14:44:43,466: t15.2024.03.08 val PER: 0.4908
2026-01-05 14:44:43,466: t15.2024.03.15 val PER: 0.4390
2026-01-05 14:44:43,466: t15.2024.03.17 val PER: 0.4100
2026-01-05 14:44:43,466: t15.2024.05.10 val PER: 0.4294
2026-01-05 14:44:43,466: t15.2024.06.14 val PER: 0.4038
2026-01-05 14:44:43,466: t15.2024.07.19 val PER: 0.5254
2026-01-05 14:44:43,466: t15.2024.07.21 val PER: 0.3724
2026-01-05 14:44:43,466: t15.2024.07.28 val PER: 0.4154
2026-01-05 14:44:43,466: t15.2025.01.10 val PER: 0.6253
2026-01-05 14:44:43,467: t15.2025.01.12 val PER: 0.4503
2026-01-05 14:44:43,467: t15.2025.03.14 val PER: 0.6346
2026-01-05 14:44:43,467: t15.2025.03.16 val PER: 0.4843
2026-01-05 14:44:43,467: t15.2025.03.30 val PER: 0.6540
2026-01-05 14:44:43,467: t15.2025.04.13 val PER: 0.4893
2026-01-05 14:44:43,468: New best val WER(1gram) 89.09% --> 80.46%
2026-01-05 14:44:43,468: Checkpointing model
2026-01-05 14:44:44,124: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:44:44,431: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_1000
2026-01-05 14:45:03,137: Train batch 1200: loss: 32.89 grad norm: 75.59 time: 0.068
2026-01-05 14:45:22,044: Train batch 1400: loss: 36.15 grad norm: 82.09 time: 0.061
2026-01-05 14:45:31,444: Running test after training batch: 1500
2026-01-05 14:45:31,551: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:45:36,805: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt e the code it this boyde is will
2026-01-05 14:45:36,836: WER debug example
  GT : how does it keep the cost down
  PR : houde is it heap thus cost
2026-01-05 14:45:38,452: Val batch 1500: PER (avg): 0.3851 CTC Loss (avg): 37.3933 WER(1gram): 75.63% (n=64) time: 7.007
2026-01-05 14:45:38,452: WER lens: avg_true_words=6.16 avg_pred_words=4.95 max_pred_words=10
2026-01-05 14:45:38,452: t15.2023.08.13 val PER: 0.3399
2026-01-05 14:45:38,453: t15.2023.08.18 val PER: 0.3185
2026-01-05 14:45:38,453: t15.2023.08.20 val PER: 0.3050
2026-01-05 14:45:38,453: t15.2023.08.25 val PER: 0.2696
2026-01-05 14:45:38,453: t15.2023.08.27 val PER: 0.4068
2026-01-05 14:45:38,453: t15.2023.09.01 val PER: 0.2817
2026-01-05 14:45:38,453: t15.2023.09.03 val PER: 0.3907
2026-01-05 14:45:38,453: t15.2023.09.24 val PER: 0.2985
2026-01-05 14:45:38,453: t15.2023.09.29 val PER: 0.3472
2026-01-05 14:45:38,453: t15.2023.10.01 val PER: 0.3989
2026-01-05 14:45:38,453: t15.2023.10.06 val PER: 0.2917
2026-01-05 14:45:38,453: t15.2023.10.08 val PER: 0.4411
2026-01-05 14:45:38,454: t15.2023.10.13 val PER: 0.4546
2026-01-05 14:45:38,454: t15.2023.10.15 val PER: 0.3691
2026-01-05 14:45:38,454: t15.2023.10.20 val PER: 0.3121
2026-01-05 14:45:38,454: t15.2023.10.22 val PER: 0.3196
2026-01-05 14:45:38,454: t15.2023.11.03 val PER: 0.3623
2026-01-05 14:45:38,454: t15.2023.11.04 val PER: 0.1024
2026-01-05 14:45:38,454: t15.2023.11.17 val PER: 0.2224
2026-01-05 14:45:38,454: t15.2023.11.19 val PER: 0.1756
2026-01-05 14:45:38,454: t15.2023.11.26 val PER: 0.4326
2026-01-05 14:45:38,454: t15.2023.12.03 val PER: 0.3813
2026-01-05 14:45:38,454: t15.2023.12.08 val PER: 0.3642
2026-01-05 14:45:38,454: t15.2023.12.10 val PER: 0.2970
2026-01-05 14:45:38,454: t15.2023.12.17 val PER: 0.3773
2026-01-05 14:45:38,454: t15.2023.12.29 val PER: 0.3816
2026-01-05 14:45:38,454: t15.2024.02.25 val PER: 0.3076
2026-01-05 14:45:38,455: t15.2024.03.08 val PER: 0.4737
2026-01-05 14:45:38,455: t15.2024.03.15 val PER: 0.4140
2026-01-05 14:45:38,455: t15.2024.03.17 val PER: 0.3787
2026-01-05 14:45:38,455: t15.2024.05.10 val PER: 0.3893
2026-01-05 14:45:38,455: t15.2024.06.14 val PER: 0.4022
2026-01-05 14:45:38,455: t15.2024.07.19 val PER: 0.5320
2026-01-05 14:45:38,455: t15.2024.07.21 val PER: 0.3517
2026-01-05 14:45:38,455: t15.2024.07.28 val PER: 0.3772
2026-01-05 14:45:38,455: t15.2025.01.10 val PER: 0.6350
2026-01-05 14:45:38,455: t15.2025.01.12 val PER: 0.4303
2026-01-05 14:45:38,455: t15.2025.03.14 val PER: 0.5932
2026-01-05 14:45:38,455: t15.2025.03.16 val PER: 0.4620
2026-01-05 14:45:38,455: t15.2025.03.30 val PER: 0.6471
2026-01-05 14:45:38,455: t15.2025.04.13 val PER: 0.4807
2026-01-05 14:45:38,456: New best val WER(1gram) 80.46% --> 75.63%
2026-01-05 14:45:38,456: Checkpointing model
2026-01-05 14:45:39,155: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:45:39,457: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_1500
2026-01-05 14:45:48,701: Train batch 1600: loss: 37.18 grad norm: 80.40 time: 0.064
2026-01-05 14:46:07,633: Train batch 1800: loss: 35.46 grad norm: 69.06 time: 0.088
2026-01-05 14:46:26,740: Train batch 2000: loss: 33.90 grad norm: 72.20 time: 0.067
2026-01-05 14:46:26,740: Running test after training batch: 2000
2026-01-05 14:46:26,887: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:46:32,017: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned ease the code at this bonde will
2026-01-05 14:46:32,047: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the wass it
2026-01-05 14:46:33,753: Val batch 2000: PER (avg): 0.3278 CTC Loss (avg): 32.7808 WER(1gram): 70.56% (n=64) time: 7.013
2026-01-05 14:46:33,754: WER lens: avg_true_words=6.16 avg_pred_words=5.48 max_pred_words=11
2026-01-05 14:46:33,754: t15.2023.08.13 val PER: 0.3108
2026-01-05 14:46:33,754: t15.2023.08.18 val PER: 0.2506
2026-01-05 14:46:33,754: t15.2023.08.20 val PER: 0.2494
2026-01-05 14:46:33,755: t15.2023.08.25 val PER: 0.2425
2026-01-05 14:46:33,755: t15.2023.08.27 val PER: 0.3489
2026-01-05 14:46:33,755: t15.2023.09.01 val PER: 0.2338
2026-01-05 14:46:33,755: t15.2023.09.03 val PER: 0.3219
2026-01-05 14:46:33,755: t15.2023.09.24 val PER: 0.2415
2026-01-05 14:46:33,755: t15.2023.09.29 val PER: 0.2725
2026-01-05 14:46:33,755: t15.2023.10.01 val PER: 0.3210
2026-01-05 14:46:33,755: t15.2023.10.06 val PER: 0.2400
2026-01-05 14:46:33,755: t15.2023.10.08 val PER: 0.3748
2026-01-05 14:46:33,756: t15.2023.10.13 val PER: 0.3801
2026-01-05 14:46:33,756: t15.2023.10.15 val PER: 0.2947
2026-01-05 14:46:33,756: t15.2023.10.20 val PER: 0.2785
2026-01-05 14:46:33,756: t15.2023.10.22 val PER: 0.2584
2026-01-05 14:46:33,756: t15.2023.11.03 val PER: 0.3195
2026-01-05 14:46:33,756: t15.2023.11.04 val PER: 0.1058
2026-01-05 14:46:33,756: t15.2023.11.17 val PER: 0.1773
2026-01-05 14:46:33,756: t15.2023.11.19 val PER: 0.1437
2026-01-05 14:46:33,756: t15.2023.11.26 val PER: 0.3594
2026-01-05 14:46:33,756: t15.2023.12.03 val PER: 0.3193
2026-01-05 14:46:33,757: t15.2023.12.08 val PER: 0.3083
2026-01-05 14:46:33,757: t15.2023.12.10 val PER: 0.2628
2026-01-05 14:46:33,757: t15.2023.12.17 val PER: 0.3181
2026-01-05 14:46:33,757: t15.2023.12.29 val PER: 0.3336
2026-01-05 14:46:33,757: t15.2024.02.25 val PER: 0.2711
2026-01-05 14:46:33,757: t15.2024.03.08 val PER: 0.4011
2026-01-05 14:46:33,757: t15.2024.03.15 val PER: 0.3602
2026-01-05 14:46:33,757: t15.2024.03.17 val PER: 0.3403
2026-01-05 14:46:33,757: t15.2024.05.10 val PER: 0.3418
2026-01-05 14:46:33,757: t15.2024.06.14 val PER: 0.3438
2026-01-05 14:46:33,757: t15.2024.07.19 val PER: 0.4608
2026-01-05 14:46:33,758: t15.2024.07.21 val PER: 0.2938
2026-01-05 14:46:33,758: t15.2024.07.28 val PER: 0.3243
2026-01-05 14:46:33,758: t15.2025.01.10 val PER: 0.5565
2026-01-05 14:46:33,758: t15.2025.01.12 val PER: 0.3795
2026-01-05 14:46:33,758: t15.2025.03.14 val PER: 0.5518
2026-01-05 14:46:33,758: t15.2025.03.16 val PER: 0.3966
2026-01-05 14:46:33,758: t15.2025.03.30 val PER: 0.5667
2026-01-05 14:46:33,758: t15.2025.04.13 val PER: 0.4194
2026-01-05 14:46:33,759: New best val WER(1gram) 75.63% --> 70.56%
2026-01-05 14:46:33,759: Checkpointing model
2026-01-05 14:46:34,414: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:46:34,712: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_2000
2026-01-05 14:46:53,489: Train batch 2200: loss: 28.64 grad norm: 73.64 time: 0.061
2026-01-05 14:47:12,474: Train batch 2400: loss: 28.69 grad norm: 61.30 time: 0.052
2026-01-05 14:47:22,099: Running test after training batch: 2500
2026-01-05 14:47:22,277: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:47:27,384: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-05 14:47:27,415: WER debug example
  GT : how does it keep the cost down
  PR : houde des it hipp the wass it
2026-01-05 14:47:29,126: Val batch 2500: PER (avg): 0.3027 CTC Loss (avg): 30.1729 WER(1gram): 67.01% (n=64) time: 7.026
2026-01-05 14:47:29,126: WER lens: avg_true_words=6.16 avg_pred_words=5.39 max_pred_words=11
2026-01-05 14:47:29,126: t15.2023.08.13 val PER: 0.2869
2026-01-05 14:47:29,127: t15.2023.08.18 val PER: 0.2422
2026-01-05 14:47:29,127: t15.2023.08.20 val PER: 0.2367
2026-01-05 14:47:29,127: t15.2023.08.25 val PER: 0.1988
2026-01-05 14:47:29,127: t15.2023.08.27 val PER: 0.3183
2026-01-05 14:47:29,127: t15.2023.09.01 val PER: 0.2037
2026-01-05 14:47:29,127: t15.2023.09.03 val PER: 0.3005
2026-01-05 14:47:29,127: t15.2023.09.24 val PER: 0.2221
2026-01-05 14:47:29,127: t15.2023.09.29 val PER: 0.2502
2026-01-05 14:47:29,127: t15.2023.10.01 val PER: 0.3124
2026-01-05 14:47:29,128: t15.2023.10.06 val PER: 0.2228
2026-01-05 14:47:29,128: t15.2023.10.08 val PER: 0.3735
2026-01-05 14:47:29,128: t15.2023.10.13 val PER: 0.3530
2026-01-05 14:47:29,128: t15.2023.10.15 val PER: 0.2868
2026-01-05 14:47:29,128: t15.2023.10.20 val PER: 0.2752
2026-01-05 14:47:29,128: t15.2023.10.22 val PER: 0.2339
2026-01-05 14:47:29,128: t15.2023.11.03 val PER: 0.2890
2026-01-05 14:47:29,128: t15.2023.11.04 val PER: 0.0853
2026-01-05 14:47:29,128: t15.2023.11.17 val PER: 0.1431
2026-01-05 14:47:29,128: t15.2023.11.19 val PER: 0.1238
2026-01-05 14:47:29,128: t15.2023.11.26 val PER: 0.3464
2026-01-05 14:47:29,128: t15.2023.12.03 val PER: 0.2773
2026-01-05 14:47:29,129: t15.2023.12.08 val PER: 0.2810
2026-01-05 14:47:29,129: t15.2023.12.10 val PER: 0.2313
2026-01-05 14:47:29,129: t15.2023.12.17 val PER: 0.2973
2026-01-05 14:47:29,129: t15.2023.12.29 val PER: 0.3061
2026-01-05 14:47:29,129: t15.2024.02.25 val PER: 0.2402
2026-01-05 14:47:29,129: t15.2024.03.08 val PER: 0.3613
2026-01-05 14:47:29,129: t15.2024.03.15 val PER: 0.3471
2026-01-05 14:47:29,129: t15.2024.03.17 val PER: 0.3020
2026-01-05 14:47:29,129: t15.2024.05.10 val PER: 0.3224
2026-01-05 14:47:29,129: t15.2024.06.14 val PER: 0.3123
2026-01-05 14:47:29,129: t15.2024.07.19 val PER: 0.4278
2026-01-05 14:47:29,129: t15.2024.07.21 val PER: 0.2600
2026-01-05 14:47:29,130: t15.2024.07.28 val PER: 0.3081
2026-01-05 14:47:29,130: t15.2025.01.10 val PER: 0.4904
2026-01-05 14:47:29,130: t15.2025.01.12 val PER: 0.3657
2026-01-05 14:47:29,130: t15.2025.03.14 val PER: 0.4941
2026-01-05 14:47:29,130: t15.2025.03.16 val PER: 0.3757
2026-01-05 14:47:29,130: t15.2025.03.30 val PER: 0.5023
2026-01-05 14:47:29,130: t15.2025.04.13 val PER: 0.4023
2026-01-05 14:47:29,130: New best val WER(1gram) 70.56% --> 67.01%
2026-01-05 14:47:29,130: Checkpointing model
2026-01-05 14:47:29,812: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:47:30,110: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_2500
2026-01-05 14:47:39,428: Train batch 2600: loss: 34.59 grad norm: 81.22 time: 0.055
2026-01-05 14:47:58,130: Train batch 2800: loss: 25.46 grad norm: 68.45 time: 0.082
2026-01-05 14:48:16,943: Train batch 3000: loss: 32.03 grad norm: 77.58 time: 0.083
2026-01-05 14:48:16,943: Running test after training batch: 3000
2026-01-05 14:48:17,063: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:48:22,267: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the code at this point is will
2026-01-05 14:48:22,299: WER debug example
  GT : how does it keep the cost down
  PR : houde des it yip the what get
2026-01-05 14:48:24,042: Val batch 3000: PER (avg): 0.2833 CTC Loss (avg): 27.9674 WER(1gram): 64.72% (n=64) time: 7.098
2026-01-05 14:48:24,042: WER lens: avg_true_words=6.16 avg_pred_words=5.86 max_pred_words=11
2026-01-05 14:48:24,043: t15.2023.08.13 val PER: 0.2557
2026-01-05 14:48:24,043: t15.2023.08.18 val PER: 0.2221
2026-01-05 14:48:24,043: t15.2023.08.20 val PER: 0.2145
2026-01-05 14:48:24,043: t15.2023.08.25 val PER: 0.2093
2026-01-05 14:48:24,043: t15.2023.08.27 val PER: 0.3103
2026-01-05 14:48:24,043: t15.2023.09.01 val PER: 0.1924
2026-01-05 14:48:24,044: t15.2023.09.03 val PER: 0.2779
2026-01-05 14:48:24,044: t15.2023.09.24 val PER: 0.2209
2026-01-05 14:48:24,044: t15.2023.09.29 val PER: 0.2361
2026-01-05 14:48:24,044: t15.2023.10.01 val PER: 0.2999
2026-01-05 14:48:24,044: t15.2023.10.06 val PER: 0.2002
2026-01-05 14:48:24,044: t15.2023.10.08 val PER: 0.3559
2026-01-05 14:48:24,044: t15.2023.10.13 val PER: 0.3460
2026-01-05 14:48:24,044: t15.2023.10.15 val PER: 0.2703
2026-01-05 14:48:24,044: t15.2023.10.20 val PER: 0.2349
2026-01-05 14:48:24,045: t15.2023.10.22 val PER: 0.2183
2026-01-05 14:48:24,045: t15.2023.11.03 val PER: 0.2734
2026-01-05 14:48:24,045: t15.2023.11.04 val PER: 0.0785
2026-01-05 14:48:24,045: t15.2023.11.17 val PER: 0.1291
2026-01-05 14:48:24,045: t15.2023.11.19 val PER: 0.1218
2026-01-05 14:48:24,045: t15.2023.11.26 val PER: 0.3036
2026-01-05 14:48:24,045: t15.2023.12.03 val PER: 0.2584
2026-01-05 14:48:24,045: t15.2023.12.08 val PER: 0.2603
2026-01-05 14:48:24,045: t15.2023.12.10 val PER: 0.2142
2026-01-05 14:48:24,046: t15.2023.12.17 val PER: 0.2755
2026-01-05 14:48:24,046: t15.2023.12.29 val PER: 0.2972
2026-01-05 14:48:24,046: t15.2024.02.25 val PER: 0.2430
2026-01-05 14:48:24,046: t15.2024.03.08 val PER: 0.3585
2026-01-05 14:48:24,046: t15.2024.03.15 val PER: 0.3302
2026-01-05 14:48:24,046: t15.2024.03.17 val PER: 0.2943
2026-01-05 14:48:24,046: t15.2024.05.10 val PER: 0.3135
2026-01-05 14:48:24,046: t15.2024.06.14 val PER: 0.2965
2026-01-05 14:48:24,046: t15.2024.07.19 val PER: 0.4041
2026-01-05 14:48:24,047: t15.2024.07.21 val PER: 0.2290
2026-01-05 14:48:24,047: t15.2024.07.28 val PER: 0.2809
2026-01-05 14:48:24,047: t15.2025.01.10 val PER: 0.4945
2026-01-05 14:48:24,047: t15.2025.01.12 val PER: 0.3195
2026-01-05 14:48:24,047: t15.2025.03.14 val PER: 0.4527
2026-01-05 14:48:24,047: t15.2025.03.16 val PER: 0.3168
2026-01-05 14:48:24,047: t15.2025.03.30 val PER: 0.4701
2026-01-05 14:48:24,047: t15.2025.04.13 val PER: 0.3509
2026-01-05 14:48:24,048: New best val WER(1gram) 67.01% --> 64.72%
2026-01-05 14:48:24,048: Checkpointing model
2026-01-05 14:48:24,702: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:48:24,997: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_3000
2026-01-05 14:48:43,889: Train batch 3200: loss: 26.63 grad norm: 68.49 time: 0.075
2026-01-05 14:49:02,691: Train batch 3400: loss: 17.65 grad norm: 54.41 time: 0.049
2026-01-05 14:49:12,602: Running test after training batch: 3500
2026-01-05 14:49:12,749: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:49:17,877: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned eke the code at this point is wheel
2026-01-05 14:49:17,907: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it eke thus cussed get
2026-01-05 14:49:19,609: Val batch 3500: PER (avg): 0.2662 CTC Loss (avg): 26.7086 WER(1gram): 64.97% (n=64) time: 7.007
2026-01-05 14:49:19,610: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=11
2026-01-05 14:49:19,610: t15.2023.08.13 val PER: 0.2401
2026-01-05 14:49:19,610: t15.2023.08.18 val PER: 0.2003
2026-01-05 14:49:19,610: t15.2023.08.20 val PER: 0.2176
2026-01-05 14:49:19,610: t15.2023.08.25 val PER: 0.1822
2026-01-05 14:49:19,610: t15.2023.08.27 val PER: 0.2814
2026-01-05 14:49:19,610: t15.2023.09.01 val PER: 0.1729
2026-01-05 14:49:19,610: t15.2023.09.03 val PER: 0.2470
2026-01-05 14:49:19,611: t15.2023.09.24 val PER: 0.2124
2026-01-05 14:49:19,611: t15.2023.09.29 val PER: 0.2246
2026-01-05 14:49:19,611: t15.2023.10.01 val PER: 0.2761
2026-01-05 14:49:19,611: t15.2023.10.06 val PER: 0.1830
2026-01-05 14:49:19,611: t15.2023.10.08 val PER: 0.3302
2026-01-05 14:49:19,611: t15.2023.10.13 val PER: 0.3072
2026-01-05 14:49:19,611: t15.2023.10.15 val PER: 0.2558
2026-01-05 14:49:19,611: t15.2023.10.20 val PER: 0.2383
2026-01-05 14:49:19,611: t15.2023.10.22 val PER: 0.2027
2026-01-05 14:49:19,611: t15.2023.11.03 val PER: 0.2700
2026-01-05 14:49:19,611: t15.2023.11.04 val PER: 0.0785
2026-01-05 14:49:19,611: t15.2023.11.17 val PER: 0.1198
2026-01-05 14:49:19,611: t15.2023.11.19 val PER: 0.1058
2026-01-05 14:49:19,612: t15.2023.11.26 val PER: 0.2841
2026-01-05 14:49:19,612: t15.2023.12.03 val PER: 0.2405
2026-01-05 14:49:19,612: t15.2023.12.08 val PER: 0.2497
2026-01-05 14:49:19,612: t15.2023.12.10 val PER: 0.1971
2026-01-05 14:49:19,612: t15.2023.12.17 val PER: 0.2464
2026-01-05 14:49:19,612: t15.2023.12.29 val PER: 0.2594
2026-01-05 14:49:19,612: t15.2024.02.25 val PER: 0.2135
2026-01-05 14:49:19,612: t15.2024.03.08 val PER: 0.3457
2026-01-05 14:49:19,612: t15.2024.03.15 val PER: 0.3189
2026-01-05 14:49:19,612: t15.2024.03.17 val PER: 0.2748
2026-01-05 14:49:19,612: t15.2024.05.10 val PER: 0.2793
2026-01-05 14:49:19,613: t15.2024.06.14 val PER: 0.2823
2026-01-05 14:49:19,613: t15.2024.07.19 val PER: 0.3962
2026-01-05 14:49:19,613: t15.2024.07.21 val PER: 0.2159
2026-01-05 14:49:19,613: t15.2024.07.28 val PER: 0.2787
2026-01-05 14:49:19,613: t15.2025.01.10 val PER: 0.4642
2026-01-05 14:49:19,613: t15.2025.01.12 val PER: 0.2948
2026-01-05 14:49:19,613: t15.2025.03.14 val PER: 0.4467
2026-01-05 14:49:19,613: t15.2025.03.16 val PER: 0.3181
2026-01-05 14:49:19,613: t15.2025.03.30 val PER: 0.4402
2026-01-05 14:49:19,613: t15.2025.04.13 val PER: 0.3452
2026-01-05 14:49:19,914: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_3500
2026-01-05 14:49:29,372: Train batch 3600: loss: 22.44 grad norm: 61.32 time: 0.068
2026-01-05 14:49:47,940: Train batch 3800: loss: 24.76 grad norm: 66.28 time: 0.067
2026-01-05 14:50:06,926: Train batch 4000: loss: 19.39 grad norm: 58.81 time: 0.056
2026-01-05 14:50:06,926: Running test after training batch: 4000
2026-01-05 14:50:07,101: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:50:12,215: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-05 14:50:12,246: WER debug example
  GT : how does it keep the cost down
  PR : out des it keep the cussed nett
2026-01-05 14:50:13,927: Val batch 4000: PER (avg): 0.2477 CTC Loss (avg): 24.2782 WER(1gram): 62.94% (n=64) time: 7.001
2026-01-05 14:50:13,928: WER lens: avg_true_words=6.16 avg_pred_words=6.02 max_pred_words=11
2026-01-05 14:50:13,928: t15.2023.08.13 val PER: 0.2141
2026-01-05 14:50:13,928: t15.2023.08.18 val PER: 0.1970
2026-01-05 14:50:13,928: t15.2023.08.20 val PER: 0.1986
2026-01-05 14:50:13,928: t15.2023.08.25 val PER: 0.1446
2026-01-05 14:50:13,928: t15.2023.08.27 val PER: 0.2765
2026-01-05 14:50:13,928: t15.2023.09.01 val PER: 0.1664
2026-01-05 14:50:13,928: t15.2023.09.03 val PER: 0.2447
2026-01-05 14:50:13,928: t15.2023.09.24 val PER: 0.1954
2026-01-05 14:50:13,928: t15.2023.09.29 val PER: 0.2042
2026-01-05 14:50:13,928: t15.2023.10.01 val PER: 0.2596
2026-01-05 14:50:13,928: t15.2023.10.06 val PER: 0.1539
2026-01-05 14:50:13,929: t15.2023.10.08 val PER: 0.3288
2026-01-05 14:50:13,929: t15.2023.10.13 val PER: 0.2956
2026-01-05 14:50:13,929: t15.2023.10.15 val PER: 0.2386
2026-01-05 14:50:13,929: t15.2023.10.20 val PER: 0.2416
2026-01-05 14:50:13,929: t15.2023.10.22 val PER: 0.1871
2026-01-05 14:50:13,929: t15.2023.11.03 val PER: 0.2422
2026-01-05 14:50:13,929: t15.2023.11.04 val PER: 0.0683
2026-01-05 14:50:13,929: t15.2023.11.17 val PER: 0.0995
2026-01-05 14:50:13,929: t15.2023.11.19 val PER: 0.0958
2026-01-05 14:50:13,929: t15.2023.11.26 val PER: 0.2616
2026-01-05 14:50:13,929: t15.2023.12.03 val PER: 0.2237
2026-01-05 14:50:13,929: t15.2023.12.08 val PER: 0.2237
2026-01-05 14:50:13,929: t15.2023.12.10 val PER: 0.1892
2026-01-05 14:50:13,930: t15.2023.12.17 val PER: 0.2380
2026-01-05 14:50:13,930: t15.2023.12.29 val PER: 0.2546
2026-01-05 14:50:13,930: t15.2024.02.25 val PER: 0.2107
2026-01-05 14:50:13,930: t15.2024.03.08 val PER: 0.3314
2026-01-05 14:50:13,930: t15.2024.03.15 val PER: 0.2946
2026-01-05 14:50:13,930: t15.2024.03.17 val PER: 0.2587
2026-01-05 14:50:13,930: t15.2024.05.10 val PER: 0.2615
2026-01-05 14:50:13,930: t15.2024.06.14 val PER: 0.2792
2026-01-05 14:50:13,930: t15.2024.07.19 val PER: 0.3593
2026-01-05 14:50:13,930: t15.2024.07.21 val PER: 0.1917
2026-01-05 14:50:13,931: t15.2024.07.28 val PER: 0.2441
2026-01-05 14:50:13,931: t15.2025.01.10 val PER: 0.4174
2026-01-05 14:50:13,931: t15.2025.01.12 val PER: 0.2756
2026-01-05 14:50:13,931: t15.2025.03.14 val PER: 0.4127
2026-01-05 14:50:13,931: t15.2025.03.16 val PER: 0.3076
2026-01-05 14:50:13,931: t15.2025.03.30 val PER: 0.4149
2026-01-05 14:50:13,931: t15.2025.04.13 val PER: 0.3252
2026-01-05 14:50:13,932: New best val WER(1gram) 64.72% --> 62.94%
2026-01-05 14:50:13,932: Checkpointing model
2026-01-05 14:50:14,599: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:50:14,895: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_4000
2026-01-05 14:50:33,651: Train batch 4200: loss: 21.98 grad norm: 64.96 time: 0.080
2026-01-05 14:50:52,508: Train batch 4400: loss: 16.53 grad norm: 51.89 time: 0.066
2026-01-05 14:51:01,850: Running test after training batch: 4500
2026-01-05 14:51:01,997: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:51:07,130: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-05 14:51:07,162: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cussed get
2026-01-05 14:51:08,850: Val batch 4500: PER (avg): 0.2368 CTC Loss (avg): 23.2852 WER(1gram): 59.64% (n=64) time: 6.999
2026-01-05 14:51:08,850: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-05 14:51:08,850: t15.2023.08.13 val PER: 0.2089
2026-01-05 14:51:08,850: t15.2023.08.18 val PER: 0.1878
2026-01-05 14:51:08,850: t15.2023.08.20 val PER: 0.1803
2026-01-05 14:51:08,850: t15.2023.08.25 val PER: 0.1476
2026-01-05 14:51:08,850: t15.2023.08.27 val PER: 0.2460
2026-01-05 14:51:08,851: t15.2023.09.01 val PER: 0.1485
2026-01-05 14:51:08,851: t15.2023.09.03 val PER: 0.2435
2026-01-05 14:51:08,851: t15.2023.09.24 val PER: 0.1711
2026-01-05 14:51:08,851: t15.2023.09.29 val PER: 0.1946
2026-01-05 14:51:08,851: t15.2023.10.01 val PER: 0.2530
2026-01-05 14:51:08,851: t15.2023.10.06 val PER: 0.1518
2026-01-05 14:51:08,851: t15.2023.10.08 val PER: 0.3180
2026-01-05 14:51:08,851: t15.2023.10.13 val PER: 0.2979
2026-01-05 14:51:08,851: t15.2023.10.15 val PER: 0.2268
2026-01-05 14:51:08,851: t15.2023.10.20 val PER: 0.2148
2026-01-05 14:51:08,851: t15.2023.10.22 val PER: 0.1904
2026-01-05 14:51:08,852: t15.2023.11.03 val PER: 0.2476
2026-01-05 14:51:08,852: t15.2023.11.04 val PER: 0.0546
2026-01-05 14:51:08,852: t15.2023.11.17 val PER: 0.1042
2026-01-05 14:51:08,852: t15.2023.11.19 val PER: 0.0898
2026-01-05 14:51:08,852: t15.2023.11.26 val PER: 0.2572
2026-01-05 14:51:08,852: t15.2023.12.03 val PER: 0.2258
2026-01-05 14:51:08,852: t15.2023.12.08 val PER: 0.2130
2026-01-05 14:51:08,852: t15.2023.12.10 val PER: 0.1708
2026-01-05 14:51:08,852: t15.2023.12.17 val PER: 0.2287
2026-01-05 14:51:08,852: t15.2023.12.29 val PER: 0.2471
2026-01-05 14:51:08,852: t15.2024.02.25 val PER: 0.1952
2026-01-05 14:51:08,852: t15.2024.03.08 val PER: 0.3186
2026-01-05 14:51:08,852: t15.2024.03.15 val PER: 0.2833
2026-01-05 14:51:08,852: t15.2024.03.17 val PER: 0.2434
2026-01-05 14:51:08,852: t15.2024.05.10 val PER: 0.2571
2026-01-05 14:51:08,852: t15.2024.06.14 val PER: 0.2445
2026-01-05 14:51:08,853: t15.2024.07.19 val PER: 0.3316
2026-01-05 14:51:08,853: t15.2024.07.21 val PER: 0.1862
2026-01-05 14:51:08,853: t15.2024.07.28 val PER: 0.2169
2026-01-05 14:51:08,853: t15.2025.01.10 val PER: 0.3967
2026-01-05 14:51:08,853: t15.2025.01.12 val PER: 0.2687
2026-01-05 14:51:08,853: t15.2025.03.14 val PER: 0.4083
2026-01-05 14:51:08,853: t15.2025.03.16 val PER: 0.2919
2026-01-05 14:51:08,853: t15.2025.03.30 val PER: 0.4034
2026-01-05 14:51:08,853: t15.2025.04.13 val PER: 0.2896
2026-01-05 14:51:08,854: New best val WER(1gram) 62.94% --> 59.64%
2026-01-05 14:51:08,854: Checkpointing model
2026-01-05 14:51:09,506: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:51:09,806: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_4500
2026-01-05 14:51:19,197: Train batch 4600: loss: 20.20 grad norm: 64.80 time: 0.063
2026-01-05 14:51:38,089: Train batch 4800: loss: 13.52 grad norm: 51.33 time: 0.064
2026-01-05 14:51:56,868: Train batch 5000: loss: 31.61 grad norm: 85.94 time: 0.065
2026-01-05 14:51:56,868: Running test after training batch: 5000
2026-01-05 14:51:56,971: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:52:02,090: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-05 14:52:02,121: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the cost nett
2026-01-05 14:52:03,799: Val batch 5000: PER (avg): 0.2232 CTC Loss (avg): 21.9365 WER(1gram): 59.39% (n=64) time: 6.931
2026-01-05 14:52:03,799: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-05 14:52:03,800: t15.2023.08.13 val PER: 0.1975
2026-01-05 14:52:03,800: t15.2023.08.18 val PER: 0.1601
2026-01-05 14:52:03,800: t15.2023.08.20 val PER: 0.1708
2026-01-05 14:52:03,800: t15.2023.08.25 val PER: 0.1175
2026-01-05 14:52:03,800: t15.2023.08.27 val PER: 0.2363
2026-01-05 14:52:03,800: t15.2023.09.01 val PER: 0.1364
2026-01-05 14:52:03,800: t15.2023.09.03 val PER: 0.2209
2026-01-05 14:52:03,800: t15.2023.09.24 val PER: 0.1845
2026-01-05 14:52:03,800: t15.2023.09.29 val PER: 0.1800
2026-01-05 14:52:03,800: t15.2023.10.01 val PER: 0.2351
2026-01-05 14:52:03,800: t15.2023.10.06 val PER: 0.1410
2026-01-05 14:52:03,800: t15.2023.10.08 val PER: 0.3018
2026-01-05 14:52:03,801: t15.2023.10.13 val PER: 0.2824
2026-01-05 14:52:03,801: t15.2023.10.15 val PER: 0.2202
2026-01-05 14:52:03,801: t15.2023.10.20 val PER: 0.2282
2026-01-05 14:52:03,801: t15.2023.10.22 val PER: 0.1670
2026-01-05 14:52:03,801: t15.2023.11.03 val PER: 0.2273
2026-01-05 14:52:03,801: t15.2023.11.04 val PER: 0.0444
2026-01-05 14:52:03,802: t15.2023.11.17 val PER: 0.0933
2026-01-05 14:52:03,802: t15.2023.11.19 val PER: 0.0719
2026-01-05 14:52:03,802: t15.2023.11.26 val PER: 0.2326
2026-01-05 14:52:03,802: t15.2023.12.03 val PER: 0.2017
2026-01-05 14:52:03,802: t15.2023.12.08 val PER: 0.2024
2026-01-05 14:52:03,802: t15.2023.12.10 val PER: 0.1603
2026-01-05 14:52:03,802: t15.2023.12.17 val PER: 0.2183
2026-01-05 14:52:03,802: t15.2023.12.29 val PER: 0.2306
2026-01-05 14:52:03,802: t15.2024.02.25 val PER: 0.1784
2026-01-05 14:52:03,802: t15.2024.03.08 val PER: 0.3044
2026-01-05 14:52:03,802: t15.2024.03.15 val PER: 0.2802
2026-01-05 14:52:03,802: t15.2024.03.17 val PER: 0.2259
2026-01-05 14:52:03,802: t15.2024.05.10 val PER: 0.2422
2026-01-05 14:52:03,802: t15.2024.06.14 val PER: 0.2397
2026-01-05 14:52:03,803: t15.2024.07.19 val PER: 0.3342
2026-01-05 14:52:03,803: t15.2024.07.21 val PER: 0.1800
2026-01-05 14:52:03,803: t15.2024.07.28 val PER: 0.2125
2026-01-05 14:52:03,803: t15.2025.01.10 val PER: 0.3829
2026-01-05 14:52:03,803: t15.2025.01.12 val PER: 0.2433
2026-01-05 14:52:03,803: t15.2025.03.14 val PER: 0.3920
2026-01-05 14:52:03,803: t15.2025.03.16 val PER: 0.2474
2026-01-05 14:52:03,803: t15.2025.03.30 val PER: 0.3874
2026-01-05 14:52:03,803: t15.2025.04.13 val PER: 0.2910
2026-01-05 14:52:03,804: New best val WER(1gram) 59.64% --> 59.39%
2026-01-05 14:52:03,804: Checkpointing model
2026-01-05 14:52:04,468: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:52:04,774: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_5000
2026-01-05 14:52:23,882: Train batch 5200: loss: 16.26 grad norm: 59.54 time: 0.051
2026-01-05 14:52:42,542: Train batch 5400: loss: 17.46 grad norm: 60.16 time: 0.068
2026-01-05 14:52:51,893: Running test after training batch: 5500
2026-01-05 14:52:52,060: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:52:57,177: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point will
2026-01-05 14:52:57,208: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-05 14:52:58,841: Val batch 5500: PER (avg): 0.2145 CTC Loss (avg): 21.0179 WER(1gram): 55.33% (n=64) time: 6.948
2026-01-05 14:52:58,842: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-05 14:52:58,842: t15.2023.08.13 val PER: 0.1746
2026-01-05 14:52:58,842: t15.2023.08.18 val PER: 0.1576
2026-01-05 14:52:58,842: t15.2023.08.20 val PER: 0.1716
2026-01-05 14:52:58,842: t15.2023.08.25 val PER: 0.1220
2026-01-05 14:52:58,842: t15.2023.08.27 val PER: 0.2379
2026-01-05 14:52:58,842: t15.2023.09.01 val PER: 0.1242
2026-01-05 14:52:58,842: t15.2023.09.03 val PER: 0.2233
2026-01-05 14:52:58,842: t15.2023.09.24 val PER: 0.1784
2026-01-05 14:52:58,842: t15.2023.09.29 val PER: 0.1691
2026-01-05 14:52:58,843: t15.2023.10.01 val PER: 0.2338
2026-01-05 14:52:58,843: t15.2023.10.06 val PER: 0.1410
2026-01-05 14:52:58,843: t15.2023.10.08 val PER: 0.2896
2026-01-05 14:52:58,843: t15.2023.10.13 val PER: 0.2793
2026-01-05 14:52:58,843: t15.2023.10.15 val PER: 0.2136
2026-01-05 14:52:58,843: t15.2023.10.20 val PER: 0.2114
2026-01-05 14:52:58,843: t15.2023.10.22 val PER: 0.1626
2026-01-05 14:52:58,843: t15.2023.11.03 val PER: 0.2164
2026-01-05 14:52:58,843: t15.2023.11.04 val PER: 0.0512
2026-01-05 14:52:58,843: t15.2023.11.17 val PER: 0.0731
2026-01-05 14:52:58,843: t15.2023.11.19 val PER: 0.0679
2026-01-05 14:52:58,843: t15.2023.11.26 val PER: 0.2217
2026-01-05 14:52:58,843: t15.2023.12.03 val PER: 0.1838
2026-01-05 14:52:58,843: t15.2023.12.08 val PER: 0.1931
2026-01-05 14:52:58,843: t15.2023.12.10 val PER: 0.1459
2026-01-05 14:52:58,844: t15.2023.12.17 val PER: 0.2183
2026-01-05 14:52:58,844: t15.2023.12.29 val PER: 0.2155
2026-01-05 14:52:58,844: t15.2024.02.25 val PER: 0.1868
2026-01-05 14:52:58,844: t15.2024.03.08 val PER: 0.2930
2026-01-05 14:52:58,844: t15.2024.03.15 val PER: 0.2577
2026-01-05 14:52:58,844: t15.2024.03.17 val PER: 0.2183
2026-01-05 14:52:58,845: t15.2024.05.10 val PER: 0.2110
2026-01-05 14:52:58,845: t15.2024.06.14 val PER: 0.2334
2026-01-05 14:52:58,845: t15.2024.07.19 val PER: 0.3217
2026-01-05 14:52:58,845: t15.2024.07.21 val PER: 0.1579
2026-01-05 14:52:58,845: t15.2024.07.28 val PER: 0.2037
2026-01-05 14:52:58,845: t15.2025.01.10 val PER: 0.3912
2026-01-05 14:52:58,845: t15.2025.01.12 val PER: 0.2340
2026-01-05 14:52:58,845: t15.2025.03.14 val PER: 0.3609
2026-01-05 14:52:58,845: t15.2025.03.16 val PER: 0.2736
2026-01-05 14:52:58,845: t15.2025.03.30 val PER: 0.3621
2026-01-05 14:52:58,845: t15.2025.04.13 val PER: 0.2939
2026-01-05 14:52:58,846: New best val WER(1gram) 59.39% --> 55.33%
2026-01-05 14:52:58,846: Checkpointing model
2026-01-05 14:52:59,490: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:52:59,784: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_5500
2026-01-05 14:53:09,083: Train batch 5600: loss: 18.75 grad norm: 64.54 time: 0.062
2026-01-05 14:53:27,893: Train batch 5800: loss: 13.58 grad norm: 57.63 time: 0.082
2026-01-05 14:53:46,549: Train batch 6000: loss: 14.43 grad norm: 56.65 time: 0.049
2026-01-05 14:53:46,550: Running test after training batch: 6000
2026-01-05 14:53:46,751: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:53:51,889: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-05 14:53:51,921: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost nett
2026-01-05 14:53:53,604: Val batch 6000: PER (avg): 0.2105 CTC Loss (avg): 20.8309 WER(1gram): 58.12% (n=64) time: 7.054
2026-01-05 14:53:53,604: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 14:53:53,604: t15.2023.08.13 val PER: 0.1798
2026-01-05 14:53:53,604: t15.2023.08.18 val PER: 0.1618
2026-01-05 14:53:53,605: t15.2023.08.20 val PER: 0.1573
2026-01-05 14:53:53,605: t15.2023.08.25 val PER: 0.1265
2026-01-05 14:53:53,605: t15.2023.08.27 val PER: 0.2444
2026-01-05 14:53:53,605: t15.2023.09.01 val PER: 0.1226
2026-01-05 14:53:53,605: t15.2023.09.03 val PER: 0.2162
2026-01-05 14:53:53,605: t15.2023.09.24 val PER: 0.1650
2026-01-05 14:53:53,605: t15.2023.09.29 val PER: 0.1602
2026-01-05 14:53:53,605: t15.2023.10.01 val PER: 0.2186
2026-01-05 14:53:53,606: t15.2023.10.06 val PER: 0.1356
2026-01-05 14:53:53,606: t15.2023.10.08 val PER: 0.2842
2026-01-05 14:53:53,606: t15.2023.10.13 val PER: 0.2607
2026-01-05 14:53:53,606: t15.2023.10.15 val PER: 0.2195
2026-01-05 14:53:53,606: t15.2023.10.20 val PER: 0.2282
2026-01-05 14:53:53,606: t15.2023.10.22 val PER: 0.1659
2026-01-05 14:53:53,606: t15.2023.11.03 val PER: 0.2171
2026-01-05 14:53:53,606: t15.2023.11.04 val PER: 0.0512
2026-01-05 14:53:53,606: t15.2023.11.17 val PER: 0.0809
2026-01-05 14:53:53,607: t15.2023.11.19 val PER: 0.0719
2026-01-05 14:53:53,607: t15.2023.11.26 val PER: 0.2225
2026-01-05 14:53:53,607: t15.2023.12.03 val PER: 0.1712
2026-01-05 14:53:53,607: t15.2023.12.08 val PER: 0.1744
2026-01-05 14:53:53,607: t15.2023.12.10 val PER: 0.1524
2026-01-05 14:53:53,607: t15.2023.12.17 val PER: 0.1975
2026-01-05 14:53:53,607: t15.2023.12.29 val PER: 0.2128
2026-01-05 14:53:53,607: t15.2024.02.25 val PER: 0.1559
2026-01-05 14:53:53,607: t15.2024.03.08 val PER: 0.2959
2026-01-05 14:53:53,608: t15.2024.03.15 val PER: 0.2620
2026-01-05 14:53:53,608: t15.2024.03.17 val PER: 0.2071
2026-01-05 14:53:53,608: t15.2024.05.10 val PER: 0.2244
2026-01-05 14:53:53,608: t15.2024.06.14 val PER: 0.2287
2026-01-05 14:53:53,608: t15.2024.07.19 val PER: 0.3125
2026-01-05 14:53:53,608: t15.2024.07.21 val PER: 0.1676
2026-01-05 14:53:53,608: t15.2024.07.28 val PER: 0.2037
2026-01-05 14:53:53,608: t15.2025.01.10 val PER: 0.3843
2026-01-05 14:53:53,608: t15.2025.01.12 val PER: 0.2240
2026-01-05 14:53:53,608: t15.2025.03.14 val PER: 0.3891
2026-01-05 14:53:53,608: t15.2025.03.16 val PER: 0.2657
2026-01-05 14:53:53,609: t15.2025.03.30 val PER: 0.3747
2026-01-05 14:53:53,609: t15.2025.04.13 val PER: 0.2625
2026-01-05 14:53:53,891: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_6000
2026-01-05 14:54:12,521: Train batch 6200: loss: 16.43 grad norm: 59.59 time: 0.070
2026-01-05 14:54:31,066: Train batch 6400: loss: 18.76 grad norm: 68.64 time: 0.062
2026-01-05 14:54:40,213: Running test after training batch: 6500
2026-01-05 14:54:40,367: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:54:45,505: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point as will
2026-01-05 14:54:45,537: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the quast get
2026-01-05 14:54:47,211: Val batch 6500: PER (avg): 0.2038 CTC Loss (avg): 20.0010 WER(1gram): 53.81% (n=64) time: 6.998
2026-01-05 14:54:47,212: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 14:54:47,212: t15.2023.08.13 val PER: 0.1715
2026-01-05 14:54:47,212: t15.2023.08.18 val PER: 0.1408
2026-01-05 14:54:47,212: t15.2023.08.20 val PER: 0.1541
2026-01-05 14:54:47,212: t15.2023.08.25 val PER: 0.1039
2026-01-05 14:54:47,212: t15.2023.08.27 val PER: 0.2347
2026-01-05 14:54:47,213: t15.2023.09.01 val PER: 0.1153
2026-01-05 14:54:47,213: t15.2023.09.03 val PER: 0.2007
2026-01-05 14:54:47,213: t15.2023.09.24 val PER: 0.1699
2026-01-05 14:54:47,213: t15.2023.09.29 val PER: 0.1672
2026-01-05 14:54:47,213: t15.2023.10.01 val PER: 0.2285
2026-01-05 14:54:47,213: t15.2023.10.06 val PER: 0.1259
2026-01-05 14:54:47,213: t15.2023.10.08 val PER: 0.3031
2026-01-05 14:54:47,213: t15.2023.10.13 val PER: 0.2715
2026-01-05 14:54:47,213: t15.2023.10.15 val PER: 0.2109
2026-01-05 14:54:47,213: t15.2023.10.20 val PER: 0.2315
2026-01-05 14:54:47,213: t15.2023.10.22 val PER: 0.1526
2026-01-05 14:54:47,214: t15.2023.11.03 val PER: 0.2178
2026-01-05 14:54:47,214: t15.2023.11.04 val PER: 0.0410
2026-01-05 14:54:47,214: t15.2023.11.17 val PER: 0.0544
2026-01-05 14:54:47,214: t15.2023.11.19 val PER: 0.0758
2026-01-05 14:54:47,214: t15.2023.11.26 val PER: 0.2080
2026-01-05 14:54:47,214: t15.2023.12.03 val PER: 0.1660
2026-01-05 14:54:47,214: t15.2023.12.08 val PER: 0.1864
2026-01-05 14:54:47,214: t15.2023.12.10 val PER: 0.1393
2026-01-05 14:54:47,214: t15.2023.12.17 val PER: 0.1819
2026-01-05 14:54:47,214: t15.2023.12.29 val PER: 0.2004
2026-01-05 14:54:47,214: t15.2024.02.25 val PER: 0.1728
2026-01-05 14:54:47,214: t15.2024.03.08 val PER: 0.2945
2026-01-05 14:54:47,215: t15.2024.03.15 val PER: 0.2602
2026-01-05 14:54:47,215: t15.2024.03.17 val PER: 0.2043
2026-01-05 14:54:47,215: t15.2024.05.10 val PER: 0.2229
2026-01-05 14:54:47,215: t15.2024.06.14 val PER: 0.2114
2026-01-05 14:54:47,215: t15.2024.07.19 val PER: 0.2986
2026-01-05 14:54:47,215: t15.2024.07.21 val PER: 0.1462
2026-01-05 14:54:47,215: t15.2024.07.28 val PER: 0.1882
2026-01-05 14:54:47,215: t15.2025.01.10 val PER: 0.3554
2026-01-05 14:54:47,215: t15.2025.01.12 val PER: 0.2179
2026-01-05 14:54:47,215: t15.2025.03.14 val PER: 0.3728
2026-01-05 14:54:47,215: t15.2025.03.16 val PER: 0.2435
2026-01-05 14:54:47,215: t15.2025.03.30 val PER: 0.3471
2026-01-05 14:54:47,216: t15.2025.04.13 val PER: 0.2710
2026-01-05 14:54:47,216: New best val WER(1gram) 55.33% --> 53.81%
2026-01-05 14:54:47,216: Checkpointing model
2026-01-05 14:54:47,883: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:54:48,175: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_6500
2026-01-05 14:54:57,288: Train batch 6600: loss: 12.58 grad norm: 55.18 time: 0.045
2026-01-05 14:55:15,955: Train batch 6800: loss: 15.22 grad norm: 56.58 time: 0.048
2026-01-05 14:55:34,792: Train batch 7000: loss: 16.86 grad norm: 64.14 time: 0.061
2026-01-05 14:55:34,792: Running test after training batch: 7000
2026-01-05 14:55:35,105: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:55:40,276: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:55:40,307: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the quast et
2026-01-05 14:55:41,996: Val batch 7000: PER (avg): 0.1941 CTC Loss (avg): 19.1558 WER(1gram): 53.30% (n=64) time: 7.204
2026-01-05 14:55:41,998: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-05 14:55:41,998: t15.2023.08.13 val PER: 0.1538
2026-01-05 14:55:41,998: t15.2023.08.18 val PER: 0.1358
2026-01-05 14:55:41,998: t15.2023.08.20 val PER: 0.1414
2026-01-05 14:55:41,998: t15.2023.08.25 val PER: 0.1024
2026-01-05 14:55:41,998: t15.2023.08.27 val PER: 0.2138
2026-01-05 14:55:41,998: t15.2023.09.01 val PER: 0.1185
2026-01-05 14:55:41,998: t15.2023.09.03 val PER: 0.1912
2026-01-05 14:55:41,998: t15.2023.09.24 val PER: 0.1663
2026-01-05 14:55:41,998: t15.2023.09.29 val PER: 0.1653
2026-01-05 14:55:41,998: t15.2023.10.01 val PER: 0.2094
2026-01-05 14:55:41,998: t15.2023.10.06 val PER: 0.1109
2026-01-05 14:55:41,999: t15.2023.10.08 val PER: 0.2706
2026-01-05 14:55:41,999: t15.2023.10.13 val PER: 0.2607
2026-01-05 14:55:41,999: t15.2023.10.15 val PER: 0.1971
2026-01-05 14:55:41,999: t15.2023.10.20 val PER: 0.2114
2026-01-05 14:55:41,999: t15.2023.10.22 val PER: 0.1425
2026-01-05 14:55:41,999: t15.2023.11.03 val PER: 0.2008
2026-01-05 14:55:41,999: t15.2023.11.04 val PER: 0.0410
2026-01-05 14:55:41,999: t15.2023.11.17 val PER: 0.0544
2026-01-05 14:55:41,999: t15.2023.11.19 val PER: 0.0599
2026-01-05 14:55:41,999: t15.2023.11.26 val PER: 0.1841
2026-01-05 14:55:41,999: t15.2023.12.03 val PER: 0.1649
2026-01-05 14:55:41,999: t15.2023.12.08 val PER: 0.1538
2026-01-05 14:55:41,999: t15.2023.12.10 val PER: 0.1393
2026-01-05 14:55:41,999: t15.2023.12.17 val PER: 0.1684
2026-01-05 14:55:42,000: t15.2023.12.29 val PER: 0.2038
2026-01-05 14:55:42,000: t15.2024.02.25 val PER: 0.1657
2026-01-05 14:55:42,000: t15.2024.03.08 val PER: 0.2873
2026-01-05 14:55:42,000: t15.2024.03.15 val PER: 0.2427
2026-01-05 14:55:42,000: t15.2024.03.17 val PER: 0.2008
2026-01-05 14:55:42,000: t15.2024.05.10 val PER: 0.1961
2026-01-05 14:55:42,000: t15.2024.06.14 val PER: 0.2098
2026-01-05 14:55:42,000: t15.2024.07.19 val PER: 0.3045
2026-01-05 14:55:42,001: t15.2024.07.21 val PER: 0.1407
2026-01-05 14:55:42,001: t15.2024.07.28 val PER: 0.1728
2026-01-05 14:55:42,001: t15.2025.01.10 val PER: 0.3650
2026-01-05 14:55:42,001: t15.2025.01.12 val PER: 0.2009
2026-01-05 14:55:42,001: t15.2025.03.14 val PER: 0.3506
2026-01-05 14:55:42,001: t15.2025.03.16 val PER: 0.2500
2026-01-05 14:55:42,001: t15.2025.03.30 val PER: 0.3563
2026-01-05 14:55:42,001: t15.2025.04.13 val PER: 0.2725
2026-01-05 14:55:42,002: New best val WER(1gram) 53.81% --> 53.30%
2026-01-05 14:55:42,002: Checkpointing model
2026-01-05 14:55:42,327: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:55:42,631: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_7000
2026-01-05 14:56:01,528: Train batch 7200: loss: 14.83 grad norm: 57.89 time: 0.079
2026-01-05 14:56:20,130: Train batch 7400: loss: 13.71 grad norm: 56.86 time: 0.075
2026-01-05 14:56:29,394: Running test after training batch: 7500
2026-01-05 14:56:29,530: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:56:34,701: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-05 14:56:34,734: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-05 14:56:36,489: Val batch 7500: PER (avg): 0.1911 CTC Loss (avg): 18.6792 WER(1gram): 53.30% (n=64) time: 7.095
2026-01-05 14:56:36,490: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 14:56:36,490: t15.2023.08.13 val PER: 0.1486
2026-01-05 14:56:36,490: t15.2023.08.18 val PER: 0.1383
2026-01-05 14:56:36,490: t15.2023.08.20 val PER: 0.1469
2026-01-05 14:56:36,490: t15.2023.08.25 val PER: 0.1099
2026-01-05 14:56:36,490: t15.2023.08.27 val PER: 0.2106
2026-01-05 14:56:36,490: t15.2023.09.01 val PER: 0.1201
2026-01-05 14:56:36,490: t15.2023.09.03 val PER: 0.1793
2026-01-05 14:56:36,490: t15.2023.09.24 val PER: 0.1638
2026-01-05 14:56:36,491: t15.2023.09.29 val PER: 0.1615
2026-01-05 14:56:36,491: t15.2023.10.01 val PER: 0.2107
2026-01-05 14:56:36,491: t15.2023.10.06 val PER: 0.1130
2026-01-05 14:56:36,491: t15.2023.10.08 val PER: 0.2733
2026-01-05 14:56:36,491: t15.2023.10.13 val PER: 0.2475
2026-01-05 14:56:36,491: t15.2023.10.15 val PER: 0.2011
2026-01-05 14:56:36,491: t15.2023.10.20 val PER: 0.2047
2026-01-05 14:56:36,491: t15.2023.10.22 val PER: 0.1425
2026-01-05 14:56:36,491: t15.2023.11.03 val PER: 0.2035
2026-01-05 14:56:36,491: t15.2023.11.04 val PER: 0.0512
2026-01-05 14:56:36,491: t15.2023.11.17 val PER: 0.0684
2026-01-05 14:56:36,491: t15.2023.11.19 val PER: 0.0479
2026-01-05 14:56:36,491: t15.2023.11.26 val PER: 0.1971
2026-01-05 14:56:36,492: t15.2023.12.03 val PER: 0.1649
2026-01-05 14:56:36,492: t15.2023.12.08 val PER: 0.1591
2026-01-05 14:56:36,492: t15.2023.12.10 val PER: 0.1314
2026-01-05 14:56:36,492: t15.2023.12.17 val PER: 0.1715
2026-01-05 14:56:36,492: t15.2023.12.29 val PER: 0.1915
2026-01-05 14:56:36,492: t15.2024.02.25 val PER: 0.1433
2026-01-05 14:56:36,492: t15.2024.03.08 val PER: 0.2589
2026-01-05 14:56:36,492: t15.2024.03.15 val PER: 0.2477
2026-01-05 14:56:36,492: t15.2024.03.17 val PER: 0.1883
2026-01-05 14:56:36,492: t15.2024.05.10 val PER: 0.2095
2026-01-05 14:56:36,492: t15.2024.06.14 val PER: 0.1877
2026-01-05 14:56:36,492: t15.2024.07.19 val PER: 0.2907
2026-01-05 14:56:36,492: t15.2024.07.21 val PER: 0.1455
2026-01-05 14:56:36,492: t15.2024.07.28 val PER: 0.1662
2026-01-05 14:56:36,492: t15.2025.01.10 val PER: 0.3375
2026-01-05 14:56:36,492: t15.2025.01.12 val PER: 0.1901
2026-01-05 14:56:36,493: t15.2025.03.14 val PER: 0.3595
2026-01-05 14:56:36,493: t15.2025.03.16 val PER: 0.2500
2026-01-05 14:56:36,493: t15.2025.03.30 val PER: 0.3494
2026-01-05 14:56:36,493: t15.2025.04.13 val PER: 0.2454
2026-01-05 14:56:36,784: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_7500
2026-01-05 14:56:46,144: Train batch 7600: loss: 16.02 grad norm: 60.35 time: 0.069
2026-01-05 14:57:04,941: Train batch 7800: loss: 14.10 grad norm: 57.74 time: 0.056
2026-01-05 14:57:23,979: Train batch 8000: loss: 10.99 grad norm: 49.76 time: 0.072
2026-01-05 14:57:23,979: Running test after training batch: 8000
2026-01-05 14:57:24,093: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:57:29,207: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 14:57:29,241: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 14:57:30,996: Val batch 8000: PER (avg): 0.1835 CTC Loss (avg): 17.9882 WER(1gram): 56.35% (n=64) time: 7.017
2026-01-05 14:57:30,996: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 14:57:30,997: t15.2023.08.13 val PER: 0.1445
2026-01-05 14:57:30,997: t15.2023.08.18 val PER: 0.1224
2026-01-05 14:57:30,997: t15.2023.08.20 val PER: 0.1422
2026-01-05 14:57:30,997: t15.2023.08.25 val PER: 0.1114
2026-01-05 14:57:30,997: t15.2023.08.27 val PER: 0.2186
2026-01-05 14:57:30,997: t15.2023.09.01 val PER: 0.1039
2026-01-05 14:57:30,997: t15.2023.09.03 val PER: 0.1888
2026-01-05 14:57:30,997: t15.2023.09.24 val PER: 0.1517
2026-01-05 14:57:30,997: t15.2023.09.29 val PER: 0.1551
2026-01-05 14:57:30,997: t15.2023.10.01 val PER: 0.1975
2026-01-05 14:57:30,997: t15.2023.10.06 val PER: 0.1098
2026-01-05 14:57:30,997: t15.2023.10.08 val PER: 0.2693
2026-01-05 14:57:30,997: t15.2023.10.13 val PER: 0.2428
2026-01-05 14:57:30,997: t15.2023.10.15 val PER: 0.1885
2026-01-05 14:57:30,998: t15.2023.10.20 val PER: 0.1913
2026-01-05 14:57:30,998: t15.2023.10.22 val PER: 0.1347
2026-01-05 14:57:30,998: t15.2023.11.03 val PER: 0.2022
2026-01-05 14:57:30,998: t15.2023.11.04 val PER: 0.0341
2026-01-05 14:57:30,998: t15.2023.11.17 val PER: 0.0591
2026-01-05 14:57:30,998: t15.2023.11.19 val PER: 0.0679
2026-01-05 14:57:30,998: t15.2023.11.26 val PER: 0.1768
2026-01-05 14:57:30,998: t15.2023.12.03 val PER: 0.1565
2026-01-05 14:57:30,999: t15.2023.12.08 val PER: 0.1485
2026-01-05 14:57:30,999: t15.2023.12.10 val PER: 0.1314
2026-01-05 14:57:30,999: t15.2023.12.17 val PER: 0.1559
2026-01-05 14:57:30,999: t15.2023.12.29 val PER: 0.1784
2026-01-05 14:57:30,999: t15.2024.02.25 val PER: 0.1376
2026-01-05 14:57:30,999: t15.2024.03.08 val PER: 0.2703
2026-01-05 14:57:30,999: t15.2024.03.15 val PER: 0.2370
2026-01-05 14:57:30,999: t15.2024.03.17 val PER: 0.1834
2026-01-05 14:57:30,999: t15.2024.05.10 val PER: 0.1932
2026-01-05 14:57:31,000: t15.2024.06.14 val PER: 0.1877
2026-01-05 14:57:31,000: t15.2024.07.19 val PER: 0.2874
2026-01-05 14:57:31,000: t15.2024.07.21 val PER: 0.1248
2026-01-05 14:57:31,000: t15.2024.07.28 val PER: 0.1596
2026-01-05 14:57:31,000: t15.2025.01.10 val PER: 0.3127
2026-01-05 14:57:31,000: t15.2025.01.12 val PER: 0.1894
2026-01-05 14:57:31,000: t15.2025.03.14 val PER: 0.3669
2026-01-05 14:57:31,000: t15.2025.03.16 val PER: 0.2291
2026-01-05 14:57:31,000: t15.2025.03.30 val PER: 0.3425
2026-01-05 14:57:31,000: t15.2025.04.13 val PER: 0.2496
2026-01-05 14:57:31,303: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_8000
2026-01-05 14:57:50,017: Train batch 8200: loss: 9.12 grad norm: 45.41 time: 0.054
2026-01-05 14:58:08,992: Train batch 8400: loss: 9.94 grad norm: 47.36 time: 0.064
2026-01-05 14:58:18,645: Running test after training batch: 8500
2026-01-05 14:58:18,770: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:58:24,053: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 14:58:24,085: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost ent
2026-01-05 14:58:25,811: Val batch 8500: PER (avg): 0.1781 CTC Loss (avg): 17.5511 WER(1gram): 49.49% (n=64) time: 7.166
2026-01-05 14:58:25,812: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 14:58:25,812: t15.2023.08.13 val PER: 0.1414
2026-01-05 14:58:25,812: t15.2023.08.18 val PER: 0.1282
2026-01-05 14:58:25,812: t15.2023.08.20 val PER: 0.1342
2026-01-05 14:58:25,812: t15.2023.08.25 val PER: 0.1190
2026-01-05 14:58:25,812: t15.2023.08.27 val PER: 0.2026
2026-01-05 14:58:25,812: t15.2023.09.01 val PER: 0.0974
2026-01-05 14:58:25,812: t15.2023.09.03 val PER: 0.1912
2026-01-05 14:58:25,813: t15.2023.09.24 val PER: 0.1456
2026-01-05 14:58:25,813: t15.2023.09.29 val PER: 0.1512
2026-01-05 14:58:25,813: t15.2023.10.01 val PER: 0.1869
2026-01-05 14:58:25,813: t15.2023.10.06 val PER: 0.1055
2026-01-05 14:58:25,813: t15.2023.10.08 val PER: 0.2585
2026-01-05 14:58:25,813: t15.2023.10.13 val PER: 0.2366
2026-01-05 14:58:25,813: t15.2023.10.15 val PER: 0.1859
2026-01-05 14:58:25,813: t15.2023.10.20 val PER: 0.2013
2026-01-05 14:58:25,813: t15.2023.10.22 val PER: 0.1437
2026-01-05 14:58:25,813: t15.2023.11.03 val PER: 0.2008
2026-01-05 14:58:25,813: t15.2023.11.04 val PER: 0.0478
2026-01-05 14:58:25,813: t15.2023.11.17 val PER: 0.0498
2026-01-05 14:58:25,814: t15.2023.11.19 val PER: 0.0539
2026-01-05 14:58:25,814: t15.2023.11.26 val PER: 0.1754
2026-01-05 14:58:25,814: t15.2023.12.03 val PER: 0.1397
2026-01-05 14:58:25,814: t15.2023.12.08 val PER: 0.1431
2026-01-05 14:58:25,814: t15.2023.12.10 val PER: 0.1196
2026-01-05 14:58:25,814: t15.2023.12.17 val PER: 0.1611
2026-01-05 14:58:25,814: t15.2023.12.29 val PER: 0.1723
2026-01-05 14:58:25,814: t15.2024.02.25 val PER: 0.1334
2026-01-05 14:58:25,814: t15.2024.03.08 val PER: 0.2546
2026-01-05 14:58:25,814: t15.2024.03.15 val PER: 0.2239
2026-01-05 14:58:25,814: t15.2024.03.17 val PER: 0.1695
2026-01-05 14:58:25,814: t15.2024.05.10 val PER: 0.2021
2026-01-05 14:58:25,814: t15.2024.06.14 val PER: 0.1814
2026-01-05 14:58:25,814: t15.2024.07.19 val PER: 0.2749
2026-01-05 14:58:25,814: t15.2024.07.21 val PER: 0.1186
2026-01-05 14:58:25,814: t15.2024.07.28 val PER: 0.1676
2026-01-05 14:58:25,814: t15.2025.01.10 val PER: 0.3196
2026-01-05 14:58:25,815: t15.2025.01.12 val PER: 0.1840
2026-01-05 14:58:25,815: t15.2025.03.14 val PER: 0.3595
2026-01-05 14:58:25,815: t15.2025.03.16 val PER: 0.2199
2026-01-05 14:58:25,815: t15.2025.03.30 val PER: 0.3207
2026-01-05 14:58:25,815: t15.2025.04.13 val PER: 0.2297
2026-01-05 14:58:25,816: New best val WER(1gram) 53.30% --> 49.49%
2026-01-05 14:58:25,816: Checkpointing model
2026-01-05 14:58:26,520: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 14:58:26,834: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_8500
2026-01-05 14:58:36,382: Train batch 8600: loss: 15.77 grad norm: 62.70 time: 0.055
2026-01-05 14:58:55,186: Train batch 8800: loss: 15.19 grad norm: 60.64 time: 0.061
2026-01-05 14:59:14,356: Train batch 9000: loss: 15.86 grad norm: 66.20 time: 0.072
2026-01-05 14:59:14,357: Running test after training batch: 9000
2026-01-05 14:59:14,493: WER debug GT example: You can see the code at this point as well.
2026-01-05 14:59:19,591: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-05 14:59:19,623: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-05 14:59:21,378: Val batch 9000: PER (avg): 0.1747 CTC Loss (avg): 17.1975 WER(1gram): 51.27% (n=64) time: 7.020
2026-01-05 14:59:21,378: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 14:59:21,378: t15.2023.08.13 val PER: 0.1403
2026-01-05 14:59:21,378: t15.2023.08.18 val PER: 0.1249
2026-01-05 14:59:21,379: t15.2023.08.20 val PER: 0.1255
2026-01-05 14:59:21,379: t15.2023.08.25 val PER: 0.1024
2026-01-05 14:59:21,379: t15.2023.08.27 val PER: 0.2106
2026-01-05 14:59:21,379: t15.2023.09.01 val PER: 0.0982
2026-01-05 14:59:21,379: t15.2023.09.03 val PER: 0.1876
2026-01-05 14:59:21,379: t15.2023.09.24 val PER: 0.1529
2026-01-05 14:59:21,379: t15.2023.09.29 val PER: 0.1455
2026-01-05 14:59:21,379: t15.2023.10.01 val PER: 0.1909
2026-01-05 14:59:21,379: t15.2023.10.06 val PER: 0.0990
2026-01-05 14:59:21,379: t15.2023.10.08 val PER: 0.2530
2026-01-05 14:59:21,379: t15.2023.10.13 val PER: 0.2327
2026-01-05 14:59:21,379: t15.2023.10.15 val PER: 0.1813
2026-01-05 14:59:21,380: t15.2023.10.20 val PER: 0.1980
2026-01-05 14:59:21,380: t15.2023.10.22 val PER: 0.1269
2026-01-05 14:59:21,380: t15.2023.11.03 val PER: 0.2015
2026-01-05 14:59:21,380: t15.2023.11.04 val PER: 0.0341
2026-01-05 14:59:21,380: t15.2023.11.17 val PER: 0.0607
2026-01-05 14:59:21,380: t15.2023.11.19 val PER: 0.0479
2026-01-05 14:59:21,380: t15.2023.11.26 val PER: 0.1623
2026-01-05 14:59:21,380: t15.2023.12.03 val PER: 0.1387
2026-01-05 14:59:21,380: t15.2023.12.08 val PER: 0.1358
2026-01-05 14:59:21,380: t15.2023.12.10 val PER: 0.1104
2026-01-05 14:59:21,380: t15.2023.12.17 val PER: 0.1611
2026-01-05 14:59:21,380: t15.2023.12.29 val PER: 0.1675
2026-01-05 14:59:21,380: t15.2024.02.25 val PER: 0.1376
2026-01-05 14:59:21,380: t15.2024.03.08 val PER: 0.2560
2026-01-05 14:59:21,381: t15.2024.03.15 val PER: 0.2295
2026-01-05 14:59:21,381: t15.2024.03.17 val PER: 0.1729
2026-01-05 14:59:21,381: t15.2024.05.10 val PER: 0.1902
2026-01-05 14:59:21,381: t15.2024.06.14 val PER: 0.1782
2026-01-05 14:59:21,381: t15.2024.07.19 val PER: 0.2676
2026-01-05 14:59:21,381: t15.2024.07.21 val PER: 0.1193
2026-01-05 14:59:21,381: t15.2024.07.28 val PER: 0.1559
2026-01-05 14:59:21,381: t15.2025.01.10 val PER: 0.3113
2026-01-05 14:59:21,381: t15.2025.01.12 val PER: 0.1740
2026-01-05 14:59:21,381: t15.2025.03.14 val PER: 0.3595
2026-01-05 14:59:21,381: t15.2025.03.16 val PER: 0.2173
2026-01-05 14:59:21,381: t15.2025.03.30 val PER: 0.3161
2026-01-05 14:59:21,381: t15.2025.04.13 val PER: 0.2368
2026-01-05 14:59:21,713: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_9000
2026-01-05 14:59:40,765: Train batch 9200: loss: 10.88 grad norm: 50.15 time: 0.056
2026-01-05 14:59:59,749: Train batch 9400: loss: 7.12 grad norm: 43.59 time: 0.068
2026-01-05 15:00:09,268: Running test after training batch: 9500
2026-01-05 15:00:09,449: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:00:14,532: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:00:14,565: WER debug example
  GT : how does it keep the cost down
  PR : houde dusts it keep the cost it
2026-01-05 15:00:16,340: Val batch 9500: PER (avg): 0.1756 CTC Loss (avg): 17.1794 WER(1gram): 49.49% (n=64) time: 7.071
2026-01-05 15:00:16,340: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 15:00:16,340: t15.2023.08.13 val PER: 0.1310
2026-01-05 15:00:16,341: t15.2023.08.18 val PER: 0.1241
2026-01-05 15:00:16,341: t15.2023.08.20 val PER: 0.1366
2026-01-05 15:00:16,341: t15.2023.08.25 val PER: 0.1069
2026-01-05 15:00:16,341: t15.2023.08.27 val PER: 0.1961
2026-01-05 15:00:16,341: t15.2023.09.01 val PER: 0.1047
2026-01-05 15:00:16,341: t15.2023.09.03 val PER: 0.1865
2026-01-05 15:00:16,341: t15.2023.09.24 val PER: 0.1456
2026-01-05 15:00:16,341: t15.2023.09.29 val PER: 0.1538
2026-01-05 15:00:16,342: t15.2023.10.01 val PER: 0.1982
2026-01-05 15:00:16,342: t15.2023.10.06 val PER: 0.1033
2026-01-05 15:00:16,342: t15.2023.10.08 val PER: 0.2666
2026-01-05 15:00:16,342: t15.2023.10.13 val PER: 0.2436
2026-01-05 15:00:16,342: t15.2023.10.15 val PER: 0.1819
2026-01-05 15:00:16,342: t15.2023.10.20 val PER: 0.1980
2026-01-05 15:00:16,342: t15.2023.10.22 val PER: 0.1225
2026-01-05 15:00:16,342: t15.2023.11.03 val PER: 0.1872
2026-01-05 15:00:16,342: t15.2023.11.04 val PER: 0.0375
2026-01-05 15:00:16,342: t15.2023.11.17 val PER: 0.0529
2026-01-05 15:00:16,343: t15.2023.11.19 val PER: 0.0539
2026-01-05 15:00:16,343: t15.2023.11.26 val PER: 0.1536
2026-01-05 15:00:16,343: t15.2023.12.03 val PER: 0.1376
2026-01-05 15:00:16,343: t15.2023.12.08 val PER: 0.1425
2026-01-05 15:00:16,343: t15.2023.12.10 val PER: 0.1183
2026-01-05 15:00:16,343: t15.2023.12.17 val PER: 0.1611
2026-01-05 15:00:16,343: t15.2023.12.29 val PER: 0.1531
2026-01-05 15:00:16,343: t15.2024.02.25 val PER: 0.1376
2026-01-05 15:00:16,343: t15.2024.03.08 val PER: 0.2617
2026-01-05 15:00:16,343: t15.2024.03.15 val PER: 0.2283
2026-01-05 15:00:16,344: t15.2024.03.17 val PER: 0.1715
2026-01-05 15:00:16,344: t15.2024.05.10 val PER: 0.2065
2026-01-05 15:00:16,344: t15.2024.06.14 val PER: 0.1703
2026-01-05 15:00:16,344: t15.2024.07.19 val PER: 0.2775
2026-01-05 15:00:16,344: t15.2024.07.21 val PER: 0.1159
2026-01-05 15:00:16,344: t15.2024.07.28 val PER: 0.1566
2026-01-05 15:00:16,344: t15.2025.01.10 val PER: 0.3196
2026-01-05 15:00:16,344: t15.2025.01.12 val PER: 0.1740
2026-01-05 15:00:16,344: t15.2025.03.14 val PER: 0.3817
2026-01-05 15:00:16,344: t15.2025.03.16 val PER: 0.2068
2026-01-05 15:00:16,344: t15.2025.03.30 val PER: 0.3161
2026-01-05 15:00:16,345: t15.2025.04.13 val PER: 0.2354
2026-01-05 15:00:16,672: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_9500
2026-01-05 15:00:26,080: Train batch 9600: loss: 8.27 grad norm: 46.87 time: 0.074
2026-01-05 15:00:45,096: Train batch 9800: loss: 11.64 grad norm: 56.99 time: 0.063
2026-01-05 15:01:04,221: Train batch 10000: loss: 5.42 grad norm: 36.28 time: 0.061
2026-01-05 15:01:04,222: Running test after training batch: 10000
2026-01-05 15:01:04,361: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:01:09,616: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:01:09,649: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-05 15:01:11,412: Val batch 10000: PER (avg): 0.1689 CTC Loss (avg): 16.6974 WER(1gram): 48.98% (n=64) time: 7.190
2026-01-05 15:01:11,412: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-05 15:01:11,413: t15.2023.08.13 val PER: 0.1258
2026-01-05 15:01:11,413: t15.2023.08.18 val PER: 0.1232
2026-01-05 15:01:11,413: t15.2023.08.20 val PER: 0.1263
2026-01-05 15:01:11,413: t15.2023.08.25 val PER: 0.1099
2026-01-05 15:01:11,413: t15.2023.08.27 val PER: 0.2074
2026-01-05 15:01:11,413: t15.2023.09.01 val PER: 0.0909
2026-01-05 15:01:11,413: t15.2023.09.03 val PER: 0.1770
2026-01-05 15:01:11,413: t15.2023.09.24 val PER: 0.1468
2026-01-05 15:01:11,413: t15.2023.09.29 val PER: 0.1461
2026-01-05 15:01:11,414: t15.2023.10.01 val PER: 0.1810
2026-01-05 15:01:11,414: t15.2023.10.06 val PER: 0.0958
2026-01-05 15:01:11,414: t15.2023.10.08 val PER: 0.2368
2026-01-05 15:01:11,414: t15.2023.10.13 val PER: 0.2265
2026-01-05 15:01:11,414: t15.2023.10.15 val PER: 0.1773
2026-01-05 15:01:11,414: t15.2023.10.20 val PER: 0.1846
2026-01-05 15:01:11,414: t15.2023.10.22 val PER: 0.1258
2026-01-05 15:01:11,414: t15.2023.11.03 val PER: 0.1900
2026-01-05 15:01:11,414: t15.2023.11.04 val PER: 0.0341
2026-01-05 15:01:11,415: t15.2023.11.17 val PER: 0.0498
2026-01-05 15:01:11,415: t15.2023.11.19 val PER: 0.0359
2026-01-05 15:01:11,415: t15.2023.11.26 val PER: 0.1428
2026-01-05 15:01:11,415: t15.2023.12.03 val PER: 0.1355
2026-01-05 15:01:11,415: t15.2023.12.08 val PER: 0.1365
2026-01-05 15:01:11,415: t15.2023.12.10 val PER: 0.1091
2026-01-05 15:01:11,415: t15.2023.12.17 val PER: 0.1601
2026-01-05 15:01:11,415: t15.2023.12.29 val PER: 0.1503
2026-01-05 15:01:11,415: t15.2024.02.25 val PER: 0.1503
2026-01-05 15:01:11,415: t15.2024.03.08 val PER: 0.2390
2026-01-05 15:01:11,416: t15.2024.03.15 val PER: 0.2133
2026-01-05 15:01:11,416: t15.2024.03.17 val PER: 0.1611
2026-01-05 15:01:11,416: t15.2024.05.10 val PER: 0.1768
2026-01-05 15:01:11,416: t15.2024.06.14 val PER: 0.1782
2026-01-05 15:01:11,416: t15.2024.07.19 val PER: 0.2690
2026-01-05 15:01:11,416: t15.2024.07.21 val PER: 0.1145
2026-01-05 15:01:11,416: t15.2024.07.28 val PER: 0.1566
2026-01-05 15:01:11,416: t15.2025.01.10 val PER: 0.3168
2026-01-05 15:01:11,417: t15.2025.01.12 val PER: 0.1771
2026-01-05 15:01:11,417: t15.2025.03.14 val PER: 0.3536
2026-01-05 15:01:11,417: t15.2025.03.16 val PER: 0.2055
2026-01-05 15:01:11,417: t15.2025.03.30 val PER: 0.3184
2026-01-05 15:01:11,417: t15.2025.04.13 val PER: 0.2268
2026-01-05 15:01:11,417: New best val WER(1gram) 49.49% --> 48.98%
2026-01-05 15:01:11,417: Checkpointing model
2026-01-05 15:01:12,138: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 15:01:12,486: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_10000
2026-01-05 15:01:31,290: Train batch 10200: loss: 6.29 grad norm: 40.57 time: 0.050
2026-01-05 15:01:50,518: Train batch 10400: loss: 9.42 grad norm: 52.21 time: 0.072
2026-01-05 15:02:00,188: Running test after training batch: 10500
2026-01-05 15:02:00,355: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:02:05,422: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:02:05,455: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost it
2026-01-05 15:02:07,267: Val batch 10500: PER (avg): 0.1646 CTC Loss (avg): 16.6355 WER(1gram): 49.49% (n=64) time: 7.078
2026-01-05 15:02:07,267: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 15:02:07,267: t15.2023.08.13 val PER: 0.1175
2026-01-05 15:02:07,268: t15.2023.08.18 val PER: 0.1232
2026-01-05 15:02:07,268: t15.2023.08.20 val PER: 0.1239
2026-01-05 15:02:07,268: t15.2023.08.25 val PER: 0.1145
2026-01-05 15:02:07,268: t15.2023.08.27 val PER: 0.1945
2026-01-05 15:02:07,268: t15.2023.09.01 val PER: 0.0877
2026-01-05 15:02:07,268: t15.2023.09.03 val PER: 0.1758
2026-01-05 15:02:07,268: t15.2023.09.24 val PER: 0.1432
2026-01-05 15:02:07,268: t15.2023.09.29 val PER: 0.1436
2026-01-05 15:02:07,268: t15.2023.10.01 val PER: 0.1816
2026-01-05 15:02:07,269: t15.2023.10.06 val PER: 0.0915
2026-01-05 15:02:07,269: t15.2023.10.08 val PER: 0.2490
2026-01-05 15:02:07,269: t15.2023.10.13 val PER: 0.2157
2026-01-05 15:02:07,269: t15.2023.10.15 val PER: 0.1641
2026-01-05 15:02:07,269: t15.2023.10.20 val PER: 0.1879
2026-01-05 15:02:07,269: t15.2023.10.22 val PER: 0.1180
2026-01-05 15:02:07,269: t15.2023.11.03 val PER: 0.1893
2026-01-05 15:02:07,269: t15.2023.11.04 val PER: 0.0410
2026-01-05 15:02:07,269: t15.2023.11.17 val PER: 0.0451
2026-01-05 15:02:07,269: t15.2023.11.19 val PER: 0.0439
2026-01-05 15:02:07,270: t15.2023.11.26 val PER: 0.1341
2026-01-05 15:02:07,270: t15.2023.12.03 val PER: 0.1313
2026-01-05 15:02:07,270: t15.2023.12.08 val PER: 0.1265
2026-01-05 15:02:07,270: t15.2023.12.10 val PER: 0.1012
2026-01-05 15:02:07,270: t15.2023.12.17 val PER: 0.1424
2026-01-05 15:02:07,270: t15.2023.12.29 val PER: 0.1565
2026-01-05 15:02:07,270: t15.2024.02.25 val PER: 0.1306
2026-01-05 15:02:07,270: t15.2024.03.08 val PER: 0.2319
2026-01-05 15:02:07,271: t15.2024.03.15 val PER: 0.2164
2026-01-05 15:02:07,271: t15.2024.03.17 val PER: 0.1604
2026-01-05 15:02:07,271: t15.2024.05.10 val PER: 0.1738
2026-01-05 15:02:07,271: t15.2024.06.14 val PER: 0.1688
2026-01-05 15:02:07,271: t15.2024.07.19 val PER: 0.2637
2026-01-05 15:02:07,271: t15.2024.07.21 val PER: 0.1069
2026-01-05 15:02:07,271: t15.2024.07.28 val PER: 0.1515
2026-01-05 15:02:07,271: t15.2025.01.10 val PER: 0.3168
2026-01-05 15:02:07,272: t15.2025.01.12 val PER: 0.1709
2026-01-05 15:02:07,272: t15.2025.03.14 val PER: 0.3565
2026-01-05 15:02:07,272: t15.2025.03.16 val PER: 0.1950
2026-01-05 15:02:07,272: t15.2025.03.30 val PER: 0.3069
2026-01-05 15:02:07,272: t15.2025.04.13 val PER: 0.2282
2026-01-05 15:02:07,614: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_10500
2026-01-05 15:02:17,355: Train batch 10600: loss: 9.50 grad norm: 58.00 time: 0.073
2026-01-05 15:02:36,371: Train batch 10800: loss: 14.59 grad norm: 63.88 time: 0.064
2026-01-05 15:02:55,631: Train batch 11000: loss: 14.15 grad norm: 66.17 time: 0.057
2026-01-05 15:02:55,631: Running test after training batch: 11000
2026-01-05 15:02:55,807: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:03:00,927: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:03:00,961: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 15:03:02,771: Val batch 11000: PER (avg): 0.1631 CTC Loss (avg): 16.3703 WER(1gram): 48.98% (n=64) time: 7.139
2026-01-05 15:03:02,771: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-05 15:03:02,771: t15.2023.08.13 val PER: 0.1227
2026-01-05 15:03:02,771: t15.2023.08.18 val PER: 0.1190
2026-01-05 15:03:02,772: t15.2023.08.20 val PER: 0.1104
2026-01-05 15:03:02,772: t15.2023.08.25 val PER: 0.1054
2026-01-05 15:03:02,772: t15.2023.08.27 val PER: 0.1977
2026-01-05 15:03:02,772: t15.2023.09.01 val PER: 0.0812
2026-01-05 15:03:02,772: t15.2023.09.03 val PER: 0.1686
2026-01-05 15:03:02,772: t15.2023.09.24 val PER: 0.1432
2026-01-05 15:03:02,772: t15.2023.09.29 val PER: 0.1391
2026-01-05 15:03:02,772: t15.2023.10.01 val PER: 0.1816
2026-01-05 15:03:02,772: t15.2023.10.06 val PER: 0.0904
2026-01-05 15:03:02,772: t15.2023.10.08 val PER: 0.2598
2026-01-05 15:03:02,772: t15.2023.10.13 val PER: 0.2149
2026-01-05 15:03:02,772: t15.2023.10.15 val PER: 0.1694
2026-01-05 15:03:02,773: t15.2023.10.20 val PER: 0.2013
2026-01-05 15:03:02,773: t15.2023.10.22 val PER: 0.1214
2026-01-05 15:03:02,773: t15.2023.11.03 val PER: 0.1866
2026-01-05 15:03:02,773: t15.2023.11.04 val PER: 0.0444
2026-01-05 15:03:02,773: t15.2023.11.17 val PER: 0.0529
2026-01-05 15:03:02,773: t15.2023.11.19 val PER: 0.0379
2026-01-05 15:03:02,773: t15.2023.11.26 val PER: 0.1355
2026-01-05 15:03:02,773: t15.2023.12.03 val PER: 0.1197
2026-01-05 15:03:02,773: t15.2023.12.08 val PER: 0.1165
2026-01-05 15:03:02,773: t15.2023.12.10 val PER: 0.0999
2026-01-05 15:03:02,773: t15.2023.12.17 val PER: 0.1445
2026-01-05 15:03:02,773: t15.2023.12.29 val PER: 0.1462
2026-01-05 15:03:02,773: t15.2024.02.25 val PER: 0.1362
2026-01-05 15:03:02,773: t15.2024.03.08 val PER: 0.2376
2026-01-05 15:03:02,773: t15.2024.03.15 val PER: 0.2208
2026-01-05 15:03:02,773: t15.2024.03.17 val PER: 0.1569
2026-01-05 15:03:02,774: t15.2024.05.10 val PER: 0.1828
2026-01-05 15:03:02,774: t15.2024.06.14 val PER: 0.1640
2026-01-05 15:03:02,774: t15.2024.07.19 val PER: 0.2584
2026-01-05 15:03:02,774: t15.2024.07.21 val PER: 0.1062
2026-01-05 15:03:02,774: t15.2024.07.28 val PER: 0.1412
2026-01-05 15:03:02,774: t15.2025.01.10 val PER: 0.3140
2026-01-05 15:03:02,774: t15.2025.01.12 val PER: 0.1686
2026-01-05 15:03:02,774: t15.2025.03.14 val PER: 0.3609
2026-01-05 15:03:02,774: t15.2025.03.16 val PER: 0.2003
2026-01-05 15:03:02,774: t15.2025.03.30 val PER: 0.3069
2026-01-05 15:03:02,774: t15.2025.04.13 val PER: 0.2340
2026-01-05 15:03:03,122: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_11000
2026-01-05 15:03:22,385: Train batch 11200: loss: 10.00 grad norm: 50.99 time: 0.072
2026-01-05 15:03:41,441: Train batch 11400: loss: 9.53 grad norm: 54.04 time: 0.057
2026-01-05 15:03:51,053: Running test after training batch: 11500
2026-01-05 15:03:51,238: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:03:56,325: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:03:56,360: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 15:03:58,195: Val batch 11500: PER (avg): 0.1607 CTC Loss (avg): 16.3396 WER(1gram): 50.51% (n=64) time: 7.142
2026-01-05 15:03:58,196: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 15:03:58,196: t15.2023.08.13 val PER: 0.1154
2026-01-05 15:03:58,196: t15.2023.08.18 val PER: 0.1174
2026-01-05 15:03:58,196: t15.2023.08.20 val PER: 0.1183
2026-01-05 15:03:58,196: t15.2023.08.25 val PER: 0.1054
2026-01-05 15:03:58,197: t15.2023.08.27 val PER: 0.1897
2026-01-05 15:03:58,197: t15.2023.09.01 val PER: 0.0877
2026-01-05 15:03:58,197: t15.2023.09.03 val PER: 0.1686
2026-01-05 15:03:58,197: t15.2023.09.24 val PER: 0.1226
2026-01-05 15:03:58,197: t15.2023.09.29 val PER: 0.1385
2026-01-05 15:03:58,197: t15.2023.10.01 val PER: 0.1810
2026-01-05 15:03:58,197: t15.2023.10.06 val PER: 0.0840
2026-01-05 15:03:58,197: t15.2023.10.08 val PER: 0.2490
2026-01-05 15:03:58,197: t15.2023.10.13 val PER: 0.2126
2026-01-05 15:03:58,197: t15.2023.10.15 val PER: 0.1628
2026-01-05 15:03:58,198: t15.2023.10.20 val PER: 0.2215
2026-01-05 15:03:58,198: t15.2023.10.22 val PER: 0.1214
2026-01-05 15:03:58,198: t15.2023.11.03 val PER: 0.1866
2026-01-05 15:03:58,198: t15.2023.11.04 val PER: 0.0410
2026-01-05 15:03:58,198: t15.2023.11.17 val PER: 0.0482
2026-01-05 15:03:58,198: t15.2023.11.19 val PER: 0.0419
2026-01-05 15:03:58,198: t15.2023.11.26 val PER: 0.1290
2026-01-05 15:03:58,198: t15.2023.12.03 val PER: 0.1239
2026-01-05 15:03:58,198: t15.2023.12.08 val PER: 0.1152
2026-01-05 15:03:58,198: t15.2023.12.10 val PER: 0.1012
2026-01-05 15:03:58,198: t15.2023.12.17 val PER: 0.1445
2026-01-05 15:03:58,198: t15.2023.12.29 val PER: 0.1469
2026-01-05 15:03:58,198: t15.2024.02.25 val PER: 0.1222
2026-01-05 15:03:58,199: t15.2024.03.08 val PER: 0.2404
2026-01-05 15:03:58,199: t15.2024.03.15 val PER: 0.2201
2026-01-05 15:03:58,199: t15.2024.03.17 val PER: 0.1569
2026-01-05 15:03:58,199: t15.2024.05.10 val PER: 0.1679
2026-01-05 15:03:58,199: t15.2024.06.14 val PER: 0.1672
2026-01-05 15:03:58,199: t15.2024.07.19 val PER: 0.2577
2026-01-05 15:03:58,199: t15.2024.07.21 val PER: 0.1048
2026-01-05 15:03:58,199: t15.2024.07.28 val PER: 0.1382
2026-01-05 15:03:58,199: t15.2025.01.10 val PER: 0.3085
2026-01-05 15:03:58,199: t15.2025.01.12 val PER: 0.1601
2026-01-05 15:03:58,199: t15.2025.03.14 val PER: 0.3609
2026-01-05 15:03:58,200: t15.2025.03.16 val PER: 0.1937
2026-01-05 15:03:58,200: t15.2025.03.30 val PER: 0.3000
2026-01-05 15:03:58,200: t15.2025.04.13 val PER: 0.2268
2026-01-05 15:03:58,542: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_11500
2026-01-05 15:04:07,728: Train batch 11600: loss: 10.91 grad norm: 48.52 time: 0.061
2026-01-05 15:04:26,589: Train batch 11800: loss: 6.60 grad norm: 39.97 time: 0.045
2026-01-05 15:04:45,534: Train batch 12000: loss: 13.66 grad norm: 56.35 time: 0.071
2026-01-05 15:04:45,534: Running test after training batch: 12000
2026-01-05 15:04:45,634: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:04:50,797: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:04:50,832: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sette
2026-01-05 15:04:52,726: Val batch 12000: PER (avg): 0.1579 CTC Loss (avg): 16.0311 WER(1gram): 50.51% (n=64) time: 7.192
2026-01-05 15:04:52,727: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 15:04:52,727: t15.2023.08.13 val PER: 0.1195
2026-01-05 15:04:52,727: t15.2023.08.18 val PER: 0.1039
2026-01-05 15:04:52,727: t15.2023.08.20 val PER: 0.1144
2026-01-05 15:04:52,727: t15.2023.08.25 val PER: 0.1039
2026-01-05 15:04:52,727: t15.2023.08.27 val PER: 0.1865
2026-01-05 15:04:52,727: t15.2023.09.01 val PER: 0.0860
2026-01-05 15:04:52,727: t15.2023.09.03 val PER: 0.1651
2026-01-05 15:04:52,728: t15.2023.09.24 val PER: 0.1371
2026-01-05 15:04:52,728: t15.2023.09.29 val PER: 0.1283
2026-01-05 15:04:52,732: t15.2023.10.01 val PER: 0.1757
2026-01-05 15:04:52,732: t15.2023.10.06 val PER: 0.0861
2026-01-05 15:04:52,732: t15.2023.10.08 val PER: 0.2490
2026-01-05 15:04:52,732: t15.2023.10.13 val PER: 0.2141
2026-01-05 15:04:52,732: t15.2023.10.15 val PER: 0.1556
2026-01-05 15:04:52,732: t15.2023.10.20 val PER: 0.1946
2026-01-05 15:04:52,732: t15.2023.10.22 val PER: 0.1225
2026-01-05 15:04:52,732: t15.2023.11.03 val PER: 0.1805
2026-01-05 15:04:52,733: t15.2023.11.04 val PER: 0.0375
2026-01-05 15:04:52,733: t15.2023.11.17 val PER: 0.0404
2026-01-05 15:04:52,733: t15.2023.11.19 val PER: 0.0399
2026-01-05 15:04:52,733: t15.2023.11.26 val PER: 0.1268
2026-01-05 15:04:52,733: t15.2023.12.03 val PER: 0.1145
2026-01-05 15:04:52,733: t15.2023.12.08 val PER: 0.1158
2026-01-05 15:04:52,733: t15.2023.12.10 val PER: 0.0972
2026-01-05 15:04:52,733: t15.2023.12.17 val PER: 0.1445
2026-01-05 15:04:52,733: t15.2023.12.29 val PER: 0.1366
2026-01-05 15:04:52,733: t15.2024.02.25 val PER: 0.1180
2026-01-05 15:04:52,733: t15.2024.03.08 val PER: 0.2447
2026-01-05 15:04:52,733: t15.2024.03.15 val PER: 0.2120
2026-01-05 15:04:52,733: t15.2024.03.17 val PER: 0.1506
2026-01-05 15:04:52,734: t15.2024.05.10 val PER: 0.1738
2026-01-05 15:04:52,734: t15.2024.06.14 val PER: 0.1798
2026-01-05 15:04:52,734: t15.2024.07.19 val PER: 0.2558
2026-01-05 15:04:52,734: t15.2024.07.21 val PER: 0.1041
2026-01-05 15:04:52,734: t15.2024.07.28 val PER: 0.1375
2026-01-05 15:04:52,734: t15.2025.01.10 val PER: 0.3017
2026-01-05 15:04:52,734: t15.2025.01.12 val PER: 0.1586
2026-01-05 15:04:52,734: t15.2025.03.14 val PER: 0.3669
2026-01-05 15:04:52,734: t15.2025.03.16 val PER: 0.1963
2026-01-05 15:04:52,734: t15.2025.03.30 val PER: 0.3034
2026-01-05 15:04:52,734: t15.2025.04.13 val PER: 0.2126
2026-01-05 15:04:53,053: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_12000
2026-01-05 15:05:11,895: Train batch 12200: loss: 5.74 grad norm: 39.69 time: 0.065
2026-01-05 15:05:30,799: Train batch 12400: loss: 4.78 grad norm: 36.23 time: 0.041
2026-01-05 15:05:40,594: Running test after training batch: 12500
2026-01-05 15:05:40,766: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:05:45,870: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:05:45,904: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost jent
2026-01-05 15:05:47,737: Val batch 12500: PER (avg): 0.1558 CTC Loss (avg): 15.9683 WER(1gram): 47.46% (n=64) time: 7.143
2026-01-05 15:05:47,738: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 15:05:47,738: t15.2023.08.13 val PER: 0.1227
2026-01-05 15:05:47,738: t15.2023.08.18 val PER: 0.1106
2026-01-05 15:05:47,738: t15.2023.08.20 val PER: 0.1048
2026-01-05 15:05:47,738: t15.2023.08.25 val PER: 0.0919
2026-01-05 15:05:47,738: t15.2023.08.27 val PER: 0.1961
2026-01-05 15:05:47,738: t15.2023.09.01 val PER: 0.0836
2026-01-05 15:05:47,738: t15.2023.09.03 val PER: 0.1627
2026-01-05 15:05:47,738: t15.2023.09.24 val PER: 0.1286
2026-01-05 15:05:47,738: t15.2023.09.29 val PER: 0.1302
2026-01-05 15:05:47,738: t15.2023.10.01 val PER: 0.1724
2026-01-05 15:05:47,738: t15.2023.10.06 val PER: 0.0883
2026-01-05 15:05:47,738: t15.2023.10.08 val PER: 0.2558
2026-01-05 15:05:47,739: t15.2023.10.13 val PER: 0.2033
2026-01-05 15:05:47,739: t15.2023.10.15 val PER: 0.1543
2026-01-05 15:05:47,739: t15.2023.10.20 val PER: 0.2013
2026-01-05 15:05:47,739: t15.2023.10.22 val PER: 0.1169
2026-01-05 15:05:47,739: t15.2023.11.03 val PER: 0.1872
2026-01-05 15:05:47,739: t15.2023.11.04 val PER: 0.0341
2026-01-05 15:05:47,739: t15.2023.11.17 val PER: 0.0404
2026-01-05 15:05:47,739: t15.2023.11.19 val PER: 0.0299
2026-01-05 15:05:47,739: t15.2023.11.26 val PER: 0.1261
2026-01-05 15:05:47,739: t15.2023.12.03 val PER: 0.1250
2026-01-05 15:05:47,739: t15.2023.12.08 val PER: 0.1092
2026-01-05 15:05:47,740: t15.2023.12.10 val PER: 0.0959
2026-01-05 15:05:47,740: t15.2023.12.17 val PER: 0.1435
2026-01-05 15:05:47,740: t15.2023.12.29 val PER: 0.1338
2026-01-05 15:05:47,740: t15.2024.02.25 val PER: 0.1053
2026-01-05 15:05:47,740: t15.2024.03.08 val PER: 0.2404
2026-01-05 15:05:47,740: t15.2024.03.15 val PER: 0.2189
2026-01-05 15:05:47,740: t15.2024.03.17 val PER: 0.1520
2026-01-05 15:05:47,740: t15.2024.05.10 val PER: 0.1620
2026-01-05 15:05:47,740: t15.2024.06.14 val PER: 0.1703
2026-01-05 15:05:47,741: t15.2024.07.19 val PER: 0.2459
2026-01-05 15:05:47,741: t15.2024.07.21 val PER: 0.1000
2026-01-05 15:05:47,741: t15.2024.07.28 val PER: 0.1331
2026-01-05 15:05:47,741: t15.2025.01.10 val PER: 0.3127
2026-01-05 15:05:47,741: t15.2025.01.12 val PER: 0.1470
2026-01-05 15:05:47,741: t15.2025.03.14 val PER: 0.3595
2026-01-05 15:05:47,741: t15.2025.03.16 val PER: 0.1911
2026-01-05 15:05:47,741: t15.2025.03.30 val PER: 0.3115
2026-01-05 15:05:47,741: t15.2025.04.13 val PER: 0.2083
2026-01-05 15:05:47,742: New best val WER(1gram) 48.98% --> 47.46%
2026-01-05 15:05:47,742: Checkpointing model
2026-01-05 15:05:48,437: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 15:05:48,777: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_12500
2026-01-05 15:05:58,170: Train batch 12600: loss: 7.90 grad norm: 43.08 time: 0.058
2026-01-05 15:06:17,193: Train batch 12800: loss: 5.80 grad norm: 38.75 time: 0.053
2026-01-05 15:06:36,482: Train batch 13000: loss: 6.36 grad norm: 41.16 time: 0.067
2026-01-05 15:06:36,483: Running test after training batch: 13000
2026-01-05 15:06:36,585: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:06:41,670: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:06:41,703: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-05 15:06:43,577: Val batch 13000: PER (avg): 0.1543 CTC Loss (avg): 15.7962 WER(1gram): 47.46% (n=64) time: 7.094
2026-01-05 15:06:43,578: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 15:06:43,578: t15.2023.08.13 val PER: 0.1112
2026-01-05 15:06:43,578: t15.2023.08.18 val PER: 0.1140
2026-01-05 15:06:43,578: t15.2023.08.20 val PER: 0.1064
2026-01-05 15:06:43,578: t15.2023.08.25 val PER: 0.0979
2026-01-05 15:06:43,578: t15.2023.08.27 val PER: 0.1945
2026-01-05 15:06:43,578: t15.2023.09.01 val PER: 0.0820
2026-01-05 15:06:43,578: t15.2023.09.03 val PER: 0.1675
2026-01-05 15:06:43,578: t15.2023.09.24 val PER: 0.1299
2026-01-05 15:06:43,578: t15.2023.09.29 val PER: 0.1347
2026-01-05 15:06:43,578: t15.2023.10.01 val PER: 0.1704
2026-01-05 15:06:43,578: t15.2023.10.06 val PER: 0.0904
2026-01-05 15:06:43,579: t15.2023.10.08 val PER: 0.2449
2026-01-05 15:06:43,579: t15.2023.10.13 val PER: 0.2048
2026-01-05 15:06:43,579: t15.2023.10.15 val PER: 0.1569
2026-01-05 15:06:43,579: t15.2023.10.20 val PER: 0.1913
2026-01-05 15:06:43,579: t15.2023.10.22 val PER: 0.1114
2026-01-05 15:06:43,579: t15.2023.11.03 val PER: 0.1791
2026-01-05 15:06:43,579: t15.2023.11.04 val PER: 0.0341
2026-01-05 15:06:43,579: t15.2023.11.17 val PER: 0.0420
2026-01-05 15:06:43,579: t15.2023.11.19 val PER: 0.0419
2026-01-05 15:06:43,579: t15.2023.11.26 val PER: 0.1232
2026-01-05 15:06:43,579: t15.2023.12.03 val PER: 0.1092
2026-01-05 15:06:43,579: t15.2023.12.08 val PER: 0.1099
2026-01-05 15:06:43,580: t15.2023.12.10 val PER: 0.0946
2026-01-05 15:06:43,580: t15.2023.12.17 val PER: 0.1289
2026-01-05 15:06:43,580: t15.2023.12.29 val PER: 0.1380
2026-01-05 15:06:43,580: t15.2024.02.25 val PER: 0.1180
2026-01-05 15:06:43,580: t15.2024.03.08 val PER: 0.2333
2026-01-05 15:06:43,580: t15.2024.03.15 val PER: 0.2033
2026-01-05 15:06:43,580: t15.2024.03.17 val PER: 0.1416
2026-01-05 15:06:43,580: t15.2024.05.10 val PER: 0.1724
2026-01-05 15:06:43,580: t15.2024.06.14 val PER: 0.1609
2026-01-05 15:06:43,580: t15.2024.07.19 val PER: 0.2544
2026-01-05 15:06:43,580: t15.2024.07.21 val PER: 0.0993
2026-01-05 15:06:43,581: t15.2024.07.28 val PER: 0.1426
2026-01-05 15:06:43,581: t15.2025.01.10 val PER: 0.2948
2026-01-05 15:06:43,581: t15.2025.01.12 val PER: 0.1486
2026-01-05 15:06:43,581: t15.2025.03.14 val PER: 0.3388
2026-01-05 15:06:43,581: t15.2025.03.16 val PER: 0.1924
2026-01-05 15:06:43,581: t15.2025.03.30 val PER: 0.3046
2026-01-05 15:06:43,581: t15.2025.04.13 val PER: 0.2240
2026-01-05 15:06:43,906: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_13000
2026-01-05 15:07:02,693: Train batch 13200: loss: 12.63 grad norm: 59.65 time: 0.054
2026-01-05 15:07:21,248: Train batch 13400: loss: 8.74 grad norm: 48.91 time: 0.062
2026-01-05 15:07:30,621: Running test after training batch: 13500
2026-01-05 15:07:30,762: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:07:35,834: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:07:35,867: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 15:07:37,733: Val batch 13500: PER (avg): 0.1531 CTC Loss (avg): 15.6109 WER(1gram): 48.22% (n=64) time: 7.112
2026-01-05 15:07:37,734: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 15:07:37,734: t15.2023.08.13 val PER: 0.1112
2026-01-05 15:07:37,734: t15.2023.08.18 val PER: 0.1081
2026-01-05 15:07:37,734: t15.2023.08.20 val PER: 0.1160
2026-01-05 15:07:37,734: t15.2023.08.25 val PER: 0.0979
2026-01-05 15:07:37,734: t15.2023.08.27 val PER: 0.1913
2026-01-05 15:07:37,734: t15.2023.09.01 val PER: 0.0852
2026-01-05 15:07:37,734: t15.2023.09.03 val PER: 0.1675
2026-01-05 15:07:37,734: t15.2023.09.24 val PER: 0.1262
2026-01-05 15:07:37,734: t15.2023.09.29 val PER: 0.1321
2026-01-05 15:07:37,734: t15.2023.10.01 val PER: 0.1803
2026-01-05 15:07:37,734: t15.2023.10.06 val PER: 0.0893
2026-01-05 15:07:37,735: t15.2023.10.08 val PER: 0.2476
2026-01-05 15:07:37,735: t15.2023.10.13 val PER: 0.2095
2026-01-05 15:07:37,735: t15.2023.10.15 val PER: 0.1490
2026-01-05 15:07:37,735: t15.2023.10.20 val PER: 0.1745
2026-01-05 15:07:37,735: t15.2023.10.22 val PER: 0.1136
2026-01-05 15:07:37,735: t15.2023.11.03 val PER: 0.1839
2026-01-05 15:07:37,735: t15.2023.11.04 val PER: 0.0341
2026-01-05 15:07:37,735: t15.2023.11.17 val PER: 0.0420
2026-01-05 15:07:37,735: t15.2023.11.19 val PER: 0.0359
2026-01-05 15:07:37,735: t15.2023.11.26 val PER: 0.1290
2026-01-05 15:07:37,735: t15.2023.12.03 val PER: 0.1113
2026-01-05 15:07:37,735: t15.2023.12.08 val PER: 0.1039
2026-01-05 15:07:37,735: t15.2023.12.10 val PER: 0.0894
2026-01-05 15:07:37,736: t15.2023.12.17 val PER: 0.1279
2026-01-05 15:07:37,736: t15.2023.12.29 val PER: 0.1380
2026-01-05 15:07:37,736: t15.2024.02.25 val PER: 0.1110
2026-01-05 15:07:37,736: t15.2024.03.08 val PER: 0.2304
2026-01-05 15:07:37,736: t15.2024.03.15 val PER: 0.2076
2026-01-05 15:07:37,736: t15.2024.03.17 val PER: 0.1423
2026-01-05 15:07:37,736: t15.2024.05.10 val PER: 0.1486
2026-01-05 15:07:37,736: t15.2024.06.14 val PER: 0.1751
2026-01-05 15:07:37,736: t15.2024.07.19 val PER: 0.2426
2026-01-05 15:07:37,736: t15.2024.07.21 val PER: 0.1000
2026-01-05 15:07:37,736: t15.2024.07.28 val PER: 0.1324
2026-01-05 15:07:37,736: t15.2025.01.10 val PER: 0.2961
2026-01-05 15:07:37,736: t15.2025.01.12 val PER: 0.1386
2026-01-05 15:07:37,736: t15.2025.03.14 val PER: 0.3580
2026-01-05 15:07:37,736: t15.2025.03.16 val PER: 0.1767
2026-01-05 15:07:37,736: t15.2025.03.30 val PER: 0.3046
2026-01-05 15:07:37,737: t15.2025.04.13 val PER: 0.2140
2026-01-05 15:07:38,057: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_13500
2026-01-05 15:07:47,699: Train batch 13600: loss: 12.36 grad norm: 65.35 time: 0.063
2026-01-05 15:08:06,762: Train batch 13800: loss: 8.95 grad norm: 57.21 time: 0.056
2026-01-05 15:08:25,767: Train batch 14000: loss: 11.79 grad norm: 60.28 time: 0.051
2026-01-05 15:08:25,768: Running test after training batch: 14000
2026-01-05 15:08:25,896: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:08:30,965: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:08:31,000: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 15:08:32,872: Val batch 14000: PER (avg): 0.1509 CTC Loss (avg): 15.5534 WER(1gram): 46.19% (n=64) time: 7.104
2026-01-05 15:08:32,872: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-05 15:08:32,872: t15.2023.08.13 val PER: 0.1091
2026-01-05 15:08:32,872: t15.2023.08.18 val PER: 0.1031
2026-01-05 15:08:32,873: t15.2023.08.20 val PER: 0.1017
2026-01-05 15:08:32,873: t15.2023.08.25 val PER: 0.0979
2026-01-05 15:08:32,873: t15.2023.08.27 val PER: 0.1865
2026-01-05 15:08:32,873: t15.2023.09.01 val PER: 0.0812
2026-01-05 15:08:32,873: t15.2023.09.03 val PER: 0.1675
2026-01-05 15:08:32,873: t15.2023.09.24 val PER: 0.1238
2026-01-05 15:08:32,873: t15.2023.09.29 val PER: 0.1315
2026-01-05 15:08:32,873: t15.2023.10.01 val PER: 0.1737
2026-01-05 15:08:32,873: t15.2023.10.06 val PER: 0.0850
2026-01-05 15:08:32,873: t15.2023.10.08 val PER: 0.2530
2026-01-05 15:08:32,874: t15.2023.10.13 val PER: 0.1978
2026-01-05 15:08:32,874: t15.2023.10.15 val PER: 0.1470
2026-01-05 15:08:32,874: t15.2023.10.20 val PER: 0.1946
2026-01-05 15:08:32,874: t15.2023.10.22 val PER: 0.1013
2026-01-05 15:08:32,874: t15.2023.11.03 val PER: 0.1805
2026-01-05 15:08:32,874: t15.2023.11.04 val PER: 0.0375
2026-01-05 15:08:32,874: t15.2023.11.17 val PER: 0.0389
2026-01-05 15:08:32,874: t15.2023.11.19 val PER: 0.0399
2026-01-05 15:08:32,874: t15.2023.11.26 val PER: 0.1283
2026-01-05 15:08:32,874: t15.2023.12.03 val PER: 0.1218
2026-01-05 15:08:32,874: t15.2023.12.08 val PER: 0.1019
2026-01-05 15:08:32,875: t15.2023.12.10 val PER: 0.0972
2026-01-05 15:08:32,875: t15.2023.12.17 val PER: 0.1268
2026-01-05 15:08:32,878: t15.2023.12.29 val PER: 0.1366
2026-01-05 15:08:32,878: t15.2024.02.25 val PER: 0.1166
2026-01-05 15:08:32,878: t15.2024.03.08 val PER: 0.2262
2026-01-05 15:08:32,878: t15.2024.03.15 val PER: 0.2045
2026-01-05 15:08:32,878: t15.2024.03.17 val PER: 0.1464
2026-01-05 15:08:32,879: t15.2024.05.10 val PER: 0.1590
2026-01-05 15:08:32,879: t15.2024.06.14 val PER: 0.1593
2026-01-05 15:08:32,879: t15.2024.07.19 val PER: 0.2373
2026-01-05 15:08:32,879: t15.2024.07.21 val PER: 0.0959
2026-01-05 15:08:32,879: t15.2024.07.28 val PER: 0.1316
2026-01-05 15:08:32,879: t15.2025.01.10 val PER: 0.2961
2026-01-05 15:08:32,879: t15.2025.01.12 val PER: 0.1386
2026-01-05 15:08:32,879: t15.2025.03.14 val PER: 0.3417
2026-01-05 15:08:32,879: t15.2025.03.16 val PER: 0.1937
2026-01-05 15:08:32,879: t15.2025.03.30 val PER: 0.2874
2026-01-05 15:08:32,879: t15.2025.04.13 val PER: 0.2154
2026-01-05 15:08:32,880: New best val WER(1gram) 47.46% --> 46.19%
2026-01-05 15:08:32,880: Checkpointing model
2026-01-05 15:08:33,561: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 15:08:33,886: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_14000
2026-01-05 15:08:52,747: Train batch 14200: loss: 8.14 grad norm: 51.86 time: 0.056
2026-01-05 15:09:11,644: Train batch 14400: loss: 5.57 grad norm: 38.55 time: 0.064
2026-01-05 15:09:21,183: Running test after training batch: 14500
2026-01-05 15:09:21,308: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:09:26,456: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:09:26,490: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 15:09:28,369: Val batch 14500: PER (avg): 0.1509 CTC Loss (avg): 15.5520 WER(1gram): 48.48% (n=64) time: 7.186
2026-01-05 15:09:28,370: WER lens: avg_true_words=6.16 avg_pred_words=6.25 max_pred_words=11
2026-01-05 15:09:28,370: t15.2023.08.13 val PER: 0.1143
2026-01-05 15:09:28,370: t15.2023.08.18 val PER: 0.1081
2026-01-05 15:09:28,370: t15.2023.08.20 val PER: 0.1064
2026-01-05 15:09:28,370: t15.2023.08.25 val PER: 0.0889
2026-01-05 15:09:28,370: t15.2023.08.27 val PER: 0.1897
2026-01-05 15:09:28,370: t15.2023.09.01 val PER: 0.0787
2026-01-05 15:09:28,371: t15.2023.09.03 val PER: 0.1591
2026-01-05 15:09:28,371: t15.2023.09.24 val PER: 0.1371
2026-01-05 15:09:28,371: t15.2023.09.29 val PER: 0.1315
2026-01-05 15:09:28,371: t15.2023.10.01 val PER: 0.1790
2026-01-05 15:09:28,371: t15.2023.10.06 val PER: 0.0850
2026-01-05 15:09:28,371: t15.2023.10.08 val PER: 0.2476
2026-01-05 15:09:28,371: t15.2023.10.13 val PER: 0.2009
2026-01-05 15:09:28,371: t15.2023.10.15 val PER: 0.1529
2026-01-05 15:09:28,371: t15.2023.10.20 val PER: 0.1779
2026-01-05 15:09:28,371: t15.2023.10.22 val PER: 0.1125
2026-01-05 15:09:28,371: t15.2023.11.03 val PER: 0.1845
2026-01-05 15:09:28,371: t15.2023.11.04 val PER: 0.0341
2026-01-05 15:09:28,371: t15.2023.11.17 val PER: 0.0420
2026-01-05 15:09:28,371: t15.2023.11.19 val PER: 0.0379
2026-01-05 15:09:28,371: t15.2023.11.26 val PER: 0.1210
2026-01-05 15:09:28,371: t15.2023.12.03 val PER: 0.1124
2026-01-05 15:09:28,372: t15.2023.12.08 val PER: 0.0992
2026-01-05 15:09:28,372: t15.2023.12.10 val PER: 0.0828
2026-01-05 15:09:28,372: t15.2023.12.17 val PER: 0.1351
2026-01-05 15:09:28,372: t15.2023.12.29 val PER: 0.1325
2026-01-05 15:09:28,372: t15.2024.02.25 val PER: 0.1096
2026-01-05 15:09:28,372: t15.2024.03.08 val PER: 0.2319
2026-01-05 15:09:28,372: t15.2024.03.15 val PER: 0.2045
2026-01-05 15:09:28,372: t15.2024.03.17 val PER: 0.1437
2026-01-05 15:09:28,372: t15.2024.05.10 val PER: 0.1575
2026-01-05 15:09:28,372: t15.2024.06.14 val PER: 0.1640
2026-01-05 15:09:28,372: t15.2024.07.19 val PER: 0.2373
2026-01-05 15:09:28,372: t15.2024.07.21 val PER: 0.0945
2026-01-05 15:09:28,372: t15.2024.07.28 val PER: 0.1309
2026-01-05 15:09:28,372: t15.2025.01.10 val PER: 0.2934
2026-01-05 15:09:28,372: t15.2025.01.12 val PER: 0.1401
2026-01-05 15:09:28,373: t15.2025.03.14 val PER: 0.3417
2026-01-05 15:09:28,373: t15.2025.03.16 val PER: 0.1846
2026-01-05 15:09:28,373: t15.2025.03.30 val PER: 0.2954
2026-01-05 15:09:28,373: t15.2025.04.13 val PER: 0.2111
2026-01-05 15:09:28,691: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_14500
2026-01-05 15:09:38,170: Train batch 14600: loss: 12.07 grad norm: 58.71 time: 0.059
2026-01-05 15:09:57,028: Train batch 14800: loss: 5.71 grad norm: 44.16 time: 0.051
2026-01-05 15:10:16,007: Train batch 15000: loss: 8.57 grad norm: 50.03 time: 0.052
2026-01-05 15:10:16,007: Running test after training batch: 15000
2026-01-05 15:10:16,153: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:10:21,324: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:10:21,359: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 15:10:23,334: Val batch 15000: PER (avg): 0.1478 CTC Loss (avg): 15.2300 WER(1gram): 45.94% (n=64) time: 7.326
2026-01-05 15:10:23,334: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-05 15:10:23,334: t15.2023.08.13 val PER: 0.1123
2026-01-05 15:10:23,334: t15.2023.08.18 val PER: 0.1039
2026-01-05 15:10:23,334: t15.2023.08.20 val PER: 0.1025
2026-01-05 15:10:23,335: t15.2023.08.25 val PER: 0.0889
2026-01-05 15:10:23,335: t15.2023.08.27 val PER: 0.1881
2026-01-05 15:10:23,335: t15.2023.09.01 val PER: 0.0795
2026-01-05 15:10:23,335: t15.2023.09.03 val PER: 0.1568
2026-01-05 15:10:23,335: t15.2023.09.24 val PER: 0.1335
2026-01-05 15:10:23,335: t15.2023.09.29 val PER: 0.1289
2026-01-05 15:10:23,335: t15.2023.10.01 val PER: 0.1810
2026-01-05 15:10:23,335: t15.2023.10.06 val PER: 0.0840
2026-01-05 15:10:23,335: t15.2023.10.08 val PER: 0.2422
2026-01-05 15:10:23,335: t15.2023.10.13 val PER: 0.1924
2026-01-05 15:10:23,335: t15.2023.10.15 val PER: 0.1457
2026-01-05 15:10:23,336: t15.2023.10.20 val PER: 0.2047
2026-01-05 15:10:23,336: t15.2023.10.22 val PER: 0.1069
2026-01-05 15:10:23,336: t15.2023.11.03 val PER: 0.1771
2026-01-05 15:10:23,336: t15.2023.11.04 val PER: 0.0375
2026-01-05 15:10:23,336: t15.2023.11.17 val PER: 0.0435
2026-01-05 15:10:23,336: t15.2023.11.19 val PER: 0.0359
2026-01-05 15:10:23,336: t15.2023.11.26 val PER: 0.1167
2026-01-05 15:10:23,336: t15.2023.12.03 val PER: 0.1145
2026-01-05 15:10:23,336: t15.2023.12.08 val PER: 0.0972
2026-01-05 15:10:23,336: t15.2023.12.10 val PER: 0.0828
2026-01-05 15:10:23,337: t15.2023.12.17 val PER: 0.1351
2026-01-05 15:10:23,337: t15.2023.12.29 val PER: 0.1338
2026-01-05 15:10:23,337: t15.2024.02.25 val PER: 0.1081
2026-01-05 15:10:23,337: t15.2024.03.08 val PER: 0.2191
2026-01-05 15:10:23,337: t15.2024.03.15 val PER: 0.2008
2026-01-05 15:10:23,337: t15.2024.03.17 val PER: 0.1367
2026-01-05 15:10:23,337: t15.2024.05.10 val PER: 0.1634
2026-01-05 15:10:23,337: t15.2024.06.14 val PER: 0.1656
2026-01-05 15:10:23,337: t15.2024.07.19 val PER: 0.2281
2026-01-05 15:10:23,337: t15.2024.07.21 val PER: 0.0952
2026-01-05 15:10:23,337: t15.2024.07.28 val PER: 0.1243
2026-01-05 15:10:23,337: t15.2025.01.10 val PER: 0.2865
2026-01-05 15:10:23,338: t15.2025.01.12 val PER: 0.1339
2026-01-05 15:10:23,338: t15.2025.03.14 val PER: 0.3432
2026-01-05 15:10:23,338: t15.2025.03.16 val PER: 0.1675
2026-01-05 15:10:23,338: t15.2025.03.30 val PER: 0.2885
2026-01-05 15:10:23,338: t15.2025.04.13 val PER: 0.2140
2026-01-05 15:10:23,338: New best val WER(1gram) 46.19% --> 45.94%
2026-01-05 15:10:23,338: Checkpointing model
2026-01-05 15:10:24,172: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 15:10:24,515: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_15000
2026-01-05 15:10:43,366: Train batch 15200: loss: 5.08 grad norm: 42.11 time: 0.057
2026-01-05 15:11:01,891: Train batch 15400: loss: 11.14 grad norm: 57.50 time: 0.049
2026-01-05 15:11:11,297: Running test after training batch: 15500
2026-01-05 15:11:11,482: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:11:16,537: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:11:16,571: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 15:11:18,463: Val batch 15500: PER (avg): 0.1485 CTC Loss (avg): 15.2223 WER(1gram): 44.92% (n=64) time: 7.165
2026-01-05 15:11:18,463: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-05 15:11:18,464: t15.2023.08.13 val PER: 0.1133
2026-01-05 15:11:18,464: t15.2023.08.18 val PER: 0.0997
2026-01-05 15:11:18,464: t15.2023.08.20 val PER: 0.1096
2026-01-05 15:11:18,464: t15.2023.08.25 val PER: 0.1024
2026-01-05 15:11:18,464: t15.2023.08.27 val PER: 0.1849
2026-01-05 15:11:18,464: t15.2023.09.01 val PER: 0.0755
2026-01-05 15:11:18,464: t15.2023.09.03 val PER: 0.1556
2026-01-05 15:11:18,464: t15.2023.09.24 val PER: 0.1238
2026-01-05 15:11:18,464: t15.2023.09.29 val PER: 0.1302
2026-01-05 15:11:18,464: t15.2023.10.01 val PER: 0.1704
2026-01-05 15:11:18,464: t15.2023.10.06 val PER: 0.0840
2026-01-05 15:11:18,465: t15.2023.10.08 val PER: 0.2490
2026-01-05 15:11:18,465: t15.2023.10.13 val PER: 0.1994
2026-01-05 15:11:18,465: t15.2023.10.15 val PER: 0.1450
2026-01-05 15:11:18,465: t15.2023.10.20 val PER: 0.1879
2026-01-05 15:11:18,465: t15.2023.10.22 val PER: 0.1114
2026-01-05 15:11:18,465: t15.2023.11.03 val PER: 0.1744
2026-01-05 15:11:18,465: t15.2023.11.04 val PER: 0.0341
2026-01-05 15:11:18,465: t15.2023.11.17 val PER: 0.0327
2026-01-05 15:11:18,465: t15.2023.11.19 val PER: 0.0339
2026-01-05 15:11:18,465: t15.2023.11.26 val PER: 0.1109
2026-01-05 15:11:18,465: t15.2023.12.03 val PER: 0.1113
2026-01-05 15:11:18,465: t15.2023.12.08 val PER: 0.1032
2026-01-05 15:11:18,465: t15.2023.12.10 val PER: 0.0841
2026-01-05 15:11:18,466: t15.2023.12.17 val PER: 0.1279
2026-01-05 15:11:18,466: t15.2023.12.29 val PER: 0.1352
2026-01-05 15:11:18,466: t15.2024.02.25 val PER: 0.1053
2026-01-05 15:11:18,466: t15.2024.03.08 val PER: 0.2390
2026-01-05 15:11:18,466: t15.2024.03.15 val PER: 0.2026
2026-01-05 15:11:18,466: t15.2024.03.17 val PER: 0.1388
2026-01-05 15:11:18,466: t15.2024.05.10 val PER: 0.1634
2026-01-05 15:11:18,466: t15.2024.06.14 val PER: 0.1577
2026-01-05 15:11:18,466: t15.2024.07.19 val PER: 0.2327
2026-01-05 15:11:18,466: t15.2024.07.21 val PER: 0.0966
2026-01-05 15:11:18,466: t15.2024.07.28 val PER: 0.1309
2026-01-05 15:11:18,466: t15.2025.01.10 val PER: 0.2851
2026-01-05 15:11:18,467: t15.2025.01.12 val PER: 0.1409
2026-01-05 15:11:18,467: t15.2025.03.14 val PER: 0.3462
2026-01-05 15:11:18,467: t15.2025.03.16 val PER: 0.1819
2026-01-05 15:11:18,467: t15.2025.03.30 val PER: 0.2954
2026-01-05 15:11:18,467: t15.2025.04.13 val PER: 0.2054
2026-01-05 15:11:18,467: New best val WER(1gram) 45.94% --> 44.92%
2026-01-05 15:11:18,468: Checkpointing model
2026-01-05 15:11:19,308: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/best_checkpoint
2026-01-05 15:11:19,631: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_15500
2026-01-05 15:11:28,957: Train batch 15600: loss: 11.15 grad norm: 57.03 time: 0.062
2026-01-05 15:11:47,676: Train batch 15800: loss: 12.95 grad norm: 62.00 time: 0.067
2026-01-05 15:12:06,598: Train batch 16000: loss: 7.92 grad norm: 42.83 time: 0.056
2026-01-05 15:12:06,599: Running test after training batch: 16000
2026-01-05 15:12:06,762: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:12:11,829: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:12:11,864: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 15:12:13,794: Val batch 16000: PER (avg): 0.1472 CTC Loss (avg): 15.2450 WER(1gram): 45.69% (n=64) time: 7.195
2026-01-05 15:12:13,794: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=12
2026-01-05 15:12:13,794: t15.2023.08.13 val PER: 0.1071
2026-01-05 15:12:13,794: t15.2023.08.18 val PER: 0.1023
2026-01-05 15:12:13,795: t15.2023.08.20 val PER: 0.1025
2026-01-05 15:12:13,795: t15.2023.08.25 val PER: 0.0994
2026-01-05 15:12:13,795: t15.2023.08.27 val PER: 0.1865
2026-01-05 15:12:13,795: t15.2023.09.01 val PER: 0.0771
2026-01-05 15:12:13,795: t15.2023.09.03 val PER: 0.1473
2026-01-05 15:12:13,795: t15.2023.09.24 val PER: 0.1299
2026-01-05 15:12:13,795: t15.2023.09.29 val PER: 0.1295
2026-01-05 15:12:13,795: t15.2023.10.01 val PER: 0.1783
2026-01-05 15:12:13,795: t15.2023.10.06 val PER: 0.0818
2026-01-05 15:12:13,796: t15.2023.10.08 val PER: 0.2503
2026-01-05 15:12:13,796: t15.2023.10.13 val PER: 0.1947
2026-01-05 15:12:13,796: t15.2023.10.15 val PER: 0.1463
2026-01-05 15:12:13,796: t15.2023.10.20 val PER: 0.1879
2026-01-05 15:12:13,796: t15.2023.10.22 val PER: 0.1058
2026-01-05 15:12:13,796: t15.2023.11.03 val PER: 0.1771
2026-01-05 15:12:13,796: t15.2023.11.04 val PER: 0.0307
2026-01-05 15:12:13,796: t15.2023.11.17 val PER: 0.0373
2026-01-05 15:12:13,796: t15.2023.11.19 val PER: 0.0419
2026-01-05 15:12:13,796: t15.2023.11.26 val PER: 0.1152
2026-01-05 15:12:13,796: t15.2023.12.03 val PER: 0.1071
2026-01-05 15:12:13,797: t15.2023.12.08 val PER: 0.0992
2026-01-05 15:12:13,797: t15.2023.12.10 val PER: 0.0920
2026-01-05 15:12:13,797: t15.2023.12.17 val PER: 0.1268
2026-01-05 15:12:13,797: t15.2023.12.29 val PER: 0.1270
2026-01-05 15:12:13,797: t15.2024.02.25 val PER: 0.1039
2026-01-05 15:12:13,797: t15.2024.03.08 val PER: 0.2376
2026-01-05 15:12:13,797: t15.2024.03.15 val PER: 0.1976
2026-01-05 15:12:13,797: t15.2024.03.17 val PER: 0.1332
2026-01-05 15:12:13,797: t15.2024.05.10 val PER: 0.1590
2026-01-05 15:12:13,797: t15.2024.06.14 val PER: 0.1593
2026-01-05 15:12:13,797: t15.2024.07.19 val PER: 0.2307
2026-01-05 15:12:13,797: t15.2024.07.21 val PER: 0.0924
2026-01-05 15:12:13,801: t15.2024.07.28 val PER: 0.1353
2026-01-05 15:12:13,801: t15.2025.01.10 val PER: 0.2851
2026-01-05 15:12:13,801: t15.2025.01.12 val PER: 0.1378
2026-01-05 15:12:13,801: t15.2025.03.14 val PER: 0.3417
2026-01-05 15:12:13,801: t15.2025.03.16 val PER: 0.1819
2026-01-05 15:12:13,801: t15.2025.03.30 val PER: 0.2862
2026-01-05 15:12:13,801: t15.2025.04.13 val PER: 0.2040
2026-01-05 15:12:14,125: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_16000
2026-01-05 15:12:33,088: Train batch 16200: loss: 5.77 grad norm: 45.77 time: 0.056
2026-01-05 15:12:52,305: Train batch 16400: loss: 10.18 grad norm: 60.21 time: 0.057
2026-01-05 15:13:01,850: Running test after training batch: 16500
2026-01-05 15:13:01,996: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:13:07,141: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:13:07,177: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 15:13:09,156: Val batch 16500: PER (avg): 0.1465 CTC Loss (avg): 15.1259 WER(1gram): 45.94% (n=64) time: 7.306
2026-01-05 15:13:09,158: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=11
2026-01-05 15:13:09,158: t15.2023.08.13 val PER: 0.1133
2026-01-05 15:13:09,158: t15.2023.08.18 val PER: 0.0981
2026-01-05 15:13:09,158: t15.2023.08.20 val PER: 0.1033
2026-01-05 15:13:09,159: t15.2023.08.25 val PER: 0.0843
2026-01-05 15:13:09,159: t15.2023.08.27 val PER: 0.1768
2026-01-05 15:13:09,159: t15.2023.09.01 val PER: 0.0795
2026-01-05 15:13:09,159: t15.2023.09.03 val PER: 0.1591
2026-01-05 15:13:09,159: t15.2023.09.24 val PER: 0.1335
2026-01-05 15:13:09,159: t15.2023.09.29 val PER: 0.1283
2026-01-05 15:13:09,159: t15.2023.10.01 val PER: 0.1744
2026-01-05 15:13:09,159: t15.2023.10.06 val PER: 0.0883
2026-01-05 15:13:09,159: t15.2023.10.08 val PER: 0.2476
2026-01-05 15:13:09,159: t15.2023.10.13 val PER: 0.1901
2026-01-05 15:13:09,159: t15.2023.10.15 val PER: 0.1437
2026-01-05 15:13:09,159: t15.2023.10.20 val PER: 0.1879
2026-01-05 15:13:09,159: t15.2023.10.22 val PER: 0.1069
2026-01-05 15:13:09,160: t15.2023.11.03 val PER: 0.1771
2026-01-05 15:13:09,160: t15.2023.11.04 val PER: 0.0307
2026-01-05 15:13:09,160: t15.2023.11.17 val PER: 0.0342
2026-01-05 15:13:09,160: t15.2023.11.19 val PER: 0.0359
2026-01-05 15:13:09,160: t15.2023.11.26 val PER: 0.1181
2026-01-05 15:13:09,160: t15.2023.12.03 val PER: 0.1050
2026-01-05 15:13:09,160: t15.2023.12.08 val PER: 0.0959
2026-01-05 15:13:09,160: t15.2023.12.10 val PER: 0.0854
2026-01-05 15:13:09,160: t15.2023.12.17 val PER: 0.1185
2026-01-05 15:13:09,160: t15.2023.12.29 val PER: 0.1270
2026-01-05 15:13:09,161: t15.2024.02.25 val PER: 0.1039
2026-01-05 15:13:09,161: t15.2024.03.08 val PER: 0.2304
2026-01-05 15:13:09,161: t15.2024.03.15 val PER: 0.2001
2026-01-05 15:13:09,161: t15.2024.03.17 val PER: 0.1353
2026-01-05 15:13:09,161: t15.2024.05.10 val PER: 0.1575
2026-01-05 15:13:09,161: t15.2024.06.14 val PER: 0.1593
2026-01-05 15:13:09,161: t15.2024.07.19 val PER: 0.2327
2026-01-05 15:13:09,161: t15.2024.07.21 val PER: 0.0931
2026-01-05 15:13:09,161: t15.2024.07.28 val PER: 0.1279
2026-01-05 15:13:09,161: t15.2025.01.10 val PER: 0.2769
2026-01-05 15:13:09,161: t15.2025.01.12 val PER: 0.1339
2026-01-05 15:13:09,161: t15.2025.03.14 val PER: 0.3565
2026-01-05 15:13:09,161: t15.2025.03.16 val PER: 0.1885
2026-01-05 15:13:09,161: t15.2025.03.30 val PER: 0.2851
2026-01-05 15:13:09,161: t15.2025.04.13 val PER: 0.2068
2026-01-05 15:13:09,506: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_16500
2026-01-05 15:13:19,049: Train batch 16600: loss: 8.36 grad norm: 57.82 time: 0.052
2026-01-05 15:13:38,248: Train batch 16800: loss: 16.03 grad norm: 74.28 time: 0.062
2026-01-05 15:13:57,242: Train batch 17000: loss: 7.65 grad norm: 46.79 time: 0.081
2026-01-05 15:13:57,242: Running test after training batch: 17000
2026-01-05 15:13:57,357: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:14:02,421: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:14:02,455: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 15:14:04,374: Val batch 17000: PER (avg): 0.1452 CTC Loss (avg): 15.0485 WER(1gram): 46.70% (n=64) time: 7.131
2026-01-05 15:14:04,374: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-05 15:14:04,374: t15.2023.08.13 val PER: 0.1164
2026-01-05 15:14:04,374: t15.2023.08.18 val PER: 0.1048
2026-01-05 15:14:04,374: t15.2023.08.20 val PER: 0.0993
2026-01-05 15:14:04,374: t15.2023.08.25 val PER: 0.0964
2026-01-05 15:14:04,375: t15.2023.08.27 val PER: 0.1785
2026-01-05 15:14:04,375: t15.2023.09.01 val PER: 0.0722
2026-01-05 15:14:04,375: t15.2023.09.03 val PER: 0.1532
2026-01-05 15:14:04,375: t15.2023.09.24 val PER: 0.1299
2026-01-05 15:14:04,375: t15.2023.09.29 val PER: 0.1315
2026-01-05 15:14:04,375: t15.2023.10.01 val PER: 0.1697
2026-01-05 15:14:04,375: t15.2023.10.06 val PER: 0.0861
2026-01-05 15:14:04,375: t15.2023.10.08 val PER: 0.2490
2026-01-05 15:14:04,375: t15.2023.10.13 val PER: 0.1885
2026-01-05 15:14:04,375: t15.2023.10.15 val PER: 0.1411
2026-01-05 15:14:04,375: t15.2023.10.20 val PER: 0.1711
2026-01-05 15:14:04,376: t15.2023.10.22 val PER: 0.1058
2026-01-05 15:14:04,376: t15.2023.11.03 val PER: 0.1791
2026-01-05 15:14:04,376: t15.2023.11.04 val PER: 0.0341
2026-01-05 15:14:04,376: t15.2023.11.17 val PER: 0.0327
2026-01-05 15:14:04,376: t15.2023.11.19 val PER: 0.0379
2026-01-05 15:14:04,376: t15.2023.11.26 val PER: 0.1080
2026-01-05 15:14:04,376: t15.2023.12.03 val PER: 0.1050
2026-01-05 15:14:04,376: t15.2023.12.08 val PER: 0.0965
2026-01-05 15:14:04,376: t15.2023.12.10 val PER: 0.0815
2026-01-05 15:14:04,376: t15.2023.12.17 val PER: 0.1227
2026-01-05 15:14:04,376: t15.2023.12.29 val PER: 0.1318
2026-01-05 15:14:04,376: t15.2024.02.25 val PER: 0.1067
2026-01-05 15:14:04,376: t15.2024.03.08 val PER: 0.2248
2026-01-05 15:14:04,377: t15.2024.03.15 val PER: 0.2064
2026-01-05 15:14:04,377: t15.2024.03.17 val PER: 0.1332
2026-01-05 15:14:04,377: t15.2024.05.10 val PER: 0.1590
2026-01-05 15:14:04,377: t15.2024.06.14 val PER: 0.1483
2026-01-05 15:14:04,377: t15.2024.07.19 val PER: 0.2261
2026-01-05 15:14:04,377: t15.2024.07.21 val PER: 0.0897
2026-01-05 15:14:04,378: t15.2024.07.28 val PER: 0.1243
2026-01-05 15:14:04,378: t15.2025.01.10 val PER: 0.2865
2026-01-05 15:14:04,378: t15.2025.01.12 val PER: 0.1378
2026-01-05 15:14:04,378: t15.2025.03.14 val PER: 0.3447
2026-01-05 15:14:04,378: t15.2025.03.16 val PER: 0.1780
2026-01-05 15:14:04,378: t15.2025.03.30 val PER: 0.2759
2026-01-05 15:14:04,378: t15.2025.04.13 val PER: 0.2040
2026-01-05 15:14:04,722: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_17000
2026-01-05 15:14:23,703: Train batch 17200: loss: 9.24 grad norm: 50.42 time: 0.084
2026-01-05 15:14:42,962: Train batch 17400: loss: 11.46 grad norm: 57.49 time: 0.071
2026-01-05 15:14:52,482: Running test after training batch: 17500
2026-01-05 15:14:52,584: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:14:57,692: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:14:57,727: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 15:14:59,656: Val batch 17500: PER (avg): 0.1447 CTC Loss (avg): 15.0257 WER(1gram): 46.45% (n=64) time: 7.174
2026-01-05 15:14:59,657: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-05 15:14:59,657: t15.2023.08.13 val PER: 0.1040
2026-01-05 15:14:59,657: t15.2023.08.18 val PER: 0.1056
2026-01-05 15:14:59,657: t15.2023.08.20 val PER: 0.0969
2026-01-05 15:14:59,657: t15.2023.08.25 val PER: 0.0919
2026-01-05 15:14:59,657: t15.2023.08.27 val PER: 0.1801
2026-01-05 15:14:59,657: t15.2023.09.01 val PER: 0.0731
2026-01-05 15:14:59,657: t15.2023.09.03 val PER: 0.1449
2026-01-05 15:14:59,658: t15.2023.09.24 val PER: 0.1335
2026-01-05 15:14:59,658: t15.2023.09.29 val PER: 0.1244
2026-01-05 15:14:59,658: t15.2023.10.01 val PER: 0.1697
2026-01-05 15:14:59,658: t15.2023.10.06 val PER: 0.0807
2026-01-05 15:14:59,658: t15.2023.10.08 val PER: 0.2544
2026-01-05 15:14:59,658: t15.2023.10.13 val PER: 0.1885
2026-01-05 15:14:59,658: t15.2023.10.15 val PER: 0.1417
2026-01-05 15:14:59,658: t15.2023.10.20 val PER: 0.1946
2026-01-05 15:14:59,658: t15.2023.10.22 val PER: 0.1069
2026-01-05 15:14:59,658: t15.2023.11.03 val PER: 0.1764
2026-01-05 15:14:59,658: t15.2023.11.04 val PER: 0.0341
2026-01-05 15:14:59,658: t15.2023.11.17 val PER: 0.0342
2026-01-05 15:14:59,658: t15.2023.11.19 val PER: 0.0359
2026-01-05 15:14:59,658: t15.2023.11.26 val PER: 0.1109
2026-01-05 15:14:59,659: t15.2023.12.03 val PER: 0.1029
2026-01-05 15:14:59,659: t15.2023.12.08 val PER: 0.1005
2026-01-05 15:14:59,659: t15.2023.12.10 val PER: 0.0867
2026-01-05 15:14:59,659: t15.2023.12.17 val PER: 0.1237
2026-01-05 15:14:59,659: t15.2023.12.29 val PER: 0.1318
2026-01-05 15:14:59,659: t15.2024.02.25 val PER: 0.1025
2026-01-05 15:14:59,659: t15.2024.03.08 val PER: 0.2248
2026-01-05 15:14:59,659: t15.2024.03.15 val PER: 0.1970
2026-01-05 15:14:59,659: t15.2024.03.17 val PER: 0.1353
2026-01-05 15:14:59,659: t15.2024.05.10 val PER: 0.1560
2026-01-05 15:14:59,659: t15.2024.06.14 val PER: 0.1593
2026-01-05 15:14:59,659: t15.2024.07.19 val PER: 0.2261
2026-01-05 15:14:59,659: t15.2024.07.21 val PER: 0.0924
2026-01-05 15:14:59,659: t15.2024.07.28 val PER: 0.1235
2026-01-05 15:14:59,659: t15.2025.01.10 val PER: 0.2837
2026-01-05 15:14:59,659: t15.2025.01.12 val PER: 0.1324
2026-01-05 15:14:59,660: t15.2025.03.14 val PER: 0.3506
2026-01-05 15:14:59,660: t15.2025.03.16 val PER: 0.1793
2026-01-05 15:14:59,660: t15.2025.03.30 val PER: 0.2701
2026-01-05 15:14:59,660: t15.2025.04.13 val PER: 0.2154
2026-01-05 15:14:59,992: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_17500
2026-01-05 15:15:09,447: Train batch 17600: loss: 9.02 grad norm: 50.33 time: 0.051
2026-01-05 15:15:28,793: Train batch 17800: loss: 6.06 grad norm: 47.52 time: 0.041
2026-01-05 15:15:47,982: Train batch 18000: loss: 10.84 grad norm: 63.03 time: 0.061
2026-01-05 15:15:47,982: Running test after training batch: 18000
2026-01-05 15:15:48,084: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:15:53,362: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:15:53,396: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 15:15:55,337: Val batch 18000: PER (avg): 0.1444 CTC Loss (avg): 15.0552 WER(1gram): 47.46% (n=64) time: 7.354
2026-01-05 15:15:55,337: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-05 15:15:55,337: t15.2023.08.13 val PER: 0.1102
2026-01-05 15:15:55,338: t15.2023.08.18 val PER: 0.1090
2026-01-05 15:15:55,338: t15.2023.08.20 val PER: 0.1017
2026-01-05 15:15:55,338: t15.2023.08.25 val PER: 0.0964
2026-01-05 15:15:55,338: t15.2023.08.27 val PER: 0.1801
2026-01-05 15:15:55,338: t15.2023.09.01 val PER: 0.0739
2026-01-05 15:15:55,338: t15.2023.09.03 val PER: 0.1461
2026-01-05 15:15:55,338: t15.2023.09.24 val PER: 0.1299
2026-01-05 15:15:55,338: t15.2023.09.29 val PER: 0.1276
2026-01-05 15:15:55,338: t15.2023.10.01 val PER: 0.1704
2026-01-05 15:15:55,338: t15.2023.10.06 val PER: 0.0850
2026-01-05 15:15:55,338: t15.2023.10.08 val PER: 0.2490
2026-01-05 15:15:55,338: t15.2023.10.13 val PER: 0.1885
2026-01-05 15:15:55,338: t15.2023.10.15 val PER: 0.1404
2026-01-05 15:15:55,338: t15.2023.10.20 val PER: 0.1913
2026-01-05 15:15:55,339: t15.2023.10.22 val PER: 0.1047
2026-01-05 15:15:55,339: t15.2023.11.03 val PER: 0.1730
2026-01-05 15:15:55,339: t15.2023.11.04 val PER: 0.0341
2026-01-05 15:15:55,339: t15.2023.11.17 val PER: 0.0358
2026-01-05 15:15:55,339: t15.2023.11.19 val PER: 0.0379
2026-01-05 15:15:55,339: t15.2023.11.26 val PER: 0.1051
2026-01-05 15:15:55,339: t15.2023.12.03 val PER: 0.1082
2026-01-05 15:15:55,339: t15.2023.12.08 val PER: 0.0979
2026-01-05 15:15:55,340: t15.2023.12.10 val PER: 0.0841
2026-01-05 15:15:55,340: t15.2023.12.17 val PER: 0.1247
2026-01-05 15:15:55,340: t15.2023.12.29 val PER: 0.1311
2026-01-05 15:15:55,340: t15.2024.02.25 val PER: 0.1053
2026-01-05 15:15:55,340: t15.2024.03.08 val PER: 0.2205
2026-01-05 15:15:55,340: t15.2024.03.15 val PER: 0.1951
2026-01-05 15:15:55,340: t15.2024.03.17 val PER: 0.1325
2026-01-05 15:15:55,340: t15.2024.05.10 val PER: 0.1545
2026-01-05 15:15:55,340: t15.2024.06.14 val PER: 0.1546
2026-01-05 15:15:55,340: t15.2024.07.19 val PER: 0.2320
2026-01-05 15:15:55,340: t15.2024.07.21 val PER: 0.0903
2026-01-05 15:15:55,340: t15.2024.07.28 val PER: 0.1250
2026-01-05 15:15:55,340: t15.2025.01.10 val PER: 0.2741
2026-01-05 15:15:55,340: t15.2025.01.12 val PER: 0.1347
2026-01-05 15:15:55,340: t15.2025.03.14 val PER: 0.3402
2026-01-05 15:15:55,340: t15.2025.03.16 val PER: 0.1754
2026-01-05 15:15:55,341: t15.2025.03.30 val PER: 0.2678
2026-01-05 15:15:55,341: t15.2025.04.13 val PER: 0.2154
2026-01-05 15:15:55,675: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_18000
2026-01-05 15:16:15,149: Train batch 18200: loss: 7.33 grad norm: 46.71 time: 0.075
2026-01-05 15:16:33,997: Train batch 18400: loss: 4.54 grad norm: 41.53 time: 0.058
2026-01-05 15:16:43,586: Running test after training batch: 18500
2026-01-05 15:16:43,759: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:16:48,861: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:16:48,896: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 15:16:50,889: Val batch 18500: PER (avg): 0.1452 CTC Loss (avg): 14.9900 WER(1gram): 46.70% (n=64) time: 7.302
2026-01-05 15:16:50,889: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-05 15:16:50,889: t15.2023.08.13 val PER: 0.1091
2026-01-05 15:16:50,889: t15.2023.08.18 val PER: 0.1048
2026-01-05 15:16:50,890: t15.2023.08.20 val PER: 0.1009
2026-01-05 15:16:50,890: t15.2023.08.25 val PER: 0.0949
2026-01-05 15:16:50,890: t15.2023.08.27 val PER: 0.1752
2026-01-05 15:16:50,890: t15.2023.09.01 val PER: 0.0731
2026-01-05 15:16:50,890: t15.2023.09.03 val PER: 0.1544
2026-01-05 15:16:50,890: t15.2023.09.24 val PER: 0.1299
2026-01-05 15:16:50,890: t15.2023.09.29 val PER: 0.1264
2026-01-05 15:16:50,891: t15.2023.10.01 val PER: 0.1737
2026-01-05 15:16:50,891: t15.2023.10.06 val PER: 0.0861
2026-01-05 15:16:50,891: t15.2023.10.08 val PER: 0.2503
2026-01-05 15:16:50,891: t15.2023.10.13 val PER: 0.1893
2026-01-05 15:16:50,891: t15.2023.10.15 val PER: 0.1457
2026-01-05 15:16:50,891: t15.2023.10.20 val PER: 0.1846
2026-01-05 15:16:50,891: t15.2023.10.22 val PER: 0.1058
2026-01-05 15:16:50,891: t15.2023.11.03 val PER: 0.1757
2026-01-05 15:16:50,891: t15.2023.11.04 val PER: 0.0341
2026-01-05 15:16:50,891: t15.2023.11.17 val PER: 0.0389
2026-01-05 15:16:50,891: t15.2023.11.19 val PER: 0.0319
2026-01-05 15:16:50,892: t15.2023.11.26 val PER: 0.1123
2026-01-05 15:16:50,892: t15.2023.12.03 val PER: 0.1113
2026-01-05 15:16:50,892: t15.2023.12.08 val PER: 0.0979
2026-01-05 15:16:50,892: t15.2023.12.10 val PER: 0.0815
2026-01-05 15:16:50,892: t15.2023.12.17 val PER: 0.1195
2026-01-05 15:16:50,892: t15.2023.12.29 val PER: 0.1256
2026-01-05 15:16:50,892: t15.2024.02.25 val PER: 0.1067
2026-01-05 15:16:50,892: t15.2024.03.08 val PER: 0.2248
2026-01-05 15:16:50,892: t15.2024.03.15 val PER: 0.1939
2026-01-05 15:16:50,892: t15.2024.03.17 val PER: 0.1311
2026-01-05 15:16:50,893: t15.2024.05.10 val PER: 0.1530
2026-01-05 15:16:50,893: t15.2024.06.14 val PER: 0.1546
2026-01-05 15:16:50,893: t15.2024.07.19 val PER: 0.2307
2026-01-05 15:16:50,893: t15.2024.07.21 val PER: 0.0924
2026-01-05 15:16:50,893: t15.2024.07.28 val PER: 0.1272
2026-01-05 15:16:50,893: t15.2025.01.10 val PER: 0.2879
2026-01-05 15:16:50,893: t15.2025.01.12 val PER: 0.1355
2026-01-05 15:16:50,893: t15.2025.03.14 val PER: 0.3491
2026-01-05 15:16:50,893: t15.2025.03.16 val PER: 0.1767
2026-01-05 15:16:50,893: t15.2025.03.30 val PER: 0.2724
2026-01-05 15:16:50,893: t15.2025.04.13 val PER: 0.2154
2026-01-05 15:16:51,239: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_18500
2026-01-05 15:17:00,808: Train batch 18600: loss: 12.46 grad norm: 61.33 time: 0.068
2026-01-05 15:17:19,910: Train batch 18800: loss: 8.43 grad norm: 51.33 time: 0.065
2026-01-05 15:17:39,318: Train batch 19000: loss: 8.22 grad norm: 47.00 time: 0.064
2026-01-05 15:17:39,318: Running test after training batch: 19000
2026-01-05 15:17:39,469: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:17:44,552: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:17:44,588: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 15:17:46,634: Val batch 19000: PER (avg): 0.1449 CTC Loss (avg): 15.0133 WER(1gram): 46.95% (n=64) time: 7.315
2026-01-05 15:17:46,634: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-05 15:17:46,635: t15.2023.08.13 val PER: 0.1102
2026-01-05 15:17:46,635: t15.2023.08.18 val PER: 0.1073
2026-01-05 15:17:46,635: t15.2023.08.20 val PER: 0.1001
2026-01-05 15:17:46,635: t15.2023.08.25 val PER: 0.0964
2026-01-05 15:17:46,635: t15.2023.08.27 val PER: 0.1736
2026-01-05 15:17:46,635: t15.2023.09.01 val PER: 0.0747
2026-01-05 15:17:46,636: t15.2023.09.03 val PER: 0.1508
2026-01-05 15:17:46,636: t15.2023.09.24 val PER: 0.1286
2026-01-05 15:17:46,636: t15.2023.09.29 val PER: 0.1270
2026-01-05 15:17:46,636: t15.2023.10.01 val PER: 0.1664
2026-01-05 15:17:46,636: t15.2023.10.06 val PER: 0.0872
2026-01-05 15:17:46,636: t15.2023.10.08 val PER: 0.2503
2026-01-05 15:17:46,636: t15.2023.10.13 val PER: 0.1916
2026-01-05 15:17:46,636: t15.2023.10.15 val PER: 0.1417
2026-01-05 15:17:46,636: t15.2023.10.20 val PER: 0.1879
2026-01-05 15:17:46,636: t15.2023.10.22 val PER: 0.1024
2026-01-05 15:17:46,636: t15.2023.11.03 val PER: 0.1791
2026-01-05 15:17:46,636: t15.2023.11.04 val PER: 0.0341
2026-01-05 15:17:46,637: t15.2023.11.17 val PER: 0.0327
2026-01-05 15:17:46,637: t15.2023.11.19 val PER: 0.0379
2026-01-05 15:17:46,637: t15.2023.11.26 val PER: 0.1109
2026-01-05 15:17:46,637: t15.2023.12.03 val PER: 0.1071
2026-01-05 15:17:46,637: t15.2023.12.08 val PER: 0.0985
2026-01-05 15:17:46,637: t15.2023.12.10 val PER: 0.0867
2026-01-05 15:17:46,637: t15.2023.12.17 val PER: 0.1195
2026-01-05 15:17:46,637: t15.2023.12.29 val PER: 0.1297
2026-01-05 15:17:46,637: t15.2024.02.25 val PER: 0.1067
2026-01-05 15:17:46,637: t15.2024.03.08 val PER: 0.2248
2026-01-05 15:17:46,637: t15.2024.03.15 val PER: 0.1970
2026-01-05 15:17:46,637: t15.2024.03.17 val PER: 0.1318
2026-01-05 15:17:46,637: t15.2024.05.10 val PER: 0.1545
2026-01-05 15:17:46,637: t15.2024.06.14 val PER: 0.1625
2026-01-05 15:17:46,637: t15.2024.07.19 val PER: 0.2235
2026-01-05 15:17:46,638: t15.2024.07.21 val PER: 0.0945
2026-01-05 15:17:46,638: t15.2024.07.28 val PER: 0.1287
2026-01-05 15:17:46,638: t15.2025.01.10 val PER: 0.2769
2026-01-05 15:17:46,638: t15.2025.01.12 val PER: 0.1332
2026-01-05 15:17:46,638: t15.2025.03.14 val PER: 0.3491
2026-01-05 15:17:46,638: t15.2025.03.16 val PER: 0.1780
2026-01-05 15:17:46,638: t15.2025.03.30 val PER: 0.2747
2026-01-05 15:17:46,638: t15.2025.04.13 val PER: 0.2068
2026-01-05 15:17:46,982: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_19000
2026-01-05 15:18:06,002: Train batch 19200: loss: 5.74 grad norm: 46.81 time: 0.063
2026-01-05 15:18:25,114: Train batch 19400: loss: 4.83 grad norm: 37.85 time: 0.053
2026-01-05 15:18:34,641: Running test after training batch: 19500
2026-01-05 15:18:34,805: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:18:40,096: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:18:40,132: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost get
2026-01-05 15:18:42,142: Val batch 19500: PER (avg): 0.1439 CTC Loss (avg): 14.9532 WER(1gram): 46.19% (n=64) time: 7.500
2026-01-05 15:18:42,142: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-05 15:18:42,143: t15.2023.08.13 val PER: 0.1123
2026-01-05 15:18:42,143: t15.2023.08.18 val PER: 0.1048
2026-01-05 15:18:42,143: t15.2023.08.20 val PER: 0.1017
2026-01-05 15:18:42,143: t15.2023.08.25 val PER: 0.0904
2026-01-05 15:18:42,143: t15.2023.08.27 val PER: 0.1720
2026-01-05 15:18:42,143: t15.2023.09.01 val PER: 0.0755
2026-01-05 15:18:42,143: t15.2023.09.03 val PER: 0.1449
2026-01-05 15:18:42,143: t15.2023.09.24 val PER: 0.1286
2026-01-05 15:18:42,143: t15.2023.09.29 val PER: 0.1244
2026-01-05 15:18:42,143: t15.2023.10.01 val PER: 0.1664
2026-01-05 15:18:42,143: t15.2023.10.06 val PER: 0.0861
2026-01-05 15:18:42,144: t15.2023.10.08 val PER: 0.2503
2026-01-05 15:18:42,144: t15.2023.10.13 val PER: 0.1877
2026-01-05 15:18:42,144: t15.2023.10.15 val PER: 0.1397
2026-01-05 15:18:42,144: t15.2023.10.20 val PER: 0.1812
2026-01-05 15:18:42,144: t15.2023.10.22 val PER: 0.1047
2026-01-05 15:18:42,144: t15.2023.11.03 val PER: 0.1777
2026-01-05 15:18:42,144: t15.2023.11.04 val PER: 0.0341
2026-01-05 15:18:42,144: t15.2023.11.17 val PER: 0.0389
2026-01-05 15:18:42,144: t15.2023.11.19 val PER: 0.0379
2026-01-05 15:18:42,144: t15.2023.11.26 val PER: 0.1065
2026-01-05 15:18:42,144: t15.2023.12.03 val PER: 0.1029
2026-01-05 15:18:42,144: t15.2023.12.08 val PER: 0.0959
2026-01-05 15:18:42,144: t15.2023.12.10 val PER: 0.0802
2026-01-05 15:18:42,144: t15.2023.12.17 val PER: 0.1185
2026-01-05 15:18:42,144: t15.2023.12.29 val PER: 0.1249
2026-01-05 15:18:42,145: t15.2024.02.25 val PER: 0.1039
2026-01-05 15:18:42,145: t15.2024.03.08 val PER: 0.2219
2026-01-05 15:18:42,145: t15.2024.03.15 val PER: 0.1970
2026-01-05 15:18:42,145: t15.2024.03.17 val PER: 0.1318
2026-01-05 15:18:42,145: t15.2024.05.10 val PER: 0.1545
2026-01-05 15:18:42,145: t15.2024.06.14 val PER: 0.1546
2026-01-05 15:18:42,145: t15.2024.07.19 val PER: 0.2314
2026-01-05 15:18:42,145: t15.2024.07.21 val PER: 0.0924
2026-01-05 15:18:42,145: t15.2024.07.28 val PER: 0.1250
2026-01-05 15:18:42,145: t15.2025.01.10 val PER: 0.2837
2026-01-05 15:18:42,145: t15.2025.01.12 val PER: 0.1378
2026-01-05 15:18:42,145: t15.2025.03.14 val PER: 0.3417
2026-01-05 15:18:42,145: t15.2025.03.16 val PER: 0.1780
2026-01-05 15:18:42,146: t15.2025.03.30 val PER: 0.2759
2026-01-05 15:18:42,146: t15.2025.04.13 val PER: 0.2097
2026-01-05 15:18:42,483: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_19500
2026-01-05 15:18:51,938: Train batch 19600: loss: 7.48 grad norm: 46.17 time: 0.057
2026-01-05 15:19:10,858: Train batch 19800: loss: 7.10 grad norm: 51.84 time: 0.055
2026-01-05 15:19:29,905: Running test after training batch: 19999
2026-01-05 15:19:30,024: WER debug GT example: You can see the code at this point as well.
2026-01-05 15:19:35,958: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-05 15:19:35,997: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost get
2026-01-05 15:19:38,070: Val batch 19999: PER (avg): 0.1440 CTC Loss (avg): 14.9571 WER(1gram): 45.43% (n=64) time: 8.164
2026-01-05 15:19:38,070: WER lens: avg_true_words=6.16 avg_pred_words=6.22 max_pred_words=12
2026-01-05 15:19:38,070: t15.2023.08.13 val PER: 0.1112
2026-01-05 15:19:38,071: t15.2023.08.18 val PER: 0.1039
2026-01-05 15:19:38,071: t15.2023.08.20 val PER: 0.1001
2026-01-05 15:19:38,071: t15.2023.08.25 val PER: 0.0904
2026-01-05 15:19:38,071: t15.2023.08.27 val PER: 0.1720
2026-01-05 15:19:38,071: t15.2023.09.01 val PER: 0.0747
2026-01-05 15:19:38,071: t15.2023.09.03 val PER: 0.1461
2026-01-05 15:19:38,071: t15.2023.09.24 val PER: 0.1274
2026-01-05 15:19:38,071: t15.2023.09.29 val PER: 0.1270
2026-01-05 15:19:38,071: t15.2023.10.01 val PER: 0.1684
2026-01-05 15:19:38,072: t15.2023.10.06 val PER: 0.0850
2026-01-05 15:19:38,072: t15.2023.10.08 val PER: 0.2436
2026-01-05 15:19:38,072: t15.2023.10.13 val PER: 0.1901
2026-01-05 15:19:38,072: t15.2023.10.15 val PER: 0.1397
2026-01-05 15:19:38,072: t15.2023.10.20 val PER: 0.1779
2026-01-05 15:19:38,072: t15.2023.10.22 val PER: 0.1058
2026-01-05 15:19:38,072: t15.2023.11.03 val PER: 0.1798
2026-01-05 15:19:38,072: t15.2023.11.04 val PER: 0.0341
2026-01-05 15:19:38,072: t15.2023.11.17 val PER: 0.0373
2026-01-05 15:19:38,072: t15.2023.11.19 val PER: 0.0379
2026-01-05 15:19:38,072: t15.2023.11.26 val PER: 0.1087
2026-01-05 15:19:38,072: t15.2023.12.03 val PER: 0.1061
2026-01-05 15:19:38,072: t15.2023.12.08 val PER: 0.0939
2026-01-05 15:19:38,072: t15.2023.12.10 val PER: 0.0880
2026-01-05 15:19:38,072: t15.2023.12.17 val PER: 0.1206
2026-01-05 15:19:38,072: t15.2023.12.29 val PER: 0.1304
2026-01-05 15:19:38,072: t15.2024.02.25 val PER: 0.0997
2026-01-05 15:19:38,073: t15.2024.03.08 val PER: 0.2233
2026-01-05 15:19:38,073: t15.2024.03.15 val PER: 0.1951
2026-01-05 15:19:38,073: t15.2024.03.17 val PER: 0.1304
2026-01-05 15:19:38,073: t15.2024.05.10 val PER: 0.1516
2026-01-05 15:19:38,073: t15.2024.06.14 val PER: 0.1593
2026-01-05 15:19:38,073: t15.2024.07.19 val PER: 0.2340
2026-01-05 15:19:38,073: t15.2024.07.21 val PER: 0.0924
2026-01-05 15:19:38,073: t15.2024.07.28 val PER: 0.1250
2026-01-05 15:19:38,073: t15.2025.01.10 val PER: 0.2810
2026-01-05 15:19:38,073: t15.2025.01.12 val PER: 0.1363
2026-01-05 15:19:38,073: t15.2025.03.14 val PER: 0.3447
2026-01-05 15:19:38,073: t15.2025.03.16 val PER: 0.1715
2026-01-05 15:19:38,073: t15.2025.03.30 val PER: 0.2724
2026-01-05 15:19:38,073: t15.2025.04.13 val PER: 0.2054
2026-01-05 15:19:38,419: Saved model to checkpoint: /tmp/e12511253_b2t_349139/trained_models/speckleFeat_p/lr40_wd1e-5/p008_wd1e-5/checkpoint/checkpoint_batch_19999
2026-01-05 15:19:38,455: Best avg val PER achieved: 0.14848
2026-01-05 15:19:38,456: Total training time: 37.52 minutes

=== RUN p010_wd1e-5.yaml ===
