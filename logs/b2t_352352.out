TMPDIR=/home/e12511253/tmp
JOB_TMP=/home/e12511253/tmp/e12511253_b2t_352352
TORCH_EXTENSIONS_DIR=/home/e12511253/tmp/e12511253_b2t_352352/torch_extensions
WANDB_DIR=/home/e12511253/tmp/e12511253_b2t_352352/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/home/e12511253/tmp/e12511253_b2t_352352/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan 10 07:20 /home/e12511253/tmp/e12511253_b2t_352352/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
OUT_ROOT=/home/e12511253/Brain2Text/brain2text/trained_models
==============================================
Job: b2t  ID: 352352
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/xlstm/small
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/xlstm/small ==========
Num configs: 4

=== RUN xlstm_medium.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium
2026-01-10 07:20:29,265: Using device: cuda:0
2026-01-10 07:20:30,970: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-10 07:20:30,993: Using 45 sessions after filtering (from 45).
DEBUG: xlstm_backend = vanilla
2026-01-10 07:20:31,057: Using torch.compile (if available)
2026-01-10 07:20:31,058: torch.compile not available (torch<2.0). Skipping.
2026-01-10 07:20:31,058: Initialized RNN decoding model
2026-01-10 07:20:31,058: XLSTMDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (in_proj): Linear(in_features=7168, out_features=768, bias=True)
  (xlstm): xLSTMBlockStack(
    (blocks): ModuleList(
      (0): sLSTMBlock(
        (xlstm_norm): LayerNorm()
        (xlstm): sLSTMLayer(
          (conv1d): CausalConv1d(
            (conv): Conv1d(768, 768, kernel_size=(4,), stride=(1,), padding=(3,), groups=768)
          )
          (conv_act_fn): SiLU()
          (fgate): LinearHeadwiseExpand(in_features=768, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (igate): LinearHeadwiseExpand(in_features=768, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (zgate): LinearHeadwiseExpand(in_features=768, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (ogate): LinearHeadwiseExpand(in_features=768, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (slstm_cell): sLSTMCell_vanilla(function=slstm, hidden_size=768, num_heads=4)
          (group_norm): MultiHeadLayerNorm()
          (dropout): Dropout(p=0.2, inplace=False)
        )
      )
      (1): sLSTMBlock(
        (xlstm_norm): LayerNorm()
        (xlstm): sLSTMLayer(
          (conv1d): CausalConv1d(
            (conv): Conv1d(768, 768, kernel_size=(4,), stride=(1,), padding=(3,), groups=768)
          )
          (conv_act_fn): SiLU()
          (fgate): LinearHeadwiseExpand(in_features=768, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (igate): LinearHeadwiseExpand(in_features=768, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (zgate): LinearHeadwiseExpand(in_features=768, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (ogate): LinearHeadwiseExpand(in_features=768, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (slstm_cell): sLSTMCell_vanilla(function=slstm, hidden_size=768, num_heads=4)
          (group_norm): MultiHeadLayerNorm()
          (dropout): Dropout(p=0.2, inplace=False)
        )
      )
      (2): sLSTMBlock(
        (xlstm_norm): LayerNorm()
        (xlstm): sLSTMLayer(
          (conv1d): CausalConv1d(
            (conv): Conv1d(768, 768, kernel_size=(4,), stride=(1,), padding=(3,), groups=768)
          )
          (conv_act_fn): SiLU()
          (fgate): LinearHeadwiseExpand(in_features=768, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (igate): LinearHeadwiseExpand(in_features=768, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (zgate): LinearHeadwiseExpand(in_features=768, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (ogate): LinearHeadwiseExpand(in_features=768, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (slstm_cell): sLSTMCell_vanilla(function=slstm, hidden_size=768, num_heads=4)
          (group_norm): MultiHeadLayerNorm()
          (dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (post_blocks_norm): LayerNorm()
  )
  (head): Identity()
  (dropout): Dropout(p=0.2, inplace=False)
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-10 07:20:31,059: Model has 20,921,897 parameters
2026-01-10 07:20:31,059: Model has 11,819,520 day-specific parameters | 56.49% of total parameters
2026-01-10 07:20:32,328: Successfully initialized datasets
2026-01-10 07:20:32,329: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-10 07:20:34,234: Train batch 0: loss: 664.15 grad norm: 1413.82 time: 1.518
2026-01-10 07:20:34,234: Running test after training batch: 0
2026-01-10 07:20:34,457: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:21:03,716: WER debug example
  GT : you can see the code at this point as well
  PR : zeichner cabbage tessick prestidge
2026-01-10 07:21:04,050: WER debug example
  GT : how does it keep the cost down
  PR : joann schoolteacher conjecture gypsies
2026-01-10 07:21:24,307: Val batch 0: PER (avg): 4.5301 CTC Loss (avg): 654.7943 WER(1gram): 100.25% (n=64) time: 50.073
2026-01-10 07:21:24,308: WER lens: avg_true_words=6.16 avg_pred_words=3.69 max_pred_words=7
2026-01-10 07:21:24,308: t15.2023.08.13 val PER: 3.8732
2026-01-10 07:21:24,309: t15.2023.08.18 val PER: 4.2498
2026-01-10 07:21:24,309: t15.2023.08.20 val PER: 4.0151
2026-01-10 07:21:24,309: t15.2023.08.25 val PER: 4.1837
2026-01-10 07:21:24,309: t15.2023.08.27 val PER: 3.5981
2026-01-10 07:21:24,309: t15.2023.09.01 val PER: 4.2224
2026-01-10 07:21:24,309: t15.2023.09.03 val PER: 4.1057
2026-01-10 07:21:24,309: t15.2023.09.24 val PER: 4.8252
2026-01-10 07:21:24,309: t15.2023.09.29 val PER: 4.6637
2026-01-10 07:21:24,309: t15.2023.10.01 val PER: 3.5997
2026-01-10 07:21:24,309: t15.2023.10.06 val PER: 4.7589
2026-01-10 07:21:24,309: t15.2023.10.08 val PER: 3.4655
2026-01-10 07:21:24,309: t15.2023.10.13 val PER: 4.2133
2026-01-10 07:21:24,309: t15.2023.10.15 val PER: 4.6625
2026-01-10 07:21:24,309: t15.2023.10.20 val PER: 4.7617
2026-01-10 07:21:24,310: t15.2023.10.22 val PER: 4.6481
2026-01-10 07:21:24,310: t15.2023.11.03 val PER: 5.2782
2026-01-10 07:21:24,310: t15.2023.11.04 val PER: 6.2935
2026-01-10 07:21:24,310: t15.2023.11.17 val PER: 6.9129
2026-01-10 07:21:24,310: t15.2023.11.19 val PER: 5.2675
2026-01-10 07:21:24,310: t15.2023.11.26 val PER: 4.9594
2026-01-10 07:21:24,311: t15.2023.12.03 val PER: 4.5767
2026-01-10 07:21:24,311: t15.2023.12.08 val PER: 4.9361
2026-01-10 07:21:24,311: t15.2023.12.10 val PER: 5.3824
2026-01-10 07:21:24,312: t15.2023.12.17 val PER: 4.3825
2026-01-10 07:21:24,312: t15.2023.12.29 val PER: 4.2629
2026-01-10 07:21:24,312: t15.2024.02.25 val PER: 4.5829
2026-01-10 07:21:24,312: t15.2024.03.08 val PER: 3.8876
2026-01-10 07:21:24,312: t15.2024.03.15 val PER: 4.0888
2026-01-10 07:21:24,312: t15.2024.03.17 val PER: 4.2943
2026-01-10 07:21:24,313: t15.2024.05.10 val PER: 4.4368
2026-01-10 07:21:24,313: t15.2024.06.14 val PER: 5.1167
2026-01-10 07:21:24,313: t15.2024.07.19 val PER: 3.2070
2026-01-10 07:21:24,313: t15.2024.07.21 val PER: 5.3145
2026-01-10 07:21:24,313: t15.2024.07.28 val PER: 5.2500
2026-01-10 07:21:24,313: t15.2025.01.10 val PER: 3.3072
2026-01-10 07:21:24,313: t15.2025.01.12 val PER: 5.7167
2026-01-10 07:21:24,313: t15.2025.03.14 val PER: 3.4320
2026-01-10 07:21:24,313: t15.2025.03.16 val PER: 5.7513
2026-01-10 07:21:24,314: t15.2025.03.30 val PER: 4.3736
2026-01-10 07:21:24,314: t15.2025.04.13 val PER: 4.7746
2026-01-10 07:21:24,314: New best val WER(1gram) inf% --> 100.25%
2026-01-10 07:21:24,420: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_0
2026-01-10 07:24:08,785: Train batch 200: loss: 98.38 grad norm: 165.18 time: 1.079
2026-01-10 07:26:49,499: Train batch 400: loss: 94.43 grad norm: 100.33 time: 0.652
2026-01-10 07:28:09,713: Running test after training batch: 500
2026-01-10 07:28:09,978: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:28:39,065: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-10 07:28:39,103: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-10 07:28:41,277: Val batch 500: PER (avg): 0.8284 CTC Loss (avg): 102.2504 WER(1gram): 100.00% (n=64) time: 31.561
2026-01-10 07:28:41,278: WER lens: avg_true_words=6.16 avg_pred_words=0.02 max_pred_words=1
2026-01-10 07:28:41,278: t15.2023.08.13 val PER: 0.8067
2026-01-10 07:28:41,278: t15.2023.08.18 val PER: 0.8047
2026-01-10 07:28:41,278: t15.2023.08.20 val PER: 0.8094
2026-01-10 07:28:41,278: t15.2023.08.25 val PER: 0.8449
2026-01-10 07:28:41,278: t15.2023.08.27 val PER: 0.8971
2026-01-10 07:28:41,278: t15.2023.09.01 val PER: 0.7987
2026-01-10 07:28:41,278: t15.2023.09.03 val PER: 0.8420
2026-01-10 07:28:41,278: t15.2023.09.24 val PER: 0.7961
2026-01-10 07:28:41,278: t15.2023.09.29 val PER: 0.7869
2026-01-10 07:28:41,278: t15.2023.10.01 val PER: 0.8408
2026-01-10 07:28:41,279: t15.2023.10.06 val PER: 0.8073
2026-01-10 07:28:41,279: t15.2023.10.08 val PER: 0.8403
2026-01-10 07:28:41,279: t15.2023.10.13 val PER: 0.8254
2026-01-10 07:28:41,279: t15.2023.10.15 val PER: 0.8279
2026-01-10 07:28:41,279: t15.2023.10.20 val PER: 0.7685
2026-01-10 07:28:41,279: t15.2023.10.22 val PER: 0.8151
2026-01-10 07:28:41,279: t15.2023.11.03 val PER: 0.8033
2026-01-10 07:28:41,279: t15.2023.11.04 val PER: 0.7986
2026-01-10 07:28:41,279: t15.2023.11.17 val PER: 0.7729
2026-01-10 07:28:41,280: t15.2023.11.19 val PER: 0.7625
2026-01-10 07:28:41,280: t15.2023.11.26 val PER: 0.8326
2026-01-10 07:28:41,280: t15.2023.12.03 val PER: 0.8277
2026-01-10 07:28:41,280: t15.2023.12.08 val PER: 0.8176
2026-01-10 07:28:41,280: t15.2023.12.10 val PER: 0.7976
2026-01-10 07:28:41,280: t15.2023.12.17 val PER: 0.8701
2026-01-10 07:28:41,280: t15.2023.12.29 val PER: 0.8806
2026-01-10 07:28:41,280: t15.2024.02.25 val PER: 0.8272
2026-01-10 07:28:41,280: t15.2024.03.08 val PER: 0.9118
2026-01-10 07:28:41,281: t15.2024.03.15 val PER: 0.8599
2026-01-10 07:28:41,281: t15.2024.03.17 val PER: 0.8417
2026-01-10 07:28:41,281: t15.2024.05.10 val PER: 0.8529
2026-01-10 07:28:41,281: t15.2024.06.14 val PER: 0.7997
2026-01-10 07:28:41,281: t15.2024.07.19 val PER: 0.8596
2026-01-10 07:28:41,281: t15.2024.07.21 val PER: 0.7766
2026-01-10 07:28:41,281: t15.2024.07.28 val PER: 0.8029
2026-01-10 07:28:41,281: t15.2025.01.10 val PER: 0.8912
2026-01-10 07:28:41,281: t15.2025.01.12 val PER: 0.8422
2026-01-10 07:28:41,281: t15.2025.03.14 val PER: 0.8876
2026-01-10 07:28:41,281: t15.2025.03.16 val PER: 0.8285
2026-01-10 07:28:41,281: t15.2025.03.30 val PER: 0.8402
2026-01-10 07:28:41,281: t15.2025.04.13 val PER: 0.8573
2026-01-10 07:28:41,282: New best val WER(1gram) 100.25% --> 100.00%
2026-01-10 07:28:41,397: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_500
2026-01-10 07:30:02,544: Train batch 600: loss: 74.28 grad norm: 142.73 time: 0.637
2026-01-10 07:32:43,247: Train batch 800: loss: 69.04 grad norm: 163.82 time: 0.728
2026-01-10 07:35:22,817: Train batch 1000: loss: 64.27 grad norm: 94.67 time: 1.087
2026-01-10 07:35:22,819: Running test after training batch: 1000
2026-01-10 07:35:23,036: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:35:51,961: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-10 07:35:51,989: WER debug example
  GT : how does it keep the cost down
  PR : 
2026-01-10 07:35:53,607: Val batch 1000: PER (avg): 0.6575 CTC Loss (avg): 58.4528 WER(1gram): 98.98% (n=64) time: 30.788
2026-01-10 07:35:53,608: WER lens: avg_true_words=6.16 avg_pred_words=0.59 max_pred_words=3
2026-01-10 07:35:53,608: t15.2023.08.13 val PER: 0.6247
2026-01-10 07:35:53,608: t15.2023.08.18 val PER: 0.6044
2026-01-10 07:35:53,608: t15.2023.08.20 val PER: 0.5949
2026-01-10 07:35:53,608: t15.2023.08.25 val PER: 0.5889
2026-01-10 07:35:53,608: t15.2023.08.27 val PER: 0.6608
2026-01-10 07:35:53,608: t15.2023.09.01 val PER: 0.5690
2026-01-10 07:35:53,608: t15.2023.09.03 val PER: 0.6781
2026-01-10 07:35:53,608: t15.2023.09.24 val PER: 0.5825
2026-01-10 07:35:53,608: t15.2023.09.29 val PER: 0.5916
2026-01-10 07:35:53,609: t15.2023.10.01 val PER: 0.6513
2026-01-10 07:35:53,609: t15.2023.10.06 val PER: 0.5931
2026-01-10 07:35:53,609: t15.2023.10.08 val PER: 0.6482
2026-01-10 07:35:53,609: t15.2023.10.13 val PER: 0.6757
2026-01-10 07:35:53,609: t15.2023.10.15 val PER: 0.6381
2026-01-10 07:35:53,609: t15.2023.10.20 val PER: 0.5772
2026-01-10 07:35:53,609: t15.2023.10.22 val PER: 0.6058
2026-01-10 07:35:53,609: t15.2023.11.03 val PER: 0.6201
2026-01-10 07:35:53,609: t15.2023.11.04 val PER: 0.5017
2026-01-10 07:35:53,609: t15.2023.11.17 val PER: 0.5645
2026-01-10 07:35:53,609: t15.2023.11.19 val PER: 0.5329
2026-01-10 07:35:53,609: t15.2023.11.26 val PER: 0.6594
2026-01-10 07:35:53,609: t15.2023.12.03 val PER: 0.6366
2026-01-10 07:35:53,609: t15.2023.12.08 val PER: 0.6671
2026-01-10 07:35:53,609: t15.2023.12.10 val PER: 0.6110
2026-01-10 07:35:53,610: t15.2023.12.17 val PER: 0.7432
2026-01-10 07:35:53,610: t15.2023.12.29 val PER: 0.6843
2026-01-10 07:35:53,610: t15.2024.02.25 val PER: 0.6376
2026-01-10 07:35:53,610: t15.2024.03.08 val PER: 0.7425
2026-01-10 07:35:53,610: t15.2024.03.15 val PER: 0.6917
2026-01-10 07:35:53,610: t15.2024.03.17 val PER: 0.6492
2026-01-10 07:35:53,610: t15.2024.05.10 val PER: 0.6523
2026-01-10 07:35:53,610: t15.2024.06.14 val PER: 0.6782
2026-01-10 07:35:53,610: t15.2024.07.19 val PER: 0.7732
2026-01-10 07:35:53,610: t15.2024.07.21 val PER: 0.6255
2026-01-10 07:35:53,610: t15.2024.07.28 val PER: 0.6610
2026-01-10 07:35:53,610: t15.2025.01.10 val PER: 0.8444
2026-01-10 07:35:53,610: t15.2025.01.12 val PER: 0.6905
2026-01-10 07:35:53,611: t15.2025.03.14 val PER: 0.8166
2026-01-10 07:35:53,611: t15.2025.03.16 val PER: 0.7107
2026-01-10 07:35:53,611: t15.2025.03.30 val PER: 0.8644
2026-01-10 07:35:53,611: t15.2025.04.13 val PER: 0.6961
2026-01-10 07:35:53,612: New best val WER(1gram) 100.00% --> 98.98%
2026-01-10 07:35:53,717: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_1000
2026-01-10 07:38:33,624: Train batch 1200: loss: 63.63 grad norm: 129.96 time: 0.941
2026-01-10 07:41:11,171: Train batch 1400: loss: 67.69 grad norm: 96.70 time: 0.851
2026-01-10 07:42:27,997: Running test after training batch: 1500
2026-01-10 07:42:28,214: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:42:56,892: WER debug example
  GT : you can see the code at this point as well
  PR : tees uthe tug deet tese zoucha tietz zewe
2026-01-10 07:42:56,922: WER debug example
  GT : how does it keep the cost down
  PR : z thus sucked
2026-01-10 07:42:58,595: Val batch 1500: PER (avg): 0.5871 CTC Loss (avg): 51.2797 WER(1gram): 98.48% (n=64) time: 30.597
2026-01-10 07:42:58,595: WER lens: avg_true_words=6.16 avg_pred_words=2.91 max_pred_words=8
2026-01-10 07:42:58,596: t15.2023.08.13 val PER: 0.5520
2026-01-10 07:42:58,596: t15.2023.08.18 val PER: 0.5390
2026-01-10 07:42:58,596: t15.2023.08.20 val PER: 0.5218
2026-01-10 07:42:58,596: t15.2023.08.25 val PER: 0.5151
2026-01-10 07:42:58,597: t15.2023.08.27 val PER: 0.6029
2026-01-10 07:42:58,597: t15.2023.09.01 val PER: 0.5024
2026-01-10 07:42:58,597: t15.2023.09.03 val PER: 0.6354
2026-01-10 07:42:58,597: t15.2023.09.24 val PER: 0.5364
2026-01-10 07:42:58,597: t15.2023.09.29 val PER: 0.5373
2026-01-10 07:42:58,597: t15.2023.10.01 val PER: 0.5694
2026-01-10 07:42:58,597: t15.2023.10.06 val PER: 0.5231
2026-01-10 07:42:58,598: t15.2023.10.08 val PER: 0.5805
2026-01-10 07:42:58,598: t15.2023.10.13 val PER: 0.6082
2026-01-10 07:42:58,598: t15.2023.10.15 val PER: 0.5689
2026-01-10 07:42:58,598: t15.2023.10.20 val PER: 0.5268
2026-01-10 07:42:58,598: t15.2023.10.22 val PER: 0.5356
2026-01-10 07:42:58,598: t15.2023.11.03 val PER: 0.5488
2026-01-10 07:42:58,598: t15.2023.11.04 val PER: 0.4164
2026-01-10 07:42:58,598: t15.2023.11.17 val PER: 0.4681
2026-01-10 07:42:58,598: t15.2023.11.19 val PER: 0.4511
2026-01-10 07:42:58,598: t15.2023.11.26 val PER: 0.6080
2026-01-10 07:42:58,599: t15.2023.12.03 val PER: 0.5494
2026-01-10 07:42:58,599: t15.2023.12.08 val PER: 0.5905
2026-01-10 07:42:58,599: t15.2023.12.10 val PER: 0.5532
2026-01-10 07:42:58,599: t15.2023.12.17 val PER: 0.6268
2026-01-10 07:42:58,599: t15.2023.12.29 val PER: 0.6060
2026-01-10 07:42:58,599: t15.2024.02.25 val PER: 0.5421
2026-01-10 07:42:58,599: t15.2024.03.08 val PER: 0.6501
2026-01-10 07:42:58,599: t15.2024.03.15 val PER: 0.6191
2026-01-10 07:42:58,599: t15.2024.03.17 val PER: 0.5927
2026-01-10 07:42:58,599: t15.2024.05.10 val PER: 0.5914
2026-01-10 07:42:58,599: t15.2024.06.14 val PER: 0.6088
2026-01-10 07:42:58,599: t15.2024.07.19 val PER: 0.6862
2026-01-10 07:42:58,599: t15.2024.07.21 val PER: 0.5607
2026-01-10 07:42:58,600: t15.2024.07.28 val PER: 0.5956
2026-01-10 07:42:58,600: t15.2025.01.10 val PER: 0.7562
2026-01-10 07:42:58,600: t15.2025.01.12 val PER: 0.6274
2026-01-10 07:42:58,600: t15.2025.03.14 val PER: 0.7515
2026-01-10 07:42:58,600: t15.2025.03.16 val PER: 0.6387
2026-01-10 07:42:58,600: t15.2025.03.30 val PER: 0.7977
2026-01-10 07:42:58,600: t15.2025.04.13 val PER: 0.6462
2026-01-10 07:42:58,600: New best val WER(1gram) 98.98% --> 98.48%
2026-01-10 07:42:58,702: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_1500
2026-01-10 07:44:18,198: Train batch 1600: loss: 41.27 grad norm: 89.59 time: 0.587
2026-01-10 07:46:55,205: Train batch 1800: loss: 37.48 grad norm: 78.40 time: 0.598
2026-01-10 07:49:36,565: Train batch 2000: loss: 60.06 grad norm: 110.60 time: 0.670
2026-01-10 07:49:36,566: Running test after training batch: 2000
2026-01-10 07:49:36,791: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:50:05,608: WER debug example
  GT : you can see the code at this point as well
  PR : luque tees uthe ugh tat zoucha teet zewe
2026-01-10 07:50:05,639: WER debug example
  GT : how does it keep the cost down
  PR : diede zitek teethe utz sucked
2026-01-10 07:50:07,430: Val batch 2000: PER (avg): 0.5630 CTC Loss (avg): 48.7551 WER(1gram): 98.98% (n=64) time: 30.863
2026-01-10 07:50:07,430: WER lens: avg_true_words=6.16 avg_pred_words=4.27 max_pred_words=8
2026-01-10 07:50:07,430: t15.2023.08.13 val PER: 0.5177
2026-01-10 07:50:07,431: t15.2023.08.18 val PER: 0.5256
2026-01-10 07:50:07,431: t15.2023.08.20 val PER: 0.4893
2026-01-10 07:50:07,431: t15.2023.08.25 val PER: 0.4623
2026-01-10 07:50:07,431: t15.2023.08.27 val PER: 0.5756
2026-01-10 07:50:07,431: t15.2023.09.01 val PER: 0.4554
2026-01-10 07:50:07,431: t15.2023.09.03 val PER: 0.5926
2026-01-10 07:50:07,431: t15.2023.09.24 val PER: 0.5012
2026-01-10 07:50:07,431: t15.2023.09.29 val PER: 0.5220
2026-01-10 07:50:07,431: t15.2023.10.01 val PER: 0.5476
2026-01-10 07:50:07,431: t15.2023.10.06 val PER: 0.4876
2026-01-10 07:50:07,432: t15.2023.10.08 val PER: 0.5562
2026-01-10 07:50:07,432: t15.2023.10.13 val PER: 0.5935
2026-01-10 07:50:07,432: t15.2023.10.15 val PER: 0.5485
2026-01-10 07:50:07,432: t15.2023.10.20 val PER: 0.4933
2026-01-10 07:50:07,432: t15.2023.10.22 val PER: 0.4866
2026-01-10 07:50:07,432: t15.2023.11.03 val PER: 0.5197
2026-01-10 07:50:07,432: t15.2023.11.04 val PER: 0.3584
2026-01-10 07:50:07,432: t15.2023.11.17 val PER: 0.4370
2026-01-10 07:50:07,432: t15.2023.11.19 val PER: 0.4251
2026-01-10 07:50:07,432: t15.2023.11.26 val PER: 0.5681
2026-01-10 07:50:07,432: t15.2023.12.03 val PER: 0.5200
2026-01-10 07:50:07,433: t15.2023.12.08 val PER: 0.5519
2026-01-10 07:50:07,433: t15.2023.12.10 val PER: 0.5309
2026-01-10 07:50:07,433: t15.2023.12.17 val PER: 0.5842
2026-01-10 07:50:07,433: t15.2023.12.29 val PER: 0.5745
2026-01-10 07:50:07,433: t15.2024.02.25 val PER: 0.5169
2026-01-10 07:50:07,433: t15.2024.03.08 val PER: 0.6401
2026-01-10 07:50:07,433: t15.2024.03.15 val PER: 0.5941
2026-01-10 07:50:07,433: t15.2024.03.17 val PER: 0.5704
2026-01-10 07:50:07,433: t15.2024.05.10 val PER: 0.5706
2026-01-10 07:50:07,433: t15.2024.06.14 val PER: 0.5931
2026-01-10 07:50:07,434: t15.2024.07.19 val PER: 0.6790
2026-01-10 07:50:07,434: t15.2024.07.21 val PER: 0.5469
2026-01-10 07:50:07,434: t15.2024.07.28 val PER: 0.5956
2026-01-10 07:50:07,434: t15.2025.01.10 val PER: 0.7521
2026-01-10 07:50:07,434: t15.2025.01.12 val PER: 0.6112
2026-01-10 07:50:07,434: t15.2025.03.14 val PER: 0.7411
2026-01-10 07:50:07,434: t15.2025.03.16 val PER: 0.6361
2026-01-10 07:50:07,435: t15.2025.03.30 val PER: 0.8057
2026-01-10 07:50:07,435: t15.2025.04.13 val PER: 0.6419
2026-01-10 07:50:07,510: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_2000
2026-01-10 07:52:44,861: Train batch 2200: loss: 55.99 grad norm: 96.05 time: 0.789
2026-01-10 07:55:27,883: Train batch 2400: loss: 51.22 grad norm: 95.54 time: 0.742
2026-01-10 07:56:49,787: Running test after training batch: 2500
2026-01-10 07:56:50,139: WER debug GT example: You can see the code at this point as well.
2026-01-10 07:57:19,260: WER debug example
  GT : you can see the code at this point as well
  PR : luque tse uthe ugh datz saha tietz zwahlen
2026-01-10 07:57:19,291: WER debug example
  GT : how does it keep the cost down
  PR : uhde zitek teethe us sucked
2026-01-10 07:57:21,002: Val batch 2500: PER (avg): 0.4809 CTC Loss (avg): 43.7772 WER(1gram): 99.24% (n=64) time: 31.214
2026-01-10 07:57:21,003: WER lens: avg_true_words=6.16 avg_pred_words=4.39 max_pred_words=9
2026-01-10 07:57:21,003: t15.2023.08.13 val PER: 0.4584
2026-01-10 07:57:21,003: t15.2023.08.18 val PER: 0.4283
2026-01-10 07:57:21,003: t15.2023.08.20 val PER: 0.4273
2026-01-10 07:57:21,003: t15.2023.08.25 val PER: 0.3780
2026-01-10 07:57:21,003: t15.2023.08.27 val PER: 0.5129
2026-01-10 07:57:21,003: t15.2023.09.01 val PER: 0.3815
2026-01-10 07:57:21,003: t15.2023.09.03 val PER: 0.4988
2026-01-10 07:57:21,003: t15.2023.09.24 val PER: 0.4320
2026-01-10 07:57:21,004: t15.2023.09.29 val PER: 0.4225
2026-01-10 07:57:21,004: t15.2023.10.01 val PER: 0.4657
2026-01-10 07:57:21,004: t15.2023.10.06 val PER: 0.4090
2026-01-10 07:57:21,004: t15.2023.10.08 val PER: 0.4668
2026-01-10 07:57:21,004: t15.2023.10.13 val PER: 0.5229
2026-01-10 07:57:21,004: t15.2023.10.15 val PER: 0.4634
2026-01-10 07:57:21,005: t15.2023.10.20 val PER: 0.4396
2026-01-10 07:57:21,005: t15.2023.10.22 val PER: 0.4065
2026-01-10 07:57:21,005: t15.2023.11.03 val PER: 0.4444
2026-01-10 07:57:21,005: t15.2023.11.04 val PER: 0.2765
2026-01-10 07:57:21,005: t15.2023.11.17 val PER: 0.3546
2026-01-10 07:57:21,005: t15.2023.11.19 val PER: 0.3194
2026-01-10 07:57:21,005: t15.2023.11.26 val PER: 0.4935
2026-01-10 07:57:21,005: t15.2023.12.03 val PER: 0.4454
2026-01-10 07:57:21,005: t15.2023.12.08 val PER: 0.4840
2026-01-10 07:57:21,005: t15.2023.12.10 val PER: 0.4560
2026-01-10 07:57:21,005: t15.2023.12.17 val PER: 0.4969
2026-01-10 07:57:21,005: t15.2023.12.29 val PER: 0.4804
2026-01-10 07:57:21,005: t15.2024.02.25 val PER: 0.4171
2026-01-10 07:57:21,006: t15.2024.03.08 val PER: 0.5292
2026-01-10 07:57:21,006: t15.2024.03.15 val PER: 0.4997
2026-01-10 07:57:21,006: t15.2024.03.17 val PER: 0.4812
2026-01-10 07:57:21,006: t15.2024.05.10 val PER: 0.4918
2026-01-10 07:57:21,006: t15.2024.06.14 val PER: 0.5174
2026-01-10 07:57:21,006: t15.2024.07.19 val PER: 0.5992
2026-01-10 07:57:21,006: t15.2024.07.21 val PER: 0.4579
2026-01-10 07:57:21,006: t15.2024.07.28 val PER: 0.4941
2026-01-10 07:57:21,006: t15.2025.01.10 val PER: 0.6997
2026-01-10 07:57:21,006: t15.2025.01.12 val PER: 0.5273
2026-01-10 07:57:21,006: t15.2025.03.14 val PER: 0.6953
2026-01-10 07:57:21,006: t15.2025.03.16 val PER: 0.5537
2026-01-10 07:57:21,007: t15.2025.03.30 val PER: 0.7218
2026-01-10 07:57:21,007: t15.2025.04.13 val PER: 0.5392
2026-01-10 07:57:21,080: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_2500
2026-01-10 07:58:41,492: Train batch 2600: loss: 37.13 grad norm: 74.38 time: 1.243
2026-01-10 08:01:24,400: Train batch 2800: loss: 48.35 grad norm: 97.29 time: 0.969
2026-01-10 08:04:03,225: Train batch 3000: loss: 42.48 grad norm: 77.62 time: 0.976
2026-01-10 08:04:03,226: Running test after training batch: 3000
2026-01-10 08:04:03,442: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:04:32,258: WER debug example
  GT : you can see the code at this point as well
  PR : luque aunt uthe ugh tat stub tietz zumwalt
2026-01-10 08:04:32,286: WER debug example
  GT : how does it keep the cost down
  PR : ude zitek piecuch thus studt
2026-01-10 08:04:33,923: Val batch 3000: PER (avg): 0.4741 CTC Loss (avg): 41.9209 WER(1gram): 98.73% (n=64) time: 30.696
2026-01-10 08:04:33,923: WER lens: avg_true_words=6.16 avg_pred_words=4.48 max_pred_words=9
2026-01-10 08:04:33,923: t15.2023.08.13 val PER: 0.4428
2026-01-10 08:04:33,923: t15.2023.08.18 val PER: 0.3948
2026-01-10 08:04:33,924: t15.2023.08.20 val PER: 0.4019
2026-01-10 08:04:33,924: t15.2023.08.25 val PER: 0.3599
2026-01-10 08:04:33,924: t15.2023.08.27 val PER: 0.4823
2026-01-10 08:04:33,924: t15.2023.09.01 val PER: 0.3644
2026-01-10 08:04:33,924: t15.2023.09.03 val PER: 0.4881
2026-01-10 08:04:33,924: t15.2023.09.24 val PER: 0.4163
2026-01-10 08:04:33,924: t15.2023.09.29 val PER: 0.4231
2026-01-10 08:04:33,925: t15.2023.10.01 val PER: 0.4505
2026-01-10 08:04:33,925: t15.2023.10.06 val PER: 0.4069
2026-01-10 08:04:33,925: t15.2023.10.08 val PER: 0.4736
2026-01-10 08:04:33,925: t15.2023.10.13 val PER: 0.5175
2026-01-10 08:04:33,925: t15.2023.10.15 val PER: 0.4443
2026-01-10 08:04:33,925: t15.2023.10.20 val PER: 0.3926
2026-01-10 08:04:33,926: t15.2023.10.22 val PER: 0.3953
2026-01-10 08:04:33,926: t15.2023.11.03 val PER: 0.4315
2026-01-10 08:04:33,926: t15.2023.11.04 val PER: 0.2730
2026-01-10 08:04:33,926: t15.2023.11.17 val PER: 0.3375
2026-01-10 08:04:33,926: t15.2023.11.19 val PER: 0.3034
2026-01-10 08:04:33,926: t15.2023.11.26 val PER: 0.4906
2026-01-10 08:04:33,926: t15.2023.12.03 val PER: 0.4517
2026-01-10 08:04:33,926: t15.2023.12.08 val PER: 0.4794
2026-01-10 08:04:33,926: t15.2023.12.10 val PER: 0.4402
2026-01-10 08:04:33,927: t15.2023.12.17 val PER: 0.5187
2026-01-10 08:04:33,927: t15.2023.12.29 val PER: 0.4722
2026-01-10 08:04:33,927: t15.2024.02.25 val PER: 0.4185
2026-01-10 08:04:33,927: t15.2024.03.08 val PER: 0.5420
2026-01-10 08:04:33,927: t15.2024.03.15 val PER: 0.5066
2026-01-10 08:04:33,927: t15.2024.03.17 val PER: 0.4742
2026-01-10 08:04:33,927: t15.2024.05.10 val PER: 0.4978
2026-01-10 08:04:33,927: t15.2024.06.14 val PER: 0.5016
2026-01-10 08:04:33,927: t15.2024.07.19 val PER: 0.6071
2026-01-10 08:04:33,927: t15.2024.07.21 val PER: 0.4497
2026-01-10 08:04:33,928: t15.2024.07.28 val PER: 0.4868
2026-01-10 08:04:33,928: t15.2025.01.10 val PER: 0.6956
2026-01-10 08:04:33,928: t15.2025.01.12 val PER: 0.5189
2026-01-10 08:04:33,928: t15.2025.03.14 val PER: 0.6938
2026-01-10 08:04:33,928: t15.2025.03.16 val PER: 0.5458
2026-01-10 08:04:33,928: t15.2025.03.30 val PER: 0.7460
2026-01-10 08:04:33,928: t15.2025.04.13 val PER: 0.5549
2026-01-10 08:04:34,002: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_3000
2026-01-10 08:07:19,399: Train batch 3200: loss: 52.29 grad norm: 86.29 time: 0.607
2026-01-10 08:09:52,860: Train batch 3400: loss: 42.30 grad norm: 86.69 time: 0.664
2026-01-10 08:11:16,189: Running test after training batch: 3500
2026-01-10 08:11:16,407: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:11:45,360: WER debug example
  GT : you can see the code at this point as well
  PR : luque tse uthe ugh tits stub ditzel
2026-01-10 08:11:45,389: WER debug example
  GT : how does it keep the cost down
  PR : owed zitek piecuch thus stutz
2026-01-10 08:11:46,964: Val batch 3500: PER (avg): 0.4278 CTC Loss (avg): 39.2132 WER(1gram): 98.22% (n=64) time: 30.774
2026-01-10 08:11:46,964: WER lens: avg_true_words=6.16 avg_pred_words=4.50 max_pred_words=9
2026-01-10 08:11:46,964: t15.2023.08.13 val PER: 0.3867
2026-01-10 08:11:46,964: t15.2023.08.18 val PER: 0.3705
2026-01-10 08:11:46,964: t15.2023.08.20 val PER: 0.3622
2026-01-10 08:11:46,965: t15.2023.08.25 val PER: 0.3133
2026-01-10 08:11:46,965: t15.2023.08.27 val PER: 0.4341
2026-01-10 08:11:46,965: t15.2023.09.01 val PER: 0.3255
2026-01-10 08:11:46,965: t15.2023.09.03 val PER: 0.4442
2026-01-10 08:11:46,965: t15.2023.09.24 val PER: 0.3544
2026-01-10 08:11:46,965: t15.2023.09.29 val PER: 0.3657
2026-01-10 08:11:46,965: t15.2023.10.01 val PER: 0.4194
2026-01-10 08:11:46,965: t15.2023.10.06 val PER: 0.3434
2026-01-10 08:11:46,965: t15.2023.10.08 val PER: 0.4384
2026-01-10 08:11:46,965: t15.2023.10.13 val PER: 0.4686
2026-01-10 08:11:46,965: t15.2023.10.15 val PER: 0.4113
2026-01-10 08:11:46,965: t15.2023.10.20 val PER: 0.3758
2026-01-10 08:11:46,965: t15.2023.10.22 val PER: 0.3530
2026-01-10 08:11:46,966: t15.2023.11.03 val PER: 0.3840
2026-01-10 08:11:46,967: t15.2023.11.04 val PER: 0.2048
2026-01-10 08:11:46,967: t15.2023.11.17 val PER: 0.2768
2026-01-10 08:11:46,967: t15.2023.11.19 val PER: 0.2555
2026-01-10 08:11:46,967: t15.2023.11.26 val PER: 0.4543
2026-01-10 08:11:46,967: t15.2023.12.03 val PER: 0.3971
2026-01-10 08:11:46,967: t15.2023.12.08 val PER: 0.4221
2026-01-10 08:11:46,967: t15.2023.12.10 val PER: 0.3903
2026-01-10 08:11:46,968: t15.2023.12.17 val PER: 0.4543
2026-01-10 08:11:46,968: t15.2023.12.29 val PER: 0.4296
2026-01-10 08:11:46,968: t15.2024.02.25 val PER: 0.3694
2026-01-10 08:11:46,968: t15.2024.03.08 val PER: 0.5064
2026-01-10 08:11:46,968: t15.2024.03.15 val PER: 0.4841
2026-01-10 08:11:46,968: t15.2024.03.17 val PER: 0.4310
2026-01-10 08:11:46,968: t15.2024.05.10 val PER: 0.4621
2026-01-10 08:11:46,968: t15.2024.06.14 val PER: 0.4401
2026-01-10 08:11:46,968: t15.2024.07.19 val PER: 0.5465
2026-01-10 08:11:46,968: t15.2024.07.21 val PER: 0.3883
2026-01-10 08:11:46,968: t15.2024.07.28 val PER: 0.4360
2026-01-10 08:11:46,968: t15.2025.01.10 val PER: 0.6446
2026-01-10 08:11:46,968: t15.2025.01.12 val PER: 0.4611
2026-01-10 08:11:46,969: t15.2025.03.14 val PER: 0.6568
2026-01-10 08:11:46,969: t15.2025.03.16 val PER: 0.4974
2026-01-10 08:11:46,969: t15.2025.03.30 val PER: 0.7149
2026-01-10 08:11:46,969: t15.2025.04.13 val PER: 0.5078
2026-01-10 08:11:46,970: New best val WER(1gram) 98.48% --> 98.22%
2026-01-10 08:11:47,078: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_3500
2026-01-10 08:13:11,511: Train batch 3600: loss: 46.19 grad norm: 85.44 time: 0.829
2026-01-10 08:16:02,291: Train batch 3800: loss: 39.86 grad norm: 86.48 time: 0.864
2026-01-10 08:18:47,226: Train batch 4000: loss: 41.48 grad norm: 81.71 time: 0.905
2026-01-10 08:18:47,227: Running test after training batch: 4000
2026-01-10 08:18:47,444: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:19:16,167: WER debug example
  GT : you can see the code at this point as well
  PR : luque tse uthe ugh tits stub ditzel
2026-01-10 08:19:16,196: WER debug example
  GT : how does it keep the cost down
  PR : owed zitek piecuch thus studt
2026-01-10 08:19:17,850: Val batch 4000: PER (avg): 0.3989 CTC Loss (avg): 37.5181 WER(1gram): 98.73% (n=64) time: 30.622
2026-01-10 08:19:17,850: WER lens: avg_true_words=6.16 avg_pred_words=4.47 max_pred_words=9
2026-01-10 08:19:17,850: t15.2023.08.13 val PER: 0.3711
2026-01-10 08:19:17,850: t15.2023.08.18 val PER: 0.3261
2026-01-10 08:19:17,850: t15.2023.08.20 val PER: 0.3431
2026-01-10 08:19:17,851: t15.2023.08.25 val PER: 0.2937
2026-01-10 08:19:17,851: t15.2023.08.27 val PER: 0.4051
2026-01-10 08:19:17,851: t15.2023.09.01 val PER: 0.2963
2026-01-10 08:19:17,851: t15.2023.09.03 val PER: 0.4014
2026-01-10 08:19:17,851: t15.2023.09.24 val PER: 0.3167
2026-01-10 08:19:17,851: t15.2023.09.29 val PER: 0.3382
2026-01-10 08:19:17,851: t15.2023.10.01 val PER: 0.3785
2026-01-10 08:19:17,851: t15.2023.10.06 val PER: 0.2982
2026-01-10 08:19:17,851: t15.2023.10.08 val PER: 0.3978
2026-01-10 08:19:17,851: t15.2023.10.13 val PER: 0.4275
2026-01-10 08:19:17,851: t15.2023.10.15 val PER: 0.3659
2026-01-10 08:19:17,851: t15.2023.10.20 val PER: 0.3423
2026-01-10 08:19:17,851: t15.2023.10.22 val PER: 0.3229
2026-01-10 08:19:17,851: t15.2023.11.03 val PER: 0.3718
2026-01-10 08:19:17,851: t15.2023.11.04 val PER: 0.1843
2026-01-10 08:19:17,851: t15.2023.11.17 val PER: 0.2442
2026-01-10 08:19:17,852: t15.2023.11.19 val PER: 0.2216
2026-01-10 08:19:17,852: t15.2023.11.26 val PER: 0.4203
2026-01-10 08:19:17,852: t15.2023.12.03 val PER: 0.3666
2026-01-10 08:19:17,852: t15.2023.12.08 val PER: 0.3788
2026-01-10 08:19:17,852: t15.2023.12.10 val PER: 0.3469
2026-01-10 08:19:17,852: t15.2023.12.17 val PER: 0.4428
2026-01-10 08:19:17,852: t15.2023.12.29 val PER: 0.4015
2026-01-10 08:19:17,852: t15.2024.02.25 val PER: 0.3385
2026-01-10 08:19:17,852: t15.2024.03.08 val PER: 0.4765
2026-01-10 08:19:17,852: t15.2024.03.15 val PER: 0.4515
2026-01-10 08:19:17,853: t15.2024.03.17 val PER: 0.4031
2026-01-10 08:19:17,853: t15.2024.05.10 val PER: 0.4398
2026-01-10 08:19:17,853: t15.2024.06.14 val PER: 0.4180
2026-01-10 08:19:17,853: t15.2024.07.19 val PER: 0.5339
2026-01-10 08:19:17,853: t15.2024.07.21 val PER: 0.3669
2026-01-10 08:19:17,853: t15.2024.07.28 val PER: 0.4184
2026-01-10 08:19:17,853: t15.2025.01.10 val PER: 0.6116
2026-01-10 08:19:17,853: t15.2025.01.12 val PER: 0.4396
2026-01-10 08:19:17,853: t15.2025.03.14 val PER: 0.6213
2026-01-10 08:19:17,853: t15.2025.03.16 val PER: 0.4830
2026-01-10 08:19:17,853: t15.2025.03.30 val PER: 0.6977
2026-01-10 08:19:17,853: t15.2025.04.13 val PER: 0.4979
2026-01-10 08:19:17,926: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_4000
2026-01-10 08:21:58,899: Train batch 4200: loss: 41.01 grad norm: 96.36 time: 0.626
2026-01-10 08:24:38,731: Train batch 4400: loss: 51.43 grad norm: 109.28 time: 1.319
2026-01-10 08:25:59,682: Running test after training batch: 4500
2026-01-10 08:25:59,901: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:26:28,994: WER debug example
  GT : you can see the code at this point as well
  PR : luque aunts ease uthe ugh tat stub tis
2026-01-10 08:26:29,024: WER debug example
  GT : how does it keep the cost down
  PR : dudd zitek piecuch thus sta
2026-01-10 08:26:30,604: Val batch 4500: PER (avg): 0.3849 CTC Loss (avg): 35.4160 WER(1gram): 98.22% (n=64) time: 30.922
2026-01-10 08:26:30,604: WER lens: avg_true_words=6.16 avg_pred_words=4.52 max_pred_words=9
2026-01-10 08:26:30,605: t15.2023.08.13 val PER: 0.3420
2026-01-10 08:26:30,605: t15.2023.08.18 val PER: 0.3160
2026-01-10 08:26:30,605: t15.2023.08.20 val PER: 0.3249
2026-01-10 08:26:30,605: t15.2023.08.25 val PER: 0.2711
2026-01-10 08:26:30,605: t15.2023.08.27 val PER: 0.4003
2026-01-10 08:26:30,605: t15.2023.09.01 val PER: 0.2865
2026-01-10 08:26:30,605: t15.2023.09.03 val PER: 0.3848
2026-01-10 08:26:30,605: t15.2023.09.24 val PER: 0.3277
2026-01-10 08:26:30,606: t15.2023.09.29 val PER: 0.3287
2026-01-10 08:26:30,606: t15.2023.10.01 val PER: 0.3672
2026-01-10 08:26:30,606: t15.2023.10.06 val PER: 0.3057
2026-01-10 08:26:30,606: t15.2023.10.08 val PER: 0.3870
2026-01-10 08:26:30,606: t15.2023.10.13 val PER: 0.4151
2026-01-10 08:26:30,606: t15.2023.10.15 val PER: 0.3487
2026-01-10 08:26:30,606: t15.2023.10.20 val PER: 0.3154
2026-01-10 08:26:30,606: t15.2023.10.22 val PER: 0.3096
2026-01-10 08:26:30,606: t15.2023.11.03 val PER: 0.3555
2026-01-10 08:26:30,606: t15.2023.11.04 val PER: 0.1775
2026-01-10 08:26:30,606: t15.2023.11.17 val PER: 0.2457
2026-01-10 08:26:30,607: t15.2023.11.19 val PER: 0.1996
2026-01-10 08:26:30,607: t15.2023.11.26 val PER: 0.4007
2026-01-10 08:26:30,607: t15.2023.12.03 val PER: 0.3571
2026-01-10 08:26:30,607: t15.2023.12.08 val PER: 0.3655
2026-01-10 08:26:30,607: t15.2023.12.10 val PER: 0.3417
2026-01-10 08:26:30,607: t15.2023.12.17 val PER: 0.4200
2026-01-10 08:26:30,607: t15.2023.12.29 val PER: 0.3946
2026-01-10 08:26:30,607: t15.2024.02.25 val PER: 0.3160
2026-01-10 08:26:30,607: t15.2024.03.08 val PER: 0.4651
2026-01-10 08:26:30,607: t15.2024.03.15 val PER: 0.4346
2026-01-10 08:26:30,607: t15.2024.03.17 val PER: 0.3912
2026-01-10 08:26:30,607: t15.2024.05.10 val PER: 0.4175
2026-01-10 08:26:30,608: t15.2024.06.14 val PER: 0.4227
2026-01-10 08:26:30,608: t15.2024.07.19 val PER: 0.5254
2026-01-10 08:26:30,608: t15.2024.07.21 val PER: 0.3517
2026-01-10 08:26:30,608: t15.2024.07.28 val PER: 0.3934
2026-01-10 08:26:30,608: t15.2025.01.10 val PER: 0.5799
2026-01-10 08:26:30,608: t15.2025.01.12 val PER: 0.4265
2026-01-10 08:26:30,608: t15.2025.03.14 val PER: 0.6243
2026-01-10 08:26:30,608: t15.2025.03.16 val PER: 0.4660
2026-01-10 08:26:30,608: t15.2025.03.30 val PER: 0.6402
2026-01-10 08:26:30,609: t15.2025.04.13 val PER: 0.4822
2026-01-10 08:26:30,682: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_4500
2026-01-10 08:27:55,427: Train batch 4600: loss: 28.76 grad norm: 75.70 time: 0.675
2026-01-10 08:30:29,902: Train batch 4800: loss: 35.36 grad norm: 91.13 time: 0.609
2026-01-10 08:33:08,849: Train batch 5000: loss: 36.80 grad norm: 86.30 time: 0.694
2026-01-10 08:33:08,849: Running test after training batch: 5000
2026-01-10 08:33:09,067: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:33:37,800: WER debug example
  GT : you can see the code at this point as well
  PR : utech tux ease uthe uttech tat stub stitzel
2026-01-10 08:33:37,831: WER debug example
  GT : how does it keep the cost down
  PR : dudd zitek piecuch thus stutz
2026-01-10 08:33:39,421: Val batch 5000: PER (avg): 0.3612 CTC Loss (avg): 34.2126 WER(1gram): 98.48% (n=64) time: 30.571
2026-01-10 08:33:39,421: WER lens: avg_true_words=6.16 avg_pred_words=4.70 max_pred_words=10
2026-01-10 08:33:39,421: t15.2023.08.13 val PER: 0.3233
2026-01-10 08:33:39,421: t15.2023.08.18 val PER: 0.2967
2026-01-10 08:33:39,421: t15.2023.08.20 val PER: 0.2859
2026-01-10 08:33:39,421: t15.2023.08.25 val PER: 0.2395
2026-01-10 08:33:39,421: t15.2023.08.27 val PER: 0.3617
2026-01-10 08:33:39,421: t15.2023.09.01 val PER: 0.2484
2026-01-10 08:33:39,422: t15.2023.09.03 val PER: 0.3682
2026-01-10 08:33:39,422: t15.2023.09.24 val PER: 0.2937
2026-01-10 08:33:39,422: t15.2023.09.29 val PER: 0.2967
2026-01-10 08:33:39,422: t15.2023.10.01 val PER: 0.3501
2026-01-10 08:33:39,422: t15.2023.10.06 val PER: 0.2605
2026-01-10 08:33:39,422: t15.2023.10.08 val PER: 0.3735
2026-01-10 08:33:39,422: t15.2023.10.13 val PER: 0.3887
2026-01-10 08:33:39,422: t15.2023.10.15 val PER: 0.3270
2026-01-10 08:33:39,422: t15.2023.10.20 val PER: 0.3188
2026-01-10 08:33:39,422: t15.2023.10.22 val PER: 0.3029
2026-01-10 08:33:39,422: t15.2023.11.03 val PER: 0.3379
2026-01-10 08:33:39,422: t15.2023.11.04 val PER: 0.1502
2026-01-10 08:33:39,422: t15.2023.11.17 val PER: 0.2037
2026-01-10 08:33:39,422: t15.2023.11.19 val PER: 0.1677
2026-01-10 08:33:39,422: t15.2023.11.26 val PER: 0.3855
2026-01-10 08:33:39,423: t15.2023.12.03 val PER: 0.3277
2026-01-10 08:33:39,423: t15.2023.12.08 val PER: 0.3389
2026-01-10 08:33:39,423: t15.2023.12.10 val PER: 0.3127
2026-01-10 08:33:39,423: t15.2023.12.17 val PER: 0.4012
2026-01-10 08:33:39,423: t15.2023.12.29 val PER: 0.3658
2026-01-10 08:33:39,423: t15.2024.02.25 val PER: 0.2963
2026-01-10 08:33:39,423: t15.2024.03.08 val PER: 0.4324
2026-01-10 08:33:39,423: t15.2024.03.15 val PER: 0.4240
2026-01-10 08:33:39,423: t15.2024.03.17 val PER: 0.3759
2026-01-10 08:33:39,423: t15.2024.05.10 val PER: 0.3893
2026-01-10 08:33:39,424: t15.2024.06.14 val PER: 0.3927
2026-01-10 08:33:39,424: t15.2024.07.19 val PER: 0.5056
2026-01-10 08:33:39,424: t15.2024.07.21 val PER: 0.3193
2026-01-10 08:33:39,424: t15.2024.07.28 val PER: 0.3588
2026-01-10 08:33:39,424: t15.2025.01.10 val PER: 0.5579
2026-01-10 08:33:39,424: t15.2025.01.12 val PER: 0.4042
2026-01-10 08:33:39,424: t15.2025.03.14 val PER: 0.6317
2026-01-10 08:33:39,424: t15.2025.03.16 val PER: 0.4686
2026-01-10 08:33:39,424: t15.2025.03.30 val PER: 0.6322
2026-01-10 08:33:39,424: t15.2025.04.13 val PER: 0.4451
2026-01-10 08:33:39,504: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_5000
2026-01-10 08:36:18,863: Train batch 5200: loss: 35.02 grad norm: 80.85 time: 0.658
2026-01-10 08:38:56,251: Train batch 5400: loss: 26.04 grad norm: 83.19 time: 0.672
2026-01-10 08:40:16,993: Running test after training batch: 5500
2026-01-10 08:40:17,225: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:40:46,900: WER debug example
  GT : you can see the code at this point as well
  PR : luque aunts ease uthe uttech tat stub stitzel
2026-01-10 08:40:46,930: WER debug example
  GT : how does it keep the cost down
  PR : diede stitt keep kothe us stutz
2026-01-10 08:40:48,527: Val batch 5500: PER (avg): 0.3445 CTC Loss (avg): 32.8007 WER(1gram): 98.48% (n=64) time: 31.533
2026-01-10 08:40:48,528: WER lens: avg_true_words=6.16 avg_pred_words=4.77 max_pred_words=9
2026-01-10 08:40:48,528: t15.2023.08.13 val PER: 0.3056
2026-01-10 08:40:48,528: t15.2023.08.18 val PER: 0.2842
2026-01-10 08:40:48,528: t15.2023.08.20 val PER: 0.2724
2026-01-10 08:40:48,528: t15.2023.08.25 val PER: 0.2440
2026-01-10 08:40:48,528: t15.2023.08.27 val PER: 0.3617
2026-01-10 08:40:48,528: t15.2023.09.01 val PER: 0.2427
2026-01-10 08:40:48,528: t15.2023.09.03 val PER: 0.3420
2026-01-10 08:40:48,528: t15.2023.09.24 val PER: 0.2816
2026-01-10 08:40:48,528: t15.2023.09.29 val PER: 0.2853
2026-01-10 08:40:48,529: t15.2023.10.01 val PER: 0.3342
2026-01-10 08:40:48,529: t15.2023.10.06 val PER: 0.2530
2026-01-10 08:40:48,529: t15.2023.10.08 val PER: 0.3518
2026-01-10 08:40:48,529: t15.2023.10.13 val PER: 0.3739
2026-01-10 08:40:48,529: t15.2023.10.15 val PER: 0.2953
2026-01-10 08:40:48,529: t15.2023.10.20 val PER: 0.2752
2026-01-10 08:40:48,529: t15.2023.10.22 val PER: 0.2817
2026-01-10 08:40:48,529: t15.2023.11.03 val PER: 0.3209
2026-01-10 08:40:48,529: t15.2023.11.04 val PER: 0.1331
2026-01-10 08:40:48,529: t15.2023.11.17 val PER: 0.1897
2026-01-10 08:40:48,529: t15.2023.11.19 val PER: 0.1677
2026-01-10 08:40:48,529: t15.2023.11.26 val PER: 0.3572
2026-01-10 08:40:48,530: t15.2023.12.03 val PER: 0.3172
2026-01-10 08:40:48,530: t15.2023.12.08 val PER: 0.3209
2026-01-10 08:40:48,530: t15.2023.12.10 val PER: 0.2983
2026-01-10 08:40:48,530: t15.2023.12.17 val PER: 0.3877
2026-01-10 08:40:48,530: t15.2023.12.29 val PER: 0.3555
2026-01-10 08:40:48,530: t15.2024.02.25 val PER: 0.2907
2026-01-10 08:40:48,530: t15.2024.03.08 val PER: 0.4211
2026-01-10 08:40:48,530: t15.2024.03.15 val PER: 0.3965
2026-01-10 08:40:48,530: t15.2024.03.17 val PER: 0.3619
2026-01-10 08:40:48,530: t15.2024.05.10 val PER: 0.3819
2026-01-10 08:40:48,530: t15.2024.06.14 val PER: 0.3707
2026-01-10 08:40:48,530: t15.2024.07.19 val PER: 0.4970
2026-01-10 08:40:48,530: t15.2024.07.21 val PER: 0.3097
2026-01-10 08:40:48,530: t15.2024.07.28 val PER: 0.3493
2026-01-10 08:40:48,530: t15.2025.01.10 val PER: 0.5399
2026-01-10 08:40:48,531: t15.2025.01.12 val PER: 0.3841
2026-01-10 08:40:48,531: t15.2025.03.14 val PER: 0.5577
2026-01-10 08:40:48,531: t15.2025.03.16 val PER: 0.4241
2026-01-10 08:40:48,531: t15.2025.03.30 val PER: 0.6172
2026-01-10 08:40:48,531: t15.2025.04.13 val PER: 0.4223
2026-01-10 08:40:48,604: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_5500
2026-01-10 08:42:09,337: Train batch 5600: loss: 27.80 grad norm: 80.12 time: 0.848
2026-01-10 08:44:44,895: Train batch 5800: loss: 21.17 grad norm: 62.48 time: 0.693
2026-01-10 08:47:24,307: Train batch 6000: loss: 32.21 grad norm: 77.91 time: 0.601
2026-01-10 08:47:24,308: Running test after training batch: 6000
2026-01-10 08:47:24,527: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:47:53,341: WER debug example
  GT : you can see the code at this point as well
  PR : utech tux ease uthe uttech tat stub stitzel
2026-01-10 08:47:53,369: WER debug example
  GT : how does it keep the cost down
  PR : houde seat hitt keep kothe uncut
2026-01-10 08:47:54,960: Val batch 6000: PER (avg): 0.3342 CTC Loss (avg): 31.8600 WER(1gram): 98.48% (n=64) time: 30.650
2026-01-10 08:47:54,960: WER lens: avg_true_words=6.16 avg_pred_words=4.88 max_pred_words=10
2026-01-10 08:47:54,960: t15.2023.08.13 val PER: 0.2827
2026-01-10 08:47:54,960: t15.2023.08.18 val PER: 0.2691
2026-01-10 08:47:54,960: t15.2023.08.20 val PER: 0.2708
2026-01-10 08:47:54,960: t15.2023.08.25 val PER: 0.2259
2026-01-10 08:47:54,960: t15.2023.08.27 val PER: 0.3312
2026-01-10 08:47:54,960: t15.2023.09.01 val PER: 0.2321
2026-01-10 08:47:54,960: t15.2023.09.03 val PER: 0.3432
2026-01-10 08:47:54,961: t15.2023.09.24 val PER: 0.2755
2026-01-10 08:47:54,961: t15.2023.09.29 val PER: 0.2929
2026-01-10 08:47:54,961: t15.2023.10.01 val PER: 0.3131
2026-01-10 08:47:54,961: t15.2023.10.06 val PER: 0.2411
2026-01-10 08:47:54,961: t15.2023.10.08 val PER: 0.3532
2026-01-10 08:47:54,961: t15.2023.10.13 val PER: 0.3654
2026-01-10 08:47:54,961: t15.2023.10.15 val PER: 0.2900
2026-01-10 08:47:54,961: t15.2023.10.20 val PER: 0.3121
2026-01-10 08:47:54,961: t15.2023.10.22 val PER: 0.2684
2026-01-10 08:47:54,961: t15.2023.11.03 val PER: 0.3128
2026-01-10 08:47:54,961: t15.2023.11.04 val PER: 0.1126
2026-01-10 08:47:54,961: t15.2023.11.17 val PER: 0.1820
2026-01-10 08:47:54,961: t15.2023.11.19 val PER: 0.1517
2026-01-10 08:47:54,961: t15.2023.11.26 val PER: 0.3428
2026-01-10 08:47:54,961: t15.2023.12.03 val PER: 0.2973
2026-01-10 08:47:54,961: t15.2023.12.08 val PER: 0.3156
2026-01-10 08:47:54,962: t15.2023.12.10 val PER: 0.2878
2026-01-10 08:47:54,962: t15.2023.12.17 val PER: 0.3763
2026-01-10 08:47:54,962: t15.2023.12.29 val PER: 0.3356
2026-01-10 08:47:54,962: t15.2024.02.25 val PER: 0.2795
2026-01-10 08:47:54,962: t15.2024.03.08 val PER: 0.4154
2026-01-10 08:47:54,962: t15.2024.03.15 val PER: 0.3777
2026-01-10 08:47:54,962: t15.2024.03.17 val PER: 0.3508
2026-01-10 08:47:54,962: t15.2024.05.10 val PER: 0.3611
2026-01-10 08:47:54,962: t15.2024.06.14 val PER: 0.3596
2026-01-10 08:47:54,962: t15.2024.07.19 val PER: 0.4825
2026-01-10 08:47:54,962: t15.2024.07.21 val PER: 0.2897
2026-01-10 08:47:54,962: t15.2024.07.28 val PER: 0.3463
2026-01-10 08:47:54,963: t15.2025.01.10 val PER: 0.5358
2026-01-10 08:47:54,963: t15.2025.01.12 val PER: 0.3680
2026-01-10 08:47:54,963: t15.2025.03.14 val PER: 0.5725
2026-01-10 08:47:54,963: t15.2025.03.16 val PER: 0.4228
2026-01-10 08:47:54,963: t15.2025.03.30 val PER: 0.6138
2026-01-10 08:47:54,963: t15.2025.04.13 val PER: 0.4137
2026-01-10 08:47:55,036: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_6000
2026-01-10 08:50:33,324: Train batch 6200: loss: 25.22 grad norm: 69.35 time: 0.825
2026-01-10 08:53:12,777: Train batch 6400: loss: 23.08 grad norm: 67.34 time: 0.721
2026-01-10 08:54:34,629: Running test after training batch: 6500
2026-01-10 08:54:34,847: WER debug GT example: You can see the code at this point as well.
2026-01-10 08:55:03,643: WER debug example
  GT : you can see the code at this point as well
  PR : ou wruck dietz e uthe uttech tat stub stanza
2026-01-10 08:55:03,673: WER debug example
  GT : how does it keep the cost down
  PR : diede seat hitt heaped uthe us stunt
2026-01-10 08:55:05,292: Val batch 6500: PER (avg): 0.3188 CTC Loss (avg): 30.6522 WER(1gram): 100.00% (n=64) time: 30.661
2026-01-10 08:55:05,292: WER lens: avg_true_words=6.16 avg_pred_words=5.12 max_pred_words=10
2026-01-10 08:55:05,292: t15.2023.08.13 val PER: 0.2869
2026-01-10 08:55:05,292: t15.2023.08.18 val PER: 0.2548
2026-01-10 08:55:05,292: t15.2023.08.20 val PER: 0.2462
2026-01-10 08:55:05,292: t15.2023.08.25 val PER: 0.2123
2026-01-10 08:55:05,293: t15.2023.08.27 val PER: 0.3457
2026-01-10 08:55:05,293: t15.2023.09.01 val PER: 0.2265
2026-01-10 08:55:05,293: t15.2023.09.03 val PER: 0.3219
2026-01-10 08:55:05,293: t15.2023.09.24 val PER: 0.2718
2026-01-10 08:55:05,293: t15.2023.09.29 val PER: 0.2719
2026-01-10 08:55:05,293: t15.2023.10.01 val PER: 0.3137
2026-01-10 08:55:05,293: t15.2023.10.06 val PER: 0.2250
2026-01-10 08:55:05,293: t15.2023.10.08 val PER: 0.3315
2026-01-10 08:55:05,293: t15.2023.10.13 val PER: 0.3530
2026-01-10 08:55:05,293: t15.2023.10.15 val PER: 0.2716
2026-01-10 08:55:05,293: t15.2023.10.20 val PER: 0.2819
2026-01-10 08:55:05,293: t15.2023.10.22 val PER: 0.2528
2026-01-10 08:55:05,294: t15.2023.11.03 val PER: 0.3080
2026-01-10 08:55:05,294: t15.2023.11.04 val PER: 0.1058
2026-01-10 08:55:05,294: t15.2023.11.17 val PER: 0.1804
2026-01-10 08:55:05,294: t15.2023.11.19 val PER: 0.1537
2026-01-10 08:55:05,294: t15.2023.11.26 val PER: 0.3399
2026-01-10 08:55:05,294: t15.2023.12.03 val PER: 0.2731
2026-01-10 08:55:05,294: t15.2023.12.08 val PER: 0.2876
2026-01-10 08:55:05,294: t15.2023.12.10 val PER: 0.2641
2026-01-10 08:55:05,294: t15.2023.12.17 val PER: 0.3628
2026-01-10 08:55:05,294: t15.2023.12.29 val PER: 0.3281
2026-01-10 08:55:05,295: t15.2024.02.25 val PER: 0.2598
2026-01-10 08:55:05,295: t15.2024.03.08 val PER: 0.4068
2026-01-10 08:55:05,295: t15.2024.03.15 val PER: 0.3765
2026-01-10 08:55:05,295: t15.2024.03.17 val PER: 0.3312
2026-01-10 08:55:05,295: t15.2024.05.10 val PER: 0.3343
2026-01-10 08:55:05,295: t15.2024.06.14 val PER: 0.3297
2026-01-10 08:55:05,295: t15.2024.07.19 val PER: 0.4529
2026-01-10 08:55:05,295: t15.2024.07.21 val PER: 0.2731
2026-01-10 08:55:05,295: t15.2024.07.28 val PER: 0.3154
2026-01-10 08:55:05,295: t15.2025.01.10 val PER: 0.5110
2026-01-10 08:55:05,295: t15.2025.01.12 val PER: 0.3587
2026-01-10 08:55:05,295: t15.2025.03.14 val PER: 0.5562
2026-01-10 08:55:05,295: t15.2025.03.16 val PER: 0.3940
2026-01-10 08:55:05,296: t15.2025.03.30 val PER: 0.5552
2026-01-10 08:55:05,296: t15.2025.04.13 val PER: 0.4080
2026-01-10 08:55:05,368: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_6500
2026-01-10 08:56:23,985: Train batch 6600: loss: 31.81 grad norm: 85.82 time: 0.617
2026-01-10 08:59:04,606: Train batch 6800: loss: 31.59 grad norm: 98.61 time: 0.974
2026-01-10 09:01:39,620: Train batch 7000: loss: 38.98 grad norm: 101.26 time: 0.698
2026-01-10 09:01:39,621: Running test after training batch: 7000
2026-01-10 09:01:39,848: WER debug GT example: You can see the code at this point as well.
2026-01-10 09:02:08,426: WER debug example
  GT : you can see the code at this point as well
  PR : luque tux ease uthe uttech dirt atz stub tetzlaff
2026-01-10 09:02:08,457: WER debug example
  GT : how does it keep the cost down
  PR : houde seat hitt keep kothe us stat
2026-01-10 09:02:10,059: Val batch 7000: PER (avg): 0.3014 CTC Loss (avg): 29.7316 WER(1gram): 98.48% (n=64) time: 30.438
2026-01-10 09:02:10,060: WER lens: avg_true_words=6.16 avg_pred_words=5.28 max_pred_words=11
2026-01-10 09:02:10,060: t15.2023.08.13 val PER: 0.2609
2026-01-10 09:02:10,060: t15.2023.08.18 val PER: 0.2364
2026-01-10 09:02:10,060: t15.2023.08.20 val PER: 0.2367
2026-01-10 09:02:10,060: t15.2023.08.25 val PER: 0.1943
2026-01-10 09:02:10,060: t15.2023.08.27 val PER: 0.3232
2026-01-10 09:02:10,060: t15.2023.09.01 val PER: 0.2062
2026-01-10 09:02:10,060: t15.2023.09.03 val PER: 0.3219
2026-01-10 09:02:10,060: t15.2023.09.24 val PER: 0.2476
2026-01-10 09:02:10,060: t15.2023.09.29 val PER: 0.2540
2026-01-10 09:02:10,060: t15.2023.10.01 val PER: 0.2992
2026-01-10 09:02:10,060: t15.2023.10.06 val PER: 0.2164
2026-01-10 09:02:10,061: t15.2023.10.08 val PER: 0.3315
2026-01-10 09:02:10,061: t15.2023.10.13 val PER: 0.3413
2026-01-10 09:02:10,061: t15.2023.10.15 val PER: 0.2577
2026-01-10 09:02:10,061: t15.2023.10.20 val PER: 0.2785
2026-01-10 09:02:10,061: t15.2023.10.22 val PER: 0.2405
2026-01-10 09:02:10,061: t15.2023.11.03 val PER: 0.2917
2026-01-10 09:02:10,061: t15.2023.11.04 val PER: 0.1024
2026-01-10 09:02:10,061: t15.2023.11.17 val PER: 0.1586
2026-01-10 09:02:10,061: t15.2023.11.19 val PER: 0.1437
2026-01-10 09:02:10,061: t15.2023.11.26 val PER: 0.3138
2026-01-10 09:02:10,061: t15.2023.12.03 val PER: 0.2563
2026-01-10 09:02:10,061: t15.2023.12.08 val PER: 0.2763
2026-01-10 09:02:10,061: t15.2023.12.10 val PER: 0.2431
2026-01-10 09:02:10,061: t15.2023.12.17 val PER: 0.3514
2026-01-10 09:02:10,061: t15.2023.12.29 val PER: 0.3191
2026-01-10 09:02:10,062: t15.2024.02.25 val PER: 0.2374
2026-01-10 09:02:10,062: t15.2024.03.08 val PER: 0.3798
2026-01-10 09:02:10,062: t15.2024.03.15 val PER: 0.3527
2026-01-10 09:02:10,062: t15.2024.03.17 val PER: 0.3054
2026-01-10 09:02:10,062: t15.2024.05.10 val PER: 0.3165
2026-01-10 09:02:10,062: t15.2024.06.14 val PER: 0.3123
2026-01-10 09:02:10,062: t15.2024.07.19 val PER: 0.4311
2026-01-10 09:02:10,062: t15.2024.07.21 val PER: 0.2531
2026-01-10 09:02:10,062: t15.2024.07.28 val PER: 0.2971
2026-01-10 09:02:10,062: t15.2025.01.10 val PER: 0.4766
2026-01-10 09:02:10,062: t15.2025.01.12 val PER: 0.3372
2026-01-10 09:02:10,062: t15.2025.03.14 val PER: 0.5222
2026-01-10 09:02:10,062: t15.2025.03.16 val PER: 0.3717
2026-01-10 09:02:10,062: t15.2025.03.30 val PER: 0.5563
2026-01-10 09:02:10,062: t15.2025.04.13 val PER: 0.3809
2026-01-10 09:02:10,135: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_7000
2026-01-10 09:04:50,108: Train batch 7200: loss: 29.99 grad norm: 85.27 time: 0.681
2026-01-10 09:07:30,678: Train batch 7400: loss: 22.40 grad norm: 80.35 time: 0.734
2026-01-10 09:08:48,458: Running test after training batch: 7500
2026-01-10 09:08:48,675: WER debug GT example: You can see the code at this point as well.
2026-01-10 09:09:17,185: WER debug example
  GT : you can see the code at this point as well
  PR : utech tux ease uthe uttech dirt atz stub tis revuelta
2026-01-10 09:09:17,216: WER debug example
  GT : how does it keep the cost down
  PR : houde seat hitt kiki uthe utz stunt
2026-01-10 09:09:18,831: Val batch 7500: PER (avg): 0.2935 CTC Loss (avg): 29.1702 WER(1gram): 99.49% (n=64) time: 30.372
2026-01-10 09:09:18,831: WER lens: avg_true_words=6.16 avg_pred_words=5.34 max_pred_words=11
2026-01-10 09:09:18,831: t15.2023.08.13 val PER: 0.2547
2026-01-10 09:09:18,831: t15.2023.08.18 val PER: 0.2422
2026-01-10 09:09:18,831: t15.2023.08.20 val PER: 0.2208
2026-01-10 09:09:18,831: t15.2023.08.25 val PER: 0.1883
2026-01-10 09:09:18,831: t15.2023.08.27 val PER: 0.3039
2026-01-10 09:09:18,832: t15.2023.09.01 val PER: 0.2021
2026-01-10 09:09:18,832: t15.2023.09.03 val PER: 0.2945
2026-01-10 09:09:18,832: t15.2023.09.24 val PER: 0.2451
2026-01-10 09:09:18,832: t15.2023.09.29 val PER: 0.2387
2026-01-10 09:09:18,832: t15.2023.10.01 val PER: 0.2873
2026-01-10 09:09:18,832: t15.2023.10.06 val PER: 0.2088
2026-01-10 09:09:18,832: t15.2023.10.08 val PER: 0.3139
2026-01-10 09:09:18,832: t15.2023.10.13 val PER: 0.3266
2026-01-10 09:09:18,832: t15.2023.10.15 val PER: 0.2604
2026-01-10 09:09:18,832: t15.2023.10.20 val PER: 0.2416
2026-01-10 09:09:18,832: t15.2023.10.22 val PER: 0.2372
2026-01-10 09:09:18,832: t15.2023.11.03 val PER: 0.3012
2026-01-10 09:09:18,832: t15.2023.11.04 val PER: 0.0922
2026-01-10 09:09:18,833: t15.2023.11.17 val PER: 0.1695
2026-01-10 09:09:18,833: t15.2023.11.19 val PER: 0.1377
2026-01-10 09:09:18,833: t15.2023.11.26 val PER: 0.2986
2026-01-10 09:09:18,833: t15.2023.12.03 val PER: 0.2511
2026-01-10 09:09:18,833: t15.2023.12.08 val PER: 0.2703
2026-01-10 09:09:18,833: t15.2023.12.10 val PER: 0.2378
2026-01-10 09:09:18,833: t15.2023.12.17 val PER: 0.3254
2026-01-10 09:09:18,833: t15.2023.12.29 val PER: 0.3040
2026-01-10 09:09:18,833: t15.2024.02.25 val PER: 0.2346
2026-01-10 09:09:18,833: t15.2024.03.08 val PER: 0.3684
2026-01-10 09:09:18,833: t15.2024.03.15 val PER: 0.3533
2026-01-10 09:09:18,833: t15.2024.03.17 val PER: 0.2950
2026-01-10 09:09:18,833: t15.2024.05.10 val PER: 0.3180
2026-01-10 09:09:18,833: t15.2024.06.14 val PER: 0.3091
2026-01-10 09:09:18,834: t15.2024.07.19 val PER: 0.4252
2026-01-10 09:09:18,834: t15.2024.07.21 val PER: 0.2352
2026-01-10 09:09:18,834: t15.2024.07.28 val PER: 0.2794
2026-01-10 09:09:18,834: t15.2025.01.10 val PER: 0.4821
2026-01-10 09:09:18,834: t15.2025.01.12 val PER: 0.3264
2026-01-10 09:09:18,834: t15.2025.03.14 val PER: 0.5325
2026-01-10 09:09:18,834: t15.2025.03.16 val PER: 0.3796
2026-01-10 09:09:18,834: t15.2025.03.30 val PER: 0.5448
2026-01-10 09:09:18,834: t15.2025.04.13 val PER: 0.3680
2026-01-10 09:09:18,911: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_7500
2026-01-10 09:10:42,075: Train batch 7600: loss: 18.32 grad norm: 65.87 time: 0.838
2026-01-10 09:13:27,152: Train batch 7800: loss: 32.44 grad norm: 98.43 time: 0.784
2026-01-10 09:16:03,290: Train batch 8000: loss: 28.61 grad norm: 79.43 time: 0.903
2026-01-10 09:16:03,295: Running test after training batch: 8000
2026-01-10 09:16:03,519: WER debug GT example: You can see the code at this point as well.
2026-01-10 09:16:32,204: WER debug example
  GT : you can see the code at this point as well
  PR : utech tux ease uthe uttech dirt atz stub tans revuelta
2026-01-10 09:16:32,233: WER debug example
  GT : how does it keep the cost down
  PR : houde stitt kiki uthe amuck studt
2026-01-10 09:16:33,851: Val batch 8000: PER (avg): 0.2806 CTC Loss (avg): 28.1598 WER(1gram): 98.73% (n=64) time: 30.555
2026-01-10 09:16:33,851: WER lens: avg_true_words=6.16 avg_pred_words=5.45 max_pred_words=11
2026-01-10 09:16:33,851: t15.2023.08.13 val PER: 0.2370
2026-01-10 09:16:33,851: t15.2023.08.18 val PER: 0.2213
2026-01-10 09:16:33,851: t15.2023.08.20 val PER: 0.2065
2026-01-10 09:16:33,851: t15.2023.08.25 val PER: 0.1792
2026-01-10 09:16:33,851: t15.2023.08.27 val PER: 0.2878
2026-01-10 09:16:33,852: t15.2023.09.01 val PER: 0.1834
2026-01-10 09:16:33,852: t15.2023.09.03 val PER: 0.2838
2026-01-10 09:16:33,852: t15.2023.09.24 val PER: 0.2269
2026-01-10 09:16:33,852: t15.2023.09.29 val PER: 0.2342
2026-01-10 09:16:33,852: t15.2023.10.01 val PER: 0.2642
2026-01-10 09:16:33,852: t15.2023.10.06 val PER: 0.2002
2026-01-10 09:16:33,852: t15.2023.10.08 val PER: 0.3099
2026-01-10 09:16:33,852: t15.2023.10.13 val PER: 0.3266
2026-01-10 09:16:33,852: t15.2023.10.15 val PER: 0.2505
2026-01-10 09:16:33,852: t15.2023.10.20 val PER: 0.2483
2026-01-10 09:16:33,852: t15.2023.10.22 val PER: 0.2216
2026-01-10 09:16:33,852: t15.2023.11.03 val PER: 0.2795
2026-01-10 09:16:33,852: t15.2023.11.04 val PER: 0.0922
2026-01-10 09:16:33,853: t15.2023.11.17 val PER: 0.1462
2026-01-10 09:16:33,853: t15.2023.11.19 val PER: 0.1277
2026-01-10 09:16:33,853: t15.2023.11.26 val PER: 0.2906
2026-01-10 09:16:33,853: t15.2023.12.03 val PER: 0.2311
2026-01-10 09:16:33,853: t15.2023.12.08 val PER: 0.2583
2026-01-10 09:16:33,853: t15.2023.12.10 val PER: 0.2116
2026-01-10 09:16:33,853: t15.2023.12.17 val PER: 0.3243
2026-01-10 09:16:33,854: t15.2023.12.29 val PER: 0.3047
2026-01-10 09:16:33,854: t15.2024.02.25 val PER: 0.2247
2026-01-10 09:16:33,854: t15.2024.03.08 val PER: 0.3499
2026-01-10 09:16:33,854: t15.2024.03.15 val PER: 0.3390
2026-01-10 09:16:33,854: t15.2024.03.17 val PER: 0.2873
2026-01-10 09:16:33,854: t15.2024.05.10 val PER: 0.3135
2026-01-10 09:16:33,854: t15.2024.06.14 val PER: 0.2981
2026-01-10 09:16:33,854: t15.2024.07.19 val PER: 0.4153
2026-01-10 09:16:33,854: t15.2024.07.21 val PER: 0.2214
2026-01-10 09:16:33,854: t15.2024.07.28 val PER: 0.2684
2026-01-10 09:16:33,854: t15.2025.01.10 val PER: 0.4725
2026-01-10 09:16:33,854: t15.2025.01.12 val PER: 0.3156
2026-01-10 09:16:33,854: t15.2025.03.14 val PER: 0.4956
2026-01-10 09:16:33,854: t15.2025.03.16 val PER: 0.3455
2026-01-10 09:16:33,855: t15.2025.03.30 val PER: 0.5161
2026-01-10 09:16:33,855: t15.2025.04.13 val PER: 0.3666
2026-01-10 09:16:33,928: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_8000
2026-01-10 09:19:13,707: Train batch 8200: loss: 21.30 grad norm: 67.13 time: 0.782
2026-01-10 09:21:49,176: Train batch 8400: loss: 20.45 grad norm: 69.73 time: 0.578
2026-01-10 09:23:10,716: Running test after training batch: 8500
2026-01-10 09:23:10,939: WER debug GT example: You can see the code at this point as well.
2026-01-10 09:23:39,666: WER debug example
  GT : you can see the code at this point as well
  PR : utech tux eat uthe uttech dirt atz stub tao zumwalt
2026-01-10 09:23:39,694: WER debug example
  GT : how does it keep the cost down
  PR : houde seat hitt keep kothe amuck stunt
2026-01-10 09:23:41,293: Val batch 8500: PER (avg): 0.2753 CTC Loss (avg): 27.5944 WER(1gram): 100.00% (n=64) time: 30.576
2026-01-10 09:23:41,294: WER lens: avg_true_words=6.16 avg_pred_words=5.45 max_pred_words=11
2026-01-10 09:23:41,294: t15.2023.08.13 val PER: 0.2318
2026-01-10 09:23:41,294: t15.2023.08.18 val PER: 0.2054
2026-01-10 09:23:41,294: t15.2023.08.20 val PER: 0.1946
2026-01-10 09:23:41,294: t15.2023.08.25 val PER: 0.1747
2026-01-10 09:23:41,294: t15.2023.08.27 val PER: 0.2878
2026-01-10 09:23:41,294: t15.2023.09.01 val PER: 0.1810
2026-01-10 09:23:41,294: t15.2023.09.03 val PER: 0.2838
2026-01-10 09:23:41,295: t15.2023.09.24 val PER: 0.2245
2026-01-10 09:23:41,295: t15.2023.09.29 val PER: 0.2240
2026-01-10 09:23:41,295: t15.2023.10.01 val PER: 0.2748
2026-01-10 09:23:41,295: t15.2023.10.06 val PER: 0.1991
2026-01-10 09:23:41,295: t15.2023.10.08 val PER: 0.2882
2026-01-10 09:23:41,295: t15.2023.10.13 val PER: 0.3126
2026-01-10 09:23:41,295: t15.2023.10.15 val PER: 0.2551
2026-01-10 09:23:41,295: t15.2023.10.20 val PER: 0.2450
2026-01-10 09:23:41,295: t15.2023.10.22 val PER: 0.2205
2026-01-10 09:23:41,295: t15.2023.11.03 val PER: 0.2720
2026-01-10 09:23:41,295: t15.2023.11.04 val PER: 0.0956
2026-01-10 09:23:41,295: t15.2023.11.17 val PER: 0.1586
2026-01-10 09:23:41,295: t15.2023.11.19 val PER: 0.1238
2026-01-10 09:23:41,295: t15.2023.11.26 val PER: 0.2848
2026-01-10 09:23:41,295: t15.2023.12.03 val PER: 0.2332
2026-01-10 09:23:41,295: t15.2023.12.08 val PER: 0.2597
2026-01-10 09:23:41,296: t15.2023.12.10 val PER: 0.2221
2026-01-10 09:23:41,296: t15.2023.12.17 val PER: 0.3160
2026-01-10 09:23:41,296: t15.2023.12.29 val PER: 0.2951
2026-01-10 09:23:41,296: t15.2024.02.25 val PER: 0.2247
2026-01-10 09:23:41,296: t15.2024.03.08 val PER: 0.3499
2026-01-10 09:23:41,296: t15.2024.03.15 val PER: 0.3277
2026-01-10 09:23:41,296: t15.2024.03.17 val PER: 0.2741
2026-01-10 09:23:41,296: t15.2024.05.10 val PER: 0.2942
2026-01-10 09:23:41,296: t15.2024.06.14 val PER: 0.2950
2026-01-10 09:23:41,296: t15.2024.07.19 val PER: 0.4140
2026-01-10 09:23:41,297: t15.2024.07.21 val PER: 0.2138
2026-01-10 09:23:41,297: t15.2024.07.28 val PER: 0.2618
2026-01-10 09:23:41,297: t15.2025.01.10 val PER: 0.4669
2026-01-10 09:23:41,297: t15.2025.01.12 val PER: 0.3056
2026-01-10 09:23:41,297: t15.2025.03.14 val PER: 0.4896
2026-01-10 09:23:41,297: t15.2025.03.16 val PER: 0.3403
2026-01-10 09:23:41,297: t15.2025.03.30 val PER: 0.5000
2026-01-10 09:23:41,297: t15.2025.04.13 val PER: 0.3552
2026-01-10 09:23:41,371: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_8500
2026-01-10 09:25:01,963: Train batch 8600: loss: 27.92 grad norm: 139.86 time: 0.652
2026-01-10 09:27:40,325: Train batch 8800: loss: 25.92 grad norm: 80.92 time: 0.697
2026-01-10 09:30:25,174: Train batch 9000: loss: 30.94 grad norm: 86.55 time: 1.192
2026-01-10 09:30:25,178: Running test after training batch: 9000
2026-01-10 09:30:25,411: WER debug GT example: You can see the code at this point as well.
2026-01-10 09:30:53,933: WER debug example
  GT : you can see the code at this point as well
  PR : utech tux eat uthe amuck dirt atz stub tans revuelta
2026-01-10 09:30:53,962: WER debug example
  GT : how does it keep the cost down
  PR : houde stees hitt keep kothe amuck stunt
2026-01-10 09:30:55,613: Val batch 9000: PER (avg): 0.2697 CTC Loss (avg): 26.7542 WER(1gram): 101.02% (n=64) time: 30.434
2026-01-10 09:30:55,614: WER lens: avg_true_words=6.16 avg_pred_words=5.53 max_pred_words=11
2026-01-10 09:30:55,614: t15.2023.08.13 val PER: 0.2412
2026-01-10 09:30:55,614: t15.2023.08.18 val PER: 0.2037
2026-01-10 09:30:55,614: t15.2023.08.20 val PER: 0.2010
2026-01-10 09:30:55,615: t15.2023.08.25 val PER: 0.1657
2026-01-10 09:30:55,615: t15.2023.08.27 val PER: 0.2830
2026-01-10 09:30:55,615: t15.2023.09.01 val PER: 0.1705
2026-01-10 09:30:55,615: t15.2023.09.03 val PER: 0.2874
2026-01-10 09:30:55,615: t15.2023.09.24 val PER: 0.2087
2026-01-10 09:30:55,615: t15.2023.09.29 val PER: 0.2208
2026-01-10 09:30:55,615: t15.2023.10.01 val PER: 0.2734
2026-01-10 09:30:55,615: t15.2023.10.06 val PER: 0.1884
2026-01-10 09:30:55,615: t15.2023.10.08 val PER: 0.3139
2026-01-10 09:30:55,615: t15.2023.10.13 val PER: 0.3103
2026-01-10 09:30:55,615: t15.2023.10.15 val PER: 0.2498
2026-01-10 09:30:55,615: t15.2023.10.20 val PER: 0.2349
2026-01-10 09:30:55,616: t15.2023.10.22 val PER: 0.2049
2026-01-10 09:30:55,616: t15.2023.11.03 val PER: 0.2653
2026-01-10 09:30:55,616: t15.2023.11.04 val PER: 0.0922
2026-01-10 09:30:55,616: t15.2023.11.17 val PER: 0.1322
2026-01-10 09:30:55,616: t15.2023.11.19 val PER: 0.1297
2026-01-10 09:30:55,616: t15.2023.11.26 val PER: 0.2746
2026-01-10 09:30:55,616: t15.2023.12.03 val PER: 0.2206
2026-01-10 09:30:55,616: t15.2023.12.08 val PER: 0.2450
2026-01-10 09:30:55,616: t15.2023.12.10 val PER: 0.2155
2026-01-10 09:30:55,616: t15.2023.12.17 val PER: 0.3150
2026-01-10 09:30:55,616: t15.2023.12.29 val PER: 0.2807
2026-01-10 09:30:55,616: t15.2024.02.25 val PER: 0.2163
2026-01-10 09:30:55,616: t15.2024.03.08 val PER: 0.3528
2026-01-10 09:30:55,616: t15.2024.03.15 val PER: 0.3302
2026-01-10 09:30:55,616: t15.2024.03.17 val PER: 0.2650
2026-01-10 09:30:55,616: t15.2024.05.10 val PER: 0.2927
2026-01-10 09:30:55,617: t15.2024.06.14 val PER: 0.2871
2026-01-10 09:30:55,617: t15.2024.07.19 val PER: 0.3995
2026-01-10 09:30:55,617: t15.2024.07.21 val PER: 0.2090
2026-01-10 09:30:55,617: t15.2024.07.28 val PER: 0.2507
2026-01-10 09:30:55,617: t15.2025.01.10 val PER: 0.4490
2026-01-10 09:30:55,617: t15.2025.01.12 val PER: 0.2987
2026-01-10 09:30:55,617: t15.2025.03.14 val PER: 0.4793
2026-01-10 09:30:55,617: t15.2025.03.16 val PER: 0.3521
2026-01-10 09:30:55,617: t15.2025.03.30 val PER: 0.4943
2026-01-10 09:30:55,617: t15.2025.04.13 val PER: 0.3552
2026-01-10 09:30:55,691: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_9000
2026-01-10 09:33:39,583: Train batch 9200: loss: 31.67 grad norm: 93.84 time: 0.765
2026-01-10 09:36:20,675: Train batch 9400: loss: 16.91 grad norm: 68.31 time: 0.730
2026-01-10 09:37:40,027: Running test after training batch: 9500
2026-01-10 09:37:40,245: WER debug GT example: You can see the code at this point as well.
2026-01-10 09:38:08,824: WER debug example
  GT : you can see the code at this point as well
  PR : uhlich dietz eat uthe attack dirt atz stub stanza
2026-01-10 09:38:08,852: WER debug example
  GT : how does it keep the cost down
  PR : houde seat hitt keep kothe amuck stutz
2026-01-10 09:38:10,465: Val batch 9500: PER (avg): 0.2681 CTC Loss (avg): 26.5351 WER(1gram): 100.76% (n=64) time: 30.437
2026-01-10 09:38:10,466: WER lens: avg_true_words=6.16 avg_pred_words=5.59 max_pred_words=11
2026-01-10 09:38:10,466: t15.2023.08.13 val PER: 0.2277
2026-01-10 09:38:10,466: t15.2023.08.18 val PER: 0.2137
2026-01-10 09:38:10,466: t15.2023.08.20 val PER: 0.1978
2026-01-10 09:38:10,466: t15.2023.08.25 val PER: 0.1627
2026-01-10 09:38:10,466: t15.2023.08.27 val PER: 0.2974
2026-01-10 09:38:10,466: t15.2023.09.01 val PER: 0.1778
2026-01-10 09:38:10,466: t15.2023.09.03 val PER: 0.2886
2026-01-10 09:38:10,466: t15.2023.09.24 val PER: 0.2112
2026-01-10 09:38:10,467: t15.2023.09.29 val PER: 0.2221
2026-01-10 09:38:10,467: t15.2023.10.01 val PER: 0.2695
2026-01-10 09:38:10,467: t15.2023.10.06 val PER: 0.1916
2026-01-10 09:38:10,467: t15.2023.10.08 val PER: 0.3221
2026-01-10 09:38:10,467: t15.2023.10.13 val PER: 0.2987
2026-01-10 09:38:10,467: t15.2023.10.15 val PER: 0.2479
2026-01-10 09:38:10,467: t15.2023.10.20 val PER: 0.2483
2026-01-10 09:38:10,467: t15.2023.10.22 val PER: 0.2060
2026-01-10 09:38:10,467: t15.2023.11.03 val PER: 0.2653
2026-01-10 09:38:10,467: t15.2023.11.04 val PER: 0.0990
2026-01-10 09:38:10,467: t15.2023.11.17 val PER: 0.1431
2026-01-10 09:38:10,467: t15.2023.11.19 val PER: 0.1058
2026-01-10 09:38:10,467: t15.2023.11.26 val PER: 0.2754
2026-01-10 09:38:10,467: t15.2023.12.03 val PER: 0.2122
2026-01-10 09:38:10,467: t15.2023.12.08 val PER: 0.2410
2026-01-10 09:38:10,467: t15.2023.12.10 val PER: 0.2221
2026-01-10 09:38:10,468: t15.2023.12.17 val PER: 0.3264
2026-01-10 09:38:10,468: t15.2023.12.29 val PER: 0.2752
2026-01-10 09:38:10,468: t15.2024.02.25 val PER: 0.2079
2026-01-10 09:38:10,468: t15.2024.03.08 val PER: 0.3585
2026-01-10 09:38:10,468: t15.2024.03.15 val PER: 0.3296
2026-01-10 09:38:10,468: t15.2024.03.17 val PER: 0.2671
2026-01-10 09:38:10,468: t15.2024.05.10 val PER: 0.2838
2026-01-10 09:38:10,468: t15.2024.06.14 val PER: 0.2792
2026-01-10 09:38:10,468: t15.2024.07.19 val PER: 0.3902
2026-01-10 09:38:10,468: t15.2024.07.21 val PER: 0.2145
2026-01-10 09:38:10,468: t15.2024.07.28 val PER: 0.2537
2026-01-10 09:38:10,468: t15.2025.01.10 val PER: 0.4366
2026-01-10 09:38:10,469: t15.2025.01.12 val PER: 0.2948
2026-01-10 09:38:10,469: t15.2025.03.14 val PER: 0.4675
2026-01-10 09:38:10,469: t15.2025.03.16 val PER: 0.3272
2026-01-10 09:38:10,469: t15.2025.03.30 val PER: 0.4713
2026-01-10 09:38:10,469: t15.2025.04.13 val PER: 0.3666
2026-01-10 09:38:10,541: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_9500
2026-01-10 09:39:30,170: Train batch 9600: loss: 24.25 grad norm: 99.71 time: 0.603
2026-01-10 09:42:08,309: Train batch 9800: loss: 18.63 grad norm: 69.62 time: 0.801
2026-01-10 09:44:52,667: Train batch 10000: loss: 15.96 grad norm: 71.07 time: 0.851
2026-01-10 09:44:52,668: Running test after training batch: 10000
2026-01-10 09:44:52,887: WER debug GT example: You can see the code at this point as well.
2026-01-10 09:45:21,892: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch dietz eat uthe attack dirt atz stub stanza
2026-01-10 09:45:21,920: WER debug example
  GT : how does it keep the cost down
  PR : houde stees hitt keep kothe us stutz
2026-01-10 09:45:23,547: Val batch 10000: PER (avg): 0.2654 CTC Loss (avg): 26.0292 WER(1gram): 101.02% (n=64) time: 30.878
2026-01-10 09:45:23,548: WER lens: avg_true_words=6.16 avg_pred_words=5.64 max_pred_words=11
2026-01-10 09:45:23,548: t15.2023.08.13 val PER: 0.2277
2026-01-10 09:45:23,548: t15.2023.08.18 val PER: 0.2196
2026-01-10 09:45:23,548: t15.2023.08.20 val PER: 0.2017
2026-01-10 09:45:23,548: t15.2023.08.25 val PER: 0.1717
2026-01-10 09:45:23,548: t15.2023.08.27 val PER: 0.2990
2026-01-10 09:45:23,548: t15.2023.09.01 val PER: 0.1761
2026-01-10 09:45:23,548: t15.2023.09.03 val PER: 0.2827
2026-01-10 09:45:23,548: t15.2023.09.24 val PER: 0.2075
2026-01-10 09:45:23,548: t15.2023.09.29 val PER: 0.2138
2026-01-10 09:45:23,548: t15.2023.10.01 val PER: 0.2536
2026-01-10 09:45:23,548: t15.2023.10.06 val PER: 0.1841
2026-01-10 09:45:23,549: t15.2023.10.08 val PER: 0.2950
2026-01-10 09:45:23,549: t15.2023.10.13 val PER: 0.2979
2026-01-10 09:45:23,549: t15.2023.10.15 val PER: 0.2393
2026-01-10 09:45:23,549: t15.2023.10.20 val PER: 0.2483
2026-01-10 09:45:23,549: t15.2023.10.22 val PER: 0.2049
2026-01-10 09:45:23,549: t15.2023.11.03 val PER: 0.2626
2026-01-10 09:45:23,549: t15.2023.11.04 val PER: 0.0887
2026-01-10 09:45:23,549: t15.2023.11.17 val PER: 0.1306
2026-01-10 09:45:23,549: t15.2023.11.19 val PER: 0.1078
2026-01-10 09:45:23,549: t15.2023.11.26 val PER: 0.2761
2026-01-10 09:45:23,549: t15.2023.12.03 val PER: 0.2017
2026-01-10 09:45:23,549: t15.2023.12.08 val PER: 0.2410
2026-01-10 09:45:23,549: t15.2023.12.10 val PER: 0.2194
2026-01-10 09:45:23,549: t15.2023.12.17 val PER: 0.3243
2026-01-10 09:45:23,549: t15.2023.12.29 val PER: 0.2800
2026-01-10 09:45:23,550: t15.2024.02.25 val PER: 0.2065
2026-01-10 09:45:23,550: t15.2024.03.08 val PER: 0.3627
2026-01-10 09:45:23,550: t15.2024.03.15 val PER: 0.3240
2026-01-10 09:45:23,550: t15.2024.03.17 val PER: 0.2713
2026-01-10 09:45:23,550: t15.2024.05.10 val PER: 0.2823
2026-01-10 09:45:23,550: t15.2024.06.14 val PER: 0.2997
2026-01-10 09:45:23,550: t15.2024.07.19 val PER: 0.3929
2026-01-10 09:45:23,550: t15.2024.07.21 val PER: 0.2131
2026-01-10 09:45:23,550: t15.2024.07.28 val PER: 0.2485
2026-01-10 09:45:23,550: t15.2025.01.10 val PER: 0.4284
2026-01-10 09:45:23,550: t15.2025.01.12 val PER: 0.2833
2026-01-10 09:45:23,551: t15.2025.03.14 val PER: 0.4867
2026-01-10 09:45:23,551: t15.2025.03.16 val PER: 0.3246
2026-01-10 09:45:23,551: t15.2025.03.30 val PER: 0.4701
2026-01-10 09:45:23,551: t15.2025.04.13 val PER: 0.3438
2026-01-10 09:45:23,626: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_10000
2026-01-10 09:48:04,099: Train batch 10200: loss: 16.77 grad norm: 68.12 time: 0.784
2026-01-10 09:50:50,271: Train batch 10400: loss: 20.77 grad norm: 74.40 time: 0.843
2026-01-10 09:52:11,469: Running test after training batch: 10500
2026-01-10 09:52:11,691: WER debug GT example: You can see the code at this point as well.
2026-01-10 09:52:40,328: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch dietz eat uthe attack dirt atz stub tetzlaff
2026-01-10 09:52:40,357: WER debug example
  GT : how does it keep the cost down
  PR : houde seat hitt kiki uthe us stunt
2026-01-10 09:52:42,013: Val batch 10500: PER (avg): 0.2527 CTC Loss (avg): 25.3420 WER(1gram): 100.25% (n=64) time: 30.539
2026-01-10 09:52:42,014: WER lens: avg_true_words=6.16 avg_pred_words=5.62 max_pred_words=10
2026-01-10 09:52:42,014: t15.2023.08.13 val PER: 0.2152
2026-01-10 09:52:42,014: t15.2023.08.18 val PER: 0.2062
2026-01-10 09:52:42,014: t15.2023.08.20 val PER: 0.1898
2026-01-10 09:52:42,014: t15.2023.08.25 val PER: 0.1551
2026-01-10 09:52:42,014: t15.2023.08.27 val PER: 0.2685
2026-01-10 09:52:42,014: t15.2023.09.01 val PER: 0.1607
2026-01-10 09:52:42,014: t15.2023.09.03 val PER: 0.2542
2026-01-10 09:52:42,014: t15.2023.09.24 val PER: 0.1966
2026-01-10 09:52:42,014: t15.2023.09.29 val PER: 0.1997
2026-01-10 09:52:42,014: t15.2023.10.01 val PER: 0.2477
2026-01-10 09:52:42,017: t15.2023.10.06 val PER: 0.1658
2026-01-10 09:52:42,018: t15.2023.10.08 val PER: 0.2882
2026-01-10 09:52:42,018: t15.2023.10.13 val PER: 0.2886
2026-01-10 09:52:42,018: t15.2023.10.15 val PER: 0.2367
2026-01-10 09:52:42,018: t15.2023.10.20 val PER: 0.2483
2026-01-10 09:52:42,018: t15.2023.10.22 val PER: 0.1971
2026-01-10 09:52:42,018: t15.2023.11.03 val PER: 0.2551
2026-01-10 09:52:42,018: t15.2023.11.04 val PER: 0.0990
2026-01-10 09:52:42,018: t15.2023.11.17 val PER: 0.1275
2026-01-10 09:52:42,018: t15.2023.11.19 val PER: 0.1078
2026-01-10 09:52:42,018: t15.2023.11.26 val PER: 0.2645
2026-01-10 09:52:42,018: t15.2023.12.03 val PER: 0.2185
2026-01-10 09:52:42,018: t15.2023.12.08 val PER: 0.2270
2026-01-10 09:52:42,018: t15.2023.12.10 val PER: 0.2089
2026-01-10 09:52:42,018: t15.2023.12.17 val PER: 0.3025
2026-01-10 09:52:42,019: t15.2023.12.29 val PER: 0.2560
2026-01-10 09:52:42,019: t15.2024.02.25 val PER: 0.1938
2026-01-10 09:52:42,019: t15.2024.03.08 val PER: 0.3357
2026-01-10 09:52:42,019: t15.2024.03.15 val PER: 0.3152
2026-01-10 09:52:42,019: t15.2024.03.17 val PER: 0.2427
2026-01-10 09:52:42,019: t15.2024.05.10 val PER: 0.2615
2026-01-10 09:52:42,019: t15.2024.06.14 val PER: 0.2744
2026-01-10 09:52:42,019: t15.2024.07.19 val PER: 0.3718
2026-01-10 09:52:42,019: t15.2024.07.21 val PER: 0.1938
2026-01-10 09:52:42,019: t15.2024.07.28 val PER: 0.2375
2026-01-10 09:52:42,019: t15.2025.01.10 val PER: 0.4284
2026-01-10 09:52:42,019: t15.2025.01.12 val PER: 0.2687
2026-01-10 09:52:42,019: t15.2025.03.14 val PER: 0.4689
2026-01-10 09:52:42,019: t15.2025.03.16 val PER: 0.3246
2026-01-10 09:52:42,019: t15.2025.03.30 val PER: 0.4575
2026-01-10 09:52:42,020: t15.2025.04.13 val PER: 0.3466
2026-01-10 09:52:42,093: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_10500
2026-01-10 09:54:02,620: Train batch 10600: loss: 20.65 grad norm: 79.11 time: 0.549
2026-01-10 09:56:44,820: Train batch 10800: loss: 25.21 grad norm: 91.17 time: 1.203
2026-01-10 09:59:23,899: Train batch 11000: loss: 20.05 grad norm: 77.63 time: 1.193
2026-01-10 09:59:23,901: Running test after training batch: 11000
2026-01-10 09:59:24,118: WER debug GT example: You can see the code at this point as well.
2026-01-10 09:59:52,728: WER debug example
  GT : you can see the code at this point as well
  PR : utech aunts eat uthe attack dirt atz stub stanza
2026-01-10 09:59:52,758: WER debug example
  GT : how does it keep the cost down
  PR : outdid unseat hitt kiki uthe amuck stunt
2026-01-10 09:59:54,435: Val batch 11000: PER (avg): 0.2539 CTC Loss (avg): 24.9040 WER(1gram): 99.49% (n=64) time: 30.532
2026-01-10 09:59:54,435: WER lens: avg_true_words=6.16 avg_pred_words=5.62 max_pred_words=11
2026-01-10 09:59:54,435: t15.2023.08.13 val PER: 0.2245
2026-01-10 09:59:54,435: t15.2023.08.18 val PER: 0.2020
2026-01-10 09:59:54,435: t15.2023.08.20 val PER: 0.1938
2026-01-10 09:59:54,436: t15.2023.08.25 val PER: 0.1551
2026-01-10 09:59:54,436: t15.2023.08.27 val PER: 0.2862
2026-01-10 09:59:54,436: t15.2023.09.01 val PER: 0.1607
2026-01-10 09:59:54,436: t15.2023.09.03 val PER: 0.2625
2026-01-10 09:59:54,436: t15.2023.09.24 val PER: 0.2075
2026-01-10 09:59:54,436: t15.2023.09.29 val PER: 0.2004
2026-01-10 09:59:54,436: t15.2023.10.01 val PER: 0.2523
2026-01-10 09:59:54,436: t15.2023.10.06 val PER: 0.1808
2026-01-10 09:59:54,436: t15.2023.10.08 val PER: 0.2882
2026-01-10 09:59:54,436: t15.2023.10.13 val PER: 0.2886
2026-01-10 09:59:54,437: t15.2023.10.15 val PER: 0.2367
2026-01-10 09:59:54,437: t15.2023.10.20 val PER: 0.2282
2026-01-10 09:59:54,437: t15.2023.10.22 val PER: 0.1927
2026-01-10 09:59:54,437: t15.2023.11.03 val PER: 0.2537
2026-01-10 09:59:54,437: t15.2023.11.04 val PER: 0.0922
2026-01-10 09:59:54,437: t15.2023.11.17 val PER: 0.1213
2026-01-10 09:59:54,437: t15.2023.11.19 val PER: 0.1018
2026-01-10 09:59:54,437: t15.2023.11.26 val PER: 0.2645
2026-01-10 09:59:54,437: t15.2023.12.03 val PER: 0.2069
2026-01-10 09:59:54,437: t15.2023.12.08 val PER: 0.2290
2026-01-10 09:59:54,437: t15.2023.12.10 val PER: 0.2011
2026-01-10 09:59:54,437: t15.2023.12.17 val PER: 0.3119
2026-01-10 09:59:54,438: t15.2023.12.29 val PER: 0.2622
2026-01-10 09:59:54,438: t15.2024.02.25 val PER: 0.1966
2026-01-10 09:59:54,438: t15.2024.03.08 val PER: 0.3371
2026-01-10 09:59:54,438: t15.2024.03.15 val PER: 0.3240
2026-01-10 09:59:54,438: t15.2024.03.17 val PER: 0.2524
2026-01-10 09:59:54,438: t15.2024.05.10 val PER: 0.2615
2026-01-10 09:59:54,438: t15.2024.06.14 val PER: 0.2871
2026-01-10 09:59:54,438: t15.2024.07.19 val PER: 0.3705
2026-01-10 09:59:54,438: t15.2024.07.21 val PER: 0.2083
2026-01-10 09:59:54,438: t15.2024.07.28 val PER: 0.2404
2026-01-10 09:59:54,438: t15.2025.01.10 val PER: 0.4242
2026-01-10 09:59:54,439: t15.2025.01.12 val PER: 0.2679
2026-01-10 09:59:54,439: t15.2025.03.14 val PER: 0.4556
2026-01-10 09:59:54,439: t15.2025.03.16 val PER: 0.3076
2026-01-10 09:59:54,439: t15.2025.03.30 val PER: 0.4379
2026-01-10 09:59:54,439: t15.2025.04.13 val PER: 0.3310
2026-01-10 09:59:54,514: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_11000
2026-01-10 10:02:34,659: Train batch 11200: loss: 25.26 grad norm: 86.24 time: 0.877
2026-01-10 10:05:13,176: Train batch 11400: loss: 26.89 grad norm: 93.32 time: 0.659
2026-01-10 10:06:36,309: Running test after training batch: 11500
2026-01-10 10:06:36,526: WER debug GT example: You can see the code at this point as well.
2026-01-10 10:07:05,323: WER debug example
  GT : you can see the code at this point as well
  PR : uhlich aunts eat uthe attack olt at stub stanza
2026-01-10 10:07:05,352: WER debug example
  GT : how does it keep the cost down
  PR : outdid est hitt kiki uthe amuck stunt
2026-01-10 10:07:07,023: Val batch 11500: PER (avg): 0.2493 CTC Loss (avg): 24.5076 WER(1gram): 100.76% (n=64) time: 30.713
2026-01-10 10:07:07,023: WER lens: avg_true_words=6.16 avg_pred_words=5.67 max_pred_words=12
2026-01-10 10:07:07,024: t15.2023.08.13 val PER: 0.2058
2026-01-10 10:07:07,024: t15.2023.08.18 val PER: 0.2028
2026-01-10 10:07:07,024: t15.2023.08.20 val PER: 0.1787
2026-01-10 10:07:07,024: t15.2023.08.25 val PER: 0.1596
2026-01-10 10:07:07,024: t15.2023.08.27 val PER: 0.2701
2026-01-10 10:07:07,024: t15.2023.09.01 val PER: 0.1623
2026-01-10 10:07:07,024: t15.2023.09.03 val PER: 0.2637
2026-01-10 10:07:07,024: t15.2023.09.24 val PER: 0.1881
2026-01-10 10:07:07,024: t15.2023.09.29 val PER: 0.2004
2026-01-10 10:07:07,024: t15.2023.10.01 val PER: 0.2497
2026-01-10 10:07:07,024: t15.2023.10.06 val PER: 0.1733
2026-01-10 10:07:07,024: t15.2023.10.08 val PER: 0.2733
2026-01-10 10:07:07,024: t15.2023.10.13 val PER: 0.2832
2026-01-10 10:07:07,024: t15.2023.10.15 val PER: 0.2320
2026-01-10 10:07:07,024: t15.2023.10.20 val PER: 0.2483
2026-01-10 10:07:07,024: t15.2023.10.22 val PER: 0.1893
2026-01-10 10:07:07,025: t15.2023.11.03 val PER: 0.2544
2026-01-10 10:07:07,025: t15.2023.11.04 val PER: 0.0922
2026-01-10 10:07:07,025: t15.2023.11.17 val PER: 0.1229
2026-01-10 10:07:07,025: t15.2023.11.19 val PER: 0.1158
2026-01-10 10:07:07,025: t15.2023.11.26 val PER: 0.2580
2026-01-10 10:07:07,025: t15.2023.12.03 val PER: 0.2048
2026-01-10 10:07:07,025: t15.2023.12.08 val PER: 0.2164
2026-01-10 10:07:07,025: t15.2023.12.10 val PER: 0.1997
2026-01-10 10:07:07,025: t15.2023.12.17 val PER: 0.3046
2026-01-10 10:07:07,025: t15.2023.12.29 val PER: 0.2567
2026-01-10 10:07:07,025: t15.2024.02.25 val PER: 0.1798
2026-01-10 10:07:07,025: t15.2024.03.08 val PER: 0.3371
2026-01-10 10:07:07,025: t15.2024.03.15 val PER: 0.3152
2026-01-10 10:07:07,025: t15.2024.03.17 val PER: 0.2587
2026-01-10 10:07:07,026: t15.2024.05.10 val PER: 0.2630
2026-01-10 10:07:07,026: t15.2024.06.14 val PER: 0.2713
2026-01-10 10:07:07,026: t15.2024.07.19 val PER: 0.3705
2026-01-10 10:07:07,026: t15.2024.07.21 val PER: 0.2014
2026-01-10 10:07:07,026: t15.2024.07.28 val PER: 0.2331
2026-01-10 10:07:07,026: t15.2025.01.10 val PER: 0.4146
2026-01-10 10:07:07,026: t15.2025.01.12 val PER: 0.2587
2026-01-10 10:07:07,026: t15.2025.03.14 val PER: 0.4601
2026-01-10 10:07:07,026: t15.2025.03.16 val PER: 0.2971
2026-01-10 10:07:07,026: t15.2025.03.30 val PER: 0.4448
2026-01-10 10:07:07,027: t15.2025.04.13 val PER: 0.3295
2026-01-10 10:07:07,098: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_11500
2026-01-10 10:08:26,072: Train batch 11600: loss: 19.18 grad norm: 75.66 time: 0.923
2026-01-10 10:11:08,031: Train batch 11800: loss: 24.59 grad norm: 91.16 time: 0.958
2026-01-10 10:13:48,584: Train batch 12000: loss: 35.92 grad norm: 103.08 time: 0.710
2026-01-10 10:13:48,585: Running test after training batch: 12000
2026-01-10 10:13:48,802: WER debug GT example: You can see the code at this point as well.
2026-01-10 10:14:17,449: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch aunts eton uthe attack dirt at stub tetzlaff
2026-01-10 10:14:17,477: WER debug example
  GT : how does it keep the cost down
  PR : outdid est hitt quay kothe amuck stunt
2026-01-10 10:14:19,126: Val batch 12000: PER (avg): 0.2465 CTC Loss (avg): 24.2596 WER(1gram): 100.00% (n=64) time: 30.539
2026-01-10 10:14:19,126: WER lens: avg_true_words=6.16 avg_pred_words=5.72 max_pred_words=12
2026-01-10 10:14:19,126: t15.2023.08.13 val PER: 0.1975
2026-01-10 10:14:19,126: t15.2023.08.18 val PER: 0.2028
2026-01-10 10:14:19,126: t15.2023.08.20 val PER: 0.1771
2026-01-10 10:14:19,126: t15.2023.08.25 val PER: 0.1416
2026-01-10 10:14:19,126: t15.2023.08.27 val PER: 0.2653
2026-01-10 10:14:19,126: t15.2023.09.01 val PER: 0.1599
2026-01-10 10:14:19,127: t15.2023.09.03 val PER: 0.2542
2026-01-10 10:14:19,127: t15.2023.09.24 val PER: 0.1930
2026-01-10 10:14:19,127: t15.2023.09.29 val PER: 0.1940
2026-01-10 10:14:19,127: t15.2023.10.01 val PER: 0.2417
2026-01-10 10:14:19,127: t15.2023.10.06 val PER: 0.1765
2026-01-10 10:14:19,127: t15.2023.10.08 val PER: 0.2747
2026-01-10 10:14:19,127: t15.2023.10.13 val PER: 0.2863
2026-01-10 10:14:19,127: t15.2023.10.15 val PER: 0.2373
2026-01-10 10:14:19,127: t15.2023.10.20 val PER: 0.2450
2026-01-10 10:14:19,127: t15.2023.10.22 val PER: 0.1915
2026-01-10 10:14:19,127: t15.2023.11.03 val PER: 0.2564
2026-01-10 10:14:19,127: t15.2023.11.04 val PER: 0.0819
2026-01-10 10:14:19,127: t15.2023.11.17 val PER: 0.1151
2026-01-10 10:14:19,128: t15.2023.11.19 val PER: 0.1018
2026-01-10 10:14:19,128: t15.2023.11.26 val PER: 0.2616
2026-01-10 10:14:19,197: t15.2023.12.03 val PER: 0.1943
2026-01-10 10:14:19,198: t15.2023.12.08 val PER: 0.2244
2026-01-10 10:14:19,198: t15.2023.12.10 val PER: 0.1879
2026-01-10 10:14:19,198: t15.2023.12.17 val PER: 0.3035
2026-01-10 10:14:19,198: t15.2023.12.29 val PER: 0.2560
2026-01-10 10:14:19,198: t15.2024.02.25 val PER: 0.1882
2026-01-10 10:14:19,198: t15.2024.03.08 val PER: 0.3044
2026-01-10 10:14:19,198: t15.2024.03.15 val PER: 0.3146
2026-01-10 10:14:19,198: t15.2024.03.17 val PER: 0.2524
2026-01-10 10:14:19,198: t15.2024.05.10 val PER: 0.2645
2026-01-10 10:14:19,198: t15.2024.06.14 val PER: 0.2760
2026-01-10 10:14:19,198: t15.2024.07.19 val PER: 0.3672
2026-01-10 10:14:19,198: t15.2024.07.21 val PER: 0.1924
2026-01-10 10:14:19,198: t15.2024.07.28 val PER: 0.2294
2026-01-10 10:14:19,199: t15.2025.01.10 val PER: 0.4187
2026-01-10 10:14:19,199: t15.2025.01.12 val PER: 0.2625
2026-01-10 10:14:19,199: t15.2025.03.14 val PER: 0.4467
2026-01-10 10:14:19,199: t15.2025.03.16 val PER: 0.2893
2026-01-10 10:14:19,199: t15.2025.03.30 val PER: 0.4345
2026-01-10 10:14:19,199: t15.2025.04.13 val PER: 0.3267
2026-01-10 10:14:19,272: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_12000
2026-01-10 10:17:00,413: Train batch 12200: loss: 29.04 grad norm: 110.78 time: 0.776
2026-01-10 10:19:40,335: Train batch 12400: loss: 25.10 grad norm: 104.15 time: 0.823
2026-01-10 10:21:01,821: Running test after training batch: 12500
2026-01-10 10:21:02,051: WER debug GT example: You can see the code at this point as well.
2026-01-10 10:21:30,590: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch aunts eat uthe attack dirt attics stub stanza
2026-01-10 10:21:30,618: WER debug example
  GT : how does it keep the cost down
  PR : outdid est hitt quay kothe amuck stutz
2026-01-10 10:21:32,289: Val batch 12500: PER (avg): 0.2410 CTC Loss (avg): 24.0048 WER(1gram): 100.00% (n=64) time: 30.467
2026-01-10 10:21:32,289: WER lens: avg_true_words=6.16 avg_pred_words=5.81 max_pred_words=12
2026-01-10 10:21:32,289: t15.2023.08.13 val PER: 0.2017
2026-01-10 10:21:32,289: t15.2023.08.18 val PER: 0.1970
2026-01-10 10:21:32,290: t15.2023.08.20 val PER: 0.1755
2026-01-10 10:21:32,290: t15.2023.08.25 val PER: 0.1506
2026-01-10 10:21:32,290: t15.2023.08.27 val PER: 0.2669
2026-01-10 10:21:32,290: t15.2023.09.01 val PER: 0.1640
2026-01-10 10:21:32,290: t15.2023.09.03 val PER: 0.2506
2026-01-10 10:21:32,290: t15.2023.09.24 val PER: 0.1942
2026-01-10 10:21:32,290: t15.2023.09.29 val PER: 0.1934
2026-01-10 10:21:32,290: t15.2023.10.01 val PER: 0.2325
2026-01-10 10:21:32,290: t15.2023.10.06 val PER: 0.1658
2026-01-10 10:21:32,291: t15.2023.10.08 val PER: 0.2828
2026-01-10 10:21:32,291: t15.2023.10.13 val PER: 0.2708
2026-01-10 10:21:32,291: t15.2023.10.15 val PER: 0.2301
2026-01-10 10:21:32,291: t15.2023.10.20 val PER: 0.2282
2026-01-10 10:21:32,291: t15.2023.10.22 val PER: 0.1860
2026-01-10 10:21:32,291: t15.2023.11.03 val PER: 0.2456
2026-01-10 10:21:32,291: t15.2023.11.04 val PER: 0.0956
2026-01-10 10:21:32,291: t15.2023.11.17 val PER: 0.1166
2026-01-10 10:21:32,291: t15.2023.11.19 val PER: 0.0978
2026-01-10 10:21:32,291: t15.2023.11.26 val PER: 0.2529
2026-01-10 10:21:32,291: t15.2023.12.03 val PER: 0.1975
2026-01-10 10:21:32,292: t15.2023.12.08 val PER: 0.2111
2026-01-10 10:21:32,292: t15.2023.12.10 val PER: 0.1958
2026-01-10 10:21:32,292: t15.2023.12.17 val PER: 0.2931
2026-01-10 10:21:32,292: t15.2023.12.29 val PER: 0.2450
2026-01-10 10:21:32,292: t15.2024.02.25 val PER: 0.1798
2026-01-10 10:21:32,292: t15.2024.03.08 val PER: 0.3243
2026-01-10 10:21:32,292: t15.2024.03.15 val PER: 0.3046
2026-01-10 10:21:32,292: t15.2024.03.17 val PER: 0.2392
2026-01-10 10:21:32,292: t15.2024.05.10 val PER: 0.2467
2026-01-10 10:21:32,292: t15.2024.06.14 val PER: 0.2587
2026-01-10 10:21:32,292: t15.2024.07.19 val PER: 0.3546
2026-01-10 10:21:32,293: t15.2024.07.21 val PER: 0.1821
2026-01-10 10:21:32,293: t15.2024.07.28 val PER: 0.2272
2026-01-10 10:21:32,293: t15.2025.01.10 val PER: 0.3994
2026-01-10 10:21:32,293: t15.2025.01.12 val PER: 0.2587
2026-01-10 10:21:32,293: t15.2025.03.14 val PER: 0.4497
2026-01-10 10:21:32,293: t15.2025.03.16 val PER: 0.2945
2026-01-10 10:21:32,293: t15.2025.03.30 val PER: 0.4241
2026-01-10 10:21:32,293: t15.2025.04.13 val PER: 0.3267
2026-01-10 10:21:32,365: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_12500
2026-01-10 10:22:52,014: Train batch 12600: loss: 21.61 grad norm: 81.14 time: 0.749
2026-01-10 10:25:30,348: Train batch 12800: loss: 23.34 grad norm: 84.70 time: 0.706
2026-01-10 10:28:11,166: Train batch 13000: loss: 31.17 grad norm: 115.99 time: 0.916
2026-01-10 10:28:11,167: Running test after training batch: 13000
2026-01-10 10:28:11,432: WER debug GT example: You can see the code at this point as well.
2026-01-10 10:28:40,137: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch aunts eat uthe attack olt atz stub stanza
2026-01-10 10:28:40,165: WER debug example
  GT : how does it keep the cost down
  PR : houde arzt hitt quay kothe amuck stutz
2026-01-10 10:28:41,846: Val batch 13000: PER (avg): 0.2391 CTC Loss (avg): 23.8463 WER(1gram): 101.52% (n=64) time: 30.677
2026-01-10 10:28:41,846: WER lens: avg_true_words=6.16 avg_pred_words=5.83 max_pred_words=12
2026-01-10 10:28:41,846: t15.2023.08.13 val PER: 0.2006
2026-01-10 10:28:41,846: t15.2023.08.18 val PER: 0.1777
2026-01-10 10:28:41,846: t15.2023.08.20 val PER: 0.1763
2026-01-10 10:28:41,847: t15.2023.08.25 val PER: 0.1491
2026-01-10 10:28:41,847: t15.2023.08.27 val PER: 0.2540
2026-01-10 10:28:41,847: t15.2023.09.01 val PER: 0.1631
2026-01-10 10:28:41,847: t15.2023.09.03 val PER: 0.2589
2026-01-10 10:28:41,847: t15.2023.09.24 val PER: 0.1845
2026-01-10 10:28:41,847: t15.2023.09.29 val PER: 0.1921
2026-01-10 10:28:41,847: t15.2023.10.01 val PER: 0.2338
2026-01-10 10:28:41,847: t15.2023.10.06 val PER: 0.1701
2026-01-10 10:28:41,848: t15.2023.10.08 val PER: 0.2828
2026-01-10 10:28:41,848: t15.2023.10.13 val PER: 0.2731
2026-01-10 10:28:41,848: t15.2023.10.15 val PER: 0.2202
2026-01-10 10:28:41,848: t15.2023.10.20 val PER: 0.2416
2026-01-10 10:28:41,848: t15.2023.10.22 val PER: 0.1737
2026-01-10 10:28:41,848: t15.2023.11.03 val PER: 0.2456
2026-01-10 10:28:41,848: t15.2023.11.04 val PER: 0.0785
2026-01-10 10:28:41,848: t15.2023.11.17 val PER: 0.1042
2026-01-10 10:28:41,848: t15.2023.11.19 val PER: 0.1118
2026-01-10 10:28:41,848: t15.2023.11.26 val PER: 0.2522
2026-01-10 10:28:41,849: t15.2023.12.03 val PER: 0.1901
2026-01-10 10:28:41,849: t15.2023.12.08 val PER: 0.2097
2026-01-10 10:28:41,849: t15.2023.12.10 val PER: 0.1748
2026-01-10 10:28:41,849: t15.2023.12.17 val PER: 0.2890
2026-01-10 10:28:41,849: t15.2023.12.29 val PER: 0.2443
2026-01-10 10:28:41,849: t15.2024.02.25 val PER: 0.1812
2026-01-10 10:28:41,849: t15.2024.03.08 val PER: 0.3115
2026-01-10 10:28:41,849: t15.2024.03.15 val PER: 0.3046
2026-01-10 10:28:41,849: t15.2024.03.17 val PER: 0.2441
2026-01-10 10:28:41,849: t15.2024.05.10 val PER: 0.2467
2026-01-10 10:28:41,849: t15.2024.06.14 val PER: 0.2776
2026-01-10 10:28:41,850: t15.2024.07.19 val PER: 0.3487
2026-01-10 10:28:41,850: t15.2024.07.21 val PER: 0.1841
2026-01-10 10:28:41,850: t15.2024.07.28 val PER: 0.2301
2026-01-10 10:28:41,850: t15.2025.01.10 val PER: 0.4036
2026-01-10 10:28:41,850: t15.2025.01.12 val PER: 0.2517
2026-01-10 10:28:41,850: t15.2025.03.14 val PER: 0.4467
2026-01-10 10:28:41,850: t15.2025.03.16 val PER: 0.2932
2026-01-10 10:28:41,850: t15.2025.03.30 val PER: 0.4287
2026-01-10 10:28:41,850: t15.2025.04.13 val PER: 0.3281
2026-01-10 10:28:41,921: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_13000
2026-01-10 10:31:24,307: Train batch 13200: loss: 33.77 grad norm: 99.41 time: 0.709
2026-01-10 10:34:04,784: Train batch 13400: loss: 14.49 grad norm: 82.36 time: 0.683
2026-01-10 10:35:25,698: Running test after training batch: 13500
2026-01-10 10:35:25,915: WER debug GT example: You can see the code at this point as well.
2026-01-10 10:35:54,455: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch aunts eat uthe attack olt at stub stanza
2026-01-10 10:35:54,484: WER debug example
  GT : how does it keep the cost down
  PR : houde est it quay kothe amuck onstott
2026-01-10 10:35:56,173: Val batch 13500: PER (avg): 0.2375 CTC Loss (avg): 23.5604 WER(1gram): 100.25% (n=64) time: 30.473
2026-01-10 10:35:56,173: WER lens: avg_true_words=6.16 avg_pred_words=5.81 max_pred_words=12
2026-01-10 10:35:56,173: t15.2023.08.13 val PER: 0.1923
2026-01-10 10:35:56,174: t15.2023.08.18 val PER: 0.1802
2026-01-10 10:35:56,174: t15.2023.08.20 val PER: 0.1732
2026-01-10 10:35:56,174: t15.2023.08.25 val PER: 0.1476
2026-01-10 10:35:56,174: t15.2023.08.27 val PER: 0.2749
2026-01-10 10:35:56,174: t15.2023.09.01 val PER: 0.1534
2026-01-10 10:35:56,174: t15.2023.09.03 val PER: 0.2589
2026-01-10 10:35:56,174: t15.2023.09.24 val PER: 0.1954
2026-01-10 10:35:56,174: t15.2023.09.29 val PER: 0.1857
2026-01-10 10:35:56,174: t15.2023.10.01 val PER: 0.2312
2026-01-10 10:35:56,174: t15.2023.10.06 val PER: 0.1647
2026-01-10 10:35:56,174: t15.2023.10.08 val PER: 0.2720
2026-01-10 10:35:56,174: t15.2023.10.13 val PER: 0.2653
2026-01-10 10:35:56,174: t15.2023.10.15 val PER: 0.2221
2026-01-10 10:35:56,174: t15.2023.10.20 val PER: 0.2383
2026-01-10 10:35:56,174: t15.2023.10.22 val PER: 0.1860
2026-01-10 10:35:56,174: t15.2023.11.03 val PER: 0.2442
2026-01-10 10:35:56,175: t15.2023.11.04 val PER: 0.0887
2026-01-10 10:35:56,175: t15.2023.11.17 val PER: 0.1073
2026-01-10 10:35:56,175: t15.2023.11.19 val PER: 0.0978
2026-01-10 10:35:56,175: t15.2023.11.26 val PER: 0.2507
2026-01-10 10:35:56,175: t15.2023.12.03 val PER: 0.1891
2026-01-10 10:35:56,175: t15.2023.12.08 val PER: 0.2077
2026-01-10 10:35:56,175: t15.2023.12.10 val PER: 0.1866
2026-01-10 10:35:56,175: t15.2023.12.17 val PER: 0.2963
2026-01-10 10:35:56,175: t15.2023.12.29 val PER: 0.2416
2026-01-10 10:35:56,176: t15.2024.02.25 val PER: 0.1868
2026-01-10 10:35:56,176: t15.2024.03.08 val PER: 0.3115
2026-01-10 10:35:56,176: t15.2024.03.15 val PER: 0.3096
2026-01-10 10:35:56,176: t15.2024.03.17 val PER: 0.2371
2026-01-10 10:35:56,176: t15.2024.05.10 val PER: 0.2437
2026-01-10 10:35:56,176: t15.2024.06.14 val PER: 0.2650
2026-01-10 10:35:56,176: t15.2024.07.19 val PER: 0.3579
2026-01-10 10:35:56,176: t15.2024.07.21 val PER: 0.1917
2026-01-10 10:35:56,176: t15.2024.07.28 val PER: 0.2199
2026-01-10 10:35:56,176: t15.2025.01.10 val PER: 0.3843
2026-01-10 10:35:56,176: t15.2025.01.12 val PER: 0.2402
2026-01-10 10:35:56,176: t15.2025.03.14 val PER: 0.4467
2026-01-10 10:35:56,176: t15.2025.03.16 val PER: 0.2906
2026-01-10 10:35:56,176: t15.2025.03.30 val PER: 0.4172
2026-01-10 10:35:56,176: t15.2025.04.13 val PER: 0.3210
2026-01-10 10:35:56,249: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_13500
2026-01-10 10:37:18,593: Train batch 13600: loss: 28.61 grad norm: 98.84 time: 1.088
2026-01-10 10:40:04,804: Train batch 13800: loss: 16.40 grad norm: 74.66 time: 0.791
2026-01-10 10:42:46,135: Train batch 14000: loss: 24.36 grad norm: 85.14 time: 0.650
2026-01-10 10:42:46,136: Running test after training batch: 14000
2026-01-10 10:42:46,371: WER debug GT example: You can see the code at this point as well.
2026-01-10 10:43:15,019: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch aunts eat uthe attack dirt atz stub stanza
2026-01-10 10:43:15,047: WER debug example
  GT : how does it keep the cost down
  PR : outed est it quay kothe amuck onstott
2026-01-10 10:43:16,710: Val batch 14000: PER (avg): 0.2373 CTC Loss (avg): 23.5058 WER(1gram): 101.02% (n=64) time: 30.573
2026-01-10 10:43:16,711: WER lens: avg_true_words=6.16 avg_pred_words=5.88 max_pred_words=12
2026-01-10 10:43:16,711: t15.2023.08.13 val PER: 0.1902
2026-01-10 10:43:16,711: t15.2023.08.18 val PER: 0.1945
2026-01-10 10:43:16,711: t15.2023.08.20 val PER: 0.1867
2026-01-10 10:43:16,711: t15.2023.08.25 val PER: 0.1416
2026-01-10 10:43:16,711: t15.2023.08.27 val PER: 0.2572
2026-01-10 10:43:16,711: t15.2023.09.01 val PER: 0.1542
2026-01-10 10:43:16,711: t15.2023.09.03 val PER: 0.2577
2026-01-10 10:43:16,711: t15.2023.09.24 val PER: 0.1833
2026-01-10 10:43:16,711: t15.2023.09.29 val PER: 0.1876
2026-01-10 10:43:16,712: t15.2023.10.01 val PER: 0.2252
2026-01-10 10:43:16,712: t15.2023.10.06 val PER: 0.1668
2026-01-10 10:43:16,712: t15.2023.10.08 val PER: 0.2720
2026-01-10 10:43:16,712: t15.2023.10.13 val PER: 0.2754
2026-01-10 10:43:16,712: t15.2023.10.15 val PER: 0.2254
2026-01-10 10:43:16,712: t15.2023.10.20 val PER: 0.2282
2026-01-10 10:43:16,712: t15.2023.10.22 val PER: 0.1815
2026-01-10 10:43:16,712: t15.2023.11.03 val PER: 0.2429
2026-01-10 10:43:16,712: t15.2023.11.04 val PER: 0.0751
2026-01-10 10:43:16,712: t15.2023.11.17 val PER: 0.1073
2026-01-10 10:43:16,712: t15.2023.11.19 val PER: 0.0918
2026-01-10 10:43:16,712: t15.2023.11.26 val PER: 0.2399
2026-01-10 10:43:16,712: t15.2023.12.03 val PER: 0.1828
2026-01-10 10:43:16,713: t15.2023.12.08 val PER: 0.2091
2026-01-10 10:43:16,713: t15.2023.12.10 val PER: 0.1853
2026-01-10 10:43:16,713: t15.2023.12.17 val PER: 0.2827
2026-01-10 10:43:16,713: t15.2023.12.29 val PER: 0.2450
2026-01-10 10:43:16,713: t15.2024.02.25 val PER: 0.1812
2026-01-10 10:43:16,713: t15.2024.03.08 val PER: 0.3172
2026-01-10 10:43:16,713: t15.2024.03.15 val PER: 0.3152
2026-01-10 10:43:16,713: t15.2024.03.17 val PER: 0.2357
2026-01-10 10:43:16,713: t15.2024.05.10 val PER: 0.2526
2026-01-10 10:43:16,713: t15.2024.06.14 val PER: 0.2429
2026-01-10 10:43:16,713: t15.2024.07.19 val PER: 0.3573
2026-01-10 10:43:16,713: t15.2024.07.21 val PER: 0.1917
2026-01-10 10:43:16,713: t15.2024.07.28 val PER: 0.2228
2026-01-10 10:43:16,713: t15.2025.01.10 val PER: 0.3912
2026-01-10 10:43:16,713: t15.2025.01.12 val PER: 0.2517
2026-01-10 10:43:16,713: t15.2025.03.14 val PER: 0.4334
2026-01-10 10:43:16,714: t15.2025.03.16 val PER: 0.2984
2026-01-10 10:43:16,714: t15.2025.03.30 val PER: 0.4172
2026-01-10 10:43:16,714: t15.2025.04.13 val PER: 0.3167
2026-01-10 10:43:16,787: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_14000
2026-01-10 10:46:01,387: Train batch 14200: loss: 20.27 grad norm: 82.42 time: 0.679
2026-01-10 10:48:42,911: Train batch 14400: loss: 17.64 grad norm: 79.60 time: 0.731
2026-01-10 10:50:04,188: Running test after training batch: 14500
2026-01-10 10:50:04,407: WER debug GT example: You can see the code at this point as well.
2026-01-10 10:50:33,157: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch aunts eat uthe attack olt attics stub stanza
2026-01-10 10:50:33,185: WER debug example
  GT : how does it keep the cost down
  PR : outed unseat it quay kothe amuck onstott
2026-01-10 10:50:34,869: Val batch 14500: PER (avg): 0.2353 CTC Loss (avg): 23.2898 WER(1gram): 101.78% (n=64) time: 30.680
2026-01-10 10:50:34,870: WER lens: avg_true_words=6.16 avg_pred_words=5.89 max_pred_words=12
2026-01-10 10:50:34,870: t15.2023.08.13 val PER: 0.1902
2026-01-10 10:50:34,870: t15.2023.08.18 val PER: 0.1794
2026-01-10 10:50:34,870: t15.2023.08.20 val PER: 0.1755
2026-01-10 10:50:34,870: t15.2023.08.25 val PER: 0.1355
2026-01-10 10:50:34,870: t15.2023.08.27 val PER: 0.2572
2026-01-10 10:50:34,870: t15.2023.09.01 val PER: 0.1583
2026-01-10 10:50:34,870: t15.2023.09.03 val PER: 0.2482
2026-01-10 10:50:34,870: t15.2023.09.24 val PER: 0.1833
2026-01-10 10:50:34,871: t15.2023.09.29 val PER: 0.1844
2026-01-10 10:50:34,871: t15.2023.10.01 val PER: 0.2259
2026-01-10 10:50:34,871: t15.2023.10.06 val PER: 0.1690
2026-01-10 10:50:34,871: t15.2023.10.08 val PER: 0.2747
2026-01-10 10:50:34,871: t15.2023.10.13 val PER: 0.2676
2026-01-10 10:50:34,871: t15.2023.10.15 val PER: 0.2202
2026-01-10 10:50:34,871: t15.2023.10.20 val PER: 0.2282
2026-01-10 10:50:34,871: t15.2023.10.22 val PER: 0.1804
2026-01-10 10:50:34,871: t15.2023.11.03 val PER: 0.2463
2026-01-10 10:50:34,871: t15.2023.11.04 val PER: 0.0717
2026-01-10 10:50:34,871: t15.2023.11.17 val PER: 0.1058
2026-01-10 10:50:34,871: t15.2023.11.19 val PER: 0.0878
2026-01-10 10:50:34,871: t15.2023.11.26 val PER: 0.2442
2026-01-10 10:50:34,871: t15.2023.12.03 val PER: 0.1828
2026-01-10 10:50:34,872: t15.2023.12.08 val PER: 0.2064
2026-01-10 10:50:34,872: t15.2023.12.10 val PER: 0.1840
2026-01-10 10:50:34,872: t15.2023.12.17 val PER: 0.2869
2026-01-10 10:50:34,872: t15.2023.12.29 val PER: 0.2368
2026-01-10 10:50:34,872: t15.2024.02.25 val PER: 0.1713
2026-01-10 10:50:34,872: t15.2024.03.08 val PER: 0.3129
2026-01-10 10:50:34,872: t15.2024.03.15 val PER: 0.3108
2026-01-10 10:50:34,872: t15.2024.03.17 val PER: 0.2350
2026-01-10 10:50:34,872: t15.2024.05.10 val PER: 0.2437
2026-01-10 10:50:34,872: t15.2024.06.14 val PER: 0.2603
2026-01-10 10:50:34,872: t15.2024.07.19 val PER: 0.3566
2026-01-10 10:50:34,872: t15.2024.07.21 val PER: 0.1876
2026-01-10 10:50:34,872: t15.2024.07.28 val PER: 0.2191
2026-01-10 10:50:34,872: t15.2025.01.10 val PER: 0.4022
2026-01-10 10:50:34,872: t15.2025.01.12 val PER: 0.2579
2026-01-10 10:50:34,872: t15.2025.03.14 val PER: 0.4334
2026-01-10 10:50:34,873: t15.2025.03.16 val PER: 0.2932
2026-01-10 10:50:34,873: t15.2025.03.30 val PER: 0.4115
2026-01-10 10:50:34,873: t15.2025.04.13 val PER: 0.3153
2026-01-10 10:50:34,945: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_14500
2026-01-10 10:51:54,828: Train batch 14600: loss: 23.01 grad norm: 94.54 time: 0.638
2026-01-10 10:54:35,151: Train batch 14800: loss: 19.54 grad norm: 90.64 time: 0.736
2026-01-10 10:57:10,889: Train batch 15000: loss: 26.66 grad norm: 93.11 time: 0.625
2026-01-10 10:57:10,893: Running test after training batch: 15000
2026-01-10 10:57:11,121: WER debug GT example: You can see the code at this point as well.
2026-01-10 10:57:39,743: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch aunts eat uthe attack olt at stub stanza
2026-01-10 10:57:39,770: WER debug example
  GT : how does it keep the cost down
  PR : houde unseat it quay kothe amuck onstott
2026-01-10 10:57:41,466: Val batch 15000: PER (avg): 0.2325 CTC Loss (avg): 23.1025 WER(1gram): 101.27% (n=64) time: 30.572
2026-01-10 10:57:41,466: WER lens: avg_true_words=6.16 avg_pred_words=5.84 max_pred_words=12
2026-01-10 10:57:41,466: t15.2023.08.13 val PER: 0.1902
2026-01-10 10:57:41,466: t15.2023.08.18 val PER: 0.1836
2026-01-10 10:57:41,467: t15.2023.08.20 val PER: 0.1716
2026-01-10 10:57:41,467: t15.2023.08.25 val PER: 0.1386
2026-01-10 10:57:41,467: t15.2023.08.27 val PER: 0.2637
2026-01-10 10:57:41,467: t15.2023.09.01 val PER: 0.1477
2026-01-10 10:57:41,467: t15.2023.09.03 val PER: 0.2494
2026-01-10 10:57:41,467: t15.2023.09.24 val PER: 0.1917
2026-01-10 10:57:41,467: t15.2023.09.29 val PER: 0.1832
2026-01-10 10:57:41,467: t15.2023.10.01 val PER: 0.2305
2026-01-10 10:57:41,467: t15.2023.10.06 val PER: 0.1582
2026-01-10 10:57:41,467: t15.2023.10.08 val PER: 0.2598
2026-01-10 10:57:41,467: t15.2023.10.13 val PER: 0.2645
2026-01-10 10:57:41,467: t15.2023.10.15 val PER: 0.2261
2026-01-10 10:57:41,467: t15.2023.10.20 val PER: 0.2483
2026-01-10 10:57:41,467: t15.2023.10.22 val PER: 0.1737
2026-01-10 10:57:41,468: t15.2023.11.03 val PER: 0.2361
2026-01-10 10:57:41,468: t15.2023.11.04 val PER: 0.0648
2026-01-10 10:57:41,468: t15.2023.11.17 val PER: 0.1104
2026-01-10 10:57:41,468: t15.2023.11.19 val PER: 0.0998
2026-01-10 10:57:41,468: t15.2023.11.26 val PER: 0.2362
2026-01-10 10:57:41,468: t15.2023.12.03 val PER: 0.1712
2026-01-10 10:57:41,468: t15.2023.12.08 val PER: 0.2044
2026-01-10 10:57:41,468: t15.2023.12.10 val PER: 0.1761
2026-01-10 10:57:41,468: t15.2023.12.17 val PER: 0.2859
2026-01-10 10:57:41,468: t15.2023.12.29 val PER: 0.2402
2026-01-10 10:57:41,468: t15.2024.02.25 val PER: 0.1699
2026-01-10 10:57:41,468: t15.2024.03.08 val PER: 0.3101
2026-01-10 10:57:41,468: t15.2024.03.15 val PER: 0.3046
2026-01-10 10:57:41,469: t15.2024.03.17 val PER: 0.2315
2026-01-10 10:57:41,469: t15.2024.05.10 val PER: 0.2407
2026-01-10 10:57:41,469: t15.2024.06.14 val PER: 0.2539
2026-01-10 10:57:41,469: t15.2024.07.19 val PER: 0.3441
2026-01-10 10:57:41,469: t15.2024.07.21 val PER: 0.1841
2026-01-10 10:57:41,469: t15.2024.07.28 val PER: 0.2235
2026-01-10 10:57:41,469: t15.2025.01.10 val PER: 0.3953
2026-01-10 10:57:41,469: t15.2025.01.12 val PER: 0.2471
2026-01-10 10:57:41,469: t15.2025.03.14 val PER: 0.4453
2026-01-10 10:57:41,469: t15.2025.03.16 val PER: 0.2984
2026-01-10 10:57:41,469: t15.2025.03.30 val PER: 0.3977
2026-01-10 10:57:41,469: t15.2025.04.13 val PER: 0.3010
2026-01-10 10:57:41,544: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_15000
2026-01-10 11:00:29,155: Train batch 15200: loss: 17.89 grad norm: 85.26 time: 0.715
2026-01-10 11:03:06,646: Train batch 15400: loss: 21.95 grad norm: 84.21 time: 0.787
2026-01-10 11:04:28,816: Running test after training batch: 15500
2026-01-10 11:04:29,045: WER debug GT example: You can see the code at this point as well.
2026-01-10 11:04:57,588: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch aunts eton attack dirt attic stub stanza
2026-01-10 11:04:57,617: WER debug example
  GT : how does it keep the cost down
  PR : houde unseat it quay kothe amuck onstott
2026-01-10 11:04:59,323: Val batch 15500: PER (avg): 0.2324 CTC Loss (avg): 22.9876 WER(1gram): 101.52% (n=64) time: 30.506
2026-01-10 11:04:59,324: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=12
2026-01-10 11:04:59,324: t15.2023.08.13 val PER: 0.1881
2026-01-10 11:04:59,324: t15.2023.08.18 val PER: 0.1794
2026-01-10 11:04:59,324: t15.2023.08.20 val PER: 0.1692
2026-01-10 11:04:59,324: t15.2023.08.25 val PER: 0.1446
2026-01-10 11:04:59,324: t15.2023.08.27 val PER: 0.2508
2026-01-10 11:04:59,324: t15.2023.09.01 val PER: 0.1567
2026-01-10 11:04:59,324: t15.2023.09.03 val PER: 0.2411
2026-01-10 11:04:59,324: t15.2023.09.24 val PER: 0.1881
2026-01-10 11:04:59,324: t15.2023.09.29 val PER: 0.1921
2026-01-10 11:04:59,324: t15.2023.10.01 val PER: 0.2305
2026-01-10 11:04:59,325: t15.2023.10.06 val PER: 0.1636
2026-01-10 11:04:59,325: t15.2023.10.08 val PER: 0.2666
2026-01-10 11:04:59,325: t15.2023.10.13 val PER: 0.2622
2026-01-10 11:04:59,325: t15.2023.10.15 val PER: 0.2116
2026-01-10 11:04:59,325: t15.2023.10.20 val PER: 0.2148
2026-01-10 11:04:59,325: t15.2023.10.22 val PER: 0.1682
2026-01-10 11:04:59,325: t15.2023.11.03 val PER: 0.2300
2026-01-10 11:04:59,325: t15.2023.11.04 val PER: 0.0648
2026-01-10 11:04:59,325: t15.2023.11.17 val PER: 0.1026
2026-01-10 11:04:59,325: t15.2023.11.19 val PER: 0.0978
2026-01-10 11:04:59,325: t15.2023.11.26 val PER: 0.2442
2026-01-10 11:04:59,325: t15.2023.12.03 val PER: 0.1765
2026-01-10 11:04:59,325: t15.2023.12.08 val PER: 0.1964
2026-01-10 11:04:59,325: t15.2023.12.10 val PER: 0.1735
2026-01-10 11:04:59,325: t15.2023.12.17 val PER: 0.2765
2026-01-10 11:04:59,326: t15.2023.12.29 val PER: 0.2334
2026-01-10 11:04:59,326: t15.2024.02.25 val PER: 0.1812
2026-01-10 11:04:59,326: t15.2024.03.08 val PER: 0.3016
2026-01-10 11:04:59,326: t15.2024.03.15 val PER: 0.3064
2026-01-10 11:04:59,326: t15.2024.03.17 val PER: 0.2399
2026-01-10 11:04:59,326: t15.2024.05.10 val PER: 0.2377
2026-01-10 11:04:59,326: t15.2024.06.14 val PER: 0.2587
2026-01-10 11:04:59,326: t15.2024.07.19 val PER: 0.3507
2026-01-10 11:04:59,326: t15.2024.07.21 val PER: 0.1855
2026-01-10 11:04:59,326: t15.2024.07.28 val PER: 0.2228
2026-01-10 11:04:59,327: t15.2025.01.10 val PER: 0.3981
2026-01-10 11:04:59,327: t15.2025.01.12 val PER: 0.2564
2026-01-10 11:04:59,327: t15.2025.03.14 val PER: 0.4364
2026-01-10 11:04:59,327: t15.2025.03.16 val PER: 0.3024
2026-01-10 11:04:59,327: t15.2025.03.30 val PER: 0.4034
2026-01-10 11:04:59,327: t15.2025.04.13 val PER: 0.3195
2026-01-10 11:04:59,402: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_15500
2026-01-10 11:06:22,639: Train batch 15600: loss: 23.55 grad norm: 91.72 time: 0.942
2026-01-10 11:09:05,282: Train batch 15800: loss: 19.79 grad norm: 99.09 time: 0.940
2026-01-10 11:11:47,426: Train batch 16000: loss: 27.21 grad norm: 123.16 time: 1.348
2026-01-10 11:11:47,428: Running test after training batch: 16000
2026-01-10 11:11:47,654: WER debug GT example: You can see the code at this point as well.
2026-01-10 11:12:16,212: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch aunts eton ugh andert atz stub stanza
2026-01-10 11:12:16,241: WER debug example
  GT : how does it keep the cost down
  PR : houde unseat it quay kothe amuck stun
2026-01-10 11:12:17,950: Val batch 16000: PER (avg): 0.2306 CTC Loss (avg): 22.9341 WER(1gram): 101.52% (n=64) time: 30.521
2026-01-10 11:12:17,951: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=12
2026-01-10 11:12:17,951: t15.2023.08.13 val PER: 0.1798
2026-01-10 11:12:17,951: t15.2023.08.18 val PER: 0.1777
2026-01-10 11:12:17,951: t15.2023.08.20 val PER: 0.1652
2026-01-10 11:12:17,951: t15.2023.08.25 val PER: 0.1416
2026-01-10 11:12:17,951: t15.2023.08.27 val PER: 0.2508
2026-01-10 11:12:17,951: t15.2023.09.01 val PER: 0.1518
2026-01-10 11:12:17,951: t15.2023.09.03 val PER: 0.2411
2026-01-10 11:12:17,951: t15.2023.09.24 val PER: 0.1857
2026-01-10 11:12:17,952: t15.2023.09.29 val PER: 0.1825
2026-01-10 11:12:17,952: t15.2023.10.01 val PER: 0.2239
2026-01-10 11:12:17,952: t15.2023.10.06 val PER: 0.1593
2026-01-10 11:12:17,952: t15.2023.10.08 val PER: 0.2774
2026-01-10 11:12:17,952: t15.2023.10.13 val PER: 0.2645
2026-01-10 11:12:17,952: t15.2023.10.15 val PER: 0.2142
2026-01-10 11:12:17,952: t15.2023.10.20 val PER: 0.2383
2026-01-10 11:12:17,952: t15.2023.10.22 val PER: 0.1793
2026-01-10 11:12:17,952: t15.2023.11.03 val PER: 0.2327
2026-01-10 11:12:17,952: t15.2023.11.04 val PER: 0.0751
2026-01-10 11:12:17,952: t15.2023.11.17 val PER: 0.1089
2026-01-10 11:12:17,952: t15.2023.11.19 val PER: 0.0918
2026-01-10 11:12:17,952: t15.2023.11.26 val PER: 0.2384
2026-01-10 11:12:17,953: t15.2023.12.03 val PER: 0.1775
2026-01-10 11:12:17,953: t15.2023.12.08 val PER: 0.1937
2026-01-10 11:12:17,953: t15.2023.12.10 val PER: 0.1787
2026-01-10 11:12:17,953: t15.2023.12.17 val PER: 0.2765
2026-01-10 11:12:17,953: t15.2023.12.29 val PER: 0.2313
2026-01-10 11:12:17,953: t15.2024.02.25 val PER: 0.1826
2026-01-10 11:12:17,953: t15.2024.03.08 val PER: 0.3144
2026-01-10 11:12:17,953: t15.2024.03.15 val PER: 0.2989
2026-01-10 11:12:17,953: t15.2024.03.17 val PER: 0.2273
2026-01-10 11:12:17,954: t15.2024.05.10 val PER: 0.2437
2026-01-10 11:12:17,954: t15.2024.06.14 val PER: 0.2587
2026-01-10 11:12:17,954: t15.2024.07.19 val PER: 0.3448
2026-01-10 11:12:17,954: t15.2024.07.21 val PER: 0.1890
2026-01-10 11:12:17,954: t15.2024.07.28 val PER: 0.2162
2026-01-10 11:12:17,954: t15.2025.01.10 val PER: 0.3926
2026-01-10 11:12:17,954: t15.2025.01.12 val PER: 0.2417
2026-01-10 11:12:17,954: t15.2025.03.14 val PER: 0.4482
2026-01-10 11:12:17,954: t15.2025.03.16 val PER: 0.2958
2026-01-10 11:12:17,954: t15.2025.03.30 val PER: 0.4057
2026-01-10 11:12:17,954: t15.2025.04.13 val PER: 0.3096
2026-01-10 11:12:18,028: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_16000
2026-01-10 11:15:01,492: Train batch 16200: loss: 18.93 grad norm: 84.48 time: 0.707
2026-01-10 11:17:48,841: Train batch 16400: loss: 23.92 grad norm: 84.98 time: 0.796
2026-01-10 11:19:08,663: Running test after training batch: 16500
2026-01-10 11:19:08,883: WER debug GT example: You can see the code at this point as well.
2026-01-10 11:19:37,448: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch aunts eton ugh olt atkin stub stanza
2026-01-10 11:19:37,478: WER debug example
  GT : how does it keep the cost down
  PR : houde unseat it quay kothe amuck stun
2026-01-10 11:19:39,170: Val batch 16500: PER (avg): 0.2291 CTC Loss (avg): 22.7440 WER(1gram): 100.51% (n=64) time: 30.506
2026-01-10 11:19:39,170: WER lens: avg_true_words=6.16 avg_pred_words=5.91 max_pred_words=12
2026-01-10 11:19:39,171: t15.2023.08.13 val PER: 0.1850
2026-01-10 11:19:39,171: t15.2023.08.18 val PER: 0.1811
2026-01-10 11:19:39,171: t15.2023.08.20 val PER: 0.1644
2026-01-10 11:19:39,171: t15.2023.08.25 val PER: 0.1386
2026-01-10 11:19:39,171: t15.2023.08.27 val PER: 0.2476
2026-01-10 11:19:39,171: t15.2023.09.01 val PER: 0.1526
2026-01-10 11:19:39,171: t15.2023.09.03 val PER: 0.2363
2026-01-10 11:19:39,171: t15.2023.09.24 val PER: 0.1820
2026-01-10 11:19:39,171: t15.2023.09.29 val PER: 0.1800
2026-01-10 11:19:39,171: t15.2023.10.01 val PER: 0.2299
2026-01-10 11:19:39,171: t15.2023.10.06 val PER: 0.1582
2026-01-10 11:19:39,171: t15.2023.10.08 val PER: 0.2693
2026-01-10 11:19:39,171: t15.2023.10.13 val PER: 0.2607
2026-01-10 11:19:39,171: t15.2023.10.15 val PER: 0.2090
2026-01-10 11:19:39,172: t15.2023.10.20 val PER: 0.2450
2026-01-10 11:19:39,172: t15.2023.10.22 val PER: 0.1648
2026-01-10 11:19:39,172: t15.2023.11.03 val PER: 0.2388
2026-01-10 11:19:39,172: t15.2023.11.04 val PER: 0.0683
2026-01-10 11:19:39,172: t15.2023.11.17 val PER: 0.1042
2026-01-10 11:19:39,172: t15.2023.11.19 val PER: 0.0958
2026-01-10 11:19:39,172: t15.2023.11.26 val PER: 0.2399
2026-01-10 11:19:39,172: t15.2023.12.03 val PER: 0.1859
2026-01-10 11:19:39,172: t15.2023.12.08 val PER: 0.2031
2026-01-10 11:19:39,172: t15.2023.12.10 val PER: 0.1800
2026-01-10 11:19:39,173: t15.2023.12.17 val PER: 0.2817
2026-01-10 11:19:39,173: t15.2023.12.29 val PER: 0.2244
2026-01-10 11:19:39,173: t15.2024.02.25 val PER: 0.1756
2026-01-10 11:19:39,173: t15.2024.03.08 val PER: 0.3101
2026-01-10 11:19:39,173: t15.2024.03.15 val PER: 0.2983
2026-01-10 11:19:39,173: t15.2024.03.17 val PER: 0.2218
2026-01-10 11:19:39,173: t15.2024.05.10 val PER: 0.2273
2026-01-10 11:19:39,173: t15.2024.06.14 val PER: 0.2555
2026-01-10 11:19:39,173: t15.2024.07.19 val PER: 0.3368
2026-01-10 11:19:39,173: t15.2024.07.21 val PER: 0.1862
2026-01-10 11:19:39,173: t15.2024.07.28 val PER: 0.2191
2026-01-10 11:19:39,173: t15.2025.01.10 val PER: 0.3788
2026-01-10 11:19:39,173: t15.2025.01.12 val PER: 0.2394
2026-01-10 11:19:39,173: t15.2025.03.14 val PER: 0.4453
2026-01-10 11:19:39,173: t15.2025.03.16 val PER: 0.2853
2026-01-10 11:19:39,173: t15.2025.03.30 val PER: 0.3989
2026-01-10 11:19:39,174: t15.2025.04.13 val PER: 0.3281
2026-01-10 11:19:39,248: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_16500
2026-01-10 11:21:00,889: Train batch 16600: loss: 16.04 grad norm: 72.15 time: 0.640
2026-01-10 11:23:42,652: Train batch 16800: loss: 26.45 grad norm: 102.71 time: 0.858
2026-01-10 11:26:22,739: Train batch 17000: loss: 29.46 grad norm: 102.62 time: 0.917
2026-01-10 11:26:22,743: Running test after training batch: 17000
2026-01-10 11:26:22,970: WER debug GT example: You can see the code at this point as well.
2026-01-10 11:26:51,425: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch aunts eton ugh olt atkin stub stanza
2026-01-10 11:26:51,445: WER debug example
  GT : how does it keep the cost down
  PR : houde unseat it quay kothe amuck onstott
2026-01-10 11:26:52,683: Val batch 17000: PER (avg): 0.2287 CTC Loss (avg): 22.7705 WER(1gram): 101.27% (n=64) time: 29.939
2026-01-10 11:26:52,683: WER lens: avg_true_words=6.16 avg_pred_words=5.91 max_pred_words=12
2026-01-10 11:26:52,683: t15.2023.08.13 val PER: 0.1850
2026-01-10 11:26:52,683: t15.2023.08.18 val PER: 0.1802
2026-01-10 11:26:52,683: t15.2023.08.20 val PER: 0.1684
2026-01-10 11:26:52,683: t15.2023.08.25 val PER: 0.1370
2026-01-10 11:26:52,683: t15.2023.08.27 val PER: 0.2444
2026-01-10 11:26:52,683: t15.2023.09.01 val PER: 0.1494
2026-01-10 11:26:52,684: t15.2023.09.03 val PER: 0.2387
2026-01-10 11:26:52,684: t15.2023.09.24 val PER: 0.1808
2026-01-10 11:26:52,684: t15.2023.09.29 val PER: 0.1793
2026-01-10 11:26:52,684: t15.2023.10.01 val PER: 0.2232
2026-01-10 11:26:52,684: t15.2023.10.06 val PER: 0.1539
2026-01-10 11:26:52,684: t15.2023.10.08 val PER: 0.2706
2026-01-10 11:26:52,684: t15.2023.10.13 val PER: 0.2645
2026-01-10 11:26:52,684: t15.2023.10.15 val PER: 0.2156
2026-01-10 11:26:52,684: t15.2023.10.20 val PER: 0.2349
2026-01-10 11:26:52,684: t15.2023.10.22 val PER: 0.1715
2026-01-10 11:26:52,684: t15.2023.11.03 val PER: 0.2388
2026-01-10 11:26:52,684: t15.2023.11.04 val PER: 0.0751
2026-01-10 11:26:52,684: t15.2023.11.17 val PER: 0.1089
2026-01-10 11:26:52,684: t15.2023.11.19 val PER: 0.0918
2026-01-10 11:26:52,684: t15.2023.11.26 val PER: 0.2384
2026-01-10 11:26:52,684: t15.2023.12.03 val PER: 0.1828
2026-01-10 11:26:52,685: t15.2023.12.08 val PER: 0.1944
2026-01-10 11:26:52,685: t15.2023.12.10 val PER: 0.1682
2026-01-10 11:26:52,685: t15.2023.12.17 val PER: 0.2775
2026-01-10 11:26:52,685: t15.2023.12.29 val PER: 0.2306
2026-01-10 11:26:52,685: t15.2024.02.25 val PER: 0.1784
2026-01-10 11:26:52,685: t15.2024.03.08 val PER: 0.3044
2026-01-10 11:26:52,685: t15.2024.03.15 val PER: 0.2933
2026-01-10 11:26:52,685: t15.2024.03.17 val PER: 0.2232
2026-01-10 11:26:52,685: t15.2024.05.10 val PER: 0.2288
2026-01-10 11:26:52,685: t15.2024.06.14 val PER: 0.2492
2026-01-10 11:26:52,685: t15.2024.07.19 val PER: 0.3362
2026-01-10 11:26:52,685: t15.2024.07.21 val PER: 0.1828
2026-01-10 11:26:52,685: t15.2024.07.28 val PER: 0.2125
2026-01-10 11:26:52,685: t15.2025.01.10 val PER: 0.3994
2026-01-10 11:26:52,685: t15.2025.01.12 val PER: 0.2417
2026-01-10 11:26:52,685: t15.2025.03.14 val PER: 0.4541
2026-01-10 11:26:52,686: t15.2025.03.16 val PER: 0.2971
2026-01-10 11:26:52,686: t15.2025.03.30 val PER: 0.3989
2026-01-10 11:26:52,686: t15.2025.04.13 val PER: 0.3181
2026-01-10 11:26:52,760: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_17000
2026-01-10 11:29:34,764: Train batch 17200: loss: 24.22 grad norm: 117.54 time: 0.669
2026-01-10 11:32:11,334: Train batch 17400: loss: 20.62 grad norm: 92.03 time: 0.618
2026-01-10 11:33:30,575: Running test after training batch: 17500
2026-01-10 11:33:30,808: WER debug GT example: You can see the code at this point as well.
2026-01-10 11:33:59,760: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch aunts eton ugh olt attics stub stanza
2026-01-10 11:33:59,782: WER debug example
  GT : how does it keep the cost down
  PR : houde unseat it quay kothe amuck onstott
2026-01-10 11:34:01,146: Val batch 17500: PER (avg): 0.2278 CTC Loss (avg): 22.6777 WER(1gram): 101.02% (n=64) time: 30.570
2026-01-10 11:34:01,146: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=12
2026-01-10 11:34:01,146: t15.2023.08.13 val PER: 0.1819
2026-01-10 11:34:01,147: t15.2023.08.18 val PER: 0.1827
2026-01-10 11:34:01,147: t15.2023.08.20 val PER: 0.1716
2026-01-10 11:34:01,147: t15.2023.08.25 val PER: 0.1386
2026-01-10 11:34:01,147: t15.2023.08.27 val PER: 0.2476
2026-01-10 11:34:01,147: t15.2023.09.01 val PER: 0.1534
2026-01-10 11:34:01,147: t15.2023.09.03 val PER: 0.2447
2026-01-10 11:34:01,147: t15.2023.09.24 val PER: 0.1820
2026-01-10 11:34:01,147: t15.2023.09.29 val PER: 0.1749
2026-01-10 11:34:01,147: t15.2023.10.01 val PER: 0.2252
2026-01-10 11:34:01,147: t15.2023.10.06 val PER: 0.1561
2026-01-10 11:34:01,147: t15.2023.10.08 val PER: 0.2666
2026-01-10 11:34:01,148: t15.2023.10.13 val PER: 0.2669
2026-01-10 11:34:01,148: t15.2023.10.15 val PER: 0.2076
2026-01-10 11:34:01,148: t15.2023.10.20 val PER: 0.2349
2026-01-10 11:34:01,148: t15.2023.10.22 val PER: 0.1804
2026-01-10 11:34:01,148: t15.2023.11.03 val PER: 0.2395
2026-01-10 11:34:01,148: t15.2023.11.04 val PER: 0.0717
2026-01-10 11:34:01,149: t15.2023.11.17 val PER: 0.1011
2026-01-10 11:34:01,149: t15.2023.11.19 val PER: 0.0918
2026-01-10 11:34:01,149: t15.2023.11.26 val PER: 0.2333
2026-01-10 11:34:01,149: t15.2023.12.03 val PER: 0.1754
2026-01-10 11:34:01,149: t15.2023.12.08 val PER: 0.1931
2026-01-10 11:34:01,149: t15.2023.12.10 val PER: 0.1721
2026-01-10 11:34:01,149: t15.2023.12.17 val PER: 0.2703
2026-01-10 11:34:01,149: t15.2023.12.29 val PER: 0.2258
2026-01-10 11:34:01,149: t15.2024.02.25 val PER: 0.1713
2026-01-10 11:34:01,150: t15.2024.03.08 val PER: 0.3016
2026-01-10 11:34:01,150: t15.2024.03.15 val PER: 0.2921
2026-01-10 11:34:01,150: t15.2024.03.17 val PER: 0.2273
2026-01-10 11:34:01,150: t15.2024.05.10 val PER: 0.2318
2026-01-10 11:34:01,150: t15.2024.06.14 val PER: 0.2539
2026-01-10 11:34:01,150: t15.2024.07.19 val PER: 0.3415
2026-01-10 11:34:01,150: t15.2024.07.21 val PER: 0.1862
2026-01-10 11:34:01,150: t15.2024.07.28 val PER: 0.2132
2026-01-10 11:34:01,150: t15.2025.01.10 val PER: 0.3871
2026-01-10 11:34:01,151: t15.2025.01.12 val PER: 0.2294
2026-01-10 11:34:01,151: t15.2025.03.14 val PER: 0.4497
2026-01-10 11:34:01,151: t15.2025.03.16 val PER: 0.2906
2026-01-10 11:34:01,151: t15.2025.03.30 val PER: 0.4011
2026-01-10 11:34:01,151: t15.2025.04.13 val PER: 0.3181
2026-01-10 11:34:01,224: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_17500
2026-01-10 11:35:21,017: Train batch 17600: loss: 23.67 grad norm: 93.39 time: 0.667
2026-01-10 11:38:02,270: Train batch 17800: loss: 22.16 grad norm: 101.29 time: 1.137
2026-01-10 11:40:39,816: Train batch 18000: loss: 28.87 grad norm: 102.01 time: 0.745
2026-01-10 11:40:39,817: Running test after training batch: 18000
2026-01-10 11:40:40,035: WER debug GT example: You can see the code at this point as well.
2026-01-10 11:41:08,456: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch aunts eton ugh olt attics stub stanza
2026-01-10 11:41:08,479: WER debug example
  GT : how does it keep the cost down
  PR : houde unseat it quay kothe amuck stun
2026-01-10 11:41:09,838: Val batch 18000: PER (avg): 0.2275 CTC Loss (avg): 22.6327 WER(1gram): 100.00% (n=64) time: 30.020
2026-01-10 11:41:09,839: WER lens: avg_true_words=6.16 avg_pred_words=5.84 max_pred_words=12
2026-01-10 11:41:09,839: t15.2023.08.13 val PER: 0.1798
2026-01-10 11:41:09,839: t15.2023.08.18 val PER: 0.1769
2026-01-10 11:41:09,839: t15.2023.08.20 val PER: 0.1676
2026-01-10 11:41:09,839: t15.2023.08.25 val PER: 0.1446
2026-01-10 11:41:09,840: t15.2023.08.27 val PER: 0.2363
2026-01-10 11:41:09,840: t15.2023.09.01 val PER: 0.1510
2026-01-10 11:41:09,840: t15.2023.09.03 val PER: 0.2447
2026-01-10 11:41:09,840: t15.2023.09.24 val PER: 0.1857
2026-01-10 11:41:09,840: t15.2023.09.29 val PER: 0.1800
2026-01-10 11:41:09,840: t15.2023.10.01 val PER: 0.2285
2026-01-10 11:41:09,840: t15.2023.10.06 val PER: 0.1604
2026-01-10 11:41:09,840: t15.2023.10.08 val PER: 0.2666
2026-01-10 11:41:09,840: t15.2023.10.13 val PER: 0.2607
2026-01-10 11:41:09,840: t15.2023.10.15 val PER: 0.2129
2026-01-10 11:41:09,841: t15.2023.10.20 val PER: 0.2349
2026-01-10 11:41:09,841: t15.2023.10.22 val PER: 0.1704
2026-01-10 11:41:09,841: t15.2023.11.03 val PER: 0.2300
2026-01-10 11:41:09,841: t15.2023.11.04 val PER: 0.0717
2026-01-10 11:41:09,841: t15.2023.11.17 val PER: 0.1026
2026-01-10 11:41:09,841: t15.2023.11.19 val PER: 0.0978
2026-01-10 11:41:09,841: t15.2023.11.26 val PER: 0.2370
2026-01-10 11:41:09,841: t15.2023.12.03 val PER: 0.1733
2026-01-10 11:41:09,841: t15.2023.12.08 val PER: 0.1944
2026-01-10 11:41:09,841: t15.2023.12.10 val PER: 0.1669
2026-01-10 11:41:09,842: t15.2023.12.17 val PER: 0.2786
2026-01-10 11:41:09,842: t15.2023.12.29 val PER: 0.2272
2026-01-10 11:41:09,842: t15.2024.02.25 val PER: 0.1742
2026-01-10 11:41:09,842: t15.2024.03.08 val PER: 0.3087
2026-01-10 11:41:09,842: t15.2024.03.15 val PER: 0.2989
2026-01-10 11:41:09,842: t15.2024.03.17 val PER: 0.2259
2026-01-10 11:41:09,842: t15.2024.05.10 val PER: 0.2288
2026-01-10 11:41:09,842: t15.2024.06.14 val PER: 0.2492
2026-01-10 11:41:09,842: t15.2024.07.19 val PER: 0.3375
2026-01-10 11:41:09,842: t15.2024.07.21 val PER: 0.1869
2026-01-10 11:41:09,842: t15.2024.07.28 val PER: 0.2103
2026-01-10 11:41:09,843: t15.2025.01.10 val PER: 0.3857
2026-01-10 11:41:09,843: t15.2025.01.12 val PER: 0.2317
2026-01-10 11:41:09,843: t15.2025.03.14 val PER: 0.4453
2026-01-10 11:41:09,843: t15.2025.03.16 val PER: 0.2880
2026-01-10 11:41:09,843: t15.2025.03.30 val PER: 0.3966
2026-01-10 11:41:09,843: t15.2025.04.13 val PER: 0.3153
2026-01-10 11:41:09,925: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_18000
2026-01-10 11:43:47,753: Train batch 18200: loss: 16.97 grad norm: 82.46 time: 0.817
2026-01-10 11:46:27,313: Train batch 18400: loss: 22.92 grad norm: 120.29 time: 0.770
2026-01-10 11:47:46,172: Running test after training batch: 18500
2026-01-10 11:47:46,389: WER debug GT example: You can see the code at this point as well.
2026-01-10 11:48:15,161: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch aunts eton amuck andert atz stub stanza
2026-01-10 11:48:15,184: WER debug example
  GT : how does it keep the cost down
  PR : houde unseat it quay kothe amuck onstott
2026-01-10 11:48:16,545: Val batch 18500: PER (avg): 0.2280 CTC Loss (avg): 22.5926 WER(1gram): 101.52% (n=64) time: 30.372
2026-01-10 11:48:16,546: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=12
2026-01-10 11:48:16,546: t15.2023.08.13 val PER: 0.1850
2026-01-10 11:48:16,546: t15.2023.08.18 val PER: 0.1769
2026-01-10 11:48:16,546: t15.2023.08.20 val PER: 0.1628
2026-01-10 11:48:16,546: t15.2023.08.25 val PER: 0.1431
2026-01-10 11:48:16,546: t15.2023.08.27 val PER: 0.2444
2026-01-10 11:48:16,546: t15.2023.09.01 val PER: 0.1526
2026-01-10 11:48:16,546: t15.2023.09.03 val PER: 0.2470
2026-01-10 11:48:16,547: t15.2023.09.24 val PER: 0.1833
2026-01-10 11:48:16,547: t15.2023.09.29 val PER: 0.1793
2026-01-10 11:48:16,547: t15.2023.10.01 val PER: 0.2259
2026-01-10 11:48:16,547: t15.2023.10.06 val PER: 0.1604
2026-01-10 11:48:16,547: t15.2023.10.08 val PER: 0.2720
2026-01-10 11:48:16,547: t15.2023.10.13 val PER: 0.2661
2026-01-10 11:48:16,547: t15.2023.10.15 val PER: 0.2070
2026-01-10 11:48:16,547: t15.2023.10.20 val PER: 0.2383
2026-01-10 11:48:16,547: t15.2023.10.22 val PER: 0.1782
2026-01-10 11:48:16,547: t15.2023.11.03 val PER: 0.2361
2026-01-10 11:48:16,547: t15.2023.11.04 val PER: 0.0751
2026-01-10 11:48:16,547: t15.2023.11.17 val PER: 0.1011
2026-01-10 11:48:16,547: t15.2023.11.19 val PER: 0.0978
2026-01-10 11:48:16,547: t15.2023.11.26 val PER: 0.2341
2026-01-10 11:48:16,548: t15.2023.12.03 val PER: 0.1754
2026-01-10 11:48:16,548: t15.2023.12.08 val PER: 0.1957
2026-01-10 11:48:16,548: t15.2023.12.10 val PER: 0.1761
2026-01-10 11:48:16,548: t15.2023.12.17 val PER: 0.2734
2026-01-10 11:48:16,548: t15.2023.12.29 val PER: 0.2244
2026-01-10 11:48:16,548: t15.2024.02.25 val PER: 0.1742
2026-01-10 11:48:16,548: t15.2024.03.08 val PER: 0.3087
2026-01-10 11:48:16,548: t15.2024.03.15 val PER: 0.2996
2026-01-10 11:48:16,548: t15.2024.03.17 val PER: 0.2211
2026-01-10 11:48:16,548: t15.2024.05.10 val PER: 0.2259
2026-01-10 11:48:16,549: t15.2024.06.14 val PER: 0.2492
2026-01-10 11:48:16,549: t15.2024.07.19 val PER: 0.3368
2026-01-10 11:48:16,549: t15.2024.07.21 val PER: 0.1883
2026-01-10 11:48:16,549: t15.2024.07.28 val PER: 0.2140
2026-01-10 11:48:16,549: t15.2025.01.10 val PER: 0.3829
2026-01-10 11:48:16,549: t15.2025.01.12 val PER: 0.2379
2026-01-10 11:48:16,549: t15.2025.03.14 val PER: 0.4453
2026-01-10 11:48:16,549: t15.2025.03.16 val PER: 0.2788
2026-01-10 11:48:16,549: t15.2025.03.30 val PER: 0.4011
2026-01-10 11:48:16,549: t15.2025.04.13 val PER: 0.3181
2026-01-10 11:48:16,625: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_18500
2026-01-10 11:49:36,083: Train batch 18600: loss: 20.12 grad norm: 116.63 time: 1.172
2026-01-10 11:52:13,096: Train batch 18800: loss: 22.32 grad norm: 98.75 time: 0.681
2026-01-10 11:54:52,288: Train batch 19000: loss: 20.20 grad norm: 89.47 time: 0.893
2026-01-10 11:54:52,289: Running test after training batch: 19000
2026-01-10 11:54:52,507: WER debug GT example: You can see the code at this point as well.
2026-01-10 11:55:21,335: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch aunts eton amuck andert atz stub stanza
2026-01-10 11:55:21,357: WER debug example
  GT : how does it keep the cost down
  PR : houde unseat it quay kothe amuck onstott
2026-01-10 11:55:22,719: Val batch 19000: PER (avg): 0.2266 CTC Loss (avg): 22.5179 WER(1gram): 101.78% (n=64) time: 30.429
2026-01-10 11:55:22,720: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=12
2026-01-10 11:55:22,720: t15.2023.08.13 val PER: 0.1788
2026-01-10 11:55:22,720: t15.2023.08.18 val PER: 0.1794
2026-01-10 11:55:22,720: t15.2023.08.20 val PER: 0.1612
2026-01-10 11:55:22,720: t15.2023.08.25 val PER: 0.1386
2026-01-10 11:55:22,720: t15.2023.08.27 val PER: 0.2428
2026-01-10 11:55:22,720: t15.2023.09.01 val PER: 0.1485
2026-01-10 11:55:22,720: t15.2023.09.03 val PER: 0.2411
2026-01-10 11:55:22,720: t15.2023.09.24 val PER: 0.1784
2026-01-10 11:55:22,720: t15.2023.09.29 val PER: 0.1812
2026-01-10 11:55:22,720: t15.2023.10.01 val PER: 0.2259
2026-01-10 11:55:22,720: t15.2023.10.06 val PER: 0.1561
2026-01-10 11:55:22,721: t15.2023.10.08 val PER: 0.2720
2026-01-10 11:55:22,721: t15.2023.10.13 val PER: 0.2599
2026-01-10 11:55:22,721: t15.2023.10.15 val PER: 0.2070
2026-01-10 11:55:22,721: t15.2023.10.20 val PER: 0.2248
2026-01-10 11:55:22,721: t15.2023.10.22 val PER: 0.1759
2026-01-10 11:55:22,721: t15.2023.11.03 val PER: 0.2402
2026-01-10 11:55:22,721: t15.2023.11.04 val PER: 0.0785
2026-01-10 11:55:22,721: t15.2023.11.17 val PER: 0.1011
2026-01-10 11:55:22,721: t15.2023.11.19 val PER: 0.0958
2026-01-10 11:55:22,721: t15.2023.11.26 val PER: 0.2312
2026-01-10 11:55:22,721: t15.2023.12.03 val PER: 0.1754
2026-01-10 11:55:22,721: t15.2023.12.08 val PER: 0.1951
2026-01-10 11:55:22,721: t15.2023.12.10 val PER: 0.1708
2026-01-10 11:55:22,721: t15.2023.12.17 val PER: 0.2765
2026-01-10 11:55:22,722: t15.2023.12.29 val PER: 0.2237
2026-01-10 11:55:22,722: t15.2024.02.25 val PER: 0.1671
2026-01-10 11:55:22,722: t15.2024.03.08 val PER: 0.3073
2026-01-10 11:55:22,722: t15.2024.03.15 val PER: 0.2964
2026-01-10 11:55:22,722: t15.2024.03.17 val PER: 0.2238
2026-01-10 11:55:22,722: t15.2024.05.10 val PER: 0.2229
2026-01-10 11:55:22,722: t15.2024.06.14 val PER: 0.2508
2026-01-10 11:55:22,722: t15.2024.07.19 val PER: 0.3368
2026-01-10 11:55:22,722: t15.2024.07.21 val PER: 0.1848
2026-01-10 11:55:22,722: t15.2024.07.28 val PER: 0.2110
2026-01-10 11:55:22,722: t15.2025.01.10 val PER: 0.3898
2026-01-10 11:55:22,722: t15.2025.01.12 val PER: 0.2356
2026-01-10 11:55:22,722: t15.2025.03.14 val PER: 0.4453
2026-01-10 11:55:22,722: t15.2025.03.16 val PER: 0.2853
2026-01-10 11:55:22,723: t15.2025.03.30 val PER: 0.4023
2026-01-10 11:55:22,723: t15.2025.04.13 val PER: 0.3081
2026-01-10 11:55:22,801: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_19000
2026-01-10 11:58:02,701: Train batch 19200: loss: 21.25 grad norm: 81.23 time: 0.589
2026-01-10 12:00:42,523: Train batch 19400: loss: 23.53 grad norm: 96.06 time: 0.760
2026-01-10 12:02:02,857: Running test after training batch: 19500
2026-01-10 12:02:03,075: WER debug GT example: You can see the code at this point as well.
2026-01-10 12:02:31,444: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch aunts eton amuck andert atz stub stanza
2026-01-10 12:02:31,467: WER debug example
  GT : how does it keep the cost down
  PR : houde unseat it quay kothe amuck onstott
2026-01-10 12:02:32,837: Val batch 19500: PER (avg): 0.2262 CTC Loss (avg): 22.5170 WER(1gram): 101.52% (n=64) time: 29.979
2026-01-10 12:02:32,837: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=12
2026-01-10 12:02:32,838: t15.2023.08.13 val PER: 0.1809
2026-01-10 12:02:32,838: t15.2023.08.18 val PER: 0.1777
2026-01-10 12:02:32,838: t15.2023.08.20 val PER: 0.1636
2026-01-10 12:02:32,838: t15.2023.08.25 val PER: 0.1370
2026-01-10 12:02:32,838: t15.2023.08.27 val PER: 0.2524
2026-01-10 12:02:32,838: t15.2023.09.01 val PER: 0.1477
2026-01-10 12:02:32,838: t15.2023.09.03 val PER: 0.2435
2026-01-10 12:02:32,838: t15.2023.09.24 val PER: 0.1772
2026-01-10 12:02:32,838: t15.2023.09.29 val PER: 0.1787
2026-01-10 12:02:32,839: t15.2023.10.01 val PER: 0.2259
2026-01-10 12:02:32,839: t15.2023.10.06 val PER: 0.1518
2026-01-10 12:02:32,839: t15.2023.10.08 val PER: 0.2666
2026-01-10 12:02:32,839: t15.2023.10.13 val PER: 0.2622
2026-01-10 12:02:32,839: t15.2023.10.15 val PER: 0.2083
2026-01-10 12:02:32,839: t15.2023.10.20 val PER: 0.2383
2026-01-10 12:02:32,839: t15.2023.10.22 val PER: 0.1771
2026-01-10 12:02:32,839: t15.2023.11.03 val PER: 0.2388
2026-01-10 12:02:32,839: t15.2023.11.04 val PER: 0.0683
2026-01-10 12:02:32,839: t15.2023.11.17 val PER: 0.1011
2026-01-10 12:02:32,839: t15.2023.11.19 val PER: 0.0938
2026-01-10 12:02:32,840: t15.2023.11.26 val PER: 0.2304
2026-01-10 12:02:32,840: t15.2023.12.03 val PER: 0.1786
2026-01-10 12:02:32,840: t15.2023.12.08 val PER: 0.1944
2026-01-10 12:02:32,840: t15.2023.12.10 val PER: 0.1695
2026-01-10 12:02:32,840: t15.2023.12.17 val PER: 0.2807
2026-01-10 12:02:32,840: t15.2023.12.29 val PER: 0.2244
2026-01-10 12:02:32,840: t15.2024.02.25 val PER: 0.1756
2026-01-10 12:02:32,840: t15.2024.03.08 val PER: 0.3016
2026-01-10 12:02:32,840: t15.2024.03.15 val PER: 0.2958
2026-01-10 12:02:32,840: t15.2024.03.17 val PER: 0.2218
2026-01-10 12:02:32,841: t15.2024.05.10 val PER: 0.2244
2026-01-10 12:02:32,841: t15.2024.06.14 val PER: 0.2508
2026-01-10 12:02:32,841: t15.2024.07.19 val PER: 0.3355
2026-01-10 12:02:32,841: t15.2024.07.21 val PER: 0.1800
2026-01-10 12:02:32,841: t15.2024.07.28 val PER: 0.2118
2026-01-10 12:02:32,841: t15.2025.01.10 val PER: 0.3857
2026-01-10 12:02:32,841: t15.2025.01.12 val PER: 0.2333
2026-01-10 12:02:32,841: t15.2025.03.14 val PER: 0.4482
2026-01-10 12:02:32,841: t15.2025.03.16 val PER: 0.2814
2026-01-10 12:02:32,841: t15.2025.03.30 val PER: 0.3931
2026-01-10 12:02:32,841: t15.2025.04.13 val PER: 0.3110
2026-01-10 12:02:32,919: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_19500
2026-01-10 12:03:56,134: Train batch 19600: loss: 28.27 grad norm: 107.34 time: 0.752
2026-01-10 12:06:38,195: Train batch 19800: loss: 17.70 grad norm: 77.14 time: 0.828
2026-01-10 12:09:22,045: Running test after training batch: 19999
2026-01-10 12:09:22,267: WER debug GT example: You can see the code at this point as well.
2026-01-10 12:09:50,768: WER debug example
  GT : you can see the code at this point as well
  PR : eunuch aunts eton amuck olt atz stub stanza
2026-01-10 12:09:50,791: WER debug example
  GT : how does it keep the cost down
  PR : houde unseat it quay kothe amuck onstott
2026-01-10 12:09:52,187: Val batch 19999: PER (avg): 0.2253 CTC Loss (avg): 22.4830 WER(1gram): 100.76% (n=64) time: 30.139
2026-01-10 12:09:52,187: WER lens: avg_true_words=6.16 avg_pred_words=5.88 max_pred_words=12
2026-01-10 12:09:52,187: t15.2023.08.13 val PER: 0.1757
2026-01-10 12:09:52,187: t15.2023.08.18 val PER: 0.1752
2026-01-10 12:09:52,188: t15.2023.08.20 val PER: 0.1636
2026-01-10 12:09:52,188: t15.2023.08.25 val PER: 0.1370
2026-01-10 12:09:52,188: t15.2023.08.27 val PER: 0.2508
2026-01-10 12:09:52,188: t15.2023.09.01 val PER: 0.1469
2026-01-10 12:09:52,188: t15.2023.09.03 val PER: 0.2375
2026-01-10 12:09:52,188: t15.2023.09.24 val PER: 0.1760
2026-01-10 12:09:52,188: t15.2023.09.29 val PER: 0.1800
2026-01-10 12:09:52,189: t15.2023.10.01 val PER: 0.2226
2026-01-10 12:09:52,189: t15.2023.10.06 val PER: 0.1550
2026-01-10 12:09:52,189: t15.2023.10.08 val PER: 0.2679
2026-01-10 12:09:52,189: t15.2023.10.13 val PER: 0.2568
2026-01-10 12:09:52,189: t15.2023.10.15 val PER: 0.2123
2026-01-10 12:09:52,189: t15.2023.10.20 val PER: 0.2383
2026-01-10 12:09:52,189: t15.2023.10.22 val PER: 0.1759
2026-01-10 12:09:52,189: t15.2023.11.03 val PER: 0.2327
2026-01-10 12:09:52,189: t15.2023.11.04 val PER: 0.0717
2026-01-10 12:09:52,190: t15.2023.11.17 val PER: 0.0995
2026-01-10 12:09:52,190: t15.2023.11.19 val PER: 0.0918
2026-01-10 12:09:52,190: t15.2023.11.26 val PER: 0.2312
2026-01-10 12:09:52,190: t15.2023.12.03 val PER: 0.1733
2026-01-10 12:09:52,190: t15.2023.12.08 val PER: 0.1924
2026-01-10 12:09:52,190: t15.2023.12.10 val PER: 0.1721
2026-01-10 12:09:52,190: t15.2023.12.17 val PER: 0.2744
2026-01-10 12:09:52,190: t15.2023.12.29 val PER: 0.2244
2026-01-10 12:09:52,190: t15.2024.02.25 val PER: 0.1685
2026-01-10 12:09:52,190: t15.2024.03.08 val PER: 0.2987
2026-01-10 12:09:52,190: t15.2024.03.15 val PER: 0.2952
2026-01-10 12:09:52,190: t15.2024.03.17 val PER: 0.2183
2026-01-10 12:09:52,191: t15.2024.05.10 val PER: 0.2229
2026-01-10 12:09:52,191: t15.2024.06.14 val PER: 0.2461
2026-01-10 12:09:52,191: t15.2024.07.19 val PER: 0.3349
2026-01-10 12:09:52,191: t15.2024.07.21 val PER: 0.1821
2026-01-10 12:09:52,191: t15.2024.07.28 val PER: 0.2154
2026-01-10 12:09:52,191: t15.2025.01.10 val PER: 0.3857
2026-01-10 12:09:52,191: t15.2025.01.12 val PER: 0.2379
2026-01-10 12:09:52,191: t15.2025.03.14 val PER: 0.4364
2026-01-10 12:09:52,191: t15.2025.03.16 val PER: 0.2893
2026-01-10 12:09:52,191: t15.2025.03.30 val PER: 0.4000
2026-01-10 12:09:52,191: t15.2025.04.13 val PER: 0.3124
2026-01-10 12:09:52,269: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_medium/checkpoint/checkpoint_batch_19999
2026-01-10 12:09:52,292: Best avg val PER achieved: 0.42784
2026-01-10 12:09:52,293: Total training time: 289.33 minutes

=== RUN xlstm_small_lr25.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25
2026-01-10 12:09:58,363: Using device: cuda:0
2026-01-10 12:10:00,239: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-10 12:10:00,259: Using 45 sessions after filtering (from 45).
DEBUG: xlstm_backend = vanilla
2026-01-10 12:10:00,300: Using torch.compile (if available)
2026-01-10 12:10:00,300: torch.compile not available (torch<2.0). Skipping.
2026-01-10 12:10:00,300: Initialized RNN decoding model
2026-01-10 12:10:00,301: XLSTMDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (in_proj): Linear(in_features=7168, out_features=512, bias=True)
  (xlstm): xLSTMBlockStack(
    (blocks): ModuleList(
      (0): sLSTMBlock(
        (xlstm_norm): LayerNorm()
        (xlstm): sLSTMLayer(
          (conv1d): CausalConv1d(
            (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
          )
          (conv_act_fn): SiLU()
          (fgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (igate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (zgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (ogate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (slstm_cell): sLSTMCell_vanilla(function=slstm, hidden_size=512, num_heads=4)
          (group_norm): MultiHeadLayerNorm()
          (dropout): Dropout(p=0.2, inplace=False)
        )
      )
      (1): sLSTMBlock(
        (xlstm_norm): LayerNorm()
        (xlstm): sLSTMLayer(
          (conv1d): CausalConv1d(
            (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
          )
          (conv_act_fn): SiLU()
          (fgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (igate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (zgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (ogate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (slstm_cell): sLSTMCell_vanilla(function=slstm, hidden_size=512, num_heads=4)
          (group_norm): MultiHeadLayerNorm()
          (dropout): Dropout(p=0.2, inplace=False)
        )
      )
      (2): sLSTMBlock(
        (xlstm_norm): LayerNorm()
        (xlstm): sLSTMLayer(
          (conv1d): CausalConv1d(
            (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
          )
          (conv_act_fn): SiLU()
          (fgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (igate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (zgate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (ogate): LinearHeadwiseExpand(in_features=512, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )
          (slstm_cell): sLSTMCell_vanilla(function=slstm, hidden_size=512, num_heads=4)
          (group_norm): MultiHeadLayerNorm()
          (dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (post_blocks_norm): LayerNorm()
  )
  (head): Identity()
  (dropout): Dropout(p=0.2, inplace=False)
  (out): Linear(in_features=512, out_features=41, bias=True)
)
2026-01-10 12:10:00,301: Model has 17,101,353 parameters
2026-01-10 12:10:00,301: Model has 11,819,520 day-specific parameters | 69.11% of total parameters
2026-01-10 12:10:01,569: Successfully initialized datasets
2026-01-10 12:10:01,569: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-10 12:10:03,338: Train batch 0: loss: 679.42 grad norm: 935.71 time: 1.350
2026-01-10 12:10:03,338: Running test after training batch: 0
2026-01-10 12:10:03,573: WER debug GT example: You can see the code at this point as well.
2026-01-10 12:10:21,706: WER debug example
  GT : you can see the code at this point as well
  PR : waterworth knower nurnberger
2026-01-10 12:10:22,021: WER debug example
  GT : how does it keep the cost down
  PR : garbo snouffer westermeyer
2026-01-10 12:10:46,693: Val batch 0: PER (avg): 4.5593 CTC Loss (avg): 656.5860 WER(1gram): 100.00% (n=64) time: 43.355
2026-01-10 12:10:46,694: WER lens: avg_true_words=6.16 avg_pred_words=3.47 max_pred_words=8
2026-01-10 12:10:46,694: t15.2023.08.13 val PER: 3.8077
2026-01-10 12:10:46,694: t15.2023.08.18 val PER: 4.2682
2026-01-10 12:10:46,694: t15.2023.08.20 val PER: 4.0246
2026-01-10 12:10:46,694: t15.2023.08.25 val PER: 4.2590
2026-01-10 12:10:46,694: t15.2023.08.27 val PER: 3.7878
2026-01-10 12:10:46,694: t15.2023.09.01 val PER: 4.2711
2026-01-10 12:10:46,695: t15.2023.09.03 val PER: 4.2126
2026-01-10 12:10:46,695: t15.2023.09.24 val PER: 4.7670
2026-01-10 12:10:46,695: t15.2023.09.29 val PER: 4.5890
2026-01-10 12:10:46,695: t15.2023.10.01 val PER: 3.6037
2026-01-10 12:10:46,695: t15.2023.10.06 val PER: 4.7287
2026-01-10 12:10:46,695: t15.2023.10.08 val PER: 3.4398
2026-01-10 12:10:46,695: t15.2023.10.13 val PER: 4.2925
2026-01-10 12:10:46,695: t15.2023.10.15 val PER: 4.5458
2026-01-10 12:10:46,695: t15.2023.10.20 val PER: 4.8490
2026-01-10 12:10:46,695: t15.2023.10.22 val PER: 4.7094
2026-01-10 12:10:46,695: t15.2023.11.03 val PER: 5.1364
2026-01-10 12:10:46,696: t15.2023.11.04 val PER: 6.1126
2026-01-10 12:10:46,696: t15.2023.11.17 val PER: 6.6827
2026-01-10 12:10:46,696: t15.2023.11.19 val PER: 5.4351
2026-01-10 12:10:46,696: t15.2023.11.26 val PER: 4.9993
2026-01-10 12:10:46,696: t15.2023.12.03 val PER: 4.5777
2026-01-10 12:10:46,696: t15.2023.12.08 val PER: 4.9068
2026-01-10 12:10:46,696: t15.2023.12.10 val PER: 5.2865
2026-01-10 12:10:46,696: t15.2023.12.17 val PER: 4.4304
2026-01-10 12:10:46,696: t15.2023.12.29 val PER: 4.5216
2026-01-10 12:10:46,696: t15.2024.02.25 val PER: 4.5646
2026-01-10 12:10:46,696: t15.2024.03.08 val PER: 4.1408
2026-01-10 12:10:46,696: t15.2024.03.15 val PER: 4.2564
2026-01-10 12:10:46,696: t15.2024.03.17 val PER: 4.4763
2026-01-10 12:10:46,696: t15.2024.05.10 val PER: 4.4071
2026-01-10 12:10:46,696: t15.2024.06.14 val PER: 5.2997
2026-01-10 12:10:46,696: t15.2024.07.19 val PER: 3.3078
2026-01-10 12:10:46,697: t15.2024.07.21 val PER: 5.0952
2026-01-10 12:10:46,697: t15.2024.07.28 val PER: 5.3375
2026-01-10 12:10:46,697: t15.2025.01.10 val PER: 3.3567
2026-01-10 12:10:46,697: t15.2025.01.12 val PER: 5.9038
2026-01-10 12:10:46,697: t15.2025.03.14 val PER: 3.2441
2026-01-10 12:10:46,697: t15.2025.03.16 val PER: 5.8181
2026-01-10 12:10:46,697: t15.2025.03.30 val PER: 4.4575
2026-01-10 12:10:46,697: t15.2025.04.13 val PER: 4.8659
2026-01-10 12:10:46,699: New best val WER(1gram) inf% --> 100.00%
2026-01-10 12:10:46,806: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_0
2026-01-10 12:13:09,460: Train batch 200: loss: 92.68 grad norm: 143.00 time: 0.830
2026-01-10 12:15:35,410: Train batch 400: loss: 67.75 grad norm: 86.56 time: 0.610
2026-01-10 12:16:50,341: Running test after training batch: 500
2026-01-10 12:16:50,697: WER debug GT example: You can see the code at this point as well.
2026-01-10 12:17:08,445: WER debug example
  GT : you can see the code at this point as well
  PR : ooohs ooohs erte
2026-01-10 12:17:08,492: WER debug example
  GT : how does it keep the cost down
  PR : ise zue ise
2026-01-10 12:17:11,203: Val batch 500: PER (avg): 0.7385 CTC Loss (avg): 78.5083 WER(1gram): 98.48% (n=64) time: 20.861
2026-01-10 12:17:11,203: WER lens: avg_true_words=6.16 avg_pred_words=2.42 max_pred_words=6
2026-01-10 12:17:11,203: t15.2023.08.13 val PER: 0.7370
2026-01-10 12:17:11,203: t15.2023.08.18 val PER: 0.7091
2026-01-10 12:17:11,203: t15.2023.08.20 val PER: 0.7212
2026-01-10 12:17:11,203: t15.2023.08.25 val PER: 0.7003
2026-01-10 12:17:11,203: t15.2023.08.27 val PER: 0.7605
2026-01-10 12:17:11,204: t15.2023.09.01 val PER: 0.6737
2026-01-10 12:17:11,204: t15.2023.09.03 val PER: 0.7328
2026-01-10 12:17:11,204: t15.2023.09.24 val PER: 0.7051
2026-01-10 12:17:11,204: t15.2023.09.29 val PER: 0.7135
2026-01-10 12:17:11,204: t15.2023.10.01 val PER: 0.7424
2026-01-10 12:17:11,204: t15.2023.10.06 val PER: 0.7018
2026-01-10 12:17:11,204: t15.2023.10.08 val PER: 0.7605
2026-01-10 12:17:11,204: t15.2023.10.13 val PER: 0.7649
2026-01-10 12:17:11,204: t15.2023.10.15 val PER: 0.7238
2026-01-10 12:17:11,204: t15.2023.10.20 val PER: 0.7114
2026-01-10 12:17:11,204: t15.2023.10.22 val PER: 0.6971
2026-01-10 12:17:11,204: t15.2023.11.03 val PER: 0.7347
2026-01-10 12:17:11,204: t15.2023.11.04 val PER: 0.6758
2026-01-10 12:17:11,205: t15.2023.11.17 val PER: 0.6967
2026-01-10 12:17:11,205: t15.2023.11.19 val PER: 0.6447
2026-01-10 12:17:11,205: t15.2023.11.26 val PER: 0.7754
2026-01-10 12:17:11,205: t15.2023.12.03 val PER: 0.7374
2026-01-10 12:17:11,205: t15.2023.12.08 val PER: 0.7450
2026-01-10 12:17:11,205: t15.2023.12.10 val PER: 0.7319
2026-01-10 12:17:11,205: t15.2023.12.17 val PER: 0.7786
2026-01-10 12:17:11,205: t15.2023.12.29 val PER: 0.7488
2026-01-10 12:17:11,205: t15.2024.02.25 val PER: 0.7472
2026-01-10 12:17:11,205: t15.2024.03.08 val PER: 0.7596
2026-01-10 12:17:11,205: t15.2024.03.15 val PER: 0.7480
2026-01-10 12:17:11,205: t15.2024.03.17 val PER: 0.7301
2026-01-10 12:17:11,205: t15.2024.05.10 val PER: 0.7325
2026-01-10 12:17:11,205: t15.2024.06.14 val PER: 0.7082
2026-01-10 12:17:11,206: t15.2024.07.19 val PER: 0.7871
2026-01-10 12:17:11,206: t15.2024.07.21 val PER: 0.7193
2026-01-10 12:17:11,206: t15.2024.07.28 val PER: 0.7279
2026-01-10 12:17:11,206: t15.2025.01.10 val PER: 0.8127
2026-01-10 12:17:11,206: t15.2025.01.12 val PER: 0.7483
2026-01-10 12:17:11,206: t15.2025.03.14 val PER: 0.8136
2026-01-10 12:17:11,206: t15.2025.03.16 val PER: 0.7579
2026-01-10 12:17:11,206: t15.2025.03.30 val PER: 0.8287
2026-01-10 12:17:11,206: t15.2025.04.13 val PER: 0.7546
2026-01-10 12:17:11,207: New best val WER(1gram) 100.00% --> 98.48%
2026-01-10 12:17:11,298: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_500
2026-01-10 12:18:23,619: Train batch 600: loss: 85.59 grad norm: 102.69 time: 0.630
2026-01-10 12:20:50,239: Train batch 800: loss: 53.83 grad norm: 78.20 time: 0.673
2026-01-10 12:23:20,364: Train batch 1000: loss: 53.50 grad norm: 65.35 time: 0.689
2026-01-10 12:23:20,365: Running test after training batch: 1000
2026-01-10 12:23:20,602: WER debug GT example: You can see the code at this point as well.
2026-01-10 12:23:38,581: WER debug example
  GT : you can see the code at this point as well
  PR : 
2026-01-10 12:23:38,610: WER debug example
  GT : how does it keep the cost down
  PR : z
2026-01-10 12:23:40,121: Val batch 1000: PER (avg): 0.6818 CTC Loss (avg): 59.5802 WER(1gram): 100.00% (n=64) time: 19.755
2026-01-10 12:23:40,122: WER lens: avg_true_words=6.16 avg_pred_words=0.88 max_pred_words=3
2026-01-10 12:23:40,122: t15.2023.08.13 val PER: 0.6393
2026-01-10 12:23:40,122: t15.2023.08.18 val PER: 0.6178
2026-01-10 12:23:40,122: t15.2023.08.20 val PER: 0.6219
2026-01-10 12:23:40,122: t15.2023.08.25 val PER: 0.6130
2026-01-10 12:23:40,122: t15.2023.08.27 val PER: 0.6688
2026-01-10 12:23:40,122: t15.2023.09.01 val PER: 0.5966
2026-01-10 12:23:40,122: t15.2023.09.03 val PER: 0.6793
2026-01-10 12:23:40,122: t15.2023.09.24 val PER: 0.6323
2026-01-10 12:23:40,122: t15.2023.09.29 val PER: 0.6267
2026-01-10 12:23:40,122: t15.2023.10.01 val PER: 0.6684
2026-01-10 12:23:40,123: t15.2023.10.06 val PER: 0.6189
2026-01-10 12:23:40,123: t15.2023.10.08 val PER: 0.6725
2026-01-10 12:23:40,123: t15.2023.10.13 val PER: 0.7067
2026-01-10 12:23:40,123: t15.2023.10.15 val PER: 0.6599
2026-01-10 12:23:40,123: t15.2023.10.20 val PER: 0.6275
2026-01-10 12:23:40,123: t15.2023.10.22 val PER: 0.6192
2026-01-10 12:23:40,123: t15.2023.11.03 val PER: 0.6757
2026-01-10 12:23:40,123: t15.2023.11.04 val PER: 0.5392
2026-01-10 12:23:40,123: t15.2023.11.17 val PER: 0.5910
2026-01-10 12:23:40,123: t15.2023.11.19 val PER: 0.5768
2026-01-10 12:23:40,123: t15.2023.11.26 val PER: 0.7065
2026-01-10 12:23:40,123: t15.2023.12.03 val PER: 0.6607
2026-01-10 12:23:40,123: t15.2023.12.08 val PER: 0.6824
2026-01-10 12:23:40,123: t15.2023.12.10 val PER: 0.6373
2026-01-10 12:23:40,124: t15.2023.12.17 val PER: 0.7339
2026-01-10 12:23:40,124: t15.2023.12.29 val PER: 0.6767
2026-01-10 12:23:40,124: t15.2024.02.25 val PER: 0.6728
2026-01-10 12:23:40,124: t15.2024.03.08 val PER: 0.7724
2026-01-10 12:23:40,124: t15.2024.03.15 val PER: 0.7342
2026-01-10 12:23:40,124: t15.2024.03.17 val PER: 0.6911
2026-01-10 12:23:40,124: t15.2024.05.10 val PER: 0.7043
2026-01-10 12:23:40,124: t15.2024.06.14 val PER: 0.7114
2026-01-10 12:23:40,124: t15.2024.07.19 val PER: 0.7825
2026-01-10 12:23:40,124: t15.2024.07.21 val PER: 0.6828
2026-01-10 12:23:40,125: t15.2024.07.28 val PER: 0.7051
2026-01-10 12:23:40,125: t15.2025.01.10 val PER: 0.8058
2026-01-10 12:23:40,125: t15.2025.01.12 val PER: 0.7152
2026-01-10 12:23:40,125: t15.2025.03.14 val PER: 0.8121
2026-01-10 12:23:40,125: t15.2025.03.16 val PER: 0.7212
2026-01-10 12:23:40,125: t15.2025.03.30 val PER: 0.8218
2026-01-10 12:23:40,125: t15.2025.04.13 val PER: 0.7218
2026-01-10 12:23:40,181: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_1000
2026-01-10 12:26:11,623: Train batch 1200: loss: 55.91 grad norm: 67.65 time: 0.661
2026-01-10 12:28:47,125: Train batch 1400: loss: 51.12 grad norm: 64.18 time: 0.673
2026-01-10 12:30:03,150: Running test after training batch: 1500
2026-01-10 12:30:03,407: WER debug GT example: You can see the code at this point as well.
2026-01-10 12:30:21,150: WER debug example
  GT : you can see the code at this point as well
  PR : uher aunt ease durr
2026-01-10 12:30:21,179: WER debug example
  GT : how does it keep the cost down
  PR : z t
2026-01-10 12:30:22,706: Val batch 1500: PER (avg): 0.6128 CTC Loss (avg): 52.1161 WER(1gram): 99.49% (n=64) time: 19.555
2026-01-10 12:30:22,706: WER lens: avg_true_words=6.16 avg_pred_words=2.56 max_pred_words=6
2026-01-10 12:30:22,706: t15.2023.08.13 val PER: 0.5655
2026-01-10 12:30:22,706: t15.2023.08.18 val PER: 0.5599
2026-01-10 12:30:22,707: t15.2023.08.20 val PER: 0.5425
2026-01-10 12:30:22,707: t15.2023.08.25 val PER: 0.5181
2026-01-10 12:30:22,707: t15.2023.08.27 val PER: 0.5932
2026-01-10 12:30:22,707: t15.2023.09.01 val PER: 0.5146
2026-01-10 12:30:22,707: t15.2023.09.03 val PER: 0.6116
2026-01-10 12:30:22,707: t15.2023.09.24 val PER: 0.5704
2026-01-10 12:30:22,707: t15.2023.09.29 val PER: 0.5603
2026-01-10 12:30:22,707: t15.2023.10.01 val PER: 0.5911
2026-01-10 12:30:22,707: t15.2023.10.06 val PER: 0.5285
2026-01-10 12:30:22,707: t15.2023.10.08 val PER: 0.6252
2026-01-10 12:30:22,707: t15.2023.10.13 val PER: 0.6346
2026-01-10 12:30:22,707: t15.2023.10.15 val PER: 0.5821
2026-01-10 12:30:22,707: t15.2023.10.20 val PER: 0.5604
2026-01-10 12:30:22,707: t15.2023.10.22 val PER: 0.5434
2026-01-10 12:30:22,707: t15.2023.11.03 val PER: 0.5773
2026-01-10 12:30:22,707: t15.2023.11.04 val PER: 0.4300
2026-01-10 12:30:22,708: t15.2023.11.17 val PER: 0.4914
2026-01-10 12:30:22,708: t15.2023.11.19 val PER: 0.4930
2026-01-10 12:30:22,708: t15.2023.11.26 val PER: 0.6399
2026-01-10 12:30:22,708: t15.2023.12.03 val PER: 0.5987
2026-01-10 12:30:22,708: t15.2023.12.08 val PER: 0.6158
2026-01-10 12:30:22,708: t15.2023.12.10 val PER: 0.5756
2026-01-10 12:30:22,708: t15.2023.12.17 val PER: 0.6518
2026-01-10 12:30:22,708: t15.2023.12.29 val PER: 0.6108
2026-01-10 12:30:22,708: t15.2024.02.25 val PER: 0.5983
2026-01-10 12:30:22,708: t15.2024.03.08 val PER: 0.6942
2026-01-10 12:30:22,708: t15.2024.03.15 val PER: 0.6635
2026-01-10 12:30:22,708: t15.2024.03.17 val PER: 0.6220
2026-01-10 12:30:22,708: t15.2024.05.10 val PER: 0.6523
2026-01-10 12:30:22,708: t15.2024.06.14 val PER: 0.6467
2026-01-10 12:30:22,708: t15.2024.07.19 val PER: 0.7383
2026-01-10 12:30:22,709: t15.2024.07.21 val PER: 0.6041
2026-01-10 12:30:22,709: t15.2024.07.28 val PER: 0.6375
2026-01-10 12:30:22,709: t15.2025.01.10 val PER: 0.7686
2026-01-10 12:30:22,709: t15.2025.01.12 val PER: 0.6520
2026-01-10 12:30:22,709: t15.2025.03.14 val PER: 0.7885
2026-01-10 12:30:22,709: t15.2025.03.16 val PER: 0.6649
2026-01-10 12:30:22,709: t15.2025.03.30 val PER: 0.7943
2026-01-10 12:30:22,709: t15.2025.04.13 val PER: 0.6633
2026-01-10 12:30:22,764: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_1500
2026-01-10 12:31:39,165: Train batch 1600: loss: 64.40 grad norm: 116.42 time: 0.744
2026-01-10 12:34:11,905: Train batch 1800: loss: 45.95 grad norm: 71.84 time: 0.644
2026-01-10 12:36:46,998: Train batch 2000: loss: 37.27 grad norm: 65.03 time: 0.664
2026-01-10 12:36:47,000: Running test after training batch: 2000
2026-01-10 12:36:47,224: WER debug GT example: You can see the code at this point as well.
2026-01-10 12:37:04,944: WER debug example
  GT : you can see the code at this point as well
  PR : uher and ease a durr at to
2026-01-10 12:37:04,972: WER debug example
  GT : how does it keep the cost down
  PR : loud z it the os utt
2026-01-10 12:37:06,553: Val batch 2000: PER (avg): 0.5517 CTC Loss (avg): 46.8317 WER(1gram): 93.91% (n=64) time: 19.553
2026-01-10 12:37:06,553: WER lens: avg_true_words=6.16 avg_pred_words=4.27 max_pred_words=8
2026-01-10 12:37:06,553: t15.2023.08.13 val PER: 0.5187
2026-01-10 12:37:06,554: t15.2023.08.18 val PER: 0.4920
2026-01-10 12:37:06,554: t15.2023.08.20 val PER: 0.4869
2026-01-10 12:37:06,554: t15.2023.08.25 val PER: 0.4458
2026-01-10 12:37:06,554: t15.2023.08.27 val PER: 0.5531
2026-01-10 12:37:06,554: t15.2023.09.01 val PER: 0.4537
2026-01-10 12:37:06,554: t15.2023.09.03 val PER: 0.5475
2026-01-10 12:37:06,554: t15.2023.09.24 val PER: 0.5097
2026-01-10 12:37:06,554: t15.2023.09.29 val PER: 0.4984
2026-01-10 12:37:06,554: t15.2023.10.01 val PER: 0.5178
2026-01-10 12:37:06,554: t15.2023.10.06 val PER: 0.4726
2026-01-10 12:37:06,555: t15.2023.10.08 val PER: 0.5494
2026-01-10 12:37:06,555: t15.2023.10.13 val PER: 0.5741
2026-01-10 12:37:06,555: t15.2023.10.15 val PER: 0.5168
2026-01-10 12:37:06,555: t15.2023.10.20 val PER: 0.4698
2026-01-10 12:37:06,555: t15.2023.10.22 val PER: 0.4733
2026-01-10 12:37:06,555: t15.2023.11.03 val PER: 0.5156
2026-01-10 12:37:06,555: t15.2023.11.04 val PER: 0.3549
2026-01-10 12:37:06,555: t15.2023.11.17 val PER: 0.4215
2026-01-10 12:37:06,555: t15.2023.11.19 val PER: 0.4271
2026-01-10 12:37:06,555: t15.2023.11.26 val PER: 0.5841
2026-01-10 12:37:06,555: t15.2023.12.03 val PER: 0.5284
2026-01-10 12:37:06,556: t15.2023.12.08 val PER: 0.5506
2026-01-10 12:37:06,556: t15.2023.12.10 val PER: 0.5177
2026-01-10 12:37:06,556: t15.2023.12.17 val PER: 0.5748
2026-01-10 12:37:06,556: t15.2023.12.29 val PER: 0.5525
2026-01-10 12:37:06,556: t15.2024.02.25 val PER: 0.5000
2026-01-10 12:37:06,556: t15.2024.03.08 val PER: 0.6202
2026-01-10 12:37:06,556: t15.2024.03.15 val PER: 0.5979
2026-01-10 12:37:06,556: t15.2024.03.17 val PER: 0.5690
2026-01-10 12:37:06,556: t15.2024.05.10 val PER: 0.6048
2026-01-10 12:37:06,556: t15.2024.06.14 val PER: 0.6025
2026-01-10 12:37:06,556: t15.2024.07.19 val PER: 0.6684
2026-01-10 12:37:06,556: t15.2024.07.21 val PER: 0.5434
2026-01-10 12:37:06,557: t15.2024.07.28 val PER: 0.5743
2026-01-10 12:37:06,557: t15.2025.01.10 val PER: 0.7493
2026-01-10 12:37:06,557: t15.2025.01.12 val PER: 0.5982
2026-01-10 12:37:06,557: t15.2025.03.14 val PER: 0.7337
2026-01-10 12:37:06,557: t15.2025.03.16 val PER: 0.6060
2026-01-10 12:37:06,557: t15.2025.03.30 val PER: 0.7644
2026-01-10 12:37:06,557: t15.2025.04.13 val PER: 0.6248
2026-01-10 12:37:06,558: New best val WER(1gram) 98.48% --> 93.91%
2026-01-10 12:37:06,650: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_2000
2026-01-10 12:39:39,229: Train batch 2200: loss: 55.04 grad norm: 63.58 time: 0.643
2026-01-10 12:42:19,043: Train batch 2400: loss: 58.93 grad norm: 81.10 time: 0.569
2026-01-10 12:43:36,524: Running test after training batch: 2500
2026-01-10 12:43:36,748: WER debug GT example: You can see the code at this point as well.
2026-01-10 12:43:54,529: WER debug example
  GT : you can see the code at this point as well
  PR : yoor aunt ease a de at us to as
2026-01-10 12:43:54,557: WER debug example
  GT : how does it keep the cost down
  PR : vowed z it pu the sci
2026-01-10 12:43:56,189: Val batch 2500: PER (avg): 0.5161 CTC Loss (avg): 44.3920 WER(1gram): 90.10% (n=64) time: 19.664
2026-01-10 12:43:56,189: WER lens: avg_true_words=6.16 avg_pred_words=4.73 max_pred_words=9
2026-01-10 12:43:56,189: t15.2023.08.13 val PER: 0.4865
2026-01-10 12:43:56,189: t15.2023.08.18 val PER: 0.4610
2026-01-10 12:43:56,189: t15.2023.08.20 val PER: 0.4488
2026-01-10 12:43:56,190: t15.2023.08.25 val PER: 0.4157
2026-01-10 12:43:56,190: t15.2023.08.27 val PER: 0.5273
2026-01-10 12:43:56,190: t15.2023.09.01 val PER: 0.4172
2026-01-10 12:43:56,190: t15.2023.09.03 val PER: 0.5166
2026-01-10 12:43:56,190: t15.2023.09.24 val PER: 0.4660
2026-01-10 12:43:56,190: t15.2023.09.29 val PER: 0.4665
2026-01-10 12:43:56,190: t15.2023.10.01 val PER: 0.4901
2026-01-10 12:43:56,190: t15.2023.10.06 val PER: 0.4424
2026-01-10 12:43:56,190: t15.2023.10.08 val PER: 0.5196
2026-01-10 12:43:56,190: t15.2023.10.13 val PER: 0.5431
2026-01-10 12:43:56,190: t15.2023.10.15 val PER: 0.4858
2026-01-10 12:43:56,190: t15.2023.10.20 val PER: 0.4497
2026-01-10 12:43:56,190: t15.2023.10.22 val PER: 0.4488
2026-01-10 12:43:56,190: t15.2023.11.03 val PER: 0.4769
2026-01-10 12:43:56,191: t15.2023.11.04 val PER: 0.3345
2026-01-10 12:43:56,191: t15.2023.11.17 val PER: 0.3810
2026-01-10 12:43:56,191: t15.2023.11.19 val PER: 0.3812
2026-01-10 12:43:56,191: t15.2023.11.26 val PER: 0.5478
2026-01-10 12:43:56,191: t15.2023.12.03 val PER: 0.4884
2026-01-10 12:43:56,191: t15.2023.12.08 val PER: 0.5060
2026-01-10 12:43:56,191: t15.2023.12.10 val PER: 0.4862
2026-01-10 12:43:56,191: t15.2023.12.17 val PER: 0.5385
2026-01-10 12:43:56,191: t15.2023.12.29 val PER: 0.5202
2026-01-10 12:43:56,191: t15.2024.02.25 val PER: 0.4396
2026-01-10 12:43:56,191: t15.2024.03.08 val PER: 0.5775
2026-01-10 12:43:56,191: t15.2024.03.15 val PER: 0.5591
2026-01-10 12:43:56,191: t15.2024.03.17 val PER: 0.5265
2026-01-10 12:43:56,191: t15.2024.05.10 val PER: 0.5765
2026-01-10 12:43:56,192: t15.2024.06.14 val PER: 0.5394
2026-01-10 12:43:56,192: t15.2024.07.19 val PER: 0.6368
2026-01-10 12:43:56,192: t15.2024.07.21 val PER: 0.5014
2026-01-10 12:43:56,192: t15.2024.07.28 val PER: 0.5294
2026-01-10 12:43:56,192: t15.2025.01.10 val PER: 0.6956
2026-01-10 12:43:56,192: t15.2025.01.12 val PER: 0.5651
2026-01-10 12:43:56,193: t15.2025.03.14 val PER: 0.7249
2026-01-10 12:43:56,193: t15.2025.03.16 val PER: 0.5825
2026-01-10 12:43:56,193: t15.2025.03.30 val PER: 0.7264
2026-01-10 12:43:56,193: t15.2025.04.13 val PER: 0.5963
2026-01-10 12:43:56,193: New best val WER(1gram) 93.91% --> 90.10%
2026-01-10 12:43:56,294: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_2500
2026-01-10 12:45:13,614: Train batch 2600: loss: 48.70 grad norm: 77.96 time: 0.744
2026-01-10 12:47:46,714: Train batch 2800: loss: 40.66 grad norm: 59.14 time: 0.586
2026-01-10 12:50:14,379: Train batch 3000: loss: 59.70 grad norm: 80.08 time: 0.939
2026-01-10 12:50:14,380: Running test after training batch: 3000
2026-01-10 12:50:14,603: WER debug GT example: You can see the code at this point as well.
2026-01-10 12:50:32,198: WER debug example
  GT : you can see the code at this point as well
  PR : yoor and ease a uhde at is to eel
2026-01-10 12:50:32,228: WER debug example
  GT : how does it keep the cost down
  PR : owl z it pu the os and
2026-01-10 12:50:33,852: Val batch 3000: PER (avg): 0.4671 CTC Loss (avg): 41.2778 WER(1gram): 87.56% (n=64) time: 19.470
2026-01-10 12:50:33,852: WER lens: avg_true_words=6.16 avg_pred_words=5.52 max_pred_words=9
2026-01-10 12:50:33,852: t15.2023.08.13 val PER: 0.4283
2026-01-10 12:50:33,852: t15.2023.08.18 val PER: 0.4124
2026-01-10 12:50:33,853: t15.2023.08.20 val PER: 0.4035
2026-01-10 12:50:33,853: t15.2023.08.25 val PER: 0.3630
2026-01-10 12:50:33,853: t15.2023.08.27 val PER: 0.4839
2026-01-10 12:50:33,853: t15.2023.09.01 val PER: 0.3726
2026-01-10 12:50:33,853: t15.2023.09.03 val PER: 0.4537
2026-01-10 12:50:33,853: t15.2023.09.24 val PER: 0.3847
2026-01-10 12:50:33,853: t15.2023.09.29 val PER: 0.4052
2026-01-10 12:50:33,853: t15.2023.10.01 val PER: 0.4359
2026-01-10 12:50:33,853: t15.2023.10.06 val PER: 0.3757
2026-01-10 12:50:33,854: t15.2023.10.08 val PER: 0.4682
2026-01-10 12:50:33,854: t15.2023.10.13 val PER: 0.4988
2026-01-10 12:50:33,854: t15.2023.10.15 val PER: 0.4285
2026-01-10 12:50:33,854: t15.2023.10.20 val PER: 0.4094
2026-01-10 12:50:33,854: t15.2023.10.22 val PER: 0.3753
2026-01-10 12:50:33,854: t15.2023.11.03 val PER: 0.4308
2026-01-10 12:50:33,854: t15.2023.11.04 val PER: 0.2765
2026-01-10 12:50:33,854: t15.2023.11.17 val PER: 0.3453
2026-01-10 12:50:33,854: t15.2023.11.19 val PER: 0.3174
2026-01-10 12:50:33,854: t15.2023.11.26 val PER: 0.4877
2026-01-10 12:50:33,854: t15.2023.12.03 val PER: 0.4412
2026-01-10 12:50:33,855: t15.2023.12.08 val PER: 0.4581
2026-01-10 12:50:33,855: t15.2023.12.10 val PER: 0.4428
2026-01-10 12:50:33,855: t15.2023.12.17 val PER: 0.4969
2026-01-10 12:50:33,855: t15.2023.12.29 val PER: 0.4653
2026-01-10 12:50:33,855: t15.2024.02.25 val PER: 0.3947
2026-01-10 12:50:33,855: t15.2024.03.08 val PER: 0.5277
2026-01-10 12:50:33,855: t15.2024.03.15 val PER: 0.5122
2026-01-10 12:50:33,855: t15.2024.03.17 val PER: 0.4784
2026-01-10 12:50:33,855: t15.2024.05.10 val PER: 0.5230
2026-01-10 12:50:33,855: t15.2024.06.14 val PER: 0.4890
2026-01-10 12:50:33,855: t15.2024.07.19 val PER: 0.6005
2026-01-10 12:50:33,856: t15.2024.07.21 val PER: 0.4462
2026-01-10 12:50:33,856: t15.2024.07.28 val PER: 0.4787
2026-01-10 12:50:33,856: t15.2025.01.10 val PER: 0.6653
2026-01-10 12:50:33,856: t15.2025.01.12 val PER: 0.5373
2026-01-10 12:50:33,856: t15.2025.03.14 val PER: 0.6982
2026-01-10 12:50:33,856: t15.2025.03.16 val PER: 0.5458
2026-01-10 12:50:33,856: t15.2025.03.30 val PER: 0.7126
2026-01-10 12:50:33,856: t15.2025.04.13 val PER: 0.5578
2026-01-10 12:50:33,857: New best val WER(1gram) 90.10% --> 87.56%
2026-01-10 12:50:33,944: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_3000
2026-01-10 12:53:09,289: Train batch 3200: loss: 36.25 grad norm: 67.03 time: 0.695
2026-01-10 12:55:43,931: Train batch 3400: loss: 38.58 grad norm: 70.98 time: 0.599
2026-01-10 12:57:02,052: Running test after training batch: 3500
2026-01-10 12:57:02,276: WER debug GT example: You can see the code at this point as well.
2026-01-10 12:57:19,921: WER debug example
  GT : you can see the code at this point as well
  PR : yoor and ease a uhde at it otte is eel
2026-01-10 12:57:19,952: WER debug example
  GT : how does it keep the cost down
  PR : loud oz it p the os and
2026-01-10 12:57:21,623: Val batch 3500: PER (avg): 0.4231 CTC Loss (avg): 38.8458 WER(1gram): 87.82% (n=64) time: 19.569
2026-01-10 12:57:21,623: WER lens: avg_true_words=6.16 avg_pred_words=5.84 max_pred_words=10
2026-01-10 12:57:21,623: t15.2023.08.13 val PER: 0.3940
2026-01-10 12:57:21,623: t15.2023.08.18 val PER: 0.3671
2026-01-10 12:57:21,623: t15.2023.08.20 val PER: 0.3542
2026-01-10 12:57:21,623: t15.2023.08.25 val PER: 0.3072
2026-01-10 12:57:21,624: t15.2023.08.27 val PER: 0.4421
2026-01-10 12:57:21,624: t15.2023.09.01 val PER: 0.3287
2026-01-10 12:57:21,624: t15.2023.09.03 val PER: 0.4311
2026-01-10 12:57:21,624: t15.2023.09.24 val PER: 0.3519
2026-01-10 12:57:21,624: t15.2023.09.29 val PER: 0.3740
2026-01-10 12:57:21,624: t15.2023.10.01 val PER: 0.4042
2026-01-10 12:57:21,624: t15.2023.10.06 val PER: 0.3466
2026-01-10 12:57:21,624: t15.2023.10.08 val PER: 0.4438
2026-01-10 12:57:21,624: t15.2023.10.13 val PER: 0.4500
2026-01-10 12:57:21,624: t15.2023.10.15 val PER: 0.3823
2026-01-10 12:57:21,625: t15.2023.10.20 val PER: 0.3691
2026-01-10 12:57:21,625: t15.2023.10.22 val PER: 0.3419
2026-01-10 12:57:21,625: t15.2023.11.03 val PER: 0.3921
2026-01-10 12:57:21,625: t15.2023.11.04 val PER: 0.2048
2026-01-10 12:57:21,625: t15.2023.11.17 val PER: 0.2830
2026-01-10 12:57:21,625: t15.2023.11.19 val PER: 0.2655
2026-01-10 12:57:21,625: t15.2023.11.26 val PER: 0.4341
2026-01-10 12:57:21,625: t15.2023.12.03 val PER: 0.4055
2026-01-10 12:57:21,625: t15.2023.12.08 val PER: 0.3935
2026-01-10 12:57:21,625: t15.2023.12.10 val PER: 0.3745
2026-01-10 12:57:21,625: t15.2023.12.17 val PER: 0.4480
2026-01-10 12:57:21,626: t15.2023.12.29 val PER: 0.4358
2026-01-10 12:57:21,626: t15.2024.02.25 val PER: 0.3469
2026-01-10 12:57:21,626: t15.2024.03.08 val PER: 0.4851
2026-01-10 12:57:21,626: t15.2024.03.15 val PER: 0.4572
2026-01-10 12:57:21,626: t15.2024.03.17 val PER: 0.4470
2026-01-10 12:57:21,626: t15.2024.05.10 val PER: 0.4562
2026-01-10 12:57:21,626: t15.2024.06.14 val PER: 0.4338
2026-01-10 12:57:21,626: t15.2024.07.19 val PER: 0.5682
2026-01-10 12:57:21,626: t15.2024.07.21 val PER: 0.4014
2026-01-10 12:57:21,626: t15.2024.07.28 val PER: 0.4228
2026-01-10 12:57:21,626: t15.2025.01.10 val PER: 0.6295
2026-01-10 12:57:21,626: t15.2025.01.12 val PER: 0.4742
2026-01-10 12:57:21,627: t15.2025.03.14 val PER: 0.6509
2026-01-10 12:57:21,627: t15.2025.03.16 val PER: 0.5105
2026-01-10 12:57:21,627: t15.2025.03.30 val PER: 0.6644
2026-01-10 12:57:21,627: t15.2025.04.13 val PER: 0.5207
2026-01-10 12:57:21,680: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_3500
2026-01-10 12:58:38,918: Train batch 3600: loss: 37.02 grad norm: 64.65 time: 0.696
2026-01-10 13:01:16,743: Train batch 3800: loss: 41.70 grad norm: 67.32 time: 0.824
2026-01-10 13:03:50,727: Train batch 4000: loss: 34.43 grad norm: 63.10 time: 0.750
2026-01-10 13:03:50,731: Running test after training batch: 4000
2026-01-10 13:03:50,956: WER debug GT example: You can see the code at this point as well.
2026-01-10 13:04:08,547: WER debug example
  GT : you can see the code at this point as well
  PR : uhlir and ease utt uhde at it utt is ill
2026-01-10 13:04:08,579: WER debug example
  GT : how does it keep the cost down
  PR : loud oz it pu utt os and
2026-01-10 13:04:10,274: Val batch 4000: PER (avg): 0.3899 CTC Loss (avg): 36.9624 WER(1gram): 89.34% (n=64) time: 19.543
2026-01-10 13:04:10,275: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=12
2026-01-10 13:04:10,275: t15.2023.08.13 val PER: 0.3711
2026-01-10 13:04:10,275: t15.2023.08.18 val PER: 0.3286
2026-01-10 13:04:10,275: t15.2023.08.20 val PER: 0.3122
2026-01-10 13:04:10,275: t15.2023.08.25 val PER: 0.2846
2026-01-10 13:04:10,275: t15.2023.08.27 val PER: 0.4116
2026-01-10 13:04:10,275: t15.2023.09.01 val PER: 0.2963
2026-01-10 13:04:10,275: t15.2023.09.03 val PER: 0.3765
2026-01-10 13:04:10,275: t15.2023.09.24 val PER: 0.3301
2026-01-10 13:04:10,275: t15.2023.09.29 val PER: 0.3331
2026-01-10 13:04:10,275: t15.2023.10.01 val PER: 0.3686
2026-01-10 13:04:10,275: t15.2023.10.06 val PER: 0.3014
2026-01-10 13:04:10,276: t15.2023.10.08 val PER: 0.4168
2026-01-10 13:04:10,276: t15.2023.10.13 val PER: 0.4151
2026-01-10 13:04:10,276: t15.2023.10.15 val PER: 0.3441
2026-01-10 13:04:10,276: t15.2023.10.20 val PER: 0.3423
2026-01-10 13:04:10,276: t15.2023.10.22 val PER: 0.3140
2026-01-10 13:04:10,276: t15.2023.11.03 val PER: 0.3535
2026-01-10 13:04:10,276: t15.2023.11.04 val PER: 0.1911
2026-01-10 13:04:10,276: t15.2023.11.17 val PER: 0.2628
2026-01-10 13:04:10,276: t15.2023.11.19 val PER: 0.2176
2026-01-10 13:04:10,276: t15.2023.11.26 val PER: 0.4109
2026-01-10 13:04:10,276: t15.2023.12.03 val PER: 0.3550
2026-01-10 13:04:10,276: t15.2023.12.08 val PER: 0.3808
2026-01-10 13:04:10,276: t15.2023.12.10 val PER: 0.3430
2026-01-10 13:04:10,277: t15.2023.12.17 val PER: 0.4148
2026-01-10 13:04:10,277: t15.2023.12.29 val PER: 0.4022
2026-01-10 13:04:10,277: t15.2024.02.25 val PER: 0.3188
2026-01-10 13:04:10,277: t15.2024.03.08 val PER: 0.4580
2026-01-10 13:04:10,277: t15.2024.03.15 val PER: 0.4271
2026-01-10 13:04:10,277: t15.2024.03.17 val PER: 0.4114
2026-01-10 13:04:10,277: t15.2024.05.10 val PER: 0.4264
2026-01-10 13:04:10,277: t15.2024.06.14 val PER: 0.4006
2026-01-10 13:04:10,277: t15.2024.07.19 val PER: 0.5399
2026-01-10 13:04:10,277: t15.2024.07.21 val PER: 0.3607
2026-01-10 13:04:10,277: t15.2024.07.28 val PER: 0.3941
2026-01-10 13:04:10,277: t15.2025.01.10 val PER: 0.5909
2026-01-10 13:04:10,277: t15.2025.01.12 val PER: 0.4480
2026-01-10 13:04:10,277: t15.2025.03.14 val PER: 0.6169
2026-01-10 13:04:10,277: t15.2025.03.16 val PER: 0.4712
2026-01-10 13:04:10,277: t15.2025.03.30 val PER: 0.6172
2026-01-10 13:04:10,277: t15.2025.04.13 val PER: 0.4936
2026-01-10 13:04:10,340: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_4000
2026-01-10 13:06:38,873: Train batch 4200: loss: 50.99 grad norm: 78.70 time: 0.655
2026-01-10 13:09:07,835: Train batch 4400: loss: 44.66 grad norm: 70.78 time: 0.788
2026-01-10 13:10:23,830: Running test after training batch: 4500
2026-01-10 13:10:24,068: WER debug GT example: You can see the code at this point as well.
2026-01-10 13:10:41,665: WER debug example
  GT : you can see the code at this point as well
  PR : uhlir and ease a uhde at it utt is l
2026-01-10 13:10:41,696: WER debug example
  GT : how does it keep the cost down
  PR : houde oz it p utt os and
2026-01-10 13:10:43,408: Val batch 4500: PER (avg): 0.3710 CTC Loss (avg): 35.1658 WER(1gram): 89.59% (n=64) time: 19.574
2026-01-10 13:10:43,409: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-10 13:10:43,409: t15.2023.08.13 val PER: 0.3347
2026-01-10 13:10:43,409: t15.2023.08.18 val PER: 0.2959
2026-01-10 13:10:43,409: t15.2023.08.20 val PER: 0.2867
2026-01-10 13:10:43,409: t15.2023.08.25 val PER: 0.2590
2026-01-10 13:10:43,409: t15.2023.08.27 val PER: 0.3875
2026-01-10 13:10:43,409: t15.2023.09.01 val PER: 0.2735
2026-01-10 13:10:43,409: t15.2023.09.03 val PER: 0.3480
2026-01-10 13:10:43,409: t15.2023.09.24 val PER: 0.3083
2026-01-10 13:10:43,410: t15.2023.09.29 val PER: 0.3050
2026-01-10 13:10:43,410: t15.2023.10.01 val PER: 0.3507
2026-01-10 13:10:43,410: t15.2023.10.06 val PER: 0.2723
2026-01-10 13:10:43,410: t15.2023.10.08 val PER: 0.3978
2026-01-10 13:10:43,410: t15.2023.10.13 val PER: 0.4050
2026-01-10 13:10:43,410: t15.2023.10.15 val PER: 0.3289
2026-01-10 13:10:43,410: t15.2023.10.20 val PER: 0.3121
2026-01-10 13:10:43,410: t15.2023.10.22 val PER: 0.2851
2026-01-10 13:10:43,410: t15.2023.11.03 val PER: 0.3311
2026-01-10 13:10:43,410: t15.2023.11.04 val PER: 0.1809
2026-01-10 13:10:43,410: t15.2023.11.17 val PER: 0.2348
2026-01-10 13:10:43,410: t15.2023.11.19 val PER: 0.1956
2026-01-10 13:10:43,410: t15.2023.11.26 val PER: 0.3870
2026-01-10 13:10:43,410: t15.2023.12.03 val PER: 0.3288
2026-01-10 13:10:43,410: t15.2023.12.08 val PER: 0.3555
2026-01-10 13:10:43,411: t15.2023.12.10 val PER: 0.3049
2026-01-10 13:10:43,411: t15.2023.12.17 val PER: 0.3960
2026-01-10 13:10:43,411: t15.2023.12.29 val PER: 0.3809
2026-01-10 13:10:43,411: t15.2024.02.25 val PER: 0.2963
2026-01-10 13:10:43,411: t15.2024.03.08 val PER: 0.4580
2026-01-10 13:10:43,411: t15.2024.03.15 val PER: 0.4221
2026-01-10 13:10:43,411: t15.2024.03.17 val PER: 0.3808
2026-01-10 13:10:43,411: t15.2024.05.10 val PER: 0.4116
2026-01-10 13:10:43,411: t15.2024.06.14 val PER: 0.3833
2026-01-10 13:10:43,411: t15.2024.07.19 val PER: 0.5432
2026-01-10 13:10:43,411: t15.2024.07.21 val PER: 0.3497
2026-01-10 13:10:43,411: t15.2024.07.28 val PER: 0.3779
2026-01-10 13:10:43,411: t15.2025.01.10 val PER: 0.5882
2026-01-10 13:10:43,411: t15.2025.01.12 val PER: 0.4226
2026-01-10 13:10:43,412: t15.2025.03.14 val PER: 0.6183
2026-01-10 13:10:43,412: t15.2025.03.16 val PER: 0.4542
2026-01-10 13:10:43,412: t15.2025.03.30 val PER: 0.6425
2026-01-10 13:10:43,412: t15.2025.04.13 val PER: 0.4836
2026-01-10 13:10:43,465: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_4500
2026-01-10 13:11:57,826: Train batch 4600: loss: 41.24 grad norm: 109.97 time: 0.774
2026-01-10 13:14:27,872: Train batch 4800: loss: 32.22 grad norm: 63.38 time: 1.033
2026-01-10 13:17:00,506: Train batch 5000: loss: 41.17 grad norm: 78.21 time: 0.990
2026-01-10 13:17:00,507: Running test after training batch: 5000
2026-01-10 13:17:00,758: WER debug GT example: You can see the code at this point as well.
2026-01-10 13:17:18,374: WER debug example
  GT : you can see the code at this point as well
  PR : uhlir eyde ease utt owed at it otte is ill
2026-01-10 13:17:18,406: WER debug example
  GT : how does it keep the cost down
  PR : houde oz it plue uthe os and
2026-01-10 13:17:20,124: Val batch 5000: PER (avg): 0.3513 CTC Loss (avg): 33.8924 WER(1gram): 89.59% (n=64) time: 19.617
2026-01-10 13:17:20,125: WER lens: avg_true_words=6.16 avg_pred_words=6.00 max_pred_words=12
2026-01-10 13:17:20,125: t15.2023.08.13 val PER: 0.3202
2026-01-10 13:17:20,125: t15.2023.08.18 val PER: 0.2833
2026-01-10 13:17:20,125: t15.2023.08.20 val PER: 0.2740
2026-01-10 13:17:20,125: t15.2023.08.25 val PER: 0.2455
2026-01-10 13:17:20,126: t15.2023.08.27 val PER: 0.3794
2026-01-10 13:17:20,126: t15.2023.09.01 val PER: 0.2589
2026-01-10 13:17:20,126: t15.2023.09.03 val PER: 0.3409
2026-01-10 13:17:20,126: t15.2023.09.24 val PER: 0.2888
2026-01-10 13:17:20,126: t15.2023.09.29 val PER: 0.3012
2026-01-10 13:17:20,126: t15.2023.10.01 val PER: 0.3250
2026-01-10 13:17:20,126: t15.2023.10.06 val PER: 0.2745
2026-01-10 13:17:20,126: t15.2023.10.08 val PER: 0.3735
2026-01-10 13:17:20,126: t15.2023.10.13 val PER: 0.3863
2026-01-10 13:17:20,126: t15.2023.10.15 val PER: 0.3144
2026-01-10 13:17:20,126: t15.2023.10.20 val PER: 0.2886
2026-01-10 13:17:20,126: t15.2023.10.22 val PER: 0.2706
2026-01-10 13:17:20,127: t15.2023.11.03 val PER: 0.3304
2026-01-10 13:17:20,127: t15.2023.11.04 val PER: 0.1570
2026-01-10 13:17:20,127: t15.2023.11.17 val PER: 0.2177
2026-01-10 13:17:20,127: t15.2023.11.19 val PER: 0.1816
2026-01-10 13:17:20,127: t15.2023.11.26 val PER: 0.3688
2026-01-10 13:17:20,127: t15.2023.12.03 val PER: 0.3183
2026-01-10 13:17:20,127: t15.2023.12.08 val PER: 0.3415
2026-01-10 13:17:20,128: t15.2023.12.10 val PER: 0.2930
2026-01-10 13:17:20,128: t15.2023.12.17 val PER: 0.3857
2026-01-10 13:17:20,128: t15.2023.12.29 val PER: 0.3693
2026-01-10 13:17:20,128: t15.2024.02.25 val PER: 0.2823
2026-01-10 13:17:20,128: t15.2024.03.08 val PER: 0.4083
2026-01-10 13:17:20,128: t15.2024.03.15 val PER: 0.3927
2026-01-10 13:17:20,129: t15.2024.03.17 val PER: 0.3828
2026-01-10 13:17:20,129: t15.2024.05.10 val PER: 0.3715
2026-01-10 13:17:20,129: t15.2024.06.14 val PER: 0.3659
2026-01-10 13:17:20,129: t15.2024.07.19 val PER: 0.4977
2026-01-10 13:17:20,129: t15.2024.07.21 val PER: 0.3034
2026-01-10 13:17:20,129: t15.2024.07.28 val PER: 0.3471
2026-01-10 13:17:20,129: t15.2025.01.10 val PER: 0.5289
2026-01-10 13:17:20,130: t15.2025.01.12 val PER: 0.4065
2026-01-10 13:17:20,130: t15.2025.03.14 val PER: 0.5740
2026-01-10 13:17:20,130: t15.2025.03.16 val PER: 0.4332
2026-01-10 13:17:20,130: t15.2025.03.30 val PER: 0.5920
2026-01-10 13:17:20,130: t15.2025.04.13 val PER: 0.4679
2026-01-10 13:17:20,188: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_5000
2026-01-10 13:19:50,708: Train batch 5200: loss: 33.98 grad norm: 68.41 time: 0.821
2026-01-10 13:22:22,257: Train batch 5400: loss: 39.79 grad norm: 80.03 time: 0.727
2026-01-10 13:23:34,601: Running test after training batch: 5500
2026-01-10 13:23:34,844: WER debug GT example: You can see the code at this point as well.
2026-01-10 13:23:53,474: WER debug example
  GT : you can see the code at this point as well
  PR : ou and ease utt uhde at it otte is l
2026-01-10 13:23:53,507: WER debug example
  GT : how does it keep the cost down
  PR : houde oz it oop utt os and
2026-01-10 13:23:55,206: Val batch 5500: PER (avg): 0.3377 CTC Loss (avg): 32.5783 WER(1gram): 89.34% (n=64) time: 20.605
2026-01-10 13:23:55,206: WER lens: avg_true_words=6.16 avg_pred_words=5.97 max_pred_words=11
2026-01-10 13:23:55,207: t15.2023.08.13 val PER: 0.3139
2026-01-10 13:23:55,207: t15.2023.08.18 val PER: 0.2791
2026-01-10 13:23:55,207: t15.2023.08.20 val PER: 0.2637
2026-01-10 13:23:55,207: t15.2023.08.25 val PER: 0.2304
2026-01-10 13:23:55,207: t15.2023.08.27 val PER: 0.3730
2026-01-10 13:23:55,207: t15.2023.09.01 val PER: 0.2476
2026-01-10 13:23:55,207: t15.2023.09.03 val PER: 0.3373
2026-01-10 13:23:55,207: t15.2023.09.24 val PER: 0.2779
2026-01-10 13:23:55,207: t15.2023.09.29 val PER: 0.2904
2026-01-10 13:23:55,207: t15.2023.10.01 val PER: 0.3210
2026-01-10 13:23:55,207: t15.2023.10.06 val PER: 0.2530
2026-01-10 13:23:55,208: t15.2023.10.08 val PER: 0.3667
2026-01-10 13:23:55,208: t15.2023.10.13 val PER: 0.3794
2026-01-10 13:23:55,208: t15.2023.10.15 val PER: 0.2933
2026-01-10 13:23:55,208: t15.2023.10.20 val PER: 0.2852
2026-01-10 13:23:55,208: t15.2023.10.22 val PER: 0.2673
2026-01-10 13:23:55,208: t15.2023.11.03 val PER: 0.3141
2026-01-10 13:23:55,208: t15.2023.11.04 val PER: 0.1570
2026-01-10 13:23:55,208: t15.2023.11.17 val PER: 0.2100
2026-01-10 13:23:55,208: t15.2023.11.19 val PER: 0.1617
2026-01-10 13:23:55,208: t15.2023.11.26 val PER: 0.3565
2026-01-10 13:23:55,208: t15.2023.12.03 val PER: 0.2983
2026-01-10 13:23:55,208: t15.2023.12.08 val PER: 0.3216
2026-01-10 13:23:55,208: t15.2023.12.10 val PER: 0.2957
2026-01-10 13:23:55,208: t15.2023.12.17 val PER: 0.3638
2026-01-10 13:23:55,208: t15.2023.12.29 val PER: 0.3466
2026-01-10 13:23:55,209: t15.2024.02.25 val PER: 0.2739
2026-01-10 13:23:55,209: t15.2024.03.08 val PER: 0.3997
2026-01-10 13:23:55,209: t15.2024.03.15 val PER: 0.3859
2026-01-10 13:23:55,209: t15.2024.03.17 val PER: 0.3591
2026-01-10 13:23:55,209: t15.2024.05.10 val PER: 0.3744
2026-01-10 13:23:55,209: t15.2024.06.14 val PER: 0.3249
2026-01-10 13:23:55,209: t15.2024.07.19 val PER: 0.4786
2026-01-10 13:23:55,209: t15.2024.07.21 val PER: 0.2993
2026-01-10 13:23:55,209: t15.2024.07.28 val PER: 0.3279
2026-01-10 13:23:55,209: t15.2025.01.10 val PER: 0.5234
2026-01-10 13:23:55,209: t15.2025.01.12 val PER: 0.3888
2026-01-10 13:23:55,209: t15.2025.03.14 val PER: 0.5577
2026-01-10 13:23:55,209: t15.2025.03.16 val PER: 0.4149
2026-01-10 13:23:55,209: t15.2025.03.30 val PER: 0.5632
2026-01-10 13:23:55,209: t15.2025.04.13 val PER: 0.4237
2026-01-10 13:23:55,266: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_5500
2026-01-10 13:25:12,367: Train batch 5600: loss: 32.61 grad norm: 67.56 time: 0.849
2026-01-10 13:27:46,068: Train batch 5800: loss: 30.98 grad norm: 73.89 time: 0.621
2026-01-10 13:30:15,213: Train batch 6000: loss: 39.16 grad norm: 76.61 time: 0.890
2026-01-10 13:30:15,214: Running test after training batch: 6000
2026-01-10 13:30:15,440: WER debug GT example: You can see the code at this point as well.
2026-01-10 13:30:33,182: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it otte is l
2026-01-10 13:30:33,215: WER debug example
  GT : how does it keep the cost down
  PR : out is it oop utt os aunt
2026-01-10 13:30:34,924: Val batch 6000: PER (avg): 0.3265 CTC Loss (avg): 31.6302 WER(1gram): 88.83% (n=64) time: 19.708
2026-01-10 13:30:34,924: WER lens: avg_true_words=6.16 avg_pred_words=5.91 max_pred_words=10
2026-01-10 13:30:34,924: t15.2023.08.13 val PER: 0.2973
2026-01-10 13:30:34,924: t15.2023.08.18 val PER: 0.2691
2026-01-10 13:30:34,924: t15.2023.08.20 val PER: 0.2542
2026-01-10 13:30:34,924: t15.2023.08.25 val PER: 0.2169
2026-01-10 13:30:34,924: t15.2023.08.27 val PER: 0.3617
2026-01-10 13:30:34,924: t15.2023.09.01 val PER: 0.2346
2026-01-10 13:30:34,925: t15.2023.09.03 val PER: 0.3207
2026-01-10 13:30:34,925: t15.2023.09.24 val PER: 0.2755
2026-01-10 13:30:34,925: t15.2023.09.29 val PER: 0.2776
2026-01-10 13:30:34,925: t15.2023.10.01 val PER: 0.3038
2026-01-10 13:30:34,925: t15.2023.10.06 val PER: 0.2454
2026-01-10 13:30:34,925: t15.2023.10.08 val PER: 0.3545
2026-01-10 13:30:34,925: t15.2023.10.13 val PER: 0.3755
2026-01-10 13:30:34,925: t15.2023.10.15 val PER: 0.2920
2026-01-10 13:30:34,925: t15.2023.10.20 val PER: 0.2953
2026-01-10 13:30:34,925: t15.2023.10.22 val PER: 0.2606
2026-01-10 13:30:34,926: t15.2023.11.03 val PER: 0.3087
2026-01-10 13:30:34,926: t15.2023.11.04 val PER: 0.1297
2026-01-10 13:30:34,926: t15.2023.11.17 val PER: 0.2037
2026-01-10 13:30:34,926: t15.2023.11.19 val PER: 0.1677
2026-01-10 13:30:34,926: t15.2023.11.26 val PER: 0.3558
2026-01-10 13:30:34,926: t15.2023.12.03 val PER: 0.2910
2026-01-10 13:30:34,926: t15.2023.12.08 val PER: 0.3129
2026-01-10 13:30:34,926: t15.2023.12.10 val PER: 0.2838
2026-01-10 13:30:34,926: t15.2023.12.17 val PER: 0.3555
2026-01-10 13:30:34,926: t15.2023.12.29 val PER: 0.3329
2026-01-10 13:30:34,926: t15.2024.02.25 val PER: 0.2640
2026-01-10 13:30:34,926: t15.2024.03.08 val PER: 0.3969
2026-01-10 13:30:34,926: t15.2024.03.15 val PER: 0.3734
2026-01-10 13:30:34,926: t15.2024.03.17 val PER: 0.3375
2026-01-10 13:30:34,926: t15.2024.05.10 val PER: 0.3373
2026-01-10 13:30:34,927: t15.2024.06.14 val PER: 0.3249
2026-01-10 13:30:34,927: t15.2024.07.19 val PER: 0.4595
2026-01-10 13:30:34,927: t15.2024.07.21 val PER: 0.2862
2026-01-10 13:30:34,927: t15.2024.07.28 val PER: 0.3125
2026-01-10 13:30:34,927: t15.2025.01.10 val PER: 0.5124
2026-01-10 13:30:34,927: t15.2025.01.12 val PER: 0.3711
2026-01-10 13:30:34,927: t15.2025.03.14 val PER: 0.5311
2026-01-10 13:30:34,927: t15.2025.03.16 val PER: 0.4123
2026-01-10 13:30:34,927: t15.2025.03.30 val PER: 0.5356
2026-01-10 13:30:34,927: t15.2025.04.13 val PER: 0.4223
2026-01-10 13:30:34,979: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_6000
2026-01-10 13:32:59,939: Train batch 6200: loss: 31.03 grad norm: 71.68 time: 0.994
2026-01-10 13:35:25,835: Train batch 6400: loss: 27.66 grad norm: 67.11 time: 0.651
2026-01-10 13:36:36,703: Running test after training batch: 6500
2026-01-10 13:36:36,930: WER debug GT example: You can see the code at this point as well.
2026-01-10 13:36:54,621: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it otte is ill
2026-01-10 13:36:54,653: WER debug example
  GT : how does it keep the cost down
  PR : houde is it oop uthe os aunt
2026-01-10 13:36:56,362: Val batch 6500: PER (avg): 0.3180 CTC Loss (avg): 30.9314 WER(1gram): 88.32% (n=64) time: 19.658
2026-01-10 13:36:56,362: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=12
2026-01-10 13:36:56,363: t15.2023.08.13 val PER: 0.2931
2026-01-10 13:36:56,363: t15.2023.08.18 val PER: 0.2548
2026-01-10 13:36:56,363: t15.2023.08.20 val PER: 0.2526
2026-01-10 13:36:56,363: t15.2023.08.25 val PER: 0.2154
2026-01-10 13:36:56,363: t15.2023.08.27 val PER: 0.3553
2026-01-10 13:36:56,363: t15.2023.09.01 val PER: 0.2175
2026-01-10 13:36:56,363: t15.2023.09.03 val PER: 0.3088
2026-01-10 13:36:56,363: t15.2023.09.24 val PER: 0.2694
2026-01-10 13:36:56,363: t15.2023.09.29 val PER: 0.2668
2026-01-10 13:36:56,363: t15.2023.10.01 val PER: 0.3091
2026-01-10 13:36:56,363: t15.2023.10.06 val PER: 0.2336
2026-01-10 13:36:56,363: t15.2023.10.08 val PER: 0.3410
2026-01-10 13:36:56,363: t15.2023.10.13 val PER: 0.3561
2026-01-10 13:36:56,364: t15.2023.10.15 val PER: 0.2835
2026-01-10 13:36:56,364: t15.2023.10.20 val PER: 0.2584
2026-01-10 13:36:56,364: t15.2023.10.22 val PER: 0.2517
2026-01-10 13:36:56,364: t15.2023.11.03 val PER: 0.3039
2026-01-10 13:36:56,364: t15.2023.11.04 val PER: 0.1365
2026-01-10 13:36:56,364: t15.2023.11.17 val PER: 0.1944
2026-01-10 13:36:56,364: t15.2023.11.19 val PER: 0.1657
2026-01-10 13:36:56,364: t15.2023.11.26 val PER: 0.3355
2026-01-10 13:36:56,364: t15.2023.12.03 val PER: 0.2826
2026-01-10 13:36:56,364: t15.2023.12.08 val PER: 0.2996
2026-01-10 13:36:56,364: t15.2023.12.10 val PER: 0.2654
2026-01-10 13:36:56,364: t15.2023.12.17 val PER: 0.3503
2026-01-10 13:36:56,364: t15.2023.12.29 val PER: 0.3198
2026-01-10 13:36:56,364: t15.2024.02.25 val PER: 0.2598
2026-01-10 13:36:56,364: t15.2024.03.08 val PER: 0.3898
2026-01-10 13:36:56,364: t15.2024.03.15 val PER: 0.3527
2026-01-10 13:36:56,364: t15.2024.03.17 val PER: 0.3319
2026-01-10 13:36:56,365: t15.2024.05.10 val PER: 0.3328
2026-01-10 13:36:56,365: t15.2024.06.14 val PER: 0.3438
2026-01-10 13:36:56,365: t15.2024.07.19 val PER: 0.4608
2026-01-10 13:36:56,365: t15.2024.07.21 val PER: 0.2786
2026-01-10 13:36:56,365: t15.2024.07.28 val PER: 0.3022
2026-01-10 13:36:56,365: t15.2025.01.10 val PER: 0.4986
2026-01-10 13:36:56,365: t15.2025.01.12 val PER: 0.3734
2026-01-10 13:36:56,366: t15.2025.03.14 val PER: 0.5414
2026-01-10 13:36:56,366: t15.2025.03.16 val PER: 0.3835
2026-01-10 13:36:56,366: t15.2025.03.30 val PER: 0.5402
2026-01-10 13:36:56,366: t15.2025.04.13 val PER: 0.4037
2026-01-10 13:36:56,419: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_6500
2026-01-10 13:38:07,847: Train batch 6600: loss: 27.73 grad norm: 71.08 time: 0.647
2026-01-10 13:40:35,939: Train batch 6800: loss: 33.41 grad norm: 69.14 time: 0.676
2026-01-10 13:43:07,478: Train batch 7000: loss: 30.28 grad norm: 68.55 time: 0.686
2026-01-10 13:43:07,480: Running test after training batch: 7000
2026-01-10 13:43:07,733: WER debug GT example: You can see the code at this point as well.
2026-01-10 13:43:25,605: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it otte is l
2026-01-10 13:43:25,638: WER debug example
  GT : how does it keep the cost down
  PR : houde is it oop uthe os end
2026-01-10 13:43:27,367: Val batch 7000: PER (avg): 0.3110 CTC Loss (avg): 30.0062 WER(1gram): 88.83% (n=64) time: 19.886
2026-01-10 13:43:27,367: WER lens: avg_true_words=6.16 avg_pred_words=5.95 max_pred_words=11
2026-01-10 13:43:27,367: t15.2023.08.13 val PER: 0.2900
2026-01-10 13:43:27,367: t15.2023.08.18 val PER: 0.2498
2026-01-10 13:43:27,368: t15.2023.08.20 val PER: 0.2383
2026-01-10 13:43:27,368: t15.2023.08.25 val PER: 0.2093
2026-01-10 13:43:27,368: t15.2023.08.27 val PER: 0.3585
2026-01-10 13:43:27,368: t15.2023.09.01 val PER: 0.2159
2026-01-10 13:43:27,368: t15.2023.09.03 val PER: 0.3124
2026-01-10 13:43:27,368: t15.2023.09.24 val PER: 0.2512
2026-01-10 13:43:27,368: t15.2023.09.29 val PER: 0.2668
2026-01-10 13:43:27,368: t15.2023.10.01 val PER: 0.2952
2026-01-10 13:43:27,368: t15.2023.10.06 val PER: 0.2207
2026-01-10 13:43:27,368: t15.2023.10.08 val PER: 0.3424
2026-01-10 13:43:27,368: t15.2023.10.13 val PER: 0.3538
2026-01-10 13:43:27,368: t15.2023.10.15 val PER: 0.2841
2026-01-10 13:43:27,368: t15.2023.10.20 val PER: 0.2517
2026-01-10 13:43:27,368: t15.2023.10.22 val PER: 0.2483
2026-01-10 13:43:27,368: t15.2023.11.03 val PER: 0.2958
2026-01-10 13:43:27,368: t15.2023.11.04 val PER: 0.0887
2026-01-10 13:43:27,369: t15.2023.11.17 val PER: 0.1788
2026-01-10 13:43:27,369: t15.2023.11.19 val PER: 0.1517
2026-01-10 13:43:27,369: t15.2023.11.26 val PER: 0.3268
2026-01-10 13:43:27,369: t15.2023.12.03 val PER: 0.2679
2026-01-10 13:43:27,369: t15.2023.12.08 val PER: 0.3023
2026-01-10 13:43:27,369: t15.2023.12.10 val PER: 0.2681
2026-01-10 13:43:27,369: t15.2023.12.17 val PER: 0.3482
2026-01-10 13:43:27,369: t15.2023.12.29 val PER: 0.3150
2026-01-10 13:43:27,369: t15.2024.02.25 val PER: 0.2626
2026-01-10 13:43:27,369: t15.2024.03.08 val PER: 0.3713
2026-01-10 13:43:27,369: t15.2024.03.15 val PER: 0.3527
2026-01-10 13:43:27,369: t15.2024.03.17 val PER: 0.3347
2026-01-10 13:43:27,369: t15.2024.05.10 val PER: 0.3224
2026-01-10 13:43:27,369: t15.2024.06.14 val PER: 0.3155
2026-01-10 13:43:27,369: t15.2024.07.19 val PER: 0.4285
2026-01-10 13:43:27,370: t15.2024.07.21 val PER: 0.2669
2026-01-10 13:43:27,370: t15.2024.07.28 val PER: 0.3044
2026-01-10 13:43:27,370: t15.2025.01.10 val PER: 0.4738
2026-01-10 13:43:27,370: t15.2025.01.12 val PER: 0.3626
2026-01-10 13:43:27,370: t15.2025.03.14 val PER: 0.5266
2026-01-10 13:43:27,370: t15.2025.03.16 val PER: 0.3835
2026-01-10 13:43:27,370: t15.2025.03.30 val PER: 0.5356
2026-01-10 13:43:27,370: t15.2025.04.13 val PER: 0.4108
2026-01-10 13:43:27,424: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_7000
2026-01-10 13:45:56,958: Train batch 7200: loss: 29.44 grad norm: 75.96 time: 0.695
2026-01-10 13:48:23,229: Train batch 7400: loss: 36.17 grad norm: 84.72 time: 0.902
2026-01-10 13:49:37,352: Running test after training batch: 7500
2026-01-10 13:49:37,594: WER debug GT example: You can see the code at this point as well.
2026-01-10 13:49:55,278: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it otte is l
2026-01-10 13:49:55,308: WER debug example
  GT : how does it keep the cost down
  PR : houde is it oop uthe os ent
2026-01-10 13:49:56,959: Val batch 7500: PER (avg): 0.3039 CTC Loss (avg): 29.3593 WER(1gram): 88.07% (n=64) time: 19.605
2026-01-10 13:49:56,959: WER lens: avg_true_words=6.16 avg_pred_words=6.03 max_pred_words=12
2026-01-10 13:49:56,959: t15.2023.08.13 val PER: 0.2859
2026-01-10 13:49:56,959: t15.2023.08.18 val PER: 0.2372
2026-01-10 13:49:56,960: t15.2023.08.20 val PER: 0.2288
2026-01-10 13:49:56,960: t15.2023.08.25 val PER: 0.2003
2026-01-10 13:49:56,960: t15.2023.08.27 val PER: 0.3296
2026-01-10 13:49:56,960: t15.2023.09.01 val PER: 0.2021
2026-01-10 13:49:56,960: t15.2023.09.03 val PER: 0.3076
2026-01-10 13:49:56,960: t15.2023.09.24 val PER: 0.2439
2026-01-10 13:49:56,960: t15.2023.09.29 val PER: 0.2597
2026-01-10 13:49:56,960: t15.2023.10.01 val PER: 0.2880
2026-01-10 13:49:56,960: t15.2023.10.06 val PER: 0.2088
2026-01-10 13:49:56,960: t15.2023.10.08 val PER: 0.3315
2026-01-10 13:49:56,960: t15.2023.10.13 val PER: 0.3538
2026-01-10 13:49:56,960: t15.2023.10.15 val PER: 0.2762
2026-01-10 13:49:56,960: t15.2023.10.20 val PER: 0.2617
2026-01-10 13:49:56,960: t15.2023.10.22 val PER: 0.2316
2026-01-10 13:49:56,961: t15.2023.11.03 val PER: 0.2951
2026-01-10 13:49:56,961: t15.2023.11.04 val PER: 0.0853
2026-01-10 13:49:56,961: t15.2023.11.17 val PER: 0.1851
2026-01-10 13:49:56,961: t15.2023.11.19 val PER: 0.1337
2026-01-10 13:49:56,961: t15.2023.11.26 val PER: 0.3362
2026-01-10 13:49:56,961: t15.2023.12.03 val PER: 0.2616
2026-01-10 13:49:56,961: t15.2023.12.08 val PER: 0.3029
2026-01-10 13:49:56,961: t15.2023.12.10 val PER: 0.2615
2026-01-10 13:49:56,961: t15.2023.12.17 val PER: 0.3389
2026-01-10 13:49:56,961: t15.2023.12.29 val PER: 0.3082
2026-01-10 13:49:56,961: t15.2024.02.25 val PER: 0.2416
2026-01-10 13:49:56,961: t15.2024.03.08 val PER: 0.3698
2026-01-10 13:49:56,961: t15.2024.03.15 val PER: 0.3546
2026-01-10 13:49:56,961: t15.2024.03.17 val PER: 0.3187
2026-01-10 13:49:56,961: t15.2024.05.10 val PER: 0.3135
2026-01-10 13:49:56,961: t15.2024.06.14 val PER: 0.3155
2026-01-10 13:49:56,962: t15.2024.07.19 val PER: 0.4291
2026-01-10 13:49:56,962: t15.2024.07.21 val PER: 0.2676
2026-01-10 13:49:56,962: t15.2024.07.28 val PER: 0.2993
2026-01-10 13:49:56,962: t15.2025.01.10 val PER: 0.4683
2026-01-10 13:49:56,962: t15.2025.01.12 val PER: 0.3541
2026-01-10 13:49:56,962: t15.2025.03.14 val PER: 0.5178
2026-01-10 13:49:56,962: t15.2025.03.16 val PER: 0.3796
2026-01-10 13:49:56,962: t15.2025.03.30 val PER: 0.4989
2026-01-10 13:49:56,962: t15.2025.04.13 val PER: 0.3823
2026-01-10 13:49:57,018: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_7500
2026-01-10 13:51:11,089: Train batch 7600: loss: 29.44 grad norm: 74.14 time: 0.970
2026-01-10 13:53:42,450: Train batch 7800: loss: 34.85 grad norm: 105.27 time: 0.842
2026-01-10 13:56:22,150: Train batch 8000: loss: 31.64 grad norm: 78.17 time: 1.055
2026-01-10 13:56:22,151: Running test after training batch: 8000
2026-01-10 13:56:22,420: WER debug GT example: You can see the code at this point as well.
2026-01-10 13:56:40,169: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it utt is l
2026-01-10 13:56:40,198: WER debug example
  GT : how does it keep the cost down
  PR : houde is it oop uthe os end
2026-01-10 13:56:41,895: Val batch 8000: PER (avg): 0.2955 CTC Loss (avg): 28.6244 WER(1gram): 89.34% (n=64) time: 19.742
2026-01-10 13:56:41,895: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=12
2026-01-10 13:56:41,895: t15.2023.08.13 val PER: 0.2713
2026-01-10 13:56:41,896: t15.2023.08.18 val PER: 0.2364
2026-01-10 13:56:41,896: t15.2023.08.20 val PER: 0.2351
2026-01-10 13:56:41,896: t15.2023.08.25 val PER: 0.1867
2026-01-10 13:56:41,896: t15.2023.08.27 val PER: 0.3215
2026-01-10 13:56:41,896: t15.2023.09.01 val PER: 0.1940
2026-01-10 13:56:41,896: t15.2023.09.03 val PER: 0.2957
2026-01-10 13:56:41,896: t15.2023.09.24 val PER: 0.2367
2026-01-10 13:56:41,896: t15.2023.09.29 val PER: 0.2502
2026-01-10 13:56:41,896: t15.2023.10.01 val PER: 0.2939
2026-01-10 13:56:41,896: t15.2023.10.06 val PER: 0.2110
2026-01-10 13:56:41,896: t15.2023.10.08 val PER: 0.3139
2026-01-10 13:56:41,896: t15.2023.10.13 val PER: 0.3375
2026-01-10 13:56:41,896: t15.2023.10.15 val PER: 0.2690
2026-01-10 13:56:41,897: t15.2023.10.20 val PER: 0.2517
2026-01-10 13:56:41,897: t15.2023.10.22 val PER: 0.2394
2026-01-10 13:56:41,897: t15.2023.11.03 val PER: 0.2877
2026-01-10 13:56:41,897: t15.2023.11.04 val PER: 0.0819
2026-01-10 13:56:41,897: t15.2023.11.17 val PER: 0.1820
2026-01-10 13:56:41,897: t15.2023.11.19 val PER: 0.1557
2026-01-10 13:56:41,897: t15.2023.11.26 val PER: 0.3152
2026-01-10 13:56:41,897: t15.2023.12.03 val PER: 0.2532
2026-01-10 13:56:41,897: t15.2023.12.08 val PER: 0.2916
2026-01-10 13:56:41,897: t15.2023.12.10 val PER: 0.2589
2026-01-10 13:56:41,897: t15.2023.12.17 val PER: 0.3264
2026-01-10 13:56:41,897: t15.2023.12.29 val PER: 0.2889
2026-01-10 13:56:41,897: t15.2024.02.25 val PER: 0.2331
2026-01-10 13:56:41,897: t15.2024.03.08 val PER: 0.3713
2026-01-10 13:56:41,897: t15.2024.03.15 val PER: 0.3390
2026-01-10 13:56:41,898: t15.2024.03.17 val PER: 0.3047
2026-01-10 13:56:41,898: t15.2024.05.10 val PER: 0.3061
2026-01-10 13:56:41,898: t15.2024.06.14 val PER: 0.2981
2026-01-10 13:56:41,898: t15.2024.07.19 val PER: 0.4041
2026-01-10 13:56:41,898: t15.2024.07.21 val PER: 0.2490
2026-01-10 13:56:41,898: t15.2024.07.28 val PER: 0.2809
2026-01-10 13:56:41,898: t15.2025.01.10 val PER: 0.4669
2026-01-10 13:56:41,898: t15.2025.01.12 val PER: 0.3487
2026-01-10 13:56:41,898: t15.2025.03.14 val PER: 0.5000
2026-01-10 13:56:41,898: t15.2025.03.16 val PER: 0.3927
2026-01-10 13:56:41,898: t15.2025.03.30 val PER: 0.5000
2026-01-10 13:56:41,898: t15.2025.04.13 val PER: 0.3966
2026-01-10 13:56:41,953: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_8000
2026-01-10 13:59:16,843: Train batch 8200: loss: 31.78 grad norm: 72.18 time: 0.722
2026-01-10 14:01:53,768: Train batch 8400: loss: 19.73 grad norm: 57.96 time: 1.075
2026-01-10 14:03:10,218: Running test after training batch: 8500
2026-01-10 14:03:10,461: WER debug GT example: You can see the code at this point as well.
2026-01-10 14:03:28,247: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it utt is l
2026-01-10 14:03:28,279: WER debug example
  GT : how does it keep the cost down
  PR : houde is it oop uthe os end
2026-01-10 14:03:30,008: Val batch 8500: PER (avg): 0.2915 CTC Loss (avg): 28.5060 WER(1gram): 88.83% (n=64) time: 19.789
2026-01-10 14:03:30,008: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=11
2026-01-10 14:03:30,008: t15.2023.08.13 val PER: 0.2640
2026-01-10 14:03:30,008: t15.2023.08.18 val PER: 0.2305
2026-01-10 14:03:30,008: t15.2023.08.20 val PER: 0.2327
2026-01-10 14:03:30,009: t15.2023.08.25 val PER: 0.1958
2026-01-10 14:03:30,009: t15.2023.08.27 val PER: 0.3280
2026-01-10 14:03:30,009: t15.2023.09.01 val PER: 0.1948
2026-01-10 14:03:30,009: t15.2023.09.03 val PER: 0.2922
2026-01-10 14:03:30,009: t15.2023.09.24 val PER: 0.2342
2026-01-10 14:03:30,009: t15.2023.09.29 val PER: 0.2502
2026-01-10 14:03:30,009: t15.2023.10.01 val PER: 0.2814
2026-01-10 14:03:30,009: t15.2023.10.06 val PER: 0.2078
2026-01-10 14:03:30,009: t15.2023.10.08 val PER: 0.3234
2026-01-10 14:03:30,010: t15.2023.10.13 val PER: 0.3274
2026-01-10 14:03:30,010: t15.2023.10.15 val PER: 0.2571
2026-01-10 14:03:30,010: t15.2023.10.20 val PER: 0.2483
2026-01-10 14:03:30,010: t15.2023.10.22 val PER: 0.2383
2026-01-10 14:03:30,010: t15.2023.11.03 val PER: 0.2775
2026-01-10 14:03:30,010: t15.2023.11.04 val PER: 0.0819
2026-01-10 14:03:30,010: t15.2023.11.17 val PER: 0.1695
2026-01-10 14:03:30,010: t15.2023.11.19 val PER: 0.1477
2026-01-10 14:03:30,010: t15.2023.11.26 val PER: 0.3138
2026-01-10 14:03:30,010: t15.2023.12.03 val PER: 0.2584
2026-01-10 14:03:30,010: t15.2023.12.08 val PER: 0.2856
2026-01-10 14:03:30,011: t15.2023.12.10 val PER: 0.2484
2026-01-10 14:03:30,011: t15.2023.12.17 val PER: 0.3222
2026-01-10 14:03:30,011: t15.2023.12.29 val PER: 0.2931
2026-01-10 14:03:30,011: t15.2024.02.25 val PER: 0.2317
2026-01-10 14:03:30,011: t15.2024.03.08 val PER: 0.3613
2026-01-10 14:03:30,011: t15.2024.03.15 val PER: 0.3421
2026-01-10 14:03:30,011: t15.2024.03.17 val PER: 0.3075
2026-01-10 14:03:30,011: t15.2024.05.10 val PER: 0.2897
2026-01-10 14:03:30,011: t15.2024.06.14 val PER: 0.2823
2026-01-10 14:03:30,011: t15.2024.07.19 val PER: 0.4021
2026-01-10 14:03:30,011: t15.2024.07.21 val PER: 0.2517
2026-01-10 14:03:30,011: t15.2024.07.28 val PER: 0.2706
2026-01-10 14:03:30,012: t15.2025.01.10 val PER: 0.4601
2026-01-10 14:03:30,012: t15.2025.01.12 val PER: 0.3464
2026-01-10 14:03:30,012: t15.2025.03.14 val PER: 0.5237
2026-01-10 14:03:30,012: t15.2025.03.16 val PER: 0.3743
2026-01-10 14:03:30,012: t15.2025.03.30 val PER: 0.4862
2026-01-10 14:03:30,012: t15.2025.04.13 val PER: 0.3723
2026-01-10 14:03:30,066: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_8500
2026-01-10 14:04:46,618: Train batch 8600: loss: 30.72 grad norm: 78.52 time: 0.661
2026-01-10 14:07:14,974: Train batch 8800: loss: 44.01 grad norm: 90.00 time: 1.158
2026-01-10 14:09:42,538: Train batch 9000: loss: 34.72 grad norm: 81.53 time: 0.548
2026-01-10 14:09:42,539: Running test after training batch: 9000
2026-01-10 14:09:42,764: WER debug GT example: You can see the code at this point as well.
2026-01-10 14:10:00,490: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it utt is l
2026-01-10 14:10:00,518: WER debug example
  GT : how does it keep the cost down
  PR : houde us it oop uthe os ed
2026-01-10 14:10:02,205: Val batch 9000: PER (avg): 0.2898 CTC Loss (avg): 27.9379 WER(1gram): 89.34% (n=64) time: 19.665
2026-01-10 14:10:02,205: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=12
2026-01-10 14:10:02,205: t15.2023.08.13 val PER: 0.2578
2026-01-10 14:10:02,205: t15.2023.08.18 val PER: 0.2230
2026-01-10 14:10:02,205: t15.2023.08.20 val PER: 0.2153
2026-01-10 14:10:02,205: t15.2023.08.25 val PER: 0.2003
2026-01-10 14:10:02,205: t15.2023.08.27 val PER: 0.3312
2026-01-10 14:10:02,205: t15.2023.09.01 val PER: 0.1859
2026-01-10 14:10:02,206: t15.2023.09.03 val PER: 0.2945
2026-01-10 14:10:02,206: t15.2023.09.24 val PER: 0.2306
2026-01-10 14:10:02,206: t15.2023.09.29 val PER: 0.2399
2026-01-10 14:10:02,206: t15.2023.10.01 val PER: 0.2853
2026-01-10 14:10:02,206: t15.2023.10.06 val PER: 0.2099
2026-01-10 14:10:02,206: t15.2023.10.08 val PER: 0.3139
2026-01-10 14:10:02,206: t15.2023.10.13 val PER: 0.3313
2026-01-10 14:10:02,206: t15.2023.10.15 val PER: 0.2657
2026-01-10 14:10:02,206: t15.2023.10.20 val PER: 0.2383
2026-01-10 14:10:02,206: t15.2023.10.22 val PER: 0.2327
2026-01-10 14:10:02,206: t15.2023.11.03 val PER: 0.2782
2026-01-10 14:10:02,206: t15.2023.11.04 val PER: 0.0819
2026-01-10 14:10:02,206: t15.2023.11.17 val PER: 0.1695
2026-01-10 14:10:02,207: t15.2023.11.19 val PER: 0.1377
2026-01-10 14:10:02,207: t15.2023.11.26 val PER: 0.3130
2026-01-10 14:10:02,207: t15.2023.12.03 val PER: 0.2447
2026-01-10 14:10:02,207: t15.2023.12.08 val PER: 0.2823
2026-01-10 14:10:02,207: t15.2023.12.10 val PER: 0.2576
2026-01-10 14:10:02,207: t15.2023.12.17 val PER: 0.3181
2026-01-10 14:10:02,207: t15.2023.12.29 val PER: 0.2903
2026-01-10 14:10:02,207: t15.2024.02.25 val PER: 0.2261
2026-01-10 14:10:02,207: t15.2024.03.08 val PER: 0.3599
2026-01-10 14:10:02,208: t15.2024.03.15 val PER: 0.3421
2026-01-10 14:10:02,208: t15.2024.03.17 val PER: 0.3110
2026-01-10 14:10:02,208: t15.2024.05.10 val PER: 0.3031
2026-01-10 14:10:02,208: t15.2024.06.14 val PER: 0.2918
2026-01-10 14:10:02,208: t15.2024.07.19 val PER: 0.4080
2026-01-10 14:10:02,208: t15.2024.07.21 val PER: 0.2476
2026-01-10 14:10:02,208: t15.2024.07.28 val PER: 0.2846
2026-01-10 14:10:02,209: t15.2025.01.10 val PER: 0.4559
2026-01-10 14:10:02,209: t15.2025.01.12 val PER: 0.3341
2026-01-10 14:10:02,209: t15.2025.03.14 val PER: 0.5059
2026-01-10 14:10:02,209: t15.2025.03.16 val PER: 0.3783
2026-01-10 14:10:02,209: t15.2025.03.30 val PER: 0.4782
2026-01-10 14:10:02,209: t15.2025.04.13 val PER: 0.3723
2026-01-10 14:10:02,263: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_9000
2026-01-10 14:12:29,529: Train batch 9200: loss: 18.70 grad norm: 55.72 time: 0.796
2026-01-10 14:15:06,285: Train batch 9400: loss: 32.93 grad norm: 76.81 time: 0.828
2026-01-10 14:16:24,227: Running test after training batch: 9500
2026-01-10 14:16:24,456: WER debug GT example: You can see the code at this point as well.
2026-01-10 14:16:42,198: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it utt as l
2026-01-10 14:16:42,227: WER debug example
  GT : how does it keep the cost down
  PR : houde us it oop uthe os end
2026-01-10 14:16:43,941: Val batch 9500: PER (avg): 0.2825 CTC Loss (avg): 27.5318 WER(1gram): 88.83% (n=64) time: 19.713
2026-01-10 14:16:43,941: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-10 14:16:43,941: t15.2023.08.13 val PER: 0.2588
2026-01-10 14:16:43,941: t15.2023.08.18 val PER: 0.2179
2026-01-10 14:16:43,941: t15.2023.08.20 val PER: 0.2168
2026-01-10 14:16:43,941: t15.2023.08.25 val PER: 0.1837
2026-01-10 14:16:43,941: t15.2023.08.27 val PER: 0.3264
2026-01-10 14:16:43,941: t15.2023.09.01 val PER: 0.1859
2026-01-10 14:16:43,941: t15.2023.09.03 val PER: 0.2838
2026-01-10 14:16:43,942: t15.2023.09.24 val PER: 0.2330
2026-01-10 14:16:43,942: t15.2023.09.29 val PER: 0.2297
2026-01-10 14:16:43,942: t15.2023.10.01 val PER: 0.2728
2026-01-10 14:16:43,942: t15.2023.10.06 val PER: 0.2045
2026-01-10 14:16:43,942: t15.2023.10.08 val PER: 0.3031
2026-01-10 14:16:43,942: t15.2023.10.13 val PER: 0.3150
2026-01-10 14:16:43,942: t15.2023.10.15 val PER: 0.2531
2026-01-10 14:16:43,942: t15.2023.10.20 val PER: 0.2383
2026-01-10 14:16:43,942: t15.2023.10.22 val PER: 0.2294
2026-01-10 14:16:43,942: t15.2023.11.03 val PER: 0.2822
2026-01-10 14:16:43,942: t15.2023.11.04 val PER: 0.0887
2026-01-10 14:16:43,942: t15.2023.11.17 val PER: 0.1649
2026-01-10 14:16:43,942: t15.2023.11.19 val PER: 0.1397
2026-01-10 14:16:43,942: t15.2023.11.26 val PER: 0.2986
2026-01-10 14:16:43,942: t15.2023.12.03 val PER: 0.2511
2026-01-10 14:16:43,942: t15.2023.12.08 val PER: 0.2756
2026-01-10 14:16:43,943: t15.2023.12.10 val PER: 0.2510
2026-01-10 14:16:43,943: t15.2023.12.17 val PER: 0.3170
2026-01-10 14:16:43,943: t15.2023.12.29 val PER: 0.2773
2026-01-10 14:16:43,943: t15.2024.02.25 val PER: 0.2247
2026-01-10 14:16:43,943: t15.2024.03.08 val PER: 0.3570
2026-01-10 14:16:43,943: t15.2024.03.15 val PER: 0.3315
2026-01-10 14:16:43,943: t15.2024.03.17 val PER: 0.3026
2026-01-10 14:16:43,943: t15.2024.05.10 val PER: 0.2779
2026-01-10 14:16:43,943: t15.2024.06.14 val PER: 0.2855
2026-01-10 14:16:43,943: t15.2024.07.19 val PER: 0.3935
2026-01-10 14:16:43,943: t15.2024.07.21 val PER: 0.2372
2026-01-10 14:16:43,943: t15.2024.07.28 val PER: 0.2721
2026-01-10 14:16:43,943: t15.2025.01.10 val PER: 0.4532
2026-01-10 14:16:43,944: t15.2025.01.12 val PER: 0.3326
2026-01-10 14:16:43,944: t15.2025.03.14 val PER: 0.4926
2026-01-10 14:16:43,944: t15.2025.03.16 val PER: 0.3665
2026-01-10 14:16:43,944: t15.2025.03.30 val PER: 0.4667
2026-01-10 14:16:43,945: t15.2025.04.13 val PER: 0.3652
2026-01-10 14:16:44,010: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_9500
2026-01-10 14:17:58,783: Train batch 9600: loss: 25.99 grad norm: 72.10 time: 0.957
2026-01-10 14:20:24,631: Train batch 9800: loss: 28.90 grad norm: 77.27 time: 0.616
2026-01-10 14:22:52,620: Train batch 10000: loss: 32.07 grad norm: 90.81 time: 0.805
2026-01-10 14:22:52,621: Running test after training batch: 10000
2026-01-10 14:22:52,846: WER debug GT example: You can see the code at this point as well.
2026-01-10 14:23:10,516: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it utt is l
2026-01-10 14:23:10,547: WER debug example
  GT : how does it keep the cost down
  PR : houde is it oop uthe os ed
2026-01-10 14:23:12,286: Val batch 10000: PER (avg): 0.2787 CTC Loss (avg): 27.1880 WER(1gram): 88.83% (n=64) time: 19.664
2026-01-10 14:23:12,286: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=12
2026-01-10 14:23:12,286: t15.2023.08.13 val PER: 0.2474
2026-01-10 14:23:12,286: t15.2023.08.18 val PER: 0.2163
2026-01-10 14:23:12,286: t15.2023.08.20 val PER: 0.2168
2026-01-10 14:23:12,286: t15.2023.08.25 val PER: 0.1867
2026-01-10 14:23:12,287: t15.2023.08.27 val PER: 0.3232
2026-01-10 14:23:12,287: t15.2023.09.01 val PER: 0.1834
2026-01-10 14:23:12,287: t15.2023.09.03 val PER: 0.2981
2026-01-10 14:23:12,287: t15.2023.09.24 val PER: 0.2294
2026-01-10 14:23:12,287: t15.2023.09.29 val PER: 0.2317
2026-01-10 14:23:12,287: t15.2023.10.01 val PER: 0.2761
2026-01-10 14:23:12,287: t15.2023.10.06 val PER: 0.2045
2026-01-10 14:23:12,287: t15.2023.10.08 val PER: 0.2923
2026-01-10 14:23:12,287: t15.2023.10.13 val PER: 0.3165
2026-01-10 14:23:12,287: t15.2023.10.15 val PER: 0.2525
2026-01-10 14:23:12,288: t15.2023.10.20 val PER: 0.2282
2026-01-10 14:23:12,288: t15.2023.10.22 val PER: 0.2238
2026-01-10 14:23:12,288: t15.2023.11.03 val PER: 0.2700
2026-01-10 14:23:12,288: t15.2023.11.04 val PER: 0.0853
2026-01-10 14:23:12,288: t15.2023.11.17 val PER: 0.1617
2026-01-10 14:23:12,288: t15.2023.11.19 val PER: 0.1357
2026-01-10 14:23:12,288: t15.2023.11.26 val PER: 0.3007
2026-01-10 14:23:12,288: t15.2023.12.03 val PER: 0.2521
2026-01-10 14:23:12,288: t15.2023.12.08 val PER: 0.2723
2026-01-10 14:23:12,288: t15.2023.12.10 val PER: 0.2365
2026-01-10 14:23:12,288: t15.2023.12.17 val PER: 0.3129
2026-01-10 14:23:12,288: t15.2023.12.29 val PER: 0.2642
2026-01-10 14:23:12,289: t15.2024.02.25 val PER: 0.2177
2026-01-10 14:23:12,289: t15.2024.03.08 val PER: 0.3499
2026-01-10 14:23:12,289: t15.2024.03.15 val PER: 0.3296
2026-01-10 14:23:12,289: t15.2024.03.17 val PER: 0.2866
2026-01-10 14:23:12,289: t15.2024.05.10 val PER: 0.2793
2026-01-10 14:23:12,289: t15.2024.06.14 val PER: 0.2744
2026-01-10 14:23:12,289: t15.2024.07.19 val PER: 0.3850
2026-01-10 14:23:12,289: t15.2024.07.21 val PER: 0.2310
2026-01-10 14:23:12,289: t15.2024.07.28 val PER: 0.2676
2026-01-10 14:23:12,289: t15.2025.01.10 val PER: 0.4532
2026-01-10 14:23:12,289: t15.2025.01.12 val PER: 0.3326
2026-01-10 14:23:12,290: t15.2025.03.14 val PER: 0.4956
2026-01-10 14:23:12,290: t15.2025.03.16 val PER: 0.3652
2026-01-10 14:23:12,290: t15.2025.03.30 val PER: 0.4552
2026-01-10 14:23:12,290: t15.2025.04.13 val PER: 0.3538
2026-01-10 14:23:12,346: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_10000
2026-01-10 14:25:38,677: Train batch 10200: loss: 23.44 grad norm: 67.40 time: 0.724
2026-01-10 14:28:06,367: Train batch 10400: loss: 24.80 grad norm: 72.40 time: 0.624
2026-01-10 14:29:23,817: Running test after training batch: 10500
2026-01-10 14:29:24,044: WER debug GT example: You can see the code at this point as well.
2026-01-10 14:29:41,668: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it utt as l
2026-01-10 14:29:41,698: WER debug example
  GT : how does it keep the cost down
  PR : houde is it epp uthe us ed
2026-01-10 14:29:43,434: Val batch 10500: PER (avg): 0.2762 CTC Loss (avg): 26.7760 WER(1gram): 88.58% (n=64) time: 19.612
2026-01-10 14:29:43,434: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=12
2026-01-10 14:29:43,434: t15.2023.08.13 val PER: 0.2391
2026-01-10 14:29:43,434: t15.2023.08.18 val PER: 0.2121
2026-01-10 14:29:43,434: t15.2023.08.20 val PER: 0.2256
2026-01-10 14:29:43,435: t15.2023.08.25 val PER: 0.1837
2026-01-10 14:29:43,435: t15.2023.08.27 val PER: 0.3119
2026-01-10 14:29:43,435: t15.2023.09.01 val PER: 0.1826
2026-01-10 14:29:43,435: t15.2023.09.03 val PER: 0.2732
2026-01-10 14:29:43,435: t15.2023.09.24 val PER: 0.2233
2026-01-10 14:29:43,435: t15.2023.09.29 val PER: 0.2380
2026-01-10 14:29:43,435: t15.2023.10.01 val PER: 0.2761
2026-01-10 14:29:43,435: t15.2023.10.06 val PER: 0.2002
2026-01-10 14:29:43,435: t15.2023.10.08 val PER: 0.3099
2026-01-10 14:29:43,436: t15.2023.10.13 val PER: 0.3142
2026-01-10 14:29:43,436: t15.2023.10.15 val PER: 0.2406
2026-01-10 14:29:43,436: t15.2023.10.20 val PER: 0.2517
2026-01-10 14:29:43,436: t15.2023.10.22 val PER: 0.2238
2026-01-10 14:29:43,436: t15.2023.11.03 val PER: 0.2768
2026-01-10 14:29:43,436: t15.2023.11.04 val PER: 0.0853
2026-01-10 14:29:43,436: t15.2023.11.17 val PER: 0.1586
2026-01-10 14:29:43,436: t15.2023.11.19 val PER: 0.1357
2026-01-10 14:29:43,436: t15.2023.11.26 val PER: 0.3022
2026-01-10 14:29:43,436: t15.2023.12.03 val PER: 0.2363
2026-01-10 14:29:43,437: t15.2023.12.08 val PER: 0.2630
2026-01-10 14:29:43,437: t15.2023.12.10 val PER: 0.2392
2026-01-10 14:29:43,437: t15.2023.12.17 val PER: 0.3129
2026-01-10 14:29:43,437: t15.2023.12.29 val PER: 0.2629
2026-01-10 14:29:43,437: t15.2024.02.25 val PER: 0.2135
2026-01-10 14:29:43,437: t15.2024.03.08 val PER: 0.3485
2026-01-10 14:29:43,437: t15.2024.03.15 val PER: 0.3265
2026-01-10 14:29:43,437: t15.2024.03.17 val PER: 0.2810
2026-01-10 14:29:43,437: t15.2024.05.10 val PER: 0.2719
2026-01-10 14:29:43,437: t15.2024.06.14 val PER: 0.2760
2026-01-10 14:29:43,437: t15.2024.07.19 val PER: 0.3744
2026-01-10 14:29:43,437: t15.2024.07.21 val PER: 0.2331
2026-01-10 14:29:43,438: t15.2024.07.28 val PER: 0.2625
2026-01-10 14:29:43,438: t15.2025.01.10 val PER: 0.4573
2026-01-10 14:29:43,438: t15.2025.01.12 val PER: 0.3087
2026-01-10 14:29:43,438: t15.2025.03.14 val PER: 0.4926
2026-01-10 14:29:43,438: t15.2025.03.16 val PER: 0.3560
2026-01-10 14:29:43,438: t15.2025.03.30 val PER: 0.4713
2026-01-10 14:29:43,438: t15.2025.04.13 val PER: 0.3709
2026-01-10 14:29:43,495: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_10500
2026-01-10 14:30:57,041: Train batch 10600: loss: 20.35 grad norm: 60.30 time: 0.758
2026-01-10 14:33:25,892: Train batch 10800: loss: 26.97 grad norm: 79.26 time: 0.630
2026-01-10 14:35:57,827: Train batch 11000: loss: 21.07 grad norm: 66.83 time: 0.763
2026-01-10 14:35:57,832: Running test after training batch: 11000
2026-01-10 14:35:58,059: WER debug GT example: You can see the code at this point as well.
2026-01-10 14:36:15,747: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it utt as l
2026-01-10 14:36:15,777: WER debug example
  GT : how does it keep the cost down
  PR : houde us it epes uthe os ed
2026-01-10 14:36:17,510: Val batch 11000: PER (avg): 0.2704 CTC Loss (avg): 26.3288 WER(1gram): 88.58% (n=64) time: 19.678
2026-01-10 14:36:17,510: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-10 14:36:17,511: t15.2023.08.13 val PER: 0.2318
2026-01-10 14:36:17,511: t15.2023.08.18 val PER: 0.2112
2026-01-10 14:36:17,511: t15.2023.08.20 val PER: 0.2153
2026-01-10 14:36:17,511: t15.2023.08.25 val PER: 0.1702
2026-01-10 14:36:17,511: t15.2023.08.27 val PER: 0.3199
2026-01-10 14:36:17,511: t15.2023.09.01 val PER: 0.1680
2026-01-10 14:36:17,511: t15.2023.09.03 val PER: 0.2672
2026-01-10 14:36:17,512: t15.2023.09.24 val PER: 0.2100
2026-01-10 14:36:17,512: t15.2023.09.29 val PER: 0.2253
2026-01-10 14:36:17,512: t15.2023.10.01 val PER: 0.2675
2026-01-10 14:36:17,512: t15.2023.10.06 val PER: 0.1895
2026-01-10 14:36:17,512: t15.2023.10.08 val PER: 0.3004
2026-01-10 14:36:17,512: t15.2023.10.13 val PER: 0.3150
2026-01-10 14:36:17,512: t15.2023.10.15 val PER: 0.2432
2026-01-10 14:36:17,512: t15.2023.10.20 val PER: 0.2450
2026-01-10 14:36:17,512: t15.2023.10.22 val PER: 0.2160
2026-01-10 14:36:17,513: t15.2023.11.03 val PER: 0.2619
2026-01-10 14:36:17,513: t15.2023.11.04 val PER: 0.0922
2026-01-10 14:36:17,513: t15.2023.11.17 val PER: 0.1462
2026-01-10 14:36:17,513: t15.2023.11.19 val PER: 0.1178
2026-01-10 14:36:17,513: t15.2023.11.26 val PER: 0.2964
2026-01-10 14:36:17,513: t15.2023.12.03 val PER: 0.2384
2026-01-10 14:36:17,513: t15.2023.12.08 val PER: 0.2656
2026-01-10 14:36:17,513: t15.2023.12.10 val PER: 0.2300
2026-01-10 14:36:17,513: t15.2023.12.17 val PER: 0.3046
2026-01-10 14:36:17,513: t15.2023.12.29 val PER: 0.2677
2026-01-10 14:36:17,513: t15.2024.02.25 val PER: 0.2079
2026-01-10 14:36:17,513: t15.2024.03.08 val PER: 0.3585
2026-01-10 14:36:17,513: t15.2024.03.15 val PER: 0.3208
2026-01-10 14:36:17,513: t15.2024.03.17 val PER: 0.2734
2026-01-10 14:36:17,514: t15.2024.05.10 val PER: 0.2764
2026-01-10 14:36:17,514: t15.2024.06.14 val PER: 0.2855
2026-01-10 14:36:17,514: t15.2024.07.19 val PER: 0.3843
2026-01-10 14:36:17,514: t15.2024.07.21 val PER: 0.2241
2026-01-10 14:36:17,514: t15.2024.07.28 val PER: 0.2581
2026-01-10 14:36:17,514: t15.2025.01.10 val PER: 0.4477
2026-01-10 14:36:17,514: t15.2025.01.12 val PER: 0.3095
2026-01-10 14:36:17,514: t15.2025.03.14 val PER: 0.4778
2026-01-10 14:36:17,514: t15.2025.03.16 val PER: 0.3390
2026-01-10 14:36:17,514: t15.2025.03.30 val PER: 0.4425
2026-01-10 14:36:17,514: t15.2025.04.13 val PER: 0.3524
2026-01-10 14:36:17,573: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_11000
2026-01-10 14:38:48,994: Train batch 11200: loss: 21.09 grad norm: 63.22 time: 0.626
2026-01-10 14:41:19,530: Train batch 11400: loss: 30.31 grad norm: 88.63 time: 0.666
2026-01-10 14:42:33,609: Running test after training batch: 11500
2026-01-10 14:42:33,841: WER debug GT example: You can see the code at this point as well.
2026-01-10 14:42:51,815: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it otte is l
2026-01-10 14:42:51,845: WER debug example
  GT : how does it keep the cost down
  PR : houde is it oop uthe us end
2026-01-10 14:42:53,570: Val batch 11500: PER (avg): 0.2669 CTC Loss (avg): 25.9652 WER(1gram): 88.32% (n=64) time: 19.955
2026-01-10 14:42:53,570: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=12
2026-01-10 14:42:53,570: t15.2023.08.13 val PER: 0.2266
2026-01-10 14:42:53,570: t15.2023.08.18 val PER: 0.2087
2026-01-10 14:42:53,571: t15.2023.08.20 val PER: 0.2010
2026-01-10 14:42:53,571: t15.2023.08.25 val PER: 0.1732
2026-01-10 14:42:53,571: t15.2023.08.27 val PER: 0.3006
2026-01-10 14:42:53,571: t15.2023.09.01 val PER: 0.1648
2026-01-10 14:42:53,571: t15.2023.09.03 val PER: 0.2708
2026-01-10 14:42:53,571: t15.2023.09.24 val PER: 0.2039
2026-01-10 14:42:53,571: t15.2023.09.29 val PER: 0.2253
2026-01-10 14:42:53,571: t15.2023.10.01 val PER: 0.2543
2026-01-10 14:42:53,571: t15.2023.10.06 val PER: 0.1895
2026-01-10 14:42:53,572: t15.2023.10.08 val PER: 0.2977
2026-01-10 14:42:53,572: t15.2023.10.13 val PER: 0.2979
2026-01-10 14:42:53,572: t15.2023.10.15 val PER: 0.2386
2026-01-10 14:42:53,572: t15.2023.10.20 val PER: 0.2550
2026-01-10 14:42:53,572: t15.2023.10.22 val PER: 0.2116
2026-01-10 14:42:53,572: t15.2023.11.03 val PER: 0.2700
2026-01-10 14:42:53,572: t15.2023.11.04 val PER: 0.0887
2026-01-10 14:42:53,572: t15.2023.11.17 val PER: 0.1431
2026-01-10 14:42:53,572: t15.2023.11.19 val PER: 0.1158
2026-01-10 14:42:53,572: t15.2023.11.26 val PER: 0.2899
2026-01-10 14:42:53,573: t15.2023.12.03 val PER: 0.2248
2026-01-10 14:42:53,573: t15.2023.12.08 val PER: 0.2617
2026-01-10 14:42:53,573: t15.2023.12.10 val PER: 0.2260
2026-01-10 14:42:53,573: t15.2023.12.17 val PER: 0.3077
2026-01-10 14:42:53,573: t15.2023.12.29 val PER: 0.2629
2026-01-10 14:42:53,573: t15.2024.02.25 val PER: 0.2022
2026-01-10 14:42:53,573: t15.2024.03.08 val PER: 0.3428
2026-01-10 14:42:53,573: t15.2024.03.15 val PER: 0.3252
2026-01-10 14:42:53,573: t15.2024.03.17 val PER: 0.2685
2026-01-10 14:42:53,573: t15.2024.05.10 val PER: 0.2571
2026-01-10 14:42:53,574: t15.2024.06.14 val PER: 0.2871
2026-01-10 14:42:53,574: t15.2024.07.19 val PER: 0.3797
2026-01-10 14:42:53,574: t15.2024.07.21 val PER: 0.2228
2026-01-10 14:42:53,574: t15.2024.07.28 val PER: 0.2596
2026-01-10 14:42:53,574: t15.2025.01.10 val PER: 0.4408
2026-01-10 14:42:53,574: t15.2025.01.12 val PER: 0.3087
2026-01-10 14:42:53,574: t15.2025.03.14 val PER: 0.4882
2026-01-10 14:42:53,574: t15.2025.03.16 val PER: 0.3521
2026-01-10 14:42:53,574: t15.2025.03.30 val PER: 0.4356
2026-01-10 14:42:53,575: t15.2025.04.13 val PER: 0.3595
2026-01-10 14:42:53,640: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_11500
2026-01-10 14:44:11,817: Train batch 11600: loss: 35.29 grad norm: 94.77 time: 0.729
2026-01-10 14:46:47,662: Train batch 11800: loss: 15.22 grad norm: 61.93 time: 0.787
2026-01-10 14:49:24,124: Train batch 12000: loss: 28.44 grad norm: 72.95 time: 0.645
2026-01-10 14:49:24,128: Running test after training batch: 12000
2026-01-10 14:49:24,354: WER debug GT example: You can see the code at this point as well.
2026-01-10 14:49:42,027: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it it as l
2026-01-10 14:49:42,057: WER debug example
  GT : how does it keep the cost down
  PR : houde is it epes uthe os ed
2026-01-10 14:49:43,760: Val batch 12000: PER (avg): 0.2671 CTC Loss (avg): 25.9357 WER(1gram): 89.34% (n=64) time: 19.632
2026-01-10 14:49:43,761: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-10 14:49:43,761: t15.2023.08.13 val PER: 0.2297
2026-01-10 14:49:43,761: t15.2023.08.18 val PER: 0.2037
2026-01-10 14:49:43,761: t15.2023.08.20 val PER: 0.2160
2026-01-10 14:49:43,761: t15.2023.08.25 val PER: 0.1732
2026-01-10 14:49:43,761: t15.2023.08.27 val PER: 0.3055
2026-01-10 14:49:43,761: t15.2023.09.01 val PER: 0.1664
2026-01-10 14:49:43,761: t15.2023.09.03 val PER: 0.2755
2026-01-10 14:49:43,761: t15.2023.09.24 val PER: 0.2075
2026-01-10 14:49:43,761: t15.2023.09.29 val PER: 0.2202
2026-01-10 14:49:43,762: t15.2023.10.01 val PER: 0.2622
2026-01-10 14:49:43,762: t15.2023.10.06 val PER: 0.1895
2026-01-10 14:49:43,762: t15.2023.10.08 val PER: 0.2909
2026-01-10 14:49:43,762: t15.2023.10.13 val PER: 0.3041
2026-01-10 14:49:43,762: t15.2023.10.15 val PER: 0.2459
2026-01-10 14:49:43,762: t15.2023.10.20 val PER: 0.2416
2026-01-10 14:49:43,762: t15.2023.10.22 val PER: 0.2205
2026-01-10 14:49:43,762: t15.2023.11.03 val PER: 0.2639
2026-01-10 14:49:43,762: t15.2023.11.04 val PER: 0.0785
2026-01-10 14:49:43,762: t15.2023.11.17 val PER: 0.1586
2026-01-10 14:49:43,762: t15.2023.11.19 val PER: 0.1178
2026-01-10 14:49:43,762: t15.2023.11.26 val PER: 0.3000
2026-01-10 14:49:43,762: t15.2023.12.03 val PER: 0.2279
2026-01-10 14:49:43,762: t15.2023.12.08 val PER: 0.2550
2026-01-10 14:49:43,762: t15.2023.12.10 val PER: 0.2260
2026-01-10 14:49:43,762: t15.2023.12.17 val PER: 0.2994
2026-01-10 14:49:43,763: t15.2023.12.29 val PER: 0.2670
2026-01-10 14:49:43,763: t15.2024.02.25 val PER: 0.2121
2026-01-10 14:49:43,763: t15.2024.03.08 val PER: 0.3286
2026-01-10 14:49:43,763: t15.2024.03.15 val PER: 0.3283
2026-01-10 14:49:43,763: t15.2024.03.17 val PER: 0.2601
2026-01-10 14:49:43,763: t15.2024.05.10 val PER: 0.2719
2026-01-10 14:49:43,763: t15.2024.06.14 val PER: 0.2744
2026-01-10 14:49:43,763: t15.2024.07.19 val PER: 0.3718
2026-01-10 14:49:43,763: t15.2024.07.21 val PER: 0.2166
2026-01-10 14:49:43,763: t15.2024.07.28 val PER: 0.2529
2026-01-10 14:49:43,763: t15.2025.01.10 val PER: 0.4325
2026-01-10 14:49:43,763: t15.2025.01.12 val PER: 0.3002
2026-01-10 14:49:43,763: t15.2025.03.14 val PER: 0.4763
2026-01-10 14:49:43,763: t15.2025.03.16 val PER: 0.3560
2026-01-10 14:49:43,763: t15.2025.03.30 val PER: 0.4517
2026-01-10 14:49:43,763: t15.2025.04.13 val PER: 0.3609
2026-01-10 14:49:43,821: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_12000
2026-01-10 14:52:17,388: Train batch 12200: loss: 20.76 grad norm: 69.59 time: 0.749
2026-01-10 14:54:49,948: Train batch 12400: loss: 28.50 grad norm: 80.18 time: 0.635
2026-01-10 14:56:05,529: Running test after training batch: 12500
2026-01-10 14:56:05,757: WER debug GT example: You can see the code at this point as well.
2026-01-10 14:56:23,493: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it it is l
2026-01-10 14:56:23,523: WER debug example
  GT : how does it keep the cost down
  PR : houde is it oop uthe os ed
2026-01-10 14:56:25,259: Val batch 12500: PER (avg): 0.2621 CTC Loss (avg): 25.6222 WER(1gram): 88.07% (n=64) time: 19.728
2026-01-10 14:56:25,259: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-10 14:56:25,259: t15.2023.08.13 val PER: 0.2214
2026-01-10 14:56:25,259: t15.2023.08.18 val PER: 0.2104
2026-01-10 14:56:25,260: t15.2023.08.20 val PER: 0.1986
2026-01-10 14:56:25,260: t15.2023.08.25 val PER: 0.1777
2026-01-10 14:56:25,260: t15.2023.08.27 val PER: 0.3006
2026-01-10 14:56:25,260: t15.2023.09.01 val PER: 0.1558
2026-01-10 14:56:25,260: t15.2023.09.03 val PER: 0.2684
2026-01-10 14:56:25,260: t15.2023.09.24 val PER: 0.2039
2026-01-10 14:56:25,260: t15.2023.09.29 val PER: 0.2208
2026-01-10 14:56:25,260: t15.2023.10.01 val PER: 0.2576
2026-01-10 14:56:25,260: t15.2023.10.06 val PER: 0.1841
2026-01-10 14:56:25,260: t15.2023.10.08 val PER: 0.2828
2026-01-10 14:56:25,261: t15.2023.10.13 val PER: 0.2979
2026-01-10 14:56:25,261: t15.2023.10.15 val PER: 0.2406
2026-01-10 14:56:25,261: t15.2023.10.20 val PER: 0.2215
2026-01-10 14:56:25,261: t15.2023.10.22 val PER: 0.2127
2026-01-10 14:56:25,261: t15.2023.11.03 val PER: 0.2626
2026-01-10 14:56:25,261: t15.2023.11.04 val PER: 0.0785
2026-01-10 14:56:25,261: t15.2023.11.17 val PER: 0.1555
2026-01-10 14:56:25,261: t15.2023.11.19 val PER: 0.1178
2026-01-10 14:56:25,261: t15.2023.11.26 val PER: 0.2899
2026-01-10 14:56:25,261: t15.2023.12.03 val PER: 0.2206
2026-01-10 14:56:25,261: t15.2023.12.08 val PER: 0.2503
2026-01-10 14:56:25,261: t15.2023.12.10 val PER: 0.2286
2026-01-10 14:56:25,261: t15.2023.12.17 val PER: 0.2994
2026-01-10 14:56:25,261: t15.2023.12.29 val PER: 0.2505
2026-01-10 14:56:25,261: t15.2024.02.25 val PER: 0.2065
2026-01-10 14:56:25,261: t15.2024.03.08 val PER: 0.3428
2026-01-10 14:56:25,262: t15.2024.03.15 val PER: 0.3277
2026-01-10 14:56:25,262: t15.2024.03.17 val PER: 0.2622
2026-01-10 14:56:25,262: t15.2024.05.10 val PER: 0.2600
2026-01-10 14:56:25,262: t15.2024.06.14 val PER: 0.2713
2026-01-10 14:56:25,262: t15.2024.07.19 val PER: 0.3593
2026-01-10 14:56:25,262: t15.2024.07.21 val PER: 0.2138
2026-01-10 14:56:25,262: t15.2024.07.28 val PER: 0.2500
2026-01-10 14:56:25,262: t15.2025.01.10 val PER: 0.4408
2026-01-10 14:56:25,262: t15.2025.01.12 val PER: 0.2964
2026-01-10 14:56:25,262: t15.2025.03.14 val PER: 0.4867
2026-01-10 14:56:25,262: t15.2025.03.16 val PER: 0.3351
2026-01-10 14:56:25,262: t15.2025.03.30 val PER: 0.4345
2026-01-10 14:56:25,262: t15.2025.04.13 val PER: 0.3466
2026-01-10 14:56:25,318: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_12500
2026-01-10 14:57:42,627: Train batch 12600: loss: 32.42 grad norm: 94.16 time: 1.071
2026-01-10 15:00:14,337: Train batch 12800: loss: 22.98 grad norm: 64.04 time: 0.698
2026-01-10 15:02:49,910: Train batch 13000: loss: 28.29 grad norm: 92.59 time: 0.752
2026-01-10 15:02:49,911: Running test after training batch: 13000
2026-01-10 15:02:50,137: WER debug GT example: You can see the code at this point as well.
2026-01-10 15:03:08,249: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it otte as l
2026-01-10 15:03:08,279: WER debug example
  GT : how does it keep the cost down
  PR : houde us it oop uthe us ed
2026-01-10 15:03:10,015: Val batch 13000: PER (avg): 0.2609 CTC Loss (avg): 25.5532 WER(1gram): 88.32% (n=64) time: 20.103
2026-01-10 15:03:10,015: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-10 15:03:10,015: t15.2023.08.13 val PER: 0.2235
2026-01-10 15:03:10,015: t15.2023.08.18 val PER: 0.1961
2026-01-10 15:03:10,016: t15.2023.08.20 val PER: 0.1978
2026-01-10 15:03:10,016: t15.2023.08.25 val PER: 0.1762
2026-01-10 15:03:10,016: t15.2023.08.27 val PER: 0.3135
2026-01-10 15:03:10,016: t15.2023.09.01 val PER: 0.1648
2026-01-10 15:03:10,016: t15.2023.09.03 val PER: 0.2767
2026-01-10 15:03:10,016: t15.2023.09.24 val PER: 0.2136
2026-01-10 15:03:10,016: t15.2023.09.29 val PER: 0.2208
2026-01-10 15:03:10,016: t15.2023.10.01 val PER: 0.2583
2026-01-10 15:03:10,016: t15.2023.10.06 val PER: 0.1884
2026-01-10 15:03:10,016: t15.2023.10.08 val PER: 0.2855
2026-01-10 15:03:10,016: t15.2023.10.13 val PER: 0.3033
2026-01-10 15:03:10,016: t15.2023.10.15 val PER: 0.2406
2026-01-10 15:03:10,016: t15.2023.10.20 val PER: 0.2181
2026-01-10 15:03:10,016: t15.2023.10.22 val PER: 0.2105
2026-01-10 15:03:10,016: t15.2023.11.03 val PER: 0.2578
2026-01-10 15:03:10,017: t15.2023.11.04 val PER: 0.0785
2026-01-10 15:03:10,017: t15.2023.11.17 val PER: 0.1477
2026-01-10 15:03:10,017: t15.2023.11.19 val PER: 0.1118
2026-01-10 15:03:10,017: t15.2023.11.26 val PER: 0.2877
2026-01-10 15:03:10,017: t15.2023.12.03 val PER: 0.2237
2026-01-10 15:03:10,017: t15.2023.12.08 val PER: 0.2537
2026-01-10 15:03:10,017: t15.2023.12.10 val PER: 0.2234
2026-01-10 15:03:10,017: t15.2023.12.17 val PER: 0.2942
2026-01-10 15:03:10,017: t15.2023.12.29 val PER: 0.2512
2026-01-10 15:03:10,017: t15.2024.02.25 val PER: 0.2008
2026-01-10 15:03:10,017: t15.2024.03.08 val PER: 0.3343
2026-01-10 15:03:10,017: t15.2024.03.15 val PER: 0.3271
2026-01-10 15:03:10,017: t15.2024.03.17 val PER: 0.2629
2026-01-10 15:03:10,017: t15.2024.05.10 val PER: 0.2481
2026-01-10 15:03:10,017: t15.2024.06.14 val PER: 0.2713
2026-01-10 15:03:10,018: t15.2024.07.19 val PER: 0.3626
2026-01-10 15:03:10,018: t15.2024.07.21 val PER: 0.2193
2026-01-10 15:03:10,018: t15.2024.07.28 val PER: 0.2449
2026-01-10 15:03:10,018: t15.2025.01.10 val PER: 0.4256
2026-01-10 15:03:10,018: t15.2025.01.12 val PER: 0.2856
2026-01-10 15:03:10,018: t15.2025.03.14 val PER: 0.4675
2026-01-10 15:03:10,018: t15.2025.03.16 val PER: 0.3351
2026-01-10 15:03:10,018: t15.2025.03.30 val PER: 0.4230
2026-01-10 15:03:10,018: t15.2025.04.13 val PER: 0.3438
2026-01-10 15:03:10,073: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_13000
2026-01-10 15:05:44,803: Train batch 13200: loss: 22.03 grad norm: 65.11 time: 0.826
2026-01-10 15:08:19,381: Train batch 13400: loss: 26.01 grad norm: 89.29 time: 0.909
2026-01-10 15:09:36,026: Running test after training batch: 13500
2026-01-10 15:09:36,257: WER debug GT example: You can see the code at this point as well.
2026-01-10 15:09:54,023: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it it as l
2026-01-10 15:09:54,054: WER debug example
  GT : how does it keep the cost down
  PR : houde us it epp uthe os ed
2026-01-10 15:09:55,773: Val batch 13500: PER (avg): 0.2592 CTC Loss (avg): 25.3468 WER(1gram): 88.32% (n=64) time: 19.745
2026-01-10 15:09:55,774: WER lens: avg_true_words=6.16 avg_pred_words=6.27 max_pred_words=12
2026-01-10 15:09:55,774: t15.2023.08.13 val PER: 0.2235
2026-01-10 15:09:55,774: t15.2023.08.18 val PER: 0.2012
2026-01-10 15:09:55,774: t15.2023.08.20 val PER: 0.2033
2026-01-10 15:09:55,774: t15.2023.08.25 val PER: 0.1822
2026-01-10 15:09:55,774: t15.2023.08.27 val PER: 0.2846
2026-01-10 15:09:55,774: t15.2023.09.01 val PER: 0.1680
2026-01-10 15:09:55,775: t15.2023.09.03 val PER: 0.2732
2026-01-10 15:09:55,775: t15.2023.09.24 val PER: 0.2063
2026-01-10 15:09:55,775: t15.2023.09.29 val PER: 0.2125
2026-01-10 15:09:55,775: t15.2023.10.01 val PER: 0.2569
2026-01-10 15:09:55,775: t15.2023.10.06 val PER: 0.1905
2026-01-10 15:09:55,775: t15.2023.10.08 val PER: 0.2842
2026-01-10 15:09:55,775: t15.2023.10.13 val PER: 0.2979
2026-01-10 15:09:55,775: t15.2023.10.15 val PER: 0.2380
2026-01-10 15:09:55,775: t15.2023.10.20 val PER: 0.2349
2026-01-10 15:09:55,775: t15.2023.10.22 val PER: 0.2127
2026-01-10 15:09:55,776: t15.2023.11.03 val PER: 0.2592
2026-01-10 15:09:55,776: t15.2023.11.04 val PER: 0.0785
2026-01-10 15:09:55,776: t15.2023.11.17 val PER: 0.1477
2026-01-10 15:09:55,776: t15.2023.11.19 val PER: 0.1158
2026-01-10 15:09:55,776: t15.2023.11.26 val PER: 0.2841
2026-01-10 15:09:55,776: t15.2023.12.03 val PER: 0.2174
2026-01-10 15:09:55,776: t15.2023.12.08 val PER: 0.2503
2026-01-10 15:09:55,776: t15.2023.12.10 val PER: 0.2194
2026-01-10 15:09:55,776: t15.2023.12.17 val PER: 0.2869
2026-01-10 15:09:55,776: t15.2023.12.29 val PER: 0.2581
2026-01-10 15:09:55,777: t15.2024.02.25 val PER: 0.1980
2026-01-10 15:09:55,777: t15.2024.03.08 val PER: 0.3357
2026-01-10 15:09:55,777: t15.2024.03.15 val PER: 0.3240
2026-01-10 15:09:55,777: t15.2024.03.17 val PER: 0.2608
2026-01-10 15:09:55,777: t15.2024.05.10 val PER: 0.2496
2026-01-10 15:09:55,777: t15.2024.06.14 val PER: 0.2713
2026-01-10 15:09:55,777: t15.2024.07.19 val PER: 0.3593
2026-01-10 15:09:55,777: t15.2024.07.21 val PER: 0.2166
2026-01-10 15:09:55,777: t15.2024.07.28 val PER: 0.2419
2026-01-10 15:09:55,777: t15.2025.01.10 val PER: 0.4146
2026-01-10 15:09:55,777: t15.2025.01.12 val PER: 0.2841
2026-01-10 15:09:55,777: t15.2025.03.14 val PER: 0.4586
2026-01-10 15:09:55,777: t15.2025.03.16 val PER: 0.3390
2026-01-10 15:09:55,777: t15.2025.03.30 val PER: 0.4218
2026-01-10 15:09:55,777: t15.2025.04.13 val PER: 0.3466
2026-01-10 15:09:55,837: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_13500
2026-01-10 15:11:09,907: Train batch 13600: loss: 30.64 grad norm: 83.59 time: 0.755
2026-01-10 15:13:41,560: Train batch 13800: loss: 21.22 grad norm: 64.55 time: 0.645
2026-01-10 15:16:15,257: Train batch 14000: loss: 41.76 grad norm: 143.58 time: 0.666
2026-01-10 15:16:15,259: Running test after training batch: 14000
2026-01-10 15:16:15,489: WER debug GT example: You can see the code at this point as well.
2026-01-10 15:16:33,196: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it art as l
2026-01-10 15:16:33,227: WER debug example
  GT : how does it keep the cost down
  PR : houde us it epes uthe us ed
2026-01-10 15:16:34,953: Val batch 14000: PER (avg): 0.2578 CTC Loss (avg): 25.1692 WER(1gram): 88.07% (n=64) time: 19.692
2026-01-10 15:16:34,954: WER lens: avg_true_words=6.16 avg_pred_words=6.27 max_pred_words=12
2026-01-10 15:16:34,954: t15.2023.08.13 val PER: 0.2214
2026-01-10 15:16:34,954: t15.2023.08.18 val PER: 0.1945
2026-01-10 15:16:34,954: t15.2023.08.20 val PER: 0.1994
2026-01-10 15:16:34,954: t15.2023.08.25 val PER: 0.1642
2026-01-10 15:16:34,954: t15.2023.08.27 val PER: 0.2958
2026-01-10 15:16:34,954: t15.2023.09.01 val PER: 0.1688
2026-01-10 15:16:34,954: t15.2023.09.03 val PER: 0.2755
2026-01-10 15:16:34,954: t15.2023.09.24 val PER: 0.2015
2026-01-10 15:16:34,954: t15.2023.09.29 val PER: 0.2144
2026-01-10 15:16:34,954: t15.2023.10.01 val PER: 0.2503
2026-01-10 15:16:34,954: t15.2023.10.06 val PER: 0.1830
2026-01-10 15:16:34,954: t15.2023.10.08 val PER: 0.2815
2026-01-10 15:16:34,954: t15.2023.10.13 val PER: 0.2940
2026-01-10 15:16:34,955: t15.2023.10.15 val PER: 0.2367
2026-01-10 15:16:34,955: t15.2023.10.20 val PER: 0.2248
2026-01-10 15:16:34,955: t15.2023.10.22 val PER: 0.2183
2026-01-10 15:16:34,955: t15.2023.11.03 val PER: 0.2585
2026-01-10 15:16:34,955: t15.2023.11.04 val PER: 0.0785
2026-01-10 15:16:34,955: t15.2023.11.17 val PER: 0.1431
2026-01-10 15:16:34,955: t15.2023.11.19 val PER: 0.1138
2026-01-10 15:16:34,955: t15.2023.11.26 val PER: 0.2783
2026-01-10 15:16:34,955: t15.2023.12.03 val PER: 0.2164
2026-01-10 15:16:34,955: t15.2023.12.08 val PER: 0.2477
2026-01-10 15:16:34,955: t15.2023.12.10 val PER: 0.2129
2026-01-10 15:16:34,955: t15.2023.12.17 val PER: 0.2890
2026-01-10 15:16:34,955: t15.2023.12.29 val PER: 0.2526
2026-01-10 15:16:34,955: t15.2024.02.25 val PER: 0.1994
2026-01-10 15:16:34,955: t15.2024.03.08 val PER: 0.3400
2026-01-10 15:16:34,956: t15.2024.03.15 val PER: 0.3164
2026-01-10 15:16:34,956: t15.2024.03.17 val PER: 0.2573
2026-01-10 15:16:34,956: t15.2024.05.10 val PER: 0.2541
2026-01-10 15:16:34,956: t15.2024.06.14 val PER: 0.2792
2026-01-10 15:16:34,956: t15.2024.07.19 val PER: 0.3672
2026-01-10 15:16:34,956: t15.2024.07.21 val PER: 0.2166
2026-01-10 15:16:34,957: t15.2024.07.28 val PER: 0.2397
2026-01-10 15:16:34,957: t15.2025.01.10 val PER: 0.4229
2026-01-10 15:16:34,957: t15.2025.01.12 val PER: 0.2848
2026-01-10 15:16:34,957: t15.2025.03.14 val PER: 0.4660
2026-01-10 15:16:34,957: t15.2025.03.16 val PER: 0.3351
2026-01-10 15:16:34,957: t15.2025.03.30 val PER: 0.4241
2026-01-10 15:16:34,957: t15.2025.04.13 val PER: 0.3452
2026-01-10 15:16:35,017: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_14000
2026-01-10 15:19:08,750: Train batch 14200: loss: 28.72 grad norm: 78.91 time: 0.762
2026-01-10 15:21:34,972: Train batch 14400: loss: 23.00 grad norm: 75.57 time: 0.842
2026-01-10 15:22:49,461: Running test after training batch: 14500
2026-01-10 15:22:49,687: WER debug GT example: You can see the code at this point as well.
2026-01-10 15:23:07,502: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it otte as l
2026-01-10 15:23:07,532: WER debug example
  GT : how does it keep the cost down
  PR : houde us it oop uthe os ed
2026-01-10 15:23:09,265: Val batch 14500: PER (avg): 0.2570 CTC Loss (avg): 25.1087 WER(1gram): 88.32% (n=64) time: 19.802
2026-01-10 15:23:09,265: WER lens: avg_true_words=6.16 avg_pred_words=6.19 max_pred_words=12
2026-01-10 15:23:09,265: t15.2023.08.13 val PER: 0.2110
2026-01-10 15:23:09,265: t15.2023.08.18 val PER: 0.1970
2026-01-10 15:23:09,265: t15.2023.08.20 val PER: 0.1946
2026-01-10 15:23:09,265: t15.2023.08.25 val PER: 0.1732
2026-01-10 15:23:09,265: t15.2023.08.27 val PER: 0.2958
2026-01-10 15:23:09,265: t15.2023.09.01 val PER: 0.1648
2026-01-10 15:23:09,266: t15.2023.09.03 val PER: 0.2589
2026-01-10 15:23:09,266: t15.2023.09.24 val PER: 0.2015
2026-01-10 15:23:09,266: t15.2023.09.29 val PER: 0.2125
2026-01-10 15:23:09,266: t15.2023.10.01 val PER: 0.2530
2026-01-10 15:23:09,266: t15.2023.10.06 val PER: 0.1873
2026-01-10 15:23:09,266: t15.2023.10.08 val PER: 0.2760
2026-01-10 15:23:09,266: t15.2023.10.13 val PER: 0.2987
2026-01-10 15:23:09,266: t15.2023.10.15 val PER: 0.2347
2026-01-10 15:23:09,266: t15.2023.10.20 val PER: 0.2315
2026-01-10 15:23:09,266: t15.2023.10.22 val PER: 0.2094
2026-01-10 15:23:09,266: t15.2023.11.03 val PER: 0.2639
2026-01-10 15:23:09,266: t15.2023.11.04 val PER: 0.0751
2026-01-10 15:23:09,266: t15.2023.11.17 val PER: 0.1322
2026-01-10 15:23:09,266: t15.2023.11.19 val PER: 0.1098
2026-01-10 15:23:09,266: t15.2023.11.26 val PER: 0.2725
2026-01-10 15:23:09,266: t15.2023.12.03 val PER: 0.2185
2026-01-10 15:23:09,267: t15.2023.12.08 val PER: 0.2503
2026-01-10 15:23:09,267: t15.2023.12.10 val PER: 0.2142
2026-01-10 15:23:09,267: t15.2023.12.17 val PER: 0.2879
2026-01-10 15:23:09,267: t15.2023.12.29 val PER: 0.2471
2026-01-10 15:23:09,267: t15.2024.02.25 val PER: 0.1966
2026-01-10 15:23:09,267: t15.2024.03.08 val PER: 0.3229
2026-01-10 15:23:09,267: t15.2024.03.15 val PER: 0.3152
2026-01-10 15:23:09,267: t15.2024.03.17 val PER: 0.2531
2026-01-10 15:23:09,267: t15.2024.05.10 val PER: 0.2556
2026-01-10 15:23:09,267: t15.2024.06.14 val PER: 0.2729
2026-01-10 15:23:09,268: t15.2024.07.19 val PER: 0.3659
2026-01-10 15:23:09,268: t15.2024.07.21 val PER: 0.2152
2026-01-10 15:23:09,268: t15.2024.07.28 val PER: 0.2434
2026-01-10 15:23:09,268: t15.2025.01.10 val PER: 0.4325
2026-01-10 15:23:09,268: t15.2025.01.12 val PER: 0.2956
2026-01-10 15:23:09,268: t15.2025.03.14 val PER: 0.4719
2026-01-10 15:23:09,268: t15.2025.03.16 val PER: 0.3403
2026-01-10 15:23:09,268: t15.2025.03.30 val PER: 0.4264
2026-01-10 15:23:09,268: t15.2025.04.13 val PER: 0.3452
2026-01-10 15:23:09,326: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_14500
2026-01-10 15:24:25,930: Train batch 14600: loss: 17.72 grad norm: 64.05 time: 0.582
2026-01-10 15:26:57,754: Train batch 14800: loss: 23.51 grad norm: 75.62 time: 0.904
2026-01-10 15:29:25,317: Train batch 15000: loss: 28.22 grad norm: 83.37 time: 0.567
2026-01-10 15:29:25,318: Running test after training batch: 15000
2026-01-10 15:29:25,546: WER debug GT example: You can see the code at this point as well.
2026-01-10 15:29:43,255: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it otte as l
2026-01-10 15:29:43,285: WER debug example
  GT : how does it keep the cost down
  PR : houde us it epes uthe us ed
2026-01-10 15:29:44,987: Val batch 15000: PER (avg): 0.2542 CTC Loss (avg): 24.9114 WER(1gram): 88.58% (n=64) time: 19.668
2026-01-10 15:29:44,987: WER lens: avg_true_words=6.16 avg_pred_words=6.25 max_pred_words=12
2026-01-10 15:29:44,988: t15.2023.08.13 val PER: 0.2173
2026-01-10 15:29:44,988: t15.2023.08.18 val PER: 0.1911
2026-01-10 15:29:44,988: t15.2023.08.20 val PER: 0.1962
2026-01-10 15:29:44,988: t15.2023.08.25 val PER: 0.1717
2026-01-10 15:29:44,988: t15.2023.08.27 val PER: 0.2814
2026-01-10 15:29:44,988: t15.2023.09.01 val PER: 0.1615
2026-01-10 15:29:44,988: t15.2023.09.03 val PER: 0.2601
2026-01-10 15:29:44,988: t15.2023.09.24 val PER: 0.1857
2026-01-10 15:29:44,988: t15.2023.09.29 val PER: 0.2080
2026-01-10 15:29:44,988: t15.2023.10.01 val PER: 0.2477
2026-01-10 15:29:44,988: t15.2023.10.06 val PER: 0.1787
2026-01-10 15:29:44,988: t15.2023.10.08 val PER: 0.2815
2026-01-10 15:29:44,988: t15.2023.10.13 val PER: 0.2948
2026-01-10 15:29:44,988: t15.2023.10.15 val PER: 0.2301
2026-01-10 15:29:44,988: t15.2023.10.20 val PER: 0.2315
2026-01-10 15:29:44,988: t15.2023.10.22 val PER: 0.2060
2026-01-10 15:29:44,989: t15.2023.11.03 val PER: 0.2564
2026-01-10 15:29:44,989: t15.2023.11.04 val PER: 0.0751
2026-01-10 15:29:44,989: t15.2023.11.17 val PER: 0.1384
2026-01-10 15:29:44,989: t15.2023.11.19 val PER: 0.1058
2026-01-10 15:29:44,989: t15.2023.11.26 val PER: 0.2768
2026-01-10 15:29:44,989: t15.2023.12.03 val PER: 0.2132
2026-01-10 15:29:44,989: t15.2023.12.08 val PER: 0.2497
2026-01-10 15:29:44,989: t15.2023.12.10 val PER: 0.2129
2026-01-10 15:29:44,989: t15.2023.12.17 val PER: 0.2890
2026-01-10 15:29:44,989: t15.2023.12.29 val PER: 0.2533
2026-01-10 15:29:44,989: t15.2024.02.25 val PER: 0.1994
2026-01-10 15:29:44,989: t15.2024.03.08 val PER: 0.3186
2026-01-10 15:29:44,989: t15.2024.03.15 val PER: 0.3139
2026-01-10 15:29:44,989: t15.2024.03.17 val PER: 0.2580
2026-01-10 15:29:44,989: t15.2024.05.10 val PER: 0.2496
2026-01-10 15:29:44,990: t15.2024.06.14 val PER: 0.2697
2026-01-10 15:29:44,990: t15.2024.07.19 val PER: 0.3566
2026-01-10 15:29:44,990: t15.2024.07.21 val PER: 0.2076
2026-01-10 15:29:44,990: t15.2024.07.28 val PER: 0.2382
2026-01-10 15:29:44,990: t15.2025.01.10 val PER: 0.4311
2026-01-10 15:29:44,990: t15.2025.01.12 val PER: 0.2933
2026-01-10 15:29:44,990: t15.2025.03.14 val PER: 0.4675
2026-01-10 15:29:44,990: t15.2025.03.16 val PER: 0.3364
2026-01-10 15:29:44,990: t15.2025.03.30 val PER: 0.4103
2026-01-10 15:29:44,990: t15.2025.04.13 val PER: 0.3424
2026-01-10 15:29:45,052: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_15000
2026-01-10 15:32:10,899: Train batch 15200: loss: 24.93 grad norm: 75.48 time: 0.788
2026-01-10 15:34:40,165: Train batch 15400: loss: 25.73 grad norm: 72.59 time: 0.703
2026-01-10 15:35:53,748: Running test after training batch: 15500
2026-01-10 15:35:53,974: WER debug GT example: You can see the code at this point as well.
2026-01-10 15:36:11,608: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it otte as l
2026-01-10 15:36:11,640: WER debug example
  GT : how does it keep the cost down
  PR : houde us it epes uthe us ed
2026-01-10 15:36:13,362: Val batch 15500: PER (avg): 0.2539 CTC Loss (avg): 24.7603 WER(1gram): 88.32% (n=64) time: 19.612
2026-01-10 15:36:13,362: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-10 15:36:13,362: t15.2023.08.13 val PER: 0.2131
2026-01-10 15:36:13,363: t15.2023.08.18 val PER: 0.1978
2026-01-10 15:36:13,363: t15.2023.08.20 val PER: 0.1962
2026-01-10 15:36:13,363: t15.2023.08.25 val PER: 0.1687
2026-01-10 15:36:13,363: t15.2023.08.27 val PER: 0.2974
2026-01-10 15:36:13,363: t15.2023.09.01 val PER: 0.1542
2026-01-10 15:36:13,363: t15.2023.09.03 val PER: 0.2672
2026-01-10 15:36:13,363: t15.2023.09.24 val PER: 0.1978
2026-01-10 15:36:13,363: t15.2023.09.29 val PER: 0.2061
2026-01-10 15:36:13,363: t15.2023.10.01 val PER: 0.2444
2026-01-10 15:36:13,363: t15.2023.10.06 val PER: 0.1841
2026-01-10 15:36:13,364: t15.2023.10.08 val PER: 0.2747
2026-01-10 15:36:13,364: t15.2023.10.13 val PER: 0.3002
2026-01-10 15:36:13,364: t15.2023.10.15 val PER: 0.2307
2026-01-10 15:36:13,364: t15.2023.10.20 val PER: 0.2248
2026-01-10 15:36:13,364: t15.2023.10.22 val PER: 0.2149
2026-01-10 15:36:13,364: t15.2023.11.03 val PER: 0.2531
2026-01-10 15:36:13,364: t15.2023.11.04 val PER: 0.0717
2026-01-10 15:36:13,364: t15.2023.11.17 val PER: 0.1337
2026-01-10 15:36:13,364: t15.2023.11.19 val PER: 0.1078
2026-01-10 15:36:13,364: t15.2023.11.26 val PER: 0.2754
2026-01-10 15:36:13,364: t15.2023.12.03 val PER: 0.2164
2026-01-10 15:36:13,364: t15.2023.12.08 val PER: 0.2463
2026-01-10 15:36:13,364: t15.2023.12.10 val PER: 0.2116
2026-01-10 15:36:13,364: t15.2023.12.17 val PER: 0.2817
2026-01-10 15:36:13,364: t15.2023.12.29 val PER: 0.2464
2026-01-10 15:36:13,364: t15.2024.02.25 val PER: 0.1868
2026-01-10 15:36:13,365: t15.2024.03.08 val PER: 0.3272
2026-01-10 15:36:13,365: t15.2024.03.15 val PER: 0.3158
2026-01-10 15:36:13,365: t15.2024.03.17 val PER: 0.2587
2026-01-10 15:36:13,365: t15.2024.05.10 val PER: 0.2556
2026-01-10 15:36:13,365: t15.2024.06.14 val PER: 0.2666
2026-01-10 15:36:13,365: t15.2024.07.19 val PER: 0.3546
2026-01-10 15:36:13,365: t15.2024.07.21 val PER: 0.2103
2026-01-10 15:36:13,365: t15.2024.07.28 val PER: 0.2397
2026-01-10 15:36:13,365: t15.2025.01.10 val PER: 0.4270
2026-01-10 15:36:13,365: t15.2025.01.12 val PER: 0.2902
2026-01-10 15:36:13,365: t15.2025.03.14 val PER: 0.4645
2026-01-10 15:36:13,365: t15.2025.03.16 val PER: 0.3272
2026-01-10 15:36:13,365: t15.2025.03.30 val PER: 0.4172
2026-01-10 15:36:13,365: t15.2025.04.13 val PER: 0.3395
2026-01-10 15:36:13,421: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_15500
2026-01-10 15:37:28,786: Train batch 15600: loss: 26.95 grad norm: 78.53 time: 0.724
2026-01-10 15:39:54,275: Train batch 15800: loss: 16.30 grad norm: 60.79 time: 0.578
2026-01-10 15:42:20,951: Train batch 16000: loss: 24.26 grad norm: 63.69 time: 0.653
2026-01-10 15:42:20,955: Running test after training batch: 16000
2026-01-10 15:42:21,182: WER debug GT example: You can see the code at this point as well.
2026-01-10 15:42:38,874: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it otte as l
2026-01-10 15:42:38,906: WER debug example
  GT : how does it keep the cost down
  PR : houde us it epp uthe os ed
2026-01-10 15:42:40,629: Val batch 16000: PER (avg): 0.2519 CTC Loss (avg): 24.7098 WER(1gram): 88.58% (n=64) time: 19.673
2026-01-10 15:42:40,630: WER lens: avg_true_words=6.16 avg_pred_words=6.25 max_pred_words=12
2026-01-10 15:42:40,630: t15.2023.08.13 val PER: 0.2110
2026-01-10 15:42:40,630: t15.2023.08.18 val PER: 0.1953
2026-01-10 15:42:40,630: t15.2023.08.20 val PER: 0.1946
2026-01-10 15:42:40,630: t15.2023.08.25 val PER: 0.1687
2026-01-10 15:42:40,630: t15.2023.08.27 val PER: 0.2862
2026-01-10 15:42:40,630: t15.2023.09.01 val PER: 0.1631
2026-01-10 15:42:40,630: t15.2023.09.03 val PER: 0.2542
2026-01-10 15:42:40,630: t15.2023.09.24 val PER: 0.2039
2026-01-10 15:42:40,630: t15.2023.09.29 val PER: 0.2042
2026-01-10 15:42:40,630: t15.2023.10.01 val PER: 0.2470
2026-01-10 15:42:40,631: t15.2023.10.06 val PER: 0.1862
2026-01-10 15:42:40,631: t15.2023.10.08 val PER: 0.2720
2026-01-10 15:42:40,631: t15.2023.10.13 val PER: 0.2901
2026-01-10 15:42:40,631: t15.2023.10.15 val PER: 0.2274
2026-01-10 15:42:40,631: t15.2023.10.20 val PER: 0.2315
2026-01-10 15:42:40,631: t15.2023.10.22 val PER: 0.2105
2026-01-10 15:42:40,631: t15.2023.11.03 val PER: 0.2564
2026-01-10 15:42:40,631: t15.2023.11.04 val PER: 0.0717
2026-01-10 15:42:40,631: t15.2023.11.17 val PER: 0.1353
2026-01-10 15:42:40,631: t15.2023.11.19 val PER: 0.1098
2026-01-10 15:42:40,631: t15.2023.11.26 val PER: 0.2703
2026-01-10 15:42:40,631: t15.2023.12.03 val PER: 0.2101
2026-01-10 15:42:40,631: t15.2023.12.08 val PER: 0.2443
2026-01-10 15:42:40,631: t15.2023.12.10 val PER: 0.2155
2026-01-10 15:42:40,631: t15.2023.12.17 val PER: 0.2890
2026-01-10 15:42:40,631: t15.2023.12.29 val PER: 0.2402
2026-01-10 15:42:40,632: t15.2024.02.25 val PER: 0.1882
2026-01-10 15:42:40,632: t15.2024.03.08 val PER: 0.3186
2026-01-10 15:42:40,632: t15.2024.03.15 val PER: 0.3102
2026-01-10 15:42:40,632: t15.2024.03.17 val PER: 0.2580
2026-01-10 15:42:40,632: t15.2024.05.10 val PER: 0.2437
2026-01-10 15:42:40,632: t15.2024.06.14 val PER: 0.2634
2026-01-10 15:42:40,632: t15.2024.07.19 val PER: 0.3593
2026-01-10 15:42:40,632: t15.2024.07.21 val PER: 0.2014
2026-01-10 15:42:40,632: t15.2024.07.28 val PER: 0.2368
2026-01-10 15:42:40,633: t15.2025.01.10 val PER: 0.4256
2026-01-10 15:42:40,633: t15.2025.01.12 val PER: 0.2810
2026-01-10 15:42:40,633: t15.2025.03.14 val PER: 0.4615
2026-01-10 15:42:40,633: t15.2025.03.16 val PER: 0.3377
2026-01-10 15:42:40,633: t15.2025.03.30 val PER: 0.4092
2026-01-10 15:42:40,633: t15.2025.04.13 val PER: 0.3438
2026-01-10 15:42:40,688: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_16000
2026-01-10 15:45:08,342: Train batch 16200: loss: 19.35 grad norm: 61.63 time: 0.912
2026-01-10 15:47:35,663: Train batch 16400: loss: 28.54 grad norm: 83.46 time: 1.017
2026-01-10 15:48:49,358: Running test after training batch: 16500
2026-01-10 15:48:49,589: WER debug GT example: You can see the code at this point as well.
2026-01-10 15:49:07,541: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it ent as l
2026-01-10 15:49:07,573: WER debug example
  GT : how does it keep the cost down
  PR : houde us it epp uthe os ed
2026-01-10 15:49:09,321: Val batch 16500: PER (avg): 0.2523 CTC Loss (avg): 24.6167 WER(1gram): 88.07% (n=64) time: 19.962
2026-01-10 15:49:09,321: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-10 15:49:09,321: t15.2023.08.13 val PER: 0.2162
2026-01-10 15:49:09,321: t15.2023.08.18 val PER: 0.1861
2026-01-10 15:49:09,321: t15.2023.08.20 val PER: 0.1946
2026-01-10 15:49:09,321: t15.2023.08.25 val PER: 0.1657
2026-01-10 15:49:09,321: t15.2023.08.27 val PER: 0.2958
2026-01-10 15:49:09,321: t15.2023.09.01 val PER: 0.1591
2026-01-10 15:49:09,322: t15.2023.09.03 val PER: 0.2565
2026-01-10 15:49:09,322: t15.2023.09.24 val PER: 0.1917
2026-01-10 15:49:09,322: t15.2023.09.29 val PER: 0.2100
2026-01-10 15:49:09,322: t15.2023.10.01 val PER: 0.2497
2026-01-10 15:49:09,322: t15.2023.10.06 val PER: 0.1830
2026-01-10 15:49:09,322: t15.2023.10.08 val PER: 0.2788
2026-01-10 15:49:09,322: t15.2023.10.13 val PER: 0.2886
2026-01-10 15:49:09,322: t15.2023.10.15 val PER: 0.2314
2026-01-10 15:49:09,322: t15.2023.10.20 val PER: 0.2148
2026-01-10 15:49:09,322: t15.2023.10.22 val PER: 0.2094
2026-01-10 15:49:09,322: t15.2023.11.03 val PER: 0.2503
2026-01-10 15:49:09,322: t15.2023.11.04 val PER: 0.0717
2026-01-10 15:49:09,322: t15.2023.11.17 val PER: 0.1384
2026-01-10 15:49:09,322: t15.2023.11.19 val PER: 0.1098
2026-01-10 15:49:09,322: t15.2023.11.26 val PER: 0.2681
2026-01-10 15:49:09,323: t15.2023.12.03 val PER: 0.2143
2026-01-10 15:49:09,323: t15.2023.12.08 val PER: 0.2383
2026-01-10 15:49:09,323: t15.2023.12.10 val PER: 0.2116
2026-01-10 15:49:09,323: t15.2023.12.17 val PER: 0.2879
2026-01-10 15:49:09,323: t15.2023.12.29 val PER: 0.2471
2026-01-10 15:49:09,323: t15.2024.02.25 val PER: 0.1896
2026-01-10 15:49:09,323: t15.2024.03.08 val PER: 0.3172
2026-01-10 15:49:09,323: t15.2024.03.15 val PER: 0.3089
2026-01-10 15:49:09,323: t15.2024.03.17 val PER: 0.2580
2026-01-10 15:49:09,323: t15.2024.05.10 val PER: 0.2541
2026-01-10 15:49:09,323: t15.2024.06.14 val PER: 0.2697
2026-01-10 15:49:09,323: t15.2024.07.19 val PER: 0.3586
2026-01-10 15:49:09,324: t15.2024.07.21 val PER: 0.2110
2026-01-10 15:49:09,324: t15.2024.07.28 val PER: 0.2404
2026-01-10 15:49:09,324: t15.2025.01.10 val PER: 0.4256
2026-01-10 15:49:09,324: t15.2025.01.12 val PER: 0.2848
2026-01-10 15:49:09,324: t15.2025.03.14 val PER: 0.4630
2026-01-10 15:49:09,324: t15.2025.03.16 val PER: 0.3312
2026-01-10 15:49:09,324: t15.2025.03.30 val PER: 0.4057
2026-01-10 15:49:09,324: t15.2025.04.13 val PER: 0.3452
2026-01-10 15:49:09,380: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_16500
2026-01-10 15:50:23,057: Train batch 16600: loss: 27.39 grad norm: 78.53 time: 0.743
2026-01-10 15:52:51,944: Train batch 16800: loss: 22.27 grad norm: 62.75 time: 1.103
2026-01-10 15:55:23,632: Train batch 17000: loss: 26.87 grad norm: 85.35 time: 0.612
2026-01-10 15:55:23,635: Running test after training batch: 17000
2026-01-10 15:55:23,860: WER debug GT example: You can see the code at this point as well.
2026-01-10 15:55:41,560: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it ent as l
2026-01-10 15:55:41,591: WER debug example
  GT : how does it keep the cost down
  PR : houde us it epes uthe us ed
2026-01-10 15:55:43,328: Val batch 17000: PER (avg): 0.2516 CTC Loss (avg): 24.6354 WER(1gram): 88.58% (n=64) time: 19.692
2026-01-10 15:55:43,328: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-10 15:55:43,329: t15.2023.08.13 val PER: 0.2141
2026-01-10 15:55:43,329: t15.2023.08.18 val PER: 0.1920
2026-01-10 15:55:43,329: t15.2023.08.20 val PER: 0.1890
2026-01-10 15:55:43,329: t15.2023.08.25 val PER: 0.1717
2026-01-10 15:55:43,329: t15.2023.08.27 val PER: 0.2942
2026-01-10 15:55:43,329: t15.2023.09.01 val PER: 0.1591
2026-01-10 15:55:43,329: t15.2023.09.03 val PER: 0.2648
2026-01-10 15:55:43,329: t15.2023.09.24 val PER: 0.1954
2026-01-10 15:55:43,329: t15.2023.09.29 val PER: 0.2093
2026-01-10 15:55:43,329: t15.2023.10.01 val PER: 0.2444
2026-01-10 15:55:43,329: t15.2023.10.06 val PER: 0.1851
2026-01-10 15:55:43,329: t15.2023.10.08 val PER: 0.2733
2026-01-10 15:55:43,329: t15.2023.10.13 val PER: 0.2878
2026-01-10 15:55:43,329: t15.2023.10.15 val PER: 0.2367
2026-01-10 15:55:43,330: t15.2023.10.20 val PER: 0.2148
2026-01-10 15:55:43,330: t15.2023.10.22 val PER: 0.2071
2026-01-10 15:55:43,330: t15.2023.11.03 val PER: 0.2571
2026-01-10 15:55:43,330: t15.2023.11.04 val PER: 0.0717
2026-01-10 15:55:43,330: t15.2023.11.17 val PER: 0.1322
2026-01-10 15:55:43,330: t15.2023.11.19 val PER: 0.1038
2026-01-10 15:55:43,330: t15.2023.11.26 val PER: 0.2688
2026-01-10 15:55:43,330: t15.2023.12.03 val PER: 0.2164
2026-01-10 15:55:43,330: t15.2023.12.08 val PER: 0.2437
2026-01-10 15:55:43,330: t15.2023.12.10 val PER: 0.2142
2026-01-10 15:55:43,330: t15.2023.12.17 val PER: 0.2807
2026-01-10 15:55:43,330: t15.2023.12.29 val PER: 0.2409
2026-01-10 15:55:43,330: t15.2024.02.25 val PER: 0.1896
2026-01-10 15:55:43,330: t15.2024.03.08 val PER: 0.3257
2026-01-10 15:55:43,330: t15.2024.03.15 val PER: 0.3158
2026-01-10 15:55:43,331: t15.2024.03.17 val PER: 0.2483
2026-01-10 15:55:43,331: t15.2024.05.10 val PER: 0.2437
2026-01-10 15:55:43,331: t15.2024.06.14 val PER: 0.2681
2026-01-10 15:55:43,331: t15.2024.07.19 val PER: 0.3546
2026-01-10 15:55:43,331: t15.2024.07.21 val PER: 0.2007
2026-01-10 15:55:43,331: t15.2024.07.28 val PER: 0.2353
2026-01-10 15:55:43,331: t15.2025.01.10 val PER: 0.4187
2026-01-10 15:55:43,331: t15.2025.01.12 val PER: 0.2856
2026-01-10 15:55:43,331: t15.2025.03.14 val PER: 0.4586
2026-01-10 15:55:43,331: t15.2025.03.16 val PER: 0.3285
2026-01-10 15:55:43,332: t15.2025.03.30 val PER: 0.4149
2026-01-10 15:55:43,332: t15.2025.04.13 val PER: 0.3466
2026-01-10 15:55:43,389: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_17000
2026-01-10 15:58:14,578: Train batch 17200: loss: 20.49 grad norm: 66.50 time: 0.676
2026-01-10 16:00:46,194: Train batch 17400: loss: 19.93 grad norm: 67.57 time: 0.555
2026-01-10 16:02:01,350: Running test after training batch: 17500
2026-01-10 16:02:01,576: WER debug GT example: You can see the code at this point as well.
2026-01-10 16:02:19,214: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it ent as l
2026-01-10 16:02:19,246: WER debug example
  GT : how does it keep the cost down
  PR : houde us it epes uthe us ed
2026-01-10 16:02:20,984: Val batch 17500: PER (avg): 0.2514 CTC Loss (avg): 24.5652 WER(1gram): 88.58% (n=64) time: 19.633
2026-01-10 16:02:20,984: WER lens: avg_true_words=6.16 avg_pred_words=6.25 max_pred_words=12
2026-01-10 16:02:20,984: t15.2023.08.13 val PER: 0.2110
2026-01-10 16:02:20,984: t15.2023.08.18 val PER: 0.1836
2026-01-10 16:02:20,984: t15.2023.08.20 val PER: 0.1914
2026-01-10 16:02:20,984: t15.2023.08.25 val PER: 0.1672
2026-01-10 16:02:20,985: t15.2023.08.27 val PER: 0.2926
2026-01-10 16:02:20,985: t15.2023.09.01 val PER: 0.1623
2026-01-10 16:02:20,985: t15.2023.09.03 val PER: 0.2637
2026-01-10 16:02:20,985: t15.2023.09.24 val PER: 0.1990
2026-01-10 16:02:20,985: t15.2023.09.29 val PER: 0.2042
2026-01-10 16:02:20,985: t15.2023.10.01 val PER: 0.2483
2026-01-10 16:02:20,985: t15.2023.10.06 val PER: 0.1776
2026-01-10 16:02:20,985: t15.2023.10.08 val PER: 0.2720
2026-01-10 16:02:20,985: t15.2023.10.13 val PER: 0.2863
2026-01-10 16:02:20,985: t15.2023.10.15 val PER: 0.2353
2026-01-10 16:02:20,985: t15.2023.10.20 val PER: 0.2282
2026-01-10 16:02:20,985: t15.2023.10.22 val PER: 0.2071
2026-01-10 16:02:20,985: t15.2023.11.03 val PER: 0.2510
2026-01-10 16:02:20,985: t15.2023.11.04 val PER: 0.0717
2026-01-10 16:02:20,986: t15.2023.11.17 val PER: 0.1369
2026-01-10 16:02:20,986: t15.2023.11.19 val PER: 0.1018
2026-01-10 16:02:20,986: t15.2023.11.26 val PER: 0.2696
2026-01-10 16:02:20,986: t15.2023.12.03 val PER: 0.2111
2026-01-10 16:02:20,986: t15.2023.12.08 val PER: 0.2457
2026-01-10 16:02:20,986: t15.2023.12.10 val PER: 0.2089
2026-01-10 16:02:20,986: t15.2023.12.17 val PER: 0.2838
2026-01-10 16:02:20,986: t15.2023.12.29 val PER: 0.2423
2026-01-10 16:02:20,986: t15.2024.02.25 val PER: 0.1952
2026-01-10 16:02:20,986: t15.2024.03.08 val PER: 0.3272
2026-01-10 16:02:20,986: t15.2024.03.15 val PER: 0.3146
2026-01-10 16:02:20,987: t15.2024.03.17 val PER: 0.2531
2026-01-10 16:02:20,987: t15.2024.05.10 val PER: 0.2437
2026-01-10 16:02:20,987: t15.2024.06.14 val PER: 0.2634
2026-01-10 16:02:20,987: t15.2024.07.19 val PER: 0.3527
2026-01-10 16:02:20,987: t15.2024.07.21 val PER: 0.2007
2026-01-10 16:02:20,987: t15.2024.07.28 val PER: 0.2463
2026-01-10 16:02:20,987: t15.2025.01.10 val PER: 0.4201
2026-01-10 16:02:20,987: t15.2025.01.12 val PER: 0.2818
2026-01-10 16:02:20,987: t15.2025.03.14 val PER: 0.4689
2026-01-10 16:02:20,987: t15.2025.03.16 val PER: 0.3325
2026-01-10 16:02:20,987: t15.2025.03.30 val PER: 0.4092
2026-01-10 16:02:20,987: t15.2025.04.13 val PER: 0.3424
2026-01-10 16:02:21,046: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_17500
2026-01-10 16:03:35,953: Train batch 17600: loss: 16.19 grad norm: 63.35 time: 0.536
2026-01-10 16:06:02,789: Train batch 17800: loss: 24.25 grad norm: 74.27 time: 0.775
2026-01-10 16:08:29,785: Train batch 18000: loss: 20.21 grad norm: 63.91 time: 0.673
2026-01-10 16:08:29,787: Running test after training batch: 18000
2026-01-10 16:08:30,032: WER debug GT example: You can see the code at this point as well.
2026-01-10 16:08:47,902: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it ent as l
2026-01-10 16:08:47,934: WER debug example
  GT : how does it keep the cost down
  PR : houde us it epp uthe os ed
2026-01-10 16:08:49,687: Val batch 18000: PER (avg): 0.2509 CTC Loss (avg): 24.5019 WER(1gram): 88.32% (n=64) time: 19.898
2026-01-10 16:08:49,688: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-10 16:08:49,688: t15.2023.08.13 val PER: 0.2110
2026-01-10 16:08:49,688: t15.2023.08.18 val PER: 0.1894
2026-01-10 16:08:49,688: t15.2023.08.20 val PER: 0.1906
2026-01-10 16:08:49,688: t15.2023.08.25 val PER: 0.1611
2026-01-10 16:08:49,688: t15.2023.08.27 val PER: 0.2878
2026-01-10 16:08:49,688: t15.2023.09.01 val PER: 0.1631
2026-01-10 16:08:49,688: t15.2023.09.03 val PER: 0.2577
2026-01-10 16:08:49,688: t15.2023.09.24 val PER: 0.1966
2026-01-10 16:08:49,689: t15.2023.09.29 val PER: 0.2170
2026-01-10 16:08:49,689: t15.2023.10.01 val PER: 0.2477
2026-01-10 16:08:49,689: t15.2023.10.06 val PER: 0.1841
2026-01-10 16:08:49,689: t15.2023.10.08 val PER: 0.2774
2026-01-10 16:08:49,689: t15.2023.10.13 val PER: 0.2901
2026-01-10 16:08:49,689: t15.2023.10.15 val PER: 0.2301
2026-01-10 16:08:49,689: t15.2023.10.20 val PER: 0.2148
2026-01-10 16:08:49,689: t15.2023.10.22 val PER: 0.2094
2026-01-10 16:08:49,689: t15.2023.11.03 val PER: 0.2422
2026-01-10 16:08:49,689: t15.2023.11.04 val PER: 0.0717
2026-01-10 16:08:49,689: t15.2023.11.17 val PER: 0.1353
2026-01-10 16:08:49,689: t15.2023.11.19 val PER: 0.1118
2026-01-10 16:08:49,689: t15.2023.11.26 val PER: 0.2667
2026-01-10 16:08:49,689: t15.2023.12.03 val PER: 0.2153
2026-01-10 16:08:49,689: t15.2023.12.08 val PER: 0.2423
2026-01-10 16:08:49,690: t15.2023.12.10 val PER: 0.2050
2026-01-10 16:08:49,690: t15.2023.12.17 val PER: 0.2827
2026-01-10 16:08:49,690: t15.2023.12.29 val PER: 0.2423
2026-01-10 16:08:49,690: t15.2024.02.25 val PER: 0.1910
2026-01-10 16:08:49,690: t15.2024.03.08 val PER: 0.3243
2026-01-10 16:08:49,690: t15.2024.03.15 val PER: 0.3152
2026-01-10 16:08:49,690: t15.2024.03.17 val PER: 0.2497
2026-01-10 16:08:49,690: t15.2024.05.10 val PER: 0.2481
2026-01-10 16:08:49,690: t15.2024.06.14 val PER: 0.2587
2026-01-10 16:08:49,690: t15.2024.07.19 val PER: 0.3579
2026-01-10 16:08:49,690: t15.2024.07.21 val PER: 0.2021
2026-01-10 16:08:49,690: t15.2024.07.28 val PER: 0.2382
2026-01-10 16:08:49,690: t15.2025.01.10 val PER: 0.4366
2026-01-10 16:08:49,690: t15.2025.01.12 val PER: 0.2771
2026-01-10 16:08:49,690: t15.2025.03.14 val PER: 0.4541
2026-01-10 16:08:49,690: t15.2025.03.16 val PER: 0.3272
2026-01-10 16:08:49,691: t15.2025.03.30 val PER: 0.4057
2026-01-10 16:08:49,691: t15.2025.04.13 val PER: 0.3424
2026-01-10 16:08:49,748: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_18000
2026-01-10 16:11:25,726: Train batch 18200: loss: 32.02 grad norm: 90.16 time: 1.041
2026-01-10 16:14:04,760: Train batch 18400: loss: 16.10 grad norm: 62.21 time: 0.813
2026-01-10 16:15:19,598: Running test after training batch: 18500
2026-01-10 16:15:19,830: WER debug GT example: You can see the code at this point as well.
2026-01-10 16:15:37,910: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it ent as l
2026-01-10 16:15:37,942: WER debug example
  GT : how does it keep the cost down
  PR : houde us it epes uthe os ed
2026-01-10 16:15:39,693: Val batch 18500: PER (avg): 0.2498 CTC Loss (avg): 24.4014 WER(1gram): 87.82% (n=64) time: 20.093
2026-01-10 16:15:39,693: WER lens: avg_true_words=6.16 avg_pred_words=6.23 max_pred_words=12
2026-01-10 16:15:39,694: t15.2023.08.13 val PER: 0.2110
2026-01-10 16:15:39,694: t15.2023.08.18 val PER: 0.1911
2026-01-10 16:15:39,694: t15.2023.08.20 val PER: 0.1867
2026-01-10 16:15:39,694: t15.2023.08.25 val PER: 0.1672
2026-01-10 16:15:39,694: t15.2023.08.27 val PER: 0.2926
2026-01-10 16:15:39,694: t15.2023.09.01 val PER: 0.1542
2026-01-10 16:15:39,694: t15.2023.09.03 val PER: 0.2577
2026-01-10 16:15:39,694: t15.2023.09.24 val PER: 0.1954
2026-01-10 16:15:39,694: t15.2023.09.29 val PER: 0.2074
2026-01-10 16:15:39,694: t15.2023.10.01 val PER: 0.2431
2026-01-10 16:15:39,694: t15.2023.10.06 val PER: 0.1776
2026-01-10 16:15:39,694: t15.2023.10.08 val PER: 0.2774
2026-01-10 16:15:39,695: t15.2023.10.13 val PER: 0.2870
2026-01-10 16:15:39,695: t15.2023.10.15 val PER: 0.2281
2026-01-10 16:15:39,695: t15.2023.10.20 val PER: 0.2181
2026-01-10 16:15:39,695: t15.2023.10.22 val PER: 0.2071
2026-01-10 16:15:39,695: t15.2023.11.03 val PER: 0.2490
2026-01-10 16:15:39,695: t15.2023.11.04 val PER: 0.0751
2026-01-10 16:15:39,695: t15.2023.11.17 val PER: 0.1446
2026-01-10 16:15:39,695: t15.2023.11.19 val PER: 0.1058
2026-01-10 16:15:39,695: t15.2023.11.26 val PER: 0.2659
2026-01-10 16:15:39,695: t15.2023.12.03 val PER: 0.2069
2026-01-10 16:15:39,695: t15.2023.12.08 val PER: 0.2417
2026-01-10 16:15:39,696: t15.2023.12.10 val PER: 0.2076
2026-01-10 16:15:39,696: t15.2023.12.17 val PER: 0.2848
2026-01-10 16:15:39,696: t15.2023.12.29 val PER: 0.2457
2026-01-10 16:15:39,696: t15.2024.02.25 val PER: 0.1826
2026-01-10 16:15:39,696: t15.2024.03.08 val PER: 0.3229
2026-01-10 16:15:39,696: t15.2024.03.15 val PER: 0.3164
2026-01-10 16:15:39,696: t15.2024.03.17 val PER: 0.2490
2026-01-10 16:15:39,696: t15.2024.05.10 val PER: 0.2437
2026-01-10 16:15:39,696: t15.2024.06.14 val PER: 0.2681
2026-01-10 16:15:39,696: t15.2024.07.19 val PER: 0.3540
2026-01-10 16:15:39,696: t15.2024.07.21 val PER: 0.2048
2026-01-10 16:15:39,696: t15.2024.07.28 val PER: 0.2331
2026-01-10 16:15:39,696: t15.2025.01.10 val PER: 0.4284
2026-01-10 16:15:39,696: t15.2025.01.12 val PER: 0.2794
2026-01-10 16:15:39,696: t15.2025.03.14 val PER: 0.4586
2026-01-10 16:15:39,696: t15.2025.03.16 val PER: 0.3233
2026-01-10 16:15:39,697: t15.2025.03.30 val PER: 0.4046
2026-01-10 16:15:39,697: t15.2025.04.13 val PER: 0.3438
2026-01-10 16:15:39,755: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_18500
2026-01-10 16:16:58,571: Train batch 18600: loss: 16.38 grad norm: 59.03 time: 0.885
2026-01-10 16:19:34,747: Train batch 18800: loss: 20.47 grad norm: 64.66 time: 1.256
2026-01-10 16:22:12,829: Train batch 19000: loss: 29.84 grad norm: 81.39 time: 0.688
2026-01-10 16:22:12,831: Running test after training batch: 19000
2026-01-10 16:22:13,059: WER debug GT example: You can see the code at this point as well.
2026-01-10 16:22:31,217: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it ent as l
2026-01-10 16:22:31,248: WER debug example
  GT : how does it keep the cost down
  PR : houde us it epes uthe os ed
2026-01-10 16:22:33,006: Val batch 19000: PER (avg): 0.2496 CTC Loss (avg): 24.3879 WER(1gram): 88.07% (n=64) time: 20.173
2026-01-10 16:22:33,006: WER lens: avg_true_words=6.16 avg_pred_words=6.25 max_pred_words=12
2026-01-10 16:22:33,006: t15.2023.08.13 val PER: 0.2131
2026-01-10 16:22:33,006: t15.2023.08.18 val PER: 0.1936
2026-01-10 16:22:33,006: t15.2023.08.20 val PER: 0.1890
2026-01-10 16:22:33,006: t15.2023.08.25 val PER: 0.1717
2026-01-10 16:22:33,007: t15.2023.08.27 val PER: 0.2862
2026-01-10 16:22:33,007: t15.2023.09.01 val PER: 0.1542
2026-01-10 16:22:33,007: t15.2023.09.03 val PER: 0.2589
2026-01-10 16:22:33,007: t15.2023.09.24 val PER: 0.1990
2026-01-10 16:22:33,007: t15.2023.09.29 val PER: 0.2106
2026-01-10 16:22:33,007: t15.2023.10.01 val PER: 0.2431
2026-01-10 16:22:33,007: t15.2023.10.06 val PER: 0.1776
2026-01-10 16:22:33,007: t15.2023.10.08 val PER: 0.2774
2026-01-10 16:22:33,007: t15.2023.10.13 val PER: 0.2824
2026-01-10 16:22:33,007: t15.2023.10.15 val PER: 0.2301
2026-01-10 16:22:33,007: t15.2023.10.20 val PER: 0.2215
2026-01-10 16:22:33,007: t15.2023.10.22 val PER: 0.2060
2026-01-10 16:22:33,007: t15.2023.11.03 val PER: 0.2490
2026-01-10 16:22:33,007: t15.2023.11.04 val PER: 0.0717
2026-01-10 16:22:33,008: t15.2023.11.17 val PER: 0.1369
2026-01-10 16:22:33,008: t15.2023.11.19 val PER: 0.1078
2026-01-10 16:22:33,008: t15.2023.11.26 val PER: 0.2696
2026-01-10 16:22:33,008: t15.2023.12.03 val PER: 0.2090
2026-01-10 16:22:33,008: t15.2023.12.08 val PER: 0.2437
2026-01-10 16:22:33,008: t15.2023.12.10 val PER: 0.2050
2026-01-10 16:22:33,008: t15.2023.12.17 val PER: 0.2827
2026-01-10 16:22:33,008: t15.2023.12.29 val PER: 0.2416
2026-01-10 16:22:33,009: t15.2024.02.25 val PER: 0.1854
2026-01-10 16:22:33,009: t15.2024.03.08 val PER: 0.3229
2026-01-10 16:22:33,009: t15.2024.03.15 val PER: 0.3139
2026-01-10 16:22:33,009: t15.2024.03.17 val PER: 0.2483
2026-01-10 16:22:33,009: t15.2024.05.10 val PER: 0.2422
2026-01-10 16:22:33,009: t15.2024.06.14 val PER: 0.2650
2026-01-10 16:22:33,009: t15.2024.07.19 val PER: 0.3546
2026-01-10 16:22:33,009: t15.2024.07.21 val PER: 0.2028
2026-01-10 16:22:33,009: t15.2024.07.28 val PER: 0.2316
2026-01-10 16:22:33,009: t15.2025.01.10 val PER: 0.4242
2026-01-10 16:22:33,009: t15.2025.01.12 val PER: 0.2818
2026-01-10 16:22:33,009: t15.2025.03.14 val PER: 0.4527
2026-01-10 16:22:33,009: t15.2025.03.16 val PER: 0.3246
2026-01-10 16:22:33,009: t15.2025.03.30 val PER: 0.4034
2026-01-10 16:22:33,009: t15.2025.04.13 val PER: 0.3424
2026-01-10 16:22:33,064: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_19000
2026-01-10 16:25:09,894: Train batch 19200: loss: 26.19 grad norm: 81.82 time: 0.646
2026-01-10 16:27:47,069: Train batch 19400: loss: 26.17 grad norm: 80.00 time: 0.754
2026-01-10 16:29:01,571: Running test after training batch: 19500
2026-01-10 16:29:01,813: WER debug GT example: You can see the code at this point as well.
2026-01-10 16:29:19,834: WER debug example
  GT : you can see the code at this point as well
  PR : ude end ease utt owed at it ent as l
2026-01-10 16:29:19,865: WER debug example
  GT : how does it keep the cost down
  PR : houde us it epes uthe os ed
2026-01-10 16:29:21,635: Val batch 19500: PER (avg): 0.2496 CTC Loss (avg): 24.3698 WER(1gram): 88.32% (n=64) time: 20.064
2026-01-10 16:29:21,636: WER lens: avg_true_words=6.16 avg_pred_words=6.27 max_pred_words=12
2026-01-10 16:29:21,636: t15.2023.08.13 val PER: 0.2131
2026-01-10 16:29:21,636: t15.2023.08.18 val PER: 0.1911
2026-01-10 16:29:21,636: t15.2023.08.20 val PER: 0.1859
2026-01-10 16:29:21,636: t15.2023.08.25 val PER: 0.1672
2026-01-10 16:29:21,636: t15.2023.08.27 val PER: 0.2878
2026-01-10 16:29:21,636: t15.2023.09.01 val PER: 0.1583
2026-01-10 16:29:21,637: t15.2023.09.03 val PER: 0.2577
2026-01-10 16:29:21,637: t15.2023.09.24 val PER: 0.1966
2026-01-10 16:29:21,637: t15.2023.09.29 val PER: 0.2125
2026-01-10 16:29:21,637: t15.2023.10.01 val PER: 0.2483
2026-01-10 16:29:21,637: t15.2023.10.06 val PER: 0.1798
2026-01-10 16:29:21,637: t15.2023.10.08 val PER: 0.2801
2026-01-10 16:29:21,637: t15.2023.10.13 val PER: 0.2878
2026-01-10 16:29:21,637: t15.2023.10.15 val PER: 0.2281
2026-01-10 16:29:21,637: t15.2023.10.20 val PER: 0.2215
2026-01-10 16:29:21,637: t15.2023.10.22 val PER: 0.2071
2026-01-10 16:29:21,637: t15.2023.11.03 val PER: 0.2476
2026-01-10 16:29:21,638: t15.2023.11.04 val PER: 0.0717
2026-01-10 16:29:21,638: t15.2023.11.17 val PER: 0.1384
2026-01-10 16:29:21,638: t15.2023.11.19 val PER: 0.1058
2026-01-10 16:29:21,638: t15.2023.11.26 val PER: 0.2674
2026-01-10 16:29:21,638: t15.2023.12.03 val PER: 0.2132
2026-01-10 16:29:21,638: t15.2023.12.08 val PER: 0.2390
2026-01-10 16:29:21,638: t15.2023.12.10 val PER: 0.2037
2026-01-10 16:29:21,638: t15.2023.12.17 val PER: 0.2786
2026-01-10 16:29:21,638: t15.2023.12.29 val PER: 0.2450
2026-01-10 16:29:21,638: t15.2024.02.25 val PER: 0.1826
2026-01-10 16:29:21,638: t15.2024.03.08 val PER: 0.3172
2026-01-10 16:29:21,639: t15.2024.03.15 val PER: 0.3102
2026-01-10 16:29:21,639: t15.2024.03.17 val PER: 0.2503
2026-01-10 16:29:21,639: t15.2024.05.10 val PER: 0.2467
2026-01-10 16:29:21,639: t15.2024.06.14 val PER: 0.2681
2026-01-10 16:29:21,639: t15.2024.07.19 val PER: 0.3507
2026-01-10 16:29:21,639: t15.2024.07.21 val PER: 0.2041
2026-01-10 16:29:21,639: t15.2024.07.28 val PER: 0.2338
2026-01-10 16:29:21,639: t15.2025.01.10 val PER: 0.4256
2026-01-10 16:29:21,639: t15.2025.01.12 val PER: 0.2771
2026-01-10 16:29:21,639: t15.2025.03.14 val PER: 0.4586
2026-01-10 16:29:21,640: t15.2025.03.16 val PER: 0.3285
2026-01-10 16:29:21,640: t15.2025.03.30 val PER: 0.4034
2026-01-10 16:29:21,640: t15.2025.04.13 val PER: 0.3424
2026-01-10 16:29:21,696: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/small/xlstm_small_lr25/checkpoint/checkpoint_batch_19500
