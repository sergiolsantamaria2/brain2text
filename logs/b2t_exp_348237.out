TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_348237
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_348237/torch_extensions
WANDB_DIR=/tmp/e12511253_b2t_348237/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/e12511253_b2t_348237/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  3 21:13 /tmp/e12511253_b2t_348237/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
trained_models -> /tmp/e12511253_b2t_348237/trained_models
OUT_ROOT=/tmp/e12511253_b2t_348237/trained_models
==============================================
Job: b2t_exp  ID: 348237
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/gru/rnn_dropout/
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/gru/rnn_dropout/ ==========
Num configs: 6

=== RUN base.yaml ===
2026-01-03 21:13:36,760: Using device: cuda:0
2026-01-03 21:13:38,443: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-03 21:13:38,465: Using 45 sessions after filtering (from 45).
2026-01-03 21:13:38,865: Using torch.compile (if available)
2026-01-03 21:13:38,866: torch.compile not available (torch<2.0). Skipping.
2026-01-03 21:13:38,866: Initialized RNN decoding model
2026-01-03 21:13:38,866: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 21:13:38,867: Model has 44,907,305 parameters
2026-01-03 21:13:38,867: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 21:13:40,128: Successfully initialized datasets
2026-01-03 21:13:40,128: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 21:13:41,111: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.176
2026-01-03 21:13:41,111: Running test after training batch: 0
2026-01-03 21:13:41,222: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:13:46,539: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-03 21:13:47,243: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-03 21:14:21,371: Val batch 0: PER (avg): 1.4306 CTC Loss (avg): 633.3639 WER(1gram): 100.00% (n=64) time: 40.260
2026-01-03 21:14:21,372: WER lens: avg_true_words=6.16 avg_pred_words=1.78 max_pred_words=4
2026-01-03 21:14:21,372: t15.2023.08.13 val PER: 1.3046
2026-01-03 21:14:21,372: t15.2023.08.18 val PER: 1.4283
2026-01-03 21:14:21,372: t15.2023.08.20 val PER: 1.3018
2026-01-03 21:14:21,372: t15.2023.08.25 val PER: 1.3358
2026-01-03 21:14:21,372: t15.2023.08.27 val PER: 1.2524
2026-01-03 21:14:21,373: t15.2023.09.01 val PER: 1.4529
2026-01-03 21:14:21,373: t15.2023.09.03 val PER: 1.3171
2026-01-03 21:14:21,373: t15.2023.09.24 val PER: 1.5400
2026-01-03 21:14:21,373: t15.2023.09.29 val PER: 1.4671
2026-01-03 21:14:21,373: t15.2023.10.01 val PER: 1.2173
2026-01-03 21:14:21,373: t15.2023.10.06 val PER: 1.4909
2026-01-03 21:14:21,373: t15.2023.10.08 val PER: 1.1908
2026-01-03 21:14:21,373: t15.2023.10.13 val PER: 1.3933
2026-01-03 21:14:21,373: t15.2023.10.15 val PER: 1.3869
2026-01-03 21:14:21,373: t15.2023.10.20 val PER: 1.5000
2026-01-03 21:14:21,373: t15.2023.10.22 val PER: 1.3886
2026-01-03 21:14:21,373: t15.2023.11.03 val PER: 1.5977
2026-01-03 21:14:21,373: t15.2023.11.04 val PER: 2.0444
2026-01-03 21:14:21,374: t15.2023.11.17 val PER: 1.9580
2026-01-03 21:14:21,374: t15.2023.11.19 val PER: 1.6766
2026-01-03 21:14:21,374: t15.2023.11.26 val PER: 1.5406
2026-01-03 21:14:21,374: t15.2023.12.03 val PER: 1.4338
2026-01-03 21:14:21,374: t15.2023.12.08 val PER: 1.4501
2026-01-03 21:14:21,374: t15.2023.12.10 val PER: 1.6991
2026-01-03 21:14:21,374: t15.2023.12.17 val PER: 1.3087
2026-01-03 21:14:21,374: t15.2023.12.29 val PER: 1.4139
2026-01-03 21:14:21,374: t15.2024.02.25 val PER: 1.4199
2026-01-03 21:14:21,374: t15.2024.03.08 val PER: 1.3243
2026-01-03 21:14:21,374: t15.2024.03.15 val PER: 1.3177
2026-01-03 21:14:21,375: t15.2024.03.17 val PER: 1.4017
2026-01-03 21:14:21,375: t15.2024.05.10 val PER: 1.3284
2026-01-03 21:14:21,375: t15.2024.06.14 val PER: 1.5363
2026-01-03 21:14:21,375: t15.2024.07.19 val PER: 1.0811
2026-01-03 21:14:21,375: t15.2024.07.21 val PER: 1.6317
2026-01-03 21:14:21,375: t15.2024.07.28 val PER: 1.6588
2026-01-03 21:14:21,375: t15.2025.01.10 val PER: 1.0868
2026-01-03 21:14:21,375: t15.2025.01.12 val PER: 1.7644
2026-01-03 21:14:21,375: t15.2025.03.14 val PER: 1.0399
2026-01-03 21:14:21,375: t15.2025.03.16 val PER: 1.6217
2026-01-03 21:14:21,375: t15.2025.03.30 val PER: 1.2920
2026-01-03 21:14:21,375: t15.2025.04.13 val PER: 1.5877
2026-01-03 21:14:21,376: New best val WER(1gram) inf% --> 100.00%
2026-01-03 21:14:21,653: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_0
2026-01-03 21:14:38,918: Train batch 200: loss: 80.58 grad norm: 80.15 time: 0.054
2026-01-03 21:14:55,788: Train batch 400: loss: 57.95 grad norm: 93.97 time: 0.063
2026-01-03 21:15:04,438: Running test after training batch: 500
2026-01-03 21:15:04,581: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:15:09,702: WER debug example
  GT : you can see the code at this point as well
  PR : yule eanes ooohs ooohs uhde at this ide is aisle
2026-01-03 21:15:09,740: WER debug example
  GT : how does it keep the cost down
  PR : ide does it ink thus as adz
2026-01-03 21:15:12,123: Val batch 500: PER (avg): 0.5530 CTC Loss (avg): 58.3429 WER(1gram): 91.37% (n=64) time: 7.682
2026-01-03 21:15:12,127: WER lens: avg_true_words=6.16 avg_pred_words=5.70 max_pred_words=12
2026-01-03 21:15:12,130: t15.2023.08.13 val PER: 0.4948
2026-01-03 21:15:12,131: t15.2023.08.18 val PER: 0.4895
2026-01-03 21:15:12,133: t15.2023.08.20 val PER: 0.4813
2026-01-03 21:15:12,135: t15.2023.08.25 val PER: 0.4654
2026-01-03 21:15:12,137: t15.2023.08.27 val PER: 0.5547
2026-01-03 21:15:12,138: t15.2023.09.01 val PER: 0.4554
2026-01-03 21:15:12,140: t15.2023.09.03 val PER: 0.5356
2026-01-03 21:15:12,141: t15.2023.09.24 val PER: 0.4757
2026-01-03 21:15:12,143: t15.2023.09.29 val PER: 0.4997
2026-01-03 21:15:12,144: t15.2023.10.01 val PER: 0.5476
2026-01-03 21:15:12,146: t15.2023.10.06 val PER: 0.4704
2026-01-03 21:15:12,147: t15.2023.10.08 val PER: 0.5724
2026-01-03 21:15:12,149: t15.2023.10.13 val PER: 0.5888
2026-01-03 21:15:12,151: t15.2023.10.15 val PER: 0.5208
2026-01-03 21:15:12,154: t15.2023.10.20 val PER: 0.4966
2026-01-03 21:15:12,155: t15.2023.10.22 val PER: 0.4855
2026-01-03 21:15:12,157: t15.2023.11.03 val PER: 0.5366
2026-01-03 21:15:12,158: t15.2023.11.04 val PER: 0.2935
2026-01-03 21:15:12,160: t15.2023.11.17 val PER: 0.3966
2026-01-03 21:15:12,161: t15.2023.11.19 val PER: 0.3792
2026-01-03 21:15:12,163: t15.2023.11.26 val PER: 0.5935
2026-01-03 21:15:12,165: t15.2023.12.03 val PER: 0.5399
2026-01-03 21:15:12,166: t15.2023.12.08 val PER: 0.5546
2026-01-03 21:15:12,168: t15.2023.12.10 val PER: 0.4967
2026-01-03 21:15:12,169: t15.2023.12.17 val PER: 0.6185
2026-01-03 21:15:12,171: t15.2023.12.29 val PER: 0.5889
2026-01-03 21:15:12,173: t15.2024.02.25 val PER: 0.5211
2026-01-03 21:15:12,174: t15.2024.03.08 val PER: 0.6415
2026-01-03 21:15:12,176: t15.2024.03.15 val PER: 0.5879
2026-01-03 21:15:12,177: t15.2024.03.17 val PER: 0.5391
2026-01-03 21:15:12,179: t15.2024.05.10 val PER: 0.5780
2026-01-03 21:15:12,180: t15.2024.06.14 val PER: 0.5410
2026-01-03 21:15:12,182: t15.2024.07.19 val PER: 0.7053
2026-01-03 21:15:12,183: t15.2024.07.21 val PER: 0.5021
2026-01-03 21:15:12,185: t15.2024.07.28 val PER: 0.5581
2026-01-03 21:15:12,186: t15.2025.01.10 val PER: 0.7658
2026-01-03 21:15:12,188: t15.2025.01.12 val PER: 0.6020
2026-01-03 21:15:12,189: t15.2025.03.14 val PER: 0.7751
2026-01-03 21:15:12,191: t15.2025.03.16 val PER: 0.6296
2026-01-03 21:15:12,192: t15.2025.03.30 val PER: 0.7425
2026-01-03 21:15:12,194: t15.2025.04.13 val PER: 0.6120
2026-01-03 21:15:12,196: New best val WER(1gram) 100.00% --> 91.37%
2026-01-03 21:15:12,531: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_500
2026-01-03 21:15:21,177: Train batch 600: loss: 51.35 grad norm: 80.53 time: 0.078
2026-01-03 21:15:38,319: Train batch 800: loss: 43.14 grad norm: 88.90 time: 0.058
2026-01-03 21:15:55,663: Train batch 1000: loss: 45.34 grad norm: 79.46 time: 0.065
2026-01-03 21:15:55,666: Running test after training batch: 1000
2026-01-03 21:15:55,792: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:16:00,719: WER debug example
  GT : you can see the code at this point as well
  PR : used aunt ease thus uhde it its ide is oui
2026-01-03 21:16:00,753: WER debug example
  GT : how does it keep the cost down
  PR : houde is it ink that wass id
2026-01-03 21:16:02,745: Val batch 1000: PER (avg): 0.4351 CTC Loss (avg): 45.4198 WER(1gram): 86.04% (n=64) time: 7.077
2026-01-03 21:16:02,747: WER lens: avg_true_words=6.16 avg_pred_words=5.69 max_pred_words=12
2026-01-03 21:16:02,750: t15.2023.08.13 val PER: 0.4033
2026-01-03 21:16:02,751: t15.2023.08.18 val PER: 0.3588
2026-01-03 21:16:02,753: t15.2023.08.20 val PER: 0.3725
2026-01-03 21:16:02,754: t15.2023.08.25 val PER: 0.3193
2026-01-03 21:16:02,756: t15.2023.08.27 val PER: 0.4518
2026-01-03 21:16:02,757: t15.2023.09.01 val PER: 0.3295
2026-01-03 21:16:02,759: t15.2023.09.03 val PER: 0.4192
2026-01-03 21:16:02,760: t15.2023.09.24 val PER: 0.3592
2026-01-03 21:16:02,762: t15.2023.09.29 val PER: 0.3950
2026-01-03 21:16:02,764: t15.2023.10.01 val PER: 0.4267
2026-01-03 21:16:02,765: t15.2023.10.06 val PER: 0.3380
2026-01-03 21:16:02,767: t15.2023.10.08 val PER: 0.4628
2026-01-03 21:16:02,768: t15.2023.10.13 val PER: 0.4872
2026-01-03 21:16:02,769: t15.2023.10.15 val PER: 0.4041
2026-01-03 21:16:02,771: t15.2023.10.20 val PER: 0.4027
2026-01-03 21:16:02,772: t15.2023.10.22 val PER: 0.3708
2026-01-03 21:16:02,774: t15.2023.11.03 val PER: 0.4172
2026-01-03 21:16:02,776: t15.2023.11.04 val PER: 0.2184
2026-01-03 21:16:02,777: t15.2023.11.17 val PER: 0.2893
2026-01-03 21:16:02,779: t15.2023.11.19 val PER: 0.2315
2026-01-03 21:16:02,781: t15.2023.11.26 val PER: 0.4725
2026-01-03 21:16:02,783: t15.2023.12.03 val PER: 0.4265
2026-01-03 21:16:02,785: t15.2023.12.08 val PER: 0.4321
2026-01-03 21:16:02,786: t15.2023.12.10 val PER: 0.3666
2026-01-03 21:16:02,788: t15.2023.12.17 val PER: 0.4501
2026-01-03 21:16:02,789: t15.2023.12.29 val PER: 0.4303
2026-01-03 21:16:02,792: t15.2024.02.25 val PER: 0.3764
2026-01-03 21:16:02,794: t15.2024.03.08 val PER: 0.5292
2026-01-03 21:16:02,795: t15.2024.03.15 val PER: 0.4647
2026-01-03 21:16:02,797: t15.2024.03.17 val PER: 0.4324
2026-01-03 21:16:02,798: t15.2024.05.10 val PER: 0.4458
2026-01-03 21:16:02,800: t15.2024.06.14 val PER: 0.4338
2026-01-03 21:16:02,801: t15.2024.07.19 val PER: 0.5742
2026-01-03 21:16:02,803: t15.2024.07.21 val PER: 0.4069
2026-01-03 21:16:02,804: t15.2024.07.28 val PER: 0.4566
2026-01-03 21:16:02,806: t15.2025.01.10 val PER: 0.6556
2026-01-03 21:16:02,807: t15.2025.01.12 val PER: 0.4819
2026-01-03 21:16:02,809: t15.2025.03.14 val PER: 0.6568
2026-01-03 21:16:02,810: t15.2025.03.16 val PER: 0.5026
2026-01-03 21:16:02,811: t15.2025.03.30 val PER: 0.6667
2026-01-03 21:16:02,813: t15.2025.04.13 val PER: 0.5207
2026-01-03 21:16:02,814: New best val WER(1gram) 91.37% --> 86.04%
2026-01-03 21:16:03,148: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_1000
2026-01-03 21:16:20,073: Train batch 1200: loss: 36.35 grad norm: 80.06 time: 0.068
2026-01-03 21:16:37,693: Train batch 1400: loss: 39.52 grad norm: 85.81 time: 0.060
2026-01-03 21:16:46,483: Running test after training batch: 1500
2026-01-03 21:16:46,670: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:16:51,643: WER debug example
  GT : you can see the code at this point as well
  PR : yule kint ease utt code at this boyde is wheel
2026-01-03 21:16:51,676: WER debug example
  GT : how does it keep the cost down
  PR : houde is it eke that os it
2026-01-03 21:16:53,350: Val batch 1500: PER (avg): 0.4026 CTC Loss (avg): 39.5912 WER(1gram): 79.95% (n=64) time: 6.864
2026-01-03 21:16:53,352: WER lens: avg_true_words=6.16 avg_pred_words=5.06 max_pred_words=11
2026-01-03 21:16:53,355: t15.2023.08.13 val PER: 0.3628
2026-01-03 21:16:53,356: t15.2023.08.18 val PER: 0.3294
2026-01-03 21:16:53,358: t15.2023.08.20 val PER: 0.3288
2026-01-03 21:16:53,360: t15.2023.08.25 val PER: 0.2816
2026-01-03 21:16:53,361: t15.2023.08.27 val PER: 0.4212
2026-01-03 21:16:53,364: t15.2023.09.01 val PER: 0.2865
2026-01-03 21:16:53,365: t15.2023.09.03 val PER: 0.4038
2026-01-03 21:16:53,367: t15.2023.09.24 val PER: 0.3337
2026-01-03 21:16:53,369: t15.2023.09.29 val PER: 0.3618
2026-01-03 21:16:53,371: t15.2023.10.01 val PER: 0.4240
2026-01-03 21:16:53,372: t15.2023.10.06 val PER: 0.3025
2026-01-03 21:16:53,374: t15.2023.10.08 val PER: 0.4506
2026-01-03 21:16:53,375: t15.2023.10.13 val PER: 0.4562
2026-01-03 21:16:53,377: t15.2023.10.15 val PER: 0.3790
2026-01-03 21:16:53,379: t15.2023.10.20 val PER: 0.3557
2026-01-03 21:16:53,381: t15.2023.10.22 val PER: 0.3408
2026-01-03 21:16:53,383: t15.2023.11.03 val PER: 0.3860
2026-01-03 21:16:53,384: t15.2023.11.04 val PER: 0.1263
2026-01-03 21:16:53,386: t15.2023.11.17 val PER: 0.2473
2026-01-03 21:16:53,387: t15.2023.11.19 val PER: 0.2076
2026-01-03 21:16:53,389: t15.2023.11.26 val PER: 0.4406
2026-01-03 21:16:53,391: t15.2023.12.03 val PER: 0.3971
2026-01-03 21:16:53,392: t15.2023.12.08 val PER: 0.3802
2026-01-03 21:16:53,394: t15.2023.12.10 val PER: 0.3233
2026-01-03 21:16:53,396: t15.2023.12.17 val PER: 0.3898
2026-01-03 21:16:53,397: t15.2023.12.29 val PER: 0.3960
2026-01-03 21:16:53,398: t15.2024.02.25 val PER: 0.3315
2026-01-03 21:16:53,400: t15.2024.03.08 val PER: 0.4822
2026-01-03 21:16:53,402: t15.2024.03.15 val PER: 0.4334
2026-01-03 21:16:53,403: t15.2024.03.17 val PER: 0.3933
2026-01-03 21:16:53,404: t15.2024.05.10 val PER: 0.4160
2026-01-03 21:16:53,406: t15.2024.06.14 val PER: 0.4117
2026-01-03 21:16:53,408: t15.2024.07.19 val PER: 0.5511
2026-01-03 21:16:53,409: t15.2024.07.21 val PER: 0.3690
2026-01-03 21:16:53,411: t15.2024.07.28 val PER: 0.3882
2026-01-03 21:16:53,412: t15.2025.01.10 val PER: 0.6598
2026-01-03 21:16:53,414: t15.2025.01.12 val PER: 0.4503
2026-01-03 21:16:53,416: t15.2025.03.14 val PER: 0.6257
2026-01-03 21:16:53,417: t15.2025.03.16 val PER: 0.4961
2026-01-03 21:16:53,419: t15.2025.03.30 val PER: 0.6747
2026-01-03 21:16:53,421: t15.2025.04.13 val PER: 0.4893
2026-01-03 21:16:53,423: New best val WER(1gram) 86.04% --> 79.95%
2026-01-03 21:16:53,745: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_1500
2026-01-03 21:17:02,426: Train batch 1600: loss: 39.35 grad norm: 80.45 time: 0.064
2026-01-03 21:17:20,004: Train batch 1800: loss: 38.17 grad norm: 74.42 time: 0.088
2026-01-03 21:17:37,537: Train batch 2000: loss: 36.79 grad norm: 71.18 time: 0.065
2026-01-03 21:17:37,540: Running test after training batch: 2000
2026-01-03 21:17:37,665: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:17:42,554: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt ease thus code at this boyde is wheel
2026-01-03 21:17:42,588: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus las it
2026-01-03 21:17:44,290: Val batch 2000: PER (avg): 0.3529 CTC Loss (avg): 35.6756 WER(1gram): 76.40% (n=64) time: 6.748
2026-01-03 21:17:44,293: WER lens: avg_true_words=6.16 avg_pred_words=5.56 max_pred_words=12
2026-01-03 21:17:44,296: t15.2023.08.13 val PER: 0.3212
2026-01-03 21:17:44,297: t15.2023.08.18 val PER: 0.2775
2026-01-03 21:17:44,299: t15.2023.08.20 val PER: 0.2764
2026-01-03 21:17:44,300: t15.2023.08.25 val PER: 0.2560
2026-01-03 21:17:44,302: t15.2023.08.27 val PER: 0.3601
2026-01-03 21:17:44,304: t15.2023.09.01 val PER: 0.2427
2026-01-03 21:17:44,305: t15.2023.09.03 val PER: 0.3563
2026-01-03 21:17:44,307: t15.2023.09.24 val PER: 0.2791
2026-01-03 21:17:44,308: t15.2023.09.29 val PER: 0.2936
2026-01-03 21:17:44,310: t15.2023.10.01 val PER: 0.3487
2026-01-03 21:17:44,312: t15.2023.10.06 val PER: 0.2519
2026-01-03 21:17:44,313: t15.2023.10.08 val PER: 0.4195
2026-01-03 21:17:44,315: t15.2023.10.13 val PER: 0.4057
2026-01-03 21:17:44,317: t15.2023.10.15 val PER: 0.3197
2026-01-03 21:17:44,319: t15.2023.10.20 val PER: 0.3154
2026-01-03 21:17:44,321: t15.2023.10.22 val PER: 0.2728
2026-01-03 21:17:44,322: t15.2023.11.03 val PER: 0.3406
2026-01-03 21:17:44,324: t15.2023.11.04 val PER: 0.1263
2026-01-03 21:17:44,325: t15.2023.11.17 val PER: 0.2006
2026-01-03 21:17:44,327: t15.2023.11.19 val PER: 0.1737
2026-01-03 21:17:44,328: t15.2023.11.26 val PER: 0.3797
2026-01-03 21:17:44,330: t15.2023.12.03 val PER: 0.3330
2026-01-03 21:17:44,332: t15.2023.12.08 val PER: 0.3382
2026-01-03 21:17:44,333: t15.2023.12.10 val PER: 0.2852
2026-01-03 21:17:44,335: t15.2023.12.17 val PER: 0.3326
2026-01-03 21:17:44,336: t15.2023.12.29 val PER: 0.3480
2026-01-03 21:17:44,338: t15.2024.02.25 val PER: 0.3034
2026-01-03 21:17:44,339: t15.2024.03.08 val PER: 0.4282
2026-01-03 21:17:44,341: t15.2024.03.15 val PER: 0.3846
2026-01-03 21:17:44,342: t15.2024.03.17 val PER: 0.3640
2026-01-03 21:17:44,344: t15.2024.05.10 val PER: 0.3863
2026-01-03 21:17:44,345: t15.2024.06.14 val PER: 0.3454
2026-01-03 21:17:44,347: t15.2024.07.19 val PER: 0.4964
2026-01-03 21:17:44,348: t15.2024.07.21 val PER: 0.3283
2026-01-03 21:17:44,350: t15.2024.07.28 val PER: 0.3522
2026-01-03 21:17:44,351: t15.2025.01.10 val PER: 0.5868
2026-01-03 21:17:44,353: t15.2025.01.12 val PER: 0.4134
2026-01-03 21:17:44,354: t15.2025.03.14 val PER: 0.5710
2026-01-03 21:17:44,356: t15.2025.03.16 val PER: 0.4424
2026-01-03 21:17:44,357: t15.2025.03.30 val PER: 0.6023
2026-01-03 21:17:44,359: t15.2025.04.13 val PER: 0.4522
2026-01-03 21:17:44,361: New best val WER(1gram) 79.95% --> 76.40%
2026-01-03 21:17:44,690: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_2000
2026-01-03 21:18:01,912: Train batch 2200: loss: 32.47 grad norm: 82.03 time: 0.060
2026-01-03 21:18:19,696: Train batch 2400: loss: 31.91 grad norm: 71.86 time: 0.052
2026-01-03 21:18:28,420: Running test after training batch: 2500
2026-01-03 21:18:28,564: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:18:33,421: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt e the good at this boyde is will
2026-01-03 21:18:33,455: WER debug example
  GT : how does it keep the cost down
  PR : houde does it yip the cost it
2026-01-03 21:18:35,137: Val batch 2500: PER (avg): 0.3309 CTC Loss (avg): 32.8127 WER(1gram): 70.30% (n=64) time: 6.715
2026-01-03 21:18:35,139: WER lens: avg_true_words=6.16 avg_pred_words=5.41 max_pred_words=11
2026-01-03 21:18:35,142: t15.2023.08.13 val PER: 0.3119
2026-01-03 21:18:35,144: t15.2023.08.18 val PER: 0.2582
2026-01-03 21:18:35,145: t15.2023.08.20 val PER: 0.2597
2026-01-03 21:18:35,147: t15.2023.08.25 val PER: 0.2304
2026-01-03 21:18:35,148: t15.2023.08.27 val PER: 0.3537
2026-01-03 21:18:35,150: t15.2023.09.01 val PER: 0.2338
2026-01-03 21:18:35,151: t15.2023.09.03 val PER: 0.3278
2026-01-03 21:18:35,152: t15.2023.09.24 val PER: 0.2561
2026-01-03 21:18:35,154: t15.2023.09.29 val PER: 0.2776
2026-01-03 21:18:35,155: t15.2023.10.01 val PER: 0.3309
2026-01-03 21:18:35,157: t15.2023.10.06 val PER: 0.2357
2026-01-03 21:18:35,158: t15.2023.10.08 val PER: 0.4019
2026-01-03 21:18:35,160: t15.2023.10.13 val PER: 0.3817
2026-01-03 21:18:35,161: t15.2023.10.15 val PER: 0.3059
2026-01-03 21:18:35,162: t15.2023.10.20 val PER: 0.2987
2026-01-03 21:18:35,164: t15.2023.10.22 val PER: 0.2572
2026-01-03 21:18:35,165: t15.2023.11.03 val PER: 0.3155
2026-01-03 21:18:35,167: t15.2023.11.04 val PER: 0.1058
2026-01-03 21:18:35,168: t15.2023.11.17 val PER: 0.1788
2026-01-03 21:18:35,170: t15.2023.11.19 val PER: 0.1357
2026-01-03 21:18:35,171: t15.2023.11.26 val PER: 0.3754
2026-01-03 21:18:35,173: t15.2023.12.03 val PER: 0.3162
2026-01-03 21:18:35,174: t15.2023.12.08 val PER: 0.3069
2026-01-03 21:18:35,176: t15.2023.12.10 val PER: 0.2628
2026-01-03 21:18:35,177: t15.2023.12.17 val PER: 0.3160
2026-01-03 21:18:35,178: t15.2023.12.29 val PER: 0.3377
2026-01-03 21:18:35,180: t15.2024.02.25 val PER: 0.2640
2026-01-03 21:18:35,181: t15.2024.03.08 val PER: 0.3898
2026-01-03 21:18:35,183: t15.2024.03.15 val PER: 0.3615
2026-01-03 21:18:35,184: t15.2024.03.17 val PER: 0.3278
2026-01-03 21:18:35,186: t15.2024.05.10 val PER: 0.3373
2026-01-03 21:18:35,187: t15.2024.06.14 val PER: 0.3312
2026-01-03 21:18:35,188: t15.2024.07.19 val PER: 0.4766
2026-01-03 21:18:35,190: t15.2024.07.21 val PER: 0.2959
2026-01-03 21:18:35,191: t15.2024.07.28 val PER: 0.3301
2026-01-03 21:18:35,193: t15.2025.01.10 val PER: 0.5482
2026-01-03 21:18:35,194: t15.2025.01.12 val PER: 0.3980
2026-01-03 21:18:35,195: t15.2025.03.14 val PER: 0.5488
2026-01-03 21:18:35,197: t15.2025.03.16 val PER: 0.3927
2026-01-03 21:18:35,198: t15.2025.03.30 val PER: 0.5747
2026-01-03 21:18:35,200: t15.2025.04.13 val PER: 0.4208
2026-01-03 21:18:35,201: New best val WER(1gram) 76.40% --> 70.30%
2026-01-03 21:18:35,536: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_2500
2026-01-03 21:18:44,428: Train batch 2600: loss: 38.94 grad norm: 88.77 time: 0.055
2026-01-03 21:19:02,154: Train batch 2800: loss: 29.04 grad norm: 75.03 time: 0.081
2026-01-03 21:19:19,872: Train batch 3000: loss: 34.26 grad norm: 81.55 time: 0.083
2026-01-03 21:19:19,874: Running test after training batch: 3000
2026-01-03 21:19:19,981: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:19:25,234: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned eke the code at this point is will
2026-01-03 21:19:25,266: WER debug example
  GT : how does it keep the cost down
  PR : houde does it hipp the cost
2026-01-03 21:19:26,966: Val batch 3000: PER (avg): 0.3067 CTC Loss (avg): 30.5799 WER(1gram): 65.99% (n=64) time: 7.089
2026-01-03 21:19:26,968: WER lens: avg_true_words=6.16 avg_pred_words=5.61 max_pred_words=11
2026-01-03 21:19:26,971: t15.2023.08.13 val PER: 0.2921
2026-01-03 21:19:26,973: t15.2023.08.18 val PER: 0.2506
2026-01-03 21:19:26,976: t15.2023.08.20 val PER: 0.2446
2026-01-03 21:19:26,977: t15.2023.08.25 val PER: 0.2214
2026-01-03 21:19:26,979: t15.2023.08.27 val PER: 0.3248
2026-01-03 21:19:26,981: t15.2023.09.01 val PER: 0.2159
2026-01-03 21:19:26,982: t15.2023.09.03 val PER: 0.3124
2026-01-03 21:19:26,984: t15.2023.09.24 val PER: 0.2294
2026-01-03 21:19:26,985: t15.2023.09.29 val PER: 0.2642
2026-01-03 21:19:26,986: t15.2023.10.01 val PER: 0.3085
2026-01-03 21:19:26,988: t15.2023.10.06 val PER: 0.2217
2026-01-03 21:19:26,989: t15.2023.10.08 val PER: 0.3694
2026-01-03 21:19:26,991: t15.2023.10.13 val PER: 0.3615
2026-01-03 21:19:26,993: t15.2023.10.15 val PER: 0.2933
2026-01-03 21:19:26,994: t15.2023.10.20 val PER: 0.2819
2026-01-03 21:19:26,996: t15.2023.10.22 val PER: 0.2283
2026-01-03 21:19:26,997: t15.2023.11.03 val PER: 0.2999
2026-01-03 21:19:26,999: t15.2023.11.04 val PER: 0.0922
2026-01-03 21:19:27,000: t15.2023.11.17 val PER: 0.1446
2026-01-03 21:19:27,002: t15.2023.11.19 val PER: 0.1218
2026-01-03 21:19:27,003: t15.2023.11.26 val PER: 0.3246
2026-01-03 21:19:27,005: t15.2023.12.03 val PER: 0.2784
2026-01-03 21:19:27,006: t15.2023.12.08 val PER: 0.2816
2026-01-03 21:19:27,008: t15.2023.12.10 val PER: 0.2339
2026-01-03 21:19:27,009: t15.2023.12.17 val PER: 0.2983
2026-01-03 21:19:27,011: t15.2023.12.29 val PER: 0.3075
2026-01-03 21:19:27,012: t15.2024.02.25 val PER: 0.2697
2026-01-03 21:19:27,014: t15.2024.03.08 val PER: 0.3798
2026-01-03 21:19:27,016: t15.2024.03.15 val PER: 0.3640
2026-01-03 21:19:27,017: t15.2024.03.17 val PER: 0.3159
2026-01-03 21:19:27,019: t15.2024.05.10 val PER: 0.3210
2026-01-03 21:19:27,020: t15.2024.06.14 val PER: 0.3076
2026-01-03 21:19:27,022: t15.2024.07.19 val PER: 0.4252
2026-01-03 21:19:27,023: t15.2024.07.21 val PER: 0.2614
2026-01-03 21:19:27,025: t15.2024.07.28 val PER: 0.2963
2026-01-03 21:19:27,026: t15.2025.01.10 val PER: 0.5069
2026-01-03 21:19:27,028: t15.2025.01.12 val PER: 0.3610
2026-01-03 21:19:27,029: t15.2025.03.14 val PER: 0.4941
2026-01-03 21:19:27,031: t15.2025.03.16 val PER: 0.3560
2026-01-03 21:19:27,032: t15.2025.03.30 val PER: 0.5333
2026-01-03 21:19:27,034: t15.2025.04.13 val PER: 0.3823
2026-01-03 21:19:27,035: New best val WER(1gram) 70.30% --> 65.99%
2026-01-03 21:19:27,369: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_3000
2026-01-03 21:19:44,665: Train batch 3200: loss: 30.32 grad norm: 69.95 time: 0.075
2026-01-03 21:20:01,961: Train batch 3400: loss: 21.63 grad norm: 60.20 time: 0.048
2026-01-03 21:20:10,762: Running test after training batch: 3500
2026-01-03 21:20:10,888: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:20:15,759: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is wheel
2026-01-03 21:20:15,790: WER debug example
  GT : how does it keep the cost down
  PR : houde des it hipp thus cus get
2026-01-03 21:20:17,520: Val batch 3500: PER (avg): 0.2939 CTC Loss (avg): 29.5231 WER(1gram): 69.04% (n=64) time: 6.755
2026-01-03 21:20:17,523: WER lens: avg_true_words=6.16 avg_pred_words=5.94 max_pred_words=11
2026-01-03 21:20:17,526: t15.2023.08.13 val PER: 0.2786
2026-01-03 21:20:17,528: t15.2023.08.18 val PER: 0.2297
2026-01-03 21:20:17,530: t15.2023.08.20 val PER: 0.2295
2026-01-03 21:20:17,532: t15.2023.08.25 val PER: 0.2063
2026-01-03 21:20:17,533: t15.2023.08.27 val PER: 0.3103
2026-01-03 21:20:17,535: t15.2023.09.01 val PER: 0.2054
2026-01-03 21:20:17,536: t15.2023.09.03 val PER: 0.2910
2026-01-03 21:20:17,538: t15.2023.09.24 val PER: 0.2269
2026-01-03 21:20:17,539: t15.2023.09.29 val PER: 0.2406
2026-01-03 21:20:17,541: t15.2023.10.01 val PER: 0.3012
2026-01-03 21:20:17,542: t15.2023.10.06 val PER: 0.2153
2026-01-03 21:20:17,544: t15.2023.10.08 val PER: 0.3681
2026-01-03 21:20:17,546: t15.2023.10.13 val PER: 0.3545
2026-01-03 21:20:17,547: t15.2023.10.15 val PER: 0.2775
2026-01-03 21:20:17,549: t15.2023.10.20 val PER: 0.2651
2026-01-03 21:20:17,550: t15.2023.10.22 val PER: 0.2294
2026-01-03 21:20:17,552: t15.2023.11.03 val PER: 0.2768
2026-01-03 21:20:17,555: t15.2023.11.04 val PER: 0.0956
2026-01-03 21:20:17,556: t15.2023.11.17 val PER: 0.1431
2026-01-03 21:20:17,558: t15.2023.11.19 val PER: 0.1257
2026-01-03 21:20:17,559: t15.2023.11.26 val PER: 0.3087
2026-01-03 21:20:17,561: t15.2023.12.03 val PER: 0.2605
2026-01-03 21:20:17,564: t15.2023.12.08 val PER: 0.2823
2026-01-03 21:20:17,566: t15.2023.12.10 val PER: 0.2194
2026-01-03 21:20:17,567: t15.2023.12.17 val PER: 0.2827
2026-01-03 21:20:17,568: t15.2023.12.29 val PER: 0.2876
2026-01-03 21:20:17,570: t15.2024.02.25 val PER: 0.2374
2026-01-03 21:20:17,571: t15.2024.03.08 val PER: 0.3613
2026-01-03 21:20:17,573: t15.2024.03.15 val PER: 0.3408
2026-01-03 21:20:17,574: t15.2024.03.17 val PER: 0.3061
2026-01-03 21:20:17,576: t15.2024.05.10 val PER: 0.3016
2026-01-03 21:20:17,577: t15.2024.06.14 val PER: 0.3044
2026-01-03 21:20:17,578: t15.2024.07.19 val PER: 0.4232
2026-01-03 21:20:17,580: t15.2024.07.21 val PER: 0.2552
2026-01-03 21:20:17,581: t15.2024.07.28 val PER: 0.2949
2026-01-03 21:20:17,583: t15.2025.01.10 val PER: 0.4890
2026-01-03 21:20:17,584: t15.2025.01.12 val PER: 0.3349
2026-01-03 21:20:17,586: t15.2025.03.14 val PER: 0.4837
2026-01-03 21:20:17,587: t15.2025.03.16 val PER: 0.3613
2026-01-03 21:20:17,589: t15.2025.03.30 val PER: 0.5046
2026-01-03 21:20:17,590: t15.2025.04.13 val PER: 0.3638
2026-01-03 21:20:17,820: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_3500
2026-01-03 21:20:26,690: Train batch 3600: loss: 24.88 grad norm: 64.45 time: 0.065
2026-01-03 21:20:43,700: Train batch 3800: loss: 28.71 grad norm: 69.65 time: 0.065
2026-01-03 21:21:00,853: Train batch 4000: loss: 22.48 grad norm: 59.48 time: 0.055
2026-01-03 21:21:00,855: Running test after training batch: 4000
2026-01-03 21:21:01,005: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:21:05,925: WER debug example
  GT : you can see the code at this point as well
  PR : yule kent sci the code at this point is will
2026-01-03 21:21:05,957: WER debug example
  GT : how does it keep the cost down
  PR : hout des it heap the us nit
2026-01-03 21:21:07,677: Val batch 4000: PER (avg): 0.2790 CTC Loss (avg): 27.5525 WER(1gram): 68.02% (n=64) time: 6.819
2026-01-03 21:21:07,679: WER lens: avg_true_words=6.16 avg_pred_words=5.84 max_pred_words=11
2026-01-03 21:21:07,683: t15.2023.08.13 val PER: 0.2661
2026-01-03 21:21:07,685: t15.2023.08.18 val PER: 0.2372
2026-01-03 21:21:07,687: t15.2023.08.20 val PER: 0.2248
2026-01-03 21:21:07,689: t15.2023.08.25 val PER: 0.1807
2026-01-03 21:21:07,691: t15.2023.08.27 val PER: 0.3006
2026-01-03 21:21:07,693: t15.2023.09.01 val PER: 0.1964
2026-01-03 21:21:07,695: t15.2023.09.03 val PER: 0.2743
2026-01-03 21:21:07,697: t15.2023.09.24 val PER: 0.2087
2026-01-03 21:21:07,699: t15.2023.09.29 val PER: 0.2291
2026-01-03 21:21:07,701: t15.2023.10.01 val PER: 0.2880
2026-01-03 21:21:07,702: t15.2023.10.06 val PER: 0.1927
2026-01-03 21:21:07,704: t15.2023.10.08 val PER: 0.3586
2026-01-03 21:21:07,706: t15.2023.10.13 val PER: 0.3359
2026-01-03 21:21:07,708: t15.2023.10.15 val PER: 0.2624
2026-01-03 21:21:07,709: t15.2023.10.20 val PER: 0.2752
2026-01-03 21:21:07,711: t15.2023.10.22 val PER: 0.2327
2026-01-03 21:21:07,713: t15.2023.11.03 val PER: 0.2551
2026-01-03 21:21:07,714: t15.2023.11.04 val PER: 0.0751
2026-01-03 21:21:07,716: t15.2023.11.17 val PER: 0.1166
2026-01-03 21:21:07,718: t15.2023.11.19 val PER: 0.1118
2026-01-03 21:21:07,719: t15.2023.11.26 val PER: 0.3007
2026-01-03 21:21:07,721: t15.2023.12.03 val PER: 0.2426
2026-01-03 21:21:07,723: t15.2023.12.08 val PER: 0.2603
2026-01-03 21:21:07,725: t15.2023.12.10 val PER: 0.2024
2026-01-03 21:21:07,726: t15.2023.12.17 val PER: 0.2796
2026-01-03 21:21:07,728: t15.2023.12.29 val PER: 0.2773
2026-01-03 21:21:07,730: t15.2024.02.25 val PER: 0.2346
2026-01-03 21:21:07,732: t15.2024.03.08 val PER: 0.3570
2026-01-03 21:21:07,733: t15.2024.03.15 val PER: 0.3283
2026-01-03 21:21:07,735: t15.2024.03.17 val PER: 0.2880
2026-01-03 21:21:07,737: t15.2024.05.10 val PER: 0.3120
2026-01-03 21:21:07,739: t15.2024.06.14 val PER: 0.2871
2026-01-03 21:21:07,741: t15.2024.07.19 val PER: 0.4047
2026-01-03 21:21:07,742: t15.2024.07.21 val PER: 0.2234
2026-01-03 21:21:07,744: t15.2024.07.28 val PER: 0.2728
2026-01-03 21:21:07,746: t15.2025.01.10 val PER: 0.4490
2026-01-03 21:21:07,747: t15.2025.01.12 val PER: 0.3156
2026-01-03 21:21:07,749: t15.2025.03.14 val PER: 0.4482
2026-01-03 21:21:07,751: t15.2025.03.16 val PER: 0.3442
2026-01-03 21:21:07,753: t15.2025.03.30 val PER: 0.4839
2026-01-03 21:21:07,755: t15.2025.04.13 val PER: 0.3409
2026-01-03 21:21:07,982: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_4000
2026-01-03 21:21:25,221: Train batch 4200: loss: 26.09 grad norm: 70.99 time: 0.078
2026-01-03 21:21:42,459: Train batch 4400: loss: 19.89 grad norm: 59.05 time: 0.066
2026-01-03 21:21:51,098: Running test after training batch: 4500
2026-01-03 21:21:51,213: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:21:56,281: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-03 21:21:56,313: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it yip the cost get
2026-01-03 21:21:57,997: Val batch 4500: PER (avg): 0.2650 CTC Loss (avg): 26.1884 WER(1gram): 64.97% (n=64) time: 6.897
2026-01-03 21:21:58,000: WER lens: avg_true_words=6.16 avg_pred_words=5.88 max_pred_words=11
2026-01-03 21:21:58,002: t15.2023.08.13 val PER: 0.2578
2026-01-03 21:21:58,004: t15.2023.08.18 val PER: 0.2112
2026-01-03 21:21:58,006: t15.2023.08.20 val PER: 0.2081
2026-01-03 21:21:58,007: t15.2023.08.25 val PER: 0.1747
2026-01-03 21:21:58,009: t15.2023.08.27 val PER: 0.2814
2026-01-03 21:21:58,011: t15.2023.09.01 val PER: 0.1859
2026-01-03 21:21:58,013: t15.2023.09.03 val PER: 0.2684
2026-01-03 21:21:58,015: t15.2023.09.24 val PER: 0.1978
2026-01-03 21:21:58,016: t15.2023.09.29 val PER: 0.2221
2026-01-03 21:21:58,018: t15.2023.10.01 val PER: 0.2801
2026-01-03 21:21:58,020: t15.2023.10.06 val PER: 0.1744
2026-01-03 21:21:58,022: t15.2023.10.08 val PER: 0.3315
2026-01-03 21:21:58,023: t15.2023.10.13 val PER: 0.3181
2026-01-03 21:21:58,025: t15.2023.10.15 val PER: 0.2472
2026-01-03 21:21:58,027: t15.2023.10.20 val PER: 0.2483
2026-01-03 21:21:58,029: t15.2023.10.22 val PER: 0.2105
2026-01-03 21:21:58,031: t15.2023.11.03 val PER: 0.2598
2026-01-03 21:21:58,033: t15.2023.11.04 val PER: 0.0717
2026-01-03 21:21:58,034: t15.2023.11.17 val PER: 0.1182
2026-01-03 21:21:58,037: t15.2023.11.19 val PER: 0.1178
2026-01-03 21:21:58,039: t15.2023.11.26 val PER: 0.2884
2026-01-03 21:21:58,040: t15.2023.12.03 val PER: 0.2279
2026-01-03 21:21:58,042: t15.2023.12.08 val PER: 0.2417
2026-01-03 21:21:58,044: t15.2023.12.10 val PER: 0.1945
2026-01-03 21:21:58,046: t15.2023.12.17 val PER: 0.2630
2026-01-03 21:21:58,048: t15.2023.12.29 val PER: 0.2608
2026-01-03 21:21:58,050: t15.2024.02.25 val PER: 0.2275
2026-01-03 21:21:58,051: t15.2024.03.08 val PER: 0.3514
2026-01-03 21:21:58,054: t15.2024.03.15 val PER: 0.3127
2026-01-03 21:21:58,056: t15.2024.03.17 val PER: 0.2713
2026-01-03 21:21:58,058: t15.2024.05.10 val PER: 0.2749
2026-01-03 21:21:58,059: t15.2024.06.14 val PER: 0.2823
2026-01-03 21:21:58,061: t15.2024.07.19 val PER: 0.3935
2026-01-03 21:21:58,062: t15.2024.07.21 val PER: 0.2090
2026-01-03 21:21:58,064: t15.2024.07.28 val PER: 0.2610
2026-01-03 21:21:58,065: t15.2025.01.10 val PER: 0.4353
2026-01-03 21:21:58,067: t15.2025.01.12 val PER: 0.3002
2026-01-03 21:21:58,069: t15.2025.03.14 val PER: 0.4334
2026-01-03 21:21:58,070: t15.2025.03.16 val PER: 0.3050
2026-01-03 21:21:58,072: t15.2025.03.30 val PER: 0.4506
2026-01-03 21:21:58,074: t15.2025.04.13 val PER: 0.3281
2026-01-03 21:21:58,076: New best val WER(1gram) 65.99% --> 64.97%
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mbase[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../tmp/e12511253_b2t_348237/wandb/wandb/run-20260103_211337-at7jbkgg/logs[0m
