TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_351047
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_351047/torch_extensions
WANDB_DIR=/tmp/e12511253_b2t_351047/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/e12511253_b2t_351047/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  8 12:34 /tmp/e12511253_b2t_351047/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
OUT_ROOT=/home/e12511253/Brain2Text/brain2text/trained_models
==============================================
Job: b2t5g  ID: 351047
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_5gram_only.yaml
Folders: configs/experiments/gru/input_dropout/lr40_wd1e-5
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/gru/input_dropout/lr40_wd1e-5 ==========
Num configs: 5

=== RUN base_input_dropout_wd1e-5.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5
2026-01-08 12:34:23,196: Using device: cuda:0
2026-01-08 12:38:14,710: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-08 12:38:14,736: Using 45 sessions after filtering (from 45).
2026-01-08 12:38:15,183: Using torch.compile (if available)
2026-01-08 12:38:15,183: torch.compile not available (torch<2.0). Skipping.
2026-01-08 12:38:15,183: Initialized RNN decoding model
2026-01-08 12:38:15,183: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-08 12:38:15,184: Model has 44,907,305 parameters
2026-01-08 12:38:15,184: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-08 12:38:16,433: Successfully initialized datasets
2026-01-08 12:38:16,433: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-08 12:38:17,922: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.195
2026-01-08 12:38:17,923: Running test after training batch: 0
2026-01-08 12:38:18,029: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:38:23,881: WER debug example
  GT : you can see the code at this point as well
  PR : she has from his
2026-01-08 12:38:24,893: WER debug example
  GT : how does it keep the cost down
  PR : money from
2026-01-08 12:42:15,131: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(5gram): 99.67% (n=256) time: 237.209
2026-01-08 12:42:15,136: WER lens: avg_true_words=5.99 avg_pred_words=2.82 max_pred_words=7
2026-01-08 12:42:15,140: t15.2023.08.13 val PER: 1.3056
2026-01-08 12:42:15,146: t15.2023.08.18 val PER: 1.4208
2026-01-08 12:42:15,146: t15.2023.08.20 val PER: 1.3002
2026-01-08 12:42:15,146: t15.2023.08.25 val PER: 1.3389
2026-01-08 12:42:15,147: t15.2023.08.27 val PER: 1.2460
2026-01-08 12:42:15,147: t15.2023.09.01 val PER: 1.4537
2026-01-08 12:42:15,147: t15.2023.09.03 val PER: 1.3171
2026-01-08 12:42:15,147: t15.2023.09.24 val PER: 1.5461
2026-01-08 12:42:15,147: t15.2023.09.29 val PER: 1.4671
2026-01-08 12:42:15,147: t15.2023.10.01 val PER: 1.2147
2026-01-08 12:42:15,147: t15.2023.10.06 val PER: 1.4876
2026-01-08 12:42:15,147: t15.2023.10.08 val PER: 1.1827
2026-01-08 12:42:15,147: t15.2023.10.13 val PER: 1.3964
2026-01-08 12:42:15,147: t15.2023.10.15 val PER: 1.3889
2026-01-08 12:42:15,148: t15.2023.10.20 val PER: 1.4866
2026-01-08 12:42:15,148: t15.2023.10.22 val PER: 1.3942
2026-01-08 12:42:15,148: t15.2023.11.03 val PER: 1.5923
2026-01-08 12:42:15,148: t15.2023.11.04 val PER: 2.0171
2026-01-08 12:42:15,148: t15.2023.11.17 val PER: 1.9518
2026-01-08 12:42:15,148: t15.2023.11.19 val PER: 1.6707
2026-01-08 12:42:15,148: t15.2023.11.26 val PER: 1.5413
2026-01-08 12:42:15,148: t15.2023.12.03 val PER: 1.4254
2026-01-08 12:42:15,148: t15.2023.12.08 val PER: 1.4487
2026-01-08 12:42:15,148: t15.2023.12.10 val PER: 1.6899
2026-01-08 12:42:15,148: t15.2023.12.17 val PER: 1.3077
2026-01-08 12:42:15,149: t15.2023.12.29 val PER: 1.4063
2026-01-08 12:42:15,149: t15.2024.02.25 val PER: 1.4228
2026-01-08 12:42:15,149: t15.2024.03.08 val PER: 1.3257
2026-01-08 12:42:15,149: t15.2024.03.15 val PER: 1.3196
2026-01-08 12:42:15,149: t15.2024.03.17 val PER: 1.4052
2026-01-08 12:42:15,149: t15.2024.05.10 val PER: 1.3224
2026-01-08 12:42:15,149: t15.2024.06.14 val PER: 1.5315
2026-01-08 12:42:15,149: t15.2024.07.19 val PER: 1.0817
2026-01-08 12:42:15,150: t15.2024.07.21 val PER: 1.6290
2026-01-08 12:42:15,150: t15.2024.07.28 val PER: 1.6588
2026-01-08 12:42:15,150: t15.2025.01.10 val PER: 1.0923
2026-01-08 12:42:15,150: t15.2025.01.12 val PER: 1.7629
2026-01-08 12:42:15,150: t15.2025.03.14 val PER: 1.0414
2026-01-08 12:42:15,150: t15.2025.03.16 val PER: 1.6257
2026-01-08 12:42:15,150: t15.2025.03.30 val PER: 1.2874
2026-01-08 12:42:15,150: t15.2025.04.13 val PER: 1.5949
2026-01-08 12:42:15,151: New best val WER(5gram) inf% --> 99.67%
2026-01-08 12:42:15,151: Checkpointing model
2026-01-08 12:42:15,307: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 12:42:33,653: Train batch 200: loss: 77.58 grad norm: 106.18 time: 0.055
2026-01-08 12:42:51,946: Train batch 400: loss: 54.04 grad norm: 90.50 time: 0.064
2026-01-08 12:43:00,872: Running test after training batch: 500
2026-01-08 12:43:00,999: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:43:06,207: WER debug example
  GT : you can see the code at this point as well
  PR : used and is thus had at this and is all
2026-01-08 12:43:06,360: WER debug example
  GT : how does it keep the cost down
  PR : and does it think that this is
2026-01-08 12:43:53,844: Val batch 500: PER (avg): 0.5151 CTC Loss (avg): 55.2685 WER(5gram): 74.19% (n=256) time: 52.972
2026-01-08 12:43:53,845: WER lens: avg_true_words=5.99 avg_pred_words=5.72 max_pred_words=12
2026-01-08 12:43:53,845: t15.2023.08.13 val PER: 0.4563
2026-01-08 12:43:53,845: t15.2023.08.18 val PER: 0.4451
2026-01-08 12:43:53,845: t15.2023.08.20 val PER: 0.4440
2026-01-08 12:43:53,845: t15.2023.08.25 val PER: 0.4277
2026-01-08 12:43:53,845: t15.2023.08.27 val PER: 0.5225
2026-01-08 12:43:53,845: t15.2023.09.01 val PER: 0.4205
2026-01-08 12:43:53,845: t15.2023.09.03 val PER: 0.4869
2026-01-08 12:43:53,845: t15.2023.09.24 val PER: 0.4260
2026-01-08 12:43:53,846: t15.2023.09.29 val PER: 0.4633
2026-01-08 12:43:53,846: t15.2023.10.01 val PER: 0.5192
2026-01-08 12:43:53,846: t15.2023.10.06 val PER: 0.4306
2026-01-08 12:43:53,846: t15.2023.10.08 val PER: 0.5399
2026-01-08 12:43:53,846: t15.2023.10.13 val PER: 0.5780
2026-01-08 12:43:53,846: t15.2023.10.15 val PER: 0.4964
2026-01-08 12:43:53,846: t15.2023.10.20 val PER: 0.4597
2026-01-08 12:43:53,846: t15.2023.10.22 val PER: 0.4510
2026-01-08 12:43:53,846: t15.2023.11.03 val PER: 0.5075
2026-01-08 12:43:53,846: t15.2023.11.04 val PER: 0.2594
2026-01-08 12:43:53,846: t15.2023.11.17 val PER: 0.3593
2026-01-08 12:43:53,846: t15.2023.11.19 val PER: 0.3353
2026-01-08 12:43:53,846: t15.2023.11.26 val PER: 0.5536
2026-01-08 12:43:53,846: t15.2023.12.03 val PER: 0.4926
2026-01-08 12:43:53,847: t15.2023.12.08 val PER: 0.5087
2026-01-08 12:43:53,847: t15.2023.12.10 val PER: 0.4468
2026-01-08 12:43:53,847: t15.2023.12.17 val PER: 0.5593
2026-01-08 12:43:53,847: t15.2023.12.29 val PER: 0.5374
2026-01-08 12:43:53,847: t15.2024.02.25 val PER: 0.4846
2026-01-08 12:43:53,847: t15.2024.03.08 val PER: 0.6031
2026-01-08 12:43:53,847: t15.2024.03.15 val PER: 0.5503
2026-01-08 12:43:53,847: t15.2024.03.17 val PER: 0.5091
2026-01-08 12:43:53,847: t15.2024.05.10 val PER: 0.5513
2026-01-08 12:43:53,847: t15.2024.06.14 val PER: 0.5047
2026-01-08 12:43:53,847: t15.2024.07.19 val PER: 0.6658
2026-01-08 12:43:53,847: t15.2024.07.21 val PER: 0.4731
2026-01-08 12:43:53,847: t15.2024.07.28 val PER: 0.5059
2026-01-08 12:43:53,848: t15.2025.01.10 val PER: 0.7328
2026-01-08 12:43:53,848: t15.2025.01.12 val PER: 0.5504
2026-01-08 12:43:53,848: t15.2025.03.14 val PER: 0.7426
2026-01-08 12:43:53,848: t15.2025.03.16 val PER: 0.5785
2026-01-08 12:43:53,848: t15.2025.03.30 val PER: 0.7241
2026-01-08 12:43:53,848: t15.2025.04.13 val PER: 0.5678
2026-01-08 12:43:53,851: New best val WER(5gram) 99.67% --> 74.19%
2026-01-08 12:43:53,851: Checkpointing model
2026-01-08 12:43:53,990: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 12:44:02,821: Train batch 600: loss: 50.34 grad norm: 84.42 time: 0.078
2026-01-08 12:44:20,448: Train batch 800: loss: 40.77 grad norm: 86.73 time: 0.057
2026-01-08 12:44:38,030: Train batch 1000: loss: 42.81 grad norm: 78.00 time: 0.066
2026-01-08 12:44:38,031: Running test after training batch: 1000
2026-01-08 12:44:38,158: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:44:43,279: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this and is well
2026-01-08 12:44:43,493: WER debug example
  GT : how does it keep the cost down
  PR : howled as it is that what it
2026-01-08 12:45:13,113: Val batch 1000: PER (avg): 0.4080 CTC Loss (avg): 42.3796 WER(5gram): 51.83% (n=256) time: 35.082
2026-01-08 12:45:13,113: WER lens: avg_true_words=5.99 avg_pred_words=5.58 max_pred_words=12
2026-01-08 12:45:13,114: t15.2023.08.13 val PER: 0.3825
2026-01-08 12:45:13,114: t15.2023.08.18 val PER: 0.3353
2026-01-08 12:45:13,114: t15.2023.08.20 val PER: 0.3479
2026-01-08 12:45:13,114: t15.2023.08.25 val PER: 0.2922
2026-01-08 12:45:13,114: t15.2023.08.27 val PER: 0.4196
2026-01-08 12:45:13,114: t15.2023.09.01 val PER: 0.2995
2026-01-08 12:45:13,114: t15.2023.09.03 val PER: 0.3895
2026-01-08 12:45:13,114: t15.2023.09.24 val PER: 0.3434
2026-01-08 12:45:13,114: t15.2023.09.29 val PER: 0.3542
2026-01-08 12:45:13,114: t15.2023.10.01 val PER: 0.4089
2026-01-08 12:45:13,114: t15.2023.10.06 val PER: 0.3132
2026-01-08 12:45:13,114: t15.2023.10.08 val PER: 0.4479
2026-01-08 12:45:13,115: t15.2023.10.13 val PER: 0.4701
2026-01-08 12:45:13,115: t15.2023.10.15 val PER: 0.3751
2026-01-08 12:45:13,115: t15.2023.10.20 val PER: 0.3624
2026-01-08 12:45:13,115: t15.2023.10.22 val PER: 0.3519
2026-01-08 12:45:13,115: t15.2023.11.03 val PER: 0.4016
2026-01-08 12:45:13,115: t15.2023.11.04 val PER: 0.1570
2026-01-08 12:45:13,115: t15.2023.11.17 val PER: 0.2706
2026-01-08 12:45:13,115: t15.2023.11.19 val PER: 0.2156
2026-01-08 12:45:13,115: t15.2023.11.26 val PER: 0.4435
2026-01-08 12:45:13,115: t15.2023.12.03 val PER: 0.4013
2026-01-08 12:45:13,115: t15.2023.12.08 val PER: 0.4088
2026-01-08 12:45:13,115: t15.2023.12.10 val PER: 0.3509
2026-01-08 12:45:13,115: t15.2023.12.17 val PER: 0.4210
2026-01-08 12:45:13,116: t15.2023.12.29 val PER: 0.3974
2026-01-08 12:45:13,116: t15.2024.02.25 val PER: 0.3399
2026-01-08 12:45:13,116: t15.2024.03.08 val PER: 0.4964
2026-01-08 12:45:13,116: t15.2024.03.15 val PER: 0.4409
2026-01-08 12:45:13,116: t15.2024.03.17 val PER: 0.4066
2026-01-08 12:45:13,116: t15.2024.05.10 val PER: 0.4279
2026-01-08 12:45:13,117: t15.2024.06.14 val PER: 0.4022
2026-01-08 12:45:13,117: t15.2024.07.19 val PER: 0.5326
2026-01-08 12:45:13,117: t15.2024.07.21 val PER: 0.3683
2026-01-08 12:45:13,117: t15.2024.07.28 val PER: 0.4088
2026-01-08 12:45:13,117: t15.2025.01.10 val PER: 0.6253
2026-01-08 12:45:13,117: t15.2025.01.12 val PER: 0.4473
2026-01-08 12:45:13,117: t15.2025.03.14 val PER: 0.6331
2026-01-08 12:45:13,117: t15.2025.03.16 val PER: 0.4869
2026-01-08 12:45:13,117: t15.2025.03.30 val PER: 0.6552
2026-01-08 12:45:13,117: t15.2025.04.13 val PER: 0.4950
2026-01-08 12:45:13,118: New best val WER(5gram) 74.19% --> 51.83%
2026-01-08 12:45:13,118: Checkpointing model
2026-01-08 12:45:13,266: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 12:45:30,622: Train batch 1200: loss: 33.26 grad norm: 74.50 time: 0.069
2026-01-08 12:45:48,366: Train batch 1400: loss: 36.20 grad norm: 81.40 time: 0.062
2026-01-08 12:45:57,192: Running test after training batch: 1500
2026-01-08 12:45:57,341: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:46:02,136: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-08 12:46:02,218: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us in
2026-01-08 12:46:19,875: Val batch 1500: PER (avg): 0.3801 CTC Loss (avg): 37.1847 WER(5gram): 36.44% (n=256) time: 22.682
2026-01-08 12:46:19,876: WER lens: avg_true_words=5.99 avg_pred_words=5.29 max_pred_words=12
2026-01-08 12:46:19,876: t15.2023.08.13 val PER: 0.3462
2026-01-08 12:46:19,876: t15.2023.08.18 val PER: 0.3219
2026-01-08 12:46:19,876: t15.2023.08.20 val PER: 0.3082
2026-01-08 12:46:19,876: t15.2023.08.25 val PER: 0.2560
2026-01-08 12:46:19,876: t15.2023.08.27 val PER: 0.3987
2026-01-08 12:46:19,876: t15.2023.09.01 val PER: 0.2727
2026-01-08 12:46:19,876: t15.2023.09.03 val PER: 0.3729
2026-01-08 12:46:19,877: t15.2023.09.24 val PER: 0.3143
2026-01-08 12:46:19,877: t15.2023.09.29 val PER: 0.3395
2026-01-08 12:46:19,877: t15.2023.10.01 val PER: 0.3989
2026-01-08 12:46:19,877: t15.2023.10.06 val PER: 0.2831
2026-01-08 12:46:19,877: t15.2023.10.08 val PER: 0.4465
2026-01-08 12:46:19,877: t15.2023.10.13 val PER: 0.4476
2026-01-08 12:46:19,877: t15.2023.10.15 val PER: 0.3632
2026-01-08 12:46:19,877: t15.2023.10.20 val PER: 0.3356
2026-01-08 12:46:19,877: t15.2023.10.22 val PER: 0.3140
2026-01-08 12:46:19,877: t15.2023.11.03 val PER: 0.3514
2026-01-08 12:46:19,877: t15.2023.11.04 val PER: 0.0990
2026-01-08 12:46:19,877: t15.2023.11.17 val PER: 0.2146
2026-01-08 12:46:19,877: t15.2023.11.19 val PER: 0.1737
2026-01-08 12:46:19,877: t15.2023.11.26 val PER: 0.4217
2026-01-08 12:46:19,877: t15.2023.12.03 val PER: 0.3519
2026-01-08 12:46:19,877: t15.2023.12.08 val PER: 0.3635
2026-01-08 12:46:19,878: t15.2023.12.10 val PER: 0.2917
2026-01-08 12:46:19,878: t15.2023.12.17 val PER: 0.3815
2026-01-08 12:46:19,878: t15.2023.12.29 val PER: 0.3638
2026-01-08 12:46:19,878: t15.2024.02.25 val PER: 0.3188
2026-01-08 12:46:19,878: t15.2024.03.08 val PER: 0.4651
2026-01-08 12:46:19,878: t15.2024.03.15 val PER: 0.4115
2026-01-08 12:46:19,878: t15.2024.03.17 val PER: 0.3752
2026-01-08 12:46:19,878: t15.2024.05.10 val PER: 0.3952
2026-01-08 12:46:19,878: t15.2024.06.14 val PER: 0.3943
2026-01-08 12:46:19,878: t15.2024.07.19 val PER: 0.5234
2026-01-08 12:46:19,878: t15.2024.07.21 val PER: 0.3462
2026-01-08 12:46:19,878: t15.2024.07.28 val PER: 0.3721
2026-01-08 12:46:19,878: t15.2025.01.10 val PER: 0.6171
2026-01-08 12:46:19,879: t15.2025.01.12 val PER: 0.4349
2026-01-08 12:46:19,879: t15.2025.03.14 val PER: 0.5991
2026-01-08 12:46:19,879: t15.2025.03.16 val PER: 0.4555
2026-01-08 12:46:19,879: t15.2025.03.30 val PER: 0.6310
2026-01-08 12:46:19,879: t15.2025.04.13 val PER: 0.4650
2026-01-08 12:46:19,880: New best val WER(5gram) 51.83% --> 36.44%
2026-01-08 12:46:19,880: Checkpointing model
2026-01-08 12:46:20,022: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 12:46:28,958: Train batch 1600: loss: 36.61 grad norm: 78.28 time: 0.064
2026-01-08 12:46:46,742: Train batch 1800: loss: 35.03 grad norm: 69.81 time: 0.089
2026-01-08 12:47:04,370: Train batch 2000: loss: 33.33 grad norm: 69.01 time: 0.067
2026-01-08 12:47:04,373: Running test after training batch: 2000
2026-01-08 12:47:04,498: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:47:09,500: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 12:47:09,572: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us in
2026-01-08 12:47:26,198: Val batch 2000: PER (avg): 0.3249 CTC Loss (avg): 32.7113 WER(5gram): 29.27% (n=256) time: 21.822
2026-01-08 12:47:26,201: WER lens: avg_true_words=5.99 avg_pred_words=5.81 max_pred_words=12
2026-01-08 12:47:26,203: t15.2023.08.13 val PER: 0.3067
2026-01-08 12:47:26,205: t15.2023.08.18 val PER: 0.2456
2026-01-08 12:47:26,207: t15.2023.08.20 val PER: 0.2566
2026-01-08 12:47:26,208: t15.2023.08.25 val PER: 0.2244
2026-01-08 12:47:26,210: t15.2023.08.27 val PER: 0.3441
2026-01-08 12:47:26,211: t15.2023.09.01 val PER: 0.2224
2026-01-08 12:47:26,213: t15.2023.09.03 val PER: 0.3230
2026-01-08 12:47:26,214: t15.2023.09.24 val PER: 0.2524
2026-01-08 12:47:26,216: t15.2023.09.29 val PER: 0.2712
2026-01-08 12:47:26,217: t15.2023.10.01 val PER: 0.3289
2026-01-08 12:47:26,219: t15.2023.10.06 val PER: 0.2379
2026-01-08 12:47:26,221: t15.2023.10.08 val PER: 0.3884
2026-01-08 12:47:26,222: t15.2023.10.13 val PER: 0.3794
2026-01-08 12:47:26,224: t15.2023.10.15 val PER: 0.3032
2026-01-08 12:47:26,225: t15.2023.10.20 val PER: 0.2953
2026-01-08 12:47:26,227: t15.2023.10.22 val PER: 0.2506
2026-01-08 12:47:26,229: t15.2023.11.03 val PER: 0.3182
2026-01-08 12:47:26,231: t15.2023.11.04 val PER: 0.1024
2026-01-08 12:47:26,232: t15.2023.11.17 val PER: 0.1664
2026-01-08 12:47:26,234: t15.2023.11.19 val PER: 0.1357
2026-01-08 12:47:26,235: t15.2023.11.26 val PER: 0.3674
2026-01-08 12:47:26,237: t15.2023.12.03 val PER: 0.3078
2026-01-08 12:47:26,238: t15.2023.12.08 val PER: 0.3083
2026-01-08 12:47:26,240: t15.2023.12.10 val PER: 0.2602
2026-01-08 12:47:26,241: t15.2023.12.17 val PER: 0.3150
2026-01-08 12:47:26,242: t15.2023.12.29 val PER: 0.3260
2026-01-08 12:47:26,244: t15.2024.02.25 val PER: 0.2893
2026-01-08 12:47:26,245: t15.2024.03.08 val PER: 0.3855
2026-01-08 12:47:26,248: t15.2024.03.15 val PER: 0.3552
2026-01-08 12:47:26,250: t15.2024.03.17 val PER: 0.3340
2026-01-08 12:47:26,251: t15.2024.05.10 val PER: 0.3269
2026-01-08 12:47:26,253: t15.2024.06.14 val PER: 0.3391
2026-01-08 12:47:26,254: t15.2024.07.19 val PER: 0.4581
2026-01-08 12:47:26,256: t15.2024.07.21 val PER: 0.2959
2026-01-08 12:47:26,257: t15.2024.07.28 val PER: 0.3184
2026-01-08 12:47:26,259: t15.2025.01.10 val PER: 0.5317
2026-01-08 12:47:26,260: t15.2025.01.12 val PER: 0.3818
2026-01-08 12:47:26,262: t15.2025.03.14 val PER: 0.5399
2026-01-08 12:47:26,263: t15.2025.03.16 val PER: 0.3887
2026-01-08 12:47:26,264: t15.2025.03.30 val PER: 0.5379
2026-01-08 12:47:26,266: t15.2025.04.13 val PER: 0.4094
2026-01-08 12:47:26,267: New best val WER(5gram) 36.44% --> 29.27%
2026-01-08 12:47:26,269: Checkpointing model
2026-01-08 12:47:26,411: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 12:47:43,985: Train batch 2200: loss: 28.69 grad norm: 73.37 time: 0.061
2026-01-08 12:48:01,440: Train batch 2400: loss: 28.95 grad norm: 63.31 time: 0.052
2026-01-08 12:48:10,646: Running test after training batch: 2500
2026-01-08 12:48:10,796: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:48:15,777: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 12:48:15,844: WER debug example
  GT : how does it keep the cost down
  PR : how does it in the s it
2026-01-08 12:48:35,528: Val batch 2500: PER (avg): 0.3041 CTC Loss (avg): 30.1373 WER(5gram): 27.84% (n=256) time: 24.879
2026-01-08 12:48:35,561: WER lens: avg_true_words=5.99 avg_pred_words=5.87 max_pred_words=12
2026-01-08 12:48:35,564: t15.2023.08.13 val PER: 0.2869
2026-01-08 12:48:35,566: t15.2023.08.18 val PER: 0.2515
2026-01-08 12:48:35,568: t15.2023.08.20 val PER: 0.2335
2026-01-08 12:48:35,570: t15.2023.08.25 val PER: 0.2093
2026-01-08 12:48:35,572: t15.2023.08.27 val PER: 0.3360
2026-01-08 12:48:35,573: t15.2023.09.01 val PER: 0.2119
2026-01-08 12:48:35,575: t15.2023.09.03 val PER: 0.2898
2026-01-08 12:48:35,577: t15.2023.09.24 val PER: 0.2367
2026-01-08 12:48:35,579: t15.2023.09.29 val PER: 0.2559
2026-01-08 12:48:35,581: t15.2023.10.01 val PER: 0.3124
2026-01-08 12:48:35,583: t15.2023.10.06 val PER: 0.2110
2026-01-08 12:48:35,585: t15.2023.10.08 val PER: 0.3789
2026-01-08 12:48:35,586: t15.2023.10.13 val PER: 0.3522
2026-01-08 12:48:35,588: t15.2023.10.15 val PER: 0.2894
2026-01-08 12:48:35,590: t15.2023.10.20 val PER: 0.2886
2026-01-08 12:48:35,592: t15.2023.10.22 val PER: 0.2350
2026-01-08 12:48:35,593: t15.2023.11.03 val PER: 0.2958
2026-01-08 12:48:35,595: t15.2023.11.04 val PER: 0.0853
2026-01-08 12:48:35,597: t15.2023.11.17 val PER: 0.1369
2026-01-08 12:48:35,598: t15.2023.11.19 val PER: 0.1178
2026-01-08 12:48:35,600: t15.2023.11.26 val PER: 0.3428
2026-01-08 12:48:35,602: t15.2023.12.03 val PER: 0.2805
2026-01-08 12:48:35,603: t15.2023.12.08 val PER: 0.2816
2026-01-08 12:48:35,605: t15.2023.12.10 val PER: 0.2392
2026-01-08 12:48:35,607: t15.2023.12.17 val PER: 0.3160
2026-01-08 12:48:35,609: t15.2023.12.29 val PER: 0.3061
2026-01-08 12:48:35,610: t15.2024.02.25 val PER: 0.2444
2026-01-08 12:48:35,613: t15.2024.03.08 val PER: 0.3698
2026-01-08 12:48:35,615: t15.2024.03.15 val PER: 0.3415
2026-01-08 12:48:35,616: t15.2024.03.17 val PER: 0.3054
2026-01-08 12:48:35,618: t15.2024.05.10 val PER: 0.3150
2026-01-08 12:48:35,620: t15.2024.06.14 val PER: 0.3155
2026-01-08 12:48:35,621: t15.2024.07.19 val PER: 0.4364
2026-01-08 12:48:35,623: t15.2024.07.21 val PER: 0.2586
2026-01-08 12:48:35,625: t15.2024.07.28 val PER: 0.3007
2026-01-08 12:48:35,626: t15.2025.01.10 val PER: 0.5000
2026-01-08 12:48:35,628: t15.2025.01.12 val PER: 0.3557
2026-01-08 12:48:35,630: t15.2025.03.14 val PER: 0.5015
2026-01-08 12:48:35,631: t15.2025.03.16 val PER: 0.3508
2026-01-08 12:48:35,632: t15.2025.03.30 val PER: 0.5103
2026-01-08 12:48:35,634: t15.2025.04.13 val PER: 0.3894
2026-01-08 12:48:35,636: New best val WER(5gram) 29.27% --> 27.84%
2026-01-08 12:48:35,637: Checkpointing model
2026-01-08 12:48:35,777: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 12:48:44,949: Train batch 2600: loss: 35.31 grad norm: 81.96 time: 0.055
2026-01-08 12:49:02,643: Train batch 2800: loss: 25.79 grad norm: 72.09 time: 0.081
2026-01-08 12:49:20,184: Train batch 3000: loss: 31.39 grad norm: 77.03 time: 0.083
2026-01-08 12:49:20,186: Running test after training batch: 3000
2026-01-08 12:49:20,288: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:49:25,532: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 12:49:25,595: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost of
2026-01-08 12:49:40,180: Val batch 3000: PER (avg): 0.2805 CTC Loss (avg): 27.8767 WER(5gram): 24.45% (n=256) time: 19.993
2026-01-08 12:49:40,183: WER lens: avg_true_words=5.99 avg_pred_words=5.99 max_pred_words=12
2026-01-08 12:49:40,185: t15.2023.08.13 val PER: 0.2526
2026-01-08 12:49:40,186: t15.2023.08.18 val PER: 0.2163
2026-01-08 12:49:40,188: t15.2023.08.20 val PER: 0.2073
2026-01-08 12:49:40,190: t15.2023.08.25 val PER: 0.1973
2026-01-08 12:49:40,191: t15.2023.08.27 val PER: 0.2926
2026-01-08 12:49:40,193: t15.2023.09.01 val PER: 0.1956
2026-01-08 12:49:40,194: t15.2023.09.03 val PER: 0.2862
2026-01-08 12:49:40,196: t15.2023.09.24 val PER: 0.2087
2026-01-08 12:49:40,197: t15.2023.09.29 val PER: 0.2323
2026-01-08 12:49:40,199: t15.2023.10.01 val PER: 0.2900
2026-01-08 12:49:40,200: t15.2023.10.06 val PER: 0.1927
2026-01-08 12:49:40,202: t15.2023.10.08 val PER: 0.3532
2026-01-08 12:49:40,203: t15.2023.10.13 val PER: 0.3468
2026-01-08 12:49:40,205: t15.2023.10.15 val PER: 0.2690
2026-01-08 12:49:40,206: t15.2023.10.20 val PER: 0.2550
2026-01-08 12:49:40,208: t15.2023.10.22 val PER: 0.2138
2026-01-08 12:49:40,209: t15.2023.11.03 val PER: 0.2754
2026-01-08 12:49:40,211: t15.2023.11.04 val PER: 0.0751
2026-01-08 12:49:40,212: t15.2023.11.17 val PER: 0.1275
2026-01-08 12:49:40,214: t15.2023.11.19 val PER: 0.1277
2026-01-08 12:49:40,215: t15.2023.11.26 val PER: 0.3022
2026-01-08 12:49:40,217: t15.2023.12.03 val PER: 0.2521
2026-01-08 12:49:40,218: t15.2023.12.08 val PER: 0.2517
2026-01-08 12:49:40,220: t15.2023.12.10 val PER: 0.2194
2026-01-08 12:49:40,222: t15.2023.12.17 val PER: 0.2859
2026-01-08 12:49:40,224: t15.2023.12.29 val PER: 0.2835
2026-01-08 12:49:40,226: t15.2024.02.25 val PER: 0.2331
2026-01-08 12:49:40,227: t15.2024.03.08 val PER: 0.3457
2026-01-08 12:49:40,229: t15.2024.03.15 val PER: 0.3315
2026-01-08 12:49:40,230: t15.2024.03.17 val PER: 0.2880
2026-01-08 12:49:40,232: t15.2024.05.10 val PER: 0.3016
2026-01-08 12:49:40,233: t15.2024.06.14 val PER: 0.2871
2026-01-08 12:49:40,234: t15.2024.07.19 val PER: 0.4021
2026-01-08 12:49:40,236: t15.2024.07.21 val PER: 0.2248
2026-01-08 12:49:40,238: t15.2024.07.28 val PER: 0.2779
2026-01-08 12:49:40,239: t15.2025.01.10 val PER: 0.4917
2026-01-08 12:49:40,240: t15.2025.01.12 val PER: 0.3310
2026-01-08 12:49:40,242: t15.2025.03.14 val PER: 0.4527
2026-01-08 12:49:40,243: t15.2025.03.16 val PER: 0.3181
2026-01-08 12:49:40,245: t15.2025.03.30 val PER: 0.4759
2026-01-08 12:49:40,246: t15.2025.04.13 val PER: 0.3566
2026-01-08 12:49:40,248: New best val WER(5gram) 27.84% --> 24.45%
2026-01-08 12:49:40,249: Checkpointing model
2026-01-08 12:49:40,393: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 12:49:58,214: Train batch 3200: loss: 26.48 grad norm: 66.61 time: 0.077
2026-01-08 12:50:15,647: Train batch 3400: loss: 18.65 grad norm: 56.61 time: 0.049
2026-01-08 12:50:24,752: Running test after training batch: 3500
2026-01-08 12:50:24,884: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:50:29,675: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 12:50:29,730: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us it
2026-01-08 12:50:43,638: Val batch 3500: PER (avg): 0.2679 CTC Loss (avg): 26.5995 WER(5gram): 24.51% (n=256) time: 18.884
2026-01-08 12:50:43,641: WER lens: avg_true_words=5.99 avg_pred_words=6.10 max_pred_words=12
2026-01-08 12:50:43,643: t15.2023.08.13 val PER: 0.2401
2026-01-08 12:50:43,645: t15.2023.08.18 val PER: 0.2154
2026-01-08 12:50:43,647: t15.2023.08.20 val PER: 0.2113
2026-01-08 12:50:43,648: t15.2023.08.25 val PER: 0.1928
2026-01-08 12:50:43,650: t15.2023.08.27 val PER: 0.2749
2026-01-08 12:50:43,652: t15.2023.09.01 val PER: 0.1769
2026-01-08 12:50:43,654: t15.2023.09.03 val PER: 0.2494
2026-01-08 12:50:43,655: t15.2023.09.24 val PER: 0.2112
2026-01-08 12:50:43,657: t15.2023.09.29 val PER: 0.2157
2026-01-08 12:50:43,658: t15.2023.10.01 val PER: 0.2781
2026-01-08 12:50:43,660: t15.2023.10.06 val PER: 0.1916
2026-01-08 12:50:43,662: t15.2023.10.08 val PER: 0.3518
2026-01-08 12:50:43,663: t15.2023.10.13 val PER: 0.3173
2026-01-08 12:50:43,665: t15.2023.10.15 val PER: 0.2571
2026-01-08 12:50:43,666: t15.2023.10.20 val PER: 0.2282
2026-01-08 12:50:43,668: t15.2023.10.22 val PER: 0.1982
2026-01-08 12:50:43,670: t15.2023.11.03 val PER: 0.2646
2026-01-08 12:50:43,671: t15.2023.11.04 val PER: 0.0717
2026-01-08 12:50:43,674: t15.2023.11.17 val PER: 0.1166
2026-01-08 12:50:43,675: t15.2023.11.19 val PER: 0.1038
2026-01-08 12:50:43,677: t15.2023.11.26 val PER: 0.2949
2026-01-08 12:50:43,678: t15.2023.12.03 val PER: 0.2311
2026-01-08 12:50:43,680: t15.2023.12.08 val PER: 0.2437
2026-01-08 12:50:43,681: t15.2023.12.10 val PER: 0.1997
2026-01-08 12:50:43,683: t15.2023.12.17 val PER: 0.2547
2026-01-08 12:50:43,685: t15.2023.12.29 val PER: 0.2546
2026-01-08 12:50:43,687: t15.2024.02.25 val PER: 0.2205
2026-01-08 12:50:43,689: t15.2024.03.08 val PER: 0.3457
2026-01-08 12:50:43,690: t15.2024.03.15 val PER: 0.3202
2026-01-08 12:50:43,692: t15.2024.03.17 val PER: 0.2852
2026-01-08 12:50:43,693: t15.2024.05.10 val PER: 0.2630
2026-01-08 12:50:43,695: t15.2024.06.14 val PER: 0.2792
2026-01-08 12:50:43,696: t15.2024.07.19 val PER: 0.3896
2026-01-08 12:50:43,698: t15.2024.07.21 val PER: 0.2345
2026-01-08 12:50:43,699: t15.2024.07.28 val PER: 0.2846
2026-01-08 12:50:43,701: t15.2025.01.10 val PER: 0.4848
2026-01-08 12:50:43,702: t15.2025.01.12 val PER: 0.2871
2026-01-08 12:50:43,704: t15.2025.03.14 val PER: 0.4438
2026-01-08 12:50:43,705: t15.2025.03.16 val PER: 0.3207
2026-01-08 12:50:43,707: t15.2025.03.30 val PER: 0.4529
2026-01-08 12:50:43,708: t15.2025.04.13 val PER: 0.3381
2026-01-08 12:50:52,670: Train batch 3600: loss: 22.87 grad norm: 61.20 time: 0.069
2026-01-08 12:51:10,184: Train batch 3800: loss: 25.09 grad norm: 66.34 time: 0.067
2026-01-08 12:51:27,782: Train batch 4000: loss: 19.21 grad norm: 53.38 time: 0.055
2026-01-08 12:51:27,785: Running test after training batch: 4000
2026-01-08 12:51:27,925: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:51:32,736: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-08 12:51:32,794: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 12:51:45,699: Val batch 4000: PER (avg): 0.2474 CTC Loss (avg): 24.2430 WER(5gram): 24.25% (n=256) time: 17.912
2026-01-08 12:51:45,701: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-08 12:51:45,703: t15.2023.08.13 val PER: 0.2235
2026-01-08 12:51:45,705: t15.2023.08.18 val PER: 0.2003
2026-01-08 12:51:45,707: t15.2023.08.20 val PER: 0.2081
2026-01-08 12:51:45,709: t15.2023.08.25 val PER: 0.1627
2026-01-08 12:51:45,710: t15.2023.08.27 val PER: 0.2958
2026-01-08 12:51:45,712: t15.2023.09.01 val PER: 0.1558
2026-01-08 12:51:45,714: t15.2023.09.03 val PER: 0.2447
2026-01-08 12:51:45,716: t15.2023.09.24 val PER: 0.1893
2026-01-08 12:51:45,717: t15.2023.09.29 val PER: 0.1972
2026-01-08 12:51:45,719: t15.2023.10.01 val PER: 0.2536
2026-01-08 12:51:45,721: t15.2023.10.06 val PER: 0.1636
2026-01-08 12:51:45,723: t15.2023.10.08 val PER: 0.3180
2026-01-08 12:51:45,725: t15.2023.10.13 val PER: 0.3002
2026-01-08 12:51:45,727: t15.2023.10.15 val PER: 0.2413
2026-01-08 12:51:45,728: t15.2023.10.20 val PER: 0.2248
2026-01-08 12:51:45,730: t15.2023.10.22 val PER: 0.1837
2026-01-08 12:51:45,732: t15.2023.11.03 val PER: 0.2429
2026-01-08 12:51:45,734: t15.2023.11.04 val PER: 0.0614
2026-01-08 12:51:45,735: t15.2023.11.17 val PER: 0.0918
2026-01-08 12:51:45,737: t15.2023.11.19 val PER: 0.0978
2026-01-08 12:51:45,738: t15.2023.11.26 val PER: 0.2630
2026-01-08 12:51:45,740: t15.2023.12.03 val PER: 0.2090
2026-01-08 12:51:45,742: t15.2023.12.08 val PER: 0.2237
2026-01-08 12:51:45,743: t15.2023.12.10 val PER: 0.1774
2026-01-08 12:51:45,745: t15.2023.12.17 val PER: 0.2495
2026-01-08 12:51:45,746: t15.2023.12.29 val PER: 0.2450
2026-01-08 12:51:45,748: t15.2024.02.25 val PER: 0.2135
2026-01-08 12:51:45,749: t15.2024.03.08 val PER: 0.3286
2026-01-08 12:51:45,750: t15.2024.03.15 val PER: 0.3089
2026-01-08 12:51:45,752: t15.2024.03.17 val PER: 0.2552
2026-01-08 12:51:45,753: t15.2024.05.10 val PER: 0.2585
2026-01-08 12:51:45,755: t15.2024.06.14 val PER: 0.2713
2026-01-08 12:51:45,756: t15.2024.07.19 val PER: 0.3619
2026-01-08 12:51:45,758: t15.2024.07.21 val PER: 0.1814
2026-01-08 12:51:45,759: t15.2024.07.28 val PER: 0.2404
2026-01-08 12:51:45,761: t15.2025.01.10 val PER: 0.4242
2026-01-08 12:51:45,762: t15.2025.01.12 val PER: 0.2841
2026-01-08 12:51:45,764: t15.2025.03.14 val PER: 0.4053
2026-01-08 12:51:45,765: t15.2025.03.16 val PER: 0.3076
2026-01-08 12:51:45,767: t15.2025.03.30 val PER: 0.4115
2026-01-08 12:51:45,768: t15.2025.04.13 val PER: 0.3224
2026-01-08 12:51:45,770: New best val WER(5gram) 24.45% --> 24.25%
2026-01-08 12:51:45,772: Checkpointing model
2026-01-08 12:51:45,915: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 12:52:03,576: Train batch 4200: loss: 23.21 grad norm: 67.08 time: 0.079
2026-01-08 12:52:21,654: Train batch 4400: loss: 17.00 grad norm: 56.81 time: 0.067
2026-01-08 12:52:30,613: Running test after training batch: 4500
2026-01-08 12:52:30,734: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:52:35,529: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 12:52:35,584: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost at
2026-01-08 12:52:47,629: Val batch 4500: PER (avg): 0.2350 CTC Loss (avg): 23.1124 WER(5gram): 22.10% (n=256) time: 17.014
2026-01-08 12:52:47,631: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-08 12:52:47,633: t15.2023.08.13 val PER: 0.2037
2026-01-08 12:52:47,635: t15.2023.08.18 val PER: 0.1802
2026-01-08 12:52:47,636: t15.2023.08.20 val PER: 0.1835
2026-01-08 12:52:47,638: t15.2023.08.25 val PER: 0.1461
2026-01-08 12:52:47,639: t15.2023.08.27 val PER: 0.2476
2026-01-08 12:52:47,641: t15.2023.09.01 val PER: 0.1542
2026-01-08 12:52:47,642: t15.2023.09.03 val PER: 0.2328
2026-01-08 12:52:47,644: t15.2023.09.24 val PER: 0.1760
2026-01-08 12:52:47,645: t15.2023.09.29 val PER: 0.2017
2026-01-08 12:52:47,646: t15.2023.10.01 val PER: 0.2602
2026-01-08 12:52:47,648: t15.2023.10.06 val PER: 0.1399
2026-01-08 12:52:47,649: t15.2023.10.08 val PER: 0.3194
2026-01-08 12:52:47,651: t15.2023.10.13 val PER: 0.2933
2026-01-08 12:52:47,653: t15.2023.10.15 val PER: 0.2301
2026-01-08 12:52:47,655: t15.2023.10.20 val PER: 0.2181
2026-01-08 12:52:47,656: t15.2023.10.22 val PER: 0.1782
2026-01-08 12:52:47,659: t15.2023.11.03 val PER: 0.2436
2026-01-08 12:52:47,660: t15.2023.11.04 val PER: 0.0614
2026-01-08 12:52:47,662: t15.2023.11.17 val PER: 0.0964
2026-01-08 12:52:47,663: t15.2023.11.19 val PER: 0.0918
2026-01-08 12:52:47,665: t15.2023.11.26 val PER: 0.2638
2026-01-08 12:52:47,666: t15.2023.12.03 val PER: 0.1943
2026-01-08 12:52:47,667: t15.2023.12.08 val PER: 0.2031
2026-01-08 12:52:47,669: t15.2023.12.10 val PER: 0.1813
2026-01-08 12:52:47,670: t15.2023.12.17 val PER: 0.2245
2026-01-08 12:52:47,672: t15.2023.12.29 val PER: 0.2443
2026-01-08 12:52:47,674: t15.2024.02.25 val PER: 0.1966
2026-01-08 12:52:47,675: t15.2024.03.08 val PER: 0.3101
2026-01-08 12:52:47,677: t15.2024.03.15 val PER: 0.2914
2026-01-08 12:52:47,678: t15.2024.03.17 val PER: 0.2301
2026-01-08 12:52:47,680: t15.2024.05.10 val PER: 0.2511
2026-01-08 12:52:47,681: t15.2024.06.14 val PER: 0.2382
2026-01-08 12:52:47,683: t15.2024.07.19 val PER: 0.3362
2026-01-08 12:52:47,684: t15.2024.07.21 val PER: 0.1710
2026-01-08 12:52:47,686: t15.2024.07.28 val PER: 0.2257
2026-01-08 12:52:47,688: t15.2025.01.10 val PER: 0.4174
2026-01-08 12:52:47,689: t15.2025.01.12 val PER: 0.2633
2026-01-08 12:52:47,691: t15.2025.03.14 val PER: 0.4053
2026-01-08 12:52:47,692: t15.2025.03.16 val PER: 0.2866
2026-01-08 12:52:47,694: t15.2025.03.30 val PER: 0.4057
2026-01-08 12:52:47,696: t15.2025.04.13 val PER: 0.2825
2026-01-08 12:52:47,697: New best val WER(5gram) 24.25% --> 22.10%
2026-01-08 12:52:47,699: Checkpointing model
2026-01-08 12:52:47,839: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 12:52:56,568: Train batch 4600: loss: 20.04 grad norm: 62.92 time: 0.063
2026-01-08 12:53:14,460: Train batch 4800: loss: 13.75 grad norm: 53.29 time: 0.064
2026-01-08 12:53:32,122: Train batch 5000: loss: 31.63 grad norm: 81.95 time: 0.064
2026-01-08 12:53:32,124: Running test after training batch: 5000
2026-01-08 12:53:32,251: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:53:37,032: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 12:53:37,095: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 12:53:49,576: Val batch 5000: PER (avg): 0.2237 CTC Loss (avg): 21.8227 WER(5gram): 22.56% (n=256) time: 17.449
2026-01-08 12:53:49,578: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-08 12:53:49,580: t15.2023.08.13 val PER: 0.2017
2026-01-08 12:53:49,582: t15.2023.08.18 val PER: 0.1685
2026-01-08 12:53:49,583: t15.2023.08.20 val PER: 0.1771
2026-01-08 12:53:49,586: t15.2023.08.25 val PER: 0.1340
2026-01-08 12:53:49,588: t15.2023.08.27 val PER: 0.2363
2026-01-08 12:53:49,589: t15.2023.09.01 val PER: 0.1347
2026-01-08 12:53:49,591: t15.2023.09.03 val PER: 0.2280
2026-01-08 12:53:49,593: t15.2023.09.24 val PER: 0.1820
2026-01-08 12:53:49,595: t15.2023.09.29 val PER: 0.1832
2026-01-08 12:53:49,596: t15.2023.10.01 val PER: 0.2398
2026-01-08 12:53:49,598: t15.2023.10.06 val PER: 0.1356
2026-01-08 12:53:49,599: t15.2023.10.08 val PER: 0.3085
2026-01-08 12:53:49,601: t15.2023.10.13 val PER: 0.2839
2026-01-08 12:53:49,602: t15.2023.10.15 val PER: 0.2228
2026-01-08 12:53:49,604: t15.2023.10.20 val PER: 0.2416
2026-01-08 12:53:49,606: t15.2023.10.22 val PER: 0.1715
2026-01-08 12:53:49,607: t15.2023.11.03 val PER: 0.2300
2026-01-08 12:53:49,609: t15.2023.11.04 val PER: 0.0546
2026-01-08 12:53:49,611: t15.2023.11.17 val PER: 0.0871
2026-01-08 12:53:49,613: t15.2023.11.19 val PER: 0.0798
2026-01-08 12:53:49,614: t15.2023.11.26 val PER: 0.2290
2026-01-08 12:53:49,616: t15.2023.12.03 val PER: 0.2006
2026-01-08 12:53:49,617: t15.2023.12.08 val PER: 0.1917
2026-01-08 12:53:49,621: t15.2023.12.10 val PER: 0.1590
2026-01-08 12:53:49,622: t15.2023.12.17 val PER: 0.2131
2026-01-08 12:53:49,624: t15.2023.12.29 val PER: 0.2141
2026-01-08 12:53:49,625: t15.2024.02.25 val PER: 0.1770
2026-01-08 12:53:49,627: t15.2024.03.08 val PER: 0.3058
2026-01-08 12:53:49,628: t15.2024.03.15 val PER: 0.2720
2026-01-08 12:53:49,629: t15.2024.03.17 val PER: 0.2294
2026-01-08 12:53:49,631: t15.2024.05.10 val PER: 0.2422
2026-01-08 12:53:49,632: t15.2024.06.14 val PER: 0.2492
2026-01-08 12:53:49,634: t15.2024.07.19 val PER: 0.3243
2026-01-08 12:53:49,635: t15.2024.07.21 val PER: 0.1786
2026-01-08 12:53:49,637: t15.2024.07.28 val PER: 0.2118
2026-01-08 12:53:49,638: t15.2025.01.10 val PER: 0.3871
2026-01-08 12:53:49,640: t15.2025.01.12 val PER: 0.2502
2026-01-08 12:53:49,641: t15.2025.03.14 val PER: 0.3713
2026-01-08 12:53:49,642: t15.2025.03.16 val PER: 0.2723
2026-01-08 12:53:49,644: t15.2025.03.30 val PER: 0.3862
2026-01-08 12:53:49,645: t15.2025.04.13 val PER: 0.3039
2026-01-08 12:54:08,052: Train batch 5200: loss: 16.45 grad norm: 57.63 time: 0.053
2026-01-08 12:54:26,406: Train batch 5400: loss: 17.23 grad norm: 59.71 time: 0.068
2026-01-08 12:54:35,619: Running test after training batch: 5500
2026-01-08 12:54:35,771: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:54:40,802: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 12:54:40,859: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 12:54:52,793: Val batch 5500: PER (avg): 0.2152 CTC Loss (avg): 20.9808 WER(5gram): 20.60% (n=256) time: 17.171
2026-01-08 12:54:52,795: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-08 12:54:52,797: t15.2023.08.13 val PER: 0.1778
2026-01-08 12:54:52,799: t15.2023.08.18 val PER: 0.1660
2026-01-08 12:54:52,800: t15.2023.08.20 val PER: 0.1732
2026-01-08 12:54:52,801: t15.2023.08.25 val PER: 0.1160
2026-01-08 12:54:52,803: t15.2023.08.27 val PER: 0.2395
2026-01-08 12:54:52,804: t15.2023.09.01 val PER: 0.1291
2026-01-08 12:54:52,806: t15.2023.09.03 val PER: 0.2245
2026-01-08 12:54:52,807: t15.2023.09.24 val PER: 0.1711
2026-01-08 12:54:52,809: t15.2023.09.29 val PER: 0.1761
2026-01-08 12:54:52,810: t15.2023.10.01 val PER: 0.2318
2026-01-08 12:54:52,813: t15.2023.10.06 val PER: 0.1378
2026-01-08 12:54:52,814: t15.2023.10.08 val PER: 0.2896
2026-01-08 12:54:52,816: t15.2023.10.13 val PER: 0.2754
2026-01-08 12:54:52,818: t15.2023.10.15 val PER: 0.2123
2026-01-08 12:54:52,820: t15.2023.10.20 val PER: 0.2282
2026-01-08 12:54:52,821: t15.2023.10.22 val PER: 0.1648
2026-01-08 12:54:52,822: t15.2023.11.03 val PER: 0.2198
2026-01-08 12:54:52,823: t15.2023.11.04 val PER: 0.0580
2026-01-08 12:54:52,825: t15.2023.11.17 val PER: 0.0762
2026-01-08 12:54:52,826: t15.2023.11.19 val PER: 0.0739
2026-01-08 12:54:52,828: t15.2023.11.26 val PER: 0.2138
2026-01-08 12:54:52,830: t15.2023.12.03 val PER: 0.1796
2026-01-08 12:54:52,831: t15.2023.12.08 val PER: 0.1904
2026-01-08 12:54:52,832: t15.2023.12.10 val PER: 0.1485
2026-01-08 12:54:52,834: t15.2023.12.17 val PER: 0.2193
2026-01-08 12:54:52,835: t15.2023.12.29 val PER: 0.2114
2026-01-08 12:54:52,837: t15.2024.02.25 val PER: 0.1756
2026-01-08 12:54:52,838: t15.2024.03.08 val PER: 0.2916
2026-01-08 12:54:52,840: t15.2024.03.15 val PER: 0.2614
2026-01-08 12:54:52,842: t15.2024.03.17 val PER: 0.2197
2026-01-08 12:54:52,843: t15.2024.05.10 val PER: 0.2407
2026-01-08 12:54:52,845: t15.2024.06.14 val PER: 0.2303
2026-01-08 12:54:52,846: t15.2024.07.19 val PER: 0.3204
2026-01-08 12:54:52,848: t15.2024.07.21 val PER: 0.1697
2026-01-08 12:54:52,849: t15.2024.07.28 val PER: 0.2110
2026-01-08 12:54:52,850: t15.2025.01.10 val PER: 0.3884
2026-01-08 12:54:52,852: t15.2025.01.12 val PER: 0.2340
2026-01-08 12:54:52,853: t15.2025.03.14 val PER: 0.3728
2026-01-08 12:54:52,854: t15.2025.03.16 val PER: 0.2539
2026-01-08 12:54:52,856: t15.2025.03.30 val PER: 0.3517
2026-01-08 12:54:52,860: t15.2025.04.13 val PER: 0.2924
2026-01-08 12:54:52,862: New best val WER(5gram) 22.10% --> 20.60%
2026-01-08 12:54:52,864: Checkpointing model
2026-01-08 12:54:53,008: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 12:55:01,920: Train batch 5600: loss: 19.43 grad norm: 64.00 time: 0.062
2026-01-08 12:55:19,734: Train batch 5800: loss: 13.83 grad norm: 58.13 time: 0.088
2026-01-08 12:55:36,890: Train batch 6000: loss: 14.11 grad norm: 56.70 time: 0.050
2026-01-08 12:55:36,892: Running test after training batch: 6000
2026-01-08 12:55:37,014: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:55:41,751: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 12:55:41,807: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 12:55:53,114: Val batch 6000: PER (avg): 0.2108 CTC Loss (avg): 20.8218 WER(5gram): 21.06% (n=256) time: 16.220
2026-01-08 12:55:53,117: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 12:55:53,119: t15.2023.08.13 val PER: 0.1871
2026-01-08 12:55:53,120: t15.2023.08.18 val PER: 0.1760
2026-01-08 12:55:53,122: t15.2023.08.20 val PER: 0.1700
2026-01-08 12:55:53,124: t15.2023.08.25 val PER: 0.1190
2026-01-08 12:55:53,125: t15.2023.08.27 val PER: 0.2428
2026-01-08 12:55:53,127: t15.2023.09.01 val PER: 0.1266
2026-01-08 12:55:53,129: t15.2023.09.03 val PER: 0.2150
2026-01-08 12:55:53,130: t15.2023.09.24 val PER: 0.1723
2026-01-08 12:55:53,132: t15.2023.09.29 val PER: 0.1621
2026-01-08 12:55:53,133: t15.2023.10.01 val PER: 0.2094
2026-01-08 12:55:53,135: t15.2023.10.06 val PER: 0.1259
2026-01-08 12:55:53,137: t15.2023.10.08 val PER: 0.2896
2026-01-08 12:55:53,138: t15.2023.10.13 val PER: 0.2669
2026-01-08 12:55:53,140: t15.2023.10.15 val PER: 0.2175
2026-01-08 12:55:53,141: t15.2023.10.20 val PER: 0.2148
2026-01-08 12:55:53,143: t15.2023.10.22 val PER: 0.1704
2026-01-08 12:55:53,145: t15.2023.11.03 val PER: 0.2205
2026-01-08 12:55:53,146: t15.2023.11.04 val PER: 0.0444
2026-01-08 12:55:53,148: t15.2023.11.17 val PER: 0.0809
2026-01-08 12:55:53,150: t15.2023.11.19 val PER: 0.0758
2026-01-08 12:55:53,151: t15.2023.11.26 val PER: 0.2159
2026-01-08 12:55:53,153: t15.2023.12.03 val PER: 0.1723
2026-01-08 12:55:53,154: t15.2023.12.08 val PER: 0.1704
2026-01-08 12:55:53,156: t15.2023.12.10 val PER: 0.1577
2026-01-08 12:55:53,158: t15.2023.12.17 val PER: 0.2027
2026-01-08 12:55:53,159: t15.2023.12.29 val PER: 0.2155
2026-01-08 12:55:53,161: t15.2024.02.25 val PER: 0.1601
2026-01-08 12:55:53,162: t15.2024.03.08 val PER: 0.2916
2026-01-08 12:55:53,164: t15.2024.03.15 val PER: 0.2689
2026-01-08 12:55:53,165: t15.2024.03.17 val PER: 0.2148
2026-01-08 12:55:53,167: t15.2024.05.10 val PER: 0.2199
2026-01-08 12:55:53,169: t15.2024.06.14 val PER: 0.2161
2026-01-08 12:55:53,170: t15.2024.07.19 val PER: 0.3092
2026-01-08 12:55:53,172: t15.2024.07.21 val PER: 0.1662
2026-01-08 12:55:53,174: t15.2024.07.28 val PER: 0.2022
2026-01-08 12:55:53,175: t15.2025.01.10 val PER: 0.3884
2026-01-08 12:55:53,177: t15.2025.01.12 val PER: 0.2186
2026-01-08 12:55:53,179: t15.2025.03.14 val PER: 0.3624
2026-01-08 12:55:53,181: t15.2025.03.16 val PER: 0.2526
2026-01-08 12:55:53,182: t15.2025.03.30 val PER: 0.3667
2026-01-08 12:55:53,185: t15.2025.04.13 val PER: 0.2753
2026-01-08 12:56:10,919: Train batch 6200: loss: 17.04 grad norm: 66.27 time: 0.073
2026-01-08 12:56:28,631: Train batch 6400: loss: 18.87 grad norm: 64.21 time: 0.064
2026-01-08 12:56:37,736: Running test after training batch: 6500
2026-01-08 12:56:37,890: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:56:42,680: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 12:56:42,736: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 12:56:53,967: Val batch 6500: PER (avg): 0.2035 CTC Loss (avg): 19.9522 WER(5gram): 17.41% (n=256) time: 16.228
2026-01-08 12:56:53,970: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-08 12:56:53,972: t15.2023.08.13 val PER: 0.1746
2026-01-08 12:56:53,974: t15.2023.08.18 val PER: 0.1526
2026-01-08 12:56:53,976: t15.2023.08.20 val PER: 0.1652
2026-01-08 12:56:53,977: t15.2023.08.25 val PER: 0.1084
2026-01-08 12:56:53,980: t15.2023.08.27 val PER: 0.2331
2026-01-08 12:56:53,982: t15.2023.09.01 val PER: 0.1258
2026-01-08 12:56:53,983: t15.2023.09.03 val PER: 0.2090
2026-01-08 12:56:53,985: t15.2023.09.24 val PER: 0.1735
2026-01-08 12:56:53,987: t15.2023.09.29 val PER: 0.1698
2026-01-08 12:56:53,988: t15.2023.10.01 val PER: 0.2206
2026-01-08 12:56:53,990: t15.2023.10.06 val PER: 0.1152
2026-01-08 12:56:53,991: t15.2023.10.08 val PER: 0.2882
2026-01-08 12:56:53,992: t15.2023.10.13 val PER: 0.2560
2026-01-08 12:56:53,994: t15.2023.10.15 val PER: 0.2169
2026-01-08 12:56:53,995: t15.2023.10.20 val PER: 0.2047
2026-01-08 12:56:53,997: t15.2023.10.22 val PER: 0.1581
2026-01-08 12:56:53,998: t15.2023.11.03 val PER: 0.2117
2026-01-08 12:56:54,000: t15.2023.11.04 val PER: 0.0444
2026-01-08 12:56:54,001: t15.2023.11.17 val PER: 0.0653
2026-01-08 12:56:54,002: t15.2023.11.19 val PER: 0.0719
2026-01-08 12:56:54,004: t15.2023.11.26 val PER: 0.2094
2026-01-08 12:56:54,005: t15.2023.12.03 val PER: 0.1817
2026-01-08 12:56:54,007: t15.2023.12.08 val PER: 0.1724
2026-01-08 12:56:54,008: t15.2023.12.10 val PER: 0.1340
2026-01-08 12:56:54,010: t15.2023.12.17 val PER: 0.1861
2026-01-08 12:56:54,011: t15.2023.12.29 val PER: 0.1990
2026-01-08 12:56:54,012: t15.2024.02.25 val PER: 0.1587
2026-01-08 12:56:54,014: t15.2024.03.08 val PER: 0.2817
2026-01-08 12:56:54,015: t15.2024.03.15 val PER: 0.2595
2026-01-08 12:56:54,017: t15.2024.03.17 val PER: 0.2050
2026-01-08 12:56:54,018: t15.2024.05.10 val PER: 0.2273
2026-01-08 12:56:54,019: t15.2024.06.14 val PER: 0.2129
2026-01-08 12:56:54,021: t15.2024.07.19 val PER: 0.2966
2026-01-08 12:56:54,022: t15.2024.07.21 val PER: 0.1455
2026-01-08 12:56:54,024: t15.2024.07.28 val PER: 0.1882
2026-01-08 12:56:54,025: t15.2025.01.10 val PER: 0.3747
2026-01-08 12:56:54,027: t15.2025.01.12 val PER: 0.2025
2026-01-08 12:56:54,028: t15.2025.03.14 val PER: 0.3757
2026-01-08 12:56:54,029: t15.2025.03.16 val PER: 0.2395
2026-01-08 12:56:54,031: t15.2025.03.30 val PER: 0.3517
2026-01-08 12:56:54,032: t15.2025.04.13 val PER: 0.2753
2026-01-08 12:56:54,034: New best val WER(5gram) 20.60% --> 17.41%
2026-01-08 12:56:54,035: Checkpointing model
2026-01-08 12:56:54,183: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 12:57:03,197: Train batch 6600: loss: 12.43 grad norm: 54.38 time: 0.046
2026-01-08 12:57:21,322: Train batch 6800: loss: 15.33 grad norm: 57.43 time: 0.049
2026-01-08 12:57:39,434: Train batch 7000: loss: 17.36 grad norm: 63.38 time: 0.061
2026-01-08 12:57:39,437: Running test after training batch: 7000
2026-01-08 12:57:39,542: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:57:44,332: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 12:57:44,380: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 12:57:55,354: Val batch 7000: PER (avg): 0.1935 CTC Loss (avg): 18.9239 WER(5gram): 18.84% (n=256) time: 15.915
2026-01-08 12:57:55,356: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-08 12:57:55,358: t15.2023.08.13 val PER: 0.1601
2026-01-08 12:57:55,359: t15.2023.08.18 val PER: 0.1450
2026-01-08 12:57:55,361: t15.2023.08.20 val PER: 0.1525
2026-01-08 12:57:55,362: t15.2023.08.25 val PER: 0.1024
2026-01-08 12:57:55,364: t15.2023.08.27 val PER: 0.2186
2026-01-08 12:57:55,366: t15.2023.09.01 val PER: 0.1161
2026-01-08 12:57:55,368: t15.2023.09.03 val PER: 0.1995
2026-01-08 12:57:55,369: t15.2023.09.24 val PER: 0.1602
2026-01-08 12:57:55,370: t15.2023.09.29 val PER: 0.1710
2026-01-08 12:57:55,372: t15.2023.10.01 val PER: 0.2173
2026-01-08 12:57:55,373: t15.2023.10.06 val PER: 0.1076
2026-01-08 12:57:55,375: t15.2023.10.08 val PER: 0.2747
2026-01-08 12:57:55,376: t15.2023.10.13 val PER: 0.2506
2026-01-08 12:57:55,377: t15.2023.10.15 val PER: 0.1931
2026-01-08 12:57:55,379: t15.2023.10.20 val PER: 0.1913
2026-01-08 12:57:55,380: t15.2023.10.22 val PER: 0.1470
2026-01-08 12:57:55,382: t15.2023.11.03 val PER: 0.2062
2026-01-08 12:57:55,383: t15.2023.11.04 val PER: 0.0307
2026-01-08 12:57:55,385: t15.2023.11.17 val PER: 0.0653
2026-01-08 12:57:55,386: t15.2023.11.19 val PER: 0.0479
2026-01-08 12:57:55,388: t15.2023.11.26 val PER: 0.1949
2026-01-08 12:57:55,389: t15.2023.12.03 val PER: 0.1576
2026-01-08 12:57:55,390: t15.2023.12.08 val PER: 0.1491
2026-01-08 12:57:55,391: t15.2023.12.10 val PER: 0.1380
2026-01-08 12:57:55,393: t15.2023.12.17 val PER: 0.1788
2026-01-08 12:57:55,394: t15.2023.12.29 val PER: 0.1942
2026-01-08 12:57:55,395: t15.2024.02.25 val PER: 0.1601
2026-01-08 12:57:55,396: t15.2024.03.08 val PER: 0.2774
2026-01-08 12:57:55,398: t15.2024.03.15 val PER: 0.2402
2026-01-08 12:57:55,399: t15.2024.03.17 val PER: 0.2001
2026-01-08 12:57:55,400: t15.2024.05.10 val PER: 0.2021
2026-01-08 12:57:55,402: t15.2024.06.14 val PER: 0.2066
2026-01-08 12:57:55,403: t15.2024.07.19 val PER: 0.3019
2026-01-08 12:57:55,404: t15.2024.07.21 val PER: 0.1283
2026-01-08 12:57:55,406: t15.2024.07.28 val PER: 0.1743
2026-01-08 12:57:55,407: t15.2025.01.10 val PER: 0.3567
2026-01-08 12:57:55,408: t15.2025.01.12 val PER: 0.2017
2026-01-08 12:57:55,410: t15.2025.03.14 val PER: 0.3491
2026-01-08 12:57:55,411: t15.2025.03.16 val PER: 0.2382
2026-01-08 12:57:55,412: t15.2025.03.30 val PER: 0.3437
2026-01-08 12:57:55,413: t15.2025.04.13 val PER: 0.2725
2026-01-08 12:58:12,953: Train batch 7200: loss: 14.48 grad norm: 57.10 time: 0.079
2026-01-08 12:58:30,384: Train batch 7400: loss: 13.72 grad norm: 54.62 time: 0.076
2026-01-08 12:58:39,114: Running test after training batch: 7500
2026-01-08 12:58:39,229: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:58:44,171: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 12:58:44,222: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 12:58:55,383: Val batch 7500: PER (avg): 0.1891 CTC Loss (avg): 18.6399 WER(5gram): 17.28% (n=256) time: 16.267
2026-01-08 12:58:55,386: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-08 12:58:55,389: t15.2023.08.13 val PER: 0.1590
2026-01-08 12:58:55,390: t15.2023.08.18 val PER: 0.1324
2026-01-08 12:58:55,392: t15.2023.08.20 val PER: 0.1438
2026-01-08 12:58:55,394: t15.2023.08.25 val PER: 0.1069
2026-01-08 12:58:55,395: t15.2023.08.27 val PER: 0.2138
2026-01-08 12:58:55,397: t15.2023.09.01 val PER: 0.1136
2026-01-08 12:58:55,398: t15.2023.09.03 val PER: 0.1948
2026-01-08 12:58:55,400: t15.2023.09.24 val PER: 0.1456
2026-01-08 12:58:55,402: t15.2023.09.29 val PER: 0.1595
2026-01-08 12:58:55,403: t15.2023.10.01 val PER: 0.2067
2026-01-08 12:58:55,405: t15.2023.10.06 val PER: 0.1087
2026-01-08 12:58:55,406: t15.2023.10.08 val PER: 0.2720
2026-01-08 12:58:55,408: t15.2023.10.13 val PER: 0.2506
2026-01-08 12:58:55,410: t15.2023.10.15 val PER: 0.1971
2026-01-08 12:58:55,411: t15.2023.10.20 val PER: 0.2047
2026-01-08 12:58:55,413: t15.2023.10.22 val PER: 0.1459
2026-01-08 12:58:55,415: t15.2023.11.03 val PER: 0.1988
2026-01-08 12:58:55,416: t15.2023.11.04 val PER: 0.0410
2026-01-08 12:58:55,418: t15.2023.11.17 val PER: 0.0622
2026-01-08 12:58:55,420: t15.2023.11.19 val PER: 0.0419
2026-01-08 12:58:55,421: t15.2023.11.26 val PER: 0.1891
2026-01-08 12:58:55,423: t15.2023.12.03 val PER: 0.1565
2026-01-08 12:58:55,425: t15.2023.12.08 val PER: 0.1511
2026-01-08 12:58:55,427: t15.2023.12.10 val PER: 0.1261
2026-01-08 12:58:55,429: t15.2023.12.17 val PER: 0.1798
2026-01-08 12:58:55,430: t15.2023.12.29 val PER: 0.1915
2026-01-08 12:58:55,432: t15.2024.02.25 val PER: 0.1461
2026-01-08 12:58:55,433: t15.2024.03.08 val PER: 0.2745
2026-01-08 12:58:55,435: t15.2024.03.15 val PER: 0.2439
2026-01-08 12:58:55,437: t15.2024.03.17 val PER: 0.1785
2026-01-08 12:58:55,438: t15.2024.05.10 val PER: 0.2125
2026-01-08 12:58:55,440: t15.2024.06.14 val PER: 0.1972
2026-01-08 12:58:55,441: t15.2024.07.19 val PER: 0.2887
2026-01-08 12:58:55,443: t15.2024.07.21 val PER: 0.1283
2026-01-08 12:58:55,444: t15.2024.07.28 val PER: 0.1750
2026-01-08 12:58:55,446: t15.2025.01.10 val PER: 0.3471
2026-01-08 12:58:55,447: t15.2025.01.12 val PER: 0.1848
2026-01-08 12:58:55,449: t15.2025.03.14 val PER: 0.3609
2026-01-08 12:58:55,451: t15.2025.03.16 val PER: 0.2382
2026-01-08 12:58:55,453: t15.2025.03.30 val PER: 0.3563
2026-01-08 12:58:55,454: t15.2025.04.13 val PER: 0.2511
2026-01-08 12:58:55,456: New best val WER(5gram) 17.41% --> 17.28%
2026-01-08 12:58:55,458: Checkpointing model
2026-01-08 12:58:55,602: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 12:59:04,910: Train batch 7600: loss: 16.08 grad norm: 60.34 time: 0.069
2026-01-08 12:59:23,530: Train batch 7800: loss: 13.89 grad norm: 58.74 time: 0.057
2026-01-08 12:59:41,783: Train batch 8000: loss: 11.00 grad norm: 48.10 time: 0.073
2026-01-08 12:59:41,785: Running test after training batch: 8000
2026-01-08 12:59:41,887: WER debug GT example: You can see the code at this point as well.
2026-01-08 12:59:46,634: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 12:59:46,687: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 12:59:57,887: Val batch 8000: PER (avg): 0.1845 CTC Loss (avg): 18.1481 WER(5gram): 18.12% (n=256) time: 16.100
2026-01-08 12:59:57,889: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=13
2026-01-08 12:59:57,891: t15.2023.08.13 val PER: 0.1445
2026-01-08 12:59:57,893: t15.2023.08.18 val PER: 0.1324
2026-01-08 12:59:57,895: t15.2023.08.20 val PER: 0.1485
2026-01-08 12:59:57,897: t15.2023.08.25 val PER: 0.1024
2026-01-08 12:59:57,898: t15.2023.08.27 val PER: 0.2090
2026-01-08 12:59:57,900: t15.2023.09.01 val PER: 0.1039
2026-01-08 12:59:57,902: t15.2023.09.03 val PER: 0.1971
2026-01-08 12:59:57,904: t15.2023.09.24 val PER: 0.1505
2026-01-08 12:59:57,906: t15.2023.09.29 val PER: 0.1532
2026-01-08 12:59:57,908: t15.2023.10.01 val PER: 0.1935
2026-01-08 12:59:57,910: t15.2023.10.06 val PER: 0.1076
2026-01-08 12:59:57,912: t15.2023.10.08 val PER: 0.2747
2026-01-08 12:59:57,914: t15.2023.10.13 val PER: 0.2428
2026-01-08 12:59:57,915: t15.2023.10.15 val PER: 0.1819
2026-01-08 12:59:57,917: t15.2023.10.20 val PER: 0.2047
2026-01-08 12:59:57,918: t15.2023.10.22 val PER: 0.1459
2026-01-08 12:59:57,920: t15.2023.11.03 val PER: 0.2110
2026-01-08 12:59:57,921: t15.2023.11.04 val PER: 0.0341
2026-01-08 12:59:57,923: t15.2023.11.17 val PER: 0.0560
2026-01-08 12:59:57,925: t15.2023.11.19 val PER: 0.0599
2026-01-08 12:59:57,926: t15.2023.11.26 val PER: 0.1775
2026-01-08 12:59:57,928: t15.2023.12.03 val PER: 0.1513
2026-01-08 12:59:57,929: t15.2023.12.08 val PER: 0.1405
2026-01-08 12:59:57,931: t15.2023.12.10 val PER: 0.1222
2026-01-08 12:59:57,932: t15.2023.12.17 val PER: 0.1788
2026-01-08 12:59:57,934: t15.2023.12.29 val PER: 0.1771
2026-01-08 12:59:57,936: t15.2024.02.25 val PER: 0.1306
2026-01-08 12:59:57,937: t15.2024.03.08 val PER: 0.2745
2026-01-08 12:59:57,939: t15.2024.03.15 val PER: 0.2420
2026-01-08 12:59:57,940: t15.2024.03.17 val PER: 0.1792
2026-01-08 12:59:57,942: t15.2024.05.10 val PER: 0.1917
2026-01-08 12:59:57,943: t15.2024.06.14 val PER: 0.2066
2026-01-08 12:59:57,945: t15.2024.07.19 val PER: 0.2828
2026-01-08 12:59:57,947: t15.2024.07.21 val PER: 0.1228
2026-01-08 12:59:57,949: t15.2024.07.28 val PER: 0.1640
2026-01-08 12:59:57,951: t15.2025.01.10 val PER: 0.3278
2026-01-08 12:59:57,952: t15.2025.01.12 val PER: 0.1909
2026-01-08 12:59:57,954: t15.2025.03.14 val PER: 0.3639
2026-01-08 12:59:57,955: t15.2025.03.16 val PER: 0.2356
2026-01-08 12:59:57,957: t15.2025.03.30 val PER: 0.3391
2026-01-08 12:59:57,958: t15.2025.04.13 val PER: 0.2596
2026-01-08 13:00:15,497: Train batch 8200: loss: 9.72 grad norm: 46.97 time: 0.054
2026-01-08 13:00:33,185: Train batch 8400: loss: 9.99 grad norm: 46.78 time: 0.063
2026-01-08 13:00:42,761: Running test after training batch: 8500
2026-01-08 13:00:42,872: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:00:47,657: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:00:47,703: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 13:01:00,748: Val batch 8500: PER (avg): 0.1782 CTC Loss (avg): 17.6859 WER(5gram): 16.95% (n=256) time: 17.985
2026-01-08 13:01:00,751: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-08 13:01:00,753: t15.2023.08.13 val PER: 0.1455
2026-01-08 13:01:00,755: t15.2023.08.18 val PER: 0.1350
2026-01-08 13:01:00,756: t15.2023.08.20 val PER: 0.1430
2026-01-08 13:01:00,758: t15.2023.08.25 val PER: 0.1130
2026-01-08 13:01:00,759: t15.2023.08.27 val PER: 0.2106
2026-01-08 13:01:00,761: t15.2023.09.01 val PER: 0.1055
2026-01-08 13:01:00,762: t15.2023.09.03 val PER: 0.1960
2026-01-08 13:01:00,763: t15.2023.09.24 val PER: 0.1481
2026-01-08 13:01:00,765: t15.2023.09.29 val PER: 0.1500
2026-01-08 13:01:00,766: t15.2023.10.01 val PER: 0.1929
2026-01-08 13:01:00,768: t15.2023.10.06 val PER: 0.1012
2026-01-08 13:01:00,769: t15.2023.10.08 val PER: 0.2720
2026-01-08 13:01:00,770: t15.2023.10.13 val PER: 0.2250
2026-01-08 13:01:00,772: t15.2023.10.15 val PER: 0.1740
2026-01-08 13:01:00,773: t15.2023.10.20 val PER: 0.1913
2026-01-08 13:01:00,774: t15.2023.10.22 val PER: 0.1470
2026-01-08 13:01:00,776: t15.2023.11.03 val PER: 0.1967
2026-01-08 13:01:00,777: t15.2023.11.04 val PER: 0.0546
2026-01-08 13:01:00,778: t15.2023.11.17 val PER: 0.0638
2026-01-08 13:01:00,780: t15.2023.11.19 val PER: 0.0539
2026-01-08 13:01:00,781: t15.2023.11.26 val PER: 0.1703
2026-01-08 13:01:00,782: t15.2023.12.03 val PER: 0.1366
2026-01-08 13:01:00,784: t15.2023.12.08 val PER: 0.1372
2026-01-08 13:01:00,785: t15.2023.12.10 val PER: 0.1183
2026-01-08 13:01:00,787: t15.2023.12.17 val PER: 0.1705
2026-01-08 13:01:00,789: t15.2023.12.29 val PER: 0.1620
2026-01-08 13:01:00,790: t15.2024.02.25 val PER: 0.1320
2026-01-08 13:01:00,792: t15.2024.03.08 val PER: 0.2703
2026-01-08 13:01:00,793: t15.2024.03.15 val PER: 0.2351
2026-01-08 13:01:00,795: t15.2024.03.17 val PER: 0.1785
2026-01-08 13:01:00,796: t15.2024.05.10 val PER: 0.1932
2026-01-08 13:01:00,798: t15.2024.06.14 val PER: 0.1877
2026-01-08 13:01:00,800: t15.2024.07.19 val PER: 0.2709
2026-01-08 13:01:00,801: t15.2024.07.21 val PER: 0.1145
2026-01-08 13:01:00,802: t15.2024.07.28 val PER: 0.1632
2026-01-08 13:01:00,803: t15.2025.01.10 val PER: 0.3085
2026-01-08 13:01:00,805: t15.2025.01.12 val PER: 0.1778
2026-01-08 13:01:00,807: t15.2025.03.14 val PER: 0.3521
2026-01-08 13:01:00,808: t15.2025.03.16 val PER: 0.2081
2026-01-08 13:01:00,809: t15.2025.03.30 val PER: 0.3299
2026-01-08 13:01:00,811: t15.2025.04.13 val PER: 0.2439
2026-01-08 13:01:00,813: New best val WER(5gram) 17.28% --> 16.95%
2026-01-08 13:01:00,814: Checkpointing model
2026-01-08 13:01:00,957: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:01:09,823: Train batch 8600: loss: 15.88 grad norm: 61.10 time: 0.055
2026-01-08 13:01:27,232: Train batch 8800: loss: 14.90 grad norm: 58.37 time: 0.060
2026-01-08 13:01:45,030: Train batch 9000: loss: 15.47 grad norm: 61.47 time: 0.073
2026-01-08 13:01:45,032: Running test after training batch: 9000
2026-01-08 13:01:45,159: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:01:49,896: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:01:49,945: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 13:02:01,129: Val batch 9000: PER (avg): 0.1740 CTC Loss (avg): 17.3401 WER(5gram): 16.82% (n=256) time: 16.094
2026-01-08 13:02:01,131: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 13:02:01,133: t15.2023.08.13 val PER: 0.1341
2026-01-08 13:02:01,135: t15.2023.08.18 val PER: 0.1324
2026-01-08 13:02:01,137: t15.2023.08.20 val PER: 0.1350
2026-01-08 13:02:01,138: t15.2023.08.25 val PER: 0.1054
2026-01-08 13:02:01,140: t15.2023.08.27 val PER: 0.2074
2026-01-08 13:02:01,141: t15.2023.09.01 val PER: 0.0990
2026-01-08 13:02:01,143: t15.2023.09.03 val PER: 0.1876
2026-01-08 13:02:01,145: t15.2023.09.24 val PER: 0.1481
2026-01-08 13:02:01,147: t15.2023.09.29 val PER: 0.1436
2026-01-08 13:02:01,148: t15.2023.10.01 val PER: 0.1909
2026-01-08 13:02:01,150: t15.2023.10.06 val PER: 0.0969
2026-01-08 13:02:01,151: t15.2023.10.08 val PER: 0.2544
2026-01-08 13:02:01,153: t15.2023.10.13 val PER: 0.2250
2026-01-08 13:02:01,154: t15.2023.10.15 val PER: 0.1833
2026-01-08 13:02:01,158: t15.2023.10.20 val PER: 0.2013
2026-01-08 13:02:01,160: t15.2023.10.22 val PER: 0.1281
2026-01-08 13:02:01,162: t15.2023.11.03 val PER: 0.1947
2026-01-08 13:02:01,164: t15.2023.11.04 val PER: 0.0410
2026-01-08 13:02:01,165: t15.2023.11.17 val PER: 0.0498
2026-01-08 13:02:01,167: t15.2023.11.19 val PER: 0.0519
2026-01-08 13:02:01,168: t15.2023.11.26 val PER: 0.1587
2026-01-08 13:02:01,170: t15.2023.12.03 val PER: 0.1408
2026-01-08 13:02:01,172: t15.2023.12.08 val PER: 0.1298
2026-01-08 13:02:01,173: t15.2023.12.10 val PER: 0.1170
2026-01-08 13:02:01,175: t15.2023.12.17 val PER: 0.1674
2026-01-08 13:02:01,176: t15.2023.12.29 val PER: 0.1585
2026-01-08 13:02:01,178: t15.2024.02.25 val PER: 0.1461
2026-01-08 13:02:01,179: t15.2024.03.08 val PER: 0.2632
2026-01-08 13:02:01,181: t15.2024.03.15 val PER: 0.2245
2026-01-08 13:02:01,182: t15.2024.03.17 val PER: 0.1750
2026-01-08 13:02:01,184: t15.2024.05.10 val PER: 0.1798
2026-01-08 13:02:01,185: t15.2024.06.14 val PER: 0.1893
2026-01-08 13:02:01,187: t15.2024.07.19 val PER: 0.2670
2026-01-08 13:02:01,188: t15.2024.07.21 val PER: 0.1110
2026-01-08 13:02:01,190: t15.2024.07.28 val PER: 0.1522
2026-01-08 13:02:01,192: t15.2025.01.10 val PER: 0.3085
2026-01-08 13:02:01,193: t15.2025.01.12 val PER: 0.1817
2026-01-08 13:02:01,195: t15.2025.03.14 val PER: 0.3476
2026-01-08 13:02:01,197: t15.2025.03.16 val PER: 0.2160
2026-01-08 13:02:01,198: t15.2025.03.30 val PER: 0.3253
2026-01-08 13:02:01,200: t15.2025.04.13 val PER: 0.2411
2026-01-08 13:02:01,202: New best val WER(5gram) 16.95% --> 16.82%
2026-01-08 13:02:01,203: Checkpointing model
2026-01-08 13:02:01,358: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:02:18,757: Train batch 9200: loss: 10.95 grad norm: 51.11 time: 0.057
2026-01-08 13:02:36,415: Train batch 9400: loss: 7.60 grad norm: 46.55 time: 0.068
2026-01-08 13:02:45,231: Running test after training batch: 9500
2026-01-08 13:02:45,376: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:02:50,249: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:02:50,305: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:03:01,577: Val batch 9500: PER (avg): 0.1731 CTC Loss (avg): 17.1995 WER(5gram): 16.30% (n=256) time: 16.344
2026-01-08 13:03:01,578: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 13:03:01,580: t15.2023.08.13 val PER: 0.1320
2026-01-08 13:03:01,581: t15.2023.08.18 val PER: 0.1241
2026-01-08 13:03:01,583: t15.2023.08.20 val PER: 0.1295
2026-01-08 13:03:01,584: t15.2023.08.25 val PER: 0.1009
2026-01-08 13:03:01,586: t15.2023.08.27 val PER: 0.2058
2026-01-08 13:03:01,587: t15.2023.09.01 val PER: 0.0950
2026-01-08 13:03:01,588: t15.2023.09.03 val PER: 0.1793
2026-01-08 13:03:01,590: t15.2023.09.24 val PER: 0.1505
2026-01-08 13:03:01,591: t15.2023.09.29 val PER: 0.1423
2026-01-08 13:03:01,592: t15.2023.10.01 val PER: 0.1902
2026-01-08 13:03:01,594: t15.2023.10.06 val PER: 0.1098
2026-01-08 13:03:01,595: t15.2023.10.08 val PER: 0.2544
2026-01-08 13:03:01,596: t15.2023.10.13 val PER: 0.2273
2026-01-08 13:03:01,598: t15.2023.10.15 val PER: 0.1846
2026-01-08 13:03:01,599: t15.2023.10.20 val PER: 0.1846
2026-01-08 13:03:01,600: t15.2023.10.22 val PER: 0.1347
2026-01-08 13:03:01,605: t15.2023.11.03 val PER: 0.1981
2026-01-08 13:03:01,607: t15.2023.11.04 val PER: 0.0341
2026-01-08 13:03:01,608: t15.2023.11.17 val PER: 0.0575
2026-01-08 13:03:01,609: t15.2023.11.19 val PER: 0.0459
2026-01-08 13:03:01,611: t15.2023.11.26 val PER: 0.1529
2026-01-08 13:03:01,612: t15.2023.12.03 val PER: 0.1334
2026-01-08 13:03:01,613: t15.2023.12.08 val PER: 0.1385
2026-01-08 13:03:01,615: t15.2023.12.10 val PER: 0.1170
2026-01-08 13:03:01,617: t15.2023.12.17 val PER: 0.1549
2026-01-08 13:03:01,618: t15.2023.12.29 val PER: 0.1579
2026-01-08 13:03:01,619: t15.2024.02.25 val PER: 0.1306
2026-01-08 13:03:01,621: t15.2024.03.08 val PER: 0.2475
2026-01-08 13:03:01,622: t15.2024.03.15 val PER: 0.2295
2026-01-08 13:03:01,623: t15.2024.03.17 val PER: 0.1709
2026-01-08 13:03:01,624: t15.2024.05.10 val PER: 0.1738
2026-01-08 13:03:01,626: t15.2024.06.14 val PER: 0.1798
2026-01-08 13:03:01,627: t15.2024.07.19 val PER: 0.2676
2026-01-08 13:03:01,628: t15.2024.07.21 val PER: 0.1241
2026-01-08 13:03:01,629: t15.2024.07.28 val PER: 0.1574
2026-01-08 13:03:01,631: t15.2025.01.10 val PER: 0.3182
2026-01-08 13:03:01,632: t15.2025.01.12 val PER: 0.1717
2026-01-08 13:03:01,633: t15.2025.03.14 val PER: 0.3609
2026-01-08 13:03:01,636: t15.2025.03.16 val PER: 0.2120
2026-01-08 13:03:01,637: t15.2025.03.30 val PER: 0.3218
2026-01-08 13:03:01,638: t15.2025.04.13 val PER: 0.2368
2026-01-08 13:03:01,640: New best val WER(5gram) 16.82% --> 16.30%
2026-01-08 13:03:01,641: Checkpointing model
2026-01-08 13:03:01,784: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:03:10,331: Train batch 9600: loss: 8.46 grad norm: 44.83 time: 0.074
2026-01-08 13:03:28,138: Train batch 9800: loss: 12.10 grad norm: 56.84 time: 0.064
2026-01-08 13:03:46,898: Train batch 10000: loss: 5.31 grad norm: 36.13 time: 0.062
2026-01-08 13:03:46,900: Running test after training batch: 10000
2026-01-08 13:03:47,031: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:03:51,914: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:03:51,962: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 13:04:03,081: Val batch 10000: PER (avg): 0.1705 CTC Loss (avg): 16.8214 WER(5gram): 17.86% (n=256) time: 16.179
2026-01-08 13:04:03,083: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=13
2026-01-08 13:04:03,085: t15.2023.08.13 val PER: 0.1258
2026-01-08 13:04:03,087: t15.2023.08.18 val PER: 0.1274
2026-01-08 13:04:03,088: t15.2023.08.20 val PER: 0.1239
2026-01-08 13:04:03,090: t15.2023.08.25 val PER: 0.1054
2026-01-08 13:04:03,092: t15.2023.08.27 val PER: 0.2058
2026-01-08 13:04:03,094: t15.2023.09.01 val PER: 0.0958
2026-01-08 13:04:03,096: t15.2023.09.03 val PER: 0.1841
2026-01-08 13:04:03,097: t15.2023.09.24 val PER: 0.1493
2026-01-08 13:04:03,098: t15.2023.09.29 val PER: 0.1468
2026-01-08 13:04:03,100: t15.2023.10.01 val PER: 0.1816
2026-01-08 13:04:03,101: t15.2023.10.06 val PER: 0.1109
2026-01-08 13:04:03,103: t15.2023.10.08 val PER: 0.2558
2026-01-08 13:04:03,104: t15.2023.10.13 val PER: 0.2242
2026-01-08 13:04:03,106: t15.2023.10.15 val PER: 0.1793
2026-01-08 13:04:03,107: t15.2023.10.20 val PER: 0.1846
2026-01-08 13:04:03,109: t15.2023.10.22 val PER: 0.1325
2026-01-08 13:04:03,110: t15.2023.11.03 val PER: 0.1906
2026-01-08 13:04:03,112: t15.2023.11.04 val PER: 0.0375
2026-01-08 13:04:03,114: t15.2023.11.17 val PER: 0.0560
2026-01-08 13:04:03,116: t15.2023.11.19 val PER: 0.0499
2026-01-08 13:04:03,117: t15.2023.11.26 val PER: 0.1442
2026-01-08 13:04:03,119: t15.2023.12.03 val PER: 0.1376
2026-01-08 13:04:03,120: t15.2023.12.08 val PER: 0.1378
2026-01-08 13:04:03,121: t15.2023.12.10 val PER: 0.1183
2026-01-08 13:04:03,123: t15.2023.12.17 val PER: 0.1507
2026-01-08 13:04:03,124: t15.2023.12.29 val PER: 0.1510
2026-01-08 13:04:03,125: t15.2024.02.25 val PER: 0.1320
2026-01-08 13:04:03,127: t15.2024.03.08 val PER: 0.2319
2026-01-08 13:04:03,129: t15.2024.03.15 val PER: 0.2226
2026-01-08 13:04:03,130: t15.2024.03.17 val PER: 0.1681
2026-01-08 13:04:03,132: t15.2024.05.10 val PER: 0.1768
2026-01-08 13:04:03,134: t15.2024.06.14 val PER: 0.1751
2026-01-08 13:04:03,135: t15.2024.07.19 val PER: 0.2769
2026-01-08 13:04:03,136: t15.2024.07.21 val PER: 0.1138
2026-01-08 13:04:03,138: t15.2024.07.28 val PER: 0.1544
2026-01-08 13:04:03,139: t15.2025.01.10 val PER: 0.3017
2026-01-08 13:04:03,141: t15.2025.01.12 val PER: 0.1732
2026-01-08 13:04:03,143: t15.2025.03.14 val PER: 0.3476
2026-01-08 13:04:03,144: t15.2025.03.16 val PER: 0.2134
2026-01-08 13:04:03,145: t15.2025.03.30 val PER: 0.3126
2026-01-08 13:04:03,147: t15.2025.04.13 val PER: 0.2354
2026-01-08 13:04:20,847: Train batch 10200: loss: 6.23 grad norm: 37.22 time: 0.049
2026-01-08 13:04:38,814: Train batch 10400: loss: 9.12 grad norm: 53.20 time: 0.073
2026-01-08 13:04:47,813: Running test after training batch: 10500
2026-01-08 13:04:47,944: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:04:52,961: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:04:53,004: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:05:04,001: Val batch 10500: PER (avg): 0.1645 CTC Loss (avg): 16.5679 WER(5gram): 17.08% (n=256) time: 16.185
2026-01-08 13:05:04,003: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-08 13:05:04,005: t15.2023.08.13 val PER: 0.1237
2026-01-08 13:05:04,007: t15.2023.08.18 val PER: 0.1157
2026-01-08 13:05:04,008: t15.2023.08.20 val PER: 0.1279
2026-01-08 13:05:04,010: t15.2023.08.25 val PER: 0.0949
2026-01-08 13:05:04,011: t15.2023.08.27 val PER: 0.2026
2026-01-08 13:05:04,013: t15.2023.09.01 val PER: 0.0950
2026-01-08 13:05:04,014: t15.2023.09.03 val PER: 0.1793
2026-01-08 13:05:04,015: t15.2023.09.24 val PER: 0.1481
2026-01-08 13:05:04,017: t15.2023.09.29 val PER: 0.1449
2026-01-08 13:05:04,018: t15.2023.10.01 val PER: 0.1836
2026-01-08 13:05:04,019: t15.2023.10.06 val PER: 0.0980
2026-01-08 13:05:04,021: t15.2023.10.08 val PER: 0.2490
2026-01-08 13:05:04,022: t15.2023.10.13 val PER: 0.2133
2026-01-08 13:05:04,023: t15.2023.10.15 val PER: 0.1760
2026-01-08 13:05:04,025: t15.2023.10.20 val PER: 0.1980
2026-01-08 13:05:04,028: t15.2023.10.22 val PER: 0.1203
2026-01-08 13:05:04,029: t15.2023.11.03 val PER: 0.1954
2026-01-08 13:05:04,030: t15.2023.11.04 val PER: 0.0307
2026-01-08 13:05:04,032: t15.2023.11.17 val PER: 0.0529
2026-01-08 13:05:04,033: t15.2023.11.19 val PER: 0.0499
2026-01-08 13:05:04,035: t15.2023.11.26 val PER: 0.1391
2026-01-08 13:05:04,036: t15.2023.12.03 val PER: 0.1303
2026-01-08 13:05:04,037: t15.2023.12.08 val PER: 0.1165
2026-01-08 13:05:04,039: t15.2023.12.10 val PER: 0.1064
2026-01-08 13:05:04,040: t15.2023.12.17 val PER: 0.1372
2026-01-08 13:05:04,041: t15.2023.12.29 val PER: 0.1572
2026-01-08 13:05:04,043: t15.2024.02.25 val PER: 0.1250
2026-01-08 13:05:04,045: t15.2024.03.08 val PER: 0.2418
2026-01-08 13:05:04,046: t15.2024.03.15 val PER: 0.2170
2026-01-08 13:05:04,048: t15.2024.03.17 val PER: 0.1541
2026-01-08 13:05:04,049: t15.2024.05.10 val PER: 0.1664
2026-01-08 13:05:04,051: t15.2024.06.14 val PER: 0.1719
2026-01-08 13:05:04,052: t15.2024.07.19 val PER: 0.2577
2026-01-08 13:05:04,054: t15.2024.07.21 val PER: 0.1034
2026-01-08 13:05:04,055: t15.2024.07.28 val PER: 0.1346
2026-01-08 13:05:04,056: t15.2025.01.10 val PER: 0.3168
2026-01-08 13:05:04,058: t15.2025.01.12 val PER: 0.1594
2026-01-08 13:05:04,061: t15.2025.03.14 val PER: 0.3565
2026-01-08 13:05:04,063: t15.2025.03.16 val PER: 0.1937
2026-01-08 13:05:04,064: t15.2025.03.30 val PER: 0.3161
2026-01-08 13:05:04,065: t15.2025.04.13 val PER: 0.2325
2026-01-08 13:05:13,237: Train batch 10600: loss: 9.89 grad norm: 59.21 time: 0.073
2026-01-08 13:05:30,797: Train batch 10800: loss: 14.62 grad norm: 63.13 time: 0.065
2026-01-08 13:05:48,656: Train batch 11000: loss: 14.32 grad norm: 64.39 time: 0.058
2026-01-08 13:05:48,658: Running test after training batch: 11000
2026-01-08 13:05:48,757: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:05:53,589: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:05:53,647: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-08 13:06:04,644: Val batch 11000: PER (avg): 0.1649 CTC Loss (avg): 16.5385 WER(5gram): 16.30% (n=256) time: 15.984
2026-01-08 13:06:04,646: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 13:06:04,648: t15.2023.08.13 val PER: 0.1237
2026-01-08 13:06:04,650: t15.2023.08.18 val PER: 0.1232
2026-01-08 13:06:04,651: t15.2023.08.20 val PER: 0.1215
2026-01-08 13:06:04,653: t15.2023.08.25 val PER: 0.0979
2026-01-08 13:06:04,655: t15.2023.08.27 val PER: 0.1945
2026-01-08 13:06:04,657: t15.2023.09.01 val PER: 0.0820
2026-01-08 13:06:04,658: t15.2023.09.03 val PER: 0.1734
2026-01-08 13:06:04,660: t15.2023.09.24 val PER: 0.1432
2026-01-08 13:06:04,661: t15.2023.09.29 val PER: 0.1404
2026-01-08 13:06:04,662: t15.2023.10.01 val PER: 0.1948
2026-01-08 13:06:04,664: t15.2023.10.06 val PER: 0.0926
2026-01-08 13:06:04,665: t15.2023.10.08 val PER: 0.2585
2026-01-08 13:06:04,667: t15.2023.10.13 val PER: 0.2172
2026-01-08 13:06:04,668: t15.2023.10.15 val PER: 0.1727
2026-01-08 13:06:04,669: t15.2023.10.20 val PER: 0.2047
2026-01-08 13:06:04,671: t15.2023.10.22 val PER: 0.1247
2026-01-08 13:06:04,672: t15.2023.11.03 val PER: 0.2015
2026-01-08 13:06:04,673: t15.2023.11.04 val PER: 0.0307
2026-01-08 13:06:04,675: t15.2023.11.17 val PER: 0.0451
2026-01-08 13:06:04,676: t15.2023.11.19 val PER: 0.0459
2026-01-08 13:06:04,678: t15.2023.11.26 val PER: 0.1370
2026-01-08 13:06:04,679: t15.2023.12.03 val PER: 0.1303
2026-01-08 13:06:04,682: t15.2023.12.08 val PER: 0.1185
2026-01-08 13:06:04,683: t15.2023.12.10 val PER: 0.0959
2026-01-08 13:06:04,685: t15.2023.12.17 val PER: 0.1466
2026-01-08 13:06:04,686: t15.2023.12.29 val PER: 0.1366
2026-01-08 13:06:04,688: t15.2024.02.25 val PER: 0.1306
2026-01-08 13:06:04,689: t15.2024.03.08 val PER: 0.2504
2026-01-08 13:06:04,690: t15.2024.03.15 val PER: 0.2214
2026-01-08 13:06:04,692: t15.2024.03.17 val PER: 0.1646
2026-01-08 13:06:04,693: t15.2024.05.10 val PER: 0.1813
2026-01-08 13:06:04,694: t15.2024.06.14 val PER: 0.1656
2026-01-08 13:06:04,696: t15.2024.07.19 val PER: 0.2591
2026-01-08 13:06:04,697: t15.2024.07.21 val PER: 0.1069
2026-01-08 13:06:04,698: t15.2024.07.28 val PER: 0.1463
2026-01-08 13:06:04,700: t15.2025.01.10 val PER: 0.3140
2026-01-08 13:06:04,701: t15.2025.01.12 val PER: 0.1655
2026-01-08 13:06:04,704: t15.2025.03.14 val PER: 0.3565
2026-01-08 13:06:04,705: t15.2025.03.16 val PER: 0.2003
2026-01-08 13:06:04,707: t15.2025.03.30 val PER: 0.2977
2026-01-08 13:06:04,708: t15.2025.04.13 val PER: 0.2240
2026-01-08 13:06:22,027: Train batch 11200: loss: 10.54 grad norm: 56.68 time: 0.075
2026-01-08 13:06:39,500: Train batch 11400: loss: 9.63 grad norm: 52.68 time: 0.058
2026-01-08 13:06:48,546: Running test after training batch: 11500
2026-01-08 13:06:48,646: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:06:53,475: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:06:53,527: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:07:04,608: Val batch 11500: PER (avg): 0.1619 CTC Loss (avg): 16.3730 WER(5gram): 16.43% (n=256) time: 16.059
2026-01-08 13:07:04,610: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-08 13:07:04,613: t15.2023.08.13 val PER: 0.1164
2026-01-08 13:07:04,615: t15.2023.08.18 val PER: 0.1157
2026-01-08 13:07:04,617: t15.2023.08.20 val PER: 0.1191
2026-01-08 13:07:04,619: t15.2023.08.25 val PER: 0.1009
2026-01-08 13:07:04,621: t15.2023.08.27 val PER: 0.1977
2026-01-08 13:07:04,623: t15.2023.09.01 val PER: 0.0828
2026-01-08 13:07:04,625: t15.2023.09.03 val PER: 0.1698
2026-01-08 13:07:04,627: t15.2023.09.24 val PER: 0.1299
2026-01-08 13:07:04,629: t15.2023.09.29 val PER: 0.1372
2026-01-08 13:07:04,630: t15.2023.10.01 val PER: 0.1856
2026-01-08 13:07:04,632: t15.2023.10.06 val PER: 0.0861
2026-01-08 13:07:04,635: t15.2023.10.08 val PER: 0.2585
2026-01-08 13:07:04,637: t15.2023.10.13 val PER: 0.2180
2026-01-08 13:07:04,641: t15.2023.10.15 val PER: 0.1668
2026-01-08 13:07:04,643: t15.2023.10.20 val PER: 0.1913
2026-01-08 13:07:04,645: t15.2023.10.22 val PER: 0.1225
2026-01-08 13:07:04,647: t15.2023.11.03 val PER: 0.1872
2026-01-08 13:07:04,649: t15.2023.11.04 val PER: 0.0341
2026-01-08 13:07:04,650: t15.2023.11.17 val PER: 0.0467
2026-01-08 13:07:04,652: t15.2023.11.19 val PER: 0.0399
2026-01-08 13:07:04,654: t15.2023.11.26 val PER: 0.1326
2026-01-08 13:07:04,655: t15.2023.12.03 val PER: 0.1261
2026-01-08 13:07:04,657: t15.2023.12.08 val PER: 0.1145
2026-01-08 13:07:04,659: t15.2023.12.10 val PER: 0.1025
2026-01-08 13:07:04,661: t15.2023.12.17 val PER: 0.1507
2026-01-08 13:07:04,662: t15.2023.12.29 val PER: 0.1421
2026-01-08 13:07:04,664: t15.2024.02.25 val PER: 0.1208
2026-01-08 13:07:04,665: t15.2024.03.08 val PER: 0.2390
2026-01-08 13:07:04,667: t15.2024.03.15 val PER: 0.2189
2026-01-08 13:07:04,669: t15.2024.03.17 val PER: 0.1611
2026-01-08 13:07:04,670: t15.2024.05.10 val PER: 0.1709
2026-01-08 13:07:04,672: t15.2024.06.14 val PER: 0.1735
2026-01-08 13:07:04,674: t15.2024.07.19 val PER: 0.2564
2026-01-08 13:07:04,675: t15.2024.07.21 val PER: 0.1000
2026-01-08 13:07:04,677: t15.2024.07.28 val PER: 0.1485
2026-01-08 13:07:04,679: t15.2025.01.10 val PER: 0.3196
2026-01-08 13:07:04,680: t15.2025.01.12 val PER: 0.1609
2026-01-08 13:07:04,682: t15.2025.03.14 val PER: 0.3432
2026-01-08 13:07:04,683: t15.2025.03.16 val PER: 0.2042
2026-01-08 13:07:04,685: t15.2025.03.30 val PER: 0.3069
2026-01-08 13:07:04,687: t15.2025.04.13 val PER: 0.2282
2026-01-08 13:07:13,217: Train batch 11600: loss: 10.57 grad norm: 46.05 time: 0.062
2026-01-08 13:07:30,697: Train batch 11800: loss: 6.94 grad norm: 43.58 time: 0.045
2026-01-08 13:07:47,944: Train batch 12000: loss: 13.71 grad norm: 59.54 time: 0.071
2026-01-08 13:07:47,946: Running test after training batch: 12000
2026-01-08 13:07:48,045: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:07:53,205: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:07:53,256: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-08 13:08:04,331: Val batch 12000: PER (avg): 0.1592 CTC Loss (avg): 16.1930 WER(5gram): 16.69% (n=256) time: 16.382
2026-01-08 13:08:04,333: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-08 13:08:04,335: t15.2023.08.13 val PER: 0.1206
2026-01-08 13:08:04,337: t15.2023.08.18 val PER: 0.1232
2026-01-08 13:08:04,338: t15.2023.08.20 val PER: 0.1144
2026-01-08 13:08:04,340: t15.2023.08.25 val PER: 0.1054
2026-01-08 13:08:04,341: t15.2023.08.27 val PER: 0.1913
2026-01-08 13:08:04,342: t15.2023.09.01 val PER: 0.0885
2026-01-08 13:08:04,344: t15.2023.09.03 val PER: 0.1556
2026-01-08 13:08:04,345: t15.2023.09.24 val PER: 0.1250
2026-01-08 13:08:04,346: t15.2023.09.29 val PER: 0.1353
2026-01-08 13:08:04,348: t15.2023.10.01 val PER: 0.1764
2026-01-08 13:08:04,349: t15.2023.10.06 val PER: 0.0861
2026-01-08 13:08:04,351: t15.2023.10.08 val PER: 0.2517
2026-01-08 13:08:04,352: t15.2023.10.13 val PER: 0.2102
2026-01-08 13:08:04,353: t15.2023.10.15 val PER: 0.1628
2026-01-08 13:08:04,355: t15.2023.10.20 val PER: 0.1879
2026-01-08 13:08:04,357: t15.2023.10.22 val PER: 0.1158
2026-01-08 13:08:04,358: t15.2023.11.03 val PER: 0.1886
2026-01-08 13:08:04,359: t15.2023.11.04 val PER: 0.0307
2026-01-08 13:08:04,361: t15.2023.11.17 val PER: 0.0389
2026-01-08 13:08:04,362: t15.2023.11.19 val PER: 0.0399
2026-01-08 13:08:04,364: t15.2023.11.26 val PER: 0.1333
2026-01-08 13:08:04,366: t15.2023.12.03 val PER: 0.1176
2026-01-08 13:08:04,367: t15.2023.12.08 val PER: 0.1039
2026-01-08 13:08:04,369: t15.2023.12.10 val PER: 0.0933
2026-01-08 13:08:04,370: t15.2023.12.17 val PER: 0.1528
2026-01-08 13:08:04,372: t15.2023.12.29 val PER: 0.1448
2026-01-08 13:08:04,373: t15.2024.02.25 val PER: 0.1166
2026-01-08 13:08:04,374: t15.2024.03.08 val PER: 0.2532
2026-01-08 13:08:04,376: t15.2024.03.15 val PER: 0.2095
2026-01-08 13:08:04,377: t15.2024.03.17 val PER: 0.1485
2026-01-08 13:08:04,378: t15.2024.05.10 val PER: 0.1813
2026-01-08 13:08:04,380: t15.2024.06.14 val PER: 0.1893
2026-01-08 13:08:04,382: t15.2024.07.19 val PER: 0.2571
2026-01-08 13:08:04,383: t15.2024.07.21 val PER: 0.0993
2026-01-08 13:08:04,384: t15.2024.07.28 val PER: 0.1426
2026-01-08 13:08:04,386: t15.2025.01.10 val PER: 0.3072
2026-01-08 13:08:04,387: t15.2025.01.12 val PER: 0.1578
2026-01-08 13:08:04,390: t15.2025.03.14 val PER: 0.3491
2026-01-08 13:08:04,392: t15.2025.03.16 val PER: 0.1963
2026-01-08 13:08:04,393: t15.2025.03.30 val PER: 0.3080
2026-01-08 13:08:04,394: t15.2025.04.13 val PER: 0.2211
2026-01-08 13:08:21,626: Train batch 12200: loss: 5.47 grad norm: 40.32 time: 0.065
2026-01-08 13:08:39,148: Train batch 12400: loss: 4.82 grad norm: 34.79 time: 0.041
2026-01-08 13:08:47,980: Running test after training batch: 12500
2026-01-08 13:08:48,118: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:08:53,079: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:08:53,123: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:09:04,298: Val batch 12500: PER (avg): 0.1565 CTC Loss (avg): 15.9529 WER(5gram): 15.91% (n=256) time: 16.316
2026-01-08 13:09:04,300: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 13:09:04,302: t15.2023.08.13 val PER: 0.1143
2026-01-08 13:09:04,304: t15.2023.08.18 val PER: 0.1132
2026-01-08 13:09:04,305: t15.2023.08.20 val PER: 0.1104
2026-01-08 13:09:04,307: t15.2023.08.25 val PER: 0.0889
2026-01-08 13:09:04,308: t15.2023.08.27 val PER: 0.1977
2026-01-08 13:09:04,309: t15.2023.09.01 val PER: 0.0812
2026-01-08 13:09:04,311: t15.2023.09.03 val PER: 0.1544
2026-01-08 13:09:04,312: t15.2023.09.24 val PER: 0.1274
2026-01-08 13:09:04,315: t15.2023.09.29 val PER: 0.1359
2026-01-08 13:09:04,318: t15.2023.10.01 val PER: 0.1684
2026-01-08 13:09:04,320: t15.2023.10.06 val PER: 0.0840
2026-01-08 13:09:04,321: t15.2023.10.08 val PER: 0.2558
2026-01-08 13:09:04,323: t15.2023.10.13 val PER: 0.2056
2026-01-08 13:09:04,324: t15.2023.10.15 val PER: 0.1569
2026-01-08 13:09:04,326: t15.2023.10.20 val PER: 0.2047
2026-01-08 13:09:04,327: t15.2023.10.22 val PER: 0.1180
2026-01-08 13:09:04,328: t15.2023.11.03 val PER: 0.1886
2026-01-08 13:09:04,330: t15.2023.11.04 val PER: 0.0341
2026-01-08 13:09:04,331: t15.2023.11.17 val PER: 0.0498
2026-01-08 13:09:04,333: t15.2023.11.19 val PER: 0.0339
2026-01-08 13:09:04,334: t15.2023.11.26 val PER: 0.1246
2026-01-08 13:09:04,336: t15.2023.12.03 val PER: 0.1239
2026-01-08 13:09:04,337: t15.2023.12.08 val PER: 0.1045
2026-01-08 13:09:04,338: t15.2023.12.10 val PER: 0.0946
2026-01-08 13:09:04,340: t15.2023.12.17 val PER: 0.1497
2026-01-08 13:09:04,341: t15.2023.12.29 val PER: 0.1448
2026-01-08 13:09:04,342: t15.2024.02.25 val PER: 0.1138
2026-01-08 13:09:04,344: t15.2024.03.08 val PER: 0.2390
2026-01-08 13:09:04,345: t15.2024.03.15 val PER: 0.2158
2026-01-08 13:09:04,347: t15.2024.03.17 val PER: 0.1562
2026-01-08 13:09:04,348: t15.2024.05.10 val PER: 0.1738
2026-01-08 13:09:04,349: t15.2024.06.14 val PER: 0.1735
2026-01-08 13:09:04,351: t15.2024.07.19 val PER: 0.2386
2026-01-08 13:09:04,352: t15.2024.07.21 val PER: 0.1028
2026-01-08 13:09:04,353: t15.2024.07.28 val PER: 0.1375
2026-01-08 13:09:04,355: t15.2025.01.10 val PER: 0.2934
2026-01-08 13:09:04,356: t15.2025.01.12 val PER: 0.1586
2026-01-08 13:09:04,357: t15.2025.03.14 val PER: 0.3506
2026-01-08 13:09:04,360: t15.2025.03.16 val PER: 0.2042
2026-01-08 13:09:04,361: t15.2025.03.30 val PER: 0.2908
2026-01-08 13:09:04,363: t15.2025.04.13 val PER: 0.2197
2026-01-08 13:09:04,365: New best val WER(5gram) 16.30% --> 15.91%
2026-01-08 13:09:04,366: Checkpointing model
2026-01-08 13:09:04,505: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:09:13,184: Train batch 12600: loss: 7.78 grad norm: 46.87 time: 0.058
2026-01-08 13:09:31,306: Train batch 12800: loss: 5.77 grad norm: 37.50 time: 0.053
2026-01-08 13:09:50,138: Train batch 13000: loss: 6.41 grad norm: 43.79 time: 0.067
2026-01-08 13:09:50,140: Running test after training batch: 13000
2026-01-08 13:09:50,257: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:09:55,115: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:09:55,160: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 13:10:06,421: Val batch 13000: PER (avg): 0.1562 CTC Loss (avg): 15.8157 WER(5gram): 15.25% (n=256) time: 16.279
2026-01-08 13:10:06,423: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=13
2026-01-08 13:10:06,425: t15.2023.08.13 val PER: 0.1175
2026-01-08 13:10:06,427: t15.2023.08.18 val PER: 0.1174
2026-01-08 13:10:06,428: t15.2023.08.20 val PER: 0.1104
2026-01-08 13:10:06,429: t15.2023.08.25 val PER: 0.0979
2026-01-08 13:10:06,433: t15.2023.08.27 val PER: 0.1897
2026-01-08 13:10:06,435: t15.2023.09.01 val PER: 0.0860
2026-01-08 13:10:06,436: t15.2023.09.03 val PER: 0.1627
2026-01-08 13:10:06,438: t15.2023.09.24 val PER: 0.1311
2026-01-08 13:10:06,440: t15.2023.09.29 val PER: 0.1334
2026-01-08 13:10:06,441: t15.2023.10.01 val PER: 0.1744
2026-01-08 13:10:06,443: t15.2023.10.06 val PER: 0.0904
2026-01-08 13:10:06,444: t15.2023.10.08 val PER: 0.2571
2026-01-08 13:10:06,446: t15.2023.10.13 val PER: 0.2017
2026-01-08 13:10:06,447: t15.2023.10.15 val PER: 0.1595
2026-01-08 13:10:06,449: t15.2023.10.20 val PER: 0.1711
2026-01-08 13:10:06,451: t15.2023.10.22 val PER: 0.1158
2026-01-08 13:10:06,452: t15.2023.11.03 val PER: 0.1893
2026-01-08 13:10:06,454: t15.2023.11.04 val PER: 0.0375
2026-01-08 13:10:06,455: t15.2023.11.17 val PER: 0.0435
2026-01-08 13:10:06,457: t15.2023.11.19 val PER: 0.0359
2026-01-08 13:10:06,458: t15.2023.11.26 val PER: 0.1239
2026-01-08 13:10:06,460: t15.2023.12.03 val PER: 0.1261
2026-01-08 13:10:06,461: t15.2023.12.08 val PER: 0.1099
2026-01-08 13:10:06,463: t15.2023.12.10 val PER: 0.0999
2026-01-08 13:10:06,464: t15.2023.12.17 val PER: 0.1393
2026-01-08 13:10:06,466: t15.2023.12.29 val PER: 0.1462
2026-01-08 13:10:06,467: t15.2024.02.25 val PER: 0.1152
2026-01-08 13:10:06,469: t15.2024.03.08 val PER: 0.2404
2026-01-08 13:10:06,470: t15.2024.03.15 val PER: 0.2051
2026-01-08 13:10:06,472: t15.2024.03.17 val PER: 0.1499
2026-01-08 13:10:06,473: t15.2024.05.10 val PER: 0.1664
2026-01-08 13:10:06,475: t15.2024.06.14 val PER: 0.1751
2026-01-08 13:10:06,476: t15.2024.07.19 val PER: 0.2485
2026-01-08 13:10:06,478: t15.2024.07.21 val PER: 0.0972
2026-01-08 13:10:06,479: t15.2024.07.28 val PER: 0.1456
2026-01-08 13:10:06,482: t15.2025.01.10 val PER: 0.2824
2026-01-08 13:10:06,484: t15.2025.01.12 val PER: 0.1440
2026-01-08 13:10:06,485: t15.2025.03.14 val PER: 0.3462
2026-01-08 13:10:06,487: t15.2025.03.16 val PER: 0.1846
2026-01-08 13:10:06,488: t15.2025.03.30 val PER: 0.3092
2026-01-08 13:10:06,490: t15.2025.04.13 val PER: 0.2168
2026-01-08 13:10:06,491: New best val WER(5gram) 15.91% --> 15.25%
2026-01-08 13:10:06,493: Checkpointing model
2026-01-08 13:10:06,636: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:10:25,113: Train batch 13200: loss: 12.14 grad norm: 59.22 time: 0.056
2026-01-08 13:10:43,504: Train batch 13400: loss: 8.67 grad norm: 49.90 time: 0.066
2026-01-08 13:10:52,763: Running test after training batch: 13500
2026-01-08 13:10:52,881: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:10:57,598: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:10:57,643: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:11:08,676: Val batch 13500: PER (avg): 0.1531 CTC Loss (avg): 15.6581 WER(5gram): 15.84% (n=256) time: 15.911
2026-01-08 13:11:08,679: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 13:11:08,681: t15.2023.08.13 val PER: 0.1154
2026-01-08 13:11:08,683: t15.2023.08.18 val PER: 0.1115
2026-01-08 13:11:08,684: t15.2023.08.20 val PER: 0.1088
2026-01-08 13:11:08,686: t15.2023.08.25 val PER: 0.0934
2026-01-08 13:11:08,687: t15.2023.08.27 val PER: 0.1865
2026-01-08 13:11:08,689: t15.2023.09.01 val PER: 0.0820
2026-01-08 13:11:08,690: t15.2023.09.03 val PER: 0.1663
2026-01-08 13:11:08,691: t15.2023.09.24 val PER: 0.1286
2026-01-08 13:11:08,693: t15.2023.09.29 val PER: 0.1327
2026-01-08 13:11:08,694: t15.2023.10.01 val PER: 0.1744
2026-01-08 13:11:08,696: t15.2023.10.06 val PER: 0.0883
2026-01-08 13:11:08,697: t15.2023.10.08 val PER: 0.2571
2026-01-08 13:11:08,699: t15.2023.10.13 val PER: 0.2064
2026-01-08 13:11:08,700: t15.2023.10.15 val PER: 0.1569
2026-01-08 13:11:08,701: t15.2023.10.20 val PER: 0.1879
2026-01-08 13:11:08,703: t15.2023.10.22 val PER: 0.1147
2026-01-08 13:11:08,704: t15.2023.11.03 val PER: 0.1859
2026-01-08 13:11:08,706: t15.2023.11.04 val PER: 0.0375
2026-01-08 13:11:08,707: t15.2023.11.17 val PER: 0.0498
2026-01-08 13:11:08,708: t15.2023.11.19 val PER: 0.0259
2026-01-08 13:11:08,710: t15.2023.11.26 val PER: 0.1232
2026-01-08 13:11:08,711: t15.2023.12.03 val PER: 0.1113
2026-01-08 13:11:08,713: t15.2023.12.08 val PER: 0.1059
2026-01-08 13:11:08,714: t15.2023.12.10 val PER: 0.0959
2026-01-08 13:11:08,716: t15.2023.12.17 val PER: 0.1279
2026-01-08 13:11:08,717: t15.2023.12.29 val PER: 0.1325
2026-01-08 13:11:08,719: t15.2024.02.25 val PER: 0.0997
2026-01-08 13:11:08,720: t15.2024.03.08 val PER: 0.2404
2026-01-08 13:11:08,722: t15.2024.03.15 val PER: 0.2020
2026-01-08 13:11:08,723: t15.2024.03.17 val PER: 0.1534
2026-01-08 13:11:08,724: t15.2024.05.10 val PER: 0.1694
2026-01-08 13:11:08,726: t15.2024.06.14 val PER: 0.1577
2026-01-08 13:11:08,727: t15.2024.07.19 val PER: 0.2386
2026-01-08 13:11:08,729: t15.2024.07.21 val PER: 0.0897
2026-01-08 13:11:08,730: t15.2024.07.28 val PER: 0.1353
2026-01-08 13:11:08,731: t15.2025.01.10 val PER: 0.2948
2026-01-08 13:11:08,733: t15.2025.01.12 val PER: 0.1470
2026-01-08 13:11:08,734: t15.2025.03.14 val PER: 0.3417
2026-01-08 13:11:08,735: t15.2025.03.16 val PER: 0.1898
2026-01-08 13:11:08,737: t15.2025.03.30 val PER: 0.3023
2026-01-08 13:11:08,738: t15.2025.04.13 val PER: 0.2183
2026-01-08 13:11:18,077: Train batch 13600: loss: 12.76 grad norm: 65.02 time: 0.065
2026-01-08 13:11:36,821: Train batch 13800: loss: 8.57 grad norm: 55.59 time: 0.057
2026-01-08 13:11:55,507: Train batch 14000: loss: 11.72 grad norm: 58.32 time: 0.051
2026-01-08 13:11:55,509: Running test after training batch: 14000
2026-01-08 13:11:55,627: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:12:00,387: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:12:00,433: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:12:11,912: Val batch 14000: PER (avg): 0.1513 CTC Loss (avg): 15.5273 WER(5gram): 14.80% (n=256) time: 16.400
2026-01-08 13:12:11,914: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 13:12:11,916: t15.2023.08.13 val PER: 0.1071
2026-01-08 13:12:11,917: t15.2023.08.18 val PER: 0.1056
2026-01-08 13:12:11,919: t15.2023.08.20 val PER: 0.1096
2026-01-08 13:12:11,920: t15.2023.08.25 val PER: 0.0994
2026-01-08 13:12:11,921: t15.2023.08.27 val PER: 0.1833
2026-01-08 13:12:11,923: t15.2023.09.01 val PER: 0.0747
2026-01-08 13:12:11,924: t15.2023.09.03 val PER: 0.1746
2026-01-08 13:12:11,925: t15.2023.09.24 val PER: 0.1262
2026-01-08 13:12:11,927: t15.2023.09.29 val PER: 0.1295
2026-01-08 13:12:11,928: t15.2023.10.01 val PER: 0.1737
2026-01-08 13:12:11,930: t15.2023.10.06 val PER: 0.0829
2026-01-08 13:12:11,931: t15.2023.10.08 val PER: 0.2585
2026-01-08 13:12:11,933: t15.2023.10.13 val PER: 0.2017
2026-01-08 13:12:11,935: t15.2023.10.15 val PER: 0.1516
2026-01-08 13:12:11,936: t15.2023.10.20 val PER: 0.2013
2026-01-08 13:12:11,938: t15.2023.10.22 val PER: 0.1058
2026-01-08 13:12:11,939: t15.2023.11.03 val PER: 0.1791
2026-01-08 13:12:11,940: t15.2023.11.04 val PER: 0.0341
2026-01-08 13:12:11,942: t15.2023.11.17 val PER: 0.0404
2026-01-08 13:12:11,943: t15.2023.11.19 val PER: 0.0339
2026-01-08 13:12:11,944: t15.2023.11.26 val PER: 0.1304
2026-01-08 13:12:11,946: t15.2023.12.03 val PER: 0.1187
2026-01-08 13:12:11,947: t15.2023.12.08 val PER: 0.1052
2026-01-08 13:12:11,949: t15.2023.12.10 val PER: 0.0959
2026-01-08 13:12:11,950: t15.2023.12.17 val PER: 0.1247
2026-01-08 13:12:11,952: t15.2023.12.29 val PER: 0.1290
2026-01-08 13:12:11,953: t15.2024.02.25 val PER: 0.1096
2026-01-08 13:12:11,955: t15.2024.03.08 val PER: 0.2333
2026-01-08 13:12:11,956: t15.2024.03.15 val PER: 0.2001
2026-01-08 13:12:11,958: t15.2024.03.17 val PER: 0.1527
2026-01-08 13:12:11,959: t15.2024.05.10 val PER: 0.1590
2026-01-08 13:12:11,960: t15.2024.06.14 val PER: 0.1656
2026-01-08 13:12:11,962: t15.2024.07.19 val PER: 0.2353
2026-01-08 13:12:11,964: t15.2024.07.21 val PER: 0.0862
2026-01-08 13:12:11,966: t15.2024.07.28 val PER: 0.1287
2026-01-08 13:12:11,967: t15.2025.01.10 val PER: 0.2975
2026-01-08 13:12:11,968: t15.2025.01.12 val PER: 0.1447
2026-01-08 13:12:11,970: t15.2025.03.14 val PER: 0.3358
2026-01-08 13:12:11,971: t15.2025.03.16 val PER: 0.2003
2026-01-08 13:12:11,972: t15.2025.03.30 val PER: 0.2954
2026-01-08 13:12:11,973: t15.2025.04.13 val PER: 0.2197
2026-01-08 13:12:11,975: New best val WER(5gram) 15.25% --> 14.80%
2026-01-08 13:12:11,977: Checkpointing model
2026-01-08 13:12:12,117: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:12:29,714: Train batch 14200: loss: 8.08 grad norm: 51.69 time: 0.057
2026-01-08 13:12:47,369: Train batch 14400: loss: 5.68 grad norm: 38.55 time: 0.065
2026-01-08 13:12:56,207: Running test after training batch: 14500
2026-01-08 13:12:56,308: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:13:01,078: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:13:01,122: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:13:12,488: Val batch 14500: PER (avg): 0.1508 CTC Loss (avg): 15.5339 WER(5gram): 16.43% (n=256) time: 16.279
2026-01-08 13:13:12,491: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=12
2026-01-08 13:13:12,493: t15.2023.08.13 val PER: 0.1133
2026-01-08 13:13:12,496: t15.2023.08.18 val PER: 0.1048
2026-01-08 13:13:12,498: t15.2023.08.20 val PER: 0.1080
2026-01-08 13:13:12,500: t15.2023.08.25 val PER: 0.0919
2026-01-08 13:13:12,501: t15.2023.08.27 val PER: 0.1849
2026-01-08 13:13:12,503: t15.2023.09.01 val PER: 0.0812
2026-01-08 13:13:12,504: t15.2023.09.03 val PER: 0.1603
2026-01-08 13:13:12,507: t15.2023.09.24 val PER: 0.1335
2026-01-08 13:13:12,508: t15.2023.09.29 val PER: 0.1283
2026-01-08 13:13:12,510: t15.2023.10.01 val PER: 0.1737
2026-01-08 13:13:12,512: t15.2023.10.06 val PER: 0.0807
2026-01-08 13:13:12,513: t15.2023.10.08 val PER: 0.2571
2026-01-08 13:13:12,514: t15.2023.10.13 val PER: 0.2048
2026-01-08 13:13:12,516: t15.2023.10.15 val PER: 0.1569
2026-01-08 13:13:12,517: t15.2023.10.20 val PER: 0.1846
2026-01-08 13:13:12,519: t15.2023.10.22 val PER: 0.1125
2026-01-08 13:13:12,520: t15.2023.11.03 val PER: 0.1832
2026-01-08 13:13:12,521: t15.2023.11.04 val PER: 0.0341
2026-01-08 13:13:12,523: t15.2023.11.17 val PER: 0.0451
2026-01-08 13:13:12,524: t15.2023.11.19 val PER: 0.0339
2026-01-08 13:13:12,525: t15.2023.11.26 val PER: 0.1239
2026-01-08 13:13:12,526: t15.2023.12.03 val PER: 0.1166
2026-01-08 13:13:12,527: t15.2023.12.08 val PER: 0.1019
2026-01-08 13:13:12,528: t15.2023.12.10 val PER: 0.0959
2026-01-08 13:13:12,530: t15.2023.12.17 val PER: 0.1445
2026-01-08 13:13:12,531: t15.2023.12.29 val PER: 0.1318
2026-01-08 13:13:12,532: t15.2024.02.25 val PER: 0.1081
2026-01-08 13:13:12,533: t15.2024.03.08 val PER: 0.2191
2026-01-08 13:13:12,534: t15.2024.03.15 val PER: 0.2051
2026-01-08 13:13:12,535: t15.2024.03.17 val PER: 0.1437
2026-01-08 13:13:12,536: t15.2024.05.10 val PER: 0.1575
2026-01-08 13:13:12,537: t15.2024.06.14 val PER: 0.1562
2026-01-08 13:13:12,538: t15.2024.07.19 val PER: 0.2327
2026-01-08 13:13:12,539: t15.2024.07.21 val PER: 0.0897
2026-01-08 13:13:12,540: t15.2024.07.28 val PER: 0.1368
2026-01-08 13:13:12,542: t15.2025.01.10 val PER: 0.2851
2026-01-08 13:13:12,543: t15.2025.01.12 val PER: 0.1432
2026-01-08 13:13:12,544: t15.2025.03.14 val PER: 0.3373
2026-01-08 13:13:12,545: t15.2025.03.16 val PER: 0.1806
2026-01-08 13:13:12,546: t15.2025.03.30 val PER: 0.2954
2026-01-08 13:13:12,547: t15.2025.04.13 val PER: 0.2068
2026-01-08 13:13:21,321: Train batch 14600: loss: 11.81 grad norm: 57.92 time: 0.058
2026-01-08 13:13:38,911: Train batch 14800: loss: 5.76 grad norm: 44.96 time: 0.051
2026-01-08 13:13:56,867: Train batch 15000: loss: 9.12 grad norm: 44.11 time: 0.052
2026-01-08 13:13:56,869: Running test after training batch: 15000
2026-01-08 13:13:56,993: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:14:01,737: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:14:01,787: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:14:13,019: Val batch 15000: PER (avg): 0.1502 CTC Loss (avg): 15.3669 WER(5gram): 14.60% (n=256) time: 16.147
2026-01-08 13:14:13,021: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=12
2026-01-08 13:14:13,024: t15.2023.08.13 val PER: 0.1050
2026-01-08 13:14:13,026: t15.2023.08.18 val PER: 0.1014
2026-01-08 13:14:13,028: t15.2023.08.20 val PER: 0.1136
2026-01-08 13:14:13,029: t15.2023.08.25 val PER: 0.0979
2026-01-08 13:14:13,031: t15.2023.08.27 val PER: 0.1881
2026-01-08 13:14:13,033: t15.2023.09.01 val PER: 0.0795
2026-01-08 13:14:13,034: t15.2023.09.03 val PER: 0.1544
2026-01-08 13:14:13,036: t15.2023.09.24 val PER: 0.1323
2026-01-08 13:14:13,038: t15.2023.09.29 val PER: 0.1321
2026-01-08 13:14:13,040: t15.2023.10.01 val PER: 0.1731
2026-01-08 13:14:13,041: t15.2023.10.06 val PER: 0.0807
2026-01-08 13:14:13,044: t15.2023.10.08 val PER: 0.2558
2026-01-08 13:14:13,046: t15.2023.10.13 val PER: 0.2033
2026-01-08 13:14:13,048: t15.2023.10.15 val PER: 0.1523
2026-01-08 13:14:13,049: t15.2023.10.20 val PER: 0.2047
2026-01-08 13:14:13,051: t15.2023.10.22 val PER: 0.1080
2026-01-08 13:14:13,053: t15.2023.11.03 val PER: 0.1805
2026-01-08 13:14:13,055: t15.2023.11.04 val PER: 0.0375
2026-01-08 13:14:13,057: t15.2023.11.17 val PER: 0.0311
2026-01-08 13:14:13,059: t15.2023.11.19 val PER: 0.0379
2026-01-08 13:14:13,061: t15.2023.11.26 val PER: 0.1203
2026-01-08 13:14:13,062: t15.2023.12.03 val PER: 0.1229
2026-01-08 13:14:13,064: t15.2023.12.08 val PER: 0.0979
2026-01-08 13:14:13,066: t15.2023.12.10 val PER: 0.0959
2026-01-08 13:14:13,067: t15.2023.12.17 val PER: 0.1341
2026-01-08 13:14:13,069: t15.2023.12.29 val PER: 0.1318
2026-01-08 13:14:13,071: t15.2024.02.25 val PER: 0.1096
2026-01-08 13:14:13,072: t15.2024.03.08 val PER: 0.2333
2026-01-08 13:14:13,074: t15.2024.03.15 val PER: 0.2008
2026-01-08 13:14:13,077: t15.2024.03.17 val PER: 0.1381
2026-01-08 13:14:13,079: t15.2024.05.10 val PER: 0.1590
2026-01-08 13:14:13,081: t15.2024.06.14 val PER: 0.1640
2026-01-08 13:14:13,082: t15.2024.07.19 val PER: 0.2307
2026-01-08 13:14:13,084: t15.2024.07.21 val PER: 0.0883
2026-01-08 13:14:13,086: t15.2024.07.28 val PER: 0.1324
2026-01-08 13:14:13,087: t15.2025.01.10 val PER: 0.2989
2026-01-08 13:14:13,089: t15.2025.01.12 val PER: 0.1447
2026-01-08 13:14:13,091: t15.2025.03.14 val PER: 0.3476
2026-01-08 13:14:13,093: t15.2025.03.16 val PER: 0.1885
2026-01-08 13:14:13,094: t15.2025.03.30 val PER: 0.2862
2026-01-08 13:14:13,096: t15.2025.04.13 val PER: 0.2097
2026-01-08 13:14:13,098: New best val WER(5gram) 14.80% --> 14.60%
2026-01-08 13:14:13,100: Checkpointing model
2026-01-08 13:14:13,247: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:14:30,678: Train batch 15200: loss: 4.73 grad norm: 39.04 time: 0.058
2026-01-08 13:14:48,032: Train batch 15400: loss: 11.50 grad norm: 61.29 time: 0.049
2026-01-08 13:14:56,635: Running test after training batch: 15500
2026-01-08 13:14:56,732: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:15:01,572: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:15:01,623: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost set
2026-01-08 13:15:13,256: Val batch 15500: PER (avg): 0.1502 CTC Loss (avg): 15.2897 WER(5gram): 14.47% (n=256) time: 16.619
2026-01-08 13:15:13,258: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 13:15:13,260: t15.2023.08.13 val PER: 0.1102
2026-01-08 13:15:13,262: t15.2023.08.18 val PER: 0.1031
2026-01-08 13:15:13,263: t15.2023.08.20 val PER: 0.1096
2026-01-08 13:15:13,265: t15.2023.08.25 val PER: 0.1039
2026-01-08 13:15:13,266: t15.2023.08.27 val PER: 0.1801
2026-01-08 13:15:13,267: t15.2023.09.01 val PER: 0.0812
2026-01-08 13:15:13,269: t15.2023.09.03 val PER: 0.1603
2026-01-08 13:15:13,270: t15.2023.09.24 val PER: 0.1274
2026-01-08 13:15:13,271: t15.2023.09.29 val PER: 0.1264
2026-01-08 13:15:13,273: t15.2023.10.01 val PER: 0.1724
2026-01-08 13:15:13,274: t15.2023.10.06 val PER: 0.0807
2026-01-08 13:15:13,275: t15.2023.10.08 val PER: 0.2530
2026-01-08 13:15:13,278: t15.2023.10.13 val PER: 0.2025
2026-01-08 13:15:13,279: t15.2023.10.15 val PER: 0.1510
2026-01-08 13:15:13,280: t15.2023.10.20 val PER: 0.1980
2026-01-08 13:15:13,281: t15.2023.10.22 val PER: 0.1114
2026-01-08 13:15:13,283: t15.2023.11.03 val PER: 0.1750
2026-01-08 13:15:13,284: t15.2023.11.04 val PER: 0.0307
2026-01-08 13:15:13,285: t15.2023.11.17 val PER: 0.0280
2026-01-08 13:15:13,287: t15.2023.11.19 val PER: 0.0339
2026-01-08 13:15:13,288: t15.2023.11.26 val PER: 0.1145
2026-01-08 13:15:13,289: t15.2023.12.03 val PER: 0.1271
2026-01-08 13:15:13,290: t15.2023.12.08 val PER: 0.0979
2026-01-08 13:15:13,292: t15.2023.12.10 val PER: 0.0972
2026-01-08 13:15:13,293: t15.2023.12.17 val PER: 0.1299
2026-01-08 13:15:13,294: t15.2023.12.29 val PER: 0.1290
2026-01-08 13:15:13,295: t15.2024.02.25 val PER: 0.1053
2026-01-08 13:15:13,297: t15.2024.03.08 val PER: 0.2319
2026-01-08 13:15:13,298: t15.2024.03.15 val PER: 0.2020
2026-01-08 13:15:13,299: t15.2024.03.17 val PER: 0.1471
2026-01-08 13:15:13,300: t15.2024.05.10 val PER: 0.1530
2026-01-08 13:15:13,302: t15.2024.06.14 val PER: 0.1609
2026-01-08 13:15:13,303: t15.2024.07.19 val PER: 0.2327
2026-01-08 13:15:13,304: t15.2024.07.21 val PER: 0.0931
2026-01-08 13:15:13,305: t15.2024.07.28 val PER: 0.1382
2026-01-08 13:15:13,307: t15.2025.01.10 val PER: 0.2948
2026-01-08 13:15:13,308: t15.2025.01.12 val PER: 0.1478
2026-01-08 13:15:13,309: t15.2025.03.14 val PER: 0.3432
2026-01-08 13:15:13,310: t15.2025.03.16 val PER: 0.1872
2026-01-08 13:15:13,312: t15.2025.03.30 val PER: 0.3034
2026-01-08 13:15:13,313: t15.2025.04.13 val PER: 0.2097
2026-01-08 13:15:13,314: New best val WER(5gram) 14.60% --> 14.47%
2026-01-08 13:15:13,316: Checkpointing model
2026-01-08 13:15:13,457: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/base_input_dropout_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:15:22,037: Train batch 15600: loss: 11.68 grad norm: 56.23 time: 0.063
2026-01-08 13:15:39,192: Train batch 15800: loss: 13.51 grad norm: 62.11 time: 0.067
2026-01-08 13:15:56,715: Train batch 16000: loss: 8.74 grad norm: 46.26 time: 0.056
2026-01-08 13:15:56,716: Running test after training batch: 16000
2026-01-08 13:15:56,823: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:16:01,541: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:16:01,591: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:16:13,140: Val batch 16000: PER (avg): 0.1483 CTC Loss (avg): 15.2802 WER(5gram): 14.60% (n=256) time: 16.421
2026-01-08 13:16:13,142: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 13:16:13,144: t15.2023.08.13 val PER: 0.1060
2026-01-08 13:16:13,146: t15.2023.08.18 val PER: 0.1090
2026-01-08 13:16:13,147: t15.2023.08.20 val PER: 0.1072
2026-01-08 13:16:13,148: t15.2023.08.25 val PER: 0.0934
2026-01-08 13:16:13,150: t15.2023.08.27 val PER: 0.1833
2026-01-08 13:16:13,151: t15.2023.09.01 val PER: 0.0763
2026-01-08 13:16:13,152: t15.2023.09.03 val PER: 0.1496
2026-01-08 13:16:13,154: t15.2023.09.24 val PER: 0.1286
2026-01-08 13:16:13,155: t15.2023.09.29 val PER: 0.1276
2026-01-08 13:16:13,157: t15.2023.10.01 val PER: 0.1625
2026-01-08 13:16:13,158: t15.2023.10.06 val PER: 0.0818
2026-01-08 13:16:13,159: t15.2023.10.08 val PER: 0.2476
2026-01-08 13:16:13,161: t15.2023.10.13 val PER: 0.2002
2026-01-08 13:16:13,162: t15.2023.10.15 val PER: 0.1490
2026-01-08 13:16:13,163: t15.2023.10.20 val PER: 0.1980
2026-01-08 13:16:13,165: t15.2023.10.22 val PER: 0.1080
2026-01-08 13:16:13,166: t15.2023.11.03 val PER: 0.1737
2026-01-08 13:16:13,167: t15.2023.11.04 val PER: 0.0375
2026-01-08 13:16:13,169: t15.2023.11.17 val PER: 0.0358
2026-01-08 13:16:13,170: t15.2023.11.19 val PER: 0.0439
2026-01-08 13:16:13,171: t15.2023.11.26 val PER: 0.1130
2026-01-08 13:16:13,172: t15.2023.12.03 val PER: 0.1187
2026-01-08 13:16:13,175: t15.2023.12.08 val PER: 0.0972
2026-01-08 13:16:13,176: t15.2023.12.10 val PER: 0.0894
2026-01-08 13:16:13,178: t15.2023.12.17 val PER: 0.1268
2026-01-08 13:16:13,179: t15.2023.12.29 val PER: 0.1242
2026-01-08 13:16:13,180: t15.2024.02.25 val PER: 0.1081
2026-01-08 13:16:13,182: t15.2024.03.08 val PER: 0.2361
2026-01-08 13:16:13,183: t15.2024.03.15 val PER: 0.1932
2026-01-08 13:16:13,184: t15.2024.03.17 val PER: 0.1409
2026-01-08 13:16:13,185: t15.2024.05.10 val PER: 0.1664
2026-01-08 13:16:13,187: t15.2024.06.14 val PER: 0.1577
2026-01-08 13:16:13,188: t15.2024.07.19 val PER: 0.2287
2026-01-08 13:16:13,189: t15.2024.07.21 val PER: 0.0924
2026-01-08 13:16:13,191: t15.2024.07.28 val PER: 0.1390
2026-01-08 13:16:13,192: t15.2025.01.10 val PER: 0.2989
2026-01-08 13:16:13,193: t15.2025.01.12 val PER: 0.1424
2026-01-08 13:16:13,194: t15.2025.03.14 val PER: 0.3491
2026-01-08 13:16:13,196: t15.2025.03.16 val PER: 0.1950
2026-01-08 13:16:13,197: t15.2025.03.30 val PER: 0.2885
2026-01-08 13:16:13,199: t15.2025.04.13 val PER: 0.2154
2026-01-08 13:16:31,561: Train batch 16200: loss: 6.02 grad norm: 41.45 time: 0.056
2026-01-08 13:16:49,864: Train batch 16400: loss: 10.92 grad norm: 65.60 time: 0.058
2026-01-08 13:16:59,118: Running test after training batch: 16500
2026-01-08 13:16:59,246: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:17:03,988: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:17:04,038: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 13:17:15,706: Val batch 16500: PER (avg): 0.1478 CTC Loss (avg): 15.1603 WER(5gram): 15.32% (n=256) time: 16.586
2026-01-08 13:17:15,708: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 13:17:15,711: t15.2023.08.13 val PER: 0.1071
2026-01-08 13:17:15,713: t15.2023.08.18 val PER: 0.1039
2026-01-08 13:17:15,715: t15.2023.08.20 val PER: 0.1080
2026-01-08 13:17:15,716: t15.2023.08.25 val PER: 0.0949
2026-01-08 13:17:15,718: t15.2023.08.27 val PER: 0.1785
2026-01-08 13:17:15,720: t15.2023.09.01 val PER: 0.0731
2026-01-08 13:17:15,721: t15.2023.09.03 val PER: 0.1544
2026-01-08 13:17:15,723: t15.2023.09.24 val PER: 0.1335
2026-01-08 13:17:15,724: t15.2023.09.29 val PER: 0.1283
2026-01-08 13:17:15,726: t15.2023.10.01 val PER: 0.1658
2026-01-08 13:17:15,727: t15.2023.10.06 val PER: 0.0840
2026-01-08 13:17:15,729: t15.2023.10.08 val PER: 0.2490
2026-01-08 13:17:15,730: t15.2023.10.13 val PER: 0.1955
2026-01-08 13:17:15,731: t15.2023.10.15 val PER: 0.1457
2026-01-08 13:17:15,733: t15.2023.10.20 val PER: 0.2047
2026-01-08 13:17:15,734: t15.2023.10.22 val PER: 0.1114
2026-01-08 13:17:15,736: t15.2023.11.03 val PER: 0.1798
2026-01-08 13:17:15,737: t15.2023.11.04 val PER: 0.0341
2026-01-08 13:17:15,738: t15.2023.11.17 val PER: 0.0295
2026-01-08 13:17:15,740: t15.2023.11.19 val PER: 0.0379
2026-01-08 13:17:15,741: t15.2023.11.26 val PER: 0.1094
2026-01-08 13:17:15,743: t15.2023.12.03 val PER: 0.1155
2026-01-08 13:17:15,744: t15.2023.12.08 val PER: 0.0979
2026-01-08 13:17:15,746: t15.2023.12.10 val PER: 0.0959
2026-01-08 13:17:15,747: t15.2023.12.17 val PER: 0.1227
2026-01-08 13:17:15,749: t15.2023.12.29 val PER: 0.1263
2026-01-08 13:17:15,750: t15.2024.02.25 val PER: 0.1039
2026-01-08 13:17:15,752: t15.2024.03.08 val PER: 0.2304
2026-01-08 13:17:15,753: t15.2024.03.15 val PER: 0.1914
2026-01-08 13:17:15,754: t15.2024.03.17 val PER: 0.1381
2026-01-08 13:17:15,756: t15.2024.05.10 val PER: 0.1605
2026-01-08 13:17:15,757: t15.2024.06.14 val PER: 0.1640
2026-01-08 13:17:15,759: t15.2024.07.19 val PER: 0.2360
2026-01-08 13:17:15,760: t15.2024.07.21 val PER: 0.0897
2026-01-08 13:17:15,762: t15.2024.07.28 val PER: 0.1324
2026-01-08 13:17:15,763: t15.2025.01.10 val PER: 0.2865
2026-01-08 13:17:15,764: t15.2025.01.12 val PER: 0.1432
2026-01-08 13:17:15,766: t15.2025.03.14 val PER: 0.3506
2026-01-08 13:17:15,768: t15.2025.03.16 val PER: 0.1990
2026-01-08 13:17:15,769: t15.2025.03.30 val PER: 0.2908
2026-01-08 13:17:15,770: t15.2025.04.13 val PER: 0.2154
2026-01-08 13:17:25,090: Train batch 16600: loss: 8.46 grad norm: 53.44 time: 0.054
2026-01-08 13:17:43,631: Train batch 16800: loss: 16.55 grad norm: 73.34 time: 0.064
2026-01-08 13:18:02,128: Train batch 17000: loss: 8.03 grad norm: 50.05 time: 0.082
2026-01-08 13:18:02,130: Running test after training batch: 17000
2026-01-08 13:18:02,228: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:18:07,011: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:18:07,057: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:18:18,900: Val batch 17000: PER (avg): 0.1463 CTC Loss (avg): 15.0349 WER(5gram): 15.91% (n=256) time: 16.769
2026-01-08 13:18:18,903: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=13
2026-01-08 13:18:18,904: t15.2023.08.13 val PER: 0.1040
2026-01-08 13:18:18,906: t15.2023.08.18 val PER: 0.1090
2026-01-08 13:18:18,908: t15.2023.08.20 val PER: 0.1080
2026-01-08 13:18:18,909: t15.2023.08.25 val PER: 0.0964
2026-01-08 13:18:18,910: t15.2023.08.27 val PER: 0.1817
2026-01-08 13:18:18,912: t15.2023.09.01 val PER: 0.0714
2026-01-08 13:18:18,913: t15.2023.09.03 val PER: 0.1532
2026-01-08 13:18:18,915: t15.2023.09.24 val PER: 0.1238
2026-01-08 13:18:18,916: t15.2023.09.29 val PER: 0.1283
2026-01-08 13:18:18,918: t15.2023.10.01 val PER: 0.1592
2026-01-08 13:18:18,919: t15.2023.10.06 val PER: 0.0786
2026-01-08 13:18:18,921: t15.2023.10.08 val PER: 0.2463
2026-01-08 13:18:18,922: t15.2023.10.13 val PER: 0.1908
2026-01-08 13:18:18,923: t15.2023.10.15 val PER: 0.1516
2026-01-08 13:18:18,925: t15.2023.10.20 val PER: 0.1779
2026-01-08 13:18:18,926: t15.2023.10.22 val PER: 0.1102
2026-01-08 13:18:18,928: t15.2023.11.03 val PER: 0.1764
2026-01-08 13:18:18,929: t15.2023.11.04 val PER: 0.0341
2026-01-08 13:18:18,930: t15.2023.11.17 val PER: 0.0358
2026-01-08 13:18:18,932: t15.2023.11.19 val PER: 0.0379
2026-01-08 13:18:18,934: t15.2023.11.26 val PER: 0.1094
2026-01-08 13:18:18,936: t15.2023.12.03 val PER: 0.1113
2026-01-08 13:18:18,937: t15.2023.12.08 val PER: 0.0945
2026-01-08 13:18:18,938: t15.2023.12.10 val PER: 0.0920
2026-01-08 13:18:18,940: t15.2023.12.17 val PER: 0.1289
2026-01-08 13:18:18,941: t15.2023.12.29 val PER: 0.1242
2026-01-08 13:18:18,942: t15.2024.02.25 val PER: 0.1152
2026-01-08 13:18:18,943: t15.2024.03.08 val PER: 0.2347
2026-01-08 13:18:18,945: t15.2024.03.15 val PER: 0.1976
2026-01-08 13:18:18,946: t15.2024.03.17 val PER: 0.1437
2026-01-08 13:18:18,947: t15.2024.05.10 val PER: 0.1590
2026-01-08 13:18:18,948: t15.2024.06.14 val PER: 0.1562
2026-01-08 13:18:18,950: t15.2024.07.19 val PER: 0.2254
2026-01-08 13:18:18,951: t15.2024.07.21 val PER: 0.0890
2026-01-08 13:18:18,952: t15.2024.07.28 val PER: 0.1301
2026-01-08 13:18:18,954: t15.2025.01.10 val PER: 0.2851
2026-01-08 13:18:18,955: t15.2025.01.12 val PER: 0.1424
2026-01-08 13:18:18,956: t15.2025.03.14 val PER: 0.3432
2026-01-08 13:18:18,958: t15.2025.03.16 val PER: 0.1832
2026-01-08 13:18:18,959: t15.2025.03.30 val PER: 0.2862
2026-01-08 13:18:18,960: t15.2025.04.13 val PER: 0.2097
2026-01-08 13:18:37,311: Train batch 17200: loss: 9.26 grad norm: 49.47 time: 0.083
2026-01-08 13:18:55,019: Train batch 17400: loss: 11.70 grad norm: 60.92 time: 0.072
2026-01-08 13:19:03,708: Running test after training batch: 17500
2026-01-08 13:19:03,844: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:19:08,894: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:19:08,942: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:19:20,935: Val batch 17500: PER (avg): 0.1466 CTC Loss (avg): 15.1212 WER(5gram): 14.73% (n=256) time: 17.225
2026-01-08 13:19:20,938: WER lens: avg_true_words=5.99 avg_pred_words=6.17 max_pred_words=12
2026-01-08 13:19:20,940: t15.2023.08.13 val PER: 0.1071
2026-01-08 13:19:20,942: t15.2023.08.18 val PER: 0.1065
2026-01-08 13:19:20,944: t15.2023.08.20 val PER: 0.1088
2026-01-08 13:19:20,946: t15.2023.08.25 val PER: 0.0979
2026-01-08 13:19:20,948: t15.2023.08.27 val PER: 0.1785
2026-01-08 13:19:20,950: t15.2023.09.01 val PER: 0.0739
2026-01-08 13:19:20,952: t15.2023.09.03 val PER: 0.1520
2026-01-08 13:19:20,953: t15.2023.09.24 val PER: 0.1323
2026-01-08 13:19:20,955: t15.2023.09.29 val PER: 0.1302
2026-01-08 13:19:20,956: t15.2023.10.01 val PER: 0.1717
2026-01-08 13:19:20,958: t15.2023.10.06 val PER: 0.0797
2026-01-08 13:19:20,960: t15.2023.10.08 val PER: 0.2422
2026-01-08 13:19:20,961: t15.2023.10.13 val PER: 0.1916
2026-01-08 13:19:20,963: t15.2023.10.15 val PER: 0.1463
2026-01-08 13:19:20,965: t15.2023.10.20 val PER: 0.1946
2026-01-08 13:19:20,966: t15.2023.10.22 val PER: 0.1069
2026-01-08 13:19:20,968: t15.2023.11.03 val PER: 0.1791
2026-01-08 13:19:20,969: t15.2023.11.04 val PER: 0.0307
2026-01-08 13:19:20,971: t15.2023.11.17 val PER: 0.0295
2026-01-08 13:19:20,972: t15.2023.11.19 val PER: 0.0379
2026-01-08 13:19:20,973: t15.2023.11.26 val PER: 0.1123
2026-01-08 13:19:20,975: t15.2023.12.03 val PER: 0.1176
2026-01-08 13:19:20,976: t15.2023.12.08 val PER: 0.0912
2026-01-08 13:19:20,978: t15.2023.12.10 val PER: 0.0907
2026-01-08 13:19:20,979: t15.2023.12.17 val PER: 0.1299
2026-01-08 13:19:20,981: t15.2023.12.29 val PER: 0.1277
2026-01-08 13:19:20,982: t15.2024.02.25 val PER: 0.1039
2026-01-08 13:19:20,983: t15.2024.03.08 val PER: 0.2333
2026-01-08 13:19:20,985: t15.2024.03.15 val PER: 0.1895
2026-01-08 13:19:20,986: t15.2024.03.17 val PER: 0.1423
2026-01-08 13:19:20,988: t15.2024.05.10 val PER: 0.1590
2026-01-08 13:19:20,989: t15.2024.06.14 val PER: 0.1593
2026-01-08 13:19:20,991: t15.2024.07.19 val PER: 0.2215
2026-01-08 13:19:20,992: t15.2024.07.21 val PER: 0.0924
2026-01-08 13:19:20,993: t15.2024.07.28 val PER: 0.1301
2026-01-08 13:19:20,995: t15.2025.01.10 val PER: 0.2879
2026-01-08 13:19:20,996: t15.2025.01.12 val PER: 0.1393
2026-01-08 13:19:20,998: t15.2025.03.14 val PER: 0.3536
2026-01-08 13:19:20,999: t15.2025.03.16 val PER: 0.1859
2026-01-08 13:19:21,001: t15.2025.03.30 val PER: 0.2805
2026-01-08 13:19:21,002: t15.2025.04.13 val PER: 0.2083
2026-01-08 13:19:29,620: Train batch 17600: loss: 9.53 grad norm: 51.14 time: 0.052
2026-01-08 13:19:47,201: Train batch 17800: loss: 6.10 grad norm: 47.77 time: 0.043
2026-01-08 13:20:04,565: Train batch 18000: loss: 11.09 grad norm: 63.87 time: 0.064
2026-01-08 13:20:04,567: Running test after training batch: 18000
2026-01-08 13:20:04,706: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:20:09,592: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:20:09,641: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:20:21,669: Val batch 18000: PER (avg): 0.1456 CTC Loss (avg): 15.0458 WER(5gram): 15.65% (n=256) time: 17.099
2026-01-08 13:20:21,672: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-08 13:20:21,675: t15.2023.08.13 val PER: 0.1029
2026-01-08 13:20:21,677: t15.2023.08.18 val PER: 0.1039
2026-01-08 13:20:21,679: t15.2023.08.20 val PER: 0.1056
2026-01-08 13:20:21,680: t15.2023.08.25 val PER: 0.0934
2026-01-08 13:20:21,682: t15.2023.08.27 val PER: 0.1849
2026-01-08 13:20:21,684: t15.2023.09.01 val PER: 0.0779
2026-01-08 13:20:21,685: t15.2023.09.03 val PER: 0.1461
2026-01-08 13:20:21,687: t15.2023.09.24 val PER: 0.1286
2026-01-08 13:20:21,689: t15.2023.09.29 val PER: 0.1276
2026-01-08 13:20:21,692: t15.2023.10.01 val PER: 0.1697
2026-01-08 13:20:21,694: t15.2023.10.06 val PER: 0.0807
2026-01-08 13:20:21,695: t15.2023.10.08 val PER: 0.2463
2026-01-08 13:20:21,697: t15.2023.10.13 val PER: 0.1971
2026-01-08 13:20:21,699: t15.2023.10.15 val PER: 0.1470
2026-01-08 13:20:21,701: t15.2023.10.20 val PER: 0.2013
2026-01-08 13:20:21,703: t15.2023.10.22 val PER: 0.1047
2026-01-08 13:20:21,704: t15.2023.11.03 val PER: 0.1777
2026-01-08 13:20:21,706: t15.2023.11.04 val PER: 0.0341
2026-01-08 13:20:21,707: t15.2023.11.17 val PER: 0.0342
2026-01-08 13:20:21,709: t15.2023.11.19 val PER: 0.0319
2026-01-08 13:20:21,710: t15.2023.11.26 val PER: 0.1080
2026-01-08 13:20:21,712: t15.2023.12.03 val PER: 0.1050
2026-01-08 13:20:21,713: t15.2023.12.08 val PER: 0.0985
2026-01-08 13:20:21,715: t15.2023.12.10 val PER: 0.0933
2026-01-08 13:20:21,716: t15.2023.12.17 val PER: 0.1279
2026-01-08 13:20:21,718: t15.2023.12.29 val PER: 0.1311
2026-01-08 13:20:21,719: t15.2024.02.25 val PER: 0.1081
2026-01-08 13:20:21,721: t15.2024.03.08 val PER: 0.2390
2026-01-08 13:20:21,722: t15.2024.03.15 val PER: 0.1895
2026-01-08 13:20:21,723: t15.2024.03.17 val PER: 0.1430
2026-01-08 13:20:21,725: t15.2024.05.10 val PER: 0.1575
2026-01-08 13:20:21,726: t15.2024.06.14 val PER: 0.1451
2026-01-08 13:20:21,728: t15.2024.07.19 val PER: 0.2241
2026-01-08 13:20:21,730: t15.2024.07.21 val PER: 0.0869
2026-01-08 13:20:21,731: t15.2024.07.28 val PER: 0.1272
2026-01-08 13:20:21,733: t15.2025.01.10 val PER: 0.2961
2026-01-08 13:20:21,734: t15.2025.01.12 val PER: 0.1363
2026-01-08 13:20:21,736: t15.2025.03.14 val PER: 0.3402
2026-01-08 13:20:21,738: t15.2025.03.16 val PER: 0.1741
2026-01-08 13:20:21,739: t15.2025.03.30 val PER: 0.2747
2026-01-08 13:20:21,741: t15.2025.04.13 val PER: 0.2097
2026-01-08 13:20:40,696: Train batch 18200: loss: 7.06 grad norm: 46.01 time: 0.075
2026-01-08 13:20:59,132: Train batch 18400: loss: 4.38 grad norm: 39.63 time: 0.059
2026-01-08 13:21:08,432: Running test after training batch: 18500
2026-01-08 13:21:08,569: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:21:13,288: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:21:13,337: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:21:25,371: Val batch 18500: PER (avg): 0.1460 CTC Loss (avg): 15.0087 WER(5gram): 15.38% (n=256) time: 16.937
2026-01-08 13:21:25,374: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-08 13:21:25,376: t15.2023.08.13 val PER: 0.1071
2026-01-08 13:21:25,378: t15.2023.08.18 val PER: 0.1065
2026-01-08 13:21:25,379: t15.2023.08.20 val PER: 0.1104
2026-01-08 13:21:25,381: t15.2023.08.25 val PER: 0.0934
2026-01-08 13:21:25,382: t15.2023.08.27 val PER: 0.1801
2026-01-08 13:21:25,384: t15.2023.09.01 val PER: 0.0714
2026-01-08 13:21:25,385: t15.2023.09.03 val PER: 0.1449
2026-01-08 13:21:25,386: t15.2023.09.24 val PER: 0.1299
2026-01-08 13:21:25,388: t15.2023.09.29 val PER: 0.1289
2026-01-08 13:21:25,389: t15.2023.10.01 val PER: 0.1671
2026-01-08 13:21:25,390: t15.2023.10.06 val PER: 0.0797
2026-01-08 13:21:25,392: t15.2023.10.08 val PER: 0.2422
2026-01-08 13:21:25,393: t15.2023.10.13 val PER: 0.1947
2026-01-08 13:21:25,395: t15.2023.10.15 val PER: 0.1503
2026-01-08 13:21:25,396: t15.2023.10.20 val PER: 0.1980
2026-01-08 13:21:25,397: t15.2023.10.22 val PER: 0.1069
2026-01-08 13:21:25,399: t15.2023.11.03 val PER: 0.1798
2026-01-08 13:21:25,400: t15.2023.11.04 val PER: 0.0307
2026-01-08 13:21:25,402: t15.2023.11.17 val PER: 0.0327
2026-01-08 13:21:25,403: t15.2023.11.19 val PER: 0.0339
2026-01-08 13:21:25,405: t15.2023.11.26 val PER: 0.1109
2026-01-08 13:21:25,406: t15.2023.12.03 val PER: 0.1082
2026-01-08 13:21:25,408: t15.2023.12.08 val PER: 0.0952
2026-01-08 13:21:25,410: t15.2023.12.10 val PER: 0.0880
2026-01-08 13:21:25,411: t15.2023.12.17 val PER: 0.1258
2026-01-08 13:21:25,412: t15.2023.12.29 val PER: 0.1249
2026-01-08 13:21:25,414: t15.2024.02.25 val PER: 0.1011
2026-01-08 13:21:25,416: t15.2024.03.08 val PER: 0.2347
2026-01-08 13:21:25,417: t15.2024.03.15 val PER: 0.1907
2026-01-08 13:21:25,418: t15.2024.03.17 val PER: 0.1395
2026-01-08 13:21:25,420: t15.2024.05.10 val PER: 0.1590
2026-01-08 13:21:25,421: t15.2024.06.14 val PER: 0.1514
2026-01-08 13:21:25,423: t15.2024.07.19 val PER: 0.2261
2026-01-08 13:21:25,424: t15.2024.07.21 val PER: 0.0862
2026-01-08 13:21:25,425: t15.2024.07.28 val PER: 0.1309
2026-01-08 13:21:25,427: t15.2025.01.10 val PER: 0.2893
2026-01-08 13:21:25,428: t15.2025.01.12 val PER: 0.1432
2026-01-08 13:21:25,429: t15.2025.03.14 val PER: 0.3491
2026-01-08 13:21:25,431: t15.2025.03.16 val PER: 0.1885
2026-01-08 13:21:25,433: t15.2025.03.30 val PER: 0.2874
2026-01-08 13:21:25,434: t15.2025.04.13 val PER: 0.2054
2026-01-08 13:21:34,756: Train batch 18600: loss: 11.93 grad norm: 61.89 time: 0.068
2026-01-08 13:21:53,010: Train batch 18800: loss: 8.09 grad norm: 50.66 time: 0.064
2026-01-08 13:22:11,310: Train batch 19000: loss: 7.99 grad norm: 45.16 time: 0.067
2026-01-08 13:22:11,313: Running test after training batch: 19000
2026-01-08 13:22:11,442: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:22:16,175: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:22:16,225: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:22:28,190: Val batch 19000: PER (avg): 0.1461 CTC Loss (avg): 15.0137 WER(5gram): 15.19% (n=256) time: 16.876
2026-01-08 13:22:28,193: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=13
2026-01-08 13:22:28,195: t15.2023.08.13 val PER: 0.1091
2026-01-08 13:22:28,198: t15.2023.08.18 val PER: 0.1006
2026-01-08 13:22:28,201: t15.2023.08.20 val PER: 0.1128
2026-01-08 13:22:28,203: t15.2023.08.25 val PER: 0.0949
2026-01-08 13:22:28,204: t15.2023.08.27 val PER: 0.1785
2026-01-08 13:22:28,206: t15.2023.09.01 val PER: 0.0739
2026-01-08 13:22:28,207: t15.2023.09.03 val PER: 0.1473
2026-01-08 13:22:28,209: t15.2023.09.24 val PER: 0.1335
2026-01-08 13:22:28,211: t15.2023.09.29 val PER: 0.1257
2026-01-08 13:22:28,212: t15.2023.10.01 val PER: 0.1704
2026-01-08 13:22:28,214: t15.2023.10.06 val PER: 0.0764
2026-01-08 13:22:28,216: t15.2023.10.08 val PER: 0.2517
2026-01-08 13:22:28,217: t15.2023.10.13 val PER: 0.1885
2026-01-08 13:22:28,219: t15.2023.10.15 val PER: 0.1496
2026-01-08 13:22:28,221: t15.2023.10.20 val PER: 0.2013
2026-01-08 13:22:28,223: t15.2023.10.22 val PER: 0.1036
2026-01-08 13:22:28,224: t15.2023.11.03 val PER: 0.1805
2026-01-08 13:22:28,226: t15.2023.11.04 val PER: 0.0307
2026-01-08 13:22:28,230: t15.2023.11.17 val PER: 0.0295
2026-01-08 13:22:28,232: t15.2023.11.19 val PER: 0.0359
2026-01-08 13:22:28,234: t15.2023.11.26 val PER: 0.1109
2026-01-08 13:22:28,236: t15.2023.12.03 val PER: 0.1145
2026-01-08 13:22:28,238: t15.2023.12.08 val PER: 0.0925
2026-01-08 13:22:28,240: t15.2023.12.10 val PER: 0.0854
2026-01-08 13:22:28,242: t15.2023.12.17 val PER: 0.1310
2026-01-08 13:22:28,243: t15.2023.12.29 val PER: 0.1242
2026-01-08 13:22:28,245: t15.2024.02.25 val PER: 0.1025
2026-01-08 13:22:28,247: t15.2024.03.08 val PER: 0.2390
2026-01-08 13:22:28,248: t15.2024.03.15 val PER: 0.1870
2026-01-08 13:22:28,250: t15.2024.03.17 val PER: 0.1437
2026-01-08 13:22:28,251: t15.2024.05.10 val PER: 0.1471
2026-01-08 13:22:28,253: t15.2024.06.14 val PER: 0.1625
2026-01-08 13:22:28,255: t15.2024.07.19 val PER: 0.2235
2026-01-08 13:22:28,256: t15.2024.07.21 val PER: 0.0897
2026-01-08 13:22:28,258: t15.2024.07.28 val PER: 0.1309
2026-01-08 13:22:28,260: t15.2025.01.10 val PER: 0.2906
2026-01-08 13:22:28,261: t15.2025.01.12 val PER: 0.1386
2026-01-08 13:22:28,263: t15.2025.03.14 val PER: 0.3462
2026-01-08 13:22:28,265: t15.2025.03.16 val PER: 0.1846
2026-01-08 13:22:28,266: t15.2025.03.30 val PER: 0.2839
2026-01-08 13:22:28,268: t15.2025.04.13 val PER: 0.2168
2026-01-08 13:22:46,843: Train batch 19200: loss: 6.11 grad norm: 48.03 time: 0.064
2026-01-08 13:23:05,313: Train batch 19400: loss: 4.72 grad norm: 36.69 time: 0.056
2026-01-08 13:23:14,645: Running test after training batch: 19500
2026-01-08 13:23:14,743: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:23:19,494: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:23:19,543: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:23:31,518: Val batch 19500: PER (avg): 0.1461 CTC Loss (avg): 14.9501 WER(5gram): 15.78% (n=256) time: 16.871
2026-01-08 13:23:31,520: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=13
2026-01-08 13:23:31,522: t15.2023.08.13 val PER: 0.1071
2026-01-08 13:23:31,523: t15.2023.08.18 val PER: 0.0972
2026-01-08 13:23:31,525: t15.2023.08.20 val PER: 0.1104
2026-01-08 13:23:31,526: t15.2023.08.25 val PER: 0.0934
2026-01-08 13:23:31,528: t15.2023.08.27 val PER: 0.1801
2026-01-08 13:23:31,529: t15.2023.09.01 val PER: 0.0739
2026-01-08 13:23:31,530: t15.2023.09.03 val PER: 0.1449
2026-01-08 13:23:31,532: t15.2023.09.24 val PER: 0.1323
2026-01-08 13:23:31,533: t15.2023.09.29 val PER: 0.1295
2026-01-08 13:23:31,535: t15.2023.10.01 val PER: 0.1724
2026-01-08 13:23:31,536: t15.2023.10.06 val PER: 0.0807
2026-01-08 13:23:31,537: t15.2023.10.08 val PER: 0.2449
2026-01-08 13:23:31,538: t15.2023.10.13 val PER: 0.1924
2026-01-08 13:23:31,540: t15.2023.10.15 val PER: 0.1496
2026-01-08 13:23:31,541: t15.2023.10.20 val PER: 0.1946
2026-01-08 13:23:31,542: t15.2023.10.22 val PER: 0.1036
2026-01-08 13:23:31,544: t15.2023.11.03 val PER: 0.1832
2026-01-08 13:23:31,545: t15.2023.11.04 val PER: 0.0341
2026-01-08 13:23:31,546: t15.2023.11.17 val PER: 0.0327
2026-01-08 13:23:31,547: t15.2023.11.19 val PER: 0.0339
2026-01-08 13:23:31,549: t15.2023.11.26 val PER: 0.1123
2026-01-08 13:23:31,550: t15.2023.12.03 val PER: 0.1103
2026-01-08 13:23:31,551: t15.2023.12.08 val PER: 0.0959
2026-01-08 13:23:31,552: t15.2023.12.10 val PER: 0.0894
2026-01-08 13:23:31,554: t15.2023.12.17 val PER: 0.1247
2026-01-08 13:23:31,555: t15.2023.12.29 val PER: 0.1249
2026-01-08 13:23:31,556: t15.2024.02.25 val PER: 0.1067
2026-01-08 13:23:31,558: t15.2024.03.08 val PER: 0.2376
2026-01-08 13:23:31,559: t15.2024.03.15 val PER: 0.1889
2026-01-08 13:23:31,560: t15.2024.03.17 val PER: 0.1367
2026-01-08 13:23:31,561: t15.2024.05.10 val PER: 0.1545
2026-01-08 13:23:31,563: t15.2024.06.14 val PER: 0.1656
2026-01-08 13:23:31,564: t15.2024.07.19 val PER: 0.2314
2026-01-08 13:23:31,566: t15.2024.07.21 val PER: 0.0883
2026-01-08 13:23:31,567: t15.2024.07.28 val PER: 0.1272
2026-01-08 13:23:31,568: t15.2025.01.10 val PER: 0.2837
2026-01-08 13:23:31,569: t15.2025.01.12 val PER: 0.1370
2026-01-08 13:23:31,571: t15.2025.03.14 val PER: 0.3491
2026-01-08 13:23:31,572: t15.2025.03.16 val PER: 0.1859
2026-01-08 13:23:31,574: t15.2025.03.30 val PER: 0.2805
2026-01-08 13:23:31,575: t15.2025.04.13 val PER: 0.2083
2026-01-08 13:23:40,810: Train batch 19600: loss: 7.38 grad norm: 45.03 time: 0.059
2026-01-08 13:23:59,108: Train batch 19800: loss: 7.28 grad norm: 50.54 time: 0.057
2026-01-08 13:24:17,550: Running test after training batch: 19999
2026-01-08 13:24:17,642: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:24:22,294: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:24:22,345: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:24:34,599: Val batch 19999: PER (avg): 0.1459 CTC Loss (avg): 14.9527 WER(5gram): 15.38% (n=256) time: 17.048
2026-01-08 13:24:34,601: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-08 13:24:34,603: t15.2023.08.13 val PER: 0.1060
2026-01-08 13:24:34,604: t15.2023.08.18 val PER: 0.1014
2026-01-08 13:24:34,607: t15.2023.08.20 val PER: 0.1088
2026-01-08 13:24:34,608: t15.2023.08.25 val PER: 0.0949
2026-01-08 13:24:34,609: t15.2023.08.27 val PER: 0.1833
2026-01-08 13:24:34,611: t15.2023.09.01 val PER: 0.0714
2026-01-08 13:24:34,612: t15.2023.09.03 val PER: 0.1473
2026-01-08 13:24:34,614: t15.2023.09.24 val PER: 0.1274
2026-01-08 13:24:34,615: t15.2023.09.29 val PER: 0.1257
2026-01-08 13:24:34,617: t15.2023.10.01 val PER: 0.1678
2026-01-08 13:24:34,619: t15.2023.10.06 val PER: 0.0775
2026-01-08 13:24:34,621: t15.2023.10.08 val PER: 0.2409
2026-01-08 13:24:34,622: t15.2023.10.13 val PER: 0.1955
2026-01-08 13:24:34,623: t15.2023.10.15 val PER: 0.1523
2026-01-08 13:24:34,625: t15.2023.10.20 val PER: 0.1913
2026-01-08 13:24:34,626: t15.2023.10.22 val PER: 0.1013
2026-01-08 13:24:34,627: t15.2023.11.03 val PER: 0.1825
2026-01-08 13:24:34,628: t15.2023.11.04 val PER: 0.0307
2026-01-08 13:24:34,630: t15.2023.11.17 val PER: 0.0327
2026-01-08 13:24:34,631: t15.2023.11.19 val PER: 0.0359
2026-01-08 13:24:34,632: t15.2023.11.26 val PER: 0.1101
2026-01-08 13:24:34,634: t15.2023.12.03 val PER: 0.1071
2026-01-08 13:24:34,635: t15.2023.12.08 val PER: 0.0959
2026-01-08 13:24:34,636: t15.2023.12.10 val PER: 0.0894
2026-01-08 13:24:34,638: t15.2023.12.17 val PER: 0.1310
2026-01-08 13:24:34,639: t15.2023.12.29 val PER: 0.1290
2026-01-08 13:24:34,640: t15.2024.02.25 val PER: 0.1053
2026-01-08 13:24:34,642: t15.2024.03.08 val PER: 0.2390
2026-01-08 13:24:34,643: t15.2024.03.15 val PER: 0.1889
2026-01-08 13:24:34,645: t15.2024.03.17 val PER: 0.1388
2026-01-08 13:24:34,646: t15.2024.05.10 val PER: 0.1560
2026-01-08 13:24:34,647: t15.2024.06.14 val PER: 0.1546
2026-01-08 13:24:34,649: t15.2024.07.19 val PER: 0.2261
2026-01-08 13:24:34,650: t15.2024.07.21 val PER: 0.0890
2026-01-08 13:24:34,651: t15.2024.07.28 val PER: 0.1301
2026-01-08 13:24:34,653: t15.2025.01.10 val PER: 0.2837
2026-01-08 13:24:34,654: t15.2025.01.12 val PER: 0.1393
2026-01-08 13:24:34,655: t15.2025.03.14 val PER: 0.3536
2026-01-08 13:24:34,657: t15.2025.03.16 val PER: 0.1832
2026-01-08 13:24:34,658: t15.2025.03.30 val PER: 0.2862
2026-01-08 13:24:34,659: t15.2025.04.13 val PER: 0.2083
2026-01-08 13:24:35,097: Best avg val PER achieved: 0.15022
2026-01-08 13:24:35,100: Total training time: 46.30 minutes

=== RUN id05_wd1e-5.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id05_wd1e-5
2026-01-08 13:26:31,250: Using device: cuda:0
2026-01-08 13:30:22,423: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-08 13:30:22,448: Using 45 sessions after filtering (from 45).
2026-01-08 13:30:22,905: Using torch.compile (if available)
2026-01-08 13:30:22,907: torch.compile not available (torch<2.0). Skipping.
2026-01-08 13:30:22,909: Initialized RNN decoding model
2026-01-08 13:30:22,911: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.05, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-08 13:30:22,912: Model has 44,907,305 parameters
2026-01-08 13:30:22,914: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-08 13:30:24,185: Successfully initialized datasets
2026-01-08 13:30:24,187: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-08 13:30:25,625: Train batch 0: loss: 580.59 grad norm: 1422.57 time: 0.168
2026-01-08 13:30:25,627: Running test after training batch: 0
2026-01-08 13:30:25,741: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:30:31,455: WER debug example
  GT : you can see the code at this point as well
  PR : she has from his
2026-01-08 13:30:32,470: WER debug example
  GT : how does it keep the cost down
  PR : money from
2026-01-08 13:34:24,270: Val batch 0: PER (avg): 1.4289 CTC Loss (avg): 633.1762 WER(5gram): 99.67% (n=256) time: 238.641
2026-01-08 13:34:24,277: WER lens: avg_true_words=5.99 avg_pred_words=2.82 max_pred_words=7
2026-01-08 13:34:24,281: t15.2023.08.13 val PER: 1.3056
2026-01-08 13:34:24,285: t15.2023.08.18 val PER: 1.4300
2026-01-08 13:34:24,286: t15.2023.08.20 val PER: 1.2986
2026-01-08 13:34:24,287: t15.2023.08.25 val PER: 1.3404
2026-01-08 13:34:24,287: t15.2023.08.27 val PER: 1.2524
2026-01-08 13:34:24,287: t15.2023.09.01 val PER: 1.4440
2026-01-08 13:34:24,287: t15.2023.09.03 val PER: 1.3207
2026-01-08 13:34:24,287: t15.2023.09.24 val PER: 1.5400
2026-01-08 13:34:24,287: t15.2023.09.29 val PER: 1.4652
2026-01-08 13:34:24,287: t15.2023.10.01 val PER: 1.2114
2026-01-08 13:34:24,287: t15.2023.10.06 val PER: 1.4909
2026-01-08 13:34:24,288: t15.2023.10.08 val PER: 1.1881
2026-01-08 13:34:24,288: t15.2023.10.13 val PER: 1.3918
2026-01-08 13:34:24,288: t15.2023.10.15 val PER: 1.3896
2026-01-08 13:34:24,288: t15.2023.10.20 val PER: 1.5034
2026-01-08 13:34:24,288: t15.2023.10.22 val PER: 1.3920
2026-01-08 13:34:24,288: t15.2023.11.03 val PER: 1.5909
2026-01-08 13:34:24,288: t15.2023.11.04 val PER: 2.0273
2026-01-08 13:34:24,288: t15.2023.11.17 val PER: 1.9627
2026-01-08 13:34:24,288: t15.2023.11.19 val PER: 1.6786
2026-01-08 13:34:24,288: t15.2023.11.26 val PER: 1.5384
2026-01-08 13:34:24,289: t15.2023.12.03 val PER: 1.4265
2026-01-08 13:34:24,289: t15.2023.12.08 val PER: 1.4521
2026-01-08 13:34:24,289: t15.2023.12.10 val PER: 1.7004
2026-01-08 13:34:24,289: t15.2023.12.17 val PER: 1.3056
2026-01-08 13:34:24,289: t15.2023.12.29 val PER: 1.4063
2026-01-08 13:34:24,289: t15.2024.02.25 val PER: 1.4256
2026-01-08 13:34:24,289: t15.2024.03.08 val PER: 1.3201
2026-01-08 13:34:24,289: t15.2024.03.15 val PER: 1.3158
2026-01-08 13:34:24,289: t15.2024.03.17 val PER: 1.3996
2026-01-08 13:34:24,289: t15.2024.05.10 val PER: 1.3224
2026-01-08 13:34:24,289: t15.2024.06.14 val PER: 1.5315
2026-01-08 13:34:24,290: t15.2024.07.19 val PER: 1.0817
2026-01-08 13:34:24,290: t15.2024.07.21 val PER: 1.6269
2026-01-08 13:34:24,290: t15.2024.07.28 val PER: 1.6529
2026-01-08 13:34:24,290: t15.2025.01.10 val PER: 1.0964
2026-01-08 13:34:24,290: t15.2025.01.12 val PER: 1.7667
2026-01-08 13:34:24,290: t15.2025.03.14 val PER: 1.0399
2026-01-08 13:34:24,290: t15.2025.03.16 val PER: 1.6165
2026-01-08 13:34:24,290: t15.2025.03.30 val PER: 1.2851
2026-01-08 13:34:24,290: t15.2025.04.13 val PER: 1.5906
2026-01-08 13:34:24,291: New best val WER(5gram) inf% --> 99.67%
2026-01-08 13:34:24,291: Checkpointing model
2026-01-08 13:34:24,430: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:34:42,609: Train batch 200: loss: 77.36 grad norm: 105.26 time: 0.055
2026-01-08 13:35:00,935: Train batch 400: loss: 53.25 grad norm: 94.04 time: 0.063
2026-01-08 13:35:09,871: Running test after training batch: 500
2026-01-08 13:35:09,998: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:35:14,975: WER debug example
  GT : you can see the code at this point as well
  PR : food and is thus had at this guide is all
2026-01-08 13:35:15,128: WER debug example
  GT : how does it keep the cost down
  PR : and does it think this is due
2026-01-08 13:36:01,543: Val batch 500: PER (avg): 0.5131 CTC Loss (avg): 55.3751 WER(5gram): 75.29% (n=256) time: 51.670
2026-01-08 13:36:01,546: WER lens: avg_true_words=5.99 avg_pred_words=5.71 max_pred_words=12
2026-01-08 13:36:01,548: t15.2023.08.13 val PER: 0.4605
2026-01-08 13:36:01,550: t15.2023.08.18 val PER: 0.4384
2026-01-08 13:36:01,551: t15.2023.08.20 val PER: 0.4361
2026-01-08 13:36:01,553: t15.2023.08.25 val PER: 0.4352
2026-01-08 13:36:01,554: t15.2023.08.27 val PER: 0.5209
2026-01-08 13:36:01,556: t15.2023.09.01 val PER: 0.4115
2026-01-08 13:36:01,558: t15.2023.09.03 val PER: 0.4976
2026-01-08 13:36:01,559: t15.2023.09.24 val PER: 0.4163
2026-01-08 13:36:01,561: t15.2023.09.29 val PER: 0.4620
2026-01-08 13:36:01,563: t15.2023.10.01 val PER: 0.5139
2026-01-08 13:36:01,564: t15.2023.10.06 val PER: 0.4187
2026-01-08 13:36:01,566: t15.2023.10.08 val PER: 0.5413
2026-01-08 13:36:01,567: t15.2023.10.13 val PER: 0.5609
2026-01-08 13:36:01,568: t15.2023.10.15 val PER: 0.4977
2026-01-08 13:36:01,570: t15.2023.10.20 val PER: 0.4396
2026-01-08 13:36:01,571: t15.2023.10.22 val PER: 0.4454
2026-01-08 13:36:01,573: t15.2023.11.03 val PER: 0.4946
2026-01-08 13:36:01,575: t15.2023.11.04 val PER: 0.2662
2026-01-08 13:36:01,576: t15.2023.11.17 val PER: 0.3701
2026-01-08 13:36:01,578: t15.2023.11.19 val PER: 0.3453
2026-01-08 13:36:01,579: t15.2023.11.26 val PER: 0.5420
2026-01-08 13:36:01,581: t15.2023.12.03 val PER: 0.5105
2026-01-08 13:36:01,583: t15.2023.12.08 val PER: 0.5186
2026-01-08 13:36:01,585: t15.2023.12.10 val PER: 0.4468
2026-01-08 13:36:01,586: t15.2023.12.17 val PER: 0.5551
2026-01-08 13:36:01,587: t15.2023.12.29 val PER: 0.5264
2026-01-08 13:36:01,589: t15.2024.02.25 val PER: 0.4916
2026-01-08 13:36:01,590: t15.2024.03.08 val PER: 0.6031
2026-01-08 13:36:01,592: t15.2024.03.15 val PER: 0.5460
2026-01-08 13:36:01,593: t15.2024.03.17 val PER: 0.4986
2026-01-08 13:36:01,595: t15.2024.05.10 val PER: 0.5319
2026-01-08 13:36:01,596: t15.2024.06.14 val PER: 0.5095
2026-01-08 13:36:01,598: t15.2024.07.19 val PER: 0.6678
2026-01-08 13:36:01,599: t15.2024.07.21 val PER: 0.4766
2026-01-08 13:36:01,601: t15.2024.07.28 val PER: 0.5059
2026-01-08 13:36:01,602: t15.2025.01.10 val PER: 0.7438
2026-01-08 13:36:01,604: t15.2025.01.12 val PER: 0.5543
2026-01-08 13:36:01,605: t15.2025.03.14 val PER: 0.7618
2026-01-08 13:36:01,607: t15.2025.03.16 val PER: 0.5785
2026-01-08 13:36:01,608: t15.2025.03.30 val PER: 0.7149
2026-01-08 13:36:01,609: t15.2025.04.13 val PER: 0.5663
2026-01-08 13:36:01,611: New best val WER(5gram) 99.67% --> 75.29%
2026-01-08 13:36:01,613: Checkpointing model
2026-01-08 13:36:01,767: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:36:11,034: Train batch 600: loss: 48.83 grad norm: 75.29 time: 0.080
2026-01-08 13:36:29,280: Train batch 800: loss: 41.41 grad norm: 88.50 time: 0.058
2026-01-08 13:36:47,847: Train batch 1000: loss: 42.26 grad norm: 78.45 time: 0.067
2026-01-08 13:36:47,850: Running test after training batch: 1000
2026-01-08 13:36:47,977: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:36:53,017: WER debug example
  GT : you can see the code at this point as well
  PR : you'd hand me the code at this and is well
2026-01-08 13:36:53,153: WER debug example
  GT : how does it keep the cost down
  PR : howled as it is that it's not
2026-01-08 13:37:20,556: Val batch 1000: PER (avg): 0.4104 CTC Loss (avg): 42.2316 WER(5gram): 52.67% (n=256) time: 32.693
2026-01-08 13:37:20,559: WER lens: avg_true_words=5.99 avg_pred_words=5.55 max_pred_words=12
2026-01-08 13:37:20,561: t15.2023.08.13 val PER: 0.3825
2026-01-08 13:37:20,562: t15.2023.08.18 val PER: 0.3512
2026-01-08 13:37:20,564: t15.2023.08.20 val PER: 0.3392
2026-01-08 13:37:20,565: t15.2023.08.25 val PER: 0.2907
2026-01-08 13:37:20,567: t15.2023.08.27 val PER: 0.4148
2026-01-08 13:37:20,568: t15.2023.09.01 val PER: 0.3060
2026-01-08 13:37:20,570: t15.2023.09.03 val PER: 0.4097
2026-01-08 13:37:20,572: t15.2023.09.24 val PER: 0.3301
2026-01-08 13:37:20,574: t15.2023.09.29 val PER: 0.3701
2026-01-08 13:37:20,575: t15.2023.10.01 val PER: 0.4108
2026-01-08 13:37:20,577: t15.2023.10.06 val PER: 0.3165
2026-01-08 13:37:20,578: t15.2023.10.08 val PER: 0.4547
2026-01-08 13:37:20,580: t15.2023.10.13 val PER: 0.4616
2026-01-08 13:37:20,581: t15.2023.10.15 val PER: 0.3790
2026-01-08 13:37:20,583: t15.2023.10.20 val PER: 0.3591
2026-01-08 13:37:20,584: t15.2023.10.22 val PER: 0.3519
2026-01-08 13:37:20,586: t15.2023.11.03 val PER: 0.3962
2026-01-08 13:37:20,588: t15.2023.11.04 val PER: 0.1638
2026-01-08 13:37:20,589: t15.2023.11.17 val PER: 0.2597
2026-01-08 13:37:20,591: t15.2023.11.19 val PER: 0.2056
2026-01-08 13:37:20,592: t15.2023.11.26 val PER: 0.4478
2026-01-08 13:37:20,594: t15.2023.12.03 val PER: 0.4065
2026-01-08 13:37:20,595: t15.2023.12.08 val PER: 0.4068
2026-01-08 13:37:20,597: t15.2023.12.10 val PER: 0.3495
2026-01-08 13:37:20,599: t15.2023.12.17 val PER: 0.4179
2026-01-08 13:37:20,601: t15.2023.12.29 val PER: 0.4001
2026-01-08 13:37:20,602: t15.2024.02.25 val PER: 0.3624
2026-01-08 13:37:20,604: t15.2024.03.08 val PER: 0.4979
2026-01-08 13:37:20,605: t15.2024.03.15 val PER: 0.4428
2026-01-08 13:37:20,606: t15.2024.03.17 val PER: 0.4079
2026-01-08 13:37:20,608: t15.2024.05.10 val PER: 0.4220
2026-01-08 13:37:20,609: t15.2024.06.14 val PER: 0.3959
2026-01-08 13:37:20,611: t15.2024.07.19 val PER: 0.5366
2026-01-08 13:37:20,612: t15.2024.07.21 val PER: 0.3855
2026-01-08 13:37:20,614: t15.2024.07.28 val PER: 0.4140
2026-01-08 13:37:20,615: t15.2025.01.10 val PER: 0.6116
2026-01-08 13:37:20,617: t15.2025.01.12 val PER: 0.4580
2026-01-08 13:37:20,618: t15.2025.03.14 val PER: 0.6450
2026-01-08 13:37:20,619: t15.2025.03.16 val PER: 0.4856
2026-01-08 13:37:20,621: t15.2025.03.30 val PER: 0.6598
2026-01-08 13:37:20,622: t15.2025.04.13 val PER: 0.4864
2026-01-08 13:37:20,624: New best val WER(5gram) 75.29% --> 52.67%
2026-01-08 13:37:20,626: Checkpointing model
2026-01-08 13:37:20,764: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:37:39,254: Train batch 1200: loss: 32.52 grad norm: 74.46 time: 0.068
2026-01-08 13:37:57,652: Train batch 1400: loss: 35.54 grad norm: 78.38 time: 0.061
2026-01-08 13:38:06,904: Running test after training batch: 1500
2026-01-08 13:38:07,068: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:38:11,857: WER debug example
  GT : you can see the code at this point as well
  PR : you can't see the code at this point is well
2026-01-08 13:38:11,932: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 13:38:28,522: Val batch 1500: PER (avg): 0.3809 CTC Loss (avg): 36.9769 WER(5gram): 35.46% (n=256) time: 21.615
2026-01-08 13:38:28,524: WER lens: avg_true_words=5.99 avg_pred_words=5.24 max_pred_words=12
2026-01-08 13:38:28,526: t15.2023.08.13 val PER: 0.3514
2026-01-08 13:38:28,528: t15.2023.08.18 val PER: 0.3068
2026-01-08 13:38:28,530: t15.2023.08.20 val PER: 0.3098
2026-01-08 13:38:28,532: t15.2023.08.25 val PER: 0.2620
2026-01-08 13:38:28,534: t15.2023.08.27 val PER: 0.4196
2026-01-08 13:38:28,537: t15.2023.09.01 val PER: 0.2735
2026-01-08 13:38:28,538: t15.2023.09.03 val PER: 0.3800
2026-01-08 13:38:28,540: t15.2023.09.24 val PER: 0.3107
2026-01-08 13:38:28,541: t15.2023.09.29 val PER: 0.3382
2026-01-08 13:38:28,543: t15.2023.10.01 val PER: 0.3983
2026-01-08 13:38:28,545: t15.2023.10.06 val PER: 0.2842
2026-01-08 13:38:28,546: t15.2023.10.08 val PER: 0.4438
2026-01-08 13:38:28,548: t15.2023.10.13 val PER: 0.4430
2026-01-08 13:38:28,549: t15.2023.10.15 val PER: 0.3606
2026-01-08 13:38:28,551: t15.2023.10.20 val PER: 0.3121
2026-01-08 13:38:28,553: t15.2023.10.22 val PER: 0.3174
2026-01-08 13:38:28,554: t15.2023.11.03 val PER: 0.3609
2026-01-08 13:38:28,556: t15.2023.11.04 val PER: 0.1058
2026-01-08 13:38:28,557: t15.2023.11.17 val PER: 0.2240
2026-01-08 13:38:28,559: t15.2023.11.19 val PER: 0.1677
2026-01-08 13:38:28,560: t15.2023.11.26 val PER: 0.4196
2026-01-08 13:38:28,562: t15.2023.12.03 val PER: 0.3655
2026-01-08 13:38:28,563: t15.2023.12.08 val PER: 0.3535
2026-01-08 13:38:28,565: t15.2023.12.10 val PER: 0.2996
2026-01-08 13:38:28,566: t15.2023.12.17 val PER: 0.3836
2026-01-08 13:38:28,568: t15.2023.12.29 val PER: 0.3850
2026-01-08 13:38:28,569: t15.2024.02.25 val PER: 0.3132
2026-01-08 13:38:28,571: t15.2024.03.08 val PER: 0.4651
2026-01-08 13:38:28,572: t15.2024.03.15 val PER: 0.4228
2026-01-08 13:38:28,574: t15.2024.03.17 val PER: 0.3856
2026-01-08 13:38:28,576: t15.2024.05.10 val PER: 0.3908
2026-01-08 13:38:28,577: t15.2024.06.14 val PER: 0.3943
2026-01-08 13:38:28,579: t15.2024.07.19 val PER: 0.5214
2026-01-08 13:38:28,580: t15.2024.07.21 val PER: 0.3455
2026-01-08 13:38:28,582: t15.2024.07.28 val PER: 0.3676
2026-01-08 13:38:28,583: t15.2025.01.10 val PER: 0.5992
2026-01-08 13:38:28,584: t15.2025.01.12 val PER: 0.4280
2026-01-08 13:38:28,586: t15.2025.03.14 val PER: 0.6109
2026-01-08 13:38:28,587: t15.2025.03.16 val PER: 0.4490
2026-01-08 13:38:28,589: t15.2025.03.30 val PER: 0.6161
2026-01-08 13:38:28,590: t15.2025.04.13 val PER: 0.4693
2026-01-08 13:38:28,592: New best val WER(5gram) 52.67% --> 35.46%
2026-01-08 13:38:28,593: Checkpointing model
2026-01-08 13:38:28,743: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:38:37,091: Train batch 1600: loss: 35.95 grad norm: 79.55 time: 0.065
2026-01-08 13:38:54,455: Train batch 1800: loss: 35.11 grad norm: 73.59 time: 0.089
2026-01-08 13:39:12,105: Train batch 2000: loss: 33.34 grad norm: 71.34 time: 0.069
2026-01-08 13:39:12,108: Running test after training batch: 2000
2026-01-08 13:39:12,240: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:39:17,065: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 13:39:17,127: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us and
2026-01-08 13:39:33,431: Val batch 2000: PER (avg): 0.3249 CTC Loss (avg): 32.6777 WER(5gram): 29.14% (n=256) time: 21.322
2026-01-08 13:39:33,434: WER lens: avg_true_words=5.99 avg_pred_words=5.80 max_pred_words=12
2026-01-08 13:39:33,436: t15.2023.08.13 val PER: 0.3046
2026-01-08 13:39:33,437: t15.2023.08.18 val PER: 0.2506
2026-01-08 13:39:33,439: t15.2023.08.20 val PER: 0.2470
2026-01-08 13:39:33,440: t15.2023.08.25 val PER: 0.2349
2026-01-08 13:39:33,441: t15.2023.08.27 val PER: 0.3280
2026-01-08 13:39:33,443: t15.2023.09.01 val PER: 0.2248
2026-01-08 13:39:33,444: t15.2023.09.03 val PER: 0.3195
2026-01-08 13:39:33,445: t15.2023.09.24 val PER: 0.2549
2026-01-08 13:39:33,447: t15.2023.09.29 val PER: 0.2699
2026-01-08 13:39:33,448: t15.2023.10.01 val PER: 0.3210
2026-01-08 13:39:33,449: t15.2023.10.06 val PER: 0.2422
2026-01-08 13:39:33,451: t15.2023.10.08 val PER: 0.3992
2026-01-08 13:39:33,452: t15.2023.10.13 val PER: 0.3856
2026-01-08 13:39:33,454: t15.2023.10.15 val PER: 0.3013
2026-01-08 13:39:33,456: t15.2023.10.20 val PER: 0.2685
2026-01-08 13:39:33,457: t15.2023.10.22 val PER: 0.2606
2026-01-08 13:39:33,458: t15.2023.11.03 val PER: 0.3223
2026-01-08 13:39:33,460: t15.2023.11.04 val PER: 0.1024
2026-01-08 13:39:33,461: t15.2023.11.17 val PER: 0.1804
2026-01-08 13:39:33,462: t15.2023.11.19 val PER: 0.1357
2026-01-08 13:39:33,463: t15.2023.11.26 val PER: 0.3674
2026-01-08 13:39:33,465: t15.2023.12.03 val PER: 0.3067
2026-01-08 13:39:33,466: t15.2023.12.08 val PER: 0.3016
2026-01-08 13:39:33,467: t15.2023.12.10 val PER: 0.2628
2026-01-08 13:39:33,469: t15.2023.12.17 val PER: 0.3056
2026-01-08 13:39:33,470: t15.2023.12.29 val PER: 0.3288
2026-01-08 13:39:33,471: t15.2024.02.25 val PER: 0.2809
2026-01-08 13:39:33,473: t15.2024.03.08 val PER: 0.4083
2026-01-08 13:39:33,474: t15.2024.03.15 val PER: 0.3634
2026-01-08 13:39:33,475: t15.2024.03.17 val PER: 0.3389
2026-01-08 13:39:33,476: t15.2024.05.10 val PER: 0.3299
2026-01-08 13:39:33,478: t15.2024.06.14 val PER: 0.3438
2026-01-08 13:39:33,479: t15.2024.07.19 val PER: 0.4502
2026-01-08 13:39:33,480: t15.2024.07.21 val PER: 0.2952
2026-01-08 13:39:33,483: t15.2024.07.28 val PER: 0.3287
2026-01-08 13:39:33,484: t15.2025.01.10 val PER: 0.5275
2026-01-08 13:39:33,485: t15.2025.01.12 val PER: 0.3757
2026-01-08 13:39:33,487: t15.2025.03.14 val PER: 0.5237
2026-01-08 13:39:33,488: t15.2025.03.16 val PER: 0.3822
2026-01-08 13:39:33,489: t15.2025.03.30 val PER: 0.5253
2026-01-08 13:39:33,490: t15.2025.04.13 val PER: 0.4165
2026-01-08 13:39:33,492: New best val WER(5gram) 35.46% --> 29.14%
2026-01-08 13:39:33,493: Checkpointing model
2026-01-08 13:39:33,646: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:39:50,820: Train batch 2200: loss: 28.70 grad norm: 72.99 time: 0.066
2026-01-08 13:40:08,551: Train batch 2400: loss: 28.50 grad norm: 63.80 time: 0.054
2026-01-08 13:40:17,261: Running test after training batch: 2500
2026-01-08 13:40:17,409: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:40:22,740: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 13:40:22,813: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost at
2026-01-08 13:40:38,666: Val batch 2500: PER (avg): 0.3044 CTC Loss (avg): 30.0905 WER(5gram): 27.71% (n=256) time: 21.403
2026-01-08 13:40:38,668: WER lens: avg_true_words=5.99 avg_pred_words=5.85 max_pred_words=12
2026-01-08 13:40:38,670: t15.2023.08.13 val PER: 0.2859
2026-01-08 13:40:38,671: t15.2023.08.18 val PER: 0.2439
2026-01-08 13:40:38,673: t15.2023.08.20 val PER: 0.2359
2026-01-08 13:40:38,674: t15.2023.08.25 val PER: 0.1973
2026-01-08 13:40:38,676: t15.2023.08.27 val PER: 0.3167
2026-01-08 13:40:38,677: t15.2023.09.01 val PER: 0.2143
2026-01-08 13:40:38,678: t15.2023.09.03 val PER: 0.3005
2026-01-08 13:40:38,680: t15.2023.09.24 val PER: 0.2257
2026-01-08 13:40:38,681: t15.2023.09.29 val PER: 0.2591
2026-01-08 13:40:38,683: t15.2023.10.01 val PER: 0.3118
2026-01-08 13:40:38,684: t15.2023.10.06 val PER: 0.2196
2026-01-08 13:40:38,685: t15.2023.10.08 val PER: 0.3735
2026-01-08 13:40:38,687: t15.2023.10.13 val PER: 0.3592
2026-01-08 13:40:38,688: t15.2023.10.15 val PER: 0.2861
2026-01-08 13:40:38,689: t15.2023.10.20 val PER: 0.2852
2026-01-08 13:40:38,691: t15.2023.10.22 val PER: 0.2249
2026-01-08 13:40:38,692: t15.2023.11.03 val PER: 0.2972
2026-01-08 13:40:38,694: t15.2023.11.04 val PER: 0.0853
2026-01-08 13:40:38,695: t15.2023.11.17 val PER: 0.1400
2026-01-08 13:40:38,696: t15.2023.11.19 val PER: 0.1238
2026-01-08 13:40:38,698: t15.2023.11.26 val PER: 0.3529
2026-01-08 13:40:38,699: t15.2023.12.03 val PER: 0.2889
2026-01-08 13:40:38,701: t15.2023.12.08 val PER: 0.2870
2026-01-08 13:40:38,702: t15.2023.12.10 val PER: 0.2221
2026-01-08 13:40:38,703: t15.2023.12.17 val PER: 0.2931
2026-01-08 13:40:38,705: t15.2023.12.29 val PER: 0.3185
2026-01-08 13:40:38,706: t15.2024.02.25 val PER: 0.2444
2026-01-08 13:40:38,708: t15.2024.03.08 val PER: 0.3556
2026-01-08 13:40:38,709: t15.2024.03.15 val PER: 0.3433
2026-01-08 13:40:38,710: t15.2024.03.17 val PER: 0.3152
2026-01-08 13:40:38,712: t15.2024.05.10 val PER: 0.3165
2026-01-08 13:40:38,713: t15.2024.06.14 val PER: 0.3123
2026-01-08 13:40:38,714: t15.2024.07.19 val PER: 0.4423
2026-01-08 13:40:38,716: t15.2024.07.21 val PER: 0.2676
2026-01-08 13:40:38,717: t15.2024.07.28 val PER: 0.2882
2026-01-08 13:40:38,719: t15.2025.01.10 val PER: 0.4986
2026-01-08 13:40:38,720: t15.2025.01.12 val PER: 0.3580
2026-01-08 13:40:38,721: t15.2025.03.14 val PER: 0.5089
2026-01-08 13:40:38,723: t15.2025.03.16 val PER: 0.3534
2026-01-08 13:40:38,724: t15.2025.03.30 val PER: 0.5023
2026-01-08 13:40:38,726: t15.2025.04.13 val PER: 0.3909
2026-01-08 13:40:38,728: New best val WER(5gram) 29.14% --> 27.71%
2026-01-08 13:40:38,729: Checkpointing model
2026-01-08 13:40:38,877: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:40:47,589: Train batch 2600: loss: 34.34 grad norm: 81.22 time: 0.055
2026-01-08 13:41:04,771: Train batch 2800: loss: 25.64 grad norm: 72.12 time: 0.082
2026-01-08 13:41:22,051: Train batch 3000: loss: 30.63 grad norm: 73.84 time: 0.083
2026-01-08 13:41:22,054: Running test after training batch: 3000
2026-01-08 13:41:22,151: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:41:26,937: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 13:41:27,006: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost of
2026-01-08 13:41:41,261: Val batch 3000: PER (avg): 0.2805 CTC Loss (avg): 27.6790 WER(5gram): 24.64% (n=256) time: 19.205
2026-01-08 13:41:41,263: WER lens: avg_true_words=5.99 avg_pred_words=6.04 max_pred_words=12
2026-01-08 13:41:41,266: t15.2023.08.13 val PER: 0.2599
2026-01-08 13:41:41,268: t15.2023.08.18 val PER: 0.2196
2026-01-08 13:41:41,269: t15.2023.08.20 val PER: 0.2160
2026-01-08 13:41:41,271: t15.2023.08.25 val PER: 0.2003
2026-01-08 13:41:41,272: t15.2023.08.27 val PER: 0.2862
2026-01-08 13:41:41,274: t15.2023.09.01 val PER: 0.1948
2026-01-08 13:41:41,275: t15.2023.09.03 val PER: 0.2732
2026-01-08 13:41:41,276: t15.2023.09.24 val PER: 0.2112
2026-01-08 13:41:41,278: t15.2023.09.29 val PER: 0.2355
2026-01-08 13:41:41,279: t15.2023.10.01 val PER: 0.2913
2026-01-08 13:41:41,281: t15.2023.10.06 val PER: 0.1895
2026-01-08 13:41:41,282: t15.2023.10.08 val PER: 0.3464
2026-01-08 13:41:41,284: t15.2023.10.13 val PER: 0.3429
2026-01-08 13:41:41,285: t15.2023.10.15 val PER: 0.2709
2026-01-08 13:41:41,286: t15.2023.10.20 val PER: 0.2617
2026-01-08 13:41:41,288: t15.2023.10.22 val PER: 0.2138
2026-01-08 13:41:41,289: t15.2023.11.03 val PER: 0.2727
2026-01-08 13:41:41,291: t15.2023.11.04 val PER: 0.0751
2026-01-08 13:41:41,292: t15.2023.11.17 val PER: 0.1306
2026-01-08 13:41:41,293: t15.2023.11.19 val PER: 0.1158
2026-01-08 13:41:41,295: t15.2023.11.26 val PER: 0.3036
2026-01-08 13:41:41,296: t15.2023.12.03 val PER: 0.2626
2026-01-08 13:41:41,298: t15.2023.12.08 val PER: 0.2590
2026-01-08 13:41:41,299: t15.2023.12.10 val PER: 0.1997
2026-01-08 13:41:41,301: t15.2023.12.17 val PER: 0.2765
2026-01-08 13:41:41,302: t15.2023.12.29 val PER: 0.2807
2026-01-08 13:41:41,303: t15.2024.02.25 val PER: 0.2317
2026-01-08 13:41:41,305: t15.2024.03.08 val PER: 0.3514
2026-01-08 13:41:41,306: t15.2024.03.15 val PER: 0.3277
2026-01-08 13:41:41,308: t15.2024.03.17 val PER: 0.2901
2026-01-08 13:41:41,309: t15.2024.05.10 val PER: 0.2883
2026-01-08 13:41:41,311: t15.2024.06.14 val PER: 0.3013
2026-01-08 13:41:41,312: t15.2024.07.19 val PER: 0.3922
2026-01-08 13:41:41,313: t15.2024.07.21 val PER: 0.2248
2026-01-08 13:41:41,315: t15.2024.07.28 val PER: 0.2801
2026-01-08 13:41:41,316: t15.2025.01.10 val PER: 0.4752
2026-01-08 13:41:41,318: t15.2025.01.12 val PER: 0.3341
2026-01-08 13:41:41,319: t15.2025.03.14 val PER: 0.4615
2026-01-08 13:41:41,320: t15.2025.03.16 val PER: 0.3259
2026-01-08 13:41:41,322: t15.2025.03.30 val PER: 0.4885
2026-01-08 13:41:41,323: t15.2025.04.13 val PER: 0.3566
2026-01-08 13:41:41,327: New best val WER(5gram) 27.71% --> 24.64%
2026-01-08 13:41:41,328: Checkpointing model
2026-01-08 13:41:41,478: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:41:58,477: Train batch 3200: loss: 25.85 grad norm: 67.03 time: 0.077
2026-01-08 13:42:15,570: Train batch 3400: loss: 17.80 grad norm: 55.06 time: 0.049
2026-01-08 13:42:24,394: Running test after training batch: 3500
2026-01-08 13:42:24,527: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:42:29,319: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 13:42:29,371: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the us in
2026-01-08 13:42:42,641: Val batch 3500: PER (avg): 0.2647 CTC Loss (avg): 26.4653 WER(5gram): 23.40% (n=256) time: 18.246
2026-01-08 13:42:42,644: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-08 13:42:42,646: t15.2023.08.13 val PER: 0.2349
2026-01-08 13:42:42,648: t15.2023.08.18 val PER: 0.2037
2026-01-08 13:42:42,649: t15.2023.08.20 val PER: 0.2184
2026-01-08 13:42:42,651: t15.2023.08.25 val PER: 0.1913
2026-01-08 13:42:42,652: t15.2023.08.27 val PER: 0.2781
2026-01-08 13:42:42,654: t15.2023.09.01 val PER: 0.1672
2026-01-08 13:42:42,655: t15.2023.09.03 val PER: 0.2447
2026-01-08 13:42:42,657: t15.2023.09.24 val PER: 0.2136
2026-01-08 13:42:42,659: t15.2023.09.29 val PER: 0.2214
2026-01-08 13:42:42,660: t15.2023.10.01 val PER: 0.2781
2026-01-08 13:42:42,661: t15.2023.10.06 val PER: 0.1776
2026-01-08 13:42:42,663: t15.2023.10.08 val PER: 0.3545
2026-01-08 13:42:42,664: t15.2023.10.13 val PER: 0.3196
2026-01-08 13:42:42,666: t15.2023.10.15 val PER: 0.2439
2026-01-08 13:42:42,667: t15.2023.10.20 val PER: 0.2517
2026-01-08 13:42:42,669: t15.2023.10.22 val PER: 0.2038
2026-01-08 13:42:42,670: t15.2023.11.03 val PER: 0.2585
2026-01-08 13:42:42,672: t15.2023.11.04 val PER: 0.0717
2026-01-08 13:42:42,674: t15.2023.11.17 val PER: 0.1166
2026-01-08 13:42:42,675: t15.2023.11.19 val PER: 0.0978
2026-01-08 13:42:42,676: t15.2023.11.26 val PER: 0.2891
2026-01-08 13:42:42,678: t15.2023.12.03 val PER: 0.2332
2026-01-08 13:42:42,679: t15.2023.12.08 val PER: 0.2463
2026-01-08 13:42:42,680: t15.2023.12.10 val PER: 0.2037
2026-01-08 13:42:42,682: t15.2023.12.17 val PER: 0.2526
2026-01-08 13:42:42,685: t15.2023.12.29 val PER: 0.2663
2026-01-08 13:42:42,687: t15.2024.02.25 val PER: 0.2008
2026-01-08 13:42:42,688: t15.2024.03.08 val PER: 0.3385
2026-01-08 13:42:42,689: t15.2024.03.15 val PER: 0.3096
2026-01-08 13:42:42,691: t15.2024.03.17 val PER: 0.2824
2026-01-08 13:42:42,692: t15.2024.05.10 val PER: 0.2615
2026-01-08 13:42:42,694: t15.2024.06.14 val PER: 0.2729
2026-01-08 13:42:42,695: t15.2024.07.19 val PER: 0.3942
2026-01-08 13:42:42,697: t15.2024.07.21 val PER: 0.2124
2026-01-08 13:42:42,698: t15.2024.07.28 val PER: 0.2662
2026-01-08 13:42:42,699: t15.2025.01.10 val PER: 0.4490
2026-01-08 13:42:42,701: t15.2025.01.12 val PER: 0.2918
2026-01-08 13:42:42,702: t15.2025.03.14 val PER: 0.4408
2026-01-08 13:42:42,703: t15.2025.03.16 val PER: 0.3272
2026-01-08 13:42:42,705: t15.2025.03.30 val PER: 0.4632
2026-01-08 13:42:42,706: t15.2025.04.13 val PER: 0.3338
2026-01-08 13:42:42,708: New best val WER(5gram) 24.64% --> 23.40%
2026-01-08 13:42:42,709: Checkpointing model
2026-01-08 13:42:42,856: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:42:51,541: Train batch 3600: loss: 22.45 grad norm: 64.94 time: 0.067
2026-01-08 13:43:08,422: Train batch 3800: loss: 25.63 grad norm: 70.48 time: 0.069
2026-01-08 13:43:25,909: Train batch 4000: loss: 19.17 grad norm: 56.73 time: 0.058
2026-01-08 13:43:25,911: Running test after training batch: 4000
2026-01-08 13:43:26,063: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:43:30,851: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 13:43:30,910: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost net
2026-01-08 13:43:43,889: Val batch 4000: PER (avg): 0.2452 CTC Loss (avg): 24.3962 WER(5gram): 24.64% (n=256) time: 17.975
2026-01-08 13:43:43,891: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=12
2026-01-08 13:43:43,893: t15.2023.08.13 val PER: 0.2245
2026-01-08 13:43:43,894: t15.2023.08.18 val PER: 0.2012
2026-01-08 13:43:43,896: t15.2023.08.20 val PER: 0.1970
2026-01-08 13:43:43,898: t15.2023.08.25 val PER: 0.1491
2026-01-08 13:43:43,899: t15.2023.08.27 val PER: 0.2733
2026-01-08 13:43:43,901: t15.2023.09.01 val PER: 0.1607
2026-01-08 13:43:43,902: t15.2023.09.03 val PER: 0.2363
2026-01-08 13:43:43,904: t15.2023.09.24 val PER: 0.1881
2026-01-08 13:43:43,906: t15.2023.09.29 val PER: 0.2004
2026-01-08 13:43:43,908: t15.2023.10.01 val PER: 0.2583
2026-01-08 13:43:43,909: t15.2023.10.06 val PER: 0.1529
2026-01-08 13:43:43,911: t15.2023.10.08 val PER: 0.3369
2026-01-08 13:43:43,913: t15.2023.10.13 val PER: 0.3010
2026-01-08 13:43:43,914: t15.2023.10.15 val PER: 0.2353
2026-01-08 13:43:43,916: t15.2023.10.20 val PER: 0.2248
2026-01-08 13:43:43,917: t15.2023.10.22 val PER: 0.1904
2026-01-08 13:43:43,919: t15.2023.11.03 val PER: 0.2408
2026-01-08 13:43:43,920: t15.2023.11.04 val PER: 0.0751
2026-01-08 13:43:43,922: t15.2023.11.17 val PER: 0.0995
2026-01-08 13:43:43,923: t15.2023.11.19 val PER: 0.0978
2026-01-08 13:43:43,925: t15.2023.11.26 val PER: 0.2471
2026-01-08 13:43:43,926: t15.2023.12.03 val PER: 0.2048
2026-01-08 13:43:43,928: t15.2023.12.08 val PER: 0.2144
2026-01-08 13:43:43,929: t15.2023.12.10 val PER: 0.1853
2026-01-08 13:43:43,931: t15.2023.12.17 val PER: 0.2360
2026-01-08 13:43:43,932: t15.2023.12.29 val PER: 0.2512
2026-01-08 13:43:43,934: t15.2024.02.25 val PER: 0.2177
2026-01-08 13:43:43,935: t15.2024.03.08 val PER: 0.3371
2026-01-08 13:43:43,937: t15.2024.03.15 val PER: 0.3002
2026-01-08 13:43:43,939: t15.2024.03.17 val PER: 0.2580
2026-01-08 13:43:43,940: t15.2024.05.10 val PER: 0.2645
2026-01-08 13:43:43,942: t15.2024.06.14 val PER: 0.2666
2026-01-08 13:43:43,944: t15.2024.07.19 val PER: 0.3487
2026-01-08 13:43:43,945: t15.2024.07.21 val PER: 0.1814
2026-01-08 13:43:43,947: t15.2024.07.28 val PER: 0.2316
2026-01-08 13:43:43,948: t15.2025.01.10 val PER: 0.4229
2026-01-08 13:43:43,950: t15.2025.01.12 val PER: 0.2810
2026-01-08 13:43:43,951: t15.2025.03.14 val PER: 0.4157
2026-01-08 13:43:43,953: t15.2025.03.16 val PER: 0.3076
2026-01-08 13:43:43,954: t15.2025.03.30 val PER: 0.4138
2026-01-08 13:43:43,955: t15.2025.04.13 val PER: 0.3124
2026-01-08 13:44:02,011: Train batch 4200: loss: 22.46 grad norm: 65.40 time: 0.081
2026-01-08 13:44:20,267: Train batch 4400: loss: 16.21 grad norm: 52.30 time: 0.067
2026-01-08 13:44:29,337: Running test after training batch: 4500
2026-01-08 13:44:29,462: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:44:34,260: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:44:34,321: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost net
2026-01-08 13:44:46,787: Val batch 4500: PER (avg): 0.2378 CTC Loss (avg): 23.1412 WER(5gram): 22.62% (n=256) time: 17.448
2026-01-08 13:44:46,790: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 13:44:46,791: t15.2023.08.13 val PER: 0.2058
2026-01-08 13:44:46,793: t15.2023.08.18 val PER: 0.1794
2026-01-08 13:44:46,794: t15.2023.08.20 val PER: 0.1906
2026-01-08 13:44:46,796: t15.2023.08.25 val PER: 0.1401
2026-01-08 13:44:46,797: t15.2023.08.27 val PER: 0.2717
2026-01-08 13:44:46,798: t15.2023.09.01 val PER: 0.1518
2026-01-08 13:44:46,800: t15.2023.09.03 val PER: 0.2435
2026-01-08 13:44:46,801: t15.2023.09.24 val PER: 0.1784
2026-01-08 13:44:46,803: t15.2023.09.29 val PER: 0.1997
2026-01-08 13:44:46,804: t15.2023.10.01 val PER: 0.2655
2026-01-08 13:44:46,805: t15.2023.10.06 val PER: 0.1485
2026-01-08 13:44:46,807: t15.2023.10.08 val PER: 0.3153
2026-01-08 13:44:46,809: t15.2023.10.13 val PER: 0.2925
2026-01-08 13:44:46,811: t15.2023.10.15 val PER: 0.2301
2026-01-08 13:44:46,812: t15.2023.10.20 val PER: 0.2215
2026-01-08 13:44:46,814: t15.2023.10.22 val PER: 0.1871
2026-01-08 13:44:46,815: t15.2023.11.03 val PER: 0.2463
2026-01-08 13:44:46,816: t15.2023.11.04 val PER: 0.0580
2026-01-08 13:44:46,818: t15.2023.11.17 val PER: 0.1089
2026-01-08 13:44:46,819: t15.2023.11.19 val PER: 0.0898
2026-01-08 13:44:46,821: t15.2023.11.26 val PER: 0.2500
2026-01-08 13:44:46,822: t15.2023.12.03 val PER: 0.2153
2026-01-08 13:44:46,824: t15.2023.12.08 val PER: 0.2077
2026-01-08 13:44:46,825: t15.2023.12.10 val PER: 0.1853
2026-01-08 13:44:46,827: t15.2023.12.17 val PER: 0.2318
2026-01-08 13:44:46,828: t15.2023.12.29 val PER: 0.2519
2026-01-08 13:44:46,829: t15.2024.02.25 val PER: 0.1910
2026-01-08 13:44:46,831: t15.2024.03.08 val PER: 0.3044
2026-01-08 13:44:46,832: t15.2024.03.15 val PER: 0.2821
2026-01-08 13:44:46,834: t15.2024.03.17 val PER: 0.2455
2026-01-08 13:44:46,835: t15.2024.05.10 val PER: 0.2585
2026-01-08 13:44:46,836: t15.2024.06.14 val PER: 0.2587
2026-01-08 13:44:46,838: t15.2024.07.19 val PER: 0.3408
2026-01-08 13:44:46,839: t15.2024.07.21 val PER: 0.1697
2026-01-08 13:44:46,840: t15.2024.07.28 val PER: 0.2191
2026-01-08 13:44:46,842: t15.2025.01.10 val PER: 0.4118
2026-01-08 13:44:46,843: t15.2025.01.12 val PER: 0.2610
2026-01-08 13:44:46,844: t15.2025.03.14 val PER: 0.4009
2026-01-08 13:44:46,847: t15.2025.03.16 val PER: 0.3050
2026-01-08 13:44:46,848: t15.2025.03.30 val PER: 0.4080
2026-01-08 13:44:46,850: t15.2025.04.13 val PER: 0.2924
2026-01-08 13:44:46,852: New best val WER(5gram) 23.40% --> 22.62%
2026-01-08 13:44:46,853: Checkpointing model
2026-01-08 13:44:46,994: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:44:56,171: Train batch 4600: loss: 19.80 grad norm: 61.34 time: 0.063
2026-01-08 13:45:14,475: Train batch 4800: loss: 13.33 grad norm: 51.29 time: 0.065
2026-01-08 13:45:32,797: Train batch 5000: loss: 30.99 grad norm: 83.75 time: 0.064
2026-01-08 13:45:32,799: Running test after training batch: 5000
2026-01-08 13:45:32,915: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:45:37,967: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:45:38,015: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 13:45:50,216: Val batch 5000: PER (avg): 0.2243 CTC Loss (avg): 21.9057 WER(5gram): 22.95% (n=256) time: 17.415
2026-01-08 13:45:50,218: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-08 13:45:50,220: t15.2023.08.13 val PER: 0.2006
2026-01-08 13:45:50,221: t15.2023.08.18 val PER: 0.1660
2026-01-08 13:45:50,223: t15.2023.08.20 val PER: 0.1652
2026-01-08 13:45:50,225: t15.2023.08.25 val PER: 0.1250
2026-01-08 13:45:50,226: t15.2023.08.27 val PER: 0.2379
2026-01-08 13:45:50,228: t15.2023.09.01 val PER: 0.1356
2026-01-08 13:45:50,229: t15.2023.09.03 val PER: 0.2363
2026-01-08 13:45:50,231: t15.2023.09.24 val PER: 0.1857
2026-01-08 13:45:50,232: t15.2023.09.29 val PER: 0.1806
2026-01-08 13:45:50,234: t15.2023.10.01 val PER: 0.2338
2026-01-08 13:45:50,236: t15.2023.10.06 val PER: 0.1367
2026-01-08 13:45:50,237: t15.2023.10.08 val PER: 0.3058
2026-01-08 13:45:50,239: t15.2023.10.13 val PER: 0.2855
2026-01-08 13:45:50,240: t15.2023.10.15 val PER: 0.2136
2026-01-08 13:45:50,242: t15.2023.10.20 val PER: 0.2215
2026-01-08 13:45:50,243: t15.2023.10.22 val PER: 0.1704
2026-01-08 13:45:50,245: t15.2023.11.03 val PER: 0.2205
2026-01-08 13:45:50,247: t15.2023.11.04 val PER: 0.0512
2026-01-08 13:45:50,248: t15.2023.11.17 val PER: 0.0824
2026-01-08 13:45:50,250: t15.2023.11.19 val PER: 0.0818
2026-01-08 13:45:50,251: t15.2023.11.26 val PER: 0.2399
2026-01-08 13:45:50,253: t15.2023.12.03 val PER: 0.1996
2026-01-08 13:45:50,254: t15.2023.12.08 val PER: 0.1957
2026-01-08 13:45:50,256: t15.2023.12.10 val PER: 0.1669
2026-01-08 13:45:50,257: t15.2023.12.17 val PER: 0.2131
2026-01-08 13:45:50,259: t15.2023.12.29 val PER: 0.2224
2026-01-08 13:45:50,260: t15.2024.02.25 val PER: 0.1896
2026-01-08 13:45:50,262: t15.2024.03.08 val PER: 0.3186
2026-01-08 13:45:50,263: t15.2024.03.15 val PER: 0.2733
2026-01-08 13:45:50,265: t15.2024.03.17 val PER: 0.2343
2026-01-08 13:45:50,266: t15.2024.05.10 val PER: 0.2437
2026-01-08 13:45:50,268: t15.2024.06.14 val PER: 0.2508
2026-01-08 13:45:50,269: t15.2024.07.19 val PER: 0.3223
2026-01-08 13:45:50,271: t15.2024.07.21 val PER: 0.1800
2026-01-08 13:45:50,272: t15.2024.07.28 val PER: 0.2125
2026-01-08 13:45:50,274: t15.2025.01.10 val PER: 0.3884
2026-01-08 13:45:50,275: t15.2025.01.12 val PER: 0.2502
2026-01-08 13:45:50,277: t15.2025.03.14 val PER: 0.3891
2026-01-08 13:45:50,278: t15.2025.03.16 val PER: 0.2657
2026-01-08 13:45:50,280: t15.2025.03.30 val PER: 0.3977
2026-01-08 13:45:50,281: t15.2025.04.13 val PER: 0.3081
2026-01-08 13:46:08,583: Train batch 5200: loss: 16.07 grad norm: 58.98 time: 0.053
2026-01-08 13:46:27,149: Train batch 5400: loss: 17.32 grad norm: 57.46 time: 0.068
2026-01-08 13:46:36,394: Running test after training batch: 5500
2026-01-08 13:46:36,539: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:46:41,403: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 13:46:41,457: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 13:46:53,519: Val batch 5500: PER (avg): 0.2139 CTC Loss (avg): 20.9330 WER(5gram): 21.64% (n=256) time: 17.124
2026-01-08 13:46:53,522: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-08 13:46:53,523: t15.2023.08.13 val PER: 0.1809
2026-01-08 13:46:53,525: t15.2023.08.18 val PER: 0.1551
2026-01-08 13:46:53,526: t15.2023.08.20 val PER: 0.1676
2026-01-08 13:46:53,528: t15.2023.08.25 val PER: 0.1250
2026-01-08 13:46:53,529: t15.2023.08.27 val PER: 0.2363
2026-01-08 13:46:53,531: t15.2023.09.01 val PER: 0.1250
2026-01-08 13:46:53,532: t15.2023.09.03 val PER: 0.2090
2026-01-08 13:46:53,533: t15.2023.09.24 val PER: 0.1748
2026-01-08 13:46:53,535: t15.2023.09.29 val PER: 0.1729
2026-01-08 13:46:53,536: t15.2023.10.01 val PER: 0.2252
2026-01-08 13:46:53,538: t15.2023.10.06 val PER: 0.1378
2026-01-08 13:46:53,539: t15.2023.10.08 val PER: 0.3018
2026-01-08 13:46:53,540: t15.2023.10.13 val PER: 0.2785
2026-01-08 13:46:53,542: t15.2023.10.15 val PER: 0.2116
2026-01-08 13:46:53,543: t15.2023.10.20 val PER: 0.2416
2026-01-08 13:46:53,546: t15.2023.10.22 val PER: 0.1592
2026-01-08 13:46:53,547: t15.2023.11.03 val PER: 0.2144
2026-01-08 13:46:53,549: t15.2023.11.04 val PER: 0.0580
2026-01-08 13:46:53,550: t15.2023.11.17 val PER: 0.0778
2026-01-08 13:46:53,551: t15.2023.11.19 val PER: 0.0659
2026-01-08 13:46:53,553: t15.2023.11.26 val PER: 0.2210
2026-01-08 13:46:53,554: t15.2023.12.03 val PER: 0.1870
2026-01-08 13:46:53,555: t15.2023.12.08 val PER: 0.1824
2026-01-08 13:46:53,557: t15.2023.12.10 val PER: 0.1498
2026-01-08 13:46:53,558: t15.2023.12.17 val PER: 0.2079
2026-01-08 13:46:53,559: t15.2023.12.29 val PER: 0.2141
2026-01-08 13:46:53,560: t15.2024.02.25 val PER: 0.1854
2026-01-08 13:46:53,562: t15.2024.03.08 val PER: 0.2987
2026-01-08 13:46:53,563: t15.2024.03.15 val PER: 0.2633
2026-01-08 13:46:53,565: t15.2024.03.17 val PER: 0.2176
2026-01-08 13:46:53,566: t15.2024.05.10 val PER: 0.2392
2026-01-08 13:46:53,567: t15.2024.06.14 val PER: 0.2413
2026-01-08 13:46:53,569: t15.2024.07.19 val PER: 0.3144
2026-01-08 13:46:53,570: t15.2024.07.21 val PER: 0.1559
2026-01-08 13:46:53,572: t15.2024.07.28 val PER: 0.2110
2026-01-08 13:46:53,573: t15.2025.01.10 val PER: 0.3719
2026-01-08 13:46:53,574: t15.2025.01.12 val PER: 0.2348
2026-01-08 13:46:53,576: t15.2025.03.14 val PER: 0.3565
2026-01-08 13:46:53,577: t15.2025.03.16 val PER: 0.2709
2026-01-08 13:46:53,579: t15.2025.03.30 val PER: 0.3609
2026-01-08 13:46:53,580: t15.2025.04.13 val PER: 0.2967
2026-01-08 13:46:53,581: New best val WER(5gram) 22.62% --> 21.64%
2026-01-08 13:46:53,583: Checkpointing model
2026-01-08 13:46:53,731: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:47:02,590: Train batch 5600: loss: 19.11 grad norm: 62.88 time: 0.062
2026-01-08 13:47:20,084: Train batch 5800: loss: 13.76 grad norm: 59.70 time: 0.083
2026-01-08 13:47:37,762: Train batch 6000: loss: 13.85 grad norm: 55.66 time: 0.049
2026-01-08 13:47:37,764: Running test after training batch: 6000
2026-01-08 13:47:37,870: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:47:42,705: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:47:42,765: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 13:47:54,833: Val batch 6000: PER (avg): 0.2121 CTC Loss (avg): 20.7080 WER(5gram): 23.01% (n=256) time: 17.067
2026-01-08 13:47:54,836: WER lens: avg_true_words=5.99 avg_pred_words=6.26 max_pred_words=12
2026-01-08 13:47:54,838: t15.2023.08.13 val PER: 0.1871
2026-01-08 13:47:54,839: t15.2023.08.18 val PER: 0.1727
2026-01-08 13:47:54,841: t15.2023.08.20 val PER: 0.1668
2026-01-08 13:47:54,842: t15.2023.08.25 val PER: 0.1175
2026-01-08 13:47:54,844: t15.2023.08.27 val PER: 0.2395
2026-01-08 13:47:54,845: t15.2023.09.01 val PER: 0.1323
2026-01-08 13:47:54,847: t15.2023.09.03 val PER: 0.2126
2026-01-08 13:47:54,848: t15.2023.09.24 val PER: 0.1553
2026-01-08 13:47:54,850: t15.2023.09.29 val PER: 0.1717
2026-01-08 13:47:54,851: t15.2023.10.01 val PER: 0.2292
2026-01-08 13:47:54,853: t15.2023.10.06 val PER: 0.1281
2026-01-08 13:47:54,854: t15.2023.10.08 val PER: 0.2936
2026-01-08 13:47:54,856: t15.2023.10.13 val PER: 0.2614
2026-01-08 13:47:54,857: t15.2023.10.15 val PER: 0.2149
2026-01-08 13:47:54,859: t15.2023.10.20 val PER: 0.2081
2026-01-08 13:47:54,860: t15.2023.10.22 val PER: 0.1637
2026-01-08 13:47:54,862: t15.2023.11.03 val PER: 0.2320
2026-01-08 13:47:54,863: t15.2023.11.04 val PER: 0.0478
2026-01-08 13:47:54,865: t15.2023.11.17 val PER: 0.0731
2026-01-08 13:47:54,866: t15.2023.11.19 val PER: 0.0699
2026-01-08 13:47:54,868: t15.2023.11.26 val PER: 0.2246
2026-01-08 13:47:54,869: t15.2023.12.03 val PER: 0.1733
2026-01-08 13:47:54,870: t15.2023.12.08 val PER: 0.1731
2026-01-08 13:47:54,872: t15.2023.12.10 val PER: 0.1432
2026-01-08 13:47:54,873: t15.2023.12.17 val PER: 0.2110
2026-01-08 13:47:54,874: t15.2023.12.29 val PER: 0.2121
2026-01-08 13:47:54,876: t15.2024.02.25 val PER: 0.1671
2026-01-08 13:47:54,877: t15.2024.03.08 val PER: 0.2902
2026-01-08 13:47:54,878: t15.2024.03.15 val PER: 0.2670
2026-01-08 13:47:54,880: t15.2024.03.17 val PER: 0.2099
2026-01-08 13:47:54,881: t15.2024.05.10 val PER: 0.2288
2026-01-08 13:47:54,883: t15.2024.06.14 val PER: 0.2177
2026-01-08 13:47:54,884: t15.2024.07.19 val PER: 0.3125
2026-01-08 13:47:54,886: t15.2024.07.21 val PER: 0.1648
2026-01-08 13:47:54,887: t15.2024.07.28 val PER: 0.2051
2026-01-08 13:47:54,888: t15.2025.01.10 val PER: 0.3636
2026-01-08 13:47:54,890: t15.2025.01.12 val PER: 0.2256
2026-01-08 13:47:54,891: t15.2025.03.14 val PER: 0.3831
2026-01-08 13:47:54,892: t15.2025.03.16 val PER: 0.2513
2026-01-08 13:47:54,894: t15.2025.03.30 val PER: 0.3770
2026-01-08 13:47:54,895: t15.2025.04.13 val PER: 0.2839
2026-01-08 13:48:13,285: Train batch 6200: loss: 16.39 grad norm: 59.67 time: 0.073
2026-01-08 13:48:30,891: Train batch 6400: loss: 19.05 grad norm: 67.97 time: 0.063
2026-01-08 13:48:39,356: Running test after training batch: 6500
2026-01-08 13:48:39,476: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:48:44,260: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:48:44,314: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost in
2026-01-08 13:48:55,599: Val batch 6500: PER (avg): 0.2001 CTC Loss (avg): 19.8224 WER(5gram): 18.51% (n=256) time: 16.241
2026-01-08 13:48:55,601: WER lens: avg_true_words=5.99 avg_pred_words=6.16 max_pred_words=12
2026-01-08 13:48:55,603: t15.2023.08.13 val PER: 0.1705
2026-01-08 13:48:55,604: t15.2023.08.18 val PER: 0.1459
2026-01-08 13:48:55,606: t15.2023.08.20 val PER: 0.1525
2026-01-08 13:48:55,607: t15.2023.08.25 val PER: 0.1145
2026-01-08 13:48:55,609: t15.2023.08.27 val PER: 0.2186
2026-01-08 13:48:55,610: t15.2023.09.01 val PER: 0.1201
2026-01-08 13:48:55,611: t15.2023.09.03 val PER: 0.2055
2026-01-08 13:48:55,614: t15.2023.09.24 val PER: 0.1578
2026-01-08 13:48:55,615: t15.2023.09.29 val PER: 0.1704
2026-01-08 13:48:55,617: t15.2023.10.01 val PER: 0.2120
2026-01-08 13:48:55,619: t15.2023.10.06 val PER: 0.1249
2026-01-08 13:48:55,620: t15.2023.10.08 val PER: 0.3072
2026-01-08 13:48:55,621: t15.2023.10.13 val PER: 0.2661
2026-01-08 13:48:55,623: t15.2023.10.15 val PER: 0.2050
2026-01-08 13:48:55,624: t15.2023.10.20 val PER: 0.1946
2026-01-08 13:48:55,625: t15.2023.10.22 val PER: 0.1470
2026-01-08 13:48:55,627: t15.2023.11.03 val PER: 0.2056
2026-01-08 13:48:55,628: t15.2023.11.04 val PER: 0.0512
2026-01-08 13:48:55,629: t15.2023.11.17 val PER: 0.0715
2026-01-08 13:48:55,631: t15.2023.11.19 val PER: 0.0719
2026-01-08 13:48:55,632: t15.2023.11.26 val PER: 0.2065
2026-01-08 13:48:55,634: t15.2023.12.03 val PER: 0.1691
2026-01-08 13:48:55,635: t15.2023.12.08 val PER: 0.1638
2026-01-08 13:48:55,636: t15.2023.12.10 val PER: 0.1432
2026-01-08 13:48:55,638: t15.2023.12.17 val PER: 0.1819
2026-01-08 13:48:55,639: t15.2023.12.29 val PER: 0.1887
2026-01-08 13:48:55,640: t15.2024.02.25 val PER: 0.1629
2026-01-08 13:48:55,641: t15.2024.03.08 val PER: 0.2945
2026-01-08 13:48:55,643: t15.2024.03.15 val PER: 0.2589
2026-01-08 13:48:55,644: t15.2024.03.17 val PER: 0.1974
2026-01-08 13:48:55,646: t15.2024.05.10 val PER: 0.2155
2026-01-08 13:48:55,647: t15.2024.06.14 val PER: 0.2114
2026-01-08 13:48:55,648: t15.2024.07.19 val PER: 0.2947
2026-01-08 13:48:55,650: t15.2024.07.21 val PER: 0.1434
2026-01-08 13:48:55,651: t15.2024.07.28 val PER: 0.1765
2026-01-08 13:48:55,653: t15.2025.01.10 val PER: 0.3554
2026-01-08 13:48:55,654: t15.2025.01.12 val PER: 0.2217
2026-01-08 13:48:55,656: t15.2025.03.14 val PER: 0.3580
2026-01-08 13:48:55,657: t15.2025.03.16 val PER: 0.2408
2026-01-08 13:48:55,658: t15.2025.03.30 val PER: 0.3540
2026-01-08 13:48:55,659: t15.2025.04.13 val PER: 0.2710
2026-01-08 13:48:55,661: New best val WER(5gram) 21.64% --> 18.51%
2026-01-08 13:48:55,662: Checkpointing model
2026-01-08 13:48:55,806: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:49:04,288: Train batch 6600: loss: 12.09 grad norm: 52.54 time: 0.045
2026-01-08 13:49:22,240: Train batch 6800: loss: 15.20 grad norm: 60.37 time: 0.050
2026-01-08 13:49:40,671: Train batch 7000: loss: 16.52 grad norm: 62.54 time: 0.061
2026-01-08 13:49:40,673: Running test after training batch: 7000
2026-01-08 13:49:40,825: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:49:45,577: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:49:45,620: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 13:49:56,633: Val batch 7000: PER (avg): 0.1945 CTC Loss (avg): 19.2089 WER(5gram): 19.75% (n=256) time: 15.958
2026-01-08 13:49:56,635: WER lens: avg_true_words=5.99 avg_pred_words=6.22 max_pred_words=12
2026-01-08 13:49:56,637: t15.2023.08.13 val PER: 0.1559
2026-01-08 13:49:56,638: t15.2023.08.18 val PER: 0.1450
2026-01-08 13:49:56,640: t15.2023.08.20 val PER: 0.1446
2026-01-08 13:49:56,641: t15.2023.08.25 val PER: 0.1009
2026-01-08 13:49:56,643: t15.2023.08.27 val PER: 0.2122
2026-01-08 13:49:56,644: t15.2023.09.01 val PER: 0.1128
2026-01-08 13:49:56,646: t15.2023.09.03 val PER: 0.1888
2026-01-08 13:49:56,647: t15.2023.09.24 val PER: 0.1638
2026-01-08 13:49:56,648: t15.2023.09.29 val PER: 0.1672
2026-01-08 13:49:56,650: t15.2023.10.01 val PER: 0.2067
2026-01-08 13:49:56,651: t15.2023.10.06 val PER: 0.1141
2026-01-08 13:49:56,653: t15.2023.10.08 val PER: 0.2842
2026-01-08 13:49:56,655: t15.2023.10.13 val PER: 0.2576
2026-01-08 13:49:56,656: t15.2023.10.15 val PER: 0.1879
2026-01-08 13:49:56,657: t15.2023.10.20 val PER: 0.2081
2026-01-08 13:49:56,659: t15.2023.10.22 val PER: 0.1492
2026-01-08 13:49:56,660: t15.2023.11.03 val PER: 0.2096
2026-01-08 13:49:56,663: t15.2023.11.04 val PER: 0.0410
2026-01-08 13:49:56,664: t15.2023.11.17 val PER: 0.0622
2026-01-08 13:49:56,666: t15.2023.11.19 val PER: 0.0599
2026-01-08 13:49:56,667: t15.2023.11.26 val PER: 0.2014
2026-01-08 13:49:56,669: t15.2023.12.03 val PER: 0.1628
2026-01-08 13:49:56,670: t15.2023.12.08 val PER: 0.1618
2026-01-08 13:49:56,671: t15.2023.12.10 val PER: 0.1353
2026-01-08 13:49:56,673: t15.2023.12.17 val PER: 0.1840
2026-01-08 13:49:56,674: t15.2023.12.29 val PER: 0.1935
2026-01-08 13:49:56,676: t15.2024.02.25 val PER: 0.1545
2026-01-08 13:49:56,677: t15.2024.03.08 val PER: 0.2802
2026-01-08 13:49:56,679: t15.2024.03.15 val PER: 0.2427
2026-01-08 13:49:56,680: t15.2024.03.17 val PER: 0.2029
2026-01-08 13:49:56,682: t15.2024.05.10 val PER: 0.2095
2026-01-08 13:49:56,683: t15.2024.06.14 val PER: 0.2114
2026-01-08 13:49:56,687: t15.2024.07.19 val PER: 0.3072
2026-01-08 13:49:56,688: t15.2024.07.21 val PER: 0.1345
2026-01-08 13:49:56,690: t15.2024.07.28 val PER: 0.1735
2026-01-08 13:49:56,691: t15.2025.01.10 val PER: 0.3264
2026-01-08 13:49:56,692: t15.2025.01.12 val PER: 0.2071
2026-01-08 13:49:56,694: t15.2025.03.14 val PER: 0.3580
2026-01-08 13:49:56,695: t15.2025.03.16 val PER: 0.2356
2026-01-08 13:49:56,697: t15.2025.03.30 val PER: 0.3540
2026-01-08 13:49:56,699: t15.2025.04.13 val PER: 0.2653
2026-01-08 13:50:15,006: Train batch 7200: loss: 14.27 grad norm: 58.00 time: 0.080
2026-01-08 13:50:33,202: Train batch 7400: loss: 13.31 grad norm: 55.57 time: 0.076
2026-01-08 13:50:42,317: Running test after training batch: 7500
2026-01-08 13:50:42,504: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:50:47,222: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:50:47,267: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 13:50:58,261: Val batch 7500: PER (avg): 0.1880 CTC Loss (avg): 18.6204 WER(5gram): 17.21% (n=256) time: 15.942
2026-01-08 13:50:58,263: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 13:50:58,268: t15.2023.08.13 val PER: 0.1351
2026-01-08 13:50:58,270: t15.2023.08.18 val PER: 0.1459
2026-01-08 13:50:58,272: t15.2023.08.20 val PER: 0.1461
2026-01-08 13:50:58,273: t15.2023.08.25 val PER: 0.0949
2026-01-08 13:50:58,275: t15.2023.08.27 val PER: 0.2106
2026-01-08 13:50:58,276: t15.2023.09.01 val PER: 0.1169
2026-01-08 13:50:58,278: t15.2023.09.03 val PER: 0.1888
2026-01-08 13:50:58,279: t15.2023.09.24 val PER: 0.1493
2026-01-08 13:50:58,281: t15.2023.09.29 val PER: 0.1544
2026-01-08 13:50:58,282: t15.2023.10.01 val PER: 0.2021
2026-01-08 13:50:58,284: t15.2023.10.06 val PER: 0.1119
2026-01-08 13:50:58,286: t15.2023.10.08 val PER: 0.2733
2026-01-08 13:50:58,287: t15.2023.10.13 val PER: 0.2459
2026-01-08 13:50:58,289: t15.2023.10.15 val PER: 0.1938
2026-01-08 13:50:58,290: t15.2023.10.20 val PER: 0.2013
2026-01-08 13:50:58,292: t15.2023.10.22 val PER: 0.1470
2026-01-08 13:50:58,294: t15.2023.11.03 val PER: 0.2103
2026-01-08 13:50:58,295: t15.2023.11.04 val PER: 0.0478
2026-01-08 13:50:58,297: t15.2023.11.17 val PER: 0.0684
2026-01-08 13:50:58,299: t15.2023.11.19 val PER: 0.0539
2026-01-08 13:50:58,300: t15.2023.11.26 val PER: 0.1833
2026-01-08 13:50:58,302: t15.2023.12.03 val PER: 0.1481
2026-01-08 13:50:58,303: t15.2023.12.08 val PER: 0.1491
2026-01-08 13:50:58,305: t15.2023.12.10 val PER: 0.1301
2026-01-08 13:50:58,306: t15.2023.12.17 val PER: 0.1778
2026-01-08 13:50:58,308: t15.2023.12.29 val PER: 0.1791
2026-01-08 13:50:58,309: t15.2024.02.25 val PER: 0.1390
2026-01-08 13:50:58,311: t15.2024.03.08 val PER: 0.2802
2026-01-08 13:50:58,312: t15.2024.03.15 val PER: 0.2395
2026-01-08 13:50:58,314: t15.2024.03.17 val PER: 0.1729
2026-01-08 13:50:58,315: t15.2024.05.10 val PER: 0.1976
2026-01-08 13:50:58,316: t15.2024.06.14 val PER: 0.1940
2026-01-08 13:50:58,318: t15.2024.07.19 val PER: 0.2960
2026-01-08 13:50:58,319: t15.2024.07.21 val PER: 0.1359
2026-01-08 13:50:58,321: t15.2024.07.28 val PER: 0.1632
2026-01-08 13:50:58,322: t15.2025.01.10 val PER: 0.3471
2026-01-08 13:50:58,324: t15.2025.01.12 val PER: 0.2002
2026-01-08 13:50:58,325: t15.2025.03.14 val PER: 0.3669
2026-01-08 13:50:58,327: t15.2025.03.16 val PER: 0.2330
2026-01-08 13:50:58,328: t15.2025.03.30 val PER: 0.3552
2026-01-08 13:50:58,329: t15.2025.04.13 val PER: 0.2468
2026-01-08 13:50:58,331: New best val WER(5gram) 18.51% --> 17.21%
2026-01-08 13:50:58,332: Checkpointing model
2026-01-08 13:50:58,483: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:51:07,125: Train batch 7600: loss: 15.65 grad norm: 58.45 time: 0.069
2026-01-08 13:51:24,403: Train batch 7800: loss: 13.62 grad norm: 56.70 time: 0.056
2026-01-08 13:51:42,569: Train batch 8000: loss: 10.72 grad norm: 52.47 time: 0.073
2026-01-08 13:51:42,571: Running test after training batch: 8000
2026-01-08 13:51:42,675: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:51:47,400: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:51:47,446: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 13:51:58,446: Val batch 8000: PER (avg): 0.1826 CTC Loss (avg): 18.0793 WER(5gram): 19.43% (n=256) time: 15.872
2026-01-08 13:51:58,448: WER lens: avg_true_words=5.99 avg_pred_words=6.25 max_pred_words=12
2026-01-08 13:51:58,450: t15.2023.08.13 val PER: 0.1518
2026-01-08 13:51:58,452: t15.2023.08.18 val PER: 0.1308
2026-01-08 13:51:58,453: t15.2023.08.20 val PER: 0.1430
2026-01-08 13:51:58,456: t15.2023.08.25 val PER: 0.1024
2026-01-08 13:51:58,457: t15.2023.08.27 val PER: 0.2090
2026-01-08 13:51:58,459: t15.2023.09.01 val PER: 0.0998
2026-01-08 13:51:58,460: t15.2023.09.03 val PER: 0.1936
2026-01-08 13:51:58,462: t15.2023.09.24 val PER: 0.1566
2026-01-08 13:51:58,463: t15.2023.09.29 val PER: 0.1493
2026-01-08 13:51:58,464: t15.2023.10.01 val PER: 0.1968
2026-01-08 13:51:58,466: t15.2023.10.06 val PER: 0.1076
2026-01-08 13:51:58,468: t15.2023.10.08 val PER: 0.2652
2026-01-08 13:51:58,470: t15.2023.10.13 val PER: 0.2436
2026-01-08 13:51:58,471: t15.2023.10.15 val PER: 0.1852
2026-01-08 13:51:58,472: t15.2023.10.20 val PER: 0.2047
2026-01-08 13:51:58,474: t15.2023.10.22 val PER: 0.1459
2026-01-08 13:51:58,475: t15.2023.11.03 val PER: 0.1940
2026-01-08 13:51:58,476: t15.2023.11.04 val PER: 0.0444
2026-01-08 13:51:58,478: t15.2023.11.17 val PER: 0.0607
2026-01-08 13:51:58,479: t15.2023.11.19 val PER: 0.0559
2026-01-08 13:51:58,480: t15.2023.11.26 val PER: 0.1783
2026-01-08 13:51:58,482: t15.2023.12.03 val PER: 0.1544
2026-01-08 13:51:58,483: t15.2023.12.08 val PER: 0.1438
2026-01-08 13:51:58,484: t15.2023.12.10 val PER: 0.1314
2026-01-08 13:51:58,486: t15.2023.12.17 val PER: 0.1611
2026-01-08 13:51:58,487: t15.2023.12.29 val PER: 0.1736
2026-01-08 13:51:58,488: t15.2024.02.25 val PER: 0.1376
2026-01-08 13:51:58,489: t15.2024.03.08 val PER: 0.2617
2026-01-08 13:51:58,491: t15.2024.03.15 val PER: 0.2370
2026-01-08 13:51:58,492: t15.2024.03.17 val PER: 0.1729
2026-01-08 13:51:58,494: t15.2024.05.10 val PER: 0.1976
2026-01-08 13:51:58,495: t15.2024.06.14 val PER: 0.2019
2026-01-08 13:51:58,497: t15.2024.07.19 val PER: 0.2894
2026-01-08 13:51:58,498: t15.2024.07.21 val PER: 0.1159
2026-01-08 13:51:58,500: t15.2024.07.28 val PER: 0.1537
2026-01-08 13:51:58,501: t15.2025.01.10 val PER: 0.3306
2026-01-08 13:51:58,502: t15.2025.01.12 val PER: 0.1917
2026-01-08 13:51:58,503: t15.2025.03.14 val PER: 0.3698
2026-01-08 13:51:58,505: t15.2025.03.16 val PER: 0.2225
2026-01-08 13:51:58,507: t15.2025.03.30 val PER: 0.3425
2026-01-08 13:51:58,509: t15.2025.04.13 val PER: 0.2454
2026-01-08 13:52:16,462: Train batch 8200: loss: 9.76 grad norm: 52.09 time: 0.054
2026-01-08 13:52:34,845: Train batch 8400: loss: 9.99 grad norm: 49.57 time: 0.064
2026-01-08 13:52:44,325: Running test after training batch: 8500
2026-01-08 13:52:44,437: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:52:49,186: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:52:49,231: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 13:53:00,195: Val batch 8500: PER (avg): 0.1781 CTC Loss (avg): 17.7271 WER(5gram): 16.69% (n=256) time: 15.869
2026-01-08 13:53:00,198: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 13:53:00,200: t15.2023.08.13 val PER: 0.1320
2026-01-08 13:53:00,202: t15.2023.08.18 val PER: 0.1333
2026-01-08 13:53:00,204: t15.2023.08.20 val PER: 0.1319
2026-01-08 13:53:00,206: t15.2023.08.25 val PER: 0.1160
2026-01-08 13:53:00,207: t15.2023.08.27 val PER: 0.2186
2026-01-08 13:53:00,209: t15.2023.09.01 val PER: 0.1047
2026-01-08 13:53:00,211: t15.2023.09.03 val PER: 0.1924
2026-01-08 13:53:00,212: t15.2023.09.24 val PER: 0.1432
2026-01-08 13:53:00,214: t15.2023.09.29 val PER: 0.1551
2026-01-08 13:53:00,216: t15.2023.10.01 val PER: 0.1909
2026-01-08 13:53:00,218: t15.2023.10.06 val PER: 0.1098
2026-01-08 13:53:00,219: t15.2023.10.08 val PER: 0.2693
2026-01-08 13:53:00,221: t15.2023.10.13 val PER: 0.2289
2026-01-08 13:53:00,222: t15.2023.10.15 val PER: 0.1734
2026-01-08 13:53:00,224: t15.2023.10.20 val PER: 0.1913
2026-01-08 13:53:00,226: t15.2023.10.22 val PER: 0.1425
2026-01-08 13:53:00,227: t15.2023.11.03 val PER: 0.1879
2026-01-08 13:53:00,228: t15.2023.11.04 val PER: 0.0512
2026-01-08 13:53:00,230: t15.2023.11.17 val PER: 0.0529
2026-01-08 13:53:00,231: t15.2023.11.19 val PER: 0.0459
2026-01-08 13:53:00,232: t15.2023.11.26 val PER: 0.1754
2026-01-08 13:53:00,234: t15.2023.12.03 val PER: 0.1408
2026-01-08 13:53:00,235: t15.2023.12.08 val PER: 0.1378
2026-01-08 13:53:00,237: t15.2023.12.10 val PER: 0.1051
2026-01-08 13:53:00,238: t15.2023.12.17 val PER: 0.1632
2026-01-08 13:53:00,239: t15.2023.12.29 val PER: 0.1695
2026-01-08 13:53:00,241: t15.2024.02.25 val PER: 0.1348
2026-01-08 13:53:00,242: t15.2024.03.08 val PER: 0.2745
2026-01-08 13:53:00,244: t15.2024.03.15 val PER: 0.2383
2026-01-08 13:53:00,245: t15.2024.03.17 val PER: 0.1688
2026-01-08 13:53:00,248: t15.2024.05.10 val PER: 0.1947
2026-01-08 13:53:00,250: t15.2024.06.14 val PER: 0.1845
2026-01-08 13:53:00,251: t15.2024.07.19 val PER: 0.2742
2026-01-08 13:53:00,252: t15.2024.07.21 val PER: 0.1159
2026-01-08 13:53:00,254: t15.2024.07.28 val PER: 0.1684
2026-01-08 13:53:00,255: t15.2025.01.10 val PER: 0.3251
2026-01-08 13:53:00,257: t15.2025.01.12 val PER: 0.1801
2026-01-08 13:53:00,258: t15.2025.03.14 val PER: 0.3728
2026-01-08 13:53:00,259: t15.2025.03.16 val PER: 0.2055
2026-01-08 13:53:00,261: t15.2025.03.30 val PER: 0.3299
2026-01-08 13:53:00,262: t15.2025.04.13 val PER: 0.2354
2026-01-08 13:53:00,264: New best val WER(5gram) 17.21% --> 16.69%
2026-01-08 13:53:00,265: Checkpointing model
2026-01-08 13:53:00,406: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:53:09,166: Train batch 8600: loss: 15.79 grad norm: 61.15 time: 0.055
2026-01-08 13:53:26,360: Train batch 8800: loss: 14.59 grad norm: 54.24 time: 0.061
2026-01-08 13:53:43,873: Train batch 9000: loss: 15.73 grad norm: 66.71 time: 0.073
2026-01-08 13:53:43,875: Running test after training batch: 9000
2026-01-08 13:53:44,006: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:53:48,835: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:53:48,880: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 13:53:59,911: Val batch 9000: PER (avg): 0.1725 CTC Loss (avg): 17.3147 WER(5gram): 18.19% (n=256) time: 16.034
2026-01-08 13:53:59,914: WER lens: avg_true_words=5.99 avg_pred_words=6.24 max_pred_words=13
2026-01-08 13:53:59,915: t15.2023.08.13 val PER: 0.1310
2026-01-08 13:53:59,917: t15.2023.08.18 val PER: 0.1257
2026-01-08 13:53:59,919: t15.2023.08.20 val PER: 0.1303
2026-01-08 13:53:59,920: t15.2023.08.25 val PER: 0.0904
2026-01-08 13:53:59,922: t15.2023.08.27 val PER: 0.1913
2026-01-08 13:53:59,923: t15.2023.09.01 val PER: 0.0933
2026-01-08 13:53:59,925: t15.2023.09.03 val PER: 0.1817
2026-01-08 13:53:59,926: t15.2023.09.24 val PER: 0.1444
2026-01-08 13:53:59,928: t15.2023.09.29 val PER: 0.1417
2026-01-08 13:53:59,929: t15.2023.10.01 val PER: 0.1909
2026-01-08 13:53:59,930: t15.2023.10.06 val PER: 0.1001
2026-01-08 13:53:59,932: t15.2023.10.08 val PER: 0.2612
2026-01-08 13:53:59,934: t15.2023.10.13 val PER: 0.2265
2026-01-08 13:53:59,935: t15.2023.10.15 val PER: 0.1747
2026-01-08 13:53:59,937: t15.2023.10.20 val PER: 0.1980
2026-01-08 13:53:59,938: t15.2023.10.22 val PER: 0.1303
2026-01-08 13:53:59,940: t15.2023.11.03 val PER: 0.2035
2026-01-08 13:53:59,941: t15.2023.11.04 val PER: 0.0444
2026-01-08 13:53:59,942: t15.2023.11.17 val PER: 0.0482
2026-01-08 13:53:59,944: t15.2023.11.19 val PER: 0.0399
2026-01-08 13:53:59,945: t15.2023.11.26 val PER: 0.1681
2026-01-08 13:53:59,948: t15.2023.12.03 val PER: 0.1397
2026-01-08 13:53:59,949: t15.2023.12.08 val PER: 0.1265
2026-01-08 13:53:59,950: t15.2023.12.10 val PER: 0.1078
2026-01-08 13:53:59,952: t15.2023.12.17 val PER: 0.1580
2026-01-08 13:53:59,953: t15.2023.12.29 val PER: 0.1709
2026-01-08 13:53:59,954: t15.2024.02.25 val PER: 0.1376
2026-01-08 13:53:59,956: t15.2024.03.08 val PER: 0.2575
2026-01-08 13:53:59,957: t15.2024.03.15 val PER: 0.2276
2026-01-08 13:53:59,959: t15.2024.03.17 val PER: 0.1667
2026-01-08 13:53:59,960: t15.2024.05.10 val PER: 0.1813
2026-01-08 13:53:59,961: t15.2024.06.14 val PER: 0.1735
2026-01-08 13:53:59,963: t15.2024.07.19 val PER: 0.2650
2026-01-08 13:53:59,964: t15.2024.07.21 val PER: 0.1124
2026-01-08 13:53:59,965: t15.2024.07.28 val PER: 0.1581
2026-01-08 13:53:59,967: t15.2025.01.10 val PER: 0.3251
2026-01-08 13:53:59,968: t15.2025.01.12 val PER: 0.1755
2026-01-08 13:53:59,970: t15.2025.03.14 val PER: 0.3580
2026-01-08 13:53:59,972: t15.2025.03.16 val PER: 0.2147
2026-01-08 13:53:59,973: t15.2025.03.30 val PER: 0.3149
2026-01-08 13:53:59,975: t15.2025.04.13 val PER: 0.2354
2026-01-08 13:54:18,089: Train batch 9200: loss: 10.87 grad norm: 49.53 time: 0.062
2026-01-08 13:54:36,568: Train batch 9400: loss: 7.34 grad norm: 44.30 time: 0.070
2026-01-08 13:54:45,697: Running test after training batch: 9500
2026-01-08 13:54:45,862: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:54:50,889: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:54:50,930: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:55:01,811: Val batch 9500: PER (avg): 0.1706 CTC Loss (avg): 17.1037 WER(5gram): 17.34% (n=256) time: 16.111
2026-01-08 13:55:01,813: WER lens: avg_true_words=5.99 avg_pred_words=6.23 max_pred_words=12
2026-01-08 13:55:01,817: t15.2023.08.13 val PER: 0.1258
2026-01-08 13:55:01,819: t15.2023.08.18 val PER: 0.1199
2026-01-08 13:55:01,820: t15.2023.08.20 val PER: 0.1247
2026-01-08 13:55:01,822: t15.2023.08.25 val PER: 0.0949
2026-01-08 13:55:01,823: t15.2023.08.27 val PER: 0.2010
2026-01-08 13:55:01,824: t15.2023.09.01 val PER: 0.0998
2026-01-08 13:55:01,826: t15.2023.09.03 val PER: 0.1805
2026-01-08 13:55:01,827: t15.2023.09.24 val PER: 0.1420
2026-01-08 13:55:01,828: t15.2023.09.29 val PER: 0.1442
2026-01-08 13:55:01,830: t15.2023.10.01 val PER: 0.1836
2026-01-08 13:55:01,831: t15.2023.10.06 val PER: 0.1001
2026-01-08 13:55:01,832: t15.2023.10.08 val PER: 0.2530
2026-01-08 13:55:01,834: t15.2023.10.13 val PER: 0.2281
2026-01-08 13:55:01,835: t15.2023.10.15 val PER: 0.1813
2026-01-08 13:55:01,836: t15.2023.10.20 val PER: 0.1913
2026-01-08 13:55:01,838: t15.2023.10.22 val PER: 0.1381
2026-01-08 13:55:01,839: t15.2023.11.03 val PER: 0.1879
2026-01-08 13:55:01,841: t15.2023.11.04 val PER: 0.0410
2026-01-08 13:55:01,842: t15.2023.11.17 val PER: 0.0498
2026-01-08 13:55:01,843: t15.2023.11.19 val PER: 0.0559
2026-01-08 13:55:01,844: t15.2023.11.26 val PER: 0.1471
2026-01-08 13:55:01,846: t15.2023.12.03 val PER: 0.1376
2026-01-08 13:55:01,849: t15.2023.12.08 val PER: 0.1358
2026-01-08 13:55:01,850: t15.2023.12.10 val PER: 0.1156
2026-01-08 13:55:01,851: t15.2023.12.17 val PER: 0.1528
2026-01-08 13:55:01,853: t15.2023.12.29 val PER: 0.1592
2026-01-08 13:55:01,854: t15.2024.02.25 val PER: 0.1264
2026-01-08 13:55:01,855: t15.2024.03.08 val PER: 0.2575
2026-01-08 13:55:01,857: t15.2024.03.15 val PER: 0.2220
2026-01-08 13:55:01,858: t15.2024.03.17 val PER: 0.1611
2026-01-08 13:55:01,860: t15.2024.05.10 val PER: 0.1976
2026-01-08 13:55:01,861: t15.2024.06.14 val PER: 0.1703
2026-01-08 13:55:01,862: t15.2024.07.19 val PER: 0.2551
2026-01-08 13:55:01,864: t15.2024.07.21 val PER: 0.1159
2026-01-08 13:55:01,865: t15.2024.07.28 val PER: 0.1537
2026-01-08 13:55:01,866: t15.2025.01.10 val PER: 0.3113
2026-01-08 13:55:01,868: t15.2025.01.12 val PER: 0.1832
2026-01-08 13:55:01,869: t15.2025.03.14 val PER: 0.3683
2026-01-08 13:55:01,870: t15.2025.03.16 val PER: 0.1976
2026-01-08 13:55:01,872: t15.2025.03.30 val PER: 0.3172
2026-01-08 13:55:01,873: t15.2025.04.13 val PER: 0.2439
2026-01-08 13:55:10,567: Train batch 9600: loss: 8.30 grad norm: 46.80 time: 0.075
2026-01-08 13:55:28,616: Train batch 9800: loss: 11.60 grad norm: 56.71 time: 0.064
2026-01-08 13:55:47,319: Train batch 10000: loss: 5.06 grad norm: 35.77 time: 0.064
2026-01-08 13:55:47,321: Running test after training batch: 10000
2026-01-08 13:55:47,429: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:55:52,276: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:55:52,321: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 13:56:03,399: Val batch 10000: PER (avg): 0.1668 CTC Loss (avg): 16.7368 WER(5gram): 17.93% (n=256) time: 16.069
2026-01-08 13:56:03,401: WER lens: avg_true_words=5.99 avg_pred_words=6.23 max_pred_words=13
2026-01-08 13:56:03,403: t15.2023.08.13 val PER: 0.1310
2026-01-08 13:56:03,405: t15.2023.08.18 val PER: 0.1182
2026-01-08 13:56:03,406: t15.2023.08.20 val PER: 0.1255
2026-01-08 13:56:03,407: t15.2023.08.25 val PER: 0.0994
2026-01-08 13:56:03,409: t15.2023.08.27 val PER: 0.1897
2026-01-08 13:56:03,410: t15.2023.09.01 val PER: 0.0998
2026-01-08 13:56:03,412: t15.2023.09.03 val PER: 0.1781
2026-01-08 13:56:03,413: t15.2023.09.24 val PER: 0.1383
2026-01-08 13:56:03,416: t15.2023.09.29 val PER: 0.1512
2026-01-08 13:56:03,417: t15.2023.10.01 val PER: 0.1737
2026-01-08 13:56:03,418: t15.2023.10.06 val PER: 0.1055
2026-01-08 13:56:03,420: t15.2023.10.08 val PER: 0.2544
2026-01-08 13:56:03,421: t15.2023.10.13 val PER: 0.2188
2026-01-08 13:56:03,422: t15.2023.10.15 val PER: 0.1608
2026-01-08 13:56:03,424: t15.2023.10.20 val PER: 0.1879
2026-01-08 13:56:03,425: t15.2023.10.22 val PER: 0.1303
2026-01-08 13:56:03,426: t15.2023.11.03 val PER: 0.1852
2026-01-08 13:56:03,428: t15.2023.11.04 val PER: 0.0410
2026-01-08 13:56:03,429: t15.2023.11.17 val PER: 0.0467
2026-01-08 13:56:03,431: t15.2023.11.19 val PER: 0.0479
2026-01-08 13:56:03,432: t15.2023.11.26 val PER: 0.1399
2026-01-08 13:56:03,433: t15.2023.12.03 val PER: 0.1261
2026-01-08 13:56:03,435: t15.2023.12.08 val PER: 0.1238
2026-01-08 13:56:03,436: t15.2023.12.10 val PER: 0.1143
2026-01-08 13:56:03,437: t15.2023.12.17 val PER: 0.1497
2026-01-08 13:56:03,439: t15.2023.12.29 val PER: 0.1510
2026-01-08 13:56:03,441: t15.2024.02.25 val PER: 0.1362
2026-01-08 13:56:03,442: t15.2024.03.08 val PER: 0.2432
2026-01-08 13:56:03,443: t15.2024.03.15 val PER: 0.2233
2026-01-08 13:56:03,445: t15.2024.03.17 val PER: 0.1506
2026-01-08 13:56:03,446: t15.2024.05.10 val PER: 0.1768
2026-01-08 13:56:03,447: t15.2024.06.14 val PER: 0.1798
2026-01-08 13:56:03,449: t15.2024.07.19 val PER: 0.2637
2026-01-08 13:56:03,450: t15.2024.07.21 val PER: 0.1159
2026-01-08 13:56:03,451: t15.2024.07.28 val PER: 0.1515
2026-01-08 13:56:03,453: t15.2025.01.10 val PER: 0.3044
2026-01-08 13:56:03,454: t15.2025.01.12 val PER: 0.1747
2026-01-08 13:56:03,455: t15.2025.03.14 val PER: 0.3639
2026-01-08 13:56:03,457: t15.2025.03.16 val PER: 0.2081
2026-01-08 13:56:03,458: t15.2025.03.30 val PER: 0.3115
2026-01-08 13:56:03,459: t15.2025.04.13 val PER: 0.2311
2026-01-08 13:56:22,950: Train batch 10200: loss: 6.35 grad norm: 39.62 time: 0.052
2026-01-08 13:56:41,664: Train batch 10400: loss: 8.74 grad norm: 51.22 time: 0.073
2026-01-08 13:56:51,015: Running test after training batch: 10500
2026-01-08 13:56:51,112: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:56:56,123: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:56:56,168: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:57:07,166: Val batch 10500: PER (avg): 0.1651 CTC Loss (avg): 16.6485 WER(5gram): 15.97% (n=256) time: 16.149
2026-01-08 13:57:07,168: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=13
2026-01-08 13:57:07,170: t15.2023.08.13 val PER: 0.1247
2026-01-08 13:57:07,172: t15.2023.08.18 val PER: 0.1174
2026-01-08 13:57:07,173: t15.2023.08.20 val PER: 0.1144
2026-01-08 13:57:07,175: t15.2023.08.25 val PER: 0.1039
2026-01-08 13:57:07,176: t15.2023.08.27 val PER: 0.1994
2026-01-08 13:57:07,178: t15.2023.09.01 val PER: 0.0925
2026-01-08 13:57:07,179: t15.2023.09.03 val PER: 0.1734
2026-01-08 13:57:07,181: t15.2023.09.24 val PER: 0.1359
2026-01-08 13:57:07,182: t15.2023.09.29 val PER: 0.1487
2026-01-08 13:57:07,184: t15.2023.10.01 val PER: 0.1803
2026-01-08 13:57:07,185: t15.2023.10.06 val PER: 0.0980
2026-01-08 13:57:07,187: t15.2023.10.08 val PER: 0.2490
2026-01-08 13:57:07,190: t15.2023.10.13 val PER: 0.2087
2026-01-08 13:57:07,192: t15.2023.10.15 val PER: 0.1727
2026-01-08 13:57:07,193: t15.2023.10.20 val PER: 0.1879
2026-01-08 13:57:07,195: t15.2023.10.22 val PER: 0.1192
2026-01-08 13:57:07,196: t15.2023.11.03 val PER: 0.1866
2026-01-08 13:57:07,198: t15.2023.11.04 val PER: 0.0375
2026-01-08 13:57:07,199: t15.2023.11.17 val PER: 0.0575
2026-01-08 13:57:07,201: t15.2023.11.19 val PER: 0.0559
2026-01-08 13:57:07,202: t15.2023.11.26 val PER: 0.1420
2026-01-08 13:57:07,203: t15.2023.12.03 val PER: 0.1271
2026-01-08 13:57:07,205: t15.2023.12.08 val PER: 0.1152
2026-01-08 13:57:07,206: t15.2023.12.10 val PER: 0.1078
2026-01-08 13:57:07,208: t15.2023.12.17 val PER: 0.1403
2026-01-08 13:57:07,209: t15.2023.12.29 val PER: 0.1606
2026-01-08 13:57:07,210: t15.2024.02.25 val PER: 0.1236
2026-01-08 13:57:07,212: t15.2024.03.08 val PER: 0.2304
2026-01-08 13:57:07,213: t15.2024.03.15 val PER: 0.2245
2026-01-08 13:57:07,214: t15.2024.03.17 val PER: 0.1590
2026-01-08 13:57:07,216: t15.2024.05.10 val PER: 0.1887
2026-01-08 13:57:07,217: t15.2024.06.14 val PER: 0.1798
2026-01-08 13:57:07,218: t15.2024.07.19 val PER: 0.2571
2026-01-08 13:57:07,220: t15.2024.07.21 val PER: 0.1007
2026-01-08 13:57:07,221: t15.2024.07.28 val PER: 0.1404
2026-01-08 13:57:07,222: t15.2025.01.10 val PER: 0.3196
2026-01-08 13:57:07,223: t15.2025.01.12 val PER: 0.1809
2026-01-08 13:57:07,225: t15.2025.03.14 val PER: 0.3669
2026-01-08 13:57:07,226: t15.2025.03.16 val PER: 0.1976
2026-01-08 13:57:07,227: t15.2025.03.30 val PER: 0.3103
2026-01-08 13:57:07,229: t15.2025.04.13 val PER: 0.2254
2026-01-08 13:57:07,231: New best val WER(5gram) 16.69% --> 15.97%
2026-01-08 13:57:07,232: Checkpointing model
2026-01-08 13:57:07,378: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-08 13:57:16,844: Train batch 10600: loss: 8.61 grad norm: 54.46 time: 0.075
2026-01-08 13:57:35,264: Train batch 10800: loss: 14.45 grad norm: 70.47 time: 0.066
2026-01-08 13:57:53,593: Train batch 11000: loss: 13.83 grad norm: 61.38 time: 0.057
2026-01-08 13:57:53,595: Running test after training batch: 11000
2026-01-08 13:57:53,736: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:57:58,502: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:57:58,557: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 13:58:09,926: Val batch 11000: PER (avg): 0.1639 CTC Loss (avg): 16.5121 WER(5gram): 16.36% (n=256) time: 16.329
2026-01-08 13:58:09,928: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-08 13:58:09,930: t15.2023.08.13 val PER: 0.1206
2026-01-08 13:58:09,933: t15.2023.08.18 val PER: 0.1157
2026-01-08 13:58:09,934: t15.2023.08.20 val PER: 0.1183
2026-01-08 13:58:09,936: t15.2023.08.25 val PER: 0.0979
2026-01-08 13:58:09,937: t15.2023.08.27 val PER: 0.1881
2026-01-08 13:58:09,939: t15.2023.09.01 val PER: 0.0877
2026-01-08 13:58:09,940: t15.2023.09.03 val PER: 0.1710
2026-01-08 13:58:09,941: t15.2023.09.24 val PER: 0.1468
2026-01-08 13:58:09,943: t15.2023.09.29 val PER: 0.1398
2026-01-08 13:58:09,944: t15.2023.10.01 val PER: 0.1836
2026-01-08 13:58:09,946: t15.2023.10.06 val PER: 0.1001
2026-01-08 13:58:09,947: t15.2023.10.08 val PER: 0.2571
2026-01-08 13:58:09,948: t15.2023.10.13 val PER: 0.2180
2026-01-08 13:58:09,950: t15.2023.10.15 val PER: 0.1694
2026-01-08 13:58:09,951: t15.2023.10.20 val PER: 0.2081
2026-01-08 13:58:09,952: t15.2023.10.22 val PER: 0.1214
2026-01-08 13:58:09,954: t15.2023.11.03 val PER: 0.1893
2026-01-08 13:58:09,955: t15.2023.11.04 val PER: 0.0307
2026-01-08 13:58:09,957: t15.2023.11.17 val PER: 0.0482
2026-01-08 13:58:09,958: t15.2023.11.19 val PER: 0.0439
2026-01-08 13:58:09,959: t15.2023.11.26 val PER: 0.1406
2026-01-08 13:58:09,961: t15.2023.12.03 val PER: 0.1271
2026-01-08 13:58:09,962: t15.2023.12.08 val PER: 0.1165
2026-01-08 13:58:09,964: t15.2023.12.10 val PER: 0.1012
2026-01-08 13:58:09,965: t15.2023.12.17 val PER: 0.1455
2026-01-08 13:58:09,967: t15.2023.12.29 val PER: 0.1469
2026-01-08 13:58:09,968: t15.2024.02.25 val PER: 0.1278
2026-01-08 13:58:09,970: t15.2024.03.08 val PER: 0.2390
2026-01-08 13:58:09,971: t15.2024.03.15 val PER: 0.2164
2026-01-08 13:58:09,973: t15.2024.03.17 val PER: 0.1555
2026-01-08 13:58:09,974: t15.2024.05.10 val PER: 0.1857
2026-01-08 13:58:09,976: t15.2024.06.14 val PER: 0.1798
2026-01-08 13:58:09,977: t15.2024.07.19 val PER: 0.2584
2026-01-08 13:58:09,979: t15.2024.07.21 val PER: 0.0966
2026-01-08 13:58:09,980: t15.2024.07.28 val PER: 0.1500
2026-01-08 13:58:09,981: t15.2025.01.10 val PER: 0.3113
2026-01-08 13:58:09,983: t15.2025.01.12 val PER: 0.1663
2026-01-08 13:58:09,984: t15.2025.03.14 val PER: 0.3565
2026-01-08 13:58:09,986: t15.2025.03.16 val PER: 0.2003
2026-01-08 13:58:09,987: t15.2025.03.30 val PER: 0.3092
2026-01-08 13:58:09,988: t15.2025.04.13 val PER: 0.2382
2026-01-08 13:58:28,405: Train batch 11200: loss: 10.22 grad norm: 52.74 time: 0.072
2026-01-08 13:58:45,301: Train batch 11400: loss: 9.39 grad norm: 53.01 time: 0.057
2026-01-08 13:58:54,004: Running test after training batch: 11500
2026-01-08 13:58:54,156: WER debug GT example: You can see the code at this point as well.
2026-01-08 13:58:58,904: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 13:58:58,961: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 13:59:10,156: Val batch 11500: PER (avg): 0.1604 CTC Loss (avg): 16.3508 WER(5gram): 17.67% (n=256) time: 16.150
2026-01-08 13:59:10,158: WER lens: avg_true_words=5.99 avg_pred_words=6.24 max_pred_words=13
2026-01-08 13:59:10,161: t15.2023.08.13 val PER: 0.1185
2026-01-08 13:59:10,163: t15.2023.08.18 val PER: 0.1132
2026-01-08 13:59:10,164: t15.2023.08.20 val PER: 0.1120
2026-01-08 13:59:10,166: t15.2023.08.25 val PER: 0.1039
2026-01-08 13:59:10,168: t15.2023.08.27 val PER: 0.2058
2026-01-08 13:59:10,169: t15.2023.09.01 val PER: 0.0820
2026-01-08 13:59:10,171: t15.2023.09.03 val PER: 0.1698
2026-01-08 13:59:10,173: t15.2023.09.24 val PER: 0.1311
2026-01-08 13:59:10,174: t15.2023.09.29 val PER: 0.1359
2026-01-08 13:59:10,175: t15.2023.10.01 val PER: 0.1757
2026-01-08 13:59:10,177: t15.2023.10.06 val PER: 0.0872
2026-01-08 13:59:10,178: t15.2023.10.08 val PER: 0.2530
2026-01-08 13:59:10,179: t15.2023.10.13 val PER: 0.2056
2026-01-08 13:59:10,181: t15.2023.10.15 val PER: 0.1668
2026-01-08 13:59:10,182: t15.2023.10.20 val PER: 0.1812
2026-01-08 13:59:10,184: t15.2023.10.22 val PER: 0.1169
2026-01-08 13:59:10,185: t15.2023.11.03 val PER: 0.1859
2026-01-08 13:59:10,187: t15.2023.11.04 val PER: 0.0341
2026-01-08 13:59:10,188: t15.2023.11.17 val PER: 0.0467
2026-01-08 13:59:10,189: t15.2023.11.19 val PER: 0.0479
2026-01-08 13:59:10,191: t15.2023.11.26 val PER: 0.1362
2026-01-08 13:59:10,192: t15.2023.12.03 val PER: 0.1313
2026-01-08 13:59:10,194: t15.2023.12.08 val PER: 0.1172
2026-01-08 13:59:10,196: t15.2023.12.10 val PER: 0.1091
2026-01-08 13:59:10,198: t15.2023.12.17 val PER: 0.1486
2026-01-08 13:59:10,200: t15.2023.12.29 val PER: 0.1421
2026-01-08 13:59:10,202: t15.2024.02.25 val PER: 0.1236
2026-01-08 13:59:10,203: t15.2024.03.08 val PER: 0.2262
2026-01-08 13:59:10,204: t15.2024.03.15 val PER: 0.2164
2026-01-08 13:59:10,206: t15.2024.03.17 val PER: 0.1471
2026-01-08 13:59:10,207: t15.2024.05.10 val PER: 0.1738
2026-01-08 13:59:10,209: t15.2024.06.14 val PER: 0.1924
2026-01-08 13:59:10,210: t15.2024.07.19 val PER: 0.2538
2026-01-08 13:59:10,211: t15.2024.07.21 val PER: 0.0966
2026-01-08 13:59:10,213: t15.2024.07.28 val PER: 0.1478
2026-01-08 13:59:10,214: t15.2025.01.10 val PER: 0.3085
2026-01-08 13:59:10,215: t15.2025.01.12 val PER: 0.1586
2026-01-08 13:59:10,217: t15.2025.03.14 val PER: 0.3595
2026-01-08 13:59:10,218: t15.2025.03.16 val PER: 0.1976
2026-01-08 13:59:10,220: t15.2025.03.30 val PER: 0.3080
2026-01-08 13:59:10,221: t15.2025.04.13 val PER: 0.2240
2026-01-08 13:59:19,221: Train batch 11600: loss: 10.34 grad norm: 46.36 time: 0.062
2026-01-08 13:59:37,542: Train batch 11800: loss: 6.29 grad norm: 40.58 time: 0.045
2026-01-08 13:59:55,634: Train batch 12000: loss: 13.29 grad norm: 51.84 time: 0.072
2026-01-08 13:59:55,636: Running test after training batch: 12000
2026-01-08 13:59:55,744: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:00:00,451: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:00:00,500: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 14:00:11,446: Val batch 12000: PER (avg): 0.1575 CTC Loss (avg): 16.2073 WER(5gram): 16.36% (n=256) time: 15.805
2026-01-08 14:00:11,448: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 14:00:11,450: t15.2023.08.13 val PER: 0.1143
2026-01-08 14:00:11,452: t15.2023.08.18 val PER: 0.1098
2026-01-08 14:00:11,454: t15.2023.08.20 val PER: 0.1136
2026-01-08 14:00:11,455: t15.2023.08.25 val PER: 0.0979
2026-01-08 14:00:11,457: t15.2023.08.27 val PER: 0.1897
2026-01-08 14:00:11,458: t15.2023.09.01 val PER: 0.0917
2026-01-08 14:00:11,460: t15.2023.09.03 val PER: 0.1591
2026-01-08 14:00:11,462: t15.2023.09.24 val PER: 0.1371
2026-01-08 14:00:11,463: t15.2023.09.29 val PER: 0.1334
2026-01-08 14:00:11,465: t15.2023.10.01 val PER: 0.1750
2026-01-08 14:00:11,466: t15.2023.10.06 val PER: 0.0915
2026-01-08 14:00:11,468: t15.2023.10.08 val PER: 0.2530
2026-01-08 14:00:11,470: t15.2023.10.13 val PER: 0.2087
2026-01-08 14:00:11,471: t15.2023.10.15 val PER: 0.1562
2026-01-08 14:00:11,473: t15.2023.10.20 val PER: 0.1779
2026-01-08 14:00:11,475: t15.2023.10.22 val PER: 0.1180
2026-01-08 14:00:11,476: t15.2023.11.03 val PER: 0.1879
2026-01-08 14:00:11,478: t15.2023.11.04 val PER: 0.0375
2026-01-08 14:00:11,479: t15.2023.11.17 val PER: 0.0404
2026-01-08 14:00:11,481: t15.2023.11.19 val PER: 0.0319
2026-01-08 14:00:11,482: t15.2023.11.26 val PER: 0.1254
2026-01-08 14:00:11,484: t15.2023.12.03 val PER: 0.1250
2026-01-08 14:00:11,486: t15.2023.12.08 val PER: 0.1059
2026-01-08 14:00:11,487: t15.2023.12.10 val PER: 0.0986
2026-01-08 14:00:11,489: t15.2023.12.17 val PER: 0.1497
2026-01-08 14:00:11,491: t15.2023.12.29 val PER: 0.1407
2026-01-08 14:00:11,492: t15.2024.02.25 val PER: 0.1278
2026-01-08 14:00:11,494: t15.2024.03.08 val PER: 0.2290
2026-01-08 14:00:11,496: t15.2024.03.15 val PER: 0.2108
2026-01-08 14:00:11,497: t15.2024.03.17 val PER: 0.1492
2026-01-08 14:00:11,499: t15.2024.05.10 val PER: 0.1768
2026-01-08 14:00:11,501: t15.2024.06.14 val PER: 0.1877
2026-01-08 14:00:11,503: t15.2024.07.19 val PER: 0.2426
2026-01-08 14:00:11,506: t15.2024.07.21 val PER: 0.1048
2026-01-08 14:00:11,508: t15.2024.07.28 val PER: 0.1412
2026-01-08 14:00:11,509: t15.2025.01.10 val PER: 0.2934
2026-01-08 14:00:11,511: t15.2025.01.12 val PER: 0.1524
2026-01-08 14:00:11,512: t15.2025.03.14 val PER: 0.3669
2026-01-08 14:00:11,514: t15.2025.03.16 val PER: 0.2003
2026-01-08 14:00:11,516: t15.2025.03.30 val PER: 0.3023
2026-01-08 14:00:11,519: t15.2025.04.13 val PER: 0.2140
2026-01-08 14:00:29,893: Train batch 12200: loss: 5.18 grad norm: 38.77 time: 0.067
2026-01-08 14:00:48,606: Train batch 12400: loss: 4.83 grad norm: 36.33 time: 0.042
2026-01-08 14:00:58,465: Running test after training batch: 12500
2026-01-08 14:00:58,566: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:01:03,288: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:01:03,342: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 14:01:14,602: Val batch 12500: PER (avg): 0.1559 CTC Loss (avg): 15.9777 WER(5gram): 17.14% (n=256) time: 16.135
2026-01-08 14:01:14,604: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=12
2026-01-08 14:01:14,607: t15.2023.08.13 val PER: 0.1195
2026-01-08 14:01:14,609: t15.2023.08.18 val PER: 0.1115
2026-01-08 14:01:14,611: t15.2023.08.20 val PER: 0.1072
2026-01-08 14:01:14,612: t15.2023.08.25 val PER: 0.0858
2026-01-08 14:01:14,614: t15.2023.08.27 val PER: 0.1945
2026-01-08 14:01:14,615: t15.2023.09.01 val PER: 0.0820
2026-01-08 14:01:14,617: t15.2023.09.03 val PER: 0.1627
2026-01-08 14:01:14,618: t15.2023.09.24 val PER: 0.1274
2026-01-08 14:01:14,620: t15.2023.09.29 val PER: 0.1321
2026-01-08 14:01:14,621: t15.2023.10.01 val PER: 0.1691
2026-01-08 14:01:14,623: t15.2023.10.06 val PER: 0.0861
2026-01-08 14:01:14,624: t15.2023.10.08 val PER: 0.2625
2026-01-08 14:01:14,626: t15.2023.10.13 val PER: 0.2033
2026-01-08 14:01:14,627: t15.2023.10.15 val PER: 0.1575
2026-01-08 14:01:14,628: t15.2023.10.20 val PER: 0.1980
2026-01-08 14:01:14,630: t15.2023.10.22 val PER: 0.1158
2026-01-08 14:01:14,631: t15.2023.11.03 val PER: 0.1771
2026-01-08 14:01:14,633: t15.2023.11.04 val PER: 0.0410
2026-01-08 14:01:14,634: t15.2023.11.17 val PER: 0.0513
2026-01-08 14:01:14,635: t15.2023.11.19 val PER: 0.0399
2026-01-08 14:01:14,637: t15.2023.11.26 val PER: 0.1312
2026-01-08 14:01:14,639: t15.2023.12.03 val PER: 0.1239
2026-01-08 14:01:14,640: t15.2023.12.08 val PER: 0.1025
2026-01-08 14:01:14,641: t15.2023.12.10 val PER: 0.1012
2026-01-08 14:01:14,643: t15.2023.12.17 val PER: 0.1538
2026-01-08 14:01:14,644: t15.2023.12.29 val PER: 0.1428
2026-01-08 14:01:14,646: t15.2024.02.25 val PER: 0.1053
2026-01-08 14:01:14,647: t15.2024.03.08 val PER: 0.2248
2026-01-08 14:01:14,648: t15.2024.03.15 val PER: 0.2101
2026-01-08 14:01:14,650: t15.2024.03.17 val PER: 0.1499
2026-01-08 14:01:14,651: t15.2024.05.10 val PER: 0.1768
2026-01-08 14:01:14,652: t15.2024.06.14 val PER: 0.1845
2026-01-08 14:01:14,654: t15.2024.07.19 val PER: 0.2347
2026-01-08 14:01:14,655: t15.2024.07.21 val PER: 0.0966
2026-01-08 14:01:14,656: t15.2024.07.28 val PER: 0.1382
2026-01-08 14:01:14,658: t15.2025.01.10 val PER: 0.3003
2026-01-08 14:01:14,659: t15.2025.01.12 val PER: 0.1540
2026-01-08 14:01:14,662: t15.2025.03.14 val PER: 0.3654
2026-01-08 14:01:14,664: t15.2025.03.16 val PER: 0.1911
2026-01-08 14:01:14,665: t15.2025.03.30 val PER: 0.3103
2026-01-08 14:01:14,667: t15.2025.04.13 val PER: 0.2154
2026-01-08 14:01:23,289: Train batch 12600: loss: 7.36 grad norm: 44.03 time: 0.060
2026-01-08 14:01:40,795: Train batch 12800: loss: 5.39 grad norm: 37.48 time: 0.053
2026-01-08 14:01:59,171: Train batch 13000: loss: 6.23 grad norm: 41.02 time: 0.066
2026-01-08 14:01:59,173: Running test after training batch: 13000
2026-01-08 14:01:59,288: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:02:03,978: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:02:04,027: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 14:02:14,719: Val batch 13000: PER (avg): 0.1546 CTC Loss (avg): 15.7594 WER(5gram): 14.80% (n=256) time: 15.543
2026-01-08 14:02:14,720: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=12
2026-01-08 14:02:14,722: t15.2023.08.13 val PER: 0.1227
2026-01-08 14:02:14,724: t15.2023.08.18 val PER: 0.1115
2026-01-08 14:02:14,725: t15.2023.08.20 val PER: 0.1112
2026-01-08 14:02:14,728: t15.2023.08.25 val PER: 0.0919
2026-01-08 14:02:14,729: t15.2023.08.27 val PER: 0.1929
2026-01-08 14:02:14,730: t15.2023.09.01 val PER: 0.0836
2026-01-08 14:02:14,732: t15.2023.09.03 val PER: 0.1710
2026-01-08 14:02:14,733: t15.2023.09.24 val PER: 0.1250
2026-01-08 14:02:14,735: t15.2023.09.29 val PER: 0.1283
2026-01-08 14:02:14,736: t15.2023.10.01 val PER: 0.1737
2026-01-08 14:02:14,738: t15.2023.10.06 val PER: 0.0947
2026-01-08 14:02:14,739: t15.2023.10.08 val PER: 0.2476
2026-01-08 14:02:14,741: t15.2023.10.13 val PER: 0.2056
2026-01-08 14:02:14,742: t15.2023.10.15 val PER: 0.1595
2026-01-08 14:02:14,743: t15.2023.10.20 val PER: 0.1812
2026-01-08 14:02:14,745: t15.2023.10.22 val PER: 0.1125
2026-01-08 14:02:14,746: t15.2023.11.03 val PER: 0.1852
2026-01-08 14:02:14,747: t15.2023.11.04 val PER: 0.0375
2026-01-08 14:02:14,748: t15.2023.11.17 val PER: 0.0404
2026-01-08 14:02:14,750: t15.2023.11.19 val PER: 0.0419
2026-01-08 14:02:14,751: t15.2023.11.26 val PER: 0.1275
2026-01-08 14:02:14,753: t15.2023.12.03 val PER: 0.1218
2026-01-08 14:02:14,754: t15.2023.12.08 val PER: 0.1052
2026-01-08 14:02:14,755: t15.2023.12.10 val PER: 0.0933
2026-01-08 14:02:14,757: t15.2023.12.17 val PER: 0.1351
2026-01-08 14:02:14,758: t15.2023.12.29 val PER: 0.1386
2026-01-08 14:02:14,759: t15.2024.02.25 val PER: 0.1194
2026-01-08 14:02:14,761: t15.2024.03.08 val PER: 0.2333
2026-01-08 14:02:14,762: t15.2024.03.15 val PER: 0.2114
2026-01-08 14:02:14,763: t15.2024.03.17 val PER: 0.1437
2026-01-08 14:02:14,764: t15.2024.05.10 val PER: 0.1486
2026-01-08 14:02:14,766: t15.2024.06.14 val PER: 0.1767
2026-01-08 14:02:14,767: t15.2024.07.19 val PER: 0.2413
2026-01-08 14:02:14,768: t15.2024.07.21 val PER: 0.0952
2026-01-08 14:02:14,769: t15.2024.07.28 val PER: 0.1382
2026-01-08 14:02:14,771: t15.2025.01.10 val PER: 0.2989
2026-01-08 14:02:14,772: t15.2025.01.12 val PER: 0.1470
2026-01-08 14:02:14,773: t15.2025.03.14 val PER: 0.3417
2026-01-08 14:02:14,774: t15.2025.03.16 val PER: 0.1859
2026-01-08 14:02:14,775: t15.2025.03.30 val PER: 0.3046
2026-01-08 14:02:14,777: t15.2025.04.13 val PER: 0.2168
2026-01-08 14:02:14,778: New best val WER(5gram) 15.97% --> 14.80%
2026-01-08 14:02:14,779: Checkpointing model
2026-01-08 14:02:14,926: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-08 14:02:32,194: Train batch 13200: loss: 12.02 grad norm: 59.05 time: 0.055
2026-01-08 14:02:50,625: Train batch 13400: loss: 8.40 grad norm: 51.99 time: 0.063
2026-01-08 14:03:00,122: Running test after training batch: 13500
2026-01-08 14:03:00,239: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:03:05,073: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:03:05,128: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost it
2026-01-08 14:03:18,742: Val batch 13500: PER (avg): 0.1525 CTC Loss (avg): 15.5993 WER(5gram): 16.23% (n=256) time: 18.618
2026-01-08 14:03:18,744: WER lens: avg_true_words=5.99 avg_pred_words=6.22 max_pred_words=13
2026-01-08 14:03:18,746: t15.2023.08.13 val PER: 0.1206
2026-01-08 14:03:18,748: t15.2023.08.18 val PER: 0.0997
2026-01-08 14:03:18,749: t15.2023.08.20 val PER: 0.1072
2026-01-08 14:03:18,751: t15.2023.08.25 val PER: 0.0858
2026-01-08 14:03:18,752: t15.2023.08.27 val PER: 0.1801
2026-01-08 14:03:18,754: t15.2023.09.01 val PER: 0.0869
2026-01-08 14:03:18,756: t15.2023.09.03 val PER: 0.1580
2026-01-08 14:03:18,758: t15.2023.09.24 val PER: 0.1274
2026-01-08 14:03:18,759: t15.2023.09.29 val PER: 0.1302
2026-01-08 14:03:18,761: t15.2023.10.01 val PER: 0.1684
2026-01-08 14:03:18,762: t15.2023.10.06 val PER: 0.0936
2026-01-08 14:03:18,764: t15.2023.10.08 val PER: 0.2476
2026-01-08 14:03:18,765: t15.2023.10.13 val PER: 0.2009
2026-01-08 14:03:18,767: t15.2023.10.15 val PER: 0.1628
2026-01-08 14:03:18,769: t15.2023.10.20 val PER: 0.1779
2026-01-08 14:03:18,770: t15.2023.10.22 val PER: 0.1125
2026-01-08 14:03:18,772: t15.2023.11.03 val PER: 0.1900
2026-01-08 14:03:18,773: t15.2023.11.04 val PER: 0.0307
2026-01-08 14:03:18,774: t15.2023.11.17 val PER: 0.0420
2026-01-08 14:03:18,776: t15.2023.11.19 val PER: 0.0339
2026-01-08 14:03:18,778: t15.2023.11.26 val PER: 0.1275
2026-01-08 14:03:18,779: t15.2023.12.03 val PER: 0.1113
2026-01-08 14:03:18,780: t15.2023.12.08 val PER: 0.1012
2026-01-08 14:03:18,782: t15.2023.12.10 val PER: 0.0946
2026-01-08 14:03:18,783: t15.2023.12.17 val PER: 0.1393
2026-01-08 14:03:18,784: t15.2023.12.29 val PER: 0.1352
2026-01-08 14:03:18,786: t15.2024.02.25 val PER: 0.1180
2026-01-08 14:03:18,787: t15.2024.03.08 val PER: 0.2333
2026-01-08 14:03:18,790: t15.2024.03.15 val PER: 0.2064
2026-01-08 14:03:18,792: t15.2024.03.17 val PER: 0.1409
2026-01-08 14:03:18,793: t15.2024.05.10 val PER: 0.1620
2026-01-08 14:03:18,795: t15.2024.06.14 val PER: 0.1577
2026-01-08 14:03:18,796: t15.2024.07.19 val PER: 0.2314
2026-01-08 14:03:18,798: t15.2024.07.21 val PER: 0.0972
2026-01-08 14:03:18,799: t15.2024.07.28 val PER: 0.1331
2026-01-08 14:03:18,800: t15.2025.01.10 val PER: 0.2961
2026-01-08 14:03:18,802: t15.2025.01.12 val PER: 0.1563
2026-01-08 14:03:18,803: t15.2025.03.14 val PER: 0.3358
2026-01-08 14:03:18,804: t15.2025.03.16 val PER: 0.1806
2026-01-08 14:03:18,806: t15.2025.03.30 val PER: 0.3092
2026-01-08 14:03:18,807: t15.2025.04.13 val PER: 0.2211
2026-01-08 14:03:28,420: Train batch 13600: loss: 12.23 grad norm: 65.82 time: 0.063
2026-01-08 14:03:46,802: Train batch 13800: loss: 8.39 grad norm: 56.77 time: 0.056
2026-01-08 14:04:04,606: Train batch 14000: loss: 10.60 grad norm: 55.47 time: 0.051
2026-01-08 14:04:04,608: Running test after training batch: 14000
2026-01-08 14:04:04,719: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:04:09,960: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:04:10,013: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 14:04:21,447: Val batch 14000: PER (avg): 0.1507 CTC Loss (avg): 15.5378 WER(5gram): 16.04% (n=256) time: 16.836
2026-01-08 14:04:21,449: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=13
2026-01-08 14:04:21,451: t15.2023.08.13 val PER: 0.1123
2026-01-08 14:04:21,452: t15.2023.08.18 val PER: 0.0972
2026-01-08 14:04:21,454: t15.2023.08.20 val PER: 0.1033
2026-01-08 14:04:21,456: t15.2023.08.25 val PER: 0.0994
2026-01-08 14:04:21,457: t15.2023.08.27 val PER: 0.1913
2026-01-08 14:04:21,459: t15.2023.09.01 val PER: 0.0812
2026-01-08 14:04:21,460: t15.2023.09.03 val PER: 0.1686
2026-01-08 14:04:21,462: t15.2023.09.24 val PER: 0.1274
2026-01-08 14:04:21,463: t15.2023.09.29 val PER: 0.1302
2026-01-08 14:04:21,464: t15.2023.10.01 val PER: 0.1790
2026-01-08 14:04:21,466: t15.2023.10.06 val PER: 0.0850
2026-01-08 14:04:21,467: t15.2023.10.08 val PER: 0.2463
2026-01-08 14:04:21,469: t15.2023.10.13 val PER: 0.1955
2026-01-08 14:04:21,470: t15.2023.10.15 val PER: 0.1523
2026-01-08 14:04:21,472: t15.2023.10.20 val PER: 0.1812
2026-01-08 14:04:21,473: t15.2023.10.22 val PER: 0.1058
2026-01-08 14:04:21,474: t15.2023.11.03 val PER: 0.1818
2026-01-08 14:04:21,476: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:04:21,477: t15.2023.11.17 val PER: 0.0404
2026-01-08 14:04:21,478: t15.2023.11.19 val PER: 0.0339
2026-01-08 14:04:21,480: t15.2023.11.26 val PER: 0.1239
2026-01-08 14:04:21,481: t15.2023.12.03 val PER: 0.1155
2026-01-08 14:04:21,483: t15.2023.12.08 val PER: 0.1012
2026-01-08 14:04:21,485: t15.2023.12.10 val PER: 0.0959
2026-01-08 14:04:21,487: t15.2023.12.17 val PER: 0.1299
2026-01-08 14:04:21,488: t15.2023.12.29 val PER: 0.1283
2026-01-08 14:04:21,490: t15.2024.02.25 val PER: 0.1152
2026-01-08 14:04:21,491: t15.2024.03.08 val PER: 0.2233
2026-01-08 14:04:21,493: t15.2024.03.15 val PER: 0.2033
2026-01-08 14:04:21,494: t15.2024.03.17 val PER: 0.1430
2026-01-08 14:04:21,496: t15.2024.05.10 val PER: 0.1605
2026-01-08 14:04:21,497: t15.2024.06.14 val PER: 0.1577
2026-01-08 14:04:21,499: t15.2024.07.19 val PER: 0.2294
2026-01-08 14:04:21,500: t15.2024.07.21 val PER: 0.0883
2026-01-08 14:04:21,502: t15.2024.07.28 val PER: 0.1360
2026-01-08 14:04:21,503: t15.2025.01.10 val PER: 0.2920
2026-01-08 14:04:21,504: t15.2025.01.12 val PER: 0.1424
2026-01-08 14:04:21,506: t15.2025.03.14 val PER: 0.3491
2026-01-08 14:04:21,507: t15.2025.03.16 val PER: 0.1990
2026-01-08 14:04:21,508: t15.2025.03.30 val PER: 0.3023
2026-01-08 14:04:21,510: t15.2025.04.13 val PER: 0.2254
2026-01-08 14:04:40,033: Train batch 14200: loss: 7.77 grad norm: 49.93 time: 0.056
2026-01-08 14:04:59,491: Train batch 14400: loss: 5.34 grad norm: 39.58 time: 0.064
2026-01-08 14:05:08,718: Running test after training batch: 14500
2026-01-08 14:05:08,810: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:05:13,604: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:05:13,653: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 14:05:24,996: Val batch 14500: PER (avg): 0.1519 CTC Loss (avg): 15.5146 WER(5gram): 16.10% (n=256) time: 16.275
2026-01-08 14:05:24,998: WER lens: avg_true_words=5.99 avg_pred_words=6.22 max_pred_words=12
2026-01-08 14:05:25,000: t15.2023.08.13 val PER: 0.1185
2026-01-08 14:05:25,001: t15.2023.08.18 val PER: 0.1065
2026-01-08 14:05:25,003: t15.2023.08.20 val PER: 0.1120
2026-01-08 14:05:25,007: t15.2023.08.25 val PER: 0.0919
2026-01-08 14:05:25,008: t15.2023.08.27 val PER: 0.1849
2026-01-08 14:05:25,010: t15.2023.09.01 val PER: 0.0812
2026-01-08 14:05:25,012: t15.2023.09.03 val PER: 0.1663
2026-01-08 14:05:25,013: t15.2023.09.24 val PER: 0.1286
2026-01-08 14:05:25,015: t15.2023.09.29 val PER: 0.1257
2026-01-08 14:05:25,017: t15.2023.10.01 val PER: 0.1737
2026-01-08 14:05:25,018: t15.2023.10.06 val PER: 0.0915
2026-01-08 14:05:25,020: t15.2023.10.08 val PER: 0.2422
2026-01-08 14:05:25,022: t15.2023.10.13 val PER: 0.2017
2026-01-08 14:05:25,024: t15.2023.10.15 val PER: 0.1536
2026-01-08 14:05:25,026: t15.2023.10.20 val PER: 0.1879
2026-01-08 14:05:25,027: t15.2023.10.22 val PER: 0.1036
2026-01-08 14:05:25,029: t15.2023.11.03 val PER: 0.1839
2026-01-08 14:05:25,030: t15.2023.11.04 val PER: 0.0375
2026-01-08 14:05:25,033: t15.2023.11.17 val PER: 0.0420
2026-01-08 14:05:25,035: t15.2023.11.19 val PER: 0.0379
2026-01-08 14:05:25,036: t15.2023.11.26 val PER: 0.1261
2026-01-08 14:05:25,038: t15.2023.12.03 val PER: 0.1092
2026-01-08 14:05:25,039: t15.2023.12.08 val PER: 0.0999
2026-01-08 14:05:25,041: t15.2023.12.10 val PER: 0.0841
2026-01-08 14:05:25,042: t15.2023.12.17 val PER: 0.1466
2026-01-08 14:05:25,044: t15.2023.12.29 val PER: 0.1304
2026-01-08 14:05:25,045: t15.2024.02.25 val PER: 0.1138
2026-01-08 14:05:25,047: t15.2024.03.08 val PER: 0.2248
2026-01-08 14:05:25,048: t15.2024.03.15 val PER: 0.2083
2026-01-08 14:05:25,050: t15.2024.03.17 val PER: 0.1437
2026-01-08 14:05:25,052: t15.2024.05.10 val PER: 0.1530
2026-01-08 14:05:25,054: t15.2024.06.14 val PER: 0.1609
2026-01-08 14:05:25,055: t15.2024.07.19 val PER: 0.2386
2026-01-08 14:05:25,057: t15.2024.07.21 val PER: 0.0910
2026-01-08 14:05:25,058: t15.2024.07.28 val PER: 0.1360
2026-01-08 14:05:25,060: t15.2025.01.10 val PER: 0.2920
2026-01-08 14:05:25,061: t15.2025.01.12 val PER: 0.1501
2026-01-08 14:05:25,063: t15.2025.03.14 val PER: 0.3580
2026-01-08 14:05:25,064: t15.2025.03.16 val PER: 0.1846
2026-01-08 14:05:25,066: t15.2025.03.30 val PER: 0.2977
2026-01-08 14:05:25,068: t15.2025.04.13 val PER: 0.2225
2026-01-08 14:05:33,599: Train batch 14600: loss: 11.59 grad norm: 55.43 time: 0.059
2026-01-08 14:05:51,632: Train batch 14800: loss: 5.63 grad norm: 44.78 time: 0.051
2026-01-08 14:06:09,774: Train batch 15000: loss: 8.62 grad norm: 50.92 time: 0.053
2026-01-08 14:06:09,776: Running test after training batch: 15000
2026-01-08 14:06:09,896: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:06:14,612: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:06:14,663: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 14:06:25,863: Val batch 15000: PER (avg): 0.1481 CTC Loss (avg): 15.3416 WER(5gram): 14.67% (n=256) time: 16.085
2026-01-08 14:06:25,866: WER lens: avg_true_words=5.99 avg_pred_words=6.18 max_pred_words=12
2026-01-08 14:06:25,868: t15.2023.08.13 val PER: 0.1071
2026-01-08 14:06:25,870: t15.2023.08.18 val PER: 0.1023
2026-01-08 14:06:25,871: t15.2023.08.20 val PER: 0.1009
2026-01-08 14:06:25,873: t15.2023.08.25 val PER: 0.0904
2026-01-08 14:06:25,874: t15.2023.08.27 val PER: 0.1817
2026-01-08 14:06:25,876: t15.2023.09.01 val PER: 0.0812
2026-01-08 14:06:25,877: t15.2023.09.03 val PER: 0.1425
2026-01-08 14:06:25,879: t15.2023.09.24 val PER: 0.1311
2026-01-08 14:06:25,880: t15.2023.09.29 val PER: 0.1244
2026-01-08 14:06:25,884: t15.2023.10.01 val PER: 0.1671
2026-01-08 14:06:25,886: t15.2023.10.06 val PER: 0.0861
2026-01-08 14:06:25,888: t15.2023.10.08 val PER: 0.2355
2026-01-08 14:06:25,889: t15.2023.10.13 val PER: 0.1978
2026-01-08 14:06:25,891: t15.2023.10.15 val PER: 0.1490
2026-01-08 14:06:25,892: t15.2023.10.20 val PER: 0.1745
2026-01-08 14:06:25,894: t15.2023.10.22 val PER: 0.1036
2026-01-08 14:06:25,895: t15.2023.11.03 val PER: 0.1798
2026-01-08 14:06:25,897: t15.2023.11.04 val PER: 0.0444
2026-01-08 14:06:25,899: t15.2023.11.17 val PER: 0.0451
2026-01-08 14:06:25,902: t15.2023.11.19 val PER: 0.0299
2026-01-08 14:06:25,904: t15.2023.11.26 val PER: 0.1225
2026-01-08 14:06:25,905: t15.2023.12.03 val PER: 0.1113
2026-01-08 14:06:25,907: t15.2023.12.08 val PER: 0.0985
2026-01-08 14:06:25,908: t15.2023.12.10 val PER: 0.0946
2026-01-08 14:06:25,910: t15.2023.12.17 val PER: 0.1476
2026-01-08 14:06:25,911: t15.2023.12.29 val PER: 0.1318
2026-01-08 14:06:25,913: t15.2024.02.25 val PER: 0.1096
2026-01-08 14:06:25,914: t15.2024.03.08 val PER: 0.2276
2026-01-08 14:06:25,916: t15.2024.03.15 val PER: 0.1976
2026-01-08 14:06:25,917: t15.2024.03.17 val PER: 0.1346
2026-01-08 14:06:25,919: t15.2024.05.10 val PER: 0.1605
2026-01-08 14:06:25,920: t15.2024.06.14 val PER: 0.1672
2026-01-08 14:06:25,922: t15.2024.07.19 val PER: 0.2261
2026-01-08 14:06:25,923: t15.2024.07.21 val PER: 0.0869
2026-01-08 14:06:25,924: t15.2024.07.28 val PER: 0.1324
2026-01-08 14:06:25,926: t15.2025.01.10 val PER: 0.2975
2026-01-08 14:06:25,927: t15.2025.01.12 val PER: 0.1509
2026-01-08 14:06:25,930: t15.2025.03.14 val PER: 0.3476
2026-01-08 14:06:25,932: t15.2025.03.16 val PER: 0.1819
2026-01-08 14:06:25,933: t15.2025.03.30 val PER: 0.2885
2026-01-08 14:06:25,934: t15.2025.04.13 val PER: 0.2168
2026-01-08 14:06:25,936: New best val WER(5gram) 14.80% --> 14.67%
2026-01-08 14:06:25,937: Checkpointing model
2026-01-08 14:06:26,081: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id05_wd1e-5/checkpoint/best_checkpoint
2026-01-08 14:06:44,519: Train batch 15200: loss: 4.67 grad norm: 42.50 time: 0.058
2026-01-08 14:07:02,554: Train batch 15400: loss: 10.80 grad norm: 57.59 time: 0.051
2026-01-08 14:07:11,806: Running test after training batch: 15500
2026-01-08 14:07:11,974: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:07:16,703: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:07:16,754: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 14:07:28,108: Val batch 15500: PER (avg): 0.1466 CTC Loss (avg): 15.1973 WER(5gram): 15.25% (n=256) time: 16.300
2026-01-08 14:07:28,111: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=12
2026-01-08 14:07:28,113: t15.2023.08.13 val PER: 0.1050
2026-01-08 14:07:28,117: t15.2023.08.18 val PER: 0.0997
2026-01-08 14:07:28,118: t15.2023.08.20 val PER: 0.1072
2026-01-08 14:07:28,120: t15.2023.08.25 val PER: 0.0858
2026-01-08 14:07:28,121: t15.2023.08.27 val PER: 0.1801
2026-01-08 14:07:28,122: t15.2023.09.01 val PER: 0.0747
2026-01-08 14:07:28,124: t15.2023.09.03 val PER: 0.1473
2026-01-08 14:07:28,126: t15.2023.09.24 val PER: 0.1262
2026-01-08 14:07:28,127: t15.2023.09.29 val PER: 0.1200
2026-01-08 14:07:28,128: t15.2023.10.01 val PER: 0.1697
2026-01-08 14:07:28,131: t15.2023.10.06 val PER: 0.0829
2026-01-08 14:07:28,132: t15.2023.10.08 val PER: 0.2327
2026-01-08 14:07:28,134: t15.2023.10.13 val PER: 0.1963
2026-01-08 14:07:28,135: t15.2023.10.15 val PER: 0.1477
2026-01-08 14:07:28,136: t15.2023.10.20 val PER: 0.1745
2026-01-08 14:07:28,138: t15.2023.10.22 val PER: 0.1069
2026-01-08 14:07:28,139: t15.2023.11.03 val PER: 0.1750
2026-01-08 14:07:28,140: t15.2023.11.04 val PER: 0.0444
2026-01-08 14:07:28,142: t15.2023.11.17 val PER: 0.0373
2026-01-08 14:07:28,143: t15.2023.11.19 val PER: 0.0319
2026-01-08 14:07:28,144: t15.2023.11.26 val PER: 0.1145
2026-01-08 14:07:28,146: t15.2023.12.03 val PER: 0.1113
2026-01-08 14:07:28,147: t15.2023.12.08 val PER: 0.0999
2026-01-08 14:07:28,148: t15.2023.12.10 val PER: 0.0867
2026-01-08 14:07:28,150: t15.2023.12.17 val PER: 0.1383
2026-01-08 14:07:28,151: t15.2023.12.29 val PER: 0.1242
2026-01-08 14:07:28,152: t15.2024.02.25 val PER: 0.1011
2026-01-08 14:07:28,154: t15.2024.03.08 val PER: 0.2276
2026-01-08 14:07:28,155: t15.2024.03.15 val PER: 0.1970
2026-01-08 14:07:28,157: t15.2024.03.17 val PER: 0.1395
2026-01-08 14:07:28,158: t15.2024.05.10 val PER: 0.1560
2026-01-08 14:07:28,159: t15.2024.06.14 val PER: 0.1609
2026-01-08 14:07:28,160: t15.2024.07.19 val PER: 0.2208
2026-01-08 14:07:28,162: t15.2024.07.21 val PER: 0.0945
2026-01-08 14:07:28,163: t15.2024.07.28 val PER: 0.1301
2026-01-08 14:07:28,164: t15.2025.01.10 val PER: 0.2975
2026-01-08 14:07:28,167: t15.2025.01.12 val PER: 0.1517
2026-01-08 14:07:28,168: t15.2025.03.14 val PER: 0.3565
2026-01-08 14:07:28,169: t15.2025.03.16 val PER: 0.1767
2026-01-08 14:07:28,170: t15.2025.03.30 val PER: 0.2897
2026-01-08 14:07:28,172: t15.2025.04.13 val PER: 0.2197
2026-01-08 14:07:37,377: Train batch 15600: loss: 10.91 grad norm: 57.65 time: 0.063
2026-01-08 14:07:55,709: Train batch 15800: loss: 13.00 grad norm: 62.22 time: 0.067
2026-01-08 14:08:14,208: Train batch 16000: loss: 8.06 grad norm: 48.15 time: 0.057
2026-01-08 14:08:14,210: Running test after training batch: 16000
2026-01-08 14:08:14,348: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:08:19,103: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:08:19,156: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 14:08:30,668: Val batch 16000: PER (avg): 0.1475 CTC Loss (avg): 15.2904 WER(5gram): 16.10% (n=256) time: 16.455
2026-01-08 14:08:30,670: WER lens: avg_true_words=5.99 avg_pred_words=6.22 max_pred_words=12
2026-01-08 14:08:30,673: t15.2023.08.13 val PER: 0.1112
2026-01-08 14:08:30,674: t15.2023.08.18 val PER: 0.1073
2026-01-08 14:08:30,676: t15.2023.08.20 val PER: 0.1064
2026-01-08 14:08:30,678: t15.2023.08.25 val PER: 0.0949
2026-01-08 14:08:30,679: t15.2023.08.27 val PER: 0.1833
2026-01-08 14:08:30,681: t15.2023.09.01 val PER: 0.0755
2026-01-08 14:08:30,683: t15.2023.09.03 val PER: 0.1496
2026-01-08 14:08:30,684: t15.2023.09.24 val PER: 0.1226
2026-01-08 14:08:30,686: t15.2023.09.29 val PER: 0.1270
2026-01-08 14:08:30,688: t15.2023.10.01 val PER: 0.1717
2026-01-08 14:08:30,690: t15.2023.10.06 val PER: 0.0850
2026-01-08 14:08:30,691: t15.2023.10.08 val PER: 0.2449
2026-01-08 14:08:30,693: t15.2023.10.13 val PER: 0.1963
2026-01-08 14:08:30,695: t15.2023.10.15 val PER: 0.1450
2026-01-08 14:08:30,696: t15.2023.10.20 val PER: 0.1779
2026-01-08 14:08:30,698: t15.2023.10.22 val PER: 0.0980
2026-01-08 14:08:30,699: t15.2023.11.03 val PER: 0.1757
2026-01-08 14:08:30,701: t15.2023.11.04 val PER: 0.0375
2026-01-08 14:08:30,702: t15.2023.11.17 val PER: 0.0373
2026-01-08 14:08:30,704: t15.2023.11.19 val PER: 0.0319
2026-01-08 14:08:30,706: t15.2023.11.26 val PER: 0.1217
2026-01-08 14:08:30,707: t15.2023.12.03 val PER: 0.1092
2026-01-08 14:08:30,709: t15.2023.12.08 val PER: 0.0959
2026-01-08 14:08:30,710: t15.2023.12.10 val PER: 0.0959
2026-01-08 14:08:30,712: t15.2023.12.17 val PER: 0.1331
2026-01-08 14:08:30,714: t15.2023.12.29 val PER: 0.1256
2026-01-08 14:08:30,715: t15.2024.02.25 val PER: 0.1039
2026-01-08 14:08:30,717: t15.2024.03.08 val PER: 0.2262
2026-01-08 14:08:30,718: t15.2024.03.15 val PER: 0.1989
2026-01-08 14:08:30,720: t15.2024.03.17 val PER: 0.1283
2026-01-08 14:08:30,721: t15.2024.05.10 val PER: 0.1634
2026-01-08 14:08:30,723: t15.2024.06.14 val PER: 0.1609
2026-01-08 14:08:30,725: t15.2024.07.19 val PER: 0.2248
2026-01-08 14:08:30,726: t15.2024.07.21 val PER: 0.0972
2026-01-08 14:08:30,729: t15.2024.07.28 val PER: 0.1294
2026-01-08 14:08:30,731: t15.2025.01.10 val PER: 0.3044
2026-01-08 14:08:30,732: t15.2025.01.12 val PER: 0.1463
2026-01-08 14:08:30,734: t15.2025.03.14 val PER: 0.3462
2026-01-08 14:08:30,735: t15.2025.03.16 val PER: 0.1924
2026-01-08 14:08:30,737: t15.2025.03.30 val PER: 0.2874
2026-01-08 14:08:30,738: t15.2025.04.13 val PER: 0.2140
2026-01-08 14:08:49,021: Train batch 16200: loss: 5.80 grad norm: 40.03 time: 0.058
2026-01-08 14:09:07,256: Train batch 16400: loss: 10.10 grad norm: 63.43 time: 0.060
2026-01-08 14:09:16,498: Running test after training batch: 16500
2026-01-08 14:09:16,637: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:09:21,390: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:09:21,454: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 14:09:33,128: Val batch 16500: PER (avg): 0.1471 CTC Loss (avg): 15.1697 WER(5gram): 15.71% (n=256) time: 16.628
2026-01-08 14:09:33,130: WER lens: avg_true_words=5.99 avg_pred_words=6.22 max_pred_words=12
2026-01-08 14:09:33,132: t15.2023.08.13 val PER: 0.1143
2026-01-08 14:09:33,133: t15.2023.08.18 val PER: 0.0972
2026-01-08 14:09:33,135: t15.2023.08.20 val PER: 0.1056
2026-01-08 14:09:33,136: t15.2023.08.25 val PER: 0.0873
2026-01-08 14:09:33,137: t15.2023.08.27 val PER: 0.1849
2026-01-08 14:09:33,139: t15.2023.09.01 val PER: 0.0747
2026-01-08 14:09:33,140: t15.2023.09.03 val PER: 0.1556
2026-01-08 14:09:33,141: t15.2023.09.24 val PER: 0.1262
2026-01-08 14:09:33,143: t15.2023.09.29 val PER: 0.1225
2026-01-08 14:09:33,144: t15.2023.10.01 val PER: 0.1731
2026-01-08 14:09:33,147: t15.2023.10.06 val PER: 0.0872
2026-01-08 14:09:33,149: t15.2023.10.08 val PER: 0.2463
2026-01-08 14:09:33,150: t15.2023.10.13 val PER: 0.1924
2026-01-08 14:09:33,152: t15.2023.10.15 val PER: 0.1463
2026-01-08 14:09:33,153: t15.2023.10.20 val PER: 0.1711
2026-01-08 14:09:33,155: t15.2023.10.22 val PER: 0.0980
2026-01-08 14:09:33,156: t15.2023.11.03 val PER: 0.1750
2026-01-08 14:09:33,157: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:09:33,159: t15.2023.11.17 val PER: 0.0389
2026-01-08 14:09:33,160: t15.2023.11.19 val PER: 0.0379
2026-01-08 14:09:33,162: t15.2023.11.26 val PER: 0.1203
2026-01-08 14:09:33,163: t15.2023.12.03 val PER: 0.1092
2026-01-08 14:09:33,165: t15.2023.12.08 val PER: 0.0999
2026-01-08 14:09:33,166: t15.2023.12.10 val PER: 0.0946
2026-01-08 14:09:33,167: t15.2023.12.17 val PER: 0.1351
2026-01-08 14:09:33,169: t15.2023.12.29 val PER: 0.1270
2026-01-08 14:09:33,170: t15.2024.02.25 val PER: 0.1053
2026-01-08 14:09:33,172: t15.2024.03.08 val PER: 0.2304
2026-01-08 14:09:33,173: t15.2024.03.15 val PER: 0.2033
2026-01-08 14:09:33,174: t15.2024.03.17 val PER: 0.1360
2026-01-08 14:09:33,176: t15.2024.05.10 val PER: 0.1605
2026-01-08 14:09:33,177: t15.2024.06.14 val PER: 0.1562
2026-01-08 14:09:33,179: t15.2024.07.19 val PER: 0.2215
2026-01-08 14:09:33,180: t15.2024.07.21 val PER: 0.0897
2026-01-08 14:09:33,182: t15.2024.07.28 val PER: 0.1287
2026-01-08 14:09:33,184: t15.2025.01.10 val PER: 0.3003
2026-01-08 14:09:33,185: t15.2025.01.12 val PER: 0.1463
2026-01-08 14:09:33,187: t15.2025.03.14 val PER: 0.3476
2026-01-08 14:09:33,188: t15.2025.03.16 val PER: 0.1859
2026-01-08 14:09:33,190: t15.2025.03.30 val PER: 0.2816
2026-01-08 14:09:33,191: t15.2025.04.13 val PER: 0.2140
2026-01-08 14:09:42,399: Train batch 16600: loss: 8.25 grad norm: 51.29 time: 0.053
2026-01-08 14:10:00,554: Train batch 16800: loss: 15.95 grad norm: 73.23 time: 0.063
2026-01-08 14:10:18,669: Train batch 17000: loss: 7.71 grad norm: 47.35 time: 0.083
2026-01-08 14:10:18,671: Running test after training batch: 17000
2026-01-08 14:10:18,780: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:10:23,811: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:10:23,865: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 14:10:35,492: Val batch 17000: PER (avg): 0.1449 CTC Loss (avg): 15.0774 WER(5gram): 15.65% (n=256) time: 16.819
2026-01-08 14:10:35,494: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-08 14:10:35,496: t15.2023.08.13 val PER: 0.1133
2026-01-08 14:10:35,497: t15.2023.08.18 val PER: 0.1006
2026-01-08 14:10:35,499: t15.2023.08.20 val PER: 0.1080
2026-01-08 14:10:35,500: t15.2023.08.25 val PER: 0.0934
2026-01-08 14:10:35,502: t15.2023.08.27 val PER: 0.1785
2026-01-08 14:10:35,504: t15.2023.09.01 val PER: 0.0714
2026-01-08 14:10:35,505: t15.2023.09.03 val PER: 0.1496
2026-01-08 14:10:35,506: t15.2023.09.24 val PER: 0.1274
2026-01-08 14:10:35,508: t15.2023.09.29 val PER: 0.1270
2026-01-08 14:10:35,509: t15.2023.10.01 val PER: 0.1638
2026-01-08 14:10:35,511: t15.2023.10.06 val PER: 0.0861
2026-01-08 14:10:35,512: t15.2023.10.08 val PER: 0.2355
2026-01-08 14:10:35,513: t15.2023.10.13 val PER: 0.1916
2026-01-08 14:10:35,515: t15.2023.10.15 val PER: 0.1457
2026-01-08 14:10:35,516: t15.2023.10.20 val PER: 0.1711
2026-01-08 14:10:35,518: t15.2023.10.22 val PER: 0.0991
2026-01-08 14:10:35,519: t15.2023.11.03 val PER: 0.1750
2026-01-08 14:10:35,520: t15.2023.11.04 val PER: 0.0375
2026-01-08 14:10:35,522: t15.2023.11.17 val PER: 0.0435
2026-01-08 14:10:35,523: t15.2023.11.19 val PER: 0.0339
2026-01-08 14:10:35,525: t15.2023.11.26 val PER: 0.1138
2026-01-08 14:10:35,526: t15.2023.12.03 val PER: 0.1113
2026-01-08 14:10:35,527: t15.2023.12.08 val PER: 0.0952
2026-01-08 14:10:35,529: t15.2023.12.10 val PER: 0.0907
2026-01-08 14:10:35,530: t15.2023.12.17 val PER: 0.1279
2026-01-08 14:10:35,532: t15.2023.12.29 val PER: 0.1229
2026-01-08 14:10:35,533: t15.2024.02.25 val PER: 0.0997
2026-01-08 14:10:35,534: t15.2024.03.08 val PER: 0.2333
2026-01-08 14:10:35,536: t15.2024.03.15 val PER: 0.1995
2026-01-08 14:10:35,537: t15.2024.03.17 val PER: 0.1325
2026-01-08 14:10:35,538: t15.2024.05.10 val PER: 0.1605
2026-01-08 14:10:35,540: t15.2024.06.14 val PER: 0.1640
2026-01-08 14:10:35,541: t15.2024.07.19 val PER: 0.2182
2026-01-08 14:10:35,543: t15.2024.07.21 val PER: 0.0910
2026-01-08 14:10:35,544: t15.2024.07.28 val PER: 0.1331
2026-01-08 14:10:35,545: t15.2025.01.10 val PER: 0.2865
2026-01-08 14:10:35,547: t15.2025.01.12 val PER: 0.1339
2026-01-08 14:10:35,548: t15.2025.03.14 val PER: 0.3476
2026-01-08 14:10:35,549: t15.2025.03.16 val PER: 0.1741
2026-01-08 14:10:35,551: t15.2025.03.30 val PER: 0.2713
2026-01-08 14:10:35,552: t15.2025.04.13 val PER: 0.2197
2026-01-08 14:10:53,334: Train batch 17200: loss: 8.81 grad norm: 47.20 time: 0.085
2026-01-08 14:11:10,704: Train batch 17400: loss: 10.85 grad norm: 60.04 time: 0.071
2026-01-08 14:11:19,256: Running test after training batch: 17500
2026-01-08 14:11:19,394: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:11:24,472: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:11:24,525: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 14:11:36,126: Val batch 17500: PER (avg): 0.1446 CTC Loss (avg): 15.0575 WER(5gram): 14.99% (n=256) time: 16.867
2026-01-08 14:11:36,128: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-08 14:11:36,130: t15.2023.08.13 val PER: 0.1185
2026-01-08 14:11:36,134: t15.2023.08.18 val PER: 0.0964
2026-01-08 14:11:36,136: t15.2023.08.20 val PER: 0.1056
2026-01-08 14:11:36,137: t15.2023.08.25 val PER: 0.0873
2026-01-08 14:11:36,139: t15.2023.08.27 val PER: 0.1704
2026-01-08 14:11:36,141: t15.2023.09.01 val PER: 0.0739
2026-01-08 14:11:36,142: t15.2023.09.03 val PER: 0.1449
2026-01-08 14:11:36,146: t15.2023.09.24 val PER: 0.1347
2026-01-08 14:11:36,148: t15.2023.09.29 val PER: 0.1213
2026-01-08 14:11:36,150: t15.2023.10.01 val PER: 0.1658
2026-01-08 14:11:36,152: t15.2023.10.06 val PER: 0.0818
2026-01-08 14:11:36,155: t15.2023.10.08 val PER: 0.2422
2026-01-08 14:11:36,156: t15.2023.10.13 val PER: 0.1877
2026-01-08 14:11:36,158: t15.2023.10.15 val PER: 0.1457
2026-01-08 14:11:36,159: t15.2023.10.20 val PER: 0.1846
2026-01-08 14:11:36,161: t15.2023.10.22 val PER: 0.1024
2026-01-08 14:11:36,162: t15.2023.11.03 val PER: 0.1723
2026-01-08 14:11:36,164: t15.2023.11.04 val PER: 0.0307
2026-01-08 14:11:36,165: t15.2023.11.17 val PER: 0.0435
2026-01-08 14:11:36,167: t15.2023.11.19 val PER: 0.0379
2026-01-08 14:11:36,168: t15.2023.11.26 val PER: 0.1181
2026-01-08 14:11:36,170: t15.2023.12.03 val PER: 0.1061
2026-01-08 14:11:36,171: t15.2023.12.08 val PER: 0.0945
2026-01-08 14:11:36,173: t15.2023.12.10 val PER: 0.0894
2026-01-08 14:11:36,174: t15.2023.12.17 val PER: 0.1299
2026-01-08 14:11:36,176: t15.2023.12.29 val PER: 0.1263
2026-01-08 14:11:36,178: t15.2024.02.25 val PER: 0.0983
2026-01-08 14:11:36,179: t15.2024.03.08 val PER: 0.2205
2026-01-08 14:11:36,181: t15.2024.03.15 val PER: 0.1945
2026-01-08 14:11:36,182: t15.2024.03.17 val PER: 0.1276
2026-01-08 14:11:36,184: t15.2024.05.10 val PER: 0.1530
2026-01-08 14:11:36,185: t15.2024.06.14 val PER: 0.1546
2026-01-08 14:11:36,187: t15.2024.07.19 val PER: 0.2254
2026-01-08 14:11:36,189: t15.2024.07.21 val PER: 0.0834
2026-01-08 14:11:36,191: t15.2024.07.28 val PER: 0.1272
2026-01-08 14:11:36,192: t15.2025.01.10 val PER: 0.2989
2026-01-08 14:11:36,194: t15.2025.01.12 val PER: 0.1401
2026-01-08 14:11:36,196: t15.2025.03.14 val PER: 0.3521
2026-01-08 14:11:36,198: t15.2025.03.16 val PER: 0.1793
2026-01-08 14:11:36,199: t15.2025.03.30 val PER: 0.2839
2026-01-08 14:11:36,201: t15.2025.04.13 val PER: 0.2254
2026-01-08 14:11:44,603: Train batch 17600: loss: 8.93 grad norm: 56.93 time: 0.051
2026-01-08 14:12:02,216: Train batch 17800: loss: 6.03 grad norm: 51.59 time: 0.042
2026-01-08 14:12:19,728: Train batch 18000: loss: 10.18 grad norm: 65.10 time: 0.061
2026-01-08 14:12:19,730: Running test after training batch: 18000
2026-01-08 14:12:19,864: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:12:25,020: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:12:25,072: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 14:12:36,673: Val batch 18000: PER (avg): 0.1437 CTC Loss (avg): 15.0082 WER(5gram): 15.38% (n=256) time: 16.941
2026-01-08 14:12:36,675: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=12
2026-01-08 14:12:36,677: t15.2023.08.13 val PER: 0.1112
2026-01-08 14:12:36,679: t15.2023.08.18 val PER: 0.0964
2026-01-08 14:12:36,681: t15.2023.08.20 val PER: 0.1033
2026-01-08 14:12:36,683: t15.2023.08.25 val PER: 0.0889
2026-01-08 14:12:36,685: t15.2023.08.27 val PER: 0.1736
2026-01-08 14:12:36,687: t15.2023.09.01 val PER: 0.0722
2026-01-08 14:12:36,689: t15.2023.09.03 val PER: 0.1473
2026-01-08 14:12:36,690: t15.2023.09.24 val PER: 0.1286
2026-01-08 14:12:36,692: t15.2023.09.29 val PER: 0.1213
2026-01-08 14:12:36,694: t15.2023.10.01 val PER: 0.1658
2026-01-08 14:12:36,695: t15.2023.10.06 val PER: 0.0850
2026-01-08 14:12:36,697: t15.2023.10.08 val PER: 0.2368
2026-01-08 14:12:36,698: t15.2023.10.13 val PER: 0.1893
2026-01-08 14:12:36,700: t15.2023.10.15 val PER: 0.1450
2026-01-08 14:12:36,701: t15.2023.10.20 val PER: 0.1846
2026-01-08 14:12:36,703: t15.2023.10.22 val PER: 0.1002
2026-01-08 14:12:36,704: t15.2023.11.03 val PER: 0.1723
2026-01-08 14:12:36,706: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:12:36,707: t15.2023.11.17 val PER: 0.0327
2026-01-08 14:12:36,709: t15.2023.11.19 val PER: 0.0339
2026-01-08 14:12:36,710: t15.2023.11.26 val PER: 0.1123
2026-01-08 14:12:36,712: t15.2023.12.03 val PER: 0.1040
2026-01-08 14:12:36,713: t15.2023.12.08 val PER: 0.0939
2026-01-08 14:12:36,715: t15.2023.12.10 val PER: 0.0880
2026-01-08 14:12:36,716: t15.2023.12.17 val PER: 0.1331
2026-01-08 14:12:36,717: t15.2023.12.29 val PER: 0.1256
2026-01-08 14:12:36,719: t15.2024.02.25 val PER: 0.1025
2026-01-08 14:12:36,720: t15.2024.03.08 val PER: 0.2290
2026-01-08 14:12:36,721: t15.2024.03.15 val PER: 0.1945
2026-01-08 14:12:36,723: t15.2024.03.17 val PER: 0.1255
2026-01-08 14:12:36,724: t15.2024.05.10 val PER: 0.1545
2026-01-08 14:12:36,726: t15.2024.06.14 val PER: 0.1514
2026-01-08 14:12:36,727: t15.2024.07.19 val PER: 0.2221
2026-01-08 14:12:36,728: t15.2024.07.21 val PER: 0.0869
2026-01-08 14:12:36,730: t15.2024.07.28 val PER: 0.1324
2026-01-08 14:12:36,731: t15.2025.01.10 val PER: 0.2906
2026-01-08 14:12:36,733: t15.2025.01.12 val PER: 0.1432
2026-01-08 14:12:36,734: t15.2025.03.14 val PER: 0.3373
2026-01-08 14:12:36,735: t15.2025.03.16 val PER: 0.1885
2026-01-08 14:12:36,737: t15.2025.03.30 val PER: 0.2816
2026-01-08 14:12:36,738: t15.2025.04.13 val PER: 0.2140
2026-01-08 14:12:55,420: Train batch 18200: loss: 7.12 grad norm: 47.04 time: 0.078
2026-01-08 14:13:13,102: Train batch 18400: loss: 4.47 grad norm: 42.45 time: 0.060
2026-01-08 14:13:22,222: Running test after training batch: 18500
2026-01-08 14:13:22,332: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:13:27,106: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:13:27,163: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 14:13:38,986: Val batch 18500: PER (avg): 0.1437 CTC Loss (avg): 15.0019 WER(5gram): 15.78% (n=256) time: 16.762
2026-01-08 14:13:38,989: WER lens: avg_true_words=5.99 avg_pred_words=6.20 max_pred_words=12
2026-01-08 14:13:38,991: t15.2023.08.13 val PER: 0.1154
2026-01-08 14:13:38,993: t15.2023.08.18 val PER: 0.0981
2026-01-08 14:13:38,994: t15.2023.08.20 val PER: 0.1048
2026-01-08 14:13:38,996: t15.2023.08.25 val PER: 0.0934
2026-01-08 14:13:38,997: t15.2023.08.27 val PER: 0.1736
2026-01-08 14:13:38,999: t15.2023.09.01 val PER: 0.0722
2026-01-08 14:13:39,001: t15.2023.09.03 val PER: 0.1473
2026-01-08 14:13:39,002: t15.2023.09.24 val PER: 0.1299
2026-01-08 14:13:39,004: t15.2023.09.29 val PER: 0.1219
2026-01-08 14:13:39,005: t15.2023.10.01 val PER: 0.1658
2026-01-08 14:13:39,007: t15.2023.10.06 val PER: 0.0850
2026-01-08 14:13:39,009: t15.2023.10.08 val PER: 0.2314
2026-01-08 14:13:39,011: t15.2023.10.13 val PER: 0.1885
2026-01-08 14:13:39,012: t15.2023.10.15 val PER: 0.1424
2026-01-08 14:13:39,014: t15.2023.10.20 val PER: 0.1779
2026-01-08 14:13:39,015: t15.2023.10.22 val PER: 0.0947
2026-01-08 14:13:39,017: t15.2023.11.03 val PER: 0.1750
2026-01-08 14:13:39,018: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:13:39,020: t15.2023.11.17 val PER: 0.0451
2026-01-08 14:13:39,021: t15.2023.11.19 val PER: 0.0379
2026-01-08 14:13:39,023: t15.2023.11.26 val PER: 0.1109
2026-01-08 14:13:39,024: t15.2023.12.03 val PER: 0.1050
2026-01-08 14:13:39,025: t15.2023.12.08 val PER: 0.0925
2026-01-08 14:13:39,027: t15.2023.12.10 val PER: 0.0920
2026-01-08 14:13:39,028: t15.2023.12.17 val PER: 0.1320
2026-01-08 14:13:39,030: t15.2023.12.29 val PER: 0.1297
2026-01-08 14:13:39,031: t15.2024.02.25 val PER: 0.0997
2026-01-08 14:13:39,033: t15.2024.03.08 val PER: 0.2248
2026-01-08 14:13:39,034: t15.2024.03.15 val PER: 0.1970
2026-01-08 14:13:39,036: t15.2024.03.17 val PER: 0.1262
2026-01-08 14:13:39,037: t15.2024.05.10 val PER: 0.1501
2026-01-08 14:13:39,039: t15.2024.06.14 val PER: 0.1609
2026-01-08 14:13:39,040: t15.2024.07.19 val PER: 0.2221
2026-01-08 14:13:39,042: t15.2024.07.21 val PER: 0.0848
2026-01-08 14:13:39,043: t15.2024.07.28 val PER: 0.1338
2026-01-08 14:13:39,045: t15.2025.01.10 val PER: 0.2948
2026-01-08 14:13:39,046: t15.2025.01.12 val PER: 0.1378
2026-01-08 14:13:39,048: t15.2025.03.14 val PER: 0.3358
2026-01-08 14:13:39,049: t15.2025.03.16 val PER: 0.1793
2026-01-08 14:13:39,051: t15.2025.03.30 val PER: 0.2747
2026-01-08 14:13:39,052: t15.2025.04.13 val PER: 0.2126
2026-01-08 14:13:47,821: Train batch 18600: loss: 11.69 grad norm: 60.73 time: 0.067
2026-01-08 14:14:05,528: Train batch 18800: loss: 7.53 grad norm: 49.00 time: 0.065
2026-01-08 14:14:23,101: Train batch 19000: loss: 7.35 grad norm: 42.92 time: 0.065
2026-01-08 14:14:23,103: Running test after training batch: 19000
2026-01-08 14:14:23,227: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:14:28,005: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:14:28,063: WER debug example
  GT : how does it keep the cost down
  PR : how does it keep the cost it
2026-01-08 14:14:39,986: Val batch 19000: PER (avg): 0.1437 CTC Loss (avg): 15.0279 WER(5gram): 15.45% (n=256) time: 16.882
2026-01-08 14:14:39,988: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=12
2026-01-08 14:14:39,990: t15.2023.08.13 val PER: 0.1081
2026-01-08 14:14:39,992: t15.2023.08.18 val PER: 0.0997
2026-01-08 14:14:39,994: t15.2023.08.20 val PER: 0.1033
2026-01-08 14:14:39,995: t15.2023.08.25 val PER: 0.0858
2026-01-08 14:14:39,997: t15.2023.08.27 val PER: 0.1688
2026-01-08 14:14:39,998: t15.2023.09.01 val PER: 0.0722
2026-01-08 14:14:40,000: t15.2023.09.03 val PER: 0.1496
2026-01-08 14:14:40,002: t15.2023.09.24 val PER: 0.1335
2026-01-08 14:14:40,003: t15.2023.09.29 val PER: 0.1213
2026-01-08 14:14:40,005: t15.2023.10.01 val PER: 0.1651
2026-01-08 14:14:40,006: t15.2023.10.06 val PER: 0.0829
2026-01-08 14:14:40,008: t15.2023.10.08 val PER: 0.2314
2026-01-08 14:14:40,010: t15.2023.10.13 val PER: 0.1862
2026-01-08 14:14:40,011: t15.2023.10.15 val PER: 0.1417
2026-01-08 14:14:40,013: t15.2023.10.20 val PER: 0.1812
2026-01-08 14:14:40,015: t15.2023.10.22 val PER: 0.0947
2026-01-08 14:14:40,016: t15.2023.11.03 val PER: 0.1730
2026-01-08 14:14:40,018: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:14:40,020: t15.2023.11.17 val PER: 0.0420
2026-01-08 14:14:40,021: t15.2023.11.19 val PER: 0.0319
2026-01-08 14:14:40,023: t15.2023.11.26 val PER: 0.1145
2026-01-08 14:14:40,024: t15.2023.12.03 val PER: 0.1092
2026-01-08 14:14:40,026: t15.2023.12.08 val PER: 0.0899
2026-01-08 14:14:40,028: t15.2023.12.10 val PER: 0.0946
2026-01-08 14:14:40,030: t15.2023.12.17 val PER: 0.1279
2026-01-08 14:14:40,031: t15.2023.12.29 val PER: 0.1270
2026-01-08 14:14:40,033: t15.2024.02.25 val PER: 0.1039
2026-01-08 14:14:40,034: t15.2024.03.08 val PER: 0.2304
2026-01-08 14:14:40,036: t15.2024.03.15 val PER: 0.1945
2026-01-08 14:14:40,037: t15.2024.03.17 val PER: 0.1297
2026-01-08 14:14:40,039: t15.2024.05.10 val PER: 0.1516
2026-01-08 14:14:40,041: t15.2024.06.14 val PER: 0.1577
2026-01-08 14:14:40,043: t15.2024.07.19 val PER: 0.2254
2026-01-08 14:14:40,044: t15.2024.07.21 val PER: 0.0883
2026-01-08 14:14:40,047: t15.2024.07.28 val PER: 0.1324
2026-01-08 14:14:40,049: t15.2025.01.10 val PER: 0.2975
2026-01-08 14:14:40,050: t15.2025.01.12 val PER: 0.1378
2026-01-08 14:14:40,052: t15.2025.03.14 val PER: 0.3402
2026-01-08 14:14:40,053: t15.2025.03.16 val PER: 0.1819
2026-01-08 14:14:40,055: t15.2025.03.30 val PER: 0.2782
2026-01-08 14:14:40,056: t15.2025.04.13 val PER: 0.2154
2026-01-08 14:14:57,322: Train batch 19200: loss: 5.82 grad norm: 47.12 time: 0.064
2026-01-08 14:15:14,794: Train batch 19400: loss: 4.69 grad norm: 36.43 time: 0.053
2026-01-08 14:15:23,417: Running test after training batch: 19500
2026-01-08 14:15:23,544: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:15:28,284: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:15:28,339: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 14:15:40,013: Val batch 19500: PER (avg): 0.1445 CTC Loss (avg): 14.9698 WER(5gram): 15.78% (n=256) time: 16.595
2026-01-08 14:15:40,016: WER lens: avg_true_words=5.99 avg_pred_words=6.21 max_pred_words=12
2026-01-08 14:15:40,018: t15.2023.08.13 val PER: 0.1112
2026-01-08 14:15:40,020: t15.2023.08.18 val PER: 0.0981
2026-01-08 14:15:40,022: t15.2023.08.20 val PER: 0.1072
2026-01-08 14:15:40,023: t15.2023.08.25 val PER: 0.0858
2026-01-08 14:15:40,025: t15.2023.08.27 val PER: 0.1672
2026-01-08 14:15:40,027: t15.2023.09.01 val PER: 0.0739
2026-01-08 14:15:40,028: t15.2023.09.03 val PER: 0.1390
2026-01-08 14:15:40,030: t15.2023.09.24 val PER: 0.1286
2026-01-08 14:15:40,032: t15.2023.09.29 val PER: 0.1232
2026-01-08 14:15:40,033: t15.2023.10.01 val PER: 0.1691
2026-01-08 14:15:40,035: t15.2023.10.06 val PER: 0.0883
2026-01-08 14:15:40,036: t15.2023.10.08 val PER: 0.2260
2026-01-08 14:15:40,038: t15.2023.10.13 val PER: 0.1901
2026-01-08 14:15:40,039: t15.2023.10.15 val PER: 0.1450
2026-01-08 14:15:40,041: t15.2023.10.20 val PER: 0.1846
2026-01-08 14:15:40,042: t15.2023.10.22 val PER: 0.1002
2026-01-08 14:15:40,044: t15.2023.11.03 val PER: 0.1784
2026-01-08 14:15:40,045: t15.2023.11.04 val PER: 0.0410
2026-01-08 14:15:40,047: t15.2023.11.17 val PER: 0.0389
2026-01-08 14:15:40,048: t15.2023.11.19 val PER: 0.0359
2026-01-08 14:15:40,050: t15.2023.11.26 val PER: 0.1130
2026-01-08 14:15:40,051: t15.2023.12.03 val PER: 0.1082
2026-01-08 14:15:40,053: t15.2023.12.08 val PER: 0.0925
2026-01-08 14:15:40,054: t15.2023.12.10 val PER: 0.0867
2026-01-08 14:15:40,056: t15.2023.12.17 val PER: 0.1320
2026-01-08 14:15:40,058: t15.2023.12.29 val PER: 0.1263
2026-01-08 14:15:40,059: t15.2024.02.25 val PER: 0.1039
2026-01-08 14:15:40,061: t15.2024.03.08 val PER: 0.2248
2026-01-08 14:15:40,062: t15.2024.03.15 val PER: 0.1920
2026-01-08 14:15:40,063: t15.2024.03.17 val PER: 0.1318
2026-01-08 14:15:40,065: t15.2024.05.10 val PER: 0.1560
2026-01-08 14:15:40,067: t15.2024.06.14 val PER: 0.1562
2026-01-08 14:15:40,072: t15.2024.07.19 val PER: 0.2248
2026-01-08 14:15:40,074: t15.2024.07.21 val PER: 0.0869
2026-01-08 14:15:40,075: t15.2024.07.28 val PER: 0.1279
2026-01-08 14:15:40,077: t15.2025.01.10 val PER: 0.2920
2026-01-08 14:15:40,078: t15.2025.01.12 val PER: 0.1424
2026-01-08 14:15:40,079: t15.2025.03.14 val PER: 0.3432
2026-01-08 14:15:40,081: t15.2025.03.16 val PER: 0.1846
2026-01-08 14:15:40,082: t15.2025.03.30 val PER: 0.2862
2026-01-08 14:15:40,083: t15.2025.04.13 val PER: 0.2197
2026-01-08 14:15:48,905: Train batch 19600: loss: 7.84 grad norm: 48.14 time: 0.058
2026-01-08 14:16:06,483: Train batch 19800: loss: 6.65 grad norm: 45.74 time: 0.056
2026-01-08 14:16:24,165: Running test after training batch: 19999
2026-01-08 14:16:24,259: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:16:29,014: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:16:29,066: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-08 14:16:40,775: Val batch 19999: PER (avg): 0.1433 CTC Loss (avg): 14.9604 WER(5gram): 16.17% (n=256) time: 16.607
2026-01-08 14:16:40,777: WER lens: avg_true_words=5.99 avg_pred_words=6.22 max_pred_words=13
2026-01-08 14:16:40,779: t15.2023.08.13 val PER: 0.1112
2026-01-08 14:16:40,781: t15.2023.08.18 val PER: 0.0981
2026-01-08 14:16:40,782: t15.2023.08.20 val PER: 0.1064
2026-01-08 14:16:40,784: t15.2023.08.25 val PER: 0.0873
2026-01-08 14:16:40,785: t15.2023.08.27 val PER: 0.1720
2026-01-08 14:16:40,786: t15.2023.09.01 val PER: 0.0706
2026-01-08 14:16:40,789: t15.2023.09.03 val PER: 0.1437
2026-01-08 14:16:40,791: t15.2023.09.24 val PER: 0.1214
2026-01-08 14:16:40,792: t15.2023.09.29 val PER: 0.1161
2026-01-08 14:16:40,794: t15.2023.10.01 val PER: 0.1671
2026-01-08 14:16:40,795: t15.2023.10.06 val PER: 0.0850
2026-01-08 14:16:40,796: t15.2023.10.08 val PER: 0.2287
2026-01-08 14:16:40,799: t15.2023.10.13 val PER: 0.1877
2026-01-08 14:16:40,800: t15.2023.10.15 val PER: 0.1430
2026-01-08 14:16:40,802: t15.2023.10.20 val PER: 0.1812
2026-01-08 14:16:40,803: t15.2023.10.22 val PER: 0.0991
2026-01-08 14:16:40,804: t15.2023.11.03 val PER: 0.1757
2026-01-08 14:16:40,806: t15.2023.11.04 val PER: 0.0410
2026-01-08 14:16:40,807: t15.2023.11.17 val PER: 0.0404
2026-01-08 14:16:40,808: t15.2023.11.19 val PER: 0.0339
2026-01-08 14:16:40,810: t15.2023.11.26 val PER: 0.1080
2026-01-08 14:16:40,811: t15.2023.12.03 val PER: 0.1071
2026-01-08 14:16:40,812: t15.2023.12.08 val PER: 0.0965
2026-01-08 14:16:40,814: t15.2023.12.10 val PER: 0.0920
2026-01-08 14:16:40,815: t15.2023.12.17 val PER: 0.1279
2026-01-08 14:16:40,816: t15.2023.12.29 val PER: 0.1242
2026-01-08 14:16:40,817: t15.2024.02.25 val PER: 0.1053
2026-01-08 14:16:40,819: t15.2024.03.08 val PER: 0.2290
2026-01-08 14:16:40,820: t15.2024.03.15 val PER: 0.1932
2026-01-08 14:16:40,821: t15.2024.03.17 val PER: 0.1227
2026-01-08 14:16:40,823: t15.2024.05.10 val PER: 0.1516
2026-01-08 14:16:40,824: t15.2024.06.14 val PER: 0.1593
2026-01-08 14:16:40,825: t15.2024.07.19 val PER: 0.2281
2026-01-08 14:16:40,827: t15.2024.07.21 val PER: 0.0897
2026-01-08 14:16:40,828: t15.2024.07.28 val PER: 0.1243
2026-01-08 14:16:40,830: t15.2025.01.10 val PER: 0.2920
2026-01-08 14:16:40,831: t15.2025.01.12 val PER: 0.1440
2026-01-08 14:16:40,832: t15.2025.03.14 val PER: 0.3476
2026-01-08 14:16:40,834: t15.2025.03.16 val PER: 0.1819
2026-01-08 14:16:40,835: t15.2025.03.30 val PER: 0.2759
2026-01-08 14:16:40,836: t15.2025.04.13 val PER: 0.2154
2026-01-08 14:16:41,299: Best avg val PER achieved: 0.14812
2026-01-08 14:16:41,301: Total training time: 46.28 minutes

=== RUN id10_wd1e-5.yaml ===
JOB_OUT_DIR=/home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id10_wd1e-5
2026-01-08 14:18:38,636: Using device: cuda:0
2026-01-08 14:22:28,128: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/languageModel
2026-01-08 14:22:28,152: Using 45 sessions after filtering (from 45).
2026-01-08 14:22:28,598: Using torch.compile (if available)
2026-01-08 14:22:28,599: torch.compile not available (torch<2.0). Skipping.
2026-01-08 14:22:28,601: Initialized RNN decoding model
2026-01-08 14:22:28,603: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.1, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-08 14:22:28,604: Model has 44,907,305 parameters
2026-01-08 14:22:28,606: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-08 14:22:29,858: Successfully initialized datasets
2026-01-08 14:22:29,860: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-08 14:22:31,366: Train batch 0: loss: 581.36 grad norm: 1417.85 time: 0.186
2026-01-08 14:22:31,367: Running test after training batch: 0
2026-01-08 14:22:31,479: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:22:37,176: WER debug example
  GT : you can see the code at this point as well
  PR : she has from his
2026-01-08 14:22:38,197: WER debug example
  GT : how does it keep the cost down
  PR : money from
2026-01-08 14:26:29,625: Val batch 0: PER (avg): 1.4300 CTC Loss (avg): 633.1626 WER(5gram): 99.67% (n=256) time: 238.256
2026-01-08 14:26:29,633: WER lens: avg_true_words=5.99 avg_pred_words=2.81 max_pred_words=7
2026-01-08 14:26:29,637: t15.2023.08.13 val PER: 1.3035
2026-01-08 14:26:29,641: t15.2023.08.18 val PER: 1.4250
2026-01-08 14:26:29,642: t15.2023.08.20 val PER: 1.2986
2026-01-08 14:26:29,642: t15.2023.08.25 val PER: 1.3434
2026-01-08 14:26:29,642: t15.2023.08.27 val PER: 1.2524
2026-01-08 14:26:29,642: t15.2023.09.01 val PER: 1.4562
2026-01-08 14:26:29,642: t15.2023.09.03 val PER: 1.3171
2026-01-08 14:26:29,642: t15.2023.09.24 val PER: 1.5413
2026-01-08 14:26:29,643: t15.2023.09.29 val PER: 1.4716
2026-01-08 14:26:29,643: t15.2023.10.01 val PER: 1.2094
2026-01-08 14:26:29,643: t15.2023.10.06 val PER: 1.4919
2026-01-08 14:26:29,643: t15.2023.10.08 val PER: 1.1894
2026-01-08 14:26:29,643: t15.2023.10.13 val PER: 1.3910
2026-01-08 14:26:29,643: t15.2023.10.15 val PER: 1.3896
2026-01-08 14:26:29,643: t15.2023.10.20 val PER: 1.4933
2026-01-08 14:26:29,643: t15.2023.10.22 val PER: 1.3942
2026-01-08 14:26:29,644: t15.2023.11.03 val PER: 1.5936
2026-01-08 14:26:29,644: t15.2023.11.04 val PER: 2.0273
2026-01-08 14:26:29,644: t15.2023.11.17 val PER: 1.9580
2026-01-08 14:26:29,644: t15.2023.11.19 val PER: 1.6826
2026-01-08 14:26:29,644: t15.2023.11.26 val PER: 1.5391
2026-01-08 14:26:29,644: t15.2023.12.03 val PER: 1.4338
2026-01-08 14:26:29,644: t15.2023.12.08 val PER: 1.4534
2026-01-08 14:26:29,644: t15.2023.12.10 val PER: 1.7017
2026-01-08 14:26:29,644: t15.2023.12.17 val PER: 1.3046
2026-01-08 14:26:29,644: t15.2023.12.29 val PER: 1.4111
2026-01-08 14:26:29,645: t15.2024.02.25 val PER: 1.4270
2026-01-08 14:26:29,645: t15.2024.03.08 val PER: 1.3229
2026-01-08 14:26:29,645: t15.2024.03.15 val PER: 1.3164
2026-01-08 14:26:29,645: t15.2024.03.17 val PER: 1.3982
2026-01-08 14:26:29,645: t15.2024.05.10 val PER: 1.3195
2026-01-08 14:26:29,645: t15.2024.06.14 val PER: 1.5284
2026-01-08 14:26:29,645: t15.2024.07.19 val PER: 1.0798
2026-01-08 14:26:29,645: t15.2024.07.21 val PER: 1.6366
2026-01-08 14:26:29,645: t15.2024.07.28 val PER: 1.6529
2026-01-08 14:26:29,646: t15.2025.01.10 val PER: 1.0909
2026-01-08 14:26:29,646: t15.2025.01.12 val PER: 1.7683
2026-01-08 14:26:29,646: t15.2025.03.14 val PER: 1.0399
2026-01-08 14:26:29,646: t15.2025.03.16 val PER: 1.6139
2026-01-08 14:26:29,646: t15.2025.03.30 val PER: 1.2931
2026-01-08 14:26:29,646: t15.2025.04.13 val PER: 1.5849
2026-01-08 14:26:29,646: New best val WER(5gram) inf% --> 99.67%
2026-01-08 14:26:29,647: Checkpointing model
2026-01-08 14:26:29,792: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-08 14:26:47,646: Train batch 200: loss: 78.32 grad norm: 92.99 time: 0.056
2026-01-08 14:27:04,886: Train batch 400: loss: 52.73 grad norm: 79.01 time: 0.065
2026-01-08 14:27:13,423: Running test after training batch: 500
2026-01-08 14:27:13,550: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:27:18,573: WER debug example
  GT : you can see the code at this point as well
  PR : e t e t t t t
2026-01-08 14:27:18,761: WER debug example
  GT : how does it keep the cost down
  PR : d c d e c
2026-01-08 14:27:57,371: Val batch 500: PER (avg): 0.5024 CTC Loss (avg): 52.2829 WER(5gram): 97.72% (n=256) time: 43.946
2026-01-08 14:27:57,374: WER lens: avg_true_words=5.99 avg_pred_words=4.34 max_pred_words=11
2026-01-08 14:27:57,376: t15.2023.08.13 val PER: 0.4605
2026-01-08 14:27:57,377: t15.2023.08.18 val PER: 0.4376
2026-01-08 14:27:57,379: t15.2023.08.20 val PER: 0.4567
2026-01-08 14:27:57,380: t15.2023.08.25 val PER: 0.4217
2026-01-08 14:27:57,382: t15.2023.08.27 val PER: 0.5338
2026-01-08 14:27:57,383: t15.2023.09.01 val PER: 0.4172
2026-01-08 14:27:57,385: t15.2023.09.03 val PER: 0.4929
2026-01-08 14:27:57,386: t15.2023.09.24 val PER: 0.4345
2026-01-08 14:27:57,388: t15.2023.09.29 val PER: 0.4371
2026-01-08 14:27:57,389: t15.2023.10.01 val PER: 0.4908
2026-01-08 14:27:57,391: t15.2023.10.06 val PER: 0.4198
2026-01-08 14:27:57,392: t15.2023.10.08 val PER: 0.5413
2026-01-08 14:27:57,394: t15.2023.10.13 val PER: 0.5337
2026-01-08 14:27:57,395: t15.2023.10.15 val PER: 0.4845
2026-01-08 14:27:57,397: t15.2023.10.20 val PER: 0.4597
2026-01-08 14:27:57,399: t15.2023.10.22 val PER: 0.4432
2026-01-08 14:27:57,400: t15.2023.11.03 val PER: 0.4803
2026-01-08 14:27:57,402: t15.2023.11.04 val PER: 0.2867
2026-01-08 14:27:57,403: t15.2023.11.17 val PER: 0.3546
2026-01-08 14:27:57,405: t15.2023.11.19 val PER: 0.3234
2026-01-08 14:27:57,406: t15.2023.11.26 val PER: 0.5210
2026-01-08 14:27:57,408: t15.2023.12.03 val PER: 0.4832
2026-01-08 14:27:57,409: t15.2023.12.08 val PER: 0.5107
2026-01-08 14:27:57,411: t15.2023.12.10 val PER: 0.4402
2026-01-08 14:27:57,412: t15.2023.12.17 val PER: 0.5457
2026-01-08 14:27:57,414: t15.2023.12.29 val PER: 0.5244
2026-01-08 14:27:57,416: t15.2024.02.25 val PER: 0.4635
2026-01-08 14:27:57,417: t15.2024.03.08 val PER: 0.5633
2026-01-08 14:27:57,418: t15.2024.03.15 val PER: 0.5322
2026-01-08 14:27:57,420: t15.2024.03.17 val PER: 0.4909
2026-01-08 14:27:57,422: t15.2024.05.10 val PER: 0.5082
2026-01-08 14:27:57,424: t15.2024.06.14 val PER: 0.5331
2026-01-08 14:27:57,426: t15.2024.07.19 val PER: 0.6480
2026-01-08 14:27:57,427: t15.2024.07.21 val PER: 0.4710
2026-01-08 14:27:57,429: t15.2024.07.28 val PER: 0.5059
2026-01-08 14:27:57,430: t15.2025.01.10 val PER: 0.7149
2026-01-08 14:27:57,432: t15.2025.01.12 val PER: 0.5335
2026-01-08 14:27:57,434: t15.2025.03.14 val PER: 0.6953
2026-01-08 14:27:57,435: t15.2025.03.16 val PER: 0.5759
2026-01-08 14:27:57,436: t15.2025.03.30 val PER: 0.7046
2026-01-08 14:27:57,438: t15.2025.04.13 val PER: 0.5378
2026-01-08 14:27:57,439: New best val WER(5gram) 99.67% --> 97.72%
2026-01-08 14:27:57,441: Checkpointing model
2026-01-08 14:27:57,580: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-08 14:28:06,267: Train batch 600: loss: 48.44 grad norm: 78.58 time: 0.079
2026-01-08 14:28:23,442: Train batch 800: loss: 41.35 grad norm: 88.28 time: 0.057
2026-01-08 14:28:41,183: Train batch 1000: loss: 42.52 grad norm: 76.11 time: 0.067
2026-01-08 14:28:41,186: Running test after training batch: 1000
2026-01-08 14:28:41,306: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:28:46,225: WER debug example
  GT : you can see the code at this point as well
  PR : u o t e a t t t t t y li
2026-01-08 14:28:46,312: WER debug example
  GT : how does it keep the cost down
  PR : d d c d e p a c and
2026-01-08 14:29:10,226: Val batch 1000: PER (avg): 0.4164 CTC Loss (avg): 41.6791 WER(5gram): 111.21% (n=256) time: 29.038
2026-01-08 14:29:10,228: WER lens: avg_true_words=5.99 avg_pred_words=6.78 max_pred_words=15
2026-01-08 14:29:10,244: t15.2023.08.13 val PER: 0.3857
2026-01-08 14:29:10,246: t15.2023.08.18 val PER: 0.3462
2026-01-08 14:29:10,249: t15.2023.08.20 val PER: 0.3407
2026-01-08 14:29:10,251: t15.2023.08.25 val PER: 0.3238
2026-01-08 14:29:10,254: t15.2023.08.27 val PER: 0.4389
2026-01-08 14:29:10,256: t15.2023.09.01 val PER: 0.3198
2026-01-08 14:29:10,257: t15.2023.09.03 val PER: 0.4133
2026-01-08 14:29:10,258: t15.2023.09.24 val PER: 0.3410
2026-01-08 14:29:10,262: t15.2023.09.29 val PER: 0.3676
2026-01-08 14:29:10,268: t15.2023.10.01 val PER: 0.4181
2026-01-08 14:29:10,271: t15.2023.10.06 val PER: 0.3315
2026-01-08 14:29:10,275: t15.2023.10.08 val PER: 0.4628
2026-01-08 14:29:10,278: t15.2023.10.13 val PER: 0.4779
2026-01-08 14:29:10,280: t15.2023.10.15 val PER: 0.3955
2026-01-08 14:29:10,282: t15.2023.10.20 val PER: 0.3456
2026-01-08 14:29:10,283: t15.2023.10.22 val PER: 0.3452
2026-01-08 14:29:10,285: t15.2023.11.03 val PER: 0.4091
2026-01-08 14:29:10,288: t15.2023.11.04 val PER: 0.1775
2026-01-08 14:29:10,290: t15.2023.11.17 val PER: 0.2737
2026-01-08 14:29:10,295: t15.2023.11.19 val PER: 0.2415
2026-01-08 14:29:10,298: t15.2023.11.26 val PER: 0.4406
2026-01-08 14:29:10,302: t15.2023.12.03 val PER: 0.3918
2026-01-08 14:29:10,304: t15.2023.12.08 val PER: 0.4088
2026-01-08 14:29:10,308: t15.2023.12.10 val PER: 0.3666
2026-01-08 14:29:10,310: t15.2023.12.17 val PER: 0.4179
2026-01-08 14:29:10,311: t15.2023.12.29 val PER: 0.4146
2026-01-08 14:29:10,313: t15.2024.02.25 val PER: 0.3638
2026-01-08 14:29:10,315: t15.2024.03.08 val PER: 0.4964
2026-01-08 14:29:10,317: t15.2024.03.15 val PER: 0.4540
2026-01-08 14:29:10,318: t15.2024.03.17 val PER: 0.4031
2026-01-08 14:29:10,320: t15.2024.05.10 val PER: 0.4339
2026-01-08 14:29:10,322: t15.2024.06.14 val PER: 0.4227
2026-01-08 14:29:10,325: t15.2024.07.19 val PER: 0.5419
2026-01-08 14:29:10,327: t15.2024.07.21 val PER: 0.3855
2026-01-08 14:29:10,329: t15.2024.07.28 val PER: 0.4213
2026-01-08 14:29:10,330: t15.2025.01.10 val PER: 0.6198
2026-01-08 14:29:10,331: t15.2025.01.12 val PER: 0.4388
2026-01-08 14:29:10,333: t15.2025.03.14 val PER: 0.6287
2026-01-08 14:29:10,335: t15.2025.03.16 val PER: 0.5223
2026-01-08 14:29:10,343: t15.2025.03.30 val PER: 0.6632
2026-01-08 14:29:10,345: t15.2025.04.13 val PER: 0.4922
2026-01-08 14:29:28,112: Train batch 1200: loss: 34.11 grad norm: 73.81 time: 0.070
2026-01-08 14:29:46,149: Train batch 1400: loss: 36.38 grad norm: 76.53 time: 0.061
2026-01-08 14:29:54,837: Running test after training batch: 1500
2026-01-08 14:29:54,981: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:29:59,961: WER debug example
  GT : you can see the code at this point as well
  PR : you ou no e a d d d y
2026-01-08 14:30:00,034: WER debug example
  GT : how does it keep the cost down
  PR : c t p a c to
2026-01-08 14:30:20,603: Val batch 1500: PER (avg): 0.3781 CTC Loss (avg): 36.6735 WER(5gram): 102.74% (n=256) time: 25.764
2026-01-08 14:30:20,605: WER lens: avg_true_words=5.99 avg_pred_words=5.55 max_pred_words=15
2026-01-08 14:30:20,607: t15.2023.08.13 val PER: 0.3524
2026-01-08 14:30:20,608: t15.2023.08.18 val PER: 0.3185
2026-01-08 14:30:20,610: t15.2023.08.20 val PER: 0.3106
2026-01-08 14:30:20,611: t15.2023.08.25 val PER: 0.2590
2026-01-08 14:30:20,613: t15.2023.08.27 val PER: 0.3971
2026-01-08 14:30:20,614: t15.2023.09.01 val PER: 0.2711
2026-01-08 14:30:20,615: t15.2023.09.03 val PER: 0.3634
2026-01-08 14:30:20,616: t15.2023.09.24 val PER: 0.3155
2026-01-08 14:30:20,618: t15.2023.09.29 val PER: 0.3299
2026-01-08 14:30:20,619: t15.2023.10.01 val PER: 0.3890
2026-01-08 14:30:20,620: t15.2023.10.06 val PER: 0.2971
2026-01-08 14:30:20,621: t15.2023.10.08 val PER: 0.4371
2026-01-08 14:30:20,623: t15.2023.10.13 val PER: 0.4414
2026-01-08 14:30:20,624: t15.2023.10.15 val PER: 0.3586
2026-01-08 14:30:20,626: t15.2023.10.20 val PER: 0.3154
2026-01-08 14:30:20,627: t15.2023.10.22 val PER: 0.3073
2026-01-08 14:30:20,628: t15.2023.11.03 val PER: 0.3684
2026-01-08 14:30:20,629: t15.2023.11.04 val PER: 0.1331
2026-01-08 14:30:20,630: t15.2023.11.17 val PER: 0.2395
2026-01-08 14:30:20,632: t15.2023.11.19 val PER: 0.1956
2026-01-08 14:30:20,633: t15.2023.11.26 val PER: 0.4051
2026-01-08 14:30:20,634: t15.2023.12.03 val PER: 0.3508
2026-01-08 14:30:20,635: t15.2023.12.08 val PER: 0.3569
2026-01-08 14:30:20,637: t15.2023.12.10 val PER: 0.3101
2026-01-08 14:30:20,638: t15.2023.12.17 val PER: 0.3742
2026-01-08 14:30:20,639: t15.2023.12.29 val PER: 0.3850
2026-01-08 14:30:20,640: t15.2024.02.25 val PER: 0.3034
2026-01-08 14:30:20,642: t15.2024.03.08 val PER: 0.4424
2026-01-08 14:30:20,643: t15.2024.03.15 val PER: 0.4246
2026-01-08 14:30:20,644: t15.2024.03.17 val PER: 0.3794
2026-01-08 14:30:20,646: t15.2024.05.10 val PER: 0.3938
2026-01-08 14:30:20,647: t15.2024.06.14 val PER: 0.3959
2026-01-08 14:30:20,648: t15.2024.07.19 val PER: 0.5162
2026-01-08 14:30:20,649: t15.2024.07.21 val PER: 0.3469
2026-01-08 14:30:20,651: t15.2024.07.28 val PER: 0.3596
2026-01-08 14:30:20,652: t15.2025.01.10 val PER: 0.5978
2026-01-08 14:30:20,653: t15.2025.01.12 val PER: 0.4018
2026-01-08 14:30:20,655: t15.2025.03.14 val PER: 0.5976
2026-01-08 14:30:20,656: t15.2025.03.16 val PER: 0.4372
2026-01-08 14:30:20,657: t15.2025.03.30 val PER: 0.6540
2026-01-08 14:30:20,659: t15.2025.04.13 val PER: 0.4394
2026-01-08 14:30:29,167: Train batch 1600: loss: 36.05 grad norm: 79.22 time: 0.065
2026-01-08 14:30:46,569: Train batch 1800: loss: 35.33 grad norm: 74.26 time: 0.089
2026-01-08 14:31:04,269: Train batch 2000: loss: 34.02 grad norm: 78.58 time: 0.069
2026-01-08 14:31:04,271: Running test after training batch: 2000
2026-01-08 14:31:04,377: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:31:09,623: WER debug example
  GT : you can see the code at this point as well
  PR : you ou no e a k and the us and we
2026-01-08 14:31:09,674: WER debug example
  GT : how does it keep the cost down
  PR : c t e p a c and
2026-01-08 14:31:25,921: Val batch 2000: PER (avg): 0.3384 CTC Loss (avg): 32.8714 WER(5gram): 99.61% (n=256) time: 21.648
2026-01-08 14:31:25,924: WER lens: avg_true_words=5.99 avg_pred_words=6.15 max_pred_words=15
2026-01-08 14:31:25,925: t15.2023.08.13 val PER: 0.3035
2026-01-08 14:31:25,927: t15.2023.08.18 val PER: 0.2791
2026-01-08 14:31:25,929: t15.2023.08.20 val PER: 0.2573
2026-01-08 14:31:25,931: t15.2023.08.25 val PER: 0.2334
2026-01-08 14:31:25,932: t15.2023.08.27 val PER: 0.3521
2026-01-08 14:31:25,934: t15.2023.09.01 val PER: 0.2394
2026-01-08 14:31:25,935: t15.2023.09.03 val PER: 0.3373
2026-01-08 14:31:25,937: t15.2023.09.24 val PER: 0.2633
2026-01-08 14:31:25,939: t15.2023.09.29 val PER: 0.2808
2026-01-08 14:31:25,940: t15.2023.10.01 val PER: 0.3402
2026-01-08 14:31:25,942: t15.2023.10.06 val PER: 0.2680
2026-01-08 14:31:25,943: t15.2023.10.08 val PER: 0.3992
2026-01-08 14:31:25,945: t15.2023.10.13 val PER: 0.3646
2026-01-08 14:31:25,946: t15.2023.10.15 val PER: 0.3059
2026-01-08 14:31:25,948: t15.2023.10.20 val PER: 0.2987
2026-01-08 14:31:25,949: t15.2023.10.22 val PER: 0.2762
2026-01-08 14:31:25,951: t15.2023.11.03 val PER: 0.3324
2026-01-08 14:31:25,953: t15.2023.11.04 val PER: 0.1058
2026-01-08 14:31:25,954: t15.2023.11.17 val PER: 0.1897
2026-01-08 14:31:25,956: t15.2023.11.19 val PER: 0.1577
2026-01-08 14:31:25,957: t15.2023.11.26 val PER: 0.3725
2026-01-08 14:31:25,959: t15.2023.12.03 val PER: 0.3246
2026-01-08 14:31:25,961: t15.2023.12.08 val PER: 0.3189
2026-01-08 14:31:25,962: t15.2023.12.10 val PER: 0.2825
2026-01-08 14:31:25,964: t15.2023.12.17 val PER: 0.3056
2026-01-08 14:31:25,966: t15.2023.12.29 val PER: 0.3500
2026-01-08 14:31:25,967: t15.2024.02.25 val PER: 0.2949
2026-01-08 14:31:25,969: t15.2024.03.08 val PER: 0.4097
2026-01-08 14:31:25,970: t15.2024.03.15 val PER: 0.3752
2026-01-08 14:31:25,972: t15.2024.03.17 val PER: 0.3445
2026-01-08 14:31:25,973: t15.2024.05.10 val PER: 0.3418
2026-01-08 14:31:25,975: t15.2024.06.14 val PER: 0.3628
2026-01-08 14:31:25,976: t15.2024.07.19 val PER: 0.4773
2026-01-08 14:31:25,977: t15.2024.07.21 val PER: 0.3021
2026-01-08 14:31:25,979: t15.2024.07.28 val PER: 0.3309
2026-01-08 14:31:25,980: t15.2025.01.10 val PER: 0.5661
2026-01-08 14:31:25,982: t15.2025.01.12 val PER: 0.3811
2026-01-08 14:31:25,983: t15.2025.03.14 val PER: 0.5695
2026-01-08 14:31:25,985: t15.2025.03.16 val PER: 0.4188
2026-01-08 14:31:25,986: t15.2025.03.30 val PER: 0.5897
2026-01-08 14:31:25,988: t15.2025.04.13 val PER: 0.4123
2026-01-08 14:31:44,218: Train batch 2200: loss: 29.46 grad norm: 68.85 time: 0.061
2026-01-08 14:32:02,426: Train batch 2400: loss: 29.29 grad norm: 68.74 time: 0.053
2026-01-08 14:32:11,775: Running test after training batch: 2500
2026-01-08 14:32:11,930: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:32:16,764: WER debug example
  GT : you can see the code at this point as well
  PR : you ou no e a do it this way
2026-01-08 14:32:16,801: WER debug example
  GT : how does it keep the cost down
  PR : how is it a
2026-01-08 14:32:30,033: Val batch 2500: PER (avg): 0.3152 CTC Loss (avg): 30.1687 WER(5gram): 95.70% (n=256) time: 18.255
2026-01-08 14:32:30,035: WER lens: avg_true_words=5.99 avg_pred_words=5.57 max_pred_words=13
2026-01-08 14:32:30,037: t15.2023.08.13 val PER: 0.2848
2026-01-08 14:32:30,038: t15.2023.08.18 val PER: 0.2590
2026-01-08 14:32:30,040: t15.2023.08.20 val PER: 0.2375
2026-01-08 14:32:30,042: t15.2023.08.25 val PER: 0.2244
2026-01-08 14:32:30,043: t15.2023.08.27 val PER: 0.3489
2026-01-08 14:32:30,045: t15.2023.09.01 val PER: 0.2183
2026-01-08 14:32:30,046: t15.2023.09.03 val PER: 0.3266
2026-01-08 14:32:30,048: t15.2023.09.24 val PER: 0.2464
2026-01-08 14:32:30,049: t15.2023.09.29 val PER: 0.2642
2026-01-08 14:32:30,051: t15.2023.10.01 val PER: 0.3263
2026-01-08 14:32:30,053: t15.2023.10.06 val PER: 0.2304
2026-01-08 14:32:30,054: t15.2023.10.08 val PER: 0.3654
2026-01-08 14:32:30,056: t15.2023.10.13 val PER: 0.3545
2026-01-08 14:32:30,058: t15.2023.10.15 val PER: 0.2841
2026-01-08 14:32:30,060: t15.2023.10.20 val PER: 0.2919
2026-01-08 14:32:30,061: t15.2023.10.22 val PER: 0.2584
2026-01-08 14:32:30,063: t15.2023.11.03 val PER: 0.3019
2026-01-08 14:32:30,064: t15.2023.11.04 val PER: 0.0887
2026-01-08 14:32:30,066: t15.2023.11.17 val PER: 0.1586
2026-01-08 14:32:30,067: t15.2023.11.19 val PER: 0.1238
2026-01-08 14:32:30,069: t15.2023.11.26 val PER: 0.3457
2026-01-08 14:32:30,071: t15.2023.12.03 val PER: 0.2868
2026-01-08 14:32:30,073: t15.2023.12.08 val PER: 0.2983
2026-01-08 14:32:30,074: t15.2023.12.10 val PER: 0.2615
2026-01-08 14:32:30,075: t15.2023.12.17 val PER: 0.3067
2026-01-08 14:32:30,077: t15.2023.12.29 val PER: 0.3246
2026-01-08 14:32:30,079: t15.2024.02.25 val PER: 0.2556
2026-01-08 14:32:30,080: t15.2024.03.08 val PER: 0.3656
2026-01-08 14:32:30,082: t15.2024.03.15 val PER: 0.3552
2026-01-08 14:32:30,084: t15.2024.03.17 val PER: 0.3285
2026-01-08 14:32:30,085: t15.2024.05.10 val PER: 0.3105
2026-01-08 14:32:30,087: t15.2024.06.14 val PER: 0.3391
2026-01-08 14:32:30,088: t15.2024.07.19 val PER: 0.4535
2026-01-08 14:32:30,091: t15.2024.07.21 val PER: 0.2724
2026-01-08 14:32:30,092: t15.2024.07.28 val PER: 0.3162
2026-01-08 14:32:30,094: t15.2025.01.10 val PER: 0.5124
2026-01-08 14:32:30,095: t15.2025.01.12 val PER: 0.3495
2026-01-08 14:32:30,097: t15.2025.03.14 val PER: 0.5266
2026-01-08 14:32:30,098: t15.2025.03.16 val PER: 0.3809
2026-01-08 14:32:30,100: t15.2025.03.30 val PER: 0.5529
2026-01-08 14:32:30,101: t15.2025.04.13 val PER: 0.4023
2026-01-08 14:32:30,103: New best val WER(5gram) 97.72% --> 95.70%
2026-01-08 14:32:30,105: Checkpointing model
2026-01-08 14:32:30,253: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-08 14:32:39,206: Train batch 2600: loss: 35.32 grad norm: 82.97 time: 0.056
2026-01-08 14:32:57,250: Train batch 2800: loss: 25.41 grad norm: 73.75 time: 0.082
2026-01-08 14:33:15,326: Train batch 3000: loss: 31.76 grad norm: 73.80 time: 0.084
2026-01-08 14:33:15,328: Running test after training batch: 3000
2026-01-08 14:33:15,432: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:33:20,365: WER debug example
  GT : you can see the code at this point as well
  PR : you ou no e a do it this way
2026-01-08 14:33:20,403: WER debug example
  GT : how does it keep the cost down
  PR : d c e t u p a a
2026-01-08 14:33:32,271: Val batch 3000: PER (avg): 0.2948 CTC Loss (avg): 27.8614 WER(5gram): 91.59% (n=256) time: 16.941
2026-01-08 14:33:32,273: WER lens: avg_true_words=5.99 avg_pred_words=5.82 max_pred_words=14
2026-01-08 14:33:32,275: t15.2023.08.13 val PER: 0.2599
2026-01-08 14:33:32,276: t15.2023.08.18 val PER: 0.2464
2026-01-08 14:33:32,278: t15.2023.08.20 val PER: 0.2288
2026-01-08 14:33:32,279: t15.2023.08.25 val PER: 0.1883
2026-01-08 14:33:32,281: t15.2023.08.27 val PER: 0.3280
2026-01-08 14:33:32,282: t15.2023.09.01 val PER: 0.1964
2026-01-08 14:33:32,283: t15.2023.09.03 val PER: 0.2957
2026-01-08 14:33:32,285: t15.2023.09.24 val PER: 0.2342
2026-01-08 14:33:32,286: t15.2023.09.29 val PER: 0.2457
2026-01-08 14:33:32,288: t15.2023.10.01 val PER: 0.3032
2026-01-08 14:33:32,289: t15.2023.10.06 val PER: 0.2034
2026-01-08 14:33:32,291: t15.2023.10.08 val PER: 0.3586
2026-01-08 14:33:32,292: t15.2023.10.13 val PER: 0.3437
2026-01-08 14:33:32,293: t15.2023.10.15 val PER: 0.2610
2026-01-08 14:33:32,295: t15.2023.10.20 val PER: 0.2953
2026-01-08 14:33:32,296: t15.2023.10.22 val PER: 0.2383
2026-01-08 14:33:32,297: t15.2023.11.03 val PER: 0.2870
2026-01-08 14:33:32,298: t15.2023.11.04 val PER: 0.0922
2026-01-08 14:33:32,300: t15.2023.11.17 val PER: 0.1400
2026-01-08 14:33:32,301: t15.2023.11.19 val PER: 0.1317
2026-01-08 14:33:32,302: t15.2023.11.26 val PER: 0.3036
2026-01-08 14:33:32,304: t15.2023.12.03 val PER: 0.2584
2026-01-08 14:33:32,305: t15.2023.12.08 val PER: 0.2736
2026-01-08 14:33:32,306: t15.2023.12.10 val PER: 0.2352
2026-01-08 14:33:32,308: t15.2023.12.17 val PER: 0.2879
2026-01-08 14:33:32,309: t15.2023.12.29 val PER: 0.3013
2026-01-08 14:33:32,310: t15.2024.02.25 val PER: 0.2542
2026-01-08 14:33:32,312: t15.2024.03.08 val PER: 0.3684
2026-01-08 14:33:32,313: t15.2024.03.15 val PER: 0.3458
2026-01-08 14:33:32,314: t15.2024.03.17 val PER: 0.2887
2026-01-08 14:33:32,315: t15.2024.05.10 val PER: 0.3120
2026-01-08 14:33:32,317: t15.2024.06.14 val PER: 0.3139
2026-01-08 14:33:32,318: t15.2024.07.19 val PER: 0.4192
2026-01-08 14:33:32,319: t15.2024.07.21 val PER: 0.2572
2026-01-08 14:33:32,321: t15.2024.07.28 val PER: 0.3007
2026-01-08 14:33:32,322: t15.2025.01.10 val PER: 0.5000
2026-01-08 14:33:32,323: t15.2025.01.12 val PER: 0.3364
2026-01-08 14:33:32,325: t15.2025.03.14 val PER: 0.4911
2026-01-08 14:33:32,326: t15.2025.03.16 val PER: 0.3586
2026-01-08 14:33:32,327: t15.2025.03.30 val PER: 0.5046
2026-01-08 14:33:32,328: t15.2025.04.13 val PER: 0.3766
2026-01-08 14:33:32,330: New best val WER(5gram) 95.70% --> 91.59%
2026-01-08 14:33:32,331: Checkpointing model
2026-01-08 14:33:32,480: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-08 14:33:50,872: Train batch 3200: loss: 26.28 grad norm: 67.39 time: 0.077
2026-01-08 14:34:08,162: Train batch 3400: loss: 19.30 grad norm: 67.82 time: 0.050
2026-01-08 14:34:16,799: Running test after training batch: 3500
2026-01-08 14:34:16,934: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:34:21,768: WER debug example
  GT : you can see the code at this point as well
  PR : you ou de e a k due to this boy we
2026-01-08 14:34:21,804: WER debug example
  GT : how does it keep the cost down
  PR : c t e p a c
2026-01-08 14:34:33,117: Val batch 3500: PER (avg): 0.2802 CTC Loss (avg): 26.5902 WER(5gram): 94.65% (n=256) time: 16.315
2026-01-08 14:34:33,120: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=14
2026-01-08 14:34:33,122: t15.2023.08.13 val PER: 0.2297
2026-01-08 14:34:33,124: t15.2023.08.18 val PER: 0.2313
2026-01-08 14:34:33,126: t15.2023.08.20 val PER: 0.2280
2026-01-08 14:34:33,127: t15.2023.08.25 val PER: 0.1717
2026-01-08 14:34:33,129: t15.2023.08.27 val PER: 0.3039
2026-01-08 14:34:33,131: t15.2023.09.01 val PER: 0.1932
2026-01-08 14:34:33,132: t15.2023.09.03 val PER: 0.2684
2026-01-08 14:34:33,135: t15.2023.09.24 val PER: 0.2087
2026-01-08 14:34:33,137: t15.2023.09.29 val PER: 0.2425
2026-01-08 14:34:33,140: t15.2023.10.01 val PER: 0.2966
2026-01-08 14:34:33,141: t15.2023.10.06 val PER: 0.1959
2026-01-08 14:34:33,143: t15.2023.10.08 val PER: 0.3383
2026-01-08 14:34:33,145: t15.2023.10.13 val PER: 0.3460
2026-01-08 14:34:33,146: t15.2023.10.15 val PER: 0.2538
2026-01-08 14:34:33,148: t15.2023.10.20 val PER: 0.2617
2026-01-08 14:34:33,150: t15.2023.10.22 val PER: 0.2094
2026-01-08 14:34:33,152: t15.2023.11.03 val PER: 0.2727
2026-01-08 14:34:33,154: t15.2023.11.04 val PER: 0.0785
2026-01-08 14:34:33,156: t15.2023.11.17 val PER: 0.1260
2026-01-08 14:34:33,158: t15.2023.11.19 val PER: 0.0998
2026-01-08 14:34:33,160: t15.2023.11.26 val PER: 0.2783
2026-01-08 14:34:33,162: t15.2023.12.03 val PER: 0.2437
2026-01-08 14:34:33,164: t15.2023.12.08 val PER: 0.2656
2026-01-08 14:34:33,166: t15.2023.12.10 val PER: 0.2102
2026-01-08 14:34:33,168: t15.2023.12.17 val PER: 0.2661
2026-01-08 14:34:33,169: t15.2023.12.29 val PER: 0.2883
2026-01-08 14:34:33,171: t15.2024.02.25 val PER: 0.2360
2026-01-08 14:34:33,173: t15.2024.03.08 val PER: 0.3343
2026-01-08 14:34:33,175: t15.2024.03.15 val PER: 0.3208
2026-01-08 14:34:33,177: t15.2024.03.17 val PER: 0.2943
2026-01-08 14:34:33,179: t15.2024.05.10 val PER: 0.2749
2026-01-08 14:34:33,181: t15.2024.06.14 val PER: 0.2965
2026-01-08 14:34:33,183: t15.2024.07.19 val PER: 0.4153
2026-01-08 14:34:33,184: t15.2024.07.21 val PER: 0.2421
2026-01-08 14:34:33,186: t15.2024.07.28 val PER: 0.2824
2026-01-08 14:34:33,187: t15.2025.01.10 val PER: 0.4821
2026-01-08 14:34:33,189: t15.2025.01.12 val PER: 0.3241
2026-01-08 14:34:33,190: t15.2025.03.14 val PER: 0.4793
2026-01-08 14:34:33,193: t15.2025.03.16 val PER: 0.3586
2026-01-08 14:34:33,195: t15.2025.03.30 val PER: 0.4793
2026-01-08 14:34:33,196: t15.2025.04.13 val PER: 0.3566
2026-01-08 14:34:42,439: Train batch 3600: loss: 23.25 grad norm: 65.61 time: 0.068
2026-01-08 14:35:01,042: Train batch 3800: loss: 25.95 grad norm: 64.07 time: 0.067
2026-01-08 14:35:19,370: Train batch 4000: loss: 19.32 grad norm: 57.64 time: 0.057
2026-01-08 14:35:19,372: Running test after training batch: 4000
2026-01-08 14:35:19,516: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:35:24,489: WER debug example
  GT : you can see the code at this point as well
  PR : you ou no e a due to this boy will
2026-01-08 14:35:24,528: WER debug example
  GT : how does it keep the cost down
  PR : how i see it the way to
2026-01-08 14:35:35,672: Val batch 4000: PER (avg): 0.2542 CTC Loss (avg): 24.5750 WER(5gram): 75.29% (n=256) time: 16.298
2026-01-08 14:35:35,893: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=14
2026-01-08 14:35:35,896: t15.2023.08.13 val PER: 0.2204
2026-01-08 14:35:35,898: t15.2023.08.18 val PER: 0.2045
2026-01-08 14:35:35,899: t15.2023.08.20 val PER: 0.2017
2026-01-08 14:35:35,901: t15.2023.08.25 val PER: 0.1596
2026-01-08 14:35:35,903: t15.2023.08.27 val PER: 0.2685
2026-01-08 14:35:35,908: t15.2023.09.01 val PER: 0.1550
2026-01-08 14:35:35,909: t15.2023.09.03 val PER: 0.2482
2026-01-08 14:35:35,911: t15.2023.09.24 val PER: 0.1930
2026-01-08 14:35:35,913: t15.2023.09.29 val PER: 0.2163
2026-01-08 14:35:35,914: t15.2023.10.01 val PER: 0.2616
2026-01-08 14:35:35,916: t15.2023.10.06 val PER: 0.1668
2026-01-08 14:35:35,917: t15.2023.10.08 val PER: 0.3383
2026-01-08 14:35:35,919: t15.2023.10.13 val PER: 0.2948
2026-01-08 14:35:35,921: t15.2023.10.15 val PER: 0.2327
2026-01-08 14:35:35,925: t15.2023.10.20 val PER: 0.2450
2026-01-08 14:35:35,926: t15.2023.10.22 val PER: 0.2094
2026-01-08 14:35:35,928: t15.2023.11.03 val PER: 0.2517
2026-01-08 14:35:35,929: t15.2023.11.04 val PER: 0.0683
2026-01-08 14:35:35,931: t15.2023.11.17 val PER: 0.1089
2026-01-08 14:35:35,933: t15.2023.11.19 val PER: 0.1038
2026-01-08 14:35:35,936: t15.2023.11.26 val PER: 0.2703
2026-01-08 14:35:35,938: t15.2023.12.03 val PER: 0.2195
2026-01-08 14:35:35,940: t15.2023.12.08 val PER: 0.2377
2026-01-08 14:35:35,942: t15.2023.12.10 val PER: 0.1761
2026-01-08 14:35:35,943: t15.2023.12.17 val PER: 0.2547
2026-01-08 14:35:35,946: t15.2023.12.29 val PER: 0.2759
2026-01-08 14:35:35,948: t15.2024.02.25 val PER: 0.2079
2026-01-08 14:35:35,950: t15.2024.03.08 val PER: 0.3201
2026-01-08 14:35:35,954: t15.2024.03.15 val PER: 0.2946
2026-01-08 14:35:35,955: t15.2024.03.17 val PER: 0.2524
2026-01-08 14:35:35,957: t15.2024.05.10 val PER: 0.2511
2026-01-08 14:35:35,958: t15.2024.06.14 val PER: 0.2902
2026-01-08 14:35:35,960: t15.2024.07.19 val PER: 0.3757
2026-01-08 14:35:35,962: t15.2024.07.21 val PER: 0.2083
2026-01-08 14:35:35,970: t15.2024.07.28 val PER: 0.2412
2026-01-08 14:35:35,973: t15.2025.01.10 val PER: 0.4449
2026-01-08 14:35:35,975: t15.2025.01.12 val PER: 0.2848
2026-01-08 14:35:35,976: t15.2025.03.14 val PER: 0.4364
2026-01-08 14:35:35,978: t15.2025.03.16 val PER: 0.3181
2026-01-08 14:35:35,980: t15.2025.03.30 val PER: 0.4356
2026-01-08 14:35:35,982: t15.2025.04.13 val PER: 0.3466
2026-01-08 14:35:35,984: New best val WER(5gram) 91.59% --> 75.29%
2026-01-08 14:35:35,985: Checkpointing model
2026-01-08 14:35:36,133: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-08 14:35:53,729: Train batch 4200: loss: 23.08 grad norm: 66.96 time: 0.080
2026-01-08 14:36:11,208: Train batch 4400: loss: 17.71 grad norm: 59.66 time: 0.067
2026-01-08 14:36:19,887: Running test after training batch: 4500
2026-01-08 14:36:20,013: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:36:24,930: WER debug example
  GT : you can see the code at this point as well
  PR : u k e a due to this boy will
2026-01-08 14:36:24,967: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the
2026-01-08 14:36:35,126: Val batch 4500: PER (avg): 0.2495 CTC Loss (avg): 23.7708 WER(5gram): 77.90% (n=256) time: 15.237
2026-01-08 14:36:35,129: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=13
2026-01-08 14:36:35,131: t15.2023.08.13 val PER: 0.2079
2026-01-08 14:36:35,132: t15.2023.08.18 val PER: 0.1978
2026-01-08 14:36:35,133: t15.2023.08.20 val PER: 0.1867
2026-01-08 14:36:35,135: t15.2023.08.25 val PER: 0.1551
2026-01-08 14:36:35,136: t15.2023.08.27 val PER: 0.2621
2026-01-08 14:36:35,137: t15.2023.09.01 val PER: 0.1631
2026-01-08 14:36:35,138: t15.2023.09.03 val PER: 0.2482
2026-01-08 14:36:35,140: t15.2023.09.24 val PER: 0.1942
2026-01-08 14:36:35,141: t15.2023.09.29 val PER: 0.2119
2026-01-08 14:36:35,143: t15.2023.10.01 val PER: 0.2583
2026-01-08 14:36:35,144: t15.2023.10.06 val PER: 0.1593
2026-01-08 14:36:35,145: t15.2023.10.08 val PER: 0.3315
2026-01-08 14:36:35,147: t15.2023.10.13 val PER: 0.2933
2026-01-08 14:36:35,149: t15.2023.10.15 val PER: 0.2307
2026-01-08 14:36:35,150: t15.2023.10.20 val PER: 0.2315
2026-01-08 14:36:35,151: t15.2023.10.22 val PER: 0.2105
2026-01-08 14:36:35,153: t15.2023.11.03 val PER: 0.2558
2026-01-08 14:36:35,154: t15.2023.11.04 val PER: 0.0546
2026-01-08 14:36:35,155: t15.2023.11.17 val PER: 0.0995
2026-01-08 14:36:35,156: t15.2023.11.19 val PER: 0.0918
2026-01-08 14:36:35,158: t15.2023.11.26 val PER: 0.2543
2026-01-08 14:36:35,159: t15.2023.12.03 val PER: 0.2269
2026-01-08 14:36:35,160: t15.2023.12.08 val PER: 0.2337
2026-01-08 14:36:35,163: t15.2023.12.10 val PER: 0.1879
2026-01-08 14:36:35,165: t15.2023.12.17 val PER: 0.2536
2026-01-08 14:36:35,166: t15.2023.12.29 val PER: 0.2581
2026-01-08 14:36:35,167: t15.2024.02.25 val PER: 0.2093
2026-01-08 14:36:35,169: t15.2024.03.08 val PER: 0.3058
2026-01-08 14:36:35,170: t15.2024.03.15 val PER: 0.2989
2026-01-08 14:36:35,171: t15.2024.03.17 val PER: 0.2531
2026-01-08 14:36:35,173: t15.2024.05.10 val PER: 0.2897
2026-01-08 14:36:35,174: t15.2024.06.14 val PER: 0.2618
2026-01-08 14:36:35,175: t15.2024.07.19 val PER: 0.3546
2026-01-08 14:36:35,177: t15.2024.07.21 val PER: 0.1959
2026-01-08 14:36:35,178: t15.2024.07.28 val PER: 0.2419
2026-01-08 14:36:35,179: t15.2025.01.10 val PER: 0.4298
2026-01-08 14:36:35,180: t15.2025.01.12 val PER: 0.2856
2026-01-08 14:36:35,182: t15.2025.03.14 val PER: 0.4157
2026-01-08 14:36:35,183: t15.2025.03.16 val PER: 0.3154
2026-01-08 14:36:35,184: t15.2025.03.30 val PER: 0.4356
2026-01-08 14:36:35,185: t15.2025.04.13 val PER: 0.3324
2026-01-08 14:36:43,874: Train batch 4600: loss: 21.02 grad norm: 65.06 time: 0.066
2026-01-08 14:37:01,284: Train batch 4800: loss: 15.46 grad norm: 58.80 time: 0.065
2026-01-08 14:37:18,695: Train batch 5000: loss: 33.80 grad norm: 86.62 time: 0.064
2026-01-08 14:37:18,698: Running test after training batch: 5000
2026-01-08 14:37:18,806: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:37:24,133: WER debug example
  GT : you can see the code at this point as well
  PR : you can e a due to this point will
2026-01-08 14:37:24,176: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the ca sta
2026-01-08 14:37:35,732: Val batch 5000: PER (avg): 0.2343 CTC Loss (avg): 22.5628 WER(5gram): 58.67% (n=256) time: 17.031
2026-01-08 14:37:35,734: WER lens: avg_true_words=5.99 avg_pred_words=6.11 max_pred_words=12
2026-01-08 14:37:35,737: t15.2023.08.13 val PER: 0.1985
2026-01-08 14:37:35,739: t15.2023.08.18 val PER: 0.1869
2026-01-08 14:37:35,741: t15.2023.08.20 val PER: 0.1787
2026-01-08 14:37:35,742: t15.2023.08.25 val PER: 0.1491
2026-01-08 14:37:35,744: t15.2023.08.27 val PER: 0.2508
2026-01-08 14:37:35,749: t15.2023.09.01 val PER: 0.1445
2026-01-08 14:37:35,752: t15.2023.09.03 val PER: 0.2375
2026-01-08 14:37:35,754: t15.2023.09.24 val PER: 0.1845
2026-01-08 14:37:35,756: t15.2023.09.29 val PER: 0.1978
2026-01-08 14:37:35,758: t15.2023.10.01 val PER: 0.2398
2026-01-08 14:37:35,760: t15.2023.10.06 val PER: 0.1550
2026-01-08 14:37:35,761: t15.2023.10.08 val PER: 0.3112
2026-01-08 14:37:35,763: t15.2023.10.13 val PER: 0.2801
2026-01-08 14:37:35,765: t15.2023.10.15 val PER: 0.2109
2026-01-08 14:37:35,766: t15.2023.10.20 val PER: 0.2752
2026-01-08 14:37:35,768: t15.2023.10.22 val PER: 0.1871
2026-01-08 14:37:35,770: t15.2023.11.03 val PER: 0.2388
2026-01-08 14:37:35,772: t15.2023.11.04 val PER: 0.0444
2026-01-08 14:37:35,774: t15.2023.11.17 val PER: 0.0809
2026-01-08 14:37:35,777: t15.2023.11.19 val PER: 0.0798
2026-01-08 14:37:35,778: t15.2023.11.26 val PER: 0.2377
2026-01-08 14:37:35,780: t15.2023.12.03 val PER: 0.2080
2026-01-08 14:37:35,782: t15.2023.12.08 val PER: 0.2150
2026-01-08 14:37:35,783: t15.2023.12.10 val PER: 0.1616
2026-01-08 14:37:35,785: t15.2023.12.17 val PER: 0.2277
2026-01-08 14:37:35,786: t15.2023.12.29 val PER: 0.2334
2026-01-08 14:37:35,788: t15.2024.02.25 val PER: 0.1966
2026-01-08 14:37:35,790: t15.2024.03.08 val PER: 0.2930
2026-01-08 14:37:35,791: t15.2024.03.15 val PER: 0.2871
2026-01-08 14:37:35,793: t15.2024.03.17 val PER: 0.2427
2026-01-08 14:37:35,794: t15.2024.05.10 val PER: 0.2318
2026-01-08 14:37:35,796: t15.2024.06.14 val PER: 0.2618
2026-01-08 14:37:35,798: t15.2024.07.19 val PER: 0.3342
2026-01-08 14:37:35,799: t15.2024.07.21 val PER: 0.2007
2026-01-08 14:37:35,801: t15.2024.07.28 val PER: 0.2507
2026-01-08 14:37:35,802: t15.2025.01.10 val PER: 0.3994
2026-01-08 14:37:35,804: t15.2025.01.12 val PER: 0.2587
2026-01-08 14:37:35,805: t15.2025.03.14 val PER: 0.4083
2026-01-08 14:37:35,807: t15.2025.03.16 val PER: 0.2880
2026-01-08 14:37:35,809: t15.2025.03.30 val PER: 0.4034
2026-01-08 14:37:35,810: t15.2025.04.13 val PER: 0.3153
2026-01-08 14:37:35,812: New best val WER(5gram) 75.29% --> 58.67%
2026-01-08 14:37:35,814: Checkpointing model
2026-01-08 14:37:35,959: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-08 14:37:54,627: Train batch 5200: loss: 17.65 grad norm: 63.97 time: 0.054
2026-01-08 14:38:13,112: Train batch 5400: loss: 17.81 grad norm: 62.00 time: 0.073
2026-01-08 14:38:22,287: Running test after training batch: 5500
2026-01-08 14:38:22,450: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:38:27,229: WER debug example
  GT : you can see the code at this point as well
  PR : you can see a cold to this point will
2026-01-08 14:38:27,264: WER debug example
  GT : how does it keep the cost down
  PR : how to see it keep the cost
2026-01-08 14:38:37,402: Val batch 5500: PER (avg): 0.2270 CTC Loss (avg): 21.5369 WER(5gram): 58.41% (n=256) time: 15.113
2026-01-08 14:38:37,404: WER lens: avg_true_words=5.99 avg_pred_words=6.12 max_pred_words=13
2026-01-08 14:38:37,406: t15.2023.08.13 val PER: 0.1944
2026-01-08 14:38:37,408: t15.2023.08.18 val PER: 0.1744
2026-01-08 14:38:37,409: t15.2023.08.20 val PER: 0.1644
2026-01-08 14:38:37,411: t15.2023.08.25 val PER: 0.1536
2026-01-08 14:38:37,412: t15.2023.08.27 val PER: 0.2572
2026-01-08 14:38:37,415: t15.2023.09.01 val PER: 0.1364
2026-01-08 14:38:37,416: t15.2023.09.03 val PER: 0.2280
2026-01-08 14:38:37,418: t15.2023.09.24 val PER: 0.1784
2026-01-08 14:38:37,419: t15.2023.09.29 val PER: 0.1883
2026-01-08 14:38:37,421: t15.2023.10.01 val PER: 0.2351
2026-01-08 14:38:37,422: t15.2023.10.06 val PER: 0.1518
2026-01-08 14:38:37,424: t15.2023.10.08 val PER: 0.3018
2026-01-08 14:38:37,425: t15.2023.10.13 val PER: 0.2839
2026-01-08 14:38:37,427: t15.2023.10.15 val PER: 0.2103
2026-01-08 14:38:37,428: t15.2023.10.20 val PER: 0.2517
2026-01-08 14:38:37,430: t15.2023.10.22 val PER: 0.1759
2026-01-08 14:38:37,431: t15.2023.11.03 val PER: 0.2273
2026-01-08 14:38:37,433: t15.2023.11.04 val PER: 0.0614
2026-01-08 14:38:37,437: t15.2023.11.17 val PER: 0.0886
2026-01-08 14:38:37,439: t15.2023.11.19 val PER: 0.0739
2026-01-08 14:38:37,441: t15.2023.11.26 val PER: 0.2355
2026-01-08 14:38:37,443: t15.2023.12.03 val PER: 0.1954
2026-01-08 14:38:37,444: t15.2023.12.08 val PER: 0.1924
2026-01-08 14:38:37,446: t15.2023.12.10 val PER: 0.1748
2026-01-08 14:38:37,447: t15.2023.12.17 val PER: 0.2152
2026-01-08 14:38:37,449: t15.2023.12.29 val PER: 0.2320
2026-01-08 14:38:37,450: t15.2024.02.25 val PER: 0.1882
2026-01-08 14:38:37,451: t15.2024.03.08 val PER: 0.3001
2026-01-08 14:38:37,453: t15.2024.03.15 val PER: 0.2614
2026-01-08 14:38:37,454: t15.2024.03.17 val PER: 0.2245
2026-01-08 14:38:37,455: t15.2024.05.10 val PER: 0.2496
2026-01-08 14:38:37,457: t15.2024.06.14 val PER: 0.2571
2026-01-08 14:38:37,458: t15.2024.07.19 val PER: 0.3388
2026-01-08 14:38:37,460: t15.2024.07.21 val PER: 0.1724
2026-01-08 14:38:37,461: t15.2024.07.28 val PER: 0.2404
2026-01-08 14:38:37,463: t15.2025.01.10 val PER: 0.3981
2026-01-08 14:38:37,464: t15.2025.01.12 val PER: 0.2540
2026-01-08 14:38:37,465: t15.2025.03.14 val PER: 0.3994
2026-01-08 14:38:37,467: t15.2025.03.16 val PER: 0.2788
2026-01-08 14:38:37,468: t15.2025.03.30 val PER: 0.3874
2026-01-08 14:38:37,470: t15.2025.04.13 val PER: 0.3138
2026-01-08 14:38:37,472: New best val WER(5gram) 58.67% --> 58.41%
2026-01-08 14:38:37,473: Checkpointing model
2026-01-08 14:38:37,620: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-08 14:38:46,822: Train batch 5600: loss: 21.09 grad norm: 87.17 time: 0.063
2026-01-08 14:39:05,180: Train batch 5800: loss: 14.81 grad norm: 59.32 time: 0.087
2026-01-08 14:39:23,579: Train batch 6000: loss: 14.33 grad norm: 57.72 time: 0.050
2026-01-08 14:39:23,581: Running test after training batch: 6000
2026-01-08 14:39:23,694: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:39:28,498: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the go to this point will
2026-01-08 14:39:28,536: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the
2026-01-08 14:39:38,169: Val batch 6000: PER (avg): 0.2177 CTC Loss (avg): 20.9570 WER(5gram): 53.78% (n=256) time: 14.584
2026-01-08 14:39:38,171: WER lens: avg_true_words=5.99 avg_pred_words=5.94 max_pred_words=14
2026-01-08 14:39:38,174: t15.2023.08.13 val PER: 0.1902
2026-01-08 14:39:38,176: t15.2023.08.18 val PER: 0.1785
2026-01-08 14:39:38,177: t15.2023.08.20 val PER: 0.1747
2026-01-08 14:39:38,178: t15.2023.08.25 val PER: 0.1521
2026-01-08 14:39:38,180: t15.2023.08.27 val PER: 0.2605
2026-01-08 14:39:38,182: t15.2023.09.01 val PER: 0.1372
2026-01-08 14:39:38,183: t15.2023.09.03 val PER: 0.2185
2026-01-08 14:39:38,184: t15.2023.09.24 val PER: 0.1748
2026-01-08 14:39:38,186: t15.2023.09.29 val PER: 0.1844
2026-01-08 14:39:38,187: t15.2023.10.01 val PER: 0.2259
2026-01-08 14:39:38,189: t15.2023.10.06 val PER: 0.1421
2026-01-08 14:39:38,190: t15.2023.10.08 val PER: 0.3031
2026-01-08 14:39:38,191: t15.2023.10.13 val PER: 0.2715
2026-01-08 14:39:38,193: t15.2023.10.15 val PER: 0.2070
2026-01-08 14:39:38,194: t15.2023.10.20 val PER: 0.2617
2026-01-08 14:39:38,195: t15.2023.10.22 val PER: 0.1748
2026-01-08 14:39:38,197: t15.2023.11.03 val PER: 0.2246
2026-01-08 14:39:38,198: t15.2023.11.04 val PER: 0.0580
2026-01-08 14:39:38,199: t15.2023.11.17 val PER: 0.0762
2026-01-08 14:39:38,201: t15.2023.11.19 val PER: 0.0818
2026-01-08 14:39:38,202: t15.2023.11.26 val PER: 0.2210
2026-01-08 14:39:38,203: t15.2023.12.03 val PER: 0.1828
2026-01-08 14:39:38,205: t15.2023.12.08 val PER: 0.1957
2026-01-08 14:39:38,206: t15.2023.12.10 val PER: 0.1695
2026-01-08 14:39:38,207: t15.2023.12.17 val PER: 0.1850
2026-01-08 14:39:38,209: t15.2023.12.29 val PER: 0.2272
2026-01-08 14:39:38,210: t15.2024.02.25 val PER: 0.1868
2026-01-08 14:39:38,211: t15.2024.03.08 val PER: 0.2817
2026-01-08 14:39:38,212: t15.2024.03.15 val PER: 0.2595
2026-01-08 14:39:38,214: t15.2024.03.17 val PER: 0.2099
2026-01-08 14:39:38,215: t15.2024.05.10 val PER: 0.2363
2026-01-08 14:39:38,217: t15.2024.06.14 val PER: 0.2271
2026-01-08 14:39:38,218: t15.2024.07.19 val PER: 0.3151
2026-01-08 14:39:38,219: t15.2024.07.21 val PER: 0.1628
2026-01-08 14:39:38,221: t15.2024.07.28 val PER: 0.2176
2026-01-08 14:39:38,222: t15.2025.01.10 val PER: 0.3705
2026-01-08 14:39:38,223: t15.2025.01.12 val PER: 0.2302
2026-01-08 14:39:38,225: t15.2025.03.14 val PER: 0.3698
2026-01-08 14:39:38,226: t15.2025.03.16 val PER: 0.2736
2026-01-08 14:39:38,227: t15.2025.03.30 val PER: 0.3563
2026-01-08 14:39:38,229: t15.2025.04.13 val PER: 0.2924
2026-01-08 14:39:38,230: New best val WER(5gram) 58.41% --> 53.78%
2026-01-08 14:39:38,231: Checkpointing model
2026-01-08 14:39:38,374: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-08 14:39:56,764: Train batch 6200: loss: 16.06 grad norm: 61.28 time: 0.070
2026-01-08 14:40:15,038: Train batch 6400: loss: 21.06 grad norm: 66.24 time: 0.063
2026-01-08 14:40:24,522: Running test after training batch: 6500
2026-01-08 14:40:24,653: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:40:29,447: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point is well
2026-01-08 14:40:29,486: WER debug example
  GT : how does it keep the cost down
  PR : how i see it keep the cost
2026-01-08 14:40:39,194: Val batch 6500: PER (avg): 0.2111 CTC Loss (avg): 20.5256 WER(5gram): 44.33% (n=256) time: 14.669
2026-01-08 14:40:39,196: WER lens: avg_true_words=5.99 avg_pred_words=6.13 max_pred_words=12
2026-01-08 14:40:39,198: t15.2023.08.13 val PER: 0.1830
2026-01-08 14:40:39,199: t15.2023.08.18 val PER: 0.1626
2026-01-08 14:40:39,200: t15.2023.08.20 val PER: 0.1628
2026-01-08 14:40:39,202: t15.2023.08.25 val PER: 0.1325
2026-01-08 14:40:39,203: t15.2023.08.27 val PER: 0.2379
2026-01-08 14:40:39,204: t15.2023.09.01 val PER: 0.1282
2026-01-08 14:40:39,205: t15.2023.09.03 val PER: 0.2126
2026-01-08 14:40:39,207: t15.2023.09.24 val PER: 0.1626
2026-01-08 14:40:39,208: t15.2023.09.29 val PER: 0.1691
2026-01-08 14:40:39,209: t15.2023.10.01 val PER: 0.2199
2026-01-08 14:40:39,211: t15.2023.10.06 val PER: 0.1356
2026-01-08 14:40:39,212: t15.2023.10.08 val PER: 0.2963
2026-01-08 14:40:39,213: t15.2023.10.13 val PER: 0.2700
2026-01-08 14:40:39,214: t15.2023.10.15 val PER: 0.2037
2026-01-08 14:40:39,216: t15.2023.10.20 val PER: 0.2215
2026-01-08 14:40:39,217: t15.2023.10.22 val PER: 0.1715
2026-01-08 14:40:39,219: t15.2023.11.03 val PER: 0.2205
2026-01-08 14:40:39,220: t15.2023.11.04 val PER: 0.0512
2026-01-08 14:40:39,221: t15.2023.11.17 val PER: 0.0747
2026-01-08 14:40:39,223: t15.2023.11.19 val PER: 0.0739
2026-01-08 14:40:39,224: t15.2023.11.26 val PER: 0.2072
2026-01-08 14:40:39,226: t15.2023.12.03 val PER: 0.1870
2026-01-08 14:40:39,227: t15.2023.12.08 val PER: 0.1831
2026-01-08 14:40:39,228: t15.2023.12.10 val PER: 0.1564
2026-01-08 14:40:39,229: t15.2023.12.17 val PER: 0.1965
2026-01-08 14:40:39,231: t15.2023.12.29 val PER: 0.2128
2026-01-08 14:40:39,232: t15.2024.02.25 val PER: 0.1756
2026-01-08 14:40:39,233: t15.2024.03.08 val PER: 0.2845
2026-01-08 14:40:39,235: t15.2024.03.15 val PER: 0.2564
2026-01-08 14:40:39,236: t15.2024.03.17 val PER: 0.2183
2026-01-08 14:40:39,237: t15.2024.05.10 val PER: 0.2318
2026-01-08 14:40:39,239: t15.2024.06.14 val PER: 0.2177
2026-01-08 14:40:39,240: t15.2024.07.19 val PER: 0.3223
2026-01-08 14:40:39,241: t15.2024.07.21 val PER: 0.1586
2026-01-08 14:40:39,242: t15.2024.07.28 val PER: 0.2103
2026-01-08 14:40:39,243: t15.2025.01.10 val PER: 0.3815
2026-01-08 14:40:39,245: t15.2025.01.12 val PER: 0.2156
2026-01-08 14:40:39,246: t15.2025.03.14 val PER: 0.3743
2026-01-08 14:40:39,248: t15.2025.03.16 val PER: 0.2579
2026-01-08 14:40:39,249: t15.2025.03.30 val PER: 0.3460
2026-01-08 14:40:39,251: t15.2025.04.13 val PER: 0.2839
2026-01-08 14:40:39,252: New best val WER(5gram) 53.78% --> 44.33%
2026-01-08 14:40:39,255: Checkpointing model
2026-01-08 14:40:39,400: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-08 14:40:48,358: Train batch 6600: loss: 11.97 grad norm: 47.22 time: 0.046
2026-01-08 14:41:06,755: Train batch 6800: loss: 15.62 grad norm: 57.78 time: 0.049
2026-01-08 14:41:25,369: Train batch 7000: loss: 17.64 grad norm: 60.41 time: 0.061
2026-01-08 14:41:25,370: Running test after training batch: 7000
2026-01-08 14:41:25,530: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:41:30,356: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the co due to this boy will
2026-01-08 14:41:30,394: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the cost
2026-01-08 14:41:39,728: Val batch 7000: PER (avg): 0.2022 CTC Loss (avg): 19.5025 WER(5gram): 49.35% (n=256) time: 14.354
2026-01-08 14:41:39,735: WER lens: avg_true_words=5.99 avg_pred_words=5.82 max_pred_words=11
2026-01-08 14:41:39,741: t15.2023.08.13 val PER: 0.1601
2026-01-08 14:41:39,744: t15.2023.08.18 val PER: 0.1551
2026-01-08 14:41:39,746: t15.2023.08.20 val PER: 0.1549
2026-01-08 14:41:39,750: t15.2023.08.25 val PER: 0.1280
2026-01-08 14:41:39,753: t15.2023.08.27 val PER: 0.2412
2026-01-08 14:41:39,758: t15.2023.09.01 val PER: 0.1153
2026-01-08 14:41:39,761: t15.2023.09.03 val PER: 0.2055
2026-01-08 14:41:39,763: t15.2023.09.24 val PER: 0.1541
2026-01-08 14:41:39,765: t15.2023.09.29 val PER: 0.1704
2026-01-08 14:41:39,769: t15.2023.10.01 val PER: 0.2153
2026-01-08 14:41:39,773: t15.2023.10.06 val PER: 0.1302
2026-01-08 14:41:39,775: t15.2023.10.08 val PER: 0.2774
2026-01-08 14:41:39,778: t15.2023.10.13 val PER: 0.2521
2026-01-08 14:41:39,781: t15.2023.10.15 val PER: 0.1879
2026-01-08 14:41:39,785: t15.2023.10.20 val PER: 0.2349
2026-01-08 14:41:39,787: t15.2023.10.22 val PER: 0.1503
2026-01-08 14:41:39,790: t15.2023.11.03 val PER: 0.2096
2026-01-08 14:41:39,793: t15.2023.11.04 val PER: 0.0546
2026-01-08 14:41:39,795: t15.2023.11.17 val PER: 0.0715
2026-01-08 14:41:39,797: t15.2023.11.19 val PER: 0.0659
2026-01-08 14:41:39,799: t15.2023.11.26 val PER: 0.1957
2026-01-08 14:41:39,801: t15.2023.12.03 val PER: 0.1670
2026-01-08 14:41:39,806: t15.2023.12.08 val PER: 0.1738
2026-01-08 14:41:39,812: t15.2023.12.10 val PER: 0.1380
2026-01-08 14:41:39,814: t15.2023.12.17 val PER: 0.1881
2026-01-08 14:41:39,817: t15.2023.12.29 val PER: 0.1956
2026-01-08 14:41:39,818: t15.2024.02.25 val PER: 0.1756
2026-01-08 14:41:39,820: t15.2024.03.08 val PER: 0.2703
2026-01-08 14:41:39,821: t15.2024.03.15 val PER: 0.2389
2026-01-08 14:41:39,823: t15.2024.03.17 val PER: 0.2120
2026-01-08 14:41:39,826: t15.2024.05.10 val PER: 0.2155
2026-01-08 14:41:39,828: t15.2024.06.14 val PER: 0.2161
2026-01-08 14:41:39,831: t15.2024.07.19 val PER: 0.3256
2026-01-08 14:41:39,834: t15.2024.07.21 val PER: 0.1455
2026-01-08 14:41:39,836: t15.2024.07.28 val PER: 0.1934
2026-01-08 14:41:39,838: t15.2025.01.10 val PER: 0.3512
2026-01-08 14:41:39,843: t15.2025.01.12 val PER: 0.2279
2026-01-08 14:41:39,845: t15.2025.03.14 val PER: 0.3683
2026-01-08 14:41:39,846: t15.2025.03.16 val PER: 0.2487
2026-01-08 14:41:39,848: t15.2025.03.30 val PER: 0.3632
2026-01-08 14:41:39,850: t15.2025.04.13 val PER: 0.2867
2026-01-08 14:41:57,563: Train batch 7200: loss: 14.40 grad norm: 55.04 time: 0.079
2026-01-08 14:42:14,769: Train batch 7400: loss: 15.01 grad norm: 65.27 time: 0.076
2026-01-08 14:42:23,234: Running test after training batch: 7500
2026-01-08 14:42:23,341: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:42:28,268: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code to this point will
2026-01-08 14:42:28,312: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the cost
2026-01-08 14:42:38,695: Val batch 7500: PER (avg): 0.1950 CTC Loss (avg): 19.1220 WER(5gram): 41.26% (n=256) time: 15.459
2026-01-08 14:42:38,697: WER lens: avg_true_words=5.99 avg_pred_words=6.00 max_pred_words=11
2026-01-08 14:42:38,699: t15.2023.08.13 val PER: 0.1642
2026-01-08 14:42:38,700: t15.2023.08.18 val PER: 0.1593
2026-01-08 14:42:38,702: t15.2023.08.20 val PER: 0.1517
2026-01-08 14:42:38,703: t15.2023.08.25 val PER: 0.1265
2026-01-08 14:42:38,704: t15.2023.08.27 val PER: 0.2058
2026-01-08 14:42:38,707: t15.2023.09.01 val PER: 0.1299
2026-01-08 14:42:38,708: t15.2023.09.03 val PER: 0.1971
2026-01-08 14:42:38,710: t15.2023.09.24 val PER: 0.1444
2026-01-08 14:42:38,711: t15.2023.09.29 val PER: 0.1634
2026-01-08 14:42:38,713: t15.2023.10.01 val PER: 0.1942
2026-01-08 14:42:38,714: t15.2023.10.06 val PER: 0.1259
2026-01-08 14:42:38,715: t15.2023.10.08 val PER: 0.2679
2026-01-08 14:42:38,717: t15.2023.10.13 val PER: 0.2405
2026-01-08 14:42:38,718: t15.2023.10.15 val PER: 0.1813
2026-01-08 14:42:38,720: t15.2023.10.20 val PER: 0.2315
2026-01-08 14:42:38,721: t15.2023.10.22 val PER: 0.1481
2026-01-08 14:42:38,722: t15.2023.11.03 val PER: 0.2103
2026-01-08 14:42:38,724: t15.2023.11.04 val PER: 0.0444
2026-01-08 14:42:38,725: t15.2023.11.17 val PER: 0.0622
2026-01-08 14:42:38,726: t15.2023.11.19 val PER: 0.0559
2026-01-08 14:42:38,728: t15.2023.11.26 val PER: 0.1964
2026-01-08 14:42:38,729: t15.2023.12.03 val PER: 0.1786
2026-01-08 14:42:38,731: t15.2023.12.08 val PER: 0.1578
2026-01-08 14:42:38,732: t15.2023.12.10 val PER: 0.1472
2026-01-08 14:42:38,734: t15.2023.12.17 val PER: 0.1830
2026-01-08 14:42:38,736: t15.2023.12.29 val PER: 0.1942
2026-01-08 14:42:38,737: t15.2024.02.25 val PER: 0.1531
2026-01-08 14:42:38,739: t15.2024.03.08 val PER: 0.2703
2026-01-08 14:42:38,740: t15.2024.03.15 val PER: 0.2408
2026-01-08 14:42:38,742: t15.2024.03.17 val PER: 0.1813
2026-01-08 14:42:38,744: t15.2024.05.10 val PER: 0.2110
2026-01-08 14:42:38,745: t15.2024.06.14 val PER: 0.2050
2026-01-08 14:42:38,746: t15.2024.07.19 val PER: 0.3006
2026-01-08 14:42:38,748: t15.2024.07.21 val PER: 0.1407
2026-01-08 14:42:38,749: t15.2024.07.28 val PER: 0.1860
2026-01-08 14:42:38,750: t15.2025.01.10 val PER: 0.3540
2026-01-08 14:42:38,752: t15.2025.01.12 val PER: 0.2117
2026-01-08 14:42:38,753: t15.2025.03.14 val PER: 0.3743
2026-01-08 14:42:38,754: t15.2025.03.16 val PER: 0.2487
2026-01-08 14:42:38,755: t15.2025.03.30 val PER: 0.3425
2026-01-08 14:42:38,757: t15.2025.04.13 val PER: 0.2596
2026-01-08 14:42:38,758: New best val WER(5gram) 44.33% --> 41.26%
2026-01-08 14:42:38,760: Checkpointing model
2026-01-08 14:42:38,907: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-08 14:42:47,423: Train batch 7600: loss: 16.81 grad norm: 63.46 time: 0.070
2026-01-08 14:43:04,585: Train batch 7800: loss: 14.70 grad norm: 61.91 time: 0.055
2026-01-08 14:43:22,118: Train batch 8000: loss: 11.55 grad norm: 56.28 time: 0.072
2026-01-08 14:43:22,120: Running test after training batch: 8000
2026-01-08 14:43:22,218: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:43:27,069: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point is well
2026-01-08 14:43:27,109: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the cost
2026-01-08 14:43:36,218: Val batch 8000: PER (avg): 0.1910 CTC Loss (avg): 18.3498 WER(5gram): 42.18% (n=256) time: 14.096
2026-01-08 14:43:36,220: WER lens: avg_true_words=5.99 avg_pred_words=5.89 max_pred_words=13
2026-01-08 14:43:36,222: t15.2023.08.13 val PER: 0.1663
2026-01-08 14:43:36,223: t15.2023.08.18 val PER: 0.1492
2026-01-08 14:43:36,225: t15.2023.08.20 val PER: 0.1406
2026-01-08 14:43:36,227: t15.2023.08.25 val PER: 0.1265
2026-01-08 14:43:36,228: t15.2023.08.27 val PER: 0.2170
2026-01-08 14:43:36,230: t15.2023.09.01 val PER: 0.1128
2026-01-08 14:43:36,231: t15.2023.09.03 val PER: 0.1888
2026-01-08 14:43:36,233: t15.2023.09.24 val PER: 0.1529
2026-01-08 14:43:36,234: t15.2023.09.29 val PER: 0.1538
2026-01-08 14:43:36,236: t15.2023.10.01 val PER: 0.1968
2026-01-08 14:43:36,238: t15.2023.10.06 val PER: 0.1087
2026-01-08 14:43:36,239: t15.2023.10.08 val PER: 0.2760
2026-01-08 14:43:36,241: t15.2023.10.13 val PER: 0.2413
2026-01-08 14:43:36,242: t15.2023.10.15 val PER: 0.1793
2026-01-08 14:43:36,244: t15.2023.10.20 val PER: 0.2383
2026-01-08 14:43:36,246: t15.2023.10.22 val PER: 0.1514
2026-01-08 14:43:36,247: t15.2023.11.03 val PER: 0.2090
2026-01-08 14:43:36,249: t15.2023.11.04 val PER: 0.0375
2026-01-08 14:43:36,250: t15.2023.11.17 val PER: 0.0607
2026-01-08 14:43:36,252: t15.2023.11.19 val PER: 0.0659
2026-01-08 14:43:36,253: t15.2023.11.26 val PER: 0.1848
2026-01-08 14:43:36,255: t15.2023.12.03 val PER: 0.1649
2026-01-08 14:43:36,256: t15.2023.12.08 val PER: 0.1651
2026-01-08 14:43:36,258: t15.2023.12.10 val PER: 0.1445
2026-01-08 14:43:36,259: t15.2023.12.17 val PER: 0.1840
2026-01-08 14:43:36,261: t15.2023.12.29 val PER: 0.1887
2026-01-08 14:43:36,262: t15.2024.02.25 val PER: 0.1348
2026-01-08 14:43:36,264: t15.2024.03.08 val PER: 0.2632
2026-01-08 14:43:36,265: t15.2024.03.15 val PER: 0.2376
2026-01-08 14:43:36,267: t15.2024.03.17 val PER: 0.1897
2026-01-08 14:43:36,268: t15.2024.05.10 val PER: 0.2036
2026-01-08 14:43:36,270: t15.2024.06.14 val PER: 0.2161
2026-01-08 14:43:36,271: t15.2024.07.19 val PER: 0.2914
2026-01-08 14:43:36,273: t15.2024.07.21 val PER: 0.1269
2026-01-08 14:43:36,274: t15.2024.07.28 val PER: 0.1750
2026-01-08 14:43:36,276: t15.2025.01.10 val PER: 0.3499
2026-01-08 14:43:36,277: t15.2025.01.12 val PER: 0.2025
2026-01-08 14:43:36,279: t15.2025.03.14 val PER: 0.3609
2026-01-08 14:43:36,280: t15.2025.03.16 val PER: 0.2421
2026-01-08 14:43:36,282: t15.2025.03.30 val PER: 0.3356
2026-01-08 14:43:36,283: t15.2025.04.13 val PER: 0.2753
2026-01-08 14:43:53,166: Train batch 8200: loss: 9.51 grad norm: 46.37 time: 0.056
2026-01-08 14:44:10,260: Train batch 8400: loss: 10.03 grad norm: 51.66 time: 0.065
2026-01-08 14:44:18,732: Running test after training batch: 8500
2026-01-08 14:44:18,837: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:44:23,801: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point will
2026-01-08 14:44:23,841: WER debug example
  GT : how does it keep the cost down
  PR : how it keep the cost
2026-01-08 14:44:33,209: Val batch 8500: PER (avg): 0.1858 CTC Loss (avg): 17.9769 WER(5gram): 42.24% (n=256) time: 14.475
2026-01-08 14:44:33,211: WER lens: avg_true_words=5.99 avg_pred_words=5.98 max_pred_words=11
2026-01-08 14:44:33,213: t15.2023.08.13 val PER: 0.1538
2026-01-08 14:44:33,215: t15.2023.08.18 val PER: 0.1509
2026-01-08 14:44:33,217: t15.2023.08.20 val PER: 0.1342
2026-01-08 14:44:33,218: t15.2023.08.25 val PER: 0.1280
2026-01-08 14:44:33,220: t15.2023.08.27 val PER: 0.2170
2026-01-08 14:44:33,222: t15.2023.09.01 val PER: 0.0990
2026-01-08 14:44:33,223: t15.2023.09.03 val PER: 0.1960
2026-01-08 14:44:33,225: t15.2023.09.24 val PER: 0.1444
2026-01-08 14:44:33,226: t15.2023.09.29 val PER: 0.1557
2026-01-08 14:44:33,227: t15.2023.10.01 val PER: 0.1909
2026-01-08 14:44:33,229: t15.2023.10.06 val PER: 0.1119
2026-01-08 14:44:33,230: t15.2023.10.08 val PER: 0.2666
2026-01-08 14:44:33,232: t15.2023.10.13 val PER: 0.2467
2026-01-08 14:44:33,233: t15.2023.10.15 val PER: 0.1859
2026-01-08 14:44:33,235: t15.2023.10.20 val PER: 0.2181
2026-01-08 14:44:33,237: t15.2023.10.22 val PER: 0.1448
2026-01-08 14:44:33,238: t15.2023.11.03 val PER: 0.2008
2026-01-08 14:44:33,240: t15.2023.11.04 val PER: 0.0444
2026-01-08 14:44:33,241: t15.2023.11.17 val PER: 0.0607
2026-01-08 14:44:33,242: t15.2023.11.19 val PER: 0.0459
2026-01-08 14:44:33,244: t15.2023.11.26 val PER: 0.1841
2026-01-08 14:44:33,245: t15.2023.12.03 val PER: 0.1492
2026-01-08 14:44:33,247: t15.2023.12.08 val PER: 0.1551
2026-01-08 14:44:33,248: t15.2023.12.10 val PER: 0.1275
2026-01-08 14:44:33,250: t15.2023.12.17 val PER: 0.1694
2026-01-08 14:44:33,251: t15.2023.12.29 val PER: 0.1798
2026-01-08 14:44:33,253: t15.2024.02.25 val PER: 0.1404
2026-01-08 14:44:33,254: t15.2024.03.08 val PER: 0.2560
2026-01-08 14:44:33,256: t15.2024.03.15 val PER: 0.2270
2026-01-08 14:44:33,258: t15.2024.03.17 val PER: 0.1778
2026-01-08 14:44:33,260: t15.2024.05.10 val PER: 0.2184
2026-01-08 14:44:33,262: t15.2024.06.14 val PER: 0.2050
2026-01-08 14:44:33,263: t15.2024.07.19 val PER: 0.2881
2026-01-08 14:44:33,265: t15.2024.07.21 val PER: 0.1241
2026-01-08 14:44:33,266: t15.2024.07.28 val PER: 0.1787
2026-01-08 14:44:33,267: t15.2025.01.10 val PER: 0.3347
2026-01-08 14:44:33,269: t15.2025.01.12 val PER: 0.1963
2026-01-08 14:44:33,270: t15.2025.03.14 val PER: 0.3743
2026-01-08 14:44:33,272: t15.2025.03.16 val PER: 0.2225
2026-01-08 14:44:33,273: t15.2025.03.30 val PER: 0.3287
2026-01-08 14:44:33,274: t15.2025.04.13 val PER: 0.2511
2026-01-08 14:44:41,928: Train batch 8600: loss: 15.63 grad norm: 57.14 time: 0.054
2026-01-08 14:44:59,397: Train batch 8800: loss: 15.92 grad norm: 61.60 time: 0.061
2026-01-08 14:45:16,847: Train batch 9000: loss: 16.37 grad norm: 68.97 time: 0.075
2026-01-08 14:45:16,849: Running test after training batch: 9000
2026-01-08 14:45:16,969: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:45:22,204: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 14:45:22,245: WER debug example
  GT : how does it keep the cost down
  PR : how i see it keep the cost
2026-01-08 14:45:31,522: Val batch 9000: PER (avg): 0.1808 CTC Loss (avg): 17.6291 WER(5gram): 34.29% (n=256) time: 14.671
2026-01-08 14:45:31,524: WER lens: avg_true_words=5.99 avg_pred_words=5.95 max_pred_words=13
2026-01-08 14:45:31,525: t15.2023.08.13 val PER: 0.1466
2026-01-08 14:45:31,527: t15.2023.08.18 val PER: 0.1408
2026-01-08 14:45:31,529: t15.2023.08.20 val PER: 0.1303
2026-01-08 14:45:31,530: t15.2023.08.25 val PER: 0.1205
2026-01-08 14:45:31,532: t15.2023.08.27 val PER: 0.2154
2026-01-08 14:45:31,534: t15.2023.09.01 val PER: 0.0990
2026-01-08 14:45:31,535: t15.2023.09.03 val PER: 0.1936
2026-01-08 14:45:31,537: t15.2023.09.24 val PER: 0.1468
2026-01-08 14:45:31,538: t15.2023.09.29 val PER: 0.1455
2026-01-08 14:45:31,540: t15.2023.10.01 val PER: 0.1988
2026-01-08 14:45:31,541: t15.2023.10.06 val PER: 0.1141
2026-01-08 14:45:31,543: t15.2023.10.08 val PER: 0.2679
2026-01-08 14:45:31,544: t15.2023.10.13 val PER: 0.2273
2026-01-08 14:45:31,546: t15.2023.10.15 val PER: 0.1773
2026-01-08 14:45:31,547: t15.2023.10.20 val PER: 0.2114
2026-01-08 14:45:31,549: t15.2023.10.22 val PER: 0.1425
2026-01-08 14:45:31,551: t15.2023.11.03 val PER: 0.2076
2026-01-08 14:45:31,552: t15.2023.11.04 val PER: 0.0410
2026-01-08 14:45:31,554: t15.2023.11.17 val PER: 0.0560
2026-01-08 14:45:31,556: t15.2023.11.19 val PER: 0.0559
2026-01-08 14:45:31,557: t15.2023.11.26 val PER: 0.1761
2026-01-08 14:45:31,558: t15.2023.12.03 val PER: 0.1597
2026-01-08 14:45:31,560: t15.2023.12.08 val PER: 0.1365
2026-01-08 14:45:31,561: t15.2023.12.10 val PER: 0.1235
2026-01-08 14:45:31,563: t15.2023.12.17 val PER: 0.1715
2026-01-08 14:45:31,565: t15.2023.12.29 val PER: 0.1723
2026-01-08 14:45:31,566: t15.2024.02.25 val PER: 0.1461
2026-01-08 14:45:31,568: t15.2024.03.08 val PER: 0.2589
2026-01-08 14:45:31,569: t15.2024.03.15 val PER: 0.2351
2026-01-08 14:45:31,572: t15.2024.03.17 val PER: 0.1695
2026-01-08 14:45:31,574: t15.2024.05.10 val PER: 0.1842
2026-01-08 14:45:31,576: t15.2024.06.14 val PER: 0.1861
2026-01-08 14:45:31,577: t15.2024.07.19 val PER: 0.2815
2026-01-08 14:45:31,579: t15.2024.07.21 val PER: 0.1200
2026-01-08 14:45:31,580: t15.2024.07.28 val PER: 0.1669
2026-01-08 14:45:31,581: t15.2025.01.10 val PER: 0.2948
2026-01-08 14:45:31,583: t15.2025.01.12 val PER: 0.1886
2026-01-08 14:45:31,585: t15.2025.03.14 val PER: 0.3550
2026-01-08 14:45:31,586: t15.2025.03.16 val PER: 0.2408
2026-01-08 14:45:31,588: t15.2025.03.30 val PER: 0.3161
2026-01-08 14:45:31,589: t15.2025.04.13 val PER: 0.2553
2026-01-08 14:45:31,591: New best val WER(5gram) 41.26% --> 34.29%
2026-01-08 14:45:31,594: Checkpointing model
2026-01-08 14:45:31,736: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-08 14:45:49,213: Train batch 9200: loss: 10.60 grad norm: 51.30 time: 0.056
2026-01-08 14:46:06,459: Train batch 9400: loss: 7.53 grad norm: 45.18 time: 0.069
2026-01-08 14:46:15,338: Running test after training batch: 9500
2026-01-08 14:46:15,480: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:46:20,444: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point as well
2026-01-08 14:46:20,482: WER debug example
  GT : how does it keep the cost down
  PR : how i see it keep the cost
2026-01-08 14:46:29,817: Val batch 9500: PER (avg): 0.1747 CTC Loss (avg): 17.3555 WER(5gram): 31.03% (n=256) time: 14.477
2026-01-08 14:46:29,820: WER lens: avg_true_words=5.99 avg_pred_words=6.19 max_pred_words=13
2026-01-08 14:46:29,822: t15.2023.08.13 val PER: 0.1476
2026-01-08 14:46:29,825: t15.2023.08.18 val PER: 0.1375
2026-01-08 14:46:29,827: t15.2023.08.20 val PER: 0.1271
2026-01-08 14:46:29,829: t15.2023.08.25 val PER: 0.1190
2026-01-08 14:46:29,831: t15.2023.08.27 val PER: 0.2058
2026-01-08 14:46:29,835: t15.2023.09.01 val PER: 0.0901
2026-01-08 14:46:29,837: t15.2023.09.03 val PER: 0.1770
2026-01-08 14:46:29,839: t15.2023.09.24 val PER: 0.1396
2026-01-08 14:46:29,840: t15.2023.09.29 val PER: 0.1378
2026-01-08 14:46:29,842: t15.2023.10.01 val PER: 0.1902
2026-01-08 14:46:29,843: t15.2023.10.06 val PER: 0.1066
2026-01-08 14:46:29,846: t15.2023.10.08 val PER: 0.2693
2026-01-08 14:46:29,850: t15.2023.10.13 val PER: 0.2250
2026-01-08 14:46:29,853: t15.2023.10.15 val PER: 0.1701
2026-01-08 14:46:29,855: t15.2023.10.20 val PER: 0.2248
2026-01-08 14:46:29,857: t15.2023.10.22 val PER: 0.1448
2026-01-08 14:46:29,859: t15.2023.11.03 val PER: 0.1913
2026-01-08 14:46:29,861: t15.2023.11.04 val PER: 0.0307
2026-01-08 14:46:29,863: t15.2023.11.17 val PER: 0.0560
2026-01-08 14:46:29,868: t15.2023.11.19 val PER: 0.0639
2026-01-08 14:46:29,869: t15.2023.11.26 val PER: 0.1703
2026-01-08 14:46:29,877: t15.2023.12.03 val PER: 0.1439
2026-01-08 14:46:29,879: t15.2023.12.08 val PER: 0.1358
2026-01-08 14:46:29,881: t15.2023.12.10 val PER: 0.1183
2026-01-08 14:46:29,884: t15.2023.12.17 val PER: 0.1601
2026-01-08 14:46:29,886: t15.2023.12.29 val PER: 0.1654
2026-01-08 14:46:29,891: t15.2024.02.25 val PER: 0.1320
2026-01-08 14:46:29,896: t15.2024.03.08 val PER: 0.2376
2026-01-08 14:46:29,900: t15.2024.03.15 val PER: 0.2258
2026-01-08 14:46:29,902: t15.2024.03.17 val PER: 0.1618
2026-01-08 14:46:29,904: t15.2024.05.10 val PER: 0.1887
2026-01-08 14:46:29,907: t15.2024.06.14 val PER: 0.1767
2026-01-08 14:46:29,911: t15.2024.07.19 val PER: 0.2683
2026-01-08 14:46:29,914: t15.2024.07.21 val PER: 0.1179
2026-01-08 14:46:29,915: t15.2024.07.28 val PER: 0.1581
2026-01-08 14:46:29,917: t15.2025.01.10 val PER: 0.3264
2026-01-08 14:46:29,919: t15.2025.01.12 val PER: 0.1824
2026-01-08 14:46:29,920: t15.2025.03.14 val PER: 0.3565
2026-01-08 14:46:29,921: t15.2025.03.16 val PER: 0.2160
2026-01-08 14:46:29,923: t15.2025.03.30 val PER: 0.3126
2026-01-08 14:46:29,926: t15.2025.04.13 val PER: 0.2539
2026-01-08 14:46:29,928: New best val WER(5gram) 34.29% --> 31.03%
2026-01-08 14:46:29,929: Checkpointing model
2026-01-08 14:46:30,088: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-08 14:46:38,525: Train batch 9600: loss: 8.61 grad norm: 50.90 time: 0.074
2026-01-08 14:46:55,785: Train batch 9800: loss: 13.94 grad norm: 67.00 time: 0.064
2026-01-08 14:47:13,630: Train batch 10000: loss: 5.57 grad norm: 38.82 time: 0.062
2026-01-08 14:47:13,633: Running test after training batch: 10000
2026-01-08 14:47:13,746: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:47:18,532: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the cold at this point will
2026-01-08 14:47:18,570: WER debug example
  GT : how does it keep the cost down
  PR : how i see it keep the cost
2026-01-08 14:47:27,828: Val batch 10000: PER (avg): 0.1689 CTC Loss (avg): 16.9294 WER(5gram): 43.02% (n=256) time: 14.192
2026-01-08 14:47:27,831: WER lens: avg_true_words=5.99 avg_pred_words=6.25 max_pred_words=14
2026-01-08 14:47:27,833: t15.2023.08.13 val PER: 0.1351
2026-01-08 14:47:27,836: t15.2023.08.18 val PER: 0.1308
2026-01-08 14:47:27,838: t15.2023.08.20 val PER: 0.1231
2026-01-08 14:47:27,839: t15.2023.08.25 val PER: 0.1145
2026-01-08 14:47:27,841: t15.2023.08.27 val PER: 0.2010
2026-01-08 14:47:27,846: t15.2023.09.01 val PER: 0.0885
2026-01-08 14:47:27,848: t15.2023.09.03 val PER: 0.1698
2026-01-08 14:47:27,850: t15.2023.09.24 val PER: 0.1371
2026-01-08 14:47:27,851: t15.2023.09.29 val PER: 0.1366
2026-01-08 14:47:27,853: t15.2023.10.01 val PER: 0.1882
2026-01-08 14:47:27,854: t15.2023.10.06 val PER: 0.1066
2026-01-08 14:47:27,856: t15.2023.10.08 val PER: 0.2368
2026-01-08 14:47:27,857: t15.2023.10.13 val PER: 0.2126
2026-01-08 14:47:27,859: t15.2023.10.15 val PER: 0.1608
2026-01-08 14:47:27,860: t15.2023.10.20 val PER: 0.2047
2026-01-08 14:47:27,862: t15.2023.10.22 val PER: 0.1448
2026-01-08 14:47:27,864: t15.2023.11.03 val PER: 0.1839
2026-01-08 14:47:27,865: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:47:27,867: t15.2023.11.17 val PER: 0.0513
2026-01-08 14:47:27,869: t15.2023.11.19 val PER: 0.0459
2026-01-08 14:47:27,870: t15.2023.11.26 val PER: 0.1522
2026-01-08 14:47:27,872: t15.2023.12.03 val PER: 0.1397
2026-01-08 14:47:27,874: t15.2023.12.08 val PER: 0.1352
2026-01-08 14:47:27,879: t15.2023.12.10 val PER: 0.1196
2026-01-08 14:47:27,880: t15.2023.12.17 val PER: 0.1497
2026-01-08 14:47:27,882: t15.2023.12.29 val PER: 0.1606
2026-01-08 14:47:27,884: t15.2024.02.25 val PER: 0.1362
2026-01-08 14:47:27,889: t15.2024.03.08 val PER: 0.2418
2026-01-08 14:47:27,890: t15.2024.03.15 val PER: 0.2170
2026-01-08 14:47:27,892: t15.2024.03.17 val PER: 0.1618
2026-01-08 14:47:27,893: t15.2024.05.10 val PER: 0.1813
2026-01-08 14:47:27,894: t15.2024.06.14 val PER: 0.1719
2026-01-08 14:47:27,896: t15.2024.07.19 val PER: 0.2564
2026-01-08 14:47:27,901: t15.2024.07.21 val PER: 0.1214
2026-01-08 14:47:27,906: t15.2024.07.28 val PER: 0.1610
2026-01-08 14:47:27,911: t15.2025.01.10 val PER: 0.3140
2026-01-08 14:47:27,913: t15.2025.01.12 val PER: 0.1763
2026-01-08 14:47:27,915: t15.2025.03.14 val PER: 0.3476
2026-01-08 14:47:27,919: t15.2025.03.16 val PER: 0.2225
2026-01-08 14:47:27,921: t15.2025.03.30 val PER: 0.2943
2026-01-08 14:47:27,926: t15.2025.04.13 val PER: 0.2425
2026-01-08 14:47:46,024: Train batch 10200: loss: 6.66 grad norm: 42.62 time: 0.051
2026-01-08 14:48:04,522: Train batch 10400: loss: 9.44 grad norm: 52.85 time: 0.073
2026-01-08 14:48:13,817: Running test after training batch: 10500
2026-01-08 14:48:13,945: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:48:18,674: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 14:48:18,714: WER debug example
  GT : how does it keep the cost down
  PR : how i see it keep the cost
2026-01-08 14:48:27,891: Val batch 10500: PER (avg): 0.1687 CTC Loss (avg): 16.6869 WER(5gram): 37.29% (n=256) time: 14.071
2026-01-08 14:48:27,893: WER lens: avg_true_words=5.99 avg_pred_words=6.15 max_pred_words=12
2026-01-08 14:48:27,902: t15.2023.08.13 val PER: 0.1403
2026-01-08 14:48:27,926: t15.2023.08.18 val PER: 0.1366
2026-01-08 14:48:27,940: t15.2023.08.20 val PER: 0.1263
2026-01-08 14:48:27,942: t15.2023.08.25 val PER: 0.1145
2026-01-08 14:48:27,944: t15.2023.08.27 val PER: 0.1945
2026-01-08 14:48:27,945: t15.2023.09.01 val PER: 0.0982
2026-01-08 14:48:27,947: t15.2023.09.03 val PER: 0.1829
2026-01-08 14:48:27,949: t15.2023.09.24 val PER: 0.1274
2026-01-08 14:48:27,950: t15.2023.09.29 val PER: 0.1398
2026-01-08 14:48:27,952: t15.2023.10.01 val PER: 0.1836
2026-01-08 14:48:27,953: t15.2023.10.06 val PER: 0.0926
2026-01-08 14:48:27,955: t15.2023.10.08 val PER: 0.2476
2026-01-08 14:48:27,956: t15.2023.10.13 val PER: 0.2149
2026-01-08 14:48:27,958: t15.2023.10.15 val PER: 0.1674
2026-01-08 14:48:27,959: t15.2023.10.20 val PER: 0.2081
2026-01-08 14:48:27,960: t15.2023.10.22 val PER: 0.1325
2026-01-08 14:48:27,962: t15.2023.11.03 val PER: 0.1927
2026-01-08 14:48:27,964: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:48:27,965: t15.2023.11.17 val PER: 0.0498
2026-01-08 14:48:27,967: t15.2023.11.19 val PER: 0.0519
2026-01-08 14:48:27,968: t15.2023.11.26 val PER: 0.1464
2026-01-08 14:48:27,970: t15.2023.12.03 val PER: 0.1303
2026-01-08 14:48:27,971: t15.2023.12.08 val PER: 0.1238
2026-01-08 14:48:27,972: t15.2023.12.10 val PER: 0.1170
2026-01-08 14:48:27,974: t15.2023.12.17 val PER: 0.1538
2026-01-08 14:48:27,975: t15.2023.12.29 val PER: 0.1531
2026-01-08 14:48:27,977: t15.2024.02.25 val PER: 0.1278
2026-01-08 14:48:27,979: t15.2024.03.08 val PER: 0.2575
2026-01-08 14:48:27,981: t15.2024.03.15 val PER: 0.2233
2026-01-08 14:48:27,982: t15.2024.03.17 val PER: 0.1576
2026-01-08 14:48:27,984: t15.2024.05.10 val PER: 0.1887
2026-01-08 14:48:27,985: t15.2024.06.14 val PER: 0.1767
2026-01-08 14:48:27,987: t15.2024.07.19 val PER: 0.2544
2026-01-08 14:48:27,988: t15.2024.07.21 val PER: 0.1145
2026-01-08 14:48:27,989: t15.2024.07.28 val PER: 0.1581
2026-01-08 14:48:27,991: t15.2025.01.10 val PER: 0.3044
2026-01-08 14:48:27,992: t15.2025.01.12 val PER: 0.1717
2026-01-08 14:48:27,994: t15.2025.03.14 val PER: 0.3550
2026-01-08 14:48:27,995: t15.2025.03.16 val PER: 0.2120
2026-01-08 14:48:27,996: t15.2025.03.30 val PER: 0.3103
2026-01-08 14:48:27,998: t15.2025.04.13 val PER: 0.2397
2026-01-08 14:48:37,464: Train batch 10600: loss: 9.36 grad norm: 65.22 time: 0.075
2026-01-08 14:48:55,791: Train batch 10800: loss: 15.37 grad norm: 64.33 time: 0.068
2026-01-08 14:49:14,441: Train batch 11000: loss: 14.43 grad norm: 62.90 time: 0.058
2026-01-08 14:49:14,443: Running test after training batch: 11000
2026-01-08 14:49:14,585: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:49:19,349: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 14:49:19,394: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 14:49:28,942: Val batch 11000: PER (avg): 0.1652 CTC Loss (avg): 16.5808 WER(5gram): 39.50% (n=256) time: 14.496
2026-01-08 14:49:28,944: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=13
2026-01-08 14:49:28,947: t15.2023.08.13 val PER: 0.1289
2026-01-08 14:49:28,948: t15.2023.08.18 val PER: 0.1241
2026-01-08 14:49:28,950: t15.2023.08.20 val PER: 0.1183
2026-01-08 14:49:28,951: t15.2023.08.25 val PER: 0.1099
2026-01-08 14:49:28,953: t15.2023.08.27 val PER: 0.1929
2026-01-08 14:49:28,954: t15.2023.09.01 val PER: 0.0958
2026-01-08 14:49:28,956: t15.2023.09.03 val PER: 0.1805
2026-01-08 14:49:28,958: t15.2023.09.24 val PER: 0.1335
2026-01-08 14:49:28,959: t15.2023.09.29 val PER: 0.1398
2026-01-08 14:49:28,961: t15.2023.10.01 val PER: 0.1777
2026-01-08 14:49:28,962: t15.2023.10.06 val PER: 0.0926
2026-01-08 14:49:28,964: t15.2023.10.08 val PER: 0.2544
2026-01-08 14:49:28,965: t15.2023.10.13 val PER: 0.2071
2026-01-08 14:49:28,968: t15.2023.10.15 val PER: 0.1694
2026-01-08 14:49:28,969: t15.2023.10.20 val PER: 0.1980
2026-01-08 14:49:28,971: t15.2023.10.22 val PER: 0.1258
2026-01-08 14:49:28,972: t15.2023.11.03 val PER: 0.1893
2026-01-08 14:49:28,974: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:49:28,975: t15.2023.11.17 val PER: 0.0467
2026-01-08 14:49:28,977: t15.2023.11.19 val PER: 0.0539
2026-01-08 14:49:28,978: t15.2023.11.26 val PER: 0.1355
2026-01-08 14:49:28,980: t15.2023.12.03 val PER: 0.1355
2026-01-08 14:49:28,981: t15.2023.12.08 val PER: 0.1198
2026-01-08 14:49:28,983: t15.2023.12.10 val PER: 0.1170
2026-01-08 14:49:28,984: t15.2023.12.17 val PER: 0.1549
2026-01-08 14:49:28,986: t15.2023.12.29 val PER: 0.1489
2026-01-08 14:49:28,987: t15.2024.02.25 val PER: 0.1292
2026-01-08 14:49:28,989: t15.2024.03.08 val PER: 0.2304
2026-01-08 14:49:28,991: t15.2024.03.15 val PER: 0.2245
2026-01-08 14:49:28,992: t15.2024.03.17 val PER: 0.1548
2026-01-08 14:49:28,994: t15.2024.05.10 val PER: 0.1961
2026-01-08 14:49:28,995: t15.2024.06.14 val PER: 0.1735
2026-01-08 14:49:28,997: t15.2024.07.19 val PER: 0.2544
2026-01-08 14:49:28,998: t15.2024.07.21 val PER: 0.1007
2026-01-08 14:49:29,001: t15.2024.07.28 val PER: 0.1522
2026-01-08 14:49:29,003: t15.2025.01.10 val PER: 0.3058
2026-01-08 14:49:29,005: t15.2025.01.12 val PER: 0.1647
2026-01-08 14:49:29,006: t15.2025.03.14 val PER: 0.3462
2026-01-08 14:49:29,008: t15.2025.03.16 val PER: 0.2160
2026-01-08 14:49:29,010: t15.2025.03.30 val PER: 0.3138
2026-01-08 14:49:29,011: t15.2025.04.13 val PER: 0.2382
2026-01-08 14:49:46,901: Train batch 11200: loss: 11.48 grad norm: 54.48 time: 0.072
2026-01-08 14:50:04,541: Train batch 11400: loss: 10.85 grad norm: 58.30 time: 0.058
2026-01-08 14:50:13,647: Running test after training batch: 11500
2026-01-08 14:50:13,801: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:50:18,539: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 14:50:18,585: WER debug example
  GT : how does it keep the cost down
  PR : how i see it keep the cost
2026-01-08 14:50:27,863: Val batch 11500: PER (avg): 0.1625 CTC Loss (avg): 16.2755 WER(5gram): 37.55% (n=256) time: 14.214
2026-01-08 14:50:27,865: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=13
2026-01-08 14:50:27,876: t15.2023.08.13 val PER: 0.1299
2026-01-08 14:50:27,877: t15.2023.08.18 val PER: 0.1232
2026-01-08 14:50:27,879: t15.2023.08.20 val PER: 0.1207
2026-01-08 14:50:27,880: t15.2023.08.25 val PER: 0.1114
2026-01-08 14:50:27,882: t15.2023.08.27 val PER: 0.1945
2026-01-08 14:50:27,884: t15.2023.09.01 val PER: 0.0828
2026-01-08 14:50:27,886: t15.2023.09.03 val PER: 0.1710
2026-01-08 14:50:27,887: t15.2023.09.24 val PER: 0.1274
2026-01-08 14:50:27,888: t15.2023.09.29 val PER: 0.1391
2026-01-08 14:50:27,890: t15.2023.10.01 val PER: 0.1757
2026-01-08 14:50:27,891: t15.2023.10.06 val PER: 0.0861
2026-01-08 14:50:27,893: t15.2023.10.08 val PER: 0.2395
2026-01-08 14:50:27,894: t15.2023.10.13 val PER: 0.2102
2026-01-08 14:50:27,895: t15.2023.10.15 val PER: 0.1602
2026-01-08 14:50:27,897: t15.2023.10.20 val PER: 0.1879
2026-01-08 14:50:27,898: t15.2023.10.22 val PER: 0.1370
2026-01-08 14:50:27,900: t15.2023.11.03 val PER: 0.1750
2026-01-08 14:50:27,901: t15.2023.11.04 val PER: 0.0444
2026-01-08 14:50:27,902: t15.2023.11.17 val PER: 0.0544
2026-01-08 14:50:27,904: t15.2023.11.19 val PER: 0.0319
2026-01-08 14:50:27,905: t15.2023.11.26 val PER: 0.1457
2026-01-08 14:50:27,907: t15.2023.12.03 val PER: 0.1197
2026-01-08 14:50:27,908: t15.2023.12.08 val PER: 0.1165
2026-01-08 14:50:27,909: t15.2023.12.10 val PER: 0.1130
2026-01-08 14:50:27,911: t15.2023.12.17 val PER: 0.1528
2026-01-08 14:50:27,912: t15.2023.12.29 val PER: 0.1448
2026-01-08 14:50:27,913: t15.2024.02.25 val PER: 0.1390
2026-01-08 14:50:27,915: t15.2024.03.08 val PER: 0.2347
2026-01-08 14:50:27,917: t15.2024.03.15 val PER: 0.2176
2026-01-08 14:50:27,918: t15.2024.03.17 val PER: 0.1499
2026-01-08 14:50:27,920: t15.2024.05.10 val PER: 0.1783
2026-01-08 14:50:27,921: t15.2024.06.14 val PER: 0.1609
2026-01-08 14:50:27,922: t15.2024.07.19 val PER: 0.2538
2026-01-08 14:50:27,924: t15.2024.07.21 val PER: 0.1048
2026-01-08 14:50:27,925: t15.2024.07.28 val PER: 0.1544
2026-01-08 14:50:27,926: t15.2025.01.10 val PER: 0.3196
2026-01-08 14:50:27,928: t15.2025.01.12 val PER: 0.1724
2026-01-08 14:50:27,929: t15.2025.03.14 val PER: 0.3402
2026-01-08 14:50:27,931: t15.2025.03.16 val PER: 0.2055
2026-01-08 14:50:27,932: t15.2025.03.30 val PER: 0.3011
2026-01-08 14:50:27,933: t15.2025.04.13 val PER: 0.2382
2026-01-08 14:50:36,539: Train batch 11600: loss: 11.54 grad norm: 50.31 time: 0.062
2026-01-08 14:50:53,683: Train batch 11800: loss: 6.92 grad norm: 44.14 time: 0.050
2026-01-08 14:51:11,375: Train batch 12000: loss: 13.99 grad norm: 62.83 time: 0.072
2026-01-08 14:51:11,377: Running test after training batch: 12000
2026-01-08 14:51:11,483: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:51:16,238: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 14:51:16,286: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost
2026-01-08 14:51:25,690: Val batch 12000: PER (avg): 0.1609 CTC Loss (avg): 16.2370 WER(5gram): 38.07% (n=256) time: 14.307
2026-01-08 14:51:25,692: WER lens: avg_true_words=5.99 avg_pred_words=6.38 max_pred_words=11
2026-01-08 14:51:25,696: t15.2023.08.13 val PER: 0.1331
2026-01-08 14:51:25,698: t15.2023.08.18 val PER: 0.1182
2026-01-08 14:51:25,700: t15.2023.08.20 val PER: 0.1160
2026-01-08 14:51:25,702: t15.2023.08.25 val PER: 0.1114
2026-01-08 14:51:25,703: t15.2023.08.27 val PER: 0.1865
2026-01-08 14:51:25,705: t15.2023.09.01 val PER: 0.0885
2026-01-08 14:51:25,707: t15.2023.09.03 val PER: 0.1651
2026-01-08 14:51:25,709: t15.2023.09.24 val PER: 0.1214
2026-01-08 14:51:25,710: t15.2023.09.29 val PER: 0.1321
2026-01-08 14:51:25,711: t15.2023.10.01 val PER: 0.1797
2026-01-08 14:51:25,713: t15.2023.10.06 val PER: 0.0980
2026-01-08 14:51:25,714: t15.2023.10.08 val PER: 0.2530
2026-01-08 14:51:25,715: t15.2023.10.13 val PER: 0.2087
2026-01-08 14:51:25,717: t15.2023.10.15 val PER: 0.1655
2026-01-08 14:51:25,718: t15.2023.10.20 val PER: 0.1879
2026-01-08 14:51:25,720: t15.2023.10.22 val PER: 0.1314
2026-01-08 14:51:25,721: t15.2023.11.03 val PER: 0.1825
2026-01-08 14:51:25,722: t15.2023.11.04 val PER: 0.0375
2026-01-08 14:51:25,723: t15.2023.11.17 val PER: 0.0389
2026-01-08 14:51:25,725: t15.2023.11.19 val PER: 0.0419
2026-01-08 14:51:25,727: t15.2023.11.26 val PER: 0.1391
2026-01-08 14:51:25,728: t15.2023.12.03 val PER: 0.1239
2026-01-08 14:51:25,729: t15.2023.12.08 val PER: 0.1132
2026-01-08 14:51:25,731: t15.2023.12.10 val PER: 0.1156
2026-01-08 14:51:25,732: t15.2023.12.17 val PER: 0.1403
2026-01-08 14:51:25,733: t15.2023.12.29 val PER: 0.1469
2026-01-08 14:51:25,736: t15.2024.02.25 val PER: 0.1208
2026-01-08 14:51:25,737: t15.2024.03.08 val PER: 0.2376
2026-01-08 14:51:25,738: t15.2024.03.15 val PER: 0.2220
2026-01-08 14:51:25,740: t15.2024.03.17 val PER: 0.1534
2026-01-08 14:51:25,741: t15.2024.05.10 val PER: 0.1724
2026-01-08 14:51:25,743: t15.2024.06.14 val PER: 0.1656
2026-01-08 14:51:25,744: t15.2024.07.19 val PER: 0.2492
2026-01-08 14:51:25,745: t15.2024.07.21 val PER: 0.1034
2026-01-08 14:51:25,747: t15.2024.07.28 val PER: 0.1559
2026-01-08 14:51:25,748: t15.2025.01.10 val PER: 0.3058
2026-01-08 14:51:25,749: t15.2025.01.12 val PER: 0.1524
2026-01-08 14:51:25,751: t15.2025.03.14 val PER: 0.3417
2026-01-08 14:51:25,752: t15.2025.03.16 val PER: 0.2068
2026-01-08 14:51:25,753: t15.2025.03.30 val PER: 0.2954
2026-01-08 14:51:25,754: t15.2025.04.13 val PER: 0.2282
2026-01-08 14:51:43,340: Train batch 12200: loss: 5.84 grad norm: 38.65 time: 0.066
2026-01-08 14:52:00,707: Train batch 12400: loss: 5.20 grad norm: 37.49 time: 0.041
2026-01-08 14:52:09,576: Running test after training batch: 12500
2026-01-08 14:52:09,714: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:52:14,471: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 14:52:14,516: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 14:52:23,959: Val batch 12500: PER (avg): 0.1594 CTC Loss (avg): 16.0679 WER(5gram): 36.18% (n=256) time: 14.380
2026-01-08 14:52:23,961: WER lens: avg_true_words=5.99 avg_pred_words=6.31 max_pred_words=12
2026-01-08 14:52:23,963: t15.2023.08.13 val PER: 0.1195
2026-01-08 14:52:23,964: t15.2023.08.18 val PER: 0.1123
2026-01-08 14:52:23,966: t15.2023.08.20 val PER: 0.1144
2026-01-08 14:52:23,967: t15.2023.08.25 val PER: 0.1054
2026-01-08 14:52:23,970: t15.2023.08.27 val PER: 0.1945
2026-01-08 14:52:23,972: t15.2023.09.01 val PER: 0.0804
2026-01-08 14:52:23,973: t15.2023.09.03 val PER: 0.1651
2026-01-08 14:52:23,974: t15.2023.09.24 val PER: 0.1250
2026-01-08 14:52:23,976: t15.2023.09.29 val PER: 0.1289
2026-01-08 14:52:23,978: t15.2023.10.01 val PER: 0.1770
2026-01-08 14:52:23,980: t15.2023.10.06 val PER: 0.0915
2026-01-08 14:52:23,981: t15.2023.10.08 val PER: 0.2503
2026-01-08 14:52:23,983: t15.2023.10.13 val PER: 0.2071
2026-01-08 14:52:23,984: t15.2023.10.15 val PER: 0.1615
2026-01-08 14:52:23,986: t15.2023.10.20 val PER: 0.1846
2026-01-08 14:52:23,988: t15.2023.10.22 val PER: 0.1258
2026-01-08 14:52:23,989: t15.2023.11.03 val PER: 0.1811
2026-01-08 14:52:23,991: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:52:23,992: t15.2023.11.17 val PER: 0.0435
2026-01-08 14:52:23,994: t15.2023.11.19 val PER: 0.0339
2026-01-08 14:52:23,995: t15.2023.11.26 val PER: 0.1370
2026-01-08 14:52:23,997: t15.2023.12.03 val PER: 0.1313
2026-01-08 14:52:23,998: t15.2023.12.08 val PER: 0.1138
2026-01-08 14:52:24,001: t15.2023.12.10 val PER: 0.1038
2026-01-08 14:52:24,002: t15.2023.12.17 val PER: 0.1476
2026-01-08 14:52:24,004: t15.2023.12.29 val PER: 0.1421
2026-01-08 14:52:24,005: t15.2024.02.25 val PER: 0.1292
2026-01-08 14:52:24,007: t15.2024.03.08 val PER: 0.2233
2026-01-08 14:52:24,009: t15.2024.03.15 val PER: 0.2208
2026-01-08 14:52:24,010: t15.2024.03.17 val PER: 0.1520
2026-01-08 14:52:24,012: t15.2024.05.10 val PER: 0.1768
2026-01-08 14:52:24,013: t15.2024.06.14 val PER: 0.1688
2026-01-08 14:52:24,015: t15.2024.07.19 val PER: 0.2577
2026-01-08 14:52:24,016: t15.2024.07.21 val PER: 0.0986
2026-01-08 14:52:24,018: t15.2024.07.28 val PER: 0.1478
2026-01-08 14:52:24,019: t15.2025.01.10 val PER: 0.2961
2026-01-08 14:52:24,021: t15.2025.01.12 val PER: 0.1555
2026-01-08 14:52:24,022: t15.2025.03.14 val PER: 0.3462
2026-01-08 14:52:24,024: t15.2025.03.16 val PER: 0.2068
2026-01-08 14:52:24,026: t15.2025.03.30 val PER: 0.3080
2026-01-08 14:52:24,027: t15.2025.04.13 val PER: 0.2282
2026-01-08 14:52:32,502: Train batch 12600: loss: 8.21 grad norm: 44.73 time: 0.058
2026-01-08 14:52:50,424: Train batch 12800: loss: 6.24 grad norm: 44.14 time: 0.053
2026-01-08 14:53:08,330: Train batch 13000: loss: 6.56 grad norm: 42.29 time: 0.067
2026-01-08 14:53:08,332: Running test after training batch: 13000
2026-01-08 14:53:08,427: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:53:13,229: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 14:53:13,274: WER debug example
  GT : how does it keep the cost down
  PR : how dense it keep the cost
2026-01-08 14:53:22,702: Val batch 13000: PER (avg): 0.1565 CTC Loss (avg): 15.6638 WER(5gram): 31.81% (n=256) time: 14.368
2026-01-08 14:53:22,705: WER lens: avg_true_words=5.99 avg_pred_words=6.14 max_pred_words=13
2026-01-08 14:53:22,707: t15.2023.08.13 val PER: 0.1216
2026-01-08 14:53:22,708: t15.2023.08.18 val PER: 0.1065
2026-01-08 14:53:22,710: t15.2023.08.20 val PER: 0.1112
2026-01-08 14:53:22,712: t15.2023.08.25 val PER: 0.1054
2026-01-08 14:53:22,715: t15.2023.08.27 val PER: 0.1945
2026-01-08 14:53:22,717: t15.2023.09.01 val PER: 0.0771
2026-01-08 14:53:22,719: t15.2023.09.03 val PER: 0.1686
2026-01-08 14:53:22,721: t15.2023.09.24 val PER: 0.1238
2026-01-08 14:53:22,724: t15.2023.09.29 val PER: 0.1270
2026-01-08 14:53:22,728: t15.2023.10.01 val PER: 0.1684
2026-01-08 14:53:22,730: t15.2023.10.06 val PER: 0.0893
2026-01-08 14:53:22,733: t15.2023.10.08 val PER: 0.2544
2026-01-08 14:53:22,735: t15.2023.10.13 val PER: 0.2102
2026-01-08 14:53:22,737: t15.2023.10.15 val PER: 0.1668
2026-01-08 14:53:22,738: t15.2023.10.20 val PER: 0.1980
2026-01-08 14:53:22,740: t15.2023.10.22 val PER: 0.1347
2026-01-08 14:53:22,744: t15.2023.11.03 val PER: 0.1750
2026-01-08 14:53:22,746: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:53:22,749: t15.2023.11.17 val PER: 0.0404
2026-01-08 14:53:22,751: t15.2023.11.19 val PER: 0.0399
2026-01-08 14:53:22,756: t15.2023.11.26 val PER: 0.1355
2026-01-08 14:53:22,760: t15.2023.12.03 val PER: 0.1313
2026-01-08 14:53:22,762: t15.2023.12.08 val PER: 0.1125
2026-01-08 14:53:22,763: t15.2023.12.10 val PER: 0.1025
2026-01-08 14:53:22,767: t15.2023.12.17 val PER: 0.1414
2026-01-08 14:53:22,770: t15.2023.12.29 val PER: 0.1441
2026-01-08 14:53:22,771: t15.2024.02.25 val PER: 0.1166
2026-01-08 14:53:22,773: t15.2024.03.08 val PER: 0.2162
2026-01-08 14:53:22,775: t15.2024.03.15 val PER: 0.2151
2026-01-08 14:53:22,777: t15.2024.03.17 val PER: 0.1416
2026-01-08 14:53:22,779: t15.2024.05.10 val PER: 0.1798
2026-01-08 14:53:22,781: t15.2024.06.14 val PER: 0.1593
2026-01-08 14:53:22,782: t15.2024.07.19 val PER: 0.2432
2026-01-08 14:53:22,784: t15.2024.07.21 val PER: 0.0945
2026-01-08 14:53:22,786: t15.2024.07.28 val PER: 0.1426
2026-01-08 14:53:22,791: t15.2025.01.10 val PER: 0.2824
2026-01-08 14:53:22,797: t15.2025.01.12 val PER: 0.1601
2026-01-08 14:53:22,801: t15.2025.03.14 val PER: 0.3462
2026-01-08 14:53:22,803: t15.2025.03.16 val PER: 0.1950
2026-01-08 14:53:22,804: t15.2025.03.30 val PER: 0.2908
2026-01-08 14:53:22,808: t15.2025.04.13 val PER: 0.2311
2026-01-08 14:53:40,044: Train batch 13200: loss: 12.81 grad norm: 63.83 time: 0.057
2026-01-08 14:53:57,304: Train batch 13400: loss: 9.52 grad norm: 54.27 time: 0.063
2026-01-08 14:54:06,219: Running test after training batch: 13500
2026-01-08 14:54:06,398: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:54:11,270: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 14:54:11,315: WER debug example
  GT : how does it keep the cost down
  PR : how dense it keep the cost
2026-01-08 14:54:20,923: Val batch 13500: PER (avg): 0.1522 CTC Loss (avg): 15.5545 WER(5gram): 34.35% (n=256) time: 14.702
2026-01-08 14:54:20,926: WER lens: avg_true_words=5.99 avg_pred_words=6.35 max_pred_words=12
2026-01-08 14:54:20,931: t15.2023.08.13 val PER: 0.1175
2026-01-08 14:54:20,933: t15.2023.08.18 val PER: 0.1065
2026-01-08 14:54:20,934: t15.2023.08.20 val PER: 0.1080
2026-01-08 14:54:20,936: t15.2023.08.25 val PER: 0.1190
2026-01-08 14:54:20,937: t15.2023.08.27 val PER: 0.1865
2026-01-08 14:54:20,939: t15.2023.09.01 val PER: 0.0795
2026-01-08 14:54:20,940: t15.2023.09.03 val PER: 0.1532
2026-01-08 14:54:20,941: t15.2023.09.24 val PER: 0.1214
2026-01-08 14:54:20,943: t15.2023.09.29 val PER: 0.1321
2026-01-08 14:54:20,945: t15.2023.10.01 val PER: 0.1664
2026-01-08 14:54:20,946: t15.2023.10.06 val PER: 0.0861
2026-01-08 14:54:20,948: t15.2023.10.08 val PER: 0.2368
2026-01-08 14:54:20,949: t15.2023.10.13 val PER: 0.1955
2026-01-08 14:54:20,951: t15.2023.10.15 val PER: 0.1589
2026-01-08 14:54:20,952: t15.2023.10.20 val PER: 0.1711
2026-01-08 14:54:20,954: t15.2023.10.22 val PER: 0.1247
2026-01-08 14:54:20,955: t15.2023.11.03 val PER: 0.1757
2026-01-08 14:54:20,956: t15.2023.11.04 val PER: 0.0273
2026-01-08 14:54:20,958: t15.2023.11.17 val PER: 0.0373
2026-01-08 14:54:20,959: t15.2023.11.19 val PER: 0.0339
2026-01-08 14:54:20,961: t15.2023.11.26 val PER: 0.1297
2026-01-08 14:54:20,962: t15.2023.12.03 val PER: 0.1187
2026-01-08 14:54:20,964: t15.2023.12.08 val PER: 0.1085
2026-01-08 14:54:20,965: t15.2023.12.10 val PER: 0.0959
2026-01-08 14:54:20,967: t15.2023.12.17 val PER: 0.1227
2026-01-08 14:54:20,968: t15.2023.12.29 val PER: 0.1393
2026-01-08 14:54:20,970: t15.2024.02.25 val PER: 0.1124
2026-01-08 14:54:20,971: t15.2024.03.08 val PER: 0.2134
2026-01-08 14:54:20,973: t15.2024.03.15 val PER: 0.2064
2026-01-08 14:54:20,974: t15.2024.03.17 val PER: 0.1457
2026-01-08 14:54:20,975: t15.2024.05.10 val PER: 0.1694
2026-01-08 14:54:20,977: t15.2024.06.14 val PER: 0.1609
2026-01-08 14:54:20,978: t15.2024.07.19 val PER: 0.2406
2026-01-08 14:54:20,979: t15.2024.07.21 val PER: 0.0945
2026-01-08 14:54:20,981: t15.2024.07.28 val PER: 0.1397
2026-01-08 14:54:20,982: t15.2025.01.10 val PER: 0.2893
2026-01-08 14:54:20,984: t15.2025.01.12 val PER: 0.1570
2026-01-08 14:54:20,985: t15.2025.03.14 val PER: 0.3388
2026-01-08 14:54:20,986: t15.2025.03.16 val PER: 0.1924
2026-01-08 14:54:20,988: t15.2025.03.30 val PER: 0.2920
2026-01-08 14:54:20,990: t15.2025.04.13 val PER: 0.2197
2026-01-08 14:54:30,224: Train batch 13600: loss: 12.88 grad norm: 67.87 time: 0.065
2026-01-08 14:54:48,881: Train batch 13800: loss: 8.69 grad norm: 51.79 time: 0.057
2026-01-08 14:55:07,442: Train batch 14000: loss: 11.85 grad norm: 64.98 time: 0.051
2026-01-08 14:55:07,444: Running test after training batch: 14000
2026-01-08 14:55:07,558: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:55:12,316: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 14:55:12,363: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 14:55:21,779: Val batch 14000: PER (avg): 0.1531 CTC Loss (avg): 15.4175 WER(5gram): 38.46% (n=256) time: 14.332
2026-01-08 14:55:21,781: WER lens: avg_true_words=5.99 avg_pred_words=6.34 max_pred_words=13
2026-01-08 14:55:21,783: t15.2023.08.13 val PER: 0.1164
2026-01-08 14:55:21,784: t15.2023.08.18 val PER: 0.1048
2026-01-08 14:55:21,786: t15.2023.08.20 val PER: 0.1104
2026-01-08 14:55:21,787: t15.2023.08.25 val PER: 0.1099
2026-01-08 14:55:21,789: t15.2023.08.27 val PER: 0.1833
2026-01-08 14:55:21,791: t15.2023.09.01 val PER: 0.0763
2026-01-08 14:55:21,792: t15.2023.09.03 val PER: 0.1663
2026-01-08 14:55:21,794: t15.2023.09.24 val PER: 0.1226
2026-01-08 14:55:21,796: t15.2023.09.29 val PER: 0.1270
2026-01-08 14:55:21,797: t15.2023.10.01 val PER: 0.1704
2026-01-08 14:55:21,799: t15.2023.10.06 val PER: 0.0936
2026-01-08 14:55:21,800: t15.2023.10.08 val PER: 0.2341
2026-01-08 14:55:21,802: t15.2023.10.13 val PER: 0.1971
2026-01-08 14:55:21,803: t15.2023.10.15 val PER: 0.1602
2026-01-08 14:55:21,805: t15.2023.10.20 val PER: 0.1913
2026-01-08 14:55:21,806: t15.2023.10.22 val PER: 0.1180
2026-01-08 14:55:21,807: t15.2023.11.03 val PER: 0.1791
2026-01-08 14:55:21,809: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:55:21,810: t15.2023.11.17 val PER: 0.0467
2026-01-08 14:55:21,813: t15.2023.11.19 val PER: 0.0379
2026-01-08 14:55:21,814: t15.2023.11.26 val PER: 0.1333
2026-01-08 14:55:21,816: t15.2023.12.03 val PER: 0.1124
2026-01-08 14:55:21,817: t15.2023.12.08 val PER: 0.1045
2026-01-08 14:55:21,818: t15.2023.12.10 val PER: 0.1051
2026-01-08 14:55:21,820: t15.2023.12.17 val PER: 0.1268
2026-01-08 14:55:21,821: t15.2023.12.29 val PER: 0.1359
2026-01-08 14:55:21,823: t15.2024.02.25 val PER: 0.1250
2026-01-08 14:55:21,825: t15.2024.03.08 val PER: 0.2233
2026-01-08 14:55:21,826: t15.2024.03.15 val PER: 0.2089
2026-01-08 14:55:21,828: t15.2024.03.17 val PER: 0.1395
2026-01-08 14:55:21,829: t15.2024.05.10 val PER: 0.1724
2026-01-08 14:55:21,831: t15.2024.06.14 val PER: 0.1593
2026-01-08 14:55:21,832: t15.2024.07.19 val PER: 0.2399
2026-01-08 14:55:21,833: t15.2024.07.21 val PER: 0.0897
2026-01-08 14:55:21,835: t15.2024.07.28 val PER: 0.1463
2026-01-08 14:55:21,836: t15.2025.01.10 val PER: 0.2879
2026-01-08 14:55:21,837: t15.2025.01.12 val PER: 0.1570
2026-01-08 14:55:21,839: t15.2025.03.14 val PER: 0.3299
2026-01-08 14:55:21,840: t15.2025.03.16 val PER: 0.2003
2026-01-08 14:55:21,842: t15.2025.03.30 val PER: 0.2874
2026-01-08 14:55:21,844: t15.2025.04.13 val PER: 0.2325
2026-01-08 14:55:40,128: Train batch 14200: loss: 7.94 grad norm: 56.04 time: 0.057
2026-01-08 14:55:57,558: Train batch 14400: loss: 6.26 grad norm: 38.68 time: 0.064
2026-01-08 14:56:06,466: Running test after training batch: 14500
2026-01-08 14:56:06,714: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:56:11,488: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 14:56:11,531: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 14:56:21,037: Val batch 14500: PER (avg): 0.1527 CTC Loss (avg): 15.4248 WER(5gram): 35.66% (n=256) time: 14.568
2026-01-08 14:56:21,039: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=12
2026-01-08 14:56:21,041: t15.2023.08.13 val PER: 0.1299
2026-01-08 14:56:21,042: t15.2023.08.18 val PER: 0.1006
2026-01-08 14:56:21,044: t15.2023.08.20 val PER: 0.1017
2026-01-08 14:56:21,045: t15.2023.08.25 val PER: 0.0979
2026-01-08 14:56:21,047: t15.2023.08.27 val PER: 0.1785
2026-01-08 14:56:21,049: t15.2023.09.01 val PER: 0.0747
2026-01-08 14:56:21,050: t15.2023.09.03 val PER: 0.1591
2026-01-08 14:56:21,052: t15.2023.09.24 val PER: 0.1226
2026-01-08 14:56:21,053: t15.2023.09.29 val PER: 0.1225
2026-01-08 14:56:21,055: t15.2023.10.01 val PER: 0.1711
2026-01-08 14:56:21,056: t15.2023.10.06 val PER: 0.0893
2026-01-08 14:56:21,057: t15.2023.10.08 val PER: 0.2449
2026-01-08 14:56:21,059: t15.2023.10.13 val PER: 0.1994
2026-01-08 14:56:21,060: t15.2023.10.15 val PER: 0.1628
2026-01-08 14:56:21,061: t15.2023.10.20 val PER: 0.1846
2026-01-08 14:56:21,063: t15.2023.10.22 val PER: 0.1203
2026-01-08 14:56:21,064: t15.2023.11.03 val PER: 0.1859
2026-01-08 14:56:21,065: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:56:21,067: t15.2023.11.17 val PER: 0.0435
2026-01-08 14:56:21,068: t15.2023.11.19 val PER: 0.0399
2026-01-08 14:56:21,070: t15.2023.11.26 val PER: 0.1174
2026-01-08 14:56:21,071: t15.2023.12.03 val PER: 0.1134
2026-01-08 14:56:21,072: t15.2023.12.08 val PER: 0.1092
2026-01-08 14:56:21,074: t15.2023.12.10 val PER: 0.1025
2026-01-08 14:56:21,075: t15.2023.12.17 val PER: 0.1331
2026-01-08 14:56:21,077: t15.2023.12.29 val PER: 0.1359
2026-01-08 14:56:21,078: t15.2024.02.25 val PER: 0.1236
2026-01-08 14:56:21,079: t15.2024.03.08 val PER: 0.2191
2026-01-08 14:56:21,081: t15.2024.03.15 val PER: 0.2108
2026-01-08 14:56:21,082: t15.2024.03.17 val PER: 0.1395
2026-01-08 14:56:21,083: t15.2024.05.10 val PER: 0.1738
2026-01-08 14:56:21,085: t15.2024.06.14 val PER: 0.1593
2026-01-08 14:56:21,086: t15.2024.07.19 val PER: 0.2386
2026-01-08 14:56:21,087: t15.2024.07.21 val PER: 0.0910
2026-01-08 14:56:21,089: t15.2024.07.28 val PER: 0.1434
2026-01-08 14:56:21,090: t15.2025.01.10 val PER: 0.2879
2026-01-08 14:56:21,091: t15.2025.01.12 val PER: 0.1578
2026-01-08 14:56:21,093: t15.2025.03.14 val PER: 0.3402
2026-01-08 14:56:21,094: t15.2025.03.16 val PER: 0.1911
2026-01-08 14:56:21,096: t15.2025.03.30 val PER: 0.3046
2026-01-08 14:56:21,097: t15.2025.04.13 val PER: 0.2140
2026-01-08 14:56:30,058: Train batch 14600: loss: 12.71 grad norm: 59.29 time: 0.058
2026-01-08 14:56:48,181: Train batch 14800: loss: 5.91 grad norm: 43.00 time: 0.052
2026-01-08 14:57:06,576: Train batch 15000: loss: 8.68 grad norm: 49.02 time: 0.053
2026-01-08 14:57:06,578: Running test after training batch: 15000
2026-01-08 14:57:06,686: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:57:11,446: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 14:57:11,489: WER debug example
  GT : how does it keep the cost down
  PR : how i see it keep the cost
2026-01-08 14:57:21,045: Val batch 15000: PER (avg): 0.1497 CTC Loss (avg): 15.1967 WER(5gram): 34.29% (n=256) time: 14.464
2026-01-08 14:57:21,047: WER lens: avg_true_words=5.99 avg_pred_words=6.36 max_pred_words=12
2026-01-08 14:57:21,049: t15.2023.08.13 val PER: 0.1175
2026-01-08 14:57:21,050: t15.2023.08.18 val PER: 0.0939
2026-01-08 14:57:21,051: t15.2023.08.20 val PER: 0.1064
2026-01-08 14:57:21,053: t15.2023.08.25 val PER: 0.1054
2026-01-08 14:57:21,054: t15.2023.08.27 val PER: 0.1768
2026-01-08 14:57:21,056: t15.2023.09.01 val PER: 0.0779
2026-01-08 14:57:21,059: t15.2023.09.03 val PER: 0.1532
2026-01-08 14:57:21,060: t15.2023.09.24 val PER: 0.1201
2026-01-08 14:57:21,062: t15.2023.09.29 val PER: 0.1251
2026-01-08 14:57:21,064: t15.2023.10.01 val PER: 0.1671
2026-01-08 14:57:21,065: t15.2023.10.06 val PER: 0.0829
2026-01-08 14:57:21,066: t15.2023.10.08 val PER: 0.2395
2026-01-08 14:57:21,068: t15.2023.10.13 val PER: 0.2017
2026-01-08 14:57:21,069: t15.2023.10.15 val PER: 0.1536
2026-01-08 14:57:21,070: t15.2023.10.20 val PER: 0.1812
2026-01-08 14:57:21,072: t15.2023.10.22 val PER: 0.1169
2026-01-08 14:57:21,074: t15.2023.11.03 val PER: 0.1798
2026-01-08 14:57:21,075: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:57:21,077: t15.2023.11.17 val PER: 0.0451
2026-01-08 14:57:21,078: t15.2023.11.19 val PER: 0.0339
2026-01-08 14:57:21,079: t15.2023.11.26 val PER: 0.1181
2026-01-08 14:57:21,081: t15.2023.12.03 val PER: 0.1071
2026-01-08 14:57:21,082: t15.2023.12.08 val PER: 0.1092
2026-01-08 14:57:21,083: t15.2023.12.10 val PER: 0.0972
2026-01-08 14:57:21,085: t15.2023.12.17 val PER: 0.1227
2026-01-08 14:57:21,086: t15.2023.12.29 val PER: 0.1290
2026-01-08 14:57:21,087: t15.2024.02.25 val PER: 0.1180
2026-01-08 14:57:21,088: t15.2024.03.08 val PER: 0.2233
2026-01-08 14:57:21,090: t15.2024.03.15 val PER: 0.2089
2026-01-08 14:57:21,091: t15.2024.03.17 val PER: 0.1360
2026-01-08 14:57:21,092: t15.2024.05.10 val PER: 0.1724
2026-01-08 14:57:21,093: t15.2024.06.14 val PER: 0.1514
2026-01-08 14:57:21,095: t15.2024.07.19 val PER: 0.2320
2026-01-08 14:57:21,096: t15.2024.07.21 val PER: 0.0924
2026-01-08 14:57:21,097: t15.2024.07.28 val PER: 0.1390
2026-01-08 14:57:21,099: t15.2025.01.10 val PER: 0.2810
2026-01-08 14:57:21,100: t15.2025.01.12 val PER: 0.1555
2026-01-08 14:57:21,101: t15.2025.03.14 val PER: 0.3476
2026-01-08 14:57:21,102: t15.2025.03.16 val PER: 0.1911
2026-01-08 14:57:21,104: t15.2025.03.30 val PER: 0.2943
2026-01-08 14:57:21,105: t15.2025.04.13 val PER: 0.2140
2026-01-08 14:57:38,659: Train batch 15200: loss: 4.88 grad norm: 41.50 time: 0.058
2026-01-08 14:57:55,612: Train batch 15400: loss: 11.00 grad norm: 53.83 time: 0.050
2026-01-08 14:58:04,291: Running test after training batch: 15500
2026-01-08 14:58:04,399: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:58:09,526: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 14:58:09,576: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 14:58:19,123: Val batch 15500: PER (avg): 0.1491 CTC Loss (avg): 15.1207 WER(5gram): 36.77% (n=256) time: 14.829
2026-01-08 14:58:19,125: WER lens: avg_true_words=5.99 avg_pred_words=6.41 max_pred_words=13
2026-01-08 14:58:19,127: t15.2023.08.13 val PER: 0.1143
2026-01-08 14:58:19,129: t15.2023.08.18 val PER: 0.1106
2026-01-08 14:58:19,130: t15.2023.08.20 val PER: 0.1056
2026-01-08 14:58:19,132: t15.2023.08.25 val PER: 0.0964
2026-01-08 14:58:19,133: t15.2023.08.27 val PER: 0.1736
2026-01-08 14:58:19,135: t15.2023.09.01 val PER: 0.0739
2026-01-08 14:58:19,136: t15.2023.09.03 val PER: 0.1591
2026-01-08 14:58:19,138: t15.2023.09.24 val PER: 0.1250
2026-01-08 14:58:19,140: t15.2023.09.29 val PER: 0.1257
2026-01-08 14:58:19,141: t15.2023.10.01 val PER: 0.1697
2026-01-08 14:58:19,143: t15.2023.10.06 val PER: 0.0829
2026-01-08 14:58:19,145: t15.2023.10.08 val PER: 0.2395
2026-01-08 14:58:19,147: t15.2023.10.13 val PER: 0.1978
2026-01-08 14:58:19,149: t15.2023.10.15 val PER: 0.1556
2026-01-08 14:58:19,150: t15.2023.10.20 val PER: 0.2013
2026-01-08 14:58:19,152: t15.2023.10.22 val PER: 0.1225
2026-01-08 14:58:19,153: t15.2023.11.03 val PER: 0.1757
2026-01-08 14:58:19,155: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:58:19,157: t15.2023.11.17 val PER: 0.0373
2026-01-08 14:58:19,159: t15.2023.11.19 val PER: 0.0339
2026-01-08 14:58:19,160: t15.2023.11.26 val PER: 0.1167
2026-01-08 14:58:19,162: t15.2023.12.03 val PER: 0.1040
2026-01-08 14:58:19,163: t15.2023.12.08 val PER: 0.1025
2026-01-08 14:58:19,164: t15.2023.12.10 val PER: 0.1064
2026-01-08 14:58:19,166: t15.2023.12.17 val PER: 0.1299
2026-01-08 14:58:19,169: t15.2023.12.29 val PER: 0.1229
2026-01-08 14:58:19,171: t15.2024.02.25 val PER: 0.1110
2026-01-08 14:58:19,172: t15.2024.03.08 val PER: 0.2262
2026-01-08 14:58:19,173: t15.2024.03.15 val PER: 0.1982
2026-01-08 14:58:19,175: t15.2024.03.17 val PER: 0.1353
2026-01-08 14:58:19,176: t15.2024.05.10 val PER: 0.1694
2026-01-08 14:58:19,177: t15.2024.06.14 val PER: 0.1656
2026-01-08 14:58:19,179: t15.2024.07.19 val PER: 0.2307
2026-01-08 14:58:19,180: t15.2024.07.21 val PER: 0.0938
2026-01-08 14:58:19,182: t15.2024.07.28 val PER: 0.1404
2026-01-08 14:58:19,183: t15.2025.01.10 val PER: 0.2796
2026-01-08 14:58:19,184: t15.2025.01.12 val PER: 0.1486
2026-01-08 14:58:19,185: t15.2025.03.14 val PER: 0.3388
2026-01-08 14:58:19,187: t15.2025.03.16 val PER: 0.1911
2026-01-08 14:58:19,188: t15.2025.03.30 val PER: 0.2885
2026-01-08 14:58:19,190: t15.2025.04.13 val PER: 0.2183
2026-01-08 14:58:28,356: Train batch 15600: loss: 11.39 grad norm: 58.31 time: 0.065
2026-01-08 14:58:46,584: Train batch 15800: loss: 14.27 grad norm: 69.50 time: 0.068
2026-01-08 14:59:04,308: Train batch 16000: loss: 7.50 grad norm: 45.54 time: 0.055
2026-01-08 14:59:04,310: Running test after training batch: 16000
2026-01-08 14:59:04,432: WER debug GT example: You can see the code at this point as well.
2026-01-08 14:59:09,610: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 14:59:09,655: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 14:59:19,231: Val batch 16000: PER (avg): 0.1489 CTC Loss (avg): 15.1682 WER(5gram): 35.66% (n=256) time: 14.919
2026-01-08 14:59:19,235: WER lens: avg_true_words=5.99 avg_pred_words=6.46 max_pred_words=14
2026-01-08 14:59:19,239: t15.2023.08.13 val PER: 0.1175
2026-01-08 14:59:19,245: t15.2023.08.18 val PER: 0.1048
2026-01-08 14:59:19,248: t15.2023.08.20 val PER: 0.1041
2026-01-08 14:59:19,251: t15.2023.08.25 val PER: 0.1009
2026-01-08 14:59:19,253: t15.2023.08.27 val PER: 0.1881
2026-01-08 14:59:19,257: t15.2023.09.01 val PER: 0.0722
2026-01-08 14:59:19,260: t15.2023.09.03 val PER: 0.1544
2026-01-08 14:59:19,262: t15.2023.09.24 val PER: 0.1214
2026-01-08 14:59:19,267: t15.2023.09.29 val PER: 0.1270
2026-01-08 14:59:19,271: t15.2023.10.01 val PER: 0.1645
2026-01-08 14:59:19,273: t15.2023.10.06 val PER: 0.0840
2026-01-08 14:59:19,275: t15.2023.10.08 val PER: 0.2355
2026-01-08 14:59:19,279: t15.2023.10.13 val PER: 0.1908
2026-01-08 14:59:19,283: t15.2023.10.15 val PER: 0.1602
2026-01-08 14:59:19,285: t15.2023.10.20 val PER: 0.1879
2026-01-08 14:59:19,287: t15.2023.10.22 val PER: 0.1136
2026-01-08 14:59:19,290: t15.2023.11.03 val PER: 0.1682
2026-01-08 14:59:19,294: t15.2023.11.04 val PER: 0.0341
2026-01-08 14:59:19,296: t15.2023.11.17 val PER: 0.0451
2026-01-08 14:59:19,297: t15.2023.11.19 val PER: 0.0339
2026-01-08 14:59:19,300: t15.2023.11.26 val PER: 0.1167
2026-01-08 14:59:19,302: t15.2023.12.03 val PER: 0.1071
2026-01-08 14:59:19,304: t15.2023.12.08 val PER: 0.0979
2026-01-08 14:59:19,305: t15.2023.12.10 val PER: 0.0972
2026-01-08 14:59:19,307: t15.2023.12.17 val PER: 0.1279
2026-01-08 14:59:19,308: t15.2023.12.29 val PER: 0.1325
2026-01-08 14:59:19,310: t15.2024.02.25 val PER: 0.1152
2026-01-08 14:59:19,312: t15.2024.03.08 val PER: 0.2105
2026-01-08 14:59:19,313: t15.2024.03.15 val PER: 0.2076
2026-01-08 14:59:19,314: t15.2024.03.17 val PER: 0.1381
2026-01-08 14:59:19,316: t15.2024.05.10 val PER: 0.1724
2026-01-08 14:59:19,318: t15.2024.06.14 val PER: 0.1514
2026-01-08 14:59:19,320: t15.2024.07.19 val PER: 0.2307
2026-01-08 14:59:19,323: t15.2024.07.21 val PER: 0.0966
2026-01-08 14:59:19,324: t15.2024.07.28 val PER: 0.1426
2026-01-08 14:59:19,326: t15.2025.01.10 val PER: 0.2824
2026-01-08 14:59:19,332: t15.2025.01.12 val PER: 0.1532
2026-01-08 14:59:19,333: t15.2025.03.14 val PER: 0.3388
2026-01-08 14:59:19,335: t15.2025.03.16 val PER: 0.1859
2026-01-08 14:59:19,336: t15.2025.03.30 val PER: 0.2931
2026-01-08 14:59:19,338: t15.2025.04.13 val PER: 0.2225
2026-01-08 14:59:37,068: Train batch 16200: loss: 6.70 grad norm: 46.60 time: 0.058
2026-01-08 14:59:54,423: Train batch 16400: loss: 10.60 grad norm: 62.98 time: 0.057
2026-01-08 15:00:03,347: Running test after training batch: 16500
2026-01-08 15:00:03,466: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:00:08,527: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 15:00:08,573: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 15:00:18,431: Val batch 16500: PER (avg): 0.1482 CTC Loss (avg): 15.0725 WER(5gram): 32.01% (n=256) time: 15.082
2026-01-08 15:00:18,433: WER lens: avg_true_words=5.99 avg_pred_words=6.32 max_pred_words=12
2026-01-08 15:00:18,436: t15.2023.08.13 val PER: 0.1154
2026-01-08 15:00:18,438: t15.2023.08.18 val PER: 0.1073
2026-01-08 15:00:18,440: t15.2023.08.20 val PER: 0.1064
2026-01-08 15:00:18,442: t15.2023.08.25 val PER: 0.1114
2026-01-08 15:00:18,444: t15.2023.08.27 val PER: 0.1785
2026-01-08 15:00:18,446: t15.2023.09.01 val PER: 0.0755
2026-01-08 15:00:18,447: t15.2023.09.03 val PER: 0.1532
2026-01-08 15:00:18,449: t15.2023.09.24 val PER: 0.1274
2026-01-08 15:00:18,451: t15.2023.09.29 val PER: 0.1264
2026-01-08 15:00:18,452: t15.2023.10.01 val PER: 0.1684
2026-01-08 15:00:18,454: t15.2023.10.06 val PER: 0.0893
2026-01-08 15:00:18,456: t15.2023.10.08 val PER: 0.2300
2026-01-08 15:00:18,457: t15.2023.10.13 val PER: 0.1947
2026-01-08 15:00:18,459: t15.2023.10.15 val PER: 0.1549
2026-01-08 15:00:18,460: t15.2023.10.20 val PER: 0.1846
2026-01-08 15:00:18,462: t15.2023.10.22 val PER: 0.1147
2026-01-08 15:00:18,465: t15.2023.11.03 val PER: 0.1757
2026-01-08 15:00:18,467: t15.2023.11.04 val PER: 0.0375
2026-01-08 15:00:18,468: t15.2023.11.17 val PER: 0.0404
2026-01-08 15:00:18,470: t15.2023.11.19 val PER: 0.0359
2026-01-08 15:00:18,471: t15.2023.11.26 val PER: 0.1174
2026-01-08 15:00:18,473: t15.2023.12.03 val PER: 0.1061
2026-01-08 15:00:18,475: t15.2023.12.08 val PER: 0.0959
2026-01-08 15:00:18,476: t15.2023.12.10 val PER: 0.0894
2026-01-08 15:00:18,478: t15.2023.12.17 val PER: 0.1331
2026-01-08 15:00:18,479: t15.2023.12.29 val PER: 0.1270
2026-01-08 15:00:18,481: t15.2024.02.25 val PER: 0.1138
2026-01-08 15:00:18,482: t15.2024.03.08 val PER: 0.2148
2026-01-08 15:00:18,484: t15.2024.03.15 val PER: 0.2051
2026-01-08 15:00:18,486: t15.2024.03.17 val PER: 0.1304
2026-01-08 15:00:18,487: t15.2024.05.10 val PER: 0.1709
2026-01-08 15:00:18,489: t15.2024.06.14 val PER: 0.1530
2026-01-08 15:00:18,490: t15.2024.07.19 val PER: 0.2380
2026-01-08 15:00:18,492: t15.2024.07.21 val PER: 0.0897
2026-01-08 15:00:18,493: t15.2024.07.28 val PER: 0.1316
2026-01-08 15:00:18,495: t15.2025.01.10 val PER: 0.2686
2026-01-08 15:00:18,496: t15.2025.01.12 val PER: 0.1470
2026-01-08 15:00:18,498: t15.2025.03.14 val PER: 0.3476
2026-01-08 15:00:18,500: t15.2025.03.16 val PER: 0.1832
2026-01-08 15:00:18,501: t15.2025.03.30 val PER: 0.2897
2026-01-08 15:00:18,503: t15.2025.04.13 val PER: 0.2225
2026-01-08 15:00:27,637: Train batch 16600: loss: 8.10 grad norm: 52.21 time: 0.054
2026-01-08 15:00:45,069: Train batch 16800: loss: 16.70 grad norm: 71.62 time: 0.062
2026-01-08 15:01:02,676: Train batch 17000: loss: 8.41 grad norm: 48.68 time: 0.083
2026-01-08 15:01:02,679: Running test after training batch: 17000
2026-01-08 15:01:02,783: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:01:07,691: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 15:01:07,739: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 15:01:17,542: Val batch 17000: PER (avg): 0.1476 CTC Loss (avg): 15.0525 WER(5gram): 34.42% (n=256) time: 14.861
2026-01-08 15:01:17,545: WER lens: avg_true_words=5.99 avg_pred_words=6.39 max_pred_words=12
2026-01-08 15:01:17,548: t15.2023.08.13 val PER: 0.1112
2026-01-08 15:01:17,549: t15.2023.08.18 val PER: 0.0964
2026-01-08 15:01:17,551: t15.2023.08.20 val PER: 0.1048
2026-01-08 15:01:17,552: t15.2023.08.25 val PER: 0.0949
2026-01-08 15:01:17,554: t15.2023.08.27 val PER: 0.1849
2026-01-08 15:01:17,556: t15.2023.09.01 val PER: 0.0706
2026-01-08 15:01:17,557: t15.2023.09.03 val PER: 0.1591
2026-01-08 15:01:17,559: t15.2023.09.24 val PER: 0.1250
2026-01-08 15:01:17,560: t15.2023.09.29 val PER: 0.1251
2026-01-08 15:01:17,562: t15.2023.10.01 val PER: 0.1651
2026-01-08 15:01:17,564: t15.2023.10.06 val PER: 0.0861
2026-01-08 15:01:17,565: t15.2023.10.08 val PER: 0.2409
2026-01-08 15:01:17,567: t15.2023.10.13 val PER: 0.1893
2026-01-08 15:01:17,568: t15.2023.10.15 val PER: 0.1549
2026-01-08 15:01:17,570: t15.2023.10.20 val PER: 0.1846
2026-01-08 15:01:17,571: t15.2023.10.22 val PER: 0.1180
2026-01-08 15:01:17,573: t15.2023.11.03 val PER: 0.1784
2026-01-08 15:01:17,574: t15.2023.11.04 val PER: 0.0375
2026-01-08 15:01:17,576: t15.2023.11.17 val PER: 0.0420
2026-01-08 15:01:17,577: t15.2023.11.19 val PER: 0.0319
2026-01-08 15:01:17,579: t15.2023.11.26 val PER: 0.1145
2026-01-08 15:01:17,580: t15.2023.12.03 val PER: 0.1019
2026-01-08 15:01:17,582: t15.2023.12.08 val PER: 0.0925
2026-01-08 15:01:17,583: t15.2023.12.10 val PER: 0.0946
2026-01-08 15:01:17,585: t15.2023.12.17 val PER: 0.1268
2026-01-08 15:01:17,586: t15.2023.12.29 val PER: 0.1297
2026-01-08 15:01:17,588: t15.2024.02.25 val PER: 0.1138
2026-01-08 15:01:17,589: t15.2024.03.08 val PER: 0.2119
2026-01-08 15:01:17,591: t15.2024.03.15 val PER: 0.2051
2026-01-08 15:01:17,592: t15.2024.03.17 val PER: 0.1395
2026-01-08 15:01:17,594: t15.2024.05.10 val PER: 0.1753
2026-01-08 15:01:17,596: t15.2024.06.14 val PER: 0.1483
2026-01-08 15:01:17,597: t15.2024.07.19 val PER: 0.2373
2026-01-08 15:01:17,599: t15.2024.07.21 val PER: 0.0890
2026-01-08 15:01:17,600: t15.2024.07.28 val PER: 0.1353
2026-01-08 15:01:17,601: t15.2025.01.10 val PER: 0.2727
2026-01-08 15:01:17,603: t15.2025.01.12 val PER: 0.1455
2026-01-08 15:01:17,604: t15.2025.03.14 val PER: 0.3402
2026-01-08 15:01:17,605: t15.2025.03.16 val PER: 0.1937
2026-01-08 15:01:17,606: t15.2025.03.30 val PER: 0.2908
2026-01-08 15:01:17,608: t15.2025.04.13 val PER: 0.2197
2026-01-08 15:01:34,738: Train batch 17200: loss: 10.11 grad norm: 53.59 time: 0.084
2026-01-08 15:01:52,100: Train batch 17400: loss: 11.69 grad norm: 57.80 time: 0.071
2026-01-08 15:02:00,509: Running test after training batch: 17500
2026-01-08 15:02:00,652: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:02:05,563: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 15:02:05,607: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 15:02:15,238: Val batch 17500: PER (avg): 0.1474 CTC Loss (avg): 15.0501 WER(5gram): 33.25% (n=256) time: 14.727
2026-01-08 15:02:15,241: WER lens: avg_true_words=5.99 avg_pred_words=6.40 max_pred_words=13
2026-01-08 15:02:15,243: t15.2023.08.13 val PER: 0.1091
2026-01-08 15:02:15,245: t15.2023.08.18 val PER: 0.1023
2026-01-08 15:02:15,247: t15.2023.08.20 val PER: 0.1048
2026-01-08 15:02:15,249: t15.2023.08.25 val PER: 0.1009
2026-01-08 15:02:15,251: t15.2023.08.27 val PER: 0.1865
2026-01-08 15:02:15,252: t15.2023.09.01 val PER: 0.0682
2026-01-08 15:02:15,253: t15.2023.09.03 val PER: 0.1639
2026-01-08 15:02:15,255: t15.2023.09.24 val PER: 0.1129
2026-01-08 15:02:15,256: t15.2023.09.29 val PER: 0.1251
2026-01-08 15:02:15,258: t15.2023.10.01 val PER: 0.1651
2026-01-08 15:02:15,259: t15.2023.10.06 val PER: 0.0786
2026-01-08 15:02:15,261: t15.2023.10.08 val PER: 0.2382
2026-01-08 15:02:15,263: t15.2023.10.13 val PER: 0.1939
2026-01-08 15:02:15,264: t15.2023.10.15 val PER: 0.1549
2026-01-08 15:02:15,266: t15.2023.10.20 val PER: 0.1779
2026-01-08 15:02:15,267: t15.2023.10.22 val PER: 0.1147
2026-01-08 15:02:15,269: t15.2023.11.03 val PER: 0.1777
2026-01-08 15:02:15,271: t15.2023.11.04 val PER: 0.0410
2026-01-08 15:02:15,272: t15.2023.11.17 val PER: 0.0389
2026-01-08 15:02:15,274: t15.2023.11.19 val PER: 0.0319
2026-01-08 15:02:15,275: t15.2023.11.26 val PER: 0.1109
2026-01-08 15:02:15,277: t15.2023.12.03 val PER: 0.1071
2026-01-08 15:02:15,278: t15.2023.12.08 val PER: 0.1045
2026-01-08 15:02:15,280: t15.2023.12.10 val PER: 0.0972
2026-01-08 15:02:15,281: t15.2023.12.17 val PER: 0.1258
2026-01-08 15:02:15,283: t15.2023.12.29 val PER: 0.1304
2026-01-08 15:02:15,284: t15.2024.02.25 val PER: 0.1138
2026-01-08 15:02:15,285: t15.2024.03.08 val PER: 0.2205
2026-01-08 15:02:15,287: t15.2024.03.15 val PER: 0.2014
2026-01-08 15:02:15,288: t15.2024.03.17 val PER: 0.1332
2026-01-08 15:02:15,289: t15.2024.05.10 val PER: 0.1694
2026-01-08 15:02:15,291: t15.2024.06.14 val PER: 0.1498
2026-01-08 15:02:15,292: t15.2024.07.19 val PER: 0.2347
2026-01-08 15:02:15,294: t15.2024.07.21 val PER: 0.0897
2026-01-08 15:02:15,295: t15.2024.07.28 val PER: 0.1390
2026-01-08 15:02:15,297: t15.2025.01.10 val PER: 0.2713
2026-01-08 15:02:15,298: t15.2025.01.12 val PER: 0.1478
2026-01-08 15:02:15,299: t15.2025.03.14 val PER: 0.3299
2026-01-08 15:02:15,301: t15.2025.03.16 val PER: 0.1963
2026-01-08 15:02:15,302: t15.2025.03.30 val PER: 0.2874
2026-01-08 15:02:15,304: t15.2025.04.13 val PER: 0.2168
2026-01-08 15:02:24,492: Train batch 17600: loss: 9.62 grad norm: 56.59 time: 0.054
2026-01-08 15:02:43,194: Train batch 17800: loss: 6.26 grad norm: 45.67 time: 0.042
2026-01-08 15:03:00,926: Train batch 18000: loss: 10.92 grad norm: 63.70 time: 0.062
2026-01-08 15:03:00,929: Running test after training batch: 18000
2026-01-08 15:03:01,062: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:03:06,012: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 15:03:06,059: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost
2026-01-08 15:03:15,755: Val batch 18000: PER (avg): 0.1464 CTC Loss (avg): 14.9814 WER(5gram): 33.38% (n=256) time: 14.823
2026-01-08 15:03:15,757: WER lens: avg_true_words=5.99 avg_pred_words=6.39 max_pred_words=12
2026-01-08 15:03:15,759: t15.2023.08.13 val PER: 0.1112
2026-01-08 15:03:15,760: t15.2023.08.18 val PER: 0.1014
2026-01-08 15:03:15,762: t15.2023.08.20 val PER: 0.1041
2026-01-08 15:03:15,763: t15.2023.08.25 val PER: 0.1069
2026-01-08 15:03:15,765: t15.2023.08.27 val PER: 0.1801
2026-01-08 15:03:15,766: t15.2023.09.01 val PER: 0.0722
2026-01-08 15:03:15,767: t15.2023.09.03 val PER: 0.1544
2026-01-08 15:03:15,769: t15.2023.09.24 val PER: 0.1201
2026-01-08 15:03:15,770: t15.2023.09.29 val PER: 0.1232
2026-01-08 15:03:15,771: t15.2023.10.01 val PER: 0.1697
2026-01-08 15:03:15,773: t15.2023.10.06 val PER: 0.0840
2026-01-08 15:03:15,774: t15.2023.10.08 val PER: 0.2300
2026-01-08 15:03:15,775: t15.2023.10.13 val PER: 0.1885
2026-01-08 15:03:15,777: t15.2023.10.15 val PER: 0.1536
2026-01-08 15:03:15,779: t15.2023.10.20 val PER: 0.1779
2026-01-08 15:03:15,781: t15.2023.10.22 val PER: 0.1169
2026-01-08 15:03:15,782: t15.2023.11.03 val PER: 0.1764
2026-01-08 15:03:15,784: t15.2023.11.04 val PER: 0.0273
2026-01-08 15:03:15,785: t15.2023.11.17 val PER: 0.0373
2026-01-08 15:03:15,786: t15.2023.11.19 val PER: 0.0299
2026-01-08 15:03:15,788: t15.2023.11.26 val PER: 0.1130
2026-01-08 15:03:15,789: t15.2023.12.03 val PER: 0.1082
2026-01-08 15:03:15,790: t15.2023.12.08 val PER: 0.0979
2026-01-08 15:03:15,792: t15.2023.12.10 val PER: 0.0920
2026-01-08 15:03:15,794: t15.2023.12.17 val PER: 0.1279
2026-01-08 15:03:15,795: t15.2023.12.29 val PER: 0.1249
2026-01-08 15:03:15,796: t15.2024.02.25 val PER: 0.1110
2026-01-08 15:03:15,798: t15.2024.03.08 val PER: 0.2091
2026-01-08 15:03:15,799: t15.2024.03.15 val PER: 0.2039
2026-01-08 15:03:15,800: t15.2024.03.17 val PER: 0.1346
2026-01-08 15:03:15,803: t15.2024.05.10 val PER: 0.1694
2026-01-08 15:03:15,804: t15.2024.06.14 val PER: 0.1467
2026-01-08 15:03:15,805: t15.2024.07.19 val PER: 0.2347
2026-01-08 15:03:15,807: t15.2024.07.21 val PER: 0.0841
2026-01-08 15:03:15,808: t15.2024.07.28 val PER: 0.1346
2026-01-08 15:03:15,810: t15.2025.01.10 val PER: 0.2727
2026-01-08 15:03:15,811: t15.2025.01.12 val PER: 0.1470
2026-01-08 15:03:15,812: t15.2025.03.14 val PER: 0.3417
2026-01-08 15:03:15,813: t15.2025.03.16 val PER: 0.1806
2026-01-08 15:03:15,815: t15.2025.03.30 val PER: 0.2885
2026-01-08 15:03:15,816: t15.2025.04.13 val PER: 0.2225
2026-01-08 15:03:33,649: Train batch 18200: loss: 8.32 grad norm: 49.51 time: 0.074
2026-01-08 15:03:50,992: Train batch 18400: loss: 4.83 grad norm: 40.85 time: 0.059
2026-01-08 15:03:59,881: Running test after training batch: 18500
2026-01-08 15:04:00,022: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:04:04,757: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 15:04:04,800: WER debug example
  GT : how does it keep the cost down
  PR : how do see it keep the cost
2026-01-08 15:04:14,620: Val batch 18500: PER (avg): 0.1459 CTC Loss (avg): 14.9513 WER(5gram): 32.46% (n=256) time: 14.737
2026-01-08 15:04:14,623: WER lens: avg_true_words=5.99 avg_pred_words=6.39 max_pred_words=12
2026-01-08 15:04:14,625: t15.2023.08.13 val PER: 0.1133
2026-01-08 15:04:14,627: t15.2023.08.18 val PER: 0.1006
2026-01-08 15:04:14,629: t15.2023.08.20 val PER: 0.1041
2026-01-08 15:04:14,631: t15.2023.08.25 val PER: 0.0994
2026-01-08 15:04:14,632: t15.2023.08.27 val PER: 0.1785
2026-01-08 15:04:14,634: t15.2023.09.01 val PER: 0.0706
2026-01-08 15:04:14,636: t15.2023.09.03 val PER: 0.1532
2026-01-08 15:04:14,638: t15.2023.09.24 val PER: 0.1129
2026-01-08 15:04:14,639: t15.2023.09.29 val PER: 0.1225
2026-01-08 15:04:14,641: t15.2023.10.01 val PER: 0.1664
2026-01-08 15:04:14,642: t15.2023.10.06 val PER: 0.0861
2026-01-08 15:04:14,644: t15.2023.10.08 val PER: 0.2314
2026-01-08 15:04:14,645: t15.2023.10.13 val PER: 0.1939
2026-01-08 15:04:14,647: t15.2023.10.15 val PER: 0.1543
2026-01-08 15:04:14,649: t15.2023.10.20 val PER: 0.1846
2026-01-08 15:04:14,650: t15.2023.10.22 val PER: 0.1180
2026-01-08 15:04:14,652: t15.2023.11.03 val PER: 0.1723
2026-01-08 15:04:14,653: t15.2023.11.04 val PER: 0.0307
2026-01-08 15:04:14,655: t15.2023.11.17 val PER: 0.0373
2026-01-08 15:04:14,656: t15.2023.11.19 val PER: 0.0319
2026-01-08 15:04:14,658: t15.2023.11.26 val PER: 0.1080
2026-01-08 15:04:14,659: t15.2023.12.03 val PER: 0.1061
2026-01-08 15:04:14,661: t15.2023.12.08 val PER: 0.0965
2026-01-08 15:04:14,662: t15.2023.12.10 val PER: 0.0986
2026-01-08 15:04:14,663: t15.2023.12.17 val PER: 0.1133
2026-01-08 15:04:14,665: t15.2023.12.29 val PER: 0.1208
2026-01-08 15:04:14,667: t15.2024.02.25 val PER: 0.1067
2026-01-08 15:04:14,668: t15.2024.03.08 val PER: 0.2063
2026-01-08 15:04:14,670: t15.2024.03.15 val PER: 0.2033
2026-01-08 15:04:14,671: t15.2024.03.17 val PER: 0.1276
2026-01-08 15:04:14,673: t15.2024.05.10 val PER: 0.1724
2026-01-08 15:04:14,674: t15.2024.06.14 val PER: 0.1451
2026-01-08 15:04:14,676: t15.2024.07.19 val PER: 0.2386
2026-01-08 15:04:14,677: t15.2024.07.21 val PER: 0.0890
2026-01-08 15:04:14,678: t15.2024.07.28 val PER: 0.1397
2026-01-08 15:04:14,680: t15.2025.01.10 val PER: 0.2796
2026-01-08 15:04:14,681: t15.2025.01.12 val PER: 0.1470
2026-01-08 15:04:14,683: t15.2025.03.14 val PER: 0.3432
2026-01-08 15:04:14,684: t15.2025.03.16 val PER: 0.1937
2026-01-08 15:04:14,686: t15.2025.03.30 val PER: 0.2931
2026-01-08 15:04:14,687: t15.2025.04.13 val PER: 0.2140
2026-01-08 15:04:23,319: Train batch 18600: loss: 12.50 grad norm: 60.77 time: 0.067
2026-01-08 15:04:41,637: Train batch 18800: loss: 8.86 grad norm: 56.80 time: 0.065
2026-01-08 15:05:00,232: Train batch 19000: loss: 8.25 grad norm: 49.75 time: 0.065
2026-01-08 15:05:00,234: Running test after training batch: 19000
2026-01-08 15:05:00,367: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:05:05,139: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 15:05:05,186: WER debug example
  GT : how does it keep the cost down
  PR : how dense it keep the cost
2026-01-08 15:05:15,166: Val batch 19000: PER (avg): 0.1452 CTC Loss (avg): 14.9502 WER(5gram): 33.05% (n=256) time: 14.930
2026-01-08 15:05:15,169: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=12
2026-01-08 15:05:15,171: t15.2023.08.13 val PER: 0.1091
2026-01-08 15:05:15,172: t15.2023.08.18 val PER: 0.0972
2026-01-08 15:05:15,174: t15.2023.08.20 val PER: 0.1001
2026-01-08 15:05:15,175: t15.2023.08.25 val PER: 0.1039
2026-01-08 15:05:15,177: t15.2023.08.27 val PER: 0.1801
2026-01-08 15:05:15,178: t15.2023.09.01 val PER: 0.0731
2026-01-08 15:05:15,180: t15.2023.09.03 val PER: 0.1544
2026-01-08 15:05:15,181: t15.2023.09.24 val PER: 0.1274
2026-01-08 15:05:15,183: t15.2023.09.29 val PER: 0.1257
2026-01-08 15:05:15,184: t15.2023.10.01 val PER: 0.1638
2026-01-08 15:05:15,186: t15.2023.10.06 val PER: 0.0861
2026-01-08 15:05:15,187: t15.2023.10.08 val PER: 0.2260
2026-01-08 15:05:15,189: t15.2023.10.13 val PER: 0.1932
2026-01-08 15:05:15,190: t15.2023.10.15 val PER: 0.1516
2026-01-08 15:05:15,192: t15.2023.10.20 val PER: 0.1779
2026-01-08 15:05:15,194: t15.2023.10.22 val PER: 0.1147
2026-01-08 15:05:15,196: t15.2023.11.03 val PER: 0.1757
2026-01-08 15:05:15,198: t15.2023.11.04 val PER: 0.0307
2026-01-08 15:05:15,199: t15.2023.11.17 val PER: 0.0389
2026-01-08 15:05:15,200: t15.2023.11.19 val PER: 0.0319
2026-01-08 15:05:15,202: t15.2023.11.26 val PER: 0.1058
2026-01-08 15:05:15,204: t15.2023.12.03 val PER: 0.1103
2026-01-08 15:05:15,205: t15.2023.12.08 val PER: 0.0979
2026-01-08 15:05:15,207: t15.2023.12.10 val PER: 0.0933
2026-01-08 15:05:15,209: t15.2023.12.17 val PER: 0.1258
2026-01-08 15:05:15,210: t15.2023.12.29 val PER: 0.1242
2026-01-08 15:05:15,212: t15.2024.02.25 val PER: 0.1096
2026-01-08 15:05:15,213: t15.2024.03.08 val PER: 0.2063
2026-01-08 15:05:15,215: t15.2024.03.15 val PER: 0.1989
2026-01-08 15:05:15,216: t15.2024.03.17 val PER: 0.1297
2026-01-08 15:05:15,218: t15.2024.05.10 val PER: 0.1738
2026-01-08 15:05:15,219: t15.2024.06.14 val PER: 0.1435
2026-01-08 15:05:15,220: t15.2024.07.19 val PER: 0.2320
2026-01-08 15:05:15,221: t15.2024.07.21 val PER: 0.0883
2026-01-08 15:05:15,223: t15.2024.07.28 val PER: 0.1368
2026-01-08 15:05:15,224: t15.2025.01.10 val PER: 0.2672
2026-01-08 15:05:15,225: t15.2025.01.12 val PER: 0.1470
2026-01-08 15:05:15,227: t15.2025.03.14 val PER: 0.3373
2026-01-08 15:05:15,228: t15.2025.03.16 val PER: 0.1793
2026-01-08 15:05:15,229: t15.2025.03.30 val PER: 0.2851
2026-01-08 15:05:15,230: t15.2025.04.13 val PER: 0.2183
2026-01-08 15:05:33,455: Train batch 19200: loss: 6.51 grad norm: 49.91 time: 0.064
2026-01-08 15:05:51,232: Train batch 19400: loss: 5.19 grad norm: 42.60 time: 0.054
2026-01-08 15:06:00,042: Running test after training batch: 19500
2026-01-08 15:06:00,138: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:06:04,895: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 15:06:04,942: WER debug example
  GT : how does it keep the cost down
  PR : how dense it keep the cost
2026-01-08 15:06:14,942: Val batch 19500: PER (avg): 0.1458 CTC Loss (avg): 14.8991 WER(5gram): 31.29% (n=256) time: 14.898
2026-01-08 15:06:14,945: WER lens: avg_true_words=5.99 avg_pred_words=6.37 max_pred_words=12
2026-01-08 15:06:14,947: t15.2023.08.13 val PER: 0.1143
2026-01-08 15:06:14,949: t15.2023.08.18 val PER: 0.0981
2026-01-08 15:06:14,950: t15.2023.08.20 val PER: 0.1033
2026-01-08 15:06:14,952: t15.2023.08.25 val PER: 0.1039
2026-01-08 15:06:14,954: t15.2023.08.27 val PER: 0.1768
2026-01-08 15:06:14,955: t15.2023.09.01 val PER: 0.0698
2026-01-08 15:06:14,957: t15.2023.09.03 val PER: 0.1580
2026-01-08 15:06:14,958: t15.2023.09.24 val PER: 0.1189
2026-01-08 15:06:14,960: t15.2023.09.29 val PER: 0.1270
2026-01-08 15:06:14,961: t15.2023.10.01 val PER: 0.1645
2026-01-08 15:06:14,963: t15.2023.10.06 val PER: 0.0883
2026-01-08 15:06:14,964: t15.2023.10.08 val PER: 0.2300
2026-01-08 15:06:14,965: t15.2023.10.13 val PER: 0.1877
2026-01-08 15:06:14,968: t15.2023.10.15 val PER: 0.1496
2026-01-08 15:06:14,970: t15.2023.10.20 val PER: 0.1745
2026-01-08 15:06:14,971: t15.2023.10.22 val PER: 0.1136
2026-01-08 15:06:14,973: t15.2023.11.03 val PER: 0.1757
2026-01-08 15:06:14,974: t15.2023.11.04 val PER: 0.0307
2026-01-08 15:06:14,976: t15.2023.11.17 val PER: 0.0373
2026-01-08 15:06:14,977: t15.2023.11.19 val PER: 0.0339
2026-01-08 15:06:14,978: t15.2023.11.26 val PER: 0.1072
2026-01-08 15:06:14,980: t15.2023.12.03 val PER: 0.1113
2026-01-08 15:06:14,982: t15.2023.12.08 val PER: 0.0979
2026-01-08 15:06:14,983: t15.2023.12.10 val PER: 0.0894
2026-01-08 15:06:14,984: t15.2023.12.17 val PER: 0.1279
2026-01-08 15:06:14,986: t15.2023.12.29 val PER: 0.1270
2026-01-08 15:06:14,987: t15.2024.02.25 val PER: 0.1081
2026-01-08 15:06:14,989: t15.2024.03.08 val PER: 0.2077
2026-01-08 15:06:14,990: t15.2024.03.15 val PER: 0.2051
2026-01-08 15:06:14,992: t15.2024.03.17 val PER: 0.1276
2026-01-08 15:06:14,993: t15.2024.05.10 val PER: 0.1724
2026-01-08 15:06:14,994: t15.2024.06.14 val PER: 0.1467
2026-01-08 15:06:14,996: t15.2024.07.19 val PER: 0.2314
2026-01-08 15:06:14,997: t15.2024.07.21 val PER: 0.0862
2026-01-08 15:06:14,999: t15.2024.07.28 val PER: 0.1412
2026-01-08 15:06:15,000: t15.2025.01.10 val PER: 0.2700
2026-01-08 15:06:15,002: t15.2025.01.12 val PER: 0.1424
2026-01-08 15:06:15,003: t15.2025.03.14 val PER: 0.3447
2026-01-08 15:06:15,005: t15.2025.03.16 val PER: 0.1911
2026-01-08 15:06:15,006: t15.2025.03.30 val PER: 0.2874
2026-01-08 15:06:15,008: t15.2025.04.13 val PER: 0.2197
2026-01-08 15:06:24,020: Train batch 19600: loss: 7.92 grad norm: 51.58 time: 0.059
2026-01-08 15:06:42,027: Train batch 19800: loss: 7.47 grad norm: 48.93 time: 0.055
2026-01-08 15:06:59,285: Running test after training batch: 19999
2026-01-08 15:06:59,380: WER debug GT example: You can see the code at this point as well.
2026-01-08 15:07:04,101: WER debug example
  GT : you can see the code at this point as well
  PR : you can see the code at this point will
2026-01-08 15:07:04,147: WER debug example
  GT : how does it keep the cost down
  PR : how dense it keep the cost
2026-01-08 15:07:14,265: Val batch 19999: PER (avg): 0.1449 CTC Loss (avg): 14.8990 WER(5gram): 30.18% (n=256) time: 14.978
2026-01-08 15:07:14,267: WER lens: avg_true_words=5.99 avg_pred_words=6.32 max_pred_words=12
2026-01-08 15:07:14,270: t15.2023.08.13 val PER: 0.1133
2026-01-08 15:07:14,271: t15.2023.08.18 val PER: 0.0981
2026-01-08 15:07:14,273: t15.2023.08.20 val PER: 0.1025
2026-01-08 15:07:14,275: t15.2023.08.25 val PER: 0.1069
2026-01-08 15:07:14,276: t15.2023.08.27 val PER: 0.1785
2026-01-08 15:07:14,278: t15.2023.09.01 val PER: 0.0698
2026-01-08 15:07:14,279: t15.2023.09.03 val PER: 0.1568
2026-01-08 15:07:14,281: t15.2023.09.24 val PER: 0.1214
2026-01-08 15:07:14,283: t15.2023.09.29 val PER: 0.1244
2026-01-08 15:07:14,284: t15.2023.10.01 val PER: 0.1658
2026-01-08 15:07:14,286: t15.2023.10.06 val PER: 0.0893
2026-01-08 15:07:14,287: t15.2023.10.08 val PER: 0.2327
2026-01-08 15:07:14,289: t15.2023.10.13 val PER: 0.1924
2026-01-08 15:07:14,291: t15.2023.10.15 val PER: 0.1463
2026-01-08 15:07:14,292: t15.2023.10.20 val PER: 0.1779
2026-01-08 15:07:14,295: t15.2023.10.22 val PER: 0.1158
2026-01-08 15:07:14,296: t15.2023.11.03 val PER: 0.1723
2026-01-08 15:07:14,300: t15.2023.11.04 val PER: 0.0341
2026-01-08 15:07:14,302: t15.2023.11.17 val PER: 0.0358
2026-01-08 15:07:14,303: t15.2023.11.19 val PER: 0.0319
2026-01-08 15:07:14,308: t15.2023.11.26 val PER: 0.1036
2026-01-08 15:07:14,310: t15.2023.12.03 val PER: 0.1113
2026-01-08 15:07:14,312: t15.2023.12.08 val PER: 0.0972
2026-01-08 15:07:14,314: t15.2023.12.10 val PER: 0.0907
2026-01-08 15:07:14,316: t15.2023.12.17 val PER: 0.1206
2026-01-08 15:07:14,317: t15.2023.12.29 val PER: 0.1283
2026-01-08 15:07:14,319: t15.2024.02.25 val PER: 0.1096
2026-01-08 15:07:14,320: t15.2024.03.08 val PER: 0.2091
2026-01-08 15:07:14,321: t15.2024.03.15 val PER: 0.2026
2026-01-08 15:07:14,323: t15.2024.03.17 val PER: 0.1297
2026-01-08 15:07:14,325: t15.2024.05.10 val PER: 0.1694
2026-01-08 15:07:14,327: t15.2024.06.14 val PER: 0.1451
2026-01-08 15:07:14,328: t15.2024.07.19 val PER: 0.2287
2026-01-08 15:07:14,330: t15.2024.07.21 val PER: 0.0876
2026-01-08 15:07:14,331: t15.2024.07.28 val PER: 0.1360
2026-01-08 15:07:14,333: t15.2025.01.10 val PER: 0.2713
2026-01-08 15:07:14,334: t15.2025.01.12 val PER: 0.1455
2026-01-08 15:07:14,335: t15.2025.03.14 val PER: 0.3373
2026-01-08 15:07:14,337: t15.2025.03.16 val PER: 0.1806
2026-01-08 15:07:14,338: t15.2025.03.30 val PER: 0.2759
2026-01-08 15:07:14,340: t15.2025.04.13 val PER: 0.2183
2026-01-08 15:07:14,342: New best val WER(5gram) 31.03% --> 30.18%
2026-01-08 15:07:14,343: Checkpointing model
2026-01-08 15:07:14,496: Saved model to checkpoint: /home/e12511253/Brain2Text/brain2text/trained_models/lr40_wd1e-5/id10_wd1e-5/checkpoint/best_checkpoint
2026-01-08 15:07:15,008: Best avg val PER achieved: 0.14491
2026-01-08 15:07:15,011: Total training time: 44.75 minutes
