TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_348248
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_348248/torch_extensions
WANDB_DIR=/tmp/e12511253_b2t_348248/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/e12511253_b2t_348248/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  3 21:58 /tmp/e12511253_b2t_348248/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
trained_models -> /tmp/e12511253_b2t_348248/trained_models
OUT_ROOT=/tmp/e12511253_b2t_348248/trained_models
==============================================
Job: b2t_exp  ID: 348248
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/gru/rnn_dropout/lr40
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/gru/rnn_dropout/lr40 ==========
Num configs: 5

=== RUN base.yaml ===
2026-01-03 21:58:08,313: Using device: cuda:0
2026-01-03 21:58:09,925: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-03 21:58:09,951: Using 45 sessions after filtering (from 45).
2026-01-03 21:58:10,381: Using torch.compile (if available)
2026-01-03 21:58:10,381: torch.compile not available (torch<2.0). Skipping.
2026-01-03 21:58:10,381: Initialized RNN decoding model
2026-01-03 21:58:10,382: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 21:58:10,382: Model has 44,907,305 parameters
2026-01-03 21:58:10,382: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 21:58:11,644: Successfully initialized datasets
2026-01-03 21:58:11,644: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 21:58:12,568: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.177
2026-01-03 21:58:12,568: Running test after training batch: 0
2026-01-03 21:58:12,678: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:58:17,917: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-03 21:58:18,628: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-03 21:58:52,287: Val batch 0: PER (avg): 1.4293 CTC Loss (avg): 633.1811 WER(1gram): 100.00% (n=64) time: 39.719
2026-01-03 21:58:52,287: WER lens: avg_true_words=6.16 avg_pred_words=1.77 max_pred_words=4
2026-01-03 21:58:52,287: t15.2023.08.13 val PER: 1.3056
2026-01-03 21:58:52,287: t15.2023.08.18 val PER: 1.4208
2026-01-03 21:58:52,288: t15.2023.08.20 val PER: 1.3002
2026-01-03 21:58:52,288: t15.2023.08.25 val PER: 1.3389
2026-01-03 21:58:52,288: t15.2023.08.27 val PER: 1.2460
2026-01-03 21:58:52,288: t15.2023.09.01 val PER: 1.4537
2026-01-03 21:58:52,288: t15.2023.09.03 val PER: 1.3171
2026-01-03 21:58:52,288: t15.2023.09.24 val PER: 1.5461
2026-01-03 21:58:52,288: t15.2023.09.29 val PER: 1.4671
2026-01-03 21:58:52,288: t15.2023.10.01 val PER: 1.2147
2026-01-03 21:58:52,288: t15.2023.10.06 val PER: 1.4876
2026-01-03 21:58:52,288: t15.2023.10.08 val PER: 1.1827
2026-01-03 21:58:52,288: t15.2023.10.13 val PER: 1.3964
2026-01-03 21:58:52,288: t15.2023.10.15 val PER: 1.3889
2026-01-03 21:58:52,288: t15.2023.10.20 val PER: 1.4866
2026-01-03 21:58:52,288: t15.2023.10.22 val PER: 1.3942
2026-01-03 21:58:52,288: t15.2023.11.03 val PER: 1.5923
2026-01-03 21:58:52,288: t15.2023.11.04 val PER: 2.0171
2026-01-03 21:58:52,289: t15.2023.11.17 val PER: 1.9518
2026-01-03 21:58:52,289: t15.2023.11.19 val PER: 1.6707
2026-01-03 21:58:52,289: t15.2023.11.26 val PER: 1.5413
2026-01-03 21:58:52,289: t15.2023.12.03 val PER: 1.4254
2026-01-03 21:58:52,289: t15.2023.12.08 val PER: 1.4487
2026-01-03 21:58:52,289: t15.2023.12.10 val PER: 1.6899
2026-01-03 21:58:52,289: t15.2023.12.17 val PER: 1.3077
2026-01-03 21:58:52,289: t15.2023.12.29 val PER: 1.4063
2026-01-03 21:58:52,289: t15.2024.02.25 val PER: 1.4228
2026-01-03 21:58:52,289: t15.2024.03.08 val PER: 1.3257
2026-01-03 21:58:52,289: t15.2024.03.15 val PER: 1.3196
2026-01-03 21:58:52,289: t15.2024.03.17 val PER: 1.4052
2026-01-03 21:58:52,289: t15.2024.05.10 val PER: 1.3224
2026-01-03 21:58:52,289: t15.2024.06.14 val PER: 1.5315
2026-01-03 21:58:52,289: t15.2024.07.19 val PER: 1.0817
2026-01-03 21:58:52,290: t15.2024.07.21 val PER: 1.6290
2026-01-03 21:58:52,290: t15.2024.07.28 val PER: 1.6588
2026-01-03 21:58:52,290: t15.2025.01.10 val PER: 1.0923
2026-01-03 21:58:52,290: t15.2025.01.12 val PER: 1.7629
2026-01-03 21:58:52,290: t15.2025.03.14 val PER: 1.0414
2026-01-03 21:58:52,290: t15.2025.03.16 val PER: 1.6257
2026-01-03 21:58:52,290: t15.2025.03.30 val PER: 1.2874
2026-01-03 21:58:52,290: t15.2025.04.13 val PER: 1.5949
2026-01-03 21:58:52,291: New best val WER(1gram) inf% --> 100.00%
2026-01-03 21:58:52,291: Checkpointing model
2026-01-03 21:58:52,528: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 21:58:52,769: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_0
2026-01-03 21:59:10,172: Train batch 200: loss: 77.59 grad norm: 105.96 time: 0.055
2026-01-03 21:59:27,233: Train batch 400: loss: 53.53 grad norm: 86.79 time: 0.063
2026-01-03 21:59:35,754: Running test after training batch: 500
2026-01-03 21:59:35,886: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:59:41,109: WER debug example
  GT : you can see the code at this point as well
  PR : used and ease thus uhde at this ide is aisle
2026-01-03 21:59:41,142: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink thus as adz
2026-01-03 21:59:43,243: Val batch 500: PER (avg): 0.5334 CTC Loss (avg): 55.0887 WER(1gram): 89.59% (n=64) time: 7.489
2026-01-03 21:59:43,244: WER lens: avg_true_words=6.16 avg_pred_words=5.47 max_pred_words=11
2026-01-03 21:59:43,244: t15.2023.08.13 val PER: 0.4761
2026-01-03 21:59:43,244: t15.2023.08.18 val PER: 0.4593
2026-01-03 21:59:43,244: t15.2023.08.20 val PER: 0.4527
2026-01-03 21:59:43,244: t15.2023.08.25 val PER: 0.4428
2026-01-03 21:59:43,244: t15.2023.08.27 val PER: 0.5370
2026-01-03 21:59:43,244: t15.2023.09.01 val PER: 0.4302
2026-01-03 21:59:43,244: t15.2023.09.03 val PER: 0.5154
2026-01-03 21:59:43,245: t15.2023.09.24 val PER: 0.4345
2026-01-03 21:59:43,245: t15.2023.09.29 val PER: 0.4742
2026-01-03 21:59:43,245: t15.2023.10.01 val PER: 0.5291
2026-01-03 21:59:43,245: t15.2023.10.06 val PER: 0.4403
2026-01-03 21:59:43,245: t15.2023.10.08 val PER: 0.5440
2026-01-03 21:59:43,245: t15.2023.10.13 val PER: 0.5896
2026-01-03 21:59:43,245: t15.2023.10.15 val PER: 0.5063
2026-01-03 21:59:43,245: t15.2023.10.20 val PER: 0.4765
2026-01-03 21:59:43,246: t15.2023.10.22 val PER: 0.4666
2026-01-03 21:59:43,246: t15.2023.11.03 val PER: 0.5102
2026-01-03 21:59:43,246: t15.2023.11.04 val PER: 0.2628
2026-01-03 21:59:43,246: t15.2023.11.17 val PER: 0.3701
2026-01-03 21:59:43,246: t15.2023.11.19 val PER: 0.3234
2026-01-03 21:59:43,246: t15.2023.11.26 val PER: 0.5717
2026-01-03 21:59:43,246: t15.2023.12.03 val PER: 0.5116
2026-01-03 21:59:43,246: t15.2023.12.08 val PER: 0.5373
2026-01-03 21:59:43,246: t15.2023.12.10 val PER: 0.4625
2026-01-03 21:59:43,246: t15.2023.12.17 val PER: 0.5780
2026-01-03 21:59:43,247: t15.2023.12.29 val PER: 0.5484
2026-01-03 21:59:43,247: t15.2024.02.25 val PER: 0.4874
2026-01-03 21:59:43,247: t15.2024.03.08 val PER: 0.6373
2026-01-03 21:59:43,247: t15.2024.03.15 val PER: 0.5760
2026-01-03 21:59:43,247: t15.2024.03.17 val PER: 0.5335
2026-01-03 21:59:43,247: t15.2024.05.10 val PER: 0.5661
2026-01-03 21:59:43,247: t15.2024.06.14 val PER: 0.5347
2026-01-03 21:59:43,247: t15.2024.07.19 val PER: 0.6961
2026-01-03 21:59:43,247: t15.2024.07.21 val PER: 0.4966
2026-01-03 21:59:43,247: t15.2024.07.28 val PER: 0.5419
2026-01-03 21:59:43,247: t15.2025.01.10 val PER: 0.7658
2026-01-03 21:59:43,247: t15.2025.01.12 val PER: 0.5797
2026-01-03 21:59:43,247: t15.2025.03.14 val PER: 0.7604
2026-01-03 21:59:43,247: t15.2025.03.16 val PER: 0.6113
2026-01-03 21:59:43,247: t15.2025.03.30 val PER: 0.7632
2026-01-03 21:59:43,248: t15.2025.04.13 val PER: 0.6006
2026-01-03 21:59:43,248: New best val WER(1gram) 100.00% --> 89.59%
2026-01-03 21:59:43,248: Checkpointing model
2026-01-03 21:59:43,812: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 21:59:44,056: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_500
2026-01-03 21:59:52,736: Train batch 600: loss: 48.85 grad norm: 77.37 time: 0.078
2026-01-03 22:00:09,953: Train batch 800: loss: 41.22 grad norm: 86.11 time: 0.057
2026-01-03 22:00:27,276: Train batch 1000: loss: 42.92 grad norm: 80.89 time: 0.066
2026-01-03 22:00:27,276: Running test after training batch: 1000
2026-01-03 22:00:27,405: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:00:32,198: WER debug example
  GT : you can see the code at this point as well
  PR : used ent ease thus code it this uhde is while
2026-01-03 22:00:32,231: WER debug example
  GT : how does it keep the cost down
  PR : houde is it eke thus wass
2026-01-03 22:00:34,060: Val batch 1000: PER (avg): 0.4093 CTC Loss (avg): 42.4819 WER(1gram): 81.98% (n=64) time: 6.783
2026-01-03 22:00:34,060: WER lens: avg_true_words=6.16 avg_pred_words=5.47 max_pred_words=12
2026-01-03 22:00:34,061: t15.2023.08.13 val PER: 0.3773
2026-01-03 22:00:34,061: t15.2023.08.18 val PER: 0.3445
2026-01-03 22:00:34,061: t15.2023.08.20 val PER: 0.3471
2026-01-03 22:00:34,061: t15.2023.08.25 val PER: 0.2952
2026-01-03 22:00:34,061: t15.2023.08.27 val PER: 0.4228
2026-01-03 22:00:34,061: t15.2023.09.01 val PER: 0.3068
2026-01-03 22:00:34,061: t15.2023.09.03 val PER: 0.4062
2026-01-03 22:00:34,061: t15.2023.09.24 val PER: 0.3143
2026-01-03 22:00:34,061: t15.2023.09.29 val PER: 0.3593
2026-01-03 22:00:34,061: t15.2023.10.01 val PER: 0.3996
2026-01-03 22:00:34,061: t15.2023.10.06 val PER: 0.3122
2026-01-03 22:00:34,061: t15.2023.10.08 val PER: 0.4587
2026-01-03 22:00:34,061: t15.2023.10.13 val PER: 0.4631
2026-01-03 22:00:34,061: t15.2023.10.15 val PER: 0.3784
2026-01-03 22:00:34,062: t15.2023.10.20 val PER: 0.3792
2026-01-03 22:00:34,062: t15.2023.10.22 val PER: 0.3530
2026-01-03 22:00:34,062: t15.2023.11.03 val PER: 0.4050
2026-01-03 22:00:34,062: t15.2023.11.04 val PER: 0.1433
2026-01-03 22:00:34,062: t15.2023.11.17 val PER: 0.2582
2026-01-03 22:00:34,062: t15.2023.11.19 val PER: 0.2096
2026-01-03 22:00:34,062: t15.2023.11.26 val PER: 0.4478
2026-01-03 22:00:34,062: t15.2023.12.03 val PER: 0.4034
2026-01-03 22:00:34,062: t15.2023.12.08 val PER: 0.4028
2026-01-03 22:00:34,062: t15.2023.12.10 val PER: 0.3522
2026-01-03 22:00:34,062: t15.2023.12.17 val PER: 0.4075
2026-01-03 22:00:34,062: t15.2023.12.29 val PER: 0.4104
2026-01-03 22:00:34,062: t15.2024.02.25 val PER: 0.3610
2026-01-03 22:00:34,062: t15.2024.03.08 val PER: 0.5021
2026-01-03 22:00:34,063: t15.2024.03.15 val PER: 0.4465
2026-01-03 22:00:34,063: t15.2024.03.17 val PER: 0.4114
2026-01-03 22:00:34,063: t15.2024.05.10 val PER: 0.4324
2026-01-03 22:00:34,063: t15.2024.06.14 val PER: 0.4054
2026-01-03 22:00:34,063: t15.2024.07.19 val PER: 0.5379
2026-01-03 22:00:34,063: t15.2024.07.21 val PER: 0.3779
2026-01-03 22:00:34,063: t15.2024.07.28 val PER: 0.4206
2026-01-03 22:00:34,064: t15.2025.01.10 val PER: 0.6088
2026-01-03 22:00:34,064: t15.2025.01.12 val PER: 0.4450
2026-01-03 22:00:34,064: t15.2025.03.14 val PER: 0.6405
2026-01-03 22:00:34,064: t15.2025.03.16 val PER: 0.4699
2026-01-03 22:00:34,064: t15.2025.03.30 val PER: 0.6471
2026-01-03 22:00:34,064: t15.2025.04.13 val PER: 0.4922
2026-01-03 22:00:34,065: New best val WER(1gram) 89.59% --> 81.98%
2026-01-03 22:00:34,065: Checkpointing model
2026-01-03 22:00:34,660: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:00:34,905: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_1000
2026-01-03 22:00:52,079: Train batch 1200: loss: 33.14 grad norm: 72.38 time: 0.068
2026-01-03 22:01:09,482: Train batch 1400: loss: 36.58 grad norm: 83.57 time: 0.061
2026-01-03 22:01:18,100: Running test after training batch: 1500
2026-01-03 22:01:18,254: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:01:23,170: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt ease the code at this boyde is while
2026-01-03 22:01:23,201: WER debug example
  GT : how does it keep the cost down
  PR : houde is it heap thus os it
2026-01-03 22:01:24,807: Val batch 1500: PER (avg): 0.3754 CTC Loss (avg): 37.0066 WER(1gram): 75.63% (n=64) time: 6.707
2026-01-03 22:01:24,808: WER lens: avg_true_words=6.16 avg_pred_words=5.22 max_pred_words=11
2026-01-03 22:01:24,808: t15.2023.08.13 val PER: 0.3441
2026-01-03 22:01:24,808: t15.2023.08.18 val PER: 0.3076
2026-01-03 22:01:24,808: t15.2023.08.20 val PER: 0.3034
2026-01-03 22:01:24,808: t15.2023.08.25 val PER: 0.2651
2026-01-03 22:01:24,808: t15.2023.08.27 val PER: 0.3875
2026-01-03 22:01:24,808: t15.2023.09.01 val PER: 0.2727
2026-01-03 22:01:24,808: t15.2023.09.03 val PER: 0.3741
2026-01-03 22:01:24,808: t15.2023.09.24 val PER: 0.3119
2026-01-03 22:01:24,808: t15.2023.09.29 val PER: 0.3357
2026-01-03 22:01:24,808: t15.2023.10.01 val PER: 0.3851
2026-01-03 22:01:24,809: t15.2023.10.06 val PER: 0.2831
2026-01-03 22:01:24,809: t15.2023.10.08 val PER: 0.4398
2026-01-03 22:01:24,809: t15.2023.10.13 val PER: 0.4407
2026-01-03 22:01:24,809: t15.2023.10.15 val PER: 0.3612
2026-01-03 22:01:24,809: t15.2023.10.20 val PER: 0.3356
2026-01-03 22:01:24,809: t15.2023.10.22 val PER: 0.3040
2026-01-03 22:01:24,809: t15.2023.11.03 val PER: 0.3562
2026-01-03 22:01:24,809: t15.2023.11.04 val PER: 0.1263
2026-01-03 22:01:24,809: t15.2023.11.17 val PER: 0.2240
2026-01-03 22:01:24,809: t15.2023.11.19 val PER: 0.1756
2026-01-03 22:01:24,809: t15.2023.11.26 val PER: 0.4123
2026-01-03 22:01:24,809: t15.2023.12.03 val PER: 0.3687
2026-01-03 22:01:24,809: t15.2023.12.08 val PER: 0.3515
2026-01-03 22:01:24,809: t15.2023.12.10 val PER: 0.2904
2026-01-03 22:01:24,810: t15.2023.12.17 val PER: 0.3711
2026-01-03 22:01:24,810: t15.2023.12.29 val PER: 0.3741
2026-01-03 22:01:24,810: t15.2024.02.25 val PER: 0.3062
2026-01-03 22:01:24,810: t15.2024.03.08 val PER: 0.4538
2026-01-03 22:01:24,810: t15.2024.03.15 val PER: 0.4140
2026-01-03 22:01:24,810: t15.2024.03.17 val PER: 0.3717
2026-01-03 22:01:24,810: t15.2024.05.10 val PER: 0.3744
2026-01-03 22:01:24,810: t15.2024.06.14 val PER: 0.3833
2026-01-03 22:01:24,810: t15.2024.07.19 val PER: 0.5148
2026-01-03 22:01:24,810: t15.2024.07.21 val PER: 0.3400
2026-01-03 22:01:24,810: t15.2024.07.28 val PER: 0.3632
2026-01-03 22:01:24,810: t15.2025.01.10 val PER: 0.5937
2026-01-03 22:01:24,810: t15.2025.01.12 val PER: 0.4142
2026-01-03 22:01:24,810: t15.2025.03.14 val PER: 0.6095
2026-01-03 22:01:24,810: t15.2025.03.16 val PER: 0.4450
2026-01-03 22:01:24,810: t15.2025.03.30 val PER: 0.6310
2026-01-03 22:01:24,811: t15.2025.04.13 val PER: 0.4551
2026-01-03 22:01:24,812: New best val WER(1gram) 81.98% --> 75.63%
2026-01-03 22:01:24,812: Checkpointing model
2026-01-03 22:01:25,387: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:01:25,630: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_1500
2026-01-03 22:01:34,156: Train batch 1600: loss: 36.60 grad norm: 81.48 time: 0.064
2026-01-03 22:01:51,480: Train batch 1800: loss: 35.31 grad norm: 73.89 time: 0.088
2026-01-03 22:02:08,934: Train batch 2000: loss: 34.00 grad norm: 72.63 time: 0.066
2026-01-03 22:02:08,935: Running test after training batch: 2000
2026-01-03 22:02:09,057: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:02:13,799: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this bonde wheel
2026-01-03 22:02:13,829: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the wass it
2026-01-03 22:02:15,425: Val batch 2000: PER (avg): 0.3269 CTC Loss (avg): 32.6303 WER(1gram): 68.78% (n=64) time: 6.490
2026-01-03 22:02:15,426: WER lens: avg_true_words=6.16 avg_pred_words=5.41 max_pred_words=11
2026-01-03 22:02:15,426: t15.2023.08.13 val PER: 0.3015
2026-01-03 22:02:15,426: t15.2023.08.18 val PER: 0.2548
2026-01-03 22:02:15,426: t15.2023.08.20 val PER: 0.2462
2026-01-03 22:02:15,426: t15.2023.08.25 val PER: 0.2289
2026-01-03 22:02:15,426: t15.2023.08.27 val PER: 0.3376
2026-01-03 22:02:15,426: t15.2023.09.01 val PER: 0.2370
2026-01-03 22:02:15,426: t15.2023.09.03 val PER: 0.3242
2026-01-03 22:02:15,427: t15.2023.09.24 val PER: 0.2524
2026-01-03 22:02:15,427: t15.2023.09.29 val PER: 0.2712
2026-01-03 22:02:15,427: t15.2023.10.01 val PER: 0.3256
2026-01-03 22:02:15,427: t15.2023.10.06 val PER: 0.2422
2026-01-03 22:02:15,427: t15.2023.10.08 val PER: 0.3965
2026-01-03 22:02:15,427: t15.2023.10.13 val PER: 0.3770
2026-01-03 22:02:15,427: t15.2023.10.15 val PER: 0.3045
2026-01-03 22:02:15,427: t15.2023.10.20 val PER: 0.3255
2026-01-03 22:02:15,427: t15.2023.10.22 val PER: 0.2606
2026-01-03 22:02:15,427: t15.2023.11.03 val PER: 0.3284
2026-01-03 22:02:15,427: t15.2023.11.04 val PER: 0.1024
2026-01-03 22:02:15,427: t15.2023.11.17 val PER: 0.1664
2026-01-03 22:02:15,427: t15.2023.11.19 val PER: 0.1417
2026-01-03 22:02:15,427: t15.2023.11.26 val PER: 0.3688
2026-01-03 22:02:15,428: t15.2023.12.03 val PER: 0.3109
2026-01-03 22:02:15,428: t15.2023.12.08 val PER: 0.3009
2026-01-03 22:02:15,428: t15.2023.12.10 val PER: 0.2536
2026-01-03 22:02:15,428: t15.2023.12.17 val PER: 0.3150
2026-01-03 22:02:15,428: t15.2023.12.29 val PER: 0.3212
2026-01-03 22:02:15,428: t15.2024.02.25 val PER: 0.2739
2026-01-03 22:02:15,428: t15.2024.03.08 val PER: 0.3926
2026-01-03 22:02:15,428: t15.2024.03.15 val PER: 0.3627
2026-01-03 22:02:15,428: t15.2024.03.17 val PER: 0.3382
2026-01-03 22:02:15,428: t15.2024.05.10 val PER: 0.3314
2026-01-03 22:02:15,428: t15.2024.06.14 val PER: 0.3407
2026-01-03 22:02:15,428: t15.2024.07.19 val PER: 0.4661
2026-01-03 22:02:15,428: t15.2024.07.21 val PER: 0.2945
2026-01-03 22:02:15,428: t15.2024.07.28 val PER: 0.3250
2026-01-03 22:02:15,428: t15.2025.01.10 val PER: 0.5262
2026-01-03 22:02:15,428: t15.2025.01.12 val PER: 0.3841
2026-01-03 22:02:15,428: t15.2025.03.14 val PER: 0.5296
2026-01-03 22:02:15,429: t15.2025.03.16 val PER: 0.4018
2026-01-03 22:02:15,429: t15.2025.03.30 val PER: 0.5425
2026-01-03 22:02:15,429: t15.2025.04.13 val PER: 0.4223
2026-01-03 22:02:15,430: New best val WER(1gram) 75.63% --> 68.78%
2026-01-03 22:02:15,430: Checkpointing model
2026-01-03 22:02:16,033: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:02:16,277: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_2000
2026-01-03 22:02:33,464: Train batch 2200: loss: 28.94 grad norm: 71.80 time: 0.061
2026-01-03 22:02:50,950: Train batch 2400: loss: 29.36 grad norm: 62.57 time: 0.052
2026-01-03 22:02:59,614: Running test after training batch: 2500
2026-01-03 22:02:59,764: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:03:04,503: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-03 22:03:04,530: WER debug example
  GT : how does it keep the cost down
  PR : houde des it eke the us it
2026-01-03 22:03:06,223: Val batch 2500: PER (avg): 0.3023 CTC Loss (avg): 30.2633 WER(1gram): 69.80% (n=64) time: 6.609
2026-01-03 22:03:06,224: WER lens: avg_true_words=6.16 avg_pred_words=5.78 max_pred_words=11
2026-01-03 22:03:06,224: t15.2023.08.13 val PER: 0.2869
2026-01-03 22:03:06,224: t15.2023.08.18 val PER: 0.2355
2026-01-03 22:03:06,224: t15.2023.08.20 val PER: 0.2327
2026-01-03 22:03:06,224: t15.2023.08.25 val PER: 0.2003
2026-01-03 22:03:06,224: t15.2023.08.27 val PER: 0.3296
2026-01-03 22:03:06,224: t15.2023.09.01 val PER: 0.2167
2026-01-03 22:03:06,225: t15.2023.09.03 val PER: 0.2910
2026-01-03 22:03:06,225: t15.2023.09.24 val PER: 0.2245
2026-01-03 22:03:06,225: t15.2023.09.29 val PER: 0.2527
2026-01-03 22:03:06,225: t15.2023.10.01 val PER: 0.3151
2026-01-03 22:03:06,225: t15.2023.10.06 val PER: 0.2164
2026-01-03 22:03:06,225: t15.2023.10.08 val PER: 0.3681
2026-01-03 22:03:06,225: t15.2023.10.13 val PER: 0.3491
2026-01-03 22:03:06,225: t15.2023.10.15 val PER: 0.2821
2026-01-03 22:03:06,225: t15.2023.10.20 val PER: 0.2752
2026-01-03 22:03:06,225: t15.2023.10.22 val PER: 0.2283
2026-01-03 22:03:06,225: t15.2023.11.03 val PER: 0.2938
2026-01-03 22:03:06,225: t15.2023.11.04 val PER: 0.0717
2026-01-03 22:03:06,225: t15.2023.11.17 val PER: 0.1431
2026-01-03 22:03:06,225: t15.2023.11.19 val PER: 0.1218
2026-01-03 22:03:06,226: t15.2023.11.26 val PER: 0.3457
2026-01-03 22:03:06,226: t15.2023.12.03 val PER: 0.2836
2026-01-03 22:03:06,226: t15.2023.12.08 val PER: 0.2776
2026-01-03 22:03:06,226: t15.2023.12.10 val PER: 0.2405
2026-01-03 22:03:06,226: t15.2023.12.17 val PER: 0.2869
2026-01-03 22:03:06,226: t15.2023.12.29 val PER: 0.3109
2026-01-03 22:03:06,226: t15.2024.02.25 val PER: 0.2388
2026-01-03 22:03:06,226: t15.2024.03.08 val PER: 0.3499
2026-01-03 22:03:06,226: t15.2024.03.15 val PER: 0.3483
2026-01-03 22:03:06,226: t15.2024.03.17 val PER: 0.3082
2026-01-03 22:03:06,226: t15.2024.05.10 val PER: 0.3061
2026-01-03 22:03:06,226: t15.2024.06.14 val PER: 0.3013
2026-01-03 22:03:06,226: t15.2024.07.19 val PER: 0.4509
2026-01-03 22:03:06,226: t15.2024.07.21 val PER: 0.2662
2026-01-03 22:03:06,227: t15.2024.07.28 val PER: 0.3044
2026-01-03 22:03:06,227: t15.2025.01.10 val PER: 0.5000
2026-01-03 22:03:06,227: t15.2025.01.12 val PER: 0.3518
2026-01-03 22:03:06,227: t15.2025.03.14 val PER: 0.4956
2026-01-03 22:03:06,227: t15.2025.03.16 val PER: 0.3599
2026-01-03 22:03:06,227: t15.2025.03.30 val PER: 0.5011
2026-01-03 22:03:06,227: t15.2025.04.13 val PER: 0.3866
2026-01-03 22:03:06,462: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_2500
2026-01-03 22:03:15,008: Train batch 2600: loss: 35.65 grad norm: 88.61 time: 0.055
2026-01-03 22:03:32,363: Train batch 2800: loss: 25.94 grad norm: 68.35 time: 0.081
2026-01-03 22:03:49,470: Train batch 3000: loss: 30.84 grad norm: 72.45 time: 0.083
2026-01-03 22:03:49,470: Running test after training batch: 3000
2026-01-03 22:03:49,565: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:03:54,821: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-03 22:03:54,851: WER debug example
  GT : how does it keep the cost down
  PR : houde des it yip the cost get
2026-01-03 22:03:56,541: Val batch 3000: PER (avg): 0.2816 CTC Loss (avg): 27.9061 WER(1gram): 67.01% (n=64) time: 7.070
2026-01-03 22:03:56,541: WER lens: avg_true_words=6.16 avg_pred_words=5.92 max_pred_words=11
2026-01-03 22:03:56,542: t15.2023.08.13 val PER: 0.2588
2026-01-03 22:03:56,542: t15.2023.08.18 val PER: 0.2213
2026-01-03 22:03:56,542: t15.2023.08.20 val PER: 0.2017
2026-01-03 22:03:56,542: t15.2023.08.25 val PER: 0.2003
2026-01-03 22:03:56,542: t15.2023.08.27 val PER: 0.3023
2026-01-03 22:03:56,542: t15.2023.09.01 val PER: 0.1875
2026-01-03 22:03:56,542: t15.2023.09.03 val PER: 0.2779
2026-01-03 22:03:56,542: t15.2023.09.24 val PER: 0.2197
2026-01-03 22:03:56,542: t15.2023.09.29 val PER: 0.2310
2026-01-03 22:03:56,542: t15.2023.10.01 val PER: 0.2933
2026-01-03 22:03:56,542: t15.2023.10.06 val PER: 0.2002
2026-01-03 22:03:56,542: t15.2023.10.08 val PER: 0.3464
2026-01-03 22:03:56,543: t15.2023.10.13 val PER: 0.3390
2026-01-03 22:03:56,543: t15.2023.10.15 val PER: 0.2637
2026-01-03 22:03:56,543: t15.2023.10.20 val PER: 0.2685
2026-01-03 22:03:56,543: t15.2023.10.22 val PER: 0.2149
2026-01-03 22:03:56,543: t15.2023.11.03 val PER: 0.2714
2026-01-03 22:03:56,543: t15.2023.11.04 val PER: 0.0785
2026-01-03 22:03:56,543: t15.2023.11.17 val PER: 0.1353
2026-01-03 22:03:56,543: t15.2023.11.19 val PER: 0.1218
2026-01-03 22:03:56,543: t15.2023.11.26 val PER: 0.3007
2026-01-03 22:03:56,543: t15.2023.12.03 val PER: 0.2511
2026-01-03 22:03:56,544: t15.2023.12.08 val PER: 0.2530
2026-01-03 22:03:56,544: t15.2023.12.10 val PER: 0.2181
2026-01-03 22:03:56,544: t15.2023.12.17 val PER: 0.2723
2026-01-03 22:03:56,544: t15.2023.12.29 val PER: 0.2793
2026-01-03 22:03:56,544: t15.2024.02.25 val PER: 0.2374
2026-01-03 22:03:56,544: t15.2024.03.08 val PER: 0.3684
2026-01-03 22:03:56,544: t15.2024.03.15 val PER: 0.3196
2026-01-03 22:03:56,544: t15.2024.03.17 val PER: 0.2887
2026-01-03 22:03:56,544: t15.2024.05.10 val PER: 0.3091
2026-01-03 22:03:56,544: t15.2024.06.14 val PER: 0.3028
2026-01-03 22:03:56,544: t15.2024.07.19 val PER: 0.4028
2026-01-03 22:03:56,544: t15.2024.07.21 val PER: 0.2352
2026-01-03 22:03:56,544: t15.2024.07.28 val PER: 0.2809
2026-01-03 22:03:56,544: t15.2025.01.10 val PER: 0.4959
2026-01-03 22:03:56,544: t15.2025.01.12 val PER: 0.3303
2026-01-03 22:03:56,544: t15.2025.03.14 val PER: 0.4571
2026-01-03 22:03:56,545: t15.2025.03.16 val PER: 0.3272
2026-01-03 22:03:56,545: t15.2025.03.30 val PER: 0.4966
2026-01-03 22:03:56,545: t15.2025.04.13 val PER: 0.3652
2026-01-03 22:03:56,546: New best val WER(1gram) 68.78% --> 67.01%
2026-01-03 22:03:56,546: Checkpointing model
2026-01-03 22:03:57,175: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:03:57,419: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_3000
2026-01-03 22:04:14,515: Train batch 3200: loss: 26.13 grad norm: 66.73 time: 0.074
2026-01-03 22:04:31,735: Train batch 3400: loss: 18.49 grad norm: 55.52 time: 0.048
2026-01-03 22:04:40,382: Running test after training batch: 3500
2026-01-03 22:04:40,540: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:04:45,315: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point will
2026-01-03 22:04:45,344: WER debug example
  GT : how does it keep the cost down
  PR : houde des it epp thus cussed get
2026-01-03 22:04:46,978: Val batch 3500: PER (avg): 0.2676 CTC Loss (avg): 26.6241 WER(1gram): 67.01% (n=64) time: 6.596
2026-01-03 22:04:46,978: WER lens: avg_true_words=6.16 avg_pred_words=5.94 max_pred_words=11
2026-01-03 22:04:46,978: t15.2023.08.13 val PER: 0.2432
2026-01-03 22:04:46,978: t15.2023.08.18 val PER: 0.2037
2026-01-03 22:04:46,979: t15.2023.08.20 val PER: 0.2168
2026-01-03 22:04:46,979: t15.2023.08.25 val PER: 0.1867
2026-01-03 22:04:46,979: t15.2023.08.27 val PER: 0.2765
2026-01-03 22:04:46,979: t15.2023.09.01 val PER: 0.1802
2026-01-03 22:04:46,979: t15.2023.09.03 val PER: 0.2660
2026-01-03 22:04:46,979: t15.2023.09.24 val PER: 0.2112
2026-01-03 22:04:46,979: t15.2023.09.29 val PER: 0.2119
2026-01-03 22:04:46,979: t15.2023.10.01 val PER: 0.2768
2026-01-03 22:04:46,979: t15.2023.10.06 val PER: 0.1938
2026-01-03 22:04:46,979: t15.2023.10.08 val PER: 0.3478
2026-01-03 22:04:46,979: t15.2023.10.13 val PER: 0.3204
2026-01-03 22:04:46,979: t15.2023.10.15 val PER: 0.2413
2026-01-03 22:04:46,979: t15.2023.10.20 val PER: 0.2315
2026-01-03 22:04:46,980: t15.2023.10.22 val PER: 0.2116
2026-01-03 22:04:46,980: t15.2023.11.03 val PER: 0.2632
2026-01-03 22:04:46,980: t15.2023.11.04 val PER: 0.0717
2026-01-03 22:04:46,980: t15.2023.11.17 val PER: 0.1151
2026-01-03 22:04:46,980: t15.2023.11.19 val PER: 0.1138
2026-01-03 22:04:46,980: t15.2023.11.26 val PER: 0.2884
2026-01-03 22:04:46,980: t15.2023.12.03 val PER: 0.2468
2026-01-03 22:04:46,980: t15.2023.12.08 val PER: 0.2483
2026-01-03 22:04:46,980: t15.2023.12.10 val PER: 0.2050
2026-01-03 22:04:46,980: t15.2023.12.17 val PER: 0.2557
2026-01-03 22:04:46,980: t15.2023.12.29 val PER: 0.2670
2026-01-03 22:04:46,980: t15.2024.02.25 val PER: 0.2135
2026-01-03 22:04:46,980: t15.2024.03.08 val PER: 0.3457
2026-01-03 22:04:46,980: t15.2024.03.15 val PER: 0.3227
2026-01-03 22:04:46,980: t15.2024.03.17 val PER: 0.2699
2026-01-03 22:04:46,980: t15.2024.05.10 val PER: 0.2704
2026-01-03 22:04:46,981: t15.2024.06.14 val PER: 0.2855
2026-01-03 22:04:46,981: t15.2024.07.19 val PER: 0.3883
2026-01-03 22:04:46,981: t15.2024.07.21 val PER: 0.2186
2026-01-03 22:04:46,981: t15.2024.07.28 val PER: 0.2765
2026-01-03 22:04:46,981: t15.2025.01.10 val PER: 0.4683
2026-01-03 22:04:46,981: t15.2025.01.12 val PER: 0.3072
2026-01-03 22:04:46,981: t15.2025.03.14 val PER: 0.4497
2026-01-03 22:04:46,981: t15.2025.03.16 val PER: 0.3089
2026-01-03 22:04:46,981: t15.2025.03.30 val PER: 0.4506
2026-01-03 22:04:46,981: t15.2025.04.13 val PER: 0.3338
2026-01-03 22:04:47,217: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_3500
2026-01-03 22:04:55,727: Train batch 3600: loss: 22.53 grad norm: 62.44 time: 0.066
2026-01-03 22:05:12,737: Train batch 3800: loss: 25.77 grad norm: 67.50 time: 0.066
2026-01-03 22:05:30,269: Train batch 4000: loss: 19.46 grad norm: 53.90 time: 0.056
2026-01-03 22:05:30,269: Running test after training batch: 4000
2026-01-03 22:05:30,374: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:05:35,140: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point will
2026-01-03 22:05:35,168: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the us et
2026-01-03 22:05:36,783: Val batch 4000: PER (avg): 0.2489 CTC Loss (avg): 24.3194 WER(1gram): 64.47% (n=64) time: 6.514
2026-01-03 22:05:36,784: WER lens: avg_true_words=6.16 avg_pred_words=5.98 max_pred_words=11
2026-01-03 22:05:36,784: t15.2023.08.13 val PER: 0.2225
2026-01-03 22:05:36,784: t15.2023.08.18 val PER: 0.2045
2026-01-03 22:05:36,784: t15.2023.08.20 val PER: 0.1978
2026-01-03 22:05:36,784: t15.2023.08.25 val PER: 0.1596
2026-01-03 22:05:36,784: t15.2023.08.27 val PER: 0.2862
2026-01-03 22:05:36,784: t15.2023.09.01 val PER: 0.1623
2026-01-03 22:05:36,784: t15.2023.09.03 val PER: 0.2553
2026-01-03 22:05:36,784: t15.2023.09.24 val PER: 0.1942
2026-01-03 22:05:36,784: t15.2023.09.29 val PER: 0.1991
2026-01-03 22:05:36,784: t15.2023.10.01 val PER: 0.2602
2026-01-03 22:05:36,784: t15.2023.10.06 val PER: 0.1572
2026-01-03 22:05:36,785: t15.2023.10.08 val PER: 0.3194
2026-01-03 22:05:36,785: t15.2023.10.13 val PER: 0.3010
2026-01-03 22:05:36,785: t15.2023.10.15 val PER: 0.2399
2026-01-03 22:05:36,785: t15.2023.10.20 val PER: 0.2349
2026-01-03 22:05:36,785: t15.2023.10.22 val PER: 0.1804
2026-01-03 22:05:36,785: t15.2023.11.03 val PER: 0.2415
2026-01-03 22:05:36,785: t15.2023.11.04 val PER: 0.0614
2026-01-03 22:05:36,785: t15.2023.11.17 val PER: 0.1120
2026-01-03 22:05:36,785: t15.2023.11.19 val PER: 0.1078
2026-01-03 22:05:36,786: t15.2023.11.26 val PER: 0.2616
2026-01-03 22:05:36,786: t15.2023.12.03 val PER: 0.2185
2026-01-03 22:05:36,786: t15.2023.12.08 val PER: 0.2217
2026-01-03 22:05:36,786: t15.2023.12.10 val PER: 0.1787
2026-01-03 22:05:36,786: t15.2023.12.17 val PER: 0.2516
2026-01-03 22:05:36,786: t15.2023.12.29 val PER: 0.2505
2026-01-03 22:05:36,786: t15.2024.02.25 val PER: 0.2163
2026-01-03 22:05:36,786: t15.2024.03.08 val PER: 0.3357
2026-01-03 22:05:36,786: t15.2024.03.15 val PER: 0.3077
2026-01-03 22:05:36,786: t15.2024.03.17 val PER: 0.2531
2026-01-03 22:05:36,786: t15.2024.05.10 val PER: 0.2689
2026-01-03 22:05:36,786: t15.2024.06.14 val PER: 0.2681
2026-01-03 22:05:36,786: t15.2024.07.19 val PER: 0.3619
2026-01-03 22:05:36,786: t15.2024.07.21 val PER: 0.1890
2026-01-03 22:05:36,786: t15.2024.07.28 val PER: 0.2338
2026-01-03 22:05:36,787: t15.2025.01.10 val PER: 0.4256
2026-01-03 22:05:36,787: t15.2025.01.12 val PER: 0.2810
2026-01-03 22:05:36,787: t15.2025.03.14 val PER: 0.4246
2026-01-03 22:05:36,787: t15.2025.03.16 val PER: 0.3076
2026-01-03 22:05:36,787: t15.2025.03.30 val PER: 0.4195
2026-01-03 22:05:36,787: t15.2025.04.13 val PER: 0.3153
2026-01-03 22:05:36,788: New best val WER(1gram) 67.01% --> 64.47%
2026-01-03 22:05:36,788: Checkpointing model
2026-01-03 22:05:37,376: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:05:37,621: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_4000
2026-01-03 22:05:54,982: Train batch 4200: loss: 22.06 grad norm: 66.15 time: 0.080
2026-01-03 22:06:12,232: Train batch 4400: loss: 16.87 grad norm: 59.96 time: 0.065
2026-01-03 22:06:20,969: Running test after training batch: 4500
2026-01-03 22:06:21,086: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:06:26,094: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-03 22:06:26,123: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it yip the cost get
2026-01-03 22:06:27,697: Val batch 4500: PER (avg): 0.2380 CTC Loss (avg): 23.1783 WER(1gram): 62.18% (n=64) time: 6.728
2026-01-03 22:06:27,697: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 22:06:27,697: t15.2023.08.13 val PER: 0.2089
2026-01-03 22:06:27,698: t15.2023.08.18 val PER: 0.1894
2026-01-03 22:06:27,698: t15.2023.08.20 val PER: 0.1970
2026-01-03 22:06:27,698: t15.2023.08.25 val PER: 0.1476
2026-01-03 22:06:27,698: t15.2023.08.27 val PER: 0.2444
2026-01-03 22:06:27,698: t15.2023.09.01 val PER: 0.1558
2026-01-03 22:06:27,698: t15.2023.09.03 val PER: 0.2423
2026-01-03 22:06:27,698: t15.2023.09.24 val PER: 0.1772
2026-01-03 22:06:27,698: t15.2023.09.29 val PER: 0.1997
2026-01-03 22:06:27,698: t15.2023.10.01 val PER: 0.2550
2026-01-03 22:06:27,698: t15.2023.10.06 val PER: 0.1529
2026-01-03 22:06:27,698: t15.2023.10.08 val PER: 0.3018
2026-01-03 22:06:27,698: t15.2023.10.13 val PER: 0.3072
2026-01-03 22:06:27,698: t15.2023.10.15 val PER: 0.2367
2026-01-03 22:06:27,699: t15.2023.10.20 val PER: 0.2450
2026-01-03 22:06:27,699: t15.2023.10.22 val PER: 0.1826
2026-01-03 22:06:27,699: t15.2023.11.03 val PER: 0.2300
2026-01-03 22:06:27,699: t15.2023.11.04 val PER: 0.0614
2026-01-03 22:06:27,699: t15.2023.11.17 val PER: 0.1026
2026-01-03 22:06:27,699: t15.2023.11.19 val PER: 0.0918
2026-01-03 22:06:27,699: t15.2023.11.26 val PER: 0.2630
2026-01-03 22:06:27,699: t15.2023.12.03 val PER: 0.2006
2026-01-03 22:06:27,699: t15.2023.12.08 val PER: 0.2071
2026-01-03 22:06:27,699: t15.2023.12.10 val PER: 0.1827
2026-01-03 22:06:27,699: t15.2023.12.17 val PER: 0.2287
2026-01-03 22:06:27,699: t15.2023.12.29 val PER: 0.2443
2026-01-03 22:06:27,699: t15.2024.02.25 val PER: 0.2008
2026-01-03 22:06:27,699: t15.2024.03.08 val PER: 0.3172
2026-01-03 22:06:27,699: t15.2024.03.15 val PER: 0.2839
2026-01-03 22:06:27,699: t15.2024.03.17 val PER: 0.2378
2026-01-03 22:06:27,700: t15.2024.05.10 val PER: 0.2585
2026-01-03 22:06:27,700: t15.2024.06.14 val PER: 0.2445
2026-01-03 22:06:27,700: t15.2024.07.19 val PER: 0.3395
2026-01-03 22:06:27,700: t15.2024.07.21 val PER: 0.1731
2026-01-03 22:06:27,700: t15.2024.07.28 val PER: 0.2243
2026-01-03 22:06:27,700: t15.2025.01.10 val PER: 0.4174
2026-01-03 22:06:27,700: t15.2025.01.12 val PER: 0.2679
2026-01-03 22:06:27,700: t15.2025.03.14 val PER: 0.4098
2026-01-03 22:06:27,700: t15.2025.03.16 val PER: 0.2997
2026-01-03 22:06:27,700: t15.2025.03.30 val PER: 0.4080
2026-01-03 22:06:27,700: t15.2025.04.13 val PER: 0.2939
2026-01-03 22:06:27,702: New best val WER(1gram) 64.47% --> 62.18%
2026-01-03 22:06:27,702: Checkpointing model
2026-01-03 22:06:28,301: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:06:28,546: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_4500
2026-01-03 22:06:37,048: Train batch 4600: loss: 20.21 grad norm: 70.55 time: 0.061
2026-01-03 22:06:54,304: Train batch 4800: loss: 13.57 grad norm: 49.99 time: 0.063
2026-01-03 22:07:11,506: Train batch 5000: loss: 32.35 grad norm: 79.70 time: 0.063
2026-01-03 22:07:11,506: Running test after training batch: 5000
2026-01-03 22:07:11,632: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:07:16,373: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point is will
2026-01-03 22:07:16,403: WER debug example
  GT : how does it keep the cost down
  PR : houde just it heap the cost get
2026-01-03 22:07:18,114: Val batch 5000: PER (avg): 0.2257 CTC Loss (avg): 22.1107 WER(1gram): 60.15% (n=64) time: 6.607
2026-01-03 22:07:18,114: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 22:07:18,114: t15.2023.08.13 val PER: 0.1975
2026-01-03 22:07:18,114: t15.2023.08.18 val PER: 0.1744
2026-01-03 22:07:18,114: t15.2023.08.20 val PER: 0.1827
2026-01-03 22:07:18,115: t15.2023.08.25 val PER: 0.1416
2026-01-03 22:07:18,115: t15.2023.08.27 val PER: 0.2476
2026-01-03 22:07:18,115: t15.2023.09.01 val PER: 0.1445
2026-01-03 22:07:18,115: t15.2023.09.03 val PER: 0.2352
2026-01-03 22:07:18,115: t15.2023.09.24 val PER: 0.1833
2026-01-03 22:07:18,115: t15.2023.09.29 val PER: 0.1838
2026-01-03 22:07:18,115: t15.2023.10.01 val PER: 0.2332
2026-01-03 22:07:18,115: t15.2023.10.06 val PER: 0.1485
2026-01-03 22:07:18,115: t15.2023.10.08 val PER: 0.3099
2026-01-03 22:07:18,116: t15.2023.10.13 val PER: 0.2669
2026-01-03 22:07:18,116: t15.2023.10.15 val PER: 0.2169
2026-01-03 22:07:18,116: t15.2023.10.20 val PER: 0.2315
2026-01-03 22:07:18,116: t15.2023.10.22 val PER: 0.1693
2026-01-03 22:07:18,116: t15.2023.11.03 val PER: 0.2246
2026-01-03 22:07:18,116: t15.2023.11.04 val PER: 0.0614
2026-01-03 22:07:18,116: t15.2023.11.17 val PER: 0.0871
2026-01-03 22:07:18,116: t15.2023.11.19 val PER: 0.0778
2026-01-03 22:07:18,116: t15.2023.11.26 val PER: 0.2457
2026-01-03 22:07:18,116: t15.2023.12.03 val PER: 0.2017
2026-01-03 22:07:18,116: t15.2023.12.08 val PER: 0.1964
2026-01-03 22:07:18,117: t15.2023.12.10 val PER: 0.1603
2026-01-03 22:07:18,117: t15.2023.12.17 val PER: 0.2266
2026-01-03 22:07:18,117: t15.2023.12.29 val PER: 0.2224
2026-01-03 22:07:18,117: t15.2024.02.25 val PER: 0.1910
2026-01-03 22:07:18,117: t15.2024.03.08 val PER: 0.3087
2026-01-03 22:07:18,117: t15.2024.03.15 val PER: 0.2758
2026-01-03 22:07:18,117: t15.2024.03.17 val PER: 0.2322
2026-01-03 22:07:18,117: t15.2024.05.10 val PER: 0.2496
2026-01-03 22:07:18,117: t15.2024.06.14 val PER: 0.2476
2026-01-03 22:07:18,117: t15.2024.07.19 val PER: 0.3263
2026-01-03 22:07:18,117: t15.2024.07.21 val PER: 0.1800
2026-01-03 22:07:18,117: t15.2024.07.28 val PER: 0.2118
2026-01-03 22:07:18,118: t15.2025.01.10 val PER: 0.3774
2026-01-03 22:07:18,118: t15.2025.01.12 val PER: 0.2517
2026-01-03 22:07:18,118: t15.2025.03.14 val PER: 0.3861
2026-01-03 22:07:18,118: t15.2025.03.16 val PER: 0.2657
2026-01-03 22:07:18,118: t15.2025.03.30 val PER: 0.3885
2026-01-03 22:07:18,118: t15.2025.04.13 val PER: 0.2924
2026-01-03 22:07:18,119: New best val WER(1gram) 62.18% --> 60.15%
2026-01-03 22:07:18,119: Checkpointing model
2026-01-03 22:07:18,707: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:07:18,950: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_5000
2026-01-03 22:07:36,033: Train batch 5200: loss: 16.49 grad norm: 59.80 time: 0.051
2026-01-03 22:07:53,099: Train batch 5400: loss: 17.52 grad norm: 59.71 time: 0.068
2026-01-03 22:08:01,678: Running test after training batch: 5500
2026-01-03 22:08:01,777: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:08:06,734: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-03 22:08:06,763: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost it
2026-01-03 22:08:08,382: Val batch 5500: PER (avg): 0.2167 CTC Loss (avg): 21.0423 WER(1gram): 55.84% (n=64) time: 6.704
2026-01-03 22:08:08,383: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 22:08:08,383: t15.2023.08.13 val PER: 0.1746
2026-01-03 22:08:08,383: t15.2023.08.18 val PER: 0.1559
2026-01-03 22:08:08,383: t15.2023.08.20 val PER: 0.1668
2026-01-03 22:08:08,383: t15.2023.08.25 val PER: 0.1295
2026-01-03 22:08:08,383: t15.2023.08.27 val PER: 0.2412
2026-01-03 22:08:08,383: t15.2023.09.01 val PER: 0.1291
2026-01-03 22:08:08,383: t15.2023.09.03 val PER: 0.2268
2026-01-03 22:08:08,383: t15.2023.09.24 val PER: 0.1735
2026-01-03 22:08:08,384: t15.2023.09.29 val PER: 0.1761
2026-01-03 22:08:08,384: t15.2023.10.01 val PER: 0.2285
2026-01-03 22:08:08,384: t15.2023.10.06 val PER: 0.1367
2026-01-03 22:08:08,384: t15.2023.10.08 val PER: 0.2909
2026-01-03 22:08:08,384: t15.2023.10.13 val PER: 0.2801
2026-01-03 22:08:08,384: t15.2023.10.15 val PER: 0.2156
2026-01-03 22:08:08,384: t15.2023.10.20 val PER: 0.2416
2026-01-03 22:08:08,384: t15.2023.10.22 val PER: 0.1682
2026-01-03 22:08:08,384: t15.2023.11.03 val PER: 0.2239
2026-01-03 22:08:08,384: t15.2023.11.04 val PER: 0.0683
2026-01-03 22:08:08,384: t15.2023.11.17 val PER: 0.0824
2026-01-03 22:08:08,384: t15.2023.11.19 val PER: 0.0739
2026-01-03 22:08:08,384: t15.2023.11.26 val PER: 0.2239
2026-01-03 22:08:08,384: t15.2023.12.03 val PER: 0.1891
2026-01-03 22:08:08,385: t15.2023.12.08 val PER: 0.1877
2026-01-03 22:08:08,385: t15.2023.12.10 val PER: 0.1695
2026-01-03 22:08:08,385: t15.2023.12.17 val PER: 0.2173
2026-01-03 22:08:08,385: t15.2023.12.29 val PER: 0.2148
2026-01-03 22:08:08,385: t15.2024.02.25 val PER: 0.1784
2026-01-03 22:08:08,385: t15.2024.03.08 val PER: 0.2859
2026-01-03 22:08:08,385: t15.2024.03.15 val PER: 0.2589
2026-01-03 22:08:08,385: t15.2024.03.17 val PER: 0.2162
2026-01-03 22:08:08,385: t15.2024.05.10 val PER: 0.2303
2026-01-03 22:08:08,385: t15.2024.06.14 val PER: 0.2382
2026-01-03 22:08:08,385: t15.2024.07.19 val PER: 0.3177
2026-01-03 22:08:08,386: t15.2024.07.21 val PER: 0.1683
2026-01-03 22:08:08,386: t15.2024.07.28 val PER: 0.2176
2026-01-03 22:08:08,386: t15.2025.01.10 val PER: 0.3939
2026-01-03 22:08:08,386: t15.2025.01.12 val PER: 0.2317
2026-01-03 22:08:08,386: t15.2025.03.14 val PER: 0.3713
2026-01-03 22:08:08,386: t15.2025.03.16 val PER: 0.2592
2026-01-03 22:08:08,386: t15.2025.03.30 val PER: 0.3632
2026-01-03 22:08:08,386: t15.2025.04.13 val PER: 0.2967
2026-01-03 22:08:08,387: New best val WER(1gram) 60.15% --> 55.84%
2026-01-03 22:08:08,387: Checkpointing model
2026-01-03 22:08:08,999: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:08:09,241: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_5500
2026-01-03 22:08:17,914: Train batch 5600: loss: 19.68 grad norm: 62.81 time: 0.061
2026-01-03 22:08:35,243: Train batch 5800: loss: 13.68 grad norm: 57.00 time: 0.081
2026-01-03 22:08:52,501: Train batch 6000: loss: 14.55 grad norm: 56.60 time: 0.048
2026-01-03 22:08:52,501: Running test after training batch: 6000
2026-01-03 22:08:52,612: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:08:57,344: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 22:08:57,376: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost tit
2026-01-03 22:08:59,062: Val batch 6000: PER (avg): 0.2144 CTC Loss (avg): 20.9284 WER(1gram): 57.11% (n=64) time: 6.561
2026-01-03 22:08:59,063: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 22:08:59,063: t15.2023.08.13 val PER: 0.1861
2026-01-03 22:08:59,063: t15.2023.08.18 val PER: 0.1718
2026-01-03 22:08:59,063: t15.2023.08.20 val PER: 0.1763
2026-01-03 22:08:59,063: t15.2023.08.25 val PER: 0.1250
2026-01-03 22:08:59,063: t15.2023.08.27 val PER: 0.2444
2026-01-03 22:08:59,063: t15.2023.09.01 val PER: 0.1339
2026-01-03 22:08:59,063: t15.2023.09.03 val PER: 0.2126
2026-01-03 22:08:59,063: t15.2023.09.24 val PER: 0.1760
2026-01-03 22:08:59,064: t15.2023.09.29 val PER: 0.1634
2026-01-03 22:08:59,064: t15.2023.10.01 val PER: 0.2199
2026-01-03 22:08:59,064: t15.2023.10.06 val PER: 0.1389
2026-01-03 22:08:59,064: t15.2023.10.08 val PER: 0.2909
2026-01-03 22:08:59,064: t15.2023.10.13 val PER: 0.2700
2026-01-03 22:08:59,064: t15.2023.10.15 val PER: 0.2208
2026-01-03 22:08:59,064: t15.2023.10.20 val PER: 0.2148
2026-01-03 22:08:59,064: t15.2023.10.22 val PER: 0.1659
2026-01-03 22:08:59,064: t15.2023.11.03 val PER: 0.2259
2026-01-03 22:08:59,064: t15.2023.11.04 val PER: 0.0614
2026-01-03 22:08:59,064: t15.2023.11.17 val PER: 0.0824
2026-01-03 22:08:59,064: t15.2023.11.19 val PER: 0.0818
2026-01-03 22:08:59,064: t15.2023.11.26 val PER: 0.2203
2026-01-03 22:08:59,064: t15.2023.12.03 val PER: 0.1691
2026-01-03 22:08:59,065: t15.2023.12.08 val PER: 0.1838
2026-01-03 22:08:59,065: t15.2023.12.10 val PER: 0.1564
2026-01-03 22:08:59,065: t15.2023.12.17 val PER: 0.2110
2026-01-03 22:08:59,065: t15.2023.12.29 val PER: 0.2279
2026-01-03 22:08:59,065: t15.2024.02.25 val PER: 0.1685
2026-01-03 22:08:59,065: t15.2024.03.08 val PER: 0.3001
2026-01-03 22:08:59,065: t15.2024.03.15 val PER: 0.2583
2026-01-03 22:08:59,065: t15.2024.03.17 val PER: 0.2127
2026-01-03 22:08:59,065: t15.2024.05.10 val PER: 0.2199
2026-01-03 22:08:59,065: t15.2024.06.14 val PER: 0.2224
2026-01-03 22:08:59,065: t15.2024.07.19 val PER: 0.3092
2026-01-03 22:08:59,065: t15.2024.07.21 val PER: 0.1634
2026-01-03 22:08:59,065: t15.2024.07.28 val PER: 0.2118
2026-01-03 22:08:59,065: t15.2025.01.10 val PER: 0.3843
2026-01-03 22:08:59,065: t15.2025.01.12 val PER: 0.2340
2026-01-03 22:08:59,065: t15.2025.03.14 val PER: 0.3802
2026-01-03 22:08:59,066: t15.2025.03.16 val PER: 0.2552
2026-01-03 22:08:59,066: t15.2025.03.30 val PER: 0.3713
2026-01-03 22:08:59,066: t15.2025.04.13 val PER: 0.2653
2026-01-03 22:08:59,300: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_6000
2026-01-03 22:09:16,264: Train batch 6200: loss: 16.66 grad norm: 59.05 time: 0.069
2026-01-03 22:09:33,359: Train batch 6400: loss: 18.29 grad norm: 64.72 time: 0.062
2026-01-03 22:09:41,767: Running test after training batch: 6500
2026-01-03 22:09:41,897: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:09:46,621: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sze the could at this point is will
2026-01-03 22:09:46,651: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost sette
2026-01-03 22:09:48,288: Val batch 6500: PER (avg): 0.2041 CTC Loss (avg): 20.0970 WER(1gram): 54.06% (n=64) time: 6.520
2026-01-03 22:09:48,288: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 22:09:48,289: t15.2023.08.13 val PER: 0.1757
2026-01-03 22:09:48,289: t15.2023.08.18 val PER: 0.1375
2026-01-03 22:09:48,289: t15.2023.08.20 val PER: 0.1604
2026-01-03 22:09:48,289: t15.2023.08.25 val PER: 0.1175
2026-01-03 22:09:48,289: t15.2023.08.27 val PER: 0.2283
2026-01-03 22:09:48,289: t15.2023.09.01 val PER: 0.1201
2026-01-03 22:09:48,289: t15.2023.09.03 val PER: 0.2067
2026-01-03 22:09:48,289: t15.2023.09.24 val PER: 0.1626
2026-01-03 22:09:48,289: t15.2023.09.29 val PER: 0.1659
2026-01-03 22:09:48,289: t15.2023.10.01 val PER: 0.2153
2026-01-03 22:09:48,289: t15.2023.10.06 val PER: 0.1206
2026-01-03 22:09:48,289: t15.2023.10.08 val PER: 0.3031
2026-01-03 22:09:48,290: t15.2023.10.13 val PER: 0.2723
2026-01-03 22:09:48,290: t15.2023.10.15 val PER: 0.2116
2026-01-03 22:09:48,290: t15.2023.10.20 val PER: 0.2148
2026-01-03 22:09:48,290: t15.2023.10.22 val PER: 0.1592
2026-01-03 22:09:48,290: t15.2023.11.03 val PER: 0.2205
2026-01-03 22:09:48,290: t15.2023.11.04 val PER: 0.0614
2026-01-03 22:09:48,290: t15.2023.11.17 val PER: 0.0638
2026-01-03 22:09:48,290: t15.2023.11.19 val PER: 0.0659
2026-01-03 22:09:48,290: t15.2023.11.26 val PER: 0.2123
2026-01-03 22:09:48,290: t15.2023.12.03 val PER: 0.1639
2026-01-03 22:09:48,290: t15.2023.12.08 val PER: 0.1638
2026-01-03 22:09:48,290: t15.2023.12.10 val PER: 0.1353
2026-01-03 22:09:48,290: t15.2023.12.17 val PER: 0.2048
2026-01-03 22:09:48,290: t15.2023.12.29 val PER: 0.2011
2026-01-03 22:09:48,290: t15.2024.02.25 val PER: 0.1685
2026-01-03 22:09:48,290: t15.2024.03.08 val PER: 0.2916
2026-01-03 22:09:48,291: t15.2024.03.15 val PER: 0.2570
2026-01-03 22:09:48,291: t15.2024.03.17 val PER: 0.2015
2026-01-03 22:09:48,291: t15.2024.05.10 val PER: 0.2259
2026-01-03 22:09:48,291: t15.2024.06.14 val PER: 0.2035
2026-01-03 22:09:48,291: t15.2024.07.19 val PER: 0.3026
2026-01-03 22:09:48,291: t15.2024.07.21 val PER: 0.1586
2026-01-03 22:09:48,291: t15.2024.07.28 val PER: 0.1912
2026-01-03 22:09:48,291: t15.2025.01.10 val PER: 0.3747
2026-01-03 22:09:48,291: t15.2025.01.12 val PER: 0.2140
2026-01-03 22:09:48,291: t15.2025.03.14 val PER: 0.3743
2026-01-03 22:09:48,291: t15.2025.03.16 val PER: 0.2382
2026-01-03 22:09:48,291: t15.2025.03.30 val PER: 0.3529
2026-01-03 22:09:48,291: t15.2025.04.13 val PER: 0.2611
2026-01-03 22:09:48,293: New best val WER(1gram) 55.84% --> 54.06%
2026-01-03 22:09:48,293: Checkpointing model
2026-01-03 22:09:48,915: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:09:49,157: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_6500
2026-01-03 22:09:57,534: Train batch 6600: loss: 12.41 grad norm: 58.18 time: 0.044
2026-01-03 22:10:14,712: Train batch 6800: loss: 15.57 grad norm: 59.22 time: 0.048
2026-01-03 22:10:32,101: Train batch 7000: loss: 17.20 grad norm: 62.24 time: 0.060
2026-01-03 22:10:32,101: Running test after training batch: 7000
2026-01-03 22:10:32,252: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:10:37,021: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point as will
2026-01-03 22:10:37,051: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-03 22:10:38,768: Val batch 7000: PER (avg): 0.1962 CTC Loss (avg): 19.2609 WER(1gram): 54.06% (n=64) time: 6.666
2026-01-03 22:10:38,768: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 22:10:38,768: t15.2023.08.13 val PER: 0.1632
2026-01-03 22:10:38,768: t15.2023.08.18 val PER: 0.1459
2026-01-03 22:10:38,768: t15.2023.08.20 val PER: 0.1557
2026-01-03 22:10:38,768: t15.2023.08.25 val PER: 0.0979
2026-01-03 22:10:38,769: t15.2023.08.27 val PER: 0.2267
2026-01-03 22:10:38,769: t15.2023.09.01 val PER: 0.1177
2026-01-03 22:10:38,769: t15.2023.09.03 val PER: 0.1960
2026-01-03 22:10:38,769: t15.2023.09.24 val PER: 0.1541
2026-01-03 22:10:38,769: t15.2023.09.29 val PER: 0.1627
2026-01-03 22:10:38,769: t15.2023.10.01 val PER: 0.2100
2026-01-03 22:10:38,769: t15.2023.10.06 val PER: 0.1163
2026-01-03 22:10:38,769: t15.2023.10.08 val PER: 0.2909
2026-01-03 22:10:38,769: t15.2023.10.13 val PER: 0.2529
2026-01-03 22:10:38,769: t15.2023.10.15 val PER: 0.1931
2026-01-03 22:10:38,770: t15.2023.10.20 val PER: 0.2047
2026-01-03 22:10:38,770: t15.2023.10.22 val PER: 0.1448
2026-01-03 22:10:38,770: t15.2023.11.03 val PER: 0.1995
2026-01-03 22:10:38,770: t15.2023.11.04 val PER: 0.0444
2026-01-03 22:10:38,770: t15.2023.11.17 val PER: 0.0669
2026-01-03 22:10:38,770: t15.2023.11.19 val PER: 0.0539
2026-01-03 22:10:38,770: t15.2023.11.26 val PER: 0.1993
2026-01-03 22:10:38,770: t15.2023.12.03 val PER: 0.1628
2026-01-03 22:10:38,770: t15.2023.12.08 val PER: 0.1571
2026-01-03 22:10:38,770: t15.2023.12.10 val PER: 0.1511
2026-01-03 22:10:38,770: t15.2023.12.17 val PER: 0.1809
2026-01-03 22:10:38,770: t15.2023.12.29 val PER: 0.1935
2026-01-03 22:10:38,770: t15.2024.02.25 val PER: 0.1699
2026-01-03 22:10:38,770: t15.2024.03.08 val PER: 0.2817
2026-01-03 22:10:38,771: t15.2024.03.15 val PER: 0.2483
2026-01-03 22:10:38,771: t15.2024.03.17 val PER: 0.1967
2026-01-03 22:10:38,771: t15.2024.05.10 val PER: 0.2199
2026-01-03 22:10:38,771: t15.2024.06.14 val PER: 0.2114
2026-01-03 22:10:38,771: t15.2024.07.19 val PER: 0.2960
2026-01-03 22:10:38,771: t15.2024.07.21 val PER: 0.1310
2026-01-03 22:10:38,771: t15.2024.07.28 val PER: 0.1824
2026-01-03 22:10:38,771: t15.2025.01.10 val PER: 0.3760
2026-01-03 22:10:38,771: t15.2025.01.12 val PER: 0.2063
2026-01-03 22:10:38,771: t15.2025.03.14 val PER: 0.3432
2026-01-03 22:10:38,771: t15.2025.03.16 val PER: 0.2408
2026-01-03 22:10:38,771: t15.2025.03.30 val PER: 0.3690
2026-01-03 22:10:38,771: t15.2025.04.13 val PER: 0.2639
2026-01-03 22:10:39,005: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_7000
2026-01-03 22:10:55,919: Train batch 7200: loss: 14.50 grad norm: 55.20 time: 0.078
2026-01-03 22:11:12,877: Train batch 7400: loss: 14.19 grad norm: 56.91 time: 0.075
2026-01-03 22:11:21,393: Running test after training batch: 7500
2026-01-03 22:11:21,487: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:11:26,276: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point is will
2026-01-03 22:11:26,307: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost it
2026-01-03 22:11:28,040: Val batch 7500: PER (avg): 0.1919 CTC Loss (avg): 18.7374 WER(1gram): 54.06% (n=64) time: 6.647
2026-01-03 22:11:28,041: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 22:11:28,041: t15.2023.08.13 val PER: 0.1507
2026-01-03 22:11:28,041: t15.2023.08.18 val PER: 0.1324
2026-01-03 22:11:28,041: t15.2023.08.20 val PER: 0.1485
2026-01-03 22:11:28,041: t15.2023.08.25 val PER: 0.1099
2026-01-03 22:11:28,041: t15.2023.08.27 val PER: 0.2138
2026-01-03 22:11:28,042: t15.2023.09.01 val PER: 0.1128
2026-01-03 22:11:28,042: t15.2023.09.03 val PER: 0.1936
2026-01-03 22:11:28,042: t15.2023.09.24 val PER: 0.1553
2026-01-03 22:11:28,042: t15.2023.09.29 val PER: 0.1544
2026-01-03 22:11:28,042: t15.2023.10.01 val PER: 0.2054
2026-01-03 22:11:28,042: t15.2023.10.06 val PER: 0.1152
2026-01-03 22:11:28,042: t15.2023.10.08 val PER: 0.2720
2026-01-03 22:11:28,042: t15.2023.10.13 val PER: 0.2560
2026-01-03 22:11:28,042: t15.2023.10.15 val PER: 0.1971
2026-01-03 22:11:28,042: t15.2023.10.20 val PER: 0.1980
2026-01-03 22:11:28,042: t15.2023.10.22 val PER: 0.1403
2026-01-03 22:11:28,042: t15.2023.11.03 val PER: 0.2069
2026-01-03 22:11:28,042: t15.2023.11.04 val PER: 0.0512
2026-01-03 22:11:28,042: t15.2023.11.17 val PER: 0.0653
2026-01-03 22:11:28,042: t15.2023.11.19 val PER: 0.0519
2026-01-03 22:11:28,043: t15.2023.11.26 val PER: 0.1884
2026-01-03 22:11:28,043: t15.2023.12.03 val PER: 0.1534
2026-01-03 22:11:28,043: t15.2023.12.08 val PER: 0.1498
2026-01-03 22:11:28,043: t15.2023.12.10 val PER: 0.1393
2026-01-03 22:11:28,043: t15.2023.12.17 val PER: 0.1788
2026-01-03 22:11:28,043: t15.2023.12.29 val PER: 0.1826
2026-01-03 22:11:28,043: t15.2024.02.25 val PER: 0.1531
2026-01-03 22:11:28,043: t15.2024.03.08 val PER: 0.2859
2026-01-03 22:11:28,043: t15.2024.03.15 val PER: 0.2408
2026-01-03 22:11:28,043: t15.2024.03.17 val PER: 0.1918
2026-01-03 22:11:28,043: t15.2024.05.10 val PER: 0.2140
2026-01-03 22:11:28,044: t15.2024.06.14 val PER: 0.1956
2026-01-03 22:11:28,044: t15.2024.07.19 val PER: 0.2940
2026-01-03 22:11:28,044: t15.2024.07.21 val PER: 0.1359
2026-01-03 22:11:28,044: t15.2024.07.28 val PER: 0.1757
2026-01-03 22:11:28,044: t15.2025.01.10 val PER: 0.3554
2026-01-03 22:11:28,044: t15.2025.01.12 val PER: 0.2017
2026-01-03 22:11:28,044: t15.2025.03.14 val PER: 0.3787
2026-01-03 22:11:28,044: t15.2025.03.16 val PER: 0.2526
2026-01-03 22:11:28,044: t15.2025.03.30 val PER: 0.3540
2026-01-03 22:11:28,045: t15.2025.04.13 val PER: 0.2582
2026-01-03 22:11:28,277: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_7500
2026-01-03 22:11:36,845: Train batch 7600: loss: 16.38 grad norm: 62.98 time: 0.068
2026-01-03 22:11:54,029: Train batch 7800: loss: 15.18 grad norm: 61.94 time: 0.056
2026-01-03 22:12:11,529: Train batch 8000: loss: 11.35 grad norm: 52.74 time: 0.071
2026-01-03 22:12:11,529: Running test after training batch: 8000
2026-01-03 22:12:11,635: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:12:16,365: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point is will
2026-01-03 22:12:16,397: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost it
2026-01-03 22:12:18,145: Val batch 8000: PER (avg): 0.1862 CTC Loss (avg): 18.1831 WER(1gram): 53.55% (n=64) time: 6.616
2026-01-03 22:12:18,146: WER lens: avg_true_words=6.16 avg_pred_words=6.11 max_pred_words=11
2026-01-03 22:12:18,146: t15.2023.08.13 val PER: 0.1435
2026-01-03 22:12:18,146: t15.2023.08.18 val PER: 0.1350
2026-01-03 22:12:18,146: t15.2023.08.20 val PER: 0.1469
2026-01-03 22:12:18,146: t15.2023.08.25 val PER: 0.1084
2026-01-03 22:12:18,146: t15.2023.08.27 val PER: 0.2170
2026-01-03 22:12:18,146: t15.2023.09.01 val PER: 0.1104
2026-01-03 22:12:18,146: t15.2023.09.03 val PER: 0.1865
2026-01-03 22:12:18,146: t15.2023.09.24 val PER: 0.1517
2026-01-03 22:12:18,146: t15.2023.09.29 val PER: 0.1538
2026-01-03 22:12:18,147: t15.2023.10.01 val PER: 0.2028
2026-01-03 22:12:18,147: t15.2023.10.06 val PER: 0.1130
2026-01-03 22:12:18,147: t15.2023.10.08 val PER: 0.2720
2026-01-03 22:12:18,147: t15.2023.10.13 val PER: 0.2498
2026-01-03 22:12:18,147: t15.2023.10.15 val PER: 0.1931
2026-01-03 22:12:18,147: t15.2023.10.20 val PER: 0.1779
2026-01-03 22:12:18,147: t15.2023.10.22 val PER: 0.1381
2026-01-03 22:12:18,147: t15.2023.11.03 val PER: 0.2042
2026-01-03 22:12:18,147: t15.2023.11.04 val PER: 0.0375
2026-01-03 22:12:18,147: t15.2023.11.17 val PER: 0.0607
2026-01-03 22:12:18,147: t15.2023.11.19 val PER: 0.0659
2026-01-03 22:12:18,147: t15.2023.11.26 val PER: 0.1862
2026-01-03 22:12:18,147: t15.2023.12.03 val PER: 0.1492
2026-01-03 22:12:18,148: t15.2023.12.08 val PER: 0.1498
2026-01-03 22:12:18,148: t15.2023.12.10 val PER: 0.1380
2026-01-03 22:12:18,148: t15.2023.12.17 val PER: 0.1715
2026-01-03 22:12:18,148: t15.2023.12.29 val PER: 0.1874
2026-01-03 22:12:18,148: t15.2024.02.25 val PER: 0.1489
2026-01-03 22:12:18,148: t15.2024.03.08 val PER: 0.2760
2026-01-03 22:12:18,148: t15.2024.03.15 val PER: 0.2345
2026-01-03 22:12:18,148: t15.2024.03.17 val PER: 0.1813
2026-01-03 22:12:18,148: t15.2024.05.10 val PER: 0.1947
2026-01-03 22:12:18,148: t15.2024.06.14 val PER: 0.2019
2026-01-03 22:12:18,148: t15.2024.07.19 val PER: 0.2848
2026-01-03 22:12:18,148: t15.2024.07.21 val PER: 0.1179
2026-01-03 22:12:18,148: t15.2024.07.28 val PER: 0.1566
2026-01-03 22:12:18,148: t15.2025.01.10 val PER: 0.3347
2026-01-03 22:12:18,148: t15.2025.01.12 val PER: 0.1932
2026-01-03 22:12:18,148: t15.2025.03.14 val PER: 0.3476
2026-01-03 22:12:18,149: t15.2025.03.16 val PER: 0.2251
2026-01-03 22:12:18,149: t15.2025.03.30 val PER: 0.3494
2026-01-03 22:12:18,149: t15.2025.04.13 val PER: 0.2668
2026-01-03 22:12:18,150: New best val WER(1gram) 54.06% --> 53.55%
2026-01-03 22:12:18,150: Checkpointing model
2026-01-03 22:12:18,776: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:12:19,021: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_8000
2026-01-03 22:12:36,340: Train batch 8200: loss: 9.99 grad norm: 51.04 time: 0.053
2026-01-03 22:12:53,746: Train batch 8400: loss: 9.95 grad norm: 45.62 time: 0.062
2026-01-03 22:13:02,588: Running test after training batch: 8500
2026-01-03 22:13:02,699: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:13:07,443: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 22:13:07,473: WER debug example
  GT : how does it keep the cost down
  PR : houde us it keep the cost et
2026-01-03 22:13:09,201: Val batch 8500: PER (avg): 0.1804 CTC Loss (avg): 17.7351 WER(1gram): 50.00% (n=64) time: 6.613
2026-01-03 22:13:09,202: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 22:13:09,202: t15.2023.08.13 val PER: 0.1414
2026-01-03 22:13:09,202: t15.2023.08.18 val PER: 0.1383
2026-01-03 22:13:09,202: t15.2023.08.20 val PER: 0.1358
2026-01-03 22:13:09,202: t15.2023.08.25 val PER: 0.1160
2026-01-03 22:13:09,202: t15.2023.08.27 val PER: 0.2090
2026-01-03 22:13:09,202: t15.2023.09.01 val PER: 0.1031
2026-01-03 22:13:09,202: t15.2023.09.03 val PER: 0.1971
2026-01-03 22:13:09,202: t15.2023.09.24 val PER: 0.1602
2026-01-03 22:13:09,203: t15.2023.09.29 val PER: 0.1487
2026-01-03 22:13:09,203: t15.2023.10.01 val PER: 0.1922
2026-01-03 22:13:09,203: t15.2023.10.06 val PER: 0.1023
2026-01-03 22:13:09,203: t15.2023.10.08 val PER: 0.2530
2026-01-03 22:13:09,203: t15.2023.10.13 val PER: 0.2389
2026-01-03 22:13:09,203: t15.2023.10.15 val PER: 0.1852
2026-01-03 22:13:09,203: t15.2023.10.20 val PER: 0.1980
2026-01-03 22:13:09,203: t15.2023.10.22 val PER: 0.1448
2026-01-03 22:13:09,203: t15.2023.11.03 val PER: 0.1940
2026-01-03 22:13:09,203: t15.2023.11.04 val PER: 0.0512
2026-01-03 22:13:09,203: t15.2023.11.17 val PER: 0.0622
2026-01-03 22:13:09,203: t15.2023.11.19 val PER: 0.0439
2026-01-03 22:13:09,204: t15.2023.11.26 val PER: 0.1797
2026-01-03 22:13:09,204: t15.2023.12.03 val PER: 0.1376
2026-01-03 22:13:09,204: t15.2023.12.08 val PER: 0.1398
2026-01-03 22:13:09,204: t15.2023.12.10 val PER: 0.1196
2026-01-03 22:13:09,204: t15.2023.12.17 val PER: 0.1674
2026-01-03 22:13:09,204: t15.2023.12.29 val PER: 0.1640
2026-01-03 22:13:09,204: t15.2024.02.25 val PER: 0.1362
2026-01-03 22:13:09,204: t15.2024.03.08 val PER: 0.2646
2026-01-03 22:13:09,204: t15.2024.03.15 val PER: 0.2245
2026-01-03 22:13:09,204: t15.2024.03.17 val PER: 0.1764
2026-01-03 22:13:09,205: t15.2024.05.10 val PER: 0.1857
2026-01-03 22:13:09,205: t15.2024.06.14 val PER: 0.1924
2026-01-03 22:13:09,205: t15.2024.07.19 val PER: 0.2755
2026-01-03 22:13:09,205: t15.2024.07.21 val PER: 0.1179
2026-01-03 22:13:09,205: t15.2024.07.28 val PER: 0.1676
2026-01-03 22:13:09,205: t15.2025.01.10 val PER: 0.3471
2026-01-03 22:13:09,205: t15.2025.01.12 val PER: 0.1863
2026-01-03 22:13:09,205: t15.2025.03.14 val PER: 0.3669
2026-01-03 22:13:09,205: t15.2025.03.16 val PER: 0.2147
2026-01-03 22:13:09,205: t15.2025.03.30 val PER: 0.3402
2026-01-03 22:13:09,205: t15.2025.04.13 val PER: 0.2482
2026-01-03 22:13:09,206: New best val WER(1gram) 53.55% --> 50.00%
2026-01-03 22:13:09,206: Checkpointing model
2026-01-03 22:13:09,816: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:13:10,059: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_8500
2026-01-03 22:13:18,534: Train batch 8600: loss: 16.08 grad norm: 63.24 time: 0.055
2026-01-03 22:13:35,531: Train batch 8800: loss: 15.69 grad norm: 61.37 time: 0.060
2026-01-03 22:13:52,878: Train batch 9000: loss: 16.40 grad norm: 68.32 time: 0.071
2026-01-03 22:13:52,879: Running test after training batch: 9000
2026-01-03 22:13:52,999: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:13:57,729: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the could at this point as will
2026-01-03 22:13:57,761: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-03 22:13:59,505: Val batch 9000: PER (avg): 0.1747 CTC Loss (avg): 17.2681 WER(1gram): 51.02% (n=64) time: 6.627
2026-01-03 22:13:59,506: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 22:13:59,506: t15.2023.08.13 val PER: 0.1310
2026-01-03 22:13:59,506: t15.2023.08.18 val PER: 0.1257
2026-01-03 22:13:59,506: t15.2023.08.20 val PER: 0.1303
2026-01-03 22:13:59,506: t15.2023.08.25 val PER: 0.1009
2026-01-03 22:13:59,506: t15.2023.08.27 val PER: 0.2042
2026-01-03 22:13:59,507: t15.2023.09.01 val PER: 0.0933
2026-01-03 22:13:59,507: t15.2023.09.03 val PER: 0.1865
2026-01-03 22:13:59,507: t15.2023.09.24 val PER: 0.1566
2026-01-03 22:13:59,507: t15.2023.09.29 val PER: 0.1442
2026-01-03 22:13:59,507: t15.2023.10.01 val PER: 0.1902
2026-01-03 22:13:59,507: t15.2023.10.06 val PER: 0.0915
2026-01-03 22:13:59,507: t15.2023.10.08 val PER: 0.2585
2026-01-03 22:13:59,507: t15.2023.10.13 val PER: 0.2436
2026-01-03 22:13:59,507: t15.2023.10.15 val PER: 0.1859
2026-01-03 22:13:59,508: t15.2023.10.20 val PER: 0.1812
2026-01-03 22:13:59,508: t15.2023.10.22 val PER: 0.1303
2026-01-03 22:13:59,508: t15.2023.11.03 val PER: 0.2028
2026-01-03 22:13:59,508: t15.2023.11.04 val PER: 0.0410
2026-01-03 22:13:59,508: t15.2023.11.17 val PER: 0.0529
2026-01-03 22:13:59,508: t15.2023.11.19 val PER: 0.0419
2026-01-03 22:13:59,508: t15.2023.11.26 val PER: 0.1732
2026-01-03 22:13:59,508: t15.2023.12.03 val PER: 0.1303
2026-01-03 22:13:59,508: t15.2023.12.08 val PER: 0.1272
2026-01-03 22:13:59,509: t15.2023.12.10 val PER: 0.1117
2026-01-03 22:13:59,509: t15.2023.12.17 val PER: 0.1632
2026-01-03 22:13:59,509: t15.2023.12.29 val PER: 0.1627
2026-01-03 22:13:59,509: t15.2024.02.25 val PER: 0.1390
2026-01-03 22:13:59,509: t15.2024.03.08 val PER: 0.2603
2026-01-03 22:13:59,509: t15.2024.03.15 val PER: 0.2245
2026-01-03 22:13:59,509: t15.2024.03.17 val PER: 0.1827
2026-01-03 22:13:59,509: t15.2024.05.10 val PER: 0.1902
2026-01-03 22:13:59,509: t15.2024.06.14 val PER: 0.1703
2026-01-03 22:13:59,509: t15.2024.07.19 val PER: 0.2676
2026-01-03 22:13:59,510: t15.2024.07.21 val PER: 0.1186
2026-01-03 22:13:59,510: t15.2024.07.28 val PER: 0.1500
2026-01-03 22:13:59,510: t15.2025.01.10 val PER: 0.3099
2026-01-03 22:13:59,510: t15.2025.01.12 val PER: 0.1740
2026-01-03 22:13:59,510: t15.2025.03.14 val PER: 0.3447
2026-01-03 22:13:59,510: t15.2025.03.16 val PER: 0.2147
2026-01-03 22:13:59,510: t15.2025.03.30 val PER: 0.3425
2026-01-03 22:13:59,510: t15.2025.04.13 val PER: 0.2496
2026-01-03 22:13:59,742: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_9000
2026-01-03 22:14:17,010: Train batch 9200: loss: 10.98 grad norm: 51.22 time: 0.056
2026-01-03 22:14:34,287: Train batch 9400: loss: 8.08 grad norm: 47.91 time: 0.067
2026-01-03 22:14:42,950: Running test after training batch: 9500
2026-01-03 22:14:43,097: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:14:47,797: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 22:14:47,827: WER debug example
  GT : how does it keep the cost down
  PR : how dusts it keep the cost it
2026-01-03 22:14:49,549: Val batch 9500: PER (avg): 0.1721 CTC Loss (avg): 17.2069 WER(1gram): 47.97% (n=64) time: 6.599
2026-01-03 22:14:49,549: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 22:14:49,550: t15.2023.08.13 val PER: 0.1268
2026-01-03 22:14:49,550: t15.2023.08.18 val PER: 0.1123
2026-01-03 22:14:49,550: t15.2023.08.20 val PER: 0.1287
2026-01-03 22:14:49,550: t15.2023.08.25 val PER: 0.1024
2026-01-03 22:14:49,550: t15.2023.08.27 val PER: 0.2074
2026-01-03 22:14:49,550: t15.2023.09.01 val PER: 0.0893
2026-01-03 22:14:49,550: t15.2023.09.03 val PER: 0.1675
2026-01-03 22:14:49,550: t15.2023.09.24 val PER: 0.1517
2026-01-03 22:14:49,550: t15.2023.09.29 val PER: 0.1449
2026-01-03 22:14:49,550: t15.2023.10.01 val PER: 0.1929
2026-01-03 22:14:49,550: t15.2023.10.06 val PER: 0.0990
2026-01-03 22:14:49,550: t15.2023.10.08 val PER: 0.2463
2026-01-03 22:14:49,550: t15.2023.10.13 val PER: 0.2312
2026-01-03 22:14:49,551: t15.2023.10.15 val PER: 0.1800
2026-01-03 22:14:49,551: t15.2023.10.20 val PER: 0.1980
2026-01-03 22:14:49,551: t15.2023.10.22 val PER: 0.1247
2026-01-03 22:14:49,551: t15.2023.11.03 val PER: 0.1940
2026-01-03 22:14:49,551: t15.2023.11.04 val PER: 0.0410
2026-01-03 22:14:49,551: t15.2023.11.17 val PER: 0.0591
2026-01-03 22:14:49,551: t15.2023.11.19 val PER: 0.0519
2026-01-03 22:14:49,551: t15.2023.11.26 val PER: 0.1565
2026-01-03 22:14:49,551: t15.2023.12.03 val PER: 0.1387
2026-01-03 22:14:49,551: t15.2023.12.08 val PER: 0.1325
2026-01-03 22:14:49,551: t15.2023.12.10 val PER: 0.1222
2026-01-03 22:14:49,551: t15.2023.12.17 val PER: 0.1632
2026-01-03 22:14:49,551: t15.2023.12.29 val PER: 0.1585
2026-01-03 22:14:49,551: t15.2024.02.25 val PER: 0.1404
2026-01-03 22:14:49,551: t15.2024.03.08 val PER: 0.2518
2026-01-03 22:14:49,551: t15.2024.03.15 val PER: 0.2258
2026-01-03 22:14:49,552: t15.2024.03.17 val PER: 0.1569
2026-01-03 22:14:49,552: t15.2024.05.10 val PER: 0.1902
2026-01-03 22:14:49,552: t15.2024.06.14 val PER: 0.1562
2026-01-03 22:14:49,552: t15.2024.07.19 val PER: 0.2683
2026-01-03 22:14:49,552: t15.2024.07.21 val PER: 0.1124
2026-01-03 22:14:49,552: t15.2024.07.28 val PER: 0.1581
2026-01-03 22:14:49,552: t15.2025.01.10 val PER: 0.3237
2026-01-03 22:14:49,552: t15.2025.01.12 val PER: 0.1855
2026-01-03 22:14:49,552: t15.2025.03.14 val PER: 0.3683
2026-01-03 22:14:49,552: t15.2025.03.16 val PER: 0.2120
2026-01-03 22:14:49,552: t15.2025.03.30 val PER: 0.3207
2026-01-03 22:14:49,552: t15.2025.04.13 val PER: 0.2397
2026-01-03 22:14:49,554: New best val WER(1gram) 50.00% --> 47.97%
2026-01-03 22:14:49,554: Checkpointing model
2026-01-03 22:14:50,168: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/best_checkpoint
2026-01-03 22:14:50,416: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_9500
2026-01-03 22:14:58,988: Train batch 9600: loss: 8.56 grad norm: 47.13 time: 0.073
2026-01-03 22:15:16,023: Train batch 9800: loss: 12.72 grad norm: 57.53 time: 0.063
2026-01-03 22:15:33,208: Train batch 10000: loss: 5.52 grad norm: 39.56 time: 0.060
2026-01-03 22:15:33,208: Running test after training batch: 10000
2026-01-03 22:15:33,333: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:15:38,169: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 22:15:38,202: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sette
2026-01-03 22:15:39,979: Val batch 10000: PER (avg): 0.1709 CTC Loss (avg): 16.9614 WER(1gram): 50.76% (n=64) time: 6.771
2026-01-03 22:15:39,979: WER lens: avg_true_words=6.16 avg_pred_words=6.14 max_pred_words=11
2026-01-03 22:15:39,979: t15.2023.08.13 val PER: 0.1310
2026-01-03 22:15:39,980: t15.2023.08.18 val PER: 0.1249
2026-01-03 22:15:39,980: t15.2023.08.20 val PER: 0.1326
2026-01-03 22:15:39,980: t15.2023.08.25 val PER: 0.0934
2026-01-03 22:15:39,980: t15.2023.08.27 val PER: 0.2106
2026-01-03 22:15:39,980: t15.2023.09.01 val PER: 0.0950
2026-01-03 22:15:39,980: t15.2023.09.03 val PER: 0.1853
2026-01-03 22:15:39,980: t15.2023.09.24 val PER: 0.1493
2026-01-03 22:15:39,980: t15.2023.09.29 val PER: 0.1493
2026-01-03 22:15:39,980: t15.2023.10.01 val PER: 0.1856
2026-01-03 22:15:39,980: t15.2023.10.06 val PER: 0.1033
2026-01-03 22:15:39,980: t15.2023.10.08 val PER: 0.2639
2026-01-03 22:15:39,980: t15.2023.10.13 val PER: 0.2281
2026-01-03 22:15:39,980: t15.2023.10.15 val PER: 0.1694
2026-01-03 22:15:39,980: t15.2023.10.20 val PER: 0.1879
2026-01-03 22:15:39,980: t15.2023.10.22 val PER: 0.1381
2026-01-03 22:15:39,980: t15.2023.11.03 val PER: 0.1906
2026-01-03 22:15:39,980: t15.2023.11.04 val PER: 0.0375
2026-01-03 22:15:39,981: t15.2023.11.17 val PER: 0.0435
2026-01-03 22:15:39,981: t15.2023.11.19 val PER: 0.0519
2026-01-03 22:15:39,985: t15.2023.11.26 val PER: 0.1457
2026-01-03 22:15:39,985: t15.2023.12.03 val PER: 0.1408
2026-01-03 22:15:39,985: t15.2023.12.08 val PER: 0.1312
2026-01-03 22:15:39,985: t15.2023.12.10 val PER: 0.1104
2026-01-03 22:15:39,985: t15.2023.12.17 val PER: 0.1601
2026-01-03 22:15:39,985: t15.2023.12.29 val PER: 0.1469
2026-01-03 22:15:39,985: t15.2024.02.25 val PER: 0.1390
2026-01-03 22:15:39,985: t15.2024.03.08 val PER: 0.2475
2026-01-03 22:15:39,985: t15.2024.03.15 val PER: 0.2226
2026-01-03 22:15:39,985: t15.2024.03.17 val PER: 0.1667
2026-01-03 22:15:39,985: t15.2024.05.10 val PER: 0.1828
2026-01-03 22:15:39,985: t15.2024.06.14 val PER: 0.1814
2026-01-03 22:15:39,986: t15.2024.07.19 val PER: 0.2604
2026-01-03 22:15:39,986: t15.2024.07.21 val PER: 0.1152
2026-01-03 22:15:39,986: t15.2024.07.28 val PER: 0.1559
2026-01-03 22:15:39,986: t15.2025.01.10 val PER: 0.3058
2026-01-03 22:15:39,986: t15.2025.01.12 val PER: 0.1724
2026-01-03 22:15:39,986: t15.2025.03.14 val PER: 0.3595
2026-01-03 22:15:39,986: t15.2025.03.16 val PER: 0.2160
2026-01-03 22:15:39,986: t15.2025.03.30 val PER: 0.3138
2026-01-03 22:15:39,986: t15.2025.04.13 val PER: 0.2397
2026-01-03 22:15:40,222: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_10000
2026-01-03 22:15:57,302: Train batch 10200: loss: 6.28 grad norm: 39.02 time: 0.050
2026-01-03 22:16:14,922: Train batch 10400: loss: 9.63 grad norm: 50.60 time: 0.072
2026-01-03 22:16:23,655: Running test after training batch: 10500
2026-01-03 22:16:23,779: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:16:28,465: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point us will
2026-01-03 22:16:28,496: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-03 22:16:30,299: Val batch 10500: PER (avg): 0.1679 CTC Loss (avg): 16.6931 WER(1gram): 48.48% (n=64) time: 6.644
2026-01-03 22:16:30,299: WER lens: avg_true_words=6.16 avg_pred_words=6.17 max_pred_words=11
2026-01-03 22:16:30,299: t15.2023.08.13 val PER: 0.1289
2026-01-03 22:16:30,299: t15.2023.08.18 val PER: 0.1249
2026-01-03 22:16:30,300: t15.2023.08.20 val PER: 0.1223
2026-01-03 22:16:30,300: t15.2023.08.25 val PER: 0.1084
2026-01-03 22:16:30,300: t15.2023.08.27 val PER: 0.2074
2026-01-03 22:16:30,300: t15.2023.09.01 val PER: 0.0998
2026-01-03 22:16:30,300: t15.2023.09.03 val PER: 0.1781
2026-01-03 22:16:30,300: t15.2023.09.24 val PER: 0.1468
2026-01-03 22:16:30,300: t15.2023.09.29 val PER: 0.1506
2026-01-03 22:16:30,300: t15.2023.10.01 val PER: 0.1889
2026-01-03 22:16:30,300: t15.2023.10.06 val PER: 0.0936
2026-01-03 22:16:30,301: t15.2023.10.08 val PER: 0.2530
2026-01-03 22:16:30,301: t15.2023.10.13 val PER: 0.2141
2026-01-03 22:16:30,301: t15.2023.10.15 val PER: 0.1767
2026-01-03 22:16:30,301: t15.2023.10.20 val PER: 0.1745
2026-01-03 22:16:30,301: t15.2023.10.22 val PER: 0.1247
2026-01-03 22:16:30,301: t15.2023.11.03 val PER: 0.2001
2026-01-03 22:16:30,301: t15.2023.11.04 val PER: 0.0512
2026-01-03 22:16:30,301: t15.2023.11.17 val PER: 0.0560
2026-01-03 22:16:30,301: t15.2023.11.19 val PER: 0.0599
2026-01-03 22:16:30,302: t15.2023.11.26 val PER: 0.1399
2026-01-03 22:16:30,302: t15.2023.12.03 val PER: 0.1292
2026-01-03 22:16:30,302: t15.2023.12.08 val PER: 0.1205
2026-01-03 22:16:30,302: t15.2023.12.10 val PER: 0.1051
2026-01-03 22:16:30,302: t15.2023.12.17 val PER: 0.1486
2026-01-03 22:16:30,302: t15.2023.12.29 val PER: 0.1531
2026-01-03 22:16:30,302: t15.2024.02.25 val PER: 0.1222
2026-01-03 22:16:30,302: t15.2024.03.08 val PER: 0.2432
2026-01-03 22:16:30,302: t15.2024.03.15 val PER: 0.2264
2026-01-03 22:16:30,302: t15.2024.03.17 val PER: 0.1639
2026-01-03 22:16:30,303: t15.2024.05.10 val PER: 0.1783
2026-01-03 22:16:30,303: t15.2024.06.14 val PER: 0.1782
2026-01-03 22:16:30,303: t15.2024.07.19 val PER: 0.2591
2026-01-03 22:16:30,303: t15.2024.07.21 val PER: 0.1007
2026-01-03 22:16:30,303: t15.2024.07.28 val PER: 0.1441
2026-01-03 22:16:30,303: t15.2025.01.10 val PER: 0.3099
2026-01-03 22:16:30,303: t15.2025.01.12 val PER: 0.1671
2026-01-03 22:16:30,303: t15.2025.03.14 val PER: 0.3787
2026-01-03 22:16:30,303: t15.2025.03.16 val PER: 0.1911
2026-01-03 22:16:30,304: t15.2025.03.30 val PER: 0.3207
2026-01-03 22:16:30,304: t15.2025.04.13 val PER: 0.2282
2026-01-03 22:16:30,540: Saved model to checkpoint: trained_models/rnn_dropout/lr40/base/checkpoint/checkpoint_batch_10500
2026-01-03 22:16:39,375: Train batch 10600: loss: 9.79 grad norm: 60.94 time: 0.073
2026-01-03 22:16:56,647: Train batch 10800: loss: 14.82 grad norm: 64.51 time: 0.064
2026-01-03 22:17:14,423: Train batch 11000: loss: 14.21 grad norm: 60.48 time: 0.056
2026-01-03 22:17:14,423: Running test after training batch: 11000
2026-01-03 22:17:14,518: WER debug GT example: You can see the code at this point as well.
2026-01-03 22:17:19,212: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 22:17:19,243: WER debug example
  GT : how does it keep the cost down
  PR : how dust it keep the cost et
2026-01-03 22:17:21,046: Val batch 11000: PER (avg): 0.1668 CTC Loss (avg): 16.6017 WER(1gram): 49.49% (n=64) time: 6.622
2026-01-03 22:17:21,046: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 22:17:21,046: t15.2023.08.13 val PER: 0.1237
2026-01-03 22:17:21,046: t15.2023.08.18 val PER: 0.1232
2026-01-03 22:17:21,046: t15.2023.08.20 val PER: 0.1223
2026-01-03 22:17:21,046: t15.2023.08.25 val PER: 0.0934
2026-01-03 22:17:21,046: t15.2023.08.27 val PER: 0.2106
2026-01-03 22:17:21,046: t15.2023.09.01 val PER: 0.0860
2026-01-03 22:17:21,047: t15.2023.09.03 val PER: 0.1734
2026-01-03 22:17:21,047: t15.2023.09.24 val PER: 0.1468
2026-01-03 22:17:21,047: t15.2023.09.29 val PER: 0.1429
2026-01-03 22:17:21,047: t15.2023.10.01 val PER: 0.1968
2026-01-03 22:17:21,047: t15.2023.10.06 val PER: 0.0840
2026-01-03 22:17:21,047: t15.2023.10.08 val PER: 0.2463
2026-01-03 22:17:21,047: t15.2023.10.13 val PER: 0.2157
2026-01-03 22:17:21,047: t15.2023.10.15 val PER: 0.1753
2026-01-03 22:17:21,047: t15.2023.10.20 val PER: 0.1879
2026-01-03 22:17:21,047: t15.2023.10.22 val PER: 0.1203
2026-01-03 22:17:21,048: t15.2023.11.03 val PER: 0.1906
2026-01-03 22:17:21,048: t15.2023.11.04 val PER: 0.0341
2026-01-03 22:17:21,048: t15.2023.11.17 val PER: 0.0529
2026-01-03 22:17:21,048: t15.2023.11.19 val PER: 0.0479
2026-01-03 22:17:21,048: t15.2023.11.26 val PER: 0.1522
2026-01-03 22:17:21,048: t15.2023.12.03 val PER: 0.1366
2026-01-03 22:17:21,048: t15.2023.12.08 val PER: 0.1198
2026-01-03 22:17:21,048: t15.2023.12.10 val PER: 0.1078
2026-01-03 22:17:21,048: t15.2023.12.17 val PER: 0.1559
2026-01-03 22:17:21,049: t15.2023.12.29 val PER: 0.1476
2026-01-03 22:17:21,049: t15.2024.02.25 val PER: 0.1376
2026-01-03 22:17:21,049: t15.2024.03.08 val PER: 0.2475
2026-01-03 22:17:21,050: t15.2024.03.15 val PER: 0.2245
2026-01-03 22:17:21,050: t15.2024.03.17 val PER: 0.1555
2026-01-03 22:17:21,050: t15.2024.05.10 val PER: 0.1842
2026-01-03 22:17:21,050: t15.2024.06.14 val PER: 0.1688
2026-01-03 22:17:21,050: t15.2024.07.19 val PER: 0.2577
2026-01-03 22:17:21,050: t15.2024.07.21 val PER: 0.1069
2026-01-03 22:17:21,050: t15.2024.07.28 val PER: 0.1493
2026-01-03 22:17:21,050: t15.2025.01.10 val PER: 0.3113
2026-01-03 22:17:21,051: t15.2025.01.12 val PER: 0.1732
2026-01-03 22:17:21,051: t15.2025.03.14 val PER: 0.3565
2026-01-03 22:17:21,051: t15.2025.03.16 val PER: 0.2068
2026-01-03 22:17:21,051: t15.2025.03.30 val PER: 0.3115
2026-01-03 22:17:21,051: t15.2025.04.13 val PER: 0.2254
[1;34mwandb[0m: 
[1;34mwandb[0m:  View run [33mbase[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../tmp/e12511253_b2t_348248/wandb/wandb/run-20260103_215808-dvoo6dx8/logs[0m
