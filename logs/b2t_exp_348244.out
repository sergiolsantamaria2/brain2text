TMPDIR=/tmp
JOB_TMP=/tmp/e12511253_b2t_348244
TORCH_EXTENSIONS_DIR=/tmp/e12511253_b2t_348244/torch_extensions
WANDB_DIR=/tmp/e12511253_b2t_348244/wandb
torch CUDA runtime expects: 11.7
CUDA_HOME=/home/e12511253/miniforge3/envs/brain2text
which nvcc: /home/e12511253/miniforge3/envs/brain2text/bin/nvcc
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_May__3_18:49:52_PDT_2022
Cuda compilation tools, release 11.7, V11.7.64
Build cuda_11.7.r11.7/compiler.31294372_0
nvcc release: 11.7
CONDA_PREFIX=/home/e12511253/miniforge3/envs/brain2text
TORCH_LIB=/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib
FST_SO=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
LD_LIBRARY_PATH=/tmp/e12511253_b2t_348244/lm_runtime_libs:/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs:/home/e12511253/miniforge3/envs/brain2text/lib:/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/lib:/home/e12511253/miniforge3/envs/brain2text/lib64:/home/e12511253/miniforge3/envs/brain2text/lib:
lrwxrwxrwx 1 e12511253 e12511253 153 Jan  3 21:32 /tmp/e12511253_b2t_348244/lm_runtime_libs/libfst.so.8 -> /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/runtime/server/x86/fc_base/openfst-build/src/lib/.libs/libfst.so.8.0.0
lm_decoder import: OK
CUDART_SO=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
CUDA_LIB=/home/e12511253/miniforge3/envs/brain2text/lib64
LD_PRELOAD=/home/e12511253/miniforge3/envs/brain2text/lib64/libcudart.so.11.7.60
TORCH_USE_RTLD_GLOBAL=1
trained_models -> /tmp/e12511253_b2t_348244/trained_models
OUT_ROOT=/tmp/e12511253_b2t_348244/trained_models
==============================================
Job: b2t_exp  ID: 348244
Base: configs/rnn_args.yaml
Global override 1: configs/overrides/wer_1gram_only.yaml
Folders: configs/experiments/gru/rnn_dropout/
Host: a-l40s-o-1
CUDA_VISIBLE_DEVICES=0
==============================================

========== FOLDER: configs/experiments/gru/rnn_dropout/ ==========
Num configs: 6

=== RUN base.yaml ===
2026-01-03 21:32:52,250: Using device: cuda:0
2026-01-03 21:32:53,864: Local LM WER enabled. lm_dir=/home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil
2026-01-03 21:32:53,886: Using 45 sessions after filtering (from 45).
2026-01-03 21:32:54,292: Using torch.compile (if available)
2026-01-03 21:32:54,293: torch.compile not available (torch<2.0). Skipping.
2026-01-03 21:32:54,293: Initialized RNN decoding model
2026-01-03 21:32:54,293: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.2)
  (head): Sequential(
    (0): ResidualFFNBlock(
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (lin): Linear(in_features=768, out_features=768, bias=True)
      (act): GELU(approximate='none')
      (drop): Dropout(p=0.1, inplace=False)
    )
  )
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2026-01-03 21:32:54,293: Model has 44,907,305 parameters
2026-01-03 21:32:54,293: Model has 11,819,520 day-specific parameters | 26.32% of total parameters
2026-01-03 21:32:55,576: Successfully initialized datasets
2026-01-03 21:32:55,576: AdamW(fused) not available in this torch build. Using standard AdamW.
2026-01-03 21:32:56,477: Train batch 0: loss: 581.04 grad norm: 1398.64 time: 0.172
2026-01-03 21:32:56,477: Running test after training batch: 0
2026-01-03 21:32:56,588: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:33:01,957: WER debug example
  GT : you can see the code at this point as well
  PR : laham zapf
2026-01-03 21:33:02,684: WER debug example
  GT : how does it keep the cost down
  PR : anafranil
2026-01-03 21:33:37,234: Val batch 0: PER (avg): 1.4306 CTC Loss (avg): 633.3639 WER(1gram): 100.00% (n=64) time: 40.757
2026-01-03 21:33:37,235: WER lens: avg_true_words=6.16 avg_pred_words=1.78 max_pred_words=4
2026-01-03 21:33:37,235: t15.2023.08.13 val PER: 1.3046
2026-01-03 21:33:37,235: t15.2023.08.18 val PER: 1.4283
2026-01-03 21:33:37,235: t15.2023.08.20 val PER: 1.3018
2026-01-03 21:33:37,235: t15.2023.08.25 val PER: 1.3358
2026-01-03 21:33:37,235: t15.2023.08.27 val PER: 1.2524
2026-01-03 21:33:37,235: t15.2023.09.01 val PER: 1.4529
2026-01-03 21:33:37,235: t15.2023.09.03 val PER: 1.3171
2026-01-03 21:33:37,235: t15.2023.09.24 val PER: 1.5400
2026-01-03 21:33:37,236: t15.2023.09.29 val PER: 1.4671
2026-01-03 21:33:37,236: t15.2023.10.01 val PER: 1.2173
2026-01-03 21:33:37,236: t15.2023.10.06 val PER: 1.4909
2026-01-03 21:33:37,236: t15.2023.10.08 val PER: 1.1908
2026-01-03 21:33:37,236: t15.2023.10.13 val PER: 1.3933
2026-01-03 21:33:37,236: t15.2023.10.15 val PER: 1.3869
2026-01-03 21:33:37,236: t15.2023.10.20 val PER: 1.5000
2026-01-03 21:33:37,236: t15.2023.10.22 val PER: 1.3886
2026-01-03 21:33:37,236: t15.2023.11.03 val PER: 1.5977
2026-01-03 21:33:37,236: t15.2023.11.04 val PER: 2.0444
2026-01-03 21:33:37,236: t15.2023.11.17 val PER: 1.9580
2026-01-03 21:33:37,236: t15.2023.11.19 val PER: 1.6766
2026-01-03 21:33:37,236: t15.2023.11.26 val PER: 1.5406
2026-01-03 21:33:37,237: t15.2023.12.03 val PER: 1.4338
2026-01-03 21:33:37,237: t15.2023.12.08 val PER: 1.4501
2026-01-03 21:33:37,237: t15.2023.12.10 val PER: 1.6991
2026-01-03 21:33:37,237: t15.2023.12.17 val PER: 1.3087
2026-01-03 21:33:37,237: t15.2023.12.29 val PER: 1.4139
2026-01-03 21:33:37,237: t15.2024.02.25 val PER: 1.4199
2026-01-03 21:33:37,237: t15.2024.03.08 val PER: 1.3243
2026-01-03 21:33:37,237: t15.2024.03.15 val PER: 1.3177
2026-01-03 21:33:37,237: t15.2024.03.17 val PER: 1.4017
2026-01-03 21:33:37,237: t15.2024.05.10 val PER: 1.3284
2026-01-03 21:33:37,237: t15.2024.06.14 val PER: 1.5363
2026-01-03 21:33:37,237: t15.2024.07.19 val PER: 1.0811
2026-01-03 21:33:37,237: t15.2024.07.21 val PER: 1.6317
2026-01-03 21:33:37,237: t15.2024.07.28 val PER: 1.6588
2026-01-03 21:33:37,237: t15.2025.01.10 val PER: 1.0868
2026-01-03 21:33:37,237: t15.2025.01.12 val PER: 1.7644
2026-01-03 21:33:37,238: t15.2025.03.14 val PER: 1.0399
2026-01-03 21:33:37,238: t15.2025.03.16 val PER: 1.6217
2026-01-03 21:33:37,238: t15.2025.03.30 val PER: 1.2920
2026-01-03 21:33:37,238: t15.2025.04.13 val PER: 1.5877
2026-01-03 21:33:37,239: New best val WER(1gram) inf% --> 100.00%
2026-01-03 21:33:37,515: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_0
2026-01-03 21:33:54,478: Train batch 200: loss: 80.58 grad norm: 79.94 time: 0.054
2026-01-03 21:34:11,192: Train batch 400: loss: 57.92 grad norm: 93.90 time: 0.063
2026-01-03 21:34:19,626: Running test after training batch: 500
2026-01-03 21:34:19,757: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:34:24,762: WER debug example
  GT : you can see the code at this point as well
  PR : yule eanes ooohs ooohs uhde at this ide is aisle
2026-01-03 21:34:24,801: WER debug example
  GT : how does it keep the cost down
  PR : ide does it ink thus as twos
2026-01-03 21:34:27,285: Val batch 500: PER (avg): 0.5500 CTC Loss (avg): 58.5543 WER(1gram): 91.12% (n=64) time: 7.657
2026-01-03 21:34:27,288: WER lens: avg_true_words=6.16 avg_pred_words=5.78 max_pred_words=12
2026-01-03 21:34:27,290: t15.2023.08.13 val PER: 0.4917
2026-01-03 21:34:27,292: t15.2023.08.18 val PER: 0.4895
2026-01-03 21:34:27,293: t15.2023.08.20 val PER: 0.4829
2026-01-03 21:34:27,295: t15.2023.08.25 val PER: 0.4654
2026-01-03 21:34:27,296: t15.2023.08.27 val PER: 0.5498
2026-01-03 21:34:27,298: t15.2023.09.01 val PER: 0.4505
2026-01-03 21:34:27,299: t15.2023.09.03 val PER: 0.5273
2026-01-03 21:34:27,300: t15.2023.09.24 val PER: 0.4830
2026-01-03 21:34:27,302: t15.2023.09.29 val PER: 0.5022
2026-01-03 21:34:27,303: t15.2023.10.01 val PER: 0.5476
2026-01-03 21:34:27,305: t15.2023.10.06 val PER: 0.4586
2026-01-03 21:34:27,306: t15.2023.10.08 val PER: 0.5737
2026-01-03 21:34:27,308: t15.2023.10.13 val PER: 0.5857
2026-01-03 21:34:27,309: t15.2023.10.15 val PER: 0.5214
2026-01-03 21:34:27,310: t15.2023.10.20 val PER: 0.4933
2026-01-03 21:34:27,312: t15.2023.10.22 val PER: 0.4811
2026-01-03 21:34:27,313: t15.2023.11.03 val PER: 0.5312
2026-01-03 21:34:27,314: t15.2023.11.04 val PER: 0.3003
2026-01-03 21:34:27,316: t15.2023.11.17 val PER: 0.3919
2026-01-03 21:34:27,317: t15.2023.11.19 val PER: 0.3752
2026-01-03 21:34:27,319: t15.2023.11.26 val PER: 0.5826
2026-01-03 21:34:27,320: t15.2023.12.03 val PER: 0.5315
2026-01-03 21:34:27,321: t15.2023.12.08 val PER: 0.5519
2026-01-03 21:34:27,323: t15.2023.12.10 val PER: 0.5046
2026-01-03 21:34:27,324: t15.2023.12.17 val PER: 0.6133
2026-01-03 21:34:27,325: t15.2023.12.29 val PER: 0.5806
2026-01-03 21:34:27,327: t15.2024.02.25 val PER: 0.5126
2026-01-03 21:34:27,328: t15.2024.03.08 val PER: 0.6472
2026-01-03 21:34:27,330: t15.2024.03.15 val PER: 0.5910
2026-01-03 21:34:27,331: t15.2024.03.17 val PER: 0.5342
2026-01-03 21:34:27,332: t15.2024.05.10 val PER: 0.5780
2026-01-03 21:34:27,334: t15.2024.06.14 val PER: 0.5363
2026-01-03 21:34:27,335: t15.2024.07.19 val PER: 0.6987
2026-01-03 21:34:27,336: t15.2024.07.21 val PER: 0.5007
2026-01-03 21:34:27,338: t15.2024.07.28 val PER: 0.5522
2026-01-03 21:34:27,339: t15.2025.01.10 val PER: 0.7590
2026-01-03 21:34:27,340: t15.2025.01.12 val PER: 0.5982
2026-01-03 21:34:27,342: t15.2025.03.14 val PER: 0.7707
2026-01-03 21:34:27,343: t15.2025.03.16 val PER: 0.6204
2026-01-03 21:34:27,344: t15.2025.03.30 val PER: 0.7425
2026-01-03 21:34:27,346: t15.2025.04.13 val PER: 0.6049
2026-01-03 21:34:27,347: New best val WER(1gram) 100.00% --> 91.12%
2026-01-03 21:34:27,670: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_500
2026-01-03 21:34:36,325: Train batch 600: loss: 52.17 grad norm: 87.43 time: 0.078
2026-01-03 21:34:53,401: Train batch 800: loss: 43.59 grad norm: 87.75 time: 0.057
2026-01-03 21:35:10,666: Train batch 1000: loss: 44.98 grad norm: 81.72 time: 0.065
2026-01-03 21:35:10,668: Running test after training batch: 1000
2026-01-03 21:35:10,788: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:35:15,655: WER debug example
  GT : you can see the code at this point as well
  PR : used aunt ease thus uhde at this royd is whiles
2026-01-03 21:35:15,690: WER debug example
  GT : how does it keep the cost down
  PR : houde does it ink that us id
2026-01-03 21:35:17,738: Val batch 1000: PER (avg): 0.4343 CTC Loss (avg): 45.4422 WER(1gram): 83.50% (n=64) time: 7.061
2026-01-03 21:35:17,741: WER lens: avg_true_words=6.16 avg_pred_words=5.64 max_pred_words=12
2026-01-03 21:35:17,743: t15.2023.08.13 val PER: 0.4033
2026-01-03 21:35:17,745: t15.2023.08.18 val PER: 0.3705
2026-01-03 21:35:17,747: t15.2023.08.20 val PER: 0.3685
2026-01-03 21:35:17,749: t15.2023.08.25 val PER: 0.3238
2026-01-03 21:35:17,750: t15.2023.08.27 val PER: 0.4405
2026-01-03 21:35:17,752: t15.2023.09.01 val PER: 0.3279
2026-01-03 21:35:17,753: t15.2023.09.03 val PER: 0.4276
2026-01-03 21:35:17,755: t15.2023.09.24 val PER: 0.3665
2026-01-03 21:35:17,757: t15.2023.09.29 val PER: 0.3918
2026-01-03 21:35:17,758: t15.2023.10.01 val PER: 0.4313
2026-01-03 21:35:17,760: t15.2023.10.06 val PER: 0.3358
2026-01-03 21:35:17,762: t15.2023.10.08 val PER: 0.4655
2026-01-03 21:35:17,763: t15.2023.10.13 val PER: 0.4911
2026-01-03 21:35:17,765: t15.2023.10.15 val PER: 0.4113
2026-01-03 21:35:17,767: t15.2023.10.20 val PER: 0.3926
2026-01-03 21:35:17,768: t15.2023.10.22 val PER: 0.3731
2026-01-03 21:35:17,769: t15.2023.11.03 val PER: 0.4220
2026-01-03 21:35:17,771: t15.2023.11.04 val PER: 0.2048
2026-01-03 21:35:17,772: t15.2023.11.17 val PER: 0.2799
2026-01-03 21:35:17,775: t15.2023.11.19 val PER: 0.2275
2026-01-03 21:35:17,776: t15.2023.11.26 val PER: 0.4717
2026-01-03 21:35:17,778: t15.2023.12.03 val PER: 0.4370
2026-01-03 21:35:17,779: t15.2023.12.08 val PER: 0.4201
2026-01-03 21:35:17,781: t15.2023.12.10 val PER: 0.3745
2026-01-03 21:35:17,782: t15.2023.12.17 val PER: 0.4501
2026-01-03 21:35:17,784: t15.2023.12.29 val PER: 0.4235
2026-01-03 21:35:17,786: t15.2024.02.25 val PER: 0.3820
2026-01-03 21:35:17,787: t15.2024.03.08 val PER: 0.5277
2026-01-03 21:35:17,789: t15.2024.03.15 val PER: 0.4690
2026-01-03 21:35:17,790: t15.2024.03.17 val PER: 0.4282
2026-01-03 21:35:17,792: t15.2024.05.10 val PER: 0.4591
2026-01-03 21:35:17,797: t15.2024.06.14 val PER: 0.4338
2026-01-03 21:35:17,798: t15.2024.07.19 val PER: 0.5616
2026-01-03 21:35:17,800: t15.2024.07.21 val PER: 0.3979
2026-01-03 21:35:17,802: t15.2024.07.28 val PER: 0.4441
2026-01-03 21:35:17,803: t15.2025.01.10 val PER: 0.6460
2026-01-03 21:35:17,805: t15.2025.01.12 val PER: 0.4758
2026-01-03 21:35:17,806: t15.2025.03.14 val PER: 0.6672
2026-01-03 21:35:17,808: t15.2025.03.16 val PER: 0.5039
2026-01-03 21:35:17,809: t15.2025.03.30 val PER: 0.6690
2026-01-03 21:35:17,811: t15.2025.04.13 val PER: 0.5093
2026-01-03 21:35:17,812: New best val WER(1gram) 91.12% --> 83.50%
2026-01-03 21:35:18,138: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_1000
2026-01-03 21:35:35,565: Train batch 1200: loss: 36.48 grad norm: 80.45 time: 0.068
2026-01-03 21:35:53,285: Train batch 1400: loss: 39.64 grad norm: 86.94 time: 0.060
2026-01-03 21:36:01,871: Running test after training batch: 1500
2026-01-03 21:36:01,981: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:36:06,920: WER debug example
  GT : you can see the code at this point as well
  PR : yule kint ease utt good it this boyde is wheel
2026-01-03 21:36:06,955: WER debug example
  GT : how does it keep the cost down
  PR : houde is it eke that os it
2026-01-03 21:36:08,649: Val batch 1500: PER (avg): 0.4043 CTC Loss (avg): 39.6618 WER(1gram): 81.98% (n=64) time: 6.776
2026-01-03 21:36:08,652: WER lens: avg_true_words=6.16 avg_pred_words=5.09 max_pred_words=11
2026-01-03 21:36:08,654: t15.2023.08.13 val PER: 0.3690
2026-01-03 21:36:08,656: t15.2023.08.18 val PER: 0.3395
2026-01-03 21:36:08,657: t15.2023.08.20 val PER: 0.3328
2026-01-03 21:36:08,659: t15.2023.08.25 val PER: 0.2726
2026-01-03 21:36:08,660: t15.2023.08.27 val PER: 0.4341
2026-01-03 21:36:08,662: t15.2023.09.01 val PER: 0.2857
2026-01-03 21:36:08,663: t15.2023.09.03 val PER: 0.3943
2026-01-03 21:36:08,665: t15.2023.09.24 val PER: 0.3277
2026-01-03 21:36:08,666: t15.2023.09.29 val PER: 0.3631
2026-01-03 21:36:08,668: t15.2023.10.01 val PER: 0.4168
2026-01-03 21:36:08,669: t15.2023.10.06 val PER: 0.3197
2026-01-03 21:36:08,671: t15.2023.10.08 val PER: 0.4520
2026-01-03 21:36:08,672: t15.2023.10.13 val PER: 0.4686
2026-01-03 21:36:08,674: t15.2023.10.15 val PER: 0.3810
2026-01-03 21:36:08,675: t15.2023.10.20 val PER: 0.3523
2026-01-03 21:36:08,677: t15.2023.10.22 val PER: 0.3307
2026-01-03 21:36:08,679: t15.2023.11.03 val PER: 0.3779
2026-01-03 21:36:08,680: t15.2023.11.04 val PER: 0.1365
2026-01-03 21:36:08,682: t15.2023.11.17 val PER: 0.2566
2026-01-03 21:36:08,683: t15.2023.11.19 val PER: 0.2076
2026-01-03 21:36:08,685: t15.2023.11.26 val PER: 0.4543
2026-01-03 21:36:08,686: t15.2023.12.03 val PER: 0.4002
2026-01-03 21:36:08,688: t15.2023.12.08 val PER: 0.3842
2026-01-03 21:36:08,690: t15.2023.12.10 val PER: 0.3285
2026-01-03 21:36:08,691: t15.2023.12.17 val PER: 0.3971
2026-01-03 21:36:08,693: t15.2023.12.29 val PER: 0.3898
2026-01-03 21:36:08,694: t15.2024.02.25 val PER: 0.3357
2026-01-03 21:36:08,696: t15.2024.03.08 val PER: 0.4765
2026-01-03 21:36:08,697: t15.2024.03.15 val PER: 0.4403
2026-01-03 21:36:08,699: t15.2024.03.17 val PER: 0.4031
2026-01-03 21:36:08,701: t15.2024.05.10 val PER: 0.4175
2026-01-03 21:36:08,702: t15.2024.06.14 val PER: 0.4211
2026-01-03 21:36:08,704: t15.2024.07.19 val PER: 0.5583
2026-01-03 21:36:08,705: t15.2024.07.21 val PER: 0.3690
2026-01-03 21:36:08,707: t15.2024.07.28 val PER: 0.3926
2026-01-03 21:36:08,708: t15.2025.01.10 val PER: 0.6515
2026-01-03 21:36:08,710: t15.2025.01.12 val PER: 0.4488
2026-01-03 21:36:08,712: t15.2025.03.14 val PER: 0.6109
2026-01-03 21:36:08,713: t15.2025.03.16 val PER: 0.4869
2026-01-03 21:36:08,715: t15.2025.03.30 val PER: 0.6598
2026-01-03 21:36:08,716: t15.2025.04.13 val PER: 0.4993
2026-01-03 21:36:08,718: New best val WER(1gram) 83.50% --> 81.98%
2026-01-03 21:36:09,041: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_1500
2026-01-03 21:36:17,753: Train batch 1600: loss: 39.36 grad norm: 81.45 time: 0.064
2026-01-03 21:36:35,320: Train batch 1800: loss: 38.06 grad norm: 74.17 time: 0.088
2026-01-03 21:36:52,755: Train batch 2000: loss: 37.18 grad norm: 72.97 time: 0.066
2026-01-03 21:36:52,757: Running test after training batch: 2000
2026-01-03 21:36:52,886: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:36:57,713: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt wheeze the code at this boyde is will
2026-01-03 21:36:57,749: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke thus us it
2026-01-03 21:36:59,452: Val batch 2000: PER (avg): 0.3531 CTC Loss (avg): 35.7034 WER(1gram): 75.89% (n=64) time: 6.692
2026-01-03 21:36:59,455: WER lens: avg_true_words=6.16 avg_pred_words=5.58 max_pred_words=12
2026-01-03 21:36:59,458: t15.2023.08.13 val PER: 0.3347
2026-01-03 21:36:59,460: t15.2023.08.18 val PER: 0.2875
2026-01-03 21:36:59,462: t15.2023.08.20 val PER: 0.2756
2026-01-03 21:36:59,464: t15.2023.08.25 val PER: 0.2395
2026-01-03 21:36:59,466: t15.2023.08.27 val PER: 0.3521
2026-01-03 21:36:59,467: t15.2023.09.01 val PER: 0.2411
2026-01-03 21:36:59,469: t15.2023.09.03 val PER: 0.3563
2026-01-03 21:36:59,471: t15.2023.09.24 val PER: 0.2816
2026-01-03 21:36:59,473: t15.2023.09.29 val PER: 0.2936
2026-01-03 21:36:59,474: t15.2023.10.01 val PER: 0.3454
2026-01-03 21:36:59,476: t15.2023.10.06 val PER: 0.2583
2026-01-03 21:36:59,478: t15.2023.10.08 val PER: 0.4235
2026-01-03 21:36:59,480: t15.2023.10.13 val PER: 0.3995
2026-01-03 21:36:59,482: t15.2023.10.15 val PER: 0.3204
2026-01-03 21:36:59,484: t15.2023.10.20 val PER: 0.2987
2026-01-03 21:36:59,485: t15.2023.10.22 val PER: 0.2795
2026-01-03 21:36:59,487: t15.2023.11.03 val PER: 0.3372
2026-01-03 21:36:59,489: t15.2023.11.04 val PER: 0.1331
2026-01-03 21:36:59,491: t15.2023.11.17 val PER: 0.1991
2026-01-03 21:36:59,493: t15.2023.11.19 val PER: 0.1697
2026-01-03 21:36:59,495: t15.2023.11.26 val PER: 0.3978
2026-01-03 21:36:59,497: t15.2023.12.03 val PER: 0.3235
2026-01-03 21:36:59,498: t15.2023.12.08 val PER: 0.3395
2026-01-03 21:36:59,500: t15.2023.12.10 val PER: 0.2838
2026-01-03 21:36:59,502: t15.2023.12.17 val PER: 0.3337
2026-01-03 21:36:59,503: t15.2023.12.29 val PER: 0.3418
2026-01-03 21:36:59,505: t15.2024.02.25 val PER: 0.3104
2026-01-03 21:36:59,507: t15.2024.03.08 val PER: 0.4324
2026-01-03 21:36:59,508: t15.2024.03.15 val PER: 0.3834
2026-01-03 21:36:59,510: t15.2024.03.17 val PER: 0.3626
2026-01-03 21:36:59,511: t15.2024.05.10 val PER: 0.3744
2026-01-03 21:36:59,513: t15.2024.06.14 val PER: 0.3707
2026-01-03 21:36:59,515: t15.2024.07.19 val PER: 0.5036
2026-01-03 21:36:59,516: t15.2024.07.21 val PER: 0.3276
2026-01-03 21:36:59,518: t15.2024.07.28 val PER: 0.3544
2026-01-03 21:36:59,519: t15.2025.01.10 val PER: 0.5565
2026-01-03 21:36:59,521: t15.2025.01.12 val PER: 0.4057
2026-01-03 21:36:59,522: t15.2025.03.14 val PER: 0.5769
2026-01-03 21:36:59,524: t15.2025.03.16 val PER: 0.4529
2026-01-03 21:36:59,526: t15.2025.03.30 val PER: 0.5897
2026-01-03 21:36:59,528: t15.2025.04.13 val PER: 0.4579
2026-01-03 21:36:59,529: New best val WER(1gram) 81.98% --> 75.89%
2026-01-03 21:36:59,861: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_2000
2026-01-03 21:37:16,972: Train batch 2200: loss: 32.69 grad norm: 79.65 time: 0.060
2026-01-03 21:37:34,197: Train batch 2400: loss: 32.22 grad norm: 65.59 time: 0.052
2026-01-03 21:37:42,931: Running test after training batch: 2500
2026-01-03 21:37:43,073: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:37:47,936: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt sze the good it this boyde is wheel
2026-01-03 21:37:47,970: WER debug example
  GT : how does it keep the cost down
  PR : houde does it eke that us it
2026-01-03 21:37:49,694: Val batch 2500: PER (avg): 0.3265 CTC Loss (avg): 32.6768 WER(1gram): 70.05% (n=64) time: 6.760
2026-01-03 21:37:49,696: WER lens: avg_true_words=6.16 avg_pred_words=5.47 max_pred_words=11
2026-01-03 21:37:49,698: t15.2023.08.13 val PER: 0.3056
2026-01-03 21:37:49,700: t15.2023.08.18 val PER: 0.2674
2026-01-03 21:37:49,702: t15.2023.08.20 val PER: 0.2502
2026-01-03 21:37:49,704: t15.2023.08.25 val PER: 0.2214
2026-01-03 21:37:49,706: t15.2023.08.27 val PER: 0.3457
2026-01-03 21:37:49,708: t15.2023.09.01 val PER: 0.2273
2026-01-03 21:37:49,709: t15.2023.09.03 val PER: 0.3349
2026-01-03 21:37:49,711: t15.2023.09.24 val PER: 0.2621
2026-01-03 21:37:49,713: t15.2023.09.29 val PER: 0.2840
2026-01-03 21:37:49,715: t15.2023.10.01 val PER: 0.3309
2026-01-03 21:37:49,717: t15.2023.10.06 val PER: 0.2314
2026-01-03 21:37:49,718: t15.2023.10.08 val PER: 0.3870
2026-01-03 21:37:49,720: t15.2023.10.13 val PER: 0.3801
2026-01-03 21:37:49,722: t15.2023.10.15 val PER: 0.3052
2026-01-03 21:37:49,724: t15.2023.10.20 val PER: 0.2819
2026-01-03 21:37:49,726: t15.2023.10.22 val PER: 0.2528
2026-01-03 21:37:49,727: t15.2023.11.03 val PER: 0.3094
2026-01-03 21:37:49,729: t15.2023.11.04 val PER: 0.1126
2026-01-03 21:37:49,731: t15.2023.11.17 val PER: 0.1711
2026-01-03 21:37:49,732: t15.2023.11.19 val PER: 0.1477
2026-01-03 21:37:49,734: t15.2023.11.26 val PER: 0.3703
2026-01-03 21:37:49,736: t15.2023.12.03 val PER: 0.3067
2026-01-03 21:37:49,737: t15.2023.12.08 val PER: 0.3003
2026-01-03 21:37:49,739: t15.2023.12.10 val PER: 0.2523
2026-01-03 21:37:49,741: t15.2023.12.17 val PER: 0.3139
2026-01-03 21:37:49,742: t15.2023.12.29 val PER: 0.3219
2026-01-03 21:37:49,744: t15.2024.02.25 val PER: 0.2669
2026-01-03 21:37:49,746: t15.2024.03.08 val PER: 0.3983
2026-01-03 21:37:49,748: t15.2024.03.15 val PER: 0.3577
2026-01-03 21:37:49,749: t15.2024.03.17 val PER: 0.3291
2026-01-03 21:37:49,751: t15.2024.05.10 val PER: 0.3224
2026-01-03 21:37:49,753: t15.2024.06.14 val PER: 0.3360
2026-01-03 21:37:49,754: t15.2024.07.19 val PER: 0.4621
2026-01-03 21:37:49,756: t15.2024.07.21 val PER: 0.2910
2026-01-03 21:37:49,757: t15.2024.07.28 val PER: 0.3301
2026-01-03 21:37:49,759: t15.2025.01.10 val PER: 0.5331
2026-01-03 21:37:49,761: t15.2025.01.12 val PER: 0.3957
2026-01-03 21:37:49,762: t15.2025.03.14 val PER: 0.5311
2026-01-03 21:37:49,764: t15.2025.03.16 val PER: 0.3901
2026-01-03 21:37:49,766: t15.2025.03.30 val PER: 0.5494
2026-01-03 21:37:49,767: t15.2025.04.13 val PER: 0.4194
2026-01-03 21:37:49,769: New best val WER(1gram) 75.89% --> 70.05%
2026-01-03 21:37:50,095: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_2500
2026-01-03 21:37:58,449: Train batch 2600: loss: 38.34 grad norm: 83.83 time: 0.055
2026-01-03 21:38:15,594: Train batch 2800: loss: 28.75 grad norm: 77.31 time: 0.081
2026-01-03 21:38:32,677: Train batch 3000: loss: 34.02 grad norm: 76.97 time: 0.082
2026-01-03 21:38:32,679: Running test after training batch: 3000
2026-01-03 21:38:32,775: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:38:37,914: WER debug example
  GT : you can see the code at this point as well
  PR : yule end eke the good at this point is will
2026-01-03 21:38:37,946: WER debug example
  GT : how does it keep the cost down
  PR : houde does it heap the rost it
2026-01-03 21:38:39,679: Val batch 3000: PER (avg): 0.3063 CTC Loss (avg): 30.5572 WER(1gram): 65.99% (n=64) time: 6.998
2026-01-03 21:38:39,682: WER lens: avg_true_words=6.16 avg_pred_words=5.72 max_pred_words=11
2026-01-03 21:38:39,685: t15.2023.08.13 val PER: 0.2942
2026-01-03 21:38:39,687: t15.2023.08.18 val PER: 0.2406
2026-01-03 21:38:39,689: t15.2023.08.20 val PER: 0.2407
2026-01-03 21:38:39,693: t15.2023.08.25 val PER: 0.2139
2026-01-03 21:38:39,694: t15.2023.08.27 val PER: 0.3280
2026-01-03 21:38:39,696: t15.2023.09.01 val PER: 0.2078
2026-01-03 21:38:39,698: t15.2023.09.03 val PER: 0.3076
2026-01-03 21:38:39,700: t15.2023.09.24 val PER: 0.2342
2026-01-03 21:38:39,702: t15.2023.09.29 val PER: 0.2636
2026-01-03 21:38:39,704: t15.2023.10.01 val PER: 0.3032
2026-01-03 21:38:39,706: t15.2023.10.06 val PER: 0.2121
2026-01-03 21:38:39,707: t15.2023.10.08 val PER: 0.3789
2026-01-03 21:38:39,709: t15.2023.10.13 val PER: 0.3491
2026-01-03 21:38:39,711: t15.2023.10.15 val PER: 0.2861
2026-01-03 21:38:39,714: t15.2023.10.20 val PER: 0.2752
2026-01-03 21:38:39,716: t15.2023.10.22 val PER: 0.2327
2026-01-03 21:38:39,718: t15.2023.11.03 val PER: 0.2965
2026-01-03 21:38:39,720: t15.2023.11.04 val PER: 0.0887
2026-01-03 21:38:39,722: t15.2023.11.17 val PER: 0.1555
2026-01-03 21:38:39,723: t15.2023.11.19 val PER: 0.1317
2026-01-03 21:38:39,726: t15.2023.11.26 val PER: 0.3210
2026-01-03 21:38:39,727: t15.2023.12.03 val PER: 0.2920
2026-01-03 21:38:39,730: t15.2023.12.08 val PER: 0.2796
2026-01-03 21:38:39,732: t15.2023.12.10 val PER: 0.2418
2026-01-03 21:38:39,734: t15.2023.12.17 val PER: 0.3004
2026-01-03 21:38:39,735: t15.2023.12.29 val PER: 0.3034
2026-01-03 21:38:39,737: t15.2024.02.25 val PER: 0.2654
2026-01-03 21:38:39,739: t15.2024.03.08 val PER: 0.3755
2026-01-03 21:38:39,740: t15.2024.03.15 val PER: 0.3502
2026-01-03 21:38:39,742: t15.2024.03.17 val PER: 0.3110
2026-01-03 21:38:39,744: t15.2024.05.10 val PER: 0.3210
2026-01-03 21:38:39,745: t15.2024.06.14 val PER: 0.3312
2026-01-03 21:38:39,747: t15.2024.07.19 val PER: 0.4384
2026-01-03 21:38:39,749: t15.2024.07.21 val PER: 0.2690
2026-01-03 21:38:39,750: t15.2024.07.28 val PER: 0.2963
2026-01-03 21:38:39,752: t15.2025.01.10 val PER: 0.5138
2026-01-03 21:38:39,753: t15.2025.01.12 val PER: 0.3564
2026-01-03 21:38:39,755: t15.2025.03.14 val PER: 0.5118
2026-01-03 21:38:39,757: t15.2025.03.16 val PER: 0.3665
2026-01-03 21:38:39,758: t15.2025.03.30 val PER: 0.5345
2026-01-03 21:38:39,760: t15.2025.04.13 val PER: 0.3866
2026-01-03 21:38:39,761: New best val WER(1gram) 70.05% --> 65.99%
2026-01-03 21:38:40,090: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_3000
2026-01-03 21:38:57,224: Train batch 3200: loss: 30.08 grad norm: 71.86 time: 0.076
2026-01-03 21:39:14,678: Train batch 3400: loss: 21.45 grad norm: 61.14 time: 0.049
2026-01-03 21:39:23,450: Running test after training batch: 3500
2026-01-03 21:39:23,574: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:39:28,453: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point wheel
2026-01-03 21:39:28,485: WER debug example
  GT : how does it keep the cost down
  PR : houde des it eke thus cus nit
2026-01-03 21:39:30,193: Val batch 3500: PER (avg): 0.2931 CTC Loss (avg): 29.2587 WER(1gram): 70.56% (n=64) time: 6.741
2026-01-03 21:39:30,196: WER lens: avg_true_words=6.16 avg_pred_words=5.84 max_pred_words=11
2026-01-03 21:39:30,199: t15.2023.08.13 val PER: 0.2640
2026-01-03 21:39:30,201: t15.2023.08.18 val PER: 0.2406
2026-01-03 21:39:30,203: t15.2023.08.20 val PER: 0.2335
2026-01-03 21:39:30,204: t15.2023.08.25 val PER: 0.2048
2026-01-03 21:39:30,206: t15.2023.08.27 val PER: 0.3039
2026-01-03 21:39:30,208: t15.2023.09.01 val PER: 0.1924
2026-01-03 21:39:30,210: t15.2023.09.03 val PER: 0.2886
2026-01-03 21:39:30,211: t15.2023.09.24 val PER: 0.2136
2026-01-03 21:39:30,213: t15.2023.09.29 val PER: 0.2476
2026-01-03 21:39:30,214: t15.2023.10.01 val PER: 0.2900
2026-01-03 21:39:30,216: t15.2023.10.06 val PER: 0.2142
2026-01-03 21:39:30,218: t15.2023.10.08 val PER: 0.3640
2026-01-03 21:39:30,219: t15.2023.10.13 val PER: 0.3499
2026-01-03 21:39:30,221: t15.2023.10.15 val PER: 0.2868
2026-01-03 21:39:30,223: t15.2023.10.20 val PER: 0.2550
2026-01-03 21:39:30,225: t15.2023.10.22 val PER: 0.2294
2026-01-03 21:39:30,226: t15.2023.11.03 val PER: 0.2802
2026-01-03 21:39:30,228: t15.2023.11.04 val PER: 0.0887
2026-01-03 21:39:30,230: t15.2023.11.17 val PER: 0.1415
2026-01-03 21:39:30,231: t15.2023.11.19 val PER: 0.1138
2026-01-03 21:39:30,233: t15.2023.11.26 val PER: 0.3087
2026-01-03 21:39:30,234: t15.2023.12.03 val PER: 0.2731
2026-01-03 21:39:30,236: t15.2023.12.08 val PER: 0.2750
2026-01-03 21:39:30,238: t15.2023.12.10 val PER: 0.2273
2026-01-03 21:39:30,239: t15.2023.12.17 val PER: 0.2744
2026-01-03 21:39:30,241: t15.2023.12.29 val PER: 0.2889
2026-01-03 21:39:30,242: t15.2024.02.25 val PER: 0.2317
2026-01-03 21:39:30,244: t15.2024.03.08 val PER: 0.3542
2026-01-03 21:39:30,245: t15.2024.03.15 val PER: 0.3421
2026-01-03 21:39:30,247: t15.2024.03.17 val PER: 0.2964
2026-01-03 21:39:30,249: t15.2024.05.10 val PER: 0.3195
2026-01-03 21:39:30,251: t15.2024.06.14 val PER: 0.3013
2026-01-03 21:39:30,252: t15.2024.07.19 val PER: 0.4179
2026-01-03 21:39:30,254: t15.2024.07.21 val PER: 0.2538
2026-01-03 21:39:30,255: t15.2024.07.28 val PER: 0.3007
2026-01-03 21:39:30,257: t15.2025.01.10 val PER: 0.4890
2026-01-03 21:39:30,258: t15.2025.01.12 val PER: 0.3510
2026-01-03 21:39:30,261: t15.2025.03.14 val PER: 0.4926
2026-01-03 21:39:30,263: t15.2025.03.16 val PER: 0.3508
2026-01-03 21:39:30,264: t15.2025.03.30 val PER: 0.5046
2026-01-03 21:39:30,266: t15.2025.04.13 val PER: 0.3609
2026-01-03 21:39:30,496: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_3500
2026-01-03 21:39:39,129: Train batch 3600: loss: 25.46 grad norm: 66.45 time: 0.067
2026-01-03 21:39:56,203: Train batch 3800: loss: 28.76 grad norm: 74.13 time: 0.067
2026-01-03 21:40:13,576: Train batch 4000: loss: 22.38 grad norm: 58.56 time: 0.056
2026-01-03 21:40:13,579: Running test after training batch: 4000
2026-01-03 21:40:13,675: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:40:18,568: WER debug example
  GT : you can see the code at this point as well
  PR : yule aunt eke the code at this point is wheel
2026-01-03 21:40:18,600: WER debug example
  GT : how does it keep the cost down
  PR : out des it heap the cussed nit
2026-01-03 21:40:20,348: Val batch 4000: PER (avg): 0.2772 CTC Loss (avg): 27.4034 WER(1gram): 67.77% (n=64) time: 6.767
2026-01-03 21:40:20,351: WER lens: avg_true_words=6.16 avg_pred_words=5.81 max_pred_words=11
2026-01-03 21:40:20,354: t15.2023.08.13 val PER: 0.2661
2026-01-03 21:40:20,356: t15.2023.08.18 val PER: 0.2137
2026-01-03 21:40:20,359: t15.2023.08.20 val PER: 0.2288
2026-01-03 21:40:20,360: t15.2023.08.25 val PER: 0.1792
2026-01-03 21:40:20,362: t15.2023.08.27 val PER: 0.3023
2026-01-03 21:40:20,364: t15.2023.09.01 val PER: 0.1948
2026-01-03 21:40:20,366: t15.2023.09.03 val PER: 0.2577
2026-01-03 21:40:20,368: t15.2023.09.24 val PER: 0.2027
2026-01-03 21:40:20,370: t15.2023.09.29 val PER: 0.2317
2026-01-03 21:40:20,373: t15.2023.10.01 val PER: 0.2893
2026-01-03 21:40:20,375: t15.2023.10.06 val PER: 0.1905
2026-01-03 21:40:20,377: t15.2023.10.08 val PER: 0.3342
2026-01-03 21:40:20,379: t15.2023.10.13 val PER: 0.3390
2026-01-03 21:40:20,381: t15.2023.10.15 val PER: 0.2643
2026-01-03 21:40:20,383: t15.2023.10.20 val PER: 0.2852
2026-01-03 21:40:20,384: t15.2023.10.22 val PER: 0.2171
2026-01-03 21:40:20,386: t15.2023.11.03 val PER: 0.2558
2026-01-03 21:40:20,388: t15.2023.11.04 val PER: 0.0683
2026-01-03 21:40:20,390: t15.2023.11.17 val PER: 0.1182
2026-01-03 21:40:20,392: t15.2023.11.19 val PER: 0.1277
2026-01-03 21:40:20,393: t15.2023.11.26 val PER: 0.2899
2026-01-03 21:40:20,395: t15.2023.12.03 val PER: 0.2574
2026-01-03 21:40:20,397: t15.2023.12.08 val PER: 0.2610
2026-01-03 21:40:20,399: t15.2023.12.10 val PER: 0.2142
2026-01-03 21:40:20,401: t15.2023.12.17 val PER: 0.2599
2026-01-03 21:40:20,402: t15.2023.12.29 val PER: 0.2896
2026-01-03 21:40:20,404: t15.2024.02.25 val PER: 0.2528
2026-01-03 21:40:20,406: t15.2024.03.08 val PER: 0.3528
2026-01-03 21:40:20,408: t15.2024.03.15 val PER: 0.3246
2026-01-03 21:40:20,410: t15.2024.03.17 val PER: 0.2845
2026-01-03 21:40:20,412: t15.2024.05.10 val PER: 0.3046
2026-01-03 21:40:20,413: t15.2024.06.14 val PER: 0.2886
2026-01-03 21:40:20,415: t15.2024.07.19 val PER: 0.4061
2026-01-03 21:40:20,417: t15.2024.07.21 val PER: 0.2207
2026-01-03 21:40:20,419: t15.2024.07.28 val PER: 0.2713
2026-01-03 21:40:20,420: t15.2025.01.10 val PER: 0.4490
2026-01-03 21:40:20,422: t15.2025.01.12 val PER: 0.3179
2026-01-03 21:40:20,424: t15.2025.03.14 val PER: 0.4586
2026-01-03 21:40:20,426: t15.2025.03.16 val PER: 0.3351
2026-01-03 21:40:20,427: t15.2025.03.30 val PER: 0.4529
2026-01-03 21:40:20,429: t15.2025.04.13 val PER: 0.3466
2026-01-03 21:40:20,663: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_4000
2026-01-03 21:40:37,758: Train batch 4200: loss: 25.57 grad norm: 67.90 time: 0.079
2026-01-03 21:40:54,736: Train batch 4400: loss: 20.20 grad norm: 63.47 time: 0.067
2026-01-03 21:41:03,431: Running test after training batch: 4500
2026-01-03 21:41:03,527: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:41:08,762: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-03 21:41:08,795: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it heap the cost et
2026-01-03 21:41:10,487: Val batch 4500: PER (avg): 0.2626 CTC Loss (avg): 25.8599 WER(1gram): 63.96% (n=64) time: 7.054
2026-01-03 21:41:10,490: WER lens: avg_true_words=6.16 avg_pred_words=5.94 max_pred_words=11
2026-01-03 21:41:10,492: t15.2023.08.13 val PER: 0.2536
2026-01-03 21:41:10,494: t15.2023.08.18 val PER: 0.2037
2026-01-03 21:41:10,496: t15.2023.08.20 val PER: 0.2033
2026-01-03 21:41:10,497: t15.2023.08.25 val PER: 0.1627
2026-01-03 21:41:10,499: t15.2023.08.27 val PER: 0.2765
2026-01-03 21:41:10,501: t15.2023.09.01 val PER: 0.1753
2026-01-03 21:41:10,504: t15.2023.09.03 val PER: 0.2613
2026-01-03 21:41:10,507: t15.2023.09.24 val PER: 0.1978
2026-01-03 21:41:10,508: t15.2023.09.29 val PER: 0.2221
2026-01-03 21:41:10,510: t15.2023.10.01 val PER: 0.2787
2026-01-03 21:41:10,511: t15.2023.10.06 val PER: 0.1561
2026-01-03 21:41:10,513: t15.2023.10.08 val PER: 0.3288
2026-01-03 21:41:10,515: t15.2023.10.13 val PER: 0.3243
2026-01-03 21:41:10,517: t15.2023.10.15 val PER: 0.2492
2026-01-03 21:41:10,518: t15.2023.10.20 val PER: 0.2483
2026-01-03 21:41:10,520: t15.2023.10.22 val PER: 0.2004
2026-01-03 21:41:10,522: t15.2023.11.03 val PER: 0.2605
2026-01-03 21:41:10,523: t15.2023.11.04 val PER: 0.0648
2026-01-03 21:41:10,525: t15.2023.11.17 val PER: 0.1120
2026-01-03 21:41:10,526: t15.2023.11.19 val PER: 0.1038
2026-01-03 21:41:10,528: t15.2023.11.26 val PER: 0.2862
2026-01-03 21:41:10,529: t15.2023.12.03 val PER: 0.2437
2026-01-03 21:41:10,531: t15.2023.12.08 val PER: 0.2410
2026-01-03 21:41:10,532: t15.2023.12.10 val PER: 0.2050
2026-01-03 21:41:10,534: t15.2023.12.17 val PER: 0.2547
2026-01-03 21:41:10,536: t15.2023.12.29 val PER: 0.2663
2026-01-03 21:41:10,537: t15.2024.02.25 val PER: 0.2205
2026-01-03 21:41:10,539: t15.2024.03.08 val PER: 0.3286
2026-01-03 21:41:10,541: t15.2024.03.15 val PER: 0.3152
2026-01-03 21:41:10,542: t15.2024.03.17 val PER: 0.2615
2026-01-03 21:41:10,544: t15.2024.05.10 val PER: 0.2764
2026-01-03 21:41:10,545: t15.2024.06.14 val PER: 0.2823
2026-01-03 21:41:10,547: t15.2024.07.19 val PER: 0.3744
2026-01-03 21:41:10,550: t15.2024.07.21 val PER: 0.2048
2026-01-03 21:41:10,551: t15.2024.07.28 val PER: 0.2596
2026-01-03 21:41:10,553: t15.2025.01.10 val PER: 0.4435
2026-01-03 21:41:10,554: t15.2025.01.12 val PER: 0.3064
2026-01-03 21:41:10,556: t15.2025.03.14 val PER: 0.4334
2026-01-03 21:41:10,558: t15.2025.03.16 val PER: 0.3154
2026-01-03 21:41:10,559: t15.2025.03.30 val PER: 0.4563
2026-01-03 21:41:10,561: t15.2025.04.13 val PER: 0.3338
2026-01-03 21:41:10,563: New best val WER(1gram) 65.99% --> 63.96%
2026-01-03 21:41:10,888: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_4500
2026-01-03 21:41:19,406: Train batch 4600: loss: 24.80 grad norm: 79.61 time: 0.062
2026-01-03 21:41:36,749: Train batch 4800: loss: 17.27 grad norm: 60.36 time: 0.063
2026-01-03 21:41:53,881: Train batch 5000: loss: 36.53 grad norm: 82.98 time: 0.064
2026-01-03 21:41:53,883: Running test after training batch: 5000
2026-01-03 21:41:54,008: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:41:58,870: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the code at this point is will
2026-01-03 21:41:58,902: WER debug example
  GT : how does it keep the cost down
  PR : houde des it heap the cussed nit
2026-01-03 21:42:00,617: Val batch 5000: PER (avg): 0.2525 CTC Loss (avg): 24.9594 WER(1gram): 63.96% (n=64) time: 6.731
2026-01-03 21:42:00,619: WER lens: avg_true_words=6.16 avg_pred_words=5.95 max_pred_words=11
2026-01-03 21:42:00,622: t15.2023.08.13 val PER: 0.2297
2026-01-03 21:42:00,624: t15.2023.08.18 val PER: 0.1894
2026-01-03 21:42:00,626: t15.2023.08.20 val PER: 0.1962
2026-01-03 21:42:00,628: t15.2023.08.25 val PER: 0.1491
2026-01-03 21:42:00,631: t15.2023.08.27 val PER: 0.2588
2026-01-03 21:42:00,633: t15.2023.09.01 val PER: 0.1696
2026-01-03 21:42:00,634: t15.2023.09.03 val PER: 0.2482
2026-01-03 21:42:00,636: t15.2023.09.24 val PER: 0.1978
2026-01-03 21:42:00,638: t15.2023.09.29 val PER: 0.1991
2026-01-03 21:42:00,640: t15.2023.10.01 val PER: 0.2616
2026-01-03 21:42:00,641: t15.2023.10.06 val PER: 0.1787
2026-01-03 21:42:00,643: t15.2023.10.08 val PER: 0.3302
2026-01-03 21:42:00,645: t15.2023.10.13 val PER: 0.3072
2026-01-03 21:42:00,647: t15.2023.10.15 val PER: 0.2465
2026-01-03 21:42:00,649: t15.2023.10.20 val PER: 0.2483
2026-01-03 21:42:00,650: t15.2023.10.22 val PER: 0.1960
2026-01-03 21:42:00,652: t15.2023.11.03 val PER: 0.2490
2026-01-03 21:42:00,655: t15.2023.11.04 val PER: 0.0717
2026-01-03 21:42:00,657: t15.2023.11.17 val PER: 0.0918
2026-01-03 21:42:00,658: t15.2023.11.19 val PER: 0.0898
2026-01-03 21:42:00,660: t15.2023.11.26 val PER: 0.2732
2026-01-03 21:42:00,661: t15.2023.12.03 val PER: 0.2321
2026-01-03 21:42:00,663: t15.2023.12.08 val PER: 0.2230
2026-01-03 21:42:00,665: t15.2023.12.10 val PER: 0.1708
2026-01-03 21:42:00,667: t15.2023.12.17 val PER: 0.2526
2026-01-03 21:42:00,669: t15.2023.12.29 val PER: 0.2539
2026-01-03 21:42:00,671: t15.2024.02.25 val PER: 0.2093
2026-01-03 21:42:00,672: t15.2024.03.08 val PER: 0.3300
2026-01-03 21:42:00,674: t15.2024.03.15 val PER: 0.2989
2026-01-03 21:42:00,676: t15.2024.03.17 val PER: 0.2678
2026-01-03 21:42:00,679: t15.2024.05.10 val PER: 0.2689
2026-01-03 21:42:00,683: t15.2024.06.14 val PER: 0.2823
2026-01-03 21:42:00,685: t15.2024.07.19 val PER: 0.3639
2026-01-03 21:42:00,686: t15.2024.07.21 val PER: 0.2055
2026-01-03 21:42:00,688: t15.2024.07.28 val PER: 0.2471
2026-01-03 21:42:00,690: t15.2025.01.10 val PER: 0.4518
2026-01-03 21:42:00,691: t15.2025.01.12 val PER: 0.2810
2026-01-03 21:42:00,693: t15.2025.03.14 val PER: 0.4320
2026-01-03 21:42:00,694: t15.2025.03.16 val PER: 0.2958
2026-01-03 21:42:00,696: t15.2025.03.30 val PER: 0.4391
2026-01-03 21:42:00,697: t15.2025.04.13 val PER: 0.3238
2026-01-03 21:42:00,927: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_5000
2026-01-03 21:42:18,200: Train batch 5200: loss: 20.72 grad norm: 69.44 time: 0.052
2026-01-03 21:42:35,274: Train batch 5400: loss: 21.29 grad norm: 64.32 time: 0.068
2026-01-03 21:42:43,918: Running test after training batch: 5500
2026-01-03 21:42:44,074: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:42:48,962: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point will
2026-01-03 21:42:48,994: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cussed nett
2026-01-03 21:42:50,673: Val batch 5500: PER (avg): 0.2421 CTC Loss (avg): 23.8344 WER(1gram): 60.15% (n=64) time: 6.751
2026-01-03 21:42:50,675: WER lens: avg_true_words=6.16 avg_pred_words=5.95 max_pred_words=11
2026-01-03 21:42:50,679: t15.2023.08.13 val PER: 0.2110
2026-01-03 21:42:50,681: t15.2023.08.18 val PER: 0.1978
2026-01-03 21:42:50,683: t15.2023.08.20 val PER: 0.1867
2026-01-03 21:42:50,685: t15.2023.08.25 val PER: 0.1401
2026-01-03 21:42:50,687: t15.2023.08.27 val PER: 0.2701
2026-01-03 21:42:50,689: t15.2023.09.01 val PER: 0.1550
2026-01-03 21:42:50,691: t15.2023.09.03 val PER: 0.2387
2026-01-03 21:42:50,693: t15.2023.09.24 val PER: 0.1893
2026-01-03 21:42:50,695: t15.2023.09.29 val PER: 0.2042
2026-01-03 21:42:50,697: t15.2023.10.01 val PER: 0.2477
2026-01-03 21:42:50,699: t15.2023.10.06 val PER: 0.1582
2026-01-03 21:42:50,700: t15.2023.10.08 val PER: 0.3099
2026-01-03 21:42:50,702: t15.2023.10.13 val PER: 0.3049
2026-01-03 21:42:50,704: t15.2023.10.15 val PER: 0.2215
2026-01-03 21:42:50,706: t15.2023.10.20 val PER: 0.2282
2026-01-03 21:42:50,707: t15.2023.10.22 val PER: 0.1793
2026-01-03 21:42:50,709: t15.2023.11.03 val PER: 0.2429
2026-01-03 21:42:50,711: t15.2023.11.04 val PER: 0.0717
2026-01-03 21:42:50,712: t15.2023.11.17 val PER: 0.1011
2026-01-03 21:42:50,714: t15.2023.11.19 val PER: 0.0858
2026-01-03 21:42:50,716: t15.2023.11.26 val PER: 0.2572
2026-01-03 21:42:50,717: t15.2023.12.03 val PER: 0.2048
2026-01-03 21:42:50,719: t15.2023.12.08 val PER: 0.2257
2026-01-03 21:42:50,721: t15.2023.12.10 val PER: 0.1708
2026-01-03 21:42:50,723: t15.2023.12.17 val PER: 0.2578
2026-01-03 21:42:50,724: t15.2023.12.29 val PER: 0.2409
2026-01-03 21:42:50,726: t15.2024.02.25 val PER: 0.1994
2026-01-03 21:42:50,728: t15.2024.03.08 val PER: 0.3243
2026-01-03 21:42:50,729: t15.2024.03.15 val PER: 0.2814
2026-01-03 21:42:50,731: t15.2024.03.17 val PER: 0.2545
2026-01-03 21:42:50,733: t15.2024.05.10 val PER: 0.2719
2026-01-03 21:42:50,734: t15.2024.06.14 val PER: 0.2476
2026-01-03 21:42:50,736: t15.2024.07.19 val PER: 0.3645
2026-01-03 21:42:50,738: t15.2024.07.21 val PER: 0.1897
2026-01-03 21:42:50,740: t15.2024.07.28 val PER: 0.2463
2026-01-03 21:42:50,741: t15.2025.01.10 val PER: 0.4229
2026-01-03 21:42:50,743: t15.2025.01.12 val PER: 0.2733
2026-01-03 21:42:50,745: t15.2025.03.14 val PER: 0.3876
2026-01-03 21:42:50,747: t15.2025.03.16 val PER: 0.2880
2026-01-03 21:42:50,748: t15.2025.03.30 val PER: 0.3966
2026-01-03 21:42:50,750: t15.2025.04.13 val PER: 0.3267
2026-01-03 21:42:50,752: New best val WER(1gram) 63.96% --> 60.15%
2026-01-03 21:42:51,083: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_5500
2026-01-03 21:42:59,643: Train batch 5600: loss: 23.79 grad norm: 67.70 time: 0.062
2026-01-03 21:43:17,109: Train batch 5800: loss: 17.37 grad norm: 62.96 time: 0.081
2026-01-03 21:43:34,284: Train batch 6000: loss: 17.60 grad norm: 62.02 time: 0.049
2026-01-03 21:43:34,286: Running test after training batch: 6000
2026-01-03 21:43:34,389: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:43:39,487: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-03 21:43:39,525: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost get
2026-01-03 21:43:41,229: Val batch 6000: PER (avg): 0.2396 CTC Loss (avg): 23.4092 WER(1gram): 60.41% (n=64) time: 6.940
2026-01-03 21:43:41,231: WER lens: avg_true_words=6.16 avg_pred_words=6.06 max_pred_words=11
2026-01-03 21:43:41,235: t15.2023.08.13 val PER: 0.2048
2026-01-03 21:43:41,237: t15.2023.08.18 val PER: 0.1920
2026-01-03 21:43:41,239: t15.2023.08.20 val PER: 0.1930
2026-01-03 21:43:41,241: t15.2023.08.25 val PER: 0.1446
2026-01-03 21:43:41,243: t15.2023.08.27 val PER: 0.2878
2026-01-03 21:43:41,245: t15.2023.09.01 val PER: 0.1510
2026-01-03 21:43:41,246: t15.2023.09.03 val PER: 0.2423
2026-01-03 21:43:41,248: t15.2023.09.24 val PER: 0.1893
2026-01-03 21:43:41,250: t15.2023.09.29 val PER: 0.1991
2026-01-03 21:43:41,252: t15.2023.10.01 val PER: 0.2503
2026-01-03 21:43:41,254: t15.2023.10.06 val PER: 0.1593
2026-01-03 21:43:41,255: t15.2023.10.08 val PER: 0.3194
2026-01-03 21:43:41,257: t15.2023.10.13 val PER: 0.3111
2026-01-03 21:43:41,259: t15.2023.10.15 val PER: 0.2340
2026-01-03 21:43:41,261: t15.2023.10.20 val PER: 0.2483
2026-01-03 21:43:41,263: t15.2023.10.22 val PER: 0.1915
2026-01-03 21:43:41,265: t15.2023.11.03 val PER: 0.2395
2026-01-03 21:43:41,267: t15.2023.11.04 val PER: 0.0683
2026-01-03 21:43:41,269: t15.2023.11.17 val PER: 0.0964
2026-01-03 21:43:41,271: t15.2023.11.19 val PER: 0.0798
2026-01-03 21:43:41,273: t15.2023.11.26 val PER: 0.2572
2026-01-03 21:43:41,275: t15.2023.12.03 val PER: 0.1964
2026-01-03 21:43:41,276: t15.2023.12.08 val PER: 0.2190
2026-01-03 21:43:41,278: t15.2023.12.10 val PER: 0.1827
2026-01-03 21:43:41,280: t15.2023.12.17 val PER: 0.2235
2026-01-03 21:43:41,282: t15.2023.12.29 val PER: 0.2430
2026-01-03 21:43:41,284: t15.2024.02.25 val PER: 0.1910
2026-01-03 21:43:41,285: t15.2024.03.08 val PER: 0.3172
2026-01-03 21:43:41,287: t15.2024.03.15 val PER: 0.2889
2026-01-03 21:43:41,289: t15.2024.03.17 val PER: 0.2559
2026-01-03 21:43:41,291: t15.2024.05.10 val PER: 0.2437
2026-01-03 21:43:41,292: t15.2024.06.14 val PER: 0.2397
2026-01-03 21:43:41,294: t15.2024.07.19 val PER: 0.3566
2026-01-03 21:43:41,296: t15.2024.07.21 val PER: 0.1848
2026-01-03 21:43:41,297: t15.2024.07.28 val PER: 0.2250
2026-01-03 21:43:41,299: t15.2025.01.10 val PER: 0.4160
2026-01-03 21:43:41,300: t15.2025.01.12 val PER: 0.2533
2026-01-03 21:43:41,302: t15.2025.03.14 val PER: 0.3979
2026-01-03 21:43:41,304: t15.2025.03.16 val PER: 0.2840
2026-01-03 21:43:41,305: t15.2025.03.30 val PER: 0.3954
2026-01-03 21:43:41,307: t15.2025.04.13 val PER: 0.3067
2026-01-03 21:43:41,543: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_6000
2026-01-03 21:43:58,341: Train batch 6200: loss: 20.13 grad norm: 61.77 time: 0.070
2026-01-03 21:44:15,314: Train batch 6400: loss: 22.77 grad norm: 69.18 time: 0.062
2026-01-03 21:44:23,680: Running test after training batch: 6500
2026-01-03 21:44:23,776: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:44:28,642: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the could at this point is will
2026-01-03 21:44:28,676: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost nett
2026-01-03 21:44:30,350: Val batch 6500: PER (avg): 0.2292 CTC Loss (avg): 22.5333 WER(1gram): 58.63% (n=64) time: 6.667
2026-01-03 21:44:30,352: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 21:44:30,355: t15.2023.08.13 val PER: 0.1985
2026-01-03 21:44:30,357: t15.2023.08.18 val PER: 0.1702
2026-01-03 21:44:30,359: t15.2023.08.20 val PER: 0.1835
2026-01-03 21:44:30,362: t15.2023.08.25 val PER: 0.1386
2026-01-03 21:44:30,365: t15.2023.08.27 val PER: 0.2621
2026-01-03 21:44:30,367: t15.2023.09.01 val PER: 0.1323
2026-01-03 21:44:30,368: t15.2023.09.03 val PER: 0.2268
2026-01-03 21:44:30,370: t15.2023.09.24 val PER: 0.1845
2026-01-03 21:44:30,372: t15.2023.09.29 val PER: 0.1876
2026-01-03 21:44:30,373: t15.2023.10.01 val PER: 0.2365
2026-01-03 21:44:30,375: t15.2023.10.06 val PER: 0.1475
2026-01-03 21:44:30,377: t15.2023.10.08 val PER: 0.3045
2026-01-03 21:44:30,378: t15.2023.10.13 val PER: 0.2847
2026-01-03 21:44:30,380: t15.2023.10.15 val PER: 0.2208
2026-01-03 21:44:30,381: t15.2023.10.20 val PER: 0.2248
2026-01-03 21:44:30,383: t15.2023.10.22 val PER: 0.1793
2026-01-03 21:44:30,384: t15.2023.11.03 val PER: 0.2436
2026-01-03 21:44:30,386: t15.2023.11.04 val PER: 0.0648
2026-01-03 21:44:30,388: t15.2023.11.17 val PER: 0.0918
2026-01-03 21:44:30,389: t15.2023.11.19 val PER: 0.0858
2026-01-03 21:44:30,391: t15.2023.11.26 val PER: 0.2543
2026-01-03 21:44:30,392: t15.2023.12.03 val PER: 0.1985
2026-01-03 21:44:30,394: t15.2023.12.08 val PER: 0.2017
2026-01-03 21:44:30,396: t15.2023.12.10 val PER: 0.1629
2026-01-03 21:44:30,398: t15.2023.12.17 val PER: 0.2152
2026-01-03 21:44:30,399: t15.2023.12.29 val PER: 0.2327
2026-01-03 21:44:30,401: t15.2024.02.25 val PER: 0.1882
2026-01-03 21:44:30,402: t15.2024.03.08 val PER: 0.3186
2026-01-03 21:44:30,404: t15.2024.03.15 val PER: 0.2814
2026-01-03 21:44:30,406: t15.2024.03.17 val PER: 0.2392
2026-01-03 21:44:30,407: t15.2024.05.10 val PER: 0.2437
2026-01-03 21:44:30,409: t15.2024.06.14 val PER: 0.2240
2026-01-03 21:44:30,411: t15.2024.07.19 val PER: 0.3329
2026-01-03 21:44:30,412: t15.2024.07.21 val PER: 0.1772
2026-01-03 21:44:30,414: t15.2024.07.28 val PER: 0.2228
2026-01-03 21:44:30,416: t15.2025.01.10 val PER: 0.4160
2026-01-03 21:44:30,417: t15.2025.01.12 val PER: 0.2510
2026-01-03 21:44:30,419: t15.2025.03.14 val PER: 0.3935
2026-01-03 21:44:30,420: t15.2025.03.16 val PER: 0.2906
2026-01-03 21:44:30,422: t15.2025.03.30 val PER: 0.3667
2026-01-03 21:44:30,424: t15.2025.04.13 val PER: 0.2981
2026-01-03 21:44:30,426: New best val WER(1gram) 60.15% --> 58.63%
2026-01-03 21:44:30,760: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_6500
2026-01-03 21:44:39,354: Train batch 6600: loss: 15.20 grad norm: 60.90 time: 0.045
2026-01-03 21:44:56,521: Train batch 6800: loss: 18.78 grad norm: 60.52 time: 0.048
2026-01-03 21:45:13,898: Train batch 7000: loss: 21.83 grad norm: 71.89 time: 0.060
2026-01-03 21:45:13,900: Running test after training batch: 7000
2026-01-03 21:45:14,053: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:45:18,886: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point is will
2026-01-03 21:45:18,920: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost get
2026-01-03 21:45:20,654: Val batch 7000: PER (avg): 0.2199 CTC Loss (avg): 21.6394 WER(1gram): 57.11% (n=64) time: 6.751
2026-01-03 21:45:20,656: WER lens: avg_true_words=6.16 avg_pred_words=6.05 max_pred_words=11
2026-01-03 21:45:20,659: t15.2023.08.13 val PER: 0.1861
2026-01-03 21:45:20,661: t15.2023.08.18 val PER: 0.1710
2026-01-03 21:45:20,664: t15.2023.08.20 val PER: 0.1716
2026-01-03 21:45:20,666: t15.2023.08.25 val PER: 0.1325
2026-01-03 21:45:20,668: t15.2023.08.27 val PER: 0.2476
2026-01-03 21:45:20,670: t15.2023.09.01 val PER: 0.1315
2026-01-03 21:45:20,671: t15.2023.09.03 val PER: 0.2150
2026-01-03 21:45:20,673: t15.2023.09.24 val PER: 0.1699
2026-01-03 21:45:20,675: t15.2023.09.29 val PER: 0.1889
2026-01-03 21:45:20,677: t15.2023.10.01 val PER: 0.2246
2026-01-03 21:45:20,678: t15.2023.10.06 val PER: 0.1324
2026-01-03 21:45:20,680: t15.2023.10.08 val PER: 0.2963
2026-01-03 21:45:20,682: t15.2023.10.13 val PER: 0.2700
2026-01-03 21:45:20,684: t15.2023.10.15 val PER: 0.2109
2026-01-03 21:45:20,685: t15.2023.10.20 val PER: 0.2248
2026-01-03 21:45:20,687: t15.2023.10.22 val PER: 0.1682
2026-01-03 21:45:20,689: t15.2023.11.03 val PER: 0.2191
2026-01-03 21:45:20,691: t15.2023.11.04 val PER: 0.0614
2026-01-03 21:45:20,693: t15.2023.11.17 val PER: 0.0747
2026-01-03 21:45:20,695: t15.2023.11.19 val PER: 0.0719
2026-01-03 21:45:20,697: t15.2023.11.26 val PER: 0.2239
2026-01-03 21:45:20,699: t15.2023.12.03 val PER: 0.1796
2026-01-03 21:45:20,701: t15.2023.12.08 val PER: 0.1997
2026-01-03 21:45:20,702: t15.2023.12.10 val PER: 0.1643
2026-01-03 21:45:20,704: t15.2023.12.17 val PER: 0.2069
2026-01-03 21:45:20,706: t15.2023.12.29 val PER: 0.2306
2026-01-03 21:45:20,707: t15.2024.02.25 val PER: 0.1742
2026-01-03 21:45:20,709: t15.2024.03.08 val PER: 0.3058
2026-01-03 21:45:20,711: t15.2024.03.15 val PER: 0.2627
2026-01-03 21:45:20,712: t15.2024.03.17 val PER: 0.2197
2026-01-03 21:45:20,714: t15.2024.05.10 val PER: 0.2481
2026-01-03 21:45:20,715: t15.2024.06.14 val PER: 0.2445
2026-01-03 21:45:20,716: t15.2024.07.19 val PER: 0.3388
2026-01-03 21:45:20,718: t15.2024.07.21 val PER: 0.1655
2026-01-03 21:45:20,720: t15.2024.07.28 val PER: 0.2037
2026-01-03 21:45:20,722: t15.2025.01.10 val PER: 0.3953
2026-01-03 21:45:20,724: t15.2025.01.12 val PER: 0.2517
2026-01-03 21:45:20,725: t15.2025.03.14 val PER: 0.3950
2026-01-03 21:45:20,726: t15.2025.03.16 val PER: 0.2670
2026-01-03 21:45:20,728: t15.2025.03.30 val PER: 0.3736
2026-01-03 21:45:20,729: t15.2025.04.13 val PER: 0.3010
2026-01-03 21:45:20,731: New best val WER(1gram) 58.63% --> 57.11%
2026-01-03 21:45:21,059: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_7000
2026-01-03 21:45:38,528: Train batch 7200: loss: 17.83 grad norm: 61.75 time: 0.079
2026-01-03 21:45:55,693: Train batch 7400: loss: 17.58 grad norm: 59.29 time: 0.076
2026-01-03 21:46:04,220: Running test after training batch: 7500
2026-01-03 21:46:04,336: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:46:09,425: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sze the could at this point is will
2026-01-03 21:46:09,458: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cussed it
2026-01-03 21:46:11,179: Val batch 7500: PER (avg): 0.2155 CTC Loss (avg): 21.0838 WER(1gram): 60.66% (n=64) time: 6.957
2026-01-03 21:46:11,181: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 21:46:11,183: t15.2023.08.13 val PER: 0.1913
2026-01-03 21:46:11,185: t15.2023.08.18 val PER: 0.1668
2026-01-03 21:46:11,187: t15.2023.08.20 val PER: 0.1700
2026-01-03 21:46:11,189: t15.2023.08.25 val PER: 0.1340
2026-01-03 21:46:11,191: t15.2023.08.27 val PER: 0.2476
2026-01-03 21:46:11,192: t15.2023.09.01 val PER: 0.1299
2026-01-03 21:46:11,194: t15.2023.09.03 val PER: 0.2173
2026-01-03 21:46:11,196: t15.2023.09.24 val PER: 0.1663
2026-01-03 21:46:11,198: t15.2023.09.29 val PER: 0.1774
2026-01-03 21:46:11,199: t15.2023.10.01 val PER: 0.2338
2026-01-03 21:46:11,201: t15.2023.10.06 val PER: 0.1378
2026-01-03 21:46:11,203: t15.2023.10.08 val PER: 0.2882
2026-01-03 21:46:11,204: t15.2023.10.13 val PER: 0.2715
2026-01-03 21:46:11,206: t15.2023.10.15 val PER: 0.2123
2026-01-03 21:46:11,208: t15.2023.10.20 val PER: 0.2148
2026-01-03 21:46:11,210: t15.2023.10.22 val PER: 0.1670
2026-01-03 21:46:11,211: t15.2023.11.03 val PER: 0.2151
2026-01-03 21:46:11,213: t15.2023.11.04 val PER: 0.0614
2026-01-03 21:46:11,214: t15.2023.11.17 val PER: 0.0778
2026-01-03 21:46:11,216: t15.2023.11.19 val PER: 0.0639
2026-01-03 21:46:11,217: t15.2023.11.26 val PER: 0.2254
2026-01-03 21:46:11,219: t15.2023.12.03 val PER: 0.1838
2026-01-03 21:46:11,220: t15.2023.12.08 val PER: 0.1858
2026-01-03 21:46:11,222: t15.2023.12.10 val PER: 0.1511
2026-01-03 21:46:11,224: t15.2023.12.17 val PER: 0.2110
2026-01-03 21:46:11,226: t15.2023.12.29 val PER: 0.2224
2026-01-03 21:46:11,227: t15.2024.02.25 val PER: 0.1671
2026-01-03 21:46:11,229: t15.2024.03.08 val PER: 0.2987
2026-01-03 21:46:11,230: t15.2024.03.15 val PER: 0.2602
2026-01-03 21:46:11,232: t15.2024.03.17 val PER: 0.2064
2026-01-03 21:46:11,234: t15.2024.05.10 val PER: 0.2259
2026-01-03 21:46:11,235: t15.2024.06.14 val PER: 0.2224
2026-01-03 21:46:11,237: t15.2024.07.19 val PER: 0.3171
2026-01-03 21:46:11,239: t15.2024.07.21 val PER: 0.1655
2026-01-03 21:46:11,240: t15.2024.07.28 val PER: 0.2015
2026-01-03 21:46:11,242: t15.2025.01.10 val PER: 0.3857
2026-01-03 21:46:11,243: t15.2025.01.12 val PER: 0.2356
2026-01-03 21:46:11,245: t15.2025.03.14 val PER: 0.3772
2026-01-03 21:46:11,246: t15.2025.03.16 val PER: 0.2853
2026-01-03 21:46:11,248: t15.2025.03.30 val PER: 0.3713
2026-01-03 21:46:11,249: t15.2025.04.13 val PER: 0.2953
2026-01-03 21:46:11,480: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_7500
2026-01-03 21:46:19,833: Train batch 7600: loss: 19.51 grad norm: 64.77 time: 0.069
2026-01-03 21:46:36,779: Train batch 7800: loss: 18.75 grad norm: 68.43 time: 0.055
2026-01-03 21:46:54,103: Train batch 8000: loss: 14.45 grad norm: 56.17 time: 0.072
2026-01-03 21:46:54,107: Running test after training batch: 8000
2026-01-03 21:46:54,203: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:46:59,023: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point us will
2026-01-03 21:46:59,056: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost nett
2026-01-03 21:47:00,804: Val batch 8000: PER (avg): 0.2094 CTC Loss (avg): 20.5395 WER(1gram): 58.38% (n=64) time: 6.695
2026-01-03 21:47:00,807: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 21:47:00,810: t15.2023.08.13 val PER: 0.1850
2026-01-03 21:47:00,812: t15.2023.08.18 val PER: 0.1576
2026-01-03 21:47:00,814: t15.2023.08.20 val PER: 0.1644
2026-01-03 21:47:00,816: t15.2023.08.25 val PER: 0.1370
2026-01-03 21:47:00,818: t15.2023.08.27 val PER: 0.2347
2026-01-03 21:47:00,820: t15.2023.09.01 val PER: 0.1250
2026-01-03 21:47:00,821: t15.2023.09.03 val PER: 0.2138
2026-01-03 21:47:00,823: t15.2023.09.24 val PER: 0.1748
2026-01-03 21:47:00,826: t15.2023.09.29 val PER: 0.1666
2026-01-03 21:47:00,828: t15.2023.10.01 val PER: 0.2259
2026-01-03 21:47:00,830: t15.2023.10.06 val PER: 0.1346
2026-01-03 21:47:00,831: t15.2023.10.08 val PER: 0.2950
2026-01-03 21:47:00,833: t15.2023.10.13 val PER: 0.2653
2026-01-03 21:47:00,835: t15.2023.10.15 val PER: 0.2070
2026-01-03 21:47:00,837: t15.2023.10.20 val PER: 0.2248
2026-01-03 21:47:00,839: t15.2023.10.22 val PER: 0.1715
2026-01-03 21:47:00,841: t15.2023.11.03 val PER: 0.2246
2026-01-03 21:47:00,842: t15.2023.11.04 val PER: 0.0444
2026-01-03 21:47:00,844: t15.2023.11.17 val PER: 0.0747
2026-01-03 21:47:00,847: t15.2023.11.19 val PER: 0.0679
2026-01-03 21:47:00,849: t15.2023.11.26 val PER: 0.2123
2026-01-03 21:47:00,851: t15.2023.12.03 val PER: 0.1744
2026-01-03 21:47:00,852: t15.2023.12.08 val PER: 0.1771
2026-01-03 21:47:00,854: t15.2023.12.10 val PER: 0.1459
2026-01-03 21:47:00,855: t15.2023.12.17 val PER: 0.1902
2026-01-03 21:47:00,857: t15.2023.12.29 val PER: 0.2093
2026-01-03 21:47:00,858: t15.2024.02.25 val PER: 0.1601
2026-01-03 21:47:00,860: t15.2024.03.08 val PER: 0.2930
2026-01-03 21:47:00,862: t15.2024.03.15 val PER: 0.2595
2026-01-03 21:47:00,865: t15.2024.03.17 val PER: 0.1994
2026-01-03 21:47:00,866: t15.2024.05.10 val PER: 0.2169
2026-01-03 21:47:00,868: t15.2024.06.14 val PER: 0.2208
2026-01-03 21:47:00,869: t15.2024.07.19 val PER: 0.3098
2026-01-03 21:47:00,871: t15.2024.07.21 val PER: 0.1524
2026-01-03 21:47:00,872: t15.2024.07.28 val PER: 0.1897
2026-01-03 21:47:00,874: t15.2025.01.10 val PER: 0.3623
2026-01-03 21:47:00,875: t15.2025.01.12 val PER: 0.2271
2026-01-03 21:47:00,877: t15.2025.03.14 val PER: 0.3728
2026-01-03 21:47:00,878: t15.2025.03.16 val PER: 0.2814
2026-01-03 21:47:00,880: t15.2025.03.30 val PER: 0.3701
2026-01-03 21:47:00,881: t15.2025.04.13 val PER: 0.2896
2026-01-03 21:47:01,116: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_8000
2026-01-03 21:47:18,339: Train batch 8200: loss: 11.71 grad norm: 50.48 time: 0.054
2026-01-03 21:47:35,411: Train batch 8400: loss: 13.36 grad norm: 53.09 time: 0.064
2026-01-03 21:47:44,088: Running test after training batch: 8500
2026-01-03 21:47:44,192: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:47:49,036: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-03 21:47:49,071: WER debug example
  GT : how does it keep the cost down
  PR : how just it keep the cost ent
2026-01-03 21:47:50,867: Val batch 8500: PER (avg): 0.2072 CTC Loss (avg): 20.1706 WER(1gram): 57.11% (n=64) time: 6.777
2026-01-03 21:47:50,870: WER lens: avg_true_words=6.16 avg_pred_words=6.09 max_pred_words=11
2026-01-03 21:47:50,872: t15.2023.08.13 val PER: 0.1705
2026-01-03 21:47:50,874: t15.2023.08.18 val PER: 0.1660
2026-01-03 21:47:50,876: t15.2023.08.20 val PER: 0.1620
2026-01-03 21:47:50,878: t15.2023.08.25 val PER: 0.1310
2026-01-03 21:47:50,880: t15.2023.08.27 val PER: 0.2347
2026-01-03 21:47:50,882: t15.2023.09.01 val PER: 0.1242
2026-01-03 21:47:50,883: t15.2023.09.03 val PER: 0.2090
2026-01-03 21:47:50,885: t15.2023.09.24 val PER: 0.1675
2026-01-03 21:47:50,887: t15.2023.09.29 val PER: 0.1704
2026-01-03 21:47:50,889: t15.2023.10.01 val PER: 0.2107
2026-01-03 21:47:50,890: t15.2023.10.06 val PER: 0.1195
2026-01-03 21:47:50,892: t15.2023.10.08 val PER: 0.2950
2026-01-03 21:47:50,894: t15.2023.10.13 val PER: 0.2661
2026-01-03 21:47:50,895: t15.2023.10.15 val PER: 0.2116
2026-01-03 21:47:50,897: t15.2023.10.20 val PER: 0.2114
2026-01-03 21:47:50,899: t15.2023.10.22 val PER: 0.1670
2026-01-03 21:47:50,901: t15.2023.11.03 val PER: 0.2157
2026-01-03 21:47:50,903: t15.2023.11.04 val PER: 0.0580
2026-01-03 21:47:50,904: t15.2023.11.17 val PER: 0.0809
2026-01-03 21:47:50,906: t15.2023.11.19 val PER: 0.0699
2026-01-03 21:47:50,908: t15.2023.11.26 val PER: 0.2145
2026-01-03 21:47:50,909: t15.2023.12.03 val PER: 0.1817
2026-01-03 21:47:50,911: t15.2023.12.08 val PER: 0.1804
2026-01-03 21:47:50,913: t15.2023.12.10 val PER: 0.1459
2026-01-03 21:47:50,914: t15.2023.12.17 val PER: 0.1965
2026-01-03 21:47:50,916: t15.2023.12.29 val PER: 0.2093
2026-01-03 21:47:50,917: t15.2024.02.25 val PER: 0.1629
2026-01-03 21:47:50,919: t15.2024.03.08 val PER: 0.2902
2026-01-03 21:47:50,921: t15.2024.03.15 val PER: 0.2502
2026-01-03 21:47:50,922: t15.2024.03.17 val PER: 0.2057
2026-01-03 21:47:50,924: t15.2024.05.10 val PER: 0.2199
2026-01-03 21:47:50,925: t15.2024.06.14 val PER: 0.2145
2026-01-03 21:47:50,927: t15.2024.07.19 val PER: 0.3085
2026-01-03 21:47:50,928: t15.2024.07.21 val PER: 0.1503
2026-01-03 21:47:50,930: t15.2024.07.28 val PER: 0.1926
2026-01-03 21:47:50,931: t15.2025.01.10 val PER: 0.3623
2026-01-03 21:47:50,933: t15.2025.01.12 val PER: 0.2217
2026-01-03 21:47:50,934: t15.2025.03.14 val PER: 0.3787
2026-01-03 21:47:50,936: t15.2025.03.16 val PER: 0.2500
2026-01-03 21:47:50,939: t15.2025.03.30 val PER: 0.3609
2026-01-03 21:47:50,940: t15.2025.04.13 val PER: 0.2725
2026-01-03 21:47:51,172: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_8500
2026-01-03 21:47:59,666: Train batch 8600: loss: 19.02 grad norm: 64.67 time: 0.054
2026-01-03 21:48:16,800: Train batch 8800: loss: 19.73 grad norm: 64.25 time: 0.060
2026-01-03 21:48:34,209: Train batch 9000: loss: 19.65 grad norm: 65.97 time: 0.072
2026-01-03 21:48:34,211: Running test after training batch: 9000
2026-01-03 21:48:34,320: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:48:39,414: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned e the code at this point as will
2026-01-03 21:48:39,448: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-03 21:48:41,231: Val batch 9000: PER (avg): 0.2011 CTC Loss (avg): 19.6496 WER(1gram): 58.88% (n=64) time: 7.018
2026-01-03 21:48:41,234: WER lens: avg_true_words=6.16 avg_pred_words=6.20 max_pred_words=11
2026-01-03 21:48:41,236: t15.2023.08.13 val PER: 0.1726
2026-01-03 21:48:41,238: t15.2023.08.18 val PER: 0.1467
2026-01-03 21:48:41,240: t15.2023.08.20 val PER: 0.1557
2026-01-03 21:48:41,242: t15.2023.08.25 val PER: 0.1114
2026-01-03 21:48:41,244: t15.2023.08.27 val PER: 0.2379
2026-01-03 21:48:41,246: t15.2023.09.01 val PER: 0.1096
2026-01-03 21:48:41,247: t15.2023.09.03 val PER: 0.1995
2026-01-03 21:48:41,249: t15.2023.09.24 val PER: 0.1541
2026-01-03 21:48:41,251: t15.2023.09.29 val PER: 0.1736
2026-01-03 21:48:41,253: t15.2023.10.01 val PER: 0.2252
2026-01-03 21:48:41,255: t15.2023.10.06 val PER: 0.1195
2026-01-03 21:48:41,257: t15.2023.10.08 val PER: 0.2828
2026-01-03 21:48:41,259: t15.2023.10.13 val PER: 0.2669
2026-01-03 21:48:41,261: t15.2023.10.15 val PER: 0.2004
2026-01-03 21:48:41,263: t15.2023.10.20 val PER: 0.1913
2026-01-03 21:48:41,265: t15.2023.10.22 val PER: 0.1537
2026-01-03 21:48:41,267: t15.2023.11.03 val PER: 0.2191
2026-01-03 21:48:41,269: t15.2023.11.04 val PER: 0.0478
2026-01-03 21:48:41,270: t15.2023.11.17 val PER: 0.0715
2026-01-03 21:48:41,272: t15.2023.11.19 val PER: 0.0579
2026-01-03 21:48:41,274: t15.2023.11.26 val PER: 0.2065
2026-01-03 21:48:41,275: t15.2023.12.03 val PER: 0.1712
2026-01-03 21:48:41,277: t15.2023.12.08 val PER: 0.1651
2026-01-03 21:48:41,279: t15.2023.12.10 val PER: 0.1393
2026-01-03 21:48:41,281: t15.2023.12.17 val PER: 0.1892
2026-01-03 21:48:41,282: t15.2023.12.29 val PER: 0.1984
2026-01-03 21:48:41,284: t15.2024.02.25 val PER: 0.1573
2026-01-03 21:48:41,286: t15.2024.03.08 val PER: 0.2788
2026-01-03 21:48:41,287: t15.2024.03.15 val PER: 0.2533
2026-01-03 21:48:41,289: t15.2024.03.17 val PER: 0.2064
2026-01-03 21:48:41,290: t15.2024.05.10 val PER: 0.2155
2026-01-03 21:48:41,292: t15.2024.06.14 val PER: 0.2129
2026-01-03 21:48:41,294: t15.2024.07.19 val PER: 0.3039
2026-01-03 21:48:41,295: t15.2024.07.21 val PER: 0.1414
2026-01-03 21:48:41,296: t15.2024.07.28 val PER: 0.1890
2026-01-03 21:48:41,298: t15.2025.01.10 val PER: 0.3526
2026-01-03 21:48:41,299: t15.2025.01.12 val PER: 0.2086
2026-01-03 21:48:41,301: t15.2025.03.14 val PER: 0.3669
2026-01-03 21:48:41,302: t15.2025.03.16 val PER: 0.2474
2026-01-03 21:48:41,303: t15.2025.03.30 val PER: 0.3575
2026-01-03 21:48:41,305: t15.2025.04.13 val PER: 0.2696
2026-01-03 21:48:41,535: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_9000
2026-01-03 21:48:58,537: Train batch 9200: loss: 13.96 grad norm: 54.54 time: 0.055
2026-01-03 21:49:15,696: Train batch 9400: loss: 10.44 grad norm: 52.06 time: 0.067
2026-01-03 21:49:24,316: Running test after training batch: 9500
2026-01-03 21:49:24,457: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:49:29,259: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the code at this point as will
2026-01-03 21:49:29,293: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost nett
2026-01-03 21:49:31,056: Val batch 9500: PER (avg): 0.1975 CTC Loss (avg): 19.3493 WER(1gram): 55.33% (n=64) time: 6.738
2026-01-03 21:49:31,058: WER lens: avg_true_words=6.16 avg_pred_words=6.16 max_pred_words=11
2026-01-03 21:49:31,061: t15.2023.08.13 val PER: 0.1632
2026-01-03 21:49:31,063: t15.2023.08.18 val PER: 0.1375
2026-01-03 21:49:31,065: t15.2023.08.20 val PER: 0.1541
2026-01-03 21:49:31,067: t15.2023.08.25 val PER: 0.1130
2026-01-03 21:49:31,069: t15.2023.08.27 val PER: 0.2412
2026-01-03 21:49:31,070: t15.2023.09.01 val PER: 0.1128
2026-01-03 21:49:31,072: t15.2023.09.03 val PER: 0.1960
2026-01-03 21:49:31,074: t15.2023.09.24 val PER: 0.1529
2026-01-03 21:49:31,075: t15.2023.09.29 val PER: 0.1634
2026-01-03 21:49:31,077: t15.2023.10.01 val PER: 0.2114
2026-01-03 21:49:31,079: t15.2023.10.06 val PER: 0.1163
2026-01-03 21:49:31,081: t15.2023.10.08 val PER: 0.2882
2026-01-03 21:49:31,083: t15.2023.10.13 val PER: 0.2545
2026-01-03 21:49:31,084: t15.2023.10.15 val PER: 0.2030
2026-01-03 21:49:31,086: t15.2023.10.20 val PER: 0.2013
2026-01-03 21:49:31,087: t15.2023.10.22 val PER: 0.1370
2026-01-03 21:49:31,089: t15.2023.11.03 val PER: 0.2090
2026-01-03 21:49:31,091: t15.2023.11.04 val PER: 0.0375
2026-01-03 21:49:31,092: t15.2023.11.17 val PER: 0.0653
2026-01-03 21:49:31,094: t15.2023.11.19 val PER: 0.0599
2026-01-03 21:49:31,096: t15.2023.11.26 val PER: 0.1978
2026-01-03 21:49:31,097: t15.2023.12.03 val PER: 0.1723
2026-01-03 21:49:31,099: t15.2023.12.08 val PER: 0.1724
2026-01-03 21:49:31,100: t15.2023.12.10 val PER: 0.1340
2026-01-03 21:49:31,102: t15.2023.12.17 val PER: 0.1902
2026-01-03 21:49:31,103: t15.2023.12.29 val PER: 0.1922
2026-01-03 21:49:31,105: t15.2024.02.25 val PER: 0.1601
2026-01-03 21:49:31,107: t15.2024.03.08 val PER: 0.2760
2026-01-03 21:49:31,108: t15.2024.03.15 val PER: 0.2502
2026-01-03 21:49:31,110: t15.2024.03.17 val PER: 0.1939
2026-01-03 21:49:31,112: t15.2024.05.10 val PER: 0.2095
2026-01-03 21:49:31,114: t15.2024.06.14 val PER: 0.2098
2026-01-03 21:49:31,115: t15.2024.07.19 val PER: 0.2953
2026-01-03 21:49:31,117: t15.2024.07.21 val PER: 0.1524
2026-01-03 21:49:31,119: t15.2024.07.28 val PER: 0.1912
2026-01-03 21:49:31,121: t15.2025.01.10 val PER: 0.3554
2026-01-03 21:49:31,122: t15.2025.01.12 val PER: 0.2117
2026-01-03 21:49:31,124: t15.2025.03.14 val PER: 0.3757
2026-01-03 21:49:31,127: t15.2025.03.16 val PER: 0.2369
2026-01-03 21:49:31,129: t15.2025.03.30 val PER: 0.3425
2026-01-03 21:49:31,131: t15.2025.04.13 val PER: 0.2568
2026-01-03 21:49:31,133: New best val WER(1gram) 57.11% --> 55.33%
2026-01-03 21:49:31,460: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_9500
2026-01-03 21:49:39,851: Train batch 9600: loss: 11.42 grad norm: 53.98 time: 0.073
2026-01-03 21:49:56,823: Train batch 9800: loss: 17.04 grad norm: 63.48 time: 0.063
2026-01-03 21:50:13,992: Train batch 10000: loss: 8.15 grad norm: 44.30 time: 0.061
2026-01-03 21:50:13,994: Running test after training batch: 10000
2026-01-03 21:50:14,098: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:50:18,924: WER debug example
  GT : you can see the code at this point as well
  PR : yule canned sci the could at this point is will
2026-01-03 21:50:18,957: WER debug example
  GT : how does it keep the cost down
  PR : houde dust it keep the cost sette
2026-01-03 21:50:20,754: Val batch 10000: PER (avg): 0.1947 CTC Loss (avg): 18.9722 WER(1gram): 55.58% (n=64) time: 6.758
2026-01-03 21:50:20,757: WER lens: avg_true_words=6.16 avg_pred_words=6.08 max_pred_words=11
2026-01-03 21:50:20,759: t15.2023.08.13 val PER: 0.1601
2026-01-03 21:50:20,761: t15.2023.08.18 val PER: 0.1517
2026-01-03 21:50:20,763: t15.2023.08.20 val PER: 0.1485
2026-01-03 21:50:20,765: t15.2023.08.25 val PER: 0.1190
2026-01-03 21:50:20,766: t15.2023.08.27 val PER: 0.2251
2026-01-03 21:50:20,768: t15.2023.09.01 val PER: 0.1096
2026-01-03 21:50:20,770: t15.2023.09.03 val PER: 0.2031
2026-01-03 21:50:20,771: t15.2023.09.24 val PER: 0.1626
2026-01-03 21:50:20,773: t15.2023.09.29 val PER: 0.1672
2026-01-03 21:50:20,775: t15.2023.10.01 val PER: 0.2081
2026-01-03 21:50:20,776: t15.2023.10.06 val PER: 0.1130
2026-01-03 21:50:20,778: t15.2023.10.08 val PER: 0.2760
2026-01-03 21:50:20,779: t15.2023.10.13 val PER: 0.2428
2026-01-03 21:50:20,781: t15.2023.10.15 val PER: 0.2076
2026-01-03 21:50:20,783: t15.2023.10.20 val PER: 0.2081
2026-01-03 21:50:20,784: t15.2023.10.22 val PER: 0.1392
2026-01-03 21:50:20,786: t15.2023.11.03 val PER: 0.2056
2026-01-03 21:50:20,787: t15.2023.11.04 val PER: 0.0341
2026-01-03 21:50:20,789: t15.2023.11.17 val PER: 0.0638
2026-01-03 21:50:20,791: t15.2023.11.19 val PER: 0.0579
2026-01-03 21:50:20,792: t15.2023.11.26 val PER: 0.1913
2026-01-03 21:50:20,794: t15.2023.12.03 val PER: 0.1712
2026-01-03 21:50:20,795: t15.2023.12.08 val PER: 0.1618
2026-01-03 21:50:20,797: t15.2023.12.10 val PER: 0.1327
2026-01-03 21:50:20,798: t15.2023.12.17 val PER: 0.1871
2026-01-03 21:50:20,800: t15.2023.12.29 val PER: 0.1942
2026-01-03 21:50:20,802: t15.2024.02.25 val PER: 0.1545
2026-01-03 21:50:20,803: t15.2024.03.08 val PER: 0.2731
2026-01-03 21:50:20,805: t15.2024.03.15 val PER: 0.2439
2026-01-03 21:50:20,806: t15.2024.03.17 val PER: 0.1960
2026-01-03 21:50:20,808: t15.2024.05.10 val PER: 0.1961
2026-01-03 21:50:20,809: t15.2024.06.14 val PER: 0.2050
2026-01-03 21:50:20,811: t15.2024.07.19 val PER: 0.2900
2026-01-03 21:50:20,812: t15.2024.07.21 val PER: 0.1359
2026-01-03 21:50:20,814: t15.2024.07.28 val PER: 0.1897
2026-01-03 21:50:20,815: t15.2025.01.10 val PER: 0.3526
2026-01-03 21:50:20,817: t15.2025.01.12 val PER: 0.2025
2026-01-03 21:50:20,818: t15.2025.03.14 val PER: 0.3580
2026-01-03 21:50:20,820: t15.2025.03.16 val PER: 0.2579
2026-01-03 21:50:20,821: t15.2025.03.30 val PER: 0.3379
2026-01-03 21:50:20,823: t15.2025.04.13 val PER: 0.2525
2026-01-03 21:50:21,063: Saved model to checkpoint: trained_models/dev_run/checkpoint/checkpoint_batch_10000
2026-01-03 21:50:37,735: Train batch 10200: loss: 8.75 grad norm: 43.99 time: 0.050
2026-01-03 21:50:54,829: Train batch 10400: loss: 13.19 grad norm: 60.01 time: 0.072
2026-01-03 21:51:03,409: Running test after training batch: 10500
2026-01-03 21:51:03,504: WER debug GT example: You can see the code at this point as well.
2026-01-03 21:51:08,732: WER debug example
  GT : you can see the code at this point as well
  PR : yu canned sci the code at this point as will
2026-01-03 21:51:08,766: WER debug example
  GT : how does it keep the cost down
  PR : houde just it keep the cost et
2026-01-03 21:51:10,587: Val batch 10500: PER (avg): 0.1924 CTC Loss (avg): 18.7144 WER(1gram): 54.31% (n=64) time: 7.176
2026-01-03 21:51:10,589: WER lens: avg_true_words=6.16 avg_pred_words=6.12 max_pred_words=11
2026-01-03 21:51:10,592: t15.2023.08.13 val PER: 0.1570
2026-01-03 21:51:10,594: t15.2023.08.18 val PER: 0.1475
2026-01-03 21:51:10,595: t15.2023.08.20 val PER: 0.1461
2026-01-03 21:51:10,597: t15.2023.08.25 val PER: 0.1205
2026-01-03 21:51:10,599: t15.2023.08.27 val PER: 0.2058
2026-01-03 21:51:10,601: t15.2023.09.01 val PER: 0.1136
2026-01-03 21:51:10,602: t15.2023.09.03 val PER: 0.2007
2026-01-03 21:51:10,604: t15.2023.09.24 val PER: 0.1638
2026-01-03 21:51:10,606: t15.2023.09.29 val PER: 0.1659
2026-01-03 21:51:10,607: t15.2023.10.01 val PER: 0.2107
2026-01-03 21:51:10,609: t15.2023.10.06 val PER: 0.1163
2026-01-03 21:51:10,610: t15.2023.10.08 val PER: 0.2733
2026-01-03 21:51:10,612: t15.2023.10.13 val PER: 0.2444
2026-01-03 21:51:10,613: t15.2023.10.15 val PER: 0.2004
2026-01-03 21:51:10,615: t15.2023.10.20 val PER: 0.1913
2026-01-03 21:51:10,616: t15.2023.10.22 val PER: 0.1403
2026-01-03 21:51:10,618: t15.2023.11.03 val PER: 0.2110
2026-01-03 21:51:10,619: t15.2023.11.04 val PER: 0.0375
2026-01-03 21:51:10,621: t15.2023.11.17 val PER: 0.0653
2026-01-03 21:51:10,622: t15.2023.11.19 val PER: 0.0619
2026-01-03 21:51:10,624: t15.2023.11.26 val PER: 0.1833
2026-01-03 21:51:10,625: t15.2023.12.03 val PER: 0.1639
2026-01-03 21:51:10,627: t15.2023.12.08 val PER: 0.1598
2026-01-03 21:51:10,628: t15.2023.12.10 val PER: 0.1419
2026-01-03 21:51:10,630: t15.2023.12.17 val PER: 0.1809
2026-01-03 21:51:10,631: t15.2023.12.29 val PER: 0.1833
2026-01-03 21:51:10,633: t15.2024.02.25 val PER: 0.1531
2026-01-03 21:51:10,634: t15.2024.03.08 val PER: 0.2731
2026-01-03 21:51:10,636: t15.2024.03.15 val PER: 0.2389
2026-01-03 21:51:10,637: t15.2024.03.17 val PER: 0.1876
2026-01-03 21:51:10,639: t15.2024.05.10 val PER: 0.2006
2026-01-03 21:51:10,640: t15.2024.06.14 val PER: 0.2035
2026-01-03 21:51:10,642: t15.2024.07.19 val PER: 0.2920
2026-01-03 21:51:10,643: t15.2024.07.21 val PER: 0.1317
2026-01-03 21:51:10,645: t15.2024.07.28 val PER: 0.1787
2026-01-03 21:51:10,647: t15.2025.01.10 val PER: 0.3540
2026-01-03 21:51:10,649: t15.2025.01.12 val PER: 0.2063
2026-01-03 21:51:10,650: t15.2025.03.14 val PER: 0.3565
2026-01-03 21:51:10,652: t15.2025.03.16 val PER: 0.2395
2026-01-03 21:51:10,653: t15.2025.03.30 val PER: 0.3471
2026-01-03 21:51:10,654: t15.2025.04.13 val PER: 0.2439
2026-01-03 21:51:10,656: New best val WER(1gram) 55.33% --> 54.31%
[1;34mwandb[0m: 
[1;34mwandb[0m:  View run [33mbase[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../tmp/e12511253_b2t_348244/wandb/wandb/run-20260103_213252-mcf8sm1g/logs[0m
