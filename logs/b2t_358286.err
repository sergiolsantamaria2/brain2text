[2026-01-17T12:54:17.618] error: TMPDIR [/tmp] is not writeable
[2026-01-17T12:54:17.618] error: Setting TMPDIR to /tmp
/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/utils/cpp_extension.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging  # type: ignore[attr-defined]
wandb: Currently logged in as: sergiolsantamaria (sergiolsantamaria-tu-wien) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run 8a0c6988
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in /home/e12511253/tmp/e12511253_b2t_358286/wandb/wandb/run-20260117_125423-8a0c6988
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run No diphones
wandb: â­ï¸ View project at https://wandb.ai/sergiolsantamaria-tu-wien/brain2text
wandb: ðŸš€ View run at https://wandb.ai/sergiolsantamaria-tu-wien/brain2text/runs/8a0c6988
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0117 12:54:24.114959 2894109 brain_speech_decoder.h:52] Reading fst /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil/TLG.fst
I0117 12:54:24.150698 2894109 brain_speech_decoder.h:58] Reading lm fst /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil/TLG.fst
I0117 12:54:24.214577 2894109 brain_speech_decoder.h:81] Reading symbol table /home/e12511253/Brain2Text/brain2text/references/nejm-brain-to-text/language_model/pretrained_language_models/openwebtext_1gram_lm_sil/words.txt
/home/e12511253/miniforge3/envs/brain2text/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/var/spool/slurm/d/job358286/slurm_script: line 215: 2894109 Killed                  python -u - "$BASE" "$OVR1" "$CFG" <<'PY'
import os, sys, ctypes, runpy

base, ovr1, cfg = sys.argv[1:4]

# Make future dlopen() global
sys.setdlopenflags(os.RTLD_GLOBAL | os.RTLD_NOW)

# Preload cudart globally (critical)
ctypes.CDLL(os.environ["CUDART_SO"], mode=ctypes.RTLD_GLOBAL)

# Run the training module with the same CLI args
sys.argv = [
  "train_model",
  "--config", base,
  "--config", cfg,
  "--config", ovr1,   # override al final
]

runpy.run_module("brain2text.model_training.train_model", run_name="__main__")
PY

[2026-01-17T13:53:04.455] error: Detected 1 oom_kill event in StepId=358286.batch. Some of the step tasks have been OOM Killed.
